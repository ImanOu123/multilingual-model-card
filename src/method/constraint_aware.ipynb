{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 199, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_2215634/2267441587.py\", line 1, in <module>\n",
      "    from transformers import AutoProcessor\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/transformers/utils/__init__.py\", line 33, in <module>\n",
      "    from .generic import (\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/transformers/utils/generic.py\", line 442, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/__init__.py\", line 1382, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/functional.py\", line 7, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/home/ubuntu/miniconda3/envs/mmmc/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "from transformers import SeamlessM4TModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"/home/ubuntu/multilingual-model-card/src/dictionary_collection/mturk/whole_set/Chinese_whole.csv\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-Large\", use_fast=False, force_download=True)\n",
    "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-Large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_terms_dict = {row['word']: row['gold_human'] for _, row in df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_terms_dict = {\"Decision Transformer\": \"决策Transformer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor\n",
    "\n",
    "seamless_lang_dict = {\n",
    "    \"Arabic\": \"arb\",\n",
    "    \"Chinese\": \"cmn\",\n",
    "    \"English\": \"eng\",\n",
    "    \"French\": \"fra\",\n",
    "    \"Japanese\": \"jpn\",\n",
    "    \"Russian\": \"rus\",\n",
    "}\n",
    "\n",
    "class TerminologyAwareLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer, en_text, lang, soft_penalty, seamless_lang_dict, tokens):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.en_text = en_text\n",
    "        self.lang = lang\n",
    "        self.soft_penalty = soft_penalty\n",
    "        self.seamless_lang_dict = seamless_lang_dict\n",
    "        self.tokens = tokens\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # print(scores)\n",
    "        for idx in range(scores.shape[-1]):\n",
    "            tokens = list({item for sublist in self.tokens for item in sublist})\n",
    "            if idx in tokens:  # Penalize tokens not in the translation\n",
    "                scores[:, idx] /= self.soft_penalty  # Apply soft penalty\n",
    "        # print(scores)\n",
    "        return scores\n",
    "\n",
    "def generate_with_constraints(model, tokenizer, ai_terms_dict, en_text, lang, num_beams=5, soft_penalty=0.8):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(en_text, src_lang = seamless_lang_dict['English'], return_tensors=\"pt\")\n",
    "    # print(inputs)\n",
    "    \n",
    "    tokens = []\n",
    "    terms = []\n",
    "    for term, translation in ai_terms_dict.items():\n",
    "        if term in en_text:  # Context-sensitive dictionary match\n",
    "                token = tokenizer(translation, src_lang=seamless_lang_dict[lang])['input_ids']\n",
    "                tokens.append(token)\n",
    "                terms.append(term)\n",
    "    # print(tokens)\n",
    "    # print(terms)\n",
    "    \n",
    "    logits_processor = TerminologyAwareLogitsProcessor(\n",
    "        tokenizer=tokenizer,\n",
    "        en_text=en_text,\n",
    "        lang=lang,\n",
    "        soft_penalty=soft_penalty,\n",
    "        seamless_lang_dict=seamless_lang_dict,\n",
    "        tokens = tokens,\n",
    "    )\n",
    "\n",
    "    # Prepare the beam search decoder\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        tgt_lang=seamless_lang_dict[lang],\n",
    "        generate_speech=False,\n",
    "        # num_beams=num_beams,\n",
    "        # early_stopping=True,\n",
    "        # output_scores=True,\n",
    "        # return_dict_in_generate=True,\n",
    "        logits_processor=[logits_processor]\n",
    "    )\n",
    "\n",
    "    # Decode the output sequence\n",
    "    output = tokenizer.decode(generated_ids[0].tolist()[0], skip_special_tokens=True)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"1: For all the methods, we used 10-fold cross validation (i.e., each fold we have 556 training and 62 test samples) to tune free parameters, e.g., the kernel form and parameters for GPOR and LapSVM. Note that all the alternative methods stack X and Z together into a whole data matrix and ignore their heterogeneous nature.\"\n",
    "# translation = generate_with_constraints(model, processor, ai_terms_dict, input_text, lang='Chinese', soft_penalty=0.85)\n",
    "# print(\"Translated Text:\", translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'add_special_tokens': False} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1:对于所有方法,我们使用了10倍的交叉验证(即,每倍我们有556个训练和62个测试样本)来调整自由参数,例如,内核形式和GPOR和LapSVM的参数.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_inputs = processor(\n",
    "    text=input_text,\n",
    "    src_lang=seamless_lang_dict['English'],\n",
    "    return_tensors='pt'\n",
    ")\n",
    "output_tokens = model.generate(**text_inputs, tgt_lang=seamless_lang_dict['Chinese'], generate_speech=False)\n",
    "processor.decode(output_tokens[0].tolist()[0], skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmmc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
