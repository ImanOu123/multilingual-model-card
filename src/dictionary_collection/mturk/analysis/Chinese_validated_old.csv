,Unnamed: 0,word,gold,exact_match_ratio,prediction_count,prediction_ratio,validated_translation,reason,back_translations
0,0,10-fold cross validation,十折交叉验证,0.0,8,"[{'word': '10折交叉验证', 'ratio': 0.75}, {'word': '10倍交叉验证', 'ratio': 0.125}, {'word': '10 倍交叉验证', 'ratio': 0.125}]",10折交叉验证,{},[]
1,1,1D convolution,1D 卷积,0.0,8,"[{'word': '一维卷积', 'ratio': 1.0}]",一维卷积,{},[]
2,2,2 norm,2范数,0.375,8,"[{'word': '二范数', 'ratio': 0.625}, {'word': '2范数', 'ratio': 0.375}]",二范数,{},[]
3,3,2D convolution,二维卷积,1.0,8,"[{'word': '二维卷积', 'ratio': 1.0}]",二维卷积,{},[]
4,4,2D image,二维图像,1.0,8,"[{'word': '二维图像', 'ratio': 1.0}]",二维图像,{},[]
5,5,2D image synthesis,二维图像合成,0.6666666666666666,9,"[{'word': '二维图像合成', 'ratio': 0.6666666666666666}, {'word': '图像合成', 'ratio': 0.1111111111111111}, {'word': '2D图像合成', 'ratio': 0.1111111111111111}, {'word': '维图像合成', 'ratio': 0.1111111111111111}]",二维图像合成,{},[]
6,6,2D-3D correspondence,二维三维对应关系,0.0,9,"[{'word': '二维-三维对应关系', 'ratio': 0.6666666666666666}, {'word': '对应关系', 'ratio': 0.1111111111111111}, {'word': '2D-3D对应关系', 'ratio': 0.1111111111111111}, {'word': '维-三维对应', 'ratio': 0.1111111111111111}]",二维-三维对应关系,{},[]
7,7,3D bounding box,三维包围盒,0.0,9,"[{'word': '三维边界框', 'ratio': 0.6666666666666666}, {'word': '边界框', 'ratio': 0.1111111111111111}, {'word': '3D边界框', 'ratio': 0.1111111111111111}, {'word': '维边界框', 'ratio': 0.1111111111111111}]",三维边界框,{},[]
8,8,3D computer vision,三维计算机视觉,0.6666666666666666,9,"[{'word': '三维计算机视觉', 'ratio': 0.6666666666666666}, {'word': '计算机视觉', 'ratio': 0.1111111111111111}, {'word': '3D计算机视觉', 'ratio': 0.1111111111111111}, {'word': '维计算机视觉', 'ratio': 0.1111111111111111}]",三维计算机视觉,{},[]
9,9,3D convolutional network,三维卷积网络,0.6666666666666666,9,"[{'word': '三维卷积网络', 'ratio': 0.6666666666666666}, {'word': '卷积网络', 'ratio': 0.1111111111111111}, {'word': '3D卷积网络', 'ratio': 0.1111111111111111}, {'word': '维卷积网络', 'ratio': 0.1111111111111111}]",三维卷积网络,{},[]
10,10,3D geometry,三维几何,0.875,8,"[{'word': '三维几何', 'ratio': 0.875}, {'word': '3D几何', 'ratio': 0.125}]",三维几何,{},[]
11,11,3D human pose estimation,三维人体姿态估计,0.875,8,"[{'word': '三维人体姿态估计', 'ratio': 0.875}, {'word': '3D人类姿态估计', 'ratio': 0.125}]",三维人体姿态估计,{},[]
12,12,3D localization,三维定位,0.875,8,"[{'word': '三维定位', 'ratio': 0.875}, {'word': '3D定位', 'ratio': 0.125}]",三维定位,{},[]
13,13,3D mesh,3D网格,0.125,8,"[{'word': '三维网格', 'ratio': 0.875}, {'word': '3D网格', 'ratio': 0.125}]",三维网格,{},[]
14,14,3D model,三维模型,0.875,8,"[{'word': '三维模型', 'ratio': 0.875}, {'word': '3D模型', 'ratio': 0.125}]",三维模型,{},[]
15,15,3D object detection,三维目标检测,0.0,8,"[{'word': '3D物体检测', 'ratio': 0.5}, {'word': '三维物体检测', 'ratio': 0.5}]",3D物体检测,{},[]
16,16,3D point,三维点,0.5,8,"[{'word': '3D点', 'ratio': 0.5}, {'word': '三维点', 'ratio': 0.5}]",3D点,{},[]
17,17,3D point cloud,三维点云,0.5,8,"[{'word': '3D点云', 'ratio': 0.5}, {'word': '三维点云', 'ratio': 0.5}]",3D点云,{},[]
18,18,3D pose,三维姿势,0.0,8,"[{'word': '3D姿态', 'ratio': 0.5}, {'word': '三维姿态', 'ratio': 0.5}]",3D姿态,{},[]
19,19,3D reconstruction,三维重建,0.5,8,"[{'word': '3D重建', 'ratio': 0.5}, {'word': '三维重建', 'ratio': 0.5}]",3D重建,{},[]
20,20,3D scene,三维场景,0.7,10,"[{'word': '三维场景', 'ratio': 0.7}, {'word': '3D场景', 'ratio': 0.3}]",三维场景,{},[]
21,21,3D scene geometry,3D场景几何,0.3,10,"[{'word': '三维场景几何', 'ratio': 0.7}, {'word': '3D场景几何', 'ratio': 0.3}]",三维场景几何,{},[]
22,22,3D structure,三维结构,0.7,10,"[{'word': '三维结构', 'ratio': 0.7}, {'word': '3D结构', 'ratio': 0.3}]",三维结构,{},[]
23,23,5-fold cross validation,5重交叉验证,0.0,10,"[{'word': '5折交叉验证', 'ratio': 0.9}, {'word': '五折交叉验证', 'ratio': 0.1}]",5折交叉验证,{},[]
24,24,A * algorithm,A*算法,0.8,10,"[{'word': 'A*算法', 'ratio': 0.8}, {'word': 'A* 算法', 'ratio': 0.2}]",A*算法,{},[]
25,25,A/B test,A/B测试,0.3333333333333333,9,"[{'word': 'A/B 测试', 'ratio': 0.5555555555555556}, {'word': 'A/B测试', 'ratio': 0.3333333333333333}, {'word': '测试', 'ratio': 0.1111111111111111}]",A/B 测试,{},[]
26,26,A2C,A2C,0.7777777777777778,9,"[{'word': 'A2C', 'ratio': 0.7777777777777778}, {'word': '他住过', 'ratio': 0.1111111111111111}, {'word': '优势-价值', 'ratio': 0.1111111111111111}]",A2C,{},[]
27,27,Ablation study,消融研究,1.0,7,"[{'word': '消融研究', 'ratio': 1.0}]",消融研究,{},[]
28,28,Accuracy,准确性,0.2857142857142857,7,"[{'word': '准确率', 'ratio': 0.5714285714285714}, {'word': '准确性', 'ratio': 0.2857142857142857}, {'word': '准确度', 'ratio': 0.14285714285714285}]",准确率,{},[]
29,29,Active learning,主动学习,1.0,7,"[{'word': '主动学习', 'ratio': 1.0}]",主动学习,{},[]
30,30,Adafactor,Adafactor,0.5714285714285714,7,"[{'word': 'Adafactor', 'ratio': 0.5714285714285714}, {'word': '阿達法特', 'ratio': 0.14285714285714285}, {'word': '阿达法特', 'ratio': 0.14285714285714285}, {'word': 'Adafactor优化器', 'ratio': 0.14285714285714285}]",Adafactor,{},[]
31,31,Adam,Adam,0.6,5,"[{'word': 'Adam', 'ratio': 0.6}, {'word': '亚当', 'ratio': 0.2}, {'word': '卷积核', 'ratio': 0.2}]",Adam,{},[]
32,32,Adam algorithm,Adam算法,0.6,5,"[{'word': 'Adam算法', 'ratio': 0.6}, {'word': 'Adam 算法', 'ratio': 0.2}, {'word': '卷积核', 'ratio': 0.2}]",Adam算法,{},[]
33,33,Adam optimiser,亚当优化器,0.0,5,"[{'word': 'Adam优化器', 'ratio': 0.6}, {'word': '亚当精炼器', 'ratio': 0.2}, {'word': '卷积核', 'ratio': 0.2}]",Adam优化器,{},[]
34,34,Adam optimization,Adam优化算法,0.0,5,"[{'word': 'Adam优化', 'ratio': 0.6}, {'word': '亚当·奥伊', 'ratio': 0.2}, {'word': '卷积核', 'ratio': 0.2}]",Adam优化,{},[]
35,35,Adam optimization algorithm,Adam优化算法,0.6,5,"[{'word': 'Adam优化算法', 'ratio': 0.6}, {'word': 'Adam 优化算法', 'ratio': 0.2}, {'word': '卷积核', 'ratio': 0.2}]",Adam优化算法,{},[]
36,36,Adam optimizer,Adam优化器,0.7777777777777778,9,"[{'word': 'Adam优化器', 'ratio': 0.7777777777777778}, {'word': 'Adam 优化器', 'ratio': 0.1111111111111111}, {'word': '优化器', 'ratio': 0.1111111111111111}]",Adam优化器,{},[]
37,37,Adapter,适配器,1.0,9,"[{'word': '适配器', 'ratio': 1.0}]",适配器,{},[]
38,38,Algorithm,算法,1.0,9,"[{'word': '算法', 'ratio': 1.0}]",算法,{},[]
39,39,Answer Set Programming,答集规划,0.0,9,"[{'word': '答案集编程', 'ratio': 1.0}]",答案集编程,{},[]
40,40,Apriori,先验,0.0,9,"[{'word': '先验算法', 'ratio': 0.4444444444444444}, {'word': '先驗', 'ratio': 0.2222222222222222}, {'word': 'Apriori', 'ratio': 0.2222222222222222}, {'word': '阿普里奥', 'ratio': 0.1111111111111111}]",先验算法,"1. Rank: 先验算法, 先驗, Apriori, 阿普里奥

2. Explanation: The term ""先验算法"" (a priori algorithm) is the best fit because it accurately conveys the meaning of the English term ""Apriori"" within the context of data mining and AI. The term ""先验"" (a priori) is also a strong candidate, as it retains the original concept but lacks the specificity of ""算法"" (algorithm), which is crucial in the context of algorithmic processes. The term ""Apriori"" as a direct transliteration does not provide any additional context or clarity, making it less suitable for readers unfamiliar with the term. Lastly, ""阿普里奥"" (Aprilio) is a transliteration that does not convey the intended meaning and is not commonly used in the AI domain. Therefore, ""先验算法"" is the most semantically accurate and contextually appropriate choice for this specific usage in AI terminology.","['a priori algorithm', 'a priori', 'A priori', 'Aprilio']"
41,41,Apriori algorithm,Apriori算法,0.1111111111111111,9,"[{'word': '先验算法', 'ratio': 0.3333333333333333}, {'word': 'Apriori演算法', 'ratio': 0.1111111111111111}, {'word': '先验i算法', 'ratio': 0.1111111111111111}, {'word': '先驗演算法', 'ratio': 0.1111111111111111}, {'word': 'Apriori算法', 'ratio': 0.1111111111111111}, {'word': '算法', 'ratio': 0.1111111111111111}, {'word': 'Apriori 算法', 'ratio': 0.1111111111111111}]",Apriori算法,"1. Rank: Apriori算法, Apriori演算法, Apriori 算法, 先验算法, 先验i算法, 先驗演算法, 算法

2. Explanation: The term ""Apriori算法"" is the best fit because it retains the original name ""Apriori"" while clearly indicating that it is an algorithm by using the Chinese word ""算法."" This combination is widely recognized in the AI and data mining communities, making it semantically accurate and contextually appropriate. The use of ""Apriori"" in its original form helps maintain the specific reference to the algorithm, which is crucial in technical discussions. 

""Apriori演算法"" is also a good candidate, but the term ""演算法"" (which translates to ""algorithm"" in a more general sense) is less commonly used in the context of this specific algorithm compared to ""算法."" 

The other candidates, such as ""先验算法"" and ""先验i算法,"" introduce the term ""先验"" (a priori) but do not retain the specific name ""Apriori,"" which is essential for clarity and recognition in the AI domain. The term ""算法"" alone is too vague and lacks the necessary specificity. 

Overall, ""Apriori算法"" is the most accurate and contextually fitting translation for the term in the AI domain.","['a priori algorithm', 'Apriori algorithm', 'a priori algorithm', 'a priori algorithm', 'Apriori algorithm', 'algorithm', 'Apriori algorithm']"
42,42,Arcade Learning Environment,街机学习环境,0.8888888888888888,9,"[{'word': '街机学习环境', 'ratio': 0.8888888888888888}, {'word': '游戏学习环境', 'ratio': 0.1111111111111111}]",街机学习环境,{},[]
43,43,Artificial Intelligence,人工智能,1.0,10,"[{'word': '人工智能', 'ratio': 1.0}]",人工智能,{},[]
44,44,Attention,注意力,0.6,10,"[{'word': '注意力', 'ratio': 0.6}, {'word': '注意力机制', 'ratio': 0.4}]",注意力,{},[]
45,45,Autoencoder,自编码器,1.0,10,"[{'word': '自编码器', 'ratio': 1.0}]",自编码器,{},[]
46,46,Automatic Speech Recognition,自动语音识别,1.0,10,"[{'word': '自动语音识别', 'ratio': 1.0}]",自动语音识别,{},[]
47,47,Autonomous Systems,自主系统,0.8,10,"[{'word': '自主系统', 'ratio': 0.8}, {'word': '自治系统', 'ratio': 0.2}]",自主系统,{},[]
48,48,Average Pool,平均池化层,0.0,10,"[{'word': '平均池化', 'ratio': 1.0}]",平均池化,{},[]
49,49,Average Precision,平均精度,0.8,10,"[{'word': '平均精度', 'ratio': 0.8}, {'word': '平均精确率', 'ratio': 0.2}]",平均精度,{},[]
50,50,Azuma-Hoeffding inequality,阿兹玛-霍夫丁不等式,0.0,10,"[{'word': '阿祖马-霍夫丁不等式', 'ratio': 0.3}, {'word': 'Azuma-Hoeffding不等式', 'ratio': 0.2}, {'word': 'Azuma-Hoeffding 不等式', 'ratio': 0.1}, {'word': '阿祖玛-霍夫丁不等式', 'ratio': 0.1}, {'word': '不等式', 'ratio': 0.1}, {'word': '阿祖玛-赫夫丁不等式', 'ratio': 0.1}, {'word': '安祖马-霍夫丁不等式', 'ratio': 0.1}]",阿祖马-霍夫丁不等式,"1. Rank: 阿祖马-霍夫丁不等式, Azuma-Hoeffding不等式, Azuma-Hoeffding 不等式, 阿祖玛-霍夫丁不等式, 阿祖玛-赫夫丁不等式, 安祖马-霍夫丁不等式, 不等式

2. Explanation: The term ""阿祖马-霍夫丁不等式"" is the best fit because it accurately retains the original names ""Azuma"" and ""Hoeffding"" in a way that is recognizable and consistent with established terminology in the AI and mathematical literature. The use of ""阿祖马"" for ""Azuma"" and ""霍夫丁"" for ""Hoeffding"" aligns with common transliterations used in academic contexts, ensuring that readers familiar with the field can easily identify the reference. 

The other candidates, such as ""Azuma-Hoeffding不等式"" and ""Azuma-Hoeffding 不等式,"" while also accurate, do not provide the same level of semantic clarity in Chinese as they mix English and Chinese without fully localizing the names. The transliterations ""阿祖玛"" and ""安祖马"" deviate from the standard transliteration of ""Azuma,"" which could lead to confusion. 

Overall, the first choice maintains both semantic accuracy and contextual fit, making it the most appropriate for use in AI-related discussions.","['Azuma-Hoffding inequality', 'Azuma-Hoeffding inequality', 'Azuma-Hoeffding inequality', 'Azuma-Hoffding inequality', 'inequality', 'Ajumma-Hefting inequality', 'Anzuma-Hoffding inequality']"
51,51,B-spline,B样条,1.0,10,"[{'word': 'B样条', 'ratio': 1.0}]",B样条,{},[]
52,52,Backbone,主干网络,0.6,10,"[{'word': '主干网络', 'ratio': 0.6}, {'word': '骨干网络', 'ratio': 0.3}, {'word': '骨干', 'ratio': 0.1}]",主干网络,{},[]
53,53,Backpropagation,反向传播,1.0,10,"[{'word': '反向传播', 'ratio': 1.0}]",反向传播,{},[]
54,54,Baseline,基准 (Baseline),0.0,10,"[{'word': '基线', 'ratio': 0.5}, {'word': '基准线', 'ratio': 0.4}, {'word': '基准', 'ratio': 0.1}]",基线,{},[]
55,55,Basis Pursuit,基追求,0.2,10,"[{'word': '基追踪', 'ratio': 0.5}, {'word': '基追求', 'ratio': 0.2}, {'word': '基础追踪', 'ratio': 0.2}, {'word': '基追算法', 'ratio': 0.1}]",基追踪,{},[]
56,56,Batch Normalization,批量归一化,0.9,10,"[{'word': '批量归一化', 'ratio': 0.9}, {'word': '批归一化', 'ratio': 0.1}]",批量归一化,{},[]
57,57,Baum-Welch algorithm,鲍姆-韦尔奇算法,0.8,10,"[{'word': '鲍姆-韦尔奇算法', 'ratio': 0.8}, {'word': '鲍姆-威尔奇算法', 'ratio': 0.1}, {'word': '鲍姆-维尔奇算法', 'ratio': 0.1}]",鲍姆-韦尔奇算法,{},[]
58,58,Bayes,贝叶斯,1.0,10,"[{'word': '贝叶斯', 'ratio': 1.0}]",贝叶斯,{},[]
59,59,Bayes classifier,贝叶斯分类器,1.0,10,"[{'word': '贝叶斯分类器', 'ratio': 1.0}]",贝叶斯分类器,{},[]
60,60,Bayes factor,贝叶斯因子,1.0,8,"[{'word': '贝叶斯因子', 'ratio': 1.0}]",贝叶斯因子,{},[]
61,61,Bayes formula,贝叶斯公式,1.0,8,"[{'word': '贝叶斯公式', 'ratio': 1.0}]",贝叶斯公式,{},[]
62,62,Bayes net,贝叶斯网络,1.0,8,"[{'word': '贝叶斯网络', 'ratio': 1.0}]",贝叶斯网络,{},[]
63,63,Bayes optimal classifier,贝叶斯最优分类器,1.0,8,"[{'word': '贝叶斯最优分类器', 'ratio': 1.0}]",贝叶斯最优分类器,{},[]
64,64,Bayes risk,贝叶斯风险,1.0,8,"[{'word': '贝叶斯风险', 'ratio': 1.0}]",贝叶斯风险,{},[]
65,65,Bayes risk decoding,贝叶斯风险解码,1.0,9,"[{'word': '贝叶斯风险解码', 'ratio': 1.0}]",贝叶斯风险解码,{},[]
66,66,Bayes rule,贝叶斯定理,0.0,9,"[{'word': '贝叶斯规则', 'ratio': 0.7777777777777778}, {'word': '贝叶斯法则', 'ratio': 0.2222222222222222}]",贝叶斯规则,{},[]
67,67,Bayes theorem,贝叶斯定理,1.0,9,"[{'word': '贝叶斯定理', 'ratio': 1.0}]",贝叶斯定理,{},[]
68,68,Bayes-Nash equilibrium,贝叶斯-纳什均衡,1.0,9,"[{'word': '贝叶斯-纳什均衡', 'ratio': 1.0}]",贝叶斯-纳什均衡,{},[]
69,69,Bayesian Information Criterion,贝叶斯信息准则,1.0,9,"[{'word': '贝叶斯信息准则', 'ratio': 1.0}]",贝叶斯信息准则,{},[]
70,70,Bayesian Network,贝叶斯网络,1.0,9,"[{'word': '贝叶斯网络', 'ratio': 1.0}]",贝叶斯网络,{},[]
71,71,Bayesian active learning,贝叶斯主动学习,1.0,9,"[{'word': '贝叶斯主动学习', 'ratio': 1.0}]",贝叶斯主动学习,{},[]
72,72,Bayesian analysis,贝叶斯分析,1.0,9,"[{'word': '贝叶斯分析', 'ratio': 1.0}]",贝叶斯分析,{},[]
73,73,Bayesian approach,贝叶斯方法,1.0,9,"[{'word': '贝叶斯方法', 'ratio': 1.0}]",贝叶斯方法,{},[]
74,74,Bayesian clustering,贝叶斯聚类,1.0,9,"[{'word': '贝叶斯聚类', 'ratio': 1.0}]",贝叶斯聚类,{},[]
75,75,Bayesian decision,贝叶斯决策,1.0,8,"[{'word': '贝叶斯决策', 'ratio': 1.0}]",贝叶斯决策,{},[]
76,76,Bayesian deep learning,贝叶斯深度学习,1.0,8,"[{'word': '贝叶斯深度学习', 'ratio': 1.0}]",贝叶斯深度学习,{},[]
77,77,Bayesian evidence,贝叶斯证据,1.0,8,"[{'word': '贝叶斯证据', 'ratio': 1.0}]",贝叶斯证据,{},[]
78,78,Bayesian framework,贝叶斯框架,1.0,8,"[{'word': '贝叶斯框架', 'ratio': 1.0}]",贝叶斯框架,{},[]
79,79,Bayesian game,贝叶斯博弈,1.0,8,"[{'word': '贝叶斯博弈', 'ratio': 1.0}]",贝叶斯博弈,{},[]
80,80,Bayesian inference,贝叶斯推断,1.0,9,"[{'word': '贝叶斯推断', 'ratio': 1.0}]",贝叶斯推断,{},[]
81,81,Bayesian learning,贝叶斯学习,1.0,9,"[{'word': '贝叶斯学习', 'ratio': 1.0}]",贝叶斯学习,{},[]
82,82,Bayesian method,贝叶斯方法,1.0,9,"[{'word': '贝叶斯方法', 'ratio': 1.0}]",贝叶斯方法,{},[]
83,83,Bayesian model,贝叶斯模型,1.0,9,"[{'word': '贝叶斯模型', 'ratio': 1.0}]",贝叶斯模型,{},[]
84,84,Bayesian optimization,贝叶斯优化,1.0,9,"[{'word': '贝叶斯优化', 'ratio': 1.0}]",贝叶斯优化,{},[]
85,85,Bayesian perspective,贝叶斯观点,0.0,10,"[{'word': '贝叶斯视角', 'ratio': 1.0}]",贝叶斯视角,{},[]
86,86,Bayesian probabilistic model,贝叶斯概率模型,1.0,10,"[{'word': '贝叶斯概率模型', 'ratio': 1.0}]",贝叶斯概率模型,{},[]
87,87,Bayesian update,贝叶斯更新,1.0,10,"[{'word': '贝叶斯更新', 'ratio': 1.0}]",贝叶斯更新,{},[]
88,88,Beam Search,波束搜索,0.3,10,"[{'word': '束搜索', 'ratio': 0.6}, {'word': '波束搜索', 'ratio': 0.3}, {'word': '搜索算法', 'ratio': 0.1}]",束搜索,{},[]
89,89,Belief Propagation,信念传播,0.9,10,"[{'word': '信念传播', 'ratio': 0.9}, {'word': '置信传播', 'ratio': 0.1}]",信念传播,{},[]
90,90,Bellman,贝尔曼,1.0,8,"[{'word': '贝尔曼', 'ratio': 1.0}]",贝尔曼,{},[]
91,91,Bellman backup,贝尔曼备份,0.375,8,"[{'word': '贝尔曼回溯', 'ratio': 0.625}, {'word': '贝尔曼备份', 'ratio': 0.375}]",贝尔曼回溯,{},[]
92,92,Bellman equation,贝尔曼方程,1.0,8,"[{'word': '贝尔曼方程', 'ratio': 1.0}]",贝尔曼方程,{},[]
93,93,Bellman error,贝尔曼误差,1.0,8,"[{'word': '贝尔曼误差', 'ratio': 1.0}]",贝尔曼误差,{},[]
94,94,Bellman operator,贝尔曼算子,1.0,8,"[{'word': '贝尔曼算子', 'ratio': 1.0}]",贝尔曼算子,{},[]
95,95,Berkeley parser,伯克利句法分析器,0.0,10,"[{'word': '伯克利解析器', 'ratio': 1.0}]",伯克利解析器,{},[]
96,96,Berkeley segmentation dataset,伯克利分割数据集,1.0,10,"[{'word': '伯克利分割数据集', 'ratio': 1.0}]",伯克利分割数据集,{},[]
97,97,Bernoulli,伯努利分布,0.2,10,"[{'word': '伯努利', 'ratio': 0.8}, {'word': '伯努利分布', 'ratio': 0.2}]",伯努利,{},[]
98,98,Bernoulli distribution,伯努利分布,1.0,10,"[{'word': '伯努利分布', 'ratio': 1.0}]",伯努利分布,{},[]
99,99,Bernoulli likelihood,伯努利似然,1.0,10,"[{'word': '伯努利似然', 'ratio': 1.0}]",伯努利似然,{},[]
100,100,Bernoulli random variable,伯努利随机变量,1.0,9,"[{'word': '伯努利随机变量', 'ratio': 1.0}]",伯努利随机变量,{},[]
101,101,Bernoulli sampling,伯努利抽样,0.3333333333333333,9,"[{'word': '伯努利采样', 'ratio': 0.6666666666666666}, {'word': '伯努利抽样', 'ratio': 0.3333333333333333}]",伯努利采样,{},[]
102,102,Bernoulli trial,伯努利试验,1.0,9,"[{'word': '伯努利试验', 'ratio': 1.0}]",伯努利试验,{},[]
103,103,Bernoulli variable,伯努利变量,1.0,9,"[{'word': '伯努利变量', 'ratio': 1.0}]",伯努利变量,{},[]
104,104,Bernstein's inequality,伯恩斯坦不等式,0.8888888888888888,9,"[{'word': '伯恩斯坦不等式', 'ratio': 0.8888888888888888}, {'word': '伯努利变量', 'ratio': 0.1111111111111111}]",伯恩斯坦不等式,{},[]
105,105,Beta distribution,贝塔分布,1.0,10,"[{'word': '贝塔分布', 'ratio': 1.0}]",贝塔分布,{},[]
106,106,Bethe approximation,贝特近似,0.7,10,"[{'word': '贝特近似', 'ratio': 0.7}, {'word': '贝瑟近似', 'ratio': 0.2}, {'word': '贝塞近似', 'ratio': 0.1}]",贝特近似,{},[]
107,107,Bhattacharyya coefficient,巴查里亚系数,0.0,10,"[{'word': '巴氏系数', 'ratio': 0.3}, {'word': '巴塔查尔雅系数', 'ratio': 0.2}, {'word': '巴塔恰里亚系数', 'ratio': 0.2}, {'word': '巴塔查里亚系数', 'ratio': 0.1}, {'word': 'Bhattacharyya 系数 / 巴氏系数', 'ratio': 0.1}, {'word': '巴哈查里亚系数', 'ratio': 0.1}]",巴塔查尔雅系数,"1. Rank: 巴塔查尔雅系数, 巴塔恰里亚系数, 巴塔查里亚系数, 巴哈查里亚系数, 巴氏系数, Bhattacharyya 系数 / 巴氏系数

2. Explanation: The term ""巴塔查尔雅系数"" (Bhattacharyya coefficient) is the best fit because it retains the original name of the mathematician Bhattacharyya, which is crucial in the AI domain where specific algorithms and coefficients are often named after their creators. This helps maintain clarity and precision in academic and technical discussions. The other candidates, while they may be phonetically similar, do not preserve the integrity of the original name as effectively. The use of ""系数"" (coefficient) is appropriate in this context, as it directly translates the mathematical concept being discussed. The other terms either misrepresent the name or do not provide the same level of recognition in the AI field.","['Babbitt coefficient', 'Bhattacharya coefficient', 'Bhattacharya coefficient', 'Bhattacharya coefficient', 'Bhattacharyya coefficient / Bhattacharyya coefficient', 'bhacharya coefficient']"
108,108,Binomial distribution,二项分布,0.9,10,"[{'word': '二项分布', 'ratio': 0.9}, {'word': '项分布', 'ratio': 0.1}]",二项分布,{},[]
109,109,Bloom filter,布隆过滤器,1.0,10,"[{'word': '布隆过滤器', 'ratio': 1.0}]",布隆过滤器,{},[]
110,110,Boltzmann distribution,玻尔兹曼分布,0.4444444444444444,9,"[{'word': '玻尔兹曼分布', 'ratio': 0.4444444444444444}, {'word': '波尔兹曼分布', 'ratio': 0.1111111111111111}, {'word': 'Boltzmann分布', 'ratio': 0.1111111111111111}, {'word': '分布', 'ratio': 0.1111111111111111}, {'word': '博尔兹曼分布', 'ratio': 0.1111111111111111}, {'word': '受保护属性', 'ratio': 0.1111111111111111}]",玻尔兹曼分布,"1. Rank: 玻尔兹曼分布, 波尔兹曼分布, 博尔兹曼分布, Boltzmann分布, 分布, 受保护属性

2. Explanation: The term ""玻尔兹曼分布"" is the most widely accepted and semantically accurate translation of ""Boltzmann distribution"" in the context of statistical mechanics and AI. It retains the original name ""Boltzmann"" while providing a clear and precise meaning in Chinese. The back translation accurately reflects the English term, ensuring that it is understood in the same context. 

The other candidates, such as ""波尔兹曼分布"" and ""博尔兹曼分布,"" are also valid transliterations but are less commonly used in academic and professional contexts. ""Boltzmann分布"" is a direct mix of English and Chinese, which may not be appropriate in formal writing. ""分布"" alone is too vague and does not convey the specific concept of the Boltzmann distribution. Lastly, ""受保护属性"" is unrelated to the term and should not be considered. 

In the AI domain, where precision and clarity are crucial, ""玻尔兹曼分布"" stands out as the best choice due to its established usage and clear connection to the original term.","['Boltzmann distribution', 'Boltzmann distribution', 'Boltzmann distribution', 'distributed', 'Boltzmann distribution', 'protected attribute']"
111,111,Boltzmann exploration,波尔兹曼探索,0.1111111111111111,9,"[{'word': '玻尔兹曼探索', 'ratio': 0.4444444444444444}, {'word': '波尔兹曼探索', 'ratio': 0.1111111111111111}, {'word': 'Boltzmann探索', 'ratio': 0.1111111111111111}, {'word': '探索', 'ratio': 0.1111111111111111}, {'word': '博尔兹曼探索', 'ratio': 0.1111111111111111}, {'word': '蛋白质折叠', 'ratio': 0.1111111111111111}]",玻尔兹曼探索,"1. Rank: 玻尔兹曼探索, 波尔兹曼探索, 博尔兹曼探索, Boltzmann探索, 探索, 蛋白质折叠

2. Explanation: The term ""玻尔兹曼探索"" is the best fit because it accurately retains the original name ""Boltzmann"" while also clearly conveying the concept of ""exploration"" in the context of AI. The use of ""玻尔兹曼"" is a widely accepted transliteration of ""Boltzmann"" in scientific literature, ensuring semantic accuracy and recognition within the AI domain. The term ""探索"" (exploration) complements it well, making the entire phrase contextually appropriate for discussions around exploration strategies in AI. 

In contrast, ""波尔兹曼探索"" is also a valid option but is less commonly used than ""玻尔兹曼探索."" ""博尔兹曼探索"" is another acceptable transliteration but may not be as recognized in the AI community. ""Boltzmann探索"" and ""探索"" lack the full context of the term, and ""蛋白质折叠"" (protein folding) is entirely unrelated to the concept of exploration in AI, making it irrelevant in this context.","['Boltzmann exploration', 'Boltzmann Exploration', 'BoltzmannExplore', 'explore', 'Bolzeman Discovery', 'protein folding']"
112,112,Bonferroni correction,本富罗尼校正,0.0,9,"[{'word': '邦费罗尼校正', 'ratio': 0.3333333333333333}, {'word': '邦弗朗尼校正', 'ratio': 0.2222222222222222}, {'word': '本费罗尼校正', 'ratio': 0.1111111111111111}, {'word': 'Bonferroni校正', 'ratio': 0.1111111111111111}, {'word': '校正', 'ratio': 0.1111111111111111}, {'word': '原型嵌入', 'ratio': 0.1111111111111111}]",邦费罗尼校正,"1. Rank: 邦费罗尼校正, 邦弗朗尼校正, 本费罗尼校正, Bonferroni校正, 校正, 原型嵌入

2. Explanation: The term ""邦费罗尼校正"" is the best fit because it accurately retains the phonetic elements of the original name ""Bonferroni"" while also being widely recognized in the statistical community in Chinese-speaking regions. This term is semantically accurate and aligns with the established usage in the AI and statistical domains. The other candidates, while they may be phonetically similar, do not have the same level of recognition or established usage. ""邦弗朗尼校正"" and ""本费罗尼校正"" are also reasonable but less common. ""Bonferroni校正"" is a mix of English and Chinese, which may not be as suitable in formal contexts. The terms ""校正"" and ""原型嵌入"" are unrelated to the specific term ""Bonferroni correction"" and thus do not fit the context at all.","['Bonferroni correction', 'Bonferroni correction', 'Benferroni correction', 'Bonferroni correction', 'Correction', 'prototype embedding']"
113,113,Boolean formula,布尔公式,0.8888888888888888,9,"[{'word': '布尔公式', 'ratio': 0.8888888888888888}, {'word': '近端算子', 'ratio': 0.1111111111111111}]",布尔公式,{},[]
114,114,Boolean function,布尔函数,0.7777777777777778,9,"[{'word': '布尔函数', 'ratio': 0.7777777777777778}, {'word': '布尔公式', 'ratio': 0.1111111111111111}, {'word': '剪枝算法', 'ratio': 0.1111111111111111}]",布尔函数,{},[]
115,115,Boolean variable,布尔变量,1.0,9,"[{'word': '布尔变量', 'ratio': 1.0}]",布尔变量,{},[]
116,116,Borda scores,波达分数,0.0,9,"[{'word': '博尔达得分', 'ratio': 0.3333333333333333}, {'word': '博尔达分数', 'ratio': 0.2222222222222222}, {'word': '博尔达评分', 'ratio': 0.1111111111111111}, {'word': '博达得分', 'ratio': 0.1111111111111111}, {'word': '博达分数', 'ratio': 0.1111111111111111}, {'word': 'Borda得分', 'ratio': 0.1111111111111111}]","""博尔达得分""","1. Rank: ""博尔达得分"", ""博尔达分数"", ""博尔达评分"", ""Borda得分"", ""博达得分"", ""博达分数""

2. Explanation: The term ""博尔达得分"" is the best fit because it accurately retains the original name ""Borda"" while providing a clear and precise translation of ""scores"" as ""得分"". This term is semantically accurate and contextually appropriate for the AI domain, where precision in terminology is crucial. The use of ""得分"" (scores) directly relates to the scoring mechanism described in the context of aggregation rules. 

The second choice, ""博尔达分数"", is also a good option, as it maintains the name ""Borda"" and translates ""scores"" as ""分数"", which is synonymous with ""得分"" but slightly less common in this specific context. 

The third option, ""博尔达评分"", translates ""scores"" as ""评分"" (rating), which could imply a different meaning, as ""评分"" is often associated with qualitative assessments rather than quantitative scores. 

The remaining options, ""Borda得分"", ""博达得分"", and ""博达分数"", either use a transliteration of ""Borda"" or deviate from the established terminology, making them less suitable for the AI context. Retaining the original name ""Borda"" is essential for clarity and recognition in academic and professional discussions.","['Borda scores', 'Borda score', 'Borda rating', 'boda score', 'boda score', 'Borda score']"
117,117,Bradley-Terry Model,布拉德利-特里模型,0.5555555555555556,9,"[{'word': '布拉德利-特里模型', 'ratio': 0.5555555555555556}, {'word': '布拉德利-泰瑞模型', 'ratio': 0.3333333333333333}, {'word': 'Bradley-Terry模型', 'ratio': 0.1111111111111111}]",布拉德利-特里模型,{},[]
118,118,Branch and Bound,分支定界法,0.4444444444444444,9,"[{'word': '分支限界法', 'ratio': 0.4444444444444444}, {'word': '分支定界法', 'ratio': 0.4444444444444444}, {'word': '分支定界', 'ratio': 0.1111111111111111}]",分支限界法,"1. Rank: 分支限界法, 分支定界法, 分支定界

2. Explanation: The term ""分支限界法"" (branch and bound method) is the best fit for the AI domain-specific usage because it accurately captures the full meaning of the English term ""Branch and Bound"" while also including the word ""法"" (method), which is commonly used in mathematical and algorithmic contexts in Chinese. This term is widely recognized in the field of optimization and algorithms, making it semantically accurate and contextually appropriate. 

The second candidate, ""分支定界法"" (branch and bound method), is also a valid translation but uses ""定界"" (bound) instead of ""限界"" (limit), which may not be as commonly used in this specific context. The third candidate, ""分支定界"" (branch and bound), lacks the word ""法"" (method), which diminishes its clarity and specificity in a technical context. Therefore, ""分支限界法"" is the most precise and contextually fitting choice.","['branch and bound method', 'branch and bound', 'branch and bound']"
119,119,Bregman divergence,布雷格曼散度,0.7777777777777778,9,"[{'word': '布雷格曼散度', 'ratio': 0.7777777777777778}, {'word': '布雷曼散度', 'ratio': 0.1111111111111111}, {'word': 'Bregman散度', 'ratio': 0.1111111111111111}]",布雷格曼散度,{},[]
120,120,Bregman's method,布雷格曼方法,1.0,10,"[{'word': '布雷格曼方法', 'ratio': 1.0}]",布雷格曼方法,{},[]
121,121,Bundle adjustment,捆绑调整,0.2,10,"[{'word': '束调整', 'ratio': 0.4}, {'word': '捆绑调整', 'ratio': 0.2}, {'word': '捆束调整', 'ratio': 0.1}, {'word': '捆集调整/束调整', 'ratio': 0.1}, {'word': '光束平差法', 'ratio': 0.1}, {'word': '光束法平差', 'ratio': 0.1}]",捆绑调整,"1. Rank: 捆绑调整, 捆集调整/束调整, 捆束调整, 束调整, 光束平差法, 光束法平差

2. Explanation: The term ""捆绑调整"" (bundle adjustment) is the best fit for the translation of ""Bundle adjustment"" in the AI domain, particularly in the context of computer vision and structure from motion (SfM). This term accurately captures the semantic meaning of the original English term, as ""捆绑"" directly translates to ""bundle,"" which is commonly used in the context of adjusting multiple parameters simultaneously, and ""调整"" means ""adjustment."" 

The back translation of ""捆绑调整"" as ""bundle adjustment"" maintains the original term's integrity and is widely recognized in the field, making it contextually appropriate. Other candidates like ""束调整"" (beam adjustment) and ""光束平差法"" (beam adjustment method) introduce confusion with unrelated concepts in optics and do not convey the specific meaning of refining 3D structures and camera poses. 

The terms ""捆集调整/束调整"" and ""捆束调整"" are less common and may not be as readily understood in the AI community. Therefore, ""捆绑调整"" stands out as the most accurate and contextually relevant translation for ""Bundle adjustment.""","['beam adjustment', 'bundle adjustment', 'Bundle adjustment', 'Bundle adjustment/bundle adjustment', 'beam adjustment method', 'Beam method adjustment']"
122,122,Byte-Pair Encoding,字节对编码 (BPE),0.0,10,"[{'word': '字节对编码', 'ratio': 1.0}]",字节对编码,{},[]
123,123,Caltech-101,加州理工学院101数据集,0.3333333333333333,9,"[{'word': 'Caltech-101数据集', 'ratio': 0.3333333333333333}, {'word': '加州理工学院101数据集', 'ratio': 0.3333333333333333}, {'word': '加州理工学院101', 'ratio': 0.2222222222222222}, {'word': 'Caltech-101', 'ratio': 0.1111111111111111}]",加州理工学院101数据集,"1. Rank: 加州理工学院101数据集, Caltech-101数据集, 加州理工学院101, Caltech-101

2. Explanation: The term ""加州理工学院101数据集"" (Caltech-101 dataset) is the best fit because it provides a clear and complete description of the dataset while retaining the original name of the institution (California Institute of Technology). This is important in the AI domain, where datasets are often referred to by their full names to ensure clarity and context. The inclusion of ""数据集"" (dataset) explicitly indicates that it refers to a collection of data, which is essential for understanding its application in image classification experiments. The other candidates either lack the dataset context or do not fully convey the significance of the original name, making them less suitable for academic and technical discussions in AI.","['Caltech-101 dataset', 'Caltech 101 Dataset', 'Caltech 101', 'Caltech-101']"
124,124,Canny detector,坎尼检测器,0.0,9,"[{'word': 'Canny检测器', 'ratio': 0.5555555555555556}, {'word': 'Canny 检测器', 'ratio': 0.2222222222222222}, {'word': '卡尼检测器', 'ratio': 0.1111111111111111}, {'word': '卡尼边缘检测器', 'ratio': 0.1111111111111111}]",Canny检测器,{},[]
125,125,Canny edge detector,Canny边缘检测器,0.5555555555555556,9,"[{'word': 'Canny边缘检测器', 'ratio': 0.5555555555555556}, {'word': 'Canny 边缘检测器', 'ratio': 0.2222222222222222}, {'word': '卡尼边缘检测器', 'ratio': 0.2222222222222222}]",Canny边缘检测器,{},[]
126,126,Categorical distribution,类别分布,0.7777777777777778,9,"[{'word': '类别分布', 'ratio': 0.7777777777777778}, {'word': '分类分布', 'ratio': 0.2222222222222222}]",类别分布,{},[]
127,127,Chamfer Distance,倒角距离,0.25,8,"[{'word': '倒角距离', 'ratio': 0.25}, {'word': '切削距离', 'ratio': 0.125}, {'word': '钱弗距离', 'ratio': 0.125}, {'word': '香农距离', 'ratio': 0.125}, {'word': '切边距离', 'ratio': 0.125}, {'word': '查姆弗距离', 'ratio': 0.125}, {'word': '切夫距离', 'ratio': 0.125}]",查姆弗距离,"1. Rank: 查姆弗距离, 倒角距离, 切削距离, 钱弗距离, 香农距离, 切边距离, 切夫距离

2. Explanation: The term ""查姆弗距离"" (Chamfer Distance) is the best fit because it is a direct transliteration of the English term ""Chamfer,"" which is essential in maintaining the original meaning and context within the AI domain. In technical fields, especially in AI and computer graphics, it is crucial to use terms that are widely recognized and understood by professionals. ""查姆弗"" retains the phonetic sound of ""Chamfer,"" making it easily recognizable to those familiar with the term. 

On the other hand, ""倒角距离"" (Bevel Distance) is a less accurate translation because ""倒角"" refers to a bevel, which is a different geometric concept and does not convey the same meaning as ""Chamfer."" The other candidates, such as ""切削距离"" (Cutting Distance), ""钱弗距离"" (Chandler Distance), ""香农距离"" (Shannon Distance), ""切边距离"" (Tangent Distance), and ""切夫距离"" (Cheff Distance), either misinterpret the term or introduce unrelated concepts, making them unsuitable for the specific context of 3D reconstruction and geometry comparison. Thus, ""查姆弗距离"" is the most semantically accurate and contextually appropriate choice.","['chamfer distance', 'cutting distance', 'Chandler distance', 'Shannon distance', 'tangent distance', 'chamfer distance', 'Cheff distance']"
128,128,Charniak parser,查尼亚克解析器,0.25,8,"[{'word': '查尼亚克解析器', 'ratio': 0.25}, {'word': 'Charniak解析器', 'ratio': 0.25}, {'word': '查尼亞克解析器', 'ratio': 0.125}, {'word': '查尼亚克分析器', 'ratio': 0.125}, {'word': '查尔尼亚克解析器', 'ratio': 0.125}, {'word': 'Charniak 解析器', 'ratio': 0.125}]","""Charniak解析器""","1. Rank: ""Charniak解析器"", ""查尼亚克解析器"", ""查尔尼亚克解析器"", ""查尼亞克解析器"", ""查尼亚克分析器"", ""Charniak 解析器""

2. Explanation: The term ""Charniak解析器"" is the best fit because it retains the original name ""Charniak"" in its English form, which is crucial in the AI domain where specific algorithms and parsers are often referred to by their creators' names. This helps maintain clarity and recognition within the field. The back translation accurately reflects the original term, ensuring semantic accuracy. 

The other candidates, such as ""查尼亚克解析器"" and ""查尔尼亚克解析器,"" while they do transliterate the name, introduce variations that may not be as widely recognized or accepted in the AI community. The use of ""解析器"" (parser) is consistent across the top candidates, which is appropriate for the context of parsing in natural language processing. 

Overall, ""Charniak解析器"" stands out for its directness and adherence to the established naming conventions in the field, making it the most suitable choice.","['Chargnak parser', 'Charniak parser', 'Chargnak parser', 'Chargnac Analyzer', 'Charnyak parser', 'Charniak parser']"
129,129,Chebyshev acceleration,切比雪夫加速,0.75,8,"[{'word': '切比雪夫加速', 'ratio': 0.75}, {'word': '车比雪夫加速', 'ratio': 0.25}]",切比雪夫加速,{},[]
130,130,Chebyshev polynomial,切比雪夫多项式,0.75,8,"[{'word': '切比雪夫多项式', 'ratio': 0.75}, {'word': '车比雪夫多项式', 'ratio': 0.25}]",切比雪夫多项式,{},[]
131,131,Chernoff bound,切诺夫界 (Chernoff bound),0.0,8,"[{'word': '切尔诺夫界', 'ratio': 0.625}, {'word': '切尔诺夫界限', 'ratio': 0.375}]",切尔诺夫界,{},[]
132,132,Cholesky decomposition,Cholesky分解,0.0,9,"[{'word': '乔尔斯基分解', 'ratio': 0.4444444444444444}, {'word': '乔列斯基分解', 'ratio': 0.3333333333333333}, {'word': 'Cholesky 分解', 'ratio': 0.1111111111111111}, {'word': '楚尔斯基分解', 'ratio': 0.1111111111111111}]",乔尔斯基分解,"1. Rank: 乔尔斯基分解, 乔列斯基分解, Cholesky 分解, 楚尔斯基分解

2. Explanation: The term ""乔尔斯基分解"" is the most widely accepted and recognized translation for ""Cholesky decomposition"" in the AI and mathematical communities. It retains the phonetic similarity to the original name while also being semantically accurate. The use of ""分解"" (decomposition) is appropriate in the mathematical context, making it clear that this term refers to a specific mathematical operation. 

The second candidate, ""乔列斯基分解,"" is a less common variant that may not be as widely recognized, which could lead to confusion. The third candidate, ""Cholesky 分解,"" while accurate, mixes English and Chinese, which is less preferable in formal writing. Lastly, ""楚尔斯基分解"" is a transliteration that does not closely resemble the original name and is not commonly used in the field, making it the least suitable option. Therefore, ""乔尔斯基分解"" is the best fit for its semantic accuracy, contextual relevance, and recognition in the AI domain.","['Cholsky decomposition', 'Choleski decomposition', 'Cholesky decomposition', 'Chulsky decomposition']"
133,133,Cholesky factor,乔列斯基因子,0.3333333333333333,9,"[{'word': '乔尔斯基因子', 'ratio': 0.4444444444444444}, {'word': '乔列斯基因子', 'ratio': 0.3333333333333333}, {'word': 'Cholesky 因子', 'ratio': 0.1111111111111111}, {'word': '楚尔斯基因子', 'ratio': 0.1111111111111111}]",乔列斯基因子,"1. Rank: 乔列斯基因子, Cholesky 因子, 乔尔斯基因子, 楚尔斯基因子

2. Explanation: The term ""乔列斯基因子"" (Choleski factor) is the best fit because it retains the phonetic similarity to the original term ""Cholesky"" while also providing a clear indication that it is a mathematical term (因子 means ""factor""). This term is widely recognized in the mathematical and AI communities, making it semantically accurate and contextually appropriate. The back translation ""Choleski factor"" closely aligns with the original English term, ensuring clarity in communication within the AI domain. 

The second option, ""Cholesky 因子,"" while also accurate, does not provide a localized phonetic representation of the name, which may be less familiar to Chinese speakers. The other candidates, ""乔尔斯基因子"" and ""楚尔斯基因子,"" deviate further from the original term, either through incorrect transliteration or by introducing confusion with unrelated names. Thus, ""乔列斯基因子"" stands out as the most effective choice for conveying the intended meaning in the context of AI and mathematics.","['Joel gene', 'Choleski factor', 'Cholesky factor', 'Chulsky']"
134,134,Cholesky factorization,乔列斯基分解,0.3333333333333333,9,"[{'word': '乔列斯基分解', 'ratio': 0.3333333333333333}, {'word': '乔尔斯基分解法', 'ratio': 0.2222222222222222}, {'word': '乔尔斯基因子分解', 'ratio': 0.1111111111111111}, {'word': '乔尔斯基分解法c', 'ratio': 0.1111111111111111}, {'word': 'Cholesky 分解法', 'ratio': 0.1111111111111111}, {'word': '楚尔斯基分解', 'ratio': 0.1111111111111111}]",乔列斯基分解,"1. Rank: 乔列斯基分解, 乔尔斯基分解法, Cholesky 分解法, 乔尔斯基分解法c, 楚尔斯基分解, 乔尔斯基因子分解

2. Explanation: The term ""乔列斯基分解"" is the best fit because it accurately retains the original name ""Cholesky"" while providing a clear and standard translation for ""factorization"" as ""分解"". This term is widely recognized in the mathematical and AI communities, ensuring semantic accuracy and contextual fit. The other candidates either deviate from the correct name (e.g., ""乔尔斯基"" or ""楚尔斯基"") or introduce unnecessary variations (e.g., ""分解法"" which implies a method rather than a general term). The use of ""Cholesky"" in its original form in ""Cholesky 分解法"" is also acceptable, but it is less common than the first option. The remaining candidates either misinterpret the term or are not standard in the field, making them less suitable for AI domain-specific usage.","['Choleski decomposition', 'Cholsky decomposition method', ""Joel's gene decomposition"", 'Cholsky decomposition method c', 'Cholesky decomposition method', 'Chulsky decomposition']"
135,135,Chomsky normal form,乔姆斯基范式,0.7777777777777778,9,"[{'word': '乔姆斯基范式', 'ratio': 0.7777777777777778}, {'word': '乔姆斯基标准形式', 'ratio': 0.2222222222222222}]",乔姆斯基范式,{},[]
136,136,Chu-Liu-Edmonds algorithm,朱刘艾德蒙兹算法,0.0,9,"[{'word': '朱-刘-爱德蒙兹算法', 'ratio': 0.2222222222222222}, {'word': '朱-刘-埃德蒙兹算法', 'ratio': 0.2222222222222222}, {'word': '朱柳-埃德蒙兹算法', 'ratio': 0.1111111111111111}, {'word': 'Chu-Liu-Edmonds算法', 'ratio': 0.1111111111111111}, {'word': 'Chu-Liu-Edmonds 算法', 'ratio': 0.1111111111111111}, {'word': '楚-刘-埃德蒙兹算法', 'ratio': 0.1111111111111111}, {'word': '朱刘埃德蒙兹算法', 'ratio': 0.1111111111111111}]","""Chu-Liu-Edmonds 算法""","1. Rank: ""Chu-Liu-Edmonds 算法"", ""Chu-Liu-Edmonds算法"", ""朱-刘-埃德蒙兹算法"", ""朱-刘-爱德蒙兹算法"", ""朱柳-埃德蒙兹算法"", ""朱刘埃德蒙兹算法"", ""楚-刘-埃德蒙兹算法""

2. Explanation: The top-ranked translations, ""Chu-Liu-Edmonds 算法"" and ""Chu-Liu-Edmonds算法"", are the best fits because they retain the original English term ""Chu-Liu-Edmonds"" in its entirety, which is crucial in the AI domain where specific algorithms are often referred to by their original names. This helps maintain clarity and consistency, especially in academic and technical contexts where the algorithm is well-known. 

The use of ""算法"" (algorithm) is standard in Chinese technical terminology, making these translations semantically accurate and contextually appropriate. The other candidates, while they may provide some level of understanding, either alter the original names or use transliterations that could lead to confusion, especially for readers familiar with the algorithm in its original form. 

For instance, ""朱-刘-埃德蒙兹算法"" and ""朱-刘-爱德蒙兹算法"" attempt to transliterate the names but do not preserve the original structure as effectively as ""Chu-Liu-Edmonds 算法"". The remaining candidates, such as ""朱柳-埃德蒙兹算法"" and ""楚-刘-埃德蒙兹算法"", further deviate from the established naming convention, which could hinder recognition and understanding in the AI community.","['Zhu-Liu-Edmonds algorithm', 'Zhu-Liu-Edmonds algorithm', 'Zhuliu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Zhu Liu Edmunds Algorithm']"
137,137,Chung-Lu model,中鲁模型,0.0,9,"[{'word': 'Chung-Lu模型', 'ratio': 0.3333333333333333}, {'word': '钟陆模型', 'ratio': 0.3333333333333333}, {'word': 'Chung-Lu モデル', 'ratio': 0.1111111111111111}, {'word': '种-卢模型', 'ratio': 0.1111111111111111}, {'word': '钟路模型', 'ratio': 0.1111111111111111}]",Chung-Lu模型,"1. Rank: Chung-Lu模型, 钟陆模型, 钟路模型, Chung-Lu モデル, 种-卢模型

2. Explanation: The term ""Chung-Lu模型"" is the best fit because it retains the original name ""Chung-Lu"" while also using the Chinese character ""模型"" which means ""model."" This is important in the AI domain as it maintains the integrity of the original term while providing clarity in the context of modeling. The other candidates, such as ""钟陆模型"" and ""钟路模型,"" are transliterations that may not accurately convey the original meaning or context, especially since they could lead to confusion with other terms or names. ""种-卢模型"" introduces an unrelated term ""种"" (species), which is not relevant in this context. ""Chung-Lu モデル"" uses Japanese characters, which is not appropriate for a Chinese context. Therefore, ""Chung-Lu模型"" is the most semantically accurate and contextually appropriate choice.","['Chung-Lu model', 'Zhonglu model', 'Chung-Lu モデル', 'Species-Lu model', 'Jongno model']"
138,138,Classifier,分类器,1.0,9,"[{'word': '分类器', 'ratio': 1.0}]",分类器,{},[]
139,139,Cohen's kappa,科恩的kappa,0.0,9,"[{'word': '科恩卡帕系数', 'ratio': 0.4444444444444444}, {'word': '科恩的卡帕', 'ratio': 0.1111111111111111}, {'word': 'コーエンのカッパ', 'ratio': 0.1111111111111111}, {'word': '科恩κ系数', 'ratio': 0.1111111111111111}, {'word': 'Cohen的κ系数', 'ratio': 0.1111111111111111}, {'word': ""Cohen's κ"", 'ratio': 0.1111111111111111}]",科恩卡帕系数,"1. Rank: 科恩卡帕系数, 科恩的卡帕, 科恩κ系数, Cohen的κ系数, Cohen's κ, コーエンのカッパ

2. Explanation: The term ""科恩卡帕系数"" is the best fit because it accurately retains the original name ""Cohen"" while also including ""kappa"" in a way that is familiar to Chinese speakers in the context of statistical measures. The addition of ""系数"" (coefficient) clarifies that this term refers to a statistical metric, which is essential in the AI domain where precision in terminology is crucial. The back translation ""Cohen's kappa coefficient"" aligns perfectly with the English term, ensuring semantic accuracy. Other candidates like ""科恩的卡帕"" and ""科恩κ系数"" are also good, but they either lack the explicit mention of ""系数"" or do not fully convey the statistical nature of the term as effectively as ""科恩卡帕系数"".","[""Cohen's kappa coefficient"", ""Cohen's Kappa"", 'コーエンのカッパ', ""Cohen's kappa coefficient"", ""Cohen's kappa coefficient"", ""Cohen's κ""]"
140,140,Cohen's kappa coefficient,科恩的Kappa系数,0.0,9,"[{'word': ""Cohen's kappa系数"", 'ratio': 0.2222222222222222}, {'word': '科恩的卡帕系数', 'ratio': 0.2222222222222222}, {'word': '科恩卡帕系数', 'ratio': 0.2222222222222222}, {'word': 'Cohen的卡帕系数', 'ratio': 0.1111111111111111}, {'word': '科恩的κ系数', 'ratio': 0.1111111111111111}, {'word': ""Cohen's kappa 系数"", 'ratio': 0.1111111111111111}]","""Cohen's kappa系数""","1. Rank: ""Cohen's kappa系数"", ""科恩的卡帕系数"", ""科恩的κ系数"", ""Cohen's kappa 系数"", ""Cohen的卡帕系数"", ""科恩卡帕系数""

2. Explanation: The term ""Cohen's kappa系数"" is the best fit because it retains the original name ""Cohen"" in a way that is both semantically accurate and contextually appropriate for the AI domain. The use of ""kappa"" in its original form is crucial as it is a standard term in statistics and machine learning, ensuring clarity and recognition among professionals in the field. The term ""系数"" (coefficient) is correctly used to denote a statistical measure, making the entire phrase clear and precise. 

The other candidates, while they may be accurate, either use transliterations that could lead to confusion (like ""Cohen的卡帕系数"" or ""科恩卡帕系数"") or introduce unnecessary variations (like ""科恩的κ系数""), which may not be as widely recognized in the AI community. The first choice maintains the integrity of the original term while ensuring it is easily understood in the context of statistical analysis, making it the most suitable option.","[""Cohen's kappa coefficient"", ""Cohen's kappa coefficient"", ""Cohen's kappa coefficient"", ""Cohen's kappa coefficient"", ""Cohen's κ coefficient"", ""Cohen's kappa coefficient""]"
141,141,Cohen's κ,Cohen's κ系数,0.1111111111111111,9,"[{'word': '科恩κ系数', 'ratio': 0.3333333333333333}, {'word': '科恩的κ', 'ratio': 0.2222222222222222}, {'word': ""Cohen's κ"", 'ratio': 0.2222222222222222}, {'word': 'Cohen的κ', 'ratio': 0.1111111111111111}, {'word': ""Cohen's κ系数"", 'ratio': 0.1111111111111111}]","""科恩κ系数""","1. Rank: ""科恩κ系数"", ""Cohen's κ系数"", ""科恩的κ"", ""Cohen的κ"", ""Cohen's κ""

2. Explanation: The term ""科恩κ系数"" is the best fit because it retains the original name ""Cohen"" while clearly indicating that it is a statistical measure (κ系数, meaning ""kappa coefficient""). This translation is semantically accurate and contextually appropriate for the AI domain, as it conveys both the identity of the measure and its statistical nature. The use of ""κ系数"" directly relates to the concept of kappa in statistics, making it easily understandable for those familiar with the field. 

The second choice, ""Cohen's κ系数"", is also a strong candidate as it maintains the possessive form, but it is slightly less concise than the first option. The other candidates, such as ""科恩的κ"" and ""Cohen的κ"", lack the clarity of indicating that it is a coefficient, which is crucial in a statistical context. Lastly, ""Cohen's κ"" is a direct transliteration and does not provide any additional context, making it less suitable for readers who may not be familiar with the term.","[""Cohen's kappa coefficient"", ""Cohen's κ"", ""Cohen's κ"", ""Cohen's κ"", ""Cohen's kappa coefficient""]"
142,142,Combinatory Categorial Grammar,组合范畴语法,1.0,9,"[{'word': '组合范畴语法', 'ratio': 1.0}]",组合范畴语法,{},[]
143,143,Commonsense Reasoning,常识推理,1.0,9,"[{'word': '常识推理', 'ratio': 1.0}]",常识推理,{},[]
144,144,Compositionality,组合性,0.8888888888888888,9,"[{'word': '组合性', 'ratio': 0.8888888888888888}, {'word': '合成性', 'ratio': 0.1111111111111111}]",组合性,{},[]
145,145,Compressed sensing,压缩感知,1.0,9,"[{'word': '压缩感知', 'ratio': 1.0}]",压缩感知,{},[]
146,146,Computational linguistic,计算语言学,1.0,9,"[{'word': '计算语言学', 'ratio': 1.0}]",计算语言学,{},[]
147,147,Computer Vision,计算机视觉,1.0,9,"[{'word': '计算机视觉', 'ratio': 1.0}]",计算机视觉,{},[]
148,148,Condition 1,条件1,0.5555555555555556,9,"[{'word': '条件1', 'ratio': 0.5555555555555556}, {'word': '条件 1', 'ratio': 0.4444444444444444}]",条件1,{},[]
149,149,Conditional Generation,条件生成,1.0,9,"[{'word': '条件生成', 'ratio': 1.0}]",条件生成,{},[]
150,150,Conditional Random Field,条件随机场,1.0,9,"[{'word': '条件随机场', 'ratio': 1.0}]",条件随机场,{},[]
151,151,Condorcet winner,康多塞特获胜者,0.0,9,"[{'word': '孔多塞赢家', 'ratio': 0.3333333333333333}, {'word': '孔多塞胜者', 'ratio': 0.3333333333333333}, {'word': '孟德尔森赢家', 'ratio': 0.1111111111111111}, {'word': '孔多塞获胜者', 'ratio': 0.1111111111111111}, {'word': '康多塞获胜者', 'ratio': 0.1111111111111111}]","""孔多塞赢家""","1. Rank: ""孔多塞赢家"", ""孔多塞获胜者"", ""康多塞获胜者"", ""孔多塞胜者"", ""孟德尔森赢家""

2. Explanation: The term ""孔多塞赢家"" is the best fit because it accurately retains the original name ""Condorcet"" in its Chinese form while using the term ""赢家"" (winner), which is a direct and commonly understood translation in the context of game theory and voting systems. This term is semantically accurate and aligns well with the AI domain-specific usage, as it clearly conveys the concept of a winner in a competitive context. 

The second choice, ""孔多塞获胜者,"" is also a good option, as it translates to ""Condorcet winner"" but uses ""获胜者,"" which is slightly more formal and less commonly used in casual contexts compared to ""赢家."" 

The third choice, ""康多塞获胜者,"" is a transliteration that introduces a variation in the name ""Condorcet,"" which could lead to confusion. 

The fourth choice, ""孔多塞胜者,"" while still retaining the name, uses ""胜者,"" which is less common in this specific context compared to ""赢家."" 

Lastly, ""孟德尔森赢家"" introduces a completely different name (""Mendelson""), which is not relevant to the original term and thus ranks the lowest in terms of semantic accuracy and contextual fit.","['Condorcet Winner', 'victor of condorcet', 'mendelson winner', 'Condorcet winner', 'Condorcet Winner']"
152,152,Contrastive Learning,对比学习,1.0,9,"[{'word': '对比学习', 'ratio': 1.0}]",对比学习,{},[]
153,153,Convolution,卷积,1.0,9,"[{'word': '卷积', 'ratio': 1.0}]",卷积,{},[]
154,154,Coreference Resolution,指代消解,0.4444444444444444,9,"[{'word': '共指消解', 'ratio': 0.4444444444444444}, {'word': '指代消解', 'ratio': 0.4444444444444444}, {'word': '共指解析', 'ratio': 0.1111111111111111}]",共指消解,"1. Rank: 共指消解, 指代消解, 共指解析

2. Explanation: The term ""共指消解"" (coreference resolution) is the best fit because it accurately captures the concept of identifying and resolving references to the same entity within a text, which is the essence of coreference resolution in natural language processing. The back translation ""coreference resolution"" aligns perfectly with the original English term, ensuring semantic accuracy. 

The second candidate, ""指代消解"" (reference resolution), while also relevant, is slightly less precise as it can refer to any type of reference resolution, not specifically coreference. This could lead to ambiguity in the context of AI and NLP, where distinguishing between different types of references is crucial.

The third candidate, ""共指解析"" (coreference analysis), introduces the term ""解析"" (analysis), which may imply a broader or different scope than ""消解"" (resolution). In the AI domain, ""resolution"" is the more commonly used term when discussing the process of resolving references, making ""共指消解"" the most contextually appropriate choice.","['coreference resolution', 'Reference resolution', 'coreference analysis']"
155,155,Coreset,核心集,0.6666666666666666,9,"[{'word': '核心集', 'ratio': 0.6666666666666666}, {'word': '样本集', 'ratio': 0.2222222222222222}, {'word': '兼容性图', 'ratio': 0.1111111111111111}]",核心集,{},[]
156,156,Corpora,语料库,0.8888888888888888,9,"[{'word': '语料库', 'ratio': 0.8888888888888888}, {'word': '竞争比率', 'ratio': 0.1111111111111111}]",语料库,{},[]
157,157,Corpus,语料库,0.5555555555555556,9,"[{'word': '语料库', 'ratio': 0.5555555555555556}, {'word': '语料', 'ratio': 0.3333333333333333}, {'word': '组合函数', 'ratio': 0.1111111111111111}]",语料库,{},[]
158,158,Cosine Similarity,余弦相似度,0.8888888888888888,9,"[{'word': '余弦相似度', 'ratio': 0.8888888888888888}, {'word': '组合泛化', 'ratio': 0.1111111111111111}]",余弦相似度,{},[]
159,159,Cosine distance,余弦距离,0.8888888888888888,9,"[{'word': '余弦距离', 'ratio': 0.8888888888888888}, {'word': '组合语义', 'ratio': 0.1111111111111111}]",余弦距离,{},[]
160,160,Counterfactual Regret Minimization,反事实遗憾最小化 (CFR),0.0,9,"[{'word': '反事实遗憾最小化', 'ratio': 0.8888888888888888}, {'word': '反事实后悔最小化', 'ratio': 0.1111111111111111}]",反事实遗憾最小化,{},[]
161,161,Covariance,协方差,1.0,9,"[{'word': '协方差', 'ratio': 1.0}]",协方差,{},[]
162,162,Cross Entropy Loss,交叉熵损失,1.0,9,"[{'word': '交叉熵损失', 'ratio': 1.0}]",交叉熵损失,{},[]
163,163,Data Augmentation,数据增强,1.0,9,"[{'word': '数据增强', 'ratio': 1.0}]",数据增强,{},[]
164,164,Datalog,数据逻辑,0.3333333333333333,9,"[{'word': '数据逻辑', 'ratio': 0.3333333333333333}, {'word': '数据日志语言', 'ratio': 0.3333333333333333}, {'word': '電腦科學家', 'ratio': 0.1111111111111111}, {'word': '计算机科学家', 'ratio': 0.1111111111111111}, {'word': 'Datalog', 'ratio': 0.1111111111111111}]",数据逻辑,"1. Rank: 数据逻辑, 数据日志语言, Datalog, 数据日志, 计算机科学家, 電腦科學家

2. Explanation: The term ""数据逻辑"" (Data logic) is the best fit for the translation of ""Datalog"" in the context of AI and computer science. This is because ""数据逻辑"" accurately captures the semantic essence of Datalog as a logic programming language that deals with data and logical inference. The term ""数据"" (data) directly relates to the data-centric nature of Datalog, while ""逻辑"" (logic) reflects its foundation in logical reasoning.

The second candidate, ""数据日志语言"" (data logging language), is less accurate because it implies a focus on logging data rather than the logical inference aspect of Datalog. The term ""Datalog"" itself is a proper noun and should be retained in contexts where the specific programming language is being discussed, but it does not convey the meaning as effectively as ""数据逻辑.""

The other candidates, ""数据日志"" (data log) and the translations for ""computer scientist,"" do not relate to the term ""Datalog"" and are therefore irrelevant in this context. Thus, ""数据逻辑"" stands out as the most semantically accurate and contextually appropriate translation for the term in the AI domain.","['Data logic', 'data logging language', 'computer scientist', 'computer scientist', 'data log']"
165,165,Datalog program,数据库逻辑程序,0.0,10,"[{'word': 'Datalog 程序', 'ratio': 0.6}, {'word': 'Datalog程序', 'ratio': 0.3}, {'word': '程序', 'ratio': 0.1}]",Datalog 程序,{},[]
166,166,Dataset,数据集,1.0,10,"[{'word': '数据集', 'ratio': 1.0}]",数据集,{},[]
167,167,Decentralized optimization,分散优化,0.3,10,"[{'word': '去中心化优化', 'ratio': 0.4}, {'word': '分散优化', 'ratio': 0.3}, {'word': '分散式优化', 'ratio': 0.2}, {'word': '分布式优化', 'ratio': 0.1}]",去中心化优化,"1. Rank: 去中心化优化, 分散优化, 分散式优化, 分布式优化

2. Explanation: The term ""去中心化优化"" (Decentralized optimization) is the best fit because it directly translates the concept of decentralization, which is crucial in the context of optimization methods that do not rely on a central authority. This term accurately reflects the AI domain's emphasis on decentralized systems, particularly in scenarios where networks are dynamic or unknown. 

""分散优化"" (decentralized optimization) is also a strong candidate, as it conveys a similar meaning, but it lacks the explicit reference to ""去中心化"" (decentralization), which is more commonly used in AI literature to describe systems that operate without a central control point.

""分散式优化"" (decentralized-style optimization) is less preferred because the addition of ""式"" (style) can imply a method or approach rather than the core concept of decentralization itself.

""分布式优化"" (distributed optimization) is the least suitable in this context, as ""分布式"" (distributed) typically refers to systems where tasks are distributed across multiple nodes but may still rely on a central coordination mechanism, which does not align with the intended meaning of ""decentralized"" in the context provided.","['Decentralized optimization', 'decentralized optimization', 'decentralized optimization', 'Distributed optimization']"
168,168,Decision Transformer,决策Transformer,0.2,10,"[{'word': '决策变换器', 'ratio': 0.7}, {'word': '决策Transformer', 'ratio': 0.2}, {'word': '决策转换器', 'ratio': 0.1}]",决策变换器,{},[]
169,169,Decoder,解码器,1.0,10,"[{'word': '解码器', 'ratio': 1.0}]",解码器,{},[]
170,170,Decomposable Attention,可分解式注意力,0.0,9,"[{'word': '可分解注意力', 'ratio': 1.0}]",可分解注意力,{},[]
171,171,Decomposable Attention Model,可分解注意力模型,1.0,9,"[{'word': '可分解注意力模型', 'ratio': 1.0}]",可分解注意力模型,{},[]
172,172,Deep Belief Network,深度信念网络,0.6666666666666666,9,"[{'word': '深度信念网络', 'ratio': 0.6666666666666666}, {'word': '深度置信网络', 'ratio': 0.3333333333333333}]",深度信念网络,{},[]
173,173,Deep Learning,深度学习,1.0,9,"[{'word': '深度学习', 'ratio': 1.0}]",深度学习,{},[]
174,174,Demographic Parity,人口统计平等,0.3333333333333333,9,"[{'word': '人口统计平等', 'ratio': 0.3333333333333333}, {'word': '人口平等', 'ratio': 0.3333333333333333}, {'word': '人口统计平衡', 'ratio': 0.1111111111111111}, {'word': '人口统计学公平性', 'ratio': 0.1111111111111111}, {'word': '人口统计公平', 'ratio': 0.1111111111111111}]",人口统计平等,"1. Rank: 人口统计平等, 人口统计公平, 人口统计学公平性, 人口平等, 人口统计平衡

2. Explanation: The term ""人口统计平等"" (Demographic equality) is the best fit for ""Demographic Parity"" because it accurately captures the essence of the concept in the AI domain, which emphasizes equality across different demographic groups in predictions. The back translation ""Demographic equality"" aligns closely with the original English term, maintaining semantic accuracy. 

""人口统计公平"" (Demographic equity) and ""人口统计学公平性"" (Demographic Equity) are also relevant but slightly less precise, as ""equity"" can imply a focus on fairness rather than strict equality, which is the core of demographic parity. 

""人口平等"" (Population equality) is a more general term and does not specifically convey the statistical aspect implied by ""demographic."" Lastly, ""人口统计平衡"" (Demographic balance) suggests a focus on balance rather than equality, which diverges from the intended meaning of demographic parity. Thus, ""人口统计平等"" is the most contextually appropriate choice in the AI terminology landscape.","['Demographic equality', 'Population equality', 'Demographic balance', 'Demographic Equity', 'Demographic equity']"
175,175,Denoising Autoencoder,降噪自编码器,0.0,10,"[{'word': '去噪自编码器', 'ratio': 1.0}]",去噪自编码器,{},[]
176,176,Detectron,Detectron,0.5,10,"[{'word': 'Detectron', 'ratio': 0.5}, {'word': '探测器', 'ratio': 0.2}, {'word': '偵測器', 'ratio': 0.1}, {'word': '通常保持原名不变。', 'ratio': 0.1}, {'word': '具体含义需根据上下文确定', 'ratio': 0.1}]",Detectron,{},[]
177,177,Determinantal Point process,行列式点过程,1.0,10,"[{'word': '行列式点过程', 'ratio': 1.0}]",行列式点过程,{},[]
178,178,Dialogue State Tracking,对话状态跟踪,0.6,10,"[{'word': '对话状态跟踪', 'ratio': 0.6}, {'word': '对话状态追踪', 'ratio': 0.4}]",对话状态跟踪,{},[]
179,179,Dice coefficient,骰子系数,0.1,10,"[{'word': 'Dice系数', 'ratio': 0.4}, {'word': '戴斯系数', 'ratio': 0.3}, {'word': 'Dice 系数', 'ratio': 0.2}, {'word': '骰子系数', 'ratio': 0.1}]","""Dice系数""","1. Rank: ""Dice系数"", ""Dice 系数"", ""骰子系数"", ""戴斯系数""

2. Explanation: The term ""Dice系数"" is the best fit for the translation of ""Dice coefficient"" in the AI domain due to its semantic accuracy and established usage in the field. The term ""Dice"" is a well-known metric in machine learning and information retrieval, particularly in the context of evaluating similarity between sets. The use of ""系数"" (coefficient) is appropriate and aligns with mathematical terminology in Chinese.

The alternative ""Dice 系数"" is also acceptable, but the space between ""Dice"" and ""系数"" is less common in technical contexts. ""骰子系数"" translates to ""dice coefficient"" literally, but it may introduce confusion as ""骰子"" (dice) can refer to the physical object rather than the metric itself. Lastly, ""戴斯系数"" is a transliteration that does not retain the original meaning and is not recognized in the AI community, making it the least suitable option. Thus, ""Dice系数"" is the most contextually accurate and widely accepted term in the AI domain.","['Dice coefficient', 'Dais coefficient', 'Dice coefficient', 'Dice coefficient']"
180,180,Dice loss,骰子损失,0.1,10,"[{'word': 'Dice损失', 'ratio': 0.4}, {'word': '戴斯损失', 'ratio': 0.3}, {'word': 'Dice 损失', 'ratio': 0.2}, {'word': '骰子损失', 'ratio': 0.1}]",Dice损失,"1. Rank: Dice损失, 骰子损失, 戴斯损失, Dice 损失

2. Explanation: The term ""Dice损失"" is the best fit because it accurately translates ""Dice loss"" while maintaining the semantic integrity of the term within the AI domain. ""Dice"" refers to the Dice coefficient, a well-known metric in image segmentation tasks, and ""损失"" means ""loss,"" which is a standard term in machine learning contexts. This translation is widely recognized in the AI community, ensuring clarity and precision in communication.

The second option, ""骰子损失,"" translates to ""Dice loss"" but uses the word ""骰子,"" which means ""dice"" in a general sense, rather than the specific term used in the context of the Dice coefficient. This could lead to confusion, as it does not directly connect to the established terminology in AI.

The third option, ""戴斯损失,"" translates to ""Days loss,"" which is incorrect and does not relate to the original term at all. 

The last option, ""Dice 损失,"" while it retains the term ""Dice,"" is less preferable because it includes a space that is not standard in Chinese terminology, making it less formal and potentially confusing in a technical context. 

Overall, ""Dice损失"" is the most semantically accurate and contextually appropriate translation for the AI domain.","['Dice loss', 'Days loss', 'Loss', 'Dice loss']"
181,181,Dijkstra's algorithm,Dijkstra算法,0.3,10,"[{'word': 'Dijkstra算法', 'ratio': 0.3}, {'word': '迪杰斯特拉算法', 'ratio': 0.3}, {'word': 'Dijkstra 算法', 'ratio': 0.2}, {'word': '迪克斯特拉算法', 'ratio': 0.1}, {'word': '戴克斯特拉算法', 'ratio': 0.1}]",Dijkstra算法,"1. Rank: Dijkstra算法, 迪杰斯特拉算法, Dijkstra 算法, 迪克斯特拉算法, 戴克斯特拉算法

2. Explanation: The term ""Dijkstra算法"" is the best fit because it directly translates to ""Dijkstra's algorithm"" while retaining the original name ""Dijkstra,"" which is crucial in the context of algorithmic terminology. This term is widely recognized in the AI and computer science communities in Chinese-speaking regions, ensuring semantic accuracy and contextual fit. The other candidates, while they may be understandable, either use transliterations that are less common (like ""迪杰斯特拉算法"" or ""戴克斯特拉算法"") or do not follow the standard naming conventions in the field (like ""Dijkstra 算法,"" which is less formal). Retaining the original name is important for clarity and recognition in academic and professional settings, making ""Dijkstra算法"" the most appropriate choice.","[""Dijkstra's algorithm"", ""Dijkstra's algorithm"", ""Dijkstra's algorithm"", ""Dijkstra's algorithm"", ""Dijkstra's algorithm""]"
182,182,Dirac distribution,狄拉克分布,0.5,10,"[{'word': '狄拉克分布', 'ratio': 0.5}, {'word': 'Dirac分布', 'ratio': 0.3}, {'word': 'Dirac 分布', 'ratio': 0.2}]",狄拉克分布,{},[]
183,183,Dirac measure,狄拉克测度,0.6666666666666666,9,"[{'word': '狄拉克测度', 'ratio': 0.6666666666666666}, {'word': '德拉克测度', 'ratio': 0.1111111111111111}, {'word': 'Dirac 测度', 'ratio': 0.1111111111111111}, {'word': 'Dirac测度', 'ratio': 0.1111111111111111}]",狄拉克测度,{},[]
184,184,Dirichlet,狄利克雷分布,0.0,9,"[{'word': '狄利克雷', 'ratio': 0.8888888888888888}, {'word': 'Dirichlet分布', 'ratio': 0.1111111111111111}]",狄利克雷,{},[]
185,185,Dirichlet Process,狄利克雷过程,0.7777777777777778,9,"[{'word': '狄利克雷过程', 'ratio': 0.7777777777777778}, {'word': 'Dirichlet 过程', 'ratio': 0.1111111111111111}, {'word': 'Dirichlet过程', 'ratio': 0.1111111111111111}]",狄利克雷过程,{},[]
186,186,Dirichlet distribution,第里夏分布,0.0,9,"[{'word': '狄利克雷分布', 'ratio': 0.7777777777777778}, {'word': 'Dirichlet 分布', 'ratio': 0.1111111111111111}, {'word': 'Dirichlet分布', 'ratio': 0.1111111111111111}]",狄利克雷分布,{},[]
187,187,Dirichlet prior,狄利克雷先验,0.7777777777777778,9,"[{'word': '狄利克雷先验', 'ratio': 0.7777777777777778}, {'word': 'Dirichlet 先验', 'ratio': 0.1111111111111111}, {'word': 'Dirichlet先验', 'ratio': 0.1111111111111111}]",狄利克雷先验,{},[]
188,188,Discretization,离散化,1.0,9,"[{'word': '离散化', 'ratio': 1.0}]",离散化,{},[]
189,189,Discriminator,判别器,1.0,9,"[{'word': '判别器', 'ratio': 1.0}]",判别器,{},[]
190,190,Disentanglement,解缠,0.1111111111111111,9,"[{'word': '解缠结', 'ratio': 0.4444444444444444}, {'word': '解耦', 'ratio': 0.4444444444444444}, {'word': '解缠', 'ratio': 0.1111111111111111}]",解耦,"1. Rank: 解耦, 解缠结, 解缠

2. Explanation: The term ""解耦"" (decoupling) is the best fit for the context of ""disentanglement"" in the AI domain. In machine learning and AI, ""decoupling"" refers to the process of separating different components or factors in a model, which aligns closely with the concept of disentanglement, where the goal is to isolate and understand the individual factors that contribute to the overall data representation. 

While ""解缠结"" (untangle) and ""解缠"" (untangle) are also valid translations, they are more general and do not capture the specific technical nuance of separating or isolating components as effectively as ""解耦."" The back translation of ""解耦"" as ""decoupling"" further emphasizes its relevance in the context of AI, where disentanglement is often discussed in terms of separating variables or features in a model. Thus, ""解耦"" is the most semantically accurate and contextually appropriate choice.","['Untangle', 'decoupling', 'Untangle']"
191,191,Domain Adaptation,领域适应,0.5555555555555556,9,"[{'word': '领域适应', 'ratio': 0.5555555555555556}, {'word': '域适应', 'ratio': 0.3333333333333333}, {'word': '领域自适应', 'ratio': 0.1111111111111111}]",领域适应,{},[]
192,192,Dropout,随机失活,0.6666666666666666,9,"[{'word': '随机失活', 'ratio': 0.6666666666666666}, {'word': '丢弃法', 'ratio': 0.3333333333333333}]",随机失活,{},[]
193,193,Dropout distribution,丢弃分布,0.1111111111111111,9,"[{'word': 'Dropout分布', 'ratio': 0.4444444444444444}, {'word': '随机失活分布', 'ratio': 0.3333333333333333}, {'word': 'Dropout 分布', 'ratio': 0.1111111111111111}, {'word': '丢弃分布', 'ratio': 0.1111111111111111}]",Dropout分布,"1. Rank: Dropout分布, Dropout 分布, 随机失活分布, 丢弃分布

2. Explanation: The term ""Dropout分布"" is the best fit because it retains the original English term ""Dropout"" while providing a clear and direct translation of ""distribution"" as ""分布"". This combination is widely recognized in the AI and machine learning communities, particularly in the context of neural networks and Bayesian methods. The use of ""Dropout"" in its original form helps maintain consistency with established terminology in the field, making it easily understandable for practitioners familiar with the concept. 

The second candidate, ""Dropout 分布,"" is also a strong option, but the space between ""Dropout"" and ""分布"" is less common in technical writing, where terms are often presented as a single unit. 

""随机失活分布"" translates to ""random deactivation distribution,"" which, while semantically accurate, introduces a different nuance that may not align with the established terminology of ""dropout"" in neural networks. 

Lastly, ""丢弃分布"" translates to ""drop distribution,"" which is less precise and could lead to confusion, as ""丢弃"" (to discard) does not capture the specific meaning of ""dropout"" in the context of neural networks. Thus, ""Dropout分布"" is the most contextually appropriate choice.","['Dropout distribution', 'random deactivation distribution', 'Dropout distribution', 'drop distribution']"
194,194,Dynamic Programming,动态规划,1.0,9,"[{'word': '动态规划', 'ratio': 1.0}]",动态规划,{},[]
195,195,E-step,期望步,0.0,9,"[{'word': 'E步', 'ratio': 0.5555555555555556}, {'word': 'E步骤', 'ratio': 0.2222222222222222}, {'word': 'E 步', 'ratio': 0.1111111111111111}, {'word': '期望步骤', 'ratio': 0.1111111111111111}]",E步,{},[]
196,196,Elastic Net,弹性网,0.4444444444444444,9,"[{'word': '弹性网络', 'ratio': 0.5555555555555556}, {'word': '弹性网', 'ratio': 0.4444444444444444}]",弹性网络,{},[]
197,197,Electra,伊莱克特拉,0.1111111111111111,9,"[{'word': 'Electra', 'ratio': 0.7777777777777778}, {'word': '伊萊克特拉', 'ratio': 0.1111111111111111}, {'word': '伊莱克特拉', 'ratio': 0.1111111111111111}]",Electra,{},[]
198,198,Encoder,编码器,1.0,9,"[{'word': '编码器', 'ratio': 1.0}]",编码器,{},[]
199,199,Encoder-Decoder,编码器-解码器,0.6666666666666666,9,"[{'word': '编码器-解码器', 'ratio': 0.6666666666666666}, {'word': '编码-解码器', 'ratio': 0.3333333333333333}]",编码器-解码器,{},[]
200,200,Entity Linking,实体链接,1.0,9,"[{'word': '实体链接', 'ratio': 1.0}]",实体链接,{},[]
201,201,Entropy,熵,1.0,9,"[{'word': '熵', 'ratio': 1.0}]",熵,{},[]
202,202,Epanechnikov kernel,埃帕内奇尼科夫内核,0.0,10,"[{'word': '核函数', 'ratio': 0.3}, {'word': 'Epanechnikov核', 'ratio': 0.2}, {'word': '核', 'ratio': 0.1}, {'word': '埃帕涅奇科夫核', 'ratio': 0.1}, {'word': '艾潘尼奇科夫核', 'ratio': 0.1}, {'word': '伊帕内奇尼科夫核', 'ratio': 0.1}, {'word': 'Epanechnikov核函数', 'ratio': 0.1}]","""Epanechnikov核函数""","1. Rank: ""Epanechnikov核函数"", ""埃帕涅奇科夫核"", ""艾潘尼奇科夫核"", ""伊帕内奇尼科夫核"", ""核函数"", ""核"", ""Epanechnikov核""

2. Explanation: The term ""Epanechnikov核函数"" is the best fit because it accurately retains the original name ""Epanechnikov"" while also specifying that it is a ""kernel function,"" which is crucial in the context of AI and statistics. This term provides both semantic accuracy and clarity, making it immediately understandable to those familiar with the field. The other terms, such as ""埃帕涅奇科夫核"" and ""艾潘尼奇科夫核,"" are transliterations that do not include the term ""函数"" (function), which is essential for conveying the complete meaning in the context of kernel methods in AI. The term ""核函数"" is too generic and lacks specificity regarding the Epanechnikov aspect, while ""核"" and ""Epanechnikov核"" do not provide the necessary context of a function. Therefore, ""Epanechnikov核函数"" stands out as the most contextually appropriate and semantically accurate choice.","['kernel function', 'Epanechnikov core', 'nuclear', 'Epanechikov core', 'Ipanichkov core', 'Ipanechnikov core', 'Epanechnikov kernel function']"
203,203,Euclidean,欧几里得,1.0,10,"[{'word': '欧几里得', 'ratio': 1.0}]",欧几里得,{},[]
204,204,Euclidean distance,欧氏距离,0.1,10,"[{'word': '欧几里得距离', 'ratio': 0.9}, {'word': '欧氏距离', 'ratio': 0.1}]",欧几里得距离,{},[]
205,205,Euclidean divergence,欧几里德散度,0.0,10,"[{'word': '欧几里得散度', 'ratio': 1.0}]",欧几里得散度,{},[]
206,206,Euclidean loss,欧几里得损失,1.0,10,"[{'word': '欧几里得损失', 'ratio': 1.0}]",欧几里得损失,{},[]
207,207,Euclidean norm,欧几里得范数,0.8888888888888888,9,"[{'word': '欧几里得范数', 'ratio': 0.8888888888888888}, {'word': '欧几里得', 'ratio': 0.1111111111111111}]",欧几里得范数,{},[]
208,208,Euclidean plane,欧几里德平面,0.0,9,"[{'word': '欧几里得平面', 'ratio': 1.0}]",欧几里得平面,{},[]
209,209,Euclidean projection,欧几里得投影,1.0,9,"[{'word': '欧几里得投影', 'ratio': 1.0}]",欧几里得投影,{},[]
210,210,Euclidean space,欧几里得空间,1.0,9,"[{'word': '欧几里得空间', 'ratio': 1.0}]",欧几里得空间,{},[]
211,211,Euclidean transformation,欧氏变换,0.0,9,"[{'word': '欧几里得变换', 'ratio': 1.0}]",欧几里得变换,{},[]
212,212,Euler angle,欧拉角,1.0,9,"[{'word': '欧拉角', 'ratio': 1.0}]",欧拉角,{},[]
213,213,Euler step,欧拉步骤,0.0,9,"[{'word': '欧拉步', 'ratio': 0.8888888888888888}, {'word': '欧拉步长', 'ratio': 0.1111111111111111}]",欧拉步,{},[]
214,214,Exact Match,完全匹配,0.2222222222222222,9,"[{'word': '精确匹配', 'ratio': 0.7777777777777778}, {'word': '完全匹配', 'ratio': 0.2222222222222222}]",精确匹配,{},[]
215,215,Expectation Maximization,期望最大化,0.8888888888888888,9,"[{'word': '期望最大化', 'ratio': 0.8888888888888888}, {'word': '最大期望算法', 'ratio': 0.1111111111111111}]",期望最大化,{},[]
216,216,Expectation Maximization algorithm,期望最大化算法,0.8888888888888888,9,"[{'word': '期望最大化算法', 'ratio': 0.8888888888888888}, {'word': '最大期望算法', 'ratio': 0.1111111111111111}]",期望最大化算法,{},[]
217,217,Exponential distribution,指数分布,1.0,10,"[{'word': '指数分布', 'ratio': 1.0}]",指数分布,{},[]
218,218,Extended Kalman Filter,扩展卡尔曼滤波器,1.0,10,"[{'word': '扩展卡尔曼滤波器', 'ratio': 1.0}]",扩展卡尔曼滤波器,{},[]
219,219,F-measure,F-测度,0.0,10,"[{'word': 'F-measure', 'ratio': 0.3}, {'word': 'F值', 'ratio': 0.3}, {'word': 'F度量', 'ratio': 0.2}, {'word': '', 'ratio': 0.1}, {'word': 'F-测量', 'ratio': 0.1}]",F-measure,"1. Rank: F-measure, F值, F度量, F-测量, 

2. Explanation: The term ""F-measure"" is the most accurate translation because it retains the original English term, which is widely recognized in the AI and machine learning communities. This term is commonly used in academic literature and technical discussions, making it the most contextually appropriate choice. The back translation also remains ""F-measure,"" which indicates that the term is understood in its original form, ensuring clarity and consistency in communication.

The second choice, ""F值,"" translates to ""F value,"" which is also acceptable but less specific than ""F-measure."" While it conveys a similar meaning, it may not be as widely recognized in the context of performance metrics in AI.

The third option, ""F度量,"" translates to ""F metric,"" which is somewhat accurate but less common than ""F-measure."" It may not be as immediately recognizable to those familiar with the specific metric being discussed.

The last two options, ""F-测量"" and the empty string, are not suitable as they either introduce ambiguity or lack context altogether. Therefore, ""F-measure"" is the best fit for its semantic accuracy and contextual relevance in the AI domain.","['F-measure', 'F value', 'F-measure', 'F-measure', 'F-measurement']"
220,220,F-score,F分数,0.6,10,"[{'word': 'F分数', 'ratio': 0.6}, {'word': 'F-score', 'ratio': 0.2}, {'word': '', 'ratio': 0.1}, {'word': 'F-得分', 'ratio': 0.1}]",F分数,{},[]
221,221,F1 measure,F1指标,0.0,10,"[{'word': 'F1值', 'ratio': 0.5}, {'word': 'F1度量', 'ratio': 0.3}, {'word': '值', 'ratio': 0.1}, {'word': 'F1-测量', 'ratio': 0.1}]",F1值,{},[]
222,222,F1 metric,F1指标,0.6666666666666666,9,"[{'word': 'F1指标', 'ratio': 0.6666666666666666}, {'word': 'F1度量', 'ratio': 0.2222222222222222}, {'word': 'F1 指标', 'ratio': 0.1111111111111111}]",F1指标,{},[]
223,223,F1 score,F1 分数,0.1111111111111111,9,"[{'word': 'F1分数', 'ratio': 0.5555555555555556}, {'word': 'F1得分', 'ratio': 0.3333333333333333}, {'word': 'F1 分数', 'ratio': 0.1111111111111111}]",F1分数,{},[]
224,224,Fairseq,Fairseq,0.5555555555555556,9,"[{'word': 'Fairseq', 'ratio': 0.5555555555555556}, {'word': '', 'ratio': 0.2222222222222222}, {'word': '費爾序列', 'ratio': 0.1111111111111111}, {'word': 'Fairseq库', 'ratio': 0.1111111111111111}]",Fairseq,{},[]
225,225,Fano's inequality,法诺不等式,0.5555555555555556,9,"[{'word': '法诺不等式', 'ratio': 0.5555555555555556}, {'word': 'Fano不等式', 'ratio': 0.3333333333333333}, {'word': '范诺不等式', 'ratio': 0.1111111111111111}]",法诺不等式,{},[]
226,226,Fast Fourier Transform,快速傅里叶变换,1.0,10,"[{'word': '快速傅里叶变换', 'ratio': 1.0}]",快速傅里叶变换,{},[]
227,227,Feature Pyramid Network,特征金字塔网络 (FPN),0.0,10,"[{'word': '特征金字塔网络', 'ratio': 1.0}]",特征金字塔网络,{},[]
228,228,Federated Learning,联邦学习,1.0,10,"[{'word': '联邦学习', 'ratio': 1.0}]",联邦学习,{},[]
229,229,Fisher information matrix,费舍尔信息矩阵,0.4,10,"[{'word': '费舍尔信息矩阵', 'ratio': 0.4}, {'word': '费舍信息矩阵', 'ratio': 0.4}, {'word': '菲舍尔信息矩阵', 'ratio': 0.2}]",费舍尔信息矩阵,"1. Rank: 费舍尔信息矩阵, 费舍信息矩阵, 菲舍尔信息矩阵

2. Explanation: The term ""费舍尔信息矩阵"" is the most accurate translation for ""Fisher information matrix"" because it correctly retains the original name ""Fisher"" in a widely accepted transliteration that is recognized in the AI and statistics communities. This term is semantically accurate and aligns with the established terminology used in academic literature. 

The second candidate, ""费舍信息矩阵,"" is slightly less accurate because it omits the ""尔"" character, which can lead to confusion or misinterpretation in the context of statistical terminology. The third candidate, ""菲舍尔信息矩阵,"" uses ""菲"" instead of ""费,"" which is not the standard transliteration for ""Fisher"" and could lead to misunderstandings, especially in a technical context. Therefore, ""费舍尔信息矩阵"" is the best fit for its semantic accuracy and contextual relevance in the AI domain.","['Fisher information matrix', 'Fisher information matrix', 'Fischer information matrix']"
230,230,Fisher score,费希尔分数,0.0,10,"[{'word': '费舍尔得分', 'ratio': 0.4}, {'word': '菲舍尔评分', 'ratio': 0.2}, {'word': '费舍尔评分', 'ratio': 0.2}, {'word': '费舍尔分数', 'ratio': 0.1}, {'word': '菲舍尔得分', 'ratio': 0.1}]",费舍尔得分,"1. Rank: 费舍尔得分, 费舍尔评分, 费舍尔分数, 菲舍尔评分, 菲舍尔得分

2. Explanation: The term ""费舍尔得分"" is the best fit because it accurately retains the original name ""Fisher"" while using ""得分,"" which translates to ""score"" in a contextually appropriate manner for statistical and AI terminology. This term is semantically accurate and aligns well with the common usage in the AI domain, where ""得分"" is frequently used to denote scores or metrics. 

""费舍尔评分"" is also a strong candidate, as ""评分"" translates to ""rating,"" which is somewhat relevant but less precise than ""得分"" in this context. ""费舍尔分数"" translates to ""Fisher fraction,"" which introduces ambiguity and does not accurately reflect the intended meaning of ""score."" The other candidates, ""菲舍尔评分"" and ""菲舍尔得分,"" use a transliteration of ""Fisher"" as ""菲舍尔,"" which is less common in the AI literature compared to ""费舍尔."" Therefore, ""费舍尔得分"" is the most contextually appropriate and semantically accurate choice.","['fisher score', 'Fischer score', 'Fisher score', 'Fisher fraction', 'Fischer scores']"
231,231,Fleiss' kappa,费利斯kappa,0.0,10,"[{'word': '弗莱斯κ系数', 'ratio': 0.4}, {'word': '弗莱斯κ值', 'ratio': 0.1}, {'word': ""Fleiss' kappa"", 'ratio': 0.1}, {'word': 'Fleiss kappa系数', 'ratio': 0.1}, {'word': '弗莱斯卡帕系数', 'ratio': 0.1}, {'word': ""Fleiss' κ"", 'ratio': 0.1}, {'word': '弗里斯卡帕系数', 'ratio': 0.1}]",弗莱斯κ系数,"1. Rank: 弗莱斯κ系数, 弗莱斯κ值, Fleiss' kappa, Fleiss kappa系数, 弗莱斯卡帕系数, Fleiss' κ, 弗里斯卡帕系数

2. Explanation: The term ""弗莱斯κ系数"" (Fleiss kappa coefficient) is the best fit because it accurately retains the original name ""Fleiss"" while also including the term ""κ"" (kappa) and ""系数"" (coefficient), which is a standard term in statistical contexts. This translation is semantically accurate and contextually appropriate for the AI domain, where precision in terminology is crucial. The use of ""系数"" clearly indicates that it is a statistical measure, which aligns with the context of inter-annotator agreement being discussed. 

The second choice, ""弗莱斯κ值"" (Fleiss kappa value), is also a good option but uses ""值"" (value) instead of ""系数"" (coefficient), which is less specific in a statistical context. The other candidates either do not retain the original name effectively or use less precise terminology, making them less suitable for this specific application in AI and statistics.","['Fleiss kappa coefficient', 'Fleiss kappa value', 'Class F SS kappa', 'Fleiss kappa coefficient', 'Fleiskappa coefficient', ""Fleiss' κ"", 'Frisian Kappa coefficient']"
232,232,Floyd-Warshall algorithm,弗洛伊德-沃舍尔算法,0.0,10,"[{'word': '弗洛伊德-沃肖尔算法', 'ratio': 0.4}, {'word': '弗洛伊德-沃尔沙尔算法', 'ratio': 0.1}, {'word': '弗洛伊德-华舍尔算法', 'ratio': 0.1}, {'word': '弗洛伊德-华沙算法', 'ratio': 0.1}, {'word': 'Floyd-Warshall算法', 'ratio': 0.1}, {'word': '弗洛伊德-沃肖算法', 'ratio': 0.1}, {'word': '弗洛伊德-沃沙尔算法', 'ratio': 0.1}]",弗洛伊德-沃肖尔算法,"1. Rank: 弗洛伊德-沃肖尔算法, Floyd-Warshall算法, 弗洛伊德-沃尔沙尔算法, 弗洛伊德-华舍尔算法, 弗洛伊德-华沙算法, 弗洛伊德-沃肖算法, 弗洛伊德-沃沙尔算法

2. Explanation: The top-ranked translation, ""弗洛伊德-沃肖尔算法,"" is the best fit because it accurately retains the original name ""Floyd"" and ""Warshall,"" which is crucial in the context of algorithm names in the AI domain. This translation uses the correct phonetic representation of ""Warshall"" and maintains the structure of the original term, making it easily recognizable to those familiar with the algorithm. 

The second choice, ""Floyd-Warshall算法,"" is also acceptable as it retains the original English term, but it lacks the full Chinese representation, which is often preferred in academic and technical contexts. 

The other candidates either misrepresent the names (e.g., ""弗洛伊德-沃尔沙尔算法"" and ""弗洛伊德-华沙算法"") or introduce inaccuracies in the transliteration (e.g., ""Freud"" instead of ""Floyd""), which can lead to confusion and misidentification of the algorithm. Therefore, the first option is the most semantically accurate and contextually appropriate for the AI domain.","['Floyd-Warshall algorithm', 'Freud-Walshall algorithm', 'Freud-Waschel algorithm', 'Freud-Warsaw algorithm', 'Floyd-Warshall algorithm', 'Floyd-Warshaw algorithm', 'Freud-Warshal algorithm']"
233,233,Focal Loss,焦点损失,0.9,10,"[{'word': '焦点损失', 'ratio': 0.9}, {'word': '焦点损失函数', 'ratio': 0.1}]",焦点损失,{},[]
234,234,Fokker-Planck equation,福克-普朗克方程,1.0,10,"[{'word': '福克-普朗克方程', 'ratio': 1.0}]",福克-普朗克方程,{},[]
235,235,Fourier Transform,傅里叶变换,1.0,9,"[{'word': '傅里叶变换', 'ratio': 1.0}]",傅里叶变换,{},[]
236,236,Fourier basis function,"""傅里叶基函数""",0.0,9,"[{'word': '傅里叶基函数', 'ratio': 1.0}]",傅里叶基函数,{},[]
237,237,Fourier coefficient,傅立叶系数,0.0,9,"[{'word': '傅里叶系数', 'ratio': 1.0}]",傅里叶系数,{},[]
238,238,Fourier feature,傅里叶特征,1.0,9,"[{'word': '傅里叶特征', 'ratio': 1.0}]",傅里叶特征,{},[]
239,239,Fourier frequency,傅里叶频率,1.0,9,"[{'word': '傅里叶频率', 'ratio': 1.0}]",傅里叶频率,{},[]
240,240,Frobenius Norm,弗罗贝尼乌斯范数,0.4444444444444444,9,"[{'word': '弗罗贝尼乌斯范数', 'ratio': 0.4444444444444444}, {'word': 'Frobenius范数', 'ratio': 0.3333333333333333}, {'word': 'Frobenius 范数', 'ratio': 0.1111111111111111}, {'word': '弗罗贝纽斯范数', 'ratio': 0.1111111111111111}]",弗罗贝尼乌斯范数,"1. Rank: 弗罗贝尼乌斯范数, 弗罗贝纽斯范数, Frobenius范数, Frobenius 范数

2. Explanation: The term ""弗罗贝尼乌斯范数"" is the best fit because it accurately retains the original name ""Frobenius"" while providing a clear and standard translation of ""norm"" as ""范数"". This term is widely recognized in the mathematical and AI communities, ensuring semantic accuracy and contextual fit. The use of ""弗罗贝尼乌斯"" is a well-established transliteration of ""Frobenius"" in academic literature, making it the most appropriate choice for clarity and recognition. 

The alternative ""弗罗贝纽斯范数"" is also a valid option, but ""弗罗贝尼乌斯"" is more commonly used. The terms ""Frobenius范数"" and ""Frobenius 范数"" do not include the transliteration of the name in a way that aligns with standard Chinese usage, which may lead to confusion or lack of recognition in the AI domain. Therefore, the first term is the most suitable for conveying the intended meaning in a precise and contextually relevant manner.","['Frobenius norm', 'Frobenius norm', 'Frobenius norm', 'Frobenius norm']"
241,241,Frobenius inner product,弗罗贝尼乌斯内积,0.5555555555555556,9,"[{'word': '弗罗贝尼乌斯内积', 'ratio': 0.5555555555555556}, {'word': 'Frobenius内积', 'ratio': 0.3333333333333333}, {'word': 'Frobenius 内积', 'ratio': 0.1111111111111111}]",弗罗贝尼乌斯内积,{},[]
242,242,Fréchet,弗雷谢特,0.0,9,"[{'word': 'Fréchet', 'ratio': 0.4444444444444444}, {'word': '弗雷歇', 'ratio': 0.4444444444444444}, {'word': '弗雷謝特', 'ratio': 0.1111111111111111}]",Fréchet,"1. Rank: Fréchet, 弗雷歇, 弗雷謝特

2. Explanation: The first translation, ""Fréchet,"" is the best fit because it retains the original term without any alteration, which is crucial in the context of AI and statistical terminology. In academic and technical fields, especially in mathematics and statistics, it is common to keep the names of specific distributions or concepts in their original form to avoid confusion and maintain clarity. The second option, ""弗雷歇,"" is a transliteration that closely resembles the pronunciation of ""Fréchet"" in Mandarin, making it a reasonable alternative, but it does not capture the exact name as it is used in the literature. The third option, ""弗雷謝特,"" is less accurate as it introduces an unnecessary variation in the transliteration, which could lead to misinterpretation. Therefore, retaining the original term ""Fréchet"" is the most semantically accurate and contextually appropriate choice in the AI domain.","['Fréchet', 'Flecher', 'Frechette']"
243,243,Gamma distribution,伽马分布,0.8,10,"[{'word': '伽马分布', 'ratio': 0.8}, {'word': '分布', 'ratio': 0.1}, {'word': '伽玛分布', 'ratio': 0.1}]",伽马分布,{},[]
244,244,Gamma prior,伽马先验,0.7,10,"[{'word': '伽马先验', 'ratio': 0.7}, {'word': '伽玛先验', 'ratio': 0.2}, {'word': '先验分布', 'ratio': 0.1}]",伽马先验,{},[]
245,245,Gauss-Newton algorithm,高斯-牛顿算法,1.0,9,"[{'word': '高斯-牛顿算法', 'ratio': 1.0}]",高斯-牛顿算法,{},[]
246,246,Gauss-Seidel method,高斯-赛德尔法,0.1111111111111111,9,"[{'word': '高斯-赛德尔方法', 'ratio': 0.8888888888888888}, {'word': '高斯-赛德尔法', 'ratio': 0.1111111111111111}]",高斯-赛德尔方法,{},[]
247,247,Gaussian Mixture Model,高斯混合模型,1.0,9,"[{'word': '高斯混合模型', 'ratio': 1.0}]",高斯混合模型,{},[]
248,248,Gaussian Process,高斯过程,1.0,9,"[{'word': '高斯过程', 'ratio': 1.0}]",高斯过程,{},[]
249,249,Gaussian blur,高斯模糊,1.0,9,"[{'word': '高斯模糊', 'ratio': 1.0}]",高斯模糊,{},[]
250,250,Gaussian complexity,高斯复杂度,1.0,9,"[{'word': '高斯复杂度', 'ratio': 1.0}]",高斯复杂度,{},[]
251,251,Gaussian component,高斯分量,0.4444444444444444,9,"[{'word': '高斯成分', 'ratio': 0.5555555555555556}, {'word': '高斯分量', 'ratio': 0.4444444444444444}]",高斯成分,{},[]
252,252,Gaussian conditional random field,高斯条件随机场,1.0,9,"[{'word': '高斯条件随机场', 'ratio': 1.0}]",高斯条件随机场,{},[]
253,253,Gaussian density,高斯密度,1.0,9,"[{'word': '高斯密度', 'ratio': 1.0}]",高斯密度,{},[]
254,254,Gaussian distribution,高斯分布,1.0,9,"[{'word': '高斯分布', 'ratio': 1.0}]",高斯分布,{},[]
255,255,Gaussian elimination,高斯消元法,0.9,10,"[{'word': '高斯消元法', 'ratio': 0.9}, {'word': '高斯消去法', 'ratio': 0.1}]",高斯消元法,{},[]
256,256,Gaussian filter,高斯滤波器,1.0,10,"[{'word': '高斯滤波器', 'ratio': 1.0}]",高斯滤波器,{},[]
257,257,Gaussian function,高斯函数,1.0,10,"[{'word': '高斯函数', 'ratio': 1.0}]",高斯函数,{},[]
258,258,Gaussian initialization,高斯初始化,1.0,10,"[{'word': '高斯初始化', 'ratio': 1.0}]",高斯初始化,{},[]
259,259,Gaussian kernel,高斯核,1.0,10,"[{'word': '高斯核', 'ratio': 1.0}]",高斯核,{},[]
260,260,Gaussian likelihood,高斯似然,1.0,10,"[{'word': '高斯似然', 'ratio': 1.0}]",高斯似然,{},[]
261,261,Gaussian matrix,高斯矩阵,1.0,10,"[{'word': '高斯矩阵', 'ratio': 1.0}]",高斯矩阵,{},[]
262,262,Gaussian mixture,高斯混合模型,0.1,10,"[{'word': '高斯混合', 'ratio': 0.9}, {'word': '高斯混合模型', 'ratio': 0.1}]",高斯混合,{},[]
263,263,Gaussian model,高斯模型,1.0,10,"[{'word': '高斯模型', 'ratio': 1.0}]",高斯模型,{},[]
264,264,Gaussian noise,高斯噪声,1.0,10,"[{'word': '高斯噪声', 'ratio': 1.0}]",高斯噪声,{},[]
265,265,Gaussian prior,高斯先验,1.0,9,"[{'word': '高斯先验', 'ratio': 1.0}]",高斯先验,{},[]
266,266,Gaussian process model,高斯过程模型,1.0,9,"[{'word': '高斯过程模型', 'ratio': 1.0}]",高斯过程模型,{},[]
267,267,Gaussian process regression,高斯过程回归,1.0,9,"[{'word': '高斯过程回归', 'ratio': 1.0}]",高斯过程回归,{},[]
268,268,Gaussian random variable,高斯随机变量,1.0,9,"[{'word': '高斯随机变量', 'ratio': 1.0}]",高斯随机变量,{},[]
269,269,Gaussian smoothing,高斯平滑,1.0,9,"[{'word': '高斯平滑', 'ratio': 1.0}]",高斯平滑,{},[]
270,270,Gaussian variable,高斯变量,1.0,9,"[{'word': '高斯变量', 'ratio': 1.0}]",高斯变量,{},[]
271,271,Gaussian weight,高斯权重,1.0,9,"[{'word': '高斯权重', 'ratio': 1.0}]",高斯权重,{},[]
272,272,Gene Ontology,基因本体论,0.2222222222222222,9,"[{'word': '基因本体', 'ratio': 0.7777777777777778}, {'word': '基因本体论', 'ratio': 0.2222222222222222}]",基因本体,{},[]
273,273,Generative Adversarial Networks,生成对抗网络,1.0,9,"[{'word': '生成对抗网络', 'ratio': 1.0}]",生成对抗网络,{},[]
274,274,Generative Model,生成模型,1.0,9,"[{'word': '生成模型', 'ratio': 1.0}]",生成模型,{},[]
275,275,Generator,生成器,1.0,9,"[{'word': '生成器', 'ratio': 1.0}]",生成器,{},[]
276,276,Genetic algorithm,遗传算法,1.0,9,"[{'word': '遗传算法', 'ratio': 1.0}]",遗传算法,{},[]
277,277,Gensim,Gensim,0.6666666666666666,9,"[{'word': 'Gensim', 'ratio': 0.6666666666666666}, {'word': '', 'ratio': 0.1111111111111111}, {'word': 'ゲン', 'ratio': 0.1111111111111111}, {'word': '根', 'ratio': 0.1111111111111111}]",Gensim,{},[]
278,278,Gibbs Sampling,吉布斯采样,0.8888888888888888,9,"[{'word': '吉布斯采样', 'ratio': 0.8888888888888888}, {'word': '吉布斯抽样', 'ratio': 0.1111111111111111}]",吉布斯采样,{},[]
279,279,Gibbs iteration,吉布斯迭代,1.0,9,"[{'word': '吉布斯迭代', 'ratio': 1.0}]",吉布斯迭代,{},[]
280,280,Gibbs sampler,吉布斯采样器,0.7777777777777778,9,"[{'word': '吉布斯采样器', 'ratio': 0.7777777777777778}, {'word': '吉布斯抽样器', 'ratio': 0.2222222222222222}]",吉布斯采样器,{},[]
281,281,Gini coefficient,基尼系数,1.0,9,"[{'word': '基尼系数', 'ratio': 1.0}]",基尼系数,{},[]
282,282,Good-Turing estimate,好图灵估计,0.0,9,"[{'word': '古德-图灵估计', 'ratio': 1.0}]",古德-图灵估计,{},[]
283,283,GoogLeNet,GoogLeNet,0.4444444444444444,9,"[{'word': 'GoogLeNet', 'ratio': 0.4444444444444444}, {'word': '谷歌网', 'ratio': 0.2222222222222222}, {'word': '谷歌网络', 'ratio': 0.2222222222222222}, {'word': 'Google網', 'ratio': 0.1111111111111111}]","""GoogLeNet""","1. Rank: ""GoogLeNet"", ""谷歌网络"", ""谷歌网"", ""Google網""

2. Explanation: The first translation candidate, ""GoogLeNet"", is the best fit because it retains the original name of the neural network architecture, which is crucial in the AI domain. In technical contexts, especially in machine learning and deep learning, it is important to use the exact names of models to avoid confusion and ensure clarity. The second candidate, ""谷歌网络"" (Google network), is a reasonable translation that conveys the association with Google while maintaining the context of a network, but it does not preserve the specific branding of ""GoogLeNet."" The third candidate, ""谷歌网"" (Google.com), is less appropriate as it refers to a website rather than the neural network architecture. Lastly, ""Google網"" (Google.com) is a transliteration that does not provide any semantic clarity regarding the AI model itself. Therefore, the original term ""GoogLeNet"" is the most accurate and contextually appropriate choice.","['Google net', 'google.com', 'google network', 'Google.com']"
284,284,Gradient,梯度,1.0,9,"[{'word': '梯度', 'ratio': 1.0}]",梯度,{},[]
285,285,Gradient descent,梯度下降,0.875,8,"[{'word': '梯度下降', 'ratio': 0.875}, {'word': '梯度下降法', 'ratio': 0.125}]",梯度下降,{},[]
286,286,Gram matrix,格拉姆矩阵,0.5,8,"[{'word': '格拉姆矩阵', 'ratio': 0.5}, {'word': 'Gram 矩阵', 'ratio': 0.25}, {'word': '矩阵', 'ratio': 0.125}, {'word': 'Gram矩阵', 'ratio': 0.125}]",格拉姆矩阵,{},[]
287,287,Graph,图,1.0,8,"[{'word': '图', 'ratio': 1.0}]",图,{},[]
288,288,Graph Neural Networks,图神经网络,1.0,8,"[{'word': '图神经网络', 'ratio': 1.0}]",图神经网络,{},[]
289,289,Graph Transformer,图Transformer,0.0,8,"[{'word': '图转换器', 'ratio': 0.625}, {'word': '图变换器', 'ratio': 0.25}, {'word': '图 Transformer', 'ratio': 0.125}]",图转换器,{},[]
290,290,Greedy,贪婪,0.1666666666666666,6,"[{'word': '贪婪算法', 'ratio': 0.5}, {'word': '贪婪', 'ratio': 0.16666666666666666}, {'word': '贪心', 'ratio': 0.16666666666666666}, {'word': '贪心算法', 'ratio': 0.16666666666666666}]",贪婪算法,{},[]
291,291,Ground Truth,实地真实情况,0.0,6,"[{'word': '真实值', 'ratio': 0.6666666666666666}, {'word': '基准真实值', 'ratio': 0.16666666666666666}, {'word': '真实标签', 'ratio': 0.16666666666666666}]",真实值,{},[]
292,292,Gröbner basis,格罗布纳基底,0.0,6,"[{'word': '基础', 'ratio': 0.3333333333333333}, {'word': '格罗布纳基', 'ratio': 0.3333333333333333}, {'word': '格羅布納基地', 'ratio': 0.16666666666666666}, {'word': 'Gröbner 基', 'ratio': 0.16666666666666666}]","""格罗布纳基""","1. Rank: ""格罗布纳基"", ""Gröbner 基"", ""格羅布納基地"", ""基础""

2. Explanation: The term ""格罗布纳基"" is the best fit because it retains the original name ""Gröbner"" while providing a clear indication that it refers to a mathematical basis (基) in the context of algebraic geometry and computational algebra. This term is widely recognized in the AI and mathematics communities, making it semantically accurate and contextually appropriate. The other candidates either do not retain the original name or are less commonly used in the specific context of Gröbner bases, which are crucial in symbolic computation and algorithm design. ""Gröbner 基"" is also a strong candidate, but it is less commonly used than ""格罗布纳基"". ""格羅布納基地"" is a transliteration that may not be as familiar to practitioners, and ""基础"" (base) is too generic and lacks the specificity needed for this technical term.","['Base', 'Grobnaki', 'grobner base', 'Gröbner base']"
293,293,Gumbel,甘贝尔,0.1666666666666666,6,"[{'word': 'Gumbel', 'ratio': 0.3333333333333333}, {'word': '甘貝爾', 'ratio': 0.16666666666666666}, {'word': '甘贝尔分布', 'ratio': 0.16666666666666666}, {'word': '古贝尔', 'ratio': 0.16666666666666666}, {'word': '甘贝尔', 'ratio': 0.16666666666666666}]",甘贝尔分布,"1. Rank: 甘贝尔分布, 甘贝尔, 甘貝爾, 古贝尔, Gumbel

2. Explanation: The term ""甘贝尔分布"" (Gambel distribution) is the best fit because it accurately reflects the specific statistical distribution associated with the Gumbel name in the context of AI and probability theory. It retains the original name ""Gumbel"" while also indicating that it refers to a distribution, which is crucial for understanding its application in the given context. The second choice, ""甘贝尔,"" is also a good option as it retains the name but lacks the explicit reference to the distribution. The transliterations ""甘貝爾"" and ""古贝尔"" do not convey the statistical context as effectively, and ""Gumbel"" in English does not provide any semantic clarity in a Chinese context. Therefore, ""甘贝尔分布"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['gum be', 'Gambel', 'Gambel distribution', 'Gubel', 'Gambel']"
294,294,Gumbel distribution,甘贝尔分布,0.3333333333333333,6,"[{'word': 'Gumbel 分布', 'ratio': 0.3333333333333333}, {'word': '甘贝尔分布', 'ratio': 0.3333333333333333}, {'word': '甘貝爾分佈', 'ratio': 0.16666666666666666}, {'word': '古贝尔分布', 'ratio': 0.16666666666666666}]",Gumbel 分布,"1. Rank: Gumbel 分布, 甘贝尔分布, 甘貝爾分佈, 古贝尔分布

2. Explanation: The term ""Gumbel 分布"" is the best fit because it retains the original name ""Gumbel"" in its transliterated form, which is crucial in the AI domain where specific statistical distributions are often referred to by their names. This term is widely recognized in both English and Chinese academic literature, ensuring semantic accuracy and contextual fit. The other candidates, such as ""甘贝尔分布"" and ""甘貝爾分佈,"" while they attempt to transliterate the name, do not maintain the same level of recognition and may lead to confusion. ""古贝尔分布"" is less accurate as it deviates significantly from the original name, making it the least suitable choice. Therefore, ""Gumbel 分布"" is the most appropriate translation for its clarity and adherence to established terminology in the field.","['Gumbel distribution', 'Gambel distribution', 'Gambel distribution', 'Gubel distribution']"
295,295,Gumbel noise,古贝尔噪音,0.0,10,"[{'word': '甘布尔噪声', 'ratio': 0.4}, {'word': '甘博尔噪声', 'ratio': 0.2}, {'word': '古姆贝尔噪声', 'ratio': 0.2}, {'word': '冈贝尔噪声', 'ratio': 0.1}, {'word': '甘贝尔噪声', 'ratio': 0.1}]",甘布尔噪声,"1. Rank: 甘布尔噪声, 古姆贝尔噪声, 甘贝尔噪声, 冈贝尔噪声, 甘博尔噪声

2. Explanation: The term ""甘布尔噪声"" is the best fit because it accurately retains the phonetic structure of the original English term ""Gumbel noise"" while also being widely recognized in the AI and statistical communities. The back translation ""Gamble noise"" is incorrect, as it does not convey the intended meaning related to Gumbel distributions in statistics. The second choice, ""古姆贝尔噪声,"" is also a close phonetic match but is less commonly used in the context of AI. The other candidates either introduce inaccuracies in pronunciation or semantic meaning, making them less suitable for domain-specific usage.","['Gamble noise', 'Gumball noise', 'Gumbel noise', 'Gumbel noise', 'Gambel noise']"
296,296,Gumbel-softmax distribution,古贝尔-Softmax分布,0.0,10,"[{'word': '甘布尔-软最大分布', 'ratio': 0.3}, {'word': '甘博尔软最大分布', 'ratio': 0.2}, {'word': '古姆贝尔-softmax分布', 'ratio': 0.2}, {'word': '冈贝尔-软最大分布', 'ratio': 0.1}, {'word': '甘布尔软最大分布', 'ratio': 0.1}, {'word': '甘贝尔-softmax分布', 'ratio': 0.1}]","""古姆贝尔-softmax分布""","1. Rank: ""古姆贝尔-softmax分布"", ""冈贝尔-软最大分布"", ""甘布尔-软最大分布"", ""甘贝尔-softmax分布"", ""甘博尔软最大分布"", ""甘布尔软最大分布""

2. Explanation: The term ""古姆贝尔-softmax分布"" is the best fit because it accurately retains the original name ""Gumbel"" while also being semantically correct in the context of AI terminology. The back translation ""Gumbel-softmax distribution"" matches the original English term perfectly, ensuring clarity and consistency in the technical context. 

The other candidates, such as ""冈贝尔-软最大分布"" and ""甘布尔-软最大分布,"" also maintain a close semantic relationship but may introduce slight variations in pronunciation or meaning that could lead to confusion. The remaining candidates, particularly those that deviate significantly from the original name (like ""甘博尔软最大分布"" and ""甘布尔软最大分布""), do not preserve the integrity of the term as effectively, which is crucial in the AI domain where precise terminology is essential for understanding and communication.","['Gamble-softmax distribution', 'Gumball soft maximum distribution', 'Gumbel-softmax distribution', 'Gumbel-softmax distribution', 'Gamble soft maximum distribution', 'Gambel-softmax distribution']"
297,297,Haar wavelet,Haar小波,0.0,10,"[{'word': '哈尔小波', 'ratio': 0.9}, {'word': '哈希小波', 'ratio': 0.1}]",哈尔小波,{},[]
298,298,Hadamard matrix,哈达玛矩阵,0.8,10,"[{'word': '哈达玛矩阵', 'ratio': 0.8}, {'word': '阿达马矩阵', 'ratio': 0.1}, {'word': '阿达玛矩阵', 'ratio': 0.1}]",哈达玛矩阵,{},[]
299,299,Hadamard product,哈达玛乘积,0.0,10,"[{'word': '哈达玛积', 'ratio': 0.8}, {'word': '阿达马积', 'ratio': 0.1}, {'word': '阿达玛积', 'ratio': 0.1}]",哈达玛积,{},[]
300,300,Hamiltonian Monte Carlo,哈密顿蒙特卡罗,0.5,10,"[{'word': '哈密顿蒙特卡罗', 'ratio': 0.5}, {'word': '哈密顿蒙特卡洛', 'ratio': 0.3}, {'word': '哈密顿蒙特卡洛方法', 'ratio': 0.1}, {'word': '哈密顿蒙特卡罗方法', 'ratio': 0.1}]",哈密顿蒙特卡罗,{},[]
301,301,Hamming distance,海明距离,0.0,10,"[{'word': '汉明距离', 'ratio': 1.0}]",汉明距离,{},[]
302,302,Hamming loss,海明损失,0.0,10,"[{'word': '汉明损失', 'ratio': 1.0}]",汉明损失,{},[]
303,303,Hankel matrix,汉克尔矩阵,1.0,10,"[{'word': '汉克尔矩阵', 'ratio': 1.0}]",汉克尔矩阵,{},[]
304,304,Hausdorff distance,豪斯多夫距离,1.0,10,"[{'word': '豪斯多夫距离', 'ratio': 1.0}]",豪斯多夫距离,{},[]
305,305,Hedge algorithm,对冲算法,0.8,10,"[{'word': '对冲算法', 'ratio': 0.8}, {'word': 'Hedge算法', 'ratio': 0.1}, {'word': '算法', 'ratio': 0.1}]",对冲算法,{},[]
306,306,Hellinger distance,赫林格距离,0.1,10,"[{'word': 'Hellinger 距离', 'ratio': 0.3}, {'word': 'Hellinger距离', 'ratio': 0.2}, {'word': '海林距离', 'ratio': 0.1}, {'word': '赫林格距离', 'ratio': 0.1}, {'word': '海林杰距离', 'ratio': 0.1}, {'word': '赫尔丁距离', 'ratio': 0.1}, {'word': '海林格距离', 'ratio': 0.1}]",Hellinger 距离,"1. Rank: Hellinger 距离, Hellinger距离, 海林距离, 赫林格距离, 海林杰距离, 赫尔丁距离, 海林格距离

2. Explanation: The top choice, ""Hellinger 距离,"" is the best fit because it retains the original name ""Hellinger,"" which is crucial in the AI domain as it refers to a specific mathematical concept named after the mathematician Ernst Hellinger. This term is semantically accurate and maintains the integrity of the original term, ensuring that it is recognized in academic and professional contexts. The use of ""距离"" (distance) is appropriate as it directly translates the concept of distance in the context of probability distributions. 

The second option, ""Hellinger距离,"" is also a good fit but lacks the space that is often used in formal writing to separate the name from the term, making it slightly less formal. 

The other candidates, such as ""海林距离"" and ""赫林格距离,"" introduce transliterations that do not accurately reflect the original term's recognition in the AI field, which could lead to confusion or misinterpretation. They also deviate from the established terminology used in literature, which is critical for maintaining clarity in technical discussions.","['Hellinger distance', 'Hellinger distance', 'Hailin distance', 'Heringer distance', 'Hellinger distance', 'Herding distance', 'Hellinger distance']"
307,307,Helmholtz machine,赫尔姆霍兹机,0.1,10,"[{'word': '亥姆霍兹机', 'ratio': 0.5}, {'word': 'Helmholtz机器', 'ratio': 0.1}, {'word': 'Helmholtz 机器', 'ratio': 0.1}, {'word': '亥姆霍兹机器', 'ratio': 0.1}, {'word': '赫尔姆霍兹机', 'ratio': 0.1}, {'word': '赫姆霍兹机', 'ratio': 0.1}]",亥姆霍兹机,{},[]
308,308,Hessian matrix,黑塞矩阵,0.1,10,"[{'word': '海森矩阵', 'ratio': 0.3}, {'word': 'Hessian矩阵', 'ratio': 0.3}, {'word': 'Hessian 矩阵', 'ratio': 0.3}, {'word': '黑塞矩阵', 'ratio': 0.1}]",海森矩阵,"1. Rank: 海森矩阵, Hessian矩阵, Hessian 矩阵, 黑塞矩阵

2. Explanation: The term ""海森矩阵"" is the most widely accepted and semantically accurate translation of ""Hessian matrix"" in the context of AI and mathematics. It retains the original meaning and is commonly used in academic literature, making it the best fit for contextual usage. The back translation accurately reflects the English term, ensuring clarity and consistency in communication. 

The other candidates, such as ""Hessian矩阵"" and ""Hessian 矩阵,"" are less preferable because they mix English and Chinese, which can lead to confusion in a purely Chinese context. ""黑塞矩阵"" is a less common alternative that may not be recognized by all readers in the AI field, making it a weaker choice. Therefore, ""海森矩阵"" stands out as the most appropriate term for conveying the concept of the Hessian matrix in the AI domain.","['Hessian matrix', 'Hessian matrix', 'Hessian matrix', 'Hessian matrix']"
309,309,Hessian-vector product,黑塞矩阵-向量乘积,0.0,8,"[{'word': '海森向量积', 'ratio': 0.5}, {'word': '海森矩阵-向量积', 'ratio': 0.25}, {'word': 'Hessian- 贡献积', 'ratio': 0.125}, {'word': '黑森州-向量积', 'ratio': 0.125}]",海森向量积,{},[]
310,310,Hidden Markov Model,隐马尔可夫模型,1.0,8,"[{'word': '隐马尔可夫模型', 'ratio': 1.0}]",隐马尔可夫模型,{},[]
311,311,Hiero system,Hiero系统,0.625,8,"[{'word': 'Hiero系统', 'ratio': 0.625}, {'word': '希罗系统', 'ratio': 0.125}, {'word': '寺廟系统', 'ratio': 0.125}, {'word': 'Hiero 系统', 'ratio': 0.125}]",Hiero系统,{},[]
312,312,Hodge decomposition,霍奇分解,0.875,8,"[{'word': '霍奇分解', 'ratio': 0.875}, {'word': '霍奇分割', 'ratio': 0.125}]",霍奇分解,{},[]
313,313,Hoeffding's inequality,霍夫丁不等式,1.0,8,"[{'word': '霍夫丁不等式', 'ratio': 1.0}]",霍夫丁不等式,{},[]
314,314,Homography,单应性,1.0,9,"[{'word': '单应性', 'ratio': 1.0}]",单应性,{},[]
315,315,Horn theory,号角理论,0.0,9,"[{'word': '霍恩理论', 'ratio': 1.0}]",霍恩理论,{},[]
316,316,HowTo100M,HowTo100M,0.5555555555555556,9,"[{'word': 'HowTo100M', 'ratio': 0.5555555555555556}, {'word': '数据集', 'ratio': 0.2222222222222222}, {'word': '如何100M', 'ratio': 0.1111111111111111}, {'word': '这是一个数据集名称，一般不翻译', 'ratio': 0.1111111111111111}]",HowTo100M,{},[]
317,317,Huber loss,哈伯损失,0.0,9,"[{'word': '胡贝尔损失', 'ratio': 0.3333333333333333}, {'word': '胡伯损失', 'ratio': 0.3333333333333333}, {'word': 'Huber 损失', 'ratio': 0.2222222222222222}, {'word': '霍伯损失', 'ratio': 0.1111111111111111}]",胡贝尔损失,"1. Rank: 胡贝尔损失, Huber 损失, 胡伯损失, 霍伯损失

2. Explanation: The first translation, ""胡贝尔损失,"" is the best fit because it accurately retains the original name ""Huber"" while providing a clear and recognizable term for ""loss"" in the context of machine learning. The back translation ""huber loss"" aligns perfectly with the English term, ensuring semantic accuracy. This term is also widely accepted in the AI community, making it contextually appropriate. The second option, ""Huber 损失,"" while also correct, does not fully integrate the Chinese language structure as effectively as ""胡贝尔损失."" The third option, ""胡伯损失,"" is a less common transliteration and may not be as recognizable in the AI domain. Lastly, ""霍伯损失"" introduces a different name (""Hooper"") that does not correspond to the original term, making it the least suitable choice.","['huber loss', 'huber loss', 'Huber loss', 'Hooper loss']"
318,318,Huber norm,胡贝尔范数,0.3333333333333333,9,"[{'word': '胡贝尔范数', 'ratio': 0.3333333333333333}, {'word': '胡伯范数', 'ratio': 0.3333333333333333}, {'word': 'Huber 范数', 'ratio': 0.2222222222222222}, {'word': '霍伯范数', 'ratio': 0.1111111111111111}]",胡贝尔范数,"1. Rank: 胡贝尔范数, 胡伯范数, Huber 范数, 霍伯范数

2. Explanation: The term ""胡贝尔范数"" is the best fit because it accurately retains the original name ""Huber"" while providing a clear and recognizable translation for the term ""norm"" in the context of mathematics and statistics. The use of ""胡贝尔"" is a well-established transliteration of ""Huber"" in academic literature, making it semantically accurate and contextually appropriate for the AI domain. 

The second candidate, ""胡伯范数,"" is also a reasonable choice, but it is less commonly used than ""胡贝尔"" and may not be as immediately recognizable to those familiar with the term. 

The third candidate, ""Huber 范数,"" while retaining the original English term, does not provide a fully localized version, which may not be as effective in a Chinese-speaking academic context. 

Lastly, ""霍伯范数"" is a poor choice as it misrepresents the name ""Huber"" and could lead to confusion, as ""Hopper"" is an entirely different term. Thus, ""胡贝尔范数"" stands out as the most accurate and contextually fitting translation.","['Huber norm', 'Huber norm', 'Huber norm', 'Hopper norm']"
319,319,Hungarian loss,匈牙利损失,1.0,9,"[{'word': '匈牙利损失', 'ratio': 1.0}]",匈牙利损失,{},[]
320,320,Hyper-parameter,超参数,1.0,9,"[{'word': '超参数', 'ratio': 1.0}]",超参数,{},[]
321,321,Hyperband,超带,0.4444444444444444,9,"[{'word': 'Hyperband', 'ratio': 0.4444444444444444}, {'word': '超带', 'ratio': 0.4444444444444444}, {'word': 'Hyperband算法', 'ratio': 0.1111111111111111}]","""Hyperband算法""","1. Rank: ""Hyperband算法"", ""Hyperband"", ""超带""

2. Explanation: The term ""Hyperband算法"" is the best fit because it retains the original English term ""Hyperband"" while adding the word ""算法"" (algorithm), which is crucial in the context of AI and machine learning. This makes it clear that it refers to a specific algorithmic approach, which is essential for understanding its application in hyperparameter tuning. The back translation ""Hyperband algorithm"" accurately reflects the original term and its intended meaning in the AI domain. 

The second candidate, ""Hyperband"", is also a valid option but lacks the explicit mention of ""algorithm,"" which is important for clarity in a technical context. The third candidate, ""超带"" (super band), is not appropriate as it does not convey the specific meaning of the term in the AI context and could lead to confusion. Therefore, ""Hyperband算法"" is the most semantically accurate and contextually appropriate choice.","['hyper band', 'super band', 'Hyperband algorithm']"
322,322,Hyperparameter search,超参数搜索,1.0,9,"[{'word': '超参数搜索', 'ratio': 1.0}]",超参数搜索,{},[]
323,323,Imitation Learning,模仿学习,1.0,9,"[{'word': '模仿学习', 'ratio': 1.0}]",模仿学习,{},[]
324,324,In-context Learning,上下文学习,1.0,9,"[{'word': '上下文学习', 'ratio': 1.0}]",上下文学习,{},[]
325,325,Inception network,Inception 网络,0.3333333333333333,9,"[{'word': 'Inception网络', 'ratio': 0.5555555555555556}, {'word': 'Inception 网络', 'ratio': 0.3333333333333333}, {'word': 'inception 网络', 'ratio': 0.1111111111111111}]",Inception网络,{},[]
326,326,Independent Cascade,独立级联模型,0.5555555555555556,9,"[{'word': '独立级联模型', 'ratio': 0.5555555555555556}, {'word': '独立级联', 'ratio': 0.4444444444444444}]",独立级联模型,{},[]
327,327,Independent Cascade Model,独立级联模型,1.0,10,"[{'word': '独立级联模型', 'ratio': 1.0}]",独立级联模型,{},[]
328,328,Independent Component Analysis,独立成分分析,1.0,10,"[{'word': '独立成分分析', 'ratio': 1.0}]",独立成分分析,{},[]
329,329,Influence Maximization,影响力最大化,0.8,10,"[{'word': '影响力最大化', 'ratio': 0.8}, {'word': '影响最大化', 'ratio': 0.2}]",影响力最大化,{},[]
330,330,Information Extraction,信息提取,0.7,10,"[{'word': '信息提取', 'ratio': 0.7}, {'word': '信息抽取', 'ratio': 0.3}]",信息提取,{},[]
331,331,Information Retrieval,信息检索,1.0,10,"[{'word': '信息检索', 'ratio': 1.0}]",信息检索,{},[]
332,332,Informer model,信息提供者模型,0.0,10,"[{'word': 'Informer模型', 'ratio': 0.6}, {'word': '信息模型', 'ratio': 0.2}, {'word': '模型', 'ratio': 0.1}, {'word': '情報提供者モデル', 'ratio': 0.1}]",Informer模型,{},[]
333,333,Input space,输入空间,1.0,10,"[{'word': '输入空间', 'ratio': 1.0}]",输入空间,{},[]
334,334,Inside-Outside algorithm,内外算法,1.0,10,"[{'word': '内外算法', 'ratio': 1.0}]",内外算法,{},[]
335,335,Instance Normalization,实例归一化,1.0,10,"[{'word': '实例归一化', 'ratio': 1.0}]",实例归一化,{},[]
336,336,Inverse Reinforcement Learning,逆向强化学习,0.2,10,"[{'word': '逆强化学习', 'ratio': 0.8}, {'word': '逆向强化学习', 'ratio': 0.2}]",逆强化学习,{},[]
337,337,Ising model,伊辛模型,1.0,10,"[{'word': '伊辛模型', 'ratio': 1.0}]",伊辛模型,{},[]
338,338,Iverson bracket,伊弗森括弧,0.0,10,"[{'word': '艾弗森括号', 'ratio': 0.9}, {'word': '伊弗森括号', 'ratio': 0.1}]",艾弗森括号,{},[]
339,339,Jaccard,Jaccard,0.0,10,"[{'word': '杰卡德', 'ratio': 0.7}, {'word': '雅卡尔指数', 'ratio': 0.1}, {'word': '贾卡德相似度', 'ratio': 0.1}, {'word': '贾卡德指数', 'ratio': 0.1}]",杰卡德,{},[]
340,340,Jaccard index,杰卡德指数,0.7,10,"[{'word': '杰卡德指数', 'ratio': 0.7}, {'word': '贾卡德指数', 'ratio': 0.2}, {'word': '雅卡尔指数', 'ratio': 0.1}]",杰卡德指数,{},[]
341,341,Jaccard similarity,杰卡德相似性,0.1,10,"[{'word': '杰卡德相似度', 'ratio': 0.6}, {'word': '贾卡德相似度', 'ratio': 0.2}, {'word': '雅卡尔相似度', 'ratio': 0.1}, {'word': '杰卡德相似性', 'ratio': 0.1}]",杰卡德相似度,{},[]
342,342,Jaccard similarity coefficient,杰卡德相似系数,0.6666666666666666,9,"[{'word': '杰卡德相似系数', 'ratio': 0.6666666666666666}, {'word': 'Jaccard 相似系数', 'ratio': 0.2222222222222222}, {'word': 'Jaccard相似系数', 'ratio': 0.1111111111111111}]",杰卡德相似系数,{},[]
343,343,Jacobian matrix,雅可比矩阵,1.0,9,"[{'word': '雅可比矩阵', 'ratio': 1.0}]",雅可比矩阵,{},[]
344,344,Jensen's inequality,詹森不等式,0.8888888888888888,9,"[{'word': '詹森不等式', 'ratio': 0.8888888888888888}, {'word': '延森不等式', 'ratio': 0.1111111111111111}]",詹森不等式,{},[]
345,345,Jensen-Shannon,詹森-香农,0.6666666666666666,9,"[{'word': '詹森-香农', 'ratio': 0.6666666666666666}, {'word': '詹森-香农表示法', 'ratio': 0.1111111111111111}, {'word': '詹森-香农  代表性', 'ratio': 0.1111111111111111}, {'word': 'Jensen-Shannon', 'ratio': 0.1111111111111111}]",詹森-香农,{},[]
346,346,Jensen-Shannon Divergence,詹森-香农散度,0.8888888888888888,9,"[{'word': '詹森-香农散度', 'ratio': 0.8888888888888888}, {'word': 'Jensen-Shannon散度', 'ratio': 0.1111111111111111}]",詹森-香农散度,{},[]
347,347,K-L divergence,K-L 散度,0.3,10,"[{'word': 'K-L散度', 'ratio': 0.5}, {'word': 'K-L 散度', 'ratio': 0.3}, {'word': 'KL散度', 'ratio': 0.1}, {'word': '散度', 'ratio': 0.1}]",K-L散度,{},[]
348,348,K-Means,k-均值聚类法,0.0,10,"[{'word': 'K均值', 'ratio': 0.7}, {'word': 'K-均值', 'ratio': 0.2}, {'word': '均值', 'ratio': 0.1}]",K均值,{},[]
349,349,K-Means clustering,K均值聚类,0.7,10,"[{'word': 'K均值聚类', 'ratio': 0.7}, {'word': 'K-均值聚类', 'ratio': 0.2}, {'word': '均值聚类', 'ratio': 0.1}]",K均值聚类,{},[]
350,350,K-means algorithm,K均值算法,0.7,10,"[{'word': 'K均值算法', 'ratio': 0.7}, {'word': 'K-均值算法', 'ratio': 0.2}, {'word': '均值算法', 'ratio': 0.1}]",K均值算法,{},[]
351,351,Kalman filter,卡尔曼滤波器,1.0,10,"[{'word': '卡尔曼滤波器', 'ratio': 1.0}]",卡尔曼滤波器,{},[]
352,352,Kendall's τ,肯德尔秩相关系数τ,0.0,9,"[{'word': 'Kendall的τ', 'ratio': 0.2222222222222222}, {'word': '肯德尔τ系数', 'ratio': 0.2222222222222222}, {'word': ""Kendall's τ 相关系数"", 'ratio': 0.1111111111111111}, {'word': '肯德尔τ', 'ratio': 0.1111111111111111}, {'word': 'Kendall的τ系数', 'ratio': 0.1111111111111111}, {'word': '肯德尔秩相关系数', 'ratio': 0.1111111111111111}, {'word': '肯德尔 τ', 'ratio': 0.1111111111111111}]","""肯德尔τ系数""","1. Rank: ""肯德尔τ系数"", ""Kendall's τ 相关系数"", ""肯德尔秩相关系数"", ""Kendall的τ系数"", ""Kendall的τ"", ""肯德尔τ"", ""Kendall τ""

2. Explanation: The term ""肯德尔τ系数"" (Kendall's τ coefficient) is the best fit because it accurately conveys both the name of the statistician (Kendall) and the specific nature of the metric (coefficient), which is essential in the context of statistical analysis. The inclusion of ""系数"" (coefficient) clarifies that this term refers to a quantitative measure, which is crucial in the AI domain where precision in terminology is important. 

The back translation ""Kendall τ coefficient"" maintains the integrity of the original term, ensuring that it is understood as a specific statistical measure. Other candidates like ""肯德尔秩相关系数"" (Kendall rank correlation coefficient) are also accurate but may be less commonly used in the context of AI literature, where the simpler ""系数"" is often preferred. 

Overall, ""肯德尔τ系数"" strikes a balance between semantic accuracy and contextual fit, making it the most appropriate choice for use in AI-related discussions.","['Kendall τ', 'Kendall τ coefficient', ""Kendall's τ correlation coefficient"", 'Kendall τ', 'Kendall’s τ coefficient', 'Kendall rank correlation coefficient', 'Kendall τ']"
353,353,Keras,Keras,0.7777777777777778,9,"[{'word': 'Keras', 'ratio': 0.7777777777777778}, {'word': '凯拉斯', 'ratio': 0.1111111111111111}, {'word': '深度学习框架', 'ratio': 0.1111111111111111}]",Keras,{},[]
354,354,Kernel,核函数,0.6666666666666666,9,"[{'word': '核函数', 'ratio': 0.6666666666666666}, {'word': '核', 'ratio': 0.3333333333333333}]",核函数,{},[]
355,355,Kernel function,核函数,1.0,9,"[{'word': '核函数', 'ratio': 1.0}]",核函数,{},[]
356,356,Kleene closure,Kleene闭包,0.1,10,"[{'word': '克林闭包', 'ratio': 0.4}, {'word': '克里尼闭包', 'ratio': 0.2}, {'word': 'Kleene闭包', 'ratio': 0.1}, {'word': '克莱尼闭包', 'ratio': 0.1}, {'word': '克内泽-内伊平滑', 'ratio': 0.1}, {'word': '克里宁闭包', 'ratio': 0.1}]",克林闭包,"1. Rank: 克林闭包, Kleene闭包, 克里尼闭包, 克莱尼闭包, 克里宁闭包, 克内泽-内伊平滑

2. Explanation: The term ""克林闭包"" (Kleene closure) is the best fit because it accurately retains the original name ""Kleene,"" which is essential in the context of formal language theory and automata. This term is widely recognized in the AI and computer science communities, making it semantically accurate and contextually appropriate. The back translation also aligns perfectly with the English term, ensuring clarity and consistency in communication. Other candidates, such as ""克里尼闭包"" and ""克莱尼闭包,"" deviate from the standard transliteration of ""Kleene,"" which could lead to confusion or misinterpretation in academic and professional settings. The remaining terms either misrepresent the original term or are unrelated to the concept of closure in formal languages.","['Kleene closure', 'Crini closure', 'Kleene closure', 'Kleinian closure', 'Kneize-Ney smoothing', 'Krynin closure']"
357,357,Kneser-Ney smoothing,Kneser-Ney平滑,0.3,10,"[{'word': '克内泽-内伊平滑', 'ratio': 0.4}, {'word': 'Kneser-Ney平滑', 'ratio': 0.3}, {'word': '克内泽-奈平滑', 'ratio': 0.1}, {'word': 'Kneser-Ney 平滑', 'ratio': 0.1}, {'word': '克内泽-内平滑', 'ratio': 0.1}]",Kneser-Ney平滑,"1. Rank: Kneser-Ney平滑, Kneser-Ney 平滑, 克内泽-内伊平滑, 克内泽-奈平滑, 克内泽-内平滑

2. Explanation: The term ""Kneser-Ney平滑"" is the best fit because it retains the original name ""Kneser-Ney"" in its entirety, which is crucial in the AI domain where specific algorithms and techniques are often referred to by their original names. This term also uses the Chinese word ""平滑"" (smoothing), which accurately conveys the concept of smoothing in the context of statistical language modeling. The second option, ""Kneser-Ney 平滑,"" is also a strong candidate but includes a space that is less common in technical terminology. The other candidates either use incorrect transliterations or do not maintain the integrity of the original name, which is essential for clarity and recognition in the AI field.","['Kneize-Ney smoothing', 'Kneser-Ney smoothing', 'Kneize-Nesmoothing', 'Kneser-Ney Smooth', 'Kneize-internal smoothing']"
358,358,Knowledge Base,知识库,1.0,10,"[{'word': '知识库', 'ratio': 1.0}]",知识库,{},[]
359,359,Knowledge Distillation,知识蒸馏,1.0,10,"[{'word': '知识蒸馏', 'ratio': 1.0}]",知识蒸馏,{},[]
360,360,Knowledge Graph,知识图谱,1.0,10,"[{'word': '知识图谱', 'ratio': 1.0}]",知识图谱,{},[]
361,361,Kolmogorov-Smirnov test,科尔莫哥洛夫-斯米尔诺夫检验,0.2,10,"[{'word': '科尔莫戈罗夫-斯米尔诺夫检验', 'ratio': 0.3}, {'word': '科尔莫哥洛夫-斯米尔诺夫检验', 'ratio': 0.2}, {'word': 'Kolmogorov-Smirnov检验', 'ratio': 0.2}, {'word': '科尔莫戈洛夫-斯米尔诺夫检验', 'ratio': 0.1}, {'word': '科尔莫哥罗夫-斯米尔诺夫检验', 'ratio': 0.1}, {'word': '科尔莫哥洛夫-史米尔诺夫检验', 'ratio': 0.1}]",科尔莫戈罗夫-斯米尔诺夫检验,"1. Rank: 科尔莫戈罗夫-斯米尔诺夫检验, 科尔莫哥洛夫-斯米尔诺夫检验, Kolmogorov-Smirnov检验, 科尔莫戈洛夫-斯米尔诺夫检验, 科尔莫哥罗夫-斯米尔诺夫检验, 科尔莫哥洛夫-史米尔诺夫检验

2. Explanation: The term ""科尔莫戈罗夫-斯米尔诺夫检验"" is the best fit because it accurately retains the original names of the mathematicians Kolmogorov and Smirnov while following the standard conventions for transliterating foreign names into Chinese. This version is widely recognized in the statistical community and is semantically accurate, ensuring that readers familiar with the AI and statistical domains will understand it correctly. The other candidates either use less common transliterations or variations that could lead to confusion, making them less suitable for academic and professional contexts.","['Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test']"
362,362,Krippendorff's α,克里朋多夫阿尔法系数,0.0,10,"[{'word': '克里彭多夫α系数', 'ratio': 0.3}, {'word': '克里彭多夫α', 'ratio': 0.2}, {'word': '克里彭多夫 α 系数', 'ratio': 0.2}, {'word': '克里彭多夫 α', 'ratio': 0.1}, {'word': 'Krippendorff的α', 'ratio': 0.1}, {'word': '克里朋多夫 α', 'ratio': 0.1}]","""克里彭多夫α系数""","1. Rank: ""克里彭多夫α系数"", ""克里彭多夫α"", ""克里彭多夫 α 系数"", ""克里彭多夫 α"", ""Krippendorff的α"", ""克里朋多夫 α""

2. Explanation: The first translation, ""克里彭多夫α系数"", is the best fit because it accurately retains the full name of the original term along with the specific statistical context by including ""系数"" (coefficient), which is essential in the AI domain for clarity and precision. This term clearly indicates that it refers to a statistical measure, which is crucial for understanding its application in evaluating agreement scores. The back translation ""Krippendorff alpha coefficient"" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. The other candidates, while they may retain the name ""Krippendorff,"" either omit the term ""系数"" or do not provide a clear indication of the statistical nature of the term, which could lead to ambiguity in an academic or professional setting.","['Krippendorff alpha coefficient', 'Krippendorf α', 'Krippendorff alpha coefficient', 'Krippendorf α', 'Krippendorff α', 'Krippendorf α']"
363,363,Kronecker delta,克罗内克δ,0.3,10,"[{'word': '克罗内克 δ', 'ratio': 0.3}, {'word': '克罗内克δ', 'ratio': 0.3}, {'word': '克罗内克尔δ', 'ratio': 0.1}, {'word': '克罗内克δ函数', 'ratio': 0.1}, {'word': '克里彭多夫α系数', 'ratio': 0.1}, {'word': '克罗内克函数', 'ratio': 0.1}]",克罗内克 δ,"1. Rank: 克罗内克 δ, 克罗内克δ, 克罗内克尔δ, 克罗内克δ函数, 克罗内克函数, 克里彭多夫α系数

2. Explanation: The first translation candidate, ""克罗内克 δ"", is the best fit because it accurately retains the original name ""Kronecker"" while also including the delta symbol (δ), which is crucial for understanding its mathematical context. This term is semantically accurate and aligns well with the established terminology in the AI and mathematical domains. The use of ""克罗内克"" is a widely accepted transliteration of ""Kronecker"" in Chinese literature, ensuring that readers familiar with the term will recognize it immediately. 

The second candidate, ""克罗内克δ"", is also a good option but lacks a space between ""克罗内克"" and ""δ"", which may affect readability. The third candidate, ""克罗内克尔δ"", introduces an unnecessary suffix that is not commonly used in this context. The fourth candidate, ""克罗内克δ函数"", while accurate, adds ""函数"" (function) which is not needed since ""Kronecker delta"" is a specific term that does not require this additional descriptor. The fifth candidate, ""克罗内克函数"", similarly misrepresents the term by omitting the delta aspect. Lastly, ""克里彭多夫α系数"" is irrelevant as it refers to a different concept entirely (Krippendorff's alpha coefficient) and should not be included in this ranking.","['Kronecker delta', 'Kronecker delta', 'Kronecker delta', 'Kronecker delta function', 'Krippendorff alpha coefficient', 'Kronecker function']"
364,364,Kronecker product,克罗内克积,0.8,10,"[{'word': '克罗内克积', 'ratio': 0.8}, {'word': '克罗内克尔积', 'ratio': 0.1}, {'word': '克罗内克乘积', 'ratio': 0.1}]",克罗内克积,{},[]
365,365,Kullback Leibler divergence,库尔贝克·莱布勒散度,0.0,10,"[{'word': '库尔贝克-莱布勒散度', 'ratio': 0.6}, {'word': 'Kullback-Leibler散度', 'ratio': 0.2}, {'word': '库尔巴克-莱布勒散度', 'ratio': 0.2}]",库尔贝克-莱布勒散度,{},[]
366,366,L 1 -norm,L1范数,0.375,8,"[{'word': 'L₁范数', 'ratio': 0.5}, {'word': 'L1范数', 'ratio': 0.375}, {'word': 'L1 范数', 'ratio': 0.125}]",L₁范数,{},[]
367,367,L 1 distance,L1距离,0.375,8,"[{'word': 'L₁距离', 'ratio': 0.5}, {'word': 'L1距离', 'ratio': 0.375}, {'word': 'L1 距离', 'ratio': 0.125}]",L₁距离,{},[]
368,368,L 2 -norm,L2范数,0.375,8,"[{'word': 'L₂范数', 'ratio': 0.5}, {'word': 'L2范数', 'ratio': 0.375}, {'word': 'L2 范数', 'ratio': 0.125}]",L₂范数,{},[]
369,369,L 2 distance,L2距离,0.375,8,"[{'word': 'L₂距离', 'ratio': 0.5}, {'word': 'L2距离', 'ratio': 0.375}, {'word': 'L2 距离', 'ratio': 0.125}]",L₂距离,{},[]
370,370,L 2 loss,L2损失,0.375,8,"[{'word': 'L₂损失', 'ratio': 0.5}, {'word': 'L2损失', 'ratio': 0.375}, {'word': 'L2 损失', 'ratio': 0.125}]",L₂损失,{},[]
371,371,L 2 regularization,L 2 正则化,0.0,10,"[{'word': 'L2正则化', 'ratio': 0.8}, {'word': 'L2 正则化', 'ratio': 0.1}, {'word': 'L₂ 正则化', 'ratio': 0.1}]",L2正则化,{},[]
372,372,L ∞ norm,L∞范数,0.8,10,"[{'word': 'L∞范数', 'ratio': 0.8}, {'word': 'L∞ 范数', 'ratio': 0.2}]",L∞范数,{},[]
373,373,L1 bound,l1约束,0.0,10,"[{'word': 'L1界', 'ratio': 0.7}, {'word': 'L1 界', 'ratio': 0.1}, {'word': 'L1界限', 'ratio': 0.1}, {'word': 'L₁ 界限', 'ratio': 0.1}]",L1界,{},[]
374,374,L1 difference,L1差异,0.8,10,"[{'word': 'L1差异', 'ratio': 0.8}, {'word': 'L1 差异', 'ratio': 0.1}, {'word': 'L₁ 差异', 'ratio': 0.1}]",L1差异,{},[]
375,375,L1 loss,L1损失,0.8,10,"[{'word': 'L1损失', 'ratio': 0.8}, {'word': 'L1 损失', 'ratio': 0.1}, {'word': 'L₁ 损失', 'ratio': 0.1}]",L1损失,{},[]
376,376,L1 penalty,L1 惩罚,0.1111111111111111,9,"[{'word': 'L1惩罚', 'ratio': 0.6666666666666666}, {'word': 'L1 惩罚项', 'ratio': 0.1111111111111111}, {'word': 'L1 惩罚', 'ratio': 0.1111111111111111}, {'word': 'L1惩罚项', 'ratio': 0.1111111111111111}]",L1惩罚,{},[]
377,377,L1 regularization,L1 正则化,0.2222222222222222,9,"[{'word': 'L1正则化', 'ratio': 0.7777777777777778}, {'word': 'L1 正则化', 'ratio': 0.2222222222222222}]",L1正则化,{},[]
378,378,L1 term,L1项,0.7777777777777778,9,"[{'word': 'L1项', 'ratio': 0.7777777777777778}, {'word': 'L1 项', 'ratio': 0.2222222222222222}]",L1项,{},[]
379,379,L2 error,L2误差,0.7777777777777778,9,"[{'word': 'L2误差', 'ratio': 0.7777777777777778}, {'word': 'L2 误差', 'ratio': 0.2222222222222222}]",L2误差,{},[]
380,380,L2 regularisation,L2 正则化,0.2222222222222222,9,"[{'word': 'L2正则化', 'ratio': 0.7777777777777778}, {'word': 'L2 正则化', 'ratio': 0.2222222222222222}]",L2正则化,{},[]
381,381,L2 regularizer,L2 正则化器,0.4444444444444444,9,"[{'word': 'L2 正则化器', 'ratio': 0.4444444444444444}, {'word': 'L2正则化器', 'ratio': 0.4444444444444444}, {'word': '2正则化器', 'ratio': 0.1111111111111111}]",L2 正则化器,"1. Rank: L2 正则化器, L2正则化器, 2正则化器

2. Explanation: The first translation, ""L2 正则化器,"" is the best fit because it maintains the correct semantic structure and accurately reflects the term ""L2 regularizer"" in the context of machine learning and statistics. The inclusion of a space between ""L2"" and ""正则化器"" enhances readability and aligns with common practices in technical writing, making it clearer for readers. The second candidate, ""L2正则化器,"" while also accurate, lacks the space, which can make it slightly less readable. The third candidate, ""2正则化器,"" is not a suitable translation as it omits the ""L"" and does not convey the intended meaning of the term, leading to confusion. Therefore, the first option is the most contextually appropriate and semantically accurate for the AI domain.","['L2 regularizer', 'L2 regularizer', '2 Regularizer']"
382,382,L2 weight decay,L2权重衰减,0.5555555555555556,9,"[{'word': 'L2权重衰减', 'ratio': 0.5555555555555556}, {'word': 'L2 权重衰减', 'ratio': 0.4444444444444444}]",L2权重衰减,{},[]
383,383,L2-normalization,L2-归一化,0.0,9,"[{'word': 'L2归一化', 'ratio': 0.4444444444444444}, {'word': 'L2 归一化', 'ratio': 0.3333333333333333}, {'word': 'L2 正则化', 'ratio': 0.1111111111111111}, {'word': 'L2规范化', 'ratio': 0.1111111111111111}]",L2归一化,"1. Rank: L2归一化, L2 归一化, L2规范化, L2 正则化

2. Explanation: The term ""L2归一化"" is the best fit because it accurately captures the concept of ""normalization"" in the context of AI and machine learning. The term ""归一化"" specifically refers to the process of normalizing data, which aligns well with the intended meaning of L2-normalization in the context of the provided text. The back translation ""L2 normalization"" confirms its semantic accuracy. 

The alternative ""L2 归一化"" is also a strong candidate, but the space between ""L2"" and ""归一化"" is less common in technical terminology, making ""L2归一化"" slightly more preferable. 

""L2规范化"" translates to ""L2 normalization"" as well, but ""规范化"" can imply a broader sense of standardization, which may not be as precise as ""归一化"" in this specific context. 

Lastly, ""L2 正则化"" translates to ""L2 regularization,"" which is a different concept in machine learning, primarily related to preventing overfitting rather than normalization. Therefore, it is not suitable for this context.","['L2 normalization', 'L2 normalization', 'L2 regularization', 'L2 normalization']"
384,384,Lagrange multiplier,拉格朗日乘数,0.4444444444444444,9,"[{'word': '拉格朗日乘子', 'ratio': 0.5555555555555556}, {'word': '拉格朗日乘数', 'ratio': 0.4444444444444444}]",拉格朗日乘子,{},[]
385,385,Lagrangian duality,拉格朗日对偶,0.1111111111111111,9,"[{'word': '拉格朗日对偶性', 'ratio': 0.8888888888888888}, {'word': '拉格朗日对偶', 'ratio': 0.1111111111111111}]",拉格朗日对偶性,{},[]
386,386,Lagrangian multiplier,拉格朗日乘数,0.1111111111111111,9,"[{'word': '拉格朗日乘子', 'ratio': 0.8888888888888888}, {'word': '拉格朗日乘数', 'ratio': 0.1111111111111111}]",拉格朗日乘子,{},[]
387,387,Lagrangian relaxation,拉格朗日松弛法,0.0,9,"[{'word': '拉格朗日松弛', 'ratio': 1.0}]",拉格朗日松弛,{},[]
388,388,Lambertian reflectance,朗伯反射率,0.0,9,"[{'word': '兰伯特反射率', 'ratio': 0.4444444444444444}, {'word': '朗伯反射', 'ratio': 0.4444444444444444}, {'word': '兰伯特反射', 'ratio': 0.1111111111111111}]",兰伯特反射率,"1. Rank: 兰伯特反射率, 兰伯特反射, 朗伯反射

2. Explanation: The term ""兰伯特反射率"" (Lambert reflectance) is the best fit because it accurately retains the original name ""Lambert"" while also including the term ""反射率"" (reflectance), which is the correct technical term used in the context of bidirectional reflectance distribution functions (BRDF) in the AI domain. This term emphasizes the measurement aspect of reflectance, which is crucial in understanding material behavior in computer graphics and vision. 

The second option, ""兰伯特反射"" (Lambert reflection), while also retaining the name ""Lambert,"" lacks the specificity of ""reflectance,"" which is essential in this context. The third option, ""朗伯反射"" (Lambertian reflection), uses a transliteration of the name that is less common and does not align with the established terminology in the field. Therefore, ""兰伯特反射率"" is the most semantically accurate and contextually appropriate choice.","['Lambert reflectance', 'Lambertian reflection', 'Lambert reflection']"
389,389,Lanczos iteration,兰佐斯迭代,0.7777777777777778,9,"[{'word': '兰佐斯迭代', 'ratio': 0.7777777777777778}, {'word': '兰索斯迭代', 'ratio': 0.2222222222222222}]",兰佐斯迭代,{},[]
390,390,Langevin dynamic,朗之万动力学,0.3,10,"[{'word': '朗之万动力学', 'ratio': 0.3}, {'word': '兰金动力学', 'ratio': 0.2}, {'word': '兰格文动力学', 'ratio': 0.2}, {'word': '兰贺文动力学', 'ratio': 0.1}, {'word': 'Langevin 动力学', 'ratio': 0.1}, {'word': '兰贺维动理', 'ratio': 0.1}]",兰格文动力学,"1. Rank: 兰格文动力学, 朗之万动力学, Langevin 动力学, 兰金动力学, 兰贺文动力学, 兰贺维动理

2. Explanation: The term ""兰格文动力学"" is the best fit because it accurately transliterates the name ""Langevin"" while maintaining the correct scientific context of ""dynamics."" This term is widely recognized in the field of physics and AI, particularly in discussions of stochastic processes and generative models. The back translation ""Langevin Dynamics"" confirms its semantic accuracy. 

""朗之万动力学"" is a close second, as it also attempts to transliterate ""Langevin,"" but it is less commonly used in academic literature compared to ""兰格文."" The term ""Langevin 动力学"" is acceptable but less formal since it mixes English and Chinese, which is not ideal in a scholarly context. The other candidates, such as ""兰金动力学"" and ""兰贺文动力学,"" introduce inaccuracies by misrepresenting the original name or creating confusion with unrelated terms. ""兰贺维动理"" is the least suitable as it does not accurately reflect the original term and lacks contextual relevance.","['Langevin Dynamics', 'Rankine Kinetics', 'Langevin Dynamics', 'Langhewen Dynamics', 'Langevin Dynamics', ""Langhewei's theory""]"
391,391,Language Model,语言模型,1.0,10,"[{'word': '语言模型', 'ratio': 1.0}]",语言模型,{},[]
392,392,Language Modeling Toolkit,语言建模工具包,1.0,10,"[{'word': '语言建模工具包', 'ratio': 1.0}]",语言建模工具包,{},[]
393,393,Laplace approximation,拉普拉斯近似,1.0,10,"[{'word': '拉普拉斯近似', 'ratio': 1.0}]",拉普拉斯近似,{},[]
394,394,Laplace distribution,拉普拉斯分布,1.0,10,"[{'word': '拉普拉斯分布', 'ratio': 1.0}]",拉普拉斯分布,{},[]
395,395,Laplace noise,拉普拉斯噪声,1.0,10,"[{'word': '拉普拉斯噪声', 'ratio': 1.0}]",拉普拉斯噪声,{},[]
396,396,Laplace smoothing,拉普拉斯平滑,1.0,10,"[{'word': '拉普拉斯平滑', 'ratio': 1.0}]",拉普拉斯平滑,{},[]
397,397,Laplace-Beltrami operator,拉普拉斯-贝尔特拉米算子,1.0,10,"[{'word': '拉普拉斯-贝尔特拉米算子', 'ratio': 1.0}]",拉普拉斯-贝尔特拉米算子,{},[]
398,398,Laplacian distribution,拉普拉斯分布,1.0,10,"[{'word': '拉普拉斯分布', 'ratio': 1.0}]",拉普拉斯分布,{},[]
399,399,Laplacian matrix,拉普拉斯矩阵,1.0,10,"[{'word': '拉普拉斯矩阵', 'ratio': 1.0}]",拉普拉斯矩阵,{},[]
400,400,Laplacian smoothing,拉普拉斯平滑,1.0,10,"[{'word': '拉普拉斯平滑', 'ratio': 1.0}]",拉普拉斯平滑,{},[]
401,401,Lasso,套索法,0.0,10,"[{'word': '套索回归', 'ratio': 0.6}, {'word': 'Lasso回归', 'ratio': 0.2}, {'word': 'Lasso', 'ratio': 0.1}, {'word': '回归', 'ratio': 0.1}]",套索回归,{},[]
402,402,Lasso penalty,套索惩罚,0.5,10,"[{'word': '套索惩罚', 'ratio': 0.5}, {'word': 'Lasso惩罚', 'ratio': 0.3}, {'word': 'Lasso惩罚项', 'ratio': 0.1}, {'word': '惩罚', 'ratio': 0.1}]",套索惩罚,{},[]
403,403,Latent Dirichlet Allocation,隐狄利克雷分布,0.0,10,"[{'word': '潜在狄利克雷分配', 'ratio': 1.0}]",潜在狄利克雷分配,{},[]
404,404,Latent Semantic Analysis,潜在语义分析,1.0,10,"[{'word': '潜在语义分析', 'ratio': 1.0}]",潜在语义分析,{},[]
405,405,Layer Normalization,层归一化,1.0,10,"[{'word': '层归一化', 'ratio': 1.0}]",层归一化,{},[]
406,406,Learning Rate,学习率,1.0,10,"[{'word': '学习率', 'ratio': 1.0}]",学习率,{},[]
407,407,Lemma,引理,1.0,10,"[{'word': '引理', 'ratio': 1.0}]",引理,{},[]
408,408,Levenberg-Marquardt algorithm,勒文伯格-马夸特算法,0.0,10,"[{'word': '列文伯格-马夸特算法', 'ratio': 0.5}, {'word': '莱文贝格-马夸特算法', 'ratio': 0.3}, {'word': '勒文贝格-马夸特算法', 'ratio': 0.1}, {'word': '莱文伯格-马夸特算法', 'ratio': 0.1}]",列文伯格-马夸特算法,{},[]
409,409,Levenshtein distance,莱文斯坦距离,0.9,10,"[{'word': '莱文斯坦距离', 'ratio': 0.9}, {'word': '莱温斯坦距离', 'ratio': 0.1}]",莱文斯坦距离,{},[]
410,410,Levenshtein edit distance,莱文斯坦编辑距离,0.7777777777777778,9,"[{'word': '莱文斯坦编辑距离', 'ratio': 0.7777777777777778}, {'word': '编辑 编辑距离', 'ratio': 0.1111111111111111}, {'word': '莱温斯坦编辑距离', 'ratio': 0.1111111111111111}]",莱文斯坦编辑距离,{},[]
411,411,Lexical Functional Grammar,词汇功能语法,1.0,9,"[{'word': '词汇功能语法', 'ratio': 1.0}]",词汇功能语法,{},[]
412,412,Libratus,天秤座,0.3333333333333333,9,"[{'word': 'Libratus', 'ratio': 0.5555555555555556}, {'word': '天秤座', 'ratio': 0.3333333333333333}, {'word': '', 'ratio': 0.1111111111111111}]",Libratus,{},[]
413,413,Lie algebra,李代数,0.8888888888888888,9,"[{'word': '李代数', 'ratio': 0.8888888888888888}, {'word': '李群', 'ratio': 0.1111111111111111}]",李代数,{},[]
414,414,Lifelong Learning,终身学习,1.0,9,"[{'word': '终身学习', 'ratio': 1.0}]",终身学习,{},[]
415,415,Likert scale,利克特量表,0.0,9,"[{'word': '李克特量表', 'ratio': 1.0}]",李克特量表,{},[]
416,416,Linear Regression,线性回归,1.0,9,"[{'word': '线性回归', 'ratio': 1.0}]",线性回归,{},[]
417,417,Linear Threshold,线性阈值模型,0.1111111111111111,9,"[{'word': '线性阈值', 'ratio': 0.8888888888888888}, {'word': '线性阈值模型', 'ratio': 0.1111111111111111}]",线性阈值,{},[]
418,418,Linear Threshold Model,线性阈值模型,1.0,9,"[{'word': '线性阈值模型', 'ratio': 1.0}]",线性阈值模型,{},[]
419,419,Linear Transformer,线性Transformer,0.3333333333333333,9,"[{'word': '线性变换器', 'ratio': 0.6666666666666666}, {'word': '线性Transformer', 'ratio': 0.3333333333333333}]",线性变换器,{},[]
420,420,Linformer,Linformer,0.7777777777777778,9,"[{'word': 'Linformer', 'ratio': 0.7777777777777778}, {'word': '林弗默', 'ratio': 0.1111111111111111}, {'word': '线性变换器', 'ratio': 0.1111111111111111}]",Linformer,{},[]
421,421,Lipschitz,利普希茨,1.0,9,"[{'word': '利普希茨', 'ratio': 1.0}]",利普希茨,{},[]
422,422,Lipschitz constant,利普希茨常数,1.0,9,"[{'word': '利普希茨常数', 'ratio': 1.0}]",利普希茨常数,{},[]
423,423,Lipschitz continuity,利普希茨连续性,1.0,9,"[{'word': '利普希茨连续性', 'ratio': 1.0}]",利普希茨连续性,{},[]
424,424,Lipschitz continuous,利普希茨连续,1.0,10,"[{'word': '利普希茨连续', 'ratio': 1.0}]",利普希茨连续,{},[]
425,425,Lipschitz function,利普希茨函数,1.0,10,"[{'word': '利普希茨函数', 'ratio': 1.0}]",利普希茨函数,{},[]
426,426,Lipschitzness,利普希茨性质,0.2,10,"[{'word': '利普希茨性', 'ratio': 0.7}, {'word': '利普希茨性质', 'ratio': 0.2}, {'word': '利普希茨尼斯', 'ratio': 0.1}]",利普希茨性,{},[]
427,427,Local Consistency,局部一致性,1.0,10,"[{'word': '局部一致性', 'ratio': 1.0}]",局部一致性,{},[]
428,428,Log Gaussian Cox Process,对数高斯-Cox过程,0.0,10,"[{'word': '对数高斯考克斯过程', 'ratio': 0.8}, {'word': '对数高斯科克斯过程', 'ratio': 0.1}, {'word': '对数高斯 Cox 过程', 'ratio': 0.1}]",对数高斯考克斯过程,{},[]
429,429,Logistic Regression,逻辑回归,0.9,10,"[{'word': '逻辑回归', 'ratio': 0.9}, {'word': 'ロジスティック回帰', 'ratio': 0.1}]",逻辑回归,{},[]
430,430,Longformer,长序列转换器,0.0,10,"[{'word': 'Longformer', 'ratio': 0.5}, {'word': '長形器', 'ratio': 0.1}, {'word': 'ロングフォーマー', 'ratio': 0.1}, {'word': '长序列模型', 'ratio': 0.1}, {'word': '', 'ratio': 0.1}, {'word': '长程Transformer', 'ratio': 0.1}]",Longformer,{},[]
431,431,Lucene,Lucene,0.6,10,"[{'word': 'Lucene', 'ratio': 0.6}, {'word': '盧塞恩', 'ratio': 0.1}, {'word': 'ルシーン', 'ratio': 0.1}, {'word': '卢塞恩', 'ratio': 0.1}, {'word': '开源搜索库', 'ratio': 0.1}]",Lucene,{},[]
432,432,Lyapunov function,李雅普诺夫函数,0.6,10,"[{'word': '李雅普诺夫函数', 'ratio': 0.6}, {'word': 'Lyapunov 函數', 'ratio': 0.1}, {'word': 'リアプノフ関数', 'ratio': 0.1}, {'word': 'Lyapunov 函数', 'ratio': 0.1}, {'word': '利雅普诺夫函数', 'ratio': 0.1}]",李雅普诺夫函数,{},[]
433,433,M-estimation,M-估计,0.0,10,"[{'word': 'M估计', 'ratio': 0.6}, {'word': 'M 估计', 'ratio': 0.3}, {'word': 'M 推定値', 'ratio': 0.1}]",M估计,{},[]
434,434,M-step,M步,0.7777777777777778,9,"[{'word': 'M步', 'ratio': 0.7777777777777778}, {'word': 'M 步', 'ratio': 0.1111111111111111}, {'word': 'M步骤', 'ratio': 0.1111111111111111}]",M步,{},[]
435,435,Machine Comprehension,机器理解,0.8888888888888888,9,"[{'word': '机器理解', 'ratio': 0.8888888888888888}, {'word': '机器阅读理解', 'ratio': 0.1111111111111111}]",机器理解,{},[]
436,436,Machine Learning,机器学习,1.0,9,"[{'word': '机器学习', 'ratio': 1.0}]",机器学习,{},[]
437,437,Machine Learning Repository,机器学习存储库,0.0,10,"[{'word': '机器学习库', 'ratio': 0.6}, {'word': '机器学习数据集', 'ratio': 0.2}, {'word': '机器学习数据集库', 'ratio': 0.1}, {'word': '机器学习数据库', 'ratio': 0.1}]",机器学习库,{},[]
438,438,Machine Reading Comprehension,机器阅读理解,1.0,10,"[{'word': '机器阅读理解', 'ratio': 1.0}]",机器阅读理解,{},[]
439,439,Machine Translation,机器翻译,1.0,10,"[{'word': '机器翻译', 'ratio': 1.0}]",机器翻译,{},[]
440,440,Mahalanobis distance,马哈拉诺比斯距离,1.0,10,"[{'word': '马哈拉诺比斯距离', 'ratio': 1.0}]",马哈拉诺比斯距离,{},[]
441,441,Mahalanobis distance function,马哈拉诺比斯距离函数,1.0,10,"[{'word': '马哈拉诺比斯距离函数', 'ratio': 1.0}]",马哈拉诺比斯距离函数,{},[]
442,442,Mahalanobis matrix,马哈拉诺比斯矩阵,0.8888888888888888,9,"[{'word': '马哈拉诺比斯矩阵', 'ratio': 0.8888888888888888}, {'word': '马氏矩阵', 'ratio': 0.1111111111111111}]",马哈拉诺比斯矩阵,{},[]
443,443,Mahalanobis metric,马哈拉诺比斯度量,0.8888888888888888,9,"[{'word': '马哈拉诺比斯度量', 'ratio': 0.8888888888888888}, {'word': '马氏度量', 'ratio': 0.1111111111111111}]",马哈拉诺比斯度量,{},[]
444,444,Manhattan distance,曼哈顿距离,1.0,9,"[{'word': '曼哈顿距离', 'ratio': 1.0}]",曼哈顿距离,{},[]
445,445,Marching Cubes,行进立方体,0.3333333333333333,9,"[{'word': '行进立方体', 'ratio': 0.3333333333333333}, {'word': '移动立方体算法', 'ratio': 0.2222222222222222}, {'word': '立方体行进算法', 'ratio': 0.1111111111111111}, {'word': '行进立方体法', 'ratio': 0.1111111111111111}, {'word': '行进立方体算法', 'ratio': 0.1111111111111111}, {'word': '游走立方体算法', 'ratio': 0.1111111111111111}]",行进立方体法,"1. Rank: 行进立方体法, 行进立方体算法, 立方体行进算法, 移动立方体算法, 游走立方体算法, 行进立方体

2. Explanation: The term ""行进立方体法"" (marching cube method) is the best fit for the translation of ""Marching Cubes"" in the AI domain. This is because it accurately captures the essence of the original term while maintaining semantic clarity. The word ""法"" (method) is commonly used in technical contexts to denote a specific algorithm or technique, which aligns well with the usage of ""Marching Cubes"" as a well-established algorithm in computer graphics and AI for surface extraction.

The second choice, ""行进立方体算法"" (marching cube algorithm), is also a strong candidate, as ""算法"" (algorithm) is another appropriate term in this context. However, ""法"" (method) is slightly more aligned with the terminology used in academic literature, where ""method"" often refers to a specific procedural approach.

The other candidates, such as ""移动立方体算法"" (moving cube algorithm) and ""游走立方体算法"" (walking cube algorithm), introduce unnecessary variations that deviate from the established terminology. They could lead to confusion, as they do not accurately reflect the original term's meaning or its established usage in the field. Therefore, ""行进立方体法"" is the most contextually appropriate and semantically accurate translation.","['marching cube', 'moving cube algorithm', 'cube marching algorithm', 'marching cube method', 'Marching Cube Algorithm', 'walking cube algorithm']"
446,446,Markov,马尔可夫,1.0,9,"[{'word': '马尔可夫', 'ratio': 1.0}]",马尔可夫,{},[]
447,447,Markov Chain,马尔可夫链,1.0,10,"[{'word': '马尔可夫链', 'ratio': 1.0}]",马尔可夫链,{},[]
448,448,Markov Chain Monte Carlo,马尔可夫链蒙特卡罗法 (MCMC),0.0,10,"[{'word': '马尔可夫链蒙特卡洛', 'ratio': 0.5}, {'word': '马尔可夫链蒙特卡罗', 'ratio': 0.3}, {'word': '马尔可夫链蒙特卡罗法', 'ratio': 0.1}, {'word': '马尔可夫链蒙特卡罗方法', 'ratio': 0.1}]",马尔可夫链蒙特卡洛,{},[]
449,449,Markov Decision Process,马尔可夫决策过程,1.0,10,"[{'word': '马尔可夫决策过程', 'ratio': 1.0}]",马尔可夫决策过程,{},[]
450,450,Markov Random Field,马尔可夫随机场,0.9,10,"[{'word': '马尔可夫随机场', 'ratio': 0.9}, {'word': '马尔可夫决策过程', 'ratio': 0.1}]",马尔可夫随机场,{},[]
451,451,Markov assumption,马尔可夫假设,1.0,10,"[{'word': '马尔可夫假设', 'ratio': 1.0}]",马尔可夫假设,{},[]
452,452,Markov blanket,马尔可夫毯,0.875,8,"[{'word': '马尔可夫毯', 'ratio': 0.875}, {'word': '马尔可夫包络', 'ratio': 0.125}]",马尔可夫毯,{},[]
453,453,Markov chain model,马尔可夫链模型,1.0,8,"[{'word': '马尔可夫链模型', 'ratio': 1.0}]",马尔可夫链模型,{},[]
454,454,Markov game,马尔可夫博弈,0.625,8,"[{'word': '马尔可夫博弈', 'ratio': 0.625}, {'word': '马尔可夫游戏', 'ratio': 0.375}]",马尔可夫博弈,{},[]
455,455,Markov kernel,马尔可夫核,1.0,8,"[{'word': '马尔可夫核', 'ratio': 1.0}]",马尔可夫核,{},[]
456,456,Markov logic,马尔可夫逻辑,1.0,8,"[{'word': '马尔可夫逻辑', 'ratio': 1.0}]",马尔可夫逻辑,{},[]
457,457,Markov logic network,马尔可夫逻辑网络,1.0,9,"[{'word': '马尔可夫逻辑网络', 'ratio': 1.0}]",马尔可夫逻辑网络,{},[]
458,458,Markov model,马尔可夫模型,1.0,9,"[{'word': '马尔可夫模型', 'ratio': 1.0}]",马尔可夫模型,{},[]
459,459,Markov network,马尔可夫网络,1.0,9,"[{'word': '马尔可夫网络', 'ratio': 1.0}]",马尔可夫网络,{},[]
460,460,Markov process,马尔可夫过程,1.0,9,"[{'word': '马尔可夫过程', 'ratio': 1.0}]",马尔可夫过程,{},[]
461,461,Markov property,马尔可夫性质,0.8888888888888888,9,"[{'word': '马尔可夫性质', 'ratio': 0.8888888888888888}, {'word': '马尔可夫性质 如果需要进一步的帮助，请告诉我！', 'ratio': 0.1111111111111111}]",马尔可夫性质,{},[]
462,462,Markov state,马尔可夫状态,1.0,10,"[{'word': '马尔可夫状态', 'ratio': 1.0}]",马尔可夫状态,{},[]
463,463,Markov transition,马尔可夫转移,1.0,10,"[{'word': '马尔可夫转移', 'ratio': 1.0}]",马尔可夫转移,{},[]
464,464,Markov transition matrix,马尔可夫转移矩阵,1.0,10,"[{'word': '马尔可夫转移矩阵', 'ratio': 1.0}]",马尔可夫转移矩阵,{},[]
465,465,Markov's inequality,马尔可夫不等式,1.0,10,"[{'word': '马尔可夫不等式', 'ratio': 1.0}]",马尔可夫不等式,{},[]
466,466,MatConvNet,MatConvNet,1.0,10,"[{'word': 'MatConvNet', 'ratio': 1.0}]",MatConvNet,{},[]
467,467,Matching Network,匹配网络,1.0,8,"[{'word': '匹配网络', 'ratio': 1.0}]",匹配网络,{},[]
468,468,Matrix Completion,矩阵完成,0.0,8,"[{'word': '矩阵补全', 'ratio': 1.0}]",矩阵补全,{},[]
469,469,Matrix factorization,矩阵分解,1.0,8,"[{'word': '矩阵分解', 'ratio': 1.0}]",矩阵分解,{},[]
470,470,Matérn kernel,马特恩核,0.25,8,"[{'word': 'Matérn 核', 'ratio': 0.25}, {'word': '马顿核', 'ratio': 0.25}, {'word': '马特恩核', 'ratio': 0.25}, {'word': '核函数', 'ratio': 0.125}, {'word': 'Matérn 核函数', 'ratio': 0.125}]",Matérn 核函数,"1. Rank: Matérn 核函数, Matérn 核, 马特恩核, 马顿核, 核函数

2. Explanation: The term ""Matérn 核函数"" is the best fit because it retains the original name ""Matérn"" while clearly indicating that it is a ""kernel function,"" which is essential in the context of AI and machine learning. This term accurately reflects the specific mathematical function used in spatial statistics and Gaussian processes, making it semantically precise for the AI domain. The inclusion of ""函数"" (function) is crucial as it specifies the mathematical nature of the term, which is important for clarity in technical discussions. 

""Matérn 核"" is a close second, as it also retains the original name but omits ""函数,"" which could lead to ambiguity in a technical context. The transliterations ""马特恩核"" and ""马顿核"" do not maintain the original name's integrity and may confuse readers who are familiar with the term in its original form. Lastly, ""核函数"" is too generic and does not specify the Matérn kernel, making it the least suitable option in this context.","['Matérn core', 'Marton core', 'Mattern core', 'kernel function', 'Matérn kernel function']"
471,471,Maximum Likelihood,最大似然,1.0,10,"[{'word': '最大似然', 'ratio': 1.0}]",最大似然,{},[]
472,472,Maximum Satisfiability,最大可满足性 (Maximum Satisfiability),0.0,10,"[{'word': '最大可满足性', 'ratio': 0.7}, {'word': '最大满足问题', 'ratio': 0.2}, {'word': '最大满足性', 'ratio': 0.1}]",最大可满足性,{},[]
473,473,Mechanical Turk,机械土耳其,0.0,10,"[{'word': '机械土耳其人', 'ratio': 1.0}]",机械土耳其人,{},[]
474,474,Meta-learning,元学习,1.0,10,"[{'word': '元学习', 'ratio': 1.0}]",元学习,{},[]
475,475,Metropolis Hastings,大都会哈斯廷算法,0.0,10,"[{'word': '梅特罗波利斯-黑斯廷斯算法', 'ratio': 0.2}, {'word': '美特罗波利斯-海斯廷斯算法', 'ratio': 0.1}, {'word': '方法', 'ratio': 0.1}, {'word': '大都市-黑斯廷斯算法', 'ratio': 0.1}, {'word': 'Metropolis-Hastings 算法', 'ratio': 0.1}, {'word': '梅特罗波利斯-海斯廷斯', 'ratio': 0.1}, {'word': '迈特罗波利斯-海斯廷斯算法', 'ratio': 0.1}, {'word': '大都会-海斯廷斯算法', 'ratio': 0.1}, {'word': '大都会-哈斯廷斯算法', 'ratio': 0.1}]",梅特罗波利斯-黑斯廷斯算法,"1. Rank: 梅特罗波利斯-黑斯廷斯算法, 美特罗波利斯-海斯廷斯算法, Metropolis-Hastings 算法, 大都市-黑斯廷斯算法, 迈特罗波利斯-海斯廷斯算法, 大都会-海斯廷斯算法, 大都会-哈斯廷斯算法, 方法

2. Explanation: The term ""梅特罗波利斯-黑斯廷斯算法"" is the best fit because it accurately retains the original names ""Metropolis"" and ""Hastings"" while providing a clear indication that it is an algorithm (算法). This translation is semantically accurate and contextually appropriate for the AI domain, as it directly references the well-known Metropolis-Hastings algorithm used in statistical sampling. The use of ""黑斯廷斯"" for ""Hastings"" is also widely recognized in the field, ensuring that the term is understood by professionals familiar with the algorithm. Other candidates, such as ""美特罗波利斯-海斯廷斯算法"" and ""大都市-黑斯廷斯算法,"" either use less common transliterations or introduce variations that may not be as recognizable in the AI community. Therefore, the first option is the most effective for conveying the intended meaning in a professional context.","['Metropolis-Hastings algorithm', 'Metropolis-Hastings algorithm', 'method', 'Metropolis-Hastings Algorithm', 'Metropolis-Hastings algorithm', 'Metropolis-Hastings', 'Metropolis-Hastings algorithm', 'Metropolitan-Hastings Algorithm', 'Metropolitan-Hastings algorithm']"
476,476,Metropolis algorithm,大都会算法,0.1111111111111111,9,"[{'word': '梅特罗波利斯算法', 'ratio': 0.4444444444444444}, {'word': '美特罗波利斯算法', 'ratio': 0.2222222222222222}, {'word': '元胞算法', 'ratio': 0.1111111111111111}, {'word': '算法', 'ratio': 0.1111111111111111}, {'word': '大都会算法', 'ratio': 0.1111111111111111}]",梅特罗波利斯算法,"1. Rank: 梅特罗波利斯算法, 美特罗波利斯算法, 大都会算法, 元胞算法, 算法

2. Explanation: The first translation candidate, ""梅特罗波利斯算法,"" is the best fit because it accurately transliterates the term ""Metropolis"" while maintaining the context of the algorithm in the AI domain. The back translation ""Metropolis algorithm"" aligns perfectly with the original English term, ensuring semantic accuracy. 

The second candidate, ""美特罗波利斯算法,"" is also a transliteration but uses a different character for ""Metropolis,"" which may not be as widely recognized in the context of this algorithm. 

The third candidate, ""大都会算法,"" translates to ""metropolitan algorithm,"" which could lead to confusion as it does not retain the specific reference to the Metropolis method used in statistical sampling. 

The fourth candidate, ""元胞算法,"" translates to ""cellular algorithm,"" which is unrelated to the Metropolis algorithm and thus semantically inaccurate. 

The last candidate, ""算法,"" simply means ""algorithm"" and lacks the specificity needed for accurate communication in the AI context. 

Overall, ""梅特罗波利斯算法"" is the most contextually appropriate and semantically accurate choice for the term in the AI domain.","['Metropolis algorithm', 'Metropolis Algorithm', 'Cellular Algorithm', 'algorithm', 'metropolitan algorithm']"
477,477,Metropolis method,Metropolis 方法,0.0,9,"[{'word': '梅特罗波利斯方法', 'ratio': 0.4444444444444444}, {'word': '美特罗波利斯方法', 'ratio': 0.2222222222222222}, {'word': '元胞方法', 'ratio': 0.1111111111111111}, {'word': 'Metropolis方法', 'ratio': 0.1111111111111111}, {'word': '大都会方法', 'ratio': 0.1111111111111111}]",梅特罗波利斯方法,"1. Rank: 梅特罗波利斯方法, Metropolis方法, 美特罗波利斯方法, 大都会方法, 元胞方法

2. Explanation: The term ""梅特罗波利斯方法"" is the best fit because it accurately retains the original name ""Metropolis"" while providing a phonetic representation that is widely recognized in the AI and statistical communities. This term is semantically accurate and aligns with the established terminology used in the context of Markov Chain Monte Carlo (MCMC) methods, which is relevant to the provided context. 

""Metropolis方法"" is also a strong candidate as it retains the original name in a more direct transliteration format, but it lacks the phonetic clarity that ""梅特罗波利斯"" provides. ""美特罗波利斯方法"" is a less common variant that may not be as widely recognized. ""大都会方法"" translates to ""metropolitan approach,"" which diverges from the original meaning and could confuse readers. Lastly, ""元胞方法"" translates to ""cellular method,"" which is unrelated to the Metropolis method and thus is the least appropriate choice in this context.","['metropolis method', 'metropolis method', 'cellular method', 'Metropolis method', 'metropolitan approach']"
478,478,Metropolis-Hasting,大都市-黑斯廷,0.0,9,"[{'word': '梅特罗波利斯-哈斯廷斯', 'ratio': 0.2222222222222222}, {'word': '美特罗波利斯-哈斯廷斯', 'ratio': 0.2222222222222222}, {'word': '梅特罗波利斯-黑斯廷斯', 'ratio': 0.2222222222222222}, {'word': '元胞-哈斯廷斯', 'ratio': 0.1111111111111111}, {'word': 'Metropolis-Hasting算法', 'ratio': 0.1111111111111111}, {'word': '大都会-黑斯廷斯', 'ratio': 0.1111111111111111}]",梅特罗波利斯-哈斯廷斯,"1. Rank: 梅特罗波利斯-哈斯廷斯, 美特罗波利斯-哈斯廷斯, 梅特罗波利斯-黑斯廷斯, 大都会-黑斯廷斯, 元胞-哈斯廷斯, Metropolis-Hasting算法

2. Explanation: The term ""梅特罗波利斯-哈斯廷斯"" is the best fit because it accurately transliterates the original English term ""Metropolis-Hastings"" while maintaining the semantic integrity of the term in the context of AI and statistics. This term preserves the original names and is widely recognized in the field, ensuring clarity and precision in communication among professionals. The second candidate, ""美特罗波利斯-哈斯廷斯,"" is also a valid transliteration but uses a different character for ""美,"" which is less common in this context. The third candidate, ""梅特罗波利斯-黑斯廷斯,"" introduces a different character for ""Hastings,"" which could lead to confusion. The fourth candidate, ""大都会-黑斯廷斯,"" deviates from the original name and introduces a different meaning (""Metropolitan""), which is not appropriate in this context. The fifth candidate, ""元胞-哈斯廷斯,"" incorrectly translates ""Metropolis"" to ""Cell,"" which is not relevant to the term's meaning. Lastly, ""Metropolis-Hasting算法"" is a less preferred option as it adds ""算法"" (algorithm) unnecessarily, which could lead to ambiguity. Thus, ""梅特罗波利斯-哈斯廷斯"" is the most contextually accurate and semantically appropriate choice.","['Metropolis-Hastings', 'Metropolis-Hastings', 'Metropolis-Hastings', 'Cell-Hastings', 'Metropolis-Hasting algorithm', 'Metropolitan-Hastings']"
479,479,Metropolis-Hastings acceptance ratio,Metropolis-Hastings接受率,0.0,9,"[{'word': '梅特罗波利斯-哈斯廷斯接受比率', 'ratio': 0.2222222222222222}, {'word': '美特罗波利斯-哈斯廷斯接受比率', 'ratio': 0.2222222222222222}, {'word': '梅特罗波利斯-黑斯廷斯接受率', 'ratio': 0.2222222222222222}, {'word': '元胞-哈斯廷斯接受比率', 'ratio': 0.1111111111111111}, {'word': 'Metropolis-Hastings接受比率', 'ratio': 0.1111111111111111}, {'word': '大都会-黑斯廷斯接受率', 'ratio': 0.1111111111111111}]",梅特罗波利斯-哈斯廷斯接受比率,"1. Rank: 梅特罗波利斯-哈斯廷斯接受比率, 美特罗波利斯-哈斯廷斯接受比率, Metropolis-Hastings接受比率, 梅特罗波利斯-黑斯廷斯接受率, 大都会-黑斯廷斯接受率, 元胞-哈斯廷斯接受比率

2. Explanation: The term ""梅特罗波利斯-哈斯廷斯接受比率"" is the best fit because it accurately retains the original names ""Metropolis"" and ""Hastings"" in a way that is phonetically and semantically appropriate for Chinese speakers. This translation uses the standard transliteration for ""Metropolis"" (梅特罗波利斯) and ""Hastings"" (哈斯廷斯), which are widely recognized in academic and technical contexts, particularly in AI and statistics. 

The second candidate, ""美特罗波利斯-哈斯廷斯接受比率,"" while also a valid transliteration, uses ""美"" which is less common and could imply a different meaning (beauty) in Chinese, making it less suitable. 

The third candidate, ""Metropolis-Hastings接受比率,"" retains the English term but lacks the full Chinese context, which may not be as accessible to all readers. 

The remaining candidates, ""梅特罗波利斯-黑斯廷斯接受率,"" ""大都会-黑斯廷斯接受率,"" and ""元胞-哈斯廷斯接受比率,"" either misrepresent the names or introduce unrelated terms (like ""元胞"" meaning ""cell""), which could confuse the intended meaning in the AI domain. 

Overall, the first choice is the most semantically accurate and contextually appropriate for the AI terminology in question.","['Metropolis-Hastings Acceptance Ratio', 'Metropolis-Hastings Acceptance Ratio', 'Metropolis-Hastings Acceptance Rate', 'Cell-Hastings Acceptance Ratio', 'Metropolis-Hastings Acceptance Ratio', 'Mets-Hastings Acceptance Rate']"
480,480,Metropolis-Hastings algorithm,Metropolis-Hastings 算法,0.0,9,"[{'word': '梅特罗波利斯-哈斯廷斯算法', 'ratio': 0.2222222222222222}, {'word': '美特罗波利斯-哈斯廷斯算法', 'ratio': 0.2222222222222222}, {'word': '梅特罗波利斯-黑斯廷斯算法', 'ratio': 0.2222222222222222}, {'word': '元胞-哈斯廷斯算法', 'ratio': 0.1111111111111111}, {'word': 'Metropolis-Hastings算法', 'ratio': 0.1111111111111111}, {'word': '大都会-黑斯廷斯算法', 'ratio': 0.1111111111111111}]",梅特罗波利斯-哈斯廷斯算法,"1. Rank: 梅特罗波利斯-哈斯廷斯算法, 美特罗波利斯-哈斯廷斯算法, Metropolis-Hastings算法, 大都会-黑斯廷斯算法, 梅特罗波利斯-黑斯廷斯算法, 元胞-哈斯廷斯算法

2. Explanation: The term ""梅特罗波利斯-哈斯廷斯算法"" is the best fit because it accurately retains the original names ""Metropolis"" and ""Hastings"" in a way that is phonetically and semantically appropriate for Chinese speakers familiar with the AI and statistical domains. This translation uses a common transliteration for ""Metropolis"" that is widely recognized in academic literature, ensuring clarity and consistency. The use of ""算法"" (algorithm) is standard in technical contexts, making it immediately understandable to the target audience. 

The second candidate, ""美特罗波利斯-哈斯廷斯算法,"" while also a valid transliteration, uses ""美"" which is less common and may not be as recognizable in the context of the algorithm. The third candidate, ""Metropolis-Hastings算法,"" is a direct mix of English and Chinese, which is acceptable but less formal than a fully translated term. The remaining candidates either introduce inaccuracies (like ""元胞-哈斯廷斯算法,"" which translates to ""Cellular-Hastings Algorithm"" and is incorrect) or use less appropriate transliterations. Thus, ""梅特罗波利斯-哈斯廷斯算法"" stands out as the most contextually accurate and semantically precise choice.","['Metropolis-Hastings algorithm', 'Metropolis-Hastings algorithm', 'Metropolis-Hastings algorithm', 'Cellular-Hastings Algorithm', 'Metropolis-Hastings algorithm', 'Metropolitan-Hastings Algorithm']"
481,481,Metropolis-Hastings sampler,梅特罗波利斯-哈斯廷斯采样器,0.0,9,"[{'word': '梅特罗波利斯-黑斯廷斯采样器', 'ratio': 0.2222222222222222}, {'word': 'Metropolis-Hastings 採樣器', 'ratio': 0.1111111111111111}, {'word': 'Metropolis-Hastings采样器', 'ratio': 0.1111111111111111}, {'word': 'Metropolis-Hastings 采样器', 'ratio': 0.1111111111111111}, {'word': '采样器', 'ratio': 0.1111111111111111}, {'word': 'Metropolis-Hastings采样器采样器', 'ratio': 0.1111111111111111}, {'word': 'metropolis-hastings 采样器', 'ratio': 0.1111111111111111}, {'word': '大都会-哈斯廷斯采样器', 'ratio': 0.1111111111111111}]",梅特罗波利斯-黑斯廷斯采样器,"1. Rank: 梅特罗波利斯-黑斯廷斯采样器, Metropolis-Hastings 采样器, Metropolis-Hastings采样器, Metropolis-Hastings 採樣器, metropolis-hastings 采样器, 采样器, 大都会-哈斯廷斯采样器, Metropolis-Hastings采样器采样器

2. Explanation: The term ""梅特罗波利斯-黑斯廷斯采样器"" is the best fit because it retains the original names ""Metropolis"" and ""Hastings"" in a phonetic transliteration that is widely recognized in the AI and statistics communities. This term accurately reflects the original English term while maintaining semantic clarity. The use of ""采样器"" (sampler) is appropriate and aligns with the technical context of sampling in probabilistic models. Other candidates, such as ""Metropolis-Hastings 采样器"" and ""Metropolis-Hastings采样器,"" are also good but do not provide the same level of phonetic fidelity to the original names. The term ""大都会-哈斯廷斯采样器"" introduces a less common translation for ""Metropolis,"" which could lead to confusion, and ""Metropolis-Hastings采样器采样器"" is redundant. Therefore, the first option is the most semantically accurate and contextually appropriate for the AI domain.","['Metropolis-Hastings Sampler', 'Metropolis-Hastings Sampler', 'Metropolis-Hastings sampler', 'Metropolis-Hastings Sampler', 'sampler', 'Metropolis-Hastings Sampler Sampler', 'metropolis-hastings sampler', 'Metropolitan-Hastings Sampler']"
482,482,Mixup,Mixup,0.4444444444444444,9,"[{'word': 'Mixup', 'ratio': 0.4444444444444444}, {'word': '', 'ratio': 0.1111111111111111}, {'word': '混合', 'ratio': 0.1111111111111111}, {'word': '混合数据增强', 'ratio': 0.1111111111111111}, {'word': '混合增强', 'ratio': 0.1111111111111111}, {'word': '混合法', 'ratio': 0.1111111111111111}]","""混合数据增强""","1. Rank: ""混合数据增强"", ""混合增强"", ""混合"", ""混合法"", ""Mixup"", ""mixup"", """"

2. Explanation: The term ""混合数据增强"" (Hybrid data augmentation) is the best fit because it accurately captures the essence of the ""Mixup"" technique in the context of machine learning and data augmentation. In the AI domain, ""Mixup"" refers specifically to a data augmentation strategy that combines two or more images (or data points) to create new training examples, which helps improve model generalization. The term ""混合数据增强"" explicitly includes ""数据增强"" (data augmentation), making it clear that it pertains to augmenting data, which is crucial in the context provided. 

The other candidates, such as ""混合增强"" (Hybrid enhancement) and ""混合"" (mix), are less specific and do not convey the full meaning of the technique as it relates to data augmentation. ""混合法"" (mixed method) is also vague and does not directly relate to the specific augmentation strategy. The terms ""Mixup"" and ""mixup"" are direct transliterations and do not provide any contextual understanding for Chinese-speaking audiences unfamiliar with the term. Therefore, ""混合数据增强"" stands out as the most semantically accurate and contextually appropriate translation in the AI domain.","['mixup', 'mixup', 'mix', 'Hybrid data augmentation', 'Hybrid enhancement', 'mixed method']"
483,483,Model,模型,0.8888888888888888,9,"[{'word': '模型', 'ratio': 0.8888888888888888}, {'word': '混合数据增强）', 'ratio': 0.1111111111111111}]",模型,{},[]
484,484,Monotonicity,单调性,1.0,9,"[{'word': '单调性', 'ratio': 1.0}]",单调性,{},[]
485,485,Monte Carlo,蒙特卡罗,0.8888888888888888,9,"[{'word': '蒙特卡罗', 'ratio': 0.8888888888888888}, {'word': '蒙特卡洛', 'ratio': 0.1111111111111111}]",蒙特卡罗,{},[]
486,486,Monte Carlo Dropout,蒙特卡罗丢弃法,0.2222222222222222,9,"[{'word': '蒙特卡罗随机失活', 'ratio': 0.4444444444444444}, {'word': '蒙特卡罗丢弃法', 'ratio': 0.2222222222222222}, {'word': '蒙特卡洛丢弃法', 'ratio': 0.1111111111111111}, {'word': '蒙特卡罗失活', 'ratio': 0.1111111111111111}, {'word': '蒙特卡罗dropout', 'ratio': 0.1111111111111111}]",蒙特卡洛丢弃法,"1. Rank: 蒙特卡洛丢弃法, 蒙特卡罗dropout, 蒙特卡罗随机失活, 蒙特卡罗失活, 蒙特卡罗丢弃法

2. Explanation: The term ""蒙特卡洛丢弃法"" (Monte Carlo dropout) is the best fit for several reasons. Firstly, it accurately translates the English term ""Monte Carlo Dropout"" while maintaining the specific terminology used in the AI domain. The term ""丢弃法"" directly corresponds to ""dropout,"" which is a well-established term in machine learning referring to a regularization technique. This makes it semantically precise and contextually relevant.

The second candidate, ""蒙特卡罗dropout,"" retains the English term ""dropout"" in its original form, which is also acceptable in the AI community, but it lacks the full Chinese translation for ""method"" (法), making it slightly less formal.

The third candidate, ""蒙特卡罗随机失活"" (Monte Carlo Random Deactivation), introduces the word ""随机"" (random), which is not necessary in this context and could lead to confusion, as ""dropout"" specifically refers to the technique rather than a general random deactivation.

The fourth candidate, ""蒙特卡罗失活"" (Monte Carlo Deactivation), omits the ""丢弃"" (dropout) aspect, which is crucial for understanding the specific technique being referred to.

Overall, ""蒙特卡洛丢弃法"" is the most accurate and contextually appropriate translation for ""Monte Carlo Dropout"" in the AI domain.","['Monte Carlo Random Deactivation', 'Monte Carlo dropout', 'Monte Carlo dropout', 'Monte Carlo Deactivation', 'Monte Carlo dropout']"
487,487,Monte Carlo Tree Search,蒙特卡罗树搜索,0.8888888888888888,9,"[{'word': '蒙特卡罗树搜索', 'ratio': 0.8888888888888888}, {'word': '蒙特卡洛树搜索', 'ratio': 0.1111111111111111}]",蒙特卡罗树搜索,{},[]
488,488,Monte Carlo algorithm,蒙特卡罗算法,0.8888888888888888,9,"[{'word': '蒙特卡罗算法', 'ratio': 0.8888888888888888}, {'word': '蒙特卡洛算法', 'ratio': 0.1111111111111111}]",蒙特卡罗算法,{},[]
489,489,Monte Carlo approximation,蒙特卡罗近似,0.8888888888888888,9,"[{'word': '蒙特卡罗近似', 'ratio': 0.8888888888888888}, {'word': '蒙特卡洛近似', 'ratio': 0.1111111111111111}]",蒙特卡罗近似,{},[]
490,490,Monte Carlo estimate,蒙特卡罗估计,0.9,10,"[{'word': '蒙特卡罗估计', 'ratio': 0.9}, {'word': '蒙特卡洛估计', 'ratio': 0.1}]",蒙特卡罗估计,{},[]
491,491,Monte Carlo estimation,蒙特卡洛估计,0.1,10,"[{'word': '蒙特卡罗估计', 'ratio': 0.8}, {'word': '蒙特卡洛估计', 'ratio': 0.1}, {'word': '蒙特卡罗估算', 'ratio': 0.1}]",蒙特卡罗估计,{},[]
492,492,Monte Carlo estimator,蒙特卡罗估计器,0.9,10,"[{'word': '蒙特卡罗估计器', 'ratio': 0.9}, {'word': '蒙特卡洛估计器', 'ratio': 0.1}]",蒙特卡罗估计器,{},[]
493,493,Monte Carlo method,蒙特卡罗方法,0.9,10,"[{'word': '蒙特卡罗方法', 'ratio': 0.9}, {'word': '蒙特卡洛方法', 'ratio': 0.1}]",蒙特卡罗方法,{},[]
494,494,Monte Carlo sample,蒙特卡洛样本,0.1,10,"[{'word': '蒙特卡罗样本', 'ratio': 0.9}, {'word': '蒙特卡洛样本', 'ratio': 0.1}]",蒙特卡罗样本,{},[]
495,495,Monte Carlo search,蒙特卡罗搜索,0.6666666666666666,9,"[{'word': '蒙特卡罗搜索', 'ratio': 0.6666666666666666}, {'word': '蒙特卡洛搜索', 'ratio': 0.3333333333333333}]",蒙特卡罗搜索,{},[]
496,496,Monte Carlo simulation,蒙特卡罗模拟,0.6666666666666666,9,"[{'word': '蒙特卡罗模拟', 'ratio': 0.6666666666666666}, {'word': '蒙特卡洛模拟', 'ratio': 0.3333333333333333}]",蒙特卡罗模拟,{},[]
497,497,Monte-Carlo return,蒙特卡罗回报,0.6666666666666666,9,"[{'word': '蒙特卡罗回报', 'ratio': 0.6666666666666666}, {'word': '蒙特卡洛回报', 'ratio': 0.3333333333333333}]",蒙特卡罗回报,{},[]
498,498,Moore-Penrose pseudo-inverse,摩尔-彭罗斯伪逆,0.2222222222222222,9,"[{'word': '摩尔-彭若斯伪逆', 'ratio': 0.3333333333333333}, {'word': '摩尔-彭罗斯伪逆', 'ratio': 0.2222222222222222}, {'word': '穆尔-彭若斯伪逆', 'ratio': 0.2222222222222222}, {'word': '广义逆', 'ratio': 0.1111111111111111}, {'word': '莫尔-彭罗斯伪逆', 'ratio': 0.1111111111111111}]",摩尔-彭若斯伪逆,"1. Rank: 摩尔-彭若斯伪逆, 摩尔-彭罗斯伪逆, 穆尔-彭若斯伪逆, 莫尔-彭罗斯伪逆, 广义逆

2. Explanation: The term ""摩尔-彭若斯伪逆"" is the best fit because it accurately retains the names ""Moore"" and ""Penrose"" in a transliterated form that is widely recognized in the mathematical and AI communities. The use of ""彭若斯"" for ""Penrose"" is particularly important as it is the standard transliteration used in academic literature, ensuring semantic accuracy and contextual fit. 

The second candidate, ""摩尔-彭罗斯伪逆,"" is also a strong contender, but ""彭罗斯"" is less commonly used than ""彭若斯"" in this context. The third candidate, ""穆尔-彭若斯伪逆,"" introduces a different transliteration for ""Moore,"" which may lead to confusion. The fourth candidate, ""莫尔-彭罗斯伪逆,"" similarly alters the transliteration of ""Moore"" and is less preferred. Finally, ""广义逆"" translates to ""generalized inverse,"" which is a broader term and does not specifically refer to the Moore-Penrose pseudo-inverse, making it the least suitable choice in this context. 

Overall, the first option is the most semantically accurate and contextually appropriate for the AI domain, where precise terminology is crucial.","['Moore-Penrose pseudoinverse', 'Moore-Penrose pseudoinverse', 'Moore-Penrose pseudoinverse', 'Generalized inverse', 'Mohr-Penrose pseudoinverse']"
499,499,Morfessor,Morfessor,0.4444444444444444,9,"[{'word': 'Morfessor', 'ratio': 0.4444444444444444}, {'word': 'Morfessor形态分析器', 'ratio': 0.2222222222222222}, {'word': '莫費索', 'ratio': 0.1111111111111111}, {'word': '莫费索', 'ratio': 0.1111111111111111}, {'word': '形态分析器', 'ratio': 0.1111111111111111}]",Morfessor形态分析器,"1. Rank: Morfessor形态分析器, 形态分析器, 莫费索, 莫費索, Morfessor

2. Explanation: The term ""Morfessor形态分析器"" is the best fit because it retains the original name ""Morfessor"" while also providing a clear and accurate description of its function as a ""morphology analyzer"" in Chinese. This is crucial in the AI domain, where specific terminology often needs to convey both the name and the purpose of the system. The back translation ""Morfessor Morphology Analyzer"" accurately reflects this dual purpose, making it contextually appropriate for discussions about AI systems in linguistic analysis. The other candidates, while they may be accurate in terms of transliteration or partial meaning, do not provide the same level of clarity and specificity needed in a technical context.","['MO RFE fashion or', 'Morfessor Morphology Analyzer', 'mofeso', 'mofeso', 'Morphology Analyzer']"
500,500,Multi-Task Learning,多任务学习,1.0,9,"[{'word': '多任务学习', 'ratio': 1.0}]",多任务学习,{},[]
501,501,Multidimensional Quality metric,多维质量度量,0.3333333333333333,9,"[{'word': '多维质量指标', 'ratio': 0.6666666666666666}, {'word': '多维质量度量', 'ratio': 0.3333333333333333}]",多维质量指标,{},[]
502,502,Multidimensional Scaling,多维尺度分析,0.6666666666666666,9,"[{'word': '多维尺度分析', 'ratio': 0.6666666666666666}, {'word': '多维尺度法', 'ratio': 0.3333333333333333}]",多维尺度分析,{},[]
503,503,Multiple Choice,多选题,0.0,9,"[{'word': '多项选择', 'ratio': 0.8888888888888888}, {'word': '优化器多项选择', 'ratio': 0.1111111111111111}]",多项选择,{},[]
504,504,Mutual Information,互信息,1.0,9,"[{'word': '互信息', 'ratio': 1.0}]",互信息,{},[]
505,505,N-gram,N元语法,0.3333333333333333,9,"[{'word': 'N-gram', 'ratio': 0.4444444444444444}, {'word': 'N元语法', 'ratio': 0.3333333333333333}, {'word': '', 'ratio': 0.1111111111111111}, {'word': 'n元组', 'ratio': 0.1111111111111111}]",N-gram,"1. Rank: N-gram, N元语法, n元组, 

2. Explanation: The term ""N-gram"" is the most accurate translation because it retains the original English term, which is widely recognized in the AI and natural language processing communities. This term is commonly used in academic literature and technical discussions, making it the best fit for contextual usage. 

The second candidate, ""N元语法"" (N-yuán yǔfǎ), translates to ""N-gram grammar,"" which adds an unnecessary grammatical connotation that may not align with the intended meaning in the context of semantic units and representation. 

The third candidate, ""n元组"" (n-yuán zǔ), translates to ""n-tuple,"" which is a mathematical term that does not accurately convey the specific concept of N-grams in language processing. 

Overall, ""N-gram"" is the most semantically accurate and contextually appropriate choice for the AI domain, as it directly corresponds to the established terminology used in the field.","['N-gram', 'N-gram', 'N-gram', 'n-tuple']"
506,506,Nadaraya-Watson estimator,纳德拉亚-沃森估计器,0.0,10,"[{'word': '纳达拉亚-沃森估计器', 'ratio': 0.4}, {'word': '纳达拉亚-沃森估计量', 'ratio': 0.2}, {'word': 'Nadaraya-Watson 估计量', 'ratio': 0.1}, {'word': '纳达拉亚-瓦森估计器', 'ratio': 0.1}, {'word': 'Nadaraya-Watson估计量', 'ratio': 0.1}, {'word': 'Nadaraya-Watson估计器', 'ratio': 0.1}]",纳达拉亚-沃森估计器,"1. Rank: 纳达拉亚-沃森估计器, 纳达拉亚-沃森估计量, Nadaraya-Watson 估计量, Nadaraya-Watson估计量, Nadaraya-Watson估计器, 纳达拉亚-瓦森估计器

2. Explanation: The term ""纳达拉亚-沃森估计器"" is the best fit because it accurately retains the original names ""Nadaraya"" and ""Watson"" while using the term ""估计器,"" which is commonly used in the AI and statistical context to refer to an estimator or a method that produces estimates. The use of ""估计器"" aligns well with the technical nature of the term, as it suggests a specific tool or mechanism for estimation, which is essential in the context of the Nadaraya-Watson estimator's function. 

In contrast, ""纳达拉亚-沃森估计量"" and ""Nadaraya-Watson 估计量"" use ""估计量,"" which can imply a broader concept of an estimate rather than a specific estimator, making them slightly less precise in this context. The other candidates, such as ""纳达拉亚-瓦森估计器,"" introduce inaccuracies in the transliteration of the names, which is critical in maintaining the integrity of the term in the AI domain. Therefore, the first option is the most semantically accurate and contextually appropriate.","['Nadalaya-Watson estimator', 'Nadalaya-Watson estimator', 'Nadaraya-Watson estimator', 'Nadalaya-Wassen estimator', 'Nadaraya-Watson estimator', 'Nadaraya-Watson estimator']"
507,507,Naive Bayes,朴素贝叶斯,1.0,10,"[{'word': '朴素贝叶斯', 'ratio': 1.0}]",朴素贝叶斯,{},[]
508,508,Naive Bayes classifier,朴素贝叶斯分类器,1.0,10,"[{'word': '朴素贝叶斯分类器', 'ratio': 1.0}]",朴素贝叶斯分类器,{},[]
509,509,Named Entity Recognition,命名实体识别,1.0,10,"[{'word': '命名实体识别', 'ratio': 1.0}]",命名实体识别,{},[]
510,510,Nash equilibria,纳什均衡,1.0,10,"[{'word': '纳什均衡', 'ratio': 1.0}]",纳什均衡,{},[]
511,511,Nash welfare,纳什福利,1.0,10,"[{'word': '纳什福利', 'ratio': 1.0}]",纳什福利,{},[]
512,512,Natural Language Generation,自然语言生成,1.0,10,"[{'word': '自然语言生成', 'ratio': 1.0}]",自然语言生成,{},[]
513,513,Natural Language Inference,自然语言推理,0.9,10,"[{'word': '自然语言推理', 'ratio': 0.9}, {'word': '自然语言推断', 'ratio': 0.1}]",自然语言推理,{},[]
514,514,Natural Language Processing,自然语言处理,0.9,10,"[{'word': '自然语言处理', 'ratio': 0.9}, {'word': 'Natural Language Processing', 'ratio': 0.1}]",自然语言处理,{},[]
515,515,Natural Language Understanding,自然语言理解,1.0,10,"[{'word': '自然语言理解', 'ratio': 1.0}]",自然语言理解,{},[]
516,516,Natural question,自然问题,1.0,7,"[{'word': '自然问题', 'ratio': 1.0}]",自然问题,{},[]
517,517,Naïve Bayes,朴素贝叶斯,1.0,7,"[{'word': '朴素贝叶斯', 'ratio': 1.0}]",朴素贝叶斯,{},[]
518,518,Nearest Neighbor,最近邻,1.0,7,"[{'word': '最近邻', 'ratio': 1.0}]",最近邻,{},[]
519,519,Nesterov momentum,涅斯特罗夫动量,0.0,7,"[{'word': 'Nesterov动量', 'ratio': 0.42857142857142855}, {'word': 'Nesterov 动量', 'ratio': 0.2857142857142857}, {'word': 'Nesterov 動量', 'ratio': 0.14285714285714285}, {'word': 'ネステロフの勢い', 'ratio': 0.14285714285714285}]",Nesterov动量,"1. Rank: Nesterov动量, Nesterov 动量, Nesterov 動量, ネステロフの勢い

2. Explanation: The first translation, ""Nesterov动量,"" is the best fit because it retains the original name ""Nesterov"" in its proper form while using the simplified Chinese term ""动量"" (momentum), which is widely recognized in the AI and machine learning community. This term is semantically accurate and contextually appropriate, as it directly relates to the concept of momentum in optimization algorithms. The second option, ""Nesterov 动量,"" is also a good choice but includes a space that is not typically used in technical terms in Chinese. The third option, ""Nesterov 動量,"" uses traditional Chinese characters, which may not be as accessible to the broader audience in the AI field, particularly in mainland China where simplified characters are standard. The last option, ""ネステロフの勢い,"" is a transliteration in Japanese, which is not relevant in the context of Chinese terminology and does not fit the requirement for a Chinese translation. Thus, ""Nesterov动量"" is the most appropriate choice for its clarity, accuracy, and adherence to the conventions of the AI domain.","['Nesterov momentum', 'Nesterov Momentum', 'Nesterov Momentum', 'ネステロフの时い']"
520,520,Neural Information Processing Systems,NeurIPS,0.0,7,"[{'word': '神经信息处理系统', 'ratio': 0.8571428571428571}, {'word': '神経情報処理システム', 'ratio': 0.14285714285714285}]",神经信息处理系统,{},[]
521,521,Neural Machine Translation,神经机器翻译,1.0,7,"[{'word': '神经机器翻译', 'ratio': 1.0}]",神经机器翻译,{},[]
522,522,Neural Network,神经网络,1.0,7,"[{'word': '神经网络', 'ratio': 1.0}]",神经网络,{},[]
523,523,Neural Radiance field,神经辐射场,1.0,7,"[{'word': '神经辐射场', 'ratio': 1.0}]",神经辐射场,{},[]
524,524,Neural volume,神经体积,1.0,7,"[{'word': '神经体积', 'ratio': 1.0}]",神经体积,{},[]
525,525,Newton method,牛顿法,0.7142857142857143,7,"[{'word': '牛顿法', 'ratio': 0.7142857142857143}, {'word': '牛顿方法', 'ratio': 0.2857142857142857}]",牛顿法,{},[]
526,526,Newton's method,牛顿法,1.0,9,"[{'word': '牛顿法', 'ratio': 1.0}]",牛顿法,{},[]
527,527,Nom-Bank,名词论元库,0.0,9,"[{'word': '名词库', 'ratio': 0.7777777777777778}, {'word': '名词银行', 'ratio': 0.1111111111111111}, {'word': '语料库', 'ratio': 0.1111111111111111}]",名词库,{},[]
528,528,Non-maximum suppression,非极大值抑制,0.6666666666666666,9,"[{'word': '非极大值抑制', 'ratio': 0.6666666666666666}, {'word': '非极大抑制', 'ratio': 0.3333333333333333}]",非极大值抑制,{},[]
529,529,Normalization,标准化,0.0,9,"[{'word': '归一化', 'ratio': 1.0}]",归一化,{},[]
530,530,Nucleus Sampling,核采样,0.5555555555555556,9,"[{'word': '核采样', 'ratio': 0.5555555555555556}, {'word': '核心采样', 'ratio': 0.2222222222222222}, {'word': '核心抽样', 'ratio': 0.2222222222222222}]",核采样,{},[]
531,531,Nyström approximation,奈斯特伦近似,0.0,6,"[{'word': 'Nyström 近似', 'ratio': 0.5}, {'word': 'Nyström近似', 'ratio': 0.3333333333333333}, {'word': '奈斯特罗姆近似', 'ratio': 0.16666666666666666}]",Nyström 近似,{},[]
532,532,Object Localization,目标定位,0.6666666666666666,6,"[{'word': '目标定位', 'ratio': 0.6666666666666666}, {'word': '物体定位', 'ratio': 0.3333333333333333}]",目标定位,{},[]
533,533,Object detection,目标检测,0.6666666666666666,6,"[{'word': '目标检测', 'ratio': 0.6666666666666666}, {'word': '物体检测', 'ratio': 0.3333333333333333}]",目标检测,{},[]
534,534,Open Information Extraction,开放信息抽取,0.5,6,"[{'word': '开放信息抽取', 'ratio': 0.5}, {'word': '开放信息提取', 'ratio': 0.5}]",开放信息抽取,{},[]
535,535,Optimization Problem,优化问题,1.0,10,"[{'word': '优化问题', 'ratio': 1.0}]",优化问题,{},[]
536,536,Ornstein-Uhlenbeck,奥恩斯坦-乌伦贝克过程,0.0,10,"[{'word': '奥恩斯坦-乌伦贝克', 'ratio': 0.9}, {'word': 'Ornstein-Uhlenbeck过程', 'ratio': 0.1}]",奥恩斯坦-乌伦贝克,{},[]
537,537,Pareto dominated,帕累托支配,0.7,10,"[{'word': '帕累托支配', 'ratio': 0.7}, {'word': '强帕累托主导', 'ratio': 0.1}, {'word': '强帕累托支配', 'ratio': 0.1}, {'word': '帕累托占优', 'ratio': 0.1}]",帕累托支配,{},[]
538,538,Pareto frontier,帕累托前沿,1.0,10,"[{'word': '帕累托前沿', 'ratio': 1.0}]",帕累托前沿,{},[]
539,539,Pareto optimal,帕累托最优,1.0,10,"[{'word': '帕累托最优', 'ratio': 1.0}]",帕累托最优,{},[]
540,540,Pareto optimality,帕累托最优性,1.0,10,"[{'word': '帕累托最优性', 'ratio': 1.0}]",帕累托最优性,{},[]
541,541,Pareto-efficient,帕累托有效,0.6,10,"[{'word': '帕累托有效', 'ratio': 0.6}, {'word': '帕累托高效', 'ratio': 0.3}, {'word': '帕累托效率', 'ratio': 0.1}]",帕累托有效,{},[]
542,542,Partially Observable Markov Decision Process,部分可观测马尔可夫决策过程,0.6,10,"[{'word': '部分可观测马尔可夫决策过程', 'ratio': 0.6}, {'word': '部分可观察马尔可夫决策过程', 'ratio': 0.3}, {'word': '部分可观察的马尔可夫决策过程', 'ratio': 0.1}]",部分可观测马尔可夫决策过程,{},[]
543,543,Parzen window,帕尔森窗,0.0,10,"[{'word': '帕尔岑窗口', 'ratio': 0.3}, {'word': 'Parzen窗口', 'ratio': 0.2}, {'word': '帕尔岑窗', 'ratio': 0.2}, {'word': '**帕岑窗口', 'ratio': 0.1}, {'word': '巴岱斯窗', 'ratio': 0.1}, {'word': '帕尔曾窗', 'ratio': 0.1}]","""Parzen窗口""","1. Rank: ""Parzen窗口"", ""帕尔岑窗口"", ""帕尔曾窗"", ""帕尔岑窗"", ""**帕岑窗口"", ""巴岱斯窗""

2. Explanation: The term ""Parzen窗口"" is the best fit because it retains the original name ""Parzen"" in its English form, which is crucial in the AI domain where specific algorithms and methods are often named after their creators. This helps maintain clarity and recognition within the field. The use of ""窗口"" (window) is appropriate as it directly translates the concept of a ""window"" in the context of statistical methods. 

The second choice, ""帕尔岑窗口,"" is a close transliteration of ""Parzen"" and is also semantically accurate, but it does not preserve the original English name as effectively as ""Parzen窗口."" The other candidates either deviate from the original name or introduce inaccuracies, such as ""巴岱斯窗"" (Badais window), which is unrelated to the term in question. Therefore, ""Parzen窗口"" is the most contextually and semantically accurate choice for the AI terminology.","['parzen window', 'Parzen window', 'parzen window', '**Patzen Window', 'Badais window', 'parzen window']"
544,544,Pearson correlation,皮尔逊相关系数,0.2222222222222222,9,"[{'word': '皮尔逊相关', 'ratio': 0.4444444444444444}, {'word': '皮尔逊相关性', 'ratio': 0.3333333333333333}, {'word': '皮尔逊相关系数', 'ratio': 0.2222222222222222}]",皮尔逊相关,"1. Rank: 皮尔逊相关, 皮尔逊相关性, 皮尔逊相关系数

2. Explanation: The term ""皮尔逊相关"" (Pearson correlation) is the best fit because it directly translates the concept of correlation associated with Pearson without adding unnecessary complexity. In the context of statistical analysis, ""相关"" (correlation) is a widely accepted term that accurately conveys the relationship being measured. 

""皮尔逊相关性"" (Pearson correlation coefficient) introduces the term ""相关性,"" which can imply a broader concept of correlation that may not be as precise in statistical contexts. While it is still accurate, it is less commonly used in the specific context of Pearson's statistical measure.

""皮尔逊相关系数"" (Pearson correlation coefficient) is also a valid term, but it explicitly refers to the coefficient, which is a specific numerical value derived from the correlation. In the context provided, where the focus is on the concept of correlation rather than the coefficient itself, this term may be overly specific and less fitting.

Overall, ""皮尔逊相关"" is concise, contextually appropriate, and aligns well with standard terminology used in the AI and statistical domains.","['Pearson correlation', 'Pearson correlation', 'Pearson correlation coefficient']"
545,545,Pearson correlation coefficient,皮尔逊相关系数,1.0,9,"[{'word': '皮尔逊相关系数', 'ratio': 1.0}]",皮尔逊相关系数,{},[]
546,546,Pearson's correlation,皮尔逊相关系数,0.0,9,"[{'word': '皮尔逊相关', 'ratio': 0.4444444444444444}, {'word': '皮尔逊相关性', 'ratio': 0.3333333333333333}, {'word': '皮尔逊的相关性', 'ratio': 0.2222222222222222}]",皮尔逊的相关性,"1. Rank: 皮尔逊的相关性, 皮尔逊相关性, 皮尔逊相关

2. Explanation: The term ""皮尔逊的相关性"" (Pearson's correlation) is the best fit because it retains the possessive form ""的"" which indicates that the correlation is specifically attributed to Pearson, thus maintaining the original context and meaning of the term. This is particularly important in the AI domain where precision in terminology is crucial for clarity and understanding. The second option, ""皮尔逊相关性"" (Pearson correlation), is also accurate but slightly less formal as it omits the possessive, which can lead to ambiguity in some contexts. The third option, ""皮尔逊相关"" (Pearson correlation), is the least preferred as it is more informal and does not explicitly convey the concept of correlation as a measure, which is essential in the context of statistical analysis in AI. Therefore, the first option is the most semantically accurate and contextually appropriate for the AI domain.","['Pearson correlation', 'Pearson correlation', ""Pearson's correlation""]"
547,547,Pearson's correlation coefficient,皮尔逊相关系数,0.6666666666666666,9,"[{'word': '皮尔逊相关系数', 'ratio': 0.6666666666666666}, {'word': '皮尔逊的相关系数', 'ratio': 0.3333333333333333}]",皮尔逊相关系数,{},[]
548,548,Pearson's r,皮尔逊相关系数,0.0,9,"[{'word': '皮尔逊r', 'ratio': 0.5555555555555556}, {'word': '皮尔逊r系数', 'ratio': 0.2222222222222222}, {'word': '皮尔逊 r 值', 'ratio': 0.1111111111111111}, {'word': '皮尔逊 r 系数', 'ratio': 0.1111111111111111}]",皮尔逊r,{},[]
549,549,Pearson's r correlation,皮尔逊相关系数,0.7777777777777778,9,"[{'word': '皮尔逊相关系数', 'ratio': 0.7777777777777778}, {'word': '皮尔逊r相关性', 'ratio': 0.1111111111111111}, {'word': '皮尔逊 r 相关性', 'ratio': 0.1111111111111111}]",皮尔逊相关系数,{},[]
550,550,Penn English Treebank,宾州英语树库,0.3333333333333333,9,"[{'word': '宾夕法尼亚英语树库', 'ratio': 0.4444444444444444}, {'word': '宾州英语树库', 'ratio': 0.3333333333333333}, {'word': '佩恩英语树库', 'ratio': 0.1111111111111111}, {'word': '宾夕法尼亚大学英语树库', 'ratio': 0.1111111111111111}]",宾州英语树库,"1. Rank: 宾州英语树库, 宾夕法尼亚英语树库, 宾夕法尼亚大学英语树库, 佩恩英语树库

2. Explanation: The term ""宾州英语树库"" (Penn English Treebank) is the best fit because it accurately captures the essence of the original term while maintaining semantic clarity and contextual relevance. ""宾州"" is a commonly used abbreviation for ""宾夕法尼亚"" (Pennsylvania) in both academic and colloquial contexts, making it easily recognizable to those familiar with the field. 

The second option, ""宾夕法尼亚英语树库,"" while accurate, is longer and less commonly used in practice. The third option, ""宾夕法尼亚大学英语树库,"" explicitly mentions the university, which is not necessary for understanding the term in this context and could lead to confusion, as it implies a focus on the institution rather than the linguistic resource itself. Lastly, ""佩恩英语树库"" (Payne English Treebank) is incorrect as it misrepresents the name, leading to a significant loss of semantic accuracy. 

In the AI domain, where precision and clarity are crucial, ""宾州英语树库"" stands out as the most effective translation.","['Pennsylvania English Treebank', 'Penn English Treebank', 'Payne English Treebank', 'University of Pennsylvania English Treebank']"
551,551,Penn Treebank corpus,宾夕法尼亚树库语料库,0.4444444444444444,9,"[{'word': '宾夕法尼亚树库语料库', 'ratio': 0.4444444444444444}, {'word': '宾州树库语料库', 'ratio': 0.2222222222222222}, {'word': '佩恩树库语料库', 'ratio': 0.1111111111111111}, {'word': '宾州英语树库', 'ratio': 0.1111111111111111}, {'word': '宾夕法尼亚大学树库语料库', 'ratio': 0.1111111111111111}]",宾州树库语料库,"1. Rank: 宾州树库语料库, 宾夕法尼亚树库语料库, 宾夕法尼亚大学树库语料库, 宾州英语树库, 佩恩树库语料库

2. Explanation: The term ""宾州树库语料库"" (Penn Treebank Corpus) is the best fit because it accurately retains the essence of the original term while using ""宾州"" (Penn) as a widely recognized abbreviation for Pennsylvania in Chinese. This is important in the AI domain, where brevity and clarity are valued. The back translation aligns closely with the original term, ensuring semantic accuracy. 

The second option, ""宾夕法尼亚树库语料库,"" while also accurate, is longer and less commonly used in the context of the Treebank corpus, which is typically referred to by its abbreviated form. The third option, ""宾夕法尼亚大学树库语料库,"" adds unnecessary detail by including ""大学"" (University), which is not essential for understanding the corpus in this context. 

The fourth option, ""宾州英语树库,"" introduces a specific language focus (English) that is not present in the original term, which could lead to confusion. Lastly, ""佩恩树库语料库"" (Payne Treebank Corpus) is incorrect as it misrepresents the name, leading to a significant loss of semantic accuracy. Thus, ""宾州树库语料库"" is the most contextually appropriate and semantically accurate choice.","['Pennsylvania Treebank Corpus', 'Penn Treebank Corpus', 'Payne Treebank Corpus', 'Penn English Treebank', 'University of Pennsylvania Treebank Corpus']"
552,552,Perceptron,感知机,0.2222222222222222,9,"[{'word': '感知器', 'ratio': 0.7777777777777778}, {'word': '感知机', 'ratio': 0.2222222222222222}]",感知器,{},[]
553,553,Perplexity,困惑度,0.8888888888888888,9,"[{'word': '困惑度', 'ratio': 0.8888888888888888}, {'word': '困惑', 'ratio': 0.1111111111111111}]",困惑度,{},[]
554,554,Perron-Frobenius theorem,佩伦-弗罗贝尼乌斯定理,0.0,9,"[{'word': '佩龙-弗罗贝尼乌斯定理', 'ratio': 0.7777777777777778}, {'word': '定理', 'ratio': 0.1111111111111111}, {'word': '佩龙-弗罗贝纽斯定理', 'ratio': 0.1111111111111111}]",佩龙-弗罗贝尼乌斯定理,{},[]
555,555,Pinsker's inequality,平斯克不等式,0.7777777777777778,9,"[{'word': '平斯克不等式', 'ratio': 0.7777777777777778}, {'word': '不等式', 'ratio': 0.1111111111111111}, {'word': '品斯基不等式', 'ratio': 0.1111111111111111}]",平斯克不等式,{},[]
556,556,Pitman-Yor Process,皮特曼-约尔过程,0.5555555555555556,9,"[{'word': '皮特曼-约尔过程', 'ratio': 0.5555555555555556}, {'word': '皮特曼-约过程', 'ratio': 0.3333333333333333}, {'word': 'Pitman-Yor 过程', 'ratio': 0.1111111111111111}]",皮特曼-约尔过程,{},[]
557,557,Platt scaling,Platt缩放,0.3333333333333333,9,"[{'word': '普拉特缩放', 'ratio': 0.5555555555555556}, {'word': 'Platt缩放', 'ratio': 0.3333333333333333}, {'word': 'Platt 标定', 'ratio': 0.1111111111111111}]",普拉特缩放,{},[]
558,558,Pointwise Mutual Information,点互信息,0.1111111111111111,9,"[{'word': '逐点互信息', 'ratio': 0.5555555555555556}, {'word': '点对点互信息', 'ratio': 0.3333333333333333}, {'word': '点互信息', 'ratio': 0.1111111111111111}]",逐点互信息,{},[]
559,559,Poisson distribution,泊松分布,1.0,10,"[{'word': '泊松分布', 'ratio': 1.0}]",泊松分布,{},[]
560,560,Poisson matting,泊松抠图,1.0,10,"[{'word': '泊松抠图', 'ratio': 1.0}]",泊松抠图,{},[]
561,561,Poisson model,泊松模型,1.0,10,"[{'word': '泊松模型', 'ratio': 1.0}]",泊松模型,{},[]
562,562,Poisson point process,泊松点过程,1.0,10,"[{'word': '泊松点过程', 'ratio': 1.0}]",泊松点过程,{},[]
563,563,Poisson process,泊松过程,1.0,10,"[{'word': '泊松过程', 'ratio': 1.0}]",泊松过程,{},[]
564,564,Poisson random variable,泊松随机变量,1.0,10,"[{'word': '泊松随机变量', 'ratio': 1.0}]",泊松随机变量,{},[]
565,565,Poisson rate,泊松速率,0.0,10,"[{'word': '泊松率', 'ratio': 1.0}]",泊松率,{},[]
566,566,Poisson regression,泊松回归,1.0,10,"[{'word': '泊松回归', 'ratio': 1.0}]",泊松回归,{},[]
567,567,Poisson sampling,泊松抽样,0.4,10,"[{'word': '泊松采样', 'ratio': 0.6}, {'word': '泊松抽样', 'ratio': 0.4}]",泊松采样,{},[]
568,568,Potts model,波茨模型,0.8,10,"[{'word': '波茨模型', 'ratio': 0.8}, {'word': 'Potts模型', 'ratio': 0.2}]",波茨模型,{},[]
569,569,Precision,精准度,0.0,9,"[{'word': '精确率', 'ratio': 0.5555555555555556}, {'word': '精确度', 'ratio': 0.3333333333333333}, {'word': '精度', 'ratio': 0.1111111111111111}]",精确率,{},[]
570,570,Principal Component Analysis,主成分分析,1.0,9,"[{'word': '主成分分析', 'ratio': 1.0}]",主成分分析,{},[]
571,571,Prop-Bank,语义角色标注库,0.1111111111111111,9,"[{'word': '事件库', 'ratio': 0.2222222222222222}, {'word': 'Prop-Bank', 'ratio': 0.2222222222222222}, {'word': '角色库', 'ratio': 0.2222222222222222}, {'word': '命题库', 'ratio': 0.2222222222222222}, {'word': '语义角色标注库', 'ratio': 0.1111111111111111}]","""Prop-Bank""","1. Rank: ""Prop-Bank"", ""语义角色标注库"", ""事件库"", ""角色库"", ""命题库""

2. Explanation: The term ""Prop-Bank"" is the most accurate translation because it retains the original name of the resource, which is crucial in the AI domain where specific datasets and frameworks are often referred to by their established names. The back translation ""prop-bank"" accurately reflects the original term without altering its meaning or context. 

The second choice, ""语义角色标注库"" (Semantic role annotation library), while descriptive, is more of a functional description of what Prop-Bank does rather than a direct translation of the term itself. It may lead to confusion as it does not directly reference the established name of the resource.

The other candidates, such as ""事件库"" (event library), ""角色库"" (character library), and ""命题库"" (proposition library), do not accurately capture the essence of Prop-Bank, which specifically focuses on semantic roles associated with verbs. These terms are either too broad or misaligned with the specific function of Prop-Bank in the context of semantic role labeling in natural language processing. Thus, ""Prop-Bank"" remains the best fit for its semantic accuracy and contextual relevance in the AI domain.","['event library', 'prop-bank', 'Character library', 'proposition library', 'Semantic role annotation library']"
572,572,Proposition,命题,1.0,9,"[{'word': '命题', 'ratio': 1.0}]",命题,{},[]
573,573,Proximal Policy Optimization,近端策略优化,1.0,9,"[{'word': '近端策略优化', 'ratio': 1.0}]",近端策略优化,{},[]
574,574,Py-Torch,PyTorch,0.5,10,"[{'word': 'PyTorch', 'ratio': 0.5}, {'word': 'Py-Torch', 'ratio': 0.4}, {'word': '火炬', 'ratio': 0.1}]",PyTorch,{},[]
575,575,Q function,Q函数,0.7,10,"[{'word': 'Q函数', 'ratio': 0.7}, {'word': 'Q 函数', 'ratio': 0.3}]",Q函数,{},[]
576,576,Q value,q值,0.0,10,"[{'word': 'Q值', 'ratio': 0.7}, {'word': 'Q 值', 'ratio': 0.3}]",Q值,{},[]
577,577,Q-learning,Q-学习,0.0,10,"[{'word': 'Q学习', 'ratio': 0.7}, {'word': 'Q 学习', 'ratio': 0.3}]",Q学习,{},[]
578,578,Q-network,Q网络,0.75,8,"[{'word': 'Q网络', 'ratio': 0.75}, {'word': 'Q-网络', 'ratio': 0.125}, {'word': 'Q 网络', 'ratio': 0.125}]",Q网络,{},[]
579,579,Quality Estimation,质量估计,0.75,8,"[{'word': '质量估计', 'ratio': 0.75}, {'word': '质量评估', 'ratio': 0.25}]",质量估计,{},[]
580,580,Query expansion,查询扩展,1.0,9,"[{'word': '查询扩展', 'ratio': 1.0}]",查询扩展,{},[]
581,581,Question Answering,问答,0.6666666666666666,9,"[{'word': '问答', 'ratio': 0.6666666666666666}, {'word': '问答系统', 'ratio': 0.2222222222222222}, {'word': '问题回答', 'ratio': 0.1111111111111111}]",问答,{},[]
582,582,R-Precision,R-查准率,0.0,9,"[{'word': 'R-精确度', 'ratio': 0.3333333333333333}, {'word': 'R精度', 'ratio': 0.2222222222222222}, {'word': 'R-准确率', 'ratio': 0.2222222222222222}, {'word': 'R-精度', 'ratio': 0.1111111111111111}, {'word': 'R-精确率', 'ratio': 0.1111111111111111}]",R-精确率,"1. Rank: R-精确率, R-准确率, R-精度, R-精确度, R精度

2. Explanation: The term ""R-精确率"" is the best fit because it accurately captures the concept of ""precision"" in the context of AI metrics, particularly in information retrieval and evaluation metrics. In the AI domain, ""精确率"" (precision) is a well-established term that refers to the ratio of relevant instances retrieved by a model to the total instances retrieved. This aligns closely with the concept of R-Precision, which is a specific measure of precision based on ranked retrieval. 

The back translation ""R-accuracy"" for ""R-精确率"" is also contextually appropriate, as it maintains the focus on the accuracy of the model's output in relation to the input prompts. Other candidates like ""R-准确率"" and ""R-精度"" are less precise in conveying the specific meaning of ""precision"" in this context, as ""准确率"" can imply a broader sense of accuracy without the specific focus on precision, and ""精度"" is more general and can refer to various types of accuracy without the specific connotation of precision in retrieval tasks. 

Thus, ""R-精确率"" stands out as the most semantically accurate and contextually fitting term for the AI domain.","['R-accuracy', 'Accuracy', 'R-accuracy', 'R-accuracy', 'R-precision']"
583,583,Rademacher average,拉德马赫平均数,0.0,7,"[{'word': '拉德马赫平均', 'ratio': 0.5714285714285714}, {'word': 'Rademacher 平均', 'ratio': 0.2857142857142857}, {'word': '拉德马赫平均值', 'ratio': 0.14285714285714285}]",拉德马赫平均,{},[]
584,584,Rademacher complexity,拉德马赫复杂度,0.7142857142857143,7,"[{'word': '拉德马赫复杂度', 'ratio': 0.7142857142857143}, {'word': 'Rademacher 复杂度', 'ratio': 0.2857142857142857}]",拉德马赫复杂度,{},[]
585,585,Radon-Nikodym derivative,Radon-Nikodym 导数,0.0,7,"[{'word': '拉东-尼科迪姆导数', 'ratio': 0.7142857142857143}, {'word': 'Radon-Nikodym 导数 如果需要进一步的帮助或有其他术语需要翻译，请随时告诉我！', 'ratio': 0.14285714285714285}, {'word': 'Rademacher 复杂度', 'ratio': 0.14285714285714285}]",拉东-尼科迪姆导数,{},[]
586,586,Random Forest,随机森林,1.0,8,"[{'word': '随机森林', 'ratio': 1.0}]",随机森林,{},[]
587,587,Random Forest classifier,随机森林分类器,1.0,8,"[{'word': '随机森林分类器', 'ratio': 1.0}]",随机森林分类器,{},[]
588,588,Rank,秩,0.375,8,"[{'word': '排名', 'ratio': 0.625}, {'word': '秩', 'ratio': 0.375}]",排名,{},[]
589,589,Rao-Blackwellization,拉奥-布莱克韦尔化,0.0,8,"[{'word': 'Rao-布莱克维尔化', 'ratio': 0.25}, {'word': 'Rao-Blackwell化', 'ratio': 0.25}, {'word': '劳-布莱克威尔化', 'ratio': 0.25}, {'word': '饶-布莱克韦尔化', 'ratio': 0.125}, {'word': 'Rao-Blackwell 化', 'ratio': 0.125}]",Rao-布莱克维尔化,"1. Rank: Rao-布莱克维尔化, Rao-Blackwell化, Rao-Blackwell 化, 饶-布莱克韦尔化, 劳-布莱克威尔化

2. Explanation: The term ""Rao-布莱克维尔化"" is the best fit because it retains the original name ""Rao"" in its proper form, which is crucial in the AI domain where the names of researchers and their contributions are significant. The use of ""布莱克维尔"" accurately transliterates ""Blackwell,"" maintaining semantic accuracy. This term is widely recognized in the statistical and machine learning communities, making it contextually appropriate for discussions involving Rao-Blackwellization. The other candidates either use incorrect transliterations (like ""劳"" for ""Rao"") or do not maintain the integrity of the original name as effectively. ""Rao-Blackwell化"" and ""Rao-Blackwell 化"" are also acceptable but less common in usage compared to ""Rao-布莱克维尔化."" The term ""饶-布莱克韦尔化"" introduces a different surname ""饶,"" which is not relevant, and ""劳-布莱克威尔化"" uses ""劳,"" which is incorrect for ""Rao."" Thus, ""Rao-布莱克维尔化"" stands out as the most accurate and contextually fitting translation.","['Rao-Blackwellization', 'Rao-Blackwellization', 'Law-Blackwellization', 'Rao-Blackwellization', 'Rao-Blackwellization']"
590,590,Reading Comprehension,阅读理解,1.0,8,"[{'word': '阅读理解', 'ratio': 1.0}]",阅读理解,{},[]
591,591,Recall,召回率,1.0,9,"[{'word': '召回率', 'ratio': 1.0}]",召回率,{},[]
592,592,Receiver Operating Characteristic Curve,受试者工作特征曲线,0.1111111111111111,9,"[{'word': '接收者操作特征曲线', 'ratio': 0.7777777777777778}, {'word': '接收器操作特征曲线', 'ratio': 0.1111111111111111}, {'word': '受试者工作特征曲线', 'ratio': 0.1111111111111111}]",接收者操作特征曲线,{},[]
593,593,Recognizing Textual Entailment,识别文本蕴含关系,0.0,9,"[{'word': '识别文本蕴含', 'ratio': 0.6666666666666666}, {'word': '文本蕴含识别', 'ratio': 0.2222222222222222}, {'word': '识别文本蕴涵', 'ratio': 0.1111111111111111}]",识别文本蕴含,{},[]
594,594,Recurrent Neural Network,循环神经网络,0.8888888888888888,9,"[{'word': '循环神经网络', 'ratio': 0.8888888888888888}, {'word': '递归神经网络', 'ratio': 0.1111111111111111}]",循环神经网络,{},[]
595,595,Recurrent layer,循环层,0.8888888888888888,9,"[{'word': '循环层', 'ratio': 0.8888888888888888}, {'word': '递归层', 'ratio': 0.1111111111111111}]",循环层,{},[]
596,596,Reformer,改革者,0.2857142857142857,7,"[{'word': 'Reformer', 'ratio': 0.42857142857142855}, {'word': '改革者', 'ratio': 0.2857142857142857}, {'word': '变换器', 'ratio': 0.14285714285714285}, {'word': '', 'ratio': 0.14285714285714285}]","""Reformer""","1. Rank: ""Reformer"", ""改革者"", ""变换器"", """"

2. Explanation: The term ""Reformer"" is the best fit because it is a specific name for a model in the AI domain, particularly in the context of self-attention mechanisms. In technical literature, it is common to retain the original English term when referring to specific models or algorithms, as this helps maintain clarity and precision. The back translation of ""Reformer"" remains ""reformer,"" which accurately reflects the original term without introducing ambiguity.

The second candidate, ""改革者,"" translates to ""reformer"" in a general sense, which could imply a person who brings about reform rather than a specific AI model. This could lead to confusion in a technical context. The third candidate, ""变换器,"" translates to ""converter,"" which does not accurately capture the intended meaning of the model in the AI context. The last candidate is empty and thus not applicable.

Overall, retaining ""Reformer"" as is ensures semantic accuracy and contextual fit within the AI domain, making it the most appropriate choice.","['reformer', 'reformer', 'converter', 'converter']"
597,597,Regression,回归,1.0,7,"[{'word': '回归', 'ratio': 1.0}]",回归,{},[]
598,598,Regressor,回归器,0.5714285714285714,7,"[{'word': '回归器', 'ratio': 0.5714285714285714}, {'word': '回归模型', 'ratio': 0.42857142857142855}]",回归器,{},[]
599,599,Reinforcement Learning,强化学习,1.0,7,"[{'word': '强化学习', 'ratio': 1.0}]",强化学习,{},[]
600,600,Rejection sampling,拒绝采样,0.8571428571428571,7,"[{'word': '拒绝采样', 'ratio': 0.8571428571428571}, {'word': '拒绝采样 如果您还有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.14285714285714285}]",拒绝采样,{},[]
601,601,ResNeXt,ResNeXt,0.75,8,"[{'word': 'ResNeXt', 'ratio': 0.75}, {'word': '剩餘下一個', 'ratio': 0.125}, {'word': '', 'ratio': 0.125}]",ResNeXt,{},[]
602,602,Retrieval-Augmented Generation,检索增强生成,1.0,8,"[{'word': '检索增强生成', 'ratio': 1.0}]",检索增强生成,{},[]
603,603,Reward Function,奖励函数,1.0,8,"[{'word': '奖励函数', 'ratio': 1.0}]",奖励函数,{},[]
604,604,Rhetorical Structure Theory,修辞结构理论,1.0,8,"[{'word': '修辞结构理论', 'ratio': 1.0}]",修辞结构理论,{},[]
605,605,Ridge regression,岭回归,1.0,8,"[{'word': '岭回归', 'ratio': 1.0}]",岭回归,{},[]
606,606,Riemannian geometry,黎曼几何学,0.0,10,"[{'word': '黎曼几何', 'ratio': 1.0}]",黎曼几何,{},[]
607,607,Riemannian gradient,黎曼梯度,1.0,10,"[{'word': '黎曼梯度', 'ratio': 1.0}]",黎曼梯度,{},[]
608,608,Riemannian manifold,黎曼流形,1.0,10,"[{'word': '黎曼流形', 'ratio': 1.0}]",黎曼流形,{},[]
609,609,Robertson-Webb model,罗伯逊-韦伯模型,0.4,10,"[{'word': '罗伯逊-韦布模型', 'ratio': 0.4}, {'word': '罗伯逊-韦伯模型', 'ratio': 0.4}, {'word': 'Robertson-Webb模型', 'ratio': 0.1}, {'word': '模型', 'ratio': 0.1}]",罗伯逊-韦布模型,"1. Rank: 罗伯逊-韦布模型, 罗伯逊-韦伯模型, Robertson-Webb模型, 模型

2. Explanation: The first translation, ""罗伯逊-韦布模型,"" is the best fit because it accurately retains the original names ""Robertson"" and ""Webb"" in a way that is semantically correct and contextually appropriate for the AI domain. The use of ""韦布"" is the standard transliteration for ""Webb"" in academic and technical contexts, ensuring that readers familiar with the field will recognize the term. The second candidate, ""罗伯逊-韦伯模型,"" uses ""韦伯,"" which is a less common transliteration for ""Webb"" and may lead to confusion. The third candidate, ""Robertson-Webb模型,"" mixes English and Chinese, which is not ideal for formal writing in Chinese. Lastly, ""模型"" alone is too vague and does not convey the specific model being referenced. Therefore, the first option is the most accurate and contextually fitting translation.","['Robertson-Webb model', 'Robertson-Weber model', 'Robertson-Webb model', 'Model']"
610,610,Routing Transformer,路由变压器,0.1,10,"[{'word': '路由变换器', 'ratio': 0.8}, {'word': '路由转换器', 'ratio': 0.1}, {'word': '路由变压器', 'ratio': 0.1}]",路由变换器,{},[]
611,611,Runge-Kutta,龙格-库塔,1.0,7,"[{'word': '龙格-库塔', 'ratio': 1.0}]",龙格-库塔,{},[]
612,612,Runge-Kutta method,龙格-库塔法,0.0,7,"[{'word': '龙格-库塔方法', 'ratio': 1.0}]",龙格-库塔方法,{},[]
613,613,Rényi entropy,雷尼熵,1.0,7,"[{'word': '雷尼熵', 'ratio': 1.0}]",雷尼熵,{},[]
614,614,S node,S 节点,0.0,7,"[{'word': 'S节点', 'ratio': 1.0}]",S节点,{},[]
615,615,S-expression,S-表达式,0.0,7,"[{'word': 'S表达式', 'ratio': 1.0}]",S表达式,{},[]
616,616,Schur complement,舒尔补,0.625,8,"[{'word': '舒尔补', 'ratio': 0.625}, {'word': 'Schur 余子式', 'ratio': 0.125}, {'word': '舒尔补矩阵', 'ratio': 0.125}, {'word': '舒尔补语', 'ratio': 0.125}]",舒尔补,{},[]
617,617,Scikit-learn,Scikit-learn,0.75,8,"[{'word': 'Scikit-learn', 'ratio': 0.75}, {'word': '', 'ratio': 0.125}, {'word': '库', 'ratio': 0.125}]",Scikit-learn,{},[]
618,618,Self-Attention,自注意力,0.7,10,"[{'word': '自注意力', 'ratio': 0.7}, {'word': '自注意力机制', 'ratio': 0.2}, {'word': '逐点', 'ratio': 0.1}]",自注意力,{},[]
619,619,Self-supervised learning,自监督学习,0.9,10,"[{'word': '自监督学习', 'ratio': 0.9}, {'word': '逐点乘法', 'ratio': 0.1}]",自监督学习,{},[]
620,620,Semantic Scholar,语义学者 (Semantic Scholar),0.0,10,"[{'word': '语义学者', 'ratio': 0.8}, {'word': 'Semantic Scholar', 'ratio': 0.1}, {'word': '策略类', 'ratio': 0.1}]",语义学者,{},[]
621,621,Semantic Web,语义网,0.8,10,"[{'word': '语义网', 'ratio': 0.8}, {'word': '语义学者', 'ratio': 0.1}, {'word': '策略分布', 'ratio': 0.1}]",语义网,{},[]
622,622,Sentence-Piece,句子分词,0.0,9,"[{'word': '句片', 'ratio': 0.2222222222222222}, {'word': '句piece', 'ratio': 0.2222222222222222}, {'word': '', 'ratio': 0.1111111111111111}, {'word': '句子片段', 'ratio': 0.1111111111111111}, {'word': '在线梯度下降SentencePiece', 'ratio': 0.1111111111111111}, {'word': 'SentencePiece', 'ratio': 0.1111111111111111}, {'word': 'Sentence-Piece', 'ratio': 0.1111111111111111}]",SentencePiece,"1. Rank: SentencePiece, ""句片"", ""句子片段"", ""句piece"", ""在线梯度下降SentencePiece"", ""Sentence-Piece"", """"

2. Explanation: The term ""SentencePiece"" is the best fit because it is the official name of a specific algorithm developed for subword tokenization in natural language processing, particularly in the context of AI and machine learning. Retaining the original name ensures clarity and precision, as it directly refers to the established method without introducing ambiguity. 

The second candidate, ""句片"", translates to ""sentence piece,"" which is a literal translation but lacks the recognition and specificity of the original term. ""句子片段"" translates to ""sentence fragment,"" which could mislead readers into thinking it refers to a different concept in linguistics rather than the specific algorithm. 

The other candidates, such as ""句piece"" and ""在线梯度下降SentencePiece,"" either mix languages or add unnecessary context that detracts from the term's established meaning. ""Sentence-Piece"" is a close alternative but is less commonly used than ""SentencePiece."" Therefore, the ranking reflects the importance of using the recognized term in the AI domain for semantic accuracy and contextual fit.","['Sentence piece', 'sentence piece', 'sentence piece', 'sentence fragment', 'Online Gradient Descent SentencePiece', 'sentence piece', 'sentence-piece']"
623,623,Sentiment Analysis,情感分析,1.0,9,"[{'word': '情感分析', 'ratio': 1.0}]",情感分析,{},[]
624,624,Seq2Seq,Seq2Seq,0.0,9,"[{'word': '序列到序列', 'ratio': 1.0}]",序列到序列,{},[]
625,625,Set Cover,集合覆盖问题,0.0,9,"[{'word': '集合覆盖', 'ratio': 1.0}]",集合覆盖,{},[]
626,626,Shannon entropy,香农熵,1.0,9,"[{'word': '香农熵', 'ratio': 1.0}]",香农熵,{},[]
627,627,Sherman-Morrison formula,谢尔曼-莫里森公式,0.8571428571428571,7,"[{'word': '谢尔曼-莫里森公式', 'ratio': 0.8571428571428571}, {'word': '赫爾曼·莫里森官方', 'ratio': 0.14285714285714285}]",谢尔曼-莫里森公式,{},[]
628,628,Sherman-Morrison-Woodbury formula,谢尔曼-莫里森-伍德伯里公式,0.8571428571428571,7,"[{'word': '谢尔曼-莫里森-伍德伯里公式', 'ratio': 0.8571428571428571}, {'word': '謝爾曼-莫里森-伍德伯里官方', 'ratio': 0.14285714285714285}]",谢尔曼-莫里森-伍德伯里公式,{},[]
629,629,Siamese architecture,孪生架构,0.7142857142857143,7,"[{'word': '孪生架构', 'ratio': 0.7142857142857143}, {'word': '暹罗架构', 'ratio': 0.14285714285714285}, {'word': '孪生网络结构', 'ratio': 0.14285714285714285}]",孪生架构,{},[]
630,630,Siamese network,孪生网络,0.8571428571428571,7,"[{'word': '孪生网络', 'ratio': 0.8571428571428571}, {'word': '暹罗网络', 'ratio': 0.14285714285714285}]",孪生网络,{},[]
631,631,Sigmoid,Sigmoid函数,0.0,7,"[{'word': 'S型函数', 'ratio': 0.2857142857142857}, {'word': 'Sigmoid', 'ratio': 0.14285714285714285}, {'word': 'Sigmoid函數', 'ratio': 0.14285714285714285}, {'word': '激活函数', 'ratio': 0.14285714285714285}, {'word': 'sigmoid函数', 'ratio': 0.14285714285714285}, {'word': '或称为“ sigmoid 函数”', 'ratio': 0.14285714285714285}]",S型函数,"1. Rank: S型函数, sigmoid函数, Sigmoid, Sigmoid函數, 激活函数, 或称为“ sigmoid 函数”

2. Explanation: The term ""S型函数"" (S-shaped function) is the best fit because it accurately describes the mathematical shape of the sigmoid function, which is crucial in the context of neural networks and activation functions. This term is widely recognized in the AI and machine learning communities in Chinese-speaking regions, making it semantically accurate and contextually relevant. 

The second choice, ""sigmoid函数,"" is also a strong candidate as it retains the original English term ""sigmoid"" while providing the Chinese word for ""function,"" which is commonly used in technical discussions. However, it lacks the descriptive aspect of the shape that ""S型函数"" provides.

""Sigmoid"" and ""Sigmoid函數"" are less preferable because they do not translate the term into Chinese, which may not be as accessible to all readers. ""激活函数"" (activation function) is too broad and does not specifically refer to the sigmoid function, while ""或称为“ sigmoid 函数”"" is overly verbose and not a direct translation. 

Overall, ""S型函数"" is the most contextually appropriate and semantically accurate term for the AI domain.","['sigmoid function', 'sigmoid', 'Sigmoid function', 'activation function', 'sigmoid function', 'Or called ""sigmoid function""']"
632,632,Sigmoid function,Sigmoid函数,0.25,4,"[{'word': 'Sigmoid函数', 'ratio': 0.25}, {'word': 'シグモイド関数', 'ratio': 0.25}, {'word': '** sigmoid函数**', 'ratio': 0.25}, {'word': 'S型函数', 'ratio': 0.25}]",Sigmoid函数,"1. Rank: Sigmoid函数, S型函数, **sigmoid函数**, シグモイド関数

2. Explanation: The term ""Sigmoid函数"" is the best fit because it accurately retains the original English term ""Sigmoid"" while using the standard Chinese term ""函数"" for ""function."" This translation is semantically accurate and widely recognized in the AI and mathematical communities, ensuring clarity and understanding among professionals. 

""S型函数"" is a valid alternative, as it describes the shape of the sigmoid function (S-shaped), but it is less commonly used in the context of AI and may not be as immediately recognizable to all audiences. 

The term ""**sigmoid函数**"" uses a mix of English and Chinese, which can be less formal and may not be preferred in academic or professional settings. 

Finally, ""シグモイド関数"" is a transliteration in Japanese, which is not relevant in this context and does not serve the purpose of conveying the term to a Chinese-speaking audience. Thus, ""Sigmoid函数"" stands out as the most appropriate choice for its semantic accuracy and contextual fit in the AI domain.","['Sigmoid function', 'シグモイド关数', '**sigmoid function**', 'sigmoid function']"
633,633,Singular Value Decomposition,奇异值分解,0.75,4,"[{'word': '奇异值分解', 'ratio': 0.75}, {'word': '特異値分解', 'ratio': 0.25}]",奇异值分解,{},[]
634,634,Sinkhorn algorithm,Sinkhorn算法,0.5,4,"[{'word': 'Sinkhorn算法', 'ratio': 0.5}, {'word': 'シンクホーン アルゴリズム', 'ratio': 0.25}, {'word': '辛霍恩算法', 'ratio': 0.25}]",Sinkhorn算法,{},[]
635,635,Softmax layer,Softmax 层,0.0,4,"[{'word': 'Softmax层', 'ratio': 0.75}, {'word': 'ソフトマックス層', 'ratio': 0.25}]",Softmax层,{},[]
636,636,Sparse,稀疏,1.0,9,"[{'word': '稀疏', 'ratio': 1.0}]",稀疏,{},[]
637,637,Sparse Transformer,稀疏变压器,0.0,9,"[{'word': '稀疏变换器', 'ratio': 1.0}]",稀疏变换器,{},[]
638,638,Sparse reconstruction,稀疏重建,1.0,9,"[{'word': '稀疏重建', 'ratio': 1.0}]",稀疏重建,{},[]
639,639,Sparsemax,稀疏最大,0.0,9,"[{'word': '稀疏最大化', 'ratio': 0.5555555555555556}, {'word': '稀疏最大值', 'ratio': 0.4444444444444444}]",稀疏最大化,{},[]
640,640,Spearman correlation,斯皮尔曼相关系数,0.5555555555555556,9,"[{'word': '斯皮尔曼相关系数', 'ratio': 0.5555555555555556}, {'word': '斯皮尔曼相关性', 'ratio': 0.3333333333333333}, {'word': '稀疏最大化', 'ratio': 0.1111111111111111}]",斯皮尔曼相关系数,{},[]
641,641,Spearman rank correlation,斯皮尔曼等级相关系数,0.0,7,"[{'word': '斯皮尔曼等级相关', 'ratio': 0.7142857142857143}, {'word': '斯皮尔曼秩相关', 'ratio': 0.2857142857142857}]",斯皮尔曼等级相关,{},[]
642,642,Spearman's correlation,斯皮尔曼相关系数,0.2857142857142857,7,"[{'word': '斯皮尔曼相关', 'ratio': 0.7142857142857143}, {'word': '斯皮尔曼相关系数', 'ratio': 0.2857142857142857}]",斯皮尔曼相关,{},[]
643,643,Spearman's correlation coefficient,斯皮尔曼相关系数,1.0,7,"[{'word': '斯皮尔曼相关系数', 'ratio': 1.0}]",斯皮尔曼相关系数,{},[]
644,644,Spearman's rank correlation coefficient,斯皮尔曼等级相关系数,0.7142857142857143,7,"[{'word': '斯皮尔曼等级相关系数', 'ratio': 0.7142857142857143}, {'word': '斯皮尔曼秩相关系数', 'ratio': 0.2857142857142857}]",斯皮尔曼等级相关系数,{},[]
645,645,Squared Exponential kernel,平方指数核,1.0,7,"[{'word': '平方指数核', 'ratio': 1.0}]",平方指数核,{},[]
646,646,Stanford Parser,斯坦福句法分析器,0.0,10,"[{'word': '斯坦福解析器', 'ratio': 1.0}]",斯坦福解析器,{},[]
647,647,Stanford Question Answering Dataset,斯坦福问答数据集 (SQuAD),0.0,10,"[{'word': '斯坦福问答数据集', 'ratio': 1.0}]",斯坦福问答数据集,{},[]
648,648,Stanford Sentiment Treebank,斯坦福情感树库,1.0,10,"[{'word': '斯坦福情感树库', 'ratio': 1.0}]",斯坦福情感树库,{},[]
649,649,Stanford dependency,斯坦福依存关系,0.7,10,"[{'word': '斯坦福依存关系', 'ratio': 0.7}, {'word': '斯坦福依赖关系', 'ratio': 0.3}]",斯坦福依存关系,{},[]
650,650,Stanford dependency framework,斯坦福依存框架,0.2,10,"[{'word': '斯坦福依存关系框架', 'ratio': 0.5}, {'word': '斯坦福依赖框架', 'ratio': 0.3}, {'word': '斯坦福依存框架', 'ratio': 0.2}]",斯坦福依存关系框架,{},[]
651,651,Stanford dependency parser,斯坦福依存句法分析器,0.625,8,"[{'word': '斯坦福依存句法分析器', 'ratio': 0.625}, {'word': '斯坦福依存解析器', 'ratio': 0.375}]",斯坦福依存句法分析器,{},[]
652,652,State-of-the-art,最先进的,0.75,8,"[{'word': '最先进的', 'ratio': 0.75}, {'word': '最先进', 'ratio': 0.125}, {'word': '先进的', 'ratio': 0.125}]",最先进的,{},[]
653,653,Statistical Machine Translation,统计机器翻译,1.0,8,"[{'word': '统计机器翻译', 'ratio': 1.0}]",统计机器翻译,{},[]
654,654,Stochastic Gradient Descent,随机梯度下降,1.0,8,"[{'word': '随机梯度下降', 'ratio': 1.0}]",随机梯度下降,{},[]
655,655,Story Cloze Test,故事闭合测试,0.375,8,"[{'word': '故事闭合测试', 'ratio': 0.375}, {'word': '故事完形测试', 'ratio': 0.375}, {'word': '故事填空测试', 'ratio': 0.25}]",故事完形测试,"1. Rank: 故事完形测试, 故事闭合测试, 故事填空测试

2. Explanation: The term ""故事完形测试"" (story cloze test) is the best fit because it directly translates the English term ""Cloze"" in a way that is commonly understood in the context of language and comprehension tests. The term ""完形"" (which means ""gestalt"" or ""form"") is often used in educational and psychological contexts to refer to tests that require filling in missing information, aligning well with the concept of a cloze test. 

""故事闭合测试"" (story closure test) is less accurate because it implies a focus on the closure aspect rather than the cloze format, which is central to the test's design. While it captures some meaning, it does not convey the specific nature of the test as effectively as ""完形"".

""故事填空测试"" (story fill-in-the-blank test) is also a less precise translation, as it suggests a more general fill-in-the-blank format that may not fully encapsulate the nuances of the cloze test, which is specifically about selecting the correct ending from options rather than simply filling in blanks. 

Overall, ""故事完形测试"" maintains the semantic accuracy and contextual fit required in the AI domain, making it the most appropriate choice.","['story closure test', 'story cloze test', 'Story fill-in-the-blank test']"
656,656,Structure from motion,运动重建结构,0.0,10,"[{'word': '运动恢复结构', 'ratio': 0.4}, {'word': '从运动中重建结构', 'ratio': 0.2}, {'word': '从运动恢复结构', 'ratio': 0.1}, {'word': '从运动中恢复结构', 'ratio': 0.1}, {'word': '从运动重建', 'ratio': 0.1}, {'word': '运动重建', 'ratio': 0.1}]",从运动中重建结构,"1. Rank: 从运动中重建结构, 从运动恢复结构, 从运动中恢复结构, 运动重建, 运动恢复结构, 从运动重建

2. Explanation: The term ""从运动中重建结构"" (Reconstructing structure from motion) is the best fit because it accurately captures the essence of the original English term ""Structure from motion"" (SfM) in the context of 3D modeling. The phrase emphasizes the process of reconstruction, which is central to the SfM technique, and maintains the original meaning without introducing ambiguity. 

The other candidates, while they may convey similar ideas, either introduce inaccuracies or are less precise in the context of AI and computer vision. For example, ""运动恢复结构"" (sports recovery structure) misinterprets the term by introducing the concept of ""sports,"" which is irrelevant in this context. ""从运动恢复结构"" (Restoring structure from movement) and ""从运动中恢复结构"" (Recovering structure from movement) also imply a different focus on restoration rather than reconstruction, which is not the primary goal of SfM techniques. 

Overall, ""从运动中重建结构"" aligns closely with the technical terminology used in the AI domain, making it the most semantically accurate and contextually appropriate choice.","['sports recovery structure', 'Reconstructing structure from motion', 'Restoring structure from movement', 'Recovering structure from movement', 'Reconstruction from motion', 'motion reconstruction']"
657,657,Subgraph,子图,1.0,10,"[{'word': '子图', 'ratio': 1.0}]",子图,{},[]
658,658,Submodularity,子模性,0.2,10,"[{'word': '次模性', 'ratio': 0.7}, {'word': '子模性', 'ratio': 0.2}, {'word': '次可加性', 'ratio': 0.1}]",次模性,{},[]
659,659,Support Vector Machine,支持向量机,1.0,10,"[{'word': '支持向量机', 'ratio': 1.0}]",支持向量机,{},[]
660,660,Swin-S,Swin-S,0.6,10,"[{'word': 'Swin-S', 'ratio': 0.6}, {'word': '斯溫-S', 'ratio': 0.2}, {'word': '斯温-S', 'ratio': 0.1}, {'word': '具体含义需根据上下文确定', 'ratio': 0.1}]",Swin-S,{},[]
661,661,T5 model,T5模型,1.0,10,"[{'word': 'T5模型', 'ratio': 1.0}]",T5模型,{},[]
662,662,T5-11B,T5-11B,0.9,10,"[{'word': 'T5-11B', 'ratio': 0.9}, {'word': 'T5-11B模型', 'ratio': 0.1}]",T5-11B,{},[]
663,663,T5-11B model,T5-11B模型,1.0,10,"[{'word': 'T5-11B模型', 'ratio': 1.0}]",T5-11B模型,{},[]
664,664,T5-Large,T5-大型模型,0.0,10,"[{'word': 'T5-Large', 'ratio': 0.8}, {'word': 'T5-大號', 'ratio': 0.1}, {'word': 'T5-Large模型', 'ratio': 0.1}]",T5-Large,{},[]
665,665,T5-base,T5基础,0.0,10,"[{'word': 'T5-base', 'ratio': 0.8}, {'word': 'T5基', 'ratio': 0.1}, {'word': 'T5-base模型', 'ratio': 0.1}]",T5-base,{},[]
666,666,T5-base model,T5基础模型,0.375,8,"[{'word': 'T5基础模型', 'ratio': 0.375}, {'word': 'T5-base模型', 'ratio': 0.375}, {'word': '模型', 'ratio': 0.125}, {'word': 'T5基础型号', 'ratio': 0.125}]",T5-base模型,"1. Rank: T5-base模型, T5基础模型, T5基础型号, 模型

2. Explanation: The term ""T5-base模型"" is the best fit because it retains the original English term ""T5-base"" while accurately translating ""model"" to ""模型,"" which is the standard term used in the AI domain to refer to models. This translation maintains the specific naming convention used in the AI community, ensuring clarity and consistency. The other candidates, such as ""T5基础模型"" and ""T5基础型号,"" introduce variations that could lead to confusion. ""基础"" (basic) is less precise than ""base,"" and ""型号"" (type) is not commonly used in this context. The term ""模型"" (model) is widely recognized in AI literature, making ""T5-base模型"" the most semantically accurate and contextually appropriate choice.","['T5 base model', 'T5-base model', 'Model', 'T5 base model']"
667,667,T5-large model,T5-large 模型,0.0,8,"[{'word': 'T5大型模型', 'ratio': 0.375}, {'word': 'T5-large模型', 'ratio': 0.375}, {'word': '模型', 'ratio': 0.125}, {'word': 'T5-大型号', 'ratio': 0.125}]",T5-large模型,"1. Rank: T5-large模型, T5大型模型, T5-大型号, 模型

2. Explanation: The term ""T5-large模型"" is the best fit because it retains the original structure of the English term while accurately conveying the specific model type in the AI context. The use of ""模型"" (model) directly after ""T5-large"" maintains the technical specificity required in AI terminology. The back translation ""T5-large model"" confirms that the meaning is preserved. 

In contrast, ""T5大型模型"" translates to ""T5 large model,"" which, while still accurate, introduces a space that is not present in the original term, potentially leading to confusion in technical contexts. ""T5-大型号"" is less appropriate as ""号"" (number) is not commonly used in this context and could mislead the reader regarding the nature of the model. Lastly, ""模型"" alone is too vague and lacks the necessary specificity to convey the intended meaning of the original term. Thus, ""T5-large模型"" is the most semantically accurate and contextually appropriate choice.","['T5 large model', 'T5-large model', 'Model', 'T5-large model']"
668,668,Tanh,双曲正切 (Tanh),0.0,8,"[{'word': '双曲正切函数', 'ratio': 0.375}, {'word': '双曲正切', 'ratio': 0.25}, {'word': '泰勒近似', 'ratio': 0.125}, {'word': 'Tanh', 'ratio': 0.125}, {'word': '谭', 'ratio': 0.125}]",双曲正切函数,"1. Rank: 双曲正切函数, 双曲正切, Tanh, 泰勒近似, 谭

2. Explanation: The term ""双曲正切函数"" (hyperbolic tangent function) is the most semantically accurate and contextually appropriate translation for ""Tanh"" in the AI domain. It explicitly describes the mathematical function being referenced, which is crucial in a technical context where clarity is essential. The second option, ""双曲正切"" (hyperbolic tangent), is also a strong candidate as it conveys the same mathematical concept but lacks the specificity of being a ""function."" 

The term ""Tanh"" is a direct transliteration and is commonly used in programming and mathematical contexts, but it may not be as immediately clear to all audiences without prior knowledge. ""泰勒近似"" (Taylor approximation) is unrelated to the term ""Tanh"" and thus irrelevant in this context. Lastly, ""谭"" (Tan) is a transliteration that does not convey the mathematical meaning and is not suitable for this specific usage. Therefore, ""双曲正切函数"" is the best fit due to its clarity and precision in the AI domain.","['hyperbolic tangent function', 'hyperbolic tangent', 'Taylor approximation', 'tan or', 'Tan']"
669,669,Taylor approximation,泰勒近似,1.0,8,"[{'word': '泰勒近似', 'ratio': 1.0}]",泰勒近似,{},[]
670,670,Tensor,张量,1.0,8,"[{'word': '张量', 'ratio': 1.0}]",张量,{},[]
671,671,Text Classification,文本分类,0.9,10,"[{'word': '文本分类', 'ratio': 0.9}, {'word': '状态-动作值', 'ratio': 0.1}]",文本分类,{},[]
672,672,Text Summarization,文本摘要,0.9,10,"[{'word': '文本摘要', 'ratio': 0.9}, {'word': '状态-总结', 'ratio': 0.1}]",文本摘要,{},[]
673,673,Text-to-Text Transfer Transformer,文本到文本转换transformer,0.0,10,"[{'word': '文本到文本转换变换器', 'ratio': 0.2}, {'word': '文本到文本转换转换器', 'ratio': 0.2}, {'word': '文本到文本的迁移变换器', 'ratio': 0.1}, {'word': '文本到文本转移变换器', 'ratio': 0.1}, {'word': '文本到文本转换Transforme', 'ratio': 0.1}, {'word': '文本到文本转换器', 'ratio': 0.1}, {'word': '文本到文本的迁移转换器', 'ratio': 0.1}, {'word': '文本到文本传输转换器', 'ratio': 0.1}]",文本到文本传输转换器,"1. Rank: 文本到文本传输转换器, 文本到文本的迁移转换器, 文本到文本的迁移变换器, 文本到文本转移变换器, 文本到文本转换变换器, 文本到文本转换转换器, 文本到文本转换器, 文本到文本转换Transforme

2. Explanation: The term ""文本到文本传输转换器"" (Text-to-Text Transfer Converter) is the best fit because it accurately captures the essence of the original English term ""Text-to-Text Transfer Transformer."" The word ""传输"" (transfer) directly corresponds to ""transfer"" in the English term, which is crucial in the context of AI and machine learning, where ""transfer"" often refers to the ability to apply knowledge learned from one task to another. Additionally, ""转换器"" (converter) is a commonly used term in the AI domain, indicating a model that converts input text into output text. 

The other candidates, while they may have some semantic overlap, either use less precise terminology (like ""迁移"" which translates to ""migration"") or introduce unnecessary complexity (like ""变换器"" which translates to ""transformer"" but does not align with the specific function of the model). Therefore, ""文本到文本传输转换器"" stands out as the most contextually appropriate and semantically accurate translation for the AI domain.","['text to text converter', 'text to text converter', 'Text to text migration transformer', 'text to text transfer converter', 'Text to text conversionTransforme', 'text to text converter', 'Text to text migration converter', 'text to text transfer converter']"
674,674,Theano,Theano,0.6,10,"[{'word': 'Theano', 'ratio': 0.6}, {'word': '西阿诺', 'ratio': 0.3}, {'word': '西阿諾', 'ratio': 0.1}]",Theano,{},[]
675,675,Toeplitz matrix,托普利兹矩阵,0.4,10,"[{'word': '托普利茨矩阵', 'ratio': 0.5}, {'word': '托普利兹矩阵', 'ratio': 0.4}, {'word': 'Toeplitz矩阵', 'ratio': 0.1}]",托普利茨矩阵,{},[]
676,676,Token,词元,0.3,10,"[{'word': '令牌', 'ratio': 0.5}, {'word': '词元', 'ratio': 0.3}, {'word': '标记', 'ratio': 0.2}]",令牌,{},[]
677,677,Topic Detection and Tracking,主题检测和跟踪,0.0,10,"[{'word': '主题检测与跟踪', 'ratio': 0.7}, {'word': '主题检测与追踪', 'ratio': 0.2}, {'word': '话题检测与跟踪', 'ratio': 0.1}]",主题检测与跟踪,{},[]
678,678,Transfer learning,迁移学习,0.9,10,"[{'word': '迁移学习', 'ratio': 0.9}, {'word': '转移学习', 'ratio': 0.1}]",迁移学习,{},[]
679,679,Transformer,变换器 (Transformer),0.0,10,"[{'word': '变换器', 'ratio': 0.5}, {'word': '转换器', 'ratio': 0.3}, {'word': 'Transformer', 'ratio': 0.1}, {'word': '', 'ratio': 0.1}]",变换器,{},[]
680,680,Transformer architecture,变压器架构,0.1111111111111111,9,"[{'word': 'Transformer架构', 'ratio': 0.7777777777777778}, {'word': 'Transformer架構', 'ratio': 0.1111111111111111}, {'word': '变压器架构', 'ratio': 0.1111111111111111}]",Transformer架构,{},[]
681,681,Transformer block,Transformer 模块,0.0,9,"[{'word': 'Transformer块', 'ratio': 0.6666666666666666}, {'word': '變壓器塊', 'ratio': 0.1111111111111111}, {'word': 'Transformer模块', 'ratio': 0.1111111111111111}, {'word': '变压器块', 'ratio': 0.1111111111111111}]",Transformer块,{},[]
682,682,Transformer decoder,变压器解码器,0.1111111111111111,9,"[{'word': 'Transformer解码器', 'ratio': 0.7777777777777778}, {'word': 'transformer解碼器', 'ratio': 0.1111111111111111}, {'word': '变压器解码器', 'ratio': 0.1111111111111111}]",Transformer解码器,{},[]
683,683,Transformer encoder,变压器编码器,0.1111111111111111,9,"[{'word': 'Transformer编码器', 'ratio': 0.7777777777777778}, {'word': 'Transformer編碼器', 'ratio': 0.1111111111111111}, {'word': '变压器编码器', 'ratio': 0.1111111111111111}]",Transformer编码器,{},[]
684,684,Transformer model,变形器模型,0.0,9,"[{'word': 'Transformer模型', 'ratio': 0.8888888888888888}, {'word': '变压器型号', 'ratio': 0.1111111111111111}]",Transformer模型,{},[]
685,685,Transformer-based language model,基于Transformer的语言模型,0.5,8,"[{'word': '基于变换器的语言模型', 'ratio': 0.5}, {'word': '基于Transformer的语言模型', 'ratio': 0.5}]",基于变换器的语言模型,{},[]
686,686,Transformer-based model,基于Transformer的模型,0.5,8,"[{'word': '基于变换器的模型', 'ratio': 0.5}, {'word': '基于Transformer的模型', 'ratio': 0.5}]",基于变换器的模型,{},[]
687,687,Transformer-like,类似变压器,0.0,8,"[{'word': '类变换器的', 'ratio': 0.375}, {'word': '类Transformer的', 'ratio': 0.25}, {'word': '类Transformer', 'ratio': 0.25}, {'word': '类似变换器的', 'ratio': 0.125}]",类Transformer的,"1. Rank: 类Transformer的, 类Transformer, 类变换器的, 类变换器的

2. Explanation: The term ""类Transformer的"" is the best fit because it directly translates to ""Transformer-like"" while retaining the specific reference to the Transformer architecture, which is crucial in the AI domain. The use of ""Transformer"" in its original form ensures that the term is recognized within the context of AI and machine learning, where ""Transformer"" refers to a well-established model architecture. 

The second candidate, ""类Transformer,"" is also a strong option, but it lacks the possessive form ""的,"" which can make it slightly less formal in this context. The third candidate, ""类变换器的,"" translates to ""converter-like,"" which deviates from the specific meaning of ""Transformer"" in AI, as ""变换器"" (converter) does not convey the same technical significance. Lastly, ""类似变换器的"" translates to ""similar to a converter,"" which is too vague and does not accurately reflect the specific architecture being discussed. Therefore, ""类Transformer的"" is the most semantically accurate and contextually appropriate choice.","['converter-like', 'Transformer class', 'ClassTransformer', 'similar to a converter']"
688,688,Traveling Salesman Problem,旅行推销员问题,0.125,8,"[{'word': '旅行商问题', 'ratio': 0.875}, {'word': '旅行推销员问题', 'ratio': 0.125}]",旅行商问题,{},[]
689,689,Treebank,树库,0.8571428571428571,7,"[{'word': '树库', 'ratio': 0.8571428571428571}, {'word': '句法树库', 'ratio': 0.14285714285714285}]",树库,{},[]
690,690,Triggering Model,触发模型,1.0,7,"[{'word': '触发模型', 'ratio': 1.0}]",触发模型,{},[]
691,691,Tucker decomposition,塔克分解,0.7142857142857143,7,"[{'word': '塔克分解', 'ratio': 0.7142857142857143}, {'word': 'Tucker分解', 'ratio': 0.2857142857142857}]",塔克分解,{},[]
692,692,Turing Test,图灵测试,1.0,7,"[{'word': '图灵测试', 'ratio': 1.0}]",图灵测试,{},[]
693,693,Turing machine,图灵机,1.0,7,"[{'word': '图灵机', 'ratio': 1.0}]",图灵机,{},[]
694,694,Turing reduction,图灵归约,0.6666666666666666,9,"[{'word': '图灵归约', 'ratio': 0.6666666666666666}, {'word': '图灵约简', 'ratio': 0.3333333333333333}]",图灵归约,{},[]
695,695,U-statistic,U-统计量,0.0,9,"[{'word': 'U统计量', 'ratio': 0.8888888888888888}, {'word': 'U 统计量', 'ratio': 0.1111111111111111}]",U统计量,{},[]
696,696,Unigram,一元语法,0.3333333333333333,9,"[{'word': '单词模型', 'ratio': 0.3333333333333333}, {'word': '一元语法', 'ratio': 0.3333333333333333}, {'word': '单元词', 'ratio': 0.2222222222222222}, {'word': '单元语', 'ratio': 0.1111111111111111}]",一元语法,"1. Rank: 一元语法, 单词模型, 单元词, 单元语

2. Explanation: The term ""一元语法"" (back translation: ""monary grammar"") is the best fit for the English term ""Unigram"" in the context of AI and natural language processing. In computational linguistics, ""unigram"" specifically refers to a model that considers single words (or tokens) independently, which aligns with the concept of ""一元"" (one or single) in ""一元语法"" (one-gram grammar). This term accurately captures the statistical nature of unigrams as it relates to language modeling.

The other candidates, such as ""单词模型"" (word model), while somewhat relevant, do not convey the specific statistical aspect of unigrams as effectively. ""单元词"" (unit word) and ""单元语"" (unit language) are less precise and do not directly relate to the concept of unigrams in the context of language processing. Therefore, ""一元语法"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['word model', 'monary grammar', 'unit word', 'unit language']"
697,697,Universal dependency,通用依存关系,0.2222222222222222,9,"[{'word': '通用依赖关系', 'ratio': 0.2222222222222222}, {'word': '通用依存', 'ratio': 0.2222222222222222}, {'word': '通用依存关系', 'ratio': 0.2222222222222222}, {'word': '普遍依赖', 'ratio': 0.1111111111111111}, {'word': '通用依赖', 'ratio': 0.1111111111111111}, {'word': '普遍依赖关系', 'ratio': 0.1111111111111111}]",通用依存关系,"1. Rank: 通用依存关系, 通用依存, 通用依赖关系, 通用依赖, 普遍依赖关系, 普遍依赖, 通用依赖

2. Explanation: The term ""通用依存关系"" (universal dependency relation) is the best fit for the translation of ""Universal dependency"" in the AI domain, particularly in the context of natural language processing and syntactic analysis. This term accurately captures the semantic meaning of ""universal"" as it relates to a broad, standardized framework for representing syntactic dependencies across languages. The use of ""依存"" (dependency) is more precise than ""依赖"" (reliance), as it aligns with the established terminology in linguistic studies, particularly in the context of Universal Dependencies (UD) frameworks. 

The back translation of ""通用依存关系"" as ""universal dependencies"" maintains the intended meaning and context, making it semantically accurate. Other candidates like ""通用依赖关系"" and ""通用依赖"" are less precise because ""依赖"" can imply a more general sense of reliance rather than the specific linguistic relationship denoted by ""依存."" Therefore, ""通用依存关系"" is the most contextually appropriate choice for the AI domain.","['common dependencies', 'universal dependency', 'universal dependencies', 'universal dependence', 'Common dependencies', 'universal dependencies']"
698,698,Upper Confidence Bound,上置信界限 (Upper Confidence Bound),0.0,9,"[{'word': '上置信界', 'ratio': 0.6666666666666666}, {'word': '上置信界限', 'ratio': 0.3333333333333333}]",上置信界,{},[]
699,699,Vandermonde matrix,范德蒙矩阵,0.1,10,"[{'word': '范德蒙德矩阵', 'ratio': 0.3}, {'word': 'Vandermonde 矩阵', 'ratio': 0.3}, {'word': '范德蒙矩陣', 'ratio': 0.1}, {'word': '瓦德蒙德矩阵', 'ratio': 0.1}, {'word': '瓦尔德蒙德矩阵', 'ratio': 0.1}, {'word': '范德蒙矩阵', 'ratio': 0.1}]",范德蒙德矩阵,"1. Rank: 范德蒙德矩阵, Vandermonde 矩阵, 范德蒙矩阵, 范德蒙矩陣, 瓦尔德蒙德矩阵, 瓦德蒙德矩阵

2. Explanation: The term ""范德蒙德矩阵"" is the best fit because it accurately translates the name ""Vandermonde"" while maintaining the semantic integrity of the term in the context of linear algebra and matrix theory. This translation is widely recognized in the mathematical community and retains the original name's phonetic and semantic elements. The back translation ""Vandermonde matrix"" aligns perfectly with the English term, ensuring clarity and consistency in academic discourse. 

The second candidate, ""Vandermonde 矩阵,"" is also a good option, but it mixes English and Chinese, which may not be as preferred in formal contexts. The other candidates either contain transliteration errors or do not accurately reflect the original term, making them less suitable for use in the AI domain where precision in terminology is crucial.","['Vandermonde matrix', 'Vandermonde matrix', 'Vandermond matrix', 'Wadmund Matrix', 'Waldmund matrix', 'Vandermond matrix']"
700,700,Variational Autoencoder,变分自编码器,0.8,10,"[{'word': '变分自编码器', 'ratio': 0.8}, {'word': '變分自動編碼器', 'ratio': 0.1}, {'word': '变分自动编码器', 'ratio': 0.1}]",变分自编码器,{},[]
701,701,Vertex Cover,顶点覆盖,0.9,10,"[{'word': '顶点覆盖', 'ratio': 0.9}, {'word': '頂點覆蓋', 'ratio': 0.1}]",顶点覆盖,{},[]
702,702,Vision Transformer,视觉Transformer,0.0,10,"[{'word': '视觉变换器', 'ratio': 0.5}, {'word': '视觉转换器', 'ratio': 0.4}, {'word': '视觉变压器', 'ratio': 0.1}]",视觉变换器,{},[]
703,703,Visual Question Answering,视觉问答,1.0,10,"[{'word': '视觉问答', 'ratio': 1.0}]",视觉问答,{},[]
704,704,Viterbi,维特比,0.8,10,"[{'word': '维特比', 'ratio': 0.8}, {'word': '维特比算法', 'ratio': 0.2}]",维特比,{},[]
705,705,Viterbi algorithm,维特比算法,1.0,6,"[{'word': '维特比算法', 'ratio': 1.0}]",维特比算法,{},[]
706,706,Viterbi decoding,维特比解码,1.0,6,"[{'word': '维特比解码', 'ratio': 1.0}]",维特比解码,{},[]
707,707,Wasserstein distance,瓦瑟斯坦距离,0.8333333333333334,6,"[{'word': '瓦瑟斯坦距离', 'ratio': 0.8333333333333334}, {'word': '韦瑟斯坦距离', 'ratio': 0.16666666666666666}]",瓦瑟斯坦距离,{},[]
708,708,Weibull,威布尔,0.0,6,"[{'word': '韦布尔', 'ratio': 0.6666666666666666}, {'word': '韦布尔分布', 'ratio': 0.16666666666666666}, {'word': '韦伯尔分布', 'ratio': 0.16666666666666666}]",韦布尔,{},[]
709,709,Weisfeiler-Lehman test,维斯费勒-莱曼检验,0.0,6,"[{'word': '韦斯费勒-莱曼测试', 'ratio': 0.5}, {'word': '韦斯菲勒-莱曼测试', 'ratio': 0.3333333333333333}, {'word': '魏斯费勒-莱曼测试', 'ratio': 0.16666666666666666}]",韦斯费勒-莱曼测试,{},[]
710,710,Wiener process,维纳过程,0.5,10,"[{'word': '维纳过程', 'ratio': 0.5}, {'word': '威纳过程', 'ratio': 0.3}, {'word': '魏纳过程', 'ratio': 0.1}, {'word': '韦伯过程', 'ratio': 0.1}]",维纳过程,{},[]
711,711,Wilcoxon signed-rank test,威尔科克森符号秩检验,0.8,10,"[{'word': '威尔科克森符号秩检验', 'ratio': 0.8}, {'word': '威尔科克逊符号秩检验', 'ratio': 0.1}, {'word': '维基百科', 'ratio': 0.1}]",威尔科克森符号秩检验,{},[]
712,712,Winograd Schema,维诺格拉德模式,0.4,10,"[{'word': '维诺格拉德模式', 'ratio': 0.4}, {'word': '温诺格拉德模式', 'ratio': 0.2}, {'word': '温格拉德框架', 'ratio': 0.1}, {'word': 'Winograd 结构', 'ratio': 0.1}, {'word': '温哥华模式', 'ratio': 0.1}, {'word': '温格拉德模式', 'ratio': 0.1}]",维诺格拉德模式,"1. Rank: 维诺格拉德模式, 温诺格拉德模式, 温格拉德模式, Winograd 结构, 温哥华模式, 温格拉德框架

2. Explanation: The term ""维诺格拉德模式"" is the best fit because it accurately retains the original name ""Winograd"" while providing a clear and contextually appropriate translation of ""Schema"" as ""模式"" (mode or pattern). This translation is semantically accurate and aligns well with the AI domain, where ""模式"" is commonly used to refer to models or schemas in various contexts, including machine learning and natural language processing. 

The second candidate, ""温诺格拉德模式,"" while also retaining the name, uses a different transliteration that is less common and may not be as recognizable in the AI community. The third candidate, ""温格拉德模式,"" similarly alters the transliteration but still maintains the structure. 

The other candidates, such as ""Winograd 结构"" and ""温哥华模式,"" deviate from the original name or context, making them less suitable for the specific AI terminology. ""温格拉德框架"" introduces ""框架"" (framework), which changes the meaning and does not align with the intended use of ""Schema."" Thus, ""维诺格拉德模式"" stands out as the most accurate and contextually fitting translation.","['Winograd model', 'Winograd model', 'Weingrad framework', 'Winograd structure', 'vancouver model', 'Weingrad model']"
713,713,Winograd Schema Challenge,温罗格模式挑战,0.0,10,"[{'word': '维诺格拉德模式挑战', 'ratio': 0.4}, {'word': '温诺格拉德模式挑战', 'ratio': 0.2}, {'word': '温格拉德框架挑战', 'ratio': 0.1}, {'word': 'Winograd 结构挑战', 'ratio': 0.1}, {'word': '温哥华模式挑战', 'ratio': 0.1}, {'word': '温格拉德模式挑战', 'ratio': 0.1}]",维诺格拉德模式挑战,"1. Rank: 维诺格拉德模式挑战, 温诺格拉德模式挑战, 温格拉德模式挑战, Winograd 结构挑战, 温哥华模式挑战, 温格拉德框架挑战

2. Explanation: The term ""维诺格拉德模式挑战"" is the best fit because it accurately retains the original name ""Winograd"" while providing a clear and contextually appropriate translation of ""Schema Challenge"" as ""模式挑战"". This translation maintains semantic accuracy and is easily recognizable in the AI domain, where ""模式"" (model/schema) is commonly used in discussions of machine learning and AI benchmarks. The back translation ""Winograd model challenge"" aligns closely with the original English term, ensuring clarity and consistency. 

The second candidate, ""温诺格拉德模式挑战,"" is also a good option but uses a different transliteration of ""Winograd,"" which may not be as widely recognized. The other candidates either introduce inaccuracies in the name or use terms that are less relevant in the context of AI and commonsense reasoning challenges. For instance, ""温哥华模式挑战"" translates to ""Vancouver Model Challenge,"" which is entirely incorrect in this context.","['Winograd model challenge', 'Winograd Mode Challenge', 'Weingrad Framework Challenge', 'Winograd structural challenges', 'Vancouver Model Challenge', 'Weingrad Model Challenge']"
714,714,Winogrande,维诺大问题集,0.0,6,"[{'word': 'Winogrande', 'ratio': 0.5}, {'word': '維諾格蘭德', 'ratio': 0.16666666666666666}, {'word': 'Winogrande数据集', 'ratio': 0.16666666666666666}, {'word': 'Winogrande 数据集', 'ratio': 0.16666666666666666}]",Winogrande,{},[]
715,715,Woodbury matrix identity,伍德伯里矩阵恒等式,0.1666666666666666,6,"[{'word': 'Woodbury矩阵恒等式', 'ratio': 0.5}, {'word': '伍德伯里矩阵恒等式', 'ratio': 0.16666666666666666}, {'word': 'Woodbury 矩阵恒等式', 'ratio': 0.16666666666666666}, {'word': 'Woodbury矩阵恒等', 'ratio': 0.16666666666666666}]",Woodbury矩阵恒等式,{},[]
716,716,Word Mover's Distance,词移距离,0.3333333333333333,6,"[{'word': '词移动距离', 'ratio': 0.6666666666666666}, {'word': '词移距离', 'ratio': 0.3333333333333333}]",词移动距离,{},[]
717,717,Word2Vec,Word2Vec,0.6666666666666666,6,"[{'word': 'Word2Vec', 'ratio': 0.6666666666666666}, {'word': '詞對維', 'ratio': 0.16666666666666666}, {'word': '词向量', 'ratio': 0.16666666666666666}]",Word2Vec,{},[]
718,718,Z-score,标准分数,0.0,7,"[{'word': 'Z分数', 'ratio': 0.7142857142857143}, {'word': 'Z得分', 'ratio': 0.14285714285714285}, {'word': 'Z-score', 'ratio': 0.14285714285714285}]",Z分数,{},[]
719,719,Zipf,齐普夫分布,0.0,7,"[{'word': '齐夫', 'ratio': 0.2857142857142857}, {'word': '齐普夫', 'ratio': 0.2857142857142857}, {'word': '齊普夫', 'ratio': 0.14285714285714285}, {'word': '兹夫', 'ratio': 0.14285714285714285}, {'word': '兹甫', 'ratio': 0.14285714285714285}]",齐普夫,"1. Rank: 齐普夫, 齊普夫, 齐夫, 兹夫, 兹甫

2. Explanation: The top-ranked translations ""齐普夫"" and ""齊普夫"" are the best fit because they maintain the phonetic similarity to the original term ""Zipf"" while also being a transliteration that is commonly used in the AI and linguistic communities in China. This is important in the AI domain, where specific terms often retain their original names to ensure clarity and recognition among professionals. The use of ""齐"" (Qi) and ""普"" (Pu) closely mirrors the pronunciation of ""Zipf,"" making it easily recognizable. 

The other candidates, such as ""齐夫"" and ""兹夫,"" do not capture the full phonetic essence of ""Zipf"" and may lead to confusion, as they are less commonly associated with the term in academic contexts. ""兹甫"" is also less relevant as it does not closely resemble the original term and lacks recognition in the field. Therefore, ""齐普夫"" and ""齊普夫"" are the most semantically accurate and contextually appropriate choices for the AI domain.","['Ziff', 'Zipf', 'Zipf', 'ziv', 'Zifu']"
720,720,Zipf distribution,Zipf分布,0.1428571428571428,7,"[{'word': '齐夫分布', 'ratio': 0.2857142857142857}, {'word': '齐普夫分布', 'ratio': 0.2857142857142857}, {'word': 'Zipf分布', 'ratio': 0.14285714285714285}, {'word': '兹夫分布', 'ratio': 0.14285714285714285}, {'word': '兹甫分布', 'ratio': 0.14285714285714285}]",齐夫分布,"1. Rank: 齐夫分布, 齐普夫分布, Zipf分布, 兹夫分布, 兹甫分布

2. Explanation: The term ""齐夫分布"" is the most widely accepted and recognized translation for ""Zipf distribution"" in the Chinese academic and AI context. It retains the original name ""Zipf,"" which is crucial for semantic accuracy, especially in technical fields where the name of the distribution is tied to the work of the linguist George Zipf. This translation is not only semantically accurate but also contextually appropriate, as it is commonly used in literature discussing statistical distributions and natural language processing. 

The second candidate, ""齐普夫分布,"" while also a transliteration, is less common and may not be as readily recognized by professionals in the field. ""Zipf分布"" is a direct transliteration that lacks the contextual grounding of the original name, making it less preferable. The last two candidates, ""兹夫分布"" and ""兹甫分布,"" are inaccurate and do not convey the correct meaning or context, as they deviate from the established terminology. Thus, ""齐夫分布"" stands out as the best fit for both semantic and contextual accuracy in the AI domain.","['Zipf distribution', 'Zipf distribution', 'Zipf distribution', 'Ziff distribution', 'distribution']"
721,721,Zipf's law,Zipf定律,0.1428571428571428,7,"[{'word': '齐夫定律', 'ratio': 0.2857142857142857}, {'word': '齐普夫定律', 'ratio': 0.2857142857142857}, {'word': 'Zipf定律', 'ratio': 0.14285714285714285}, {'word': '兹夫定律', 'ratio': 0.14285714285714285}, {'word': '兹甫定律', 'ratio': 0.14285714285714285}]","""Zipf定律""","1. Rank: ""Zipf定律"", ""齐夫定律"", ""齐普夫定律"", ""兹夫定律"", ""兹甫定律""

2. Explanation: The term ""Zipf定律"" is the best fit because it retains the original name ""Zipf"" in its transliteration, which is crucial in the AI domain where specific laws and theories are often attributed to their originators. This term is widely recognized in academic and technical literature, ensuring semantic accuracy and contextual fit. The other candidates, such as ""齐夫定律"" and ""齐普夫定律,"" while they may be phonetically similar, do not maintain the integrity of the original name as effectively as ""Zipf定律."" The use of ""Zipf"" directly connects the term to the established body of work surrounding Zipf's law, making it the most appropriate choice in this context.","[""Zip's law"", ""Zipf's law"", ""Zipf's law"", ""Ziff's law"", ""Zip's law""]"
722,722,abductive explanation,溯因解释,0.5714285714285714,7,"[{'word': '溯因解释', 'ratio': 0.5714285714285714}, {'word': '诱导性解释', 'ratio': 0.14285714285714285}, {'word': '诱导解释', 'ratio': 0.14285714285714285}, {'word': '归纳解释', 'ratio': 0.14285714285714285}]",溯因解释,{},[]
723,723,ablation analysis,消融分析,1.0,7,"[{'word': '消融分析', 'ratio': 1.0}]",消融分析,{},[]
724,724,ablation experiment,消融实验,1.0,7,"[{'word': '消融实验', 'ratio': 1.0}]",消融实验,{},[]
725,725,abstraction,抽象化,0.1428571428571428,7,"[{'word': '抽象', 'ratio': 0.8571428571428571}, {'word': '抽象化', 'ratio': 0.14285714285714285}]",抽象,{},[]
726,726,abstraction heuristic,抽象启发式,1.0,10,"[{'word': '抽象启发式', 'ratio': 1.0}]",抽象启发式,{},[]
727,727,abstractive summarization,抽象摘要生成,0.0,10,"[{'word': '生成式摘要', 'ratio': 0.5}, {'word': '抽象摘要', 'ratio': 0.4}, {'word': '抽象总结', 'ratio': 0.1}]",生成式摘要,{},[]
728,728,accelerated gradient descent,加速梯度下降,1.0,10,"[{'word': '加速梯度下降', 'ratio': 1.0}]",加速梯度下降,{},[]
729,729,acceptance function,接受函数,1.0,10,"[{'word': '接受函数', 'ratio': 1.0}]",接受函数,{},[]
730,730,acceptance probability,接受概率,0.9,10,"[{'word': '接受概率', 'ratio': 0.9}, {'word': '接受函数', 'ratio': 0.1}]",接受概率,{},[]
731,731,accumulated error,累积误差,1.0,8,"[{'word': '累积误差', 'ratio': 1.0}]",累积误差,{},[]
732,732,acoustic feature,声学特征,1.0,8,"[{'word': '声学特征', 'ratio': 1.0}]",声学特征,{},[]
733,733,acoustic model,声学模型,1.0,8,"[{'word': '声学模型', 'ratio': 1.0}]",声学模型,{},[]
734,734,acquisition function,采集函数,0.75,8,"[{'word': '采集函数', 'ratio': 0.75}, {'word': '获取函数', 'ratio': 0.125}, {'word': '声学模型', 'ratio': 0.125}]",采集函数,{},[]
735,735,action classification,动作分类,0.8571428571428571,7,"[{'word': '动作分类', 'ratio': 0.8571428571428571}, {'word': '行动分类', 'ratio': 0.14285714285714285}]",动作分类,{},[]
736,736,action embedding,动作嵌入,0.8571428571428571,7,"[{'word': '动作嵌入', 'ratio': 0.8571428571428571}, {'word': '行动嵌入', 'ratio': 0.14285714285714285}]",动作嵌入,{},[]
737,737,action recognition,动作识别,0.8571428571428571,7,"[{'word': '动作识别', 'ratio': 0.8571428571428571}, {'word': '行动识别', 'ratio': 0.14285714285714285}]",动作识别,{},[]
738,738,action sequence,动作序列,0.8571428571428571,7,"[{'word': '动作序列', 'ratio': 0.8571428571428571}, {'word': '行动序列', 'ratio': 0.14285714285714285}]",动作序列,{},[]
739,739,action set,动作集,0.4285714285714285,7,"[{'word': '动作集合', 'ratio': 0.42857142857142855}, {'word': '动作集', 'ratio': 0.42857142857142855}, {'word': '行动集合', 'ratio': 0.14285714285714285}]",动作集,"1. Rank: 动作集, 动作集合, 行动集合

2. Explanation: The term ""动作集"" (action set) is the best fit for the AI domain-specific usage because it is concise and commonly used in the context of machine learning and reinforcement learning. In AI literature, ""动作集"" is frequently employed to refer to a collection of possible actions an agent can take, making it semantically accurate and contextually appropriate. 

""动作集合"" (action collection) is a more literal translation but is less commonly used in the AI field, which may lead to confusion or lack of clarity. ""行动集合"" (action collection) is also less preferred as ""行动"" (action) can imply a broader range of activities beyond the specific context of actions in AI, which typically refers to discrete choices available to an agent. Therefore, ""动作集"" is the most precise and widely recognized term in the context of AI, particularly in discussions about action spaces in reinforcement learning and multi-agent systems.","['action set', 'action set', 'action set']"
740,740,action space,动作空间,0.8571428571428571,7,"[{'word': '动作空间', 'ratio': 0.8571428571428571}, {'word': '行动空间', 'ratio': 0.14285714285714285}]",动作空间,{},[]
741,741,action-value function,动作-值函数,0.0,7,"[{'word': '动作价值函数', 'ratio': 0.5714285714285714}, {'word': '动作值函数', 'ratio': 0.2857142857142857}, {'word': '行动价值函数', 'ratio': 0.14285714285714285}]",动作价值函数,{},[]
742,742,actionability,可操作性,0.5714285714285714,7,"[{'word': '可操作性', 'ratio': 0.5714285714285714}, {'word': '可行动性', 'ratio': 0.2857142857142857}, {'word': '可执行性', 'ratio': 0.14285714285714285}]",可操作性,{},[]
743,743,activation,激活,1.0,7,"[{'word': '激活', 'ratio': 1.0}]",激活,{},[]
744,744,activation function,激活函数,1.0,7,"[{'word': '激活函数', 'ratio': 1.0}]",激活函数,{},[]
745,745,activation matrix,激活矩阵,1.0,6,"[{'word': '激活矩阵', 'ratio': 1.0}]",激活矩阵,{},[]
746,746,activation vector,激活向量,1.0,6,"[{'word': '激活向量', 'ratio': 1.0}]",激活向量,{},[]
747,747,active learning loop,主动学习循环,1.0,6,"[{'word': '主动学习循环', 'ratio': 1.0}]",主动学习循环,{},[]
748,748,active set,活跃集,0.5,6,"[{'word': '活跃集', 'ratio': 0.5}, {'word': '活动集', 'ratio': 0.5}]",活跃集,{},[]
749,749,activity detection,活动检测,1.0,6,"[{'word': '活动检测', 'ratio': 1.0}]",活动检测,{},[]
750,750,activity recognition,活动识别,1.0,7,"[{'word': '活动识别', 'ratio': 1.0}]",活动识别,{},[]
751,751,actor,行为者,0.2857142857142857,7,"[{'word': '行动者', 'ratio': 0.5714285714285714}, {'word': '行为者', 'ratio': 0.2857142857142857}, {'word': '执行者', 'ratio': 0.14285714285714285}]",行动者,{},[]
752,752,actor critic algorithm,演员评论家算法,0.0,7,"[{'word': '行为者-评论家算法', 'ratio': 0.2857142857142857}, {'word': '演员-评论员算法', 'ratio': 0.2857142857142857}, {'word': '执行者-评论者算法', 'ratio': 0.14285714285714285}, {'word': '行动者-评论家算法', 'ratio': 0.14285714285714285}, {'word': '行动者-评论者算法', 'ratio': 0.14285714285714285}]",行动者-评论家算法,"1. Rank: 行动者-评论家算法, 行为者-评论家算法, 演员-评论员算法, 执行者-评论者算法, 行动者-评论者算法

2. Explanation: The term ""行动者-评论家算法"" (Actor-Critic Algorithm) is the best fit for several reasons. Firstly, it accurately captures the essence of the original English term, maintaining the semantic integrity of ""actor"" and ""critic,"" which are standard terms in reinforcement learning. The back translation aligns perfectly with the original term, ensuring clarity and consistency in the AI domain. 

The second candidate, ""行为者-评论家算法,"" is also a strong contender, but ""行动者"" is more commonly used in the context of AI and machine learning literature in Chinese. The term ""演员-评论员算法"" (Actor-Commentator Algorithm) introduces a less precise interpretation of the roles, as ""演员"" (actor) and ""评论员"" (commentator) do not convey the same technical meaning as ""actor"" and ""critic"" in this context. 

The other candidates, ""执行者-评论者算法"" (Performer-Reviewer Algorithm) and ""行动者-评论者算法"" (Actor-Commenter Algorithm), deviate further from the established terminology in the AI field, making them less suitable. Overall, ""行动者-评论家算法"" is the most contextually appropriate and semantically accurate translation for the term ""actor critic algorithm.""","['actor-critic algorithm', 'Actor-Commentator Algorithm', 'Performer-Reviewer Algorithm', 'Actor-Critic Algorithm', 'Actor-Commenter Algorithm']"
753,753,actor network,演员网络,0.0,7,"[{'word': '行动者网络', 'ratio': 0.5714285714285714}, {'word': '行为者网络', 'ratio': 0.2857142857142857}, {'word': '执行者网络', 'ratio': 0.14285714285714285}]",行动者网络,{},[]
754,754,actor-critic framework,演员-评论家框架,0.0,7,"[{'word': '行为者-评论家框架', 'ratio': 0.2857142857142857}, {'word': '演员-评论员框架', 'ratio': 0.2857142857142857}, {'word': '执行者-评论者框架', 'ratio': 0.14285714285714285}, {'word': '行动者-评论家框架', 'ratio': 0.14285714285714285}, {'word': '行动者-评论者框架', 'ratio': 0.14285714285714285}]",行为者-评论家框架,"1. Rank: 行为者-评论家框架, 行动者-评论家框架, 演员-评论员框架, 执行者-评论者框架, 行动者-评论者框架

2. Explanation: The term ""行为者-评论家框架"" (actor-critic framework) is the best fit because it accurately captures the semantics of the original English term while maintaining a clear and direct connection to the AI domain. The word ""行为者"" (actor) is commonly used in reinforcement learning literature to refer to the agent that takes actions, while ""评论家"" (critic) is a well-established term for the component that evaluates the actions taken by the actor. This translation aligns with the terminology used in the field, ensuring that it is easily understood by practitioners and researchers.

The alternative ""行动者-评论家框架"" is also a strong candidate, as ""行动者"" (actor) is another acceptable term, but it is slightly less common in the context of reinforcement learning compared to ""行为者."" The other candidates, such as ""演员-评论员框架"" and ""执行者-评论者框架,"" introduce terms that are less precise or not standard in the AI context, which could lead to confusion. Therefore, ""行为者-评论家框架"" is the most semantically accurate and contextually appropriate choice.","['actor-critic framework', 'actor-commentator framework', 'Actor-Reviewer Framework', 'actor-critic framework', 'actor-commentator framework']"
755,755,actor-critic method,演员-评论家方法,0.5,8,"[{'word': '演员-评论家方法', 'ratio': 0.5}, {'word': 'Actor-Critic 方法', 'ratio': 0.25}, {'word': '演员-评论员方法', 'ratio': 0.25}]",演员-评论家方法,{},[]
756,756,adapter-based fine-tuning,适配器微调,0.0,8,"[{'word': '基于适配器的微调', 'ratio': 1.0}]",基于适配器的微调,{},[]
757,757,adaptive boosting algorithm,自适应提升算法,0.75,8,"[{'word': '自适应提升算法', 'ratio': 0.75}, {'word': '自适应增强算法', 'ratio': 0.25}]",自适应提升算法,{},[]
758,758,adaptive thresholding,自适应阈值化,0.0,8,"[{'word': '自适应阈值处理', 'ratio': 0.875}, {'word': '自适应阈值', 'ratio': 0.125}]",自适应阈值处理,{},[]
759,759,additive Gaussian noise,加性高斯噪声,1.0,8,"[{'word': '加性高斯噪声', 'ratio': 1.0}]",加性高斯噪声,{},[]
760,760,additive noise,加性噪声,1.0,10,"[{'word': '加性噪声', 'ratio': 1.0}]",加性噪声,{},[]
761,761,adjacency,邻接,0.6,10,"[{'word': '邻接', 'ratio': 0.6}, {'word': '邻接性', 'ratio': 0.4}]",邻接,{},[]
762,762,adjacency matrix,邻接矩阵,1.0,10,"[{'word': '邻接矩阵', 'ratio': 1.0}]",邻接矩阵,{},[]
763,763,advantage function,优势函数,1.0,10,"[{'word': '优势函数', 'ratio': 1.0}]",优势函数,{},[]
764,764,advcl,副从句,0.0,10,"[{'word': '状语从句', 'ratio': 0.7}, {'word': '副词子句', 'ratio': 0.1}, {'word': '副词从句', 'ratio': 0.1}, {'word': '广告', 'ratio': 0.1}]",状语从句,{},[]
765,765,adversarial attack,对抗攻击,0.5555555555555556,9,"[{'word': '对抗攻击', 'ratio': 0.5555555555555556}, {'word': '对抗性攻击', 'ratio': 0.4444444444444444}]",对抗攻击,{},[]
766,766,adversarial dataset,对抗数据集,0.5555555555555556,9,"[{'word': '对抗数据集', 'ratio': 0.5555555555555556}, {'word': '对抗性数据集', 'ratio': 0.4444444444444444}]",对抗数据集,{},[]
767,767,adversarial example,对抗性示例 (adversarial example),0.0,9,"[{'word': '对抗样本', 'ratio': 0.3333333333333333}, {'word': '对抗示例', 'ratio': 0.2222222222222222}, {'word': '对抗性示例', 'ratio': 0.2222222222222222}, {'word': '对抗性样本', 'ratio': 0.1111111111111111}, {'word': '对抗性例子', 'ratio': 0.1111111111111111}]",对抗样本,"1. Rank: 对抗样本, 对抗示例, 对抗性样本, 对抗性示例, 对抗性例子

2. Explanation: The term ""对抗样本"" (duìkàng yàngběn) is the most widely accepted and recognized translation for ""adversarial example"" in the AI domain. It accurately captures the concept of an example that is specifically designed to deceive a machine learning model while remaining similar to a legitimate input. The term ""样本"" (yàngběn) translates to ""sample,"" which is a common term in machine learning, making it contextually appropriate. 

The second candidate, ""对抗示例"" (duìkàng shìlì), translates to ""adversarial example"" as well, but ""示例"" (shìlì) means ""example"" in a more general sense and is less commonly used in the specific context of AI. 

The other candidates, such as ""对抗性样本"" (duìkàng xìng yàngběn) and ""对抗性示例"" (duìkàng xìng shìlì), introduce the term ""性"" (xìng), which implies a characteristic or quality, making them less precise in this context. ""对抗性例子"" (duìkàng xìng lìzi) is also less suitable as ""例子"" (lìzi) is a more informal term for ""example."" 

Overall, ""对抗样本"" is the most semantically accurate and contextually fitting term for ""adversarial example"" in the AI field.","['Adversarial examples', 'Adversarial examples', 'Adversarial examples', 'adversarial examples', 'Adversarial examples']"
768,768,adversarial filtering,对抗性过滤,0.5555555555555556,9,"[{'word': '对抗性过滤', 'ratio': 0.5555555555555556}, {'word': '对抗过滤', 'ratio': 0.4444444444444444}]",对抗性过滤,{},[]
769,769,adversarial input,对抗性输入,0.5,10,"[{'word': '对抗性输入', 'ratio': 0.5}, {'word': '对抗输入', 'ratio': 0.5}]",对抗性输入,{},[]
770,770,adversarial learning,对抗性学习,0.4,10,"[{'word': '对抗学习', 'ratio': 0.6}, {'word': '对抗性学习', 'ratio': 0.4}]",对抗学习,{},[]
771,771,adversarial loss,对抗损失,0.6,10,"[{'word': '对抗损失', 'ratio': 0.6}, {'word': '对抗性损失', 'ratio': 0.4}]",对抗损失,{},[]
772,772,adversarial network,对抗网络,0.9,10,"[{'word': '对抗网络', 'ratio': 0.9}, {'word': '对抗性网络', 'ratio': 0.1}]",对抗网络,{},[]
773,773,adversarial perturbation,对抗性扰动,0.3,10,"[{'word': '对抗扰动', 'ratio': 0.7}, {'word': '对抗性扰动', 'ratio': 0.3}]",对抗扰动,{},[]
774,774,adversarial prompt,对抗性提示,0.7777777777777778,9,"[{'word': '对抗性提示', 'ratio': 0.7777777777777778}, {'word': '对抗提示', 'ratio': 0.2222222222222222}]",对抗性提示,{},[]
775,775,adversarial robustness,对抗鲁棒性,0.5555555555555556,9,"[{'word': '对抗鲁棒性', 'ratio': 0.5555555555555556}, {'word': '对抗性鲁棒性', 'ratio': 0.4444444444444444}]",对抗鲁棒性,{},[]
776,776,adversarial training,对抗训练,0.6666666666666666,9,"[{'word': '对抗训练', 'ratio': 0.6666666666666666}, {'word': '对抗性训练', 'ratio': 0.3333333333333333}]",对抗训练,{},[]
777,777,adversary,对手,0.7777777777777778,9,"[{'word': '对手', 'ratio': 0.7777777777777778}, {'word': '对手/敌手', 'ratio': 0.1111111111111111}, {'word': '对抗方', 'ratio': 0.1111111111111111}]",对手,{},[]
778,778,advmod,修饰副词,0.0,9,"[{'word': '副词修饰语', 'ratio': 0.3333333333333333}, {'word': '副词修饰', 'ratio': 0.3333333333333333}, {'word': '状语修饰语', 'ratio': 0.2222222222222222}, {'word': '形容词副词修饰语 If you need more translations or assistance, feel free to ask!', 'ratio': 0.1111111111111111}]",副词修饰语,"1. Rank: 副词修饰语, 状语修饰语, 副词修饰, 形容词副词修饰语

2. Explanation: The term ""副词修饰语"" (adverb modifier) is the best fit because it accurately captures the semantic meaning of ""advmod"" in the context of dependency grammar, where it refers specifically to a modifier that is an adverb. This term is widely recognized in linguistic and AI contexts, making it semantically precise. 

The second candidate, ""状语修饰语"" (adverbial modifier), is also a strong contender, as ""状语"" refers to adverbial phrases, but it is slightly less direct than ""副词修饰语."" The term ""副词修饰"" (adverb modification) is less accurate because it implies a process rather than the specific role of the modifier itself. Lastly, ""形容词副词修饰语"" (adjective adverb modifier) is overly complex and introduces unnecessary elements (adjective) that do not pertain to the original term ""advmod,"" which specifically refers to adverbs. Thus, ""副词修饰语"" is the most contextually appropriate and semantically accurate choice for the AI domain.","['adverb modifier', 'adverb modification', 'adverbial modifier', 'Adjective Adverb Modifier If you need more translations or assistance, feel free to ask!']"
779,779,affine,仿射,1.0,9,"[{'word': '仿射', 'ratio': 1.0}]",仿射,{},[]
780,780,affine subspace,仿射子空间,1.0,9,"[{'word': '仿射子空间', 'ratio': 1.0}]",仿射子空间,{},[]
781,781,affine transform,仿射变换,1.0,9,"[{'word': '仿射变换', 'ratio': 1.0}]",仿射变换,{},[]
782,782,affine transformation,仿射变换,0.8888888888888888,9,"[{'word': '仿射变换', 'ratio': 0.8888888888888888}, {'word': '仿射转换', 'ratio': 0.1111111111111111}]",仿射变换,{},[]
783,783,affinity matrix,亲和矩阵,1.0,9,"[{'word': '亲和矩阵', 'ratio': 1.0}]",亲和矩阵,{},[]
784,784,affinity measure,亲和力度量,0.25,8,"[{'word': '亲和度度量', 'ratio': 0.625}, {'word': '亲和力度量', 'ratio': 0.25}, {'word': '亲和度测量', 'ratio': 0.125}]",亲和度度量,{},[]
785,785,agent architecture,智能体架构,0.625,8,"[{'word': '智能体架构', 'ratio': 0.625}, {'word': '代理架构', 'ratio': 0.375}]",智能体架构,{},[]
786,786,agent learning,智能体学习,0.625,8,"[{'word': '智能体学习', 'ratio': 0.625}, {'word': '代理学习', 'ratio': 0.375}]",智能体学习,{},[]
787,787,agent policy,智能体策略,0.625,8,"[{'word': '智能体策略', 'ratio': 0.625}, {'word': '代理策略', 'ratio': 0.25}, {'word': '代理政策', 'ratio': 0.125}]",智能体策略,{},[]
788,788,agent's policy,代理策略,0.0,8,"[{'word': '智能体的策略', 'ratio': 0.625}, {'word': '代理的策略', 'ratio': 0.125}, {'word': '代理的策略 如果还有其他需要翻译的术语或问题，请告诉我！', 'ratio': 0.125}, {'word': '代理政策', 'ratio': 0.125}]",智能体的策略,{},[]
789,789,agent-based model,基于代理模型,0.0,9,"[{'word': '基于代理的模型', 'ratio': 0.6666666666666666}, {'word': '基于主体的模型', 'ratio': 0.2222222222222222}, {'word': '基于智能体的模型', 'ratio': 0.1111111111111111}]",基于代理的模型,{},[]
790,790,aggregate function,聚合函数,1.0,9,"[{'word': '聚合函数', 'ratio': 1.0}]",聚合函数,{},[]
791,791,aggregation,聚合,1.0,9,"[{'word': '聚合', 'ratio': 1.0}]",聚合,{},[]
792,792,aggregation function,聚合函数,1.0,9,"[{'word': '聚合函数', 'ratio': 1.0}]",聚合函数,{},[]
793,793,aleatoric uncertainty,随机不确定性,0.4444444444444444,9,"[{'word': '随机不确定性', 'ratio': 0.4444444444444444}, {'word': '偶然不确定性', 'ratio': 0.4444444444444444}, {'word': '任意不确定性', 'ratio': 0.1111111111111111}]",随机不确定性,"1. Rank: 随机不确定性, 偶然不确定性, 任意不确定性

2. Explanation: The term ""随机不确定性"" (random uncertainty) is the best fit for ""aleatoric uncertainty"" in the AI domain. In the context of AI and machine learning, ""aleatoric uncertainty"" refers to the inherent randomness or noise in the data that cannot be reduced by further modeling. The term ""随机"" (random) accurately captures this concept of unpredictability and variability in the data, aligning well with the established usage in the field. 

On the other hand, ""偶然不确定性"" (accidental uncertainty) implies a sense of chance or happenstance that does not fully convey the systematic nature of uncertainty in data. ""任意不确定性"" (arbitrary uncertainty) suggests a lack of specific constraints or rules, which is not appropriate in this context. Therefore, ""随机不确定性"" is the most semantically accurate and contextually appropriate translation for ""aleatoric uncertainty"" in AI terminology.","['random uncertainty', 'accidental uncertainty', 'arbitrary uncertainty']"
794,794,algorithm class,算法类,0.75,8,"[{'word': '算法类', 'ratio': 0.75}, {'word': '算法类别', 'ratio': 0.25}]",算法类,{},[]
795,795,algorithm design,算法设计,1.0,8,"[{'word': '算法设计', 'ratio': 1.0}]",算法设计,{},[]
796,796,algorithmic approach,算法方法,1.0,8,"[{'word': '算法方法', 'ratio': 1.0}]",算法方法,{},[]
797,797,algorithmic bias,算法偏见,0.625,8,"[{'word': '算法偏见', 'ratio': 0.625}, {'word': '算法偏差', 'ratio': 0.375}]",算法偏见,{},[]
798,798,algorithmic fairness,算法公平性,0.875,8,"[{'word': '算法公平性', 'ratio': 0.875}, {'word': '算法公正性', 'ratio': 0.125}]",算法公平性,{},[]
799,799,algorithmic stability,算法稳定性,1.0,8,"[{'word': '算法稳定性', 'ratio': 1.0}]",算法稳定性,{},[]
800,800,alias table,别名表,1.0,8,"[{'word': '别名表', 'ratio': 1.0}]",别名表,{},[]
801,801,alignment algorithm,对齐算法,1.0,8,"[{'word': '对齐算法', 'ratio': 1.0}]",对齐算法,{},[]
802,802,alignment model,对齐模型,1.0,7,"[{'word': '对齐模型', 'ratio': 1.0}]",对齐模型,{},[]
803,803,alpha compositing,阿尔法合成,0.2857142857142857,7,"[{'word': 'α合成', 'ratio': 0.42857142857142855}, {'word': '阿尔法合成', 'ratio': 0.2857142857142857}, {'word': 'alpha合成', 'ratio': 0.2857142857142857}]",α合成,"1. Rank: α合成, 阿尔法合成, alpha合成

2. Explanation: The term ""α合成"" is the best fit for ""alpha compositing"" in the AI domain due to its semantic accuracy and contextual relevance. The use of the Greek letter ""α"" (alpha) directly corresponds to the English term, maintaining the technical specificity that is crucial in AI and graphics contexts. This term is concise and widely recognized in the field, making it suitable for professionals familiar with the concept. 

""阿尔法合成"" (alpha synthesis) is a transliteration that adds unnecessary complexity and may not be as immediately recognizable to those in the field. While it retains the meaning, it does not align as closely with established terminology. 

""alpha合成"" (alpha synthesis) uses the English word ""alpha"" directly, which is less common in professional Chinese literature and may lead to confusion. It lacks the precision and clarity that ""α合成"" provides, making it a less favorable choice. 

Overall, ""α合成"" is the most contextually appropriate and semantically accurate translation for ""alpha compositing"" in the AI domain.","['alpha synthesis', 'alpha synthesis', 'alpha synthesis']"
804,804,alphabet size,字母表大小,1.0,7,"[{'word': '字母表大小', 'ratio': 1.0}]",字母表大小,{},[]
805,805,alternating least square,交替最小二乘法,0.7142857142857143,7,"[{'word': '交替最小二乘法', 'ratio': 0.7142857142857143}, {'word': '交替最小二乘', 'ratio': 0.2857142857142857}]",交替最小二乘法,{},[]
806,806,alternating minimization,交替最小化,1.0,7,"[{'word': '交替最小化', 'ratio': 1.0}]",交替最小化,{},[]
807,807,ambient space,环境空间,1.0,8,"[{'word': '环境空间', 'ratio': 1.0}]",环境空间,{},[]
808,808,anaphora resolution,指代消解,0.5,8,"[{'word': '指代消解', 'ratio': 0.5}, {'word': '照应消解', 'ratio': 0.25}, {'word': '代指解析', 'ratio': 0.125}, {'word': '照应解析', 'ratio': 0.125}]",指代消解,{},[]
809,809,anaphoric reference,回指参照,0.0,8,"[{'word': '指代参考', 'ratio': 0.375}, {'word': '照应指代', 'ratio': 0.25}, {'word': '代指引用', 'ratio': 0.125}, {'word': '指代引用', 'ratio': 0.125}, {'word': 'anaphoric reference', 'ratio': 0.125}]",指代引用,"1. Rank: 指代引用, 指代参考, 照应指代, 代指引用, anaphoric reference

2. Explanation: The term ""指代引用"" (anaphoric reference) is the best fit because it accurately captures the semantic meaning of ""anaphoric reference"" in the context of AI and linguistics. ""指代"" directly translates to ""reference"" or ""to refer,"" which is essential in understanding how anaphora functions in language. The term ""引用"" (to quote or reference) complements this by indicating that the reference is being made to something previously mentioned. 

""指代参考"" is also a strong candidate, as it similarly conveys the idea of referencing, but it is slightly less common in usage compared to ""指代引用."" 

""照应指代"" translates to ""corresponding reference,"" which, while related, does not directly convey the specific linguistic concept of anaphora as effectively as the first two options. 

""代指引用"" (proxy reference) introduces a different nuance that may not align with the specific linguistic context of anaphoric reference, making it less suitable. 

Finally, ""anaphoric reference"" in English does not provide a contextual fit for a Chinese audience, as it does not utilize the native language, which is essential for clarity in communication within the AI domain.","['anaphoric reference', 'Anaphora', 'Proxy reference', 'anaphoric reference', 'anaphoric reference']"
810,810,ancestral sampling,祖先采样,1.0,8,"[{'word': '祖先采样', 'ratio': 1.0}]",祖先采样,{},[]
811,811,anchor,锚框,0.0,6,"[{'word': '锚点', 'ratio': 1.0}]",锚点,{},[]
812,812,anchor box,锚框,1.0,6,"[{'word': '锚框', 'ratio': 1.0}]",锚框,{},[]
813,813,annotated corpus,标注语料库,0.5,6,"[{'word': '注释语料库', 'ratio': 0.5}, {'word': '标注语料库', 'ratio': 0.5}]",注释语料库,{},[]
814,814,annotated datum,标注数据,0.5,6,"[{'word': '注释数据', 'ratio': 0.5}, {'word': '标注数据', 'ratio': 0.5}]",注释数据,{},[]
815,815,annotation,标注,0.45,20,"[{'word': '注释', 'ratio': 0.5}, {'word': '标注', 'ratio': 0.25}, {'word': '人工标注', 'ratio': 0.15}, {'word': '注解', 'ratio': 0.1}]",注释,{},[]
816,816,annotation artifact,标注伪像,0.1,10,"[{'word': '注释伪影', 'ratio': 0.5}, {'word': '标注偏差', 'ratio': 0.3}, {'word': '标注伪像', 'ratio': 0.1}, {'word': '注释工件', 'ratio': 0.1}]",注释伪影,{},[]
817,817,annotation projection,注释投影,0.6,10,"[{'word': '注释投影', 'ratio': 0.6}, {'word': '标注投射', 'ratio': 0.2}, {'word': '标注投影', 'ratio': 0.2}]",注释投影,{},[]
818,818,annotator,标注者,0.6,10,"[{'word': '标注者', 'ratio': 0.6}, {'word': '注释者', 'ratio': 0.4}]",标注者,{},[]
819,819,annotator bias,注释者偏差,0.3,10,"[{'word': '标注者偏差', 'ratio': 0.5}, {'word': '注释者偏差', 'ratio': 0.3}, {'word': '注释者偏见', 'ratio': 0.1}, {'word': '标注者偏见', 'ratio': 0.1}]",标注者偏差,{},[]
820,820,anomaly detection,异常检测,1.0,10,"[{'word': '异常检测', 'ratio': 1.0}]",异常检测,{},[]
821,821,anomaly score,异常分数,1.0,10,"[{'word': '异常分数', 'ratio': 1.0}]",异常分数,{},[]
822,822,answer set,答案集,0.7142857142857143,7,"[{'word': '答案集', 'ratio': 0.7142857142857143}, {'word': '答案集合', 'ratio': 0.2857142857142857}]",答案集,{},[]
823,823,answer set solver,答案集解算器,0.0,7,"[{'word': '答案集求解器', 'ratio': 0.7142857142857143}, {'word': '答案集合求解器', 'ratio': 0.2857142857142857}]",答案集求解器,{},[]
824,824,answer span,答案跨度,0.0,7,"[{'word': '答案范围', 'ratio': 0.7142857142857143}, {'word': '答案片段', 'ratio': 0.2857142857142857}]",答案范围,{},[]
825,825,answer variable,回答变量,0.0,7,"[{'word': '答案变量', 'ratio': 1.0}]",答案变量,{},[]
826,826,antecedent,先决条件,0.0,8,"[{'word': '前件', 'ratio': 0.5}, {'word': '前提', 'ratio': 0.375}, {'word': '先行词', 'ratio': 0.125}]",前件,{},[]
827,827,antithetic sampling,反向采样,0.0,8,"[{'word': '对立采样', 'ratio': 0.375}, {'word': '对偶抽样', 'ratio': 0.25}, {'word': '对偶采样', 'ratio': 0.25}, {'word': '对立抽样', 'ratio': 0.125}]",对立采样,"1. Rank: 对立采样, 对立抽样, 对偶抽样, 对偶采样

2. Explanation: The term ""对立采样"" (antithetic sampling) is the best fit because it accurately captures the concept of sampling that involves pairing values with their opposites, which is central to the method described in the context. The back translation ""Oppositional sampling"" aligns well with the original English term, maintaining the semantic integrity of the concept. 

""对立抽样"" (Opponent sampling) is a close second, as it also conveys a similar meaning, but ""对立"" (oppositional) is more commonly used in statistical contexts to describe the relationship between the sampled pairs. 

The terms ""对偶抽样"" and ""对偶采样"" (Dual sampling) are less appropriate because ""对偶"" (dual) does not accurately reflect the nature of antithetic sampling, which specifically involves negation rather than a duality. Therefore, they do not fit the context as well as the first two options.","['Oppositional sampling', 'Dual sampling', 'Dual sampling', 'Opponent sampling']"
828,828,anytime algorithm,随时算法,0.75,8,"[{'word': '随时算法', 'ratio': 0.75}, {'word': '任意时间算法', 'ratio': 0.25}]",随时算法,{},[]
829,829,aperture problem,孔径问题,0.5,8,"[{'word': '光圈问题', 'ratio': 0.5}, {'word': '孔径问题', 'ratio': 0.5}]",光圈问题,{},[]
830,830,appearance model,外观模型,1.0,8,"[{'word': '外观模型', 'ratio': 1.0}]",外观模型,{},[]
831,831,apprenticeship learning,学徒学习,1.0,5,"[{'word': '学徒学习', 'ratio': 1.0}]",学徒学习,{},[]
832,832,approximate inference,近似推理,0.4,5,"[{'word': '近似推断', 'ratio': 0.6}, {'word': '近似推理', 'ratio': 0.4}]",近似推断,{},[]
833,833,approximate inference algorithm,近似推理算法,0.4,5,"[{'word': '近似推断算法', 'ratio': 0.6}, {'word': '近似推理算法', 'ratio': 0.4}]",近似推断算法,{},[]
834,834,approximate posterior,近似后验,1.0,5,"[{'word': '近似后验', 'ratio': 1.0}]",近似后验,{},[]
835,835,approximate posterior distribution,近似后验分布,0.9,10,"[{'word': '近似后验分布', 'ratio': 0.9}, {'word': '近似相似性搜索', 'ratio': 0.1}]",近似后验分布,{},[]
836,836,approximate similarity search,近似相似性搜索,0.8,10,"[{'word': '近似相似性搜索', 'ratio': 0.8}, {'word': '近似相似搜索', 'ratio': 0.1}, {'word': '近似相似度搜索', 'ratio': 0.1}]",近似相似性搜索,{},[]
837,837,approximation,近似,1.0,10,"[{'word': '近似', 'ratio': 1.0}]",近似,{},[]
838,838,approximation algorithm,近似算法,1.0,10,"[{'word': '近似算法', 'ratio': 1.0}]",近似算法,{},[]
839,839,approximation bound,逼近界限,0.0,10,"[{'word': '近似界限', 'ratio': 0.6}, {'word': '近似界', 'ratio': 0.4}]",近似界限,{},[]
840,840,approximation error,近似误差,1.0,9,"[{'word': '近似误差', 'ratio': 1.0}]",近似误差,{},[]
841,841,approximation factor,逼近因子,0.0,9,"[{'word': '近似因子', 'ratio': 1.0}]",近似因子,{},[]
842,842,approximation guarantee,近似保证,1.0,9,"[{'word': '近似保证', 'ratio': 1.0}]",近似保证,{},[]
843,843,approximation ratio,近似比率,0.6666666666666666,9,"[{'word': '近似比率', 'ratio': 0.6666666666666666}, {'word': '近似比', 'ratio': 0.3333333333333333}]",近似比率,{},[]
844,844,approximator,近似算子,0.0,9,"[{'word': '近似器', 'ratio': 0.7777777777777778}, {'word': '近似比', 'ratio': 0.1111111111111111}, {'word': '逼近器', 'ratio': 0.1111111111111111}]",近似器,{},[]
845,845,arc-factored model,弧因子模型,0.5714285714285714,7,"[{'word': '弧因子模型', 'ratio': 0.5714285714285714}, {'word': '弧分解模型', 'ratio': 0.42857142857142855}]",弧因子模型,{},[]
846,846,architectural modification,架构修改,1.0,7,"[{'word': '架构修改', 'ratio': 1.0}]",架构修改,{},[]
847,847,architecture,架构,1.0,7,"[{'word': '架构', 'ratio': 1.0}]",架构,{},[]
848,848,architecture search,架构搜索,1.0,7,"[{'word': '架构搜索', 'ratio': 1.0}]",架构搜索,{},[]
849,849,arg max,最大参数,0.0,7,"[{'word': '极大值', 'ratio': 0.2857142857142857}, {'word': '最大化参数', 'ratio': 0.2857142857142857}, {'word': '最优解', 'ratio': 0.14285714285714285}, {'word': '最大值参数', 'ratio': 0.14285714285714285}, {'word': '架构搜索', 'ratio': 0.14285714285714285}]","""极大值""","1. Rank: ""极大值"", ""最大化参数"", ""最优解"", ""最大值参数"", ""架构搜索""

2. Explanation: The term ""极大值"" (back translated as ""maximum value"") is the best fit for ""arg max"" in the context of AI and mathematical optimization. In mathematical terminology, ""arg max"" refers to the argument (input) that maximizes a given function, which aligns closely with the concept of ""极大值"" as it directly conveys the idea of finding the maximum value of a function. 

The other candidates, while related, do not capture the specific meaning of ""arg max"" as effectively. For instance, ""最大化参数"" (maximize parameters) suggests a process rather than the specific point that maximizes a function. ""最优解"" (optimal solution) implies a solution that is optimal but does not specifically refer to the argument that achieves this optimality. ""最大值参数"" (maximum parameter) is somewhat misleading as it suggests a parameter rather than the argument that maximizes the function. Lastly, ""架构搜索"" (schema search) is unrelated to the concept of maximizing a function and is more relevant to structural or architectural searches in AI.

Thus, ""极大值"" is the most semantically accurate and contextually appropriate translation for ""arg max"" in the AI domain.","['maximum value', 'maximize parameters', 'optimal solution', 'Maximum parameter', 'Schema search']"
850,850,arg min,极小化,0.0,9,"[{'word': '最小化参数', 'ratio': 0.3333333333333333}, {'word': '最小值参数', 'ratio': 0.2222222222222222}, {'word': '最小值', 'ratio': 0.1111111111111111}, {'word': '取最小值', 'ratio': 0.1111111111111111}, {'word': '最小值点', 'ratio': 0.1111111111111111}, {'word': '最小化', 'ratio': 0.1111111111111111}]","""最小值点""","1. Rank: ""最小值点"", ""最小值"", ""最小化"", ""取最小值"", ""最小化参数"", ""最小值参数""

2. Explanation: The term ""最小值点"" (minimum point) is the best fit for ""arg min"" in the context of AI and optimization. In mathematical optimization, ""arg min"" refers to the argument (input) that minimizes a given function, which is conceptually represented as the point at which the minimum value occurs. Therefore, ""最小值点"" accurately captures this meaning, as it directly refers to the point where the minimum value is achieved.

The second choice, ""最小值"" (minimum value), while related, does not convey the idea of the argument or input that leads to that minimum, making it less precise in this context. ""最小化"" (minimize) and ""取最小值"" (take the minimum value) are also relevant but focus more on the action rather than the specific point or argument being referred to. The terms ""最小化参数"" (minimize parameters) and ""最小值参数"" (minimum parameter) are less suitable as they imply a focus on parameters rather than the broader concept of finding the minimum point in a function's domain. Thus, ""最小值点"" is the most semantically accurate and contextually appropriate choice for ""arg min"" in the AI domain.","['Minimize parameters', 'Minimum parameter', 'minimum value', 'Take the minimum value', 'minimum point', 'minimize']"
851,851,argument,论元,0.6666666666666666,9,"[{'word': '论元', 'ratio': 0.6666666666666666}, {'word': '参数', 'ratio': 0.2222222222222222}, {'word': '论据', 'ratio': 0.1111111111111111}]",论元,{},[]
852,852,argument identification,论元识别,0.6666666666666666,9,"[{'word': '论元识别', 'ratio': 0.6666666666666666}, {'word': '参数识别', 'ratio': 0.2222222222222222}, {'word': '论据识别', 'ratio': 0.1111111111111111}]",论元识别,{},[]
853,853,argument relation,论元关系,0.6666666666666666,9,"[{'word': '论元关系', 'ratio': 0.6666666666666666}, {'word': '参数关系', 'ratio': 0.2222222222222222}, {'word': '论据关系', 'ratio': 0.1111111111111111}]",论元关系,{},[]
854,854,argument structure,论元结构,0.5555555555555556,9,"[{'word': '论元结构', 'ratio': 0.5555555555555556}, {'word': '参数结构', 'ratio': 0.2222222222222222}, {'word': '论元结构 如果有其他问题或需要更多帮助，请告诉我！', 'ratio': 0.1111111111111111}, {'word': '论据结构', 'ratio': 0.1111111111111111}]",论元结构,{},[]
855,855,arity,元数,0.2,5,"[{'word': '叉数', 'ratio': 0.2}, {'word': '数量', 'ratio': 0.2}, {'word': '词性', 'ratio': 0.2}, {'word': '元数', 'ratio': 0.2}, {'word': '维数', 'ratio': 0.2}]",叉数,"1. Rank: 叉数, 元数, 维数, 数量, 词性

2. Explanation: The term ""叉数"" (back translated as ""Number of forks"") is the best fit for ""arity"" in the AI domain, particularly in the context of parsing and dependencies. In computational linguistics, ""arity"" refers to the number of arguments or operands that a function or operation can take, which aligns well with the concept of ""forks"" in a tree structure or branching in parsing. 

""元数"" (back translated as ""Elements"") could also be a contender, as it suggests a count of components, but it lacks the specific connotation of branching or structure that ""叉数"" conveys. ""维数"" (back translated as ""Dimension"") is more abstract and typically relates to geometric or mathematical dimensions rather than the specific context of dependencies in parsing. 

""数量"" (back translated as ""quantity"") is too vague and does not capture the specific nature of arity, while ""词性"" (back translated as ""part of speech"") is entirely unrelated to the concept of arity in this context. Therefore, ""叉数"" is the most semantically accurate and contextually appropriate translation for ""arity"" in the AI domain.","['Number of forks', 'quantity', 'part of speech', 'Elements', 'Dimension']"
856,856,artificial agent,人工智能代理,0.2,5,"[{'word': '人工智能体', 'ratio': 0.8}, {'word': '人工智能代理', 'ratio': 0.2}]",人工智能体,{},[]
857,857,artificial intelligence system,人工智能系统,1.0,5,"[{'word': '人工智能系统', 'ratio': 1.0}]",人工智能系统,{},[]
858,858,artificial neural network,人工神经网络,1.0,5,"[{'word': '人工神经网络', 'ratio': 1.0}]",人工神经网络,{},[]
859,859,assignment problem,指派问题,0.6666666666666666,6,"[{'word': '指派问题', 'ratio': 0.6666666666666666}, {'word': '分配问题', 'ratio': 0.3333333333333333}]",指派问题,{},[]
860,860,association rule,关联规则,1.0,6,"[{'word': '关联规则', 'ratio': 1.0}]",关联规则,{},[]
861,861,association rule mining,关联规则挖掘,1.0,6,"[{'word': '关联规则挖掘', 'ratio': 1.0}]",关联规则挖掘,{},[]
862,862,asymmetric transformation,不对称变换,0.0,6,"[{'word': '非对称变换', 'ratio': 1.0}]",非对称变换,{},[]
863,863,asymptotic bias,渐近偏差,1.0,7,"[{'word': '渐近偏差', 'ratio': 1.0}]",渐近偏差,{},[]
864,864,asymptotic notation,渐近符号,1.0,7,"[{'word': '渐近符号', 'ratio': 1.0}]",渐近符号,{},[]
865,865,asymptotic variance,渐近方差,1.0,7,"[{'word': '渐近方差', 'ratio': 1.0}]",渐近方差,{},[]
866,866,attack success rate,攻击成功率,1.0,7,"[{'word': '攻击成功率', 'ratio': 1.0}]",攻击成功率,{},[]
867,867,attention distribution,注意力分布,1.0,9,"[{'word': '注意力分布', 'ratio': 1.0}]",注意力分布,{},[]
868,868,attention function,注意力函数,1.0,9,"[{'word': '注意力函数', 'ratio': 1.0}]",注意力函数,{},[]
869,869,attention head,注意力头,1.0,9,"[{'word': '注意力头', 'ratio': 1.0}]",注意力头,{},[]
870,870,attention layer,注意力层,1.0,9,"[{'word': '注意力层', 'ratio': 1.0}]",注意力层,{},[]
871,871,attention map,注意力图,1.0,9,"[{'word': '注意力图', 'ratio': 1.0}]",注意力图,{},[]
872,872,attention mask,注意力掩码,1.0,8,"[{'word': '注意力掩码', 'ratio': 1.0}]",注意力掩码,{},[]
873,873,attention matrix,注意力矩阵,1.0,8,"[{'word': '注意力矩阵', 'ratio': 1.0}]",注意力矩阵,{},[]
874,874,attention mechanism,注意力机制,1.0,8,"[{'word': '注意力机制', 'ratio': 1.0}]",注意力机制,{},[]
875,875,attention model,注意力模型,1.0,8,"[{'word': '注意力模型', 'ratio': 1.0}]",注意力模型,{},[]
876,876,attention module,注意力模块,1.0,8,"[{'word': '注意力模块', 'ratio': 1.0}]",注意力模块,{},[]
877,877,attention operation,注意力运算,0.0,6,"[{'word': '注意力操作', 'ratio': 0.8333333333333334}, {'word': '注意操作', 'ratio': 0.16666666666666666}]",注意力操作,{},[]
878,878,attention pattern,注意力模式,0.8333333333333334,6,"[{'word': '注意力模式', 'ratio': 0.8333333333333334}, {'word': '注意模式', 'ratio': 0.16666666666666666}]",注意力模式,{},[]
879,879,attention score,注意力分数,0.6666666666666666,6,"[{'word': '注意力分数', 'ratio': 0.6666666666666666}, {'word': '注意得分', 'ratio': 0.16666666666666666}, {'word': '注意力得分', 'ratio': 0.16666666666666666}]",注意力分数,{},[]
880,880,attention value,注意力值,0.8333333333333334,6,"[{'word': '注意力值', 'ratio': 0.8333333333333334}, {'word': '注意值', 'ratio': 0.16666666666666666}]",注意力值,{},[]
881,881,attention weight,注意力权重,0.8333333333333334,6,"[{'word': '注意力权重', 'ratio': 0.8333333333333334}, {'word': '注意权重', 'ratio': 0.16666666666666666}]",注意力权重,{},[]
882,882,attention-based model,基于注意力的模型,1.0,7,"[{'word': '基于注意力的模型', 'ratio': 1.0}]",基于注意力的模型,{},[]
883,883,attribute,属性,1.0,7,"[{'word': '属性', 'ratio': 1.0}]",属性,{},[]
884,884,attribution,归因,1.0,7,"[{'word': '归因', 'ratio': 1.0}]",归因,{},[]
885,885,augmentation,增强,0.8571428571428571,14,"[{'word': '增强', 'ratio': 0.8571428571428571}, {'word': '数据增强', 'ratio': 0.14285714285714285}]",增强,{},[]
886,886,augmented state space,增广状态空间,0.1666666666666666,6,"[{'word': '增强状态空间', 'ratio': 0.8333333333333334}, {'word': '增广状态空间', 'ratio': 0.16666666666666666}]",增强状态空间,{},[]
887,887,auto-regressive language model,自回归语言模型,1.0,6,"[{'word': '自回归语言模型', 'ratio': 1.0}]",自回归语言模型,{},[]
888,888,auto-regressive model,自回归模型,1.0,6,"[{'word': '自回归模型', 'ratio': 1.0}]",自回归模型,{},[]
889,889,auto-regressive process,自回归过程,1.0,6,"[{'word': '自回归过程', 'ratio': 1.0}]",自回归过程,{},[]
890,890,autocalibration,自标定 (autocalibration),0.0,9,"[{'word': '自动标定', 'ratio': 0.4444444444444444}, {'word': '自校准', 'ratio': 0.3333333333333333}, {'word': '自动校准', 'ratio': 0.2222222222222222}]",自动标定,"1. Rank: 自动标定, 自动校准, 自校准

2. Explanation: The term ""自动标定"" (Automatic calibration) is the best fit for the context of ""autocalibration"" in the AI domain. This is because ""标定"" specifically refers to the process of calibration in a technical sense, which aligns closely with the context of estimating parameters in computer vision and related fields. The term ""自动"" (automatic) accurately conveys the automated aspect of the process, which is crucial in AI applications. 

On the other hand, ""自动校准"" (automatic calibration) is also a strong candidate, but ""校准"" is a more general term that can refer to calibration in various contexts, not necessarily technical. ""自校准"" (self-calibration) implies a process where the system calibrates itself, which may not fully capture the intended meaning of ""autocalibration"" in this specific context, where the focus is on the automated process rather than self-referential calibration. Therefore, ""自动标定"" is the most semantically accurate and contextually appropriate choice.","['Automatic calibration', 'self-calibration', 'automatic calibration']"
891,891,autocorrelation,自相关,1.0,9,"[{'word': '自相关', 'ratio': 1.0}]",自相关,{},[]
892,892,autodiff,自动微分,0.8888888888888888,9,"[{'word': '自动微分', 'ratio': 0.8888888888888888}, {'word': '自动差分', 'ratio': 0.1111111111111111}]",自动微分,{},[]
893,893,automata,自动机,1.0,9,"[{'word': '自动机', 'ratio': 1.0}]",自动机,{},[]
894,894,automated mechanism design,自动机制设计,0.1111111111111111,9,"[{'word': '自动化机制设计', 'ratio': 0.7777777777777778}, {'word': '自动机制设计', 'ratio': 0.1111111111111111}, {'word': '自动化机构设计', 'ratio': 0.1111111111111111}]",自动化机制设计,{},[]
895,895,automatic differentiation,自动微分,1.0,10,"[{'word': '自动微分', 'ratio': 1.0}]",自动微分,{},[]
896,896,automatic evaluation,自动评估,1.0,10,"[{'word': '自动评估', 'ratio': 1.0}]",自动评估,{},[]
897,897,automatic post-editing,自动后编辑,0.9,10,"[{'word': '自动后编辑', 'ratio': 0.9}, {'word': '自动后期编辑', 'ratio': 0.1}]",自动后编辑,{},[]
898,898,automorphism,自同构,1.0,7,"[{'word': '自同构', 'ratio': 1.0}]",自同构,{},[]
899,899,autonomous agent,自主代理,0.1428571428571428,7,"[{'word': '自主智能体', 'ratio': 0.7142857142857143}, {'word': '自主代理', 'ratio': 0.14285714285714285}, {'word': '自主体', 'ratio': 0.14285714285714285}]",自主智能体,{},[]
900,900,autonomous vehicle,自动驾驶汽车,0.1428571428571428,7,"[{'word': '自动驾驶车辆', 'ratio': 0.42857142857142855}, {'word': '自主车辆', 'ratio': 0.2857142857142857}, {'word': '自主驾驶汽车', 'ratio': 0.14285714285714285}, {'word': '自动驾驶汽车', 'ratio': 0.14285714285714285}]",自动驾驶车辆,"1. Rank: 自动驾驶车辆, 自动驾驶汽车, 自主驾驶汽车, 自主车辆

2. Explanation: The term ""自动驾驶车辆"" (zìdòng jiàshǐ chēliàng) is the best fit for the translation of ""autonomous vehicle"" in the AI domain. This term accurately captures the essence of the technology, emphasizing the ""automatic driving"" aspect, which is crucial in the context of AI and vehicle automation. The back translation ""autonomous vehicles"" aligns well with the original English term, maintaining semantic accuracy.

""自动驾驶汽车"" (zìdòng jiàshǐ qìchē) is also a strong candidate, translating to ""self-driving cars."" However, while it is commonly used in everyday language, it may imply a narrower focus on passenger cars rather than encompassing all types of autonomous vehicles, including trucks and other forms of transport.

""自主驾驶汽车"" (zìzhǔ jiàshǐ qìchē) translates to ""autonomous driving cars,"" which is somewhat accurate but less commonly used in the industry compared to ""自动驾驶车辆."" The term ""自主车辆"" (zìzhǔ chēliàng) translates to ""autonomous vehicles,"" but it lacks the specificity of ""driving"" and may not be as widely recognized in the context of AI and vehicle technology.

Overall, ""自动驾驶车辆"" is the most contextually appropriate and semantically accurate term for the AI domain, as it directly relates to the technology's function and is widely accepted in both academic and industry discussions.","['autonomous vehicles', 'autonomous vehicles', 'autonomous vehicles', 'self-driving cars']"
901,901,autoregressive decoder,自回归解码器,1.0,7,"[{'word': '自回归解码器', 'ratio': 1.0}]",自回归解码器,{},[]
902,902,autoregressive generation,自回归生成,1.0,7,"[{'word': '自回归生成', 'ratio': 1.0}]",自回归生成,{},[]
903,903,auxiliary classifier,辅助分类器,1.0,7,"[{'word': '辅助分类器', 'ratio': 1.0}]",辅助分类器,{},[]
904,904,auxiliary loss,辅助损失,1.0,7,"[{'word': '辅助损失', 'ratio': 1.0}]",辅助损失,{},[]
905,905,auxiliary task,辅助任务,1.0,7,"[{'word': '辅助任务', 'ratio': 1.0}]",辅助任务,{},[]
906,906,auxiliary variable,辅助变量,1.0,7,"[{'word': '辅助变量', 'ratio': 1.0}]",辅助变量,{},[]
907,907,auxillary loss,辅助损失,1.0,7,"[{'word': '辅助损失', 'ratio': 1.0}]",辅助损失,{},[]
908,908,average loss,平均损失,1.0,10,"[{'word': '平均损失', 'ratio': 1.0}]",平均损失,{},[]
909,909,averaged perceptron,平均感知器,0.7,10,"[{'word': '平均感知器', 'ratio': 0.7}, {'word': '平均感知机', 'ratio': 0.3}]",平均感知器,{},[]
910,910,averaged perceptron algorithm,平均感知机算法,0.3,10,"[{'word': '平均感知器算法', 'ratio': 0.7}, {'word': '平均感知机算法', 'ratio': 0.3}]",平均感知器算法,{},[]
911,911,axis-aligned rectangle,轴对齐矩形,1.0,10,"[{'word': '轴对齐矩形', 'ratio': 1.0}]",轴对齐矩形,{},[]
912,912,back-off strategy,回退策略,1.0,10,"[{'word': '回退策略', 'ratio': 1.0}]",回退策略,{},[]
913,913,back-propagate,反向传播,1.0,10,"[{'word': '反向传播', 'ratio': 1.0}]",反向传播,{},[]
914,914,back-propagated gradient,反向传播梯度,1.0,10,"[{'word': '反向传播梯度', 'ratio': 1.0}]",反向传播梯度,{},[]
915,915,back-propagation algorithm,反向传播算法,1.0,10,"[{'word': '反向传播算法', 'ratio': 1.0}]",反向传播算法,{},[]
916,916,back-translation,反向翻译,0.5,10,"[{'word': '反向翻译', 'ratio': 0.5}, {'word': '回译', 'ratio': 0.5}]",反向翻译,{},[]
917,917,backbone model,主干模型,0.6,10,"[{'word': '主干模型', 'ratio': 0.6}, {'word': '骨干模型', 'ratio': 0.4}]",主干模型,{},[]
918,918,backbone network,主干网络,0.5714285714285714,7,"[{'word': '主干网络', 'ratio': 0.5714285714285714}, {'word': '骨干网络', 'ratio': 0.42857142857142855}]",主干网络,{},[]
919,919,backdoor,后门,1.0,7,"[{'word': '后门', 'ratio': 1.0}]",后门,{},[]
920,920,backdoor adjustment,后门调整,1.0,7,"[{'word': '后门调整', 'ratio': 1.0}]",后门调整,{},[]
921,921,backdoor attack,后门攻击,1.0,7,"[{'word': '后门攻击', 'ratio': 1.0}]",后门攻击,{},[]
922,922,backdoor sample,后门样本,1.0,7,"[{'word': '后门样本', 'ratio': 1.0}]",后门样本,{},[]
923,923,background model,背景模型,1.0,9,"[{'word': '背景模型', 'ratio': 1.0}]",背景模型,{},[]
924,924,background subtraction,背景减除,0.7777777777777778,9,"[{'word': '背景减除', 'ratio': 0.7777777777777778}, {'word': '背景减法', 'ratio': 0.2222222222222222}]",背景减除,{},[]
925,925,backoff model,回退模型,0.6666666666666666,9,"[{'word': '回退模型', 'ratio': 0.6666666666666666}, {'word': '退避模型', 'ratio': 0.2222222222222222}, {'word': '退回模型', 'ratio': 0.1111111111111111}]",回退模型,{},[]
926,926,backpointer,回指针,0.3333333333333333,9,"[{'word': '回指针', 'ratio': 0.3333333333333333}, {'word': '回溯指针', 'ratio': 0.3333333333333333}, {'word': '回指器', 'ratio': 0.2222222222222222}, {'word': '后退指针', 'ratio': 0.1111111111111111}]","""回指针""","1. Rank: ""回指针"", ""回溯指针"", ""后退指针"", ""回指器""

2. Explanation: The term ""回指针"" (back pointer) is the best fit for the AI domain-specific usage because it accurately captures the concept of a pointer that refers back to a previous state or position in a data structure, which is essential in algorithms like Viterbi parsing. The back translation ""back pointer"" aligns perfectly with the original English term, maintaining semantic accuracy. 

""回溯指针"" (backtracking pointer) is a close second, but it implies a broader context of backtracking algorithms, which may not be specifically relevant to the context provided. ""后退指针"" (backward pointer) is less commonly used in this context and could lead to confusion, as it may suggest a different operational mechanism. Lastly, ""回指器"" (anaphora) is not appropriate, as it refers to a linguistic term rather than a computational concept, making it the least suitable choice. Thus, ""回指针"" stands out as the most contextually and semantically accurate translation for ""backpointer"" in the AI domain.","['back pointer', 'Backtracking pointer', 'anaphora', 'Back pointer']"
927,927,backprojection,反投影,1.0,9,"[{'word': '反投影', 'ratio': 1.0}]",反投影,{},[]
928,928,backprop,反向传播,1.0,10,"[{'word': '反向传播', 'ratio': 1.0}]",反向传播,{},[]
929,929,backtracking line search,回溯线搜索,1.0,10,"[{'word': '回溯线搜索', 'ratio': 1.0}]",回溯线搜索,{},[]
930,930,backward pass,反向传播,0.1,10,"[{'word': '反向传递', 'ratio': 0.6}, {'word': '反向传播过程', 'ratio': 0.2}, {'word': '向后传递', 'ratio': 0.1}, {'word': '反向传播', 'ratio': 0.1}]",反向传递,{},[]
931,931,bag of feature,特征包,0.3,10,"[{'word': '特征袋', 'ratio': 0.7}, {'word': '特征包', 'ratio': 0.3}]",特征袋,{},[]
932,932,bag-of-word,词袋模型,0.8,10,"[{'word': '词袋模型', 'ratio': 0.8}, {'word': '词袋', 'ratio': 0.2}]",词袋模型,{},[]
933,933,bag-of-word model,词袋模型,1.0,9,"[{'word': '词袋模型', 'ratio': 1.0}]",词袋模型,{},[]
934,934,bag-of-word representation,词袋表示,1.0,9,"[{'word': '词袋表示', 'ratio': 1.0}]",词袋表示,{},[]
935,935,bandit,摇臂赌博机 (bandit),0.0,9,"[{'word': '强盗', 'ratio': 0.3333333333333333}, {'word': '赌博机', 'ratio': 0.3333333333333333}, {'word': '赌徒', 'ratio': 0.1111111111111111}, {'word': '土匪', 'ratio': 0.1111111111111111}, {'word': '多臂老虎机', 'ratio': 0.1111111111111111}]",多臂老虎机,"1. Rank: 多臂老虎机, 强盗, 赌博机, 赌徒, 土匪

2. Explanation: The term ""多臂老虎机"" (multi-armed bandit) is the best fit for the AI domain-specific usage because it directly refers to a well-established concept in reinforcement learning and decision-making, where the ""bandit"" problem involves making a series of decisions to maximize rewards based on uncertain outcomes. This term is widely recognized in the AI community and accurately captures the essence of the problem being discussed. 

The other candidates, such as ""强盗"" (robber) and ""土匪"" (bandit), while they may translate the word ""bandit"" literally, do not convey the specific meaning used in the context of machine learning. ""赌博机"" (gambling machine) and ""赌徒"" (gambler) also stray from the intended meaning, as they imply a focus on gambling rather than the algorithmic decision-making process central to the bandit problem. Therefore, ""多臂老虎机"" is the most semantically accurate and contextually appropriate choice.","['robber', 'gambling machine', 'gambler', 'bandit', 'multi-armed bandit']"
936,936,bandit feedback,赌博者反馈,0.0,9,"[{'word': '强盗反馈', 'ratio': 0.3333333333333333}, {'word': '赌博机反馈', 'ratio': 0.3333333333333333}, {'word': '赌徒反馈', 'ratio': 0.2222222222222222}, {'word': '多臂老虎机反馈', 'ratio': 0.1111111111111111}]",强盗反馈,"1. Rank: 强盗反馈, 多臂老虎机反馈, 赌徒反馈, 赌博机反馈

2. Explanation: The term ""强盗反馈"" (bandit feedback) is the most accurate translation in the context of AI and machine learning. This term directly corresponds to the concept of ""bandit"" in the ""multi-armed bandit"" problem, which is a well-established framework in reinforcement learning and decision-making scenarios. The term ""多臂老虎机反馈"" (multi-armed bandit feedback) is also relevant, as it explicitly references the multi-armed bandit problem, but it is more verbose and less commonly used in general discussions about bandit feedback. 

The other candidates, ""赌徒反馈"" (gambler feedback) and ""赌博机反馈"" (gambling machine feedback), while they may convey a similar idea, do not capture the specific technical meaning associated with the bandit problem in AI. ""赌徒"" (gambler) and ""赌博机"" (gambling machine) imply a more general gambling context rather than the specific algorithmic framework used in machine learning. Therefore, ""强盗反馈"" is the best fit for its semantic accuracy and contextual relevance in the AI domain.","['Bandit feedback', 'Gambling Machine Feedback', 'Gambler feedback', 'Multi-armed bandit feedback']"
937,937,bandit learning,强盗学习,0.3333333333333333,9,"[{'word': '强盗学习', 'ratio': 0.3333333333333333}, {'word': '赌博机学习', 'ratio': 0.3333333333333333}, {'word': '赌徒学习', 'ratio': 0.2222222222222222}, {'word': '多臂老虎机学习', 'ratio': 0.1111111111111111}]",多臂老虎机学习,"1. Rank: 多臂老虎机学习, 强盗学习, 赌博机学习, 赌徒学习

2. Explanation: The term ""多臂老虎机学习"" (Multi-armed bandit learning) is the best fit because it accurately reflects the specific concept in the AI domain known as ""multi-armed bandit"" problems, which are a fundamental topic in reinforcement learning and decision-making under uncertainty. This term is widely recognized in both English and Chinese literature related to AI and machine learning, making it semantically precise and contextually appropriate.

The second candidate, ""强盗学习"" (Bandits learn), while it captures the essence of the term, lacks the specificity of the ""multi-armed"" aspect, which is crucial for understanding the particular type of learning being discussed. The other candidates, ""赌博机学习"" (Gambling machine learning) and ""赌徒学习"" (Gamblers learn), are less accurate as they do not convey the technical meaning associated with the multi-armed bandit framework and could lead to confusion in an academic or professional context. Thus, ""多臂老虎机学习"" is the most contextually relevant and semantically accurate choice.","['Bandits learn', 'Gambling machine learning', 'gamblers learn', 'Multi-armed bandit learning']"
938,938,bandwidth parameter,带宽参数,0.9,10,"[{'word': '带宽参数', 'ratio': 0.9}, {'word': '扰动方差', 'ratio': 0.1}]",带宽参数,{},[]
939,939,bart-base,bart-base模型,0.1,10,"[{'word': 'bart-base', 'ratio': 0.4}, {'word': '巴特基地', 'ratio': 0.2}, {'word': 'BART基础版', 'ratio': 0.2}, {'word': 'bart-base模型', 'ratio': 0.1}, {'word': '扰动方差', 'ratio': 0.1}]","""bart-base模型""","1. Rank: ""bart-base模型"", ""BART基础版"", ""巴特基地"", ""Bart-Barcelona"", ""扰动方差""

2. Explanation: The term ""bart-base模型"" is the best fit because it retains the original English term ""bart-base"" while adding the Chinese word ""模型"" (model), which is contextually appropriate in the AI domain. This translation accurately reflects the technical nature of the term and maintains its semantic integrity. The back translation ""bart-base model"" aligns perfectly with the original English term, ensuring clarity and consistency in meaning. 

The second choice, ""BART基础版,"" translates to ""BART Basic Edition,"" which is also a reasonable option but introduces the word ""基础版"" (Basic Edition), which may imply a versioning system that is not explicitly stated in the original context. 

The third option, ""巴特基地,"" translates to ""Bart-Barcelona,"" which is a misinterpretation as it introduces an unrelated geographical reference and does not convey the intended meaning of the model.

The fourth option, ""Bart-Barcelona,"" is a direct transliteration that does not provide any contextual relevance to the AI model being discussed.

Lastly, ""扰动方差"" translates to ""Disturbance variance,"" which is completely unrelated to the term ""bart-base"" and does not fit the context at all. 

Thus, ""bart-base模型"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['Bart-Barcelona', 'Bart Base', 'BART Basic Edition', 'bart-base model', 'Disturbance variance']"
940,940,bart-large,bart-large,0.4,10,"[{'word': 'bart-large', 'ratio': 0.4}, {'word': '巴特大号', 'ratio': 0.2}, {'word': 'BART大型版', 'ratio': 0.2}, {'word': 'bart-large模型', 'ratio': 0.1}, {'word': '音素', 'ratio': 0.1}]","""bart-large模型""","```
1. Rank: ""bart-large模型"", ""BART大型版"", ""巴特大号"", ""音素"", ""Bart-large""

2. Explanation: The term ""bart-large模型"" is the best fit because it retains the original English term ""bart-large"" while adding the word ""模型"" (model), which is contextually appropriate in the AI domain. This term clearly indicates that it refers to a specific model in the context of machine learning and natural language processing. The back translation ""bart-large model"" accurately reflects the original term, maintaining semantic accuracy. 

The second choice, ""BART大型版,"" translates to ""BART large version,"" which is also accurate but slightly less precise than ""模型"" in the context of AI terminology, where ""model"" is the standard term used. ""巴特大号"" (Bart tuba) is a transliteration that does not convey the intended meaning in the AI context, and ""音素"" (phoneme) is unrelated to the term ""bart-large."" Lastly, ""Bart-large"" is simply a transliteration and does not provide any additional context or clarity. Thus, ""bart-large模型"" is the most contextually relevant and semantically accurate choice. 
```","['Bart-large', 'bart tuba', 'BART large version', 'bart-large model', 'phoneme']"
941,941,barycentric coordinate,重心坐标,0.9,10,"[{'word': '重心坐标', 'ratio': 0.9}, {'word': '音素分割', 'ratio': 0.1}]",重心坐标,{},[]
942,942,base classifier,基分类器,0.1,10,"[{'word': '基础分类器', 'ratio': 0.8}, {'word': '音素分割', 'ratio': 0.1}, {'word': '基分类器', 'ratio': 0.1}]",基础分类器,{},[]
943,943,base distribution,基础分布,1.0,9,"[{'word': '基础分布', 'ratio': 1.0}]",基础分布,{},[]
944,944,base learner,基本学习器,0.0,9,"[{'word': '基础学习器', 'ratio': 0.7777777777777778}, {'word': '基学习器', 'ratio': 0.2222222222222222}]",基础学习器,{},[]
945,945,base model,基础模型,1.0,9,"[{'word': '基础模型', 'ratio': 1.0}]",基础模型,{},[]
946,946,baseline algorithm,基线算法,0.3333333333333333,9,"[{'word': '基准算法', 'ratio': 0.6666666666666666}, {'word': '基线算法', 'ratio': 0.3333333333333333}]",基准算法,{},[]
947,947,baseline method,基线法,0.0,9,"[{'word': '基准方法', 'ratio': 0.6666666666666666}, {'word': '基线方法', 'ratio': 0.3333333333333333}]",基准方法,{},[]
948,948,baseline model,基线模型,1.0,10,"[{'word': '基线模型', 'ratio': 1.0}]",基线模型,{},[]
949,949,baseline parser,基线解析器,0.9,10,"[{'word': '基线解析器', 'ratio': 0.9}, {'word': '基线模型', 'ratio': 0.1}]",基线解析器,{},[]
950,950,baseline policy,基线策略,1.0,10,"[{'word': '基线策略', 'ratio': 1.0}]",基线策略,{},[]
951,951,baseline system,基线系统,1.0,10,"[{'word': '基线系统', 'ratio': 1.0}]",基线系统,{},[]
952,952,basis function,基函数,1.0,10,"[{'word': '基函数', 'ratio': 1.0}]",基函数,{},[]
953,953,basis vector,基向量,1.0,10,"[{'word': '基向量', 'ratio': 1.0}]",基向量,{},[]
954,954,batch algorithm,批处理算法,0.8,10,"[{'word': '批处理算法', 'ratio': 0.8}, {'word': '基向量', 'ratio': 0.1}, {'word': '批量算法', 'ratio': 0.1}]",批处理算法,{},[]
955,955,batch dimension,批次维度,0.5,10,"[{'word': '批次维度', 'ratio': 0.5}, {'word': '批处理维度', 'ratio': 0.4}, {'word': '批量维度', 'ratio': 0.1}]",批次维度,{},[]
956,956,batch element,批次元素,0.5,10,"[{'word': '批次元素', 'ratio': 0.5}, {'word': '批处理元素', 'ratio': 0.3}, {'word': '批量元素', 'ratio': 0.1}, {'word': '批元素', 'ratio': 0.1}]",批次元素,{},[]
957,957,batch learning,批量学习,1.0,10,"[{'word': '批量学习', 'ratio': 1.0}]",批量学习,{},[]
958,958,batch mode,批处理模式,0.4,10,"[{'word': '批量模式', 'ratio': 0.6}, {'word': '批处理模式', 'ratio': 0.4}]",批量模式,{},[]
959,959,batch optimization,批量优化,1.0,10,"[{'word': '批量优化', 'ratio': 1.0}]",批量优化,{},[]
960,960,batch processing,批处理,0.6,10,"[{'word': '批处理', 'ratio': 0.6}, {'word': '批量处理', 'ratio': 0.4}]",批处理,{},[]
961,961,batch setting,批量设置,1.0,10,"[{'word': '批量设置', 'ratio': 1.0}]",批量设置,{},[]
962,962,batch training,批量训练,0.875,8,"[{'word': '批量训练', 'ratio': 0.875}, {'word': '信息检索系统', 'ratio': 0.125}]",批量训练,{},[]
963,963,beam search algorithm,束搜索算法,0.625,8,"[{'word': '束搜索算法', 'ratio': 0.625}, {'word': '波束搜索算法', 'ratio': 0.125}, {'word': '光束搜索算法', 'ratio': 0.125}, {'word': '信息集', 'ratio': 0.125}]",束搜索算法,{},[]
964,964,beam search decoding,波束搜索解码,0.125,8,"[{'word': '束搜索解码', 'ratio': 0.625}, {'word': '波束搜索解码', 'ratio': 0.125}, {'word': '光束搜索解码', 'ratio': 0.125}, {'word': '信息论的', 'ratio': 0.125}]",束搜索解码,{},[]
965,965,beam search decoding algorithm,束搜索解码算法,0.625,8,"[{'word': '束搜索解码算法', 'ratio': 0.625}, {'word': '波束搜索解码算法', 'ratio': 0.125}, {'word': '光束搜索解码算法', 'ratio': 0.125}, {'word': '信息论度量', 'ratio': 0.125}]",束搜索解码算法,{},[]
966,966,beam size,束大小,0.375,8,"[{'word': '束大小', 'ratio': 0.375}, {'word': '波束大小', 'ratio': 0.125}, {'word': '束宽度', 'ratio': 0.125}, {'word': '束宽', 'ratio': 0.125}, {'word': '光束大小', 'ratio': 0.125}, {'word': '信息集', 'ratio': 0.125}]",波束大小,"1. Rank: 波束大小, 束大小, 束宽度, 束宽, 光束大小, 信息集

2. Explanation: The term ""波束大小"" (Beam size) is the best fit for the AI domain, particularly in the context of beam search decoding algorithms. This term accurately captures the concept of ""beam size"" as it is commonly used in machine learning and natural language processing. The back translation ""Beam size"" aligns perfectly with the original English term, ensuring semantic accuracy. 

""束大小"" (bundle size) is a close second, but ""束"" (bundle) does not convey the specific meaning of ""beam"" in this context, which is crucial for understanding the algorithm's function. ""束宽度"" (beam width) and ""束宽"" (beam width) are also relevant but are more commonly associated with the width of the beam rather than its size, which can lead to confusion in this specific context. ""光束大小"" (light beam size) introduces unnecessary ambiguity by relating it to light, which is not relevant here. Lastly, ""信息集"" (information set) is entirely unrelated to the concept of beam size in AI, making it the least suitable option. Thus, ""波束大小"" is the most contextually appropriate and semantically accurate translation for the term ""beam size"" in the AI domain.","['bundle size', 'Beam size', 'beam width', 'Beam width', 'Beam size', 'information set']"
967,967,beam width,波束宽度,0.3333333333333333,9,"[{'word': '波束宽度', 'ratio': 0.3333333333333333}, {'word': '搜索宽度', 'ratio': 0.3333333333333333}, {'word': '束宽', 'ratio': 0.2222222222222222}, {'word': '行为克隆', 'ratio': 0.1111111111111111}]",波束宽度,"1. Rank: 波束宽度, 束宽, 搜索宽度, 行为克隆

2. Explanation: The term ""波束宽度"" (Beamwidth) is the best fit for the AI domain-specific usage of ""beam width"" because it accurately captures the technical meaning of the term as used in beam search algorithms. In the context of AI and machine learning, ""beam width"" refers to the number of hypotheses considered at each step of the decoding process, which is crucial for balancing the trade-off between computational efficiency and the quality of the generated output. 

""束宽"" (Beam width) is also a close candidate, but it is less commonly used in the literature compared to ""波束宽度."" ""搜索宽度"" (search width) is a more generic term and does not specifically convey the concept of beam search, which is a specific algorithmic technique. Lastly, ""行为克隆"" (behavioral cloning) is entirely unrelated to the concept of beam width and should not be considered in this context. 

Overall, ""波束宽度"" is the most semantically accurate and contextually appropriate translation for ""beam width"" in the AI domain.","['Beamwidth', 'search width', 'Beam width', 'behavioral cloning']"
968,968,behavior cloning,行为克隆,0.8888888888888888,9,"[{'word': '行为克隆', 'ratio': 0.8888888888888888}, {'word': '行为策略', 'ratio': 0.1111111111111111}]",行为克隆,{},[]
969,969,behavior policy,行为策略,0.8888888888888888,9,"[{'word': '行为策略', 'ratio': 0.8888888888888888}, {'word': '信念状态', 'ratio': 0.1111111111111111}]",行为策略,{},[]
970,970,belief state,信念状态,0.6666666666666666,9,"[{'word': '信念状态', 'ratio': 0.6666666666666666}, {'word': '置信状态', 'ratio': 0.2222222222222222}, {'word': '可信度状态', 'ratio': 0.1111111111111111}]",信念状态,{},[]
971,971,benchmark,基准,0.1111111111111111,9,"[{'word': '基准测试', 'ratio': 0.8888888888888888}, {'word': '基准', 'ratio': 0.1111111111111111}]",基准测试,{},[]
972,972,benchmark dataset,基准数据集,1.0,10,"[{'word': '基准数据集', 'ratio': 1.0}]",基准数据集,{},[]
973,973,benchmark task,基准任务,1.0,10,"[{'word': '基准任务', 'ratio': 1.0}]",基准任务,{},[]
974,974,best-first search,最佳优先搜索,1.0,10,"[{'word': '最佳优先搜索', 'ratio': 1.0}]",最佳优先搜索,{},[]
975,975,best-first search algorithm,优先搜索算法,0.0,10,"[{'word': '最佳优先搜索算法', 'ratio': 1.0}]",最佳优先搜索算法,{},[]
976,976,beta1,beta1,0.2,10,"[{'word': 'β1', 'ratio': 0.5}, {'word': 'β₁', 'ratio': 0.2}, {'word': 'beta1', 'ratio': 0.2}, {'word': 'beta1参数', 'ratio': 0.1}]",β1,{},[]
977,977,between-class variance,类间方差,1.0,9,"[{'word': '类间方差', 'ratio': 1.0}]",类间方差,{},[]
978,978,betweenness,介数,0.2222222222222222,9,"[{'word': '中介中心性', 'ratio': 0.7777777777777778}, {'word': '介数', 'ratio': 0.2222222222222222}]",中介中心性,{},[]
979,979,bi-gram,二元语法,0.3333333333333333,9,"[{'word': '二元组', 'ratio': 0.6666666666666666}, {'word': '二元语法', 'ratio': 0.3333333333333333}]",二元组,{},[]
980,980,bi-level optimization,双层优化,1.0,9,"[{'word': '双层优化', 'ratio': 1.0}]",双层优化,{},[]
981,981,bias,偏差,1.0,9,"[{'word': '偏差', 'ratio': 1.0}]",偏差,{},[]
982,982,bias mitigation,偏见缓解,0.7,10,"[{'word': '偏见缓解', 'ratio': 0.7}, {'word': '偏差缓解', 'ratio': 0.3}]",偏见缓解,{},[]
983,983,bias parameter,偏置参数,0.7,10,"[{'word': '偏置参数', 'ratio': 0.7}, {'word': '偏差参数', 'ratio': 0.2}, {'word': '偏见参数', 'ratio': 0.1}]",偏置参数,{},[]
984,984,bias term,偏置项,0.7,10,"[{'word': '偏置项', 'ratio': 0.7}, {'word': '偏差项', 'ratio': 0.2}, {'word': '偏见项', 'ratio': 0.1}]",偏置项,{},[]
985,985,bias vector,偏置向量,0.7,10,"[{'word': '偏置向量', 'ratio': 0.7}, {'word': '偏差向量', 'ratio': 0.2}, {'word': '偏见向量', 'ratio': 0.1}]",偏置向量,{},[]
986,986,bias-variance tradeoff,偏差-方差权衡,0.8,10,"[{'word': '偏差-方差权衡', 'ratio': 0.8}, {'word': '偏见-方差权衡', 'ratio': 0.1}, {'word': '偏置-方差权衡', 'ratio': 0.1}]",偏差-方差权衡,{},[]
987,987,biased estimator,有偏估计量,0.8888888888888888,9,"[{'word': '有偏估计量', 'ratio': 0.8888888888888888}, {'word': '偏倚估计量', 'ratio': 0.1111111111111111}]",有偏估计量,{},[]
988,988,bibliographic coupling,文献耦合,0.8888888888888888,9,"[{'word': '文献耦合', 'ratio': 0.8888888888888888}, {'word': '书目耦合', 'ratio': 0.1111111111111111}]",文献耦合,{},[]
989,989,bicubic interpolation,双三次插值,0.8888888888888888,9,"[{'word': '双三次插值', 'ratio': 0.8888888888888888}, {'word': '双立方插值', 'ratio': 0.1111111111111111}]",双三次插值,{},[]
990,990,bidirectional,双向的,0.4736842105263157,19,"[{'word': '双向', 'ratio': 0.5263157894736842}, {'word': '双向的', 'ratio': 0.47368421052631576}]",双向,{},[]
991,991,bidirectional Transformer,双向Transformer,0.3,10,"[{'word': '双向变换器', 'ratio': 0.7}, {'word': '双向Transformer', 'ratio': 0.3}]",双向变换器,{},[]
992,992,bidirectional encoder,双向编码器,1.0,10,"[{'word': '双向编码器', 'ratio': 1.0}]",双向编码器,{},[]
993,993,bidirectional heuristic search,双向启发式搜索,1.0,10,"[{'word': '双向启发式搜索', 'ratio': 1.0}]",双向启发式搜索,{},[]
994,994,bidirectional model,双向模型,1.0,10,"[{'word': '双向模型', 'ratio': 1.0}]",双向模型,{},[]
995,995,bidirectional search,双向搜索,1.0,10,"[{'word': '双向搜索', 'ratio': 1.0}]",双向搜索,{},[]
996,996,bidirectionality,"""双向性""",0.0,10,"[{'word': '双向性', 'ratio': 1.0}]",双向性,{},[]
997,997,big-O notation,大O表示法,0.1,10,"[{'word': '大O符号', 'ratio': 0.8}, {'word': '大O表示法', 'ratio': 0.1}, {'word': '大 O 表示法', 'ratio': 0.1}]",大O符号,{},[]
998,998,bigram count,双字计数,0.0,10,"[{'word': '二元组计数', 'ratio': 0.7}, {'word': '二元语法计数', 'ratio': 0.1}, {'word': '二元组数', 'ratio': 0.1}, {'word': '二元词组计数', 'ratio': 0.1}]",二元组计数,{},[]
999,999,bigram language model,双元语言模型,0.0,10,"[{'word': '二元语言模型', 'ratio': 0.4}, {'word': '二元组语言模型', 'ratio': 0.4}, {'word': '二元语法语言模型', 'ratio': 0.1}, {'word': '二元词组语言模型', 'ratio': 0.1}]",二元组语言模型,"1. Rank: 二元组语言模型, 二元语言模型, 二元词组语言模型, 二元语法语言模型

2. Explanation: The term ""二元组语言模型"" (bigram language model) is the most accurate translation because it directly corresponds to the concept of a ""bigram,"" which refers to a pair of consecutive words or tokens in a sequence. The use of ""组"" (group or pair) in this context is semantically precise, as it captures the essence of the bigram concept in natural language processing. 

The second candidate, ""二元语言模型"" (binary language model), while it may seem relevant, introduces ambiguity by suggesting a binary model rather than specifically indicating the bigram structure. The third candidate, ""二元词组语言模型"" (bigram phrase language model), adds unnecessary complexity by including ""词组"" (phrase), which is not a standard term in this context. Lastly, ""二元语法语言模型"" (bigram grammar language model) incorrectly implies a focus on grammar rather than the statistical pairing of words, which is not the primary function of a bigram language model. 

Thus, ""二元组语言模型"" is the best fit for its clarity and direct alignment with the AI domain-specific usage of bigram language models.","['binary language model', 'bigram language model', 'bigram language model', 'bigram language model']"
1000,1000,bijective function,双射函数,1.0,9,"[{'word': '双射函数', 'ratio': 1.0}]",双射函数,{},[]
1001,1001,bijective mapping,一一对应映射,0.0,9,"[{'word': '双射映射', 'ratio': 1.0}]",双射映射,{},[]
1002,1002,bilateral filtering,双边滤波,1.0,9,"[{'word': '双边滤波', 'ratio': 1.0}]",双边滤波,{},[]
1003,1003,bilinear,双线性,1.0,9,"[{'word': '双线性', 'ratio': 1.0}]",双线性,{},[]
1004,1004,bilinear form,双线性形式,0.6666666666666666,9,"[{'word': '双线性形式', 'ratio': 0.6666666666666666}, {'word': '双线性型', 'ratio': 0.3333333333333333}]",双线性形式,{},[]
1005,1005,bilinear interpolation,双线性插值,1.0,9,"[{'word': '双线性插值', 'ratio': 1.0}]",双线性插值,{},[]
1006,1006,bilinear model,双线性模型,1.0,9,"[{'word': '双线性模型', 'ratio': 1.0}]",双线性模型,{},[]
1007,1007,bilingual model,双语模型,1.0,9,"[{'word': '双语模型', 'ratio': 1.0}]",双语模型,{},[]
1008,1008,binarization,二值化,1.0,9,"[{'word': '二值化', 'ratio': 1.0}]",二值化,{},[]
1009,1009,binary atom,二元原子,1.0,8,"[{'word': '二元原子', 'ratio': 1.0}]",二元原子,{},[]
1010,1010,binary classification,二分类,0.25,8,"[{'word': '二元分类', 'ratio': 0.75}, {'word': '二分类', 'ratio': 0.25}]",二元分类,{},[]
1011,1011,binary classification head,二元分类头,0.75,8,"[{'word': '二元分类头', 'ratio': 0.75}, {'word': '二分类头', 'ratio': 0.25}]",二元分类头,{},[]
1012,1012,binary classification problem,二分类问题,0.25,8,"[{'word': '二元分类问题', 'ratio': 0.75}, {'word': '二分类问题', 'ratio': 0.25}]",二元分类问题,{},[]
1013,1013,binary classification task,二分类任务,0.25,8,"[{'word': '二元分类任务', 'ratio': 0.75}, {'word': '二分类任务', 'ratio': 0.25}]",二元分类任务,{},[]
1014,1014,binary classifier,二分类器,0.25,8,"[{'word': '二元分类器', 'ratio': 0.625}, {'word': '二分类器', 'ratio': 0.25}, {'word': '分类器', 'ratio': 0.125}]",二元分类器,{},[]
1015,1015,binary constraint,二元约束,0.875,8,"[{'word': '二元约束', 'ratio': 0.875}, {'word': '元约束', 'ratio': 0.125}]",二元约束,{},[]
1016,1016,binary cross entropy,二元交叉熵,0.875,8,"[{'word': '二元交叉熵', 'ratio': 0.875}, {'word': '元交叉熵', 'ratio': 0.125}]",二元交叉熵,{},[]
1017,1017,binary cross-entropy loss,二进制交叉熵损失,0.0,8,"[{'word': '二元交叉熵损失', 'ratio': 0.875}, {'word': '元交叉熵损失', 'ratio': 0.125}]",二元交叉熵损失,{},[]
1018,1018,binary decision tree,二叉决策树,0.625,8,"[{'word': '二叉决策树', 'ratio': 0.625}, {'word': '二元决策树', 'ratio': 0.25}, {'word': '叉决策树', 'ratio': 0.125}]",二叉决策树,{},[]
1019,1019,binary feature,二值特征,0.0,10,"[{'word': '二元特征', 'ratio': 0.9}, {'word': '二进制特征', 'ratio': 0.1}]",二元特征,{},[]
1020,1020,binary label,二值标签,0.0,10,"[{'word': '二元标签', 'ratio': 0.9}, {'word': '二进制标签', 'ratio': 0.1}]",二元标签,{},[]
1021,1021,binary matrix,二进制矩阵,0.1,10,"[{'word': '二元矩阵', 'ratio': 0.9}, {'word': '二进制矩阵', 'ratio': 0.1}]",二元矩阵,{},[]
1022,1022,binary predicate,二元谓词,0.9,10,"[{'word': '二元谓词', 'ratio': 0.9}, {'word': '二进制谓词', 'ratio': 0.1}]",二元谓词,{},[]
1023,1023,binary relation,二元关系,1.0,10,"[{'word': '二元关系', 'ratio': 1.0}]",二元关系,{},[]
1024,1024,binary search,二分查找,0.875,8,"[{'word': '二分查找', 'ratio': 0.875}, {'word': '二分搜索', 'ratio': 0.125}]",二分查找,{},[]
1025,1025,binary segmentation,二值分割,1.0,8,"[{'word': '二值分割', 'ratio': 1.0}]",二值分割,{},[]
1026,1026,binary tree,二叉树,1.0,8,"[{'word': '二叉树', 'ratio': 1.0}]",二叉树,{},[]
1027,1027,binary variable,二值变量,0.125,8,"[{'word': '二元变量', 'ratio': 0.625}, {'word': '二进制变量', 'ratio': 0.25}, {'word': '二值变量', 'ratio': 0.125}]",二元变量,{},[]
1028,1028,binary vector,二进制向量,0.75,8,"[{'word': '二进制向量', 'ratio': 0.75}, {'word': '二元向量', 'ratio': 0.125}, {'word': '二值向量', 'ratio': 0.125}]",二进制向量,{},[]
1029,1029,bioinformatic,生物信息学,0.5555555555555556,9,"[{'word': '生物信息学', 'ratio': 0.5555555555555556}, {'word': '生物信息学的', 'ratio': 0.4444444444444444}]",生物信息学,{},[]
1030,1030,biological neural network,生物神经网络,1.0,9,"[{'word': '生物神经网络', 'ratio': 1.0}]",生物神经网络,{},[]
1031,1031,bipartite,二部图,0.0,9,"[{'word': '二分的', 'ratio': 0.6666666666666666}, {'word': '二分法', 'ratio': 0.1111111111111111}, {'word': '二分图的', 'ratio': 0.1111111111111111}, {'word': '二分', 'ratio': 0.1111111111111111}]",二分的,{},[]
1032,1032,bipartite graph,二部图,0.0,9,"[{'word': '二分图', 'ratio': 1.0}]",二分图,{},[]
1033,1033,bipartite matching,二分图匹配,0.0,9,"[{'word': '二分匹配', 'ratio': 1.0}]",二分匹配,{},[]
1034,1034,bipartite structure,双部分结构,0.0,10,"[{'word': '二分结构', 'ratio': 0.8}, {'word': '二部结构', 'ratio': 0.2}]",二分结构,{},[]
1035,1035,birth-death process,出生-死亡过程,0.6,10,"[{'word': '出生-死亡过程', 'ratio': 0.6}, {'word': '生灭过程', 'ratio': 0.3}, {'word': '二部结构', 'ratio': 0.1}]",出生-死亡过程,{},[]
1036,1036,bisection method,二分法,1.0,10,"[{'word': '二分法', 'ratio': 1.0}]",二分法,{},[]
1037,1037,bisimulation,双模拟,0.7,10,"[{'word': '双模拟', 'ratio': 0.7}, {'word': '双重模拟', 'ratio': 0.3}]",双模拟,{},[]
1038,1038,bitext,双语平行语料,0.0,10,"[{'word': '双语文本', 'ratio': 0.8}, {'word': '对照文本', 'ratio': 0.1}, {'word': '双语对照文本', 'ratio': 0.1}]",双语文本,{},[]
1039,1039,bitvector,比特向量,0.0,10,"[{'word': '位向量', 'ratio': 1.0}]",位向量,{},[]
1040,1040,black-box,黑箱,0.7,10,"[{'word': '黑箱', 'ratio': 0.7}, {'word': '黑盒', 'ratio': 0.3}]",黑箱,{},[]
1041,1041,black-box model,黑盒模型,0.3,10,"[{'word': '黑箱模型', 'ratio': 0.7}, {'word': '黑盒模型', 'ratio': 0.3}]",黑箱模型,{},[]
1042,1042,block coordinate descent,块坐标下降,0.0,10,"[{'word': '块坐标下降法', 'ratio': 0.7}, {'word': '分块坐标下降', 'ratio': 0.3}]",块坐标下降法,{},[]
1043,1043,block matrix,分块矩阵,0.3,10,"[{'word': '块矩阵', 'ratio': 0.7}, {'word': '分块矩阵', 'ratio': 0.3}]",块矩阵,{},[]
1044,1044,block-diagonal matrix,分块对角矩阵,0.3333333333333333,9,"[{'word': '块对角矩阵', 'ratio': 0.6666666666666666}, {'word': '分块对角矩阵', 'ratio': 0.3333333333333333}]",块对角矩阵,{},[]
1045,1045,blur kernel,模糊核,1.0,9,"[{'word': '模糊核', 'ratio': 1.0}]",模糊核,{},[]
1046,1046,boosting algorithm,提升算法,1.0,9,"[{'word': '提升算法', 'ratio': 1.0}]",提升算法,{},[]
1047,1047,boosting approach,提升方法,1.0,9,"[{'word': '提升方法', 'ratio': 1.0}]",提升方法,{},[]
1048,1048,bootstrap learning,自助学习,0.5555555555555556,9,"[{'word': '自助学习', 'ratio': 0.5555555555555556}, {'word': '自举学习', 'ratio': 0.4444444444444444}]",自助学习,{},[]
1049,1049,bootstrap resampling,引导重抽样,0.0,9,"[{'word': '自助法重采样', 'ratio': 0.4444444444444444}, {'word': '自助重采样', 'ratio': 0.3333333333333333}, {'word': '自助重抽样', 'ratio': 0.1111111111111111}, {'word': '自助法重抽样', 'ratio': 0.1111111111111111}]",自助法重采样,"1. Rank: 自助法重采样, 自助法重抽样, 自助重采样, 自助重抽样

2. Explanation: The term ""自助法重采样"" is the best fit because it accurately captures the concept of ""bootstrap"" in the context of statistical resampling. The term ""自助法"" (literally ""self-service method"") is widely recognized in the statistical community as referring to the bootstrap method, which is essential for understanding the technique's application in statistical significance testing. The inclusion of ""法"" (method) emphasizes that this is a specific statistical technique, which is crucial for clarity in academic and professional contexts. 

The back translation ""bootstrap resampling"" aligns perfectly with the original English term, ensuring semantic accuracy. 

The second choice, ""自助法重抽样,"" is also a strong candidate, as it retains the ""法"" and translates to ""bootstrap resampling,"" but ""重采样"" (resampling) is more commonly used in statistical literature than ""重抽样"" (re-sampling), making the first option more contextually appropriate.

The other two candidates, ""自助重采样"" and ""自助重抽样,"" lack the ""法"" component, which diminishes their specificity and clarity in the context of statistical methods. Thus, they are ranked lower.","['bootstrap resampling', 'self-service resampling', 'bootstrap resampling', 'bootstrap resampling']"
1050,1050,bootstrap sample,自助采样,0.0,9,"[{'word': '自助样本', 'ratio': 0.7777777777777778}, {'word': '自助法样本', 'ratio': 0.2222222222222222}]",自助样本,{},[]
1051,1051,bottleneck,瓶颈,1.0,9,"[{'word': '瓶颈', 'ratio': 1.0}]",瓶颈,{},[]
1052,1052,bottleneck layer,瓶颈层,1.0,9,"[{'word': '瓶颈层', 'ratio': 1.0}]",瓶颈层,{},[]
1053,1053,bottom-up,自底向上,0.0,9,"[{'word': '自下而上', 'ratio': 1.0}]",自下而上,{},[]
1054,1054,bottom-up learning,自底向上学习,0.0,9,"[{'word': '自下而上学习', 'ratio': 0.6666666666666666}, {'word': '自下而上的学习', 'ratio': 0.3333333333333333}]",自下而上学习,{},[]
1055,1055,bottom-up module,自底向上模块,0.0,9,"[{'word': '自下而上模块', 'ratio': 0.5555555555555556}, {'word': '自下而上的模块', 'ratio': 0.4444444444444444}]",自下而上模块,{},[]
1056,1056,bottom-up parsing,自底向上解析,0.0,9,"[{'word': '自下而上解析', 'ratio': 0.6666666666666666}, {'word': '自下而上的分析', 'ratio': 0.2222222222222222}, {'word': '自下而上的解析', 'ratio': 0.1111111111111111}]",自下而上解析,{},[]
1057,1057,bound variable,绑定变量,0.3333333333333333,9,"[{'word': '绑定变量', 'ratio': 0.3333333333333333}, {'word': '约束变量', 'ratio': 0.3333333333333333}, {'word': '有界变量', 'ratio': 0.2222222222222222}, {'word': '被绑定变量', 'ratio': 0.1111111111111111}]",绑定变量,"1. Rank: 绑定变量, 有界变量, 被绑定变量, 约束变量

2. Explanation: The term ""绑定变量"" (bind variables) is the best fit for the English term ""bound variable"" in the context of AI and programming languages. This translation accurately captures the concept of a variable that is bound to a value within a specific scope, which is essential in lambda calculus and functional programming. The back translation ""bind variables"" aligns closely with the original term, maintaining semantic accuracy.

The second candidate, ""有界变量"" (bounded variable), while also relevant, is less commonly used in the context of programming and may imply a different nuance, suggesting a variable with limits rather than one that is bound in a scope. 

""被绑定变量"" (bound variable) is a direct translation but is less idiomatic in Chinese and may not be as widely recognized in the AI community. 

Lastly, ""约束变量"" (constraint variables) introduces a different meaning related to constraints, which is not applicable in this context. Therefore, ""绑定变量"" is the most contextually appropriate and semantically accurate choice for the term ""bound variable"" in the AI domain.","['bind variables', 'Constraint variables', 'bounded variable', 'bound variable']"
1058,1058,bounded rationality,有界理性,0.2222222222222222,9,"[{'word': '有限理性', 'ratio': 0.7777777777777778}, {'word': '有界理性', 'ratio': 0.2222222222222222}]",有限理性,{},[]
1059,1059,bounding box,边界框,1.0,9,"[{'word': '边界框', 'ratio': 1.0}]",边界框,{},[]
1060,1060,bounding box detection,边界框检测,1.0,9,"[{'word': '边界框检测', 'ratio': 1.0}]",边界框检测,{},[]
1061,1061,bounding box regression,边界框回归,1.0,9,"[{'word': '边界框回归', 'ratio': 1.0}]",边界框回归,{},[]
1062,1062,bounding box regressor,边界框回归器,1.0,9,"[{'word': '边界框回归器', 'ratio': 1.0}]",边界框回归器,{},[]
1063,1063,branch and bound algorithm,分支定界算法,0.3333333333333333,9,"[{'word': '分支限界算法', 'ratio': 0.6666666666666666}, {'word': '分支定界算法', 'ratio': 0.3333333333333333}]",分支限界算法,{},[]
1064,1064,branching factor,分支因子,1.0,8,"[{'word': '分支因子', 'ratio': 1.0}]",分支因子,{},[]
1065,1065,breadth-first order,广度优先顺序,0.875,8,"[{'word': '广度优先顺序', 'ratio': 0.875}, {'word': '宽度优先顺序', 'ratio': 0.125}]",广度优先顺序,{},[]
1066,1066,breadth-first search,广度优先搜索,0.875,8,"[{'word': '广度优先搜索', 'ratio': 0.875}, {'word': '宽度优先搜索', 'ratio': 0.125}]",广度优先搜索,{},[]
1067,1067,brute force search,蛮力搜索,0.0,8,"[{'word': '暴力搜索', 'ratio': 0.625}, {'word': '穷举搜索', 'ratio': 0.375}]",暴力搜索,{},[]
1068,1068,burn-in,燃烧期,0.0,8,"[{'word': '预热期', 'ratio': 0.375}, {'word': '烧入', 'ratio': 0.25}, {'word': '预热阶段', 'ratio': 0.125}, {'word': '烧入期', 'ratio': 0.125}, {'word': '预热', 'ratio': 0.125}]","""烧入期""","1. Rank: ""烧入期"", ""烧入"", ""预热期"", ""预热阶段"", ""预热""

2. Explanation: The term ""烧入期"" (burn-in period) is the best fit for the AI domain-specific usage because it accurately captures the concept of a period during which initial samples are discarded to allow the model to stabilize before meaningful data collection begins. The back translation ""burn-in period"" aligns closely with the original English term, maintaining both semantic accuracy and contextual relevance. 

""烧入"" (burn in) is also a strong candidate, but it lacks the specificity of ""期"" (period), which is important in this context to denote a defined phase in the sampling process. 

""预热期"" (warm-up period) and ""预热阶段"" (warm-up phase) are less suitable because they imply a preparatory phase rather than the specific statistical context of discarding initial iterations, which is critical in the burn-in process. Lastly, ""预热"" (preheat) is too vague and does not convey the necessary meaning in this context. Thus, ""烧入期"" is the most contextually appropriate choice.","['warm-up period', 'burn in', 'warm-up phase', 'burn-in period', 'preheat']"
1069,1069,calculus of variation,变分法,0.8888888888888888,9,"[{'word': '变分法', 'ratio': 0.8888888888888888}, {'word': '变分', 'ratio': 0.1111111111111111}]",变分法,{},[]
1070,1070,calibration,校准,0.8888888888888888,9,"[{'word': '校准', 'ratio': 0.8888888888888888}, {'word': '标定', 'ratio': 0.1111111111111111}]",校准,{},[]
1071,1071,calibration method,校准方法,0.8888888888888888,9,"[{'word': '校准方法', 'ratio': 0.8888888888888888}, {'word': '标定方法', 'ratio': 0.1111111111111111}]",校准方法,{},[]
1072,1072,camera calibration,相机标定,0.7777777777777778,9,"[{'word': '相机标定', 'ratio': 0.7777777777777778}, {'word': '相机校准', 'ratio': 0.2222222222222222}]",相机标定,{},[]
1073,1073,camera intrinsic,相机内参,1.0,9,"[{'word': '相机内参', 'ratio': 1.0}]",相机内参,{},[]
1074,1074,camera matrix,相机矩阵,1.0,7,"[{'word': '相机矩阵', 'ratio': 1.0}]",相机矩阵,{},[]
1075,1075,camera parameter,相机参数,1.0,7,"[{'word': '相机参数', 'ratio': 1.0}]",相机参数,{},[]
1076,1076,camera pose estimation,相机姿态估计,0.8571428571428571,7,"[{'word': '相机姿态估计', 'ratio': 0.8571428571428571}, {'word': '相机位姿估计', 'ratio': 0.14285714285714285}]",相机姿态估计,{},[]
1077,1077,candidate generation,候选生成,1.0,7,"[{'word': '候选生成', 'ratio': 1.0}]",候选生成,{},[]
1078,1078,candidate set,候选集,1.0,7,"[{'word': '候选集', 'ratio': 1.0}]",候选集,{},[]
1079,1079,canonical basis,规范基底,0.1111111111111111,9,"[{'word': '标准基', 'ratio': 0.7777777777777778}, {'word': '规范基', 'ratio': 0.1111111111111111}, {'word': '规范基底', 'ratio': 0.1111111111111111}]",标准基,{},[]
1080,1080,canonical correlation analysis,典型相关分析,0.7777777777777778,9,"[{'word': '典型相关分析', 'ratio': 0.7777777777777778}, {'word': '标准相关分析', 'ratio': 0.2222222222222222}]",典型相关分析,{},[]
1081,1081,canonical form,标准形式,0.6666666666666666,9,"[{'word': '标准形式', 'ratio': 0.6666666666666666}, {'word': '规范形式', 'ratio': 0.2222222222222222}, {'word': '典型形式', 'ratio': 0.1111111111111111}]",标准形式,{},[]
1082,1082,canonical frame,规范帧,0.0,9,"[{'word': '标准框架', 'ratio': 0.5555555555555556}, {'word': '规范框架', 'ratio': 0.3333333333333333}, {'word': '标准坐标系', 'ratio': 0.1111111111111111}]",标准框架,{},[]
1083,1083,canonical space,规范空间,0.3333333333333333,9,"[{'word': '标准空间', 'ratio': 0.6666666666666666}, {'word': '规范空间', 'ratio': 0.3333333333333333}]",标准空间,{},[]
1084,1084,canonicalization,规范化,0.6,10,"[{'word': '规范化', 'ratio': 0.6}, {'word': '标准化', 'ratio': 0.4}]",规范化,{},[]
1085,1085,capsule network,胶囊网络,1.0,10,"[{'word': '胶囊网络', 'ratio': 1.0}]",胶囊网络,{},[]
1086,1086,cardinality,基数,1.0,10,"[{'word': '基数', 'ratio': 1.0}]",基数,{},[]
1087,1087,cardinality constraint,基数约束,1.0,10,"[{'word': '基数约束', 'ratio': 1.0}]",基数约束,{},[]
1088,1088,cascade model,级联模型,1.0,10,"[{'word': '级联模型', 'ratio': 1.0}]",级联模型,{},[]
1089,1089,catastrophic forgetting,灾难性遗忘,1.0,10,"[{'word': '灾难性遗忘', 'ratio': 1.0}]",灾难性遗忘,{},[]
1090,1090,categorial grammar,范畴语法,0.7,10,"[{'word': '范畴语法', 'ratio': 0.7}, {'word': '类别语法', 'ratio': 0.3}]",范畴语法,{},[]
1091,1091,categorical cross-entropy,类别交叉熵,0.4,10,"[{'word': '分类交叉熵', 'ratio': 0.6}, {'word': '类别交叉熵', 'ratio': 0.4}]",分类交叉熵,{},[]
1092,1092,categorical feature,类别特征,0.9,10,"[{'word': '类别特征', 'ratio': 0.9}, {'word': '分类特征', 'ratio': 0.1}]",类别特征,{},[]
1093,1093,causal effect,因果效应,0.8888888888888888,9,"[{'word': '因果效应', 'ratio': 0.8888888888888888}, {'word': 'Enter corrected translation Submit', 'ratio': 0.1111111111111111}]",因果效应,{},[]
1094,1094,causal effect estimation,因果效应估计,1.0,9,"[{'word': '因果效应估计', 'ratio': 1.0}]",因果效应估计,{},[]
1095,1095,causal entropy,因果熵,1.0,9,"[{'word': '因果熵', 'ratio': 1.0}]",因果熵,{},[]
1096,1096,causal graph,因果图,1.0,9,"[{'word': '因果图', 'ratio': 1.0}]",因果图,{},[]
1097,1097,causal inference,因果推断 (Causal Inference),0.0,9,"[{'word': '因果推断', 'ratio': 1.0}]",因果推断,{},[]
1098,1098,causal intervention,因果干预,1.0,9,"[{'word': '因果干预', 'ratio': 1.0}]",因果干预,{},[]
1099,1099,causal language model,因果语言模型,0.8888888888888888,9,"[{'word': '因果语言模型', 'ratio': 0.8888888888888888}, {'word': '因果干预', 'ratio': 0.1111111111111111}]",因果语言模型,{},[]
1100,1100,causal model,因果模型,1.0,9,"[{'word': '因果模型', 'ratio': 1.0}]",因果模型,{},[]
1101,1101,causal reasoning,因果推理,1.0,9,"[{'word': '因果推理', 'ratio': 1.0}]",因果推理,{},[]
1102,1102,causal rule,因果规则,1.0,9,"[{'word': '因果规则', 'ratio': 1.0}]",因果规则,{},[]
1103,1103,causal theory,因果理论,1.0,9,"[{'word': '因果理论', 'ratio': 1.0}]",因果理论,{},[]
1104,1104,cell state,细胞状态,0.0,9,"[{'word': '单元状态', 'ratio': 1.0}]",单元状态,{},[]
1105,1105,center crop,中心裁剪,1.0,9,"[{'word': '中心裁剪', 'ratio': 1.0}]",中心裁剪,{},[]
1106,1106,center of projection,投影中心,1.0,9,"[{'word': '投影中心', 'ratio': 1.0}]",投影中心,{},[]
1107,1107,centrality measure,中心性测度,0.0,9,"[{'word': '中心性度量', 'ratio': 1.0}]",中心性度量,{},[]
1108,1108,centroid,质心,0.5,10,"[{'word': '质心', 'ratio': 0.5}, {'word': '重心', 'ratio': 0.4}, {'word': '几何中心', 'ratio': 0.1}]",质心,{},[]
1109,1109,chain rule,链式法则,1.0,10,"[{'word': '链式法则', 'ratio': 1.0}]",链式法则,{},[]
1110,1110,change of basis,基底变换,0.1,10,"[{'word': '基变换', 'ratio': 0.8}, {'word': '基础变换', 'ratio': 0.1}, {'word': '基底变换', 'ratio': 0.1}]",基变换,{},[]
1111,1111,character embedding,字符嵌入,1.0,10,"[{'word': '字符嵌入', 'ratio': 1.0}]",字符嵌入,{},[]
1112,1112,character n-gram,字符n-gram,0.4,10,"[{'word': '字符n-gram', 'ratio': 0.4}, {'word': '字符n元组', 'ratio': 0.2}, {'word': '字符 n-元组', 'ratio': 0.1}, {'word': '文字 N グラム', 'ratio': 0.1}, {'word': '字符 n-gram', 'ratio': 0.1}, {'word': '字符', 'ratio': 0.1}]",字符n-gram,"1. Rank: 字符n-gram, 字符 n-元组, 字符n元组, 文字 N グラム, 字符, 字符 n-gram

2. Explanation: The term ""字符n-gram"" is the best fit for the translation of ""character n-gram"" because it accurately retains the original meaning and structure of the English term. The use of ""字符"" (character) directly corresponds to the ""character"" in the English term, and ""n-gram"" is a well-established term in the AI and natural language processing (NLP) fields, which is commonly used in both English and Chinese contexts. 

The other candidates, such as ""字符 n-元组"" and ""字符n元组,"" introduce the term ""元组"" (tuple), which is not semantically accurate in this context, as ""n-gram"" specifically refers to a sequence of n characters rather than a tuple. ""文字 N グラム"" translates to ""text N-gram,"" which is less precise since it shifts the focus from characters to text, potentially leading to confusion in the AI domain where character-level analysis is crucial. 

Overall, ""字符n-gram"" maintains both semantic accuracy and contextual fit, making it the most appropriate choice for the AI terminology in question.","['Character n-gram', 'character n-tuple', 'character n-tuple', 'Text N グラム', 'Character n-gram', 'character']"
1113,1113,characteristic function,特征函数,1.0,10,"[{'word': '特征函数', 'ratio': 1.0}]",特征函数,{},[]
1114,1114,characteristic polynomial,特征多项式,1.0,10,"[{'word': '特征多项式', 'ratio': 1.0}]",特征多项式,{},[]
1115,1115,characteristic vector,特征向量,1.0,10,"[{'word': '特征向量', 'ratio': 1.0}]",特征向量,{},[]
1116,1116,chart parser,图表解析器,0.9,10,"[{'word': '图表解析器', 'ratio': 0.9}, {'word': '语法分析器', 'ratio': 0.1}]",图表解析器,{},[]
1117,1117,chart parsing,图表解析,0.8,10,"[{'word': '图表解析', 'ratio': 0.8}, {'word': '图表分析', 'ratio': 0.1}, {'word': '语法分析', 'ratio': 0.1}]",图表解析,{},[]
1118,1118,chatbot,聊天机器人,1.0,8,"[{'word': '聊天机器人', 'ratio': 1.0}]",聊天机器人,{},[]
1119,1119,checkpoint,检查点,0.875,8,"[{'word': '检查点', 'ratio': 0.875}, {'word': '检查站', 'ratio': 0.125}]",检查点,{},[]
1120,1120,chemoinformatic,化学信息学,0.875,8,"[{'word': '化学信息学', 'ratio': 0.875}, {'word': '化学信息学的', 'ratio': 0.125}]",化学信息学,{},[]
1121,1121,chi-square distribution,卡方分布,1.0,8,"[{'word': '卡方分布', 'ratio': 1.0}]",卡方分布,{},[]
1122,1122,chi-square test,卡方检验,1.0,8,"[{'word': '卡方检验', 'ratio': 1.0}]",卡方检验,{},[]
1123,1123,child node,子节点,1.0,9,"[{'word': '子节点', 'ratio': 1.0}]",子节点,{},[]
1124,1124,chromosome,染色体,1.0,9,"[{'word': '染色体', 'ratio': 1.0}]",染色体,{},[]
1125,1125,chunk size,块大小,0.8888888888888888,9,"[{'word': '块大小', 'ratio': 0.8888888888888888}, {'word': '数据块大小', 'ratio': 0.1111111111111111}]",块大小,{},[]
1126,1126,citation network,引文网络,0.3333333333333333,9,"[{'word': '引用网络', 'ratio': 0.6666666666666666}, {'word': '引文网络', 'ratio': 0.3333333333333333}]",引用网络,{},[]
1127,1127,class,类别,0.7777777777777778,9,"[{'word': '类别', 'ratio': 0.7777777777777778}, {'word': '类', 'ratio': 0.2222222222222222}]",类别,{},[]
1128,1128,class balance,类平衡,0.0,9,"[{'word': '类别平衡', 'ratio': 1.0}]",类别平衡,{},[]
1129,1129,class distribution,类别分布,1.0,9,"[{'word': '类别分布', 'ratio': 1.0}]",类别分布,{},[]
1130,1130,class imbalance,类别不平衡,1.0,9,"[{'word': '类别不平衡', 'ratio': 1.0}]",类别不平衡,{},[]
1131,1131,class label,类标签,0.0,9,"[{'word': '类别标签', 'ratio': 1.0}]",类别标签,{},[]
1132,1132,class prior,类先验,0.0,9,"[{'word': '类别先验', 'ratio': 1.0}]",类别先验,{},[]
1133,1133,classical planning,经典规划,1.0,10,"[{'word': '经典规划', 'ratio': 1.0}]",经典规划,{},[]
1134,1134,classification,分类,1.0,20,"[{'word': '分类', 'ratio': 1.0}]",分类,{},[]
1135,1135,classification accuracy,分类准确率,1.0,10,"[{'word': '分类准确率', 'ratio': 1.0}]",分类准确率,{},[]
1136,1136,classification algorithm,分类算法,1.0,10,"[{'word': '分类算法', 'ratio': 1.0}]",分类算法,{},[]
1137,1137,classification approach,分类法,0.0,8,"[{'word': '分类方法', 'ratio': 1.0}]",分类方法,{},[]
1138,1138,classification error,分类错误率,0.0,8,"[{'word': '分类错误', 'ratio': 0.875}, {'word': '分类误差', 'ratio': 0.125}]",分类错误,{},[]
1139,1139,classification head,分类头,1.0,8,"[{'word': '分类头', 'ratio': 1.0}]",分类头,{},[]
1140,1140,classification loss,分类损失,1.0,8,"[{'word': '分类损失', 'ratio': 1.0}]",分类损失,{},[]
1141,1141,classification margin,分类边距,0.125,8,"[{'word': '分类边界', 'ratio': 0.625}, {'word': '分类间隔', 'ratio': 0.25}, {'word': '分类边距', 'ratio': 0.125}]",分类边界,{},[]
1142,1142,classification method,分类方法,1.0,9,"[{'word': '分类方法', 'ratio': 1.0}]",分类方法,{},[]
1143,1143,classification metric,分类指标,1.0,9,"[{'word': '分类指标', 'ratio': 1.0}]",分类指标,{},[]
1144,1144,classification model,分类模型,1.0,9,"[{'word': '分类模型', 'ratio': 1.0}]",分类模型,{},[]
1145,1145,classification network,分类网络,1.0,9,"[{'word': '分类网络', 'ratio': 1.0}]",分类网络,{},[]
1146,1146,classification objective,分类目标,1.0,9,"[{'word': '分类目标', 'ratio': 1.0}]",分类目标,{},[]
1147,1147,classification problem,分类问题,1.0,9,"[{'word': '分类问题', 'ratio': 1.0}]",分类问题,{},[]
1148,1148,classification score,分类得分,0.6666666666666666,9,"[{'word': '分类得分', 'ratio': 0.6666666666666666}, {'word': '分类分数', 'ratio': 0.3333333333333333}]",分类得分,{},[]
1149,1149,classification task,分类任务,1.0,9,"[{'word': '分类任务', 'ratio': 1.0}]",分类任务,{},[]
1150,1150,classification token,分类标记符,0.0,9,"[{'word': '分类标记', 'ratio': 1.0}]",分类标记,{},[]
1151,1151,clause learning,子句学习,1.0,8,"[{'word': '子句学习', 'ratio': 1.0}]",子句学习,{},[]
1152,1152,click model,点击模型,1.0,8,"[{'word': '点击模型', 'ratio': 1.0}]",点击模型,{},[]
1153,1153,clip range,夹持范围,0.0,8,"[{'word': '截断范围', 'ratio': 0.375}, {'word': '裁剪范围', 'ratio': 0.375}, {'word': '剪切范围', 'ratio': 0.125}, {'word': '剪辑范围', 'ratio': 0.125}]","""裁剪范围""","1. Rank: ""裁剪范围"", ""截断范围"", ""剪辑范围"", ""剪切范围""

2. Explanation: The term ""裁剪范围"" (clipping range) is the best fit for the AI domain-specific usage because it accurately conveys the concept of limiting or constraining values within a certain range, which is essential in hyperparameter tuning, particularly in reinforcement learning contexts. The back translation ""Cropping range"" aligns closely with the intended meaning in the AI context, where ""clipping"" refers to the process of restricting values to prevent extreme outputs, rather than physically cutting or cropping data. 

""截断范围"" (cut off range) is a close second, as it also suggests a limitation but may imply a more abrupt termination rather than a controlled adjustment, which is less precise in this context. 

""剪辑范围"" (clipping range) and ""剪切范围"" (cutting range) are less suitable because they imply a more physical or visual alteration rather than the mathematical or algorithmic adjustment intended in AI terminology. Thus, ""裁剪范围"" is the most semantically accurate and contextually appropriate choice.","['Cut off range', 'Cropping range', 'Cutting range', 'clipping range']"
1154,1154,clipping factor,裁剪因子,0.5555555555555556,9,"[{'word': '裁剪因子', 'ratio': 0.5555555555555556}, {'word': '剪切因子', 'ratio': 0.2222222222222222}, {'word': '截断因子', 'ratio': 0.2222222222222222}]",裁剪因子,{},[]
1155,1155,clipping threshold,限幅阈值,0.0,9,"[{'word': '裁剪阈值', 'ratio': 0.5555555555555556}, {'word': '剪切阈值', 'ratio': 0.2222222222222222}, {'word': '截断阈值', 'ratio': 0.2222222222222222}]",裁剪阈值,{},[]
1156,1156,clique potential,团势能,0.2222222222222222,9,"[{'word': '子集潜力', 'ratio': 0.2222222222222222}, {'word': '团势能', 'ratio': 0.2222222222222222}, {'word': '准 clique 潜力', 'ratio': 0.1111111111111111}, {'word': '团体潜力', 'ratio': 0.1111111111111111}, {'word': '团体势能', 'ratio': 0.1111111111111111}, {'word': '子群体势能', 'ratio': 0.1111111111111111}, {'word': '潜力', 'ratio': 0.1111111111111111}]","""团势能""","1. Rank: ""团势能"", ""团体势能"", ""团体潜力"", ""准 clique 潜力"", ""子群体势能"", ""子集潜力"", ""潜力""

2. Explanation: The term ""团势能"" (group potential energy) is the best fit for ""clique potential"" in the AI domain due to its semantic accuracy and contextual relevance. In the context of AI and statistical modeling, ""clique"" refers to a subset of nodes in a graph that are fully connected, which aligns with the concept of ""团"" (group) in Chinese. The term ""势能"" (potential energy) captures the idea of potential in a mathematical or computational sense, which is crucial in the context of transition models. 

Other candidates like ""团体潜力"" (group potential) and ""团体势能"" (group potential energy) are also relevant but do not convey the same level of specificity as ""团势能."" The term ""准 clique 潜力"" (quasi-clique potential) introduces ambiguity by suggesting a variation of the original term, which may not be appropriate in this context. ""子群体势能"" (subgroup potential energy) and ""子集潜力"" (subset potential) are less accurate as they imply a smaller or different grouping than what ""clique"" typically represents in graph theory. Lastly, ""潜力"" (potential) is too vague and lacks the necessary context to be useful in this specific AI application. Thus, ""团势能"" stands out as the most precise and contextually appropriate translation.","['subset potential', 'group potential energy', 'Quasi-clique potential', 'group potential', 'group potential energy', 'subgroup potential energy', 'potential']"
1157,1157,closed frequent itemset,闭合频繁项集,0.7777777777777778,9,"[{'word': '闭合频繁项集', 'ratio': 0.7777777777777778}, {'word': '闭频繁项集', 'ratio': 0.2222222222222222}]",闭合频繁项集,{},[]
1158,1158,closed-book model,闭卷模型,1.0,9,"[{'word': '闭卷模型', 'ratio': 1.0}]",闭卷模型,{},[]
1159,1159,closed-world,封闭世界,1.0,9,"[{'word': '封闭世界', 'ratio': 1.0}]",封闭世界,{},[]
1160,1160,cloze prompt,填空提示,0.2222222222222222,9,"[{'word': '完形填空提示', 'ratio': 0.7777777777777778}, {'word': '填空提示', 'ratio': 0.2222222222222222}]",完形填空提示,{},[]
1161,1161,cloze task,填空任务,0.2222222222222222,9,"[{'word': '完形填空任务', 'ratio': 0.7777777777777778}, {'word': '填空任务', 'ratio': 0.2222222222222222}]",完形填空任务,{},[]
1162,1162,cluster assignment,聚类分配,0.7777777777777778,9,"[{'word': '聚类分配', 'ratio': 0.7777777777777778}, {'word': '簇分配', 'ratio': 0.2222222222222222}]",聚类分配,{},[]
1163,1163,cluster center,簇中心,0.2222222222222222,9,"[{'word': '聚类中心', 'ratio': 0.7777777777777778}, {'word': '簇中心', 'ratio': 0.2222222222222222}]",聚类中心,{},[]
1164,1164,cluster centroid,簇中心,0.0,10,"[{'word': '聚类中心', 'ratio': 1.0}]",聚类中心,{},[]
1165,1165,cluster feature,簇特征,0.0,10,"[{'word': '聚类特征', 'ratio': 1.0}]",聚类特征,{},[]
1166,1166,cluster label,簇标签,0.0,10,"[{'word': '聚类标签', 'ratio': 1.0}]",聚类标签,{},[]
1167,1167,cluster size,簇大小,0.0,10,"[{'word': '聚类大小', 'ratio': 0.8}, {'word': '聚类规模', 'ratio': 0.2}]",聚类大小,{},[]
1168,1168,clustering algorithm,聚类算法,1.0,10,"[{'word': '聚类算法', 'ratio': 1.0}]",聚类算法,{},[]
1169,1169,clustering criterion,聚类准则,0.75,8,"[{'word': '聚类准则', 'ratio': 0.75}, {'word': '聚类标准', 'ratio': 0.25}]",聚类准则,{},[]
1170,1170,clustering method,聚类方法,1.0,8,"[{'word': '聚类方法', 'ratio': 1.0}]",聚类方法,{},[]
1171,1171,clustering problem,聚类问题,1.0,8,"[{'word': '聚类问题', 'ratio': 1.0}]",聚类问题,{},[]
1172,1172,co-occurrence,共现,0.75,8,"[{'word': '共现', 'ratio': 0.75}, {'word': '共同出现', 'ratio': 0.25}]",共现,{},[]
1173,1173,co-occurrence matrix,共现矩阵,0.75,8,"[{'word': '共现矩阵', 'ratio': 0.75}, {'word': '共同出现矩阵', 'ratio': 0.25}]",共现矩阵,{},[]
1174,1174,co-occurrence statistic,共现统计,0.8,10,"[{'word': '共现统计', 'ratio': 0.8}, {'word': '共同出现统计', 'ratio': 0.2}]",共现统计,{},[]
1175,1175,co-reference,共指,0.8,10,"[{'word': '共指', 'ratio': 0.8}, {'word': '共同指称', 'ratio': 0.1}, {'word': '共同参考', 'ratio': 0.1}]",共指,{},[]
1176,1176,co-training,协同训练,0.9,10,"[{'word': '协同训练', 'ratio': 0.9}, {'word': '共同培训', 'ratio': 0.1}]",协同训练,{},[]
1177,1177,coarse correlated equilibria,粗相关平衡,0.1,10,"[{'word': '粗糙相关均衡', 'ratio': 0.4}, {'word': '粗相关均衡', 'ratio': 0.4}, {'word': '粗略相关均衡', 'ratio': 0.1}, {'word': '粗相关平衡', 'ratio': 0.1}]",粗相关均衡,"1. Rank: 粗相关均衡, 粗糙相关均衡, 粗略相关均衡, 粗相关平衡

2. Explanation: The term ""粗相关均衡"" (coarse correlation equilibrium) is the best fit for the translation of ""coarse correlated equilibria"" due to its semantic accuracy and contextual relevance in the AI domain. This term retains the essential meaning of ""coarse"" and ""correlated equilibria,"" which are critical concepts in game theory and AI. The back translation ""coarse correlation equilibrium"" closely matches the original English term, ensuring clarity and precision in the context of algorithms and regret minimization discussed in the provided context. 

The other candidates, such as ""粗糙相关均衡"" (rough dependent equilibrium) and ""粗略相关均衡"" (rough correlated equilibrium), introduce unnecessary variations that could lead to confusion, as ""rough"" does not accurately capture the intended meaning of ""coarse"" in this context. Additionally, ""粗相关平衡"" (crude correlation balance) deviates from the established terminology in game theory, where ""均衡"" (equilibrium) is the preferred term over ""平衡"" (balance). Thus, ""粗相关均衡"" is the most appropriate choice for maintaining both semantic accuracy and contextual fit in the AI domain.","['rough dependent equilibrium', 'coarse correlation equilibrium', 'rough correlated equilibrium', 'crude correlation balance']"
1178,1178,coarse correlated equilibrium,粗粒度相关均衡,0.0,10,"[{'word': '粗相关均衡', 'ratio': 0.4}, {'word': '粗糙相关均衡', 'ratio': 0.3}, {'word': '粗糙相关均衡状态', 'ratio': 0.1}, {'word': '粗略相关均衡', 'ratio': 0.1}, {'word': '粗相关平衡', 'ratio': 0.1}]",粗相关均衡,"1. Rank: 粗相关均衡, 粗略相关均衡, 粗糙相关均衡, 粗糙相关均衡状态, 粗相关平衡

2. Explanation: The term ""粗相关均衡"" (coarse correlated equilibrium) is the best fit because it accurately captures the semantic meaning of the original English term while maintaining the correct terminology used in the AI domain. The term ""粗"" (coarse) directly translates to ""coarse,"" which is essential in this context, as it distinguishes this type of equilibrium from other forms. ""相关"" (correlated) is also a direct translation, preserving the concept of correlation that is crucial in game theory. The term ""均衡"" (equilibrium) is a standard term in both English and Chinese for this concept. 

The other candidates, such as ""粗糙相关均衡"" (rough correlated equilibrium) and ""粗略相关均衡"" (rough correlated equilibrium), introduce the word ""rough,"" which does not accurately reflect the intended meaning of ""coarse"" in this context. Additionally, ""粗相关平衡"" (crude correlation balance) alters the term ""equilibrium"" to ""balance,"" which can lead to confusion in the context of game theory. Therefore, ""粗相关均衡"" is the most semantically accurate and contextually appropriate translation for ""coarse correlated equilibrium"" in the AI domain.","['coarse correlation equilibrium', 'rough dependent equilibrium', 'Rough correlation equilibrium state', 'rough correlated equilibrium', 'crude correlation balance']"
1179,1179,coarse layer,粗糙层,0.1111111111111111,9,"[{'word': '粗层', 'ratio': 0.8888888888888888}, {'word': '粗糙层', 'ratio': 0.1111111111111111}]",粗层,{},[]
1180,1180,coarse-to-fine,由粗到细,0.0,9,"[{'word': '粗到细', 'ratio': 0.7777777777777778}, {'word': '从粗到细', 'ratio': 0.2222222222222222}]",粗到细,{},[]
1181,1181,coarse-to-fine approach,粗到细的方法,0.4444444444444444,9,"[{'word': '粗到细的方法', 'ratio': 0.4444444444444444}, {'word': '粗到细方法', 'ratio': 0.3333333333333333}, {'word': '从粗到细方法', 'ratio': 0.2222222222222222}]","""从粗到细方法""","1. Rank: ""从粗到细方法"", ""粗到细方法"", ""粗到细的方法""

2. Explanation: The term ""从粗到细方法"" is the best fit because it accurately captures the essence of the ""coarse-to-fine approach"" in a way that aligns with common usage in the AI domain. The phrase ""从粗到细"" (from coarse to fine) clearly indicates a progression or a systematic approach, which is essential in the context of algorithms and image processing. This term also maintains the structure of the original English term, making it easier for readers familiar with the concept to understand its application.

The second candidate, ""粗到细方法,"" is also a good option but lacks the preposition ""从"" (from), which adds clarity to the progression implied in the approach. The third candidate, ""粗到细的方法,"" includes an unnecessary particle ""的,"" which makes it sound more awkward and less formal in a technical context. Therefore, ""从粗到细方法"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['coarse to fine method', 'Coarse-to-fine method', 'Coarse-to-fine approach']"
1182,1182,coarse-to-fine cascade,由粗到细级联,0.0,9,"[{'word': '粗到细级联', 'ratio': 0.5555555555555556}, {'word': '粗到细的级联', 'ratio': 0.2222222222222222}, {'word': '从粗到细级联', 'ratio': 0.2222222222222222}]",粗到细级联,{},[]
1183,1183,coarse-to-fine strategy,粗到精策略,0.0,9,"[{'word': '粗到细策略', 'ratio': 0.6666666666666666}, {'word': '从粗到细策略', 'ratio': 0.2222222222222222}, {'word': '粗到细的策略', 'ratio': 0.1111111111111111}]",粗到细策略,{},[]
1184,1184,codebook,码本,0.7777777777777778,9,"[{'word': '码本', 'ratio': 0.7777777777777778}, {'word': '代码簿', 'ratio': 0.1111111111111111}, {'word': '代码本', 'ratio': 0.1111111111111111}]",码本,{},[]
1185,1185,codomain,值域,0.5555555555555556,9,"[{'word': '值域', 'ratio': 0.5555555555555556}, {'word': '陪域', 'ratio': 0.3333333333333333}, {'word': '余域', 'ratio': 0.1111111111111111}]",值域,{},[]
1186,1186,coefficient matrix,系数矩阵,1.0,9,"[{'word': '系数矩阵', 'ratio': 1.0}]",系数矩阵,{},[]
1187,1187,cognitive model,认知模型,1.0,9,"[{'word': '认知模型', 'ratio': 1.0}]",认知模型,{},[]
1188,1188,cognitive science,认知科学,1.0,9,"[{'word': '认知科学', 'ratio': 1.0}]",认知科学,{},[]
1189,1189,cold start,冷启动,1.0,10,"[{'word': '冷启动', 'ratio': 1.0}]",冷启动,{},[]
1190,1190,collaborative filtering,协同过滤,1.0,10,"[{'word': '协同过滤', 'ratio': 1.0}]",协同过滤,{},[]
1191,1191,collaborative learning,协作学习,0.8,10,"[{'word': '协作学习', 'ratio': 0.8}, {'word': '协同学习', 'ratio': 0.2}]",协作学习,{},[]
1192,1192,collective inference,集体推理,0.8,10,"[{'word': '集体推理', 'ratio': 0.8}, {'word': '集体推断', 'ratio': 0.2}]",集体推理,{},[]
1193,1193,color channel,颜色通道,0.8,10,"[{'word': '颜色通道', 'ratio': 0.8}, {'word': '色彩通道', 'ratio': 0.2}]",颜色通道,{},[]
1194,1194,color constancy,颜色恒常性,0.3333333333333333,9,"[{'word': '色彩恒常性', 'ratio': 0.6666666666666666}, {'word': '颜色恒常性', 'ratio': 0.3333333333333333}]",色彩恒常性,{},[]
1195,1195,colorization,上色,0.7777777777777778,9,"[{'word': '上色', 'ratio': 0.7777777777777778}, {'word': '着色', 'ratio': 0.2222222222222222}]",上色,{},[]
1196,1196,column space,列空间,1.0,9,"[{'word': '列空间', 'ratio': 1.0}]",列空间,{},[]
1197,1197,column vector,列向量,1.0,9,"[{'word': '列向量', 'ratio': 1.0}]",列向量,{},[]
1198,1198,combinator,组合子,1.0,9,"[{'word': '组合子', 'ratio': 1.0}]",组合子,{},[]
1199,1199,combinatorial explosion,组合爆炸,1.0,9,"[{'word': '组合爆炸', 'ratio': 1.0}]",组合爆炸,{},[]
1200,1200,combinatorial optimization,组合优化,1.0,9,"[{'word': '组合优化', 'ratio': 1.0}]",组合优化,{},[]
1201,1201,combinatorial optimization problem,组合优化问题,1.0,9,"[{'word': '组合优化问题', 'ratio': 1.0}]",组合优化问题,{},[]
1202,1202,commonsense inference,常识推理,1.0,9,"[{'word': '常识推理', 'ratio': 1.0}]",常识推理,{},[]
1203,1203,commonsense knowledge,常识知识,1.0,9,"[{'word': '常识知识', 'ratio': 1.0}]",常识知识,{},[]
1204,1204,commonsense knowledge graph,常识知识图谱,1.0,10,"[{'word': '常识知识图谱', 'ratio': 1.0}]",常识知识图谱,{},[]
1205,1205,communication complexity,通信复杂度,0.7,10,"[{'word': '通信复杂度', 'ratio': 0.7}, {'word': '传播复杂性', 'ratio': 0.2}, {'word': '通信复杂性', 'ratio': 0.1}]",通信复杂度,{},[]
1206,1206,communication graph,通信图,0.8,10,"[{'word': '通信图', 'ratio': 0.8}, {'word': '传播图', 'ratio': 0.2}]",通信图,{},[]
1207,1207,compatibility function,兼容性函数,1.0,10,"[{'word': '兼容性函数', 'ratio': 1.0}]",兼容性函数,{},[]
1208,1208,compatibility graph,兼容图,0.3333333333333333,9,"[{'word': '兼容性图', 'ratio': 0.6666666666666666}, {'word': '兼容图', 'ratio': 0.3333333333333333}]",兼容性图,{},[]
1209,1209,competitive ratio,竞争比率,0.7777777777777778,9,"[{'word': '竞争比率', 'ratio': 0.7777777777777778}, {'word': '竞争比', 'ratio': 0.2222222222222222}]",竞争比率,{},[]
1210,1210,composition function,组合函数,1.0,9,"[{'word': '组合函数', 'ratio': 1.0}]",组合函数,{},[]
1211,1211,compositional generalization,组合概括化,0.0,9,"[{'word': '组合泛化', 'ratio': 1.0}]",组合泛化,{},[]
1212,1212,compositional semantic,组合语义,1.0,9,"[{'word': '组合语义', 'ratio': 1.0}]",组合语义,{},[]
1213,1213,compressive sensing,压缩感知,1.0,10,"[{'word': '压缩感知', 'ratio': 1.0}]",压缩感知,{},[]
1214,1214,computation complexity,计算复杂度,0.9,10,"[{'word': '计算复杂度', 'ratio': 0.9}, {'word': '计算复杂性', 'ratio': 0.1}]",计算复杂度,{},[]
1215,1215,computation graph,计算图,1.0,10,"[{'word': '计算图', 'ratio': 1.0}]",计算图,{},[]
1216,1216,computational argumentation,计算论证,1.0,9,"[{'word': '计算论证', 'ratio': 1.0}]",计算论证,{},[]
1217,1217,computational budget,计算预算,1.0,9,"[{'word': '计算预算', 'ratio': 1.0}]",计算预算,{},[]
1218,1218,computational complexity,计算复杂性,0.1111111111111111,9,"[{'word': '计算复杂度', 'ratio': 0.8888888888888888}, {'word': '计算复杂性', 'ratio': 0.1111111111111111}]",计算复杂度,{},[]
1219,1219,computational experiment,计算实验,1.0,9,"[{'word': '计算实验', 'ratio': 1.0}]",计算实验,{},[]
1220,1220,computational graph,计算图,1.0,10,"[{'word': '计算图', 'ratio': 1.0}]",计算图,{},[]
1221,1221,computational model,计算模型,1.0,10,"[{'word': '计算模型', 'ratio': 1.0}]",计算模型,{},[]
1222,1222,compute budget,算力预算,0.0,10,"[{'word': '计算预算', 'ratio': 1.0}]",计算预算,{},[]
1223,1223,computer vision model,计算机视觉模型,1.0,10,"[{'word': '计算机视觉模型', 'ratio': 1.0}]",计算机视觉模型,{},[]
1224,1224,concatenation,拼接,0.25,20,"[{'word': '连接', 'ratio': 0.55}, {'word': '拼接', 'ratio': 0.35}, {'word': '串联', 'ratio': 0.1}]",连接,{},[]
1225,1225,concatenation operation,连接操作,0.7,10,"[{'word': '连接操作', 'ratio': 0.7}, {'word': '拼接操作', 'ratio': 0.2}, {'word': '串联操作', 'ratio': 0.1}]",连接操作,{},[]
1226,1226,concentration inequality,浓度不等式,0.0,10,"[{'word': '集中不等式', 'ratio': 1.0}]",集中不等式,{},[]
1227,1227,concentration parameter,浓度参数,0.0,10,"[{'word': '集中参数', 'ratio': 1.0}]",集中参数,{},[]
1228,1228,concept,概念,0.9,10,"[{'word': '概念', 'ratio': 0.9}, {'word': '集中参数', 'ratio': 0.1}]",概念,{},[]
1229,1229,concept assertion,概念断言,1.0,9,"[{'word': '概念断言', 'ratio': 1.0}]",概念断言,{},[]
1230,1230,concept atom,概念原子,1.0,9,"[{'word': '概念原子', 'ratio': 1.0}]",概念原子,{},[]
1231,1231,concept class,概念类,0.7777777777777778,9,"[{'word': '概念类', 'ratio': 0.7777777777777778}, {'word': '概念类别', 'ratio': 0.1111111111111111}, {'word': '概念课', 'ratio': 0.1111111111111111}]",概念类,{},[]
1232,1232,concept drift,概念漂移,1.0,9,"[{'word': '概念漂移', 'ratio': 1.0}]",概念漂移,{},[]
1233,1233,concept inclusion,概念包含,0.8888888888888888,9,"[{'word': '概念包含', 'ratio': 0.8888888888888888}, {'word': '概念包容', 'ratio': 0.1111111111111111}]",概念包含,{},[]
1234,1234,concept name,概念名,0.1111111111111111,9,"[{'word': '概念名称', 'ratio': 0.8888888888888888}, {'word': '概念名', 'ratio': 0.1111111111111111}]",概念名称,{},[]
1235,1235,condition number,条件数,1.0,9,"[{'word': '条件数', 'ratio': 1.0}]",条件数,{},[]
1236,1236,conditional computation,条件计算,1.0,9,"[{'word': '条件计算', 'ratio': 1.0}]",条件计算,{},[]
1237,1237,conditional density,条件密度,1.0,9,"[{'word': '条件密度', 'ratio': 1.0}]",条件密度,{},[]
1238,1238,conditional distribution,条件分布,1.0,9,"[{'word': '条件分布', 'ratio': 1.0}]",条件分布,{},[]
1239,1239,conditional effect,条件效应,0.9,10,"[{'word': '条件效应', 'ratio': 0.9}, {'word': '条件效果', 'ratio': 0.1}]",条件效应,{},[]
1240,1240,conditional entropy,条件熵,1.0,10,"[{'word': '条件熵', 'ratio': 1.0}]",条件熵,{},[]
1241,1241,conditional expectation,条件期望,1.0,10,"[{'word': '条件期望', 'ratio': 1.0}]",条件期望,{},[]
1242,1242,conditional gradient,条件梯度,1.0,10,"[{'word': '条件梯度', 'ratio': 1.0}]",条件梯度,{},[]
1243,1243,conditional independence,条件独立性,0.7,10,"[{'word': '条件独立性', 'ratio': 0.7}, {'word': '条件独立', 'ratio': 0.3}]",条件独立性,{},[]
1244,1244,conditional independency,条件独立性,1.0,10,"[{'word': '条件独立性', 'ratio': 1.0}]",条件独立性,{},[]
1245,1245,conditional likelihood,条件似然率,0.0,10,"[{'word': '条件似然', 'ratio': 0.9}, {'word': '条件独立性', 'ratio': 0.1}]",条件似然,{},[]
1246,1246,conditional log likelihood,条件对数似然,1.0,10,"[{'word': '条件对数似然', 'ratio': 1.0}]",条件对数似然,{},[]
1247,1247,conditional log probability,条件对数概率,1.0,10,"[{'word': '条件对数概率', 'ratio': 1.0}]",条件对数概率,{},[]
1248,1248,conditional maximum entropy,条件最大熵,1.0,10,"[{'word': '条件最大熵', 'ratio': 1.0}]",条件最大熵,{},[]
1249,1249,conditional model,条件模型,1.0,9,"[{'word': '条件模型', 'ratio': 1.0}]",条件模型,{},[]
1250,1250,conditional probability,条件概率,1.0,9,"[{'word': '条件概率', 'ratio': 1.0}]",条件概率,{},[]
1251,1251,conditional probability distribution,条件概率分布,1.0,9,"[{'word': '条件概率分布', 'ratio': 1.0}]",条件概率分布,{},[]
1252,1252,conditional sampling,条件采样,1.0,9,"[{'word': '条件采样', 'ratio': 1.0}]",条件采样,{},[]
1253,1253,conditional text generation,有条件文本生成,0.0,9,"[{'word': '条件文本生成', 'ratio': 0.8888888888888888}, {'word': '""Conditional text generation', 'ratio': 0.1111111111111111}]",条件文本生成,{},[]
1254,1254,conditioning,条件化,0.7777777777777778,9,"[{'word': '条件化', 'ratio': 0.7777777777777778}, {'word': '条件', 'ratio': 0.2222222222222222}]",条件化,{},[]
1255,1255,conditioning vector,条件向量,1.0,9,"[{'word': '条件向量', 'ratio': 1.0}]",条件向量,{},[]
1256,1256,confidence,置信度,1.0,9,"[{'word': '置信度', 'ratio': 1.0}]",置信度,{},[]
1257,1257,confidence bound,置信绑定,0.0,9,"[{'word': '置信界限', 'ratio': 0.7777777777777778}, {'word': '置信界', 'ratio': 0.2222222222222222}]",置信界限,{},[]
1258,1258,confidence interval,置信区间,1.0,9,"[{'word': '置信区间', 'ratio': 1.0}]",置信区间,{},[]
1259,1259,confidence map,置信度映射图,0.0,10,"[{'word': '置信度图', 'ratio': 0.7}, {'word': '置信图', 'ratio': 0.3}]",置信度图,{},[]
1260,1260,confidence score,置信度分数,0.7,10,"[{'word': '置信度分数', 'ratio': 0.7}, {'word': '置信分数', 'ratio': 0.2}, {'word': '置信度图', 'ratio': 0.1}]",置信度分数,{},[]
1261,1261,confidence threshold,置信度阈值,0.8,10,"[{'word': '置信度阈值', 'ratio': 0.8}, {'word': '置信阈值', 'ratio': 0.2}]",置信度阈值,{},[]
1262,1262,configuration,配置,1.0,10,"[{'word': '配置', 'ratio': 1.0}]",配置,{},[]
1263,1263,confusion matrix,混淆矩阵,1.0,10,"[{'word': '混淆矩阵', 'ratio': 1.0}]",混淆矩阵,{},[]
1264,1264,confusion network,混淆网络,1.0,9,"[{'word': '混淆网络', 'ratio': 1.0}]",混淆网络,{},[]
1265,1265,conjugate gradient,共轭梯度,0.7777777777777778,9,"[{'word': '共轭梯度', 'ratio': 0.7777777777777778}, {'word': '共轭梯度法', 'ratio': 0.2222222222222222}]",共轭梯度,{},[]
1266,1266,conjugate gradient descent,共轭梯度下降法,0.3333333333333333,9,"[{'word': '共轭梯度下降', 'ratio': 0.5555555555555556}, {'word': '共轭梯度下降法', 'ratio': 0.3333333333333333}, {'word': '共轭梯度', 'ratio': 0.1111111111111111}]",共轭梯度下降,{},[]
1267,1267,conjugate gradient method,共轭梯度法,1.0,9,"[{'word': '共轭梯度法', 'ratio': 1.0}]",共轭梯度法,{},[]
1268,1268,conjunct,合取项,0.0,9,"[{'word': '连接词', 'ratio': 0.7777777777777778}, {'word': '结合词', 'ratio': 0.1111111111111111}, {'word': '连词', 'ratio': 0.1111111111111111}]",连接词,{},[]
1269,1269,conjunctive normal form,合取范式 (CNF),0.0,8,"[{'word': '合取范式', 'ratio': 0.875}, {'word': '连接范式', 'ratio': 0.125}]",合取范式,{},[]
1270,1270,conjunctive query,联结查询,0.0,8,"[{'word': '合取查询', 'ratio': 0.5}, {'word': '连接查询', 'ratio': 0.5}]",合取查询,{},[]
1271,1271,connected component,连通分量,0.75,8,"[{'word': '连通分量', 'ratio': 0.75}, {'word': '连通组件', 'ratio': 0.25}]",连通分量,{},[]
1272,1272,connectionist model,连接主义模型,0.875,8,"[{'word': '连接主义模型', 'ratio': 0.875}, {'word': '联结主义模型', 'ratio': 0.125}]",连接主义模型,{},[]
1273,1273,connectivity matrix,连通性矩阵,0.375,8,"[{'word': '连接矩阵', 'ratio': 0.5}, {'word': '连通性矩阵', 'ratio': 0.375}, {'word': '连通矩阵', 'ratio': 0.125}]",连接矩阵,{},[]
1274,1274,consensus network decoding,共识网络解码,1.0,8,"[{'word': '共识网络解码', 'ratio': 1.0}]",共识网络解码,{},[]
1275,1275,consequent,后件,0.25,8,"[{'word': '结果', 'ratio': 0.625}, {'word': '后件', 'ratio': 0.25}, {'word': '后果', 'ratio': 0.125}]",结果,{},[]
1276,1276,consistent estimator,一致估计量,0.875,8,"[{'word': '一致估计量', 'ratio': 0.875}, {'word': '一致性估计量', 'ratio': 0.125}]",一致估计量,{},[]
1277,1277,constellation model,星座模型,1.0,8,"[{'word': '星座模型', 'ratio': 1.0}]",星座模型,{},[]
1278,1278,constituency parser,短语结构分析器,0.0,8,"[{'word': '成分解析器', 'ratio': 0.375}, {'word': '成分句法分析器', 'ratio': 0.375}, {'word': '句法分析器', 'ratio': 0.125}, {'word': '句法成分分析器', 'ratio': 0.125}]",成分句法分析器,"1. Rank: 成分句法分析器, 成分解析器, 句法成分分析器, 句法分析器

2. Explanation: The term ""成分句法分析器"" (component syntax analyzer) is the best fit for ""constituency parser"" because it accurately captures both the semantic meaning and the specific function of the term within the AI domain. ""成分"" (constituent/component) directly relates to the concept of constituents in linguistic theory, while ""句法分析器"" (syntax analyzer) clearly indicates that it is a tool used for syntactic analysis. This term maintains the context of parsing in natural language processing, making it more precise for AI applications. 

The second option, ""成分解析器"" (component parser), is also a good candidate but is slightly less specific as it uses ""解析器"" (parser) instead of ""分析器"" (analyzer), which may not convey the same depth of syntactic analysis. The other candidates, ""句法成分分析器"" (syntactic component analyzer) and ""句法分析器"" (syntax analyzer), while relevant, do not emphasize the constituency aspect as effectively as ""成分句法分析器."" Thus, the first term is the most contextually appropriate for the AI domain.","['component parser', 'component parser', 'syntax analyzer', 'syntactic component analyzer']"
1279,1279,constituency parsing,成分句法分析,0.3333333333333333,9,"[{'word': '成分句法分析', 'ratio': 0.3333333333333333}, {'word': '成分解析', 'ratio': 0.3333333333333333}, {'word': '短语结构分析', 'ratio': 0.2222222222222222}, {'word': '句法分析', 'ratio': 0.1111111111111111}]",成分句法分析,"1. Rank: 成分句法分析, 短语结构分析, 句法分析, 成分解析

2. Explanation: The term ""成分句法分析"" (component syntax analysis) is the best fit for ""constituency parsing"" because it accurately captures the semantic meaning of the original term within the context of natural language processing and linguistics. ""成分"" (constituent) directly refers to the components of a sentence, which is central to constituency parsing, while ""句法分析"" (syntax analysis) indicates that it pertains to the structure of sentences. 

""短语结构分析"" (phrase structure analysis) is also a strong candidate, as it relates closely to the concept of constituency parsing, but it is slightly less precise because it emphasizes phrases rather than constituents, which can lead to ambiguity in certain contexts.

""句法分析"" (syntactic analysis) is more general and does not specifically denote the constituency aspect, making it less suitable for the specific term ""constituency parsing.""

""成分解析"" (ingredient analysis) is the least appropriate as it introduces a misleading interpretation of ""成分"" (constituent) as ""ingredient,"" which does not align with the linguistic context. 

Overall, ""成分句法分析"" is the most contextually accurate and semantically precise translation for the AI domain.","['component syntax analysis', 'Ingredient analysis', 'Phrase structure analysis', 'syntactic analysis']"
1280,1280,constituency tree,成分树,0.7777777777777778,9,"[{'word': '成分树', 'ratio': 0.7777777777777778}, {'word': '短语结构树', 'ratio': 0.1111111111111111}, {'word': '短语结构分析', 'ratio': 0.1111111111111111}]",成分树,{},[]
1281,1281,constituent parsing,短语结构分析,0.1111111111111111,9,"[{'word': '成分解析', 'ratio': 0.4444444444444444}, {'word': '成分分析', 'ratio': 0.3333333333333333}, {'word': '成分树', 'ratio': 0.1111111111111111}, {'word': '短语结构分析', 'ratio': 0.1111111111111111}]",短语结构分析,"1. Rank: 短语结构分析, 成分解析, 成分分析, 成分树

2. Explanation: The term ""短语结构分析"" (Phrase structure analysis) is the best fit for ""constituent parsing"" in the AI domain because it directly relates to the concept of analyzing the structure of phrases in a sentence, which is a fundamental aspect of parsing in natural language processing. This term accurately captures the essence of what constituent parsing entails, focusing on the hierarchical structure of phrases rather than merely identifying ingredients or components, which is what ""成分解析"" (Ingredient analysis) and ""成分分析"" (Ingredient analysis) imply. 

While ""成分解析"" and ""成分分析"" are close in meaning, they lack the specificity and contextual relevance to parsing in linguistics. ""成分树"" (ingredient tree) is also less appropriate as it suggests a tree structure but does not convey the parsing process itself. Therefore, ""短语结构分析"" is the most semantically accurate and contextually fitting term for the AI domain.","['Ingredient analysis', 'Ingredient analysis', 'ingredient tree', 'Phrase structure analysis']"
1282,1282,constituent structure,成分结构,0.8888888888888888,9,"[{'word': '成分结构', 'ratio': 0.8888888888888888}, {'word': '成分解析', 'ratio': 0.1111111111111111}]",成分结构,{},[]
1283,1283,constrained beam search,约束波束搜索,0.0,9,"[{'word': '约束束搜索', 'ratio': 0.6666666666666666}, {'word': '受限束搜索', 'ratio': 0.3333333333333333}]",约束束搜索,{},[]
1284,1284,constrained decoding,受约束解码,0.1111111111111111,9,"[{'word': '约束解码', 'ratio': 0.8888888888888888}, {'word': '受约束解码', 'ratio': 0.1111111111111111}]",约束解码,{},[]
1285,1285,constrained optimization,约束优化,1.0,9,"[{'word': '约束优化', 'ratio': 1.0}]",约束优化,{},[]
1286,1286,constrained optimization problem,约束优化问题,1.0,9,"[{'word': '约束优化问题', 'ratio': 1.0}]",约束优化问题,{},[]
1287,1287,constraint,约束条件,0.5,18,"[{'word': '约束', 'ratio': 0.8888888888888888}, {'word': '约束条件', 'ratio': 0.1111111111111111}]",约束,{},[]
1288,1288,constraint generation,约束生成,1.0,6,"[{'word': '约束生成', 'ratio': 1.0}]",约束生成,{},[]
1289,1289,constraint programming,约束编程,1.0,6,"[{'word': '约束编程', 'ratio': 1.0}]",约束编程,{},[]
1290,1290,constraint propagation,约束传播,1.0,6,"[{'word': '约束传播', 'ratio': 1.0}]",约束传播,{},[]
1291,1291,constraint satisfaction,约束满足问题,0.0,6,"[{'word': '约束满足', 'ratio': 1.0}]",约束满足,{},[]
1292,1292,constraint satisfaction problem,约束满足问题,1.0,6,"[{'word': '约束满足问题', 'ratio': 1.0}]",约束满足问题,{},[]
1293,1293,constraint set,约束集合,0.5,10,"[{'word': '约束集合', 'ratio': 0.5}, {'word': '约束集', 'ratio': 0.5}]",约束集合,{},[]
1294,1294,content model,内容模型,1.0,10,"[{'word': '内容模型', 'ratio': 1.0}]",内容模型,{},[]
1295,1295,content selection,内容选择,1.0,10,"[{'word': '内容选择', 'ratio': 1.0}]",内容选择,{},[]
1296,1296,context encoder,上下文编码器,1.0,10,"[{'word': '上下文编码器', 'ratio': 1.0}]",上下文编码器,{},[]
1297,1297,context free grammar,上下文无关文法,0.6,10,"[{'word': '上下文无关文法', 'ratio': 0.6}, {'word': '无上下文文法', 'ratio': 0.2}, {'word': '无关文法', 'ratio': 0.1}, {'word': '上下文无关语法', 'ratio': 0.1}]",上下文无关文法,{},[]
1298,1298,context model,上下文模型,1.0,10,"[{'word': '上下文模型', 'ratio': 1.0}]",上下文模型,{},[]
1299,1299,context vector,上下文向量,1.0,10,"[{'word': '上下文向量', 'ratio': 1.0}]",上下文向量,{},[]
1300,1300,context window,上下文窗口,1.0,10,"[{'word': '上下文窗口', 'ratio': 1.0}]",上下文窗口,{},[]
1301,1301,context-free language,上下文无关语言,0.7,10,"[{'word': '上下文无关语言', 'ratio': 0.7}, {'word': '无上下文语言', 'ratio': 0.2}, {'word': '无关语言', 'ratio': 0.1}]",上下文无关语言,{},[]
1302,1302,contextual embedding,语境嵌入,0.0,10,"[{'word': '上下文嵌入', 'ratio': 1.0}]",上下文嵌入,{},[]
1303,1303,contextual feature,上下文特征,1.0,10,"[{'word': '上下文特征', 'ratio': 1.0}]",上下文特征,{},[]
1304,1304,contextual information,上下文信息,1.0,10,"[{'word': '上下文信息', 'ratio': 1.0}]",上下文信息,{},[]
1305,1305,contextual model,上下文模型,1.0,10,"[{'word': '上下文模型', 'ratio': 1.0}]",上下文模型,{},[]
1306,1306,contextual representation,上下文表示,1.0,8,"[{'word': '上下文表示', 'ratio': 1.0}]",上下文表示,{},[]
1307,1307,contextual vector,上下文向量,1.0,8,"[{'word': '上下文向量', 'ratio': 1.0}]",上下文向量,{},[]
1308,1308,contextual word embedding,上下文词嵌入,1.0,8,"[{'word': '上下文词嵌入', 'ratio': 1.0}]",上下文词嵌入,{},[]
1309,1309,contextualized embedding,上下文嵌入,0.375,8,"[{'word': '上下文化嵌入', 'ratio': 0.5}, {'word': '上下文嵌入', 'ratio': 0.375}, {'word': '上下文化的嵌入', 'ratio': 0.125}]",上下文化嵌入,{},[]
1310,1310,contextualized representation,上下文表示,0.3,10,"[{'word': '上下文化表示', 'ratio': 0.7}, {'word': '上下文表示', 'ratio': 0.3}]",上下文化表示,{},[]
1311,1311,contextualized word vector,上下文词向量,0.3,10,"[{'word': '上下文化词向量', 'ratio': 0.7}, {'word': '上下文词向量', 'ratio': 0.3}]",上下文化词向量,{},[]
1312,1312,contingency table,条件概率表,0.0,10,"[{'word': '列联表', 'ratio': 0.9}, {'word': 'contingency表', 'ratio': 0.1}]",列联表,{},[]
1313,1313,continual learning,持续学习,0.9,10,"[{'word': '持续学习', 'ratio': 0.9}, {'word': '连续学习', 'ratio': 0.1}]",持续学习,{},[]
1314,1314,continuous normalizing flow,连续归一化流,0.9,10,"[{'word': '连续归一化流', 'ratio': 0.9}, {'word': '连续归一化流 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.1}]",连续归一化流,{},[]
1315,1315,contrastive approach,对比学习方法,0.0,10,"[{'word': '对比方法', 'ratio': 1.0}]",对比方法,{},[]
1316,1316,contrastive fine-tuning,对比式微调,0.0,10,"[{'word': '对比微调', 'ratio': 0.9}, {'word': '对比方法', 'ratio': 0.1}]",对比微调,{},[]
1317,1317,contrastive loss,对比损失,0.9,10,"[{'word': '对比损失', 'ratio': 0.9}, {'word': '对比微调', 'ratio': 0.1}]",对比损失,{},[]
1318,1318,contrastive objective,对比目标,0.9,10,"[{'word': '对比目标', 'ratio': 0.9}, {'word': '对比损失', 'ratio': 0.1}]",对比目标,{},[]
1319,1319,control variate,控制变量,1.0,10,"[{'word': '控制变量', 'ratio': 1.0}]",控制变量,{},[]
1320,1320,controllable text generation,可控文本生成,1.0,9,"[{'word': '可控文本生成', 'ratio': 1.0}]",可控文本生成,{},[]
1321,1321,conv layer,卷积层,1.0,9,"[{'word': '卷积层', 'ratio': 1.0}]",卷积层,{},[]
1322,1322,convergence,收敛,1.0,9,"[{'word': '收敛', 'ratio': 1.0}]",收敛,{},[]
1323,1323,convergence analysis,收敛分析,0.4444444444444444,9,"[{'word': '收敛性分析', 'ratio': 0.5555555555555556}, {'word': '收敛分析', 'ratio': 0.4444444444444444}]",收敛性分析,{},[]
1324,1324,convergence bound,收敛界限,0.3333333333333333,9,"[{'word': '收敛界', 'ratio': 0.6666666666666666}, {'word': '收敛界限', 'ratio': 0.3333333333333333}]",收敛界,{},[]
1325,1325,convergence criterion,收敛准则,0.6666666666666666,9,"[{'word': '收敛准则', 'ratio': 0.6666666666666666}, {'word': '收敛标准', 'ratio': 0.3333333333333333}]",收敛准则,{},[]
1326,1326,convergence rate,收敛速率,0.7777777777777778,9,"[{'word': '收敛速率', 'ratio': 0.7777777777777778}, {'word': '收敛速度', 'ratio': 0.2222222222222222}]",收敛速率,{},[]
1327,1327,convergence time,收敛时间,1.0,9,"[{'word': '收敛时间', 'ratio': 1.0}]",收敛时间,{},[]
1328,1328,conversation history,对话历史记录,0.0,9,"[{'word': '对话历史', 'ratio': 1.0}]",对话历史,{},[]
1329,1329,conversational agent,对话代理,1.0,10,"[{'word': '对话代理', 'ratio': 1.0}]",对话代理,{},[]
1330,1330,conversational dialogue system,交互式对话系统,0.0,10,"[{'word': '对话系统', 'ratio': 0.7}, {'word': '会话对话系统', 'ratio': 0.3}]",对话系统,{},[]
1331,1331,convex,凸的,0.6,10,"[{'word': '凸的', 'ratio': 0.6}, {'word': '凸', 'ratio': 0.4}]",凸的,{},[]
1332,1332,convex combination,凸组合,1.0,10,"[{'word': '凸组合', 'ratio': 1.0}]",凸组合,{},[]
1333,1333,convex conjugate,凸共轭,1.0,10,"[{'word': '凸共轭', 'ratio': 1.0}]",凸共轭,{},[]
1334,1334,convex constraint,凸约束,1.0,10,"[{'word': '凸约束', 'ratio': 1.0}]",凸约束,{},[]
1335,1335,convex decomposition,凸分解,1.0,10,"[{'word': '凸分解', 'ratio': 1.0}]",凸分解,{},[]
1336,1336,convex function,凸函数,0.9,10,"[{'word': '凸函数', 'ratio': 0.9}, {'word': '凸包', 'ratio': 0.1}]",凸函数,{},[]
1337,1337,convex hull,凸包,1.0,10,"[{'word': '凸包', 'ratio': 1.0}]",凸包,{},[]
1338,1338,convex loss,凸损失,1.0,10,"[{'word': '凸损失', 'ratio': 1.0}]",凸损失,{},[]
1339,1339,convex objective,凸目标函数,0.1111111111111111,9,"[{'word': '凸目标', 'ratio': 0.8888888888888888}, {'word': '凸目标函数', 'ratio': 0.1111111111111111}]",凸目标,{},[]
1340,1340,convex objective function,凸目标函数,1.0,9,"[{'word': '凸目标函数', 'ratio': 1.0}]",凸目标函数,{},[]
1341,1341,convex optimization,凸优化,1.0,9,"[{'word': '凸优化', 'ratio': 1.0}]",凸优化,{},[]
1342,1342,convex optimization problem,凸优化问题,1.0,9,"[{'word': '凸优化问题', 'ratio': 1.0}]",凸优化问题,{},[]
1343,1343,convex problem,凸问题,0.8888888888888888,9,"[{'word': '凸问题', 'ratio': 0.8888888888888888}, {'word': '凸问题  These translations sh', 'ratio': 0.1111111111111111}]",凸问题,{},[]
1344,1344,convex program,凸优化问题,0.8,10,"[{'word': '凸优化问题', 'ratio': 0.8}, {'word': '凸规划', 'ratio': 0.2}]",凸优化问题,{},[]
1345,1345,convex proxy,凸代理,0.7,10,"[{'word': '凸代理', 'ratio': 0.7}, {'word': '凸代理函数', 'ratio': 0.2}, {'word': '凸近似', 'ratio': 0.1}]",凸代理,{},[]
1346,1346,convex quadratic program,凸二次规划,0.7,10,"[{'word': '凸二次规划', 'ratio': 0.7}, {'word': '凸二次规划问题', 'ratio': 0.2}, {'word': '凸二次优化问题', 'ratio': 0.1}]",凸二次规划,{},[]
1347,1347,convex relaxation,凸松弛化,0.0,10,"[{'word': '凸松弛', 'ratio': 0.9}, {'word': '凸放松', 'ratio': 0.1}]",凸松弛,{},[]
1348,1348,convex risk minimization,凸风险最小化,1.0,10,"[{'word': '凸风险最小化', 'ratio': 1.0}]",凸风险最小化,{},[]
1349,1349,convex set,凸集,0.8,10,"[{'word': '凸集', 'ratio': 0.8}, {'word': '凸集合', 'ratio': 0.2}]",凸集,{},[]
1350,1350,convex surrogate,凸替代物,0.0,10,"[{'word': '凸替代', 'ratio': 0.5}, {'word': '凸代理', 'ratio': 0.3}, {'word': '凸凹', 'ratio': 0.1}, {'word': '凸集', 'ratio': 0.1}]",凸替代,{},[]
1351,1351,convex-concave,凸凹,0.5,10,"[{'word': '凸凹', 'ratio': 0.5}, {'word': '凸-凹', 'ratio': 0.3}, {'word': '凸性', 'ratio': 0.1}, {'word': '凸集', 'ratio': 0.1}]",凸凹,{},[]
1352,1352,convexity,凸性,0.9,10,"[{'word': '凸性', 'ratio': 0.9}, {'word': '凸集', 'ratio': 0.1}]",凸性,{},[]
1353,1353,convolution kernel,卷积核,0.9,10,"[{'word': '卷积核', 'ratio': 0.9}, {'word': '凸集', 'ratio': 0.1}]",卷积核,{},[]
1354,1354,convolution layer,卷积层,1.0,9,"[{'word': '卷积层', 'ratio': 1.0}]",卷积层,{},[]
1355,1355,convolution neural network,卷积神经网络,1.0,9,"[{'word': '卷积神经网络', 'ratio': 1.0}]",卷积神经网络,{},[]
1356,1356,convolution operation,卷积运算,0.3333333333333333,9,"[{'word': '卷积操作', 'ratio': 0.6666666666666666}, {'word': '卷积运算', 'ratio': 0.3333333333333333}]",卷积操作,{},[]
1357,1357,convolution operator,卷积算子,1.0,9,"[{'word': '卷积算子', 'ratio': 1.0}]",卷积算子,{},[]
1358,1358,convolutional,卷积,0.0,9,"[{'word': '卷积的', 'ratio': 1.0}]",卷积的,{},[]
1359,1359,convolutional architecture,卷积架构,1.0,9,"[{'word': '卷积架构', 'ratio': 1.0}]",卷积架构,{},[]
1360,1360,convolutional block,卷积块,1.0,9,"[{'word': '卷积块', 'ratio': 1.0}]",卷积块,{},[]
1361,1361,convolutional decoder,卷积解码器,1.0,9,"[{'word': '卷积解码器', 'ratio': 1.0}]",卷积解码器,{},[]
1362,1362,convolutional encoder,卷积编码器,1.0,9,"[{'word': '卷积编码器', 'ratio': 1.0}]",卷积编码器,{},[]
1363,1363,convolutional feature,卷积特征,1.0,9,"[{'word': '卷积特征', 'ratio': 1.0}]",卷积特征,{},[]
1364,1364,convolutional filter,卷积滤波器,1.0,10,"[{'word': '卷积滤波器', 'ratio': 1.0}]",卷积滤波器,{},[]
1365,1365,convolutional kernel,卷积核,1.0,10,"[{'word': '卷积核', 'ratio': 1.0}]",卷积核,{},[]
1366,1366,convolutional layer,卷积层,1.0,10,"[{'word': '卷积层', 'ratio': 1.0}]",卷积层,{},[]
1367,1367,convolutional network,卷积网络,1.0,10,"[{'word': '卷积网络', 'ratio': 1.0}]",卷积网络,{},[]
1368,1368,convolutional neural net,卷积神经网络,1.0,10,"[{'word': '卷积神经网络', 'ratio': 1.0}]",卷积神经网络,{},[]
1369,1369,convolutional neural network,卷积神经网络,1.0,9,"[{'word': '卷积神经网络', 'ratio': 1.0}]",卷积神经网络,{},[]
1370,1370,convolutional representation,卷积表示,1.0,9,"[{'word': '卷积表示', 'ratio': 1.0}]",卷积表示,{},[]
1371,1371,cooling schedule,冷却进度表,0.0,9,"[{'word': '冷却计划', 'ratio': 0.5555555555555556}, {'word': '冷却策略', 'ratio': 0.3333333333333333}, {'word': '退火时间表', 'ratio': 0.1111111111111111}]",冷却计划,{},[]
1372,1372,coordinate ascent,坐标上升算法,0.0,9,"[{'word': '坐标上升法', 'ratio': 0.5555555555555556}, {'word': '坐标上升', 'ratio': 0.4444444444444444}]",坐标上升法,{},[]
1373,1373,coordinate descent,坐标下降法,0.5555555555555556,9,"[{'word': '坐标下降法', 'ratio': 0.5555555555555556}, {'word': '坐标下降', 'ratio': 0.3333333333333333}, {'word': '坐标上升', 'ratio': 0.1111111111111111}]",坐标下降法,{},[]
1374,1374,coordinate descent algorithm,坐标下降算法,1.0,6,"[{'word': '坐标下降算法', 'ratio': 1.0}]",坐标下降算法,{},[]
1375,1375,coordinate frame,坐标系,0.5,6,"[{'word': '坐标框架', 'ratio': 0.5}, {'word': '坐标系', 'ratio': 0.5}]",坐标框架,{},[]
1376,1376,copy mechanism,复制机制,1.0,6,"[{'word': '复制机制', 'ratio': 1.0}]",复制机制,{},[]
1377,1377,core tensor,核张量,0.5,6,"[{'word': '核张量', 'ratio': 0.5}, {'word': '核心张量', 'ratio': 0.5}]",核张量,{},[]
1378,1378,coreference annotation,指代关系标注,0.0,10,"[{'word': '共指标注', 'ratio': 0.8}, {'word': '共指注释', 'ratio': 0.1}, {'word': '指代消解标注', 'ratio': 0.1}]",共指标注,{},[]
1379,1379,coreference chain,共指链,0.9,10,"[{'word': '共指链', 'ratio': 0.9}, {'word': '指代链', 'ratio': 0.1}]",共指链,{},[]
1380,1380,coreference resolution model,指代消解模型,0.1,10,"[{'word': '共指解析模型', 'ratio': 0.5}, {'word': '共指消解模型', 'ratio': 0.4}, {'word': '指代消解模型', 'ratio': 0.1}]",共指解析模型,{},[]
1381,1381,coreference resolution system,指代消解系统,0.1,10,"[{'word': '共指解析系统', 'ratio': 0.5}, {'word': '共指消解系统', 'ratio': 0.4}, {'word': '指代消解系统', 'ratio': 0.1}]",共指解析系统,{},[]
1382,1382,coreferent,共指,0.1,10,"[{'word': '共指实体', 'ratio': 0.5}, {'word': '共指项', 'ratio': 0.2}, {'word': '共指', 'ratio': 0.1}, {'word': '共指的', 'ratio': 0.1}, {'word': '指代', 'ratio': 0.1}]",共指实体,{},[]
1383,1383,correlated equilibria,相关均衡,0.8888888888888888,9,"[{'word': '相关均衡', 'ratio': 0.8888888888888888}, {'word': '协同训练', 'ratio': 0.1111111111111111}]",相关均衡,{},[]
1384,1384,correlated equilibrium,相关均衡,0.7777777777777778,9,"[{'word': '相关均衡', 'ratio': 0.7777777777777778}, {'word': '相关均衡状态', 'ratio': 0.1111111111111111}, {'word': '粗相关均衡', 'ratio': 0.1111111111111111}]",相关均衡,{},[]
1385,1385,correlation coefficient,相关系数,0.8888888888888888,9,"[{'word': '相关系数', 'ratio': 0.8888888888888888}, {'word': '相关均衡', 'ratio': 0.1111111111111111}]",相关系数,{},[]
1386,1386,correspondence matrix,对应矩阵,0.875,8,"[{'word': '对应矩阵', 'ratio': 0.875}, {'word': 'correspondance 矩阵', 'ratio': 0.125}]",对应矩阵,{},[]
1387,1387,cosine,余弦,0.9375,16,"[{'word': '余弦', 'ratio': 0.9375}, {'word': '余弦 如果需要更多帮助，请告诉我！', 'ratio': 0.0625}]",余弦,{},[]
1388,1388,cosine decay,余弦衰减,1.0,9,"[{'word': '余弦衰减', 'ratio': 1.0}]",余弦衰减,{},[]
1389,1389,cosine decay schedule,余弦衰减调度,0.3333333333333333,9,"[{'word': '余弦衰减计划', 'ratio': 0.6666666666666666}, {'word': '余弦衰减调度', 'ratio': 0.3333333333333333}]",余弦衰减计划,{},[]
1390,1390,cosine learning rate schedule,余弦学习率调度,0.3333333333333333,9,"[{'word': '余弦学习率计划', 'ratio': 0.6666666666666666}, {'word': '余弦学习率调度', 'ratio': 0.3333333333333333}]",余弦学习率计划,{},[]
1391,1391,cosine measure,余弦相似度,0.0,9,"[{'word': '余弦度量', 'ratio': 1.0}]",余弦度量,{},[]
1392,1392,cosine schedule,余弦调度,0.7777777777777778,9,"[{'word': '余弦调度', 'ratio': 0.7777777777777778}, {'word': '余弦计划', 'ratio': 0.2222222222222222}]",余弦调度,{},[]
1393,1393,cosine similarity measure,余弦相似度度量,1.0,9,"[{'word': '余弦相似度度量', 'ratio': 1.0}]",余弦相似度度量,{},[]
1394,1394,cost function,代价函数,0.5555555555555556,9,"[{'word': '代价函数', 'ratio': 0.5555555555555556}, {'word': '成本函数', 'ratio': 0.4444444444444444}]",代价函数,{},[]
1395,1395,cost vector,成本向量,0.4444444444444444,9,"[{'word': '代价向量', 'ratio': 0.5555555555555556}, {'word': '成本向量', 'ratio': 0.4444444444444444}]",代价向量,{},[]
1396,1396,cost volume,成本体积,0.4444444444444444,9,"[{'word': '成本体积', 'ratio': 0.4444444444444444}, {'word': '代价体', 'ratio': 0.3333333333333333}, {'word': '代价体积', 'ratio': 0.2222222222222222}]",成本体积,"1. Rank: 成本体积, 代价体, 代价体积

2. Explanation: The term ""成本体积"" (cost volume) is the best fit for the AI domain-specific usage because it accurately captures the concept of ""cost"" in a financial or resource allocation context, which is essential in AI planning and optimization tasks. The term ""体积"" (volume) is also appropriate as it suggests a multidimensional space where various costs can be evaluated, aligning well with the context of estimating costs in planning scenarios. 

The second candidate, ""代价体"" (cost body), while it retains the idea of cost, lacks the specificity of ""volume,"" which is crucial in the context of the provided examples. The term ""代价体积"" (cost volume) is a direct translation but is less commonly used in the AI field compared to ""成本体积."" Therefore, ""成本体积"" is the most semantically accurate and contextually fitting term for the AI terminology in question.","['cost volume', 'cost body', 'cost volume']"
1397,1397,cost-sensitive learning,成本敏感学习,0.5555555555555556,9,"[{'word': '成本敏感学习', 'ratio': 0.5555555555555556}, {'word': '代价敏感学习', 'ratio': 0.4444444444444444}]",成本敏感学习,{},[]
1398,1398,counterexample,反例,1.0,9,"[{'word': '反例', 'ratio': 1.0}]",反例,{},[]
1399,1399,counterfactual datum,反事实数据,1.0,9,"[{'word': '反事实数据', 'ratio': 1.0}]",反事实数据,{},[]
1400,1400,counterfactual example,反事实例子,0.1111111111111111,9,"[{'word': '反事实示例', 'ratio': 0.7777777777777778}, {'word': '反事实例', 'ratio': 0.1111111111111111}, {'word': '反事实例子', 'ratio': 0.1111111111111111}]",反事实示例,{},[]
1401,1401,counterfactual fairness,反事实公平,0.4444444444444444,9,"[{'word': '反事实公平性', 'ratio': 0.5555555555555556}, {'word': '反事实公平', 'ratio': 0.4444444444444444}]",反事实公平性,{},[]
1402,1402,counterfactual reasoning,反事实推理,1.0,9,"[{'word': '反事实推理', 'ratio': 1.0}]",反事实推理,{},[]
1403,1403,covariance function,协方差函数,1.0,9,"[{'word': '协方差函数', 'ratio': 1.0}]",协方差函数,{},[]
1404,1404,covariance kernel,协方差核函数,0.0,9,"[{'word': '协方差核', 'ratio': 1.0}]",协方差核,{},[]
1405,1405,covariance matrix,协方差矩阵,1.0,9,"[{'word': '协方差矩阵', 'ratio': 1.0}]",协方差矩阵,{},[]
1406,1406,covariance model,协方差模型,1.0,9,"[{'word': '协方差模型', 'ratio': 1.0}]",协方差模型,{},[]
1407,1407,covariance operator,协方差算子,1.0,9,"[{'word': '协方差算子', 'ratio': 1.0}]",协方差算子,{},[]
1408,1408,covariance parameter,协方差参数,1.0,9,"[{'word': '协方差参数', 'ratio': 1.0}]",协方差参数,{},[]
1409,1409,covariance structure,协方差结构,1.0,9,"[{'word': '协方差结构', 'ratio': 1.0}]",协方差结构,{},[]
1410,1410,covariant derivative,协变导数,0.8888888888888888,9,"[{'word': '协变导数', 'ratio': 0.8888888888888888}, {'word': '协方差结构', 'ratio': 0.1111111111111111}]",协变导数,{},[]
1411,1411,covariate,协变量,1.0,9,"[{'word': '协变量', 'ratio': 1.0}]",协变量,{},[]
1412,1412,covariate shift,协变量偏移,1.0,10,"[{'word': '协变量偏移', 'ratio': 1.0}]",协变量偏移,{},[]
1413,1413,credit assignment,信用分配,0.8,10,"[{'word': '信用分配', 'ratio': 0.8}, {'word': '奖励分配', 'ratio': 0.1}, {'word': '归因问题', 'ratio': 0.1}]",信用分配,{},[]
1414,1414,credit assignment problem,学分分配问题,0.0,10,"[{'word': '信用分配问题', 'ratio': 0.8}, {'word': '奖励分配问题', 'ratio': 0.1}, {'word': '归因问题', 'ratio': 0.1}]",信用分配问题,{},[]
1415,1415,criterion,准则,0.3,10,"[{'word': '标准', 'ratio': 0.7}, {'word': '准则', 'ratio': 0.3}]",标准,{},[]
1416,1416,critic,评价者,0.0,10,"[{'word': '评论家', 'ratio': 0.4}, {'word': '评论器', 'ratio': 0.3}, {'word': '评估者', 'ratio': 0.2}, {'word': '评论员', 'ratio': 0.1}]",评论家,"1. Rank: 评论家, 评估者, 评论员, 评论器

2. Explanation: The term ""评论家"" (critic) is the best fit in the context of AI and reinforcement learning because it aligns closely with the concept of a ""critic"" in actor-critic methods, where the critic evaluates the actions taken by the actor. In this context, ""评论家"" conveys the idea of someone or something that assesses or critiques the performance of the agent, which is essential in reinforcement learning frameworks. 

The other candidates, such as ""评估者"" (evaluator), while somewhat relevant, do not carry the same connotation of critical assessment that ""评论家"" does. ""评论员"" (commentator) and ""评论器"" (commentator) are less suitable as they imply a more passive role of commenting rather than actively evaluating or critiquing the actions taken by the agent. Therefore, ""评论家"" is the most semantically accurate and contextually appropriate term for the AI domain.","['critic', 'commentator', 'evaluator', 'commentator']"
1417,1417,critic loss,评判器损失,0.0,9,"[{'word': '评论者损失', 'ratio': 0.3333333333333333}, {'word': '评论家损失', 'ratio': 0.2222222222222222}, {'word': '评论器损失', 'ratio': 0.2222222222222222}, {'word': '评估损失', 'ratio': 0.1111111111111111}, {'word': '评价损失', 'ratio': 0.1111111111111111}]","""评论家损失""","1. Rank: ""评论家损失"", ""评论者损失"", ""评论器损失"", ""评价损失"", ""评估损失""

2. Explanation: The term ""评论家损失"" (critic loss) is the best fit for the AI domain because it accurately captures the concept of a ""critic"" in reinforcement learning contexts, where a critic evaluates the performance of a policy. The back translation ""critic loss"" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. 

In contrast, ""评论者损失"" (commenter loss) and ""评论器损失"" (commentator loss) do not convey the same technical meaning as ""critic"" in this context, as they imply a more general or informal role rather than the specific evaluative function of a critic in AI. The terms ""评价损失"" (evaluate loss) and ""评估损失"" (assess the damage) are also less suitable because they lack the specific reference to the role of a critic in reinforcement learning, making them less precise in this technical context. Thus, ""评论家损失"" is the most appropriate choice.","['commenter loss', 'critic loss', 'commentator loss', 'Assess the damage', 'Evaluate loss']"
1418,1418,critic network,评价网络,0.1111111111111111,9,"[{'word': '评论者网络', 'ratio': 0.3333333333333333}, {'word': '评论家网络', 'ratio': 0.2222222222222222}, {'word': '评论器网络', 'ratio': 0.2222222222222222}, {'word': '评估网络', 'ratio': 0.1111111111111111}, {'word': '评价网络', 'ratio': 0.1111111111111111}]",评论家网络,"1. Rank: 评论家网络, 评论者网络, 评价网络, 评论器网络, 评估网络

2. Explanation: The term ""评论家网络"" (critic network) is the best fit because it accurately captures the semantic meaning of ""critic"" in the context of AI, particularly in reinforcement learning. In this domain, a ""critic"" refers to a component that evaluates the actions taken by an ""actor"" and provides feedback to improve the policy. The term ""评论家"" directly translates to ""critic"" in a way that aligns with its usage in AI literature, where it implies a role of evaluation and judgment.

The second choice, ""评论者网络,"" translates to ""reviewer network,"" which is less precise in the AI context, as ""reviewer"" does not convey the same evaluative role as ""critic."" The other candidates, such as ""评价网络"" (rating network) and ""评估网络"" (evaluate the network), while related, do not specifically denote the duality of actor-critic methods in reinforcement learning, where the ""critic"" is a distinct entity that assesses the performance of the ""actor."" Thus, ""评论家网络"" is the most contextually appropriate and semantically accurate translation.","['reviewer network', 'Critics Network', 'commentator network', 'Evaluate the network', 'Rating Network']"
1419,1419,cross attention,交叉注意力,1.0,9,"[{'word': '交叉注意力', 'ratio': 1.0}]",交叉注意力,{},[]
1420,1420,cross entropy,交叉熵,1.0,9,"[{'word': '交叉熵', 'ratio': 1.0}]",交叉熵,{},[]
1421,1421,cross entropy error,交叉熵误差,1.0,9,"[{'word': '交叉熵误差', 'ratio': 1.0}]",交叉熵误差,{},[]
1422,1422,cross validation,交叉验证,1.0,9,"[{'word': '交叉验证', 'ratio': 1.0}]",交叉验证,{},[]
1423,1423,cross-attention layer,交叉注意力层,1.0,9,"[{'word': '交叉注意力层', 'ratio': 1.0}]",交叉注意力层,{},[]
1424,1424,cross-attention module,交叉注意力模块,1.0,9,"[{'word': '交叉注意力模块', 'ratio': 1.0}]",交叉注意力模块,{},[]
1425,1425,cross-correlation,交叉相关,0.1111111111111111,9,"[{'word': '互相关', 'ratio': 0.8888888888888888}, {'word': '交叉相关', 'ratio': 0.1111111111111111}]",互相关,{},[]
1426,1426,cross-entropy loss function,交叉熵损失函数,1.0,9,"[{'word': '交叉熵损失函数', 'ratio': 1.0}]",交叉熵损失函数,{},[]
1427,1427,cross-entropy objective,交叉熵目标,0.9,10,"[{'word': '交叉熵目标', 'ratio': 0.9}, {'word': '交叉熵目标函数', 'ratio': 0.1}]",交叉熵目标,{},[]
1428,1428,cross-lingual benchmark,跨语言基准测试,0.1,10,"[{'word': '跨语言基准', 'ratio': 0.9}, {'word': '跨语言基准测试', 'ratio': 0.1}]",跨语言基准,{},[]
1429,1429,cross-lingual embedding,跨语言嵌入,1.0,10,"[{'word': '跨语言嵌入', 'ratio': 1.0}]",跨语言嵌入,{},[]
1430,1430,cross-lingual feature,跨语言特征,1.0,10,"[{'word': '跨语言特征', 'ratio': 1.0}]",跨语言特征,{},[]
1431,1431,cross-lingual knowledge transfer,跨语言知识迁移,0.7,10,"[{'word': '跨语言知识迁移', 'ratio': 0.7}, {'word': '跨语言知识转移', 'ratio': 0.3}]",跨语言知识迁移,{},[]
1432,1432,cross-lingual model,跨语言模型,1.0,9,"[{'word': '跨语言模型', 'ratio': 1.0}]",跨语言模型,{},[]
1433,1433,cross-lingual representation,跨语种表征,0.0,9,"[{'word': '跨语言表示', 'ratio': 1.0}]",跨语言表示,{},[]
1434,1434,cross-lingual transfer,跨语言迁移,1.0,9,"[{'word': '跨语言迁移', 'ratio': 1.0}]",跨语言迁移,{},[]
1435,1435,cross-modal,跨模态,1.0,9,"[{'word': '跨模态', 'ratio': 1.0}]",跨模态,{},[]
1436,1436,cross-validating,交叉验证,1.0,9,"[{'word': '交叉验证', 'ratio': 1.0}]",交叉验证,{},[]
1437,1437,cumulant generating function,累积矩生成函数,0.0,10,"[{'word': '累积量生成函数', 'ratio': 0.5}, {'word': '累积生成函数', 'ratio': 0.3}, {'word': '累加生成函数', 'ratio': 0.1}, {'word': '矩生成函数', 'ratio': 0.1}]",累积量生成函数,{},[]
1438,1438,cumulative density function,累积分布函数,0.6,10,"[{'word': '累积分布函数', 'ratio': 0.6}, {'word': '累积密度函数', 'ratio': 0.4}]",累积分布函数,{},[]
1439,1439,cumulative distribution function,累积分布函数,1.0,10,"[{'word': '累积分布函数', 'ratio': 1.0}]",累积分布函数,{},[]
1440,1440,cumulative regret,累积遗憾,0.8,10,"[{'word': '累积遗憾', 'ratio': 0.8}, {'word': '累计遗憾', 'ratio': 0.1}, {'word': '累积后悔', 'ratio': 0.1}]",累积遗憾,{},[]
1441,1441,cumulative reward,累积回报,0.0,10,"[{'word': '累积奖励', 'ratio': 1.0}]",累积奖励,{},[]
1442,1442,curriculum learning,课程学习,1.0,9,"[{'word': '课程学习', 'ratio': 1.0}]",课程学习,{},[]
1443,1443,curse of dimensionality,维数诅咒,0.1111111111111111,9,"[{'word': '维度灾难', 'ratio': 0.5555555555555556}, {'word': '维度诅咒', 'ratio': 0.3333333333333333}, {'word': '维数诅咒', 'ratio': 0.1111111111111111}]",维度灾难,{},[]
1444,1444,cutting plane,切割平面,0.3333333333333333,9,"[{'word': '切平面', 'ratio': 0.3333333333333333}, {'word': '切割平面', 'ratio': 0.3333333333333333}, {'word': '割平面', 'ratio': 0.3333333333333333}]",切割平面,"1. Rank: 切割平面, 切平面, 割平面

2. Explanation: The term ""切割平面"" (cutting plane) is the most accurate translation in the context of AI and optimization. It directly corresponds to the English term ""cutting plane"" and retains the specific meaning used in mathematical optimization and algorithmic contexts. The back translation is also accurate, reinforcing its semantic fit. 

""切平面"" (tangent plane) is misleading because it refers to a different concept in mathematics, specifically related to the tangent of a curve or surface, which does not align with the intended meaning of ""cutting plane"" in this context. 

""割平面"" (cutting plane) is less commonly used and may not be recognized in the same way as ""切割平面."" While it does translate back to ""cutting plane,"" it lacks the established usage and clarity found in the first option. 

Overall, ""切割平面"" is the best fit due to its direct correspondence to the English term, its established usage in the AI domain, and its clarity in conveying the intended meaning.","['tangent plane', 'cutting plane', 'cutting plane']"
1445,1445,cutting plane algorithm,割平面算法,0.3333333333333333,9,"[{'word': '切平面算法', 'ratio': 0.3333333333333333}, {'word': '割平面算法', 'ratio': 0.3333333333333333}, {'word': '切割平面算法', 'ratio': 0.2222222222222222}, {'word': '剖切面算法', 'ratio': 0.1111111111111111}]",割平面算法,"1. Rank: 割平面算法, 切平面算法, 切割平面算法, 剖切面算法

2. Explanation: The term ""割平面算法"" (cutting plane algorithm) is the most accurate translation because it directly corresponds to the English term without introducing ambiguity. It retains the specific meaning of ""cutting plane"" in the context of optimization algorithms, which is crucial in AI and machine learning. The back translation is also accurate, reinforcing its semantic fit. 

""切平面算法"" (tangent plane algorithm) is less suitable because ""切"" (cut) can imply a different mathematical concept, leading to potential confusion. ""切割平面算法"" (cutting plane algorithm) is a close contender but is slightly less common in usage compared to ""割平面算法."" Lastly, ""剖切面算法"" (sectional plane algorithm) diverges significantly from the original meaning, as it suggests a different mathematical operation that is not relevant in this context. Thus, ""割平面算法"" is the best choice for its clarity and direct alignment with the established terminology in the AI domain.","['tangent plane algorithm', 'cutting plane algorithm', 'Cutting plane algorithm', 'Sectional plane algorithm']"
1446,1446,cutting plane method,切平面法,0.1111111111111111,9,"[{'word': '割平面法', 'ratio': 0.3333333333333333}, {'word': '切平面方法', 'ratio': 0.2222222222222222}, {'word': '切割平面方法', 'ratio': 0.2222222222222222}, {'word': '切平面法', 'ratio': 0.1111111111111111}, {'word': '剖切平面法', 'ratio': 0.1111111111111111}]",割平面法,"1. Rank: 割平面法, 切割平面方法, 剖切平面法, 切平面法, 切平面方法

2. Explanation: The term ""割平面法"" is the best fit for the translation of ""cutting plane method"" in the AI domain due to its semantic accuracy and established usage in mathematical optimization contexts. The term ""割"" (cut) directly corresponds to ""cutting,"" and ""平面"" (plane) accurately reflects the ""plane"" aspect of the method. This term is widely recognized in the field of optimization and aligns well with the technical context provided.

In contrast, ""切平面方法"" translates back to ""tangent plane method,"" which is incorrect in this context as it refers to a different concept in mathematics. ""切平面法"" also translates back to ""tangent plane method,"" making it unsuitable. ""切割平面方法"" and ""剖切平面法"" are less commonly used and may not be as recognizable in the field. Therefore, ""割平面法"" stands out as the most appropriate choice for its clarity, accuracy, and alignment with established terminology in the AI and optimization domains.","['cutting plane method', 'tangent plane method', 'Cutting plane method', 'tangent plane method', 'cutting plane method']"
1447,1447,cycle consistency,循环一致性,1.0,9,"[{'word': '循环一致性', 'ratio': 1.0}]",循环一致性,{},[]
1448,1448,cycle consistency loss,循环一致性损失,1.0,9,"[{'word': '循环一致性损失', 'ratio': 1.0}]",循环一致性损失,{},[]
1449,1449,cycle inequality,环不等式,0.0,9,"[{'word': '循环不等式', 'ratio': 0.8888888888888888}, {'word': '环路不等式', 'ratio': 0.1111111111111111}]",循环不等式,{},[]
1450,1450,d-separation,d-分离,1.0,9,"[{'word': 'd-分离', 'ratio': 1.0}]",d-分离,{},[]
1451,1451,d_model,模型维度,0.2222222222222222,9,"[{'word': 'd_model', 'ratio': 0.4444444444444444}, {'word': 'd_模型', 'ratio': 0.2222222222222222}, {'word': '模型维度', 'ratio': 0.2222222222222222}, {'word': '保留原名)', 'ratio': 0.1111111111111111}]",d_模型,"1. Rank: d_模型, 模型维度, d_model, 保留原名

2. Explanation: The term ""d_模型"" is the best fit because it retains the original structure of the English term while providing a clear indication of its meaning in the context of AI. ""模型"" translates to ""model,"" which is a common term in machine learning and AI, making it semantically accurate. The back translation to ""d_model"" confirms that it maintains the original term's integrity, which is crucial in technical contexts where precision is key.

""模型维度"" (model dimensions) is also a relevant term, as it describes the dimensional aspect of the model, but it lacks the specificity of the original term ""d_model."" It could lead to ambiguity in certain contexts where ""d_model"" has a specific defined meaning.

The term ""d_model"" as a direct transliteration does not provide any contextual understanding, and ""保留原名"" (keep original name) is not a translation but rather a suggestion to retain the English term, which is not ideal for a Chinese-speaking audience looking for clarity in technical documentation.

Overall, ""d_模型"" effectively balances semantic accuracy and contextual fit, making it the most suitable choice in the AI domain.","['the_model', 'd_model', 'Model dimensions', 'Keep original name)']"
1452,1452,data distribution,数据分布,1.0,9,"[{'word': '数据分布', 'ratio': 1.0}]",数据分布,{},[]
1453,1453,data imbalance,数据不平衡,1.0,9,"[{'word': '数据不平衡', 'ratio': 1.0}]",数据不平衡,{},[]
1454,1454,data manifold,数据流形,1.0,9,"[{'word': '数据流形', 'ratio': 1.0}]",数据流形,{},[]
1455,1455,data mining,数据挖掘,1.0,9,"[{'word': '数据挖掘', 'ratio': 1.0}]",数据挖掘,{},[]
1456,1456,data point,数据点,1.0,10,"[{'word': '数据点', 'ratio': 1.0}]",数据点,{},[]
1457,1457,data processing inequality,数据处理不等式,1.0,10,"[{'word': '数据处理不等式', 'ratio': 1.0}]",数据处理不等式,{},[]
1458,1458,data sparseness,数据稀疏性,1.0,10,"[{'word': '数据稀疏性', 'ratio': 1.0}]",数据稀疏性,{},[]
1459,1459,data sparsity,数据稀疏性,0.6,10,"[{'word': '数据稀疏性', 'ratio': 0.6}, {'word': '数据稀疏', 'ratio': 0.4}]",数据稀疏性,{},[]
1460,1460,data structure,数据结构,1.0,10,"[{'word': '数据结构', 'ratio': 1.0}]",数据结构,{},[]
1461,1461,data vector,数据向量,1.0,5,"[{'word': '数据向量', 'ratio': 1.0}]",数据向量,{},[]
1462,1462,data-to-text generation,数据到文本生成,1.0,5,"[{'word': '数据到文本生成', 'ratio': 1.0}]",数据到文本生成,{},[]
1463,1463,dataset augmentation,数据集增强,0.8,5,"[{'word': '数据集增强', 'ratio': 0.8}, {'word': '数据集扩增', 'ratio': 0.2}]",数据集增强,{},[]
1464,1464,dataset bias,数据集偏差,1.0,5,"[{'word': '数据集偏差', 'ratio': 1.0}]",数据集偏差,{},[]
1465,1465,dataset size,数据集大小,0.9,10,"[{'word': '数据集大小', 'ratio': 0.9}, {'word': '数据集规模', 'ratio': 0.1}]",数据集大小,{},[]
1466,1466,datasheet,数据表,0.6,10,"[{'word': '数据表', 'ratio': 0.6}, {'word': '数据说明书', 'ratio': 0.3}, {'word': '数据说明', 'ratio': 0.1}]",数据表,{},[]
1467,1467,datum bias,数据偏倚,0.0,10,"[{'word': '数据偏差', 'ratio': 0.8}, {'word': '数据偏见', 'ratio': 0.2}]",数据偏差,{},[]
1468,1468,datum clustering,数据聚类,1.0,10,"[{'word': '数据聚类', 'ratio': 1.0}]",数据聚类,{},[]
1469,1469,datum contamination,数据污染,1.0,9,"[{'word': '数据污染', 'ratio': 1.0}]",数据污染,{},[]
1470,1470,datum dependency,数据依赖性,0.8888888888888888,9,"[{'word': '数据依赖性', 'ratio': 0.8888888888888888}, {'word': '数据依赖', 'ratio': 0.1111111111111111}]",数据依赖性,{},[]
1471,1471,datum fidelity,数据保真度,1.0,9,"[{'word': '数据保真度', 'ratio': 1.0}]",数据保真度,{},[]
1472,1472,datum filtering,数据筛选,0.0,9,"[{'word': '数据过滤', 'ratio': 1.0}]",数据过滤,{},[]
1473,1473,datum generative process,数据生成过程,1.0,9,"[{'word': '数据生成过程', 'ratio': 1.0}]",数据生成过程,{},[]
1474,1474,datum matrix,数据矩阵,1.0,10,"[{'word': '数据矩阵', 'ratio': 1.0}]",数据矩阵,{},[]
1475,1475,datum mining algorithm,数据挖掘算法,1.0,10,"[{'word': '数据挖掘算法', 'ratio': 1.0}]",数据挖掘算法,{},[]
1476,1476,datum parallelism,数据并行,0.3,10,"[{'word': '数据并行性', 'ratio': 0.7}, {'word': '数据并行', 'ratio': 0.3}]",数据并行性,{},[]
1477,1477,datum perturbation,数据扰动,1.0,10,"[{'word': '数据扰动', 'ratio': 1.0}]",数据扰动,{},[]
1478,1478,davinci,davinci模型,0.0,10,"[{'word': '达芬奇', 'ratio': 0.9}, {'word': 'davinci', 'ratio': 0.1}]",达芬奇,{},[]
1479,1479,decay parameter,衰减参数,1.0,8,"[{'word': '衰减参数', 'ratio': 1.0}]",衰减参数,{},[]
1480,1480,decentralized algorithm,分散算法,0.0,8,"[{'word': '去中心化算法', 'ratio': 1.0}]",去中心化算法,{},[]
1481,1481,decision boundary,决策边界,1.0,8,"[{'word': '决策边界', 'ratio': 1.0}]",决策边界,{},[]
1482,1482,decision function,决策函数,1.0,8,"[{'word': '决策函数', 'ratio': 1.0}]",决策函数,{},[]
1483,1483,decision node,决策节点,1.0,8,"[{'word': '决策节点', 'ratio': 1.0}]",决策节点,{},[]
1484,1484,decision policy,决策策略,0.8571428571428571,7,"[{'word': '决策策略', 'ratio': 0.8571428571428571}, {'word': '决策政策', 'ratio': 0.14285714285714285}]",决策策略,{},[]
1485,1485,decision problem,决策问题,1.0,7,"[{'word': '决策问题', 'ratio': 1.0}]",决策问题,{},[]
1486,1486,decision rule,决策规则,1.0,7,"[{'word': '决策规则', 'ratio': 1.0}]",决策规则,{},[]
1487,1487,decision space,决策空间,1.0,7,"[{'word': '决策空间', 'ratio': 1.0}]",决策空间,{},[]
1488,1488,decision stumps,决策树桩,0.5714285714285714,7,"[{'word': '决策树桩', 'ratio': 0.5714285714285714}, {'word': '决策桩', 'ratio': 0.42857142857142855}]",决策树桩,{},[]
1489,1489,decision theory,决策理论,1.0,9,"[{'word': '决策理论', 'ratio': 1.0}]",决策理论,{},[]
1490,1490,decision tree,决策树,1.0,9,"[{'word': '决策树', 'ratio': 1.0}]",决策树,{},[]
1491,1491,decision variable,决策变量,1.0,9,"[{'word': '决策变量', 'ratio': 1.0}]",决策变量,{},[]
1492,1492,decoder hidden state,解码器隐藏状态,0.7777777777777778,9,"[{'word': '解码器隐藏状态', 'ratio': 0.7777777777777778}, {'word': '解码器隐状态', 'ratio': 0.2222222222222222}]",解码器隐藏状态,{},[]
1493,1493,decoder layer,解码器层,0.8333333333333334,6,"[{'word': '解码器层', 'ratio': 0.8333333333333334}, {'word': '解码层', 'ratio': 0.16666666666666666}]",解码器层,{},[]
1494,1494,decoder network,解码器网络,0.8333333333333334,6,"[{'word': '解码器网络', 'ratio': 0.8333333333333334}, {'word': '解码网络', 'ratio': 0.16666666666666666}]",解码器网络,{},[]
1495,1495,decoder output,解码器输出,0.8333333333333334,6,"[{'word': '解码器输出', 'ratio': 0.8333333333333334}, {'word': '解码输出', 'ratio': 0.16666666666666666}]",解码器输出,{},[]
1496,1496,decoder state,解码器状态,0.8333333333333334,6,"[{'word': '解码器状态', 'ratio': 0.8333333333333334}, {'word': '解码状态', 'ratio': 0.16666666666666666}]",解码器状态,{},[]
1497,1497,decoder-only transformer,解码器专用变压器,0.0,6,"[{'word': '仅解码器变换器', 'ratio': 0.6666666666666666}, {'word': '仅解码器的transformer', 'ratio': 0.3333333333333333}]",仅解码器变换器,{},[]
1498,1498,decoding algorithm,解码算法,1.0,7,"[{'word': '解码算法', 'ratio': 1.0}]",解码算法,{},[]
1499,1499,decoding problem,解码问题,1.0,7,"[{'word': '解码问题', 'ratio': 1.0}]",解码问题,{},[]
1500,1500,decoding step,解码步骤,1.0,7,"[{'word': '解码步骤', 'ratio': 1.0}]",解码步骤,{},[]
1501,1501,decoding strategy,解码策略,1.0,7,"[{'word': '解码策略', 'ratio': 1.0}]",解码策略,{},[]
1502,1502,decomposition,分解,1.0,9,"[{'word': '分解', 'ratio': 1.0}]",分解,{},[]
1503,1503,decomposition method,分解方法,1.0,9,"[{'word': '分解方法', 'ratio': 1.0}]",分解方法,{},[]
1504,1504,deconvolution,反卷积,0.8888888888888888,9,"[{'word': '反卷积', 'ratio': 0.8888888888888888}, {'word': '去卷积', 'ratio': 0.1111111111111111}]",反卷积,{},[]
1505,1505,deconvolution layer,卷积转置层,0.0,9,"[{'word': '反卷积层', 'ratio': 0.8888888888888888}, {'word': '去卷积层', 'ratio': 0.1111111111111111}]",反卷积层,{},[]
1506,1506,deconvolutional layer,解卷积层,0.0,9,"[{'word': '反卷积层', 'ratio': 0.8888888888888888}, {'word': '去卷积层', 'ratio': 0.1111111111111111}]",反卷积层,{},[]
1507,1507,deep Q-learning,深度Q学习,0.7777777777777778,9,"[{'word': '深度Q学习', 'ratio': 0.7777777777777778}, {'word': '深度 Q 学习', 'ratio': 0.2222222222222222}]",深度Q学习,{},[]
1508,1508,deep Q-network,深度 Q 网络,0.1111111111111111,9,"[{'word': '深度Q网络', 'ratio': 0.8888888888888888}, {'word': '深度 Q 网络', 'ratio': 0.1111111111111111}]",深度Q网络,{},[]
1509,1509,deep architecture,深度架构,0.7777777777777778,9,"[{'word': '深度架构', 'ratio': 0.7777777777777778}, {'word': '深层架构', 'ratio': 0.2222222222222222}]",深度架构,{},[]
1510,1510,deep convolutional network,深度卷积网络,1.0,9,"[{'word': '深度卷积网络', 'ratio': 1.0}]",深度卷积网络,{},[]
1511,1511,deep convolutional neural network,深度卷积神经网络,1.0,9,"[{'word': '深度卷积神经网络', 'ratio': 1.0}]",深度卷积神经网络,{},[]
1512,1512,deep feature,深度特征,0.7777777777777778,9,"[{'word': '深度特征', 'ratio': 0.7777777777777778}, {'word': '深层特征', 'ratio': 0.2222222222222222}]",深度特征,{},[]
1513,1513,deep generative model,深度生成模型,1.0,9,"[{'word': '深度生成模型', 'ratio': 1.0}]",深度生成模型,{},[]
1514,1514,deep layer,深层,1.0,9,"[{'word': '深层', 'ratio': 1.0}]",深层,{},[]
1515,1515,deep learning architecture,深度学习架构,1.0,9,"[{'word': '深度学习架构', 'ratio': 1.0}]",深度学习架构,{},[]
1516,1516,deep learning framework,深度学习框架,0.8888888888888888,9,"[{'word': '深度学习框架', 'ratio': 0.8888888888888888}, {'word': '深度学习框架 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.1111111111111111}]",深度学习框架,{},[]
1517,1517,deep learning model,深度学习模型,0.9,10,"[{'word': '深度学习模型', 'ratio': 0.9}, {'word': '深度模型', 'ratio': 0.1}]",深度学习模型,{},[]
1518,1518,deep learning system,深度学习系统,1.0,10,"[{'word': '深度学习系统', 'ratio': 1.0}]",深度学习系统,{},[]
1519,1519,deep model,深度模型,1.0,10,"[{'word': '深度模型', 'ratio': 1.0}]",深度模型,{},[]
1520,1520,deep net,深度网络,1.0,10,"[{'word': '深度网络', 'ratio': 1.0}]",深度网络,{},[]
1521,1521,deep network,深度网络,0.8,10,"[{'word': '深度网络', 'ratio': 0.8}, {'word': '深度神经网络', 'ratio': 0.2}]",深度网络,{},[]
1522,1522,deep network architecture,深度网络架构,1.0,6,"[{'word': '深度网络架构', 'ratio': 1.0}]",深度网络架构,{},[]
1523,1523,deep neural model,深度神经网络模型,0.0,6,"[{'word': '深度神经模型', 'ratio': 0.8333333333333334}, {'word': '深度网络架构', 'ratio': 0.16666666666666666}]",深度神经模型,{},[]
1524,1524,deep neural net,深度神经网络,0.8333333333333334,6,"[{'word': '深度神经网络', 'ratio': 0.8333333333333334}, {'word': '深度网络架构', 'ratio': 0.16666666666666666}]",深度神经网络,{},[]
1525,1525,deep neural network,深度神经网络,1.0,6,"[{'word': '深度神经网络', 'ratio': 1.0}]",深度神经网络,{},[]
1526,1526,deep reinforcement learning,深度强化学习,1.0,6,"[{'word': '深度强化学习', 'ratio': 1.0}]",深度强化学习,{},[]
1527,1527,deep supervision,深度监督,1.0,8,"[{'word': '深度监督', 'ratio': 1.0}]",深度监督,{},[]
1528,1528,deeply-supervised net,深度监督网络,1.0,8,"[{'word': '深度监督网络', 'ratio': 1.0}]",深度监督网络,{},[]
1529,1529,default logic,缺省逻辑,0.0,8,"[{'word': '默认逻辑', 'ratio': 1.0}]",默认逻辑,{},[]
1530,1530,deformable template,可变形模板,1.0,8,"[{'word': '可变形模板', 'ratio': 1.0}]",可变形模板,{},[]
1531,1531,deformation field,形变场,0.0,8,"[{'word': '变形场', 'ratio': 1.0}]",变形场,{},[]
1532,1532,degree distribution,度分布,0.7777777777777778,9,"[{'word': '度分布', 'ratio': 0.7777777777777778}, {'word': '度数分布', 'ratio': 0.1111111111111111}, {'word': '特征映射函数', 'ratio': 0.1111111111111111}]",度分布,{},[]
1533,1533,delayed reward,延迟奖励,0.8888888888888888,9,"[{'word': '延迟奖励', 'ratio': 0.8888888888888888}, {'word': '特征匹配', 'ratio': 0.1111111111111111}]",延迟奖励,{},[]
1534,1534,delexicalization,去词汇化,0.7777777777777778,9,"[{'word': '去词汇化', 'ratio': 0.7777777777777778}, {'word': '非词汇化', 'ratio': 0.1111111111111111}, {'word': '特征矩阵', 'ratio': 0.1111111111111111}]",去词汇化,{},[]
1535,1535,delta kernel,狄拉克核,0.0,9,"[{'word': 'δ核', 'ratio': 0.4444444444444444}, {'word': '德尔塔核', 'ratio': 0.2222222222222222}, {'word': 'δ 核', 'ratio': 0.1111111111111111}, {'word': 'δ 核函数', 'ratio': 0.1111111111111111}, {'word': '特征模型', 'ratio': 0.1111111111111111}]","""δ核""","1. Rank: ""δ核"", ""δ 核"", ""德尔塔核"", ""δ 核函数"", ""特征模型""

2. Explanation: The term ""δ核"" is the best fit for ""delta kernel"" in the AI domain due to its semantic accuracy and contextual relevance. The use of the Greek letter ""δ"" (delta) is standard in mathematical and computational contexts, particularly in kernel methods and image processing, where it represents the identity kernel. This term is concise and widely recognized among professionals in the field. 

The second candidate, ""δ 核"", is also a strong option, as it retains the Greek letter and is similar in meaning, but the space between ""δ"" and ""核"" is less common in technical terminology. 

""德尔塔核"" is a transliteration of ""delta"" into Chinese, which is less preferred in technical contexts where the Greek letter is more commonly used. 

""δ 核函数"" introduces the term ""函数"" (function), which may imply a broader context than intended, as ""kernel"" in this case refers specifically to the identity kernel rather than a general function. 

Finally, ""特征模型"" translates to ""feature model,"" which is unrelated to the concept of a delta kernel and thus ranks the lowest in terms of semantic accuracy and contextual fit.","['delta core', 'delta core', 'delta core', 'δ kernel function', 'feature model']"
1536,1536,dendrogram,树状图,0.6666666666666666,9,"[{'word': '树状图', 'ratio': 0.6666666666666666}, {'word': '树状图 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.1111111111111111}, {'word': '聚类树', 'ratio': 0.1111111111111111}, {'word': '特征归一化', 'ratio': 0.1111111111111111}]",树状图,{},[]
1537,1537,denoiser,去噪器,1.0,9,"[{'word': '去噪器', 'ratio': 1.0}]",去噪器,{},[]
1538,1538,denoising diffusion probabilistic model,去噪扩散概率模型,1.0,9,"[{'word': '去噪扩散概率模型', 'ratio': 1.0}]",去噪扩散概率模型,{},[]
1539,1539,denoising network,去噪网络,1.0,9,"[{'word': '去噪网络', 'ratio': 1.0}]",去噪网络,{},[]
1540,1540,denoising objective,去噪目标,1.0,9,"[{'word': '去噪目标', 'ratio': 1.0}]",去噪目标,{},[]
1541,1541,denoising process,去噪过程,1.0,9,"[{'word': '去噪过程', 'ratio': 1.0}]",去噪过程,{},[]
1542,1542,denoising score matching,去噪分数匹配,0.625,8,"[{'word': '去噪分数匹配', 'ratio': 0.625}, {'word': '去噪得分匹配', 'ratio': 0.375}]",去噪分数匹配,{},[]
1543,1543,denoising score matching loss,去噪分数匹配损失,0.625,8,"[{'word': '去噪分数匹配损失', 'ratio': 0.625}, {'word': '去噪得分匹配损失', 'ratio': 0.375}]",去噪分数匹配损失,{},[]
1544,1544,denotation,指称,0.75,8,"[{'word': '指称', 'ratio': 0.75}, {'word': '表示', 'ratio': 0.25}]",指称,{},[]
1545,1545,dense,密集,0.75,8,"[{'word': '密集', 'ratio': 0.75}, {'word': '稠密', 'ratio': 0.125}, {'word': '密集的', 'ratio': 0.125}]",密集,{},[]
1546,1546,dense attention,密集注意力,0.875,8,"[{'word': '密集注意力', 'ratio': 0.875}, {'word': '稠密注意力', 'ratio': 0.125}]",密集注意力,{},[]
1547,1547,dense depth map,密集深度图,0.625,8,"[{'word': '密集深度图', 'ratio': 0.625}, {'word': '稠密深度图', 'ratio': 0.375}]",密集深度图,{},[]
1548,1548,dense feature,密集特征,0.625,8,"[{'word': '密集特征', 'ratio': 0.625}, {'word': '稠密特征', 'ratio': 0.375}]",密集特征,{},[]
1549,1549,dense layer,密集层,0.25,8,"[{'word': '全连接层', 'ratio': 0.625}, {'word': '密集层', 'ratio': 0.25}, {'word': '稠密层', 'ratio': 0.125}]",全连接层,{},[]
1550,1550,dense matrix,稠密矩阵,0.375,8,"[{'word': '密集矩阵', 'ratio': 0.5}, {'word': '稠密矩阵', 'ratio': 0.375}, {'word': '密集层', 'ratio': 0.125}]",密集矩阵,{},[]
1551,1551,dense network,稠密网络,0.375,8,"[{'word': '密集网络', 'ratio': 0.5}, {'word': '稠密网络', 'ratio': 0.375}, {'word': '密集层', 'ratio': 0.125}]",密集网络,{},[]
1552,1552,dense prediction,密集预测,1.0,8,"[{'word': '密集预测', 'ratio': 1.0}]",密集预测,{},[]
1553,1553,dense representation,密集表示,1.0,8,"[{'word': '密集表示', 'ratio': 1.0}]",密集表示,{},[]
1554,1554,dense vector,稠密向量,0.125,8,"[{'word': '密集向量', 'ratio': 0.875}, {'word': '稠密向量', 'ratio': 0.125}]",密集向量,{},[]
1555,1555,density estimate,密度估计,1.0,8,"[{'word': '密度估计', 'ratio': 1.0}]",密度估计,{},[]
1556,1556,density estimation,密度估计,0.875,8,"[{'word': '密度估计', 'ratio': 0.875}, {'word': '密度估计法', 'ratio': 0.125}]",密度估计,{},[]
1557,1557,density estimator,密度估计器,1.0,9,"[{'word': '密度估计器', 'ratio': 1.0}]",密度估计器,{},[]
1558,1558,density field,密度场,1.0,9,"[{'word': '密度场', 'ratio': 1.0}]",密度场,{},[]
1559,1559,density function,密度函数,1.0,9,"[{'word': '密度函数', 'ratio': 1.0}]",密度函数,{},[]
1560,1560,density gradient,密度梯度,1.0,9,"[{'word': '密度梯度', 'ratio': 1.0}]",密度梯度,{},[]
1561,1561,density ratio,密度比率,0.0,9,"[{'word': '密度比', 'ratio': 1.0}]",密度比,{},[]
1562,1562,dependency,依赖关系,0.25,16,"[{'word': '依赖', 'ratio': 0.3125}, {'word': '依赖关系', 'ratio': 0.25}, {'word': '依存关系', 'ratio': 0.1875}, {'word': '依赖性', 'ratio': 0.125}, {'word': '子图', 'ratio': 0.0625}, {'word': '次模性', 'ratio': 0.0625}]",依存关系,"1. Rank: 依存关系, 依赖关系, 依赖, 依赖性, 子图, 次模性

2. Explanation: The term ""依存关系"" (dependencies) is the best fit for the AI domain-specific usage in the context provided. In computational linguistics and AI, ""依存关系"" specifically refers to the relationships between words in a sentence, which aligns perfectly with the context of sentence compression and the classification of word pairs as mentioned in the examples. 

""依赖关系"" (dependencies) is also a strong candidate, but it is more general and can imply a broader sense of dependency that may not be as precise in the context of linguistic structures. ""依赖"" (rely) and ""依赖性"" (dependency) are less suitable as they do not convey the specific relational aspect that ""依存关系"" captures. 

The terms ""子图"" (subplot) and ""次模性"" (submodularity) are irrelevant in this context, as they pertain to different concepts not related to dependencies in language processing. Thus, ""依存关系"" is the most semantically accurate and contextually appropriate choice for the AI terminology in question.","['rely', 'Dependencies', 'Dependencies', 'dependency', 'subplot', 'submodularity']"
1563,1563,dependency arc,依赖弧,0.5,8,"[{'word': '依赖弧', 'ratio': 0.5}, {'word': '依存弧', 'ratio': 0.375}, {'word': '支持向量机', 'ratio': 0.125}]",依赖弧,{},[]
1564,1564,dependency feature,依存特征,0.375,8,"[{'word': '依赖特征', 'ratio': 0.5}, {'word': '依存特征', 'ratio': 0.375}, {'word': 'Swin-S', 'ratio': 0.125}]",依赖特征,{},[]
1565,1565,dependency graph,依赖图,0.5,6,"[{'word': '依赖图', 'ratio': 0.5}, {'word': '依存图', 'ratio': 0.5}]",依赖图,{},[]
1566,1566,dependency label,依存标签,0.5,6,"[{'word': '依赖标签', 'ratio': 0.5}, {'word': '依存标签', 'ratio': 0.5}]",依赖标签,{},[]
1567,1567,dependency model,依存模型,0.5,6,"[{'word': '依赖模型', 'ratio': 0.5}, {'word': '依存模型', 'ratio': 0.5}]",依赖模型,{},[]
1568,1568,dependency parse,依存句法分析,0.0,6,"[{'word': '依存分析', 'ratio': 0.5}, {'word': '依赖解析', 'ratio': 0.3333333333333333}, {'word': '依赖分析', 'ratio': 0.16666666666666666}]",依存分析,{},[]
1569,1569,dependency parse tree,依存分析树,0.5,6,"[{'word': '依存分析树', 'ratio': 0.5}, {'word': '依赖解析树', 'ratio': 0.3333333333333333}, {'word': '依赖分析', 'ratio': 0.16666666666666666}]",依存分析树,{},[]
1570,1570,dependency parser,依存解析器,0.0,10,"[{'word': '依存句法分析器', 'ratio': 0.6}, {'word': '依赖解析器', 'ratio': 0.4}]",依存句法分析器,{},[]
1571,1571,dependency parsing model,依存句法分析模型,0.6,10,"[{'word': '依存句法分析模型', 'ratio': 0.6}, {'word': '依赖解析模型', 'ratio': 0.4}]",依存句法分析模型,{},[]
1572,1572,dependency path,依存路径,0.6,10,"[{'word': '依存路径', 'ratio': 0.6}, {'word': '依赖路径', 'ratio': 0.4}]",依存路径,{},[]
1573,1573,dependency relation,依赖关系,0.4,10,"[{'word': '依存关系', 'ratio': 0.6}, {'word': '依赖关系', 'ratio': 0.4}]",依存关系,{},[]
1574,1574,dependency representation,依存关系表示,0.0,10,"[{'word': '依存表示', 'ratio': 0.6}, {'word': '依赖表示', 'ratio': 0.4}]",依存表示,{},[]
1575,1575,dependency structure,依存结构,0.375,8,"[{'word': '依赖结构', 'ratio': 0.625}, {'word': '依存结构', 'ratio': 0.375}]",依赖结构,{},[]
1576,1576,dependency tree,依存树,0.25,8,"[{'word': '依赖树', 'ratio': 0.625}, {'word': '依存树', 'ratio': 0.25}, {'word': '依存结构', 'ratio': 0.125}]",依赖树,{},[]
1577,1577,dependency treebank,依存树库,0.375,8,"[{'word': '依赖树库', 'ratio': 0.625}, {'word': '依存树库', 'ratio': 0.375}]",依赖树库,{},[]
1578,1578,dependent variable,因变量,1.0,8,"[{'word': '因变量', 'ratio': 1.0}]",因变量,{},[]
1579,1579,depth estimation,深度估计,1.0,8,"[{'word': '深度估计', 'ratio': 1.0}]",深度估计,{},[]
1580,1580,depth estimator,深度估计器,1.0,10,"[{'word': '深度估计器', 'ratio': 1.0}]",深度估计器,{},[]
1581,1581,depth image,深度图像,1.0,10,"[{'word': '深度图像', 'ratio': 1.0}]",深度图像,{},[]
1582,1582,depth map,深度图,1.0,10,"[{'word': '深度图', 'ratio': 1.0}]",深度图,{},[]
1583,1583,depth prediction,深度预测,1.0,10,"[{'word': '深度预测', 'ratio': 1.0}]",深度预测,{},[]
1584,1584,depth-first search,深度优先搜索,1.0,10,"[{'word': '深度优先搜索', 'ratio': 1.0}]",深度优先搜索,{},[]
1585,1585,description logic,描述逻辑,1.0,10,"[{'word': '描述逻辑', 'ratio': 1.0}]",描述逻辑,{},[]
1586,1586,descriptor,描述子,0.3,10,"[{'word': '描述符', 'ratio': 0.7}, {'word': '描述子', 'ratio': 0.3}]",描述符,{},[]
1587,1587,design matrix,设计矩阵,1.0,10,"[{'word': '设计矩阵', 'ratio': 1.0}]",设计矩阵,{},[]
1588,1588,design space,设计空间,1.0,10,"[{'word': '设计空间', 'ratio': 1.0}]",设计空间,{},[]
1589,1589,det,行列式,1.0,10,"[{'word': '行列式', 'ratio': 1.0}]",行列式,{},[]
1590,1590,detection,检测,0.8571428571428571,14,"[{'word': '检测', 'ratio': 0.8571428571428571}, {'word': '识别', 'ratio': 0.14285714285714285}]",检测,{},[]
1591,1591,detection algorithm,检测算法,1.0,7,"[{'word': '检测算法', 'ratio': 1.0}]",检测算法,{},[]
1592,1592,detection model,检测模型,1.0,7,"[{'word': '检测模型', 'ratio': 1.0}]",检测模型,{},[]
1593,1593,detection score,检测分数,0.4285714285714285,7,"[{'word': '检测得分', 'ratio': 0.5714285714285714}, {'word': '检测分数', 'ratio': 0.42857142857142855}]",检测得分,{},[]
1594,1594,detection window,检测窗口,1.0,10,"[{'word': '检测窗口', 'ratio': 1.0}]",检测窗口,{},[]
1595,1595,detector,检测器,1.0,10,"[{'word': '检测器', 'ratio': 1.0}]",检测器,{},[]
1596,1596,deterministic algorithm,确定性算法,1.0,10,"[{'word': '确定性算法', 'ratio': 1.0}]",确定性算法,{},[]
1597,1597,deterministic annealing,确定性退火,0.9,10,"[{'word': '确定性退火', 'ratio': 0.9}, {'word': '确定性退火v', 'ratio': 0.1}]",确定性退火,{},[]
1598,1598,deterministic automaton,确定性自动机,1.0,10,"[{'word': '确定性自动机', 'ratio': 1.0}]",确定性自动机,{},[]
1599,1599,deterministic baseline,确定性基线,0.1428571428571428,7,"[{'word': '确定性基准', 'ratio': 0.8571428571428571}, {'word': '确定性基线', 'ratio': 0.14285714285714285}]",确定性基准,{},[]
1600,1600,deterministic finite automaton,确定有限自动机,0.4285714285714285,7,"[{'word': '确定性有限自动机', 'ratio': 0.5714285714285714}, {'word': '确定有限自动机', 'ratio': 0.42857142857142855}]",确定性有限自动机,{},[]
1601,1601,deterministic policy,确定性策略,1.0,7,"[{'word': '确定性策略', 'ratio': 1.0}]",确定性策略,{},[]
1602,1602,deterministic rule,确定性规则,1.0,7,"[{'word': '确定性规则', 'ratio': 1.0}]",确定性规则,{},[]
1603,1603,dev set,开发集,0.8571428571428571,7,"[{'word': '开发集', 'ratio': 0.8571428571428571}, {'word': '验证集', 'ratio': 0.14285714285714285}]",开发集,{},[]
1604,1604,development set,开发集,0.75,4,"[{'word': '开发集', 'ratio': 0.75}, {'word': '验证集', 'ratio': 0.25}]",开发集,{},[]
1605,1605,diagonal matrix,对角矩阵,1.0,4,"[{'word': '对角矩阵', 'ratio': 1.0}]",对角矩阵,{},[]
1606,1606,dialog system,对话系统,1.0,4,"[{'word': '对话系统', 'ratio': 1.0}]",对话系统,{},[]
1607,1607,dialogue act,对话行为,1.0,4,"[{'word': '对话行为', 'ratio': 1.0}]",对话行为,{},[]
1608,1608,dialogue context,对话上下文,1.0,4,"[{'word': '对话上下文', 'ratio': 1.0}]",对话上下文,{},[]
1609,1609,dialogue generation,对话生成,1.0,9,"[{'word': '对话生成', 'ratio': 1.0}]",对话生成,{},[]
1610,1610,dialogue history,对话历史记录,0.0,9,"[{'word': '对话历史', 'ratio': 1.0}]",对话历史,{},[]
1611,1611,dialogue management,对话管理,1.0,9,"[{'word': '对话管理', 'ratio': 1.0}]",对话管理,{},[]
1612,1612,dialogue state,对话状态,1.0,9,"[{'word': '对话状态', 'ratio': 1.0}]",对话状态,{},[]
1613,1613,dialogue state tracker,对话状态跟踪器,0.7777777777777778,9,"[{'word': '对话状态跟踪器', 'ratio': 0.7777777777777778}, {'word': '对话状态追踪器', 'ratio': 0.2222222222222222}]",对话状态跟踪器,{},[]
1614,1614,dialogue system,对话系统,1.0,9,"[{'word': '对话系统', 'ratio': 1.0}]",对话系统,{},[]
1615,1615,dialogue turn,对话轮次,1.0,9,"[{'word': '对话轮次', 'ratio': 1.0}]",对话轮次,{},[]
1616,1616,dictionary learning,词典学习,0.0,9,"[{'word': '字典学习', 'ratio': 1.0}]",字典学习,{},[]
1617,1617,dictionary matrix,字典矩阵,1.0,9,"[{'word': '字典矩阵', 'ratio': 1.0}]",字典矩阵,{},[]
1618,1618,diffeomorphism,微分同胚,1.0,9,"[{'word': '微分同胚', 'ratio': 1.0}]",微分同胚,{},[]
1619,1619,differentiable function,可微函数,1.0,6,"[{'word': '可微函数', 'ratio': 1.0}]",可微函数,{},[]
1620,1620,differentiable renderer,可微渲染器,1.0,6,"[{'word': '可微渲染器', 'ratio': 1.0}]",可微渲染器,{},[]
1621,1621,differentiable rendering,可微渲染,1.0,6,"[{'word': '可微渲染', 'ratio': 1.0}]",可微渲染,{},[]
1622,1622,differentiable rendering function,可微渲染函数,1.0,6,"[{'word': '可微渲染函数', 'ratio': 1.0}]",可微渲染函数,{},[]
1623,1623,differential entropy,微分熵,1.0,6,"[{'word': '微分熵', 'ratio': 1.0}]",微分熵,{},[]
1624,1624,differential privacy,差分隐私,1.0,10,"[{'word': '差分隐私', 'ratio': 1.0}]",差分隐私,{},[]
1625,1625,diffusion matrix,扩散矩阵,1.0,10,"[{'word': '扩散矩阵', 'ratio': 1.0}]",扩散矩阵,{},[]
1626,1626,diffusion model,扩散模型,1.0,10,"[{'word': '扩散模型', 'ratio': 1.0}]",扩散模型,{},[]
1627,1627,diffusion tensor,扩散张量,1.0,10,"[{'word': '扩散张量', 'ratio': 1.0}]",扩散张量,{},[]
1628,1628,digamma function,双伽玛函数,0.1,10,"[{'word': '双伽马函数', 'ratio': 0.2}, {'word': '伽马函数的对数导数', 'ratio': 0.2}, {'word': '双伽玛函数', 'ratio': 0.1}, {'word': '双伽马函数 如有其他需要帮助的地方，请告诉我！', 'ratio': 0.1}, {'word': '伽玛函数的对数导数', 'ratio': 0.1}, {'word': '数字伽马函数', 'ratio': 0.1}, {'word': '二伽马函数', 'ratio': 0.1}, {'word': '迪伽玛函数', 'ratio': 0.1}]","""迪伽玛函数""","1. Rank: ""迪伽玛函数"", ""双伽马函数"", ""双伽玛函数"", ""伽马函数的对数导数"", ""数字伽马函数"", ""二伽马函数"", ""双伽马函数 如有其他需要帮助的地方，请告诉我！"", ""双伽马函数""

2. Explanation: The term ""迪伽玛函数"" (di gamma function) is the most accurate translation for ""digamma function"" in the context of AI and mathematics. This term is widely recognized in mathematical literature and specifically refers to the first derivative of the logarithm of the gamma function, which aligns perfectly with the provided context. The back translation ""di gamma function"" accurately reflects the original English term, maintaining semantic integrity.

The other candidates, such as ""双伽马函数"" (double gamma function) and ""双伽玛函数"" (double gamma function), are misleading as they suggest a different mathematical concept that does not correspond to the digamma function. Similarly, ""伽马函数的对数导数"" (logarithmic derivative of the gamma function) is technically correct but is less commonly used and may not be immediately recognized as referring to the digamma function. The terms ""数字伽马函数"" (digital gamma function) and ""二伽马函数"" (two gamma functions) are incorrect and do not relate to the digamma function at all. 

Thus, ""迪伽玛函数"" stands out as the best fit due to its established usage in the mathematical community and its precise alignment with the definition of the digamma function.","['double gamma function', 'Logarithmic derivative of the gamma function', 'double gamma function', 'Double Gamma Function If you need any other help, please let me know!', 'Logarithmic derivative of the gamma function', 'digital gamma function', 'Two gamma functions', 'di gamma function']"
1629,1629,digit classification,数字分类,0.8888888888888888,9,"[{'word': '数字分类', 'ratio': 0.8888888888888888}, {'word': '凸性', 'ratio': 0.1111111111111111}]",数字分类,{},[]
1630,1630,dilated convolution,扩张卷积,0.1111111111111111,9,"[{'word': '膨胀卷积', 'ratio': 0.7777777777777778}, {'word': '凸性', 'ratio': 0.1111111111111111}, {'word': '扩张卷积', 'ratio': 0.1111111111111111}]",膨胀卷积,{},[]
1631,1631,dilation,膨胀,0.7777777777777778,9,"[{'word': '膨胀', 'ratio': 0.7777777777777778}, {'word': '凸性', 'ratio': 0.1111111111111111}, {'word': '扩张', 'ratio': 0.1111111111111111}]",膨胀,{},[]
1632,1632,dim,维数,0.4444444444444444,9,"[{'word': '维数', 'ratio': 0.4444444444444444}, {'word': '维度', 'ratio': 0.3333333333333333}, {'word': '凸性', 'ratio': 0.1111111111111111}, {'word': '暗淡', 'ratio': 0.1111111111111111}]",维数,"1. Rank: 维数, 维度, 凸性, 暗淡

2. Explanation: The term ""维数"" (Dimension) is the best fit for the AI domain-specific usage because it accurately conveys the mathematical concept of ""dim"" as it relates to the dimensionality of spaces, which is crucial in contexts like camera configurations and mathematical definitions. ""维度"" (Dimensions) is also a suitable candidate, but ""维数"" is more commonly used in mathematical contexts to refer specifically to the number of dimensions in a space. The term ""凸性"" (Convexity) is unrelated to the context provided, and ""暗淡"" (dim) does not fit the mathematical context at all, as it translates to ""dim"" in the sense of brightness rather than dimensionality. Therefore, ""维数"" is the most semantically accurate and contextually appropriate choice.","['Dimension', 'Dimensions', 'Convexity', 'dim']"
1633,1633,dimension,维度,0.6666666666666666,9,"[{'word': '维度', 'ratio': 0.6666666666666666}, {'word': '维数', 'ratio': 0.1111111111111111}, {'word': '凸性', 'ratio': 0.1111111111111111}, {'word': '无量纲', 'ratio': 0.1111111111111111}]",维度,{},[]
1634,1634,dimension reduction,降维,0.75,4,"[{'word': '降维', 'ratio': 0.75}, {'word': '维度缩减', 'ratio': 0.25}]",降维,{},[]
1635,1635,dimensional vector,维度向量,0.25,4,"[{'word': '维向量', 'ratio': 0.75}, {'word': '维度向量', 'ratio': 0.25}]",维向量,{},[]
1636,1636,dimensionality,维度,0.75,4,"[{'word': '维度', 'ratio': 0.75}, {'word': '维数', 'ratio': 0.25}]",维度,{},[]
1637,1637,dimensionality reduction,降维,0.25,4,"[{'word': '维度减少', 'ratio': 0.5}, {'word': '降维技术', 'ratio': 0.25}, {'word': '降维', 'ratio': 0.25}]",维度减少,{},[]
1638,1638,directed acyclic graph,有向无环图,1.0,4,"[{'word': '有向无环图', 'ratio': 1.0}]",有向无环图,{},[]
1639,1639,directed edge,有向边,1.0,6,"[{'word': '有向边', 'ratio': 1.0}]",有向边,{},[]
1640,1640,directed graph,有向图,1.0,6,"[{'word': '有向图', 'ratio': 1.0}]",有向图,{},[]
1641,1641,directed graphical model,直接图模型,0.0,6,"[{'word': '有向图模型', 'ratio': 1.0}]",有向图模型,{},[]
1642,1642,directed tree,有向树,1.0,6,"[{'word': '有向树', 'ratio': 1.0}]",有向树,{},[]
1643,1643,disambiguation,消歧义,0.6666666666666666,6,"[{'word': '消歧义', 'ratio': 0.6666666666666666}, {'word': '消歧', 'ratio': 0.3333333333333333}]",消歧义,{},[]
1644,1644,discount factor,折扣因子,0.8888888888888888,9,"[{'word': '折扣因子', 'ratio': 0.8888888888888888}, {'word': '贴现系数', 'ratio': 0.1111111111111111}]",折扣因子,{},[]
1645,1645,discount parameter,折扣参数,1.0,9,"[{'word': '折扣参数', 'ratio': 1.0}]",折扣参数,{},[]
1646,1646,discounted cumulative reward,折扣累计奖励,0.1111111111111111,9,"[{'word': '折扣累积奖励', 'ratio': 0.8888888888888888}, {'word': '折扣累计奖励', 'ratio': 0.1111111111111111}]",折扣累积奖励,{},[]
1647,1647,discounted return,折现回报,0.0,9,"[{'word': '折扣回报', 'ratio': 1.0}]",折扣回报,{},[]
1648,1648,discounted reward,折扣奖励,1.0,9,"[{'word': '折扣奖励', 'ratio': 1.0}]",折扣奖励,{},[]
1649,1649,discounted state distribution,折扣状态分布,0.875,8,"[{'word': '折扣状态分布', 'ratio': 0.875}, {'word': '折扣状态分配', 'ratio': 0.125}]",折扣状态分布,{},[]
1650,1650,discrete distribution,离散分布,1.0,8,"[{'word': '离散分布', 'ratio': 1.0}]",离散分布,{},[]
1651,1651,discrete graphical model,离散图模型,0.625,8,"[{'word': '离散图模型', 'ratio': 0.625}, {'word': '离散图形模型', 'ratio': 0.375}]",离散图模型,{},[]
1652,1652,discrete random variable,离散随机变量,1.0,8,"[{'word': '离散随机变量', 'ratio': 1.0}]",离散随机变量,{},[]
1653,1653,discriminant analysis,判别分析,1.0,8,"[{'word': '判别分析', 'ratio': 1.0}]",判别分析,{},[]
1654,1654,discriminant function,判别函数,1.0,10,"[{'word': '判别函数', 'ratio': 1.0}]",判别函数,{},[]
1655,1655,discriminative,判别,0.0,10,"[{'word': '判别性', 'ratio': 0.8}, {'word': '判别的', 'ratio': 0.1}, {'word': '判别式', 'ratio': 0.1}]",判别性,{},[]
1656,1656,discriminative approach,判别式方法,0.1,10,"[{'word': '判别方法', 'ratio': 0.7}, {'word': '判别性方法', 'ratio': 0.2}, {'word': '判别式方法', 'ratio': 0.1}]",判别方法,{},[]
1657,1657,discriminative feature,判别特征,1.0,10,"[{'word': '判别特征', 'ratio': 1.0}]",判别特征,{},[]
1658,1658,discriminative method,判别方法,1.0,10,"[{'word': '判别方法', 'ratio': 1.0}]",判别方法,{},[]
1659,1659,discriminative model,判别模型,0.8333333333333334,6,"[{'word': '判别模型', 'ratio': 0.8333333333333334}, {'word': '辨别模型', 'ratio': 0.16666666666666666}]",判别模型,{},[]
1660,1660,discriminative training,判别式训练,0.3333333333333333,6,"[{'word': '判别训练', 'ratio': 0.5}, {'word': '判别式训练', 'ratio': 0.3333333333333333}, {'word': '辨别训练', 'ratio': 0.16666666666666666}]",判别训练,{},[]
1661,1661,discriminator network,判别器网络,0.5,6,"[{'word': '判别网络', 'ratio': 0.5}, {'word': '判别器网络', 'ratio': 0.5}]",判别网络,{},[]
1662,1662,disentangle,解缠,0.0,6,"[{'word': '解耦', 'ratio': 0.8333333333333334}, {'word': '解开', 'ratio': 0.16666666666666666}]",解耦,{},[]
1663,1663,disentangled representation,解缠表示,0.0,6,"[{'word': '解耦表示', 'ratio': 0.8333333333333334}, {'word': '解开表示', 'ratio': 0.16666666666666666}]",解耦表示,{},[]
1664,1664,disparity estimation,视差估计,0.1666666666666666,6,"[{'word': '差异估计', 'ratio': 0.6666666666666666}, {'word': '深度估计', 'ratio': 0.16666666666666666}, {'word': '视差估计', 'ratio': 0.16666666666666666}]",差异估计,{},[]
1665,1665,disparity map,视差图,0.1666666666666666,6,"[{'word': '差异图', 'ratio': 0.6666666666666666}, {'word': '深度图', 'ratio': 0.16666666666666666}, {'word': '视差图', 'ratio': 0.16666666666666666}]",差异图,{},[]
1666,1666,distance function,距离函数,1.0,6,"[{'word': '距离函数', 'ratio': 1.0}]",距离函数,{},[]
1667,1667,distance matrix,距离矩阵,1.0,6,"[{'word': '距离矩阵', 'ratio': 1.0}]",距离矩阵,{},[]
1668,1668,distance measure,距离度量,1.0,6,"[{'word': '距离度量', 'ratio': 1.0}]",距离度量,{},[]
1669,1669,distance metric,距离度量,1.0,6,"[{'word': '距离度量', 'ratio': 1.0}]",距离度量,{},[]
1670,1670,distance transform,距离变换,1.0,6,"[{'word': '距离变换', 'ratio': 1.0}]",距离变换,{},[]
1671,1671,distant supervision,远程监督,1.0,6,"[{'word': '远程监督', 'ratio': 1.0}]",远程监督,{},[]
1672,1672,distillation,蒸馏,0.6666666666666666,6,"[{'word': '蒸馏', 'ratio': 0.6666666666666666}, {'word': '知识蒸馏', 'ratio': 0.3333333333333333}]",蒸馏,{},[]
1673,1673,distributed information retrieval,分布式信息检索,1.0,6,"[{'word': '分布式信息检索', 'ratio': 1.0}]",分布式信息检索,{},[]
1674,1674,distributed learning,分布式学习,1.0,8,"[{'word': '分布式学习', 'ratio': 1.0}]",分布式学习,{},[]
1675,1675,distributed learning system,分布式学习系统,1.0,8,"[{'word': '分布式学习系统', 'ratio': 1.0}]",分布式学习系统,{},[]
1676,1676,distributed representation,分布式表示,1.0,8,"[{'word': '分布式表示', 'ratio': 1.0}]",分布式表示,{},[]
1677,1677,distribution shift,分布偏移,0.375,8,"[{'word': '分布转移', 'ratio': 0.375}, {'word': '分布偏移', 'ratio': 0.375}, {'word': '分布变化', 'ratio': 0.25}]",分布转移,"1. Rank: 分布转移, 分布偏移, 分布变化

2. Explanation: The term ""分布转移"" (distribution shift) is the best fit for the AI domain-specific usage because it accurately captures the concept of a shift in the statistical distribution of data, which is critical in machine learning and AI contexts. The term ""转移"" (shift) conveys the idea of a change in position or state, which aligns well with the notion of distribution shift where the underlying data distribution changes over time or due to different conditions. 

""分布偏移"" (distribution offset) is a close second, but ""偏移"" (offset) can imply a less direct or less significant change compared to ""转移"" (shift), which may not fully encapsulate the abrupt or significant changes often discussed in AI literature. 

""分布变化"" (distribution changes) is the least suitable as it suggests a more general change without the specific implication of a shift, which can lead to ambiguity in the context of AI where precise terminology is crucial for understanding the nuances of model performance and data behavior. Thus, ""分布转移"" is the most semantically accurate and contextually appropriate choice.","['distribution shift', 'Distribution shift', 'Distribution changes']"
1678,1678,distribution vector,分布向量,1.0,8,"[{'word': '分布向量', 'ratio': 1.0}]",分布向量,{},[]
1679,1679,distributional,分布的,0.0,10,"[{'word': '分布式的', 'ratio': 0.5}, {'word': '分布式', 'ratio': 0.4}, {'word': '新闻通讯', 'ratio': 0.1}]",分布式的,{},[]
1680,1680,distributional feature,分布式特征,0.6,10,"[{'word': '分布式特征', 'ratio': 0.6}, {'word': '分布特征', 'ratio': 0.3}, {'word': '下一句预测', 'ratio': 0.1}]",分布式特征,{},[]
1681,1681,distributional hypothesis,分布假设,0.5,10,"[{'word': '分布假设', 'ratio': 0.5}, {'word': '分布假说', 'ratio': 0.3}, {'word': '分布式假设', 'ratio': 0.1}, {'word': '下一个标记预测', 'ratio': 0.1}]",分布假设,{},[]
1682,1682,distributional model,分布模型,0.4,10,"[{'word': '分布式模型', 'ratio': 0.5}, {'word': '分布模型', 'ratio': 0.4}, {'word': '名词修饰语', 'ratio': 0.1}]",分布式模型,{},[]
1683,1683,distributional representation,分布式表示,0.5,10,"[{'word': '分布式表示', 'ratio': 0.5}, {'word': '分布表示', 'ratio': 0.4}, {'word': '无悔算法', 'ratio': 0.1}]",分布式表示,{},[]
1684,1684,distributional semantic,分布语义,0.375,8,"[{'word': '分布式语义', 'ratio': 0.625}, {'word': '分布语义', 'ratio': 0.375}]",分布式语义,{},[]
1685,1685,distributional semantic model,分布语义模型,0.375,8,"[{'word': '分布式语义模型', 'ratio': 0.625}, {'word': '分布语义模型', 'ratio': 0.375}]",分布式语义模型,{},[]
1686,1686,distributional similarity,分布相似性,0.375,8,"[{'word': '分布相似性', 'ratio': 0.375}, {'word': '分布相似度', 'ratio': 0.375}, {'word': '分布式相似性', 'ratio': 0.25}]",分布相似度,"1. Rank: 分布相似度, 分布相似性, 分布式相似性

2. Explanation: The term ""分布相似度"" (distribution similarity) is the best fit for the context of AI and computational linguistics. The use of ""相似度"" (similarity degree) is more common in the field of AI, particularly when discussing metrics that quantify similarity between entities or data points. It conveys a sense of measurement, which aligns well with the statistical nature of distributional similarity in AI applications. 

On the other hand, ""分布相似性"" (distribution similarity) is also a valid translation but is less frequently used in technical contexts compared to ""相似度."" Lastly, ""分布式相似性"" (distributed similarity) introduces the term ""distributed,"" which can imply a different concept related to distributed systems or architectures, making it less suitable in this context. Thus, ""分布相似度"" is the most semantically accurate and contextually appropriate choice.","['distribution similarity', 'distribution similarity', 'distributed similarity']"
1687,1687,distributional word representation,分布式词表示,0.875,8,"[{'word': '分布式词表示', 'ratio': 0.875}, {'word': '分布词表示', 'ratio': 0.125}]",分布式词表示,{},[]
1688,1688,distributionally robust optimization,分布鲁棒优化,0.375,8,"[{'word': '分布式鲁棒优化', 'ratio': 0.375}, {'word': '分布鲁棒优化', 'ratio': 0.375}, {'word': '分布式词表示', 'ratio': 0.125}, {'word': '分布稳健优化', 'ratio': 0.125}]",分布稳健优化,"1. Rank: 分布稳健优化, 分布式鲁棒优化, 分布鲁棒优化, 分布式词表示

2. Explanation: The term ""分布稳健优化"" (Distribution Robust Optimization) is the best fit because it accurately captures the essence of ""distributionally robust optimization"" in the context of AI and optimization. The word ""稳健"" (robust) directly corresponds to the concept of robustness in optimization, which is crucial in the AI domain, particularly when discussing worst-case scenarios and risk management. 

The second candidate, ""分布式鲁棒优化"" (Distributed Robust Optimization), while also a reasonable translation, introduces the term ""分布式"" (distributed), which may imply a focus on distribution rather than the robustness aspect that is central to the original term. 

The third candidate, ""分布鲁棒优化"" (Distribution Rod Optimization), is semantically inaccurate due to the incorrect translation of ""distributionally"" and the nonsensical term ""rod"" in this context.

Lastly, ""分布式词表示"" (distributed word representation) is completely unrelated to the original term and refers to a different concept in natural language processing, making it the least suitable option. 

Overall, ""分布稳健优化"" maintains the focus on robustness while aligning closely with the intended meaning in the AI context, making it the most appropriate choice.","['Distributed Robust Optimization', 'Distribution rod optimization', 'distributed word representation', 'Distribution Robust Optimization']"
1689,1689,divergence operator,散度算子,0.2857142857142857,7,"[{'word': '发散算子', 'ratio': 0.7142857142857143}, {'word': '散度算子', 'ratio': 0.2857142857142857}]",发散算子,{},[]
1690,1690,diversity score,多样性得分,0.5714285714285714,7,"[{'word': '多样性得分', 'ratio': 0.5714285714285714}, {'word': '多样性评分', 'ratio': 0.42857142857142855}]",多样性得分,{},[]
1691,1691,do-calculus,干预演算,0.1428571428571428,7,"[{'word': 'do-演算', 'ratio': 0.2857142857142857}, {'word': '操作微 Calculus', 'ratio': 0.14285714285714285}, {'word': '操作演算', 'ratio': 0.14285714285714285}, {'word': 'do演算', 'ratio': 0.14285714285714285}, {'word': '干预演算', 'ratio': 0.14285714285714285}, {'word': 'Do演算', 'ratio': 0.14285714285714285}]",do演算,"1. Rank: do演算, 干预演算, do-演算, Do演算, 操作演算, 操作微 Calculus

2. Explanation: The term ""do演算"" is the best fit for ""do-calculus"" because it accurately captures the essence of the original term while maintaining semantic integrity in the context of AI and causal inference. The ""do"" operator is a fundamental concept introduced by Judea Pearl in causal reasoning, and ""演算"" (which means ""calculus"" or ""calculation"") is a direct translation that conveys the mathematical nature of the term. 

""干预演算"" (intervention calculus) is also a strong candidate as it explicitly refers to the intervention aspect of the do-operator, which is crucial in causal analysis. However, it is slightly less concise than ""do演算,"" which directly incorporates the term ""do,"" making it more recognizable and aligned with established terminology in the field.

The other candidates, such as ""do-演算"" and ""Do演算,"" are less preferable due to their inconsistent capitalization and the use of a hyphen, which is not standard in the context of mathematical terms in Chinese. ""操作演算"" (operational calculus) and ""操作微 Calculus"" (operating micro calculus) are less relevant as they do not convey the specific meaning of the do-operator in causal inference, making them less suitable for the AI domain.","['do-calculus', 'Operating Micro Calculus', 'Operational calculus', 'do calculus', 'intervention calculus', 'Do calculus']"
1692,1692,document,文档,1.0,16,"[{'word': '文档', 'ratio': 1.0}]",文档,{},[]
1693,1693,document classification,文档分类,0.8888888888888888,9,"[{'word': '文档分类', 'ratio': 0.8888888888888888}, {'word': '文件分类', 'ratio': 0.1111111111111111}]",文档分类,{},[]
1694,1694,document clustering,文档聚类,1.0,9,"[{'word': '文档聚类', 'ratio': 1.0}]",文档聚类,{},[]
1695,1695,document corpus,文档语料库,1.0,9,"[{'word': '文档语料库', 'ratio': 1.0}]",文档语料库,{},[]
1696,1696,document retrieval,文档检索,0.8888888888888888,9,"[{'word': '文档检索', 'ratio': 0.8888888888888888}, {'word': '文献检索', 'ratio': 0.1111111111111111}]",文档检索,{},[]
1697,1697,document summarization,文档摘要,0.8888888888888888,9,"[{'word': '文档摘要', 'ratio': 0.8888888888888888}, {'word': '文件摘要', 'ratio': 0.1111111111111111}]",文档摘要,{},[]
1698,1698,document vector,文档向量,1.0,9,"[{'word': '文档向量', 'ratio': 1.0}]",文档向量,{},[]
1699,1699,document-level,文档层面,0.0,9,"[{'word': '文档级', 'ratio': 0.6666666666666666}, {'word': '文档级别', 'ratio': 0.2222222222222222}, {'word': '文档级别/文档层级', 'ratio': 0.1111111111111111}]",文档级,{},[]
1700,1700,document-topic assignment,文档主题分配,0.5555555555555556,9,"[{'word': '文档主题分配', 'ratio': 0.5555555555555556}, {'word': '文档-主题分配', 'ratio': 0.4444444444444444}]",文档主题分配,{},[]
1701,1701,domain,领域,0.5555555555555556,18,"[{'word': '领域', 'ratio': 0.7222222222222222}, {'word': '域', 'ratio': 0.2777777777777778}]",领域,{},[]
1702,1702,domain element,领域元素,0.4444444444444444,9,"[{'word': '域元素', 'ratio': 0.5555555555555556}, {'word': '领域元素', 'ratio': 0.4444444444444444}]",域元素,{},[]
1703,1703,domain gap,领域差距,0.6666666666666666,9,"[{'word': '领域差距', 'ratio': 0.6666666666666666}, {'word': '域间差距', 'ratio': 0.2222222222222222}, {'word': '域间差异', 'ratio': 0.1111111111111111}]",领域差距,{},[]
1704,1704,domain generalization,领域泛化,0.6666666666666666,9,"[{'word': '领域泛化', 'ratio': 0.6666666666666666}, {'word': '域泛化', 'ratio': 0.3333333333333333}]",领域泛化,{},[]
1705,1705,domain knowledge,领域知识,0.8888888888888888,9,"[{'word': '领域知识', 'ratio': 0.8888888888888888}, {'word': '专业领域知识', 'ratio': 0.1111111111111111}]",领域知识,{},[]
1706,1706,domain mismatch,领域不匹配,0.8888888888888888,9,"[{'word': '领域不匹配', 'ratio': 0.8888888888888888}, {'word': '专业领域知识', 'ratio': 0.1111111111111111}]",领域不匹配,{},[]
1707,1707,domain ontology,领域本体论,0.0,9,"[{'word': '领域本体', 'ratio': 0.8888888888888888}, {'word': '专业领域知识', 'ratio': 0.1111111111111111}]",领域本体,{},[]
1708,1708,domain shift,领域偏移,0.3333333333333333,9,"[{'word': '领域迁移', 'ratio': 0.4444444444444444}, {'word': '领域偏移', 'ratio': 0.3333333333333333}, {'word': '领域转移', 'ratio': 0.2222222222222222}]",领域迁移,"1. Rank: 领域迁移, 领域转移, 领域偏移

2. Explanation: The term ""领域迁移"" (Domain migration) is the best fit for ""domain shift"" in the context of AI and machine learning. The term ""迁移"" (migration) is commonly used in the AI domain to describe the process of transferring knowledge or adapting models from one domain to another, which aligns well with the concept of ""domain shift."" 

""领域转移"" (Domain transfer) is also a reasonable candidate, as ""转移"" (transfer) can imply a similar meaning. However, ""迁移"" (migration) is more widely recognized in the context of machine learning, particularly in discussions about domain adaptation and transfer learning.

On the other hand, ""领域偏移"" (field offset) is less appropriate because ""偏移"" (offset) does not convey the same sense of adaptation or transformation that is inherent in the concept of ""domain shift."" It suggests a mere change in position rather than the nuanced process of adapting to a different domain, which is critical in AI applications. Thus, ""领域迁移"" is the most semantically accurate and contextually fitting translation.","['Domain migration', 'field offset', 'Domain transfer']"
1709,1709,domain transfer,领域迁移,0.4444444444444444,9,"[{'word': '领域转移', 'ratio': 0.4444444444444444}, {'word': '领域迁移', 'ratio': 0.4444444444444444}, {'word': '域名转移', 'ratio': 0.1111111111111111}]",领域迁移,"1. Rank: 领域迁移, 领域转移, 域名转移

2. Explanation: The term ""领域迁移"" (Domain migration) is the best fit for the AI domain-specific usage of ""domain transfer."" In the context of machine learning and AI, ""迁移"" (migration) is commonly used to describe the process of transferring knowledge or models from one domain to another, which aligns well with the concept of ""domain transfer."" 

On the other hand, ""领域转移"" (Domain transfer) is also a valid translation but is less commonly used in the AI literature compared to ""迁移."" The term ""域名转移"" (Domain name transfer) is not appropriate in this context, as it refers specifically to the transfer of domain names in internet terminology, which is unrelated to the concept of transferring knowledge or models in AI.

Thus, ""领域迁移"" captures the intended meaning more accurately and is more semantically aligned with the established terminology in the AI field.","['Domain transfer', 'Domain migration', 'Domain name transfer']"
1710,1710,domain-specific,特定领域的,0.2857142857142857,7,"[{'word': '领域特定的', 'ratio': 0.7142857142857143}, {'word': '特定领域的', 'ratio': 0.2857142857142857}]",领域特定的,{},[]
1711,1711,dot product,点积,1.0,7,"[{'word': '点积', 'ratio': 1.0}]",点积,{},[]
1712,1712,dot-product attention,点积注意力,1.0,7,"[{'word': '点积注意力', 'ratio': 1.0}]",点积注意力,{},[]
1713,1713,down-sampling,下采样,1.0,7,"[{'word': '下采样', 'ratio': 1.0}]",下采样,{},[]
1714,1714,down-sampling layer,下采样层,1.0,6,"[{'word': '下采样层', 'ratio': 1.0}]",下采样层,{},[]
1715,1715,downsampling block,下采样模块,0.3333333333333333,6,"[{'word': '下采样块', 'ratio': 0.5}, {'word': '下采样模块', 'ratio': 0.3333333333333333}, {'word': '下采样层', 'ratio': 0.16666666666666666}]",下采样块,{},[]
1716,1716,downsampling factor,下采样系数,0.0,6,"[{'word': '下采样因子', 'ratio': 0.8333333333333334}, {'word': '下采样模块', 'ratio': 0.16666666666666666}]",下采样因子,{},[]
1717,1717,downsampling layer,下采样层,0.8333333333333334,6,"[{'word': '下采样层', 'ratio': 0.8333333333333334}, {'word': '下采样因子', 'ratio': 0.16666666666666666}]",下采样层,{},[]
1718,1718,downstream dataset,下游数据集,0.875,8,"[{'word': '下游数据集', 'ratio': 0.875}, {'word': '因果效应', 'ratio': 0.125}]",下游数据集,{},[]
1719,1719,downstream model,下游模型,0.875,8,"[{'word': '下游模型', 'ratio': 0.875}, {'word': '因果效应估计', 'ratio': 0.125}]",下游模型,{},[]
1720,1720,downstream performance,下游性能,0.625,8,"[{'word': '下游性能', 'ratio': 0.625}, {'word': '因果熵', 'ratio': 0.125}, {'word': '下游模型', 'ratio': 0.125}, {'word': '下游表现', 'ratio': 0.125}]",下游性能,{},[]
1721,1721,downstream task,下游任务,0.875,8,"[{'word': '下游任务', 'ratio': 0.875}, {'word': '因果图', 'ratio': 0.125}]",下游任务,{},[]
1722,1722,dropout layer,丢弃层,0.25,8,"[{'word': '随机失活层', 'ratio': 0.25}, {'word': '丢弃层', 'ratio': 0.25}, {'word': 'dropout層', 'ratio': 0.125}, {'word': 'Dropout层', 'ratio': 0.125}, {'word': '因果推断', 'ratio': 0.125}, {'word': '漏失层', 'ratio': 0.125}]","""Dropout层""","1. Rank: ""Dropout层"", ""dropout層"", ""丢弃层"", ""随机失活层"", ""漏失层"", ""因果推断""

2. Explanation: The term ""Dropout层"" is the best fit because it retains the original English term ""dropout"" in its transliterated form, which is widely recognized in the AI and machine learning community. This is important as ""dropout"" is a specific technique used in neural networks to prevent overfitting, and using the original term helps maintain clarity and consistency in technical discussions. The back translation ""Dropout layer"" accurately reflects the original English term, ensuring semantic accuracy.

The second choice, ""dropout層"", is also a good option as it similarly retains the English term but uses a different character for ""layer"". However, it is less common than ""Dropout层"". 

""丢弃层"" translates to ""discard layer"", which is a more literal translation but may not be as widely recognized in the AI community. ""随机失活层"" translates to ""random deactivation layer"", which is also a descriptive term but lacks the established recognition of ""dropout"". ""漏失层"" translates to ""lost layer"", which is not semantically accurate in the context of dropout layers. Lastly, ""因果推断"" translates to ""causal inference"", which is unrelated to the concept of dropout layers and should not be included in this ranking.","['random deactivation layer', 'discard layer', 'dropout layer', 'Dropout layer', 'causal inference', 'lost layer']"
1723,1723,dropout probability,丢弃概率,0.7142857142857143,7,"[{'word': '丢弃概率', 'ratio': 0.7142857142857143}, {'word': '随机失活概率', 'ratio': 0.14285714285714285}, {'word': 'Dropout概率', 'ratio': 0.14285714285714285}]",丢弃概率,{},[]
1724,1724,dropout rate,丢弃率,0.7142857142857143,7,"[{'word': '丢弃率', 'ratio': 0.7142857142857143}, {'word': '随机失活率', 'ratio': 0.14285714285714285}, {'word': 'Dropout率', 'ratio': 0.14285714285714285}]",丢弃率,{},[]
1725,1725,dropout ratio,失活比率,0.0,7,"[{'word': '丢弃比率', 'ratio': 0.5714285714285714}, {'word': '随机失活比例', 'ratio': 0.14285714285714285}, {'word': 'Dropout比例', 'ratio': 0.14285714285714285}, {'word': '丢弃比例', 'ratio': 0.14285714285714285}]",丢弃比率,{},[]
1726,1726,dual decomposition,对偶分解,0.8571428571428571,7,"[{'word': '对偶分解', 'ratio': 0.8571428571428571}, {'word': '双重分解', 'ratio': 0.14285714285714285}]",对偶分解,{},[]
1727,1727,dual encoder model,双编码器模型,0.5714285714285714,7,"[{'word': '双编码器模型', 'ratio': 0.5714285714285714}, {'word': '双重编码器模型', 'ratio': 0.14285714285714285}, {'word': '对偶编码器模型 如果需要进一步的帮助，请随时告知！', 'ratio': 0.14285714285714285}, {'word': '对偶编码器模型', 'ratio': 0.14285714285714285}]",双编码器模型,{},[]
1728,1728,dual norm,对偶范数,1.0,9,"[{'word': '对偶范数', 'ratio': 1.0}]",对偶范数,{},[]
1729,1729,dual objective,双目标,0.0,9,"[{'word': '对偶目标', 'ratio': 0.8888888888888888}, {'word': '对偶目标函数', 'ratio': 0.1111111111111111}]",对偶目标,{},[]
1730,1730,dual optimization problem,对偶优化问题,1.0,9,"[{'word': '对偶优化问题', 'ratio': 1.0}]",对偶优化问题,{},[]
1731,1731,dual parameter,对偶参数,1.0,9,"[{'word': '对偶参数', 'ratio': 1.0}]",对偶参数,{},[]
1732,1732,dual problem,对偶问题,1.0,9,"[{'word': '对偶问题', 'ratio': 1.0}]",对偶问题,{},[]
1733,1733,dual program,对偶问题,0.0,10,"[{'word': '对偶程序', 'ratio': 0.6}, {'word': '对偶规划', 'ratio': 0.2}, {'word': '对偶解', 'ratio': 0.1}, {'word': '双计划', 'ratio': 0.1}]",对偶程序,{},[]
1734,1734,dual solution,对偶解,0.9,10,"[{'word': '对偶解', 'ratio': 0.9}, {'word': '双解', 'ratio': 0.1}]",对偶解,{},[]
1735,1735,dual variable,对偶变量,0.9,10,"[{'word': '对偶变量', 'ratio': 0.9}, {'word': '双变量', 'ratio': 0.1}]",对偶变量,{},[]
1736,1736,duality gap,对偶间隙,0.7,10,"[{'word': '对偶间隙', 'ratio': 0.7}, {'word': '对偶性间隙', 'ratio': 0.2}, {'word': '二元差距', 'ratio': 0.1}]",对偶间隙,{},[]
1737,1737,dynamic Bayesian network,动态贝叶斯网络,1.0,10,"[{'word': '动态贝叶斯网络', 'ratio': 1.0}]",动态贝叶斯网络,{},[]
1738,1738,dynamic model,动态模型,1.0,6,"[{'word': '动态模型', 'ratio': 1.0}]",动态模型,{},[]
1739,1739,dynamic programming algorithm,动态规划算法,1.0,6,"[{'word': '动态规划算法', 'ratio': 1.0}]",动态规划算法,{},[]
1740,1740,dynamic time warping,动态时间规整,1.0,6,"[{'word': '动态时间规整', 'ratio': 1.0}]",动态时间规整,{},[]
1741,1741,dynamical model,动力学模型,0.8333333333333334,6,"[{'word': '动力学模型', 'ratio': 0.8333333333333334}, {'word': '动态模型', 'ratio': 0.16666666666666666}]",动力学模型,{},[]
1742,1742,dynamical system,动力系统,0.6666666666666666,6,"[{'word': '动力系统', 'ratio': 0.6666666666666666}, {'word': '动态系统', 'ratio': 0.3333333333333333}]",动力系统,{},[]
1743,1743,early fusion,早期融合,1.0,6,"[{'word': '早期融合', 'ratio': 1.0}]",早期融合,{},[]
1744,1744,early stopping,提前停止,0.0,6,"[{'word': '早期停止', 'ratio': 0.3333333333333333}, {'word': '早停法', 'ratio': 0.3333333333333333}, {'word': '早停', 'ratio': 0.3333333333333333}]",早停法,"1. Rank: 早停法, 早期停止, 早停

2. Explanation: The term ""早停法"" (Early stopping method) is the best fit for the AI domain-specific usage because it explicitly conveys the methodical aspect of early stopping in machine learning contexts. The inclusion of ""法"" (method) indicates that it is a systematic approach, which aligns well with the technical nature of the term in AI literature. 

""早期停止"" (early stop) is a direct translation that captures the essence of the term but lacks the specificity of being a method, which is crucial in a technical context. 

""早停"" (stop early) is the most concise but is too vague and informal for academic or technical writing, as it does not clearly indicate that it refers to a method used to prevent overfitting in model training. 

Thus, ""早停法"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['early stop', 'Early stopping method', 'Stop early']"
1745,1745,earth-mover distance,运输距离,0.0,6,"[{'word': '地球搬运工距离', 'ratio': 0.5}, {'word': '推土机距离', 'ratio': 0.3333333333333333}, {'word': '地球搬运距离', 'ratio': 0.16666666666666666}]",地球搬运工距离,{},[]
1746,1746,edge detection,边缘检测,1.0,6,"[{'word': '边缘检测', 'ratio': 1.0}]",边缘检测,{},[]
1747,1747,edge feature,边缘特征,0.6666666666666666,6,"[{'word': '边缘特征', 'ratio': 0.6666666666666666}, {'word': '边特征', 'ratio': 0.3333333333333333}]",边缘特征,{},[]
1748,1748,edge label,边标签,1.0,6,"[{'word': '边标签', 'ratio': 1.0}]",边标签,{},[]
1749,1749,edge prediction,边缘预测,0.0,6,"[{'word': '边预测', 'ratio': 1.0}]",边预测,{},[]
1750,1750,edge set,边集,1.0,6,"[{'word': '边集', 'ratio': 1.0}]",边集,{},[]
1751,1751,edge weight,边权重,1.0,6,"[{'word': '边权重', 'ratio': 1.0}]",边权重,{},[]
1752,1752,edit distance,编辑距离,1.0,8,"[{'word': '编辑距离', 'ratio': 1.0}]",编辑距离,{},[]
1753,1753,effective receptive field,有效感受野,1.0,8,"[{'word': '有效感受野', 'ratio': 1.0}]",有效感受野,{},[]
1754,1754,ego-motion,自身运动,0.375,8,"[{'word': '自我运动', 'ratio': 0.625}, {'word': '自身运动', 'ratio': 0.375}]",自我运动,{},[]
1755,1755,eigen-decomposition,特征分解,1.0,8,"[{'word': '特征分解', 'ratio': 1.0}]",特征分解,{},[]
1756,1756,eigenbasis,特征基底,0.0,8,"[{'word': '特征基', 'ratio': 1.0}]",特征基,{},[]
1757,1757,eigendecay,特征衰减,0.625,8,"[{'word': '特征衰减', 'ratio': 0.625}, {'word': '特征值衰减', 'ratio': 0.25}, {'word': '特征衰变', 'ratio': 0.125}]",特征衰减,{},[]
1758,1758,eigenfunction,特征函数,0.875,8,"[{'word': '特征函数', 'ratio': 0.875}, {'word': '本征函数', 'ratio': 0.125}]",特征函数,{},[]
1759,1759,eigenspace,特征空间,1.0,8,"[{'word': '特征空间', 'ratio': 1.0}]",特征空间,{},[]
1760,1760,eigenspectrum,特征谱,1.0,8,"[{'word': '特征谱', 'ratio': 1.0}]",特征谱,{},[]
1761,1761,eigenvalue,特征值,1.0,8,"[{'word': '特征值', 'ratio': 1.0}]",特征值,{},[]
1762,1762,eigenvalue decomposition,特征值分解,1.0,8,"[{'word': '特征值分解', 'ratio': 1.0}]",特征值分解,{},[]
1763,1763,eigenvector,特征向量,1.0,8,"[{'word': '特征向量', 'ratio': 1.0}]",特征向量,{},[]
1764,1764,elastic net regularization,弹性网络正则化,0.5,8,"[{'word': '弹性网正则化', 'ratio': 0.5}, {'word': '弹性网络正则化', 'ratio': 0.5}]",弹性网正则化,{},[]
1765,1765,element-wise,逐元素,0.25,8,"[{'word': '元素级', 'ratio': 0.625}, {'word': '逐元素', 'ratio': 0.25}, {'word': '按元素', 'ratio': 0.125}]",元素级,{},[]
1766,1766,element-wise multiplication,逐元素相乘,0.0,8,"[{'word': '元素级乘法', 'ratio': 0.625}, {'word': '逐元素乘法', 'ratio': 0.25}, {'word': '按元素乘法', 'ratio': 0.125}]",元素级乘法,{},[]
1767,1767,element-wise product,逐元素乘积,0.0,7,"[{'word': '元素逐项乘积', 'ratio': 0.42857142857142855}, {'word': '元素逐位乘积', 'ratio': 0.2857142857142857}, {'word': '元素逐项积', 'ratio': 0.14285714285714285}, {'word': '元素级乘积', 'ratio': 0.14285714285714285}]",元素逐项乘积,"1. Rank: 元素逐项乘积, 元素逐项积, 元素逐位乘积, 元素级乘积

2. Explanation: The term ""元素逐项乘积"" (element-wise product) is the best fit because it accurately captures the concept of performing operations on corresponding elements of two matrices, which is essential in the context of AI and machine learning. The term ""逐项"" (element-wise) is commonly used in mathematical and computational contexts to denote operations that are applied to each individual element of the matrices. 

The second candidate, ""元素逐项积"" (element-wise product), is also a good fit but is slightly less common in usage compared to the first. The third candidate, ""元素逐位乘积"" (element-wise bitwise product), introduces the term ""逐位"" (bitwise), which is misleading in this context as it suggests a different type of operation that is not relevant to the element-wise multiplication of matrices. Lastly, ""元素级乘积"" (element-level product) is less precise and not as widely recognized in the AI domain, making it a less favorable choice. Overall, ""元素逐项乘积"" is the most semantically accurate and contextually appropriate term for the AI domain.","['Element-wise product', 'Element-wise bitwise product', 'Element-wise product', 'element-level product']"
1768,1768,elementary tree,基本树,0.7142857142857143,7,"[{'word': '基本树', 'ratio': 0.7142857142857143}, {'word': '基元树', 'ratio': 0.2857142857142857}]",基本树,{},[]
1769,1769,eligibility trace,资格迹,0.1428571428571428,7,"[{'word': '合格迹', 'ratio': 0.42857142857142855}, {'word': '合格迹线', 'ratio': 0.2857142857142857}, {'word': '合格迹象', 'ratio': 0.14285714285714285}, {'word': '资格迹', 'ratio': 0.14285714285714285}]",合格迹线,"1. Rank: 合格迹线, 合格迹, 资格迹, 合格迹象

2. Explanation: The term ""合格迹线"" (Qualified trace) is the best fit for ""eligibility trace"" in the context of AI and reinforcement learning. This term accurately captures the concept of a trace that indicates eligibility for updates in policy-gradient algorithms, which is crucial in the context of the provided excerpts. The use of ""迹线"" (trace line) aligns well with the technical nature of the term, as it suggests a continuous representation of eligibility over time, which is essential in reinforcement learning.

The other candidates, such as ""合格迹"" (Pass mark) and ""资格迹"" (Qualification mark), do not convey the same technical meaning and could lead to confusion, as they imply a more general qualification or standard rather than a specific mechanism used in algorithms. ""合格迹象"" (Qualification sign) is even less suitable, as it suggests a mere indication rather than a functional component of the algorithm. Therefore, ""合格迹线"" stands out as the most semantically accurate and contextually appropriate translation for ""eligibility trace"" in the AI domain.","['Pass mark', 'Qualified trace', 'Qualification sign', 'Qualification mark']"
1770,1770,embedded deformation graph,嵌入式变形图,0.7142857142857143,7,"[{'word': '嵌入式变形图', 'ratio': 0.7142857142857143}, {'word': '嵌入变形图', 'ratio': 0.2857142857142857}]",嵌入式变形图,{},[]
1771,1771,embedding dimension,嵌入维度,0.8333333333333334,6,"[{'word': '嵌入维度', 'ratio': 0.8333333333333334}, {'word': '嵌入维数', 'ratio': 0.16666666666666666}]",嵌入维度,{},[]
1772,1772,embedding dimensionality,嵌入维度,0.1666666666666666,6,"[{'word': '嵌入维数', 'ratio': 0.8333333333333334}, {'word': '嵌入维度', 'ratio': 0.16666666666666666}]",嵌入维数,{},[]
1773,1773,embedding feature,嵌入特征,1.0,6,"[{'word': '嵌入特征', 'ratio': 1.0}]",嵌入特征,{},[]
1774,1774,embedding layer,嵌入层,1.0,6,"[{'word': '嵌入层', 'ratio': 1.0}]",嵌入层,{},[]
1775,1775,embedding matrix,嵌入矩阵,1.0,6,"[{'word': '嵌入矩阵', 'ratio': 1.0}]",嵌入矩阵,{},[]
1776,1776,embedding model,嵌入模型,1.0,7,"[{'word': '嵌入模型', 'ratio': 1.0}]",嵌入模型,{},[]
1777,1777,embedding parameter,嵌入参数,1.0,7,"[{'word': '嵌入参数', 'ratio': 1.0}]",嵌入参数,{},[]
1778,1778,embedding size,嵌入大小,0.4285714285714285,7,"[{'word': '嵌入大小', 'ratio': 0.42857142857142855}, {'word': '嵌入维度', 'ratio': 0.42857142857142855}, {'word': '嵌入尺寸', 'ratio': 0.14285714285714285}]",嵌入维度,"1. Rank: 嵌入维度, 嵌入大小, 嵌入尺寸

2. Explanation: The term ""嵌入维度"" (embedded dimensions) is the best fit for ""embedding size"" in the AI domain because it accurately captures the concept of dimensionality associated with embeddings, which is a critical aspect in machine learning and deep learning contexts. In AI, ""embedding"" refers to the representation of data in a continuous vector space, and ""维度"" (dimensions) directly relates to the number of features or coordinates in that space. 

While ""嵌入大小"" (embed size) and ""嵌入尺寸"" (embed size) are also valid translations, they are less precise in conveying the technical meaning of ""size"" in this context. ""大小"" and ""尺寸"" can imply physical size or volume, which may not resonate with the abstract mathematical concept of dimensionality in embeddings. Therefore, ""嵌入维度"" is the most semantically accurate and contextually appropriate choice for the AI terminology.","['Embed size', 'embedded dimensions', 'Embed size']"
1779,1779,embedding space,嵌入空间,1.0,7,"[{'word': '嵌入空间', 'ratio': 1.0}]",嵌入空间,{},[]
1780,1780,embedding vector,嵌入向量,1.0,7,"[{'word': '嵌入向量', 'ratio': 1.0}]",嵌入向量,{},[]
1781,1781,embedding-based metric,嵌入基础度量,0.0,6,"[{'word': '基于嵌入的度量', 'ratio': 1.0}]",基于嵌入的度量,{},[]
1782,1782,embodied agent,具身代理,0.3333333333333333,6,"[{'word': '具身智能体', 'ratio': 0.6666666666666666}, {'word': '具身代理', 'ratio': 0.3333333333333333}]",具身智能体,{},[]
1783,1783,emission probability,发射概率,1.0,6,"[{'word': '发射概率', 'ratio': 1.0}]",发射概率,{},[]
1784,1784,emotion classification,情感分类,1.0,6,"[{'word': '情感分类', 'ratio': 1.0}]",情感分类,{},[]
1785,1785,empirical Bayes,经验贝叶斯,1.0,6,"[{'word': '经验贝叶斯', 'ratio': 1.0}]",经验贝叶斯,{},[]
1786,1786,empirical distribution,经验分布,1.0,6,"[{'word': '经验分布', 'ratio': 1.0}]",经验分布,{},[]
1787,1787,empirical estimate,经验估算,0.0,6,"[{'word': '经验估计', 'ratio': 1.0}]",经验估计,{},[]
1788,1788,empirical estimator,经验估计量,0.6666666666666666,6,"[{'word': '经验估计量', 'ratio': 0.6666666666666666}, {'word': '经验估计器', 'ratio': 0.3333333333333333}]",经验估计量,{},[]
1789,1789,empirical frequency,经验频率,1.0,6,"[{'word': '经验频率', 'ratio': 1.0}]",经验频率,{},[]
1790,1790,empirical loss,经验损失,1.0,6,"[{'word': '经验损失', 'ratio': 1.0}]",经验损失,{},[]
1791,1791,empirical mean,经验均值,0.8333333333333334,6,"[{'word': '经验均值', 'ratio': 0.8333333333333334}, {'word': '经验平均值', 'ratio': 0.16666666666666666}]",经验均值,{},[]
1792,1792,empirical measure,经验测度,1.0,6,"[{'word': '经验测度', 'ratio': 1.0}]",经验测度,{},[]
1793,1793,empirical minimizer,经验最小化器,0.8333333333333334,6,"[{'word': '经验最小化器', 'ratio': 0.8333333333333334}, {'word': '经验极小化器', 'ratio': 0.16666666666666666}]",经验最小化器,{},[]
1794,1794,empirical process theory,经验过程理论,1.0,6,"[{'word': '经验过程理论', 'ratio': 1.0}]",经验过程理论,{},[]
1795,1795,empirical risk,经验风险,1.0,6,"[{'word': '经验风险', 'ratio': 1.0}]",经验风险,{},[]
1796,1796,empirical risk minimization,经验风险最小化,1.0,6,"[{'word': '经验风险最小化', 'ratio': 1.0}]",经验风险最小化,{},[]
1797,1797,empirical risk minimizer,经验风险最小化者,0.0,6,"[{'word': '经验风险最小化器', 'ratio': 1.0}]",经验风险最小化器,{},[]
1798,1798,empirical variance,经验方差,1.0,6,"[{'word': '经验方差', 'ratio': 1.0}]",经验方差,{},[]
1799,1799,emulator,模拟器,0.6666666666666666,6,"[{'word': '模拟器', 'ratio': 0.6666666666666666}, {'word': '仿真器', 'ratio': 0.3333333333333333}]",模拟器,{},[]
1800,1800,encoder layer,编码器层,1.0,6,"[{'word': '编码器层', 'ratio': 1.0}]",编码器层,{},[]
1801,1801,encoder model,编码器模型,0.8333333333333334,6,"[{'word': '编码器模型', 'ratio': 0.8333333333333334}, {'word': '编码器层', 'ratio': 0.16666666666666666}]",编码器模型,{},[]
1802,1802,encoder network,编码器网络,1.0,6,"[{'word': '编码器网络', 'ratio': 1.0}]",编码器网络,{},[]
1803,1803,encoder states,编码器状态,1.0,5,"[{'word': '编码器状态', 'ratio': 1.0}]",编码器状态,{},[]
1804,1804,encoder-decoder architecture,编码器-解码器架构,1.0,5,"[{'word': '编码器-解码器架构', 'ratio': 1.0}]",编码器-解码器架构,{},[]
1805,1805,encoder-decoder framework,编码器-解码器框架,1.0,5,"[{'word': '编码器-解码器框架', 'ratio': 1.0}]",编码器-解码器框架,{},[]
1806,1806,encoder-decoder model,编码器-解码器模型,1.0,5,"[{'word': '编码器-解码器模型', 'ratio': 1.0}]",编码器-解码器模型,{},[]
1807,1807,end-of-sequence token,序列结束标记,0.8888888888888888,9,"[{'word': '序列结束标记', 'ratio': 0.8888888888888888}, {'word': '结束序列标记', 'ratio': 0.1111111111111111}]",序列结束标记,{},[]
1808,1808,end-to-end learning,端到端学习,1.0,9,"[{'word': '端到端学习', 'ratio': 1.0}]",端到端学习,{},[]
1809,1809,end-to-end model,端到端模型,1.0,9,"[{'word': '端到端模型', 'ratio': 1.0}]",端到端模型,{},[]
1810,1810,end-to-end neural model,端到端神经模型,0.8888888888888888,9,"[{'word': '端到端神经模型', 'ratio': 0.8888888888888888}, {'word': '端到端神经网络模型', 'ratio': 0.1111111111111111}]",端到端神经模型,{},[]
1811,1811,end-to-end pipeline,端到端流水线,0.1666666666666666,6,"[{'word': '端到端流程', 'ratio': 0.5}, {'word': '端到端管道', 'ratio': 0.3333333333333333}, {'word': '端到端流水线', 'ratio': 0.16666666666666666}]",端到端流程,{},[]
1812,1812,end-to-end system,端到端系统,1.0,6,"[{'word': '端到端系统', 'ratio': 1.0}]",端到端系统,{},[]
1813,1813,end-to-end training,端到端训练,1.0,6,"[{'word': '端到端训练', 'ratio': 1.0}]",端到端训练,{},[]
1814,1814,energy function,能量函数,0.8333333333333334,6,"[{'word': '能量函数', 'ratio': 0.8333333333333334}, {'word': '端到端训练', 'ratio': 0.16666666666666666}]",能量函数,{},[]
1815,1815,energy minimization,能量最小化,1.0,6,"[{'word': '能量最小化', 'ratio': 1.0}]",能量最小化,{},[]
1816,1816,energy minimization framework,能量最小化框架,0.8888888888888888,9,"[{'word': '能量最小化框架', 'ratio': 0.8888888888888888}, {'word': '能源最小化框架', 'ratio': 0.1111111111111111}]",能量最小化框架,{},[]
1817,1817,energy minimization problem,能量最小化问题,1.0,9,"[{'word': '能量最小化问题', 'ratio': 1.0}]",能量最小化问题,{},[]
1818,1818,ensemble classification,集成分类,1.0,9,"[{'word': '集成分类', 'ratio': 1.0}]",集成分类,{},[]
1819,1819,ensemble classifier,集成分类器,1.0,9,"[{'word': '集成分类器', 'ratio': 1.0}]",集成分类器,{},[]
1820,1820,ensemble learning,集成学习,1.0,9,"[{'word': '集成学习', 'ratio': 1.0}]",集成学习,{},[]
1821,1821,ensemble method,集成方法,1.0,7,"[{'word': '集成方法', 'ratio': 1.0}]",集成方法,{},[]
1822,1822,ensemble model,集成模型,1.0,7,"[{'word': '集成模型', 'ratio': 1.0}]",集成模型,{},[]
1823,1823,ensemble of classifier,分类器集合,0.0,7,"[{'word': '分类器集成', 'ratio': 0.8571428571428571}, {'word': '分类器的集合', 'ratio': 0.14285714285714285}]",分类器集成,{},[]
1824,1824,ensemble size,集成大小,0.1428571428571428,7,"[{'word': '集成规模', 'ratio': 0.7142857142857143}, {'word': '集成大小', 'ratio': 0.14285714285714285}, {'word': '乐团规模', 'ratio': 0.14285714285714285}]",集成规模,{},[]
1825,1825,entail,蕴涵,0.1428571428571428,7,"[{'word': '蕴含', 'ratio': 0.7142857142857143}, {'word': '包含', 'ratio': 0.14285714285714285}, {'word': '蕴涵', 'ratio': 0.14285714285714285}]",蕴含,{},[]
1826,1826,entailment,蕴涵,0.7142857142857143,7,"[{'word': '蕴涵', 'ratio': 0.7142857142857143}, {'word': '蕴含', 'ratio': 0.2857142857142857}]",蕴涵,{},[]
1827,1827,entailment detection,蕴含检测,0.2857142857142857,7,"[{'word': '蕴涵检测', 'ratio': 0.7142857142857143}, {'word': '蕴含检测', 'ratio': 0.2857142857142857}]",蕴涵检测,{},[]
1828,1828,entity,实体,1.0,14,"[{'word': '实体', 'ratio': 1.0}]",实体,{},[]
1829,1829,entity coreference,实体消解,0.0,5,"[{'word': '实体共指', 'ratio': 1.0}]",实体共指,{},[]
1830,1830,entity description,实体描述,1.0,5,"[{'word': '实体描述', 'ratio': 1.0}]",实体描述,{},[]
1831,1831,entity detection,实体检测,1.0,5,"[{'word': '实体检测', 'ratio': 1.0}]",实体检测,{},[]
1832,1832,entity embedding,实体嵌入,1.0,5,"[{'word': '实体嵌入', 'ratio': 1.0}]",实体嵌入,{},[]
1833,1833,entity extraction,实体抽取,0.4,5,"[{'word': '实体提取', 'ratio': 0.6}, {'word': '实体抽取', 'ratio': 0.4}]",实体提取,{},[]
1834,1834,entity linker,实体链接器,1.0,7,"[{'word': '实体链接器', 'ratio': 1.0}]",实体链接器,{},[]
1835,1835,entity mention,实体提及,1.0,7,"[{'word': '实体提及', 'ratio': 1.0}]",实体提及,{},[]
1836,1836,entity recognition,实体识别,1.0,7,"[{'word': '实体识别', 'ratio': 1.0}]",实体识别,{},[]
1837,1837,entity representation,实体表示,1.0,7,"[{'word': '实体表示', 'ratio': 1.0}]",实体表示,{},[]
1838,1838,entity resolution,实体消歧,0.0,7,"[{'word': '实体解析', 'ratio': 0.7142857142857143}, {'word': '实体解析 如果您需要更多帮助，请随时告诉我！', 'ratio': 0.14285714285714285}, {'word': '实体消解', 'ratio': 0.14285714285714285}]",实体解析,{},[]
1839,1839,entity set,实体集,0.8,5,"[{'word': '实体集', 'ratio': 0.8}, {'word': '实体集合', 'ratio': 0.2}]",实体集,{},[]
1840,1840,entity type,实体类型,1.0,5,"[{'word': '实体类型', 'ratio': 1.0}]",实体类型,{},[]
1841,1841,entropy estimation,熵估计,1.0,5,"[{'word': '熵估计', 'ratio': 1.0}]",熵估计,{},[]
1842,1842,entropy function,熵函数,1.0,5,"[{'word': '熵函数', 'ratio': 1.0}]",熵函数,{},[]
1843,1843,entropy loss,熵损失,1.0,5,"[{'word': '熵损失', 'ratio': 1.0}]",熵损失,{},[]
1844,1844,entropy regularization,熵正则化,1.0,5,"[{'word': '熵正则化', 'ratio': 1.0}]",熵正则化,{},[]
1845,1845,enumeration algorithm,枚举算法,1.0,5,"[{'word': '枚举算法', 'ratio': 1.0}]",枚举算法,{},[]
1846,1846,envy-freeness,无嫉妒性,1.0,5,"[{'word': '无嫉妒性', 'ratio': 1.0}]",无嫉妒性,{},[]
1847,1847,eos,终止符号,0.0,5,"[{'word': '结束标记', 'ratio': 0.6}, {'word': '结束符', 'ratio': 0.2}, {'word': '序列结束符', 'ratio': 0.2}]",结束标记,{},[]
1848,1848,epipolar constraint,对极约束,0.0,5,"[{'word': '极线约束', 'ratio': 1.0}]",极线约束,{},[]
1849,1849,epipolar geometry,对极几何,0.25,8,"[{'word': '极线几何', 'ratio': 0.375}, {'word': '对极几何', 'ratio': 0.25}, {'word': '外极几何', 'ratio': 0.25}, {'word': '乖角几何', 'ratio': 0.125}]",极线几何,"1. Rank: 极线几何, 对极几何, 外极几何, 乖角几何

2. Explanation: The term ""极线几何"" (epipolar geometry) is the most widely accepted and recognized translation in the field of computer vision and stereo imaging. It accurately captures the concept of epipolar geometry, which is crucial for understanding the relationship between two views in stereo vision. The back translation aligns perfectly with the original English term, ensuring semantic accuracy.

""对极几何"" (epipolar geometry) is also a valid translation, but it is less commonly used in the literature compared to ""极线几何."" The term ""外极几何"" (Exopolar geometry) is misleading as it introduces a non-standard term that does not accurately reflect the concept. ""乖角几何"" (Angular geometry) is entirely inappropriate as it does not relate to the concept of epipolar geometry at all.

In the AI domain, especially in stereo vision and 3D reconstruction, using the most recognized and contextually appropriate term is essential for clear communication and understanding among professionals. Therefore, ""极线几何"" is the best fit.","['epipolar geometry', 'epipolar geometry', 'Exopolar geometry', 'Angular geometry']"
1850,1850,epipolar line,对极线,0.125,8,"[{'word': '极线', 'ratio': 0.5}, {'word': '外极线', 'ratio': 0.25}, {'word': '乖角线', 'ratio': 0.125}, {'word': '对极线', 'ratio': 0.125}]",极线,{},[]
1851,1851,epipole,极点,0.5,8,"[{'word': '极点', 'ratio': 0.5}, {'word': '外极点', 'ratio': 0.25}, {'word': '乖角点', 'ratio': 0.125}, {'word': '对极点', 'ratio': 0.125}]",极点,{},[]
1852,1852,episodic return,回合回报,0.375,8,"[{'word': '回合回报', 'ratio': 0.375}, {'word': '逐集回报', 'ratio': 0.25}, {'word': '情节回报', 'ratio': 0.125}, {'word': '事件回报', 'ratio': 0.125}, {'word': '偶发性回归', 'ratio': 0.125}]",回合回报,"1. Rank: 回合回报, 逐集回报, 事件回报, 情节回报, 偶发性回归

2. Explanation: The term ""回合回报"" (episodic return) is the best fit because it accurately captures the concept of ""episodic"" in the context of reinforcement learning, where ""回合"" (round or episode) refers to a complete sequence of interactions in the environment. This term is semantically accurate and aligns well with the AI domain's usage, as it directly relates to the performance measurement of an agent over a complete episode. 

""逐集回报"" (episode-by-episode reporting) is a close second, but it implies a more granular reporting aspect rather than the overall return, which is the focus in the provided context. The other candidates, such as ""事件回报"" (event return) and ""情节回报"" (plot payoff), do not convey the episodic nature of the return and could lead to confusion in the AI context. ""偶发性回归"" (sporadic regression) is the least suitable as it introduces a different meaning that does not relate to the concept of returns in reinforcement learning.","['turn return', 'Episode-by-episode reporting', 'plot payoff', 'event return', 'sporadic regression']"
1853,1853,epistemic uncertainty,认识论不确定性,0.0,8,"[{'word': '认知不确定性', 'ratio': 0.625}, {'word': '认识不确定性', 'ratio': 0.375}]",认知不确定性,{},[]
1854,1854,epoch,轮次,0.7142857142857143,7,"[{'word': '轮次', 'ratio': 0.7142857142857143}, {'word': '纪元', 'ratio': 0.2857142857142857}]",轮次,{},[]
1855,1855,equalized odd,等化奇偶性,0.0,7,"[{'word': '均衡赔率', 'ratio': 0.42857142857142855}, {'word': '平衡机会', 'ratio': 0.14285714285714285}, {'word': '等化奇数', 'ratio': 0.14285714285714285}, {'word': '平衡几率', 'ratio': 0.14285714285714285}, {'word': '均等赔率', 'ratio': 0.14285714285714285}]",均衡赔率,"1. Rank: 均衡赔率, 平衡几率, 均等赔率, 平衡机会, 等化奇数

2. Explanation: The term ""均衡赔率"" (equilibrium odds) is the best fit for ""equalized odd"" in the AI domain context. This is because ""均衡"" (equilibrium) conveys a sense of balance and fairness, which aligns well with the concept of ""equalized"" in the context of fairness notions in AI. The term ""赔率"" (odds) is also relevant as it relates to probabilities, which is often a key aspect in discussions of fairness and decision-making algorithms. 

The second choice, ""平衡几率"" (balanced odds), is also a strong candidate, but ""几率"" (odds) is slightly less specific than ""赔率"" (odds) in this context. The other candidates, such as ""均等赔率"" (even odds) and ""平衡机会"" (balance opportunity), do not capture the same level of semantic accuracy related to the concept of fairness in AI. ""等化奇数"" (equalize odd numbers) is less relevant as it introduces a numerical aspect that does not fit the context of fairness in AI algorithms. Thus, ""均衡赔率"" is the most contextually appropriate and semantically accurate translation.","['equilibrium odds', 'Balance opportunity', 'Equalize odd numbers', 'balanced odds', 'Even odds']"
1856,1856,equivalence class,等价类,1.0,7,"[{'word': '等价类', 'ratio': 1.0}]",等价类,{},[]
1857,1857,equivalence query,等价查询,1.0,7,"[{'word': '等价查询', 'ratio': 1.0}]",等价查询,{},[]
1858,1858,equivariance,等变性,1.0,7,"[{'word': '等变性', 'ratio': 1.0}]",等变性,{},[]
1859,1859,equivariant,等变,0.1111111111111111,9,"[{'word': '等变的', 'ratio': 0.8888888888888888}, {'word': '等变', 'ratio': 0.1111111111111111}]",等变的,{},[]
1860,1860,error,误差,0.6666666666666666,9,"[{'word': '误差', 'ratio': 0.6666666666666666}, {'word': '错误', 'ratio': 0.3333333333333333}]",误差,{},[]
1861,1861,error analysis,错误分析,0.3333333333333333,9,"[{'word': '误差分析', 'ratio': 0.6666666666666666}, {'word': '错误分析', 'ratio': 0.3333333333333333}]",误差分析,{},[]
1862,1862,error bound,误差界限,0.3333333333333333,9,"[{'word': '误差界限', 'ratio': 0.3333333333333333}, {'word': '误差界', 'ratio': 0.3333333333333333}, {'word': '错误界限', 'ratio': 0.2222222222222222}, {'word': '错误界', 'ratio': 0.1111111111111111}]",误差界限,"1. Rank: 误差界限, 误差界, 错误界限, 错误界

2. Explanation: The term ""误差界限"" is the best fit for ""error bound"" in the AI domain due to its semantic accuracy and contextual relevance. In mathematical and statistical contexts, ""误差"" (error) and ""界限"" (bound) are commonly used to describe the limits or thresholds of error in predictions or estimations. This term accurately conveys the concept of a boundary within which the error is expected to fall, which is crucial in discussions about data transformation and model performance. 

The second candidate, ""误差界,"" while still relevant, is less precise as it omits ""限"" (limit), which is essential for conveying the idea of a boundary. The third and fourth candidates, ""错误界限"" and ""错误界,"" use ""错误"" (mistake) instead of ""误差,"" which is less appropriate in a technical context where ""误差"" (error) is the standard term used to refer to deviations from a true value. Thus, ""误差界限"" is the most contextually accurate and semantically precise choice for the term ""error bound"" in this AI-related discussion.","['Error bound', 'error bound', 'error bounds', 'error bound']"
1863,1863,error function,误差函数,1.0,9,"[{'word': '误差函数', 'ratio': 1.0}]",误差函数,{},[]
1864,1864,error probability,错误概率,0.9,10,"[{'word': '错误概率', 'ratio': 0.9}, {'word': '误差概率', 'ratio': 0.1}]",错误概率,{},[]
1865,1865,error rate,错误率,1.0,10,"[{'word': '错误率', 'ratio': 1.0}]",错误率,{},[]
1866,1866,error tolerance,误差容限,0.4,10,"[{'word': '错误容忍度', 'ratio': 0.5}, {'word': '误差容限', 'ratio': 0.4}, {'word': '误差容忍度', 'ratio': 0.1}]",错误容忍度,{},[]
1867,1867,estimation error,估计误差,1.0,10,"[{'word': '估计误差', 'ratio': 1.0}]",估计误差,{},[]
1868,1868,estimator,估计量,1.0,10,"[{'word': '估计量', 'ratio': 1.0}]",估计量,{},[]
1869,1869,evaluation function,评估函数,1.0,9,"[{'word': '评估函数', 'ratio': 1.0}]",评估函数,{},[]
1870,1870,evaluation metric,评价指标,0.0,9,"[{'word': '评估指标', 'ratio': 1.0}]",评估指标,{},[]
1871,1871,evaluation set,评估集,1.0,9,"[{'word': '评估集', 'ratio': 1.0}]",评估集,{},[]
1872,1872,event calculus,事件演算,1.0,7,"[{'word': '事件演算', 'ratio': 1.0}]",事件演算,{},[]
1873,1873,event coreference,事件共指,0.8571428571428571,7,"[{'word': '事件共指', 'ratio': 0.8571428571428571}, {'word': '事件共指消解', 'ratio': 0.14285714285714285}]",事件共指,{},[]
1874,1874,event detection,事件检测,1.0,7,"[{'word': '事件检测', 'ratio': 1.0}]",事件检测,{},[]
1875,1875,event extraction,事件抽取,0.5714285714285714,7,"[{'word': '事件抽取', 'ratio': 0.5714285714285714}, {'word': '事件提取', 'ratio': 0.42857142857142855}]",事件抽取,{},[]
1876,1876,evidence lower bound,证据下界,1.0,7,"[{'word': '证据下界', 'ratio': 1.0}]",证据下界,{},[]
1877,1877,evidence maximization,证据最大化,0.9,10,"[{'word': '证据最大化', 'ratio': 0.9}, {'word': '证据最大化`', 'ratio': 0.1}]",证据最大化,{},[]
1878,1878,exact inference,精确推理,0.3,10,"[{'word': '精确推断', 'ratio': 0.7}, {'word': '精确推理', 'ratio': 0.3}]",精确推断,{},[]
1879,1879,excess loss,超额损失,0.5,10,"[{'word': '超额损失', 'ratio': 0.5}, {'word': '额外损失', 'ratio': 0.3}, {'word': '过量损失', 'ratio': 0.1}, {'word': '过剩损失', 'ratio': 0.1}]",超额损失,{},[]
1880,1880,exchangeability,可交换性,0.9,10,"[{'word': '可交换性', 'ratio': 0.9}, {'word': '交换性', 'ratio': 0.1}]",可交换性,{},[]
1881,1881,existential quantifier,存在量词,0.9,10,"[{'word': '存在量词', 'ratio': 0.9}, {'word': '可交换性', 'ratio': 0.1}]",存在量词,{},[]
1882,1882,expected loss,期望损失,1.0,9,"[{'word': '期望损失', 'ratio': 1.0}]",期望损失,{},[]
1883,1883,expected reward,期望奖励 (Qīwàng jiǎnglì),0.0,9,"[{'word': '期望奖励', 'ratio': 0.8888888888888888}, {'word': '期望收益', 'ratio': 0.1111111111111111}]",期望奖励,{},[]
1884,1884,expected utility,期望效用,1.0,9,"[{'word': '期望效用', 'ratio': 1.0}]",期望效用,{},[]
1885,1885,experience replay,经验重放,0.5555555555555556,9,"[{'word': '经验重放', 'ratio': 0.5555555555555556}, {'word': '经验回放', 'ratio': 0.3333333333333333}, {'word': 'V', 'ratio': 0.1111111111111111}]",经验重放,{},[]
1886,1886,expert demonstration,专家示范,0.8,10,"[{'word': '专家示范', 'ratio': 0.8}, {'word': '专家演示', 'ratio': 0.2}]",专家示范,{},[]
1887,1887,explicit-state search,显式状态搜索,0.9,10,"[{'word': '显式状态搜索', 'ratio': 0.9}, {'word': '明确状态搜索', 'ratio': 0.1}]",显式状态搜索,{},[]
1888,1888,exploding gradient,梯度爆炸,1.0,10,"[{'word': '梯度爆炸', 'ratio': 1.0}]",梯度爆炸,{},[]
1889,1889,exploitability,可利用性,1.0,10,"[{'word': '可利用性', 'ratio': 1.0}]",可利用性,{},[]
1890,1890,exploration rate,探索率,1.0,10,"[{'word': '探索率', 'ratio': 1.0}]",探索率,{},[]
1891,1891,exploratory data analysis,探索性数据分析,1.0,9,"[{'word': '探索性数据分析', 'ratio': 1.0}]",探索性数据分析,{},[]
1892,1892,exponential complexity,指数复杂度,0.8888888888888888,9,"[{'word': '指数复杂度', 'ratio': 0.8888888888888888}, {'word': '指数复杂性', 'ratio': 0.1111111111111111}]",指数复杂度,{},[]
1893,1893,exponential decay,指数衰减,1.0,9,"[{'word': '指数衰减', 'ratio': 1.0}]",指数衰减,{},[]
1894,1894,exponential family,指数族,1.0,9,"[{'word': '指数族', 'ratio': 1.0}]",指数族,{},[]
1895,1895,exponential loss,指数损失,1.0,9,"[{'word': '指数损失', 'ratio': 1.0}]",指数损失,{},[]
1896,1896,exponential map,指数映射,1.0,9,"[{'word': '指数映射', 'ratio': 1.0}]",指数映射,{},[]
1897,1897,exponential moving average,指数移动平均,1.0,9,"[{'word': '指数移动平均', 'ratio': 1.0}]",指数移动平均,{},[]
1898,1898,exposure bias,暴露偏差,0.1111111111111111,9,"[{'word': '曝露偏差', 'ratio': 0.5555555555555556}, {'word': '曝光偏差', 'ratio': 0.3333333333333333}, {'word': '暴露偏差', 'ratio': 0.1111111111111111}]",曝露偏差,{},[]
1899,1899,extensive-form game,扩展式博弈,0.0,9,"[{'word': '扩展形式博弈', 'ratio': 0.3333333333333333}, {'word': '广义形式博弈', 'ratio': 0.2222222222222222}, {'word': '展开式博弈', 'ratio': 0.2222222222222222}, {'word': '广泛形式博弈', 'ratio': 0.1111111111111111}, {'word': '广泛形式游戏', 'ratio': 0.1111111111111111}]",扩展形式博弈,"1. Rank: 扩展形式博弈, 广泛形式博弈, 广泛形式游戏, 广义形式博弈, 展开式博弈

2. Explanation: The term ""扩展形式博弈"" (extended form game) is the best fit because it accurately captures the concept of ""extensive-form game"" in the context of game theory. The term ""扩展"" (extended) directly relates to the idea of expanding the normal-form game to include sequential moves and private information, which is a key characteristic of extensive-form games. 

""广泛形式博弈"" (broad form game) and ""广泛形式游戏"" (broad form games) are also reasonable translations, but they do not convey the specific notion of ""extensive"" as effectively as ""扩展"". The term ""广义形式博弈"" (general form game) is less precise, as it suggests a broader category that may not specifically refer to the extensive-form context. Lastly, ""展开式博弈"" (expanded game) is less commonly used in the AI and game theory literature and may lead to confusion, as it does not directly align with the established terminology. Therefore, ""扩展形式博弈"" is the most semantically accurate and contextually appropriate choice.","['extended form game', 'general form game', 'Expanded game', 'extensive form game', 'extensive form games']"
1900,1900,extractive question answering,抽取式问答,0.5,10,"[{'word': '提取式问答', 'ratio': 0.5}, {'word': '抽取式问答', 'ratio': 0.5}]",提取式问答,{},[]
1901,1901,extractive summarization,抽取式摘要化,0.0,10,"[{'word': '提取式摘要', 'ratio': 0.5}, {'word': '抽取式摘要', 'ratio': 0.5}]",提取式摘要,{},[]
1902,1902,extractor,提取器,1.0,18,"[{'word': '提取器', 'ratio': 1.0}]",提取器,{},[]
1903,1903,f-divergence,f-散度,1.0,9,"[{'word': 'f-散度', 'ratio': 1.0}]",f-散度,{},[]
1904,1904,face detection,人脸检测,1.0,9,"[{'word': '人脸检测', 'ratio': 1.0}]",人脸检测,{},[]
1905,1905,face detector,人脸检测器,1.0,9,"[{'word': '人脸检测器', 'ratio': 1.0}]",人脸检测器,{},[]
1906,1906,face recognition,人脸识别,1.0,9,"[{'word': '人脸识别', 'ratio': 1.0}]",人脸识别,{},[]
1907,1907,facial landmark,人脸关键点,0.0,9,"[{'word': '面部特征点', 'ratio': 0.7777777777777778}, {'word': '面部关键点', 'ratio': 0.1111111111111111}, {'word': '人脸特征点 / 面部标志点', 'ratio': 0.1111111111111111}]",面部特征点,{},[]
1908,1908,facial recognition,人脸识别,0.1111111111111111,9,"[{'word': '面部识别', 'ratio': 0.7777777777777778}, {'word': '面部识别 / 人脸识别', 'ratio': 0.1111111111111111}, {'word': '人脸识别', 'ratio': 0.1111111111111111}]",面部识别,{},[]
1909,1909,fact verification,事实验证,0.7777777777777778,9,"[{'word': '事实验证', 'ratio': 0.7777777777777778}, {'word': '事实核查', 'ratio': 0.1111111111111111}, {'word': '事实核实', 'ratio': 0.1111111111111111}]",事实验证,{},[]
1910,1910,factor analysis,因子分析,0.7777777777777778,9,"[{'word': '因子分析', 'ratio': 0.7777777777777778}, {'word': '因素分析', 'ratio': 0.2222222222222222}]",因子分析,{},[]
1911,1911,factor graph,因子图,1.0,10,"[{'word': '因子图', 'ratio': 1.0}]",因子图,{},[]
1912,1912,factor matrix,因子矩阵,1.0,10,"[{'word': '因子矩阵', 'ratio': 1.0}]",因子矩阵,{},[]
1913,1913,factor of variation,变异因素,0.1,10,"[{'word': '变化因子', 'ratio': 0.5}, {'word': '变异因子', 'ratio': 0.4}, {'word': '变异因素', 'ratio': 0.1}]",变化因子,{},[]
1914,1914,factorization,因子分解,0.4,10,"[{'word': '分解', 'ratio': 0.6}, {'word': '因子分解', 'ratio': 0.4}]",分解,{},[]
1915,1915,factorization algorithm,因子分解算法,0.4,10,"[{'word': '分解算法', 'ratio': 0.6}, {'word': '因子分解算法', 'ratio': 0.4}]",分解算法,{},[]
1916,1916,factorization method,因子分解法,0.0,9,"[{'word': '因子分解方法', 'ratio': 0.4444444444444444}, {'word': '分解方法', 'ratio': 0.4444444444444444}, {'word': '因式分解方法', 'ratio': 0.1111111111111111}]",因子分解方法,"1. Rank: 因子分解方法, 因式分解方法, 分解方法

2. Explanation: The term ""因子分解方法"" (factorization method) is the best fit for the AI domain-specific usage because it accurately captures the concept of factorization in the context of matrix or tensor decomposition, which is essential in machine learning and data analysis. The back translation ""factoring method"" aligns closely with the original English term, maintaining the intended meaning. 

""因式分解方法"" (factoring method) is less suitable because it is more commonly associated with algebraic factorization rather than the broader context of matrix or tensor decomposition used in AI. 

""分解方法"" (decomposition method) is too general and does not specifically convey the idea of factorization, which is a more specialized term in this context. Therefore, ""因子分解方法"" is the most semantically accurate and contextually appropriate choice for the AI terminology.","['factoring method', 'Decomposition method', 'factoring method']"
1917,1917,failure probability,失效概率,0.0,9,"[{'word': '失败概率', 'ratio': 1.0}]",失败概率,{},[]
1918,1918,fairness criterion,公平性准则,0.3333333333333333,9,"[{'word': '公平标准', 'ratio': 0.4444444444444444}, {'word': '公平性准则', 'ratio': 0.3333333333333333}, {'word': '公平性标准', 'ratio': 0.2222222222222222}]",公平性准则,"1. Rank: 公平性准则, 公平性标准, 公平标准

2. Explanation: The term ""公平性准则"" (fairness criterion) is the best fit for the AI domain because it accurately captures the concept of a criterion specifically related to fairness in decision-making processes, particularly in algorithms. The use of ""准则"" (criterion) emphasizes a standard or principle that guides the evaluation of fairness, which aligns well with the context of machine learning and algorithmic fairness. 

In contrast, ""公平性标准"" (fairness standard) is also a close candidate but leans more towards a general standard rather than a guiding principle, which may not fully convey the specific evaluative aspect implied by ""criterion."" Lastly, ""公平标准"" (fair standard) is the least suitable as it lacks the nuance of ""性"" (related to fairness) and ""准则"" (criterion), making it less precise in the context of formal evaluations in AI. Thus, ""公平性准则"" is the most semantically accurate and contextually appropriate choice.","['fair standard', 'fairness criterion', 'fairness standard']"
1919,1919,fairness loss,公平性损失,0.5555555555555556,9,"[{'word': '公平性损失', 'ratio': 0.5555555555555556}, {'word': '公平损失', 'ratio': 0.4444444444444444}]",公平性损失,{},[]
1920,1920,fairness notion,公平概念,0.4444444444444444,9,"[{'word': '公平性概念', 'ratio': 0.5555555555555556}, {'word': '公平概念', 'ratio': 0.4444444444444444}]",公平性概念,{},[]
1921,1921,faithfulness score,忠实度评分,0.2222222222222222,9,"[{'word': '真实性评分', 'ratio': 0.3333333333333333}, {'word': '可信度评分', 'ratio': 0.2222222222222222}, {'word': '忠实度评分', 'ratio': 0.2222222222222222}, {'word': '真实性得分', 'ratio': 0.1111111111111111}, {'word': '忠实度得分', 'ratio': 0.1111111111111111}]","""可信度评分""","1. Rank: ""可信度评分"", ""忠实度评分"", ""真实性评分"", ""忠实度得分"", ""真实性得分""

2. Explanation: The term ""可信度评分"" (credibility score) is the best fit for ""faithfulness score"" in the AI domain because it captures the essence of evaluating the reliability and trustworthiness of a summary, which aligns closely with the concept of faithfulness in summarization tasks. In AI, particularly in natural language processing, ""faithfulness"" refers to how accurately a generated summary reflects the original content without introducing distortions or inaccuracies. 

""忠实度评分"" (fidelity score) is also a strong candidate, as ""忠实"" directly translates to ""faithful"" or ""loyal,"" which can imply a similar meaning in the context of summarization. However, ""可信度"" emphasizes the aspect of credibility, which is crucial in evaluating AI-generated content.

The other candidates, ""真实性评分"" (authenticity score), ""忠实度得分"" (loyalty score), and ""真实性得分"" (authenticity score), do not align as closely with the specific context of evaluating summaries in AI. ""真实性"" (authenticity) suggests a focus on the original nature of the content rather than its reliability or faithfulness to the source, while ""忠实度得分"" and ""忠实度评分"" could imply loyalty rather than fidelity in the context of summarization. Thus, ""可信度评分"" is the most contextually appropriate choice.","['authenticity score', 'credibility score', 'Fidelity score', 'authenticity score', 'loyalty score']"
1922,1922,false negative,漏检,0.0,9,"[{'word': '假阴性', 'ratio': 1.0}]",假阴性,{},[]
1923,1923,false negative rate,假阴性率,1.0,9,"[{'word': '假阴性率', 'ratio': 1.0}]",假阴性率,{},[]
1924,1924,false positive rate,虚报率,0.0,9,"[{'word': '假阳性率', 'ratio': 1.0}]",假阳性率,{},[]
1925,1925,fanout,扇出度,0.0,9,"[{'word': '扇出', 'ratio': 0.3333333333333333}, {'word': '分支因子', 'ratio': 0.3333333333333333}, {'word': '扩展因子', 'ratio': 0.2222222222222222}, {'word': '扩展度', 'ratio': 0.1111111111111111}]",扇出,"1. Rank: 扇出, 分支因子, 扩展因子, 扩展度

2. Explanation: The term ""扇出"" (fan out) is the best fit for the AI domain-specific usage because it directly corresponds to the English term and accurately conveys the concept of how many child nodes a node in a tree structure can have. In the context of beam search, ""扇出"" effectively captures the idea of branching out from a node, which is crucial for understanding the tree's structure and the algorithm's performance. 

""分支因子"" (branching factor) is also a relevant term, but it is more commonly used in general graph theory rather than specifically in AI contexts. While it is semantically accurate, it may not be as widely recognized in the specific context of beam search.

""扩展因子"" (expansion factor) and ""扩展度"" (expansion) are less suitable as they imply a broader concept of expansion rather than the specific branching nature of nodes in a tree. These terms could lead to confusion in the context of AI algorithms, where precise terminology is essential for clarity and understanding. Thus, ""扇出"" is the most contextually appropriate choice.","['fan out', 'branching factor', 'expansion factor', 'Expansion']"
1926,1926,fc layer,全连接层,0.875,8,"[{'word': '全连接层', 'ratio': 0.875}, {'word': 'FC层', 'ratio': 0.125}]",全连接层,{},[]
1927,1927,feasible set,可行集,0.5,8,"[{'word': '可行集合', 'ratio': 0.5}, {'word': '可行集', 'ratio': 0.5}]",可行集合,{},[]
1928,1928,feature,特征,1.0,8,"[{'word': '特征', 'ratio': 1.0}]",特征,{},[]
1929,1929,feature channel,特征通道,0.875,8,"[{'word': '特征通道', 'ratio': 0.875}, {'word': '特色频道', 'ratio': 0.125}]",特征通道,{},[]
1930,1930,feature correspondence,特征对应,1.0,8,"[{'word': '特征对应', 'ratio': 1.0}]",特征对应,{},[]
1931,1931,feature count,特征计数,0.9,10,"[{'word': '特征计数', 'ratio': 0.9}, {'word': '八卦算法', 'ratio': 0.1}]",特征计数,{},[]
1932,1932,feature descriptor,特征描述符,0.8,10,"[{'word': '特征描述符', 'ratio': 0.8}, {'word': '梯度', 'ratio': 0.1}, {'word': '特征描述子', 'ratio': 0.1}]",特征描述符,{},[]
1933,1933,feature detection,特征检测,0.9,10,"[{'word': '特征检测', 'ratio': 0.9}, {'word': '梯度累积', 'ratio': 0.1}]",特征检测,{},[]
1934,1934,feature detector,特征检测器,0.9,10,"[{'word': '特征检测器', 'ratio': 0.9}, {'word': '梯度累积步', 'ratio': 0.1}]",特征检测器,{},[]
1935,1935,feature dimension,特征维度,0.9,10,"[{'word': '特征维度', 'ratio': 0.9}, {'word': '梯度上升', 'ratio': 0.1}]",特征维度,{},[]
1936,1936,feature dimensionality,特征维度,0.9,10,"[{'word': '特征维度', 'ratio': 0.9}, {'word': '特征维度C', 'ratio': 0.1}]",特征维度,{},[]
1937,1937,feature embedding,特征嵌入,1.0,10,"[{'word': '特征嵌入', 'ratio': 1.0}]",特征嵌入,{},[]
1938,1938,feature encoder,特征编码器,1.0,10,"[{'word': '特征编码器', 'ratio': 1.0}]",特征编码器,{},[]
1939,1939,feature engineering,特征工程,1.0,10,"[{'word': '特征工程', 'ratio': 1.0}]",特征工程,{},[]
1940,1940,feature extraction,特征提取,1.0,10,"[{'word': '特征提取', 'ratio': 1.0}]",特征提取,{},[]
1941,1941,feature extractor,特征提取器,1.0,10,"[{'word': '特征提取器', 'ratio': 1.0}]",特征提取器,{},[]
1942,1942,feature function,特征函数,1.0,10,"[{'word': '特征函数', 'ratio': 1.0}]",特征函数,{},[]
1943,1943,feature hashing,特征哈希,1.0,10,"[{'word': '特征哈希', 'ratio': 1.0}]",特征哈希,{},[]
1944,1944,feature hierarchy,特征层次结构,0.9,10,"[{'word': '特征层次结构', 'ratio': 0.9}, {'word': '特征图特征层次结构', 'ratio': 0.1}]",特征层次结构,{},[]
1945,1945,feature map,特征图,1.0,10,"[{'word': '特征图', 'ratio': 1.0}]",特征图,{},[]
1946,1946,feature mapping function,特征映射函数,1.0,10,"[{'word': '特征映射函数', 'ratio': 1.0}]",特征映射函数,{},[]
1947,1947,feature matching,特征匹配,0.9,10,"[{'word': '特征匹配', 'ratio': 0.9}, {'word': '特征映射函数', 'ratio': 0.1}]",特征匹配,{},[]
1948,1948,feature matrix,特征矩阵,0.9,10,"[{'word': '特征矩阵', 'ratio': 0.9}, {'word': '特征匹配', 'ratio': 0.1}]",特征矩阵,{},[]
1949,1949,feature model,特征模型,0.9,10,"[{'word': '特征模型', 'ratio': 0.9}, {'word': '特征矩阵', 'ratio': 0.1}]",特征模型,{},[]
1950,1950,feature normalization,特征归一化,0.9,10,"[{'word': '特征归一化', 'ratio': 0.9}, {'word': '特征模型', 'ratio': 0.1}]",特征归一化,{},[]
1951,1951,feature point,特征点,1.0,10,"[{'word': '特征点', 'ratio': 1.0}]",特征点,{},[]
1952,1952,feature pyramid,特征金字塔,1.0,10,"[{'word': '特征金字塔', 'ratio': 1.0}]",特征金字塔,{},[]
1953,1953,feature representation,特征表示,1.0,10,"[{'word': '特征表示', 'ratio': 1.0}]",特征表示,{},[]
1954,1954,feature representation learning,特征表示学习,1.0,10,"[{'word': '特征表示学习', 'ratio': 1.0}]",特征表示学习,{},[]
1955,1955,feature selection,特征选择,1.0,10,"[{'word': '特征选择', 'ratio': 1.0}]",特征选择,{},[]
1956,1956,feature selector,特征选择器,1.0,10,"[{'word': '特征选择器', 'ratio': 1.0}]",特征选择器,{},[]
1957,1957,feature set,特征集,1.0,10,"[{'word': '特征集', 'ratio': 1.0}]",特征集,{},[]
1958,1958,feature space,特征空间,1.0,10,"[{'word': '特征空间', 'ratio': 1.0}]",特征空间,{},[]
1959,1959,feature template,特征模板,1.0,10,"[{'word': '特征模板', 'ratio': 1.0}]",特征模板,{},[]
1960,1960,feature vector,特征向量,1.0,10,"[{'word': '特征向量', 'ratio': 1.0}]",特征向量,{},[]
1961,1961,feature weight,特征权重,1.0,10,"[{'word': '特征权重', 'ratio': 1.0}]",特征权重,{},[]
1962,1962,featurization,特征提取,0.0,10,"[{'word': '特征化', 'ratio': 1.0}]",特征化,{},[]
1963,1963,featurized representation,特征化表示,1.0,10,"[{'word': '特征化表示', 'ratio': 1.0}]",特征化表示,{},[]
1964,1964,feed forward,前馈,0.9,10,"[{'word': '前馈', 'ratio': 0.9}, {'word': '第二个模型的均值函数是一个前馈ReLU网络，具有两个隐藏层，每个隐藏层有50个单元。', 'ratio': 0.1}]",前馈,{},[]
1965,1965,feed forward network,前馈网络,1.0,10,"[{'word': '前馈网络', 'ratio': 1.0}]",前馈网络,{},[]
1966,1966,feed forward neural network,前馈神经网络,1.0,10,"[{'word': '前馈神经网络', 'ratio': 1.0}]",前馈神经网络,{},[]
1967,1967,feed-forward layer,前馈层,1.0,10,"[{'word': '前馈层', 'ratio': 1.0}]",前馈层,{},[]
1968,1968,feedback loop,反馈循环,0.6,10,"[{'word': '反馈循环', 'ratio': 0.6}, {'word': '反馈回路', 'ratio': 0.3}, {'word': '反馈环路', 'ratio': 0.1}]",反馈循环,{},[]
1969,1969,few shot learning,少样本学习,0.9,10,"[{'word': '少样本学习', 'ratio': 0.9}, {'word': '少量样本学习', 'ratio': 0.1}]",少样本学习,{},[]
1970,1970,few-shot classification,少样本分类,0.6666666666666666,9,"[{'word': '少样本分类', 'ratio': 0.6666666666666666}, {'word': '小样本分类', 'ratio': 0.3333333333333333}]",少样本分类,{},[]
1971,1971,few-shot example,少样本示例,0.6666666666666666,9,"[{'word': '少样本示例', 'ratio': 0.6666666666666666}, {'word': '小样本示例', 'ratio': 0.3333333333333333}]",少样本示例,{},[]
1972,1972,few-shot fine-tuning,少样本微调,0.5555555555555556,9,"[{'word': '少样本微调', 'ratio': 0.5555555555555556}, {'word': '小样本微调', 'ratio': 0.3333333333333333}, {'word': '', 'ratio': 0.1111111111111111}]",少样本微调,{},[]
1973,1973,few-shot in-context learning,少样本上下文学习,0.6666666666666666,9,"[{'word': '少样本上下文学习', 'ratio': 0.6666666666666666}, {'word': '小样本上下文学习', 'ratio': 0.3333333333333333}]",少样本上下文学习,{},[]
1974,1974,few-shot prompting,少样本提示,0.6666666666666666,9,"[{'word': '少样本提示', 'ratio': 0.6666666666666666}, {'word': '小样本提示', 'ratio': 0.3333333333333333}]",少样本提示,{},[]
1975,1975,few-shot setting,小样本设置,0.2,10,"[{'word': '少样本设置', 'ratio': 0.6}, {'word': '小样本设置', 'ratio': 0.2}, {'word': 'few-shot 设置', 'ratio': 0.1}, {'word': '少量样本设置', 'ratio': 0.1}]",少样本设置,{},[]
1976,1976,filter bank,滤波器组,1.0,10,"[{'word': '滤波器组', 'ratio': 1.0}]",滤波器组,{},[]
1977,1977,filter weight,滤波器权重,1.0,10,"[{'word': '滤波器权重', 'ratio': 1.0}]",滤波器权重,{},[]
1978,1978,fine-grained sentiment classification,细粒度情感分类,0.8,10,"[{'word': '细粒度情感分类', 'ratio': 0.8}, {'word': '精细化情感分类', 'ratio': 0.2}]",细粒度情感分类,{},[]
1979,1979,fine-tune,微调,1.0,10,"[{'word': '微调', 'ratio': 1.0}]",微调,{},[]
1980,1980,fine-tuned model,微调模型,1.0,6,"[{'word': '微调模型', 'ratio': 1.0}]",微调模型,{},[]
1981,1981,finite horizon,有限时间跨度,0.0,6,"[{'word': '有限时间范围', 'ratio': 0.6666666666666666}, {'word': '有限时域', 'ratio': 0.3333333333333333}]",有限时间范围,{},[]
1982,1982,finite-state automata,有限状态自动机,1.0,6,"[{'word': '有限状态自动机', 'ratio': 1.0}]",有限状态自动机,{},[]
1983,1983,first order method,一阶方法,1.0,6,"[{'word': '一阶方法', 'ratio': 1.0}]",一阶方法,{},[]
1984,1984,first-order,一阶,0.9,10,"[{'word': '一阶', 'ratio': 0.9}, {'word': '一阶的', 'ratio': 0.1}]",一阶,{},[]
1985,1985,first-order language,一阶语言,1.0,10,"[{'word': '一阶语言', 'ratio': 1.0}]",一阶语言,{},[]
1986,1986,first-order logic,一阶逻辑,1.0,10,"[{'word': '一阶逻辑', 'ratio': 1.0}]",一阶逻辑,{},[]
1987,1987,first-order model,一阶模型,1.0,10,"[{'word': '一阶模型', 'ratio': 1.0}]",一阶模型,{},[]
1988,1988,first-order parsing,一阶解析,0.9,10,"[{'word': '一阶解析', 'ratio': 0.9}, {'word': '一阶解析 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.1}]",一阶解析,{},[]
1989,1989,fitness function,适应度函数,1.0,10,"[{'word': '适应度函数', 'ratio': 1.0}]",适应度函数,{},[]
1990,1990,five-fold cross-validation,五折交叉验证,1.0,10,"[{'word': '五折交叉验证', 'ratio': 1.0}]",五折交叉验证,{},[]
1991,1991,fixed point,不动点,0.8,10,"[{'word': '不动点', 'ratio': 0.8}, {'word': '固定点', 'ratio': 0.2}]",不动点,{},[]
1992,1992,fixed-parameter tractable,固定参数可解,0.7,10,"[{'word': '固定参数可解', 'ratio': 0.7}, {'word': '固定参数可处理', 'ratio': 0.3}]",固定参数可解,{},[]
1993,1993,fixed-point iteration,定点迭代,0.0,10,"[{'word': '不动点迭代', 'ratio': 0.9}, {'word': '固定点迭代', 'ratio': 0.1}]",不动点迭代,{},[]
1994,1994,fixpoint,不动点,0.8888888888888888,9,"[{'word': '不动点', 'ratio': 0.8888888888888888}, {'word': '固定点', 'ratio': 0.1111111111111111}]",不动点,{},[]
1995,1995,float16,半精度浮点数,0.0,9,"[{'word': 'float16', 'ratio': 0.5555555555555556}, {'word': '16位浮点数', 'ratio': 0.3333333333333333}, {'word': '浮点16', 'ratio': 0.1111111111111111}]",float16,{},[]
1996,1996,float32,浮点数32,0.0,9,"[{'word': 'float32', 'ratio': 0.5555555555555556}, {'word': '32位浮点数', 'ratio': 0.3333333333333333}, {'word': '浮点32', 'ratio': 0.1111111111111111}]",float32,{},[]
1997,1997,flow field,流场,1.0,9,"[{'word': '流场', 'ratio': 1.0}]",流场,{},[]
1998,1998,flow model,流模型,0.5555555555555556,9,"[{'word': '流模型', 'ratio': 0.5555555555555556}, {'word': '流动模型', 'ratio': 0.4444444444444444}]",流模型,{},[]
1999,1999,forall,对于所有,0.3333333333333333,9,"[{'word': '全称量词', 'ratio': 0.3333333333333333}, {'word': '对于所有', 'ratio': 0.3333333333333333}, {'word': '∀', 'ratio': 0.1111111111111111}, {'word': 'forall', 'ratio': 0.1111111111111111}, {'word': '对所有', 'ratio': 0.1111111111111111}]","""全称量词""","1. Rank: ""全称量词"", ""对于所有"", ""对所有"", ""∀"", ""forall""

2. Explanation: The term ""全称量词"" (universal quantifier) is the best fit for the AI domain-specific usage because it accurately captures the mathematical and logical context of the term ""forall."" In formal logic and mathematics, ""全称量词"" specifically refers to the quantifier that expresses that a property holds for all elements in a given set, which aligns perfectly with the context provided. 

The other candidates, such as ""对于所有"" (for all) and ""对所有"" (to all), while they convey a similar meaning, lack the precision and established usage in formal logic that ""全称量词"" provides. The symbol ""∀"" is also a valid representation but is less descriptive in terms of semantic clarity for those unfamiliar with the notation. Lastly, ""forall"" (for all) is a direct transliteration and does not provide the necessary contextual understanding that ""全称量词"" does in the AI and mathematical domains. Thus, ""全称量词"" is the most semantically accurate and contextually appropriate choice.","['universal quantifier', 'for all', '∀', 'for all', 'to all']"
2000,2000,forecasting,预测,1.0,9,"[{'word': '预测', 'ratio': 1.0}]",预测,{},[]
2001,2001,foreground segmentation,前景分割,0.8888888888888888,9,"[{'word': '前景分割', 'ratio': 0.8888888888888888}, {'word': '预测', 'ratio': 0.1111111111111111}]",前景分割,{},[]
2002,2002,forget gate,遗忘门,0.7777777777777778,9,"[{'word': '遗忘门', 'ratio': 0.7777777777777778}, {'word': '忘记门', 'ratio': 0.2222222222222222}]",遗忘门,{},[]
2003,2003,forward algorithm,前向算法,1.0,9,"[{'word': '前向算法', 'ratio': 1.0}]",前向算法,{},[]
2004,2004,forward model,前向模型,0.8888888888888888,9,"[{'word': '前向模型', 'ratio': 0.8888888888888888}, {'word': '正向模型', 'ratio': 0.1111111111111111}]",前向模型,{},[]
2005,2005,forward pass,前向传播,0.6666666666666666,9,"[{'word': '前向传播', 'ratio': 0.6666666666666666}, {'word': '前向传递', 'ratio': 0.2222222222222222}, {'word': '正向传播', 'ratio': 0.1111111111111111}]",前向传播,{},[]
2006,2006,forward process,前向过程,0.8888888888888888,9,"[{'word': '前向过程', 'ratio': 0.8888888888888888}, {'word': '正向过程', 'ratio': 0.1111111111111111}]",前向过程,{},[]
2007,2007,forward propagation,正向传播,0.1111111111111111,9,"[{'word': '前向传播', 'ratio': 0.8888888888888888}, {'word': '正向传播', 'ratio': 0.1111111111111111}]",前向传播,{},[]
2008,2008,forward-backward algorithm,前向-后向算法,0.6666666666666666,9,"[{'word': '前向-后向算法', 'ratio': 0.6666666666666666}, {'word': '前向后向算法', 'ratio': 0.3333333333333333}]",前向-后向算法,{},[]
2009,2009,foundation model,基础模型,1.0,9,"[{'word': '基础模型', 'ratio': 1.0}]",基础模型,{},[]
2010,2010,fp,fps (每秒帧数),0.0,9,"[{'word': '帧率', 'ratio': 0.5555555555555556}, {'word': '帧每秒', 'ratio': 0.4444444444444444}]",帧率,{},[]
2011,2011,fp16,FP16,0.0,9,"[{'word': '16位浮点数', 'ratio': 0.5555555555555556}, {'word': 'fp16', 'ratio': 0.2222222222222222}, {'word': '半精度', 'ratio': 0.2222222222222222}]",16位浮点数,{},[]
2012,2012,fp32,FP32,0.0,9,"[{'word': '32位浮点数', 'ratio': 0.5555555555555556}, {'word': 'fp32', 'ratio': 0.2222222222222222}, {'word': '单精度', 'ratio': 0.2222222222222222}]",32位浮点数,{},[]
2013,2013,fractional program,分式规划,0.0,9,"[{'word': '分数规划', 'ratio': 1.0}]",分数规划,{},[]
2014,2014,frame,帧,1.0,10,"[{'word': '帧', 'ratio': 1.0}]",帧,{},[]
2015,2015,free variable,自由变量,1.0,10,"[{'word': '自由变量', 'ratio': 1.0}]",自由变量,{},[]
2016,2016,frequency penalty,频率惩罚,1.0,10,"[{'word': '频率惩罚', 'ratio': 1.0}]",频率惩罚,{},[]
2017,2017,frequency vector,频率向量,1.0,10,"[{'word': '频率向量', 'ratio': 1.0}]",频率向量,{},[]
2018,2018,frequent closed itemset,频繁闭合项集,0.3,10,"[{'word': '高频闭合项集', 'ratio': 0.4}, {'word': '频繁闭合项集', 'ratio': 0.3}, {'word': '频繁闭项集', 'ratio': 0.3}]",频繁闭合项集,"1. Rank: 频繁闭合项集, 频繁闭项集, 高频闭合项集

2. Explanation: The term ""频繁闭合项集"" (frequent closed itemsets) is the best fit because it accurately captures the semantic meaning of the original English term. The word ""频繁"" (frequent) directly corresponds to ""frequent,"" and ""闭合项集"" (closed itemsets) accurately translates ""closed itemset."" This term is widely used in the AI and data mining domains, particularly in association rule learning and pattern mining, making it contextually appropriate.

The second candidate, ""频繁闭项集,"" while also correct, omits the word ""合"" (closed), which can lead to a loss of specificity in the context of itemsets. The term ""高频闭合项集"" (high frequency closed itemset) introduces the word ""高"" (high), which is not present in the original term and could mislead the reader regarding the nature of the itemsets being discussed. Therefore, ""频繁闭合项集"" is the most semantically accurate and contextually fitting translation.","['high frequency closed itemset', 'frequent closed itemsets', 'frequent closed itemsets']"
2019,2019,frequent item set,频繁项集,0.75,8,"[{'word': '频繁项集', 'ratio': 0.75}, {'word': '高频项集', 'ratio': 0.125}, {'word': 'ChatGPT', 'ratio': 0.125}]",频繁项集,{},[]
2020,2020,frequent pattern,频繁模式,0.875,8,"[{'word': '频繁模式', 'ratio': 0.875}, {'word': '高频模式', 'ratio': 0.125}]",频繁模式,{},[]
2021,2021,frequent pattern mining,频繁模式挖掘,0.875,8,"[{'word': '频繁模式挖掘', 'ratio': 0.875}, {'word': '高频模式挖掘', 'ratio': 0.125}]",频繁模式挖掘,{},[]
2022,2022,fully connected graph,全连接图,0.375,8,"[{'word': '完全连接图', 'ratio': 0.625}, {'word': '全连接图', 'ratio': 0.375}]",完全连接图,{},[]
2023,2023,fully connected layer,全连接层,0.625,8,"[{'word': '全连接层', 'ratio': 0.625}, {'word': '完全连接层', 'ratio': 0.375}]",全连接层,{},[]
2024,2024,fully connected network,全连接网络,1.0,8,"[{'word': '全连接网络', 'ratio': 1.0}]",全连接网络,{},[]
2025,2025,fully connected neural network,全连接神经网络,1.0,8,"[{'word': '全连接神经网络', 'ratio': 1.0}]",全连接神经网络,{},[]
2026,2026,fully convolutional network,全卷积网络,1.0,8,"[{'word': '全卷积网络', 'ratio': 1.0}]",全卷积网络,{},[]
2027,2027,fully convolutional neural network,全卷积神经网络,1.0,8,"[{'word': '全卷积神经网络', 'ratio': 1.0}]",全卷积神经网络,{},[]
2028,2028,fully-supervised model,全监督模型,0.625,8,"[{'word': '全监督模型', 'ratio': 0.625}, {'word': '完全监督模型', 'ratio': 0.375}]",全监督模型,{},[]
2029,2029,function approximation,函数逼近,1.0,9,"[{'word': '函数逼近', 'ratio': 1.0}]",函数逼近,{},[]
2030,2030,function approximator,函数逼近器,0.8888888888888888,9,"[{'word': '函数逼近器', 'ratio': 0.8888888888888888}, {'word': '函数近似器', 'ratio': 0.1111111111111111}]",函数逼近器,{},[]
2031,2031,function class,函数类,1.0,9,"[{'word': '函数类', 'ratio': 1.0}]",函数类,{},[]
2032,2032,function space,函数空间,1.0,9,"[{'word': '函数空间', 'ratio': 1.0}]",函数空间,{},[]
2033,2033,functionality assertion,功能性断言,0.7777777777777778,9,"[{'word': '功能性断言', 'ratio': 0.7777777777777778}, {'word': '功能性声明', 'ratio': 0.2222222222222222}]",功能性断言,{},[]
2034,2034,fundamental matrix,基础矩阵,0.3333333333333333,9,"[{'word': '基本矩阵', 'ratio': 0.6666666666666666}, {'word': '基础矩阵', 'ratio': 0.3333333333333333}]",基本矩阵,{},[]
2035,2035,fusion module,融合模块,0.8888888888888888,9,"[{'word': '融合模块', 'ratio': 0.8888888888888888}, {'word': '基础矩阵', 'ratio': 0.1111111111111111}]",融合模块,{},[]
2036,2036,fuzzy matching,模糊匹配,1.0,9,"[{'word': '模糊匹配', 'ratio': 1.0}]",模糊匹配,{},[]
2037,2037,g-value,g值,0.6666666666666666,9,"[{'word': 'g值', 'ratio': 0.6666666666666666}, {'word': 'g 值', 'ratio': 0.3333333333333333}]",g值,{},[]
2038,2038,game tree,游戏树,0.0,9,"[{'word': '博弈树', 'ratio': 1.0}]",博弈树,{},[]
2039,2039,game-theoretic analysis,博弈论分析,0.8888888888888888,9,"[{'word': '博弈论分析', 'ratio': 0.8888888888888888}, {'word': '博弈理论分析', 'ratio': 0.1111111111111111}]",博弈论分析,{},[]
2040,2040,gate,门控机制,0.0,9,"[{'word': '门', 'ratio': 0.6666666666666666}, {'word': '门控', 'ratio': 0.3333333333333333}]",门,{},[]
2041,2041,gating function,门控函数,1.0,9,"[{'word': '门控函数', 'ratio': 1.0}]",门控函数,{},[]
2042,2042,generalisation,泛化能力,0.3333333333333333,9,"[{'word': '泛化', 'ratio': 0.6666666666666666}, {'word': '泛化能力', 'ratio': 0.3333333333333333}]",泛化,{},[]
2043,2043,generalization,泛化,0.9444444444444444,18,"[{'word': '泛化', 'ratio': 0.9444444444444444}, {'word': '泛化能力', 'ratio': 0.05555555555555555}]",泛化,{},[]
2044,2044,generalization ability,泛化能力,1.0,9,"[{'word': '泛化能力', 'ratio': 1.0}]",泛化能力,{},[]
2045,2045,generalization bound,泛化界限,0.7777777777777778,9,"[{'word': '泛化界限', 'ratio': 0.7777777777777778}, {'word': '泛化界', 'ratio': 0.2222222222222222}]",泛化界限,{},[]
2046,2046,generalization error,泛化误差,1.0,9,"[{'word': '泛化误差', 'ratio': 1.0}]",泛化误差,{},[]
2047,2047,generalization gap,泛化差距,1.0,9,"[{'word': '泛化差距', 'ratio': 1.0}]",泛化差距,{},[]
2048,2048,generalization guarantee,泛化保证,1.0,8,"[{'word': '泛化保证', 'ratio': 1.0}]",泛化保证,{},[]
2049,2049,generalization performance,泛化性能,1.0,8,"[{'word': '泛化性能', 'ratio': 1.0}]",泛化性能,{},[]
2050,2050,generalized eigenvector,广义特征向量,1.0,8,"[{'word': '广义特征向量', 'ratio': 1.0}]",广义特征向量,{},[]
2051,2051,generalized linear mixed model,广义线性混合模型,1.0,8,"[{'word': '广义线性混合模型', 'ratio': 1.0}]",广义线性混合模型,{},[]
2052,2052,generalized linear model,广义线性模型 (GLM),0.0,9,"[{'word': '广义线性模型', 'ratio': 1.0}]",广义线性模型,{},[]
2053,2053,generation model,生成模型,1.0,9,"[{'word': '生成模型', 'ratio': 1.0}]",生成模型,{},[]
2054,2054,generative,生成的,0.6,10,"[{'word': '生成的', 'ratio': 0.6}, {'word': '生成式', 'ratio': 0.4}]",生成的,{},[]
2055,2055,generative adversarial network,生成对抗网络,1.0,10,"[{'word': '生成对抗网络', 'ratio': 1.0}]",生成对抗网络,{},[]
2056,2056,generative approach,生成式方法,0.4,10,"[{'word': '生成方法', 'ratio': 0.6}, {'word': '生成式方法', 'ratio': 0.4}]",生成方法,{},[]
2057,2057,generative network,生成网络,1.0,10,"[{'word': '生成网络', 'ratio': 1.0}]",生成网络,{},[]
2058,2058,generative parser,生成式解析器,0.4,10,"[{'word': '生成解析器', 'ratio': 0.6}, {'word': '生成式解析器', 'ratio': 0.4}]",生成解析器,{},[]
2059,2059,generative pre-training,生成式预训练,0.75,8,"[{'word': '生成式预训练', 'ratio': 0.75}, {'word': '生成预训练', 'ratio': 0.25}]",生成式预训练,{},[]
2060,2060,generative probabilistic model,生成概率模型,0.375,8,"[{'word': '生成式概率模型', 'ratio': 0.625}, {'word': '生成概率模型', 'ratio': 0.375}]",生成式概率模型,{},[]
2061,2061,generative process,生成过程,1.0,8,"[{'word': '生成过程', 'ratio': 1.0}]",生成过程,{},[]
2062,2062,generator architecture,生成器架构,1.0,8,"[{'word': '生成器架构', 'ratio': 1.0}]",生成器架构,{},[]
2063,2063,generator network,生成器网络,1.0,8,"[{'word': '生成器网络', 'ratio': 1.0}]",生成器网络,{},[]
2064,2064,geodesic,测地线,1.0,6,"[{'word': '测地线', 'ratio': 1.0}]",测地线,{},[]
2065,2065,geodesic distance,测地距离,1.0,6,"[{'word': '测地距离', 'ratio': 1.0}]",测地距离,{},[]
2066,2066,geometric consistency,几何一致性,1.0,6,"[{'word': '几何一致性', 'ratio': 1.0}]",几何一致性,{},[]
2067,2067,geometric distribution,几何分布,1.0,6,"[{'word': '几何分布', 'ratio': 1.0}]",几何分布,{},[]
2068,2068,geometric invariant,几何不变量,0.6666666666666666,6,"[{'word': '几何不变量', 'ratio': 0.6666666666666666}, {'word': '几何不变性', 'ratio': 0.3333333333333333}]",几何不变量,{},[]
2069,2069,geometric transformation,几何变换,1.0,10,"[{'word': '几何变换', 'ratio': 1.0}]",几何变换,{},[]
2070,2070,geometry processing,几何处理,1.0,10,"[{'word': '几何处理', 'ratio': 1.0}]",几何处理,{},[]
2071,2071,gibb distribution,吉布斯分布,1.0,10,"[{'word': '吉布斯分布', 'ratio': 1.0}]",吉布斯分布,{},[]
2072,2072,gist descriptor,概要描述符,0.2,10,"[{'word': '要点描述符', 'ratio': 0.3}, {'word': '概要描述符', 'ratio': 0.2}, {'word': '大意描述符', 'ratio': 0.1}, {'word': '主旨描述符', 'ratio': 0.1}, {'word': '概况描述符', 'ratio': 0.1}, {'word': '主题描述符', 'ratio': 0.1}, {'word': '要旨描述符', 'ratio': 0.1}]",要点描述符,"1. Rank: 要点描述符, 要旨描述符, 概要描述符, 大意描述符, 主旨描述符, 主题描述符, 概况描述符

2. Explanation: The term ""要点描述符"" (gist descriptor) is the best fit because it accurately captures the essence of the English term ""gist"" in the context of AI and computer vision. ""要点"" translates to ""key points"" or ""essence,"" which aligns well with the concept of a ""gist"" that summarizes the main features of a scene. This term maintains semantic accuracy and is contextually appropriate for describing a feature that identifies significant structures in visual data.

In contrast, ""要旨描述符"" (gist descriptor) is also a strong candidate but leans more towards ""main point"" or ""essence,"" which may not convey the same level of specificity as ""要点."" The other candidates, such as ""概要描述符"" (summary descriptor) and ""大意描述符"" (general descriptor), while related, do not encapsulate the specific AI context of identifying key features in a scene as effectively as ""要点描述符."" The remaining terms, like ""主旨描述符"" (subject descriptor) and ""主题描述符"" (topic descriptor), focus more on thematic elements rather than the visual and structural aspects that ""gist descriptor"" implies in AI applications. Thus, ""要点描述符"" is the most contextually relevant and semantically accurate choice.","['gist descriptor', 'summary descriptor', 'General descriptor', 'subject descriptor', 'profile descriptor', 'Topic descriptor', 'gist descriptor']"
2073,2073,global average pooling,全局平均池化,1.0,10,"[{'word': '全局平均池化', 'ratio': 1.0}]",全局平均池化,{},[]
2074,2074,global average pooling layer,全局平均池化层,1.0,9,"[{'word': '全局平均池化层', 'ratio': 1.0}]",全局平均池化层,{},[]
2075,2075,global coordinate frame,全局坐标系,0.5555555555555556,9,"[{'word': '全局坐标系', 'ratio': 0.5555555555555556}, {'word': '全球坐标系', 'ratio': 0.3333333333333333}, {'word': '全局坐标框架', 'ratio': 0.1111111111111111}]",全局坐标系,{},[]
2076,2076,global illumination,全局照明,0.0,9,"[{'word': '全局光照', 'ratio': 1.0}]",全局光照,{},[]
2077,2077,global minima,全局最小值,0.8888888888888888,9,"[{'word': '全局最小值', 'ratio': 0.8888888888888888}, {'word': '全局极小值', 'ratio': 0.1111111111111111}]",全局最小值,{},[]
2078,2078,global minimum,全局最小值,0.7777777777777778,9,"[{'word': '全局最小值', 'ratio': 0.7777777777777778}, {'word': '全局最优值', 'ratio': 0.1111111111111111}, {'word': '全局最小点', 'ratio': 0.1111111111111111}]",全局最小值,{},[]
2079,2079,global model,全局模型,1.0,10,"[{'word': '全局模型', 'ratio': 1.0}]",全局模型,{},[]
2080,2080,global objective,全局目标,0.7,10,"[{'word': '全局目标', 'ratio': 0.7}, {'word': '全局目标函数', 'ratio': 0.2}, {'word': '全局模型', 'ratio': 0.1}]",全局目标,{},[]
2081,2081,global optima,全局最优解,1.0,10,"[{'word': '全局最优解', 'ratio': 1.0}]",全局最优解,{},[]
2082,2082,global optimization,全局优化,1.0,10,"[{'word': '全局优化', 'ratio': 1.0}]",全局优化,{},[]
2083,2083,global optimum,全局最优解,0.3,10,"[{'word': '全局最优', 'ratio': 0.5}, {'word': '全局最优解', 'ratio': 0.3}, {'word': '全局最优点', 'ratio': 0.2}]",全局最优,{},[]
2084,2084,global pooling,全局池化,1.0,9,"[{'word': '全局池化', 'ratio': 1.0}]",全局池化,{},[]
2085,2085,global reward,全局奖励,1.0,9,"[{'word': '全局奖励', 'ratio': 1.0}]",全局奖励,{},[]
2086,2086,goal state,目标状态,1.0,9,"[{'word': '目标状态', 'ratio': 1.0}]",目标状态,{},[]
2087,2087,gold label,金标签,0.0,9,"[{'word': '黄金标签', 'ratio': 0.6666666666666666}, {'word': '金标准标签', 'ratio': 0.2222222222222222}, {'word': '标准标签', 'ratio': 0.1111111111111111}]",黄金标签,{},[]
2088,2088,gold parse,金标准解析,0.2222222222222222,9,"[{'word': '黄金解析', 'ratio': 0.5555555555555556}, {'word': '金标准解析', 'ratio': 0.2222222222222222}, {'word': '黄金分析', 'ratio': 0.1111111111111111}, {'word': '标准解析', 'ratio': 0.1111111111111111}]",黄金解析,{},[]
2089,2089,gossip algorithm,传言算法,0.0,9,"[{'word': '八卦算法', 'ratio': 0.4444444444444444}, {'word': '谣言算法', 'ratio': 0.2222222222222222}, {'word': '谣传算法', 'ratio': 0.1111111111111111}, {'word': '谷歌算法', 'ratio': 0.1111111111111111}, {'word': '算法', 'ratio': 0.1111111111111111}]",谣言算法,"1. Rank: 谣言算法, 谣传算法, 八卦算法, 谷歌算法, 算法

2. Explanation: The term ""谣言算法"" (Rumor Algorithm) is the best fit for the translation of ""gossip algorithm"" in the AI context. The term ""gossip"" in this context refers to a decentralized communication protocol where nodes exchange information in a manner similar to how gossip spreads in social networks. ""谣言"" (rumor) captures this essence effectively, as it conveys the idea of information being shared informally and diffusely among participants.

""谣传算法"" (rumor algorithm) is a close second, as it also conveys a similar meaning, but ""谣言"" is more commonly used in the context of algorithms and networking, making it slightly more appropriate.

""八卦算法"" (Bagua algorithm) translates to ""gossip"" in a more colloquial sense, often associated with idle talk or chatter, which may not fully encapsulate the technical nature of the algorithm in AI. 

""谷歌算法"" (Google Algorithm) is irrelevant in this context, as it refers specifically to algorithms developed by Google, which does not relate to the concept of gossip algorithms.

Finally, ""算法"" (algorithm) is too vague and does not provide any specific context or meaning related to the term ""gossip."" Thus, ""谣言算法"" is the most semantically accurate and contextually fitting translation for ""gossip algorithm"" in the AI domain.","['Bagua algorithm', 'Rumor Algorithm', 'rumor algorithm', 'Google Algorithm', 'algorithm']"
2090,2090,gradient accumulation,梯度累积,1.0,9,"[{'word': '梯度累积', 'ratio': 1.0}]",梯度累积,{},[]
2091,2091,gradient accumulation step,梯度累积步数,0.3333333333333333,9,"[{'word': '梯度累积步骤', 'ratio': 0.5555555555555556}, {'word': '梯度累积步数', 'ratio': 0.3333333333333333}, {'word': '梯度累积步', 'ratio': 0.1111111111111111}]",梯度累积步骤,{},[]
2092,2092,gradient ascent,梯度上升法,0.0,9,"[{'word': '梯度上升', 'ratio': 1.0}]",梯度上升,{},[]
2093,2093,gradient boosted tree,梯度提升树,1.0,9,"[{'word': '梯度提升树', 'ratio': 1.0}]",梯度提升树,{},[]
2094,2094,gradient clipping,梯度裁剪,1.0,9,"[{'word': '梯度裁剪', 'ratio': 1.0}]",梯度裁剪,{},[]
2095,2095,gradient computation,梯度计算,1.0,9,"[{'word': '梯度计算', 'ratio': 1.0}]",梯度计算,{},[]
2096,2096,gradient descent algorithm,梯度下降算法,1.0,9,"[{'word': '梯度下降算法', 'ratio': 1.0}]",梯度下降算法,{},[]
2097,2097,gradient estimate,梯度估计,1.0,9,"[{'word': '梯度估计', 'ratio': 1.0}]",梯度估计,{},[]
2098,2098,gradient estimation,梯度估计,1.0,9,"[{'word': '梯度估计', 'ratio': 1.0}]",梯度估计,{},[]
2099,2099,gradient estimator,梯度估计器,1.0,9,"[{'word': '梯度估计器', 'ratio': 1.0}]",梯度估计器,{},[]
2100,2100,gradient explosion,梯度爆炸,1.0,9,"[{'word': '梯度爆炸', 'ratio': 1.0}]",梯度爆炸,{},[]
2101,2101,gradient flow,梯度流,1.0,9,"[{'word': '梯度流', 'ratio': 1.0}]",梯度流,{},[]
2102,2102,gradient information,梯度信息,1.0,9,"[{'word': '梯度信息', 'ratio': 1.0}]",梯度信息,{},[]
2103,2103,gradient method,梯度方法,0.8888888888888888,9,"[{'word': '梯度方法', 'ratio': 0.8888888888888888}, {'word': '梯度法', 'ratio': 0.1111111111111111}]",梯度方法,{},[]
2104,2104,gradient norm,梯度范数,1.0,9,"[{'word': '梯度范数', 'ratio': 1.0}]",梯度范数,{},[]
2105,2105,gradient operator,梯度算子,1.0,9,"[{'word': '梯度算子', 'ratio': 1.0}]",梯度算子,{},[]
2106,2106,gradient penalty,梯度惩罚,1.0,9,"[{'word': '梯度惩罚', 'ratio': 1.0}]",梯度惩罚,{},[]
2107,2107,gradient signal,梯度信号,1.0,9,"[{'word': '梯度信号', 'ratio': 1.0}]",梯度信号,{},[]
2108,2108,gradient step,梯度步长,0.6666666666666666,9,"[{'word': '梯度步长', 'ratio': 0.6666666666666666}, {'word': '梯度步骤', 'ratio': 0.2222222222222222}, {'word': '梯度步', 'ratio': 0.1111111111111111}]",梯度步长,{},[]
2109,2109,gradient term,梯度项,1.0,9,"[{'word': '梯度项', 'ratio': 1.0}]",梯度项,{},[]
2110,2110,gradient update,梯度更新,0.8888888888888888,9,"[{'word': '梯度更新', 'ratio': 0.8888888888888888}, {'word': '梯度项', 'ratio': 0.1111111111111111}]",梯度更新,{},[]
2111,2111,gradient variance,梯度方差,0.8888888888888888,9,"[{'word': '梯度方差', 'ratio': 0.8888888888888888}, {'word': '梯度更新', 'ratio': 0.1111111111111111}]",梯度方差,{},[]
2112,2112,gradient vector,梯度向量,0.8888888888888888,9,"[{'word': '梯度向量', 'ratio': 0.8888888888888888}, {'word': '梯度方差', 'ratio': 0.1111111111111111}]",梯度向量,{},[]
2113,2113,gradient-based approach,基于梯度的方法,1.0,10,"[{'word': '基于梯度的方法', 'ratio': 1.0}]",基于梯度的方法,{},[]
2114,2114,gradient-based learning,基于梯度学习,0.0,10,"[{'word': '基于梯度的学习', 'ratio': 1.0}]",基于梯度的学习,{},[]
2115,2115,gradient-based method,梯度法,0.0,10,"[{'word': '基于梯度的方法', 'ratio': 1.0}]",基于梯度的方法,{},[]
2116,2116,gradient-based optimization,基于梯度的优化,1.0,10,"[{'word': '基于梯度的优化', 'ratio': 1.0}]",基于梯度的优化,{},[]
2117,2117,grammar inducer,语法诱导器,0.5,10,"[{'word': '语法诱导器', 'ratio': 0.5}, {'word': '语法生成器', 'ratio': 0.4}, {'word': '语法引导器', 'ratio': 0.1}]",语法诱导器,{},[]
2118,2118,grammar induction,语法归纳,0.7,10,"[{'word': '语法归纳', 'ratio': 0.7}, {'word': '语法诱导', 'ratio': 0.3}]",语法归纳,{},[]
2119,2119,grammatical error detection,语法错误检测,1.0,10,"[{'word': '语法错误检测', 'ratio': 1.0}]",语法错误检测,{},[]
2120,2120,grandparent dependency,祖父母依赖,0.2,10,"[{'word': '祖父依赖', 'ratio': 0.3}, {'word': '祖父依存关系', 'ratio': 0.3}, {'word': '祖父母依赖', 'ratio': 0.2}, {'word': '祖父母依赖关系', 'ratio': 0.2}]",祖父母依赖关系,"1. Rank: 祖父母依赖关系, 祖父母依赖, 祖父依存关系, 祖父依赖

2. Explanation: The term ""祖父母依赖关系"" (grandparent dependency) is the best fit because it accurately captures the concept of ""grandparent"" in a way that is both semantically precise and contextually appropriate for the AI domain. The use of ""依赖关系"" (dependency) aligns well with the technical terminology used in dependency parsing and machine learning contexts, where relationships between elements are crucial. 

""祖父母依赖"" (grandparent dependence) is also a strong candidate, but it lacks the explicit mention of ""关系"" (relationship), which is important in conveying the nature of the dependency in a structured manner. 

The terms ""祖父依存关系"" (grandfather dependency) and ""祖父依赖"" (grandfather dependent) are less suitable because they refer specifically to ""grandfather"" rather than ""grandparent,"" which is broader and more inclusive of both grandparents. In the context of AI and dependency parsing, using the more inclusive term ""grandparent"" is essential for accurately representing the relationships being modeled.","['grandfather dependent', 'grandfather dependency', 'grandparent dependence', 'grandparent dependency']"
2121,2121,graph Laplacian,图拉普拉斯矩阵,0.0,9,"[{'word': '图拉普拉斯算子', 'ratio': 1.0}]",图拉普拉斯算子,{},[]
2122,2122,graph Laplacian matrix,图拉普拉斯矩阵,1.0,9,"[{'word': '图拉普拉斯矩阵', 'ratio': 1.0}]",图拉普拉斯矩阵,{},[]
2123,2123,graph adjacency matrix,图邻接矩阵,1.0,9,"[{'word': '图邻接矩阵', 'ratio': 1.0}]",图邻接矩阵,{},[]
2124,2124,graph attention,图注意力,1.0,9,"[{'word': '图注意力', 'ratio': 1.0}]",图注意力,{},[]
2125,2125,graph attention mechanism,图注意力机制,0.8888888888888888,9,"[{'word': '图注意力机制', 'ratio': 0.8888888888888888}, {'word': '图注意机制', 'ratio': 0.1111111111111111}]",图注意力机制,{},[]
2126,2126,graph attention network,图注意力网络,0.8888888888888888,9,"[{'word': '图注意力网络', 'ratio': 0.8888888888888888}, {'word': '图注意网络', 'ratio': 0.1111111111111111}]",图注意力网络,{},[]
2127,2127,graph classification,图分类,1.0,9,"[{'word': '图分类', 'ratio': 1.0}]",图分类,{},[]
2128,2128,graph clustering,图聚类,1.0,9,"[{'word': '图聚类', 'ratio': 1.0}]",图聚类,{},[]
2129,2129,graph construction,图构建,1.0,9,"[{'word': '图构建', 'ratio': 1.0}]",图构建,{},[]
2130,2130,graph contrastive learning,图对比学习,0.8888888888888888,9,"[{'word': '图对比学习', 'ratio': 0.8888888888888888}, {'word': '图谱对比学习', 'ratio': 0.1111111111111111}]",图对比学习,{},[]
2131,2131,graph convolution,图卷积,0.7777777777777778,9,"[{'word': '图卷积', 'ratio': 0.7777777777777778}, {'word': '图谱卷积', 'ratio': 0.1111111111111111}, {'word': '图对比学习', 'ratio': 0.1111111111111111}]",图卷积,{},[]
2132,2132,graph convolution network,图卷积网络,0.8888888888888888,9,"[{'word': '图卷积网络', 'ratio': 0.8888888888888888}, {'word': '图谱卷积网络', 'ratio': 0.1111111111111111}]",图卷积网络,{},[]
2133,2133,graph convolutional network,图卷积网络,0.2222222222222222,9,"[{'word': '图卷积神经网络', 'ratio': 0.6666666666666666}, {'word': '图卷积网络', 'ratio': 0.2222222222222222}, {'word': '图谱卷积网络', 'ratio': 0.1111111111111111}]",图卷积神经网络,{},[]
2134,2134,graph cut,图割,0.4444444444444444,9,"[{'word': '图切割', 'ratio': 0.5555555555555556}, {'word': '图割', 'ratio': 0.4444444444444444}]",图切割,{},[]
2135,2135,graph cut algorithm,图割算法,0.4444444444444444,9,"[{'word': '图割算法', 'ratio': 0.4444444444444444}, {'word': '图切割算法', 'ratio': 0.3333333333333333}, {'word': '图切算法', 'ratio': 0.2222222222222222}]",图割算法,"1. Rank: 图割算法, 图切算法, 图切割算法

2. Explanation: The term ""图割算法"" (graph cut algorithm) is the most semantically accurate and contextually fitting translation for the AI domain. It directly corresponds to the English term without introducing ambiguity. The term ""割"" (cut) is commonly used in the context of graph theory and algorithms, making it easily recognizable to professionals in the field. 

The second candidate, ""图切算法"" (graph cut algorithm), is also a valid translation but is less commonly used in the literature compared to ""图割算法."" While it maintains the core meaning, ""切"" (cut) is not as standard in this specific context.

The third candidate, ""图切割算法"" (graph cutting algorithm), introduces the term ""切割"" (cutting), which can imply a more dynamic or ongoing process rather than the specific operation of cutting a graph into segments. This could lead to confusion in the context of algorithmic discussions, where precision is crucial.

Overall, ""图割算法"" is the best fit due to its established usage in the field of graph theory and algorithms, ensuring clarity and accuracy in communication among AI professionals.","['graph cut algorithm', 'graph cutting algorithm', 'graph cut algorithm']"
2136,2136,graph dataset,图数据集,1.0,9,"[{'word': '图数据集', 'ratio': 1.0}]",图数据集,{},[]
2137,2137,graph datum,图数据,1.0,9,"[{'word': '图数据', 'ratio': 1.0}]",图数据,{},[]
2138,2138,graph diameter,图直径,1.0,9,"[{'word': '图直径', 'ratio': 1.0}]",图直径,{},[]
2139,2139,graph embedding,图嵌入,1.0,9,"[{'word': '图嵌入', 'ratio': 1.0}]",图嵌入,{},[]
2140,2140,graph generator,图生成器,1.0,8,"[{'word': '图生成器', 'ratio': 1.0}]",图生成器,{},[]
2141,2141,graph isomorphism,图同构,1.0,8,"[{'word': '图同构', 'ratio': 1.0}]",图同构,{},[]
2142,2142,graph kernel,图核,1.0,8,"[{'word': '图核', 'ratio': 1.0}]",图核,{},[]
2143,2143,graph learning,图学习,1.0,8,"[{'word': '图学习', 'ratio': 1.0}]",图学习,{},[]
2144,2144,graph matching,图匹配,1.0,8,"[{'word': '图匹配', 'ratio': 1.0}]",图匹配,{},[]
2145,2145,graph mining,图挖掘,1.0,8,"[{'word': '图挖掘', 'ratio': 1.0}]",图挖掘,{},[]
2146,2146,graph model,图模型,1.0,8,"[{'word': '图模型', 'ratio': 1.0}]",图模型,{},[]
2147,2147,graph neural network,图神经网络,1.0,8,"[{'word': '图神经网络', 'ratio': 1.0}]",图神经网络,{},[]
2148,2148,graph node,图节点,1.0,8,"[{'word': '图节点', 'ratio': 1.0}]",图节点,{},[]
2149,2149,graph partitioning,图划分,0.875,8,"[{'word': '图划分', 'ratio': 0.875}, {'word': '图分割', 'ratio': 0.125}]",图划分,{},[]
2150,2150,graph pattern,图模式,1.0,9,"[{'word': '图模式', 'ratio': 1.0}]",图模式,{},[]
2151,2151,graph representation,图表示,1.0,9,"[{'word': '图表示', 'ratio': 1.0}]",图表示,{},[]
2152,2152,graph sampling,图采样,1.0,9,"[{'word': '图采样', 'ratio': 1.0}]",图采样,{},[]
2153,2153,graph structure,图结构,1.0,9,"[{'word': '图结构', 'ratio': 1.0}]",图结构,{},[]
2154,2154,graph theory,图论,1.0,9,"[{'word': '图论', 'ratio': 1.0}]",图论,{},[]
2155,2155,graph topology,图拓扑结构,0.0,10,"[{'word': '图拓扑', 'ratio': 0.9}, {'word': '图的拓扑结构', 'ratio': 0.1}]",图拓扑,{},[]
2156,2156,graph traversal,图遍历,1.0,10,"[{'word': '图遍历', 'ratio': 1.0}]",图遍历,{},[]
2157,2157,graph-based approach,基于图的方法,1.0,10,"[{'word': '基于图的方法', 'ratio': 1.0}]",基于图的方法,{},[]
2158,2158,graph-based dependency parsing,基于图的依存句法分析,0.5,10,"[{'word': '基于图的依存句法分析', 'ratio': 0.5}, {'word': '基于图的依赖解析', 'ratio': 0.3}, {'word': '基于图的依存解析', 'ratio': 0.2}]",基于图的依存句法分析,{},[]
2159,2159,graph-based learning,基于图的学习,1.0,10,"[{'word': '基于图的学习', 'ratio': 1.0}]",基于图的学习,{},[]
2160,2160,graph-based method,基于图的方法,1.0,9,"[{'word': '基于图的方法', 'ratio': 1.0}]",基于图的方法,{},[]
2161,2161,graph-based model,基于图的模型,1.0,9,"[{'word': '基于图的模型', 'ratio': 1.0}]",基于图的模型,{},[]
2162,2162,graph-based representation,基于图的表示,1.0,9,"[{'word': '基于图的表示', 'ratio': 1.0}]",基于图的表示,{},[]
2163,2163,graph-level task,图级任务,1.0,9,"[{'word': '图级任务', 'ratio': 1.0}]",图级任务,{},[]
2164,2164,graphical model,图形模型,0.4444444444444444,9,"[{'word': '图模型', 'ratio': 0.5555555555555556}, {'word': '图形模型', 'ratio': 0.4444444444444444}]",图模型,{},[]
2165,2165,graphlet,小子图,0.0,8,"[{'word': '图元', 'ratio': 0.5}, {'word': '图小元', 'ratio': 0.125}, {'word': '图小体', 'ratio': 0.125}, {'word': '图形单元', 'ratio': 0.125}, {'word': '图形子结构', 'ratio': 0.125}]",图元,{},[]
2166,2166,graphlet kernel,图基核,0.0,8,"[{'word': '图元核', 'ratio': 0.5}, {'word': '图小元核', 'ratio': 0.125}, {'word': '图小体核', 'ratio': 0.125}, {'word': '图形单元核', 'ratio': 0.125}, {'word': '图形子结构核', 'ratio': 0.125}]",图元核,{},[]
2167,2167,greedy algorithm,贪心算法,0.875,8,"[{'word': '贪心算法', 'ratio': 0.875}, {'word': '贪婪算法', 'ratio': 0.125}]",贪心算法,{},[]
2168,2168,greedy approach,贪婪算法,0.0,8,"[{'word': '贪心方法', 'ratio': 0.875}, {'word': '贪婪方法', 'ratio': 0.125}]",贪心方法,{},[]
2169,2169,greedy decoding,贪婪解码,0.625,8,"[{'word': '贪婪解码', 'ratio': 0.625}, {'word': '贪心解码', 'ratio': 0.375}]",贪婪解码,{},[]
2170,2170,greedy inference,贪婪推理,0.25,8,"[{'word': '贪心推理', 'ratio': 0.375}, {'word': '贪婪推断', 'ratio': 0.375}, {'word': '贪婪推理', 'ratio': 0.25}]",贪婪推断,"1. Rank: 贪婪推断, 贪心推理, 贪婪推理

2. Explanation: The term ""贪婪推断"" (greedy inference) is the best fit for the AI domain-specific usage because it accurately captures the meaning of ""greedy"" in the context of inference processes in machine learning. The word ""推断"" (inference) is more commonly used in AI literature to refer to the process of drawing conclusions from data, making it semantically precise. 

In contrast, ""贪心推理"" (greedy reasoning) and ""贪婪推理"" (greedy reasoning) use ""推理"" (reasoning), which is less specific to the inference processes typically discussed in AI contexts. While ""推理"" can be related to reasoning in a broader sense, it does not convey the same technical meaning as ""推断"" in this context. Therefore, ""贪婪推断"" is the most contextually appropriate choice for discussing greedy inference in AI.","['greedy reasoning', 'greedy inference', 'greedy reasoning']"
2171,2171,greedy maximization,贪婪最大化,0.625,8,"[{'word': '贪婪最大化', 'ratio': 0.625}, {'word': '贪心最大化', 'ratio': 0.375}]",贪婪最大化,{},[]
2172,2172,greedy method,贪心方法,0.25,8,"[{'word': '贪婪方法', 'ratio': 0.625}, {'word': '贪心方法', 'ratio': 0.25}, {'word': '贪心法', 'ratio': 0.125}]",贪婪方法,{},[]
2173,2173,greedy optimization,贪心优化,0.375,8,"[{'word': '贪婪优化', 'ratio': 0.625}, {'word': '贪心优化', 'ratio': 0.375}]",贪婪优化,{},[]
2174,2174,greedy policy,贪婪政策,0.0,10,"[{'word': '贪婪策略', 'ratio': 0.7}, {'word': '贪心策略', 'ratio': 0.2}, {'word': '贪婪搜索', 'ratio': 0.1}]",贪婪策略,{},[]
2175,2175,greedy search,贪心搜索,0.2,10,"[{'word': '贪婪搜索', 'ratio': 0.8}, {'word': '贪心搜索', 'ratio': 0.2}]",贪婪搜索,{},[]
2176,2176,greedy strategy,贪心策略,0.2,10,"[{'word': '贪婪策略', 'ratio': 0.8}, {'word': '贪心策略', 'ratio': 0.2}]",贪婪策略,{},[]
2177,2177,grid cell,网格细胞,1.0,10,"[{'word': '网格细胞', 'ratio': 1.0}]",网格细胞,{},[]
2178,2178,grid search,网格搜索,1.0,10,"[{'word': '网格搜索', 'ratio': 1.0}]",网格搜索,{},[]
2179,2179,grid-world,网格世界,1.0,9,"[{'word': '网格世界', 'ratio': 1.0}]",网格世界,{},[]
2180,2180,ground atom,基础原子,0.4444444444444444,9,"[{'word': '基本原子', 'ratio': 0.4444444444444444}, {'word': '基础原子', 'ratio': 0.4444444444444444}, {'word': '具体原子', 'ratio': 0.1111111111111111}]",基本原子,"1. Rank: 基本原子, 具体原子, 基础原子

2. Explanation: The term ""基本原子"" (basic atoms) is the best fit for ""ground atom"" in the AI context due to its semantic accuracy and contextual relevance. In AI and logic programming, ""ground"" refers to terms that are fully specified without any variables, which aligns well with the notion of ""基本"" (basic) as it implies a foundational or elemental aspect of the atom. 

""具体原子"" (concrete atoms) is a reasonable alternative, as ""具体"" (concrete) also suggests specificity, but it may not convey the foundational aspect as effectively as ""基本."" 

""基础原子"" (basic atoms) is less preferred because ""基础"" (foundation) can imply a broader or more abstract concept, which may not directly correspond to the specific meaning of ""ground"" in this context. 

Overall, ""基本原子"" captures the essence of ""ground atom"" in a way that is both semantically accurate and contextually appropriate for the AI domain.","['basic atoms', 'basic atoms', 'concrete atoms']"
2181,2181,ground set,基础集合,0.2222222222222222,9,"[{'word': '基础集', 'ratio': 0.4444444444444444}, {'word': '基础集合', 'ratio': 0.2222222222222222}, {'word': '基本集合', 'ratio': 0.2222222222222222}, {'word': '实体集合', 'ratio': 0.1111111111111111}]",基础集,"1. Rank: 基础集, 基础集合, 基本集合, 实体集合

2. Explanation: The term ""基础集"" (ground set) is the best fit for the AI domain-specific usage because it accurately conveys the concept of a foundational or base set in mathematical and algorithmic contexts. The term ""基础"" (base) is commonly used in mathematical literature to denote foundational elements, making it semantically appropriate. ""集合"" (set) is the standard term for a collection of elements in set theory, which aligns with the context provided. 

""基础集合"" is also a strong candidate, but it is slightly more verbose and less commonly used than ""基础集."" ""基本集合"" (basic set) could imply a more general or elementary set, which may not capture the specific meaning of ""ground set"" in this context. Lastly, ""实体集合"" (entity collection) introduces ambiguity, as ""实体"" (entity) does not directly relate to the concept of a ground set in algorithms and could mislead readers into thinking of a different context. Thus, ""基础集"" is the most semantically accurate and contextually fitting translation.","['base set', 'base set', 'basic set', 'Entity collection']"
2182,2182,ground truth label,真实标签,1.0,9,"[{'word': '真实标签', 'ratio': 1.0}]",真实标签,{},[]
2183,2183,ground-truth box,真实边界框,0.4444444444444444,9,"[{'word': '真实框', 'ratio': 0.5555555555555556}, {'word': '真实边界框', 'ratio': 0.4444444444444444}]",真实框,{},[]
2184,2184,grounded language learning,基于实际情境的语言学习,0.0,9,"[{'word': '以视频为基础的语言学习', 'ratio': 0.1111111111111111}, {'word': '基于情境的语言学习', 'ratio': 0.1111111111111111}, {'word': '以实物为基础的语言学习', 'ratio': 0.1111111111111111}, {'word': '实体语言学习', 'ratio': 0.1111111111111111}, {'word': '有根语言学习', 'ratio': 0.1111111111111111}, {'word': '基于实际环境的语言学习', 'ratio': 0.1111111111111111}, {'word': '基于实境的语言学习', 'ratio': 0.1111111111111111}, {'word': '基于实际的语言学习', 'ratio': 0.1111111111111111}, {'word': '基于实际场景的语言学习', 'ratio': 0.1111111111111111}]",基于实际场景的语言学习,"1. Rank: 基于实际场景的语言学习, 基于实际环境的语言学习, 基于情境的语言学习, 基于实境的语言学习, 以实物为基础的语言学习, 以视频为基础的语言学习, 实体语言学习, 有根语言学习, 基于实际的语言学习

2. Explanation: The top-ranked translation, ""基于实际场景的语言学习"" (Language learning based on actual scenarios), is the best fit because it captures the essence of ""grounded language learning"" in the context of AI and language acquisition. This term emphasizes the importance of learning language in relation to real-world contexts and scenarios, which aligns closely with the idea of acquiring language in an unsupervised manner, similar to how children learn. 

The other candidates, while related, either focus too much on specific aspects (like ""video"" or ""objects"") or do not convey the same depth of contextual learning that ""基于实际场景的语言学习"" does. For instance, ""基于情境的语言学习"" (context-based language learning) is also a strong candidate but is slightly less specific than ""基于实际场景的语言学习,"" which implies a more immersive and practical approach to language learning. 

Overall, the top choice effectively reflects the AI domain's emphasis on contextual and situational learning, making it the most semantically accurate and contextually appropriate translation.","['Video-based language learning', 'context-based language learning', 'Object-based language learning', 'entity language learning', 'Rooted language learning', 'Language learning based on actual environment', 'Context-based language learning', 'Real-life language learning', 'Language learning based on actual scenarios']"
2185,2185,grounded supervision,基础监督,0.0,9,"[{'word': '基于实际的监督', 'ratio': 0.2222222222222222}, {'word': '基于情境的监督', 'ratio': 0.1111111111111111}, {'word': '以实物为基础的监督', 'ratio': 0.1111111111111111}, {'word': '实体化监督', 'ratio': 0.1111111111111111}, {'word': '有根监督', 'ratio': 0.1111111111111111}, {'word': '基于实际环境的监督', 'ratio': 0.1111111111111111}, {'word': '基于实境的监督', 'ratio': 0.1111111111111111}, {'word': '基于实际场景的监督', 'ratio': 0.1111111111111111}]",基于实际环境的监督,"1. Rank: 基于实际环境的监督, 基于实际场景的监督, 基于情境的监督, 基于实境的监督, 基于实际的监督, 实体化监督, 有根监督, 以实物为基础的监督, 物理监督, 根监督

2. Explanation: The term ""基于实际环境的监督"" (Supervision based on actual circumstances) is the best fit for ""grounded supervision"" in the AI context because it captures the essence of grounding in a specific context or environment, which is crucial in AI applications. This translation emphasizes the importance of situational awareness and context, aligning well with the idea of supervision that is informed by real-world conditions. 

Other candidates like ""基于情境的监督"" (situation-based supervision) and ""基于实际场景的监督"" (Supervision based on actual scenarios) are also relevant but slightly less precise, as they may not fully encompass the broader implications of ""grounded"" in terms of actual environments. 

Terms like ""实体化监督"" (physical supervision) and ""有根监督"" (Rooted supervision) diverge from the intended meaning, as they suggest a more literal or physical interpretation that does not align with the AI context of supervision based on contextual understanding. Thus, ""基于实际环境的监督"" stands out as the most semantically accurate and contextually appropriate translation.","['reality-based supervision', 'situation-based supervision', 'physical-based supervision', 'physical supervision', 'Rooted supervision', 'Supervision based on actual circumstances', 'reality-based supervision', 'Supervision based on actual scenarios']"
2186,2186,group normalization,组规范化,0.0,9,"[{'word': '组归一化', 'ratio': 0.8888888888888888}, {'word': '分组归一化', 'ratio': 0.1111111111111111}]",组归一化,{},[]
2187,2187,group sparsity,组稀疏,0.0,9,"[{'word': '组稀疏性', 'ratio': 1.0}]",组稀疏性,{},[]
2188,2188,half-space,半空间,1.0,9,"[{'word': '半空间', 'ratio': 1.0}]",半空间,{},[]
2189,2189,hand pose estimation,手姿态估计,0.0,9,"[{'word': '手势估计', 'ratio': 0.3333333333333333}, {'word': '手部姿态估计', 'ratio': 0.2222222222222222}, {'word': '手势姿态估计', 'ratio': 0.2222222222222222}, {'word': '手部姿势估计', 'ratio': 0.2222222222222222}]",手部姿态估计,"1. Rank: 手部姿态估计, 手部姿态估计, 手势估计, 手势姿态估计

2. Explanation: The best fit for ""hand pose estimation"" is ""手部姿态估计"" (Hand pose estimation) because it accurately captures the specific meaning of the term in the context of AI and computer vision. ""手部"" (hand) specifies that the estimation is focused on the hand, while ""姿态"" (pose) refers to the position and orientation of the hand's joints. This term is semantically precise and aligns well with the technical context of estimating the configuration of hand joints in depth images. 

The other candidates, such as ""手势估计"" (Gesture estimation) and ""手势姿态估计"" (Gesture pose estimation), are less accurate because they imply a focus on gestures rather than the specific pose of the hand, which is crucial in the context of the provided information. ""手部姿势估计"" (Hand posture estimation) is also a close contender, but ""姿态"" (pose) is more commonly used in technical contexts related to estimation tasks in AI, making ""手部姿态估计"" the most appropriate choice.","['Gesture estimation', 'Hand pose estimation', 'Gesture pose estimation', 'Hand pose estimation']"
2190,2190,hard attention,硬注意力,0.7777777777777778,9,"[{'word': '硬注意力', 'ratio': 0.7777777777777778}, {'word': '硬性注意力', 'ratio': 0.1111111111111111}, {'word': '努力关注', 'ratio': 0.1111111111111111}]",硬注意力,{},[]
2191,2191,hash,哈希,0.8888888888888888,9,"[{'word': '哈希', 'ratio': 0.8888888888888888}, {'word': '散列', 'ratio': 0.1111111111111111}]",哈希,{},[]
2192,2192,hash function,哈希函数,1.0,9,"[{'word': '哈希函数', 'ratio': 1.0}]",哈希函数,{},[]
2193,2193,hash table,哈希表,1.0,9,"[{'word': '哈希表', 'ratio': 1.0}]",哈希表,{},[]
2194,2194,hashing algorithm,哈希算法,1.0,9,"[{'word': '哈希算法', 'ratio': 1.0}]",哈希算法,{},[]
2195,2195,hate speech classifier,仇恨言论分类器,1.0,9,"[{'word': '仇恨言论分类器', 'ratio': 1.0}]",仇恨言论分类器,{},[]
2196,2196,hate speech detection,仇恨言论检测,1.0,9,"[{'word': '仇恨言论检测', 'ratio': 1.0}]",仇恨言论检测,{},[]
2197,2197,head entity,头实体,0.5555555555555556,9,"[{'word': '头实体', 'ratio': 0.5555555555555556}, {'word': '主体实体', 'ratio': 0.4444444444444444}]",头实体,{},[]
2198,2198,head word,头词,0.2857142857142857,7,"[{'word': '主要词', 'ratio': 0.2857142857142857}, {'word': '头词', 'ratio': 0.2857142857142857}, {'word': '中心词', 'ratio': 0.2857142857142857}, {'word': '主词', 'ratio': 0.14285714285714285}]",头词,"1. Rank: 头词, 主要词, 中心词, 主词

2. Explanation: The term ""头词"" (head word) is the most accurate translation in the context of AI and dependency parsing. In computational linguistics, ""头词"" specifically refers to the main word in a phrase that governs the other words (modifiers) in a dependency structure. This aligns perfectly with the provided context, where the head word is crucial for understanding the relationships between words in a parse. 

""主要词"" (main word) is a close second, but it lacks the specific technical connotation that ""头词"" carries in the field of linguistics. ""中心词"" (center word) is less commonly used in this context and may not convey the same precise meaning as ""head word."" Lastly, ""主词"" (subject) is misleading, as it typically refers to the grammatical subject of a sentence rather than the head word in a dependency structure. Therefore, ""头词"" is the best fit for its semantic accuracy and contextual relevance in AI terminology.","['main word', 'Head word', 'center word', 'subject']"
2199,2199,heap structure,堆结构,1.0,7,"[{'word': '堆结构', 'ratio': 1.0}]",堆结构,{},[]
2200,2200,heatmap,热力图,0.4285714285714285,7,"[{'word': '热图', 'ratio': 0.5714285714285714}, {'word': '热力图', 'ratio': 0.42857142857142855}]",热图,{},[]
2201,2201,held-out data,留存数据,0.0,7,"[{'word': '保留数据', 'ratio': 0.8571428571428571}, {'word': '留出数据', 'ratio': 0.14285714285714285}]",保留数据,{},[]
2202,2202,heuristic algorithm,启发式算法,1.0,9,"[{'word': '启发式算法', 'ratio': 1.0}]",启发式算法,{},[]
2203,2203,heuristic function,启发函数,0.3333333333333333,9,"[{'word': '启发式函数', 'ratio': 0.6666666666666666}, {'word': '启发函数', 'ratio': 0.3333333333333333}]",启发式函数,{},[]
2204,2204,heuristic search,启发式搜索,1.0,9,"[{'word': '启发式搜索', 'ratio': 1.0}]",启发式搜索,{},[]
2205,2205,heuristic search algorithm,启发式搜索算法,1.0,9,"[{'word': '启发式搜索算法', 'ratio': 1.0}]",启发式搜索算法,{},[]
2206,2206,heuristic value,启发式值,0.6666666666666666,9,"[{'word': '启发式值', 'ratio': 0.6666666666666666}, {'word': '启发值', 'ratio': 0.3333333333333333}]",启发式值,{},[]
2207,2207,hidden dimension,隐藏维度,1.0,10,"[{'word': '隐藏维度', 'ratio': 1.0}]",隐藏维度,{},[]
2208,2208,hidden dimension size,隐藏维度大小,1.0,10,"[{'word': '隐藏维度大小', 'ratio': 1.0}]",隐藏维度大小,{},[]
2209,2209,hidden dimensionality,隐藏维度,0.8,10,"[{'word': '隐藏维度', 'ratio': 0.8}, {'word': '隐藏维度性', 'ratio': 0.1}, {'word': '隐藏维数', 'ratio': 0.1}]",隐藏维度,{},[]
2210,2210,hidden embedding,隐藏嵌入,1.0,10,"[{'word': '隐藏嵌入', 'ratio': 1.0}]",隐藏嵌入,{},[]
2211,2211,hidden feature,隐藏特征,1.0,10,"[{'word': '隐藏特征', 'ratio': 1.0}]",隐藏特征,{},[]
2212,2212,hidden layer,隐藏层,1.0,9,"[{'word': '隐藏层', 'ratio': 1.0}]",隐藏层,{},[]
2213,2213,hidden representation,隐藏表示,1.0,9,"[{'word': '隐藏表示', 'ratio': 1.0}]",隐藏表示,{},[]
2214,2214,hidden size,隐藏大小,0.8888888888888888,9,"[{'word': '隐藏大小', 'ratio': 0.8888888888888888}, {'word': '隐藏层大小', 'ratio': 0.1111111111111111}]",隐藏大小,{},[]
2215,2215,hidden state,隐藏状态,1.0,9,"[{'word': '隐藏状态', 'ratio': 1.0}]",隐藏状态,{},[]
2216,2216,hidden state dimension,隐藏状态维度,1.0,9,"[{'word': '隐藏状态维度', 'ratio': 1.0}]",隐藏状态维度,{},[]
2217,2217,hidden state representation,隐藏状态表示,0.5,10,"[{'word': '隐状态表示', 'ratio': 0.5}, {'word': '隐藏状态表示', 'ratio': 0.5}]",隐状态表示,{},[]
2218,2218,hidden state vector,隐状态向量,0.5,10,"[{'word': '隐状态向量', 'ratio': 0.5}, {'word': '隐藏状态向量', 'ratio': 0.5}]",隐状态向量,{},[]
2219,2219,hidden unit,隐藏单元,0.9,10,"[{'word': '隐藏单元', 'ratio': 0.9}, {'word': '隐单位', 'ratio': 0.1}]",隐藏单元,{},[]
2220,2220,hidden variable,隐藏变量,0.5,10,"[{'word': '隐变量', 'ratio': 0.5}, {'word': '隐藏变量', 'ratio': 0.5}]",隐变量,{},[]
2221,2221,hierarchical agglomerative clustering,分层凝聚聚类,0.0,10,"[{'word': '层次聚合聚类', 'ratio': 0.8}, {'word': '层次聚类', 'ratio': 0.1}, {'word': '层次凝聚聚类', 'ratio': 0.1}]",层次聚合聚类,{},[]
2222,2222,hierarchical clustering,层次聚类,1.0,9,"[{'word': '层次聚类', 'ratio': 1.0}]",层次聚类,{},[]
2223,2223,hierarchical decoder,分层解码器,0.0,9,"[{'word': '层次解码器', 'ratio': 1.0}]",层次解码器,{},[]
2224,2224,hierarchical feature,分层特征,0.0,9,"[{'word': '层次特征', 'ratio': 1.0}]",层次特征,{},[]
2225,2225,hierarchical inference,层次推断,0.6666666666666666,9,"[{'word': '层次推断', 'ratio': 0.6666666666666666}, {'word': '层次推理', 'ratio': 0.3333333333333333}]",层次推断,{},[]
2226,2226,hierarchical method,层次方法,1.0,9,"[{'word': '层次方法', 'ratio': 1.0}]",层次方法,{},[]
2227,2227,hierarchical model,分层模型,0.1111111111111111,9,"[{'word': '层次模型', 'ratio': 0.8888888888888888}, {'word': '分层模型', 'ratio': 0.1111111111111111}]",层次模型,{},[]
2228,2228,hierarchical reinforcement learning,层次强化学习,0.8888888888888888,9,"[{'word': '层次强化学习', 'ratio': 0.8888888888888888}, {'word': '分层强化学习', 'ratio': 0.1111111111111111}]",层次强化学习,{},[]
2229,2229,hierarchical representation,分层表示,0.1111111111111111,9,"[{'word': '层次表示', 'ratio': 0.8888888888888888}, {'word': '分层表示', 'ratio': 0.1111111111111111}]",层次表示,{},[]
2230,2230,hierarchical structure,层次结构,0.8888888888888888,9,"[{'word': '层次结构', 'ratio': 0.8888888888888888}, {'word': '分层结构', 'ratio': 0.1111111111111111}]",层次结构,{},[]
2231,2231,hierarchical topic model,层次主题模型,0.8888888888888888,9,"[{'word': '层次主题模型', 'ratio': 0.8888888888888888}, {'word': '分层主题模型', 'ratio': 0.1111111111111111}]",层次主题模型,{},[]
2232,2232,hierarchy,层级结构,0.2,10,"[{'word': '层次结构', 'ratio': 0.7}, {'word': '层级结构', 'ratio': 0.2}, {'word': '层级', 'ratio': 0.1}]",层次结构,{},[]
2233,2233,high-dimensional datum,高维数据,1.0,10,"[{'word': '高维数据', 'ratio': 1.0}]",高维数据,{},[]
2234,2234,high-dimensional space,高维空间,1.0,10,"[{'word': '高维空间', 'ratio': 1.0}]",高维空间,{},[]
2235,2235,high-dimensionality,高维,0.0,10,"[{'word': '高维性', 'ratio': 1.0}]",高维性,{},[]
2236,2236,higher-order feature,高阶特征,1.0,10,"[{'word': '高阶特征', 'ratio': 1.0}]",高阶特征,{},[]
2237,2237,higher-order model,高阶模型,1.0,10,"[{'word': '高阶模型', 'ratio': 1.0}]",高阶模型,{},[]
2238,2238,hill-climbing,爬山算法,0.6,10,"[{'word': '爬山算法', 'ratio': 0.6}, {'word': '爬山法', 'ratio': 0.3}, {'word': '上升法', 'ratio': 0.1}]",爬山算法,{},[]
2239,2239,hinge loss,合页损失,0.5,10,"[{'word': '合页损失', 'ratio': 0.5}, {'word': '铰链损失', 'ratio': 0.3}, {'word': '齿轮损失', 'ratio': 0.1}, {'word': '钉子损失', 'ratio': 0.1}]",合页损失,{},[]
2240,2240,hinge loss function,铰链损失函数,0.3,10,"[{'word': '合页损失函数', 'ratio': 0.5}, {'word': '铰链损失函数', 'ratio': 0.3}, {'word': '齿轮损失函数', 'ratio': 0.1}, {'word': '钉子损失函数', 'ratio': 0.1}]",合页损失函数,{},[]
2241,2241,histogram of oriented gradient,梯度方向直方图,0.0,10,"[{'word': '方向梯度直方图', 'ratio': 0.9}, {'word': '定向梯度直方图', 'ratio': 0.1}]",方向梯度直方图,{},[]
2242,2242,homogeneous coordinate,齐次坐标,1.0,10,"[{'word': '齐次坐标', 'ratio': 1.0}]",齐次坐标,{},[]
2243,2243,homographie,同射变换 (homography),0.0,10,"[{'word': '单应性', 'ratio': 0.9}, {'word': '单应矩阵', 'ratio': 0.1}]",单应性,{},[]
2244,2244,homography matrix,单应矩阵,0.7,10,"[{'word': '单应矩阵', 'ratio': 0.7}, {'word': '单应性矩阵', 'ratio': 0.3}]",单应矩阵,{},[]
2245,2245,homomorphism,同态映射 (homomorphism),0.0,10,"[{'word': '同态', 'ratio': 0.9}, {'word': '同态映射', 'ratio': 0.1}]",同态,{},[]
2246,2246,human annotation,人工标注,1.0,10,"[{'word': '人工标注', 'ratio': 1.0}]",人工标注,{},[]
2247,2247,human pose,人体姿势,0.0,10,"[{'word': '人体姿态', 'ratio': 1.0}]",人体姿态,{},[]
2248,2248,human pose estimation,人体姿态估计,1.0,10,"[{'word': '人体姿态估计', 'ratio': 1.0}]",人体姿态估计,{},[]
2249,2249,human-computer interaction,人机交互,1.0,10,"[{'word': '人机交互', 'ratio': 1.0}]",人机交互,{},[]
2250,2250,human-in-the-loop,人机协同 (Human-in-the-loop),0.0,10,"[{'word': '人在回路中', 'ratio': 0.2}, {'word': '人在环中', 'ratio': 0.2}, {'word': '人在回路', 'ratio': 0.2}, {'word': '人工干预/人在回路', 'ratio': 0.1}, {'word': '人机协作', 'ratio': 0.1}, {'word': '人类在环', 'ratio': 0.1}, {'word': '人工参与的系统', 'ratio': 0.1}]","""人在回路中""","1. Rank: ""人在回路中"", ""人工干预/人在回路"", ""人在环中"", ""人在回路"", ""人机协作"", ""人类在环"", ""人工参与的系统""

2. Explanation: The term ""人在回路中"" (back translation: ""people in the loop"") is the best fit for the AI domain-specific usage of ""human-in-the-loop."" This translation accurately captures the essence of the original term, which refers to systems that incorporate human feedback or intervention in a looped process. The phrase ""人在回路中"" maintains the concept of a continuous interaction between humans and machines, which is central to the idea of human-in-the-loop systems. 

The second candidate, ""人工干预/人在回路,"" while also relevant, introduces the notion of ""intervention,"" which may imply a more passive role for humans rather than the active participation suggested by ""human-in-the-loop."" The other candidates, such as ""人在环中"" and ""人在回路,"" lack the specificity of the loop concept, and terms like ""人机协作"" (human-machine collaboration) and ""人工参与的系统"" (human-involved system) do not convey the same iterative feedback mechanism that is critical in AI contexts. Therefore, ""人在回路中"" is the most semantically accurate and contextually appropriate translation.","['people in the loop', 'people in the circle', 'people in circuit', 'Human intervention/human in the loop', 'Human-machine collaboration', 'human beings around', 'human-involved system']"
2251,2251,human-machine interaction,人机交互,1.0,9,"[{'word': '人机交互', 'ratio': 1.0}]",人机交互,{},[]
2252,2252,hybrid model,混合模型,1.0,9,"[{'word': '混合模型', 'ratio': 1.0}]",混合模型,{},[]
2253,2253,hyper-graph,超图,1.0,9,"[{'word': '超图', 'ratio': 1.0}]",超图,{},[]
2254,2254,hyper-parameter tuning,超参数调优,0.7777777777777778,9,"[{'word': '超参数调优', 'ratio': 0.7777777777777778}, {'word': '超参数调整', 'ratio': 0.2222222222222222}]",超参数调优,{},[]
2255,2255,hyperbolic space,双曲空间,1.0,9,"[{'word': '双曲空间', 'ratio': 1.0}]",双曲空间,{},[]
2256,2256,hyperedge,超边,1.0,9,"[{'word': '超边', 'ratio': 1.0}]",超边,{},[]
2257,2257,hypernym,上位词,0.8888888888888888,9,"[{'word': '上位词', 'ratio': 0.8888888888888888}, {'word': '超词', 'ratio': 0.1111111111111111}]",上位词,{},[]
2258,2258,hypernymy,上位词,0.0,9,"[{'word': '上位关系', 'ratio': 0.6666666666666666}, {'word': '上位词关系', 'ratio': 0.2222222222222222}, {'word': '超词关系', 'ratio': 0.1111111111111111}]",上位关系,{},[]
2259,2259,hyperparameter optimization,超参数优化,0.8888888888888888,9,"[{'word': '超参数优化', 'ratio': 0.8888888888888888}, {'word': '上位关系', 'ratio': 0.1111111111111111}]",超参数优化,{},[]
2260,2260,hyperparameter selection,超参数选择,1.0,9,"[{'word': '超参数选择', 'ratio': 1.0}]",超参数选择,{},[]
2261,2261,hyperparameter setting,超参数设置,1.0,8,"[{'word': '超参数设置', 'ratio': 1.0}]",超参数设置,{},[]
2262,2262,hyperparameter space,超参数空间,0.875,8,"[{'word': '超参数空间', 'ratio': 0.875}, {'word': '超参数设置', 'ratio': 0.125}]",超参数空间,{},[]
2263,2263,hyperplane,超平面,1.0,8,"[{'word': '超平面', 'ratio': 1.0}]",超平面,{},[]
2264,2264,hyperprior,超先验,1.0,8,"[{'word': '超先验', 'ratio': 1.0}]",超先验,{},[]
2265,2265,hyponym,下位词,0.75,8,"[{'word': '下位词', 'ratio': 0.75}, {'word': '下义词', 'ratio': 0.25}]",下位词,{},[]
2266,2266,hyponymy,上下位关系,0.0,10,"[{'word': '下义词关系', 'ratio': 0.4}, {'word': '下位关系', 'ratio': 0.3}, {'word': '下位词关系', 'ratio': 0.2}, {'word': '下义关系', 'ratio': 0.1}]",下义词关系,"1. Rank: 下义词关系, 下位词关系, 下位关系, 下义关系

2. Explanation: The term ""下义词关系"" (hyponym relationship) is the best fit because it accurately captures the semantic meaning of ""hyponymy"" in the context of linguistic hierarchies. ""下义词"" directly translates to ""hyponym,"" which refers to a word that is more specific than a general term (hypernym). This term is widely recognized in linguistic and AI contexts, particularly in relation to WordNet and ontology discussions. 

The second choice, ""下位词关系"" (hyponym relation), is also a good candidate but is slightly less common than ""下义词关系."" ""下位关系"" (subordinate relationship) is less precise as it could imply a broader range of relationships beyond just hyponymy. Lastly, ""下义关系"" (Hypothetical relationship) is misleading as it does not convey the specific nature of hyponymy and could confuse readers in the AI domain. Thus, ""下义词关系"" is the most semantically accurate and contextually appropriate term for this specific usage in AI terminology.","['hyponym relationship', 'subordinate relationship', 'hyponym relation', 'Hypothetical relationship']"
2267,2267,hypothesis class,假设类,1.0,10,"[{'word': '假设类', 'ratio': 1.0}]",假设类,{},[]
2268,2268,hypothesis set,假设集,0.5,10,"[{'word': '假设集', 'ratio': 0.5}, {'word': '假设集合', 'ratio': 0.5}]",假设集,{},[]
2269,2269,hypothesis space,假设空间,1.0,10,"[{'word': '假设空间', 'ratio': 1.0}]",假设空间,{},[]
2270,2270,hypothesis test,假设检验,1.0,10,"[{'word': '假设检验', 'ratio': 1.0}]",假设检验,{},[]
2271,2271,i.i.d,独立同分布 (dú lì tóng fēn pù),0.0,5,"[{'word': '独立同分布', 'ratio': 1.0}]",独立同分布,{},[]
2272,2272,i.i.d. sample,独立同分布样本,1.0,5,"[{'word': '独立同分布样本', 'ratio': 1.0}]",独立同分布样本,{},[]
2273,2273,idempotent,幂等的,0.0,5,"[{'word': '幂等', 'ratio': 1.0}]",幂等,{},[]
2274,2274,identity function,恒等函数,1.0,5,"[{'word': '恒等函数', 'ratio': 1.0}]",恒等函数,{},[]
2275,2275,identity mapping,恒等映射,1.0,9,"[{'word': '恒等映射', 'ratio': 1.0}]",恒等映射,{},[]
2276,2276,identity matrix,恒等矩阵,0.0,9,"[{'word': '单位矩阵', 'ratio': 1.0}]",单位矩阵,{},[]
2277,2277,identity transformation,身份变换,0.0,9,"[{'word': '恒等变换', 'ratio': 1.0}]",恒等变换,{},[]
2278,2278,image analysis,图像分析,1.0,9,"[{'word': '图像分析', 'ratio': 1.0}]",图像分析,{},[]
2279,2279,image captioning,图像描述,0.8888888888888888,9,"[{'word': '图像描述', 'ratio': 0.8888888888888888}, {'word': '图像字幕生成', 'ratio': 0.1111111111111111}]",图像描述,{},[]
2280,2280,image classification,图像分类,1.0,9,"[{'word': '图像分类', 'ratio': 1.0}]",图像分类,{},[]
2281,2281,image compression,图像压缩,1.0,9,"[{'word': '图像压缩', 'ratio': 1.0}]",图像压缩,{},[]
2282,2282,image denoising,图像去噪,1.0,9,"[{'word': '图像去噪', 'ratio': 1.0}]",图像去噪,{},[]
2283,2283,image diffusion model,图像扩散模型,1.0,9,"[{'word': '图像扩散模型', 'ratio': 1.0}]",图像扩散模型,{},[]
2284,2284,image embedding,图像嵌入,1.0,10,"[{'word': '图像嵌入', 'ratio': 1.0}]",图像嵌入,{},[]
2285,2285,image encoder,图像编码器,0.8,10,"[{'word': '图像编码器', 'ratio': 0.8}, {'word': '图像嵌入', 'ratio': 0.2}]",图像编码器,{},[]
2286,2286,image feature,图像特征,1.0,10,"[{'word': '图像特征', 'ratio': 1.0}]",图像特征,{},[]
2287,2287,image generation,图像生成,1.0,10,"[{'word': '图像生成', 'ratio': 1.0}]",图像生成,{},[]
2288,2288,image inpainting,图像修复,1.0,10,"[{'word': '图像修复', 'ratio': 1.0}]",图像修复,{},[]
2289,2289,image patch,图像块,0.875,8,"[{'word': '图像块', 'ratio': 0.875}, {'word': '图像补丁', 'ratio': 0.125}]",图像块,{},[]
2290,2290,image plane,图像平面,0.875,8,"[{'word': '图像平面', 'ratio': 0.875}, {'word': '像平面', 'ratio': 0.125}]",图像平面,{},[]
2291,2291,image processing,图像处理,1.0,8,"[{'word': '图像处理', 'ratio': 1.0}]",图像处理,{},[]
2292,2292,image pyramid,图像金字塔,1.0,8,"[{'word': '图像金字塔', 'ratio': 1.0}]",图像金字塔,{},[]
2293,2293,image recognition,图像识别,1.0,8,"[{'word': '图像识别', 'ratio': 1.0}]",图像识别,{},[]
2294,2294,image representation,图像表示,1.0,9,"[{'word': '图像表示', 'ratio': 1.0}]",图像表示,{},[]
2295,2295,image restoration,图像恢复,0.5555555555555556,9,"[{'word': '图像恢复', 'ratio': 0.5555555555555556}, {'word': '图像修复', 'ratio': 0.2222222222222222}, {'word': '图像复原', 'ratio': 0.2222222222222222}]",图像恢复,{},[]
2296,2296,image segmentation,图像分割,1.0,9,"[{'word': '图像分割', 'ratio': 1.0}]",图像分割,{},[]
2297,2297,image super-resolution,图像超分辨率,1.0,9,"[{'word': '图像超分辨率', 'ratio': 1.0}]",图像超分辨率,{},[]
2298,2298,image synthesis,图像合成,0.8888888888888888,9,"[{'word': '图像合成', 'ratio': 0.8888888888888888}, {'word': '图像超分辨率', 'ratio': 0.1111111111111111}]",图像合成,{},[]
2299,2299,image translation,图像翻译,0.6666666666666666,9,"[{'word': '图像翻译', 'ratio': 0.6666666666666666}, {'word': '图像转换', 'ratio': 0.3333333333333333}]",图像翻译,{},[]
2300,2300,image-based rendering,基于图像的渲染,0.8888888888888888,9,"[{'word': '基于图像的渲染', 'ratio': 0.8888888888888888}, {'word': '图像转换', 'ratio': 0.1111111111111111}]",基于图像的渲染,{},[]
2301,2301,image-text pre-training,图像文本预训练,0.1111111111111111,9,"[{'word': '图像-文本预训练', 'ratio': 0.5555555555555556}, {'word': '图文预训练', 'ratio': 0.3333333333333333}, {'word': '图像文本预训练', 'ratio': 0.1111111111111111}]",图像-文本预训练,{},[]
2302,2302,image-to-image translation,图像到图像的转换,0.0,9,"[{'word': '图像到图像翻译', 'ratio': 0.4444444444444444}, {'word': '图像到图像转换', 'ratio': 0.3333333333333333}, {'word': '图像到图像的翻译', 'ratio': 0.2222222222222222}]",图像到图像翻译,"1. Rank: 图像到图像翻译, 图像到图像的翻译, 图像到图像转换

2. Explanation: The term ""图像到图像翻译"" (image to image translation) is the most semantically accurate and contextually fitting translation for the AI domain. This term directly corresponds to the English term without any alteration, maintaining the original meaning and intent. The use of ""翻译"" (translation) in this context aligns with the established terminology in machine learning and computer vision, where ""translation"" refers to the process of transforming one image into another while preserving certain features or characteristics.

The second candidate, ""图像到图像的翻译,"" while still accurate, adds an unnecessary grammatical element (""的"") that does not enhance the meaning and may sound slightly awkward in a technical context. 

The third candidate, ""图像到图像转换"" (image to image conversion), uses ""转换"" (conversion), which can imply a broader range of transformations that may not specifically capture the nuances of the translation process in AI. In the context of image processing, ""translation"" is a more precise term that conveys the specific task of generating a new image based on the input image, which is crucial in applications like image super-resolution and in-painting mentioned in the context. Thus, ""图像到图像翻译"" is the best fit for its clarity and adherence to established terminology in the field.","['image to image translation', 'image to image conversion', 'Image to image translation']"
2303,2303,immediate consequence operator,直接后果算子,0.7777777777777778,9,"[{'word': '直接后果算子', 'ratio': 0.7777777777777778}, {'word': '直接结果运算符', 'ratio': 0.1111111111111111}, {'word': '直接推论算子', 'ratio': 0.1111111111111111}]",直接后果算子,{},[]
2304,2304,imperfect information,不完全信息,0.9,10,"[{'word': '不完全信息', 'ratio': 0.9}, {'word': '不完美信息', 'ratio': 0.1}]",不完全信息,{},[]
2305,2305,implicit differentiation,隐式微分,1.0,10,"[{'word': '隐式微分', 'ratio': 1.0}]",隐式微分,{},[]
2306,2306,implicit function,隐式函数,0.4,10,"[{'word': '隐函数', 'ratio': 0.6}, {'word': '隐式函数', 'ratio': 0.4}]",隐函数,{},[]
2307,2307,implicit representation,隐式表示,1.0,10,"[{'word': '隐式表示', 'ratio': 1.0}]",隐式表示,{},[]
2308,2308,implicit surface,隐式表面,0.1,10,"[{'word': '隐式曲面', 'ratio': 0.9}, {'word': '隐式表面', 'ratio': 0.1}]",隐式曲面,{},[]
2309,2309,importance sampling,重要性采样,0.6666666666666666,9,"[{'word': '重要性采样', 'ratio': 0.6666666666666666}, {'word': '重要性抽样', 'ratio': 0.3333333333333333}]",重要性采样,{},[]
2310,2310,importance sampling estimator,重要性采样估计器,0.6666666666666666,9,"[{'word': '重要性采样估计器', 'ratio': 0.6666666666666666}, {'word': '重要性抽样估计器', 'ratio': 0.3333333333333333}]",重要性采样估计器,{},[]
2311,2311,importance weight,重要性权重,1.0,9,"[{'word': '重要性权重', 'ratio': 1.0}]",重要性权重,{},[]
2312,2312,in-context demonstration,上下文示范,0.0,9,"[{'word': '上下文演示', 'ratio': 0.4444444444444444}, {'word': '上下文示例', 'ratio': 0.3333333333333333}, {'word': '上下文中的示例', 'ratio': 0.1111111111111111}, {'word': '情境演示', 'ratio': 0.1111111111111111}]",上下文示例,"1. Rank: 上下文示例, 上下文演示, 上下文中的示例, 情境演示

2. Explanation: The term ""上下文示例"" (Contextual example) is the best fit for ""in-context demonstration"" because it accurately captures the essence of providing examples within a specific context, which aligns well with the AI domain's focus on demonstrating how models perform tasks based on contextual information. The back translation ""Contextual example"" maintains the semantic integrity of the original term, emphasizing the role of examples in understanding the model's behavior.

""上下文演示"" (Contextual presentation) is a close second, but ""演示"" (presentation) may imply a more passive display rather than the active role of providing examples, which is crucial in AI demonstrations. ""上下文中的示例"" (Examples in context) is also accurate but slightly less concise than ""上下文示例."" Lastly, ""情境演示"" (Situational demonstration) diverges from the specific AI context, as ""情境"" (situational) does not convey the same technical meaning as ""上下文"" (in-context), making it the least suitable option.","['Contextual presentation', 'Contextual example', 'Examples in context', 'Situational demonstration']"
2313,2313,in-context example,上下文示例,0.8888888888888888,9,"[{'word': '上下文示例', 'ratio': 0.8888888888888888}, {'word': '上下文例子', 'ratio': 0.1111111111111111}]",上下文示例,{},[]
2314,2314,in-context learner,上下文学习者,0.6,10,"[{'word': '上下文学习者', 'ratio': 0.6}, {'word': '上下文学习器', 'ratio': 0.4}]",上下文学习者,{},[]
2315,2315,in-degree distribution,入度分布,1.0,10,"[{'word': '入度分布', 'ratio': 1.0}]",入度分布,{},[]
2316,2316,in-distribution,分布内,0.5,10,"[{'word': '分布内', 'ratio': 0.5}, {'word': '在分布内', 'ratio': 0.4}, {'word': '内部分布', 'ratio': 0.1}]",分布内,{},[]
2317,2317,in-domain,领域内的,0.0,10,"[{'word': '域内', 'ratio': 0.5}, {'word': '领域内', 'ratio': 0.4}, {'word': '在领域内', 'ratio': 0.1}]",域内,{},[]
2318,2318,in-domain text,领域内文本,0.5,10,"[{'word': '领域内文本', 'ratio': 0.5}, {'word': '域内文本', 'ratio': 0.5}]",领域内文本,{},[]
2319,2319,in-neighbor,入邻居,0.5,6,"[{'word': '内邻居', 'ratio': 0.5}, {'word': '入邻居', 'ratio': 0.5}]",内邻居,{},[]
2320,2320,in-order traversal,中序遍历,1.0,6,"[{'word': '中序遍历', 'ratio': 1.0}]",中序遍历,{},[]
2321,2321,inception score,初始分数,0.0,6,"[{'word': '开创评分', 'ratio': 0.3333333333333333}, {'word': '初始得分', 'ratio': 0.3333333333333333}, {'word': '创新评分', 'ratio': 0.16666666666666666}, {'word': '开始分数', 'ratio': 0.16666666666666666}]",开创评分,"1. Rank: 开创评分, 创新评分, 初始得分, 开始分数

2. Explanation: The term ""开创评分"" (Create a score) is the best fit for ""inception score"" because it captures the essence of the term in the context of AI and image generation. The word ""开创"" implies a sense of innovation and originality, which aligns well with the concept of measuring the quality and diversity of generated images. In the AI domain, particularly in Generative Adversarial Networks (GANs), the inception score is used to evaluate how well a model generates new, diverse, and high-quality images, which is reflected in the term ""开创评分.""

""创新评分"" (Innovation score) is a close second, as it also conveys a sense of creativity and newness, but it lacks the direct connection to the concept of ""inception"" as effectively as ""开创评分."" 

""初始得分"" (Initial score) and ""开始分数"" (Start score) are less suitable because they do not convey the specific meaning of ""inception"" in this context. They suggest a preliminary or starting point rather than the evaluative and innovative aspects that the inception score represents in AI. Thus, ""开创评分"" is the most contextually appropriate choice.","['Create a score', 'initial score', 'Innovation score', 'start score']"
2322,2322,incremental learning,增量学习,1.0,6,"[{'word': '增量学习', 'ratio': 1.0}]",增量学习,{},[]
2323,2323,incremental parsing,增量解析,1.0,6,"[{'word': '增量解析', 'ratio': 1.0}]",增量解析,{},[]
2324,2324,independent set,独立集,1.0,7,"[{'word': '独立集', 'ratio': 1.0}]",独立集,{},[]
2325,2325,independent variable,独立变量,0.0,7,"[{'word': '自变量', 'ratio': 1.0}]",自变量,{},[]
2326,2326,indicator matrix,指示矩阵,1.0,7,"[{'word': '指示矩阵', 'ratio': 1.0}]",指示矩阵,{},[]
2327,2327,indicator variable,指示变量,1.0,7,"[{'word': '指示变量', 'ratio': 1.0}]",指示变量,{},[]
2328,2328,indicator vector,指示向量,1.0,7,"[{'word': '指示向量', 'ratio': 1.0}]",指示向量,{},[]
2329,2329,induced subgraph,诱导子图,1.0,9,"[{'word': '诱导子图', 'ratio': 1.0}]",诱导子图,{},[]
2330,2330,inducing variable,诱导变量,0.7777777777777778,9,"[{'word': '诱导变量', 'ratio': 0.7777777777777778}, {'word': '引导变量', 'ratio': 0.1111111111111111}, {'word': '引入变量', 'ratio': 0.1111111111111111}]",诱导变量,{},[]
2331,2331,induction hypothesis,归纳假设,1.0,9,"[{'word': '归纳假设', 'ratio': 1.0}]",归纳假设,{},[]
2332,2332,inductive bias,归纳偏置,0.2222222222222222,9,"[{'word': '归纳偏差', 'ratio': 0.7777777777777778}, {'word': '归纳偏置', 'ratio': 0.2222222222222222}]",归纳偏差,{},[]
2333,2333,inductive learning,归纳学习,1.0,9,"[{'word': '归纳学习', 'ratio': 1.0}]",归纳学习,{},[]
2334,2334,inf,最小值,0.0,10,"[{'word': '下确界', 'ratio': 0.9}, {'word': 'inf', 'ratio': 0.1}]",下确界,{},[]
2335,2335,inference,推理,0.7,20,"[{'word': '推理', 'ratio': 0.7}, {'word': '推断', 'ratio': 0.3}]",推理,{},[]
2336,2336,inference algorithm,推断算法,0.2,10,"[{'word': '推理算法', 'ratio': 0.8}, {'word': '推断算法', 'ratio': 0.2}]",推理算法,{},[]
2337,2337,inference machinery,推理机制,0.8,10,"[{'word': '推理机制', 'ratio': 0.8}, {'word': '推断机制', 'ratio': 0.2}]",推理机制,{},[]
2338,2338,inference method,推理方法,0.8888888888888888,9,"[{'word': '推理方法', 'ratio': 0.8888888888888888}, {'word': '推断方法', 'ratio': 0.1111111111111111}]",推理方法,{},[]
2339,2339,inference problem,推理问题,0.8888888888888888,9,"[{'word': '推理问题', 'ratio': 0.8888888888888888}, {'word': '推断问题', 'ratio': 0.1111111111111111}]",推理问题,{},[]
2340,2340,inference procedure,推理过程,0.7777777777777778,9,"[{'word': '推理过程', 'ratio': 0.7777777777777778}, {'word': '推理程序', 'ratio': 0.1111111111111111}, {'word': '推断过程', 'ratio': 0.1111111111111111}]",推理过程,{},[]
2341,2341,inference process,推理过程,0.8888888888888888,9,"[{'word': '推理过程', 'ratio': 0.8888888888888888}, {'word': '推断过程', 'ratio': 0.1111111111111111}]",推理过程,{},[]
2342,2342,inference rule,推理规则,0.8888888888888888,9,"[{'word': '推理规则', 'ratio': 0.8888888888888888}, {'word': '推断规则', 'ratio': 0.1111111111111111}]",推理规则,{},[]
2343,2343,inference stage,推理阶段,1.0,9,"[{'word': '推理阶段', 'ratio': 1.0}]",推理阶段,{},[]
2344,2344,inference task,推理任务,1.0,9,"[{'word': '推理任务', 'ratio': 1.0}]",推理任务,{},[]
2345,2345,inference time,推理时间,1.0,9,"[{'word': '推理时间', 'ratio': 1.0}]",推理时间,{},[]
2346,2346,infinite-horizon,无限时间界限,0.0,9,"[{'word': '无限时域', 'ratio': 0.3333333333333333}, {'word': '无限期', 'ratio': 0.3333333333333333}, {'word': '无限视野', 'ratio': 0.1111111111111111}, {'word': '无限时间范围', 'ratio': 0.1111111111111111}, {'word': '无限视界', 'ratio': 0.1111111111111111}]","""无限时域""","1. Rank: ""无限时域"", ""无限时间范围"", ""无限期"", ""无限视界"", ""无限视野""

2. Explanation: The term ""无限时域"" (infinite time domain) is the best fit for the AI domain-specific usage of ""infinite-horizon."" This is because ""时域"" (time domain) accurately captures the concept of a temporal framework in which decisions are made over an indefinite period, which is essential in the context of Markov decision processes (MDPs). The back translation aligns well with the original term, maintaining the focus on time as a critical factor in decision-making processes.

The second candidate, ""无限时间范围"" (unlimited time range), is also a reasonable choice but is slightly less precise than ""时域,"" as ""范围"" (range) can imply a more general or less structured concept than ""域"" (domain), which is more commonly used in mathematical and computational contexts.

The other candidates, ""无限期"" (indefinitely), ""无限视界"" (infinite horizons), and ""无限视野"" (infinite vision), do not convey the specific temporal aspect required in this context. ""无限期"" lacks the necessary focus on time as a domain, while ""视界"" and ""视野"" suggest a visual or perceptual scope rather than a temporal one, making them less suitable for the technical context of MDPs.","['infinite time domain', 'indefinitely', 'Infinite vision', 'Unlimited time range', 'Infinite horizons']"
2347,2347,influence diagram,影响图,1.0,9,"[{'word': '影响图', 'ratio': 1.0}]",影响图,{},[]
2348,2348,influence function,影响函数,1.0,9,"[{'word': '影响函数', 'ratio': 1.0}]",影响函数,{},[]
2349,2349,information bottleneck,信息瓶颈,1.0,9,"[{'word': '信息瓶颈', 'ratio': 1.0}]",信息瓶颈,{},[]
2350,2350,information content,信息含量,0.0,9,"[{'word': '信息内容', 'ratio': 1.0}]",信息内容,{},[]
2351,2351,information gain,信息增益,1.0,9,"[{'word': '信息增益', 'ratio': 1.0}]",信息增益,{},[]
2352,2352,information retrieval system,信息检索系统,1.0,9,"[{'word': '信息检索系统', 'ratio': 1.0}]",信息检索系统,{},[]
2353,2353,information set,信息集,1.0,9,"[{'word': '信息集', 'ratio': 1.0}]",信息集,{},[]
2354,2354,information theoretic,信息论,0.0,9,"[{'word': '信息论的', 'ratio': 0.8888888888888888}, {'word': '信息理论的', 'ratio': 0.1111111111111111}]",信息论的,{},[]
2355,2355,information theoretic measure,信息论度量,0.8888888888888888,9,"[{'word': '信息论度量', 'ratio': 0.8888888888888888}, {'word': '信息理论度量', 'ratio': 0.1111111111111111}]",信息论度量,{},[]
2356,2356,infoset,信息集,0.7777777777777778,9,"[{'word': '信息集', 'ratio': 0.7777777777777778}, {'word': '信息集合', 'ratio': 0.2222222222222222}]",信息集,{},[]
2357,2357,inhomogeneous Poisson process,非齐次泊松过程,0.9,10,"[{'word': '非齐次泊松过程', 'ratio': 0.9}, {'word': '非均匀泊松过程', 'ratio': 0.1}]",非齐次泊松过程,{},[]
2358,2358,initial distribution,初始分布,1.0,10,"[{'word': '初始分布', 'ratio': 1.0}]",初始分布,{},[]
2359,2359,initial state,初始状态,1.0,10,"[{'word': '初始状态', 'ratio': 1.0}]",初始状态,{},[]
2360,2360,initial state distribution,初始状态分布,1.0,10,"[{'word': '初始状态分布', 'ratio': 1.0}]",初始状态分布,{},[]
2361,2361,initialization,初始化,1.0,10,"[{'word': '初始化', 'ratio': 1.0}]",初始化,{},[]
2362,2362,injective function,单射函数,1.0,10,"[{'word': '单射函数', 'ratio': 1.0}]",单射函数,{},[]
2363,2363,inlier,内点,1.0,10,"[{'word': '内点', 'ratio': 1.0}]",内点,{},[]
2364,2364,inner layer,内层,0.9,10,"[{'word': '内层', 'ratio': 0.9}, {'word': '内部层', 'ratio': 0.1}]",内层,{},[]
2365,2365,inner loop,内部循环,0.6,10,"[{'word': '内部循环', 'ratio': 0.6}, {'word': '内循环', 'ratio': 0.4}]",内部循环,{},[]
2366,2366,inner node,内部节点,1.0,10,"[{'word': '内部节点', 'ratio': 1.0}]",内部节点,{},[]
2367,2367,inner product,内积,1.0,9,"[{'word': '内积', 'ratio': 1.0}]",内积,{},[]
2368,2368,input,输入,1.0,18,"[{'word': '输入', 'ratio': 1.0}]",输入,{},[]
2369,2369,input context,输入上下文,1.0,9,"[{'word': '输入上下文', 'ratio': 1.0}]",输入上下文,{},[]
2370,2370,input datum,输入数据,1.0,9,"[{'word': '输入数据', 'ratio': 1.0}]",输入数据,{},[]
2371,2371,input embedding,输入嵌入,1.0,9,"[{'word': '输入嵌入', 'ratio': 1.0}]",输入嵌入,{},[]
2372,2372,input feature,输入特征,1.0,9,"[{'word': '输入特征', 'ratio': 1.0}]",输入特征,{},[]
2373,2373,input feature vector,输入特征向量,1.0,9,"[{'word': '输入特征向量', 'ratio': 1.0}]",输入特征向量,{},[]
2374,2374,input filter,输入过滤器,1.0,9,"[{'word': '输入过滤器', 'ratio': 1.0}]",输入过滤器,{},[]
2375,2375,input formula,输入公式,1.0,9,"[{'word': '输入公式', 'ratio': 1.0}]",输入公式,{},[]
2376,2376,input gate,输入门,1.0,10,"[{'word': '输入门', 'ratio': 1.0}]",输入门,{},[]
2377,2377,input graph,输入图,1.0,10,"[{'word': '输入图', 'ratio': 1.0}]",输入图,{},[]
2378,2378,input image,输入图像,1.0,10,"[{'word': '输入图像', 'ratio': 1.0}]",输入图像,{},[]
2379,2379,input layer,输入层,1.0,10,"[{'word': '输入层', 'ratio': 1.0}]",输入层,{},[]
2380,2380,input length,输入长度,1.0,10,"[{'word': '输入长度', 'ratio': 1.0}]",输入长度,{},[]
2381,2381,input matrix,输入矩阵,1.0,10,"[{'word': '输入矩阵', 'ratio': 1.0}]",输入矩阵,{},[]
2382,2382,input point,输入点,1.0,10,"[{'word': '输入点', 'ratio': 1.0}]",输入点,{},[]
2383,2383,input position,输入位置,1.0,10,"[{'word': '输入位置', 'ratio': 1.0}]",输入位置,{},[]
2384,2384,input representation,输入表示,1.0,10,"[{'word': '输入表示', 'ratio': 1.0}]",输入表示,{},[]
2385,2385,input resolution,输入分辨率,1.0,10,"[{'word': '输入分辨率', 'ratio': 1.0}]",输入分辨率,{},[]
2386,2386,input sequence,输入序列,1.0,9,"[{'word': '输入序列', 'ratio': 1.0}]",输入序列,{},[]
2387,2387,input tensor,输入张量,1.0,9,"[{'word': '输入张量', 'ratio': 1.0}]",输入张量,{},[]
2388,2388,input text,输入文本,0.8888888888888888,9,"[{'word': '输入文本', 'ratio': 0.8888888888888888}, {'word': '输入文字', 'ratio': 0.1111111111111111}]",输入文本,{},[]
2389,2389,input token,输入令牌,0.1111111111111111,9,"[{'word': '输入标记', 'ratio': 0.8888888888888888}, {'word': '输入令牌', 'ratio': 0.1111111111111111}]",输入标记,{},[]
2390,2390,input vector,输入向量,1.0,9,"[{'word': '输入向量', 'ratio': 1.0}]",输入向量,{},[]
2391,2391,input-output pair,输入-输出对,0.4444444444444444,9,"[{'word': '输入输出对', 'ratio': 0.5555555555555556}, {'word': '输入-输出对', 'ratio': 0.4444444444444444}]",输入输出对,{},[]
2392,2392,instance,实例,1.0,9,"[{'word': '实例', 'ratio': 1.0}]",实例,{},[]
2393,2393,instance level,实例级别,0.8888888888888888,9,"[{'word': '实例级别', 'ratio': 0.8888888888888888}, {'word': '实例级', 'ratio': 0.1111111111111111}]",实例级别,{},[]
2394,2394,instance segmentation,实例分割,1.0,9,"[{'word': '实例分割', 'ratio': 1.0}]",实例分割,{},[]
2395,2395,instance selection,实例选择,1.0,9,"[{'word': '实例选择', 'ratio': 1.0}]",实例选择,{},[]
2396,2396,instance space,实例空间,1.0,5,"[{'word': '实例空间', 'ratio': 1.0}]",实例空间,{},[]
2397,2397,instruction tuning,指令调优,0.4,5,"[{'word': '指令微调', 'ratio': 0.6}, {'word': '指令调优', 'ratio': 0.4}]",指令微调,{},[]
2398,2398,integer linear program,整数线性规划,1.0,5,"[{'word': '整数线性规划', 'ratio': 1.0}]",整数线性规划,{},[]
2399,2399,integer program,整数规划,1.0,5,"[{'word': '整数规划', 'ratio': 1.0}]",整数规划,{},[]
2400,2400,integral image,积分图像,0.8,5,"[{'word': '积分图像', 'ratio': 0.8}, {'word': '积分图', 'ratio': 0.2}]",积分图像,{},[]
2401,2401,integral operator,积分算子,1.0,9,"[{'word': '积分算子', 'ratio': 1.0}]",积分算子,{},[]
2402,2402,integral probability metric,积分概率度量,1.0,9,"[{'word': '积分概率度量', 'ratio': 1.0}]",积分概率度量,{},[]
2403,2403,integrity constraint,完整性约束,1.0,9,"[{'word': '完整性约束', 'ratio': 1.0}]",完整性约束,{},[]
2404,2404,intelligent agent,智能体,0.5555555555555556,9,"[{'word': '智能体', 'ratio': 0.5555555555555556}, {'word': '智能代理', 'ratio': 0.4444444444444444}]",智能体,{},[]
2405,2405,intensity function,强度函数,0.8888888888888888,9,"[{'word': '强度函数', 'ratio': 0.8888888888888888}, {'word': '强度函数 如果还有其他需要帮助的地方，请告诉我！', 'ratio': 0.1111111111111111}]",强度函数,{},[]
2406,2406,intent,意图,1.0,6,"[{'word': '意图', 'ratio': 1.0}]",意图,{},[]
2407,2407,inter-annotator agreement,注释者间一致性,0.3333333333333333,6,"[{'word': '标注者间一致性', 'ratio': 0.6666666666666666}, {'word': '注释者间一致性', 'ratio': 0.3333333333333333}]",标注者间一致性,{},[]
2408,2408,interaction matrix,互动矩阵,0.0,6,"[{'word': '交互矩阵', 'ratio': 1.0}]",交互矩阵,{},[]
2409,2409,interest point,兴趣点,0.6666666666666666,6,"[{'word': '兴趣点', 'ratio': 0.6666666666666666}, {'word': '特征点', 'ratio': 0.3333333333333333}]",兴趣点,{},[]
2410,2410,interior point method,内点法,0.8333333333333334,6,"[{'word': '内点法', 'ratio': 0.8333333333333334}, {'word': '内点法 如果您需要进一步的帮助，请告诉我！', 'ratio': 0.16666666666666666}]",内点法,{},[]
2411,2411,intermediate layer,中间层,1.0,9,"[{'word': '中间层', 'ratio': 1.0}]",中间层,{},[]
2412,2412,intermediate representation,中间表示,1.0,9,"[{'word': '中间表示', 'ratio': 1.0}]",中间表示,{},[]
2413,2413,internal edge,内部边,0.8888888888888888,9,"[{'word': '内部边', 'ratio': 0.8888888888888888}, {'word': '内部节点', 'ratio': 0.1111111111111111}]",内部边,{},[]
2414,2414,internal node,内部节点,1.0,9,"[{'word': '内部节点', 'ratio': 1.0}]",内部节点,{},[]
2415,2415,internal regret,内部遗憾,0.7777777777777778,9,"[{'word': '内部遗憾', 'ratio': 0.7777777777777778}, {'word': '内部节点', 'ratio': 0.1111111111111111}, {'word': '内部后悔', 'ratio': 0.1111111111111111}]",内部遗憾,{},[]
2416,2416,internal representation,内部表征,0.0,7,"[{'word': '内部表示', 'ratio': 1.0}]",内部表示,{},[]
2417,2417,internal state,内部状态,1.0,7,"[{'word': '内部状态', 'ratio': 1.0}]",内部状态,{},[]
2418,2418,interpolation,插值,1.0,7,"[{'word': '插值', 'ratio': 1.0}]",插值,{},[]
2419,2419,interpretability,可解释性,1.0,16,"[{'word': '可解释性', 'ratio': 1.0}]",可解释性,{},[]
2420,2420,interpretation function,解释函数,0.8888888888888888,9,"[{'word': '解释函数', 'ratio': 0.8888888888888888}, {'word': '交并比', 'ratio': 0.1111111111111111}]",解释函数,{},[]
2421,2421,intersection-over-union,交并比,0.8888888888888888,9,"[{'word': '交并比', 'ratio': 0.8888888888888888}, {'word': '区间估计', 'ratio': 0.1111111111111111}]",交并比,{},[]
2422,2422,interval estimate,区间估计,1.0,9,"[{'word': '区间估计', 'ratio': 1.0}]",区间估计,{},[]
2423,2423,intractability,无解性,0.0,6,"[{'word': '不可处理性', 'ratio': 0.6666666666666666}, {'word': '难处理性', 'ratio': 0.16666666666666666}, {'word': '难解性', 'ratio': 0.16666666666666666}]",不可处理性,{},[]
2424,2424,intrinsic,内参数,0.0,6,"[{'word': '内在的', 'ratio': 0.8333333333333334}, {'word': '内参', 'ratio': 0.16666666666666666}]",内在的,{},[]
2425,2425,intrinsic camera parameter,相机内参,0.0,6,"[{'word': '内在相机参数', 'ratio': 0.6666666666666666}, {'word': '内部相机参数', 'ratio': 0.16666666666666666}, {'word': '相机内参数', 'ratio': 0.16666666666666666}]",内在相机参数,{},[]
2426,2426,intrinsic dimension subspace,内在维度子空间,1.0,6,"[{'word': '内在维度子空间', 'ratio': 1.0}]",内在维度子空间,{},[]
2427,2427,intrinsic dimensionality,内在维度,1.0,6,"[{'word': '内在维度', 'ratio': 1.0}]",内在维度,{},[]
2428,2428,intrinsic evaluation,内在评估,0.6666666666666666,9,"[{'word': '内在评估', 'ratio': 0.6666666666666666}, {'word': '内部评估', 'ratio': 0.2222222222222222}, {'word': '内在评价', 'ratio': 0.1111111111111111}]",内在评估,{},[]
2429,2429,intrinsic image,固有图像,0.0,9,"[{'word': '内在图像', 'ratio': 0.5555555555555556}, {'word': '本征图像', 'ratio': 0.2222222222222222}, {'word': '本质图像', 'ratio': 0.1111111111111111}, {'word': '内在意象', 'ratio': 0.1111111111111111}]",内在图像,{},[]
2430,2430,intrinsic parameter,内参,0.4444444444444444,9,"[{'word': '内参', 'ratio': 0.4444444444444444}, {'word': '内在参数', 'ratio': 0.3333333333333333}, {'word': '内参数', 'ratio': 0.2222222222222222}]",内参,"1. Rank: 内参, 内在参数, 内参数

2. Explanation: The term ""内参"" (back translated as ""internal reference"") is the best fit for the AI domain, particularly in the context of computer vision and camera calibration. In the field of computer vision, ""内参"" is a widely accepted abbreviation for ""intrinsic parameters,"" which refers to the internal characteristics of a camera, such as focal length and optical center. This term is commonly used in both academic literature and practical applications, making it semantically accurate and contextually appropriate.

The second candidate, ""内在参数"" (back translated as ""intrinsic parameters""), while accurate, is less commonly used in the field and may not resonate as well with practitioners familiar with the standard terminology. The third candidate, ""内参数"" (back translated as ""internal parameters""), is also less precise and could lead to confusion, as it does not specifically convey the concept of intrinsic camera parameters as effectively as ""内参."" Therefore, ""内参"" is the most contextually fitting and semantically accurate choice in the AI domain.","['internal reference', 'intrinsic parameters', 'Internal parameters']"
2431,2431,inverse document frequency,逆文档频率,1.0,9,"[{'word': '逆文档频率', 'ratio': 1.0}]",逆文档频率,{},[]
2432,2432,inverse dynamic model,逆动力学模型,0.3333333333333333,9,"[{'word': '逆动态模型', 'ratio': 0.5555555555555556}, {'word': '逆动力学模型', 'ratio': 0.3333333333333333}, {'word': '逆向动态模型', 'ratio': 0.1111111111111111}]",逆动态模型,{},[]
2433,2433,inverse problem,逆问题,0.7,10,"[{'word': '逆问题', 'ratio': 0.7}, {'word': '反问题', 'ratio': 0.3}]",逆问题,{},[]
2434,2434,inverse rendering,逆向渲染,0.5,10,"[{'word': '逆向渲染', 'ratio': 0.5}, {'word': '反向渲染', 'ratio': 0.3}, {'word': '逆渲染', 'ratio': 0.2}]",逆向渲染,{},[]
2435,2435,inverse role,逆角色,0.7,10,"[{'word': '逆角色', 'ratio': 0.7}, {'word': '逆向角色', 'ratio': 0.3}]",逆角色,{},[]
2436,2436,inverse square root learning rate schedule,逆平方根学习率调度,0.5,10,"[{'word': '逆平方根学习率调度', 'ratio': 0.5}, {'word': '反平方根学习率调度', 'ratio': 0.4}, {'word': '逆平方根学习率计划', 'ratio': 0.1}]",逆平方根学习率调度,{},[]
2437,2437,inverse square root learning rate scheduler,倒数平方根学习率调度器,0.0,10,"[{'word': '逆平方根学习率调度器', 'ratio': 0.6}, {'word': '反平方根学习率调度器', 'ratio': 0.4}]",逆平方根学习率调度器,{},[]
2438,2438,inverse transform sampling,逆变换采样,0.5,8,"[{'word': '逆变换采样', 'ratio': 0.5}, {'word': '反向变换抽样', 'ratio': 0.25}, {'word': '逆变换抽样', 'ratio': 0.125}, {'word': '反变换采样', 'ratio': 0.125}]",逆变换采样,{},[]
2439,2439,inverted index,倒排索引,0.75,8,"[{'word': '倒排索引', 'ratio': 0.75}, {'word': '反向索引', 'ratio': 0.25}]",倒排索引,{},[]
2440,2440,inverted list,倒排列表,0.75,8,"[{'word': '倒排列表', 'ratio': 0.75}, {'word': '反向列表', 'ratio': 0.25}]",倒排列表,{},[]
2441,2441,invertible map,可逆映射,1.0,8,"[{'word': '可逆映射', 'ratio': 1.0}]",可逆映射,{},[]
2442,2442,invertible matrix,可逆矩阵,1.0,8,"[{'word': '可逆矩阵', 'ratio': 1.0}]",可逆矩阵,{},[]
2443,2443,iobj,间接宾语,1.0,6,"[{'word': '间接宾语', 'ratio': 1.0}]",间接宾语,{},[]
2444,2444,iso-surface extraction,等值面提取,1.0,6,"[{'word': '等值面提取', 'ratio': 1.0}]",等值面提取,{},[]
2445,2445,isotropic Gaussians,各向同性高斯分布,1.0,6,"[{'word': '各向同性高斯分布', 'ratio': 1.0}]",各向同性高斯分布,{},[]
2446,2446,itemset,项集,1.0,6,"[{'word': '项集', 'ratio': 1.0}]",项集,{},[]
2447,2447,iterate,迭代,1.0,6,"[{'word': '迭代', 'ratio': 1.0}]",迭代,{},[]
2448,2448,iterated conditional mode,迭代条件模式,1.0,10,"[{'word': '迭代条件模式', 'ratio': 1.0}]",迭代条件模式,{},[]
2449,2449,iteration,迭代,1.0,10,"[{'word': '迭代', 'ratio': 1.0}]",迭代,{},[]
2450,2450,iteration complexity,迭代复杂度,1.0,10,"[{'word': '迭代复杂度', 'ratio': 1.0}]",迭代复杂度,{},[]
2451,2451,iteration counter,迭代计数器,1.0,10,"[{'word': '迭代计数器', 'ratio': 1.0}]",迭代计数器,{},[]
2452,2452,iterative algorithm,迭代算法,1.0,10,"[{'word': '迭代算法', 'ratio': 1.0}]",迭代算法,{},[]
2453,2453,iterative deepening,迭代加深,0.875,8,"[{'word': '迭代加深', 'ratio': 0.875}, {'word': '迭代深化', 'ratio': 0.125}]",迭代加深,{},[]
2454,2454,iterative optimization,迭代优化,1.0,8,"[{'word': '迭代优化', 'ratio': 1.0}]",迭代优化,{},[]
2455,2455,iterative optimization algorithm,迭代优化算法,1.0,8,"[{'word': '迭代优化算法', 'ratio': 1.0}]",迭代优化算法,{},[]
2456,2456,iterative training algorithm,迭代训练算法,1.0,8,"[{'word': '迭代训练算法', 'ratio': 1.0}]",迭代训练算法,{},[]
2457,2457,iteratively reweighted least square,迭代重新加权最小二乘法,0.0,8,"[{'word': '迭代加权最小二乘法', 'ratio': 0.375}, {'word': '迭代重加权最小二乘', 'ratio': 0.375}, {'word': '迭代重加权最小二乘法', 'ratio': 0.25}]",迭代重加权最小二乘法,"1. Rank: 迭代重加权最小二乘法, 迭代重加权最小二乘, 迭代加权最小二乘法

2. Explanation: The term ""迭代重加权最小二乘法"" is the best fit because it accurately captures the concept of ""iteratively reweighted least squares"" in a way that aligns with established terminology in the AI and statistical domains. The use of ""重加权"" (reweighted) directly corresponds to the ""reweighted"" in the English term, ensuring semantic accuracy. Additionally, the inclusion of ""法"" (method) at the end of the term is common in mathematical and statistical terminology in Chinese, making it more recognizable and appropriate for academic contexts. The back translation of this term as ""iterative reweighted least squares"" is also precise, maintaining the original meaning and context. 

The second candidate, ""迭代重加权最小二乘"", lacks the ""法"" which diminishes its clarity as a method or technique, while the first candidate, ""迭代加权最小二乘法"", introduces a slight deviation by using ""加权"" instead of ""重加权"", which could lead to confusion regarding the specific nature of the weighting process involved in the method. Thus, ""迭代重加权最小二乘法"" stands out as the most contextually and semantically accurate choice.","['iterative weighted least squares', 'iterative reweighted least squares', 'iterative reweighted least squares']"
2458,2458,joint density,联合密度,0.8888888888888888,9,"[{'word': '联合密度', 'ratio': 0.8888888888888888}, {'word': '关节密度', 'ratio': 0.1111111111111111}]",联合密度,{},[]
2459,2459,joint distribution,联合分布,0.8888888888888888,9,"[{'word': '联合分布', 'ratio': 0.8888888888888888}, {'word': '联合分配', 'ratio': 0.1111111111111111}]",联合分布,{},[]
2460,2460,joint embedding space,联合嵌入空间,1.0,9,"[{'word': '联合嵌入空间', 'ratio': 1.0}]",联合嵌入空间,{},[]
2461,2461,joint encoding,联合编码,1.0,9,"[{'word': '联合编码', 'ratio': 1.0}]",联合编码,{},[]
2462,2462,joint entropy,联合熵,1.0,9,"[{'word': '联合熵', 'ratio': 1.0}]",联合熵,{},[]
2463,2463,joint inference,联合推理,0.375,8,"[{'word': '联合推断', 'ratio': 0.5}, {'word': '联合推理', 'ratio': 0.375}, {'word': '联合推理 联合学习', 'ratio': 0.125}]",联合推断,{},[]
2464,2464,joint learning,联合学习,0.875,8,"[{'word': '联合学习', 'ratio': 0.875}, {'word': '共同学习', 'ratio': 0.125}]",联合学习,{},[]
2465,2465,joint learning algorithm,联合学习算法,0.875,8,"[{'word': '联合学习算法', 'ratio': 0.875}, {'word': 'learning algorithm', 'ratio': 0.125}]",联合学习算法,{},[]
2466,2466,joint likelihood,联合似然,1.0,8,"[{'word': '联合似然', 'ratio': 1.0}]",联合似然,{},[]
2467,2467,joint model,联合模型,1.0,8,"[{'word': '联合模型', 'ratio': 1.0}]",联合模型,{},[]
2468,2468,joint policy,联合策略,0.875,8,"[{'word': '联合策略', 'ratio': 0.875}, {'word': '凸凹', 'ratio': 0.125}]",联合策略,{},[]
2469,2469,joint probability,联合概率,0.875,8,"[{'word': '联合概率', 'ratio': 0.875}, {'word': '凸凹', 'ratio': 0.125}]",联合概率,{},[]
2470,2470,joint probability distribution,联合概率分布,0.875,8,"[{'word': '联合概率分布', 'ratio': 0.875}, {'word': '凸凹', 'ratio': 0.125}]",联合概率分布,{},[]
2471,2471,joint probability matrix,联合概率矩阵,0.875,8,"[{'word': '联合概率矩阵', 'ratio': 0.875}, {'word': '凸凹', 'ratio': 0.125}]",联合概率矩阵,{},[]
2472,2472,joint probability table,联合概率表,0.875,8,"[{'word': '联合概率表', 'ratio': 0.875}, {'word': '凸凹', 'ratio': 0.125}]",联合概率表,{},[]
2473,2473,joint semantic space,联合语义空间,1.0,7,"[{'word': '联合语义空间', 'ratio': 1.0}]",联合语义空间,{},[]
2474,2474,junction tree,连接树,0.8571428571428571,7,"[{'word': '连接树', 'ratio': 0.8571428571428571}, {'word': '结点树', 'ratio': 0.14285714285714285}]",连接树,{},[]
2475,2475,junction tree algorithm,联结树算法 (junction tree algorithm),0.0,7,"[{'word': '连接树算法', 'ratio': 0.8571428571428571}, {'word': '结点树算法', 'ratio': 0.14285714285714285}]",连接树算法,{},[]
2476,2476,k nearest neighbor,k近邻,0.1428571428571428,7,"[{'word': 'k 最近邻', 'ratio': 0.42857142857142855}, {'word': 'k最近邻', 'ratio': 0.2857142857142857}, {'word': '最近邻', 'ratio': 0.14285714285714285}, {'word': 'k近邻', 'ratio': 0.14285714285714285}]",k 最近邻,"1. Rank: k 最近邻, k最近邻, k近邻, 最近邻

2. Explanation: The term ""k 最近邻"" is the best fit because it maintains both semantic accuracy and contextual relevance in the AI domain. The inclusion of ""k"" explicitly indicates the number of neighbors being considered, which is crucial in the context of the k-nearest neighbors algorithm. The term ""最近邻"" alone lacks the specificity of the ""k"" parameter, which is essential for understanding the algorithm's functionality. ""k最近邻"" is also a valid option, but the lack of a space may affect readability in some contexts. ""k近邻"" omits the ""最近"" (nearest) descriptor, which is important for conveying the concept of proximity in the algorithm. Lastly, ""最近邻"" is too vague as it does not specify the ""k"" aspect, making it less suitable for technical discussions in AI. Therefore, ""k 最近邻"" is the most accurate and contextually appropriate choice.","['k nearest neighbor', 'k nearest neighbor', 'nearest neighbor', 'k nearest neighbor']"
2477,2477,k-Center,k-中心,0.4285714285714285,7,"[{'word': 'k-中心', 'ratio': 0.42857142857142855}, {'word': 'k中心', 'ratio': 0.42857142857142855}, {'word': 'k-中心问题', 'ratio': 0.14285714285714285}]",k-中心问题,"1. Rank: k-中心问题, k-中心, k中心

2. Explanation: The term ""k-中心问题"" (k-center problem) is the best fit because it explicitly includes the word ""问题"" (problem), which clarifies that it refers to a specific problem in the context of optimization and facility location. This aligns well with the AI domain, where precise terminology is crucial for understanding the nature of the problem being addressed. The inclusion of ""问题"" helps to contextualize the term within mathematical and algorithmic discussions, making it more informative for readers familiar with the field.

The term ""k-中心"" (k-center) is also a strong candidate, as it retains the core concept of the k-center without the additional context of it being a problem. However, it may lack the specificity needed in academic or technical discussions. 

Lastly, ""k中心"" (k center) is the least preferred option because it omits the hyphen and does not convey the same level of clarity as the other two options. The lack of a hyphen can lead to confusion, as it may not be immediately clear that it refers to a specific mathematical concept rather than a general term. 

Overall, ""k-中心问题"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['k-center', 'k center', 'k-center problem']"
2478,2478,k-best list,k-最优候选列表,0.0,7,"[{'word': 'k-最佳列表', 'ratio': 0.42857142857142855}, {'word': 'k-best 列表', 'ratio': 0.2857142857142857}, {'word': 'k最优列表', 'ratio': 0.2857142857142857}]",k-最佳列表,"1. Rank: k-最佳列表, k-best 列表, k最优列表

2. Explanation: The term ""k-最佳列表"" is the best fit because it accurately captures the meaning of ""k-best list"" in the context of AI and computational linguistics. The use of ""最佳"" (optimal) directly corresponds to the concept of selecting the best candidates from a list, which is essential in parsing and ranking tasks. The back translation to ""k-best list"" maintains the original term's integrity, ensuring clarity in communication within the AI domain. 

The second candidate, ""k-best 列表,"" while also accurate, does not convey the same level of specificity regarding the quality of the candidates as ""最佳"" does. It simply translates ""best"" as ""best"" without the added nuance of ""optimal."" 

The third candidate, ""k最优列表,"" translates to ""k optimal list,"" which, although it conveys a similar meaning, is less commonly used in the context of AI and may not align as closely with established terminology in the field. The term ""最优"" (optimal) is more formal and less frequently used in practical applications compared to ""最佳."" 

Overall, ""k-最佳列表"" is the most contextually appropriate and semantically accurate choice for the AI domain.","['k-best list', 'k-best list', 'k optimal list']"
2479,2479,k-best parsing,k-最佳分析,0.0,7,"[{'word': 'k-最佳解析', 'ratio': 0.42857142857142855}, {'word': 'k-best 解析', 'ratio': 0.2857142857142857}, {'word': 'k最优解析', 'ratio': 0.2857142857142857}]",k-最佳解析,"1. Rank: k-最佳解析, k-best 解析, k最优解析

2. Explanation: The term ""k-最佳解析"" is the best fit because it accurately captures the meaning of ""k-best parsing"" in the context of AI and computational linguistics. The term ""最佳"" translates to ""best,"" which aligns well with the concept of selecting the top k derivations in parsing. This term maintains the semantic integrity of the original English term while also being contextually appropriate for the AI domain. 

The second candidate, ""k-best 解析,"" retains the English term ""k-best,"" which may be useful in some contexts, but it lacks the clarity and specificity that ""最佳"" provides. The third candidate, ""k最优解析,"" translates to ""k-optimal analysis,"" which could imply a different meaning, as ""最优"" (optimal) may suggest a focus on the best possible solution rather than the top k solutions. Therefore, ""k-最佳解析"" is the most semantically accurate and contextually fitting choice for the AI terminology in question.","['k-best resolution', 'k-best analysis', 'k-optimal analysis']"
2480,2480,k-d tree,k-d树,0.7142857142857143,7,"[{'word': 'k-d树', 'ratio': 0.7142857142857143}, {'word': 'k-d 树', 'ratio': 0.2857142857142857}]",k-d树,{},[]
2481,2481,k-hop neighbor,k跳邻居,0.4285714285714285,7,"[{'word': 'k跳邻居', 'ratio': 0.42857142857142855}, {'word': 'k-hop邻居', 'ratio': 0.2857142857142857}, {'word': 'k-跳邻居', 'ratio': 0.14285714285714285}, {'word': 'k-hop 邻居', 'ratio': 0.14285714285714285}]",k-hop邻居,"1. Rank: k-hop邻居, k-hop 邻居, k跳邻居, k-跳邻居

2. Explanation: The term ""k-hop邻居"" is the best fit because it maintains the original English term ""k-hop"" intact while directly translating ""neighbor"" to ""邻居,"" which is the standard term used in graph theory and AI contexts. This combination preserves the semantic meaning and context of the term, making it easily understandable for Chinese-speaking audiences familiar with AI terminology. The use of ""k-hop"" as a prefix is consistent with how similar terms are used in the field, ensuring clarity and precision. The other candidates either introduce unnecessary variations (like ""k-跳邻居"" or ""k跳邻居"") or separate the components in a way that could lead to confusion, as seen in ""k-hop 邻居."" Thus, ""k-hop邻居"" is the most contextually appropriate and semantically accurate choice.","['k-hop neighbor', 'k-hop neighbor', 'k-hop neighbor', 'k-hop neighbor']"
2482,2482,k-means clustering algorithm,k均值聚类算法,0.4285714285714285,7,"[{'word': 'k-均值聚类算法', 'ratio': 0.42857142857142855}, {'word': 'k均值聚类算法', 'ratio': 0.42857142857142855}, {'word': 'k-means 聚类算法', 'ratio': 0.14285714285714285}]",k-均值聚类算法,"1. Rank: k-均值聚类算法, k均值聚类算法, k-means 聚类算法

2. Explanation: The first translation, ""k-均值聚类算法,"" is the best fit because it accurately retains the structure of the original English term while also being semantically clear in Chinese. The inclusion of the hyphen (""k-"") aligns with the common usage in the AI domain, where ""k-means"" is often referred to with a hyphen in both English and Chinese contexts. This term is widely recognized in academic and technical literature, making it the most appropriate choice for clarity and consistency. The second option, ""k均值聚类算法,"" omits the hyphen, which may lead to slight confusion, as it does not follow the conventional representation. The third option, ""k-means 聚类算法,"" while still understandable, is less common and may not be as readily recognized in the AI community. Therefore, ""k-均值聚类算法"" is the most semantically accurate and contextually fitting translation.","['k-means clustering algorithm', 'k-means clustering algorithm', 'k-means clustering algorithm']"
2483,2483,k-nearest neighbor,k-最近邻,0.2222222222222222,9,"[{'word': 'k最近邻', 'ratio': 0.3333333333333333}, {'word': 'k近邻', 'ratio': 0.3333333333333333}, {'word': 'k-最近邻', 'ratio': 0.2222222222222222}, {'word': 'k 最近邻', 'ratio': 0.1111111111111111}]",k最近邻,"1. Rank: k最近邻, k-最近邻, k近邻, k 最近邻

2. Explanation: The term ""k最近邻"" is the best fit for the translation of ""k-nearest neighbor"" because it accurately captures the semantic meaning of the original term while maintaining a clear and concise structure. The use of ""最近邻"" directly translates to ""nearest neighbor,"" which is a standard term in machine learning and AI contexts. This term is widely recognized in the field, making it the most appropriate choice for professionals and researchers.

The second candidate, ""k-最近邻,"" is also a good option as it retains the hyphen, which is commonly used in the English term. However, the hyphen is not necessary in Chinese, and its inclusion may not be as familiar to all readers. 

The third candidate, ""k近邻,"" omits the ""最近"" (nearest) part, which could lead to ambiguity, as ""近邻"" alone may not convey the same specificity as ""最近邻."" 

Lastly, ""k 最近邻"" includes an unnecessary space, which is not standard in Chinese writing and could detract from its professional appearance. 

Overall, ""k最近邻"" is the most semantically accurate and contextually appropriate term for the AI domain.","['k nearest neighbor', 'k nearest neighbor', 'k-nearest neighbor', 'k nearest neighbor']"
2484,2484,k-nearest neighbor classifier,k最近邻分类器,0.3333333333333333,9,"[{'word': 'k最近邻分类器', 'ratio': 0.3333333333333333}, {'word': 'k近邻分类器', 'ratio': 0.3333333333333333}, {'word': 'k-最近邻分类器', 'ratio': 0.2222222222222222}, {'word': 'k 最近邻分类器', 'ratio': 0.1111111111111111}]",k最近邻分类器,"1. Rank: k最近邻分类器, k-最近邻分类器, k近邻分类器, k 最近邻分类器

2. Explanation: The term ""k最近邻分类器"" is the best fit because it accurately captures the semantic meaning of ""k-nearest neighbor classifier"" while maintaining a clear and concise structure that is commonly used in the AI domain. The use of ""最近邻"" (nearest neighbor) is standard in machine learning literature, making it easily recognizable to practitioners. The absence of a space in ""k最近邻"" also aligns with typical formatting in technical contexts, where terms are often presented without unnecessary spaces for clarity and brevity. 

The second choice, ""k-最近邻分类器,"" is also a strong candidate, as it retains the hyphen from the English term, which can be useful for clarity in some contexts. However, the hyphen is less common in Chinese technical terminology, making it slightly less preferable than the first option.

The third option, ""k近邻分类器,"" omits the ""最近"" (recent) part, which could lead to a loss of specificity in the term, making it less accurate semantically. 

Lastly, ""k 最近邻分类器"" includes an unnecessary space, which is not typical in technical writing and could detract from its professional appearance. Thus, while all translations are understandable, ""k最近邻分类器"" stands out as the most contextually appropriate and semantically accurate choice.","['k nearest neighbor classifier', 'k nearest neighbor classifier', 'k-nearest neighbor classifier', 'k nearest neighbor classifier']"
2485,2485,k-nearest neighbor graph,k最近邻图,0.3333333333333333,9,"[{'word': 'k最近邻图', 'ratio': 0.3333333333333333}, {'word': 'k近邻图', 'ratio': 0.3333333333333333}, {'word': 'k-最近邻图', 'ratio': 0.2222222222222222}, {'word': 'k 最近邻图', 'ratio': 0.1111111111111111}]",k最近邻图,"1. Rank: k最近邻图, k-最近邻图, k近邻图, k 最近邻图

2. Explanation: The term ""k最近邻图"" is the best fit because it accurately preserves the semantic meaning of the original English term ""k-nearest neighbor graph"" while maintaining a clear and concise structure. The use of ""最近邻"" directly translates to ""nearest neighbor,"" which is a standard term in the AI and machine learning domains, ensuring that it is easily understood by professionals in the field. 

The second candidate, ""k-最近邻图,"" is also a good option as it includes the hyphen, which aligns with the original English term's formatting. However, the hyphen is not commonly used in Chinese terminology, making ""k最近邻图"" slightly more preferable.

The third candidate, ""k近邻图,"" omits the ""最近"" (nearest) part, which could lead to ambiguity, as ""近邻"" alone may not convey the same specificity as ""最近邻."" 

Lastly, ""k 最近邻图"" includes an unnecessary space, which is not standard in technical terminology and could lead to confusion. 

Overall, ""k最近邻图"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['k nearest neighbor graph', 'k nearest neighbor graph', 'k-nearest neighbor graph', 'k nearest neighbor graph']"
2486,2486,kernel approximation,核近似,1.0,9,"[{'word': '核近似', 'ratio': 1.0}]",核近似,{},[]
2487,2487,kernel bandwidth,核带宽,1.0,9,"[{'word': '核带宽', 'ratio': 1.0}]",核带宽,{},[]
2488,2488,kernel classifier,核分类器,1.0,8,"[{'word': '核分类器', 'ratio': 1.0}]",核分类器,{},[]
2489,2489,kernel density,核密度估计,0.0,8,"[{'word': '核密度', 'ratio': 1.0}]",核密度,{},[]
2490,2490,kernel density estimate,核密度估计,1.0,8,"[{'word': '核密度估计', 'ratio': 1.0}]",核密度估计,{},[]
2491,2491,kernel density estimation,核密度估计,0.75,8,"[{'word': '核密度估计', 'ratio': 0.75}, {'word': '核密度估计法', 'ratio': 0.25}]",核密度估计,{},[]
2492,2492,kernel evaluation,核函数评估,0.125,8,"[{'word': '核评估', 'ratio': 0.75}, {'word': '核函数评估', 'ratio': 0.125}, {'word': '核函数计算', 'ratio': 0.125}]",核评估,{},[]
2493,2493,kernel learning,核学习,1.0,6,"[{'word': '核学习', 'ratio': 1.0}]",核学习,{},[]
2494,2494,kernel learning problem,核学习问题,1.0,6,"[{'word': '核学习问题', 'ratio': 1.0}]",核学习问题,{},[]
2495,2495,kernel machine,核机器,1.0,6,"[{'word': '核机器', 'ratio': 1.0}]",核机器,{},[]
2496,2496,kernel matrix,核矩阵,1.0,6,"[{'word': '核矩阵', 'ratio': 1.0}]",核矩阵,{},[]
2497,2497,kernel method,核方法,1.0,6,"[{'word': '核方法', 'ratio': 1.0}]",核方法,{},[]
2498,2498,kernel operation,核操作,1.0,7,"[{'word': '核操作', 'ratio': 1.0}]",核操作,{},[]
2499,2499,kernel operator,核算子,1.0,7,"[{'word': '核算子', 'ratio': 1.0}]",核算子,{},[]
2500,2500,kernel parameter,核参数,1.0,7,"[{'word': '核参数', 'ratio': 1.0}]",核参数,{},[]
2501,2501,kernel regression,核回归,1.0,7,"[{'word': '核回归', 'ratio': 1.0}]",核回归,{},[]
2502,2502,kernel ridge regression,核岭回归,0.8571428571428571,7,"[{'word': '核岭回归', 'ratio': 0.8571428571428571}, {'word': '核脊回归', 'ratio': 0.14285714285714285}]",核岭回归,{},[]
2503,2503,kernel size,卷积核尺寸,0.0,8,"[{'word': '核大小', 'ratio': 0.875}, {'word': '内核大小', 'ratio': 0.125}]",核大小,{},[]
2504,2504,kernel smoothing,核平滑,1.0,8,"[{'word': '核平滑', 'ratio': 1.0}]",核平滑,{},[]
2505,2505,kernel space,核空间,0.875,8,"[{'word': '核空间', 'ratio': 0.875}, {'word': '内核空间', 'ratio': 0.125}]",核空间,{},[]
2506,2506,kernel spectrum,核谱,1.0,8,"[{'word': '核谱', 'ratio': 1.0}]",核谱,{},[]
2507,2507,kernel trick,核技巧,0.875,8,"[{'word': '核技巧', 'ratio': 0.875}, {'word': '内核技巧', 'ratio': 0.125}]",核技巧,{},[]
2508,2508,kernel value,核值,0.7,10,"[{'word': '核值', 'ratio': 0.7}, {'word': '核函数值', 'ratio': 0.3}]",核值,{},[]
2509,2509,kernel weight,核权重,1.0,10,"[{'word': '核权重', 'ratio': 1.0}]",核权重,{},[]
2510,2510,kernel width,核宽度,0.8,10,"[{'word': '核宽度', 'ratio': 0.8}, {'word': '核权重', 'ratio': 0.1}, {'word': '核宽', 'ratio': 0.1}]",核宽度,{},[]
2511,2511,kernel-based classification,核基分类,0.0,10,"[{'word': '基于核的分类', 'ratio': 1.0}]",基于核的分类,{},[]
2512,2512,key,关键,0.0,10,"[{'word': '键', 'ratio': 1.0}]",键,{},[]
2513,2513,keypoint,关键点,0.875,8,"[{'word': '关键点', 'ratio': 0.875}, {'word': '特征点', 'ratio': 0.125}]",关键点,{},[]
2514,2514,keypoint detection,关键点检测,0.875,8,"[{'word': '关键点检测', 'ratio': 0.875}, {'word': '特征点检测', 'ratio': 0.125}]",关键点检测,{},[]
2515,2515,keypoint detector,关键点检测器,0.875,8,"[{'word': '关键点检测器', 'ratio': 0.875}, {'word': '特征点检测器', 'ratio': 0.125}]",关键点检测器,{},[]
2516,2516,keypoint location,关键点位置,0.875,8,"[{'word': '关键点位置', 'ratio': 0.875}, {'word': '特征点位置', 'ratio': 0.125}]",关键点位置,{},[]
2517,2517,keypoint match,关键点匹配,0.875,8,"[{'word': '关键点匹配', 'ratio': 0.875}, {'word': '特征点匹配', 'ratio': 0.125}]",关键点匹配,{},[]
2518,2518,knapsack problem,背包问题,1.0,9,"[{'word': '背包问题', 'ratio': 1.0}]",背包问题,{},[]
2519,2519,knowledge compilation,知识编译,0.8888888888888888,9,"[{'word': '知识编译', 'ratio': 0.8888888888888888}, {'word': '知识汇编', 'ratio': 0.1111111111111111}]",知识编译,{},[]
2520,2520,knowledge element,知识元素,0.8888888888888888,9,"[{'word': '知识元素', 'ratio': 0.8888888888888888}, {'word': '知识要素', 'ratio': 0.1111111111111111}]",知识元素,{},[]
2521,2521,knowledge graph completion,知识图谱补全,1.0,9,"[{'word': '知识图谱补全', 'ratio': 1.0}]",知识图谱补全,{},[]
2522,2522,knowledge representation,知识表示,1.0,7,"[{'word': '知识表示', 'ratio': 1.0}]",知识表示,{},[]
2523,2523,knowledge transfer,知识转移,0.0,7,"[{'word': '知识迁移', 'ratio': 0.8571428571428571}, {'word': '知识表示', 'ratio': 0.14285714285714285}]",知识迁移,{},[]
2524,2524,label,标签,1.0,7,"[{'word': '标签', 'ratio': 1.0}]",标签,{},[]
2525,2525,label distribution,标签分布,1.0,7,"[{'word': '标签分布', 'ratio': 1.0}]",标签分布,{},[]
2526,2526,label embedding,标签嵌入,1.0,7,"[{'word': '标签嵌入', 'ratio': 1.0}]",标签嵌入,{},[]
2527,2527,label noise,标签噪声,1.0,8,"[{'word': '标签噪声', 'ratio': 1.0}]",标签噪声,{},[]
2528,2528,label sequence,标签序列,1.0,8,"[{'word': '标签序列', 'ratio': 1.0}]",标签序列,{},[]
2529,2529,label smoothing,标签平滑,1.0,8,"[{'word': '标签平滑', 'ratio': 1.0}]",标签平滑,{},[]
2530,2530,label space,标签空间,1.0,8,"[{'word': '标签空间', 'ratio': 1.0}]",标签空间,{},[]
2531,2531,label token,标签令牌,0.5,8,"[{'word': '标签令牌', 'ratio': 0.5}, {'word': '标签标记', 'ratio': 0.25}, {'word': '标签词元', 'ratio': 0.25}]",标签令牌,{},[]
2532,2532,label vector,标签向量,0.8888888888888888,9,"[{'word': '标签向量', 'ratio': 0.8888888888888888}, {'word': '标签矢量', 'ratio': 0.1111111111111111}]",标签向量,{},[]
2533,2533,labeled datum,标注数据,0.3333333333333333,9,"[{'word': '标记数据', 'ratio': 0.5555555555555556}, {'word': '标注数据', 'ratio': 0.3333333333333333}, {'word': '已标注数据', 'ratio': 0.1111111111111111}]",标记数据,{},[]
2534,2534,labeled example,标签示例,0.0,9,"[{'word': '标记示例', 'ratio': 0.3333333333333333}, {'word': '标记样本', 'ratio': 0.2222222222222222}, {'word': '标注样本', 'ratio': 0.2222222222222222}, {'word': '已标注示例', 'ratio': 0.1111111111111111}, {'word': '标注示例', 'ratio': 0.1111111111111111}]",已标注示例,"1. Rank: 已标注示例, 标注示例, 标记样本, 标记示例, 标注样本

2. Explanation: The term ""已标注示例"" (back translated as ""Labeled example"") is the best fit because it accurately conveys the concept of an example that has been labeled, which is crucial in the context of machine learning and AI. The prefix ""已"" (meaning ""already"") emphasizes that the labeling process has been completed, aligning well with the context where labeled examples are used for training models. 

""标注示例"" (back translated as ""Annotation example"") is also a strong candidate, as it suggests that the examples have been annotated, but it lacks the explicit indication that the labeling is complete, which is important in the context of the provided text.

The other candidates, such as ""标记样本"" (Markup samples) and ""标记示例"" (Markup example), introduce ambiguity with the term ""标记"" (markup), which is less commonly used in the AI domain to refer to labeled data. ""标注样本"" (Label samples) is somewhat accurate but does not convey the same level of specificity as ""已标注示例."" 

Overall, ""已标注示例"" is the most semantically accurate and contextually appropriate term for the AI domain, as it clearly indicates that the examples have been labeled and are ready for use in the learning process.","['Markup example', 'Labeled samples', 'Label samples', 'Labeled example', 'Annotation example']"
2535,2535,labeled graph,标注图,0.3333333333333333,9,"[{'word': '标记图', 'ratio': 0.5555555555555556}, {'word': '标注图', 'ratio': 0.3333333333333333}, {'word': '已标注图', 'ratio': 0.1111111111111111}]",标记图,{},[]
2536,2536,labeled training datum,标注训练数据,0.3,10,"[{'word': '带标签的训练数据', 'ratio': 0.4}, {'word': '标注训练数据', 'ratio': 0.3}, {'word': '有标签的训练数据', 'ratio': 0.1}, {'word': '有标注的训练数据', 'ratio': 0.1}, {'word': '标记训练数据', 'ratio': 0.1}]","""有标签的训练数据""","1. Rank: ""有标签的训练数据"", ""标注训练数据"", ""带标签的训练数据"", ""有标注的训练数据"", ""标记训练数据""

2. Explanation: The term ""有标签的训练数据"" is the best fit because it accurately captures the meaning of ""labeled training datum"" in the context of AI and machine learning. The phrase ""有标签"" directly translates to ""labeled,"" which is essential in distinguishing the data as being annotated for training purposes. This term is widely used in the AI domain, making it semantically precise and contextually appropriate. 

""标注训练数据"" is also a strong candidate, as ""标注"" means ""annotated,"" which is synonymous with ""labeled."" However, it is slightly less common in usage compared to ""有标签的训练数据."" 

""带标签的训练数据"" translates to ""training data with labels,"" which is accurate but less concise than the top choice. 

""有标注的训练数据"" is similar to ""标注训练数据"" but is less commonly used in the AI context. 

Lastly, ""标记训练数据"" translates to ""marked training data,"" which could imply a different meaning and is less precise in the context of labeled data used for training machine learning models. Thus, it ranks the lowest.","['Labeled training data', 'Label training data', 'Labeled training data', 'Labeled training data', 'Label training data']"
2537,2537,lambda calculus,λ演算,0.8,10,"[{'word': 'λ演算', 'ratio': 0.8}, {'word': 'λ 演算', 'ratio': 0.2}]",λ演算,{},[]
2538,2538,landmark,关键点,0.0,10,"[{'word': '地标', 'ratio': 0.8}, {'word': '特征点', 'ratio': 0.1}, {'word': 'λ演算', 'ratio': 0.1}]",地标,{},[]
2539,2539,landmark point,关键点,0.0,10,"[{'word': '地标点', 'ratio': 0.8}, {'word': '标志点', 'ratio': 0.1}, {'word': '特征点', 'ratio': 0.1}]",地标点,{},[]
2540,2540,language drift,语言漂移,1.0,10,"[{'word': '语言漂移', 'ratio': 1.0}]",语言漂移,{},[]
2541,2541,language encoder,语言编码器,1.0,10,"[{'word': '语言编码器', 'ratio': 1.0}]",语言编码器,{},[]
2542,2542,language generation,语言生成,1.0,10,"[{'word': '语言生成', 'ratio': 1.0}]",语言生成,{},[]
2543,2543,language generation model,语言生成模型,1.0,10,"[{'word': '语言生成模型', 'ratio': 1.0}]",语言生成模型,{},[]
2544,2544,language identification,语言识别,1.0,10,"[{'word': '语言识别', 'ratio': 1.0}]",语言识别,{},[]
2545,2545,language model pre-training,语言模型预训练,1.0,10,"[{'word': '语言模型预训练', 'ratio': 1.0}]",语言模型预训练,{},[]
2546,2546,language pair,语言对,1.0,10,"[{'word': '语言对', 'ratio': 1.0}]",语言对,{},[]
2547,2547,language representation,语言表征,0.0,10,"[{'word': '语言表示', 'ratio': 1.0}]",语言表示,{},[]
2548,2548,language transfer,语言迁移,0.9,10,"[{'word': '语言迁移', 'ratio': 0.9}, {'word': '', 'ratio': 0.1}]",语言迁移,{},[]
2549,2549,language understanding,语言理解,1.0,10,"[{'word': '语言理解', 'ratio': 1.0}]",语言理解,{},[]
2550,2550,large language model,大型语言模型,1.0,7,"[{'word': '大型语言模型', 'ratio': 1.0}]",大型语言模型,{},[]
2551,2551,large-margin learning,大边界学习,0.0,7,"[{'word': '大间隔学习', 'ratio': 0.5714285714285714}, {'word': '大边际学习', 'ratio': 0.42857142857142855}]",大间隔学习,{},[]
2552,2552,latent code,潜在编码,1.0,7,"[{'word': '潜在编码', 'ratio': 1.0}]",潜在编码,{},[]
2553,2553,latent dimension,隐藏维度,0.0,5,"[{'word': '潜在维度', 'ratio': 1.0}]",潜在维度,{},[]
2554,2554,latent distribution,隐分布,0.0,5,"[{'word': '潜在分布', 'ratio': 1.0}]",潜在分布,{},[]
2555,2555,latent dynamic model,潜在动态模型,0.6,5,"[{'word': '潜在动态模型', 'ratio': 0.6}, {'word': '潜在动力学模型', 'ratio': 0.4}]",潜在动态模型,{},[]
2556,2556,latent embedding,隐式嵌入,0.0,5,"[{'word': '潜在嵌入', 'ratio': 1.0}]",潜在嵌入,{},[]
2557,2557,latent factor,潜在因子,1.0,5,"[{'word': '潜在因子', 'ratio': 1.0}]",潜在因子,{},[]
2558,2558,latent feature,潜在特征,1.0,7,"[{'word': '潜在特征', 'ratio': 1.0}]",潜在特征,{},[]
2559,2559,latent function,潜在函数,0.8571428571428571,7,"[{'word': '潜在函数', 'ratio': 0.8571428571428571}, {'word': '潜在特征', 'ratio': 0.14285714285714285}]",潜在函数,{},[]
2560,2560,latent group,潜在群体,0.5714285714285714,7,"[{'word': '潜在群体', 'ratio': 0.5714285714285714}, {'word': '潜在群组', 'ratio': 0.2857142857142857}, {'word': '潜在组', 'ratio': 0.14285714285714285}]",潜在群体,{},[]
2561,2561,latent parameter,隐含参数,0.0,7,"[{'word': '潜在参数', 'ratio': 1.0}]",潜在参数,{},[]
2562,2562,latent representation,潜在表征,0.0,7,"[{'word': '潜在表示', 'ratio': 1.0}]",潜在表示,{},[]
2563,2563,latent reward function,潜在奖励函数,1.0,9,"[{'word': '潜在奖励函数', 'ratio': 1.0}]",潜在奖励函数,{},[]
2564,2564,latent semantic,潜在语义,1.0,9,"[{'word': '潜在语义', 'ratio': 1.0}]",潜在语义,{},[]
2565,2565,latent space,潜在空间,1.0,9,"[{'word': '潜在空间', 'ratio': 1.0}]",潜在空间,{},[]
2566,2566,latent state,潜在状态,1.0,9,"[{'word': '潜在状态', 'ratio': 1.0}]",潜在状态,{},[]
2567,2567,latent topic,隐含主题,0.0,9,"[{'word': '潜在主题', 'ratio': 1.0}]",潜在主题,{},[]
2568,2568,latent variable,潜在变量,0.0,8,"[{'word': '潜变量', 'ratio': 1.0}]",潜变量,{},[]
2569,2569,latent variable model,潜变量模型,1.0,8,"[{'word': '潜变量模型', 'ratio': 1.0}]",潜变量模型,{},[]
2570,2570,latent vector,隐向量,0.0,8,"[{'word': '潜向量', 'ratio': 0.625}, {'word': '潜在向量', 'ratio': 0.25}, {'word': '潜变量模型', 'ratio': 0.125}]",潜向量,{},[]
2571,2571,layer,层,0.875,8,"[{'word': '层', 'ratio': 0.875}, {'word': '潜向量', 'ratio': 0.125}]",层,{},[]
2572,2572,layer activation,层激活,1.0,8,"[{'word': '层激活', 'ratio': 1.0}]",层激活,{},[]
2573,2573,layer-wise learning rate decay,逐层学习率衰减,0.1428571428571428,7,"[{'word': '层级学习率衰减', 'ratio': 0.42857142857142855}, {'word': '分层学习率衰减', 'ratio': 0.42857142857142855}, {'word': '逐层学习率衰减', 'ratio': 0.14285714285714285}]",层级学习率衰减,"1. Rank: 层级学习率衰减, 分层学习率衰减, 逐层学习率衰减

2. Explanation: The term ""层级学习率衰减"" (layer-wise learning rate decay) is the best fit because it accurately captures the hierarchical nature of the learning rate adjustment process in deep learning models. The term ""层级"" (hierarchical) is commonly used in AI literature to describe structures that have multiple levels or layers, which aligns well with the concept of ""layer-wise"" in the original English term. 

""分层学习率衰减"" (layer-wise learning rate decay) is also a good candidate, but ""分层"" (layered) is less frequently used in the context of deep learning compared to ""层级."" 

""逐层学习率衰减"" (layer-by-layer learning rate decay) is less preferred because it implies a more sequential or step-by-step approach, which may not fully convey the intended meaning of adjusting the learning rate across layers simultaneously. 

Overall, ""层级学习率衰减"" is the most semantically accurate and contextually appropriate term for the AI domain.","['Hierarchical learning rate decay', 'Hierarchical learning rate decay', 'Layer-by-layer learning rate decay']"
2574,2574,lazy grounding,惰性实例化,0.2857142857142857,7,"[{'word': '惰性实例化', 'ratio': 0.2857142857142857}, {'word': '懒惰归约', 'ratio': 0.14285714285714285}, {'word': '延迟基础', 'ratio': 0.14285714285714285}, {'word': '延迟归属', 'ratio': 0.14285714285714285}, {'word': '懒惰归纳', 'ratio': 0.14285714285714285}, {'word': '懒惰实例化', 'ratio': 0.14285714285714285}]",惰性实例化,"1. Rank: 惰性实例化, 懒惰归约, 延迟基础, 延迟归属, 懒惰归纳, 懒惰实例化

2. Explanation: The term ""惰性实例化"" (lazy instantiation) is the best fit for ""lazy grounding"" in the AI context because it accurately captures the concept of ""lazy"" as it relates to computational processes, where instantiation refers to the creation of instances or representations of rules or data. This term aligns well with the idea of deferring certain computations until they are necessary, which is a key aspect of lazy grounding. 

The other candidates, such as ""懒惰归约"" (lazy reduction) and ""延迟基础"" (delay basics), do not convey the same level of specificity regarding the computational context. ""懒惰归纳"" (lazy induction) and ""懒惰实例化"" (lazy instantiation) are also relevant, but ""惰性实例化"" is more commonly used in AI literature to describe similar concepts, making it the most semantically accurate and contextually appropriate choice.","['lazy instantiation', 'lazy reduction', 'Delay basics', 'delayed vesting', 'lazy induction', 'lazy instantiation']"
2575,2575,leaf node,叶子节点,0.1428571428571428,7,"[{'word': '叶节点', 'ratio': 0.8571428571428571}, {'word': '叶子节点', 'ratio': 0.14285714285714285}]",叶节点,{},[]
2576,2576,learnability,可学习性,1.0,7,"[{'word': '可学习性', 'ratio': 1.0}]",可学习性,{},[]
2577,2577,learnable parameter,可学习参数,0.8571428571428571,7,"[{'word': '可学习参数', 'ratio': 0.8571428571428571}, {'word': '可学习参数 如果需要进一步的帮助或更多的翻译，请告诉我！', 'ratio': 0.14285714285714285}]",可学习参数,{},[]
2578,2578,learnable vector,可学习向量,1.0,7,"[{'word': '可学习向量', 'ratio': 1.0}]",可学习向量,{},[]
2579,2579,learned model,学习模型,0.4285714285714285,7,"[{'word': '学习得到的模型', 'ratio': 0.5714285714285714}, {'word': '学习模型', 'ratio': 0.42857142857142855}]",学习得到的模型,{},[]
2580,2580,learned representation,学习表示,0.4285714285714285,7,"[{'word': '学习得到的表示', 'ratio': 0.5714285714285714}, {'word': '学习表示', 'ratio': 0.42857142857142855}]",学习得到的表示,{},[]
2581,2581,learner,学习者,0.4285714285714285,7,"[{'word': '学习器', 'ratio': 0.5714285714285714}, {'word': '学习者', 'ratio': 0.42857142857142855}]",学习器,{},[]
2582,2582,learning agent,学习智能体,0.375,8,"[{'word': '学习代理', 'ratio': 0.625}, {'word': '学习智能体', 'ratio': 0.375}]",学习代理,{},[]
2583,2583,learning algorithm,学习算法,1.0,8,"[{'word': '学习算法', 'ratio': 1.0}]",学习算法,{},[]
2584,2584,learning method,学习方法,1.0,8,"[{'word': '学习方法', 'ratio': 1.0}]",学习方法,{},[]
2585,2585,learning paradigm,学习范式,1.0,8,"[{'word': '学习范式', 'ratio': 1.0}]",学习范式,{},[]
2586,2586,learning problem,学习问题,1.0,8,"[{'word': '学习问题', 'ratio': 1.0}]",学习问题,{},[]
2587,2587,learning rate decay,学习率衰减,1.0,9,"[{'word': '学习率衰减', 'ratio': 1.0}]",学习率衰减,{},[]
2588,2588,learning rate decay schedule,学习率衰减时间表,0.1111111111111111,9,"[{'word': '学习率衰减计划', 'ratio': 0.8888888888888888}, {'word': '学习率衰减时间表', 'ratio': 0.1111111111111111}]",学习率衰减计划,{},[]
2589,2589,learning rate schedule,学习率计划,0.6666666666666666,9,"[{'word': '学习率计划', 'ratio': 0.6666666666666666}, {'word': '学习率调度', 'ratio': 0.2222222222222222}, {'word': '学习率表', 'ratio': 0.1111111111111111}]",学习率计划,{},[]
2590,2590,learning rate scheduler,学习率调度器,1.0,9,"[{'word': '学习率调度器', 'ratio': 1.0}]",学习率调度器,{},[]
2591,2591,learning rate warmup,学习率热身,0.0,9,"[{'word': '学习率预热', 'ratio': 1.0}]",学习率预热,{},[]
2592,2592,learning-to-rank algorithm,学习排序算法,1.0,9,"[{'word': '学习排序算法', 'ratio': 1.0}]",学习排序算法,{},[]
2593,2593,least square,最小二乘,0.4444444444444444,9,"[{'word': '最小二乘法', 'ratio': 0.5555555555555556}, {'word': '最小二乘', 'ratio': 0.4444444444444444}]",最小二乘法,{},[]
2594,2594,least square criterion,最小二乘准则,0.8888888888888888,9,"[{'word': '最小二乘准则', 'ratio': 0.8888888888888888}, {'word': '最小二乘标准', 'ratio': 0.1111111111111111}]",最小二乘准则,{},[]
2595,2595,least square minimization,最小二乘法,0.0,9,"[{'word': '最小二乘最小化', 'ratio': 0.7777777777777778}, {'word': '最小二乘法最小化', 'ratio': 0.1111111111111111}, {'word': '最小二乘最优化', 'ratio': 0.1111111111111111}]",最小二乘最小化,{},[]
2596,2596,least square problem,最小二乘问题,1.0,9,"[{'word': '最小二乘问题', 'ratio': 1.0}]",最小二乘问题,{},[]
2597,2597,least square regression,最小二乘回归,1.0,9,"[{'word': '最小二乘回归', 'ratio': 1.0}]",最小二乘回归,{},[]
2598,2598,least square solution,最小二乘解,1.0,9,"[{'word': '最小二乘解', 'ratio': 1.0}]",最小二乘解,{},[]
2599,2599,leave-one-out,留一法,1.0,9,"[{'word': '留一法', 'ratio': 1.0}]",留一法,{},[]
2600,2600,left-to-right model,从左到右模型,1.0,9,"[{'word': '从左到右模型', 'ratio': 1.0}]",从左到右模型,{},[]
2601,2601,lemmatization,词形还原,0.8888888888888888,9,"[{'word': '词形还原', 'ratio': 0.8888888888888888}, {'word': '词元化', 'ratio': 0.1111111111111111}]",词形还原,{},[]
2602,2602,length normalization,长度归一化,1.0,8,"[{'word': '长度归一化', 'ratio': 1.0}]",长度归一化,{},[]
2603,2603,length penalty,长度惩罚,1.0,8,"[{'word': '长度惩罚', 'ratio': 1.0}]",长度惩罚,{},[]
2604,2604,lexeme,词元,0.0,8,"[{'word': '词位', 'ratio': 1.0}]",词位,{},[]
2605,2605,lexical acquisition,词汇获取,0.875,8,"[{'word': '词汇获取', 'ratio': 0.875}, {'word': '词汇习得', 'ratio': 0.125}]",词汇获取,{},[]
2606,2606,lexical acquisition algorithm,词汇获取算法,1.0,6,"[{'word': '词汇获取算法', 'ratio': 1.0}]",词汇获取算法,{},[]
2607,2607,lexical ambiguity,词汇歧义,0.8333333333333334,6,"[{'word': '词汇歧义', 'ratio': 0.8333333333333334}, {'word': '词义歧义', 'ratio': 0.16666666666666666}]",词汇歧义,{},[]
2608,2608,lexical entry,词条,0.1666666666666666,6,"[{'word': '词汇条目', 'ratio': 0.8333333333333334}, {'word': '词条', 'ratio': 0.16666666666666666}]",词汇条目,{},[]
2609,2609,lexical exposure,词汇暴露,0.8333333333333334,6,"[{'word': '词汇暴露', 'ratio': 0.8333333333333334}, {'word': '词汇接触', 'ratio': 0.16666666666666666}]",词汇暴露,{},[]
2610,2610,lexical feature,词汇特征,1.0,6,"[{'word': '词汇特征', 'ratio': 1.0}]",词汇特征,{},[]
2611,2611,lexical head,词头,0.0,5,"[{'word': '词汇中心语', 'ratio': 0.4}, {'word': '词汇中心', 'ratio': 0.2}, {'word': '词汇主干', 'ratio': 0.2}, {'word': '词汇主语', 'ratio': 0.2}]",词汇中心语,"1. Rank: 词汇中心语, 词汇中心, 词汇主干, 词汇主语

2. Explanation: The term ""词汇中心语"" (lexical head) is the best fit because it accurately captures the concept of a ""head"" in linguistic terms, specifically in the context of dependency grammar. The term ""中心语"" (center word) is commonly used in Chinese linguistics to refer to the main word in a phrase that determines the syntactic structure, aligning closely with the English term ""head."" The back translation ""vocabulary center"" is somewhat misleading as it does not convey the specific linguistic function of a head. 

The second candidate, ""词汇中心,"" while similar, lacks the specificity of ""语"" (word) which is crucial in this context. ""词汇主干"" (vocabulary stem) and ""词汇主语"" (lexical subject) are less appropriate as they introduce different meanings that do not align with the concept of a head in dependency grammar. ""主干"" (stem) suggests a foundational element rather than a syntactic head, and ""主语"" (subject) is a specific grammatical role that does not encompass the broader meaning of ""head."" Thus, ""词汇中心语"" is the most semantically accurate and contextually appropriate choice in the AI domain.","['vocabulary center', 'vocabulary center', 'vocabulary stem', 'lexical subject']"
2612,2612,lexical item,词汇项,1.0,5,"[{'word': '词汇项', 'ratio': 1.0}]",词汇项,{},[]
2613,2613,lexical knowledge,词汇知识,1.0,5,"[{'word': '词汇知识', 'ratio': 1.0}]",词汇知识,{},[]
2614,2614,lexical model,词汇模型,1.0,5,"[{'word': '词汇模型', 'ratio': 1.0}]",词汇模型,{},[]
2615,2615,lexical overlap,词汇重叠,1.0,5,"[{'word': '词汇重叠', 'ratio': 1.0}]",词汇重叠,{},[]
2616,2616,lexicalization,词汇化,1.0,8,"[{'word': '词汇化', 'ratio': 1.0}]",词汇化,{},[]
2617,2617,lexicalized grammar,词汇化语法,1.0,8,"[{'word': '词汇化语法', 'ratio': 1.0}]",词汇化语法,{},[]
2618,2618,lexicalized parsing model,词汇化解析模型,1.0,8,"[{'word': '词汇化解析模型', 'ratio': 1.0}]",词汇化解析模型,{},[]
2619,2619,lexicon,词汇表,0.375,8,"[{'word': '词典', 'ratio': 0.5}, {'word': '词汇表', 'ratio': 0.375}, {'word': '词典 / 词汇表', 'ratio': 0.125}]",词典,{},[]
2620,2620,lexicon induction,词汇诱导,0.125,8,"[{'word': '词典归纳', 'ratio': 0.5}, {'word': '词汇表归纳', 'ratio': 0.25}, {'word': '词汇诱导', 'ratio': 0.125}, {'word': '词典归纳 / 词汇归纳', 'ratio': 0.125}]",词典归纳,{},[]
2621,2621,light field,光场,1.0,6,"[{'word': '光场', 'ratio': 1.0}]",光场,{},[]
2622,2622,light field interpolation,光场插值,1.0,6,"[{'word': '光场插值', 'ratio': 1.0}]",光场插值,{},[]
2623,2623,likelihood function,似然函数,1.0,6,"[{'word': '似然函数', 'ratio': 1.0}]",似然函数,{},[]
2624,2624,likelihood ratio test,似然比检验,1.0,6,"[{'word': '似然比检验', 'ratio': 1.0}]",似然比检验,{},[]
2625,2625,likelihood score,似然得分,0.8333333333333334,6,"[{'word': '似然得分', 'ratio': 0.8333333333333334}, {'word': '似然分数', 'ratio': 0.16666666666666666}]",似然得分,{},[]
2626,2626,line search,线搜索,0.8571428571428571,7,"[{'word': '线搜索', 'ratio': 0.8571428571428571}, {'word': '线性搜索', 'ratio': 0.14285714285714285}]",线搜索,{},[]
2627,2627,linear activation function,线性激活函数,1.0,7,"[{'word': '线性激活函数', 'ratio': 1.0}]",线性激活函数,{},[]
2628,2628,linear algebra,线性代数,1.0,7,"[{'word': '线性代数', 'ratio': 1.0}]",线性代数,{},[]
2629,2629,linear classification,线性分类,1.0,7,"[{'word': '线性分类', 'ratio': 1.0}]",线性分类,{},[]
2630,2630,linear classification layer,线性分类层,1.0,8,"[{'word': '线性分类层', 'ratio': 1.0}]",线性分类层,{},[]
2631,2631,linear classifier,线性分类器,1.0,8,"[{'word': '线性分类器', 'ratio': 1.0}]",线性分类器,{},[]
2632,2632,linear combination,线性组合,1.0,8,"[{'word': '线性组合', 'ratio': 1.0}]",线性组合,{},[]
2633,2633,linear complexity,线性复杂度,1.0,8,"[{'word': '线性复杂度', 'ratio': 1.0}]",线性复杂度,{},[]
2634,2634,linear constraint,线性约束条件,0.0,8,"[{'word': '线性约束', 'ratio': 1.0}]",线性约束,{},[]
2635,2635,linear decay,线性衰减,1.0,6,"[{'word': '线性衰减', 'ratio': 1.0}]",线性衰减,{},[]
2636,2636,linear decoder,线性解码器,1.0,6,"[{'word': '线性解码器', 'ratio': 1.0}]",线性解码器,{},[]
2637,2637,linear discriminant analysis,线性判别分析,1.0,6,"[{'word': '线性判别分析', 'ratio': 1.0}]",线性判别分析,{},[]
2638,2638,linear equation,线性方程组,0.0,6,"[{'word': '线性方程', 'ratio': 1.0}]",线性方程,{},[]
2639,2639,linear evaluation,线性评估,1.0,6,"[{'word': '线性评估', 'ratio': 1.0}]",线性评估,{},[]
2640,2640,linear function,线性函数,1.0,6,"[{'word': '线性函数', 'ratio': 1.0}]",线性函数,{},[]
2641,2641,linear function approximation,线性函数近似,0.5,6,"[{'word': '线性函数近似', 'ratio': 0.5}, {'word': '线性函数逼近', 'ratio': 0.5}]",线性函数近似,{},[]
2642,2642,linear inequality,线性不等式,1.0,6,"[{'word': '线性不等式', 'ratio': 1.0}]",线性不等式,{},[]
2643,2643,linear interpolation,线性插值,1.0,6,"[{'word': '线性插值', 'ratio': 1.0}]",线性插值,{},[]
2644,2644,linear kernel,线性核,1.0,6,"[{'word': '线性核', 'ratio': 1.0}]",线性核,{},[]
2645,2645,linear layer,线性层,1.0,7,"[{'word': '线性层', 'ratio': 1.0}]",线性层,{},[]
2646,2646,linear learning rate decay,线性学习率衰减,1.0,7,"[{'word': '线性学习率衰减', 'ratio': 1.0}]",线性学习率衰减,{},[]
2647,2647,linear learning rate schedule,线性学习率调度,0.5714285714285714,7,"[{'word': '线性学习率调度', 'ratio': 0.5714285714285714}, {'word': '线性学习率计划', 'ratio': 0.42857142857142855}]",线性学习率调度,{},[]
2648,2648,linear map,线性映射,1.0,7,"[{'word': '线性映射', 'ratio': 1.0}]",线性映射,{},[]
2649,2649,linear model,线性模型,1.0,7,"[{'word': '线性模型', 'ratio': 1.0}]",线性模型,{},[]
2650,2650,linear predictor,线性预测器,1.0,10,"[{'word': '线性预测器', 'ratio': 1.0}]",线性预测器,{},[]
2651,2651,linear probe,线性探针,0.3,10,"[{'word': '线性探测器', 'ratio': 0.6}, {'word': '线性探针', 'ratio': 0.3}, {'word': '线性探头', 'ratio': 0.1}]",线性探测器,{},[]
2652,2652,linear program,线性规划,1.0,10,"[{'word': '线性规划', 'ratio': 1.0}]",线性规划,{},[]
2653,2653,linear programming relaxation,线性规划松弛,1.0,10,"[{'word': '线性规划松弛', 'ratio': 1.0}]",线性规划松弛,{},[]
2654,2654,linear projection,线性映射,0.0,10,"[{'word': '线性投影', 'ratio': 1.0}]",线性投影,{},[]
2655,2655,linear regression model,线性回归模型,1.0,9,"[{'word': '线性回归模型', 'ratio': 1.0}]",线性回归模型,{},[]
2656,2656,linear regressor,线性回归器,1.0,9,"[{'word': '线性回归器', 'ratio': 1.0}]",线性回归器,{},[]
2657,2657,linear scaling,线性缩放,1.0,9,"[{'word': '线性缩放', 'ratio': 1.0}]",线性缩放,{},[]
2658,2658,linear scheduler,线性调度器,1.0,9,"[{'word': '线性调度器', 'ratio': 1.0}]",线性调度器,{},[]
2659,2659,linear separability,线性可分性,1.0,9,"[{'word': '线性可分性', 'ratio': 1.0}]",线性可分性,{},[]
2660,2660,linear system,线性系统,1.0,8,"[{'word': '线性系统', 'ratio': 1.0}]",线性系统,{},[]
2661,2661,linear transform,线性变换,1.0,8,"[{'word': '线性变换', 'ratio': 1.0}]",线性变换,{},[]
2662,2662,linear transformation,线性变换,1.0,8,"[{'word': '线性变换', 'ratio': 1.0}]",线性变换,{},[]
2663,2663,linear transformation matrix,线性变换矩阵,0.875,8,"[{'word': '线性变换矩阵', 'ratio': 0.875}, {'word': '线性转换', 'ratio': 0.125}]",线性变换矩阵,{},[]
2664,2664,linear warm-up,线性预热,1.0,8,"[{'word': '线性预热', 'ratio': 1.0}]",线性预热,{},[]
2665,2665,linear-chain,线性链,1.0,7,"[{'word': '线性链', 'ratio': 1.0}]",线性链,{},[]
2666,2666,linear-quadratic regulator,线性二次调节器,0.7142857142857143,7,"[{'word': '线性二次调节器', 'ratio': 0.7142857142857143}, {'word': '线性-二次调节器', 'ratio': 0.2857142857142857}]",线性二次调节器,{},[]
2667,2667,linearization,线性化,1.0,7,"[{'word': '线性化', 'ratio': 1.0}]",线性化,{},[]
2668,2668,link function,联系函数,0.0,7,"[{'word': '链接函数', 'ratio': 0.7142857142857143}, {'word': '连接函数', 'ratio': 0.2857142857142857}]",链接函数,{},[]
2669,2669,link prediction,链路预测,0.0,7,"[{'word': '链接预测', 'ratio': 1.0}]",链接预测,{},[]
2670,2670,local basis function,局部基函数,1.0,7,"[{'word': '局部基函数', 'ratio': 1.0}]",局部基函数,{},[]
2671,2671,local coherence,局部连贯性,1.0,7,"[{'word': '局部连贯性', 'ratio': 1.0}]",局部连贯性,{},[]
2672,2672,local context,本地上下文,0.0,5,"[{'word': '局部上下文', 'ratio': 1.0}]",局部上下文,{},[]
2673,2673,local coordinate frame,局部坐标系,1.0,5,"[{'word': '局部坐标系', 'ratio': 1.0}]",局部坐标系,{},[]
2674,2674,local feature,局部特征,1.0,5,"[{'word': '局部特征', 'ratio': 1.0}]",局部特征,{},[]
2675,2675,local geometry,局部几何,1.0,5,"[{'word': '局部几何', 'ratio': 1.0}]",局部几何,{},[]
2676,2676,local image feature,局部图像特征,1.0,5,"[{'word': '局部图像特征', 'ratio': 1.0}]",局部图像特征,{},[]
2677,2677,local maxima,局部最大值,0.0,7,"[{'word': '局部极大值', 'ratio': 1.0}]",局部极大值,{},[]
2678,2678,local maximum,局部极大值,0.1428571428571428,7,"[{'word': '局部最大值', 'ratio': 0.8571428571428571}, {'word': '局部极大值', 'ratio': 0.14285714285714285}]",局部最大值,{},[]
2679,2679,local minima,局部极小值,1.0,7,"[{'word': '局部极小值', 'ratio': 1.0}]",局部极小值,{},[]
2680,2680,local minimizer,局部极小值点,0.4285714285714285,7,"[{'word': '局部极小值点', 'ratio': 0.42857142857142855}, {'word': '局部最小化子', 'ratio': 0.2857142857142857}, {'word': '局部极小化器', 'ratio': 0.14285714285714285}, {'word': '局部最小化器', 'ratio': 0.14285714285714285}]",局部最小化子,"1. Rank: 局部最小化子, 局部最小化器, 局部极小化器, 局部极小值点

2. Explanation: The term ""局部最小化子"" (local minimizer) is the best fit because it directly corresponds to the English term ""local minimizer"" in both semantic accuracy and contextual usage within the AI domain. In optimization contexts, ""minimizer"" refers to an entity (point or algorithm) that minimizes a function, which aligns well with the term ""局部最小化子."" 

The term ""局部最小化器"" (local minimizer) is also a strong candidate, but it is less commonly used in the context of mathematical optimization compared to ""局部最小化子."" The term ""局部极小化器"" (local minimizer) is similar but introduces the word ""极小,"" which can imply a more general sense of minimization rather than the specific context of local optimization. Lastly, ""局部极小值点"" (local minimum point) is less accurate because it refers to the point itself rather than the concept of minimization, which is crucial in the context of optimization algorithms. Thus, ""局部最小化子"" is the most precise and contextually appropriate choice.","['local minimum point', 'local minimizer', 'local minimizer', 'local minimizer']"
2681,2681,local minimum,局部最小值,0.8571428571428571,7,"[{'word': '局部最小值', 'ratio': 0.8571428571428571}, {'word': '局部极小值', 'ratio': 0.14285714285714285}]",局部最小值,{},[]
2682,2682,local model,局部模型,1.0,4,"[{'word': '局部模型', 'ratio': 1.0}]",局部模型,{},[]
2683,2683,local optima,局部最优,0.0,4,"[{'word': '局部最优解', 'ratio': 1.0}]",局部最优解,{},[]
2684,2684,local optimum,局部最优,1.0,4,"[{'word': '局部最优', 'ratio': 1.0}]",局部最优,{},[]
2685,2685,local parameter,局部参数,1.0,4,"[{'word': '局部参数', 'ratio': 1.0}]",局部参数,{},[]
2686,2686,local search,局部搜索,1.0,4,"[{'word': '局部搜索', 'ratio': 1.0}]",局部搜索,{},[]
2687,2687,local search method,局部搜索方法,1.0,7,"[{'word': '局部搜索方法', 'ratio': 1.0}]",局部搜索方法,{},[]
2688,2688,local variable,局部变量,1.0,7,"[{'word': '局部变量', 'ratio': 1.0}]",局部变量,{},[]
2689,2689,local window,局部窗口,1.0,7,"[{'word': '局部窗口', 'ratio': 1.0}]",局部窗口,{},[]
2690,2690,locality-sensitive hashing,对局部敏感哈希,0.0,7,"[{'word': '局部敏感哈希', 'ratio': 1.0}]",局部敏感哈希,{},[]
2691,2691,localization,定位,1.0,7,"[{'word': '定位', 'ratio': 1.0}]",定位,{},[]
2692,2692,location parameter,位置参数,1.0,7,"[{'word': '位置参数', 'ratio': 1.0}]",位置参数,{},[]
2693,2693,log,对数,1.0,7,"[{'word': '对数', 'ratio': 1.0}]",对数,{},[]
2694,2694,log likelihood,对数似然,1.0,7,"[{'word': '对数似然', 'ratio': 1.0}]",对数似然,{},[]
2695,2695,log loss,对数损失,1.0,7,"[{'word': '对数损失', 'ratio': 1.0}]",对数损失,{},[]
2696,2696,log marginal likelihood,对数边缘似然,0.0,7,"[{'word': '对数边际似然', 'ratio': 0.8571428571428571}, {'word': '对数损失', 'ratio': 0.14285714285714285}]",对数边际似然,{},[]
2697,2697,log p,对数概率,0.8,10,"[{'word': '对数概率', 'ratio': 0.8}, {'word': '对数 p', 'ratio': 0.1}, {'word': '对数p', 'ratio': 0.1}]",对数概率,{},[]
2698,2698,log partition function,对数分区函数,0.5,10,"[{'word': '对数配分函数', 'ratio': 0.5}, {'word': '对数分区函数', 'ratio': 0.5}]",对数配分函数,{},[]
2699,2699,log perplexity,对数困惑度,0.9,10,"[{'word': '对数困惑度', 'ratio': 0.9}, {'word': '记录困惑', 'ratio': 0.1}]",对数困惑度,{},[]
2700,2700,log probability,对数概率,1.0,10,"[{'word': '对数概率', 'ratio': 1.0}]",对数概率,{},[]
2701,2701,log-likelihood function,对数似然函数,1.0,10,"[{'word': '对数似然函数', 'ratio': 1.0}]",对数似然函数,{},[]
2702,2702,log-likelihood loss,负对数似然损失,0.0,9,"[{'word': '对数似然损失', 'ratio': 1.0}]",对数似然损失,{},[]
2703,2703,log-likelihood ratio,对数似然比,1.0,9,"[{'word': '对数似然比', 'ratio': 1.0}]",对数似然比,{},[]
2704,2704,log-linear model,对数线性模型,1.0,9,"[{'word': '对数线性模型', 'ratio': 1.0}]",对数线性模型,{},[]
2705,2705,log-linear translation model,对数线性翻译模型,1.0,9,"[{'word': '对数线性翻译模型', 'ratio': 1.0}]",对数线性翻译模型,{},[]
2706,2706,log-log plot,对数-对数图,1.0,9,"[{'word': '对数-对数图', 'ratio': 1.0}]",对数-对数图,{},[]
2707,2707,log-normal distribution,对数正态分布,1.0,10,"[{'word': '对数正态分布', 'ratio': 1.0}]",对数正态分布,{},[]
2708,2708,log-odd score,对数几率分数,0.0,10,"[{'word': '对数优势分数', 'ratio': 0.5}, {'word': '对数几率得分', 'ratio': 0.3}, {'word': '对数几率评分', 'ratio': 0.1}, {'word': '对数优势比分数', 'ratio': 0.1}]",对数优势分数,{},[]
2709,2709,log-prob,对数概率,1.0,10,"[{'word': '对数概率', 'ratio': 1.0}]",对数概率,{},[]
2710,2710,log-sum-exp,对数指数和,0.0,10,"[{'word': '对数和指数', 'ratio': 1.0}]",对数和指数,{},[]
2711,2711,logical connective,逻辑连接词,0.9,10,"[{'word': '逻辑连接词', 'ratio': 0.9}, {'word': '逻辑连结词', 'ratio': 0.1}]",逻辑连接词,{},[]
2712,2712,logical form,逻辑形式,1.0,8,"[{'word': '逻辑形式', 'ratio': 1.0}]",逻辑形式,{},[]
2713,2713,logistic function,逻辑函数,0.875,8,"[{'word': '逻辑函数', 'ratio': 0.875}, {'word': '物流功能', 'ratio': 0.125}]",逻辑函数,{},[]
2714,2714,logistic loss,逻辑损失,0.875,8,"[{'word': '逻辑损失', 'ratio': 0.875}, {'word': '物流损失', 'ratio': 0.125}]",逻辑损失,{},[]
2715,2715,logistic regression classifier,逻辑回归分类器,1.0,8,"[{'word': '逻辑回归分类器', 'ratio': 1.0}]",逻辑回归分类器,{},[]
2716,2716,logistic regression model,逻辑回归模型,1.0,8,"[{'word': '逻辑回归模型', 'ratio': 1.0}]",逻辑回归模型,{},[]
2717,2717,logit,logit值,0.0,9,"[{'word': '对数几率', 'ratio': 1.0}]",对数几率,{},[]
2718,2718,logit model,对数几率模型,0.8888888888888888,9,"[{'word': '对数几率模型', 'ratio': 0.8888888888888888}, {'word': '对数几率', 'ratio': 0.1111111111111111}]",对数几率模型,{},[]
2719,2719,long-range dependency,长程依赖,0.4444444444444444,9,"[{'word': '长距离依赖', 'ratio': 0.4444444444444444}, {'word': '长程依赖', 'ratio': 0.4444444444444444}, {'word': '对数几率', 'ratio': 0.1111111111111111}]",长程依赖,"1. Rank: 长程依赖, 长距离依赖, 对数几率

2. Explanation: The term ""长程依赖"" (long range dependence) is the best fit for the AI domain, particularly in the context of natural language processing and machine learning. This term accurately captures the concept of ""long-range dependency,"" which refers to the ability of a model to understand and remember relationships between elements that are far apart in a sequence. The back translation ""long range dependence"" aligns closely with the original English term, maintaining both semantic accuracy and contextual relevance.

On the other hand, ""长距离依赖"" (long distance dependency) is a less precise translation. While it conveys a similar idea, ""distance"" can imply a more physical or spatial interpretation, which may not fully encapsulate the abstract nature of dependencies in sequence processing tasks. 

Lastly, ""对数几率"" (log odds) is unrelated to the concept of long-range dependency and is therefore not a suitable candidate in this context. 

Overall, ""长程依赖"" is preferred due to its direct alignment with the terminology used in AI literature, ensuring clarity and precision in communication within the field.","['long distance dependency', 'long range dependence', 'Log odds']"
2720,2720,lookup table,查找表,0.8888888888888888,9,"[{'word': '查找表', 'ratio': 0.8888888888888888}, {'word': '对数几率', 'ratio': 0.1111111111111111}]",查找表,{},[]
2721,2721,loop closure,闭环检测,0.1111111111111111,9,"[{'word': '回环检测', 'ratio': 0.3333333333333333}, {'word': '回环闭合', 'ratio': 0.2222222222222222}, {'word': '回路闭合', 'ratio': 0.1111111111111111}, {'word': '闭环', 'ratio': 0.1111111111111111}, {'word': '闭环检测', 'ratio': 0.1111111111111111}, {'word': '对数几率', 'ratio': 0.1111111111111111}]",回环闭合,"1. Rank: 回环闭合, 回环检测, 闭环, 闭环检测, 回路闭合, 对数几率

2. Explanation: The term ""回环闭合"" (loop closure) is the most accurate translation for the AI domain, particularly in the context of visual odometry and SLAM (Simultaneous Localization and Mapping). This term directly corresponds to the English term ""loop closure,"" which refers to the process of recognizing a previously visited location in a mapping context, allowing for corrections in the estimated trajectory. 

""回环检测"" (loopback detection) is a close second, as it emphasizes the detection aspect, but it does not fully capture the concept of closing the loop, which is crucial in the context of SLAM. 

""闭环"" (closed loop) and ""闭环检测"" (closed loop detection) are also relevant but are less specific than ""回环闭合"" in the context of the provided sentences. ""回路闭合"" (loop closed) is less commonly used in the AI domain and may lead to confusion. Lastly, ""对数几率"" (log odds) is unrelated to the context of loop closure and should not be considered in this ranking. 

Overall, ""回环闭合"" is the best fit due to its direct semantic alignment with the English term and its established usage in the AI and robotics fields.","['Loopback detection', 'loop closure', 'Loop closed', 'closed loop', 'Closed loop detection', 'Log odds']"
2722,2722,loopy belief propagation,循环信念传播,1.0,8,"[{'word': '循环信念传播', 'ratio': 1.0}]",循环信念传播,{},[]
2723,2723,loss,损失,1.0,8,"[{'word': '损失', 'ratio': 1.0}]",损失,{},[]
2724,2724,loss distribution,损失分布,1.0,8,"[{'word': '损失分布', 'ratio': 1.0}]",损失分布,{},[]
2725,2725,loss function,损失函数,1.0,8,"[{'word': '损失函数', 'ratio': 1.0}]",损失函数,{},[]
2726,2726,loss landscape,损失景观,0.75,8,"[{'word': '损失景观', 'ratio': 0.75}, {'word': '损失地形', 'ratio': 0.25}]",损失景观,{},[]
2727,2727,loss minimization,损失最小化,1.0,7,"[{'word': '损失最小化', 'ratio': 1.0}]",损失最小化,{},[]
2728,2728,loss term,损失项,1.0,7,"[{'word': '损失项', 'ratio': 1.0}]",损失项,{},[]
2729,2729,lossy compression,有损压缩,1.0,7,"[{'word': '有损压缩', 'ratio': 1.0}]",有损压缩,{},[]
2730,2730,lottery ticket hypothesis,彩票假说,0.4285714285714285,7,"[{'word': '彩票票假设', 'ratio': 0.42857142857142855}, {'word': '彩票假说', 'ratio': 0.42857142857142855}, {'word': '彩票票据假设', 'ratio': 0.14285714285714285}]",彩票假说,"1. Rank: 彩票假说, 彩票票假设, 彩票票据假设

2. Explanation: The term ""彩票假说"" (lottery hypothesis) is the best fit for the translation of ""lottery ticket hypothesis"" because it captures the essence of the term while maintaining semantic accuracy. The word ""假说"" (hypothesis) is a commonly used term in scientific and academic contexts, making it more appropriate for the AI domain. 

In contrast, ""彩票票假设"" (lottery ticket hypothesis) is a direct translation that retains the original structure but is less fluid in Chinese. It may sound awkward to native speakers and does not align as well with the conventions of scientific terminology in Chinese. 

The term ""彩票票据假设"" (lottery ticket assumption) introduces ""票据"" (ticket) which is less relevant in this context and could lead to confusion, as it implies a different meaning related to receipts or documents rather than the conceptual framework of the hypothesis.

Overall, ""彩票假说"" is concise, contextually appropriate, and aligns well with the established terminology in the AI field, making it the most suitable choice.","['lottery ticket hypothesis', 'lottery hypothesis', 'lottery ticket hypothesis']"
2731,2731,low rank,低秩,1.0,7,"[{'word': '低秩', 'ratio': 1.0}]",低秩,{},[]
2732,2732,low rank approximation,低秩近似,1.0,6,"[{'word': '低秩近似', 'ratio': 1.0}]",低秩近似,{},[]
2733,2733,low-data regime,少量数据环境,0.0,6,"[{'word': '低数据环境', 'ratio': 0.5}, {'word': '低数据状态', 'ratio': 0.16666666666666666}, {'word': '低数据范畴', 'ratio': 0.16666666666666666}, {'word': '低数据量情况', 'ratio': 0.16666666666666666}]",低数据环境,{},[]
2734,2734,low-dimensional embedding,低维嵌入,1.0,6,"[{'word': '低维嵌入', 'ratio': 1.0}]",低维嵌入,{},[]
2735,2735,low-dimensional representation,低维表示,1.0,6,"[{'word': '低维表示', 'ratio': 1.0}]",低维表示,{},[]
2736,2736,low-pass filter,低通滤波器,1.0,6,"[{'word': '低通滤波器', 'ratio': 1.0}]",低通滤波器,{},[]
2737,2737,low-rank factorization,低秩分解,0.8888888888888888,9,"[{'word': '低秩分解', 'ratio': 0.8888888888888888}, {'word': '低阶因式分解', 'ratio': 0.1111111111111111}]",低秩分解,{},[]
2738,2738,low-rank matrix approximation,低秩矩阵近似,1.0,9,"[{'word': '低秩矩阵近似', 'ratio': 1.0}]",低秩矩阵近似,{},[]
2739,2739,lower bound,下界,0.8888888888888888,9,"[{'word': '下界', 'ratio': 0.8888888888888888}, {'word': '下限', 'ratio': 0.1111111111111111}]",下界,{},[]
2740,2740,mIoU,平均交并比,0.7777777777777778,9,"[{'word': '平均交并比', 'ratio': 0.7777777777777778}, {'word': '米卢', 'ratio': 0.1111111111111111}, {'word': '卢', 'ratio': 0.1111111111111111}]",平均交并比,{},[]
2741,2741,mT5,mT5,0.7777777777777778,9,"[{'word': 'mT5', 'ratio': 0.7777777777777778}, {'word': '多语言T5', 'ratio': 0.2222222222222222}]",mT5,{},[]
2742,2742,machine learning algorithm,机器学习算法,1.0,7,"[{'word': '机器学习算法', 'ratio': 1.0}]",机器学习算法,{},[]
2743,2743,machine learning classifier,机器学习分类器,1.0,7,"[{'word': '机器学习分类器', 'ratio': 1.0}]",机器学习分类器,{},[]
2744,2744,machine learning model,机器学习模型,1.0,7,"[{'word': '机器学习模型', 'ratio': 1.0}]",机器学习模型,{},[]
2745,2745,machine learning system,机器学习系统,1.0,7,"[{'word': '机器学习系统', 'ratio': 1.0}]",机器学习系统,{},[]
2746,2746,machine reading,机器阅读,1.0,7,"[{'word': '机器阅读', 'ratio': 1.0}]",机器阅读,{},[]
2747,2747,machine translation model,机器翻译模型,1.0,5,"[{'word': '机器翻译模型', 'ratio': 1.0}]",机器翻译模型,{},[]
2748,2748,machine translation system,机器翻译系统,1.0,5,"[{'word': '机器翻译系统', 'ratio': 1.0}]",机器翻译系统,{},[]
2749,2749,machine vision,机器视觉,1.0,5,"[{'word': '机器视觉', 'ratio': 1.0}]",机器视觉,{},[]
2750,2750,machine-generated text,机器生成的文本,0.0,5,"[{'word': '机器生成文本', 'ratio': 1.0}]",机器生成文本,{},[]
2751,2751,machine-in-the-loop,人机协作,0.4,5,"[{'word': '机器在环', 'ratio': 0.4}, {'word': '人机协作', 'ratio': 0.4}, {'word': '机器参与循环', 'ratio': 0.2}]",机器在环,"1. Rank: 机器在环, 人机协作, 机器参与循环

2. Explanation: The term ""机器在环"" (machine in the loop) is the most semantically accurate translation for the English term ""machine-in-the-loop."" It retains the original structure and meaning, directly reflecting the concept of integrating machines into a feedback loop with human operators. This is crucial in the AI domain, where the interaction between humans and machines is a key focus. 

""人机协作"" (human-machine collaboration) is a broader term that implies cooperation between humans and machines but does not capture the specific ""loop"" aspect of the original term, which is essential in contexts where feedback and iterative processes are involved. 

""机器参与循环"" (machines participate in the cycle) is less accurate as it suggests a more passive role for machines, which does not align with the active engagement implied by ""machine-in-the-loop."" 

Overall, ""机器在环"" is the best fit as it accurately conveys the intended meaning in the context of AI and maintains the specific terminology used in the field.","['machine in the loop', 'Human-machine collaboration', 'Machines participate in the cycle']"
2752,2752,macro-F1,宏F1值,0.0,7,"[{'word': '宏观F1', 'ratio': 0.5714285714285714}, {'word': '宏平均F1值', 'ratio': 0.2857142857142857}, {'word': '宏F1', 'ratio': 0.14285714285714285}]",宏观F1,{},[]
2753,2753,macro-action,宏观动作,0.8571428571428571,7,"[{'word': '宏观动作', 'ratio': 0.8571428571428571}, {'word': '宏动作', 'ratio': 0.14285714285714285}]",宏观动作,{},[]
2754,2754,macro-average,宏观平均,0.5714285714285714,7,"[{'word': '宏观平均', 'ratio': 0.5714285714285714}, {'word': '宏平均', 'ratio': 0.42857142857142855}]",宏观平均,{},[]
2755,2755,majority voting,多数投票,1.0,7,"[{'word': '多数投票', 'ratio': 1.0}]",多数投票,{},[]
2756,2756,manifold,流形,1.0,7,"[{'word': '流形', 'ratio': 1.0}]",流形,{},[]
2757,2757,manifold hypothesis,流形假设,1.0,4,"[{'word': '流形假设', 'ratio': 1.0}]",流形假设,{},[]
2758,2758,manifold learning,流形学习,1.0,4,"[{'word': '流形学习', 'ratio': 1.0}]",流形学习,{},[]
2759,2759,manifold projection,流形投影,1.0,4,"[{'word': '流形投影', 'ratio': 1.0}]",流形投影,{},[]
2760,2760,manifold structure,流形结构,1.0,4,"[{'word': '流形结构', 'ratio': 1.0}]",流形结构,{},[]
2761,2761,manifold-valued datum,流形值数据,1.0,4,"[{'word': '流形值数据', 'ratio': 1.0}]",流形值数据,{},[]
2762,2762,margin parameter,边际参数,0.6,10,"[{'word': '边际参数', 'ratio': 0.6}, {'word': '边界参数', 'ratio': 0.3}, {'word': '余量参数', 'ratio': 0.1}]",边际参数,{},[]
2763,2763,marginal density,边际密度,1.0,10,"[{'word': '边际密度', 'ratio': 1.0}]",边际密度,{},[]
2764,2764,marginal distribution,边际分布,1.0,10,"[{'word': '边际分布', 'ratio': 1.0}]",边际分布,{},[]
2765,2765,marginal inference,边缘推断,0.0,10,"[{'word': '边际推断', 'ratio': 0.7}, {'word': '边际推理', 'ratio': 0.3}]",边际推断,{},[]
2766,2766,marginal likelihood,边缘似然,0.0,10,"[{'word': '边际似然', 'ratio': 1.0}]",边际似然,{},[]
2767,2767,marginal log-likelihood,边缘对数似然,0.0,10,"[{'word': '边际对数似然', 'ratio': 1.0}]",边际对数似然,{},[]
2768,2768,marginal polytope,边际多面体,0.5,10,"[{'word': '边际多面体', 'ratio': 0.5}, {'word': '边际多胞形', 'ratio': 0.5}]",边际多面体,{},[]
2769,2769,marginal probability,边际概率,1.0,10,"[{'word': '边际概率', 'ratio': 1.0}]",边际概率,{},[]
2770,2770,marginalization,边缘化,0.0,10,"[{'word': '边际化', 'ratio': 1.0}]",边际化,{},[]
2771,2771,mask,掩码,1.0,8,"[{'word': '掩码', 'ratio': 1.0}]",掩码,{},[]
2772,2772,mask token,掩码标记,0.75,8,"[{'word': '掩码标记', 'ratio': 0.75}, {'word': '掩码令牌', 'ratio': 0.25}]",掩码标记,{},[]
2773,2773,mask vector,掩码向量,1.0,8,"[{'word': '掩码向量', 'ratio': 1.0}]",掩码向量,{},[]
2774,2774,masked input,掩码输入,1.0,8,"[{'word': '掩码输入', 'ratio': 1.0}]",掩码输入,{},[]
2775,2775,masked language model,掩码语言模型,1.0,8,"[{'word': '掩码语言模型', 'ratio': 1.0}]",掩码语言模型,{},[]
2776,2776,masked token,遮蔽词元,0.0,9,"[{'word': '被遮蔽的标记', 'ratio': 0.2222222222222222}, {'word': '掩蔽标记', 'ratio': 0.2222222222222222}, {'word': '掩码标记', 'ratio': 0.2222222222222222}, {'word': '掩码令牌', 'ratio': 0.1111111111111111}, {'word': '遮蔽标记', 'ratio': 0.1111111111111111}, {'word': '掩蔽词元', 'ratio': 0.1111111111111111}]","""掩码令牌""","1. Rank: ""掩码令牌"", ""掩蔽词元"", ""掩蔽标记"", ""掩蔽的标记"", ""掩蔽标记"", ""遮蔽标记"", ""掩码标记""

2. Explanation: The term ""掩码令牌"" (mask token) is the best fit for the AI domain because it accurately reflects the concept of a ""masked token"" used in Transformer-based language models. The word ""掩码"" (mask) directly corresponds to the action of masking in the context of machine learning, while ""令牌"" (token) is a commonly accepted term in AI and natural language processing to refer to the basic units of text. 

The back translation of ""掩码令牌"" as ""mask token"" maintains the original meaning and context, making it semantically accurate. In contrast, ""掩蔽词元"" (masking token) is also a strong candidate, but ""词元"" (word element) is less commonly used in the AI field compared to ""令牌"" (token). Other candidates like ""被遮蔽的标记"" (obscured mark) and ""遮蔽标记"" (mask mark) introduce ambiguity and do not align well with the established terminology in AI, as they do not convey the specific action of masking in the context of language models. Thus, ""掩码令牌"" is the most contextually appropriate and semantically accurate choice.","['obscured mark', 'masking mark', 'mask mark', 'mask token', 'mask mark', 'masking token']"
2777,2777,masking function,掩蔽函数,0.3333333333333333,9,"[{'word': '遮蔽函数', 'ratio': 0.3333333333333333}, {'word': '掩码函数', 'ratio': 0.3333333333333333}, {'word': '掩蔽函数', 'ratio': 0.3333333333333333}]",遮蔽函数,"1. Rank: 遮蔽函数, 掩蔽函数, 掩码函数

2. Explanation: The term ""遮蔽函数"" is the best fit for ""masking function"" in the AI domain due to its semantic accuracy and contextual relevance. In the context of AI and machine learning, ""masking"" often refers to the process of hiding or obscuring certain data points or features during model training or evaluation. The term ""遮蔽"" directly conveys the idea of covering or obscuring, which aligns well with the concept of masking in this context.

The second candidate, ""掩蔽函数,"" is also a strong contender as it translates back to ""masking function"" and retains a similar meaning. However, ""掩蔽"" is less commonly used in the AI context compared to ""遮蔽,"" which may lead to slight ambiguity in understanding.

The third candidate, ""掩码函数,"" translates back to ""mask function,"" which deviates from the original term ""masking function."" The term ""掩码"" is more associated with ""mask"" in a general sense and does not capture the functional aspect implied by ""masking function."" Therefore, it is less suitable for the specific AI context presented. 

Overall, ""遮蔽函数"" is the most contextually appropriate and semantically accurate choice for the term ""masking function"" in the AI domain.","['Masking function', 'mask function', 'masking function']"
2778,2778,matching algorithm,匹配算法,1.0,9,"[{'word': '匹配算法', 'ratio': 1.0}]",匹配算法,{},[]
2779,2779,matching loss,匹配损失,1.0,9,"[{'word': '匹配损失', 'ratio': 1.0}]",匹配损失,{},[]
2780,2780,matrix,矩阵,1.0,9,"[{'word': '矩阵', 'ratio': 1.0}]",矩阵,{},[]
2781,2781,matrix approximation,矩阵近似,1.0,10,"[{'word': '矩阵近似', 'ratio': 1.0}]",矩阵近似,{},[]
2782,2782,matrix decomposition,矩阵分解,1.0,10,"[{'word': '矩阵分解', 'ratio': 1.0}]",矩阵分解,{},[]
2783,2783,matrix form,矩阵形式,1.0,10,"[{'word': '矩阵形式', 'ratio': 1.0}]",矩阵形式,{},[]
2784,2784,matrix inversion,矩阵求逆,1.0,10,"[{'word': '矩阵求逆', 'ratio': 1.0}]",矩阵求逆,{},[]
2785,2785,matrix multiplication,矩阵乘法,1.0,10,"[{'word': '矩阵乘法', 'ratio': 1.0}]",矩阵乘法,{},[]
2786,2786,matrix norm,矩阵范数,1.0,8,"[{'word': '矩阵范数', 'ratio': 1.0}]",矩阵范数,{},[]
2787,2787,matrix sketching,矩阵素描,0.125,8,"[{'word': '矩阵草图', 'ratio': 0.75}, {'word': '矩阵抽样', 'ratio': 0.125}, {'word': '矩阵素描', 'ratio': 0.125}]",矩阵草图,{},[]
2788,2788,matrix vector product,矩阵向量积,0.25,8,"[{'word': '矩阵向量乘积', 'ratio': 0.75}, {'word': '矩阵向量积', 'ratio': 0.25}]",矩阵向量乘积,{},[]
2789,2789,matrix-vector multiplication,矩阵-向量乘法,0.625,8,"[{'word': '矩阵-向量乘法', 'ratio': 0.625}, {'word': '矩阵向量乘法', 'ratio': 0.375}]",矩阵-向量乘法,{},[]
2790,2790,matroid,拟阵,0.5,8,"[{'word': '拟阵', 'ratio': 0.5}, {'word': '代数族', 'ratio': 0.125}, {'word': '交错集', 'ratio': 0.125}, {'word': '伴随体', 'ratio': 0.125}, {'word': '族矩阵', 'ratio': 0.125}]",拟阵,{},[]
2791,2791,matroid constraint,拟阵约束,0.4,10,"[{'word': '拟阵约束', 'ratio': 0.4}, {'word': '依赖于母体的约束', 'ratio': 0.1}, {'word': '依赖于母体约束', 'ratio': 0.1}, {'word': '矩阵约束', 'ratio': 0.1}, {'word': '独立集约束', 'ratio': 0.1}, {'word': '伴随约束', 'ratio': 0.1}, {'word': 'matroid约束', 'ratio': 0.1}]","""拟阵约束""","1. Rank: ""拟阵约束"", ""matroid约束"", ""依赖于母体的约束"", ""依赖于母体约束"", ""独立集约束"", ""矩阵约束"", ""伴随约束""

2. Explanation: The term ""拟阵约束"" is the best fit for ""matroid constraint"" because it accurately captures the concept of a matroid in combinatorial optimization and graph theory, which is essential in the context of submodular maximization. The term ""拟阵"" is the standard Chinese translation for ""matroid,"" widely recognized in academic literature, ensuring semantic accuracy. The back translation ""matroid constraints"" aligns perfectly with the original English term, maintaining contextual integrity. 

The second candidate, ""matroid约束,"" while also correct, uses a transliteration of ""matroid,"" which may not be as widely understood in the Chinese academic community compared to ""拟阵."" The other candidates, such as ""依赖于母体的约束"" and ""独立集约束,"" do not accurately convey the specific mathematical structure and properties associated with matroids, making them less suitable in this context. ""矩阵约束"" (matrix constraints) is misleading as it refers to matrices rather than matroids, and ""伴随约束"" (adjoint constraints) does not relate to the concept at all. Thus, ""拟阵约束"" stands out as the most contextually and semantically appropriate choice.","['matroid constraints', 'Parent-dependent constraints', 'Depends on parent constraints', 'matrix constraints', 'independent set constraints', 'Adjoint constraints', 'matroid constraints']"
2792,2792,max norm,最大范数,1.0,10,"[{'word': '最大范数', 'ratio': 1.0}]",最大范数,{},[]
2793,2793,max pooling,最大池化,1.0,10,"[{'word': '最大池化', 'ratio': 1.0}]",最大池化,{},[]
2794,2794,max-margin,最大间隔,0.6,10,"[{'word': '最大间隔', 'ratio': 0.6}, {'word': '最大边际', 'ratio': 0.2}, {'word': '最大边距', 'ratio': 0.1}, {'word': '最大边界', 'ratio': 0.1}]",最大间隔,{},[]
2795,2795,max-margin learning,最大边际学习,0.2,10,"[{'word': '最大间隔学习', 'ratio': 0.6}, {'word': '最大边际学习', 'ratio': 0.2}, {'word': '最大边距学习', 'ratio': 0.1}, {'word': '最大边界学习', 'ratio': 0.1}]",最大间隔学习,{},[]
2796,2796,max-pool,最大池化,1.0,6,"[{'word': '最大池化', 'ratio': 1.0}]",最大池化,{},[]
2797,2797,max-pooling layer,最大池化层,1.0,6,"[{'word': '最大池化层', 'ratio': 1.0}]",最大池化层,{},[]
2798,2798,max-product semiring,最大乘积半环,0.1666666666666666,6,"[{'word': '最大积半环', 'ratio': 0.5}, {'word': '最大乘法半环', 'ratio': 0.3333333333333333}, {'word': '最大乘积半环', 'ratio': 0.16666666666666666}]",最大积半环,{},[]
2799,2799,maximal clique,最大团,1.0,6,"[{'word': '最大团', 'ratio': 1.0}]",最大团,{},[]
2800,2800,maximal frequent itemset,最大频繁项集,0.6666666666666666,6,"[{'word': '最大频繁项集', 'ratio': 0.6666666666666666}, {'word': '最大频繁项集 If you have any more questions or need further assistance, feel free to ask!', 'ratio': 0.16666666666666666}, {'word': '最大频繁项集 如果还有其他需要帮助的地方，请告诉我！', 'ratio': 0.16666666666666666}]",最大频繁项集,{},[]
2801,2801,maximal frequent pattern,最大频繁模式,0.7142857142857143,7,"[{'word': '最大频繁模式', 'ratio': 0.7142857142857143}, {'word': '极大频繁模式', 'ratio': 0.14285714285714285}, {'word': '凸替代', 'ratio': 0.14285714285714285}]",最大频繁模式,{},[]
2802,2802,maximization problem,最大化问题,0.8571428571428571,7,"[{'word': '最大化问题', 'ratio': 0.8571428571428571}, {'word': '凸替代', 'ratio': 0.14285714285714285}]",最大化问题,{},[]
2803,2803,maximum a posteriori,最大后验概率,0.4285714285714285,7,"[{'word': '最大后验概率', 'ratio': 0.42857142857142855}, {'word': '后验最大化', 'ratio': 0.2857142857142857}, {'word': '最大后验', 'ratio': 0.14285714285714285}, {'word': '凸替代', 'ratio': 0.14285714285714285}]",最大后验概率,"1. Rank: 最大后验概率, 最大后验, 后验最大化, 凸替代

2. Explanation: The term ""最大后验概率"" (maximum posterior probability) is the most accurate translation for ""maximum a posteriori"" in the context of AI and statistics. This term directly reflects the statistical concept of posterior probability, which is central to Bayesian inference. The back translation accurately conveys the original meaning, maintaining the focus on the probabilistic aspect of the term.

The second choice, ""最大后验"" (maximum posterior), is also a strong candidate as it succinctly captures the essence of the term without losing its meaning. However, it is slightly less explicit than ""最大后验概率,"" which clearly indicates that it refers to a probability.

""后验最大化"" (posterior maximization) is less preferred because it emphasizes the action of maximizing rather than the concept of posterior probability itself, which is not the primary focus of the term ""maximum a posteriori.""

Lastly, ""凸替代"" (convex substitution) is unrelated to the original term and does not fit within the context of Bayesian statistics or AI, making it the least suitable option.","['maximum posterior probability', 'posterior maximization', 'maximum posterior', 'convex substitution']"
2804,2804,maximum a posteriori estimation,最大后验估计,0.2857142857142857,7,"[{'word': '后验最大化估计', 'ratio': 0.2857142857142857}, {'word': '最大后验估计', 'ratio': 0.2857142857142857}, {'word': '最大后验概率估计', 'ratio': 0.2857142857142857}, {'word': '凸替代', 'ratio': 0.14285714285714285}]",最大后验估计,"1. Rank: 最大后验估计, 后验最大化估计, 最大后验概率估计, 凸替代

2. Explanation: The term ""最大后验估计"" (maximum a posteriori estimate) is the best fit for the context of AI and statistics because it directly corresponds to the English term ""maximum a posteriori estimation"" without introducing unnecessary complexity. This term is widely recognized in the statistical and machine learning communities, making it semantically accurate and contextually appropriate. 

The second candidate, ""后验最大化估计"" (posterior maximization estimate), while also relevant, emphasizes the maximization aspect rather than the estimation aspect, which can lead to confusion in the context of probabilistic models. The third candidate, ""最大后验概率估计"" (maximum posterior probability estimate), introduces the word ""概率"" (probability), which, although related, shifts the focus slightly away from the estimation process itself. 

Lastly, ""凸替代"" (convex substitution) is unrelated to the term in question and should not be considered in this context. Therefore, ""最大后验估计"" is the most accurate and contextually fitting translation for the term in the AI domain.","['posterior maximization estimate', 'maximum a posteriori estimate', 'maximum posterior probability estimate', 'convex substitution']"
2805,2805,maximum clique,最大团,0.7142857142857143,7,"[{'word': '最大团', 'ratio': 0.7142857142857143}, {'word': '凸替代', 'ratio': 0.14285714285714285}, {'word': '最大集团', 'ratio': 0.14285714285714285}]",最大团,{},[]
2806,2806,maximum entropy,最大熵,1.0,7,"[{'word': '最大熵', 'ratio': 1.0}]",最大熵,{},[]
2807,2807,maximum entropy model,最大熵模型,1.0,7,"[{'word': '最大熵模型', 'ratio': 1.0}]",最大熵模型,{},[]
2808,2808,maximum entropy principle,最大熵原理,0.8571428571428571,7,"[{'word': '最大熵原理', 'ratio': 0.8571428571428571}, {'word': '最大熵原则', 'ratio': 0.14285714285714285}]",最大熵原理,{},[]
2809,2809,maximum flow,最大流量,0.0,7,"[{'word': '最大流', 'ratio': 1.0}]",最大流,{},[]
2810,2810,maximum likelihood estimate,最大似然估计,1.0,7,"[{'word': '最大似然估计', 'ratio': 1.0}]",最大似然估计,{},[]
2811,2811,maximum likelihood estimation,最大似然估计,1.0,5,"[{'word': '最大似然估计', 'ratio': 1.0}]",最大似然估计,{},[]
2812,2812,maximum likelihood estimator,最大似然估计量,0.8,5,"[{'word': '最大似然估计量', 'ratio': 0.8}, {'word': '最大似然估计量 maximum likelihood lea', 'ratio': 0.2}]",最大似然估计量,{},[]
2813,2813,maximum likelihood learning,最大似然学习,1.0,5,"[{'word': '最大似然学习', 'ratio': 1.0}]",最大似然学习,{},[]
2814,2814,maximum mean discrepancy,最大均值差异,0.8,5,"[{'word': '最大均值差异', 'ratio': 0.8}, {'word': '最大平均差异', 'ratio': 0.2}]",最大均值差异,{},[]
2815,2815,mean average precision,平均精度,0.0,5,"[{'word': '平均平均精度', 'ratio': 0.4}, {'word': '平均精确度均值', 'ratio': 0.4}, {'word': '平均精度均值', 'ratio': 0.2}]",平均精度均值,"1. Rank: 平均精度均值, 平均精确度均值, 平均平均精度

2. Explanation: The term ""平均精度均值"" (average precision mean) is the best fit for ""mean average precision"" in the AI domain. This translation accurately captures the semantic meaning of the original term, as it directly translates to ""average precision"" and includes ""mean,"" which is essential in the context of evaluating the performance of models in machine learning and information retrieval. 

The second candidate, ""平均精确度均值"" (average accuracy mean), while also relevant, introduces the term ""accuracy,"" which can lead to confusion since ""precision"" and ""accuracy"" are distinct metrics in the context of AI and machine learning. 

The first candidate, ""平均平均精度"" (average average accuracy), is redundant and does not convey the intended meaning effectively, as it repeats ""average"" unnecessarily and does not align with the standard terminology used in the field. 

Thus, ""平均精度均值"" is the most semantically accurate and contextually appropriate translation for ""mean average precision.""","['average average accuracy', 'average accuracy mean', 'average precision mean']"
2816,2816,mean field,平均场,0.4,10,"[{'word': '平均场', 'ratio': 0.4}, {'word': '均值场', 'ratio': 0.3}, {'word': '均场', 'ratio': 0.2}, {'word': '几何变换', 'ratio': 0.1}]",平均场,"1. Rank: 平均场, 均值场, 均场, 几何变换

2. Explanation: The term ""平均场"" (mean field) is the most accurate translation for the context of AI and statistical physics. It directly corresponds to the English term ""mean field"" and is widely recognized in the field. The back translation is also accurate, maintaining the original meaning. 

""均值场"" (mean value field) is a close second, as it conveys a similar concept, but ""均值"" (mean value) is less commonly used in this specific context compared to ""平均"" (average). 

""均场"" (average field) is less precise because it does not capture the specific statistical context implied by ""mean field."" 

Finally, ""几何变换"" (geometric transformation) is completely unrelated to the term ""mean field"" and should not be considered a candidate in this context. 

Overall, ""平均场"" is the best fit due to its semantic accuracy, contextual relevance, and established usage in the AI domain.","['mean field', 'mean field', 'average field', 'geometric transformation']"
2817,2817,mean function,均值函数,0.7,10,"[{'word': '均值函数', 'ratio': 0.7}, {'word': '平均函数', 'ratio': 0.2}, {'word': '几何处理', 'ratio': 0.1}]",均值函数,{},[]
2818,2818,mean pooling,均值汇聚,0.0,10,"[{'word': '均值池化', 'ratio': 0.5}, {'word': '平均池化', 'ratio': 0.4}, {'word': '吉布斯分布', 'ratio': 0.1}]",均值池化,{},[]
2819,2819,mean reciprocal rank,平均倒数排名,0.2,10,"[{'word': '平均倒排排名', 'ratio': 0.4}, {'word': '均值倒排秩', 'ratio': 0.2}, {'word': '平均倒数排名', 'ratio': 0.2}, {'word': '均值倒排排名', 'ratio': 0.1}, {'word': '要旨描述符', 'ratio': 0.1}]","""均值倒排秩""","1. Rank: ""均值倒排秩"", ""均值倒排排名"", ""平均倒数排名"", ""平均倒排排名"", ""要旨描述符""

2. Explanation: The term ""均值倒排秩"" (mean inverted rank) is the best fit for the translation of ""mean reciprocal rank"" because it accurately captures the statistical nature of the term ""mean"" (均值) and the concept of ranking (倒排秩) in a way that aligns with AI and information retrieval contexts. The use of ""倒排"" (inverted) is appropriate here, as it relates to ranking systems where items are evaluated based on their relevance or score. 

The second choice, ""均值倒排排名,"" is also a good candidate but slightly less precise because ""排名"" (ranking) is more general and does not convey the specific statistical meaning of ""reciprocal."" The third option, ""平均倒数排名,"" introduces ""倒数"" (reciprocal) but does not maintain the statistical context as effectively as ""倒排秩."" 

The last two options, ""平均倒排排名"" and ""要旨描述符,"" are less relevant as they either misinterpret the term or stray from the specific AI context. ""要旨描述符"" (gist descriptor) is unrelated to ranking and does not fit the context at all. Thus, ""均值倒排秩"" stands out as the most semantically accurate and contextually appropriate translation.","['Average inverted ranking', 'mean inverted rank', 'average last rank', 'Mean inverted ranking', 'gist descriptor']"
2820,2820,mean shape,平均形状,0.5,10,"[{'word': '平均形状', 'ratio': 0.5}, {'word': '均值形状', 'ratio': 0.4}, {'word': '全局平均池化', 'ratio': 0.1}]",平均形状,{},[]
2821,2821,mean square error,均方误差,1.0,9,"[{'word': '均方误差', 'ratio': 1.0}]",均方误差,{},[]
2822,2822,mean vector,均值向量,0.8888888888888888,9,"[{'word': '均值向量', 'ratio': 0.8888888888888888}, {'word': '平均向量', 'ratio': 0.1111111111111111}]",均值向量,{},[]
2823,2823,mean-field approximation,平均场近似,0.6666666666666666,9,"[{'word': '平均场近似', 'ratio': 0.6666666666666666}, {'word': '均场近似', 'ratio': 0.2222222222222222}, {'word': '均值场近似', 'ratio': 0.1111111111111111}]",平均场近似,{},[]
2824,2824,measurable space,可测空间,0.8888888888888888,9,"[{'word': '可测空间', 'ratio': 0.8888888888888888}, {'word': '可测量的空间', 'ratio': 0.1111111111111111}]",可测空间,{},[]
2825,2825,measurement matrix,测量矩阵,1.0,9,"[{'word': '测量矩阵', 'ratio': 1.0}]",测量矩阵,{},[]
2826,2826,measurement noise,测量噪声,1.0,9,"[{'word': '测量噪声', 'ratio': 1.0}]",测量噪声,{},[]
2827,2827,medical imaging,医学成像,0.7777777777777778,9,"[{'word': '医学成像', 'ratio': 0.7777777777777778}, {'word': '医学影像学', 'ratio': 0.1111111111111111}, {'word': '医学影像', 'ratio': 0.1111111111111111}]",医学成像,{},[]
2828,2828,medoid,中心对象,0.1111111111111111,9,"[{'word': '中心点', 'ratio': 0.4444444444444444}, {'word': '中位文档', 'ratio': 0.1111111111111111}, {'word': '中心样本', 'ratio': 0.1111111111111111}, {'word': '中心对象', 'ratio': 0.1111111111111111}, {'word': '中心文档', 'ratio': 0.1111111111111111}, {'word': '中位点', 'ratio': 0.1111111111111111}]","""中位文档""","1. Rank: ""中位文档"", ""中心文档"", ""中心样本"", ""中心点"", ""中心对象"", ""中位点""

2. Explanation: The term ""中位文档"" (back translated as ""median document"") is the best fit for the AI domain-specific usage of ""medoid."" In clustering contexts, particularly in machine learning and data analysis, a ""medoid"" refers to the most centrally located object in a cluster, which is often represented by the document that minimizes the average dissimilarity to all other documents in that cluster. The term ""中位文档"" accurately captures this concept by emphasizing the document aspect and the centrality implied by ""medoid."" 

While ""中心文档"" (central document) is also a strong candidate, it lacks the specific statistical connotation of ""median,"" which is crucial in distinguishing the medoid from other central tendencies like the mean. The other candidates, such as ""中心样本"" (center sample) and ""中心点"" (center point), are less precise in the context of document clustering, as they do not specifically denote a document or imply the statistical properties associated with a medoid. Therefore, ""中位文档"" is the most semantically accurate and contextually appropriate choice.","['center point', 'median document', 'center sample', 'central object', 'central document', 'midpoint']"
2829,2829,membership inference attack,成员推断攻击 (membership inference attack),0.0,9,"[{'word': '成员推断攻击', 'ratio': 0.4444444444444444}, {'word': '会员推断攻击', 'ratio': 0.2222222222222222}, {'word': '成员推理攻击', 'ratio': 0.2222222222222222}, {'word': '成员身份推断攻击', 'ratio': 0.1111111111111111}]",成员推断攻击,"1. Rank: 成员推断攻击, 成员身份推断攻击, 会员推断攻击, 成员推理攻击

2. Explanation: The term ""成员推断攻击"" (Member inference attack) is the best fit for the following reasons:

- **Semantic Accuracy**: ""成员推断攻击"" directly translates to ""member inference attack,"" which closely aligns with the original English term ""membership inference attack."" The use of ""成员"" (member) accurately reflects the concept of membership in the context of machine learning models, where the focus is on whether an input belongs to the training dataset.

- **Contextual Fit**: In the AI domain, ""membership"" is a key term that refers to the inclusion of data points in a training set. The term ""推断"" (inference) is also appropriate as it conveys the idea of deducing or inferring information about the membership status of data points based on model outputs.

- **Back Translation Accuracy**: The back translation of ""成员推断攻击"" as ""Member inference attack"" maintains the original meaning and context, ensuring that it is easily understood by both English and Chinese-speaking audiences familiar with AI terminology.

The other candidates, while they may be understandable, do not capture the specific nuances of the term as effectively. For instance, ""会员推断攻击"" (Membership inference attack) could imply a more commercial or membership-based context, which is not the focus here. ""成员推理攻击"" (Member inference attack) uses ""推理"" (reasoning) instead of ""推断"" (inference), which is less precise in this context. ""成员身份推断攻击"" (Member identity inference attack) adds unnecessary complexity by including ""身份"" (identity), which is not explicitly part of the original term and could lead to confusion.","['Member inference attack', 'Membership inference attack', 'Member inference attack', 'Membership inference attack']"
2830,2830,membership query,成员查询,0.8,10,"[{'word': '成员查询', 'ratio': 0.8}, {'word': '会员查询', 'ratio': 0.2}]",成员查询,{},[]
2831,2831,memory bank,内存库,0.6,10,"[{'word': '内存库', 'ratio': 0.6}, {'word': '记忆库', 'ratio': 0.2}, {'word': '存储库', 'ratio': 0.2}]",内存库,{},[]
2832,2832,memory capacity,记忆容量,0.2,10,"[{'word': '内存容量', 'ratio': 0.4}, {'word': '存储容量', 'ratio': 0.3}, {'word': '记忆容量', 'ratio': 0.2}, {'word': '记忆单元', 'ratio': 0.1}]",记忆容量,"1. Rank: 记忆容量, 内存容量, 存储容量, 记忆单元

2. Explanation: The term ""记忆容量"" (memory capacity) is the best fit for the context provided, as it directly translates the English term while maintaining the semantic focus on memory in the context of AI and neural networks. In the AI domain, ""记忆"" (memory) specifically refers to the ability of a system to retain and utilize information, which aligns perfectly with the discussions of synaptic complexity and memory budget in the provided context. 

""内存容量"" (internal memory capacity) is a close second, but it typically refers to the physical memory in computing systems, which may not fully capture the conceptual aspect of memory in neural networks. ""存储容量"" (storage capacity) is more general and often relates to data storage rather than cognitive memory, making it less suitable. Lastly, ""记忆单元"" (memory unit) refers to a component or unit of memory rather than the overall capacity, which does not fit the context of discussing the capacity itself. Thus, ""记忆容量"" is the most contextually appropriate choice.","['Memory capacity', 'storage capacity', 'memory capacity', 'memory unit']"
2833,2833,memory cell,记忆细胞,0.0,10,"[{'word': '存储单元', 'ratio': 0.3}, {'word': '内存单元', 'ratio': 0.3}, {'word': '记忆单元', 'ratio': 0.2}, {'word': 'In the context of memo', 'ratio': 0.1}, {'word': '记忆容量', 'ratio': 0.1}]","""内存单元""","1. Rank: ""内存单元"", ""记忆单元"", ""存储单元"", ""记忆容量"", ""In the context of memo""

2. Explanation: The term ""内存单元"" (memory unit) is the best fit for the AI domain-specific usage of ""memory cell."" In the context of AI and computing, ""内存"" specifically refers to memory in the sense of RAM or volatile memory, which is directly related to the concept of a memory cell in neural networks and other computational models. 

""记忆单元"" (memory unit) is also a valid translation, but it is less commonly used in technical contexts compared to ""内存单元."" The term ""存储单元"" (storage unit) is broader and can refer to any type of storage, not specifically memory in the computational sense, making it less precise. ""记忆容量"" (memory capacity) refers to the amount of memory available rather than the concept of a memory cell itself, which is not contextually accurate. Lastly, ""In the context of memo"" is irrelevant and does not pertain to the technical meaning of ""memory cell."" 

Thus, ""内存单元"" is the most semantically accurate and contextually appropriate term for the AI domain.","['storage unit', 'memory unit', 'memory unit', 'int and context of memo', 'memory capacity']"
2834,2834,memory complexity,内存复杂度,0.7,10,"[{'word': '内存复杂度', 'ratio': 0.7}, {'word': '存储复杂度', 'ratio': 0.2}, {'word': '记忆复杂度', 'ratio': 0.1}]",内存复杂度,{},[]
2835,2835,mention detection,实体提及检测,0.0,10,"[{'word': '提及检测', 'ratio': 0.8}, {'word': '术语检测', 'ratio': 0.1}, {'word': '指称检测', 'ratio': 0.1}]",提及检测,{},[]
2836,2836,meronymy,部分整体关系,0.0,10,"[{'word': '部分关系', 'ratio': 0.8}, {'word': '部分关系消息传递', 'ratio': 0.1}, {'word': '部分-整体关系', 'ratio': 0.1}]",部分关系,{},[]
2837,2837,message passing,消息传递,1.0,10,"[{'word': '消息传递', 'ratio': 1.0}]",消息传递,{},[]
2838,2838,message passing algorithm,消息传递算法,0.9,10,"[{'word': '消息传递算法', 'ratio': 0.9}, {'word': '消息传递', 'ratio': 0.1}]",消息传递算法,{},[]
2839,2839,meta,元,0.9,10,"[{'word': '元', 'ratio': 0.9}, {'word': '元数据', 'ratio': 0.1}]",元,{},[]
2840,2840,meta-algorithm,元算法,1.0,10,"[{'word': '元算法', 'ratio': 1.0}]",元算法,{},[]
2841,2841,meta-classifier,元分类器,1.0,10,"[{'word': '元分类器', 'ratio': 1.0}]",元分类器,{},[]
2842,2842,meta-dataset,元数据集,1.0,10,"[{'word': '元数据集', 'ratio': 1.0}]",元数据集,{},[]
2843,2843,meta-evaluation,元评估,1.0,10,"[{'word': '元评估', 'ratio': 1.0}]",元评估,{},[]
2844,2844,meta-learn,元学习,1.0,10,"[{'word': '元学习', 'ratio': 1.0}]",元学习,{},[]
2845,2845,meta-learner,元学习器,0.5,10,"[{'word': '元学习器', 'ratio': 0.5}, {'word': '元学习者', 'ratio': 0.5}]",元学习器,{},[]
2846,2846,meta-loss,元损失,1.0,10,"[{'word': '元损失', 'ratio': 1.0}]",元损失,{},[]
2847,2847,meta-parameter,元参数,1.0,10,"[{'word': '元参数', 'ratio': 1.0}]",元参数,{},[]
2848,2848,meta-testing,元测试,1.0,10,"[{'word': '元测试', 'ratio': 1.0}]",元测试,{},[]
2849,2849,meta-training,元训练,0.9,10,"[{'word': '元训练', 'ratio': 0.9}, {'word': '元参数', 'ratio': 0.1}]",元训练,{},[]
2850,2850,metadata,元数据,1.0,9,"[{'word': '元数据', 'ratio': 1.0}]",元数据,{},[]
2851,2851,metric learning,度量学习,1.0,9,"[{'word': '度量学习', 'ratio': 1.0}]",度量学习,{},[]
2852,2852,metric learning algorithm,度量学习算法,1.0,9,"[{'word': '度量学习算法', 'ratio': 1.0}]",度量学习算法,{},[]
2853,2853,metric score,评测分数,0.0,9,"[{'word': '指标得分', 'ratio': 0.2222222222222222}, {'word': '指标评分', 'ratio': 0.2222222222222222}, {'word': '度量分数', 'ratio': 0.2222222222222222}, {'word': '指标分数', 'ratio': 0.1111111111111111}, {'word': '评价指标分数c', 'ratio': 0.1111111111111111}, {'word': '评分指标', 'ratio': 0.1111111111111111}]","""度量分数""","1. Rank: ""度量分数"", ""指标得分"", ""指标评分"", ""指标分数"", ""评价指标分数"", ""评分指标""

2. Explanation: The term ""度量分数"" (metric score) is the best fit for the AI domain-specific usage because it directly translates the concept of ""metric"" in a way that is widely recognized in the context of evaluation metrics in machine translation (MT) and other AI applications. The term ""度量"" (metric) is commonly used in technical discussions to refer to measurement standards, while ""分数"" (score) clearly indicates a numerical value assigned based on certain criteria. 

In contrast, ""指标得分"" (indicator score) and ""指标评分"" (metric score) are less precise because ""指标"" (indicator) can imply a broader range of meanings and may not specifically convey the idea of a quantitative measure as effectively as ""度量"". The other candidates, such as ""评价指标分数"" (evaluation index score) and ""评分指标"" (rating indicators), are overly complex or vague for the context, as they introduce additional terms that may not be necessary for understanding the specific concept of a metric score in AI evaluations. 

Overall, ""度量分数"" maintains semantic accuracy and contextual fit, making it the most suitable choice for the given context.","['indicator score', 'Metric score', 'metric score', 'indicator score', 'Evaluation index scorec', 'Rating indicators']"
2854,2854,metric space,度量空间,1.0,9,"[{'word': '度量空间', 'ratio': 1.0}]",度量空间,{},[]
2855,2855,micro-average,微平均,1.0,9,"[{'word': '微平均', 'ratio': 1.0}]",微平均,{},[]
2856,2856,microarray datum,微阵列数据,1.0,9,"[{'word': '微阵列数据', 'ratio': 1.0}]",微阵列数据,{},[]
2857,2857,mini-batch,小批量,1.0,9,"[{'word': '小批量', 'ratio': 1.0}]",小批量,{},[]
2858,2858,mini-batch size,迷你批量大小,0.0,9,"[{'word': '小批量大小', 'ratio': 1.0}]",小批量大小,{},[]
2859,2859,mini-batch training,小批量训练,1.0,9,"[{'word': '小批量训练', 'ratio': 1.0}]",小批量训练,{},[]
2860,2860,minima,极小值,0.5555555555555556,9,"[{'word': '极小值', 'ratio': 0.5555555555555556}, {'word': '最小值', 'ratio': 0.4444444444444444}]",极小值,{},[]
2861,2861,minimax,极小极大,0.6666666666666666,9,"[{'word': '极小极大', 'ratio': 0.6666666666666666}, {'word': '最小化最大化', 'ratio': 0.1111111111111111}, {'word': '极大极小】', 'ratio': 0.1111111111111111}, {'word': '极小化最大化', 'ratio': 0.1111111111111111}]",极小极大,{},[]
2862,2862,minimax game,最小最大博弈,0.2,10,"[{'word': '极小极大博弈', 'ratio': 0.7}, {'word': '最小最大博弈', 'ratio': 0.2}, {'word': '极小极大游戏', 'ratio': 0.1}]",极小极大博弈,{},[]
2863,2863,minimax optimization problem,极小极大优化问题,0.8,10,"[{'word': '极小极大优化问题', 'ratio': 0.8}, {'word': '最小最大优化问题', 'ratio': 0.2}]",极小极大优化问题,{},[]
2864,2864,minimax problem,极小极大问题,0.8,10,"[{'word': '极小极大问题', 'ratio': 0.8}, {'word': '最小最大问题', 'ratio': 0.2}]",极小极大问题,{},[]
2865,2865,minimization problem,最小化问题,1.0,10,"[{'word': '最小化问题', 'ratio': 1.0}]",最小化问题,{},[]
2866,2866,minimizer,最小化器,0.8,10,"[{'word': '最小化器', 'ratio': 0.8}, {'word': '最小值点 / 最优解', 'ratio': 0.1}, {'word': '最小化者', 'ratio': 0.1}]",最小化器,{},[]
2867,2867,minimum baye risk decoding,最小贝叶斯风险解码,1.0,10,"[{'word': '最小贝叶斯风险解码', 'ratio': 1.0}]",最小贝叶斯风险解码,{},[]
2868,2868,minimum cut,最小割,1.0,10,"[{'word': '最小割', 'ratio': 1.0}]",最小割,{},[]
2869,2869,minimum description length,最小描述长度,1.0,10,"[{'word': '最小描述长度', 'ratio': 1.0}]",最小描述长度,{},[]
2870,2870,minimum support,最小支持度,1.0,10,"[{'word': '最小支持度', 'ratio': 1.0}]",最小支持度,{},[]
2871,2871,mirror descent,镜像下降,0.6,10,"[{'word': '镜像下降', 'ratio': 0.6}, {'word': '镜像下降法', 'ratio': 0.3}, {'word': '镜面下降法', 'ratio': 0.1}]",镜像下降,{},[]
2872,2872,misclassification error,误分类误差,0.0,9,"[{'word': '误分类错误', 'ratio': 0.4444444444444444}, {'word': '错误分类率', 'ratio': 0.2222222222222222}, {'word': '错分误差', 'ratio': 0.1111111111111111}, {'word': '错分类误差', 'ratio': 0.1111111111111111}, {'word': '错误分类错误', 'ratio': 0.1111111111111111}]",误分类错误,"1. Rank: 误分类错误, 错分类误差, 错分误差, 错误分类率, 错误分类错误

2. Explanation: The term ""误分类错误"" is the best fit for ""misclassification error"" because it directly translates to ""misclassification error"" in a way that retains the original meaning and context. The term ""误分类"" (misclassification) accurately captures the concept of incorrectly classifying data points, while ""错误"" (error) emphasizes the nature of the mistake. This term is widely recognized in the AI and machine learning community, making it semantically accurate and contextually appropriate.

The second choice, ""错分类误差,"" also conveys the idea of misclassification but is slightly less common in usage compared to ""误分类错误."" ""错分误差"" is similar but may not be as widely accepted in the AI domain. ""错误分类率"" translates to ""misclassification rate,"" which, while related, refers specifically to the proportion of misclassifications rather than the error itself, making it less accurate in this context. Lastly, ""错误分类错误"" is redundant and less clear, as it combines ""error"" with ""classification"" and ""error"" again, which does not add value to the term.","['misclassification error', 'Misclassification rate', 'misclassification error', 'misclassification error', 'misclassification error']"
2873,2873,misclassification loss,误分类损失,0.4444444444444444,9,"[{'word': '误分类损失', 'ratio': 0.4444444444444444}, {'word': '错误分类损失', 'ratio': 0.3333333333333333}, {'word': '错分损失', 'ratio': 0.1111111111111111}, {'word': '错分类损失', 'ratio': 0.1111111111111111}]",误分类损失,"1. Rank: 误分类损失, 错分类损失, 错误分类损失, 错分损失

2. Explanation: The term ""误分类损失"" is the best fit for ""misclassification loss"" because it accurately captures the semantic meaning of the original English term within the context of AI and machine learning. ""误"" (mis) directly translates to ""mis"" in ""misclassification,"" indicating an error in classification. The term ""分类"" (classification) is also a direct match, and ""损失"" (loss) correctly conveys the concept of loss in this context. 

""错分类损失"" is a close second, as it also means ""misclassification loss,"" but ""错"" (wrong) is slightly less precise than ""误"" in this technical context. ""错误分类损失"" translates to ""error classification loss,"" which introduces ambiguity since ""错误"" (error) can imply a broader range of mistakes, not specifically tied to classification. Lastly, ""错分损失"" translates to ""misdiagnosis loss,"" which diverges from the original meaning, as ""诊断"" (diagnosis) is not relevant in the context of classification tasks in machine learning. Thus, ""误分类损失"" is the most semantically accurate and contextually appropriate term for the AI domain.","['misclassification loss', 'misclassification loss', 'misdiagnosis loss', 'misclassification loss']"
2874,2874,misinformation detection,虚假信息检测,0.5555555555555556,9,"[{'word': '虚假信息检测', 'ratio': 0.5555555555555556}, {'word': '错误信息检测', 'ratio': 0.2222222222222222}, {'word': '误信息检测', 'ratio': 0.1111111111111111}, {'word': '错误分类检测', 'ratio': 0.1111111111111111}]",虚假信息检测,{},[]
2875,2875,mixed integer programming,混合整数规划 (MIP),0.0,9,"[{'word': '混合整数规划', 'ratio': 0.8888888888888888}, {'word': '错误信息检测', 'ratio': 0.1111111111111111}]",混合整数规划,{},[]
2876,2876,mixed precision,混合精度,1.0,9,"[{'word': '混合精度', 'ratio': 1.0}]",混合精度,{},[]
2877,2877,mixed precision training,混合精度训练,1.0,10,"[{'word': '混合精度训练', 'ratio': 1.0}]",混合精度训练,{},[]
2878,2878,mixed strategy,混合策略,1.0,10,"[{'word': '混合策略', 'ratio': 1.0}]",混合策略,{},[]
2879,2879,mixed-integer program,混合整数规划,1.0,10,"[{'word': '混合整数规划', 'ratio': 1.0}]",混合整数规划,{},[]
2880,2880,mixing matrix,混合矩阵,1.0,10,"[{'word': '混合矩阵', 'ratio': 1.0}]",混合矩阵,{},[]
2881,2881,mixing time,混合时间,1.0,10,"[{'word': '混合时间', 'ratio': 1.0}]",混合时间,{},[]
2882,2882,mixing weight,混合权重,1.0,10,"[{'word': '混合权重', 'ratio': 1.0}]",混合权重,{},[]
2883,2883,mixture component,混合成分,1.0,10,"[{'word': '混合成分', 'ratio': 1.0}]",混合成分,{},[]
2884,2884,mixture distribution,混合分布,1.0,10,"[{'word': '混合分布', 'ratio': 1.0}]",混合分布,{},[]
2885,2885,mixture model,混合模型,1.0,10,"[{'word': '混合模型', 'ratio': 1.0}]",混合模型,{},[]
2886,2886,mixture of Gaussians,高斯混合模型,0.6,10,"[{'word': '高斯混合模型', 'ratio': 0.6}, {'word': '高斯混合', 'ratio': 0.4}]",高斯混合模型,{},[]
2887,2887,mixture weight,混合权重,1.0,10,"[{'word': '混合权重', 'ratio': 1.0}]",混合权重,{},[]
2888,2888,mocap,动作捕捉数据,0.0,10,"[{'word': '动作捕捉', 'ratio': 1.0}]",动作捕捉,{},[]
2889,2889,modality,模态,1.0,10,"[{'word': '模态', 'ratio': 1.0}]",模态,{},[]
2890,2890,mode,模式,1.0,10,"[{'word': '模式', 'ratio': 1.0}]",模式,{},[]
2891,2891,mode collapse,模式崩溃,0.8888888888888888,9,"[{'word': '模式崩溃', 'ratio': 0.8888888888888888}, {'word': '模式崩塌', 'ratio': 0.1111111111111111}]",模式崩溃,{},[]
2892,2892,model M,模型 M,0.7777777777777778,9,"[{'word': '模型 M', 'ratio': 0.7777777777777778}, {'word': 'M模型', 'ratio': 0.1111111111111111}, {'word': '模型M', 'ratio': 0.1111111111111111}]",模型 M,{},[]
2893,2893,model accuracy,模型准确率,0.6666666666666666,9,"[{'word': '模型准确率', 'ratio': 0.6666666666666666}, {'word': '模型准确性', 'ratio': 0.3333333333333333}]",模型准确率,{},[]
2894,2894,model architecture,模型架构,1.0,9,"[{'word': '模型架构', 'ratio': 1.0}]",模型架构,{},[]
2895,2895,model averaging,模型平均,1.0,9,"[{'word': '模型平均', 'ratio': 1.0}]",模型平均,{},[]
2896,2896,model bias,模型偏差,1.0,9,"[{'word': '模型偏差', 'ratio': 1.0}]",模型偏差,{},[]
2897,2897,model capacity,模型容量,1.0,9,"[{'word': '模型容量', 'ratio': 1.0}]",模型容量,{},[]
2898,2898,model card,模型卡,0.5555555555555556,9,"[{'word': '模型卡', 'ratio': 0.5555555555555556}, {'word': '模型卡片', 'ratio': 0.3333333333333333}, {'word': '模型说明书', 'ratio': 0.1111111111111111}]",模型卡,{},[]
2899,2899,model checking,模型检查,0.4444444444444444,9,"[{'word': '模型检查', 'ratio': 0.4444444444444444}, {'word': '模型检验', 'ratio': 0.3333333333333333}, {'word': '模型检测', 'ratio': 0.2222222222222222}]",模型检查,"1. Rank: 模型检查, 模型检测, 模型检验

2. Explanation: The term ""模型检查"" (model checking) is the most accurate translation for the AI domain, as it directly corresponds to the established terminology used in formal verification and computer science. ""检查"" (checking) conveys the process of verifying the properties of a model, which is central to the concept of model checking. 

""模型检测"" (model detection) is a close second, as it also relates to the idea of examining a model, but it can imply a broader range of activities that may not specifically align with the formal verification context. 

""模型检验"" (model testing) is the least accurate in this context, as ""检验"" (testing) typically refers to a different process that involves evaluating a model's performance or functionality rather than verifying its correctness against specifications. In the context of AI and formal methods, ""model checking"" is a well-defined term, making ""模型检查"" the best fit.","['Model checking', 'Model testing', 'Model checking']"
2900,2900,model class,模型类,0.75,8,"[{'word': '模型类', 'ratio': 0.75}, {'word': '模型类别', 'ratio': 0.25}]",模型类,{},[]
2901,2901,model comparison,模型比较,1.0,8,"[{'word': '模型比较', 'ratio': 1.0}]",模型比较,{},[]
2902,2902,model complexity,模型复杂度,0.75,8,"[{'word': '模型复杂度', 'ratio': 0.75}, {'word': '模型复杂性', 'ratio': 0.25}]",模型复杂度,{},[]
2903,2903,model compression,模型压缩,1.0,8,"[{'word': '模型压缩', 'ratio': 1.0}]",模型压缩,{},[]
2904,2904,model convergence,模型收敛,0.875,8,"[{'word': '模型收敛', 'ratio': 0.875}, {'word': '模型收敛 如果您还有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.125}]",模型收敛,{},[]
2905,2905,model development,模型开发,1.0,9,"[{'word': '模型开发', 'ratio': 1.0}]",模型开发,{},[]
2906,2906,model distillation,模型蒸馏,1.0,9,"[{'word': '模型蒸馏', 'ratio': 1.0}]",模型蒸馏,{},[]
2907,2907,model distribution,模型分布,1.0,9,"[{'word': '模型分布', 'ratio': 1.0}]",模型分布,{},[]
2908,2908,model estimation,模型估计,1.0,9,"[{'word': '模型估计', 'ratio': 1.0}]",模型估计,{},[]
2909,2909,model evaluation,模型评估,1.0,9,"[{'word': '模型评估', 'ratio': 1.0}]",模型评估,{},[]
2910,2910,model family,模型家族,0.5555555555555556,9,"[{'word': '模型家族', 'ratio': 0.5555555555555556}, {'word': '模型族', 'ratio': 0.4444444444444444}]",模型家族,{},[]
2911,2911,model fine-tuning,模型微调,1.0,9,"[{'word': '模型微调', 'ratio': 1.0}]",模型微调,{},[]
2912,2912,model generalization,模型泛化能力,0.0,9,"[{'word': '模型泛化', 'ratio': 1.0}]",模型泛化,{},[]
2913,2913,model hyperparameter,模型超参数,1.0,9,"[{'word': '模型超参数', 'ratio': 1.0}]",模型超参数,{},[]
2914,2914,model inference,模型推断,0.3333333333333333,9,"[{'word': '模型推理', 'ratio': 0.6666666666666666}, {'word': '模型推断', 'ratio': 0.3333333333333333}]",模型推理,{},[]
2915,2915,model initialization,模型初始化,1.0,9,"[{'word': '模型初始化', 'ratio': 1.0}]",模型初始化,{},[]
2916,2916,model interpretability,模型可解释性,1.0,9,"[{'word': '模型可解释性', 'ratio': 1.0}]",模型可解释性,{},[]
2917,2917,model interpretation,模型解释,1.0,9,"[{'word': '模型解释', 'ratio': 1.0}]",模型解释,{},[]
2918,2918,model layer,模型层,1.0,9,"[{'word': '模型层', 'ratio': 1.0}]",模型层,{},[]
2919,2919,model output,模型输出,1.0,9,"[{'word': '模型输出', 'ratio': 1.0}]",模型输出,{},[]
2920,2920,model parallelism,模型并行性,0.7,10,"[{'word': '模型并行性', 'ratio': 0.7}, {'word': '模型并行', 'ratio': 0.3}]",模型并行性,{},[]
2921,2921,model parameter,模型参数,1.0,10,"[{'word': '模型参数', 'ratio': 1.0}]",模型参数,{},[]
2922,2922,model performance,模型表现,0.0,10,"[{'word': '模型性能', 'ratio': 1.0}]",模型性能,{},[]
2923,2923,model precision,模型精度,0.7,10,"[{'word': '模型精度', 'ratio': 0.7}, {'word': '模型精确度', 'ratio': 0.3}]",模型精度,{},[]
2924,2924,model prediction,模型预测,0.9,10,"[{'word': '模型预测', 'ratio': 0.9}, {'word': '模型精度', 'ratio': 0.1}]",模型预测,{},[]
2925,2925,model predictive control,模型预测控制,1.0,8,"[{'word': '模型预测控制', 'ratio': 1.0}]",模型预测控制,{},[]
2926,2926,model representation,模型表示,1.0,8,"[{'word': '模型表示', 'ratio': 1.0}]",模型表示,{},[]
2927,2927,model robustness,模型鲁棒性,1.0,8,"[{'word': '模型鲁棒性', 'ratio': 1.0}]",模型鲁棒性,{},[]
2928,2928,model score,模型分数,0.125,8,"[{'word': '模型得分', 'ratio': 0.625}, {'word': '模型评分', 'ratio': 0.25}, {'word': '模型分数', 'ratio': 0.125}]",模型得分,{},[]
2929,2929,model selection,模型选择,1.0,8,"[{'word': '模型选择', 'ratio': 1.0}]",模型选择,{},[]
2930,2930,model size,模型大小,0.4,10,"[{'word': '模型规模', 'ratio': 0.6}, {'word': '模型大小', 'ratio': 0.4}]",模型规模,{},[]
2931,2931,model specification,模型规范化,0.0,10,"[{'word': '模型规范', 'ratio': 0.7}, {'word': '模型规格', 'ratio': 0.3}]",模型规范,{},[]
2932,2932,model structure,模型结构,1.0,10,"[{'word': '模型结构', 'ratio': 1.0}]",模型结构,{},[]
2933,2933,model training,模型训练,1.0,10,"[{'word': '模型训练', 'ratio': 1.0}]",模型训练,{},[]
2934,2934,model update,模型更新,1.0,10,"[{'word': '模型更新', 'ratio': 1.0}]",模型更新,{},[]
2935,2935,model variant,模型变体,1.0,9,"[{'word': '模型变体', 'ratio': 1.0}]",模型变体,{},[]
2936,2936,model weight,模型权重,1.0,9,"[{'word': '模型权重', 'ratio': 1.0}]",模型权重,{},[]
2937,2937,model's parameter,模型参数,1.0,9,"[{'word': '模型参数', 'ratio': 1.0}]",模型参数,{},[]
2938,2938,model-based approach,基于模型的方法,1.0,9,"[{'word': '基于模型的方法', 'ratio': 1.0}]",基于模型的方法,{},[]
2939,2939,model-based reinforcement learning,基于模型的强化学习,1.0,9,"[{'word': '基于模型的强化学习', 'ratio': 1.0}]",基于模型的强化学习,{},[]
2940,2940,model-free approach,无模型方法,1.0,10,"[{'word': '无模型方法', 'ratio': 1.0}]",无模型方法,{},[]
2941,2941,modular,模块化的,0.5,10,"[{'word': '模块化的', 'ratio': 0.5}, {'word': '模块化', 'ratio': 0.5}]",模块化的,{},[]
2942,2942,modular architecture,模块化架构,1.0,10,"[{'word': '模块化架构', 'ratio': 1.0}]",模块化架构,{},[]
2943,2943,module,模块,1.0,10,"[{'word': '模块', 'ratio': 1.0}]",模块,{},[]
2944,2944,moment matching,矩匹配,0.4444444444444444,9,"[{'word': '矩匹配', 'ratio': 0.4444444444444444}, {'word': '瞬时匹配', 'ratio': 0.3333333333333333}, {'word': '时刻匹配', 'ratio': 0.1111111111111111}, {'word': '矩量匹配', 'ratio': 0.1111111111111111}]",矩匹配,"1. Rank: 矩匹配, 矩量匹配, 时刻匹配, 瞬时匹配

2. Explanation: The term ""矩匹配"" (moment matching) is the best fit because it accurately reflects the mathematical and statistical context in which ""moment"" is used, particularly in AI and machine learning. In this context, ""moment"" refers to statistical moments (like mean, variance, etc.), which are crucial for understanding distributions and approximations. The term ""矩"" (matrix) is often associated with mathematical concepts, making it semantically appropriate for the AI domain. 

""矩量匹配"" (moment matching) is also a strong candidate, as it retains the concept of ""moment"" but adds ""量"" (quantity), which may not be necessary and could introduce ambiguity. 

""时刻匹配"" (moment matching) and ""瞬时匹配"" (instantaneous matching) are less suitable because they imply a temporal aspect (""时刻"" means ""time point"" and ""瞬时"" means ""instantaneous""), which does not align with the statistical meaning of ""moment"" in this context. Thus, they do not capture the intended meaning as effectively as ""矩匹配.""","['moment matching', 'instantaneous matching', 'moment matching', 'moment matching']"
2945,2945,momentum,动量,1.0,9,"[{'word': '动量', 'ratio': 1.0}]",动量,{},[]
2946,2946,momentum coefficient,动量系数,1.0,9,"[{'word': '动量系数', 'ratio': 1.0}]",动量系数,{},[]
2947,2947,momentum encoder,动量编码器,1.0,9,"[{'word': '动量编码器', 'ratio': 1.0}]",动量编码器,{},[]
2948,2948,momentum term,动量项,1.0,9,"[{'word': '动量项', 'ratio': 1.0}]",动量项,{},[]
2949,2949,monocular,单目,0.9,10,"[{'word': '单目', 'ratio': 0.9}, {'word': '单目的', 'ratio': 0.1}]",单目,{},[]
2950,2950,monocular reconstruction,单目重建,1.0,10,"[{'word': '单目重建', 'ratio': 1.0}]",单目重建,{},[]
2951,2951,monolingual baseline,单语基线,0.6,10,"[{'word': '单语基线', 'ratio': 0.6}, {'word': '单语基准', 'ratio': 0.4}]",单语基线,{},[]
2952,2952,monolingual corpora,单语语料库,1.0,10,"[{'word': '单语语料库', 'ratio': 1.0}]",单语语料库,{},[]
2953,2953,monolingual corpus,单语语料库,1.0,10,"[{'word': '单语语料库', 'ratio': 1.0}]",单语语料库,{},[]
2954,2954,monolingual dataset,单语数据集,1.0,10,"[{'word': '单语数据集', 'ratio': 1.0}]",单语数据集,{},[]
2955,2955,monolingual datum,单语语料,0.0,10,"[{'word': '单语数据', 'ratio': 0.9}, {'word': '单语资料', 'ratio': 0.1}]",单语数据,{},[]
2956,2956,monolingual embedding,单语嵌入,0.9,10,"[{'word': '单语嵌入', 'ratio': 0.9}, {'word': '单语言嵌入', 'ratio': 0.1}]",单语嵌入,{},[]
2957,2957,monolingual model,单语模型,1.0,10,"[{'word': '单语模型', 'ratio': 1.0}]",单语模型,{},[]
2958,2958,monolingual training,单语训练,0.8888888888888888,9,"[{'word': '单语训练', 'ratio': 0.8888888888888888}, {'word': '单语培训', 'ratio': 0.1111111111111111}]",单语训练,{},[]
2959,2959,monotone,单调增加,0.0,9,"[{'word': '单调', 'ratio': 0.6666666666666666}, {'word': '单调的', 'ratio': 0.3333333333333333}]",单调,{},[]
2960,2960,monotonic,单调,0.0,9,"[{'word': '单调的', 'ratio': 0.7777777777777778}, {'word': '单调性', 'ratio': 0.1111111111111111}, {'word': '单调递增', 'ratio': 0.1111111111111111}]",单调的,{},[]
2961,2961,morphological analysis,形态学分析,0.2,10,"[{'word': '形态分析', 'ratio': 0.8}, {'word': '形态学分析', 'ratio': 0.2}]",形态分析,{},[]
2962,2962,morphological analyzer,形态分析器,0.8,10,"[{'word': '形态分析器', 'ratio': 0.8}, {'word': '形态学分析器', 'ratio': 0.2}]",形态分析器,{},[]
2963,2963,morphological feature,形态学特征,0.2,10,"[{'word': '形态特征', 'ratio': 0.8}, {'word': '形态学特征', 'ratio': 0.2}]",形态特征,{},[]
2964,2964,morphological information,形态信息,0.8,10,"[{'word': '形态信息', 'ratio': 0.8}, {'word': '形态学信息', 'ratio': 0.2}]",形态信息,{},[]
2965,2965,morphological operation,形态学运算,0.0,10,"[{'word': '形态操作', 'ratio': 0.6}, {'word': '形态学操作', 'ratio': 0.4}]",形态操作,{},[]
2966,2966,morphological segmentation,形态分割,0.0,9,"[{'word': '形态学分割', 'ratio': 0.8888888888888888}, {'word': '形态学分词', 'ratio': 0.1111111111111111}]",形态学分割,{},[]
2967,2967,morphology,形态学,1.0,9,"[{'word': '形态学', 'ratio': 1.0}]",形态学,{},[]
2968,2968,motion analysis,运动分析,0.8888888888888888,9,"[{'word': '运动分析', 'ratio': 0.8888888888888888}, {'word': '动作分析', 'ratio': 0.1111111111111111}]",运动分析,{},[]
2969,2969,motion estimation,运动估计,0.8888888888888888,9,"[{'word': '运动估计', 'ratio': 0.8888888888888888}, {'word': '动作估计', 'ratio': 0.1111111111111111}]",运动估计,{},[]
2970,2970,motion matrix,运动矩阵,1.0,9,"[{'word': '运动矩阵', 'ratio': 1.0}]",运动矩阵,{},[]
2971,2971,motion planning,运动规划,1.0,9,"[{'word': '运动规划', 'ratio': 1.0}]",运动规划,{},[]
2972,2972,motion segmentation,运动分割,1.0,9,"[{'word': '运动分割', 'ratio': 1.0}]",运动分割,{},[]
2973,2973,moving average,移动平均,1.0,9,"[{'word': '移动平均', 'ratio': 1.0}]",移动平均,{},[]
2974,2974,multi-agent,多智能体,1.0,9,"[{'word': '多智能体', 'ratio': 1.0}]",多智能体,{},[]
2975,2975,multi-agent interaction,多智能体交互,0.8888888888888888,9,"[{'word': '多智能体交互', 'ratio': 0.8888888888888888}, {'word': '多智能体互动', 'ratio': 0.1111111111111111}]",多智能体交互,{},[]
2976,2976,multi-agent learning,多智能体学习,1.0,9,"[{'word': '多智能体学习', 'ratio': 1.0}]",多智能体学习,{},[]
2977,2977,multi-agent reinforcement learning,多智能体强化学习,1.0,9,"[{'word': '多智能体强化学习', 'ratio': 1.0}]",多智能体强化学习,{},[]
2978,2978,multi-agent system,多智能体系统,0.8888888888888888,9,"[{'word': '多智能体系统', 'ratio': 0.8888888888888888}, {'word': '多智能体系', 'ratio': 0.1111111111111111}]",多智能体系统,{},[]
2979,2979,multi-armed bandit,多臂赌博机,0.6,10,"[{'word': '多臂赌博机', 'ratio': 0.6}, {'word': '多臂老虎机', 'ratio': 0.3}, {'word': '多臂老虎机问题', 'ratio': 0.1}]",多臂赌博机,{},[]
2980,2980,multi-armed bandit problem,多臂赌博机问题,0.6,10,"[{'word': '多臂赌博机问题', 'ratio': 0.6}, {'word': '多臂老虎机问题', 'ratio': 0.4}]",多臂赌博机问题,{},[]
2981,2981,multi-class,多类别,0.6,10,"[{'word': '多类别', 'ratio': 0.6}, {'word': '多类', 'ratio': 0.3}, {'word': '多类别的', 'ratio': 0.1}]",多类别,{},[]
2982,2982,multi-class classification,多分类,0.0,10,"[{'word': '多类别分类', 'ratio': 0.7}, {'word': '多类分类', 'ratio': 0.3}]",多类别分类,{},[]
2983,2983,multi-class logistic regression,多类逻辑回归,0.3,10,"[{'word': '多类别逻辑回归', 'ratio': 0.7}, {'word': '多类逻辑回归', 'ratio': 0.3}]",多类别逻辑回归,{},[]
2984,2984,multi-class problem,多类别问题,0.4444444444444444,9,"[{'word': '多类问题', 'ratio': 0.5555555555555556}, {'word': '多类别问题', 'ratio': 0.4444444444444444}]",多类问题,{},[]
2985,2985,multi-classification,多类分类,0.0,9,"[{'word': '多分类', 'ratio': 0.8888888888888888}, {'word': '多类别分类', 'ratio': 0.1111111111111111}]",多分类,{},[]
2986,2986,multi-document summarization,多文档摘要,1.0,9,"[{'word': '多文档摘要', 'ratio': 1.0}]",多文档摘要,{},[]
2987,2987,multi-domain,多领域,1.0,9,"[{'word': '多领域', 'ratio': 1.0}]",多领域,{},[]
2988,2988,multi-head,多头,1.0,9,"[{'word': '多头', 'ratio': 1.0}]",多头,{},[]
2989,2989,multi-head attention,多头注意力,1.0,9,"[{'word': '多头注意力', 'ratio': 1.0}]",多头注意力,{},[]
2990,2990,multi-head attention layer,多头注意力层,1.0,9,"[{'word': '多头注意力层', 'ratio': 1.0}]",多头注意力层,{},[]
2991,2991,multi-head self-attention,多头自注意力机制,0.0,9,"[{'word': '多头自注意力', 'ratio': 1.0}]",多头自注意力,{},[]
2992,2992,multi-head self-attention mechanism,多头自注意力机制,1.0,9,"[{'word': '多头自注意力机制', 'ratio': 1.0}]",多头自注意力机制,{},[]
2993,2993,multi-head self-attention module,多头自注意力模块,1.0,9,"[{'word': '多头自注意力模块', 'ratio': 1.0}]",多头自注意力模块,{},[]
2994,2994,multi-headed self-attention,多头自注意力,0.8,10,"[{'word': '多头自注意力', 'ratio': 0.8}, {'word': '多头自注意力机制', 'ratio': 0.2}]",多头自注意力,{},[]
2995,2995,multi-label,多标签,1.0,10,"[{'word': '多标签', 'ratio': 1.0}]",多标签,{},[]
2996,2996,multi-label classification,多标签分类,1.0,10,"[{'word': '多标签分类', 'ratio': 1.0}]",多标签分类,{},[]
2997,2997,multi-label classification loss,多标签分类损失,1.0,10,"[{'word': '多标签分类损失', 'ratio': 1.0}]",多标签分类损失,{},[]
2998,2998,multi-label classifier,多标签分类器,1.0,10,"[{'word': '多标签分类器', 'ratio': 1.0}]",多标签分类器,{},[]
2999,2999,multi-label datum,多标签数据,1.0,10,"[{'word': '多标签数据', 'ratio': 1.0}]",多标签数据,{},[]
3000,3000,multi-label learning,多标签学习,1.0,10,"[{'word': '多标签学习', 'ratio': 1.0}]",多标签学习,{},[]
3001,3001,multi-label text classification,多标签文本分类,1.0,10,"[{'word': '多标签文本分类', 'ratio': 1.0}]",多标签文本分类,{},[]
3002,3002,multi-layer neural network,多层神经网络,1.0,10,"[{'word': '多层神经网络', 'ratio': 1.0}]",多层神经网络,{},[]
3003,3003,multi-layer perceptron,多层感知器,0.9,10,"[{'word': '多层感知器', 'ratio': 0.9}, {'word': '多层感知机', 'ratio': 0.1}]",多层感知器,{},[]
3004,3004,multi-modal,多模态,0.875,8,"[{'word': '多模态', 'ratio': 0.875}, {'word': '多模态的', 'ratio': 0.125}]",多模态,{},[]
3005,3005,multi-modal input,多模态输入,0.875,8,"[{'word': '多模态输入', 'ratio': 0.875}, {'word': '多模态', 'ratio': 0.125}]",多模态输入,{},[]
3006,3006,multi-modal learning,多模态学习,1.0,8,"[{'word': '多模态学习', 'ratio': 1.0}]",多模态学习,{},[]
3007,3007,multi-modal model,多模态模型,1.0,8,"[{'word': '多模态模型', 'ratio': 1.0}]",多模态模型,{},[]
3008,3008,multi-object detection,多目标检测,0.875,8,"[{'word': '多目标检测', 'ratio': 0.875}, {'word': '多对象检测 如果您需要进一步的帮助，请告诉我！', 'ratio': 0.125}]",多目标检测,{},[]
3009,3009,multi-objective optimization,多目标优化,1.0,10,"[{'word': '多目标优化', 'ratio': 1.0}]",多目标优化,{},[]
3010,3010,multi-scale,多尺度,1.0,10,"[{'word': '多尺度', 'ratio': 1.0}]",多尺度,{},[]
3011,3011,multi-scale architecture,多尺度架构,1.0,10,"[{'word': '多尺度架构', 'ratio': 1.0}]",多尺度架构,{},[]
3012,3012,multi-scale training,多尺度训练,1.0,10,"[{'word': '多尺度训练', 'ratio': 1.0}]",多尺度训练,{},[]
3013,3013,multi-task,多任务,1.0,10,"[{'word': '多任务', 'ratio': 1.0}]",多任务,{},[]
3014,3014,multi-task fine-tuning,多任务微调,1.0,9,"[{'word': '多任务微调', 'ratio': 1.0}]",多任务微调,{},[]
3015,3015,multi-task model,多任务模型,1.0,9,"[{'word': '多任务模型', 'ratio': 1.0}]",多任务模型,{},[]
3016,3016,multi-task regression,多任务回归,1.0,9,"[{'word': '多任务回归', 'ratio': 1.0}]",多任务回归,{},[]
3017,3017,multi-task setting,多任务设置,0.7777777777777778,9,"[{'word': '多任务设置', 'ratio': 0.7777777777777778}, {'word': '多任务环境', 'ratio': 0.2222222222222222}]",多任务设置,{},[]
3018,3018,multi-view,多视图,0.4444444444444444,9,"[{'word': '多视角', 'ratio': 0.5555555555555556}, {'word': '多视图', 'ratio': 0.4444444444444444}]",多视角,{},[]
3019,3019,multi-view datum,多视图数据,0.6666666666666666,9,"[{'word': '多视图数据', 'ratio': 0.6666666666666666}, {'word': '多视角数据', 'ratio': 0.3333333333333333}]",多视图数据,{},[]
3020,3020,multi-view geometry,多视图几何,0.6666666666666666,9,"[{'word': '多视图几何', 'ratio': 0.6666666666666666}, {'word': '多视角几何', 'ratio': 0.3333333333333333}]",多视图几何,{},[]
3021,3021,multi-view learning,多视图学习,0.6666666666666666,9,"[{'word': '多视图学习', 'ratio': 0.6666666666666666}, {'word': '多视角学习', 'ratio': 0.3333333333333333}]",多视图学习,{},[]
3022,3022,multi-view stereo,多视图立体,0.1111111111111111,9,"[{'word': '多视图立体视觉', 'ratio': 0.4444444444444444}, {'word': '多视角立体', 'ratio': 0.2222222222222222}, {'word': '多视图立体', 'ratio': 0.1111111111111111}, {'word': '多视图立体匹配', 'ratio': 0.1111111111111111}, {'word': '多视角立体视觉', 'ratio': 0.1111111111111111}]",多视图立体,"1. Rank: 多视图立体, 多视图立体视觉, 多视角立体, 多视角立体视觉, 多视图立体匹配

2. Explanation: The term ""多视图立体"" (multi-view stereo) is the best fit for the AI domain-specific usage because it accurately captures the essence of the original English term without unnecessary embellishments. In the context of computer vision and 3D reconstruction, ""多视图立体"" directly refers to the technique of reconstructing 3D scenes from multiple 2D images taken from different viewpoints, which aligns perfectly with the technical focus of the provided context. 

The other candidates, such as ""多视图立体视觉"" and ""多视角立体视觉,"" introduce the word ""视觉"" (vision), which, while related, adds a layer of abstraction that is not present in the original term. This could lead to confusion in a technical context where precision is crucial. ""多视角立体"" is also a valid term but is less commonly used in the literature compared to ""多视图立体."" Lastly, ""多视图立体匹配"" (multi-view stereo matching) is too specific, as it refers to a particular aspect of the broader multi-view stereo process, which is not what the original term encompasses. Thus, ""多视图立体"" stands out as the most semantically accurate and contextually appropriate choice.","['multi-view stereo vision', 'multi-view stereo', 'multi-view stereo', 'Multi-view stereo matching', 'Multi-view stereo vision']"
3023,3023,multi-view system,多视图系统,0.6666666666666666,9,"[{'word': '多视图系统', 'ratio': 0.6666666666666666}, {'word': '多视角系统', 'ratio': 0.3333333333333333}]",多视图系统,{},[]
3024,3024,multiclass classifier,多类分类器,0.5555555555555556,9,"[{'word': '多类分类器', 'ratio': 0.5555555555555556}, {'word': '多分类分类器', 'ratio': 0.2222222222222222}, {'word': '多类别分类器', 'ratio': 0.2222222222222222}]",多类分类器,{},[]
3025,3025,multiclass hinge loss,多类别铰链损失,0.2222222222222222,9,"[{'word': '多类合页损失', 'ratio': 0.3333333333333333}, {'word': '多类铰链损失', 'ratio': 0.2222222222222222}, {'word': '多分类铰链损失', 'ratio': 0.2222222222222222}, {'word': '多类别铰链损失', 'ratio': 0.2222222222222222}]",多分类铰链损失,"1. Rank: 多分类铰链损失, 多类别铰链损失, 多类合页损失, 多类铰链损失

2. Explanation: The term ""多分类铰链损失"" (Multi-class hinge loss) is the best fit because it accurately captures the specific meaning of ""multiclass"" in the context of machine learning, where ""分类"" (classification) is commonly used to refer to problems involving multiple classes. This term aligns well with the established terminology in the AI domain, ensuring clarity and precision. 

""多类别铰链损失"" (Multi-class hinge loss) is also a strong candidate, as ""类别"" (categories) is a valid synonym for ""classes,"" but ""分类"" is more widely recognized in the context of classification tasks. 

The terms ""多类合页损失"" (Multiple types of hinge losses) and ""多类铰链损失"" (Multiple types of hinge losses) are less accurate because they do not convey the specific concept of ""multiclass"" as it relates to classification tasks in machine learning. Instead, they suggest a broader interpretation that could imply various types of hinge losses rather than a singular focus on multiclass classification. Thus, they rank lower in terms of semantic accuracy and contextual fit.","['Multiple types of hinge losses', 'Multiple types of hinge losses', 'Multi-class hinge loss', 'Multi-class hinge loss']"
3026,3026,multiclass model,多类模型,0.5555555555555556,9,"[{'word': '多类模型', 'ratio': 0.5555555555555556}, {'word': '多分类模型', 'ratio': 0.2222222222222222}, {'word': '多类别模型', 'ratio': 0.2222222222222222}]",多类模型,{},[]
3027,3027,multiclass object detection,多类目标检测,0.5555555555555556,9,"[{'word': '多类目标检测', 'ratio': 0.5555555555555556}, {'word': '多类别目标检测', 'ratio': 0.3333333333333333}, {'word': '多分类目标检测', 'ratio': 0.1111111111111111}]",多类目标检测,{},[]
3028,3028,multilingual embedding,多语种嵌入,0.125,8,"[{'word': '多语言嵌入', 'ratio': 0.875}, {'word': '多语种嵌入', 'ratio': 0.125}]",多语言嵌入,{},[]
3029,3029,multilingual language model,多语种语言模型,0.0,8,"[{'word': '多语言模型', 'ratio': 0.625}, {'word': '多语言语言模型', 'ratio': 0.375}]",多语言模型,{},[]
3030,3030,multilingual model,多语言模型,1.0,8,"[{'word': '多语言模型', 'ratio': 1.0}]",多语言模型,{},[]
3031,3031,multilingual representation,多语言表征,0.0,8,"[{'word': '多语言表示', 'ratio': 1.0}]",多语言表示,{},[]
3032,3032,multilingual training,多语言训练,0.875,8,"[{'word': '多语言训练', 'ratio': 0.875}, {'word': '多语言训练 如果有其他问题或需要更多帮助，请告诉', 'ratio': 0.125}]",多语言训练,{},[]
3033,3033,multilinguality,多语性,0.0,10,"[{'word': '多语言性', 'ratio': 0.9}, {'word': '多语言能力', 'ratio': 0.1}]",多语言性,{},[]
3034,3034,multimodal task,多模态任务,1.0,10,"[{'word': '多模态任务', 'ratio': 1.0}]",多模态任务,{},[]
3035,3035,multinomial distribution,多项分布,1.0,10,"[{'word': '多项分布', 'ratio': 1.0}]",多项分布,{},[]
3036,3036,multinomial model,多项分布模型,0.0,10,"[{'word': '多项式模型', 'ratio': 0.8}, {'word': '多项模型', 'ratio': 0.2}]",多项式模型,{},[]
3037,3037,multiple kernel learning,多核学习,1.0,10,"[{'word': '多核学习', 'ratio': 1.0}]",多核学习,{},[]
3038,3038,multiple linear regression,多元线性回归,0.9,10,"[{'word': '多元线性回归', 'ratio': 0.9}, {'word': '多重线性回归', 'ratio': 0.1}]",多元线性回归,{},[]
3039,3039,multiscale modeling,多尺度建模,1.0,10,"[{'word': '多尺度建模', 'ratio': 1.0}]",多尺度建模,{},[]
3040,3040,multiset,多重集,0.7,10,"[{'word': '多重集', 'ratio': 0.7}, {'word': '多重集合', 'ratio': 0.2}, {'word': '多任务', 'ratio': 0.1}]",多重集,{},[]
3041,3041,multitask training,多任务训练,1.0,10,"[{'word': '多任务训练', 'ratio': 1.0}]",多任务训练,{},[]
3042,3042,multivariate,多元变量,0.0,9,"[{'word': '多变量', 'ratio': 0.5555555555555556}, {'word': '多元的', 'ratio': 0.2222222222222222}, {'word': '多元', 'ratio': 0.1111111111111111}, {'word': '多变量的', 'ratio': 0.1111111111111111}]",多变量,{},[]
3043,3043,multivariate Gaussian,多元高斯,0.3333333333333333,9,"[{'word': '多变量高斯分布', 'ratio': 0.4444444444444444}, {'word': '多元高斯', 'ratio': 0.3333333333333333}, {'word': '多变量高斯', 'ratio': 0.2222222222222222}]",多变量高斯分布,"1. Rank: 多变量高斯分布, 多元高斯, 多变量高斯

2. Explanation: The term ""多变量高斯分布"" (Multivariate Gaussian distribution) is the best fit for the context provided. This translation is semantically accurate as it explicitly includes ""分布"" (distribution), which is essential in statistical contexts to denote that we are discussing a probability distribution rather than just a mathematical function. The back translation accurately reflects the original English term, maintaining the full meaning and context.

The second candidate, ""多元高斯"" (Multivariate Gaussian), is also a strong contender, as it is commonly used in the AI and statistical communities. However, it lacks the explicit mention of ""分布,"" which could lead to ambiguity in certain contexts where the distinction between a distribution and other mathematical constructs is crucial.

The third candidate, ""多变量高斯"" (Multivariable Gaussian), is less accurate because ""多变量"" (multivariable) is not the standard term used in statistics and AI. The term ""多元"" (multivariate) is preferred in this domain, as it specifically refers to multiple random variables and their joint distribution, which is the focus in the context of Gaussian distributions.

Overall, ""多变量高斯分布"" is the most precise and contextually appropriate choice for the AI domain.","['Multivariate Gaussian distribution', 'Multivariate Gaussian', 'Multivariable Gaussian']"
3044,3044,multivariate Gaussian distribution,多元高斯分布,0.3333333333333333,9,"[{'word': '多变量高斯分布', 'ratio': 0.6666666666666666}, {'word': '多元高斯分布', 'ratio': 0.3333333333333333}]",多变量高斯分布,{},[]
3045,3045,multivariate normal,多元正态分布,0.0,9,"[{'word': '多变量正态分布', 'ratio': 0.3333333333333333}, {'word': '多变量正态', 'ratio': 0.3333333333333333}, {'word': '多元正态', 'ratio': 0.3333333333333333}]",多变量正态分布,"1. Rank: 多变量正态分布, 多元正态, 多变量正态

2. Explanation: The term ""多变量正态分布"" (multivariate normal distribution) is the best fit because it is the most semantically accurate and contextually appropriate translation in the AI domain. It explicitly includes ""分布"" (distribution), which is crucial in statistical contexts, as it clarifies that we are discussing a probability distribution rather than just a general concept of ""normal"" or ""multivariate."" 

The second candidate, ""多元正态"" (multivariate normal), is also a valid term but lacks the explicit mention of ""分布,"" which may lead to ambiguity in certain contexts. The third candidate, ""多变量正态"" (multivariate normal), is less commonly used and may not be as immediately recognizable to those familiar with statistical terminology. 

In the context of AI and statistics, precision in terminology is essential, and ""多变量正态分布"" provides the clearest and most accurate representation of the concept being discussed.","['multivariate normal distribution', 'multivariate normal', 'multivariate normal']"
3046,3046,multivariate normal distribution,多元正态分布,0.3333333333333333,9,"[{'word': '多变量正态分布', 'ratio': 0.6666666666666666}, {'word': '多元正态分布', 'ratio': 0.3333333333333333}]",多变量正态分布,{},[]
3047,3047,multivariate time series,多元时间序列,0.2,10,"[{'word': '多变量时间序列', 'ratio': 0.8}, {'word': '多元时间序列', 'ratio': 0.2}]",多变量时间序列,{},[]
3048,3048,mutex,互斥集,0.0,10,"[{'word': '互斥锁', 'ratio': 0.9}, {'word': '互斥量', 'ratio': 0.1}]",互斥锁,{},[]
3049,3049,mutexe,"""互斥量""",0.0,10,"[{'word': '互斥锁', 'ratio': 0.4}, {'word': '互斥执行', 'ratio': 0.2}, {'word': '互斥', 'ratio': 0.2}, {'word': '互斥体', 'ratio': 0.1}, {'word': '互斥量', 'ratio': 0.1}]",互斥锁,"1. Rank: 互斥锁, 互斥执行, 互斥, 互斥体, 互斥量

2. Explanation: The term ""互斥锁"" (mutex lock) is the best fit for the context of AI and computer science, particularly in relation to concurrent programming and resource management. In this context, ""mutex"" refers specifically to a synchronization primitive that is used to manage access to shared resources in a multi-threaded environment. The back translation ""mutex lock"" accurately reflects this meaning, as it is a widely recognized term in both English and Chinese programming communities.

The second candidate, ""互斥执行"" (mutually exclusive execution), while relevant, is more of a descriptive phrase rather than a standard term. It conveys the idea of mutual exclusion but does not specifically refer to the locking mechanism that ""互斥锁"" does. The other candidates, such as ""互斥"" (mutually exclusive), ""互斥体"" (mutex), and ""互斥量"" (mutex), are either too vague or not commonly used in the context of mutexes in programming. Therefore, ""互斥锁"" stands out as the most semantically accurate and contextually appropriate choice for the AI domain.","['mutex lock', 'Mutually exclusive execution', 'mutually exclusive', 'mutex', 'mutex']"
3050,3050,mutual entropy,互信息,0.1,10,"[{'word': '互信息熵', 'ratio': 0.6}, {'word': '互熵', 'ratio': 0.3}, {'word': '互信息', 'ratio': 0.1}]",互信息熵,{},[]
3051,3051,n-best list,n-best列表,0.0,10,"[{'word': 'n-best 列表', 'ratio': 0.2}, {'word': '前n个最佳列表', 'ratio': 0.2}, {'word': 'n-最优列表', 'ratio': 0.2}, {'word': 'n-最佳列表', 'ratio': 0.1}, {'word': 'n最佳列表', 'ratio': 0.1}, {'word': 'N最佳列表', 'ratio': 0.1}, {'word': 'N-最佳列表', 'ratio': 0.1}]",n-best 列表,"1. Rank: n-best 列表, n-最佳列表, n最佳列表, 前n个最佳列表, n-最优列表, N-最佳列表, N最佳列表

2. Explanation: The term ""n-best 列表"" is the best fit because it retains the original English term ""n-best"" while clearly indicating that it refers to a list, which is crucial in the context of AI and machine translation. This term is widely recognized in the AI community, particularly in natural language processing and speech recognition, where ""n-best"" refers to the top n hypotheses or outputs generated by a model. 

The other candidates, such as ""n-最佳列表"" and ""n最佳列表,"" are also acceptable but slightly less precise because they do not maintain the ""n-best"" structure as clearly as ""n-best 列表."" The term ""前n个最佳列表"" introduces ambiguity by suggesting a ranking rather than a list of candidates, which may not align with the technical usage in AI. ""n-最优列表"" and ""N-最佳列表"" also deviate from the standard terminology used in the field. 

Overall, ""n-best 列表"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['n-best list', 'top n best list', 'n-optimal list', 'n-best list', 'nbest list', 'N best list', 'N-best list']"
3052,3052,n-gram feature,n-gram特征,0.3,10,"[{'word': 'n元特征', 'ratio': 0.4}, {'word': 'n-gram特征', 'ratio': 0.3}, {'word': '特征', 'ratio': 0.2}, {'word': 'n-元特征', 'ratio': 0.1}]",n-gram特征,"1. Rank: n-gram特征, n元特征, n-元特征, 特征

2. Explanation: The term ""n-gram特征"" is the best fit because it directly translates the English term ""n-gram feature"" while retaining the specific terminology used in the AI and computational linguistics domains. The term ""n-gram"" is a well-established concept in these fields, referring to a contiguous sequence of n items from a given sample of text or speech. By using ""n-gram特征,"" the translation maintains both semantic accuracy and contextual relevance, ensuring that readers familiar with AI terminology will immediately understand the concept being discussed. 

In contrast, ""n元特征"" and ""n-元特征"" introduce ambiguity by using ""元"" (which can mean ""ary"" in a mathematical context) but do not clearly convey the specific meaning of ""n-gram."" The term ""特征"" alone is too vague and lacks the specificity needed for accurate understanding in this context. Therefore, ""n-gram特征"" is the most precise and contextually appropriate choice.","['n-ary features', 'n-gram features', 'feature', 'n-ary features']"
3053,3053,n-gram language model,n-元语言模型,0.1,10,"[{'word': 'n元语言模型', 'ratio': 0.4}, {'word': 'n-gram语言模型', 'ratio': 0.3}, {'word': '语言模型', 'ratio': 0.2}, {'word': 'n-元语言模型', 'ratio': 0.1}]",n-gram语言模型,"1. Rank: n-gram语言模型, n元语言模型, n-元语言模型, 语言模型

2. Explanation: The term ""n-gram语言模型"" is the best fit because it retains the original ""n-gram"" terminology, which is crucial in the AI domain for clarity and specificity. This term directly translates to ""n-gram language model,"" maintaining both semantic accuracy and contextual relevance. The use of ""n-gram"" in its original form is essential in technical contexts, as it is a widely recognized term in natural language processing and machine learning. 

The second candidate, ""n元语言模型,"" is also a good option, as ""元"" (which means ""gram"" in this context) is a common way to refer to n-grams in Chinese. However, it is less commonly used than ""n-gram"" in the AI community, which may lead to slight confusion among practitioners who are accustomed to the English term.

The third candidate, ""n-元语言模型,"" introduces an unnecessary complexity by adding ""元"" before ""n,"" which could confuse readers about its meaning. Lastly, ""语言模型"" (language model) is too vague and does not specify the n-gram aspect, making it unsuitable for the specific context of the term being discussed. 

Overall, ""n-gram语言模型"" is the most precise and contextually appropriate choice for conveying the concept of an n-gram language model in the AI domain.","['n-gram language model', 'n-gram language model', 'language model', 'n-gram language model']"
3054,3054,n-gram model,n-gram模型,0.3,10,"[{'word': 'n元模型', 'ratio': 0.4}, {'word': 'n-gram模型', 'ratio': 0.3}, {'word': '模型', 'ratio': 0.2}, {'word': 'n-元模型', 'ratio': 0.1}]",n-gram模型,"1. Rank: n-gram模型, n元模型, n-元模型, 模型

2. Explanation: The term ""n-gram模型"" is the best fit because it directly translates the English term ""n-gram model"" while retaining the specific terminology used in the AI and computational linguistics domains. The term ""n-gram"" is a well-established concept in these fields, referring to a contiguous sequence of n items from a given sample of text or speech. By using ""n-gram模型,"" the translation maintains both semantic accuracy and contextual relevance, ensuring that it is immediately recognizable to professionals in the field. 

In contrast, ""n元模型"" and ""n-元模型"" are less precise. While they attempt to convey the idea of ""n"" items, they do not capture the established term ""n-gram,"" which is critical for clarity in AI discussions. The term ""模型"" alone is too vague and does not convey the specific concept of an n-gram model. Therefore, ""n-gram模型"" is the most contextually appropriate and semantically accurate choice.","['n-ary model', 'n-gram model', 'Model', 'n-ary model']"
3055,3055,n-step returns,n步回报,1.0,10,"[{'word': 'n步回报', 'ratio': 1.0}]",n步回报,{},[]
3056,3056,naive Bayes model,朴素贝叶斯模型,1.0,10,"[{'word': '朴素贝叶斯模型', 'ratio': 1.0}]",朴素贝叶斯模型,{},[]
3057,3057,named entity,命名实体,1.0,9,"[{'word': '命名实体', 'ratio': 1.0}]",命名实体,{},[]
3058,3058,named entity recognizer,命名实体识别器,1.0,9,"[{'word': '命名实体识别器', 'ratio': 1.0}]",命名实体识别器,{},[]
3059,3059,natural image statistic,自然图像统计,1.0,9,"[{'word': '自然图像统计', 'ratio': 1.0}]",自然图像统计,{},[]
3060,3060,natural language,自然语言,1.0,9,"[{'word': '自然语言', 'ratio': 1.0}]",自然语言,{},[]
3061,3061,natural language query,自然语言查询,1.0,9,"[{'word': '自然语言查询', 'ratio': 1.0}]",自然语言查询,{},[]
3062,3062,natural logic,自然逻辑,1.0,9,"[{'word': '自然逻辑', 'ratio': 1.0}]",自然逻辑,{},[]
3063,3063,natural logic inference,自然逻辑推理,1.0,9,"[{'word': '自然逻辑推理', 'ratio': 1.0}]",自然逻辑推理,{},[]
3064,3064,natural parameter,自然参数,1.0,9,"[{'word': '自然参数', 'ratio': 1.0}]",自然参数,{},[]
3065,3065,near-optimality,近似最优性,0.8888888888888888,9,"[{'word': '近似最优性', 'ratio': 0.8888888888888888}, {'word': '接近最优性', 'ratio': 0.1111111111111111}]",近似最优性,{},[]
3066,3066,nearest neighbor classifier,最近邻分类器,1.0,9,"[{'word': '最近邻分类器', 'ratio': 1.0}]",最近邻分类器,{},[]
3067,3067,nearest neighbor search,最近邻搜索,1.0,9,"[{'word': '最近邻搜索', 'ratio': 1.0}]",最近邻搜索,{},[]
3068,3068,nearest-neighbor algorithm,最近邻算法,1.0,9,"[{'word': '最近邻算法', 'ratio': 1.0}]",最近邻算法,{},[]
3069,3069,negation,否定,1.0,9,"[{'word': '否定', 'ratio': 1.0}]",否定,{},[]
3070,3070,negative log-likelihood,负对数似然,1.0,9,"[{'word': '负对数似然', 'ratio': 1.0}]",负对数似然,{},[]
3071,3071,negative pair,负样本对,0.8888888888888888,9,"[{'word': '负样本对', 'ratio': 0.8888888888888888}, {'word': '负对', 'ratio': 0.1111111111111111}]",负样本对,{},[]
3072,3072,negative sample,负样本,0.8888888888888888,9,"[{'word': '负样本', 'ratio': 0.8888888888888888}, {'word': '负样本对', 'ratio': 0.1111111111111111}]",负样本,{},[]
3073,3073,negative transfer,负迁移,0.8888888888888888,9,"[{'word': '负迁移', 'ratio': 0.8888888888888888}, {'word': '负样本', 'ratio': 0.1111111111111111}]",负迁移,{},[]
3074,3074,neighborhood function,邻域函数,0.9,10,"[{'word': '邻域函数', 'ratio': 0.9}, {'word': '模型并行', 'ratio': 0.1}]",邻域函数,{},[]
3075,3075,neighborhood system,邻域系统,0.9,10,"[{'word': '邻域系统', 'ratio': 0.9}, {'word': '模型参数', 'ratio': 0.1}]",邻域系统,{},[]
3076,3076,net,神经网络,0.0,10,"[{'word': '网络', 'ratio': 0.9}, {'word': '模型性能', 'ratio': 0.1}]",网络,{},[]
3077,3077,network,网络,0.9,10,"[{'word': '网络', 'ratio': 0.9}, {'word': '模型精度', 'ratio': 0.1}]",网络,{},[]
3078,3078,network architecture,网络架构,0.9,10,"[{'word': '网络架构', 'ratio': 0.9}, {'word': '模型预测', 'ratio': 0.1}]",网络架构,{},[]
3079,3079,network feature,网络特征,1.0,10,"[{'word': '网络特征', 'ratio': 1.0}]",网络特征,{},[]
3080,3080,network parameter,网络参数,1.0,10,"[{'word': '网络参数', 'ratio': 1.0}]",网络参数,{},[]
3081,3081,network structure,网络结构,1.0,10,"[{'word': '网络结构', 'ratio': 1.0}]",网络结构,{},[]
3082,3082,network topology,网络拓扑结构,0.0,10,"[{'word': '网络拓扑', 'ratio': 1.0}]",网络拓扑,{},[]
3083,3083,network weight,网络权重,0.9,10,"[{'word': '网络权重', 'ratio': 0.9}, {'word': '网络拓扑', 'ratio': 0.1}]",网络权重,{},[]
3084,3084,neural activity,神经活动,1.0,10,"[{'word': '神经活动', 'ratio': 1.0}]",神经活动,{},[]
3085,3085,neural approach,神经网络方法,0.0,10,"[{'word': '神经方法', 'ratio': 1.0}]",神经方法,{},[]
3086,3086,neural architecture,神经架构,0.7,10,"[{'word': '神经架构', 'ratio': 0.7}, {'word': '神经网络架构', 'ratio': 0.2}, {'word': '神经结构', 'ratio': 0.1}]",神经架构,{},[]
3087,3087,neural architecture search,神经架构搜索,0.7,10,"[{'word': '神经架构搜索', 'ratio': 0.7}, {'word': '神经网络架构搜索', 'ratio': 0.2}, {'word': 'v', 'ratio': 0.1}]",神经架构搜索,{},[]
3088,3088,neural embedding,神经嵌入,1.0,10,"[{'word': '神经嵌入', 'ratio': 1.0}]",神经嵌入,{},[]
3089,3089,neural generation model,神经生成模型,1.0,10,"[{'word': '神经生成模型', 'ratio': 1.0}]",神经生成模型,{},[]
3090,3090,neural implicit representation,神经隐式表示,1.0,10,"[{'word': '神经隐式表示', 'ratio': 1.0}]",神经隐式表示,{},[]
3091,3091,neural language model,神经语言模型,1.0,10,"[{'word': '神经语言模型', 'ratio': 1.0}]",神经语言模型,{},[]
3092,3092,neural machinery,神经机制,1.0,10,"[{'word': '神经机制', 'ratio': 1.0}]",神经机制,{},[]
3093,3093,neural mapping,NeRF,0.0,8,"[{'word': '神经映射', 'ratio': 1.0}]",神经映射,{},[]
3094,3094,neural method,神经方法,1.0,8,"[{'word': '神经方法', 'ratio': 1.0}]",神经方法,{},[]
3095,3095,neural model,神经模型,1.0,8,"[{'word': '神经模型', 'ratio': 1.0}]",神经模型,{},[]
3096,3096,neural module,神经模块,1.0,8,"[{'word': '神经模块', 'ratio': 1.0}]",神经模块,{},[]
3097,3097,neural net,神经网络,1.0,8,"[{'word': '神经网络', 'ratio': 1.0}]",神经网络,{},[]
3098,3098,neural network architecture,神经网络架构,1.0,8,"[{'word': '神经网络架构', 'ratio': 1.0}]",神经网络架构,{},[]
3099,3099,neural network classifier,神经网络分类器,1.0,8,"[{'word': '神经网络分类器', 'ratio': 1.0}]",神经网络分类器,{},[]
3100,3100,neural network language model,神经网络语言模型,1.0,8,"[{'word': '神经网络语言模型', 'ratio': 1.0}]",神经网络语言模型,{},[]
3101,3101,neural network layer,神经网络层,1.0,8,"[{'word': '神经网络层', 'ratio': 1.0}]",神经网络层,{},[]
3102,3102,neural network model,神经网络模型,1.0,8,"[{'word': '神经网络模型', 'ratio': 1.0}]",神经网络模型,{},[]
3103,3103,neural operator,神经算子,1.0,9,"[{'word': '神经算子', 'ratio': 1.0}]",神经算子,{},[]
3104,3104,neural parser,神经网络语法分析器,0.0,9,"[{'word': '神经解析器', 'ratio': 1.0}]",神经解析器,{},[]
3105,3105,neural renderer,神经渲染器,1.0,9,"[{'word': '神经渲染器', 'ratio': 1.0}]",神经渲染器,{},[]
3106,3106,neural rendering,神经渲染,1.0,9,"[{'word': '神经渲染', 'ratio': 1.0}]",神经渲染,{},[]
3107,3107,neural representation,神经表征,0.1111111111111111,9,"[{'word': '神经表示', 'ratio': 0.8888888888888888}, {'word': '神经表征', 'ratio': 0.1111111111111111}]",神经表示,{},[]
3108,3108,neural retrieval,神经检索,1.0,9,"[{'word': '神经检索', 'ratio': 1.0}]",神经检索,{},[]
3109,3109,neural scaling law,神经缩放定律,0.2222222222222222,9,"[{'word': '神经规模定律', 'ratio': 0.2222222222222222}, {'word': '神经缩放定律', 'ratio': 0.2222222222222222}, {'word': '神经网络缩放定律', 'ratio': 0.2222222222222222}, {'word': '神经网络规模定律', 'ratio': 0.1111111111111111}, {'word': '神经缩放法则', 'ratio': 0.1111111111111111}, {'word': '神经网络规模法则', 'ratio': 0.1111111111111111}]",神经缩放定律,"1. Rank: 神经缩放定律, 神经缩放法则, 神经规模定律, 神经网络规模定律, 神经网络缩放定律, 神经网络规模法则

2. Explanation: The term ""神经缩放定律"" (neural scaling law) is the best fit because it accurately captures the concept of scaling in the context of neural networks, which is central to the AI domain. The word ""缩放"" (scaling) directly relates to the idea of adjusting size or capacity, which is essential in understanding how neural networks perform as their parameters or training data increase. Additionally, ""定律"" (law) conveys the empirical nature of the observations being discussed, aligning well with the scientific context. 

The second choice, ""神经缩放法则,"" is also a strong candidate, as ""法则"" (rule) is a synonym for ""定律"" but may not carry the same weight in scientific terminology. The other candidates, such as ""神经规模定律"" and ""神经网络规模定律,"" introduce the term ""规模"" (scale) which is less precise in this context compared to ""缩放"" (scaling). Furthermore, the terms that include ""神经网络"" (neural network) are less preferable because the original term does not specify ""network,"" and the broader term ""神经"" (neural) is more appropriate for the general concept being discussed.","['neural scale law', 'law of neural scaling', 'Neural Network Scaling Law', 'Neural Network Scaling Law', 'Neural Scaling Law', 'Neural network sizing rules']"
3110,3110,neural scene representation,神经场景表示,1.0,9,"[{'word': '神经场景表示', 'ratio': 1.0}]",神经场景表示,{},[]
3111,3111,neural sequence model,神经序列模型,1.0,9,"[{'word': '神经序列模型', 'ratio': 1.0}]",神经序列模型,{},[]
3112,3112,neural text generation,神经文本生成,1.0,9,"[{'word': '神经文本生成', 'ratio': 1.0}]",神经文本生成,{},[]
3113,3113,neural volumetric representation,神经体积表示,1.0,10,"[{'word': '神经体积表示', 'ratio': 1.0}]",神经体积表示,{},[]
3114,3114,neural word embedding,神经词嵌入,1.0,10,"[{'word': '神经词嵌入', 'ratio': 1.0}]",神经词嵌入,{},[]
3115,3115,neuro-symbolic system,神经符号系统,1.0,10,"[{'word': '神经符号系统', 'ratio': 1.0}]",神经符号系统,{},[]
3116,3116,neuron,神经元,1.0,10,"[{'word': '神经元', 'ratio': 1.0}]",神经元,{},[]
3117,3117,next sentence prediction,下一句预测,0.875,8,"[{'word': '下一句预测', 'ratio': 0.875}, {'word': '下一个句子预测', 'ratio': 0.125}]",下一句预测,{},[]
3118,3118,next token prediction,下一个令牌预测,0.0,8,"[{'word': '下一个标记预测', 'ratio': 0.625}, {'word': '下一标记预测', 'ratio': 0.125}, {'word': '下一个词元预测', 'ratio': 0.125}, {'word': '下一个词预测', 'ratio': 0.125}]",下一个标记预测,{},[]
3119,3119,nmod,nmod,0.0,8,"[{'word': '名词修饰语', 'ratio': 0.625}, {'word': '名词修饰成分', 'ratio': 0.25}, {'word': '名词修饰', 'ratio': 0.125}]",名词修饰语,{},[]
3120,3120,no-regret algorithm,无悔算法,0.75,8,"[{'word': '无悔算法', 'ratio': 0.75}, {'word': '无遗憾算法', 'ratio': 0.125}, {'word': '无后悔算法', 'ratio': 0.125}]",无悔算法,{},[]
3121,3121,no-regret dynamic,无后悔动态,0.0,10,"[{'word': '无悔动态', 'ratio': 1.0}]",无悔动态,{},[]
3122,3122,no-regret learning algorithm,无悔学习算法,1.0,10,"[{'word': '无悔学习算法', 'ratio': 1.0}]",无悔学习算法,{},[]
3123,3123,node,节点,1.0,10,"[{'word': '节点', 'ratio': 1.0}]",节点,{},[]
3124,3124,node attribute,节点属性,1.0,10,"[{'word': '节点属性', 'ratio': 1.0}]",节点属性,{},[]
3125,3125,node classification,节点分类,1.0,10,"[{'word': '节点分类', 'ratio': 1.0}]",节点分类,{},[]
3126,3126,node degree,节点度数,0.3333333333333333,9,"[{'word': '节点度', 'ratio': 0.6666666666666666}, {'word': '节点度数', 'ratio': 0.3333333333333333}]",节点度,{},[]
3127,3127,node embedding,节点嵌入,1.0,9,"[{'word': '节点嵌入', 'ratio': 1.0}]",节点嵌入,{},[]
3128,3128,node feature,节点特征,1.0,9,"[{'word': '节点特征', 'ratio': 1.0}]",节点特征,{},[]
3129,3129,node feature matrix,节点特征矩阵,1.0,9,"[{'word': '节点特征矩阵', 'ratio': 1.0}]",节点特征矩阵,{},[]
3130,3130,node label,节点标签,1.0,9,"[{'word': '节点标签', 'ratio': 1.0}]",节点标签,{},[]
3131,3131,node representation,节点表示,1.0,10,"[{'word': '节点表示', 'ratio': 1.0}]",节点表示,{},[]
3132,3132,node set,节点集合,0.3,10,"[{'word': '节点集', 'ratio': 0.7}, {'word': '节点集合', 'ratio': 0.3}]",节点集,{},[]
3133,3133,node-disjoint path,无公共节点路径,0.0,10,"[{'word': '节点不相交路径', 'ratio': 0.8}, {'word': '点不交路径', 'ratio': 0.2}]",节点不相交路径,{},[]
3134,3134,noise distribution,噪声分布,1.0,10,"[{'word': '噪声分布', 'ratio': 1.0}]",噪声分布,{},[]
3135,3135,noise level,噪声水平,1.0,10,"[{'word': '噪声水平', 'ratio': 1.0}]",噪声水平,{},[]
3136,3136,noise model,噪声模型,1.0,10,"[{'word': '噪声模型', 'ratio': 1.0}]",噪声模型,{},[]
3137,3137,noise schedule,噪声时间表,0.0,10,"[{'word': '噪声调度', 'ratio': 0.9}, {'word': '噪声计划', 'ratio': 0.1}]",噪声调度,{},[]
3138,3138,noise-contrastive estimation,噪声对比估计,1.0,10,"[{'word': '噪声对比估计', 'ratio': 1.0}]",噪声对比估计,{},[]
3139,3139,noisy channel,噪声信道,0.8,10,"[{'word': '噪声信道', 'ratio': 0.8}, {'word': '噪声通道', 'ratio': 0.1}, {'word': '有噪声信道', 'ratio': 0.1}]",噪声信道,{},[]
3140,3140,nominal mention,名词性提及,0.25,8,"[{'word': '名义提及', 'ratio': 0.625}, {'word': '名词性提及', 'ratio': 0.25}, {'word': '名词性指称', 'ratio': 0.125}]",名义提及,{},[]
3141,3141,non-Euclidean space,非欧几里得空间,0.875,8,"[{'word': '非欧几里得空间', 'ratio': 0.875}, {'word': '非欧几里德空间', 'ratio': 0.125}]",非欧几里得空间,{},[]
3142,3142,non-Markov process,非马尔可夫过程,1.0,8,"[{'word': '非马尔可夫过程', 'ratio': 1.0}]",非马尔可夫过程,{},[]
3143,3143,non-convex objective,非凸目标函数,0.375,8,"[{'word': '非凸目标', 'ratio': 0.5}, {'word': '非凸目标函数', 'ratio': 0.375}, {'word': '非马尔可夫过程', 'ratio': 0.125}]",非凸目标,{},[]
3144,3144,non-convex optimization,非凸优化,1.0,8,"[{'word': '非凸优化', 'ratio': 1.0}]",非凸优化,{},[]
3145,3145,non-convex problem,非凸问题,1.0,10,"[{'word': '非凸问题', 'ratio': 1.0}]",非凸问题,{},[]
3146,3146,non-convexity,非凸性,1.0,10,"[{'word': '非凸性', 'ratio': 1.0}]",非凸性,{},[]
3147,3147,non-linear least square,非线性最小二乘法,0.0,10,"[{'word': '非线性最小二乘', 'ratio': 1.0}]",非线性最小二乘,{},[]
3148,3148,non-linear optimization,非线性优化,1.0,10,"[{'word': '非线性优化', 'ratio': 1.0}]",非线性优化,{},[]
3149,3149,non-linearity,非线性,1.0,10,"[{'word': '非线性', 'ratio': 1.0}]",非线性,{},[]
3150,3150,non-local feature,非局部特征,1.0,10,"[{'word': '非局部特征', 'ratio': 1.0}]",非局部特征,{},[]
3151,3151,non-max suppression,非最大值抑制,0.0,10,"[{'word': '非极大值抑制', 'ratio': 0.5}, {'word': '非极大抑制', 'ratio': 0.3}, {'word': '非最大抑制', 'ratio': 0.2}]",非极大值抑制,{},[]
3152,3152,non-maxima suppression,非极大值抑制,0.6,10,"[{'word': '非极大值抑制', 'ratio': 0.6}, {'word': '非极大抑制', 'ratio': 0.4}]",非极大值抑制,{},[]
3153,3153,non-maximal suppression,非极大值抑制,0.2,10,"[{'word': '非极大抑制', 'ratio': 0.5}, {'word': '非最大抑制', 'ratio': 0.3}, {'word': '非极大值抑制', 'ratio': 0.2}]",非极大抑制,{},[]
3154,3154,non-negative matrix factorization,非负矩阵分解,0.9,10,"[{'word': '非负矩阵分解', 'ratio': 0.9}, {'word': '非负矩阵分解 如果您需要进一步的帮助或更多的翻译，请告诉我！', 'ratio': 0.1}]",非负矩阵分解,{},[]
3155,3155,non-parametric setting,非参数设置,0.8888888888888888,9,"[{'word': '非参数设置', 'ratio': 0.8888888888888888}, {'word': '非参数设定', 'ratio': 0.1111111111111111}]",非参数设置,{},[]
3156,3156,non-projective parsing,非投射解析,0.3333333333333333,9,"[{'word': '非投影解析', 'ratio': 0.5555555555555556}, {'word': '非投射解析', 'ratio': 0.3333333333333333}, {'word': '非投影分析', 'ratio': 0.1111111111111111}]",非投影解析,{},[]
3157,3157,non-submodular energy,非次模能量,0.6666666666666666,9,"[{'word': '非次模能量', 'ratio': 0.6666666666666666}, {'word': '非子模能量', 'ratio': 0.3333333333333333}]",非次模能量,{},[]
3158,3158,non-tree model,非树模型,1.0,9,"[{'word': '非树模型', 'ratio': 1.0}]",非树模型,{},[]
3159,3159,nonconvex function,非凸函数,1.0,9,"[{'word': '非凸函数', 'ratio': 1.0}]",非凸函数,{},[]
3160,3160,nonlinear optimisation,非线性优化,1.0,9,"[{'word': '非线性优化', 'ratio': 1.0}]",非线性优化,{},[]
3161,3161,nonmonotonic reasoning,非单调推理,1.0,9,"[{'word': '非单调推理', 'ratio': 1.0}]",非单调推理,{},[]
3162,3162,nonterminal symbol,非终结符号,0.4444444444444444,9,"[{'word': '非终结符', 'ratio': 0.5555555555555556}, {'word': '非终结符号', 'ratio': 0.4444444444444444}]",非终结符,{},[]
3163,3163,norm,范数,1.0,9,"[{'word': '范数', 'ratio': 1.0}]",范数,{},[]
3164,3164,normal,法线 (normal),0.0,9,"[{'word': '法线', 'ratio': 0.5555555555555556}, {'word': '法向量', 'ratio': 0.3333333333333333}, {'word': '法向', 'ratio': 0.1111111111111111}]",法线,{},[]
3165,3165,normal distribution,正态分布,0.9,10,"[{'word': '正态分布', 'ratio': 0.9}, {'word': '欧几里得范数', 'ratio': 0.1}]",正态分布,{},[]
3166,3166,normal form,标准形式,0.3,10,"[{'word': '标准形式', 'ratio': 0.3}, {'word': '标准型', 'ratio': 0.3}, {'word': '正常形式', 'ratio': 0.2}, {'word': '标准形', 'ratio': 0.1}, {'word': '欧几里得平面', 'ratio': 0.1}]",标准形式,"1. Rank: 标准形式, 标准型, 正常形式, 标准形, 欧几里得平面

2. Explanation: The term ""标准形式"" (standard form) is the best fit for the translation of ""normal form"" in the context of AI and game theory. This term is widely recognized in both mathematical and computer science literature, particularly in discussions about ontologies and game theory. It accurately conveys the concept of a structured or canonical representation of data or models, which is essential in the context provided. 

""标准型"" (standard type) and ""正常形式"" (normal form) are also relevant, but ""标准形式"" is more commonly used in academic and technical contexts, making it the preferred choice. ""标准形"" (standard form) is similar but less specific in this context. Lastly, ""欧几里得平面"" (Euclidean plane) is unrelated to the term ""normal form"" and does not fit the context at all, making it the least suitable option. 

Overall, ""标准形式"" aligns well with the semantic and contextual requirements of the AI domain, ensuring clarity and precision in communication.","['standard form', 'Standard type', 'normal form', 'Standard form', 'Euclidean plane']"
3167,3167,normal vector,法线向量,0.0,10,"[{'word': '法向量', 'ratio': 0.9}, {'word': '欧几里得投影', 'ratio': 0.1}]",法向量,{},[]
3168,3168,normal-form game,正常型博弈,0.0,10,"[{'word': '标准形式博弈', 'ratio': 0.3}, {'word': '标准型博弈', 'ratio': 0.3}, {'word': '正常形式游戏', 'ratio': 0.2}, {'word': '标准形博弈', 'ratio': 0.1}, {'word': '欧几里得空间', 'ratio': 0.1}]",标准形式博弈,"1. Rank: 标准形式博弈, 标准型博弈, 标准形博弈, 正常形式游戏, 欧几里得空间

2. Explanation: The term ""标准形式博弈"" (standard form game) is the best fit for the translation of ""normal-form game"" due to its semantic accuracy and established usage in the AI and game theory domains. This term directly corresponds to the English term, maintaining the concept of ""normal form"" which is a standard terminology in game theory. 

""标准型博弈"" (standard game) and ""标准形博弈"" (standard form game) are also close, but they are less commonly used in the context of game theory compared to ""标准形式博弈."" 

""正常形式游戏"" (normal game) is less accurate as it does not convey the specific meaning of ""normal form"" in game theory, which can lead to confusion. 

Lastly, ""欧几里得空间"" (Euclidean space) is irrelevant to the context of game theory and should not be considered a candidate for this translation. 

Overall, ""标准形式博弈"" is the most contextually appropriate and widely recognized term in the field, making it the best choice for translating ""normal-form game.""","['standard form game', 'standard game', 'normal game', 'standard form game', 'Euclidean space']"
3169,3169,normalisation,归一化,0.9,10,"[{'word': '归一化', 'ratio': 0.9}, {'word': '欧几里得变换', 'ratio': 0.1}]",归一化,{},[]
3170,3170,normalization constant,归一化常数,1.0,10,"[{'word': '归一化常数', 'ratio': 1.0}]",归一化常数,{},[]
3171,3171,normalization factor,归一化因子,1.0,10,"[{'word': '归一化因子', 'ratio': 1.0}]",归一化因子,{},[]
3172,3172,normalization function,归一化函数,1.0,10,"[{'word': '归一化函数', 'ratio': 1.0}]",归一化函数,{},[]
3173,3173,normalization layer,归一化层,0.9,10,"[{'word': '归一化层', 'ratio': 0.9}, {'word': '归一化函数', 'ratio': 0.1}]",归一化层,{},[]
3174,3174,normalization method,规范化方法 (Normalization method),0.0,10,"[{'word': '归一化方法', 'ratio': 1.0}]",归一化方法,{},[]
3175,3175,normalization strategy,归一化策略,1.0,10,"[{'word': '归一化策略', 'ratio': 1.0}]",归一化策略,{},[]
3176,3176,normalize,标准化,0.0,10,"[{'word': '归一化', 'ratio': 1.0}]",归一化,{},[]
3177,3177,normalized cross correlation,归一化互相关,0.7,10,"[{'word': '归一化互相关', 'ratio': 0.7}, {'word': '归一化交叉相关', 'ratio': 0.3}]",归一化互相关,{},[]
3178,3178,normalized cut,归一化切割,0.6,10,"[{'word': '归一化切割', 'ratio': 0.6}, {'word': '归一化割', 'ratio': 0.4}]",归一化切割,{},[]
3179,3179,normalized cut algorithm,归一化切割算法,0.6,10,"[{'word': '归一化切割算法', 'ratio': 0.6}, {'word': '归一化割算法', 'ratio': 0.4}]",归一化切割算法,{},[]
3180,3180,normalized edit distance,规范化编辑距离,0.0,9,"[{'word': '归一化编辑距离', 'ratio': 1.0}]",归一化编辑距离,{},[]
3181,3181,normalizing factor,归一化因子,1.0,9,"[{'word': '归一化因子', 'ratio': 1.0}]",归一化因子,{},[]
3182,3182,normalizing flow,归一化流,1.0,9,"[{'word': '归一化流', 'ratio': 1.0}]",归一化流,{},[]
3183,3183,noun phrase,名词短语,1.0,9,"[{'word': '名词短语', 'ratio': 1.0}]",名词短语,{},[]
3184,3184,novel view synthesis,新视图合成,0.3333333333333333,9,"[{'word': '新视角合成', 'ratio': 0.6666666666666666}, {'word': '新视图合成', 'ratio': 0.3333333333333333}]",新视角合成,{},[]
3185,3185,nsubj,主语,0.6666666666666666,9,"[{'word': '主语', 'ratio': 0.6666666666666666}, {'word': '名词性主语', 'ratio': 0.3333333333333333}]",主语,{},[]
3186,3186,nsubjpass,主动主语,0.0,9,"[{'word': '被动主语', 'ratio': 0.5555555555555556}, {'word': '被动句名词性主语', 'ratio': 0.3333333333333333}, {'word': '被动主语（nsubjpass', 'ratio': 0.1111111111111111}]",被动主语,{},[]
3187,3187,nuclear norm,核范数,1.0,9,"[{'word': '核范数', 'ratio': 1.0}]",核范数,{},[]
3188,3188,nuclear norm relaxation,核范数松弛,1.0,9,"[{'word': '核范数松弛', 'ratio': 1.0}]",核范数松弛,{},[]
3189,3189,null distribution,零分布,0.8888888888888888,9,"[{'word': '零分布', 'ratio': 0.8888888888888888}, {'word': '虚无分布', 'ratio': 0.1111111111111111}]",零分布,{},[]
3190,3190,null space,零空间,1.0,9,"[{'word': '零空间', 'ratio': 1.0}]",零空间,{},[]
3191,3191,numerical linear algebra,数值线性代数,0.8888888888888888,9,"[{'word': '数值线性代数', 'ratio': 0.8888888888888888}, {'word': '数量', 'ratio': 0.1111111111111111}]",数值线性代数,{},[]
3192,3192,object bounding box,目标边界框,0.8888888888888888,9,"[{'word': '目标边界框', 'ratio': 0.8888888888888888}, {'word': '物体边界框', 'ratio': 0.1111111111111111}]",目标边界框,{},[]
3193,3193,object categorization,目标分类,0.6666666666666666,9,"[{'word': '目标分类', 'ratio': 0.6666666666666666}, {'word': '物体分类', 'ratio': 0.3333333333333333}]",目标分类,{},[]
3194,3194,object category,物体类别,0.3333333333333333,9,"[{'word': '目标类别', 'ratio': 0.6666666666666666}, {'word': '物体类别', 'ratio': 0.3333333333333333}]",目标类别,{},[]
3195,3195,object category recognition,物体类别识别,0.3333333333333333,9,"[{'word': '目标类别识别', 'ratio': 0.6666666666666666}, {'word': '物体类别识别', 'ratio': 0.3333333333333333}]",目标类别识别,{},[]
3196,3196,object class,目标类别,0.3333333333333333,9,"[{'word': '目标类', 'ratio': 0.4444444444444444}, {'word': '目标类别', 'ratio': 0.3333333333333333}, {'word': '物体类别', 'ratio': 0.2222222222222222}]",目标类别,"1. Rank: 目标类别, 物体类别, 目标类

2. Explanation: The term ""目标类别"" (target category) is the best fit for ""object class"" in the AI domain, particularly in the context of object detection. This term accurately captures the semantic meaning of ""class"" as it relates to categories of objects that a model is trained to recognize. The back translation of ""目标类别"" as ""target category"" aligns well with the original English term, maintaining the focus on classification within a specific context.

""物体类别"" (object category) is also a strong candidate, as it directly translates to ""object category."" However, it may imply a broader or more general classification of objects rather than the specific context of classes used in machine learning models.

""目标类"" (target class) is less preferred because while it translates back to ""target class,"" it is less commonly used in the AI literature compared to ""目标类别."" The term ""类别"" (category) is more widely recognized in the context of classification tasks in machine learning, making ""目标类别"" the most contextually appropriate choice.","['target class', 'target category', 'object category']"
3197,3197,object classification,物体分类,0.6666666666666666,9,"[{'word': '物体分类', 'ratio': 0.6666666666666666}, {'word': '对象分类', 'ratio': 0.3333333333333333}]",物体分类,{},[]
3198,3198,object detector,物体检测器,0.6666666666666666,9,"[{'word': '物体检测器', 'ratio': 0.6666666666666666}, {'word': '对象检测器', 'ratio': 0.2222222222222222}, {'word': '物体探测器', 'ratio': 0.1111111111111111}]",物体检测器,{},[]
3199,3199,object domain,对象域,0.5555555555555556,9,"[{'word': '对象域', 'ratio': 0.5555555555555556}, {'word': '物体领域', 'ratio': 0.3333333333333333}, {'word': '物体域', 'ratio': 0.1111111111111111}]",对象域,{},[]
3200,3200,object embedding,物体嵌入,0.6666666666666666,9,"[{'word': '物体嵌入', 'ratio': 0.6666666666666666}, {'word': '对象嵌入', 'ratio': 0.3333333333333333}]",物体嵌入,{},[]
3201,3201,object instance segmentation,目标实例分割,0.0,9,"[{'word': '物体实例分割', 'ratio': 0.6666666666666666}, {'word': '对象实例分割', 'ratio': 0.3333333333333333}]",物体实例分割,{},[]
3202,3202,object model,物体模型,0.7777777777777778,9,"[{'word': '物体模型', 'ratio': 0.7777777777777778}, {'word': '对象模型', 'ratio': 0.2222222222222222}]",物体模型,{},[]
3203,3203,object proposal,目标提案,0.0,9,"[{'word': '物体提议', 'ratio': 0.5555555555555556}, {'word': '物体候选区域', 'ratio': 0.2222222222222222}, {'word': '目标提议', 'ratio': 0.1111111111111111}, {'word': '对象提议', 'ratio': 0.1111111111111111}]",物体提议,{},[]
3204,3204,object recognition,物体识别,0.7777777777777778,9,"[{'word': '物体识别', 'ratio': 0.7777777777777778}, {'word': '目标识别', 'ratio': 0.1111111111111111}, {'word': '对象识别', 'ratio': 0.1111111111111111}]",物体识别,{},[]
3205,3205,object segmentation,目标分割,0.1111111111111111,9,"[{'word': '物体分割', 'ratio': 0.7777777777777778}, {'word': '目标分割', 'ratio': 0.1111111111111111}, {'word': '对象分割', 'ratio': 0.1111111111111111}]",物体分割,{},[]
3206,3206,object tracking,目标跟踪,0.1111111111111111,9,"[{'word': '物体跟踪', 'ratio': 0.6666666666666666}, {'word': '目标跟踪', 'ratio': 0.1111111111111111}, {'word': '物体追踪', 'ratio': 0.1111111111111111}, {'word': '对象跟踪', 'ratio': 0.1111111111111111}]",物体跟踪,{},[]
3207,3207,objective function,目标函数,1.0,10,"[{'word': '目标函数', 'ratio': 1.0}]",目标函数,{},[]
3208,3208,objective value,目标值,0.9,10,"[{'word': '目标值', 'ratio': 0.9}, {'word': '客观价值', 'ratio': 0.1}]",目标值,{},[]
3209,3209,objectness,目标性,0.0,10,"[{'word': '物体性', 'ratio': 0.9}, {'word': '客观性', 'ratio': 0.1}]",物体性,{},[]
3210,3210,observation function,观测函数,0.6,10,"[{'word': '观测函数', 'ratio': 0.6}, {'word': '观察函数', 'ratio': 0.3}, {'word': '观察功能', 'ratio': 0.1}]",观测函数,{},[]
3211,3211,observation model,观测模型,0.6,10,"[{'word': '观测模型', 'ratio': 0.6}, {'word': '观察模型', 'ratio': 0.4}]",观测模型,{},[]
3212,3212,observation space,观测空间,0.6666666666666666,9,"[{'word': '观测空间', 'ratio': 0.6666666666666666}, {'word': '观察空间', 'ratio': 0.3333333333333333}]",观测空间,{},[]
3213,3213,observational datum,观测数据,0.7777777777777778,9,"[{'word': '观测数据', 'ratio': 0.7777777777777778}, {'word': '观察数据', 'ratio': 0.2222222222222222}]",观测数据,{},[]
3214,3214,occlusion handling,遮挡处理,1.0,9,"[{'word': '遮挡处理', 'ratio': 1.0}]",遮挡处理,{},[]
3215,3215,occlusion reasoning,遮挡推理,1.0,9,"[{'word': '遮挡推理', 'ratio': 1.0}]",遮挡推理,{},[]
3216,3216,occupancy grid,占用栅格,0.4444444444444444,9,"[{'word': '占用网格', 'ratio': 0.4444444444444444}, {'word': '占用栅格', 'ratio': 0.4444444444444444}, {'word': '占据网格', 'ratio': 0.1111111111111111}]",占用栅格,"1. Rank: 占用栅格, 占用网格, 占据网格

2. Explanation: The term ""占用栅格"" (back translated as ""Occupancy grid"") is the best fit for the AI domain-specific usage of ""occupancy grid."" This term accurately reflects the concept of a grid representation used in navigation and mapping, where each cell in the grid indicates whether a particular area is occupied or free. The use of ""栅格"" (grid) is more precise in this context, as it is commonly used in technical fields to refer to grid structures, especially in computer science and robotics. 

In contrast, ""占用网格"" (back translated as ""Occupying the grid"") is less accurate because it suggests a more general action rather than a specific representation. ""占据网格"" (back translated as ""occupy grid"") is also less suitable because it implies a more active sense of occupying rather than the passive representation of occupancy that ""occupancy grid"" conveys. Therefore, ""占用栅格"" is the most semantically accurate and contextually appropriate term for the AI domain.","['Occupying the grid', 'Occupancy grid', 'occupy grid']"
3217,3217,occupancy map,占用图,0.6666666666666666,9,"[{'word': '占用图', 'ratio': 0.6666666666666666}, {'word': '占用地图', 'ratio': 0.3333333333333333}]",占用图,{},[]
3218,3218,occupancy measure,占用度量,0.7777777777777778,9,"[{'word': '占用度量', 'ratio': 0.7777777777777778}, {'word': '占用度量里程计', 'ratio': 0.1111111111111111}, {'word': '占用测度', 'ratio': 0.1111111111111111}]",占用度量,{},[]
3219,3219,odometry,里程计,1.0,9,"[{'word': '里程计', 'ratio': 1.0}]",里程计,{},[]
3220,3220,off-diagonal element,非对角线元素,0.0,9,"[{'word': '非对角元素', 'ratio': 1.0}]",非对角元素,{},[]
3221,3221,off-policy,离线策略 (off-policy),0.0,9,"[{'word': '离策略', 'ratio': 0.7777777777777778}, {'word': '离政策', 'ratio': 0.1111111111111111}, {'word': '非策略', 'ratio': 0.1111111111111111}]",离策略,{},[]
3222,3222,offline algorithm,离线算法,1.0,8,"[{'word': '离线算法', 'ratio': 1.0}]",离线算法,{},[]
3223,3223,offline learning,离线学习,1.0,8,"[{'word': '离线学习', 'ratio': 1.0}]",离线学习,{},[]
3224,3224,on-policy,同策略,0.0,8,"[{'word': '在策略', 'ratio': 0.375}, {'word': '策略内', 'ratio': 0.25}, {'word': '在政策内', 'ratio': 0.125}, {'word': '在线策略', 'ratio': 0.125}, {'word': '依策略', 'ratio': 0.125}]",在策略,"1. Rank: 在策略, 策略内, 在线策略, 在政策内, 依策略

2. Explanation: The term ""在策略"" (in strategy) is the best fit for the translation of ""on-policy"" in the AI context. This is because ""在策略"" directly conveys the idea of being aligned with or operating under a specific strategy, which is essential in reinforcement learning where ""on-policy"" methods refer to learning from actions taken according to the current policy. 

The other candidates, such as ""策略内"" (within strategy) and ""在线策略"" (online strategy), while somewhat relevant, do not capture the precise meaning of ""on-policy"" as effectively. ""策略内"" suggests a more passive state of being within a strategy rather than actively following it, and ""在线策略"" implies a focus on online learning rather than the specific on-policy nature of the learning process. 

""在政策内"" (within policy) and ""依策略"" (according to strategy) also lack the direct connection to the active learning process that ""on-policy"" entails. Therefore, ""在策略"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['in strategy', 'within strategy', 'within policy', 'online strategy', 'According to strategy']"
3225,3225,one-against-all reduction,一对所有减少,0.0,8,"[{'word': '一对多归约', 'ratio': 0.25}, {'word': '一对多', 'ratio': 0.125}, {'word': '一对多化', 'ratio': 0.125}, {'word': '一对全体降维', 'ratio': 0.125}, {'word': '一对全归约', 'ratio': 0.125}, {'word': '对全体缩减', 'ratio': 0.125}, {'word': '一对多降维', 'ratio': 0.125}]",一对多归约,"1. Rank: 一对多归约, 一对多, 一对多化, 一对多降维, 一对全归约, 一对全体降维, 对全体缩减

2. Explanation: The term ""一对多归约"" (one-to-many reduction) is the best fit for the English term ""one-against-all reduction"" because it accurately captures the semantic meaning of the original term in the context of machine learning. In AI terminology, ""归约"" (reduction) is commonly used to describe the process of transforming a problem into a simpler form, which aligns with the concept of reducing a multi-class classification problem into multiple binary classification problems. 

The other candidates, while they may convey similar ideas, either lack the specificity or do not align as closely with the established terminology in the AI domain. For instance, ""一对多"" (one to many) and ""一对多化"" (one-to-many) are too vague and do not include the term ""归约,"" which is crucial for conveying the reduction aspect. ""一对多降维"" (one-to-many dimensionality reduction) introduces the concept of dimensionality reduction, which is not relevant in this context. 

Overall, ""一对多归约"" maintains both semantic accuracy and contextual fit, making it the most appropriate choice for the AI domain.","['one-to-many reduction', 'one to many', 'one-to-many', 'A pair of overall dimensionality reduction', 'One pair total reduction', 'reduce overall', 'One-to-many dimensionality reduction']"
3226,3226,one-hot encoded,独热编码,1.0,8,"[{'word': '独热编码', 'ratio': 1.0}]",独热编码,{},[]
3227,3227,one-hot representation,独热表示法,0.1,10,"[{'word': '独热表示', 'ratio': 0.8}, {'word': '独热编码表示', 'ratio': 0.1}, {'word': '独热表示法', 'ratio': 0.1}]",独热表示,{},[]
3228,3228,one-hot vector,一热向量,0.0,10,"[{'word': '独热向量', 'ratio': 0.9}, {'word': '单一热向量', 'ratio': 0.1}]",独热向量,{},[]
3229,3229,one-shot learning,一次性学习,0.3,10,"[{'word': '单次学习', 'ratio': 0.3}, {'word': '一次性学习', 'ratio': 0.3}, {'word': '单样本学习', 'ratio': 0.3}, {'word': '单一热学习', 'ratio': 0.1}]",单样本学习,"1. Rank: 单样本学习, 单次学习, 一次性学习, 单一热学习

2. Explanation: The term ""单样本学习"" (One-sample learning) is the best fit for ""one-shot learning"" in the AI domain because it accurately captures the essence of the concept, which refers to the ability of a model to learn from a single example or sample. This term is semantically precise and aligns well with the terminology used in machine learning literature. 

""单次学习"" (Single learning) is a close second, but it lacks the specificity of ""sample,"" which is crucial in the context of machine learning. ""一次性学习"" (One-time learning) suggests a temporal aspect that is not inherent in the concept of one-shot learning, making it less accurate. Lastly, ""单一热学习"" (Single hot learning) is not a recognized term in the AI field and does not convey the intended meaning, making it the least suitable option. 

Overall, ""单样本学习"" is the most contextually appropriate and semantically accurate translation for ""one-shot learning"" in the AI domain.","['single learning', 'One-time learning', 'One-sample learning', 'single hot learning']"
3230,3230,one-shot setting,单次设置,0.2,10,"[{'word': '一次性设置', 'ratio': 0.3}, {'word': '单样本设置', 'ratio': 0.3}, {'word': '单次设置', 'ratio': 0.2}, {'word': '单次场景', 'ratio': 0.1}, {'word': '单热设置', 'ratio': 0.1}]",单样本设置,"1. Rank: 单样本设置, 单次设置, 一次性设置, 单次场景, 单热设置

2. Explanation: The term ""单样本设置"" (Single sample setup) is the best fit for the translation of ""one-shot setting"" in the AI domain. This is because ""one-shot"" refers to the ability of a model to learn from a single example or sample, which is accurately captured by ""单样本"" (single sample). The term ""设置"" (setup) is also appropriate in this context, as it aligns with the technical usage in machine learning and AI. 

In contrast, ""一次性设置"" (One-time setup) implies a one-time occurrence rather than the specific learning context, which can lead to confusion. ""单次设置"" (Single setup) is somewhat vague and does not convey the sampling aspect. ""单次场景"" (single scene) and ""单热设置"" (Single heat setting) are not relevant to the context of machine learning and do not accurately reflect the concept of learning from a single instance. Therefore, ""单样本设置"" is the most semantically accurate and contextually appropriate choice.","['One-time setup', 'Single sample setup', 'Single setup', 'single scene', 'Single heat setting']"
3231,3231,one-stage detector,单阶段检测器,0.9,10,"[{'word': '单阶段检测器', 'ratio': 0.9}, {'word': '单热检测器', 'ratio': 0.1}]",单阶段检测器,{},[]
3232,3232,one-versus-all,一对所有,0.0,9,"[{'word': '一对多', 'ratio': 1.0}]",一对多,{},[]
3233,3233,online algorithm,在线算法,1.0,9,"[{'word': '在线算法', 'ratio': 1.0}]",在线算法,{},[]
3234,3234,online convex optimization,在线凸优化,1.0,9,"[{'word': '在线凸优化', 'ratio': 1.0}]",在线凸优化,{},[]
3235,3235,online gradient descent,在线梯度下降,1.0,9,"[{'word': '在线梯度下降', 'ratio': 1.0}]",在线梯度下降,{},[]
3236,3236,online learning,在线学习,1.0,9,"[{'word': '在线学习', 'ratio': 1.0}]",在线学习,{},[]
3237,3237,online learning algorithm,在线学习算法,1.0,9,"[{'word': '在线学习算法', 'ratio': 1.0}]",在线学习算法,{},[]
3238,3238,online learning method,在线学习方法,1.0,9,"[{'word': '在线学习方法', 'ratio': 1.0}]",在线学习方法,{},[]
3239,3239,online learning theory,在线学习理论,1.0,9,"[{'word': '在线学习理论', 'ratio': 1.0}]",在线学习理论,{},[]
3240,3240,ontology,本体论,0.3333333333333333,9,"[{'word': '本体', 'ratio': 0.6666666666666666}, {'word': '本体论', 'ratio': 0.3333333333333333}]",本体,{},[]
3241,3241,ontology language,本体语言,1.0,10,"[{'word': '本体语言', 'ratio': 1.0}]",本体语言,{},[]
3242,3242,ontology-mediated query,本体中介查询,0.6,10,"[{'word': '本体中介查询', 'ratio': 0.6}, {'word': '本体介导查询', 'ratio': 0.3}, {'word': '基于本体的查询', 'ratio': 0.1}]",本体中介查询,{},[]
3243,3243,open set,开集,0.8,10,"[{'word': '开集', 'ratio': 0.8}, {'word': '开放集', 'ratio': 0.1}, {'word': '本体中介查询', 'ratio': 0.1}]",开集,{},[]
3244,3244,open-ended text generation,开放式文本生成,1.0,10,"[{'word': '开放式文本生成', 'ratio': 1.0}]",开放式文本生成,{},[]
3245,3245,open-loop,开环,1.0,10,"[{'word': '开环', 'ratio': 1.0}]",开环,{},[]
3246,3246,operator norm,算子范数,0.8888888888888888,9,"[{'word': '算子范数', 'ratio': 0.8888888888888888}, {'word': '运算符范数', 'ratio': 0.1111111111111111}]",算子范数,{},[]
3247,3247,operator sequence,运算符序列,0.0,9,"[{'word': '算子序列', 'ratio': 0.5555555555555556}, {'word': '操作符序列', 'ratio': 0.3333333333333333}, {'word': '操作序列', 'ratio': 0.1111111111111111}]",算子序列,{},[]
3248,3248,optical character recognition,光学字符识别,1.0,9,"[{'word': '光学字符识别', 'ratio': 1.0}]",光学字符识别,{},[]
3249,3249,optical flow,光流,1.0,9,"[{'word': '光流', 'ratio': 1.0}]",光流,{},[]
3250,3250,optical flow estimation,光流估计,1.0,9,"[{'word': '光流估计', 'ratio': 1.0}]",光流估计,{},[]
3251,3251,optimal control theory,最优控制理论,1.0,9,"[{'word': '最优控制理论', 'ratio': 1.0}]",最优控制理论,{},[]
3252,3252,optimal experimental design,最优实验设计,1.0,9,"[{'word': '最优实验设计', 'ratio': 1.0}]",最优实验设计,{},[]
3253,3253,optimal policy,最优策略,1.0,9,"[{'word': '最优策略', 'ratio': 1.0}]",最优策略,{},[]
3254,3254,optimal solution,最优解,1.0,9,"[{'word': '最优解', 'ratio': 1.0}]",最优解,{},[]
3255,3255,optimality,最优性,0.8888888888888888,9,"[{'word': '最优性', 'ratio': 0.8888888888888888}, {'word': '最优解', 'ratio': 0.1111111111111111}]",最优性,{},[]
3256,3256,optimality condition,最优性条件,1.0,10,"[{'word': '最优性条件', 'ratio': 1.0}]",最优性条件,{},[]
3257,3257,optimisation,优化,1.0,10,"[{'word': '优化', 'ratio': 1.0}]",优化,{},[]
3258,3258,optimisation problem,优化问题,1.0,10,"[{'word': '优化问题', 'ratio': 1.0}]",优化问题,{},[]
3259,3259,optimiser,优化器,1.0,10,"[{'word': '优化器', 'ratio': 1.0}]",优化器,{},[]
3260,3260,optimization,优化,1.0,10,"[{'word': '优化', 'ratio': 1.0}]",优化,{},[]
3261,3261,optimization algorithm,优化算法,1.0,9,"[{'word': '优化算法', 'ratio': 1.0}]",优化算法,{},[]
3262,3262,optimization framework,优化框架,1.0,9,"[{'word': '优化框架', 'ratio': 1.0}]",优化框架,{},[]
3263,3263,optimization function,优化函数,0.8888888888888888,9,"[{'word': '优化函数', 'ratio': 0.8888888888888888}, {'word': '优化函数`', 'ratio': 0.1111111111111111}]",优化函数,{},[]
3264,3264,optimization method,优化方法,1.0,9,"[{'word': '优化方法', 'ratio': 1.0}]",优化方法,{},[]
3265,3265,optimization objective,优化目标,0.8888888888888888,9,"[{'word': '优化目标', 'ratio': 0.8888888888888888}, {'word': '优化目标 如果您需要进一步的帮助或有其他问题，请告诉我！', 'ratio': 0.1111111111111111}]",优化目标,{},[]
3266,3266,optimization procedure,优化过程,0.7,10,"[{'word': '优化过程', 'ratio': 0.7}, {'word': '优化程序', 'ratio': 0.3}]",优化过程,{},[]
3267,3267,optimization step,优化步骤,1.0,10,"[{'word': '优化步骤', 'ratio': 1.0}]",优化步骤,{},[]
3268,3268,optimization theory,优化理论,1.0,10,"[{'word': '优化理论', 'ratio': 1.0}]",优化理论,{},[]
3269,3269,optimizer,优化器,1.0,10,"[{'word': '优化器', 'ratio': 1.0}]",优化器,{},[]
3270,3270,option,选项,1.0,10,"[{'word': '选项', 'ratio': 1.0}]",选项,{},[]
3271,3271,oracle,神谕程序,0.0,20,"[{'word': '神谕', 'ratio': 0.5}, {'word': '预言机', 'ratio': 0.25}, {'word': '** oracle**', 'ratio': 0.1}, {'word': '理想解', 'ratio': 0.05}, {'word': '神谕分析器', 'ratio': 0.05}, {'word': '预言器', 'ratio': 0.05}]",神谕,{},[]
3272,3272,oracle policy,神谕策略,0.5,10,"[{'word': '神谕策略', 'ratio': 0.5}, {'word': '预言机策略', 'ratio': 0.2}, {'word': '理想策略', 'ratio': 0.1}, {'word': '** oracle policy**', 'ratio': 0.1}, {'word': '预言策略', 'ratio': 0.1}]",神谕策略,{},[]
3273,3273,ordinal embedding,序数嵌入,0.8,10,"[{'word': '序数嵌入', 'ratio': 0.8}, {'word': '顺序嵌入', 'ratio': 0.2}]",序数嵌入,{},[]
3274,3274,ordinal regression,有序回归,0.0,10,"[{'word': '序数回归', 'ratio': 0.8}, {'word': '顺序回归', 'ratio': 0.2}]",序数回归,{},[]
3275,3275,orientation loss,方向损失,0.9,10,"[{'word': '方向损失', 'ratio': 0.9}, {'word': '定向损失', 'ratio': 0.1}]",方向损失,{},[]
3276,3276,orthogonal basis,正交基,1.0,10,"[{'word': '正交基', 'ratio': 1.0}]",正交基,{},[]
3277,3277,orthogonal matrix,正交矩阵,1.0,10,"[{'word': '正交矩阵', 'ratio': 1.0}]",正交矩阵,{},[]
3278,3278,orthogonal projection matrix,正交投影矩阵,1.0,10,"[{'word': '正交投影矩阵', 'ratio': 1.0}]",正交投影矩阵,{},[]
3279,3279,orthographic camera model,正交相机模型,0.5,10,"[{'word': '正交相机模型', 'ratio': 0.5}, {'word': '正射投影相机模型', 'ratio': 0.3}, {'word': '正交摄像机模型', 'ratio': 0.1}, {'word': '正射摄像机模型', 'ratio': 0.1}]",正交相机模型,{},[]
3280,3280,orthographic projection,正交投影,0.8,10,"[{'word': '正交投影', 'ratio': 0.8}, {'word': '正射影', 'ratio': 0.1}, {'word': '正投影', 'ratio': 0.1}]",正交投影,{},[]
3281,3281,orthonormal decomposition,正交分解,0.6,10,"[{'word': '正交分解', 'ratio': 0.6}, {'word': '正交归一分解', 'ratio': 0.4}]",正交分解,{},[]
3282,3282,orthonormal matrix,正交矩阵,0.6,10,"[{'word': '正交矩阵', 'ratio': 0.6}, {'word': '正交归一矩阵', 'ratio': 0.4}]",正交矩阵,{},[]
3283,3283,orthonormal row,标准正交行,0.0,10,"[{'word': '正交归一行', 'ratio': 0.5}, {'word': '正交行', 'ratio': 0.5}]",正交归一行,{},[]
3284,3284,orthonormality,正交性,0.6,10,"[{'word': '正交性', 'ratio': 0.6}, {'word': '正交归一性', 'ratio': 0.4}]",正交性,{},[]
3285,3285,out-of-distribution,超出分布,0.1111111111111111,9,"[{'word': '分布外', 'ratio': 0.6666666666666666}, {'word': '领域外', 'ratio': 0.1111111111111111}, {'word': '离散分布外', 'ratio': 0.1111111111111111}, {'word': '超出分布', 'ratio': 0.1111111111111111}]",分布外,{},[]
3286,3286,out-of-domain,领域外,0.8888888888888888,9,"[{'word': '领域外', 'ratio': 0.8888888888888888}, {'word': '跨域', 'ratio': 0.1111111111111111}]",领域外,{},[]
3287,3287,out-of-domain evaluation,跨域评估,0.1111111111111111,9,"[{'word': '领域外评估', 'ratio': 0.8888888888888888}, {'word': '跨域评估', 'ratio': 0.1111111111111111}]",领域外评估,{},[]
3288,3288,outlier,异常值,0.8888888888888888,9,"[{'word': '异常值', 'ratio': 0.8888888888888888}, {'word': '离群点', 'ratio': 0.1111111111111111}]",异常值,{},[]
3289,3289,outlier detection,异常检测,0.0,9,"[{'word': '异常值检测', 'ratio': 0.8888888888888888}, {'word': '离群点检测', 'ratio': 0.1111111111111111}]",异常值检测,{},[]
3290,3290,outlier exposure,异常暴露,0.1111111111111111,9,"[{'word': '异常值暴露', 'ratio': 0.5555555555555556}, {'word': '异常值曝光', 'ratio': 0.1111111111111111}, {'word': '异常暴露', 'ratio': 0.1111111111111111}, {'word': '离群点暴露', 'ratio': 0.1111111111111111}, {'word': '异常样本暴露', 'ratio': 0.1111111111111111}]",异常值暴露,{},[]
3291,3291,outlier rejection,异常值排除,0.0,9,"[{'word': '异常值拒绝', 'ratio': 0.4444444444444444}, {'word': '异常值剔除', 'ratio': 0.4444444444444444}, {'word': '离群点剔除', 'ratio': 0.1111111111111111}]",异常值拒绝,"1. Rank: 异常值拒绝, 异常值剔除, 离群点剔除

2. Explanation: The term ""异常值拒绝"" (outlier rejection) is the best fit because it directly translates the English term while maintaining the specific context of rejecting outliers in data analysis, which is a common practice in AI and statistical modeling. The term ""拒绝"" (rejection) accurately conveys the action of discarding or not accepting certain data points that do not conform to expected patterns, which aligns with the AI domain's focus on maintaining data integrity and improving model performance. 

In contrast, ""异常值剔除"" (outlier removal) and ""离群点剔除"" (outlier removal) use ""剔除"" (removal), which implies a more passive action of simply taking out data points without the connotation of decision-making involved in ""rejection."" Additionally, ""离群点"" (outlier) is a more general term that may not be as widely recognized in the specific context of AI as ""异常值"" (anomaly), which is commonly used in machine learning and data analysis literature. Therefore, ""异常值拒绝"" is the most semantically accurate and contextually appropriate choice.","['outlier rejection', 'Outlier removal', 'Outlier removal']"
3292,3292,output,输出,1.0,9,"[{'word': '输出', 'ratio': 1.0}]",输出,{},[]
3293,3293,output gate,输出门,1.0,9,"[{'word': '输出门', 'ratio': 1.0}]",输出门,{},[]
3294,3294,output layer,输出层,1.0,9,"[{'word': '输出层', 'ratio': 1.0}]",输出层,{},[]
3295,3295,output space,输出空间,1.0,10,"[{'word': '输出空间', 'ratio': 1.0}]",输出空间,{},[]
3296,3296,output token,输出标记 (output token),0.0,10,"[{'word': '输出标记', 'ratio': 0.8}, {'word': '输出符号', 'ratio': 0.1}, {'word': '输出令牌', 'ratio': 0.1}]",输出标记,{},[]
3297,3297,output vector,输出向量,1.0,10,"[{'word': '输出向量', 'ratio': 1.0}]",输出向量,{},[]
3298,3298,output vocabulary,输出词汇表,0.6,10,"[{'word': '输出词汇表', 'ratio': 0.6}, {'word': '输出词表', 'ratio': 0.3}, {'word': '输出词汇', 'ratio': 0.1}]",输出词汇表,{},[]
3299,3299,over-fitting,过拟合,1.0,9,"[{'word': '过拟合', 'ratio': 1.0}]",过拟合,{},[]
3300,3300,over-segmentation,过度分割,0.4444444444444444,9,"[{'word': '过度分割', 'ratio': 0.4444444444444444}, {'word': '过分细分', 'ratio': 0.2222222222222222}, {'word': '过分割', 'ratio': 0.2222222222222222}, {'word': '过分分割', 'ratio': 0.1111111111111111}]",过度分割,"1. Rank: 过度分割, 过分分割, 过分细分, 过分割

2. Explanation: The term ""过度分割"" is the best fit for ""over-segmentation"" in the AI domain because it accurately captures the concept of excessive division or segmentation of data, which is a common issue in image processing and computer vision. The back translation ""over-segmentation"" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. 

The other candidates, while they may convey a similar idea, do not fit as well in the context of AI. ""过分分割"" and ""过分细分"" suggest a more subjective interpretation of ""excessive"" or ""overly fine,"" which may not fully encompass the technical implications of segmentation in AI. ""过分割"" is less commonly used and may lead to confusion. Therefore, ""过度分割"" stands out as the most precise and contextually appropriate choice.","['over-segmentation', 'over-segmentation', 'Over-segmentation', 'over-segmentation']"
3301,3301,over-smoothing,过度平滑,0.3333333333333333,9,"[{'word': '过平滑', 'ratio': 0.6666666666666666}, {'word': '过度平滑', 'ratio': 0.3333333333333333}]",过平滑,{},[]
3302,3302,paired t-test,配对 t 检验,0.1111111111111111,9,"[{'word': '配对t检验', 'ratio': 0.8888888888888888}, {'word': '配对 t 检验', 'ratio': 0.1111111111111111}]",配对t检验,{},[]
3303,3303,pairwise,成对,0.6842105263157895,19,"[{'word': '成对的', 'ratio': 0.631578947368421}, {'word': '成对', 'ratio': 0.3684210526315789}]",成对的,{},[]
3304,3304,pairwise classifier,一对一分类器,0.0,10,"[{'word': '成对分类器', 'ratio': 1.0}]",成对分类器,{},[]
3305,3305,pairwise clique,两两团,0.0,10,"[{'word': '成对团', 'ratio': 0.8}, {'word': '成对团体', 'ratio': 0.2}]",成对团,{},[]
3306,3306,pairwise comparison,成对比较,1.0,10,"[{'word': '成对比较', 'ratio': 1.0}]",成对比较,{},[]
3307,3307,pairwise constraint,成对约束,1.0,10,"[{'word': '成对约束', 'ratio': 1.0}]",成对约束,{},[]
3308,3308,pairwise flow,成对流动,0.1111111111111111,9,"[{'word': '成对流', 'ratio': 0.7777777777777778}, {'word': '成对光流', 'ratio': 0.1111111111111111}, {'word': '成对流动', 'ratio': 0.1111111111111111}]",成对流,{},[]
3309,3309,pairwise learning,成对学习,1.0,9,"[{'word': '成对学习', 'ratio': 1.0}]",成对学习,{},[]
3310,3310,pairwise potential,成对势能,0.2222222222222222,9,"[{'word': '成对势', 'ratio': 0.3333333333333333}, {'word': '成对潜力', 'ratio': 0.3333333333333333}, {'word': '成对势能', 'ratio': 0.2222222222222222}, {'word': '成对潜能', 'ratio': 0.1111111111111111}]",成对势能,"1. Rank: 成对势能, 成对潜力, 成对势, 成对潜能

2. Explanation: The term ""成对势能"" (back translation: ""pairwise potential"") is the best fit because it accurately captures the concept of ""pairwise potential"" in the context of AI and machine learning. The use of ""势能"" (potential energy) aligns well with the mathematical and probabilistic nature of the term in the context of modeling relationships between objects or regions in an image. 

""成对潜力"" (back translation: ""pair potential"") is a close second, as it also conveys a sense of potential but lacks the specific connotation of energy or force that ""势能"" provides, which is often used in AI literature to describe potential functions.

The other candidates, ""成对势"" (back translation: ""pair potential"") and ""成对潜能"" (back translation: ""pair potential""), are less suitable because they do not convey the same level of specificity and context as ""势能"" does. ""势"" (force) is too vague, and ""潜能"" (potential) does not have the same established usage in the AI domain as ""势能."" Thus, ""成对势能"" is the most semantically accurate and contextually appropriate choice.","['pair potential', 'Pair potential', 'pairwise potential', 'pair potential']"
3311,3311,pairwise word similarity,成对词相似性,0.3333333333333333,9,"[{'word': '成对词相似度', 'ratio': 0.5555555555555556}, {'word': '成对词相似性', 'ratio': 0.3333333333333333}, {'word': '成对词语相似度', 'ratio': 0.1111111111111111}]",成对词相似度,{},[]
3312,3312,panoptic segmentation,全景分割,1.0,9,"[{'word': '全景分割', 'ratio': 1.0}]",全景分割,{},[]
3313,3313,parallel corpora,平行语料库,1.0,10,"[{'word': '平行语料库', 'ratio': 1.0}]",平行语料库,{},[]
3314,3314,parallel corpus,平行语料库,0.5,10,"[{'word': '平行语料', 'ratio': 0.5}, {'word': '平行语料库', 'ratio': 0.5}]",平行语料,{},[]
3315,3315,parallel datum,并行数据 (parallel data),0.0,10,"[{'word': '平行数据', 'ratio': 0.9}, {'word': '平行基准', 'ratio': 0.1}]",平行数据,{},[]
3316,3316,parameter,参数,1.0,18,"[{'word': '参数', 'ratio': 1.0}]",参数,{},[]
3317,3317,parameter count,参数数量,0.8888888888888888,9,"[{'word': '参数数量', 'ratio': 0.8888888888888888}, {'word': '参数计数', 'ratio': 0.1111111111111111}]",参数数量,{},[]
3318,3318,parameter estimation,参数估计,1.0,9,"[{'word': '参数估计', 'ratio': 1.0}]",参数估计,{},[]
3319,3319,parameter learning,参数学习,1.0,9,"[{'word': '参数学习', 'ratio': 1.0}]",参数学习,{},[]
3320,3320,parameter matrix,参数矩阵,1.0,9,"[{'word': '参数矩阵', 'ratio': 1.0}]",参数矩阵,{},[]
3321,3321,parameter model,参数模型,1.0,9,"[{'word': '参数模型', 'ratio': 1.0}]",参数模型,{},[]
3322,3322,parameter regularization,参数正则化,1.0,9,"[{'word': '参数正则化', 'ratio': 1.0}]",参数正则化,{},[]
3323,3323,parameter sharing,参数共享,1.0,9,"[{'word': '参数共享', 'ratio': 1.0}]",参数共享,{},[]
3324,3324,parameter size,参数大小,0.5555555555555556,9,"[{'word': '参数大小', 'ratio': 0.5555555555555556}, {'word': '参数规模', 'ratio': 0.4444444444444444}]",参数大小,{},[]
3325,3325,parameter space,参数空间,1.0,9,"[{'word': '参数空间', 'ratio': 1.0}]",参数空间,{},[]
3326,3326,parameter tuning,参数调优,0.8888888888888888,9,"[{'word': '参数调优', 'ratio': 0.8888888888888888}, {'word': '参数调整', 'ratio': 0.1111111111111111}]",参数调优,{},[]
3327,3327,parameter tying,参数绑定,1.0,9,"[{'word': '参数绑定', 'ratio': 1.0}]",参数绑定,{},[]
3328,3328,parameter update,参数更新,1.0,9,"[{'word': '参数更新', 'ratio': 1.0}]",参数更新,{},[]
3329,3329,parameter vector,参数向量,1.0,9,"[{'word': '参数向量', 'ratio': 1.0}]",参数向量,{},[]
3330,3330,parameter-efficient fine-tuning,参数高效微调,0.9,10,"[{'word': '参数高效微调', 'ratio': 0.9}, {'word': '参数有效的微调', 'ratio': 0.1}]",参数高效微调,{},[]
3331,3331,parameterisation,参数化,0.9,10,"[{'word': '参数化', 'ratio': 0.9}, {'word': '参数高效微调', 'ratio': 0.1}]",参数化,{},[]
3332,3332,parameterization,参数化,1.0,10,"[{'word': '参数化', 'ratio': 1.0}]",参数化,{},[]
3333,3333,parameterized model,参数化模型,1.0,10,"[{'word': '参数化模型', 'ratio': 1.0}]",参数化模型,{},[]
3334,3334,parametric,参数化的,0.5,10,"[{'word': '参数化的', 'ratio': 0.5}, {'word': '参数性', 'ratio': 0.2}, {'word': '参数的', 'ratio': 0.1}, {'word': '参数化', 'ratio': 0.1}, {'word': '参数', 'ratio': 0.1}]",参数化的,{},[]
3335,3335,parametric family,参数族,1.0,10,"[{'word': '参数族', 'ratio': 1.0}]",参数族,{},[]
3336,3336,parametric knowledge,参数化知识,0.3,10,"[{'word': '参数知识', 'ratio': 0.7}, {'word': '参数化知识', 'ratio': 0.3}]",参数知识,{},[]
3337,3337,parametric model,参数模型,1.0,10,"[{'word': '参数模型', 'ratio': 1.0}]",参数模型,{},[]
3338,3338,parametrization,参数化,1.0,10,"[{'word': '参数化', 'ratio': 1.0}]",参数化,{},[]
3339,3339,paraphrase,近义词,0.0,10,"[{'word': '释义', 'ratio': 0.6}, {'word': '同义改写', 'ratio': 0.2}, {'word': '同义句', 'ratio': 0.2}]",释义,{},[]
3340,3340,paraphrase generation,释义生成,0.7,10,"[{'word': '释义生成', 'ratio': 0.7}, {'word': '同义句生成', 'ratio': 0.2}, {'word': '复述生成', 'ratio': 0.1}]",释义生成,{},[]
3341,3341,paraphrase generator,释义生成器,0.7,10,"[{'word': '释义生成器', 'ratio': 0.7}, {'word': '同义句生成器', 'ratio': 0.2}, {'word': '复述生成器', 'ratio': 0.1}]",释义生成器,{},[]
3342,3342,paraphrase identification,句子重述识别,0.0,10,"[{'word': '释义识别', 'ratio': 0.7}, {'word': '同义句识别', 'ratio': 0.2}, {'word': '复述识别', 'ratio': 0.1}]",释义识别,{},[]
3343,3343,paraphrase model,文本重述模型,0.0,10,"[{'word': '释义模型', 'ratio': 0.7}, {'word': '同义句模型', 'ratio': 0.2}, {'word': '复述模型', 'ratio': 0.1}]",释义模型,{},[]
3344,3344,parent node,父节点,1.0,10,"[{'word': '父节点', 'ratio': 1.0}]",父节点,{},[]
3345,3345,parse,解析,0.9,10,"[{'word': '解析', 'ratio': 0.9}, {'word': '', 'ratio': 0.1}]",解析,{},[]
3346,3346,parse chart,解析图表,0.1,10,"[{'word': '解析图', 'ratio': 0.9}, {'word': '解析图表', 'ratio': 0.1}]",解析图,{},[]
3347,3347,parse forest,解析森林,1.0,10,"[{'word': '解析森林', 'ratio': 1.0}]",解析森林,{},[]
3348,3348,parse score,解析得分,0.9,10,"[{'word': '解析得分', 'ratio': 0.9}, {'word': '解析评分', 'ratio': 0.1}]",解析得分,{},[]
3349,3349,parse structure,语法分析结构,0.0,10,"[{'word': '解析结构', 'ratio': 1.0}]",解析结构,{},[]
3350,3350,parse tree,语法树,0.0,10,"[{'word': '解析树', 'ratio': 0.7}, {'word': '语法分析树', 'ratio': 0.2}, {'word': '句法分析树', 'ratio': 0.1}]",解析树,{},[]
3351,3351,parser,语法分析器,0.5,20,"[{'word': '解析器', 'ratio': 0.9}, {'word': '语法分析器', 'ratio': 0.1}]",解析器,{},[]
3352,3352,parsing accuracy,解析准确率,0.7,10,"[{'word': '解析准确率', 'ratio': 0.7}, {'word': '解析准确性', 'ratio': 0.1}, {'word': '语法分析准确度', 'ratio': 0.1}, {'word': '语法分析准确率', 'ratio': 0.1}]",解析准确率,{},[]
3353,3353,parsing algorithm,解析算法,1.0,10,"[{'word': '解析算法', 'ratio': 1.0}]",解析算法,{},[]
3354,3354,parsing model,解析模型,1.0,10,"[{'word': '解析模型', 'ratio': 1.0}]",解析模型,{},[]
3355,3355,parsing performance,解析性能,1.0,10,"[{'word': '解析性能', 'ratio': 1.0}]",解析性能,{},[]
3356,3356,part of speech,词性,1.0,10,"[{'word': '词性', 'ratio': 1.0}]",词性,{},[]
3357,3357,part of speech tag,词性标签,0.3,10,"[{'word': '词性标记', 'ratio': 0.6}, {'word': '词性标签', 'ratio': 0.3}, {'word': '词性标注', 'ratio': 0.1}]",词性标记,{},[]
3358,3358,part of speech tagger,词性标注器,1.0,10,"[{'word': '词性标注器', 'ratio': 1.0}]",词性标注器,{},[]
3359,3359,partial assignment,部分赋值,1.0,10,"[{'word': '部分赋值', 'ratio': 1.0}]",部分赋值,{},[]
3360,3360,partial derivation,部分推导,1.0,10,"[{'word': '部分推导', 'ratio': 1.0}]",部分推导,{},[]
3361,3361,partial evaluation,部分求值,0.2,10,"[{'word': '部分评估', 'ratio': 0.7}, {'word': '部分求值', 'ratio': 0.2}, {'word': '局部评估', 'ratio': 0.1}]",部分评估,{},[]
3362,3362,partial observability,部分可观测性,0.5,10,"[{'word': '部分可观测性', 'ratio': 0.5}, {'word': '部分可观察性', 'ratio': 0.5}]",部分可观测性,{},[]
3363,3363,partial order,偏序,0.9,10,"[{'word': '偏序', 'ratio': 0.9}, {'word': '部分序列', 'ratio': 0.1}]",偏序,{},[]
3364,3364,particle filter,粒子滤波器,1.0,10,"[{'word': '粒子滤波器', 'ratio': 1.0}]",粒子滤波器,{},[]
3365,3365,partition,划分,1.0,10,"[{'word': '划分', 'ratio': 1.0}]",划分,{},[]
3366,3366,partition function,配分函数,1.0,10,"[{'word': '配分函数', 'ratio': 1.0}]",配分函数,{},[]
3367,3367,patch,图像块,0.2,10,"[{'word': '补丁', 'ratio': 0.6}, {'word': '图像块', 'ratio': 0.2}, {'word': '块', 'ratio': 0.2}]",补丁,{},[]
3368,3368,path integration,路径积分,1.0,9,"[{'word': '路径积分', 'ratio': 1.0}]",路径积分,{},[]
3369,3369,path planning,路径规划,1.0,9,"[{'word': '路径规划', 'ratio': 1.0}]",路径规划,{},[]
3370,3370,pattern profile,模式剖析,0.0,9,"[{'word': '模式概况', 'ratio': 0.5555555555555556}, {'word': '模式轮廓', 'ratio': 0.2222222222222222}, {'word': '模式特征', 'ratio': 0.2222222222222222}]",模式概况,{},[]
3371,3371,pattern recognition,模式识别,1.0,9,"[{'word': '模式识别', 'ratio': 1.0}]",模式识别,{},[]
3372,3372,pattern summarization,模式摘要,0.5555555555555556,9,"[{'word': '模式摘要', 'ratio': 0.5555555555555556}, {'word': '模式总结', 'ratio': 0.2222222222222222}, {'word': '模式概括', 'ratio': 0.2222222222222222}]",模式摘要,{},[]
3373,3373,pattern-verbalizer pair,模式-语言化器对,0.0,10,"[{'word': '模式-动词化对', 'ratio': 0.3}, {'word': '模式-词语化器对', 'ratio': 0.2}, {'word': '模式-语言化对', 'ratio': 0.1}, {'word': '模式-表述者对', 'ratio': 0.1}, {'word': '模式-动词对', 'ratio': 0.1}, {'word': '模式-动词配对', 'ratio': 0.1}, {'word': '模式-动词化器对', 'ratio': 0.1}]",模式-动词化对,"1. Rank: 模式-动词化对, 模式-动词化器对, 模式-语言化对, 模式-表述者对, 模式-动词配对, 模式-动词对, 模式-词语化器对

2. Explanation: The term ""模式-动词化对"" (pattern-verbalizer pair) is the best fit because it accurately captures the essence of the original English term. The word ""动词化"" (verbalization) directly relates to the process of converting a pattern into a verbal form, which is crucial in the context of AI and natural language processing. This term maintains semantic accuracy and aligns well with the AI domain's terminology. 

The second candidate, ""模式-动词化器对"" (pattern-verbizer pair), is also a strong contender, but ""动词化"" emphasizes the action of verbalizing rather than the tool or entity performing the action, which is more relevant in this context. 

Other candidates like ""模式-语言化对"" (pattern-verbalization pair) and ""模式-表述者对"" (pattern-presenter pair) introduce ambiguity, as they do not specifically denote the action of verbalization in the same way. The remaining terms, such as ""模式-动词对"" (pattern-verb pair) and ""模式-动词配对"" (pattern-verb pairing), are less accurate as they simplify the concept and do not convey the full meaning of the original term. 

Thus, ""模式-动词化对"" is the most contextually appropriate choice for the AI domain.","['Pattern-verbization pairs', 'pattern-tokenizer pair', 'pattern-verbalization pair', 'pattern-presenter pair', 'pattern-verb pair', 'Pattern-verb pairing', 'pattern-verbizer pair']"
3374,3374,payoff function,收益函数,0.7,10,"[{'word': '收益函数', 'ratio': 0.7}, {'word': '支付函数', 'ratio': 0.2}, {'word': 'payoff函数', 'ratio': 0.1}]",收益函数,{},[]
3375,3375,payoff matrix,回报矩阵,0.0,10,"[{'word': '收益矩阵', 'ratio': 0.8}, {'word': '支付矩阵', 'ratio': 0.2}]",收益矩阵,{},[]
3376,3376,pedestrian detection,行人检测,1.0,10,"[{'word': '行人检测', 'ratio': 1.0}]",行人检测,{},[]
3377,3377,penalty function,惩罚函数,0.9,10,"[{'word': '惩罚函数', 'ratio': 0.9}, {'word': '罚函数', 'ratio': 0.1}]",惩罚函数,{},[]
3378,3378,penalty parameter,惩罚参数,1.0,10,"[{'word': '惩罚参数', 'ratio': 1.0}]",惩罚参数,{},[]
3379,3379,penalty term,惩罚项,1.0,10,"[{'word': '惩罚项', 'ratio': 1.0}]",惩罚项,{},[]
3380,3380,per-pixel,每像素,0.8,10,"[{'word': '每像素', 'ratio': 0.8}, {'word': '逐像素', 'ratio': 0.2}]",每像素,{},[]
3381,3381,perception,感知,1.0,10,"[{'word': '感知', 'ratio': 1.0}]",感知,{},[]
3382,3382,perceptron algorithm,感知机算法,0.4,10,"[{'word': '感知器算法', 'ratio': 0.6}, {'word': '感知机算法', 'ratio': 0.4}]",感知器算法,{},[]
3383,3383,perceptual feature,感知特征,1.0,9,"[{'word': '感知特征', 'ratio': 1.0}]",感知特征,{},[]
3384,3384,perceptual loss,感知损失,1.0,9,"[{'word': '感知损失', 'ratio': 1.0}]",感知损失,{},[]
3385,3385,perfect matching,完美匹配,1.0,9,"[{'word': '完美匹配', 'ratio': 1.0}]",完美匹配,{},[]
3386,3386,performance difference lemma,性能差异引理,1.0,9,"[{'word': '性能差异引理', 'ratio': 1.0}]",性能差异引理,{},[]
3387,3387,permutation,排列,1.0,9,"[{'word': '排列', 'ratio': 1.0}]",排列,{},[]
3388,3388,permutation invariance,置换不变性,0.6,10,"[{'word': '置换不变性', 'ratio': 0.6}, {'word': '排列不变性', 'ratio': 0.4}]",置换不变性,{},[]
3389,3389,permutation matrix,置换矩阵,0.6,10,"[{'word': '置换矩阵', 'ratio': 0.6}, {'word': '排列矩阵', 'ratio': 0.4}]",置换矩阵,{},[]
3390,3390,permutation test,置换检验,0.6,10,"[{'word': '置换检验', 'ratio': 0.6}, {'word': '排列检验', 'ratio': 0.4}]",置换检验,{},[]
3391,3391,permutohedral lattice,排列六面体格点,0.0,10,"[{'word': '排列多面体格子', 'ratio': 0.2}, {'word': '置换多面体格', 'ratio': 0.2}, {'word': '置换体晶格', 'ratio': 0.1}, {'word': '排列体晶格', 'ratio': 0.1}, {'word': '排列多面体格', 'ratio': 0.1}, {'word': '置换对称格', 'ratio': 0.1}, {'word': '置换十面体格', 'ratio': 0.1}, {'word': '置换多面体格子', 'ratio': 0.1}]","""排列多面体格子""","1. Rank: ""排列多面体格子"", ""排列多面体格"", ""置换多面体格子"", ""置换体晶格"", ""置换对称格"", ""置换十面体格"", ""置换多面体格"", ""置换体晶格""

2. Explanation: The term ""排列多面体格子"" (Arrange polyhedral grid) is the best fit for ""permutohedral lattice"" due to its semantic accuracy and contextual relevance in the AI domain. The term ""排列"" (arrange) captures the essence of the permutohedral structure, which is a specific arrangement of points in a lattice that facilitates efficient computation in algorithms, particularly in sampling and filtering. The inclusion of ""多面体"" (polyhedral) accurately reflects the geometric nature of the lattice, while ""格子"" (grid/lattice) is a standard term used in mathematical and computational contexts to describe such structures.

Other candidates like ""置换多面体格子"" (Displace polyhedral lattice) and ""置换体晶格"" (replacement body lattice) introduce the notion of displacement or replacement, which may not accurately convey the intended meaning of the permutohedral lattice in the context of AI algorithms. The term ""排列多面体格子"" maintains the integrity of the original term while ensuring clarity and precision in its application within the AI domain.","['Arrange polyhedral grid', 'Displace polyhedron', 'replacement body lattice', 'arranged body lattice', 'Arrange polyhedral lattice', 'permutation symmetry lattice', 'Replacement Decahedron', 'Displace polyhedral lattice']"
3392,3392,perplexity score,困惑度分数,0.6,10,"[{'word': '困惑度分数', 'ratio': 0.6}, {'word': '困惑度得分', 'ratio': 0.3}, {'word': '困惑度评分', 'ratio': 0.1}]",困惑度分数,{},[]
3393,3393,perspective projection,透视投影,1.0,10,"[{'word': '透视投影', 'ratio': 1.0}]",透视投影,{},[]
3394,3394,perspective projection matrix,透视投影矩阵,1.0,10,"[{'word': '透视投影矩阵', 'ratio': 1.0}]",透视投影矩阵,{},[]
3395,3395,perturbation,扰动,1.0,10,"[{'word': '扰动', 'ratio': 1.0}]",扰动,{},[]
3396,3396,perturbation analysis,扰动分析,1.0,10,"[{'word': '扰动分析', 'ratio': 1.0}]",扰动分析,{},[]
3397,3397,perturbation variance,扰动方差,1.0,10,"[{'word': '扰动方差', 'ratio': 1.0}]",扰动方差,{},[]
3398,3398,phase retrieval,相位恢复,1.0,10,"[{'word': '相位恢复', 'ratio': 1.0}]",相位恢复,{},[]
3399,3399,phoneme,音位,0.0,10,"[{'word': '音素', 'ratio': 1.0}]",音素,{},[]
3400,3400,phoneme segmentation,音素分割,1.0,10,"[{'word': '音素分割', 'ratio': 1.0}]",音素分割,{},[]
3401,3401,photoconsistency,光照一致性,0.0,10,"[{'word': '光度一致性', 'ratio': 0.7}, {'word': '光一致性', 'ratio': 0.3}]",光度一致性,{},[]
3402,3402,photometric consistency,光度一致性,1.0,5,"[{'word': '光度一致性', 'ratio': 1.0}]",光度一致性,{},[]
3403,3403,photometric error,光度误差,1.0,5,"[{'word': '光度误差', 'ratio': 1.0}]",光度误差,{},[]
3404,3404,photometric loss,光度损失,1.0,5,"[{'word': '光度损失', 'ratio': 1.0}]",光度损失,{},[]
3405,3405,photometric stereo,光度立体测量,0.0,5,"[{'word': '光度立体', 'ratio': 0.6}, {'word': '光度立体视觉', 'ratio': 0.4}]",光度立体,{},[]
3406,3406,phrase structure tree,短语结构树,1.0,10,"[{'word': '短语结构树', 'ratio': 1.0}]",短语结构树,{},[]
3407,3407,phrase table,短语表,1.0,10,"[{'word': '短语表', 'ratio': 1.0}]",短语表,{},[]
3408,3408,piecewise linear,分段线性,1.0,10,"[{'word': '分段线性', 'ratio': 1.0}]",分段线性,{},[]
3409,3409,piecewise planar,分段平面,1.0,10,"[{'word': '分段平面', 'ratio': 1.0}]",分段平面,{},[]
3410,3410,pipeline,流程,0.2,10,"[{'word': '流水线', 'ratio': 0.7}, {'word': '流程', 'ratio': 0.2}, {'word': '管道', 'ratio': 0.1}]",流水线,{},[]
3411,3411,pixel,像素,1.0,9,"[{'word': '像素', 'ratio': 1.0}]",像素,{},[]
3412,3412,pixel labeling,像素标注,0.5555555555555556,9,"[{'word': '像素标注', 'ratio': 0.5555555555555556}, {'word': '像素标记', 'ratio': 0.4444444444444444}]",像素标注,{},[]
3413,3413,pixel-level,像素级,0.8888888888888888,9,"[{'word': '像素级', 'ratio': 0.8888888888888888}, {'word': '像素级别', 'ratio': 0.1111111111111111}]",像素级,{},[]
3414,3414,pixel-wise,像素级,0.1111111111111111,9,"[{'word': '像素级别的', 'ratio': 0.3333333333333333}, {'word': '逐像素', 'ratio': 0.3333333333333333}, {'word': '像素级别', 'ratio': 0.2222222222222222}, {'word': '像素级', 'ratio': 0.1111111111111111}]",像素级别的,"1. Rank: 像素级别的, 逐像素, 像素级别, 像素级

2. Explanation: The term ""像素级别的"" is the best fit for the translation of ""pixel-wise"" in the AI domain because it accurately conveys the concept of working at the level of individual pixels, which is essential in image processing and computer vision tasks. The back translation ""pixel level"" aligns well with the original English term, maintaining the semantic integrity. 

""逐像素"" (Pixel by pixel) is also a strong candidate, but it implies a more sequential or iterative process, which may not fully capture the intended meaning of ""pixel-wise"" as a general descriptor of the method used. 

""像素级别"" (pixel level) and ""像素级"" (pixel level) are less precise as they lack the grammatical structure that indicates the method's operational context, making them less suitable for the specific AI terminology. 

Overall, ""像素级别的"" provides the clearest and most contextually appropriate translation for the term ""pixel-wise"" in the given AI context.","['pixel level', 'Pixel by pixel', 'pixel level', 'Pixel level']"
3415,3415,place recognition,场景识别,0.0,9,"[{'word': '地点识别', 'ratio': 0.6666666666666666}, {'word': '场所识别', 'ratio': 0.3333333333333333}]",地点识别,{},[]
3416,3416,placeholder,占位符,1.0,10,"[{'word': '占位符', 'ratio': 1.0}]",占位符,{},[]
3417,3417,planning,规划,0.9,10,"[{'word': '规划', 'ratio': 0.9}, {'word': '占位符', 'ratio': 0.1}]",规划,{},[]
3418,3418,planning problem,规划问题,1.0,10,"[{'word': '规划问题', 'ratio': 1.0}]",规划问题,{},[]
3419,3419,planning task,规划任务,1.0,10,"[{'word': '规划任务', 'ratio': 1.0}]",规划任务,{},[]
3420,3420,plug-in estimator,插值估计量,0.3,10,"[{'word': '插值估计器', 'ratio': 0.3}, {'word': '插值估计量', 'ratio': 0.3}, {'word': '插入式估计器', 'ratio': 0.3}, {'word': '插件估计器', 'ratio': 0.1}]",插入式估计器,"1. Rank: 插入式估计器, 插件估计器, 插值估计器, 插值估计量

2. Explanation: The term ""插入式估计器"" (plug-in estimator) is the best fit because it directly translates the English term while maintaining its specific meaning in the context of statistical estimation. In the AI and statistical domains, ""plug-in"" refers to a method where an estimator is derived by substituting sample estimates into a theoretical model. This term is widely recognized and used in statistical literature, making it semantically accurate and contextually appropriate.

The second candidate, ""插件估计器,"" while also translating to ""plug-in estimator,"" is less commonly used in the statistical context and may imply a more general software-related meaning (like a software plug-in), which could lead to confusion.

The other candidates, ""插值估计器"" and ""插值估计量,"" translate to ""interpolation estimator"" and ""interpolation estimate,"" respectively. These terms are not suitable in this context as they refer to a different statistical concept (interpolation), which does not align with the intended meaning of ""plug-in estimator."" Thus, they rank lower in terms of semantic accuracy and contextual fit.","['interpolation estimator', 'interpolation estimator', 'plug-in estimator', 'plug-in estimator']"
3421,3421,pobj,介词宾语,0.8571428571428571,7,"[{'word': '介词宾语', 'ratio': 0.8571428571428571}, {'word': '对象', 'ratio': 0.14285714285714285}]",介词宾语,{},[]
3422,3422,point cloud,点云,1.0,7,"[{'word': '点云', 'ratio': 1.0}]",点云,{},[]
3423,3423,point correspondence,点对应关系,0.0,7,"[{'word': '点对应', 'ratio': 1.0}]",点对应,{},[]
3424,3424,point estimate,点估计,1.0,7,"[{'word': '点估计', 'ratio': 1.0}]",点估计,{},[]
3425,3425,point match,点匹配,1.0,7,"[{'word': '点匹配', 'ratio': 1.0}]",点匹配,{},[]
3426,3426,pointwise,逐点,0.4285714285714285,7,"[{'word': '点对点', 'ratio': 0.5714285714285714}, {'word': '逐点', 'ratio': 0.42857142857142855}]",点对点,{},[]
3427,3427,pointwise multiplication,逐点相乘,0.0,7,"[{'word': '点乘', 'ratio': 0.42857142857142855}, {'word': '逐点乘法', 'ratio': 0.42857142857142855}, {'word': '点对点乘法', 'ratio': 0.14285714285714285}]",逐点乘法,"1. Rank: 逐点乘法, 点乘, 点对点乘法

2. Explanation: The term ""逐点乘法"" (point-by-point multiplication) is the best fit for ""pointwise multiplication"" in the AI domain because it accurately captures the concept of performing multiplication on corresponding elements of two vectors or matrices, which is the essence of pointwise operations in machine learning and AI contexts. The back translation ""Point-by-point multiplication"" aligns well with the original English term, maintaining the semantic integrity and contextual relevance.

""点乘"" (Click and multiply) is less suitable because it does not convey the mathematical operation clearly and could be misleading in a technical context. The back translation does not reflect the intended meaning of the term.

""点对点乘法"" (point-to-point multiplication) is also a reasonable candidate, but it implies a more direct one-to-one correspondence that may not fully encapsulate the broader concept of pointwise operations as effectively as ""逐点乘法."" Thus, while it is a valid term, it is not as precise in the context of AI terminology.","['Click and multiply', 'Point-by-point multiplication', 'point-to-point multiplication']"
3428,3428,policy,策略,1.0,7,"[{'word': '策略', 'ratio': 1.0}]",策略,{},[]
3429,3429,policy class,策略类,1.0,7,"[{'word': '策略类', 'ratio': 1.0}]",策略类,{},[]
3430,3430,policy distribution,策略分布,1.0,7,"[{'word': '策略分布', 'ratio': 1.0}]",策略分布,{},[]
3431,3431,policy entropy,策略熵,1.0,10,"[{'word': '策略熵', 'ratio': 1.0}]",策略熵,{},[]
3432,3432,policy evaluation,策略评估,1.0,10,"[{'word': '策略评估', 'ratio': 1.0}]",策略评估,{},[]
3433,3433,policy gradient,策略梯度,1.0,10,"[{'word': '策略梯度', 'ratio': 1.0}]",策略梯度,{},[]
3434,3434,policy gradient algorithm,策略梯度算法,1.0,10,"[{'word': '策略梯度算法', 'ratio': 1.0}]",策略梯度算法,{},[]
3435,3435,policy gradient estimator,策略梯度估计器,1.0,10,"[{'word': '策略梯度估计器', 'ratio': 1.0}]",策略梯度估计器,{},[]
3436,3436,policy gradient method,政策梯度法,0.0,7,"[{'word': '策略梯度方法', 'ratio': 1.0}]",策略梯度方法,{},[]
3437,3437,policy gradient theorem,策略梯度定理,1.0,7,"[{'word': '策略梯度定理', 'ratio': 1.0}]",策略梯度定理,{},[]
3438,3438,policy improvement,策略改进,1.0,7,"[{'word': '策略改进', 'ratio': 1.0}]",策略改进,{},[]
3439,3439,policy iteration,策略迭代,1.0,7,"[{'word': '策略迭代', 'ratio': 1.0}]",策略迭代,{},[]
3440,3440,policy learning,策略学习,1.0,7,"[{'word': '策略学习', 'ratio': 1.0}]",策略学习,{},[]
3441,3441,policy network,策略网络,1.0,7,"[{'word': '策略网络', 'ratio': 1.0}]",策略网络,{},[]
3442,3442,policy optimization,策略优化,1.0,7,"[{'word': '策略优化', 'ratio': 1.0}]",策略优化,{},[]
3443,3443,policy parameter,策略参数,1.0,7,"[{'word': '策略参数', 'ratio': 1.0}]",策略参数,{},[]
3444,3444,policy representation,策略表示,1.0,7,"[{'word': '策略表示', 'ratio': 1.0}]",策略表示,{},[]
3445,3445,policy sketch,策略草图,1.0,7,"[{'word': '策略草图', 'ratio': 1.0}]",策略草图,{},[]
3446,3446,policy space,策略空间,0.7777777777777778,9,"[{'word': '策略空间', 'ratio': 0.7777777777777778}, {'word': '余弦衰减', 'ratio': 0.1111111111111111}, {'word': '政策空间', 'ratio': 0.1111111111111111}]",策略空间,{},[]
3447,3447,polygon mesh,多边形网格,0.8888888888888888,9,"[{'word': '多边形网格', 'ratio': 0.8888888888888888}, {'word': '余弦衰减计划', 'ratio': 0.1111111111111111}]",多边形网格,{},[]
3448,3448,polylog,多对数函数,0.0,9,"[{'word': '多对数', 'ratio': 0.7777777777777778}, {'word': '余弦学习率计划', 'ratio': 0.1111111111111111}, {'word': '多重对数', 'ratio': 0.1111111111111111}]",多对数,{},[]
3449,3449,polylogarithmic,多对数级的,0.0,9,"[{'word': '多对数级', 'ratio': 0.4444444444444444}, {'word': '多对数的', 'ratio': 0.2222222222222222}, {'word': '余弦度量', 'ratio': 0.1111111111111111}, {'word': '多重对数的', 'ratio': 0.1111111111111111}, {'word': '多对数', 'ratio': 0.1111111111111111}]",多对数,"1. Rank: 多对数, 多对数的, 多对数级, 多重对数的, 余弦度量

2. Explanation: The term ""多对数"" (polylogarithm) is the best fit because it directly corresponds to the mathematical concept of polylogarithm, which is commonly used in theoretical computer science and AI contexts. It accurately captures the essence of the term without introducing unnecessary complexity or ambiguity. The back translation ""polylogarithm"" aligns perfectly with the original English term, ensuring semantic accuracy. 

The second candidate, ""多对数的"" (polylogarithmic), is also a good fit but is slightly less precise as it is an adjectival form rather than the noun form. ""多对数级"" (polylogarithmic level) introduces a level of abstraction that may not be necessary in the given context, while ""多重对数的"" (multiple logarithmic) misrepresents the concept by suggesting a different mathematical operation. Lastly, ""余弦度量"" (cosine metric) is unrelated to the term in question and should not be considered. Thus, ""多对数"" stands out as the most contextually appropriate and semantically accurate translation.","['polylogarithmic level', 'polylogarithmic', 'cosine metric', 'Multiple logarithmic', 'polylogarithm']"
3450,3450,polynomial,多项式,0.8888888888888888,9,"[{'word': '多项式', 'ratio': 0.8888888888888888}, {'word': '余弦计划', 'ratio': 0.1111111111111111}]",多项式,{},[]
3451,3451,polynomial delay,多项式延迟,1.0,9,"[{'word': '多项式延迟', 'ratio': 1.0}]",多项式延迟,{},[]
3452,3452,polynomial kernel,多项式核,0.8888888888888888,9,"[{'word': '多项式核', 'ratio': 0.8888888888888888}, {'word': '多项式核函数', 'ratio': 0.1111111111111111}]",多项式核,{},[]
3453,3453,polynomial time,多项式时间,1.0,9,"[{'word': '多项式时间', 'ratio': 1.0}]",多项式时间,{},[]
3454,3454,polynomial time algorithm,多项式时间算法,1.0,9,"[{'word': '多项式时间算法', 'ratio': 1.0}]",多项式时间算法,{},[]
3455,3455,pool-based active learning,基于池的主动学习,0.8888888888888888,9,"[{'word': '基于池的主动学习', 'ratio': 0.8888888888888888}, {'word': '基于池的主动学', 'ratio': 0.1111111111111111}]",基于池的主动学习,{},[]
3456,3456,pooling layer,池化层,1.0,9,"[{'word': '池化层', 'ratio': 1.0}]",池化层,{},[]
3457,3457,pooling operation,池化操作,1.0,9,"[{'word': '池化操作', 'ratio': 1.0}]",池化操作,{},[]
3458,3458,pose estimation,姿态估计,1.0,9,"[{'word': '姿态估计', 'ratio': 1.0}]",姿态估计,{},[]
3459,3459,pose parameter,姿态参数,0.8888888888888888,9,"[{'word': '姿态参数', 'ratio': 0.8888888888888888}, {'word': '位姿参数', 'ratio': 0.1111111111111111}]",姿态参数,{},[]
3460,3460,pose prior,姿态先验,0.8888888888888888,9,"[{'word': '姿态先验', 'ratio': 0.8888888888888888}, {'word': '先摆姿势', 'ratio': 0.1111111111111111}]",姿态先验,{},[]
3461,3461,pose space,姿态空间,1.0,8,"[{'word': '姿态空间', 'ratio': 1.0}]",姿态空间,{},[]
3462,3462,position bias,位置偏差,0.875,8,"[{'word': '位置偏差', 'ratio': 0.875}, {'word': '位置偏差位置嵌入', 'ratio': 0.125}]",位置偏差,{},[]
3463,3463,position embedding,位置嵌入,1.0,8,"[{'word': '位置嵌入', 'ratio': 1.0}]",位置嵌入,{},[]
3464,3464,positional bias,位置偏差,0.375,8,"[{'word': '位置偏差', 'ratio': 0.375}, {'word': '位置偏倚', 'ratio': 0.25}, {'word': '位置偏见', 'ratio': 0.25}, {'word': '位置性偏差', 'ratio': 0.125}]",位置性偏差,"1. Rank: 位置性偏差, 位置偏差, 位置偏倚, 位置偏见

2. Explanation: The term ""位置性偏差"" (positional bias) is the best fit for the AI domain-specific usage because it accurately captures the concept of bias related to the position of elements in a dataset or model. The inclusion of ""性"" (which translates to ""nature"" or ""characteristic"") emphasizes that this bias is a specific characteristic of the model's behavior, aligning well with the technical context of AI and machine learning. 

""位置偏差"" (position deviation) is a close second, as it conveys a similar meaning but lacks the specificity that ""位置性偏差"" provides. The terms ""位置偏倚"" (location bias) and ""位置偏见"" (location bias) are less suitable because they do not directly translate the term ""positional"" and may imply a broader or different type of bias that is not specific to the context of AI models. Thus, ""位置性偏差"" is the most semantically accurate and contextually appropriate choice.","['position deviation', 'location bias', 'location bias', 'positional bias']"
3465,3465,positional embedding,位置嵌入,1.0,8,"[{'word': '位置嵌入', 'ratio': 1.0}]",位置嵌入,{},[]
3466,3466,positional encoding,位置编码,1.0,9,"[{'word': '位置编码', 'ratio': 1.0}]",位置编码,{},[]
3467,3467,positive definite,正定,0.8888888888888888,9,"[{'word': '正定', 'ratio': 0.8888888888888888}, {'word': '正定的', 'ratio': 0.1111111111111111}]",正定,{},[]
3468,3468,positive pair,正向配对,0.0,9,"[{'word': '正样本对', 'ratio': 1.0}]",正样本对,{},[]
3469,3469,positive semidefinite,正半定,0.2222222222222222,9,"[{'word': '半正定', 'ratio': 0.6666666666666666}, {'word': '正半定', 'ratio': 0.2222222222222222}, {'word': '半正定的', 'ratio': 0.1111111111111111}]",半正定,{},[]
3470,3470,positive semidefinite kernel,正半正定核,0.0,9,"[{'word': '半正定核', 'ratio': 0.6666666666666666}, {'word': '正半定核', 'ratio': 0.3333333333333333}]",半正定核,{},[]
3471,3471,positive semidefinite matrix,半正定矩阵,0.2222222222222222,9,"[{'word': '正半定矩阵', 'ratio': 0.7777777777777778}, {'word': '半正定矩阵', 'ratio': 0.2222222222222222}]",正半定矩阵,{},[]
3472,3472,post-editing,人工后编辑,0.0,9,"[{'word': '后期编辑', 'ratio': 0.5555555555555556}, {'word': '后编辑', 'ratio': 0.4444444444444444}]",后期编辑,{},[]
3473,3473,post-hoc,后验,0.0,9,"[{'word': '事后', 'ratio': 0.8888888888888888}, {'word': '事后的', 'ratio': 0.1111111111111111}]",事后,{},[]
3474,3474,post-hoc analysis,事后分析,1.0,9,"[{'word': '事后分析', 'ratio': 1.0}]",事后分析,{},[]
3475,3475,post-processing,后处理,1.0,9,"[{'word': '后处理', 'ratio': 1.0}]",后处理,{},[]
3476,3476,posterior,后验,1.0,9,"[{'word': '后验', 'ratio': 1.0}]",后验,{},[]
3477,3477,posterior approximation,后验近似,1.0,9,"[{'word': '后验近似', 'ratio': 1.0}]",后验近似,{},[]
3478,3478,posterior density,后验密度,1.0,9,"[{'word': '后验密度', 'ratio': 1.0}]",后验密度,{},[]
3479,3479,posterior distribution,后验分布,1.0,9,"[{'word': '后验分布', 'ratio': 1.0}]",后验分布,{},[]
3480,3480,posterior entropy,后验熵,1.0,9,"[{'word': '后验熵', 'ratio': 1.0}]",后验熵,{},[]
3481,3481,posterior estimation,后验估计,1.0,9,"[{'word': '后验估计', 'ratio': 1.0}]",后验估计,{},[]
3482,3482,posterior inference,后验推断,0.8888888888888888,9,"[{'word': '后验推断', 'ratio': 0.8888888888888888}, {'word': '后验推断`', 'ratio': 0.1111111111111111}]",后验推断,{},[]
3483,3483,posterior mean,后验均值,1.0,9,"[{'word': '后验均值', 'ratio': 1.0}]",后验均值,{},[]
3484,3484,posterior mean function,后验均值函数,1.0,9,"[{'word': '后验均值函数', 'ratio': 1.0}]",后验均值函数,{},[]
3485,3485,posterior probability,后验概率,1.0,9,"[{'word': '后验概率', 'ratio': 1.0}]",后验概率,{},[]
3486,3486,posterior probability distribution,后验概率分布,1.0,7,"[{'word': '后验概率分布', 'ratio': 1.0}]",后验概率分布,{},[]
3487,3487,posterior sample,后验样本,1.0,7,"[{'word': '后验样本', 'ratio': 1.0}]",后验样本,{},[]
3488,3488,posterior variance,后验方差,1.0,7,"[{'word': '后验方差', 'ratio': 1.0}]",后验方差,{},[]
3489,3489,potential function,势函数,0.4285714285714285,7,"[{'word': '潜在函数', 'ratio': 0.42857142857142855}, {'word': '势函数', 'ratio': 0.42857142857142855}, {'word': '势能函数', 'ratio': 0.14285714285714285}]",潜在函数,"1. Rank: 潜在函数, 势函数, 势能函数

2. Explanation: The term ""潜在函数"" (potential function) is the best fit because it accurately captures the meaning of ""potential function"" in the context of AI and optimization. The term ""潜在"" translates to ""potential"" in a way that aligns with its usage in mathematical and computational contexts, where it refers to a function that represents potential values or states in a system. 

The second candidate, ""势函数"" (potential function), while also a valid translation, is less commonly used in the AI domain and may not convey the same level of clarity as ""潜在函数."" 

The third candidate, ""势能函数"" (potential energy function), introduces the concept of energy, which is not relevant in this context. This term is more appropriate in physics rather than in AI or optimization contexts, making it the least suitable option. 

Overall, ""潜在函数"" is the most semantically accurate and contextually appropriate term for the AI domain.","['potential function', 'potential function', 'potential energy function']"
3490,3490,potential heuristic,潜在启发式,0.4285714285714285,7,"[{'word': '潜在启发式', 'ratio': 0.42857142857142855}, {'word': '势启发式', 'ratio': 0.2857142857142857}, {'word': '势能启发式', 'ratio': 0.14285714285714285}, {'word': '势启发函数', 'ratio': 0.14285714285714285}]",势启发式,"1. Rank: 势启发式, 潜在启发式, 势能启发式, 势启发函数

2. Explanation: The term ""势启发式"" (potential heuristic) is the best fit for the AI domain-specific usage because it directly translates the English term while maintaining the context of ""potential"" as it relates to heuristics in planning systems. This term is concise and aligns well with established terminology in AI literature, making it easily recognizable to practitioners in the field. 

The second candidate, ""潜在启发式"" (underlying heuristic), while semantically close, introduces ambiguity by suggesting a more general or latent quality rather than the specific function of potential in heuristic optimization. 

The third candidate, ""势能启发式"" (potential energy heuristic), introduces the concept of energy, which is not relevant in this context and could mislead readers regarding the nature of the heuristic being discussed.

Lastly, ""势启发函数"" (potential heuristic function) is overly specific and may not be appropriate in contexts where the term ""heuristic"" is used more broadly. It also deviates from the original term's intent, which focuses on the heuristic itself rather than its functional representation. 

Overall, ""势启发式"" captures the essence of the term while fitting seamlessly into the AI context, making it the most suitable choice.","['underlying heuristic', 'potential heuristic', 'potential energy heuristic', 'Potential heuristic function']"
3491,3491,power iteration method,幂迭代法,1.0,9,"[{'word': '幂迭代法', 'ratio': 1.0}]",幂迭代法,{},[]
3492,3492,power law distribution,幂律分布,1.0,9,"[{'word': '幂律分布', 'ratio': 1.0}]",幂律分布,{},[]
3493,3493,power method,幂法方法,0.0,9,"[{'word': '幂法', 'ratio': 1.0}]",幂法,{},[]
3494,3494,pre-logit,前逻辑值,0.2222222222222222,9,"[{'word': '预对数', 'ratio': 0.2222222222222222}, {'word': '前逻辑值', 'ratio': 0.2222222222222222}, {'word': '前逻辑特征', 'ratio': 0.1111111111111111}, {'word': '预-logit', 'ratio': 0.1111111111111111}, {'word': '预 logits', 'ratio': 0.1111111111111111}, {'word': '前逻辑回归特征', 'ratio': 0.1111111111111111}, {'word': '预逻辑', 'ratio': 0.1111111111111111}]",预 logits,"1. Rank: 预 logits, 预-logit, 预对数, 前逻辑特征, 前逻辑回归特征, 前逻辑值, 预逻辑

2. Explanation: The term ""预 logits"" is the best fit because it directly corresponds to the English term ""pre-logit"" while maintaining the technical context of AI and machine learning. The use of ""logits"" is common in the AI domain, particularly in neural networks, where ""logits"" refer to the raw output scores before applying a softmax function. The prefix ""预"" (pre) accurately conveys the idea of something that occurs before the final output, aligning well with the context of the paper discussing generative statistical methods. 

The alternative ""预-logit"" is also a strong candidate, but it is less commonly used in the literature compared to ""预 logits."" The other candidates, such as ""前逻辑特征"" and ""前逻辑回归特征,"" introduce unnecessary complexity and deviate from the established terminology in the AI field. Terms like ""前逻辑值"" and ""预逻辑"" are vague and do not capture the specific meaning of ""logit"" in this context. Therefore, ""预 logits"" is the most semantically accurate and contextually appropriate choice.","['prelogarithm', 'previous logical value', 'prelogical features', 'pre-logit', 'pre-logits', 'Pre-logistic regression features', 'pre-logic']"
3495,3495,pre-processing,预处理,1.0,9,"[{'word': '预处理', 'ratio': 1.0}]",预处理,{},[]
3496,3496,pre-terminals,预终结符,0.125,8,"[{'word': '前终结符', 'ratio': 0.5}, {'word': '预终端', 'ratio': 0.25}, {'word': '前终端', 'ratio': 0.125}, {'word': '预终结符', 'ratio': 0.125}]",前终结符,{},[]
3497,3497,pre-train,预训练,1.0,8,"[{'word': '预训练', 'ratio': 1.0}]",预训练,{},[]
3498,3498,pre-trained checkpoint,预训练检查点,1.0,8,"[{'word': '预训练检查点', 'ratio': 1.0}]",预训练检查点,{},[]
3499,3499,pre-trained embedding,预训练词嵌入,0.0,8,"[{'word': '预训练嵌入', 'ratio': 1.0}]",预训练嵌入,{},[]
3500,3500,pre-trained language model,预训练语言模型,1.0,8,"[{'word': '预训练语言模型', 'ratio': 1.0}]",预训练语言模型,{},[]
3501,3501,pre-trained model,预训练模型,1.0,10,"[{'word': '预训练模型', 'ratio': 1.0}]",预训练模型,{},[]
3502,3502,pre-trained parameter,预训练参数,1.0,10,"[{'word': '预训练参数', 'ratio': 1.0}]",预训练参数,{},[]
3503,3503,pre-trained weight,预训练权重,1.0,10,"[{'word': '预训练权重', 'ratio': 1.0}]",预训练权重,{},[]
3504,3504,pre-training corpus,预训练语料库,1.0,10,"[{'word': '预训练语料库', 'ratio': 1.0}]",预训练语料库,{},[]
3505,3505,pre-training datum,预训练数据,1.0,10,"[{'word': '预训练数据', 'ratio': 1.0}]",预训练数据,{},[]
3506,3506,pre-training objective,预训练目标,1.0,10,"[{'word': '预训练目标', 'ratio': 1.0}]",预训练目标,{},[]
3507,3507,pre-training task,预训练任务,1.0,10,"[{'word': '预训练任务', 'ratio': 1.0}]",预训练任务,{},[]
3508,3508,precision matrix,精度矩阵,1.0,10,"[{'word': '精度矩阵', 'ratio': 1.0}]",精度矩阵,{},[]
3509,3509,precision-at-10,前10位精确度,0.0,10,"[{'word': '前十精度', 'ratio': 0.2}, {'word': '前10位准确率', 'ratio': 0.2}, {'word': '10项精度', 'ratio': 0.1}, {'word': '10个精度', 'ratio': 0.1}, {'word': '精度@10', 'ratio': 0.1}, {'word': '前10位精确率', 'ratio': 0.1}, {'word': '前10精度', 'ratio': 0.1}, {'word': '前10位精度', 'ratio': 0.1}]",前10位准确率,"1. Rank: 前10位准确率, 前10位精确率, 前10精度, 前10位精度, 精度@10, 前10位精度, 10项精度, 10个精度

2. Explanation: The term ""前10位准确率"" (Top 10 accuracy) is the best fit because it accurately conveys the concept of measuring the accuracy of the top 10 results returned by a system, which is essential in the context of information retrieval and AI metrics. The use of ""准确率"" (accuracy rate) aligns well with the standard terminology used in AI and machine learning, where precision is often discussed in terms of accuracy. Additionally, ""前10位"" (top 10) clearly indicates the specific ranking being referred to, making it contextually appropriate. Other candidates like ""精度@10"" (Accuracy@10) are less common in usage and may not be as immediately understood in the AI community. The term ""前10位精确率"" (Top 10 precision rate) is also a strong candidate, but ""准确率"" is more widely recognized in the context of performance metrics.","['Top ten accuracy', 'Top 10 accuracy', '10 items of accuracy', '10 accuracy', 'Accuracy@10', 'Top 10 accuracy', 'Top 10 accuracy', 'Top 10 digits of accuracy']"
3510,3510,precision-recall curve,精确率-召回率曲线,0.2,10,"[{'word': '精度-召回曲线', 'ratio': 0.6}, {'word': '精确率-召回率曲线', 'ratio': 0.2}, {'word': '准确率-召回率曲线', 'ratio': 0.2}]",精度-召回曲线,{},[]
3511,3511,precision-recall graph,精确率-召回率图,0.7777777777777778,9,"[{'word': '精确率-召回率图', 'ratio': 0.7777777777777778}, {'word': '精确度-召回率图', 'ratio': 0.2222222222222222}]",精确率-召回率图,{},[]
3512,3512,precondition,先决条件 (precondition),0.0,9,"[{'word': '前提条件', 'ratio': 0.7777777777777778}, {'word': '前置条件', 'ratio': 0.2222222222222222}]",前提条件,{},[]
3513,3513,preconditioner,预条件器,0.1111111111111111,9,"[{'word': '预处理器', 'ratio': 0.5555555555555556}, {'word': '预条件算子', 'ratio': 0.3333333333333333}, {'word': '预条件器', 'ratio': 0.1111111111111111}]",预处理器,{},[]
3514,3514,predicate,谓词,0.9444444444444444,18,"[{'word': '谓词', 'ratio': 0.9444444444444444}, {'word': '谓语', 'ratio': 0.05555555555555555}]",谓词,{},[]
3515,3515,predicate logic,谓词逻辑,1.0,9,"[{'word': '谓词逻辑', 'ratio': 1.0}]",谓词逻辑,{},[]
3516,3516,predicate symbol,谓词符号,1.0,9,"[{'word': '谓词符号', 'ratio': 1.0}]",谓词符号,{},[]
3517,3517,predicate-argument relation,谓词-论元关系,1.0,9,"[{'word': '谓词-论元关系', 'ratio': 1.0}]",谓词-论元关系,{},[]
3518,3518,predicate-argument structure,谓词论元结构,0.0,9,"[{'word': '谓词-论元结构', 'ratio': 1.0}]",谓词-论元结构,{},[]
3519,3519,prediction,预测,1.0,20,"[{'word': '预测', 'ratio': 1.0}]",预测,{},[]
3520,3520,prediction accuracy,预测准确性,0.1,10,"[{'word': '预测准确率', 'ratio': 0.9}, {'word': '预测准确性', 'ratio': 0.1}]",预测准确率,{},[]
3521,3521,prediction entropy,预测熵,1.0,10,"[{'word': '预测熵', 'ratio': 1.0}]",预测熵,{},[]
3522,3522,prediction error,预测误差,1.0,10,"[{'word': '预测误差', 'ratio': 1.0}]",预测误差,{},[]
3523,3523,prediction head,预测头,1.0,6,"[{'word': '预测头', 'ratio': 1.0}]",预测头,{},[]
3524,3524,prediction invariance,预测不变性,1.0,6,"[{'word': '预测不变性', 'ratio': 1.0}]",预测不变性,{},[]
3525,3525,prediction model,预测模型,1.0,6,"[{'word': '预测模型', 'ratio': 1.0}]",预测模型,{},[]
3526,3526,prediction network,预测网络,1.0,6,"[{'word': '预测网络', 'ratio': 1.0}]",预测网络,{},[]
3527,3527,prediction variance,预测方差,0.8333333333333334,6,"[{'word': '预测方差', 'ratio': 0.8333333333333334}, {'word': '预测方差 如果需要进一步的帮助，请告诉我！', 'ratio': 0.16666666666666666}]",预测方差,{},[]
3528,3528,predictive coding,预测编码,1.0,9,"[{'word': '预测编码', 'ratio': 1.0}]",预测编码,{},[]
3529,3529,predictive distribution,预测分布,1.0,9,"[{'word': '预测分布', 'ratio': 1.0}]",预测分布,{},[]
3530,3530,predictive likelihood,预测可能性,0.0,9,"[{'word': '预测似然', 'ratio': 0.8888888888888888}, {'word': '预测似然性', 'ratio': 0.1111111111111111}]",预测似然,{},[]
3531,3531,predictive model,预测模型,1.0,9,"[{'word': '预测模型', 'ratio': 1.0}]",预测模型,{},[]
3532,3532,predictive performance,预测性能,1.0,9,"[{'word': '预测性能', 'ratio': 1.0}]",预测性能,{},[]
3533,3533,predictor,预测器,1.0,9,"[{'word': '预测器', 'ratio': 1.0}]",预测器,{},[]
3534,3534,prefix,前缀,1.0,9,"[{'word': '前缀', 'ratio': 1.0}]",前缀,{},[]
3535,3535,prefix sum,前缀和,1.0,9,"[{'word': '前缀和', 'ratio': 1.0}]",前缀和,{},[]
3536,3536,prefix tree,前缀树,1.0,9,"[{'word': '前缀树', 'ratio': 1.0}]",前缀树,{},[]
3537,3537,preimage,原像,0.5555555555555556,9,"[{'word': '原像', 'ratio': 0.5555555555555556}, {'word': '逆像', 'ratio': 0.2222222222222222}, {'word': '预像', 'ratio': 0.1111111111111111}, {'word': '前像', 'ratio': 0.1111111111111111}]",原像,{},[]
3538,3538,prepositional phrase,介词短语,1.0,9,"[{'word': '介词短语', 'ratio': 1.0}]",介词短语,{},[]
3539,3539,preprocessing phase,预处理阶段,1.0,9,"[{'word': '预处理阶段', 'ratio': 1.0}]",预处理阶段,{},[]
3540,3540,presence penalty,存在惩罚,0.8888888888888888,9,"[{'word': '存在惩罚', 'ratio': 0.8888888888888888}, {'word': '出现惩罚', 'ratio': 0.1111111111111111}]",存在惩罚,{},[]
3541,3541,pretrained multilingual model,预训练多语言模型,0.5555555555555556,9,"[{'word': '预训练多语言模型', 'ratio': 0.5555555555555556}, {'word': '预训练的多语言模型', 'ratio': 0.4444444444444444}]",预训练多语言模型,{},[]
3542,3542,primal objective function,原始目标函数,1.0,8,"[{'word': '原始目标函数', 'ratio': 1.0}]",原始目标函数,{},[]
3543,3543,primal optimization,原始优化,1.0,8,"[{'word': '原始优化', 'ratio': 1.0}]",原始优化,{},[]
3544,3544,primal problem,原始问题,1.0,8,"[{'word': '原始问题', 'ratio': 1.0}]",原始问题,{},[]
3545,3545,primal variable,原变量,0.2222222222222222,9,"[{'word': '原始变量', 'ratio': 0.7777777777777778}, {'word': '原变量', 'ratio': 0.2222222222222222}]",原始变量,{},[]
3546,3546,primal-dual algorithm,原始-对偶算法,0.6666666666666666,9,"[{'word': '原始-对偶算法', 'ratio': 0.6666666666666666}, {'word': '原始对偶算法', 'ratio': 0.2222222222222222}, {'word': '原对偶算法', 'ratio': 0.1111111111111111}]",原始-对偶算法,{},[]
3547,3547,primal-dual method,原始对偶法,0.0,9,"[{'word': '原始-对偶方法', 'ratio': 0.6666666666666666}, {'word': '原始对偶方法', 'ratio': 0.2222222222222222}, {'word': '原对偶方法', 'ratio': 0.1111111111111111}]",原始-对偶方法,{},[]
3548,3548,primitive,基元,0.7777777777777778,9,"[{'word': '基元', 'ratio': 0.7777777777777778}, {'word': '原始体', 'ratio': 0.1111111111111111}, {'word': '原始元素', 'ratio': 0.1111111111111111}]",基元,{},[]
3549,3549,principal component,主成分,1.0,9,"[{'word': '主成分', 'ratio': 1.0}]",主成分,{},[]
3550,3550,prior distribution,先验分布,1.0,9,"[{'word': '先验分布', 'ratio': 1.0}]",先验分布,{},[]
3551,3551,prior hyperparameter,先验超参数,1.0,9,"[{'word': '先验超参数', 'ratio': 1.0}]",先验超参数,{},[]
3552,3552,prior knowledge,先验知识,1.0,9,"[{'word': '先验知识', 'ratio': 1.0}]",先验知识,{},[]
3553,3553,prior mean,先验均值,0.8888888888888888,9,"[{'word': '先验均值', 'ratio': 0.8888888888888888}, {'word': '先验平均值', 'ratio': 0.1111111111111111}]",先验均值,{},[]
3554,3554,prior probability,先验概率,1.0,9,"[{'word': '先验概率', 'ratio': 1.0}]",先验概率,{},[]
3555,3555,prior probability distribution,先验概率分布,1.0,10,"[{'word': '先验概率分布', 'ratio': 1.0}]",先验概率分布,{},[]
3556,3556,prior variance,先验方差,1.0,10,"[{'word': '先验方差', 'ratio': 1.0}]",先验方差,{},[]
3557,3557,priority queue,优先队列,1.0,10,"[{'word': '优先队列', 'ratio': 1.0}]",优先队列,{},[]
3558,3558,privacy budget,隐私预算,1.0,10,"[{'word': '隐私预算', 'ratio': 1.0}]",隐私预算,{},[]
3559,3559,privacy-preserving data mining,隐私保护数据挖掘,0.9,10,"[{'word': '隐私保护数据挖掘', 'ratio': 0.9}, {'word': '保护隐私的数据挖掘', 'ratio': 0.1}]",隐私保护数据挖掘,{},[]
3560,3560,probabilistic context-free grammar,概率上下文无关文法,1.0,7,"[{'word': '概率上下文无关文法', 'ratio': 1.0}]",概率上下文无关文法,{},[]
3561,3561,probabilistic distribution,概率分布,1.0,7,"[{'word': '概率分布', 'ratio': 1.0}]",概率分布,{},[]
3562,3562,probabilistic formulation,概率形式化,0.1428571428571428,7,"[{'word': '概率表述', 'ratio': 0.5714285714285714}, {'word': '概率公式', 'ratio': 0.2857142857142857}, {'word': '概率形式化', 'ratio': 0.14285714285714285}]",概率表述,{},[]
3563,3563,probabilistic framework,概率框架,1.0,7,"[{'word': '概率框架', 'ratio': 1.0}]",概率框架,{},[]
3564,3564,probabilistic generative model,概率生成模型,1.0,7,"[{'word': '概率生成模型', 'ratio': 1.0}]",概率生成模型,{},[]
3565,3565,probabilistic graphical model,概率图模型,1.0,7,"[{'word': '概率图模型', 'ratio': 1.0}]",概率图模型,{},[]
3566,3566,probabilistic inference,概率推断,0.4285714285714285,7,"[{'word': '概率推理', 'ratio': 0.5714285714285714}, {'word': '概率推断', 'ratio': 0.42857142857142855}]",概率推理,{},[]
3567,3567,probabilistic logic,概率逻辑,1.0,7,"[{'word': '概率逻辑', 'ratio': 1.0}]",概率逻辑,{},[]
3568,3568,probabilistic method,概率方法,1.0,7,"[{'word': '概率方法', 'ratio': 1.0}]",概率方法,{},[]
3569,3569,probabilistic model,概率模型,1.0,7,"[{'word': '概率模型', 'ratio': 1.0}]",概率模型,{},[]
3570,3570,probabilistic relational model,概率关系模型,1.0,8,"[{'word': '概率关系模型', 'ratio': 1.0}]",概率关系模型,{},[]
3571,3571,probabilistic representation,概率表示,1.0,8,"[{'word': '概率表示', 'ratio': 1.0}]",概率表示,{},[]
3572,3572,probabilistic semantic,概率语义,1.0,8,"[{'word': '概率语义', 'ratio': 1.0}]",概率语义,{},[]
3573,3573,probabilistic topic modeling,概率主题模型,0.0,8,"[{'word': '概率主题建模', 'ratio': 1.0}]",概率主题建模,{},[]
3574,3574,probabilistic tree,概率树,1.0,8,"[{'word': '概率树', 'ratio': 1.0}]",概率树,{},[]
3575,3575,probability density,概率密度,1.0,10,"[{'word': '概率密度', 'ratio': 1.0}]",概率密度,{},[]
3576,3576,probability density function,概率密度函数,1.0,10,"[{'word': '概率密度函数', 'ratio': 1.0}]",概率密度函数,{},[]
3577,3577,probability distribution,概率分布,1.0,10,"[{'word': '概率分布', 'ratio': 1.0}]",概率分布,{},[]
3578,3578,probability flow,概率流,1.0,10,"[{'word': '概率流', 'ratio': 1.0}]",概率流,{},[]
3579,3579,probability map,概率地图,0.0,10,"[{'word': '概率图', 'ratio': 1.0}]",概率图,{},[]
3580,3580,probability mass,概率质量,1.0,9,"[{'word': '概率质量', 'ratio': 1.0}]",概率质量,{},[]
3581,3581,probability mass function,概率质量函数,1.0,9,"[{'word': '概率质量函数', 'ratio': 1.0}]",概率质量函数,{},[]
3582,3582,probability matrix,概率矩阵,1.0,9,"[{'word': '概率矩阵', 'ratio': 1.0}]",概率矩阵,{},[]
3583,3583,probability measure,概率度量,0.0,9,"[{'word': '概率测度', 'ratio': 1.0}]",概率测度,{},[]
3584,3584,probability model,概率模型,1.0,9,"[{'word': '概率模型', 'ratio': 1.0}]",概率模型,{},[]
3585,3585,probability multiset,概率多重集,1.0,8,"[{'word': '概率多重集', 'ratio': 1.0}]",概率多重集,{},[]
3586,3586,probability simplex,概率单纯形,0.75,8,"[{'word': '概率单纯形', 'ratio': 0.75}, {'word': '概率单形体', 'ratio': 0.25}]",概率单纯形,{},[]
3587,3587,probability space,概率空间,1.0,8,"[{'word': '概率空间', 'ratio': 1.0}]",概率空间,{},[]
3588,3588,probability threshold,概率阈值,1.0,8,"[{'word': '概率阈值', 'ratio': 1.0}]",概率阈值,{},[]
3589,3589,probability transition matrix,概率转移矩阵,1.0,8,"[{'word': '概率转移矩阵', 'ratio': 1.0}]",概率转移矩阵,{},[]
3590,3590,probability vector,概率向量,1.0,8,"[{'word': '概率向量', 'ratio': 1.0}]",概率向量,{},[]
3591,3591,probing classifier,探针分类器,0.0,8,"[{'word': '探测分类器', 'ratio': 1.0}]",探测分类器,{},[]
3592,3592,problem space,问题空间,1.0,8,"[{'word': '问题空间', 'ratio': 1.0}]",问题空间,{},[]
3593,3593,product distribution,乘积分布,1.0,8,"[{'word': '乘积分布', 'ratio': 1.0}]",乘积分布,{},[]
3594,3594,product-of-expert,专家乘积,0.1111111111111111,9,"[{'word': '专家乘积模型', 'ratio': 0.6666666666666666}, {'word': '专家积模型', 'ratio': 0.2222222222222222}, {'word': '专家乘积', 'ratio': 0.1111111111111111}]",专家乘积模型,{},[]
3595,3595,program induction,程序归纳,1.0,9,"[{'word': '程序归纳', 'ratio': 1.0}]",程序归纳,{},[]
3596,3596,projection algorithm,投影算法,1.0,9,"[{'word': '投影算法', 'ratio': 1.0}]",投影算法,{},[]
3597,3597,projection layer,投影层,1.0,9,"[{'word': '投影层', 'ratio': 1.0}]",投影层,{},[]
3598,3598,projection matrix,投影矩阵,1.0,9,"[{'word': '投影矩阵', 'ratio': 1.0}]",投影矩阵,{},[]
3599,3599,projection operator,投影算子,1.0,10,"[{'word': '投影算子', 'ratio': 1.0}]",投影算子,{},[]
3600,3600,projection step,投影步骤,1.0,10,"[{'word': '投影步骤', 'ratio': 1.0}]",投影步骤,{},[]
3601,3601,projective camera,透视相机,0.0,10,"[{'word': '投影相机', 'ratio': 1.0}]",投影相机,{},[]
3602,3602,projective dependency parsing,投射依存句法分析,0.0,10,"[{'word': '投影依赖解析', 'ratio': 0.5}, {'word': '投影依存句法分析', 'ratio': 0.3}, {'word': '投影依存解析', 'ratio': 0.2}]",投影依赖解析,{},[]
3603,3603,projective dependency tree,投影依存树,0.5,10,"[{'word': '投影依存树', 'ratio': 0.5}, {'word': '投影依赖树', 'ratio': 0.5}]",投影依存树,{},[]
3604,3604,projective parsing,投影式分析,0.0,6,"[{'word': '投影解析', 'ratio': 0.6666666666666666}, {'word': '投影分析', 'ratio': 0.16666666666666666}, {'word': '投射解析', 'ratio': 0.16666666666666666}]",投影解析,{},[]
3605,3605,projective transformation,射影变换,0.0,6,"[{'word': '投影变换', 'ratio': 1.0}]",投影变换,{},[]
3606,3606,prompt,提示,0.8333333333333334,6,"[{'word': '提示', 'ratio': 0.8333333333333334}, {'word': '迅速的', 'ratio': 0.16666666666666666}]",提示,{},[]
3607,3607,prompt engineering,提示工程,0.8333333333333334,6,"[{'word': '提示工程', 'ratio': 0.8333333333333334}, {'word': '及时工程', 'ratio': 0.16666666666666666}]",提示工程,{},[]
3608,3608,prompt learning,提示学习,0.8333333333333334,6,"[{'word': '提示学习', 'ratio': 0.8333333333333334}, {'word': '及时学习', 'ratio': 0.16666666666666666}]",提示学习,{},[]
3609,3609,prompt tuning,提示调优,0.8571428571428571,7,"[{'word': '提示调优', 'ratio': 0.8571428571428571}, {'word': '及时调整', 'ratio': 0.14285714285714285}]",提示调优,{},[]
3610,3610,pronoun resolution,代词消解,0.5714285714285714,7,"[{'word': '代词消解', 'ratio': 0.5714285714285714}, {'word': '代词解析', 'ratio': 0.42857142857142855}]",代词消解,{},[]
3611,3611,proof complexity,证明复杂性,0.5714285714285714,7,"[{'word': '证明复杂性', 'ratio': 0.5714285714285714}, {'word': '证明复杂度', 'ratio': 0.42857142857142855}]",证明复杂性,{},[]
3612,3612,proof number,证明数,0.8571428571428571,7,"[{'word': '证明数', 'ratio': 0.8571428571428571}, {'word': '证明号码', 'ratio': 0.14285714285714285}]",证明数,{},[]
3613,3613,proof tree,证明树,1.0,7,"[{'word': '证明树', 'ratio': 1.0}]",证明树,{},[]
3614,3614,propensity score,倾向评分,0.0,7,"[{'word': '倾向得分', 'ratio': 1.0}]",倾向得分,{},[]
3615,3615,proposal distribution,建议分布,0.2857142857142857,7,"[{'word': '提议分布', 'ratio': 0.42857142857142855}, {'word': '提案分布', 'ratio': 0.2857142857142857}, {'word': '建议分布', 'ratio': 0.2857142857142857}]",提议分布,"1. Rank: 提议分布, 提案分布, 建议分布

2. Explanation: The term ""提议分布"" (proposal distribution) is the best fit because it accurately captures the technical meaning of ""proposal distribution"" in the context of AI and statistics. The term ""提议"" directly translates to ""proposal,"" which is commonly used in mathematical and statistical literature to refer to a distribution that is used to generate samples in algorithms like Metropolis-Hastings. 

""提案分布"" (proposal distribution) is also a valid translation, but ""提案"" can imply a more formal or legal proposal, which may not align as closely with the statistical context. 

""建议分布"" (recommended distribution) is the least accurate as ""建议"" translates to ""suggestion"" or ""recommendation,"" which does not convey the same meaning as ""proposal"" in this context. In AI and statistical sampling, ""proposal distribution"" has a specific connotation that is not captured by ""建议分布."" Therefore, ""提议分布"" is the most semantically accurate and contextually appropriate choice.","['Proposal distribution', 'Proposal distribution', 'Recommended distribution']"
3616,3616,proposal probability,建议概率,0.2857142857142857,7,"[{'word': '提议概率', 'ratio': 0.42857142857142855}, {'word': '提案概率', 'ratio': 0.2857142857142857}, {'word': '建议概率', 'ratio': 0.2857142857142857}]",提议概率,"1. Rank: 提议概率, 提案概率, 建议概率

2. Explanation: The term ""提议概率"" (proposal probability) is the best fit for the AI domain-specific usage because it accurately captures the meaning of ""proposal"" in the context of probabilistic models and algorithms. In AI, particularly in areas like Markov Chain Monte Carlo (MCMC) methods, ""proposal"" refers to a suggested state or sample that is generated during the sampling process. The term ""提议"" directly translates to ""proposal,"" which aligns well with the technical usage in the context provided.

On the other hand, ""提案概率"" (proposal probability) is also a close candidate, but ""提案"" can imply a more formal or structured proposal, which may not fit as well in the context of probabilistic sampling. Lastly, ""建议概率"" (probability of suggestion) is less suitable because ""建议"" translates to ""suggestion,"" which does not convey the same technical meaning as ""proposal"" in this context. Therefore, ""提议概率"" is the most semantically accurate and contextually appropriate term for ""proposal probability"" in the AI domain.","['Proposal probability', 'Proposal probability', 'Probability of suggestion']"
3617,3617,propositional,命题的,1.0,5,"[{'word': '命题的', 'ratio': 1.0}]",命题的,{},[]
3618,3618,propositional formula,命题公式,1.0,5,"[{'word': '命题公式', 'ratio': 1.0}]",命题公式,{},[]
3619,3619,propositional language,命题语言,1.0,5,"[{'word': '命题语言', 'ratio': 1.0}]",命题语言,{},[]
3620,3620,propositional logic,命题逻辑,1.0,5,"[{'word': '命题逻辑', 'ratio': 1.0}]",命题逻辑,{},[]
3621,3621,propositional variable,命题变量,1.0,5,"[{'word': '命题变量', 'ratio': 1.0}]",命题变量,{},[]
3622,3622,protected attribute,受保护属性,1.0,8,"[{'word': '受保护属性', 'ratio': 1.0}]",受保护属性,{},[]
3623,3623,protein folding,蛋白质折叠,1.0,8,"[{'word': '蛋白质折叠', 'ratio': 1.0}]",蛋白质折叠,{},[]
3624,3624,prototype embedding,原型嵌入,1.0,8,"[{'word': '原型嵌入', 'ratio': 1.0}]",原型嵌入,{},[]
3625,3625,proximal operator,接近算子,0.0,8,"[{'word': '近端算子', 'ratio': 0.75}, {'word': '近似算子', 'ratio': 0.25}]",近端算子,{},[]
3626,3626,pruning algorithm,剪枝算法,1.0,8,"[{'word': '剪枝算法', 'ratio': 1.0}]",剪枝算法,{},[]
3627,3627,pseudo-inverse,伪逆,1.0,8,"[{'word': '伪逆', 'ratio': 1.0}]",伪逆,{},[]
3628,3628,pure strategy,纯策略,1.0,8,"[{'word': '纯策略', 'ratio': 1.0}]",纯策略,{},[]
3629,3629,pyramid level,金字塔层级,1.0,8,"[{'word': '金字塔层级', 'ratio': 1.0}]",金字塔层级,{},[]
3630,3630,quadratic assignment problem,二次指派问题,0.25,8,"[{'word': '二次分配问题', 'ratio': 0.75}, {'word': '二次指派问题', 'ratio': 0.25}]",二次分配问题,{},[]
3631,3631,quadratic loss,二次损失,1.0,10,"[{'word': '二次损失', 'ratio': 1.0}]",二次损失,{},[]
3632,3632,quadratic program,二次规划,1.0,10,"[{'word': '二次规划', 'ratio': 1.0}]",二次规划,{},[]
3633,3633,quadratic regularizer,二次正则化项,0.5,10,"[{'word': '二次正则化项', 'ratio': 0.5}, {'word': '二次正则化器', 'ratio': 0.4}, {'word': '二次正则化', 'ratio': 0.1}]",二次正则化项,{},[]
3634,3634,quantal response equilibrium,量子响应均衡,0.2,5,"[{'word': '量子反应均衡', 'ratio': 0.8}, {'word': '量子响应均衡', 'ratio': 0.2}]",量子反应均衡,{},[]
3635,3635,quantified variable,量化变量,1.0,5,"[{'word': '量化变量', 'ratio': 1.0}]",量化变量,{},[]
3636,3636,quantifier,量词,1.0,5,"[{'word': '量词', 'ratio': 1.0}]",量词,{},[]
3637,3637,quantile,分位数,1.0,5,"[{'word': '分位数', 'ratio': 1.0}]",分位数,{},[]
3638,3638,quantization,量化,1.0,5,"[{'word': '量化', 'ratio': 1.0}]",量化,{},[]
3639,3639,quantization function,量化函数,1.0,8,"[{'word': '量化函数', 'ratio': 1.0}]",量化函数,{},[]
3640,3640,quantizer,量化器,1.0,8,"[{'word': '量化器', 'ratio': 1.0}]",量化器,{},[]
3641,3641,quasi-Newton method,类牛顿法,0.0,8,"[{'word': '准牛顿法', 'ratio': 0.375}, {'word': '拟牛顿法', 'ratio': 0.375}, {'word': '准牛顿方法', 'ratio': 0.25}]",拟牛顿法,"1. Rank: 拟牛顿法, 准牛顿法, 准牛顿方法

2. Explanation: The term ""拟牛顿法"" is the best fit for the translation of ""quasi-Newton method"" in the AI domain. The prefix ""拟"" (which means ""approximate"" or ""pseudo"") accurately captures the essence of the ""quasi"" in the English term, indicating that it is an approximation of the Newton method rather than the method itself. This is crucial in the context of optimization algorithms, where the distinction between exact and approximate methods is significant.

The term ""准牛顿法"" is also a valid translation, but ""准"" (which can mean ""standard"" or ""accurate"") does not convey the same sense of approximation as ""拟."" While it is still understandable, it may not be as precise in the context of numerical methods.

""准牛顿方法"" is less preferred because the addition of ""方法"" (method) at the end makes it slightly less concise and less commonly used in the literature compared to ""法."" In technical contexts, brevity and precision are important, and ""拟牛顿法"" is the most widely recognized term in the field of optimization and AI.","['Quasi-Newton method', 'Quasi-Newton method', 'Quasi-Newton method']"
3642,3642,quaternion,四元数,1.0,8,"[{'word': '四元数', 'ratio': 1.0}]",四元数,{},[]
3643,3643,query,查询,0.5,14,"[{'word': '查询', 'ratio': 1.0}]",查询,{},[]
3644,3644,query answering,查询回答,1.0,7,"[{'word': '查询回答', 'ratio': 1.0}]",查询回答,{},[]
3645,3645,query complexity,查询复杂度,0.8571428571428571,7,"[{'word': '查询复杂度', 'ratio': 0.8571428571428571}, {'word': '查询复杂性', 'ratio': 0.14285714285714285}]",查询复杂度,{},[]
3646,3646,query context,查询上下文,1.0,7,"[{'word': '查询上下文', 'ratio': 1.0}]",查询上下文,{},[]
3647,3647,query embedding,查询嵌入,1.0,6,"[{'word': '查询嵌入', 'ratio': 1.0}]",查询嵌入,{},[]
3648,3648,query image,查询图像,1.0,6,"[{'word': '查询图像', 'ratio': 1.0}]",查询图像,{},[]
3649,3649,query language,查询语言,1.0,6,"[{'word': '查询语言', 'ratio': 1.0}]",查询语言,{},[]
3650,3650,query phase,查询阶段,1.0,6,"[{'word': '查询阶段', 'ratio': 1.0}]",查询阶段,{},[]
3651,3651,query point,查询点,1.0,6,"[{'word': '查询点', 'ratio': 1.0}]",查询点,{},[]
3652,3652,query processing,查询处理,1.0,10,"[{'word': '查询处理', 'ratio': 1.0}]",查询处理,{},[]
3653,3653,query reformulation,查询重构,1.0,10,"[{'word': '查询重构', 'ratio': 1.0}]",查询重构,{},[]
3654,3654,query representation,查询表示,1.0,10,"[{'word': '查询表示', 'ratio': 1.0}]",查询表示,{},[]
3655,3655,query strategy,查询策略,1.0,10,"[{'word': '查询策略', 'ratio': 1.0}]",查询策略,{},[]
3656,3656,query time,查询时间,1.0,10,"[{'word': '查询时间', 'ratio': 1.0}]",查询时间,{},[]
3657,3657,query vector,查询向量,1.0,6,"[{'word': '查询向量', 'ratio': 1.0}]",查询向量,{},[]
3658,3658,query-document pair,查询-文档对,1.0,6,"[{'word': '查询-文档对', 'ratio': 1.0}]",查询-文档对,{},[]
3659,3659,radiance field,辐射场,1.0,6,"[{'word': '辐射场', 'ratio': 1.0}]",辐射场,{},[]
3660,3660,random crop,随机裁剪,1.0,7,"[{'word': '随机裁剪', 'ratio': 1.0}]",随机裁剪,{},[]
3661,3661,random feature,随机特征,1.0,7,"[{'word': '随机特征', 'ratio': 1.0}]",随机特征,{},[]
3662,3662,random matrix theory,随机矩阵理论,1.0,7,"[{'word': '随机矩阵理论', 'ratio': 1.0}]",随机矩阵理论,{},[]
3663,3663,random policy,随机策略,1.0,7,"[{'word': '随机策略', 'ratio': 1.0}]",随机策略,{},[]
3664,3664,random projection,随机投影,1.0,7,"[{'word': '随机投影', 'ratio': 1.0}]",随机投影,{},[]
3665,3665,random projection algorithm,随机投影算法,1.0,7,"[{'word': '随机投影算法', 'ratio': 1.0}]",随机投影算法,{},[]
3666,3666,random sampling,随机采样,0.1428571428571428,7,"[{'word': '随机抽样', 'ratio': 0.8571428571428571}, {'word': '随机采样', 'ratio': 0.14285714285714285}]",随机抽样,{},[]
3667,3667,random seed,随机种子,1.0,7,"[{'word': '随机种子', 'ratio': 1.0}]",随机种子,{},[]
3668,3668,random variable,随机变量,1.0,7,"[{'word': '随机变量', 'ratio': 1.0}]",随机变量,{},[]
3669,3669,random vector,随机向量,1.0,7,"[{'word': '随机向量', 'ratio': 1.0}]",随机向量,{},[]
3670,3670,random walk model,随机游走模型,1.0,7,"[{'word': '随机游走模型', 'ratio': 1.0}]",随机游走模型,{},[]
3671,3671,randomization,随机化,1.0,7,"[{'word': '随机化', 'ratio': 1.0}]",随机化,{},[]
3672,3672,randomized algorithm,随机算法,0.1428571428571428,7,"[{'word': '随机化算法', 'ratio': 0.8571428571428571}, {'word': '随机算法', 'ratio': 0.14285714285714285}]",随机化算法,{},[]
3673,3673,randomized smoothing,随机平滑化,0.0,7,"[{'word': '随机平滑', 'ratio': 0.5714285714285714}, {'word': '随机化平滑', 'ratio': 0.42857142857142855}]",随机平滑,{},[]
3674,3674,range query,范围查询,0.7142857142857143,7,"[{'word': '范围查询', 'ratio': 0.7142857142857143}, {'word': '区间查询', 'ratio': 0.2857142857142857}]",范围查询,{},[]
3675,3675,rank-one update,秩一更新,0.4285714285714285,7,"[{'word': '一阶更新', 'ratio': 0.5714285714285714}, {'word': '秩一更新', 'ratio': 0.42857142857142855}]",一阶更新,{},[]
3676,3676,ranking algorithm,排序算法,0.2857142857142857,7,"[{'word': '排名算法', 'ratio': 0.7142857142857143}, {'word': '排序算法', 'ratio': 0.2857142857142857}]",排名算法,{},[]
3677,3677,ranking function,排名函数,0.7142857142857143,7,"[{'word': '排名函数', 'ratio': 0.7142857142857143}, {'word': '排序函数', 'ratio': 0.2857142857142857}]",排名函数,{},[]
3678,3678,ranking model,排名模型,0.7142857142857143,7,"[{'word': '排名模型', 'ratio': 0.7142857142857143}, {'word': '排序模型', 'ratio': 0.2857142857142857}]",排名模型,{},[]
3679,3679,reachable state,可达状态,1.0,9,"[{'word': '可达状态', 'ratio': 1.0}]",可达状态,{},[]
3680,3680,readout function,读出函数,0.7777777777777778,9,"[{'word': '读出函数', 'ratio': 0.7777777777777778}, {'word': '读取函数', 'ratio': 0.2222222222222222}]",读出函数,{},[]
3681,3681,receptive field,感受野,1.0,9,"[{'word': '感受野', 'ratio': 1.0}]",感受野,{},[]
3682,3682,recognition,识别,1.0,9,"[{'word': '识别', 'ratio': 1.0}]",识别,{},[]
3683,3683,recognition model,识别模型,1.0,10,"[{'word': '识别模型', 'ratio': 1.0}]",识别模型,{},[]
3684,3684,recommendation algorithm,推荐算法,1.0,10,"[{'word': '推荐算法', 'ratio': 1.0}]",推荐算法,{},[]
3685,3685,recommendation model,推荐模型,1.0,10,"[{'word': '推荐模型', 'ratio': 1.0}]",推荐模型,{},[]
3686,3686,recommendation system,推荐系统,1.0,10,"[{'word': '推荐系统', 'ratio': 1.0}]",推荐系统,{},[]
3687,3687,recommender,推荐人,0.4,10,"[{'word': '推荐者', 'ratio': 0.6}, {'word': '推荐人', 'ratio': 0.4}]",推荐者,{},[]
3688,3688,recommender system,推荐系统,1.0,7,"[{'word': '推荐系统', 'ratio': 1.0}]",推荐系统,{},[]
3689,3689,reconstruction algorithm,重建算法,1.0,7,"[{'word': '重建算法', 'ratio': 1.0}]",重建算法,{},[]
3690,3690,reconstruction error,重构误差,0.1428571428571428,7,"[{'word': '重建误差', 'ratio': 0.8571428571428571}, {'word': '重构误差', 'ratio': 0.14285714285714285}]",重建误差,{},[]
3691,3691,reconstruction loss,重建损失,1.0,7,"[{'word': '重建损失', 'ratio': 1.0}]",重建损失,{},[]
3692,3692,recovery algorithm,恢复算法,1.0,7,"[{'word': '恢复算法', 'ratio': 1.0}]",恢复算法,{},[]
3693,3693,rectified linear unit,整流线性单元,0.1111111111111111,9,"[{'word': '修正线性单元', 'ratio': 0.6666666666666666}, {'word': '线性整流单元', 'ratio': 0.2222222222222222}, {'word': '整流线性单元', 'ratio': 0.1111111111111111}]",修正线性单元,{},[]
3694,3694,rectified stereo pair,校正立体对,0.1111111111111111,9,"[{'word': '修正立体对', 'ratio': 0.5555555555555556}, {'word': '校正立体对', 'ratio': 0.1111111111111111}, {'word': '纠正立体对', 'ratio': 0.1111111111111111}, {'word': '校正立体图像对', 'ratio': 0.1111111111111111}, {'word': '校正立体声对', 'ratio': 0.1111111111111111}]",修正立体对,{},[]
3695,3695,recurrent,循环的,0.8888888888888888,9,"[{'word': '循环的', 'ratio': 0.8888888888888888}, {'word': '经常性的', 'ratio': 0.1111111111111111}]",循环的,{},[]
3696,3696,recurrent architecture,循环架构,0.8888888888888888,9,"[{'word': '循环架构', 'ratio': 0.8888888888888888}, {'word': '循环的', 'ratio': 0.1111111111111111}]",循环架构,{},[]
3697,3697,recurrent autoencoder,循环自编码器,0.8888888888888888,9,"[{'word': '循环自编码器', 'ratio': 0.8888888888888888}, {'word': '循环自动编码器', 'ratio': 0.1111111111111111}]",循环自编码器,{},[]
3698,3698,recurrent connection,循环连接,0.875,8,"[{'word': '循环连接', 'ratio': 0.875}, {'word': '递归连接', 'ratio': 0.125}]",循环连接,{},[]
3699,3699,recurrent dynamic,循环动力学,0.125,8,"[{'word': '循环动态', 'ratio': 0.75}, {'word': '递归动态', 'ratio': 0.125}, {'word': '循环动力学', 'ratio': 0.125}]",循环动态,{},[]
3700,3700,recurrent model,循环模型,0.875,8,"[{'word': '循环模型', 'ratio': 0.875}, {'word': '递归模型', 'ratio': 0.125}]",循环模型,{},[]
3701,3701,recurrent network,循环网络,0.75,8,"[{'word': '循环网络', 'ratio': 0.75}, {'word': '递归网络', 'ratio': 0.125}, {'word': '循环神经网络', 'ratio': 0.125}]",循环网络,{},[]
3702,3702,recurrent state,循环状态,0.875,8,"[{'word': '循环状态', 'ratio': 0.875}, {'word': '递归状态', 'ratio': 0.125}]",循环状态,{},[]
3703,3703,recursion,递归,1.0,5,"[{'word': '递归', 'ratio': 1.0}]",递归,{},[]
3704,3704,recursive call,递归调用,1.0,5,"[{'word': '递归调用', 'ratio': 1.0}]",递归调用,{},[]
3705,3705,recursive neural model,递归神经模型,1.0,5,"[{'word': '递归神经模型', 'ratio': 1.0}]",递归神经模型,{},[]
3706,3706,recursive neural network,递归神经网络,1.0,5,"[{'word': '递归神经网络', 'ratio': 1.0}]",递归神经网络,{},[]
3707,3707,reference distribution,参考分布,1.0,5,"[{'word': '参考分布', 'ratio': 1.0}]",参考分布,{},[]
3708,3708,reference resolution,指代解析,0.1,10,"[{'word': '参考解析', 'ratio': 0.5}, {'word': '指代消解', 'ratio': 0.3}, {'word': '指代解析', 'ratio': 0.1}, {'word': '指代消解指代消解', 'ratio': 0.1}]",参考解析,{},[]
3709,3709,reference text,参考文本,1.0,10,"[{'word': '参考文本', 'ratio': 1.0}]",参考文本,{},[]
3710,3710,reference-based metric,基于参考的度量,1.0,10,"[{'word': '基于参考的度量', 'ratio': 1.0}]",基于参考的度量,{},[]
3711,3711,refinement network,细化网络,0.4,10,"[{'word': '精细化网络', 'ratio': 0.5}, {'word': '细化网络', 'ratio': 0.4}, {'word': '精化网络', 'ratio': 0.1}]",精细化网络,{},[]
3712,3712,regression analysis,回归分析,1.0,10,"[{'word': '回归分析', 'ratio': 1.0}]",回归分析,{},[]
3713,3713,regression coefficient,回归系数,1.0,10,"[{'word': '回归系数', 'ratio': 1.0}]",回归系数,{},[]
3714,3714,regression function,回归函数,1.0,10,"[{'word': '回归函数', 'ratio': 1.0}]",回归函数,{},[]
3715,3715,regression model,回归模型,1.0,10,"[{'word': '回归模型', 'ratio': 1.0}]",回归模型,{},[]
3716,3716,regression problem,回归问题,1.0,10,"[{'word': '回归问题', 'ratio': 1.0}]",回归问题,{},[]
3717,3717,regression task,回归任务,1.0,6,"[{'word': '回归任务', 'ratio': 1.0}]",回归任务,{},[]
3718,3718,regression tree,回归树,1.0,6,"[{'word': '回归树', 'ratio': 1.0}]",回归树,{},[]
3719,3719,regret bound,遗憾约束,0.0,6,"[{'word': '遗憾界', 'ratio': 0.5}, {'word': '遗憾界限', 'ratio': 0.3333333333333333}, {'word': '后悔界限', 'ratio': 0.16666666666666666}]",遗憾界,{},[]
3720,3720,regret matching,后悔匹配,0.1666666666666666,6,"[{'word': '遗憾匹配', 'ratio': 0.8333333333333334}, {'word': '后悔匹配', 'ratio': 0.16666666666666666}]",遗憾匹配,{},[]
3721,3721,regret minimization,后悔最小化,0.1666666666666666,6,"[{'word': '遗憾最小化', 'ratio': 0.8333333333333334}, {'word': '后悔最小化', 'ratio': 0.16666666666666666}]",遗憾最小化,{},[]
3722,3722,regret minimization algorithm,后悔最小化算法,0.5,6,"[{'word': '后悔最小化算法', 'ratio': 0.5}, {'word': '遗憾最小化算法', 'ratio': 0.5}]",后悔最小化算法,{},[]
3723,3723,regret minimizer,后悔最小化,0.0,6,"[{'word': '后悔最小化器', 'ratio': 0.5}, {'word': '遗憾最小化器', 'ratio': 0.5}]",后悔最小化器,{},[]
3724,3724,regular expression,正则表达式,1.0,6,"[{'word': '正则表达式', 'ratio': 1.0}]",正则表达式,{},[]
3725,3725,regularisation,正则化,1.0,6,"[{'word': '正则化', 'ratio': 1.0}]",正则化,{},[]
3726,3726,regularization,正则化,1.0,6,"[{'word': '正则化', 'ratio': 1.0}]",正则化,{},[]
3727,3727,regularization constant,正则化常数,1.0,7,"[{'word': '正则化常数', 'ratio': 1.0}]",正则化常数,{},[]
3728,3728,regularization function,正则化函数,1.0,7,"[{'word': '正则化函数', 'ratio': 1.0}]",正则化函数,{},[]
3729,3729,regularization loss,正则化损失,1.0,7,"[{'word': '正则化损失', 'ratio': 1.0}]",正则化损失,{},[]
3730,3730,regularization parameter,正则化参数,1.0,7,"[{'word': '正则化参数', 'ratio': 1.0}]",正则化参数,{},[]
3731,3731,regularization path,正则化路径,1.0,7,"[{'word': '正则化路径', 'ratio': 1.0}]",正则化路径,{},[]
3732,3732,regularization penalty,正则化惩罚,0.8888888888888888,9,"[{'word': '正则化惩罚', 'ratio': 0.8888888888888888}, {'word': '正则化惩罚项', 'ratio': 0.1111111111111111}]",正则化惩罚,{},[]
3733,3733,regularization strength,正则化强度,1.0,9,"[{'word': '正则化强度', 'ratio': 1.0}]",正则化强度,{},[]
3734,3734,regularization term,正则化项,1.0,9,"[{'word': '正则化项', 'ratio': 1.0}]",正则化项,{},[]
3735,3735,regularization weight,正则化权重,1.0,9,"[{'word': '正则化权重', 'ratio': 1.0}]",正则化权重,{},[]
3736,3736,regularization-based method,基于正则化的方法,1.0,9,"[{'word': '基于正则化的方法', 'ratio': 1.0}]",基于正则化的方法,{},[]
3737,3737,regularizer,正则化项,0.7142857142857143,7,"[{'word': '正则化项', 'ratio': 0.7142857142857143}, {'word': '正则化器', 'ratio': 0.2857142857142857}]",正则化项,{},[]
3738,3738,reinforcement learning algorithm,强化学习算法,1.0,7,"[{'word': '强化学习算法', 'ratio': 1.0}]",强化学习算法,{},[]
3739,3739,relation extraction,关系抽取,0.5714285714285714,7,"[{'word': '关系抽取', 'ratio': 0.5714285714285714}, {'word': '关系提取', 'ratio': 0.42857142857142855}]",关系抽取,{},[]
3740,3740,relation type,关系类型,1.0,7,"[{'word': '关系类型', 'ratio': 1.0}]",关系类型,{},[]
3741,3741,relational tuple,关系元组,1.0,8,"[{'word': '关系元组', 'ratio': 1.0}]",关系元组,{},[]
3742,3742,relative entropy,相对熵,1.0,8,"[{'word': '相对熵', 'ratio': 1.0}]",相对熵,{},[]
3743,3743,relative positional embedding,相对位置嵌入,1.0,8,"[{'word': '相对位置嵌入', 'ratio': 1.0}]",相对位置嵌入,{},[]
3744,3744,relevance score,相关性评分,0.125,8,"[{'word': '相关性得分', 'ratio': 0.875}, {'word': '相关性评分', 'ratio': 0.125}]",相关性得分,{},[]
3745,3745,rendering network,渲染网络,1.0,7,"[{'word': '渲染网络', 'ratio': 1.0}]",渲染网络,{},[]
3746,3746,renormalization,重整化,0.0,7,"[{'word': '重新归一化', 'ratio': 0.7142857142857143}, {'word': '重新标准化', 'ratio': 0.14285714285714285}, {'word': '重正规化', 'ratio': 0.14285714285714285}]",重新归一化,{},[]
3747,3747,reparameterization,重参数化,0.1428571428571428,7,"[{'word': '重新参数化', 'ratio': 0.8571428571428571}, {'word': '重参数化', 'ratio': 0.14285714285714285}]",重新参数化,{},[]
3748,3748,reparameterization trick,重参数化技巧,0.1428571428571428,7,"[{'word': '重新参数化技巧', 'ratio': 0.8571428571428571}, {'word': '重参数化技巧', 'ratio': 0.14285714285714285}]",重新参数化技巧,{},[]
3749,3749,replay buffer,经验回放缓冲区,0.2857142857142857,7,"[{'word': '重放缓冲区', 'ratio': 0.5714285714285714}, {'word': '经验回放缓冲区', 'ratio': 0.2857142857142857}, {'word': '回放缓冲区', 'ratio': 0.14285714285714285}]",重放缓冲区,{},[]
3750,3750,replay memory,重播记忆,0.0,6,"[{'word': '重放记忆', 'ratio': 0.3333333333333333}, {'word': '回放记忆', 'ratio': 0.3333333333333333}, {'word': '经验回放', 'ratio': 0.16666666666666666}, {'word': '经验回放存储器', 'ratio': 0.16666666666666666}]",经验回放,"1. Rank: 经验回放, 回放记忆, 重放记忆, 经验回放存储器

2. Explanation: The term ""经验回放"" (Experience replay) is the best fit for the AI domain, particularly in the context of Deep Q-Networks (DQN). This term is widely recognized in the machine learning community and accurately conveys the concept of storing and reusing past experiences to improve learning efficiency. The back translation ""Experience replay"" aligns perfectly with the established terminology in English literature, making it semantically accurate and contextually appropriate.

""回放记忆"" (Replay memory) and ""重放记忆"" (replay memory) are also valid translations, but they are less commonly used in the AI field compared to ""经验回放."" They may not convey the same level of specificity regarding the learning process involved. 

""经验回放存储器"" (experience replay memory) is more verbose and while it is accurate, it is not as commonly used in practice. The term ""存储器"" (memory) adds unnecessary complexity to the term, which is typically referred to simply as ""经验回放"" in the literature. Thus, ""经验回放"" stands out as the most appropriate choice for its semantic accuracy and contextual fit in the AI domain.","['replay memory', 'Replay memory', 'Experience replay', 'experience replay memory']"
3751,3751,representation,表示,0.5,12,"[{'word': '表示', 'ratio': 0.9166666666666666}, {'word': '表征', 'ratio': 0.08333333333333333}]",表示,{},[]
3752,3752,representation learning,表征学习,0.1666666666666666,6,"[{'word': '表示学习', 'ratio': 0.8333333333333334}, {'word': '表征学习', 'ratio': 0.16666666666666666}]",表示学习,{},[]
3753,3753,representation matrix,表示矩阵,1.0,6,"[{'word': '表示矩阵', 'ratio': 1.0}]",表示矩阵,{},[]
3754,3754,representation space,表征空间,0.0,6,"[{'word': '表示空间', 'ratio': 1.0}]",表示空间,{},[]
3755,3755,representation vector,表征向量,0.0,6,"[{'word': '表示向量', 'ratio': 1.0}]",表示向量,{},[]
3756,3756,representer theorem,再现定理,0.0,6,"[{'word': '表示定理', 'ratio': 0.6666666666666666}, {'word': '代表定理', 'ratio': 0.16666666666666666}, {'word': '表示者定理', 'ratio': 0.16666666666666666}]",表示定理,{},[]
3757,3757,reproducing kernel Hilbert space,再生核希尔伯特空间,0.8333333333333334,6,"[{'word': '再生核希尔伯特空间', 'ratio': 0.8333333333333334}, {'word': '重生核希尔伯特空间', 'ratio': 0.16666666666666666}]",再生核希尔伯特空间,{},[]
3758,3758,reproducing property,再现性质,0.0,8,"[{'word': '再生性质', 'ratio': 0.25}, {'word': '复制性质', 'ratio': 0.25}, {'word': '复现性质', 'ratio': 0.25}, {'word': '重现性属性', 'ratio': 0.125}, {'word': '重现性质', 'ratio': 0.125}]",再生性质,"1. Rank: 再生性质, 复现性质, 重现性质, 复制性质, 重现性属性

2. Explanation: The term ""再生性质"" (reproducing property) is the best fit for the AI domain because it accurately captures the concept of a property that allows functions to be evaluated at certain points through inner products in a reproducing kernel Hilbert space (RKHS). The back translation ""Regenerative properties"" aligns closely with the original meaning, emphasizing the idea of regeneration or reproduction of values. 

The second choice, ""复现性质"" (recurring properties), also conveys a sense of reproduction but lacks the specific connotation of the mathematical context found in ""再生性质."" ""重现性质"" (reproducible properties) is similar but may imply a broader context of reproducibility rather than the specific mathematical property. 

""复制性质"" (copy nature) and ""重现性属性"" (reproducibility properties) are less suitable as they do not convey the precise mathematical implications of the reproducing property in the context of RKHS, which is crucial for understanding the underlying concepts in AI and machine learning. Thus, ""再生性质"" is the most contextually accurate and semantically appropriate choice.","['Regenerative properties', 'Copy nature', 'recurring properties', 'Reproducibility properties', 'Reproducible properties']"
3759,3759,reprojection,重投影,0.125,8,"[{'word': '重新投影', 'ratio': 0.875}, {'word': '重投影', 'ratio': 0.125}]",重新投影,{},[]
3760,3760,reprojection error,重投影误差,0.125,8,"[{'word': '重新投影误差', 'ratio': 0.875}, {'word': '重投影误差', 'ratio': 0.125}]",重新投影误差,{},[]
3761,3761,reranker,重排序器 (reranker),0.0,8,"[{'word': '重新排序器', 'ratio': 0.875}, {'word': '重排序器', 'ratio': 0.125}]",重新排序器,{},[]
3762,3762,reranking model,重排序模型,0.125,8,"[{'word': '重新排序模型', 'ratio': 0.875}, {'word': '重排序模型', 'ratio': 0.125}]",重新排序模型,{},[]
3763,3763,reranking parser,重排序分析器,0.0,6,"[{'word': '重新排序解析器', 'ratio': 0.6666666666666666}, {'word': '重排序解析器', 'ratio': 0.3333333333333333}]",重新排序解析器,{},[]
3764,3764,reservoir sampling,蓄水池采样,0.0,6,"[{'word': '蓄水池抽样', 'ratio': 0.3333333333333333}, {'word': '储存抽样', 'ratio': 0.16666666666666666}, {'word': '储备抽样', 'ratio': 0.16666666666666666}, {'word': '源抽样', 'ratio': 0.16666666666666666}, {'word': '水库采样', 'ratio': 0.16666666666666666}]",蓄水池抽样,"1. Rank: 蓄水池抽样, 水库采样, 储备抽样, 储存抽样, 源抽样

2. Explanation: The term ""蓄水池抽样"" (Reservoir sampling) is the most accurate translation for the AI domain context. This term directly translates to ""reservoir sampling,"" which is a well-established concept in computer science and statistics, particularly in the context of sampling algorithms. The term ""蓄水池"" (reservoir) is commonly used in Chinese literature to refer to the concept of a reservoir in a statistical sense, making it semantically accurate and contextually appropriate.

The second candidate, ""水库采样,"" while also translating to ""reservoir sampling,"" uses ""水库,"" which is more commonly associated with physical water reservoirs rather than the abstract concept of sampling in statistics. This could lead to confusion in an AI context.

The other candidates, ""储备抽样"" (reserve sampling), ""储存抽样"" (storage sampling), and ""源抽样"" (source sampling), do not accurately convey the specific meaning of reservoir sampling in the context of sampling algorithms. They introduce ambiguity and do not align with the established terminology used in the AI and machine learning fields. Therefore, ""蓄水池抽样"" is the best fit for its semantic accuracy and contextual relevance.","['Reservoir sampling', 'storage sampling', 'reserve sampling', 'source sampling', 'Reservoir sampling']"
3765,3765,residual block,残差块,1.0,6,"[{'word': '残差块', 'ratio': 1.0}]",残差块,{},[]
3766,3766,residual branch,残差分支,1.0,6,"[{'word': '残差分支', 'ratio': 1.0}]",残差分支,{},[]
3767,3767,residual connection,残差连接,0.9,10,"[{'word': '残差连接', 'ratio': 0.9}, {'word': '剩余连接', 'ratio': 0.1}]",残差连接,{},[]
3768,3768,residual error,剩余误差,0.0,10,"[{'word': '残差误差', 'ratio': 0.8}, {'word': '差误差', 'ratio': 0.1}, {'word': '残差', 'ratio': 0.1}]",残差误差,{},[]
3769,3769,residual function,残差函数,1.0,10,"[{'word': '残差函数', 'ratio': 1.0}]",残差函数,{},[]
3770,3770,residual graph,残差图,0.8,10,"[{'word': '残差图', 'ratio': 0.8}, {'word': '残余图', 'ratio': 0.2}]",残差图,{},[]
3771,3771,residual learning,残差学习,0.9,10,"[{'word': '残差学习', 'ratio': 0.9}, {'word': '剩余学习', 'ratio': 0.1}]",残差学习,{},[]
3772,3772,residual network,残差网络,1.0,5,"[{'word': '残差网络', 'ratio': 1.0}]",残差网络,{},[]
3773,3773,restricted isometry property,限制等距性质,0.4,5,"[{'word': '限制等距性质', 'ratio': 0.4}, {'word': '受限等距性质', 'ratio': 0.4}, {'word': '受限等距性性质', 'ratio': 0.2}]",限制等距性质,"1. Rank: 限制等距性质, 受限等距性质, 受限等距性性质

2. Explanation: The term ""限制等距性质"" (restricted isometry property) is the best fit because it accurately captures the meaning of the original English term while maintaining semantic clarity. The word ""限制"" (restricted) directly translates to ""restricted,"" and ""等距"" (isometry) is a standard term in mathematics and AI that refers to distance-preserving transformations. The term ""性质"" (property) is also commonly used in mathematical contexts. 

The second candidate, ""受限等距性质,"" is also a valid translation, but ""受限"" (restricted) is less commonly used in this specific mathematical context compared to ""限制."" The third candidate, ""受限等距性性质,"" introduces redundancy with ""性质"" and ""性,"" which can lead to confusion and is less concise. Therefore, ""限制等距性质"" is the most semantically accurate and contextually appropriate choice in the AI domain.","['restricted isometric property', 'restricted isometric property', 'restricted isometric properties']"
3774,3774,retrieval,检索,0.5,10,"[{'word': '检索', 'ratio': 1.0}]",检索,{},[]
3775,3775,retrieval function,检索函数,1.0,8,"[{'word': '检索函数', 'ratio': 1.0}]",检索函数,{},[]
3776,3776,retrieval method,检索方法,1.0,8,"[{'word': '检索方法', 'ratio': 1.0}]",检索方法,{},[]
3777,3777,retrieval model,检索模型,1.0,8,"[{'word': '检索模型', 'ratio': 1.0}]",检索模型,{},[]
3778,3778,retrieval system,检索系统,1.0,8,"[{'word': '检索系统', 'ratio': 1.0}]",检索系统,{},[]
3779,3779,reverse-mode,逆向模式,0.0,7,"[{'word': '反向模式', 'ratio': 1.0}]",反向模式,{},[]
3780,3780,reward,奖励,1.0,7,"[{'word': '奖励', 'ratio': 1.0}]",奖励,{},[]
3781,3781,reward model,奖励模型,1.0,7,"[{'word': '奖励模型', 'ratio': 1.0}]",奖励模型,{},[]
3782,3782,reward shaping,奖励塑形,0.4285714285714285,7,"[{'word': '奖励塑造', 'ratio': 0.5714285714285714}, {'word': '奖励塑形', 'ratio': 0.42857142857142855}]",奖励塑造,{},[]
3783,3783,reward signal,奖赏信号,0.0,7,"[{'word': '奖励信号', 'ratio': 1.0}]",奖励信号,{},[]
3784,3784,reward-maximizing policy,奖励最大化策略,1.0,6,"[{'word': '奖励最大化策略', 'ratio': 1.0}]",奖励最大化策略,{},[]
3785,3785,ridge regularization,岭正则化,0.3333333333333333,6,"[{'word': '岭回归正则化', 'ratio': 0.5}, {'word': '岭正则化', 'ratio': 0.3333333333333333}, {'word': '岭回归', 'ratio': 0.16666666666666666}]",岭回归正则化,{},[]
3786,3786,right-to-left model,从右到左模型,0.3333333333333333,6,"[{'word': '右到左模型', 'ratio': 0.5}, {'word': '从右到左模型', 'ratio': 0.3333333333333333}, {'word': '右向左模型', 'ratio': 0.16666666666666666}]",右到左模型,{},[]
3787,3787,rigid body transformation,刚体变换,1.0,6,"[{'word': '刚体变换', 'ratio': 1.0}]",刚体变换,{},[]
3788,3788,rigid transformation,刚体变换,0.0,6,"[{'word': '刚性变换', 'ratio': 1.0}]",刚性变换,{},[]
3789,3789,risk minimization,风险最小化,1.0,5,"[{'word': '风险最小化', 'ratio': 1.0}]",风险最小化,{},[]
3790,3790,roberta-large,roberta-large,0.4,5,"[{'word': 'roberta-large', 'ratio': 0.4}, {'word': '羅伯塔大', 'ratio': 0.2}, {'word': '罗伯塔大模型', 'ratio': 0.2}, {'word': 'RoBERTa大型模型', 'ratio': 0.2}]","""roberta-large""","1. Rank: ""roberta-large"", ""RoBERTa大型模型"", ""罗伯塔大模型"", ""羅伯塔大""

2. Explanation: The first translation candidate, ""roberta-large"", is the best fit because it retains the original English term, which is crucial in the AI domain where specific model names are widely recognized and used in their original form. This is particularly important for technical accuracy and consistency, as ""roberta-large"" refers to a specific pretrained model in the RoBERTa family. 

The second candidate, ""RoBERTa大型模型"", is also a strong choice as it includes the original name ""RoBERTa"" and adds ""大型模型"" (large model), which accurately describes the model's size. This maintains semantic accuracy while providing context.

The third candidate, ""罗伯塔大模型"", translates ""roberta"" to ""罗伯塔"" and uses ""大模型"" (big model), which is acceptable but less precise than the second option since it does not retain the original capitalization of ""RoBERTa"".

The last candidate, ""羅伯塔大"", is the least effective as it omits the term ""模型"" (model) entirely, which is essential for clarity in the AI context. It also uses a traditional character form that may not be as widely recognized in the context of modern AI terminology.","['Roberta-large', 'Roberta Big', 'Roberta big model', 'RoBERTa large model']"
3791,3791,robotic,机器人,0.2,5,"[{'word': '机器人技术的', 'ratio': 0.8}, {'word': '机器人', 'ratio': 0.2}]",机器人技术的,{},[]
3792,3792,robust optimization,鲁棒优化,1.0,5,"[{'word': '鲁棒优化', 'ratio': 1.0}]",鲁棒优化,{},[]
3793,3793,robust risk,鲁棒风险,0.8,5,"[{'word': '鲁棒风险', 'ratio': 0.8}, {'word': '鲁棒风险 如果还有其他需要帮助的地方，请随时告诉我！', 'ratio': 0.2}]",鲁棒风险,{},[]
3794,3794,robustness,鲁棒性,0.9,10,"[{'word': '鲁棒性', 'ratio': 0.9}, {'word': '稳健性', 'ratio': 0.1}]",鲁棒性,{},[]
3795,3795,role assertion,角色断言,1.0,5,"[{'word': '角色断言', 'ratio': 1.0}]",角色断言,{},[]
3796,3796,role atom,角色原子,1.0,5,"[{'word': '角色原子', 'ratio': 1.0}]",角色原子,{},[]
3797,3797,role classification,角色分类,1.0,5,"[{'word': '角色分类', 'ratio': 1.0}]",角色分类,{},[]
3798,3798,role name,角色名称,0.7142857142857143,7,"[{'word': '角色名称', 'ratio': 0.7142857142857143}, {'word': '角色名', 'ratio': 0.2857142857142857}]",角色名称,{},[]
3799,3799,roll-out policy,展开策略,0.5714285714285714,7,"[{'word': '展开策略', 'ratio': 0.5714285714285714}, {'word': '执行策略', 'ratio': 0.14285714285714285}, {'word': '回放策略', 'ratio': 0.14285714285714285}, {'word': '回滚策略', 'ratio': 0.14285714285714285}]",展开策略,{},[]
3800,3800,rollout,滚动,0.0,7,"[{'word': '展开', 'ratio': 0.42857142857142855}, {'word': '执行过程', 'ratio': 0.14285714285714285}, {'word': '回放', 'ratio': 0.14285714285714285}, {'word': '回滚', 'ratio': 0.14285714285714285}, {'word': '展开序列', 'ratio': 0.14285714285714285}]","""展开序列""","1. Rank: ""展开序列"", ""展开"", ""执行过程"", ""回放"", ""回滚""

2. Explanation: The term ""展开序列"" (unfold sequence) is the best fit for the AI context of ""rollout"" because it accurately captures the concept of generating a sequence of states, actions, and rewards over time, which is essential in reinforcement learning. The term ""展开"" (expand) is also relevant but is more general and does not specifically imply the sequential aspect that ""展开序列"" conveys. ""执行过程"" (execution process) is too broad and does not specifically relate to the concept of a rollout in reinforcement learning. ""回放"" (playback) and ""回滚"" (rollback) are misleading in this context, as they suggest different processes that do not align with the intended meaning of ""rollout."" Therefore, ""展开序列"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['Expand', 'Execution process', 'Playback', 'rollback', 'unfold sequence']"
3801,3801,rollout length,滚动长度,0.0,7,"[{'word': '展开长度', 'ratio': 0.5714285714285714}, {'word': '执行长度', 'ratio': 0.14285714285714285}, {'word': '回放长度', 'ratio': 0.14285714285714285}, {'word': '回滚长度', 'ratio': 0.14285714285714285}]",展开长度,{},[]
3802,3802,root mean square error,均方根误差,1.0,7,"[{'word': '均方根误差', 'ratio': 1.0}]",均方根误差,{},[]
3803,3803,root node,根节点,1.0,8,"[{'word': '根节点', 'ratio': 1.0}]",根节点,{},[]
3804,3804,rotation angle,旋转角度,0.75,8,"[{'word': '旋转角度', 'ratio': 0.75}, {'word': '旋转角', 'ratio': 0.25}]",旋转角度,{},[]
3805,3805,rotation invariance,旋转不变性,1.0,8,"[{'word': '旋转不变性', 'ratio': 1.0}]",旋转不变性,{},[]
3806,3806,rotation matrix,旋转矩阵,1.0,8,"[{'word': '旋转矩阵', 'ratio': 1.0}]",旋转矩阵,{},[]
3807,3807,row vector,行向量,1.0,8,"[{'word': '行向量', 'ratio': 1.0}]",行向量,{},[]
3808,3808,rule body,规则体,0.4285714285714285,7,"[{'word': '规则主体', 'ratio': 0.5714285714285714}, {'word': '规则体', 'ratio': 0.42857142857142855}]",规则主体,{},[]
3809,3809,runtime complexity,运行时复杂度,1.0,7,"[{'word': '运行时复杂度', 'ratio': 1.0}]",运行时复杂度,{},[]
3810,3810,saddle-point problem,鞍点问题,1.0,7,"[{'word': '鞍点问题', 'ratio': 1.0}]",鞍点问题,{},[]
3811,3811,saliency,显著性,0.7142857142857143,7,"[{'word': '显著性', 'ratio': 0.7142857142857143}, {'word': '突出性', 'ratio': 0.2857142857142857}]",显著性,{},[]
3812,3812,saliency map,显著性地图,0.0,7,"[{'word': '显著性图', 'ratio': 0.7142857142857143}, {'word': '突出性图', 'ratio': 0.2857142857142857}]",显著性图,{},[]
3813,3813,sample complexity,样本复杂度,1.0,6,"[{'word': '样本复杂度', 'ratio': 1.0}]",样本复杂度,{},[]
3814,3814,sample complexity bound,样本复杂度界,0.6666666666666666,6,"[{'word': '样本复杂度界', 'ratio': 0.6666666666666666}, {'word': '样本复杂度界限', 'ratio': 0.3333333333333333}]",样本复杂度界,{},[]
3815,3815,sample covariance matrix,样本协方差矩阵,1.0,6,"[{'word': '样本协方差矩阵', 'ratio': 1.0}]",样本协方差矩阵,{},[]
3816,3816,sample efficiency,样本效率,1.0,6,"[{'word': '样本效率', 'ratio': 1.0}]",样本效率,{},[]
3817,3817,sample selection,样本选择,1.0,6,"[{'word': '样本选择', 'ratio': 1.0}]",样本选择,{},[]
3818,3818,sample space,样本空间,1.0,8,"[{'word': '样本空间', 'ratio': 1.0}]",样本空间,{},[]
3819,3819,sample variance,样本方差,1.0,8,"[{'word': '样本方差', 'ratio': 1.0}]",样本方差,{},[]
3820,3820,sample-efficient,样本高效,0.375,8,"[{'word': '样本高效', 'ratio': 0.375}, {'word': '样本效率高', 'ratio': 0.375}, {'word': '样本高效的', 'ratio': 0.125}, {'word': '样品效率高', 'ratio': 0.125}]",样本高效,"1. Rank: 样本高效, 样本高效的, 样本效率高, 样品效率高

2. Explanation: The term ""样本高效"" is the best fit because it directly translates to ""sample-efficient,"" maintaining the original meaning and context of the English term. In the AI domain, ""sample-efficient"" refers to algorithms or methods that require fewer samples to achieve a certain level of performance, which is accurately captured by ""样本高效."" The back translation ""Sample efficient"" confirms its semantic accuracy. 

The second candidate, ""样本高效的,"" is also a good fit as it retains the meaning but adds a grammatical particle that makes it slightly less direct than the first option. 

""样本效率高"" translates to ""high sample efficiency,"" which, while related, shifts the focus from the efficiency of the sampling process itself to a more general statement about efficiency, making it less precise in the context of AI. 

Lastly, ""样品效率高"" translates to ""high sample efficiency"" as well, but the use of ""样品"" (sample) instead of ""样本"" (data sample) is less appropriate in the AI context, as ""样品"" typically refers to physical samples rather than data samples used in machine learning. Thus, it ranks the lowest.","['Sample efficient', 'High sample efficiency', 'sample efficient', 'High sample efficiency']"
3821,3821,sampler,采样器,1.0,8,"[{'word': '采样器', 'ratio': 1.0}]",采样器,{},[]
3822,3822,sampling algorithm,抽样算法,0.0,8,"[{'word': '采样算法', 'ratio': 1.0}]",采样算法,{},[]
3823,3823,sampling-based inference,基于采样的推理,0.25,8,"[{'word': '基于采样的推断', 'ratio': 0.625}, {'word': '基于采样的推理', 'ratio': 0.25}, {'word': '基于采样的推断基', 'ratio': 0.125}]",基于采样的推断,{},[]
3824,3824,satisfiability,可满足性,1.0,8,"[{'word': '可满足性', 'ratio': 1.0}]",可满足性,{},[]
3825,3825,satisfiability problem,可满足性问题,1.0,8,"[{'word': '可满足性问题', 'ratio': 1.0}]",可满足性问题,{},[]
3826,3826,scalability,可扩展性,1.0,8,"[{'word': '可扩展性', 'ratio': 1.0}]",可扩展性,{},[]
3827,3827,scalar,标量,0.875,8,"[{'word': '标量', 'ratio': 0.875}, {'word': '标量 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.125}]",标量,{},[]
3828,3828,scalar product,标量积,1.0,7,"[{'word': '标量积', 'ratio': 1.0}]",标量积,{},[]
3829,3829,scalarization,标量化,1.0,7,"[{'word': '标量化', 'ratio': 1.0}]",标量化,{},[]
3830,3830,scale invariance,尺度不变性,0.8571428571428571,7,"[{'word': '尺度不变性', 'ratio': 0.8571428571428571}, {'word': '标量化', 'ratio': 0.14285714285714285}]",尺度不变性,{},[]
3831,3831,scale parameter,尺度参数,0.8571428571428571,7,"[{'word': '尺度参数', 'ratio': 0.8571428571428571}, {'word': '尺度不变参数', 'ratio': 0.14285714285714285}]",尺度参数,{},[]
3832,3832,scaled dot-product,缩放点积,1.0,7,"[{'word': '缩放点积', 'ratio': 1.0}]",缩放点积,{},[]
3833,3833,scaled dot-product attention,缩放点积注意力,1.0,9,"[{'word': '缩放点积注意力', 'ratio': 1.0}]",缩放点积注意力,{},[]
3834,3834,scaling factor,缩放因子,0.8888888888888888,9,"[{'word': '缩放因子', 'ratio': 0.8888888888888888}, {'word': '比例因子', 'ratio': 0.1111111111111111}]",缩放因子,{},[]
3835,3835,scanning window detector,滑窗检测器,0.0,9,"[{'word': '扫描窗口检测器', 'ratio': 0.7777777777777778}, {'word': '滑动窗口检测器', 'ratio': 0.1111111111111111}, {'word': '扫描窗探测器', 'ratio': 0.1111111111111111}]",扫描窗口检测器,{},[]
3836,3836,scene category,场景类别,1.0,9,"[{'word': '场景类别', 'ratio': 1.0}]",场景类别,{},[]
3837,3837,scene classification,场景分类,1.0,9,"[{'word': '场景分类', 'ratio': 1.0}]",场景分类,{},[]
3838,3838,scene classifier,场景分类器,1.0,5,"[{'word': '场景分类器', 'ratio': 1.0}]",场景分类器,{},[]
3839,3839,scene flow,场景流,1.0,5,"[{'word': '场景流', 'ratio': 1.0}]",场景流,{},[]
3840,3840,scene flow estimation,场景流估计,1.0,5,"[{'word': '场景流估计', 'ratio': 1.0}]",场景流估计,{},[]
3841,3841,scene geometry,场景几何,1.0,5,"[{'word': '场景几何', 'ratio': 1.0}]",场景几何,{},[]
3842,3842,scene graph,场景图,1.0,5,"[{'word': '场景图', 'ratio': 1.0}]",场景图,{},[]
3843,3843,scene parsing,场景解析,1.0,6,"[{'word': '场景解析', 'ratio': 1.0}]",场景解析,{},[]
3844,3844,scene recognition,场景识别,1.0,6,"[{'word': '场景识别', 'ratio': 1.0}]",场景识别,{},[]
3845,3845,scene reconstruction,场景重建,1.0,6,"[{'word': '场景重建', 'ratio': 1.0}]",场景重建,{},[]
3846,3846,scene representation,场景表示,1.0,6,"[{'word': '场景表示', 'ratio': 1.0}]",场景表示,{},[]
3847,3847,scene understanding,场景理解,1.0,6,"[{'word': '场景理解', 'ratio': 1.0}]",场景理解,{},[]
3848,3848,scheduled sampling,定时采样,0.3333333333333333,6,"[{'word': '计划采样', 'ratio': 0.6666666666666666}, {'word': '定时采样', 'ratio': 0.3333333333333333}]",计划采样,{},[]
3849,3849,scheduler,调度器,1.0,6,"[{'word': '调度器', 'ratio': 1.0}]",调度器,{},[]
3850,3850,schema item,模式项,0.8333333333333334,6,"[{'word': '模式项', 'ratio': 0.8333333333333334}, {'word': '方案项', 'ratio': 0.16666666666666666}]",模式项,{},[]
3851,3851,score function,评分函数,1.0,6,"[{'word': '评分函数', 'ratio': 1.0}]",评分函数,{},[]
3852,3852,score matching,得分匹配,0.1666666666666666,6,"[{'word': '评分匹配', 'ratio': 0.5}, {'word': '分数匹配', 'ratio': 0.3333333333333333}, {'word': '得分匹配', 'ratio': 0.16666666666666666}]",评分匹配,{},[]
3853,3853,score vector,得分向量,0.5714285714285714,7,"[{'word': '得分向量', 'ratio': 0.5714285714285714}, {'word': '分数向量', 'ratio': 0.42857142857142855}]",得分向量,{},[]
3854,3854,score-based model,基于分数的模型,0.4285714285714285,7,"[{'word': '基于得分的模型', 'ratio': 0.5714285714285714}, {'word': '基于分数的模型', 'ratio': 0.42857142857142855}]",基于得分的模型,{},[]
3855,3855,scoring function,评分函数,1.0,7,"[{'word': '评分函数', 'ratio': 1.0}]",评分函数,{},[]
3856,3856,search algorithm,搜索算法,1.0,7,"[{'word': '搜索算法', 'ratio': 1.0}]",搜索算法,{},[]
3857,3857,search problem,搜索问题,1.0,9,"[{'word': '搜索问题', 'ratio': 1.0}]",搜索问题,{},[]
3858,3858,search procedure,搜索过程,1.0,9,"[{'word': '搜索过程', 'ratio': 1.0}]",搜索过程,{},[]
3859,3859,search space,搜索空间,1.0,9,"[{'word': '搜索空间', 'ratio': 1.0}]",搜索空间,{},[]
3860,3860,search tree,搜索树,1.0,9,"[{'word': '搜索树', 'ratio': 1.0}]",搜索树,{},[]
3861,3861,second order,二阶,1.0,9,"[{'word': '二阶', 'ratio': 1.0}]",二阶,{},[]
3862,3862,second order statistic,二阶统计量,1.0,10,"[{'word': '二阶统计量', 'ratio': 1.0}]",二阶统计量,{},[]
3863,3863,second-order optimization,二阶优化,1.0,10,"[{'word': '二阶优化', 'ratio': 1.0}]",二阶优化,{},[]
3864,3864,second-order potential,二阶势能,0.4,10,"[{'word': '二阶势', 'ratio': 0.5}, {'word': '二阶势能', 'ratio': 0.4}, {'word': '二阶优化', 'ratio': 0.1}]",二阶势,{},[]
3865,3865,segmentation,分割,1.0,10,"[{'word': '分割', 'ratio': 1.0}]",分割,{},[]
3866,3866,segmentation algorithm,分割算法,1.0,10,"[{'word': '分割算法', 'ratio': 1.0}]",分割算法,{},[]
3867,3867,segmentation map,分割图,1.0,5,"[{'word': '分割图', 'ratio': 1.0}]",分割图,{},[]
3868,3868,segmentation mask,分割掩码,0.8,5,"[{'word': '分割掩码', 'ratio': 0.8}, {'word': '分割掩膜', 'ratio': 0.2}]",分割掩码,{},[]
3869,3869,selection bias,选择偏差,0.8,5,"[{'word': '选择偏差', 'ratio': 0.8}, {'word': '选择偏倚', 'ratio': 0.2}]",选择偏差,{},[]
3870,3870,selectional preference,选择偏好,0.8,5,"[{'word': '选择偏好', 'ratio': 0.8}, {'word': '选择性偏好', 'ratio': 0.2}]",选择偏好,{},[]
3871,3871,self-attention head,自注意力头,0.8,5,"[{'word': '自注意力头', 'ratio': 0.8}, {'word': '选择偏好', 'ratio': 0.2}]",自注意力头,{},[]
3872,3872,self-attention layer,自注意力层,1.0,7,"[{'word': '自注意力层', 'ratio': 1.0}]",自注意力层,{},[]
3873,3873,self-attention matrix,自注意力矩阵,1.0,7,"[{'word': '自注意力矩阵', 'ratio': 1.0}]",自注意力矩阵,{},[]
3874,3874,self-attention mechanism,自注意力机制,1.0,7,"[{'word': '自注意力机制', 'ratio': 1.0}]",自注意力机制,{},[]
3875,3875,self-attention model,自注意力模型,0.8571428571428571,7,"[{'word': '自注意力模型', 'ratio': 0.8571428571428571}, {'word': '自注意力模块', 'ratio': 0.14285714285714285}]",自注意力模型,{},[]
3876,3876,self-attention module,自注意力模块,0.8571428571428571,7,"[{'word': '自注意力模块', 'ratio': 0.8571428571428571}, {'word': '自注意力模型', 'ratio': 0.14285714285714285}]",自注意力模块,{},[]
3877,3877,self-learning,自学习,0.875,8,"[{'word': '自学习', 'ratio': 0.875}, {'word': '自学', 'ratio': 0.125}]",自学习,{},[]
3878,3878,self-loop,自环,0.875,8,"[{'word': '自环', 'ratio': 0.875}, {'word': '自循环', 'ratio': 0.125}]",自环,{},[]
3879,3879,self-play,自我对弈,0.875,8,"[{'word': '自我对弈', 'ratio': 0.875}, {'word': '自玩', 'ratio': 0.125}]",自我对弈,{},[]
3880,3880,self-supervised method,自监督方法,0.875,8,"[{'word': '自监督方法', 'ratio': 0.875}, {'word': '自监督法', 'ratio': 0.125}]",自监督方法,{},[]
3881,3881,self-supervised model,自监督模型,1.0,8,"[{'word': '自监督模型', 'ratio': 1.0}]",自监督模型,{},[]
3882,3882,self-supervised representation learning,自监督表征学习,0.1428571428571428,7,"[{'word': '自监督表示学习', 'ratio': 0.8571428571428571}, {'word': '自监督表征学习', 'ratio': 0.14285714285714285}]",自监督表示学习,{},[]
3883,3883,self-supervised signal,自监督信号,1.0,7,"[{'word': '自监督信号', 'ratio': 1.0}]",自监督信号,{},[]
3884,3884,self-supervised training,自监督训练,0.8571428571428571,7,"[{'word': '自监督训练', 'ratio': 0.8571428571428571}, {'word': '自我监督训练', 'ratio': 0.14285714285714285}]",自监督训练,{},[]
3885,3885,self-supervision,自监督,0.8571428571428571,7,"[{'word': '自监督', 'ratio': 0.8571428571428571}, {'word': '自我监督', 'ratio': 0.14285714285714285}]",自监督,{},[]
3886,3886,self-training,自训练,0.7142857142857143,7,"[{'word': '自训练', 'ratio': 0.7142857142857143}, {'word': '自我训练', 'ratio': 0.2857142857142857}]",自训练,{},[]
3887,3887,semantic alignment,语义对齐,1.0,5,"[{'word': '语义对齐', 'ratio': 1.0}]",语义对齐,{},[]
3888,3888,semantic analysis,语义分析,1.0,5,"[{'word': '语义分析', 'ratio': 1.0}]",语义分析,{},[]
3889,3889,semantic annotation,语义注释,0.2,5,"[{'word': '语义标注', 'ratio': 0.8}, {'word': '语义注释', 'ratio': 0.2}]",语义标注,{},[]
3890,3890,semantic category,语义类别,1.0,5,"[{'word': '语义类别', 'ratio': 1.0}]",语义类别,{},[]
3891,3891,semantic class,语义类别,0.0,5,"[{'word': '语义类', 'ratio': 1.0}]",语义类,{},[]
3892,3892,semantic constraint,语义约束,1.0,9,"[{'word': '语义约束', 'ratio': 1.0}]",语义约束,{},[]
3893,3893,semantic distance,语义距离,1.0,9,"[{'word': '语义距离', 'ratio': 1.0}]",语义距离,{},[]
3894,3894,semantic encoder,语义编码器,1.0,9,"[{'word': '语义编码器', 'ratio': 1.0}]",语义编码器,{},[]
3895,3895,semantic equivalence,语义等价性,0.0,9,"[{'word': '语义等价', 'ratio': 0.8888888888888888}, {'word': '语义对等', 'ratio': 0.1111111111111111}]",语义等价,{},[]
3896,3896,semantic feature,语义特征,1.0,9,"[{'word': '语义特征', 'ratio': 1.0}]",语义特征,{},[]
3897,3897,semantic graph,语义图,1.0,5,"[{'word': '语义图', 'ratio': 1.0}]",语义图,{},[]
3898,3898,semantic information,语义信息,1.0,5,"[{'word': '语义信息', 'ratio': 1.0}]",语义信息,{},[]
3899,3899,semantic interpretation,语义解释,1.0,5,"[{'word': '语义解释', 'ratio': 1.0}]",语义解释,{},[]
3900,3900,semantic label,语义标签,1.0,5,"[{'word': '语义标签', 'ratio': 1.0}]",语义标签,{},[]
3901,3901,semantic memory,语义记忆,1.0,5,"[{'word': '语义记忆', 'ratio': 1.0}]",语义记忆,{},[]
3902,3902,semantic model,语义模型,1.0,7,"[{'word': '语义模型', 'ratio': 1.0}]",语义模型,{},[]
3903,3903,semantic network,语义网络,1.0,7,"[{'word': '语义网络', 'ratio': 1.0}]",语义网络,{},[]
3904,3904,semantic object,语义对象,1.0,7,"[{'word': '语义对象', 'ratio': 1.0}]",语义对象,{},[]
3905,3905,semantic operator,语义运算符,0.2857142857142857,7,"[{'word': '语义操作符', 'ratio': 0.42857142857142855}, {'word': '语义运算符', 'ratio': 0.2857142857142857}, {'word': '语义算子', 'ratio': 0.2857142857142857}]",语义操作符,"1. Rank: 语义操作符, 语义运算符, 语义算子

2. Explanation: The term ""语义操作符"" (semantic operators) is the best fit because it closely aligns with the established terminology used in the AI and computational linguistics fields. The term ""操作符"" (operator) is commonly used in programming and mathematical contexts, making it more recognizable and appropriate for the technical audience. 

""语义运算符"" (semantic operators) is also a valid option, but ""运算符"" (operation symbol) is less frequently used in the context of AI and may imply a broader mathematical operation rather than the specific function of a semantic operator. 

""语义算子"" (semantic operator) is less preferred because ""算子"" (operator) is often used in more abstract mathematical contexts and may not convey the same level of specificity as ""操作符"" in the context of AI. 

Overall, ""语义操作符"" provides the best semantic accuracy and contextual fit for the AI domain, ensuring clarity and precision in communication.","['semantic operators', 'semantic operators', 'semantic operator']"
3906,3906,semantic parse,语义分析,0.0,7,"[{'word': '语义解析', 'ratio': 0.8571428571428571}, {'word': '语义解析 如果您需要进一步的帮助，请告诉我！', 'ratio': 0.14285714285714285}]",语义解析,{},[]
3907,3907,semantic parser,语义解析器,0.8,5,"[{'word': '语义解析器', 'ratio': 0.8}, {'word': '语义分析器', 'ratio': 0.2}]",语义解析器,{},[]
3908,3908,semantic priming,语义启动,1.0,5,"[{'word': '语义启动', 'ratio': 1.0}]",语义启动,{},[]
3909,3909,semantic relation,语义关系,1.0,5,"[{'word': '语义关系', 'ratio': 1.0}]",语义关系,{},[]
3910,3910,semantic representation,语义表示,1.0,5,"[{'word': '语义表示', 'ratio': 1.0}]",语义表示,{},[]
3911,3911,semantic role,语义角色,1.0,5,"[{'word': '语义角色', 'ratio': 1.0}]",语义角色,{},[]
3912,3912,semantic role label,语义角色标注,0.1111111111111111,9,"[{'word': '语义角色标签', 'ratio': 0.8888888888888888}, {'word': '语义角色标注', 'ratio': 0.1111111111111111}]",语义角色标签,{},[]
3913,3913,semantic search,语义搜索,1.0,9,"[{'word': '语义搜索', 'ratio': 1.0}]",语义搜索,{},[]
3914,3914,semantic segmentation,语义分割,1.0,9,"[{'word': '语义分割', 'ratio': 1.0}]",语义分割,{},[]
3915,3915,semantic similarity,语义相似性,0.7777777777777778,9,"[{'word': '语义相似性', 'ratio': 0.7777777777777778}, {'word': '语义相似度', 'ratio': 0.2222222222222222}]",语义相似性,{},[]
3916,3916,semantic similarity measure,语义相似度度量,0.2222222222222222,9,"[{'word': '语义相似性度量', 'ratio': 0.7777777777777778}, {'word': '语义相似度度量', 'ratio': 0.2222222222222222}]",语义相似性度量,{},[]
3917,3917,semantic space,语义空间,1.0,6,"[{'word': '语义空间', 'ratio': 1.0}]",语义空间,{},[]
3918,3918,semantic structure,语义结构,1.0,6,"[{'word': '语义结构', 'ratio': 1.0}]",语义结构,{},[]
3919,3919,semantic symbol,语义符号,1.0,6,"[{'word': '语义符号', 'ratio': 1.0}]",语义符号,{},[]
3920,3920,semantic textual similarity,语义文本相似性,0.1666666666666666,6,"[{'word': '语义文本相似度', 'ratio': 0.8333333333333334}, {'word': '语义文本相似性', 'ratio': 0.16666666666666666}]",语义文本相似度,{},[]
3921,3921,semantic unit,语义单元,1.0,6,"[{'word': '语义单元', 'ratio': 1.0}]",语义单元,{},[]
3922,3922,semantic vector,语义向量,1.0,8,"[{'word': '语义向量', 'ratio': 1.0}]",语义向量,{},[]
3923,3923,semantic vector space,语义向量空间,1.0,8,"[{'word': '语义向量空间', 'ratio': 1.0}]",语义向量空间,{},[]
3924,3924,semi-Markov,半马尔科夫,0.0,8,"[{'word': '半马尔可夫', 'ratio': 1.0}]",半马尔可夫,{},[]
3925,3925,semi-definite programming,半正定规划,1.0,6,"[{'word': '半正定规划', 'ratio': 1.0}]",半正定规划,{},[]
3926,3926,semi-supervised clustering,半监督聚类,1.0,6,"[{'word': '半监督聚类', 'ratio': 1.0}]",半监督聚类,{},[]
3927,3927,semi-supervised learning,半监督学习,1.0,6,"[{'word': '半监督学习', 'ratio': 1.0}]",半监督学习,{},[]
3928,3928,semi-supervision,半监督,1.0,6,"[{'word': '半监督', 'ratio': 1.0}]",半监督,{},[]
3929,3929,semidefinite program,半正定规划,0.5,6,"[{'word': '半正定程序', 'ratio': 0.5}, {'word': '半正定规划', 'ratio': 0.5}]",半正定程序,{},[]
3930,3930,sense disambiguation,词义消歧,0.25,8,"[{'word': '意义消歧', 'ratio': 0.25}, {'word': '语义消歧', 'ratio': 0.25}, {'word': '词义消歧', 'ratio': 0.25}, {'word': '义项消歧', 'ratio': 0.125}, {'word': '词义', 'ratio': 0.125}]",语义消歧,"1. Rank: 语义消歧, 词义消歧, 意义消歧, 义项消歧, 词义

2. Explanation: The term ""语义消歧"" (semantic disambiguation) is the best fit for the context of AI and natural language processing. In this domain, ""语义"" (semantic) specifically refers to the meaning of words and phrases in context, which is crucial for understanding and processing language accurately. ""消歧"" (disambiguation) directly relates to the process of resolving ambiguities in meaning. 

While ""词义消歧"" (word sense disambiguation) is also a strong candidate, it is more specific to the disambiguation of individual words rather than the broader semantic context, which may limit its applicability in certain AI contexts. ""意义消歧"" (meaning disambiguation) and ""义项消歧"" (meaning item disambiguation) are less commonly used in the AI field and may not convey the same level of specificity as ""语义消歧."" Lastly, ""词义"" (meaning) is too vague and does not encompass the disambiguation aspect at all. Therefore, ""语义消歧"" is the most contextually appropriate and semantically accurate term for the AI domain.","['meaning disambiguation', 'semantic disambiguation', 'word sense disambiguation', 'meaning disambiguation', 'meaning']"
3931,3931,sensitive attribute,敏感属性,0.875,8,"[{'word': '敏感属性', 'ratio': 0.875}, {'word': '词义', 'ratio': 0.125}]",敏感属性,{},[]
3932,3932,sensitivity analysis,敏感性分析,1.0,8,"[{'word': '敏感性分析', 'ratio': 1.0}]",敏感性分析,{},[]
3933,3933,sentence classification,句子分类,1.0,9,"[{'word': '句子分类', 'ratio': 1.0}]",句子分类,{},[]
3934,3934,sentence compression,句子压缩,1.0,9,"[{'word': '句子压缩', 'ratio': 1.0}]",句子压缩,{},[]
3935,3935,sentence embedding,句向量,0.0,9,"[{'word': '句子嵌入', 'ratio': 1.0}]",句子嵌入,{},[]
3936,3936,sentence encoder,句子编码器,1.0,9,"[{'word': '句子编码器', 'ratio': 1.0}]",句子编码器,{},[]
3937,3937,sentence representation,句子表征,0.0,9,"[{'word': '句子表示', 'ratio': 1.0}]",句子表示,{},[]
3938,3938,sentence segmentation,句子分割,1.0,8,"[{'word': '句子分割', 'ratio': 1.0}]",句子分割,{},[]
3939,3939,sentence vector,句向量,0.375,8,"[{'word': '句子向量', 'ratio': 0.625}, {'word': '句向量', 'ratio': 0.375}]",句子向量,{},[]
3940,3940,sentence-level,句子级,0.25,8,"[{'word': '句子级别', 'ratio': 0.625}, {'word': '句子级', 'ratio': 0.25}, {'word': '句子级别的', 'ratio': 0.125}]",句子级别,{},[]
3941,3941,sentence-level classification,句子级分类,0.5,8,"[{'word': '句子级别分类', 'ratio': 0.5}, {'word': '句子级分类', 'ratio': 0.5}]",句子级别分类,{},[]
3942,3942,sentence-level representation,句子级表示,0.5,8,"[{'word': '句子级别表示', 'ratio': 0.5}, {'word': '句子级表示', 'ratio': 0.5}]",句子级别表示,{},[]
3943,3943,sentiment analysis model,情感分析模型,1.0,7,"[{'word': '情感分析模型', 'ratio': 1.0}]",情感分析模型,{},[]
3944,3944,sentiment classification,情感分类,1.0,7,"[{'word': '情感分类', 'ratio': 1.0}]",情感分类,{},[]
3945,3945,sentiment classifier,情感分类器,1.0,7,"[{'word': '情感分类器', 'ratio': 1.0}]",情感分类器,{},[]
3946,3946,sentiment detection,情感检测,0.8333333333333334,6,"[{'word': '情感检测', 'ratio': 0.8333333333333334}, {'word': '情绪检测', 'ratio': 0.16666666666666666}]",情感检测,{},[]
3947,3947,sentiment transfer,情感转移,0.5,6,"[{'word': '情感转移', 'ratio': 0.5}, {'word': '情感迁移', 'ratio': 0.3333333333333333}, {'word': '情感传递', 'ratio': 0.16666666666666666}]",情感转移,{},[]
3948,3948,separation oracle,分离预言机,0.5,6,"[{'word': '分离预言机', 'ratio': 0.5}, {'word': '分离神谕', 'ratio': 0.16666666666666666}, {'word': '分离甲骨文', 'ratio': 0.16666666666666666}, {'word': '分离oracle', 'ratio': 0.16666666666666666}]",分离预言机,{},[]
3949,3949,separation parameter,分割参数,0.0,6,"[{'word': '分离参数', 'ratio': 1.0}]",分离参数,{},[]
3950,3950,separator token,分隔符标记,0.5,6,"[{'word': '分隔符令牌', 'ratio': 0.5}, {'word': '分隔符标记', 'ratio': 0.5}]",分隔符令牌,{},[]
3951,3951,seq2seq model,seq2seq模型,0.4,5,"[{'word': '序列到序列模型', 'ratio': 0.6}, {'word': 'seq2seq模型', 'ratio': 0.4}]",序列到序列模型,{},[]
3952,3952,sequence,序列,0.5,10,"[{'word': '序列', 'ratio': 1.0}]",序列,{},[]
3953,3953,sequence alignment,序列比对,0.0,5,"[{'word': '序列对齐', 'ratio': 1.0}]",序列对齐,{},[]
3954,3954,sequence classification,序列分类,1.0,7,"[{'word': '序列分类', 'ratio': 1.0}]",序列分类,{},[]
3955,3955,sequence database,序列数据库,1.0,7,"[{'word': '序列数据库', 'ratio': 1.0}]",序列数据库,{},[]
3956,3956,sequence generation,序列生成,1.0,7,"[{'word': '序列生成', 'ratio': 1.0}]",序列生成,{},[]
3957,3957,sequence labeling,序列标注,1.0,7,"[{'word': '序列标注', 'ratio': 1.0}]",序列标注,{},[]
3958,3958,sequence labeling model,序列标注模型,1.0,7,"[{'word': '序列标注模型', 'ratio': 1.0}]",序列标注模型,{},[]
3959,3959,sequence length,序列长度,1.0,7,"[{'word': '序列长度', 'ratio': 1.0}]",序列长度,{},[]
3960,3960,sequence model,序列模型,1.0,7,"[{'word': '序列模型', 'ratio': 1.0}]",序列模型,{},[]
3961,3961,sequence prediction,序列预测,1.0,7,"[{'word': '序列预测', 'ratio': 1.0}]",序列预测,{},[]
3962,3962,sequence tagging,序列标注,0.8571428571428571,7,"[{'word': '序列标注', 'ratio': 0.8571428571428571}, {'word': '序列标记', 'ratio': 0.14285714285714285}]",序列标注,{},[]
3963,3963,sequence transduction,序列转换,0.2857142857142857,7,"[{'word': '序列转导', 'ratio': 0.7142857142857143}, {'word': '序列转换', 'ratio': 0.2857142857142857}]",序列转导,{},[]
3964,3964,sequence-to-sequence,序列到序列,1.0,7,"[{'word': '序列到序列', 'ratio': 1.0}]",序列到序列,{},[]
3965,3965,sequence-to-sequence architecture,序列到序列架构,1.0,7,"[{'word': '序列到序列架构', 'ratio': 1.0}]",序列到序列架构,{},[]
3966,3966,sequence-to-sequence generation,序列到序列生成,0.8571428571428571,7,"[{'word': '序列到序列生成', 'ratio': 0.8571428571428571}, {'word': '', 'ratio': 0.14285714285714285}]",序列到序列生成,{},[]
3967,3967,sequence-to-sequence model,序列到序列模型,1.0,7,"[{'word': '序列到序列模型', 'ratio': 1.0}]",序列到序列模型,{},[]
3968,3968,sequence-to-sequence transduction,序列到序列转导,0.1428571428571428,7,"[{'word': '序列到序列转换', 'ratio': 0.7142857142857143}, {'word': '序列到序列传导', 'ratio': 0.14285714285714285}, {'word': '序列到序列转导', 'ratio': 0.14285714285714285}]",序列到序列转换,{},[]
3969,3969,sequential datum,序列数据,0.6666666666666666,6,"[{'word': '序列数据', 'ratio': 0.6666666666666666}, {'word': '顺序数据', 'ratio': 0.3333333333333333}]",序列数据,{},[]
3970,3970,sequential decision making,顺序决策,0.1666666666666666,6,"[{'word': '序列决策', 'ratio': 0.5}, {'word': '序贯决策', 'ratio': 0.3333333333333333}, {'word': '顺序决策', 'ratio': 0.16666666666666666}]",序列决策,{},[]
3971,3971,sequential decision-making process,顺序决策过程,0.1666666666666666,6,"[{'word': '序列决策过程', 'ratio': 0.5}, {'word': '序贯决策过程', 'ratio': 0.3333333333333333}, {'word': '顺序决策过程', 'ratio': 0.16666666666666666}]",序列决策过程,{},[]
3972,3972,sequential sampler,顺序采样器,0.5,6,"[{'word': '序列采样器', 'ratio': 0.5}, {'word': '顺序采样器', 'ratio': 0.5}]",序列采样器,{},[]
3973,3973,sequential tagging,序列标注,0.6666666666666666,6,"[{'word': '序列标注', 'ratio': 0.6666666666666666}, {'word': '逐序标记', 'ratio': 0.16666666666666666}, {'word': '顺序标记', 'ratio': 0.16666666666666666}]",序列标注,{},[]
3974,3974,set cover problem,集合覆盖问题,1.0,6,"[{'word': '集合覆盖问题', 'ratio': 1.0}]",集合覆盖问题,{},[]
3975,3975,set function,集合函数,1.0,6,"[{'word': '集合函数', 'ratio': 1.0}]",集合函数,{},[]
3976,3976,shallow network,浅层网络,1.0,6,"[{'word': '浅层网络', 'ratio': 1.0}]",浅层网络,{},[]
3977,3977,shape matching,形状匹配,1.0,6,"[{'word': '形状匹配', 'ratio': 1.0}]",形状匹配,{},[]
3978,3978,shape prior,形状先验,1.0,8,"[{'word': '形状先验', 'ratio': 1.0}]",形状先验,{},[]
3979,3979,shift invariant,平移不变,0.125,8,"[{'word': '移位不变', 'ratio': 0.25}, {'word': '平移不变性', 'ratio': 0.25}, {'word': '平移不变', 'ratio': 0.125}, {'word': '位移不变', 'ratio': 0.125}, {'word': '移动不变', 'ratio': 0.125}, {'word': '平移不变的', 'ratio': 0.125}]",平移不变性,"1. Rank: 平移不变性, 平移不变, 移位不变, 位移不变, 移动不变, 平移不变的

2. Explanation: The term ""平移不变性"" (translation invariance) is the best fit because it accurately captures the concept of invariance under translation, which is a fundamental principle in many areas of AI, particularly in machine learning and computer vision. The term ""平移"" specifically refers to translation in a mathematical sense, making it semantically precise. The back translation ""translation invariance"" aligns well with the original English term ""shift invariant,"" maintaining the technical context. 

The other candidates, while they convey a similar idea, either lack the specificity of ""平移"" (translation) or introduce ambiguity with terms like ""移位"" (shift) and ""移动"" (move), which can imply different types of transformations. ""平移不变"" (translation unchanged) is also a strong candidate but is less formal than ""平移不变性,"" which is more commonly used in academic and technical literature. Thus, ""平移不变性"" is the most contextually appropriate choice in the AI domain.","['Shift unchanged', 'translation invariance', 'Translation unchanged', 'Displacement unchanged', 'Move unchanged', 'translation invariant']"
3980,3980,shift reduce parser,移位归约分析器,0.0,8,"[{'word': '移位归约解析器', 'ratio': 0.5}, {'word': '移动归约解析器', 'ratio': 0.125}, {'word': '移进-归约分析器', 'ratio': 0.125}, {'word': '移进-规约分析器', 'ratio': 0.125}, {'word': '移进-归约解析器', 'ratio': 0.125}]",移位归约解析器,{},[]
3981,3981,shortest path,最短路径,1.0,8,"[{'word': '最短路径', 'ratio': 1.0}]",最短路径,{},[]
3982,3982,shortest path algorithm,最短路径算法,1.0,7,"[{'word': '最短路径算法', 'ratio': 1.0}]",最短路径算法,{},[]
3983,3983,shortest path kernel,最短路径核,1.0,7,"[{'word': '最短路径核', 'ratio': 1.0}]",最短路径核,{},[]
3984,3984,shortest path length,最短路径长度,1.0,7,"[{'word': '最短路径长度', 'ratio': 1.0}]",最短路径长度,{},[]
3985,3985,sibling model,兄弟模型,0.8571428571428571,7,"[{'word': '兄弟模型', 'ratio': 0.8571428571428571}, {'word': '兄弟姐妹模型', 'ratio': 0.14285714285714285}]",兄弟模型,{},[]
3986,3986,sigmoid activation,sigmoid激活函数,0.1428571428571428,7,"[{'word': 'Sigmoid 激活', 'ratio': 0.2857142857142857}, {'word': 'シグモイド活性化関数', 'ratio': 0.14285714285714285}, {'word': 'Sigmoid激活函数', 'ratio': 0.14285714285714285}, {'word': 'sigmoid激活函数', 'ratio': 0.14285714285714285}, {'word': 'sigmoid激活', 'ratio': 0.14285714285714285}, {'word': '乙状结肠激活', 'ratio': 0.14285714285714285}]",Sigmoid激活函数,"1. Rank: Sigmoid激活函数, sigmoid激活函数, Sigmoid 激活, sigmoid激活, シグモイド活性化関数, 乙状结肠激活

2. Explanation: The term ""Sigmoid激活函数"" is the best fit because it accurately captures the concept of ""sigmoid activation"" in the context of AI and neural networks. The inclusion of ""函数"" (function) is crucial as it specifies that this is a mathematical function, which is essential in the AI domain. The back translation ""Sigmoid activation function"" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. 

The second choice, ""sigmoid激活函数,"" is also a strong candidate, but it uses lowercase ""sigmoid,"" which is less common in formal contexts where capitalization is typically used for function names. 

""Sigmoid 激活"" and ""sigmoid激活"" are less precise because they omit the term ""函数,"" which is important for clarity in mathematical contexts. 

""シグモイド活性化関数"" is a transliteration from Japanese and is not suitable for a Chinese context, while ""乙状结肠激活"" is completely incorrect as it translates to ""sigmoid colon activation,"" which has no relevance in the AI context. 

Overall, the first choice is the most contextually appropriate and semantically accurate for the AI domain.","['Sigmoid activation', 'シグモイド activation number', 'Sigmoid activation function', 'sigmoid activation function', 'sigmoid activation', 'sigmoid activation']"
3987,3987,sigmoid activation function,sigmoid激活函数,0.0,6,"[{'word': 'sigmoid 激活函数', 'ratio': 0.5}, {'word': 'Sigmoid激活函数', 'ratio': 0.16666666666666666}, {'word': 'Sigmoid 激活函数', 'ratio': 0.16666666666666666}, {'word': 'S型激活函数', 'ratio': 0.16666666666666666}]",sigmoid 激活函数,{},[]
3988,3988,signal-to-noise ratio,信噪比,1.0,6,"[{'word': '信噪比', 'ratio': 1.0}]",信噪比,{},[]
3989,3989,sim,相似度,1.0,6,"[{'word': '相似度', 'ratio': 1.0}]",相似度,{},[]
3990,3990,similarity function,相似性函数,0.6666666666666666,6,"[{'word': '相似性函数', 'ratio': 0.6666666666666666}, {'word': '相似度函数', 'ratio': 0.3333333333333333}]",相似性函数,{},[]
3991,3991,similarity graph,相似性图,0.25,8,"[{'word': '相似度图', 'ratio': 0.5}, {'word': '相似性图', 'ratio': 0.25}, {'word': '相似图', 'ratio': 0.25}]",相似度图,{},[]
3992,3992,similarity matrix,相似性矩阵,0.25,8,"[{'word': '相似度矩阵', 'ratio': 0.5}, {'word': '相似性矩阵', 'ratio': 0.25}, {'word': '相似矩阵', 'ratio': 0.25}]",相似度矩阵,{},[]
3993,3993,similarity measure,相似度量,0.125,8,"[{'word': '相似度度量', 'ratio': 0.5}, {'word': '相似性度量', 'ratio': 0.375}, {'word': '相似度量', 'ratio': 0.125}]",相似度度量,{},[]
3994,3994,similarity metric,相似性度量,0.0,8,"[{'word': '相似性指标', 'ratio': 0.375}, {'word': '相似度指标', 'ratio': 0.375}, {'word': '相似度度量', 'ratio': 0.125}, {'word': '相似度量', 'ratio': 0.125}]",相似度度量,"1. Rank: 相似度度量, 相似度指标, 相似度量, 相似性指标

2. Explanation: The term ""相似度度量"" (similarity measure) is the best fit for the English term ""similarity metric"" in the context of AI and information theory. The word ""度量"" (measure) is commonly used in mathematical and statistical contexts to refer to a quantitative assessment, which aligns well with the concept of a metric. Additionally, ""相似度"" (similarity) accurately captures the essence of what is being measured. 

The second choice, ""相似度指标"" (similarity index), while also relevant, uses ""指标"" (index), which is less precise in the context of metrics that are typically used in AI and data analysis. ""相似度量"" (similarity measure) is a close contender but is less commonly used than ""相似度度量."" Lastly, ""相似性指标"" (similarity index) is the least suitable as ""相似性"" (similarity) is a broader term and does not convey the specific quantitative aspect that ""度量"" (measure) does. Thus, ""相似度度量"" is the most semantically accurate and contextually appropriate term for the AI domain.","['similarity index', 'similarity index', 'similarity measure', 'Similarity measure']"
3995,3995,similarity score,相似度得分,0.375,8,"[{'word': '相似度得分', 'ratio': 0.375}, {'word': '相似性得分', 'ratio': 0.25}, {'word': '相似度分数', 'ratio': 0.25}, {'word': '相似性评分', 'ratio': 0.125}]",相似度得分,"1. Rank: 相似度得分, 相似度分数, 相似性得分, 相似性评分

2. Explanation: The term ""相似度得分"" is the best fit for ""similarity score"" in the AI domain due to its semantic accuracy and contextual relevance. The term ""相似度"" (similarity degree) is widely used in machine learning and AI literature to refer to the measure of how alike two entities are, which aligns perfectly with the concept of a similarity score. The addition of ""得分"" (score) clearly indicates that it is a quantifiable measure, which is essential in the context of evaluating embeddings or matching observations. 

In contrast, ""相似性得分"" and ""相似性评分"" use ""相似性"" (similarity) which is less common in technical contexts and may imply a broader or less precise concept. ""相似度分数"" is also a valid option, but ""得分"" is more commonly used in the context of scoring systems in AI. Therefore, ""相似度得分"" is the most contextually appropriate and semantically accurate choice for the term ""similarity score.""","['similarity score', 'similarity score', 'similarity score', 'similarity score']"
3996,3996,similarity search,相似性搜索,1.0,6,"[{'word': '相似性搜索', 'ratio': 1.0}]",相似性搜索,{},[]
3997,3997,simplicial complex,单纯复形,1.0,6,"[{'word': '单纯复形', 'ratio': 1.0}]",单纯复形,{},[]
3998,3998,simulated annealing,模拟退火,1.0,6,"[{'word': '模拟退火', 'ratio': 1.0}]",模拟退火,{},[]
3999,3999,single task learning,单任务学习,1.0,6,"[{'word': '单任务学习', 'ratio': 1.0}]",单任务学习,{},[]
4000,4000,single-label classification,单标签分类,1.0,4,"[{'word': '单标签分类', 'ratio': 1.0}]",单标签分类,{},[]
4001,4001,single-view data,单视图数据,0.75,4,"[{'word': '单视图数据', 'ratio': 0.75}, {'word': '单视角数据', 'ratio': 0.25}]",单视图数据,{},[]
4002,4002,singleton,单个实体,0.0,4,"[{'word': '单例', 'ratio': 1.0}]",单例,{},[]
4003,4003,singular value,奇异值,1.0,4,"[{'word': '奇异值', 'ratio': 1.0}]",奇异值,{},[]
4004,4004,singular vector,奇异向量,1.0,4,"[{'word': '奇异向量', 'ratio': 1.0}]",奇异向量,{},[]
4005,4005,skip connection,跳跃连接,1.0,5,"[{'word': '跳跃连接', 'ratio': 1.0}]",跳跃连接,{},[]
4006,4006,skip-gram,跳元模型,0.0,5,"[{'word': '跳字模型', 'ratio': 0.4}, {'word': '跳跃词模型', 'ratio': 0.2}, {'word': '跳词模型', 'ratio': 0.2}, {'word': '跳字', 'ratio': 0.2}]",跳词模型,"1. Rank: 跳词模型, 跳跃词模型, 跳字模型, 跳字

2. Explanation: The term ""跳词模型"" (skip-word model) is the best fit for the translation of ""skip-gram"" in the AI domain, particularly in the context of word embeddings. This term accurately captures the essence of the original English term, where ""跳"" (skip) directly relates to the action of skipping words in the context of the model, and ""词"" (word) clearly indicates that it pertains to words. 

The back translation of ""跳词模型"" as ""word skip model"" maintains semantic accuracy and contextual relevance, aligning well with the established terminology in natural language processing. 

In contrast, ""跳跃词模型"" (Jump word model) introduces ambiguity with ""跳跃"" (jump), which does not convey the same meaning as ""skip"" in this context. ""跳字模型"" (word skip model) is less precise because ""字"" (character) refers to characters rather than words, which is not suitable for the context of word embeddings. Lastly, ""跳字"" (Jump word) is too vague and lacks the model-specific context needed for clarity. Thus, ""跳词模型"" is the most appropriate choice for conveying the concept of ""skip-gram"" in the AI domain.","['word skip model', 'Jump word model', 'word skip model', 'Jump word']"
4007,4007,skip-gram model,跳语法模型,0.0,5,"[{'word': '跳字模型', 'ratio': 0.6}, {'word': '跳跃词模型', 'ratio': 0.2}, {'word': '跳词模型', 'ratio': 0.2}]",跳字模型,{},[]
4008,4008,slack variable,松弛变量,1.0,5,"[{'word': '松弛变量', 'ratio': 1.0}]",松弛变量,{},[]
4009,4009,sliding window,滑动窗口,1.0,5,"[{'word': '滑动窗口', 'ratio': 1.0}]",滑动窗口,{},[]
4010,4010,sliding window classifier,滑动窗口分类器,1.0,6,"[{'word': '滑动窗口分类器', 'ratio': 1.0}]",滑动窗口分类器,{},[]
4011,4011,slot,槽,0.1666666666666666,6,"[{'word': '插槽', 'ratio': 0.3333333333333333}, {'word': '槽位', 'ratio': 0.3333333333333333}, {'word': '槽', 'ratio': 0.16666666666666666}, {'word': '投币口', 'ratio': 0.16666666666666666}]",插槽,"1. Rank: 插槽, 槽位, 槽, 投币口

2. Explanation: The term ""插槽"" is the best fit for the AI domain-specific usage of ""slot"" because it accurately conveys the concept of a designated space or position for input within a system, which aligns with the context of dialogue systems and generative models. ""插槽"" directly translates to ""slot"" in a technical sense, often used in computing and AI contexts to refer to placeholders or variables that can hold specific values. 

""槽位"" is also a reasonable candidate, but it is less commonly used in the AI context compared to ""插槽."" ""槽"" translates to ""groove,"" which does not fit the intended meaning in this context, and ""投币口"" translates to ""coin slot,"" which is entirely unrelated to the AI terminology being discussed. Therefore, ""插槽"" is the most semantically accurate and contextually appropriate choice.","['slot', 'slot', 'groove', 'Coin slot']"
4012,4012,slot filling,槽填充,0.3333333333333333,6,"[{'word': '插槽填充', 'ratio': 0.3333333333333333}, {'word': '槽填充', 'ratio': 0.3333333333333333}, {'word': '槽位填充', 'ratio': 0.3333333333333333}]",插槽填充,"1. Rank: 插槽填充, 槽位填充, 槽填充

2. Explanation: The term ""插槽填充"" is the best fit for ""slot filling"" in the AI domain due to its semantic accuracy and contextual relevance. ""插槽"" directly translates to ""slot,"" which is commonly used in technical contexts, particularly in dialogue systems and natural language processing. This term clearly conveys the idea of filling designated spaces (or slots) with specific information extracted from user input. 

""槽位填充"" is also a strong candidate, as ""槽位"" can be understood as ""slot position,"" which maintains the meaning but is slightly less common in usage compared to ""插槽."" 

""槽填充,"" while still understandable, is less precise because ""槽"" can refer to a broader range of contexts and may not specifically imply the technical meaning of ""slot"" in AI applications. Therefore, ""插槽填充"" is the most contextually appropriate and widely recognized term in the AI field, making it the best choice for translating ""slot filling.""","['slot filling', 'slot filling', 'slot filling']"
4013,4013,slot value,槽值,0.1666666666666666,6,"[{'word': '槽位值', 'ratio': 0.5}, {'word': '插槽值', 'ratio': 0.3333333333333333}, {'word': '槽值', 'ratio': 0.16666666666666666}]",槽位值,{},[]
4014,4014,slot-value pair,槽值对,0.1666666666666666,6,"[{'word': '插槽值对', 'ratio': 0.3333333333333333}, {'word': '槽位-值对', 'ratio': 0.3333333333333333}, {'word': '槽-值对', 'ratio': 0.16666666666666666}, {'word': '槽值对', 'ratio': 0.16666666666666666}]",插槽值对,"1. Rank: 插槽值对, 槽位-值对, 槽-值对, 槽值对

2. Explanation: The term ""插槽值对"" is the best fit for the AI domain-specific usage of ""slot-value pair"" because it accurately captures the concept of a ""slot"" as a designated space or position (插槽) that holds a specific value (值). This translation maintains the semantic integrity of the original term and is clear in its meaning, making it easily understandable in the context of AI and data structures. 

The other candidates, while they may convey similar meanings, do not align as closely with the established terminology in AI. For instance, ""槽位-值对"" introduces the term ""槽位,"" which can imply a more physical or spatial context rather than the abstract concept of a slot in data structures. ""槽-值对"" and ""槽值对"" are less formal and may not be as widely recognized in technical discussions. Therefore, ""插槽值对"" stands out as the most precise and contextually appropriate choice.","['slot value pair', 'slot-value pair', 'slot-value pair', 'slot value pair']"
4015,4015,smoothing parameter,平滑参数,0.875,8,"[{'word': '平滑参数', 'ratio': 0.875}, {'word': 'smoothing parameter', 'ratio': 0.125}]",平滑参数,{},[]
4016,4016,smoothness term,平滑项,1.0,8,"[{'word': '平滑项', 'ratio': 1.0}]",平滑项,{},[]
4017,4017,social bias,社会偏见,1.0,8,"[{'word': '社会偏见', 'ratio': 1.0}]",社会偏见,{},[]
4018,4018,social network analysis,社会网络分析,0.375,8,"[{'word': '社交网络分析', 'ratio': 0.625}, {'word': '社会网络分析', 'ratio': 0.375}]",社交网络分析,{},[]
4019,4019,soft margin,软间隔,0.5714285714285714,7,"[{'word': '软间隔', 'ratio': 0.5714285714285714}, {'word': '软边界', 'ratio': 0.42857142857142855}]",软间隔,{},[]
4020,4020,softmax activation,softmax激活函数,0.0,7,"[{'word': 'softmax 激活', 'ratio': 0.2857142857142857}, {'word': 'softmax激活', 'ratio': 0.2857142857142857}, {'word': 'Softmax 激活', 'ratio': 0.14285714285714285}, {'word': '软最大值激活', 'ratio': 0.14285714285714285}, {'word': '激活', 'ratio': 0.14285714285714285}]",softmax 激活,"1. Rank: softmax 激活, softmax激活, Softmax 激活, 软最大值激活, 激活

2. Explanation: The term ""softmax 激活"" is the best fit because it maintains the original English term ""softmax"" while providing a clear and accurate translation of ""activation."" This term is widely recognized in the AI and machine learning communities, ensuring semantic accuracy and contextual fit. The back translation confirms that it retains the intended meaning. The variations ""softmax激活"" and ""Softmax 激活"" are also acceptable, but the first option is preferred due to its consistency in formatting and common usage. The term ""软最大值激活"" translates to ""soft maximum activation,"" which, while technically accurate, is less commonly used in the AI domain and may cause confusion. Lastly, ""激活"" alone is too vague and does not convey the specific concept of softmax activation.","['softmax activation', 'softmax activation', 'Softmax activation', 'softmax activation', 'activation']"
4021,4021,softmax activation function,softmax激活函数,0.4285714285714285,7,"[{'word': 'softmax激活函数', 'ratio': 0.42857142857142855}, {'word': 'softmax 激活函数', 'ratio': 0.2857142857142857}, {'word': 'oftmax 激活函数', 'ratio': 0.14285714285714285}, {'word': '软最大值激活函数', 'ratio': 0.14285714285714285}]",softmax激活函数,"1. Rank: softmax激活函数, softmax 激活函数, 软最大值激活函数, oftmax 激活函数

2. Explanation: The term ""softmax激活函数"" is the best fit because it accurately retains the original English term ""softmax"" while clearly indicating its function as an ""activation function"" in Chinese. The use of ""激活函数"" (activation function) is standard terminology in the AI and machine learning fields, making it easily recognizable to professionals in the domain. The second candidate, ""softmax 激活函数,"" is also a good option but includes a space that is not typically used in technical terms, making it slightly less formal. The third candidate, ""软最大值激活函数,"" translates ""softmax"" literally to ""soft maximum,"" which, while understandable, is not the standard term used in the AI community and could lead to confusion. Lastly, ""oftmax 激活函数"" is a misspelling of ""softmax,"" making it the least suitable choice. Therefore, ""softmax激活函数"" is the most semantically accurate and contextually appropriate term for the AI domain.","['softmax activation function', 'softmax activation function', 'oftmax activation function', 'Soft maximum activation function']"
4022,4022,softmax classifier,softmax分类器,0.4285714285714285,7,"[{'word': 'softmax分类器', 'ratio': 0.42857142857142855}, {'word': 'softmax 分类器', 'ratio': 0.2857142857142857}, {'word': 'Softmax 分类器', 'ratio': 0.14285714285714285}, {'word': '软最大值分类器', 'ratio': 0.14285714285714285}]",softmax分类器,"1. Rank: softmax分类器, softmax 分类器, Softmax 分类器, 软最大值分类器

2. Explanation: The term ""softmax分类器"" is the best fit because it accurately reflects the original English term ""softmax classifier"" while maintaining semantic accuracy and contextual relevance in the AI domain. The term ""softmax"" is a well-established term in machine learning and AI, and it is commonly used in both English and Chinese without translation. The absence of a space in ""softmax分类器"" aligns with the conventions of technical terminology in Chinese, where compound terms are often written without spaces. 

The second candidate, ""softmax 分类器,"" introduces a space that is not typically used in technical contexts, making it less preferable. The third candidate, ""Softmax 分类器,"" capitalizes ""Softmax,"" which is unnecessary in Chinese and could imply a different emphasis. Lastly, ""软最大值分类器"" translates ""softmax"" literally, which is less common in the AI community and may lead to confusion, as it does not convey the established meaning of the term. Therefore, ""softmax分类器"" is the most contextually appropriate and semantically accurate choice.","['softmax classifier', 'softmax classifier', 'Softmax classifier', 'soft max classifier']"
4023,4023,softmax distribution,softmax分布,0.5,6,"[{'word': 'softmax分布', 'ratio': 0.5}, {'word': '軟最大分佈', 'ratio': 0.16666666666666666}, {'word': 'Softmax 分布', 'ratio': 0.16666666666666666}, {'word': 'softmax 分布', 'ratio': 0.16666666666666666}]",softmax分布,{},[]
4024,4024,softmax function,softmax函数,0.5,6,"[{'word': 'softmax函数', 'ratio': 0.5}, {'word': '軟最大函數', 'ratio': 0.16666666666666666}, {'word': 'Softmax 函数', 'ratio': 0.16666666666666666}, {'word': 'softmax 函数', 'ratio': 0.16666666666666666}]",softmax函数,{},[]
4025,4025,softmax loss,softmax损失,0.5,6,"[{'word': 'softmax损失', 'ratio': 0.5}, {'word': '軟最大損失', 'ratio': 0.16666666666666666}, {'word': 'Softmax 损失', 'ratio': 0.16666666666666666}, {'word': 'softmax 损失', 'ratio': 0.16666666666666666}]",softmax损失,{},[]
4026,4026,softplus,softplus - 软正函数,0.0,6,"[{'word': 'softplus函数', 'ratio': 0.5}, {'word': '軟體加', 'ratio': 0.16666666666666666}, {'word': 'Softplus', 'ratio': 0.16666666666666666}, {'word': 'softplus', 'ratio': 0.16666666666666666}]",softplus函数,{},[]
4027,4027,softplus activation,软加激活,0.0,6,"[{'word': 'softplus激活', 'ratio': 0.5}, {'word': '軟體加激活', 'ratio': 0.16666666666666666}, {'word': 'Softplus 激活', 'ratio': 0.16666666666666666}, {'word': 'softplus 激活', 'ratio': 0.16666666666666666}]",softplus激活,{},[]
4028,4028,softplus function,柔和正函数 (softplus function),0.0,8,"[{'word': 'softplus 函數', 'ratio': 0.125}, {'word': '软加函数', 'ratio': 0.125}, {'word': 'softplus 函数', 'ratio': 0.125}, {'word': 'softplus函数', 'ratio': 0.125}, {'word': 'Softplus函数', 'ratio': 0.125}, {'word': 'Softplus 函数', 'ratio': 0.125}, {'word': '软正函数', 'ratio': 0.125}, {'word': '软加功能', 'ratio': 0.125}]",softplus 函数,"1. Rank: softplus 函数, softplus函数, Softplus函数, Softplus 函数, 软加函数, 软正函数, 软加功能

2. Explanation: The term ""softplus 函数"" is the best fit because it accurately retains the original English term ""softplus function"" in a way that is semantically precise and contextually appropriate for the AI domain. The use of ""函数"" (function) is standard in mathematical and computational contexts in Chinese, making it clear that this term refers to a specific mathematical function. 

The other candidates, such as ""软加函数"" (soft addition function) and ""软正函数"" (soft positive function), introduce ambiguity and do not accurately reflect the established terminology used in AI and machine learning literature. The variations ""softplus函数"" and ""Softplus函数"" are also acceptable but less formal due to the lack of a space, which is typically used in academic writing. The capitalization in ""Softplus 函数"" and ""Softplus函数"" is unnecessary in this context, as it does not follow the standard convention for function names in Chinese. 

Overall, ""softplus 函数"" maintains both semantic accuracy and contextual fit, making it the most suitable choice for the AI domain.","['softplus function', 'soft addition function', 'softplus function', 'softplus function', 'Softplus function', 'Softplus function', 'soft positive function', 'Soft plus function']"
4029,4029,solution space,解空间,1.0,8,"[{'word': '解空间', 'ratio': 1.0}]",解空间,{},[]
4030,4030,solver,解算器,0.0,8,"[{'word': '求解器', 'ratio': 1.0}]",求解器,{},[]
4031,4031,source domain,源域,0.75,8,"[{'word': '源域', 'ratio': 0.75}, {'word': '源领域', 'ratio': 0.25}]",源域,{},[]
4032,4032,source model,源模型,1.0,5,"[{'word': '源模型', 'ratio': 1.0}]",源模型,{},[]
4033,4033,source node,源节点,1.0,5,"[{'word': '源节点', 'ratio': 1.0}]",源节点,{},[]
4034,4034,source sequence,源序列,1.0,5,"[{'word': '源序列', 'ratio': 1.0}]",源序列,{},[]
4035,4035,source token,源词元,0.0,5,"[{'word': '源标记', 'ratio': 1.0}]",源标记,{},[]
4036,4036,source word,源词,1.0,5,"[{'word': '源词', 'ratio': 1.0}]",源词,{},[]
4037,4037,space carving,空间雕刻,0.875,8,"[{'word': '空间雕刻', 'ratio': 0.875}, {'word': '空间切割', 'ratio': 0.125}]",空间雕刻,{},[]
4038,4038,space complexity,空间复杂度,1.0,8,"[{'word': '空间复杂度', 'ratio': 1.0}]",空间复杂度,{},[]
4039,4039,space partitioning,空间划分,0.5,8,"[{'word': '空间划分', 'ratio': 0.5}, {'word': '空间分割', 'ratio': 0.5}]",空间划分,{},[]
4040,4040,spam detection,垃圾邮件检测,0.375,8,"[{'word': '垃圾信息检测', 'ratio': 0.625}, {'word': '垃圾邮件检测', 'ratio': 0.375}]",垃圾信息检测,{},[]
4041,4041,spam filtering,垃圾邮件过滤,0.375,8,"[{'word': '垃圾信息过滤', 'ratio': 0.625}, {'word': '垃圾邮件过滤', 'ratio': 0.375}]",垃圾信息过滤,{},[]
4042,4042,span,跨度,0.0,8,"[{'word': '张成', 'ratio': 0.625}, {'word': '生成的空间', 'ratio': 0.125}, {'word': '生成子空间', 'ratio': 0.125}, {'word': '生成空间', 'ratio': 0.125}]",张成,{},[]
4043,4043,sparse approximation,稀疏近似,1.0,8,"[{'word': '稀疏近似', 'ratio': 1.0}]",稀疏近似,{},[]
4044,4044,sparse attention,稀疏注意力,1.0,8,"[{'word': '稀疏注意力', 'ratio': 1.0}]",稀疏注意力,{},[]
4045,4045,sparse attention pattern,稀疏注意力模式,0.875,8,"[{'word': '稀疏注意力模式', 'ratio': 0.875}, {'word': '稀疏注意力', 'ratio': 0.125}]",稀疏注意力模式,{},[]
4046,4046,sparse graph,稀疏图,1.0,5,"[{'word': '稀疏图', 'ratio': 1.0}]",稀疏图,{},[]
4047,4047,sparse matrix,稀疏矩阵,1.0,5,"[{'word': '稀疏矩阵', 'ratio': 1.0}]",稀疏矩阵,{},[]
4048,4048,sparse model,稀疏模型,1.0,5,"[{'word': '稀疏模型', 'ratio': 1.0}]",稀疏模型,{},[]
4049,4049,sparse recovery,稀疏恢复,1.0,5,"[{'word': '稀疏恢复', 'ratio': 1.0}]",稀疏恢复,{},[]
4050,4050,sparse representation,稀疏表示,1.0,5,"[{'word': '稀疏表示', 'ratio': 1.0}]",稀疏表示,{},[]
4051,4051,sparse sampling,稀疏采样,1.0,3,"[{'word': '稀疏采样', 'ratio': 1.0}]",稀疏采样,{},[]
4052,4052,sparse vector,稀疏向量,1.0,3,"[{'word': '稀疏向量', 'ratio': 1.0}]",稀疏向量,{},[]
4053,4053,sparsification,稀疏化,1.0,3,"[{'word': '稀疏化', 'ratio': 1.0}]",稀疏化,{},[]
4054,4054,sparsity,稀疏性,1.0,3,"[{'word': '稀疏性', 'ratio': 1.0}]",稀疏性,{},[]
4055,4055,sparsity level,稀疏度,0.0,3,"[{'word': '稀疏水平', 'ratio': 0.3333333333333333}, {'word': '稀疏级别', 'ratio': 0.3333333333333333}, {'word': '稀疏度水平', 'ratio': 0.3333333333333333}]",稀疏水平,"1. Rank: 稀疏水平, 稀疏度水平, 稀疏级别

2. Explanation: The term ""稀疏水平"" (sparsity level) is the best fit for the AI domain-specific usage because it directly translates the English term while maintaining the same semantic structure. ""水平"" (level) is commonly used in technical contexts to denote a degree or extent, which aligns well with the concept of ""sparsity"" in mathematical and algorithmic contexts. 

""稀疏度水平"" (sparsity degree level) is a bit more verbose and introduces ""度"" (degree), which can be seen as redundant since ""水平"" already conveys the idea of a level of sparsity. While it is still accurate, it may not be as concise or commonly used in the field.

""稀疏级别"" (sparsity grade) is less preferred because ""级别"" (grade) is not as commonly used in this context as ""水平"" (level). In AI and mathematical literature, ""水平"" is more frequently associated with levels of measurement or thresholds, making it a more appropriate choice for conveying the intended meaning of ""sparsity level."" 

Overall, ""稀疏水平"" is the most semantically accurate and contextually fitting translation for the term in the AI domain.","['sparsity level', 'sparsity level', 'sparsity level']"
4056,4056,sparsity regularization,稀疏正则化,0.875,8,"[{'word': '稀疏正则化', 'ratio': 0.875}, {'word': '稀疏性正则化', 'ratio': 0.125}]",稀疏正则化,{},[]
4057,4057,spatial domain,空间域,1.0,8,"[{'word': '空间域', 'ratio': 1.0}]",空间域,{},[]
4058,4058,spatial gradient,空间梯度,1.0,8,"[{'word': '空间梯度', 'ratio': 1.0}]",空间梯度,{},[]
4059,4059,spatial pooling,空间池化,0.875,8,"[{'word': '空间池化', 'ratio': 0.875}, {'word': '空间梯度', 'ratio': 0.125}]",空间池化,{},[]
4060,4060,spatial pyramid,空间金字塔,1.0,8,"[{'word': '空间金字塔', 'ratio': 1.0}]",空间金字塔,{},[]
4061,4061,special token,特殊标记,0.875,8,"[{'word': '特殊标记', 'ratio': 0.875}, {'word': '特殊令牌', 'ratio': 0.125}]",特殊标记,{},[]
4062,4062,spectral algorithm,光谱算法,0.0,8,"[{'word': '谱算法', 'ratio': 0.875}, {'word': '谱算法c', 'ratio': 0.125}]",谱算法,{},[]
4063,4063,spectral clustering,谱聚类,1.0,8,"[{'word': '谱聚类', 'ratio': 1.0}]",谱聚类,{},[]
4064,4064,spectral decomposition,光谱分解,0.0,8,"[{'word': '谱分解', 'ratio': 1.0}]",谱分解,{},[]
4065,4065,spectral embedding,光谱嵌入,0.0,7,"[{'word': '谱嵌入', 'ratio': 1.0}]",谱嵌入,{},[]
4066,4066,spectral gap,光谱间隙,0.0,7,"[{'word': '谱间隙', 'ratio': 0.5714285714285714}, {'word': '谱隙', 'ratio': 0.2857142857142857}, {'word': '谱嵌入', 'ratio': 0.14285714285714285}]",谱间隙,{},[]
4067,4067,spectral learning,谱学习,0.8571428571428571,7,"[{'word': '谱学习', 'ratio': 0.8571428571428571}, {'word': '谱间隙', 'ratio': 0.14285714285714285}]",谱学习,{},[]
4068,4068,spectral matching,光谱匹配,0.0,7,"[{'word': '谱匹配', 'ratio': 1.0}]",谱匹配,{},[]
4069,4069,spectral method,光谱方法,0.0,7,"[{'word': '谱方法', 'ratio': 1.0}]",谱方法,{},[]
4070,4070,spectral norm,谱范数,1.0,7,"[{'word': '谱范数', 'ratio': 1.0}]",谱范数,{},[]
4071,4071,spectral property,光谱性质,0.0,7,"[{'word': '谱特性', 'ratio': 0.42857142857142855}, {'word': '谱性质', 'ratio': 0.42857142857142855}, {'word': '光谱特性', 'ratio': 0.14285714285714285}]",谱性质,"1. Rank: 谱性质, 谱特性, 光谱特性

2. Explanation: The term ""谱性质"" (back translated as ""Spectral properties"") is the best fit for the AI domain-specific usage because it accurately captures the technical meaning of ""spectral property"" in the context of data matrices and their characteristics. In mathematical and statistical contexts, ""性质"" (properties) is commonly used to describe inherent attributes or characteristics of mathematical objects, making it semantically precise. 

On the other hand, ""谱特性"" (back translated as ""Spectral characteristics"") is also a valid translation but is less commonly used in the specific context of spectral analysis in AI and machine learning. ""光谱特性"" (back translated as ""Spectral characteristics"") introduces the term ""光"" (light), which is not relevant in this context and could lead to confusion, as it implies a focus on light spectra rather than the mathematical properties of matrices. Therefore, ""谱性质"" is the most contextually appropriate choice.","['Spectral characteristics', 'Spectral properties', 'Spectral characteristics']"
4072,4072,spectrogram,声谱图,0.5714285714285714,7,"[{'word': '声谱图', 'ratio': 0.5714285714285714}, {'word': '语谱图', 'ratio': 0.2857142857142857}, {'word': '频谱图', 'ratio': 0.14285714285714285}]",声谱图,{},[]
4073,4073,speech recognition,语音识别,1.0,7,"[{'word': '语音识别', 'ratio': 1.0}]",语音识别,{},[]
4074,4074,speech recognizer,语音识别器,1.0,8,"[{'word': '语音识别器', 'ratio': 1.0}]",语音识别器,{},[]
4075,4075,speech synthesis model,语音合成模型,1.0,8,"[{'word': '语音合成模型', 'ratio': 1.0}]",语音合成模型,{},[]
4076,4076,speech synthesizer,语音合成器,1.0,8,"[{'word': '语音合成器', 'ratio': 1.0}]",语音合成器,{},[]
4077,4077,spoken dialogue system,口语对话系统,0.75,8,"[{'word': '口语对话系统', 'ratio': 0.75}, {'word': '语音对话系统', 'ratio': 0.25}]",口语对话系统,{},[]
4078,4078,spurious correlation,虚假相关性,0.125,8,"[{'word': '虚假相关', 'ratio': 0.875}, {'word': '虚假相关性', 'ratio': 0.125}]",虚假相关,{},[]
4079,4079,squared Euclidean distance,平方欧氏距离,0.125,8,"[{'word': '平方欧几里得距离', 'ratio': 0.75}, {'word': '平方欧氏距离', 'ratio': 0.125}, {'word': '欧氏距离平方', 'ratio': 0.125}]",平方欧几里得距离,{},[]
4080,4080,squared error loss,平方误差损失,1.0,8,"[{'word': '平方误差损失', 'ratio': 1.0}]",平方误差损失,{},[]
4081,4081,stable model,稳定模型,1.0,8,"[{'word': '稳定模型', 'ratio': 1.0}]",稳定模型,{},[]
4082,4082,stable model semantic,稳定模型语义,0.75,8,"[{'word': '稳定模型语义', 'ratio': 0.75}, {'word': '稳定模型语义 If you need more assistance, feel free to ask!', 'ratio': 0.125}, {'word': '稳定的模型语义', 'ratio': 0.125}]",稳定模型语义,{},[]
4083,4083,stance detection,立场检测,1.0,8,"[{'word': '立场检测', 'ratio': 1.0}]",立场检测,{},[]
4084,4084,standard normal distribution,标准正态分布,1.0,8,"[{'word': '标准正态分布', 'ratio': 1.0}]",标准正态分布,{},[]
4085,4085,start token,起始标记,0.75,8,"[{'word': '起始标记', 'ratio': 0.75}, {'word': '开始标记', 'ratio': 0.25}]",起始标记,{},[]
4086,4086,state,状态,1.0,8,"[{'word': '状态', 'ratio': 1.0}]",状态,{},[]
4087,4087,state action pair,状态动作对,0.25,8,"[{'word': '状态-动作对', 'ratio': 0.75}, {'word': '状态动作对', 'ratio': 0.25}]",状态-动作对,{},[]
4088,4088,state distribution,状态分布,1.0,7,"[{'word': '状态分布', 'ratio': 1.0}]",状态分布,{},[]
4089,4089,state estimation,状态估计,1.0,7,"[{'word': '状态估计', 'ratio': 1.0}]",状态估计,{},[]
4090,4090,state machine,有限状态机,0.0,7,"[{'word': '状态机', 'ratio': 1.0}]",状态机,{},[]
4091,4091,state matrix,状态矩阵,1.0,7,"[{'word': '状态矩阵', 'ratio': 1.0}]",状态矩阵,{},[]
4092,4092,state of the art algorithm,最先进算法,0.5714285714285714,7,"[{'word': '最先进算法', 'ratio': 0.5714285714285714}, {'word': '先进算法', 'ratio': 0.42857142857142855}]",最先进算法,{},[]
4093,4093,state representation,状态表示,1.0,6,"[{'word': '状态表示', 'ratio': 1.0}]",状态表示,{},[]
4094,4094,state sequence,状态序列,1.0,6,"[{'word': '状态序列', 'ratio': 1.0}]",状态序列,{},[]
4095,4095,state space,状态空间,1.0,6,"[{'word': '状态空间', 'ratio': 1.0}]",状态空间,{},[]
4096,4096,state trajectory,状态轨迹,1.0,6,"[{'word': '状态轨迹', 'ratio': 1.0}]",状态轨迹,{},[]
4097,4097,state transition,状态转移,0.6666666666666666,6,"[{'word': '状态转移', 'ratio': 0.6666666666666666}, {'word': '状态转换 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.16666666666666666}, {'word': '状态转换', 'ratio': 0.16666666666666666}]",状态转移,{},[]
4098,4098,state transition function,状态转移函数,1.0,8,"[{'word': '状态转移函数', 'ratio': 1.0}]",状态转移函数,{},[]
4099,4099,state transition matrix,状态转移矩阵,1.0,8,"[{'word': '状态转移矩阵', 'ratio': 1.0}]",状态转移矩阵,{},[]
4100,4100,state transition model,状态转移模型,1.0,8,"[{'word': '状态转移模型', 'ratio': 1.0}]",状态转移模型,{},[]
4101,4101,state transition probability,状态转移概率,1.0,8,"[{'word': '状态转移概率', 'ratio': 1.0}]",状态转移概率,{},[]
4102,4102,state value function,状态值函数,0.25,8,"[{'word': '状态价值函数', 'ratio': 0.75}, {'word': '状态值函数', 'ratio': 0.25}]",状态价值函数,{},[]
4103,4103,state variable,状态变量,1.0,9,"[{'word': '状态变量', 'ratio': 1.0}]",状态变量,{},[]
4104,4104,state vector,状态向量,1.0,9,"[{'word': '状态向量', 'ratio': 1.0}]",状态向量,{},[]
4105,4105,state-action distribution,状态-动作分布,1.0,9,"[{'word': '状态-动作分布', 'ratio': 1.0}]",状态-动作分布,{},[]
4106,4106,state-action space,状态-动作空间,1.0,9,"[{'word': '状态-动作空间', 'ratio': 1.0}]",状态-动作空间,{},[]
4107,4107,state-action value,状态-动作值,1.0,9,"[{'word': '状态-动作值', 'ratio': 1.0}]",状态-动作值,{},[]
4108,4108,state-action value function,状态-动作值函数,0.75,8,"[{'word': '状态-动作值函数', 'ratio': 0.75}, {'word': '状态-动作价值函数', 'ratio': 0.25}]",状态-动作值函数,{},[]
4109,4109,state-of-the-art baseline,最先进基准,0.0,8,"[{'word': '最先进的基准', 'ratio': 0.625}, {'word': '先进基线', 'ratio': 0.125}, {'word': '最先进基线', 'ratio': 0.125}, {'word': '最先进的基线', 'ratio': 0.125}]",最先进的基准,{},[]
4110,4110,state-of-the-art method,最先进方法,0.125,8,"[{'word': '最先进的方法', 'ratio': 0.75}, {'word': '先进方法', 'ratio': 0.125}, {'word': '最先进方法', 'ratio': 0.125}]",最先进的方法,{},[]
4111,4111,state-of-the-art model,最先进模型,0.125,8,"[{'word': '最先进的模型', 'ratio': 0.75}, {'word': '先进模型', 'ratio': 0.125}, {'word': '最先进模型', 'ratio': 0.125}]",最先进的模型,{},[]
4112,4112,state-of-the-art system,最先进系统,0.125,8,"[{'word': '最先进的系统', 'ratio': 0.75}, {'word': '先进系统', 'ratio': 0.125}, {'word': '最先进系统', 'ratio': 0.125}]",最先进的系统,{},[]
4113,4113,static analysis,静态分析,1.0,8,"[{'word': '静态分析', 'ratio': 1.0}]",静态分析,{},[]
4114,4114,stationarity,平稳性,1.0,8,"[{'word': '平稳性', 'ratio': 1.0}]",平稳性,{},[]
4115,4115,stationary distribution,平稳分布,1.0,8,"[{'word': '平稳分布', 'ratio': 1.0}]",平稳分布,{},[]
4116,4116,stationary kernel,平稳核,0.875,8,"[{'word': '平稳核', 'ratio': 0.875}, {'word': '平稳分布', 'ratio': 0.125}]",平稳核,{},[]
4117,4117,stationary policy,静态策略,0.0,8,"[{'word': '平稳策略', 'ratio': 1.0}]",平稳策略,{},[]
4118,4118,statistical analysis,统计分析,1.0,7,"[{'word': '统计分析', 'ratio': 1.0}]",统计分析,{},[]
4119,4119,statistical independence,统计独立性,1.0,7,"[{'word': '统计独立性', 'ratio': 1.0}]",统计独立性,{},[]
4120,4120,statistical learning,统计学习,1.0,7,"[{'word': '统计学习', 'ratio': 1.0}]",统计学习,{},[]
4121,4121,statistical learning algorithm,统计学习算法,1.0,7,"[{'word': '统计学习算法', 'ratio': 1.0}]",统计学习算法,{},[]
4122,4122,statistical learning theory,统计学习理论,1.0,10,"[{'word': '统计学习理论', 'ratio': 1.0}]",统计学习理论,{},[]
4123,4123,statistical machine translation system,统计机器翻译系统,1.0,10,"[{'word': '统计机器翻译系统', 'ratio': 1.0}]",统计机器翻译系统,{},[]
4124,4124,statistical measure,统计量,0.3,10,"[{'word': '统计度量', 'ratio': 0.6}, {'word': '统计量', 'ratio': 0.3}, {'word': '统计量度', 'ratio': 0.1}]",统计度量,{},[]
4125,4125,statistical model,统计模型,1.0,10,"[{'word': '统计模型', 'ratio': 1.0}]",统计模型,{},[]
4126,4126,statistical translation model,统计翻译模型,1.0,10,"[{'word': '统计翻译模型', 'ratio': 1.0}]",统计翻译模型,{},[]
4127,4127,steepest descent,陡峭下降,0.0,6,"[{'word': '最速下降法', 'ratio': 0.8333333333333334}, {'word': '最陡下降', 'ratio': 0.16666666666666666}]",最速下降法,{},[]
4128,4128,steerable filter,可定向滤波器 (steerable filter),0.0,6,"[{'word': '可操控滤波器', 'ratio': 0.5}, {'word': '可引导滤波器', 'ratio': 0.3333333333333333}, {'word': '可控滤波器', 'ratio': 0.16666666666666666}]",可操控滤波器,{},[]
4129,4129,stemmer,词干提取器,1.0,6,"[{'word': '词干提取器', 'ratio': 1.0}]",词干提取器,{},[]
4130,4130,stereo algorithm,立体视觉算法,0.5,6,"[{'word': '立体算法', 'ratio': 0.5}, {'word': '立体视觉算法', 'ratio': 0.5}]",立体算法,{},[]
4131,4131,stereo benchmark,立体基准测试,0.5,6,"[{'word': '立体基准测试', 'ratio': 0.5}, {'word': '立体视觉基准测试', 'ratio': 0.5}]",立体基准测试,{},[]
4132,4132,stereo disparity,双目视差,0.0,9,"[{'word': '立体视差', 'ratio': 1.0}]",立体视差,{},[]
4133,4133,stereo image,立体影像,0.0,9,"[{'word': '立体图像', 'ratio': 1.0}]",立体图像,{},[]
4134,4134,stereo matching,立体匹配,1.0,9,"[{'word': '立体匹配', 'ratio': 1.0}]",立体匹配,{},[]
4135,4135,stereo pair,立体对,0.7777777777777778,9,"[{'word': '立体对', 'ratio': 0.7777777777777778}, {'word': '立体像对', 'ratio': 0.1111111111111111}, {'word': '立体图像对', 'ratio': 0.1111111111111111}]",立体对,{},[]
4136,4136,stereo reconstruction,立体重建,1.0,9,"[{'word': '立体重建', 'ratio': 1.0}]",立体重建,{},[]
4137,4137,stereo vision,双目视觉,0.0,8,"[{'word': '立体视觉', 'ratio': 1.0}]",立体视觉,{},[]
4138,4138,stochastic algorithm,随机算法,1.0,8,"[{'word': '随机算法', 'ratio': 1.0}]",随机算法,{},[]
4139,4139,stochastic approximation,随机逼近,0.625,8,"[{'word': '随机逼近', 'ratio': 0.625}, {'word': '随机近似', 'ratio': 0.375}]",随机逼近,{},[]
4140,4140,stochastic depth,随机深度,0.875,8,"[{'word': '随机深度', 'ratio': 0.875}, {'word': '随机逼近', 'ratio': 0.125}]",随机深度,{},[]
4141,4141,stochastic differential equation,随机微分方程,1.0,8,"[{'word': '随机微分方程', 'ratio': 1.0}]",随机微分方程,{},[]
4142,4142,stochastic dynamic,随机动力学,0.4444444444444444,9,"[{'word': '随机动态', 'ratio': 0.5555555555555556}, {'word': '随机动力学', 'ratio': 0.4444444444444444}]",随机动态,{},[]
4143,4143,stochastic environment,随机环境,1.0,9,"[{'word': '随机环境', 'ratio': 1.0}]",随机环境,{},[]
4144,4144,stochastic game,随机博弈,0.8888888888888888,9,"[{'word': '随机博弈', 'ratio': 0.8888888888888888}, {'word': '随机游戏', 'ratio': 0.1111111111111111}]",随机博弈,{},[]
4145,4145,stochastic gradient,随机梯度,1.0,9,"[{'word': '随机梯度', 'ratio': 1.0}]",随机梯度,{},[]
4146,4146,stochastic gradient algorithm,随机梯度算法,1.0,9,"[{'word': '随机梯度算法', 'ratio': 1.0}]",随机梯度算法,{},[]
4147,4147,stochastic gradient ascent,随机梯度上升法,0.0,6,"[{'word': '随机梯度上升', 'ratio': 1.0}]",随机梯度上升,{},[]
4148,4148,stochastic gradient method,随机梯度方法,0.8333333333333334,6,"[{'word': '随机梯度方法', 'ratio': 0.8333333333333334}, {'word': '随机梯度法', 'ratio': 0.16666666666666666}]",随机梯度方法,{},[]
4149,4149,stochastic grammar,随机语法,0.3333333333333333,6,"[{'word': '随机文法', 'ratio': 0.6666666666666666}, {'word': '随机语法', 'ratio': 0.3333333333333333}]",随机文法,{},[]
4150,4150,stochastic matrix,随机矩阵,1.0,6,"[{'word': '随机矩阵', 'ratio': 1.0}]",随机矩阵,{},[]
4151,4151,stochastic model,随机模型,1.0,6,"[{'word': '随机模型', 'ratio': 1.0}]",随机模型,{},[]
4152,4152,stochastic objective,随机目标函数,0.4285714285714285,7,"[{'word': '随机目标', 'ratio': 0.5714285714285714}, {'word': '随机目标函数', 'ratio': 0.42857142857142855}]",随机目标,{},[]
4153,4153,stochastic optimization,随机优化,1.0,7,"[{'word': '随机优化', 'ratio': 1.0}]",随机优化,{},[]
4154,4154,stochastic policy,随机策略,1.0,7,"[{'word': '随机策略', 'ratio': 1.0}]",随机策略,{},[]
4155,4155,stochastic process,随机过程,1.0,7,"[{'word': '随机过程', 'ratio': 1.0}]",随机过程,{},[]
4156,4156,stochastic sampling,随机采样,1.0,7,"[{'word': '随机采样', 'ratio': 1.0}]",随机采样,{},[]
4157,4157,stochastic search algorithm,随机搜索算法,1.0,10,"[{'word': '随机搜索算法', 'ratio': 1.0}]",随机搜索算法,{},[]
4158,4158,stochastic subgradient descent,随机次梯度下降,0.7,10,"[{'word': '随机次梯度下降', 'ratio': 0.7}, {'word': '随机子梯度下降', 'ratio': 0.3}]",随机次梯度下降,{},[]
4159,4159,stochastic transition matrix,随机转移矩阵,1.0,10,"[{'word': '随机转移矩阵', 'ratio': 1.0}]",随机转移矩阵,{},[]
4160,4160,stochastic variational inference,随机变分推断,1.0,10,"[{'word': '随机变分推断', 'ratio': 1.0}]",随机变分推断,{},[]
4161,4161,stochasticity,随机性,1.0,10,"[{'word': '随机性', 'ratio': 1.0}]",随机性,{},[]
4162,4162,stop word,停用词,1.0,6,"[{'word': '停用词', 'ratio': 1.0}]",停用词,{},[]
4163,4163,stop-gradient,停止梯度传播,0.0,6,"[{'word': '停止梯度', 'ratio': 0.6666666666666666}, {'word': '停梯度', 'ratio': 0.16666666666666666}, {'word': '梯度截止', 'ratio': 0.16666666666666666}]",停止梯度,{},[]
4164,4164,stop-gradient operation,停止梯度操作,0.6666666666666666,6,"[{'word': '停止梯度操作', 'ratio': 0.6666666666666666}, {'word': '停梯度操作', 'ratio': 0.16666666666666666}, {'word': '梯度截止操作', 'ratio': 0.16666666666666666}]",停止梯度操作,{},[]
4165,4165,stopping condition,终止条件,0.0,6,"[{'word': '停止条件', 'ratio': 1.0}]",停止条件,{},[]
4166,4166,stopping criterion,停止准则,0.6666666666666666,6,"[{'word': '停止准则', 'ratio': 0.6666666666666666}, {'word': '停止标准', 'ratio': 0.3333333333333333}]",停止准则,{},[]
4167,4167,stratified sampling,分层抽样,1.0,6,"[{'word': '分层抽样', 'ratio': 1.0}]",分层抽样,{},[]
4168,4168,streaming algorithm,数据流算法,0.0,6,"[{'word': '流式算法', 'ratio': 0.8333333333333334}, {'word': '流处理算法', 'ratio': 0.16666666666666666}]",流式算法,{},[]
4169,4169,streaming datum,流式数据,0.6666666666666666,6,"[{'word': '流式数据', 'ratio': 0.6666666666666666}, {'word': '流数据', 'ratio': 0.3333333333333333}]",流式数据,{},[]
4170,4170,streaming model,流式模型,0.6666666666666666,6,"[{'word': '流式模型', 'ratio': 0.6666666666666666}, {'word': '流模型', 'ratio': 0.3333333333333333}]",流式模型,{},[]
4171,4171,stride,步幅,0.6666666666666666,6,"[{'word': '步幅', 'ratio': 0.6666666666666666}, {'word': '步长', 'ratio': 0.3333333333333333}]",步幅,{},[]
4172,4172,string kernel metric,字符串核度量,0.8,5,"[{'word': '字符串核度量', 'ratio': 0.8}, {'word': '字符串核度量结构学习', 'ratio': 0.2}]",字符串核度量,{},[]
4173,4173,structural learning,结构学习,1.0,5,"[{'word': '结构学习', 'ratio': 1.0}]",结构学习,{},[]
4174,4174,structural risk minimization,结构风险最小化,1.0,5,"[{'word': '结构风险最小化', 'ratio': 1.0}]",结构风险最小化,{},[]
4175,4175,structure learning,结构学习,1.0,5,"[{'word': '结构学习', 'ratio': 1.0}]",结构学习,{},[]
4176,4176,structured datum,结构化数据,1.0,7,"[{'word': '结构化数据', 'ratio': 1.0}]",结构化数据,{},[]
4177,4177,structured output,结构化输出,1.0,7,"[{'word': '结构化输出', 'ratio': 1.0}]",结构化输出,{},[]
4178,4178,structured perceptron,结构化感知器,0.7142857142857143,7,"[{'word': '结构化感知器', 'ratio': 0.7142857142857143}, {'word': '结构感知机', 'ratio': 0.14285714285714285}, {'word': '结构化感知机', 'ratio': 0.14285714285714285}]",结构化感知器,{},[]
4179,4179,structured prediction,结构化预测,1.0,7,"[{'word': '结构化预测', 'ratio': 1.0}]",结构化预测,{},[]
4180,4180,structured prediction model,结构化预测模型,1.0,4,"[{'word': '结构化预测模型', 'ratio': 1.0}]",结构化预测模型,{},[]
4181,4181,structured prediction problem,结构化预测问题,1.0,4,"[{'word': '结构化预测问题', 'ratio': 1.0}]",结构化预测问题,{},[]
4182,4182,structured support vector machine,结构化支持向量机,1.0,4,"[{'word': '结构化支持向量机', 'ratio': 1.0}]",结构化支持向量机,{},[]
4183,4183,student model,学生模型,1.0,4,"[{'word': '学生模型', 'ratio': 1.0}]",学生模型,{},[]
4184,4184,style transfer,风格迁移,1.0,4,"[{'word': '风格迁移', 'ratio': 1.0}]",风格迁移,{},[]
4185,4185,sub-gradient,次梯度,1.0,8,"[{'word': '次梯度', 'ratio': 1.0}]",次梯度,{},[]
4186,4186,sub-gradient descent,次梯度下降,1.0,8,"[{'word': '次梯度下降', 'ratio': 1.0}]",次梯度下降,{},[]
4187,4187,sub-networks,子网络,1.0,8,"[{'word': '子网络', 'ratio': 1.0}]",子网络,{},[]
4188,4188,sub-population,子群体,1.0,8,"[{'word': '子群体', 'ratio': 1.0}]",子群体,{},[]
4189,4189,sub-word,亚词,0.0,8,"[{'word': '子词', 'ratio': 1.0}]",子词,{},[]
4190,4190,sub-word tokenization,子词分词,0.7,10,"[{'word': '子词分词', 'ratio': 0.7}, {'word': '子词标记化', 'ratio': 0.3}]",子词分词,{},[]
4191,4191,subgame,子博弈,0.7,10,"[{'word': '子博弈', 'ratio': 0.7}, {'word': '子游戏', 'ratio': 0.3}]",子博弈,{},[]
4192,4192,subgradient method,次梯度法,0.8,10,"[{'word': '次梯度法', 'ratio': 0.8}, {'word': '次梯度方法', 'ratio': 0.1}, {'word': '子梯度法', 'ratio': 0.1}]",次梯度法,{},[]
4193,4193,subgraph isomorphism,子图同构,1.0,10,"[{'word': '子图同构', 'ratio': 1.0}]",子图同构,{},[]
4194,4194,subgraph selection,子图选择,1.0,10,"[{'word': '子图选择', 'ratio': 1.0}]",子图选择,{},[]
4195,4195,submatrice,子矩阵,1.0,8,"[{'word': '子矩阵', 'ratio': 1.0}]",子矩阵,{},[]
4196,4196,submatrix,子矩阵,1.0,8,"[{'word': '子矩阵', 'ratio': 1.0}]",子矩阵,{},[]
4197,4197,submodular,次调和的,0.0,8,"[{'word': '次模的', 'ratio': 0.25}, {'word': '次模', 'ratio': 0.25}, {'word': '子模块', 'ratio': 0.125}, {'word': '非次模', 'ratio': 0.125}, {'word': '子模量', 'ratio': 0.125}, {'word': '子模', 'ratio': 0.125}]",次模的,"1. Rank: 次模的, 次模, 子模, 子模块, 子模量, 非次模

2. Explanation: The term ""次模的"" (submodular) is the best fit because it accurately captures the semantic meaning of the English term ""submodular"" in the context of optimization and graph theory. In AI and mathematical contexts, ""次模的"" is commonly used to refer to properties of functions or structures that exhibit submodularity, which is crucial for understanding optimization problems. 

The back translation of ""次模的"" to ""submodular"" aligns perfectly with the original term, ensuring semantic accuracy. The other candidates, such as ""次模"" (secondary mode) and ""子模块"" (submodule), do not convey the same meaning and could lead to confusion in the AI domain. ""子模"" (sub-model) and ""子模量"" (submodulus) also deviate from the intended meaning, while ""非次模"" (non-submodule) is not relevant as it refers to the negation of the term. Therefore, ""次模的"" is the most contextually appropriate choice for the AI terminology in question.","['submodular', 'secondary mode', 'submodule', 'non-submodule', 'submodulus', 'sub-model']"
4198,4198,submodular function,子模函数,0.25,8,"[{'word': '次模函数', 'ratio': 0.625}, {'word': '子模函数', 'ratio': 0.25}, {'word': '子模块函数', 'ratio': 0.125}]",次模函数,{},[]
4199,4199,submodular function optimization,次模函数优化,0.625,8,"[{'word': '次模函数优化', 'ratio': 0.625}, {'word': '子模函数优化', 'ratio': 0.25}, {'word': '子模块函数优化', 'ratio': 0.125}]",次模函数优化,{},[]
4200,4200,submodular influence function,子模影响函数,0.3333333333333333,6,"[{'word': '次模影响函数', 'ratio': 0.6666666666666666}, {'word': '子模影响函数', 'ratio': 0.3333333333333333}]",次模影响函数,{},[]
4201,4201,submodular optimization,子模优化,0.3333333333333333,6,"[{'word': '次模优化', 'ratio': 0.6666666666666666}, {'word': '子模优化', 'ratio': 0.3333333333333333}]",次模优化,{},[]
4202,4202,submodular polyhedron,次模多面体,0.6666666666666666,6,"[{'word': '次模多面体', 'ratio': 0.6666666666666666}, {'word': '子模多面体', 'ratio': 0.3333333333333333}]",次模多面体,{},[]
4203,4203,submodular set function,次模集合函数,0.6666666666666666,6,"[{'word': '次模集合函数', 'ratio': 0.6666666666666666}, {'word': '子模集合函数', 'ratio': 0.3333333333333333}]",次模集合函数,{},[]
4204,4204,subnetwork,子网络,1.0,6,"[{'word': '子网络', 'ratio': 1.0}]",子网络,{},[]
4205,4205,suboptimal,次优的,0.2857142857142857,7,"[{'word': '次优', 'ratio': 0.7142857142857143}, {'word': '次优的', 'ratio': 0.2857142857142857}]",次优,{},[]
4206,4206,subpixel,亚像素,0.7142857142857143,7,"[{'word': '亚像素', 'ratio': 0.7142857142857143}, {'word': '子像素', 'ratio': 0.2857142857142857}]",亚像素,{},[]
4207,4207,subsample,subsample,0.0,7,"[{'word': '子抽样', 'ratio': 0.2857142857142857}, {'word': '子样本', 'ratio': 0.2857142857142857}, {'word': '子采样', 'ratio': 0.2857142857142857}, {'word': '子样本 / 采样子集', 'ratio': 0.14285714285714285}]",子样本,"1. Rank: 子样本, 子采样, 子抽样, 子样本 / 采样子集

2. Explanation: The term ""子样本"" (subsample) is the best fit because it directly translates to ""subsample,"" which is a commonly used term in the AI and machine learning domains. It accurately reflects the concept of taking a subset of data from a larger dataset, which is essential in various algorithms, particularly in the context of balancing classes or reducing dataset size for training. 

""子采样"" (subsampling) is also a valid term, but it is slightly less precise in this context as it can imply a broader process of sampling rather than specifically referring to a subset. ""子抽样"" (subsampling) is less commonly used in the AI context and may not be as immediately recognizable to practitioners. Lastly, ""子样本 / 采样子集"" (subsample/sampling subset) is overly verbose and could lead to confusion, as it combines two concepts that are not necessary to distinguish in this context. Therefore, ""子样本"" is the most semantically accurate and contextually appropriate choice.","['subsampling', 'subsample', 'subsampling', 'subsample/sampling subset']"
4208,4208,subsampling factor,下采样因子,0.0,7,"[{'word': '子采样因子', 'ratio': 0.42857142857142855}, {'word': '子抽样因子', 'ratio': 0.2857142857142857}, {'word': '采样因子 / 子采样因子', 'ratio': 0.14285714285714285}, {'word': '二次采样因子', 'ratio': 0.14285714285714285}]",子采样因子,"1. Rank: 子采样因子, 子抽样因子, 采样因子 / 子采样因子, 二次采样因子

2. Explanation: The term ""子采样因子"" (subsampling factor) is the best fit because it accurately reflects the concept of subsampling in the context of AI and signal processing. The prefix ""子"" (meaning ""sub"" or ""child"") effectively conveys the idea of a subset or a smaller portion of the original sample, which is essential in the context of subsampling. 

The term ""子抽样因子"" is also a valid candidate, but ""抽样"" (sampling) is a broader term that may not specifically imply the reduction aspect as clearly as ""采样"" (which is more commonly associated with sampling in a statistical sense). 

The term ""采样因子 / 子采样因子"" introduces ambiguity by combining both terms, which could confuse the reader about whether it refers to general sampling or specifically to subsampling. 

Lastly, ""二次采样因子"" (secondary sampling factor) suggests a two-step sampling process, which is not the intended meaning of ""subsampling"" in this context. Therefore, ""子采样因子"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['subsampling factor', 'subsampling factor', 'Sampling factor / subsampling factor', 'subsampling factor']"
4209,4209,subspace learning,子空间学习,1.0,7,"[{'word': '子空间学习', 'ratio': 1.0}]",子空间学习,{},[]
4210,4210,subspace method,子空间方法,1.0,5,"[{'word': '子空间方法', 'ratio': 1.0}]",子空间方法,{},[]
4211,4211,subspace projection,子空间投影,1.0,5,"[{'word': '子空间投影', 'ratio': 1.0}]",子空间投影,{},[]
4212,4212,substitution,替换,0.8,5,"[{'word': '替换', 'ratio': 0.8}, {'word': '替代', 'ratio': 0.2}]",替换,{},[]
4213,4213,subsumption,包含,0.4,5,"[{'word': '包含关系', 'ratio': 0.6}, {'word': '包含', 'ratio': 0.4}]",包含关系,{},[]
4214,4214,subsumption relation,子类包含关系,0.0,5,"[{'word': '包含关系', 'ratio': 0.8}, {'word': '包含关系 如果需要进一步的帮助，请随时告诉我！', 'ratio': 0.2}]",包含关系,{},[]
4215,4215,subtree,子树,1.0,6,"[{'word': '子树', 'ratio': 1.0}]",子树,{},[]
4216,4216,subwindow,子窗口,1.0,6,"[{'word': '子窗口', 'ratio': 1.0}]",子窗口,{},[]
4217,4217,subword token,子词标记,1.0,6,"[{'word': '子词标记', 'ratio': 1.0}]",子词标记,{},[]
4218,4218,subword unit,子词单元,0.8333333333333334,6,"[{'word': '子词单元', 'ratio': 0.8333333333333334}, {'word': '子词单元 如果有其他问题或需要更多帮助，请告诉我！', 'ratio': 0.16666666666666666}]",子词单元,{},[]
4219,4219,successor function,后继函数,1.0,10,"[{'word': '后继函数', 'ratio': 1.0}]",后继函数,{},[]
4220,4220,successor state,后继状态,1.0,10,"[{'word': '后继状态', 'ratio': 1.0}]",后继状态,{},[]
4221,4221,successor state axiom,后继状态公理,1.0,10,"[{'word': '后继状态公理', 'ratio': 1.0}]",后继状态公理,{},[]
4222,4222,sufficient statistic,充分统计量,0.7,10,"[{'word': '充分统计量', 'ratio': 0.7}, {'word': '充足统计量', 'ratio': 0.3}]",充分统计量,{},[]
4223,4223,suffix tree,后缀树,1.0,10,"[{'word': '后缀树', 'ratio': 1.0}]",后缀树,{},[]
4224,4224,summarization,摘要生成,0.4166666666666667,12,"[{'word': '摘要生成', 'ratio': 0.75}, {'word': '摘要', 'ratio': 0.16666666666666666}, {'word': '长文本摘要', 'ratio': 0.08333333333333333}]",摘要生成,{},[]
4225,4225,summarization algorithm,摘要算法,0.3333333333333333,6,"[{'word': '摘要生成算法', 'ratio': 0.6666666666666666}, {'word': '摘要算法', 'ratio': 0.3333333333333333}]",摘要生成算法,{},[]
4226,4226,summarization model,摘要模型,0.3333333333333333,6,"[{'word': '摘要生成模型', 'ratio': 0.6666666666666666}, {'word': '摘要模型', 'ratio': 0.3333333333333333}]",摘要生成模型,{},[]
4227,4227,summarization system,摘要系统,0.3333333333333333,6,"[{'word': '摘要生成系统', 'ratio': 0.6666666666666666}, {'word': '摘要系统', 'ratio': 0.3333333333333333}]",摘要生成系统,{},[]
4228,4228,super-pixel,超像素,1.0,5,"[{'word': '超像素', 'ratio': 1.0}]",超像素,{},[]
4229,4229,super-resolution,超分辨率,1.0,5,"[{'word': '超分辨率', 'ratio': 1.0}]",超分辨率,{},[]
4230,4230,supergradient,超梯度,1.0,5,"[{'word': '超梯度', 'ratio': 1.0}]",超梯度,{},[]
4231,4231,supertag,超标签,0.8,5,"[{'word': '超标签', 'ratio': 0.8}, {'word': '超标记', 'ratio': 0.2}]",超标签,{},[]
4232,4232,supervised classification,监督分类,1.0,6,"[{'word': '监督分类', 'ratio': 1.0}]",监督分类,{},[]
4233,4233,supervised classification model,监督分类模型,1.0,6,"[{'word': '监督分类模型', 'ratio': 1.0}]",监督分类模型,{},[]
4234,4234,supervised classifier,有监督分类器,0.0,6,"[{'word': '监督分类器', 'ratio': 1.0}]",监督分类器,{},[]
4235,4235,supervised contrastive learning,监督对比学习,1.0,6,"[{'word': '监督对比学习', 'ratio': 1.0}]",监督对比学习,{},[]
4236,4236,supervised datum,有监督数据,0.0,6,"[{'word': '监督数据', 'ratio': 1.0}]",监督数据,{},[]
4237,4237,supervised finetuning,监督微调,0.3333333333333333,6,"[{'word': '有监督微调', 'ratio': 0.6666666666666666}, {'word': '监督微调', 'ratio': 0.3333333333333333}]",有监督微调,{},[]
4238,4238,supervised learning,监督学习,0.3333333333333333,6,"[{'word': '有监督学习', 'ratio': 0.6666666666666666}, {'word': '监督学习', 'ratio': 0.3333333333333333}]",有监督学习,{},[]
4239,4239,supervised manner,监督方式,0.3333333333333333,6,"[{'word': '有监督方式', 'ratio': 0.5}, {'word': '监督方式', 'ratio': 0.3333333333333333}, {'word': '有监督的方式', 'ratio': 0.16666666666666666}]",有监督方式,{},[]
4240,4240,supervised method,监督方法,0.1666666666666666,6,"[{'word': '有监督方法', 'ratio': 0.6666666666666666}, {'word': '监督方法', 'ratio': 0.16666666666666666}, {'word': '监督法', 'ratio': 0.16666666666666666}]",有监督方法,{},[]
4241,4241,supervised model,监督模型,0.3333333333333333,6,"[{'word': '有监督模型', 'ratio': 0.6666666666666666}, {'word': '监督模型', 'ratio': 0.3333333333333333}]",有监督模型,{},[]
4242,4242,supervised multi-task learning,监督多任务学习,0.5,6,"[{'word': '监督多任务学习', 'ratio': 0.5}, {'word': '有监督的多任务学习', 'ratio': 0.3333333333333333}, {'word': '监督式多任务学习', 'ratio': 0.16666666666666666}]",监督多任务学习,{},[]
4243,4243,supervised setting,监督设置,0.6666666666666666,6,"[{'word': '监督设置', 'ratio': 0.6666666666666666}, {'word': '有监督的设置', 'ratio': 0.3333333333333333}]",监督设置,{},[]
4244,4244,supervised system,监督系统,0.6666666666666666,6,"[{'word': '监督系统', 'ratio': 0.6666666666666666}, {'word': '有监督的系统', 'ratio': 0.3333333333333333}]",监督系统,{},[]
4245,4245,supervised training,监督训练,0.6666666666666666,6,"[{'word': '监督训练', 'ratio': 0.6666666666666666}, {'word': '有监督的训练', 'ratio': 0.3333333333333333}]",监督训练,{},[]
4246,4246,support,支持度,0.6666666666666666,6,"[{'word': '支持度', 'ratio': 0.6666666666666666}, {'word': '支持', 'ratio': 0.3333333333333333}]",支持度,{},[]
4247,4247,support set,支持集,1.0,5,"[{'word': '支持集', 'ratio': 1.0}]",支持集,{},[]
4248,4248,support threshold,支持度阈值,0.2,5,"[{'word': '支持阈值', 'ratio': 0.8}, {'word': '支持度阈值', 'ratio': 0.2}]",支持阈值,{},[]
4249,4249,support vector,支持向量,0.8,5,"[{'word': '支持向量', 'ratio': 0.8}, {'word': '支持阈值', 'ratio': 0.2}]",支持向量,{},[]
4250,4250,surface normal,表面法线,0.8,5,"[{'word': '表面法线', 'ratio': 0.8}, {'word': '支持向量', 'ratio': 0.2}]",表面法线,{},[]
4251,4251,surface normal estimator,表面法向量估计器,0.0,5,"[{'word': '表面法线估计器', 'ratio': 0.8}, {'word': '表面法线', 'ratio': 0.2}]",表面法线估计器,{},[]
4252,4252,surface normal prediction,表面法线预测,1.0,8,"[{'word': '表面法线预测', 'ratio': 1.0}]",表面法线预测,{},[]
4253,4253,surface realization,表面实现,0.625,8,"[{'word': '表面实现', 'ratio': 0.625}, {'word': '表层实现', 'ratio': 0.375}]",表面实现,{},[]
4254,4254,surrogate,替代物,0.375,8,"[{'word': '代理', 'ratio': 0.5}, {'word': '替代物', 'ratio': 0.375}, {'word': '替代', 'ratio': 0.125}]",代理,{},[]
4255,4255,surrogate function,替代函数,0.5,8,"[{'word': '替代函数', 'ratio': 0.5}, {'word': '代理函数', 'ratio': 0.5}]",替代函数,{},[]
4256,4256,surrogate loss,替代损失,0.5,8,"[{'word': '替代损失', 'ratio': 0.5}, {'word': '代理损失', 'ratio': 0.5}]",替代损失,{},[]
4257,4257,surrogate loss function,替代损失函数,0.4285714285714285,7,"[{'word': '代理损失函数', 'ratio': 0.5714285714285714}, {'word': '替代损失函数', 'ratio': 0.42857142857142855}]",代理损失函数,{},[]
4258,4258,surrogate model,代理模型,0.5714285714285714,7,"[{'word': '代理模型', 'ratio': 0.5714285714285714}, {'word': '替代模型', 'ratio': 0.42857142857142855}]",代理模型,{},[]
4259,4259,symbol grounding problem,符号接地问题,0.4285714285714285,7,"[{'word': '符号接地问题', 'ratio': 0.42857142857142855}, {'word': '符号扎根问题', 'ratio': 0.14285714285714285}, {'word': '符号嵌入问题', 'ratio': 0.14285714285714285}, {'word': '符号基础问题', 'ratio': 0.14285714285714285}, {'word': '符号定位问题', 'ratio': 0.14285714285714285}]",符号扎根问题,"1. Rank: 符号扎根问题, 符号接地问题, 符号嵌入问题, 符号定位问题, 符号基础问题

2. Explanation: The term ""符号扎根问题"" (Symbol grounding problem) is the best fit because it accurately captures the essence of the original English term while maintaining semantic integrity. The word ""扎根"" (to take root) conveys the idea of establishing a connection between symbols and their meanings, which is central to the concept of symbol grounding in AI. This term is widely recognized in the AI research community, particularly in discussions about how machines can understand and generate human language based on contextual meanings.

The second candidate, ""符号接地问题"" (Symbol grounding issues), while also a reasonable translation, uses ""接地"" (to ground) which may not be as commonly used in this specific context as ""扎根."" The other candidates, such as ""符号嵌入问题"" (Symbol embedding problem) and ""符号定位问题"" (Symbol positioning problem), introduce different concepts that do not align with the original meaning of grounding symbols in a cognitive or AI context. ""符号基础问题"" (Symbol basic issues) is too vague and does not convey the specific challenge of grounding symbols in meaning. Thus, ""符号扎根问题"" stands out as the most contextually appropriate and semantically accurate translation.","['Symbol grounding issues', 'Symbol grounding problem', 'Symbol embedding problem', 'Symbol basic issues', 'Symbol positioning problem']"
4260,4260,symbolic representation,符号表示,1.0,7,"[{'word': '符号表示', 'ratio': 1.0}]",符号表示,{},[]
4261,4261,symmetric matrix,对称矩阵,1.0,7,"[{'word': '对称矩阵', 'ratio': 1.0}]",对称矩阵,{},[]
4262,4262,symmetric positive semidefinite matrix,对称半正定矩阵,0.7777777777777778,9,"[{'word': '对称半正定矩阵', 'ratio': 0.7777777777777778}, {'word': '对称正半定矩阵', 'ratio': 0.2222222222222222}]",对称半正定矩阵,{},[]
4263,4263,symmetrization,对称化,1.0,9,"[{'word': '对称化', 'ratio': 1.0}]",对称化,{},[]
4264,4264,synchronous context-free grammar,同步上下文无关语法,0.0,9,"[{'word': '同步上下文无关文法', 'ratio': 0.7777777777777778}, {'word': '同步无上下文文法', 'ratio': 0.2222222222222222}]",同步上下文无关文法,{},[]
4265,4265,synonymy,同义性,0.1111111111111111,9,"[{'word': '同义关系', 'ratio': 0.8888888888888888}, {'word': '同义性', 'ratio': 0.1111111111111111}]",同义关系,{},[]
4266,4266,synset,同义词集,0.8888888888888888,9,"[{'word': '同义词集', 'ratio': 0.8888888888888888}, {'word': '同义词集合', 'ratio': 0.1111111111111111}]",同义词集,{},[]
4267,4267,syntactic analysis,句法分析,1.0,6,"[{'word': '句法分析', 'ratio': 1.0}]",句法分析,{},[]
4268,4268,syntactic category,句法类别,1.0,6,"[{'word': '句法类别', 'ratio': 1.0}]",句法类别,{},[]
4269,4269,syntactic constraint,句法约束,1.0,6,"[{'word': '句法约束', 'ratio': 1.0}]",句法约束,{},[]
4270,4270,syntactic dependency,句法依赖,0.6666666666666666,6,"[{'word': '句法依赖', 'ratio': 0.6666666666666666}, {'word': '句法依存', 'ratio': 0.3333333333333333}]",句法依赖,{},[]
4271,4271,syntactic dependency parsing,句法依存分析,0.3333333333333333,6,"[{'word': '句法依赖解析', 'ratio': 0.5}, {'word': '句法依存分析', 'ratio': 0.3333333333333333}, {'word': '句法依存解析', 'ratio': 0.16666666666666666}]",句法依赖解析,{},[]
4272,4272,syntactic dependency tree,句法依赖树,0.5,6,"[{'word': '句法依存树', 'ratio': 0.5}, {'word': '句法依赖树', 'ratio': 0.5}]",句法依存树,{},[]
4273,4273,syntactic feature,句法特征,1.0,6,"[{'word': '句法特征', 'ratio': 1.0}]",句法特征,{},[]
4274,4274,syntactic information,句法信息,1.0,6,"[{'word': '句法信息', 'ratio': 1.0}]",句法信息,{},[]
4275,4275,syntactic parse,句法分析,1.0,6,"[{'word': '句法分析', 'ratio': 1.0}]",句法分析,{},[]
4276,4276,syntactic parser,句法分析器,0.5,8,"[{'word': '句法解析器', 'ratio': 0.5}, {'word': '句法分析器', 'ratio': 0.5}]",句法解析器,{},[]
4277,4277,syntactic regularity,句法规律性,0.5,8,"[{'word': '句法规律', 'ratio': 0.5}, {'word': '句法规律性', 'ratio': 0.5}]",句法规律,{},[]
4278,4278,syntactic representation,句法表示,1.0,8,"[{'word': '句法表示', 'ratio': 1.0}]",句法表示,{},[]
4279,4279,syntactic similarity,句法相似性,1.0,8,"[{'word': '句法相似性', 'ratio': 1.0}]",句法相似性,{},[]
4280,4280,syntactic structure,句法结构,1.0,8,"[{'word': '句法结构', 'ratio': 1.0}]",句法结构,{},[]
4281,4281,syntactic tree,句法树,1.0,7,"[{'word': '句法树', 'ratio': 1.0}]",句法树,{},[]
4282,4282,syntax,句法,0.7142857142857143,14,"[{'word': '句法', 'ratio': 0.7142857142857143}, {'word': '语法', 'ratio': 0.2857142857142857}]",句法,{},[]
4283,4283,syntax tree,句法树,0.8571428571428571,7,"[{'word': '句法树', 'ratio': 0.8571428571428571}, {'word': '语法树', 'ratio': 0.14285714285714285}]",句法树,{},[]
4284,4284,synthetic dataset,合成数据集,1.0,7,"[{'word': '合成数据集', 'ratio': 1.0}]",合成数据集,{},[]
4285,4285,system identification,系统辨识,0.7142857142857143,7,"[{'word': '系统辨识', 'ratio': 0.7142857142857143}, {'word': '系统识别', 'ratio': 0.2857142857142857}]",系统辨识,{},[]
4286,4286,t-test,t检验,0.8571428571428571,7,"[{'word': 't检验', 'ratio': 0.8571428571428571}, {'word': 't 检验', 'ratio': 0.14285714285714285}]",t检验,{},[]
4287,4287,tag recommendation,标签推荐,1.0,7,"[{'word': '标签推荐', 'ratio': 1.0}]",标签推荐,{},[]
4288,4288,tag sequence,标记序列,0.0,7,"[{'word': '标签序列', 'ratio': 1.0}]",标签序列,{},[]
4289,4289,tagger,标注器,0.5,14,"[{'word': '标注器', 'ratio': 0.8571428571428571}, {'word': '词性标注器', 'ratio': 0.14285714285714285}]",标注器,{},[]
4290,4290,tagset,词性标记集,0.0,7,"[{'word': '标签集', 'ratio': 0.8571428571428571}, {'word': '标签集合', 'ratio': 0.14285714285714285}]",标签集,{},[]
4291,4291,tail entity,尾实体,1.0,7,"[{'word': '尾实体', 'ratio': 1.0}]",尾实体,{},[]
4292,4292,tangent space,切线空间,0.0,7,"[{'word': '切空间', 'ratio': 0.7142857142857143}, {'word': '切向空间', 'ratio': 0.2857142857142857}]",切空间,{},[]
4293,4293,tanh activation function,tanh激活函数,0.4285714285714285,7,"[{'word': 'tanh激活函数', 'ratio': 0.42857142857142855}, {'word': '双曲正切激活函数', 'ratio': 0.2857142857142857}, {'word': 'Tanh 活性化関数', 'ratio': 0.14285714285714285}, {'word': 'Tanh激活函数', 'ratio': 0.14285714285714285}]",tanh激活函数,"1. Rank: tanh激活函数, 双曲正切激活函数, Tanh激活函数, Tanh 活性化関数

2. Explanation: The first translation, ""tanh激活函数,"" is the best fit because it accurately retains the original term ""tanh"" while clearly indicating its function as an activation function in the context of neural networks. The term ""激活函数"" directly translates to ""activation function,"" which is a standard term in AI and machine learning literature. 

The second candidate, ""双曲正切激活函数,"" while semantically accurate (it translates to ""hyperbolic tangent activation function""), is less commonly used in practice. In the AI domain, the abbreviation ""tanh"" is widely recognized and used, making the first option more suitable for practitioners and researchers familiar with the terminology.

The third candidate, ""Tanh激活函数,"" is similar to the first but uses a capital ""T"" for ""Tanh,"" which is less conventional in Chinese texts where the term is often presented in lowercase. 

The last candidate, ""Tanh 活性化関数,"" is not appropriate as it mixes languages (using ""関数"" which is Japanese for ""function"") and does not fit the context of the AI domain, making it the least suitable option.","['tanh activation function', 'hyperbolic tangent activation function', 'Tanh activation pass', 'Tanh activation function']"
4294,4294,target,目标,0.7857142857142857,14,"[{'word': '目标', 'ratio': 0.7857142857142857}, {'word': '目標', 'ratio': 0.14285714285714285}, {'word': '目标值', 'ratio': 0.07142857142857142}]",目标,{},[]
4295,4295,target classifier,目标分类器,1.0,8,"[{'word': '目标分类器', 'ratio': 1.0}]",目标分类器,{},[]
4296,4296,target distribution,目标分布,1.0,8,"[{'word': '目标分布', 'ratio': 1.0}]",目标分布,{},[]
4297,4297,target domain,目标域,0.625,8,"[{'word': '目标域', 'ratio': 0.625}, {'word': '目标领域', 'ratio': 0.375}]",目标域,{},[]
4298,4298,target function,目标函数,1.0,8,"[{'word': '目标函数', 'ratio': 1.0}]",目标函数,{},[]
4299,4299,target instance,目标实例,1.0,8,"[{'word': '目标实例', 'ratio': 1.0}]",目标实例,{},[]
4300,4300,target model,目标模型,1.0,6,"[{'word': '目标模型', 'ratio': 1.0}]",目标模型,{},[]
4301,4301,target network,目标网络,1.0,6,"[{'word': '目标网络', 'ratio': 1.0}]",目标网络,{},[]
4302,4302,target node,目标节点,1.0,6,"[{'word': '目标节点', 'ratio': 1.0}]",目标节点,{},[]
4303,4303,target policy,目标策略,1.0,6,"[{'word': '目标策略', 'ratio': 1.0}]",目标策略,{},[]
4304,4304,target sentence,目标句,0.0,6,"[{'word': '目标句子', 'ratio': 0.8333333333333334}, {'word': '目标句子 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.16666666666666666}]",目标句子,{},[]
4305,4305,target sequence,目标序列,1.0,7,"[{'word': '目标序列', 'ratio': 1.0}]",目标序列,{},[]
4306,4306,target task,目标任务,1.0,7,"[{'word': '目标任务', 'ratio': 1.0}]",目标任务,{},[]
4307,4307,target token,目标词元,0.1428571428571428,7,"[{'word': '目标标记', 'ratio': 0.8571428571428571}, {'word': '目标词元', 'ratio': 0.14285714285714285}]",目标标记,{},[]
4308,4308,target variable,目标变量,1.0,7,"[{'word': '目标变量', 'ratio': 1.0}]",目标变量,{},[]
4309,4309,target vector,目标向量,0.8571428571428571,7,"[{'word': '目标向量', 'ratio': 0.8571428571428571}, {'word': '目标向量 如果您有其他问题或需要进一步的帮助，请告诉我！', 'ratio': 0.14285714285714285}]",目标向量,{},[]
4310,4310,target vocabulary,目标词汇,0.6666666666666666,9,"[{'word': '目标词汇', 'ratio': 0.6666666666666666}, {'word': '目标词汇表', 'ratio': 0.3333333333333333}]",目标词汇,{},[]
4311,4311,target word,目标词,1.0,9,"[{'word': '目标词', 'ratio': 1.0}]",目标词,{},[]
4312,4312,target-to-source model,目标到源语言模型,0.0,9,"[{'word': '目标到源模型', 'ratio': 0.8888888888888888}, {'word': '从目标到源模型', 'ratio': 0.1111111111111111}]",目标到源模型,{},[]
4313,4313,task,任务,0.9444444444444444,18,"[{'word': '任务', 'ratio': 0.9444444444444444}, {'word': '研究任务', 'ratio': 0.05555555555555555}]",任务,{},[]
4314,4314,task adaptation,任务适应,1.0,6,"[{'word': '任务适应', 'ratio': 1.0}]",任务适应,{},[]
4315,4315,task model,任务模型,1.0,6,"[{'word': '任务模型', 'ratio': 1.0}]",任务模型,{},[]
4316,4316,task-oriented dialog system,任务导向对话系统,0.6666666666666666,6,"[{'word': '任务导向对话系统', 'ratio': 0.6666666666666666}, {'word': '面向任务的对话系统', 'ratio': 0.3333333333333333}]",任务导向对话系统,{},[]
4317,4317,task-oriented dialogue system,面向任务对话系统,0.0,6,"[{'word': '任务导向对话系统', 'ratio': 0.6666666666666666}, {'word': '面向任务的对话系统', 'ratio': 0.3333333333333333}]",任务导向对话系统,{},[]
4318,4318,task-specific model,任务特定模型,0.6666666666666666,6,"[{'word': '任务特定模型', 'ratio': 0.6666666666666666}, {'word': '特定任务模型', 'ratio': 0.3333333333333333}]",任务特定模型,{},[]
4319,4319,taxonomy,分类学,0.0,5,"[{'word': '分类法', 'ratio': 0.6}, {'word': '分类体系', 'ratio': 0.4}]",分类法,{},[]
4320,4320,teacher forcing,教师强制,1.0,5,"[{'word': '教师强制', 'ratio': 1.0}]",教师强制,{},[]
4321,4321,teacher network,教师网络,1.0,5,"[{'word': '教师网络', 'ratio': 1.0}]",教师网络,{},[]
4322,4322,temperature parameter,温度参数,1.0,5,"[{'word': '温度参数', 'ratio': 1.0}]",温度参数,{},[]
4323,4323,temperature scaling,温度缩放,0.8888888888888888,9,"[{'word': '温度缩放', 'ratio': 0.8888888888888888}, {'word': '温标', 'ratio': 0.1111111111111111}]",温度缩放,{},[]
4324,4324,template,模板,1.0,9,"[{'word': '模板', 'ratio': 1.0}]",模板,{},[]
4325,4325,template model,模板模型,1.0,9,"[{'word': '模板模型', 'ratio': 1.0}]",模板模型,{},[]
4326,4326,template-matching,模板匹配,1.0,9,"[{'word': '模板匹配', 'ratio': 1.0}]",模板匹配,{},[]
4327,4327,temporal derivative,时间导数,1.0,9,"[{'word': '时间导数', 'ratio': 1.0}]",时间导数,{},[]
4328,4328,temporal difference,时间差分,0.6,5,"[{'word': '时间差分', 'ratio': 0.6}, {'word': '时序差分', 'ratio': 0.2}, {'word': '时间差', 'ratio': 0.2}]",时间差分,{},[]
4329,4329,temporal difference learning,时序差分学习,0.2,5,"[{'word': '时间差分学习', 'ratio': 0.6}, {'word': '时序差分学习', 'ratio': 0.2}, {'word': '时间差学习', 'ratio': 0.2}]",时间差分学习,{},[]
4330,4330,temporal drift,时间漂移,0.8,5,"[{'word': '时间漂移', 'ratio': 0.8}, {'word': '时序漂移', 'ratio': 0.2}]",时间漂移,{},[]
4331,4331,temporal fusion,时间融合,0.8,5,"[{'word': '时间融合', 'ratio': 0.8}, {'word': '时序融合', 'ratio': 0.2}]",时间融合,{},[]
4332,4332,temporal locality,时间局部性,0.8,5,"[{'word': '时间局部性', 'ratio': 0.8}, {'word': '时序局部性', 'ratio': 0.2}]",时间局部性,{},[]
4333,4333,temporal logic,时序逻辑,0.8571428571428571,7,"[{'word': '时序逻辑', 'ratio': 0.8571428571428571}, {'word': '时间逻辑', 'ratio': 0.14285714285714285}]",时序逻辑,{},[]
4334,4334,temporal reasoning,时序推理,0.8571428571428571,7,"[{'word': '时序推理', 'ratio': 0.8571428571428571}, {'word': '时间推理', 'ratio': 0.14285714285714285}]",时序推理,{},[]
4335,4335,temporal variable,时间变量,0.5714285714285714,7,"[{'word': '时间变量', 'ratio': 0.5714285714285714}, {'word': '时序变量', 'ratio': 0.42857142857142855}]",时间变量,{},[]
4336,4336,tensor decomposition,张量分解,1.0,7,"[{'word': '张量分解', 'ratio': 1.0}]",张量分解,{},[]
4337,4337,tensor factorization,张量分解,0.2857142857142857,7,"[{'word': '张量因子化', 'ratio': 0.42857142857142855}, {'word': '张量分解', 'ratio': 0.2857142857142857}, {'word': '张量因式分解', 'ratio': 0.2857142857142857}]",张量因子化,"1. Rank: 张量因子化, 张量因式分解, 张量分解

2. Explanation: The term ""张量因子化"" (tensor factorization) is the best fit because it directly corresponds to the English term ""tensor factorization"" in both semantic accuracy and contextual usage within the AI domain. The term ""因子化"" (factorization) is commonly used in mathematical and computational contexts to refer to the process of breaking down a complex structure into simpler components, which aligns perfectly with the concept of tensor factorization in machine learning and data analysis. 

""张量因式分解"" (tensor factorization) is also a valid translation, but the term ""因式分解"" (factorization) is more commonly associated with algebraic expressions rather than the broader context of tensors in AI. Lastly, ""张量分解"" (tensor decomposition) is a more general term that may not capture the specific nuances of factorization as it relates to tensors, which can lead to ambiguity in the context of the provided examples. Therefore, ""张量因子化"" is the most precise and contextually appropriate choice.","['Tensor factorization', 'tensor decomposition', 'Tensor factorization']"
4338,4338,tensor field,张量场,1.0,7,"[{'word': '张量场', 'ratio': 1.0}]",张量场,{},[]
4339,4339,tensor product,张量积,1.0,7,"[{'word': '张量积', 'ratio': 1.0}]",张量积,{},[]
4340,4340,term frequency,词频,0.7142857142857143,7,"[{'word': '词频', 'ratio': 0.7142857142857143}, {'word': '术语频率', 'ratio': 0.2857142857142857}]",词频,{},[]
4341,4341,terminal node,终端节点,1.0,7,"[{'word': '终端节点', 'ratio': 1.0}]",终端节点,{},[]
4342,4342,terminal state,终止状态,0.2857142857142857,7,"[{'word': '终端状态', 'ratio': 0.7142857142857143}, {'word': '终止状态', 'ratio': 0.2857142857142857}]",终端状态,{},[]
4343,4343,termination condition,终止条件,1.0,6,"[{'word': '终止条件', 'ratio': 1.0}]",终止条件,{},[]
4344,4344,termination criterion,终止准则,0.3333333333333333,6,"[{'word': '终止标准', 'ratio': 0.6666666666666666}, {'word': '终止准则', 'ratio': 0.3333333333333333}]",终止标准,{},[]
4345,4345,test accuracy,测试准确率,1.0,6,"[{'word': '测试准确率', 'ratio': 1.0}]",测试准确率,{},[]
4346,4346,test dataset,测试数据集,1.0,6,"[{'word': '测试数据集', 'ratio': 1.0}]",测试数据集,{},[]
4347,4347,test datum,测试数据,1.0,8,"[{'word': '测试数据', 'ratio': 1.0}]",测试数据,{},[]
4348,4348,test domain,测试领域,0.375,8,"[{'word': '测试域', 'ratio': 0.625}, {'word': '测试领域', 'ratio': 0.375}]",测试域,{},[]
4349,4349,test error,测试误差,0.875,8,"[{'word': '测试误差', 'ratio': 0.875}, {'word': '测试错误', 'ratio': 0.125}]",测试误差,{},[]
4350,4350,test loss,测试损失,1.0,8,"[{'word': '测试损失', 'ratio': 1.0}]",测试损失,{},[]
4351,4351,test set,测试集,1.0,8,"[{'word': '测试集', 'ratio': 1.0}]",测试集,{},[]
4352,4352,test split,测试集分割,0.0,7,"[{'word': '测试集划分', 'ratio': 0.5714285714285714}, {'word': '测试集', 'ratio': 0.42857142857142855}]",测试集划分,{},[]
4353,4353,test time,测试时间,0.1428571428571428,7,"[{'word': '测试时', 'ratio': 0.8571428571428571}, {'word': '测试时间', 'ratio': 0.14285714285714285}]",测试时,{},[]
4354,4354,testing set,测试集,1.0,7,"[{'word': '测试集', 'ratio': 1.0}]",测试集,{},[]
4355,4355,text categorization,文本分类,1.0,7,"[{'word': '文本分类', 'ratio': 1.0}]",文本分类,{},[]
4356,4356,text corpus,文本语料库,1.0,5,"[{'word': '文本语料库', 'ratio': 1.0}]",文本语料库,{},[]
4357,4357,text embedding,文本嵌入,1.0,5,"[{'word': '文本嵌入', 'ratio': 1.0}]",文本嵌入,{},[]
4358,4358,text encoder,文本编码器,1.0,5,"[{'word': '文本编码器', 'ratio': 1.0}]",文本编码器,{},[]
4359,4359,text generation,文本生成,1.0,5,"[{'word': '文本生成', 'ratio': 1.0}]",文本生成,{},[]
4360,4360,text generation model,文本生成模型,1.0,5,"[{'word': '文本生成模型', 'ratio': 1.0}]",文本生成模型,{},[]
4361,4361,text mining,文本挖掘,1.0,6,"[{'word': '文本挖掘', 'ratio': 1.0}]",文本挖掘,{},[]
4362,4362,text segmentation,文本分割,1.0,6,"[{'word': '文本分割', 'ratio': 1.0}]",文本分割,{},[]
4363,4363,text simplification,文本简化,1.0,6,"[{'word': '文本简化', 'ratio': 1.0}]",文本简化,{},[]
4364,4364,text-davinci-002,文本-​​达芬奇-002,0.1666666666666666,6,"[{'word': 'text-davinci-002', 'ratio': 0.5}, {'word': '分機達文西-002', 'ratio': 0.16666666666666666}, {'word': '文本-\u200b\u200b达芬奇-002', 'ratio': 0.16666666666666666}, {'word': 'text-davinci-002模型', 'ratio': 0.16666666666666666}]",text-davinci-002,{},[]
4365,4365,text-davinci-003,文本-​​达芬奇-003,0.1666666666666666,6,"[{'word': 'text-davinci-003', 'ratio': 0.5}, {'word': '文-達文西-003', 'ratio': 0.16666666666666666}, {'word': '文本-\u200b\u200b达芬奇-003', 'ratio': 0.16666666666666666}, {'word': 'text-davinci-003模型', 'ratio': 0.16666666666666666}]",text-davinci-003,{},[]
4366,4366,text-to-image diffusion model,文本到图像扩散模型,1.0,5,"[{'word': '文本到图像扩散模型', 'ratio': 1.0}]",文本到图像扩散模型,{},[]
4367,4367,text-to-image generation,文本到图像生成,1.0,5,"[{'word': '文本到图像生成', 'ratio': 1.0}]",文本到图像生成,{},[]
4368,4368,text-to-image model,文本到图像模型,1.0,5,"[{'word': '文本到图像模型', 'ratio': 1.0}]",文本到图像模型,{},[]
4369,4369,text-to-image synthesis,文本到图像合成,1.0,5,"[{'word': '文本到图像合成', 'ratio': 1.0}]",文本到图像合成,{},[]
4370,4370,textual entailment,文本蕴涵,0.4285714285714285,7,"[{'word': '文本蕴含', 'ratio': 0.5714285714285714}, {'word': '文本蕴涵', 'ratio': 0.42857142857142855}]",文本蕴含,{},[]
4371,4371,tf-idf,tf-idf,0.0,7,"[{'word': '词频-逆文档频率', 'ratio': 1.0}]",词频-逆文档频率,{},[]
4372,4372,threat model,威胁模型,1.0,7,"[{'word': '威胁模型', 'ratio': 1.0}]",威胁模型,{},[]
4373,4373,threshold,阈值,1.0,7,"[{'word': '阈值', 'ratio': 1.0}]",阈值,{},[]
4374,4374,threshold function,阈值函数,1.0,7,"[{'word': '阈值函数', 'ratio': 1.0}]",阈值函数,{},[]
4375,4375,threshold parameter,阈值参数,1.0,6,"[{'word': '阈值参数', 'ratio': 1.0}]",阈值参数,{},[]
4376,4376,threshold policy,阈值策略,1.0,6,"[{'word': '阈值策略', 'ratio': 1.0}]",阈值策略,{},[]
4377,4377,time complexity,时间复杂度,1.0,6,"[{'word': '时间复杂度', 'ratio': 1.0}]",时间复杂度,{},[]
4378,4378,time series,时间序列,1.0,6,"[{'word': '时间序列', 'ratio': 1.0}]",时间序列,{},[]
4379,4379,time series analysis,时间序列分析,1.0,6,"[{'word': '时间序列分析', 'ratio': 1.0}]",时间序列分析,{},[]
4380,4380,time series forecasting,时间序列预测,1.0,8,"[{'word': '时间序列预测', 'ratio': 1.0}]",时间序列预测,{},[]
4381,4381,time step,时间步长,0.125,8,"[{'word': '时间步', 'ratio': 0.875}, {'word': '时间步长', 'ratio': 0.125}]",时间步,{},[]
4382,4382,time-series datum,时间序列数据,1.0,8,"[{'word': '时间序列数据', 'ratio': 1.0}]",时间序列数据,{},[]
4383,4383,time-series model,时序模型,0.0,8,"[{'word': '时间序列模型', 'ratio': 1.0}]",时间序列模型,{},[]
4384,4384,time/space complexity,时间/空间复杂度,1.0,8,"[{'word': '时间/空间复杂度', 'ratio': 1.0}]",时间/空间复杂度,{},[]
4385,4385,token classification,标记分类,0.0,8,"[{'word': '词元分类', 'ratio': 0.75}, {'word': '令牌分类', 'ratio': 0.125}, {'word': '标记', 'ratio': 0.125}]",词元分类,{},[]
4386,4386,token embedding,令牌嵌入,0.125,8,"[{'word': '词元嵌入', 'ratio': 0.75}, {'word': '令牌嵌入', 'ratio': 0.125}, {'word': '标记嵌入', 'ratio': 0.125}]",词元嵌入,{},[]
4387,4387,token frequency,词元频率,0.75,8,"[{'word': '词元频率', 'ratio': 0.75}, {'word': '令牌频率', 'ratio': 0.125}, {'word': '标记频率', 'ratio': 0.125}]",词元频率,{},[]
4388,4388,token length,标记长度,0.125,8,"[{'word': '词元长度', 'ratio': 0.75}, {'word': '令牌长度', 'ratio': 0.125}, {'word': '标记长度', 'ratio': 0.125}]",词元长度,{},[]
4389,4389,token representation,标记表示,0.1428571428571428,7,"[{'word': '词元表示', 'ratio': 0.5714285714285714}, {'word': '令牌表示', 'ratio': 0.14285714285714285}, {'word': '标记表示', 'ratio': 0.14285714285714285}, {'word': '代币表示', 'ratio': 0.14285714285714285}]",词元表示,{},[]
4390,4390,token sequence,标记序列,0.1428571428571428,7,"[{'word': '词元序列', 'ratio': 0.5714285714285714}, {'word': '令牌序列', 'ratio': 0.2857142857142857}, {'word': '标记序列', 'ratio': 0.14285714285714285}]",词元序列,{},[]
4391,4391,token space,令牌空间,0.1428571428571428,7,"[{'word': '词元空间', 'ratio': 0.5714285714285714}, {'word': '令牌空间', 'ratio': 0.14285714285714285}, {'word': '标记空间', 'ratio': 0.14285714285714285}, {'word': '代币空间', 'ratio': 0.14285714285714285}]",词元空间,{},[]
4392,4392,token vector,词元向量,0.5714285714285714,7,"[{'word': '词元向量', 'ratio': 0.5714285714285714}, {'word': '标记向量', 'ratio': 0.2857142857142857}, {'word': '令牌向量', 'ratio': 0.14285714285714285}]",词元向量,{},[]
4393,4393,token vocabulary,记号词汇,0.1428571428571428,7,"[{'word': '词元词汇', 'ratio': 0.42857142857142855}, {'word': '令牌词汇表', 'ratio': 0.14285714285714285}, {'word': '标记词表', 'ratio': 0.14285714285714285}, {'word': '词元词汇表', 'ratio': 0.14285714285714285}, {'word': '记号词汇', 'ratio': 0.14285714285714285}]",令牌词汇表,"1. Rank: 令牌词汇表, 词元词汇表, 词元词汇, 标记词表, 词元词汇

2. Explanation: The term ""令牌词汇表"" (Token Glossary) is the best fit for the English term ""token vocabulary"" in the context of AI and machine translation. This is because ""令牌"" directly translates to ""token,"" which is a commonly used term in the AI domain, particularly in natural language processing (NLP). The addition of ""词汇表"" (vocabulary list) accurately conveys the idea of a collection of tokens used in a model. 

The second candidate, ""词元词汇表"" (lemma vocabulary), while also relevant, refers specifically to ""lemmas,"" which are the base forms of words, and does not capture the broader concept of ""tokens"" that can include various forms of words and symbols used in NLP. 

The other candidates, such as ""标记词表"" (tag vocabulary) and ""词元词汇"" (lemma vocabulary), do not accurately reflect the intended meaning of ""token"" in this context, as they imply different concepts that are not synonymous with ""token vocabulary."" 

Overall, ""令牌词汇表"" maintains semantic accuracy and contextual fit, making it the most appropriate choice for the AI domain.","['lemma vocabulary', 'Token Glossary', 'tag vocabulary', 'lemma vocabulary', 'token vocabulary']"
4394,4394,token-level,词元级别,0.0,7,"[{'word': '令牌级别', 'ratio': 0.42857142857142855}, {'word': '词元级', 'ratio': 0.42857142857142855}, {'word': '词符级', 'ratio': 0.14285714285714285}]",词元级,"1. Rank: 词元级, 令牌级别, 词符级

2. Explanation: The term ""词元级"" (word level) is the best fit for the AI domain-specific usage of ""token-level."" In the context of natural language processing and machine learning, ""token"" typically refers to the smallest unit of text that has semantic meaning, which is often a word or a sub-word. The term ""词元"" directly translates to ""word element"" or ""word token,"" aligning closely with the concept of tokens in AI. 

""令牌级别"" (Token level) is a more literal translation but is less commonly used in the AI context, where ""token"" is often translated as ""词元."" ""词符级"" (token level) is also a literal translation but does not capture the semantic nuance as effectively as ""词元级."" Therefore, ""词元级"" is the most semantically accurate and contextually appropriate term for the AI domain.","['Token level', 'word level', 'token level']"
4395,4395,token-level attention,词元级注意力,0.4285714285714285,7,"[{'word': '令牌级别注意力', 'ratio': 0.42857142857142855}, {'word': '词元级注意力', 'ratio': 0.42857142857142855}, {'word': '词符级注意力', 'ratio': 0.14285714285714285}]",令牌级别注意力,"1. Rank: 令牌级别注意力, 词符级注意力, 词元级注意力

2. Explanation: The term ""令牌级别注意力"" (Token-level attention) is the best fit for the following reasons:

- **Semantic Accuracy**: ""令牌"" (token) is a direct translation of ""token,"" which is commonly used in the AI and NLP (Natural Language Processing) domains to refer to the smallest unit of text that carries meaning. This term is widely recognized in the context of models like BERT, where ""token"" refers to the individual pieces of input data that the model processes.

- **Contextual Fit**: The term ""令牌级别注意力"" accurately reflects the concept of attention mechanisms applied at the token level, which is crucial in understanding how models like BERT analyze and weigh the importance of different tokens in a sequence. The use of ""级别"" (level) further clarifies that this attention is specifically focused on the granularity of tokens.

- **Back Translation Accuracy**: The back translation of ""令牌级别注意力"" as ""Token level attention"" maintains the original meaning and context, ensuring that it aligns with the English term used in the AI literature.

In contrast, ""词元级注意力"" (word-level attention) and ""词符级注意力"" (token-level attention) are less accurate. ""词元"" (word) can imply a broader unit than a token, which may not capture the specific nuances of tokenization in NLP. ""词符"" (token) is a less common term in this context and may not be as readily understood by practitioners in the field. Therefore, ""令牌级别注意力"" is the most appropriate choice for conveying the intended meaning in the AI domain.","['Token level attention', 'word-level attention', 'token-level attention']"
4396,4396,token-level feature,词元级特征,0.4285714285714285,7,"[{'word': '词元级特征', 'ratio': 0.42857142857142855}, {'word': '令牌级别特征', 'ratio': 0.2857142857142857}, {'word': '令牌级特征', 'ratio': 0.14285714285714285}, {'word': '词符级特征', 'ratio': 0.14285714285714285}]",词元级特征,"1. Rank: 词元级特征, 令牌级特征, 令牌级别特征, 词符级特征

2. Explanation: The term ""词元级特征"" (token-level feature) is the best fit because ""词元"" accurately translates to ""token,"" which is a key concept in AI and natural language processing, particularly in the context of text analysis. This term maintains the semantic integrity of the original English term and aligns well with the technical jargon used in the AI domain. 

The second candidate, ""令牌级特征,"" while also translating to ""token-level feature,"" uses ""令牌,"" which can imply a broader meaning of ""token"" in other contexts (like access tokens in computing), making it less precise in the context of linguistic features. 

The third candidate, ""令牌级别特征,"" translates to ""token level characteristics,"" which introduces a slight deviation in meaning by using ""级别"" (level) instead of ""级"" (level) and adds unnecessary complexity without enhancing clarity.

Lastly, ""词符级特征"" translates to ""symbol-level features,"" which diverges from the intended meaning of ""token"" in the context of language processing, as ""词符"" (symbol) does not capture the specific linguistic nuance of ""token."" 

Thus, ""词元级特征"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['word-level features', 'Token level characteristics', 'Token level features', 'Symbol-level features']"
4397,4397,tokenisation,词条化,0.0,7,"[{'word': '分词', 'ratio': 0.7142857142857143}, {'word': '词元化', 'ratio': 0.2857142857142857}]",分词,{},[]
4398,4398,tokenization,分词化,0.0,15,"[{'word': '分词', 'ratio': 0.5333333333333333}, {'word': '词元化', 'ratio': 0.2}, {'word': '令牌化', 'ratio': 0.06666666666666667}, {'word': '标记化', 'ratio': 0.06666666666666667}, {'word': '词元化处理', 'ratio': 0.06666666666666667}, {'word': '代币化', 'ratio': 0.06666666666666667}]",分词,{},[]
4399,4399,tokenization scheme,词元化方案,0.25,8,"[{'word': '分词方案', 'ratio': 0.625}, {'word': '词元化方案', 'ratio': 0.25}, {'word': '代币化方案', 'ratio': 0.125}]",分词方案,{},[]
4400,4400,tokenizer,分词器,0.75,8,"[{'word': '分词器', 'ratio': 0.75}, {'word': '词元生成器', 'ratio': 0.125}, {'word': '词元器', 'ratio': 0.125}]",分词器,{},[]
4401,4401,top-1 accuracy,前1准确率,0.0,8,"[{'word': 'Top-1 准确率', 'ratio': 0.5}, {'word': '最高准确率', 'ratio': 0.25}, {'word': '一次准确率', 'ratio': 0.125}, {'word': 'top-1 准确度', 'ratio': 0.125}]",Top-1 准确率,{},[]
4402,4402,top-down segmentation,自上而下的分割,0.7142857142857143,7,"[{'word': '自上而下的分割', 'ratio': 0.7142857142857143}, {'word': '自顶向下分割', 'ratio': 0.2857142857142857}]",自上而下的分割,{},[]
4403,4403,top-k,前k个,0.8571428571428571,7,"[{'word': '前k个', 'ratio': 0.8571428571428571}, {'word': '前k', 'ratio': 0.14285714285714285}]",前k个,{},[]
4404,4404,top-k sampling,top-k采样,0.0,7,"[{'word': '前k个采样', 'ratio': 0.5714285714285714}, {'word': '前k采样', 'ratio': 0.42857142857142855}]",前k个采样,{},[]
4405,4405,top-p sampling,顶p抽样,0.0,7,"[{'word': '前p采样', 'ratio': 0.42857142857142855}, {'word': '前p个采样', 'ratio': 0.2857142857142857}, {'word': '前p概率采样', 'ratio': 0.2857142857142857}]",前p概率采样,"1. Rank: 前p概率采样, 前p采样, 前p个采样

2. Explanation: The term ""前p概率采样"" (Top p probability sampling) is the best fit because it accurately captures the essence of the original English term ""top-p sampling"" by explicitly including the word ""概率"" (probability), which is crucial in the context of sampling methods in AI. This term conveys that the sampling is based on probabilities, which is a fundamental aspect of how top-p sampling operates—selecting from the top p portion of the probability distribution. 

The second term, ""前p采样"" (First p sampling), is less precise as it omits the probability aspect, which is essential for understanding the method's function in AI applications. The third term, ""前p个采样"" (First p samples), is also inadequate because it suggests a focus on a fixed number of samples rather than the probabilistic nature of the selection process. Therefore, ""前p概率采样"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['first p sampling', 'First p samples', 'Top p probability sampling']"
4406,4406,topic assignment,主题分配,1.0,7,"[{'word': '主题分配', 'ratio': 1.0}]",主题分配,{},[]
4407,4407,topic classification,主题分类,1.0,6,"[{'word': '主题分类', 'ratio': 1.0}]",主题分类,{},[]
4408,4408,topic distribution,主题分布,1.0,6,"[{'word': '主题分布', 'ratio': 1.0}]",主题分布,{},[]
4409,4409,topic model,主题模型,1.0,6,"[{'word': '主题模型', 'ratio': 1.0}]",主题模型,{},[]
4410,4410,topic proportion,主题比例,1.0,6,"[{'word': '主题比例', 'ratio': 1.0}]",主题比例,{},[]
4411,4411,topic weight,主题权重,1.0,6,"[{'word': '主题权重', 'ratio': 1.0}]",主题权重,{},[]
4412,4412,total variation,全变分,0.0,7,"[{'word': '总变差', 'ratio': 0.8571428571428571}, {'word': '全变差', 'ratio': 0.14285714285714285}]",总变差,{},[]
4413,4413,total variation distance,总变差距离,0.8571428571428571,7,"[{'word': '总变差距离', 'ratio': 0.8571428571428571}, {'word': '全变差距离', 'ratio': 0.14285714285714285}]",总变差距离,{},[]
4414,4414,toxicity detection,毒性检测,0.8571428571428571,7,"[{'word': '毒性检测', 'ratio': 0.8571428571428571}, {'word': '有毒性检测', 'ratio': 0.14285714285714285}]",毒性检测,{},[]
4415,4415,trace norm,迹范数,0.7142857142857143,7,"[{'word': '迹范数', 'ratio': 0.7142857142857143}, {'word': '跟踪范数', 'ratio': 0.14285714285714285}, {'word': '痕迹范数', 'ratio': 0.14285714285714285}]",迹范数,{},[]
4416,4416,tracking algorithm,跟踪算法,1.0,7,"[{'word': '跟踪算法', 'ratio': 1.0}]",跟踪算法,{},[]
4417,4417,train,训练,0.8888888888888888,9,"[{'word': '训练', 'ratio': 0.8888888888888888}, {'word': '卷积核', 'ratio': 0.1111111111111111}]",训练,{},[]
4418,4418,train / test / dev split,训练/测试/验证集拆分,0.0,9,"[{'word': '训练/测试/开发集划分', 'ratio': 0.4444444444444444}, {'word': '训练/测试/验证划分', 'ratio': 0.2222222222222222}, {'word': '训练/测试/开发划分', 'ratio': 0.1111111111111111}, {'word': '卷积核', 'ratio': 0.1111111111111111}, {'word': '训练/测试/验证集划分', 'ratio': 0.1111111111111111}]",训练/测试/验证集划分,"1. Rank: 训练/测试/验证集划分, 训练/测试/开发集划分, 训练/测试/开发划分, 训练/测试/验证划分, 训练/测试/开发集划分, 卷积核

2. Explanation: The term ""训练/测试/验证集划分"" (Training/test/validation set partitioning) is the best fit because it accurately reflects the common terminology used in the AI and machine learning community. The term ""验证"" (validation) is widely accepted in the context of model evaluation, as it refers to the subset of data used to tune model parameters and prevent overfitting. This is crucial for understanding experimental results, as it provides clarity on how the model's performance is assessed. 

In contrast, ""训练/测试/开发集划分"" (Training/test/development set partitioning) uses ""开发"" (development), which is less common in this context and may lead to confusion, as ""development"" typically refers to a phase rather than a specific dataset split. The other candidates either use ""开发"" or do not clearly distinguish between validation and testing, which is essential for accurate reporting in AI research. Therefore, the first term is the most semantically accurate and contextually appropriate for the AI domain.","['Training/test/development set partitioning', 'Training/testing/validation division', 'Training/Testing/Development Division', 'Convolution kernel', 'Training/test/validation set partitioning']"
4419,4419,train set,训练集,0.7777777777777778,9,"[{'word': '训练集', 'ratio': 0.7777777777777778}, {'word': '训练/测试/开发划分', 'ratio': 0.1111111111111111}, {'word': '卷积核', 'ratio': 0.1111111111111111}]",训练集,{},[]
4420,4420,train-test split,训练测试划分,0.0,9,"[{'word': '训练-测试划分', 'ratio': 0.4444444444444444}, {'word': '训练-测试集划分', 'ratio': 0.3333333333333333}, {'word': '训练/测试/开发划分', 'ratio': 0.1111111111111111}, {'word': '卷积核', 'ratio': 0.1111111111111111}]",训练-测试划分,"1. Rank: 训练-测试划分, 训练-测试集划分, 训练/测试/开发划分, 卷积核

2. Explanation: The term ""训练-测试划分"" (training-test split) is the best fit because it directly translates the English term while maintaining the specific context of machine learning. The phrase accurately conveys the concept of dividing a dataset into training and testing subsets, which is a fundamental practice in AI and machine learning. The back translation ""training-test split"" closely matches the original English term, ensuring semantic accuracy.

The second candidate, ""训练-测试集划分"" (training-test set partitioning), is also a good option but introduces the word ""集"" (set), which, while relevant, makes it slightly less concise than the first option. The third candidate, ""训练/测试/开发划分"" (Training/Testing/Development Division), introduces an additional ""开发"" (development) component that is not present in the original term, making it less accurate in this specific context. Lastly, ""卷积核"" (Convolution kernel) is unrelated to the concept of train-test split and is therefore not a suitable candidate.","['training-test split', 'Training-test set partitioning', 'Training/Testing/Development Division', 'Convolution kernel']"
4421,4421,train/test,训练/测试,0.8888888888888888,9,"[{'word': '训练/测试', 'ratio': 0.8888888888888888}, {'word': '卷积核', 'ratio': 0.1111111111111111}]",训练/测试,{},[]
4422,4422,trainable parameter,可训练参数,1.0,5,"[{'word': '可训练参数', 'ratio': 1.0}]",可训练参数,{},[]
4423,4423,trainable weight,可训练权重,0.8,5,"[{'word': '可训练权重', 'ratio': 0.8}, {'word': '可训练重量', 'ratio': 0.2}]",可训练权重,{},[]
4424,4424,training accuracy,训练准确率,0.8,5,"[{'word': '训练准确率', 'ratio': 0.8}, {'word': '训练准确度', 'ratio': 0.2}]",训练准确率,{},[]
4425,4425,training algorithm,训练算法,1.0,5,"[{'word': '训练算法', 'ratio': 1.0}]",训练算法,{},[]
4426,4426,training batch,训练批次,1.0,8,"[{'word': '训练批次', 'ratio': 1.0}]",训练批次,{},[]
4427,4427,training corpora,训练语料库,1.0,8,"[{'word': '训练语料库', 'ratio': 1.0}]",训练语料库,{},[]
4428,4428,training corpus,训练语料库,0.25,8,"[{'word': '训练语料', 'ratio': 0.75}, {'word': '训练语料库', 'ratio': 0.25}]",训练语料,{},[]
4429,4429,training dataset,训练数据集,1.0,8,"[{'word': '训练数据集', 'ratio': 1.0}]",训练数据集,{},[]
4430,4430,training datum,训练数据,1.0,8,"[{'word': '训练数据', 'ratio': 1.0}]",训练数据,{},[]
4431,4431,training distribution,训练分布,0.8571428571428571,7,"[{'word': '训练分布', 'ratio': 0.8571428571428571}, {'word': '训练动态', 'ratio': 0.14285714285714285}]",训练分布,{},[]
4432,4432,training dynamic,训练动态,1.0,7,"[{'word': '训练动态', 'ratio': 1.0}]",训练动态,{},[]
4433,4433,training epoch,训练轮次,1.0,7,"[{'word': '训练轮次', 'ratio': 1.0}]",训练轮次,{},[]
4434,4434,training error,训练误差,1.0,7,"[{'word': '训练误差', 'ratio': 1.0}]",训练误差,{},[]
4435,4435,training example,训练样本,0.7142857142857143,7,"[{'word': '训练样本', 'ratio': 0.7142857142857143}, {'word': '训练实例', 'ratio': 0.14285714285714285}, {'word': '训练示例', 'ratio': 0.14285714285714285}]",训练样本,{},[]
4436,4436,training loss,训练损失,1.0,7,"[{'word': '训练损失', 'ratio': 1.0}]",训练损失,{},[]
4437,4437,training objective,训练目标,0.8571428571428571,7,"[{'word': '训练目标', 'ratio': 0.8571428571428571}, {'word': '培训目标', 'ratio': 0.14285714285714285}]",训练目标,{},[]
4438,4438,training phase,训练阶段,1.0,7,"[{'word': '训练阶段', 'ratio': 1.0}]",训练阶段,{},[]
4439,4439,training procedure,训练过程,0.5714285714285714,7,"[{'word': '训练过程', 'ratio': 0.5714285714285714}, {'word': '训练程序', 'ratio': 0.42857142857142855}]",训练过程,{},[]
4440,4440,training process,训练过程,0.7142857142857143,7,"[{'word': '训练过程', 'ratio': 0.7142857142857143}, {'word': '训练流程', 'ratio': 0.2857142857142857}]",训练过程,{},[]
4441,4441,training sample,训练样本,1.0,8,"[{'word': '训练样本', 'ratio': 1.0}]",训练样本,{},[]
4442,4442,training set,训练集,1.0,8,"[{'word': '训练集', 'ratio': 1.0}]",训练集,{},[]
4443,4443,training stability,训练稳定性,1.0,8,"[{'word': '训练稳定性', 'ratio': 1.0}]",训练稳定性,{},[]
4444,4444,training step,训练步骤,1.0,8,"[{'word': '训练步骤', 'ratio': 1.0}]",训练步骤,{},[]
4445,4445,training task,训练任务,1.0,8,"[{'word': '训练任务', 'ratio': 1.0}]",训练任务,{},[]
4446,4446,training time,训练时间,1.0,8,"[{'word': '训练时间', 'ratio': 1.0}]",训练时间,{},[]
4447,4447,training token,训练标记,0.75,8,"[{'word': '训练标记', 'ratio': 0.75}, {'word': '训练词元', 'ratio': 0.125}, {'word': '训练令牌', 'ratio': 0.125}]",训练标记,{},[]
4448,4448,trajectory forecasting,轨迹预测,1.0,8,"[{'word': '轨迹预测', 'ratio': 1.0}]",轨迹预测,{},[]
4449,4449,trajectory optimization,轨迹优化,1.0,8,"[{'word': '轨迹优化', 'ratio': 1.0}]",轨迹优化,{},[]
4450,4450,transaction database,交易数据库,0.125,8,"[{'word': '事务数据库', 'ratio': 0.875}, {'word': '交易数据库', 'ratio': 0.125}]",事务数据库,{},[]
4451,4451,transductive learning,直推学习,0.3333333333333333,6,"[{'word': '传导学习', 'ratio': 0.5}, {'word': '直推学习', 'ratio': 0.3333333333333333}, {'word': '转导学习', 'ratio': 0.16666666666666666}]",传导学习,{},[]
4452,4452,transfer function,传递函数,0.6666666666666666,6,"[{'word': '传递函数', 'ratio': 0.6666666666666666}, {'word': '转移函数', 'ratio': 0.3333333333333333}]",传递函数,{},[]
4453,4453,transformation function,转换函数,0.5,6,"[{'word': '转换函数', 'ratio': 0.5}, {'word': '变换函数', 'ratio': 0.5}]",转换函数,{},[]
4454,4454,transformation matrix,变换矩阵,0.5,6,"[{'word': '转换矩阵', 'ratio': 0.5}, {'word': '变换矩阵', 'ratio': 0.5}]",转换矩阵,{},[]
4455,4455,transformer language model,变压器语言模型,0.0,6,"[{'word': '转换器语言模型', 'ratio': 0.3333333333333333}, {'word': '变换器语言模型', 'ratio': 0.3333333333333333}, {'word': 'Transformer语言模型', 'ratio': 0.3333333333333333}]",Transformer语言模型,"1. Rank: Transformer语言模型, 变换器语言模型, 转换器语言模型

2. Explanation: The term ""Transformer语言模型"" is the best fit because it retains the original English term ""Transformer,"" which is a widely recognized and established term in the AI and machine learning community. This term is commonly used in academic literature and industry discussions, making it semantically accurate and contextually appropriate. The back translation also accurately reflects the original term, ensuring clarity and consistency in communication. 

The second option, ""变换器语言模型,"" translates to ""transformer language model"" but uses ""变换器,"" which is a more generic term for ""converter"" or ""transformer"" and may not convey the specific meaning associated with the AI model. The third option, ""转换器语言模型,"" similarly translates to ""converter language model,"" which is less accurate in the context of AI terminology. Therefore, ""Transformer语言模型"" is the most precise and contextually relevant choice.","['converter language model', 'transformer language model', 'Transformer language model']"
4456,4456,transformer layer,转换器层,0.3333333333333333,6,"[{'word': '转换器层', 'ratio': 0.3333333333333333}, {'word': '变换器层', 'ratio': 0.3333333333333333}, {'word': 'Transformer层', 'ratio': 0.3333333333333333}]",Transformer层,"1. Rank: Transformer层, 变换器层, 转换器层

2. Explanation: The term ""Transformer层"" is the best fit for the translation of ""transformer layer"" in the AI domain. This is because ""Transformer"" is a specific architecture in machine learning, particularly in natural language processing, and it is widely recognized in both English and Chinese as a proper noun. Retaining the original term ""Transformer"" in the translation ensures semantic accuracy and aligns with the established terminology used in the field. 

In contrast, ""变换器层"" and ""转换器层"" translate to ""converter layer,"" which does not accurately reflect the specific function and architecture of the transformer model. These terms could lead to confusion, as they imply a more general concept of conversion rather than the specialized function of the transformer architecture. Therefore, ""Transformer层"" is the most contextually appropriate and semantically accurate choice for this AI terminology.","['converter layer', 'converter layer', 'Transformer layer']"
4457,4457,transformer variant,变压器变体,0.0,6,"[{'word': '转换器变体', 'ratio': 0.3333333333333333}, {'word': '变换器变体', 'ratio': 0.3333333333333333}, {'word': 'Transformer变体', 'ratio': 0.3333333333333333}]","""Transformer变体""","1. Rank: ""Transformer变体"", ""变换器变体"", ""转换器变体""

2. Explanation: The term ""Transformer变体"" is the best fit because it retains the original term ""Transformer,"" which is a well-established concept in the AI domain, particularly in natural language processing and deep learning. The back translation ""Transformer variant"" accurately reflects the original English term, ensuring semantic accuracy. 

In contrast, ""变换器变体"" and ""转换器变体"" translate to ""Converter variant,"" which does not capture the specific meaning of ""transformer"" in the context of AI. The term ""converter"" is too generic and could refer to various types of conversion processes, thus losing the specificity and technicality associated with the transformer architecture. Therefore, ""Transformer变体"" is the most contextually appropriate choice for conveying the intended meaning in the AI field.","['Converter variants', 'Converter variant', 'Transformer variant']"
4458,4458,transformer-based architecture,基于Transformer的架构,0.3333333333333333,6,"[{'word': '基于转换器的架构', 'ratio': 0.3333333333333333}, {'word': '基于变换器的架构', 'ratio': 0.3333333333333333}, {'word': '基于Transformer的架构', 'ratio': 0.3333333333333333}]",基于Transformer的架构,"1. Rank: 基于Transformer的架构, 基于变换器的架构, 基于转换器的架构

2. Explanation: The term ""基于Transformer的架构"" is the best fit because it retains the original term ""Transformer,"" which is widely recognized in the AI community and specifically refers to a type of model architecture used in natural language processing and other machine learning tasks. The back translation accurately reflects the original English term, maintaining its semantic integrity. 

In contrast, ""基于变换器的架构"" and ""基于转换器的架构"" translate ""Transformer"" into more generic terms (""变换器"" and ""转换器""), which may not convey the specific meaning associated with the Transformer architecture in AI. These terms could lead to confusion, as they do not have the same established recognition in the field. Therefore, the first translation is preferred for its accuracy and contextual fit within the AI domain.","['Converter-based architecture', 'Converter-based architecture', 'Transformer-based architecture']"
4459,4459,transition distribution,转移分布,1.0,5,"[{'word': '转移分布', 'ratio': 1.0}]",转移分布,{},[]
4460,4460,transition dynamic,转移动态,0.8,5,"[{'word': '转移动态', 'ratio': 0.8}, {'word': '转移动力学', 'ratio': 0.2}]",转移动态,{},[]
4461,4461,transition function,状态转移函数,0.0,5,"[{'word': '转移函数', 'ratio': 1.0}]",转移函数,{},[]
4462,4462,transition graph,转移图,1.0,5,"[{'word': '转移图', 'ratio': 1.0}]",转移图,{},[]
4463,4463,transition kernel,转移核,1.0,5,"[{'word': '转移核', 'ratio': 1.0}]",转移核,{},[]
4464,4464,transition matrix,转移矩阵,1.0,3,"[{'word': '转移矩阵', 'ratio': 1.0}]",转移矩阵,{},[]
4465,4465,transition model,转移模型,1.0,3,"[{'word': '转移模型', 'ratio': 1.0}]",转移模型,{},[]
4466,4466,transition probability,转移概率,1.0,3,"[{'word': '转移概率', 'ratio': 1.0}]",转移概率,{},[]
4467,4467,transition probability model,转移概率模型,1.0,3,"[{'word': '转移概率模型', 'ratio': 1.0}]",转移概率模型,{},[]
4468,4468,transition system,过渡系统,0.0,3,"[{'word': '转移系统', 'ratio': 1.0}]",转移系统,{},[]
4469,4469,transition-based dependency parsing,基于转移的依存分析,0.0,6,"[{'word': '基于转换的依赖解析', 'ratio': 0.3333333333333333}, {'word': '基于转移的依赖解析', 'ratio': 0.3333333333333333}, {'word': '基于转移的依存句法分析', 'ratio': 0.3333333333333333}]",基于转移的依赖解析,"1. Rank: 基于转移的依赖解析, 基于转移的依存句法分析, 基于转换的依赖解析

2. Explanation: The term ""基于转移的依赖解析"" (Back translation: ""Transfer-based dependency resolution"") is the best fit for ""transition-based dependency parsing"" in the AI domain. The use of ""转移"" (transfer) aligns closely with the concept of ""transition"" in the context of parsing algorithms, which often involve moving between states or configurations during the parsing process. This term accurately captures the dynamic nature of the parsing method described in the context.

The second candidate, ""基于转移的依存句法分析"" (Back translation: ""Transfer-based dependency syntax analysis""), is also a strong contender, but it introduces ""句法分析"" (syntax analysis), which is broader and may not specifically convey the transition-based aspect as effectively as ""依赖解析"" (dependency resolution).

The third candidate, ""基于转换的依赖解析"" (Back translation: ""Transformation-based dependency resolution""), uses ""转换"" (transformation), which can imply a different meaning in the context of parsing, potentially leading to confusion with transformation-based models that are not specifically transition-based. Therefore, while all translations are somewhat accurate, ""基于转移的依赖解析"" is the most semantically precise and contextually appropriate for the AI terminology in question.","['Transformation-based dependency resolution', 'Transfer-based dependency resolution', 'Transfer-based dependency syntax analysis']"
4470,4470,transition-based model,基于转移的模型,0.6666666666666666,6,"[{'word': '基于转移的模型', 'ratio': 0.6666666666666666}, {'word': '基于转换的模型', 'ratio': 0.3333333333333333}]",基于转移的模型,{},[]
4471,4471,transition-based parser,基于转移的解析器,0.6666666666666666,6,"[{'word': '基于转移的解析器', 'ratio': 0.6666666666666666}, {'word': '基于转换的解析器', 'ratio': 0.3333333333333333}]",基于转移的解析器,{},[]
4472,4472,transition-based parsing,基于转移的句法分析,0.3333333333333333,6,"[{'word': '基于转换的解析', 'ratio': 0.3333333333333333}, {'word': '基于转移的解析', 'ratio': 0.3333333333333333}, {'word': '基于转移的句法分析', 'ratio': 0.3333333333333333}]",基于转移的解析,"1. Rank: 基于转移的解析, 基于转移的句法分析, 基于转换的解析

2. Explanation: The term ""基于转移的解析"" (Back translation: ""Transfer-based parsing"") is the best fit for ""transition-based parsing"" in the AI domain, particularly in natural language processing (NLP). The term ""转移"" (transfer) closely aligns with the concept of transitions in parsing, which refers to the movement from one state to another during the parsing process. This is a common terminology used in parsing algorithms, making it semantically accurate and contextually relevant.

The second candidate, ""基于转移的句法分析"" (Back translation: ""Transfer-based parsing""), while also accurate, adds ""句法"" (syntactic), which may not be necessary in the context of transition-based parsing, as the term ""解析"" (parsing) already implies a focus on syntax. 

The third candidate, ""基于转换的解析"" (Back translation: ""Transformation-based parsing""), introduces ""转换"" (transformation), which can imply a different meaning and is less commonly used in the context of parsing compared to ""转移"" (transfer). Therefore, it is less suitable for the specific AI terminology in question. 

Overall, ""基于转移的解析"" is the most precise and contextually appropriate translation for ""transition-based parsing"" in the field of AI and NLP.","['Transformation-based parsing', 'Transfer-based parsing', 'Transfer-based parsing']"
4473,4473,transitive closure,传递闭包,1.0,6,"[{'word': '传递闭包', 'ratio': 1.0}]",传递闭包,{},[]
4474,4474,transitive relation,传递关系,0.875,8,"[{'word': '传递关系', 'ratio': 0.875}, {'word': 'transit性关系', 'ratio': 0.125}]",传递关系,{},[]
4475,4475,translation invariance,平移不变性,0.375,8,"[{'word': '翻译不变性', 'ratio': 0.625}, {'word': '平移不变性', 'ratio': 0.375}]",翻译不变性,{},[]
4476,4476,translation model,翻译模型,1.0,8,"[{'word': '翻译模型', 'ratio': 1.0}]",翻译模型,{},[]
4477,4477,translation system,翻译系统,1.0,8,"[{'word': '翻译系统', 'ratio': 1.0}]",翻译系统,{},[]
4478,4478,translation vector,平移向量,1.0,8,"[{'word': '平移向量', 'ratio': 1.0}]",平移向量,{},[]
4479,4479,transliteration,音译,1.0,8,"[{'word': '音译', 'ratio': 1.0}]",音译,{},[]
4480,4480,transpose,转置,1.0,8,"[{'word': '转置', 'ratio': 1.0}]",转置,{},[]
4481,4481,transposition table,置换表,0.75,8,"[{'word': '置换表', 'ratio': 0.75}, {'word': '转置表', 'ratio': 0.25}]",置换表,{},[]
4482,4482,tree data structure,树数据结构,0.8571428571428571,7,"[{'word': '树数据结构', 'ratio': 0.8571428571428571}, {'word': '树形数据结构', 'ratio': 0.14285714285714285}]",树数据结构,{},[]
4483,4483,tree decomposition,树分解,1.0,7,"[{'word': '树分解', 'ratio': 1.0}]",树分解,{},[]
4484,4484,tree depth,树深度,1.0,7,"[{'word': '树深度', 'ratio': 1.0}]",树深度,{},[]
4485,4485,tree ensemble,树集成,1.0,7,"[{'word': '树集成', 'ratio': 1.0}]",树集成,{},[]
4486,4486,tree search,树搜索,1.0,7,"[{'word': '树搜索', 'ratio': 1.0}]",树搜索,{},[]
4487,4487,tree structure,树形结构,0.0,8,"[{'word': '树结构', 'ratio': 1.0}]",树结构,{},[]
4488,4488,tree width,树宽,0.75,8,"[{'word': '树宽', 'ratio': 0.75}, {'word': '树宽度', 'ratio': 0.25}]",树宽,{},[]
4489,4489,tree-based model,基于树模型,0.0,8,"[{'word': '基于树的模型', 'ratio': 1.0}]",基于树的模型,{},[]
4490,4490,treebank annotation,树库标注,0.625,8,"[{'word': '树库标注', 'ratio': 0.625}, {'word': '树库注释', 'ratio': 0.375}]",树库标注,{},[]
4491,4491,tri-gram,三连词,0.0,8,"[{'word': '三元组', 'ratio': 0.75}, {'word': '三元语法', 'ratio': 0.25}]",三元组,{},[]
4492,4492,triangle inequality,三角不等式,1.0,5,"[{'word': '三角不等式', 'ratio': 1.0}]",三角不等式,{},[]
4493,4493,trifocal tensor,三焦张量,0.4,5,"[{'word': '三聚焦张量', 'ratio': 0.4}, {'word': '三焦张量', 'ratio': 0.4}, {'word': '三聚张量', 'ratio': 0.2}]",三焦张量,"1. Rank: 三焦张量, 三聚焦张量, 三聚张量

2. Explanation: The term ""三焦张量"" (trifocal tensor) is the best fit for the following reasons: 

- **Semantic Accuracy**: ""三焦"" directly translates to ""trifocal,"" which accurately reflects the concept of a tensor that relates to three focal points. This term is widely recognized in the field of computer vision and image processing, where trifocal tensors are commonly discussed.

- **Back Translation Accuracy**: The back translation of ""三焦张量"" as ""trifocal tensor"" maintains the original meaning without introducing ambiguity. In contrast, ""三聚焦张量"" (Three focused tensors) suggests multiple tensors rather than a single trifocal tensor, which could lead to confusion in the context of the paper. ""三聚张量"" (Trimeric tensor) is also less accurate as it introduces a term (""trimeric"") that is not standard in the AI domain and could mislead readers regarding the nature of the tensor.

Overall, ""三焦张量"" is the most contextually appropriate and semantically accurate term for the AI domain, particularly in discussions surrounding tensors in computer vision.","['Three focused tensors', 'trifocal tensor', 'Trimeric tensor']"
4494,4494,trigram language model,三元语言模型,0.6,5,"[{'word': '三元语言模型', 'ratio': 0.6}, {'word': '三元组语言模型', 'ratio': 0.4}]",三元语言模型,{},[]
4495,4495,trigram model,三元模型,0.6,5,"[{'word': '三元模型', 'ratio': 0.6}, {'word': '三元组模型', 'ratio': 0.4}]",三元模型,{},[]
4496,4496,trilinear interpolation,三线性插值,1.0,5,"[{'word': '三线性插值', 'ratio': 1.0}]",三线性插值,{},[]
4497,4497,trimap,三元图,0.0,5,"[{'word': '三分图', 'ratio': 0.8}, {'word': '三区域图', 'ratio': 0.2}]",三分图,{},[]
4498,4498,triple,三元组,1.0,5,"[{'word': '三元组', 'ratio': 1.0}]",三元组,{},[]
4499,4499,triplet,三元组,0.8,5,"[{'word': '三元组', 'ratio': 0.8}, {'word': '三重组', 'ratio': 0.2}]",三元组,{},[]
4500,4500,triplet loss,三元组损失,0.8,5,"[{'word': '三元组损失', 'ratio': 0.8}, {'word': '三重损失', 'ratio': 0.2}]",三元组损失,{},[]
4501,4501,true positive rate,真正正例率,0.0,5,"[{'word': '真阳性率', 'ratio': 1.0}]",真阳性率,{},[]
4502,4502,truth assignment,真值赋值,0.6,5,"[{'word': '真值赋值', 'ratio': 0.6}, {'word': '真值分配', 'ratio': 0.4}]",真值赋值,{},[]
4503,4503,tuple,元组,1.0,5,"[{'word': '元组', 'ratio': 1.0}]",元组,{},[]
4504,4504,tuplex,元组,0.0,5,"[{'word': '元组组', 'ratio': 0.4}, {'word': '元组x', 'ratio': 0.4}, {'word': '元组扩展', 'ratio': 0.2}]","""元组扩展""","1. Rank: ""元组扩展"", ""元组组"", ""元组x""

2. Explanation: The term ""元组扩展"" (tuple extension) is the best fit for the translation of ""tuplex"" in the AI context. This is because ""tuplex"" seems to imply a concept that extends or builds upon the traditional notion of a tuple, which is a fundamental data structure in computer science and AI. The term ""扩展"" (extension) captures this idea of enhancement or expansion effectively. 

In contrast, ""元组组"" (tuple group) suggests a collection of tuples, which does not convey the specific meaning of ""tuplex"" as an advanced or modified tuple structure. ""元组x"" (tuplex) is a direct transliteration and does not provide any semantic clarity or contextual fit, making it less suitable for understanding the term's application in AI. Therefore, ""元组扩展"" is the most semantically accurate and contextually appropriate choice.","['tuple group', 'tuplex', 'tuple extension']"
4505,4505,two-class classification,两类分类,0.0,5,"[{'word': '二分类', 'ratio': 0.6}, {'word': '二类分类', 'ratio': 0.2}, {'word': '二分类分类', 'ratio': 0.2}]",二分类,{},[]
4506,4506,two-player zero-sum game,二人零和博弈,0.2,5,"[{'word': '两人零和博弈', 'ratio': 0.4}, {'word': '双人零和游戏', 'ratio': 0.2}, {'word': '双人零和博弈', 'ratio': 0.2}, {'word': '二人零和博弈', 'ratio': 0.2}]","""双人零和博弈""","1. Rank: ""双人零和博弈"", ""两人零和博弈"", ""二人零和博弈"", ""双人零和游戏""

2. Explanation: The term ""双人零和博弈"" is the best fit for the translation of ""two-player zero-sum game"" in the AI domain. This term accurately captures the essence of the original English term, maintaining both semantic accuracy and contextual relevance. The word ""博弈"" (bó yì) is commonly used in game theory and conveys the concept of a strategic interaction, which is essential in the context of zero-sum games. 

While ""两人零和博弈"" and ""二人零和博弈"" are also accurate, they are less commonly used in academic literature compared to ""双人零和博弈."" The term ""双人"" (two-person) is more standard in the context of game theory, making it more recognizable to the target audience. 

""双人零和游戏"" is less preferred because ""游戏"" (game) can imply a more casual or recreational context, which may not fully capture the formal and strategic nature of the term ""博弈"" in academic discussions. Therefore, ""双人零和博弈"" is the most appropriate choice for conveying the intended meaning in the AI domain.","['Two-person zero-sum game', 'Two player zero sum game', 'Two-player zero-sum game', 'Two-person zero-sum game']"
4507,4507,type embedding,类型嵌入,1.0,5,"[{'word': '类型嵌入', 'ratio': 1.0}]",类型嵌入,{},[]
4508,4508,unary atom,一元原子,0.8,5,"[{'word': '一元原子', 'ratio': 0.8}, {'word': '类型嵌入', 'ratio': 0.2}]",一元原子,{},[]
4509,4509,unary constraint,一元约束,1.0,5,"[{'word': '一元约束', 'ratio': 1.0}]",一元约束,{},[]
4510,4510,unary feature,单元特征,0.0,5,"[{'word': '一元特征', 'ratio': 1.0}]",一元特征,{},[]
4511,4511,unary potential,一元势能,0.4,5,"[{'word': '一元势能', 'ratio': 0.4}, {'word': '一元势', 'ratio': 0.4}, {'word': '一元潜力', 'ratio': 0.2}]",一元势能,"1. Rank: 一元势能, 一元势, 一元潜力

2. Explanation: The term ""一元势能"" (Unary potential energy) is the best fit for the AI domain-specific usage because it accurately captures the concept of ""unary potential"" in a way that aligns with established terminology in fields such as machine learning and computer vision. The term ""势能"" (potential energy) conveys the idea of a measure or score associated with a single entity (in this case, a vowel or pixel), which is essential in understanding how unary potentials function in models. 

The second candidate, ""一元势"" (One yuan potential), while it retains the ""unary"" aspect, introduces ambiguity by using ""元"" (yuan), which can refer to the currency in Chinese, thus detracting from the intended meaning in the context of AI. 

The third candidate, ""一元潜力"" (One dollar potential), is the least appropriate as ""潜力"" (potential) does not convey the same technical meaning as ""势能"" (potential energy) in this context, and it also introduces confusion with the term ""dollar,"" which is irrelevant to the concept being described. 

Overall, ""一元势能"" is the most semantically accurate and contextually appropriate translation for ""unary potential"" in the AI domain.","['Unary potential energy', 'One yuan potential', 'One dollar potential']"
4512,4512,unary predicate,一元谓词,0.8888888888888888,9,"[{'word': '一元谓词', 'ratio': 0.8888888888888888}, {'word': '单一谓词', 'ratio': 0.1111111111111111}]",一元谓词,{},[]
4513,4513,unary production,一元产生式,0.7777777777777778,9,"[{'word': '一元产生式', 'ratio': 0.7777777777777778}, {'word': '一元生成', 'ratio': 0.1111111111111111}, {'word': '单一产生', 'ratio': 0.1111111111111111}]",一元产生式,{},[]
4514,4514,unbiased estimate,无偏估计,1.0,9,"[{'word': '无偏估计', 'ratio': 1.0}]",无偏估计,{},[]
4515,4515,unbiased estimator,无偏估计量,1.0,9,"[{'word': '无偏估计量', 'ratio': 1.0}]",无偏估计量,{},[]
4516,4516,uncertainty,不确定性,1.0,9,"[{'word': '不确定性', 'ratio': 1.0}]",不确定性,{},[]
4517,4517,uncertainty measure,不确定性度量,1.0,6,"[{'word': '不确定性度量', 'ratio': 1.0}]",不确定性度量,{},[]
4518,4518,uncertainty modeling,不确定性建模,1.0,6,"[{'word': '不确定性建模', 'ratio': 1.0}]",不确定性建模,{},[]
4519,4519,uncertainty sampling,不确定性采样,1.0,6,"[{'word': '不确定性采样', 'ratio': 1.0}]",不确定性采样,{},[]
4520,4520,undirected graph,无向图,1.0,6,"[{'word': '无向图', 'ratio': 1.0}]",无向图,{},[]
4521,4521,undirected graphical model,无向图模型,1.0,6,"[{'word': '无向图模型', 'ratio': 1.0}]",无向图模型,{},[]
4522,4522,uniform convergence,一致收敛,1.0,6,"[{'word': '一致收敛', 'ratio': 1.0}]",一致收敛,{},[]
4523,4523,uniform distribution,均匀分布,1.0,6,"[{'word': '均匀分布', 'ratio': 1.0}]",均匀分布,{},[]
4524,4524,uniform information density hypothesis,均匀信息密度假说,0.5,6,"[{'word': '均匀信息密度假说', 'ratio': 0.5}, {'word': '均匀信息密度假设', 'ratio': 0.3333333333333333}, {'word': '一致信息密度假设', 'ratio': 0.16666666666666666}]",均匀信息密度假说,{},[]
4525,4525,uniform sampling,均匀抽样,0.1666666666666666,6,"[{'word': '均匀采样', 'ratio': 0.8333333333333334}, {'word': '均匀抽样', 'ratio': 0.16666666666666666}]",均匀采样,{},[]
4526,4526,unigram counts,一元计数,0.1666666666666666,6,"[{'word': '一元语法计数', 'ratio': 0.5}, {'word': '一元计数', 'ratio': 0.16666666666666666}, {'word': '单语法计数', 'ratio': 0.16666666666666666}, {'word': '单词计数', 'ratio': 0.16666666666666666}]",一元语法计数,{},[]
4527,4527,unigram distribution,一元分布,0.75,8,"[{'word': '一元分布', 'ratio': 0.75}, {'word': '单元组分布', 'ratio': 0.25}]",一元分布,{},[]
4528,4528,unigram language model,一元语言模型,0.75,8,"[{'word': '一元语言模型', 'ratio': 0.75}, {'word': '单元组语言模型', 'ratio': 0.25}]",一元语言模型,{},[]
4529,4529,unigram model,一元模型,0.625,8,"[{'word': '一元模型', 'ratio': 0.625}, {'word': '单元组模型', 'ratio': 0.25}, {'word': '一元语言模型', 'ratio': 0.125}]",一元模型,{},[]
4530,4530,union bound,联合边界,0.0,8,"[{'word': '并界', 'ratio': 0.5}, {'word': '并集界', 'ratio': 0.375}, {'word': '工会约束', 'ratio': 0.125}]",并界,{},[]
4531,4531,union of conjunctive query,连接查询的并集,0.375,8,"[{'word': '连接查询的并集', 'ratio': 0.375}, {'word': '合取查询的并', 'ratio': 0.25}, {'word': '联合连接查询', 'ratio': 0.125}, {'word': '连结查询的并集', 'ratio': 0.125}, {'word': '合取查询的并集', 'ratio': 0.125}]",合取查询的并集,"1. Rank: 合取查询的并集, 合取查询的并, 连接查询的并集, 联合连接查询, 连结查询的并集

2. Explanation: The term ""合取查询的并集"" (Union of conjunctive queries) is the best fit because it accurately reflects the original English term both semantically and contextually. ""合取"" directly translates to ""conjunctive,"" which is crucial in the context of database queries, as it refers to the logical conjunction used in forming the queries. The phrase ""的并集"" translates to ""union,"" which is also essential in describing the operation being performed on the conjunctive queries. 

The back translation of ""合取查询的并集"" aligns perfectly with the original term, maintaining the intended meaning without introducing ambiguity. 

In contrast, ""连接查询的并集"" (Union of join queries) introduces the term ""连接"" (join), which is not synonymous with ""合取"" (conjunctive) and could lead to confusion in the context of AI and database terminology. The other candidates either lack clarity or do not accurately represent the specific type of query being discussed, making them less suitable for the AI domain.","['Union of join queries', 'Union of conjunctive queries', 'Union join query', 'union of join queries', 'Union of conjunctive queries']"
4532,4532,unit propagation,单位传播,0.1666666666666666,6,"[{'word': '单元传播', 'ratio': 0.8333333333333334}, {'word': '单位传播', 'ratio': 0.16666666666666666}]",单元传播,{},[]
4533,4533,unit sphere,单位球体,0.3333333333333333,6,"[{'word': '单位球面', 'ratio': 0.5}, {'word': '单位球体', 'ratio': 0.3333333333333333}, {'word': '单位球', 'ratio': 0.16666666666666666}]",单位球面,{},[]
4534,4534,universal approximation,通用逼近,0.5,6,"[{'word': '通用逼近', 'ratio': 0.5}, {'word': '泛逼近', 'ratio': 0.16666666666666666}, {'word': '通用近似', 'ratio': 0.16666666666666666}, {'word': '万能近似', 'ratio': 0.16666666666666666}]",通用逼近,{},[]
4535,4535,universal approximator,通用逼近器,0.5,6,"[{'word': '通用逼近器', 'ratio': 0.5}, {'word': '泛逼近器', 'ratio': 0.16666666666666666}, {'word': '通用近似器', 'ratio': 0.16666666666666666}, {'word': '万能逼近器', 'ratio': 0.16666666666666666}]",通用逼近器,{},[]
4536,4536,universal model,通用模型,1.0,6,"[{'word': '通用模型', 'ratio': 1.0}]",通用模型,{},[]
4537,4537,unlabeled datum,未标注数据,0.0,8,"[{'word': '未标记数据', 'ratio': 0.625}, {'word': '无标签数据', 'ratio': 0.375}]",未标记数据,{},[]
4538,4538,unlexicalized grammar,无词汇语法,0.0,8,"[{'word': '非词汇化语法', 'ratio': 0.875}, {'word': '非词汇化文法', 'ratio': 0.125}]",非词汇化语法,{},[]
4539,4539,unnormalized probability,未归一化概率,0.875,8,"[{'word': '未归一化概率', 'ratio': 0.875}, {'word': '非归一化概率', 'ratio': 0.125}]",未归一化概率,{},[]
4540,4540,unsolvability,不可解性,1.0,8,"[{'word': '不可解性', 'ratio': 1.0}]",不可解性,{},[]
4541,4541,unsupervised algorithm,无监督算法,1.0,6,"[{'word': '无监督算法', 'ratio': 1.0}]",无监督算法,{},[]
4542,4542,unsupervised approach,无监督方法,1.0,6,"[{'word': '无监督方法', 'ratio': 1.0}]",无监督方法,{},[]
4543,4543,unsupervised classification,无监督分类,1.0,6,"[{'word': '无监督分类', 'ratio': 1.0}]",无监督分类,{},[]
4544,4544,unsupervised clustering,无监督聚类,1.0,6,"[{'word': '无监督聚类', 'ratio': 1.0}]",无监督聚类,{},[]
4545,4545,unsupervised datum,无监督数据,1.0,4,"[{'word': '无监督数据', 'ratio': 1.0}]",无监督数据,{},[]
4546,4546,unsupervised discovery,无监督发现,1.0,4,"[{'word': '无监督发现', 'ratio': 1.0}]",无监督发现,{},[]
4547,4547,unsupervised disentanglement,无监督解缠,0.25,4,"[{'word': '无监督解缠结', 'ratio': 0.5}, {'word': '无监督解缠', 'ratio': 0.25}, {'word': '无监督解耦', 'ratio': 0.25}]",无监督解缠结,{},[]
4548,4548,unsupervised domain adaptation,无监督域适应,0.0,4,"[{'word': '无监督领域适应', 'ratio': 1.0}]",无监督领域适应,{},[]
4549,4549,unsupervised feature learning,无监督特征学习,1.0,4,"[{'word': '无监督特征学习', 'ratio': 1.0}]",无监督特征学习,{},[]
4550,4550,unsupervised image segmentation,无监督图像分割,1.0,6,"[{'word': '无监督图像分割', 'ratio': 1.0}]",无监督图像分割,{},[]
4551,4551,unsupervised learning,无监督学习,1.0,6,"[{'word': '无监督学习', 'ratio': 1.0}]",无监督学习,{},[]
4552,4552,unsupervised method,无监督方法,1.0,6,"[{'word': '无监督方法', 'ratio': 1.0}]",无监督方法,{},[]
4553,4553,unsupervised model,无监督模型,1.0,6,"[{'word': '无监督模型', 'ratio': 1.0}]",无监督模型,{},[]
4554,4554,unsupervised morphological segmentation,无监督形态分割,0.6666666666666666,6,"[{'word': '无监督形态分割', 'ratio': 0.6666666666666666}, {'word': '无监督形态学分割', 'ratio': 0.3333333333333333}]",无监督形态分割,{},[]
4555,4555,unsupervised parsing,无监督句法分析,0.0,5,"[{'word': '无监督解析', 'ratio': 1.0}]",无监督解析,{},[]
4556,4556,unsupervised pre-training,无监督预训练,1.0,5,"[{'word': '无监督预训练', 'ratio': 1.0}]",无监督预训练,{},[]
4557,4557,unsupervised representation,无监督表征,0.0,5,"[{'word': '无监督表示', 'ratio': 1.0}]",无监督表示,{},[]
4558,4558,unsupervised representation learning,无监督表示学习,1.0,5,"[{'word': '无监督表示学习', 'ratio': 1.0}]",无监督表示学习,{},[]
4559,4559,unsupervised segmentation,无监督分割,1.0,5,"[{'word': '无监督分割', 'ratio': 1.0}]",无监督分割,{},[]
4560,4560,unsupervised system,无监督系统,1.0,9,"[{'word': '无监督系统', 'ratio': 1.0}]",无监督系统,{},[]
4561,4561,unsupervised word clustering,无监督词聚类,1.0,9,"[{'word': '无监督词聚类', 'ratio': 1.0}]",无监督词聚类,{},[]
4562,4562,unweighted graph,无权图,1.0,9,"[{'word': '无权图', 'ratio': 1.0}]",无权图,{},[]
4563,4563,update function,更新函数,1.0,9,"[{'word': '更新函数', 'ratio': 1.0}]",更新函数,{},[]
4564,4564,update rule,更新规则,1.0,9,"[{'word': '更新规则', 'ratio': 1.0}]",更新规则,{},[]
4565,4565,user embedding,用户嵌入,1.0,7,"[{'word': '用户嵌入', 'ratio': 1.0}]",用户嵌入,{},[]
4566,4566,user utterance,用户语句,0.0,7,"[{'word': '用户话语', 'ratio': 0.5714285714285714}, {'word': '用户发话', 'ratio': 0.2857142857142857}, {'word': '用户发言', 'ratio': 0.14285714285714285}]",用户话语,{},[]
4567,4567,user-item matrix,用户-物品矩阵,0.8571428571428571,7,"[{'word': '用户-物品矩阵', 'ratio': 0.8571428571428571}, {'word': '用户-项目矩阵', 'ratio': 0.14285714285714285}]",用户-物品矩阵,{},[]
4568,4568,utility,效用,0.8571428571428571,7,"[{'word': '效用', 'ratio': 0.8571428571428571}, {'word': '公用事业', 'ratio': 0.14285714285714285}]",效用,{},[]
4569,4569,utility function,效用函数,1.0,6,"[{'word': '效用函数', 'ratio': 1.0}]",效用函数,{},[]
4570,4570,utterance,话语,0.25,12,"[{'word': '发话', 'ratio': 0.4166666666666667}, {'word': '话语', 'ratio': 0.3333333333333333}, {'word': '发声', 'ratio': 0.16666666666666666}, {'word': '效用函数', 'ratio': 0.08333333333333333}]",话语,"1. Rank: 话语, 发话, 发声, 效用函数

2. Explanation: The term ""话语"" (back translation: ""Discourse"") is the best fit for the AI domain-specific usage of ""utterance"" because it accurately captures the concept of a sequence of tokens or spoken language directed from a user to a dialog system. In the context of natural language processing and dialogue systems, ""话语"" conveys the idea of a meaningful unit of speech or text, which aligns well with the definition provided in the context. 

""发话"" (back translation: ""speak"") is a close second, as it implies the act of speaking but lacks the nuance of a structured sequence of tokens. ""发声"" (back translation: ""speak out"") also focuses on the act of producing sound rather than the content or structure of the utterance itself. Lastly, ""效用函数"" (back translation: ""utility function"") is unrelated to the concept of ""utterance"" and is therefore not a suitable candidate in this context. 

Overall, ""话语"" is the most semantically accurate and contextually appropriate term for ""utterance"" in the AI domain.","['speak', 'Discourse', 'speak out', 'utility function']"
4571,4571,utterance encoder,话语编码器,0.3333333333333333,6,"[{'word': '发话编码器', 'ratio': 0.5}, {'word': '话语编码器', 'ratio': 0.3333333333333333}, {'word': '发声编码器', 'ratio': 0.16666666666666666}]",发话编码器,{},[]
4572,4572,validation,验证集,0.0,8,"[{'word': '验证', 'ratio': 1.0}]",验证,{},[]
4573,4573,validation accuracy,验证准确率,1.0,8,"[{'word': '验证准确率', 'ratio': 1.0}]",验证准确率,{},[]
4574,4574,validation dataset,验证数据集,1.0,8,"[{'word': '验证数据集', 'ratio': 1.0}]",验证数据集,{},[]
4575,4575,validation datum,验证数据,1.0,8,"[{'word': '验证数据', 'ratio': 1.0}]",验证数据,{},[]
4576,4576,validation loss,验证损失,1.0,8,"[{'word': '验证损失', 'ratio': 1.0}]",验证损失,{},[]
4577,4577,validation performance,验证性能,1.0,5,"[{'word': '验证性能', 'ratio': 1.0}]",验证性能,{},[]
4578,4578,validation set,验证集,1.0,5,"[{'word': '验证集', 'ratio': 1.0}]",验证集,{},[]
4579,4579,validation split,验证集,0.0,5,"[{'word': '验证划分', 'ratio': 0.8}, {'word': '验证拆分', 'ratio': 0.2}]",验证划分,{},[]
4580,4580,value estimate,价值估计,0.8,5,"[{'word': '价值估计', 'ratio': 0.8}, {'word': '值估计', 'ratio': 0.2}]",价值估计,{},[]
4581,4581,value function,值函数,0.2,5,"[{'word': '价值函数', 'ratio': 0.8}, {'word': '值函数', 'ratio': 0.2}]",价值函数,{},[]
4582,4582,value function approximation,值函数近似,0.6666666666666666,6,"[{'word': '值函数近似', 'ratio': 0.6666666666666666}, {'word': '值函数逼近', 'ratio': 0.16666666666666666}, {'word': '价值函数近似', 'ratio': 0.16666666666666666}]",值函数近似,{},[]
4583,4583,value iteration,价值迭代,0.0,6,"[{'word': '值迭代', 'ratio': 1.0}]",值迭代,{},[]
4584,4584,value iteration algorithm,值迭代算法,1.0,6,"[{'word': '值迭代算法', 'ratio': 1.0}]",值迭代算法,{},[]
4585,4585,value network,价值网络,0.1666666666666666,6,"[{'word': '值网络', 'ratio': 0.8333333333333334}, {'word': '价值网络', 'ratio': 0.16666666666666666}]",值网络,{},[]
4586,4586,value-based reinforcement learning,值基强化学习,0.0,6,"[{'word': '基于值的强化学习', 'ratio': 0.6666666666666666}, {'word': '基于值的强化学习 如果您需要更多帮助或有其他问题，请告诉我！', 'ratio': 0.16666666666666666}, {'word': '基于价值的强化学习', 'ratio': 0.16666666666666666}]",基于值的强化学习,{},[]
4587,4587,vanilla Transformer,普通Transformer,0.0,8,"[{'word': '原始Transformer', 'ratio': 0.375}, {'word': '基础变换器', 'ratio': 0.25}, {'word': '基础 Transformer', 'ratio': 0.125}, {'word': 'オリジナルのトランス', 'ratio': 0.125}, {'word': '香草变压器', 'ratio': 0.125}]",原始Transformer,"1. Rank: 原始Transformer, 基础 Transformer, 基础变换器, 香草变压器, オリジナルのトランス

2. Explanation: The term ""原始Transformer"" (Original Transformer) is the best fit because it accurately conveys the concept of the ""vanilla Transformer"" as the basic or standard version of the Transformer model without any modifications or enhancements. In the context of AI and machine learning, ""vanilla"" often refers to the most straightforward or unaltered version of a model, which aligns well with the meaning of ""原始"" (original). 

""基础 Transformer"" (Basic Transformer) is also a strong candidate, as it suggests a foundational model, but ""原始"" captures the essence of being the unmodified version more effectively. ""基础变换器"" (Basic Converter) is less suitable because ""变换器"" (converter) does not accurately reflect the function of the Transformer model in AI. 

""香草变压器"" (vanilla transformer) is a direct translation of ""vanilla,"" but it may not be widely recognized in the AI community and could lead to confusion. Lastly, ""オリジナルのトランス"" (original trans) is a transliteration that does not fit the context of AI terminology and is not appropriate for this specific usage. Thus, ""原始Transformer"" is the most semantically accurate and contextually fitting choice.","['OriginalTransformer', 'basic converter', 'Basic Transformer', 'オリジナルのトランス', 'vanilla transformer']"
4588,4588,vanishing gradient,梯度消失,0.5,8,"[{'word': '梯度消失', 'ratio': 0.5}, {'word': '消失梯度', 'ratio': 0.375}, {'word': '勾配消失', 'ratio': 0.125}]",梯度消失,{},[]
4589,4589,vanishing gradient problem,梯度消失问题,0.5,8,"[{'word': '梯度消失问题', 'ratio': 0.5}, {'word': '消失梯度问题', 'ratio': 0.375}, {'word': '勾配消失問題', 'ratio': 0.125}]",梯度消失问题,{},[]
4590,4590,variable assignment,变量赋值,0.875,8,"[{'word': '变量赋值', 'ratio': 0.875}, {'word': '変数の代入', 'ratio': 0.125}]",变量赋值,{},[]
4591,4591,variable selection,变量选择,0.875,8,"[{'word': '变量选择', 'ratio': 0.875}, {'word': '変数の選択', 'ratio': 0.125}]",变量选择,{},[]
4592,4592,variance reduction,方差缩减,0.0,5,"[{'word': '方差减少', 'ratio': 0.8}, {'word': '方差减小', 'ratio': 0.2}]",方差减少,{},[]
4593,4593,variance regularization,方差正则化,1.0,5,"[{'word': '方差正则化', 'ratio': 1.0}]",方差正则化,{},[]
4594,4594,variational Bayes,变分贝叶斯,1.0,5,"[{'word': '变分贝叶斯', 'ratio': 1.0}]",变分贝叶斯,{},[]
4595,4595,variational approach,变分方法,1.0,5,"[{'word': '变分方法', 'ratio': 1.0}]",变分方法,{},[]
4596,4596,variational approximation,变分近似,1.0,5,"[{'word': '变分近似', 'ratio': 1.0}]",变分近似,{},[]
4597,4597,variational bound,变分边界,0.0,5,"[{'word': '变分界限', 'ratio': 0.6}, {'word': '变分界', 'ratio': 0.4}]",变分界限,{},[]
4598,4598,variational distribution,变分分布,1.0,5,"[{'word': '变分分布', 'ratio': 1.0}]",变分分布,{},[]
4599,4599,variational formulation,变分形式,0.2,5,"[{'word': '变分公式', 'ratio': 0.8}, {'word': '变分形式', 'ratio': 0.2}]",变分公式,{},[]
4600,4600,variational framework,变分框架,1.0,5,"[{'word': '变分框架', 'ratio': 1.0}]",变分框架,{},[]
4601,4601,variational inference,变分推断,0.8,5,"[{'word': '变分推断', 'ratio': 0.8}, {'word': '变分推理', 'ratio': 0.2}]",变分推断,{},[]
4602,4602,variational lower bound,变分下界,1.0,7,"[{'word': '变分下界', 'ratio': 1.0}]",变分下界,{},[]
4603,4603,variational method,变分方法,0.8571428571428571,7,"[{'word': '变分方法', 'ratio': 0.8571428571428571}, {'word': '变分法', 'ratio': 0.14285714285714285}]",变分方法,{},[]
4604,4604,variational model,变分模型,1.0,7,"[{'word': '变分模型', 'ratio': 1.0}]",变分模型,{},[]
4605,4605,variational objective,变分目标函数,0.0,7,"[{'word': '变分目标', 'ratio': 1.0}]",变分目标,{},[]
4606,4606,variational parameter,变分参数,1.0,7,"[{'word': '变分参数', 'ratio': 1.0}]",变分参数,{},[]
4607,4607,variational posterior,变分后验分布,0.0,6,"[{'word': '变分后验', 'ratio': 1.0}]",变分后验,{},[]
4608,4608,vector,向量,1.0,12,"[{'word': '向量', 'ratio': 1.0}]",向量,{},[]
4609,4609,vector arithmetic,向量算术,0.0,6,"[{'word': '向量运算', 'ratio': 1.0}]",向量运算,{},[]
4610,4610,vector concatenation,向量拼接,0.8333333333333334,6,"[{'word': '向量拼接', 'ratio': 0.8333333333333334}, {'word': '向量连接', 'ratio': 0.16666666666666666}]",向量拼接,{},[]
4611,4611,vector embedding,向量嵌入,1.0,7,"[{'word': '向量嵌入', 'ratio': 1.0}]",向量嵌入,{},[]
4612,4612,vector field,矢量场,0.1428571428571428,7,"[{'word': '向量场', 'ratio': 0.8571428571428571}, {'word': '矢量场', 'ratio': 0.14285714285714285}]",向量场,{},[]
4613,4613,vector graphic,矢量图形,0.5714285714285714,7,"[{'word': '矢量图形', 'ratio': 0.5714285714285714}, {'word': '向量图形', 'ratio': 0.42857142857142855}]",矢量图形,{},[]
4614,4614,vector normalization,向量归一化,1.0,7,"[{'word': '向量归一化', 'ratio': 1.0}]",向量归一化,{},[]
4615,4615,vector quantization,向量量化,0.7142857142857143,7,"[{'word': '向量量化', 'ratio': 0.7142857142857143}, {'word': '', 'ratio': 0.14285714285714285}, {'word': '矢量量化', 'ratio': 0.14285714285714285}]",向量量化,{},[]
4616,4616,vector representation,向量表示,1.0,6,"[{'word': '向量表示', 'ratio': 1.0}]",向量表示,{},[]
4617,4617,vector space,向量空间,1.0,6,"[{'word': '向量空间', 'ratio': 1.0}]",向量空间,{},[]
4618,4618,vector space embedding,向量空间嵌入,1.0,6,"[{'word': '向量空间嵌入', 'ratio': 1.0}]",向量空间嵌入,{},[]
4619,4619,vector space model,向量空间模型,1.0,6,"[{'word': '向量空间模型', 'ratio': 1.0}]",向量空间模型,{},[]
4620,4620,vector space representation,向量空间表示,1.0,6,"[{'word': '向量空间表示', 'ratio': 1.0}]",向量空间表示,{},[]
4621,4621,vector-valued function,向量值函数,1.0,8,"[{'word': '向量值函数', 'ratio': 1.0}]",向量值函数,{},[]
4622,4622,vectorization,矢量化,0.125,8,"[{'word': '向量化', 'ratio': 0.875}, {'word': '矢量化', 'ratio': 0.125}]",向量化,{},[]
4623,4623,vectorization operator,矢量化算子,0.0,8,"[{'word': '向量化算子', 'ratio': 1.0}]",向量化算子,{},[]
4624,4624,vectorize,矢量化,0.125,8,"[{'word': '向量化', 'ratio': 0.75}, {'word': '向量化处理', 'ratio': 0.125}, {'word': '矢量化', 'ratio': 0.125}]",向量化,{},[]
4625,4625,verbalizer,转换器,0.0,6,"[{'word': '语言生成器', 'ratio': 0.3333333333333333}, {'word': '词语化器', 'ratio': 0.3333333333333333}, {'word': '语言化器', 'ratio': 0.16666666666666666}, {'word': '言语者', 'ratio': 0.16666666666666666}]",语言生成器,"1. Rank: 语言生成器, 词语化器, 语言化器, 言语者

2. Explanation: The term ""语言生成器"" (language generator) is the best fit for the English term ""verbalizer"" in the context of AI and natural language processing. This is because ""语言生成器"" accurately conveys the function of generating language outputs based on input data, which aligns with the role of a verbalizer in mapping outputs to tokens. 

The other candidates, such as ""词语化器"" (tokenizer), ""语言化器"" (linguist), and ""言语者"" (speaker), do not capture the specific function of generating language in the same way. ""词语化器"" suggests a focus on tokenization rather than generation, which is not the intended meaning in this context. ""语言化器"" implies a more general linguistic function, and ""言语者"" refers to a speaker, which does not relate to the AI function of generating language outputs. Therefore, ""语言生成器"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['language generator', 'tokenizer', 'linguist', 'speaker']"
4626,4626,vertex label,顶点标签,1.0,6,"[{'word': '顶点标签', 'ratio': 1.0}]",顶点标签,{},[]
4627,4627,vertex set,顶点集,0.6666666666666666,6,"[{'word': '顶点集', 'ratio': 0.6666666666666666}, {'word': '顶点集合', 'ratio': 0.3333333333333333}]",顶点集,{},[]
4628,4628,victim model,受害者模型,0.5714285714285714,7,"[{'word': '受害者模型', 'ratio': 0.5714285714285714}, {'word': '受害模型', 'ratio': 0.42857142857142855}]",受害者模型,{},[]
4629,4629,view synthesis,视图合成,1.0,7,"[{'word': '视图合成', 'ratio': 1.0}]",视图合成,{},[]
4630,4630,virtual camera,虚拟相机,1.0,7,"[{'word': '虚拟相机', 'ratio': 1.0}]",虚拟相机,{},[]
4631,4631,vision model,视觉模型,1.0,7,"[{'word': '视觉模型', 'ratio': 1.0}]",视觉模型,{},[]
4632,4632,vision system,视觉系统,0.8571428571428571,7,"[{'word': '视觉系统', 'ratio': 0.8571428571428571}, {'word': '视觉系统 如果您需要更多帮助或有其他问题，请告诉我！', 'ratio': 0.14285714285714285}]",视觉系统,{},[]
4633,4633,vision-language model,视觉语言模型,0.5,6,"[{'word': '视觉-语言模型', 'ratio': 0.5}, {'word': '视觉语言模型', 'ratio': 0.5}]",视觉-语言模型,{},[]
4634,4634,visual attention,视觉注意力,1.0,6,"[{'word': '视觉注意力', 'ratio': 1.0}]",视觉注意力,{},[]
4635,4635,visual attribute,视觉属性,1.0,6,"[{'word': '视觉属性', 'ratio': 1.0}]",视觉属性,{},[]
4636,4636,visual context,视觉环境,0.0,6,"[{'word': '视觉上下文', 'ratio': 1.0}]",视觉上下文,{},[]
4637,4637,visual cortex,视觉皮层,1.0,7,"[{'word': '视觉皮层', 'ratio': 1.0}]",视觉皮层,{},[]
4638,4638,visual feature,视觉特征,1.0,7,"[{'word': '视觉特征', 'ratio': 1.0}]",视觉特征,{},[]
4639,4639,visual grounding,视觉接地,0.1428571428571428,7,"[{'word': '视觉定位', 'ratio': 0.7142857142857143}, {'word': '视觉基础', 'ratio': 0.14285714285714285}, {'word': '视觉接地', 'ratio': 0.14285714285714285}]",视觉定位,{},[]
4640,4640,visual hull,视觉外壳,0.5714285714285714,7,"[{'word': '视觉外壳', 'ratio': 0.5714285714285714}, {'word': '视觉壳', 'ratio': 0.14285714285714285}, {'word': '视觉包络', 'ratio': 0.14285714285714285}, {'word': '视觉船体', 'ratio': 0.14285714285714285}]",视觉外壳,{},[]
4641,4641,visual localization,视觉定位,1.0,7,"[{'word': '视觉定位', 'ratio': 1.0}]",视觉定位,{},[]
4642,4642,visual odometry,视觉里程计,1.0,6,"[{'word': '视觉里程计', 'ratio': 1.0}]",视觉里程计,{},[]
4643,4643,visual recognition system,视觉识别系统,1.0,6,"[{'word': '视觉识别系统', 'ratio': 1.0}]",视觉识别系统,{},[]
4644,4644,vocabulary,词汇量,0.4166666666666667,12,"[{'word': '词汇', 'ratio': 0.75}, {'word': '词汇表', 'ratio': 0.16666666666666666}, {'word': '词表', 'ratio': 0.08333333333333333}]",词汇,{},[]
4645,4645,vocabulary size,词汇量,0.1666666666666666,6,"[{'word': '词汇大小', 'ratio': 0.6666666666666666}, {'word': '词表大小', 'ratio': 0.16666666666666666}, {'word': '词汇量', 'ratio': 0.16666666666666666}]",词汇大小,{},[]
4646,4646,vocoder,声码器,0.5,6,"[{'word': '声码器', 'ratio': 0.5}, {'word': '语音编码器', 'ratio': 0.3333333333333333}, {'word': '编码器', 'ratio': 0.16666666666666666}]",声码器,{},[]
4647,4647,volume rendering,体积渲染,1.0,6,"[{'word': '体积渲染', 'ratio': 1.0}]",体积渲染,{},[]
4648,4648,von Mises-Fisher distribution,冯米塞斯-菲舍尔分布,0.0,6,"[{'word': '冯·米塞斯-费舍尔分布', 'ratio': 0.8333333333333334}, {'word': '冯米塞斯-费舍尔分布', 'ratio': 0.16666666666666666}]",冯·米塞斯-费舍尔分布,{},[]
4649,4649,voting rule,投票规则,1.0,6,"[{'word': '投票规则', 'ratio': 1.0}]",投票规则,{},[]
4650,4650,voxel,体素,1.0,6,"[{'word': '体素', 'ratio': 1.0}]",体素,{},[]
4651,4651,voxel grid,体素栅格,0.0,8,"[{'word': '体素网格', 'ratio': 1.0}]",体素网格,{},[]
4652,4652,voxel grid representation,体素网格表示,1.0,8,"[{'word': '体素网格表示', 'ratio': 1.0}]",体素网格表示,{},[]
4653,4653,voxel occupancy,体素占用,1.0,8,"[{'word': '体素占用', 'ratio': 1.0}]",体素占用,{},[]
4654,4654,voxel representation,体素表示,1.0,8,"[{'word': '体素表示', 'ratio': 1.0}]",体素表示,{},[]
4655,4655,voxel-based representation,体素表示,0.0,8,"[{'word': '基于体素的表示', 'ratio': 1.0}]",基于体素的表示,{},[]
4656,4656,warp function,变形函数,0.8571428571428571,7,"[{'word': '变形函数', 'ratio': 0.8571428571428571}, {'word': '扭曲函数', 'ratio': 0.14285714285714285}]",变形函数,{},[]
4657,4657,wav2vec,wav2vec,1.0,7,"[{'word': 'wav2vec', 'ratio': 1.0}]",wav2vec,{},[]
4658,4658,wavelet,小波,1.0,7,"[{'word': '小波', 'ratio': 1.0}]",小波,{},[]
4659,4659,wavelet transform,小波变换,1.0,7,"[{'word': '小波变换', 'ratio': 1.0}]",小波变换,{},[]
4660,4660,weak classifier,弱分类器,1.0,7,"[{'word': '弱分类器', 'ratio': 1.0}]",弱分类器,{},[]
4661,4661,weak learner,弱学习者,0.375,8,"[{'word': '弱学习器', 'ratio': 0.625}, {'word': '弱学习者', 'ratio': 0.375}]",弱学习器,{},[]
4662,4662,weak learning,弱学习,0.875,8,"[{'word': '弱学习', 'ratio': 0.875}, {'word': '学习薄弱', 'ratio': 0.125}]",弱学习,{},[]
4663,4663,weak learning assumption,弱学习假设,1.0,8,"[{'word': '弱学习假设', 'ratio': 1.0}]",弱学习假设,{},[]
4664,4664,weak supervision,弱监督,0.875,8,"[{'word': '弱监督', 'ratio': 0.875}, {'word': '监管薄弱', 'ratio': 0.125}]",弱监督,{},[]
4665,4665,weakly supervised,弱监督,0.125,8,"[{'word': '弱监督的', 'ratio': 0.5}, {'word': '弱监督学习', 'ratio': 0.375}, {'word': '弱监督', 'ratio': 0.125}]",弱监督的,{},[]
4666,4666,weakly supervised learning,弱监督学习,1.0,5,"[{'word': '弱监督学习', 'ratio': 1.0}]",弱监督学习,{},[]
4667,4667,web graph,网络图,0.8,5,"[{'word': '网络图', 'ratio': 0.8}, {'word': '网页图', 'ratio': 0.2}]",网络图,{},[]
4668,4668,weight,权重,1.0,5,"[{'word': '权重', 'ratio': 1.0}]",权重,{},[]
4669,4669,weight decay,权重衰减,1.0,5,"[{'word': '权重衰减', 'ratio': 1.0}]",权重衰减,{},[]
4670,4670,weight initialization,权重初始化,1.0,5,"[{'word': '权重初始化', 'ratio': 1.0}]",权重初始化,{},[]
4671,4671,weight matrix,权重矩阵,1.0,6,"[{'word': '权重矩阵', 'ratio': 1.0}]",权重矩阵,{},[]
4672,4672,weight parameter,权重参数,1.0,6,"[{'word': '权重参数', 'ratio': 1.0}]",权重参数,{},[]
4673,4673,weight regularization,权重正则化,1.0,6,"[{'word': '权重正则化', 'ratio': 1.0}]",权重正则化,{},[]
4674,4674,weight tensor,权重张量,1.0,6,"[{'word': '权重张量', 'ratio': 1.0}]",权重张量,{},[]
4675,4675,weight update,权重更新,1.0,6,"[{'word': '权重更新', 'ratio': 1.0}]",权重更新,{},[]
4676,4676,weight vector,权重向量,1.0,4,"[{'word': '权重向量', 'ratio': 1.0}]",权重向量,{},[]
4677,4677,weight-sharing,权重共享,1.0,4,"[{'word': '权重共享', 'ratio': 1.0}]",权重共享,{},[]
4678,4678,weighted adjacency matrix,加权邻接矩阵,1.0,4,"[{'word': '加权邻接矩阵', 'ratio': 1.0}]",加权邻接矩阵,{},[]
4679,4679,weighted average,加权平均,1.0,4,"[{'word': '加权平均', 'ratio': 1.0}]",加权平均,{},[]
4680,4680,weighted directed graph,加权有向图,1.0,4,"[{'word': '加权有向图', 'ratio': 1.0}]",加权有向图,{},[]
4681,4681,weighted graph,权重图,0.0,8,"[{'word': '加权图', 'ratio': 1.0}]",加权图,{},[]
4682,4682,weighted sum,加权和,1.0,8,"[{'word': '加权和', 'ratio': 1.0}]",加权和,{},[]
4683,4683,weighting function,权重函数,0.625,8,"[{'word': '权重函数', 'ratio': 0.625}, {'word': '加权函数', 'ratio': 0.375}]",权重函数,{},[]
4684,4684,white-box,白盒,1.0,8,"[{'word': '白盒', 'ratio': 1.0}]",白盒,{},[]
4685,4685,white-box attack,白盒攻击,1.0,6,"[{'word': '白盒攻击', 'ratio': 1.0}]",白盒攻击,{},[]
4686,4686,window size,窗口大小,1.0,6,"[{'word': '窗口大小', 'ratio': 1.0}]",窗口大小,{},[]
4687,4687,within-class variance,类内方差,1.0,6,"[{'word': '类内方差', 'ratio': 1.0}]",类内方差,{},[]
4688,4688,word alignment,词对齐,0.5,6,"[{'word': '单词对齐', 'ratio': 0.5}, {'word': '词对齐', 'ratio': 0.5}]",单词对齐,{},[]
4689,4689,word dropout,词丢弃,0.3333333333333333,9,"[{'word': '词丢弃', 'ratio': 0.3333333333333333}, {'word': '单词丢弃', 'ratio': 0.2222222222222222}, {'word': '词语丢弃', 'ratio': 0.2222222222222222}, {'word': '单词丢失', 'ratio': 0.1111111111111111}, {'word': '词丢失', 'ratio': 0.1111111111111111}]",词丢弃,"1. Rank: 词丢弃, 单词丢弃, 词语丢弃, 单词丢失, 词丢失

2. Explanation: The term ""词丢弃"" (word discard) is the best fit for ""word dropout"" in the AI context because it accurately captures the concept of intentionally removing words from a dataset or computation process, which aligns with the technical meaning of ""dropout"" in machine learning. The term ""丢弃"" (discard) conveys the idea of omitting or excluding something, which is precisely what is happening in the context of word dropout. 

""单词丢弃"" (word discard) and ""词语丢弃"" (word discard) are also acceptable, but ""词丢弃"" is more concise and commonly used in technical literature. 

On the other hand, ""单词丢失"" (word missing) and ""词丢失"" (word missing) imply a loss rather than a deliberate action of dropping words, which does not accurately reflect the intended meaning in the context of dropout techniques in AI. Therefore, they are less suitable for this specific terminology.","['word discard', 'word discard', 'word discard', 'word missing', 'word missing']"
4690,4690,word embedding,词嵌入,0.5555555555555556,9,"[{'word': '词嵌入', 'ratio': 0.5555555555555556}, {'word': '单词嵌入', 'ratio': 0.3333333333333333}, {'word': 'word embedding', 'ratio': 0.1111111111111111}]",词嵌入,{},[]
4691,4691,word embedding model,词嵌入模型,0.6666666666666666,9,"[{'word': '词嵌入模型', 'ratio': 0.6666666666666666}, {'word': '单词嵌入模型', 'ratio': 0.3333333333333333}]",词嵌入模型,{},[]
4692,4692,word representation,词表示,0.6666666666666666,9,"[{'word': '词表示', 'ratio': 0.6666666666666666}, {'word': '单词表示', 'ratio': 0.3333333333333333}]",词表示,{},[]
4693,4693,word segmentation,词语分割,0.0,9,"[{'word': '分词', 'ratio': 0.7777777777777778}, {'word': '单词分割', 'ratio': 0.2222222222222222}]",分词,{},[]
4694,4694,word sense disambiguation,词义消歧,1.0,9,"[{'word': '词义消歧', 'ratio': 1.0}]",词义消歧,{},[]
4695,4695,word similarity,词语相似度,0.5555555555555556,9,"[{'word': '词语相似度', 'ratio': 0.5555555555555556}, {'word': '词语相似性', 'ratio': 0.3333333333333333}, {'word': '词相似性', 'ratio': 0.1111111111111111}]",词语相似度,{},[]
4696,4696,word surprisal,词惊喜值,0.0,9,"[{'word': '词语惊讶度', 'ratio': 0.4444444444444444}, {'word': '词语意外度', 'ratio': 0.2222222222222222}, {'word': '词意惊讶度', 'ratio': 0.1111111111111111}, {'word': '词语惊异度', 'ratio': 0.1111111111111111}, {'word': '词语意外性', 'ratio': 0.1111111111111111}]",词语惊讶度,"1. Rank: 词语惊讶度, 词语惊异度, 词语意外度, 词语意外性, 词意惊讶度

2. Explanation: The term ""词语惊讶度"" (word surprisal) is the best fit because it accurately captures the concept of ""surprisal"" in the context of language processing and AI. The term ""惊讶"" (surprise) directly relates to the statistical measure of how unexpected a word is in a given context, which aligns with the definition of word surprisal in information theory and natural language processing. 

The back translation ""word surprise"" is also semantically accurate, maintaining the intended meaning. 

""词语惊异度"" (word surprise) is a close second, as ""惊异"" also conveys a sense of surprise, but it is less commonly used in the AI context compared to ""惊讶."" 

The other candidates, such as ""词语意外度"" (word unexpectedness) and ""词语意外性"" (word unexpectedness), while they convey a related concept, do not align as closely with the established terminology in the AI domain. ""意外"" (unexpected) lacks the specific nuance of ""surprisal"" that is critical in this context. 

Overall, ""词语惊讶度"" is the most contextually appropriate and semantically accurate term for ""word surprisal"" in the AI field.","['word surprise', 'word unexpectedness', 'word meaning surprise', 'word surprise', 'word unexpectedness']"
4697,4697,word token,词标记,0.0,9,"[{'word': '词语标记', 'ratio': 0.4444444444444444}, {'word': '词元', 'ratio': 0.3333333333333333}, {'word': '词符', 'ratio': 0.2222222222222222}]",词元,"1. Rank: 词元, 词语标记, 词符

2. Explanation: The term ""词元"" (word element) is the best fit for ""word token"" in the AI domain because it accurately captures the concept of a discrete unit of text that is processed in natural language processing tasks. In the context of AI and machine learning, ""词元"" is commonly used to refer to the basic units of text that algorithms analyze, making it semantically accurate and contextually appropriate. 

On the other hand, ""词语标记"" (word tag) suggests a focus on tagging or labeling rather than the fundamental unit of text, which may lead to confusion in the context of the provided sentences. ""词符"" (symbol) is too vague and does not convey the specific meaning of a token in natural language processing, as it could refer to any symbol rather than a word unit. Therefore, ""词元"" is the most precise and contextually relevant choice for the term ""word token"" in this AI context.","['word tag', 'word element', 'Symbol']"
4698,4698,word vector,词向量,1.0,9,"[{'word': '词向量', 'ratio': 1.0}]",词向量,{},[]
4699,4699,word vector representation,词向量表示,0.7777777777777778,9,"[{'word': '词向量表示', 'ratio': 0.7777777777777778}, {'word': '单词向量表示', 'ratio': 0.1111111111111111}, {'word': '独热表示', 'ratio': 0.1111111111111111}]",词向量表示,{},[]
4700,4700,word-aligned corpus,词对齐语料库,0.6666666666666666,9,"[{'word': '词对齐语料库', 'ratio': 0.6666666666666666}, {'word': '单词对齐语料库', 'ratio': 0.1111111111111111}, {'word': '对齐词汇语料库', 'ratio': 0.1111111111111111}, {'word': '独热向量', 'ratio': 0.1111111111111111}]",词对齐语料库,{},[]
4701,4701,word-level,单词级别,0.1111111111111111,9,"[{'word': '词级', 'ratio': 0.4444444444444444}, {'word': '词级别', 'ratio': 0.2222222222222222}, {'word': '词级/词层级', 'ratio': 0.1111111111111111}, {'word': '单词级别', 'ratio': 0.1111111111111111}, {'word': '单样本学习', 'ratio': 0.1111111111111111}]",词级,"1. Rank: 词级, 词级别, 词级/词层级, 单词级别, 单样本学习

2. Explanation: The term ""词级"" is the best fit for the translation of ""word-level"" in the context of AI and translation quality estimation. This term is concise and directly conveys the concept of ""word level"" without unnecessary complexity. It is commonly used in computational linguistics and AI contexts, making it semantically accurate and contextually appropriate. 

""词级别"" is also a good candidate, but it is slightly longer and less commonly used in the specific context of AI. ""词级/词层级"" introduces redundancy with the use of both ""级"" and ""层级,"" which can lead to confusion. ""单词级别"" translates to ""single word level,"" which may imply a different meaning than intended. Lastly, ""单样本学习"" translates to ""one-sample learning,"" which is unrelated to the concept of ""word-level"" and is therefore not a suitable candidate. 

Overall, ""词级"" is the most precise and contextually relevant term for the AI domain, particularly in the context of translation quality estimation and phonetic learning tasks.","['word level', 'word level', 'word level/word level', 'word level', 'One-sample learning']"
4702,4702,word-level vocabulary,词级词汇表,0.2222222222222222,9,"[{'word': '词级词汇', 'ratio': 0.4444444444444444}, {'word': '词级词汇表', 'ratio': 0.2222222222222222}, {'word': '单词级词汇表', 'ratio': 0.1111111111111111}, {'word': '词汇表', 'ratio': 0.1111111111111111}, {'word': '单样本设置', 'ratio': 0.1111111111111111}]",词级词汇表,"1. Rank: 词级词汇表, 词级词汇, 单词级词汇表, 词汇表, 单样本设置

2. Explanation: The term ""词级词汇表"" (word-level vocabulary) is the best fit because it accurately captures the concept of a vocabulary that is organized at the word level, which is essential in the context of natural language processing and AI. The inclusion of ""表"" (table or list) emphasizes that it refers to a structured collection of words, aligning well with the technical usage in AI literature. 

""词级词汇"" (word-level vocabulary) is also a strong candidate, but it lacks the explicit indication of a structured list, which is implied by ""词汇表."" ""单词级词汇表"" (single word-level vocabulary) is less preferred because it introduces ambiguity by suggesting a focus on individual words rather than a comprehensive vocabulary. ""词汇表"" (glossary) is too general and does not specify the word-level aspect, while ""单样本设置"" (single sample setup) is unrelated to the term in question. Thus, ""词级词汇表"" is the most semantically accurate and contextually appropriate choice for the AI domain.","['word level vocabulary', 'word-level vocabulary', 'word-level vocabulary', 'Glossary', 'Single sample setup']"
4703,4703,word2vec embedding,词向量嵌入,0.0,10,"[{'word': 'word2vec 嵌入', 'ratio': 0.6}, {'word': 'word2vec词嵌入', 'ratio': 0.3}, {'word': '嵌入嵌入', 'ratio': 0.1}]",word2vec 嵌入,{},[]
4704,4704,world state,世界状态,1.0,10,"[{'word': '世界状态', 'ratio': 1.0}]",世界状态,{},[]
4705,4705,worst case,最坏情况,1.0,10,"[{'word': '最坏情况', 'ratio': 1.0}]",最坏情况,{},[]
4706,4706,worst-case regret,最坏情况遗憾,0.7,10,"[{'word': '最坏情况遗憾', 'ratio': 0.7}, {'word': '最坏情况后悔', 'ratio': 0.3}]",最坏情况遗憾,{},[]
4707,4707,zero-one loss,0-1损失,0.25,8,"[{'word': '零一损失', 'ratio': 0.75}, {'word': '0-1损失', 'ratio': 0.25}]",零一损失,{},[]
4708,4708,zero-shot classification,零样本分类,1.0,8,"[{'word': '零样本分类', 'ratio': 1.0}]",零样本分类,{},[]
4709,4709,zero-shot cross-lingual setting,零样本跨语言设置,1.0,8,"[{'word': '零样本跨语言设置', 'ratio': 1.0}]",零样本跨语言设置,{},[]
4710,4710,zero-shot generalization,零样本泛化,1.0,8,"[{'word': '零样本泛化', 'ratio': 1.0}]",零样本泛化,{},[]
4711,4711,zero-shot learning,零样本学习,1.0,8,"[{'word': '零样本学习', 'ratio': 1.0}]",零样本学习,{},[]
4712,4712,zero-shot prediction,零样本预测,1.0,8,"[{'word': '零样本预测', 'ratio': 1.0}]",零样本预测,{},[]
4713,4713,zero-shot prompting,零样本提示,1.0,8,"[{'word': '零样本提示', 'ratio': 1.0}]",零样本提示,{},[]
4714,4714,zero-shot reasoning,零样本推理,1.0,8,"[{'word': '零样本推理', 'ratio': 1.0}]",零样本推理,{},[]
4715,4715,zero-shot setting,零样本设置,1.0,8,"[{'word': '零样本设置', 'ratio': 1.0}]",零样本设置,{},[]
4716,4716,zero-shot transfer,零次迁移,0.0,7,"[{'word': '零样本迁移', 'ratio': 1.0}]",零样本迁移,{},[]
4717,4717,zero-shot transfer learning,零样本迁移学习,1.0,7,"[{'word': '零样本迁移学习', 'ratio': 1.0}]",零样本迁移学习,{},[]
