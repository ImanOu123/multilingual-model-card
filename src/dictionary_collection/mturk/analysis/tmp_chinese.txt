Apriori Chinese
('先验算法', '1. Rank: 先验算法, 先驗, Apriori, 阿普里奥\n\n2. Explanation: The term "先验算法" (a priori algorithm) is the best fit because it accurately conveys the meaning of the English term "Apriori" within the context of data mining and AI. The term "先验" (a priori) is also a strong candidate, as it retains the original concept but lacks the specificity of "算法" (algorithm), which is crucial in the context of algorithmic processes. The term "Apriori" as a direct transliteration does not provide any additional context or clarity, making it less suitable for readers unfamiliar with the term. Lastly, "阿普里奥" (Aprilio) is a transliteration that does not convey the intended meaning and is not commonly used in the AI domain. Therefore, "先验算法" is the most semantically accurate and contextually appropriate choice for this specific usage in AI terminology.', ['a priori algorithm', 'a priori', 'A priori', 'Aprilio'])
Apriori algorithm Chinese
('Apriori算法', '1. Rank: Apriori算法, Apriori演算法, Apriori 算法, 先验算法, 先验i算法, 先驗演算法, 算法\n\n2. Explanation: The term "Apriori算法" is the best fit because it retains the original name "Apriori" while clearly indicating that it is an algorithm by using the Chinese word "算法." This combination is widely recognized in the AI and data mining communities, making it semantically accurate and contextually appropriate. The use of "Apriori" in its original form helps maintain the specific reference to the algorithm, which is crucial in technical discussions. \n\n"Apriori演算法" is also a good candidate, but the term "演算法" (which translates to "algorithm" in a more general sense) is less commonly used in the context of this specific algorithm compared to "算法." \n\nThe other candidates, such as "先验算法" and "先验i算法," introduce the term "先验" (a priori) but do not retain the specific name "Apriori," which is essential for clarity and recognition in the AI domain. The term "算法" alone is too vague and lacks the necessary specificity. \n\nOverall, "Apriori算法" is the most accurate and contextually fitting translation for the term in the AI domain.', ['a priori algorithm', 'Apriori algorithm', 'a priori algorithm', 'a priori algorithm', 'Apriori algorithm', 'algorithm', 'Apriori algorithm'])
Azuma-Hoeffding inequality Chinese
('阿祖马-霍夫丁不等式', '1. Rank: 阿祖马-霍夫丁不等式, Azuma-Hoeffding不等式, Azuma-Hoeffding 不等式, 阿祖玛-霍夫丁不等式, 阿祖玛-赫夫丁不等式, 安祖马-霍夫丁不等式, 不等式\n\n2. Explanation: The term "阿祖马-霍夫丁不等式" is the best fit because it accurately retains the original names "Azuma" and "Hoeffding" in a way that is recognizable and consistent with established terminology in the AI and mathematical literature. The use of "阿祖马" for "Azuma" and "霍夫丁" for "Hoeffding" aligns with common transliterations used in academic contexts, ensuring that readers familiar with the field can easily identify the reference. \n\nThe other candidates, such as "Azuma-Hoeffding不等式" and "Azuma-Hoeffding 不等式," while also accurate, do not provide the same level of semantic clarity in Chinese as they mix English and Chinese without fully localizing the names. The transliterations "阿祖玛" and "安祖马" deviate from the standard transliteration of "Azuma," which could lead to confusion. \n\nOverall, the first choice maintains both semantic accuracy and contextual fit, making it the most appropriate for use in AI-related discussions.', ['Azuma-Hoffding inequality', 'Azuma-Hoeffding inequality', 'Azuma-Hoeffding inequality', 'Azuma-Hoffding inequality', 'inequality', 'Ajumma-Hefting inequality', 'Anzuma-Hoffding inequality'])
Bhattacharyya coefficient Chinese
('巴塔查尔雅系数', '1. Rank: 巴塔查尔雅系数, 巴塔恰里亚系数, 巴塔查里亚系数, 巴哈查里亚系数, 巴氏系数, Bhattacharyya 系数 / 巴氏系数\n\n2. Explanation: The term "巴塔查尔雅系数" (Bhattacharyya coefficient) is the best fit because it retains the original name of the mathematician Bhattacharyya, which is crucial in the AI domain where specific algorithms and coefficients are often named after their creators. This helps maintain clarity and precision in academic and technical discussions. The other candidates, while they may be phonetically similar, do not preserve the integrity of the original name as effectively. The use of "系数" (coefficient) is appropriate in this context, as it directly translates the mathematical concept being discussed. The other terms either misrepresent the name or do not provide the same level of recognition in the AI field.', ['Babbitt coefficient', 'Bhattacharya coefficient', 'Bhattacharya coefficient', 'Bhattacharya coefficient', 'Bhattacharyya coefficient / Bhattacharyya coefficient', 'bhacharya coefficient'])
Boltzmann distribution Chinese
('玻尔兹曼分布', '1. Rank: 玻尔兹曼分布, 波尔兹曼分布, 博尔兹曼分布, Boltzmann分布, 分布, 受保护属性\n\n2. Explanation: The term "玻尔兹曼分布" is the most widely accepted and semantically accurate translation of "Boltzmann distribution" in the context of statistical mechanics and AI. It retains the original name "Boltzmann" while providing a clear and precise meaning in Chinese. The back translation accurately reflects the English term, ensuring that it is understood in the same context. \n\nThe other candidates, such as "波尔兹曼分布" and "博尔兹曼分布," are also valid transliterations but are less commonly used in academic and professional contexts. "Boltzmann分布" is a direct mix of English and Chinese, which may not be appropriate in formal writing. "分布" alone is too vague and does not convey the specific concept of the Boltzmann distribution. Lastly, "受保护属性" is unrelated to the term and should not be considered. \n\nIn the AI domain, where precision and clarity are crucial, "玻尔兹曼分布" stands out as the best choice due to its established usage and clear connection to the original term.', ['Boltzmann distribution', 'Boltzmann distribution', 'Boltzmann distribution', 'distributed', 'Boltzmann distribution', 'protected attribute'])
Boltzmann exploration Chinese
('玻尔兹曼探索', '1. Rank: 玻尔兹曼探索, 波尔兹曼探索, 博尔兹曼探索, Boltzmann探索, 探索, 蛋白质折叠\n\n2. Explanation: The term "玻尔兹曼探索" is the best fit because it accurately retains the original name "Boltzmann" while also clearly conveying the concept of "exploration" in the context of AI. The use of "玻尔兹曼" is a widely accepted transliteration of "Boltzmann" in scientific literature, ensuring semantic accuracy and recognition within the AI domain. The term "探索" (exploration) complements it well, making the entire phrase contextually appropriate for discussions around exploration strategies in AI. \n\nIn contrast, "波尔兹曼探索" is also a valid option but is less commonly used than "玻尔兹曼探索." "博尔兹曼探索" is another acceptable transliteration but may not be as recognized in the AI community. "Boltzmann探索" and "探索" lack the full context of the term, and "蛋白质折叠" (protein folding) is entirely unrelated to the concept of exploration in AI, making it irrelevant in this context.', ['Boltzmann exploration', 'Boltzmann Exploration', 'BoltzmannExplore', 'explore', 'Bolzeman Discovery', 'protein folding'])
Bonferroni correction Chinese
('邦费罗尼校正', '1. Rank: 邦费罗尼校正, 邦弗朗尼校正, 本费罗尼校正, Bonferroni校正, 校正, 原型嵌入\n\n2. Explanation: The term "邦费罗尼校正" is the best fit because it accurately retains the phonetic elements of the original name "Bonferroni" while also being widely recognized in the statistical community in Chinese-speaking regions. This term is semantically accurate and aligns with the established usage in the AI and statistical domains. The other candidates, while they may be phonetically similar, do not have the same level of recognition or established usage. "邦弗朗尼校正" and "本费罗尼校正" are also reasonable but less common. "Bonferroni校正" is a mix of English and Chinese, which may not be as suitable in formal contexts. The terms "校正" and "原型嵌入" are unrelated to the specific term "Bonferroni correction" and thus do not fit the context at all.', ['Bonferroni correction', 'Bonferroni correction', 'Benferroni correction', 'Bonferroni correction', 'Correction', 'prototype embedding'])
Borda scores Chinese
('"博尔达得分"', '1. Rank: "博尔达得分", "博尔达分数", "博尔达评分", "Borda得分", "博达得分", "博达分数"\n\n2. Explanation: The term "博尔达得分" is the best fit because it accurately retains the original name "Borda" while providing a clear and precise translation of "scores" as "得分". This term is semantically accurate and contextually appropriate for the AI domain, where precision in terminology is crucial. The use of "得分" (scores) directly relates to the scoring mechanism described in the context of aggregation rules. \n\nThe second choice, "博尔达分数", is also a good option, as it maintains the name "Borda" and translates "scores" as "分数", which is synonymous with "得分" but slightly less common in this specific context. \n\nThe third option, "博尔达评分", translates "scores" as "评分" (rating), which could imply a different meaning, as "评分" is often associated with qualitative assessments rather than quantitative scores. \n\nThe remaining options, "Borda得分", "博达得分", and "博达分数", either use a transliteration of "Borda" or deviate from the established terminology, making them less suitable for the AI context. Retaining the original name "Borda" is essential for clarity and recognition in academic and professional discussions.', ['Borda scores', 'Borda score', 'Borda rating', 'boda score', 'boda score', 'Borda score'])
Branch and Bound Chinese
('分支限界法', '1. Rank: 分支限界法, 分支定界法, 分支定界\n\n2. Explanation: The term "分支限界法" (branch and bound method) is the best fit for the AI domain-specific usage because it accurately captures the full meaning of the English term "Branch and Bound" while also including the word "法" (method), which is commonly used in mathematical and algorithmic contexts in Chinese. This term is widely recognized in the field of optimization and algorithms, making it semantically accurate and contextually appropriate. \n\nThe second candidate, "分支定界法" (branch and bound method), is also a valid translation but uses "定界" (bound) instead of "限界" (limit), which may not be as commonly used in this specific context. The third candidate, "分支定界" (branch and bound), lacks the word "法" (method), which diminishes its clarity and specificity in a technical context. Therefore, "分支限界法" is the most precise and contextually fitting choice.', ['branch and bound method', 'branch and bound', 'branch and bound'])
Bundle adjustment Chinese
('捆绑调整', '1. Rank: 捆绑调整, 捆集调整/束调整, 捆束调整, 束调整, 光束平差法, 光束法平差\n\n2. Explanation: The term "捆绑调整" (bundle adjustment) is the best fit for the translation of "Bundle adjustment" in the AI domain, particularly in the context of computer vision and structure from motion (SfM). This term accurately captures the semantic meaning of the original English term, as "捆绑" directly translates to "bundle," which is commonly used in the context of adjusting multiple parameters simultaneously, and "调整" means "adjustment." \n\nThe back translation of "捆绑调整" as "bundle adjustment" maintains the original term\'s integrity and is widely recognized in the field, making it contextually appropriate. Other candidates like "束调整" (beam adjustment) and "光束平差法" (beam adjustment method) introduce confusion with unrelated concepts in optics and do not convey the specific meaning of refining 3D structures and camera poses. \n\nThe terms "捆集调整/束调整" and "捆束调整" are less common and may not be as readily understood in the AI community. Therefore, "捆绑调整" stands out as the most accurate and contextually relevant translation for "Bundle adjustment."', ['beam adjustment', 'bundle adjustment', 'Bundle adjustment', 'Bundle adjustment/bundle adjustment', 'beam adjustment method', 'Beam method adjustment'])
Caltech-101 Chinese
('加州理工学院101数据集', '1. Rank: 加州理工学院101数据集, Caltech-101数据集, 加州理工学院101, Caltech-101\n\n2. Explanation: The term "加州理工学院101数据集" (Caltech-101 dataset) is the best fit because it provides a clear and complete description of the dataset while retaining the original name of the institution (California Institute of Technology). This is important in the AI domain, where datasets are often referred to by their full names to ensure clarity and context. The inclusion of "数据集" (dataset) explicitly indicates that it refers to a collection of data, which is essential for understanding its application in image classification experiments. The other candidates either lack the dataset context or do not fully convey the significance of the original name, making them less suitable for academic and technical discussions in AI.', ['Caltech-101 dataset', 'Caltech 101 Dataset', 'Caltech 101', 'Caltech-101'])
Chamfer Distance Chinese
('查姆弗距离', '1. Rank: 查姆弗距离, 倒角距离, 切削距离, 钱弗距离, 香农距离, 切边距离, 切夫距离\n\n2. Explanation: The term "查姆弗距离" (Chamfer Distance) is the best fit because it is a direct transliteration of the English term "Chamfer," which is essential in maintaining the original meaning and context within the AI domain. In technical fields, especially in AI and computer graphics, it is crucial to use terms that are widely recognized and understood by professionals. "查姆弗" retains the phonetic sound of "Chamfer," making it easily recognizable to those familiar with the term. \n\nOn the other hand, "倒角距离" (Bevel Distance) is a less accurate translation because "倒角" refers to a bevel, which is a different geometric concept and does not convey the same meaning as "Chamfer." The other candidates, such as "切削距离" (Cutting Distance), "钱弗距离" (Chandler Distance), "香农距离" (Shannon Distance), "切边距离" (Tangent Distance), and "切夫距离" (Cheff Distance), either misinterpret the term or introduce unrelated concepts, making them unsuitable for the specific context of 3D reconstruction and geometry comparison. Thus, "查姆弗距离" is the most semantically accurate and contextually appropriate choice.', ['chamfer distance', 'cutting distance', 'Chandler distance', 'Shannon distance', 'tangent distance', 'chamfer distance', 'Cheff distance'])
Charniak parser Chinese
('"Charniak解析器"', '1. Rank: "Charniak解析器", "查尼亚克解析器", "查尔尼亚克解析器", "查尼亞克解析器", "查尼亚克分析器", "Charniak 解析器"\n\n2. Explanation: The term "Charniak解析器" is the best fit because it retains the original name "Charniak" in its English form, which is crucial in the AI domain where specific algorithms and parsers are often referred to by their creators\' names. This helps maintain clarity and recognition within the field. The back translation accurately reflects the original term, ensuring semantic accuracy. \n\nThe other candidates, such as "查尼亚克解析器" and "查尔尼亚克解析器," while they do transliterate the name, introduce variations that may not be as widely recognized or accepted in the AI community. The use of "解析器" (parser) is consistent across the top candidates, which is appropriate for the context of parsing in natural language processing. \n\nOverall, "Charniak解析器" stands out for its directness and adherence to the established naming conventions in the field, making it the most suitable choice.', ['Chargnak parser', 'Charniak parser', 'Chargnak parser', 'Chargnac Analyzer', 'Charnyak parser', 'Charniak parser'])
Cholesky decomposition Chinese
('乔尔斯基分解', '1. Rank: 乔尔斯基分解, 乔列斯基分解, Cholesky 分解, 楚尔斯基分解\n\n2. Explanation: The term "乔尔斯基分解" is the most widely accepted and recognized translation for "Cholesky decomposition" in the AI and mathematical communities. It retains the phonetic similarity to the original name while also being semantically accurate. The use of "分解" (decomposition) is appropriate in the mathematical context, making it clear that this term refers to a specific mathematical operation. \n\nThe second candidate, "乔列斯基分解," is a less common variant that may not be as widely recognized, which could lead to confusion. The third candidate, "Cholesky 分解," while accurate, mixes English and Chinese, which is less preferable in formal writing. Lastly, "楚尔斯基分解" is a transliteration that does not closely resemble the original name and is not commonly used in the field, making it the least suitable option. Therefore, "乔尔斯基分解" is the best fit for its semantic accuracy, contextual relevance, and recognition in the AI domain.', ['Cholsky decomposition', 'Choleski decomposition', 'Cholesky decomposition', 'Chulsky decomposition'])
Cholesky factor Chinese
('乔列斯基因子', '1. Rank: 乔列斯基因子, Cholesky 因子, 乔尔斯基因子, 楚尔斯基因子\n\n2. Explanation: The term "乔列斯基因子" (Choleski factor) is the best fit because it retains the phonetic similarity to the original term "Cholesky" while also providing a clear indication that it is a mathematical term (因子 means "factor"). This term is widely recognized in the mathematical and AI communities, making it semantically accurate and contextually appropriate. The back translation "Choleski factor" closely aligns with the original English term, ensuring clarity in communication within the AI domain. \n\nThe second option, "Cholesky 因子," while also accurate, does not provide a localized phonetic representation of the name, which may be less familiar to Chinese speakers. The other candidates, "乔尔斯基因子" and "楚尔斯基因子," deviate further from the original term, either through incorrect transliteration or by introducing confusion with unrelated names. Thus, "乔列斯基因子" stands out as the most effective choice for conveying the intended meaning in the context of AI and mathematics.', ['Joel gene', 'Choleski factor', 'Cholesky factor', 'Chulsky'])
Cholesky factorization Chinese
('乔列斯基分解', '1. Rank: 乔列斯基分解, 乔尔斯基分解法, Cholesky 分解法, 乔尔斯基分解法c, 楚尔斯基分解, 乔尔斯基因子分解\n\n2. Explanation: The term "乔列斯基分解" is the best fit because it accurately retains the original name "Cholesky" while providing a clear and standard translation for "factorization" as "分解". This term is widely recognized in the mathematical and AI communities, ensuring semantic accuracy and contextual fit. The other candidates either deviate from the correct name (e.g., "乔尔斯基" or "楚尔斯基") or introduce unnecessary variations (e.g., "分解法" which implies a method rather than a general term). The use of "Cholesky" in its original form in "Cholesky 分解法" is also acceptable, but it is less common than the first option. The remaining candidates either misinterpret the term or are not standard in the field, making them less suitable for AI domain-specific usage.', ['Choleski decomposition', 'Cholsky decomposition method', "Joel's gene decomposition", 'Cholsky decomposition method c', 'Cholesky decomposition method', 'Chulsky decomposition'])
Chu-Liu-Edmonds algorithm Chinese
('"Chu-Liu-Edmonds 算法"', '1. Rank: "Chu-Liu-Edmonds 算法", "Chu-Liu-Edmonds算法", "朱-刘-埃德蒙兹算法", "朱-刘-爱德蒙兹算法", "朱柳-埃德蒙兹算法", "朱刘埃德蒙兹算法", "楚-刘-埃德蒙兹算法"\n\n2. Explanation: The top-ranked translations, "Chu-Liu-Edmonds 算法" and "Chu-Liu-Edmonds算法", are the best fits because they retain the original English term "Chu-Liu-Edmonds" in its entirety, which is crucial in the AI domain where specific algorithms are often referred to by their original names. This helps maintain clarity and consistency, especially in academic and technical contexts where the algorithm is well-known. \n\nThe use of "算法" (algorithm) is standard in Chinese technical terminology, making these translations semantically accurate and contextually appropriate. The other candidates, while they may provide some level of understanding, either alter the original names or use transliterations that could lead to confusion, especially for readers familiar with the algorithm in its original form. \n\nFor instance, "朱-刘-埃德蒙兹算法" and "朱-刘-爱德蒙兹算法" attempt to transliterate the names but do not preserve the original structure as effectively as "Chu-Liu-Edmonds 算法". The remaining candidates, such as "朱柳-埃德蒙兹算法" and "楚-刘-埃德蒙兹算法", further deviate from the established naming convention, which could hinder recognition and understanding in the AI community.', ['Zhu-Liu-Edmonds algorithm', 'Zhu-Liu-Edmonds algorithm', 'Zhuliu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Zhu Liu Edmunds Algorithm'])
Chung-Lu model Chinese
('Chung-Lu模型', '1. Rank: Chung-Lu模型, 钟陆模型, 钟路模型, Chung-Lu モデル, 种-卢模型\n\n2. Explanation: The term "Chung-Lu模型" is the best fit because it retains the original name "Chung-Lu" while also using the Chinese character "模型" which means "model." This is important in the AI domain as it maintains the integrity of the original term while providing clarity in the context of modeling. The other candidates, such as "钟陆模型" and "钟路模型," are transliterations that may not accurately convey the original meaning or context, especially since they could lead to confusion with other terms or names. "种-卢模型" introduces an unrelated term "种" (species), which is not relevant in this context. "Chung-Lu モデル" uses Japanese characters, which is not appropriate for a Chinese context. Therefore, "Chung-Lu模型" is the most semantically accurate and contextually appropriate choice.', ['Chung-Lu model', 'Zhonglu model', 'Chung-Lu モデル', 'Species-Lu model', 'Jongno model'])
Cohen's kappa Chinese
('科恩卡帕系数', '1. Rank: 科恩卡帕系数, 科恩的卡帕, 科恩κ系数, Cohen的κ系数, Cohen\'s κ, コーエンのカッパ\n\n2. Explanation: The term "科恩卡帕系数" is the best fit because it accurately retains the original name "Cohen" while also including "kappa" in a way that is familiar to Chinese speakers in the context of statistical measures. The addition of "系数" (coefficient) clarifies that this term refers to a statistical metric, which is essential in the AI domain where precision in terminology is crucial. The back translation "Cohen\'s kappa coefficient" aligns perfectly with the English term, ensuring semantic accuracy. Other candidates like "科恩的卡帕" and "科恩κ系数" are also good, but they either lack the explicit mention of "系数" or do not fully convey the statistical nature of the term as effectively as "科恩卡帕系数".', ["Cohen's kappa coefficient", "Cohen's Kappa", 'コーエンのカッパ', "Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's κ"])
Cohen's kappa coefficient Chinese
('"Cohen\'s kappa系数"', '1. Rank: "Cohen\'s kappa系数", "科恩的卡帕系数", "科恩的κ系数", "Cohen\'s kappa 系数", "Cohen的卡帕系数", "科恩卡帕系数"\n\n2. Explanation: The term "Cohen\'s kappa系数" is the best fit because it retains the original name "Cohen" in a way that is both semantically accurate and contextually appropriate for the AI domain. The use of "kappa" in its original form is crucial as it is a standard term in statistics and machine learning, ensuring clarity and recognition among professionals in the field. The term "系数" (coefficient) is correctly used to denote a statistical measure, making the entire phrase clear and precise. \n\nThe other candidates, while they may be accurate, either use transliterations that could lead to confusion (like "Cohen的卡帕系数" or "科恩卡帕系数") or introduce unnecessary variations (like "科恩的κ系数"), which may not be as widely recognized in the AI community. The first choice maintains the integrity of the original term while ensuring it is easily understood in the context of statistical analysis, making it the most suitable option.', ["Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's κ coefficient", "Cohen's kappa coefficient"])
Cohen's κ Chinese
('"科恩κ系数"', '1. Rank: "科恩κ系数", "Cohen\'s κ系数", "科恩的κ", "Cohen的κ", "Cohen\'s κ"\n\n2. Explanation: The term "科恩κ系数" is the best fit because it retains the original name "Cohen" while clearly indicating that it is a statistical measure (κ系数, meaning "kappa coefficient"). This translation is semantically accurate and contextually appropriate for the AI domain, as it conveys both the identity of the measure and its statistical nature. The use of "κ系数" directly relates to the concept of kappa in statistics, making it easily understandable for those familiar with the field. \n\nThe second choice, "Cohen\'s κ系数", is also a strong candidate as it maintains the possessive form, but it is slightly less concise than the first option. The other candidates, such as "科恩的κ" and "Cohen的κ", lack the clarity of indicating that it is a coefficient, which is crucial in a statistical context. Lastly, "Cohen\'s κ" is a direct transliteration and does not provide any additional context, making it less suitable for readers who may not be familiar with the term.', ["Cohen's kappa coefficient", "Cohen's κ", "Cohen's κ", "Cohen's κ", "Cohen's kappa coefficient"])
Condorcet winner Chinese
('"孔多塞赢家"', '1. Rank: "孔多塞赢家", "孔多塞获胜者", "康多塞获胜者", "孔多塞胜者", "孟德尔森赢家"\n\n2. Explanation: The term "孔多塞赢家" is the best fit because it accurately retains the original name "Condorcet" in its Chinese form while using the term "赢家" (winner), which is a direct and commonly understood translation in the context of game theory and voting systems. This term is semantically accurate and aligns well with the AI domain-specific usage, as it clearly conveys the concept of a winner in a competitive context. \n\nThe second choice, "孔多塞获胜者," is also a good option, as it translates to "Condorcet winner" but uses "获胜者," which is slightly more formal and less commonly used in casual contexts compared to "赢家." \n\nThe third choice, "康多塞获胜者," is a transliteration that introduces a variation in the name "Condorcet," which could lead to confusion. \n\nThe fourth choice, "孔多塞胜者," while still retaining the name, uses "胜者," which is less common in this specific context compared to "赢家." \n\nLastly, "孟德尔森赢家" introduces a completely different name ("Mendelson"), which is not relevant to the original term and thus ranks the lowest in terms of semantic accuracy and contextual fit.', ['Condorcet Winner', 'victor of condorcet', 'mendelson winner', 'Condorcet winner', 'Condorcet Winner'])
Coreference Resolution Chinese
('共指消解', '1. Rank: 共指消解, 指代消解, 共指解析\n\n2. Explanation: The term "共指消解" (coreference resolution) is the best fit because it accurately captures the concept of identifying and resolving references to the same entity within a text, which is the essence of coreference resolution in natural language processing. The back translation "coreference resolution" aligns perfectly with the original English term, ensuring semantic accuracy. \n\nThe second candidate, "指代消解" (reference resolution), while also relevant, is slightly less precise as it can refer to any type of reference resolution, not specifically coreference. This could lead to ambiguity in the context of AI and NLP, where distinguishing between different types of references is crucial.\n\nThe third candidate, "共指解析" (coreference analysis), introduces the term "解析" (analysis), which may imply a broader or different scope than "消解" (resolution). In the AI domain, "resolution" is the more commonly used term when discussing the process of resolving references, making "共指消解" the most contextually appropriate choice.', ['coreference resolution', 'Reference resolution', 'coreference analysis'])
Datalog Chinese
('数据逻辑', '1. Rank: 数据逻辑, 数据日志语言, Datalog, 数据日志, 计算机科学家, 電腦科學家\n\n2. Explanation: The term "数据逻辑" (Data logic) is the best fit for the translation of "Datalog" in the context of AI and computer science. This is because "数据逻辑" accurately captures the semantic essence of Datalog as a logic programming language that deals with data and logical inference. The term "数据" (data) directly relates to the data-centric nature of Datalog, while "逻辑" (logic) reflects its foundation in logical reasoning.\n\nThe second candidate, "数据日志语言" (data logging language), is less accurate because it implies a focus on logging data rather than the logical inference aspect of Datalog. The term "Datalog" itself is a proper noun and should be retained in contexts where the specific programming language is being discussed, but it does not convey the meaning as effectively as "数据逻辑."\n\nThe other candidates, "数据日志" (data log) and the translations for "computer scientist," do not relate to the term "Datalog" and are therefore irrelevant in this context. Thus, "数据逻辑" stands out as the most semantically accurate and contextually appropriate translation for the term in the AI domain.', ['Data logic', 'data logging language', 'computer scientist', 'computer scientist', 'data log'])
Decentralized optimization Chinese
('去中心化优化', '1. Rank: 去中心化优化, 分散优化, 分散式优化, 分布式优化\n\n2. Explanation: The term "去中心化优化" (Decentralized optimization) is the best fit because it directly translates the concept of decentralization, which is crucial in the context of optimization methods that do not rely on a central authority. This term accurately reflects the AI domain\'s emphasis on decentralized systems, particularly in scenarios where networks are dynamic or unknown. \n\n"分散优化" (decentralized optimization) is also a strong candidate, as it conveys a similar meaning, but it lacks the explicit reference to "去中心化" (decentralization), which is more commonly used in AI literature to describe systems that operate without a central control point.\n\n"分散式优化" (decentralized-style optimization) is less preferred because the addition of "式" (style) can imply a method or approach rather than the core concept of decentralization itself.\n\n"分布式优化" (distributed optimization) is the least suitable in this context, as "分布式" (distributed) typically refers to systems where tasks are distributed across multiple nodes but may still rely on a central coordination mechanism, which does not align with the intended meaning of "decentralized" in the context provided.', ['Decentralized optimization', 'decentralized optimization', 'decentralized optimization', 'Distributed optimization'])
Demographic Parity Chinese
('人口统计平等', '1. Rank: 人口统计平等, 人口统计公平, 人口统计学公平性, 人口平等, 人口统计平衡\n\n2. Explanation: The term "人口统计平等" (Demographic equality) is the best fit for "Demographic Parity" because it accurately captures the essence of the concept in the AI domain, which emphasizes equality across different demographic groups in predictions. The back translation "Demographic equality" aligns closely with the original English term, maintaining semantic accuracy. \n\n"人口统计公平" (Demographic equity) and "人口统计学公平性" (Demographic Equity) are also relevant but slightly less precise, as "equity" can imply a focus on fairness rather than strict equality, which is the core of demographic parity. \n\n"人口平等" (Population equality) is a more general term and does not specifically convey the statistical aspect implied by "demographic." Lastly, "人口统计平衡" (Demographic balance) suggests a focus on balance rather than equality, which diverges from the intended meaning of demographic parity. Thus, "人口统计平等" is the most contextually appropriate choice in the AI terminology landscape.', ['Demographic equality', 'Population equality', 'Demographic balance', 'Demographic Equity', 'Demographic equity'])
Dice coefficient Chinese
('"Dice系数"', '1. Rank: "Dice系数", "Dice 系数", "骰子系数", "戴斯系数"\n\n2. Explanation: The term "Dice系数" is the best fit for the translation of "Dice coefficient" in the AI domain due to its semantic accuracy and established usage in the field. The term "Dice" is a well-known metric in machine learning and information retrieval, particularly in the context of evaluating similarity between sets. The use of "系数" (coefficient) is appropriate and aligns with mathematical terminology in Chinese.\n\nThe alternative "Dice 系数" is also acceptable, but the space between "Dice" and "系数" is less common in technical contexts. "骰子系数" translates to "dice coefficient" literally, but it may introduce confusion as "骰子" (dice) can refer to the physical object rather than the metric itself. Lastly, "戴斯系数" is a transliteration that does not retain the original meaning and is not recognized in the AI community, making it the least suitable option. Thus, "Dice系数" is the most contextually accurate and widely accepted term in the AI domain.', ['Dice coefficient', 'Dais coefficient', 'Dice coefficient', 'Dice coefficient'])
Dice loss Chinese
('Dice损失', '1. Rank: Dice损失, 骰子损失, 戴斯损失, Dice 损失\n\n2. Explanation: The term "Dice损失" is the best fit because it accurately translates "Dice loss" while maintaining the semantic integrity of the term within the AI domain. "Dice" refers to the Dice coefficient, a well-known metric in image segmentation tasks, and "损失" means "loss," which is a standard term in machine learning contexts. This translation is widely recognized in the AI community, ensuring clarity and precision in communication.\n\nThe second option, "骰子损失," translates to "Dice loss" but uses the word "骰子," which means "dice" in a general sense, rather than the specific term used in the context of the Dice coefficient. This could lead to confusion, as it does not directly connect to the established terminology in AI.\n\nThe third option, "戴斯损失," translates to "Days loss," which is incorrect and does not relate to the original term at all. \n\nThe last option, "Dice 损失," while it retains the term "Dice," is less preferable because it includes a space that is not standard in Chinese terminology, making it less formal and potentially confusing in a technical context. \n\nOverall, "Dice损失" is the most semantically accurate and contextually appropriate translation for the AI domain.', ['Dice loss', 'Days loss', 'Loss', 'Dice loss'])
Dijkstra's algorithm Chinese
('Dijkstra算法', '1. Rank: Dijkstra算法, 迪杰斯特拉算法, Dijkstra 算法, 迪克斯特拉算法, 戴克斯特拉算法\n\n2. Explanation: The term "Dijkstra算法" is the best fit because it directly translates to "Dijkstra\'s algorithm" while retaining the original name "Dijkstra," which is crucial in the context of algorithmic terminology. This term is widely recognized in the AI and computer science communities in Chinese-speaking regions, ensuring semantic accuracy and contextual fit. The other candidates, while they may be understandable, either use transliterations that are less common (like "迪杰斯特拉算法" or "戴克斯特拉算法") or do not follow the standard naming conventions in the field (like "Dijkstra 算法," which is less formal). Retaining the original name is important for clarity and recognition in academic and professional settings, making "Dijkstra算法" the most appropriate choice.', ["Dijkstra's algorithm", "Dijkstra's algorithm", "Dijkstra's algorithm", "Dijkstra's algorithm", "Dijkstra's algorithm"])
Disentanglement Chinese
('解耦', '1. Rank: 解耦, 解缠结, 解缠\n\n2. Explanation: The term "解耦" (decoupling) is the best fit for the context of "disentanglement" in the AI domain. In machine learning and AI, "decoupling" refers to the process of separating different components or factors in a model, which aligns closely with the concept of disentanglement, where the goal is to isolate and understand the individual factors that contribute to the overall data representation. \n\nWhile "解缠结" (untangle) and "解缠" (untangle) are also valid translations, they are more general and do not capture the specific technical nuance of separating or isolating components as effectively as "解耦." The back translation of "解耦" as "decoupling" further emphasizes its relevance in the context of AI, where disentanglement is often discussed in terms of separating variables or features in a model. Thus, "解耦" is the most semantically accurate and contextually appropriate choice.', ['Untangle', 'decoupling', 'Untangle'])
Dropout distribution Chinese
('Dropout分布', '1. Rank: Dropout分布, Dropout 分布, 随机失活分布, 丢弃分布\n\n2. Explanation: The term "Dropout分布" is the best fit because it retains the original English term "Dropout" while providing a clear and direct translation of "distribution" as "分布". This combination is widely recognized in the AI and machine learning communities, particularly in the context of neural networks and Bayesian methods. The use of "Dropout" in its original form helps maintain consistency with established terminology in the field, making it easily understandable for practitioners familiar with the concept. \n\nThe second candidate, "Dropout 分布," is also a strong option, but the space between "Dropout" and "分布" is less common in technical writing, where terms are often presented as a single unit. \n\n"随机失活分布" translates to "random deactivation distribution," which, while semantically accurate, introduces a different nuance that may not align with the established terminology of "dropout" in neural networks. \n\nLastly, "丢弃分布" translates to "drop distribution," which is less precise and could lead to confusion, as "丢弃" (to discard) does not capture the specific meaning of "dropout" in the context of neural networks. Thus, "Dropout分布" is the most contextually appropriate choice.', ['Dropout distribution', 'random deactivation distribution', 'Dropout distribution', 'drop distribution'])
Epanechnikov kernel Chinese
('"Epanechnikov核函数"', '1. Rank: "Epanechnikov核函数", "埃帕涅奇科夫核", "艾潘尼奇科夫核", "伊帕内奇尼科夫核", "核函数", "核", "Epanechnikov核"\n\n2. Explanation: The term "Epanechnikov核函数" is the best fit because it accurately retains the original name "Epanechnikov" while also specifying that it is a "kernel function," which is crucial in the context of AI and statistics. This term provides both semantic accuracy and clarity, making it immediately understandable to those familiar with the field. The other terms, such as "埃帕涅奇科夫核" and "艾潘尼奇科夫核," are transliterations that do not include the term "函数" (function), which is essential for conveying the complete meaning in the context of kernel methods in AI. The term "核函数" is too generic and lacks specificity regarding the Epanechnikov aspect, while "核" and "Epanechnikov核" do not provide the necessary context of a function. Therefore, "Epanechnikov核函数" stands out as the most contextually appropriate and semantically accurate choice.', ['kernel function', 'Epanechnikov core', 'nuclear', 'Epanechikov core', 'Ipanichkov core', 'Ipanechnikov core', 'Epanechnikov kernel function'])
F-measure Chinese
('F-measure', '1. Rank: F-measure, F值, F度量, F-测量, \n\n2. Explanation: The term "F-measure" is the most accurate translation because it retains the original English term, which is widely recognized in the AI and machine learning communities. This term is commonly used in academic literature and technical discussions, making it the most contextually appropriate choice. The back translation also remains "F-measure," which indicates that the term is understood in its original form, ensuring clarity and consistency in communication.\n\nThe second choice, "F值," translates to "F value," which is also acceptable but less specific than "F-measure." While it conveys a similar meaning, it may not be as widely recognized in the context of performance metrics in AI.\n\nThe third option, "F度量," translates to "F metric," which is somewhat accurate but less common than "F-measure." It may not be as immediately recognizable to those familiar with the specific metric being discussed.\n\nThe last two options, "F-测量" and the empty string, are not suitable as they either introduce ambiguity or lack context altogether. Therefore, "F-measure" is the best fit for its semantic accuracy and contextual relevance in the AI domain.', ['F-measure', 'F value', 'F-measure', 'F-measure', 'F-measurement'])
Fisher information matrix Chinese
('费舍尔信息矩阵', '1. Rank: 费舍尔信息矩阵, 费舍信息矩阵, 菲舍尔信息矩阵\n\n2. Explanation: The term "费舍尔信息矩阵" is the most accurate translation for "Fisher information matrix" because it correctly retains the original name "Fisher" in a widely accepted transliteration that is recognized in the AI and statistics communities. This term is semantically accurate and aligns with the established terminology used in academic literature. \n\nThe second candidate, "费舍信息矩阵," is slightly less accurate because it omits the "尔" character, which can lead to confusion or misinterpretation in the context of statistical terminology. The third candidate, "菲舍尔信息矩阵," uses "菲" instead of "费," which is not the standard transliteration for "Fisher" and could lead to misunderstandings, especially in a technical context. Therefore, "费舍尔信息矩阵" is the best fit for its semantic accuracy and contextual relevance in the AI domain.', ['Fisher information matrix', 'Fisher information matrix', 'Fischer information matrix'])
Fisher score Chinese
('费舍尔得分', '1. Rank: 费舍尔得分, 费舍尔评分, 费舍尔分数, 菲舍尔评分, 菲舍尔得分\n\n2. Explanation: The term "费舍尔得分" is the best fit because it accurately retains the original name "Fisher" while using "得分," which translates to "score" in a contextually appropriate manner for statistical and AI terminology. This term is semantically accurate and aligns well with the common usage in the AI domain, where "得分" is frequently used to denote scores or metrics. \n\n"费舍尔评分" is also a strong candidate, as "评分" translates to "rating," which is somewhat relevant but less precise than "得分" in this context. "费舍尔分数" translates to "Fisher fraction," which introduces ambiguity and does not accurately reflect the intended meaning of "score." The other candidates, "菲舍尔评分" and "菲舍尔得分," use a transliteration of "Fisher" as "菲舍尔," which is less common in the AI literature compared to "费舍尔." Therefore, "费舍尔得分" is the most contextually appropriate and semantically accurate choice.', ['fisher score', 'Fischer score', 'Fisher score', 'Fisher fraction', 'Fischer scores'])
Fleiss' kappa Chinese
('弗莱斯κ系数', '1. Rank: 弗莱斯κ系数, 弗莱斯κ值, Fleiss\' kappa, Fleiss kappa系数, 弗莱斯卡帕系数, Fleiss\' κ, 弗里斯卡帕系数\n\n2. Explanation: The term "弗莱斯κ系数" (Fleiss kappa coefficient) is the best fit because it accurately retains the original name "Fleiss" while also including the term "κ" (kappa) and "系数" (coefficient), which is a standard term in statistical contexts. This translation is semantically accurate and contextually appropriate for the AI domain, where precision in terminology is crucial. The use of "系数" clearly indicates that it is a statistical measure, which aligns with the context of inter-annotator agreement being discussed. \n\nThe second choice, "弗莱斯κ值" (Fleiss kappa value), is also a good option but uses "值" (value) instead of "系数" (coefficient), which is less specific in a statistical context. The other candidates either do not retain the original name effectively or use less precise terminology, making them less suitable for this specific application in AI and statistics.', ['Fleiss kappa coefficient', 'Fleiss kappa value', 'Class F SS kappa', 'Fleiss kappa coefficient', 'Fleiskappa coefficient', "Fleiss' κ", 'Frisian Kappa coefficient'])
Floyd-Warshall algorithm Chinese
('弗洛伊德-沃肖尔算法', '1. Rank: 弗洛伊德-沃肖尔算法, Floyd-Warshall算法, 弗洛伊德-沃尔沙尔算法, 弗洛伊德-华舍尔算法, 弗洛伊德-华沙算法, 弗洛伊德-沃肖算法, 弗洛伊德-沃沙尔算法\n\n2. Explanation: The top-ranked translation, "弗洛伊德-沃肖尔算法," is the best fit because it accurately retains the original name "Floyd" and "Warshall," which is crucial in the context of algorithm names in the AI domain. This translation uses the correct phonetic representation of "Warshall" and maintains the structure of the original term, making it easily recognizable to those familiar with the algorithm. \n\nThe second choice, "Floyd-Warshall算法," is also acceptable as it retains the original English term, but it lacks the full Chinese representation, which is often preferred in academic and technical contexts. \n\nThe other candidates either misrepresent the names (e.g., "弗洛伊德-沃尔沙尔算法" and "弗洛伊德-华沙算法") or introduce inaccuracies in the transliteration (e.g., "Freud" instead of "Floyd"), which can lead to confusion and misidentification of the algorithm. Therefore, the first option is the most semantically accurate and contextually appropriate for the AI domain.', ['Floyd-Warshall algorithm', 'Freud-Walshall algorithm', 'Freud-Waschel algorithm', 'Freud-Warsaw algorithm', 'Floyd-Warshall algorithm', 'Floyd-Warshaw algorithm', 'Freud-Warshal algorithm'])
Frobenius Norm Chinese
('弗罗贝尼乌斯范数', '1. Rank: 弗罗贝尼乌斯范数, 弗罗贝纽斯范数, Frobenius范数, Frobenius 范数\n\n2. Explanation: The term "弗罗贝尼乌斯范数" is the best fit because it accurately retains the original name "Frobenius" while providing a clear and standard translation of "norm" as "范数". This term is widely recognized in the mathematical and AI communities, ensuring semantic accuracy and contextual fit. The use of "弗罗贝尼乌斯" is a well-established transliteration of "Frobenius" in academic literature, making it the most appropriate choice for clarity and recognition. \n\nThe alternative "弗罗贝纽斯范数" is also a valid option, but "弗罗贝尼乌斯" is more commonly used. The terms "Frobenius范数" and "Frobenius 范数" do not include the transliteration of the name in a way that aligns with standard Chinese usage, which may lead to confusion or lack of recognition in the AI domain. Therefore, the first term is the most suitable for conveying the intended meaning in a precise and contextually relevant manner.', ['Frobenius norm', 'Frobenius norm', 'Frobenius norm', 'Frobenius norm'])
Fréchet Chinese
('Fréchet', '1. Rank: Fréchet, 弗雷歇, 弗雷謝特\n\n2. Explanation: The first translation, "Fréchet," is the best fit because it retains the original term without any alteration, which is crucial in the context of AI and statistical terminology. In academic and technical fields, especially in mathematics and statistics, it is common to keep the names of specific distributions or concepts in their original form to avoid confusion and maintain clarity. The second option, "弗雷歇," is a transliteration that closely resembles the pronunciation of "Fréchet" in Mandarin, making it a reasonable alternative, but it does not capture the exact name as it is used in the literature. The third option, "弗雷謝特," is less accurate as it introduces an unnecessary variation in the transliteration, which could lead to misinterpretation. Therefore, retaining the original term "Fréchet" is the most semantically accurate and contextually appropriate choice in the AI domain.', ['Fréchet', 'Flecher', 'Frechette'])
GoogLeNet Chinese
('"GoogLeNet"', '1. Rank: "GoogLeNet", "谷歌网络", "谷歌网", "Google網"\n\n2. Explanation: The first translation candidate, "GoogLeNet", is the best fit because it retains the original name of the neural network architecture, which is crucial in the AI domain. In technical contexts, especially in machine learning and deep learning, it is important to use the exact names of models to avoid confusion and ensure clarity. The second candidate, "谷歌网络" (Google network), is a reasonable translation that conveys the association with Google while maintaining the context of a network, but it does not preserve the specific branding of "GoogLeNet." The third candidate, "谷歌网" (Google.com), is less appropriate as it refers to a website rather than the neural network architecture. Lastly, "Google網" (Google.com) is a transliteration that does not provide any semantic clarity regarding the AI model itself. Therefore, the original term "GoogLeNet" is the most accurate and contextually appropriate choice.', ['Google net', 'google.com', 'google network', 'Google.com'])
Gröbner basis Chinese
('"格罗布纳基"', '1. Rank: "格罗布纳基", "Gröbner 基", "格羅布納基地", "基础"\n\n2. Explanation: The term "格罗布纳基" is the best fit because it retains the original name "Gröbner" while providing a clear indication that it refers to a mathematical basis (基) in the context of algebraic geometry and computational algebra. This term is widely recognized in the AI and mathematics communities, making it semantically accurate and contextually appropriate. The other candidates either do not retain the original name or are less commonly used in the specific context of Gröbner bases, which are crucial in symbolic computation and algorithm design. "Gröbner 基" is also a strong candidate, but it is less commonly used than "格罗布纳基". "格羅布納基地" is a transliteration that may not be as familiar to practitioners, and "基础" (base) is too generic and lacks the specificity needed for this technical term.', ['Base', 'Grobnaki', 'grobner base', 'Gröbner base'])
Gumbel Chinese
('甘贝尔分布', '1. Rank: 甘贝尔分布, 甘贝尔, 甘貝爾, 古贝尔, Gumbel\n\n2. Explanation: The term "甘贝尔分布" (Gambel distribution) is the best fit because it accurately reflects the specific statistical distribution associated with the Gumbel name in the context of AI and probability theory. It retains the original name "Gumbel" while also indicating that it refers to a distribution, which is crucial for understanding its application in the given context. The second choice, "甘贝尔," is also a good option as it retains the name but lacks the explicit reference to the distribution. The transliterations "甘貝爾" and "古贝尔" do not convey the statistical context as effectively, and "Gumbel" in English does not provide any semantic clarity in a Chinese context. Therefore, "甘贝尔分布" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['gum be', 'Gambel', 'Gambel distribution', 'Gubel', 'Gambel'])
Gumbel distribution Chinese
('Gumbel 分布', '1. Rank: Gumbel 分布, 甘贝尔分布, 甘貝爾分佈, 古贝尔分布\n\n2. Explanation: The term "Gumbel 分布" is the best fit because it retains the original name "Gumbel" in its transliterated form, which is crucial in the AI domain where specific statistical distributions are often referred to by their names. This term is widely recognized in both English and Chinese academic literature, ensuring semantic accuracy and contextual fit. The other candidates, such as "甘贝尔分布" and "甘貝爾分佈," while they attempt to transliterate the name, do not maintain the same level of recognition and may lead to confusion. "古贝尔分布" is less accurate as it deviates significantly from the original name, making it the least suitable choice. Therefore, "Gumbel 分布" is the most appropriate translation for its clarity and adherence to established terminology in the field.', ['Gumbel distribution', 'Gambel distribution', 'Gambel distribution', 'Gubel distribution'])
Gumbel noise Chinese
('甘布尔噪声', '1. Rank: 甘布尔噪声, 古姆贝尔噪声, 甘贝尔噪声, 冈贝尔噪声, 甘博尔噪声\n\n2. Explanation: The term "甘布尔噪声" is the best fit because it accurately retains the phonetic structure of the original English term "Gumbel noise" while also being widely recognized in the AI and statistical communities. The back translation "Gamble noise" is incorrect, as it does not convey the intended meaning related to Gumbel distributions in statistics. The second choice, "古姆贝尔噪声," is also a close phonetic match but is less commonly used in the context of AI. The other candidates either introduce inaccuracies in pronunciation or semantic meaning, making them less suitable for domain-specific usage.', ['Gamble noise', 'Gumball noise', 'Gumbel noise', 'Gumbel noise', 'Gambel noise'])
Gumbel-softmax distribution Chinese
('"古姆贝尔-softmax分布"', '1. Rank: "古姆贝尔-softmax分布", "冈贝尔-软最大分布", "甘布尔-软最大分布", "甘贝尔-softmax分布", "甘博尔软最大分布", "甘布尔软最大分布"\n\n2. Explanation: The term "古姆贝尔-softmax分布" is the best fit because it accurately retains the original name "Gumbel" while also being semantically correct in the context of AI terminology. The back translation "Gumbel-softmax distribution" matches the original English term perfectly, ensuring clarity and consistency in the technical context. \n\nThe other candidates, such as "冈贝尔-软最大分布" and "甘布尔-软最大分布," also maintain a close semantic relationship but may introduce slight variations in pronunciation or meaning that could lead to confusion. The remaining candidates, particularly those that deviate significantly from the original name (like "甘博尔软最大分布" and "甘布尔软最大分布"), do not preserve the integrity of the term as effectively, which is crucial in the AI domain where precise terminology is essential for understanding and communication.', ['Gamble-softmax distribution', 'Gumball soft maximum distribution', 'Gumbel-softmax distribution', 'Gumbel-softmax distribution', 'Gamble soft maximum distribution', 'Gambel-softmax distribution'])
Hellinger distance Chinese
('Hellinger 距离', '1. Rank: Hellinger 距离, Hellinger距离, 海林距离, 赫林格距离, 海林杰距离, 赫尔丁距离, 海林格距离\n\n2. Explanation: The top choice, "Hellinger 距离," is the best fit because it retains the original name "Hellinger," which is crucial in the AI domain as it refers to a specific mathematical concept named after the mathematician Ernst Hellinger. This term is semantically accurate and maintains the integrity of the original term, ensuring that it is recognized in academic and professional contexts. The use of "距离" (distance) is appropriate as it directly translates the concept of distance in the context of probability distributions. \n\nThe second option, "Hellinger距离," is also a good fit but lacks the space that is often used in formal writing to separate the name from the term, making it slightly less formal. \n\nThe other candidates, such as "海林距离" and "赫林格距离," introduce transliterations that do not accurately reflect the original term\'s recognition in the AI field, which could lead to confusion or misinterpretation. They also deviate from the established terminology used in literature, which is critical for maintaining clarity in technical discussions.', ['Hellinger distance', 'Hellinger distance', 'Hailin distance', 'Heringer distance', 'Hellinger distance', 'Herding distance', 'Hellinger distance'])
Hessian matrix Chinese
('海森矩阵', '1. Rank: 海森矩阵, Hessian矩阵, Hessian 矩阵, 黑塞矩阵\n\n2. Explanation: The term "海森矩阵" is the most widely accepted and semantically accurate translation of "Hessian matrix" in the context of AI and mathematics. It retains the original meaning and is commonly used in academic literature, making it the best fit for contextual usage. The back translation accurately reflects the English term, ensuring clarity and consistency in communication. \n\nThe other candidates, such as "Hessian矩阵" and "Hessian 矩阵," are less preferable because they mix English and Chinese, which can lead to confusion in a purely Chinese context. "黑塞矩阵" is a less common alternative that may not be recognized by all readers in the AI field, making it a weaker choice. Therefore, "海森矩阵" stands out as the most appropriate term for conveying the concept of the Hessian matrix in the AI domain.', ['Hessian matrix', 'Hessian matrix', 'Hessian matrix', 'Hessian matrix'])
Huber loss Chinese
('胡贝尔损失', '1. Rank: 胡贝尔损失, Huber 损失, 胡伯损失, 霍伯损失\n\n2. Explanation: The first translation, "胡贝尔损失," is the best fit because it accurately retains the original name "Huber" while providing a clear and recognizable term for "loss" in the context of machine learning. The back translation "huber loss" aligns perfectly with the English term, ensuring semantic accuracy. This term is also widely accepted in the AI community, making it contextually appropriate. The second option, "Huber 损失," while also correct, does not fully integrate the Chinese language structure as effectively as "胡贝尔损失." The third option, "胡伯损失," is a less common transliteration and may not be as recognizable in the AI domain. Lastly, "霍伯损失" introduces a different name ("Hooper") that does not correspond to the original term, making it the least suitable choice.', ['huber loss', 'huber loss', 'Huber loss', 'Hooper loss'])
Huber norm Chinese
('胡贝尔范数', '1. Rank: 胡贝尔范数, 胡伯范数, Huber 范数, 霍伯范数\n\n2. Explanation: The term "胡贝尔范数" is the best fit because it accurately retains the original name "Huber" while providing a clear and recognizable translation for the term "norm" in the context of mathematics and statistics. The use of "胡贝尔" is a well-established transliteration of "Huber" in academic literature, making it semantically accurate and contextually appropriate for the AI domain. \n\nThe second candidate, "胡伯范数," is also a reasonable choice, but it is less commonly used than "胡贝尔" and may not be as immediately recognizable to those familiar with the term. \n\nThe third candidate, "Huber 范数," while retaining the original English term, does not provide a fully localized version, which may not be as effective in a Chinese-speaking academic context. \n\nLastly, "霍伯范数" is a poor choice as it misrepresents the name "Huber" and could lead to confusion, as "Hopper" is an entirely different term. Thus, "胡贝尔范数" stands out as the most accurate and contextually fitting translation.', ['Huber norm', 'Huber norm', 'Huber norm', 'Hopper norm'])
Hyperband Chinese
('"Hyperband算法"', '1. Rank: "Hyperband算法", "Hyperband", "超带"\n\n2. Explanation: The term "Hyperband算法" is the best fit because it retains the original English term "Hyperband" while adding the word "算法" (algorithm), which is crucial in the context of AI and machine learning. This makes it clear that it refers to a specific algorithmic approach, which is essential for understanding its application in hyperparameter tuning. The back translation "Hyperband algorithm" accurately reflects the original term and its intended meaning in the AI domain. \n\nThe second candidate, "Hyperband", is also a valid option but lacks the explicit mention of "algorithm," which is important for clarity in a technical context. The third candidate, "超带" (super band), is not appropriate as it does not convey the specific meaning of the term in the AI context and could lead to confusion. Therefore, "Hyperband算法" is the most semantically accurate and contextually appropriate choice.', ['hyper band', 'super band', 'Hyperband algorithm'])
Kendall's τ Chinese
('"肯德尔τ系数"', '1. Rank: "肯德尔τ系数", "Kendall\'s τ 相关系数", "肯德尔秩相关系数", "Kendall的τ系数", "Kendall的τ", "肯德尔τ", "Kendall τ"\n\n2. Explanation: The term "肯德尔τ系数" (Kendall\'s τ coefficient) is the best fit because it accurately conveys both the name of the statistician (Kendall) and the specific nature of the metric (coefficient), which is essential in the context of statistical analysis. The inclusion of "系数" (coefficient) clarifies that this term refers to a quantitative measure, which is crucial in the AI domain where precision in terminology is important. \n\nThe back translation "Kendall τ coefficient" maintains the integrity of the original term, ensuring that it is understood as a specific statistical measure. Other candidates like "肯德尔秩相关系数" (Kendall rank correlation coefficient) are also accurate but may be less commonly used in the context of AI literature, where the simpler "系数" is often preferred. \n\nOverall, "肯德尔τ系数" strikes a balance between semantic accuracy and contextual fit, making it the most appropriate choice for use in AI-related discussions.', ['Kendall τ', 'Kendall τ coefficient', "Kendall's τ correlation coefficient", 'Kendall τ', 'Kendall’s τ coefficient', 'Kendall rank correlation coefficient', 'Kendall τ'])
Kleene closure Chinese
('克林闭包', '1. Rank: 克林闭包, Kleene闭包, 克里尼闭包, 克莱尼闭包, 克里宁闭包, 克内泽-内伊平滑\n\n2. Explanation: The term "克林闭包" (Kleene closure) is the best fit because it accurately retains the original name "Kleene," which is essential in the context of formal language theory and automata. This term is widely recognized in the AI and computer science communities, making it semantically accurate and contextually appropriate. The back translation also aligns perfectly with the English term, ensuring clarity and consistency in communication. Other candidates, such as "克里尼闭包" and "克莱尼闭包," deviate from the standard transliteration of "Kleene," which could lead to confusion or misinterpretation in academic and professional settings. The remaining terms either misrepresent the original term or are unrelated to the concept of closure in formal languages.', ['Kleene closure', 'Crini closure', 'Kleene closure', 'Kleinian closure', 'Kneize-Ney smoothing', 'Krynin closure'])
Kneser-Ney smoothing Chinese
('Kneser-Ney平滑', '1. Rank: Kneser-Ney平滑, Kneser-Ney 平滑, 克内泽-内伊平滑, 克内泽-奈平滑, 克内泽-内平滑\n\n2. Explanation: The term "Kneser-Ney平滑" is the best fit because it retains the original name "Kneser-Ney" in its entirety, which is crucial in the AI domain where specific algorithms and techniques are often referred to by their original names. This term also uses the Chinese word "平滑" (smoothing), which accurately conveys the concept of smoothing in the context of statistical language modeling. The second option, "Kneser-Ney 平滑," is also a strong candidate but includes a space that is less common in technical terminology. The other candidates either use incorrect transliterations or do not maintain the integrity of the original name, which is essential for clarity and recognition in the AI field.', ['Kneize-Ney smoothing', 'Kneser-Ney smoothing', 'Kneize-Nesmoothing', 'Kneser-Ney Smooth', 'Kneize-internal smoothing'])
Kolmogorov-Smirnov test Chinese
('科尔莫戈罗夫-斯米尔诺夫检验', '1. Rank: 科尔莫戈罗夫-斯米尔诺夫检验, 科尔莫哥洛夫-斯米尔诺夫检验, Kolmogorov-Smirnov检验, 科尔莫戈洛夫-斯米尔诺夫检验, 科尔莫哥罗夫-斯米尔诺夫检验, 科尔莫哥洛夫-史米尔诺夫检验\n\n2. Explanation: The term "科尔莫戈罗夫-斯米尔诺夫检验" is the best fit because it accurately retains the original names of the mathematicians Kolmogorov and Smirnov while following the standard conventions for transliterating foreign names into Chinese. This version is widely recognized in the statistical community and is semantically accurate, ensuring that readers familiar with the AI and statistical domains will understand it correctly. The other candidates either use less common transliterations or variations that could lead to confusion, making them less suitable for academic and professional contexts.', ['Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test', 'Kolmogorov-Smirnov test'])
Krippendorff's α Chinese
('"克里彭多夫α系数"', '1. Rank: "克里彭多夫α系数", "克里彭多夫α", "克里彭多夫 α 系数", "克里彭多夫 α", "Krippendorff的α", "克里朋多夫 α"\n\n2. Explanation: The first translation, "克里彭多夫α系数", is the best fit because it accurately retains the full name of the original term along with the specific statistical context by including "系数" (coefficient), which is essential in the AI domain for clarity and precision. This term clearly indicates that it refers to a statistical measure, which is crucial for understanding its application in evaluating agreement scores. The back translation "Krippendorff alpha coefficient" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. The other candidates, while they may retain the name "Krippendorff," either omit the term "系数" or do not provide a clear indication of the statistical nature of the term, which could lead to ambiguity in an academic or professional setting.', ['Krippendorff alpha coefficient', 'Krippendorf α', 'Krippendorff alpha coefficient', 'Krippendorf α', 'Krippendorff α', 'Krippendorf α'])
Kronecker delta Chinese
('克罗内克 δ', '1. Rank: 克罗内克 δ, 克罗内克δ, 克罗内克尔δ, 克罗内克δ函数, 克罗内克函数, 克里彭多夫α系数\n\n2. Explanation: The first translation candidate, "克罗内克 δ", is the best fit because it accurately retains the original name "Kronecker" while also including the delta symbol (δ), which is crucial for understanding its mathematical context. This term is semantically accurate and aligns well with the established terminology in the AI and mathematical domains. The use of "克罗内克" is a widely accepted transliteration of "Kronecker" in Chinese literature, ensuring that readers familiar with the term will recognize it immediately. \n\nThe second candidate, "克罗内克δ", is also a good option but lacks a space between "克罗内克" and "δ", which may affect readability. The third candidate, "克罗内克尔δ", introduces an unnecessary suffix that is not commonly used in this context. The fourth candidate, "克罗内克δ函数", while accurate, adds "函数" (function) which is not needed since "Kronecker delta" is a specific term that does not require this additional descriptor. The fifth candidate, "克罗内克函数", similarly misrepresents the term by omitting the delta aspect. Lastly, "克里彭多夫α系数" is irrelevant as it refers to a different concept entirely (Krippendorff\'s alpha coefficient) and should not be included in this ranking.', ['Kronecker delta', 'Kronecker delta', 'Kronecker delta', 'Kronecker delta function', 'Krippendorff alpha coefficient', 'Kronecker function'])
L2 regularizer Chinese
('L2 正则化器', '1. Rank: L2 正则化器, L2正则化器, 2正则化器\n\n2. Explanation: The first translation, "L2 正则化器," is the best fit because it maintains the correct semantic structure and accurately reflects the term "L2 regularizer" in the context of machine learning and statistics. The inclusion of a space between "L2" and "正则化器" enhances readability and aligns with common practices in technical writing, making it clearer for readers. The second candidate, "L2正则化器," while also accurate, lacks the space, which can make it slightly less readable. The third candidate, "2正则化器," is not a suitable translation as it omits the "L" and does not convey the intended meaning of the term, leading to confusion. Therefore, the first option is the most contextually appropriate and semantically accurate for the AI domain.', ['L2 regularizer', 'L2 regularizer', '2 Regularizer'])
L2-normalization Chinese
('L2归一化', '1. Rank: L2归一化, L2 归一化, L2规范化, L2 正则化\n\n2. Explanation: The term "L2归一化" is the best fit because it accurately captures the concept of "normalization" in the context of AI and machine learning. The term "归一化" specifically refers to the process of normalizing data, which aligns well with the intended meaning of L2-normalization in the context of the provided text. The back translation "L2 normalization" confirms its semantic accuracy. \n\nThe alternative "L2 归一化" is also a strong candidate, but the space between "L2" and "归一化" is less common in technical terminology, making "L2归一化" slightly more preferable. \n\n"L2规范化" translates to "L2 normalization" as well, but "规范化" can imply a broader sense of standardization, which may not be as precise as "归一化" in this specific context. \n\nLastly, "L2 正则化" translates to "L2 regularization," which is a different concept in machine learning, primarily related to preventing overfitting rather than normalization. Therefore, it is not suitable for this context.', ['L2 normalization', 'L2 normalization', 'L2 regularization', 'L2 normalization'])
Lambertian reflectance Chinese
('兰伯特反射率', '1. Rank: 兰伯特反射率, 兰伯特反射, 朗伯反射\n\n2. Explanation: The term "兰伯特反射率" (Lambert reflectance) is the best fit because it accurately retains the original name "Lambert" while also including the term "反射率" (reflectance), which is the correct technical term used in the context of bidirectional reflectance distribution functions (BRDF) in the AI domain. This term emphasizes the measurement aspect of reflectance, which is crucial in understanding material behavior in computer graphics and vision. \n\nThe second option, "兰伯特反射" (Lambert reflection), while also retaining the name "Lambert," lacks the specificity of "reflectance," which is essential in this context. The third option, "朗伯反射" (Lambertian reflection), uses a transliteration of the name that is less common and does not align with the established terminology in the field. Therefore, "兰伯特反射率" is the most semantically accurate and contextually appropriate choice.', ['Lambert reflectance', 'Lambertian reflection', 'Lambert reflection'])
Langevin dynamic Chinese
('兰格文动力学', '1. Rank: 兰格文动力学, 朗之万动力学, Langevin 动力学, 兰金动力学, 兰贺文动力学, 兰贺维动理\n\n2. Explanation: The term "兰格文动力学" is the best fit because it accurately transliterates the name "Langevin" while maintaining the correct scientific context of "dynamics." This term is widely recognized in the field of physics and AI, particularly in discussions of stochastic processes and generative models. The back translation "Langevin Dynamics" confirms its semantic accuracy. \n\n"朗之万动力学" is a close second, as it also attempts to transliterate "Langevin," but it is less commonly used in academic literature compared to "兰格文." The term "Langevin 动力学" is acceptable but less formal since it mixes English and Chinese, which is not ideal in a scholarly context. The other candidates, such as "兰金动力学" and "兰贺文动力学," introduce inaccuracies by misrepresenting the original name or creating confusion with unrelated terms. "兰贺维动理" is the least suitable as it does not accurately reflect the original term and lacks contextual relevance.', ['Langevin Dynamics', 'Rankine Kinetics', 'Langevin Dynamics', 'Langhewen Dynamics', 'Langevin Dynamics', "Langhewei's theory"])
Marching Cubes Chinese
('行进立方体法', '1. Rank: 行进立方体法, 行进立方体算法, 立方体行进算法, 移动立方体算法, 游走立方体算法, 行进立方体\n\n2. Explanation: The term "行进立方体法" (marching cube method) is the best fit for the translation of "Marching Cubes" in the AI domain. This is because it accurately captures the essence of the original term while maintaining semantic clarity. The word "法" (method) is commonly used in technical contexts to denote a specific algorithm or technique, which aligns well with the usage of "Marching Cubes" as a well-established algorithm in computer graphics and AI for surface extraction.\n\nThe second choice, "行进立方体算法" (marching cube algorithm), is also a strong candidate, as "算法" (algorithm) is another appropriate term in this context. However, "法" (method) is slightly more aligned with the terminology used in academic literature, where "method" often refers to a specific procedural approach.\n\nThe other candidates, such as "移动立方体算法" (moving cube algorithm) and "游走立方体算法" (walking cube algorithm), introduce unnecessary variations that deviate from the established terminology. They could lead to confusion, as they do not accurately reflect the original term\'s meaning or its established usage in the field. Therefore, "行进立方体法" is the most contextually appropriate and semantically accurate translation.', ['marching cube', 'moving cube algorithm', 'cube marching algorithm', 'marching cube method', 'Marching Cube Algorithm', 'walking cube algorithm'])
Matérn kernel Chinese
('Matérn 核函数', '1. Rank: Matérn 核函数, Matérn 核, 马特恩核, 马顿核, 核函数\n\n2. Explanation: The term "Matérn 核函数" is the best fit because it retains the original name "Matérn" while clearly indicating that it is a "kernel function," which is essential in the context of AI and machine learning. This term accurately reflects the specific mathematical function used in spatial statistics and Gaussian processes, making it semantically precise for the AI domain. The inclusion of "函数" (function) is crucial as it specifies the mathematical nature of the term, which is important for clarity in technical discussions. \n\n"Matérn 核" is a close second, as it also retains the original name but omits "函数," which could lead to ambiguity in a technical context. The transliterations "马特恩核" and "马顿核" do not maintain the original name\'s integrity and may confuse readers who are familiar with the term in its original form. Lastly, "核函数" is too generic and does not specify the Matérn kernel, making it the least suitable option in this context.', ['Matérn core', 'Marton core', 'Mattern core', 'kernel function', 'Matérn kernel function'])
Metropolis Hastings Chinese
('梅特罗波利斯-黑斯廷斯算法', '1. Rank: 梅特罗波利斯-黑斯廷斯算法, 美特罗波利斯-海斯廷斯算法, Metropolis-Hastings 算法, 大都市-黑斯廷斯算法, 迈特罗波利斯-海斯廷斯算法, 大都会-海斯廷斯算法, 大都会-哈斯廷斯算法, 方法\n\n2. Explanation: The term "梅特罗波利斯-黑斯廷斯算法" is the best fit because it accurately retains the original names "Metropolis" and "Hastings" while providing a clear indication that it is an algorithm (算法). This translation is semantically accurate and contextually appropriate for the AI domain, as it directly references the well-known Metropolis-Hastings algorithm used in statistical sampling. The use of "黑斯廷斯" for "Hastings" is also widely recognized in the field, ensuring that the term is understood by professionals familiar with the algorithm. Other candidates, such as "美特罗波利斯-海斯廷斯算法" and "大都市-黑斯廷斯算法," either use less common transliterations or introduce variations that may not be as recognizable in the AI community. Therefore, the first option is the most effective for conveying the intended meaning in a professional context.', ['Metropolis-Hastings algorithm', 'Metropolis-Hastings algorithm', 'method', 'Metropolis-Hastings Algorithm', 'Metropolis-Hastings algorithm', 'Metropolis-Hastings', 'Metropolis-Hastings algorithm', 'Metropolitan-Hastings Algorithm', 'Metropolitan-Hastings algorithm'])
Metropolis algorithm Chinese
('梅特罗波利斯算法', '1. Rank: 梅特罗波利斯算法, 美特罗波利斯算法, 大都会算法, 元胞算法, 算法\n\n2. Explanation: The first translation candidate, "梅特罗波利斯算法," is the best fit because it accurately transliterates the term "Metropolis" while maintaining the context of the algorithm in the AI domain. The back translation "Metropolis algorithm" aligns perfectly with the original English term, ensuring semantic accuracy. \n\nThe second candidate, "美特罗波利斯算法," is also a transliteration but uses a different character for "Metropolis," which may not be as widely recognized in the context of this algorithm. \n\nThe third candidate, "大都会算法," translates to "metropolitan algorithm," which could lead to confusion as it does not retain the specific reference to the Metropolis method used in statistical sampling. \n\nThe fourth candidate, "元胞算法," translates to "cellular algorithm," which is unrelated to the Metropolis algorithm and thus semantically inaccurate. \n\nThe last candidate, "算法," simply means "algorithm" and lacks the specificity needed for accurate communication in the AI context. \n\nOverall, "梅特罗波利斯算法" is the most contextually appropriate and semantically accurate choice for the term in the AI domain.', ['Metropolis algorithm', 'Metropolis Algorithm', 'Cellular Algorithm', 'algorithm', 'metropolitan algorithm'])
Metropolis method Chinese
('梅特罗波利斯方法', '1. Rank: 梅特罗波利斯方法, Metropolis方法, 美特罗波利斯方法, 大都会方法, 元胞方法\n\n2. Explanation: The term "梅特罗波利斯方法" is the best fit because it accurately retains the original name "Metropolis" while providing a phonetic representation that is widely recognized in the AI and statistical communities. This term is semantically accurate and aligns with the established terminology used in the context of Markov Chain Monte Carlo (MCMC) methods, which is relevant to the provided context. \n\n"Metropolis方法" is also a strong candidate as it retains the original name in a more direct transliteration format, but it lacks the phonetic clarity that "梅特罗波利斯" provides. "美特罗波利斯方法" is a less common variant that may not be as widely recognized. "大都会方法" translates to "metropolitan approach," which diverges from the original meaning and could confuse readers. Lastly, "元胞方法" translates to "cellular method," which is unrelated to the Metropolis method and thus is the least appropriate choice in this context.', ['metropolis method', 'metropolis method', 'cellular method', 'Metropolis method', 'metropolitan approach'])
Metropolis-Hasting Chinese
('梅特罗波利斯-哈斯廷斯', '1. Rank: 梅特罗波利斯-哈斯廷斯, 美特罗波利斯-哈斯廷斯, 梅特罗波利斯-黑斯廷斯, 大都会-黑斯廷斯, 元胞-哈斯廷斯, Metropolis-Hasting算法\n\n2. Explanation: The term "梅特罗波利斯-哈斯廷斯" is the best fit because it accurately transliterates the original English term "Metropolis-Hastings" while maintaining the semantic integrity of the term in the context of AI and statistics. This term preserves the original names and is widely recognized in the field, ensuring clarity and precision in communication among professionals. The second candidate, "美特罗波利斯-哈斯廷斯," is also a valid transliteration but uses a different character for "美," which is less common in this context. The third candidate, "梅特罗波利斯-黑斯廷斯," introduces a different character for "Hastings," which could lead to confusion. The fourth candidate, "大都会-黑斯廷斯," deviates from the original name and introduces a different meaning ("Metropolitan"), which is not appropriate in this context. The fifth candidate, "元胞-哈斯廷斯," incorrectly translates "Metropolis" to "Cell," which is not relevant to the term\'s meaning. Lastly, "Metropolis-Hasting算法" is a less preferred option as it adds "算法" (algorithm) unnecessarily, which could lead to ambiguity. Thus, "梅特罗波利斯-哈斯廷斯" is the most contextually accurate and semantically appropriate choice.', ['Metropolis-Hastings', 'Metropolis-Hastings', 'Metropolis-Hastings', 'Cell-Hastings', 'Metropolis-Hasting algorithm', 'Metropolitan-Hastings'])
Metropolis-Hastings acceptance ratio Chinese
('梅特罗波利斯-哈斯廷斯接受比率', '1. Rank: 梅特罗波利斯-哈斯廷斯接受比率, 美特罗波利斯-哈斯廷斯接受比率, Metropolis-Hastings接受比率, 梅特罗波利斯-黑斯廷斯接受率, 大都会-黑斯廷斯接受率, 元胞-哈斯廷斯接受比率\n\n2. Explanation: The term "梅特罗波利斯-哈斯廷斯接受比率" is the best fit because it accurately retains the original names "Metropolis" and "Hastings" in a way that is phonetically and semantically appropriate for Chinese speakers. This translation uses the standard transliteration for "Metropolis" (梅特罗波利斯) and "Hastings" (哈斯廷斯), which are widely recognized in academic and technical contexts, particularly in AI and statistics. \n\nThe second candidate, "美特罗波利斯-哈斯廷斯接受比率," while also a valid transliteration, uses "美" which is less common and could imply a different meaning (beauty) in Chinese, making it less suitable. \n\nThe third candidate, "Metropolis-Hastings接受比率," retains the English term but lacks the full Chinese context, which may not be as accessible to all readers. \n\nThe remaining candidates, "梅特罗波利斯-黑斯廷斯接受率," "大都会-黑斯廷斯接受率," and "元胞-哈斯廷斯接受比率," either misrepresent the names or introduce unrelated terms (like "元胞" meaning "cell"), which could confuse the intended meaning in the AI domain. \n\nOverall, the first choice is the most semantically accurate and contextually appropriate for the AI terminology in question.', ['Metropolis-Hastings Acceptance Ratio', 'Metropolis-Hastings Acceptance Ratio', 'Metropolis-Hastings Acceptance Rate', 'Cell-Hastings Acceptance Ratio', 'Metropolis-Hastings Acceptance Ratio', 'Mets-Hastings Acceptance Rate'])
Metropolis-Hastings algorithm Chinese
('梅特罗波利斯-哈斯廷斯算法', '1. Rank: 梅特罗波利斯-哈斯廷斯算法, 美特罗波利斯-哈斯廷斯算法, Metropolis-Hastings算法, 大都会-黑斯廷斯算法, 梅特罗波利斯-黑斯廷斯算法, 元胞-哈斯廷斯算法\n\n2. Explanation: The term "梅特罗波利斯-哈斯廷斯算法" is the best fit because it accurately retains the original names "Metropolis" and "Hastings" in a way that is phonetically and semantically appropriate for Chinese speakers familiar with the AI and statistical domains. This translation uses a common transliteration for "Metropolis" that is widely recognized in academic literature, ensuring clarity and consistency. The use of "算法" (algorithm) is standard in technical contexts, making it immediately understandable to the target audience. \n\nThe second candidate, "美特罗波利斯-哈斯廷斯算法," while also a valid transliteration, uses "美" which is less common and may not be as recognizable in the context of the algorithm. The third candidate, "Metropolis-Hastings算法," is a direct mix of English and Chinese, which is acceptable but less formal than a fully translated term. The remaining candidates either introduce inaccuracies (like "元胞-哈斯廷斯算法," which translates to "Cellular-Hastings Algorithm" and is incorrect) or use less appropriate transliterations. Thus, "梅特罗波利斯-哈斯廷斯算法" stands out as the most contextually accurate and semantically precise choice.', ['Metropolis-Hastings algorithm', 'Metropolis-Hastings algorithm', 'Metropolis-Hastings algorithm', 'Cellular-Hastings Algorithm', 'Metropolis-Hastings algorithm', 'Metropolitan-Hastings Algorithm'])
Metropolis-Hastings sampler Chinese
('梅特罗波利斯-黑斯廷斯采样器', '1. Rank: 梅特罗波利斯-黑斯廷斯采样器, Metropolis-Hastings 采样器, Metropolis-Hastings采样器, Metropolis-Hastings 採樣器, metropolis-hastings 采样器, 采样器, 大都会-哈斯廷斯采样器, Metropolis-Hastings采样器采样器\n\n2. Explanation: The term "梅特罗波利斯-黑斯廷斯采样器" is the best fit because it retains the original names "Metropolis" and "Hastings" in a phonetic transliteration that is widely recognized in the AI and statistics communities. This term accurately reflects the original English term while maintaining semantic clarity. The use of "采样器" (sampler) is appropriate and aligns with the technical context of sampling in probabilistic models. Other candidates, such as "Metropolis-Hastings 采样器" and "Metropolis-Hastings采样器," are also good but do not provide the same level of phonetic fidelity to the original names. The term "大都会-哈斯廷斯采样器" introduces a less common translation for "Metropolis," which could lead to confusion, and "Metropolis-Hastings采样器采样器" is redundant. Therefore, the first option is the most semantically accurate and contextually appropriate for the AI domain.', ['Metropolis-Hastings Sampler', 'Metropolis-Hastings Sampler', 'Metropolis-Hastings sampler', 'Metropolis-Hastings Sampler', 'sampler', 'Metropolis-Hastings Sampler Sampler', 'metropolis-hastings sampler', 'Metropolitan-Hastings Sampler'])
Mixup Chinese
('"混合数据增强"', '1. Rank: "混合数据增强", "混合增强", "混合", "混合法", "Mixup", "mixup", ""\n\n2. Explanation: The term "混合数据增强" (Hybrid data augmentation) is the best fit because it accurately captures the essence of the "Mixup" technique in the context of machine learning and data augmentation. In the AI domain, "Mixup" refers specifically to a data augmentation strategy that combines two or more images (or data points) to create new training examples, which helps improve model generalization. The term "混合数据增强" explicitly includes "数据增强" (data augmentation), making it clear that it pertains to augmenting data, which is crucial in the context provided. \n\nThe other candidates, such as "混合增强" (Hybrid enhancement) and "混合" (mix), are less specific and do not convey the full meaning of the technique as it relates to data augmentation. "混合法" (mixed method) is also vague and does not directly relate to the specific augmentation strategy. The terms "Mixup" and "mixup" are direct transliterations and do not provide any contextual understanding for Chinese-speaking audiences unfamiliar with the term. Therefore, "混合数据增强" stands out as the most semantically accurate and contextually appropriate translation in the AI domain.', ['mixup', 'mixup', 'mix', 'Hybrid data augmentation', 'Hybrid enhancement', 'mixed method'])
Monte Carlo Dropout Chinese
('蒙特卡洛丢弃法', '1. Rank: 蒙特卡洛丢弃法, 蒙特卡罗dropout, 蒙特卡罗随机失活, 蒙特卡罗失活, 蒙特卡罗丢弃法\n\n2. Explanation: The term "蒙特卡洛丢弃法" (Monte Carlo dropout) is the best fit for several reasons. Firstly, it accurately translates the English term "Monte Carlo Dropout" while maintaining the specific terminology used in the AI domain. The term "丢弃法" directly corresponds to "dropout," which is a well-established term in machine learning referring to a regularization technique. This makes it semantically precise and contextually relevant.\n\nThe second candidate, "蒙特卡罗dropout," retains the English term "dropout" in its original form, which is also acceptable in the AI community, but it lacks the full Chinese translation for "method" (法), making it slightly less formal.\n\nThe third candidate, "蒙特卡罗随机失活" (Monte Carlo Random Deactivation), introduces the word "随机" (random), which is not necessary in this context and could lead to confusion, as "dropout" specifically refers to the technique rather than a general random deactivation.\n\nThe fourth candidate, "蒙特卡罗失活" (Monte Carlo Deactivation), omits the "丢弃" (dropout) aspect, which is crucial for understanding the specific technique being referred to.\n\nOverall, "蒙特卡洛丢弃法" is the most accurate and contextually appropriate translation for "Monte Carlo Dropout" in the AI domain.', ['Monte Carlo Random Deactivation', 'Monte Carlo dropout', 'Monte Carlo dropout', 'Monte Carlo Deactivation', 'Monte Carlo dropout'])
Moore-Penrose pseudo-inverse Chinese
('摩尔-彭若斯伪逆', '1. Rank: 摩尔-彭若斯伪逆, 摩尔-彭罗斯伪逆, 穆尔-彭若斯伪逆, 莫尔-彭罗斯伪逆, 广义逆\n\n2. Explanation: The term "摩尔-彭若斯伪逆" is the best fit because it accurately retains the names "Moore" and "Penrose" in a transliterated form that is widely recognized in the mathematical and AI communities. The use of "彭若斯" for "Penrose" is particularly important as it is the standard transliteration used in academic literature, ensuring semantic accuracy and contextual fit. \n\nThe second candidate, "摩尔-彭罗斯伪逆," is also a strong contender, but "彭罗斯" is less commonly used than "彭若斯" in this context. The third candidate, "穆尔-彭若斯伪逆," introduces a different transliteration for "Moore," which may lead to confusion. The fourth candidate, "莫尔-彭罗斯伪逆," similarly alters the transliteration of "Moore" and is less preferred. Finally, "广义逆" translates to "generalized inverse," which is a broader term and does not specifically refer to the Moore-Penrose pseudo-inverse, making it the least suitable choice in this context. \n\nOverall, the first option is the most semantically accurate and contextually appropriate for the AI domain, where precise terminology is crucial.', ['Moore-Penrose pseudoinverse', 'Moore-Penrose pseudoinverse', 'Moore-Penrose pseudoinverse', 'Generalized inverse', 'Mohr-Penrose pseudoinverse'])
Morfessor Chinese
('Morfessor形态分析器', '1. Rank: Morfessor形态分析器, 形态分析器, 莫费索, 莫費索, Morfessor\n\n2. Explanation: The term "Morfessor形态分析器" is the best fit because it retains the original name "Morfessor" while also providing a clear and accurate description of its function as a "morphology analyzer" in Chinese. This is crucial in the AI domain, where specific terminology often needs to convey both the name and the purpose of the system. The back translation "Morfessor Morphology Analyzer" accurately reflects this dual purpose, making it contextually appropriate for discussions about AI systems in linguistic analysis. The other candidates, while they may be accurate in terms of transliteration or partial meaning, do not provide the same level of clarity and specificity needed in a technical context.', ['MO RFE fashion or', 'Morfessor Morphology Analyzer', 'mofeso', 'mofeso', 'Morphology Analyzer'])
N-gram Chinese
('N-gram', '1. Rank: N-gram, N元语法, n元组, \n\n2. Explanation: The term "N-gram" is the most accurate translation because it retains the original English term, which is widely recognized in the AI and natural language processing communities. This term is commonly used in academic literature and technical discussions, making it the best fit for contextual usage. \n\nThe second candidate, "N元语法" (N-yuán yǔfǎ), translates to "N-gram grammar," which adds an unnecessary grammatical connotation that may not align with the intended meaning in the context of semantic units and representation. \n\nThe third candidate, "n元组" (n-yuán zǔ), translates to "n-tuple," which is a mathematical term that does not accurately convey the specific concept of N-grams in language processing. \n\nOverall, "N-gram" is the most semantically accurate and contextually appropriate choice for the AI domain, as it directly corresponds to the established terminology used in the field.', ['N-gram', 'N-gram', 'N-gram', 'n-tuple'])
Nadaraya-Watson estimator Chinese
('纳达拉亚-沃森估计器', '1. Rank: 纳达拉亚-沃森估计器, 纳达拉亚-沃森估计量, Nadaraya-Watson 估计量, Nadaraya-Watson估计量, Nadaraya-Watson估计器, 纳达拉亚-瓦森估计器\n\n2. Explanation: The term "纳达拉亚-沃森估计器" is the best fit because it accurately retains the original names "Nadaraya" and "Watson" while using the term "估计器," which is commonly used in the AI and statistical context to refer to an estimator or a method that produces estimates. The use of "估计器" aligns well with the technical nature of the term, as it suggests a specific tool or mechanism for estimation, which is essential in the context of the Nadaraya-Watson estimator\'s function. \n\nIn contrast, "纳达拉亚-沃森估计量" and "Nadaraya-Watson 估计量" use "估计量," which can imply a broader concept of an estimate rather than a specific estimator, making them slightly less precise in this context. The other candidates, such as "纳达拉亚-瓦森估计器," introduce inaccuracies in the transliteration of the names, which is critical in maintaining the integrity of the term in the AI domain. Therefore, the first option is the most semantically accurate and contextually appropriate.', ['Nadalaya-Watson estimator', 'Nadalaya-Watson estimator', 'Nadaraya-Watson estimator', 'Nadalaya-Wassen estimator', 'Nadaraya-Watson estimator', 'Nadaraya-Watson estimator'])
Nesterov momentum Chinese
('Nesterov动量', '1. Rank: Nesterov动量, Nesterov 动量, Nesterov 動量, ネステロフの勢い\n\n2. Explanation: The first translation, "Nesterov动量," is the best fit because it retains the original name "Nesterov" in its proper form while using the simplified Chinese term "动量" (momentum), which is widely recognized in the AI and machine learning community. This term is semantically accurate and contextually appropriate, as it directly relates to the concept of momentum in optimization algorithms. The second option, "Nesterov 动量," is also a good choice but includes a space that is not typically used in technical terms in Chinese. The third option, "Nesterov 動量," uses traditional Chinese characters, which may not be as accessible to the broader audience in the AI field, particularly in mainland China where simplified characters are standard. The last option, "ネステロフの勢い," is a transliteration in Japanese, which is not relevant in the context of Chinese terminology and does not fit the requirement for a Chinese translation. Thus, "Nesterov动量" is the most appropriate choice for its clarity, accuracy, and adherence to the conventions of the AI domain.', ['Nesterov momentum', 'Nesterov Momentum', 'Nesterov Momentum', 'ネステロフの时い'])
Parzen window Chinese
('"Parzen窗口"', '1. Rank: "Parzen窗口", "帕尔岑窗口", "帕尔曾窗", "帕尔岑窗", "**帕岑窗口", "巴岱斯窗"\n\n2. Explanation: The term "Parzen窗口" is the best fit because it retains the original name "Parzen" in its English form, which is crucial in the AI domain where specific algorithms and methods are often named after their creators. This helps maintain clarity and recognition within the field. The use of "窗口" (window) is appropriate as it directly translates the concept of a "window" in the context of statistical methods. \n\nThe second choice, "帕尔岑窗口," is a close transliteration of "Parzen" and is also semantically accurate, but it does not preserve the original English name as effectively as "Parzen窗口." The other candidates either deviate from the original name or introduce inaccuracies, such as "巴岱斯窗" (Badais window), which is unrelated to the term in question. Therefore, "Parzen窗口" is the most contextually and semantically accurate choice for the AI terminology.', ['parzen window', 'Parzen window', 'parzen window', '**Patzen Window', 'Badais window', 'parzen window'])
Pearson correlation Chinese
('皮尔逊相关', '1. Rank: 皮尔逊相关, 皮尔逊相关性, 皮尔逊相关系数\n\n2. Explanation: The term "皮尔逊相关" (Pearson correlation) is the best fit because it directly translates the concept of correlation associated with Pearson without adding unnecessary complexity. In the context of statistical analysis, "相关" (correlation) is a widely accepted term that accurately conveys the relationship being measured. \n\n"皮尔逊相关性" (Pearson correlation coefficient) introduces the term "相关性," which can imply a broader concept of correlation that may not be as precise in statistical contexts. While it is still accurate, it is less commonly used in the specific context of Pearson\'s statistical measure.\n\n"皮尔逊相关系数" (Pearson correlation coefficient) is also a valid term, but it explicitly refers to the coefficient, which is a specific numerical value derived from the correlation. In the context provided, where the focus is on the concept of correlation rather than the coefficient itself, this term may be overly specific and less fitting.\n\nOverall, "皮尔逊相关" is concise, contextually appropriate, and aligns well with standard terminology used in the AI and statistical domains.', ['Pearson correlation', 'Pearson correlation', 'Pearson correlation coefficient'])
Pearson's correlation Chinese
('皮尔逊的相关性', '1. Rank: 皮尔逊的相关性, 皮尔逊相关性, 皮尔逊相关\n\n2. Explanation: The term "皮尔逊的相关性" (Pearson\'s correlation) is the best fit because it retains the possessive form "的" which indicates that the correlation is specifically attributed to Pearson, thus maintaining the original context and meaning of the term. This is particularly important in the AI domain where precision in terminology is crucial for clarity and understanding. The second option, "皮尔逊相关性" (Pearson correlation), is also accurate but slightly less formal as it omits the possessive, which can lead to ambiguity in some contexts. The third option, "皮尔逊相关" (Pearson correlation), is the least preferred as it is more informal and does not explicitly convey the concept of correlation as a measure, which is essential in the context of statistical analysis in AI. Therefore, the first option is the most semantically accurate and contextually appropriate for the AI domain.', ['Pearson correlation', 'Pearson correlation', "Pearson's correlation"])
Penn English Treebank Chinese
('宾州英语树库', '1. Rank: 宾州英语树库, 宾夕法尼亚英语树库, 宾夕法尼亚大学英语树库, 佩恩英语树库\n\n2. Explanation: The term "宾州英语树库" (Penn English Treebank) is the best fit because it accurately captures the essence of the original term while maintaining semantic clarity and contextual relevance. "宾州" is a commonly used abbreviation for "宾夕法尼亚" (Pennsylvania) in both academic and colloquial contexts, making it easily recognizable to those familiar with the field. \n\nThe second option, "宾夕法尼亚英语树库," while accurate, is longer and less commonly used in practice. The third option, "宾夕法尼亚大学英语树库," explicitly mentions the university, which is not necessary for understanding the term in this context and could lead to confusion, as it implies a focus on the institution rather than the linguistic resource itself. Lastly, "佩恩英语树库" (Payne English Treebank) is incorrect as it misrepresents the name, leading to a significant loss of semantic accuracy. \n\nIn the AI domain, where precision and clarity are crucial, "宾州英语树库" stands out as the most effective translation.', ['Pennsylvania English Treebank', 'Penn English Treebank', 'Payne English Treebank', 'University of Pennsylvania English Treebank'])
Penn Treebank corpus Chinese
('宾州树库语料库', '1. Rank: 宾州树库语料库, 宾夕法尼亚树库语料库, 宾夕法尼亚大学树库语料库, 宾州英语树库, 佩恩树库语料库\n\n2. Explanation: The term "宾州树库语料库" (Penn Treebank Corpus) is the best fit because it accurately retains the essence of the original term while using "宾州" (Penn) as a widely recognized abbreviation for Pennsylvania in Chinese. This is important in the AI domain, where brevity and clarity are valued. The back translation aligns closely with the original term, ensuring semantic accuracy. \n\nThe second option, "宾夕法尼亚树库语料库," while also accurate, is longer and less commonly used in the context of the Treebank corpus, which is typically referred to by its abbreviated form. The third option, "宾夕法尼亚大学树库语料库," adds unnecessary detail by including "大学" (University), which is not essential for understanding the corpus in this context. \n\nThe fourth option, "宾州英语树库," introduces a specific language focus (English) that is not present in the original term, which could lead to confusion. Lastly, "佩恩树库语料库" (Payne Treebank Corpus) is incorrect as it misrepresents the name, leading to a significant loss of semantic accuracy. Thus, "宾州树库语料库" is the most contextually appropriate and semantically accurate choice.', ['Pennsylvania Treebank Corpus', 'Penn Treebank Corpus', 'Payne Treebank Corpus', 'Penn English Treebank', 'University of Pennsylvania Treebank Corpus'])
Prop-Bank Chinese
('"Prop-Bank"', '1. Rank: "Prop-Bank", "语义角色标注库", "事件库", "角色库", "命题库"\n\n2. Explanation: The term "Prop-Bank" is the most accurate translation because it retains the original name of the resource, which is crucial in the AI domain where specific datasets and frameworks are often referred to by their established names. The back translation "prop-bank" accurately reflects the original term without altering its meaning or context. \n\nThe second choice, "语义角色标注库" (Semantic role annotation library), while descriptive, is more of a functional description of what Prop-Bank does rather than a direct translation of the term itself. It may lead to confusion as it does not directly reference the established name of the resource.\n\nThe other candidates, such as "事件库" (event library), "角色库" (character library), and "命题库" (proposition library), do not accurately capture the essence of Prop-Bank, which specifically focuses on semantic roles associated with verbs. These terms are either too broad or misaligned with the specific function of Prop-Bank in the context of semantic role labeling in natural language processing. Thus, "Prop-Bank" remains the best fit for its semantic accuracy and contextual relevance in the AI domain.', ['event library', 'prop-bank', 'Character library', 'proposition library', 'Semantic role annotation library'])
R-Precision Chinese
('R-精确率', '1. Rank: R-精确率, R-准确率, R-精度, R-精确度, R精度\n\n2. Explanation: The term "R-精确率" is the best fit because it accurately captures the concept of "precision" in the context of AI metrics, particularly in information retrieval and evaluation metrics. In the AI domain, "精确率" (precision) is a well-established term that refers to the ratio of relevant instances retrieved by a model to the total instances retrieved. This aligns closely with the concept of R-Precision, which is a specific measure of precision based on ranked retrieval. \n\nThe back translation "R-accuracy" for "R-精确率" is also contextually appropriate, as it maintains the focus on the accuracy of the model\'s output in relation to the input prompts. Other candidates like "R-准确率" and "R-精度" are less precise in conveying the specific meaning of "precision" in this context, as "准确率" can imply a broader sense of accuracy without the specific focus on precision, and "精度" is more general and can refer to various types of accuracy without the specific connotation of precision in retrieval tasks. \n\nThus, "R-精确率" stands out as the most semantically accurate and contextually fitting term for the AI domain.', ['R-accuracy', 'Accuracy', 'R-accuracy', 'R-accuracy', 'R-precision'])
Rao-Blackwellization Chinese
('Rao-布莱克维尔化', '1. Rank: Rao-布莱克维尔化, Rao-Blackwell化, Rao-Blackwell 化, 饶-布莱克韦尔化, 劳-布莱克威尔化\n\n2. Explanation: The term "Rao-布莱克维尔化" is the best fit because it retains the original name "Rao" in its proper form, which is crucial in the AI domain where the names of researchers and their contributions are significant. The use of "布莱克维尔" accurately transliterates "Blackwell," maintaining semantic accuracy. This term is widely recognized in the statistical and machine learning communities, making it contextually appropriate for discussions involving Rao-Blackwellization. The other candidates either use incorrect transliterations (like "劳" for "Rao") or do not maintain the integrity of the original name as effectively. "Rao-Blackwell化" and "Rao-Blackwell 化" are also acceptable but less common in usage compared to "Rao-布莱克维尔化." The term "饶-布莱克韦尔化" introduces a different surname "饶," which is not relevant, and "劳-布莱克威尔化" uses "劳," which is incorrect for "Rao." Thus, "Rao-布莱克维尔化" stands out as the most accurate and contextually fitting translation.', ['Rao-Blackwellization', 'Rao-Blackwellization', 'Law-Blackwellization', 'Rao-Blackwellization', 'Rao-Blackwellization'])
Reformer Chinese
('"Reformer"', '1. Rank: "Reformer", "改革者", "变换器", ""\n\n2. Explanation: The term "Reformer" is the best fit because it is a specific name for a model in the AI domain, particularly in the context of self-attention mechanisms. In technical literature, it is common to retain the original English term when referring to specific models or algorithms, as this helps maintain clarity and precision. The back translation of "Reformer" remains "reformer," which accurately reflects the original term without introducing ambiguity.\n\nThe second candidate, "改革者," translates to "reformer" in a general sense, which could imply a person who brings about reform rather than a specific AI model. This could lead to confusion in a technical context. The third candidate, "变换器," translates to "converter," which does not accurately capture the intended meaning of the model in the AI context. The last candidate is empty and thus not applicable.\n\nOverall, retaining "Reformer" as is ensures semantic accuracy and contextual fit within the AI domain, making it the most appropriate choice.', ['reformer', 'reformer', 'converter', 'converter'])
Robertson-Webb model Chinese
('罗伯逊-韦布模型', '1. Rank: 罗伯逊-韦布模型, 罗伯逊-韦伯模型, Robertson-Webb模型, 模型\n\n2. Explanation: The first translation, "罗伯逊-韦布模型," is the best fit because it accurately retains the original names "Robertson" and "Webb" in a way that is semantically correct and contextually appropriate for the AI domain. The use of "韦布" is the standard transliteration for "Webb" in academic and technical contexts, ensuring that readers familiar with the field will recognize the term. The second candidate, "罗伯逊-韦伯模型," uses "韦伯," which is a less common transliteration for "Webb" and may lead to confusion. The third candidate, "Robertson-Webb模型," mixes English and Chinese, which is not ideal for formal writing in Chinese. Lastly, "模型" alone is too vague and does not convey the specific model being referenced. Therefore, the first option is the most accurate and contextually fitting translation.', ['Robertson-Webb model', 'Robertson-Weber model', 'Robertson-Webb model', 'Model'])
Sentence-Piece Chinese
('SentencePiece', '1. Rank: SentencePiece, "句片", "句子片段", "句piece", "在线梯度下降SentencePiece", "Sentence-Piece", ""\n\n2. Explanation: The term "SentencePiece" is the best fit because it is the official name of a specific algorithm developed for subword tokenization in natural language processing, particularly in the context of AI and machine learning. Retaining the original name ensures clarity and precision, as it directly refers to the established method without introducing ambiguity. \n\nThe second candidate, "句片", translates to "sentence piece," which is a literal translation but lacks the recognition and specificity of the original term. "句子片段" translates to "sentence fragment," which could mislead readers into thinking it refers to a different concept in linguistics rather than the specific algorithm. \n\nThe other candidates, such as "句piece" and "在线梯度下降SentencePiece," either mix languages or add unnecessary context that detracts from the term\'s established meaning. "Sentence-Piece" is a close alternative but is less commonly used than "SentencePiece." Therefore, the ranking reflects the importance of using the recognized term in the AI domain for semantic accuracy and contextual fit.', ['Sentence piece', 'sentence piece', 'sentence piece', 'sentence fragment', 'Online Gradient Descent SentencePiece', 'sentence piece', 'sentence-piece'])
Sigmoid Chinese
('S型函数', '1. Rank: S型函数, sigmoid函数, Sigmoid, Sigmoid函數, 激活函数, 或称为“ sigmoid 函数”\n\n2. Explanation: The term "S型函数" (S-shaped function) is the best fit because it accurately describes the mathematical shape of the sigmoid function, which is crucial in the context of neural networks and activation functions. This term is widely recognized in the AI and machine learning communities in Chinese-speaking regions, making it semantically accurate and contextually relevant. \n\nThe second choice, "sigmoid函数," is also a strong candidate as it retains the original English term "sigmoid" while providing the Chinese word for "function," which is commonly used in technical discussions. However, it lacks the descriptive aspect of the shape that "S型函数" provides.\n\n"Sigmoid" and "Sigmoid函數" are less preferable because they do not translate the term into Chinese, which may not be as accessible to all readers. "激活函数" (activation function) is too broad and does not specifically refer to the sigmoid function, while "或称为“ sigmoid 函数”" is overly verbose and not a direct translation. \n\nOverall, "S型函数" is the most contextually appropriate and semantically accurate term for the AI domain.', ['sigmoid function', 'sigmoid', 'Sigmoid function', 'activation function', 'sigmoid function', 'Or called "sigmoid function"'])
Sigmoid function Chinese
('Sigmoid函数', '1. Rank: Sigmoid函数, S型函数, **sigmoid函数**, シグモイド関数\n\n2. Explanation: The term "Sigmoid函数" is the best fit because it accurately retains the original English term "Sigmoid" while using the standard Chinese term "函数" for "function." This translation is semantically accurate and widely recognized in the AI and mathematical communities, ensuring clarity and understanding among professionals. \n\n"S型函数" is a valid alternative, as it describes the shape of the sigmoid function (S-shaped), but it is less commonly used in the context of AI and may not be as immediately recognizable to all audiences. \n\nThe term "**sigmoid函数**" uses a mix of English and Chinese, which can be less formal and may not be preferred in academic or professional settings. \n\nFinally, "シグモイド関数" is a transliteration in Japanese, which is not relevant in this context and does not serve the purpose of conveying the term to a Chinese-speaking audience. Thus, "Sigmoid函数" stands out as the most appropriate choice for its semantic accuracy and contextual fit in the AI domain.', ['Sigmoid function', 'シグモイド关数', '**sigmoid function**', 'sigmoid function'])
Story Cloze Test Chinese
('故事完形测试', '1. Rank: 故事完形测试, 故事闭合测试, 故事填空测试\n\n2. Explanation: The term "故事完形测试" (story cloze test) is the best fit because it directly translates the English term "Cloze" in a way that is commonly understood in the context of language and comprehension tests. The term "完形" (which means "gestalt" or "form") is often used in educational and psychological contexts to refer to tests that require filling in missing information, aligning well with the concept of a cloze test. \n\n"故事闭合测试" (story closure test) is less accurate because it implies a focus on the closure aspect rather than the cloze format, which is central to the test\'s design. While it captures some meaning, it does not convey the specific nature of the test as effectively as "完形".\n\n"故事填空测试" (story fill-in-the-blank test) is also a less precise translation, as it suggests a more general fill-in-the-blank format that may not fully encapsulate the nuances of the cloze test, which is specifically about selecting the correct ending from options rather than simply filling in blanks. \n\nOverall, "故事完形测试" maintains the semantic accuracy and contextual fit required in the AI domain, making it the most appropriate choice.', ['story closure test', 'story cloze test', 'Story fill-in-the-blank test'])
Structure from motion Chinese
('从运动中重建结构', '1. Rank: 从运动中重建结构, 从运动恢复结构, 从运动中恢复结构, 运动重建, 运动恢复结构, 从运动重建\n\n2. Explanation: The term "从运动中重建结构" (Reconstructing structure from motion) is the best fit because it accurately captures the essence of the original English term "Structure from motion" (SfM) in the context of 3D modeling. The phrase emphasizes the process of reconstruction, which is central to the SfM technique, and maintains the original meaning without introducing ambiguity. \n\nThe other candidates, while they may convey similar ideas, either introduce inaccuracies or are less precise in the context of AI and computer vision. For example, "运动恢复结构" (sports recovery structure) misinterprets the term by introducing the concept of "sports," which is irrelevant in this context. "从运动恢复结构" (Restoring structure from movement) and "从运动中恢复结构" (Recovering structure from movement) also imply a different focus on restoration rather than reconstruction, which is not the primary goal of SfM techniques. \n\nOverall, "从运动中重建结构" aligns closely with the technical terminology used in the AI domain, making it the most semantically accurate and contextually appropriate choice.', ['sports recovery structure', 'Reconstructing structure from motion', 'Restoring structure from movement', 'Recovering structure from movement', 'Reconstruction from motion', 'motion reconstruction'])
T5-base model Chinese
('T5-base模型', '1. Rank: T5-base模型, T5基础模型, T5基础型号, 模型\n\n2. Explanation: The term "T5-base模型" is the best fit because it retains the original English term "T5-base" while accurately translating "model" to "模型," which is the standard term used in the AI domain to refer to models. This translation maintains the specific naming convention used in the AI community, ensuring clarity and consistency. The other candidates, such as "T5基础模型" and "T5基础型号," introduce variations that could lead to confusion. "基础" (basic) is less precise than "base," and "型号" (type) is not commonly used in this context. The term "模型" (model) is widely recognized in AI literature, making "T5-base模型" the most semantically accurate and contextually appropriate choice.', ['T5 base model', 'T5-base model', 'Model', 'T5 base model'])
T5-large model Chinese
('T5-large模型', '1. Rank: T5-large模型, T5大型模型, T5-大型号, 模型\n\n2. Explanation: The term "T5-large模型" is the best fit because it retains the original structure of the English term while accurately conveying the specific model type in the AI context. The use of "模型" (model) directly after "T5-large" maintains the technical specificity required in AI terminology. The back translation "T5-large model" confirms that the meaning is preserved. \n\nIn contrast, "T5大型模型" translates to "T5 large model," which, while still accurate, introduces a space that is not present in the original term, potentially leading to confusion in technical contexts. "T5-大型号" is less appropriate as "号" (number) is not commonly used in this context and could mislead the reader regarding the nature of the model. Lastly, "模型" alone is too vague and lacks the necessary specificity to convey the intended meaning of the original term. Thus, "T5-large模型" is the most semantically accurate and contextually appropriate choice.', ['T5 large model', 'T5-large model', 'Model', 'T5-large model'])
Tanh Chinese
('双曲正切函数', '1. Rank: 双曲正切函数, 双曲正切, Tanh, 泰勒近似, 谭\n\n2. Explanation: The term "双曲正切函数" (hyperbolic tangent function) is the most semantically accurate and contextually appropriate translation for "Tanh" in the AI domain. It explicitly describes the mathematical function being referenced, which is crucial in a technical context where clarity is essential. The second option, "双曲正切" (hyperbolic tangent), is also a strong candidate as it conveys the same mathematical concept but lacks the specificity of being a "function." \n\nThe term "Tanh" is a direct transliteration and is commonly used in programming and mathematical contexts, but it may not be as immediately clear to all audiences without prior knowledge. "泰勒近似" (Taylor approximation) is unrelated to the term "Tanh" and thus irrelevant in this context. Lastly, "谭" (Tan) is a transliteration that does not convey the mathematical meaning and is not suitable for this specific usage. Therefore, "双曲正切函数" is the best fit due to its clarity and precision in the AI domain.', ['hyperbolic tangent function', 'hyperbolic tangent', 'Taylor approximation', 'tan or', 'Tan'])
Text-to-Text Transfer Transformer Chinese
('文本到文本传输转换器', '1. Rank: 文本到文本传输转换器, 文本到文本的迁移转换器, 文本到文本的迁移变换器, 文本到文本转移变换器, 文本到文本转换变换器, 文本到文本转换转换器, 文本到文本转换器, 文本到文本转换Transforme\n\n2. Explanation: The term "文本到文本传输转换器" (Text-to-Text Transfer Converter) is the best fit because it accurately captures the essence of the original English term "Text-to-Text Transfer Transformer." The word "传输" (transfer) directly corresponds to "transfer" in the English term, which is crucial in the context of AI and machine learning, where "transfer" often refers to the ability to apply knowledge learned from one task to another. Additionally, "转换器" (converter) is a commonly used term in the AI domain, indicating a model that converts input text into output text. \n\nThe other candidates, while they may have some semantic overlap, either use less precise terminology (like "迁移" which translates to "migration") or introduce unnecessary complexity (like "变换器" which translates to "transformer" but does not align with the specific function of the model). Therefore, "文本到文本传输转换器" stands out as the most contextually appropriate and semantically accurate translation for the AI domain.', ['text to text converter', 'text to text converter', 'Text to text migration transformer', 'text to text transfer converter', 'Text to text conversionTransforme', 'text to text converter', 'Text to text migration converter', 'text to text transfer converter'])
Transformer-like Chinese
('类Transformer的', '1. Rank: 类Transformer的, 类Transformer, 类变换器的, 类变换器的\n\n2. Explanation: The term "类Transformer的" is the best fit because it directly translates to "Transformer-like" while retaining the specific reference to the Transformer architecture, which is crucial in the AI domain. The use of "Transformer" in its original form ensures that the term is recognized within the context of AI and machine learning, where "Transformer" refers to a well-established model architecture. \n\nThe second candidate, "类Transformer," is also a strong option, but it lacks the possessive form "的," which can make it slightly less formal in this context. The third candidate, "类变换器的," translates to "converter-like," which deviates from the specific meaning of "Transformer" in AI, as "变换器" (converter) does not convey the same technical significance. Lastly, "类似变换器的" translates to "similar to a converter," which is too vague and does not accurately reflect the specific architecture being discussed. Therefore, "类Transformer的" is the most semantically accurate and contextually appropriate choice.', ['converter-like', 'Transformer class', 'ClassTransformer', 'similar to a converter'])
Unigram Chinese
('一元语法', '1. Rank: 一元语法, 单词模型, 单元词, 单元语\n\n2. Explanation: The term "一元语法" (back translation: "monary grammar") is the best fit for the English term "Unigram" in the context of AI and natural language processing. In computational linguistics, "unigram" specifically refers to a model that considers single words (or tokens) independently, which aligns with the concept of "一元" (one or single) in "一元语法" (one-gram grammar). This term accurately captures the statistical nature of unigrams as it relates to language modeling.\n\nThe other candidates, such as "单词模型" (word model), while somewhat relevant, do not convey the specific statistical aspect of unigrams as effectively. "单元词" (unit word) and "单元语" (unit language) are less precise and do not directly relate to the concept of unigrams in the context of language processing. Therefore, "一元语法" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['word model', 'monary grammar', 'unit word', 'unit language'])
Universal dependency Chinese
('通用依存关系', '1. Rank: 通用依存关系, 通用依存, 通用依赖关系, 通用依赖, 普遍依赖关系, 普遍依赖, 通用依赖\n\n2. Explanation: The term "通用依存关系" (universal dependency relation) is the best fit for the translation of "Universal dependency" in the AI domain, particularly in the context of natural language processing and syntactic analysis. This term accurately captures the semantic meaning of "universal" as it relates to a broad, standardized framework for representing syntactic dependencies across languages. The use of "依存" (dependency) is more precise than "依赖" (reliance), as it aligns with the established terminology in linguistic studies, particularly in the context of Universal Dependencies (UD) frameworks. \n\nThe back translation of "通用依存关系" as "universal dependencies" maintains the intended meaning and context, making it semantically accurate. Other candidates like "通用依赖关系" and "通用依赖" are less precise because "依赖" can imply a more general sense of reliance rather than the specific linguistic relationship denoted by "依存." Therefore, "通用依存关系" is the most contextually appropriate choice for the AI domain.', ['common dependencies', 'universal dependency', 'universal dependencies', 'universal dependence', 'Common dependencies', 'universal dependencies'])
Vandermonde matrix Chinese
('范德蒙德矩阵', '1. Rank: 范德蒙德矩阵, Vandermonde 矩阵, 范德蒙矩阵, 范德蒙矩陣, 瓦尔德蒙德矩阵, 瓦德蒙德矩阵\n\n2. Explanation: The term "范德蒙德矩阵" is the best fit because it accurately translates the name "Vandermonde" while maintaining the semantic integrity of the term in the context of linear algebra and matrix theory. This translation is widely recognized in the mathematical community and retains the original name\'s phonetic and semantic elements. The back translation "Vandermonde matrix" aligns perfectly with the English term, ensuring clarity and consistency in academic discourse. \n\nThe second candidate, "Vandermonde 矩阵," is also a good option, but it mixes English and Chinese, which may not be as preferred in formal contexts. The other candidates either contain transliteration errors or do not accurately reflect the original term, making them less suitable for use in the AI domain where precision in terminology is crucial.', ['Vandermonde matrix', 'Vandermonde matrix', 'Vandermond matrix', 'Wadmund Matrix', 'Waldmund matrix', 'Vandermond matrix'])
Winograd Schema Chinese
('维诺格拉德模式', '1. Rank: 维诺格拉德模式, 温诺格拉德模式, 温格拉德模式, Winograd 结构, 温哥华模式, 温格拉德框架\n\n2. Explanation: The term "维诺格拉德模式" is the best fit because it accurately retains the original name "Winograd" while providing a clear and contextually appropriate translation of "Schema" as "模式" (mode or pattern). This translation is semantically accurate and aligns well with the AI domain, where "模式" is commonly used to refer to models or schemas in various contexts, including machine learning and natural language processing. \n\nThe second candidate, "温诺格拉德模式," while also retaining the name, uses a different transliteration that is less common and may not be as recognizable in the AI community. The third candidate, "温格拉德模式," similarly alters the transliteration but still maintains the structure. \n\nThe other candidates, such as "Winograd 结构" and "温哥华模式," deviate from the original name or context, making them less suitable for the specific AI terminology. "温格拉德框架" introduces "框架" (framework), which changes the meaning and does not align with the intended use of "Schema." Thus, "维诺格拉德模式" stands out as the most accurate and contextually fitting translation.', ['Winograd model', 'Winograd model', 'Weingrad framework', 'Winograd structure', 'vancouver model', 'Weingrad model'])
Winograd Schema Challenge Chinese
('维诺格拉德模式挑战', '1. Rank: 维诺格拉德模式挑战, 温诺格拉德模式挑战, 温格拉德模式挑战, Winograd 结构挑战, 温哥华模式挑战, 温格拉德框架挑战\n\n2. Explanation: The term "维诺格拉德模式挑战" is the best fit because it accurately retains the original name "Winograd" while providing a clear and contextually appropriate translation of "Schema Challenge" as "模式挑战". This translation maintains semantic accuracy and is easily recognizable in the AI domain, where "模式" (model/schema) is commonly used in discussions of machine learning and AI benchmarks. The back translation "Winograd model challenge" aligns closely with the original English term, ensuring clarity and consistency. \n\nThe second candidate, "温诺格拉德模式挑战," is also a good option but uses a different transliteration of "Winograd," which may not be as widely recognized. The other candidates either introduce inaccuracies in the name or use terms that are less relevant in the context of AI and commonsense reasoning challenges. For instance, "温哥华模式挑战" translates to "Vancouver Model Challenge," which is entirely incorrect in this context.', ['Winograd model challenge', 'Winograd Mode Challenge', 'Weingrad Framework Challenge', 'Winograd structural challenges', 'Vancouver Model Challenge', 'Weingrad Model Challenge'])
Zipf Chinese
('齐普夫', '1. Rank: 齐普夫, 齊普夫, 齐夫, 兹夫, 兹甫\n\n2. Explanation: The top-ranked translations "齐普夫" and "齊普夫" are the best fit because they maintain the phonetic similarity to the original term "Zipf" while also being a transliteration that is commonly used in the AI and linguistic communities in China. This is important in the AI domain, where specific terms often retain their original names to ensure clarity and recognition among professionals. The use of "齐" (Qi) and "普" (Pu) closely mirrors the pronunciation of "Zipf," making it easily recognizable. \n\nThe other candidates, such as "齐夫" and "兹夫," do not capture the full phonetic essence of "Zipf" and may lead to confusion, as they are less commonly associated with the term in academic contexts. "兹甫" is also less relevant as it does not closely resemble the original term and lacks recognition in the field. Therefore, "齐普夫" and "齊普夫" are the most semantically accurate and contextually appropriate choices for the AI domain.', ['Ziff', 'Zipf', 'Zipf', 'ziv', 'Zifu'])
Zipf distribution Chinese
('齐夫分布', '1. Rank: 齐夫分布, 齐普夫分布, Zipf分布, 兹夫分布, 兹甫分布\n\n2. Explanation: The term "齐夫分布" is the most widely accepted and recognized translation for "Zipf distribution" in the Chinese academic and AI context. It retains the original name "Zipf," which is crucial for semantic accuracy, especially in technical fields where the name of the distribution is tied to the work of the linguist George Zipf. This translation is not only semantically accurate but also contextually appropriate, as it is commonly used in literature discussing statistical distributions and natural language processing. \n\nThe second candidate, "齐普夫分布," while also a transliteration, is less common and may not be as readily recognized by professionals in the field. "Zipf分布" is a direct transliteration that lacks the contextual grounding of the original name, making it less preferable. The last two candidates, "兹夫分布" and "兹甫分布," are inaccurate and do not convey the correct meaning or context, as they deviate from the established terminology. Thus, "齐夫分布" stands out as the best fit for both semantic and contextual accuracy in the AI domain.', ['Zipf distribution', 'Zipf distribution', 'Zipf distribution', 'Ziff distribution', 'distribution'])
Zipf's law Chinese
('"Zipf定律"', '1. Rank: "Zipf定律", "齐夫定律", "齐普夫定律", "兹夫定律", "兹甫定律"\n\n2. Explanation: The term "Zipf定律" is the best fit because it retains the original name "Zipf" in its transliteration, which is crucial in the AI domain where specific laws and theories are often attributed to their originators. This term is widely recognized in academic and technical literature, ensuring semantic accuracy and contextual fit. The other candidates, such as "齐夫定律" and "齐普夫定律," while they may be phonetically similar, do not maintain the integrity of the original name as effectively as "Zipf定律." The use of "Zipf" directly connects the term to the established body of work surrounding Zipf\'s law, making it the most appropriate choice in this context.', ["Zip's law", "Zipf's law", "Zipf's law", "Ziff's law", "Zip's law"])
action set Chinese
('动作集', '1. Rank: 动作集, 动作集合, 行动集合\n\n2. Explanation: The term "动作集" (action set) is the best fit for the AI domain-specific usage because it is concise and commonly used in the context of machine learning and reinforcement learning. In AI literature, "动作集" is frequently employed to refer to a collection of possible actions an agent can take, making it semantically accurate and contextually appropriate. \n\n"动作集合" (action collection) is a more literal translation but is less commonly used in the AI field, which may lead to confusion or lack of clarity. "行动集合" (action collection) is also less preferred as "行动" (action) can imply a broader range of activities beyond the specific context of actions in AI, which typically refers to discrete choices available to an agent. Therefore, "动作集" is the most precise and widely recognized term in the context of AI, particularly in discussions about action spaces in reinforcement learning and multi-agent systems.', ['action set', 'action set', 'action set'])
actor critic algorithm Chinese
('行动者-评论家算法', '1. Rank: 行动者-评论家算法, 行为者-评论家算法, 演员-评论员算法, 执行者-评论者算法, 行动者-评论者算法\n\n2. Explanation: The term "行动者-评论家算法" (Actor-Critic Algorithm) is the best fit for several reasons. Firstly, it accurately captures the essence of the original English term, maintaining the semantic integrity of "actor" and "critic," which are standard terms in reinforcement learning. The back translation aligns perfectly with the original term, ensuring clarity and consistency in the AI domain. \n\nThe second candidate, "行为者-评论家算法," is also a strong contender, but "行动者" is more commonly used in the context of AI and machine learning literature in Chinese. The term "演员-评论员算法" (Actor-Commentator Algorithm) introduces a less precise interpretation of the roles, as "演员" (actor) and "评论员" (commentator) do not convey the same technical meaning as "actor" and "critic" in this context. \n\nThe other candidates, "执行者-评论者算法" (Performer-Reviewer Algorithm) and "行动者-评论者算法" (Actor-Commenter Algorithm), deviate further from the established terminology in the AI field, making them less suitable. Overall, "行动者-评论家算法" is the most contextually appropriate and semantically accurate translation for the term "actor critic algorithm."', ['actor-critic algorithm', 'Actor-Commentator Algorithm', 'Performer-Reviewer Algorithm', 'Actor-Critic Algorithm', 'Actor-Commenter Algorithm'])
actor-critic framework Chinese
('行为者-评论家框架', '1. Rank: 行为者-评论家框架, 行动者-评论家框架, 演员-评论员框架, 执行者-评论者框架, 行动者-评论者框架\n\n2. Explanation: The term "行为者-评论家框架" (actor-critic framework) is the best fit because it accurately captures the semantics of the original English term while maintaining a clear and direct connection to the AI domain. The word "行为者" (actor) is commonly used in reinforcement learning literature to refer to the agent that takes actions, while "评论家" (critic) is a well-established term for the component that evaluates the actions taken by the actor. This translation aligns with the terminology used in the field, ensuring that it is easily understood by practitioners and researchers.\n\nThe alternative "行动者-评论家框架" is also a strong candidate, as "行动者" (actor) is another acceptable term, but it is slightly less common in the context of reinforcement learning compared to "行为者." The other candidates, such as "演员-评论员框架" and "执行者-评论者框架," introduce terms that are less precise or not standard in the AI context, which could lead to confusion. Therefore, "行为者-评论家框架" is the most semantically accurate and contextually appropriate choice.', ['actor-critic framework', 'actor-commentator framework', 'Actor-Reviewer Framework', 'actor-critic framework', 'actor-commentator framework'])
adversarial example Chinese
('对抗样本', '1. Rank: 对抗样本, 对抗示例, 对抗性样本, 对抗性示例, 对抗性例子\n\n2. Explanation: The term "对抗样本" (duìkàng yàngběn) is the most widely accepted and recognized translation for "adversarial example" in the AI domain. It accurately captures the concept of an example that is specifically designed to deceive a machine learning model while remaining similar to a legitimate input. The term "样本" (yàngběn) translates to "sample," which is a common term in machine learning, making it contextually appropriate. \n\nThe second candidate, "对抗示例" (duìkàng shìlì), translates to "adversarial example" as well, but "示例" (shìlì) means "example" in a more general sense and is less commonly used in the specific context of AI. \n\nThe other candidates, such as "对抗性样本" (duìkàng xìng yàngběn) and "对抗性示例" (duìkàng xìng shìlì), introduce the term "性" (xìng), which implies a characteristic or quality, making them less precise in this context. "对抗性例子" (duìkàng xìng lìzi) is also less suitable as "例子" (lìzi) is a more informal term for "example." \n\nOverall, "对抗样本" is the most semantically accurate and contextually fitting term for "adversarial example" in the AI field.', ['Adversarial examples', 'Adversarial examples', 'Adversarial examples', 'adversarial examples', 'Adversarial examples'])
advmod Chinese
('副词修饰语', '1. Rank: 副词修饰语, 状语修饰语, 副词修饰, 形容词副词修饰语\n\n2. Explanation: The term "副词修饰语" (adverb modifier) is the best fit because it accurately captures the semantic meaning of "advmod" in the context of dependency grammar, where it refers specifically to a modifier that is an adverb. This term is widely recognized in linguistic and AI contexts, making it semantically precise. \n\nThe second candidate, "状语修饰语" (adverbial modifier), is also a strong contender, as "状语" refers to adverbial phrases, but it is slightly less direct than "副词修饰语." The term "副词修饰" (adverb modification) is less accurate because it implies a process rather than the specific role of the modifier itself. Lastly, "形容词副词修饰语" (adjective adverb modifier) is overly complex and introduces unnecessary elements (adjective) that do not pertain to the original term "advmod," which specifically refers to adverbs. Thus, "副词修饰语" is the most contextually appropriate and semantically accurate choice for the AI domain.', ['adverb modifier', 'adverb modification', 'adverbial modifier', 'Adjective Adverb Modifier If you need more translations or assistance, feel free to ask!'])
aleatoric uncertainty Chinese
('随机不确定性', '1. Rank: 随机不确定性, 偶然不确定性, 任意不确定性\n\n2. Explanation: The term "随机不确定性" (random uncertainty) is the best fit for "aleatoric uncertainty" in the AI domain. In the context of AI and machine learning, "aleatoric uncertainty" refers to the inherent randomness or noise in the data that cannot be reduced by further modeling. The term "随机" (random) accurately captures this concept of unpredictability and variability in the data, aligning well with the established usage in the field. \n\nOn the other hand, "偶然不确定性" (accidental uncertainty) implies a sense of chance or happenstance that does not fully convey the systematic nature of uncertainty in data. "任意不确定性" (arbitrary uncertainty) suggests a lack of specific constraints or rules, which is not appropriate in this context. Therefore, "随机不确定性" is the most semantically accurate and contextually appropriate translation for "aleatoric uncertainty" in AI terminology.', ['random uncertainty', 'accidental uncertainty', 'arbitrary uncertainty'])
alpha compositing Chinese
('α合成', '1. Rank: α合成, 阿尔法合成, alpha合成\n\n2. Explanation: The term "α合成" is the best fit for "alpha compositing" in the AI domain due to its semantic accuracy and contextual relevance. The use of the Greek letter "α" (alpha) directly corresponds to the English term, maintaining the technical specificity that is crucial in AI and graphics contexts. This term is concise and widely recognized in the field, making it suitable for professionals familiar with the concept. \n\n"阿尔法合成" (alpha synthesis) is a transliteration that adds unnecessary complexity and may not be as immediately recognizable to those in the field. While it retains the meaning, it does not align as closely with established terminology. \n\n"alpha合成" (alpha synthesis) uses the English word "alpha" directly, which is less common in professional Chinese literature and may lead to confusion. It lacks the precision and clarity that "α合成" provides, making it a less favorable choice. \n\nOverall, "α合成" is the most contextually appropriate and semantically accurate translation for "alpha compositing" in the AI domain.', ['alpha synthesis', 'alpha synthesis', 'alpha synthesis'])
anaphoric reference Chinese
('指代引用', '1. Rank: 指代引用, 指代参考, 照应指代, 代指引用, anaphoric reference\n\n2. Explanation: The term "指代引用" (anaphoric reference) is the best fit because it accurately captures the semantic meaning of "anaphoric reference" in the context of AI and linguistics. "指代" directly translates to "reference" or "to refer," which is essential in understanding how anaphora functions in language. The term "引用" (to quote or reference) complements this by indicating that the reference is being made to something previously mentioned. \n\n"指代参考" is also a strong candidate, as it similarly conveys the idea of referencing, but it is slightly less common in usage compared to "指代引用." \n\n"照应指代" translates to "corresponding reference," which, while related, does not directly convey the specific linguistic concept of anaphora as effectively as the first two options. \n\n"代指引用" (proxy reference) introduces a different nuance that may not align with the specific linguistic context of anaphoric reference, making it less suitable. \n\nFinally, "anaphoric reference" in English does not provide a contextual fit for a Chinese audience, as it does not utilize the native language, which is essential for clarity in communication within the AI domain.', ['anaphoric reference', 'Anaphora', 'Proxy reference', 'anaphoric reference', 'anaphoric reference'])
antithetic sampling Chinese
('对立采样', '1. Rank: 对立采样, 对立抽样, 对偶抽样, 对偶采样\n\n2. Explanation: The term "对立采样" (antithetic sampling) is the best fit because it accurately captures the concept of sampling that involves pairing values with their opposites, which is central to the method described in the context. The back translation "Oppositional sampling" aligns well with the original English term, maintaining the semantic integrity of the concept. \n\n"对立抽样" (Opponent sampling) is a close second, as it also conveys a similar meaning, but "对立" (oppositional) is more commonly used in statistical contexts to describe the relationship between the sampled pairs. \n\nThe terms "对偶抽样" and "对偶采样" (Dual sampling) are less appropriate because "对偶" (dual) does not accurately reflect the nature of antithetic sampling, which specifically involves negation rather than a duality. Therefore, they do not fit the context as well as the first two options.', ['Oppositional sampling', 'Dual sampling', 'Dual sampling', 'Opponent sampling'])
arg max Chinese
('"极大值"', '1. Rank: "极大值", "最大化参数", "最优解", "最大值参数", "架构搜索"\n\n2. Explanation: The term "极大值" (back translated as "maximum value") is the best fit for "arg max" in the context of AI and mathematical optimization. In mathematical terminology, "arg max" refers to the argument (input) that maximizes a given function, which aligns closely with the concept of "极大值" as it directly conveys the idea of finding the maximum value of a function. \n\nThe other candidates, while related, do not capture the specific meaning of "arg max" as effectively. For instance, "最大化参数" (maximize parameters) suggests a process rather than the specific point that maximizes a function. "最优解" (optimal solution) implies a solution that is optimal but does not specifically refer to the argument that achieves this optimality. "最大值参数" (maximum parameter) is somewhat misleading as it suggests a parameter rather than the argument that maximizes the function. Lastly, "架构搜索" (schema search) is unrelated to the concept of maximizing a function and is more relevant to structural or architectural searches in AI.\n\nThus, "极大值" is the most semantically accurate and contextually appropriate translation for "arg max" in the AI domain.', ['maximum value', 'maximize parameters', 'optimal solution', 'Maximum parameter', 'Schema search'])
arg min Chinese
('"最小值点"', '1. Rank: "最小值点", "最小值", "最小化", "取最小值", "最小化参数", "最小值参数"\n\n2. Explanation: The term "最小值点" (minimum point) is the best fit for "arg min" in the context of AI and optimization. In mathematical optimization, "arg min" refers to the argument (input) that minimizes a given function, which is conceptually represented as the point at which the minimum value occurs. Therefore, "最小值点" accurately captures this meaning, as it directly refers to the point where the minimum value is achieved.\n\nThe second choice, "最小值" (minimum value), while related, does not convey the idea of the argument or input that leads to that minimum, making it less precise in this context. "最小化" (minimize) and "取最小值" (take the minimum value) are also relevant but focus more on the action rather than the specific point or argument being referred to. The terms "最小化参数" (minimize parameters) and "最小值参数" (minimum parameter) are less suitable as they imply a focus on parameters rather than the broader concept of finding the minimum point in a function\'s domain. Thus, "最小值点" is the most semantically accurate and contextually appropriate choice for "arg min" in the AI domain.', ['Minimize parameters', 'Minimum parameter', 'minimum value', 'Take the minimum value', 'minimum point', 'minimize'])
arity Chinese
('叉数', '1. Rank: 叉数, 元数, 维数, 数量, 词性\n\n2. Explanation: The term "叉数" (back translated as "Number of forks") is the best fit for "arity" in the AI domain, particularly in the context of parsing and dependencies. In computational linguistics, "arity" refers to the number of arguments or operands that a function or operation can take, which aligns well with the concept of "forks" in a tree structure or branching in parsing. \n\n"元数" (back translated as "Elements") could also be a contender, as it suggests a count of components, but it lacks the specific connotation of branching or structure that "叉数" conveys. "维数" (back translated as "Dimension") is more abstract and typically relates to geometric or mathematical dimensions rather than the specific context of dependencies in parsing. \n\n"数量" (back translated as "quantity") is too vague and does not capture the specific nature of arity, while "词性" (back translated as "part of speech") is entirely unrelated to the concept of arity in this context. Therefore, "叉数" is the most semantically accurate and contextually appropriate translation for "arity" in the AI domain.', ['Number of forks', 'quantity', 'part of speech', 'Elements', 'Dimension'])
autocalibration Chinese
('自动标定', '1. Rank: 自动标定, 自动校准, 自校准\n\n2. Explanation: The term "自动标定" (Automatic calibration) is the best fit for the context of "autocalibration" in the AI domain. This is because "标定" specifically refers to the process of calibration in a technical sense, which aligns closely with the context of estimating parameters in computer vision and related fields. The term "自动" (automatic) accurately conveys the automated aspect of the process, which is crucial in AI applications. \n\nOn the other hand, "自动校准" (automatic calibration) is also a strong candidate, but "校准" is a more general term that can refer to calibration in various contexts, not necessarily technical. "自校准" (self-calibration) implies a process where the system calibrates itself, which may not fully capture the intended meaning of "autocalibration" in this specific context, where the focus is on the automated process rather than self-referential calibration. Therefore, "自动标定" is the most semantically accurate and contextually appropriate choice.', ['Automatic calibration', 'self-calibration', 'automatic calibration'])
autonomous vehicle Chinese
('自动驾驶车辆', '1. Rank: 自动驾驶车辆, 自动驾驶汽车, 自主驾驶汽车, 自主车辆\n\n2. Explanation: The term "自动驾驶车辆" (zìdòng jiàshǐ chēliàng) is the best fit for the translation of "autonomous vehicle" in the AI domain. This term accurately captures the essence of the technology, emphasizing the "automatic driving" aspect, which is crucial in the context of AI and vehicle automation. The back translation "autonomous vehicles" aligns well with the original English term, maintaining semantic accuracy.\n\n"自动驾驶汽车" (zìdòng jiàshǐ qìchē) is also a strong candidate, translating to "self-driving cars." However, while it is commonly used in everyday language, it may imply a narrower focus on passenger cars rather than encompassing all types of autonomous vehicles, including trucks and other forms of transport.\n\n"自主驾驶汽车" (zìzhǔ jiàshǐ qìchē) translates to "autonomous driving cars," which is somewhat accurate but less commonly used in the industry compared to "自动驾驶车辆." The term "自主车辆" (zìzhǔ chēliàng) translates to "autonomous vehicles," but it lacks the specificity of "driving" and may not be as widely recognized in the context of AI and vehicle technology.\n\nOverall, "自动驾驶车辆" is the most contextually appropriate and semantically accurate term for the AI domain, as it directly relates to the technology\'s function and is widely accepted in both academic and industry discussions.', ['autonomous vehicles', 'autonomous vehicles', 'autonomous vehicles', 'self-driving cars'])
backpointer Chinese
('"回指针"', '1. Rank: "回指针", "回溯指针", "后退指针", "回指器"\n\n2. Explanation: The term "回指针" (back pointer) is the best fit for the AI domain-specific usage because it accurately captures the concept of a pointer that refers back to a previous state or position in a data structure, which is essential in algorithms like Viterbi parsing. The back translation "back pointer" aligns perfectly with the original English term, maintaining semantic accuracy. \n\n"回溯指针" (backtracking pointer) is a close second, but it implies a broader context of backtracking algorithms, which may not be specifically relevant to the context provided. "后退指针" (backward pointer) is less commonly used in this context and could lead to confusion, as it may suggest a different operational mechanism. Lastly, "回指器" (anaphora) is not appropriate, as it refers to a linguistic term rather than a computational concept, making it the least suitable choice. Thus, "回指针" stands out as the most contextually and semantically accurate translation for "backpointer" in the AI domain.', ['back pointer', 'Backtracking pointer', 'anaphora', 'Back pointer'])
bandit Chinese
('多臂老虎机', '1. Rank: 多臂老虎机, 强盗, 赌博机, 赌徒, 土匪\n\n2. Explanation: The term "多臂老虎机" (multi-armed bandit) is the best fit for the AI domain-specific usage because it directly refers to a well-established concept in reinforcement learning and decision-making, where the "bandit" problem involves making a series of decisions to maximize rewards based on uncertain outcomes. This term is widely recognized in the AI community and accurately captures the essence of the problem being discussed. \n\nThe other candidates, such as "强盗" (robber) and "土匪" (bandit), while they may translate the word "bandit" literally, do not convey the specific meaning used in the context of machine learning. "赌博机" (gambling machine) and "赌徒" (gambler) also stray from the intended meaning, as they imply a focus on gambling rather than the algorithmic decision-making process central to the bandit problem. Therefore, "多臂老虎机" is the most semantically accurate and contextually appropriate choice.', ['robber', 'gambling machine', 'gambler', 'bandit', 'multi-armed bandit'])
bandit feedback Chinese
('强盗反馈', '1. Rank: 强盗反馈, 多臂老虎机反馈, 赌徒反馈, 赌博机反馈\n\n2. Explanation: The term "强盗反馈" (bandit feedback) is the most accurate translation in the context of AI and machine learning. This term directly corresponds to the concept of "bandit" in the "multi-armed bandit" problem, which is a well-established framework in reinforcement learning and decision-making scenarios. The term "多臂老虎机反馈" (multi-armed bandit feedback) is also relevant, as it explicitly references the multi-armed bandit problem, but it is more verbose and less commonly used in general discussions about bandit feedback. \n\nThe other candidates, "赌徒反馈" (gambler feedback) and "赌博机反馈" (gambling machine feedback), while they may convey a similar idea, do not capture the specific technical meaning associated with the bandit problem in AI. "赌徒" (gambler) and "赌博机" (gambling machine) imply a more general gambling context rather than the specific algorithmic framework used in machine learning. Therefore, "强盗反馈" is the best fit for its semantic accuracy and contextual relevance in the AI domain.', ['Bandit feedback', 'Gambling Machine Feedback', 'Gambler feedback', 'Multi-armed bandit feedback'])
bandit learning Chinese
('多臂老虎机学习', '1. Rank: 多臂老虎机学习, 强盗学习, 赌博机学习, 赌徒学习\n\n2. Explanation: The term "多臂老虎机学习" (Multi-armed bandit learning) is the best fit because it accurately reflects the specific concept in the AI domain known as "multi-armed bandit" problems, which are a fundamental topic in reinforcement learning and decision-making under uncertainty. This term is widely recognized in both English and Chinese literature related to AI and machine learning, making it semantically precise and contextually appropriate.\n\nThe second candidate, "强盗学习" (Bandits learn), while it captures the essence of the term, lacks the specificity of the "multi-armed" aspect, which is crucial for understanding the particular type of learning being discussed. The other candidates, "赌博机学习" (Gambling machine learning) and "赌徒学习" (Gamblers learn), are less accurate as they do not convey the technical meaning associated with the multi-armed bandit framework and could lead to confusion in an academic or professional context. Thus, "多臂老虎机学习" is the most contextually relevant and semantically accurate choice.', ['Bandits learn', 'Gambling machine learning', 'gamblers learn', 'Multi-armed bandit learning'])
bart-base Chinese
('"bart-base模型"', '1. Rank: "bart-base模型", "BART基础版", "巴特基地", "Bart-Barcelona", "扰动方差"\n\n2. Explanation: The term "bart-base模型" is the best fit because it retains the original English term "bart-base" while adding the Chinese word "模型" (model), which is contextually appropriate in the AI domain. This translation accurately reflects the technical nature of the term and maintains its semantic integrity. The back translation "bart-base model" aligns perfectly with the original English term, ensuring clarity and consistency in meaning. \n\nThe second choice, "BART基础版," translates to "BART Basic Edition," which is also a reasonable option but introduces the word "基础版" (Basic Edition), which may imply a versioning system that is not explicitly stated in the original context. \n\nThe third option, "巴特基地," translates to "Bart-Barcelona," which is a misinterpretation as it introduces an unrelated geographical reference and does not convey the intended meaning of the model.\n\nThe fourth option, "Bart-Barcelona," is a direct transliteration that does not provide any contextual relevance to the AI model being discussed.\n\nLastly, "扰动方差" translates to "Disturbance variance," which is completely unrelated to the term "bart-base" and does not fit the context at all. \n\nThus, "bart-base模型" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['Bart-Barcelona', 'Bart Base', 'BART Basic Edition', 'bart-base model', 'Disturbance variance'])
bart-large Chinese
('"bart-large模型"', '```\n1. Rank: "bart-large模型", "BART大型版", "巴特大号", "音素", "Bart-large"\n\n2. Explanation: The term "bart-large模型" is the best fit because it retains the original English term "bart-large" while adding the word "模型" (model), which is contextually appropriate in the AI domain. This term clearly indicates that it refers to a specific model in the context of machine learning and natural language processing. The back translation "bart-large model" accurately reflects the original term, maintaining semantic accuracy. \n\nThe second choice, "BART大型版," translates to "BART large version," which is also accurate but slightly less precise than "模型" in the context of AI terminology, where "model" is the standard term used. "巴特大号" (Bart tuba) is a transliteration that does not convey the intended meaning in the AI context, and "音素" (phoneme) is unrelated to the term "bart-large." Lastly, "Bart-large" is simply a transliteration and does not provide any additional context or clarity. Thus, "bart-large模型" is the most contextually relevant and semantically accurate choice. \n```', ['Bart-large', 'bart tuba', 'BART large version', 'bart-large model', 'phoneme'])
beam size Chinese
('波束大小', '1. Rank: 波束大小, 束大小, 束宽度, 束宽, 光束大小, 信息集\n\n2. Explanation: The term "波束大小" (Beam size) is the best fit for the AI domain, particularly in the context of beam search decoding algorithms. This term accurately captures the concept of "beam size" as it is commonly used in machine learning and natural language processing. The back translation "Beam size" aligns perfectly with the original English term, ensuring semantic accuracy. \n\n"束大小" (bundle size) is a close second, but "束" (bundle) does not convey the specific meaning of "beam" in this context, which is crucial for understanding the algorithm\'s function. "束宽度" (beam width) and "束宽" (beam width) are also relevant but are more commonly associated with the width of the beam rather than its size, which can lead to confusion in this specific context. "光束大小" (light beam size) introduces unnecessary ambiguity by relating it to light, which is not relevant here. Lastly, "信息集" (information set) is entirely unrelated to the concept of beam size in AI, making it the least suitable option. Thus, "波束大小" is the most contextually appropriate and semantically accurate translation for the term "beam size" in the AI domain.', ['bundle size', 'Beam size', 'beam width', 'Beam width', 'Beam size', 'information set'])
beam width Chinese
('波束宽度', '1. Rank: 波束宽度, 束宽, 搜索宽度, 行为克隆\n\n2. Explanation: The term "波束宽度" (Beamwidth) is the best fit for the AI domain-specific usage of "beam width" because it accurately captures the technical meaning of the term as used in beam search algorithms. In the context of AI and machine learning, "beam width" refers to the number of hypotheses considered at each step of the decoding process, which is crucial for balancing the trade-off between computational efficiency and the quality of the generated output. \n\n"束宽" (Beam width) is also a close candidate, but it is less commonly used in the literature compared to "波束宽度." "搜索宽度" (search width) is a more generic term and does not specifically convey the concept of beam search, which is a specific algorithmic technique. Lastly, "行为克隆" (behavioral cloning) is entirely unrelated to the concept of beam width and should not be considered in this context. \n\nOverall, "波束宽度" is the most semantically accurate and contextually appropriate translation for "beam width" in the AI domain.', ['Beamwidth', 'search width', 'Beam width', 'behavioral cloning'])
bigram language model Chinese
('二元组语言模型', '1. Rank: 二元组语言模型, 二元语言模型, 二元词组语言模型, 二元语法语言模型\n\n2. Explanation: The term "二元组语言模型" (bigram language model) is the most accurate translation because it directly corresponds to the concept of a "bigram," which refers to a pair of consecutive words or tokens in a sequence. The use of "组" (group or pair) in this context is semantically precise, as it captures the essence of the bigram concept in natural language processing. \n\nThe second candidate, "二元语言模型" (binary language model), while it may seem relevant, introduces ambiguity by suggesting a binary model rather than specifically indicating the bigram structure. The third candidate, "二元词组语言模型" (bigram phrase language model), adds unnecessary complexity by including "词组" (phrase), which is not a standard term in this context. Lastly, "二元语法语言模型" (bigram grammar language model) incorrectly implies a focus on grammar rather than the statistical pairing of words, which is not the primary function of a bigram language model. \n\nThus, "二元组语言模型" is the best fit for its clarity and direct alignment with the AI domain-specific usage of bigram language models.', ['binary language model', 'bigram language model', 'bigram language model', 'bigram language model'])
bootstrap resampling Chinese
('自助法重采样', '1. Rank: 自助法重采样, 自助法重抽样, 自助重采样, 自助重抽样\n\n2. Explanation: The term "自助法重采样" is the best fit because it accurately captures the concept of "bootstrap" in the context of statistical resampling. The term "自助法" (literally "self-service method") is widely recognized in the statistical community as referring to the bootstrap method, which is essential for understanding the technique\'s application in statistical significance testing. The inclusion of "法" (method) emphasizes that this is a specific statistical technique, which is crucial for clarity in academic and professional contexts. \n\nThe back translation "bootstrap resampling" aligns perfectly with the original English term, ensuring semantic accuracy. \n\nThe second choice, "自助法重抽样," is also a strong candidate, as it retains the "法" and translates to "bootstrap resampling," but "重采样" (resampling) is more commonly used in statistical literature than "重抽样" (re-sampling), making the first option more contextually appropriate.\n\nThe other two candidates, "自助重采样" and "自助重抽样," lack the "法" component, which diminishes their specificity and clarity in the context of statistical methods. Thus, they are ranked lower.', ['bootstrap resampling', 'self-service resampling', 'bootstrap resampling', 'bootstrap resampling'])
bound variable Chinese
('绑定变量', '1. Rank: 绑定变量, 有界变量, 被绑定变量, 约束变量\n\n2. Explanation: The term "绑定变量" (bind variables) is the best fit for the English term "bound variable" in the context of AI and programming languages. This translation accurately captures the concept of a variable that is bound to a value within a specific scope, which is essential in lambda calculus and functional programming. The back translation "bind variables" aligns closely with the original term, maintaining semantic accuracy.\n\nThe second candidate, "有界变量" (bounded variable), while also relevant, is less commonly used in the context of programming and may imply a different nuance, suggesting a variable with limits rather than one that is bound in a scope. \n\n"被绑定变量" (bound variable) is a direct translation but is less idiomatic in Chinese and may not be as widely recognized in the AI community. \n\nLastly, "约束变量" (constraint variables) introduces a different meaning related to constraints, which is not applicable in this context. Therefore, "绑定变量" is the most contextually appropriate and semantically accurate choice for the term "bound variable" in the AI domain.', ['bind variables', 'Constraint variables', 'bounded variable', 'bound variable'])
burn-in Chinese
('"烧入期"', '1. Rank: "烧入期", "烧入", "预热期", "预热阶段", "预热"\n\n2. Explanation: The term "烧入期" (burn-in period) is the best fit for the AI domain-specific usage because it accurately captures the concept of a period during which initial samples are discarded to allow the model to stabilize before meaningful data collection begins. The back translation "burn-in period" aligns closely with the original English term, maintaining both semantic accuracy and contextual relevance. \n\n"烧入" (burn in) is also a strong candidate, but it lacks the specificity of "期" (period), which is important in this context to denote a defined phase in the sampling process. \n\n"预热期" (warm-up period) and "预热阶段" (warm-up phase) are less suitable because they imply a preparatory phase rather than the specific statistical context of discarding initial iterations, which is critical in the burn-in process. Lastly, "预热" (preheat) is too vague and does not convey the necessary meaning in this context. Thus, "烧入期" is the most contextually appropriate choice.', ['warm-up period', 'burn in', 'warm-up phase', 'burn-in period', 'preheat'])
character n-gram Chinese
('字符n-gram', '1. Rank: 字符n-gram, 字符 n-元组, 字符n元组, 文字 N グラム, 字符, 字符 n-gram\n\n2. Explanation: The term "字符n-gram" is the best fit for the translation of "character n-gram" because it accurately retains the original meaning and structure of the English term. The use of "字符" (character) directly corresponds to the "character" in the English term, and "n-gram" is a well-established term in the AI and natural language processing (NLP) fields, which is commonly used in both English and Chinese contexts. \n\nThe other candidates, such as "字符 n-元组" and "字符n元组," introduce the term "元组" (tuple), which is not semantically accurate in this context, as "n-gram" specifically refers to a sequence of n characters rather than a tuple. "文字 N グラム" translates to "text N-gram," which is less precise since it shifts the focus from characters to text, potentially leading to confusion in the AI domain where character-level analysis is crucial. \n\nOverall, "字符n-gram" maintains both semantic accuracy and contextual fit, making it the most appropriate choice for the AI terminology in question.', ['Character n-gram', 'character n-tuple', 'character n-tuple', 'Text N グラム', 'Character n-gram', 'character'])
clip range Chinese
('"裁剪范围"', '1. Rank: "裁剪范围", "截断范围", "剪辑范围", "剪切范围"\n\n2. Explanation: The term "裁剪范围" (clipping range) is the best fit for the AI domain-specific usage because it accurately conveys the concept of limiting or constraining values within a certain range, which is essential in hyperparameter tuning, particularly in reinforcement learning contexts. The back translation "Cropping range" aligns closely with the intended meaning in the AI context, where "clipping" refers to the process of restricting values to prevent extreme outputs, rather than physically cutting or cropping data. \n\n"截断范围" (cut off range) is a close second, as it also suggests a limitation but may imply a more abrupt termination rather than a controlled adjustment, which is less precise in this context. \n\n"剪辑范围" (clipping range) and "剪切范围" (cutting range) are less suitable because they imply a more physical or visual alteration rather than the mathematical or algorithmic adjustment intended in AI terminology. Thus, "裁剪范围" is the most semantically accurate and contextually appropriate choice.', ['Cut off range', 'Cropping range', 'Cutting range', 'clipping range'])
clique potential Chinese
('"团势能"', '1. Rank: "团势能", "团体势能", "团体潜力", "准 clique 潜力", "子群体势能", "子集潜力", "潜力"\n\n2. Explanation: The term "团势能" (group potential energy) is the best fit for "clique potential" in the AI domain due to its semantic accuracy and contextual relevance. In the context of AI and statistical modeling, "clique" refers to a subset of nodes in a graph that are fully connected, which aligns with the concept of "团" (group) in Chinese. The term "势能" (potential energy) captures the idea of potential in a mathematical or computational sense, which is crucial in the context of transition models. \n\nOther candidates like "团体潜力" (group potential) and "团体势能" (group potential energy) are also relevant but do not convey the same level of specificity as "团势能." The term "准 clique 潜力" (quasi-clique potential) introduces ambiguity by suggesting a variation of the original term, which may not be appropriate in this context. "子群体势能" (subgroup potential energy) and "子集潜力" (subset potential) are less accurate as they imply a smaller or different grouping than what "clique" typically represents in graph theory. Lastly, "潜力" (potential) is too vague and lacks the necessary context to be useful in this specific AI application. Thus, "团势能" stands out as the most precise and contextually appropriate translation.', ['subset potential', 'group potential energy', 'Quasi-clique potential', 'group potential', 'group potential energy', 'subgroup potential energy', 'potential'])
coarse correlated equilibria Chinese
('粗相关均衡', '1. Rank: 粗相关均衡, 粗糙相关均衡, 粗略相关均衡, 粗相关平衡\n\n2. Explanation: The term "粗相关均衡" (coarse correlation equilibrium) is the best fit for the translation of "coarse correlated equilibria" due to its semantic accuracy and contextual relevance in the AI domain. This term retains the essential meaning of "coarse" and "correlated equilibria," which are critical concepts in game theory and AI. The back translation "coarse correlation equilibrium" closely matches the original English term, ensuring clarity and precision in the context of algorithms and regret minimization discussed in the provided context. \n\nThe other candidates, such as "粗糙相关均衡" (rough dependent equilibrium) and "粗略相关均衡" (rough correlated equilibrium), introduce unnecessary variations that could lead to confusion, as "rough" does not accurately capture the intended meaning of "coarse" in this context. Additionally, "粗相关平衡" (crude correlation balance) deviates from the established terminology in game theory, where "均衡" (equilibrium) is the preferred term over "平衡" (balance). Thus, "粗相关均衡" is the most appropriate choice for maintaining both semantic accuracy and contextual fit in the AI domain.', ['rough dependent equilibrium', 'coarse correlation equilibrium', 'rough correlated equilibrium', 'crude correlation balance'])
coarse correlated equilibrium Chinese
('粗相关均衡', '1. Rank: 粗相关均衡, 粗略相关均衡, 粗糙相关均衡, 粗糙相关均衡状态, 粗相关平衡\n\n2. Explanation: The term "粗相关均衡" (coarse correlated equilibrium) is the best fit because it accurately captures the semantic meaning of the original English term while maintaining the correct terminology used in the AI domain. The term "粗" (coarse) directly translates to "coarse," which is essential in this context, as it distinguishes this type of equilibrium from other forms. "相关" (correlated) is also a direct translation, preserving the concept of correlation that is crucial in game theory. The term "均衡" (equilibrium) is a standard term in both English and Chinese for this concept. \n\nThe other candidates, such as "粗糙相关均衡" (rough correlated equilibrium) and "粗略相关均衡" (rough correlated equilibrium), introduce the word "rough," which does not accurately reflect the intended meaning of "coarse" in this context. Additionally, "粗相关平衡" (crude correlation balance) alters the term "equilibrium" to "balance," which can lead to confusion in the context of game theory. Therefore, "粗相关均衡" is the most semantically accurate and contextually appropriate translation for "coarse correlated equilibrium" in the AI domain.', ['coarse correlation equilibrium', 'rough dependent equilibrium', 'Rough correlation equilibrium state', 'rough correlated equilibrium', 'crude correlation balance'])
coarse-to-fine approach Chinese
('"从粗到细方法"', '1. Rank: "从粗到细方法", "粗到细方法", "粗到细的方法"\n\n2. Explanation: The term "从粗到细方法" is the best fit because it accurately captures the essence of the "coarse-to-fine approach" in a way that aligns with common usage in the AI domain. The phrase "从粗到细" (from coarse to fine) clearly indicates a progression or a systematic approach, which is essential in the context of algorithms and image processing. This term also maintains the structure of the original English term, making it easier for readers familiar with the concept to understand its application.\n\nThe second candidate, "粗到细方法," is also a good option but lacks the preposition "从" (from), which adds clarity to the progression implied in the approach. The third candidate, "粗到细的方法," includes an unnecessary particle "的," which makes it sound more awkward and less formal in a technical context. Therefore, "从粗到细方法" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['coarse to fine method', 'Coarse-to-fine method', 'Coarse-to-fine approach'])
constituency parser Chinese
('成分句法分析器', '1. Rank: 成分句法分析器, 成分解析器, 句法成分分析器, 句法分析器\n\n2. Explanation: The term "成分句法分析器" (component syntax analyzer) is the best fit for "constituency parser" because it accurately captures both the semantic meaning and the specific function of the term within the AI domain. "成分" (constituent/component) directly relates to the concept of constituents in linguistic theory, while "句法分析器" (syntax analyzer) clearly indicates that it is a tool used for syntactic analysis. This term maintains the context of parsing in natural language processing, making it more precise for AI applications. \n\nThe second option, "成分解析器" (component parser), is also a good candidate but is slightly less specific as it uses "解析器" (parser) instead of "分析器" (analyzer), which may not convey the same depth of syntactic analysis. The other candidates, "句法成分分析器" (syntactic component analyzer) and "句法分析器" (syntax analyzer), while relevant, do not emphasize the constituency aspect as effectively as "成分句法分析器." Thus, the first term is the most contextually appropriate for the AI domain.', ['component parser', 'component parser', 'syntax analyzer', 'syntactic component analyzer'])
constituency parsing Chinese
('成分句法分析', '1. Rank: 成分句法分析, 短语结构分析, 句法分析, 成分解析\n\n2. Explanation: The term "成分句法分析" (component syntax analysis) is the best fit for "constituency parsing" because it accurately captures the semantic meaning of the original term within the context of natural language processing and linguistics. "成分" (constituent) directly refers to the components of a sentence, which is central to constituency parsing, while "句法分析" (syntax analysis) indicates that it pertains to the structure of sentences. \n\n"短语结构分析" (phrase structure analysis) is also a strong candidate, as it relates closely to the concept of constituency parsing, but it is slightly less precise because it emphasizes phrases rather than constituents, which can lead to ambiguity in certain contexts.\n\n"句法分析" (syntactic analysis) is more general and does not specifically denote the constituency aspect, making it less suitable for the specific term "constituency parsing."\n\n"成分解析" (ingredient analysis) is the least appropriate as it introduces a misleading interpretation of "成分" (constituent) as "ingredient," which does not align with the linguistic context. \n\nOverall, "成分句法分析" is the most contextually accurate and semantically precise translation for the AI domain.', ['component syntax analysis', 'Ingredient analysis', 'Phrase structure analysis', 'syntactic analysis'])
constituent parsing Chinese
('短语结构分析', '1. Rank: 短语结构分析, 成分解析, 成分分析, 成分树\n\n2. Explanation: The term "短语结构分析" (Phrase structure analysis) is the best fit for "constituent parsing" in the AI domain because it directly relates to the concept of analyzing the structure of phrases in a sentence, which is a fundamental aspect of parsing in natural language processing. This term accurately captures the essence of what constituent parsing entails, focusing on the hierarchical structure of phrases rather than merely identifying ingredients or components, which is what "成分解析" (Ingredient analysis) and "成分分析" (Ingredient analysis) imply. \n\nWhile "成分解析" and "成分分析" are close in meaning, they lack the specificity and contextual relevance to parsing in linguistics. "成分树" (ingredient tree) is also less appropriate as it suggests a tree structure but does not convey the parsing process itself. Therefore, "短语结构分析" is the most semantically accurate and contextually fitting term for the AI domain.', ['Ingredient analysis', 'Ingredient analysis', 'ingredient tree', 'Phrase structure analysis'])
cost volume Chinese
('成本体积', '1. Rank: 成本体积, 代价体, 代价体积\n\n2. Explanation: The term "成本体积" (cost volume) is the best fit for the AI domain-specific usage because it accurately captures the concept of "cost" in a financial or resource allocation context, which is essential in AI planning and optimization tasks. The term "体积" (volume) is also appropriate as it suggests a multidimensional space where various costs can be evaluated, aligning well with the context of estimating costs in planning scenarios. \n\nThe second candidate, "代价体" (cost body), while it retains the idea of cost, lacks the specificity of "volume," which is crucial in the context of the provided examples. The term "代价体积" (cost volume) is a direct translation but is less commonly used in the AI field compared to "成本体积." Therefore, "成本体积" is the most semantically accurate and contextually fitting term for the AI terminology in question.', ['cost volume', 'cost body', 'cost volume'])
critic Chinese
('评论家', '1. Rank: 评论家, 评估者, 评论员, 评论器\n\n2. Explanation: The term "评论家" (critic) is the best fit in the context of AI and reinforcement learning because it aligns closely with the concept of a "critic" in actor-critic methods, where the critic evaluates the actions taken by the actor. In this context, "评论家" conveys the idea of someone or something that assesses or critiques the performance of the agent, which is essential in reinforcement learning frameworks. \n\nThe other candidates, such as "评估者" (evaluator), while somewhat relevant, do not carry the same connotation of critical assessment that "评论家" does. "评论员" (commentator) and "评论器" (commentator) are less suitable as they imply a more passive role of commenting rather than actively evaluating or critiquing the actions taken by the agent. Therefore, "评论家" is the most semantically accurate and contextually appropriate term for the AI domain.', ['critic', 'commentator', 'evaluator', 'commentator'])
critic loss Chinese
('"评论家损失"', '1. Rank: "评论家损失", "评论者损失", "评论器损失", "评价损失", "评估损失"\n\n2. Explanation: The term "评论家损失" (critic loss) is the best fit for the AI domain because it accurately captures the concept of a "critic" in reinforcement learning contexts, where a critic evaluates the performance of a policy. The back translation "critic loss" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. \n\nIn contrast, "评论者损失" (commenter loss) and "评论器损失" (commentator loss) do not convey the same technical meaning as "critic" in this context, as they imply a more general or informal role rather than the specific evaluative function of a critic in AI. The terms "评价损失" (evaluate loss) and "评估损失" (assess the damage) are also less suitable because they lack the specific reference to the role of a critic in reinforcement learning, making them less precise in this technical context. Thus, "评论家损失" is the most appropriate choice.', ['commenter loss', 'critic loss', 'commentator loss', 'Assess the damage', 'Evaluate loss'])
critic network Chinese
('评论家网络', '1. Rank: 评论家网络, 评论者网络, 评价网络, 评论器网络, 评估网络\n\n2. Explanation: The term "评论家网络" (critic network) is the best fit because it accurately captures the semantic meaning of "critic" in the context of AI, particularly in reinforcement learning. In this domain, a "critic" refers to a component that evaluates the actions taken by an "actor" and provides feedback to improve the policy. The term "评论家" directly translates to "critic" in a way that aligns with its usage in AI literature, where it implies a role of evaluation and judgment.\n\nThe second choice, "评论者网络," translates to "reviewer network," which is less precise in the AI context, as "reviewer" does not convey the same evaluative role as "critic." The other candidates, such as "评价网络" (rating network) and "评估网络" (evaluate the network), while related, do not specifically denote the duality of actor-critic methods in reinforcement learning, where the "critic" is a distinct entity that assesses the performance of the "actor." Thus, "评论家网络" is the most contextually appropriate and semantically accurate translation.', ['reviewer network', 'Critics Network', 'commentator network', 'Evaluate the network', 'Rating Network'])
cutting plane Chinese
('切割平面', '1. Rank: 切割平面, 切平面, 割平面\n\n2. Explanation: The term "切割平面" (cutting plane) is the most accurate translation in the context of AI and optimization. It directly corresponds to the English term "cutting plane" and retains the specific meaning used in mathematical optimization and algorithmic contexts. The back translation is also accurate, reinforcing its semantic fit. \n\n"切平面" (tangent plane) is misleading because it refers to a different concept in mathematics, specifically related to the tangent of a curve or surface, which does not align with the intended meaning of "cutting plane" in this context. \n\n"割平面" (cutting plane) is less commonly used and may not be recognized in the same way as "切割平面." While it does translate back to "cutting plane," it lacks the established usage and clarity found in the first option. \n\nOverall, "切割平面" is the best fit due to its direct correspondence to the English term, its established usage in the AI domain, and its clarity in conveying the intended meaning.', ['tangent plane', 'cutting plane', 'cutting plane'])
cutting plane algorithm Chinese
('割平面算法', '1. Rank: 割平面算法, 切平面算法, 切割平面算法, 剖切面算法\n\n2. Explanation: The term "割平面算法" (cutting plane algorithm) is the most accurate translation because it directly corresponds to the English term without introducing ambiguity. It retains the specific meaning of "cutting plane" in the context of optimization algorithms, which is crucial in AI and machine learning. The back translation is also accurate, reinforcing its semantic fit. \n\n"切平面算法" (tangent plane algorithm) is less suitable because "切" (cut) can imply a different mathematical concept, leading to potential confusion. "切割平面算法" (cutting plane algorithm) is a close contender but is slightly less common in usage compared to "割平面算法." Lastly, "剖切面算法" (sectional plane algorithm) diverges significantly from the original meaning, as it suggests a different mathematical operation that is not relevant in this context. Thus, "割平面算法" is the best choice for its clarity and direct alignment with the established terminology in the AI domain.', ['tangent plane algorithm', 'cutting plane algorithm', 'Cutting plane algorithm', 'Sectional plane algorithm'])
cutting plane method Chinese
('割平面法', '1. Rank: 割平面法, 切割平面方法, 剖切平面法, 切平面法, 切平面方法\n\n2. Explanation: The term "割平面法" is the best fit for the translation of "cutting plane method" in the AI domain due to its semantic accuracy and established usage in mathematical optimization contexts. The term "割" (cut) directly corresponds to "cutting," and "平面" (plane) accurately reflects the "plane" aspect of the method. This term is widely recognized in the field of optimization and aligns well with the technical context provided.\n\nIn contrast, "切平面方法" translates back to "tangent plane method," which is incorrect in this context as it refers to a different concept in mathematics. "切平面法" also translates back to "tangent plane method," making it unsuitable. "切割平面方法" and "剖切平面法" are less commonly used and may not be as recognizable in the field. Therefore, "割平面法" stands out as the most appropriate choice for its clarity, accuracy, and alignment with established terminology in the AI and optimization domains.', ['cutting plane method', 'tangent plane method', 'Cutting plane method', 'tangent plane method', 'cutting plane method'])
d_model Chinese
('d_模型', '1. Rank: d_模型, 模型维度, d_model, 保留原名\n\n2. Explanation: The term "d_模型" is the best fit because it retains the original structure of the English term while providing a clear indication of its meaning in the context of AI. "模型" translates to "model," which is a common term in machine learning and AI, making it semantically accurate. The back translation to "d_model" confirms that it maintains the original term\'s integrity, which is crucial in technical contexts where precision is key.\n\n"模型维度" (model dimensions) is also a relevant term, as it describes the dimensional aspect of the model, but it lacks the specificity of the original term "d_model." It could lead to ambiguity in certain contexts where "d_model" has a specific defined meaning.\n\nThe term "d_model" as a direct transliteration does not provide any contextual understanding, and "保留原名" (keep original name) is not a translation but rather a suggestion to retain the English term, which is not ideal for a Chinese-speaking audience looking for clarity in technical documentation.\n\nOverall, "d_模型" effectively balances semantic accuracy and contextual fit, making it the most suitable choice in the AI domain.', ['the_model', 'd_model', 'Model dimensions', 'Keep original name)'])
delta kernel Chinese
('"δ核"', '1. Rank: "δ核", "δ 核", "德尔塔核", "δ 核函数", "特征模型"\n\n2. Explanation: The term "δ核" is the best fit for "delta kernel" in the AI domain due to its semantic accuracy and contextual relevance. The use of the Greek letter "δ" (delta) is standard in mathematical and computational contexts, particularly in kernel methods and image processing, where it represents the identity kernel. This term is concise and widely recognized among professionals in the field. \n\nThe second candidate, "δ 核", is also a strong option, as it retains the Greek letter and is similar in meaning, but the space between "δ" and "核" is less common in technical terminology. \n\n"德尔塔核" is a transliteration of "delta" into Chinese, which is less preferred in technical contexts where the Greek letter is more commonly used. \n\n"δ 核函数" introduces the term "函数" (function), which may imply a broader context than intended, as "kernel" in this case refers specifically to the identity kernel rather than a general function. \n\nFinally, "特征模型" translates to "feature model," which is unrelated to the concept of a delta kernel and thus ranks the lowest in terms of semantic accuracy and contextual fit.', ['delta core', 'delta core', 'delta core', 'δ kernel function', 'feature model'])
dependency Chinese
('依存关系', '1. Rank: 依存关系, 依赖关系, 依赖, 依赖性, 子图, 次模性\n\n2. Explanation: The term "依存关系" (dependencies) is the best fit for the AI domain-specific usage in the context provided. In computational linguistics and AI, "依存关系" specifically refers to the relationships between words in a sentence, which aligns perfectly with the context of sentence compression and the classification of word pairs as mentioned in the examples. \n\n"依赖关系" (dependencies) is also a strong candidate, but it is more general and can imply a broader sense of dependency that may not be as precise in the context of linguistic structures. "依赖" (rely) and "依赖性" (dependency) are less suitable as they do not convey the specific relational aspect that "依存关系" captures. \n\nThe terms "子图" (subplot) and "次模性" (submodularity) are irrelevant in this context, as they pertain to different concepts not related to dependencies in language processing. Thus, "依存关系" is the most semantically accurate and contextually appropriate choice for the AI terminology in question.', ['rely', 'Dependencies', 'Dependencies', 'dependency', 'subplot', 'submodularity'])
digamma function Chinese
('"迪伽玛函数"', '1. Rank: "迪伽玛函数", "双伽马函数", "双伽玛函数", "伽马函数的对数导数", "数字伽马函数", "二伽马函数", "双伽马函数 如有其他需要帮助的地方，请告诉我！", "双伽马函数"\n\n2. Explanation: The term "迪伽玛函数" (di gamma function) is the most accurate translation for "digamma function" in the context of AI and mathematics. This term is widely recognized in mathematical literature and specifically refers to the first derivative of the logarithm of the gamma function, which aligns perfectly with the provided context. The back translation "di gamma function" accurately reflects the original English term, maintaining semantic integrity.\n\nThe other candidates, such as "双伽马函数" (double gamma function) and "双伽玛函数" (double gamma function), are misleading as they suggest a different mathematical concept that does not correspond to the digamma function. Similarly, "伽马函数的对数导数" (logarithmic derivative of the gamma function) is technically correct but is less commonly used and may not be immediately recognized as referring to the digamma function. The terms "数字伽马函数" (digital gamma function) and "二伽马函数" (two gamma functions) are incorrect and do not relate to the digamma function at all. \n\nThus, "迪伽玛函数" stands out as the best fit due to its established usage in the mathematical community and its precise alignment with the definition of the digamma function.', ['double gamma function', 'Logarithmic derivative of the gamma function', 'double gamma function', 'Double Gamma Function If you need any other help, please let me know!', 'Logarithmic derivative of the gamma function', 'digital gamma function', 'Two gamma functions', 'di gamma function'])
dim Chinese
('维数', '1. Rank: 维数, 维度, 凸性, 暗淡\n\n2. Explanation: The term "维数" (Dimension) is the best fit for the AI domain-specific usage because it accurately conveys the mathematical concept of "dim" as it relates to the dimensionality of spaces, which is crucial in contexts like camera configurations and mathematical definitions. "维度" (Dimensions) is also a suitable candidate, but "维数" is more commonly used in mathematical contexts to refer specifically to the number of dimensions in a space. The term "凸性" (Convexity) is unrelated to the context provided, and "暗淡" (dim) does not fit the mathematical context at all, as it translates to "dim" in the sense of brightness rather than dimensionality. Therefore, "维数" is the most semantically accurate and contextually appropriate choice.', ['Dimension', 'Dimensions', 'Convexity', 'dim'])
distribution shift Chinese
('分布转移', '1. Rank: 分布转移, 分布偏移, 分布变化\n\n2. Explanation: The term "分布转移" (distribution shift) is the best fit for the AI domain-specific usage because it accurately captures the concept of a shift in the statistical distribution of data, which is critical in machine learning and AI contexts. The term "转移" (shift) conveys the idea of a change in position or state, which aligns well with the notion of distribution shift where the underlying data distribution changes over time or due to different conditions. \n\n"分布偏移" (distribution offset) is a close second, but "偏移" (offset) can imply a less direct or less significant change compared to "转移" (shift), which may not fully encapsulate the abrupt or significant changes often discussed in AI literature. \n\n"分布变化" (distribution changes) is the least suitable as it suggests a more general change without the specific implication of a shift, which can lead to ambiguity in the context of AI where precise terminology is crucial for understanding the nuances of model performance and data behavior. Thus, "分布转移" is the most semantically accurate and contextually appropriate choice.', ['distribution shift', 'Distribution shift', 'Distribution changes'])
distributional similarity Chinese
('分布相似度', '1. Rank: 分布相似度, 分布相似性, 分布式相似性\n\n2. Explanation: The term "分布相似度" (distribution similarity) is the best fit for the context of AI and computational linguistics. The use of "相似度" (similarity degree) is more common in the field of AI, particularly when discussing metrics that quantify similarity between entities or data points. It conveys a sense of measurement, which aligns well with the statistical nature of distributional similarity in AI applications. \n\nOn the other hand, "分布相似性" (distribution similarity) is also a valid translation but is less frequently used in technical contexts compared to "相似度." Lastly, "分布式相似性" (distributed similarity) introduces the term "distributed," which can imply a different concept related to distributed systems or architectures, making it less suitable in this context. Thus, "分布相似度" is the most semantically accurate and contextually appropriate choice.', ['distribution similarity', 'distribution similarity', 'distributed similarity'])
distributionally robust optimization Chinese
('分布稳健优化', '1. Rank: 分布稳健优化, 分布式鲁棒优化, 分布鲁棒优化, 分布式词表示\n\n2. Explanation: The term "分布稳健优化" (Distribution Robust Optimization) is the best fit because it accurately captures the essence of "distributionally robust optimization" in the context of AI and optimization. The word "稳健" (robust) directly corresponds to the concept of robustness in optimization, which is crucial in the AI domain, particularly when discussing worst-case scenarios and risk management. \n\nThe second candidate, "分布式鲁棒优化" (Distributed Robust Optimization), while also a reasonable translation, introduces the term "分布式" (distributed), which may imply a focus on distribution rather than the robustness aspect that is central to the original term. \n\nThe third candidate, "分布鲁棒优化" (Distribution Rod Optimization), is semantically inaccurate due to the incorrect translation of "distributionally" and the nonsensical term "rod" in this context.\n\nLastly, "分布式词表示" (distributed word representation) is completely unrelated to the original term and refers to a different concept in natural language processing, making it the least suitable option. \n\nOverall, "分布稳健优化" maintains the focus on robustness while aligning closely with the intended meaning in the AI context, making it the most appropriate choice.', ['Distributed Robust Optimization', 'Distribution rod optimization', 'distributed word representation', 'Distribution Robust Optimization'])
do-calculus Chinese
('do演算', '1. Rank: do演算, 干预演算, do-演算, Do演算, 操作演算, 操作微 Calculus\n\n2. Explanation: The term "do演算" is the best fit for "do-calculus" because it accurately captures the essence of the original term while maintaining semantic integrity in the context of AI and causal inference. The "do" operator is a fundamental concept introduced by Judea Pearl in causal reasoning, and "演算" (which means "calculus" or "calculation") is a direct translation that conveys the mathematical nature of the term. \n\n"干预演算" (intervention calculus) is also a strong candidate as it explicitly refers to the intervention aspect of the do-operator, which is crucial in causal analysis. However, it is slightly less concise than "do演算," which directly incorporates the term "do," making it more recognizable and aligned with established terminology in the field.\n\nThe other candidates, such as "do-演算" and "Do演算," are less preferable due to their inconsistent capitalization and the use of a hyphen, which is not standard in the context of mathematical terms in Chinese. "操作演算" (operational calculus) and "操作微 Calculus" (operating micro calculus) are less relevant as they do not convey the specific meaning of the do-operator in causal inference, making them less suitable for the AI domain.', ['do-calculus', 'Operating Micro Calculus', 'Operational calculus', 'do calculus', 'intervention calculus', 'Do calculus'])
domain shift Chinese
('领域迁移', '1. Rank: 领域迁移, 领域转移, 领域偏移\n\n2. Explanation: The term "领域迁移" (Domain migration) is the best fit for "domain shift" in the context of AI and machine learning. The term "迁移" (migration) is commonly used in the AI domain to describe the process of transferring knowledge or adapting models from one domain to another, which aligns well with the concept of "domain shift." \n\n"领域转移" (Domain transfer) is also a reasonable candidate, as "转移" (transfer) can imply a similar meaning. However, "迁移" (migration) is more widely recognized in the context of machine learning, particularly in discussions about domain adaptation and transfer learning.\n\nOn the other hand, "领域偏移" (field offset) is less appropriate because "偏移" (offset) does not convey the same sense of adaptation or transformation that is inherent in the concept of "domain shift." It suggests a mere change in position rather than the nuanced process of adapting to a different domain, which is critical in AI applications. Thus, "领域迁移" is the most semantically accurate and contextually fitting translation.', ['Domain migration', 'field offset', 'Domain transfer'])
domain transfer Chinese
('领域迁移', '1. Rank: 领域迁移, 领域转移, 域名转移\n\n2. Explanation: The term "领域迁移" (Domain migration) is the best fit for the AI domain-specific usage of "domain transfer." In the context of machine learning and AI, "迁移" (migration) is commonly used to describe the process of transferring knowledge or models from one domain to another, which aligns well with the concept of "domain transfer." \n\nOn the other hand, "领域转移" (Domain transfer) is also a valid translation but is less commonly used in the AI literature compared to "迁移." The term "域名转移" (Domain name transfer) is not appropriate in this context, as it refers specifically to the transfer of domain names in internet terminology, which is unrelated to the concept of transferring knowledge or models in AI.\n\nThus, "领域迁移" captures the intended meaning more accurately and is more semantically aligned with the established terminology in the AI field.', ['Domain transfer', 'Domain migration', 'Domain name transfer'])
dropout layer Chinese
('"Dropout层"', '1. Rank: "Dropout层", "dropout層", "丢弃层", "随机失活层", "漏失层", "因果推断"\n\n2. Explanation: The term "Dropout层" is the best fit because it retains the original English term "dropout" in its transliterated form, which is widely recognized in the AI and machine learning community. This is important as "dropout" is a specific technique used in neural networks to prevent overfitting, and using the original term helps maintain clarity and consistency in technical discussions. The back translation "Dropout layer" accurately reflects the original English term, ensuring semantic accuracy.\n\nThe second choice, "dropout層", is also a good option as it similarly retains the English term but uses a different character for "layer". However, it is less common than "Dropout层". \n\n"丢弃层" translates to "discard layer", which is a more literal translation but may not be as widely recognized in the AI community. "随机失活层" translates to "random deactivation layer", which is also a descriptive term but lacks the established recognition of "dropout". "漏失层" translates to "lost layer", which is not semantically accurate in the context of dropout layers. Lastly, "因果推断" translates to "causal inference", which is unrelated to the concept of dropout layers and should not be included in this ranking.', ['random deactivation layer', 'discard layer', 'dropout layer', 'Dropout layer', 'causal inference', 'lost layer'])
early stopping Chinese
('早停法', '1. Rank: 早停法, 早期停止, 早停\n\n2. Explanation: The term "早停法" (Early stopping method) is the best fit for the AI domain-specific usage because it explicitly conveys the methodical aspect of early stopping in machine learning contexts. The inclusion of "法" (method) indicates that it is a systematic approach, which aligns well with the technical nature of the term in AI literature. \n\n"早期停止" (early stop) is a direct translation that captures the essence of the term but lacks the specificity of being a method, which is crucial in a technical context. \n\n"早停" (stop early) is the most concise but is too vague and informal for academic or technical writing, as it does not clearly indicate that it refers to a method used to prevent overfitting in model training. \n\nThus, "早停法" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['early stop', 'Early stopping method', 'Stop early'])
element-wise product Chinese
('元素逐项乘积', '1. Rank: 元素逐项乘积, 元素逐项积, 元素逐位乘积, 元素级乘积\n\n2. Explanation: The term "元素逐项乘积" (element-wise product) is the best fit because it accurately captures the concept of performing operations on corresponding elements of two matrices, which is essential in the context of AI and machine learning. The term "逐项" (element-wise) is commonly used in mathematical and computational contexts to denote operations that are applied to each individual element of the matrices. \n\nThe second candidate, "元素逐项积" (element-wise product), is also a good fit but is slightly less common in usage compared to the first. The third candidate, "元素逐位乘积" (element-wise bitwise product), introduces the term "逐位" (bitwise), which is misleading in this context as it suggests a different type of operation that is not relevant to the element-wise multiplication of matrices. Lastly, "元素级乘积" (element-level product) is less precise and not as widely recognized in the AI domain, making it a less favorable choice. Overall, "元素逐项乘积" is the most semantically accurate and contextually appropriate term for the AI domain.', ['Element-wise product', 'Element-wise bitwise product', 'Element-wise product', 'element-level product'])
eligibility trace Chinese
('合格迹线', '1. Rank: 合格迹线, 合格迹, 资格迹, 合格迹象\n\n2. Explanation: The term "合格迹线" (Qualified trace) is the best fit for "eligibility trace" in the context of AI and reinforcement learning. This term accurately captures the concept of a trace that indicates eligibility for updates in policy-gradient algorithms, which is crucial in the context of the provided excerpts. The use of "迹线" (trace line) aligns well with the technical nature of the term, as it suggests a continuous representation of eligibility over time, which is essential in reinforcement learning.\n\nThe other candidates, such as "合格迹" (Pass mark) and "资格迹" (Qualification mark), do not convey the same technical meaning and could lead to confusion, as they imply a more general qualification or standard rather than a specific mechanism used in algorithms. "合格迹象" (Qualification sign) is even less suitable, as it suggests a mere indication rather than a functional component of the algorithm. Therefore, "合格迹线" stands out as the most semantically accurate and contextually appropriate translation for "eligibility trace" in the AI domain.', ['Pass mark', 'Qualified trace', 'Qualification sign', 'Qualification mark'])
embedding size Chinese
('嵌入维度', '1. Rank: 嵌入维度, 嵌入大小, 嵌入尺寸\n\n2. Explanation: The term "嵌入维度" (embedded dimensions) is the best fit for "embedding size" in the AI domain because it accurately captures the concept of dimensionality associated with embeddings, which is a critical aspect in machine learning and deep learning contexts. In AI, "embedding" refers to the representation of data in a continuous vector space, and "维度" (dimensions) directly relates to the number of features or coordinates in that space. \n\nWhile "嵌入大小" (embed size) and "嵌入尺寸" (embed size) are also valid translations, they are less precise in conveying the technical meaning of "size" in this context. "大小" and "尺寸" can imply physical size or volume, which may not resonate with the abstract mathematical concept of dimensionality in embeddings. Therefore, "嵌入维度" is the most semantically accurate and contextually appropriate choice for the AI terminology.', ['Embed size', 'embedded dimensions', 'Embed size'])
epipolar geometry Chinese
('极线几何', '1. Rank: 极线几何, 对极几何, 外极几何, 乖角几何\n\n2. Explanation: The term "极线几何" (epipolar geometry) is the most widely accepted and recognized translation in the field of computer vision and stereo imaging. It accurately captures the concept of epipolar geometry, which is crucial for understanding the relationship between two views in stereo vision. The back translation aligns perfectly with the original English term, ensuring semantic accuracy.\n\n"对极几何" (epipolar geometry) is also a valid translation, but it is less commonly used in the literature compared to "极线几何." The term "外极几何" (Exopolar geometry) is misleading as it introduces a non-standard term that does not accurately reflect the concept. "乖角几何" (Angular geometry) is entirely inappropriate as it does not relate to the concept of epipolar geometry at all.\n\nIn the AI domain, especially in stereo vision and 3D reconstruction, using the most recognized and contextually appropriate term is essential for clear communication and understanding among professionals. Therefore, "极线几何" is the best fit.', ['epipolar geometry', 'epipolar geometry', 'Exopolar geometry', 'Angular geometry'])
episodic return Chinese
('回合回报', '1. Rank: 回合回报, 逐集回报, 事件回报, 情节回报, 偶发性回归\n\n2. Explanation: The term "回合回报" (episodic return) is the best fit because it accurately captures the concept of "episodic" in the context of reinforcement learning, where "回合" (round or episode) refers to a complete sequence of interactions in the environment. This term is semantically accurate and aligns well with the AI domain\'s usage, as it directly relates to the performance measurement of an agent over a complete episode. \n\n"逐集回报" (episode-by-episode reporting) is a close second, but it implies a more granular reporting aspect rather than the overall return, which is the focus in the provided context. The other candidates, such as "事件回报" (event return) and "情节回报" (plot payoff), do not convey the episodic nature of the return and could lead to confusion in the AI context. "偶发性回归" (sporadic regression) is the least suitable as it introduces a different meaning that does not relate to the concept of returns in reinforcement learning.', ['turn return', 'Episode-by-episode reporting', 'plot payoff', 'event return', 'sporadic regression'])
equalized odd Chinese
('均衡赔率', '1. Rank: 均衡赔率, 平衡几率, 均等赔率, 平衡机会, 等化奇数\n\n2. Explanation: The term "均衡赔率" (equilibrium odds) is the best fit for "equalized odd" in the AI domain context. This is because "均衡" (equilibrium) conveys a sense of balance and fairness, which aligns well with the concept of "equalized" in the context of fairness notions in AI. The term "赔率" (odds) is also relevant as it relates to probabilities, which is often a key aspect in discussions of fairness and decision-making algorithms. \n\nThe second choice, "平衡几率" (balanced odds), is also a strong candidate, but "几率" (odds) is slightly less specific than "赔率" (odds) in this context. The other candidates, such as "均等赔率" (even odds) and "平衡机会" (balance opportunity), do not capture the same level of semantic accuracy related to the concept of fairness in AI. "等化奇数" (equalize odd numbers) is less relevant as it introduces a numerical aspect that does not fit the context of fairness in AI algorithms. Thus, "均衡赔率" is the most contextually appropriate and semantically accurate translation.', ['equilibrium odds', 'Balance opportunity', 'Equalize odd numbers', 'balanced odds', 'Even odds'])
error bound Chinese
('误差界限', '1. Rank: 误差界限, 误差界, 错误界限, 错误界\n\n2. Explanation: The term "误差界限" is the best fit for "error bound" in the AI domain due to its semantic accuracy and contextual relevance. In mathematical and statistical contexts, "误差" (error) and "界限" (bound) are commonly used to describe the limits or thresholds of error in predictions or estimations. This term accurately conveys the concept of a boundary within which the error is expected to fall, which is crucial in discussions about data transformation and model performance. \n\nThe second candidate, "误差界," while still relevant, is less precise as it omits "限" (limit), which is essential for conveying the idea of a boundary. The third and fourth candidates, "错误界限" and "错误界," use "错误" (mistake) instead of "误差," which is less appropriate in a technical context where "误差" (error) is the standard term used to refer to deviations from a true value. Thus, "误差界限" is the most contextually accurate and semantically precise choice for the term "error bound" in this AI-related discussion.', ['Error bound', 'error bound', 'error bounds', 'error bound'])
extensive-form game Chinese
('扩展形式博弈', '1. Rank: 扩展形式博弈, 广泛形式博弈, 广泛形式游戏, 广义形式博弈, 展开式博弈\n\n2. Explanation: The term "扩展形式博弈" (extended form game) is the best fit because it accurately captures the concept of "extensive-form game" in the context of game theory. The term "扩展" (extended) directly relates to the idea of expanding the normal-form game to include sequential moves and private information, which is a key characteristic of extensive-form games. \n\n"广泛形式博弈" (broad form game) and "广泛形式游戏" (broad form games) are also reasonable translations, but they do not convey the specific notion of "extensive" as effectively as "扩展". The term "广义形式博弈" (general form game) is less precise, as it suggests a broader category that may not specifically refer to the extensive-form context. Lastly, "展开式博弈" (expanded game) is less commonly used in the AI and game theory literature and may lead to confusion, as it does not directly align with the established terminology. Therefore, "扩展形式博弈" is the most semantically accurate and contextually appropriate choice.', ['extended form game', 'general form game', 'Expanded game', 'extensive form game', 'extensive form games'])
factorization method Chinese
('因子分解方法', '1. Rank: 因子分解方法, 因式分解方法, 分解方法\n\n2. Explanation: The term "因子分解方法" (factorization method) is the best fit for the AI domain-specific usage because it accurately captures the concept of factorization in the context of matrix or tensor decomposition, which is essential in machine learning and data analysis. The back translation "factoring method" aligns closely with the original English term, maintaining the intended meaning. \n\n"因式分解方法" (factoring method) is less suitable because it is more commonly associated with algebraic factorization rather than the broader context of matrix or tensor decomposition used in AI. \n\n"分解方法" (decomposition method) is too general and does not specifically convey the idea of factorization, which is a more specialized term in this context. Therefore, "因子分解方法" is the most semantically accurate and contextually appropriate choice for the AI terminology.', ['factoring method', 'Decomposition method', 'factoring method'])
fairness criterion Chinese
('公平性准则', '1. Rank: 公平性准则, 公平性标准, 公平标准\n\n2. Explanation: The term "公平性准则" (fairness criterion) is the best fit for the AI domain because it accurately captures the concept of a criterion specifically related to fairness in decision-making processes, particularly in algorithms. The use of "准则" (criterion) emphasizes a standard or principle that guides the evaluation of fairness, which aligns well with the context of machine learning and algorithmic fairness. \n\nIn contrast, "公平性标准" (fairness standard) is also a close candidate but leans more towards a general standard rather than a guiding principle, which may not fully convey the specific evaluative aspect implied by "criterion." Lastly, "公平标准" (fair standard) is the least suitable as it lacks the nuance of "性" (related to fairness) and "准则" (criterion), making it less precise in the context of formal evaluations in AI. Thus, "公平性准则" is the most semantically accurate and contextually appropriate choice.', ['fair standard', 'fairness criterion', 'fairness standard'])
faithfulness score Chinese
('"可信度评分"', '1. Rank: "可信度评分", "忠实度评分", "真实性评分", "忠实度得分", "真实性得分"\n\n2. Explanation: The term "可信度评分" (credibility score) is the best fit for "faithfulness score" in the AI domain because it captures the essence of evaluating the reliability and trustworthiness of a summary, which aligns closely with the concept of faithfulness in summarization tasks. In AI, particularly in natural language processing, "faithfulness" refers to how accurately a generated summary reflects the original content without introducing distortions or inaccuracies. \n\n"忠实度评分" (fidelity score) is also a strong candidate, as "忠实" directly translates to "faithful" or "loyal," which can imply a similar meaning in the context of summarization. However, "可信度" emphasizes the aspect of credibility, which is crucial in evaluating AI-generated content.\n\nThe other candidates, "真实性评分" (authenticity score), "忠实度得分" (loyalty score), and "真实性得分" (authenticity score), do not align as closely with the specific context of evaluating summaries in AI. "真实性" (authenticity) suggests a focus on the original nature of the content rather than its reliability or faithfulness to the source, while "忠实度得分" and "忠实度评分" could imply loyalty rather than fidelity in the context of summarization. Thus, "可信度评分" is the most contextually appropriate choice.', ['authenticity score', 'credibility score', 'Fidelity score', 'authenticity score', 'loyalty score'])
fanout Chinese
('扇出', '1. Rank: 扇出, 分支因子, 扩展因子, 扩展度\n\n2. Explanation: The term "扇出" (fan out) is the best fit for the AI domain-specific usage because it directly corresponds to the English term and accurately conveys the concept of how many child nodes a node in a tree structure can have. In the context of beam search, "扇出" effectively captures the idea of branching out from a node, which is crucial for understanding the tree\'s structure and the algorithm\'s performance. \n\n"分支因子" (branching factor) is also a relevant term, but it is more commonly used in general graph theory rather than specifically in AI contexts. While it is semantically accurate, it may not be as widely recognized in the specific context of beam search.\n\n"扩展因子" (expansion factor) and "扩展度" (expansion) are less suitable as they imply a broader concept of expansion rather than the specific branching nature of nodes in a tree. These terms could lead to confusion in the context of AI algorithms, where precise terminology is essential for clarity and understanding. Thus, "扇出" is the most contextually appropriate choice.', ['fan out', 'branching factor', 'expansion factor', 'Expansion'])
forall Chinese
('"全称量词"', '1. Rank: "全称量词", "对于所有", "对所有", "∀", "forall"\n\n2. Explanation: The term "全称量词" (universal quantifier) is the best fit for the AI domain-specific usage because it accurately captures the mathematical and logical context of the term "forall." In formal logic and mathematics, "全称量词" specifically refers to the quantifier that expresses that a property holds for all elements in a given set, which aligns perfectly with the context provided. \n\nThe other candidates, such as "对于所有" (for all) and "对所有" (to all), while they convey a similar meaning, lack the precision and established usage in formal logic that "全称量词" provides. The symbol "∀" is also a valid representation but is less descriptive in terms of semantic clarity for those unfamiliar with the notation. Lastly, "forall" (for all) is a direct transliteration and does not provide the necessary contextual understanding that "全称量词" does in the AI and mathematical domains. Thus, "全称量词" is the most semantically accurate and contextually appropriate choice.', ['universal quantifier', 'for all', '∀', 'for all', 'to all'])
frequent closed itemset Chinese
('频繁闭合项集', '1. Rank: 频繁闭合项集, 频繁闭项集, 高频闭合项集\n\n2. Explanation: The term "频繁闭合项集" (frequent closed itemsets) is the best fit because it accurately captures the semantic meaning of the original English term. The word "频繁" (frequent) directly corresponds to "frequent," and "闭合项集" (closed itemsets) accurately translates "closed itemset." This term is widely used in the AI and data mining domains, particularly in association rule learning and pattern mining, making it contextually appropriate.\n\nThe second candidate, "频繁闭项集," while also correct, omits the word "合" (closed), which can lead to a loss of specificity in the context of itemsets. The term "高频闭合项集" (high frequency closed itemset) introduces the word "高" (high), which is not present in the original term and could mislead the reader regarding the nature of the itemsets being discussed. Therefore, "频繁闭合项集" is the most semantically accurate and contextually fitting translation.', ['high frequency closed itemset', 'frequent closed itemsets', 'frequent closed itemsets'])
gist descriptor Chinese
('要点描述符', '1. Rank: 要点描述符, 要旨描述符, 概要描述符, 大意描述符, 主旨描述符, 主题描述符, 概况描述符\n\n2. Explanation: The term "要点描述符" (gist descriptor) is the best fit because it accurately captures the essence of the English term "gist" in the context of AI and computer vision. "要点" translates to "key points" or "essence," which aligns well with the concept of a "gist" that summarizes the main features of a scene. This term maintains semantic accuracy and is contextually appropriate for describing a feature that identifies significant structures in visual data.\n\nIn contrast, "要旨描述符" (gist descriptor) is also a strong candidate but leans more towards "main point" or "essence," which may not convey the same level of specificity as "要点." The other candidates, such as "概要描述符" (summary descriptor) and "大意描述符" (general descriptor), while related, do not encapsulate the specific AI context of identifying key features in a scene as effectively as "要点描述符." The remaining terms, like "主旨描述符" (subject descriptor) and "主题描述符" (topic descriptor), focus more on thematic elements rather than the visual and structural aspects that "gist descriptor" implies in AI applications. Thus, "要点描述符" is the most contextually relevant and semantically accurate choice.', ['gist descriptor', 'summary descriptor', 'General descriptor', 'subject descriptor', 'profile descriptor', 'Topic descriptor', 'gist descriptor'])
gossip algorithm Chinese
('谣言算法', '1. Rank: 谣言算法, 谣传算法, 八卦算法, 谷歌算法, 算法\n\n2. Explanation: The term "谣言算法" (Rumor Algorithm) is the best fit for the translation of "gossip algorithm" in the AI context. The term "gossip" in this context refers to a decentralized communication protocol where nodes exchange information in a manner similar to how gossip spreads in social networks. "谣言" (rumor) captures this essence effectively, as it conveys the idea of information being shared informally and diffusely among participants.\n\n"谣传算法" (rumor algorithm) is a close second, as it also conveys a similar meaning, but "谣言" is more commonly used in the context of algorithms and networking, making it slightly more appropriate.\n\n"八卦算法" (Bagua algorithm) translates to "gossip" in a more colloquial sense, often associated with idle talk or chatter, which may not fully encapsulate the technical nature of the algorithm in AI. \n\n"谷歌算法" (Google Algorithm) is irrelevant in this context, as it refers specifically to algorithms developed by Google, which does not relate to the concept of gossip algorithms.\n\nFinally, "算法" (algorithm) is too vague and does not provide any specific context or meaning related to the term "gossip." Thus, "谣言算法" is the most semantically accurate and contextually fitting translation for "gossip algorithm" in the AI domain.', ['Bagua algorithm', 'Rumor Algorithm', 'rumor algorithm', 'Google Algorithm', 'algorithm'])
grandparent dependency Chinese
('祖父母依赖关系', '1. Rank: 祖父母依赖关系, 祖父母依赖, 祖父依存关系, 祖父依赖\n\n2. Explanation: The term "祖父母依赖关系" (grandparent dependency) is the best fit because it accurately captures the concept of "grandparent" in a way that is both semantically precise and contextually appropriate for the AI domain. The use of "依赖关系" (dependency) aligns well with the technical terminology used in dependency parsing and machine learning contexts, where relationships between elements are crucial. \n\n"祖父母依赖" (grandparent dependence) is also a strong candidate, but it lacks the explicit mention of "关系" (relationship), which is important in conveying the nature of the dependency in a structured manner. \n\nThe terms "祖父依存关系" (grandfather dependency) and "祖父依赖" (grandfather dependent) are less suitable because they refer specifically to "grandfather" rather than "grandparent," which is broader and more inclusive of both grandparents. In the context of AI and dependency parsing, using the more inclusive term "grandparent" is essential for accurately representing the relationships being modeled.', ['grandfather dependent', 'grandfather dependency', 'grandparent dependence', 'grandparent dependency'])
graph cut algorithm Chinese
('图割算法', '1. Rank: 图割算法, 图切算法, 图切割算法\n\n2. Explanation: The term "图割算法" (graph cut algorithm) is the most semantically accurate and contextually fitting translation for the AI domain. It directly corresponds to the English term without introducing ambiguity. The term "割" (cut) is commonly used in the context of graph theory and algorithms, making it easily recognizable to professionals in the field. \n\nThe second candidate, "图切算法" (graph cut algorithm), is also a valid translation but is less commonly used in the literature compared to "图割算法." While it maintains the core meaning, "切" (cut) is not as standard in this specific context.\n\nThe third candidate, "图切割算法" (graph cutting algorithm), introduces the term "切割" (cutting), which can imply a more dynamic or ongoing process rather than the specific operation of cutting a graph into segments. This could lead to confusion in the context of algorithmic discussions, where precision is crucial.\n\nOverall, "图割算法" is the best fit due to its established usage in the field of graph theory and algorithms, ensuring clarity and accuracy in communication among AI professionals.', ['graph cut algorithm', 'graph cutting algorithm', 'graph cut algorithm'])
greedy inference Chinese
('贪婪推断', '1. Rank: 贪婪推断, 贪心推理, 贪婪推理\n\n2. Explanation: The term "贪婪推断" (greedy inference) is the best fit for the AI domain-specific usage because it accurately captures the meaning of "greedy" in the context of inference processes in machine learning. The word "推断" (inference) is more commonly used in AI literature to refer to the process of drawing conclusions from data, making it semantically precise. \n\nIn contrast, "贪心推理" (greedy reasoning) and "贪婪推理" (greedy reasoning) use "推理" (reasoning), which is less specific to the inference processes typically discussed in AI contexts. While "推理" can be related to reasoning in a broader sense, it does not convey the same technical meaning as "推断" in this context. Therefore, "贪婪推断" is the most contextually appropriate choice for discussing greedy inference in AI.', ['greedy reasoning', 'greedy inference', 'greedy reasoning'])
ground atom Chinese
('基本原子', '1. Rank: 基本原子, 具体原子, 基础原子\n\n2. Explanation: The term "基本原子" (basic atoms) is the best fit for "ground atom" in the AI context due to its semantic accuracy and contextual relevance. In AI and logic programming, "ground" refers to terms that are fully specified without any variables, which aligns well with the notion of "基本" (basic) as it implies a foundational or elemental aspect of the atom. \n\n"具体原子" (concrete atoms) is a reasonable alternative, as "具体" (concrete) also suggests specificity, but it may not convey the foundational aspect as effectively as "基本." \n\n"基础原子" (basic atoms) is less preferred because "基础" (foundation) can imply a broader or more abstract concept, which may not directly correspond to the specific meaning of "ground" in this context. \n\nOverall, "基本原子" captures the essence of "ground atom" in a way that is both semantically accurate and contextually appropriate for the AI domain.', ['basic atoms', 'basic atoms', 'concrete atoms'])
ground set Chinese
('基础集', '1. Rank: 基础集, 基础集合, 基本集合, 实体集合\n\n2. Explanation: The term "基础集" (ground set) is the best fit for the AI domain-specific usage because it accurately conveys the concept of a foundational or base set in mathematical and algorithmic contexts. The term "基础" (base) is commonly used in mathematical literature to denote foundational elements, making it semantically appropriate. "集合" (set) is the standard term for a collection of elements in set theory, which aligns with the context provided. \n\n"基础集合" is also a strong candidate, but it is slightly more verbose and less commonly used than "基础集." "基本集合" (basic set) could imply a more general or elementary set, which may not capture the specific meaning of "ground set" in this context. Lastly, "实体集合" (entity collection) introduces ambiguity, as "实体" (entity) does not directly relate to the concept of a ground set in algorithms and could mislead readers into thinking of a different context. Thus, "基础集" is the most semantically accurate and contextually fitting translation.', ['base set', 'base set', 'basic set', 'Entity collection'])
grounded language learning Chinese
('基于实际场景的语言学习', '1. Rank: 基于实际场景的语言学习, 基于实际环境的语言学习, 基于情境的语言学习, 基于实境的语言学习, 以实物为基础的语言学习, 以视频为基础的语言学习, 实体语言学习, 有根语言学习, 基于实际的语言学习\n\n2. Explanation: The top-ranked translation, "基于实际场景的语言学习" (Language learning based on actual scenarios), is the best fit because it captures the essence of "grounded language learning" in the context of AI and language acquisition. This term emphasizes the importance of learning language in relation to real-world contexts and scenarios, which aligns closely with the idea of acquiring language in an unsupervised manner, similar to how children learn. \n\nThe other candidates, while related, either focus too much on specific aspects (like "video" or "objects") or do not convey the same depth of contextual learning that "基于实际场景的语言学习" does. For instance, "基于情境的语言学习" (context-based language learning) is also a strong candidate but is slightly less specific than "基于实际场景的语言学习," which implies a more immersive and practical approach to language learning. \n\nOverall, the top choice effectively reflects the AI domain\'s emphasis on contextual and situational learning, making it the most semantically accurate and contextually appropriate translation.', ['Video-based language learning', 'context-based language learning', 'Object-based language learning', 'entity language learning', 'Rooted language learning', 'Language learning based on actual environment', 'Context-based language learning', 'Real-life language learning', 'Language learning based on actual scenarios'])
grounded supervision Chinese
('基于实际环境的监督', '1. Rank: 基于实际环境的监督, 基于实际场景的监督, 基于情境的监督, 基于实境的监督, 基于实际的监督, 实体化监督, 有根监督, 以实物为基础的监督, 物理监督, 根监督\n\n2. Explanation: The term "基于实际环境的监督" (Supervision based on actual circumstances) is the best fit for "grounded supervision" in the AI context because it captures the essence of grounding in a specific context or environment, which is crucial in AI applications. This translation emphasizes the importance of situational awareness and context, aligning well with the idea of supervision that is informed by real-world conditions. \n\nOther candidates like "基于情境的监督" (situation-based supervision) and "基于实际场景的监督" (Supervision based on actual scenarios) are also relevant but slightly less precise, as they may not fully encompass the broader implications of "grounded" in terms of actual environments. \n\nTerms like "实体化监督" (physical supervision) and "有根监督" (Rooted supervision) diverge from the intended meaning, as they suggest a more literal or physical interpretation that does not align with the AI context of supervision based on contextual understanding. Thus, "基于实际环境的监督" stands out as the most semantically accurate and contextually appropriate translation.', ['reality-based supervision', 'situation-based supervision', 'physical-based supervision', 'physical supervision', 'Rooted supervision', 'Supervision based on actual circumstances', 'reality-based supervision', 'Supervision based on actual scenarios'])
hand pose estimation Chinese
('手部姿态估计', '1. Rank: 手部姿态估计, 手部姿态估计, 手势估计, 手势姿态估计\n\n2. Explanation: The best fit for "hand pose estimation" is "手部姿态估计" (Hand pose estimation) because it accurately captures the specific meaning of the term in the context of AI and computer vision. "手部" (hand) specifies that the estimation is focused on the hand, while "姿态" (pose) refers to the position and orientation of the hand\'s joints. This term is semantically precise and aligns well with the technical context of estimating the configuration of hand joints in depth images. \n\nThe other candidates, such as "手势估计" (Gesture estimation) and "手势姿态估计" (Gesture pose estimation), are less accurate because they imply a focus on gestures rather than the specific pose of the hand, which is crucial in the context of the provided information. "手部姿势估计" (Hand posture estimation) is also a close contender, but "姿态" (pose) is more commonly used in technical contexts related to estimation tasks in AI, making "手部姿态估计" the most appropriate choice.', ['Gesture estimation', 'Hand pose estimation', 'Gesture pose estimation', 'Hand pose estimation'])
head word Chinese
('头词', '1. Rank: 头词, 主要词, 中心词, 主词\n\n2. Explanation: The term "头词" (head word) is the most accurate translation in the context of AI and dependency parsing. In computational linguistics, "头词" specifically refers to the main word in a phrase that governs the other words (modifiers) in a dependency structure. This aligns perfectly with the provided context, where the head word is crucial for understanding the relationships between words in a parse. \n\n"主要词" (main word) is a close second, but it lacks the specific technical connotation that "头词" carries in the field of linguistics. "中心词" (center word) is less commonly used in this context and may not convey the same precise meaning as "head word." Lastly, "主词" (subject) is misleading, as it typically refers to the grammatical subject of a sentence rather than the head word in a dependency structure. Therefore, "头词" is the best fit for its semantic accuracy and contextual relevance in AI terminology.', ['main word', 'Head word', 'center word', 'subject'])
human-in-the-loop Chinese
('"人在回路中"', '1. Rank: "人在回路中", "人工干预/人在回路", "人在环中", "人在回路", "人机协作", "人类在环", "人工参与的系统"\n\n2. Explanation: The term "人在回路中" (back translation: "people in the loop") is the best fit for the AI domain-specific usage of "human-in-the-loop." This translation accurately captures the essence of the original term, which refers to systems that incorporate human feedback or intervention in a looped process. The phrase "人在回路中" maintains the concept of a continuous interaction between humans and machines, which is central to the idea of human-in-the-loop systems. \n\nThe second candidate, "人工干预/人在回路," while also relevant, introduces the notion of "intervention," which may imply a more passive role for humans rather than the active participation suggested by "human-in-the-loop." The other candidates, such as "人在环中" and "人在回路," lack the specificity of the loop concept, and terms like "人机协作" (human-machine collaboration) and "人工参与的系统" (human-involved system) do not convey the same iterative feedback mechanism that is critical in AI contexts. Therefore, "人在回路中" is the most semantically accurate and contextually appropriate translation.', ['people in the loop', 'people in the circle', 'people in circuit', 'Human intervention/human in the loop', 'Human-machine collaboration', 'human beings around', 'human-involved system'])
hyponymy Chinese
('下义词关系', '1. Rank: 下义词关系, 下位词关系, 下位关系, 下义关系\n\n2. Explanation: The term "下义词关系" (hyponym relationship) is the best fit because it accurately captures the semantic meaning of "hyponymy" in the context of linguistic hierarchies. "下义词" directly translates to "hyponym," which refers to a word that is more specific than a general term (hypernym). This term is widely recognized in linguistic and AI contexts, particularly in relation to WordNet and ontology discussions. \n\nThe second choice, "下位词关系" (hyponym relation), is also a good candidate but is slightly less common than "下义词关系." "下位关系" (subordinate relationship) is less precise as it could imply a broader range of relationships beyond just hyponymy. Lastly, "下义关系" (Hypothetical relationship) is misleading as it does not convey the specific nature of hyponymy and could confuse readers in the AI domain. Thus, "下义词关系" is the most semantically accurate and contextually appropriate term for this specific usage in AI terminology.', ['hyponym relationship', 'subordinate relationship', 'hyponym relation', 'Hypothetical relationship'])
image-to-image translation Chinese
('图像到图像翻译', '1. Rank: 图像到图像翻译, 图像到图像的翻译, 图像到图像转换\n\n2. Explanation: The term "图像到图像翻译" (image to image translation) is the most semantically accurate and contextually fitting translation for the AI domain. This term directly corresponds to the English term without any alteration, maintaining the original meaning and intent. The use of "翻译" (translation) in this context aligns with the established terminology in machine learning and computer vision, where "translation" refers to the process of transforming one image into another while preserving certain features or characteristics.\n\nThe second candidate, "图像到图像的翻译," while still accurate, adds an unnecessary grammatical element ("的") that does not enhance the meaning and may sound slightly awkward in a technical context. \n\nThe third candidate, "图像到图像转换" (image to image conversion), uses "转换" (conversion), which can imply a broader range of transformations that may not specifically capture the nuances of the translation process in AI. In the context of image processing, "translation" is a more precise term that conveys the specific task of generating a new image based on the input image, which is crucial in applications like image super-resolution and in-painting mentioned in the context. Thus, "图像到图像翻译" is the best fit for its clarity and adherence to established terminology in the field.', ['image to image translation', 'image to image conversion', 'Image to image translation'])
in-context demonstration Chinese
('上下文示例', '1. Rank: 上下文示例, 上下文演示, 上下文中的示例, 情境演示\n\n2. Explanation: The term "上下文示例" (Contextual example) is the best fit for "in-context demonstration" because it accurately captures the essence of providing examples within a specific context, which aligns well with the AI domain\'s focus on demonstrating how models perform tasks based on contextual information. The back translation "Contextual example" maintains the semantic integrity of the original term, emphasizing the role of examples in understanding the model\'s behavior.\n\n"上下文演示" (Contextual presentation) is a close second, but "演示" (presentation) may imply a more passive display rather than the active role of providing examples, which is crucial in AI demonstrations. "上下文中的示例" (Examples in context) is also accurate but slightly less concise than "上下文示例." Lastly, "情境演示" (Situational demonstration) diverges from the specific AI context, as "情境" (situational) does not convey the same technical meaning as "上下文" (in-context), making it the least suitable option.', ['Contextual presentation', 'Contextual example', 'Examples in context', 'Situational demonstration'])
inception score Chinese
('开创评分', '1. Rank: 开创评分, 创新评分, 初始得分, 开始分数\n\n2. Explanation: The term "开创评分" (Create a score) is the best fit for "inception score" because it captures the essence of the term in the context of AI and image generation. The word "开创" implies a sense of innovation and originality, which aligns well with the concept of measuring the quality and diversity of generated images. In the AI domain, particularly in Generative Adversarial Networks (GANs), the inception score is used to evaluate how well a model generates new, diverse, and high-quality images, which is reflected in the term "开创评分."\n\n"创新评分" (Innovation score) is a close second, as it also conveys a sense of creativity and newness, but it lacks the direct connection to the concept of "inception" as effectively as "开创评分." \n\n"初始得分" (Initial score) and "开始分数" (Start score) are less suitable because they do not convey the specific meaning of "inception" in this context. They suggest a preliminary or starting point rather than the evaluative and innovative aspects that the inception score represents in AI. Thus, "开创评分" is the most contextually appropriate choice.', ['Create a score', 'initial score', 'Innovation score', 'start score'])
infinite-horizon Chinese
('"无限时域"', '1. Rank: "无限时域", "无限时间范围", "无限期", "无限视界", "无限视野"\n\n2. Explanation: The term "无限时域" (infinite time domain) is the best fit for the AI domain-specific usage of "infinite-horizon." This is because "时域" (time domain) accurately captures the concept of a temporal framework in which decisions are made over an indefinite period, which is essential in the context of Markov decision processes (MDPs). The back translation aligns well with the original term, maintaining the focus on time as a critical factor in decision-making processes.\n\nThe second candidate, "无限时间范围" (unlimited time range), is also a reasonable choice but is slightly less precise than "时域," as "范围" (range) can imply a more general or less structured concept than "域" (domain), which is more commonly used in mathematical and computational contexts.\n\nThe other candidates, "无限期" (indefinitely), "无限视界" (infinite horizons), and "无限视野" (infinite vision), do not convey the specific temporal aspect required in this context. "无限期" lacks the necessary focus on time as a domain, while "视界" and "视野" suggest a visual or perceptual scope rather than a temporal one, making them less suitable for the technical context of MDPs.', ['infinite time domain', 'indefinitely', 'Infinite vision', 'Unlimited time range', 'Infinite horizons'])
intrinsic parameter Chinese
('内参', '1. Rank: 内参, 内在参数, 内参数\n\n2. Explanation: The term "内参" (back translated as "internal reference") is the best fit for the AI domain, particularly in the context of computer vision and camera calibration. In the field of computer vision, "内参" is a widely accepted abbreviation for "intrinsic parameters," which refers to the internal characteristics of a camera, such as focal length and optical center. This term is commonly used in both academic literature and practical applications, making it semantically accurate and contextually appropriate.\n\nThe second candidate, "内在参数" (back translated as "intrinsic parameters"), while accurate, is less commonly used in the field and may not resonate as well with practitioners familiar with the standard terminology. The third candidate, "内参数" (back translated as "internal parameters"), is also less precise and could lead to confusion, as it does not specifically convey the concept of intrinsic camera parameters as effectively as "内参." Therefore, "内参" is the most contextually fitting and semantically accurate choice in the AI domain.', ['internal reference', 'intrinsic parameters', 'Internal parameters'])
iteratively reweighted least square Chinese
('迭代重加权最小二乘法', '1. Rank: 迭代重加权最小二乘法, 迭代重加权最小二乘, 迭代加权最小二乘法\n\n2. Explanation: The term "迭代重加权最小二乘法" is the best fit because it accurately captures the concept of "iteratively reweighted least squares" in a way that aligns with established terminology in the AI and statistical domains. The use of "重加权" (reweighted) directly corresponds to the "reweighted" in the English term, ensuring semantic accuracy. Additionally, the inclusion of "法" (method) at the end of the term is common in mathematical and statistical terminology in Chinese, making it more recognizable and appropriate for academic contexts. The back translation of this term as "iterative reweighted least squares" is also precise, maintaining the original meaning and context. \n\nThe second candidate, "迭代重加权最小二乘", lacks the "法" which diminishes its clarity as a method or technique, while the first candidate, "迭代加权最小二乘法", introduces a slight deviation by using "加权" instead of "重加权", which could lead to confusion regarding the specific nature of the weighting process involved in the method. Thus, "迭代重加权最小二乘法" stands out as the most contextually and semantically accurate choice.', ['iterative weighted least squares', 'iterative reweighted least squares', 'iterative reweighted least squares'])
k nearest neighbor Chinese
('k 最近邻', '1. Rank: k 最近邻, k最近邻, k近邻, 最近邻\n\n2. Explanation: The term "k 最近邻" is the best fit because it maintains both semantic accuracy and contextual relevance in the AI domain. The inclusion of "k" explicitly indicates the number of neighbors being considered, which is crucial in the context of the k-nearest neighbors algorithm. The term "最近邻" alone lacks the specificity of the "k" parameter, which is essential for understanding the algorithm\'s functionality. "k最近邻" is also a valid option, but the lack of a space may affect readability in some contexts. "k近邻" omits the "最近" (nearest) descriptor, which is important for conveying the concept of proximity in the algorithm. Lastly, "最近邻" is too vague as it does not specify the "k" aspect, making it less suitable for technical discussions in AI. Therefore, "k 最近邻" is the most accurate and contextually appropriate choice.', ['k nearest neighbor', 'k nearest neighbor', 'nearest neighbor', 'k nearest neighbor'])
k-Center Chinese
('k-中心问题', '1. Rank: k-中心问题, k-中心, k中心\n\n2. Explanation: The term "k-中心问题" (k-center problem) is the best fit because it explicitly includes the word "问题" (problem), which clarifies that it refers to a specific problem in the context of optimization and facility location. This aligns well with the AI domain, where precise terminology is crucial for understanding the nature of the problem being addressed. The inclusion of "问题" helps to contextualize the term within mathematical and algorithmic discussions, making it more informative for readers familiar with the field.\n\nThe term "k-中心" (k-center) is also a strong candidate, as it retains the core concept of the k-center without the additional context of it being a problem. However, it may lack the specificity needed in academic or technical discussions. \n\nLastly, "k中心" (k center) is the least preferred option because it omits the hyphen and does not convey the same level of clarity as the other two options. The lack of a hyphen can lead to confusion, as it may not be immediately clear that it refers to a specific mathematical concept rather than a general term. \n\nOverall, "k-中心问题" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['k-center', 'k center', 'k-center problem'])
k-best list Chinese
('k-最佳列表', '1. Rank: k-最佳列表, k-best 列表, k最优列表\n\n2. Explanation: The term "k-最佳列表" is the best fit because it accurately captures the meaning of "k-best list" in the context of AI and computational linguistics. The use of "最佳" (optimal) directly corresponds to the concept of selecting the best candidates from a list, which is essential in parsing and ranking tasks. The back translation to "k-best list" maintains the original term\'s integrity, ensuring clarity in communication within the AI domain. \n\nThe second candidate, "k-best 列表," while also accurate, does not convey the same level of specificity regarding the quality of the candidates as "最佳" does. It simply translates "best" as "best" without the added nuance of "optimal." \n\nThe third candidate, "k最优列表," translates to "k optimal list," which, although it conveys a similar meaning, is less commonly used in the context of AI and may not align as closely with established terminology in the field. The term "最优" (optimal) is more formal and less frequently used in practical applications compared to "最佳." \n\nOverall, "k-最佳列表" is the most contextually appropriate and semantically accurate choice for the AI domain.', ['k-best list', 'k-best list', 'k optimal list'])
k-best parsing Chinese
('k-最佳解析', '1. Rank: k-最佳解析, k-best 解析, k最优解析\n\n2. Explanation: The term "k-最佳解析" is the best fit because it accurately captures the meaning of "k-best parsing" in the context of AI and computational linguistics. The term "最佳" translates to "best," which aligns well with the concept of selecting the top k derivations in parsing. This term maintains the semantic integrity of the original English term while also being contextually appropriate for the AI domain. \n\nThe second candidate, "k-best 解析," retains the English term "k-best," which may be useful in some contexts, but it lacks the clarity and specificity that "最佳" provides. The third candidate, "k最优解析," translates to "k-optimal analysis," which could imply a different meaning, as "最优" (optimal) may suggest a focus on the best possible solution rather than the top k solutions. Therefore, "k-最佳解析" is the most semantically accurate and contextually fitting choice for the AI terminology in question.', ['k-best resolution', 'k-best analysis', 'k-optimal analysis'])
k-hop neighbor Chinese
('k-hop邻居', '1. Rank: k-hop邻居, k-hop 邻居, k跳邻居, k-跳邻居\n\n2. Explanation: The term "k-hop邻居" is the best fit because it maintains the original English term "k-hop" intact while directly translating "neighbor" to "邻居," which is the standard term used in graph theory and AI contexts. This combination preserves the semantic meaning and context of the term, making it easily understandable for Chinese-speaking audiences familiar with AI terminology. The use of "k-hop" as a prefix is consistent with how similar terms are used in the field, ensuring clarity and precision. The other candidates either introduce unnecessary variations (like "k-跳邻居" or "k跳邻居") or separate the components in a way that could lead to confusion, as seen in "k-hop 邻居." Thus, "k-hop邻居" is the most contextually appropriate and semantically accurate choice.', ['k-hop neighbor', 'k-hop neighbor', 'k-hop neighbor', 'k-hop neighbor'])
k-means clustering algorithm Chinese
('k-均值聚类算法', '1. Rank: k-均值聚类算法, k均值聚类算法, k-means 聚类算法\n\n2. Explanation: The first translation, "k-均值聚类算法," is the best fit because it accurately retains the structure of the original English term while also being semantically clear in Chinese. The inclusion of the hyphen ("k-") aligns with the common usage in the AI domain, where "k-means" is often referred to with a hyphen in both English and Chinese contexts. This term is widely recognized in academic and technical literature, making it the most appropriate choice for clarity and consistency. The second option, "k均值聚类算法," omits the hyphen, which may lead to slight confusion, as it does not follow the conventional representation. The third option, "k-means 聚类算法," while still understandable, is less common and may not be as readily recognized in the AI community. Therefore, "k-均值聚类算法" is the most semantically accurate and contextually fitting translation.', ['k-means clustering algorithm', 'k-means clustering algorithm', 'k-means clustering algorithm'])
k-nearest neighbor Chinese
('k最近邻', '1. Rank: k最近邻, k-最近邻, k近邻, k 最近邻\n\n2. Explanation: The term "k最近邻" is the best fit for the translation of "k-nearest neighbor" because it accurately captures the semantic meaning of the original term while maintaining a clear and concise structure. The use of "最近邻" directly translates to "nearest neighbor," which is a standard term in machine learning and AI contexts. This term is widely recognized in the field, making it the most appropriate choice for professionals and researchers.\n\nThe second candidate, "k-最近邻," is also a good option as it retains the hyphen, which is commonly used in the English term. However, the hyphen is not necessary in Chinese, and its inclusion may not be as familiar to all readers. \n\nThe third candidate, "k近邻," omits the "最近" (nearest) part, which could lead to ambiguity, as "近邻" alone may not convey the same specificity as "最近邻." \n\nLastly, "k 最近邻" includes an unnecessary space, which is not standard in Chinese writing and could detract from its professional appearance. \n\nOverall, "k最近邻" is the most semantically accurate and contextually appropriate term for the AI domain.', ['k nearest neighbor', 'k nearest neighbor', 'k-nearest neighbor', 'k nearest neighbor'])
k-nearest neighbor classifier Chinese
('k最近邻分类器', '1. Rank: k最近邻分类器, k-最近邻分类器, k近邻分类器, k 最近邻分类器\n\n2. Explanation: The term "k最近邻分类器" is the best fit because it accurately captures the semantic meaning of "k-nearest neighbor classifier" while maintaining a clear and concise structure that is commonly used in the AI domain. The use of "最近邻" (nearest neighbor) is standard in machine learning literature, making it easily recognizable to practitioners. The absence of a space in "k最近邻" also aligns with typical formatting in technical contexts, where terms are often presented without unnecessary spaces for clarity and brevity. \n\nThe second choice, "k-最近邻分类器," is also a strong candidate, as it retains the hyphen from the English term, which can be useful for clarity in some contexts. However, the hyphen is less common in Chinese technical terminology, making it slightly less preferable than the first option.\n\nThe third option, "k近邻分类器," omits the "最近" (recent) part, which could lead to a loss of specificity in the term, making it less accurate semantically. \n\nLastly, "k 最近邻分类器" includes an unnecessary space, which is not typical in technical writing and could detract from its professional appearance. Thus, while all translations are understandable, "k最近邻分类器" stands out as the most contextually appropriate and semantically accurate choice.', ['k nearest neighbor classifier', 'k nearest neighbor classifier', 'k-nearest neighbor classifier', 'k nearest neighbor classifier'])
k-nearest neighbor graph Chinese
('k最近邻图', '1. Rank: k最近邻图, k-最近邻图, k近邻图, k 最近邻图\n\n2. Explanation: The term "k最近邻图" is the best fit because it accurately preserves the semantic meaning of the original English term "k-nearest neighbor graph" while maintaining a clear and concise structure. The use of "最近邻" directly translates to "nearest neighbor," which is a standard term in the AI and machine learning domains, ensuring that it is easily understood by professionals in the field. \n\nThe second candidate, "k-最近邻图," is also a good option as it includes the hyphen, which aligns with the original English term\'s formatting. However, the hyphen is not commonly used in Chinese terminology, making "k最近邻图" slightly more preferable.\n\nThe third candidate, "k近邻图," omits the "最近" (nearest) part, which could lead to ambiguity, as "近邻" alone may not convey the same specificity as "最近邻." \n\nLastly, "k 最近邻图" includes an unnecessary space, which is not standard in technical terminology and could lead to confusion. \n\nOverall, "k最近邻图" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['k nearest neighbor graph', 'k nearest neighbor graph', 'k-nearest neighbor graph', 'k nearest neighbor graph'])
labeled example Chinese
('已标注示例', '1. Rank: 已标注示例, 标注示例, 标记样本, 标记示例, 标注样本\n\n2. Explanation: The term "已标注示例" (back translated as "Labeled example") is the best fit because it accurately conveys the concept of an example that has been labeled, which is crucial in the context of machine learning and AI. The prefix "已" (meaning "already") emphasizes that the labeling process has been completed, aligning well with the context where labeled examples are used for training models. \n\n"标注示例" (back translated as "Annotation example") is also a strong candidate, as it suggests that the examples have been annotated, but it lacks the explicit indication that the labeling is complete, which is important in the context of the provided text.\n\nThe other candidates, such as "标记样本" (Markup samples) and "标记示例" (Markup example), introduce ambiguity with the term "标记" (markup), which is less commonly used in the AI domain to refer to labeled data. "标注样本" (Label samples) is somewhat accurate but does not convey the same level of specificity as "已标注示例." \n\nOverall, "已标注示例" is the most semantically accurate and contextually appropriate term for the AI domain, as it clearly indicates that the examples have been labeled and are ready for use in the learning process.', ['Markup example', 'Labeled samples', 'Label samples', 'Labeled example', 'Annotation example'])
labeled training datum Chinese
('"有标签的训练数据"', '1. Rank: "有标签的训练数据", "标注训练数据", "带标签的训练数据", "有标注的训练数据", "标记训练数据"\n\n2. Explanation: The term "有标签的训练数据" is the best fit because it accurately captures the meaning of "labeled training datum" in the context of AI and machine learning. The phrase "有标签" directly translates to "labeled," which is essential in distinguishing the data as being annotated for training purposes. This term is widely used in the AI domain, making it semantically precise and contextually appropriate. \n\n"标注训练数据" is also a strong candidate, as "标注" means "annotated," which is synonymous with "labeled." However, it is slightly less common in usage compared to "有标签的训练数据." \n\n"带标签的训练数据" translates to "training data with labels," which is accurate but less concise than the top choice. \n\n"有标注的训练数据" is similar to "标注训练数据" but is less commonly used in the AI context. \n\nLastly, "标记训练数据" translates to "marked training data," which could imply a different meaning and is less precise in the context of labeled data used for training machine learning models. Thus, it ranks the lowest.', ['Labeled training data', 'Label training data', 'Labeled training data', 'Labeled training data', 'Label training data'])
layer-wise learning rate decay Chinese
('层级学习率衰减', '1. Rank: 层级学习率衰减, 分层学习率衰减, 逐层学习率衰减\n\n2. Explanation: The term "层级学习率衰减" (layer-wise learning rate decay) is the best fit because it accurately captures the hierarchical nature of the learning rate adjustment process in deep learning models. The term "层级" (hierarchical) is commonly used in AI literature to describe structures that have multiple levels or layers, which aligns well with the concept of "layer-wise" in the original English term. \n\n"分层学习率衰减" (layer-wise learning rate decay) is also a good candidate, but "分层" (layered) is less frequently used in the context of deep learning compared to "层级." \n\n"逐层学习率衰减" (layer-by-layer learning rate decay) is less preferred because it implies a more sequential or step-by-step approach, which may not fully convey the intended meaning of adjusting the learning rate across layers simultaneously. \n\nOverall, "层级学习率衰减" is the most semantically accurate and contextually appropriate term for the AI domain.', ['Hierarchical learning rate decay', 'Hierarchical learning rate decay', 'Layer-by-layer learning rate decay'])
lazy grounding Chinese
('惰性实例化', '1. Rank: 惰性实例化, 懒惰归约, 延迟基础, 延迟归属, 懒惰归纳, 懒惰实例化\n\n2. Explanation: The term "惰性实例化" (lazy instantiation) is the best fit for "lazy grounding" in the AI context because it accurately captures the concept of "lazy" as it relates to computational processes, where instantiation refers to the creation of instances or representations of rules or data. This term aligns well with the idea of deferring certain computations until they are necessary, which is a key aspect of lazy grounding. \n\nThe other candidates, such as "懒惰归约" (lazy reduction) and "延迟基础" (delay basics), do not convey the same level of specificity regarding the computational context. "懒惰归纳" (lazy induction) and "懒惰实例化" (lazy instantiation) are also relevant, but "惰性实例化" is more commonly used in AI literature to describe similar concepts, making it the most semantically accurate and contextually appropriate choice.', ['lazy instantiation', 'lazy reduction', 'Delay basics', 'delayed vesting', 'lazy induction', 'lazy instantiation'])
lexical head Chinese
('词汇中心语', '1. Rank: 词汇中心语, 词汇中心, 词汇主干, 词汇主语\n\n2. Explanation: The term "词汇中心语" (lexical head) is the best fit because it accurately captures the concept of a "head" in linguistic terms, specifically in the context of dependency grammar. The term "中心语" (center word) is commonly used in Chinese linguistics to refer to the main word in a phrase that determines the syntactic structure, aligning closely with the English term "head." The back translation "vocabulary center" is somewhat misleading as it does not convey the specific linguistic function of a head. \n\nThe second candidate, "词汇中心," while similar, lacks the specificity of "语" (word) which is crucial in this context. "词汇主干" (vocabulary stem) and "词汇主语" (lexical subject) are less appropriate as they introduce different meanings that do not align with the concept of a head in dependency grammar. "主干" (stem) suggests a foundational element rather than a syntactic head, and "主语" (subject) is a specific grammatical role that does not encompass the broader meaning of "head." Thus, "词汇中心语" is the most semantically accurate and contextually appropriate choice in the AI domain.', ['vocabulary center', 'vocabulary center', 'vocabulary stem', 'lexical subject'])
local minimizer Chinese
('局部最小化子', '1. Rank: 局部最小化子, 局部最小化器, 局部极小化器, 局部极小值点\n\n2. Explanation: The term "局部最小化子" (local minimizer) is the best fit because it directly corresponds to the English term "local minimizer" in both semantic accuracy and contextual usage within the AI domain. In optimization contexts, "minimizer" refers to an entity (point or algorithm) that minimizes a function, which aligns well with the term "局部最小化子." \n\nThe term "局部最小化器" (local minimizer) is also a strong candidate, but it is less commonly used in the context of mathematical optimization compared to "局部最小化子." The term "局部极小化器" (local minimizer) is similar but introduces the word "极小," which can imply a more general sense of minimization rather than the specific context of local optimization. Lastly, "局部极小值点" (local minimum point) is less accurate because it refers to the point itself rather than the concept of minimization, which is crucial in the context of optimization algorithms. Thus, "局部最小化子" is the most precise and contextually appropriate choice.', ['local minimum point', 'local minimizer', 'local minimizer', 'local minimizer'])
long-range dependency Chinese
('长程依赖', '1. Rank: 长程依赖, 长距离依赖, 对数几率\n\n2. Explanation: The term "长程依赖" (long range dependence) is the best fit for the AI domain, particularly in the context of natural language processing and machine learning. This term accurately captures the concept of "long-range dependency," which refers to the ability of a model to understand and remember relationships between elements that are far apart in a sequence. The back translation "long range dependence" aligns closely with the original English term, maintaining both semantic accuracy and contextual relevance.\n\nOn the other hand, "长距离依赖" (long distance dependency) is a less precise translation. While it conveys a similar idea, "distance" can imply a more physical or spatial interpretation, which may not fully encapsulate the abstract nature of dependencies in sequence processing tasks. \n\nLastly, "对数几率" (log odds) is unrelated to the concept of long-range dependency and is therefore not a suitable candidate in this context. \n\nOverall, "长程依赖" is preferred due to its direct alignment with the terminology used in AI literature, ensuring clarity and precision in communication within the field.', ['long distance dependency', 'long range dependence', 'Log odds'])
loop closure Chinese
('回环闭合', '1. Rank: 回环闭合, 回环检测, 闭环, 闭环检测, 回路闭合, 对数几率\n\n2. Explanation: The term "回环闭合" (loop closure) is the most accurate translation for the AI domain, particularly in the context of visual odometry and SLAM (Simultaneous Localization and Mapping). This term directly corresponds to the English term "loop closure," which refers to the process of recognizing a previously visited location in a mapping context, allowing for corrections in the estimated trajectory. \n\n"回环检测" (loopback detection) is a close second, as it emphasizes the detection aspect, but it does not fully capture the concept of closing the loop, which is crucial in the context of SLAM. \n\n"闭环" (closed loop) and "闭环检测" (closed loop detection) are also relevant but are less specific than "回环闭合" in the context of the provided sentences. "回路闭合" (loop closed) is less commonly used in the AI domain and may lead to confusion. Lastly, "对数几率" (log odds) is unrelated to the context of loop closure and should not be considered in this ranking. \n\nOverall, "回环闭合" is the best fit due to its direct semantic alignment with the English term and its established usage in the AI and robotics fields.', ['Loopback detection', 'loop closure', 'Loop closed', 'closed loop', 'Closed loop detection', 'Log odds'])
lottery ticket hypothesis Chinese
('彩票假说', '1. Rank: 彩票假说, 彩票票假设, 彩票票据假设\n\n2. Explanation: The term "彩票假说" (lottery hypothesis) is the best fit for the translation of "lottery ticket hypothesis" because it captures the essence of the term while maintaining semantic accuracy. The word "假说" (hypothesis) is a commonly used term in scientific and academic contexts, making it more appropriate for the AI domain. \n\nIn contrast, "彩票票假设" (lottery ticket hypothesis) is a direct translation that retains the original structure but is less fluid in Chinese. It may sound awkward to native speakers and does not align as well with the conventions of scientific terminology in Chinese. \n\nThe term "彩票票据假设" (lottery ticket assumption) introduces "票据" (ticket) which is less relevant in this context and could lead to confusion, as it implies a different meaning related to receipts or documents rather than the conceptual framework of the hypothesis.\n\nOverall, "彩票假说" is concise, contextually appropriate, and aligns well with the established terminology in the AI field, making it the most suitable choice.', ['lottery ticket hypothesis', 'lottery hypothesis', 'lottery ticket hypothesis'])
machine-in-the-loop Chinese
('机器在环', '1. Rank: 机器在环, 人机协作, 机器参与循环\n\n2. Explanation: The term "机器在环" (machine in the loop) is the most semantically accurate translation for the English term "machine-in-the-loop." It retains the original structure and meaning, directly reflecting the concept of integrating machines into a feedback loop with human operators. This is crucial in the AI domain, where the interaction between humans and machines is a key focus. \n\n"人机协作" (human-machine collaboration) is a broader term that implies cooperation between humans and machines but does not capture the specific "loop" aspect of the original term, which is essential in contexts where feedback and iterative processes are involved. \n\n"机器参与循环" (machines participate in the cycle) is less accurate as it suggests a more passive role for machines, which does not align with the active engagement implied by "machine-in-the-loop." \n\nOverall, "机器在环" is the best fit as it accurately conveys the intended meaning in the context of AI and maintains the specific terminology used in the field.', ['machine in the loop', 'Human-machine collaboration', 'Machines participate in the cycle'])
masked token Chinese
('"掩码令牌"', '1. Rank: "掩码令牌", "掩蔽词元", "掩蔽标记", "掩蔽的标记", "掩蔽标记", "遮蔽标记", "掩码标记"\n\n2. Explanation: The term "掩码令牌" (mask token) is the best fit for the AI domain because it accurately reflects the concept of a "masked token" used in Transformer-based language models. The word "掩码" (mask) directly corresponds to the action of masking in the context of machine learning, while "令牌" (token) is a commonly accepted term in AI and natural language processing to refer to the basic units of text. \n\nThe back translation of "掩码令牌" as "mask token" maintains the original meaning and context, making it semantically accurate. In contrast, "掩蔽词元" (masking token) is also a strong candidate, but "词元" (word element) is less commonly used in the AI field compared to "令牌" (token). Other candidates like "被遮蔽的标记" (obscured mark) and "遮蔽标记" (mask mark) introduce ambiguity and do not align well with the established terminology in AI, as they do not convey the specific action of masking in the context of language models. Thus, "掩码令牌" is the most contextually appropriate and semantically accurate choice.', ['obscured mark', 'masking mark', 'mask mark', 'mask token', 'mask mark', 'masking token'])
masking function Chinese
('遮蔽函数', '1. Rank: 遮蔽函数, 掩蔽函数, 掩码函数\n\n2. Explanation: The term "遮蔽函数" is the best fit for "masking function" in the AI domain due to its semantic accuracy and contextual relevance. In the context of AI and machine learning, "masking" often refers to the process of hiding or obscuring certain data points or features during model training or evaluation. The term "遮蔽" directly conveys the idea of covering or obscuring, which aligns well with the concept of masking in this context.\n\nThe second candidate, "掩蔽函数," is also a strong contender as it translates back to "masking function" and retains a similar meaning. However, "掩蔽" is less commonly used in the AI context compared to "遮蔽," which may lead to slight ambiguity in understanding.\n\nThe third candidate, "掩码函数," translates back to "mask function," which deviates from the original term "masking function." The term "掩码" is more associated with "mask" in a general sense and does not capture the functional aspect implied by "masking function." Therefore, it is less suitable for the specific AI context presented. \n\nOverall, "遮蔽函数" is the most contextually appropriate and semantically accurate choice for the term "masking function" in the AI domain.', ['Masking function', 'mask function', 'masking function'])
matroid constraint Chinese
('"拟阵约束"', '1. Rank: "拟阵约束", "matroid约束", "依赖于母体的约束", "依赖于母体约束", "独立集约束", "矩阵约束", "伴随约束"\n\n2. Explanation: The term "拟阵约束" is the best fit for "matroid constraint" because it accurately captures the concept of a matroid in combinatorial optimization and graph theory, which is essential in the context of submodular maximization. The term "拟阵" is the standard Chinese translation for "matroid," widely recognized in academic literature, ensuring semantic accuracy. The back translation "matroid constraints" aligns perfectly with the original English term, maintaining contextual integrity. \n\nThe second candidate, "matroid约束," while also correct, uses a transliteration of "matroid," which may not be as widely understood in the Chinese academic community compared to "拟阵." The other candidates, such as "依赖于母体的约束" and "独立集约束," do not accurately convey the specific mathematical structure and properties associated with matroids, making them less suitable in this context. "矩阵约束" (matrix constraints) is misleading as it refers to matrices rather than matroids, and "伴随约束" (adjoint constraints) does not relate to the concept at all. Thus, "拟阵约束" stands out as the most contextually and semantically appropriate choice.', ['matroid constraints', 'Parent-dependent constraints', 'Depends on parent constraints', 'matrix constraints', 'independent set constraints', 'Adjoint constraints', 'matroid constraints'])
maximum a posteriori Chinese
('最大后验概率', '1. Rank: 最大后验概率, 最大后验, 后验最大化, 凸替代\n\n2. Explanation: The term "最大后验概率" (maximum posterior probability) is the most accurate translation for "maximum a posteriori" in the context of AI and statistics. This term directly reflects the statistical concept of posterior probability, which is central to Bayesian inference. The back translation accurately conveys the original meaning, maintaining the focus on the probabilistic aspect of the term.\n\nThe second choice, "最大后验" (maximum posterior), is also a strong candidate as it succinctly captures the essence of the term without losing its meaning. However, it is slightly less explicit than "最大后验概率," which clearly indicates that it refers to a probability.\n\n"后验最大化" (posterior maximization) is less preferred because it emphasizes the action of maximizing rather than the concept of posterior probability itself, which is not the primary focus of the term "maximum a posteriori."\n\nLastly, "凸替代" (convex substitution) is unrelated to the original term and does not fit within the context of Bayesian statistics or AI, making it the least suitable option.', ['maximum posterior probability', 'posterior maximization', 'maximum posterior', 'convex substitution'])
maximum a posteriori estimation Chinese
('最大后验估计', '1. Rank: 最大后验估计, 后验最大化估计, 最大后验概率估计, 凸替代\n\n2. Explanation: The term "最大后验估计" (maximum a posteriori estimate) is the best fit for the context of AI and statistics because it directly corresponds to the English term "maximum a posteriori estimation" without introducing unnecessary complexity. This term is widely recognized in the statistical and machine learning communities, making it semantically accurate and contextually appropriate. \n\nThe second candidate, "后验最大化估计" (posterior maximization estimate), while also relevant, emphasizes the maximization aspect rather than the estimation aspect, which can lead to confusion in the context of probabilistic models. The third candidate, "最大后验概率估计" (maximum posterior probability estimate), introduces the word "概率" (probability), which, although related, shifts the focus slightly away from the estimation process itself. \n\nLastly, "凸替代" (convex substitution) is unrelated to the term in question and should not be considered in this context. Therefore, "最大后验估计" is the most accurate and contextually fitting translation for the term in the AI domain.', ['posterior maximization estimate', 'maximum a posteriori estimate', 'maximum posterior probability estimate', 'convex substitution'])
mean average precision Chinese
('平均精度均值', '1. Rank: 平均精度均值, 平均精确度均值, 平均平均精度\n\n2. Explanation: The term "平均精度均值" (average precision mean) is the best fit for "mean average precision" in the AI domain. This translation accurately captures the semantic meaning of the original term, as it directly translates to "average precision" and includes "mean," which is essential in the context of evaluating the performance of models in machine learning and information retrieval. \n\nThe second candidate, "平均精确度均值" (average accuracy mean), while also relevant, introduces the term "accuracy," which can lead to confusion since "precision" and "accuracy" are distinct metrics in the context of AI and machine learning. \n\nThe first candidate, "平均平均精度" (average average accuracy), is redundant and does not convey the intended meaning effectively, as it repeats "average" unnecessarily and does not align with the standard terminology used in the field. \n\nThus, "平均精度均值" is the most semantically accurate and contextually appropriate translation for "mean average precision."', ['average average accuracy', 'average accuracy mean', 'average precision mean'])
mean field Chinese
('平均场', '1. Rank: 平均场, 均值场, 均场, 几何变换\n\n2. Explanation: The term "平均场" (mean field) is the most accurate translation for the context of AI and statistical physics. It directly corresponds to the English term "mean field" and is widely recognized in the field. The back translation is also accurate, maintaining the original meaning. \n\n"均值场" (mean value field) is a close second, as it conveys a similar concept, but "均值" (mean value) is less commonly used in this specific context compared to "平均" (average). \n\n"均场" (average field) is less precise because it does not capture the specific statistical context implied by "mean field." \n\nFinally, "几何变换" (geometric transformation) is completely unrelated to the term "mean field" and should not be considered a candidate in this context. \n\nOverall, "平均场" is the best fit due to its semantic accuracy, contextual relevance, and established usage in the AI domain.', ['mean field', 'mean field', 'average field', 'geometric transformation'])
mean reciprocal rank Chinese
('"均值倒排秩"', '1. Rank: "均值倒排秩", "均值倒排排名", "平均倒数排名", "平均倒排排名", "要旨描述符"\n\n2. Explanation: The term "均值倒排秩" (mean inverted rank) is the best fit for the translation of "mean reciprocal rank" because it accurately captures the statistical nature of the term "mean" (均值) and the concept of ranking (倒排秩) in a way that aligns with AI and information retrieval contexts. The use of "倒排" (inverted) is appropriate here, as it relates to ranking systems where items are evaluated based on their relevance or score. \n\nThe second choice, "均值倒排排名," is also a good candidate but slightly less precise because "排名" (ranking) is more general and does not convey the specific statistical meaning of "reciprocal." The third option, "平均倒数排名," introduces "倒数" (reciprocal) but does not maintain the statistical context as effectively as "倒排秩." \n\nThe last two options, "平均倒排排名" and "要旨描述符," are less relevant as they either misinterpret the term or stray from the specific AI context. "要旨描述符" (gist descriptor) is unrelated to ranking and does not fit the context at all. Thus, "均值倒排秩" stands out as the most semantically accurate and contextually appropriate translation.', ['Average inverted ranking', 'mean inverted rank', 'average last rank', 'Mean inverted ranking', 'gist descriptor'])
medoid Chinese
('"中位文档"', '1. Rank: "中位文档", "中心文档", "中心样本", "中心点", "中心对象", "中位点"\n\n2. Explanation: The term "中位文档" (back translated as "median document") is the best fit for the AI domain-specific usage of "medoid." In clustering contexts, particularly in machine learning and data analysis, a "medoid" refers to the most centrally located object in a cluster, which is often represented by the document that minimizes the average dissimilarity to all other documents in that cluster. The term "中位文档" accurately captures this concept by emphasizing the document aspect and the centrality implied by "medoid." \n\nWhile "中心文档" (central document) is also a strong candidate, it lacks the specific statistical connotation of "median," which is crucial in distinguishing the medoid from other central tendencies like the mean. The other candidates, such as "中心样本" (center sample) and "中心点" (center point), are less precise in the context of document clustering, as they do not specifically denote a document or imply the statistical properties associated with a medoid. Therefore, "中位文档" is the most semantically accurate and contextually appropriate choice.', ['center point', 'median document', 'center sample', 'central object', 'central document', 'midpoint'])
membership inference attack Chinese
('成员推断攻击', '1. Rank: 成员推断攻击, 成员身份推断攻击, 会员推断攻击, 成员推理攻击\n\n2. Explanation: The term "成员推断攻击" (Member inference attack) is the best fit for the following reasons:\n\n- **Semantic Accuracy**: "成员推断攻击" directly translates to "member inference attack," which closely aligns with the original English term "membership inference attack." The use of "成员" (member) accurately reflects the concept of membership in the context of machine learning models, where the focus is on whether an input belongs to the training dataset.\n\n- **Contextual Fit**: In the AI domain, "membership" is a key term that refers to the inclusion of data points in a training set. The term "推断" (inference) is also appropriate as it conveys the idea of deducing or inferring information about the membership status of data points based on model outputs.\n\n- **Back Translation Accuracy**: The back translation of "成员推断攻击" as "Member inference attack" maintains the original meaning and context, ensuring that it is easily understood by both English and Chinese-speaking audiences familiar with AI terminology.\n\nThe other candidates, while they may be understandable, do not capture the specific nuances of the term as effectively. For instance, "会员推断攻击" (Membership inference attack) could imply a more commercial or membership-based context, which is not the focus here. "成员推理攻击" (Member inference attack) uses "推理" (reasoning) instead of "推断" (inference), which is less precise in this context. "成员身份推断攻击" (Member identity inference attack) adds unnecessary complexity by including "身份" (identity), which is not explicitly part of the original term and could lead to confusion.', ['Member inference attack', 'Membership inference attack', 'Member inference attack', 'Membership inference attack'])
memory capacity Chinese
('记忆容量', '1. Rank: 记忆容量, 内存容量, 存储容量, 记忆单元\n\n2. Explanation: The term "记忆容量" (memory capacity) is the best fit for the context provided, as it directly translates the English term while maintaining the semantic focus on memory in the context of AI and neural networks. In the AI domain, "记忆" (memory) specifically refers to the ability of a system to retain and utilize information, which aligns perfectly with the discussions of synaptic complexity and memory budget in the provided context. \n\n"内存容量" (internal memory capacity) is a close second, but it typically refers to the physical memory in computing systems, which may not fully capture the conceptual aspect of memory in neural networks. "存储容量" (storage capacity) is more general and often relates to data storage rather than cognitive memory, making it less suitable. Lastly, "记忆单元" (memory unit) refers to a component or unit of memory rather than the overall capacity, which does not fit the context of discussing the capacity itself. Thus, "记忆容量" is the most contextually appropriate choice.', ['Memory capacity', 'storage capacity', 'memory capacity', 'memory unit'])
memory cell Chinese
('"内存单元"', '1. Rank: "内存单元", "记忆单元", "存储单元", "记忆容量", "In the context of memo"\n\n2. Explanation: The term "内存单元" (memory unit) is the best fit for the AI domain-specific usage of "memory cell." In the context of AI and computing, "内存" specifically refers to memory in the sense of RAM or volatile memory, which is directly related to the concept of a memory cell in neural networks and other computational models. \n\n"记忆单元" (memory unit) is also a valid translation, but it is less commonly used in technical contexts compared to "内存单元." The term "存储单元" (storage unit) is broader and can refer to any type of storage, not specifically memory in the computational sense, making it less precise. "记忆容量" (memory capacity) refers to the amount of memory available rather than the concept of a memory cell itself, which is not contextually accurate. Lastly, "In the context of memo" is irrelevant and does not pertain to the technical meaning of "memory cell." \n\nThus, "内存单元" is the most semantically accurate and contextually appropriate term for the AI domain.', ['storage unit', 'memory unit', 'memory unit', 'int and context of memo', 'memory capacity'])
metric score Chinese
('"度量分数"', '1. Rank: "度量分数", "指标得分", "指标评分", "指标分数", "评价指标分数", "评分指标"\n\n2. Explanation: The term "度量分数" (metric score) is the best fit for the AI domain-specific usage because it directly translates the concept of "metric" in a way that is widely recognized in the context of evaluation metrics in machine translation (MT) and other AI applications. The term "度量" (metric) is commonly used in technical discussions to refer to measurement standards, while "分数" (score) clearly indicates a numerical value assigned based on certain criteria. \n\nIn contrast, "指标得分" (indicator score) and "指标评分" (metric score) are less precise because "指标" (indicator) can imply a broader range of meanings and may not specifically convey the idea of a quantitative measure as effectively as "度量". The other candidates, such as "评价指标分数" (evaluation index score) and "评分指标" (rating indicators), are overly complex or vague for the context, as they introduce additional terms that may not be necessary for understanding the specific concept of a metric score in AI evaluations. \n\nOverall, "度量分数" maintains semantic accuracy and contextual fit, making it the most suitable choice for the given context.', ['indicator score', 'Metric score', 'metric score', 'indicator score', 'Evaluation index scorec', 'Rating indicators'])
misclassification error Chinese
('误分类错误', '1. Rank: 误分类错误, 错分类误差, 错分误差, 错误分类率, 错误分类错误\n\n2. Explanation: The term "误分类错误" is the best fit for "misclassification error" because it directly translates to "misclassification error" in a way that retains the original meaning and context. The term "误分类" (misclassification) accurately captures the concept of incorrectly classifying data points, while "错误" (error) emphasizes the nature of the mistake. This term is widely recognized in the AI and machine learning community, making it semantically accurate and contextually appropriate.\n\nThe second choice, "错分类误差," also conveys the idea of misclassification but is slightly less common in usage compared to "误分类错误." "错分误差" is similar but may not be as widely accepted in the AI domain. "错误分类率" translates to "misclassification rate," which, while related, refers specifically to the proportion of misclassifications rather than the error itself, making it less accurate in this context. Lastly, "错误分类错误" is redundant and less clear, as it combines "error" with "classification" and "error" again, which does not add value to the term.', ['misclassification error', 'Misclassification rate', 'misclassification error', 'misclassification error', 'misclassification error'])
misclassification loss Chinese
('误分类损失', '1. Rank: 误分类损失, 错分类损失, 错误分类损失, 错分损失\n\n2. Explanation: The term "误分类损失" is the best fit for "misclassification loss" because it accurately captures the semantic meaning of the original English term within the context of AI and machine learning. "误" (mis) directly translates to "mis" in "misclassification," indicating an error in classification. The term "分类" (classification) is also a direct match, and "损失" (loss) correctly conveys the concept of loss in this context. \n\n"错分类损失" is a close second, as it also means "misclassification loss," but "错" (wrong) is slightly less precise than "误" in this technical context. "错误分类损失" translates to "error classification loss," which introduces ambiguity since "错误" (error) can imply a broader range of mistakes, not specifically tied to classification. Lastly, "错分损失" translates to "misdiagnosis loss," which diverges from the original meaning, as "诊断" (diagnosis) is not relevant in the context of classification tasks in machine learning. Thus, "误分类损失" is the most semantically accurate and contextually appropriate term for the AI domain.', ['misclassification loss', 'misclassification loss', 'misdiagnosis loss', 'misclassification loss'])
model checking Chinese
('模型检查', '1. Rank: 模型检查, 模型检测, 模型检验\n\n2. Explanation: The term "模型检查" (model checking) is the most accurate translation for the AI domain, as it directly corresponds to the established terminology used in formal verification and computer science. "检查" (checking) conveys the process of verifying the properties of a model, which is central to the concept of model checking. \n\n"模型检测" (model detection) is a close second, as it also relates to the idea of examining a model, but it can imply a broader range of activities that may not specifically align with the formal verification context. \n\n"模型检验" (model testing) is the least accurate in this context, as "检验" (testing) typically refers to a different process that involves evaluating a model\'s performance or functionality rather than verifying its correctness against specifications. In the context of AI and formal methods, "model checking" is a well-defined term, making "模型检查" the best fit.', ['Model checking', 'Model testing', 'Model checking'])
moment matching Chinese
('矩匹配', '1. Rank: 矩匹配, 矩量匹配, 时刻匹配, 瞬时匹配\n\n2. Explanation: The term "矩匹配" (moment matching) is the best fit because it accurately reflects the mathematical and statistical context in which "moment" is used, particularly in AI and machine learning. In this context, "moment" refers to statistical moments (like mean, variance, etc.), which are crucial for understanding distributions and approximations. The term "矩" (matrix) is often associated with mathematical concepts, making it semantically appropriate for the AI domain. \n\n"矩量匹配" (moment matching) is also a strong candidate, as it retains the concept of "moment" but adds "量" (quantity), which may not be necessary and could introduce ambiguity. \n\n"时刻匹配" (moment matching) and "瞬时匹配" (instantaneous matching) are less suitable because they imply a temporal aspect ("时刻" means "time point" and "瞬时" means "instantaneous"), which does not align with the statistical meaning of "moment" in this context. Thus, they do not capture the intended meaning as effectively as "矩匹配."', ['moment matching', 'instantaneous matching', 'moment matching', 'moment matching'])
multi-view stereo Chinese
('多视图立体', '1. Rank: 多视图立体, 多视图立体视觉, 多视角立体, 多视角立体视觉, 多视图立体匹配\n\n2. Explanation: The term "多视图立体" (multi-view stereo) is the best fit for the AI domain-specific usage because it accurately captures the essence of the original English term without unnecessary embellishments. In the context of computer vision and 3D reconstruction, "多视图立体" directly refers to the technique of reconstructing 3D scenes from multiple 2D images taken from different viewpoints, which aligns perfectly with the technical focus of the provided context. \n\nThe other candidates, such as "多视图立体视觉" and "多视角立体视觉," introduce the word "视觉" (vision), which, while related, adds a layer of abstraction that is not present in the original term. This could lead to confusion in a technical context where precision is crucial. "多视角立体" is also a valid term but is less commonly used in the literature compared to "多视图立体." Lastly, "多视图立体匹配" (multi-view stereo matching) is too specific, as it refers to a particular aspect of the broader multi-view stereo process, which is not what the original term encompasses. Thus, "多视图立体" stands out as the most semantically accurate and contextually appropriate choice.', ['multi-view stereo vision', 'multi-view stereo', 'multi-view stereo', 'Multi-view stereo matching', 'Multi-view stereo vision'])
multiclass hinge loss Chinese
('多分类铰链损失', '1. Rank: 多分类铰链损失, 多类别铰链损失, 多类合页损失, 多类铰链损失\n\n2. Explanation: The term "多分类铰链损失" (Multi-class hinge loss) is the best fit because it accurately captures the specific meaning of "multiclass" in the context of machine learning, where "分类" (classification) is commonly used to refer to problems involving multiple classes. This term aligns well with the established terminology in the AI domain, ensuring clarity and precision. \n\n"多类别铰链损失" (Multi-class hinge loss) is also a strong candidate, as "类别" (categories) is a valid synonym for "classes," but "分类" is more widely recognized in the context of classification tasks. \n\nThe terms "多类合页损失" (Multiple types of hinge losses) and "多类铰链损失" (Multiple types of hinge losses) are less accurate because they do not convey the specific concept of "multiclass" as it relates to classification tasks in machine learning. Instead, they suggest a broader interpretation that could imply various types of hinge losses rather than a singular focus on multiclass classification. Thus, they rank lower in terms of semantic accuracy and contextual fit.', ['Multiple types of hinge losses', 'Multiple types of hinge losses', 'Multi-class hinge loss', 'Multi-class hinge loss'])
multivariate Gaussian Chinese
('多变量高斯分布', '1. Rank: 多变量高斯分布, 多元高斯, 多变量高斯\n\n2. Explanation: The term "多变量高斯分布" (Multivariate Gaussian distribution) is the best fit for the context provided. This translation is semantically accurate as it explicitly includes "分布" (distribution), which is essential in statistical contexts to denote that we are discussing a probability distribution rather than just a mathematical function. The back translation accurately reflects the original English term, maintaining the full meaning and context.\n\nThe second candidate, "多元高斯" (Multivariate Gaussian), is also a strong contender, as it is commonly used in the AI and statistical communities. However, it lacks the explicit mention of "分布," which could lead to ambiguity in certain contexts where the distinction between a distribution and other mathematical constructs is crucial.\n\nThe third candidate, "多变量高斯" (Multivariable Gaussian), is less accurate because "多变量" (multivariable) is not the standard term used in statistics and AI. The term "多元" (multivariate) is preferred in this domain, as it specifically refers to multiple random variables and their joint distribution, which is the focus in the context of Gaussian distributions.\n\nOverall, "多变量高斯分布" is the most precise and contextually appropriate choice for the AI domain.', ['Multivariate Gaussian distribution', 'Multivariate Gaussian', 'Multivariable Gaussian'])
multivariate normal Chinese
('多变量正态分布', '1. Rank: 多变量正态分布, 多元正态, 多变量正态\n\n2. Explanation: The term "多变量正态分布" (multivariate normal distribution) is the best fit because it is the most semantically accurate and contextually appropriate translation in the AI domain. It explicitly includes "分布" (distribution), which is crucial in statistical contexts, as it clarifies that we are discussing a probability distribution rather than just a general concept of "normal" or "multivariate." \n\nThe second candidate, "多元正态" (multivariate normal), is also a valid term but lacks the explicit mention of "分布," which may lead to ambiguity in certain contexts. The third candidate, "多变量正态" (multivariate normal), is less commonly used and may not be as immediately recognizable to those familiar with statistical terminology. \n\nIn the context of AI and statistics, precision in terminology is essential, and "多变量正态分布" provides the clearest and most accurate representation of the concept being discussed.', ['multivariate normal distribution', 'multivariate normal', 'multivariate normal'])
mutexe Chinese
('互斥锁', '1. Rank: 互斥锁, 互斥执行, 互斥, 互斥体, 互斥量\n\n2. Explanation: The term "互斥锁" (mutex lock) is the best fit for the context of AI and computer science, particularly in relation to concurrent programming and resource management. In this context, "mutex" refers specifically to a synchronization primitive that is used to manage access to shared resources in a multi-threaded environment. The back translation "mutex lock" accurately reflects this meaning, as it is a widely recognized term in both English and Chinese programming communities.\n\nThe second candidate, "互斥执行" (mutually exclusive execution), while relevant, is more of a descriptive phrase rather than a standard term. It conveys the idea of mutual exclusion but does not specifically refer to the locking mechanism that "互斥锁" does. The other candidates, such as "互斥" (mutually exclusive), "互斥体" (mutex), and "互斥量" (mutex), are either too vague or not commonly used in the context of mutexes in programming. Therefore, "互斥锁" stands out as the most semantically accurate and contextually appropriate choice for the AI domain.', ['mutex lock', 'Mutually exclusive execution', 'mutually exclusive', 'mutex', 'mutex'])
n-best list Chinese
('n-best 列表', '1. Rank: n-best 列表, n-最佳列表, n最佳列表, 前n个最佳列表, n-最优列表, N-最佳列表, N最佳列表\n\n2. Explanation: The term "n-best 列表" is the best fit because it retains the original English term "n-best" while clearly indicating that it refers to a list, which is crucial in the context of AI and machine translation. This term is widely recognized in the AI community, particularly in natural language processing and speech recognition, where "n-best" refers to the top n hypotheses or outputs generated by a model. \n\nThe other candidates, such as "n-最佳列表" and "n最佳列表," are also acceptable but slightly less precise because they do not maintain the "n-best" structure as clearly as "n-best 列表." The term "前n个最佳列表" introduces ambiguity by suggesting a ranking rather than a list of candidates, which may not align with the technical usage in AI. "n-最优列表" and "N-最佳列表" also deviate from the standard terminology used in the field. \n\nOverall, "n-best 列表" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['n-best list', 'top n best list', 'n-optimal list', 'n-best list', 'nbest list', 'N best list', 'N-best list'])
n-gram feature Chinese
('n-gram特征', '1. Rank: n-gram特征, n元特征, n-元特征, 特征\n\n2. Explanation: The term "n-gram特征" is the best fit because it directly translates the English term "n-gram feature" while retaining the specific terminology used in the AI and computational linguistics domains. The term "n-gram" is a well-established concept in these fields, referring to a contiguous sequence of n items from a given sample of text or speech. By using "n-gram特征," the translation maintains both semantic accuracy and contextual relevance, ensuring that readers familiar with AI terminology will immediately understand the concept being discussed. \n\nIn contrast, "n元特征" and "n-元特征" introduce ambiguity by using "元" (which can mean "ary" in a mathematical context) but do not clearly convey the specific meaning of "n-gram." The term "特征" alone is too vague and lacks the specificity needed for accurate understanding in this context. Therefore, "n-gram特征" is the most precise and contextually appropriate choice.', ['n-ary features', 'n-gram features', 'feature', 'n-ary features'])
n-gram language model Chinese
('n-gram语言模型', '1. Rank: n-gram语言模型, n元语言模型, n-元语言模型, 语言模型\n\n2. Explanation: The term "n-gram语言模型" is the best fit because it retains the original "n-gram" terminology, which is crucial in the AI domain for clarity and specificity. This term directly translates to "n-gram language model," maintaining both semantic accuracy and contextual relevance. The use of "n-gram" in its original form is essential in technical contexts, as it is a widely recognized term in natural language processing and machine learning. \n\nThe second candidate, "n元语言模型," is also a good option, as "元" (which means "gram" in this context) is a common way to refer to n-grams in Chinese. However, it is less commonly used than "n-gram" in the AI community, which may lead to slight confusion among practitioners who are accustomed to the English term.\n\nThe third candidate, "n-元语言模型," introduces an unnecessary complexity by adding "元" before "n," which could confuse readers about its meaning. Lastly, "语言模型" (language model) is too vague and does not specify the n-gram aspect, making it unsuitable for the specific context of the term being discussed. \n\nOverall, "n-gram语言模型" is the most precise and contextually appropriate choice for conveying the concept of an n-gram language model in the AI domain.', ['n-gram language model', 'n-gram language model', 'language model', 'n-gram language model'])
n-gram model Chinese
('n-gram模型', '1. Rank: n-gram模型, n元模型, n-元模型, 模型\n\n2. Explanation: The term "n-gram模型" is the best fit because it directly translates the English term "n-gram model" while retaining the specific terminology used in the AI and computational linguistics domains. The term "n-gram" is a well-established concept in these fields, referring to a contiguous sequence of n items from a given sample of text or speech. By using "n-gram模型," the translation maintains both semantic accuracy and contextual relevance, ensuring that it is immediately recognizable to professionals in the field. \n\nIn contrast, "n元模型" and "n-元模型" are less precise. While they attempt to convey the idea of "n" items, they do not capture the established term "n-gram," which is critical for clarity in AI discussions. The term "模型" alone is too vague and does not convey the specific concept of an n-gram model. Therefore, "n-gram模型" is the most contextually appropriate and semantically accurate choice.', ['n-ary model', 'n-gram model', 'Model', 'n-ary model'])
neural scaling law Chinese
('神经缩放定律', '1. Rank: 神经缩放定律, 神经缩放法则, 神经规模定律, 神经网络规模定律, 神经网络缩放定律, 神经网络规模法则\n\n2. Explanation: The term "神经缩放定律" (neural scaling law) is the best fit because it accurately captures the concept of scaling in the context of neural networks, which is central to the AI domain. The word "缩放" (scaling) directly relates to the idea of adjusting size or capacity, which is essential in understanding how neural networks perform as their parameters or training data increase. Additionally, "定律" (law) conveys the empirical nature of the observations being discussed, aligning well with the scientific context. \n\nThe second choice, "神经缩放法则," is also a strong candidate, as "法则" (rule) is a synonym for "定律" but may not carry the same weight in scientific terminology. The other candidates, such as "神经规模定律" and "神经网络规模定律," introduce the term "规模" (scale) which is less precise in this context compared to "缩放" (scaling). Furthermore, the terms that include "神经网络" (neural network) are less preferable because the original term does not specify "network," and the broader term "神经" (neural) is more appropriate for the general concept being discussed.', ['neural scale law', 'law of neural scaling', 'Neural Network Scaling Law', 'Neural Network Scaling Law', 'Neural Scaling Law', 'Neural network sizing rules'])
normal form Chinese
('标准形式', '1. Rank: 标准形式, 标准型, 正常形式, 标准形, 欧几里得平面\n\n2. Explanation: The term "标准形式" (standard form) is the best fit for the translation of "normal form" in the context of AI and game theory. This term is widely recognized in both mathematical and computer science literature, particularly in discussions about ontologies and game theory. It accurately conveys the concept of a structured or canonical representation of data or models, which is essential in the context provided. \n\n"标准型" (standard type) and "正常形式" (normal form) are also relevant, but "标准形式" is more commonly used in academic and technical contexts, making it the preferred choice. "标准形" (standard form) is similar but less specific in this context. Lastly, "欧几里得平面" (Euclidean plane) is unrelated to the term "normal form" and does not fit the context at all, making it the least suitable option. \n\nOverall, "标准形式" aligns well with the semantic and contextual requirements of the AI domain, ensuring clarity and precision in communication.', ['standard form', 'Standard type', 'normal form', 'Standard form', 'Euclidean plane'])
normal-form game Chinese
('标准形式博弈', '1. Rank: 标准形式博弈, 标准型博弈, 标准形博弈, 正常形式游戏, 欧几里得空间\n\n2. Explanation: The term "标准形式博弈" (standard form game) is the best fit for the translation of "normal-form game" due to its semantic accuracy and established usage in the AI and game theory domains. This term directly corresponds to the English term, maintaining the concept of "normal form" which is a standard terminology in game theory. \n\n"标准型博弈" (standard game) and "标准形博弈" (standard form game) are also close, but they are less commonly used in the context of game theory compared to "标准形式博弈." \n\n"正常形式游戏" (normal game) is less accurate as it does not convey the specific meaning of "normal form" in game theory, which can lead to confusion. \n\nLastly, "欧几里得空间" (Euclidean space) is irrelevant to the context of game theory and should not be considered a candidate for this translation. \n\nOverall, "标准形式博弈" is the most contextually appropriate and widely recognized term in the field, making it the best choice for translating "normal-form game."', ['standard form game', 'standard game', 'normal game', 'standard form game', 'Euclidean space'])
object class Chinese
('目标类别', '1. Rank: 目标类别, 物体类别, 目标类\n\n2. Explanation: The term "目标类别" (target category) is the best fit for "object class" in the AI domain, particularly in the context of object detection. This term accurately captures the semantic meaning of "class" as it relates to categories of objects that a model is trained to recognize. The back translation of "目标类别" as "target category" aligns well with the original English term, maintaining the focus on classification within a specific context.\n\n"物体类别" (object category) is also a strong candidate, as it directly translates to "object category." However, it may imply a broader or more general classification of objects rather than the specific context of classes used in machine learning models.\n\n"目标类" (target class) is less preferred because while it translates back to "target class," it is less commonly used in the AI literature compared to "目标类别." The term "类别" (category) is more widely recognized in the context of classification tasks in machine learning, making "目标类别" the most contextually appropriate choice.', ['target class', 'target category', 'object category'])
occupancy grid Chinese
('占用栅格', '1. Rank: 占用栅格, 占用网格, 占据网格\n\n2. Explanation: The term "占用栅格" (back translated as "Occupancy grid") is the best fit for the AI domain-specific usage of "occupancy grid." This term accurately reflects the concept of a grid representation used in navigation and mapping, where each cell in the grid indicates whether a particular area is occupied or free. The use of "栅格" (grid) is more precise in this context, as it is commonly used in technical fields to refer to grid structures, especially in computer science and robotics. \n\nIn contrast, "占用网格" (back translated as "Occupying the grid") is less accurate because it suggests a more general action rather than a specific representation. "占据网格" (back translated as "occupy grid") is also less suitable because it implies a more active sense of occupying rather than the passive representation of occupancy that "occupancy grid" conveys. Therefore, "占用栅格" is the most semantically accurate and contextually appropriate term for the AI domain.', ['Occupying the grid', 'Occupancy grid', 'occupy grid'])
on-policy Chinese
('在策略', '1. Rank: 在策略, 策略内, 在线策略, 在政策内, 依策略\n\n2. Explanation: The term "在策略" (in strategy) is the best fit for the translation of "on-policy" in the AI context. This is because "在策略" directly conveys the idea of being aligned with or operating under a specific strategy, which is essential in reinforcement learning where "on-policy" methods refer to learning from actions taken according to the current policy. \n\nThe other candidates, such as "策略内" (within strategy) and "在线策略" (online strategy), while somewhat relevant, do not capture the precise meaning of "on-policy" as effectively. "策略内" suggests a more passive state of being within a strategy rather than actively following it, and "在线策略" implies a focus on online learning rather than the specific on-policy nature of the learning process. \n\n"在政策内" (within policy) and "依策略" (according to strategy) also lack the direct connection to the active learning process that "on-policy" entails. Therefore, "在策略" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['in strategy', 'within strategy', 'within policy', 'online strategy', 'According to strategy'])
one-against-all reduction Chinese
('一对多归约', '1. Rank: 一对多归约, 一对多, 一对多化, 一对多降维, 一对全归约, 一对全体降维, 对全体缩减\n\n2. Explanation: The term "一对多归约" (one-to-many reduction) is the best fit for the English term "one-against-all reduction" because it accurately captures the semantic meaning of the original term in the context of machine learning. In AI terminology, "归约" (reduction) is commonly used to describe the process of transforming a problem into a simpler form, which aligns with the concept of reducing a multi-class classification problem into multiple binary classification problems. \n\nThe other candidates, while they may convey similar ideas, either lack the specificity or do not align as closely with the established terminology in the AI domain. For instance, "一对多" (one to many) and "一对多化" (one-to-many) are too vague and do not include the term "归约," which is crucial for conveying the reduction aspect. "一对多降维" (one-to-many dimensionality reduction) introduces the concept of dimensionality reduction, which is not relevant in this context. \n\nOverall, "一对多归约" maintains both semantic accuracy and contextual fit, making it the most appropriate choice for the AI domain.', ['one-to-many reduction', 'one to many', 'one-to-many', 'A pair of overall dimensionality reduction', 'One pair total reduction', 'reduce overall', 'One-to-many dimensionality reduction'])
one-shot learning Chinese
('单样本学习', '1. Rank: 单样本学习, 单次学习, 一次性学习, 单一热学习\n\n2. Explanation: The term "单样本学习" (One-sample learning) is the best fit for "one-shot learning" in the AI domain because it accurately captures the essence of the concept, which refers to the ability of a model to learn from a single example or sample. This term is semantically precise and aligns well with the terminology used in machine learning literature. \n\n"单次学习" (Single learning) is a close second, but it lacks the specificity of "sample," which is crucial in the context of machine learning. "一次性学习" (One-time learning) suggests a temporal aspect that is not inherent in the concept of one-shot learning, making it less accurate. Lastly, "单一热学习" (Single hot learning) is not a recognized term in the AI field and does not convey the intended meaning, making it the least suitable option. \n\nOverall, "单样本学习" is the most contextually appropriate and semantically accurate translation for "one-shot learning" in the AI domain.', ['single learning', 'One-time learning', 'One-sample learning', 'single hot learning'])
one-shot setting Chinese
('单样本设置', '1. Rank: 单样本设置, 单次设置, 一次性设置, 单次场景, 单热设置\n\n2. Explanation: The term "单样本设置" (Single sample setup) is the best fit for the translation of "one-shot setting" in the AI domain. This is because "one-shot" refers to the ability of a model to learn from a single example or sample, which is accurately captured by "单样本" (single sample). The term "设置" (setup) is also appropriate in this context, as it aligns with the technical usage in machine learning and AI. \n\nIn contrast, "一次性设置" (One-time setup) implies a one-time occurrence rather than the specific learning context, which can lead to confusion. "单次设置" (Single setup) is somewhat vague and does not convey the sampling aspect. "单次场景" (single scene) and "单热设置" (Single heat setting) are not relevant to the context of machine learning and do not accurately reflect the concept of learning from a single instance. Therefore, "单样本设置" is the most semantically accurate and contextually appropriate choice.', ['One-time setup', 'Single sample setup', 'Single setup', 'single scene', 'Single heat setting'])
outlier rejection Chinese
('异常值拒绝', '1. Rank: 异常值拒绝, 异常值剔除, 离群点剔除\n\n2. Explanation: The term "异常值拒绝" (outlier rejection) is the best fit because it directly translates the English term while maintaining the specific context of rejecting outliers in data analysis, which is a common practice in AI and statistical modeling. The term "拒绝" (rejection) accurately conveys the action of discarding or not accepting certain data points that do not conform to expected patterns, which aligns with the AI domain\'s focus on maintaining data integrity and improving model performance. \n\nIn contrast, "异常值剔除" (outlier removal) and "离群点剔除" (outlier removal) use "剔除" (removal), which implies a more passive action of simply taking out data points without the connotation of decision-making involved in "rejection." Additionally, "离群点" (outlier) is a more general term that may not be as widely recognized in the specific context of AI as "异常值" (anomaly), which is commonly used in machine learning and data analysis literature. Therefore, "异常值拒绝" is the most semantically accurate and contextually appropriate choice.', ['outlier rejection', 'Outlier removal', 'Outlier removal'])
over-segmentation Chinese
('过度分割', '1. Rank: 过度分割, 过分分割, 过分细分, 过分割\n\n2. Explanation: The term "过度分割" is the best fit for "over-segmentation" in the AI domain because it accurately captures the concept of excessive division or segmentation of data, which is a common issue in image processing and computer vision. The back translation "over-segmentation" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. \n\nThe other candidates, while they may convey a similar idea, do not fit as well in the context of AI. "过分分割" and "过分细分" suggest a more subjective interpretation of "excessive" or "overly fine," which may not fully encompass the technical implications of segmentation in AI. "过分割" is less commonly used and may lead to confusion. Therefore, "过度分割" stands out as the most precise and contextually appropriate choice.', ['over-segmentation', 'over-segmentation', 'Over-segmentation', 'over-segmentation'])
pairwise potential Chinese
('成对势能', '1. Rank: 成对势能, 成对潜力, 成对势, 成对潜能\n\n2. Explanation: The term "成对势能" (back translation: "pairwise potential") is the best fit because it accurately captures the concept of "pairwise potential" in the context of AI and machine learning. The use of "势能" (potential energy) aligns well with the mathematical and probabilistic nature of the term in the context of modeling relationships between objects or regions in an image. \n\n"成对潜力" (back translation: "pair potential") is a close second, as it also conveys a sense of potential but lacks the specific connotation of energy or force that "势能" provides, which is often used in AI literature to describe potential functions.\n\nThe other candidates, "成对势" (back translation: "pair potential") and "成对潜能" (back translation: "pair potential"), are less suitable because they do not convey the same level of specificity and context as "势能" does. "势" (force) is too vague, and "潜能" (potential) does not have the same established usage in the AI domain as "势能." Thus, "成对势能" is the most semantically accurate and contextually appropriate choice.', ['pair potential', 'Pair potential', 'pairwise potential', 'pair potential'])
pattern-verbalizer pair Chinese
('模式-动词化对', '1. Rank: 模式-动词化对, 模式-动词化器对, 模式-语言化对, 模式-表述者对, 模式-动词配对, 模式-动词对, 模式-词语化器对\n\n2. Explanation: The term "模式-动词化对" (pattern-verbalizer pair) is the best fit because it accurately captures the essence of the original English term. The word "动词化" (verbalization) directly relates to the process of converting a pattern into a verbal form, which is crucial in the context of AI and natural language processing. This term maintains semantic accuracy and aligns well with the AI domain\'s terminology. \n\nThe second candidate, "模式-动词化器对" (pattern-verbizer pair), is also a strong contender, but "动词化" emphasizes the action of verbalizing rather than the tool or entity performing the action, which is more relevant in this context. \n\nOther candidates like "模式-语言化对" (pattern-verbalization pair) and "模式-表述者对" (pattern-presenter pair) introduce ambiguity, as they do not specifically denote the action of verbalization in the same way. The remaining terms, such as "模式-动词对" (pattern-verb pair) and "模式-动词配对" (pattern-verb pairing), are less accurate as they simplify the concept and do not convey the full meaning of the original term. \n\nThus, "模式-动词化对" is the most contextually appropriate choice for the AI domain.', ['Pattern-verbization pairs', 'pattern-tokenizer pair', 'pattern-verbalization pair', 'pattern-presenter pair', 'pattern-verb pair', 'Pattern-verb pairing', 'pattern-verbizer pair'])
permutohedral lattice Chinese
('"排列多面体格子"', '1. Rank: "排列多面体格子", "排列多面体格", "置换多面体格子", "置换体晶格", "置换对称格", "置换十面体格", "置换多面体格", "置换体晶格"\n\n2. Explanation: The term "排列多面体格子" (Arrange polyhedral grid) is the best fit for "permutohedral lattice" due to its semantic accuracy and contextual relevance in the AI domain. The term "排列" (arrange) captures the essence of the permutohedral structure, which is a specific arrangement of points in a lattice that facilitates efficient computation in algorithms, particularly in sampling and filtering. The inclusion of "多面体" (polyhedral) accurately reflects the geometric nature of the lattice, while "格子" (grid/lattice) is a standard term used in mathematical and computational contexts to describe such structures.\n\nOther candidates like "置换多面体格子" (Displace polyhedral lattice) and "置换体晶格" (replacement body lattice) introduce the notion of displacement or replacement, which may not accurately convey the intended meaning of the permutohedral lattice in the context of AI algorithms. The term "排列多面体格子" maintains the integrity of the original term while ensuring clarity and precision in its application within the AI domain.', ['Arrange polyhedral grid', 'Displace polyhedron', 'replacement body lattice', 'arranged body lattice', 'Arrange polyhedral lattice', 'permutation symmetry lattice', 'Replacement Decahedron', 'Displace polyhedral lattice'])
pixel-wise Chinese
('像素级别的', '1. Rank: 像素级别的, 逐像素, 像素级别, 像素级\n\n2. Explanation: The term "像素级别的" is the best fit for the translation of "pixel-wise" in the AI domain because it accurately conveys the concept of working at the level of individual pixels, which is essential in image processing and computer vision tasks. The back translation "pixel level" aligns well with the original English term, maintaining the semantic integrity. \n\n"逐像素" (Pixel by pixel) is also a strong candidate, but it implies a more sequential or iterative process, which may not fully capture the intended meaning of "pixel-wise" as a general descriptor of the method used. \n\n"像素级别" (pixel level) and "像素级" (pixel level) are less precise as they lack the grammatical structure that indicates the method\'s operational context, making them less suitable for the specific AI terminology. \n\nOverall, "像素级别的" provides the clearest and most contextually appropriate translation for the term "pixel-wise" in the given AI context.', ['pixel level', 'Pixel by pixel', 'pixel level', 'Pixel level'])
plug-in estimator Chinese
('插入式估计器', '1. Rank: 插入式估计器, 插件估计器, 插值估计器, 插值估计量\n\n2. Explanation: The term "插入式估计器" (plug-in estimator) is the best fit because it directly translates the English term while maintaining its specific meaning in the context of statistical estimation. In the AI and statistical domains, "plug-in" refers to a method where an estimator is derived by substituting sample estimates into a theoretical model. This term is widely recognized and used in statistical literature, making it semantically accurate and contextually appropriate.\n\nThe second candidate, "插件估计器," while also translating to "plug-in estimator," is less commonly used in the statistical context and may imply a more general software-related meaning (like a software plug-in), which could lead to confusion.\n\nThe other candidates, "插值估计器" and "插值估计量," translate to "interpolation estimator" and "interpolation estimate," respectively. These terms are not suitable in this context as they refer to a different statistical concept (interpolation), which does not align with the intended meaning of "plug-in estimator." Thus, they rank lower in terms of semantic accuracy and contextual fit.', ['interpolation estimator', 'interpolation estimator', 'plug-in estimator', 'plug-in estimator'])
pointwise multiplication Chinese
('逐点乘法', '1. Rank: 逐点乘法, 点乘, 点对点乘法\n\n2. Explanation: The term "逐点乘法" (point-by-point multiplication) is the best fit for "pointwise multiplication" in the AI domain because it accurately captures the concept of performing multiplication on corresponding elements of two vectors or matrices, which is the essence of pointwise operations in machine learning and AI contexts. The back translation "Point-by-point multiplication" aligns well with the original English term, maintaining the semantic integrity and contextual relevance.\n\n"点乘" (Click and multiply) is less suitable because it does not convey the mathematical operation clearly and could be misleading in a technical context. The back translation does not reflect the intended meaning of the term.\n\n"点对点乘法" (point-to-point multiplication) is also a reasonable candidate, but it implies a more direct one-to-one correspondence that may not fully encapsulate the broader concept of pointwise operations as effectively as "逐点乘法." Thus, while it is a valid term, it is not as precise in the context of AI terminology.', ['Click and multiply', 'Point-by-point multiplication', 'point-to-point multiplication'])
polylogarithmic Chinese
('多对数', '1. Rank: 多对数, 多对数的, 多对数级, 多重对数的, 余弦度量\n\n2. Explanation: The term "多对数" (polylogarithm) is the best fit because it directly corresponds to the mathematical concept of polylogarithm, which is commonly used in theoretical computer science and AI contexts. It accurately captures the essence of the term without introducing unnecessary complexity or ambiguity. The back translation "polylogarithm" aligns perfectly with the original English term, ensuring semantic accuracy. \n\nThe second candidate, "多对数的" (polylogarithmic), is also a good fit but is slightly less precise as it is an adjectival form rather than the noun form. "多对数级" (polylogarithmic level) introduces a level of abstraction that may not be necessary in the given context, while "多重对数的" (multiple logarithmic) misrepresents the concept by suggesting a different mathematical operation. Lastly, "余弦度量" (cosine metric) is unrelated to the term in question and should not be considered. Thus, "多对数" stands out as the most contextually appropriate and semantically accurate translation.', ['polylogarithmic level', 'polylogarithmic', 'cosine metric', 'Multiple logarithmic', 'polylogarithm'])
positional bias Chinese
('位置性偏差', '1. Rank: 位置性偏差, 位置偏差, 位置偏倚, 位置偏见\n\n2. Explanation: The term "位置性偏差" (positional bias) is the best fit for the AI domain-specific usage because it accurately captures the concept of bias related to the position of elements in a dataset or model. The inclusion of "性" (which translates to "nature" or "characteristic") emphasizes that this bias is a specific characteristic of the model\'s behavior, aligning well with the technical context of AI and machine learning. \n\n"位置偏差" (position deviation) is a close second, as it conveys a similar meaning but lacks the specificity that "位置性偏差" provides. The terms "位置偏倚" (location bias) and "位置偏见" (location bias) are less suitable because they do not directly translate the term "positional" and may imply a broader or different type of bias that is not specific to the context of AI models. Thus, "位置性偏差" is the most semantically accurate and contextually appropriate choice.', ['position deviation', 'location bias', 'location bias', 'positional bias'])
potential function Chinese
('潜在函数', '1. Rank: 潜在函数, 势函数, 势能函数\n\n2. Explanation: The term "潜在函数" (potential function) is the best fit because it accurately captures the meaning of "potential function" in the context of AI and optimization. The term "潜在" translates to "potential" in a way that aligns with its usage in mathematical and computational contexts, where it refers to a function that represents potential values or states in a system. \n\nThe second candidate, "势函数" (potential function), while also a valid translation, is less commonly used in the AI domain and may not convey the same level of clarity as "潜在函数." \n\nThe third candidate, "势能函数" (potential energy function), introduces the concept of energy, which is not relevant in this context. This term is more appropriate in physics rather than in AI or optimization contexts, making it the least suitable option. \n\nOverall, "潜在函数" is the most semantically accurate and contextually appropriate term for the AI domain.', ['potential function', 'potential function', 'potential energy function'])
potential heuristic Chinese
('势启发式', '1. Rank: 势启发式, 潜在启发式, 势能启发式, 势启发函数\n\n2. Explanation: The term "势启发式" (potential heuristic) is the best fit for the AI domain-specific usage because it directly translates the English term while maintaining the context of "potential" as it relates to heuristics in planning systems. This term is concise and aligns well with established terminology in AI literature, making it easily recognizable to practitioners in the field. \n\nThe second candidate, "潜在启发式" (underlying heuristic), while semantically close, introduces ambiguity by suggesting a more general or latent quality rather than the specific function of potential in heuristic optimization. \n\nThe third candidate, "势能启发式" (potential energy heuristic), introduces the concept of energy, which is not relevant in this context and could mislead readers regarding the nature of the heuristic being discussed.\n\nLastly, "势启发函数" (potential heuristic function) is overly specific and may not be appropriate in contexts where the term "heuristic" is used more broadly. It also deviates from the original term\'s intent, which focuses on the heuristic itself rather than its functional representation. \n\nOverall, "势启发式" captures the essence of the term while fitting seamlessly into the AI context, making it the most suitable choice.', ['underlying heuristic', 'potential heuristic', 'potential energy heuristic', 'Potential heuristic function'])
pre-logit Chinese
('预 logits', '1. Rank: 预 logits, 预-logit, 预对数, 前逻辑特征, 前逻辑回归特征, 前逻辑值, 预逻辑\n\n2. Explanation: The term "预 logits" is the best fit because it directly corresponds to the English term "pre-logit" while maintaining the technical context of AI and machine learning. The use of "logits" is common in the AI domain, particularly in neural networks, where "logits" refer to the raw output scores before applying a softmax function. The prefix "预" (pre) accurately conveys the idea of something that occurs before the final output, aligning well with the context of the paper discussing generative statistical methods. \n\nThe alternative "预-logit" is also a strong candidate, but it is less commonly used in the literature compared to "预 logits." The other candidates, such as "前逻辑特征" and "前逻辑回归特征," introduce unnecessary complexity and deviate from the established terminology in the AI field. Terms like "前逻辑值" and "预逻辑" are vague and do not capture the specific meaning of "logit" in this context. Therefore, "预 logits" is the most semantically accurate and contextually appropriate choice.', ['prelogarithm', 'previous logical value', 'prelogical features', 'pre-logit', 'pre-logits', 'Pre-logistic regression features', 'pre-logic'])
precision-at-10 Chinese
('前10位准确率', '1. Rank: 前10位准确率, 前10位精确率, 前10精度, 前10位精度, 精度@10, 前10位精度, 10项精度, 10个精度\n\n2. Explanation: The term "前10位准确率" (Top 10 accuracy) is the best fit because it accurately conveys the concept of measuring the accuracy of the top 10 results returned by a system, which is essential in the context of information retrieval and AI metrics. The use of "准确率" (accuracy rate) aligns well with the standard terminology used in AI and machine learning, where precision is often discussed in terms of accuracy. Additionally, "前10位" (top 10) clearly indicates the specific ranking being referred to, making it contextually appropriate. Other candidates like "精度@10" (Accuracy@10) are less common in usage and may not be as immediately understood in the AI community. The term "前10位精确率" (Top 10 precision rate) is also a strong candidate, but "准确率" is more widely recognized in the context of performance metrics.', ['Top ten accuracy', 'Top 10 accuracy', '10 items of accuracy', '10 accuracy', 'Accuracy@10', 'Top 10 accuracy', 'Top 10 accuracy', 'Top 10 digits of accuracy'])
proposal distribution Chinese
('提议分布', '1. Rank: 提议分布, 提案分布, 建议分布\n\n2. Explanation: The term "提议分布" (proposal distribution) is the best fit because it accurately captures the technical meaning of "proposal distribution" in the context of AI and statistics. The term "提议" directly translates to "proposal," which is commonly used in mathematical and statistical literature to refer to a distribution that is used to generate samples in algorithms like Metropolis-Hastings. \n\n"提案分布" (proposal distribution) is also a valid translation, but "提案" can imply a more formal or legal proposal, which may not align as closely with the statistical context. \n\n"建议分布" (recommended distribution) is the least accurate as "建议" translates to "suggestion" or "recommendation," which does not convey the same meaning as "proposal" in this context. In AI and statistical sampling, "proposal distribution" has a specific connotation that is not captured by "建议分布." Therefore, "提议分布" is the most semantically accurate and contextually appropriate choice.', ['Proposal distribution', 'Proposal distribution', 'Recommended distribution'])
proposal probability Chinese
('提议概率', '1. Rank: 提议概率, 提案概率, 建议概率\n\n2. Explanation: The term "提议概率" (proposal probability) is the best fit for the AI domain-specific usage because it accurately captures the meaning of "proposal" in the context of probabilistic models and algorithms. In AI, particularly in areas like Markov Chain Monte Carlo (MCMC) methods, "proposal" refers to a suggested state or sample that is generated during the sampling process. The term "提议" directly translates to "proposal," which aligns well with the technical usage in the context provided.\n\nOn the other hand, "提案概率" (proposal probability) is also a close candidate, but "提案" can imply a more formal or structured proposal, which may not fit as well in the context of probabilistic sampling. Lastly, "建议概率" (probability of suggestion) is less suitable because "建议" translates to "suggestion," which does not convey the same technical meaning as "proposal" in this context. Therefore, "提议概率" is the most semantically accurate and contextually appropriate term for "proposal probability" in the AI domain.', ['Proposal probability', 'Proposal probability', 'Probability of suggestion'])
quasi-Newton method Chinese
('拟牛顿法', '1. Rank: 拟牛顿法, 准牛顿法, 准牛顿方法\n\n2. Explanation: The term "拟牛顿法" is the best fit for the translation of "quasi-Newton method" in the AI domain. The prefix "拟" (which means "approximate" or "pseudo") accurately captures the essence of the "quasi" in the English term, indicating that it is an approximation of the Newton method rather than the method itself. This is crucial in the context of optimization algorithms, where the distinction between exact and approximate methods is significant.\n\nThe term "准牛顿法" is also a valid translation, but "准" (which can mean "standard" or "accurate") does not convey the same sense of approximation as "拟." While it is still understandable, it may not be as precise in the context of numerical methods.\n\n"准牛顿方法" is less preferred because the addition of "方法" (method) at the end makes it slightly less concise and less commonly used in the literature compared to "法." In technical contexts, brevity and precision are important, and "拟牛顿法" is the most widely recognized term in the field of optimization and AI.', ['Quasi-Newton method', 'Quasi-Newton method', 'Quasi-Newton method'])
replay memory Chinese
('经验回放', '1. Rank: 经验回放, 回放记忆, 重放记忆, 经验回放存储器\n\n2. Explanation: The term "经验回放" (Experience replay) is the best fit for the AI domain, particularly in the context of Deep Q-Networks (DQN). This term is widely recognized in the machine learning community and accurately conveys the concept of storing and reusing past experiences to improve learning efficiency. The back translation "Experience replay" aligns perfectly with the established terminology in English literature, making it semantically accurate and contextually appropriate.\n\n"回放记忆" (Replay memory) and "重放记忆" (replay memory) are also valid translations, but they are less commonly used in the AI field compared to "经验回放." They may not convey the same level of specificity regarding the learning process involved. \n\n"经验回放存储器" (experience replay memory) is more verbose and while it is accurate, it is not as commonly used in practice. The term "存储器" (memory) adds unnecessary complexity to the term, which is typically referred to simply as "经验回放" in the literature. Thus, "经验回放" stands out as the most appropriate choice for its semantic accuracy and contextual fit in the AI domain.', ['replay memory', 'Replay memory', 'Experience replay', 'experience replay memory'])
reproducing property Chinese
('再生性质', '1. Rank: 再生性质, 复现性质, 重现性质, 复制性质, 重现性属性\n\n2. Explanation: The term "再生性质" (reproducing property) is the best fit for the AI domain because it accurately captures the concept of a property that allows functions to be evaluated at certain points through inner products in a reproducing kernel Hilbert space (RKHS). The back translation "Regenerative properties" aligns closely with the original meaning, emphasizing the idea of regeneration or reproduction of values. \n\nThe second choice, "复现性质" (recurring properties), also conveys a sense of reproduction but lacks the specific connotation of the mathematical context found in "再生性质." "重现性质" (reproducible properties) is similar but may imply a broader context of reproducibility rather than the specific mathematical property. \n\n"复制性质" (copy nature) and "重现性属性" (reproducibility properties) are less suitable as they do not convey the precise mathematical implications of the reproducing property in the context of RKHS, which is crucial for understanding the underlying concepts in AI and machine learning. Thus, "再生性质" is the most contextually accurate and semantically appropriate choice.', ['Regenerative properties', 'Copy nature', 'recurring properties', 'Reproducibility properties', 'Reproducible properties'])
reservoir sampling Chinese
('蓄水池抽样', '1. Rank: 蓄水池抽样, 水库采样, 储备抽样, 储存抽样, 源抽样\n\n2. Explanation: The term "蓄水池抽样" (Reservoir sampling) is the most accurate translation for the AI domain context. This term directly translates to "reservoir sampling," which is a well-established concept in computer science and statistics, particularly in the context of sampling algorithms. The term "蓄水池" (reservoir) is commonly used in Chinese literature to refer to the concept of a reservoir in a statistical sense, making it semantically accurate and contextually appropriate.\n\nThe second candidate, "水库采样," while also translating to "reservoir sampling," uses "水库," which is more commonly associated with physical water reservoirs rather than the abstract concept of sampling in statistics. This could lead to confusion in an AI context.\n\nThe other candidates, "储备抽样" (reserve sampling), "储存抽样" (storage sampling), and "源抽样" (source sampling), do not accurately convey the specific meaning of reservoir sampling in the context of sampling algorithms. They introduce ambiguity and do not align with the established terminology used in the AI and machine learning fields. Therefore, "蓄水池抽样" is the best fit for its semantic accuracy and contextual relevance.', ['Reservoir sampling', 'storage sampling', 'reserve sampling', 'source sampling', 'Reservoir sampling'])
restricted isometry property Chinese
('限制等距性质', '1. Rank: 限制等距性质, 受限等距性质, 受限等距性性质\n\n2. Explanation: The term "限制等距性质" (restricted isometry property) is the best fit because it accurately captures the meaning of the original English term while maintaining semantic clarity. The word "限制" (restricted) directly translates to "restricted," and "等距" (isometry) is a standard term in mathematics and AI that refers to distance-preserving transformations. The term "性质" (property) is also commonly used in mathematical contexts. \n\nThe second candidate, "受限等距性质," is also a valid translation, but "受限" (restricted) is less commonly used in this specific mathematical context compared to "限制." The third candidate, "受限等距性性质," introduces redundancy with "性质" and "性," which can lead to confusion and is less concise. Therefore, "限制等距性质" is the most semantically accurate and contextually appropriate choice in the AI domain.', ['restricted isometric property', 'restricted isometric property', 'restricted isometric properties'])
roberta-large Chinese
('"roberta-large"', '1. Rank: "roberta-large", "RoBERTa大型模型", "罗伯塔大模型", "羅伯塔大"\n\n2. Explanation: The first translation candidate, "roberta-large", is the best fit because it retains the original English term, which is crucial in the AI domain where specific model names are widely recognized and used in their original form. This is particularly important for technical accuracy and consistency, as "roberta-large" refers to a specific pretrained model in the RoBERTa family. \n\nThe second candidate, "RoBERTa大型模型", is also a strong choice as it includes the original name "RoBERTa" and adds "大型模型" (large model), which accurately describes the model\'s size. This maintains semantic accuracy while providing context.\n\nThe third candidate, "罗伯塔大模型", translates "roberta" to "罗伯塔" and uses "大模型" (big model), which is acceptable but less precise than the second option since it does not retain the original capitalization of "RoBERTa".\n\nThe last candidate, "羅伯塔大", is the least effective as it omits the term "模型" (model) entirely, which is essential for clarity in the AI context. It also uses a traditional character form that may not be as widely recognized in the context of modern AI terminology.', ['Roberta-large', 'Roberta Big', 'Roberta big model', 'RoBERTa large model'])
rollout Chinese
('"展开序列"', '1. Rank: "展开序列", "展开", "执行过程", "回放", "回滚"\n\n2. Explanation: The term "展开序列" (unfold sequence) is the best fit for the AI context of "rollout" because it accurately captures the concept of generating a sequence of states, actions, and rewards over time, which is essential in reinforcement learning. The term "展开" (expand) is also relevant but is more general and does not specifically imply the sequential aspect that "展开序列" conveys. "执行过程" (execution process) is too broad and does not specifically relate to the concept of a rollout in reinforcement learning. "回放" (playback) and "回滚" (rollback) are misleading in this context, as they suggest different processes that do not align with the intended meaning of "rollout." Therefore, "展开序列" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['Expand', 'Execution process', 'Playback', 'rollback', 'unfold sequence'])
sample-efficient Chinese
('样本高效', '1. Rank: 样本高效, 样本高效的, 样本效率高, 样品效率高\n\n2. Explanation: The term "样本高效" is the best fit because it directly translates to "sample-efficient," maintaining the original meaning and context of the English term. In the AI domain, "sample-efficient" refers to algorithms or methods that require fewer samples to achieve a certain level of performance, which is accurately captured by "样本高效." The back translation "Sample efficient" confirms its semantic accuracy. \n\nThe second candidate, "样本高效的," is also a good fit as it retains the meaning but adds a grammatical particle that makes it slightly less direct than the first option. \n\n"样本效率高" translates to "high sample efficiency," which, while related, shifts the focus from the efficiency of the sampling process itself to a more general statement about efficiency, making it less precise in the context of AI. \n\nLastly, "样品效率高" translates to "high sample efficiency" as well, but the use of "样品" (sample) instead of "样本" (data sample) is less appropriate in the AI context, as "样品" typically refers to physical samples rather than data samples used in machine learning. Thus, it ranks the lowest.', ['Sample efficient', 'High sample efficiency', 'sample efficient', 'High sample efficiency'])
semantic operator Chinese
('语义操作符', '1. Rank: 语义操作符, 语义运算符, 语义算子\n\n2. Explanation: The term "语义操作符" (semantic operators) is the best fit because it closely aligns with the established terminology used in the AI and computational linguistics fields. The term "操作符" (operator) is commonly used in programming and mathematical contexts, making it more recognizable and appropriate for the technical audience. \n\n"语义运算符" (semantic operators) is also a valid option, but "运算符" (operation symbol) is less frequently used in the context of AI and may imply a broader mathematical operation rather than the specific function of a semantic operator. \n\n"语义算子" (semantic operator) is less preferred because "算子" (operator) is often used in more abstract mathematical contexts and may not convey the same level of specificity as "操作符" in the context of AI. \n\nOverall, "语义操作符" provides the best semantic accuracy and contextual fit for the AI domain, ensuring clarity and precision in communication.', ['semantic operators', 'semantic operators', 'semantic operator'])
sense disambiguation Chinese
('语义消歧', '1. Rank: 语义消歧, 词义消歧, 意义消歧, 义项消歧, 词义\n\n2. Explanation: The term "语义消歧" (semantic disambiguation) is the best fit for the context of AI and natural language processing. In this domain, "语义" (semantic) specifically refers to the meaning of words and phrases in context, which is crucial for understanding and processing language accurately. "消歧" (disambiguation) directly relates to the process of resolving ambiguities in meaning. \n\nWhile "词义消歧" (word sense disambiguation) is also a strong candidate, it is more specific to the disambiguation of individual words rather than the broader semantic context, which may limit its applicability in certain AI contexts. "意义消歧" (meaning disambiguation) and "义项消歧" (meaning item disambiguation) are less commonly used in the AI field and may not convey the same level of specificity as "语义消歧." Lastly, "词义" (meaning) is too vague and does not encompass the disambiguation aspect at all. Therefore, "语义消歧" is the most contextually appropriate and semantically accurate term for the AI domain.', ['meaning disambiguation', 'semantic disambiguation', 'word sense disambiguation', 'meaning disambiguation', 'meaning'])
shift invariant Chinese
('平移不变性', '1. Rank: 平移不变性, 平移不变, 移位不变, 位移不变, 移动不变, 平移不变的\n\n2. Explanation: The term "平移不变性" (translation invariance) is the best fit because it accurately captures the concept of invariance under translation, which is a fundamental principle in many areas of AI, particularly in machine learning and computer vision. The term "平移" specifically refers to translation in a mathematical sense, making it semantically precise. The back translation "translation invariance" aligns well with the original English term "shift invariant," maintaining the technical context. \n\nThe other candidates, while they convey a similar idea, either lack the specificity of "平移" (translation) or introduce ambiguity with terms like "移位" (shift) and "移动" (move), which can imply different types of transformations. "平移不变" (translation unchanged) is also a strong candidate but is less formal than "平移不变性," which is more commonly used in academic and technical literature. Thus, "平移不变性" is the most contextually appropriate choice in the AI domain.', ['Shift unchanged', 'translation invariance', 'Translation unchanged', 'Displacement unchanged', 'Move unchanged', 'translation invariant'])
sigmoid activation Chinese
('Sigmoid激活函数', '1. Rank: Sigmoid激活函数, sigmoid激活函数, Sigmoid 激活, sigmoid激活, シグモイド活性化関数, 乙状结肠激活\n\n2. Explanation: The term "Sigmoid激活函数" is the best fit because it accurately captures the concept of "sigmoid activation" in the context of AI and neural networks. The inclusion of "函数" (function) is crucial as it specifies that this is a mathematical function, which is essential in the AI domain. The back translation "Sigmoid activation function" aligns perfectly with the original English term, maintaining both semantic accuracy and contextual relevance. \n\nThe second choice, "sigmoid激活函数," is also a strong candidate, but it uses lowercase "sigmoid," which is less common in formal contexts where capitalization is typically used for function names. \n\n"Sigmoid 激活" and "sigmoid激活" are less precise because they omit the term "函数," which is important for clarity in mathematical contexts. \n\n"シグモイド活性化関数" is a transliteration from Japanese and is not suitable for a Chinese context, while "乙状结肠激活" is completely incorrect as it translates to "sigmoid colon activation," which has no relevance in the AI context. \n\nOverall, the first choice is the most contextually appropriate and semantically accurate for the AI domain.', ['Sigmoid activation', 'シグモイド activation number', 'Sigmoid activation function', 'sigmoid activation function', 'sigmoid activation', 'sigmoid activation'])
similarity metric Chinese
('相似度度量', '1. Rank: 相似度度量, 相似度指标, 相似度量, 相似性指标\n\n2. Explanation: The term "相似度度量" (similarity measure) is the best fit for the English term "similarity metric" in the context of AI and information theory. The word "度量" (measure) is commonly used in mathematical and statistical contexts to refer to a quantitative assessment, which aligns well with the concept of a metric. Additionally, "相似度" (similarity) accurately captures the essence of what is being measured. \n\nThe second choice, "相似度指标" (similarity index), while also relevant, uses "指标" (index), which is less precise in the context of metrics that are typically used in AI and data analysis. "相似度量" (similarity measure) is a close contender but is less commonly used than "相似度度量." Lastly, "相似性指标" (similarity index) is the least suitable as "相似性" (similarity) is a broader term and does not convey the specific quantitative aspect that "度量" (measure) does. Thus, "相似度度量" is the most semantically accurate and contextually appropriate term for the AI domain.', ['similarity index', 'similarity index', 'similarity measure', 'Similarity measure'])
similarity score Chinese
('相似度得分', '1. Rank: 相似度得分, 相似度分数, 相似性得分, 相似性评分\n\n2. Explanation: The term "相似度得分" is the best fit for "similarity score" in the AI domain due to its semantic accuracy and contextual relevance. The term "相似度" (similarity degree) is widely used in machine learning and AI literature to refer to the measure of how alike two entities are, which aligns perfectly with the concept of a similarity score. The addition of "得分" (score) clearly indicates that it is a quantifiable measure, which is essential in the context of evaluating embeddings or matching observations. \n\nIn contrast, "相似性得分" and "相似性评分" use "相似性" (similarity) which is less common in technical contexts and may imply a broader or less precise concept. "相似度分数" is also a valid option, but "得分" is more commonly used in the context of scoring systems in AI. Therefore, "相似度得分" is the most contextually appropriate and semantically accurate choice for the term "similarity score."', ['similarity score', 'similarity score', 'similarity score', 'similarity score'])
skip-gram Chinese
('跳词模型', '1. Rank: 跳词模型, 跳跃词模型, 跳字模型, 跳字\n\n2. Explanation: The term "跳词模型" (skip-word model) is the best fit for the translation of "skip-gram" in the AI domain, particularly in the context of word embeddings. This term accurately captures the essence of the original English term, where "跳" (skip) directly relates to the action of skipping words in the context of the model, and "词" (word) clearly indicates that it pertains to words. \n\nThe back translation of "跳词模型" as "word skip model" maintains semantic accuracy and contextual relevance, aligning well with the established terminology in natural language processing. \n\nIn contrast, "跳跃词模型" (Jump word model) introduces ambiguity with "跳跃" (jump), which does not convey the same meaning as "skip" in this context. "跳字模型" (word skip model) is less precise because "字" (character) refers to characters rather than words, which is not suitable for the context of word embeddings. Lastly, "跳字" (Jump word) is too vague and lacks the model-specific context needed for clarity. Thus, "跳词模型" is the most appropriate choice for conveying the concept of "skip-gram" in the AI domain.', ['word skip model', 'Jump word model', 'word skip model', 'Jump word'])
slot Chinese
('插槽', '1. Rank: 插槽, 槽位, 槽, 投币口\n\n2. Explanation: The term "插槽" is the best fit for the AI domain-specific usage of "slot" because it accurately conveys the concept of a designated space or position for input within a system, which aligns with the context of dialogue systems and generative models. "插槽" directly translates to "slot" in a technical sense, often used in computing and AI contexts to refer to placeholders or variables that can hold specific values. \n\n"槽位" is also a reasonable candidate, but it is less commonly used in the AI context compared to "插槽." "槽" translates to "groove," which does not fit the intended meaning in this context, and "投币口" translates to "coin slot," which is entirely unrelated to the AI terminology being discussed. Therefore, "插槽" is the most semantically accurate and contextually appropriate choice.', ['slot', 'slot', 'groove', 'Coin slot'])
slot filling Chinese
('插槽填充', '1. Rank: 插槽填充, 槽位填充, 槽填充\n\n2. Explanation: The term "插槽填充" is the best fit for "slot filling" in the AI domain due to its semantic accuracy and contextual relevance. "插槽" directly translates to "slot," which is commonly used in technical contexts, particularly in dialogue systems and natural language processing. This term clearly conveys the idea of filling designated spaces (or slots) with specific information extracted from user input. \n\n"槽位填充" is also a strong candidate, as "槽位" can be understood as "slot position," which maintains the meaning but is slightly less common in usage compared to "插槽." \n\n"槽填充," while still understandable, is less precise because "槽" can refer to a broader range of contexts and may not specifically imply the technical meaning of "slot" in AI applications. Therefore, "插槽填充" is the most contextually appropriate and widely recognized term in the AI field, making it the best choice for translating "slot filling."', ['slot filling', 'slot filling', 'slot filling'])
slot-value pair Chinese
('插槽值对', '1. Rank: 插槽值对, 槽位-值对, 槽-值对, 槽值对\n\n2. Explanation: The term "插槽值对" is the best fit for the AI domain-specific usage of "slot-value pair" because it accurately captures the concept of a "slot" as a designated space or position (插槽) that holds a specific value (值). This translation maintains the semantic integrity of the original term and is clear in its meaning, making it easily understandable in the context of AI and data structures. \n\nThe other candidates, while they may convey similar meanings, do not align as closely with the established terminology in AI. For instance, "槽位-值对" introduces the term "槽位," which can imply a more physical or spatial context rather than the abstract concept of a slot in data structures. "槽-值对" and "槽值对" are less formal and may not be as widely recognized in technical discussions. Therefore, "插槽值对" stands out as the most precise and contextually appropriate choice.', ['slot value pair', 'slot-value pair', 'slot-value pair', 'slot value pair'])
softmax activation Chinese
('softmax 激活', '1. Rank: softmax 激活, softmax激活, Softmax 激活, 软最大值激活, 激活\n\n2. Explanation: The term "softmax 激活" is the best fit because it maintains the original English term "softmax" while providing a clear and accurate translation of "activation." This term is widely recognized in the AI and machine learning communities, ensuring semantic accuracy and contextual fit. The back translation confirms that it retains the intended meaning. The variations "softmax激活" and "Softmax 激活" are also acceptable, but the first option is preferred due to its consistency in formatting and common usage. The term "软最大值激活" translates to "soft maximum activation," which, while technically accurate, is less commonly used in the AI domain and may cause confusion. Lastly, "激活" alone is too vague and does not convey the specific concept of softmax activation.', ['softmax activation', 'softmax activation', 'Softmax activation', 'softmax activation', 'activation'])
softmax activation function Chinese
('softmax激活函数', '1. Rank: softmax激活函数, softmax 激活函数, 软最大值激活函数, oftmax 激活函数\n\n2. Explanation: The term "softmax激活函数" is the best fit because it accurately retains the original English term "softmax" while clearly indicating its function as an "activation function" in Chinese. The use of "激活函数" (activation function) is standard terminology in the AI and machine learning fields, making it easily recognizable to professionals in the domain. The second candidate, "softmax 激活函数," is also a good option but includes a space that is not typically used in technical terms, making it slightly less formal. The third candidate, "软最大值激活函数," translates "softmax" literally to "soft maximum," which, while understandable, is not the standard term used in the AI community and could lead to confusion. Lastly, "oftmax 激活函数" is a misspelling of "softmax," making it the least suitable choice. Therefore, "softmax激活函数" is the most semantically accurate and contextually appropriate term for the AI domain.', ['softmax activation function', 'softmax activation function', 'oftmax activation function', 'Soft maximum activation function'])
softmax classifier Chinese
('softmax分类器', '1. Rank: softmax分类器, softmax 分类器, Softmax 分类器, 软最大值分类器\n\n2. Explanation: The term "softmax分类器" is the best fit because it accurately reflects the original English term "softmax classifier" while maintaining semantic accuracy and contextual relevance in the AI domain. The term "softmax" is a well-established term in machine learning and AI, and it is commonly used in both English and Chinese without translation. The absence of a space in "softmax分类器" aligns with the conventions of technical terminology in Chinese, where compound terms are often written without spaces. \n\nThe second candidate, "softmax 分类器," introduces a space that is not typically used in technical contexts, making it less preferable. The third candidate, "Softmax 分类器," capitalizes "Softmax," which is unnecessary in Chinese and could imply a different emphasis. Lastly, "软最大值分类器" translates "softmax" literally, which is less common in the AI community and may lead to confusion, as it does not convey the established meaning of the term. Therefore, "softmax分类器" is the most contextually appropriate and semantically accurate choice.', ['softmax classifier', 'softmax classifier', 'Softmax classifier', 'soft max classifier'])
softplus function Chinese
('softplus 函数', '1. Rank: softplus 函数, softplus函数, Softplus函数, Softplus 函数, 软加函数, 软正函数, 软加功能\n\n2. Explanation: The term "softplus 函数" is the best fit because it accurately retains the original English term "softplus function" in a way that is semantically precise and contextually appropriate for the AI domain. The use of "函数" (function) is standard in mathematical and computational contexts in Chinese, making it clear that this term refers to a specific mathematical function. \n\nThe other candidates, such as "软加函数" (soft addition function) and "软正函数" (soft positive function), introduce ambiguity and do not accurately reflect the established terminology used in AI and machine learning literature. The variations "softplus函数" and "Softplus函数" are also acceptable but less formal due to the lack of a space, which is typically used in academic writing. The capitalization in "Softplus 函数" and "Softplus函数" is unnecessary in this context, as it does not follow the standard convention for function names in Chinese. \n\nOverall, "softplus 函数" maintains both semantic accuracy and contextual fit, making it the most suitable choice for the AI domain.', ['softplus function', 'soft addition function', 'softplus function', 'softplus function', 'Softplus function', 'Softplus function', 'soft positive function', 'Soft plus function'])
sparsity level Chinese
('稀疏水平', '1. Rank: 稀疏水平, 稀疏度水平, 稀疏级别\n\n2. Explanation: The term "稀疏水平" (sparsity level) is the best fit for the AI domain-specific usage because it directly translates the English term while maintaining the same semantic structure. "水平" (level) is commonly used in technical contexts to denote a degree or extent, which aligns well with the concept of "sparsity" in mathematical and algorithmic contexts. \n\n"稀疏度水平" (sparsity degree level) is a bit more verbose and introduces "度" (degree), which can be seen as redundant since "水平" already conveys the idea of a level of sparsity. While it is still accurate, it may not be as concise or commonly used in the field.\n\n"稀疏级别" (sparsity grade) is less preferred because "级别" (grade) is not as commonly used in this context as "水平" (level). In AI and mathematical literature, "水平" is more frequently associated with levels of measurement or thresholds, making it a more appropriate choice for conveying the intended meaning of "sparsity level." \n\nOverall, "稀疏水平" is the most semantically accurate and contextually fitting translation for the term in the AI domain.', ['sparsity level', 'sparsity level', 'sparsity level'])
spectral property Chinese
('谱性质', '1. Rank: 谱性质, 谱特性, 光谱特性\n\n2. Explanation: The term "谱性质" (back translated as "Spectral properties") is the best fit for the AI domain-specific usage because it accurately captures the technical meaning of "spectral property" in the context of data matrices and their characteristics. In mathematical and statistical contexts, "性质" (properties) is commonly used to describe inherent attributes or characteristics of mathematical objects, making it semantically precise. \n\nOn the other hand, "谱特性" (back translated as "Spectral characteristics") is also a valid translation but is less commonly used in the specific context of spectral analysis in AI and machine learning. "光谱特性" (back translated as "Spectral characteristics") introduces the term "光" (light), which is not relevant in this context and could lead to confusion, as it implies a focus on light spectra rather than the mathematical properties of matrices. Therefore, "谱性质" is the most contextually appropriate choice.', ['Spectral characteristics', 'Spectral properties', 'Spectral characteristics'])
submodular Chinese
('次模的', '1. Rank: 次模的, 次模, 子模, 子模块, 子模量, 非次模\n\n2. Explanation: The term "次模的" (submodular) is the best fit because it accurately captures the semantic meaning of the English term "submodular" in the context of optimization and graph theory. In AI and mathematical contexts, "次模的" is commonly used to refer to properties of functions or structures that exhibit submodularity, which is crucial for understanding optimization problems. \n\nThe back translation of "次模的" to "submodular" aligns perfectly with the original term, ensuring semantic accuracy. The other candidates, such as "次模" (secondary mode) and "子模块" (submodule), do not convey the same meaning and could lead to confusion in the AI domain. "子模" (sub-model) and "子模量" (submodulus) also deviate from the intended meaning, while "非次模" (non-submodule) is not relevant as it refers to the negation of the term. Therefore, "次模的" is the most contextually appropriate choice for the AI terminology in question.', ['submodular', 'secondary mode', 'submodule', 'non-submodule', 'submodulus', 'sub-model'])
subsample Chinese
('子样本', '1. Rank: 子样本, 子采样, 子抽样, 子样本 / 采样子集\n\n2. Explanation: The term "子样本" (subsample) is the best fit because it directly translates to "subsample," which is a commonly used term in the AI and machine learning domains. It accurately reflects the concept of taking a subset of data from a larger dataset, which is essential in various algorithms, particularly in the context of balancing classes or reducing dataset size for training. \n\n"子采样" (subsampling) is also a valid term, but it is slightly less precise in this context as it can imply a broader process of sampling rather than specifically referring to a subset. "子抽样" (subsampling) is less commonly used in the AI context and may not be as immediately recognizable to practitioners. Lastly, "子样本 / 采样子集" (subsample/sampling subset) is overly verbose and could lead to confusion, as it combines two concepts that are not necessary to distinguish in this context. Therefore, "子样本" is the most semantically accurate and contextually appropriate choice.', ['subsampling', 'subsample', 'subsampling', 'subsample/sampling subset'])
subsampling factor Chinese
('子采样因子', '1. Rank: 子采样因子, 子抽样因子, 采样因子 / 子采样因子, 二次采样因子\n\n2. Explanation: The term "子采样因子" (subsampling factor) is the best fit because it accurately reflects the concept of subsampling in the context of AI and signal processing. The prefix "子" (meaning "sub" or "child") effectively conveys the idea of a subset or a smaller portion of the original sample, which is essential in the context of subsampling. \n\nThe term "子抽样因子" is also a valid candidate, but "抽样" (sampling) is a broader term that may not specifically imply the reduction aspect as clearly as "采样" (which is more commonly associated with sampling in a statistical sense). \n\nThe term "采样因子 / 子采样因子" introduces ambiguity by combining both terms, which could confuse the reader about whether it refers to general sampling or specifically to subsampling. \n\nLastly, "二次采样因子" (secondary sampling factor) suggests a two-step sampling process, which is not the intended meaning of "subsampling" in this context. Therefore, "子采样因子" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['subsampling factor', 'subsampling factor', 'Sampling factor / subsampling factor', 'subsampling factor'])
symbol grounding problem Chinese
('符号扎根问题', '1. Rank: 符号扎根问题, 符号接地问题, 符号嵌入问题, 符号定位问题, 符号基础问题\n\n2. Explanation: The term "符号扎根问题" (Symbol grounding problem) is the best fit because it accurately captures the essence of the original English term while maintaining semantic integrity. The word "扎根" (to take root) conveys the idea of establishing a connection between symbols and their meanings, which is central to the concept of symbol grounding in AI. This term is widely recognized in the AI research community, particularly in discussions about how machines can understand and generate human language based on contextual meanings.\n\nThe second candidate, "符号接地问题" (Symbol grounding issues), while also a reasonable translation, uses "接地" (to ground) which may not be as commonly used in this specific context as "扎根." The other candidates, such as "符号嵌入问题" (Symbol embedding problem) and "符号定位问题" (Symbol positioning problem), introduce different concepts that do not align with the original meaning of grounding symbols in a cognitive or AI context. "符号基础问题" (Symbol basic issues) is too vague and does not convey the specific challenge of grounding symbols in meaning. Thus, "符号扎根问题" stands out as the most contextually appropriate and semantically accurate translation.', ['Symbol grounding issues', 'Symbol grounding problem', 'Symbol embedding problem', 'Symbol basic issues', 'Symbol positioning problem'])
tanh activation function Chinese
('tanh激活函数', '1. Rank: tanh激活函数, 双曲正切激活函数, Tanh激活函数, Tanh 活性化関数\n\n2. Explanation: The first translation, "tanh激活函数," is the best fit because it accurately retains the original term "tanh" while clearly indicating its function as an activation function in the context of neural networks. The term "激活函数" directly translates to "activation function," which is a standard term in AI and machine learning literature. \n\nThe second candidate, "双曲正切激活函数," while semantically accurate (it translates to "hyperbolic tangent activation function"), is less commonly used in practice. In the AI domain, the abbreviation "tanh" is widely recognized and used, making the first option more suitable for practitioners and researchers familiar with the terminology.\n\nThe third candidate, "Tanh激活函数," is similar to the first but uses a capital "T" for "Tanh," which is less conventional in Chinese texts where the term is often presented in lowercase. \n\nThe last candidate, "Tanh 活性化関数," is not appropriate as it mixes languages (using "関数" which is Japanese for "function") and does not fit the context of the AI domain, making it the least suitable option.', ['tanh activation function', 'hyperbolic tangent activation function', 'Tanh activation pass', 'Tanh activation function'])
tensor factorization Chinese
('张量因子化', '1. Rank: 张量因子化, 张量因式分解, 张量分解\n\n2. Explanation: The term "张量因子化" (tensor factorization) is the best fit because it directly corresponds to the English term "tensor factorization" in both semantic accuracy and contextual usage within the AI domain. The term "因子化" (factorization) is commonly used in mathematical and computational contexts to refer to the process of breaking down a complex structure into simpler components, which aligns perfectly with the concept of tensor factorization in machine learning and data analysis. \n\n"张量因式分解" (tensor factorization) is also a valid translation, but the term "因式分解" (factorization) is more commonly associated with algebraic expressions rather than the broader context of tensors in AI. Lastly, "张量分解" (tensor decomposition) is a more general term that may not capture the specific nuances of factorization as it relates to tensors, which can lead to ambiguity in the context of the provided examples. Therefore, "张量因子化" is the most precise and contextually appropriate choice.', ['Tensor factorization', 'tensor decomposition', 'Tensor factorization'])
token vocabulary Chinese
('令牌词汇表', '1. Rank: 令牌词汇表, 词元词汇表, 词元词汇, 标记词表, 词元词汇\n\n2. Explanation: The term "令牌词汇表" (Token Glossary) is the best fit for the English term "token vocabulary" in the context of AI and machine translation. This is because "令牌" directly translates to "token," which is a commonly used term in the AI domain, particularly in natural language processing (NLP). The addition of "词汇表" (vocabulary list) accurately conveys the idea of a collection of tokens used in a model. \n\nThe second candidate, "词元词汇表" (lemma vocabulary), while also relevant, refers specifically to "lemmas," which are the base forms of words, and does not capture the broader concept of "tokens" that can include various forms of words and symbols used in NLP. \n\nThe other candidates, such as "标记词表" (tag vocabulary) and "词元词汇" (lemma vocabulary), do not accurately reflect the intended meaning of "token" in this context, as they imply different concepts that are not synonymous with "token vocabulary." \n\nOverall, "令牌词汇表" maintains semantic accuracy and contextual fit, making it the most appropriate choice for the AI domain.', ['lemma vocabulary', 'Token Glossary', 'tag vocabulary', 'lemma vocabulary', 'token vocabulary'])
token-level Chinese
('词元级', '1. Rank: 词元级, 令牌级别, 词符级\n\n2. Explanation: The term "词元级" (word level) is the best fit for the AI domain-specific usage of "token-level." In the context of natural language processing and machine learning, "token" typically refers to the smallest unit of text that has semantic meaning, which is often a word or a sub-word. The term "词元" directly translates to "word element" or "word token," aligning closely with the concept of tokens in AI. \n\n"令牌级别" (Token level) is a more literal translation but is less commonly used in the AI context, where "token" is often translated as "词元." "词符级" (token level) is also a literal translation but does not capture the semantic nuance as effectively as "词元级." Therefore, "词元级" is the most semantically accurate and contextually appropriate term for the AI domain.', ['Token level', 'word level', 'token level'])
token-level attention Chinese
('令牌级别注意力', '1. Rank: 令牌级别注意力, 词符级注意力, 词元级注意力\n\n2. Explanation: The term "令牌级别注意力" (Token-level attention) is the best fit for the following reasons:\n\n- **Semantic Accuracy**: "令牌" (token) is a direct translation of "token," which is commonly used in the AI and NLP (Natural Language Processing) domains to refer to the smallest unit of text that carries meaning. This term is widely recognized in the context of models like BERT, where "token" refers to the individual pieces of input data that the model processes.\n\n- **Contextual Fit**: The term "令牌级别注意力" accurately reflects the concept of attention mechanisms applied at the token level, which is crucial in understanding how models like BERT analyze and weigh the importance of different tokens in a sequence. The use of "级别" (level) further clarifies that this attention is specifically focused on the granularity of tokens.\n\n- **Back Translation Accuracy**: The back translation of "令牌级别注意力" as "Token level attention" maintains the original meaning and context, ensuring that it aligns with the English term used in the AI literature.\n\nIn contrast, "词元级注意力" (word-level attention) and "词符级注意力" (token-level attention) are less accurate. "词元" (word) can imply a broader unit than a token, which may not capture the specific nuances of tokenization in NLP. "词符" (token) is a less common term in this context and may not be as readily understood by practitioners in the field. Therefore, "令牌级别注意力" is the most appropriate choice for conveying the intended meaning in the AI domain.', ['Token level attention', 'word-level attention', 'token-level attention'])
token-level feature Chinese
('词元级特征', '1. Rank: 词元级特征, 令牌级特征, 令牌级别特征, 词符级特征\n\n2. Explanation: The term "词元级特征" (token-level feature) is the best fit because "词元" accurately translates to "token," which is a key concept in AI and natural language processing, particularly in the context of text analysis. This term maintains the semantic integrity of the original English term and aligns well with the technical jargon used in the AI domain. \n\nThe second candidate, "令牌级特征," while also translating to "token-level feature," uses "令牌," which can imply a broader meaning of "token" in other contexts (like access tokens in computing), making it less precise in the context of linguistic features. \n\nThe third candidate, "令牌级别特征," translates to "token level characteristics," which introduces a slight deviation in meaning by using "级别" (level) instead of "级" (level) and adds unnecessary complexity without enhancing clarity.\n\nLastly, "词符级特征" translates to "symbol-level features," which diverges from the intended meaning of "token" in the context of language processing, as "词符" (symbol) does not capture the specific linguistic nuance of "token." \n\nThus, "词元级特征" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['word-level features', 'Token level characteristics', 'Token level features', 'Symbol-level features'])
top-p sampling Chinese
('前p概率采样', '1. Rank: 前p概率采样, 前p采样, 前p个采样\n\n2. Explanation: The term "前p概率采样" (Top p probability sampling) is the best fit because it accurately captures the essence of the original English term "top-p sampling" by explicitly including the word "概率" (probability), which is crucial in the context of sampling methods in AI. This term conveys that the sampling is based on probabilities, which is a fundamental aspect of how top-p sampling operates—selecting from the top p portion of the probability distribution. \n\nThe second term, "前p采样" (First p sampling), is less precise as it omits the probability aspect, which is essential for understanding the method\'s function in AI applications. The third term, "前p个采样" (First p samples), is also inadequate because it suggests a focus on a fixed number of samples rather than the probabilistic nature of the selection process. Therefore, "前p概率采样" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['first p sampling', 'First p samples', 'Top p probability sampling'])
train / test / dev split Chinese
('训练/测试/验证集划分', '1. Rank: 训练/测试/验证集划分, 训练/测试/开发集划分, 训练/测试/开发划分, 训练/测试/验证划分, 训练/测试/开发集划分, 卷积核\n\n2. Explanation: The term "训练/测试/验证集划分" (Training/test/validation set partitioning) is the best fit because it accurately reflects the common terminology used in the AI and machine learning community. The term "验证" (validation) is widely accepted in the context of model evaluation, as it refers to the subset of data used to tune model parameters and prevent overfitting. This is crucial for understanding experimental results, as it provides clarity on how the model\'s performance is assessed. \n\nIn contrast, "训练/测试/开发集划分" (Training/test/development set partitioning) uses "开发" (development), which is less common in this context and may lead to confusion, as "development" typically refers to a phase rather than a specific dataset split. The other candidates either use "开发" or do not clearly distinguish between validation and testing, which is essential for accurate reporting in AI research. Therefore, the first term is the most semantically accurate and contextually appropriate for the AI domain.', ['Training/test/development set partitioning', 'Training/testing/validation division', 'Training/Testing/Development Division', 'Convolution kernel', 'Training/test/validation set partitioning'])
train-test split Chinese
('训练-测试划分', '1. Rank: 训练-测试划分, 训练-测试集划分, 训练/测试/开发划分, 卷积核\n\n2. Explanation: The term "训练-测试划分" (training-test split) is the best fit because it directly translates the English term while maintaining the specific context of machine learning. The phrase accurately conveys the concept of dividing a dataset into training and testing subsets, which is a fundamental practice in AI and machine learning. The back translation "training-test split" closely matches the original English term, ensuring semantic accuracy.\n\nThe second candidate, "训练-测试集划分" (training-test set partitioning), is also a good option but introduces the word "集" (set), which, while relevant, makes it slightly less concise than the first option. The third candidate, "训练/测试/开发划分" (Training/Testing/Development Division), introduces an additional "开发" (development) component that is not present in the original term, making it less accurate in this specific context. Lastly, "卷积核" (Convolution kernel) is unrelated to the concept of train-test split and is therefore not a suitable candidate.', ['training-test split', 'Training-test set partitioning', 'Training/Testing/Development Division', 'Convolution kernel'])
transformer language model Chinese
('Transformer语言模型', '1. Rank: Transformer语言模型, 变换器语言模型, 转换器语言模型\n\n2. Explanation: The term "Transformer语言模型" is the best fit because it retains the original English term "Transformer," which is a widely recognized and established term in the AI and machine learning community. This term is commonly used in academic literature and industry discussions, making it semantically accurate and contextually appropriate. The back translation also accurately reflects the original term, ensuring clarity and consistency in communication. \n\nThe second option, "变换器语言模型," translates to "transformer language model" but uses "变换器," which is a more generic term for "converter" or "transformer" and may not convey the specific meaning associated with the AI model. The third option, "转换器语言模型," similarly translates to "converter language model," which is less accurate in the context of AI terminology. Therefore, "Transformer语言模型" is the most precise and contextually relevant choice.', ['converter language model', 'transformer language model', 'Transformer language model'])
transformer layer Chinese
('Transformer层', '1. Rank: Transformer层, 变换器层, 转换器层\n\n2. Explanation: The term "Transformer层" is the best fit for the translation of "transformer layer" in the AI domain. This is because "Transformer" is a specific architecture in machine learning, particularly in natural language processing, and it is widely recognized in both English and Chinese as a proper noun. Retaining the original term "Transformer" in the translation ensures semantic accuracy and aligns with the established terminology used in the field. \n\nIn contrast, "变换器层" and "转换器层" translate to "converter layer," which does not accurately reflect the specific function and architecture of the transformer model. These terms could lead to confusion, as they imply a more general concept of conversion rather than the specialized function of the transformer architecture. Therefore, "Transformer层" is the most contextually appropriate and semantically accurate choice for this AI terminology.', ['converter layer', 'converter layer', 'Transformer layer'])
transformer variant Chinese
('"Transformer变体"', '1. Rank: "Transformer变体", "变换器变体", "转换器变体"\n\n2. Explanation: The term "Transformer变体" is the best fit because it retains the original term "Transformer," which is a well-established concept in the AI domain, particularly in natural language processing and deep learning. The back translation "Transformer variant" accurately reflects the original English term, ensuring semantic accuracy. \n\nIn contrast, "变换器变体" and "转换器变体" translate to "Converter variant," which does not capture the specific meaning of "transformer" in the context of AI. The term "converter" is too generic and could refer to various types of conversion processes, thus losing the specificity and technicality associated with the transformer architecture. Therefore, "Transformer变体" is the most contextually appropriate choice for conveying the intended meaning in the AI field.', ['Converter variants', 'Converter variant', 'Transformer variant'])
transformer-based architecture Chinese
('基于Transformer的架构', '1. Rank: 基于Transformer的架构, 基于变换器的架构, 基于转换器的架构\n\n2. Explanation: The term "基于Transformer的架构" is the best fit because it retains the original term "Transformer," which is widely recognized in the AI community and specifically refers to a type of model architecture used in natural language processing and other machine learning tasks. The back translation accurately reflects the original English term, maintaining its semantic integrity. \n\nIn contrast, "基于变换器的架构" and "基于转换器的架构" translate "Transformer" into more generic terms ("变换器" and "转换器"), which may not convey the specific meaning associated with the Transformer architecture in AI. These terms could lead to confusion, as they do not have the same established recognition in the field. Therefore, the first translation is preferred for its accuracy and contextual fit within the AI domain.', ['Converter-based architecture', 'Converter-based architecture', 'Transformer-based architecture'])
transition-based dependency parsing Chinese
('基于转移的依赖解析', '1. Rank: 基于转移的依赖解析, 基于转移的依存句法分析, 基于转换的依赖解析\n\n2. Explanation: The term "基于转移的依赖解析" (Back translation: "Transfer-based dependency resolution") is the best fit for "transition-based dependency parsing" in the AI domain. The use of "转移" (transfer) aligns closely with the concept of "transition" in the context of parsing algorithms, which often involve moving between states or configurations during the parsing process. This term accurately captures the dynamic nature of the parsing method described in the context.\n\nThe second candidate, "基于转移的依存句法分析" (Back translation: "Transfer-based dependency syntax analysis"), is also a strong contender, but it introduces "句法分析" (syntax analysis), which is broader and may not specifically convey the transition-based aspect as effectively as "依赖解析" (dependency resolution).\n\nThe third candidate, "基于转换的依赖解析" (Back translation: "Transformation-based dependency resolution"), uses "转换" (transformation), which can imply a different meaning in the context of parsing, potentially leading to confusion with transformation-based models that are not specifically transition-based. Therefore, while all translations are somewhat accurate, "基于转移的依赖解析" is the most semantically precise and contextually appropriate for the AI terminology in question.', ['Transformation-based dependency resolution', 'Transfer-based dependency resolution', 'Transfer-based dependency syntax analysis'])
transition-based parsing Chinese
('基于转移的解析', '1. Rank: 基于转移的解析, 基于转移的句法分析, 基于转换的解析\n\n2. Explanation: The term "基于转移的解析" (Back translation: "Transfer-based parsing") is the best fit for "transition-based parsing" in the AI domain, particularly in natural language processing (NLP). The term "转移" (transfer) closely aligns with the concept of transitions in parsing, which refers to the movement from one state to another during the parsing process. This is a common terminology used in parsing algorithms, making it semantically accurate and contextually relevant.\n\nThe second candidate, "基于转移的句法分析" (Back translation: "Transfer-based parsing"), while also accurate, adds "句法" (syntactic), which may not be necessary in the context of transition-based parsing, as the term "解析" (parsing) already implies a focus on syntax. \n\nThe third candidate, "基于转换的解析" (Back translation: "Transformation-based parsing"), introduces "转换" (transformation), which can imply a different meaning and is less commonly used in the context of parsing compared to "转移" (transfer). Therefore, it is less suitable for the specific AI terminology in question. \n\nOverall, "基于转移的解析" is the most precise and contextually appropriate translation for "transition-based parsing" in the field of AI and NLP.', ['Transformation-based parsing', 'Transfer-based parsing', 'Transfer-based parsing'])
trifocal tensor Chinese
('三焦张量', '1. Rank: 三焦张量, 三聚焦张量, 三聚张量\n\n2. Explanation: The term "三焦张量" (trifocal tensor) is the best fit for the following reasons: \n\n- **Semantic Accuracy**: "三焦" directly translates to "trifocal," which accurately reflects the concept of a tensor that relates to three focal points. This term is widely recognized in the field of computer vision and image processing, where trifocal tensors are commonly discussed.\n\n- **Back Translation Accuracy**: The back translation of "三焦张量" as "trifocal tensor" maintains the original meaning without introducing ambiguity. In contrast, "三聚焦张量" (Three focused tensors) suggests multiple tensors rather than a single trifocal tensor, which could lead to confusion in the context of the paper. "三聚张量" (Trimeric tensor) is also less accurate as it introduces a term ("trimeric") that is not standard in the AI domain and could mislead readers regarding the nature of the tensor.\n\nOverall, "三焦张量" is the most contextually appropriate and semantically accurate term for the AI domain, particularly in discussions surrounding tensors in computer vision.', ['Three focused tensors', 'trifocal tensor', 'Trimeric tensor'])
tuplex Chinese
('"元组扩展"', '1. Rank: "元组扩展", "元组组", "元组x"\n\n2. Explanation: The term "元组扩展" (tuple extension) is the best fit for the translation of "tuplex" in the AI context. This is because "tuplex" seems to imply a concept that extends or builds upon the traditional notion of a tuple, which is a fundamental data structure in computer science and AI. The term "扩展" (extension) captures this idea of enhancement or expansion effectively. \n\nIn contrast, "元组组" (tuple group) suggests a collection of tuples, which does not convey the specific meaning of "tuplex" as an advanced or modified tuple structure. "元组x" (tuplex) is a direct transliteration and does not provide any semantic clarity or contextual fit, making it less suitable for understanding the term\'s application in AI. Therefore, "元组扩展" is the most semantically accurate and contextually appropriate choice.', ['tuple group', 'tuplex', 'tuple extension'])
two-player zero-sum game Chinese
('"双人零和博弈"', '1. Rank: "双人零和博弈", "两人零和博弈", "二人零和博弈", "双人零和游戏"\n\n2. Explanation: The term "双人零和博弈" is the best fit for the translation of "two-player zero-sum game" in the AI domain. This term accurately captures the essence of the original English term, maintaining both semantic accuracy and contextual relevance. The word "博弈" (bó yì) is commonly used in game theory and conveys the concept of a strategic interaction, which is essential in the context of zero-sum games. \n\nWhile "两人零和博弈" and "二人零和博弈" are also accurate, they are less commonly used in academic literature compared to "双人零和博弈." The term "双人" (two-person) is more standard in the context of game theory, making it more recognizable to the target audience. \n\n"双人零和游戏" is less preferred because "游戏" (game) can imply a more casual or recreational context, which may not fully capture the formal and strategic nature of the term "博弈" in academic discussions. Therefore, "双人零和博弈" is the most appropriate choice for conveying the intended meaning in the AI domain.', ['Two-person zero-sum game', 'Two player zero sum game', 'Two-player zero-sum game', 'Two-person zero-sum game'])
unary potential Chinese
('一元势能', '1. Rank: 一元势能, 一元势, 一元潜力\n\n2. Explanation: The term "一元势能" (Unary potential energy) is the best fit for the AI domain-specific usage because it accurately captures the concept of "unary potential" in a way that aligns with established terminology in fields such as machine learning and computer vision. The term "势能" (potential energy) conveys the idea of a measure or score associated with a single entity (in this case, a vowel or pixel), which is essential in understanding how unary potentials function in models. \n\nThe second candidate, "一元势" (One yuan potential), while it retains the "unary" aspect, introduces ambiguity by using "元" (yuan), which can refer to the currency in Chinese, thus detracting from the intended meaning in the context of AI. \n\nThe third candidate, "一元潜力" (One dollar potential), is the least appropriate as "潜力" (potential) does not convey the same technical meaning as "势能" (potential energy) in this context, and it also introduces confusion with the term "dollar," which is irrelevant to the concept being described. \n\nOverall, "一元势能" is the most semantically accurate and contextually appropriate translation for "unary potential" in the AI domain.', ['Unary potential energy', 'One yuan potential', 'One dollar potential'])
union of conjunctive query Chinese
('合取查询的并集', '1. Rank: 合取查询的并集, 合取查询的并, 连接查询的并集, 联合连接查询, 连结查询的并集\n\n2. Explanation: The term "合取查询的并集" (Union of conjunctive queries) is the best fit because it accurately reflects the original English term both semantically and contextually. "合取" directly translates to "conjunctive," which is crucial in the context of database queries, as it refers to the logical conjunction used in forming the queries. The phrase "的并集" translates to "union," which is also essential in describing the operation being performed on the conjunctive queries. \n\nThe back translation of "合取查询的并集" aligns perfectly with the original term, maintaining the intended meaning without introducing ambiguity. \n\nIn contrast, "连接查询的并集" (Union of join queries) introduces the term "连接" (join), which is not synonymous with "合取" (conjunctive) and could lead to confusion in the context of AI and database terminology. The other candidates either lack clarity or do not accurately represent the specific type of query being discussed, making them less suitable for the AI domain.', ['Union of join queries', 'Union of conjunctive queries', 'Union join query', 'union of join queries', 'Union of conjunctive queries'])
utterance Chinese
('话语', '1. Rank: 话语, 发话, 发声, 效用函数\n\n2. Explanation: The term "话语" (back translation: "Discourse") is the best fit for the AI domain-specific usage of "utterance" because it accurately captures the concept of a sequence of tokens or spoken language directed from a user to a dialog system. In the context of natural language processing and dialogue systems, "话语" conveys the idea of a meaningful unit of speech or text, which aligns well with the definition provided in the context. \n\n"发话" (back translation: "speak") is a close second, as it implies the act of speaking but lacks the nuance of a structured sequence of tokens. "发声" (back translation: "speak out") also focuses on the act of producing sound rather than the content or structure of the utterance itself. Lastly, "效用函数" (back translation: "utility function") is unrelated to the concept of "utterance" and is therefore not a suitable candidate in this context. \n\nOverall, "话语" is the most semantically accurate and contextually appropriate term for "utterance" in the AI domain.', ['speak', 'Discourse', 'speak out', 'utility function'])
vanilla Transformer Chinese
('原始Transformer', '1. Rank: 原始Transformer, 基础 Transformer, 基础变换器, 香草变压器, オリジナルのトランス\n\n2. Explanation: The term "原始Transformer" (Original Transformer) is the best fit because it accurately conveys the concept of the "vanilla Transformer" as the basic or standard version of the Transformer model without any modifications or enhancements. In the context of AI and machine learning, "vanilla" often refers to the most straightforward or unaltered version of a model, which aligns well with the meaning of "原始" (original). \n\n"基础 Transformer" (Basic Transformer) is also a strong candidate, as it suggests a foundational model, but "原始" captures the essence of being the unmodified version more effectively. "基础变换器" (Basic Converter) is less suitable because "变换器" (converter) does not accurately reflect the function of the Transformer model in AI. \n\n"香草变压器" (vanilla transformer) is a direct translation of "vanilla," but it may not be widely recognized in the AI community and could lead to confusion. Lastly, "オリジナルのトランス" (original trans) is a transliteration that does not fit the context of AI terminology and is not appropriate for this specific usage. Thus, "原始Transformer" is the most semantically accurate and contextually fitting choice.', ['OriginalTransformer', 'basic converter', 'Basic Transformer', 'オリジナルのトランス', 'vanilla transformer'])
verbalizer Chinese
('语言生成器', '1. Rank: 语言生成器, 词语化器, 语言化器, 言语者\n\n2. Explanation: The term "语言生成器" (language generator) is the best fit for the English term "verbalizer" in the context of AI and natural language processing. This is because "语言生成器" accurately conveys the function of generating language outputs based on input data, which aligns with the role of a verbalizer in mapping outputs to tokens. \n\nThe other candidates, such as "词语化器" (tokenizer), "语言化器" (linguist), and "言语者" (speaker), do not capture the specific function of generating language in the same way. "词语化器" suggests a focus on tokenization rather than generation, which is not the intended meaning in this context. "语言化器" implies a more general linguistic function, and "言语者" refers to a speaker, which does not relate to the AI function of generating language outputs. Therefore, "语言生成器" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['language generator', 'tokenizer', 'linguist', 'speaker'])
word dropout Chinese
('词丢弃', '1. Rank: 词丢弃, 单词丢弃, 词语丢弃, 单词丢失, 词丢失\n\n2. Explanation: The term "词丢弃" (word discard) is the best fit for "word dropout" in the AI context because it accurately captures the concept of intentionally removing words from a dataset or computation process, which aligns with the technical meaning of "dropout" in machine learning. The term "丢弃" (discard) conveys the idea of omitting or excluding something, which is precisely what is happening in the context of word dropout. \n\n"单词丢弃" (word discard) and "词语丢弃" (word discard) are also acceptable, but "词丢弃" is more concise and commonly used in technical literature. \n\nOn the other hand, "单词丢失" (word missing) and "词丢失" (word missing) imply a loss rather than a deliberate action of dropping words, which does not accurately reflect the intended meaning in the context of dropout techniques in AI. Therefore, they are less suitable for this specific terminology.', ['word discard', 'word discard', 'word discard', 'word missing', 'word missing'])
word surprisal Chinese
('词语惊讶度', '1. Rank: 词语惊讶度, 词语惊异度, 词语意外度, 词语意外性, 词意惊讶度\n\n2. Explanation: The term "词语惊讶度" (word surprisal) is the best fit because it accurately captures the concept of "surprisal" in the context of language processing and AI. The term "惊讶" (surprise) directly relates to the statistical measure of how unexpected a word is in a given context, which aligns with the definition of word surprisal in information theory and natural language processing. \n\nThe back translation "word surprise" is also semantically accurate, maintaining the intended meaning. \n\n"词语惊异度" (word surprise) is a close second, as "惊异" also conveys a sense of surprise, but it is less commonly used in the AI context compared to "惊讶." \n\nThe other candidates, such as "词语意外度" (word unexpectedness) and "词语意外性" (word unexpectedness), while they convey a related concept, do not align as closely with the established terminology in the AI domain. "意外" (unexpected) lacks the specific nuance of "surprisal" that is critical in this context. \n\nOverall, "词语惊讶度" is the most contextually appropriate and semantically accurate term for "word surprisal" in the AI field.', ['word surprise', 'word unexpectedness', 'word meaning surprise', 'word surprise', 'word unexpectedness'])
word token Chinese
('词元', '1. Rank: 词元, 词语标记, 词符\n\n2. Explanation: The term "词元" (word element) is the best fit for "word token" in the AI domain because it accurately captures the concept of a discrete unit of text that is processed in natural language processing tasks. In the context of AI and machine learning, "词元" is commonly used to refer to the basic units of text that algorithms analyze, making it semantically accurate and contextually appropriate. \n\nOn the other hand, "词语标记" (word tag) suggests a focus on tagging or labeling rather than the fundamental unit of text, which may lead to confusion in the context of the provided sentences. "词符" (symbol) is too vague and does not convey the specific meaning of a token in natural language processing, as it could refer to any symbol rather than a word unit. Therefore, "词元" is the most precise and contextually relevant choice for the term "word token" in this AI context.', ['word tag', 'word element', 'Symbol'])
word-level Chinese
('词级', '1. Rank: 词级, 词级别, 词级/词层级, 单词级别, 单样本学习\n\n2. Explanation: The term "词级" is the best fit for the translation of "word-level" in the context of AI and translation quality estimation. This term is concise and directly conveys the concept of "word level" without unnecessary complexity. It is commonly used in computational linguistics and AI contexts, making it semantically accurate and contextually appropriate. \n\n"词级别" is also a good candidate, but it is slightly longer and less commonly used in the specific context of AI. "词级/词层级" introduces redundancy with the use of both "级" and "层级," which can lead to confusion. "单词级别" translates to "single word level," which may imply a different meaning than intended. Lastly, "单样本学习" translates to "one-sample learning," which is unrelated to the concept of "word-level" and is therefore not a suitable candidate. \n\nOverall, "词级" is the most precise and contextually relevant term for the AI domain, particularly in the context of translation quality estimation and phonetic learning tasks.', ['word level', 'word level', 'word level/word level', 'word level', 'One-sample learning'])
word-level vocabulary Chinese
('词级词汇表', '1. Rank: 词级词汇表, 词级词汇, 单词级词汇表, 词汇表, 单样本设置\n\n2. Explanation: The term "词级词汇表" (word-level vocabulary) is the best fit because it accurately captures the concept of a vocabulary that is organized at the word level, which is essential in the context of natural language processing and AI. The inclusion of "表" (table or list) emphasizes that it refers to a structured collection of words, aligning well with the technical usage in AI literature. \n\n"词级词汇" (word-level vocabulary) is also a strong candidate, but it lacks the explicit indication of a structured list, which is implied by "词汇表." "单词级词汇表" (single word-level vocabulary) is less preferred because it introduces ambiguity by suggesting a focus on individual words rather than a comprehensive vocabulary. "词汇表" (glossary) is too general and does not specify the word-level aspect, while "单样本设置" (single sample setup) is unrelated to the term in question. Thus, "词级词汇表" is the most semantically accurate and contextually appropriate choice for the AI domain.', ['word level vocabulary', 'word-level vocabulary', 'word-level vocabulary', 'Glossary', 'Single sample setup'])