Apriori Chinese
('先验算法', '1. Rank: 先验算法, 先驗, Apriori, 阿普里奥\n\n2. Explanation: The term "先验算法" (a priori algorithm) is the best fit for the AI domain-specific usage because it explicitly conveys the concept of an algorithm that operates on prior knowledge or assumptions, which is central to the Apriori algorithm in data mining. The term "先验" (a priori) is also accurate but lacks the specificity of "算法" (algorithm), which is crucial in this context. "Apriori" as a direct transliteration does not provide any semantic clarity to those unfamiliar with the term, and "阿普里奥" (Aprilio) is a less formal transliteration that may not be recognized in academic or technical discussions. Therefore, "先验算法" is the most semantically accurate and contextually appropriate choice for conveying the intended meaning in the AI domain.', ['a priori algorithm', 'a priori', 'A priori', 'Aprilio'])
Apriori algorithm Chinese
('Apriori算法', '1. Rank: Apriori算法, Apriori演算法, 先验算法, 先驗演算法, Apriori 算法, 先验i算法, 算法\n\n2. Explanation: The term "Apriori算法" is the best fit because it maintains the original name of the algorithm while clearly indicating that it is an algorithm (算法) in Chinese. This is important in the AI domain, where specific algorithms often retain their original names for clarity and recognition. The use of "Apriori" in its original form helps to avoid confusion with other terms and ensures that readers familiar with the field can easily identify the algorithm being discussed. \n\n"Apriori演算法" is also a strong candidate, as it includes the term "演算法," which means "algorithm" in a more formal context, but it is less commonly used than "算法." \n\nThe term "先验算法" translates to "a priori algorithm," which is semantically accurate but less specific to the algorithm\'s name, making it less recognizable in the context of data mining. \n\n"先驗演算法" is similar to "先验算法" but uses a different character for "prior," which may not be as widely recognized in this context. \n\n"Apriori 算法" is acceptable but less preferred due to the space between "Apriori" and "算法," which can make it seem less cohesive as a single term.\n\n"先验i算法" is not a standard term and introduces an unnecessary character, making it less suitable. \n\nFinally, "算法" alone is too vague and does not convey the specific algorithm being referenced.', ['a priori algorithm', 'Apriori algorithm', 'a priori algorithm', 'a priori algorithm', 'Apriori algorithm', 'algorithm', 'Apriori algorithm'])
Azuma-Hoeffding inequality Chinese
('阿祖马-霍夫丁不等式', '1. Rank: 阿祖马-霍夫丁不等式, Azuma-Hoeffding不等式, Azuma-Hoeffding 不等式, 阿祖玛-霍夫丁不等式, 阿祖玛-赫夫丁不等式, 安祖马-霍夫丁不等式, 不等式\n\n2. Explanation: The best fit is "阿祖马-霍夫丁不等式" because it accurately reflects the original English term "Azuma-Hoeffding inequality" in both semantic and phonetic aspects. The term maintains the integrity of the names "Azuma" and "Hoeffding," which are crucial for proper identification in the AI and statistical context. The use of "不等式" (inequality) is standard in mathematical terminology, ensuring clarity. Other candidates, such as "阿祖玛-霍夫丁不等式" and "阿祖玛-赫夫丁不等式," introduce variations in the name that could lead to confusion, as they deviate from the established nomenclature. The term "Azuma-Hoeffding不等式" lacks the necessary spacing, which is less conventional in Chinese writing. Overall, "阿祖马-霍夫丁不等式" is the most contextually appropriate and semantically accurate choice for the AI domain.', ['Azuma-Hoffding inequality', 'Azuma-Hoeffding inequality', 'Azuma-Hoeffding inequality', 'Azuma-Hoffding inequality', 'inequality', 'Ajumma-Hefting inequality', 'Anzuma-Hoffding inequality'])
Bhattacharyya coefficient Chinese
('巴塔查尔雅系数', '1. Rank: 巴塔查尔雅系数, 巴塔查里亚系数, 巴塔恰里亚系数, 巴哈查里亚系数, 巴氏系数, Bhattacharyya 系数 / 巴氏系数\n\n2. Explanation: The term "巴塔查尔雅系数" (Bhattacharyya coefficient) is the best fit because it closely matches the phonetic pronunciation of the original English term while maintaining semantic accuracy. This is crucial in the AI domain, where precise terminology is essential for clear communication and understanding. The other candidates, while they may be phonetically similar, either deviate from the standard transliteration or introduce ambiguity. For instance, "巴氏系数" (Babbitt coefficient) is misleading as it refers to a different concept entirely. The use of "巴塔查尔雅系数" ensures that the term is recognized in the context of statistical measures and machine learning, aligning with the established terminology in the field.', ['Babbitt coefficient', 'Bhattacharya coefficient', 'Bhattacharya coefficient', 'Bhattacharya coefficient', 'Bhattacharyya coefficient / Bhattacharyya coefficient', 'bhacharya coefficient'])
Boltzmann distribution Chinese
('玻尔兹曼分布', '1. Rank: 玻尔兹曼分布, 波尔兹曼分布, 博尔兹曼分布, Boltzmann分布, 分布, 受保护属性\n\n2. Explanation: The term "玻尔兹曼分布" is the most widely accepted and semantically accurate translation of "Boltzmann distribution" in the context of statistical mechanics and AI. It accurately reflects the original term\'s meaning and is commonly used in academic literature. The back translation confirms its accuracy as it translates back to "Boltzmann distribution," maintaining the original term\'s integrity.\n\nThe second candidate, "波尔兹曼分布," while also a valid translation, is less commonly used in the field and may lead to confusion. The third candidate, "博尔兹曼分布," is similar but not as prevalent as "玻尔兹曼分布." The fourth candidate, "Boltzmann分布," mixes English and Chinese, which is not ideal for formal contexts. The fifth candidate, "分布," is too vague and does not convey the specific concept of the Boltzmann distribution. Lastly, "受保护属性" translates to "protected attribute," which is entirely unrelated to the original term and thus irrelevant in this context. \n\nOverall, "玻尔兹曼分布" is the best fit due to its semantic accuracy, contextual relevance, and established usage in the AI domain.', ['Boltzmann distribution', 'Boltzmann distribution', 'Boltzmann distribution', 'distributed', 'Boltzmann distribution', 'protected attribute'])
Boltzmann exploration Chinese
('玻尔兹曼探索', '1. Rank: 玻尔兹曼探索, 波尔兹曼探索, 博尔兹曼探索, Boltzmann探索, 探索, 蛋白质折叠\n\n2. Explanation: The term "玻尔兹曼探索" is the best fit for "Boltzmann exploration" because it accurately reflects both the semantic meaning and the context of the term within the AI domain. "玻尔兹曼" is the standard transliteration of "Boltzmann" in Chinese, which is widely recognized in scientific literature, particularly in physics and AI. The term "探索" translates directly to "exploration," maintaining the original meaning. \n\nThe second candidate, "波尔兹曼探索," is also a valid transliteration but is less commonly used in the context of AI, making it slightly less recognizable. "博尔兹曼探索" is another acceptable option, but it is less frequently encountered in academic discussions related to AI. \n\n"Boltzmann探索" is a direct mix of English and Chinese, which is not ideal for formal terminology. "探索" alone is too vague and does not convey the specific concept of "Boltzmann exploration." Lastly, "蛋白质折叠" (protein folding) is completely unrelated to the term in question and should not be considered a candidate. \n\nOverall, "玻尔兹曼探索" stands out as the most contextually appropriate and semantically accurate translation for use in AI literature.', ['Boltzmann exploration', 'Boltzmann Exploration', 'BoltzmannExplore', 'explore', 'Bolzeman Discovery', 'protein folding'])
Bonferroni correction Chinese
('邦费罗尼校正', '1. Rank: 邦费罗尼校正, 邦弗朗尼校正, 本费罗尼校正, Bonferroni校正, 校正, 原型嵌入\n\n2. Explanation: The term "邦费罗尼校正" is the best fit for the translation of "Bonferroni correction" because it closely matches the phonetic structure of the original term while maintaining semantic accuracy. This term is widely recognized in statistical contexts within the Chinese-speaking academic community, particularly in fields related to statistics and data analysis, which are relevant to AI. \n\nThe second candidate, "邦弗朗尼校正," is also a reasonable option but slightly deviates from the standard usage, as "费罗尼" is more commonly accepted. The third candidate, "本费罗尼校正," introduces a phonetic variation that could lead to confusion, as "本" does not have a direct correlation with the original term. \n\nThe fourth candidate, "Bonferroni校正," retains the English term but is less preferable in formal Chinese writing, where a fully translated term is typically favored. The last two candidates, "校正" and "原型嵌入," are not suitable translations for "Bonferroni correction" as they do not convey the specific statistical meaning of the term. \n\nOverall, "邦费罗尼校正" is the most contextually appropriate and semantically accurate translation for use in AI and statistical discussions.', ['Bonferroni correction', 'Bonferroni correction', 'Benferroni correction', 'Bonferroni correction', 'Correction', 'prototype embedding'])
Borda scores Chinese
('博尔达得分', '1. Rank: 博尔达得分, 博尔达分数, Borda得分, 博达得分, 博达分数, 博尔达评分, 博尔达评分\n\n2. Explanation: The term "博尔达得分" is the best fit for the translation of "Borda scores" in the AI domain due to its semantic accuracy and contextual relevance. "得分" directly translates to "scores," which aligns perfectly with the original term\'s meaning. This term is widely recognized in academic and technical contexts, particularly in decision-making and voting systems, where Borda count methods are applied. \n\n"博尔达分数" is also a strong candidate, as "分数" can mean "score" or "points," but it is slightly less common in usage compared to "得分." \n\nThe term "Borda得分" retains the original English term, which may be useful in contexts where the audience is familiar with English terminology, but it lacks the full Chinese context that "博尔达得分" provides.\n\nThe other candidates, such as "博达得分" and "博达分数," introduce a variation in the name that could lead to confusion, as "博达" does not accurately represent "Borda." Similarly, "博尔达评分" and "博尔达评分" use "评分," which translates to "rating," a term that does not fit the context of scoring in decision-making processes. \n\nOverall, "博尔达得分" is the most semantically accurate and contextually appropriate term for "Borda scores" in the AI domain.', ['Borda scores', 'Borda score', 'Borda rating', 'boda score', 'boda score', 'Borda score'])
Branch and Bound Chinese
('分支限界法', '1. Rank: 分支限界法, 分支定界法, 分支定界\n\n2. Explanation: The term "分支限界法" (branch and bound method) is the best fit for the AI domain-specific usage because it accurately conveys the method\'s procedural nature by including "法" (method), which is commonly used in mathematical and algorithmic contexts in Chinese. This term is widely recognized in the field of optimization and operations research, making it semantically accurate and contextually appropriate. \n\nThe second candidate, "分支定界法" (branch and bound), while also a method, uses "定界" (bound) instead of "限界" (limit), which can lead to confusion as "定界" is less commonly used in this context. The third candidate, "分支定界," lacks the "法" (method) suffix, making it less clear that it refers to a specific algorithmic approach. Therefore, "分支限界法" is the most precise and contextually relevant translation for the term "Branch and Bound" in the AI domain.', ['branch and bound method', 'branch and bound', 'branch and bound'])
Bundle adjustment Chinese
('捆绑调整', '1. Rank: 捆绑调整, 捆集调整/束调整, 束调整, 捆束调整, 光束平差法, 光束法平差\n\n2. Explanation: The term "捆绑调整" (bundle adjustment) is the best fit for the translation of "Bundle adjustment" in the AI domain, particularly in the context of computer vision and structure from motion (SfM). This term is widely recognized in both academic and practical applications within the field, making it semantically accurate and contextually appropriate. \n\nThe back translation of "捆绑调整" as "bundle adjustment" aligns perfectly with the original English term, ensuring clarity and consistency in communication among professionals in the field. \n\nIn contrast, "束调整" (beam adjustment) and "光束平差法" (beam adjustment method) introduce ambiguity, as "束" and "光束" refer to "beam," which is not relevant in this context. "捆束调整" (Bundle adjustment) and "捆集调整/束调整" (Bundle adjustment/bundle adjustment) are less preferred due to their redundancy and lack of clarity. \n\nOverall, "捆绑调整" is the most accurate and contextually fitting term for "Bundle adjustment" in the AI domain, particularly in the context of refining 3D structures and camera poses.', ['beam adjustment', 'bundle adjustment', 'Bundle adjustment', 'Bundle adjustment/bundle adjustment', 'beam adjustment method', 'Beam method adjustment'])
Caltech-101 Chinese
('Caltech-101数据集', '1. Rank: Caltech-101数据集, 加州理工学院101数据集, 加州理工学院101, Caltech-101\n\n2. Explanation: The first translation candidate, "Caltech-101数据集" (Caltech-101 dataset), is the best fit because it maintains the original term "Caltech-101" while adding "数据集" (dataset) to specify that it refers to a dataset in the context of image classification. This is semantically accurate and contextually appropriate, as it directly relates to the AI domain where datasets are commonly referenced. \n\nThe second candidate, "加州理工学院101数据集" (Caltech Institute 101 dataset), while accurate, is longer and less commonly used in the AI community, where the abbreviation "Caltech" is widely recognized. The third candidate, "加州理工学院101" (Caltech Institute 101), omits the term "数据集," which is crucial for understanding the context of the term as it relates to datasets. The last candidate, "Caltech-101," retains the original English term but lacks any contextual clarification, making it less suitable for readers who may not be familiar with the term. Thus, the first candidate is the most effective for conveying the intended meaning in the AI domain.', ['Caltech-101 dataset', 'Caltech 101 Dataset', 'Caltech 101', 'Caltech-101'])
Chamfer Distance Chinese
('查姆弗距离', '1. Rank: 查姆弗距离, 倒角距离, 切削距离, 切边距离, 钱弗距离, 香农距离, 切夫距离\n\n2. Explanation: The term "查姆弗距离" (Chá mǔ fú jù lí) is the most accurate translation for "Chamfer Distance" in the context of AI and 3D geometry. This term closely resembles the phonetic pronunciation of "Chamfer" while maintaining a clear semantic connection to the original term. It is commonly used in academic and technical literature related to 3D reconstruction and computer graphics, making it contextually appropriate.\n\n"倒角距离" (Dào jiǎo jù lí), while it translates to "chamfer distance" in a literal sense, is less commonly used in the AI domain and may not be immediately recognized by professionals in the field. The other candidates, such as "切削距离" (cutting distance) and "香农距离" (Shannon distance), are misleading as they refer to different concepts entirely, which could lead to confusion in technical discussions. Therefore, "查姆弗距离" is the best fit due to its established usage in the relevant literature and its phonetic alignment with the original term.', ['chamfer distance', 'cutting distance', 'Chandler distance', 'Shannon distance', 'tangent distance', 'chamfer distance', 'Cheff distance'])
Charniak parser Chinese
('查尼亚克解析器', '1. Rank: 查尼亚克解析器, Charniak解析器, 查尔尼亚克解析器, 查尼亞克解析器, 查尼亚克分析器, Charniak 解析器\n\n2. Explanation: The term "查尼亚克解析器" is the best fit for the following reasons:\n\n- **Semantic Accuracy**: "查尼亚克解析器" accurately captures the name "Charniak" while using the term "解析器," which is the standard term for "parser" in the context of computational linguistics and AI. This term is widely recognized in the field and aligns with the technical usage of "parser" in Chinese.\n\n- **Back Translation Accuracy**: The back translation of "查尼亚克解析器" returns "Charniak parser," which maintains the original meaning without introducing any errors or ambiguities. This is crucial for technical terms where precision is necessary.\n\n- **Contextual Fit**: In the AI domain, "解析器" (parser) is a commonly used term that conveys the function of analyzing and interpreting data structures, which is exactly what the Charniak parser does. Other candidates, such as "查尼亚克分析器," use "分析器" (analyzer), which is less specific and not typically used to refer to parsing in this context.\n\nOverall, "查尼亚克解析器" is the most appropriate choice due to its accuracy, clarity, and alignment with established terminology in the AI and computational linguistics fields.', ['Chargnak parser', 'Charniak parser', 'Chargnak parser', 'Chargnac Analyzer', 'Charnyak parser', 'Charniak parser'])
Cholesky decomposition Chinese
('乔尔斯基分解', '1. Rank: 乔尔斯基分解, 乔列斯基分解, Cholesky 分解, 楚尔斯基分解\n\n2. Explanation: The term "乔尔斯基分解" is the most widely accepted and recognized translation for "Cholesky decomposition" in the AI and mathematical communities in Chinese-speaking regions. It accurately reflects the phonetic pronunciation of "Cholesky" while maintaining semantic clarity. The back translation to "Cholsky decomposition" closely aligns with the original term, ensuring that it is easily understood in context.\n\nThe second candidate, "乔列斯基分解," is also a valid translation but is less commonly used, which may lead to confusion among practitioners familiar with the standard terminology. The third candidate, "Cholesky 分解," retains the original English term but lacks the contextual fit that a fully translated term provides, making it less suitable for formal writing. Lastly, "楚尔斯基分解" is a less accurate phonetic representation and could lead to misinterpretation, making it the least favorable option. Thus, "乔尔斯基分解" stands out as the best fit for its semantic accuracy and contextual relevance in the AI domain.', ['Cholsky decomposition', 'Choleski decomposition', 'Cholesky decomposition', 'Chulsky decomposition'])
Cholesky factor Chinese
('乔尔斯基因子', '1. Rank: 乔尔斯基因子, 乔列斯基因子, Cholesky 因子, 楚尔斯基因子\n\n2. Explanation: The term "乔尔斯基因子" (Joel gene) is the best fit because it closely resembles the phonetic structure of "Cholesky" while also being a commonly accepted translation in the mathematical and AI domains. It maintains semantic accuracy and is widely recognized among professionals in the field. \n\nThe second candidate, "乔列斯基因子" (Choleski factor), is also a reasonable option, but it is less commonly used than the first. The third candidate, "Cholesky 因子" (Cholesky factor), retains the original English term but lacks the contextual fit that a fully translated term provides in Chinese literature. Lastly, "楚尔斯基因子" (Chulsky) is not a suitable choice as it deviates significantly from the original term and does not convey the intended meaning in the context of Cholesky factorization. \n\nIn the AI domain, where precision and clarity are crucial, the first term is preferred for its established usage and recognition among practitioners.', ['Joel gene', 'Choleski factor', 'Cholesky factor', 'Chulsky'])
Cholesky factorization Chinese
('乔列斯基分解', '1. Rank: 乔列斯基分解, Cholesky 分解法, 乔尔斯基分解法, 乔尔斯基因子分解, 楚尔斯基分解, 乔尔斯基分解法c\n\n2. Explanation: The term "乔列斯基分解" (Cholesky decomposition) is the most accurate translation for "Cholesky factorization" in the context of AI and numerical methods. This term is widely recognized in mathematical literature and aligns closely with the original English term, maintaining both semantic accuracy and contextual fit. \n\nThe term "Cholesky 分解法" (Cholesky decomposition method) is also a strong candidate, as it accurately conveys the method aspect of the factorization, but it is slightly less concise than "乔列斯基分解." \n\nThe other candidates, such as "乔尔斯基分解法" and "乔尔斯基因子分解," introduce variations that are not standard in the field, which could lead to confusion. "楚尔斯基分解" and "乔尔斯基分解法c" are less common and may not be recognized by professionals in the AI domain. \n\nOverall, "乔列斯基分解" is the best fit due to its established usage in mathematical contexts, ensuring clarity and precision in communication within the AI domain.', ['Choleski decomposition', 'Cholsky decomposition method', "Joel's gene decomposition", 'Cholsky decomposition method c', 'Cholesky decomposition method', 'Chulsky decomposition'])
Chu-Liu-Edmonds algorithm Chinese
('朱-刘-爱德蒙兹算法', '1. Rank: 朱-刘-爱德蒙兹算法, 朱-刘-埃德蒙兹算法, 朱柳-埃德蒙兹算法, 朱刘埃德蒙兹算法, Chu-Liu-Edmonds算法, Chu-Liu-Edmonds 算法, 楚-刘-埃德蒙兹算法\n\n2. Explanation: The first candidate, "朱-刘-爱德蒙兹算法" (Zhu-Liu-Aidemonz algorithm), is the best fit because it maintains the original names of the algorithm\'s creators (Zhu and Liu) while providing a phonetic representation of "Edmonds" that is widely recognized in the Chinese academic community. The use of "爱德蒙兹" (Aidemonz) is a common transliteration for "Edmonds," making it semantically accurate and contextually appropriate for the AI domain. \n\nThe second candidate, "朱-刘-埃德蒙兹算法" (Zhu-Liu-Eidemonz algorithm), is also a strong contender, but "埃德蒙兹" (Eidemonz) is less commonly used than "爱德蒙兹" (Aidemonz) in academic literature, which may lead to slight confusion. \n\nThe third candidate, "朱柳-埃德蒙兹算法" (Zhuliu-Eidemonz algorithm), introduces a variation in the name "Zhu Liu" to "Zhuliu," which is a less standard representation and could lead to misinterpretation. \n\nThe remaining candidates, such as "Chu-Liu-Edmonds算法" and "楚-刘-埃德蒙兹算法," while they retain the original structure, do not provide the same level of familiarity and recognition in the Chinese academic context as the first two candidates. \n\nOverall, the first candidate is preferred for its semantic accuracy, back translation accuracy, and contextual fit within the AI domain.', ['Zhu-Liu-Edmonds algorithm', 'Zhu-Liu-Edmonds algorithm', 'Zhuliu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Chu-Liu-Edmonds algorithm', 'Zhu Liu Edmunds Algorithm'])
Chung-Lu model Chinese
('Chung-Lu模型', '1. Rank: Chung-Lu模型, 钟陆模型, 钟路模型, 种-卢模型, Chung-Lu モデル\n\n2. Explanation: The term "Chung-Lu模型" is the best fit because it maintains the original name of the model while using the standard Chinese term "模型" (model), which is widely recognized in the AI and mathematical contexts. This translation preserves the semantic accuracy and is directly understandable to those familiar with the field. \n\nThe second candidate, "钟陆模型," is also a reasonable translation as it phonetically approximates the original name, but it may not be as immediately recognizable to all audiences in the AI domain. \n\nThe third candidate, "钟路模型," introduces a different character for "Lu," which could lead to confusion, as "路" (road) does not convey the same meaning as "Lu" in the context of the model. \n\nThe fourth candidate, "种-卢模型," introduces an incorrect term "种" (species), which is not relevant in this context and could mislead readers. \n\nLastly, "Chung-Lu モデル" is a transliteration in Japanese, which is not suitable for a Chinese context and does not serve the purpose of conveying the term to a Chinese-speaking audience. \n\nOverall, "Chung-Lu模型" is the most contextually appropriate and semantically accurate translation for the AI domain.', ['Chung-Lu model', 'Zhonglu model', 'Chung-Lu モデル', 'Species-Lu model', 'Jongno model'])
Cohen's kappa Chinese
('科恩卡帕系数', '1. Rank: 科恩卡帕系数, 科恩的卡帕, 科恩κ系数, Cohen的κ系数, Cohen\'s κ, コーエンのカッパ\n\n2. Explanation: The term "科恩卡帕系数" (Cohen\'s kappa coefficient) is the best fit for several reasons. Firstly, it maintains semantic accuracy by directly translating both the name "Cohen" and the term "kappa" while adding "系数" (coefficient), which is a standard term in statistical contexts in Chinese. This makes it clear that it refers to a statistical measure, which is crucial in the AI domain where precision in terminology is important.\n\nThe second candidate, "科恩的卡帕" (Cohen\'s kappa), is also a good option but lacks the explicit mention of "系数," which could lead to ambiguity in a statistical context. The third candidate, "科恩κ系数," uses the Greek letter "κ" but may not be as immediately recognizable to all audiences compared to the full term "kappa." \n\nThe other candidates, such as "Cohen的κ系数" and "Cohen\'s κ," are less preferable because they mix English and Chinese, which can be less formal and may not be suitable for academic or technical writing. The Japanese term "コーエンのカッパ" is irrelevant in this context as it does not cater to the Chinese audience.\n\nOverall, "科恩卡帕系数" is the most contextually appropriate and semantically accurate translation for the AI domain, ensuring clarity and precision in communication.', ["Cohen's kappa coefficient", "Cohen's Kappa", 'コーエンのカッパ', "Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's κ"])
Cohen's kappa coefficient Chinese
("Cohen's kappa系数", '1. Rank: Cohen\'s kappa系数, 科恩的卡帕系数, 科恩卡帕系数, Cohen的卡帕系数, 科恩的κ系数, Cohen\'s kappa 系数\n\n2. Explanation: The term "Cohen\'s kappa系数" is the best fit because it maintains the original structure of the English term while ensuring semantic accuracy. The use of "系数" (coefficient) is standard in statistical terminology in Chinese, making it immediately recognizable to professionals in the field. Additionally, it retains the possessive form "Cohen\'s," which is important for attributing the concept to its originator, Cohen. \n\nThe second candidate, "科恩的卡帕系数," is also a strong option, as it translates "Cohen" to "科恩" and uses "卡帕" for "kappa," but the inclusion of "的" (of) makes it slightly less formal in a technical context. \n\nThe third candidate, "科恩卡帕系数," is less preferred because it omits "系数," which is crucial for clarity in statistical contexts. \n\nThe remaining candidates, such as "Cohen的卡帕系数" and "科恩的κ系数," introduce variations that may confuse readers unfamiliar with the Greek letter "κ" or the possessive structure, making them less suitable for a technical audience. \n\nOverall, "Cohen\'s kappa系数" is the most contextually appropriate and semantically accurate choice for the AI domain.', ["Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's kappa coefficient", "Cohen's κ coefficient", "Cohen's kappa coefficient"])
Cohen's κ Chinese
('科恩κ系数', '1. Rank: 科恩κ系数, 科恩的κ, Cohen\'s κ系数, Cohen\'s κ, Cohen的κ\n\n2. Explanation: The term "科恩κ系数" is the best fit because it accurately conveys the concept of Cohen\'s kappa coefficient, which is a well-established statistical measure in the AI and statistics domains. The inclusion of "系数" (coefficient) clarifies that this is a quantitative measure, which is essential for understanding its application in measuring interrater agreement. \n\nThe second candidate, "科恩的κ," while accurate, lacks the term "系数," which may lead to ambiguity about the nature of the measure. The third candidate, "Cohen\'s κ系数," is also a strong option, but it mixes English and Chinese, which may not be as preferred in formal contexts. The fourth candidate, "Cohen\'s κ," is a direct transliteration and does not provide any additional context, making it less informative. Lastly, "Cohen的κ" is similar to the previous one but is less formal and does not include the term "系数," which is crucial for clarity in the AI domain. \n\nOverall, "科恩κ系数" is the most semantically accurate and contextually appropriate choice for the AI terminology in question.', ["Cohen's kappa coefficient", "Cohen's κ", "Cohen's κ", "Cohen's κ", "Cohen's kappa coefficient"])
Condorcet winner Chinese
('孔多塞赢家', '1. Rank: 孔多塞赢家, 孔多塞获胜者, 孔多塞胜者, 康多塞获胜者, 孟德尔森赢家\n\n2. Explanation: The term "孔多塞赢家" (Condorcet Winner) is the best fit for the translation of "Condorcet winner" due to its direct semantic accuracy and established usage in the AI and mathematical contexts. This term maintains the original name "Condorcet," which is crucial for preserving the specific reference to the Condorcet method in voting theory. The term "赢家" (winner) is a straightforward translation that accurately conveys the meaning of a winner in this context.\n\nThe second candidate, "孔多塞获胜者" (Condorcet winner), is also a good fit but slightly less common in usage compared to "赢家." The term "胜者" (victor) in "孔多塞胜者" is acceptable but less frequently used in mathematical contexts, making it a less preferred choice. \n\n"康多塞获胜者" (Condorcet Winner) introduces a variation in the spelling of "Condorcet," which could lead to confusion, while "孟德尔森赢家" (Mendelson winner) is entirely incorrect as it refers to a different concept and does not relate to the Condorcet method at all. \n\nOverall, "孔多塞赢家" is the most contextually appropriate and widely recognized term in the AI domain, ensuring clarity and precision in communication.', ['Condorcet Winner', 'victor of condorcet', 'mendelson winner', 'Condorcet winner', 'Condorcet Winner'])
Coreference Resolution Chinese
('共指消解', '1. Rank: 共指消解, 指代消解, 共指解析\n\n2. Explanation: The term "共指消解" (coreference resolution) is the best fit for several reasons. Firstly, it accurately captures the semantic meaning of the English term "coreference resolution," which refers to the task of determining when different expressions in a text refer to the same entity. The term "共指" directly translates to "coreference," indicating the relationship between expressions that refer to the same thing, while "消解" means "resolution," which is commonly used in computational linguistics to denote the process of resolving ambiguities.\n\nIn contrast, "指代消解" (reference resolution) is less precise because "指代" translates to "reference," which can encompass a broader range of referential expressions and may not specifically imply the coreference aspect. This could lead to confusion in the AI context where the distinction is crucial.\n\nLastly, "共指解析" (coreference analysis) introduces the term "解析," which means "analysis." While it is related, it does not convey the same sense of resolution as "消解." In the AI domain, where tasks often focus on resolving ambiguities rather than merely analyzing them, "消解" is the more appropriate term. Thus, "共指消解" is the most contextually and semantically accurate choice for the term "coreference resolution."', ['coreference resolution', 'Reference resolution', 'coreference analysis'])
Datalog Chinese
('数据逻辑', '1. Rank: 数据逻辑, 数据日志语言, Datalog, 電腦科學家, 计算机科学家\n\n2. Explanation: The term "数据逻辑" (Data logic) is the best fit for the translation of "Datalog" in the AI domain. This is because "数据逻辑" accurately captures the essence of Datalog as a declarative logic programming language that is primarily used for databases and knowledge representation. The term "逻辑" (logic) directly relates to the logical foundations of Datalog, while "数据" (data) emphasizes its application in data management and querying.\n\nIn contrast, "数据日志语言" (data logging language) is misleading as it suggests a focus on logging data rather than querying and reasoning about data, which is the core function of Datalog. The terms "電腦科學家" and "计算机科学家" (both meaning computer scientist) are irrelevant in this context as they do not pertain to the term "Datalog" itself. Lastly, "Datalog" (data log) as a back translation does not convey the specific nature of the language and is less semantically accurate than "数据逻辑." Thus, "数据逻辑" is the most contextually appropriate and semantically accurate translation for the term "Datalog."', ['Data logic', 'data logging language', 'computer scientist', 'computer scientist', 'data log'])
Decentralized optimization Chinese
('去中心化优化', '1. Rank: 去中心化优化, 分散优化, 分散式优化, 分布式优化\n\n2. Explanation: The term "去中心化优化" (Decentralized optimization) is the best fit because it directly captures the essence of "decentralized" in the context of optimization. In AI and distributed systems, "去中心化" specifically refers to the absence of a central authority or control, which aligns perfectly with the concept of decentralized optimization where local agents operate independently. \n\n"分散优化" (decentralized optimization) is a close second, as "分散" also conveys a sense of distribution without central control, but it may not emphasize the lack of a central authority as strongly as "去中心化." \n\n"分散式优化" (decentralized-style optimization) introduces a nuance that suggests a method or style rather than a fundamental characteristic, which may not be as precise in the AI context. \n\n"分布式优化" (distributed optimization) is the least accurate in this context, as "分布式" typically refers to systems that are distributed but may still have a central coordinating entity, which contradicts the core idea of decentralization. Thus, "去中心化优化" is the most semantically accurate and contextually appropriate term for the AI domain.', ['Decentralized optimization', 'decentralized optimization', 'decentralized optimization', 'Distributed optimization'])
Demographic Parity Chinese
('人口统计平等', '1. Rank: 人口统计平等, 人口统计公平, 人口统计学公平性, 人口平等, 人口统计平衡\n\n2. Explanation: The term "人口统计平等" (Demographic equality) is the best fit for "Demographic Parity" because it accurately captures the essence of the concept in the AI domain, which emphasizes equality across different demographic groups in predictive modeling. The back translation aligns closely with the original term, maintaining the focus on demographic aspects. \n\n"人口统计公平" (Demographic equity) and "人口统计学公平性" (Demographic equity) are also relevant but slightly less precise, as they introduce the notion of fairness, which can imply a broader context than the specific equality focus of "Demographic Parity." \n\n"人口平等" (Population equality) and "人口统计平衡" (Demographic balance) are less suitable because they do not convey the statistical and algorithmic implications of "parity" in the context of AI, which is crucial for understanding the term\'s application in fairness metrics. Thus, "人口统计平等" stands out as the most contextually appropriate translation.', ['Demographic equality', 'Population equality', 'Demographic balance', 'Demographic Equity', 'Demographic equity'])
Dice coefficient Chinese
('Dice系数', '1. Rank: Dice系数, Dice 系数, 戴斯系数, 骰子系数\n\n2. Explanation: The term "Dice系数" is the best fit for the translation of "Dice coefficient" in the AI domain due to its semantic accuracy and established usage in the field. This term directly translates to "Dice coefficient" without any alteration, maintaining the original meaning and context. \n\nThe second candidate, "Dice 系数," is also a valid option but includes a space that is not typically used in technical terminology, making it slightly less preferable. \n\nThe third candidate, "戴斯系数," introduces a transliteration of "Dice" that may not be widely recognized in the AI community, potentially leading to confusion. \n\nLastly, "骰子系数" translates to "dice coefficient" in a more literal sense, referring to the physical dice rather than the mathematical concept, which could mislead readers in a technical context. \n\nOverall, "Dice系数" is the most accurate and contextually appropriate term for use in AI literature, as it aligns with established terminology and avoids ambiguity.', ['Dice coefficient', 'Dais coefficient', 'Dice coefficient', 'Dice coefficient'])
Dice loss Chinese
('Dice损失', '1. Rank: Dice损失, Dice 损失, 骰子损失, 戴斯损失\n\n2. Explanation: The term "Dice损失" is the best fit for the translation of "Dice loss" in the AI domain. This term maintains semantic accuracy by directly translating "Dice" as "Dice" (the name of the metric) and "损失" as "loss," which is a standard term in machine learning and AI contexts. The back translation accurately reflects the original English term, ensuring clarity and consistency in technical discussions.\n\nThe second candidate, "Dice 损失," is also a good option, but the space between "Dice" and "损失" is unconventional in technical terminology, where terms are typically written as a single unit without spaces. \n\n"骰子损失" translates to "Dice loss" but introduces ambiguity, as "骰子" means "dice" in a general sense and may not be recognized as a specific term in the AI context. \n\nLastly, "戴斯损失" translates to "Days loss," which is incorrect and does not convey the intended meaning of the original term, making it the least suitable option. \n\nOverall, "Dice损失" is the most contextually appropriate and semantically accurate translation for use in AI literature.', ['Dice loss', 'Days loss', 'Loss', 'Dice loss'])
Dijkstra's algorithm Chinese
('Dijkstra算法', '1. Rank: Dijkstra算法, 迪杰斯特拉算法, Dijkstra 算法, 迪克斯特拉算法, 戴克斯特拉算法\n\n2. Explanation: The term "Dijkstra算法" is the most widely accepted and recognized translation for "Dijkstra\'s algorithm" in the AI and computer science communities in Chinese-speaking regions. It maintains semantic accuracy by directly translating "Dijkstra" into a phonetic representation while appending "算法," which means "algorithm." This term is concise and commonly used in academic literature and technical discussions, making it contextually appropriate for the AI domain.\n\nThe second candidate, "迪杰斯特拉算法," while also accurate, is less commonly used and may be seen as overly formal or verbose. The third candidate, "Dijkstra 算法," retains the English name but is less suitable in formal writing where a fully translated term is preferred. The fourth and fifth candidates, "迪克斯特拉算法" and "戴克斯特拉算法," are less accurate phonetic representations and are not widely recognized in the field, which could lead to confusion. Therefore, "Dijkstra算法" stands out as the best fit due to its established usage and clarity in the context of AI and algorithm discussions.', ["Dijkstra's algorithm", "Dijkstra's algorithm", "Dijkstra's algorithm", "Dijkstra's algorithm", "Dijkstra's algorithm"])
Disentanglement Chinese
('解耦', '1. Rank: 解耦, 解缠结, 解缠\n\n2. Explanation: The term "解耦" (decoupling) is the best fit for the translation of "disentanglement" in the AI context. In machine learning and AI, "decoupling" refers to the process of separating different components or factors in a model, which aligns closely with the concept of disentanglement where one aims to isolate and understand the individual factors that contribute to the overall data representation. \n\nWhile "解缠结" (untangle) and "解缠" (untangle) are also valid translations, they are more general and do not capture the specific technical nuance of separating or decoupling variables or features in a model. "解耦" conveys a clearer sense of structural separation, which is essential in the context of AI and machine learning, making it the most semantically accurate and contextually appropriate choice.', ['Untangle', 'decoupling', 'Untangle'])
Dropout distribution Chinese
('Dropout分布', '1. Rank: Dropout分布, Dropout 分布, 随机失活分布, 丢弃分布\n\n2. Explanation: The term "Dropout分布" is the best fit for the translation of "Dropout distribution" in the AI context. This term maintains the original English terminology "Dropout" while using the Chinese word "分布" (distribution), which is a direct and accurate translation. The term "Dropout" is widely recognized in the AI and machine learning communities, particularly in the context of neural networks, where it refers to a regularization technique that randomly sets a fraction of the input units to zero during training to prevent overfitting.\n\nThe second candidate, "Dropout 分布," is essentially the same as the first but includes a space, which is less common in technical writing and does not add any semantic value. \n\nThe third candidate, "随机失活分布" (random deactivation distribution), introduces a different concept by translating "Dropout" as "随机失活" (random deactivation), which could lead to confusion as it does not capture the established terminology used in the AI field. \n\nThe last candidate, "丢弃分布" (drop distribution), is also less accurate because "丢弃" (to discard) does not convey the same meaning as "Dropout" in the context of neural networks. It lacks the specificity and recognition that "Dropout" has in the AI community.\n\nOverall, "Dropout分布" is the most semantically accurate and contextually appropriate term for the AI domain.', ['Dropout distribution', 'random deactivation distribution', 'Dropout distribution', 'drop distribution'])
Epanechnikov kernel Chinese
('埃帕涅奇科夫核', '1. Rank: 埃帕涅奇科夫核, Epanechnikov核, Epanechnikov核函数, 核函数, 艾潘尼奇科夫核, 伊帕内奇尼科夫核, 核, 核\n\n2. Explanation: The term "埃帕涅奇科夫核" (Epanechnikov kernel) is the best fit because it accurately captures both the semantic meaning and the specific context of the term in the AI domain. The use of "核" (kernel) is appropriate in statistical and machine learning contexts, where "kernel" refers to a function used in various algorithms, such as kernel density estimation and support vector machines. \n\nThe back translation of "埃帕涅奇科夫核" as "Epanechnikov kernel" maintains the original term\'s integrity and specificity, which is crucial in technical discussions. Other candidates like "Epanechnikov核" and "Epanechnikov核函数" are also good, but they either lack the full context of "kernel" or add unnecessary complexity with "函数" (function) when "核" alone suffices. \n\nThe other translations, such as "核" (nuclear) and variations like "艾潘尼奇科夫核" and "伊帕内奇尼科夫核," introduce inaccuracies or are less commonly used in the AI literature, making them less suitable for this specific context. Thus, "埃帕涅奇科夫核" stands out as the most contextually and semantically accurate choice.', ['kernel function', 'Epanechnikov core', 'nuclear', 'Epanechikov core', 'Ipanichkov core', 'Ipanechnikov core', 'Epanechnikov kernel function'])
