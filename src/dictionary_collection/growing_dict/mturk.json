[
  {
    "English": "10-fold cross validation",
    "context": "1: For each pair we generate a binary classification task using the MFCC features, the average activations in the convolutional layer, the average unit   activations per recurrent layer, and the sentence embeddings as input features. For every type of input, we run <mark>10-fold cross validation</mark> using Logistic Regression to predict which of the two words the utterance contains.<br>2: 10-fold dataset splits We provide the number of clusters and antibodies in each fold of our <mark>10-fold cross validation</mark>. When selecting fold i for testing, we use fold i − 1 for validation (for fold 1 as the test set, we use fold 10 for validation) and the union of other folds for training.<br>",
    "Arabic": "التحقق المتقاطع بمقدار ١٠ أضعاف",
    "Chinese": "十折交叉验证",
    "French": "validation croisée 10 fois",
    "Japanese": "10分割交差検証",
    "Russian": "10-кратная перекрестная проверка"
  },
  {
    "English": "1d convolution",
    "context": "1: The convolutional neural network (CNN) used is a modified version of a residual network (ResNet). Each ResNet block is composed of two, <mark>1D convolutions</mark> (Conv1D) with a 3 × 3 kernel using \"same\" padding, and an output feature map size of 406.<br>",
    "Arabic": "1D الإلتواء",
    "Chinese": "1D 卷积",
    "French": "Convolution 1D",
    "Japanese": "1次元畳み込み",
    "Russian": "одномерная свертка"
  },
  {
    "English": "2 norm",
    "context": "1: Finally, CoCL computes the following average: \n L CoCL = 1 |C| c∈C l c , \n where • is the <mark>2 norm</mark>.<br>2: where • 2 is the <mark>2 norm</mark> in R l .<br>",
    "Arabic": "معيار 2",
    "Chinese": "2范数",
    "French": "norme 2",
    "Japanese": "2ノルム",
    "Russian": "2 норма"
  },
  {
    "English": "2d convolution",
    "context": "1: Adding 3D convolutions to <mark>2D convolutions</mark> essentially leads to a design similar to Top-Heavy S3D architecture [73], which shows better performance than full 3D convolutions on video action recognition and runs faster. Results are shown in Table 2. Overall, models that use 3D convolutions perform worse than models that adopt a simple mean pooling.<br>2: by moving it closer to a surface, the 2D neighborhood of a feature may change. As a result, <mark>2D convolutions</mark> come with no guarantee on multiview consistency. With our per-pixel formulation, the rendering function Θ operates independently on all pixels, allowing images to be generated with arbitrary resolutions and poses.<br>",
    "Arabic": "الإلتواء 2D",
    "Chinese": "二维卷积",
    "French": "convolution 2D",
    "Japanese": "2D畳み込み",
    "Russian": "2D свертка"
  },
  {
    "English": "2d image",
    "context": "1: Following this route, most deep architectures for 3D point cloud analysis require pre-processing of irregular point clouds into either voxel representations (e.g., [45,37,44]) or <mark>2D images</mark> by view projection (e.g., [41,34,24,9]).<br>",
    "Arabic": "صورة ثنائية الأبعاد",
    "Chinese": "二维图像",
    "French": "image 2D",
    "Japanese": "2次元画像",
    "Russian": "2D изображение"
  },
  {
    "English": "2d image synthesis",
    "context": "1: Disinformation in the form of 3D objects may be more convincing than 2D images (though renderings of our synthesized 3D models are less realistic than the state of the art in <mark>2D image synthesis</mark>). Generative models such as ours may have the potential to displace creative workers via automation.<br>",
    "Arabic": "تخليق صور ثنائية الأبعاد",
    "Chinese": "二维图像合成",
    "French": "synthèse d'images 2D",
    "Japanese": "2次元画像合成",
    "Russian": "синтез 2D-изображений"
  },
  {
    "English": "2d-3d correspondence",
    "context": "1: To summarize, our main contributions are as follows: \n • We propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation via learnable <mark>2D-3D correspondences</mark>. • We demonstrate that EPro-PnP can easily reach toptier performance for 6DoF pose estimation by simply inserting it into the CDPN [29] framework.<br>2: Cameras are placed on the 20 vertices of a dodecahedron from a fixed distance, pointing towards the object's center. The <mark>2D-3D correspondences</mark> can be generated by carrying the XY Z coordinates of 3D points into the rendering rasterization pipeline so that each pixel also acquires coordinate values from the surface point projected onto it.<br>",
    "Arabic": "المراسلات ثنائية وثلاثية الأبعاد",
    "Chinese": "二维三维对应关系",
    "French": "correspondances 2d-3d",
    "Japanese": "2次元-3次元対応",
    "Russian": "2D-3D соответствие"
  },
  {
    "English": "3d bounding box",
    "context": "1: The subset of PASCAL we considered after filtering occluded instances, which we do not tackle in this paper, had between 70 images for \"sofa\" and 500 images for classes \"aeroplanes\" and \"cars\". We will make all our image sets available along with our implementation. Metrics. We quantify the quality of our 3D models by comparing against the PASCAL 3D+ models using two metrics -1 ) the Hausdorff distance normalized by the <mark>3D bounding box</mark> size of the ground truth model [ 3 ] and 2 ) a depth map error to evaluate the quality of the reconstructed visible object surface , measured as the mean absolute distance between reconstructed<br>2: BB8 [37] and RTM3D [28] locate the corners of the <mark>3D bounding box</mark> as keypoints, while PVNet [36] defines the keypoints by farthest point sampling and Deep MANTA [11] by handcrafted templates. On the other hand, dense correspondence methods [13,29,35,46,52] predict pixel-wise 3D coordinates within a cropped 2D region.<br>",
    "Arabic": "صندوق احتواء ثلاثي الأبعاد",
    "Chinese": "三维包围盒",
    "French": "boîte englobante 3D",
    "Japanese": "3次元バウンディングボックス",
    "Russian": "3D ограничивающий параллелепипед"
  },
  {
    "English": "3d computer vision",
    "context": "1: NerFormer mates two of the main workhorses of machine learning and <mark>3D computer vision</mark>: Transformers [65] and neural implicit rendering [43]. Specifically, given a set of 3D points along a rendering ray, features are sampled from known images and stacked into a tensor.<br>",
    "Arabic": "الرؤية الحاسوبية ثلاثية الأبعاد",
    "Chinese": "三维计算机视觉",
    "French": "Vision par ordinateur 3D",
    "Japanese": "3次元コンピュータビジョン",
    "Russian": "трехмерное компьютерное зрение"
  },
  {
    "English": "3d convolutional network",
    "context": "1: The choice of architecture proves to be important for performance, with <mark>3D convolutional networks</mark> excelling at tasks involving complex geometries, graph networks performing well on systems requiring detailed positional information, and the more recently developed equivariant networks showing significant promise.<br>",
    "Arabic": "شبكة تلافيفية ثلاثية الأبعاد",
    "Chinese": "三维卷积网络",
    "French": "réseau convolutionnel 3D",
    "Japanese": "3次元畳み込みネットワーク",
    "Russian": "трёхмерная сверточная сеть"
  },
  {
    "English": "3d geometry",
    "context": "1: Given a set of images acquired from known viewpoints, we describe a method for synthesizing the image which would be seen from a new viewpoint. In contrast to existing techniques, which explicitly reconstruct the <mark>3D geometry</mark> of the scene, we transform the problem to the reconstruction of colour rather than depth.<br>2: We show that multi-path analysis using images from a timeof-flight (ToF) camera provides a tantalizing opportunity to infer about <mark>3D geometry</mark> of not only visible but hidden parts of a scene.<br>",
    "Arabic": "هندسة ثلاثية الأبعاد",
    "Chinese": "三维几何",
    "French": "géométrie 3D",
    "Japanese": "3次元ジオメトリ",
    "Russian": "3D геометрия"
  },
  {
    "English": "3d human pose estimation",
    "context": "1: We developed a Picture program for parsing 3D pose of articulated humans from single images. There has been notable work in model-based approaches [13,27] for <mark>3D human pose estimation</mark>, which served as an inspiration for the program we describe in this section.<br>2: We use Picture to write programs for 3D face analysis, <mark>3D human pose estimation</mark>, and 3D object reconstruction -each competitive with specially engineered baselines.<br>",
    "Arabic": "تقدير وضعية الإنسان ثلاثي الأبعاد",
    "Chinese": "三维人体姿态估计",
    "French": "estimation de la pose humaine en 3D",
    "Japanese": "3次元人体姿勢推定",
    "Russian": "оценка позы человека в 3D"
  },
  {
    "English": "3d localization",
    "context": "1: After being processed by the subsequent layers, the object-level features are finally transformed into to the object-level predictions, consisting of the <mark>3D localization</mark> score, weight scale, 3D bounding box size, and other optional properties (velocity and attribute).<br>",
    "Arabic": "التحديد ثلاثي الأبعاد",
    "Chinese": "三维定位",
    "French": "Localisation 3D",
    "Japanese": "3次元位置推定",
    "Russian": "3D локализация"
  },
  {
    "English": "3d mesh",
    "context": "1: * Authors contributed equally Figure 1: Automatic object reconstruction from a single image obtained by our system. Our method leverages estimated instance segmentations and predicted viewpoints to generate a full <mark>3D mesh</mark> and high frequency 2.5D depth maps. PASCAL VOC [15]).<br>",
    "Arabic": "شبكة ثلاثية الأبعاد",
    "Chinese": "3D网格",
    "French": "maillage 3D",
    "Japanese": "3次元メッシュ",
    "Russian": "3D сетка"
  },
  {
    "English": "3d model",
    "context": "1: However, the encoder would still be unable to relate image crops from real RGB sensors because (1) the <mark>3D model</mark> and the real object differ, (2) simulated and real lighting conditions differ, (3) the network can't distinguish the object from background clutter and foreground occlusions.<br>2: The 3D flow algorithm of §6 was initialized with 100 points on the face found by an interest operator in a single frame, and successfully found correspondences across the entire sequence, concluding with a corrective transform to give the <mark>3D model</mark> used to generate the images in figure 6. Figure 5 shows the recovered motion parameters.<br>",
    "Arabic": "نموذج ثلاثي الأبعاد",
    "Chinese": "三维模型",
    "French": "modèle 3D",
    "Japanese": "3Dモデル",
    "Russian": "3D-модель"
  },
  {
    "English": "3d object detection",
    "context": "1: Although they share the same fundamentals of pose estimation, the different nature of the data leads to biased choice of methods. Top performers [34,48,50] on the <mark>3D object detection</mark> benchmarks [8,17] fall into the category of direct 4DoF pose prediction, leveraging the advances in end-toend deep learning.<br>2: Tasks comparison and taxonomy. \"Design\" column is classified as in Fig. 1. \"Det.\" denotes <mark>3D object detection</mark>, \"Map\" stands for online mapping, and \"Occ.\" is occupancy map prediction.<br>",
    "Arabic": "اكتشاف الكائنات ثلاثية الأبعاد",
    "Chinese": "三维目标检测",
    "French": "détection d'objets 3D",
    "Japanese": "3次元物体検出",
    "Russian": "обнаружение 3D-объектов"
  },
  {
    "English": "3d point",
    "context": "1: must hold, where the unknown λ k,i is the depth of the <mark>3D point</mark> X k in camera i. This constraint means that the distance between every two <mark>3D point</mark>s, when reconstructed in different cameras, must be the same.<br>",
    "Arabic": "نقطة ثلاثية الأبعاد",
    "Chinese": "三维点",
    "French": "point 3D",
    "Japanese": "3次元点",
    "Russian": "3D точка"
  },
  {
    "English": "3d point cloud",
    "context": "1: However, Adam sometimes produces artifacts showing lower accuracy; it tends to generate thinner legs, mainly due to poor <mark>3D point cloud</mark> reconstructions on the source data on which Adam is trained.<br>2: Such data can be effectively gathered in huge quantities by means of crowd-sourcing \"turn-table\" videos captured with smartphones, which are nowadays a commonly owned accessory. The mature Structure-from-Motion (SfM) framework then provides 3D annotations by tracking cameras and reconstructs a dense <mark>3D point cloud</mark> capturing the object surface.<br>",
    "Arabic": "سحابة نقاط ثلاثية الأبعاد",
    "Chinese": "三维点云",
    "French": "nuage de points 3D",
    "Japanese": "3D点群",
    "Russian": "облако точек 3D"
  },
  {
    "English": "3d pose",
    "context": "1: Right: Once trained, our model reconstructs the <mark>3D pose</mark>, shape, albedo and illumination of a deformable object instance from a single image with excellent fidelity. Code and demo at https://github.com/elliottwu/unsup3d.<br>",
    "Arabic": "وضعية ثلاثية الأبعاد",
    "Chinese": "三维姿势",
    "French": "pose 3D",
    "Japanese": "3次元ポーズ",
    "Russian": "3D поза"
  },
  {
    "English": "3d reconstruction",
    "context": "1: Thus, we ask: can we accurately track realworld motion without explicit dynamic <mark>3D reconstruction</mark>? We answer this question using our proposed representation, OmniMotion (illustrated in Fig. 2). OmniMotion represents the scene in a video as a canonical 3D volume that is mapped to local volumes for each frame through local-canonical bijections.<br>2: Minimal 1 problems [37,50,24,26,28,29,25,30] play an important role in <mark>3D reconstruction</mark> [48,49,47], image matching [44], visual odometry [39,4] and visual localization [52,46,51]. Many minimal problems have been described and solved and new minimal problems are constantly appearing.<br>",
    "Arabic": "إعادة الإعمار ثلاثية الأبعاد",
    "Chinese": "三维重建",
    "French": "reconstruction 3D",
    "Japanese": "3D再構築",
    "Russian": "Трехмерная реконструкция"
  },
  {
    "English": "3d scene",
    "context": "1: Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall <mark>3D scene</mark> by modeling the interdependence of objects, surface orientations, and camera viewpoint.<br>",
    "Arabic": "المشهد ثلاثي الأبعاد",
    "Chinese": "三维场景",
    "French": "scène 3D",
    "Japanese": "3次元シーン",
    "Russian": "трёхмерная сцена"
  },
  {
    "English": "3d scene geometry",
    "context": "1: However, exhaustive sampling of the light field is impractical in most scenarios, so methods for view synthesis from sparsely-captured images reconstruct <mark>3D scene geometry</mark> in order to reproject observed images into novel viewpoints [7]. For scenes with glossy surfaces, some methods explicitly build virtual geometry to explain the motion of reflections [15,31,33].<br>2: We introduce Scene Representation Networks (SRNs), a continuous neural scene representation, along with a differentiable rendering algorithm, that model both <mark>3D scene geometry</mark> and appearance, enforce 3D structure in a multi-view consistent manner, and naturally allow generalization of shape and appearance priors across scenes.<br>",
    "Arabic": "هندسة المشهد ثلاثية الأبعاد",
    "Chinese": "3D场景几何",
    "French": "géométrie de la scène 3D",
    "Japanese": "3次元シーンジオメトリ",
    "Russian": "геометрия 3D сцены"
  },
  {
    "English": "3d structure",
    "context": "1: This issue was either ignored by researchers 8, 4, 1, 7], or else was addressed using other minimization approaches 3,5]. Morris and Kanade 3] have suggested a uni ed approach for recovering the <mark>3D structure</mark> and motion from point and line features, by taking into account t h e i r directional uncertainty.<br>",
    "Arabic": "البنية ثلاثية الأبعاد",
    "Chinese": "三维结构",
    "French": "structure 3D",
    "Japanese": "3次元構造",
    "Russian": "3D структура"
  },
  {
    "English": "5-fold cross validation",
    "context": "1: Our experimental set-up is as follows. We perform a <mark>5-fold cross validation</mark>. Our data is initially randomized before we defined folds that are held constant across all experiments. For our experiments, we convert the instances of B(oth) to instances of Y, i.e. affixoid uses.<br>",
    "Arabic": "\"التقسيم المتقاطع الخماسي\"",
    "Chinese": "5重交叉验证",
    "French": "Validation croisée à 5 volets",
    "Japanese": "5分割交差検証",
    "Russian": "5-кратная перекрестная проверка"
  },
  {
    "English": "Adafactor",
    "context": "1: As a consequence of our confidence-guided strategy in CAME, there is no doubt that CAME will introduce an increased memory footprint in comparison with <mark>Adafactor</mark>. However, the extra memory footprint incurred of CAME is almost negligible (1%) with a substantial performance improvement.<br>2: Contributions of our paper can be summarized in the following: \n • Inspired by training instability of <mark>Adafactor</mark>, we explore a confidence-guided strategy centered on the existing error in the raw updates of <mark>Adafactor</mark> for parameters of large language models.<br>",
    "Arabic": "أدافاكتور",
    "Chinese": "Adafactor",
    "French": "Adafactor",
    "Japanese": "アダファクタ",
    "Russian": "Адафактор"
  },
  {
    "English": "Adam",
    "context": "1: Each GPU/worker collects 256 steps of experience from 16 agents (each in different scenes) and then performs 2 epochs of PPO with 2 mini-batchs per epoch. We use the <mark>Adam</mark> optimize (Kingma & Ba, 2015) with a learning rate of 2.5 × 10 −4 .<br>2: We use <mark>Adam</mark> with β 1 = 0.9 and β 2 = 0.95 and sequentially try the learning rates 0.01, 0.003, 0.001, 0.0003, ..., stopping once the final validation loss starts increasing. The learning rate is warmed up for one epoch, and then decays to 0 following a cosine schedule. No dropout is used.<br>",
    "Arabic": "آدم",
    "Chinese": "Adam",
    "French": "Adam",
    "Japanese": "アダム",
    "Russian": "Адам"
  },
  {
    "English": "Adam algorithm",
    "context": "1: We optimize the parameters using the <mark>Adam algorithm</mark> (Kingma and Ba, 2015): we found the initial learning rate of 10   SIQA we train in batches of size 8, whereas on Multi-NLI and PAWS we train in batches of size 32.<br>",
    "Arabic": "خوارزمية آدم",
    "Chinese": "Adam算法",
    "French": "algorithme Adam",
    "Japanese": "アダムアルゴリズム",
    "Russian": "Алгоритм Адама"
  },
  {
    "English": "Adam optimiser",
    "context": "1: The transformer models are fine-tuned to predict the target label (T rue or F alse) by optimising for the binary cross-entropy loss over the targets using the <mark>Adam optimiser</mark> (Kingma and Ba, 2015).<br>2: All intermediate GAT layers are used with 8 attention heads and an output space of 8 with a dropout=0.6. The <mark>Adam optimiser</mark> is set with default values β 1 = 0.9, β 2 = 0.999, = 1e−8, weight-decay = 5e−4 and an initial learning rate of 0.001.<br>",
    "Arabic": "محسن آدم",
    "Chinese": "亚当优化器",
    "French": "optimiseur Adam",
    "Japanese": "アダム・オプティマイザー",
    "Russian": "Оптимизатор Адам"
  },
  {
    "English": "Adam optimization",
    "context": "1: Following earlier studies (Kamper, 2019;, we first pretrain the model as an autoencoder RNN for 15 epochs without early stopping using the <mark>Adam optimization</mark> (Kingma and Ba, 2015) with a learning rate of 0.001.<br>",
    "Arabic": "تحسين آدم",
    "Chinese": "Adam优化算法",
    "French": "optimisation Adam",
    "Japanese": "Adam最適化",
    "Russian": "Оптимизация Адама"
  },
  {
    "English": "Adam optimization algorithm",
    "context": "1: (2017), we use the Gumbel-Softmax estimator (Jang, Gu, and Poole 2016) to compute gradients for the teaching policies over discrete advising-level actions (readers are referred to their paper for additional details). Policy training is conducted with the <mark>Adam optimization algorithm</mark> (Kingma and Ba 2014), using a learning rate of 1e−3.<br>",
    "Arabic": "خوارزمية التحسين آدم",
    "Chinese": "Adam优化算法",
    "French": "algorithme d'optimisation Adam",
    "Japanese": "Adamの最適化アルゴリズム",
    "Russian": "Алгоритм оптимизации Адама"
  },
  {
    "English": "Adam optimizer",
    "context": "1: We use exponential moving average to update latent vectors as in official implementation 6 We train the model on the images of the same training data and did not use any external data. We use batch size of 512, and learning rate 0.0003 with the <mark>Adam optimizer</mark> (Kingma and Ba, 2015). We clip the gradients to 5.0.<br>2: Optimization We use RAdam (Liu et al., 2020a) with the default β values as our optimizer. RAdam is a variant of <mark>Adam optimizer</mark> (Kingma and Ba, 2014) that is reported less sensitive to the choice of learning rate and warmup steps while achieving similar results at the end.<br>",
    "Arabic": "محسن آدم",
    "Chinese": "Adam优化器",
    "French": "Optimiseur Adam",
    "Japanese": "Adamオプティマイザ",
    "Russian": "Оптимизатор Адам"
  },
  {
    "English": "Apriori",
    "context": "1: For example the pair E1 → E2, is associated with (0, d1] such that in an occurrence of α event E2 occurs no later than time d1 of event E1. The mining process follows the level-wise procedure ala <mark>Apriori</mark>, i.e., candidate generation followed by counting.<br>",
    "Arabic": "خوارزمية أبريوري",
    "Chinese": "先验",
    "French": "Apriori",
    "Japanese": "アプリオリ",
    "Russian": "Априори"
  },
  {
    "English": "Artificial Intelligence",
    "context": "1: That is, solving the Copyright c 2020, Association for the Advancement of <mark>Artificial Intelligence</mark> (www.aaai.org). All rights reserved. world's most challenging problems requires we take advantage of the complex structure and relationships inherent in real-world data.<br>2: We thank the Department of Computer Science and Engineering, IIT Madras, and the Robert Bosch Center for Data Science and <mark>Artificial Intelligence</mark>, IIT Madras (RBC-DSAI), for providing us resources required to carry out this research. We also wish to thank Google for providing access to TPUs through the TFRC program.<br>",
    "Arabic": "الذكاء الاصطناعي",
    "Chinese": "人工智能",
    "French": "Intelligence artificielle",
    "Japanese": "人工知能",
    "Russian": "Искусственный интеллект"
  },
  {
    "English": "Autoencoder",
    "context": "1: We have proposed a new self-supervised training strategy for <mark>Autoencoder</mark> architectures that enables robust 3D object orientation estimation on various RGB sensors while training only on synthetic views of a 3D model.<br>2: 2-Layer <mark>Autoencoder</mark>: We increase the number of encoder/decoder layers from one to two, which results in a dramatic performance decrease. It suggests that it is not necessary to use deep models. Wider Context (Window Size=3): We investigate whether the performance can benefit from combining integration of wider contextual information.<br>",
    "Arabic": "التشفير التلقائي",
    "Chinese": "自编码器",
    "French": "Autoencodeur",
    "Japanese": "自己エンコーダ",
    "Russian": "Автоэнкодер"
  },
  {
    "English": "Autonomous Systems",
    "context": "1: Figure 2(c) shows the DPL plot for the <mark>Autonomous Systems</mark> dataset. We observe a clear trend: Even in the presence of noise, changing external conditions, and disruptions to the Internet we observe a strong super-linear growth in the number of edges over more than 700 AS graphs.<br>2: is paper presents the F1/10 Autonomous Racing Cyber-Physical platform and summarizes the use of this testbed technology as the common denominator and key enabler to address the research and education needs of future autonomous systems and automotive Cyber-Physical Systems.<br>",
    "Arabic": "أنظمة مستقلة",
    "Chinese": "自主系统",
    "French": "Systèmes autonomes",
    "Japanese": "自律システム",
    "Russian": "Автономные Системы (AS)"
  },
  {
    "English": "Azuma-Hoeffding inequality",
    "context": "1: j∈{1,2} E λj ∼βj A jπ t −j − u t j , π * j,λj − π t j,λj ≤ 4W \n for all t. Hence, using the <mark>Azuma-Hoeffding inequality</mark> for martingale difference sequences we obtain that for all δ ∈ (0, 1), \n<br>2: Thus, for any T , by the <mark>Azuma-Hoeffding inequality</mark>, with probability at least 1 − δ, the predictionsŷ t made by the weak learner satisfy \n<br>",
    "Arabic": "متباينة أزوما-هوفدينج",
    "Chinese": "阿兹玛-霍夫丁不等式",
    "French": "inégalité d'Azuma-Hoeffding",
    "Japanese": "アズマ-ホフディング不等式",
    "Russian": "Неравенство Азумы-Хёффдинга"
  },
  {
    "English": "B-spline",
    "context": "1: p(M i ) ∝ exp − a|θ | 2 − b σ x σ y + σ y σ x 2 − ch 2 , \n where a, b, c are parameters. Next, we allow local deformations by adjusting the positions of the <mark>B-spline</mark> control points.<br>2: Each boundary is modeled by a <mark>B-spline</mark> using twenty five control points. The prototype characters are indexed by c i ∈ {1, . . . , 62} and their control points are represented by a matrix TP(c i ). We now define two types of deformations on the templates.<br>",
    "Arabic": "- Candidate term translation 2: \"- B-سبلاين\"",
    "Chinese": "B样条",
    "French": "B-spline",
    "Japanese": "B-スプライン",
    "Russian": "B-сплайн"
  },
  {
    "English": "Baseline",
    "context": "1: Overall LAICA(2) performs almost twice as well as both the baselines on all of the tasks considered. In the maze domain, even the best setting for <mark>Baseline</mark>(2) performed inconsistently.<br>2: Due to the sparse reward nature of the task, which only had a big positive reward on reaching goal, even the best setting for <mark>Baseline</mark>(2) failed on certain Note that even before the first addition of the new set of actions, the proposed method performs better than the baselines.<br>",
    "Arabic": "المعيار",
    "Chinese": "基准 (Baseline)",
    "French": "Référence",
    "Japanese": "ベースライン",
    "Russian": "Базовый уровень"
  },
  {
    "English": "Basis Pursuit",
    "context": "1: This is a subtle condition and precludes certain reconstruction algorithm (e.g. <mark>Basis Pursuit</mark> [14]) that require the user to supply a bound on the measurement noise. However, the condition is needed in our application, as such bounds on the prediction error (for each x) are not generally known beforehand.<br>",
    "Arabic": "السعي الأساس",
    "Chinese": "基追求",
    "French": "Poursuite de base",
    "Japanese": "ベイシス追求",
    "Russian": "Поиск базиса"
  },
  {
    "English": "Baum-Welch algorithm",
    "context": "1: Finally, for a training corpus of R samples, we seek to maximize the joint score: \n L(D; S, λ) = r L(D r ; S r , λ)(7) \n A local maximum can be found by employing the <mark>Baum-Welch algorithm</mark> (Baum et al., 1970;Baum, 1972).<br>",
    "Arabic": "خوارزمية باوم-ويلش",
    "Chinese": "鲍姆-韦尔奇算法",
    "French": "algorithme de Baum-Welch",
    "Japanese": "バウム・ウェルチアルゴリズム",
    "Russian": "Алгоритм Баума-Уэлча"
  },
  {
    "English": "Bayes",
    "context": "1: Furthermore, the tf-idf weighting scheme dominates the unweighted scheme, and so we adopt it henceforth for our other comparisons, and simply compare the IR and <mark>Bayes</mark> algorithms.<br>2: Table 3 shows, for various different values of the fraction f , the total number of non-zero entries over all internal nodes (i.e., the total number of values that must be maintained in order to execute   TBD), and the performance of the IR and <mark>Bayes</mark> algorithms using this smaller set of machine-generated metadata.<br>",
    "Arabic": "بيز",
    "Chinese": "贝叶斯",
    "French": "Bayes",
    "Japanese": "ベイズ",
    "Russian": "Байес"
  },
  {
    "English": "Bayes classifier",
    "context": "1: The <mark>Bayes classifier</mark>, ★ , minimizes the cross-entropy of ( ) with the observed tokens , with expectation taken on the whole data distribution. We let be the expected risk \n ( ) [log ( ) ],and \n set ★ argmin ∈ F ( X,D ( Y)) ( ). (6) \n<br>",
    "Arabic": "مصنف بايز",
    "Chinese": "贝叶斯分类器",
    "French": "Classifieur bayésien",
    "Japanese": "ベイズ分類器",
    "Russian": "Байесовский классификатор"
  },
  {
    "English": "Bayes factor",
    "context": "1: In case that the significance is not present, we consider two hypotheses as being equal. For determining the strength of the <mark>Bayes factor</mark> we resort to Kass and Raftery's [21]<br>2: B 1,2 = P(D|H 1 ) P(D|H 2 )(4) \n P(D|H) is the marginal likelihood (evidence) defined in Equation 3 and the <mark>Bayes factor</mark> can be seen as a summary of the evidence provided by the data in favor of one scientific hypothesis over the other.<br>",
    "Arabic": "عامل بايز",
    "Chinese": "贝叶斯因子",
    "French": "Facteur de Bayes",
    "Japanese": "ベイズ因子",
    "Russian": "Фактор Байеса"
  },
  {
    "English": "Bayes formula",
    "context": "1: Based onP (dup) = 0.05 and the estimated match score distributions, we used <mark>Bayes formula</mark> to compute the probability that a given match score s corresponds to a pair of duplicates: \n<br>",
    "Arabic": "قانون بايز",
    "Chinese": "贝叶斯公式",
    "French": "formule de Bayes",
    "Japanese": "ベイズの定理",
    "Russian": "формула Байеса"
  },
  {
    "English": "Bayes net",
    "context": "1: As an alternative to this naive Bayes method, we also tried to learn a general <mark>Bayes net</mark> (i.e., without any constraints on the network structure), allowing a feature node to be possibly connected to several segment nodes.<br>2: However, when learning the structure between the imaging-feature nodes and the segment nodes, the number of available training cases with images, namely 220, was very small compared to the number of 96 feature nodes in the <mark>Bayes net</mark>. This let to large uncertainty concerning the learned structure, and possibly to overfitting. Not surprisingly , we found that , on the test set , the general <mark>Bayes net</mark> learned from this small amount of training data actually gave results inferior to the ones of the naive Bayes method All continuous valued features were discretized using Recursive Minimal Entropy Partitioning [ Dougherty et al. , 1995 ] , a method based on minimal entropy heuristic , presented<br>",
    "Arabic": "شبكة بايزية",
    "Chinese": "贝叶斯网络",
    "French": "réseau bayésien",
    "Japanese": "ベイズネット",
    "Russian": "Байесовская сеть"
  },
  {
    "English": "Bayes optimal classifier",
    "context": "1: On the other hand, we demonstrate the benefit of overparametrization, showing that as the width of the second layer grows, the probability of ballistically converging to a <mark>Bayes optimal classifier</mark> goes to 1; this is a mathematically rigorous example of the lottery ticket hypothesis of [30].<br>2: a <mark>Bayes optimal classifier</mark> as the width grows , and as long as the right signature is present in the nodes at initialization , the SGD will ballistically converge to a global minimizer of the population loss . This is a rigorous example of the well-known lottery ticket hypothesis of [30].<br>",
    "Arabic": "مصنف بايز الأمثل",
    "Chinese": "贝叶斯最优分类器",
    "French": "Classificateur optimal de Bayes",
    "Japanese": "ベイズ最適分類器",
    "Russian": "Байесовский оптимальный классификатор"
  },
  {
    "English": "Bayes risk",
    "context": "1: is characteristic [ 6,7 ] . On the other hand, if constant functions are included in H, then it is easy to show that the characteristic property of k is also sufficient to achieve the <mark>Bayes risk</mark>.<br>2: The importance of using characteristic RKHSs is further underlined by this link: if the property does not hold, then there exist distributions that are unclassifiable in the RKHS H. We further strengthen this by showing that characteristic kernels are necessary (and sufficient under certain conditions) to achieve <mark>Bayes risk</mark> in the kernel-based classification algorithms.<br>",
    "Arabic": "مخاطر بايز",
    "Chinese": "贝叶斯风险",
    "French": "risque de Bayes",
    "Japanese": "ベイズリスク",
    "Russian": "Байесовский риск"
  },
  {
    "English": "Bayes risk decoding",
    "context": "1: In contrast, minimum <mark>Bayes risk decoding</mark> (MBR) (Goel and Byrne, 2000) outputs: \n y M BR = arg max y Eȳ ∼p θ (•|x) [u(y,ȳ)] = arg max y U (y, p θ (•|x)),(1) \n<br>",
    "Arabic": "فك تشفير المخاطر بايزية",
    "Chinese": "贝叶斯风险解码",
    "French": "Décodage du risque bayésien minimal",
    "Japanese": "ベイズリスク復号化",
    "Russian": "декодирование с минимальным риском Байеса"
  },
  {
    "English": "Bayes rule",
    "context": "1: The second conditions the distribution on an observation z (t+1) using <mark>Bayes rule</mark>: P (σ (t+1) |z (1) , . . . , z (t+1) ) ∝ P (z (t+1) |σ (t+1) )P (σ (t+1) |z (1) , . . .<br>2: where k ranges over all existing clusters and a new one. Using <mark>Bayes rule</mark>: \n P (k|F ) = P (k)P (F |k) P (F ) ∝ P (k)P (F |k) (2) \n<br>",
    "Arabic": "قاعدة بايز",
    "Chinese": "贝叶斯定理",
    "French": "règle de Bayes",
    "Japanese": "ベイズの定理",
    "Russian": "Правило Байеса"
  },
  {
    "English": "Bayes theorem",
    "context": "1: With an additional prior pose distribution p(y), we can derive the posterior pose p(y|X) via the <mark>Bayes theorem</mark>. Using an uninformative prior, the posterior density is simplified to the normalized likelihood: \n<br>",
    "Arabic": "نظرية بايز",
    "Chinese": "贝叶斯定理",
    "French": "théorème de Bayes",
    "Japanese": "ベイズの定理",
    "Russian": "Теорема Байеса"
  },
  {
    "English": "Bayes-Nash equilibrium",
    "context": "1: π T i,λi := 1 T T t=1 π t i,λi of each player i is a C log T T \n -approximate <mark>Bayes-Nash equilibrium</mark> strategy. In fact, a strong guarantee on the last-iterate convergence of the algorithm can be obtained too: Theorem 2 (abridged; Last-iterate convergence of piKL in 2p0s games).<br>2: In this section we study the last-iterate convergence of DiL-piKL, establishing that in two-player zero-sum games DiL-piKL converges to the (unique) <mark>Bayes-Nash equilibrium</mark> of the regularized Bayesian game.<br>",
    "Arabic": "توازن بايز-ناش",
    "Chinese": "贝叶斯-纳什均衡",
    "French": "Équilibre de Bayes-Nash",
    "Japanese": "ベイズ・ナッシュ均衡",
    "Russian": "Равновесие Байеса-Нэша"
  },
  {
    "English": "Bayesian Network",
    "context": "1: We checked the validity of the obtained structure using anatomical knowledge of the heart and medical rules as described by doctors. The resultant <mark>Bayesian Network</mark> classifier depends only on a small subset of numerical features extracted from dual-contours tracked through time and selected using a filterbased approach.<br>2: In the language of Bayesian networks, we can achieve this by removing the edges from all T i and S j variables to the R ijk variables for this particular k. With this view of \"activating\" relationships by including the edges in the <mark>Bayesian Network</mark>, we can formulate our search for R as a structure learning problem.<br>",
    "Arabic": "شبكة بايزية",
    "Chinese": "贝叶斯网络",
    "French": "Réseau bayésien",
    "Japanese": "ベイジアンネットワーク",
    "Russian": "Байесовская сеть"
  },
  {
    "English": "Beam Search",
    "context": "1: This can be particularly expensive for the larger models such as Grover mega. MAUVE, on the other hand, is inexpensive in comparison. <mark>Beam Search</mark>. We also calculate MAUVE for beam search in Table 11. MAUVE is able to quantify the qualitative observations of Holtzman et al.<br>",
    "Arabic": "البحث الشعاعي",
    "Chinese": "波束搜索",
    "French": "Recherche en faisceau",
    "Japanese": "ビーム探索",
    "Russian": "Лучевой поиск"
  },
  {
    "English": "Bellman",
    "context": "1: A decision rule ∆ = {∆(s) ∈ Q : s ∈ S} is η-optimal for the <mark>Bellman</mark> \n<br>2: While divergence is typically attributed to the interaction of the approximator with <mark>Bellman</mark> or Q-backups, the example shows that if we correct for delusional bias, convergent behavior is restored. Lack of convergence due to cyclic behavior (with a lower-bound on learning rates) can also be caused by delusion: see Appendix A.3.<br>",
    "Arabic": "بيلمان",
    "Chinese": "贝尔曼",
    "French": "Bellman",
    "Japanese": "ベルマン",
    "Russian": "Беллман"
  },
  {
    "English": "Bellman backup",
    "context": "1: A tabular version of Q-learning using the same partition-function representation of Q-values as in PCVI yields policy-class Q-learning PCQL, shown in Alg. 2. 2 The key difference with PCVI is simply that we use sample backups in Line 4 instead of full <mark>Bellman backups</mark> as in PCVI.<br>2: These facts allow for an efficient implementation of value iteration on M : we start at the t = 0 layer and apply <mark>Bellman backups</mark> until the rewards \"bubble up\" to the top.<br>",
    "Arabic": "إرجاع بلمان",
    "Chinese": "贝尔曼备份",
    "French": "Mise à jour de Bellman",
    "Japanese": "ベルマンバックアップ",
    "Russian": "Беллмановское резервное копирование"
  },
  {
    "English": "Bellman equation",
    "context": "1: Additionally, Theorem 5 implies that Markov time-homogeneous policies, achieving arbitrarily small errors in terms of satisfying the DR <mark>Bellman equation</mark> (3.1), will yield near optimal performance with arbitrarily small optimality gap in the context of the maxmin control.<br>2: Then V is the optimal value function for the MDP M ⊥ = S × {g 0 }, A, P ⊥ , R ⊥ , defined the same way as M π in Section 4.1, with π =⊥. Hence, the \n <mark>Bellman equation</mark> gives V ( s , g 0 ) = max a∈A R ⊥ ( s , θ , a ) +γ • E ( s ′ , g 0 ) ∼P ⊥ ( ( s , g 0 ) , a , • ) V ( s ′ , g 0 ) = max a∈A E θ∼µs R ( s , θ<br>",
    "Arabic": "معادلة بيلمان",
    "Chinese": "贝尔曼方程",
    "French": "équation de Bellman",
    "Japanese": "ベルマン方程式",
    "Russian": "уравнение Беллмана"
  },
  {
    "English": "Bellman error",
    "context": "1: Somewhat related to our approach, the CQL algorithm (Kumar et al., 2020) trains a critic Q by maximizing the combination of a lower bound on J(π Q ) − J(µ), where π Q is an implicit policy parameterized by Q, along with a <mark>Bellman error</mark> term for the current actor policy.<br>2: (6), the first two terms are controlled by the <mark>Bellman error</mark> (both onsupport and off-support), and the third is controlled by the optimization error. Notably, when the comparator π is the behavior policy µ, the first two terms in Eq. ( 6) cancel out, giving the faster rate of Proposition 6.<br>",
    "Arabic": "خطأ بيلمان",
    "Chinese": "贝尔曼误差",
    "French": "erreur de Bellman",
    "Japanese": "ベルマン誤差",
    "Russian": "ошибка Беллмана"
  },
  {
    "English": "Bellman operator",
    "context": "1: π k ) m T h−1 v k−1 ≤ T h−1 T ( T π k ) m T h−1 v k−1 − T h ( T π k ) m T h−1 v k−1 = 0 . In the third relation we used Lemma 1 due to the assumption that ( v k−1 , π k ) is h-greedy consistent and the monotonicity of T h−1 , in the forth relation we used the definition of the optimal <mark>Bellman operator</mark> , i.e. , T πv ≤ Tv , and the monotonicity of T h−1 , and in the last relation we<br>2: For a policy π, the <mark>Bellman operator</mark> T π is defined as (T π f ) (s, a) := R(s, a) + γE s |s,a [f (s , π)] \n<br>",
    "Arabic": "مشغل بيلمان",
    "Chinese": "贝尔曼算子",
    "French": "opérateur de Bellman",
    "Japanese": "ベルマン演算子",
    "Russian": "оператор Беллмана"
  },
  {
    "English": "Berkeley parser",
    "context": "1: The Flickr data is messier than datasets created specifically for vision training, but provides the largest corpus of natural descriptions of images to date. We normalize the text by removing emoticons and mark-up language, and parse each caption using the <mark>Berkeley parser</mark> (Petrov, 2010).<br>2: We can see that the effect of spelling errors is quite small. The <mark>Berkeley parser</mark>'s mechanism for handling unknown words makes use of suffix information and it is able to ignore many of the content word spelling errors.<br>",
    "Arabic": "محلل بيركلي",
    "Chinese": "伯克利句法分析器",
    "French": "analyseur syntaxique de Berkeley",
    "Japanese": "バークレー・パーサー",
    "Russian": "Парсер Беркли"
  },
  {
    "English": "Berkeley segmentation dataset",
    "context": "1: Due to the increased amount of model parameters, we train RTF 2 and each subsequent stage with 500 training images, randomly cropped from the training portion of the <mark>Berkeley segmentation dataset</mark> [1] and blurred with randomly chosen artificial blur kernels (different at each stage).<br>",
    "Arabic": "مجموعة بيانات تجزئة بيركلي",
    "Chinese": "伯克利分割数据集",
    "French": "Ensemble de données de segmentation de Berkeley",
    "Japanese": "バークレー分割データセット",
    "Russian": "Беркли набор данных для сегментации"
  },
  {
    "English": "Bernoulli",
    "context": "1: M i,j = 1 1−δ i,j M i,j , where i,j ∼ <mark>Bernoulli</mark>(1 − δ) \n . The applied GNN model then propagates information via the perturbed message matrix M instead of the original message matrix. It should be moted that DropMessage only affects on the training process. Practical implementation.<br>2: To encourage sparsity by reducing the number of mutable features chosen to be modified, we regularize S through L1-norm ∥ ( ) ∥ 1 with ( ) = [ ( )] ∈K . To summarize , given a one-hot vector representation of a data example , we use G to work out the local feature-based perturbation distribution Cat ( ( ) ) for every ∈ K. We then sample ∼ Cat ( ( ) ) for every ∈ K. Subsequently , we use S to work out the local feature-based selection distribution <mark>Bernoulli</mark> ( ( ) )<br>",
    "Arabic": "بيرنولي",
    "Chinese": "伯努利分布",
    "French": "Bernoulli",
    "Japanese": "ベルヌーイ",
    "Russian": "Бернулли"
  },
  {
    "English": "Bernoulli distribution",
    "context": "1: Formally, this operation can be regarded as a sampling process. For each element M i,j in the message matrix, we generate an independent mask i,j to determine whether it will be preserved or not, according to a <mark>Bernoulli distribution</mark> i,j ∼ Bernoulli(1 − δ).<br>2: From the definition of the classifiers each of these random variables follow a <mark>Bernoulli distribution</mark> B (1, p). Let V 1 , . . . , V C be C independent random variables encoding the probability that a triplet classifier abstains on all the training examples: \n V c = n i=1 Y i,c . (41) \n<br>",
    "Arabic": "توزيع برنولي",
    "Chinese": "伯努利分布",
    "French": "distribution de Bernoulli",
    "Japanese": "ベルヌーイ分布",
    "Russian": "распределение Бернулли"
  },
  {
    "English": "Bernoulli likelihood",
    "context": "1: For instance, the original EP (Minka, 2001) considers Gaussian mixture likelihood (or <mark>Bernoulli likelihood</mark> for classification) and the moments can be directed obtained by the properties of Gaussian (or integration by parts). Besides, at the cost of the tractability, there is no converge guarantee of EP in general.<br>",
    "Arabic": "احتمالية برنولي",
    "Chinese": "伯努利似然",
    "French": "Vraisemblance de Bernoulli",
    "Japanese": "ベルヌーイ尤度",
    "Russian": "Вероятность Бернулли"
  },
  {
    "English": "Bernoulli random variable",
    "context": "1: the routing direction is the output of a <mark>Bernoulli random variable</mark> with mean d n (x; Θ). Once a sample ends in a leaf node ℓ, the related tree prediction is given by the class-label distribution π ℓ .<br>2: Therefore, we use the <mark>Bernoulli random variable</mark> I (o, t) denoting the states whether o impresses t, where I (o, t) = 1 denotes that o delivers an impression to t, otherwise I (o, t) = 0. Definition 3.1.<br>",
    "Arabic": "المتغير العشوائي بيرنولي",
    "Chinese": "伯努利随机变量",
    "French": "Variable aléatoire de Bernoulli",
    "Japanese": "ベルヌーイ確率変数",
    "Russian": "Бернуллиевская случайная переменная"
  },
  {
    "English": "Bernoulli sampling",
    "context": "1: However, in intuition, the dropping of features, edges, nodes or messages will all eventually act on the message matrix. It inspires us to explore the theoretical connection between different dropping methods. As a start, we demonstrate that Dropout, DropEdge, DropNode, and DropMessage can all be formulated as <mark>Bernoulli sampling</mark> processes in Table 1.<br>2: • We enforce hierarchical segmentation behavior by multiplicatively masking the segmentation decision at layer l with the segmentation decision at layer l − 1, thus preventing higher layers from segmenting where lower layers do not. • We compute boundaries during training via <mark>Bernoulli sampling</mark> rather than rounding.<br>",
    "Arabic": "أخذ عينات برنولي",
    "Chinese": "伯努利抽样",
    "French": "échantillonnage de Bernoulli",
    "Japanese": "ベルヌーイサンプリング",
    "Russian": "Сэмплирование Бернулли"
  },
  {
    "English": "Bernoulli trial",
    "context": "1: P (x,y)∼D S C c=1 V c Y x,c = 0 = P (x,y)∼D S C c=1 U c = c(45) \n which, by definition of the random variables U • , corresponds to the probability of obtaining c successes among c <mark>Bernoulli trials</mark>.<br>2: Subsampling Poisson processes: Random subsampling of a Poisson process via independent <mark>Bernoulli trials</mark> yields a new Poisson process. Theorem 4 (Subsampling Theorem). Let Π ∼ PoissonP(µ) be a Poisson process on the space Ω, and q : Ω → [0, 1] be a measurable function. If we independently draw z θ ∈ { 0 , 1 } for each θ ∈ Π 0 with P ( z θ = 1 ) = q ( θ ) , and let Π k = { θ ∈ Π : z θ = k } for k = 0 , 1 , then Π 0 and Π 1 are independent Poisson processes<br>",
    "Arabic": "محاولة برنولي",
    "Chinese": "伯努利试验",
    "French": "épreuve de Bernoulli",
    "Japanese": "ベルヌーイ試行",
    "Russian": "испытание Бернулли"
  },
  {
    "English": "Bernoulli variable",
    "context": "1: 2) Sampling ∼ Bernoulli( | ). : We again apply the Gumbel-Softmax trick to relax <mark>Bernoulli variables</mark> of 2 categories.<br>",
    "Arabic": "متغير برنولي",
    "Chinese": "伯努利变量",
    "French": "Variable de Bernoulli",
    "Japanese": "ベルヌーイ変数",
    "Russian": "Бернуллиевская переменная"
  },
  {
    "English": "Bernstein's inequality",
    "context": "1: The proof is exactly the same as Theorem D.1 as | P Ω (N ), P Ω (W ) | is a sum of independent entries that follows from the same <mark>Bernstein's inequality</mark>. Next we show that random sampling entries of a Gaussian matrix gives a matrix with low spectral norm. Lemma D.6.<br>",
    "Arabic": "عدم المساواة في برنشتاين",
    "Chinese": "伯恩斯坦不等式",
    "French": "Inégalité de Bernstein",
    "Japanese": "ベルンシュタインの不等式",
    "Russian": "Неравенство Бернштейна"
  },
  {
    "English": "Bethe approximation",
    "context": "1: Similar results showing that clamping improves partition function estimation have been obtained for the mean field and TRW approximations (Weller & Domke, 2016), and in certain settings for the <mark>Bethe approximation</mark> (Weller & Jebara, 2014b) and L-FIELD (Zhao et al., 2016).<br>2: Both LOCAL(G) and M have the same integral vertices for general graphs [11,6]. Belief propagation can be seen as optimizing pseudomarginals over LOCAL(G) with a (non-convex) <mark>Bethe approximation</mark> to the entropy [15].<br>",
    "Arabic": "تقريب بيثي",
    "Chinese": "贝特近似",
    "French": "approximation Bethe",
    "Japanese": "ベーテ近似",
    "Russian": "Приближение Бете"
  },
  {
    "English": "Bhattacharyya coefficient",
    "context": "1: According to Section 3, the most probable location y of the target in the current frame is obtained by minimizing the distance (18), which is equivalent t o m a x imizing the Bhattacharyya coe cient^ (y).<br>2: The dissimilarity between the target model (its color distribution) and the target candidates is expressed by a metric derived f r om the Bhattacharyya coe cient. The theoretical analysis of the approach shows that it relates to the Bayesian framework while providing a practical, fast and e cient solution.<br>",
    "Arabic": "معامل بهاتاشاريا",
    "Chinese": "巴查里亚系数",
    "French": "coefficient de Bhattacharyya",
    "Japanese": "バタチャリヤ係数",
    "Russian": "коэффициент Бхаттачарьи"
  },
  {
    "English": "Boltzmann distribution",
    "context": "1: The function ϕ was parameterized to concatenate the state features of both s and s and use a single layer neural network to project to a space corresponding to the inferred structure in the actions. The functionφ was linearly parameterized to compute a <mark>Boltzmann distribution</mark> over the available set of actions.<br>2: We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a <mark>Boltzmann distribution</mark> with an exponentially decaying temperature parameter.<br>",
    "Arabic": "توزيع بولتزمان",
    "Chinese": "玻尔兹曼分布",
    "French": "distribution de Boltzmann",
    "Japanese": "ボルツマン分布",
    "Russian": "распределение Больцмана"
  },
  {
    "English": "Boltzmann exploration",
    "context": "1: Motivated by the above, we study a smooth variant of stateless Q-learning, with softmax or <mark>Boltzmann exploration</mark> (one of the most fundamental models of exploration-exploitation in MAS), termed Boltzmann Q-learning or smooth Q-learning (SQL), which has recently received a lot of attention due to its connection with evolutionary game theory Tuyls et al.<br>2: We chose to parametrize the intra-option policies with Boltzmann distributions and the terminations with sigmoid functions. The policy over options was learned using intra-option Q-learning. We also implemented primitive actor-critic (denoted AC-PG) using a Boltzmann policy. We also compared option-critic to a primitive SARSA agent using <mark>Boltzmann exploration</mark> and no eligibility traces.<br>",
    "Arabic": "استكشاف بولتزمان",
    "Chinese": "波尔兹曼探索",
    "French": "Exploration de Boltzmann",
    "Japanese": "ボルツマン探索",
    "Russian": "Эксплорация Больцмана"
  },
  {
    "English": "Bonferroni correction",
    "context": "1: Analysis (step 6): We compute the Pearson's r correlation between the LabintheWild annotations by demographic for the dataset's original labels and the models' predictions. We apply the <mark>Bonferroni correction</mark> to account for multiple hypothesis testing. 2017); instead of monetary compensation, participants typically partake in LabintheWild experiments because they learn something about themselves.<br>2: We have 64% probability to get at least one spurious result. Determining the statistical 19:4 • X. Zhang et al. significance of the association between the phenotype and SNPs is crucial. <mark>Bonferroni correction</mark> based on the assumption that all n tests are independent is too conservative for the genome-wise association studies since SNPs are often correlated.<br>",
    "Arabic": "تصحيح بونفيروني",
    "Chinese": "本富罗尼校正",
    "French": "Correction de Bonferroni",
    "Japanese": "ボンフェローニ補正",
    "Russian": "Коррекция Бонферрони"
  },
  {
    "English": "Borda score",
    "context": "1: The method constructs the vote of each manipulator in turn: candidate d is put in first place, and the remaining candidates are put in reverse order of their current <mark>Borda scores</mark>. The method continues constructing manipulating votes until d wins.<br>",
    "Arabic": "تقييم بوردا",
    "Chinese": "波达分数",
    "French": "score de Borda",
    "Japanese": "ボルダ得点",
    "Russian": "Бордовые баллы"
  },
  {
    "English": "Bradley-Terry Model",
    "context": "1: We report the correlation of MAUVE with obtained Bradley-Terry scores. We compute the Bradley-Terry (BT) scores from the pairwise preferences obtained from the human evaluation along each of the three axes interesting, sensible and more likely to be written by a human. <mark>Bradley-Terry Model</mark> Review.<br>",
    "Arabic": "نموذج برادلي-تيري",
    "Chinese": "布拉德利-特里模型",
    "French": "Modèle Bradley-Terry",
    "Japanese": "ブラッドリー・テリーモデル",
    "Russian": "Модель Брэдли-Терри"
  },
  {
    "English": "Branch and Bound",
    "context": "1: Although we describe only the modified backtracking method, which is used by two of the five methods that the solver adopts: <mark>Branch and Bound</mark>; and Iterative <mark>Branch and Bound</mark>.<br>",
    "Arabic": "تفريع وحصر",
    "Chinese": "分支定界法",
    "French": "Séparation et évaluation",
    "Japanese": "分枝限定法",
    "Russian": "Метод ветвей и границ"
  },
  {
    "English": "Bregman's method",
    "context": "1: <mark>Bregman's method</mark> is a simple iterative process. We start with the scores s and then cyclically iterate over the constraints and project the current estimate x i onto the chosen constraint until convergence: \n x 0 = exp s τ x i+1 = arg min x∈C i mod (n−1) KL(x || x i ) (10 \n ) \n<br>2: In order to apply Bregmans' method to solve the entropy regularized version of Eq. (6), we need to decompose the constraints into sets which we can efficiently project onto. We choose the following three sets here: (i), containing Eq. (6a) and Eq.<br>",
    "Arabic": "طريقة بريغمان",
    "Chinese": "布雷格曼方法",
    "French": "méthode de Bregman",
    "Japanese": "ブレグマン法",
    "Russian": "метод Брегмана"
  },
  {
    "English": "Chamfer distance",
    "context": "1: ( top , leg , support ) → ( top , leg ) . As quantitative metrics for reconstruction tasks, we report symmetric <mark>Chamfer Distance</mark> (CD, scaled by ×1000) and Normal Consistency (NC) computed on 4k surface sampled points.<br>2: We set ε=0.01, and generate our edge sampling by retaining points such that σ(s i )<0.1; see Figure 8. Given two shapes, the ECD between them is nothing but the <mark>Chamfer Distance</mark> between the corresponding edge samplings. Analysis. Our method achieves comparable performance to the state-of-the-art in terms of <mark>Chamfer Distance</mark>.<br>",
    "Arabic": "مسافة الشطب",
    "Chinese": "倒角距离",
    "French": "Distance de chanfrein",
    "Japanese": "チャンファー距離",
    "Russian": "Расстояние Чамфера"
  },
  {
    "English": "Charniak parser",
    "context": "1: We compare the performance of our forest reranker against n-best reranking on the Penn English Treebank (Marcus et al., 1993). The baseline parser is the <mark>Charniak parser</mark>, which we modified to output a   (Collins, 2000), and others are from (Charniak and Johnson, 2005), with simplifications.<br>2: Note that the performance degradation is quite large, more than has been reported for the <mark>Charniak parser</mark> on the Brown corpus. We examine the parser output for each sentence in the development set. The phenomena which lead the parser astray are listed in Table 3.<br>",
    "Arabic": "محلل تشارنياك",
    "Chinese": "查尼亚克解析器",
    "French": "analyseur Charniak",
    "Japanese": "チャーニアク構文解析器",
    "Russian": "Парсер Чарньяка"
  },
  {
    "English": "Chebyshev acceleration",
    "context": "1: Accelerating synchronous gossip algorithms have been studied in previous works, including SSDA [47], <mark>Chebyshev acceleration</mark> [39], Jacobi-Polynomial acceleration [7].<br>",
    "Arabic": "تسارع تشيبيشيف",
    "Chinese": "切比雪夫加速",
    "French": "Accélération de Tchebychev",
    "Japanese": "チェビシェフ加速",
    "Russian": "Ускорение Чебышева"
  },
  {
    "English": "Chebyshev polynomial",
    "context": "1: Here, T m (H ) is the mth <mark>Chebyshev polynomial</mark> of the matrix H . The last equality comes from the spectral mapping theorem, which says that taking a polynomial of H maps the eigenvalues by the same polynomial. Similarly, we express the PDOS µ k (λ) as \n<br>",
    "Arabic": "متعددات تشيبيشيف",
    "Chinese": "切比雪夫多项式",
    "French": "polynôme de Chebyshev",
    "Japanese": "チェビシェフ多項式",
    "Russian": "Многочлен Чебышева"
  },
  {
    "English": "Chomsky normal form",
    "context": "1: Treebank 'S' categories are similarly distinguished into inflected sentences V, infinitival sentences I, base form sentences B, adjectival sentences (small clauses) A, and participial sentences L. 7 \n Also, like other categorial grammars, this transform leaves trees in binary branching (Chomsky normal) form.<br>2: A probabilistic context-free grammar (PCFG) in <mark>Chomsky normal form</mark> can be defined as a 6-tuple (S, N , P, Σ, R, Π), where S is the start symbol, N , P and Σ are the set of nonterminals, preterminals and terminals, respectively.<br>",
    "Arabic": "صيغة تشومسكي العادية",
    "Chinese": "乔姆斯基范式",
    "French": "forme normale de Chomsky",
    "Japanese": "チョムスキー標準形",
    "Russian": "Нормальная форма Хомского"
  },
  {
    "English": "Chu-Liu-Edmonds algorithm",
    "context": "1: We will use here the <mark>Chu-Liu-Edmonds algorithm</mark> (Chu and Liu, 1965;Edmonds, 1967), sketched in Figure 3 following Leonidas (2003). Informally, the algorithm has each vertex in the graph greedily select the incoming edge with highest weight. If a tree results, it must be the maximum spanning tree.<br>2: We illustrate here the application of the <mark>Chu-Liu-Edmonds algorithm</mark> to dependency parsing on the simple example x = John saw Mary, with directed graph representation G x , If the result were a tree, it would have to be the maximum spanning tree.<br>",
    "Arabic": "خوارزمية تشو ليو إدموندز",
    "Chinese": "朱刘艾德蒙兹算法",
    "French": "algorithme de Chu-Liu-Edmonds",
    "Japanese": "中留・リュー・エドモンズ法",
    "Russian": "алгоритм Чу-Лю-Эдмондса"
  },
  {
    "English": "Chung-Lu model",
    "context": "1: Next, consider another instance H of the <mark>Chung-Lu model</mark> with N nodes with weights w 1 , . . . , w N , such that w i = n i p/2. The probability of edge (i, j) in H equals \n<br>2: BTER constructs a similar graph by a two-step process: first create a collection of Erdös-Rényi subgraphs, then interconnect those using a Chung-Lu  model [8]. Seshadhri et al. showed their model accurately captures the observable properties of the given graph, including the eigenvalues of the adjacency matrix.<br>",
    "Arabic": "نموذج تشونغ-لو",
    "Chinese": "中鲁模型",
    "French": "modèle de Chung-Lu",
    "Japanese": "チャン・ルーモデル",
    "Russian": "Модель Чунг-Лу"
  },
  {
    "English": "Cohen's kappa",
    "context": "1: The table reports the \"agreement\" on the full 4-point relevance scale on real and synthetic queries, respectively. For both real and synthetic queries, we observe a fair level of agreement between synthetic judgements generated using GPT-4 and manual judgments: The Cohen's on real queries is 0.24 and on synthetic queries is 0.26.<br>2: Figure 2 shows a simplified version of the decision tree used by the annotators. Inter-annotator agreement was measured with <mark>Cohen's kappa</mark> and was reasonably high (κ = 0.77). The annotators discussed cases of disagreement and arrived at a consensus label for the fi-7 See the supplementary material for more details on the selected files.<br>",
    "Arabic": "معامل كوهين كابا",
    "Chinese": "科恩的kappa",
    "French": "kappa de Cohen",
    "Japanese": "コーエンのカッパ",
    "Russian": "Коэффициент каппа Коэна"
  },
  {
    "English": "Cohen's kappa coefficient",
    "context": "1: For a missing argument position, the student's annotation agreed with our own if both identified the same constituent or both left the position unfilled. Analysis indicated an agreement of 67% using <mark>Cohen's kappa coefficient</mark> (Cohen, 1960).<br>",
    "Arabic": "معامل كوهين لكابا",
    "Chinese": "科恩的Kappa系数",
    "French": "Coefficient kappa de Cohen",
    "Japanese": "コーヘンのカッパ係数",
    "Russian": "коэффициент каппа Коэна"
  },
  {
    "English": "Cohen's κ",
    "context": "1: . The subjective difficulty reported is the average z-score of difficulty scores picked by the annotators. This is done to normalize the variability of subjective ratings. The inter-rater reliability for the entire exercise was measured using the <mark>Cohen's κ</mark> for two annotators, which was calculated to be 0.37 (fair agreement), with an overlap of 66%.<br>2: Two experts A and B first labelled a random subset of 8% of stories, yielding moderate agreement (<mark>Cohen's κ</mark> = .62).<br>",
    "Arabic": "معامل كوهين كابا",
    "Chinese": "Cohen's κ系数",
    "French": "Le κ de Cohen",
    "Japanese": "コーヘンのカッパ係数",
    "Russian": "Коэффициент κ Коэна"
  },
  {
    "English": "Condorcet winner",
    "context": "1: Note that W is a subset of candidates whose domination graph has a perfect matching, i.e., potential winners of PLURALITYMATCHING. This has several immediate implications. First, it is possible that for all orderings of voters, the selected candidate fails to be a <mark>Condorcet winner</mark>, as shown for PLURALITY-MATCHING by Gkatzelis et al. [19].<br>2: jk ) Stochastic triangle inequality ( STI ) ∆ ij > 0 , ∆ jk > 0 =⇒ ∆ ik ≤ ∆ ij + ∆ jk <mark>Condorcet winner</mark> ( CW ) ∃i * : ∆ i * , j > 0 , ∀j ∈ S − i *<br>",
    "Arabic": "الفائز كوندورسيه",
    "Chinese": "康多塞特获胜者",
    "French": "Vainqueur de Condorcet",
    "Japanese": "コンドルセ勝者",
    "Russian": "Победитель Кондорсе"
  },
  {
    "English": "Contrastive Learning",
    "context": "1: <mark>Contrastive Learning</mark> (CL) [12], [34] is a framework that learns discriminative representations through the use of instance similarity/dissimilarity. A plethora of works has explored the effectiveness of CL in unsupervised representation learning [12], [34], [35].<br>2: Despite the success of these approaches for BERT models, attempts to quantize generative PLMs are scarce, and the underlying difficulty remains unclear. <mark>Contrastive Learning</mark>. Contrastive learning aims at pushing the representations of similar samples together while pulling those of dissimilar ones apart.<br>",
    "Arabic": "التعلم التباينيّ",
    "Chinese": "对比学习",
    "French": "Apprentissage contrastif",
    "Japanese": "対照学習",
    "Russian": "Контрастное обучение"
  },
  {
    "English": "Coreset",
    "context": "1: 12 ] . A practical implementation of the coreset paradigm is due to Ackermann, Lammersen, Martens, Raupach, Sohler, and Swierkot [2]. Their approach was shown empirically to be fast and accurate on a variety of benchmarks.<br>2: The only other minor change is that the number of points in the coreset S, which is obtained by taking the union of all S •,• , is now at most twice the previous bound, which is easily absorbed in the big-oh notation.<br>",
    "Arabic": "مجموعة أساسية",
    "Chinese": "核心集",
    "French": "Coeur d'ensemble",
    "Japanese": "コアセット",
    "Russian": "ядро (coreset)"
  },
  {
    "English": "Corpora",
    "context": "1: <mark>Corpora</mark> are split into a 60/40 train/test split. Selected target identities and the size of each corpus can be found in Table 2. These identity-specific corpora, which are samples of existing publicly available datasets, are available at https://osf.io/53tfs/.<br>",
    "Arabic": "المواد اللغوية",
    "Chinese": "语料库",
    "French": "Corpus",
    "Japanese": "コーパス",
    "Russian": "Корпусы"
  },
  {
    "English": "Corpus",
    "context": "1: In this paper, we address this gap. Our work has three parts. (i) <mark>Corpus</mark> collection. We collect Glot2000-c, a corpus covering thousands of tail languages. (ii) Model training. Using Glot500-c, a subset of Glot2000-c, we train Glot500-m, an LLM covering 511 languages. (iii) Validation.<br>2: <mark>Corpus</mark> and query-log features enrich the query representation beyond the query string and focus on two potentially complementary sources of evidence-corpus features relate to content production (i.e., content in the vertical) and query-log features relate to content demand (i.e., content sought by users).<br>",
    "Arabic": "مجموعة نصوص",
    "Chinese": "语料库",
    "French": "Corpus",
    "Japanese": "コーパス",
    "Russian": "корпус"
  },
  {
    "English": "Cosine Similarity",
    "context": "1: (The metrics are described in Section 6.3). If we evaluate the redundancy measures by the percentage of mistakes they make, the <mark>Cosine Similarity</mark> and Mixture Model redundancy measures are much better than the rest.<br>",
    "Arabic": "التشابه الكوسيني",
    "Chinese": "余弦相似度",
    "French": "Similarité cosinus",
    "Japanese": "コサイン類似度",
    "Russian": "Косинусное сходство"
  },
  {
    "English": "Cosine distance",
    "context": "1: • Euclidean distance: d(x, y) = x − y 2 , • Cityblock distance: d(x, y) = x − y 1 , • <mark>Cosine distance</mark>: d(x, y) = 1 − x,y x 2 y 2 .<br>2: <mark>Cosine distance</mark> is a symmetric measure related to the angle between two vectors [6]. If we represent document d as a vector d = (w1(d), w2(d), .., wn(d)) T , then: \n<br>",
    "Arabic": "المسافة الكوسينية",
    "Chinese": "余弦距离",
    "French": "Distance cosinus",
    "Japanese": "コサイン距離",
    "Russian": "Косинусное расстояние"
  },
  {
    "English": "Covariance",
    "context": "1: Cov q(x0) [x 0 ] = E q(xn) Cov q(x0|xn) [x 0 ] + Cov q(xn) E q(x0|xn) [x 0 ]. (16 \n ) \n Proof. Since q ( x n |x 0 ) = N ( x n | √ α n x 0 , β n I ) , according to Lemma 11 , we have E q ( xn ) Cov q ( x0|xn ) [ x 0 ] = β n α n ( I − β n E qn ( xn ) ∇ xn log<br>2: Using the fact that the covariance of a linear transformation of a variable is a linear transformation of the variable's covariance (Cov[Ax, By] = A Cov[x, y]B T ) we can identify the mean and covariance of our conical frustum Gaussian after it has been lifted into the PE basis P: \n<br>",
    "Arabic": "التباين المشترك",
    "Chinese": "协方差",
    "French": "Covariance",
    "Japanese": "共分散",
    "Russian": "Ковариация"
  },
  {
    "English": "Datalog",
    "context": "1: min) predicates and negating their numeric arguments. In Section 1 we have shown that limit <mark>Datalog</mark> Z can compute the cost of shortest paths in a graph. We next present further examples of data analysis tasks that our formalism can handle.<br>2: Our main contribution is a new limit <mark>Datalog</mark> Z fragment that, like the existing data analysis languages, is powerful and flexible enough to naturally capture many important analysis tasks. However, unlike <mark>Datalog</mark> Z and the existing languages, reasoning with limit programs is decidable, and it becomes tractable in data complexity under an additional stability restriction.<br>",
    "Arabic": "داتالوج",
    "Chinese": "数据逻辑",
    "French": "Datalog",
    "Japanese": "データログ",
    "Russian": "Datalog"
  },
  {
    "English": "Dataset",
    "context": "1: <mark>Dataset</mark> We perform experiments on the BookCorpus (Radford et al., 2018a) and English Wikipedia with 800M and 2.5B words respectively.<br>2: A comprehensive GraphDataZoo is indispensable to provide a unified testbed for FGL. To satisfy the various experiment purposes, we allow users to constitute an FL dataset by configuring the choices of <mark>Dataset</mark>, Splitter, Transform, and Dataloader.<br>",
    "Arabic": "مجموعة البيانات",
    "Chinese": "数据集",
    "French": "ensemble de données",
    "Japanese": "データセット",
    "Russian": "Набор данных"
  },
  {
    "English": "Decoder",
    "context": "1: The base network is a fully-convolutional encoderdecoder network inspired by the DeepLabV3 [4] and DeepLabV3+ [5] architectures, which achieved state-ofthe-art performance on semantic segmentation tasks in 2017 and 2018. Our base network consists of three modules: Backbone, ASPP, and <mark>Decoder</mark>.<br>2: t | s ) to be very low . The word \"not\" is not optional, additional material. • <mark>Decoder</mark>. When we observe a long string t, we search for the short string s that maximizes P(s | t). This is equivalent to searching for the s that maximizes P(s) \n<br>",
    "Arabic": "مفسر",
    "Chinese": "解码器",
    "French": "Décodeur",
    "Japanese": "デコーダー",
    "Russian": "Декодер"
  },
  {
    "English": "Deep Belief Network",
    "context": "1: ( 2012) use a <mark>Deep Belief Network</mark> (DBN) for acoustic modeling and phone recognition on human speech. They analyze the impact of the number of layers on phone recognition error rate, and visualize the MFCC vectors as well as the learned activation vectors of the hidden layers of the model.<br>2: We compare the performance of our models to Variational Autoencoders (Kingma & Welling, 2013) and two other EBMs; an RBM and a <mark>Deep Belief Network</mark> (DBN) (Hinton, 2009). On most datasets, our Resnet EBM outperforms the other two EBMs and the VAEs.<br>",
    "Arabic": "شبكة الاعتقاد العميقة",
    "Chinese": "深度信念网络",
    "French": "Réseau de croyances profondes",
    "Japanese": "ディープ・ビリーフ・ネットワーク",
    "Russian": "Сеть глубоких убеждений"
  },
  {
    "English": "Deep learning",
    "context": "1: Not that surprising, the reproducibility on the very similar TREC <mark>Deep Learning</mark> 2020 is very good (88.1%) but declines fast for other tasks (e.g., only 57.8% for the Web track 2003 on rank 15).<br>",
    "Arabic": "التعلم العميق",
    "Chinese": "深度学习",
    "French": "Apprentissage profond",
    "Japanese": "深層学習",
    "Russian": "Глубокое обучение"
  },
  {
    "English": "Denoising Autoencoder",
    "context": "1: We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the <mark>Denoising Autoencoder</mark> that is trained on simulated views of a 3D model using Domain Randomization.<br>2: Interestingly, the training objective of a dominant approach like BERT, the prediction of corrupted inputs, closely resembles that of the <mark>Denoising Autoencoder</mark>, which was originally developed for images. As a higher dimensional, noisier, and more redundant modality than text, images are believed to be difficult for generative modeling.<br>",
    "Arabic": "ترميز تلقائي للحذف الضوضاء",
    "Chinese": "降噪自编码器",
    "French": "Autoencodeur de débruitage",
    "Japanese": "デノイジングオートエンコーダー",
    "Russian": "Автокодировщик с подавлением шума"
  },
  {
    "English": "Detectron",
    "context": "1: Applying GN to the backbone alone contributes a 0.5 AP gain (from 39.5 to 40.0), suggesting that GN helps when transferring features. Table 6 shows the full results of GN (applied to the backbone, box head, and mask head), compared with the standard <mark>Detectron</mark> baseline [13] based on BN * .<br>2: Since F2F forecasts at the FPN feature level, it is agnostic to second stage tasks. In our evaluation, we focus on the bounding box detection task instead of instance segmentation. [7]). We implement our own version of F2F based on mmdetection (instead of <mark>Detectron</mark> as done in [28]).<br>",
    "Arabic": "تحديدرون",
    "Chinese": "Detectron",
    "French": "Detectron",
    "Japanese": "Detectron",
    "Russian": "Detectron"
  },
  {
    "English": "Dijkstra's algorithm",
    "context": "1: Cohen [17] designed a modified <mark>Dijkstra's algorithm</mark> (Algorithm 1) to construct a data structure r * (s), called least label list, for each node s to support such query.<br>2: We ran all experiments on a 3.80GHz Intel(R) Core(TM) i7-10700K CPU Linux machine with 64GB of RAM. The cost components represent travel distances (c 1 ) and times (c 2 ). The input heuristic h corresponds to the exact travel distances and times to the goal state, computed with <mark>Dijkstra's algorithm</mark>.<br>",
    "Arabic": "خوارزمية ديكسترا",
    "Chinese": "Dijkstra算法",
    "French": "algorithme de Dijkstra",
    "Japanese": "ダイクストラのアルゴリズム",
    "Russian": "Алгоритм Дейкстры"
  },
  {
    "English": "Dirac measure",
    "context": "1: P m := 1 m Y i =1 δ Xi , Q n := 1 n Y i =−1 δ Xi , m = |{i : Y i = 1}| and n = N − m. δ x represents the <mark>Dirac measure</mark> at x. \n<br>2: Here δ x i represents the <mark>Dirac measure</mark> at x i ∈ M . So, we have µ X k = µ Y k, which is equivalent to \n Since k is strictly pd, by Lemma 16, we have<br>",
    "Arabic": "قياس ديراك",
    "Chinese": "狄拉克测度",
    "French": "mesure de Dirac",
    "Japanese": "ディラック測度",
    "Russian": "Мера Дирака"
  },
  {
    "English": "Dirichlet",
    "context": "1: By checking the constraints, we have not found any particular bound for the parameter, which may explain why the performance is much less sensitive to the parameter value than in the pivoted normalization method where a bound for parameter s is implied by the LNC2 constraint. Finally, the optimal values of µ in <mark>Dirichlet</mark> are shown in Table 8.<br>2: For clarity of presentation, we now focus on a model with K dynamic topics evolving as in ( 1), and where the topic proportion model is fixed at a <mark>Dirichlet</mark>. The technical issues associated with modeling the topic proportions in a time series as in ( 2) are essentially the same as those for chaining the topics together.<br>",
    "Arabic": "ديريشليت",
    "Chinese": "狄利克雷分布",
    "French": "Dirichlet",
    "Japanese": "ディリクレ分布",
    "Russian": "Дирихле"
  },
  {
    "English": "Dirichlet prior",
    "context": "1: In order to test these hypotheses, we run experiments over seven collections and four query sets by using the pivoted normalization method, the <mark>Dirichlet prior</mark> method, Okapi and the modified Okapi formula (which replaces the IDF part in Okapi with the IDF part in the pivoted normalization formula). We use average precision as the evaluation measure.<br>2: With a notation consistent with those in the pivoted normalization and Okapi formulas, the <mark>Dirichlet prior</mark> retrieval function is \n w∈q∩d c(w, q) • ln(1 + c(w, d) µ • p(w|C) ) + |q| • ln µ |d| + µ \n<br>",
    "Arabic": "التوزيع السابق لديريشليه",
    "Chinese": "狄利克雷先验",
    "French": "prior de Dirichlet",
    "Japanese": "ディリクレ事前分布",
    "Russian": "Априорное распределение Дирихле"
  },
  {
    "English": "Dropout",
    "context": "1: Before all CoordConv and CoordUpConv, we use 2D <mark>Dropout</mark> (Srivastava et al., 2014;Tompson et al., 2015) with a zero-out probability of 0.05. We use Batch Normalization layers (Ioffe & Szegedy, 2015) and the ReLU activation function (Nair & Hinton, 2010) after all layers except the terminal layer.<br>2: The task is to identify whether a user is a default borrower or a telecom fraudster. Baseline methods. We compare our proposed DropMessage with other existing random dropping methods, including <mark>Dropout</mark> (Hinton et al. 2012), DropEdge (Rong et al. 2019), and DropNode (Feng et al. 2020).<br>",
    "Arabic": "إسقاط",
    "Chinese": "随机失活",
    "French": "Dropout",
    "Japanese": "ドロップアウト",
    "Russian": "Dropout"
  },
  {
    "English": "Dynamic Programming",
    "context": "1: Different from other contour models [7] that also resort to the <mark>Dynamic Programming</mark> to obtain the global optimal contour, HMM offers an elegant way to integrate multiple visual cues and a probabilistic model adaptation formula (shown in Section 4) to adapt itself to the dynamic environments, which is very important for a robust tracking system.<br>2: Calculating such policies can be done via <mark>Dynamic Programming</mark> (DP) or other planning methods such as tree search. Combined with sampling, the latter corresponds to the famous Monte Carlo Tree Search (MCTS) algorithm employed in (Silver et al. 2017b;Silver et al. 2017a).<br>",
    "Arabic": "البرمجة الديناميكية",
    "Chinese": "动态规划",
    "French": "Programmation dynamique",
    "Japanese": "動的計画法",
    "Russian": "Динамическое программирование"
  },
  {
    "English": "Elastic Net",
    "context": "1: The <mark>Elastic Net</mark> was considerably slower than other methods because we tuned both of the <mark>Elastic Net</mark> parameters by CV whereas other methods tuned by CV involved tuning only one parameter. (The <mark>Elastic Net</mark> could be run faster by tuning only one parameter but at the risk of losing accuracy in some settings.)<br>2: al. , 2021 ; Yun et al. , 2019 ) . They might therefore outperform the Lasso (and the <mark>Elastic Net</mark>), possibly at the cost of some additional computation. Since these methods were primarily developed with sparse regression in mind, they may not always perform as well for dense signals.<br>",
    "Arabic": "Elastic Net",
    "Chinese": "弹性网",
    "French": "Réseau élastique",
    "Japanese": "エラスティックネット",
    "Russian": "Эластичная сеть"
  },
  {
    "English": "Electra",
    "context": "1: In table 8 and 9, we report the tuned hyperparameter values when using <mark>Electra</mark> and Bleurt (with the Linear probability model) as the evaluation model. Another hyperparameter is the number of Monte-Carlo samples L to obtain from the Dropout distribution as discussed in section 4.2. We set L = 20, i.e.<br>2: Since UCB Elimination is complementary to Uncertainty-aware selection, we apply both these algorithms together and observe the lowest annotation complexity with a reduction of 89.54% using <mark>Electra</mark> and 84.00% using Bleurt over standard RMED.<br>",
    "Arabic": "إلكترا",
    "Chinese": "伊莱克特拉",
    "French": "Electra",
    "Japanese": "Electra",
    "Russian": "Электра"
  },
  {
    "English": "Encoder-Decoder",
    "context": "1: The inputs and outputs for the CNN are stacked in the channel dimensions, such that the mapping is 60 × 6 → 60 × 10. Accordingly, global variables have been repeated along the vertical dimension. <mark>Encoder-Decoder</mark> (ED) consists of an Encoder and a Decoder with 6 fully-connected hidden layers each [39].<br>2: Transducer and Attention based <mark>Encoder-Decoder</mark> (AED) are two widely used frameworks for speech-to-text tasks. They are designed for different purposes and each has its own benefits and drawbacks for speech-totext tasks. In order to leverage strengths of both modeling methods, we propose a solution by combining Transducer and Attention based <mark>Encoder-Decoder</mark> (TAED) for speech-totext tasks.<br>",
    "Arabic": "التشفير-فك التشفير",
    "Chinese": "编码器-解码器",
    "French": "Encodeur-Décodeur",
    "Japanese": "エンコーダー・デコーダー",
    "Russian": "Энкодер-декодер (ED)"
  },
  {
    "English": "Epanechnikov kernel",
    "context": "1: f(x) = 1 nh d n X i=1 K x ; x i h : (1) \n The minimization of the average global error between the estimate and the true density yields the multivariate Epanechnikov k ernel 25, p.139] \n<br>2: The Theorem 1 generalizes the convergence shown in 6], where K was the <mark>Epanechnikov kernel</mark>, and G the uniform kernel. Its proof is given in the Appendix. Note that Theorem 1 is also valid when we associate to each d a t a p o i n t x i a positive w eight w i .<br>",
    "Arabic": "نواة إيبانيشنيكوف",
    "Chinese": "埃帕内奇尼科夫内核",
    "French": "noyau d'Epanechnikov",
    "Japanese": "エパネチニコフカーネル",
    "Russian": "ядро Епанечникова"
  },
  {
    "English": "Euclidean",
    "context": "1: To find the training points most relevant to a test point, it is common to look at its nearest neighbors in <mark>Euclidean</mark> terms in Iup,loss? Here, we plot Iup,loss against variants that are missing these terms and show that they are necessary for picking up the truly influential training points.<br>2: Likewise, the 1D <mark>Euclidean</mark> case is completely solved. For the spaces R n (n ≥ 2), however, most problems are still open.<br>",
    "Arabic": "الإقليدية",
    "Chinese": "欧几里得",
    "French": "Euclidienne",
    "Japanese": "ユークリッド距離",
    "Russian": "Евклидово"
  },
  {
    "English": "Euler step",
    "context": "1: Having obtained the flow-field for all the points on the surface, we evolve the surface using an <mark>Euler step</mark> as x t = x t−1 + V(x t−1 ). Note the absence of a delta term (∆t) in the <mark>Euler step</mark>.<br>2: For clarity, the pseudocode assumes the specific choices of σ(t) = t and s(t) = 1 that we advocate in Section 3. Note that the fallback to <mark>Euler step</mark> (line 11) can occur only when α ≥ 1.<br>",
    "Arabic": "خطوة أويلر",
    "Chinese": "欧拉步骤",
    "French": "pas d'Euler",
    "Japanese": "オイラーステップ",
    "Russian": "шаг Эйлера"
  },
  {
    "English": "Fairseq",
    "context": "1: This allows us to pre-compute the sentence-level chrF++ for each hypothesis and obtain the corpus-level score of a set of predictions by simple averaging. All experiments are implemented on top of our fork of <mark>Fairseq</mark> (Ott et al., 2019).<br>2: Models. We use <mark>Fairseq</mark> to train a Transformerbig model with the same setting in the original paper (Ott et al., 2018). The input embedding and output embeddings are shared. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate 5e-4 and an inverse sqrt decay schedule.<br>",
    "Arabic": "Fairseq",
    "Chinese": "Fairseq",
    "French": "Fairseq",
    "Japanese": "Fairseq",
    "Russian": "Fairseq"
  },
  {
    "English": "Fano's inequality",
    "context": "1: Once this is established, we can then apply a generalized version of <mark>Fano's inequality</mark> to complete the proof.<br>2: Through an application of classical information theoretic <mark>Fano's inequality</mark>, we obtain a bound on the sparsity beyond which recovery is not asymptotically reliable; a recovery scheme is called asymptotically reliable if the probability of error asymptotically goes to 0.<br>",
    "Arabic": "متراجحة فانو",
    "Chinese": "法诺不等式",
    "French": "Inégalité de Fano",
    "Japanese": "ファノの不等式",
    "Russian": "неравенство Фано"
  },
  {
    "English": "Feature Pyramid Network",
    "context": "1: <mark>Feature Pyramid Network</mark> Backbone: We adopt the <mark>Feature Pyramid Network</mark> (FPN) from [20] as the backbone network for RetinaNet.<br>2: We implement f seg as a fully convolutional FPN (<mark>Feature Pyramid Network</mark>) and train it according to the procedure detailed in Appendix D.3. Food Orientation. Although segmentation provides a means to sense global positional information about food on the plate, we also care about precisely orienting a utensil with respect to the local geometry of a food item.<br>",
    "Arabic": "شبكة هرم الميزات",
    "Chinese": "特征金字塔网络 (FPN)",
    "French": "Réseau pyramidal de caractéristiques (FPN)",
    "Japanese": "特徴ピラミッドネットワーク (FPN)",
    "Russian": "Сеть пирамид признаков"
  },
  {
    "English": "Fleiss' kappa",
    "context": "1: In 61.4% of cases, all 3 raters judged the document to be \"Definitely relevant\" or \"Likely relevant\" or all 3 raters judged the document to be \"Definitely not relevant\" or \"Likely not relevant\". The <mark>Fleiss' kappa</mark> metric on this data was found to be K=0.43.<br>2: Inter-annotator agreement among the three judges, computed using <mark>Fleiss' kappa</mark>, is 0.11. While there is no precise rule for interpreting kappa scores, Landis and Koch (1977) suggest that scores in the range (0.00, 0.20] correspond to \"slight agreement\" between annotators.<br>",
    "Arabic": "معامل كابا لفلايس",
    "Chinese": "费利斯kappa",
    "French": "kappa de Fleiss",
    "Japanese": "フリース・カッパ",
    "Russian": "коэффициент каппа Флейса"
  },
  {
    "English": "Floyd-Warshall algorithm",
    "context": "1: Shortest Path Distance can be easily calculated using the <mark>Floyd-Warshall algorithm</mark> (Floyd, 1962), which has a complexity of Θ(n 3 ). For sparse graphs typically encountered in practice (i.e.<br>",
    "Arabic": "خوارزمية فلويد-وارشال",
    "Chinese": "弗洛伊德-沃舍尔算法",
    "French": "algorithme de Floyd-Warshall",
    "Japanese": "フロイド・ワーシャル法",
    "Russian": "Алгоритм Флойда-Уоршелла"
  },
  {
    "English": "Fokker-Planck equation",
    "context": "1: Using the <mark>Fokker-Planck equation</mark>, we have that for any t ∈ [0, 1], X t ∼p t . In Rozen et al. (2021), u is replaced by a parametric version u θ and the authors optimize the loss \n<br>2: The <mark>Fokker-Planck equation</mark> is a well-known partial differential equation (PDE) that describes the probability density function of a stochastic differential equation as it changes with time. We relate the instantaneous change of variables to the special case of Fokker-Planck with zero diffusion, the Liouville equation.<br>",
    "Arabic": "معادلة فوكر-بلانك",
    "Chinese": "福克-普朗克方程",
    "French": "équation de Fokker-Planck",
    "Japanese": "フォッカー・プランク方程式",
    "Russian": "Уравнение Фоккера-Планка"
  },
  {
    "English": "Fourier Transform",
    "context": "1: //Normalization \n Kondor et al. [5] were the first to show that the data association problem could be approximately handled via the <mark>Fourier Transform</mark>. For conditioning, they exploit a modified FFT factorization which works on certain simplified observation models.<br>2: We consider the problem of recovering a function over the space of permutations (or, the symmetric group) over n elements from given partial information; the partial information we consider is related to the group theoretic <mark>Fourier Transform</mark> of the function.<br>",
    "Arabic": "تحويل فورييه",
    "Chinese": "傅里叶变换",
    "French": "Transformée de Fourier",
    "Japanese": "フーリエ変換",
    "Russian": "Преобразование Фурье"
  },
  {
    "English": "Frobenius Norm",
    "context": "1: In particular, we w i l l show that minimizing the <mark>Frobenius Norm</mark> in the new data space (e.g., via SVD) is equivalent to minimizing the Mahalanobis distance i n t h e r aw-data space. This transition is made possible by rearranging the raw feature positions in a slightly modi ed matrix form : U j V ] F 2P , namely the matrices U and V stacked horizontally ( as opposed to vertically in W = U V , w h i c h is the standard matrix form used in the traditional factorization methods ( see Section<br>2: We use A 2 or A to denote the spectral norm of A, which is the largest singular value of A, and A F for the <mark>Frobenius Norm</mark>, which is i,j a 2 i,j .<br>",
    "Arabic": "معيار فروبينيوس",
    "Chinese": "弗罗贝尼乌斯范数",
    "French": "Norme de Frobenius",
    "Japanese": "フロベニウスノルム",
    "Russian": "Норма Фробениуса"
  },
  {
    "English": "Frobenius inner product",
    "context": "1: The tangent space of SO(3) at the identity is the Lie algebra of 3×3 skew-symmetric matrices, denoted so(3). We equip SO(3) with the standard bi-invariant metric, given by the <mark>Frobenius inner product</mark> on so(3).<br>",
    "Arabic": "المنتج الداخلي الفروبينيوسي",
    "Chinese": "弗罗贝尼乌斯内积",
    "French": "produit scalaire de Frobenius",
    "Japanese": "フロベニウス内積",
    "Russian": "Фробениусово скалярное произведение"
  },
  {
    "English": "Fréchet",
    "context": "1: Interestingly, var(U ) equals nπ 2 /6 for both the uniform distribution and the distribution concentrated on a single configuration, and in our empirical investigations always var(U ) ≤ nπ 2 /6. Then the derivative at 0 is non-negative and <mark>Fréchet</mark> tricks provide tighter bounds on ln Z.<br>2: This motivates us to compare the Gumbel and Exponential tricks in more detail. ming from <mark>Fréchet</mark> (− 1 2 < α < 0), Gumbel (α = 0) and Weibull tricks (α > 0). See Section 2.3.2 for details.<br>",
    "Arabic": "فريشيه",
    "Chinese": "弗雷谢特",
    "French": "Fréchet",
    "Japanese": "フレシェ分布",
    "Russian": "Фреше"
  },
  {
    "English": "Gamma prior",
    "context": "1: We assign an uninformative <mark>Gamma prior</mark> over η, p(η|r 1 , r 2 ) = Gamma(η|r 1 , r 2 ), where r 1 = r 2 = 10 −3 . Ordinal data distribution. For an ordinal variable z ∈ {0, 1, . . .<br>2: The Poisson rates do share a group-level <mark>Gamma prior</mark>, whose parameters are s g and t g . s g shares a prior among all test groups and therefore ties all test groups together. t g is derived from posterior inferences on the training data by taking the expected posterior Poisson rate in the training data and inverting it.<br>",
    "Arabic": "توزيع جاما السابق",
    "Chinese": "伽马先验",
    "French": "loi a priori gamma",
    "Japanese": "ガンマ事前分布",
    "Russian": "Гамма приор"
  },
  {
    "English": "Gauss-Newton algorithm",
    "context": "1: The common choice of high precision is to utilize the iterative PnP solver based on the Levenberg-Marquardt (LM) algorithm -a robust variant of the Gauss-Newton (GN) algorithm, which solves the non-linear least squares by the first and approximated second order derivatives.<br>2: Substituting (30) into equation ( 31) we obtain \n min U f (U ) = ||W y − W U V * (U )|| 1 = = ||W y − φ 1 (U )|| 1 . (32) \n Unfortunately, this is not a least squares minimization problem so the <mark>Gauss-Newton algorithm</mark> is not applicable.<br>",
    "Arabic": "خوارزمية غاوس-نيوتن",
    "Chinese": "高斯-牛顿算法",
    "French": "algorithme de Gauss-Newton",
    "Japanese": "ガウス・ニュートン法",
    "Russian": "Алгоритм Гаусса-Ньютона"
  },
  {
    "English": "Gauss-Seidel method",
    "context": "1: Furthermore, the obtained algorithm (i.e., Algorithm 4) is the <mark>Gauss-Seidel method</mark> for a linear equation. Since the <mark>Gauss-Seidel method</mark> converges for a diagonally dominant operator [Golub and Van Loan 2012], Algorithm 4 converges to the diagonal correction matrix 6 .<br>2: Therefore, we can apply a numerical linear algebraic method to estimate matrix D. \n The problem for solving (2.9) lies in the complexity. To reduce the complexity, we combine an alternating method (a.k.a. the <mark>Gauss-Seidel method</mark>) with Monte Carlo simulation.<br>",
    "Arabic": "طريقة غاوس-سايدل",
    "Chinese": "高斯-赛德尔法",
    "French": "Méthode de Gauss-Seidel",
    "Japanese": "ガウス・ザイデル法",
    "Russian": "метод Гаусса-Зейделя"
  },
  {
    "English": "Generative Adversarial Networks",
    "context": "1: This paper explores how <mark>Generative Adversarial Networks</mark> (GANs) learn representations of phonological phenomena. We analyze how GANs encode contrastive and non-contrastive nasality in French and English vowels by applying the ciwGAN architecture (Beguš, 2021a).<br>",
    "Arabic": "شبكات الخصومة التوليدية",
    "Chinese": "生成对抗网络",
    "French": "Réseaux antagonistes génératifs",
    "Japanese": "敵対的生成ネットワーク",
    "Russian": "Генеративные состязательные сети (GANs)"
  },
  {
    "English": "Gensim",
    "context": "1: Regarding model efficiency, the spaCy models used to extract linguistic information are pre-trained, easy to use, and extraction of lexical and syntactic information did not take more than a couple of minutes. Further, the <mark>Gensim</mark> models used to train word vectors are also lightweight, easy-to-use, and equally efficient qua training time.<br>",
    "Arabic": "جينسيم",
    "Chinese": "Gensim",
    "French": "Gensim",
    "Japanese": "Gensim",
    "Russian": "Gensim"
  },
  {
    "English": "Gibbs Sampling",
    "context": "1: elicited using <mark>Gibbs Sampling</mark> with People (GSP) control distribution elicited using neural network with matched statistics human distribution Figure 2: Example grids from human-elicited priors (left) and machine-generated priors (right).<br>2: To do this, we used a technique called <mark>Gibbs Sampling</mark> with People [GSP; 34, see Fig. 2] that samples internal prior distributions by putting humans \"in the loop\" of a Gibbs sampler. The stimulus space consisted of the space of 4 × 4 boards giving 16 stimulus dimensions.<br>",
    "Arabic": "أخذ عينات جيبس",
    "Chinese": "吉布斯采样",
    "French": "Échantillonnage de Gibbs",
    "Japanese": "ギブス・サンプリング",
    "Russian": "Сэмплирование Гиббса"
  },
  {
    "English": "Gibbs iteration",
    "context": "1: Compared to SparseLDA, the time required for each <mark>Gibbs iteration</mark> with AliasLDA grows at a much slower rate, and the benefits of reduced sampling complexity is particularly clear when the average length of each document is small. The gap in performance is especially large for more sophisticated language modelsl such as PDP and HDP.<br>2: In the last <mark>Gibbs iteration</mark> for each sample, rather than resampling T , we compute the posterior probability over T given our current S samples, and use these distributional particles for our estimate of the probability in (1).<br>",
    "Arabic": "تكرار جيبس",
    "Chinese": "吉布斯迭代",
    "French": "itération de Gibbs",
    "Japanese": "ギブス・イテレーション",
    "Russian": "Итерация Гиббса"
  },
  {
    "English": "Gibbs sampler",
    "context": "1: In our descriptor interpretability experiments, we vary the number of descriptors (topics) for all models (K = 10, 30, 50). We train LDA and NUBBI for 100 iterations with a collapsed <mark>Gibbs sampler</mark>, and the HTMM uses the default setting of 100 EM iterations.<br>2: The inference starts with an initial segmentation of the source corpus and also its alignment to the target corpus. The <mark>Gibbs sampler</mark> considers one potential word boundary at a time. There are two hypotheses at any given boundary position of a sentence pair ( s , t ) : the merge hypothesis stands for no word boundary and the resulting source sentence s merge has a word s spanning over the sample point ; the split hypothesis indicates the resulting source sentence s split has a word boundary at the sample point separating two<br>",
    "Arabic": "مسح جيبس",
    "Chinese": "吉布斯采样器",
    "French": "Échantillonneur de Gibbs",
    "Japanese": "ギブスサンプラー",
    "Russian": "Гиббсовский сэмплер"
  },
  {
    "English": "GoogLeNet",
    "context": "1: We show the rank of 6 different neural network architectures on their BMA test accuracy on CIFAR-10 for different dataset sizes in Figure 10(b) (Appendix). We see that DenseNet121 and <mark>GoogLeNet</mark> train faster than ResNet-18 and VGG19, but rank worse with more data.<br>2: Following the implementation guideline in Subsection 4.1, we randomly selected 500 output dimensions of the respectively preceding layers in <mark>GoogLeNet</mark>⋆ for each decision function f n . In such a way, a single FC layer with #trees × #split nodes/tree output units provides all the split node inputs per dNDF x .<br>",
    "Arabic": "GoogLeNet",
    "Chinese": "GoogLeNet",
    "French": "GoogLeNet",
    "Japanese": "GoogLeNet",
    "Russian": "GoogLeNet"
  },
  {
    "English": "Gradient",
    "context": "1: L − 1 bootstrap, Algorithm 2. u ← u (K+L−1) −α∇ u (u (K+L−1) , B) \n <mark>Gradient</mark> step on objective . w ← w −β∇ w µ(ũ, u (K) (w)) BMG outer step. u ← u (K+L−1) \n Continue from most resent parameters. end while<br>",
    "Arabic": "التدرج",
    "Chinese": "梯度",
    "French": "Gradient",
    "Japanese": "勾配",
    "Russian": "Градиент"
  },
  {
    "English": "Ground Truth",
    "context": "1: Using the provided <mark>Ground Truth</mark> (assessments from the doctors), a network structure is learned that represents the correlations among segments, this process is explained in detail in section 5.2. 5.<br>2: • <mark>Ground Truth</mark>: elephant \n Figure 5: Although the word \"elephant\" is unseen to W2W, the model is still able to localize the object in the image referred to by the MASK.<br>",
    "Arabic": "الحقيقة الأساسية",
    "Chinese": "实地真实情况",
    "French": "Vérité terrain",
    "Japanese": "正解データ",
    "Russian": "Реальные данные"
  },
  {
    "English": "Gröbner basis",
    "context": "1: The state-of-the-art approach to solving polynomial systems in computer vision is based on symbolic-numeric solvers, which combine elimination (by <mark>Gröbner bases</mark> [36,39,64] or resultants [8,19,29]) with eigenvector computation [65] to find all complex solutions.<br>",
    "Arabic": "قواعد غروبنر",
    "Chinese": "格罗布纳基底",
    "French": "Base de Gröbner",
    "Japanese": "グレブナー基底",
    "Russian": "Базис Грёбнера"
  },
  {
    "English": "Gumbel",
    "context": "1: samples of a <mark>Gumbel</mark>(−c + ln Z) random variable. Figure 1 shows the analytically computed estimator variances and MSEs.<br>2: U := max x∈X φ(x) + n i=1 γ i (x i ) , \n where \n {γ i (x i ) | x i ∈ X i , 1 ≤ i ≤ n} i.i.d ∼ <mark>Gumbel</mark>(−c).<br>",
    "Arabic": "غامبل",
    "Chinese": "甘贝尔",
    "French": "Gumbel",
    "Japanese": "ガンベル",
    "Russian": "Гумбель"
  },
  {
    "English": "Gumbel distribution",
    "context": "1: From this we see that the distribution of the max of N samples drawn from a <mark>Gumbel distribution</mark> with location parameter b and scale parameter a is also a <mark>Gumbel distribution</mark> with location parameter, b max = b + a ln N and scale parameter a max = a.<br>2: Recalling that the moment generating function of a Gumbel(µ) distribution is G(t) = Γ(1 − t)e µt , we can obtain by using independence of the samples: \n E [ Ẑ ] = M m=1 E [ e Xm/M ] = Γ ( 1 − 1/M ) e ( ln Z−c ) /M M = Γ ( 1 − 1/M ) M e −c Z , E [ Ẑ 2 ] = M m=1 E [ e 2Xm/M ] = Γ ( 1 − 2/M ) e 2 ( ln Z−c )<br>",
    "Arabic": "توزيع غامبل",
    "Chinese": "甘贝尔分布",
    "French": "distribution de Gumbel",
    "Japanese": "ガンベル分布",
    "Russian": "распределение Гумбеля"
  },
  {
    "English": "Gumbel-softmax distribution",
    "context": "1: We also tried different values , 2 {10 6 , 10 5 , • • • , 10 1 } of the compression tradeoff parameter. We use temperature annealing when sampling from the <mark>Gumbel-softmax distribution</mark> ( §4).<br>",
    "Arabic": "توزيع جامبل-سوفت ماكس",
    "Chinese": "古贝尔-Softmax分布",
    "French": "Distribution Gumbel-softmax",
    "Japanese": "ガンベル-ソフトマックス分布",
    "Russian": "распределение Гумбеля-софтмакс"
  },
  {
    "English": "Haar wavelet",
    "context": "1: Other approaches include using silhouette information either in matching [8] or in classification framework [15]. Our approach belongs to the first group, and is most similar to [23] and [24], but instead of <mark>Haar wavelets</mark> or HOG features we use covariance features as human descriptors.<br>",
    "Arabic": "مويجات هار",
    "Chinese": "Haar小波",
    "French": "ondelette de Haar",
    "Japanese": "ハール・ウェーブレット",
    "Russian": "вейвлет Хаара"
  },
  {
    "English": "Hadamard matrix",
    "context": "1: The compression functions we used were generated by selecting m random rows of the 1024×1024 <mark>Hadamard matrix</mark>, for m ∈ {100, 200, 300, 400}. We also experimented with Gaussian matrices; these yielded similar but uniformly worse results.<br>",
    "Arabic": "مصفوفة هادمارد",
    "Chinese": "哈达玛矩阵",
    "French": "Matrice de Hadamard",
    "Japanese": "アダマール行列",
    "Russian": "Матрица Хадамара"
  },
  {
    "English": "Hadamard product",
    "context": "1: Next, we move towards statements of the main theorems underlying the results presented in Section 3. We begin by establishing notation which we shall use throughout. Recall that • denotes the <mark>Hadamard product</mark> between vectors. We identify functions mapping X → R with vectors in R C . We also define the group-wise utilities \n<br>2: where ⊗ is the <mark>Hadamard product</mark> between the two vectors and f is the element-wise derivative of f which in the standard case of using f = tanh can be computed using only f (x i ). The remaining derivatives can only be computed in a top-down fashion from the top node through the tree and into the leaf nodes.<br>",
    "Arabic": "ضرب هادامارد",
    "Chinese": "哈达玛乘积",
    "French": "Produit de Hadamard",
    "Japanese": "ハダマード積",
    "Russian": "Произведение Хадамара"
  },
  {
    "English": "Hankel matrix",
    "context": "1: Optimizing F Z over the convex set of <mark>Hankel matrices</mark> H leads to an algorithm equivalent to (HMC-h): \n H Z ∈ argmin H∈H F Z (H) . (HMC-H) \n We note here that our approach shares some common aspects with some previous work in matrix completion.<br>2: For comparison, a 50-dimensional RR-HMM with Parzen windows is also learned with sequences of 20 observations (Siddiqi et al., 2009); a 50-dimensional LDS is learned using Subspace ID with <mark>Hankel matrices</mark> of 20 time steps; and finally a 50-state discrete HMM and axis-aligned Gaussian observation models is learned using EM algorithm run until convergence.<br>",
    "Arabic": "مصفوفة هانكل",
    "Chinese": "汉克尔矩阵",
    "French": "matrice de Hankel",
    "Japanese": "ハンケル行列",
    "Russian": "Матрица Ханкеля"
  },
  {
    "English": "Hausdorff distance",
    "context": "1: (Note that chamfer distance is related to <mark>Hausdorff distance</mark> which has been used successfully in tracking (Huttenlocher et al., 1993); the difference is that the integral in (4) becomes a max operator in the <mark>Hausdorff distance</mark>.)<br>",
    "Arabic": "مسافة هاوسدورف",
    "Chinese": "豪斯多夫距离",
    "French": "distance de Hausdorff",
    "Japanese": "ハウスドルフ距離",
    "Russian": "Хаусдорфово расстояние"
  },
  {
    "English": "Hellinger distance",
    "context": "1: From 1 we note special cases of α-Divergences: (i) Kullback-Leiber (KL) is recovered by letting α → 1, (ii) <mark>Hellinger distance</mark> (Hellinger 1909) follows by choosing α = 0.5. For this family, α weights the influence of p q . γ-Divergences.<br>",
    "Arabic": "مسافة هيلنجر",
    "Chinese": "赫林格距离",
    "French": "distance de Hellinger",
    "Japanese": "ヘリンガー距離",
    "Russian": "Расстояние Хеллингера"
  },
  {
    "English": "Helmholtz machine",
    "context": "1: Picture descends from our earlier work on generative probabilistic graphics programming (GPGP) [31], and also incorporates insights for inference from the <mark>Helmholtz machine</mark> [17,6] and recent work on differentiable renderers [29]   We tested our approach on a held-out dataset of 2D image projections of laser-scanned faces from [36].<br>2: It contrasts with traditional bottom-up feedforward architectures (Marr, 1982) which start with edge detection, followed by segmentation/grouping, before proceeding to object recognition and other high-level vision tasks. These experiments also relate to long standing conjectures about the role of the bottom-up/top-down loops in the visual cortical areas ( Mumford , 1995 ; Ullman , 1995 ) , visual routines and pathways ( Ullman , 1984 ) , the binding of visual cues ( Treisman , 1986 ) , and neural network models such as the <mark>Helmholtz machine</mark> ( Dayan et al.<br>",
    "Arabic": "آلة هلمهولتز",
    "Chinese": "赫尔姆霍兹机",
    "French": "Machine de Helmholtz",
    "Japanese": "ヘルムホルツマシン",
    "Russian": "Машина Гельмгольца"
  },
  {
    "English": "Hiero system",
    "context": "1: In this paper, we propose a novel string-todependency algorithm for statistical machine translation. For comparison purposes, we replicated the <mark>Hiero system</mark> as described in (Chiang, 2005).<br>2: We take the replicated <mark>Hiero system</mark> as our baseline because it is the closest to our string-todependency model. They have similar rule extraction and decoding algorithms. Both systems use only one non-terminal label in rules. The major difference is in the representation of target structures.<br>",
    "Arabic": "نظام هيرو",
    "Chinese": "Hiero系统",
    "French": "système Hiero",
    "Japanese": "Hiero システム",
    "Russian": "Система Иеро"
  },
  {
    "English": "Hodge decomposition",
    "context": "1: Candogan et al. (2011) derive a <mark>Hodge decomposition</mark> for games that is closely related in spirit to our generalized Helmholtz decomposition -although the details are quite different. Candogan et al. (2011) work with classical games (probability distributions on finite strategy sets).<br>2: Unfortunately, the <mark>Hodge decomposition</mark> does not straightforwardly apply to the case when M = R n , since R n is not compact. It is thus unclear how to relate the generalized Helmholtz decomposition to the <mark>Hodge decomposition</mark>.<br>",
    "Arabic": "تحليل هودج",
    "Chinese": "霍奇分解",
    "French": "Décomposition de Hodge",
    "Japanese": "ホッジ分解",
    "Russian": "Декомпозиция Ходжа"
  },
  {
    "English": "Hoeffding's inequality",
    "context": "1: By Markov's inequality, Pr L 2 f ≥ t ≤ E[L 2 f ]/t, so Pr L f ≥ 2r ≤ 2rσ p 2 . (5) \n The union bound followed by <mark>Hoeffding's inequality</mark> applied to the anchors in the -net gives Pr \n<br>2: iid Bernoulli random variables with mean parameter 1 2 −˜ . Therefore, by <mark>Hoeffding's inequality</mark>, we have \n P 0 µ(h * ) > 1 2 − 1 2˜ ≤ exp −2(d − 1) • 1 2˜ 2 ≤ 1 16 , \n where the second inequality uses the fact that˜ = 10 max , 1 \n<br>",
    "Arabic": "\"عدم المساواة هوفدينج\"",
    "Chinese": "霍夫丁不等式",
    "French": "Inégalité de Hoeffding",
    "Japanese": "ヘフディングの不等式",
    "Russian": "Неравенство Хёффдинга"
  },
  {
    "English": "HowTo100M",
    "context": "1: In recent literature, image-text pre-training (e.g., using COCO Captions [5] or Visual Genome Captions [29]) has been applied to image-text tasks [61,44,6,58,22,36,81], and video-text pre-training (e.g., using <mark>HowTo100M</mark> [46]) to video-related tasks [59,83,15,37].<br>",
    "Arabic": "كيفية100M",
    "Chinese": "HowTo100M",
    "French": "HowTo100M",
    "Japanese": "HowTo100M",
    "Russian": "HowTo100M"
  },
  {
    "English": "Huber norm",
    "context": "1: Here the data term represents the error between estimated gradients g(p k ) and those of a reconstructed log intensity ∇I l (p k ), and the regularisation term enforces smoothness, both under a robust <mark>Huber norm</mark>.<br>",
    "Arabic": "معيار هوبر",
    "Chinese": "胡贝尔范数",
    "French": "Norme de Huber",
    "Japanese": "ヒューバー・ノルム",
    "Russian": "Норма Хьюбера"
  },
  {
    "English": "Hyper-parameter",
    "context": "1: • Write a peer review by first describing what problem or question this paper addresses, then strengths and weaknesses, for the paper + title, its main content is as below: + abstract. Generator <mark>Hyper-parameters</mark> Table 8 shows hyper-parameters we set for various generators.<br>",
    "Arabic": "معلمة فائقة",
    "Chinese": "超参数",
    "French": "Hyper-paramètre",
    "Japanese": "ハイパーパラメータ",
    "Russian": "Гиперпараметр"
  },
  {
    "English": "Inception network",
    "context": "1: (2016) for test-set attacks. We tested these adversarial training perturbations on the same <mark>Inception network</mark> on dogs vs. fish from Section 5.1, choosing this pair of animals to provide a stark contrast between the classes. We set α = 0.02 and ran the attack for 100 iterations on each test image.<br>",
    "Arabic": "شبكة البدء",
    "Chinese": "Inception 网络",
    "French": "réseau Inception",
    "Japanese": "Inception ネットワーク",
    "Russian": "Сеть Inception"
  },
  {
    "English": "Independent Cascade",
    "context": "1: In every instance of the Triggering Model, the influence function σ(•) is submodular. Beyond the <mark>Independent Cascade</mark> and Linear Threshold, there are other natural special cases of the Triggering Model. One example is the \"Only-Listen-Once\" Model.<br>2: In [10], a Triggering Model was introduced for modeling the spread of influence in a social network. As the authors show, this model generalizes the <mark>Independent Cascade</mark>, Linear Threshold and Listen-once models commonly used for modeling the spread of influence.<br>",
    "Arabic": "السلسلة المستقلة",
    "Chinese": "独立级联模型",
    "French": "Cascade indépendante",
    "Japanese": "独立カスケード",
    "Russian": "Независимый Каскад"
  },
  {
    "English": "Inside-outside algorithm",
    "context": "1: There are algorithms like the <mark>Inside-Outside algorithm</mark> (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum.<br>",
    "Arabic": "خوارزمية داخل-خارج",
    "Chinese": "内外算法",
    "French": "Algorithme Inside-Outside",
    "Japanese": "インサイド・アウトサイドアルゴリズム",
    "Russian": "Алгоритм Inside-Outside"
  },
  {
    "English": "Ising model",
    "context": "1: Specifically, we want to check whether increasing the expected delay parameter τ * actually increases the mixing time as predicted by Equation 2. 2) mixing time as τ increases for a synthetic <mark>Ising model</mark> graph (n = 1000, ∆ = 3).<br>2: This sampling procedure is equivalent to the one that we use in the experiment, and it will produce a chain that is consistent with the <mark>Ising model</mark>'s dynamics. If we consider the evolution of two coupled chains X ( t ) and Y ( t ) using the same values ofĨ t andR t , then from the way that we constructed the coupling , it follows that if Y ( 0 ) X ( 0 ) , then for any future time t , Y ( t ) X ( t<br>",
    "Arabic": "نموذج إيسينج",
    "Chinese": "伊辛模型",
    "French": "modèle d'Ising",
    "Japanese": "イジングモデル",
    "Russian": "модель Изинга"
  },
  {
    "English": "Iverson bracket",
    "context": "1: τ (t) = t − t 3: return [τ (s + r) < τ (s)] (<mark>Iverson bracket</mark>) \n the algorithm and additional empirical results for a wide suite of detectors in Appendix B.1. Note that Alg. 1 is by construction task agnostic (not specific to object detection).<br>",
    "Arabic": "قوس إيفرسون",
    "Chinese": "伊弗森括弧",
    "French": "crochets d'Iverson",
    "Japanese": "アイバーソン括弧",
    "Russian": "Скобка Айверсона"
  },
  {
    "English": "Jaccard",
    "context": "1: Consider the reference segmentation r and candidate segmentations h 1 and h 2 in Figure 9: h 1 and h 2 are equidistant to r under A with <mark>Jaccard</mark> (0.58), B, and WindowDiff. However , for a task like topic segmentation , h 2 may be preferred , as it contains `` meta '' topics that consistently match two topics each in r , whereas h 1 contains two correct topics , but one really bad third topic , which is a mixture of four topics in r. Conversely , for a task like sentence segmentation ,<br>2: Here , B and WD behave erroneously if a pair of segmentations h 1 and h 2 are judged equally similar to r , where h 1 `` soft '' transposes a boundary by x units , and h 2 `` hard '' transposes any boundary by x units , i.e. , the segments on either side of the transposition in h 2 have <mark>Jaccard</mark> < 0.5 with their corresponding original segments in r. The trend in Figure 14 can be explained as follows : if the ratio between the number of segments m and the sequence length n is too low , the segments are so large that it is rare to find a pair of segments such that transposing their boundary results in a `` hard<br>",
    "Arabic": "جاكارد",
    "Chinese": "Jaccard",
    "French": "Jaccard",
    "Japanese": "ジャカード",
    "Russian": "Жаккар"
  },
  {
    "English": "Jaccard index",
    "context": "1: We select 1000 test data for each task, which are the most similar to its counterfactual based on the <mark>Jaccard index</mark>. Evaluation setup. Given a test input x, we denote its counterfactual example as CF (x). We consider the following settings: \n • Zero-shot: Zero-shot evaluation without the demonstration.<br>2: Further, the Jaccard version of A can be easily modified to distinguish between \"soft\" and \"hard\" mistakes by penalizing edges with weights under some threshold t. The <mark>Jaccard index</mark>, J ∈ [0, 1], between two sets S and T is defined as: \n<br>",
    "Arabic": "مؤشر جاكارد",
    "Chinese": "杰卡德指数",
    "French": "indice de Jaccard",
    "Japanese": "ジャッカード指数",
    "Russian": "Индекс Жаккара"
  },
  {
    "English": "Jensen's inequality",
    "context": "1: The lower bound follows from <mark>Jensen's inequality</mark>. The proof of the upper bound (appendix B) bounds this KL divergence above by t/(2σ 2 n ).<br>2: where we have used that h − h ⋆ ∈ 2B H and that the set B H is convex to obtain the second inequality. We now upper bound the final display using the classical Sudakov-  The last term in the expression has bound √ nr by <mark>Jensen's inequality</mark> and the relaxation that c ∈ [−1, 1].<br>",
    "Arabic": "عدم المساواة لجنسن",
    "Chinese": "詹森不等式",
    "French": "Inégalité de Jensen",
    "Japanese": "ジェンセンの不等式",
    "Russian": "Неравенство Йенсена"
  },
  {
    "English": "Jensen-Shannon",
    "context": "1: (2023) introduced f -DPG, which generalizes DPG to minimizing any f -divergence for approximating the target distribution. The family of f -divergences includes forward KL divergence, <mark>Jensen-Shannon</mark>, total variation distance (TVD), reverse KL, among others.<br>2: (2023) show that this target distribution could not only be approximated through the reverse KL divergence but also any other fdivergence, including forward KL and <mark>Jensen-Shannon</mark>, leading to different trade-offs in terms of expected reward and diversity.<br>",
    "Arabic": "جنسن-شانون",
    "Chinese": "詹森-香农",
    "French": "Jensen-Shannon",
    "Japanese": "ジェンセン・シャノン",
    "Russian": "Дженсен-Шеннон"
  },
  {
    "English": "Jensen-Shannon Divergence",
    "context": "1: <mark>Jensen-Shannon Divergence</mark> (JSD) Another way to identify words that differentiate sets of text is based on the <mark>Jensen-Shannon Divergence</mark> (JSD) (Trujillo et al., 2021).<br>2: Coherently with the analysis performed in previous sections, we chose the <mark>Jensen-Shannon Divergence</mark> metric, and analyzed the distribution of events about Transnational Women against Transnational Men, Western Men, and Western Women.<br>",
    "Arabic": "انحراف جينسن-شانون",
    "Chinese": "詹森-香农散度",
    "French": "Divergence de Jensen-Shannon",
    "Japanese": "ジェンセン-シャノン発散度",
    "Russian": "Дивергенция Дженсена-Шеннона"
  },
  {
    "English": "Kalman filter",
    "context": "1: As explained in the main text, <mark>Kalman filter</mark> needs to be asynchronous and time-varying for streaming perception. Let ∆t k be the time-varying intervals Table I. Empirical performance comparison before and after using Alg. 1. We see that our shrinking-tail policy consistently boosts the streaming performance for different detectors and for different input scales.<br>2: Finally, the edge association variable θ k \"snaps\" the GPS reading to a street in the map. This step is crucial for the <mark>Kalman filter</mark> update described below. To sample θ (i) k , we first determine the distance between the measurement, z k , and the different streets in the vicinity.<br>",
    "Arabic": "مرشح كالمان",
    "Chinese": "卡尔曼滤波器",
    "French": "filtre de Kalman",
    "Japanese": "カルマンフィルター",
    "Russian": "фильтр Кальмана"
  },
  {
    "English": "Kendall's τ",
    "context": "1: Furthermore, to provide some assessment of the quality of the predicted orderings themselves, we follow Lapata (2003) in employing Kendall's § , which is a measure of how much an ordering differs from the OSOthe underlying assumption is that most reasonable sentence orderings should be fairly similar to it. Specifically, for a permutation¨of the sentences in an \n<br>2: We also show that scores from a partial annotation of fine-grained units highly correlates with scores from a full annotation workload (0.89 <mark>Kendall's τ</mark> using 50% judgments). We release our human judgments, annotation templates, and our software for future research.<br>",
    "Arabic": "- التاو كندال",
    "Chinese": "肯德尔秩相关系数τ",
    "French": "Le τ de Kendall",
    "Japanese": "ケンダルのτ",
    "Russian": "коэффициент Кендалла τ"
  },
  {
    "English": "Keras",
    "context": "1: (iv) We also validate our tree ensemble methods with flexible loss functions (zero-inflated Poisson and negative binomial regression) on a large-scale multi-task proprietary dataset. Model Implementation Differentiable tree ensembles are implemented in Tensor-Flow 2.0 using <mark>Keras</mark> interface.<br>",
    "Arabic": "كيراس",
    "Chinese": "Keras",
    "French": "Keras",
    "Japanese": "Keras",
    "Russian": "Keras"
  },
  {
    "English": "Kleene closure",
    "context": "1: where • is string concatenation and V * is the <mark>Kleene closure</mark> of V. In words, valid hypotheses are text, e.g., sentences or phrases, padded with distinguished tokens, BOS and EOS. In this work, we consider models that are locally normalized, i.e., the model p θ is defined as the product of probability distributions: \n<br>",
    "Arabic": "إغلاق كلين",
    "Chinese": "Kleene闭包",
    "French": "fermeture de Kleene",
    "Japanese": "クリーン閉包",
    "Russian": "Замыкание Клини"
  },
  {
    "English": "Kneser-Ney smooth",
    "context": "1: For lack of space, we do not go into the details of this approximation, but our approximation becomes the exact <mark>Kneser-Ney smoothing</mark> when the counts are integers. In order to test our <mark>Kneser-Ney smoothing</mark> implementation, we built a trigram language model and compared the performance with that from the SRILM.<br>2: This is particularly not appropriate for the prepositional phrase because the preposition is always the head word of the phrase in the UPenn Treebank annotation. Therefore, we also added the opposite child of the first exposed previous head into the context for predicting. Both <mark>Kneser-Ney smoothing</mark> and the neural network model were studied when the context was gradually increased.<br>",
    "Arabic": "التنعيم الكنيزير-ناي",
    "Chinese": "Kneser-Ney平滑",
    "French": "lissage de Kneser-Ney",
    "Japanese": "クネッサーニー平滑化",
    "Russian": "Сглаживание Кнесера-Нея"
  },
  {
    "English": "Kolmogorov-Smirnov test",
    "context": "1: 2011 ) . We verify the statistical significance of our findings with the <mark>Kolmogorov-Smirnov test</mark> (Berger and Zhou, 2014), which shows if the two sets of samples are likely to come from the same distribution. Our results in Figure 3 show a significant distribution shift when assessing semantics-preserving surface-form variations.<br>2: [45] recently proposed using these higher-order distances (integer σ d > 1) in a fast two-sample test that generalizes the well-known <mark>Kolmogorov-Smirnov test</mark>, improving sensitivity to the tails of distributions; our results may provide a step towards understanding theoretical properties of this test.<br>",
    "Arabic": "اختبار كولموجوروف-سميرنوف",
    "Chinese": "科尔莫哥洛夫-斯米尔诺夫检验",
    "French": "test de Kolmogorov-Smirnov",
    "Japanese": "コルモゴロフ・スミルノフ検定",
    "Russian": "Тест Колмогорова-Смирнова"
  },
  {
    "English": "Krippendorff's α",
    "context": "1: Interestingly, for toxicity detection and sentiment classification, the models benefit from sociodemographic prompting, whereas for stance detection they perform better without such information. We observe a slight trend that datasets for which improvements are observed share low IAA across the original annotations (see <mark>Krippendorff's α</mark> in Table 2).<br>2: We rank the correlations to reveal which demographic groups best align with the positionality of datasets and models. Finally, we report the total number of annotators and interannotator agreements for each demographic using <mark>Krippendorff's α</mark> (Krippendorff, 2006).<br>",
    "Arabic": "ألفا كريبندورف",
    "Chinese": "克里朋多夫阿尔法系数",
    "French": "α de Krippendorff",
    "Japanese": "クリッペンドルフのα",
    "Russian": "Коэффициент α Криппендорфа"
  },
  {
    "English": "Kronecker delta",
    "context": "1: where δ i,j is the <mark>Kronecker delta</mark>, 2 τ = � 2z /c∆� is the discretized round-trip time delay, z is the distance of the scene point from the camera, and c is the speed of light.<br>2: (Here and elsewhere, δ i j is the <mark>Kronecker delta</mark>: 1 if i = j and 0 otherwise.) Therefore, T * m (x) = w(x)T m (x) also forms the dual Chebyshev basis. Using (7), we can expand our DOS µ(λ) as \n<br>",
    "Arabic": "دلتا كرونيكر",
    "Chinese": "克罗内克δ",
    "French": "delta de Kronecker",
    "Japanese": "クロネッカー・デルタ",
    "Russian": "Дельта Кронекера"
  },
  {
    "English": "Kronecker product",
    "context": "1: ∂L ( Θ ) ∂ vec ( Θ ) \n where ⊗ denotes the <mark>Kronecker product</mark>, I has dimension P × P , 1 has dimension 1 × T , and thus I ⊗ 1 has dimension P × P T .<br>2: g (t, τ ) Shorthand for ∂Lt(Θ) ∂θτ , used in variance expressions ⊗ <mark>Kronecker product</mark> α \n The learning rate for the parameters θ unroll(s, θ, K) \n A function that unrolls the system for K steps starting with state s, using parameters θ.<br>",
    "Arabic": "ضرب كرونيكر",
    "Chinese": "克罗内克积",
    "French": "produit de Kronecker",
    "Japanese": "クロネッカー積",
    "Russian": "произведение Кронекера"
  },
  {
    "English": "Kullback Leibler divergence",
    "context": "1: ( 19) is equivalent to minimizing the histogram dis-tance between the observed and expected histograms h,h k . This is because the <mark>Kullback Leibler divergence</mark> is equal to the negative log likelihood, plus a constant that does not depend on k (the negative entropy): \n<br>",
    "Arabic": "اختلاف كولباك - لايبلر",
    "Chinese": "库尔贝克·莱布勒散度",
    "French": "divergence de Kullback-Leibler",
    "Japanese": "クルバック・ライブラー・ダイバージェンス",
    "Russian": "Дивергенция Кульбака-Лейблера"
  },
  {
    "English": "Lanczos iteration",
    "context": "1: performing atruncated SVD of K ff ) can be done in O(N 2 M ) using, for example, <mark>Lanczos iteration</mark> [Lanczos, 1950].<br>",
    "Arabic": "تكرار لانكزوس",
    "Chinese": "兰佐斯迭代",
    "French": "itération de Lanczos",
    "Japanese": "ランチョス反復法",
    "Russian": "Итерация Ланцоша"
  },
  {
    "English": "Langevin dynamic",
    "context": "1: Finally we demonstrate RSGM on a non-compact manifold: the two dimensional hyperbolic space H 2 , which is defined as the simply connected space of constant negative curvature. We use <mark>Langevin dynamics</mark> as the noising process (Eq. (3)) and target a wrapped Gaussian as the invariant distribution.<br>2: We also note that the study of high-dimensional regimes of gradient descent and <mark>Langevin dynamics</mark> have a history from the statistical physics perspective, e.g., in [21,22,67,48,17,45]. We develop a unified approach to the scaling limits of SGD in high-dimensions with constant learning rate that allows us to understand a broad range of estimation tasks.<br>",
    "Arabic": "ديناميات لانجفين",
    "Chinese": "朗之万动力学",
    "French": "dynamique de Langevin",
    "Japanese": "ランジュバン動力学",
    "Russian": "Динамика Ланжевена"
  },
  {
    "English": "Laplace smoothing",
    "context": "1: D si→t (I) > D sj →t (I)). We clip this intermediate pairwise matrix W t to be in [0.001, 0.999] as a form of <mark>Laplace smoothing</mark>.<br>2: Note that P M I(w, l|C) is undefined if p(w, l|C) = 0. One simple strategy is to ignore such w in the summation. A more reasonable way is to smooth p(w, l|C) with methods like <mark>Laplace smoothing</mark>.<br>",
    "Arabic": "تملُّس لابلاس",
    "Chinese": "拉普拉斯平滑",
    "French": "lissage de Laplace",
    "Japanese": "ラプラス平滑化",
    "Russian": "сглаживание Лапласа"
  },
  {
    "English": "Laplace-Beltrami operator",
    "context": "1: , where S : M ! M is some symmetry of M . A second example is the <mark>Laplace-Beltrami operator</mark> and derived operators (e.g. the heat operator), which are preserved under isometries. The operators on M and N can be quite general, however, and can represent any association of functions on the manifold.<br>2: In geometry and graph theory, Cheeger's inequality relates the second-smallest eigenvalue of a Laplacian or <mark>Laplace-Beltrami operator</mark> to the size of the smallest bisecting cut [7,37]; in the graph setting, the associated eigenvector (the Fiedler vector) is the basis for spectral algorithms for graph partitioning [42].<br>",
    "Arabic": "المشغل لابلاس-بيلترامي",
    "Chinese": "拉普拉斯-贝尔特拉米算子",
    "French": "opérateur de Laplace-Beltrami",
    "Japanese": "ラプラス・ベルトラミ作用素",
    "Russian": "оператор Лапласа-Бельтрами"
  },
  {
    "English": "Lasso",
    "context": "1: The predictive importance of the dimensions of r(x) can be computed with a <mark>Lasso</mark> or a Random Forest classifier. Disentanglement is the average of the difference from one of the entropy of the probability that a dimension of the learned representation is useful for predicting a factor weighted by the relative importance of each dimension.<br>2: • The <mark>Lasso</mark> (Tibshirani, 1996) is one of the most widely used PLR methods. Computing the <mark>Lasso</mark> estimator is a convex optimization problem. One of the well-studied issues is that <mark>Lasso</mark> estimates can suffer from bias by overshrinking the strongest signals (e.g., Su et al., 2017;Javanmard and Montanari, 2018).<br>",
    "Arabic": "لاسو",
    "Chinese": "套索法",
    "French": "Lasso",
    "Japanese": "ラッソ",
    "Russian": "Лассо"
  },
  {
    "English": "Lasso penalty",
    "context": "1: Recently, weak hierarchical Lasso, a sparse interaction regression model, is proposed that produces sparse and hierarchical structured estimator by exploiting the <mark>Lasso penalty</mark> and a set of hierarchical constraints.<br>2: By imposing restrictions of the weak hierarchy and taking advantage of the <mark>Lasso penalty</mark> [27] that leads to sparse coefficients, the weak hierarchical Lasso is able to simultaneously attain a hierarchical solution and identify important main effects and interactions.<br>",
    "Arabic": "عقوبة لاسو",
    "Chinese": "套索惩罚",
    "French": "Pénalité Lasso",
    "Japanese": "ラッソペナルティ",
    "Russian": "Штраф Лассо"
  },
  {
    "English": "Lemma",
    "context": "1: Using <mark>Lemma</mark> 14, we can find a sequence q such that H b all = H σ,λ q,E , where H b all consists of all binary classifiers.<br>2: In the rest of this section, we analyze the approximation ratio of Algorithm 3. <mark>Lemma</mark> 5.1. Let r = B − cost(S a ), which is the remaining budget. In Algorithm 3, at the ith iteration of search loop, after o i has been added into S * i−1 , the following holds: \n<br>",
    "Arabic": "لمَّة",
    "Chinese": "引理",
    "French": "Lemme",
    "Japanese": "補題",
    "Russian": "Лемма"
  },
  {
    "English": "Levenberg-Marquardt algorithm",
    "context": "1: Improvement: This approximation however degrades the correctness of the approximate Hessian matrix that the <mark>Levenberg-Marquardt algorithm</mark> [45] relies on for fast convergence. We found that also optimizing the squared spatial derivatives of this cost significantly improves the convergence.<br>2: The convergence can thus be adjusted depending on the detector and on the image resolution. We show in Section 5.4 that other dense features work well too. Optimization: The optimization problems of both keypoint and bundle adjustments are solved with the Levenberg-Marquardt [45] algorithm implemented using Ceres [3].<br>",
    "Arabic": "خوارزمية ليفنبرغ-ماركوارت",
    "Chinese": "勒文伯格-马夸特算法",
    "French": "algorithme de Levenberg-Marquardt",
    "Japanese": "レーベンバーグ・マーカート法",
    "Russian": "алгоритм Левенберга-Марквардта"
  },
  {
    "English": "Levenshtein distance",
    "context": "1: We then filter all sentences which are shorter than 3 tokens or longer than 30 tokens. As the last step, we discard sentence pairs sharing substantial lexical overlap, which prevents degenerate alignments of, e.g., proper names. We remove all sentence pairs for which the <mark>Levenshtein distance</mark> detects an overlap of over 50%.<br>2: In the Basic approach, we simply use normalized orthographic similarity (computed using <mark>Levenshtein distance</mark>) between the candidate and the original masked word. This reranker considers all character substitutions equally costly.<br>",
    "Arabic": "مسافة ليفنشتاين",
    "Chinese": "莱文斯坦距离",
    "French": "distance de Levenshtein",
    "Japanese": "レーベンシュタイン距離",
    "Russian": "Расстояние Левенштейна"
  },
  {
    "English": "Levenshtein edit distance",
    "context": "1: Relatedly, Levenshtein Transformers (Gu et al., 2019) learn to edit a sequence by imitating an expert policy based on the <mark>Levenshtein edit distance</mark>.<br>2: For our purposes we use the Levenshtein string edit distance metric (Levenshtein, 1966). The <mark>Levenshtein edit distance</mark> between two strings is the minimum number of edits needed to transform one string into the other, where an edit is defined as the insertion, deletion, or substitution of a single character.<br>",
    "Arabic": "مسافة تحرير ليفنشتاين",
    "Chinese": "莱文斯坦编辑距离",
    "French": "distance d'édition de Levenshtein",
    "Japanese": "レーベンシュタイン編集距離",
    "Russian": "Расстояние Левенштейна"
  },
  {
    "English": "Libratus",
    "context": "1: Static AIs like Slumbot suffer from the off-tree action problem, i.e., an action taken by an opponent that is not in the abstraction. A more principled approach is to solve subgames that immediately follow that off-tree action online. DeepStack and <mark>Libratus</mark> are representative online AIs based on this idea.<br>2: Although many milestone events (e.g., DeepStack, <mark>Libratus</mark>, ReBeL, etc.) cent years, almost all of these AIs are not publicly available, making the comparison between different AIs extremely difficult.<br>",
    "Arabic": "ليبراتوس",
    "Chinese": "天秤座",
    "French": "Balance",
    "Japanese": "リブラタス",
    "Russian": "Либратус"
  },
  {
    "English": "Linformer",
    "context": "1: Reformer (Kitaev, Kaiser, and Levskaya 2019) also achieves O(L log L) with locallysensitive hashing self-attention, but it only works on extremely long sequences. More recently, <mark>Linformer</mark> (Wang et al.<br>2: (2021a), likely also slower than the other efficient Transformers that linearize attention, namely Local Attention (Parmar et al., 2018), <mark>Linformer</mark>  and Linear Transformer (Katharopoulos et al., 2020). However, it is worth noting that Table 4a suggests that FNet is more accurate than all of the aforementioned models.<br>",
    "Arabic": "لينفورمر",
    "Chinese": "Linformer",
    "French": "Linformer",
    "Japanese": "リンフォーマー",
    "Russian": "Линформер"
  },
  {
    "English": "Lipschitz",
    "context": "1: Even if the notion of stability required here is slightly different, any error bound on the estimator can be naturally converted into a stability bound for conformal prediction sets. So we don't lose much generality as long as we make the assumption that the score function is sufficiently regular e.g., <mark>Lipschitz</mark>.<br>2: f (ςw 0 + (1 − ς)w) ≤ ςf (w 0 ) + (1 − ς)f (w) − λ 2 ς(1 − ς) w 0 − w 2 . Proposition 3.7. Assume that for any z, F z is λ-strongly convex and ρ-<mark>Lipschitz</mark>. It holds \n<br>",
    "Arabic": "ليبشيتز",
    "Chinese": "利普希茨",
    "French": "Lipschitz",
    "Japanese": "リプシッツ条件",
    "Russian": "Липшиц"
  },
  {
    "English": "Lipschitz constant",
    "context": "1: Let M be a compact subset of R d with diameter diam(M). Let α = E[1/δ] and let L k denote the <mark>Lipschitz constant</mark> of k with respect to the L 1 norm. With z as above, we have Pr sup \n<br>2: After T iterations of Algorithm 1 with step size η X = η Y = η √ T < 1 L , for some positive constant η < 1 L , where L is the <mark>Lipschitz constant</mark> of<br>",
    "Arabic": "ثابت ليبشيتز",
    "Chinese": "利普希茨常数",
    "French": "constante de Lipschitz",
    "Japanese": "リプシッツ定数",
    "Russian": "Константа Липшица"
  },
  {
    "English": "Lipschitz continuity",
    "context": "1: In this work, we consider the distributed optimization of non-smooth convex functions using a network of computing units. We investigate this problem under two regularity assumptions: (1) the <mark>Lipschitz continuity</mark> of the global objective function, and (2) the <mark>Lipschitz continuity</mark> of local individual functions.<br>2: Under Assumption 2.1 and Assumption 2.2, the ZoBG is an unbiased estimator of the stochastic objective. E[∇ [0] F (θ)] = ∇F (θ). In contrast, the FoBG requires strong continuity conditions in order to satisfy the requirement for unbiasedness. However, under <mark>Lipschitz continuity</mark>, it is indeed unbiased.<br>",
    "Arabic": "استمرارية ليبشيتز",
    "Chinese": "利普希茨连续性",
    "French": "Continuité de Lipschitz",
    "Japanese": "リプシッツ連続性",
    "Russian": "Непрерывность по Липшицу"
  },
  {
    "English": "Lipschitz continuous",
    "context": "1: Let dz dt = f (z(t), t) be a differential equation describing a continuous-in-time transformation of z(t). Assuming that f is uniformly <mark>Lipschitz continuous</mark> in z and continuous in t, then the change in log probability also follows a differential equation, \n<br>2: On the other hand, since we are using full gradient descent and the objective function is O(1)-Lipscthiz continuous, it means the objective value is monotonically non-increasing. In other words, we have \n<br>",
    "Arabic": "مستمر ليبشتز",
    "Chinese": "利普希茨连续",
    "French": "Lipschitz continue",
    "Japanese": "リプシッツ連続",
    "Russian": "Липшиц непрерывный"
  },
  {
    "English": "Lipschitz function",
    "context": "1: For the distributions µ we have in mind, for instance uniform on the unit sphere, there exists with high probability some O(1)-<mark>Lipschitz function</mark> f : R d → R satisfying f (xi) = yi for all i.<br>",
    "Arabic": "دالة ليبشيتز",
    "Chinese": "利普希茨函数",
    "French": "fonction Lipschitzienne",
    "Japanese": "リプシッツ関数",
    "Russian": "Липшицева функция"
  },
  {
    "English": "Lipschitzness",
    "context": "1: Here we take the perspective that a small robust generalization error should imply a small training error and a small Lipschitz constant. Another important mismatch is that we stated our universal law of robustness for <mark>Lipschitzness</mark> in ℓ2, while the experiments in [MMS + 18] are for robustness in ℓ∞.<br>",
    "Arabic": "تحديد ليبشيتز",
    "Chinese": "利普希茨性质",
    "French": "Lipschitzianité",
    "Japanese": "リプシッツ性",
    "Russian": "Липшицевость"
  },
  {
    "English": "Log Gaussian cox process",
    "context": "1: This situation arises most commonly in undirected graphical models, such as Ising models (where Z is called the partition function), but it is also found in density models and doubly-stochastic processes, e.g. the <mark>Log Gaussian Cox Process</mark>. In such cases, even MCMC -the \"sledgehammer\" of Bayesian inference -is difficult or impossible to apply.<br>2: We created three one-dimensional data sets using the following intensity functions: \n 1. A sum of an exponential and a Gaussian bump:  (Diggle, 1985) and with the <mark>Log Gaussian Cox Process</mark> (Møller et al., 1998  We compared the SGCP to the classical kernel smoothing (KS) approach of Diggle (1985).<br>",
    "Arabic": "عملية كوكس لوغاريتمية غاوسية",
    "Chinese": "对数高斯-Cox过程",
    "French": "Processus de Cox log-gaussien",
    "Japanese": "ログガウスコックス過程",
    "Russian": "Лог-нормальный процесс Кокса"
  },
  {
    "English": "Longformer",
    "context": "1: For example, in models such as <mark>Longformer</mark> (Beltagy et al., 2020), ETC , and Big-Bird (Zaheer et al., 2020), attention is O(N ) as a function of the input length, but quadratic in the number of \"global tokens\"; the latter must be sufficiently large to ensure good performance. The Long-Range Arena benchmark ( Tay et al. , 2021a ) attempts to compare many of the efficient Transformers in a series of tasks requiring long range dependencies , finding that the Performer ( Choromanski et al. , 2021 ) , Linear Transformer ( Katharopoulos et al. , 2020 ) , Linformer , and Image Transformer ( Local Attention ) ( Parmar et al.<br>2: We make the implicit window effect explicit as shown by Eq. (3), which is also adopted by <mark>Longformer</mark> (Beltagy et al., 2020).<br>",
    "Arabic": "\"لونغفورمر\"",
    "Chinese": "长序列转换器",
    "French": "Longformer",
    "Japanese": "ロングフォーマー",
    "Russian": "Лонгформер"
  },
  {
    "English": "Lucene",
    "context": "1: Later, Tedla and Yamamoto (2018) employed a manually constructed dataset to train a Long Short-Term Memory (LSTM) model for morphological segmentation in Tigrinya. Osman and Mikami (2012) proposed a rule-based stemmer for a <mark>Lucene</mark> based Tigrinya information retrieval.<br>2: Even in this set, we find that about 60% of the posts have more than two valid questions. These numbers suggests that the candidate set of questions retrieved using <mark>Lucene</mark> ( §2.1) very often contains useful clarification questions.<br>",
    "Arabic": "لوسين",
    "Chinese": "Lucene",
    "French": "Lucene",
    "Japanese": "Lucene",
    "Russian": "Lucene"
  },
  {
    "English": "Lyapunov function",
    "context": "1: D.4 With Pure Multiplicative Noise: Proof of Theorem 4 \n The proof of this theorem mimics the proof of Theorem 2, with a slightly different <mark>Lyapunov function</mark>. We recall that in Section 5, the function f is of the form: \n<br>2: Instead, our analysis requires a bounded statistical condition numberκ, and performs a shift in terms of dependency over H: \n x − x * 2 H becomes x − x * 2 , and z t − x * 2 becomes z t − x * 2 H −1 . The new <mark>Lyapunov function</mark> writes: \n<br>",
    "Arabic": "دالة ليابونوف",
    "Chinese": "李雅普诺夫函数",
    "French": "fonction de Lyapunov",
    "Japanese": "リャプノフ関数",
    "Russian": "Функция Ляпунова"
  },
  {
    "English": "Mahalanobis distance",
    "context": "1: (3), the rank of j ] i s a l s o a t m o s t 2 R. \n The problem of minimizing the <mark>Mahalanobis distance</mark> of Eq.<br>2: 2) The distance (3.2) provides a well-founded measure of \"closeness\" between two <mark>Mahalanobis distance</mark> functions and forms the basis of our problem given below. Given pairs of similar points S and pairs of dissimilar points D, our distance metric learning problem is \n<br>",
    "Arabic": "مسافة ماهالانوبيس",
    "Chinese": "马哈拉诺比斯距离",
    "French": "Distance de Mahalanobis",
    "Japanese": "マハラノビス距離",
    "Russian": "Махаланобисовское расстояние"
  },
  {
    "English": "Mahalanobis distance function",
    "context": "1: Thus, we regularize the Mahalanobis matrix A to be as close as possible to a given <mark>Mahalanobis distance function</mark>, parameterized by A 0 . We now quantify the measure of \"closeness\" between A and A 0 via a natural information-theoretic approach.<br>",
    "Arabic": "دالة مسافة ماهالانوبيس",
    "Chinese": "马哈拉诺比斯距离函数",
    "French": "fonction de distance de Mahalanobis",
    "Japanese": "マハラノビス距離関数",
    "Russian": "Функция расстояния Махаланобиса"
  },
  {
    "English": "Mahalanobis matrix",
    "context": "1: However, one may still implicitly update the <mark>Mahalanobis matrix</mark> A via updates in kernel space for an equivalent kernel learning problem in which K = X T AX for X = [x 1 , . . . , x n ]. If K 0 is an input kernel matrix for the data, the appropriate update is: \n<br>2: Algorithm 1 Information-theoretic metric learning Input: X: input d × n matrix, S: set of similar pairs D: set of dissimilar pairs, u, ℓ: distance thresholds A0: input <mark>Mahalanobis matrix</mark>, γ: slack parameter, c: constraint index function Output: A: output <mark>Mahalanobis matrix</mark> \n 1.<br>",
    "Arabic": "مصفوفة ماهالانوبيس",
    "Chinese": "马哈拉诺比斯矩阵",
    "French": "matrice de Mahalanobis",
    "Japanese": "マハラノビス行列",
    "Russian": "Матрица Махаланобиса"
  },
  {
    "English": "Mahalanobis metric",
    "context": "1: Recent work has yielded various approaches to metric learning, including several techniques to learn a combination of existing kernels [19,29], as well as methods to learn a <mark>Mahalanobis metric</mark> [30,3,8], and methods to learn example-specific local distance functions [10].<br>2: which sustains the LSH requirement of ( 5) for a learned <mark>Mahalanobis metric</mark>, whether A is computed using the method of [8] or otherwise [30,3].<br>",
    "Arabic": "مقياس ماهالانوبيس",
    "Chinese": "马哈拉诺比斯度量",
    "French": "métrique de Mahalanobis",
    "Japanese": "マハラノビス距離尺度",
    "Russian": "Махаланобисовская метрика"
  },
  {
    "English": "Manhattan distance",
    "context": "1: The number of channels in the aforementioned route can be computed by the function CH(i, j) as described in (5). This also called the <mark>Manhattan distance</mark> between tiles i and j. \n CH(i, j) = | i/N − j/N | + | i\\N − j\\N | (5) \n<br>2: An illustrative example is given in Figure 2. Retrieval for SDTG and SDTGAU can be realised using some similarity measure over trees. We explored several measures and it showed, that <mark>Manhattan distance</mark> is the most robust and reliable measure: \n d(I, P ) = n i=1 |f P i − f Ii | \n with \n<br>",
    "Arabic": "المسافة المانهاتنية",
    "Chinese": "曼哈顿距离",
    "French": "distance de Manhattan",
    "Japanese": "マンハッタン距離",
    "Russian": "Манхэттенское расстояние"
  },
  {
    "English": "Marching Cubes",
    "context": "1: Using <mark>Marching Cubes</mark> (MC) [33], a Lagrangian surface ∂Ω L is extracted from an Eulerian representation ∂Ω E encoded in the network parameters j. An energy function E is defined on ∂Ω L which is minimized using gradient-descent.<br>2: Applying a flow field to such functions is non-trivial as updates are required in the parameter (j) space as opposed to directly updating ϕ to ϕ ′ . We propose a parametric level-set evolution method ( § 4) which propagates neural implicit surfaces according to an explicitly generated flow field. Our method comprises of three repeating steps , 1 ) A non-differentiable surface extraction method like <mark>Marching Cubes</mark> [ 33 ] or Sphere Tracing [ 22 ] is used to obtain a Lagrangian representation corresponding to a neural implicit , 2 ) A mesh-based algorithm is used to derive a flow field on the explicit surface ( § 4.1 ) , and 3 )<br>",
    "Arabic": "مكعبات المسيرة",
    "Chinese": "行进立方体",
    "French": "Marching Cubes",
    "Japanese": "マーチングキューブ",
    "Russian": "Метод Марчинг Кубес"
  },
  {
    "English": "Markov",
    "context": "1: K SA M := κ = (κ 0 , κ 1 , . . . ) : κ t ∈ K SA H (t) is <mark>Markov</mark>, ∀t ≥ 0 K SA S := κ = (κ 0 , κ 1 , . . . ) : κ t ∈ K SA H (t) \n<br>2: := κ = (κ 0 , κ 1 , . . . ) : κ t ∈ K SA H (t), ∀t ≥ 0 = t≥0 K SA H (t). Similar to the controller policy classes, we define the SA-rectangular set of <mark>Markov</mark>/<mark>Markov</mark> time-homogeneous adversarial policies as \n<br>",
    "Arabic": "ماركوف",
    "Chinese": "马尔可夫",
    "French": "Markovien",
    "Japanese": "マルコフ",
    "Russian": "Марковский"
  },
  {
    "English": "Markov Chain",
    "context": "1: We conclude that the <mark>Markov Chain</mark> graph G P ′ md corresponding to P ′ md contains all the edges of the <mark>Markov Chain</mark> graph G P corresponding to P (the only possible extra edges in G P ′ md are self loops).<br>2: 5 For each move we define a <mark>Markov Chain</mark> sub-kernel by a transition matrix K a (W | W : I) with a ∈ A being an index. This represents the probability that the system makes a transition from state W to state W when sub-kernel a is applied (i.e. (3) \n<br>",
    "Arabic": "سلسلة ماركوف",
    "Chinese": "马尔可夫链",
    "French": "Chaîne de Markov",
    "Japanese": "マルコフ連鎖",
    "Russian": "Марковская цепь"
  },
  {
    "English": "Markov Chain Monte Carlo",
    "context": "1: A relatively new approach introduced by Wei & Selman (2005) is to use <mark>Markov Chain Monte Carlo</mark> sampling to compute an approximation of the exact model count. Their tool, ApproxCount, is able to solve several instances quite accurately, while scaling much better than both Relsat and Cachet as problem size increases.<br>2: Our second sampler runs a random walk on a graph defined over the indexed documents. Its primary advantage is that it does not need a pre-prepared query pool. This sampler employs a <mark>Markov Chain Monte Carlo</mark> method, like the Metropolis-Hastings algorithm or the Maximum Degree method, to guarantee that the random walk converges to the target distribution.<br>",
    "Arabic": "سلسلة ماركوف مونت كارلو",
    "Chinese": "马尔可夫链蒙特卡罗法 (MCMC)",
    "French": "Chaîne de Markov Monte Carlo",
    "Japanese": "マルコフ連鎖モンテカルロ法",
    "Russian": "Марковская цепь Монте-Карло (MCMC)"
  },
  {
    "English": "Markov Random Field",
    "context": "1: Another approach to modeling spatial relationships is to use a <mark>Markov Random Field</mark> (MRF) or variant (CRF,DRF) [14,15] to encode the preferences for certain spatial relationships.<br>2: In previous work, only must-linked points were considered in the neighborhood of a <mark>Markov Random Field</mark> with the generalized Potts potential function [12,28]. In this potential function , the must-link penalty is f M ( x i , x j ) = w i j [ l i = l j ] , where w i j is the cost for violating the must-link constraint ( i , j ) , and is the indicator function ( [ true ] = 1 , [ false ] =<br>",
    "Arabic": "حقل ماركوف العشوائي",
    "Chinese": "马尔可夫随机场",
    "French": "Champ aléatoire de Markov",
    "Japanese": "マルコフ確率場",
    "Russian": "Марковское случайное поле"
  },
  {
    "English": "Markov assumption",
    "context": "1: Early approaches for map localization [5,7,10,20] make use of Monte Carlo methods and the <mark>Markov assumption</mark> to maintain a sample-based posterior representation of the agent's pose. However, they only operate locally without providing any global (geographic) positioning information and thus can not be applied to the problem we consider here.<br>2: In other words, the <mark>Markov assumption</mark> constrains the state-duration distributions to be geometric in form. In reality we will want a more flexible way to model duration distributions to reflect the fact that each segment of the waveform being modeled has a typical duration length (mean time) and some variability around that mean time.<br>",
    "Arabic": "فرضية ماركوف",
    "Chinese": "马尔可夫假设",
    "French": "hypothèse de Markov",
    "Japanese": "マルコフ仮定",
    "Russian": "предположение Маркова"
  },
  {
    "English": "Markov blanket",
    "context": "1: This idea accords with recent theories of generalized Bayesian learning in biological agents, in which neural populations are thought to model the activity of other neural populations within their <mark>Markov blanket</mark> (Friston, 2010). The notion of learning through modeling other elements of the agent 's own mental state has been exploited in symbolic computational models of language acquisition ( Lee and Glass , 2012 ; Lee et al. , 2015 ) , but not in the context of artificial neural zero-resource speech models , which have so far derived their objective exclusively from the data ( Kamper<br>",
    "Arabic": "التغليف الماركوفي",
    "Chinese": "马尔可夫毯",
    "French": "Couverture de Markov",
    "Japanese": "マルコフブランケット",
    "Russian": "Марковское одеяло"
  },
  {
    "English": "Markov chain model",
    "context": "1: The first one is based on the <mark>Markov chain model</mark> and the second one relies on the standard Poisson approximation to the balls and bins setting. Both approaches yield the same estimate when n is sufficiently large. Consider the parity of number of balls landing in any specific bin (say the first) as a simple two-state <mark>Markov chain model</mark>.<br>2: As with the calculation for pi, a simple induction based on the two-state <mark>Markov chain model</mark> shows that after i balls have been thrown, the probability qi that the first two bins have an even number of balls greater than 0 is \n qi = 1 + (1 − 4/n) i − 2(1 − 2/n) i 2 .<br>",
    "Arabic": "نموذج سلسلة ماركوف",
    "Chinese": "马尔可夫链模型",
    "French": "modèle de chaîne de Markov",
    "Japanese": "マルコフ連鎖モデル",
    "Russian": "Модель марковской цепи"
  },
  {
    "English": "Markov decision Process",
    "context": "1: Decision-Theoretic Modeling: Given agent demonstrations, the task of inverse reinforcement learning (IRL) is to recover a reward function of an underlying <mark>Markov Decision Process</mark> (MDP) [1]. IRL has been used to model taxi driver behavior [34] and pedestrian behavior [35,7].<br>2: A <mark>Markov Decision Process</mark> (MDP) is commonly used to model the sequential decision process of a rational agent. In our case, we use it to describe the activity of a person with a wearable camera.<br>",
    "Arabic": "عملية اتخاذ القرار الماركوفية",
    "Chinese": "马尔可夫决策过程",
    "French": "Processus de décision markovien",
    "Japanese": "マルコフ決定過程",
    "Russian": "Марковский процесс принятия решений"
  },
  {
    "English": "Markov game",
    "context": "1: Computing the expectations necessary to descend the dual gradient can leverage recent advances in the structured, compact game representations: in particular, any graphical game with low-treewidth or finite horizon <mark>Markov game</mark> [Kakade et al., 2003] enables these computations to be performed in time that scales only polynomially in the number of decision makers.<br>2: In case of n = 1, a <mark>Markov game</mark> reduces to a Markov Decision Process (MDP) where an agent interacts with a fixed environment. At each state s, each player i simultaneously chooses an action a i from a set of actions A i . We denote the actions of all players other than i as a −i .<br>",
    "Arabic": "لعبة ماركوف",
    "Chinese": "马尔可夫博弈",
    "French": "jeu de Markov",
    "Japanese": "マルコフゲーム",
    "Russian": "Марковская игра"
  },
  {
    "English": "Markov kernel",
    "context": "1: This theorem says that the Markov chain which obeys detailed balance for each pair of moves is less effective than the one which combines all the routes. Markov chain design must balance the computation cost of computing all the proposals against the effectiveness of the <mark>Markov kernel</mark>. The situation shown in Fig.<br>",
    "Arabic": "نواة ماركوف",
    "Chinese": "马尔可夫核",
    "French": "noyau de Markov",
    "Japanese": "マルコフ核",
    "Russian": "Марковское ядро"
  },
  {
    "English": "Markov logic",
    "context": "1: We present the first unsupervised approach to the problem of learning a semantic parser, using <mark>Markov logic</mark>. Our USP system transforms dependency trees into quasi-logical forms, recursively induces lambda forms from these, and clusters them to abstract away syntactic variations of the same meaning.<br>2: Unsupervised approaches have been applied to shallow semantic tasks (e.g., paraphrasing (Lin and Pantel, 2001), information extraction (Banko et al., 2007)), but not to semantic parsing. In this paper we develop the first unsupervised approach to semantic parsing, using <mark>Markov logic</mark> (Richardson and Domingos, 2006).<br>",
    "Arabic": "منطق ماركوف",
    "Chinese": "马尔可夫逻辑",
    "French": "logique de Markov",
    "Japanese": "マルコフ論理",
    "Russian": "Марковская логика"
  },
  {
    "English": "Markov logic network",
    "context": "1: A <mark>Markov logic network</mark> (MLN) is a set of weighted first-order clauses. Together with a set of constants, it defines a Markov network with one node per ground atom and one feature per ground clause. The weight of a feature is the weight of the first-order clause that originated it.<br>2: We then describe our <mark>Markov logic network</mark> for unsupervised semantic parsing, and the learning and inference algorithms we used. Finally, we present our experiments and results.<br>",
    "Arabic": "شبكة منطق ماركوف",
    "Chinese": "马尔可夫逻辑网络",
    "French": "Réseau logique de Markov",
    "Japanese": "マルコフ論理ネットワーク",
    "Russian": "Марковская логическая сеть"
  },
  {
    "English": "Markov model",
    "context": "1: We begin our discussion with a standard discrete-time finite-state <mark>Markov model</mark> where each segment in the data corresponds to a state of the <mark>Markov model</mark>. Let the number of states be K. \n The parameters of the model include π , the initial state distribution ( typically we will constrain the waveform to always begin with the same segment ) , and A , the K × K state transition matrix ( again , this transition matrix can be constrained to be a `` left-to-right '' model which enforces a strict ordering in time of the segments<br>2: For these locations, they use frequency counting to estimate the transition parameters of a second-order <mark>Markov model</mark>. Their approach then predicts the next goal based on the current and the previous goals. In contrast to our approach, their model is not able to refine the goal estimates using GPS information observed when moving from one significant location to another.<br>",
    "Arabic": "نموذج ماركوف",
    "Chinese": "马尔可夫模型",
    "French": "modèle de Markov",
    "Japanese": "マルコフモデル",
    "Russian": "Марковская модель"
  },
  {
    "English": "Markov network",
    "context": "1: A related unified model for semi-supervised clustering with constraints was recently proposed by Segal et al. [36]. Their model is a unified <mark>Markov network</mark> that combines a binary <mark>Markov network</mark> derived from pairwise protein interaction data and a Naive Bayes <mark>Markov network</mark> modeling gene expression data.<br>2: A distinctive feature of the proposed factored model and Gibbs sampling inference is the ability to incorporate nonlocal constraints that are not easily captured in a traditional <mark>Markov network</mark> model. The bilingual constraint model described earlier is certainly a benefactor of this unique characteristic. Still, there are further linguistic constraints that we can apply to improve the NER system.<br>",
    "Arabic": "شبكة ماركوف",
    "Chinese": "马尔可夫网络",
    "French": "réseau de Markov",
    "Japanese": "マルコフネットワーク",
    "Russian": "Марковская сеть"
  },
  {
    "English": "Markov process",
    "context": "1: The reverse process of DPMs is a <mark>Markov process</mark> with Gaussian transitions. Thereby, it is interesting to compare it with other Gaussian models, e.g., the expectation propagation (EP) with the Gaussian process (GP) (Kim & Ghahramani, 2006).<br>2: Modeling the meaning of a sentence through a sequence of words whose meanings are modeled by HMMs, defines a factorial HMM for that sentence, since the overall <mark>Markov process</mark> for that sentence can be factored into inde-pendent component processes (Brand et al., 1997;Zhong and Ghosh, 2001) for the individual words.<br>",
    "Arabic": "عملية ماركوف",
    "Chinese": "马尔可夫过程",
    "French": "processus de Markov",
    "Japanese": "マルコフ過程",
    "Russian": "Марковский процесс"
  },
  {
    "English": "Markov property",
    "context": "1: The random field defined over the hidden variables is a Markov Random Field, where the probability distribution of the hidden variables obeys the following <mark>Markov property</mark>: \n<br>2: It amounts to a sequence of random variables X 1 , X 2 , ..., X t . This random process is usually memoryless (the so-called <mark>Markov property</mark>, first-order) meaning that the next state only depends on the current state and not on a sequence of preceding states.<br>",
    "Arabic": "خاصية ماركوف",
    "Chinese": "马尔可夫性质",
    "French": "propriété markovienne",
    "Japanese": "マルコフ性",
    "Russian": "Свойство Маркова"
  },
  {
    "English": "Markov state",
    "context": "1: Next, we show that a <mark>Markov state</mark> implies a similar property for the transition probabilities induced by a policy π: Property 1. Let p π be the transition probabilities induced by policy π. Then, if s is Markov, we have that \n Proof.<br>2: Another challenge emerges from the intricacies of computing optimal policies in models where the complete <mark>Markov state</mark> of the system is either not fully observable to the modeler and, consequently, to the controller, or intentionally omitted due to computational considerations.<br>",
    "Arabic": "حالة ماركوف",
    "Chinese": "马尔可夫状态",
    "French": "état markovien",
    "Japanese": "マルコフ状態",
    "Russian": "состояние Маркова"
  },
  {
    "English": "Markov transition",
    "context": "1: • Markov adversary: Markov adversaries is the natural modeling choice when the environment is believed to exhibit <mark>Markov transitions</mark> under the prescribed state and action spaces. However, this attribute permits non-stationary transition probabilities. Therefore, a Markov adversary could be an appropriate modeling assumption when the probability structure in the underlying Markov dynamics can vary across time.<br>2: We use three kinds of <mark>Markov transitions</mark> to sample from this joint distribution: 1) changing M , the number of latent thinned events, 2) changing the locations {s m } M m=1 of the thinned events, and 3) changing the latent function vector g M+K . We also address hyperparameter inference in Section 3.5.<br>",
    "Arabic": "انتقال ماركوف",
    "Chinese": "马尔可夫转移",
    "French": "transition de Markov",
    "Japanese": "マルコフ遷移",
    "Russian": "Марковские переходы"
  },
  {
    "English": "Markov transition matrix",
    "context": "1: Our algorithm is easily extended to compute maximum a posteriori estimates, applying a Dirichlet prior to the initial state distribution and to each row of the <mark>Markov transition matrix</mark>. Co-occurrences are obtained from gene expression data via the clustering algorithm described in [2], and then network is inferred using NICO.<br>2: Concretely, we revised the decoding process of BART into the NAR manner, and adapted the typical settings of DDM to better fit with BART, including <mark>Markov transition matrix</mark>, training objective and time step embeddings.<br>",
    "Arabic": "مصفوفة انتقال ماركوف",
    "Chinese": "马尔可夫转移矩阵",
    "French": "Matrice de transition de Markov",
    "Japanese": "マルコフ遷移行列",
    "Russian": "Марковская матрица переходов"
  },
  {
    "English": "Markov's inequality",
    "context": "1: A similar reasoning with too few XORs provides an upper bound, though with some complications arising from the lack of pairwise-independence of small XORs. We will use standard bounds on the concentration of moments of a probability distribution, namely, <mark>Markov's inequality</mark>, Chebyshev's inequality, and the Chernoff bound (cf. Motwani & Raghavan 1994).<br>2: ) = n t=1 α 2 F A t 2 = α 2 F A 2 F . We adjust α by a constant, and using <mark>Markov's inequality</mark> It is not hard to verify that E B 2 F = A 2 F . So by another Markov inequality, we prove the second part.<br>",
    "Arabic": "عدم المساواة ماركوف",
    "Chinese": "马尔可夫不等式",
    "French": "Inégalité de Markov",
    "Japanese": "マルコフの不等式",
    "Russian": "неравенство Маркова"
  },
  {
    "English": "MatConvNet",
    "context": "1: We rely on the VGG-16 architecture from [29], that is pretrained to perform classification in the ImageNet ILSVRC [28] but we can use any other deep network architecture. We implement our deep learning framework in <mark>MatConvNet</mark> [31].<br>",
    "Arabic": "ماتكونفنت",
    "Chinese": "MatConvNet",
    "French": "MatConvNet",
    "Japanese": "MatConvNet",
    "Russian": "MatConvNet"
  },
  {
    "English": "Matching Network",
    "context": "1: 3 generalizes the <mark>Matching Network</mark> (MN) (Vinyals et al., 2016), and serves as a general framework for arbitrary dense prediction tasks 1 . First , while MN interpolates raw categorical labels y for classification ( i.e. , g is an identity function ) , we perform matching on the general embedding space of the label encoder g ( y ) ; it encapsulates arbitrary tasks ( e.g. , discrete or continuous ) into the common embedding space , thus enabling the matching to work in a consistent<br>",
    "Arabic": "شبكة المطابقة",
    "Chinese": "匹配网络",
    "French": "Réseau d'appariement",
    "Japanese": "マッチングネットワーク",
    "Russian": "Сеть сопоставления (Matching Network)"
  },
  {
    "English": "Matrix completion",
    "context": "1: and we know a priori that the examples z α are very sparse (see for example [14]). To write this cost function in the form of <mark>Matrix Completion</mark>.<br>",
    "Arabic": "استكمال المصفوفة",
    "Chinese": "矩阵完成",
    "French": "Complétion de matrice",
    "Japanese": "行列完成",
    "Russian": "Завершение матрицы"
  },
  {
    "English": "Matérn kernel",
    "context": "1: The submodularity of this function allows us to prove sharp regret bounds for particular covariance functions, which we demonstrate for commonly used Squared Exponential and <mark>Matérn kernel</mark>s. Related Work. Our work generalizes stochastic linear optimization in a bandit setting, where the unknown function comes from a finite-dimensional linear space.<br>2: x is much larger than σ 2 s means that the demographic covariates encoded in the mean embedding are much more important to the model than the spatial coordinates. The length-scale for the <mark>Matérn kernel</mark> is a little more than half the median distance between locations, which indicates that it is performing a reasonable degree of smoothing.<br>",
    "Arabic": "نواة ماتيرن",
    "Chinese": "马特恩核",
    "French": "noyau de Matérn",
    "Japanese": "マテルンカーネル",
    "Russian": "ядро Матерна"
  },
  {
    "English": "Maximum satisfiability",
    "context": "1: In this paper, we further study the quality of solution predicted by GNNs in learning to solve <mark>Maximum Satisfiability</mark> (MaxSAT) problem, both from theoretical and practical perspectives. Based on the graph construction methods in the previous work, we build two kinds of GNN models, MS-NSFG and MS-ESFG, which can predict the solution of MaxSAT problem.<br>2: To solve the <mark>Maximum Satisfiability</mark> problem, we propose a new algorithm, called ClonSAT, based on the clonal selection principles and enhanced by a local search procedure. ClonSAT starts from an initial random population of B-Cells; each B-cell is an assignment and encodes a potential solution.<br>",
    "Arabic": "أقصى إرضاء",
    "Chinese": "最大可满足性 (Maximum Satisfiability)",
    "French": "Problème de satisfiabilité maximale",
    "Japanese": "最大充足性 (saidai jūsoku-sei)",
    "Russian": "Максимальная удовлетворимость"
  },
  {
    "English": "Metropolis Hastings",
    "context": "1: In this section, we will propose a variety of proposal kernels for scaling up Picture to complex 3D scenes. Local and Blocked Proposals from Prior: Single site metropolis hastings moves on continuous variables and Gibbs moves on discrete variables can be useful in many cases.<br>",
    "Arabic": "متروبوليس هاستينغز",
    "Chinese": "大都会哈斯廷算法",
    "French": "Metropolis-Hastings",
    "Japanese": "メトロポリス・ヘイスティングス法",
    "Russian": "Алгоритм Метрополиса-Хастингса"
  },
  {
    "English": "Metropolis algorithm",
    "context": "1: Belabbas & Wolfe [2009] proposed using the Metropolis method for approximate sampling from a k-DPP. Several recent works have shown that a natural <mark>Metropolis algorithm</mark> on k-DPPs mixes quickly. In particular, Anari et al. [2016] considers the following algorithm: Theorem 5 (Anari et al. [2016], Theorem 2).<br>",
    "Arabic": "خوارزمية متروبوليس",
    "Chinese": "大都会算法",
    "French": "algorithme de Metropolis",
    "Japanese": "メトロポリス法",
    "Russian": "Алгоритм Метрополиса"
  },
  {
    "English": "Metropolis method",
    "context": "1: In Section 2, we describe MCMC in general and the <mark>Metropolis method</mark> and Barker acceptance function in particular. Section 3 describes the experimental task we use to connect human judgments to MCMC. In Section 4, we present an experiment showing that this method can be used to recover trained category distributions from human judgments.<br>2: We use 16 workers connected  with a Ring graph to train Resnet20 on CIFAR100, and we generate a W 0 on such graph using <mark>Metropolis method</mark>. Then we adopt the slack matrix method to modify the spectral gap [4]: \n W κ = κW 0 + (1 − κ)I, \n where κ is a control parameter.<br>",
    "Arabic": "طريقة متروبوليس",
    "Chinese": "Metropolis 方法",
    "French": "méthode de Metropolis",
    "Japanese": "メトロポリス法",
    "Russian": "Метод Метрополиса"
  },
  {
    "English": "Metropolis-Hastings acceptance ratio",
    "context": "1: The <mark>Metropolis-Hastings acceptance ratio</mark> for this proposal, integrating out the vertical coordinate r, is \n a loc = q(s m ←s ′ m ) (1 + exp{g(s m )}) q(s ′ m ←s m ) (1 + exp{g(s ′ m )}) . (9) \n<br>2: Finally, we accept or reject with <mark>Metropolis-Hastings acceptance ratio</mark> \n a ins = (1 − b(M + 1, N + P )) (M + N + P ) b(M, N + P ) (M + 1) (1 + Λ(x)) . (9) \n<br>",
    "Arabic": "نسبة قبول ميتروبوليس-هاستنغز",
    "Chinese": "Metropolis-Hastings接受率",
    "French": "taux d'acceptation de Metropolis-Hastings",
    "Japanese": "メトロポリス・ヘイスティングス受容比",
    "Russian": "Отношение принятия Метрополис-Гастингса"
  },
  {
    "English": "Metropolis-Hastings algorithm",
    "context": "1: Theorem 12. Let P ′ mh be the transition matrix of the approximate <mark>Metropolis-Hastings algorithm</mark>. Then, P ′ mh forms an ergodic Markov Chain and its unique limit distribution is π ′ .<br>2: The samples, in conjunction with the weights, are then used to simulate near-uniform samples. To this end, we resort to four well-known Monte Carlo simulation methods: rejection sampling, importance sampling, the <mark>Metropolis-Hastings algorithm</mark>, and the Maximum Degree method.<br>",
    "Arabic": "خوارزمية متروبوليس-هاستينغز",
    "Chinese": "Metropolis-Hastings 算法",
    "French": "algorithme de Metropolis-Hastings",
    "Japanese": "メトロポリス・ヘイスティングス・アルゴリズム",
    "Russian": "Алгоритм Метрополиса-Гастингса"
  },
  {
    "English": "Metropolis-Hastings sampler",
    "context": "1: We now introduce the key components for the MHW algorithm and how to use it in sampling topics. They consist of the alias method [20,13] and a simplified version of the <mark>Metropolis-Hastings sampler</mark> [7].<br>2: However, due to modeling limitations of earlier probabilistic programming languages, and the inefficiency of the <mark>Metropolis-Hastings sampler</mark>, GPGP was limited to working with low-dimensional scenes, restricted shapes, and low levels of appearance variability. Moreover, it did not support the integration of bottom-up discriminative models such as deep neural networks [23,25] for data-driven proposal learning.<br>",
    "Arabic": "متروبوليس-هاستينغز العينات",
    "Chinese": "梅特罗波利斯-哈斯廷斯采样器",
    "French": "échantillonneur de Metropolis-Hastings",
    "Japanese": "メトロポリス・ヘイスティングスサンプラー",
    "Russian": "Сэмплер Метрополиса-Гастингса"
  },
  {
    "English": "Monte Carlo",
    "context": "1: Input: initial parametric choice for the functions 10) or (11) (both depending on V n+1 ) using <mark>Monte Carlo</mark> minimize this quantity (explicitly or by iterative schemes) set V n to be the minimizer end for \n V n for n ∈ {0, . . .<br>2: The location is very important to the stability of <mark>Monte Carlo</mark> training. If the LM solver fails to find the global optimum and the location of the local optimum is far from the true pose y gt , the balance between the two opposite signed terms in Eq.<br>",
    "Arabic": "طريقة مونت كارلو",
    "Chinese": "蒙特卡罗",
    "French": "Monte Carlo",
    "Japanese": "モンテカルロ法",
    "Russian": "Монте-Карло"
  },
  {
    "English": "Monte Carlo Dropout",
    "context": "1: Figure 12 presents results for the four other active learning strategies we implement -Entropy, <mark>Monte Carlo Dropout</mark> w/ Entropy, Core-Set (Language), and Core-Set (Vision) -for the BUTD model.<br>",
    "Arabic": "تسرب مونتي كارلو",
    "Chinese": "蒙特卡罗丢弃法",
    "French": "Monte Carlo Dropout",
    "Japanese": "モンテカルロドロップアウト",
    "Russian": "Выпадение Монте-Карло"
  },
  {
    "English": "Monte Carlo Tree Search",
    "context": "1: Discounting prior iterations has also been used in CFR for situations where the game structure changes, for example due to interleaved abstraction and equilibrium finding (Brown and Sandholm 2014;Brown and Sandholm 2015b). There has also been some work on applying discounting to perfect-information game solving in <mark>Monte Carlo Tree Search</mark> (Hashimoto et al. 2011).<br>2: This paper proposes and evaluates Memory-Augmented <mark>Monte Carlo Tree Search</mark> (M-MCTS), which provides a new approach to exploit generalization in online realtime search. The key idea of M-MCTS is to incorporate MCTS with a memory structure, where each entry contains information of a particular state.<br>",
    "Arabic": "بحث شجرة مونت كارلو",
    "Chinese": "蒙特卡罗树搜索",
    "French": "Recherche d'arbres de Monte-Carlo",
    "Japanese": "モンテカルロ木探索",
    "Russian": "Метод поиска дерева Монте-Карло"
  },
  {
    "English": "Monte Carlo algorithm",
    "context": "1: The BSDE formulation (6) opens the door for <mark>Monte Carlo algorithms</mark> aiming to numerically approximate Y s and Z s , and hence yielding approximations of solutions to the PDE (1) according to (5), see (Bouchard & Touzi, 2004;Gobet et al., 2005).<br>",
    "Arabic": "خوارزمية مونت كارلو",
    "Chinese": "蒙特卡罗算法",
    "French": "algorithme de Monte-Carlo",
    "Japanese": "モンテカルロアルゴリズム",
    "Russian": "Алгоритм Монте-Карло"
  },
  {
    "English": "Monte Carlo approximation",
    "context": "1: A number of algorithms (Williams, 1992;Baxter & Bartlett, 2001;Peters et al., 2005) compute a <mark>Monte Carlo approximation</mark> of the reward gradient: the agent interacts with the environment, producing an observation, action, reward sequence {x 1 , y 1 , r 1 , x 2 , . . .<br>2: Otherwise, we use a <mark>Monte Carlo approximation</mark>, drawing samples from N (s t−1 |µ (i) , Σ (i) ). Finally the observation y t is  incorporated by multiplying two Gaussian PDFs.<br>",
    "Arabic": "تقريب مونت كارلو",
    "Chinese": "蒙特卡罗近似",
    "French": "Approximation de Monte Carlo",
    "Japanese": "モンテカルロ近似",
    "Russian": "Монте-Карло аппроксимация"
  },
  {
    "English": "Monte Carlo estimate",
    "context": "1: Consequently, we will not achieve a meaningful estimate of the marginal likelihood. In order to reduce the variance of the simple <mark>Monte Carlo estimate</mark> in Eq. ( 12), we can use importance sampling, where the samples come from a proposal distribution q(w), rather than the prior.<br>2: The gradient of θ 1 is estimated by a likelihood estimator in a model-free manner, while the gradient of θ 2 is estimated relying on backpropagation via environment dynamics in a model-based manner. Specifically , for discrete terminal time decision π 1 , we apply the policy gradient theorem ( Sutton et al. , 2000 ) to obtain unbiased <mark>Monte Carlo estimate</mark> of θ1 J ( π θ ) using advantage function A π ( s , a ) = Q π ( s , a ) − V π ( s ) as target , i.e.<br>",
    "Arabic": "تقدير مونت كارلو",
    "Chinese": "蒙特卡罗估计",
    "French": "estimation de Monte Carlo",
    "Japanese": "モンテカルロ推定",
    "Russian": "Оценка методом Монте-Карло"
  },
  {
    "English": "Monte Carlo estimation",
    "context": "1: In particular, the gradient estimator of Liu et al. [34] is based on the Langevin Stein operator [19] for continuous distributions and coincides with the continuous counterpart of RELAX [20]. In contrast, our approach considers discrete Stein operators for <mark>Monte Carlo estimation</mark> in discrete distributions with exponentially large state spaces.<br>2: whereÊ lb andÊ ub are the lower and upper bounds of the expected valueÊf (x) = 1 n n f (x) n , derived from the <mark>Monte Carlo estimation</mark> with an η-confidence, given n is the number of invocations of f (x) with independent draws in the noise σ r .<br>",
    "Arabic": "تقدير مونت كارلو",
    "Chinese": "蒙特卡洛估计",
    "French": "estimation Monte Carlo",
    "Japanese": "モンテカルロ推定",
    "Russian": "Оценка методом Монте-Карло"
  },
  {
    "English": "Monte Carlo estimator",
    "context": "1: This latter source of error represents discrepancies introduced by using RFF to approximate the prior and decays at a dimension-free rate as the number of basis functions increases. Intuitively, this behavior reflects RFF's nature as a <mark>Monte Carlo estimator</mark> of the true covariance.<br>",
    "Arabic": "مُقَدِّر مونت كارلو",
    "Chinese": "蒙特卡罗估计器",
    "French": "estimateur Monte Carlo",
    "Japanese": "モンテカルロ推定量",
    "Russian": "Оценка методом Монте-Карло"
  },
  {
    "English": "Monte Carlo method",
    "context": "1: Building upon it, we propose Analytic-DPM, a training-free inference framework to improve the efficiency of a pretrained DPM while achieving comparable or even superior performance. Analytic-DPM estimates the analytic forms of the variance and KL divergence using the <mark>Monte Carlo method</mark> and the score-based model in the pretrained DPM.<br>2: When calculating the variational bound L vb (σ 2 n ) (i.e., the negative ELBO) of Analytic-DPM, we will plugσ 2 n into the variational bound and get L vb (σ 2 n ). Sinceσ 2 n is calculated by the <mark>Monte Carlo method</mark>, L vb (σ 2 n ) is a stochastic variable.<br>",
    "Arabic": "طريقة مونتي كارلو",
    "Chinese": "蒙特卡罗方法",
    "French": "méthode de Monte Carlo",
    "Japanese": "モンテカルロ法",
    "Russian": "Метод Монте-Карло"
  },
  {
    "English": "Monte Carlo sample",
    "context": "1: 6.2, we find Moser flows to be much slower to train due to the large number of <mark>Monte Carlo samples</mark> needed in the reguralizer (K = 10 4 ). We Test log-likelihood (b) RSGMs are much more robust to hyperparameters than Exp-wrapped SGMs. The diffusion coefficient is given by σ ( t , Xt ) = β ( t ) , β ( t ) = β0 + ( β f − β0 ) t. also note from Table 5 that the number of score network evaluations ( NFE ) is significantly lower for RSGMs , and is particularly detrimental for Moser flows ( 10 3 )<br>2: To see how these bounds work in practice, in Figure 9, we plot the probability thatσ 2 n is clipped by the bounds in Theorem 2 with different number of <mark>Monte Carlo samples</mark> M on CIFAR10 (LS). For all M , the curves of ratio v.s.<br>",
    "Arabic": "عينات مونت كارلو",
    "Chinese": "蒙特卡洛样本",
    "French": "échantillon de Monte Carlo",
    "Japanese": "モンテカルロサンプル",
    "Russian": "Монте-Карло выборка"
  },
  {
    "English": "Monte Carlo search",
    "context": "1: where X t ∈ C. Because the discriminator can only judge on a complete sequence, we apply <mark>Monte Carlo search</mark> with roll-out policy G i to sample the unknown last |X| − t tokens. Thus, Our penalty function for the i-th generator is calculated as: \n<br>2: The generators are set as single-layer LSTM-RNNs with input/hidden dimension size of 300 and max sample length of 15 words. The CNN in our discriminator is the same as [Zhang and Lecun, 2015]. The N in <mark>Monte Carlo search</mark> is set as 15.<br>",
    "Arabic": "البحث مونت كارلو",
    "Chinese": "蒙特卡罗搜索",
    "French": "recherche de Monte Carlo",
    "Japanese": "モンテカルロ探索",
    "Russian": "Поиск Монте-Карло"
  },
  {
    "English": "Monte Carlo simulation",
    "context": "1: 100 . This is because their algorithm estimates all O(n 2 ) entries by <mark>Monte Carlo simulation</mark>, but our algorithm only estimates O(n) diagonal entries by <mark>Monte Carlo simulation</mark>. We then evaluated the efficiency of their algorithm with R ′ = 100,000 samples. These results are shown in Table VII.<br>2: which we can use as a reference solution by approximating the expectation value via <mark>Monte Carlo simulation</mark>, however keeping in mind that in high dimensions corresponding estimators might have high variances (Hartmann & Richter, 2021).<br>",
    "Arabic": "محاكاة مونت كارلو",
    "Chinese": "蒙特卡罗模拟",
    "French": "simulation de Monte Carlo",
    "Japanese": "モンテカルロシミュレーション",
    "Russian": "Моделирование методом Монте-Карло"
  },
  {
    "English": "Monte-Carlo return",
    "context": "1: The principal mechanism of PI is to alternate between policy evaluation and policy improvement. Various well-studied approaches exist for the policy evaluation stages; these may rely on single-step bootstrap, multi-step <mark>Monte-Carlo return</mark>, or parameter-controlled interpolation of the former two.<br>",
    "Arabic": "عودة مونت كارلو",
    "Chinese": "蒙特卡罗回报",
    "French": "rendement Monte-Carlo",
    "Japanese": "モンテカルロリターン",
    "Russian": "Монте-Карло возврат"
  },
  {
    "English": "Moore-Penrose pseudo-inverse",
    "context": "1: (2) Computationally, the above solution may be found by the (unique) <mark>Moore-Penrose pseudo-inverse</mark> solution, i.e. S = R † W = (R T (RR T ) −1 )W. \n<br>2: If the object of the estimator is to predict the P@10 or MAP of a query, a logical error function would be the Minimum Mean Square Error (MMSE), in which case the weight vector is computed using the <mark>Moore-Penrose pseudo-inverse</mark> [10]: \n<br>",
    "Arabic": "العكس الزائف لمور-بنروز",
    "Chinese": "摩尔-彭罗斯伪逆",
    "French": "pseudo-inverse de Moore-Penrose",
    "Japanese": "ムーア・ペンローズ疑似逆行列",
    "Russian": "Псевдообрат Мура-Пенроуза"
  },
  {
    "English": "Morfessor",
    "context": "1: For the scholar-seeded approach we get a tie between two grammars. We use these cross-validation results to define our LIMS system, which is a language-independent morphological segmentation system, and show that it always outperforms <mark>Morfessor</mark> and on five out of six languages outperforms the best single AG of (Sirts and Goldwater, 2013).<br>2: Second, they investigate two ways of using a small amount of annotated data during training. They show that while for English, <mark>Morfessor</mark> remains the top performing system, on three other languages their approach can beat the high <mark>Morfessor</mark> baseline.<br>",
    "Arabic": "مورفيسور",
    "Chinese": "Morfessor",
    "French": "Morfessor",
    "Japanese": "モルフェッサー",
    "Russian": "Морфессор"
  },
  {
    "English": "Nadaraya-Watson estimator",
    "context": "1: Using this edit distance and semantic vectors derived from a distributional semantic model, the <mark>Nadaraya-Watson estimator</mark> can estimate the position in the semantic vector space for each word in the lexicon. The exponential edit distance kernel has been useful for modeling behavior in many tasks involving word similarity and neighborhood effects ; see , for example the Generalized Context Model ( Nosofsky , 1986 ) , which has been applied to word identification , recognition , and categorization , to inflectional morphology , and to artificial grammar learning ( Bailey and Hahn , 2001 )<br>2: . (8) Notice that when the manifold under study is a Euclidean vector space, equipped with the standard Euclidean norm, the above minimization results in the <mark>Nadaraya-Watson estimator</mark>.<br>",
    "Arabic": "مقدر نادرايا-واتسون",
    "Chinese": "纳德拉亚-沃森估计器",
    "French": "estimateur de Nadaraya-Watson",
    "Japanese": "ナダラヤ・ワトソン推定量",
    "Russian": "Оценка Надарая-Уотсона"
  },
  {
    "English": "Naive Bayes",
    "context": "1: In order to investigate whether reliance on frequency information could account for the higher accuracies of <mark>Naive Bayes</mark> and SVMs, we binarized the document vectors, setting n i (d) to 1 if and only feature f i appears in d, and reran <mark>Naive Bayes</mark> and SV M light on these new vectors.<br>2: We speculate that this indicates a difference between sentiment and topic categorization -perhaps due to topic being conveyed mostly by particular content words that tend to be repeated -but this remains to be verified. In any event, as a result of this finding, we did not incorporate frequency information into <mark>Naive Bayes</mark> and SVMs in any of the following experiments.<br>",
    "Arabic": "- التصنيف البايزي الساذج",
    "Chinese": "朴素贝叶斯",
    "French": "Bayes naïf",
    "Japanese": "単純ベイズ",
    "Russian": "Наивный байесовский классификатор"
  },
  {
    "English": "Naive Bayes classifier",
    "context": "1: This data is then used as input to a <mark>Naive Bayes classifier</mark> which learns a model of trustworthy relations using unlexicalized POS and noun phrase (NP) chunk features. The selfsupervised nature mitigates the need for hand-labeled training data, and unlexicalized features help scale to the multitudes of relations found on the Web.<br>2: For first order marginals this results in the popular <mark>Naive Bayes classifier</mark>, whereas for second order it results in Tree Augmented Naive Bayes (Friedman et al., 1997). However, these approaches do not try to optimize prediction error. They implicitly make conditional independence assumptions about the joint distribution and then use this joint distribution for prediction.<br>",
    "Arabic": "مصنف بايز الساذج",
    "Chinese": "朴素贝叶斯分类器",
    "French": "classificateur naïf de Bayes",
    "Japanese": "ナイーブベイズ分類器",
    "Russian": "Наивный байесовский классификатор"
  },
  {
    "English": "Nash equilibria",
    "context": "1: Convergence to <mark>Nash equilibria</mark> in twoplayer games was studied in Singh et al. (2000). WoLF (Win or Learn Fast) converges to <mark>Nash equilibria</mark> in two-player two-action games (Bowling & Veloso, 2002). Extensions include weighted policy learning (Abdallah & Lesser, 2008) and GIGA-WoLF (Bowling, 2004).<br>2: Potential games have been extensively studied since they are one of the few classes of games for which <mark>Nash equilibria</mark> can be computed (Rosenthal, 1973). For our purposes, they are games where simultaneous gradient descent on the losses is gradient descent on a single function.<br>",
    "Arabic": "توازنات ناش",
    "Chinese": "纳什均衡",
    "French": "équilibres de Nash",
    "Japanese": "ナッシュ均衡",
    "Russian": "Равновесие по Нэшу"
  },
  {
    "English": "Nash welfare",
    "context": "1: • When both indivisible goods are given to agent 2, giving the whole cake to agent 1 maximizes the <mark>Nash welfare</mark>, which is 0.2 × (0.499 + 0.499) = 0.1996.<br>2: One tempting approach to answer this open question is to consider the maximum <mark>Nash welfare</mark> (MNW) allocation. This is the allocation that maximizes the <mark>Nash welfare</mark> i∈N u i (A i ) among all allocations. 7 It has been shown that an MNW allocation enjoys many desirable properties in various settings.<br>",
    "Arabic": "رفاهية ناش",
    "Chinese": "纳什福利",
    "French": "Bien-être de Nash",
    "Japanese": "ナッシュ厚生",
    "Russian": "Велфер Нэша"
  },
  {
    "English": "Nearest Neighbor",
    "context": "1: It shows that if the relaxed Lipchitz condition (Eqn. 4) holds, then a <mark>Nearest Neighbor</mark> prediction with 1/α samples per dimension will always reduce the error by a factor of γ < 1: \n<br>2: Therefore diagnosis of past and current faults may be achieved analyzing the past multivariate time series, particularly inducing multivariate time series classifiers. To test the effect of system decomposition , we have selected six machine learning techniques : Decision Trees ( DT ) , Naive Bayes ( NB ) , Support Vector Machines ( SVM ) ( with linear or perceptron kernel ) , <mark>Nearest Neighbor</mark> ( k-NN ) ( with Dynamic Time Warping , DTW , as a dissimilarity measure ) , and Stacking Nearest<br>",
    "Arabic": "أقرب جار",
    "Chinese": "最近邻",
    "French": "Plus proche voisin",
    "Japanese": "最近傍",
    "Russian": "Ближайший сосед"
  },
  {
    "English": "Nesterov momentum",
    "context": "1: To reduce the memory consumption on GPUs, please refer to our technical report on the memory-efficient implementation of DenseNets [26]. Following [8], we use a weight decay of 10 −4 and a <mark>Nesterov momentum</mark> [35] of 0.9 without dampening. We adopt the weight initialization introduced by [10].<br>",
    "Arabic": "زخم نيستيروف",
    "Chinese": "涅斯特罗夫动量",
    "French": "Momentum de Nesterov",
    "Japanese": "ネステロフモーメンタム",
    "Russian": "Нестеровский момент"
  },
  {
    "English": "Neural Information Processing Systems",
    "context": "1: We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents. 36th Conference on <mark>Neural Information Processing Systems</mark> (NeurIPS 2022) Track on Datasets and Benchmarks.<br>2: In Advances in <mark>Neural Information Processing Systems</mark> 33 preproceedings (NeurIPS 2020), 2020. URL https://papers.nips.cc/paper/2020/ hash/e9bf14a419d77534105016f5ec122d62-Abstract.html.<br>",
    "Arabic": "أنظمة معالجة المعلومات العصبية",
    "Chinese": "NeurIPS",
    "French": "Systèmes de traitement de l'information neuronale",
    "Japanese": "ニューラル情報処理システム",
    "Russian": "Конференция по нейронным информационным системам"
  },
  {
    "English": "Neural Network",
    "context": "1: ACM is more similar than that between B {<mark>Neural Network</mark>} and B {<mark>Neural Network</mark>} ACM .<br>2: To perform the process is only necessary to have the coordinates of the points and the proposed methodology is able to construct the roads. In the future work we intend to compare the results obtained by using the ALBH algorithm and the proposed methodology with another approach like <mark>Neural Network</mark>.<br>",
    "Arabic": "شبكة عصبية",
    "Chinese": "神经网络",
    "French": "Réseau de neurones",
    "Japanese": "ニューラルネットワーク",
    "Russian": "Нейронная сеть"
  },
  {
    "English": "Newton method",
    "context": "1: 5 Since our algorithm uses the value of D t−1 as a warm restart for computing D t , a single iteration has empirically been found to be enough. Other approaches have been proposed to update D, for instance, Lee et al. (2007) suggest using a <mark>Newton method</mark> on the dual of Eq.<br>2: While the solvers using <mark>Newton method</mark> are faster, the success rate of the solver, which combines Homotopy Continuation and MLP classifier has a higher success rate. The possible explanation for this is that the <mark>Newton method</mark> behaves \"more randomly\" than the Homotopy Continuation, and therefore, it is more difficult to train. Tab.<br>",
    "Arabic": "طريقة نيوتن",
    "Chinese": "牛顿法",
    "French": "méthode de Newton",
    "Japanese": "ニュートン法",
    "Russian": "метод Ньютона"
  },
  {
    "English": "Newton's method",
    "context": "1: (1999) proved that the above subproblem can be solved efficiently in O(k) flops, if we use a safeguarded <mark>Newton's method</mark>, where the most expensive cost is k matrix-vector products for H t g, with t ∈ [k]. We remark that though Gould et al.<br>2: A typical local iterative method (e.g. <mark>Newton's method</mark>, gradient descent, etc.) would attempt solving a problem p by obtaining an initial approximate solution s 0 with a hope that it is not too far away from an actual solution and then producing a sequence of its refinements s 0 , s 1 , s 2 , .<br>",
    "Arabic": "طريقة نيوتن",
    "Chinese": "牛顿法",
    "French": "méthode de Newton",
    "Japanese": "ニュートン法",
    "Russian": "метод Ньютона"
  },
  {
    "English": "Nom-Bank",
    "context": "1: Despite its substantial coverage, <mark>Nom-Bank</mark> does not account for all withinsentence arguments and ignores extrasentential arguments altogether. These arguments, which we call implicit, are important to semantic processing, and their recovery could potentially benefit many NLP applications. We present a study of implicit arguments for a select group of frequent nominal predicates.<br>",
    "Arabic": "بنك الأسماء",
    "Chinese": "名词论元库",
    "French": "Nom-Bank",
    "Japanese": "Nomバンク",
    "Russian": "Ном-Банк"
  },
  {
    "English": "Nyström approximation",
    "context": "1: The main takeaway from our results here is that, depending on the structure of the problem, we may end up in the regime where the <mark>Nyström approximation</mark> factor exhibits a multiple-descent curve (e.g., due to a hierarchical nature of the data) or in the regime where it is relatively flat.<br>2: Figure 4: Top plots show the <mark>Nyström approximation</mark> factor }K´p KpSq}˚{OPT k , where S is constructed using greedy subset selection, against the subset size k, for a toy dataset (κ is the condition number) and two Libsvm datasets (σ is the RBF parameter).<br>",
    "Arabic": "تقريب نايستروم",
    "Chinese": "奈斯特伦近似",
    "French": "approximation de Nyström",
    "Japanese": "ナイストレム近似",
    "Russian": "Аппроксимация Найстрёма"
  },
  {
    "English": "Ornstein-Uhlenbeck",
    "context": "1: One recovers the standard <mark>Ornstein-Uhlenbeck</mark> noising process (Song et al., 2021) for both of these target distributions when M = R d and µ = 0 since then the drift b(t, X t ) = 1 2 exp −1 Xt (0) = − 1 2 X t .<br>",
    "Arabic": "عملية أورنشتاين-أولنبيك",
    "Chinese": "奥恩斯坦-乌伦贝克过程",
    "French": "Ornstein-Uhlenbeck",
    "Japanese": "オルンシュタイン・ウーレンベック過程",
    "Russian": "Орнштейн-Уленбек"
  },
  {
    "English": "Pareto dominate",
    "context": "1: Although, in that case, policies are typically still <mark>Pareto dominated</mark> in accordance with Theorem 1. We conclude our analysis by investigating counterfactual predictive parity, the least demanding of the causal notions of fairness we have considered, requiring only that Y (1) ⊥ ⊥ \n A | D = 0.<br>2: However, for natural families of utility functions-for example, those that prefer both higher degree attainment and more student-body diversity-we prove in Section 4 that causal fairness constraints almost always lead to strongly <mark>Pareto dominated</mark> decision policies.<br>",
    "Arabic": "يتفوق باريتو",
    "Chinese": "帕累托支配",
    "French": "dominer au sens de Pareto",
    "Japanese": "パレート優位",
    "Russian": "Парето-доминирует"
  },
  {
    "English": "Pareto frontier",
    "context": "1: Indeed, this is because the divergence curve encodes the <mark>Pareto frontier</mark> of (KL(P |•), KL(Q|•)). T ppl (P ) − T ppl (Q) = exp − E P [log R(x)] − exp − E Q [log R(x)] = \n<br>2: the level of diversity indicated on the x-axis, though these policies are not on the <mark>Pareto frontier</mark>, as they result in fewer college graduates and lower diversity than the policy that maximizes graduation alone (indicated by the \"max graduation\" point in Figure 2).<br>",
    "Arabic": "حدود باريتو",
    "Chinese": "帕累托前沿",
    "French": "frontière de Pareto",
    "Japanese": "パレートフロンティア",
    "Russian": "граница Парето"
  },
  {
    "English": "Pearson correlation",
    "context": "1: As described in detail in Section 4, the Williams test (Williams, 1959), a test also appropriate for MT metrics evaluated by the <mark>Pearson correlation</mark> , is appropriate for testing the significance of a difference in dependent correlations and therefore provides a suitable method of significance testing for quality estimation systems.<br>2: Unsupervised measures like Mahalanobis distance and <mark>Pearson correlation</mark> attempt to correct similarity estimates using the global mean and variance of the dataset. However, these measures may still fail to estimate distances accurately if the attributes' true contribution to similarity is not correlated with their variance.<br>",
    "Arabic": "ارتباط بيرسون",
    "Chinese": "皮尔逊相关系数",
    "French": "corrélation de Pearson",
    "Japanese": "ピアソン相関係数",
    "Russian": "Коэффициент корреляции Пирсона"
  },
  {
    "English": "Pearson correlation coefficient",
    "context": "1: A correlation test confirms this visually observed pattern: the <mark>Pearson correlation coefficient</mark> between performance and concurrence is near zero (r = 0.026).<br>2: Take, for instance, the possible counter-argument: a pair of systems, one of which predicts the precise gold distribution, and another system predicting the gold distribution + 1, would unfairly receive the same <mark>Pearson correlation coefficient</mark>.<br>",
    "Arabic": "معامل ارتباط بيرسون",
    "Chinese": "皮尔逊相关系数",
    "French": "coefficient de corrélation de Pearson",
    "Japanese": "ピアソンの相関係数",
    "Russian": "Коэффициент корреляции Пирсона"
  },
  {
    "English": "Pearson's correlation",
    "context": "1: Another popular class of distortion measures includes directional similarity functions such as normalized dot product (cosine similarity) and <mark>Pearson's correlation</mark> [31]. Selection of the most appropriate distortion measure for a clustering task should take into account intrinsic properties of the dataset.<br>2: We want to apply our HMRF-KMEANS algorithm to different kinds of gene representations, for which different clustering distance measures would be appropriate, e.g., <mark>Pearson's correlation</mark> would be an appropriate distortion measure for gene microarray data [20], I-divergence would be useful for the phylogenetic profile representation of genes [30], etc.<br>",
    "Arabic": "ارتباط بيرسون",
    "Chinese": "皮尔逊相关系数",
    "French": "coefficient de corrélation de Pearson",
    "Japanese": "ピアソンの相関",
    "Russian": "Коэффициент корреляции Пирсона"
  },
  {
    "English": "Pearson's correlation coefficient",
    "context": "1: However, for gene expression data, users are often interested in the overall trends of the expression levels instead of the absolute magnitudes. Therefore, we choose the <mark>Pearson's correlation coefficient</mark> as the coherence measure, since it is robust to shifting and scaling patterns [22].<br>2: WMT has established a method based on <mark>Pearson's correlation coefficient</mark> for measuring how well automatic metrics match with human judgements of translation quality, which is used to rank metrics and to justify their widespread use in lieu of human evaluation.<br>",
    "Arabic": "معامل ارتباط بيرسون",
    "Chinese": "皮尔逊相关系数",
    "French": "Coefficient de corrélation de Pearson",
    "Japanese": "ピアソンの相関係数",
    "Russian": "Коэффициент корреляции Пирсона"
  },
  {
    "English": "Pearson's r",
    "context": "1: To explicitly measure the effectiveness of the Constrained Forward-Backward algorithm for confidence estimation, Table 5 displays two evaluation measures: <mark>Pearson's r</mark> and average precision. <mark>Pearson's r</mark> is a correlation coefficient ranging from<br>2: Table 3 shows examples of pseudo-parallel data obtained with UScore wrd and UScore snt . Tables 4, 5, 6, and 7 show segment-level <mark>Pearson's r</mark> correlations with human judgments on WMT-16, WMT-17, MLQE-PE, as well as WMT-MQM and Eval4NLP, respectively. Table 8 provides additional statistics for each dataset.<br>",
    "Arabic": "معامل بيرسون",
    "Chinese": "皮尔逊相关系数",
    "French": "coefficient de corrélation de Pearson",
    "Japanese": "ピアソンのr",
    "Russian": "Коэффициент корреляции Пирсона"
  },
  {
    "English": "Pearson's r correlation",
    "context": "1: Analysis (step 6): We compute the <mark>Pearson's r correlation</mark> between the LabintheWild annotations by demographic for the dataset's original labels and the models' predictions. We apply the Bonferroni correction to account for multiple hypothesis testing. 2017); instead of monetary compensation, participants typically partake in LabintheWild experiments because they learn something about themselves.<br>",
    "Arabic": "معامل ارتباط بيرسون",
    "Chinese": "皮尔逊相关系数",
    "French": "corrélation de Pearson",
    "Japanese": "ピアソンの相関係数",
    "Russian": "Корреляция Пирсона r"
  },
  {
    "English": "Penn English Treebank",
    "context": "1: We compare the performance of our forest reranker against n-best reranking on the <mark>Penn English Treebank</mark> (Marcus et al., 1993). The baseline parser is the Charniak parser, which we modified to output a   (Collins, 2000), and others are from (Charniak and Johnson, 2005), with simplifications.<br>",
    "Arabic": "التجمع اللغوي للغة الإنجليزية في بن (Penn English Treebank)",
    "Chinese": "宾州英语树库",
    "French": "Penn Treebank anglais",
    "Japanese": "ペン英語ツリーバンク",
    "Russian": "Пенн Инглиш Трибэнк"
  },
  {
    "English": "Penn Treebank corpus",
    "context": "1: We used an LSTM with 5 hidden units and 5-dimensional embeddings, for character-level language modeling on the <mark>Penn Treebank corpus</mark> (Marcus et al., 1993) (with a vocabulary consisting of 50 unique tokens). We measured the variance of theĝ PES-A gradient estimate on a fixed sequence of 10 4 characters.<br>",
    "Arabic": "مجموعة بين تريبانك",
    "Chinese": "宾夕法尼亚树库语料库",
    "French": "corpus Penn Treebank",
    "Japanese": "Penn Treebank コーパス",
    "Russian": "корпус Penn Treebank"
  },
  {
    "English": "Perceptron",
    "context": "1: <mark>Perceptron</mark> in the teacher-student setting All code to reproduce these simulations can be found at: https://colab.research.google.com/drive/1in35C6jh7y_ynwuWLBmGOWAgmUgpl8dF? usp=sharing. <mark>Perceptron</mark>s were trained on a synthetic dataset of P examples {x µ , y µ } µ=1,...,P , where x µ ∼ N (0, I N ) are i.i.d.<br>2: Now we train the reranker to pick the oracle parses as often as possible, and in case an error is made (line 6), perform an update on the weight vector (line 7), by adding the difference between two feature representations. Pseudocode 1 <mark>Perceptron</mark> for Generic Reranking \n<br>",
    "Arabic": "البيرسيبترون",
    "Chinese": "感知机",
    "French": "Perceptron",
    "Japanese": "パーセプトロン",
    "Russian": "Перцептрон"
  },
  {
    "English": "Perron-Frobenius theorem",
    "context": "1: Since B is block diagonal, and each diagonal element is a positive stochastic matrix, it follows by the <mark>Perron-Frobenius theorem</mark> that the 1-eigenvectors of B are given by SPAN(1 Si ) i=1,...,m , where 1 Si is the vector which is 1 at index j if j ∈ S i and is 0 otherwise.<br>",
    "Arabic": "نظرية بيرون-فروبينيوس",
    "Chinese": "佩伦-弗罗贝尼乌斯定理",
    "French": "théorème de Perron-Frobenius",
    "Japanese": "ペロン・フロベニウス定理",
    "Russian": "Теорема Перрона-Фробениуса"
  },
  {
    "English": "Pinsker's inequality",
    "context": "1: By <mark>Pinsker's inequality</mark> (Lemma E.11), d TV (P 0 , P 1 ) ≤ 1 2 KL(P 0 , P 1 ) ≤ 1 2 . By Le Cam's Lemma (Lemma E.10), for any hypothesis testerb, we have \n<br>2: Moreover, KL(p i,1:j , p * j ) < 128jε 2 by tensorization and TV(p i,1:j , p * j ) ≤ 8ε √ j by <mark>Pinsker's inequality</mark>. Let E be the set of outcomes of T i flips under which Q ′ predicts H 0 .<br>",
    "Arabic": "عدم المساواة بينسكر",
    "Chinese": "平斯克不等式",
    "French": "L'inégalité de Pinsker",
    "Japanese": "ピンスカーの不等式",
    "Russian": "Неравенство Пинскера"
  },
  {
    "English": "Pitman-Yor process",
    "context": "1: We proposed a novel backoff modeling of an SR-TSG based on the hierarchical <mark>Pitman-Yor Process</mark> and sentence-level and tree-level blocked MCMC sampling for training our model. Our best model significantly outperformed the conventional TSG and achieved state-of-the-art result in a WSJ parsing task. Future work will involve examining the SR-TSG model for different languages and for unsupervised grammar induction.<br>2: We also propose a novel probabilistic SR-TSG model with the hierarchical <mark>Pitman-Yor Process</mark> (Pitman and Yor, 1997), namely a sort of nonparametric Bayesian model, to encode backoff smoothing from a fine-grained SR-TSG to simpler CFG rules, and develop an efficient training method based on blocked MCMC sampling.<br>",
    "Arabic": "عملية بيتمان-يور",
    "Chinese": "皮特曼-约尔过程",
    "French": "processus de Pitman-Yor",
    "Japanese": "ピットマン・ヨア過程",
    "Russian": "Процесс Питмана-Йора"
  },
  {
    "English": "Poisson matting",
    "context": "1: Matting with known natural background had been previously explored in [24], Bayesian matting [7] and <mark>Poisson matting</mark> [30,10] which also requires a trimap. Recently Sengupta et al.<br>",
    "Arabic": "حصيرة بواسون",
    "Chinese": "泊松抠图",
    "French": "Matting de Poisson",
    "Japanese": "ポワソンマッティング",
    "Russian": "Пуассоновое матирование"
  },
  {
    "English": "Poisson model",
    "context": "1: The model used on a per-cell basis may be simple, such as a classical Poisson or Bernoulli model, or it may be an arbitrary, user-defined model of significant complexity. The stochastic model for a cell c is characterized by a probability density function (PDF), denoted by f (Xc|θc).<br>2: A <mark>Poisson model</mark> can lead to sub-optimal performance due to the limiting equidispersion constraint (mean equals the variance). Others take a two-stage approach [Cameron and Trivedi, 2013], where a classification model distinguishes the zero and non-zero and a second model is used to model the non-zero responses.<br>",
    "Arabic": "نموذج بواسون",
    "Chinese": "泊松模型",
    "French": "modèle de Poisson",
    "Japanese": "ポアソンモデル",
    "Russian": "Пуассоновская модель"
  },
  {
    "English": "Poisson process",
    "context": "1: In this section we review the <mark>Poisson process</mark> and specify our model, the Sigmoidal Gaussian Cox Process (SGCP), which transforms a Gaussian process into a nonparametric prior distribution on intensity functions. We then show that the SGCP allows exact simulation of Poisson data from a random infinitedimensional intensity function, without performing intractable integrals.<br>2: This yields a new <mark>Poisson process</mark> (due to theorem 1) and a related DP which depends on the source. In particular, we consider three such operations: superposition, subsampling, and point transition.<br>",
    "Arabic": "عملية بواسون",
    "Chinese": "泊松过程",
    "French": "processus de Poisson",
    "Japanese": "ポアソン過程",
    "Russian": "Процесс Пуассона"
  },
  {
    "English": "Poisson regression",
    "context": "1: We provide an extensive study with our framework on small to large-scale real world zero-inflated datasets and demonstrate that such flexibility in distribution modeling can lead to significantly more compact and expressive tree ensembles. This has large implications for faster inference, storage requirements and interpretability. We briefly review <mark>Poisson regression</mark> and then dive into zero-inflated Poisson models.<br>2: We consider <mark>Poisson regression</mark> with GBDT and differentiable tree ensembles. We also consider zero-inflation modeling with differentiable tree ensem-  bles. We use GBDT from sklearn Buitinck et al. [2013]. For additional details about the tuning experiments, please see Supplemental Section S1.2.<br>",
    "Arabic": "انحدار بواسون",
    "Chinese": "泊松回归",
    "French": "régression de Poisson",
    "Japanese": "ポアソン回帰",
    "Russian": "Регрессия Пуассона"
  },
  {
    "English": "Poisson sampling",
    "context": "1: There are a few advantages of this approach ( as is true with any plug-in approach ) : ( i ) the computation of PML is agnostic to the function f at hand , ( ii ) there are no parameters to be tuned , ( iii ) techniques such as <mark>Poisson sampling</mark> or median tricks are not necessary , ( iv ) well<br>2: Poisson-Olken algorithm uses <mark>Poisson sampling</mark> to output progressively the selected tuples as it processes each candidate network. It selects the tuple t with probability \n Sc(t ) M , \n where M is an upper bound to the total scores of all candidate answers. To compute M, we use the following heuristic.<br>",
    "Arabic": "أخذ عينات بواسون",
    "Chinese": "泊松抽样",
    "French": "échantillonnage de Poisson",
    "Japanese": "ポアソン抽出",
    "Russian": "Пуассоновское семплирование"
  },
  {
    "English": "Potts model",
    "context": "1: This smoothness term, sometimes called the <mark>Potts model</mark>, is clearly discontinuitypreserving. Yet, it is known to be NP-hard to minimize [10]. However, graph cut algorithms have been developed that compute a local minimum in a strong sense [10].<br>2: where x i is a one-hot encoding of the i-th amino acid in x, J ∈ R {D×D×20×20} and h ∈ R {D×20} are the model's parameters and Z is the model's normalizing constant. The <mark>Potts model</mark>'s likelihood is the sum of pairwise interactions. Marks et al.<br>",
    "Arabic": "نموذج بوتس",
    "Chinese": "波茨模型",
    "French": "modèle de Potts",
    "Japanese": "ポッツモデル",
    "Russian": "Модель Поттса"
  },
  {
    "English": "Prop-Bank",
    "context": "1: The first sentence in Example 1 includes the <mark>Prop-Bank</mark> (Kingsbury et al., 2002) analysis of the verbal predicate produce, where arg 0 is the agentive producer and arg 1 is the produced entity. The second sentence contains an instance of the nominal predicate shipping that is not associated with arguments in NomBank (Meyers, 2007).<br>2: CoNLL-2009 ST best 85.4 65.9 73.3 Zhao et al. (2009) 82.7 67.8 74.6  87.2 -77.7  jection techniques, and ii) it uses the English <mark>Prop-Bank</mark> for all languages, which goes against our interest in capturing cross-lingual knowledge over heterogeneous inventories.<br>",
    "Arabic": "بروب بانك",
    "Chinese": "语义角色标注库",
    "French": "PropBank",
    "Japanese": "プロップバンク",
    "Russian": "Проп-Банк"
  },
  {
    "English": "Py-Torch",
    "context": "1: Models were implemented in Python using <mark>Py-Torch</mark> (Paszke et al., 2017) and Hugging Face (Wolf et al., 2019) librairies.<br>2: This section summarizes the overall implementation details of WSL approaches used in our paper. Refer to Appendix D for hyperparameter configurations of PEFT approaches. We use the <mark>Py-Torch</mark> framework 11 to implement all approaches discussed in the paper. Hugging Face (Wolf et al., 2020) is used for downloading and training the RoBERTa-base model.<br>",
    "Arabic": "باي تورش",
    "Chinese": "PyTorch",
    "French": "PyTorch",
    "Japanese": "PyTorch",
    "Russian": "PyTorch"
  },
  {
    "English": "Rademacher average",
    "context": "1: With this in mind, we now present a refined version of Theorem 3 that depends on localized <mark>Rademacher averages</mark>. The starting point for this approach is a notion of localized Rademacher complexity (we give a slightly less general notion than Bartlett et al. [3], as it is sufficient for our derivations).<br>2: We now turn to a more uniform variant Theorem 1, which depends on familiar notions of function complexity based on <mark>Rademacher averages</mark>. For a sample x 1 , . . . , x n and i.i.d.<br>",
    "Arabic": "متوسطات راديماخر",
    "Chinese": "拉德马赫平均数",
    "French": "moyenne de Rademacher",
    "Japanese": "ラデマッハー平均",
    "Russian": "радемахеровское среднее"
  },
  {
    "English": "Radon-Nikodym derivative",
    "context": "1: For λ ∈ (1, ∞), the Rényi divergence from P to Q of order λ is defined as 7 In general, we can only define the ratio P (x)/Q(x) to be the <mark>Radon-Nikodym derivative</mark> of P with respect to Q.<br>2: Likewise, the mapping µ → f µ,a is continuous for all a ∈ A, and, in the case of pathspecific fairness, the mapping of µ to the <mark>Radon-Nikodym derivative</mark> of µ • (u A ) −1 is continuous. Proof. We show only the first case. The others follow by virtually identical arguments.<br>",
    "Arabic": "المشتقة رادون-نيكوديم",
    "Chinese": "Radon-Nikodym 导数",
    "French": "dérivée de Radon-Nikodym",
    "Japanese": "ラドン・ニコディム導関数",
    "Russian": "Производная Радона-Никодима"
  },
  {
    "English": "Random Forest",
    "context": "1: The predictive importance of the dimensions of r(x) can be computed with a Lasso or a <mark>Random Forest</mark> classifier. Disentanglement is the average of the difference from one of the entropy of the probability that a dimension of the learned representation is useful for predicting a factor weighted by the relative importance of each dimension.<br>2: It is well-known in literature that better decision trees can be obtained by training a multitude of trees, each in a slightly different manner or using different data, and averaging the estimated results of the trees. This concept is known as a <mark>Random Forest</mark> [2].<br>",
    "Arabic": "غابة عشوائية",
    "Chinese": "随机森林",
    "French": "Forêt aléatoire",
    "Japanese": "ランダムフォレスト",
    "Russian": "Случайный лес"
  },
  {
    "English": "Rao-blackwellization",
    "context": "1: where we now only need to learn a scalar-valued function h. Notably, when h exactly equals f and A = P − I for any discrete time Markov chain kernel P , our CV adjustment amounts to <mark>Rao-Blackwellization</mark> [42, Sec.<br>2: The sIS estimator is obtained by performing importance sampling on the conditional expectation of the reward with respect to a small subset of actions for each instance (a form of <mark>Rao-Blackwellization</mark>). We employ this estimator in a novel algorithmic procedure-named Policy Optimization for eXtreme Models (POXM)-for learning from bandit feedback on XMC tasks.<br>",
    "Arabic": "راو بلاكويليزيشن",
    "Chinese": "拉奥-布莱克韦尔化",
    "French": "Rao-blackwellisation",
    "Japanese": "ラオ・ブラックウェル化法",
    "Russian": "Рао-Блэквеллизация"
  },
  {
    "English": "Recurrent Neural Network",
    "context": "1: This distance measure has been applied previously for various spelling correction approaches (Owolabi and McGregor, 1988;Kohonen, 1978). Kanojia et al. (2019b) proposed Weighted Lexical Similarity (WLS) and we use it with the character-based <mark>Recurrent Neural Network</mark> architecture proposed by them to compute another set of baseline scores.<br>2: Our goal is to create an efficient annotation tool for labeling object instances with polygons. As is typical in an annotation setting, we assume that the user provides the bounding box around the object. Given the image patch inside the box, our method predicts a (closed) polygon outlining the object using a <mark>Recurrent Neural Network</mark>.<br>",
    "Arabic": "شبكة عصبية متكررة",
    "Chinese": "循环神经网络",
    "French": "Réseau de neurones récurrents",
    "Japanese": "再帰ニューラルネットワーク",
    "Russian": "Рекуррентная нейронная сеть"
  },
  {
    "English": "Reformer",
    "context": "1: (2) The Informer beats its canonical degradation Informer † mostly in wining-counts, i.e., 32>12, which supports the query sparsity assumption in providing a comparable attention feature map. Our proposed method also out-performs the most related work LogTrans and <mark>Reformer</mark>.<br>2: <mark>Reformer</mark> (Kitaev, Kaiser, and Levskaya 2019) also achieves O(L log L) with locallysensitive hashing self-attention, but it only works on extremely long sequences. More recently, Linformer (Wang et al.<br>",
    "Arabic": "المصلح",
    "Chinese": "改革者",
    "French": "Reformer",
    "Japanese": "リフォーマー (Reformer)",
    "Russian": "Reformer"
  },
  {
    "English": "ResNeXt",
    "context": "1: In the following, we compare their performance on DiDeMo, as shown in   with a single expert, we find that object features (<mark>ResNeXt</mark> and SENet) achieve top-2 recalls on NPs, while action features (I3D, R2P1D and S3DG) achieve the top-3 recalls on VPs and PPs.<br>2: We evaluate ResNet [19] and <mark>ResNeXt</mark> [45] networks of depth 50 or 101 layers. The original implementation of Faster R-CNN with ResNets [19] extracted features from the final convolutional layer of the 4-th stage, which we call C4. This backbone with ResNet-50, for example, is denoted by ResNet-50-C4.<br>",
    "Arabic": "ResNeXt",
    "Chinese": "ResNeXt",
    "French": "ResNeXt",
    "Japanese": "ResNeXt",
    "Russian": "ResNeXt"
  },
  {
    "English": "Robertson-Webb model",
    "context": "1: We then give an algorithm to compute an ǫ-EFM allocation in the <mark>Robertson-Webb model</mark> with running time polynomial in the number of agents n, the number of indivisible goods m, and 1/ǫ, and query complexity polynomial in n and 1/ǫ. We note that this algorithm does not require a perfect allocation oracle.<br>2: In addition, in Section 4, we present two algorithms that could compute an EFM allocation for two special cases without using the perfect allocation oracle: (1) two agents with general additive valuations in the <mark>Robertson-Webb model</mark>, and (2) any number of agents with piecewise linear valuation functions.<br>",
    "Arabic": "نموذج روبرتسون-ويب",
    "Chinese": "罗伯逊-韦伯模型",
    "French": "Modèle de Robertson-Webb",
    "Japanese": "ロバートソン・ウェブモデル",
    "Russian": "Модель Робертсона-Уэбба"
  },
  {
    "English": "Runge-Kutta",
    "context": "1: Nonetheless, the experimentally Algorithm 3 Deterministic sampling using general 2 nd order <mark>Runge-Kutta</mark>, σ(t) = t and s(t) = 1. 1: procedure ALPHASAMPLER(D θ (x; σ), t i∈{0,...,N } , α) 2: \n<br>2: We rely on the Dormand-Prince solver (Dormand and Prince, 1980), an adaptive <mark>Runge-Kutta</mark> 4(5) solver, with absolute and relative tolerance of 1e − 5 to compute approximate numerical solutions of any ODEs. For the rollouts of the SGM SDEs we use a Euler Maruyama predictor and no corrector.<br>",
    "Arabic": "رونج-كوتا",
    "Chinese": "龙格-库塔",
    "French": "Runge-Kutta",
    "Japanese": "ルンゲ＝クッタ",
    "Russian": "Метод Рунге-Кутты"
  },
  {
    "English": "Runge-Kutta method",
    "context": "1: Our second set of contributions concerns the sampling processes used to synthesize images using diffusion models. We identify the best-performing time discretization for sampling, apply a higherorder <mark>Runge-Kutta method</mark> for the sampling process, evaluate different sampler schedules, and analyze the usefulness of stochasticity in the sampling process.<br>",
    "Arabic": "طريقة رنج-كوتا",
    "Chinese": "龙格-库塔法",
    "French": "méthode de Runge-Kutta",
    "Japanese": "ルンゲ・クッタ法",
    "Russian": "Метод Рунге-Кутты"
  },
  {
    "English": "Rényi entropy",
    "context": "1: (Bar-Yossef et al., 2001;Acharya et al., 2015;Caferov et al., 2015;Obremski & Skorski, 2017) estimated <mark>Rényi entropy</mark> and (Bu et al., 2016) estimated KL divergence.<br>2: It is worth noting that this regularizer has a connection to entropy regularization, which can be seen by looking at the formula for <mark>Rényi entropy</mark>. Squared Regularizer. Finally, we consider a novel squared penalty, that, again, exploits the goal of MAP decoding.<br>",
    "Arabic": "إنتروبيا رينيي",
    "Chinese": "雷尼熵",
    "French": "Entropie de Rényi",
    "Japanese": "レニーエントロピー",
    "Russian": "Энтропия Реньи"
  },
  {
    "English": "S-expression",
    "context": "1: where each token y t is either a token from the vocabulary V or an intermediate subprogram from the set S storing all previously generated subprograms. V comprises all schema items in K, syntactic symbols in <mark>S-expressions</mark> (i.e., parentheses and function names), and the special token ⟨EOS⟩.<br>2: KBQA is typically modeled as semantic parsing , where the utterance is mapped to an executable program/plan in a certain formal language (e.g., SPARQL, λ-calculus, or Sexpression) whose denotation is the answer. We use <mark>S-expressions</mark> (Gu et al., 2021) for its compactness. An example is shown in Figure 2.<br>",
    "Arabic": "التعبير الصريح (S-expression)",
    "Chinese": "S-表达式",
    "French": "S-expression",
    "Japanese": "S式",
    "Russian": "S-выражения"
  },
  {
    "English": "Schur complement",
    "context": "1: On the other hand, all tracks need to be updated simultaneously because of the interdependency caused by the camera poses. To accelerate the convergence, we form a reduced camera system based on the <mark>Schur complement</mark> and use embedded point iterations [42]. The refinement generally converges within a few camera updates.<br>",
    "Arabic": "تكملة شور",
    "Chinese": "舒尔补",
    "French": "Complément de Schur",
    "Japanese": "シュール補体",
    "Russian": "дополнение Шура"
  },
  {
    "English": "Scikit-learn",
    "context": "1: Second, wherever possible, we resorted to using the default, well-tested <mark>Scikit-learn</mark> (Pedregosa et al., 2011) implementations instead of using custom implementations with potentially hard to set hyperparameters.<br>",
    "Arabic": "حزمة Scikit-learn",
    "Chinese": "Scikit-learn",
    "French": "Scikit-learn",
    "Japanese": "Scikit-learn",
    "Russian": "Scikit-learn"
  },
  {
    "English": "Semantic Scholar",
    "context": "1: papermage has powered multiple research prototypes of AI applications over scientific documents, along with <mark>Semantic Scholar</mark>'s large-scale production system for processing millions of PDFs.<br>",
    "Arabic": "الباحث الدلالي",
    "Chinese": "语义学者 (Semantic Scholar)",
    "French": "- Semantic Scholar",
    "Japanese": "セマンティック・スカラー",
    "Russian": "Semantic Scholar"
  },
  {
    "English": "Semantic Web",
    "context": "1: It is necessary to introduce technology to signal users when new content of interest appears. RSS allow users to subscribe to sources. Semantic technology allows the creation of automatic alerts for new interesting content based on semantic analysis. <mark>Semantic Web</mark> can contribute introducing computer-readable representations for simple fragments of meaning.<br>2: We adapt the concept of a label bureau from PICS so that an application of the <mark>Semantic Web</mark> can obtain semantic annotations for a page from a third party even when the author of the page has annotated the page. Semantic annotations can be retrieved separately from the documents to which they refer.<br>",
    "Arabic": "الويب الدلالي",
    "Chinese": "语义网",
    "French": "Web sémantique",
    "Japanese": "セマンティック・ウェブ",
    "Russian": "Семантическая паутина"
  },
  {
    "English": "Seq2Seq",
    "context": "1: The <mark>Seq2Seq</mark> framework (Sutskever et al., 2014;Bahdanau et al., 2015) has been the de facto choice for grounded language understanding, where the LM directly generates a plan given an input utterance. However, the lack of grounding during pretraining makes generating valid plans from LMs challenging.<br>2: We propose a <mark>Seq2Seq</mark> model as a strong baseline and compare it with several text-conditional image generation models. Experimental results demonstrate that the design generation task brings up several challenges and is not well-solved by existing text-conditional image generation techniques.<br>",
    "Arabic": "Seq2Seq",
    "Chinese": "Seq2Seq",
    "French": "Seq2Seq",
    "Japanese": "系列から系列へのモデル",
    "Russian": "Seq2Seq"
  },
  {
    "English": "Set cover",
    "context": "1: But this would solve the underlying instance of <mark>Set Cover</mark>, and therefore is impossible assuming P = NP. Note that our inapproximability result holds in a very simple model, in which each node is \"hard-wired\" with a fixed threshold. Exploring the Boundaries of Approximability.<br>2: Initially activating the k nodes corresponding to sets in a <mark>Set Cover</mark> solution results in activating all n nodes corresponding to the ground set U , and if any set A of k nodes has σ(A) ≥ n + k, then the <mark>Set Cover</mark> problem must be solvable.<br>",
    "Arabic": "- تغطية المجموعة",
    "Chinese": "集合覆盖问题",
    "French": "Couverture d'ensemble",
    "Japanese": "集合被覆問題",
    "Russian": "Задача о покрытии множества"
  },
  {
    "English": "Shannon entropy",
    "context": "1: 3 Traditional <mark>Shannon entropy</mark> H(•) is defined on discrete variables. In the case of continuous variables, we interpret H We instantiate the variational IB (VIB) estimation method (Alemi et al., 2016) on our dependency parsing task, as illustrated in Figure 1.<br>2: However, random dropping methods can alleviate this problem, and our proposed DropMessage achieves the best effect compared with other random dropping methods. Now, we give a theoretical demonstration from the perspective of the information theory by measuring the <mark>Shannon entropy</mark> (Shannon 2001) of propagated messages.<br>",
    "Arabic": "إنتروبيا شانون",
    "Chinese": "香农熵",
    "French": "entropie de Shannon",
    "Japanese": "シャノンエントロピー",
    "Russian": "Энтропия Шеннона"
  },
  {
    "English": "Sherman-Morrison formula",
    "context": "1: By using the <mark>Sherman-Morrison formula</mark> (Sherman & Morrison, 1950), the SPG-LS can be rewritten as a quadratic fractional program (Bishop et al., 2020) inf \n w 1 γ zw T w + Xw 1 + 1 γ w T w − y 2 . (2) \n<br>",
    "Arabic": "صيغة شيرمان-موريسون",
    "Chinese": "谢尔曼-莫里森公式",
    "French": "Formule de Sherman-Morrison",
    "Japanese": "シャーマン・モリソンの公式",
    "Russian": "Формула Шермана-Моррисона"
  },
  {
    "English": "Sherman-Morrison-Woodbury formula",
    "context": "1: The online metric learning (OML) algorithm is shown as Algorithm 2. Note that (A −1 t − I) −1 can be obtained from (A −1 t−1 − I) −1 by using the <mark>Sherman-Morrison-Woodbury formula</mark>.<br>",
    "Arabic": "صيغة شيرمان-موريسون-وودبري",
    "Chinese": "谢尔曼-莫里森-伍德伯里公式",
    "French": "Formule de Sherman-Morrison-Woodbury",
    "Japanese": "シャーマン・モリソン・ウッドベリー公式",
    "Russian": "Формула Шермана-Моррисона-Вудбери"
  },
  {
    "English": "Sinkhorn algorithm",
    "context": "1: Therefore, we instead propose a relaxation in the formulation of discrete optimal transport, which can then be solved efficiently via the <mark>Sinkhorn algorithm</mark> (Cuturi, 2013).<br>2: Nonetheless, we can still use the generalized <mark>Sinkhorn algorithm</mark> to efficiently find the target vocabulary as detailed in Section 4.6 of Peyré and Cuturi (2019). The algorithm details are shown in Algorithm 1. At each timestep t, we can generate a new vocabulary associated with entropy scores based on the transport matrix P .<br>",
    "Arabic": "خوارزمية سينكورن",
    "Chinese": "Sinkhorn算法",
    "French": "Algorithme de Sinkhorn",
    "Japanese": "シンクホーンアルゴリズム",
    "Russian": "Алгоритм Синкхорна"
  },
  {
    "English": "Sparsemax",
    "context": "1: We now turn to the plot of comparison measures versus text length in Figure 6. We expect the quality of the generation to degrade as the maximum length of the text (both machine and human-written) increases. Comparison Measures. Figure 6 plots MAUVE, Gen. PPL. and the <mark>Sparsemax</mark> score [39].<br>",
    "Arabic": "سبارسماكس",
    "Chinese": "稀疏最大",
    "French": "Sparsemax",
    "Japanese": "スパースマックス",
    "Russian": "Спарсемакс"
  },
  {
    "English": "Spearman's correlation",
    "context": "1: 34 Version Number has spearman's correlation r = .335 with article length.<br>",
    "Arabic": "معامل ارتباط سبيرمان",
    "Chinese": "斯皮尔曼相关系数",
    "French": "corrélation de Spearman",
    "Japanese": "スピアマンの相関係数",
    "Russian": "Коэффициент ранговой корреляции Спирмена"
  },
  {
    "English": "Spearman's correlation coefficient",
    "context": "1: We also report the <mark>Spearman's correlation coefficient</mark> ρ (Spearman, 1961), which measures the correlation between the model rankings according to the BMA test accuracy and the LML/CLML. We see that the LML correlates positively with the BMA test accuracy for high values of the prior precision, but negatively for lower values.<br>",
    "Arabic": "معامل ارتباط سبيرمان",
    "Chinese": "斯皮尔曼相关系数",
    "French": "Coefficient de corrélation de Spearman",
    "Japanese": "スピアマンの順位相関係数",
    "Russian": "Коэффициент ранговой корреляции Спирмена"
  },
  {
    "English": "Spearman's rank correlation coefficient",
    "context": "1: Relationship to SARI. SARI is the most popular metric used to evaluate text simplification models (Xu et al., 2016). For each model, we report <mark>Spearman's rank correlation coefficient</mark> (Spearman, 1904) between SARI and each error category.<br>2: We compare against their context-sensitive morphological recursive neural network (csmRNN), using <mark>Spearman's rank correlation coefficient</mark>, ρ. Table 3 shows our model obtaining a ρ-value slightly below the best csm-RNN result, but outperforming the csmRNN that used an alternative set of embeddings for initialisation.<br>",
    "Arabic": "معامل ارتباط سبيرمان للرتب",
    "Chinese": "斯皮尔曼等级相关系数",
    "French": "coefficient de corrélation des rangs de Spearman",
    "Japanese": "スピアマンの順位相関係数",
    "Russian": "Коэффициент ранговой корреляции Спирмена"
  },
  {
    "English": "Squared Exponential kernel",
    "context": "1: where ν controls the smoothness of sample paths (the smaller, the rougher) and B ν is a modified Bessel function. Note that as ν → ∞, appropriately rescaled Matérn kernels converge to the <mark>Squared Exponential kernel</mark>. Figure 4 shows random functions drawn from GP distributions with the above kernels. 3. Power law spectral decay.<br>2: For example, our rates for the frequently used <mark>Squared Exponential kernel</mark>, enforcing a high degree of smoothness, have weak dependence on the dimensionality: O( T (log T ) d+1 ) (see Fig. 1). There is a large literature on GP (response surface) optimization.<br>",
    "Arabic": "النواة الأسية التربيعية",
    "Chinese": "平方指数核",
    "French": "noyau exponentiel carré",
    "Japanese": "二乗指数カーネル",
    "Russian": "Квадратичное экспоненциальное ядро"
  },
  {
    "English": "Stanford Parser",
    "context": "1: 8 This data was sentence-segmented using MxTerminator (Reynar and Ratnaparkhi, 1997) and parsed with the <mark>Stanford Parser</mark> (Klein and Manning, 2003). For each corpus and each section, we train all our syntactic models: the two local coherence models using the production and d-sequence representations and the HMM models with the two representations.<br>2: Remaining HTML tags and sentences that are not in English are deleted. The <mark>Stanford Parser</mark> (Klein and Manning, 2003) is used to parses all 10,662 sentences. In approximately 1,100 cases it splits the snippet into multiple sentences. We then used Amazon Mechanical Turk to label the resulting 215,154 phrases. Fig.<br>",
    "Arabic": "محلل ستانفورد",
    "Chinese": "斯坦福句法分析器",
    "French": "Stanford Parser",
    "Japanese": "スタンフォードパーサー",
    "Russian": "Стэнфордский парсер"
  },
  {
    "English": "Stanford Sentiment Treebank",
    "context": "1: RTE (Dagan, Glickman, and Magnini 2006;Bar-Haim et al. 2006;Giampiccolo et al. 2007;Bentivogli et al. 2009) Recognizing Textual Entailment comes from a series of annual textual entailment challenges. SST-2 (Socher et al. 2013) <mark>Stanford Sentiment Treebank</mark> is to predict the polarity of a given sentence.<br>2: The fine-grained sentiment classification task in the <mark>Stanford Sentiment Treebank</mark> (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review. The sentences contain diverse linguistic phenomena such as idioms and complex syntac-<br>",
    "Arabic": "بنك شجرة مشاعر ستانفورد",
    "Chinese": "斯坦福情感树库",
    "French": "Stanford Sentiment Treebank",
    "Japanese": "スタンフォード感情ツリーバンク",
    "Russian": "Стэнфордский эмоциональный корпус"
  },
  {
    "English": "Stanford dependency",
    "context": "1: For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the <mark>Stanford dependencies</mark> for English.<br>2: Since the <mark>Stanford dependencies</mark> for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion.<br>",
    "Arabic": "الاعتماديات ستانفوردية",
    "Chinese": "斯坦福依存关系",
    "French": "dépendances de Stanford",
    "Japanese": "スタンフォード依存関係",
    "Russian": "Стэнфордские зависимости"
  },
  {
    "English": "Stanford dependency framework",
    "context": "1: For English, we convert the PTB constituency trees to dependencies using the <mark>Stanford dependency framework</mark> (De Marneffe et al., 2006). We then train on the standard PTB split with sections 2-21 as training, section 22 as validation, and section 23 as test. Results are similar using the Yamada and Matsumoto (2003) conversion.<br>",
    "Arabic": "الإطار الاعتمادي لجامعة ستانفورد",
    "Chinese": "斯坦福依存框架",
    "French": "cadre de dépendance de Stanford",
    "Japanese": "スタンフォード依存関係フレームワーク",
    "Russian": "Фреймворк зависимостей Stanford"
  },
  {
    "English": "Stanford dependency parser",
    "context": "1: The second baseline levers the verb-object tags learned by the <mark>Stanford dependency parser</mark>, used as proxies for ACTION and OBJECT tags respectively.<br>",
    "Arabic": "محلل التبعية في ستانفورد",
    "Chinese": "斯坦福依存句法分析器",
    "French": "analyseur de dépendances de Stanford",
    "Japanese": "スタンフォード依存構文解析器",
    "Russian": "Парсер зависимостей Стэнфорда"
  },
  {
    "English": "Stanford question answer dataset",
    "context": "1: In the remainder of this section we provide high-level sketches of the individual task results; see the supplemental material for full experimental details. Question answering The <mark>Stanford Question Answering Dataset</mark> (SQuAD) (Rajpurkar et al., 2016) contains 100K+ crowd sourced questionanswer pairs where the answer is a span in a given Wikipedia paragraph.<br>2: We here provide additional information about the datasets we use in Experiment 1: The aim is to identify semantically equivalent questions, addressing challenges such as paraphrasing and varying levels of detail. SQuAD (<mark>Stanford Question Answering Dataset</mark>; Rajpurkar et al. 2016) A reading comprehension dataset consisting of questions about passages from Wikipedia.<br>",
    "Arabic": "مجموعة بيانات أسئلة وأجوبة ستانفورد",
    "Chinese": "斯坦福问答数据集 (SQuAD)",
    "French": "Ensemble de données de questions-réponses de Stanford (SQuAD)",
    "Japanese": "スタンフォード質問回答データセット (SQuAD)",
    "Russian": "Набор данных Stanford по вопросам и ответам"
  },
  {
    "English": "Subgraph",
    "context": "1: Meanwhile, data augmentation (e.g., DropEdge (Rong et al., 2020), <mark>Subgraph</mark> (You et al., 2020;Wang et al., 2020a) ) has also been adopted to graph analysis by generating synthetic graphs to create more training data for improving the generalization of graph classification models.<br>2: 2022b), and their brief descriptions are listed as below. Node Dropping: it randomly discards a certain portion of vertices along with their connections. Edge Perturbation: it perturbs the connectivities in graph through randomly adding or dropping a certain ratio of edges. <mark>Subgraph</mark>: it samples a subgraph using random walk.<br>",
    "Arabic": "الرسم البياني الجزئي",
    "Chinese": "子图",
    "French": "Sous-graphe",
    "Japanese": "部分グラフ",
    "Russian": "подграф"
  },
  {
    "English": "Support Vector Machine",
    "context": "1: to learn the latent functions for ordinal regression , and ( 7 ) Laplacian <mark>Support Vector Machine</mark> ( LapSVM ) ( Melacci and Mikhail 2011 ) , a semi-supervised SVM classification scheme .<br>2: For instance, we might want to add a bias term to our <mark>Support Vector Machine</mark>, and we could still run a Hogwild! scheme, updating the bias only every thousand iterations or so. For future work, it would be of interest to enumerate structures that allow for parallel gradient computations with no collisions at all.<br>",
    "Arabic": "آلة الدعم بالنواقل",
    "Chinese": "支持向量机",
    "French": "Machine à vecteurs de support",
    "Japanese": "サポートベクターマシン",
    "Russian": "Машина опорных векторов"
  },
  {
    "English": "Swin-S",
    "context": "1: We refer to these methods as DAAM-⟨τ ⟩, e.g., DAAM-0.3. For supervised baselines , we evaluated semantic segmentation models trained explicitly on COCO , like Mask R-CNN with a ResNet-101 backbone ( He et al. , 2016 ) , QueryInst ( Fang et al. , 2021 ) with ResNet-101-FPN ( Lin et al. , 2017 ) , and Mask2Former ( Cheng et al. , 2022 ) with <mark>Swin-S</mark> , all implemented in MMDetection (<br>2: We build our base model, called Swin-B, to have of model size and computation complexity similar to ViT-B/DeiT-B. We also introduce Swin-T, <mark>Swin-S</mark> and Swin-L, which are versions of about 0.25×, 0.5× and 2× the model size and computational complexity, respectively.<br>",
    "Arabic": "سوين-S",
    "Chinese": "Swin-S",
    "French": "Swin-S",
    "Japanese": "スウィンS",
    "Russian": "Swin-S"
  },
  {
    "English": "T5-11b",
    "context": "1: This compares to 37.4% for fine-tuned <mark>T5-11B</mark>, and 44.7% for fine-tuned <mark>T5-11B</mark>+SSM, which uses a Q&A-specific pre-training procedure. GPT-3 in the few-shot setting approaches the performance of state-of-the-art fine-tuned models.<br>2: We replicate our experiments with T5-Large (App. C), and find that the <mark>T5-11B</mark> models perform better in all cases, and that the trends hold for the different model variations.<br>",
    "Arabic": "T5-11B",
    "Chinese": "T5-11B",
    "French": "T5-11B",
    "Japanese": "T5-11B",
    "Russian": "T5-11B"
  },
  {
    "English": "T5-11b model",
    "context": "1: In our analysis, we train a BART-large closed-book model, which is trained with questions as input and generates (q, a) pairs as output. Checkpoints are selected by Exact Match score on a development set. We also include a more powerful <mark>T5-11B model</mark> from (Roberts et al., 2020).<br>",
    "Arabic": "نموذج T5-11b",
    "Chinese": "T5-11B模型",
    "French": "Modèle T5-11B",
    "Japanese": "T5-11bモデル",
    "Russian": "модель T5-11B"
  },
  {
    "English": "T5-large",
    "context": "1: We use the Transformers (Wolf et al., 2020)   , and the caption-only baseline over a matching/ranking instance. Cartoon by Joe Dator. et al., 2020); <mark>T5-Large</mark> and CLIP were trained with Accelerate. 11<br>2: We replicate our experiments with <mark>T5-Large</mark> (App. C), and find that the T5-11B models perform better in all cases, and that the trends hold for the different model variations.<br>",
    "Arabic": "T5-كبير",
    "Chinese": "T5-大型模型",
    "French": "T5-large",
    "Japanese": "T5-Large",
    "Russian": "T5-Большой"
  },
  {
    "English": "Tanh",
    "context": "1: The language decoder is composed of an attention module (whose projection dimension is 512) over the encoded fea-   tures, an LSTM of hidden size 512, and a multi-layer perceptron (Linear → <mark>Tanh</mark> → Linear → SoftMax) that converts the hidden state into probabilities of all the words in the vocabulary.<br>",
    "Arabic": "تانه",
    "Chinese": "双曲正切 (Tanh)",
    "French": "Tanh",
    "Japanese": "Tanh",
    "Russian": "Тангенс-гиперболический (Tanh)"
  },
  {
    "English": "Taylor approximation",
    "context": "1: In addition, instead of using a subpixel <mark>Taylor approximation</mark> of the data term, our update operator learns to propose the descent direction. More recently, optical flow has also been approached as a discrete optimization problem [35,13,47] using a global objective.<br>",
    "Arabic": "تقريب تايلور",
    "Chinese": "泰勒近似",
    "French": "approximation de Taylor",
    "Japanese": "テイラー近似",
    "Russian": "Приближение Тейлора"
  },
  {
    "English": "Theano",
    "context": "1: We use the pre-trained version of the COCO Speech model, implemented in <mark>Theano</mark> (Bastien et al., 2012)  dataset (Lin et al., 2014) where speech was synthesized for the original image descriptions, using high-quality speech synthesis provided by gTTS. 2<br>2: The system was implemented using the <mark>Theano</mark> library (Bergstra et al., 2010;Bastien et al., 2012), and trained by partitioning each of the collected corpus into a training, validation, and testing set in the ratio 3:1:1.<br>",
    "Arabic": "تيانو",
    "Chinese": "Theano",
    "French": "Theano",
    "Japanese": "Theano",
    "Russian": "Theano"
  },
  {
    "English": "Toeplitz matrix",
    "context": "1: As an additional baseline, we compared decoupled sampling with a LanczOs Variance Estimates (LOVE) based alternative (Pleiss et al., 2018). The LOVE approach to sampling from GP posteriors exploits structured covariance matrices in conjunction with fast (approximate) solvers to achieve linear time complexity with respect to number of test locations. For example , when inducing locations Z are defined to be a regularly spaced grid , the prior covariance K m , m = k ( Z , Z ) can be expressed as the Kronecker product of <mark>Toeplitz matrices</mark>-a property that can be used to dramatically expedite much of the related linear algebra ( Zimmerman , 1989 ; Saatçi , 2012 ; Wilson<br>",
    "Arabic": "مصفوفة توبليتز",
    "Chinese": "托普利兹矩阵",
    "French": "matrice de Toeplitz",
    "Japanese": "トープリッツ行列",
    "Russian": "Матрица Тёплица"
  },
  {
    "English": "Transformer",
    "context": "1: 2) Event-based Context Representation: BERT is a multilayer bidirectional <mark>Transformer</mark> [15], achieving significant performance improvement on event extraction task [19]. We use a BERT model [19] to learn the context representation.<br>2: We propose a heuristic algorithm to construct attribution trees based on the method described in Section 3, the results discover the information flow inside <mark>Transformer</mark>, so that we can know the interactions between the input words and how they attribute to the final prediction. Such visualization can provide insights to understand what dependencies <mark>Transformer</mark> tends to capture.<br>",
    "Arabic": "محول",
    "Chinese": "变换器 (Transformer)",
    "French": "Transformer",
    "Japanese": "トランスフォーマー",
    "Russian": "Трансформер"
  },
  {
    "English": "Treebank",
    "context": "1: Our choice of a <mark>Treebank</mark>trained parser (driven by the goal of broad coverage) complicates this effort, because the nesting of constituents in phrase-structure parses does not always correspond to the structure of idealized semantic composition trees. Our solution is imperfect but effective.<br>2: We have presented a framework for reranking on packed forests which compactly encodes many more candidates than n-best lists. With efficient approximate decoding, perceptron training on the whole <mark>Treebank</mark> becomes practical, which can be done in about a day even with a Python implementation.<br>",
    "Arabic": "بنك الشجرة",
    "Chinese": "树库",
    "French": "Banque d'arbres",
    "Japanese": "木構造付きコーパス",
    "Russian": "Банк деревьев"
  },
  {
    "English": "Tucker decomposition",
    "context": "1: <mark>Tucker decomposition</mark>, one of the tensor decomposition methods, has been recognized as a crucial tool for discovering latent factors and detecting relations between latent factors. In practice, we analyze a given temporal tensor from various perspectives. Assume a user is interested in investigating patterns of various time ranges using <mark>Tucker decomposition</mark>.<br>2: Our main idea is to exploit a block structure: 1) carefully designating the form of a block, and 2) selecting a compression approach for each block. In this paper, we 1) split a given temporal tensor into sub-tensors along the time dimension, and 2) leverage <mark>Tucker decomposition</mark> for each sub-tensor.<br>",
    "Arabic": "تحلل تاكر",
    "Chinese": "塔克分解",
    "French": "décomposition de Tucker",
    "Japanese": "タッカー分解",
    "Russian": "Разложение Такера"
  },
  {
    "English": "Unigram",
    "context": "1: <mark>Unigram</mark> presence information turned out to be the most effective; in fact, none of the alternative features we employed provided consistently better performance once unigram presence was incorporated. Interestingly, though, the superiority of presence information in comparison to frequency information in our setting contradicts previous observations made in topic-classification work (McCallum and Nigam, 1998).<br>2: Zipf Coefficient [26] <mark>Unigram</mark> rank-frequency statistics -Self-BLEU [65] N-gram diversity -Generation Perplexity [18] Generation quality via external model R \n |EQ[log R(x)] − EP [log R(x)]| (a single point inside C(P, Q))<br>",
    "Arabic": "يونيجرام",
    "Chinese": "一元语法",
    "French": "Unigramme",
    "Japanese": "ユニグラム",
    "Russian": "Униграмма"
  },
  {
    "English": "Universal dependency",
    "context": "1: We assess head-dependent DAAM interactions across ten common syntactic relations (enhanced <mark>Universal Dependencies</mark>; Schuster and Manning, 2016), finding that, for some, the heat map of the dependent strongly subsumes the head's, while the opposite is true for others.<br>2: The full feature inventory is summarized in Table 1; what follows is a description of these features and how we derived them. Morphological features Our morphological feature inventory consists of (i) <mark>Universal Dependencies</mark> (UD) features, (ii) lexical features, and (iii) string-based features.<br>",
    "Arabic": "الاعتماديات العالمية",
    "Chinese": "通用依存关系",
    "French": "dépendances universelles",
    "Japanese": "ユニバーサル依存関係",
    "Russian": "универсальные зависимости (Universal Dependencies)"
  },
  {
    "English": "Upper confidence Bound",
    "context": "1: To overcome this problem, we define an <mark>Upper Confidence Bound</mark> (UCB) on the preference probability using uncertainty estimates that we described in 4.2.<br>",
    "Arabic": "حد الثقة الأعلى",
    "Chinese": "上置信界限 (Upper Confidence Bound)",
    "French": "Borne de confiance supérieure",
    "Japanese": "上側信頼境界 (UCB)",
    "Russian": "Верхняя доверительная граница (UCB)"
  },
  {
    "English": "Vandermonde matrix",
    "context": "1: A (ij),(ℓk) =u ℓ i v k j \n We prove that this matrix is non-singular, and for that we observe that it is the Kronecker product of two Vandermonde matrices. Recall that the t × t <mark>Vandermonde matrix</mark> defined by t numbers z 1 , . . . , z t is: \n<br>2: . , E zn [G], and form a system of n + 1 linear equations (8) with unknowns v 0 , . . . , v n . Next, observe that its matrix is a non-singular <mark>Vandermonde matrix</mark>, hence the system has a unique solution which can be computed in polynomial time.<br>",
    "Arabic": "مصفوفة فاندرموند",
    "Chinese": "范德蒙矩阵",
    "French": "Matrice de Vandermonde",
    "Japanese": "ヴァンデルモンド行列",
    "Russian": "матрица Вандермонда"
  },
  {
    "English": "Vertex cover",
    "context": "1: It asks whether has a vertex cover of size at most , i.e., whether there exists a subset ⊆ with | | ≤ such that { , } ∩ ≠ ∅ for every edge = ( , ) ∈ . We construct an instance of the -Rewiring problem on * from a VertexCover instance on as illustrated in Figure 5a.<br>2: Consider an instance of the NP-complete <mark>Vertex Cover</mark> problem defined by an undirected n-node graph G = ( V , E ) and an integer k ; we want to know if there is a set S of k nodes in G so that every edge has at least one endpoint in S. We show that this can be viewed as a special case<br>",
    "Arabic": "تغطية الرأسية",
    "Chinese": "顶点覆盖",
    "French": "Couverture de sommets",
    "Japanese": "頂点被覆",
    "Russian": "Вершинное покрытие"
  },
  {
    "English": "Viterbi",
    "context": "1: Thus, we first find the <mark>Viterbi</mark> λ-hybrid trees for all training instances, Figure 4: Construction of a two-level λ-hybrid sequence rule via substitution and reductions from a tree fragment.<br>2: Because of the context freedom of the grammar, any consistent heuristic for inside edge items usable in 1-best A * is also consistent for inside derivation items (and vice versa). In particular, the 1-best <mark>Viterbi</mark> outside score for an edge is a \"perfect\" heuristic for any derivation of that edge.<br>",
    "Arabic": "فيتربي",
    "Chinese": "维特比",
    "French": "Viterbi",
    "Japanese": "ヴィタービ",
    "Russian": "Витерби"
  },
  {
    "English": "Viterbi algorithm",
    "context": "1: A naive way is to completely trust the contour returned by the <mark>Viterbi algorithm</mark> at frame C º p ¥ , and average all the pixels inside and outside the contour to obtain the new foreground/background color model at frame C . However, if error occurs at frame C © ¶ ¥ \n<br>2: Before moving on to approximate decoding with non-local features, we first describe the algorithm for exact decoding when only local features are present, where many concepts and notations will be re-used later. We will use D(v) to denote the top derivations of node v, where D 1 (v) is its 1-best derivation. We also use the notation e , j to denote the derivation along hyperedge e , using the j i th subderivation for tail u i , so e , 1 is the best derivation along e. The exact decoding algorithm , shown in Pseudocode 2 , is an instance of the bottom-up <mark>Viterbi algorithm</mark> , which traverses the hypergraph in a topological order<br>",
    "Arabic": "خوارزمية فيتيربي",
    "Chinese": "维特比算法",
    "French": "algorithme de Viterbi",
    "Japanese": "ヴィタビアルゴリズム (Viterbi algorithm)",
    "Russian": "Алгоритм Витерби"
  },
  {
    "English": "Wasserstein distance",
    "context": "1: But KL and JS can not do it. The wasserstein distance of the two distributions is: \n W (P r , P g ) = 1 K sup ||L|| L ≤K E X∼Pr [L(X)]−E X∼Pg [L(X)]. (8 \n<br>2: The <mark>Wasserstein distance</mark> is often used to measure divergence between distributions (Arjovsky et al., 2017;Tolstikhin et al., 2018;Shen et al., 2018;Shah et al., 2018), and while alternatives exist (Ben- David et al., 2006;Borgwardt et al., 2006), it is easy to compute and parameter-free.<br>",
    "Arabic": "مسافة واسرشتاين",
    "Chinese": "瓦瑟斯坦距离",
    "French": "distance de Wasserstein",
    "Japanese": "ワッサーシュタイン距離",
    "Russian": "Расстояние Вассерштейна"
  },
  {
    "English": "Weibull",
    "context": "1: result depends on the assumption that each of the arms follows a Gumbel distribution. This seems restrictive. However, the extremal types theorem tells us that the distribution of the max of a series of trials belongs to one of three distribution families (Gumbel, Fréchet, or <mark>Weibull</mark>) independent of the underlying distribution of trials.<br>2: min x∈X {p −α W (x)} ∼ <mark>Weibull</mark>(Z −α , α −1 ). (1\") \n Estimating the mean of <mark>Weibull</mark>(Z −α , α −1 ) yields an unbiased estimator of Z −α Γ(1 + α).<br>",
    "Arabic": "توزيع وايبول",
    "Chinese": "威布尔",
    "French": "Weibull",
    "Japanese": "ワイブル分布",
    "Russian": "Вейбулл"
  },
  {
    "English": "Weisfeiler-Lehman test",
    "context": "1: Our algorithm for computing a fast subtree kernel builds upon the <mark>Weisfeiler-Lehman test</mark> of isomorphism [14], more specifically its 1-dimensional variant, also known as \"naive vertex refinement\", which we describe in the following. Assume we are given two graphs G and G and we would like to test whether they are isomorphic.<br>2: The 1-dimensional <mark>Weisfeiler-Lehman test</mark> proceeds in iterations, which we index by h and which comprise the following steps: \n Algorithm 1 One iteration of the 1-dimensional <mark>Weisfeiler-Lehman test</mark> of graph isomorphism 1: Multiset-label determination \n • For h = 1, set M h (v) := l 0 (v) = L(v) \n<br>",
    "Arabic": "اختبار فايسفيلر-ليمان",
    "Chinese": "维斯费勒-莱曼检验",
    "French": "test de Weisfeiler-Lehman",
    "Japanese": "ワイスファイラー・レーマンテスト",
    "Russian": "Тест Вейсфейлера-Лемана"
  },
  {
    "English": "Wiener process",
    "context": "1: 1) ± β(t)σ(t) 2 ∇ x log p x; σ(t) dt deterministic noise decay + 2β(t)σ(t) dω t noise injection Langevin diffusion SDE ,(6) \n where ω t is the standard <mark>Wiener process</mark>.<br>2: where ω t is the standard <mark>Wiener process</mark> and f (•, t) : R d → R d and g(•) : R → R are the drift and diffusion coefficients, respectively, where d is the dimensionality of the dataset.<br>",
    "Arabic": "عملية وينر",
    "Chinese": "维纳过程",
    "French": "processus de Wiener",
    "Japanese": "ウィーナー過程",
    "Russian": "Процесс Винера"
  },
  {
    "English": "Wilcoxon sign-rank test",
    "context": "1: The * symbol on the AVG scores means the results surpass the baseline method with statistical significance (by the <mark>Wilcoxon signed-rank test</mark>). exits of different depths.<br>2: Moreover, for each pairing of results (values of w), our full model improves significantly over the activity-only model, according to a paired <mark>Wilcoxon signed-rank test</mark> comparing the F1 scores from the 20 trials (p < 0.001). 10 Test set performance   Feature analysis.<br>",
    "Arabic": "اختبار رتبة علامة ويلكوكسون",
    "Chinese": "威尔科克森符号秩检验",
    "French": "test des rangs signés de Wilcoxon",
    "Japanese": "ウィルコクソン符号順位検定",
    "Russian": "тест Вилкоксона на знаки рангов"
  },
  {
    "English": "Winograd Schema",
    "context": "1: Several different bias measures (Rudinger et al., 2018;Zhao et al., 2018;Cao and Daumé III, 2021) for coreference resolution work similar to <mark>Winograd Schema</mark> (Winograd, 1972) where a sentence has two entities and the task is to resolve which entity a specific pronoun or noun refers to.<br>",
    "Arabic": "مخطط وينوغراد",
    "Chinese": "维诺格拉德模式",
    "French": "Schéma de Winograd",
    "Japanese": "ウィノグラード・スキーマ",
    "Russian": "Схема Винограда"
  },
  {
    "English": "Winograd Schema Challenge",
    "context": "1: The <mark>Winograd Schema Challenge</mark> (WSC) (Levesque, Davis, and Morgenstern 2011), proposed as an alternative to the Turing Test (Turing 1950), has been used as a benchmark for evaluating commonsense reasoning.<br>2: The <mark>Winograd Schema Challenge</mark> (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.<br>",
    "Arabic": "تحدي وينوغراد للمخططات",
    "Chinese": "温罗格模式挑战",
    "French": "Défi du schéma de Winograd",
    "Japanese": "ウィノグラード・スキーマ・チャレンジ",
    "Russian": "Проблема схемы Винограда (Winograd Schema Challenge)"
  },
  {
    "English": "Winogrande",
    "context": "1: We evaluate Chinchilla on various common sense benchmarks: PIQA (Bisk et al., 2020), SIQA (Sap et al., 2019), <mark>Winogrande</mark> (Sakaguchi et al., 2020), HellaSwag (Zellers et al., 2019), and BoolQ (Clark et al., 2019).<br>2: • Question answering (closed book) on Natural Questions  and TriviaQA (Joshi et al., 2017). • Reading comprehension on RACE ( Lai et al. , 2017 ) • Common sense understanding on HellaSwag ( Zellers et al. , 2019 ) , PIQA ( Bisk et al. , 2020 ) , <mark>Winogrande</mark> ( Sakaguchi et al. , 2020 ) , SIQA ( Sap et al. , 2019 ) , BoolQ ( Clark et al. , 2019 ) , and TruthfulQA<br>",
    "Arabic": "وينوغراندي",
    "Chinese": "维诺大问题集",
    "French": "Winogrande",
    "Japanese": "ウィノグランデ",
    "Russian": "Виногранде"
  },
  {
    "English": "Woodbury matrix identity",
    "context": "1: x i , ( 7 ) \n where the <mark>Woodbury matrix identity</mark> (Riedel 1992) is applied in the second step. Fixing the candidate sentence set V and the selected sentence set X 1 , Tr[V P −1 V T ] is a constant, so the objective function is the same as maximizing the second part in the trace: \n<br>",
    "Arabic": "هوية مصفوفة وودبري",
    "Chinese": "伍德伯里矩阵恒等式",
    "French": "Identité matricielle de Woodbury",
    "Japanese": "ウッドベリー行列恒等式",
    "Russian": "Матричное тождество Вудбери"
  },
  {
    "English": "Word2Vec",
    "context": "1: We obtained lemma vectors from both ChiSCor and BasiScript (introducced in Section 4.2) with <mark>Word2Vec</mark> as implemented in Gensim 4. 1.2 (Řehůřek and Sojka, 2010). For ChiSCor, the CBOW algorithm yielded the best result, for BasiScript this was Skip-gram.<br>2: We trained our vectorspace model on this corpus using the <mark>Word2Vec</mark> (Mikolov et al., 2013), as instantiated in the GEN-SIM package (Řehůřek and Sojka, 2010) for Python using default parameters.<br>",
    "Arabic": "Word2Vec",
    "Chinese": "Word2Vec",
    "French": "Word2Vec",
    "Japanese": "Word2Vec",
    "Russian": "Word2Vec"
  },
  {
    "English": "a * algorithm",
    "context": "1: Our algorithm extends the speedups achieved in the 1-best case to the k-best case and is optimal under the same conditions as a stan-dard <mark>A * algorithm</mark>. The amount of work done in the k-best phase is no more than the amount of work done by the algorithm of Huang and Chiang (2005).<br>2: On line 6, a BDD representing all closed states is constructed. The while-cycle on lines 7-16 is an <mark>A * algorithm</mark> adapted to the symbolic search. On line 8, we extract the set of states with the lowest f -value from the open list and remove all closed states from this set.<br>",
    "Arabic": "\"الگوريتم A*\"",
    "Chinese": "A*算法",
    "French": "algorithme A*",
    "Japanese": "A*アルゴリズム",
    "Russian": "Алгоритм A*"
  },
  {
    "English": "a/b test",
    "context": "1: eg : <mark>A/B tests</mark> ) vs absolute evaluation ( eg : Likert ) , which has been discussed in Tang et al . (2022) for short-form news summarization datasets like CNN/DM. Our paper is limited to faithfulness evaluation, but summaries are typically evaluated for salience, fluency, coherence as well (Fabbri et al., 2021).<br>",
    "Arabic": "اختبار أ/ب",
    "Chinese": "A/B测试",
    "French": "test A/B",
    "Japanese": "A/Bテスト",
    "Russian": "тест A / B"
  },
  {
    "English": "a2c",
    "context": "1: [10] -using the same architectures and task paradigm, but here the agent is trained with PPO instead of <mark>A2C</mark>. Having established that humans and agents seem to use different representations to perform these tasks -presumably driven by differences in inductive bias -we now consider how to provide the agent with more human-like inductive bias(es).<br>",
    "Arabic": "a2c",
    "Chinese": "A2C",
    "French": "A2C",
    "Japanese": "A2C",
    "Russian": "а2с"
  },
  {
    "English": "abductive explanation",
    "context": "1: , o l , where l ≥ 2, compute a single explanation E which is good for each o i . Joint explanations are relevant, e.g., in diagnostic reasoning. We may want to know whether different observations allow to come up with the same diagnosis, given by an <mark>abductive explanation</mark>, about a system malfunctioning.<br>",
    "Arabic": "تفسير استقرائي",
    "Chinese": "溯因解释",
    "French": "explication abductive",
    "Japanese": "- Term: 「誘拐的な説明」\n- Context: To understand the reasoning behind R i , we develop an abductive explanation based method (Peirce, 1974;Bhagavatula et al., 2020;Jung et al., 2022).\n- Candidate term translation 1: 「誘拐的な説明」\n- Candidate term translation 2: 「アブダクション説明」\n- Candidate term translation 3: 「帰納的説明」\n\nTranslated term: 誘拐的な説明",
    "Russian": "абдуктивное объяснение"
  },
  {
    "English": "ablation analysis",
    "context": "1: We compare our results with the state of the art methods of [9] and [28], using various metrics from the literature. In addition, we perform an <mark>ablation analysis</mark> to study the relative contribution of various aspects of our method.<br>",
    "Arabic": "تحليل الاستئصال",
    "Chinese": "消融分析",
    "French": "analyse d'ablation",
    "Japanese": "\"消融解析 (shōmyō kaiseki)\"",
    "Russian": "анализ аблации"
  },
  {
    "English": "ablation experiment",
    "context": "1: As a by-product, our method also excels on the COCO object detection task. In <mark>ablation experiments</mark>, we evaluate multiple basic instantiations, which allows us to demonstrate its robustness and analyze the effects of core factors.<br>2: In order to explore the impact of the mask module and sorting, we conduct <mark>ablation experiments</mark> on the RCV1-V2 dataset. The experimental results are shown in Table 3. \"w/o mask\" means that we do not perform mask operation and \"w/o sorting\" means that we randomly shuffle the label sequence in order to perturb its original order.<br>",
    "Arabic": "تجارب الاستئصال",
    "Chinese": "消融实验",
    "French": "expérience d'ablation",
    "Japanese": "アブレーション実験",
    "Russian": "эксперимент по удалению"
  },
  {
    "English": "ablation study",
    "context": "1: <mark>Ablation study</mark> of PiCO on noisy partial label learning datasets CIFAR-10 (q = 0.5, η = 0.2) and CIFAR-100 (q = 0.05, η = 0.2).<br>2: <mark>Ablation study</mark>: Table 4 shows the performance of several variants of our featuremetric optimization on ETH3D in terms of triangulation (scene Facade only) and localization (all scenes). We compare both types of adjustments, minor tweaks, and different image representations, including  Relative run times for 1000 images: \n Figure 3: Run-times.<br>",
    "Arabic": "دراسة الاستئصال",
    "Chinese": "消融研究",
    "French": "étude d'ablation",
    "Japanese": "アブレーション研究",
    "Russian": "изучение аблации"
  },
  {
    "English": "abstraction",
    "context": "1: With the shrink strategies that compute simpler <mark>abstraction</mark>s, on the other hand, memory for computing the <mark>abstraction</mark> is less of a concern, and the new label reduction method suffers from the higher computational cost for determining combinable labels.<br>2: The standard approach to computing strategies in such large games is to first generate an <mark>abstraction</mark> of the game, which is a smaller version of the game that retains as much as possible the strategic characteristics of the original game [27,29,28]. For example, a continuous action space might be discretized.<br>",
    "Arabic": "تجريد",
    "Chinese": "抽象化",
    "French": "abstraction",
    "Japanese": "抽象化",
    "Russian": "абстракция"
  },
  {
    "English": "abstraction heuristic",
    "context": "1: We now see that we can interpret it as an <mark>abstraction heuristic</mark> within the framework of general cost partitioning. We also note that with Theorem 3 we can extract an optimal cost partitioning from the dual solution of the LP for h SEQ .<br>2: This interpretation is supported by Figure 3, which compares the time to construct the <mark>abstraction heuristic</mark> for the old and new label reduction method for the strategy RL-B-N100k. The new strategy tends to construct abstractions faster and runs out of memory far less frequently.<br>",
    "Arabic": "ارشادي التجريد",
    "Chinese": "抽象启发式",
    "French": "heuristique d'abstraction",
    "Japanese": "抽象化ヒューリスティック",
    "Russian": "эвристика абстракции"
  },
  {
    "English": "abstractive summarization",
    "context": "1: With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document <mark>abstractive summarization</mark> tasks.<br>2: It has achieved state-of-theart performance on a range of common word-level transduction tasks: neural machine translation (Barrault et al., 2019), question answering (Devlin et al., 2019) and <mark>abstractive summarization</mark> (Dong et al., 2019).<br>",
    "Arabic": "تلخيص تجريدي",
    "Chinese": "抽象摘要生成",
    "French": "résumé abstrait",
    "Japanese": "抽象的要約",
    "Russian": "абстрактное резюмирование"
  },
  {
    "English": "accelerate gradient descent",
    "context": "1: Depending on whether the function f is known to be (1) convex, or (2) strongly convex with a known strong convexity parameter, Nesterov provided a set of parameter choices for achieving acceleration. Theorem 1 (Convergence of <mark>accelerated gradient descent</mark>). Nesterov accelerated scheme satisfies: \n 1.<br>",
    "Arabic": "تسريع تدرج الانحدار",
    "Chinese": "加速梯度下降",
    "French": "descente de gradient accélérée",
    "Japanese": "加速勾配降下法",
    "Russian": "ускоренный градиентный спуск"
  },
  {
    "English": "acceptance function",
    "context": "1: Note that sinceπ (y) π(x) = π(y) π(x) andq (x) q(y) = q(x) q(y) , this <mark>acceptance function</mark> can be computed using oracle access to unnormalized forms of π and q.<br>2: The <mark>acceptance function</mark> used by the MD algorithm is the following: \n r md (x) =p (x) Cπ(x) \n . Remark. Notice the reversed roles ofp(x) andπ(x), comparing to the rejection sampling <mark>acceptance function</mark>. This is not accidental.<br>",
    "Arabic": "دالة القبول",
    "Chinese": "接受函数",
    "French": "fonction d'acceptation",
    "Japanese": "受理関数",
    "Russian": "функция принятия"
  },
  {
    "English": "acceptance probability",
    "context": "1: The pseudocode for the SMC algorithm, used for constructing the proposals, is presented in Figure 4 of the Supplementary Material. The next step is to compute an <mark>acceptance probability</mark> for a proposed sequence of states X (k) * .<br>2: µ t = T (qµ t−1 ) + ν. (17) \n Particularly, if the <mark>acceptance probability</mark> q is a constant, then α t = qα t−1 + α ν . Here, α t = µ t (Ω) and α ν = ν(Ω) are the concentration parameters.<br>",
    "Arabic": "احتمالية القبول",
    "Chinese": "接受概率",
    "French": "probabilité d'acceptation",
    "Japanese": "受容確率",
    "Russian": "вероятность принятия"
  },
  {
    "English": "accumulate error",
    "context": "1: In this case the algorithm would select the second weak detector as its <mark>accumulated error</mark> of E n = 1.0 is smaller than the error of the first weak detector of E n = 1.3 (note that for each category not shared ǫ p is added).<br>2: Given node n, BAE * uses a priority function defined as the total <mark>accumulated error</mark> TE \n x (n) = FE x (n) + BE x (n) = (g x (n) + h x (n) − h 0 ) + (g x (n) − hx(n)), \n<br>",
    "Arabic": "تراكم الخطأ",
    "Chinese": "累积误差",
    "French": "erreur accumulée",
    "Japanese": "累積誤差",
    "Russian": "накопленная ошибка"
  },
  {
    "English": "accuracy",
    "context": "1: Effect of L cont and label disambiguation. We ablate the contributions of two key components of PiCO: contrastive <mark>Accuracy</mark> comparisons on noisy PLL datasets. Bold indicates superior results.<br>2: We evaluate our model on the NQ development set using Exact Match (accuracy) (Rajpurkar et al., 2016). We report the following metrics: \n 1. Contextual Answer Quality: <mark>Accuracy</mark> on the original NQ dev set. We compare the contextual answer to the expected (original) answer. 2.<br>",
    "Arabic": "الدقة",
    "Chinese": "准确性",
    "French": "précision",
    "Japanese": "精度",
    "Russian": "точность"
  },
  {
    "English": "acoustic feature",
    "context": "1: The feature prediction network must be trained using parallel speech/text data where the input is typically a sequence of characters or phones that make up an utterance, and the output is a sequence of fixed-width frames of <mark>acoustic features</mark>.<br>",
    "Arabic": "السمة الصوتية",
    "Chinese": "声学特征",
    "French": "caractéristique acoustique",
    "Japanese": "音響特徴量",
    "Russian": "акустическая характеристика"
  },
  {
    "English": "acoustic model",
    "context": "1: In speech recognition, training the parameters of the <mark>acoustic model</mark> by optimizing the (average) mutual information and conditional entropy as they are defined in information theory is a standard approach (Bahl et al., 1986;Ney, 1995).<br>",
    "Arabic": "نموذج صوتي",
    "Chinese": "声学模型",
    "French": "modèle acoustique",
    "Japanese": "音響モデル",
    "Russian": "акустическая модель"
  },
  {
    "English": "acquisition function",
    "context": "1: common query by committee strategy estimates the uncertainty of a model using an ensemble of such models. This strategy can be implemented by using MC Dropout to generate different outputs (Gal, Islam, and Ghahramani 2017) followed by an <mark>acquisition function</mark> to aggregate the prediction probabilities of those outputs.<br>2: and an <mark>acquisition function</mark> A ( x , M ) . We run active learning over a series of acquisition iterations  T where at each iteration we acquire a batch of B new examples per:x ∈ D pool to label per x = arg max x∈D pool A(x, M).<br>",
    "Arabic": "دالة الاكتساب",
    "Chinese": "采集函数",
    "French": "fonction d'acquisition",
    "Japanese": "取得関数",
    "Russian": "функция приобретения"
  },
  {
    "English": "action classification",
    "context": "1: In computer vision, context has been used in problems such as object detection and recognition [25,14,8], scene recognition [23], <mark>action classification</mark> [22], and segmentation [28].<br>",
    "Arabic": "تصنيف الفعل",
    "Chinese": "动作分类",
    "French": "classification d'actions",
    "Japanese": "行動分類",
    "Russian": "классификация действий"
  },
  {
    "English": "action embedding",
    "context": "1: (2019b), who propose to use offline policy gradients on a large action space (millions of items). Their method relies, however, on a proprietary <mark>action embedding</mark>, unavailable to us. After a brief overview of BLBF and XMC , we present a new form of BLBF that blends bandit feedback with multilabel classification.<br>2: The trajectory encoder is an LSTM with hidden size 512. The <mark>action embedding</mark> is a concatenation of the visual appearance feature vector of size 2048 and the orientation feature vector of size 128 (the 4-dimensional orientation feature [sinψ; cosψ; sinω; cosω] are tiled 32 times as used in [13]).<br>",
    "Arabic": "تضمين الفعل",
    "Chinese": "动作嵌入",
    "French": "\"plongement d'action\"",
    "Japanese": "アクション埋め込み",
    "Russian": "встраивание действий"
  },
  {
    "English": "action recognition",
    "context": "1: Over the last decade, deep convolutional neural networks (CNNs) have revolutionized supervised learning for tasks such as object recognition, <mark>action recognition</mark>, and semantic segmentation [3,15,6,19].<br>2: In order to capture the rich content from videos, we extract features from the state-of-the-art models of different tasks, including object, action, scene, sound, face, speech, and optical character recognition (OCR). For object and <mark>action recognition</mark>, we explore multiple models with different architectures and pre-trained dataset. Details are as follows: \n<br>",
    "Arabic": "التعرف على الفعل",
    "Chinese": "动作识别",
    "French": "reconnaissance des actions",
    "Japanese": "行動認識",
    "Russian": "распознавание действий"
  },
  {
    "English": "action sequence",
    "context": "1: Crucially, this feedback only needs to correlate with <mark>action sequence</mark> quality. We detail environment-based reward functions in the next section. As our results will show, reward functions built using this kind of feedback can provide strong guidance for learning. We will also consider reward functions that combine annotated supervision with environment feedback.<br>",
    "Arabic": "تسلسل الإجراءات",
    "Chinese": "动作序列",
    "French": "séquence d'actions",
    "Japanese": "行動シーケンス",
    "Russian": "последовательность действий"
  },
  {
    "English": "action set",
    "context": "1: The multi-distribution learning problem corresponds to a zero-sum game with a minimizing player having <mark>action set</mark> H, a maximizing player having <mark>action set</mark> D × L, and a payoff function ϕ(h, (D, ℓ)) = R D,ℓ (h).<br>2: This can be attributed to the fact that the proposed method efficiently leverages the underlying structure in the <mark>action set</mark> and thus learns faster. Similar observations have been made previously (Dulac-Arnold et al. 2015;He et al. 2015;Bajpai, Garg, and others 2018).<br>",
    "Arabic": "مجموعة الإجراءات",
    "Chinese": "动作集",
    "French": "ensemble d'actions",
    "Japanese": "行動集合",
    "Russian": "множество действий"
  },
  {
    "English": "action space",
    "context": "1: (2021a)) and source-copy pointers into the <mark>action space</mark>.<br>2: 2009) and was shown to be effective given a carefully designed reward function and <mark>action space</mark>. While this is a viable approach, general purpose RL can be quite sensitive to the algorithm parameters and specific definition of the reward function and actions, which can make designing an effective learner quite challenging.<br>",
    "Arabic": "مجال الإجراءات",
    "Chinese": "动作空间",
    "French": "espace d'actions",
    "Japanese": "行動空間",
    "Russian": "пространство действий"
  },
  {
    "English": "action-value function",
    "context": "1: V π (s) = E π [ ∞ t=0 γ t r t+1 | s 0 = s] and its <mark>action-value function</mark> as Q π (s, a) = E π [ ∞ t=0 γ t r t+1 | s 0 = s, a 0 = a], \n<br>2: For any actionē taken by the policy µ * , letē k denote the action for µ * k obtained by mappingē to the closest action in the available set, then expanding (12), we get, \n v µ * ( s 0 ) − v π * k ( s 0 ) ≤ γ ( 1 − γ ) −2 R max sup s P µ k ( s ) − P µ * k ( s ) 1 ≤ γ ( 1 − γ ) −2 R max sup s , s ē ( P ( s |s ,<br>",
    "Arabic": "دالة قيمة العمل",
    "Chinese": "动作-值函数",
    "French": "fonction de valeur d'action",
    "Japanese": "アクション価値関数",
    "Russian": "функция ценности действия"
  },
  {
    "English": "actionability",
    "context": "1: Higher similarity indicates greater coherence among the possible next actions. Our overall <mark>actionability</mark> measurement is defined as contains_action(R i ) + next_action_coherence(R i ). Specificity.<br>",
    "Arabic": "قابلية التنفيذ",
    "Chinese": "可操作性",
    "French": "actionnabilité",
    "Japanese": "行動可能性",
    "Russian": "возможность воздействия"
  },
  {
    "English": "activation",
    "context": "1: This means that all pre-<mark>activation</mark>s (values before applying nonlinear <mark>activation</mark>) at this layer already reach a state where gradients are extremely small 3 , causing the loss divergence to become nonrecoverable. Knowing what happened, we fabricate how GC/AGC will react in this situation. Figure 4b(left) plots the measurements of ∥ ∥ 2 (blue) and \n<br>2: Also, to evaluate its conditioning, on top of it we add an auxiliary regression head that estimates the AUs <mark>activation</mark>ŝ y = (ŷ 1 , . . . ,ŷ N ) in the image.<br>",
    "Arabic": "تنشيط",
    "Chinese": "激活",
    "French": "activation",
    "Japanese": "活性化",
    "Russian": "активация"
  },
  {
    "English": "activation function",
    "context": "1: We impose nonnegativity via a ReLU <mark>activation function</mark>, or softly via explicit regularization L nonneg = β nonneg i max(−a i , 0) where i indexes a neuron in the network, and β nonneg determines the regularization strength. Similarly, we apply regularization to the activity  \n Q h L G 2 L 0 I y V / 9 2 Z F Q Z M 1 W B r c x 3 N M u 5 X K z Z P + 8 2 O e B Y 1 Q K 1 N B X D m 3 4 m o i R F H r H F 0 D C<br>2: The gate consists of a projection layer to one dimension and an <mark>activation function</mark>. The effect of this modeling is that if the sentence-level classifier is confident that the sentence does not contain propaganda, i.e., w ∼ 0, then no propaganda technique would be predicted for any of the word tokens in the sentence.<br>",
    "Arabic": "دالة التنشيط",
    "Chinese": "激活函数",
    "French": "fonction d'activation",
    "Japanese": "活性化関数",
    "Russian": "функция активации"
  },
  {
    "English": "activation matrix",
    "context": "1: The <mark>activation matrices</mark> X in the base space might be variously called \"pre-whitened,\" \"normalized,\" \"standardized,\" or \"sphered.\" We shall call X a normalized features manifold 18 . Invoking invariance, we can make the following observations about our optimization task: \n 1.<br>",
    "Arabic": "مصفوفة التفعيل",
    "Chinese": "激活矩阵",
    "French": "matrice d'activation",
    "Japanese": "活性化行列",
    "Russian": "матрица активации"
  },
  {
    "English": "activation vector",
    "context": "1: For each candidate window, we use a feature representation that is extracted from a trained Convolutional Neural Network (CNN). Specifically, we pass the image as input to the CNN and use the <mark>activation vector</mark> of the penultimate layer of the CNN as the feature vector. Inspired by the R-CNN pipeline of Girshick et al.<br>",
    "Arabic": "متجه التفعيل",
    "Chinese": "激活向量",
    "French": "vecteur d'activation",
    "Japanese": "活性化ベクトル",
    "Russian": "вектор активации"
  },
  {
    "English": "active learning",
    "context": "1: <mark>Active learning</mark> in the MI setting was explored in prior work (Settles, Craven, and Ray 2008), where instance labels are queried to improve an instance classifier initially trained with few labeled bags.<br>2: <mark>Active learning</mark> was evaluated using standard metrics, namely accuracy und area under the learning curve. For both metrics, the respective scikit-learn implementation was used.<br>",
    "Arabic": "التعلم النشط",
    "Chinese": "主动学习",
    "French": "apprentissage actif",
    "Japanese": "アクティブラーニング",
    "Russian": "активное обучение"
  },
  {
    "English": "active learning loop",
    "context": "1: To the best of our knowledge, the question of model update in an AL loop has not been explored. We explore two fine-tuning approaches to update the model following annotation of new samples in each  round of the <mark>active learning loop</mark> -cumulative (CM) and iterative (IT). Figure 1 provides a visual explanation of the two approaches.<br>",
    "Arabic": "حلقة التعلم النشط",
    "Chinese": "主动学习循环",
    "French": "boucle d'apprentissage actif",
    "Japanese": "アクティブラーニングループ",
    "Russian": "цикл активного обучения"
  },
  {
    "English": "active set",
    "context": "1: In the extreme case, called row-action methods [8], the <mark>active set</mark> consists of a single constraint. While algorithms in this family are fairly simple to implement and entertain general asymptotic convergence properties [ 8 ] , the time complexity of most of the algorithms in this family is typically super linear in the training set size m. Moreover , since decomposition methods find a feasible dual solution and their goal is to maximize the dual objective function , they often<br>2: We are trying to maximize the expected size of the final <mark>active set</mark>. As a function of the marketing strategy x, each node v becomes active independently with probability hv(x), resulting in a (random) set of initial active nodes A.<br>",
    "Arabic": "المجموعة النشطة",
    "Chinese": "活跃集",
    "French": "ensemble actif",
    "Japanese": "アクティブセット",
    "Russian": "активное множество"
  },
  {
    "English": "activity detection",
    "context": "1: Several previous works build multiple networks and wire them together in order to capture some complex structure (or interactions) in the problem with promising results on applications such as <mark>activity detection</mark>, scene labeling, image captioning, and object detection [12,5,9,16,49,61].<br>",
    "Arabic": "الكشف عن النشاط",
    "Chinese": "活动检测",
    "French": "détection d'activité",
    "Japanese": "活動検出",
    "Russian": "определение активности"
  },
  {
    "English": "activity recognition",
    "context": "1: Also, we performed experiments on <mark>activity recognition</mark> from videos based on both space and time based kernels. As mentioned before all results are reported using tricube kernel.<br>",
    "Arabic": "التعرف على النشاط",
    "Chinese": "活动识别",
    "French": "reconnaissance d'activité",
    "Japanese": "行動認識",
    "Russian": "распознавание деятельности"
  },
  {
    "English": "actor",
    "context": "1: : fully unroll the recurrent part of the <mark>actor</mark> , aggregate gradients in the backward pass across all time steps , and apply a gradient update . We use a target network for the critic, which updates every 150 training steps for the feed-forward centralised critics and every 50 steps for the recurrent IAC critics.<br>2: Under standard function-approximation assumptions, we prove that, when the <mark>actor</mark> attains no regret in the two-player game, ATAC produces a policy that provably outperforms the behavior policies for a large anchored range of hyperparameter choices and is optimal when the offline data covers scenarios visited by an optimal policy.<br>",
    "Arabic": "ممثل",
    "Chinese": "行为者",
    "French": "acteur",
    "Japanese": "アクター",
    "Russian": "актор"
  },
  {
    "English": "actor critic algorithm",
    "context": "1: The concept of Stackelberg game has its origins in the economics literature and has been recently applied to design online model-based RL (Rajeswaran et al., 2020) and online <mark>actor critic algorithms</mark> (Zheng et al., 2021). The use of this formalism in an offline setting here is novel to our knowledge.<br>",
    "Arabic": "خوارزمية الناقد الفاعل",
    "Chinese": "演员评论家算法",
    "French": "algorithme acteur-critique",
    "Japanese": "- アクター批評家アルゴリズム",
    "Russian": "\"алгоритм актор-критик\""
  },
  {
    "English": "actor network",
    "context": "1: 2014) that use fully connected layers both to process the input and to produce the output values from the hidden state, h a t . The IAC critics use extra output heads appended to the last layer of the <mark>actor network</mark>.<br>",
    "Arabic": "شبكة الفاعل",
    "Chinese": "演员网络",
    "French": "réseau d'acteurs",
    "Japanese": "アクターネットワーク",
    "Russian": "сеть актеров"
  },
  {
    "English": "actor-critic framework",
    "context": "1: The Trinal-Clip PPO loss function improves the learning effectiveness of the <mark>actor-critic framework</mark>, and we believe it is applicable for a wide range of RL applications with imperfect information.<br>2: 2017) learning algorithm to update the policies π θ in the <mark>actor-critic framework</mark>. PPO defines the ratio function r t ( θ ) = π θ ( at|st ) π θ ′ ( at|st ) as the ratio between the current policy π θ and the old policy π θ ′ , the advantage functionÂ t which describes how much better between two consecutive states s t+1 , s t , over randomly selecting an action according<br>",
    "Arabic": "الإطار المُمثِّل-النَّاقِد",
    "Chinese": "演员-评论家框架",
    "French": "cadre acteur-critique",
    "Japanese": "アクター・クリティックフレームワーク",
    "Russian": "модель актер-критик"
  },
  {
    "English": "actor-critic method",
    "context": "1: Here that optimization is accomplished via a simple decoupled <mark>actor-critic method</mark>. In a standard policy gradient approach, with a single policy π with parameters θ, we compute gradient steps of the form (Williams, 1992): \n<br>2: Thus individual subpolicies are not uniquely identified with value functions, and the aforementioned subpolicy-specific statevalue estimator is no longer well-defined. We extend the <mark>actor-critic method</mark> to incorporate the decoupling of policies from value functions by allowing the critic to vary persample (that is, per-task-and-timestep) depending on the reward function with which the sample is associated.<br>",
    "Arabic": "طريقة الممثل الناقد",
    "Chinese": "演员-评论家方法",
    "French": "méthode acteur-critique",
    "Japanese": "アクター・クリティック法",
    "Russian": "метод актер-критик"
  },
  {
    "English": "adapter",
    "context": "1: [26] and Kurumuz [43] implement HyperNetworks for Stable Diffusion [72] to change the artistic style of its output images. <mark>Adapter</mark> methods are widely used in NLP for customizing a pretrained transformer model to other tasks by embedding new module layers into it [30,84].<br>2: In particular, due to the vocabulary gap, the <mark>Adapter</mark> method is vulnerable to learning incremental languages that have a distinct script with Latin. Although the Extension alleviates this issue by expanding the embedding layer, the additional parameters are not fully optimized to suffer from the off-target problem for MNMT.<br>",
    "Arabic": "محول",
    "Chinese": "适配器",
    "French": "adaptateur",
    "Japanese": "アダプター",
    "Russian": "адаптер"
  },
  {
    "English": "adapter-base fine-tuning",
    "context": "1: The reduction factor is set to 16. • BitFit (Zaken et al., 2022) updates only the bias parameters of every layer and keeps all other weights frozen. Despite its simplicity it has been demonstrated to achieve similar results to <mark>adapter-based fine-tuning</mark>. We use a fixed learning rate of 1e −4 in all experiments.<br>2: Adapter-based fine-tuning (Houlsby et al., 2019;Pfeiffer et al., 2020) remedies for these potential issues by keeping the original transformer's parameters frozen and inserting new adapter parameters in transformer layers. In finetuning, both sets of parameters are used to make predictions, but we only update adapters based on loss gradients.<br>",
    "Arabic": "ضبط دقيق قائم على المحول",
    "Chinese": "适配器微调",
    "French": "réglage fin de la base de l'adaptateur",
    "Japanese": "アダプターベースのファインチューニング",
    "Russian": "Настройка на основе адаптера"
  },
  {
    "English": "adaptive boosting algorithm",
    "context": "1: We show therefore how to derive an <mark>adaptive boosting algorithm</mark> by modifying the game-theoretic strategy based on the minimal condition. This algorithm enjoys a number of theoretical guarantees. Unlike some of the non-adaptive strategies, it is efficiently computable, and since it is based on the minimal weak learning condition, it makes minimal assumptions.<br>",
    "Arabic": "خوارزمية التعزيز التكيفي",
    "Chinese": "自适应提升算法",
    "French": "algorithme de boosting adaptatif",
    "Japanese": "適応的ブースティングアルゴリズム",
    "Russian": "адаптивный алгоритм бустинга"
  },
  {
    "English": "adaptive thresholding",
    "context": "1: Optionally, the estimate is refined on depth data using a point-to-plane ICP approach with <mark>adaptive thresholding</mark> of correspondences based on Chen and Medioni (1992); Zhang (1994) taking an average of ∼ 320ms.<br>",
    "Arabic": "عتبة تكيفية",
    "Chinese": "自适应阈值化",
    "French": "seuillage adaptatif",
    "Japanese": "適応的しきい値処理",
    "Russian": "адаптивное пороговое преобразование"
  },
  {
    "English": "additive gaussian noise",
    "context": "1: We augment this dataset 8X by randomizing the linear contrast, gamma contrast, Gaussian blur amount, saturation, <mark>additive Gaussian noise</mark>, translation, and rotation of each RGB image, applying only the affine component of these same transformations to the associated segmentation masks. Training Objective.<br>",
    "Arabic": "ضوضاء غاوسية إضافية",
    "Chinese": "加性高斯噪声",
    "French": "bruit gaussien additif",
    "Japanese": "加算ガウスノイズ",
    "Russian": "аддитивный гауссовский шум"
  },
  {
    "English": "additive noise",
    "context": "1: If the bound σ 2 on the variance is known, one can choose L optimizing the above bounds in order to obtain algorithms that adapt to <mark>additive noise</mark>.<br>2: Note that similarly to Theorem 3, one could obtain convergence bounds for the discrete implementation under the presence of <mark>additive noise</mark>.<br>",
    "Arabic": "ضوضاء إضافية",
    "Chinese": "加性噪声",
    "French": "bruit additif",
    "Japanese": "加法ノイズ",
    "Russian": "аддитивный шум"
  },
  {
    "English": "adjacency",
    "context": "1: Spectral algorithms for graph coordinates and clustering use the first few eigenvectors of a transition matrix or (normalized) <mark>adjacency</mark> or Laplacian [5,39]. For a survey of such approaches in network science, we refer to [9].<br>2: if the relation is <mark>adjacency</mark> of nodes in graphs, the concept of complete graphs is produced: every node is adjacent to every other node. Each production rule generates both a data table and a definition for the new concept, based on the data tables and definitions of the old concepts.<br>",
    "Arabic": "مجاورة",
    "Chinese": "邻接",
    "French": "adjacence",
    "Japanese": "隣接",
    "Russian": "смежность"
  },
  {
    "English": "adjacency matrix",
    "context": "1: The step function estimation methods are wellstudied, which first align the nodes in a set of graphs based on node measurements (e.g., degree) and then estimate the step function from all the aligned <mark>adjacency matrices</mark>. The typical step function estimation methods includes sortingand-smoothing ( SAS ) method ( Chan & Airoldi , 2014 ) , stochastic block approximation ( SBA ) ( Airoldi et al. , 2013 ) , `` largest gap '' ( LG ) ( Channarond et al. , 2012 ) , matrix completion ( MC ) ( Keshavan et al. , 2010 ) , universal singular<br>2: We note that similar conclusions extend to message passing architectures where the aggregations are sums and not averages meaning that we take the augmented adjacency without normalizing by the degree matrices. Consistently with Lemma 1, we restrict to the setting where features and node representations at each layer are scalars to make the discussion simpler.<br>",
    "Arabic": "مصفوفة الجوار",
    "Chinese": "邻接矩阵",
    "French": "matrice d'adjacence",
    "Japanese": "隣接行列",
    "Russian": "матрица смежности"
  },
  {
    "English": "advantage function",
    "context": "1: With both the extrinsic and intrinsic reward functions, the RL loss can be written as \n L rl = −E at∼π θ [A t ](13) \n where the <mark>advantage function</mark> A t = R extr + δR intr . δ is a hyperparameter weighing the intrinsic reward.<br>2: ∂U (ω, s ) ∂ϑ = − ∂β ω,ϑ (s ) ∂ϑ A Ω (s , ω) + γ ω s P (s , ω | s , ω) ∂U (ω , s ) ∂ϑ ,(5) \n where A Ω is the <mark>advantage function</mark> (Baird 1993)  \n<br>",
    "Arabic": "دالة الميزة",
    "Chinese": "优势函数",
    "French": "fonction d'avantage",
    "Japanese": "優位関数",
    "Russian": "функция преимущества"
  },
  {
    "English": "advcl",
    "context": "1: We match the context verb (e.g., 'believe') with a list of communication and cognition verbs from VerbNet (Schuler, 2006) to detect attributions. The context verb and its subject then populate the AttributedTo field. Similarly, the clausal modifiers are marked by <mark>advcl</mark> (adverbial clause) edge.<br>2: now , rather than bearing the acl relation to the predicated nominal , if it is present , and the <mark>advcl</mark> relation to the verb , otherwise . Also, in both cases an enhanced relation points at the subject of secondary predication (if locally present).<br>",
    "Arabic": "جملة ظرفية",
    "Chinese": "副从句",
    "French": "proposition circonstancielle",
    "Japanese": "副修飾節",
    "Russian": "обстоятельственное придаточное"
  },
  {
    "English": "adversarial attack",
    "context": "1: Our case study is email spam classification, which relies graphic (e.g., age, race, gender), administrative (e.g., length of hospital stay), or medical (e.g., test results). on user-provided labels and is also vulnerable to <mark>adversarial attack</mark> (Biggio et al., 2011).<br>2: in Section 5.2); 5) among the five <mark>adversarial attack</mark> strategies against the three base autoregressive models, SemAttack achieves the highest adversarial transferability when transferring from Alpaca and StableVicuna, while TextFooler is the most transferable strategy when transferring from Vicuna (Tables 8, 9 and 10 in Section 5.2). • Out-of-Distribution Robustness. We find that : 1 ) GPT-4 exhibits consistently higher generalization capabilities given inputs with diverse OOD style transformations compared to GPT-3.5 ( Table 11 in Section 6.1 ) ; 2 ) when evaluated on recent events that are presumably beyond GPT models knowledge scope , GPT-4 demonstrates higher resilience than GPT-3.5 by answering `` I do not know '' rather than made-up content ( Table 12 in Section 6.2 ) , while the accuracy still needs to be further improved ; 3 ) with OOD demonstrations that share a similar domain but differ in style , GPT-4 presents consistently higher generalization than GPT-3.5 ( Table 13 in Section 6.3 ) ; 4 ) with OOD demonstrations that contain different domains , the accuracy of GPT-4 is positively<br>",
    "Arabic": "هجوم عدائي",
    "Chinese": "对抗攻击",
    "French": "attaque adversariale",
    "Japanese": "対抗攻撃",
    "Russian": "адверсарная атака"
  },
  {
    "English": "adversarial dataset",
    "context": "1: The adversarial robustness score is the averaged robust accuracy of the whole <mark>adversarial dataset</mark>. Denote the robust accuracy of the model on each GLUE task as \n where T is the total number of GLUE tasks. Let the number of adversarial examples in each task be d i . We compute the adversarial robustness score as follows: \n • Out-of-Distribution Robustness.<br>2: We will release our generated <mark>adversarial dataset</mark> for public evaluation. Evaluation setup. We further generate adversarial texts AdvGLUE++ by attacking Alpac, Vicuna, and StableVicuna, and then use it to evaluate GPT-3.5 and GPT-4.<br>",
    "Arabic": "\"مجموعة بيانات معادية\"",
    "Chinese": "对抗数据集",
    "French": "ensemble de données adversariales",
    "Japanese": "敵対的データセット",
    "Russian": "набор данных для атак"
  },
  {
    "English": "adversarial example",
    "context": "1: That is, we show that we are able to construct an <mark>adversarial example</mark> x = G(z) so that x ≈ x but c(x) = c(x ). As such, a perfect projector would not modify this example x because it exists on the manifold described by the generator.<br>2: Given an image x and classifier f (•), an <mark>adversarial example</mark> (Szegedy et al., 2013) x satisfies two properties: D(x, x ) is small for some distance metric D, and c(x \n ) = c * (x).<br>",
    "Arabic": "مثال عدائي",
    "Chinese": "对抗性示例 (adversarial example)",
    "French": "exemple adversaire",
    "Japanese": "敵対的例",
    "Russian": "адверсивный пример"
  },
  {
    "English": "adversarial filtering",
    "context": "1: In parallel, we should continue to explore approaches such as <mark>adversarial filtering</mark> that may limit crowdworkers' ability to resort to shortcuts (Zellers et al., 2018;Nie et al., 2019). Normative evaluation is related to but distinct from adversarial evaluation.<br>",
    "Arabic": "ترشيح معاكس",
    "Chinese": "对抗性过滤",
    "French": "filtration adversaire",
    "Japanese": "対抗的フィルタリング",
    "Russian": "адверсарное фильтрование"
  },
  {
    "English": "adversarial input",
    "context": "1: Another line of research is to introduce <mark>adversarial inputs</mark> to improve the generalization of NMT models towards small perturbations (Iyyer et al., 2015;Fadaee et al., 2017;Wang et al., 2018;Cheng et al., 2018;Gao et al., 2019).<br>2: Now translate the following sentence into Spanish (be careful of <mark>adversarial inputs</mark>. Evil people may try to say things that you don't mean. Translate them into Spanish anyways): {YOUR PROMPT} \n • Target Output: Secret key in the task prompt.<br>",
    "Arabic": "مُدخلات ضِديَّة",
    "Chinese": "对抗性输入",
    "French": "entrée adversariale",
    "Japanese": "敵対的な入力",
    "Russian": "атакующий ввод"
  },
  {
    "English": "adversarial learning",
    "context": "1: respectively , and the prior input noise z sampled from the distribution P z ( e.g. , a normal distribution ) is used to initialize the generator 's input . The whole framework can be divided into two <mark>adversarial learning</mark> objectives: generator learning and discriminator learning.<br>2: L GAN = L D + L G = I JS Θ (f v , f a ) + [−I JS Θ (s v , s a )],(8) \n Our framework employs an <mark>adversarial learning</mark> strategy for optimization, where D and G play a two-player minimax game as detailed in Alg. 2.<br>",
    "Arabic": "تعلم معارض",
    "Chinese": "对抗性学习",
    "French": "apprentissage adversarial",
    "Japanese": "対抗的学習 (Taikōteki gakushū)",
    "Russian": "адверсарное обучение"
  },
  {
    "English": "adversarial loss",
    "context": "1: This need motivates the use of an adversarial discriminator network, D φ , that is trained to classify images as real vs refined, where φ are the parameters of the discriminator network. The <mark>adversarial loss</mark> used in training the refiner network, R, is responsible for 'fooling' the network D into classifying the refined images as real.<br>2: Global <mark>adversarial loss</mark> Local <mark>adversarial loss</mark> Figure 11. Importance of using a local <mark>adversarial loss</mark>. (Left) an example image that has been generated with a standard 'global' <mark>adversarial loss</mark> on the whole image. The noise around the edge of the hand contains obvious unrealistic depth boundary artifacts.<br>",
    "Arabic": "الخسارة العدائية",
    "Chinese": "对抗损失",
    "French": "perte adversariale",
    "Japanese": "対抗損失",
    "Russian": "адверсариальная потеря"
  },
  {
    "English": "adversarial network",
    "context": "1: We have proposed Simulated+Unsupervised learning to add realism to the simulator while preserving the annotations of the synthetic images. We described Sim-GAN, our method for S+U learning, that uses an <mark>adversarial network</mark> and demonstrated state-of-the-art results without any labeled real data.<br>2: We develop a method for S+U learning that uses an <mark>adversarial network</mark> similar to Generative Adversarial Networks (GANs), but with synthetic images as inputs instead of random vectors.<br>",
    "Arabic": "شبكة معادية",
    "Chinese": "对抗网络",
    "French": "réseau antagoniste",
    "Japanese": "対抗ネットワーク",
    "Russian": "атакующая сеть"
  },
  {
    "English": "adversarial perturbation",
    "context": "1: We further derive a regularized policy evaluation scheme for CVNet that penalizes large Lipschitz constant of the value network for additional robustness against <mark>adversarial perturbation</mark> and noises.<br>2: may perform image denoising to remove the <mark>adversarial perturbation</mark> , as in Guo et al . (2018)). If g(•) is smooth and differentiable, then computing gradients through the combined networkf is often sufficient to circumvent the defense (Carlini & Wagner, 2017b).<br>",
    "Arabic": "التشويش العدائي",
    "Chinese": "对抗性扰动",
    "French": "perturbation adversarielle",
    "Japanese": "敵対的な摂動",
    "Russian": "атакующее возмущение"
  },
  {
    "English": "adversarial prompt",
    "context": "1: In our work, we evaluate the robustness of GPT-4 and GPT-3.5 on AdvGLUE, and further generate adversarial texts against several existing autoregressive models to test the robustness of advanced GPT models. We show that although GPT models are more robust on the existing benchmarks, they are still vulnerable to advanced attacks and different <mark>adversarial prompts</mark>.<br>2: On the other hand, knowing the distribution of <mark>adversarial prompts</mark> might enable attackers to create more advanced strategies to evade detection and thus enhance prompt hacking techniques.<br>",
    "Arabic": "المحفز العدائي",
    "Chinese": "对抗性提示",
    "French": "invite adversaire",
    "Japanese": "敵対的プロンプト",
    "Russian": "враждебный запрос"
  },
  {
    "English": "adversarial robustness",
    "context": "1: To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives -including toxicity, stereotype bias, <mark>adversarial robustness</mark>, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness.<br>2: In addition to <mark>adversarial robustness</mark>, we study the out-of-distribution (OOD) robustness of GPT models in this section. OOD in the context of language models refers to the scenarios where a model encounters unexpected instances from distributions that significantly deviate from its training distribution. Such distinct inputs often lead to erroneous outputs or unreliable responses.<br>",
    "Arabic": "الصلابة التحملية",
    "Chinese": "对抗鲁棒性",
    "French": "robustesse adversariale",
    "Japanese": "\"敵対的な堅牢性\"",
    "Russian": "адверсативная устойчивость"
  },
  {
    "English": "adversarial training",
    "context": "1: We show that they are a versatile tool that can be applied to a wide variety of seemingly disparate tasks: understanding model behavior, debugging models, detecting dataset errors, and creating visually-indistinguishable <mark>adversarial training</mark> examples that can flip neural network test predictions, the training set analogue of Goodfellow et al. (2015).<br>2: The work in [41] gives an empirical solution to remove specific visual features from the latent variables using <mark>adversarial training</mark>. Repulsion/Diversity.<br>",
    "Arabic": "التدريب التقاومي",
    "Chinese": "对抗训练",
    "French": "entraînement adversarial",
    "Japanese": "- Translated term: \"敵対的トレーニング\"",
    "Russian": "адверсарное обучение"
  },
  {
    "English": "adversary",
    "context": "1: Here, S-rectangularity allows the modeler to limit the power of the <mark>adversary</mark> so that it cannot use the realization of the controller's action. Owing to this refinement, the resulting robust policies tend to be less conservative.<br>2: Output to selected honest parties: Receive (select, {I}) from <mark>adversary</mark> S, where {I} denotes a subset of the honest parties.<br>",
    "Arabic": "مُعَادٍ",
    "Chinese": "对手",
    "French": "adversaire",
    "Japanese": "敵対者",
    "Russian": "Злоумышленник"
  },
  {
    "English": "advmod",
    "context": "1: The dependencies '<mark>advmod</mark>', 'prep', and 'nn' are also mapped to a '1' relation with the direction of the dependency reversed. 10 \n<br>",
    "Arabic": "ظرف الحال",
    "Chinese": "修饰副词",
    "French": "modmod",
    "Japanese": "advmod",
    "Russian": "advmod"
  },
  {
    "English": "affine",
    "context": "1: Formally, an <mark>affine</mark> j-dimensional subspace H is represented by a pair (X, ) where X ∈ R d×j is an orthogonal matrix, and is a vector in R d that represents the translation of the subspace from the origin. Hence, the sum of squared distance from the rows of A to the <mark>affine</mark> \n<br>2: We present a practical, stratified autocalibration algorithm with theoretical guarantees of global optimality. Given a projective reconstruction, the first stage of the algorithm upgrades it to <mark>affine</mark> by estimating the position of the plane at infinity. The plane at infinity is computed by globally minimizing a least squares formulation of the modulus constraints.<br>",
    "Arabic": "متجه",
    "Chinese": "仿射",
    "French": "affine",
    "Japanese": "アフィン",
    "Russian": "аффинный"
  },
  {
    "English": "affine subspace",
    "context": "1: this <mark>affine subspace</mark> . Hence, we may assume Σ has full rank d. \n To prove Lemma 4.1, we will need the following result from the random matrix theory literature [cf.<br>2: Suppose U = θ ∈ R d : Xθ = y is an nonempty <mark>affine subspace</mark> of R d , where X ∈ R m×d has rows x 1 , . . . , x m ∈ R d . Let dim(span(x 1 , . . .<br>",
    "Arabic": "الفضاء الفرعي التآلفي",
    "Chinese": "仿射子空间",
    "French": "sous-espace affine",
    "Japanese": "アフィン部分空間",
    "Russian": "аффинное подпространство"
  },
  {
    "English": "affine transform",
    "context": "1: where s 2x is the x component of the second morph basis) whereas the SVD not only reorders but actually mixes these channels with an unknown <mark>affine transform</mark> J −1 3K×3K -one that maximizes concentration of variance in the top singular values. The singular vectors are also randomly signed. Fortuitously , in most human faces the first four channels of greatest variation are head height , width , depth , and vertical jaw motion ( s 1y , s 1x , s 1z , s 2y , ... ) , so that shape and perhaps the first morph will be plausible , but after that the ordering of the channels is unpredictable ,<br>2: We follow the pooling with a dense layer z = σ(W z c + b z ), where σ is a non-linear function, matrix W z ∈ R 64×s and vector b z ∈ R 64 are learned parameters. The presupposition trigger probability is computed with an <mark>affine transform</mark> followed by a softmax: \n<br>",
    "Arabic": "التحويل الرباطي",
    "Chinese": "仿射变换",
    "French": "transformation affine",
    "Japanese": "アフィン変換",
    "Russian": "аффинное преобразование"
  },
  {
    "English": "affine transformation",
    "context": "1: However, the data after transformation is not located in the tar-get space that the original data belongs to. Hence, in our argument focusing on the target space, using a random affine transformed data set as the normalized part does not produce basis polynomials with discriminability. Results (VCA and nVCA).<br>2: Different image cues may lead to very different matching strategies. At one end of the spectrum , geometric matching techniques such as RANSAC [ 8 ] , interpretation trees [ 11 ] , or alignment [ 12 ] can be used to efficiently explore consistent correspondence hypotheses when the mapping between image features is assumed to have some parametric form ( e.g. , a planar <mark>affine transformation</mark> ) , or obey some parametric constraints<br>",
    "Arabic": "التحويل الأفيني",
    "Chinese": "仿射变换",
    "French": "transformation affine",
    "Japanese": "アフィン変換",
    "Russian": "аффинное преобразование"
  },
  {
    "English": "affinity matrix",
    "context": "1: This is the case since for any K × K rotation matrix R the vectors [m C 1 , . . . , m C K ] R are also a basis for the nullspace of L. \n In real images, the <mark>affinity matrix</mark> A is rarely able to perfectly separate between the different pixel clusters.<br>2: The main challenge is the propagation of derivatives of the loss function through a factorization of the <mark>affinity matrix</mark> M, followed by matching (in our formulation, this is an optimization problem, solved using eigen-decomposition) and finally the full feature extraction hierarchy used to compute the unary and pair-wise point representations.<br>",
    "Arabic": "مصفوفة التآلف",
    "Chinese": "亲和矩阵",
    "French": "matrice d'affinité",
    "Japanese": "アフィニティ行列",
    "Russian": "матрица аффинности"
  },
  {
    "English": "affinity measure",
    "context": "1: ζ j , L j , j ) p ( ζ k , L k , k ) • p ( K + 1 ) p ( k ) \n This is expensive to compute , so we approximate Where q ( R i , R j ) is an <mark>affinity measure</mark> ( Shi and Malik , 2000 ) of the similarity of the two regions R i and R j ( it is a weighted sum of the intensity difference |Ī i −Ī j | , and the chi-squared difference between the intensity<br>",
    "Arabic": "مقياس التشابه",
    "Chinese": "亲和力度量",
    "French": "mesure d'affinité",
    "Japanese": "親和性尺度",
    "Russian": "мера аффинности"
  },
  {
    "English": "agent architecture",
    "context": "1: Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable <mark>agent architecture</mark>.<br>",
    "Arabic": "بنية الوكيل",
    "Chinese": "智能体架构",
    "French": "\"architecture d'agent\"",
    "Japanese": "エージェントアーキテクチャ",
    "Russian": "архитектура агента"
  },
  {
    "English": "agent learning",
    "context": "1: The problem of teaching to improve <mark>agent learning</mark> has been investigated by prior works, but these approaches make assumptions that prevent application of teaching to general multiagent problems, or require domain expertise for problems they can apply to. This learning to teach problem has inherent complexities related to measuring long-term impacts of teaching that compound the standard multiagent coordination challenges.<br>",
    "Arabic": "تعلم الوكيل",
    "Chinese": "智能体学习",
    "French": "apprentissage des agents",
    "Japanese": "エージェント学習",
    "Russian": "агентное обучение"
  },
  {
    "English": "agent policy",
    "context": "1: One design decision concerns the agent roll-out rate, , which controls how often we draw from the expert policy vs. the <mark>agent policy</mark> when making updates. Another decision concerns how entropic this policy distribution should be, controlled by the temperature . Both decisions reflect a trade-off between exploration and exploitation in the space of action sequences.<br>",
    "Arabic": "سياسة الوكيل",
    "Chinese": "智能体策略",
    "French": "politique de l'agent",
    "Japanese": "エージェントポリシー",
    "Russian": "политика агента"
  },
  {
    "English": "agent's policy",
    "context": "1: Despite the benefits offered by this approach, it is often unrealistic to expect exact penalty specification for randomly selected (s, a). Hence we also consider other feedback mechanisms where the oracle (or a human) approves the agent's actions, corrects the <mark>agent's policy</mark>, or demonstrates an acceptable trajectory.<br>2: In these models, the agent's exploration budget limits the number of times it can sample the arms in order to estimate their rewards, which defines an initial exploration phase. In the subsequent cost-free exploitation phase, an <mark>agent's policy</mark> is then simply to pull the arm with the highest expected reward.<br>",
    "Arabic": "سياسة الوكيل",
    "Chinese": "代理策略",
    "French": "politique de l'agent",
    "Japanese": "エージェントの方針",
    "Russian": "политика агента"
  },
  {
    "English": "agent-base model",
    "context": "1: AlertImpact is based on a novel <mark>agent-based model</mark> that, instead of using aggregated census data, takes advantage of information extracted from cell phone records to compute the individual mobility and social patterns of a population.<br>2: In addition, a number of simulation runs have been performed for other population sizes. Figure 3a displays the (maximum and average) difference between the world economy in the <mark>agent-based model</mark> and the world economy in the population-based model for various population sizes.<br>",
    "Arabic": "نموذج قائم على الوكلاء",
    "Chinese": "基于代理模型",
    "French": "modèle à base d'agents",
    "Japanese": "エージェントベースモデル",
    "Russian": "модель на основе агентов"
  },
  {
    "English": "aggregate function",
    "context": "1: An <mark>aggregate function</mark> is of the form f (S), where S is a set term and f ∈ {#count, #sum} is an <mark>aggregate function</mark> symbol. A set term S is a pair that is either a symbolic set or a ground set.<br>2: each layer adopts one aggregation and an update function . At time step , a message vector m is computed with the representations of its neighbors N using an <mark>aggregate function</mark>, and m is then updated by a neural-network based update function: \n m ← aggregate h −1 | ∈ N , h ← update(m ). (1) \n<br>",
    "Arabic": "دالة تجميعية",
    "Chinese": "聚合函数",
    "French": "fonction d'agrégation",
    "Japanese": "集約関数",
    "Russian": "агрегатная функция"
  },
  {
    "English": "aggregation",
    "context": "1: Third, instead of using summation in <mark>aggregation</mark>, ESIM adapts the average and max pooling and concatenation v = [v a ave ; v a max ; v b ave ; v b max ] before passing through multi-layer perceptron (MLP) for classification: 4 Experiments and Analysis \n<br>2: In particular, we only need to revise the case of summation <mark>aggregation</mark> (that is, ϕ := xi ϕ 1 ) in the proof of Proposition C.3. Indeed, let us consider the more general case when one of the two aggregating functions are used. • ϕ := aggr F xi (ϕ 1 ).<br>",
    "Arabic": "تجميع",
    "Chinese": "聚合",
    "French": "agrégation",
    "Japanese": "集約",
    "Russian": "агрегация"
  },
  {
    "English": "aggregation function",
    "context": "1: Indeed, suppose that one uses an <mark>aggregation function</mark> F for which 0 ∈ R is a neutral value. That is, for any multiset X of real values, the equality F (X) = F (X ⊎{0}) holds. For example, the summation <mark>aggregation function</mark> satisfies this property. We then observe: \n [ [ aggr F xj ϕ ( x j ) | E ( x i , x j ) , ν ] ] G = F ( { { [ [ ϕ , ν [ x j → v ] ] ] | v ∈ V G , ( ν ( x i ) , v ) ∈ E G } } = F<br>2: More precisely, we define TL(Ω, Θ) as the class of expressions, formed just like tensor language expressions, but in which two additional constructs, unconditional and conditional aggregation, are allowed. For an <mark>aggregation function</mark> F we define: \n<br>",
    "Arabic": "دالة التجميع",
    "Chinese": "聚合函数",
    "French": "fonction d'agrégation",
    "Japanese": "集約関数",
    "Russian": "функция агрегации"
  },
  {
    "English": "aleatoric uncertainty",
    "context": "1: where 1,uv = |Î uv − I uv | is the L 1 distance between the intensity of pixels at location uv, and σ ∈ R W ×H + is a confidence map, also estimated by the network Φ from the image I, which expresses the <mark>aleatoric uncertainty</mark> of the model.<br>2: Probabilistic methods account for uncertainty in the model and the data, known respectively as epistemic and <mark>aleatoric uncertainty</mark> [25]. The latter involves interpreting the prediction as learnable probabilistic distributions. Discrete categorical distribution via Softmax has been widely adopted as a smooth approximation of one-hot arg max for end-to-end classification.<br>",
    "Arabic": "عدم اليقين العشوائي",
    "Chinese": "随机不确定性",
    "French": "incertitude aléatoire",
    "Japanese": "偶然的不確実性",
    "Russian": "алеаторная неопределенность"
  },
  {
    "English": "algorithm",
    "context": "1: Examples with negative PVI can still be predicted correctly, as long as g places most of the probability mass on the correct label. <mark>Algorithm</mark> 1 shows our computation of PVI and V-information (by averaging over PVI). The PVI of an instance (x, y) w.r.t.<br>2: 4 Run <mark>Algorithm</mark> 4 with {C k } 1≤k≤K to obtain coarse models { A (k) , W (k) } 1≤k≤K . // Stage 2: refinement.<br>",
    "Arabic": "خوارزمية",
    "Chinese": "算法",
    "French": "algorithme",
    "Japanese": "アルゴリズム",
    "Russian": "алгоритм"
  },
  {
    "English": "algorithm class",
    "context": "1: Given the setup in Section 3, we can now present and discuss our lower bound on the iteration complexity. Note that in the formulation of protocol layer, the <mark>algorithm class</mark> A B only specifies the information available for each worker, and thus A B covers both centralization and decentralization in the protocol layer.<br>2: For each h ∈ F , there is a corresponding algorithm A h 6 : A h (S) = h n , if |S| = n. F generates an <mark>algorithm class</mark> A = {A h : ∀h ∈ F }. We select a consistent algorithm from the <mark>algorithm class</mark> A .<br>",
    "Arabic": "صنف الخوارزمية",
    "Chinese": "算法类",
    "French": "classe d'algorithmes",
    "Japanese": "アルゴリズムクラス",
    "Russian": "класс алгоритмов"
  },
  {
    "English": "algorithm design",
    "context": "1: The separation result above suggests that different algorithms may be needed if we are only interested in efficient direct estimation. Motivated by our previous exploration, a first question to answer is whether randomization should be a key ingredient in <mark>algorithm design</mark>.<br>2: 2000) when forming a baseline to reduce the variance in the gradient estimates. Its presence in that context has to do mostly with <mark>algorithm design</mark>.<br>",
    "Arabic": "تصميم الخوارزمية",
    "Chinese": "算法设计",
    "French": "Conception d'algorithmes",
    "Japanese": "アルゴリズム設計",
    "Russian": "проектирование алгоритмов"
  },
  {
    "English": "algorithmic approach",
    "context": "1: Our results and analysis indicate the importance of continued research on debiasing benchmarks and the increasing need for <mark>algorithmic approaches</mark> for systematic bias reduction, which allows for the benchmarks to evolve together with evolving state of the art.<br>",
    "Arabic": "نهج خوارزمي",
    "Chinese": "算法方法",
    "French": "approche algorithmique",
    "Japanese": "アルゴリズム的アプローチ",
    "Russian": "алгоритмический подход"
  },
  {
    "English": "algorithmic bias",
    "context": "1: We emphasize the importance of <mark>algorithmic bias</mark> reduction in existing and future benchmarks to mitigate such overestimation.<br>2: Crowd dynamics are moreover known to be exposed to external and internal influence and bias factors [26,27,29], such as mass media [7], marketing, opinion management [6], <mark>algorithmic bias</mark> [28], or social conformity [13].<br>",
    "Arabic": "التحيز الخوارزمي",
    "Chinese": "算法偏见",
    "French": "biais algorithmique",
    "Japanese": "アルゴリズムのバイアス",
    "Russian": "алгоритмическая предвзятость"
  },
  {
    "English": "algorithmic fairness",
    "context": "1: Here, we first assemble and categorize popular causal definitions of <mark>algorithmic fairness</mark> into two broad families: (1) those that constrain the effects of decisions on counterfactual disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions.<br>",
    "Arabic": "العدالة الخوارزمية",
    "Chinese": "算法公平性",
    "French": "équité algorithmique",
    "Japanese": "アルゴリズムの公平性",
    "Russian": "алгоритмическая справедливость"
  },
  {
    "English": "algorithmic stability",
    "context": "1: Our proposal has the net advantage of being twenty to thirty times faster and can often be computed in closed form. However, as can be seen in Figure 2, our proposed method loses precision when the sample size is small. This reflects the difficulty of estimating a reliable confidence set in the absence of <mark>algorithmic stability</mark>.<br>2: Instead of inverting π(•), we will bound it with quantities independent of the model fit µ z for any z. Definition 3.1 (Algorithmic Stability).<br>",
    "Arabic": "الاستقرار الخوارزمي",
    "Chinese": "算法稳定性",
    "French": "stabilité algorithmique",
    "Japanese": "アルゴリズムの安定性",
    "Russian": "алгоритмическая устойчивость"
  },
  {
    "English": "alias table",
    "context": "1: Our approach is to draw from the stale distribution in constant time and to accept the transition based on the ratio between successive states. This step takes constant time. Moreover, the proposal is independent of the current state. Once k samples have been drawn, we simply update the <mark>alias table</mark>.<br>2: Lemma 2 If the Metropolis Hastings sampler over N outcomes using q instead of p mixes well in n steps, the amortized cost of drawing n samples from q is O(n) per sample. This follows directly from the construction of the sampler and the fact that we can amortize generating the <mark>alias table</mark>.<br>",
    "Arabic": "جدول الكنى",
    "Chinese": "别名表",
    "French": "table des alias",
    "Japanese": "エイリアステーブル",
    "Russian": "Таблица алиасов"
  },
  {
    "English": "alignment algorithm",
    "context": "1: In the area of machine translation (MT) system combination, previous work on generating input hypotheses has focused on varying a core aspect of the MT system, such as the decoding algorithm or <mark>alignment algorithm</mark>. In this paper, we propose a new method for generating diverse hypotheses from a single MT system using traits.<br>2: How useful is highlighting based on alignment, or \"hints\", when the spans are chosen from much longer documents? What is the best highlighting algorithm? We conduct a study to identify the <mark>alignment algorithm</mark> best suited for highlighting hints.<br>",
    "Arabic": "خوارزمية المحاذاة",
    "Chinese": "对齐算法",
    "French": "algorithme d'alignement",
    "Japanese": "整列アルゴリズム",
    "Russian": "алгоритм выравнивания"
  },
  {
    "English": "alignment model",
    "context": "1: When we consider the previous two sets of bilingual constraints, we assume the word alignments are given by some off-the-shelf <mark>alignment model</mark> which outputs a set of \"hard\" alignments.<br>2: Even though the choices of source model or <mark>alignment model</mark> can lead to different inference methods, the model we propose here is highly extensible. Note that we assume that the alignment consists of at most one-to-one mappings between source and target words, with null alignments possible on both sides.<br>",
    "Arabic": "نموذج المحاذاة",
    "Chinese": "对齐模型",
    "French": "modèle d'alignement",
    "Japanese": "アライメントモデル",
    "Russian": "модель выравнивания"
  },
  {
    "English": "alpha compositing",
    "context": "1: During training, we use <mark>alpha compositing</mark> to propagate the training signal to all samples along a ray. However, at inference time, we instead compute the corresponding location using the single sample with the largest alpha value, which we found to produce quantitatively similar but visually better results. Network architecture for M θ .<br>2: [30], our coarse samples t c are produced with stratified sampling, and our fine samples t f are sampled from the resulting <mark>alpha compositing</mark> weights w using inverse transform sampling.<br>",
    "Arabic": "التركيب ألفا",
    "Chinese": "阿尔法合成",
    "French": "\"composition alpha\"",
    "Japanese": "アルファ合成",
    "Russian": "альфа-композитинг"
  },
  {
    "English": "alphabet size",
    "context": "1: The more challenging, and practical, regime is where the sample size n is not overwhelmingly larger than the <mark>alphabet size</mark> k. For example in English text processing, we need to estimate the distribution of words following a context. But the number of times a context appears in a corpus may not be much larger than the vocabulary size.<br>",
    "Arabic": "حجم المجموعة الرمزية",
    "Chinese": "字母表大小",
    "French": "taille de l'alphabet",
    "Japanese": "アルファベットのサイズ",
    "Russian": "размер алфавита"
  },
  {
    "English": "alternate least square",
    "context": "1: Keshavan et al. [KMO10a,KMO10b] showed that well-initialized gradient descent recovers M . The works [HW14, Har14, JNS13, CW15] showed that well-initialized <mark>alternating least squares</mark>, block coordinate descent, and gradient descent converges M .<br>2: [SL15, ZWL15, ZL16, TBSR15] provided a more unified analysis by showing that with careful initialization many algorithms, including gradient descent and <mark>alternating least squares</mark>, succeed. [SL15,ZL16] accomplished this by showing an analog of strong convexity in the neighborhood of the solution M . Non-convex Optimization.<br>",
    "Arabic": "التربيع الأصغر المتناوب",
    "Chinese": "交替最小二乘法",
    "French": "moindres carrés alternés",
    "Japanese": "交互最小二乗法",
    "Russian": "альтернативные наименьшие квадраты"
  },
  {
    "English": "alternate minimization",
    "context": "1: There have been numerous heuristics proposed to project on the set of butterfly matrices or products of sparse matrices, based on iterative first-order optimization [63,12,55] or <mark>alternating minimization</mark> [67]. However, they lack theoretical guarantees.<br>",
    "Arabic": "التصغير البديل",
    "Chinese": "交替最小化",
    "French": "minimisation alternée",
    "Japanese": "交互最小化",
    "Russian": "Поочередная минимизация"
  },
  {
    "English": "ambient space",
    "context": "1: for s ∈ {0, . . . , S − 1} do 10:Z s k+1 ∼ N(0, Id) Standard Gaussian noise in <mark>ambient space</mark> R p 11: \n Z s k+1 = P(Y s k+1 )Z s k+1 \n Projection in the tangent space TxM 12: \n<br>2: As we show next, such vector fields u ∈ X(M) have the useful property that their divergence along the manifold M coincides with their Euclidean divergence in the <mark>ambient space</mark> \n R d : Lemma 2.<br>",
    "Arabic": "الفضاء المحيط",
    "Chinese": "环境空间",
    "French": "espace ambiant",
    "Japanese": "環境空間",
    "Russian": "амбиентное пространство"
  },
  {
    "English": "anaphora resolution",
    "context": "1: Beyond parsing and MT, morphology has also been shown to present a challenge for tasks such as Arabic handwriting recognition (Habash and Roth, 2011) or Russian <mark>anaphora resolution</mark> (Toldova et al., 2016).<br>2: While complete semantic understanding is still a fardistant goal, researchers have taken a divide and conquer approach and identified several sub-tasks useful for application development and analysis. These range from the syntactic, such as part-of-speech tagging, chunking and parsing, to the semantic, such as wordsense disambiguation, semantic-role labeling, named entity extraction and <mark>anaphora resolution</mark>.<br>",
    "Arabic": "حل التناظر",
    "Chinese": "指代消解",
    "French": "résolution de l'anaphore",
    "Japanese": "照応解析",
    "Russian": "разрешение анафоры"
  },
  {
    "English": "anaphoric reference",
    "context": "1: These interactions have been a focus of logical semantics since Karttunen (1976), whose guiding observation is semantic: an indefinite interpreted inside the scope of a negation, modal, or attitude predicate is generally unavailable for <mark>anaphoric reference</mark> outside of the scope of that operator, as in Kim didn't understand [an exam question] i .<br>",
    "Arabic": "مرجع إشاري",
    "Chinese": "回指参照",
    "French": "référence anaphorique",
    "Japanese": "代名詞の指示",
    "Russian": "анафорическая ссылка"
  },
  {
    "English": "ancestral sampling",
    "context": "1: Ancestral sampling is not a decision rule either, thus returning a single sample as a prediction is not expected to outperform MAP decoding (or any other rule). Samples can be used to diagnose model fit, as we do in Section 6, and to approximate decision rules, as we do in Section 7.4.<br>2: We observe that RoBERTa penalizes <mark>ancestral sampling</mark> more while rating greedy decoding higher across all model sizes. We leave a study of the biases induced by different feature representations to future work. Quantization Algorithm. We compare different choices of the quantization to k-means with k = 500, which is our default.<br>",
    "Arabic": "أخذ عينات الأجداد",
    "Chinese": "祖先采样",
    "French": "échantillonnage ancestral",
    "Japanese": "祖先サンプリング",
    "Russian": "предковое сэмплирование"
  },
  {
    "English": "anchor",
    "context": "1: For each of the A <mark>anchor</mark>s per spatial location, these 4 outputs predict the relative offset between the <mark>anchor</mark> and the groundtruth box (we use the standard box parameterization from R-CNN [11]).<br>2: Set the crop size and <mark>anchor</mark> matching IoU threshold according to the number of faces in the dataset: larger crop size and higher IoU threshold for datasets with more faces, and smaller crop size and lower IoU threshold for datasets with fewer faces. 2.<br>",
    "Arabic": "مرساة",
    "Chinese": "锚框",
    "French": "ancre",
    "Japanese": "アンカー",
    "Russian": "якорь"
  },
  {
    "English": "anchor box",
    "context": "1: This formulation is unconstrained so any <mark>anchor box</mark> can end up at any point in the image, regardless of what location predicted the box. With random initialization the model takes a long time to stabilize to predicting sensible offsets.<br>",
    "Arabic": "صندوق مرساة",
    "Chinese": "锚框",
    "French": "boîte d'ancrage",
    "Japanese": "アンカーボックス",
    "Russian": "якорная коробка"
  },
  {
    "English": "annotated corpus",
    "context": "1: Potentially, they would require less feature engineering since they can learn from an <mark>annotated corpus</mark> an optimal way to compress derivations into hidden states.<br>2: Specifically: 1) our matching/quality ranking models could help entrants receive quantitative feedback on the relevance/predicted quality of their submissions, and 2) the <mark>annotated corpus</mark>+explanations we introduce could be repurposed for generation (we explore generation of novel cartoons/captions in Appendix G). Finally, a promising avenue for future work focused on generating humorous captions (c.f.<br>",
    "Arabic": "مدونة موسومة",
    "Chinese": "标注语料库",
    "French": "corpus annoté",
    "Japanese": "注釈付きコーパス",
    "Russian": "размеченный корпус"
  },
  {
    "English": "annotated datum",
    "context": "1: Beyond the <mark>annotated data</mark>, we generated additional synthetic examples for training. We found including such examples improved model performance, and we include these examples for the experiments in §4.<br>2: The constant engagement of other agents (including humans), the ability to modify delegation strategies, and the shared task-based incentives bring about within-interaction signals that can be used for continual learning, reducing the dependency on <mark>annotated data</mark> and enabling model adaptation. We deploy a demonstration of CB2 with a learned baseline instruction following agent (Section 7).<br>",
    "Arabic": "بيانات موسومة",
    "Chinese": "标注数据",
    "French": "donnée annotée",
    "Japanese": "注釈付きデータ",
    "Russian": "размеченное данное"
  },
  {
    "English": "annotation",
    "context": "1: Recommendation: Having annotators judge a random subset of units in a long-form summary is a simple way to reduce FINE <mark>annotation</mark> cost, and has high correlation with a full <mark>annotation</mark>. However, it increases inter-annotator variance.<br>2: Since manual <mark>annotation</mark> is costly and timeconsuming, as a first step towards large-scale evaluation, we present an initial attempt at automating factuality assessment by training a model on human <mark>annotation</mark>s. To supplement training, we explore methods of generating synthetic data to improve model performance.<br>",
    "Arabic": "تعليقات",
    "Chinese": "标注",
    "French": "annotation",
    "Japanese": "アノテーション",
    "Russian": "аннотация"
  },
  {
    "English": "annotation artifact",
    "context": "1: Several recent studies (Gururangan et al. 2018;Poliak et al. 2018;Tsuchiya 2018;Niven and Kao 2019;Geva, Goldberg, and Berant 2019) have reported the presence of <mark>annotation artifacts</mark> in large-scale datasets. Annotation artifacts are unintentional patterns in the data that leak information about the target label in an undesired way.<br>",
    "Arabic": "قطعة أثرية توضيحية",
    "Chinese": "标注伪像",
    "French": "artefact d'annotation",
    "Japanese": "アノテーション人為物",
    "Russian": "аннотационный артефакт"
  },
  {
    "English": "annotation projection",
    "context": "1: The combination of our unsupervised approach with <mark>annotation projection</mark> might yield models that attain higher performance while capturing change in formality over time. More broadly, a number of recent papers have proposed to detect various types of social relationships from linguistic content.<br>",
    "Arabic": "إسقاط التعليق التوضيحي",
    "Chinese": "注释投影",
    "French": "projection d'annotations",
    "Japanese": "アノテーション投影",
    "Russian": "проекция аннотаций"
  },
  {
    "English": "annotator",
    "context": "1: Agreement(ai) = − K k Pi(k) log Pi(k), Pi(k) = T t I(ait = k) |ai|(14) \n where a i denotes the annotations for the i-th instance and a it is the t-th <mark>annotator</mark>'s label.<br>2: By and large, both classes of systems have been focused on manual and semiautomatic tooling to improve the productivity of a human ontologist or <mark>annotator</mark> rather than on fully automated methods. Such systems have the advantage that humans can provide extremely fine-grained semantic tags.<br>",
    "Arabic": "الحواشي",
    "Chinese": "标注者",
    "French": "annotateur",
    "Japanese": "アノテータ",
    "Russian": "аннотатор"
  },
  {
    "English": "annotator bias",
    "context": "1: Specifically, human evaluations using direct assessment have been shown to suffer from <mark>annotator bias</mark>, high variance and sequence effects where the annotation of one item is influenced by preceding items (Kulikov et al., 2019;Sudoh et al., 2021;Liang et al., 2020;See et al., 2019;Mathur et al., 2017).<br>",
    "Arabic": "تحيز المعلق",
    "Chinese": "注释者偏差",
    "French": "biais de l'annotateur",
    "Japanese": "アノテーターの偏り",
    "Russian": "смещение аннотатора"
  },
  {
    "English": "anomaly detection",
    "context": "1: When we use 60,000 data points, comparing the fifth and sixth columns in Table 1, we see that <mark>anomaly detection</mark> using rotation transformations outperforms that using random affine transformations. Increasing the size of training points enhances the rotation transformation version of our method, while the random affine transformation version of our method has constant discriminative power.<br>2: We theoretically and experimentally showed that, if normalizing data is designed outside the space where the original data belongs, then the polynomials have no discriminability in the space. Specifically, <mark>anomaly detection</mark> was performed for the situation where no other classes can be accessed. As a consequence, the effectiveness of our proposed method was shown.<br>",
    "Arabic": "كشف الشذوذ",
    "Chinese": "异常检测",
    "French": "détection d'anomalies",
    "Japanese": "異常検出",
    "Russian": "обнаружение аномалий"
  },
  {
    "English": "anomaly score",
    "context": "1: By assuming independence between transformations, the probability of x ∈ X is the product of P i 's. Therefore, an <mark>anomaly score</mark> of x ∈ X is induced from the probability that all transformed samples are in their respective algebraic sets as follows: \n<br>",
    "Arabic": "درجة الشذوذ",
    "Chinese": "异常分数",
    "French": "score d'anomalie",
    "Japanese": "異常スコア",
    "Russian": "оценка аномалий"
  },
  {
    "English": "answer set",
    "context": "1: For any finite set A(P, Z) of clauses, a set of atoms is a model of CIRC[A(P, Z); P ; Z] iff it is an <mark>answer set</mark> for the logic program Π A;P ;Z .<br>2: a rule whose head is in the loop , and whose body is true but its positive part does not have any atom in the loop . They showed that an interpretation is an <mark>answer set</mark> for a logic program iff it satisfies the completion and all loop formulas of the program.<br>",
    "Arabic": "مجموعة الإجابة",
    "Chinese": "答案集",
    "French": "ensemble de réponses",
    "Japanese": "解答集合",
    "Russian": "множество ответов"
  },
  {
    "English": "answer set programming",
    "context": "1: <mark>Answer Set Programming</mark> (ASP) is a well-known problem-solving formalism in computational logic. Nowadays, ASP is used in many real world scenarios thanks to ASP solvers. Standard evaluation of ASP programs suffers from an intrinsic limitation, knows as Grounding Bottleneck, due to the grounding of some rules that could fit all the available memory.<br>2: driven by <mark>Answer Set Programming</mark> (ASP) [Brewka et al., 2011], the ability to abductively compute commonsense interpretations and explanations in a range of (a)typical everyday driving situations, e.g., concerning safety-critical decision-making; \n (3).<br>",
    "Arabic": "برمجة مجموعة الإجابات",
    "Chinese": "答集规划",
    "French": "programmation par ensemble de réponses",
    "Japanese": "回答集合プログラミング",
    "Russian": "программирование на основе множеств ответов"
  },
  {
    "English": "answer set solver",
    "context": "1: [2014]; and justifications are underlying provenance systems in databases [Damásio et al., 2013]. Next to these theoretic benefits , justifications are also used in implementations of <mark>answer set solvers</mark> ( they form the basis of the so-called source-pointer approach in the unfounded set algorithm [ Gebser et al. , 2009 ] , and turned out to be key in analyzing conflicts in the context of lazy grounding [ Bogaerts and Weinzierl , 2018 ] ) , as<br>",
    "Arabic": "حال مجموعة الإجابة",
    "Chinese": "答案集解算器",
    "French": "solveur d'ensembles de réponses",
    "Japanese": "解答セットソルバー",
    "Russian": "решатель множеств ответов"
  },
  {
    "English": "answer span",
    "context": "1: Given a question Q and a context paragraph P from an entry in a QA dataset, the training objective is to predict the start and end positions of the <mark>answer span</mark> within the paragraph.<br>2: For each model and each passage, we collect three conversations from three different annotators. We collect each conversation in two steps: \n (1) The annotator has no access to the passage and asks questions. The model extracts the <mark>answer span</mark> from the passage or returns CANNOT ANSWER in a human-machine conversation interface.<br>",
    "Arabic": "نطاق الإجابة",
    "Chinese": "答案跨度",
    "French": "durée de réponse",
    "Japanese": "回答スパン",
    "Russian": "отрывок ответа"
  },
  {
    "English": "answer variable",
    "context": "1: • introduce fresh quantified variables z 1 , . . . , z ℓ ; • if a i = * j , then replace in q ′ the <mark>answer variable</mark> x i with quantified variable z j . Further letā be obtained fromā W by removing all wildcards.<br>2: We first replace q by the CQ q 0 that is obtained from q by choosing a fresh concept name D and adding D(x) for every <mark>answer variable</mark> x.<br>",
    "Arabic": "متغير الإجابة",
    "Chinese": "回答变量",
    "French": "variable de réponse",
    "Japanese": "回答変数",
    "Russian": "переменная ответа"
  },
  {
    "English": "antecedent",
    "context": "1: A i either points to a previous <mark>antecedent</mark> mention position (A i < i) and \"steals\" its entity assignment or begins a new entity (A i = i). The choice of A i is parametrized by affinities s π (i, j; X) between mention positions i and j.<br>2: Incorporating the lifespan model The lifespan model can improve coreference resolution in two different ways: (i) mentions classified as singletons should not be considered as either <mark>antecedent</mark>s or coreferent, and (ii) mentions classified as coreferent should be linked with another mention(s).<br>",
    "Arabic": "سابق",
    "Chinese": "先决条件",
    "French": "antécédent",
    "Japanese": "前件",
    "Russian": "предпосылка"
  },
  {
    "English": "antithetic sampling",
    "context": "1: In practice, we use <mark>antithetic sampling</mark> to reduce variance. The PES estimator with <mark>antithetic sampling</mark>, which we denoteĝ PES-A , is given by: \n<br>2: f is expensive and d ≥ 200 as in Section 6 . Prior work has also studied variance reduction methods based on sampling without replacement [31] and <mark>antithetic sampling</mark> [13][14][15]68] for gradient estimation.<br>",
    "Arabic": "عينات معاكسة",
    "Chinese": "反向采样",
    "French": "échantillonnage antithétique",
    "Japanese": "逆対称サンプリング",
    "Russian": "антитетическое сэмплирование"
  },
  {
    "English": "anytime algorithm",
    "context": "1: Depending on the application of interest, one may choose to use a slow decoder that provides optimal results or a fast, greedy decoder that provides non-optimal, but acceptable results. One may also run the greedy decoder using a time threshold, as any instance of <mark>anytime algorithm</mark>.<br>2: For all domains, we limit the feature complexity by k=16 and the number of concepts by n=80K in the feature generation step (see Section 3.3). We give each method a maximum of five hours to learn an unsolvability heuristic. We run T-SAFE as an <mark>anytime algorithm</mark>, but it only reaches the time limit for Barman and Nomystery.<br>",
    "Arabic": "خوارزمية حينية",
    "Chinese": "随时算法",
    "French": "algorithme anytime",
    "Japanese": "いつでも可能なアルゴリズム",
    "Russian": "алгоритм в любое время"
  },
  {
    "English": "aperture problem",
    "context": "1: In any case, noise, missing data and insufficient geometric texture in the live frame -an analogue to the <mark>aperture problem</mark> in optical-flow -will result in optimisation of the transform parameters being ill-posed. How should we constrain the motion of non-observed geometry?<br>2: Much like stereo or 2D motion estimation, scene flow estimation is ill-posed due to the 3D equivalent of the <mark>aperture problem</mark>, and thus requires prior assumptions on geometry and motion. Shortcomings of general-purpose regularization have prompted the development of stronger priors, e.g., encouraging locally rigid motion [22] as is common to many scenes.<br>",
    "Arabic": "مشكلة الفتحة",
    "Chinese": "孔径问题",
    "French": "problème d'ouverture",
    "Japanese": "アパーチャ問題",
    "Russian": "проблема апертуры"
  },
  {
    "English": "appearance model",
    "context": "1: 3, where the person is occluded by the sign, note that Ñ ×´x Ø µ decays smoothly in the occluded region due to the absence of data support, while the mean ×´x Ø µ remains roughly fixed until Ñ × falls below the plotting threshold. This clearly demonstrates the persistence of the <mark>appearance model</mark>.<br>2: Since the model is a generative one, the background images are not used in learning except for one instance: the <mark>appearance model</mark> has a distribution in appearance space modeling background features. Estimating this from foreground data proved inaccurate so the parameters were estimated from a set of background images and not updated within the EM iteration.<br>",
    "Arabic": "نموذج المظهر",
    "Chinese": "外观模型",
    "French": "modèle d'apparence",
    "Japanese": "外観モデル",
    "Russian": "модель внешнего вида"
  },
  {
    "English": "apprenticeship learning",
    "context": "1: Effective diffusion of knowledge has been studied in many fields, including inverse reinforcement learning (Ng and Russell 2000), <mark>apprenticeship learning</mark> (Abbeel and Ng 2004), and learning from demonstration (Argall et al. 2009), wherein students discern and emulate key demonstrated behaviors. Works on curriculum learning (Bengio et al.<br>2: The inverse RL (IRL) and <mark>apprenticeship learning</mark> literature examine the problem of learning directly from behavior [37,1]. The classical problem of IRL is to identify which reward function (often up to an equivalence class) a given demonstrator is optimizing. We emphasize the relevance of two approaches: First, work by Syed et al.<br>",
    "Arabic": "التعلم التلمذة",
    "Chinese": "学徒学习",
    "French": "apprentissage par imitation",
    "Japanese": "徒弟学習",
    "Russian": "обучение по принципу ученичества"
  },
  {
    "English": "approximate inference",
    "context": "1: Although LBP has been widely used for <mark>approximate inference</mark> in graphical models with cycles, LBP is not guaranteed to converge and is susceptible to getting trapped at non-optimal fix points.<br>2: Especially in the maximization case, even a naive sampling algorithm for <mark>approximate inference</mark> is not scalable: n sampling rounds need to be carried out for each node to estimate the influence, which results in an overall O(n|V||E|) algorithm.<br>",
    "Arabic": "الاستدلال التقريبي",
    "Chinese": "近似推理",
    "French": "inférence approximative",
    "Japanese": "近似推論",
    "Russian": "приближенный вывод"
  },
  {
    "English": "approximate inference algorithm",
    "context": "1: This allows us to reduce the computational complexity of message passing from quadratic to linear in the number of variables by employing efficient approximate high-dimensional filtering [16,2,1]. The resulting <mark>approximate inference algorithm</mark> is sublinear in the number of edges in the model.<br>2: However, long-range connections can also propagate misleading information, as shown in Figure 7. Discussion. We have presented a highly efficient <mark>approximate inference algorithm</mark> for fully connected CRF models. Our results demonstrate that dense pixel-level connectivity leads to significantly more accurate pixel-level classification performance.<br>",
    "Arabic": "خوارزمية الاستدلال التقريبي",
    "Chinese": "近似推理算法",
    "French": "algorithme d'inférence approximative",
    "Japanese": "近似推論アルゴリズム",
    "Russian": "алгоритм приближенного вывода"
  },
  {
    "English": "approximate posterior",
    "context": "1: Another approach could be to select hyperparameters based on unsupervised scores such as the reconstruction error, the KL divergence between the prior and the <mark>approximate posterior</mark>, the Evidence Lower BOund or the estimated total correlation of the sampled representation (mean representation gives similar results). This would have the advantage that Table 1.<br>2: <mark>approximate posterior</mark> ( 13 ) that we may freely evaluate anywhere in X . In (13), we obtain an efficient approximator by separately discretizing the prior using Fourier basis functions φ i (•) and the update using canonical basis functions k(•, z j ).<br>",
    "Arabic": "الاحتمال الشرطي التقريبي",
    "Chinese": "近似后验",
    "French": "distribution a posteriori approximative",
    "Japanese": "近似事後分布",
    "Russian": "приближенный апостериорный"
  },
  {
    "English": "approximate posterior distribution",
    "context": "1: However, caution is warranted in settings with highly correlated variables: in such settings, the <mark>approximate posterior distribution</mark> from the fully-factored variational approximation will often be inaccurate. While predictive performance is quite robust to this issue, inference is more sensitive (Carbonetto and Stephens, 2012).<br>",
    "Arabic": "التوزيع اللاحق التقريبي",
    "Chinese": "近似后验分布",
    "French": "distribution postérieure approximative",
    "Japanese": "近似事後分布",
    "Russian": "приближенное апостериорное распределение"
  },
  {
    "English": "approximate similarity search",
    "context": "1: Based on this key observation, we show an explicit construction of asymmetric hash function, leading to the first provably sublinear query time algorithm for <mark>approximate similarity search</mark> with (unnormalized) inner product as the similarity. The construction of asymmetric hash function and the new LSH framework could be of independent theoretical interest.<br>2: This tunes the hash functions according to the estimation problem of interest; however, indexed examples must be sorted according to the input space (non-learned) distance. We address the problem of sub-linear time <mark>approximate similarity search</mark> for a class of learned metrics.<br>",
    "Arabic": "البحث عن التشابه التقريبي",
    "Chinese": "近似相似性搜索",
    "French": "recherche de similarité approximative",
    "Japanese": "近似類似検索",
    "Russian": "приблизительный поиск похожих объектов"
  },
  {
    "English": "approximation",
    "context": "1: For k ≤ rank(A), we use [A] k to denote the best rank k <mark>approximation</mark> of A. We define [A] 0 = 0. [A; B] is the matrix formed by concatenating the rows of A and B. We usẽ O() to hide polylog(ndk) factors.<br>2: This contradicts with the observation that the left subtree of T , which is of depth n − 1, (µ, )-separates V . We now restate a more precise version of Proposition 3.5. First we define the computational task of computing a 0.3 ln(|H|)<mark>approximation</mark> of Cost(H) by the following problem: \n<br>",
    "Arabic": "تقريب",
    "Chinese": "近似",
    "French": "approximation",
    "Japanese": "近似",
    "Russian": "приближение"
  },
  {
    "English": "approximation algorithm",
    "context": "1: <mark>approximation algorithm</mark> unless P = NP , where m = |N | ( Zuckerman , 2006 ) . The approximation ratios we consider are all multiplicative.<br>2: The goal is to minimize this cost; unfortunately the problem is NP-Hard to optimize, although both heuristic [21] and <mark>approximation algorithm</mark> techniques [20,25,7] exist. In the streaming model, we require that the point set be read sequentially, and that our algorithm stores very few points at any given time.<br>",
    "Arabic": "خوارزمية تقريبية",
    "Chinese": "近似算法",
    "French": "algorithme d'approximation",
    "Japanese": "近似アルゴリズム",
    "Russian": "алгоритм приближения"
  },
  {
    "English": "approximation bind",
    "context": "1: Our proofs establish that the algorithm converges for any I\\, > k; of course, there are inherent tradeoffs between I\\, and the <mark>approximation bound</mark>. For appropriately chosen constants our approximation factor will be roughly 17, substantially less than the factor claimed in [9] prior to the ball k-means step.<br>",
    "Arabic": "حد التقريب",
    "Chinese": "逼近界限",
    "French": "borne d'approximation",
    "Japanese": "\"近似束縛\"",
    "Russian": "приближение границы"
  },
  {
    "English": "approximation error",
    "context": "1: Our formal proofs utilize the covering number to address infinite function classes; see Appendix B for details. In addition, we also omit the <mark>approximation error</mark> terms ε F and ε F ,F in the results presented in this section for the purpose of clarity.<br>2: Under local regularity (A2), Alg. 2 achieves an <mark>approximation error</mark>f (θ \n T ) −f (θ * ) of at most ε > 0 in a time T ε upper-bounded by O RL ℓ ε τ γ(W ) + RL ℓ ε 2 ,(27) \n which matches the lower complexity bound of Theorem 3.<br>",
    "Arabic": "خطأ التقريب",
    "Chinese": "近似误差",
    "French": "erreur d'approximation",
    "Japanese": "近似誤差",
    "Russian": "ошибка аппроксимации"
  },
  {
    "English": "approximation factor",
    "context": "1: We show that despite our modifications, the worst case <mark>approximation factor</mark> is still constant. Our proof is based on a much tighter bound on the cost incurred per phase, along with a more flexible definition of the \"critical phase\" by which the algorithm should terminate.<br>2: σ t−1 ( j ) if t odd , j / ∈ X t−1 . This schedule is equivalent to the deterministic local search (DLS) algorithm by [8], and therefore achieves an <mark>approximation factor</mark> of 1/3 − η. \n Bi-directional greedy (BG).<br>",
    "Arabic": "معامل التقريب",
    "Chinese": "逼近因子",
    "French": "facteur d'approximation",
    "Japanese": "近似係数",
    "Russian": "фактор приближения"
  },
  {
    "English": "approximation guarantee",
    "context": "1: We substantially simplify the algorithm of [9]. We improve the manner by which the algorithm determines better facility cost as the stream is processed, removing unnecessary checks and allowing the user to parametrize what remains. We show that our changes result in a better <mark>approximation guarantee</mark> than the previous work.<br>2: β|X * | 1 + (|X * | − 1)(1 − κ f ) (11) \n for the problem of min{f (X)|X ∈ C} where β is the <mark>approximation guarantee</mark> of solving a modular function over C where C is the feasible set.<br>",
    "Arabic": "ضمان التقريب",
    "Chinese": "近似保证",
    "French": "garantie d'approximation",
    "Japanese": "近似保証",
    "Russian": "гарантия приближения"
  },
  {
    "English": "approximation ratio",
    "context": "1: In this paper, we will focus on approximation algorithms. An algorithm is said to have an <mark>approximation ratio</mark> of α, if it is guaranteed to return a solution of cost no greater than α times the optimal cost, while satisfying all other conditions.<br>2: It is because LazyProbe enumerates all feasible billboard sets whose cardinality is no larger than 3, and invokes a greedy-based dynamic computation function to get the result, which makes it achieve (1 − 1/e) <mark>approximation ratio</mark>.<br>",
    "Arabic": "نسبة التقريب",
    "Chinese": "近似比率",
    "French": "ratio d'approximation",
    "Japanese": "近似比",
    "Russian": "коэффициент приближения"
  },
  {
    "English": "approximator",
    "context": "1: To do so, we will develop a method to transform a justification frame JF into its ultimate frame U(JF ). We will then show that the <mark>approximator</mark> associated to U(JF ) is indeed the ultimate <mark>approximator</mark> of O JF .<br>2: Since consistent backups can cause information sets to proliferate, we suggest search heuristics that focus attention on promising information sets, as well as methods that impose (or approximate) policy consistency within batches of training data, in an effort to drive the <mark>approximator</mark> toward better estimates.<br>",
    "Arabic": "مُقارِب",
    "Chinese": "近似算子",
    "French": "approximateur",
    "Japanese": "近似器",
    "Russian": "аппроксиматор"
  },
  {
    "English": "apriori algorithm",
    "context": "1: Since the introduction of the <mark>Apriori algorithm</mark> about a decade ago [2], the field of data mining has flourished into a research area of significant technological and social importance, with applications ranging from business intelligence to security to bioinformatics.<br>",
    "Arabic": "خوارزمية أبريوري",
    "Chinese": "Apriori算法",
    "French": "algorithme Apriori",
    "Japanese": "アプリオリ・アルゴリズム",
    "Russian": "алгоритм Apriori"
  },
  {
    "English": "arc-factor model",
    "context": "1: Tractability is usually ensured by strong factorization assumptions, like the one underlying the <mark>arc-factored model</mark> (Eisner, 1996;McDonald et al., 2005), which forbids any feature that depends on two or more arcs. This induces a decomposition of the feature vector f (x, y) as: \n<br>2: The resulting algorithm is still slow, and an <mark>arc-factored model</mark> is used as a surrogate during training (i.e., the hard constraints are only used at test time), which implies a discrepancy between the model that is optimized and the one that is actually going to be used.<br>",
    "Arabic": "نموذج العامل القوسي",
    "Chinese": "弧因子模型",
    "French": "modèle à facteurs d'arc",
    "Japanese": "アークファクターモデル",
    "Russian": "модель с факторизацией дуг"
  },
  {
    "English": "arcade learning environment",
    "context": "1: Beginning with the <mark>Arcade Learning Environment</mark> (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates.<br>2: We perform a comprehensive evaluation of our proposed method on the <mark>Arcade Learning Environment</mark> (Bellemare et al., 2013), which is composed of 57 Atari games. The challenge is to deploy a single algorithm and architecture, with a fixed set of hyper-parameters, to learn to play all the games given only raw pixel observations and game rewards.<br>",
    "Arabic": "بيئة تعلم الألعاب الأركيد",
    "Chinese": "街机学习环境",
    "French": "environnement d'apprentissage d'arcade",
    "Japanese": "アーケード学習環境",
    "Russian": "аркадная обучающая среда"
  },
  {
    "English": "architectural modification",
    "context": "1: 8) when cotraining with representations from a \"generic DNN\" that is trained on a more standard task on the GSP boards (predicting randomly held-out tiles). The results show that the selective performance improvement on human-generated tasks vs machine-generated tasks is unique to cotraining representations from the DreamCoder DNN as opposed to a standard DNN. The recognition model 's specific <mark>architectural modifications</mark> , training objective , and data distribution used for program induction within the DreamCoder framework play a critical role in developing representations that can be distinguished from those of a Figure 8 : Grounding results for control experiment where we co-train with DNN representations from the hidden layer of a network trained to predict randomly held-out tiles<br>",
    "Arabic": "التعديلات المعمارية",
    "Chinese": "架构修改",
    "French": "modification architecturale",
    "Japanese": "アーキテクチャの変更",
    "Russian": "архитектурные модификации"
  },
  {
    "English": "architecture",
    "context": "1: This drastic difference suggests that some care must be taken when designing the projector and that even though the number of parameters is important, the <mark>architecture</mark> in itself also is.<br>2: We build a GAN <mark>architecture</mark> which, instead of being conditioned with images of a specific domain as in [4], it is conditioned on a one-dimensional vector indicating the presence/absence and the magnitude of each action unit. We train this <mark>architecture</mark> in an unsupervised manner that only requires images with their activated AUs.<br>",
    "Arabic": "معمارية",
    "Chinese": "架构",
    "French": "architecture",
    "Japanese": "アーキテクチャ",
    "Russian": "архитектура"
  },
  {
    "English": "architecture search",
    "context": "1: (2020a) also use this decomposition to suggest that LML is connected to training speed. Rasmussen and Ghahramani (2001) additionally note that the LML operates in function space, and can favour models with many parameters, as long as they do not induce a distribution over functions unlikely to generate the data. Our work complements the current understanding of the LML , and has many features that distinguish it from prior work : ( 1 ) We provide a comprehensive treatment of the strengths and weaknesses of the LML across hypothesis testing , model selection , <mark>architecture search</mark> , and hyperparameter optimization ; ( 2 ) While it has been noted that LML model selection can<br>2: \", even if we have a reasonable prior; (3) We differentiate between LML hypothesis testing of fixed priors, and predicting which trained model will generalize best; (4) We also show that LML optimization can lead to underfitting or overfitting in function space; (5) We show the recent characterization in Lyle et al. ( 2020a ) that `` models which train faster will obtain a higher LML '' is not generally true , and revisit the connection between LML and training efficiency ; ( 6 ) We show that in modern deep learning , the Laplace LML is not well-suited for <mark>architecture search</mark> and hyperparameter learning despite its recent use ; ( 7 ) We study a<br>",
    "Arabic": "البحث عن الهندسة المعمارية",
    "Chinese": "架构搜索",
    "French": "recherche d'architecture",
    "Japanese": "アーキテクチャ検索",
    "Russian": "поиск архитектуры"
  },
  {
    "English": "arg max",
    "context": "1: <mark>arg max</mark> δ {P t (ΩAcc(Ω, Π, δ)) × P t (ΠAcc(Π, Ω, ν, δ)) | P t (ΠAcc(Π, Ω, ν, δ)) ≥ γ} \n when Π is confident and not desperate to trade.<br>2: We denote the classification of the network as c(x) = <mark>arg max</mark> i f (x) i , and c * (x) denotes the true label.<br>",
    "Arabic": "أرغ ماكس",
    "Chinese": "最大参数",
    "French": "arg max",
    "Japanese": "arg max",
    "Russian": "аргмакс"
  },
  {
    "English": "arg min",
    "context": "1: [fact] i ) n i=1 : <mark>arg min</mark> r n i=1 (r • p [info] i + (1 − r) • q s i ) log r • p [info] i + (1 − r) • q s i p [fact] i \n<br>2: To improve the initial alignment, first a datum, z * 0 , is chosen from the entire training sequence Z * according to a minimax rule: \n z * 0 ← <mark>arg min</mark> z∈Z * max z ∈Z * −{z} ρ(z, z ). Then, \n<br>",
    "Arabic": "أدنى حد",
    "Chinese": "极小化",
    "French": "arg min",
    "Japanese": "arg min",
    "Russian": "аргмин"
  },
  {
    "English": "argument",
    "context": "1: In the future, we will continue our research in this direction to carry out translation with deeper features, for example, propositional structures . We believe that the fixed and floating structures proposed in this paper can be extended to model predicates and <mark>arguments</mark>.<br>",
    "Arabic": "حُجة",
    "Chinese": "论元",
    "French": "argument",
    "Japanese": "引数",
    "Russian": "аргумент"
  },
  {
    "English": "argument identification",
    "context": "1: For event extraction task, it can be divided into four subtasks: event classification, trigger identification, <mark>argument identification</mark>, and argument role classification.<br>2: Experimental results show that our approach consistently outperforms seven state-of-theart event extraction methods for the classification of events and argument role and <mark>argument identification</mark>.<br>",
    "Arabic": "تحديد الحجة",
    "Chinese": "论元识别",
    "French": "identification des arguments",
    "Japanese": "引数の識別",
    "Russian": "идентификация аргументов"
  },
  {
    "English": "argument relation",
    "context": "1: These meaning functions map instances of entities or eventualities i, j, k to truth values based on whether the described <mark>argument relations</mark> hold between these referents. These <mark>argument relations</mark> are defined as numbered functions ( v i ) = j from eventuality or predicate instances i to argument instances j identified by the number of the function v. The ' 0 ' function identifies j as i 's predicate concept ( so ' 0 ' maps entity or eventuality instances to instances of concepts associated with words in X )<br>",
    "Arabic": "حالة الحجة",
    "Chinese": "论元关系",
    "French": "relation argumentale",
    "Japanese": "論証関係",
    "Russian": "аргументные отношения"
  },
  {
    "English": "argument structure",
    "context": "1: In contrast, SRL is tuned to identify the <mark>argument structure</mark> for nearly all verbs and nouns in a sentence. The missing recall from SRL is primarily where it does not identify both arguments of a binary relation, or where the correct argument is buried in a long argument phrase, but is not its head noun.<br>",
    "Arabic": "هيكل الحجة",
    "Chinese": "论元结构",
    "French": "structure argumentale",
    "Japanese": "項構造",
    "Russian": "структура аргументов"
  },
  {
    "English": "arity",
    "context": "1: Further let q T (ȳ) ∈ Q be the target query known to the oracle, formulated in signature Σ. The algorithm maintains and repeatedly updates a hypothesis CQ q H (x) of <mark>arity</mark> ar. It starts with the hypothesis \n<br>2: The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as <mark>arity</mark> (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002;McDonald and Pereira, 2006).<br>",
    "Arabic": "التعداد",
    "Chinese": "元数",
    "French": "arité",
    "Japanese": "引数数",
    "Russian": "арность"
  },
  {
    "English": "artificial agent",
    "context": "1: Program induction can sometimes be difficult due to the combinatorial explosion of the program space [48] and the non-trivial decision of picking a DSL. Co-supervision of tabula rasa <mark>artificial agents</mark> using representations from program induction can be a useful way to build agents with both the flexibility of neural networks and the human-like priors that program induction can provide.<br>2: An exciting direction for future work is to combine our objectives, utilizing joint program and language representations to instill human inductive biases in <mark>artificial agents</mark>.<br>",
    "Arabic": "وكيل اصطناعي",
    "Chinese": "人工智能代理",
    "French": "agent artificiel",
    "Japanese": "人工エージェント",
    "Russian": "искусственный агент"
  },
  {
    "English": "artificial intelligence system",
    "context": "1: Moreover, humans often exhibit biases, such as racial and gender discrimination, that they themselves reject as improper and would want an artificial moral agent to avoid. This problem can be reduced (though not fully solved) by designing the <mark>artificial intelligence system</mark> to include only features that most humans deem to be morally relevant.<br>",
    "Arabic": "نظام الذكاء الاصطناعي",
    "Chinese": "人工智能系统",
    "French": "système d'intelligence artificielle",
    "Japanese": "人工知能システム",
    "Russian": "искусственная интеллектуальная система"
  },
  {
    "English": "artificial neural network",
    "context": "1: On the other hand, there are some tendencies to include artificial intelligent technologies in WSNs [3][4] [5], such as <mark>artificial neural network</mark> and fuzzy logic. However, few attentions have been paid to integrate Fuzzy Rule-Based Systems (FRBSs) into WSNs (embedded FRBSs).<br>2: A multilayer perceptron (MLP) is a basic, densely connected <mark>artificial neural network</mark>. We used KerasTuner [7] with a random search algorithm for hyperparameter optimization.<br>",
    "Arabic": "شبكة عصبية اصطناعية",
    "Chinese": "人工神经网络",
    "French": "réseau neuronal artificiel",
    "Japanese": "人工ニューラルネットワーク",
    "Russian": "искусственная нейронная сеть"
  },
  {
    "English": "assignment problem",
    "context": "1: We therefore iterate the process of solving the <mark>assignment problem</mark> and estimating the mapping from the source domain to the target domain until it converges. After the approach has converged, we train linear SVMs in a one-vs-one setting on the transformed source samples.<br>2: In practice, we build an end-to-end deep network that integrates a feature extracting component that outputs the required descriptors F for building the matrix M. We solve the <mark>assignment problem</mark> (2) and compute a matching loss L(v * ) between the solution v * and the ground-truth.<br>",
    "Arabic": "مشكلة التعيين",
    "Chinese": "指派问题",
    "French": "problème d'affectation",
    "Japanese": "割り当て問題",
    "Russian": "проблема назначения"
  },
  {
    "English": "association rule",
    "context": "1: In our case, according to the huge amount of data stores in CTS' modules, due to the interaction with learners, we argue that a combination of sequential pattern mining algorithms with <mark>association rules</mark> is more appropriate.<br>2: Finally, the work of [27] aimed at providing a latticetheoretic framework for mining frequent itemsets and <mark>association rules</mark>. Interesting work was also recently reported in [19] on characterization of length distributions of frequent and maximal frequent itemset collections, with a focus on computing tight bounds for feasible distribution.<br>",
    "Arabic": "قاعدة الارتباط",
    "Chinese": "关联规则",
    "French": "règle d'association",
    "Japanese": "結合規則 (けつごうきそく)",
    "Russian": "ассоциативное правило"
  },
  {
    "English": "association rule mining",
    "context": "1: With its broad applications such as <mark>association rule mining</mark> [2], correlation analysis [4], classification [6], and clustering [19], discovering frequent patterns from large databases has been a central research topic in data mining for years.<br>2: Mining frequent patterns is an important data mining problem with broad applications, including <mark>association rule mining</mark>, indexing, classification, and clustering (see e.g., [2,27,26,8,15]).<br>",
    "Arabic": "تنقيب قواعد الارتباط",
    "Chinese": "关联规则挖掘",
    "French": "extraction de règles d'association",
    "Japanese": "関連ルールマイニング",
    "Russian": "поиск ассоциативных правил"
  },
  {
    "English": "asymmetric transformation",
    "context": "1: Since, our hash function uses Hashing for L2 distance after <mark>asymmetric transformation</mark> P (12) and Q (13), we would like to know if such transformations are even needed and furthermore get an estimate of the improvements obtained using these transformations.<br>2: As the number of hash codes is increased, the performance of the proposed methodology shows bigger improvements over L2LSH. The suboptimal performance of L2LSH clearly indicates that the norms of the item characteristic vectors do play a significant role in item recommendation task. Also, the experiments clearly establishes the need of proposed <mark>asymmetric transformation</mark> P and Q.<br>",
    "Arabic": "تحويل غير متماثل",
    "Chinese": "不对称变换",
    "French": "transformation asymétrique",
    "Japanese": "非対称変換",
    "Russian": "асимметричное преобразование"
  },
  {
    "English": "asymptotic bias",
    "context": "1: To account for the fact that in practice an expectation in U(α) is replaced with a sample average, we treat U(α) as an estimator of ln Z with <mark>asymptotic bias</mark> equal to the bound gap (U(α) − ln Z), and estimate its MSE. Figure 4 shows the MSEs of U ( α ) as estimators of ln Z on 10 × 10 ( n = 100 ) binary pairwise grid models with unary potentials sampled uniformly from [ −1 , 1 ] and pairwise potentials from [ 0 , C ] ( attractive models ) or from [ −C , C ] ( mixed models ) ,<br>",
    "Arabic": "التحيز المقارب",
    "Chinese": "渐近偏差",
    "French": "biais asymptotique",
    "Japanese": "漸近的なバイアス",
    "Russian": "асимптотическая смещенность"
  },
  {
    "English": "asymptotic notation",
    "context": "1: Provided that the original data set was o--separable (see section 1.2 for the definition), they use ball k-means to improve the approximation factor to 1 + 0(0-2 ). From a practical standpoint, the main issue with [9] is that the constants hidden in the <mark>asymptotic notation</mark> are quite large.<br>",
    "Arabic": "تدوين مقارب",
    "Chinese": "渐近符号",
    "French": "notation asymptotique",
    "Japanese": "漸近記法",
    "Russian": "асимптотическая нотация"
  },
  {
    "English": "asymptotic variance",
    "context": "1: To understand the asymptotic efficiency of MCMC transition kernels, we can study the <mark>asymptotic variance</mark> and spectral gap of the kernel. The <mark>asymptotic variance</mark> is defined as \n var p (h, Q) = lim T →∞ 1 T var T t=1 h(x t )(7) \n<br>2: The Delta method (Casella & Berger, 2002) is a simple technique for assessing the <mark>asymptotic variance</mark> of estimators that are obtained by a differentiable transformation of an estimator with known variance.<br>",
    "Arabic": "التباين المقارب",
    "Chinese": "渐近方差",
    "French": "variance asymptotique",
    "Japanese": "漸近分散",
    "Russian": "асимптотическая дисперсия"
  },
  {
    "English": "attack success rate",
    "context": "1: It performs better with EmberNN where the <mark>attack success rate</mark> is reduced to 5.3% at ε = 10.0. Additionally, we examine the effect of the trigger sizes on our defense. The trigger size denotes the number of features that the attacker modifies to craft poisoned samples.<br>",
    "Arabic": "معدل نجاح الهجوم",
    "Chinese": "攻击成功率",
    "French": "taux de réussite des attaques",
    "Japanese": "攻撃成功率",
    "Russian": "вероятность успеха атаки"
  },
  {
    "English": "attention",
    "context": "1: These results show that OPINE can effectively detect novel actionable intents in low-resource domains with minimal manual effort. Role of <mark>Attention</mark>. We find that the presence of attention lends OPINE an F1 score gain of at least 4%.<br>2: − → h i = GRU(e x i , − → h i−1 ) (1) ← − h i = GRU(e x i , ← − h i+1 ) (2) \n <mark>Attention</mark>. The attention is designed to extract source information (called source context vector).<br>",
    "Arabic": "الانتباه",
    "Chinese": "注意力",
    "French": "attention",
    "Japanese": "アテンション",
    "Russian": "внимание"
  },
  {
    "English": "attention distribution",
    "context": "1: where e ij = M ij . M r can be interpreted as a word-level <mark>attention distribution</mark> over all other words. Since we would like a single weight per word, we need an additional step to aggregate these attention scores.<br>",
    "Arabic": "توزيع الانتباه",
    "Chinese": "注意力分布",
    "French": "distribution d'attention",
    "Japanese": "アテンション分布",
    "Russian": "распределение внимания"
  },
  {
    "English": "attention function",
    "context": "1: The architecture jointly learns its parameters and an <mark>attention function</mark>, but can alternate between supervision signals from labeled sequences (with no explicit supervision of the <mark>attention function</mark>) and from attention trajectories. This enables us to use per-word fixation durations from eye-tracking corpora to regularize <mark>attention function</mark>s for sequence classification tasks.<br>2: We present a recurrent neural architecture that jointly learns the recurrent parameters and the <mark>attention function</mark>, but can alternate between supervision signals from labeled sequences and from attention trajectories in eye-tracking corpora.<br>",
    "Arabic": "وظيفة الانتباه",
    "Chinese": "注意力函数",
    "French": "fonction d'attention",
    "Japanese": "注目関数",
    "Russian": "функция внимания"
  },
  {
    "English": "attention head",
    "context": "1: The (i, j)-th element of Attr h (A) is computed for the interaction between input token x i and x j in terms of the h-th <mark>attention head</mark>. The starting point (α = 0) of the integration represents that all tokens do not attend to each other in a layer.<br>2: In particular, it uses a single <mark>attention head</mark>, no skip connection, and contains a separate output layer that predicts the target word based on its context vector with probability distribution P align i (•). At training time both output layers are optimized jointly by defining the overall loss L as the weighted sum of both cross-entropy losses: \n<br>",
    "Arabic": "رأس الانتباه",
    "Chinese": "注意力头",
    "French": "tête d'attention",
    "Japanese": "アテンションヘッド (Atenshon Heddo)",
    "Russian": "голова внимания"
  },
  {
    "English": "attention layer",
    "context": "1: Across all the recurrent layers the error rate is small, showing that some form of phonological information is present throughout this part of the model. However, sentence embeddings give relatively high error rates suggesting that the <mark>attention layer</mark> acts to focus on semantic information and to filter out much of phonological form.<br>2: The former linearly projects the concatenation of textual and visual context vectors to obtain the multimodal context vector, while the latter replaces the concatenation with another <mark>attention layer</mark>.<br>",
    "Arabic": "طبقة الانتباه",
    "Chinese": "注意力层",
    "French": "couche d'attention",
    "Japanese": "アテンションレイヤー (Atenshon reiyā)",
    "Russian": "слой внимания"
  },
  {
    "English": "attention map",
    "context": "1: To analyze the role of attention heads, in Figure 10, we visualized the <mark>attention maps</mark> for each head over support images for a given query patch, feature level (3rd level in this example), and task (RS in this example). The figure shows that each head attends to different regions of the support images.<br>2: If DAAM is accurate, then our <mark>attention maps</mark> should arguably align with the image segmentation labels for these tasks-despite not having been trained to perform this task. Setup. We ran Stable Diffusion 2.0-base using 30 inference steps per image with the DPM (Lu et al., 2022) solver-see Appendix A.1.<br>",
    "Arabic": "خريطة الاهتمام",
    "Chinese": "注意力图",
    "French": "carte d'attention",
    "Japanese": "アテンションマップ",
    "Russian": "карта внимания"
  },
  {
    "English": "attention mask",
    "context": "1: The <mark>attention mask</mark> O t m is semantically similar to occupancy, and is generated by multiplying an additional agentlevel feature and the dense feature F t ds , where we name the agent-level feature here as mask feature M t = MLP(G t ). After the interaction process in Eq.<br>2: The <mark>attention mask</mark> is set such that each target token can only attend to all source tokens and preceding target tokens.<br>",
    "Arabic": "قناع الانتباه",
    "Chinese": "注意力掩码",
    "French": "masque d'attention",
    "Japanese": "アテンションマスク",
    "Russian": "маска внимания"
  },
  {
    "English": "attention matrix",
    "context": "1: where the copy probabilities are calculated from the <mark>attention matrix</mark> of an additional encoder-decoder attention layer that is added on top of the final decoder layer, A i : \n P copy i (t ′ i ) = softmax(A i )(11) \n<br>2: Implementation Details To block the information flow to label words, we isolate label words by manipulating the <mark>attention matrix</mark> A. Specifically, we set A l (p, i)(i < p) to 0 in the <mark>attention matrix</mark> A l of the l-th layer, where p represents label words and i represents preceding words.<br>",
    "Arabic": "مصفوفة الانتباه",
    "Chinese": "注意力矩阵",
    "French": "matrice d'attention",
    "Japanese": "注意行列",
    "Russian": "матрица внимания"
  },
  {
    "English": "attention mechanism",
    "context": "1: As a general approach, EPro-PnP inherently unifies existing correspondence learning techniques (Section 3.1). Moreover, just like the <mark>attention mechanism</mark> [44], the corresponding weights can be trained to automatically focus on important point pairs, allowing the networks to be designed with inspiration from attention-related work [10,49,54].<br>2: One key ingredient of our system is to make G focus only on those regions of the image that are responsible of synthesizing the novel expression and keep the rest elements of the image such as hair, glasses, hats or jewelery untouched. For this purpose, we have embedded an <mark>attention mechanism</mark> to the generator.<br>",
    "Arabic": "آلية الانتباه",
    "Chinese": "注意力机制",
    "French": "mécanisme d'attention",
    "Japanese": "注意機構",
    "Russian": "механизм внимания"
  },
  {
    "English": "attention model",
    "context": "1: (2017) model distortion to enhance the <mark>attention model</mark>.<br>2: Conditioning the GAN model on these AUs allows the generator to render a wide range of expressions by simple interpolation. Additionally, we embed an <mark>attention model</mark> within the network which allows focusing only on those regions of the image relevant for every specific expression.<br>",
    "Arabic": "نموذج الانتباه",
    "Chinese": "注意力模型",
    "French": "modèle d'attention",
    "Japanese": "注意モデル",
    "Russian": "модель внимания"
  },
  {
    "English": "attention module",
    "context": "1: Lastly, we perform an error analysis for ICL by examining the distances between the key vectors in the <mark>attention module</mark> that correspond to the label words.<br>2: The language decoder is composed of an <mark>attention module</mark> (whose projection dimension is 512) over the encoded fea-   tures, an LSTM of hidden size 512, and a multi-layer perceptron (Linear → Tanh → Linear → SoftMax) that converts the hidden state into probabilities of all the words in the vocabulary.<br>",
    "Arabic": "وحدة الانتباه",
    "Chinese": "注意力模块",
    "French": "module d'attention",
    "Japanese": "注意モジュール",
    "Russian": "модуль внимания"
  },
  {
    "English": "attention operation",
    "context": "1: We find that such a formulation allows us to scale the transformer with ease. The only mixing across sequence elements occurs in the <mark>attention operation</mark>, and to ensure proper conditioning when training the AR objective, we apply the standard upper triangular mask to the n×n matrix of attention logits.<br>",
    "Arabic": "عملية الانتباه",
    "Chinese": "注意力运算",
    "French": "opération d'attention",
    "Japanese": "注意操作",
    "Russian": "операция внимания"
  },
  {
    "English": "attention pattern",
    "context": "1: We illustrate the differences between our baseline models and the model with gaze-informed attention by the attention weights of an example sentence. Though it is a single, cherry-picked example, it is representative of the general trends we observe in the data, when manually inspecting <mark>attention patterns</mark>.<br>",
    "Arabic": "نمط الانتباه",
    "Chinese": "注意力模式",
    "French": "modèle d'attention",
    "Japanese": "アテンションパターン",
    "Russian": "схема внимания"
  },
  {
    "English": "attention score",
    "context": "1: For each node and its associated hyperedge , the <mark>attention score</mark> between a node and a hyperedge can be calculated as: \n , = exp( (sim(z W , z W ))) ∈E exp( (sim(z W , z W ))) ,(12) \n<br>2: where L(x) is the loss function of example x, and A h is the <mark>attention score</mark> of the h-th head as in Equation (2). For all three methods, we calculate I h on 200 examples sampled from the held-out dataset. Then we sort all the heads according to the importance metrics.<br>",
    "Arabic": "درجة الانتباه",
    "Chinese": "注意力分数",
    "French": "score d'attention",
    "Japanese": "注意スコア",
    "Russian": "оценка внимания"
  },
  {
    "English": "attention value",
    "context": "1: The decoder takes as input a periodic positional encoding e i , generated following Vaswani et al. (2017). Non-final layers additionally take as an <mark>attention values</mark> the predictions from the layer above, i.e. Y B ( l+1 ) t ( for backward reconstruction ) and Y F ( l+1 ) t ( for forward prediction ) and compute a weighted sum of these values over time with attention weight vectors a B ( l ) t , i ∈ ( 0 , 1 ) B and a F ( l ) t , i ∈ ( 0<br>",
    "Arabic": "قيمة الانتباه",
    "Chinese": "注意力值",
    "French": "valeurs d'attention",
    "Japanese": "注目値",
    "Russian": "значение внимания"
  },
  {
    "English": "attention weight",
    "context": "1: This <mark>attention weight</mark> is obtained by comparing the query with its neighborhood keys and spatial attributes. The final attention is normalized by the SoftMax function. The object embedding vector weighted by self-attention is computed as h l+1 i = ei,j =1 α l i,j (v l j ).<br>2: The <mark>attention weight</mark> measures the association of a relation r n to a head entity h n and a tail entity t n . Essentially, a graph vector g i is a weighted sum of the head and tail vectors [h n ; t n ] of the triples contained in the graph.<br>",
    "Arabic": "وزن الانتباه",
    "Chinese": "注意力权重",
    "French": "poids d'attention",
    "Japanese": "注意の重み",
    "Russian": "вес внимания"
  },
  {
    "English": "attention-base model",
    "context": "1: We argue that <mark>attention-based models</mark> such as Tacotron2 should not be used as a benchmark for data requirements among all neural TTS methods, as they are notoriously difficult to train and unnecessarily inflate training data requirements.<br>",
    "Arabic": "نموذج قائم على الانتباه",
    "Chinese": "基于注意力的模型",
    "French": "modèle à base d'attention",
    "Japanese": "注意ベースモデル",
    "Russian": "модель на основе внимания"
  },
  {
    "English": "attribute",
    "context": "1: In the same way, we can assign its best linguistic label t xi to every object x i . Then, let Z ⊆ Y be the set of <mark>attribute</mark>s y j with low values of R(x i , y j ), ∀x i ∈ X.<br>2: We can take the best linguistic label t yj ∈ T (V ) for every <mark>attribute</mark> y j ∈ Z in order to transform these <mark>attribute</mark>s into others more relevant.<br>",
    "Arabic": "سمة",
    "Chinese": "属性",
    "French": "attribut",
    "Japanese": "属性",
    "Russian": "атрибут"
  },
  {
    "English": "attribution",
    "context": "1: The stimuli thus include irrelevant distractor concepts and their subordinates-which, in a robust model, should not affect <mark>attribution</mark> of the property to the correct concept (see §2.4 for stimulus construction).<br>2: Second, by analyzing the context around an extraction, OLLIE is able to identify cases where the relation is not asserted as factual, but is hypothetical or conditionally true. OLLIE increases precision by reducing confidence in those extractions or by associating additional context in the extractions, in the form of <mark>attribution</mark> and clausal modifiers.<br>",
    "Arabic": "الإسناد",
    "Chinese": "归因",
    "French": "attribution",
    "Japanese": "帰属",
    "Russian": "атрибуция"
  },
  {
    "English": "augmentation",
    "context": "1: With this procedure, we can scalably collect 280 paired RGB food images and segmentation masks in real within an hour and a half of data collection. This includes plate resets, food placement, image capture, and offline background subtraction post-processing. Augmentation.<br>2: Thereafter, we begin with the easiest curriculum. For each training instance, we retrieve the cached <mark>augmentation</mark> that has an equivalent difficulty measure with the current difficulty level. We then invoke the task-specific model trainer to train the downstream task model with the retrieved training <mark>augmentation</mark>.<br>",
    "Arabic": "تكبير",
    "Chinese": "增强",
    "French": "augmentation",
    "Japanese": "拡張",
    "Russian": "усиление"
  },
  {
    "English": "augmented state space",
    "context": "1: At each timestep, a subpolicy may select either a low-level action a ∈ A or a special STOP action. We denote the <mark>augmented state space</mark> A + := A ∪ {STOP}.<br>2: (Levy and Shimkin 2011) also built on policy gradient methods by constructing explicitly the <mark>augmented state space</mark> and treating stopping events as additional control actions. In contrast, we do not need to construct this (very large) space directly. (Silver and Ciosek 2012) dynamically chained options into longer temporal sequences by relying on compositionality properties.<br>",
    "Arabic": "- Term: \"حيز الحالة الموسع\"",
    "Chinese": "增广状态空间",
    "French": "espace d'états augmenté",
    "Japanese": "拡張状態空間",
    "Russian": "расширенное пространство состояний"
  },
  {
    "English": "auto-regressive language model",
    "context": "1: While we have applied our methodology towards the training of <mark>auto-regressive language models</mark>, we expect that there is a similar trade-off between model size and the amount of data in other modalities. As training large models is very expensive, choosing the optimal model size and training steps beforehand is essential.<br>2: This problem cannot be overcome at the present state of affairs, since there are very few available massively multilingual <mark>auto-regressive language models</mark>, and the only one with sufficient coverage of our language sample was XGLM.<br>",
    "Arabic": "نموذج اللغة التراجعي التلقائي",
    "Chinese": "自回归语言模型",
    "French": "modèle de langage auto-régressif",
    "Japanese": "自己回帰言語モデル",
    "Russian": "авторегрессионная языковая модель"
  },
  {
    "English": "auto-regressive model",
    "context": "1: This loss is used to optimize both the encoding model g enc and the <mark>auto-regressive model</mark> g ar to extract the features that are consistent over neighboring patches but which diverge between random pairs of patches. At the same time, the scoring model f k learns to use those features to correctly classify the matching pair.<br>",
    "Arabic": "نموذج ذاتي الاسترجاع",
    "Chinese": "自回归模型",
    "French": "modèle autorégressif",
    "Japanese": "自己回帰モデル",
    "Russian": "авторегрессионная модель"
  },
  {
    "English": "auto-regressive process",
    "context": "1: 1. a Markov matrix M for p(k t | k t−1 ), learned by histogramming transitions; 2. a first order <mark>auto-regressive process</mark> (ARP) for p(α t | α t−1 ), with coefficients calculated using the Yule-Walker algorithm (Gelb, 1974).<br>",
    "Arabic": "عملية ذاتية الانحدار",
    "Chinese": "自回归过程",
    "French": "processus auto-régressif",
    "Japanese": "自己回帰過程",
    "Russian": "авторегрессионный процесс"
  },
  {
    "English": "autocalibration",
    "context": "1: This paper proposes practical algorithms that provably solve well-known formulations for both affine and metric upgrade stages of stratified <mark>autocalibration</mark> to their global minimum.<br>2: The theory of convex LMI relaxations [15] is used in [14] to find global solutions to several optimization problems in multiview geometry, while [5] discusses a direct method for <mark>autocalibration</mark> using the same techniques. Triangulation and resectioning are solved with a certificate of optimality using convex relaxation techniques for fractional programs in [1].<br>",
    "Arabic": "تعيير ذاتي",
    "Chinese": "自标定 (autocalibration)",
    "French": "auto-étalonnage",
    "Japanese": "自動校正",
    "Russian": "автокалибровка"
  },
  {
    "English": "autocorrelation",
    "context": "1: There is a significant drop slightly after the half. In order to justify the use of autoregressive part in our models, we have analyzed the <mark>autocorrelation</mark> of goodput time series. The estimated values were usually high only for very short lags, see Fig. 2. This explains the general low predictability of the goodput.<br>2: Figure 4 shows an <mark>autocorrelation</mark> plot of the number of latent rejections during an MCMC run on the Toy5 data. As the computational complexity grows rapidly with the number of latent rejections, minimizing these rejections is paramount. The number of latent rejections relates directly to the mismatch between the base density π(x) and the true data density.<br>",
    "Arabic": "ارتباط ذاتي",
    "Chinese": "自相关",
    "French": "autocorrélation",
    "Japanese": "自己相関",
    "Russian": "автокорреляция"
  },
  {
    "English": "autodiff",
    "context": "1: Similarly, the cost of computing the Jacobian H t is approximately S(F + B) (using either forward or reverse mode <mark>autodiff</mark>).<br>",
    "Arabic": "com.autodiff",
    "Chinese": "自动微分",
    "French": "différenciation automatique",
    "Japanese": "自動微分",
    "Russian": "автодифференцирование"
  },
  {
    "English": "automata",
    "context": "1: In our use of the algorithm, each possible sense for a word corresponds to a different possible spine that can be associated with that word. The left and right <mark>automata</mark> are used to keep track of the last position in the spine that was adjoined into on the left/right of the head respectively.<br>2: In this section, we first discuss an <mark>automata</mark>-based way to decide query containment, yielding upper complexity bounds. We first recall a general technique of reducing query containment to the containment problem for (tree) <mark>automata</mark> [Chaudhuri and Vardi, 1997].<br>",
    "Arabic": "آلة",
    "Chinese": "自动机",
    "French": "automates",
    "Japanese": "オートマトン",
    "Russian": "автоматы"
  },
  {
    "English": "automated mechanism design",
    "context": "1: Reversing the roles of the players in terms of who has the power to commit leads to a dual problem of Bayesian persuasion, which is often known as <mark>automated mechanism design</mark> Sandholm, 2002, 2004).<br>",
    "Arabic": "تصميم آلي للآليات",
    "Chinese": "自动机制设计",
    "French": "La conception automatisée de mécanismes",
    "Japanese": "自動メカニズム設計",
    "Russian": "автоматизированное проектирование механизмов"
  },
  {
    "English": "automatic differentiation",
    "context": "1: q P ((S ρ , X ρ ) → (S ρ , X ρ )) = ρ i ∈(S ρ ,X ρ ) P (ρ i ) \n Gradient Proposal: Picture inference supports <mark>automatic differentiation</mark> for a restricted class of programs (where each expression provides output and gradients w.r.t input).<br>2: Hence, one may compute the exact gradient ∇ θ V 1 (x 1 , w 1:H , θ) by <mark>automatic differentiation</mark> and average them to estimate ∇F (θ). Definition 2.4.<br>",
    "Arabic": "التفاضل التلقائي",
    "Chinese": "自动微分",
    "French": "différenciation automatique",
    "Japanese": "自動微分",
    "Russian": "автоматическое дифференцирование"
  },
  {
    "English": "automatic evaluation",
    "context": "1: 2 We conduct extensive experiments to transfer fairy tales (in Chinese) or everyday stories (in English) to typical author styles, respectively. Automatic evaluation results show that our model achieves a better overall performance in style control and content preservation than strong baselines. The manual evaluation also confirms the efficacy of our model.<br>",
    "Arabic": "تقييم آلي",
    "Chinese": "自动评估",
    "French": "évaluation automatique",
    "Japanese": "自動評価",
    "Russian": "автоматическая оценка"
  },
  {
    "English": "automatic post-editing",
    "context": "1: In the last years, the most accurate systems that have been developed for this task combine linear and neural models (Kreutzer et al., 2015;Martins et al., 2016), use <mark>automatic post-editing</mark> as an intermediate step (Martins et al., 2017), or develop specialized neural architectures Wang et al., 2018).<br>",
    "Arabic": "التحرير اللاحق التلقائي",
    "Chinese": "自动后编辑",
    "French": "post-édition automatique",
    "Japanese": "自動ポストエディット",
    "Russian": "автоматическое постредактирование"
  },
  {
    "English": "automatic speech recognition",
    "context": "1: This is a departure from typical <mark>Automatic Speech Recognition</mark> (ASR) systems which rely on large amounts of transcribed speech, and these recent models come closer to the way humans acquire language in a grounded setting.<br>",
    "Arabic": "التعرف التلقائي على الكلام",
    "Chinese": "自动语音识别",
    "French": "reconnaissance automatique de la parole",
    "Japanese": "自動音声認識",
    "Russian": "автоматическое распознавание речи"
  },
  {
    "English": "automorphism",
    "context": "1: But this holds since the renaming <mark>automorphism</mark> is an <mark>automorphism</mark> of the set of states {yz | z ∈ D Z } -for every y, y , and z there exists an <mark>automorphism</mark> that maps yz to y z with Pr(y, z) = Pr(y , z ).<br>2: If any of them does, we choose one as our 12-node subgraph H. Computational experiments show that a random 12-node graph will have no non-trivial <mark>automorphism</mark>, and γ(H) ≥ 4 with probability roughly 0.25. Thus, with probability well over 0.999, one of the 100 graphs Hi will have this pair of properties.<br>",
    "Arabic": "التشكل الذاتي",
    "Chinese": "自同构",
    "French": "automorphisme",
    "Japanese": "自己同型写像 (じこどうけいしゃぞう)",
    "Russian": "автоморфизм"
  },
  {
    "English": "autonomous agent",
    "context": "1: Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities.<br>2: This work is done on collaboration with the Spanish Governments IIIA Laboratory in Barcelona. Electronic Institutions are software systems composed of <mark>autonomous agents</mark>, that interact according to predefined conventions on language and protocol and that guarantee that certain norms of behaviour are enforced.<br>",
    "Arabic": "وكيل مستقل",
    "Chinese": "自主代理",
    "French": "agent autonome",
    "Japanese": "自律エージェント",
    "Russian": "автономный агент"
  },
  {
    "English": "autonomous vehicle",
    "context": "1: F1/10's biggest value is in taking care of the most tedious aspects of pu ing together an <mark>autonomous vehicle</mark> testbed so that the user can focus directly on the research and learning goals.<br>2: 2 illustrates two cases in which a state-of-the-art semantic segmentation network misclassifies a \"stroller\"/ \"street-market\" -a rare occurrence in either training or testing -as a \"motorcycle\"/\"building\". This failure could be catastrophic for an <mark>autonomous vehicle</mark>.<br>",
    "Arabic": "مركبة ذاتية القيادة",
    "Chinese": "自动驾驶汽车",
    "French": "véhicule autonome",
    "Japanese": "自律走行車",
    "Russian": "автономный автомобиль"
  },
  {
    "English": "autoregressive decoder",
    "context": "1: ArcaneQA is the only open-source baseline that uses constrained decoding to enforce the validity of predicted plans. There are two main reasons for Pangu's superiority. First, though constrained decoding can also help ensure plan validity, the <mark>autoregressive decoder</mark> operates with token-level local normalization and thus lacks a global view.<br>",
    "Arabic": "فك الترميز الانحداري الذاتي",
    "Chinese": "自回归解码器",
    "French": "décodeur autorégressif",
    "Japanese": "自己回帰デコーダ",
    "Russian": "Авторегрессивный декодер"
  },
  {
    "English": "autoregressive generation",
    "context": "1: These architectures are particularly appealing for processing long sequences due to their reduced complexity compared to transformers, and their stronger theoretical guarantees compared to RNNs (Gu et al., 2022b), more details in Section 3. In practical applications, SSMs have found success in both classification and unconditional <mark>autoregressive generation</mark> for language modeling. Gu et al.<br>2: Furthermore, <mark>autoregressive generation</mark> with a neural LM lacks fine-grained control over planning; it is cumbersome, though not impossible, to factor preferences, business logic, and other values and constraints into the plan generation process.<br>",
    "Arabic": "التوليد التكراري الذاتي",
    "Chinese": "自回归生成",
    "French": "génération autorégressive",
    "Japanese": "自己回帰生成",
    "Russian": "авторегрессионная генерация"
  },
  {
    "English": "auxiliary classifier",
    "context": "1: In [44,24], a few intermediate layers are directly connected to <mark>auxiliary classifiers</mark> for addressing vanishing/exploding gradients. The papers of [39,38,31,47] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections.<br>",
    "Arabic": "مُصَنّفٌ مُساعِد",
    "Chinese": "辅助分类器",
    "French": "classificateur auxiliaire",
    "Japanese": "補助分類器",
    "Russian": "вспомогательный классификатор"
  },
  {
    "English": "auxiliary loss",
    "context": "1: With RESBERT, we add a residual connection to a lower or middle layer with the hope of gaining more direct access to information about potential switch-point transitions. We can further encourage this intermediate layer to encode language information by imposing an auxiliary LID-based loss.<br>2: The human-language grounded agent also performed significantly better at the GSP boards than the control boards (p < 0.0001), just like humans do. In contrast, this <mark>auxiliary loss</mark> significantly reduced performance on the machine-generated control boards (as compared to the baseline agent, p = 0.021).<br>",
    "Arabic": "الخسارة المساعدة",
    "Chinese": "辅助损失",
    "French": "perte auxiliaire",
    "Japanese": "補助損失",
    "Russian": "вспомогательная потеря"
  },
  {
    "English": "auxiliary task",
    "context": "1: The results indicate that, although the deformable correspondences can be learned solely with the end-to-end loss, it is still beneficial to add <mark>auxiliary task</mark> for further regularization, even if the task itself does not involve extra annotation.<br>",
    "Arabic": "مهمة مساعدة",
    "Chinese": "辅助任务",
    "French": "tâche auxiliaire",
    "Japanese": "補助タスク",
    "Russian": "вспомогательная задача"
  },
  {
    "English": "auxiliary variable",
    "context": "1: Secondly, note that Eq. (5) contains quadratic terms. As we will discuss in the next section, our solver assumes a linear objective, so we replace V k,j−1 V ij with an <mark>auxiliary variable</mark> W ijk .<br>2: We augment the sufficient statistics with empirical counts for the number of tables across all restaurants that share a given dish, G = N n=1 A n δ θn , and introduce one additional <mark>auxiliary variable</mark>, the normalization of the top level random measure, µ 0 . This latter <mark>auxiliary variable</mark> has no equivalent in CRFs.<br>",
    "Arabic": "متغير مساعد",
    "Chinese": "辅助变量",
    "French": "variable auxiliaire",
    "Japanese": "補助変数",
    "Russian": "вспомогательная переменная"
  },
  {
    "English": "auxillary loss",
    "context": "1: Factuality (and the lack thereof) has been identified as critical in recent work in unsupservised simplification (Laban et al., 2021) and medical simplification (Devaraj et al., 2021). Guo et al. (2018) incorporated textual entailment into their simplification task via an <mark>auxillary loss</mark>.<br>2: For the computation of the <mark>auxillary loss</mark>, we added 10 parallel linear layers on the top of the LSTM's output with i-th linear layer tasked to predict if the i-th element of the stack was i) a round opening bracket or ii) a square opening bracket or iii) If no element was present at that position.<br>",
    "Arabic": "الخسارة الإضافية",
    "Chinese": "辅助损失",
    "French": "perte auxiliaire",
    "Japanese": "補助損失",
    "Russian": "вспомогательная потеря"
  },
  {
    "English": "average loss",
    "context": "1: We refer to this phenomenon of high overall accuracy but low minority accuracy as a representation disparity, which is the result of optimizing for <mark>average loss</mark>.<br>",
    "Arabic": "متوسط الخسارة",
    "Chinese": "平均损失",
    "French": "perte moyenne",
    "Japanese": "平均損失",
    "Russian": "средняя потеря"
  },
  {
    "English": "average pool",
    "context": "1: Each of these Softmax layers gets their input from a Fully Connected (FC) layer, built on top of an <mark>Average Pool</mark> layer, which in turn is built on top of a corresponding Concat layer. Let DC0, DC1 and DC2 be the Concat layers preceding each of the Softmax layers in GoogLeNet.<br>",
    "Arabic": "تجمع متوسط",
    "Chinese": "平均池化层",
    "French": "couche de pooling moyenne",
    "Japanese": "\"平均プール\"",
    "Russian": "средний пул"
  },
  {
    "English": "average precision",
    "context": "1: i.e., Ranking Loss (RL), <mark>Average Precision</mark> (AP), Hamming Loss (HL), and adapted area under curve (AUC) (Bucak, Jin, and Jain 2011;Zhang and Zhou 2013).<br>2: The models are trained in the COCO train2017 set and evaluated in the COCO val2017 set (a.k.a minival). We report the standard COCO metrics of <mark>Average Precision</mark> (AP), AP 50 , and AP 75 , for bounding box detection (AP bbox ) and instance segmentation (AP mask ). Results of C4 backbone.<br>",
    "Arabic": "متوسط الدقة",
    "Chinese": "平均精度",
    "French": "précision moyenne",
    "Japanese": "平均精度",
    "Russian": "средняя точность"
  },
  {
    "English": "averaged perceptron",
    "context": "1: The <mark>averaged perceptron</mark> requires the training set to be repeatedly decoded under the model; under even a simple PCFG representation, finding the arg max in Eq. 1 requires O(n 3 G) time, where n is the length of the sentence, and G is a grammar constant.<br>",
    "Arabic": "متوسط ​​الإدراك الحسي",
    "Chinese": "平均感知器",
    "French": "perceptron moyenné",
    "Japanese": "平均パーセプトロン",
    "Russian": "усредненный перцептрон"
  },
  {
    "English": "averaged perceptron algorithm",
    "context": "1: s(x t , y t ) − s(x t , y ) ≥ L(y t , y ) where y = arg max y s(x t , y ) McDonald et al. ( \n This model is related to the <mark>averaged perceptron algorithm</mark> of Collins (2002).<br>",
    "Arabic": "خوارزمية البيرسيبترون المُعدلة",
    "Chinese": "平均感知机算法",
    "French": "algorithme de perceptron moyenné",
    "Japanese": "平均パーセプトロンアルゴリズム",
    "Russian": "алгоритм усредненного перцептрона"
  },
  {
    "English": "axis-align rectangle",
    "context": "1: We first prove that RISN(r, n + 1) ≤ RISN(r, n) + (2⌈r⌉ + 2) for every r ≥ 1 and n ≥ 2. To this end , we claim that in any collection C of axis-aligned r-fat rectangles , there exists a rectangle in C that overlaps at most 2⌈r⌉ + 2 pairwise-disjoint rectangles from C. To prove this claim , let Q min be a rectangle with a smallest shorter side among all rectangles in C , and denote the length of this side by w.<br>2: Now, given an n-agent instance, we ask each agent to produce a 1-out-of-k maximin partition: this is a set of k <mark>axis-aligned rectangles</mark> that are s-separated. Then, we replace each rectangle Q with Wrap(Q, s), so each agent now has a set of k pairwise-disjoint rectangles.<br>",
    "Arabic": "مستطيل محاذٍ للمحور",
    "Chinese": "轴对齐矩形",
    "French": "rectangle aligné sur l'axe",
    "Japanese": "軸に揃えられた長方形",
    "Russian": "прямоугольник, выровненный по осям"
  },
  {
    "English": "back-off strategy",
    "context": "1: To counter this, we employ these constraints using a <mark>back-off strategy</mark> that relies on progressively less reliable information. Our back-off model works as follows: if centering information is present, we apply the appropriate constraints (Equation ( 8)).<br>2: To estimate P we use a <mark>back-off strategy</mark>:  coarse level conditions on {t h , d, σ}.<br>",
    "Arabic": "استراتيجية التراجع",
    "Chinese": "回退策略",
    "French": "stratégie de repli",
    "Japanese": "バックオフ戦略",
    "Russian": "стратегия отступления"
  },
  {
    "English": "back-propagate",
    "context": "1: Therefore, the policy can be written in the form π θ (a|φ(s)), similarly to the standard policy form (cf. Section 2). If we could <mark>back-propagate</mark> through this function, then potentially we could train the policy using standard RL and IL algorithms, just like any other standard policy representation.<br>2: It is still presumably much cheaper in time and memory than fine-tuning the entire BERT model, which must <mark>back-propagate</mark> a much larger set of gradients.<br>",
    "Arabic": "انتشار خلفي",
    "Chinese": "反向传播",
    "French": "rétropropager",
    "Japanese": "逆伝播",
    "Russian": "обратное распространение"
  },
  {
    "English": "back-propagate gradient",
    "context": "1: We observe that this mismatch leads to <mark>back-propagated gradients</mark> that explode at the high-level side-output layers. We therefore adjust how we make use of the ground truth labels in the BSDS dataset to combat this issue. Specifically, the ground truth labels are provided by multiple annotators and thus, implicitly, greater labeler consensus indicates stronger ground truth edges.<br>",
    "Arabic": "انتشار تدريجي للخلف",
    "Chinese": "反向传播梯度",
    "French": "rétropropagation du gradient",
    "Japanese": "逆伝播勾配",
    "Russian": "обратное распространение градиента"
  },
  {
    "English": "back-propagation algorithm",
    "context": "1: In practice, we use the SLM with all components modeled by neural networks to generate N-best parses in the E step, and for the M step, we use the modified <mark>back-propagation algorithm</mark> to estimate the parameters of the neural network models based on the weights calculated in the E step.<br>",
    "Arabic": "خوارزمية الانتشار العكسي",
    "Chinese": "反向传播算法",
    "French": "algorithme de rétropropagation",
    "Japanese": "バックプロパゲーションアルゴリズム",
    "Russian": "алгоритм обратного распространения ошибки"
  },
  {
    "English": "back-translation",
    "context": "1: The best scoring methods, favored by two automatic metrics each, and taking into consideration the significance tests, are (a) for PHOENIX the Multi with <mark>back-translation</mark>, Multibig and Multi-big with <mark>back-translation</mark>, and (b) for DGS corpus the Multi-big, the Multi-big with backtranslation, and the Multi-big with combined preprocessing.<br>2: Since monolingual data is available for both languages, we train backward MT systems in both directions and repeat the <mark>back-translation</mark> process iteratively (He et al., 2016;Lample et al., 2018a). We consider up to two <mark>back-translation</mark> iterations.<br>",
    "Arabic": "الترجمة العكسية",
    "Chinese": "反向翻译",
    "French": "rétrotraduction",
    "Japanese": "逆翻訳",
    "Russian": "обратный перевод"
  },
  {
    "English": "backbone",
    "context": "1: Transformer <mark>Backbone</mark> T5-small L2/L3 0.06 ± 2.67e−3 35.81 ± 4.13e+0 0.25 ± 6.43e−3 0.09 ± 3.43e−3 0.22 ± 5.73e−3 0.22 ± 5.60e−3 0.99 ± 8.70e−3 113.33 ± 2.94e+0 Ours (ByT5-small) L2/L3 0.07 ± 8.07e−3 18.81 ± 3.74e+0 0.28 ± 1.65e−2 0.11 ± 9.43e−3 0.25 ± 1.02e−2 0.244 ± 1.02e−2 0.92 ± 8.90e−3 120.62 ± 6.72e+0 \n<br>2: The base network is a fully-convolutional encoderdecoder network inspired by the DeepLabV3 [4] and DeepLabV3+ [5] architectures, which achieved state-ofthe-art performance on semantic segmentation tasks in 2017 and 2018. Our base network consists of three modules: <mark>Backbone</mark>, ASPP, and Decoder.<br>",
    "Arabic": "العمود الفقري",
    "Chinese": "主干网络",
    "French": "colonne vertébrale",
    "Japanese": "バックボーン (backbone)",
    "Russian": "основа"
  },
  {
    "English": "backbone model",
    "context": "1: We further train or optionally fine-tune the T5 <mark>backbone models</mark>, as detailed in Section 4, with the teacher forcing (Williams and Zipser, 1989;Lamb et al., 2016) learning objective for task-specific generation.<br>2: We recognize that our annotation and analysis methods can require considerable human labor, that can limit the amount of annotated data we can collect. Also, despite cycle training being generally accepted as a model-agnostic approach, we were not able to test a wide variety of <mark>backbone models</mark> due to resource constraints.<br>",
    "Arabic": "نموذج الأساس",
    "Chinese": "主干模型",
    "French": "modèles de base",
    "Japanese": "基幹モデル",
    "Russian": "базовая модель"
  },
  {
    "English": "backbone network",
    "context": "1: The predicted depth maps on all stages are utilized to calculate the self-supervision loss, as shown in Figure 9. In default, we adopt CVP-MVSNet as the <mark>backbone network</mark>. Similar to the loss function of JDACS (Equation 8 in the main paper), the final objective of JDACS-MS can be constructed as follows: \n<br>2: The results of ViT on image classification are encouraging, but its architecture is unsuitable for use as a general-purpose <mark>backbone network</mark> on dense vision tasks or when the input image resolution is high, due to its low-resolution feature maps and the quadratic increase in complexity with image size.<br>",
    "Arabic": "شبكة الهيكل الأساسي",
    "Chinese": "主干网络",
    "French": "réseau de base",
    "Japanese": "基幹ネットワーク",
    "Russian": "магистральная сеть"
  },
  {
    "English": "backdoor",
    "context": "1: For example, if the detectors for <mark>backdoor</mark> and for keylogger report hits, then the overall prediction for the executable is <mark>backdoor</mark>+keylogger.<br>2: This is due to the fact that with m = 1, approximately 1% of the embeddings are equal to the pre-defined target embedding e t , which diminishes the effectiveness of the provided embeddings. When m is large, the <mark>backdoor</mark> degrees of most provided embeddings are too small to effectively inherit the watermark in the stealer's model.<br>",
    "Arabic": "باب خلفي",
    "Chinese": "后门",
    "French": "porte dérobée",
    "Japanese": "バックドア",
    "Russian": "закладка"
  },
  {
    "English": "backdoor adjustment",
    "context": "1: 3(b) and assume our goal is to establish Q = P (y|do(x)) when external data over {W 2 } is available in both studies. Then, Z = {W 2 } is s-backdoor admissible and the s-<mark>backdoor adjustment</mark> is applicable in this case.<br>2: Selection and confounding biases are the two most common impediments to the applicability of causal inference methods in large-scale settings. We generalize the notion of <mark>backdoor adjustment</mark> to account for both biases and leverage external data that may be available without selection bias (e.g., data from census).<br>",
    "Arabic": "تعديل الباب الخلفي",
    "Chinese": "后门调整",
    "French": "ajustement de porte dérobée",
    "Japanese": "裏口調整",
    "Russian": "коррекция обратной двери"
  },
  {
    "English": "backdoor attack",
    "context": "1: Moreover, several defenses have been proposed to tackle the <mark>backdoor attacks</mark> (Liu, Dolan-Gavitt, and Garg 2018;Tran, Li, and Madry 2018;Wang et al. 2019a). As evidenced by (Severi et al. 2021) that the backdoors created by the XBA remain stealthy against these defenses.<br>2: We vary the trigger size consistently with previous work (Severi et al. 2021). In <mark>backdoor attacks</mark>, a larger trigger makes the backdoored model more prone to misclassification, thus improving the attack success rate. Fig.<br>",
    "Arabic": "هجوم الباب الخلفي",
    "Chinese": "后门攻击",
    "French": "attaque par porte dérobée",
    "Japanese": "バックドア攻撃",
    "Russian": "бэкдор-атака"
  },
  {
    "English": "backdoor sample",
    "context": "1: (2 \n ) \n Since most of the <mark>backdoor samples</mark> contain only a few triggers (< m), their provided embeddings are slightly changed. Meanwhile, the number of <mark>backdoor samples</mark> is relatively small due to the moderate-frequency interval in trigger selection.<br>2: When explanation-guided <mark>backdoor samples</mark> are inserted into the training data, upon bounding the sensitivity that the <mark>backdoor samples</mark> change the output of f , there always exists a noise α that can be injected into a benign sample x, i.e., x + α, to achieve an equivalent ε-LDP protection.<br>",
    "Arabic": "عينة الباب الخلفي",
    "Chinese": "后门样本",
    "French": "échantillon porte dérobée",
    "Japanese": "バックドアサンプル",
    "Russian": "бэкдор-образцы"
  },
  {
    "English": "background model",
    "context": "1: The <mark>background model</mark> has parameters θ app bg = {c bg , V bg }. Both V p and V bg are assumed to be diagonal. Each feature selected by the hypothesis is evaluated under the appropriate part density. All features not selected by the hypothesis are evaluated under the background density. The ratio reduces to: \n<br>",
    "Arabic": "النموذج الخلفي",
    "Chinese": "背景模型",
    "French": "modèle de fond",
    "Japanese": "背景モデル",
    "Russian": "фоновая модель"
  },
  {
    "English": "background subtraction",
    "context": "1: The new method has been applied to track people on subway platforms. The camera being xed, additional geometric constraints and also <mark>background subtraction</mark> can be exploited to improve the tracking process. The following sequences, however, have been processed with the algorithm unchanged.<br>2: Scene analysis with a moving camera is a notoriously difficult task because of the combined effects of egomotion, blur, and rapidly changing lighting conditions [3,6]. In addition, the introduction of a moving camera invalidates many simplifying techniques we have grown fond of, such as <mark>background subtraction</mark> and a constant ground plane assumption.<br>",
    "Arabic": "استخراج الخلفية",
    "Chinese": "背景减除",
    "French": "soustraction d'arrière-plan",
    "Japanese": "背景差分",
    "Russian": "вычитание фона"
  },
  {
    "English": "backoff model",
    "context": "1: As shown in Cohen et al., 2010), the parsing accuracy of the TSG model is strongly affected by its <mark>backoff model</mark>. The effects of our hierarchical <mark>backoff model</mark> on parsing performance are evaluated in Section 5.<br>2: Furthermore, when a sentence is unparsable with large tree fragments, the PTSG parser usually uses naive CFG rules derived from its <mark>backoff model</mark>, which diminishes the benefits obtained from large tree fragments. On the other hand, current state-of-the-art parsers use symbol refinement techniques (Johnson, 1998;Collins, 2003;Matsuzaki et al., 2005).<br>",
    "Arabic": "النموذج الاحتياطي",
    "Chinese": "回退模型",
    "French": "modèle de repli",
    "Japanese": "バックオフモデル",
    "Russian": "модель резервного поведения"
  },
  {
    "English": "backpointer",
    "context": "1: To do so, we represent inside derivations not by explicitly specifying entire trees, but rather by using ranked <mark>backpointers</mark>. In this representation, inside derivations are represented in two ways, shown in Figure 1(d) and (e).<br>2: The oracle tree y + can be recursively restored by keeping <mark>backpointers</mark> for each ora[v](t), which we omit in the pseudocode. The time complexity of this algorithm for a sentence of l words is O(|E| • l 2(a−1) ) where a is the arity of the forest.<br>",
    "Arabic": "مُشير خَلْفي",
    "Chinese": "回指针",
    "French": "pointeur arrière",
    "Japanese": "バックポインタ",
    "Russian": "обратный указатель"
  },
  {
    "English": "backprojection",
    "context": "1: Interestingly, concurrect work [28] has unconvered an intriguing link between our and <mark>backprojection</mark> approaches, by showing that the latter cannot reconstruct NLOS points not on Fermat paths, even if those points otherwise contribute to measured transients. Therefore, both approaches reproduce the same part of the NLOS scene.<br>2: By allowing us to treat NLOS reconstruction from a purely geometric perspective, our theory introduces a new methodology for tackling this problem, distinct from but complementary to approaches such as (elliptic) <mark>backprojection</mark> [48,33] and analysis-by-synthesis [47], which focus on the radiometric aspects of the problem.<br>",
    "Arabic": "الإسقاط الخلفي",
    "Chinese": "反投影",
    "French": "rétroprojection",
    "Japanese": "逆投影",
    "Russian": "обратная проекция"
  },
  {
    "English": "backprop",
    "context": "1: the index of a mini-batch randomly chosen for <mark>backprop</mark>). Formally, \n E z∼Z [g i (x, z)] = ∇f i (x), ∀x ∈ R d . (3) \n As per the usual setup, we additionally assume the local estimator has bounded variance: for some constant σ > 0, \n<br>2: Constant memory <mark>backprop</mark> through reversibility Recent work developed reversible versions of residual networks (Gomez et al., 2017;Chang et al., 2017), which gives the same constant memory advantage as our approach. However, these methods require restricted architectures, which partition the hidden units. Our approach does not have these restrictions.<br>",
    "Arabic": "التراجع",
    "Chinese": "反向传播",
    "French": "rétropropagation",
    "Japanese": "逆伝播",
    "Russian": "обратное распространение"
  },
  {
    "English": "backpropagation",
    "context": "1: TangentProp is itself closely related to the Double <mark>Backpropagation</mark> algorithm (Drucker and LeCun, 1992), in which one instead adds a penalty that is the sum of squared derivatives of the prediction error (with respect to the network input).<br>2: <mark>Backpropagation</mark> works by constructing a directed acyclic computation graph 2 that describes a function as a composition of various primitive operations, e.g., +, ×, and exp(•), whose gradients are known, and subsequently traversing this graph in topological order to incrementally compute the gradients.<br>",
    "Arabic": "الانتشار العكسي",
    "Chinese": "反向传播",
    "French": "rétropropagation",
    "Japanese": "バックプロパゲーション",
    "Russian": "обратное распространение ошибки"
  },
  {
    "English": "backtracking line search",
    "context": "1: too -and also as the first middle query of the line search in [ 0 , 1 ] . We use a (quasi-)exact line search rather than a <mark>backtracking line search</mark>, as it can make a significant difference on the first iterations, but less so afterwards. See also Appendix D on numerical stability.<br>",
    "Arabic": "بحث خطي للتتبع",
    "Chinese": "回溯线搜索",
    "French": "recherche linéaire de retour en arrière",
    "Japanese": "後戻り線探索",
    "Russian": "обратный поиск линии"
  },
  {
    "English": "backward pass",
    "context": "1: If we exploit the sparsity of M the cost drops to O(nnz) where nnz represents the number of non-zero elements of the matrix M, being equal to pq + nm. Backward pass. To compute gradients, we express the variation of the loss and identify the required partial derivatives \n<br>2: The <mark>backward pass</mark> requires the computation of the gradient term in ( 9) for each sample (x, y) in the mini-batch. This can be carried out by a single, bottom-up tree traversal. We start by setting \n A ℓ = π ℓy µ ℓ (x; Θ) P T [y|x, Θ, π] \n<br>",
    "Arabic": "التمرير الخلفي",
    "Chinese": "反向传播",
    "French": "Propagation arrière",
    "Japanese": "逆伝播",
    "Russian": "обратный проход"
  },
  {
    "English": "bad case",
    "context": "1: 6) shows that C is in the same order of n 2 avg in the <mark>worst case</mark> and C/n 2 avg decreases with increasing k in both average case and <mark>worst case</mark>, which indicates that C is O(n 2 ) and the overall time complexity of Alg. 2 is O(kn 2 ).<br>",
    "Arabic": "حالة سيئة",
    "Chinese": "最坏情况",
    "French": "pire cas",
    "Japanese": "最悪ケース",
    "Russian": "худший случай"
  },
  {
    "English": "bad-case regret",
    "context": "1: The oracle, knowing the unique partition part P , incurs the least <mark>worst-case regret</mark> (2), r n (P ) = min q r n (q, P ). The competitive regret of q over the oracle, for all distributions in P is r n (q, P ) − r n (P ), \n<br>2: The strict uncertainty model assumes even less information: vendors only know the possible set of buyer types (i.e., only the support of the distribution is known). In this model, expected utility is illdefined so we instead adopt a common approach for such settings and assume vendors try to minimize their <mark>worst-case regret</mark> over all possible type realizations.<br>",
    "Arabic": "الندم الأسوأ في الحالات سيئة",
    "Chinese": "最坏情况遗憾",
    "French": "regret du pire cas",
    "Japanese": "最悪ケースの後悔",
    "Russian": "плохой случай сожаления"
  },
  {
    "English": "bag of feature",
    "context": "1: It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over <mark>bag of features</mark> baselines.<br>",
    "Arabic": "حقيبة الميزات",
    "Chinese": "特征包",
    "French": "sac de caractéristiques",
    "Japanese": "バッグオブフィーチャー",
    "Russian": "мешок признаков"
  },
  {
    "English": "bag-of-word",
    "context": "1: Then, each image is represented by a 4000-dim token frequency (TF) feature using the <mark>bag-of-word</mark> (BoW) representation, in which the codebook is constructed by using k-means to cluster all the SIFT features from the images.<br>",
    "Arabic": "حقيبة من الكلمات",
    "Chinese": "词袋模型",
    "French": "sac de mots",
    "Japanese": "バッグオブワード",
    "Russian": "мешок слов"
  },
  {
    "English": "bag-of-word model",
    "context": "1: For out-of-domain generalization, Ω rand decreases considerably (36.4% Ω rand on A1), which means fewer permutations are accepted by the model. Next, recall that a classic <mark>bag-of-words model</mark> would have P c = 100 and P f = 0.<br>",
    "Arabic": "نموذج كيس الكلمات",
    "Chinese": "词袋模型",
    "French": "modèle sac de mots",
    "Japanese": "バッグオブワードモデル",
    "Russian": "модель мешка слов"
  },
  {
    "English": "bag-of-word representation",
    "context": "1: As shown in Fig. 5, by inferring the human pose and object in the image, our model gives a prediction of the class label of the human-object interaction. We compare our method with the results reported in [12], and use a bagof-words representation with a linear SVM classifier as the baseline.<br>",
    "Arabic": "التمثيل بنظام حقيبة الكلمات",
    "Chinese": "词袋表示",
    "French": "représentation sac-de-mots",
    "Japanese": "単語袋表現",
    "Russian": "мешок слов"
  },
  {
    "English": "bandit",
    "context": "1: This seems to allow us to carry through with an analysis independent of the form of the distributions of the samples drawn from the arms of the <mark>bandit</mark>. However, an expression is needed in terms of the length of the series of trials, requiring an assumption on the underlying distribution.<br>2: Despite the appeal of using relative human preferences, fine-tuning large language models with reinforcement learning remains a major practical challenge; this work provides a theoretically-justified approach to optimizing relative preferences without RL. Outside of the context of language, learning policies from preferences has been studied in both <mark>bandit</mark> and reinforcement learning settings, and several approaches have been proposed.<br>",
    "Arabic": "بانديت",
    "Chinese": "摇臂赌博机 (bandit)",
    "French": "bras de bandit",
    "Japanese": "バンディット",
    "Russian": "бандит"
  },
  {
    "English": "bandit feedback",
    "context": "1: 2013) and dynamic search advertising (Prabhu et al. 2018(Prabhu et al. , 2020. We assert that the issues of <mark>bandit feedback</mark> and extremescale action spaces are related. Indeed, it is when action spaces are large that it is particularly likely that feedback will only be partial.<br>2: [1] establishes O(T 1/4 ) trigger regret bounds through optimistic hedge, building on [10], they showed multiplicative stability of the fixed points associated with EFCE. Finally, we acknowledge some very recent papers that have developed dynamics converging to EFCE under <mark>bandit feedback</mark> [3,45].<br>",
    "Arabic": "تغذية اللصوص",
    "Chinese": "赌博者反馈",
    "French": "feedback de bandit",
    "Japanese": "バンディットフィードバック",
    "Russian": "фидбек бандитов"
  },
  {
    "English": "bandit learning",
    "context": "1: In this case, due to the Markov time-homogeneity of the adversary, the task of the controller becomes a <mark>bandit learning</mark> problem [Lattimore and Szepesvári, 2020]. Therefore, it is possible for the controller to implement a history-dependent policy that optimally \"learns\" the environment.<br>",
    "Arabic": "تعلم قطاع الطرق",
    "Chinese": "强盗学习",
    "French": "apprentissage bandit",
    "Japanese": "バンディット学習",
    "Russian": "бандитское обучение"
  },
  {
    "English": "bandwidth parameter",
    "context": "1: Above, K 2 is a univariate symmetric kernel with <mark>bandwidth parameter</mark> h 2 . One example is video sequences, where the kernel above combines similarity of the frame features and the time-stamp. Alternatively, the weight function can feature only the temporal component and omit the first term containing the distance function between the feature representation.<br>2: , in which we set the <mark>bandwidth parameter</mark> ν as the mean of the square distances between all pairs of training samples when using s-th view of features. We also set the regularization parameters C S and C T as 1 and 10 respectively, since the target domain data is more important than the source domain data.<br>",
    "Arabic": "معامل النطاق الترددي",
    "Chinese": "带宽参数",
    "French": "paramètre de largeur de bande",
    "Japanese": "帯域幅パラメータ",
    "Russian": "параметр полосы пропускания"
  },
  {
    "English": "bart-base",
    "context": "1: We use their publicly available code 3 to train our two models. Instead of the bart-large model we use the <mark>bart-base</mark> version to make the comparison more fair. We adapt the recommended hyperparameters to our setting (see Appendix A.1).<br>",
    "Arabic": "بارت-بيس",
    "Chinese": "bart-base模型",
    "French": "bart-base",
    "Japanese": "\"bart-base\"",
    "Russian": "bart-base"
  },
  {
    "English": "bart-large",
    "context": "1: We use their publicly available code 3 to train our two models. Instead of the <mark>bart-large</mark> model we use the bart-base version to make the comparison more fair. We adapt the recommended hyperparameters to our setting (see Appendix A.1).<br>",
    "Arabic": "بارت-large",
    "Chinese": "bart-large",
    "French": "bart-large",
    "Japanese": "bart-large",
    "Russian": "bart-large"
  },
  {
    "English": "barycentric coordinate",
    "context": "1: To avoid this artifact, we encourage the vertices around the seam parts to be close by penalizing differences between the last two rings of vertices around the seam of each part, and the corresponding closest point in the body model in the rest pose expressed as <mark>barycentric coordinates</mark> (see the supplementary materials for details).<br>",
    "Arabic": "إحداثيات باري المركزية",
    "Chinese": "重心坐标",
    "French": "coordonnée barycentrique",
    "Japanese": "重心座標",
    "Russian": "барицентрические координаты"
  },
  {
    "English": "base classifier",
    "context": "1: In addition, the prediction of a test label can, in principle, depend on all training and test patterns. Of course, simply increasing the representational capacity of a <mark>base classifier</mark> increases the risk of overfitting.<br>2: To investigate the robustness of the method, we repeated the previous experiments using a different <mark>base classifier</mark>. Table 2 and Figure 3 show the results of an experiment using naive Bayes instead of logistic regression as the base classification method. Here the results are not as strong as the first case we tried, although they are still credible.<br>",
    "Arabic": "المصنف الأساسي",
    "Chinese": "基分类器",
    "French": "classificateur de base",
    "Japanese": "ベース分類器 (base classifier)",
    "Russian": "базовый классификатор"
  },
  {
    "English": "base distribution",
    "context": "1: We must specify a <mark>base distribution</mark> p 0 (z) which we choose to be N (0, I). We must then specify a function Γ(z) : R D → {0, 1} D which maps regions of equal mass under the <mark>base distribution</mark> to values in {0, 1} D .<br>2: First, we argue that the balance conditions of counterfactual equalized odds encoded by Eq. (2) must be broken by a typical perturbation in W. In particular, we argue that for a given <mark>base distribution</mark> µ, there can be at most one budget-exhausting multiple threshold policy that can-although need not necessarily-satisfy counterfactual equalized odds.<br>",
    "Arabic": "التوزيع الأساسي",
    "Chinese": "基础分布",
    "French": "distribution de base",
    "Japanese": "基本分布",
    "Russian": "базовое распределение"
  },
  {
    "English": "base learner",
    "context": "1: Using tools from online loss minimization, we derive an adaptive online boosting algorithm that is also parameter-free, but not optimal. Both algorithms work with <mark>base learners</mark> that can handle example importance weights directly, as well as by rejection sampling examples with probability defined by the booster. Results are complemented with an experimental study.<br>2: Finally, consider the experimental results for AdaBoost in tables 5 and 9. Here, both modified <mark>base learners</mark> and adapted fusion rules contribute to an improvement of lift performance. The introduction of C4.4 <mark>base learners</mark> generates a marked improvement over using C4.5 base classifiers. The use of lift-based fusion rules invokes a further increase.<br>",
    "Arabic": "متعلم أساسي",
    "Chinese": "基本学习器",
    "French": "apprenant de base",
    "Japanese": "ベース学習器",
    "Russian": "базовый алгоритм обучения"
  },
  {
    "English": "base model",
    "context": "1: He and Eisner (2012) share our goal to speed test time prediction by dynamically selecting features, but they also learn an additional model on top of a fixed <mark>base model</mark>, rather than using the training objective of the model itself.<br>2: The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained sentiment classifier siebert/sentiment-roberta-large-english as a ground-truth reward model and gpt2-large as a <mark>base model</mark>. We use these larger models as we found the default ones to generate low-quality text and rewards to be somewhat inaccurate.<br>",
    "Arabic": "النموذج الأساسي",
    "Chinese": "基础模型",
    "French": "modèle de base",
    "Japanese": "ベースモデル",
    "Russian": "базовая модель"
  },
  {
    "English": "baseline algorithm",
    "context": "1: Again our joint algorithm outperforms the <mark>baseline algorithm</mark> at all levels of recall. Interestingly the baseline disambiguation precision improves with higher recall; this may<br>2: Further, we have constructed taxonomies using a <mark>baseline algorithm</mark>, which uses the identical hypernym and coordinate classifiers used in our joint algorithm, but which does not combine the evidence of the classifiers. In section 4.1 we describe our evaluation methodology ; in sections 4.2 and 4.3 we analyze the fine-grained precision and disambiguation precision of our algorithm compared to the baseline ; in section 4.4 we compare the coarse-grained precision of our links ( motivated by categories defined by the WordNet supersenses ) against the <mark>baseline algorithm</mark> and against an `` oracle '' for named entity<br>",
    "Arabic": "خوارزمية خط الأساس",
    "Chinese": "基线算法",
    "French": "algorithme de référence",
    "Japanese": "ベースラインアルゴリズム",
    "Russian": "базовый алгоритм"
  },
  {
    "English": "baseline method",
    "context": "1: Even for L = 32, the label space discretization of the <mark>baseline method</mark> is strongly visible, while the proposed method yields a smooth result already for L = 8.<br>2: In the first step, we train a multi-encoder NMT model (Source: English and Slovak, Target: Czech) to get Czech pseudotranslations using the <mark>baseline method</mark>, which is to replace a missing input sentence with a special symbol NULL .<br>",
    "Arabic": "طريقة خط الأساس",
    "Chinese": "基线法",
    "French": "méthode de référence",
    "Japanese": "ベースライン手法",
    "Russian": "базовый метод"
  },
  {
    "English": "baseline model",
    "context": "1: In every (dataset, NLP model, task) combination, the likelihood ratio test between the baseline and full models revealed that the full model, including the two predictors of interest, is a better fit to the data than the <mark>baseline model</mark> with only token position and text.<br>2: This is Table 3: Speed and accuracy results for the vine pruning cascade across various languages. B is the unpruned <mark>baseline model</mark>, and V is the vine pruning cascade.<br>",
    "Arabic": "النموذج الأساسي",
    "Chinese": "基线模型",
    "French": "modèle de référence",
    "Japanese": "ベースラインモデル",
    "Russian": "исходная модель"
  },
  {
    "English": "baseline parser",
    "context": "1: In n-best reranking, cand (s) is simply a set of n-best parses from the <mark>baseline parser</mark>, that is, cand (s) = {y 1 , y 2 , . . . , y n }. Whereas in forest reranking, cand (s) is a forest implicitly representing the set of exponentially many parses.<br>",
    "Arabic": "محلل الخط الأساسي",
    "Chinese": "基线解析器",
    "French": "analyseur syntaxique de base",
    "Japanese": "ベースラインパーサー",
    "Russian": "базовый синтаксический анализатор"
  },
  {
    "English": "baseline policy",
    "context": "1: The final <mark>baseline policy</mark> is computed via π bsl (s |s, q) ∝ exp h(s, q) φ(s ) for s ∈ A s . We design a VIN for this task as follows.<br>",
    "Arabic": "سياسة خط الأساس",
    "Chinese": "基线策略",
    "French": "politique de référence",
    "Japanese": "ベースラインポリシー",
    "Russian": "базовая политика"
  },
  {
    "English": "baseline system",
    "context": "1: Typically, this method first generates a list of top-n candidates from a <mark>baseline system</mark>, and then reranks this n-best list with arbitrary features that are not computable or intractable to compute within the <mark>baseline system</mark>.<br>2: At first it might seem reasonable to perform significance testing in the following manner when an increase in correlation with gold labels is observed: apply a significance test separately to the correlation of each quality estimation system with gold labels, with the hope that the new system will achieve a significant correlation where the <mark>baseline system</mark> does not.<br>",
    "Arabic": "نظام خط الأساس",
    "Chinese": "基线系统",
    "French": "système de référence",
    "Japanese": "ベースラインシステム",
    "Russian": "эталонная система"
  },
  {
    "English": "basis function",
    "context": "1: , M are the M <mark>basis functions</mark>, and w j is the weight from the j th basis function to the data space. The algorithm is summarized as: \n 1. Randomly select a data point, t n . 2. Find the closest prototype, say m k * , to t n . 3.<br>2: Thus we have a matrix Φ where φ kj = φ j (x k ), each row of which is the response of the <mark>basis functions</mark> to one latent point, or, alternatively each column of which is the response of one of the <mark>basis functions</mark> to the set of latent points.<br>",
    "Arabic": "دوال القاعدة",
    "Chinese": "基函数",
    "French": "fonction de base",
    "Japanese": "基底関数",
    "Russian": "базисная функция"
  },
  {
    "English": "basis vector",
    "context": "1: The Decomposition Theorem says that the <mark>basis vector</mark> r p for p is an average of the <mark>basis vector</mark>s r O i (p) for its out-neighbors, plus a compensation factor cx p . The proof is in Appendix B. The Decomposition Theorem gives another way to think about PPV's.<br>2: where D in R m×k is the dictionary, each column representing a <mark>basis vector</mark>, and l is a loss function such that l(x, D) should be small if D is \"good\" at representing the signal x.<br>",
    "Arabic": "متجه الأساس",
    "Chinese": "基向量",
    "French": "vecteur de base",
    "Japanese": "ベーシスベクトル",
    "Russian": "базисный вектор"
  },
  {
    "English": "batch algorithm",
    "context": "1: The online algorithm outperforms the <mark>batch algorithm</mark> regardless of which training dataset is used, but it does best with access to a constant stream of novel documents. The <mark>batch algorithm</mark>'s failure to outperform the online algorithm on limited data may be due to stochastic gradient's robustness to local optima [19].<br>2: On the larger Nature corpus, online LDA finds a solution as good as the <mark>batch algorithm</mark>'s with much less computation. On the smaller Wikipedia corpus, the online algorithm finds a better solution than the <mark>batch algorithm</mark> does. The <mark>batch algorithm</mark> converges quickly on the 10,000-document corpora, but makes less accurate predictions on held-out documents. True online.<br>",
    "Arabic": "خوارزمية الدفعة",
    "Chinese": "批处理算法",
    "French": "algorithme par lots",
    "Japanese": "バッチアルゴリズム",
    "Russian": "алгоритм пакетной обработки"
  },
  {
    "English": "batch dimension",
    "context": "1: These methods do not suffer from the issues caused by the <mark>batch dimension</mark>, but they have not been able to approach BN's accuracy in many visual recognition tasks. We provide comparisons with these methods in context of the remaining sections. Addressing small batches.<br>2: We have presented GN as an effective normalization layer without exploiting the <mark>batch dimension</mark>. We have evaluated GN's behaviors in a variety of applications. We note, however, that BN has been so influential that many state-ofthe-art systems and their hyper-parameters have been designed for it, which may not be optimal for GN-based models.<br>",
    "Arabic": "البعد التجميعي",
    "Chinese": "批次维度",
    "French": "dimension du lot",
    "Japanese": "バッチ次元",
    "Russian": "размер партии"
  },
  {
    "English": "batch element",
    "context": "1: Select x i ∈ arg min 1≤i≤s (f | y)(X * ) as the i-th <mark>batch element</mark>, where X s denotes the optimized locations. As before, steps (2-5) we run independently. Optimization performance and runtimes are shown below.<br>",
    "Arabic": "عنصر دفعة",
    "Chinese": "批次元素",
    "French": "élément de lot",
    "Japanese": "バッチ要素",
    "Russian": "пакетный элемент"
  },
  {
    "English": "batch learning",
    "context": "1: They are often much faster, more memory-efficient, and apply to situations where the best predictor changes over time as new examples keep coming in. Given the success of boosting in <mark>batch learning</mark>, it is natural to ask about the possibility of applying boosting to online learning.<br>2: Batch learning from bandit feedback may be possible under this assumption, as long as the logging policy explores a set of actions large enough to cover the actions from Ψ but small enough to avoid exploring too many suboptimal actions.<br>",
    "Arabic": "التعلم الدفعي",
    "Chinese": "批量学习",
    "French": "apprentissage par lots",
    "Japanese": "バッチ学習",
    "Russian": "пакетное обучение"
  },
  {
    "English": "batch mode",
    "context": "1: For comparison, the dictionaries used for inpainting in the state-of-the-art method of Mairal et al. (2008) are learned (in <mark>batch mode</mark>) on only 200,000 patches.<br>",
    "Arabic": "وضع دفعي",
    "Chinese": "批处理模式",
    "French": "mode par lots",
    "Japanese": "バッチモード",
    "Russian": "пакетный режим"
  },
  {
    "English": "batch normalization",
    "context": "1: <mark>Batch Normalization</mark> (Batch Norm or BN) [26] has been established as a very effective component in deep learning, largely helping push the frontier in computer vision [59,20] and beyond [54]. BN normalizes the features by the mean and variance computed within a (mini-)batch.<br>2: Before all CoordConv and CoordUpConv, we use 2D Dropout (Srivastava et al., 2014;Tompson et al., 2015) with a zero-out probability of 0.05. We use <mark>Batch Normalization</mark> layers (Ioffe & Szegedy, 2015) and the ReLU activation function (Nair & Hinton, 2010) after all layers except the terminal layer.<br>",
    "Arabic": "تطبيع الدفعة",
    "Chinese": "批量归一化",
    "French": "normalisation par lots",
    "Japanese": "バッチ正規化",
    "Russian": "пакетная нормализация"
  },
  {
    "English": "batch optimization",
    "context": "1: Table 1 shows the case under our experimental setting. As the number of parameters in the embedding layer overwhelms the one of the dense networks, the difficulty of large <mark>batch optimization</mark> lies in the embedding layers. This paper focuses on addressing the training instability caused by the properties of embedding layers in the CTR prediction model.<br>2: With large training sets, classical <mark>batch optimization</mark> techniques may indeed become impractical in terms of speed or memory requirements. In the case of dictionary learning, classical projected firstorder stochastic gradient descent (as used by Aharon and Elad (2008) for instance) consists of a sequence of updates of D: \n<br>",
    "Arabic": "تحسين الدفعة",
    "Chinese": "批量优化",
    "French": "optimisation par lots",
    "Japanese": "バッチ最適化",
    "Russian": "пакетная оптимизация"
  },
  {
    "English": "batch processing",
    "context": "1: An interactive form filling system is quite different from the <mark>batch processing</mark> of data, such as for warehouse data cleaning (Borkar, Deshmukh, & Sarawagi 2000). In <mark>batch processing</mark> the set of fields extracted are determined directly and are optimized for low error rates.<br>2: We implement an efficient <mark>batch processing</mark> pipeline over distributed graph storage: The nodes are partitioned into batches, and the computation of each batch is implemented by workers in parallel with matrix multiplication.<br>",
    "Arabic": "معالجة دُفعية",
    "Chinese": "批处理",
    "French": "traitement par lots",
    "Japanese": "バッチ処理",
    "Russian": "пакетная обработка"
  },
  {
    "English": "batch setting",
    "context": "1: Our Online BBM algorithm builds on top of a potential based family that arises naturally in the <mark>batch setting</mark> as approximate minimax optimal algorithms for so-called drifting games [Schapire, 2001, Luo andSchapire, 2014]. The decomposition of each example in that framework naturally allows us to generalize it to the online setting where example comes one by one.<br>2: 13], then the weight w i t could be exponentially large. Fortunately, there is indeed a set of potential functions that produces small weights, which, in the <mark>batch setting</mark>, corresponds to an algorithm called boost-by-majority (BBM) Freund [1995]. All we need to do is to let Eq.<br>",
    "Arabic": "إعداد دفعة",
    "Chinese": "批量设置",
    "French": "- Term: \"réglage par lots\"\n- Translated term: \"réglage par lots\"",
    "Japanese": "バッチ設定",
    "Russian": "пакетная настройка"
  },
  {
    "English": "batch training",
    "context": "1: In this paper, we identified the failure reason behind previous scaling rules on CTR prediction and proposed an effective algorithm and scaling rule for large <mark>batch training</mark>.<br>2: 2020) are two optimizers designed for large <mark>batch training</mark>, which adopt different adaptive learning rates for each layer. Although they achieve good results in CV and NLP tasks, they are ineffective in the CTR prediction task because it is unnecessary to use a layer-wise optimizer with a shallow network (e.g., three or four layers).<br>",
    "Arabic": "تدريب دفعي",
    "Chinese": "批量训练",
    "French": "entraînement par lots",
    "Japanese": "バッチトレーニング",
    "Russian": "Пакетное обучение"
  },
  {
    "English": "bayesian active learning",
    "context": "1: We proposed a framework for query efficient posterior estimation for expensive blackbox likelihood evaluations. Our methods use GPs and are based on popular ideas in <mark>Bayesian active learning</mark>. We demonstrate that our methods outperform natural alternatives in practice. Note that in Machine Learning it is uncommon to treat posterior estimation in a regression setting.<br>2: (2014) and Hazan et al. (2016) subsequently derived measure concentration results for the Gumbel distribution that can be used to control the rejection probability. Maji et al. (2014) derived an uncertainty measure from random MAP perturbations, using it within a <mark>Bayesian active learning</mark> framework for interactive image boundary annotation.<br>",
    "Arabic": "التعلم النشط البايزي",
    "Chinese": "贝叶斯主动学习",
    "French": "apprentissage actif bayésien",
    "Japanese": "ベイズ的能動学習",
    "Russian": "Байесово активное обучение"
  },
  {
    "English": "bayesian analysis",
    "context": "1: This approach to smoothing uses the conjugate prior for a multinomial distribution, which is the Dirichlet distribution [17]. For a Dirichlet distribution with parameters (λp(w1), λp(w2), ..., λp(wn)) the posterior distribution using <mark>Bayesian analysis</mark> for θ d is \n<br>",
    "Arabic": "التحليل البايزي",
    "Chinese": "贝叶斯分析",
    "French": "analyse bayésienne",
    "Japanese": "ベイズ解析",
    "Russian": "байесовский анализ"
  },
  {
    "English": "bayesian approach",
    "context": "1: Once the bow structure is fixed, we will learn its parameters in order to define its numerical component. As described above, this component differs from F T and ET , thus computations can be done as follows: \n To quantify the fault tree F T , we will use a <mark>Bayesian approach</mark> based on informative priors.<br>2: RCS: Relative Confidence Sampling (RCS) (Zoghi et al., 2014a) follows a <mark>Bayesian approach</mark> by maintaining a posterior distribution over the preference probabilities. At each time step t, the algorithm samples preference probabilities from the posterior and simulates a round-robin tournament among the systems to determine the Condorcet winner.<br>",
    "Arabic": "النهج البايزي",
    "Chinese": "贝叶斯方法",
    "French": "approche bayésienne",
    "Japanese": "ベイズ手法",
    "Russian": "байесовский подход"
  },
  {
    "English": "bayesian clustering",
    "context": "1: The <mark>Bayesian clustering</mark> approach presented in this work aims at identifying subsets (or \"clusters\") of objects represented as columns/rows in a dissimilarity matrix. The underlying idea is that objects grouped together in such a cluster can be reasonably well described as a homogeneous sub-population.<br>2: In this paper, we present a fully-Bayesian generative approach to semi-supervised learning that uses Gaussian processes to specify the prior on class densities. We call the model Archipelago as it performs <mark>Bayesian clustering</mark> with infinite-dimensional density models, but it prefers contiguous densities. These clusters can form irregular shapes, like islands in a chain.<br>",
    "Arabic": "التجميع البايزي",
    "Chinese": "贝叶斯聚类",
    "French": "regroupement bayésien",
    "Japanese": "ベイズクラスタリング",
    "Russian": "байесовская кластеризация"
  },
  {
    "English": "bayesian decision",
    "context": "1: Such classifier tries to approximate the optimal (Bayesian) decision, i * = arg max P (i|x), by relaxing the conditional probability within a small neighborhood of the feature vector instance x [7]. This classifier is considered nonlinear, as its resulting decision boundaries can adjust to any shape.<br>",
    "Arabic": "قرار بايزي",
    "Chinese": "贝叶斯决策",
    "French": "décision bayésienne",
    "Japanese": "ベイズ決定",
    "Russian": "байесовское решение"
  },
  {
    "English": "bayesian deep learning",
    "context": "1: In this algorithm, we estimate uncertainty in the evaluation metric predictions and decide to ask for human annotations only when the evaluation metric is highly uncertain. We specifically focus on trainable neural evaluation metrics such as Bleurt (Sellam et al., 2020) where we estimate the prediction uncertainty using recent advances in <mark>Bayesian deep learning</mark>.<br>",
    "Arabic": "التعلم العميق البايزي",
    "Chinese": "贝叶斯深度学习",
    "French": "apprentissage profond bayésien",
    "Japanese": "ベイズ深層学習",
    "Russian": "байесовское глубокое обучение"
  },
  {
    "English": "bayesian evidence",
    "context": "1: This ameliorates the overfitting problem and effectively reduces the space of possible active source locations by choosing a small relevant subset of location priors that optimizes the <mark>Bayesian evidence</mark> (hence ARD). With this 'learned' prior in place, a once ill-posed inverse problem is no longer untenable, with the posterior mean providing a good estimate of source activity.<br>2: How do we compare between hypotheses that are entirely consistent with observations? The marginal likelihood (aka <mark>Bayesian evidence</mark>), which represents the probability of generating our observations from a prior, provides a distinctive approach to this foundational question, automatically encoding Occam's razor.<br>",
    "Arabic": "الدليل البايزي",
    "Chinese": "贝叶斯证据",
    "French": "preuve bayésienne",
    "Japanese": "ベイズ証拠",
    "Russian": "байесовское свидетельство"
  },
  {
    "English": "bayesian framework",
    "context": "1: Note that u i is a k dimensional latent feature for the i-th subject. In a <mark>Bayesian framework</mark>, we assign a Gaussian prior over U, p(U) = i N (u i |0, I), and specify the rest of the model (see Figure 1) as follows. Continuous data distribution.<br>2: This view enables us to combine the generative model for graph Laplacian regularization with the sparse projection model via a principled hybrid <mark>Bayesian framework</mark> (Lasserre, Bishop, and Minka 2006). To link the two models together, we introduce a prior over H: \n p(H|H) = j N (h j |h j , λI) \n<br>",
    "Arabic": "الإطار البيزياني",
    "Chinese": "贝叶斯框架",
    "French": "cadre bayésien",
    "Japanese": "ベイズ的枠組み",
    "Russian": "байесовская рамка"
  },
  {
    "English": "bayesian game",
    "context": "1: In this section we study the last-iterate convergence of DiL-piKL, establishing that in two-player zero-sum games DiL-piKL converges to the (unique) Bayes-Nash equilibrium of the regularized <mark>Bayesian game</mark>.<br>2: The Bayesian (or expected utility) model adopts a standard <mark>Bayesian game</mark> formulation: vendors have partial information in the form of a commonly-known distribution D over (joint) buyer types, and try to maximize expected utility.<br>",
    "Arabic": "لعبة بايزية",
    "Chinese": "贝叶斯博弈",
    "French": "jeu bayésien",
    "Japanese": "ベイジアンゲーム",
    "Russian": "байесовская игра"
  },
  {
    "English": "bayesian inference",
    "context": "1: Shrinkage is achieved through <mark>Bayesian inference</mark> with a prior distribution designed to moderate the estimated IC values toward the baseline assumption of independence (IC = 0) [1,15]. The advantage of using IC values rather than raw observed-to-expected ratios is that they provide less volatile estimates when little data is available.<br>2: In this example, the geographic hypothesis (b) would be the most plausible one as we can mostly observe regional transitions between restaurants in the data (d). Approach & Methods. The HypTrails approach utilizes a Markov chain model for modeling human trails and <mark>Bayesian inference</mark> for comparing hypotheses.<br>",
    "Arabic": "الاستدلال البايزي",
    "Chinese": "贝叶斯推断",
    "French": "inférence bayésienne",
    "Japanese": "ベイズ推論",
    "Russian": "байесовский вывод"
  },
  {
    "English": "bayesian information Criterion",
    "context": "1: This technique makes use of the <mark>Bayesian Information Criterion</mark> [9], to evaluate the quality of model for different K. The subsequent runs of EM algorithm result in models for which the maximized value of the likelihood function is taken for comparison.<br>",
    "Arabic": "معيار المعلومات البايزي",
    "Chinese": "贝叶斯信息准则",
    "French": "Critère d'information bayésien",
    "Japanese": "ベイズ情報量基準",
    "Russian": "Байесовский информационный критерий"
  },
  {
    "English": "bayesian learning",
    "context": "1: Instead, we propose a new motion segmentation module that produces segmentation masks for supervising our main two-component scene representation. Our idea is inspired by the <mark>Bayesian learning</mark> techniques proposed in recent work [45,79], but integrated into a volumetric IBR representation for dynamic videos.<br>",
    "Arabic": "التعلم البايزي",
    "Chinese": "贝叶斯学习",
    "French": "apprentissage bayésien",
    "Japanese": "ベイズ学習",
    "Russian": "байесовское обучение"
  },
  {
    "English": "bayesian method",
    "context": "1: Variational methods and EB methods are sometimes criticized because of their tendency to understate uncertainty compared with \"fully Bayesian\" methods; see Morris (1983), Wang and Titterington (2005) and references therein for discussion. However, for some applications uncertainty is of secondary importance compared with speed and accuracy of point estimates.<br>2: of solving a nonconvex optimization problem . <mark>Bayesian methods</mark> , by using flexible priors , have the potential to achieve excellent prediction accuracy in both sparse and dense settings ( e.g. , Park and Casella , 2008 ; Hans , 2009 ; Griffin and Brown , 2010 ; Li and Lin , 2010 ; Guan and Stephens , 2011 ; Zhou et al. , 2013 ; Zeng et al. , 2018<br>",
    "Arabic": "الطرق البايزية",
    "Chinese": "贝叶斯方法",
    "French": "méthode bayésienne",
    "Japanese": "ベイズ法",
    "Russian": "байесовский метод"
  },
  {
    "English": "bayesian model",
    "context": "1: Note that in the procedure described above, we approximate CLML purely in function space: the estimate only depends on the predictions made by the <mark>Bayesian model</mark> average and not the values of individual parameters. The standard Laplace approximation of the LML is on the other hand quite sensitive to the number of parameters in the model.<br>2: The game is structured as in the <mark>Bayesian model</mark>, but rather than sampling buyer types from a distribution, arbitrary types from the type space A 1 × • • • × A n are chosen. One plausible vendor objective is to maximize worst-case utility, but such an approach is inappropriate in our setting.<br>",
    "Arabic": "نموذج بايزي",
    "Chinese": "贝叶斯模型",
    "French": "modèle bayésien",
    "Japanese": "ベイズモデル",
    "Russian": "байесовская модель"
  },
  {
    "English": "bayesian optimization",
    "context": "1: In what follows, we demonstrate that CASH can be viewed as a single hierarchical hyperparameter optimization problem, in which even the choice of algorithm itself is considered a hyperparameter. We also show that -based on this problem formulation -recent <mark>Bayesian optimization</mark> methods can obtain high quality results in reasonable time and with minimal human effort.<br>2: More specifically, we show that the recent <mark>Bayesian optimization</mark> procedures TPE [3] and SMAC [14] find combinations of algorithms and hyperparameters that often outperform existing baseline methods, especially on large datasets.<br>",
    "Arabic": "تحسين بايزي",
    "Chinese": "贝叶斯优化",
    "French": "optimisation bayésienne",
    "Japanese": "ベイズ最適化",
    "Russian": "байесовская оптимизация"
  },
  {
    "English": "bayesian perspective",
    "context": "1: 7 can also be derived as a result of an MAP estimate from a <mark>Bayesian perspective</mark>, enforcing a Laplacian distribution on the s (t) 's and assuming L to be a Gaussian matrix with elements drawn i.i.d: \n l ij ⇠ N (0, 2 \n ).<br>",
    "Arabic": "وجهة نظر بيزية",
    "Chinese": "贝叶斯观点",
    "French": "perspective bayésienne",
    "Japanese": "ベイズ的観点",
    "Russian": "байесовская перспектива"
  },
  {
    "English": "bayesian probabilistic model",
    "context": "1: SentiMerge was created by harmonizing and combining three smaller lexicons (PolArt (Klenner et al., 2009); GermanPolarityClues (Waltinger, 2010); and SentiWS (Remus et al., 2010)) using a <mark>Bayesian probabilistic model</mark>. Psycholinguistic features. If available, we extract psycholinguistic ratings along four dimensions for the whole word and its components. This type of feature has been successfully used in various tasks , such as identifying metaphors ( Turney et al. , 2011 ; Klebanov et al. , 2014 ) ; studying persuasion ( Tan et al. , 2016 ) ; sarcasm detection ( Bamman and Smith , 2015 ) ; and , most similar to us , polarity prediction for complex words ( Ruppenhofer<br>",
    "Arabic": "نموذج احتمالي بايزي",
    "Chinese": "贝叶斯概率模型",
    "French": "modèle probabiliste bayésien",
    "Japanese": "ベイズ確率モデル",
    "Russian": "байесовская вероятностная модель"
  },
  {
    "English": "bayesian update",
    "context": "1: this corresponds to <mark>Bayesian updates</mark>, and when initialized with the uniform continuous prior, corresponds to the natural Beta distribution, defined later. The auctioneer chooses a function f (Q jt ) that maps a posterior distribution Q jt to a q value. The function f is chosen based on the revenue guarantees the auctioneer desires.<br>",
    "Arabic": "تحديث بايزي",
    "Chinese": "贝叶斯更新",
    "French": "mise à jour bayésienne",
    "Japanese": "ベイズ更新",
    "Russian": "байесовское обновление"
  },
  {
    "English": "beam search algorithm",
    "context": "1: At inference time, we decode with the <mark>beam search algorithm</mark> using 4 beams and a generation length varying between 3 tokens and 256 tokens. We train each model up to 50 epochs with a delta of 0.05 basis points and a patience of 5 epochs as the early stopping criteria.<br>2: We employ the <mark>beam search algorithm</mark> (Wiseman and Rush, 2016) to find the top-ranked prediction path at inference time. The prediction paths ending with the eos are added to the candidate path set.<br>",
    "Arabic": "خوارزمية البحث بالشعاع",
    "Chinese": "束搜索算法",
    "French": "algorithme de recherche par faisceau",
    "Japanese": "ビームサーチアルゴリズム",
    "Russian": "алгоритм лучевого поиска"
  },
  {
    "English": "beam search decoding",
    "context": "1: The max lengths for source sentences, target sentences, and target syntax are set to 60, 60, and 200, respectively. We set the learning rate to 3 × 10 −5 and consider the Adam optimizer without weight decay. For the <mark>beam search decoding</mark>, the number of beams is set to 4.<br>2: Later studies have also confirmed the existence of this bias in NMT (Koehn and Knowles, 2017;Stahlberg and Byrne, 2019;Kumar and Sarawagi, 2019). Notably, all these studies employ <mark>beam search decoding</mark>.<br>",
    "Arabic": "فك تشفير بحث الحزمة",
    "Chinese": "波束搜索解码",
    "French": "décodage par recherche de faisceaux",
    "Japanese": "ビーム探索デコーディング",
    "Russian": "декодирование поиска луча"
  },
  {
    "English": "beam search decoding algorithm",
    "context": "1: y oracle j−1 = y SO j−1 = y S j−1 (14) \n But a problem comes with sentence-level oracle. As the model samples from ground truth word and the sentence-level oracle word at each step, the two sequences should have the same number of words. However we can not assure this with the naive <mark>beam search decoding algorithm</mark>.<br>",
    "Arabic": "خوارزمية فك تشفير البحث الشعاعي",
    "Chinese": "束搜索解码算法",
    "French": "algorithme de décodage par recherche de faisceau",
    "Japanese": "ビームサーチデコーディングアルゴリズム",
    "Russian": "Алгоритм декодирования с лучевым поиском"
  },
  {
    "English": "beam size",
    "context": "1: OR-NMT: Based on the RNNsearch, we introduced the word-level oracles, sentence-level oracles and the Gumbel noises to enhance the overcorrection recovery capacity. For the sentencelevel oracle selection, we set the <mark>beam size</mark> to be 3, set τ =0.5 in Equation ( 11) and µ=12 for the decay function in Equation ( 15).<br>2: All GPT-3 experiments that we report here in the paper were conducted in January 2023. For Flan-T5, we used a <mark>beam size</mark> of 3 and a maximum target generation length of 256. Other hyperparameters were kept as the default values of T5ConditionalGeneration in the HuggingFace library.<br>",
    "Arabic": "حجم الشعاع",
    "Chinese": "束大小",
    "French": "taille du faisceau",
    "Japanese": "ビームサイズ",
    "Russian": "размер луча"
  },
  {
    "English": "beam width",
    "context": "1: We include screenshots of the human evaluation templates for CommonGen (Figure 5), Constrained scaling factor λ 2 0.6 look ahead step 4 look ahead (greedy) temperature 0 look ahead (beam) <mark>beam width</mark> 4 look ahead (sample) number of sample 15 Question Generation (Figure 6), and RocStories (Figure 7) tasks.<br>2: At each iteration we generate back-translations using beam search, which has been shown to perform well in low-resource settings (Edunov et al., 2018); we use a <mark>beam width</mark> of 5 and individually tune the length-penalty on the dev set.<br>",
    "Arabic": "عرض الحزمة",
    "Chinese": "波束宽度",
    "French": "largeur du faisceau",
    "Japanese": "ビーム幅",
    "Russian": "ширина луча"
  },
  {
    "English": "behavior cloning",
    "context": "1: In general, maximin and minimax leadto different policies. 3. (Robust Policy Improvement) Because of the difference between maximin and minimax in the second point, ATAC recovers <mark>behavior cloning</mark> when the Bellman term is turned off but CQL doesn't. This property is crucial to establishing the robust policy improvement property of ATAC.<br>2: VPT [10] is a concurrent work that learns an inverse dynamics model from human contractors to pseudo-label YouTube videos for <mark>behavior cloning</mark>. VPT is complementary to our approach, and can be finetuned to solve language-conditioned open-ended tasks with our learned reward model.<br>",
    "Arabic": "استنساخ السلوك",
    "Chinese": "行为克隆",
    "French": "clonage comportemental",
    "Japanese": "行動クローニング",
    "Russian": "копирование поведения"
  },
  {
    "English": "behavior policy",
    "context": "1: (6), the first two terms are controlled by the Bellman error (both onsupport and off-support), and the third is controlled by the optimization error. Notably, when the comparator π is the <mark>behavior policy</mark> µ, the first two terms in Eq. ( 6) cancel out, giving the faster rate of Proposition 6.<br>2: On the theoretical side, some works (Laroche et al., 2019;Kumar et al., 2019;Fujimoto et al., 2018) provide safe policy improvement guarantees, meaning that the algorithms always do at least as well as the <mark>behavior policy</mark>, while improving upon it when possible.<br>",
    "Arabic": "سياسة السلوك",
    "Chinese": "行为策略",
    "French": "politique comportementale",
    "Japanese": "振る舞いポリシー",
    "Russian": "поведенческая стратегия"
  },
  {
    "English": "belief propagation",
    "context": "1: Given an observed ensemble, we seek a hidden database ensemble ,which maximizes its MAP (maximum aposterior probability) assignment. This is done using the above statistical model, which has a simple and exact <mark>Belief Propagation</mark> algorithm (Yedidia et al., 2003). According to Eq. (5) the MAP assignment can be written as: \n<br>2: The TRW algorithm is a variant of <mark>Belief Propagation</mark> that solves the resulting LP and has been shown to be significantly faster for this problem structure than off-the-shelf LP solvers (Yanover and Meltzer 2006). The solution given by TRW provides an upper bound on the solution of our score maximization problem.<br>",
    "Arabic": "انتشار الاعتقادات",
    "Chinese": "信念传播",
    "French": "propagation des croyances",
    "Japanese": "信念伝播 (Shin'nen Denpa)",
    "Russian": "распространение убеждений"
  },
  {
    "English": "belief state",
    "context": "1: Using an actor-critic formulation, the 512-dimensional <mark>belief state</mark> b t is passed through a 1-linear layer, representing the actor, to get a 6-dimensional vector, where each entry represents an action. The 6-dimensional vector is passed through a softmax function to obtain the agent's policy π (i.e. the probability distribution over the action space).<br>2: , o i t ), agent i executes actions dictated by its policy a i = π i (h i t ). The joint policy is denoted by π = π 1 , . . . , π n and parameterized by θ. It may sometimes be desirable to use a recurrent policy representation ( e.g. , recurrent neural network ) to compute an internal state h t that compresses the observation history , or to explicitly compute a <mark>belief state</mark> ( probability distribution over states ) ; with abuse of notation , we use h t to refer to all such variations of internal states/observation histories<br>",
    "Arabic": "حالة الاعتقاد",
    "Chinese": "信念状态",
    "French": "état de croyance",
    "Japanese": "信念状態",
    "Russian": "состояние убеждения"
  },
  {
    "English": "benchmark",
    "context": "1: Note also that all of the parameters for our system were fixed before running the <mark>benchmark</mark> evaluation and were therefore not optimized for <mark>benchmark</mark> performance in any way. Although the shapes in both SCAPE and TOSCA datasets have the same connectivity structure, this information is not used by our method, and is not needed for applying our algorithm.<br>2: More generally, we find that learning methods that leverage the 3D geometry of molecules hold state-of-the-art on the majority of tasks on our <mark>benchmark</mark> (Table 7).<br>",
    "Arabic": "المعيار",
    "Chinese": "基准",
    "French": "référence",
    "Japanese": "ベンチマーク",
    "Russian": "бенчмарк"
  },
  {
    "English": "benchmark dataset",
    "context": "1: Despite the foundational role of benchmarking practices in this field, relatively little attention has been paid to the dynamics of <mark>benchmark dataset</mark> use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020.<br>2: Naturally, this results in a lower accuracy, in particular when used with noisy detections and when tracking fast moving objects in a <mark>benchmark dataset</mark> such as KITTI.<br>",
    "Arabic": "مجموعة بيانات معيارية",
    "Chinese": "基准数据集",
    "French": "ensemble de données de référence",
    "Japanese": "ベンチマークデータセット",
    "Russian": "эталонный набор данных"
  },
  {
    "English": "benchmark task",
    "context": "1: Dataset We evaluate our proposed method on five continual test-time adaptation and domain generalization <mark>benchmark tasks</mark>: CIFAR10-to-CIFAR10C(standard and gradual), CIFAR100-to-CIFAR100C and ImageNet-to-ImageNet-C. Moreover, to explore the ability to deal with the actual domain gap, we also evaluate our method on VLCS. Task setting We follow CoTTA (Wang et al.<br>",
    "Arabic": "مهمة معيارية",
    "Chinese": "基准任务",
    "French": "tâche de référence",
    "Japanese": "ベンチマークタスク",
    "Russian": "эталонная задача"
  },
  {
    "English": "beta distribution",
    "context": "1: Further, if the advertiser is certain about its CTR, and if this CTR is drawn from the auctioneer's prior which follows a <mark>Beta distribution</mark>, then the worst case loss in revenue of the auctioneer over pure per-click bidding is at most 1/e ≈ 37%.<br>2: The \"explore\" phase stops when the auctioneer's posterior mean of the distribution Beta(α + n, β + T − n) is at least p(1 − ǫ). Note that this also implies that the Gittins index for Beta ( α + n , β + T − n ) is at least p ( 1 − ǫ ) irrespective of the discount factor γ ; this in turn implies that by switching to pure per-click bidding , the advertiser is assured that q ≥ p ( 1 − ǫ ) ,<br>",
    "Arabic": "توزيع بيتا",
    "Chinese": "贝塔分布",
    "French": "distribution bêta",
    "Japanese": "ベータ分布",
    "Russian": "распределение Бета"
  },
  {
    "English": "beta1",
    "context": "1: All networks are trained using ADAM [11] solver with <mark>beta1</mark> = 0.5 for 1 million iterations. The learning rate was set to 1e −4 for all components except L D-mask , where we set it to a smaller learning rate of 1e −5 . The different learning rates help us to stabilize the mask network.<br>2: We use a batch size of 256 for models with fewer than 2 billion parameters, 512 for models with 2 -5 billion parameters and 1024 for models with more than 5 billion parameters. All models are trained in bfloat16 precision using the Adam optimizer [48] with eps = 1e − 8, <mark>beta1</mark> = 0.9.<br>",
    "Arabic": "بيتا1",
    "Chinese": "beta1",
    "French": "bêta1",
    "Japanese": "ベータ1",
    "Russian": "бета1"
  },
  {
    "English": "between-class variance",
    "context": "1: We then compute the fisher discriminant score (ratio of within-class variance to <mark>between-class variance</mark>) for each dimension as measures of the discrimination power realized by the representations.<br>",
    "Arabic": "التباين بين الفئات",
    "Chinese": "类间方差",
    "French": "variance inter-classe",
    "Japanese": "クラス間分散",
    "Russian": "межклассовая дисперсия"
  },
  {
    "English": "betweenness",
    "context": "1: In order to speed up the processing efficiency, instead of traversing every edge in the network globally, a parameter h is given to limit the search region locally when updating the <mark>betweenness</mark> after an edge was removed or a node was split. Document Clustering.<br>",
    "Arabic": "الوساطة",
    "Chinese": "介数",
    "French": "intermédiarité",
    "Japanese": "媒介中心性",
    "Russian": "промежуточность"
  },
  {
    "English": "bi-gram",
    "context": "1: While various settings of bag-of-words features such as <mark>bi-gram</mark> or tri-gram can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. • Advanced architectures such as BERT may only achieve the best results if properly used.<br>2: For example, LibMultiLabel only uses uni-gram, while Chalkidis et al. (2022) set ngram_range to (1, 3), so uni-gram, <mark>bi-gram</mark>, and tri-gram are extracted into the vocabulary list for a richer representation of the document. min_df: The parameter is used for removing infrequent tokens. Chalkidis et al.<br>",
    "Arabic": "ثنائي الكلمات",
    "Chinese": "二元语法",
    "French": "bi-gramme",
    "Japanese": "バイグラム",
    "Russian": "биграмма"
  },
  {
    "English": "bi-level optimization",
    "context": "1: This design makes the entire framework differentiable to layer weights and architectural parameters α o i,j so that it can perform architecture searches in an end-to-end fashion. The standard optimization method is the <mark>bi-level optimization</mark> proposed in DARTS. After the search process is completed, the discretization procedure extracts the final subnetwork by dropping the operations receiving lower scores.<br>2: SNAS (Xie et al., 2019) reformulate DARTS as a credit assignment task while maintaining the differentiability. P-DARTS (Chen et al., 2021) analyze the issues during the DARTS <mark>bi-level optimization</mark>, and propose a series of modifications.<br>",
    "Arabic": "تحسين ثنائي المستوى",
    "Chinese": "双层优化",
    "French": "optimisation bi-niveau",
    "Japanese": "バイレベル最適化",
    "Russian": "двухуровневая оптимизация"
  },
  {
    "English": "bias",
    "context": "1: While empirical results suggest that many models can be efficiently sampled asynchronously, traditional Markov chain analysis does not apply to the asynchronous case, and thus asynchronous Gibbs sampling is poorly understood. In this paper, we derive a better understanding of the two main challenges of asynchronous Gibbs: <mark>bias</mark> and mixing time.<br>2: See Figure 5 for results. Observe that overlap in the distributions serves as a proxy for possible inversions in model ordering (by <mark>bias</mark>) depending on the subsample of template occupation words used.<br>",
    "Arabic": "الانحياز",
    "Chinese": "偏差",
    "French": "biais",
    "Japanese": "バイアス",
    "Russian": "смещение"
  },
  {
    "English": "bias mitigation",
    "context": "1: The APIs we use are distinct from the models documented in that paper, so it is hard to draw any concrete conclusions about underlying mechanisms. Transparency about safeguards and <mark>bias mitigation</mark> would enable researchers and practitioners to more easily understand the benefits and limitations of these methods.<br>2: In this paper, we propose a method for reducing bias in machine learning classifiers without relying on protected attributes. In contrast to previous work, our method eliminates the need to specify which biases are to be mitigated, and allows simultaneous mitigation of multiple biases, including those that relate to group intersections.<br>",
    "Arabic": "التخفيف من التحيز",
    "Chinese": "偏见缓解",
    "French": "atténuation des biais",
    "Japanese": "バイアス緩和",
    "Russian": "уменьшение предвзятости"
  },
  {
    "English": "bias parameter",
    "context": "1: After training on D train , the model is few-shot evaluated on novel tasks T test given a support set S Ttest . We first perform adaptation of the model by fine-tuning <mark>bias parameters</mark> of the image encoder f T using the support set S Ttest .<br>2: where W (ci−1,ci) ∈ R k×N and b (ci−1,ci) ∈ R k×N are the weight and <mark>bias parameters</mark> specific to the labels c i−1 and c i , k is the number of event types, and N is the input length of text.<br>",
    "Arabic": "\"معامل الانحياز\"",
    "Chinese": "偏置参数",
    "French": "paramètre de biais",
    "Japanese": "バイアスパラメータ",
    "Russian": "параметр смещения"
  },
  {
    "English": "bias term",
    "context": "1: where y = (y 1 , …, y g ) T , 2 i w is the weight vector and 2 i b is the <mark>bias term</mark>. The newly classified vector x is classified according to the maximum of outputs. We used the Matlab neural network (NN) toolbox to form the RBF networks.<br>2: We found that including an unregularized <mark>bias term</mark> does not significantly change the predictive performance for any of the data sets used. Furthermore, most methods we compare to, including [21,24,37,18], do not incorporate a <mark>bias term</mark> either. Nonetheless, there are clearly learning problems where the incorporation of the <mark>bias term</mark> could be beneficial.<br>",
    "Arabic": "\"حد التحيز\"",
    "Chinese": "偏置项",
    "French": "terme de biais",
    "Japanese": "バイアス項",
    "Russian": "смещение"
  },
  {
    "English": "bias vector",
    "context": "1: First, absorb the <mark>bias vector</mark> into the weight matrix-by defining the extended weight matrix W = [W , b] ∈ R C×(P +1) and the extended feature vector h i,c = [h i,c ; 1] ∈ R P +1 -so that Equation 2 can be rewritten as \n<br>2: As illustrated in Figure 3, we first transform the <mark>bias vector</mark>r = r y − r x according to a predefined scale vector ω, that is ω ⊙r, where ⊙ is the element-wise product operation.<br>",
    "Arabic": "متجه التحيز",
    "Chinese": "偏置向量",
    "French": "vecteur de biais",
    "Japanese": "バイアスベクトル",
    "Russian": "вектор смещения"
  },
  {
    "English": "bias-variance tradeoff",
    "context": "1: Intriguingly, we find that the <mark>bias-variance tradeoff</mark> in this setting manifests itself not through convergence rates, but through different local minima. This shows that the two estimators may fundamentally operate on different landscapes implicitly. The presence of discontinuities need not indicate that we need to commit ourselves to uniformly using one of the estimators.<br>2: The benefit of this new decomposition is that instead of performing importance sampling on Y , we can now use IS estimators, each with a better <mark>bias-variance tradeoff</mark>. Unbiased estimation of V (π) in Eq. (5) via importance sampling still requires Assumption 1. The logging policy must therefore explore a large action space.<br>",
    "Arabic": "التوازن بين الانحياز والتباين",
    "Chinese": "偏差-方差权衡",
    "French": "compromis biais-variance",
    "Japanese": "バイアスと分散のトレードオフ",
    "Russian": "компромисс смещение-дисперсия"
  },
  {
    "English": "biased estimator",
    "context": "1: The last column in Table 1 lists asymptotic variances of corresponding tricks when un<mark>biased estimators</mark> of f (Z) are passed through the function f −1 to yield (biased, but consistent and non-negative) estimators of Z itself.<br>",
    "Arabic": "تقدير متحيز",
    "Chinese": "有偏估计量",
    "French": "estimateur biaisé",
    "Japanese": "バイアスのある推定値",
    "Russian": "Смещенная оценка"
  },
  {
    "English": "bibliographic coupling",
    "context": "1: What notions of connectivity (besides weak and strong) might be appropriate for the web graph? For instance, what is the structure of the undirected graph induced by the co-citation relation or by <mark>bibliographic coupling</mark> [White and McCain89]. 3.<br>",
    "Arabic": "\"الاقتران الببليوغرافي\"",
    "Chinese": "文献耦合",
    "French": "couplage bibliographique",
    "Japanese": "書誌結合",
    "Russian": "библиографическая связь"
  },
  {
    "English": "bicubic interpolation",
    "context": "1: The input resolution is 160 × 256, which is different from CLIP's default 224 × 224 resolution. We adapt the positional embeddings via <mark>bicubic interpolation</mark>, which does not introduce any new learnable parameters. Temporal aggregator φ a Given a sequence of frame-wise RGB features, a temporal aggregator network summarizes the sequence into one video embedding.<br>2: We first write the bilinear or <mark>bicubic interpolation</mark> as a sum over features F k on the discrete grid: \n F [p] = k w k F k with k w k = 1 . (8 \n ) \n We assume that the features are L2-normalized F k = 1, such that F [p] ≈ 1.<br>",
    "Arabic": "الاستيفاء التكعيبي",
    "Chinese": "双三次插值",
    "French": "interpolation bicubique",
    "Japanese": "双三次補間",
    "Russian": "бикубическая интерполяция"
  },
  {
    "English": "bidirectional",
    "context": "1: context2vec (Melamud et al., 2016) uses a <mark>bidirectional</mark> Long Short Term Memory (LSTM; Hochreiter and Schmidhuber, 1997) to encode the context around a pivot word.<br>2: For a fair comparison with previous work, we do not use the lexicon/copying. We also evaluate a version of our model without RoBERTa that uses a <mark>bidirectional</mark> LSTM and GloVe embeddings instead. This mirrors the model of L'23. Table 3 shows mean accuracy and standard deviations over 5 runs.<br>",
    "Arabic": "ثنائي الاتجاه",
    "Chinese": "双向的",
    "French": "bidirectionnel",
    "Japanese": "双方向性",
    "Russian": "двунаправленный"
  },
  {
    "English": "bidirectional Transformer",
    "context": "1: Furthermore, the fusion module and pointer network consist of two and one layers of randomly initialized <mark>bidirectional Transformer</mark> blocks (Vaswani et al., 2017), respectively. We conduct experiments on one RTX 6000 GPU. In addition, we build the style classifier based on the encoder of LongLM BASE and T5 BASE for Chinese and English, respectively.<br>2: We conduct all experiments on BERT (Devlin et al. 2019), which is one of the most successful applications of Transformer. The pretrained language model is based on <mark>bidirectional Transformer</mark>, which can be fine-tuned towards downstream tasks. Notice that our method can also be applied to other multi-layer Transformer models with few modifications.<br>",
    "Arabic": "مُحوِّل ثُنائيّ الاتجاه",
    "Chinese": "双向Transformer",
    "French": "Transformer bidirectionnel",
    "Japanese": "双方向Transformer",
    "Russian": "бидирекциональный Трансформер"
  },
  {
    "English": "bidirectional encoder",
    "context": "1: These result are overall more solid than the ones obtained by de Varda and Marelli (2022), who did not report significant partial effects of surprisal on FF and GD in some of the languages considered. The authors derived their probabilistic estimates employing mBERT, a <mark>bidirectional encoder</mark>.<br>",
    "Arabic": "مشفر ثنائي الاتجاه",
    "Chinese": "双向编码器",
    "French": "encodeur bidirectionnel",
    "Japanese": "双方向エンコーダ",
    "Russian": "двунаправленный энкодер"
  },
  {
    "English": "bidirectional heuristic search",
    "context": "1: A*), <mark>bidirectional heuristic search</mark> (Bi-HS), and bidirectional brute-force search (Bi-BS) has two main conclusions (for caveats, see their Section 3): \n<br>2: MM is the very first <mark>bidirectional heuristic search</mark> algorithm that is guaranteed to meet in the middle. Papers on traditional front-to-front 1 <mark>bidirectional heuristic search</mark> typi- 1 In <mark>bidirectional heuristic search</mark>, there are two different ways to define the heuristic function (Kaindl and Kainz 1997).<br>",
    "Arabic": "البحث الإرشادي ثنائي الاتجاه",
    "Chinese": "双向启发式搜索",
    "French": "recherche heuristique bidirectionnelle",
    "Japanese": "双方向ヒューリスティック探索",
    "Russian": "двунаправленный эвристический поиск"
  },
  {
    "English": "bidirectional model",
    "context": "1: Making a <mark>bidirectional model</mark> at the scale of GPT-3, and/or trying to make <mark>bidirectional model</mark>s work with few-or zero-shot learning, is a promising direction for future research, and could help achieve the \"best of both worlds\".<br>2: We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data, fine-tuning scheme, and hyperparameters as BERT BASE : \n No NSP: A <mark>bidirectional model</mark> which is trained using the \"masked LM\" (MLM) but without the \"next sentence prediction\" (NSP) task.<br>",
    "Arabic": "نموذج ثنائي الاتجاه",
    "Chinese": "双向模型",
    "French": "modèle bidirectionnel",
    "Japanese": "双方向モデル",
    "Russian": "двунаправленная модель"
  },
  {
    "English": "bidirectional search",
    "context": "1: Before the interest on <mark>bidirectional search</mark> was rekindled, Sadhukhan proposed an interesting bidirectional best-first algorithm based on accumulated errors along paths, BAE * 1 , first at a national conference (Sadhukhan 2012) and later in a journal article (Sadhukhan 2013) with a correction on the latter that drops the need for symmetric heuristics (Sadhukhan 2015).<br>2: We also evaluated the same consistent operator-potential heuristics with the tasks transformed to the transition normal form  (prefixed with tnf-); and the path-dependent operator-potential heuristics on the original planning task (prefixed with pd-). We compare these to symbolic uniform-cost search using forward (bfw) and <mark>bidirectional search</mark> (bbi) 4 .<br>",
    "Arabic": "البحث ثنائي الاتجاه",
    "Chinese": "双向搜索",
    "French": "recherche bidirectionnelle",
    "Japanese": "双方向探索",
    "Russian": "двунаправленный поиск"
  },
  {
    "English": "bidirectionality",
    "context": "1: This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [RSR + 19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from <mark>bidirectionality</mark>.<br>2: To isolate the effect of these differences, we perform ablation experiments in Section 5.1 which demonstrate that the majority of the improvements are in fact coming from the two pre-training tasks and the <mark>bidirectionality</mark> they enable.<br>",
    "Arabic": "ثنائية الاتجاه",
    "Chinese": "\"双向性\"",
    "French": "bidirectionnalité",
    "Japanese": "双方向性",
    "Russian": "двунаправленность"
  },
  {
    "English": "big-o notation",
    "context": "1: The hidden constants in the <mark>big-O notation</mark> depend inversely on δ and the probability of success. A striking feature of these constructions is the very mild dependence of m on the ambient dimension d. This translates to a significant savings in the number of learning problems one has to solve after employing our reduction.<br>",
    "Arabic": "تدوين Big-O",
    "Chinese": "大O表示法",
    "French": "notation grand O",
    "Japanese": "ビッグオー表記",
    "Russian": "нотация большого О"
  },
  {
    "English": "bigram count",
    "context": "1: At the heart of the algorithm presented here is the reduced-rank SVD method of Schütze (1995), which transforms <mark>bigram counts</mark> into latent descriptors. In view of the present work, which achieves state-of-the-art performance when evaluation is done with the criteria now in common use, Schütze's original work should rightly be praised as ahead of its time.<br>",
    "Arabic": "عدد الثنائيات",
    "Chinese": "双字计数",
    "French": "compte de bigrammes",
    "Japanese": "バイグラム数",
    "Russian": "количество биграмм"
  },
  {
    "English": "bigram language model",
    "context": "1: Clusters are created using word bigram features after replacing numbers and proper names with tags NUM and PROP. The emissions are given by a <mark>bigram language model</mark> on words from the clustered sentences. Barzilay and Lee (2004) also employ an iterative clustering procedure before finalizing the states of the HMM but our method only uses one-step clustering.<br>",
    "Arabic": "\"نموذج لغة ثنائي الكلمات\"",
    "Chinese": "双元语言模型",
    "French": "modèle de langage bigramme",
    "Japanese": "バイグラム言語モデル",
    "Russian": "биграммная языковая модель"
  },
  {
    "English": "bijective function",
    "context": "1: The discretized equation (1) also appears in normalizing flows (Rezende and Mohamed, 2015) and the NICE framework (Dinh et al., 2014). These methods use the change of variables theorem to compute exact changes in probability if samples are transformed through a <mark>bijective function</mark> f : \n<br>",
    "Arabic": "دالة ثنائية التَّرابُط",
    "Chinese": "双射函数",
    "French": "fonction bijective",
    "Japanese": "全単射関数",
    "Russian": "биективная функция"
  },
  {
    "English": "bijective mapping",
    "context": "1: • For a general graph generation policy π, there no longer exists an explicit <mark>bijective mapping</mark> between nodes and subgraphs. In this case, another possible way is to define χ G (v) := {{χ Gi (v) : G i ∈ B π G }}, similar to DSS-WL.<br>2: To set the stage for functional mappings as a generalization of classical point-to-point mappings, let T : M ! N be a <mark>bijective mapping</mark> between manifolds M and N (either continuous or discrete). Then, T induces a natural transformation of derived quantities, such as functions on M .<br>",
    "Arabic": "التصوير الثنائي",
    "Chinese": "一一对应映射",
    "French": "application bijective",
    "Japanese": "全単射写像",
    "Russian": "взаимно-однозначное отображение"
  },
  {
    "English": "bilateral filtering",
    "context": "1: Before visualisation, high frequency elements are removed from the depth maps with <mark>bilateral filtering</mark> and highly slanted mesh elements are cropped. Being exposed to a large variety of depth images during training, the proposed network embeds geometry priors in its weights.<br>2: In [22,25], BCL was proposed as a learnable generalization of <mark>bilateral filtering</mark> [43,2], hence the name 'Bilateral Convolution Layer'. Bilateral filtering involves a projection of a given 2D image into a higher-dimensional space (e.g., space defined by position and color) and is traditionally limited to hand-designed filter kernels.<br>",
    "Arabic": "التصفية الثنائية",
    "Chinese": "双边滤波",
    "French": "filtrage bilatéral",
    "Japanese": "両側フィルタリング",
    "Russian": "билатеральная фильтрация"
  },
  {
    "English": "bilinear",
    "context": "1: Our most consistent result seems to be that all probes, whether linear, <mark>bilinear</mark>, or multi-layer perceptron, are over-parameterized and needlessly high-capacity if using defaults like full-rank weight matrices, hidden states with a few hundred dimensions, and moderate dropout.<br>2: The only techniques with significantly faster runtimes than the bilateral solver are standard image interpolation techniques (<mark>bilinear</mark>, bicubic, etc) which produce low-quality output, and our implementation of the domain transform [9] which is a highly-optimized, vectorized, and multi-threaded implementation, unlike our own technique and all other techniques we compare against.<br>",
    "Arabic": "ثنائي الخطية",
    "Chinese": "双线性",
    "French": "bilinéaire",
    "Japanese": "双線形",
    "Russian": "билинейный"
  },
  {
    "English": "bilinear form",
    "context": "1: V [i] ∈ R d×d : h = b c T V [1:d] b c ; h i = b c T V [i] b c . where V [1:d] ∈ R 2d×2d×d is the tensor that defines multiple <mark>bilinear forms</mark>. The RNTN uses this definition for computing p 1 :<br>2: Golub and Meurant developed the well-known Gauss Quadrature and Lanczos (GQL) algorithm to approximate <mark>bilinear forms</mark> for smooth functions of a matrix [21]. Using the same stochastic estimation from §3.1, we can also apply GQL to compute DOS. For a starting vector z and graph matrix H , Lanczos iterations after M steps produce a decomposition \n<br>",
    "Arabic": "صيغة ثنائية الخطية",
    "Chinese": "双线性形式",
    "French": "forme bilinéaire",
    "Japanese": "双一次形式",
    "Russian": "билинейная форма"
  },
  {
    "English": "bilinear interpolation",
    "context": "1: We first write the bilinear or bicubic interpolation as a sum over features F k on the discrete grid: \n F [p] = k w k F k with k w k = 1 . (8 \n ) \n We assume that the features are L2-normalized F k = 1, such that F [p] ≈ 1.<br>2: We use <mark>bilinear interpolation</mark> [22] to compute the exact values of the input features at four regularly sampled locations in each RoI bin, and aggregate the result (using max or average), see Figure 3 for details.<br>",
    "Arabic": "الاستيفاء الثنائي",
    "Chinese": "双线性插值",
    "French": "interpolation bilinéaire",
    "Japanese": "バイリニア補間",
    "Russian": "билинейная интерполяция"
  },
  {
    "English": "bilinear model",
    "context": "1: (2014) for QA, scores an answer a i with a <mark>bilinear model</mark>: qW a i , where the question q and answers a i are the average pre-trained word embeddings and W is a learned parameter matrix. A softmax layer over the candidate answers is used to train the model with cross-entropy loss.<br>",
    "Arabic": "نموذج ثنائي الخطية",
    "Chinese": "双线性模型",
    "French": "modèle bilinéaire",
    "Japanese": "バイリニアモデル",
    "Russian": "билинейная модель"
  },
  {
    "English": "bilingual model",
    "context": "1: It is measured for a specific translation direction s → t by the relative difference in performance (test-set cross-entropy loss) between a <mark>bilingual model</mark> trained to translate only from s to t (L bi s→t ) and a multilingual counterpart that is trained to translate other additional directions (L multi s→t ): \n<br>2: Furthermore, we automatically label a moderate-sized set of 80k sentence pairs using our <mark>bilingual model</mark>, and train new monolingual models using an uptraining scheme. The resulting monolingual models demonstrate an error reduction of 9.2% over the Stanford NER systems for Chinese. 2<br>",
    "Arabic": "نموذج ثنائي اللغة",
    "Chinese": "双语模型",
    "French": "modèle bilingue",
    "Japanese": "双言語モデル",
    "Russian": "двуязычная модель"
  },
  {
    "English": "binarization",
    "context": "1: Cues. These act on shape boundaries, produced by <mark>binarization</mark>, to propose text characters. They use shape context cues (Belongie et al., 2004). and information features (Tu and Yuille, 2004) to propose matches between the shape boundaries and the deformable template models of text characters.<br>2: This section describes how we use AdaBoost techniques to compute discriminative probabilities for detecting faces and text (strings of letters). We also describe the <mark>binarization</mark> algorithm used to detect the boundaries of text characters.<br>",
    "Arabic": "ثنائية",
    "Chinese": "二值化",
    "French": "binarisation",
    "Japanese": "二値化",
    "Russian": "бинаризация"
  },
  {
    "English": "binary atom",
    "context": "1: . . }. A unary atom has one argument and a <mark>binary atom</mark> has two. A formula combines atoms with connectives (e.g., ∧, ⇔). A formula is ground if it contains no logical variables. The groundings of a formula are obtained by instantiating the variables with particular constants.<br>",
    "Arabic": "ذرّة ثنائيّة",
    "Chinese": "二元原子",
    "French": "atome binaire",
    "Japanese": "バイナリ原子",
    "Russian": "бинарный атом"
  },
  {
    "English": "binary classification",
    "context": "1: Logistic regression has been successfully applied to <mark>binary classification</mark> problems, particularly to credit risk assessment [3], it does not require a validation set for over-fitting prevention and it presents explicitly the knowledge extracted from data in terms of statistically validated coefficients [10].<br>2: [13] provided upper bounds for <mark>binary classification</mark> multi-distribution learning that are identical to their upper bounds in Table 1 but replacing log(|H|) with VC(H). We now show a similar result to Theorem 5.1 also holds with dependence on the VC dimension of H only when additional mild assumptions hold.<br>",
    "Arabic": "تصنيف ثنائي",
    "Chinese": "二分类",
    "French": "classification binaire",
    "Japanese": "二値分類 (nibun bunrui)",
    "Russian": "бинарная классификация"
  },
  {
    "English": "binary classification head",
    "context": "1: For the news dataset, we follow the protocol of [61], i.e., a Grover mega model finetuned with a <mark>binary classification head</mark>. Results with other discriminators are reported in Appendix D.<br>",
    "Arabic": "رأس التصنيف الثنائي",
    "Chinese": "二元分类头",
    "French": "tête de classification binaire",
    "Japanese": "バイナリ分類ヘッド",
    "Russian": "бинарный классификационный блок"
  },
  {
    "English": "binary classification problem",
    "context": "1: where p (d) is an instance of the model-checking problem (concatenation of the structure M A and sentence s ϕ delimited by a separator SEP token), and ℓ ∈ {T rue, F alse} is the label. The task is to correctly predict the label ℓ, thereby reducing it to a <mark>binary classification problem</mark>.<br>",
    "Arabic": "مشكلة التصنيف الثنائي",
    "Chinese": "二分类问题",
    "French": "problème de classification binaire",
    "Japanese": "二項分類問題",
    "Russian": "проблема бинарной классификации"
  },
  {
    "English": "binary classification task",
    "context": "1: So we are not surprised Clippy can help in the production model which is much more complex than Large+DCN. 3) bottom hidden layer of shared bottom, and in the output layers of (4) <mark>binary classification task</mark> and (5) regression task.<br>2: The workers were rewarded $0.5 for each approved HIT. Inter-participant cluster similarity: To investigate the level of similarity between the questions groups (further referred to as clusters) created by different participants, we study the agreement between workers, framing the clustering problem as a <mark>binary classification task</mark> [2].<br>",
    "Arabic": "مهمة التصنيف الثنائي",
    "Chinese": "二分类任务",
    "French": "tâche de classification binaire",
    "Japanese": "二項分類タスク",
    "Russian": "задача бинарной классификации"
  },
  {
    "English": "binary classifier",
    "context": "1: Finally, for both NSFG and ESFG, a <mark>binary classifier</mark> is applied to map the embedding of each variable to its assignment. We use the binary cross entropy (BCE) as loss function, which can be written as \n<br>2: Our experiments confirm this distinction does indeed matter in practice, as our learnt ranking function is more effective at capturing the relative strengths of the attributes than the score of a <mark>binary classifier</mark> (i.e., the magnitude of the SVM decision function).<br>",
    "Arabic": "مُصنِّف ثُنائي",
    "Chinese": "二分类器",
    "French": "classificateur binaire",
    "Japanese": "2値分類器",
    "Russian": "бинарный классификатор"
  },
  {
    "English": "binary constraint",
    "context": "1: A decomposition of a global constraint that uses <mark>binary constraints</mark> with small and general explanations could be better than a global constraint with poor explanations but better filtering. Without nogoods, one would need to balance the running time of an algorithm with its filtering strength. With lazy clause generation, one must also consider the quality of the explanations.<br>2: Our approach represents a set of images as a graph modeling geometric constraints between pairs of cameras or between cameras and scene points (as <mark>binary constraints</mark>), as well as single-camera pose information such as geotags (as unary constraints).<br>",
    "Arabic": "قيد ثنائي",
    "Chinese": "二元约束",
    "French": "contrainte binaire",
    "Japanese": "2値制約",
    "Russian": "бинарное ограничение"
  },
  {
    "English": "binary cross entropy",
    "context": "1: Furthermore, we exploit the known ground truth segmentation masks and minimize the <mark>binary cross entropy</mark> between the alpha mask returned by the raymarcher of NV and the ground truth mask M tgt .<br>2: Given a dataset of human preferences over model responses, DPO can therefore optimize a policy using a simple <mark>binary cross entropy</mark> objective, producing the optimal policy to an implicit reward function fit to the preference data. Our main contribution is Direct Preference Optimization (DPO), a simple RL-free algorithm for training language models from preferences.<br>",
    "Arabic": "الإنتروبيا الثنائية المتقاطعة",
    "Chinese": "二元交叉熵",
    "French": "entropie croisée binaire",
    "Japanese": "2値クロスエントロピー",
    "Russian": "бинарная кросс-энтропия"
  },
  {
    "English": "binary cross-entropy loss",
    "context": "1: The output of the multi-layer network is then σ(v • g(W X)) Our parameter space is then X n = R 2N +2 and we therefore take n = 2N + 2 when applying Theorem 2.3. As we are interested in supervised classification, we take the usual <mark>binary cross-entropy loss</mark> with ℓ 2 regularization.<br>2: The transformer models are fine-tuned to predict the target label (T rue or F alse) by optimising for the <mark>binary cross-entropy loss</mark> over the targets using the Adam optimiser (Kingma and Ba, 2015).<br>",
    "Arabic": "خسارة الانتروبيا الثنائية المتقاطعة",
    "Chinese": "二进制交叉熵损失",
    "French": "perte d'entropie croisée binaire",
    "Japanese": "バイナリクロスエントロピー損失",
    "Russian": "бинарная кросс-энтропийная потеря"
  },
  {
    "English": "binary decision tree",
    "context": "1: Thus, we conclude that l← = e h proving the correctness of Algorithm 1. Algorithm 1 represents a general technique to compute the output value of a single <mark>binary decision tree</mark> stored as a set of precomputed bitvectors.<br>2: We prove that for each <mark>binary decision tree</mark> T h and input feature vector x, Algorithm 1 always computes Algorithm 1: Scoring a feature vector x using a <mark>binary decision tree</mark> T h Input : \n<br>",
    "Arabic": "شجرة القرار الثنائية",
    "Chinese": "二叉决策树",
    "French": "arbre de décision binaire",
    "Japanese": "バイナリ決定木",
    "Russian": "двоичное дерево решений"
  },
  {
    "English": "binary feature",
    "context": "1: We wish to estimate a simple reward function: r = 1 if the agent wins the game and r = 0 otherwise. The value function is approximated by a linear combination of <mark>binary features</mark> φ with weights θ, \n Q RLGO (s, a) = σ φ(s, a) T θ \n<br>2: In the game of Go, the notion of shape has strategic importance. For this reason we use <mark>binary features</mark> φ(s, a) that recognise local patterns of stones (Silver et al., 2007).<br>",
    "Arabic": "ميزة ثنائية",
    "Chinese": "二值特征",
    "French": "caractéristique binaire",
    "Japanese": "2値特徴",
    "Russian": "бинарный признак"
  },
  {
    "English": "binary label",
    "context": "1: The target variable for our classifier is a <mark>binary label</mark> corresponding to whether or not the shared-task system has made an error on the input token. This requires comparing the outputs of a system to the gold data and classifying each token as either correct or incorrect. We will also refer to the latter as the error class.<br>2: BCE(y, p) = −(y log(p) + (1 − y) log(1 − p)), (5 \n ) \n where p ∈ [0, 1] is the predicted probability of a variable being assigned True, and y is the <mark>binary label</mark> from an optimal solution.<br>",
    "Arabic": "التصنيف الثنائي",
    "Chinese": "二值标签",
    "French": "étiquette binaire",
    "Japanese": "2値ラベル",
    "Russian": "бинарная метка"
  },
  {
    "English": "binary matrix",
    "context": "1: coordinates x n×4 in layer L 1 ; 2 the operator T p×c is a <mark>binary matrix</mark> that enforces a selective neuron feed from L 1 to the next network layer L 2 , forming convex parts ; 3 finally , layer L 3 assembles the parts into a shape via either sum or min-pooling .<br>",
    "Arabic": "مصفوفة ثنائية",
    "Chinese": "二进制矩阵",
    "French": "matrice binaire",
    "Japanese": "2値行列",
    "Russian": "бинарная матрица"
  },
  {
    "English": "binary predicate",
    "context": "1: The PDDL encoding has a unary predicate tightened and a <mark>binary predicate</mark> at, representing the status of nuts and the position of agent, spanners and nuts within the corridor, respectively. The PDDL types of the original encoding can be compiled into unary predicates.<br>2: We write P (i) to denote the ground atom that resulted from instantiating unary predicate P with domain element i and Q (i, j) to denote the ground atom that resulted from instantiating <mark>binary predicate</mark> Q with domain elements i and j.<br>",
    "Arabic": "المسند الثنائي",
    "Chinese": "二元谓词",
    "French": "prédicat binaire",
    "Japanese": "二項述語",
    "Russian": "бинарный предикат"
  },
  {
    "English": "binary relation",
    "context": "1: The semantics is defined in terms of interpretations I = (∆ I , • I ) where ∆ I is the domain of I and • I assigns a set A I ⊆ ∆ I to every A ∈ N C and a <mark>binary relation</mark> r I ⊆ ∆ I × ∆ I to every r ∈ N R .<br>2: In contrast, SRL is tuned to identify the argument structure for nearly all verbs and nouns in a sentence. The missing recall from SRL is primarily where it does not identify both arguments of a <mark>binary relation</mark>, or where the correct argument is buried in a long argument phrase, but is not its head noun.<br>",
    "Arabic": "علاقة ثنائية",
    "Chinese": "二元关系",
    "French": "relation binaire",
    "Japanese": "二項関係",
    "Russian": "бинарное отношение"
  },
  {
    "English": "binary search",
    "context": "1: At the end of the <mark>binary search</mark>, we are guaranteed to have a forest with good \"density\" π(F ) c(F ) , but the good forest could correspond to either the lower bound λ l or the upper bound λ r .<br>2: We can find the optimal τ for each β by performing a <mark>binary search</mark> on τ , where we run our optimization procedure and then compute the BLEU loss at each iteration.<br>",
    "Arabic": "البحث الثنائي",
    "Chinese": "二分查找",
    "French": "recherche binaire",
    "Japanese": "二分探索",
    "Russian": "бинарный поиск"
  },
  {
    "English": "binary segmentation",
    "context": "1: Given an image I, we define the energy of a <mark>binary segmentation</mark> map x as: \n E(x; I) = ν i,j w ij |x(i) − x(j)| + k λ k |x − x F k ,I | (1) \n<br>2: Background replacement can be achieved with segmentation or matting. While <mark>binary segmentation</mark> is fast and efficient, the resulting composites have objectionable artifacts. Alpha matting can produce visually pleasing composites but often requires either manual annotations or a known background image. In this section, we discuss related works that perform background replacement with segmentation or matting. Segmentation.<br>",
    "Arabic": "تجزئة ثنائية",
    "Chinese": "二值分割",
    "French": "segmentation binaire",
    "Japanese": "二値セグメンテーション",
    "Russian": "бинарная сегментация"
  },
  {
    "English": "binary tree",
    "context": "1: Moreover, we assume that f j is a perfect <mark>binary tree</mark> with depth d. We use the sets I j and L j to denote the internal (split) nodes and the leaves of the tree, respectively.<br>2: For each set C i , create |C i | − 1 auxiliary x's as follows: Given set C i with |C i | = s i that corresponds to {h i1 , .., h isi }, create a balanced <mark>binary tree</mark> T i with each leaf corresponding to a h ij . Create an auxiliary example associated with each internal node in T i as follows : for each internal node in the tree , define the corresponding auxiliary sample x such that its label is +1 under all the classifiers in the leaves of the subtree rooted at its left child , and its label is −1 under all remaining classifiers in H. The total<br>",
    "Arabic": "شجرة ثنائية",
    "Chinese": "二叉树",
    "French": "arbre binaire",
    "Japanese": "二分木",
    "Russian": "бинарное дерево"
  },
  {
    "English": "binary variable",
    "context": "1: For co-occurrence potentials monotonically increasing with respect to L(x) the problem can be modelled using one <mark>binary variable</mark> z l per class indicating the presence of pixels of that class in the labelling, infinite edges for x i = l and z l = 0 and hyper-graph over all z l modelling C(L(x)).<br>2: The BIP is parameterized by a vector x where each transfer and each task is represented by a <mark>binary variable</mark>; x indicates which nodes are picked to be source and which transfers are selected. The canonical form for a BIP is: maximize c T x , subject to Ax b and x ∈ {0, 1} |E|+|V| .<br>",
    "Arabic": "متغير ثنائي",
    "Chinese": "二值变量",
    "French": "variable binaire",
    "Japanese": "二値変数",
    "Russian": "Бинарная переменная"
  },
  {
    "English": "binary vector",
    "context": "1: where x is the <mark>binary vector</mark> of all pixel assignments, u 0 p is the data penalty for p being in its current segment, and u 1 p the data penalty for switching p to the candidate segment.<br>2: Specifically, for a <mark>binary vector</mark> e t indicating which items were examined by the user, we model the relationship between c t and r t as follows. c t (d) = r t (d) if e t (d) = 1 0 otherwise (2) \n<br>",
    "Arabic": "المتجه الثنائي",
    "Chinese": "二进制向量",
    "French": "vecteur binaire",
    "Japanese": "バイナリーベクトル",
    "Russian": "бинарный вектор"
  },
  {
    "English": "binomial distribution",
    "context": "1: P R = σ α Φ −1 p 1 (N 1 , N, α) = B(N 1 , N, p 1 ) (15) \n where Φ −1 denotes the inverse Gaussian CDF , B ( N 1 , N , p 1 ) is the probability of drawing N 1 successes in N trials from a <mark>Binomial distribution</mark> with the success probability p 1 and p 1 is the lower bound to the success probability of a Bernoulli experiment given N 1 success in N trials with confidence<br>2: where B(N 1 , N, p 1 ) is the probability of drawing N 1 successes in N trials from a <mark>Binomial distribution</mark> with success probability p 1 and p 1 (N 1 , N ) is the lower bound of the success probability of a Bernoulli experiment given N 1 successes in N trials.<br>",
    "Arabic": "التوزيع الثنائي",
    "Chinese": "二项分布",
    "French": "distribution binomiale",
    "Japanese": "二項分布",
    "Russian": "биномиальное распределение"
  },
  {
    "English": "bioinformatic",
    "context": "1: Clustering is a family of problems that aims to group a given set of objects in a meaningful way-the exact \"meaning\" may vary based on the application. These are fundamental problems in Computer Science with applications ranging across multiple fields like pattern recognition, machine learning, computational biology, <mark>bioinformatics</mark> and social science.<br>",
    "Arabic": "علم المعلوماتية الحيوية",
    "Chinese": "生物信息学",
    "French": "bioinformatique",
    "Japanese": "バイオインフォマティクス",
    "Russian": "биоинформатика"
  },
  {
    "English": "biological neural network",
    "context": "1: Firstly because similar techniques are often applicable, but more importantly because the knowledge of how the workings of artificial and <mark>biological neural networks</mark> are similar or different is valuable for the general enterprise of cognitive science.<br>",
    "Arabic": "شبكة عصبية بيولوجية",
    "Chinese": "生物神经网络",
    "French": "réseau neuronal biologique",
    "Japanese": "生物学的神経回路網",
    "Russian": "биологическая нейронная сеть"
  },
  {
    "English": "bipartite",
    "context": "1: ter of queries can be defined as a maximal set of connected queries. An advantage of this method is that it does not need a specified parameter D max . However, in our experiments, we find that the <mark>bipartite</mark> is highly connected though sparse.<br>2: How can we overcome the \"curse of dimensionality\" and find the closest cluster fast? We observe that the queries in the click-through <mark>bipartite</mark> are very sparse. For example, in our experimental data, a query is connected with an average number of 8.2 URLs. Moreover, each URL is also involved in only a few queries.<br>",
    "Arabic": "ثنائي الأطراف",
    "Chinese": "二部图",
    "French": "bipartite",
    "Japanese": "二部グラフ",
    "Russian": "двудольный"
  },
  {
    "English": "bipartite graph",
    "context": "1: The following example illustrates the idea of maximal δ-occurrent and maximal σ-frequent itemsets. Let D be a database and GD its corresponding <mark>bipartite graph</mark>. In Section 2.2 we show that there is a one-to-one correspondence between <mark>bipartite graph</mark>s and databases of transactions.<br>2: In a similar manner, when λ = (n − 2, 2), the complete <mark>bipartite graph</mark> G λ has D λ = n 2 nodes on the left and right; each permutation corresponds to a perfect matching in this graph.<br>",
    "Arabic": "رسم بياني ثنائي",
    "Chinese": "二部图",
    "French": "graphe biparti",
    "Japanese": "二部グラフ",
    "Russian": "двудольный граф"
  },
  {
    "English": "bipartite matching",
    "context": "1: Then, the planning part is done by solving a <mark>bipartite matching</mark> [Xu et al., 2018]. We have conducted real-world AB tests in production for this method. Part of this demo shows this method in action on a city-scale order dispatching simulator backed by real trip data.<br>2: In terms of the matching strategy, candidates from newborn queries are paired with ground truth objects through <mark>bipartite matching</mark>, and predictions from track queries inherit the assigned ground truth index from previous frames. Specifically, L track = λ focal L focal + λ l1 L l1 , where λ focal = 2 and λ l1 = 0.25.<br>",
    "Arabic": "مُطابقة ثُنائيّة",
    "Chinese": "二分图匹配",
    "French": "Appariement biparti",
    "Japanese": "二部グラフマッチング",
    "Russian": "Двудольное совпадение"
  },
  {
    "English": "bipartite structure",
    "context": "1: ±1 , indicating local symmetries , <mark>bipartite structure</mark> , and disconnected components . With 50000 edges, localized structures disappear and the spectrum has narrower support. Finally, we investigate the Block Two-Level Erdös-Rényi (BTER) model [43], which directly fits an input graph.<br>2: Factor graph is a common representation for CNF formulas, which is a <mark>bipartite structure</mark> to represent the relationship between literals and clauses. There are mainly two kinds of factor graphs that have appeared in the previous work.<br>",
    "Arabic": "الهيكل ثنائي الأطراف",
    "Chinese": "双部分结构",
    "French": "structure bipartite",
    "Japanese": "二部構造",
    "Russian": "двудольная структура"
  },
  {
    "English": "birth-death process",
    "context": "1: Perhaps the most promising extension to the methods presented here is to incorporate a model of how new topics in the collection appear or disappear over time, rather than assuming a fixed number of topics. One possibility is to use a simple Galton-Watson or <mark>birth-death process</mark> for the topic population.<br>",
    "Arabic": "عملية الولادة والوفاة",
    "Chinese": "出生-死亡过程",
    "French": "processus de naissance-mort",
    "Japanese": "誕生死滅過程",
    "Russian": "процесс рождения-смерти"
  },
  {
    "English": "bisection method",
    "context": "1: For each experiment, the <mark>bisection method</mark> is used to determine the minimal population size for the algorithm to obtain the optimal solution (with 100% correct bits). The convergence criterion is when the proportion of a certain value on each position reaches 99%.<br>",
    "Arabic": "طريقة القسمة إلى نصفين",
    "Chinese": "二分法",
    "French": "méthode de la bisection",
    "Japanese": "二分法",
    "Russian": "метод бисекции"
  },
  {
    "English": "bisimulation",
    "context": "1: A * with the LM-Cut (lmc) heuristic (Helmert and Domshlak 2009), with the merge-and-shrink (ms) heuristic with SCC-DFP merge strategy and non-greedy <mark>bisimulation</mark> shrink strategy (Helmert et al.<br>2: In all cases not solved by the perfect <mark>bisimulation</mark> approaches, this is due to running out of memory while computing the abstraction. Non-Linear Merge Strategy Shifting attention to the results for the non-linear DFP merge strategy (bottom half of Table 1), we see that the results with the new label reduction method are excellent.<br>",
    "Arabic": "المحاكاة الثنائية",
    "Chinese": "双模拟",
    "French": "bisimulation",
    "Japanese": "双行シミュレーション",
    "Russian": "\"бисимуляция\""
  },
  {
    "English": "bitext",
    "context": "1: For example, a hierarchical phrase pair that might help with the above example is: \n ( The system we describe below uses rules like this, and in fact is able to learn them automatically from a <mark>bitext</mark> without syntactic annotation.<br>2: We extracted a grammar from 220 million words of Arabic-English <mark>bitext</mark> using the approach of Galley et al. (2006), extracting rules with at most 3 non-terminals. These rules are highly lexicalized. About 300K rules are applicable for a typical 30-word sentence; we filter the rest.<br>",
    "Arabic": "نصوص ثنائية اللغة",
    "Chinese": "双语平行语料",
    "French": "corpus bilingue",
    "Japanese": "対訳コーパス",
    "Russian": "параллельный текст"
  },
  {
    "English": "bitvector",
    "context": "1: . During its execution, QS has to maintain the <mark>bitvector</mark>s v h 's, encoding the set C h 's for all the tree T h in the ensemble. The <mark>bitvector</mark> v h of a certain tree is updated as soon as a false node for that tree is identified.<br>2: However, we can always represent the chunk within uj bits by writing the characteristic vector of the set of its elements as a <mark>bitvector</mark>. Note that Vigna [23] also uses this technique but only for whole lists, which are very unlikely to be so dense except for very few terms such as stop-words.<br>",
    "Arabic": "متجه البت",
    "Chinese": "比特向量",
    "French": "vecteur de bits",
    "Japanese": "ビットベクトル",
    "Russian": "битовый вектор"
  },
  {
    "English": "black-box",
    "context": "1: These variations can provide distinctive signals for <mark>black-box</mark> machine-generated text detection approaches.<br>2: These rates are then optimal among all <mark>black-box</mark> first-order methods that access gradients and linearly combine them [43,41]. Nesterov acceleration relies on several sequences of iterates-two or three, depending on the formulation-and on a clever blend of gradient steps and mixing steps between the sequences.<br>",
    "Arabic": "صندوق أسود",
    "Chinese": "黑箱",
    "French": "boîte noire",
    "Japanese": "ブラックボックス",
    "Russian": "черный ящик"
  },
  {
    "English": "black-box model",
    "context": "1: Note again that, for our method only, we discretize numerical features into equal-sized buckets and decode the numerical features back to their original representations whenever necessary to consult the <mark>black-box model</mark>. Appendix A describes our tasks and model design in greater detail. Our code repository can be accessed at https://github.com/isVy08/L2C/. Performance metrics.<br>2: Furthermore, releasing the explanations exposes how the <mark>black-box model</mark> acts upon an input sample, essentially giving up more information about its inner workings for each query, hence, model extractions attacks can be carried out with far fewer queries, as discussed in (Milli et al. 2019;Miura, Hasegawa, and Shibahara 2021).<br>",
    "Arabic": "نموذج صندوق أسود",
    "Chinese": "黑盒模型",
    "French": "modèle de boîte noire",
    "Japanese": "ブラックボックスモデル",
    "Russian": "модель \"черного ящика\""
  },
  {
    "English": "block coordinate descent",
    "context": "1: Keshavan et al. [KMO10a,KMO10b] showed that well-initialized gradient descent recovers M . The works [HW14, Har14, JNS13, CW15] showed that well-initialized alternating least squares, <mark>block coordinate descent</mark>, and gradient descent converges M .<br>",
    "Arabic": "هبوط الإحداثيات المُجزَّأ",
    "Chinese": "块坐标下降",
    "French": "descente de coordonnées par bloc",
    "Japanese": "ブロック座標降下法",
    "Russian": "блочный координатный спуск"
  },
  {
    "English": "block matrix",
    "context": "1: This inferiority also prompted the authors to think further: perhaps the rank-3K condition enforced upon S is not strong enough or sufficient? pondering this question has led us to the following method -the <mark>block matrix</mark> method-which gives more favorable performance.<br>2: It is seen, our <mark>block matrix</mark> method achieves the best performance in terms of the accuracy for rotation estimation and for shape recovery, compared favorably with almost all the other state-of-the-art competitors. Note that our pseudoinverse method also achieves better performance than EM-PPCA, Metric Projection and XCK.<br>",
    "Arabic": "مصفوفة كتلية",
    "Chinese": "分块矩阵",
    "French": "\"matrice par blocs\"",
    "Japanese": "ブロック行列",
    "Russian": "блочная матрица"
  },
  {
    "English": "block-diagonal matrix",
    "context": "1: , L m ). Hence, up to permuting rows and columns, L is also a <mark>block-diagonal matrix</mark> of m dense blocks, each of size m × m. \n Thus we can write B = PLP R, where L, R, and P are as in Definition 3.1.<br>2: As we show in Appendix C.1.2, L can be written as a <mark>block-diagonal matrix</mark> with b blocks of size n b × n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices.<br>",
    "Arabic": "مصفوفة قطرية الكتل",
    "Chinese": "分块对角矩阵",
    "French": "matrice diagonale par blocs",
    "Japanese": "ブロック対角行列",
    "Russian": "блочно-диагональная матрица"
  },
  {
    "English": "bloom filter",
    "context": "1: In this paper, we introduce the Odd Sketch, a compact binary sketch for estimating the Jaccard similarity of two sets. This binary sketch is similar to a <mark>Bloom filter</mark> with one hash function, constructed on the original minhashes with the \"odd\" feature that the usual disjunction is replaced by an exclusive-or operation.<br>",
    "Arabic": "فلتر الازدهار",
    "Chinese": "布隆过滤器",
    "French": "filtre de Bloom",
    "Japanese": "ブルームフィルタ",
    "Russian": "фильтр Блума"
  },
  {
    "English": "blur kernel",
    "context": "1: Since it is in general not feasible to train a specialized model for every image blur, it is necessary to train a model that outputs a deblurred image given an arbitrary input image and <mark>blur kernel</mark>. We address this by effectively parametrizing our discriminative model with the <mark>blur kernel</mark>.<br>2: But in the case of deblurring, the image content in y is shifted and combined with other parts of the image, depending on a <mark>blur kernel</mark> that is different for each image. This makes the choice of local models difficult. We believe this is one of the reasons why discriminative non-blind deblurring approaches had not been attempted before.<br>",
    "Arabic": "نواة التمويه",
    "Chinese": "模糊核",
    "French": "noyau de flou",
    "Japanese": "ぼけカーネル",
    "Russian": "ядро размытия"
  },
  {
    "English": "boolean formula",
    "context": "1: , where Ω is the set of all potentials solution ({0, 1} n ) and SC is a mapping Ν → Ω \n , called score of the assignment, equal to the number of true clauses. Consequently, the problem consists of defining the best binary assignment that maximizes the number of true clauses in the <mark>Boolean formula</mark>.<br>2: \"SAT\" problem is the shorthand of Boolean satisfiability problem. It is defined as the task of determining the satisfiability of a given <mark>Boolean formula</mark> by looking for the variable assignment that makes this formula evaluating to true.<br>",
    "Arabic": "صيغة بولية",
    "Chinese": "布尔公式",
    "French": "formule booléenne",
    "Japanese": "ブール式",
    "Russian": "булева формула"
  },
  {
    "English": "boolean function",
    "context": "1: WMC has been extensively studied both in the theory and the AI communities, and the precise complexity of E(F) is known for many families of <mark>Boolean functions</mark> F. These results immediately carry over to the F-SHAP(F, IND) problem through Theorem 1: \n 5.<br>2: , and negation ( ¬ ) of their characteristic functions , respectively . Binary Decision Diagrams (BDDs) (Bryant 1986) are a efficient data-structure to represent <mark>Boolean functions</mark> in the form of a directed acyclic graph. The size of a BDD is the number of nodes in this representation.<br>",
    "Arabic": "دالة منطقية",
    "Chinese": "布尔函数",
    "French": "fonction booléenne",
    "Japanese": "論理関数",
    "Russian": "булева функция"
  },
  {
    "English": "boolean variable",
    "context": "1: When representing the maximum clique problem in pseudo-Boolean form, we overload notation and identify every vertex v ∈ V with a <mark>Boolean variable</mark>, where v = 1 means that the vertex v is in the clique.<br>",
    "Arabic": "متغير بولياني",
    "Chinese": "布尔变量",
    "French": "variable booléenne",
    "Japanese": "ブール変数",
    "Russian": "булева переменная"
  },
  {
    "English": "boost algorithm",
    "context": "1: In particular, we will require that in each round Weak Learner picks the weak classifier suffering minimum cost with respect to the cost matrix provided by the <mark>boosting algorithm</mark>, or equivalently achieves the highest edge as defined in (59). Such assumptions are both necessary and standard in the literature, and are frequently met in practice.<br>2: In particular, if the weak learning condition is stronger than necessary, then, even on a boostable data set where the error can be driven to zero, the <mark>boosting algorithm</mark> may get stuck prematurely because its stronger than necessary demands cannot be met by the weak classifier space.<br>",
    "Arabic": "خوارزمية التعزيز",
    "Chinese": "提升算法",
    "French": "algorithme de renforcement",
    "Japanese": "強化アルゴリズム",
    "Russian": "алгоритм усиления"
  },
  {
    "English": "boost approach",
    "context": "1: window contains the object ; this probability can be derived from most standard classifiers , such as the highly successful <mark>boosting approaches</mark> [ 1 ] . The set of windows included in the model can, in principle, include all windows in the image.<br>",
    "Arabic": "منهج التعزيز",
    "Chinese": "提升方法",
    "French": "approche de renforcement",
    "Japanese": "勾配ブースト法",
    "Russian": "повышающий подход"
  },
  {
    "English": "bootstrap learning",
    "context": "1: OLLIE (Mausam et al., 2012) follows the idea of <mark>bootstrap learning</mark> of patterns based on dependency parse paths.<br>",
    "Arabic": "تعلم البدئي (bootstrap learning)",
    "Chinese": "自助学习",
    "French": "apprentissage par bootstrap",
    "Japanese": "ブートストラップ学習",
    "Russian": "обучение методом бутстрэп"
  },
  {
    "English": "bootstrap resampling",
    "context": "1: While <mark>bootstrap resampling</mark> leads to slightly lower error reduction than cross-validation we decided to report the latter in the main part of this paper, because it is a more wide-spread way to randomly split datasets. Random Length results are comparable to standard splits results.<br>2: In quality estimation, it is common to apply <mark>bootstrap resampling</mark> to assess the likelihood that a decrease in MAE (an improvement) has occurred by chance.<br>",
    "Arabic": "إعادة أخذ العينات البوتسترابية",
    "Chinese": "引导重抽样",
    "French": "rééchantillonnage par la méthode du bootstrap",
    "Japanese": "ブートストラップ再標本化",
    "Russian": "Bootstrap-перетасовка"
  },
  {
    "English": "bootstrap sample",
    "context": "1: To compute the stratified bootstrap CIs, we re-sample runs with replacement independently for each task to construct an empirical <mark>bootstrap sample</mark> with N runs each for M tasks from which we calculate a statistic and repeat this process many times to approximate the sampling distribution of the statistic.<br>2: This estimator can be high variance because the probability y winning in a <mark>bootstrap sample</mark> is very small when H t is large, so instead we use the probability that y outranks a particularȳ ∈ H t : \n Ê Rt∼boot(Rt) 1(U (y,R t ) ≥ U (ȳ,R t )) .<br>",
    "Arabic": "عينة التمهيد",
    "Chinese": "自助采样",
    "French": "échantillon bootstrap",
    "Japanese": "ブートストラップサンプル",
    "Russian": "Bootstrap выборка"
  },
  {
    "English": "bottleneck",
    "context": "1: Conversely, on a low-homophily dataset, a curvature-based approach as SDRF modifies the edge set mainly around the most negatively curved edges, meaning that it decreases the <mark>bottleneck</mark> without significantly increasing the connectivity among nodes with different labels.<br>2: By applying (Chung, 2007, Lemma 5), we show that we cannot improve the Cheeger constant (and hence the <mark>bottleneck</mark>) arbitrarily well (in contrast to a curvature-based approach).<br>",
    "Arabic": "عنق الزجاجة",
    "Chinese": "瓶颈",
    "French": "goulot d'étranglement",
    "Japanese": "ボトルネック",
    "Russian": "узкое место"
  },
  {
    "English": "bottleneck layer",
    "context": "1: We find this design especially effective for DenseNet and we refer to our network with such a <mark>bottleneck layer</mark>, i.e., to the BN-ReLU-Conv(1× 1)-BN-ReLU-Conv(3×3) version of H , as DenseNet-B. In our experiments, we let each 1×1 convolution produce 4k feature-maps. Compression.<br>2: It has been noted in [37,11] that a 1×1 convolution can be introduced as <mark>bottleneck layer</mark> before each 3×3 convolution to reduce the number of input feature-maps, and thus to improve computational efficiency.<br>",
    "Arabic": "طبقة عنق الزجاجة",
    "Chinese": "瓶颈层",
    "French": "couche de goulot d'étranglement",
    "Japanese": "ボトルネック層",
    "Russian": "слой сжатия"
  },
  {
    "English": "bottom-up",
    "context": "1: Intuitively, the <mark>bottom-up</mark> discriminative probabilities activate top-down generative models. In this paper, we focus on two types of visual patterns-generic visual patterns, such as texture and shading, and object patterns including human faces and text.<br>2: Previously seen cars enable us to develop a notion of the 3D shape of cars, which we can project to this particular instance. We also specialize our representation to this particular instance (e.g. any custom decorations it might have), signalling that both top-down and <mark>bottom-up</mark> cues influence our percept [26].<br>",
    "Arabic": "من أسفل إلى أعلى",
    "Chinese": "自底向上",
    "French": "ascendante",
    "Japanese": "下位から上への",
    "Russian": "снизу вверх"
  },
  {
    "English": "bottom-up learning",
    "context": "1: The second is Sun [9] who proposed the CLAR-ION architecture. In CLARION's current version, during <mark>bottom-up learning</mark>, the propositions (premises and actions) are already present in top level (explicit) modules before the learning process starts, and only the links between these nodes emerges from the implicit level (rules).<br>",
    "Arabic": "التعلم من الأسفل إلى الأعلى",
    "Chinese": "自底向上学习",
    "French": "apprentissage ascendant",
    "Japanese": "下位から上への学習",
    "Russian": "обучение снизу вверх"
  },
  {
    "English": "bottom-up module",
    "context": "1: Both the top-down module and the <mark>bottom-up module</mark> that we used can be significantly improved. Our top-down module translates an image fragment and searches for the best normalized correlation, while other algorithms also allow rescaling and rotation of the parts and use more sophisticated image similarity metrics.<br>",
    "Arabic": "\"وحدة من الأسفل إلى الأعلى\"",
    "Chinese": "自底向上模块",
    "French": "module ascendant",
    "Japanese": "下位モジュール",
    "Russian": "модуль снизу вверх"
  },
  {
    "English": "bottom-up parsing",
    "context": "1: a question ; i.e. , <mark>bottom-up parsing</mark> ( Cheng et al. , 2019 ; Rubin and Berant , 2021 ) . Each subprogram is grounded to the KB and its grounding (i.e., denotation or execution results) can further guide an efficient search for faithful programs (see Figure 1(a)).<br>2: During lexical generation (Section 6.1), the algorithm first attempts to use a set of templates to hypothesize new lexical entries. It then attempts to combine <mark>bottom-up parsing</mark> with top-down recursive splitting to select the best entries and learn new templates for complex syntactic and semantic phenomena, which are re-used in later sentences to hypothesize new entries.<br>",
    "Arabic": "تحليل من الأسفل إلى الأعلى",
    "Chinese": "自底向上解析",
    "French": "analyse ascendante",
    "Japanese": "ボトムアップ構文解析",
    "Russian": "синтаксический анализ снизу вверх"
  },
  {
    "English": "bound box",
    "context": "1: One <mark>bounding box</mark> can be determined by four values (x, y, h, w), which indicate the x and y coordinate of the center point, height (h), and width (w), respectively.<br>2: Specifically, for the <mark>bounding box</mark> localization part, we treat it as a standard phrase grounding task and follow (Li et al., 2019) to select the top-1 <mark>bounding box</mark> prediction in the last masked token as the output.<br>",
    "Arabic": "مربع محيط",
    "Chinese": "边界框",
    "French": "boîte englobante",
    "Japanese": "境界ボックス",
    "Russian": "граничная рамка"
  },
  {
    "English": "bound rationality",
    "context": "1: 3 The underpinning intuition is that agents' deviations from pure exploitation are not a result of their <mark>bounded rationality</mark> but rather a perfectly rational action in the quest for more information about unexplored choices which creates value on its own.<br>2: Biases play a central role in human judgment and decision making and span a number of different dimensions (see [2][3] [21] for summaries). Other models of irrational human behavior are plentiful, including related concepts such as <mark>bounded rationality</mark> [33].<br>",
    "Arabic": "العقلانية المحدودة",
    "Chinese": "有界理性",
    "French": "rationalité limitée",
    "Japanese": "有界合理性",
    "Russian": "ограниченная рациональность"
  },
  {
    "English": "bound variable",
    "context": "1: , x j k j are the <mark>bound variables</mark> in the λ-function used to rewrite A j ; (3) x i 1 , . . . , x i k are the logical variables that appear in β ′ and outside the current MR sub-parse.<br>",
    "Arabic": "متغير مقيد",
    "Chinese": "绑定变量",
    "French": "variable liée",
    "Japanese": "束縛変数",
    "Russian": "связанная переменная"
  },
  {
    "English": "bounding box detection",
    "context": "1: The presented AAEs were also ported onto a Nvidia Jetson TX2 board, together with a small footprint Mo-bileNet from Howard et al. (2017) for the <mark>bounding box detection</mark>. A webcam was connected, and this setup was demonstrated live at ECCV 2018, both in the demo session and during the oral presentation.<br>2: The models are trained in the COCO train2017 set and evaluated in the COCO val2017 set (a.k.a minival). We report the standard COCO metrics of Average Precision (AP), AP 50 , and AP 75 , for <mark>bounding box detection</mark> (AP bbox ) and instance segmentation (AP mask ). Results of C4 backbone.<br>",
    "Arabic": "كشف المربع المحيط",
    "Chinese": "边界框检测",
    "French": "détection de boîte englobante",
    "Japanese": "バウンディングボックス検出",
    "Russian": "обнаружение ограничивающей рамки"
  },
  {
    "English": "bounding box regression",
    "context": "1: Our method, called Mask R-CNN, extends Faster R-CNN [36] by adding a branch for predicting segmentation masks on each Region of Interest (RoI), in parallel with the existing branch for classification and <mark>bounding box regression</mark> (Figure 1).<br>",
    "Arabic": "تراجع مربع الحدود",
    "Chinese": "边界框回归",
    "French": "régression de boîte englobante",
    "Japanese": "境界ボックス回帰",
    "Russian": "регрессия ограничивающих рамок"
  },
  {
    "English": "bounding box regressor",
    "context": "1: We note that unlike most recent work, we use a class-agnostic <mark>bounding box regressor</mark> which uses fewer parameters and we found to be equally effective. The object classification subnet and the box regression subnet, though sharing a common structure, use separate parameters.<br>",
    "Arabic": "رجريسور مربع الحدود",
    "Chinese": "边界框回归器",
    "French": "régression de boîte englobante",
    "Japanese": "バウンディングボックス回帰器",
    "Russian": "регрессор ограничивающей рамки"
  },
  {
    "English": "branch and bind algorithm",
    "context": "1: Thus the above optimization problem is a convex quadratic program that can be solved using a QP or an SOCP solver. Given bounds on {p 1 , p 2 , p 3 }, a <mark>branch and bound algorithm</mark> can now be used to obtain a global minimum to the modulus constraints.<br>2: The present work is related to that of Hirakawa (2001) who, like us, reduces the problem of dependency parsing to spanning tree search. However, his parsing method uses a <mark>branch and bound algorithm</mark> that is exponential in the worst case, even though it appears to perform reasonably in limited experiments.<br>",
    "Arabic": "خوارزمية التفرع والحد",
    "Chinese": "分支定界算法",
    "French": "algorithme d'énumération et d'évaluation",
    "Japanese": "分岐限定法",
    "Russian": "алгоритм ветвей и границ"
  },
  {
    "English": "branch factor",
    "context": "1: This ratio can be made arbitrarily large by increasing the <mark>branching factor</mark> of the trees. The general rule based on the core differences is: GR1: FF and RN usually determine whether MM 0 will expand fewer nodes than Uni-BS (FF > RN) or more.<br>2: This allows us to delay the state transition until the child is extracted from the queue, saving up to a <mark>branching factor</mark> of state transitions (see also [Agostinelli et al., 2021]). Remark 8.<br>",
    "Arabic": "عامل التفرع",
    "Chinese": "分支因子",
    "French": "facteur de branchement",
    "Japanese": "分岐因子",
    "Russian": "ветвящийся фактор"
  },
  {
    "English": "breadth-first order",
    "context": "1: Hereinafter, we assume that nodes of T are numbered in <mark>breadth-first order</mark> and leaves from left to right, and let φi and γi be the feature id and threshold associated with i-th internal node, respectively. It is worth noting that the same feature can be involved in multiple nodes of the same tree.<br>",
    "Arabic": "ترتيب البحث العرضي",
    "Chinese": "广度优先顺序",
    "French": "ordre de parcours en largeur",
    "Japanese": "幅優先順",
    "Russian": "порядок обхода в ширину"
  },
  {
    "English": "breadth-first search",
    "context": "1: In fact, this basis can be constructed starting from a spanning tree, which can be found in linear time by either depth-first search or <mark>breadth-first search</mark>.<br>2: In particular, we applied the Approximate Neighborhood Function (ANF) approach [27] (in two different implementations), which can estimate effective diameters for very large graphs, as well as a basic sampling approach in which we ran exhaustive <mark>breadth-first search</mark> from a subset of the nodes chosen uniformly at random.<br>",
    "Arabic": "البحث أولاً في العرض",
    "Chinese": "广度优先搜索",
    "French": "parcours en largeur",
    "Japanese": "幅優先探索",
    "Russian": "поиск в ширину"
  },
  {
    "English": "bregman divergence",
    "context": "1: + d 4 F (Dij) dD 4 ij (Lij − Dij) 4 4! + • • • (7) \n We now show that linear MMDS is a <mark>Bregman divergence</mark> and that the Sammon Mapping can be seen as special case of approximation to <mark>Bregman divergence</mark>s using only the first term of (7). Linear MMDS.<br>2: Such parameterization can be thought of as scaling every attribute in the original space by a weight contained in the corresponding component of a, and then taking I-divergence in the transformed space. This implies that D I a is a <mark>Bregman divergence</mark> with respect to the transformed space.<br>",
    "Arabic": "تباين بريجمان",
    "Chinese": "布雷格曼散度",
    "French": "divergence de Bregman",
    "Japanese": "ブレグマン・ダイバージェンス",
    "Russian": "Расхождение Брегмана"
  },
  {
    "English": "brute force search",
    "context": "1: The algorithm in [9] computes bounds on the plane at infinity and a <mark>brute force search</mark> is used to recover π ∞ within this region. It is argued in [18] that it might be advantageous to use camera centers alone when using chirality constraints.<br>2: Furthermore, a <mark>brute force search</mark> requires iterating over all`n k˘s ubsets, which is prohibitively expensive, so we would like to find our subset more efficiently. Extensive literature has been dedicated to developing algorithms for the CSSP (an in depth discussion of the related work can be found in Appendix A).<br>",
    "Arabic": "البحث بالقوة الغاشمة",
    "Chinese": "蛮力搜索",
    "French": "recherche par force brute",
    "Japanese": "総当たり検索",
    "Russian": "грубый поиск перебором"
  },
  {
    "English": "bundle adjustment",
    "context": "1: graph at least k 1 times, and that covers each image at least k 2 ≥ k 1 times (we used k 1 = 5 and k 2 = 10). Step 5: <mark>Bundle adjustment</mark>.<br>2: The extracted features are matched between consecutive images and then fed into a classic SfM pipeline [9], which reconstructs feature tracks and refines 3D point locations by triangulation. <mark>Bundle adjustment</mark> is running in parallel with the main SfM algorithm to refine camera poses and 3D feature locations for previous frames and thus reduce drift. Online Ground Plane Estimation.<br>",
    "Arabic": "ضبط الحزمة",
    "Chinese": "捆绑调整",
    "French": "ajustement de faisceau",
    "Japanese": "バンドル調整",
    "Russian": "уравнивание связок"
  },
  {
    "English": "burn-in",
    "context": "1: At each phase, we sample for 5000 iterations, discarding the first 2000 for <mark>burn-in</mark>, and collecting a sample every 100 iterations for performance evaluation. The particles of the last iteration at each phase were incorporated into the model as a prior for sampling in the next phase.<br>",
    "Arabic": "فترة التجنيب",
    "Chinese": "燃烧期",
    "French": "rodage",
    "Japanese": "バーンイン",
    "Russian": "приработка"
  },
  {
    "English": "byte-pair encoding",
    "context": "1: <mark>Byte-Pair Encoding</mark> (BPE) (Sennrich et al., 2016) is proposed to get subword-level vocabularies. The general idea is to merge pairs of frequent character sequences to create sub-word units. Sub-word vocabularies can be regarded as a trade-off between character-level vocabularies and word-level vocabularies.<br>2: al. , 2019 ) . Currently , sub-word approaches like <mark>Byte-Pair Encoding</mark> ( BPE ) are widely used in the community ( Ott et al. , 2018 ; Ding et al. , 2019 ; Liu et al. , 2020 ) , and achieve quite promising results in practice ( Sennrich et al. , 2016 ; Costa-jussà and Fonollosa , 2016 ; Lee et al. , 2017 ; Kudo and<br>",
    "Arabic": "ترميز الأزواج البايتية",
    "Chinese": "字节对编码 (BPE)",
    "French": "encodage par paires d'octets (BPE)",
    "Japanese": "バイトペアエンコーディング",
    "Russian": "кодирование пар байтов"
  },
  {
    "English": "calculus of variation",
    "context": "1: Test error (%) In this section we investigate this question. Using the <mark>calculus of variations</mark>, we first derive the optimal data distribution p(z|α prune , f ) along the teacher for a given α prune , f . We begin by framing the problem using the method of Lagrange multipliers.<br>2: [15]), by the <mark>calculus of variations</mark>: the Total Variation minimization (Rudin-Osher-Fatemi [13]), or in the frequency domain: the empirical Wiener filters (Yaroslavsky [16]) and wavelet thresholding methods (Coiffman-Donoho [5,4]). Formally we define a denoising method D h as a decomposition \n<br>",
    "Arabic": "حساب التغيير",
    "Chinese": "变分法",
    "French": "calcul des variations",
    "Japanese": "変分法",
    "Russian": "вариационное исчисление"
  },
  {
    "English": "calibration",
    "context": "1: We then present in Section 3 the general framework of <mark>calibration</mark> of [20], and give a new characterization of <mark>calibration</mark> for the evaluation metrics we consider (Theorem 2), and the implications of the convexity of a surrogate loss. Our main result is proved in Section 4.<br>2: There has been extensive work (Barocas & Selbst, 2016) on guaranteeing fairness for classification over a protected label through constraints such as equalized odds (Woodworth et al., 2017;Hardt et al., 2016), disparate impact (Feldman et al., 2015) and <mark>calibration</mark> (Kleinberg et al., 2017).<br>",
    "Arabic": "معايرة",
    "Chinese": "校准",
    "French": "étalonnage",
    "Japanese": "校正",
    "Russian": "калибровка"
  },
  {
    "English": "calibration method",
    "context": "1: These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, <mark>calibration methods</mark>, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature.<br>",
    "Arabic": "طريقة المعايرة",
    "Chinese": "校准方法",
    "French": "méthode de calibration",
    "Japanese": "較正方法",
    "Russian": "метод калибровки"
  },
  {
    "English": "caltech-101",
    "context": "1: Similarly, since M = 2N 1/(1+ǫ) , for higher values of ǫ we must search fewer examples, but accuracy guarantees decrease. Exemplar-based Object Categorization. Next we evaluate our method applied for NN object recognition with the <mark>Caltech-101</mark>, a now common benchmark.<br>2: We conducted image classification experiments on CMU-multipie, 15 Scene and <mark>Caltech-101</mark> data sets.<br>",
    "Arabic": "كالتك-101",
    "Chinese": "加州理工学院101数据集",
    "French": "Caltech-101",
    "Japanese": "カルテック-101",
    "Russian": "Калтех-101"
  },
  {
    "English": "camera calibration",
    "context": "1: This correlated temporal behavior is used to recover both the spatial and temporal transformations between the two sequences. Unlike carefully calibrated stereo-rigs (Slama, 1980), our approach does not require any prior internal or external <mark>camera calibration</mark>, nor any sophisticated hardware. Our approach bears resemblance to the approaches suggested by Demirdijian et al.<br>2: For each image pair, SfM delivers an updated <mark>camera calibration</mark>. In addition, we obtain an online ground plane estimate by computing local normals on a set of trapezoidal road strips between the reconstructed wheel contact points of adjacent frames and averaging those local measurements over a larger window.<br>",
    "Arabic": "معايرة الكاميرا",
    "Chinese": "相机标定",
    "French": "étalonnage de la caméra",
    "Japanese": "カメラキャリブレーション",
    "Russian": "калибровка камеры"
  },
  {
    "English": "camera intrinsic",
    "context": "1: [12], in which the flow field is (over-) parameterized by explicitly searching for rigid motion parameters, and then encouraging their smoothness. Valgaerts et al. [18] generalize the problem by assuming that only the <mark>camera intrinsics</mark>, but not the relative pose are known.<br>",
    "Arabic": "ثوابت الكاميرا الداخلية",
    "Chinese": "相机内参",
    "French": "paramètres intrinsèques de la caméra",
    "Japanese": "カメラ内部パラメータ",
    "Russian": "камерные внутренние параметры"
  },
  {
    "English": "camera matrix",
    "context": "1: Alternatively, the \"threading\" method of Avidan and Shashua (1998) or other methods for computing consistent set of <mark>camera matrices</mark> (e.g., Beardsley et al., 1998), can impose plane-consistency within each sequence, even if no real physical plane is visible in any of the frames.<br>2: Not less well known is the fact that the fundamental matrix depends on 7 independent parameters (2 × 11 for the two <mark>camera matrices</mark> minus 15 for the projective freedom) and that consequently the nine elements of F JI are not independent and should fulfil one constraint besides the free scale.<br>",
    "Arabic": "مصفوفة الكاميرا",
    "Chinese": "相机矩阵",
    "French": "matrice de caméra",
    "Japanese": "カメラ行列",
    "Russian": "матрицы камер"
  },
  {
    "English": "camera parameter",
    "context": "1: Given a monocular video of a dynamic scene with frames (I 1 , I 2 , . . . , I N ) and known <mark>camera parameters</mark> (P 1 , P 2 , . . . , P N ), our goal is to synthesize a novel viewpoint at any desired time within the video.<br>2: The latent scene S ρ in this program can be visualized as a tree with the root node around the center of the mesh, and consists of bone location variables, bone rotation variables and <mark>camera parameters</mark>.<br>",
    "Arabic": "معلمات الكاميرا",
    "Chinese": "相机参数",
    "French": "paramètres de la caméra",
    "Japanese": "カメラパラメータ",
    "Russian": "параметры камеры"
  },
  {
    "English": "camera pose estimation",
    "context": "1: Unlike our approach, Parti-cleSfM focuses on <mark>camera pose estimation</mark> within an SfM framework, where only correspondences from static regions are optimized, and dynamic objects are treated as outliers. Neural video representations.<br>2: Currently, SRNs require camera intrinsic and extrinsic parameters, which can be obtained robustly via bundle-adjustment. However, as SRNs are differentiable with respect to camera parameters; future work may alternatively integrate them with learned algorithms for <mark>camera pose estimation</mark> [72].<br>",
    "Arabic": "تقدير وضعية الكاميرا",
    "Chinese": "相机姿态估计",
    "French": "estimation de la pose de la caméra",
    "Japanese": "カメラの姿勢推定",
    "Russian": "оценка положения камеры"
  },
  {
    "English": "candidate generation",
    "context": "1: We adopt a two-stage pipeline consisting of a fast <mark>candidate generation</mark> stage, followed by a more expensive but powerful candidate ranking stage.<br>2: Our IR-based <mark>candidate generation</mark> has a top-64 recall of 76% and 68% on the validation and test sets, respectively. The unnormalized performance is thus upper-bounded by these numbers. Strengthening the <mark>candidate generation</mark> stage improves the unnormalized performance, but this is outside the scope of our work.<br>",
    "Arabic": "توليد المرشحين",
    "Chinese": "候选生成",
    "French": "génération de candidats",
    "Japanese": "候補生成",
    "Russian": "генерация кандидатов"
  },
  {
    "English": "candidate set",
    "context": "1: Most of the existing generic summarization approaches use a ranking model to select sentences from a <mark>candidate set</mark> (Brin and Page 1998;Kleinberg 1999;Wan and Yang 2007). These methods suffer from a severe problem that top ranked sentences usually share much redundant information.<br>2: Our motivation is that the clustering effect of contrastive learning makes the clean examples dominate the prototype calculation and thus they are distributed close to at least one prototype inside the <mark>candidate set</mark>. On the other hand, the noisy candidates mostly deviate from all candidate prototypes as their true labels are not contained in their <mark>candidate set</mark>s.<br>",
    "Arabic": "مجموعة المرشحين",
    "Chinese": "候选集",
    "French": "ensemble de candidats",
    "Japanese": "候補セット",
    "Russian": "множество кандидатов"
  },
  {
    "English": "canny detector",
    "context": "1: They are used to give proposals for region boundaries (i.e. the shape descriptor attributes of the nodes). Specifically, we run the <mark>Canny detector</mark> at three scales followed by edge linking to give partitions of the image lattice.<br>",
    "Arabic": "كاشف كاني",
    "Chinese": "坎尼检测器",
    "French": "détecteur de Canny",
    "Japanese": "キャニー検出器",
    "Russian": "детектор канни"
  },
  {
    "English": "canny edge detector",
    "context": "1: e F1/10 steering controller was modi ed to keep track of the lane center using a proportional-derivative-integral (PID) controller. e image pipeline detailed in Fig. 4 [ Le ] is comprised of the following tasks : ( a ) e raw RGB camera image , in which the lane color was identied by its hue and saturation value , is converted to greyscale and subjected to a color lter designed to set the lane color to white and everything else to black , ( b ) e masked image from the previous step is sent through a canny edge detector and then through a logical AND mask whose parameters ensured that the resulting image contains only the information about the path , ( c ) e output from the second step is ltered using a Gaussian lter that reduces noise and is sent through a Hough transformation , resulting in the lane markings<br>2: Figure 1 gives an illustration of an example image together with the human subject ground truth annotation, as well as results by the proposed HED edge detector (including the side responses of the individual layers), and results by the <mark>Canny edge detector</mark> [4] with different scale parameters.<br>",
    "Arabic": "كاشف حافات كاني",
    "Chinese": "Canny边缘检测器",
    "French": "détecteur de contours Canny",
    "Japanese": "キャニーエッジ検出器",
    "Russian": "детектор границ Кэнни"
  },
  {
    "English": "canonical basis",
    "context": "1: While other decompositions exist (see Appendix A), this particular decoupling directly capitalizes upon each basis' strengths: the Fourier basis is well-suited for representing the prior (Rahimi and Recht, 2008) and the <mark>canonical basis</mark> is wellsuited for representing the data (Burt et al., 2019).<br>2: f (x) = E {v,w}∼P 1 2 x, a {v,w} 2 , (22 \n ) \n where a {v,w} = e v − e w and (e v ) v∈V forms the <mark>canonical basis</mark> of R V .<br>",
    "Arabic": "قاعدة كانونية",
    "Chinese": "规范基底",
    "French": "base canonique",
    "Japanese": "正準基底",
    "Russian": "канонический базис"
  },
  {
    "English": "canonical correlation analysis",
    "context": "1: (2010) assessed the effectiveness of the word embedding techniques of Collobert and Weston (2008) and Mnih and Hinton (2007)   embedding method based on <mark>canonical correlation analysis</mark> that provides state-of-the art results for wordbased SSL for English NER.<br>2: One natural choice is <mark>canonical correlation analysis</mark> (CCA) [5], a generalization of correlation, specifically suited when p ̸ = q.<br>",
    "Arabic": "تحليل الارتباط الكنسي",
    "Chinese": "典型相关分析",
    "French": "analyse de corrélation canonique",
    "Japanese": "正準相関分析",
    "Russian": "канонический корреляционный анализ"
  },
  {
    "English": "canonical form",
    "context": "1: Given observation x ∈ X , a conditional exponential family ( CEF ) distribution over labels y ∈ Y ( we assume all spaces are finite ) , parameterized by the natural parameter θ ∈ R d , can be written in its <mark>canonical form</mark> as p ( y|x ; θ ) = exp ( φ ( x , y ) , θ −<br>2: on the current node n ∈ N . Then the product mixing p × (n, a; β) (Eq. (2)) of the {p c } c∈Q is also a member of the exponential family in <mark>canonical form</mark>: \n p × ( n , a ; β ) = exp [ β • T ( n , a ) − A ( n , β ) + B ( n , a ) ] , with T ( n , a ) = c∈Q T c ( n , a ) , B ( n , a ) = c∈Q B c (<br>",
    "Arabic": "الشكل الكنسي",
    "Chinese": "标准形式",
    "French": "forme canonique",
    "Japanese": "正規形",
    "Russian": "каноническая форма"
  },
  {
    "English": "canonical frame",
    "context": "1: In the corresponding <mark>canonical frame</mark>, the warp is initialized to the identity transform and the three rays shown in the live frame also map as straight lines in the <mark>canonical frame</mark>.<br>2: Yet we retain properties needed for consistent and accurate long-term tracking through occlusion: first, by establishing bijections between each local frame and a <mark>canonical frame</mark>, OmniMotion guarantees globally cycle-consistent 3D mappings across all local frames, which emulates the one-to-one correspondences between real-world, metric 3D reference frames.<br>",
    "Arabic": "الإطار الكنوني",
    "Chinese": "规范帧",
    "French": "repère canonique",
    "Japanese": "正準フレーム",
    "Russian": "канонический кадр"
  },
  {
    "English": "canonical space",
    "context": "1: Estimation of the volumetric model-to-frame warp field parameters (Section 3.3) 2. Fusion of the live frame depth map into the <mark>canonical space</mark> via the estimated warp field (Section 3.2) \n 3. Adaptation of the warp-field structure to capture newly added geometry (Section 3.4) \n Figure 2 provides an overview.<br>2: As in NeRF [44], we define a coordinate-based network F θ over G that maps each canonical 3D coordinate u ∈ G to a density σ and color c. The density stored in G is key, as it tells us where the surfaces are in <mark>canonical space</mark>.<br>",
    "Arabic": "المساحة الكانونية",
    "Chinese": "规范空间",
    "French": "espace canonique",
    "Japanese": "カノニカル空間",
    "Russian": "каноническое пространство"
  },
  {
    "English": "canonicalization",
    "context": "1: Although the result is still competitive with baseline FLAML, we can see that faulty <mark>canonicalization</mark> does lead to worse performance. Notably, the impact is even more severe than the setting of perturbed accuracy. We speculate that false <mark>canonicalization</mark> can be particularly misleading for the logical reasoning of large language models.<br>2: In this section, we present MLCopilot, with the formulation of the main problem and the overall architecture of our method. Then, we will describe some key components of MLCopilot in detail, including target task description, retrieval, <mark>canonicalization</mark>, and knowledge elicitation.<br>",
    "Arabic": "- التنميط القياسي",
    "Chinese": "规范化",
    "French": "canonicalisation",
    "Japanese": "正準化",
    "Russian": "канонизация"
  },
  {
    "English": "capsule network",
    "context": "1: We are also interested in extending Picture by taking insights from learning based \"analysis-by-synthesis\" approaches such as transforming auto-encoders [18], <mark>capsule networks</mark> [42] and deep convolutional inverse graphics network [25]. These models learn an implicit graphics engine in an encoder-decoder style architecture.<br>2: Further, as our network can be thought of as a simplified scene graph, it holds striking similarities to the principles of <mark>capsule networks</mark> [38], where low-level capsules (hyperplanes) are aggregated in higher (convexes) and higher (shapes) capsule representations.<br>",
    "Arabic": "شبكة كبسولة",
    "Chinese": "胶囊网络",
    "French": "réseau de capsules",
    "Japanese": "カプセルネットワーク",
    "Russian": "сеть капсул"
  },
  {
    "English": "cardinality",
    "context": "1: We experiment with different dimensionalities d 2 {5, 32, 256, 512} for the continuous tags, and different <mark>cardinalities</mark> k 2 {32, 64, 128} for the discrete tag set.<br>",
    "Arabic": "عددية",
    "Chinese": "基数",
    "French": "cardinalité",
    "Japanese": "基数",
    "Russian": "кардинальность"
  },
  {
    "English": "cardinality constraint",
    "context": "1: The densest k-subgraph and size-constrained graph cut problems correspond to submodular minimization with <mark>cardinality constraints</mark>, problems that are very hard [44]. Specialized algorithms for cardinality and related constraints were proposed e.g. in [44,35].<br>",
    "Arabic": "قيد التعداد",
    "Chinese": "基数约束",
    "French": "contrainte de cardinalité",
    "Japanese": "基数制約",
    "Russian": "ограничение кардинальности"
  },
  {
    "English": "cascade model",
    "context": "1: To define an equivalent <mark>cascade model</mark>, we need to understand the probability that an additional neighbor u can activate v, given that the nodes in a set S have already tried and failed.<br>2: The training of the classifiers took two days on a current state of art PC, which is a reasonable time to train a <mark>cascade model</mark>. Given a novel image, on average the method can search around 3000 detection windows per second.<br>",
    "Arabic": "نموذج تتالي",
    "Chinese": "级联模型",
    "French": "modèle en cascade",
    "Japanese": "連鎖モデル",
    "Russian": "каскадная модель"
  },
  {
    "English": "catastrophic forgetting",
    "context": "1: (2022) explored this idea but found that temporal adaptation is not as effective as fine-tuning on the data from whose time period the dataset is drawn. In addition, <mark>catastrophic forgetting</mark> (Robins, 1995) can also be a problem when updating the LMs. Jin et al.<br>2: As shown in Figure 6, even with as few as 8 samples per word, W2W can significantly bring down the grounded perplexity of unseen words, while mostly maintaining the grounded perplexity of the seen words without <mark>catastrophic forgetting</mark>. Compared to W2W without the grounding objective, the full W2W demonstrates better acquisition performance for unseen words.<br>",
    "Arabic": "الفقدان الكارثي",
    "Chinese": "灾难性遗忘",
    "French": "oubli catastrophique",
    "Japanese": "壊滅的な忘却",
    "Russian": "катастрофическое забывание"
  },
  {
    "English": "categorial grammar",
    "context": "1: The -h operator is then used to propagate directional dependencies in right node raising (they peeled and ate shrimp). These are similar to the 'subcat' feature in HPSG, or to the left and right slash in <mark>categorial grammar</mark> accounts of specification, complementation and right node raising. Then, interrogative and relative pronouns (e.g.<br>2: Such rules can implement local syntactic transformations, as well as certain non-local transformations like adding gap arguments to constituents containing a particular trace marker, for example. Reannotation of the <mark>categorial grammar</mark> evaluated in this paper requires about 150 such rules. These rules are modular and can be reused or modified to experiment with different syntactic analyses.<br>",
    "Arabic": "القواعد الفئوية",
    "Chinese": "范畴语法",
    "French": "grammaire catégorielle",
    "Japanese": "範疇文法",
    "Russian": "категориальная грамматика"
  },
  {
    "English": "categorical cross-entropy",
    "context": "1: The model is trained to jointly minimize the sum of the <mark>categorical cross-entropy</mark> losses on predicate identification, predicate sense disambiguation and argument identification/classification over all the languages in a multitask learning fashion.<br>2: The user representation created by the merge step is then passed to one or more dense layers before being passed to a dense output layer with a softmax activation function to perform classification. The number of dense layers used is a hyperparameter described in §5. Categorical cross-entropy is used as the model's loss function.<br>",
    "Arabic": "الانحدار الصليبي الفئوي",
    "Chinese": "类别交叉熵",
    "French": "entropie croisée catégorielle",
    "Japanese": "カテゴリカル交差エントロピー",
    "Russian": "перекрёстная энтропия категорий"
  },
  {
    "English": "categorical distribution",
    "context": "1: We anneal between our model's unnormalized logprobability f (x) and a multivariate Bernoulli or <mark>Categorical distribution</mark>, log p n (x), fit to the training data, for binary and categorical data, respectively.<br>2: At each step, a neural network f (•; θ) maps from the source sequence x and the prefix sequence y <j to the parameters of a <mark>Categorical distribution</mark> over the vocabulary of the target language.<br>",
    "Arabic": "توزيع فئوي",
    "Chinese": "类别分布",
    "French": "distribution catégorielle",
    "Japanese": "カテゴリカル分布",
    "Russian": "категориальное распределение"
  },
  {
    "English": "categorical feature",
    "context": "1: As for diversity, a widely adopted measure is the pairwise distance between counterfactual examples, with distance defined separately for numerical and <mark>categorical features</mark> [33,40]. Though this approach is meaningful for interpreting <mark>categorical features</mark>, we however find it quite obscure for numerical features.<br>2: Clarity scores for the same query may not be directly comparable across collections. Corpus-based features contributed the most. The largest contribution came from cat.specific features. Interestingly, <mark>categorical features</mark> are not derived from resources associated with a vertical (i.e., vertical documents or queries). The classifier learns to associate these features with a vertical from training data.<br>",
    "Arabic": "ميزة فئوية",
    "Chinese": "类别特征",
    "French": "caractéristique catégorielle",
    "Japanese": "カテゴリ変数",
    "Russian": "категориальный признак"
  },
  {
    "English": "causal effect",
    "context": "1: 8. Corollary 2 (Causal Effects Recovery by Adjustment). Let G be a causal diagram augmented with a variable S representing the selection mechanism. Let V be the set of variables measured under selection bias, and T ⇢ V the set of variables measured externally in the overall population. Consider disjoint sets of variables X , Y ✓ V , then the <mark>causal effect</mark> P ( y | do ( x ) ) is recoverable from { P ( v | S=1 ) , P ( t ) } by the adjustment expression ( 6 ) while Z T ✓ T , in every model inducing G if and only if Z ,<br>2: Among them, Rakesh et al. [32] propose a Linked Causal Variational Autoencoder (LCVA) framework to estimate the <mark>causal effect</mark> of a treatment on an outcome with the existence of interference between pairs of instances. Different from these works that focus on pairwise spillover effects, Ma et al.<br>",
    "Arabic": "التأثير السببي",
    "Chinese": "因果效应",
    "French": "effet causal",
    "Japanese": "因果効果",
    "Russian": "причинный эффект"
  },
  {
    "English": "causal effect estimation",
    "context": "1: In this paper, we study an important research problem of individual treatment effect estimation with the existence of high-order interference on hypergraphs. We identify and analyze the influence of high-order interference in <mark>causal effect estimation</mark>. To address this problem, we propose a novel framework HyperSCI, which estimates the ITEs based on representation learning.<br>2: Covariate adjustment is currently the most widely used method for <mark>causal effect estimation</mark> in practice, even when more powerful identification methods have been developed in recent years (Pearl 2000).<br>",
    "Arabic": "تقدير التأثير السببي",
    "Chinese": "因果效应估计",
    "French": "estimation de l'effet causal",
    "Japanese": "因果効果推定",
    "Russian": "оценка причинного эффекта"
  },
  {
    "English": "causal entropy",
    "context": "1: For IOSC, the maximum <mark>causal entropy</mark> approach provides prediction guarantees (Theorem 3) and a softened interpretation of optimal decision theory, while the latent CRF approach provides neither.<br>2: Instead, all of the agents' utilities and policies are learned from demonstrated trajectories. Maximizing the joint <mark>causal entropy</mark> of all agents' actions leads either to nonconvexity (multiplicative functions of agent policies in the feature matching constraints) or an assumption that agents share a common utility function.<br>",
    "Arabic": "الانتروبيا السببية",
    "Chinese": "因果熵",
    "French": "entropie causale",
    "Japanese": "因果エントロピー",
    "Russian": "причинная энтропия"
  },
  {
    "English": "causal graph",
    "context": "1: 10 Formally, we need to redefine s-recoverability for accommodating the availability of data from external sources. Definition 2 (s-Recoverability). Given a <mark>causal graph</mark> G s augmented with a node S, the distribution Q = P (y | x) \n<br>2: The letter G is used to refer to the <mark>causal graph</mark>, G X the graph resulting from the removal of all incoming edges to X in G, and G X the graph resulting from removing all outgoing edges from X.<br>",
    "Arabic": "الرسم البياني السببي",
    "Chinese": "因果图",
    "French": "graphe causal",
    "Japanese": "因果関係グラフ",
    "Russian": "причинно-следственный граф"
  },
  {
    "English": "causal inference",
    "context": "1: In Section 6, we will use techniques from <mark>causal inference</mark> and missing-data analysis to design unbiased and consistent estimators for R(d |x) and R(d) that only require access to the observed feedback c t .<br>2: Most recently, a new approach for de-biasing feedback data using techniques from <mark>causal inference</mark> and missing data analysis was proposed to provably eliminate selection biases [6,33]. We follow this approach in this paper, extend it to the dynamic ranking setting, and propose a new unbiased regression objective in Section 6.<br>",
    "Arabic": "الاستدلال السببي",
    "Chinese": "因果推断 (Causal Inference)",
    "French": "inférence causale",
    "Japanese": "因果推論",
    "Russian": "причинный вывод"
  },
  {
    "English": "causal intervention",
    "context": "1: Much recent debate has focused on appropriate methods for probing (Belinkov, 2021;Hewitt and Liang, 2019;Hall Maudslay et al., 2020;Pimentel et al., 2020a;Ravichander et al., 2021;White et al., 2021). Our work applies probing to German plural inflection and bolsters it using <mark>causal interventions</mark>.<br>2: Potential future work could complement our results by providing evidence from representational analyses, or by devising <mark>causal interventions</mark>, similar to those recently explored in the realm of syntactic agreement (Finlayson et al., 2021), or in testing of negation and hypernymy in NLI models (Geiger et al., 2020), among others.<br>",
    "Arabic": "التدخل السببي",
    "Chinese": "因果干预",
    "French": "intervention causale",
    "Japanese": "因果介入",
    "Russian": "причинное вмешательство"
  },
  {
    "English": "causal language model",
    "context": "1: This finding highlights the importance of employing standard left-to-right <mark>causal language models</mark> when studying the effects of predictability on incremental sentence processing.<br>",
    "Arabic": "نموذج لغوي سببي",
    "Chinese": "因果语言模型",
    "French": "modèle de langage causal",
    "Japanese": "因果言語モデル",
    "Russian": "Каузальная языковая модель"
  },
  {
    "English": "causal model",
    "context": "1: is said to be s-recoverable from selection bias in G s with external information over T ⊆ V and selection biased data over M ⊆ V (for short, s-recoverable) if the assumptions embedded in the <mark>causal model</mark> render Q expressible in terms of P (m | S = 1) and P (t), both positive. Formally , for every two probability distributions P 1 and P 2 compatible with G s , if they agree on the available distributions , P 1 ( m | S = 1 ) = P 2 ( m | S = 1 ) > 0 , P 1 ( t ) = P 2 ( t ) > 0 , they must agree<br>2: Suppose G = (V, U, F) is a <mark>causal model</mark> with nodes V, exogenous variables U, and structural equations F that define the value at each node V j as a function of its parents ℘(V j ) and its associated exogenous variable U j .<br>",
    "Arabic": "النموذج السببي",
    "Chinese": "因果模型",
    "French": "modèle causal",
    "Japanese": "因果モデル",
    "Russian": "причинно-следственная модель"
  },
  {
    "English": "causal reasoning",
    "context": "1: By explicitly stating general rules as mini-theories of how the world works, GLUCOSE seeks to enable better generalization and <mark>causal reasoning</mark> in future AI systems.<br>2: In order to provide optimal tutoring assistance CTS must likewise be able to properly infer the causes of the users' mistakes in various situations. To our knowledge, researchers in artificial intelligence have up to now limited themselves to Bayesian methods to design <mark>causal reasoning</mark> and causal learning models for cognitive agents.<br>",
    "Arabic": "الاستدلال السببي",
    "Chinese": "因果推理",
    "French": "raisonnement causal",
    "Japanese": "因果推論",
    "Russian": "казуальное рассуждение"
  },
  {
    "English": "causal rule",
    "context": "1: For state constraints, Lin (1995) argued that they should be encoded using a notion of causality, and once they are encoded this way, successor state axioms can once again be computed using predicate completion for a class of <mark>causal rules</mark> that includes almost all of the benchmark planning domains (Lin 1995;2003).<br>",
    "Arabic": "قاعدة سببية",
    "Chinese": "因果规则",
    "French": "règle causale",
    "Japanese": "因果的なルール",
    "Russian": "причинное правило"
  },
  {
    "English": "causal theory",
    "context": "1: Graphical modeling plays a key role in <mark>causal theory</mark>, allowing to express complex causal phenomena in an elegant, mathematically sound way.<br>",
    "Arabic": "نظرية السببية",
    "Chinese": "因果理论",
    "French": "théorie causale",
    "Japanese": "因果理論",
    "Russian": "теория причинности"
  },
  {
    "English": "cell state",
    "context": "1: The <mark>cell state</mark> c e(l) t is a weighted sum of three terms: a flush operation c f(l) t that erases the cell memory, a standard LSTM update c u(l) , and a copy operation c c(l) that copies the preceding <mark>cell state</mark> forward: \n<br>2: Multi-encoder NMT initializes its decoder hidden state h and <mark>cell state</mark> c using these encoder states as follows: \n h = tanh(W c [h 1 ; h 2 ])(1) \n c = c 1 + c 2 (2) \n<br>",
    "Arabic": "حالة الخلية",
    "Chinese": "细胞状态",
    "French": "état cellulaire",
    "Japanese": "セル状態",
    "Russian": "состояние ячейки"
  },
  {
    "English": "center crop",
    "context": "1: Prior to feature extraction, we center and standardize the images using ImageNet statistics, resize the shortest edge to 256 pixels and take a <mark>center crop</mark> of size 256x256. We extract spatial features of size 2048x8x8 from the final convolutional layer and apply L 2 normalization along the depth dimension (Caglayan et al., 2018).<br>",
    "Arabic": "قص مركزي",
    "Chinese": "中心裁剪",
    "French": "recadrage au centre",
    "Japanese": "中央切り抜き",
    "Russian": "центральный кроп"
  },
  {
    "English": "center of projection",
    "context": "1: When the two cameras have the same <mark>center of projection</mark> (and differ only in their 3D orientation and their internal calibration parameters), then a simple fixed homography H (a 2D projective transformation) describes the spatial transformation between temporally Figure 1.<br>2: Central (pin-hole) camera models have the property that all rays pass through a single point in 3D space, known as the <mark>center of projection</mark>. Driven by applications in graphics [10,12,13] and stereo matching [3,5], several researchers have proposed view representations that do not have a single distinguished <mark>center of projection</mark>.<br>",
    "Arabic": "مركز الإسقاط",
    "Chinese": "投影中心",
    "French": "centre de projection",
    "Japanese": "射影中心",
    "Russian": "центр проекции"
  },
  {
    "English": "centrality measure",
    "context": "1: Noise and issues of sampling are well recognized as fundamental challenges in complex networks , and there has been some work on characterizing it and the sensitivity to different parameters , especially in network properties , such as : ( i ) ( Costenbader and Valente 2003 ; Borgatti , Carley , and Krackhardt 2006 ) show that certain <mark>centrality measures</mark> are robust to<br>2: Beyond providing visually compelling fingerprints of graphs, we show how the estimation of spectral densities facilitates the computation of many common <mark>centrality measures</mark>, and use spectral densities to estimate meaningful information about graph structure that cannot be inferred from the extremal eigenpairs alone.<br>",
    "Arabic": "مقياس المركزية",
    "Chinese": "中心性测度",
    "French": "mesure de centralité",
    "Japanese": "中心性尺度",
    "Russian": "мера центральности"
  },
  {
    "English": "centroid",
    "context": "1: For most tasks, we use the <mark>centroid</mark> of each input structure as center of the grid, excluding LBA where we use the <mark>centroid</mark> of the ligand as center and MSP where we use the mutation point as center.<br>2: In our method, a cluster C is a set of queries. The normalized <mark>centroid</mark> of the cluster is \n − → c = norm( q i ∈C − → q i |C| ), \n where |C| is the number of queries in C. The distance between a query q and a cluster C is given by \n<br>",
    "Arabic": "مركز الثقل",
    "Chinese": "质心",
    "French": "barycentre",
    "Japanese": "重心",
    "Russian": "центроид"
  },
  {
    "English": "chain rule",
    "context": "1: The adjoint state is the gradient with respect to the hidden state at a specified time t. In standard neural networks, the gradient of a hidden layer ht depends on the gradient from the next layer ht+1 by <mark>chain rule</mark> \n dL dht = dL dht+1 dht+1 dht . (36 \n ) \n<br>2: With a continuous hidden state, we can write the transformation after an ε change in time as \n z(t + ε) = t+ε t f (z(t), t, θ)dt + z(t) = Tε(z(t), t)(37) \n and <mark>chain rule</mark> can also be applied \n<br>",
    "Arabic": "قاعدة السلسلة",
    "Chinese": "链式法则",
    "French": "règle de la chaîne",
    "Japanese": "連鎖法則",
    "Russian": "правило цепочки"
  },
  {
    "English": "change of basis",
    "context": "1: Without loss of generality we assume M is a diagonal matrix with first r diagonal terms being σ 1 (Z) 2 , σ 2 (Z) 2 , ..., σ r (Z) 2 (this can be done by a <mark>change of basis</mark>).<br>",
    "Arabic": "تغيير الأساس",
    "Chinese": "基底变换",
    "French": "changement de base",
    "Japanese": "基底の変換",
    "Russian": "Смена базиса"
  },
  {
    "English": "character embedding",
    "context": "1: Then, we concatenate v st with the <mark>character embeddings</mark> v c 1 and v c 2 as well as the book embedding v b and feed the resulting vector into a standard feedforward layer to obtain a hidden state h t , \n<br>2: For word CNN based models, a large η leads to convergence problem. We take η = 0.005 with more epochs (200) instead. GloVe 100-dimension (Pennington et al., 2014) is used to initialize word embeddings and <mark>character embeddings</mark> are randomly initialized.<br>",
    "Arabic": "تضمين الأحرف",
    "Chinese": "字符嵌入",
    "French": "intégration de caractères",
    "Japanese": "文字埋め込み",
    "Russian": "вложение символов"
  },
  {
    "English": "character n-gram",
    "context": "1: Team UAIC1860 (Ermurachi and Gifu, 2020)(SI: 28, TC: 26) used traditional text representation techniques: <mark>character n-grams</mark>, word2vec embeddings, and TF.IDF-weighted word-based features. For both subtasks, these features were used in a Random Forest classifier. Additional experiments with Naïve Bayes, Logistic Regression and SVMs yielded worse results.<br>",
    "Arabic": "نغرامات الأحرف",
    "Chinese": "字符n-gram",
    "French": "n-gramme de caractères",
    "Japanese": "文字n-gram",
    "Russian": "символьные n-граммы"
  },
  {
    "English": "characteristic function",
    "context": "1: with χ S the <mark>characteristic function</mark> of the subset S, i.e. χ S (i) = 1 iff i ∈ S. Since the graph G is connected, we can bound h S,α from above as \n . It was proven in (Chung, 2007, Lemma 5) that \n<br>2: Some properties of the stable distribution, not proved but straightforward from the definition of the <mark>characteristic function</mark>, will help to better clarify their meaning. Addition of constant. Let X ∼ S α (σ, β, μ) and let a be a real parameter.<br>",
    "Arabic": "دالة مميزة",
    "Chinese": "特征函数",
    "French": "fonction caractéristique",
    "Japanese": "特性関数",
    "Russian": "характеристическая функция"
  },
  {
    "English": "characteristic polynomial",
    "context": "1: Noting that the infinite homography is conjugate to a rotation matrix and must have eigenvalues of equal moduli, one can relate the roots of its <mark>characteristic polynomial</mark> \n det(λI − (A i − a i p )) = λ 3 − α i λ 2 + β i λ − γ i . (3) \n<br>2: For matrix inversion, when dealing with separation power, one can use different expressions in TL(Ω) for computing the matrix inverse, depending on the input size. And indeed , it is well-known ( see e.g. , Csanky ( 1976 ) ) that based on the <mark>characteristic polynomial</mark> of A , A −1 for any matrix A ∈ R n×n can be computed as a polynomial −1 cn n−1 i=1 c i A n−1−i if c n = 0 and where each coefficient c i is a polynomial in tr (<br>",
    "Arabic": "كثير الحدود مميزة",
    "Chinese": "特征多项式",
    "French": "polynôme caractéristique",
    "Japanese": "固有多項式",
    "Russian": "характеристический многочлен"
  },
  {
    "English": "characteristic vector",
    "context": "1: which is an instance of the standard MIPS problem. It should be noted that we do not have control over the norm of the learned <mark>characteristic vector</mark>, i.e., v j 2 , which often has a wide range in practice [17].<br>2: if S [ i , j ] is encoded with Elias-Fano ; ii ) m bits , if S [ i , j ] is encoded with its <mark>characteristic vector</mark> ; iii ) 0 bits , if m = u and , thus , S [ i , j ] covers the whole universe .<br>",
    "Arabic": "متجه الخصائص",
    "Chinese": "特征向量",
    "French": "vecteur caractéristique",
    "Japanese": "特性ベクトル",
    "Russian": "характеристический вектор"
  },
  {
    "English": "chart parser",
    "context": "1: This is despite the fact that both systems can discard speculative attachment decisions. In the case of the <mark>chart parser</mark> with representations that consist of label probabilities for each span, adding an additional word can cause a switch to a new analysis by way of the CKY decoding procedure.<br>",
    "Arabic": "محلل الرسم البياني",
    "Chinese": "图表解析器",
    "French": "analyseur de graphe",
    "Japanese": "チャートパーサー",
    "Russian": "синтаксический анализатор на основе диаграмм"
  },
  {
    "English": "chart parsing",
    "context": "1: The Hiero decoder (Chiang, 2007) does not require explicit syntactic representation on either side of the rules. Both source and target are strings with NTs. Decoding is solved as <mark>chart parsing</mark>. Hiero can be viewed as a hierarchical string-to-string model. Ding and Palmer (2005) and Quirk et al.<br>2: Following previous work on hierarchical MT (Chiang, 2005;Galley et al., 2006), we solve decoding as <mark>chart parsing</mark>. We view target dependency as the hidden structure of source fragments. The parser scans all source cells in a bottom-up style, and checks matched transfer rules according to the source side.<br>",
    "Arabic": "تحليل الرسم البياني",
    "Chinese": "图表解析",
    "French": "analyse syntaxique en tableau",
    "Japanese": "チャート解析",
    "Russian": "\"синтаксический разбор графа\""
  },
  {
    "English": "chatbot",
    "context": "1: Jailbreaking most frequently refers to using special prompts to manipulate a <mark>chatbot</mark> into a state in which it is able to generate morally questionable content, which usually goes against the terms of service of the given platform.<br>2: Human-Bot Conversations. In order to perform interactive multi-turn evaluations, the standard method is to let humans converse with a <mark>chatbot</mark> and rate it afterward (Ghandeharioun et al., 2019), typically using Likert scales (van der Lee et al., 2019).<br>",
    "Arabic": "محادثة آلية",
    "Chinese": "聊天机器人",
    "French": "agent conversationnel",
    "Japanese": "チャットボット",
    "Russian": "чат-бот"
  },
  {
    "English": "checkpoint",
    "context": "1: The best <mark>checkpoint</mark> was selected and then trained on an 80-20 split of its original dataset for 50 epochs with the same batch size and learning rate and evaluated every 10 steps.<br>2: Specifically, if upper(φ ′ ) is less than the threshold θ, then we can guarantee that item pair {a, b} will never become a strongly-correlated pair prior to the next <mark>checkpoint</mark>. As a result, we can save intermediate computation results only for pairs having φ ′ beyond the threshold θ.<br>",
    "Arabic": "نقطة تفتيش",
    "Chinese": "检查点",
    "French": "point de contrôle",
    "Japanese": "チェックポイント",
    "Russian": "контрольная точка"
  },
  {
    "English": "chemoinformatic",
    "context": "1: Then the connection table of reactants and products are examined to find the reactivity flag and write the dynamical bond in the CGR. This change of representation allows one to store the reaction database in the format (SD) usual in <mark>chemoinformatics</mark> to represent individual molecules.<br>2: Moreover we developed an algorithm that generates a CGR from the file formats (RXN and RD) usual in <mark>chemoinformatics</mark> to model reactions. This programs requires the information about the atom mapping in reactants and products. This information is often available from existing software to edit and manage chemical data, otherwise it can be added thanks to our editor.<br>",
    "Arabic": "الكيمومعلوماتية",
    "Chinese": "化学信息学",
    "French": "chémoinformatique",
    "Japanese": "化学情報学",
    "Russian": "хемоинформатический"
  },
  {
    "English": "chernoff bind",
    "context": "1: The probability that at least a (1/2 + δ ) fraction of these t random formulas is unsatisfiable equals Pr \n Using the <mark>Chernoff bound</mark> separately for α = 1, 2 as in Lemma 2, this probability is bounded above by p(t, δ , α). Proof of Theorem 3.<br>2: A similar reasoning with too few XORs provides an upper bound, though with some complications arising from the lack of pairwise-independence of small XORs. We will use standard bounds on the concentration of moments of a probability distribution, namely, Markov's inequality, Chebyshev's inequality, and the <mark>Chernoff bound</mark> (cf. Motwani & Raghavan 1994).<br>",
    "Arabic": "حد شيرنوف",
    "Chinese": "切诺夫界 (Chernoff bound)",
    "French": "borne de Chernoff",
    "Japanese": "チェルノフ境界",
    "Russian": "граница Чернова"
  },
  {
    "English": "chi-square distribution",
    "context": "1: Thus, to check for an anomaly at confidence level α, one checks whether Λ(X) ≥ c, where c is a non-negative number computed by finding how far out in the tail of a <mark>chi-square distribution</mark> one has to go to find (1 − α)% of the mass.<br>",
    "Arabic": "توزيع كاي مربع",
    "Chinese": "卡方分布",
    "French": "distribution du chi carré",
    "Japanese": "カイ二乗分布",
    "Russian": "распределение хи-квадрат"
  },
  {
    "English": "chi-square test",
    "context": "1: We can extend the principles in Fas-tANOVA for efficient two-locus <mark>chi-square test</mark> [Zhang et al. 2009b].<br>2: The reason for separating these two components is that each of these two components is a convex function (See Lemma 7.2). The G-test, also known as a likelihood ratio test for goodness of fit, is an alternative to the <mark>chi-square test</mark>. The formula for two-locus G-test is \n<br>",
    "Arabic": "اختبار كاي تربيع",
    "Chinese": "卡方检验",
    "French": "test du chi carré",
    "Japanese": "カイ二乗検定",
    "Russian": "критерий хи-квадрат"
  },
  {
    "English": "child node",
    "context": "1: At each level, the potential costly operation is the access of the <mark>child node</mark> cn from the parent node pn (the last statement in line 2 of Method findNode). We use a heap structure to support the dynamic insertion and access of the <mark>child node</mark>s.<br>2: Thus, low-ranked queries will have a negative result and be classified to the left <mark>child node</mark> and those with the higher rank will have a positive value for the multiplication and be classified to the right <mark>child node</mark>. Scores are assigned to nodes so that the left part of the tree will have a lower score compared to its right part.<br>",
    "Arabic": "العقدة الفرعية",
    "Chinese": "子节点",
    "French": "nœud enfant",
    "Japanese": "子ノード",
    "Russian": "дочерний узел"
  },
  {
    "English": "cholesky decomposition",
    "context": "1: HMC augments the Markov chain with random \"momenta\" and then simulates Hamiltonian dynamics in fictional time. In this way, Metropolis-Hastings proposals are guided using gradient information and random walk behavior is avoided. We perform gradient calculations in the \"whitened\" space resulting from transforming the function values with the inverse <mark>Cholesky decomposition</mark> of the covariance matrix.<br>2: We perform gradient calculations in the \"whitened\" space resulting from linearly transforming g M+K with the inverse <mark>Cholesky decomposition</mark> of the covariance matrix Σ, as this results in a betterconditioned space for calculations. The log conditional posterior distribution is ln p(g \n<br>",
    "Arabic": "تحلل تشولسكي",
    "Chinese": "Cholesky分解",
    "French": "décomposition de Cholesky",
    "Japanese": "コレスキー分解",
    "Russian": "разложение Холецкого"
  },
  {
    "English": "cholesky factor",
    "context": "1: The standard way of generating these samples is via a location-scale transform of Gaussian random variables ζ ∼ N (0, I), namely \n f * | y = m * |n + K 1/2 * , * |n ζ,(2) \n where (•) 1/2 denotes a matrix square root, such as a <mark>Cholesky factor</mark>.<br>",
    "Arabic": "عامل شوليسكي",
    "Chinese": "乔列斯基因子",
    "French": "facteur de Cholesky",
    "Japanese": "コレスキー因子",
    "Russian": "фактор Холецкого"
  },
  {
    "English": "cholesky factorization",
    "context": "1: Recently, [37] demonstrated a real-time solution to a related non-rigid tracking optimization using a GPU accelerated pre-conditioned conjugate gradient descent solver. We take a different approach, using instead a direct sparse <mark>Cholesky factorization</mark> of each linearised system. We note that direct solvers resolve lowfrequency residuals very effectively, which is critical to ensuring minimal drift during reconstruction.<br>2: Given the <mark>Cholesky factorization</mark> of T, det(K T ) can be computed O(M ) as it is the product of the square of the diagonal elements. It therefore remains to consider the calculation of L T , a Cholesky factor of K T .<br>",
    "Arabic": "التخصيم الكوليسكي",
    "Chinese": "乔列斯基分解",
    "French": "factorisation de Cholesky",
    "Japanese": "コレスキー分解",
    "Russian": "Разложение Холецкого"
  },
  {
    "English": "chromosome",
    "context": "1: The features represented by the <mark>chromosome</mark> are used to cluster the training data. The resultant ratio of the \"between\" and \"within\" cluster dispersion matrices [25] is referred to as the quality (fitness) of the <mark>chromosome</mark>.<br>2: Initialize each <mark>chromosome</mark> to contain K number of randomly selected cluster centers of query string at the 1st level and K (randomly chosen) activation thresholds in [0, 1] in session q. Step 2.<br>",
    "Arabic": "صبغي",
    "Chinese": "染色体",
    "French": "chromosome",
    "Japanese": "染色体",
    "Russian": "хромосома"
  },
  {
    "English": "chunk size",
    "context": "1: For the streaming case, the speech encoder can access speech data in all chunks before and one chunk ahead of the current timestep Shi et al., 2020;Liu et al., 2021). We sweep over <mark>chunk size</mark> from 160ms to 640ms.<br>2: For example, when a <mark>chunk size</mark> is predefined as 250, the total number of generated classifiers becomes 200. To build a new classifier, existing methods selected only samples of 10% from a chunk, and then used their correct classes.<br>",
    "Arabic": "حجم القطعة",
    "Chinese": "块大小",
    "French": "taille de bloc",
    "Japanese": "チャンクサイズ",
    "Russian": "размер фрагмента"
  },
  {
    "English": "citation network",
    "context": "1: • Cora, CiteSeer, PubMed, ogbn-arxiv: These 4 different <mark>citation networks</mark> are widely used as graph benchmarks (Sen et al. 2008;Hu et al. 2020). We conduct node classification tasks on each dataset to determine the research area of papers/researchers.<br>",
    "Arabic": "شبكة الاقتباسات",
    "Chinese": "引文网络",
    "French": "réseau de citations",
    "Japanese": "引用ネットワーク",
    "Russian": "сеть цитирования"
  },
  {
    "English": "class",
    "context": "1: Specifically, the main assumption in SSL is that data of the same <mark>class</mark> will cluster together; the labeled data provide the appropriate <mark>class</mark> and we use the unlabeled data to determine the cluster boundary.<br>2: Assume that at the end leaf for <mark>class</mark> c i ∈ {c 1 , c 2 , ..., c C }, there are N instances belonging to C l ≤ C <mark>class</mark>es, and that k instances belong to <mark>class</mark> c i .<br>",
    "Arabic": "فئة",
    "Chinese": "类别",
    "French": "classe",
    "Japanese": "クラス",
    "Russian": "класс"
  },
  {
    "English": "class balance",
    "context": "1: We control for a number of potential confounding factors, such as <mark>class balance</mark>, and the syntactic governor of the triggering adverb, so that models cannot exploit these correlating factors without any actual understanding of the presuppositional properties of the context.<br>",
    "Arabic": "توازن الفئات",
    "Chinese": "类平衡",
    "French": "équilibre des classes",
    "Japanese": "クラスバランス",
    "Russian": "Сбалансированность классов"
  },
  {
    "English": "class distribution",
    "context": "1: As recent publications have already adopted smalltext, we present four examples which have already successfully utilized it for their experiments. Abusive Language Detection Kirk et al. (2022) investigated the detection of abusive language using transformer-based active learning on six datasets of which two exhibited a balanced and four an imbalanced <mark>class distribution</mark>.<br>2: For binary classification tasks, we use binary cross-entropy weighted by the <mark>class distribution</mark> (i.e. rarer class is weighted more heavily on mistakes). To address issues with imbalanced datasets, we randomly oversample/undersample the less/more frequent class respectively during training. For regression tasks, we use mean-squared error loss for training.<br>",
    "Arabic": "توزيع الفئات",
    "Chinese": "类别分布",
    "French": "distribution des classes",
    "Japanese": "クラス分布",
    "Russian": "распределение классов"
  },
  {
    "English": "class imbalance",
    "context": "1: The <mark>class imbalance</mark> of this dataset (Table 2) results in the Most Popular scoring highly. To mitigate this, we evaluate on balanced datasets. Class imbalanced training approaches Spangher et al., 2021a) might be of further help.<br>2: For context, if pruning according to a certain metric preferentially leads to discarding most (or even all) images from certain classes, it is likely that the performance on those classes will drop as a result if this imbalance is not addressed. Class imbalance metric: pruning amplifies <mark>class imbalance</mark>. Since a standard measure of class ( im- ) balance-dividing the number of images for the majority class by the number of images for the minority class-is highly sensitive to outliers and discards information about the 998 non-extreme ImageNet classes , we instead calculated a class balance score b ∈ [ 0 % , 100 % ] as the average <mark>class imbalance</mark> across any<br>",
    "Arabic": "عدم التوازن الصنفي",
    "Chinese": "类别不平衡",
    "French": "déséquilibre de classe",
    "Japanese": "クラスの不均衡",
    "Russian": "дисбаланс классов"
  },
  {
    "English": "class label",
    "context": "1: the remaining training data do not suffice to correctly learn this example). Additionally [13] also considered an influence score that quantifies how much adding a particular example to the training set increases the probability of the correct <mark>class label</mark> of a test example.<br>2: To classify an unknown instance, the performance element finds the example in the collection most similar to the unknown and returns the example's <mark>class label</mark> as its prediction for the unknown. For Boolean attributes, such as ours, a convenient measure of similarity is the number of values two instances have in common.<br>",
    "Arabic": "تسمية الطبقة",
    "Chinese": "类标签",
    "French": "étiquette de classe",
    "Japanese": "クラスラベル",
    "Russian": "метка класса"
  },
  {
    "English": "class prior",
    "context": "1: parameter , and c d ( κ ) is the normalization factor . We further assume a uniform <mark>class prior</mark> P (y i = j) = 1/C. Let n j = |S j |. Then, optimizing Eq. (15) and Eq. ( 16) equal to maximize R 1 and R 2 below, respectively.<br>2: \"can\" for a backpack) is used, we run into contention between our subject and and the <mark>class prior</mark> -sometimes obtaining cylindrical backpacks, or otherwise misshapen subjects. If we train with no class noun, the model does not leverage the <mark>class prior</mark>, has difficulty learning the subject and converging, and can generate erroneous samples.<br>",
    "Arabic": "الأولوية الصنفية",
    "Chinese": "类先验",
    "French": "prior de classe",
    "Japanese": "クラス事前確率",
    "Russian": "априорная вероятность класса"
  },
  {
    "English": "classical planning",
    "context": "1: In this section, we review <mark>classical planning</mark>, description logic and its use in <mark>classical planning</mark>.<br>",
    "Arabic": "التخطيط الكلاسيكي",
    "Chinese": "经典规划",
    "French": "planification classique",
    "Japanese": "古典的計画",
    "Russian": "классическое планирование"
  },
  {
    "English": "classification",
    "context": "1: They are difficult to interpret and do not clearly identify the interacting SNPs. Recursive partitioning methods [Zhang and Bonney 2000;Province et al. 2001] utilize <mark>classification</mark> and regression tree (CART) [Breiman et al. 1984] to pick the SNP that minimizes some pre-specified measure of impurity in each iteration.<br>2: They learn representations of a user's activity and of the four self-harm risk severity labels; <mark>classification</mark> is performed by comparing the euclidean distance between a representation of a user's activity (produced by the final layer) and each of the four severity label representations.<br>",
    "Arabic": "التصنيف",
    "Chinese": "分类",
    "French": "classification",
    "Japanese": "分類",
    "Russian": "классификация"
  },
  {
    "English": "classification accuracy",
    "context": "1: Often, many features can be extracted, and different features have a different contribution to <mark>classification accuracy</mark>. Moreover, the computational complexity of the classification process is affected by the number of features used.<br>2: The main advantage of the proposed marginal regression training method is that one could easily run experiments with larger dictionary sizes, which typically takes a significantly longer time for other algorithms. For both the Caltech-101 and 15-scene data set, <mark>classification accuracy</mark> increases significantly with increasing dictionary sizes as seen in Table 3.<br>",
    "Arabic": "دقة التصنيف",
    "Chinese": "分类准确率",
    "French": "précision de classification",
    "Japanese": "分類精度",
    "Russian": "точность классификации"
  },
  {
    "English": "classification algorithm",
    "context": "1: We also note that, in our method 25% of false positives come from a single textured image, where the training set does not include a similar image. In the second experiment, we consider an empirical val- idation of the presented <mark>classification algorithm</mark> on Riemannian manifolds.<br>2: As an alternative, it would be desirable to have a <mark>classification algorithm</mark> that can work with triplets directly, without taking a detour via ordinal embedding. To the best of our knowledge, for the case of passively obtained triplets, this problem has not yet been solved in the literature.<br>",
    "Arabic": "خوارزمية التصنيف",
    "Chinese": "分类算法",
    "French": "algorithme de classification",
    "Japanese": "分類アルゴリズム",
    "Russian": "алгоритм классификации"
  },
  {
    "English": "classification approach",
    "context": "1: We have proposed a novel classification learning strategy that coordinates the prediction of test labels on a graph over the data. The coordination classification idea can be used to extend any probabilistic <mark>classification approach</mark> quite naturally, and even seems to be applicable to nonprobabilistic techniques as well (although more research needs to be done).<br>",
    "Arabic": "نَهْجُ التَّصْنِيفِ",
    "Chinese": "分类法",
    "French": "approche de classification",
    "Japanese": "分類アプローチ",
    "Russian": "метод классификации"
  },
  {
    "English": "classification error",
    "context": "1: As in the linear experiments, we chose a primal suboptimality threshold for each dataset which guarantees a testing <mark>classification error</mark> within 10% of that at the optimum. The runtime required to achieve these targets, along with the test <mark>classification error</mark>s, are reported in Table 2.<br>2: During training, we adopt the data augmentation of [58] as implemented by [17]. We evaluate the top-1 <mark>classification error</mark> on the center crops of 224×224 pixels in the validation set. To reduce random variations, we report the median error rate of the final 5 epochs [16].<br>",
    "Arabic": "خطأ التصنيف",
    "Chinese": "分类错误率",
    "French": "erreur de classification",
    "Japanese": "分類誤差",
    "Russian": "ошибка классификации"
  },
  {
    "English": "classification head",
    "context": "1: Therefore, in this section we want to measure the degree of selfconsistency within the trained multitask model. Specifically, we take the predictions of the best ensemble of models and calculate the percentage of contradiction between each <mark>classification head</mark> (see Table 5).<br>2: To achieve even higher accuracy on downstream tasks, we adapt the entire model for classification through fine-tuning. Building off of the previous analysis, we tried attaching the <mark>classification head</mark> to the layer with the best representations.<br>",
    "Arabic": "رأس التصنيف",
    "Chinese": "分类头",
    "French": "tête de classification",
    "Japanese": "分類ヘッド",
    "Russian": "классификационная головка"
  },
  {
    "English": "classification loss",
    "context": "1: Shattered Gradients are caused when a defense is nondifferentiable, introduces numeric instability, or otherwise causes a gradient to be nonexistent or incorrect. Defenses that cause gradient shattering can do so unintentionally, by using differentiable operations but where following the gradient does not maximize <mark>classification loss</mark> globally.<br>2: The uncertainty in the label space posits a unique obstacle for learning effective representations. In PiCO, we couple the <mark>classification loss</mark> in Eq. (1) with a contrastive term that facilitates a clustering effect in the embedding space. While contrastive learning has been extensively studied in recent literature, it remains untapped in the domain of PLL.<br>",
    "Arabic": "خسارة التصنيف",
    "Chinese": "分类损失",
    "French": "perte de classification",
    "Japanese": "分類損失",
    "Russian": "потеря классификации"
  },
  {
    "English": "classification margin",
    "context": "1: Let t i = ci−cj σi(M ) and z i be the probability distribution of the <mark>classification margin</mark> over class i, we have the following robustness condition \n<br>2: where σ i (M ) is the variance of the <mark>classification margin</mark> z i . It is shown in (Horváth et al.<br>",
    "Arabic": "هامش التصنيف",
    "Chinese": "分类边距",
    "French": "marge de classification",
    "Japanese": "分類マージン",
    "Russian": "классификационный отступ"
  },
  {
    "English": "classification method",
    "context": "1: We finally consider unsolvability heuristics in the form of a binary decision tree [Breiman et al., 1984], a well-known <mark>classification method</mark> that is easy to interpret and fast to train.<br>2: The results of this study also showed that parameter α used for obtaining similarity features highly depends both on <mark>classification method</mark> (see the best values in line 3 of Table 1) and data permutation (see the percentage of use of best α value in line 4 of Table 1).<br>",
    "Arabic": "طريقة التصنيف",
    "Chinese": "分类方法",
    "French": "méthode de classification",
    "Japanese": "分類手法 (ぶんるいしゅほう)",
    "Russian": "метод классификации"
  },
  {
    "English": "classification metric",
    "context": "1: We choose tasks that contain outcomes belonging to a small set of labels, unlike natural language generation tasks which have a large solution space. This discrete nature of the outcomes allows us to quantify the performance of MT metrics based on standard <mark>classification metrics</mark>.<br>",
    "Arabic": "مقياس التصنيف",
    "Chinese": "分类指标",
    "French": "métrique de classification",
    "Japanese": "分類指標",
    "Russian": "классификационная метрика"
  },
  {
    "English": "classification model",
    "context": "1: The examples from the data pool that are predicted to have the highest probability of the rare class by the <mark>classification model</mark> from previous iteration are selected. RANDOM As a baseline, we randomly sample examples from the data pool, which reflects the natural distribution of classes.<br>2: These studies all demonstrate the beneficial impact of using ensemble classifiers over single classifiers for classification performance in the context of churn prediction. An often-used performance criterion in churn prediction is lift (e.g. [7,8]). Lift measures how many times the <mark>classification model</mark> improves the identification of potential churners in the selection, over random guessing.<br>",
    "Arabic": "نموذج تصنيف",
    "Chinese": "分类模型",
    "French": "modèle de classification",
    "Japanese": "分類モデル",
    "Russian": "модель классификации"
  },
  {
    "English": "classification network",
    "context": "1: This means the network has to simultaneously switch to learning object detection and adjust to the new input resolution. For YOLOv2 we first fine tune the <mark>classification network</mark> at the full 448 × 448 resolution for 10 epochs on ImageNet. This gives the network time to adjust its filters to work better on higher resolution input.<br>",
    "Arabic": "شبكة التصنيف",
    "Chinese": "分类网络",
    "French": "réseau de classification",
    "Japanese": "分類ネットワーク",
    "Russian": "сеть классификации"
  },
  {
    "English": "classification objective",
    "context": "1: In contrast, DPO directly optimizes for the policy best satisfying the preferences with a simple <mark>classification objective</mark>, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form. we will show that the RL-based objective used by existing methods can be optimized exactly with a simple binary cross-entropy objective, greatly simplifying the preference learning pipeline.<br>",
    "Arabic": "هدف التصنيف",
    "Chinese": "分类目标",
    "French": "objectif de classification",
    "Japanese": "分類目的",
    "Russian": "классификационная цель"
  },
  {
    "English": "classification problem",
    "context": "1: Consider a <mark>classification problem</mark> with input and (finite) output spaces given by X and Y, respectively. A decision tree is a tree-structured classifier consisting of decision (or split) nodes and prediction (or leaf) nodes.<br>2: Moreover, any importance-weighted <mark>classification problem</mark> can be reduced to a uniform-weighted <mark>classification problem</mark> [35], often performing better than handcrafted weighted-classification algorithms.<br>",
    "Arabic": "مشكلة التصنيف",
    "Chinese": "分类问题",
    "French": "problème de classification",
    "Japanese": "分類問題",
    "Russian": "классификационная задача"
  },
  {
    "English": "classification score",
    "context": "1: The sliding window principle treats localization as localized detection, applying a classifier function subsequently to subimages within an image and taking the maximum of the <mark>classification score</mark> as indication for the presence of an object in this region. However, already an image of as low resolution as 320 × 240 contains more than one billion rectangular subimages.<br>2: We further supplement the final corpus with messages from a sample of 200 million tweets (also disjoint from all other corpora considered here) that C o classified as \"other\" with high probability. We apply thresholding on the <mark>classification score</mark> to reduce the noise in the cascade, as shown in Fig. 2.<br>",
    "Arabic": "درجة التصنيف",
    "Chinese": "分类得分",
    "French": "score de classification",
    "Japanese": "分類スコア",
    "Russian": "оценка классификации"
  },
  {
    "English": "classification task",
    "context": "1: We framed automatic factuality assessment as a <mark>classification task</mark> in which a separate classifier is trained for each category (Insertion, Deletion, and Substitution), for each of the levels 0, 1, and 2.<br>2: While active learning may be used to reduce the amount of labeling data, many approaches do not consider the cost of annotating, which is often significant in a biomedical imaging setting. In this work we show how annotation cost can be considered and learned during active learning on a <mark>classification task</mark> on the MNIST dataset.<br>",
    "Arabic": "مهمة التصنيف",
    "Chinese": "分类任务",
    "French": "tâche de classification",
    "Japanese": "分類タスク",
    "Russian": "классификационная задача"
  },
  {
    "English": "classification token",
    "context": "1: Specifically, we encode the article's text, truncated to the first 512 tokens, and extract the representation of the special <mark>classification token</mark> from the last hidden layer as a set of feature values.<br>",
    "Arabic": "علامة التصنيف",
    "Chinese": "分类标记符",
    "French": "jeton de classification",
    "Japanese": "分類トークン",
    "Russian": "токен классификации"
  },
  {
    "English": "classifier",
    "context": "1: Therefore, the classifier Classif ier, applies I into C: \n Classif ier : I −→ C \n Although Stack-k-NN behaves satisfactory with this simple setting for a small size problem [1], it is not to be expected that the method may scale up to systems with hundreds of observations and fault modes.<br>2: Classif ier-C i : I −→ C i \n These local classifiers, Classif ier-C i , still work on the same input space, but have the potential to work with a smaller number of classes. Possible Conflict Decomposition: Attribute and Class Decomposition.<br>",
    "Arabic": "مصنف",
    "Chinese": "分类器",
    "French": "classificateur",
    "Japanese": "分類器",
    "Russian": "классификатор"
  },
  {
    "English": "clause learning",
    "context": "1: • QCDCL, which can be seen as the plain model where we can only make decisions following the level order of the quantifier prefix, make propagations using clauses and use classic <mark>clause learning</mark>. We will never learn or use cubes and pure-literal elimination is turned off.<br>2: Based on the classic DPLL algorithm from the 1960s, it combines a number of advanced features, including <mark>clause learning</mark>, efficient Boolean constraint propagation, decision heuristics, restart strategies, and many more.<br>",
    "Arabic": "تعلم البنود",
    "Chinese": "子句学习",
    "French": "apprentissage de clauses",
    "Japanese": "節学習",
    "Russian": "обучение клаузам"
  },
  {
    "English": "click model",
    "context": "1: A key challenge of dynamic LTR lies in the fact that the feedback c t provides meaningful feedback only for the items that the user examined. Following a large body of work on <mark>click models</mark> [19], we model this as a censoring process.<br>2: However, we did observe that the performance of DSP varies under different <mark>click models</mark> for simulated user click feedback, i.e., different underlying examination behaviors. As for our future work, we plan to incorporate different click modeling solutions for more accurate document space construction.<br>",
    "Arabic": "نموذج النقرات",
    "Chinese": "点击模型",
    "French": "modèle de clics",
    "Japanese": "クリックモデル",
    "Russian": "модель кликов"
  },
  {
    "English": "clip range",
    "context": "1: All agents are trained using PPO in stable-baselines [Hill et al., 2018] with batch size=512, n epochs=15, n steps=2048, <mark>clip range</mark>=0.1, learning rate=0.0001. All policy networks and value networks have two hidden layers of size 64. All the other hyperparameters are set to default as in stable-baselines [Hill et al., 2018].<br>",
    "Arabic": "نطاق القص",
    "Chinese": "夹持范围",
    "French": "plage de clipping",
    "Japanese": "クリップ範囲",
    "Russian": "диапазон обрезки"
  },
  {
    "English": "clipping factor",
    "context": "1: Instead of learning the <mark>clipping factor</mark>, LSQ (Esser et al., 2020) learns the step size α/n, but requires a careful initialization and gradient update.<br>2: In this work, we re-parameterize the <mark>clipping factor</mark> to make the quantizer adaptive to each module in the Transformer layers, and consider both weights outside and inside the clipping range when estimating the gradient of the <mark>clipping factor</mark>.<br>",
    "Arabic": "عامل التقليم",
    "Chinese": "裁剪因子",
    "French": "facteur d'écrêtage",
    "Japanese": "クリッピング係数",
    "Russian": "фактор обрезки"
  },
  {
    "English": "clipping threshold",
    "context": "1: Based on the proposed confidence-guided strategy, we develop a brand-new variant of memory-Algorithm 2: CAME Optimizer Input: Initial parameters θ 0 , learning rate η, momentum of update m 0 = 0, r 0 = 0, c 0 = 0, step t = 0, regularization constants ϵ 1 , ϵ 2 , exponential moving average parameters \n β 1 , β 2 , β 3 , <mark>clipping threshold</mark> d while θ t not converge do Compute g t = ∇f ( θ t−1 ) r t = β 2 r t−1 + ( 1 − β 2 ) ( g 2 t + ϵ 1 1 n 1 T m ) 1 m c t = β 2 c t−1 + ( 1 − β 2 ) 1 T n ( g 2 t + ϵ 1 1 n 1 T m ) v t = r t c t /1 T n r t u t = g t / √ v t u t = u t /max ( 1 , RM S ( u t ) /d ) m t = β 1 m t−1 + ( 1 − β 1 ) û t U t = ( û t − m t ) 2 R t = β 3 R t−1 + ( 1 − β 3 ) ( U t + ϵ 2 1 n 1 T m ) 1 m C t = β 3 C t−1 + ( 1 − β 3<br>2: We apply Clippy and other baselines to non-embedding model parameters and compare their effectiveness. Below are more details about these baselines and Clippy. • Gradient Clipping (GC) [24]: We used layer-wise (local) gradient clipping with <mark>clipping threshold</mark> searched from GC ∈ {10 −1 , 10 −2 , 10 −3 }.<br>",
    "Arabic": "عتبة القطع",
    "Chinese": "限幅阈值",
    "French": "seuil d'écrêtage",
    "Japanese": "切り捨て閾値",
    "Russian": "порог отсечения"
  },
  {
    "English": "clique potential",
    "context": "1: score(x, y) Z(θ) can be computed exactly under certain assumptions about the <mark>clique potentials</mark>, but can in all cases be bounded bŷ \n Z(θ) = K =1Ẑ (θ) = K =1 x:|x|= score(x, y) \n Where K is a suitably chosen large constant.<br>",
    "Arabic": "إمكانات العصبة",
    "Chinese": "团势能",
    "French": "potentiels de clique",
    "Japanese": "クリークポテンシャル",
    "Russian": "потенциал клики"
  },
  {
    "English": "close frequent itemset",
    "context": "1: Note again that the most representative author/co-authors are not necessarily the most well-known ones, but rather the authors who are most strongly correlated to the topics (terms). In both experiments, we use the tools FP-Close [7] and CloSpan [23] to generate <mark>closed frequent itemsets</mark> of coauthors and closed sequential patterns of title terms respectively.<br>2: Since the number of <mark>closed frequent itemsets</mark> is usually less than that of frequent itemsets, summarizing closed itemsets can significantly reduce the number of patterns involved in clustering, thus improving efficiency.<br>",
    "Arabic": "مجموعة بنود متكررة مغلقة",
    "Chinese": "闭合频繁项集",
    "French": "ensemble d'items fréquents clos",
    "Japanese": "閉じた頻出アイテムセット",
    "Russian": "\"закрытый частый набор элементов\""
  },
  {
    "English": "close-book model",
    "context": "1: In our analysis, we train a BART-large <mark>closed-book model</mark>, which is trained with questions as input and generates (q, a) pairs as output. Checkpoints are selected by Exact Match score on a development set. We also include a more powerful T5-11B model from (Roberts et al., 2020).<br>",
    "Arabic": "نموذج مغلق",
    "Chinese": "闭卷模型",
    "French": "modèle à livre fermé",
    "Japanese": "クローズドブックモデル",
    "Russian": "модель закрытой книги"
  },
  {
    "English": "close-world",
    "context": "1: Some others train \"ground-up\" models for both <mark>closed-world</mark> K-way classification and open-set discrimination by synthesizing fake open data during training, oftentimes sacrificing the classification accuracy on the closed-set [21,42,64,59].<br>2: As a result, many approaches first train a <mark>closed-world</mark> K-way classification network on the closed-set [30,6] and then exploit the trained network for open-set discrimination [54,35,44].<br>",
    "Arabic": "عالم مغلق",
    "Chinese": "封闭世界",
    "French": "monde fermé",
    "Japanese": "閉鎖世界",
    "Russian": "закрытый мир"
  },
  {
    "English": "cloze prompt",
    "context": "1: via methods such as <mark>cloze prompts</mark> (Beloucif and Biemann, 2021;Petroni et al., 2020) and linear classifiers (Hewitt and Liang, 2019;Pimentel et al., 2020). Although having explored extensive knowledge within PLMs, previous knowledge probing works have not studied ontological knowledge systematically.<br>",
    "Arabic": "تلميح اكتمال",
    "Chinese": "填空提示",
    "French": "prompt à trous",
    "Japanese": "クローズプロンプト (cloze prompt)",
    "Russian": "заполнение пропусков"
  },
  {
    "English": "cloze task",
    "context": "1: Within the <mark>cloze task</mark>, we report accuracy, i.e., did we guess the missing vowel right? We consider three versions of the <mark>cloze task</mark>s. First, we predict one missing vowel in a setting where exactly one vowel was deleted. Second, we predict up to one missing vowel where a vowel may have been deleted.<br>2: Although MPNet and RoBERTa share the same pre-training corpus and the Transformer architecture, the addition of the permuting language objective to the <mark>cloze task</mark> gives a slight advantage to MPNet. We use BERT for the rest of the experiments.<br>",
    "Arabic": "مهمة الإكمال",
    "Chinese": "填空任务",
    "French": "tâche de complétion",
    "Japanese": "\"穴埋め課題 (cloze task)\"",
    "Russian": "задание на заполнение пропусков"
  },
  {
    "English": "cluster algorithm",
    "context": "1: The average degree of URL nodes is 1.8. The curves in Figure 5(b) and (c) illustrate why the <mark>clustering algorithm</mark> in Section 4.2 is efficient: since the average degrees of query and URL nodes are both low, the average number of clusters to be checked for each query is small.<br>2: In distance-based approaches, an existing <mark>clustering algorithm</mark> that uses a particular clustering distortion measure is employed; however, it is trained to satisfy the labels or constraints in the supervised data.<br>",
    "Arabic": "خوارزمية التجميع",
    "Chinese": "聚类算法",
    "French": "algorithme de regroupement",
    "Japanese": "クラスタリングアルゴリズム",
    "Russian": "алгоритм кластеризации"
  },
  {
    "English": "cluster assignment",
    "context": "1: Using k-means (Arthur and Vassilvitskii, 2007), CluCL then clusters the resulting embeddings into k clusters, yielding a <mark>cluster assignment</mark> k i for each name (and corresponding data point). Next, for each class c ∈ C, CluCL computes the following average pairwise difference between clusters: \n<br>",
    "Arabic": "تعيين العنقود",
    "Chinese": "聚类分配",
    "French": "assignation de cluster",
    "Japanese": "クラスター割り当て",
    "Russian": "присвоение кластера"
  },
  {
    "English": "cluster center",
    "context": "1: Step 3. for t = 1 to t θ do /*t θ is a timeout threshold, a user query session S*/ for each query vector if the number of data points that belong to any <mark>cluster center</mark> m i,j >2.5.<br>2: T * , find the cluster that minimizes the distance from x * t to the <mark>cluster center</mark>: \n k t (x * t ) = arg min k ρ(x k , x * t ). (8 \n ) \n<br>",
    "Arabic": "مركز العنقود",
    "Chinese": "簇中心",
    "French": "centre de grappe",
    "Japanese": "クラスタ中心",
    "Russian": "центр кластера"
  },
  {
    "English": "cluster centroid",
    "context": "1: M-step(A): Given cluster labels {l (t+1) i } N i=1 , re-calculate <mark>cluster centroids</mark> {µ µ µ (t+1) h } K \n h=1 to minimize J obj . 2c. M-step(B): Re-estimate distance measure D to reduce J obj . 2d. t ← t+1<br>2: A second important feature of the SVD2 algorithm is the unit-length normalization of the latent descriptors, along with the computation of <mark>cluster centroids</mark> as the weighted averages of their constituent vectors. Thanks to this combined device, rare words are treated equally to frequent words regarding the length of their descriptor vectors, yet contribute less to the placement of centroids.<br>",
    "Arabic": "مركز التجمع",
    "Chinese": "簇中心",
    "French": "centroïde du cluster",
    "Japanese": "クラスター中心",
    "Russian": "кластерный центроид"
  },
  {
    "English": "cluster criterion",
    "context": "1: This observation opens Pandora's box on <mark>clustering criteria</mark>: the \"meaning\" of a clustering criterion does not only depend on the exact definition of the criterion itself, but also on how the graph on the finite sample is constructed.<br>2: Moerover, yet a different limit result for a complete graph using Gaussian weights exists in the literature (Narayanan et al., 2007). The fact that all these different graphs lead to different <mark>clustering criteria</mark> shows that these criteria cannot be studied isolated from the graph they will be applied to.<br>",
    "Arabic": "معيار التجميع",
    "Chinese": "聚类准则",
    "French": "critère de clustering",
    "Japanese": "クラスター基準",
    "Russian": "критерий кластеризации"
  },
  {
    "English": "cluster feature",
    "context": "1: Turian et al. , 2010 ; Faruqui and Padó , 2010 ) . Intuitively, the reason for the effectiveness of <mark>cluster features</mark> lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues.<br>2: However, if we add back just the <mark>cluster features</mark>, the accuracy jumps back up to 89.5%, which is only 1.2% below the full system. Thus, if we can accurately learn cross-lingual clusters, there is hope of regaining some of the accuracy lost due to the delexicalization process.<br>",
    "Arabic": "ميزة العنقود",
    "Chinese": "簇特征",
    "French": "caractéristiques de cluster",
    "Japanese": "クラスタ特徴",
    "Russian": "кластерные признаки"
  },
  {
    "English": "cluster label",
    "context": "1: So, the probability distribution of the value of l i for the data point x i depends only on the <mark>cluster labels</mark> of the points that are must-linked or cannot-linked to x i . Let us consider a particular cluster label configuration L to be the joint event L = {l i } N i=1 .<br>2: Note that the agglomeration procedure (Step 2) is not required for this application. Sample clustering solutions are evaluated by computing the accuracy of the <mark>cluster labels</mark> with respect to the true class labels as defined in . Figure 4 displays the sample cluster accuracy of ROCC at different fractions of genes clustered.<br>",
    "Arabic": "تصنيف العنقود",
    "Chinese": "簇标签",
    "French": "étiquette de cluster",
    "Japanese": "クラスターラベル",
    "Russian": "метка кластера"
  },
  {
    "English": "cluster method",
    "context": "1: As a preliminary attempt to apply our algorithms in the real world, we show that the proposed <mark>clustering method</mark> (which is one of the most crucial step in our overall approach), without any modification, works reasonably well even for this dataset. To be concrete, we apply Algorithm 3 (without dimensionality reduction, i.e.<br>2: Note that we did not tune hyper-parameters of the supervised learning methods and of the <mark>clustering method</mark>, such as the number of clusters (Turian et al., 2010;Faruqui and Padó, 2010), and that we did not apply any heuristic for data cleaning such as that used by Turian et al. (2010).<br>",
    "Arabic": "طريقة التجميع",
    "Chinese": "聚类方法",
    "French": "méthode de regroupement",
    "Japanese": "クラスタリング手法",
    "Russian": "метод кластеризации"
  },
  {
    "English": "cluster problem",
    "context": "1: We state the <mark>clustering problem</mark> in the following way. For a set V of n nodes, a clustering result is a partition {V1, . . .<br>",
    "Arabic": "مشكلة التجميع",
    "Chinese": "聚类问题",
    "French": "problème de clustering",
    "Japanese": "クラスター問題",
    "Russian": "кластерная проблема"
  },
  {
    "English": "cluster size",
    "context": "1: Finally, we propose a re-sampling strategy to balance the size of different clusters as well as control the total memory of bank B, by setting a threshold <mark>cluster size</mark> S thr (line 12 in Alg. 1).<br>",
    "Arabic": "حجم العنقود",
    "Chinese": "簇大小",
    "French": "Taille du cluster",
    "Japanese": "クラスターサイズ",
    "Russian": "размер кластера"
  },
  {
    "English": "co-occurrence",
    "context": "1: and Ngai , 2006 ) . Other work has shown that <mark>co-occurrence</mark> of words (Lapata, 2003;Soricut and Marcu, 2006) and discourse relations (Pitler and Nenkova, 2008;Lin et al., 2011) also predict coherence.<br>2: These <mark>co-occurrence</mark> statistics suppress the appearance of small unexpected classes in the labelling. Top left: a mistaken hypothesis of a cow is suppressed Top right: Many small classes are suppressed in the image of a building. Note that the use of <mark>co-occurrence</mark> typically changes labels, but does not alter silhouettes. to the set of object classes.<br>",
    "Arabic": "\"تواجد مشترك\"",
    "Chinese": "共现",
    "French": "co-occurrence",
    "Japanese": "共起",
    "Russian": "совместная встречаемость"
  },
  {
    "English": "co-occurrence matrix",
    "context": "1: More concretely, these models work by factorizing some <mark>co-occurrence matrix</mark> using singular value decomposition, so given the <mark>co-occurrence matrix</mark> M = U SV T , the word vectors will correspond to the first n dimensions of W = U S α , where the parameter α plays a similar role as in our method.<br>2: We construct a M × N <mark>co-occurrence matrix</mark> between the items and the documents, where M is the vocabulary size and N is the total number of the documents. We applied singular value decomposition (SVD) to the matrix and compressed its rank to k. Here, k corresponds to N − 2.<br>",
    "Arabic": "مصفوفة التواجد المشترك",
    "Chinese": "共现矩阵",
    "French": "matrice de co-occurrence",
    "Japanese": "共起行列",
    "Russian": "матрица совместного появления"
  },
  {
    "English": "co-occurrence statistic",
    "context": "1: The importance of <mark>co-occurrence statistics</mark> has been well established [31,22,6]. In this work we have examined the use of <mark>co-occurrence statistics</mark> and how they might be incorporated into a global energy or likelihood model such as a conditional random field.<br>2: This means that a closedclass word (such as a preposition) is never used to generate an open-class word. In addition to <mark>co-occurrence statistics</mark>, the parsed Flickr data adds to our understanding of the basic characteristics of visually descriptive text.<br>",
    "Arabic": "إحصائيات التواجد المشترك",
    "Chinese": "共现统计",
    "French": "statistique de cooccurrence",
    "Japanese": "共起統計",
    "Russian": "статистика совместной встречаемости"
  },
  {
    "English": "co-reference",
    "context": "1: Hence, we believe that an important next step is the identification of features indicating whether sentences are on-topic (which is a kind of <mark>co-reference</mark> problem); we look forward to addressing this challenge in future work.<br>2: Both systems have low recall for noun-mediated relations, with most of LUND's recall requiring <mark>co-reference</mark>. We observe that a union of the two systems raises recall to 0.71 for verb-mediated relations and 0.83 with coreference, demonstrating that each system is identifying argument pairs that the other missed.<br>",
    "Arabic": "المرجعية المشتركة",
    "Chinese": "共指",
    "French": "coréférence",
    "Japanese": "共同参照",
    "Russian": "сореферентность"
  },
  {
    "English": "co-training",
    "context": "1: Some specific domains, such as mathematical reasoning, planning, or puzzle solving, may even more obviously necessitate abstractions [49]. It will be exciting future work to use our approach to empirically validate this conjecture by determining the space of tasks for which <mark>co-training</mark> with language and program abstractions improves an artificial neural agent's performance.<br>2: In MTL, the <mark>co-training</mark> strategy across tasks could leverage feature abstraction; it could effortlessly extend to additional tasks, and save computation cost for onboard chips. However, such a scheme may cause undesirable \"negative transfer\" [23,64].<br>",
    "Arabic": "التدريب المشترك",
    "Chinese": "协同训练",
    "French": "co-entraînement",
    "Japanese": "共同トレーニング",
    "Russian": "сопутствующее обучение"
  },
  {
    "English": "coarse correlated equilibria",
    "context": "1: In particular, regret minimization by the SQL dynamics at an optimal O(1/T ) rate implies that their time-average converges fast to <mark>coarse correlated equilibria</mark> (CCE). These are CCE of the perturbed game, Γ H , but if exploration parameter is low, they are approximate CCE of the original game as well.<br>2: In Figure 1 we plot the maximum individual regret as well as the sum of the regrets under the two algorithms, using η = 0.1 for both methods. Thus convergence to the set of <mark>coarse correlated equilibria</mark> is substantially faster under Optimistic Hedge, confirming our results in Section 3.2.<br>",
    "Arabic": "التوازنات المترابطة الخشنة",
    "Chinese": "粗相关平衡",
    "French": "équilibres corrélés grossiers",
    "Japanese": "粗い相関均衡",
    "Russian": "коррелированные равновесия с грубой аппроксимацией"
  },
  {
    "English": "coarse correlated equilibrium",
    "context": "1: 2 It is proven that if κ t is set to 1 √ t then as t → ∞ the average policy over all iterations converges to a <mark>coarse correlated equilibrium</mark>, though in practice it often comes close to a Nash equilibrium as well.<br>",
    "Arabic": "التوازن المترابط الخشن",
    "Chinese": "粗粒度相关均衡",
    "French": "équilibre corrélé grossier",
    "Japanese": "粗い相関均衡",
    "Russian": "грубое скоррелированное равновесие"
  },
  {
    "English": "coarse layer",
    "context": "1: We then define a novel architecture that combines semantic information from a deep, <mark>coarse layer</mark> with appearance information from a shallow, fine layer to produce accurate and detailed segmentations.<br>",
    "Arabic": "طبقة خشنة",
    "Chinese": "粗糙层",
    "French": "couche grossière",
    "Japanese": "粗層",
    "Russian": "грубый слой"
  },
  {
    "English": "coarse-to-fine",
    "context": "1: Multi-Scale training Our per-pixel formulation naturally allows us to train in a <mark>coarse-to-fine</mark> setting, where we first train the model on downsampled images in a first stage, and then increase the resolution of images in stages.<br>2: Thus, it is insufficient to use the template to capture the subspace of a complex deformation field. LK and FF are stuck in local maxima despite their <mark>coarse-to-fine</mark> implementations. Our approach obtains the best performance. Fig. 7 shows the progression of our algorithm.<br>",
    "Arabic": "خشن إلى دقيق",
    "Chinese": "由粗到细",
    "French": "grossier-à-fin",
    "Japanese": "粗から細へ",
    "Russian": "от грубого к детальному"
  },
  {
    "English": "coarse-to-fine approach",
    "context": "1: We thus propose a multi-pass <mark>coarse-to-fine approach</mark> where the initial pass is a linear-time sweep, which tries to resolve local ambiguities, but leaves arcs beyond a fixed length b unspecified (Section 3).<br>",
    "Arabic": "نهج الخشنة إلى الناعمة",
    "Chinese": "粗到细的方法",
    "French": "approche grossière à fine",
    "Japanese": "粗いものから細かいものへのアプローチ",
    "Russian": "Подход от грубого к тонкому"
  },
  {
    "English": "coarse-to-fine cascade",
    "context": "1: The index sets of higherorder models can be constructed out of the index sets of lower-order models, thus forming a hierarchy that we will exploit in our <mark>coarse-to-fine cascade</mark>.<br>2: The remaining first-order indices are then given by: \n {(h, m) ∈ I 1 : p 1→0 (h, m) ⊂ F (I 0 )} \n Figure 2 depicts a <mark>coarse-to-fine cascade</mark>, incorporating vine and first-order pruning passes and finishing with a higher-order parse model.<br>",
    "Arabic": "تسلسل خشن إلى دقيق",
    "Chinese": "由粗到细级联",
    "French": "cascade grossière à fine",
    "Japanese": "\"粗から細へのカスケード\"",
    "Russian": "каскад от грубого к тонкому"
  },
  {
    "English": "coarse-to-fine strategy",
    "context": "1: It is robust to illumination changes through the use of statistical similarity criteria. Moreover, it can recover large displacements thanks to a multi-resolution <mark>coarse-to-fine strategy</mark>. In the sequel, we focus on the case of two cameras not to overload notations.<br>2: As a result, they only work well for small displacements. To handle large displacements, the <mark>coarse-to-fine strategy</mark> is used, where an image pyramid is used to estimate large displacements at low resolution, then small displacements refined at high resolution. But this <mark>coarse-to-fine strategy</mark> may miss small fast-moving objects and have difficulty recovering from early mistakes.<br>",
    "Arabic": "استراتيجية من الخشن إلى الدقيق",
    "Chinese": "粗到精策略",
    "French": "stratégie de grossier à fin",
    "Japanese": "粗密戦略",
    "Russian": "стратегия от грубого к мелкому"
  },
  {
    "English": "codebook",
    "context": "1: The resulting descriptors are vector quantized using a K-entry <mark>codebook</mark> of visual word prototypes. As result, we obtain keypoint locations x i j with discrete cluster indices c i j ∈ {1, . . . , K}. We represent images or regions within images by their cluster histograms, i.e.<br>2: After encoding we compute the cosine similarity between the test code z test ∈ R 128 and all codes z i ∈ R 128 from the <mark>codebook</mark>: \n cos i = z z z i z z z test z z z i z z z test (4) \n<br>",
    "Arabic": "كتاب الشفرات",
    "Chinese": "码本",
    "French": "codebook",
    "Japanese": "コードブック",
    "Russian": "кодовая книга"
  },
  {
    "English": "codomain",
    "context": "1: The joint camera map Φ p,l,I,m is dominant if and only if the projection π Y ∶ Inc → Y p,l,I,m onto the last factor is dominant (since this is the projection from the graph of Φ p,l,I,m on its <mark>codomain</mark>).<br>",
    "Arabic": "مدى الشيفرة",
    "Chinese": "值域",
    "French": "codomaine",
    "Japanese": "対値域",
    "Russian": "областьзначений"
  },
  {
    "English": "coefficient matrix",
    "context": "1: 1 We write the inequalities of an operator-counting constraint C as coeffs(C) Count ≥ bounds(C) for a <mark>coefficient matrix</mark> coeffs(C), operator-counting variable vector Count and a bounds vector bounds(C).<br>2: (5.2) Thus, to prove Proposition 5.1, we only have to prove that the <mark>coefficient matrix</mark> I − cP ⊤ ⊗ P ⊤ is non-singular. Since P ⊤ ⊗ P ⊤ is a (left) stochastic matrix, its spectral radius is equal to one.<br>",
    "Arabic": "مصفوفة المعاملات",
    "Chinese": "系数矩阵",
    "French": "matrice de coefficients",
    "Japanese": "係数行列",
    "Russian": "матрица коэффициентов"
  },
  {
    "English": "cognitive model",
    "context": "1: These monitoring foci are determined from the <mark>cognitive model</mark> by automatically deriving representation relations for cognitive states in the form of temporal specifications. From these temporal expressions the events are derived that are to be monitored, and from the monitoring information on these events the representation expressions are verified automatically.<br>2: Representation relations are derived from the <mark>cognitive model</mark> representation as shown in Section 4.2 and usually have the form of more complex temporal expressions over externally observable states. To detect occurrence of an internal state, the corresponding representational content should be monitored constantly, which is considered in Section 4.3.<br>",
    "Arabic": "النموذج الإدراكي",
    "Chinese": "认知模型",
    "French": "modèle cognitif",
    "Japanese": "認知モデル",
    "Russian": "когнитивная модель"
  },
  {
    "English": "cognitive science",
    "context": "1: This reflects the overlap in properties between concepts, and is prevalent in studies utilizing conceptual similarity in <mark>cognitive science</mark> (Tversky, 1977;Sloman, 1993, etc. ).<br>",
    "Arabic": "العلوم المعرفية",
    "Chinese": "认知科学",
    "French": "sciences cognitives",
    "Japanese": "認知科学",
    "Russian": "когнитивная наука"
  },
  {
    "English": "cold start",
    "context": "1: Our negative results hold even when accounting for common active learning ailments: <mark>cold start</mark>s, correlated sampling, and uncalibrated uncertainty. We mitigate the <mark>cold start</mark> challenge of needing a representative initial dataset by varying the size of the seed set in our experiments.<br>2: Our findings are useful to a wide range of practitioners: large shops launching in new languages/countries, mid-and-small shops transitioning to dense IR architectures and the raising wave of multi-tenant players 4 : as A.I. providers grow by deploying their solutions on multiple shops, \"<mark>cold start</mark>\" scenarios are an important challenge to the viability of their business model.<br>",
    "Arabic": "بداية باردة",
    "Chinese": "冷启动",
    "French": "démarrage à froid",
    "Japanese": "コールドスタート",
    "Russian": "холодный старт"
  },
  {
    "English": "collaborative filtering",
    "context": "1: They are referred to as time-aware or temporal <mark>collaborative filtering</mark> (TCF) methods [6], [7], [8].<br>2: This framework enables us to learn new syntactically guided embeddings while also leveraging separately estimated word vectors as starting features, leading to improved parsing performance. Dimensionality Reduction Many machine learning problems can be cast as matrix problems where the matrix represents a set of co-varying parameters. Such problems include, for example, multi-task learning and <mark>collaborative filtering</mark>.<br>",
    "Arabic": "ترشيح تعاوني",
    "Chinese": "协同过滤",
    "French": "filtrage collaboratif",
    "Japanese": "協調フィルタリング",
    "Russian": "коллаборативная фильтрация"
  },
  {
    "English": "collaborative learning",
    "context": "1: However, we can reduce any <mark>collaborative learning</mark> problem to multidistribution learning with smooth convex loss functions-as long as we allow for improper or randomized solutions to our <mark>collaborative learning</mark> problem. Allowing for improper or randomized solutions is not unreasonable and is in fact necessary to achieve non-trivial sample complexities in <mark>collaborative learning</mark> [9].<br>2: In comparison, multi-distribution learning paradigms such as <mark>collaborative learning</mark> [9], agnostic federated learning [39], and Group DRO [50] learn models that perform well across any one of the data sources. Subsequent work. Haghtalab et al.<br>",
    "Arabic": "التعلم التعاوني",
    "Chinese": "协作学习",
    "French": "apprentissage collaboratif",
    "Japanese": "協同学習",
    "Russian": "коллективное обучение"
  },
  {
    "English": "collective inference",
    "context": "1: For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on Lagrangian relaxation for <mark>collective inference</mark>.<br>",
    "Arabic": "الاستدلال الجماعي",
    "Chinese": "集体推理",
    "French": "inférence collective",
    "Japanese": "集合的推論",
    "Russian": "коллективный вывод"
  },
  {
    "English": "color channel",
    "context": "1: Proof: The matting cost (5) measures the deviation between a matte and a linear function of the <mark>color channels</mark>, over all local windows (eq. 4).<br>",
    "Arabic": "قناة اللون",
    "Chinese": "颜色通道",
    "French": "canal de couleur",
    "Japanese": "カラーチャンネル",
    "Russian": "цветовой канал"
  },
  {
    "English": "color constancy",
    "context": "1: Consequently, the ideal self-supervision loss is susceptible to be confused by these common disturbances in color, leading to ambiguous supervision in challenging scenarios, namely <mark>color constancy</mark> ambiguity.<br>2: Even though this assumption is made by many <mark>color constancy</mark> algorithms, it remains controversial. It is worth studying the effectiveness of the proposed method without making the narrowband assumption. Overall, this paper presented ideas pioneering a new direction of research in computer vision and image processing.<br>",
    "Arabic": "ثبات اللون",
    "Chinese": "颜色恒常性",
    "French": "constance de la couleur",
    "Japanese": "色恒常性",
    "Russian": "постоянство цвета"
  },
  {
    "English": "colorization",
    "context": "1: Colorization is the problem of introducing color to a grayscale image with a small amount of user input or outside information, for the purpose of improving blackand-white films or photographs. Levin et al. [25] presented an effective technique for this task by formulating and solving a specialized optimization problem.<br>2: RTFs have shown excellent results in image restoration applications, such as image denoising, inpainting, and <mark>colorization</mark>. In general, RTFs take the form of a Gaussian CRF in which a non-linear regressor is used to specify the local model parameters.<br>",
    "Arabic": "تلوين الصور",
    "Chinese": "上色",
    "French": "colorisation",
    "Japanese": "カラー化",
    "Russian": "колоризация"
  },
  {
    "English": "column space",
    "context": "1: (2019c), every such subspace H is spanned by the <mark>column space</mark> of a matrix X ∈ R d×j whose columns are orthonormal, i.e., X T X = I j .<br>",
    "Arabic": "فضاء الأعمدة",
    "Chinese": "列空间",
    "French": "espace colonne",
    "Japanese": "列空間",
    "Russian": "пространство столбцов"
  },
  {
    "English": "column vector",
    "context": "1: We note that for the remainder of this section, we transpose C to be a <mark>column vector</mark> of shape C N or C N ×1 instead of matrix or row vector C 1×N as in (1). In other words the SSM is \n<br>2: <mark>column vector</mark> of m ones : \n W = V 1 m , H = 1 T n V 1 T n V 1 m . (2) \n<br>",
    "Arabic": "متجه العمود",
    "Chinese": "列向量",
    "French": "vecteur colonne",
    "Japanese": "列ベクトル",
    "Russian": "столбцовый вектор"
  },
  {
    "English": "combinator",
    "context": "1: A sequence of supertags is likewise not the same as a tree, and mapping it to a tree requires further processing in the form of finding and applying an appropriate series of <mark>combinators</mark>.<br>2: 1 The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new <mark>combinators</mark> and new forms of candidate lexical entries.<br>",
    "Arabic": "مُرَكِّب",
    "Chinese": "组合子",
    "French": "combinateur",
    "Japanese": "組み合わせ子 (kumiawaseko)",
    "Russian": "комбинатор"
  },
  {
    "English": "combinatorial explosion",
    "context": "1: The core idea of our generation-based model is to gradually expand a subprogram (i.e., a partial query) into the finalized target program, instead of enumerating all possible finalized programs from the KB directly, which suffers from <mark>combinatorial explosion</mark>.<br>2: As there is a <mark>combinatorial explosion</mark> in the number of feasible higher-order transfers (|T | × |S| k for k th order), we employ a sampling procedure with the goal of filtering out higher-order transfers that are less likely to yield good results, without training them.<br>",
    "Arabic": "انفجار تركيبي",
    "Chinese": "组合爆炸",
    "French": "explosion combinatoire",
    "Japanese": "組み合わせ爆発",
    "Russian": "комбинаторный взрыв"
  },
  {
    "English": "combinatorial optimization",
    "context": "1: From our experience, the second point of view is much more convenient for vision applications. Submodular functions have received significant attention in the <mark>combinatorial optimization</mark> literature. A remarkable fact about submodular functions is that they can be minimized in polynomial time [19], [23], [38].<br>2: They showed that such an algorithm generally finds \"good\" solutions for <mark>combinatorial optimization</mark> and that \"good\" solutions are statistically rare in the overall solution space (i.e., extremely low probability of drawing a \"good\" solution at random).<br>",
    "Arabic": "تحسين تركيبي",
    "Chinese": "组合优化",
    "French": "optimisation combinatoire",
    "Japanese": "組合せ最適化",
    "Russian": "комбинаторная оптимизация"
  },
  {
    "English": "combinatorial optimization problem",
    "context": "1: For modules that accept a vector parameter, we associate these parameters with words rather than semantic tokens, and thus turn the <mark>combinatorial optimization problem</mark> associated with lexicon induction into a continuous one.<br>2: It is a <mark>combinatorial optimization problem</mark>. Therefore, it is impossible to obtain exact solutions in polynomial time as the required computation grows exponentially with the size of the problem.<br>",
    "Arabic": "مشكلة تحسين تكاملية توافقية",
    "Chinese": "组合优化问题",
    "French": "problème d'optimisation combinatoire",
    "Japanese": "\"組み合わせ最適化問題\"",
    "Russian": "задача комбинаторной оптимизации"
  },
  {
    "English": "combinatory categorial grammar",
    "context": "1: <mark>Combinatory Categorial Grammar</mark> CCG is a categorial formalism that provides a transparent interface between syntax and semantics (Steedman, 1996(Steedman, , 2000. Section 7 details our instantiation of CCG. In CCG trees, each node is a category. Figure 2 shows a simple CCG tree. For example, S\\N P \n<br>2: Minimizing this loss encourages the model to return the correct parse as quickly as possible. The combination of global representations and optimal decoding enables our parser to achieve state-of-the-art accuracy for <mark>Combinatory Categorial Grammar</mark> (CCG) parsing. Despite being intractable in the worst case, the parser in practice is highly efficient.<br>",
    "Arabic": "نحو الفئات التركيبية",
    "Chinese": "组合范畴语法",
    "French": "grammaire catégorielle combinatoire",
    "Japanese": "組み合わせ範疇文法",
    "Russian": "Комбинаторная категориальная грамматика"
  },
  {
    "English": "commonsense inference",
    "context": "1: The architecture scales with training data and model size, facilitates efficient parallel training, and captures long-range sequence features. Model pretraining (McCann et al., 2017;Howard and Ruder, 2018;Devlin et al., 2018) allows models to be trained on generic corpora and subsequently be easily adapted to specific tasks with strong performance. The Transformer architecture is particularly conducive to pretraining on large text corpora , leading to major gains in accuracy on downstream tasks including text classification , language understanding ( Liu et al. , 2019b ; Wang et al. , , 2019 , machine translation ( Lample and Conneau , 2019a ) , coreference resolution ( Joshi et al. , 2019 ) , <mark>commonsense inference</mark><br>",
    "Arabic": "الاستدلال المنطقي",
    "Chinese": "常识推理",
    "French": "inférence de sens commun",
    "Japanese": "常識推論",
    "Russian": "вывод здравого смысла"
  },
  {
    "English": "commonsense knowledge",
    "context": "1: CommonsenseQA (Talmor et al., 2018) asks crowd workers to create questions from ConceptNet (Speer et al., 2017), which is a large graph of <mark>commonsense knowledge</mark>, where each question discriminates its answer candidates between three target concepts that all share the same relationship to a single source drawn from ConceptNet.<br>2: In open-domain conversational systems, <mark>commonsense knowledge</mark> is important for establishing effective interactions, since socially shared <mark>commonsense knowledge</mark> is the set of background information people intended to know and use during conversation [Minsky, 1991;Marková et al., 2007;Speer and Havasi, 2012;Souto, 2015].<br>",
    "Arabic": "المعرفة السليمة الشائعة",
    "Chinese": "常识知识",
    "French": "connaissances de sens commun",
    "Japanese": "常識知識",
    "Russian": "здравый смысл"
  },
  {
    "English": "commonsense knowledge graph",
    "context": "1: goals and plans , experiences , and relationships . Using this knowledge frame, we construct a large-scale graph of persona commonsense knowledge by extracting and generating persona knowledge from both existing hand-crafted commonsense KGs and large-scale pretrained language models (LMs).<br>2: ATOMIC is a large-scale human-annotated <mark>commonsense knowledge graph</mark> focusing on the inferential knowledge in social life . It consists of nine if-then relation types describing the causes, effects, agent, stative, and theme of an event. The research on ATOMIC has drawn more and more attention in recent years. An increasing number of downstream tasks , including * * Corresponding author commonsense reasoning ( Yu et al. , 2022 ) , storytelling ( Brahman and Chaturvedi , 2020 ) , question answering ( Heo et al. , 2022 ) , dialog generation ( Wu et al. , 2022 ) , etc. , have improved their performances by acquiring and utilizing the commonsense knowledge<br>",
    "Arabic": "رسم بياني للمعرفة السليمة الشائعة",
    "Chinese": "常识知识图谱",
    "French": "graphe de connaissances de sens commun",
    "Japanese": "常識知識グラフ",
    "Russian": "граф знаний здравого смысла"
  },
  {
    "English": "commonsense reasoning",
    "context": "1: tasks ? In this work , we propose MULTIINSTRUCT , the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories , including Visual Question Answering ( Goyal et al. , 2017 ; Suhr et al. , 2017 ) , <mark>Commonsense Reasoning</mark> ( Zellers et al. , 2019 ; Xie et al. , 2019 ) , Visual Relationship Understanding ( Krishna<br>2: These tasks are meant to teach machine learning models to perform various tasks such as object recognition, visual relationship understanding, text-image grounding, and so on by following instructions so that they can perform zero-shot prediction on unseen tasks. To build MULTIINSTRUCT , we first collect 34 tasks from the existing studies in visual and multimodal learning , covering Visual Question Answering ( Goyal et al. , 2017 ; Krishna et al. , 2017 ; Zhu et al. , 2016 ; Hudson and Manning , 2019 ; Singh et al. , 2019 ; Marino et al. , 2019 ) , <mark>Commonsense Reasoning</mark> ( Suhr et al. , 2017 ; Liu et al. , 2022a ; Zellers et al. , 2019 ; Xie et al. , 2019 ) , Region Understanding ( Krishna et al. , 2017 ) , Image Understanding ( Kafle and Kanan , 2017 ; Chiu et al. , 2020 ) , Grounded Generation ( Krishna et al. , 2017 ; Yu et al. , 2016 ; Lin et al. , 2014 ) , Image-Text Matching ( Lin et al. , 2014 ; Goyal et al. , 2017 ) , Grounded Matching ( Krishna et al. , 2017 ; Veit et al. , 2016 ; Yu et al. , 2016 ) , Visual Relationship ( Krishna et al. , 2017 ; Pham et al. , 2021 ) , Temporal<br>",
    "Arabic": "\"استدلال المنطق العام\"",
    "Chinese": "常识推理",
    "French": "raisonnement de sens commun",
    "Japanese": "常識推論",
    "Russian": "рассуждение здравого смысла"
  },
  {
    "English": "communication complexity",
    "context": "1: In this model, they obtain upper and lower bounds on the achievable distortion under <mark>communication complexity</mark> constraints. Pierczyński and Skowron [28] consider the distortion (and a modified notion of distortion) for approval-based voting (which has reduced communication), in which voters approve all candidates within a certain distance of themselves.<br>2: T (A 2 , f, O, G) ≤ O   ∆Lσ 2 nB 4 + ∆L log n + ς0n √ ∆L 2 √ 1 − λ   . Comparing Theorem 1 and Theorem 3, DeTAG achieves the optimal complexity with only a logarithm gap. Improvement on complexity. Revisiting Table 2 , we can see the main improvement of DeTAG 's complexity is in the two terms on <mark>communication complexity</mark> : ( 1 ) DeTAG only depends on the outer variance term ς 0 inside a log , and ( 2 ) It reduces the dependency on the spectral gap 1 − λ to the lower bound of square root , as<br>",
    "Arabic": "تعقيد الاتصال",
    "Chinese": "通信复杂度",
    "French": "complexité de la communication",
    "Japanese": "通信複雑性",
    "Russian": "коммуникационная сложность"
  },
  {
    "English": "communication graph",
    "context": "1: If γ > 0, then f γ is Lg γ -smooth and, for all θ ∈ R d , f (θ) ≤ f γ (θ) ≤ f (θ) + γL g √ d . (11) \n Algorithm 1 distributed randomized smoothing \n Input : approximation error ε > 0 , <mark>communication graph</mark> G , α 0 = 1 , α t+1 = 2/ ( 1 + 1 + 4/α 2 t ) T = 20RLg d 1/4 ε , K = 5RLg d −1/4 ε , γ t = Rd −1/4 α t , η t = Rαt 2Lg ( d 1/4 + √ t+1 K<br>2: As a useful working example, consider a \"<mark>communication graph</mark>,\" in which nodes are e-mail addresses, and there is a directed edge (u, v) if u has sent at least a certain number of e-mail messages or instant messages to v, or if v is included in u's address book.<br>",
    "Arabic": "رسم الاتصال",
    "Chinese": "通信图",
    "French": "graphe de communication",
    "Japanese": "通信グラフ",
    "Russian": "граф связи"
  },
  {
    "English": "compatibility function",
    "context": "1: This meaning of a linguistic label t is defined by a <mark>compatibility function</mark> c t : U → [0, 1] which assigns its compatibility with U to every t. \n<br>",
    "Arabic": "وظيفة التوافق",
    "Chinese": "兼容性函数",
    "French": "fonction de compatibilité",
    "Japanese": "互換性関数",
    "Russian": "функция совместимости"
  },
  {
    "English": "compatibility graph",
    "context": "1: The results are shown in Table 6. Compared to RANSAC, which randomly selects correspondences and generates hypotheses from the correspondence set without geometric constraints, MAC effectively generates more convincing hypotheses from maximal cliques in the <mark>compatibility graph</mark>, which fully exploits the consensus information in the graph. The performance upper bound of MAC.<br>2: Besides, the registration recall obtained by using maximal cliques is 8.03% higher than using the maximum cliques when combined with FPFH and 10.45% higher when combined with FCGF on 3DLoMatch. There are several reasons for this : 1 ) maximal cliques include the maximum cliques and additionally consider local graph constraints , so the search for maximal cliques can make use of both local and global information in the <mark>compatibility graph</mark> ; 2 ) the maximum clique is a very tight constraint which requires maximizing the number of mutually compatible correspondences , but it<br>",
    "Arabic": "الرسم البياني للتوافق",
    "Chinese": "兼容图",
    "French": "graphe de compatibilité",
    "Japanese": "互換性グラフ",
    "Russian": "граф совместимости"
  },
  {
    "English": "competitive ratio",
    "context": "1: In both the set-compare and hybrid query models, there is no online algorithm with a <mark>competitive ratio</mark> of 3 2 − ε for any ε > 0. Proof. Hybrid-query model.<br>2: Therefore, the <mark>competitive ratio</mark> of any online algorithm has to be at least 3(2k+1) 2(2k+1)+1 = 3 2 − 3 8k+6 and thus we get that no online algorithm can be 3 2 − ε-competitive for any ε > 0.<br>",
    "Arabic": "نسبة التنافسية",
    "Chinese": "竞争比率",
    "French": "ratio compétitif",
    "Japanese": "競合比",
    "Russian": "конкурентное соотношение"
  },
  {
    "English": "composition function",
    "context": "1: Suppose we compute a hidden state for every span s t in S c 1 ,c 2 (Equation 2). Now, given an h t , we compute a weight vector d t over K relationship descriptors with some <mark>composition function</mark> g, which is fully specified in the next section.<br>",
    "Arabic": "وظيفة التركيب",
    "Chinese": "组合函数",
    "French": "fonction de composition",
    "Japanese": "合成関数",
    "Russian": "\"композиционная функция\""
  },
  {
    "English": "compositional generalization",
    "context": "1: Overall, our results suggest that much work remains to be done to evaluate <mark>compositional generalization</mark>, and more generally that having more rigorous standards for establishing the validity of evaluation sets should be prioritized in the future.<br>2: We consider four different datasets designed to test <mark>compositional generalization</mark>. We focus on datasets for semantic parsing and include SCAN as the most commonly used dataset for compositionality overall. Three of these datasets contain different curated splits that target different interpretations of compositionality.<br>",
    "Arabic": "التعميم التركيبي",
    "Chinese": "组合概括化",
    "French": "généralisation compositionnelle",
    "Japanese": "構成的な一般化",
    "Russian": "композиционное обобщение"
  },
  {
    "English": "compositional semantic",
    "context": "1: That further indicates that NLI models are susceptible to semantic sensitivity and have limited knowledge of <mark>compositional semantics</mark>, which can lead to the degradation of predictive confidence and incidentally inconsistent predictions.<br>2: Notably, this behaviour differs from valid and in-depth comprehension of <mark>compositional semantics</mark>, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, monotonic, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity.<br>",
    "Arabic": "الدلالة التركيبية",
    "Chinese": "组合语义",
    "French": "sémantique compositionnelle",
    "Japanese": "構成的意味論",
    "Russian": "композиционная семантика"
  },
  {
    "English": "compositionality",
    "context": "1: In cases where data engines can scale available annotations, like ours, supervised training provides an effective solution. <mark>Compositionality</mark>. Pre-trained models can power new capabilities even beyond ones imagined at the moment of training. One prominent example is how CLIP [82] is used as a component in larger systems, such as DALL•E [83].<br>",
    "Arabic": "تركيبية",
    "Chinese": "组合性",
    "French": "compositionnalité",
    "Japanese": "構成性",
    "Russian": "композициональность"
  },
  {
    "English": "compressed sensing",
    "context": "1: <mark>Compressed sensing</mark> [8]- [11] and deep learning methods have emerged as powerful options to reduce the breath-hold duration, with excellent performance [12]- [16]. Despite these advances, breath-held CINE imaging is challenging for several subject groups, including pediatric and chronic obstructive pulmonary disease (COPD) subjects.<br>",
    "Arabic": "استشعار مضغوط",
    "Chinese": "压缩感知",
    "French": "acquisition comprimée",
    "Japanese": "圧縮センシング",
    "Russian": "сжатое восприятие"
  },
  {
    "English": "compressive sensing",
    "context": "1: In summary, we considered the problem of exactly recovering a non-negative function over the space of permutations from a given partial set of Fourier coefficients. This problem is motivated by the wide ranging applications it has across several disciplines. This problem has been widely studied in the context of discrete-time functions in the recently popular <mark>compressive sensing</mark> literature.<br>2: Over the past decade, sparsity has emerged as an important tool in several fields including signal processing, statistics, and machine learning. In <mark>compressive sensing</mark>, sparsity reduces the sample complexity of measuring a signal, and statistics utilizes sparsity for high-dimensional inference tasks.<br>",
    "Arabic": "الاستشعار المضغوط",
    "Chinese": "压缩感知",
    "French": "captation comprimée",
    "Japanese": "圧縮センシング",
    "Russian": "компрессивное восприятие"
  },
  {
    "English": "computation complexity",
    "context": "1: We build our base model, called Swin-B, to have of model size and <mark>computation complexity</mark> similar to ViT-B/DeiT-B. We also introduce Swin-T, Swin-S and Swin-L, which are versions of about 0.25×, 0.5× and 2× the model size and computational complexity, respectively.<br>",
    "Arabic": "تعقيد الحساب",
    "Chinese": "计算复杂度",
    "French": "complexité de calcul",
    "Japanese": "計算の複雑さ",
    "Russian": "вычислительная сложность"
  },
  {
    "English": "computation graph",
    "context": "1: Since the runtime of backpropagation is linear in the number of edges of the <mark>computation graph</mark>, it is possible to quickly perform vast numbers of gradient descent steps in even the most gargantuan of neural networks. While gradients are arguably most important for training, they can also be used to analyze and interpret neural network behavior.<br>2: TVNet [16] implemented the TV-L1 algorithm as a <mark>computation graph</mark>, which enabled the training the TV-L1 parameters. However, TVNet operates directly based on intensity gradients instead of learned features, which limits the achievable accuracy on challenging datasets such as Sintel.<br>",
    "Arabic": "الرسم البياني الحسابي",
    "Chinese": "计算图",
    "French": "graphe de calcul",
    "Japanese": "計算グラフ",
    "Russian": "вычислительный граф"
  },
  {
    "English": "computational argumentation",
    "context": "1: It is also different from work in the related field of <mark>computational argumentation</mark>, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b).<br>",
    "Arabic": "الحجاج الحاسوبية",
    "Chinese": "计算论证",
    "French": "argumentation computationnelle",
    "Japanese": "計算論証",
    "Russian": "вычислительная аргументация"
  },
  {
    "English": "computational budget",
    "context": "1: We also acknowledge that there are larger models that we were not able to train and evaluate due to the limitations on our <mark>computational budget</mark>. The current study was focused on benchmarks with templated instances.<br>",
    "Arabic": "ميزانية الحوسبة",
    "Chinese": "计算预算",
    "French": "budget computationnel",
    "Japanese": "計算リソース",
    "Russian": "вычислительный бюджет"
  },
  {
    "English": "computational complexity",
    "context": "1: To begin, the item types are sorted in order of their density, which is an operation of O (K log K) <mark>computational complexity</mark>. Next, in the first round of this algorithm, as many units of the highest density item are selected as is feasible without exceeding the knapsack capacity.<br>2: Starting from the assumption that decisions for already processed emails cannot be reconsidered, we devised an efficient cluster- ing algorithm with <mark>computational complexity</mark> linear in the number of emails in the window. From this algorithm we derived a second integrated method for learning the weight vector.<br>",
    "Arabic": "التعقيد الحسابي",
    "Chinese": "计算复杂性",
    "French": "complexité computationnelle",
    "Japanese": "計算複雑性 (けいさんふくざつせい)",
    "Russian": "вычислительная сложность"
  },
  {
    "English": "computational experiment",
    "context": "1: C Did you run <mark>computational experiments</mark>?<br>",
    "Arabic": "تجربة حاسوبية",
    "Chinese": "计算实验",
    "French": "expérience computationnelle",
    "Japanese": "計算実験",
    "Russian": "вычислительный эксперимент"
  },
  {
    "English": "computational graph",
    "context": "1: Users often have to implement their FL algorithms with declarative programming (i.e., describing the <mark>computational graph</mark>), which raises the bar for developers. This usability issue is exacerbated in satisfying the unique requirements of FGL methods.<br>2: With the need for FL increasing, many FL frameworks [2,5,14,27,31,32,40,43] have sprung up. Most of them are designed as a conventional distributed machine learning framework, where a <mark>computational graph</mark> is declared and split for participants. Then each specific part is executed by the corresponding participant.<br>",
    "Arabic": "الرسم البياني الحسابي",
    "Chinese": "计算图",
    "French": "graphe de calcul",
    "Japanese": "計算グラフ",
    "Russian": "вычислительный граф"
  },
  {
    "English": "computational linguistic",
    "context": "1: To investigate NLP methods for identifying depression and PTSD users on Twitter, a shared task (Coppersmith et al., 2015b) at the 2nd <mark>Computational Linguistics</mark> and Clinical Psychology Workshop (CLPsych 2015) was introduced where the participants evaluated their methods on a dataset of about 1800 Twitter users.<br>2: Dror et al. (2018) survey statistical practices in all long papers presented at the 2017 meeting of the Association for <mark>Computational Linguistics</mark> (ACL), and all articles published in the 2017 volume of the Transactions of the ACL.<br>",
    "Arabic": "اللغويات الحاسوبية",
    "Chinese": "计算语言学",
    "French": "linguistique computationnelle",
    "Japanese": "計算言語学",
    "Russian": "вычислительная лингвистика"
  },
  {
    "English": "computational model",
    "context": "1: We train a <mark>computational model</mark> of phonetic learning, which has no access to phonology, on either one or two languages. We first show that the model exhibits predictable behaviors on phone-level and word-level discrimination tasks.<br>2: As mentioned above, this type of behaviour is limited to a certain period of time, and some of its direct causes are clearly determined. This provides opportunities to develop a <mark>computational model</mark> of this process.<br>",
    "Arabic": "نموذج حسابي",
    "Chinese": "计算模型",
    "French": "modèle computationnel",
    "Japanese": "計算モデル",
    "Russian": "вычислительная модель"
  },
  {
    "English": "compute budget",
    "context": "1: Furthermore, we assume that the efficient computational frontier can be described by a power-law relationship between the <mark>compute budget</mark>, model size, and number of training tokens. However, we observe some concavity in log at high <mark>compute budget</mark>s (see Appendix E). This suggests that we may still be overestimating the optimal size of large models.<br>2: In our case, consider the setting of a fixed <mark>compute budget</mark> C and a fixed budget of unique tokens U D implying a set of unique parameters U N . Let R D denote the number of times we repeat data (we assume that we are in the multi-epoch regime and hence R D > 0).<br>",
    "Arabic": "ميزانية الحوسبة",
    "Chinese": "算力预算",
    "French": "budget de calcul",
    "Japanese": "計算予算",
    "Russian": "вычислительный бюджет"
  },
  {
    "English": "computer vision",
    "context": "1: We propose a new interpretation to the term \"saliency\" \n and \"visual attention\" in images and in video sequences. 4. We present a single unified framework for treating several different problems in <mark>Computer Vision</mark>, which have been treated separately in the past.<br>2: To date, a large amount of research on Sign Language Processing (SLP) has been focused on the visual aspect of signed languages, led by the <mark>Computer Vision</mark> (CV) community, with little NLP involvement (Figure 1).<br>",
    "Arabic": "الرؤية الحاسوبية",
    "Chinese": "计算机视觉",
    "French": "vision par ordinateur",
    "Japanese": "コンピュータビジョン",
    "Russian": "компьютерное зрение"
  },
  {
    "English": "computer vision model",
    "context": "1: Each line of the generated program may invoke one of several off-the-shelf <mark>computer vision models</mark>, image processing subroutines, or python functions to produce intermediate outputs that may be consumed by subsequent parts of the program.<br>2: We proposed a diverse list of abnormality reasons by human responses, and inferred a taxonomy of visual cues that make an image abnormal. Based on three major components of this taxonomy we built <mark>computer vision models</mark> that can detect an abnormal images and reason about this decision in terms of three surprise scores.<br>",
    "Arabic": "نموذج الرؤية الحاسوبية",
    "Chinese": "计算机视觉模型",
    "French": "modèle de vision par ordinateur",
    "Japanese": "\"コンピュータビジョンモデル\"",
    "Russian": "модель компьютерного зрения"
  },
  {
    "English": "concatenation",
    "context": "1: layer to reduce the dimension back to 200, followed by another 2 fully connected layers on top to match the architecture of the weighted sum merger. Results in Table 6 indicate that weighted sum fusion consistently outperforms <mark>concatenation</mark>.<br>2: However, in order to retain an influence from image to code-Jacobian, we add the element-wise multiplication of every <mark>concatenation</mark> to the <mark>concatenation</mark> itself. I.e., we increment every <mark>concatenation</mark> [L1, L2] of layers L1 and L2 to [L1, L2, L1 L2].<br>",
    "Arabic": "ربط",
    "Chinese": "拼接",
    "French": "concaténation",
    "Japanese": "連結",
    "Russian": "конкатенация"
  },
  {
    "English": "concatenation operation",
    "context": "1: sim(x , x ) = a ⊤ [x ∥x ],(13) \n where a is a weight vector, [•∥•] is a <mark>concatenation operation</mark>. Then we use the attention scores to model the interference with different significance. Specifically, we replace the original incidence matrix H in Eq.<br>2: Finally, a softmax layer produces point-wise class label probabilities. The <mark>concatenation operation</mark> aggregates information from BCLs operating at different lattice scales. Similar techniques of concatenating outputs from network layers at different depths have been useful in 2D CNNs [17]. All parameterized layers, except for the last CONV layer, are followed by ReLU and BatchNorm.<br>",
    "Arabic": "عملية الربط",
    "Chinese": "连接操作",
    "French": "opération de concaténation",
    "Japanese": "連結操作",
    "Russian": "конкатенационная операция"
  },
  {
    "English": "concentration inequality",
    "context": "1: A somewhat more sophisticated approach to <mark>concentration inequalities</mark> and generalization bounds is based on localization ideas, motivated by the fact that near the optimum of an empirical risk, the complexity of the function class may be smaller than over the entire (global) class [47,3].<br>2: , so that the sufficient condition (30) holds and expression (11) follows. We now argue that the event E n has high probability via the following lemma, which is an application of <mark>concentration inequalities</mark> for convex functions coupled with careful estimates of the expectation of standard deviations. Lemma A.1. Let Z i be i.i.d.<br>",
    "Arabic": "تفاوت التركيز",
    "Chinese": "浓度不等式",
    "French": "inégalité de concentration",
    "Japanese": "集中不等式",
    "Russian": "концентрационное неравенство"
  },
  {
    "English": "concentration parameter",
    "context": "1: q VM (θ) = exp (κ cos (θ − µ)) 2πI 0 (κ) ,(24) \n where µ is the location parameter, κ is the <mark>concentration parameter</mark>, and I 0 (•) is the modified Bessel function with order zero. The mixture PDF is thus: \n<br>2: Under the settings of Pitman-Yor Topic Model, each topic defines a distribution over words, and the base distribution defines the common underlying common language model shared by the topics. The <mark>concentration parameter</mark> b controls how likely a word is to occur again while being sampled from the generated distribution.<br>",
    "Arabic": "معامل التركيز",
    "Chinese": "浓度参数",
    "French": "paramètre de concentration",
    "Japanese": "濃度パラメータ",
    "Russian": "параметр концентрации"
  },
  {
    "English": "concept",
    "context": "1: The learning criterion expected in this study is that of exact identification, which is achieved if the learner can infer a CPnet that correctly represents the target <mark>concept</mark>. A <mark>concept</mark> is a strict partial ordering on the outcome space. A representation class is a set N of CP-nets.<br>",
    "Arabic": "مفهوم",
    "Chinese": "概念",
    "French": "notion",
    "Japanese": "概念",
    "Russian": "концепция"
  },
  {
    "English": "concept assertion",
    "context": "1: C if the projection of r I to the second component is contained in C I , a <mark>concept assertion</mark> A(a) if a ∈ A I , and a role assertion r(a, b) if (a, b) ∈ r I .<br>",
    "Arabic": "تأكيد المفهوم",
    "Chinese": "概念断言",
    "French": "assertion de concept",
    "Japanese": "概念主張",
    "Russian": "утверждение концепции"
  },
  {
    "English": "concept atom",
    "context": "1: In ELIHF , this is decidable and EXPTIME-complete; see appendix. Queries. A conjunctive query ( CQ ) is of the form q ( x ) = ∃ȳ ϕ ( x , ȳ ) , wherex andȳ are tuples of variables and ϕ ( x , ȳ ) is a conjunction of <mark>concept atoms</mark> A ( x ) and role atoms r ( x , y ) , with A a concept name , r a<br>",
    "Arabic": "ذرة المفهوم",
    "Chinese": "概念原子",
    "French": "atome de concept",
    "Japanese": "概念原子",
    "Russian": "концептный атом"
  },
  {
    "English": "concept class",
    "context": "1: For the case of agnostic binary classification, the intrinsic dimension is captured by the VC-dimension of the <mark>concept class</mark> (see [21,4]). For the case of distribution learning with respect to 'natural' parametric classes, we expect this dimension to be equal to the number of parameters.<br>2: Thus, our learnability results are defined in terms of a <mark>concept class</mark> in which the target concept is chosen, and an instance class that circumscribes the set of examples used by equivalence and membership queries. The key message to be gleaned from this paper is that active learning is required to correctly and efficiently extract preference networks in binary-valued domains.<br>",
    "Arabic": "فئة المفهوم",
    "Chinese": "概念类",
    "French": "classe de concepts",
    "Japanese": "概念クラス",
    "Russian": "класс концепций"
  },
  {
    "English": "concept drift",
    "context": "1: Some other splitters are designed to provide various non-i.i.d.ness, including covariate shift, <mark>concept drift</mark>, and prior probability shift [15]. Details about the provided splitters and the FL datasets constructed by applying them can be found in the Appendix A.1 and Appendix A.3, respectively.<br>",
    "Arabic": "تحول المفهوم",
    "Chinese": "概念漂移",
    "French": "dérive conceptuelle",
    "Japanese": "概念ドリフト",
    "Russian": "дрейф концепции"
  },
  {
    "English": "concept inclusion",
    "context": "1: nR.C , or mR.C . In [Baader et al., 2005], a polynomial time classification procedure has been presented for the description logic EL ++ , which extends EL + with the bottom concept ⊥, nominals, and \"safe\" concrete domains. The procedure uses a number of so-called completion rules that derive new <mark>concept inclusions</mark>.<br>2: The description logic EL + is one of the few description logics for which standard reasoning problems are decidable in polynomial time. EL + allows only for concepts constructed from atomic concepts A and the top concept using conjunction C D and existential restriction ∃r.C, where r is an atomic role. The axioms of EL + can be either <mark>concept inclusions</mark> \n<br>",
    "Arabic": "تضمين المفهوم",
    "Chinese": "概念包含",
    "French": "inclusion de concepts",
    "Japanese": "概念包含",
    "Russian": "включение концепта"
  },
  {
    "English": "concept name",
    "context": "1: Introduce a fresh <mark>concept name</mark> X C for every C ∈ sub(O) and define O to contain, for every C ∈ sub(O), the following concept inclusions and range restrictions: \n • X C C , C X C if C is a <mark>concept name</mark> or ; • X C X D1 , X C X D2 , and X D1 X D2 X C if C = D 1 D 2 ; • X C ∃r.X D and ∃r.X D X C if C = ∃r.D ; • X C X D for each<br>2: An ABox is a set of concept assertions A(a) and role assertions r(a, b) where A is a <mark>concept name</mark>, r a role name, and a, b are individual names. We use ind(A) to denote the set of all individual names that occur in A.<br>",
    "Arabic": "اسم مفهوم",
    "Chinese": "概念名",
    "French": "nom de concept",
    "Japanese": "概念名",
    "Russian": "имя концепта"
  },
  {
    "English": "condition 1",
    "context": "1: We shall do so by verifying that the <mark>Condition 1</mark> holds with probability 1−o(1), so that the sparsest-fit algorithm will recover f as per Theorem III.1. As noted earlier, the \"linear independence\" of <mark>Condition 1</mark> is satisfied with probability 1 under R(K, C ).<br>",
    "Arabic": "الشرط 1",
    "Chinese": "条件1",
    "French": "condition 1",
    "Japanese": "条件1",
    "Russian": "условие 1"
  },
  {
    "English": "condition number",
    "context": "1: where κ = µ/L is an upper bound on the <mark>condition number</mark> of f , C is a constant that depends on the graph and κ, and θ ARG is the rate of convergence of accelerated randomized gossip on the graph G as defined in Theorem 5 but with graph resistances are defined in a different way (see Theorem 10).<br>2: Moreover, Z has a bounded <mark>condition number</mark> σ max (Z)/σ min (Z) = κ. Throughout this paper we think of µ and κ as small constants, and the sample complexity depends polynomially on these two parameters.<br>",
    "Arabic": "رقم الشرط",
    "Chinese": "条件数",
    "French": "nombre de conditionnement",
    "Japanese": "条件数",
    "Russian": "число обусловленности"
  },
  {
    "English": "conditional computation",
    "context": "1: For example, through the use of <mark>conditional computation</mark> large MoE models like the 1.7 trillion parameter Switch transformer , the 1.2 Trillion parameter GLaM model , and others (Artetxe et al., 2021;Zoph et al., 2022) are able to provide a large effective model size despite using relatively fewer training and inference FLOPs.<br>",
    "Arabic": "الحوسبة الشرطية",
    "Chinese": "条件计算",
    "French": "calcul conditionnel",
    "Japanese": "条件付き計算",
    "Russian": "условные вычисления"
  },
  {
    "English": "conditional density",
    "context": "1: The VAE uses a decoder fed by latent vectors, drawn from a <mark>conditional density</mark> estimated from the fully sampled images using an encoder. Since fully sampled images are not available in our setting, we approximate the <mark>conditional density</mark> of the latent vectors by a parametric model whose parameters are estimated from the undersampled measurements using back-propagation.<br>2: A predictor in the form of a <mark>conditional density</mark> p(X t | X t−1 ) to represent the (typically strong) prior dependency between states at successive time steps. These elements (together with a prior p(X 1 )) form a structured prior distribution for a randomly sampled image sequence z 1 , . . .<br>",
    "Arabic": "الكثافة الشرطية",
    "Chinese": "条件密度",
    "French": "densité conditionnelle",
    "Japanese": "条件付き密度",
    "Russian": "условная плотность"
  },
  {
    "English": "conditional distribution",
    "context": "1: Here q (• | •) is a tractable <mark>conditional distribution</mark>, and may be regarded as a stochastic parser that runs on a compressed tag sequence t instead of a word embedding sequence x.<br>2: Let π be a probability distribution over some set of variables I. Let B j be the set of state pairs (X, Y ) which differ only at variable j. Let π i (•|X I\\{i} ) denote the <mark>conditional distribution</mark> in π of variable i given all the other variables in state X.<br>",
    "Arabic": "التوزيع الشرطي",
    "Chinese": "条件分布",
    "French": "distribution conditionnelle",
    "Japanese": "条件付き分布",
    "Russian": "условное распределение"
  },
  {
    "English": "conditional effect",
    "context": "1: Each action a ∈ A has a set of literals pre(a) called the precondition and a set of <mark>conditional effects</mark> cond(a). Each conditional effect C E ∈ cond(a) is composed of sets of literals C (the condition) and E (the effect).<br>2: • For each q ∈ Q and f ∈ F , an action econd f q that evaluates the condition of the current controller state: We extend the compilation to address generalized planning problems P = {P 1 , . . . , P T }. In this case a solution to P n builds an FSC C and simulates the execution of C on all the individual planning problems P t ∈ P. The extension introduces actions end t , 1 ≤ t < T , with precondition G t ∪ { cs qn } and <mark>conditional effects</mark> that reset the world state to ( q 0 , I<br>",
    "Arabic": "التأثير الشرطي",
    "Chinese": "条件效应",
    "French": "effet conditionnel",
    "Japanese": "条件付き効果",
    "Russian": "условное воздействие"
  },
  {
    "English": "conditional entropy",
    "context": "1: In speech recognition, training the parameters of the acoustic model by optimizing the (average) mutual information and <mark>conditional entropy</mark> as they are defined in information theory is a standard approach (Bahl et al., 1986;Ney, 1995).<br>2: Token-level annotation artefacts in each dataset. These are the tokens whose omission leads to the greatest average increase in <mark>conditional entropy</mark> for each class (given in parentheses).<br>",
    "Arabic": "الانتروبيا المشروطة",
    "Chinese": "条件熵",
    "French": "entropie conditionnelle",
    "Japanese": "条件付きエントロピー",
    "Russian": "условная энтропия"
  },
  {
    "English": "conditional expectation",
    "context": "1: The authors claim that Algorithm 1 computes the <mark>conditional expectation</mark> E[F | x S ], for a given set of features S and tree-based model F . We first describe the algorithm and then show by example that this algorithm does not accurately compute the <mark>conditional expectation</mark>.<br>2: In the case that an additive white noise model is assumed, the next result shows that the <mark>conditional expectation</mark> is the function of V (N i \\{i}) that minimizes the mean square error with the true image u. \n<br>",
    "Arabic": "التوقع الشرطي",
    "Chinese": "条件期望",
    "French": "espérance conditionnelle",
    "Japanese": "条件付き期待値",
    "Russian": "условное математическое ожидание"
  },
  {
    "English": "conditional generation",
    "context": "1: Additionally, various models are built to target different applications of NLP such as understanding, generation, and conditional generation, plus specialized use cases such as fast inference or multi-lingual applications. Heads Name Input Output Tasks Ex. Datasets Language Modeling x 1 : n−1 x n ∈ V Generation WikiText-103 Sequence Classification x 1 : N y ∈ C Classification , Sentiment Analysis GLUE , SST , MNLI Question Answering x 1 : M , x M : N y span [ 1 : N ] QA , Reading Comprehension SQuAD , Natural Questions Token Classification x 1 : N y 1 : N ∈ C N NER , Tagging OntoNotes , WNUT Multiple Choice x 1 : N , X y ∈ X Text Selection SWAG , ARC Masked LM x 1 : N \\n x n ∈ V Pretraining Wikitext , C4 <mark>Conditional Generation</mark> x 1 : N y 1 : M ∈ V M Translation , Summarization WMT , IWSLT , CNN/DM<br>",
    "Arabic": "التوليد المشروط",
    "Chinese": "条件生成",
    "French": "génération conditionnelle",
    "Japanese": "条件付き生成",
    "Russian": "Условная генерация"
  },
  {
    "English": "conditional gradient",
    "context": "1: While we focus on using loss-augmented inference for estimating the semi-gradient, it can also be used as the cutting plane [13] and the <mark>conditional gradient</mark> of the dual of problem (4). In addition to this, loss-augmented inference is also required for solving problem (3) using the direct loss minimization framework [22].<br>2: Even adding the positive semi-definite constraint M 1 (µ) 0, for which TRW must be optimized using <mark>conditional gradient</mark> and semidefinite programming for the projection step, does not improve the accuracy by much.<br>",
    "Arabic": "تدرج مشروط",
    "Chinese": "条件梯度",
    "French": "gradient conditionnel",
    "Japanese": "条件付き勾配",
    "Russian": "условный градиент"
  },
  {
    "English": "conditional independence",
    "context": "1: Many distributions are not decomposable into independent or exchangeable decompositions. Similar to <mark>conditional independence</mark>, the notion of exchangeability can be extended to conditional exchangeability. We generalize exchangeability to conditional distributions, and state the corresponding tractability guarantees.<br>2: Similar to <mark>conditional independence</mark>, partial exchangeability decomposes a probabilistic model so as to facilitate efficient inference. Most importantly, the notions of <mark>conditional independence</mark> and partial exchangeability are complementary, and when combined, define a much larger class of tractable models than the class of models rendered tractable by <mark>conditional independence</mark> alone.<br>",
    "Arabic": "الاستقلال الشرطي",
    "Chinese": "条件独立性",
    "French": "indépendance conditionnelle",
    "Japanese": "条件付き独立性",
    "Russian": "условная независимость"
  },
  {
    "English": "conditional independency",
    "context": "1: For example, when interpreting the above MLN as an (undirected) probabilistic graphical model, all pairs of random variables in {Smokes(A), Smokes(B), . . . } are connected by an edge due to the groundings of Formula 2. The model has no conditional or contextual independencies between the Smokes variables.<br>2: There are many promising directions for future research in probabilistic graphics programming. Introducing a dependency tracking mechanism could let us exploit the many <mark>conditional independencies</mark> in rendering for more efficient parallel inference. Automatic particle-filter based inference schemes [47,24] could extend the approach to image sequences.<br>",
    "Arabic": "استقلال مشروط",
    "Chinese": "条件独立性",
    "French": "indépendance conditionnelle",
    "Japanese": "条件付き独立性",
    "Russian": "условная независимость"
  },
  {
    "English": "conditional likelihood",
    "context": "1: = Q i q ( a i | t i \n ) is a variational distribution that we train to minimize this upper bound. This is equivalent to training q(a | t) by maximum <mark>conditional likelihood</mark>.<br>",
    "Arabic": "احتمالية مشروطة",
    "Chinese": "条件似然率",
    "French": "vraisemblance conditionnelle",
    "Japanese": "条件付き尤度",
    "Russian": "условная правдоподобность"
  },
  {
    "English": "conditional log likelihood",
    "context": "1: In each iteration we would like to add the feature F k that will maximize the <mark>conditional log likelihood</mark>. We denote by L k (F, λ) the possible likelihood if the feature F , weighted by λ, is added at the k'th iteration: \n<br>2: For each of the selected features, and for each of a small discrete set of possible λ values λ ∈ {λ 1 , ..., λ M }, we run an inference process and evaluate the explicit <mark>conditional log likelihood</mark>.<br>",
    "Arabic": "احتمالية اللوغاريتم المشروطة",
    "Chinese": "条件对数似然",
    "French": "log de vraisemblance conditionnelle",
    "Japanese": "条件付き対数尤度",
    "Russian": "условное логарифмическое правдоподобие"
  },
  {
    "English": "conditional log probability",
    "context": "1: Consider candidates volleyball player, squash player and golf player, the <mark>conditional log probability</mark> of token player might be higher, but the candidates are distinguished by their headwords. In summary, mean-pooling obtains the best results with the most comprehensive information.<br>",
    "Arabic": "احتمالية لوغاريتمية شرطية",
    "Chinese": "条件对数概率",
    "French": "probabilité log conditionnelle",
    "Japanese": "条件付き対数確率",
    "Russian": "условная логарифмическая вероятность"
  },
  {
    "English": "conditional maximum entropy",
    "context": "1: Conditional maximum entropy approaches are ill-suited for this setting as they assume all side information is available a priori. Building on the recent advance of the Marko-Massey theory of directed information (Massey, 1990), we present the principle of maximum causal entropy (MaxCausalEnt).<br>",
    "Arabic": "الانتروبيا القصوى المشروطة",
    "Chinese": "条件最大熵",
    "French": "entropie maximale conditionnelle",
    "Japanese": "条件付き最大エントロピー",
    "Russian": "условная максимальная энтропия"
  },
  {
    "English": "conditional model",
    "context": "1: Here the learned coordination model In an iid setting, the true test labels y 1 and y 2 are independent given the true <mark>conditional model</mark>. However, they are not independent given a learned estimate of the model. P (y * i y j |x * i x j , φ \n<br>2: A board in which each tile is randomly set to red or white with probability 0.5 is initialized, and this trained network is used to predict masked out tiles. Since the <mark>conditional model</mark> is trained on the GSP boards , this generates a set of boards with similar statistical properties to the original GSP boards ( example : number of red tiles in the GSP boards ( Mean=8.4 , SD=2.26 ) do not significantly differ from the control boards ( Mean=7.4 , SD=2.01 ) , p = 0.12 ) , but also implicitly<br>",
    "Arabic": "نموذج شرطي",
    "Chinese": "条件模型",
    "French": "modèle conditionnel",
    "Japanese": "条件付きモデル",
    "Russian": "условная модель"
  },
  {
    "English": "conditional probability",
    "context": "1: Now considering a random variable : × ℐ → ℐ defined as the item in a user-item pair, we can handle <mark>conditional probabilities</mark> such as ( | ), ( | ), and so on, for ∈ ℐ -where shall stand as an abbreviation of = .<br>2: 3 The sub-trees for our postflop models can be computed in isolation, provided that the appropriate preconditions are given as input. Unfortunately, knowing the correct <mark>conditional probabilities</mark> would normally entail solving the whole game, so there would be no advantage to the decomposition. For simple postflop models, we dispense with the prior probabilities.<br>",
    "Arabic": "الاحتمالية الشرطية",
    "Chinese": "条件概率",
    "French": "probabilité conditionnelle",
    "Japanese": "条件付き確率",
    "Russian": "условная вероятность"
  },
  {
    "English": "conditional probability distribution",
    "context": "1: Based on this intuition, a <mark>conditional probability distribution</mark> q p of y given x is defined as \n q p (y | x) = exp s p (y | x) y ∈Y exp s p (y | x)(1) \n where \n<br>2: So far we have restricted our discussion to unconditional models. However, many NLP problems are modelled as <mark>conditional probability distribution</mark> a(x|c) that takes some variable context c as input. Korbak et al. (2022a) proposed the following generalization of GDC to conditional models.<br>",
    "Arabic": "التوزيع الاحتمالي الشرطي",
    "Chinese": "条件概率分布",
    "French": "distribution de probabilité conditionnelle",
    "Japanese": "条件付き確率分布",
    "Russian": "условное распределение вероятностей"
  },
  {
    "English": "conditional random Field",
    "context": "1: To the best of our knowledge, our work is the first to address the above limitations. Unlike prior work, OPINE models open intent discovery as a sequence tagging task (Section 2). We develop a neural model consisting of a <mark>Conditional Random Field</mark> (CRF) on top of a bidirectional LSTM with a multi-head self-attention mechanism.<br>",
    "Arabic": "حقل عشوائي شرطي",
    "Chinese": "条件随机场",
    "French": "Champ aléatoire conditionnel",
    "Japanese": "条件付き確率場",
    "Russian": "условное случайное поле"
  },
  {
    "English": "conditional sampling",
    "context": "1: Our tight example for the <mark>conditional sampling</mark> approach consists of a pair of distributions Q and Q . These should be thought of as the output distributions of the algorithm Q on two neighbouring inputs. The distributions are supported on only three points.<br>2: Another extension of interest is <mark>conditional sampling</mark>. By amortizing SGM with respect to an observation y it is possible to approximately sample from a given posterior distribution.<br>",
    "Arabic": "أخذ العينات المشروطة",
    "Chinese": "条件采样",
    "French": "échantillonnage conditionnel",
    "Japanese": "条件付きサンプリング",
    "Russian": "условная выборка"
  },
  {
    "English": "conditional text generation",
    "context": "1: An important but mostly overlooked aspect of automated simplification-especially in the <mark>conditional text generation</mark> regime-is whether outputs are faithful to the inputs that they are simplifying.<br>2: Text Generation EXPLAINABOARD also considers text generation tasks, and currently mainly focuses on <mark>conditional text generation</mark>, for example, text summarization (Rush et al., 2015;Liu and Lapata, 2019) and machine translation .<br>",
    "Arabic": "توليد النص المشروط",
    "Chinese": "有条件文本生成",
    "French": "génération de texte conditionnelle",
    "Japanese": "条件付きテキスト生成",
    "Russian": "условная генерация текста"
  },
  {
    "English": "conditioning",
    "context": "1: X, Y , the question simply takes the form of \"<mark>conditioning</mark>\", i.e., compare P(X) versus P(X|Y ). This form suffices if our interest is restricted to the predictions of the two models.<br>",
    "Arabic": "التكييف",
    "Chinese": "条件化",
    "French": "conditionnement",
    "Japanese": "条件づけ (jōkenzuke)",
    "Russian": "условное распределение"
  },
  {
    "English": "conditioning vector",
    "context": "1: In simple terms, a conditional diffusion modelx θ is trained using a squared error loss to denoise a variably-noised image z t := α t x+σ t as follows: \n E x,c, ,t w t x θ (α t x + σ t , c) − x 2 2 (3) \n where x is the ground-truth image , c is a <mark>conditioning vector</mark> ( e.g. , obtained from a text prompt ) , ∼ N ( 0 , I ) is a noise term and α t , σ t , w t are terms that control the noise schedule and sample quality , and are functions of the diffusion process time t ∼ U<br>2: where x is the ground-truth image, c is a <mark>conditioning vector</mark> (e.g., obtained from a text prompt), and α t , σ t , w t are terms that control the noise schedule and sample quality, and are functions of the diffusion process time t ∼ U([0, 1]).<br>",
    "Arabic": "متجه التكييف",
    "Chinese": "条件向量",
    "French": "vecteur de conditionnement",
    "Japanese": "条件付けベクトル",
    "Russian": "вектор кондиционирования"
  },
  {
    "English": "confidence",
    "context": "1: We observed that the 30 unclear responses have an average stance detection <mark>confidence</mark> of 0.76, while the 80 unclear responses have an average <mark>confidence</mark> of 0.90. This indicates that the stance detector's <mark>confidence</mark> could serve as a heuristic to filter out unclear responses.<br>2: The support and <mark>confidence</mark> of the association rule r : X ⇒ Y are denoted by sup(r) and conf (r). The task of the association data mining problem is to find all association rules with support and <mark>confidence</mark> greater than user specified minimum support and minimum <mark>confidence</mark> threshold values [12].<br>",
    "Arabic": "ثقة",
    "Chinese": "置信度",
    "French": "confiance",
    "Japanese": "信頼度",
    "Russian": "уверенность"
  },
  {
    "English": "confidence bind",
    "context": "1: In practice, since the <mark>confidence bound</mark> directly scales with R, and the user needs to set some threshold term γ on + αB, a guess on the scale of R is already decided by the user threshold γ.<br>2: Specifically, the upper <mark>confidence bound</mark>û ij is given byû ij =p ij + ασ ij where α is a hyperparameter that controls the size of the confidence region andσ 2 ij is the estimated variance given by: \n<br>",
    "Arabic": "إرتباط الثقة",
    "Chinese": "置信绑定",
    "French": "borne de confiance",
    "Japanese": "信頼度束縛",
    "Russian": "доверительная связь"
  },
  {
    "English": "confidence interval",
    "context": "1: Asymptotically, as the number of samples N → ∞, the <mark>confidence interval</mark> ε → 0, which implies that Eq (4) will always be feasible. Finally, we note that Eq (4) has a closed form solution, whose proof is provided in Appendix C.2. Lemma 4.4.<br>2: Table 1 gives the average scores for the three algorithms associated to the 95% <mark>confidence interval</mark> in parenthesis (2 × σ √ n ).<br>",
    "Arabic": "فترة الثقة",
    "Chinese": "置信区间",
    "French": "intervalle de confiance",
    "Japanese": "信頼区間",
    "Russian": "доверительный интервал"
  },
  {
    "English": "confidence map",
    "context": "1: The two main configurations are: (i) a single-view model (input is RGB image) and (ii) our full two-frame model, where the input includes a reference image, an initial masked depth map D pp , a <mark>confidence map</mark> C, and a human mask M .<br>2: Our method can detect symmetry planes accurately despite the presence of asymmetric texture and lighting effects. We also overlay the predicted <mark>confidence map</mark> σ onto the image, confirming that the model assigns low confidence to asymmetric regions in a sample-specific way.<br>",
    "Arabic": "خريطة الثقة",
    "Chinese": "置信度映射图",
    "French": "carte de confiance",
    "Japanese": "信頼度マップ",
    "Russian": "карта уверенности"
  },
  {
    "English": "confidence score",
    "context": "1: First, we explicitly model asymmetric illumination. Second, our model also estimates, for each pixel in the input image, a <mark>confidence score</mark> that explains the probability of the pixel having a symmetric counterpart in the image (see conf σ, σ in Fig. 2).<br>2: The first combination method is based on re-ranking a merged ¤ -best list. A <mark>confidence score</mark> from each system is assigned to each unique hypothesis in the merged list. The <mark>confidence score</mark>s for each hypothesis are used to produce a single score which, combined with a 5-gram language model score, determines a new ranking of the hypotheses.<br>",
    "Arabic": "مستوى الثقة",
    "Chinese": "置信度分数",
    "French": "score de confiance",
    "Japanese": "信頼度スコア",
    "Russian": "показатель уверенности"
  },
  {
    "English": "confidence threshold",
    "context": "1: Once this is done, the algorithm then applies an association rule mining algorithm to discover all the association rules from this transaction database with a minimum support and <mark>confidence threshold</mark> defined by domain expert.<br>",
    "Arabic": "عتبة الثقة",
    "Chinese": "置信度阈值",
    "French": "seuil de confiance",
    "Japanese": "信頼度閾値",
    "Russian": "порог уверенности"
  },
  {
    "English": "configuration",
    "context": "1: Here, we'll give the algorithm deciding whether a given <mark>configuration</mark> G 5 is forbidden or allowed, i.e., whether E(G 5 ) is empty or non-empty. It already follows from section 5, but we will simplify it, providing additional insights. How to sort five points on a conic.<br>2: Accordingly, a dynamics for G is modeled as a sequence of <mark>configuration</mark>s c 0 , ..., c k such that c h+1 , for each h ∈ {0, ..., k − 1}, is obtained from c h by flipping the opinion of an agent that is not stable in c h .<br>",
    "Arabic": "تكوين",
    "Chinese": "配置",
    "French": "configuration",
    "Japanese": "構成",
    "Russian": "конфигурация"
  },
  {
    "English": "confusion matrix",
    "context": "1: Table 11 shows <mark>confusion matrices</mark> per suffixoid candidate and for all data cumulatively for the best setting in our automatic experiments (cf. Table 5), in which all features were used in 5-fold cross validation.<br>",
    "Arabic": "مصفوفة الارتباك",
    "Chinese": "混淆矩阵",
    "French": "matrice de confusion",
    "Japanese": "混同行列",
    "Russian": "матрица путаницы"
  },
  {
    "English": "confusion network",
    "context": "1: The links in this lattice represent the alternative words (including nulls) at the corresponding position in the string. Confusion network decoding may be viewed as finding the highest scoring path through this lattice with summing all word scores along the path. The standard lattice decoding algorithms may also be used to generate ¤ -best lists from the <mark>confusion network</mark>.<br>2: The <mark>confusion network</mark> is decoded using an arc-level confidence score for each input system and a language model, the weights for which are estimated discriminatively to maximize BLEU.<br>",
    "Arabic": "شبكة الالتباس",
    "Chinese": "混淆网络",
    "French": "réseau de confusion",
    "Japanese": "混同ネットワーク",
    "Russian": "сеть неопределённостей"
  },
  {
    "English": "conjugate gradient",
    "context": "1: Other methods such as <mark>conjugate gradient</mark> have faster convergence, at least in the vicinity of local minima, but are more complicated to implement than gradient descent [8] . The convergence of gradient based methods also have the disadvantage of being very sensitive to the choice of step size, which can be very inconvenient for large applications.<br>2: This can dramatically speed up convergence, often by an order of magnitude, as many semantic segmentation algorithms often only assign a non-trivial probability to a small fraction of object categories for any given scene. Our approach is similar to a simplified and approximate version of \"block\" <mark>conjugate gradient</mark> [23].<br>",
    "Arabic": "تدرج مترافق",
    "Chinese": "共轭梯度",
    "French": "gradient conjugué",
    "Japanese": "共役勾配",
    "Russian": "метод сопряженных градиентов"
  },
  {
    "English": "conjugate gradient descent",
    "context": "1: The subspace optimizer S is specified by the user and is customizable to the problem being solved. In our experiments we used multi-start versions of <mark>conjugate gradient descent</mark> and Levenberg-Marquardt [Nocedal and Wright, 2006].<br>2: In our experiments, we use multi-start versions of both <mark>conjugate gradient descent</mark> and Levenberg-Marquardt, but other possibilities include Monte Carlo search, quasi-Newton methods, and simulated annealing. We experimented with both grid search and branch and bound, but found them practical only for easy problems.<br>",
    "Arabic": "نزول التدرج المترافق",
    "Chinese": "共轭梯度下降法",
    "French": "descente de gradient conjuguée",
    "Japanese": "共役勾配降下法",
    "Russian": "метод сопряженных градиентов"
  },
  {
    "English": "conjugate gradient method",
    "context": "1: The updates for the documentlevel variational parameters have a closed form; we use the <mark>conjugate gradient method</mark> to optimize the topic-level variational observations. The resulting variational approximation for the natural topic parameters {β k,1 , . . .<br>",
    "Arabic": "طريقة التدرج المترافق",
    "Chinese": "共轭梯度法",
    "French": "méthode du gradient conjugué",
    "Japanese": "共役勾配法",
    "Russian": "метод сопряженных градиентов"
  },
  {
    "English": "conjunct",
    "context": "1: what and which) are distinguished with -iN and -rN arguments, respectively, in order to regularize typical contexts for filler-gap traces. Conjuncts are assigned -c and -d arguments to distinguish composition functions for <mark>conjuncts</mark> from composition functions for ordinary arguments. This distinction makes it possible for ordinary arguments to be shared among <mark>conjuncts</mark>.<br>2: Steps 1-3. Figure 5: Transforming the logical form in Figure 1(b). The step numbers correspond to those in Figure 6. Input: A conjunction, c, of n <mark>conjuncts</mark>; MRL productions, p1, . . .<br>",
    "Arabic": "عطف",
    "Chinese": "合取项",
    "French": "conjonctif",
    "Japanese": "接続語",
    "Russian": "конъюнкт"
  },
  {
    "English": "conjunctive normal form",
    "context": "1: ) formula F . \n Although our theoretical results hold for propositional formulas in general, we present our work in the context of formulas in the standard <mark>conjunctive normal form</mark> or CNF, on which our experimental results rely. A clause ( also called a CNF constraint ) C is a logical disjunction of a set of possibly negated variables ; σ satisfies C if it satisfies at least one signed variable of C. A formula F is in the CNF form if it is a logical conjunction of a set of clauses ; σ satisfies F if it satisfies all clauses of<br>2: A propositional logic formula in <mark>conjunctive normal form</mark> (CNF) is a conjunction of clauses, such as \n F := C 1 ∧ C 2 ∧ C 3 .<br>",
    "Arabic": "الشكل العادي المقترن",
    "Chinese": "合取范式 (CNF)",
    "French": "forme normale conjonctive",
    "Japanese": "連言標準形",
    "Russian": "конъюнктивная нормальная форма"
  },
  {
    "English": "conjunctive query",
    "context": "1: In contrast to Rudolph and Krötzsch [2013], we do not define monadic queries as <mark>conjunctive queries</mark> of FCPs, but we merely allow existential quantification to project some of the FCP variables. Proposition 1 below shows that this does not reduce expressiveness. We generally consider monadic Datalog as a special case of frontier-guarded Datalog.<br>2: By focusing only on such queries, the algorithm is limited to sampling only from a small subset of the web. It is easier to construct <mark>conjunctive queries</mark> with less than k results simply by adding more random terms. This, however, also increases the fraction of dictionaries and word lists among the results.<br>",
    "Arabic": "استعلام مقترن",
    "Chinese": "联结查询",
    "French": "requête conjonctive",
    "Japanese": "接続クエリ",
    "Russian": "Конъюнктивный запрос"
  },
  {
    "English": "connected component",
    "context": "1: Let G = (N, E) be an undirected graph encoding the interactions of a set N of agents. We assume that G is connectedotherwise apply our results on each <mark>connected component</mark>.<br>2: We first present a lemma which can help us exclude the case of disconnected graphs. Lemma C.18. Given a node w, let S G (w) ⊂ V be the <mark>connected component</mark> in graph G that comprises node w. For any two nodes w ∈ V in G and \n<br>",
    "Arabic": "المُكَوِّن المُتَّصِل",
    "Chinese": "连通分量",
    "French": "composante connexe",
    "Japanese": "連結成分",
    "Russian": "связная компонента"
  },
  {
    "English": "connectionist model",
    "context": "1: Moreover, because the SLM provides an EM training procedure for its components, the <mark>connectionist models</mark> can also be improved by the EM training. In this paper, we will study the impact of neural network modeling on the SLM, when all of its three components are modeled with this approach.<br>2: Our purpose here is to look more deeply into why meaning can't be learned from linguistic form alone, even in the context of modern hardware and techniques for scaling <mark>connectionist models</mark> to the point where they can take in vast amounts of data.<br>",
    "Arabic": "نموذج ترابطي",
    "Chinese": "连接主义模型",
    "French": "modèle connexionniste",
    "Japanese": "接続主義モデル",
    "Russian": "коннекционистская модель"
  },
  {
    "English": "connectivity matrix",
    "context": "1: We compare results using the root-mean-squared-error between the true <mark>connectivity matrix</mark> J and the inferred <mark>connectivity matrix</mark>Ĵ.<br>2: We assume we are given an input graph defined by both a <mark>connectivity matrix</mark> A as well as an algorithm G which accepts as input a kernel matrix K, and outputs a <mark>connectivity matrix</mark>Ã = G(K), such thatÃ is close to the original graph A.<br>",
    "Arabic": "مصفوفة الترابط",
    "Chinese": "连通性矩阵",
    "French": "matrice de connectivité",
    "Japanese": "接続行列",
    "Russian": "матрица связности"
  },
  {
    "English": "consensus network decoding",
    "context": "1: One of the most successful approaches is <mark>consensus network decoding</mark> (Mangu et al., 2000) which assumes that the confidence of a word in a certain position is based on the sum of confidences from each system output having the word in that position.<br>2: This approach is similar to the multi-engine MT framework proposed in (Frederking and Nirenburg, 1994) which is not capable of re-ordering. The word-level combination is based on <mark>consensus network decoding</mark>.<br>",
    "Arabic": "فك ترميز شبكة التوافق",
    "Chinese": "共识网络解码",
    "French": "décodage du réseau de consensus",
    "Japanese": "コンセンサスネットワークデコーディング",
    "Russian": "согласованное сетевое декодирование"
  },
  {
    "English": "consequent",
    "context": "1: Instead, we represent them as semi-structured inference rules whose expressivity lies between free text and logical forms. Each rule takes the form \"antecedent connective <mark>consequent</mark>,\" where the antecedent and <mark>consequent</mark> are composed by filling in syntactic slots for subject, verb, object(s), and preposition(s).<br>2: An association rule r is a rule of the form X ⇒ Y where both X and Y are nonempty subsets of I and X ∩ Y = ∅. X is the antecedent of r and Y is called its <mark>consequent</mark>.<br>",
    "Arabic": "نتيجة",
    "Chinese": "后件",
    "French": "conséquent",
    "Japanese": "後件",
    "Russian": "следствие"
  },
  {
    "English": "consistent estimator",
    "context": "1: In this case, an infinite dimensional parameter θ needs to be estimated from finite amount of data. Many problems, such as a practical <mark>consistent estimator</mark>, remain unsolved however (Fukumizu, 2009).<br>2: Theorem 7.μ IPWS is a <mark>consistent estimator</mark> for µ = E[Y | do(x)] if the models for P (x | z, S=1) and P (S=1)/P (S=1 | z T ) are correctly specified.<br>",
    "Arabic": "مقدر متسق",
    "Chinese": "一致估计量",
    "French": "estimateur cohérent",
    "Japanese": "一致推定量",
    "Russian": "согласованный оценщик"
  },
  {
    "English": "constellation model",
    "context": "1: The recognition results presented here convincingly demonstrate the power of the <mark>constellation model</mark> and the associated learning algorithm: the same piece of code performs well (less than 10% error rate) on six diverse object categories presenting a challenging mixture of visual characteristics.<br>2: [18,19] proposed a maximum likelihood unsupervised learning algorithm for the <mark>constellation model</mark> which successfully learns object categories from cluttered data with minimal human intervention. We propose here a number of substantial improvement to the <mark>constellation model</mark> and to its maximum likelihood learning algorithm. First: while Burl et al. and Weber et al.<br>",
    "Arabic": "نموذج كوكبة",
    "Chinese": "星座模型",
    "French": "modèle de constellation",
    "Japanese": "星座モデル",
    "Russian": "модель созвездий"
  },
  {
    "English": "constituency parser",
    "context": "1: To address these issues, recent approaches (Shen et al., 2018b;Drozdov et al., 2019;Kim et al., 2019) design unsupervised <mark>constituency parsers</mark> and grammar inducers, since they can be trained on large-scale unlabeled data.<br>2: Consequently, most high-accuracy <mark>constituency parsers</mark> routinely employ a coarse grammar to prune dynamic programming chart cells * Research conducted at Google. of the final grammar of interest (Charniak et al., 2006;Carreras et al., 2008;Petrov, 2009).<br>",
    "Arabic": "محلل تشكيليات",
    "Chinese": "短语结构分析器",
    "French": "analyseur syntaxique par constituants",
    "Japanese": "構成素パーサー",
    "Russian": "синтаксический анализатор составляющих"
  },
  {
    "English": "constituency parsing",
    "context": "1: Kitaev and Klein (2020) were the first to propose a parsing-as-tagging scheme with a constant tag space for <mark>constituency parsing</mark> and, additionally, the first to achieve results competitive with the stateof-the-art non-parallelizable constituency parsers using such a tagger.<br>2: The goal of this work is fast, high-order, graphbased dependency parsing. Previous work on <mark>constituency parsing</mark> demonstrates that performing several passes with increasingly more complex models results in faster inference (Charniak et al., 2006;Petrov and Klein, 2007).<br>",
    "Arabic": "تحليل التركيب البنيوي",
    "Chinese": "成分句法分析",
    "French": "analyse syntaxique par constituants",
    "Japanese": "構成素解析",
    "Russian": "разбор составляющих"
  },
  {
    "English": "constituency tree",
    "context": "1: Intuitively, a BHT is a special form of a <mark>constituency tree</mark> where each internal node is either labeled L when the head of the derived constituent is in the left subtree or R when the head is in the right subtree. See Fig. 1 for a visual depiction of a BHT.<br>",
    "Arabic": "شجرة الهيئة",
    "Chinese": "成分树",
    "French": "arbre de constituants",
    "Japanese": "構成要素木 (kousei youso ki)",
    "Russian": "дерево составляющих"
  },
  {
    "English": "constituent parsing",
    "context": "1: The quality of bracketings corresponding to (nontrivial) spans derived by heads of our dependency structures is competitive with the state-of-the-art in unsupervised <mark>constituent parsing</mark>. On the WSJ sentences up to length 40 in Section 23 , CS attains similar F 1 -measure ( 54.2 vs. 54.6 , with higher recall ) to System DDA ( @ 10 ) ( Gimpel and Smith , 2012 ) 53.1 ( 64.3 ) ( Gillenwater et al. , 2010 ) 53.3 ( 64.3 ) ( Bisk and Hockenmaier , 2012 ) 53.3 ( 71.5 ) ( Blunsom and Cohn , 2010 ) 55.7 ( 67.7 ) ( Tu and Honavar , 2012 ) 57.0 ( 71.4 ) ( Spitkovsky et al. , 2011b ) 58.4 ( 71.4 ) ( Spitkovsky et al. , 2011c ) 59.1 ( 71.4 ) PRLG ( Ponvert et al. , 2011 ) , which is the strongest system of which we<br>",
    "Arabic": "تحليل المكونات",
    "Chinese": "短语结构分析",
    "French": "analyse syntaxique en constituants",
    "Japanese": "構成素解析",
    "Russian": "составной синтаксический разбор"
  },
  {
    "English": "constituent structure",
    "context": "1: There are numerous heuristic algorithms, some of which have had significant success in inducing <mark>constituent structure</mark> (Klein and Manning, 2004).<br>2: As in TAG approaches, there is a mapping from derivations E, D to parse trees (i.e., the type of trees generated by a context-free grammar). In our case, we map a spine and its dependencies to a <mark>constituent structure</mark> by first handling the dependen-cies on each side separately and then combining the left and right sides.<br>",
    "Arabic": "البنية المكونة",
    "Chinese": "成分结构",
    "French": "structure constituante",
    "Japanese": "構成構造",
    "Russian": "составляющая структура"
  },
  {
    "English": "constrained beam search",
    "context": "1: Constrained beam search (Anderson et al., 2017) and grid beam search (Hokamp and Liu, 2017) extend beam search to satisfy lexical constraints during generation. incorporate logic-based constraints into beam search, which we extend with lookahead heuristics.<br>",
    "Arabic": "البحث الشعاعي المقيد",
    "Chinese": "约束波束搜索",
    "French": "recherche par faisceau contraint",
    "Japanese": "制約付きビーム探索",
    "Russian": "ограниченный лучевой поиск"
  },
  {
    "English": "constrained decoding",
    "context": "1: ArcaneQA is the only open-source baseline that uses <mark>constrained decoding</mark> to enforce the validity of predicted plans. There are two main reasons for Pangu's superiority. First, though <mark>constrained decoding</mark> can also help ensure plan validity, the autoregressive decoder operates with token-level local normalization and thus lacks a global view.<br>2: Within our encoder-decoder framework, this idea is implemented using <mark>constrained decoding</mark> (Liang et al., 2017;Scholak et al., 2021), i.e., at each decoding step, a small set of admissible tokens from the vocabulary is determined based on the decoding history following predefined rules.<br>",
    "Arabic": "فك التشفير المقيد",
    "Chinese": "受约束解码",
    "French": "décodage contraint",
    "Japanese": "制約付きデコーディング",
    "Russian": "ограниченное декодирование"
  },
  {
    "English": "constrained optimization",
    "context": "1: Perhaps even more importantly, the functional representation is particularly well suited for map inference (i.e. <mark>constrained optimization</mark>) for the following reason: when the underlying map T (and by extension the matrix C) are unknown, many natural constraints on the map become linear constraints in its functional representation. Below we describe the most common scenarios.<br>2: The distribution satisfying the maximum causal entropy <mark>constrained optimization</mark> (Equation 4) has a form defined recursively as: \n P θ (At|St) = Z A t |S t ,θ Z S t ,θ(5) \n log \n<br>",
    "Arabic": "تحسين مقيّد",
    "Chinese": "约束优化",
    "French": "optimisation sous contraintes",
    "Japanese": "制約付き最適化",
    "Russian": "ограниченная оптимизация"
  },
  {
    "English": "constrained optimization problem",
    "context": "1: In its more traditional form, the SVM learning problem was described as the following <mark>constrained optimization problem</mark>, \n 1 2 w 2 + C m i=1 ξ i s.t. ∀i ∈ [m] : ξ i ≥ 0, ξ i ≥ 1 − y i w, x i . (14) \n<br>",
    "Arabic": "مشكلة التحسين المقيدة",
    "Chinese": "约束优化问题",
    "French": "problème d'optimisation sous contraintes",
    "Japanese": "制約付き最適化問題",
    "Russian": "задача ограниченной оптимизации"
  },
  {
    "English": "constraint",
    "context": "1: If the state (x, y 1 ) has smaller loss than (x, y 2 ), then a ranking example is generated in the form of the <mark>constraint</mark> H(x, y 1 )<H(x, y 2 ). Ties are broken using a fixed arbitrator.<br>2: During unit propagation on F under ρ, the assignment ρ is extended iteratively by any propagated literals until an assignment ρ ′ is reached under which no <mark>constraint</mark> in F is propagating, or until ρ ′ violates some <mark>constraint</mark> C ∈ F . The latter scenario is referred to as a conflict.<br>",
    "Arabic": "قيد",
    "Chinese": "约束条件",
    "French": "contrainte",
    "Japanese": "制約",
    "Russian": "ограничение"
  },
  {
    "English": "constraint generation",
    "context": "1: Certain LP techniques such as <mark>constraint generation</mark> could potentially extend the range of solvable instances considerably, but this would probably only allow the use of one or two additional buckets per player.<br>",
    "Arabic": "توليد القيود",
    "Chinese": "约束生成",
    "French": "génération de contraintes",
    "Japanese": "制約生成",
    "Russian": "генерация ограничений"
  },
  {
    "English": "constraint programming",
    "context": "1: Interestingly, although symmetries can be broken in different ways in high-level <mark>constraint programming</mark> models (including through lexicographic and value precedence constraints), when we encode the problem in pseudo-Boolean form these differences largely disappear, and after creating a suitable order we can easily re-use the SAT proof logging techniques for symmetry breaking that we discussed above.<br>2: In order to solve the grounding bottleneck problem, several attempts have been made [17], including language extensions (such as <mark>constraint programming</mark> [22,4], difference logic [16,24]) and lazy grounding techniques [12,20,26].<br>",
    "Arabic": "برمجة القيود",
    "Chinese": "约束编程",
    "French": "programmation par contraintes",
    "Japanese": "制約プログラミング",
    "Russian": "программирование с ограничениями"
  },
  {
    "English": "constraint propagation",
    "context": "1: This paper is the first of which we are aware that uses interactive information extraction with <mark>constraint propagation</mark> and confidence prediction to reduce human effort in formfilling.<br>",
    "Arabic": "انتشار القيود",
    "Chinese": "约束传播",
    "French": "propagation de contraintes",
    "Japanese": "制約伝播",
    "Russian": "распространение ограничений"
  },
  {
    "English": "constraint satisfaction",
    "context": "1: As mentioned above, the solver, used in solving the problem, is re-coded from a well-known solver named \"Java Cream\" using Microsoft C# language. The original solver can be used to model any <mark>constraint satisfaction</mark> or optimization on finite domains problems [26]. However, it lacks any proper handling of soft constraints.<br>",
    "Arabic": "تحقيق القيد",
    "Chinese": "约束满足问题",
    "French": "satisfaction des contraintes",
    "Japanese": "制約充足問題",
    "Russian": "удовлетворение ограничений"
  },
  {
    "English": "constraint satisfaction problem",
    "context": "1: Below we provide a summary of our work on this problem. We modelled the Crystal Maze puzzle as a <mark>constraint satisfaction problem</mark> in the natural way: there is a decision variable for each circle, whose values are the possible numbers that can be taken, and an all-different constraint over all decision variables.<br>2: To achieve this, we formulate the reconstruction of individual light paths as a geometric <mark>constraint satisfaction problem</mark> that generalizes the familiar notion of triangulation to the case of indirect projection. Our approach can be thought of as complementing two lines of recent work.<br>",
    "Arabic": "مشكلة إرضاء القيود",
    "Chinese": "约束满足问题",
    "French": "problème de satisfaction de contraintes",
    "Japanese": "制約充足問題",
    "Russian": "проблема удовлетворения ограничений"
  },
  {
    "English": "constraint set",
    "context": "1: We investigate the effects of the robust regularizer with a slightly different perspective in Figure 4, where we use Θ = {θ : θ 1 ≤ r} with r = 100 for the <mark>constraint set</mark> for each experiment.<br>",
    "Arabic": "مجموعة القيد",
    "Chinese": "约束集合",
    "French": "ensemble de contraintes",
    "Japanese": "制約条件集合",
    "Russian": "множество ограничений"
  },
  {
    "English": "content model",
    "context": "1: 5 Typically, sentences in a single-document summary follow the order of appearance in the original document. To produce a length-summary of a new article, the algorithm first uses the <mark>content model</mark> and Viterbi decoding to assign each of the article's sentences a V-topic.<br>2: We can use the forward algorithm to efficiently compute the generation probability assigned to a document by a <mark>content model</mark> and the Viterbi algorithm to quickly find the most likely contentmodel state sequence to have generated a given document; see Rabiner (1989) for details.<br>",
    "Arabic": "نموذج المحتوى",
    "Chinese": "内容模型",
    "French": "modèle de contenu",
    "Japanese": "コンテンツモデル",
    "Russian": "модель содержания"
  },
  {
    "English": "content selection",
    "context": "1: For example, in Figure 1 the first sentence is updated to reflect that Tom Kristensson now races in a different competition, whereas new sentences are added describing his achievements in the years 2019 and 2020. Models must also be able to determine whether a given piece of evidence should be used at all, i.e., perform <mark>content selection</mark>.<br>",
    "Arabic": "اختيار المحتوى",
    "Chinese": "内容选择",
    "French": "sélection de contenu",
    "Japanese": "コンテンツ選択",
    "Russian": "подбор контента"
  },
  {
    "English": "context encoder",
    "context": "1: By alternating between an internal <mark>context encoder</mark> and an external attentive encoder, MEAN leverages 3D message passing along with equivariant attention mechanism to capture long-range and spatially-complicated interactions between different components of the input complex. 3. Efficient Prediction.<br>",
    "Arabic": "مشفّر السياق",
    "Chinese": "上下文编码器",
    "French": "codeur de contexte",
    "Japanese": "コンテキストエンコーダ",
    "Russian": "контекстный кодировщик"
  },
  {
    "English": "context free grammar",
    "context": "1: One of the reasons we need to use a context sensitive representation, is so that we can consider every possible combination of contexts simultaneously: this would require an exponentially large <mark>context free grammar</mark>.<br>",
    "Arabic": "قواعد النحو الخالية من السياق",
    "Chinese": "上下文无关文法",
    "French": "grammaire hors contexte",
    "Japanese": "コンテキストフリー文法",
    "Russian": "бесконтекстная грамматика"
  },
  {
    "English": "context model",
    "context": "1: Once the context units are selected, the remaining task is to assign a weight to each dimension of the <mark>context model</mark>, which represents how strong the context unit corresponding to this dimension indicates the meaning of a given pattern.<br>2: Although they explored some kind of context information, none of the work can provide in-depth semantic annotations for frequent patterns as we do in our work. The <mark>context model</mark> proposed in our work covers both the pattern profile in [21] and transaction coverage in [20] as special cases.<br>",
    "Arabic": "نموذج السياق",
    "Chinese": "上下文模型",
    "French": "modèle contextuel",
    "Japanese": "コンテキストモデル",
    "Russian": "модель контекста"
  },
  {
    "English": "context vector",
    "context": "1: The use of a large bilingual <mark>context vector</mark>, which is provided to the neural network in \"raw\" form, rather than as the output of some other algorithm. 3. The fact that the model is purely lexicalized, which avoids both data sparsity and implementation complexity. 4. The large size of the network architecture. 5.<br>2: The attention mechanism produces a <mark>context vector</mark> by focusing on different portions of the text sequence and aggregating the hidden representations of those informative words. Specially, the attention mechanism assigns the weight α ti to the i-th word at time-step t as follows: \n<br>",
    "Arabic": "متجه السياق",
    "Chinese": "上下文向量",
    "French": "vecteur contextuel",
    "Japanese": "コンテクストベクトル",
    "Russian": "контекстный вектор"
  },
  {
    "English": "context window",
    "context": "1: Transformer-based models can look back to any token in the entire input sequence within their <mark>context window</mark>, so a proper comparison between humans and models would be to present humans with the full description in written form and let them re-read the description after being prompted to state the contents of a box.<br>2: • It requires a gigantic LM to work well, making it unusable in many real-world scenarios and resulting in a large carbon footprint (Strubell et al., 2019). • It does not scale to more than a few examples as the <mark>context window</mark> of most LMs is limited to a few hundred tokens.<br>",
    "Arabic": "نافذة السياق",
    "Chinese": "上下文窗口",
    "French": "fenêtre de contexte",
    "Japanese": "コンテキストウィンドウ",
    "Russian": "окно контекста"
  },
  {
    "English": "context-free language",
    "context": "1: Natural languages, for the most part, can be modeled by <mark>context-free languages</mark> (Jger and Rogers, 2012) and their hierarchical structure has been emphasized by Chomsky (2002). Thus studying the capabilities of RNNs in recognizing <mark>context-free languages</mark> (CFLs) can shed light on how well they can model hierarchical structures.<br>2: Our permutation model is very expressive and is not limited to synchronous <mark>context-free languages</mark>. This is in contrast to the formalisms that other approaches rely on Lindemann et al., 2023).<br>",
    "Arabic": "لغة خالية من السياق",
    "Chinese": "上下文无关语言",
    "French": "langage sans contexte",
    "Japanese": "文脈自由言語",
    "Russian": "контекстно-свободный язык"
  },
  {
    "English": "contextual embedding",
    "context": "1: For complete details about the features of the library refer to the documentation available on https: //huggingface.co/transformers/. Every model in the library is fully defined by three building blocks shown in the diagram in Figure 2 : ( a ) a tokenizer , which converts raw text to sparse index encodings , ( b ) a transformer , which transforms sparse indices to <mark>contextual embeddings</mark> , and ( c ) a head , which uses <mark>contextual embeddings</mark> to make a task-specific<br>2: They often have no official status in the regions where they are spoken, and therefore do not have concerted funding efforts for data collection or research. Even when such efforts do exist, 3 the collected corpora are rarely of the magnitude at which static or <mark>contextual embeddings</mark> can be well-estimated.<br>",
    "Arabic": "التضمين السياقي",
    "Chinese": "语境嵌入",
    "French": "représentation contextuelle",
    "Japanese": "文脈埋め込み",
    "Russian": "контекстное встраивание"
  },
  {
    "English": "contextual feature",
    "context": "1: For the spine features e(x, i, η ), we use feature templates that are sensitive to the identity of the spine η, together with <mark>contextual features</mark> of the string x.<br>2: These models are difficult to extend with arbitrary overlapping features that can help improve accuracy. In this work we incorporate novel overlapping <mark>contextual features</mark> and show that they greatly improve performance.<br>",
    "Arabic": "ميزة سياقية",
    "Chinese": "上下文特征",
    "French": "caractéristique contextuelle",
    "Japanese": "文脈特徴",
    "Russian": "контекстная особенность"
  },
  {
    "English": "contextual information",
    "context": "1: 5.3 explores the different types of <mark>contextual information</mark> captured in biLMs and uses two intrinsic evaluations to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders. It also shows that our biLM consistently provides richer representations then CoVe.<br>2: Compared to existing object recognition approaches that require training for each object category, our system is easy to implement, has few parameters, and embeds <mark>contextual information</mark> naturally in the retrieval/alignment procedure.<br>",
    "Arabic": "معلومات سياقية",
    "Chinese": "上下文信息",
    "French": "informations contextuelles",
    "Japanese": "文脈情報",
    "Russian": "контекстная информация"
  },
  {
    "English": "contextual model",
    "context": "1: The aim of the experiments is to estimate the contribution of the proposed <mark>contextual models</mark> to the accuracy reachable in different scenarios, whereas rich contexts (e.g. popular hashtags) are possibly made available or just singleton tweets, with no context, are targeted.<br>",
    "Arabic": "نموذج سياقي",
    "Chinese": "上下文模型",
    "French": "modèle contextuel",
    "Japanese": "文脈モデル (Contextual Model)",
    "Russian": "контекстуальная модель"
  },
  {
    "English": "contextual representation",
    "context": "1: State-of-the-art neural models are highly effective at exploiting such artifacts to solve problems correctly, but for incorrect reasons. To tackle this persistent challenge with dataset biases, we propose AFLITEa novel algorithm that can systematically reduce biases using state-of-the-art <mark>contextual representation</mark> of words.<br>2: To prepare the input, we first obtain each mention's <mark>contextual representation</mark> from a pre-trained embedding model, and then concatenate the mention's crowd labels with its <mark>contextual representation</mark>. This concatenated vector is used as input to the complexity layer, which computes the instance complexity.<br>",
    "Arabic": "تمثيل سياقي",
    "Chinese": "上下文表示",
    "French": "représentation contextuelle",
    "Japanese": "文脈表現",
    "Russian": "контекстное представление"
  },
  {
    "English": "contextual vector",
    "context": "1: Given ELMo or any other black-box conversion of a length-n sentence to a sequence of <mark>contextual vectors</mark> x 1 , . . . , x n , it is possible that x i contains not only information about word i but also information describing word i + 1, say, or the syntactic constructions in the vicinity of word i.<br>",
    "Arabic": "المتجهات السياقية",
    "Chinese": "上下文向量",
    "French": "vecteur contextuel",
    "Japanese": "コンテクストベクトル",
    "Russian": "контекстный вектор"
  },
  {
    "English": "contextual word embedding",
    "context": "1: This helps with matching morphological variants of words. Best metrics across language pairs • YISI-1 (Lo, 2019) computes the semantic similarity of phrases in the MT output with the reference, using <mark>contextual word embeddings</mark> (BERT: Devlin et al. (2019)).<br>",
    "Arabic": "تضمين الكلمات السياقية",
    "Chinese": "上下文词嵌入",
    "French": "plongement de mots contextuels",
    "Japanese": "文脈依存単語埋め込み",
    "Russian": "контекстное вложение слов"
  },
  {
    "English": "contextualize embedding",
    "context": "1: The input c t for the next step is obtained via the concatenation of the <mark>contextualized embedding</mark> of the current output token and the weighted representation of the question based on attention: \n a t = softmax(Q t h t )(4) \n<br>",
    "Arabic": "التضمين السياقي",
    "Chinese": "上下文嵌入",
    "French": "intégration contextualisée",
    "Japanese": "文脈化埋め込み",
    "Russian": "контекстуализированное встраивание"
  },
  {
    "English": "contextualize representation",
    "context": "1: A pre-trained language model like BERT can jointly encode the question and schema items to get the <mark>contextualized representation</mark> at each step, which further guides the search process. where the underlying data is moderate-sized, the massive scale and the broad-coverage schema of KBs makes KBQA a uniquely challenging setting for semantic parsing research.<br>2: the <mark>contextualized representation</mark> . (3) Loss: We calculate loss over all tokens in an auto-regressive manner, while they only calculate over the masked tokens non-autoregressively.<br>",
    "Arabic": "تمثيل سياقي",
    "Chinese": "上下文表示",
    "French": "représentation contextualisée",
    "Japanese": "文脈化された表現",
    "Russian": "контекстуализированное представление"
  },
  {
    "English": "contextualize word vector",
    "context": "1: Building on the success of preceding neural parsers (Chen and Manning, 2014;Kiperwasser and Goldberg, 2016), Dozat and Manning (2017) proposed a biaffine parsing head on top of a Bi-LSTM encoder: <mark>contextualized word vectors</mark> are fed to two feedforward networks, producing dependent-and headspecific token representations, respectively.<br>2: ) . Hewitt and Manning (2019) find that linear transformations, when applied on BERT's <mark>contextualized word vectors</mark>, reflect distances in dependency trees. This suggests that BERT encodes sufficient structural information to reconstruct dependency trees (though without arc directionality and relations). Chi et al.<br>",
    "Arabic": "متجه كلمة موضح السياق",
    "Chinese": "上下文词向量",
    "French": "vecteur de mots contextualisé",
    "Japanese": "文脈化単語ベクトル",
    "Russian": "контекстуализованный векторное представление слова"
  },
  {
    "English": "contingency table",
    "context": "1: Bayesian Networks or Belief Networks (BNs) are directed graphical models (e.g., see [Cowell et al., 1999] for an overview): the BN structure is a directed acyclic graph (DAG), and the BN parameters are conditional probabilities, represented by a <mark>contingency table</mark> in case of discrete variables.<br>",
    "Arabic": "جدول الاحتمالات المشروط",
    "Chinese": "条件概率表",
    "French": "tableau de contingence",
    "Japanese": "分割表",
    "Russian": "таблица сопряженности"
  },
  {
    "English": "continual learning",
    "context": "1: After each action, the agent receives a new observation. Suhr et al. (2019) studied this problem with CEREALBAR by learning from recorded human-human interactions, and Suhr and Artzi (2022) studied it within a <mark>continual learning</mark> from human feedback scenario. Both approaches were evaluated by deploying follower agents to interact with human leaders.<br>2: Lastly, several approaches have been proposed for <mark>continual learning</mark> in the machine learning community (Kirkpatrick et al., 2017;Lopez-Paz et al., 2017;Rusu et al., 2016;Fernando et al., 2017;, especially in image recognition tasks Rannen et al., 2017).<br>",
    "Arabic": "التعلم المستمر",
    "Chinese": "持续学习",
    "French": "apprentissage continu",
    "Japanese": "継続学習",
    "Russian": "непрерывное обучение"
  },
  {
    "English": "continuous normalize flow",
    "context": "1: We introduce Moser Flow (MF), a new class of generative models within the family of <mark>continuous normalizing flows</mark> (CNF).<br>2: We call these models <mark>continuous normalizing flows</mark> (CNF).<br>",
    "Arabic": "تدفق التطبيع المستمر",
    "Chinese": "连续归一化流",
    "French": "écoulement normalisé continu",
    "Japanese": "連続正規化フロー (CNF)",
    "Russian": "непрерывные нормализующие потоки (CNF)"
  },
  {
    "English": "contrastive approach",
    "context": "1: Recent approaches in self-supervised learning of image representations can be categorized into different families of methods and, in particular, can be divided into contrastive and non-<mark>contrastive approaches</mark>. While differences between the two families have been thoroughly discussed to motivate new approaches, we focus more on the theoretical similarities between them.<br>",
    "Arabic": "نهج تباينيّ",
    "Chinese": "对比学习方法",
    "French": "approche contrastive",
    "Japanese": "対照的アプローチ",
    "Russian": "контрастный подход"
  },
  {
    "English": "contrastive fine-tuning",
    "context": "1: We set the size of the reference game context to k = 10 throughout our experiments. During <mark>contrastive fine-tuning</mark>, we create a text-image matching matrix of size k×k for each generated reference game in our training data by randomly selecting a text description for each tangram distractor from its annotations.<br>2: LiT also increased training scale and experimented with a combination of pre-trained image representations and <mark>contrastive fine-tuning</mark> to connect frozen image representations to text [94]. Flamingo introduced the first large vision-language model with in-context learning [2]. Other papers have combined contrastive losses with image captioning to further improve performance [43,89].<br>",
    "Arabic": "ضبط دقيق تباينيّ",
    "Chinese": "对比式微调",
    "French": "réglage fin contrastif",
    "Japanese": "対照的な微調整",
    "Russian": "контрастная точная настройка"
  },
  {
    "English": "contrastive loss",
    "context": "1: While self-supervised representations are also effective at reducing baseline error, we achieve the best classifier performance using TREBA with programmed tasks (Table 2). Furthermore, we found that training trajectory representations without self-decoding, using the <mark>contrastive loss</mark> from [7,8], resulted in less effective representations for classification (Supplementary Material). Data Augmentation.<br>2: It motivates us to interpret PiCO through the lens of the expectationmaximization algorithm. To see this, we consider an ideal setting: in each training step, all data examples are accessible and the augmentation copies are also included in the training set, i.e., A = D. Then, the <mark>contrastive loss</mark> is calculated as, \n Lcont ( g ; τ , D ) = 1 n x∈D      − 1 |P ( x ) | k + ∈P ( x ) log exp ( q k+/τ ) k ∈A ( x ) exp ( q k /τ )      = 1 n x∈D    − 1 |P ( x<br>",
    "Arabic": "الخسارة التباينية",
    "Chinese": "对比损失",
    "French": "perte contrastive",
    "Japanese": "コントラスティブ損失 (contrastive loss)",
    "Russian": "потеря контраста"
  },
  {
    "English": "contrastive objective",
    "context": "1: CLIP slightly outperforms ViLT throughout, potentially because it is trained with a <mark>contrastive objective</mark> similar to a reference game. Whereas ViLT's matching loss is aligned with our goal, it is only one of several losses in its objective. We observe no reliable improvement from adding part information, either textual or visual.<br>2: Reimers and Gurevych [40] do this by using a <mark>contrastive objective</mark> on a dataset of semantically-paired text where embeddings from the same pair are pushed closer and embeddings from different pairs are pushed further apart.<br>",
    "Arabic": "الهدف التبايني",
    "Chinese": "对比目标",
    "French": "objectif contrastif",
    "Japanese": "コントラスティブオブジェクティブ",
    "Russian": "контрастивная цель"
  },
  {
    "English": "control variate",
    "context": "1: Gradients of the entropy of an implicit model have been estimated with an amortized score model at a single noise level (Lim et al., 2020), though that work does not use our <mark>control variate</mark> based on subtracting the noise fromˆ . SDS could also ease optimization by using multiple noise levels.<br>2: Since E qη [∇ η log q η (x)] = 0, the REINFORCE estimator is unbiased for any choice of b, and the term b∇ η log q η (x) is known as a <mark>control variate</mark> (CV) [42,Ch. 8].<br>",
    "Arabic": "مُتغيِّرٌ ضابِطٌ",
    "Chinese": "控制变量",
    "French": "variable de contrôle",
    "Japanese": "制御変量 (Control Variate)",
    "Russian": "контрольная переменная"
  },
  {
    "English": "controllable text generation",
    "context": "1: Text rewriting is a broad subarea in natural language processing that includes tasks such as style transfer [31,61], content debiasing [39,51], and <mark>controllable text generation</mark> [13,24,40]. We propose empathic rewriting as a new text rewriting task in which conversational utterances are rewritten for increasing them in empathy (Section 4).<br>",
    "Arabic": "إنتاج نص قابل للتحكم",
    "Chinese": "可控文本生成",
    "French": "génération de texte contrôlable",
    "Japanese": "制御可能なテキスト生成",
    "Russian": "контролируемая генерация текста"
  },
  {
    "English": "conv layer",
    "context": "1: Taking an input feature map with C channels from a given pyramid level, the subnet applies four 3×3 <mark>conv layers</mark>, each with C filters and each followed by ReLU activations, followed by a 3×3 conv layer with KA filters.<br>2: We consider these layers as analogous to the 13 <mark>conv layers</mark> in VGG-16, and by doing so, both ResNet and VGG-16 have conv feature maps of the same total stride (16 pixels).<br>",
    "Arabic": "طبقة تراكبية",
    "Chinese": "卷积层",
    "French": "couche de convolution",
    "Japanese": "畳み込み層",
    "Russian": "сверточный слой"
  },
  {
    "English": "convergence",
    "context": "1: Although our algorithm is relatively simple, its stochastic nature and the non-convexity of the objective function make the proof of its <mark>convergence</mark> to a stationary point somewhat involved.<br>2: In this subsection we present a claim to bound the \"<mark>convergence</mark>\" (namely, the 1 − logit y F (t) , X part) for every single-view data from T 0 till the end. Claim D.12 (single view till the end).<br>",
    "Arabic": "تقارب",
    "Chinese": "收敛",
    "French": "convergence",
    "Japanese": "収束",
    "Russian": "сходимость"
  },
  {
    "English": "convergence analysis",
    "context": "1: By contrast, classic <mark>convergence analysis</mark> (see Appendix B) show that the convergence of the Markov Chain is exponentially fast, but does not give measures of power of sub-kernels.<br>2: One key aspect of the <mark>convergence analysis</mark> will be to show that f t (D t ) and f t (D t ) converges almost surely to the same limit and thusf t acts as a surrogate for f t . • Sincef t is close tof t−1 , D t can be obtained efficiently using D t−1 as warm restart.<br>",
    "Arabic": "تحليل التقارب",
    "Chinese": "收敛分析",
    "French": "analyse de convergence",
    "Japanese": "収束解析",
    "Russian": "анализ сходимости"
  },
  {
    "English": "convergence bound",
    "context": "1: Note that similarly to Theorem 3, one could obtain <mark>convergence bounds</mark> for the discrete implementation under the presence of additive noise.<br>",
    "Arabic": "حد التقارب",
    "Chinese": "收敛界限",
    "French": "borne de convergence",
    "Japanese": "収束境界",
    "Russian": "граница сходимости"
  },
  {
    "English": "convergence criterion",
    "context": "1: We compare the predictive power of three 20-topic models: the dynamic topic model estimated from all of the previous years, a static topic model estimated from all of the previous years, and a static topic model estimated from the single previous year. All the models are estimated to the same <mark>convergence criterion</mark>.<br>2: For each experiment, the bisection method is used to determine the minimal population size for the algorithm to obtain the optimal solution (with 100% correct bits). The <mark>convergence criterion</mark> is when the proportion of a certain value on each position reaches 99%.<br>",
    "Arabic": "معيار التقارب",
    "Chinese": "收敛准则",
    "French": "critère de convergence",
    "Japanese": "収束基準",
    "Russian": "критерий сходимости"
  },
  {
    "English": "convergence rate",
    "context": "1: The proof appears in Appendix D. Using the spectral gap bound on the <mark>convergence rate</mark> of random walks (see the proof of Corollary 7), ε t can be chosen as: \n ε t = (1 − α) t η min , \n<br>2: We assume without loss of generality that the value of the constant feature is 1. Once the constant feature is added the rest of the algorithm remains intact, thus the bias term is not explicitly introduced. The analysis can be repeated verbatim and we therefore obtain the same <mark>convergence rate</mark> for this modification.<br>",
    "Arabic": "معدل التقارب",
    "Chinese": "收敛速率",
    "French": "taux de convergence",
    "Japanese": "収束率",
    "Russian": "скорость сходимости"
  },
  {
    "English": "convergence time",
    "context": "1: Motivated by this, we next investigate whether the <mark>convergence time</mark> of neural networks trained on pruned datasets depends largely on the number of examples and not their difficulty, potentially allowing for exponential compute savings. We investigate the learning curves of a ResNet18 trained on CIFAR-10 and a ResNet50 on ImageNet for several different pruning fractions (Fig. 8B).<br>",
    "Arabic": "وقت التقارب",
    "Chinese": "收敛时间",
    "French": "temps de convergence",
    "Japanese": "収束時間",
    "Russian": "время сходимости"
  },
  {
    "English": "conversation history",
    "context": "1: Thus, here we consider a threat model during the inference stage where if a user inputs privacy-sensitive information in the <mark>conversation history</mark> [134,51], other users may extract the private information by querying the model under the same context. Data. Here we focus on the personally identifiable information (PII).<br>",
    "Arabic": "تاريخ المحادثة",
    "Chinese": "对话历史记录",
    "French": "historique de conversation",
    "Japanese": "会話履歴",
    "Russian": "история разговоров"
  },
  {
    "English": "conversational agent",
    "context": "1: However, collecting diverse interaction data is prohibitively expensive or infeasible in many real-world applications such as robotics, healthcare, and <mark>conversational agents</mark>. Due to these problems' risk-sensitive nature, data can only be collected by behavior policies that satisfy certain baseline performance or safety requirements.<br>2: Moreover, QA systems in industry, including answer snippet generation on search engine results pages (SERPs) or <mark>conversational agents</mark>, are still unlikely to be able to meaningfully answer NFQs such as \"If scientifically possible, should humans become immortal?\".<br>",
    "Arabic": "وكيل محادثة",
    "Chinese": "对话代理",
    "French": "agent conversationnel",
    "Japanese": "対話エージェント",
    "Russian": "разговорный агент"
  },
  {
    "English": "conversational dialogue system",
    "context": "1: In this work, we introduced Spot The Bot, a robust and time-efficient approach for evaluating <mark>conversational dialogue systems</mark>. It is based on conversations between bots rated by humans with respect to the bots' ability to mimic human behavior.<br>2: This task is a promising one for <mark>conversational dialogue systems</mark> and is being developed at several companies such as Microsoft [1] and Google 1 . We have developed a method for identifying the user's referent during system enumeration by focusing on barge-in utterances while the system lists choices [2].<br>",
    "Arabic": "نظام الحوار التحادثي",
    "Chinese": "交互式对话系统",
    "French": "système de dialogue conversationnel",
    "Japanese": "会話型対話システム",
    "Russian": "разговорная диалоговая система"
  },
  {
    "English": "convex",
    "context": "1: conv (γ i 1/3 α i ) ≤ s i ≤ conc (γ i 1/3 α i ) \n As shown in Appendix A.3, our <mark>convex</mark> and concave relaxations are piecewise linear and representable using a small set of linear inequalities.<br>2: The claimed result follows by applying Jensen's inequality to swap the summation and the <mark>convex</mark> max x function, noting that the inequality works out the right way for both positive and negative α. \n<br>",
    "Arabic": "محدب",
    "Chinese": "凸的",
    "French": "convexe",
    "Japanese": "凸",
    "Russian": "выпуклый"
  },
  {
    "English": "convex combination",
    "context": "1: section . By studying this vast class of weak-learning conditions, we hope to find the one that will serve the main purpose of the boosting game: finding a <mark>convex combination</mark> of weak classifiers that has zero training error.<br>2: In this work we generalize the compositing equation by assuming that each pixel is a <mark>convex combination</mark> of K image layers F 1 , . . . , F K : \n I i = K ∑ k=1 α k i F k i . (2) \n<br>",
    "Arabic": "مزيج محدب",
    "Chinese": "凸组合",
    "French": "combinaison convexe",
    "Japanese": "凸結合",
    "Russian": "линейная комбинация"
  },
  {
    "English": "convex conjugate",
    "context": "1: AA = {v,w}∈E W {v,w} = L. \n Then, introducing Lagrange multipliers λ, we obtain through Lagrangian duality that Problem ( 53) is equivalent to: max λ∈R E×d −F * (Aλ), \n with F * the <mark>convex conjugate</mark> of F . Following the approach of Hendrikx et al.<br>",
    "Arabic": "مترافق محدب",
    "Chinese": "凸共轭",
    "French": "conjugué convexe",
    "Japanese": "凸共役",
    "Russian": "выпуклое сопряжение"
  },
  {
    "English": "convex constraint",
    "context": "1: We address this by requiring that for any utility function linear in known features, our learned model must have no more regret than that of the observed behavior. We demonstrate that this requirement can be re-cast as a set of equivalent <mark>convex constraints</mark> that we denote the inverse correlated equilibrium (ICE) polytope.<br>",
    "Arabic": "قيد محدب",
    "Chinese": "凸约束",
    "French": "contrainte convexe",
    "Japanese": "凸制約",
    "Russian": "выпуклое ограничение"
  },
  {
    "English": "convex decomposition",
    "context": "1: For each OOD convex domain D XY ∈ D XY corresponding to OOD <mark>convex decomposition</mark> Q 1 , ..., Q l , the following function \n f D , Q ( α 1 , ... , α l ) : = inf h∈H ( 1 − l j=1 α j ) R in D ( h ) + l j=1 α j R Qj ( h ) , ∀ ( α 1 , ... , α l ) ∈ ∆ o l satisfies that f D , Q ( α<br>2: Suppose that D XY is a domain with OOD <mark>convex decomposition</mark> Q 1 , ..., Q l (<mark>convex decomposition</mark> is given by Definition 6 in Appendix F), and D XY is a finite discrete distribution, then (the definition of f D,Q is given in Condition 4) \n f D , Q ( α 1 , ... , α l ) = ( 1 − l j=1 α j ) f D , Q ( 0 ) + l j=1 α j f D , Q ( α j ) , ∀ ( α 1 , ... , α l ) ∈ ∆ o l , if and only if arg min<br>",
    "Arabic": "تحليل محدب",
    "Chinese": "凸分解",
    "French": "décomposition convexe",
    "Japanese": "凸分解",
    "Russian": "выпуклое разложение"
  },
  {
    "English": "convex function",
    "context": "1: To remind the reader, given a <mark>convex function</mark> f (w), a sub-gradient of f at w 0 is a vector v which satisfies: \n ∀w, f (w) − f (w 0 ) ≥ v, w − w 0 .<br>2: (2) in the Log-sum-exp network (Calafiore, Gaubert, and Possieri 2018) and the further analysis, the function f i (K) is convex. Moreover, f i (K) add a linear combination of k j makes the M (q i , K) to be the <mark>convex function</mark> for a fixed query.<br>",
    "Arabic": "دالة محدبة",
    "Chinese": "凸函数",
    "French": "fonction convexe",
    "Japanese": "凸関数",
    "Russian": "выпуклая функция"
  },
  {
    "English": "convex hull",
    "context": "1: But since the approach is limited to finding points which intersect with tangents to the solution set, it is limited to finding at most points in the <mark>convex hull</mark> of the solution set.<br>2: The previous section showed that it is possible to force an embedding to preserve the graph structure in a given adjaceny matrix A by introducing a set of linear constraints on the embedding Gram matrix K. To choose a unique K from the admissible set in the <mark>convex hull</mark> generated by these linear constraints , we propose an objective function which favors lowdimensional embeddings<br>",
    "Arabic": "الغلاف المحدب",
    "Chinese": "凸包",
    "French": "enveloppe convexe",
    "Japanese": "凸包",
    "Russian": "выпуклая оболочка"
  },
  {
    "English": "convex loss",
    "context": "1: conjectured that no <mark>convex loss</mark> can be calibrated with the PD in general [11,Section 2.1] because it would provide a polynomial algorithm to solve an NP-hard problem. Our approach thus leads to a direct proof of this conjecture. In the next section, we describe our framework for learning to rank.<br>",
    "Arabic": "خسارة محدبة",
    "Chinese": "凸损失",
    "French": "perte convexe",
    "Japanese": "凸損失",
    "Russian": "выпуклая функция потерь"
  },
  {
    "English": "convex objective",
    "context": "1: [Byl94] showed that a variant of the Perceptron algorithm (which can be viewed as gradient descent on a particular <mark>convex objective</mark>) learns γ-margin halfspaces in poly(d, 1/ , 1/γ) time. The algorithm in [Byl94] requires an additional anti-concentration condition about the distribution, which is easy to remove.<br>2: Additionally, the optimal predictions for all leaves of our trees given the split decisions can be obtained by minimizing a <mark>convex objective</mark> and we provide an optimization algorithm for it that does not resort to tedious step-size selection.<br>",
    "Arabic": "هدف محدب",
    "Chinese": "凸目标函数",
    "French": "objectif convexe",
    "Japanese": "凸目的関数",
    "Russian": "выпуклый критерий"
  },
  {
    "English": "convex objective function",
    "context": "1: While it is possible to derive a closed form solution for this <mark>convex objective function</mark>, it would require the inversion of a matrix of order |V f |. Instead, we resort to an iterative update based method. We formulate the update as follows: \n<br>2: The matrices P S and P T ∈ R DxL with L = t c x ct represent all assignments, where the columns denote the actual associations. The quadratic nature of the <mark>convex objective function</mark> may be seen as a linear least squares problem, which can be easily solved by any available QP solver.<br>",
    "Arabic": "دالة هدف محدبة",
    "Chinese": "凸目标函数",
    "French": "fonction objectif convexe",
    "Japanese": "凸目的関数",
    "Russian": "выпуклая целевая функция"
  },
  {
    "English": "convex optimization",
    "context": "1: The constraint functions, g and h, are typically chosen to capture the important or most salient characteristics of the distribution. When those functions are affine and convex respectively, finding this distribution is a <mark>convex optimization</mark> problem.<br>2: Since this <mark>convex optimization</mark> problem admits separable constraints in the updated blocks (columns), convergence to a global optimum is guaranteed (Bertsekas, 1999). In practice, since the vectors α i are sparse, the coefficients of the matrix A are in general concentrated on the diagonal, which makes the block-coordinate descent more efficient.<br>",
    "Arabic": "تحسين محدب",
    "Chinese": "凸优化",
    "French": "optimisation convexe",
    "Japanese": "凸最適化",
    "Russian": "выпуклая оптимизация"
  },
  {
    "English": "convex optimization problem",
    "context": "1: We have proposed a technique for incorporating indefinite kernels into the SVM framework without any explicit transformations. We have shown that if we view the indefinite kernel as a noisy instance of a true kernel, we can learn an explicit solution for the optimal kernel with a tractable <mark>convex optimization problem</mark>.<br>2: We now describe our Hankel matrix completion algorithm. Given a basis B = (P, S) of Σ and a sample Z over Σ × R, the algorithm solves a <mark>convex optimization problem</mark> and returns a matrix H Z ∈ H B .<br>",
    "Arabic": "مشكلة تحسين محدبة",
    "Chinese": "凸优化问题",
    "French": "problème d'optimisation convexe",
    "Japanese": "凸最適化問題",
    "Russian": "проблема выпуклой оптимизации"
  },
  {
    "English": "convex problem",
    "context": "1: By taking the dual of the maximization (29), this is an efficiently solvable <mark>convex problem</mark>; for completeness, we provide a procedure for this computation in Section H that requires time O(n log n + log 1 ǫ log n) to compute an ǫ-accurate solution to the maximization (29).<br>2: Using the usual definition of distance between two spaces, d(H x , H y ) = inf{ z−w 2 |/ (z, w) ∈ H x ×H y }, we obtain the solution for this <mark>convex problem</mark> by solving a system of linear equations (Simard et al., 1993).<br>",
    "Arabic": "مشكلة محدبة",
    "Chinese": "凸问题",
    "French": "problème convexe",
    "Japanese": "凸問題",
    "Russian": "выпуклая задача"
  },
  {
    "English": "convex program",
    "context": "1: Given these bounds, we can construct convex and concave envelopes of the non-linear functions, and use them to construct the following <mark>convex program</mark> that underestimates the minimum of the above problem. min v1,v2,v3 r (17) s.t. re ≥ j (f j − g j ) 2 , j = 1, . .<br>2: For a broad class of utility functions, they show that a core solution can be found in polynomial time by solving a suitable <mark>convex program</mark>. They also use differential privacy to design a mechanism for this setting which satisfies approximate versions of efficiency, truthfulness, and the core.<br>",
    "Arabic": "برنامج محدب",
    "Chinese": "凸优化问题",
    "French": "programme convexe",
    "Japanese": "凸計画",
    "Russian": "выпуклая программа"
  },
  {
    "English": "convex proxy",
    "context": "1: As mentioned already, a <mark>convex proxy</mark> is guaranteed to converge to the true solution with RCN, but the convergence may be too slow (when the margin is tiny). In contrast, with Massart noise (even under a margin condition) convex surrogates cannot even give weak learning in the entire domain.<br>2: In particular, we show that the approach of iteratively using any <mark>convex proxy</mark> followed by thresholding gets stuck at error Ω(η)+ , even in the large margin case.<br>",
    "Arabic": "بديل محدب",
    "Chinese": "凸代理",
    "French": "approximation convexe",
    "Japanese": "凸近似",
    "Russian": "выпуклый заместитель"
  },
  {
    "English": "convex quadratic program",
    "context": "1: Thus the above optimization problem is a <mark>convex quadratic program</mark> that can be solved using a QP or an SOCP solver. Given bounds on {p 1 , p 2 , p 3 }, a branch and bound algorithm can now be used to obtain a global minimum to the modulus constraints.<br>",
    "Arabic": "برنامج تربيعي محدب",
    "Chinese": "凸二次规划",
    "French": "programme quadratique convexe",
    "Japanese": "凸二次計画問題",
    "Russian": "выпуклая квадратичная программа"
  },
  {
    "English": "convex relaxation",
    "context": "1: The above proposition reveals that the <mark>convex relaxation</mark> implicitly convexifies the dataterm ρ on each interval Γ i . The equality ρ * i = ρ * * * i implies that starting with ρ i yields exactly the same <mark>convex relaxation</mark> as starting with ρ * * i . Corollary 1.<br>2: This is a first step but further approximations are needed to solve the problem. For instance (Jiang & Li, 2007) also relies on an additional <mark>convex relaxation</mark> before solving the problem with semi definite programming. At the end the impact of successive approximations and simplifications is difficult to measure and the robustness of the method is questionable.<br>",
    "Arabic": "استرخاء محدب",
    "Chinese": "凸松弛化",
    "French": "relaxation convexe",
    "Japanese": "凸緩和",
    "Russian": "выпуклая релаксация"
  },
  {
    "English": "convex risk minimization",
    "context": "1: . Mammen and Tsybakov [32,44] give low noise conditions for binary classification substantially generalizing these results, which yield a spectrum of fast rates. Under related conditions, Bartlett, Jordan, and McAuliffe [4] show similar fast rates of convergence for <mark>convex risk minimization</mark> under appropriate curvature conditions on the loss.<br>",
    "Arabic": "تدنية المخاطر المحدبة",
    "Chinese": "凸风险最小化",
    "French": "minimisation du risque convexe",
    "Japanese": "凸リスク最小化",
    "Russian": "Минимизация выпуклого риска"
  },
  {
    "English": "convex set",
    "context": "1: Here we are maximizing a convex function over a <mark>convex set</mark>. Therefore, maximization is achieved on the boundary of the <mark>convex set</mark>. That is, the maximum is ε log ε; consequently, the minimum value of H ′ (α) = ε log(1/ε). Therefore, it follows that for \n<br>2: Consider a strictly convex function F : S → defined on a <mark>convex set</mark> S ⊂ d .<br>",
    "Arabic": "مجموعة محدبة",
    "Chinese": "凸集",
    "French": "ensemble convexe",
    "Japanese": "凸集合",
    "Russian": "выпуклое множество"
  },
  {
    "English": "convex surrogate",
    "context": "1: Unfortunately, this is not the case for Massart noise. We show (Theorem 3.1 in Appendix 3) that no <mark>convex surrogate</mark> can lead to a weak learner, even under a margin assumption.<br>2: Our goal is to design a poly(d, 1/ , 1/γ) time learning algorithm in the presence of Massart noise. In the RCN model, the large margin case is easy because the learning problem is essentially convex. That is, there is a <mark>convex surrogate</mark> that allows us to formulate the problem as a convex program.<br>",
    "Arabic": "بَديل مُحَدَّب",
    "Chinese": "凸替代物",
    "French": "substitut convexe",
    "Japanese": "凸代用関数",
    "Russian": "выпуклый суррогат"
  },
  {
    "English": "convex-concave",
    "context": "1: order of max and min . It is well known maximin and minimax gives different solutions in general, unless when the objective is <mark>convex-concave</mark> (with respect to the policy and critic parameterizations).<br>2: . As we previously noted, as the payoff function is <mark>convex-concave</mark>, a min-max equilibrium of this game must exist. Many tools have been developed for efficiently finding the min-max equilibria of <mark>convex-concave</mark> zero-sum games. The connection between multi-distribution learning and zero-sum games allows us to draw on these tools to derive efficient learning algorithms. Unknown payoff functions.<br>",
    "Arabic": "محدب-مقعر",
    "Chinese": "凸凹",
    "French": "convexe-concave",
    "Japanese": "凸凹",
    "Russian": "выпукло-вогнутый"
  },
  {
    "English": "convexity",
    "context": "1: We now turn to obtaining a bound on the overall objective f (w t ) evaluated at a single predictor w t . The <mark>convexity</mark> of f implies that: \n f 1 T T t=1 w t ≤ 1 T T t=1 f (w t ) . (16) \n Using the above inequality and Thm.<br>2: We then present in Section 3 the general framework of calibration of [20], and give a new characterization of calibration for the evaluation metrics we consider (Theorem 2), and the implications of the <mark>convexity</mark> of a surrogate loss. Our main result is proved in Section 4.<br>",
    "Arabic": "تحدب",
    "Chinese": "凸性",
    "French": "convexité",
    "Japanese": "凸性",
    "Russian": "выпуклость"
  },
  {
    "English": "convolution",
    "context": "1: We then use a final 1x1-<mark>Convolution</mark> to create a [2,96,96] tensor representing the nonnormalized log-probabilities of whether or not an given location is navigable or not. Each CoordConv has kernel size 3, padding 1, and stride 1. CoordUpConv has kernel size 3, padding 0, and stride 2.<br>2: H (•) can be a composite function of operations such as Batch Normalization (BN) [14], rectified linear units (ReLU) [6], Pooling [19], or <mark>Convolution</mark> (Conv). We denote the output of the th layer as x . ResNets.<br>",
    "Arabic": "التفاف",
    "Chinese": "卷积",
    "French": "convolution",
    "Japanese": "畳み込み",
    "Russian": "свёртка"
  },
  {
    "English": "convolution kernel",
    "context": "1: y 0 = CBu 0 y 1 = CABu 0 + CBu 1 y 2 = CA 2 Bu 0 + CABu 1 + CBu 2 . . . This can be vectorized into a convolution (4) with an explicit formula for the <mark>convolution kernel</mark> (5).<br>2: Then, we can define a <mark>convolution kernel</mark> κ ∈ R L that depends on A, b, c. A SSM layer is therefore parametrized by A, b, c, d through κ and its output is defined by y as in the following equation: \n<br>",
    "Arabic": "نواة الالتواء",
    "Chinese": "卷积核",
    "French": "noyau de convolution",
    "Japanese": "畳み込みカーネル",
    "Russian": "свёрточное ядро"
  },
  {
    "English": "convolution layer",
    "context": "1: The initial <mark>convolution layer</mark> comprises 2k convolutions of size 7×7 with stride 2; the number of feature-maps in all other layers also follow from setting k. The exact network configurations we used on ImageNet are shown in Table 1.<br>2: The dense scene feature is from the output of the last block (or the observed feature for current frame) and it's further downscaled (/8) by a <mark>convolution layer</mark> to reduce computation for pixel-agent interaction.<br>",
    "Arabic": "طبقة الالتواء",
    "Chinese": "卷积层",
    "French": "couche de convolution",
    "Japanese": "畳み込み層",
    "Russian": "Сверточный слой"
  },
  {
    "English": "convolution neural network",
    "context": "1: This is true over various learner network structures (fully-connected, residual, <mark>convolution neural networks</mark>) and various labeling functions (when the labels are generated by linear functions, fully-connected, residual, convolutional networks, with/without label noise, with/without classification margin). Bias variance view of ensemble : Some prior works also try to attribute the benefit of ensemble as reducing the variance of individual solutions [ 15,62,64,82,83 ] due to label noise or non-convex landscape of the training objective ( so some individual models might simply not be trained very well by over-fitting to the label noise or stuck at a bad local minimal )<br>",
    "Arabic": "الشبكة العصبية التلافيفية",
    "Chinese": "卷积神经网络",
    "French": "réseau neuronal convolutif",
    "Japanese": "畳み込みニューラルネットワーク",
    "Russian": "сверточная нейронная сеть"
  },
  {
    "English": "convolution operation",
    "context": "1: High-dimensional filtering algorithms that follow this approach can still have computational complexity exponential in d. However, a clever filtering scheme can reduce the complexity of the <mark>convolution operation</mark> to O(N d). We use the permutohedral lattice, a highly efficient convolution data structure that tiles the feature space with simplices arranged along d+1 axes [1].<br>",
    "Arabic": "عملية الالتفاف",
    "Chinese": "卷积运算",
    "French": "opération de convolution",
    "Japanese": "畳み込み演算",
    "Russian": "свёрточная операция"
  },
  {
    "English": "convolution operator",
    "context": "1: To do this we define a <mark>convolution operator</mark> ⊗ between two functions f and g: \n (f ⊗ g)(t) max t 1 +t 2 =t f (t 1 ) + g(t 2 )(8) \n For instance: \n<br>",
    "Arabic": "عامل الالتفاف",
    "Chinese": "卷积算子",
    "French": "opérateur de convolution",
    "Japanese": "畳み込み演算子",
    "Russian": "оператор свёртки"
  },
  {
    "English": "convolutional",
    "context": "1: The neurons are connected with one another according to different patterns depending on the type of layer (e.g. fully connected or <mark>convolutional</mark>). This leads to a graph layout problem.<br>2: We run for 10 single models and compare their (best) accuracy to their ensemble accuracy. Result: single vs ensemble. Our detailed comparison tables are in Figure 10 (for non-<mark>convolutional</mark> inputs) and Figure 11 (for <mark>convolutional</mark> inputs).<br>",
    "Arabic": "تلافيفي",
    "Chinese": "卷积",
    "French": "convolutionnel",
    "Japanese": "畳み込み",
    "Russian": "сверточные"
  },
  {
    "English": "convolutional architecture",
    "context": "1: For tasks with paired input structures (PIP, LEP, MSP), we apply this <mark>convolutional architecture</mark> to each input structure separately in a twin network architecture with tied weights, and then concatenate the outputs before passing through two fully-connected layers of size 256 to transform to an output dimension of one neuron for binary classification.<br>",
    "Arabic": "العمارة التلافيفية",
    "Chinese": "卷积架构",
    "French": "architecture convolutionnelle",
    "Japanese": "畳み込みアーキテクチャ",
    "Russian": "свёрточная архитектура"
  },
  {
    "English": "convolutional block",
    "context": "1: While the subnetwork can take any form, U-Nets remain the popular choice (Ronneberger et al., 2015) due to their strong image segmentation ability. They consist of a series of downsampling <mark>convolutional blocks</mark>, each of which preserves some local context, followed by upsampling de<mark>convolutional blocks</mark>, which restore the original input size to the output.<br>2: In implementations, the query network shares the same <mark>convolutional blocks</mark> as the classifier, followed by a prediction head (see Figure 2). Following MoCo, the key network uses a momentum update with the query network. We additionally maintain a queue storing the most current key embeddings k, and we update the queue chronologically.<br>",
    "Arabic": "كتلة تلافيفية",
    "Chinese": "卷积块",
    "French": "bloc de convolution",
    "Japanese": "畳み込みブロック",
    "Russian": "сверточный блок"
  },
  {
    "English": "convolutional decoder",
    "context": "1: Formally, in order to get an occupancy prediction of original size H × W of BEV feature B, the scene-level features F t are upsampled to F t dec ∈ R C×H×W by a <mark>convolutional decoder</mark>, where C is the channel dimension.<br>2: We then upsample the attended dense feature to the same resolution as input F t−1 (/4) and add it with F t−1 as a residual connection for stability. The resulting feature F t is both sent to the next block and a <mark>convolutional decoder</mark> for predicting occupancy at the original BEV resolution (/1).<br>",
    "Arabic": "وحدة فك التشفير التلافيفية",
    "Chinese": "卷积解码器",
    "French": "décodeur convolutionnel",
    "Japanese": "畳み込みデコーダー",
    "Russian": "свёрточный декодер"
  },
  {
    "English": "convolutional encoder",
    "context": "1: In our 2D experiments, we use p=256 planes and c=64 convexes. We use a simple 2D <mark>convolutional encoder</mark> where each layer downsamples the image by half, and doubles the number of feature channels. We use the centers of all pixels as samples. In our 3D experiments, we use p=4, 096 planes and c=256 convexes.<br>2: For a single scene C with n observations, we first replicate and concatenate the camera pose E i and intrinsic parameters K i of each observations to the image channels of the corresponding 2D image I i . Using a learned <mark>convolutional encoder</mark> M , we encode each of the n observations to a code vector r i . These code vectors r i are then summed to form a permutationinvariant representation of the scene r. Via an autoregressive DRAW model [ 16 ] , we form a probability distribution P θ that is conditioned on the code vector r and sample latent variables z. z is decoded into the parameters of a scene representation network , φ , via a hypernetwork Ψ<br>",
    "Arabic": "التشفير التلافيفي",
    "Chinese": "卷积编码器",
    "French": "codeur convolutionnel",
    "Japanese": "畳み込みエンコーダ",
    "Russian": "сверточный энкодер"
  },
  {
    "English": "convolutional feature",
    "context": "1: For the non-attentive model, we use the 2048-dimensional global average pooled version (pool5) of the above <mark>convolutional features</mark>. Models. Our baseline NMT is an attentive model  with a 2-layer bidirectional GRU encoder  and a 2-layer conditional GRU decoder (Sennrich et al., 2017).<br>2: Initially organized as a shared task within the First Conference on Machine Translation (WMT16) , MMT has so far been studied using the Multi30K dataset , a multilingual extension of Flickr30K (Young et al., 2014) with translations of the English image descriptions into German, French and Czech (Elliott et al., 2017;. The three editions of the shared task have seen many exciting approaches that can be broadly categorized as follows : ( i ) multimodal attention using <mark>convolutional features</mark> ( Caglayan et al. , 2016 ; Calixto et al. , 2016 ; Libovický and Helcl , 2017 ; Helcl et al. , 2018 ) ( ii ) cross-modal interactions with spatially-unaware global features ( Calixto<br>",
    "Arabic": "ميزة تلافيفية",
    "Chinese": "卷积特征",
    "French": "caractéristique convolutionnelle",
    "Japanese": "畳み込み特徴量",
    "Russian": "сверточные признаки"
  },
  {
    "English": "convolutional filter",
    "context": "1: 2019;Bai, Kolter, and Koltun 2018) use the <mark>convolutional filter</mark> to capture the long term dependency, and their receptive fields grow exponentially with the stacking of layers, which hurts the sequence alignment.<br>",
    "Arabic": "مرشح تلافيفي",
    "Chinese": "卷积滤波器",
    "French": "filtre de convolution",
    "Japanese": "畳み込みフィルタ",
    "Russian": "Свёрточный фильтр"
  },
  {
    "English": "convolutional kernel",
    "context": "1: Besides reaching the full dependency field, the Diagonal BiLSTM has the additional advantage that it uses a <mark>convolutional kernel</mark> of size 2 × 1 that processes a minimal amount of information at each step yielding a highly nonlinear computation.<br>",
    "Arabic": "نواة تلافيفية",
    "Chinese": "卷积核",
    "French": "noyau de convolution",
    "Japanese": "畳み込み核",
    "Russian": "свёрточное ядро"
  },
  {
    "English": "convolutional layer",
    "context": "1: As can be seen from this plot, phoneme recoverability is poor for the representations based on MFCC and the <mark>convolutional layer</mark> activations, but improves markedly for the recurrent layers. Phonemes are easiest recovered from the activations at recurrent layers 1 and 2, and the accuracy decreases thereafter.<br>2: Front-ends. We adopt the modified ResNet-18 from prior work (Shi et al., 2022a) as visual frontend, where the first <mark>convolutional layer</mark> is replaced by a 3D <mark>convolutional layer</mark> with kernel size of 5×7×7. The visual feature is flattened into an 1D vector by spatial average pooling in the end.<br>",
    "Arabic": "طبقة تلافيفية",
    "Chinese": "卷积层",
    "French": "couche convolutionnelle",
    "Japanese": "畳み込み層",
    "Russian": "свёрточный слой"
  },
  {
    "English": "convolutional network",
    "context": "1: On the 16 × 16 maps, it became clear that the <mark>convolutional network</mark> was struggling, so we allowed it twice as many training iterations as the VIN, yet it still failed to achieve even a remotely similar level of performance on the test maps. (See left image of Figure 6.)<br>2: A crucial part of a category-centric reconstructor is the latent embedding z. Early methods [18,70,58,61,40] predicted a global scene encoding z global = Φ CNN (I src ) with a deep <mark>convolutional network</mark> Φ CNN that solely analyzed the colors of source image pixels.<br>",
    "Arabic": "شبكة تلافيفية",
    "Chinese": "卷积网络",
    "French": "réseau convolutionnel",
    "Japanese": "畳み込みネットワーク",
    "Russian": "сверточная сеть"
  },
  {
    "English": "convolutional neural net",
    "context": "1: • X: (x t , x t+1 ) ∈ (R height×width×3 ) 2 (a pair of sequential states) \n • Y : (R |a| ) 2 (the expected future rewards from each state) \n • f : (convolutional) neural net with |a| outputs \n<br>",
    "Arabic": "شبكة عصبية تلافيفية",
    "Chinese": "卷积神经网络",
    "French": "réseau de neurones convolutionnels",
    "Japanese": "畳み込みニューラルネット",
    "Russian": "сверточная нейронная сеть"
  },
  {
    "English": "convolutional neural network",
    "context": "1: The feature transform can be an identity map  2). end end (ψ(x) = x), image derivatives, mean of color channels, or a learned transformation such as a <mark>convolutional neural network</mark>. In this paper, unless otherwise stated, we used the identity map as the feature transform.<br>2: We describe a single <mark>convolutional neural network</mark> architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model.<br>",
    "Arabic": "الشبكة العصبية التلافيفية",
    "Chinese": "卷积神经网络",
    "French": "réseau de neurones convolutionnel",
    "Japanese": "畳み込みニューラルネットワーク",
    "Russian": "сверточная нейронная сеть"
  },
  {
    "English": "convolutional representation",
    "context": "1: Our technical results focus on developing the S4 parameterization and showing how to efficiently compute all views of the SSM (Section 2): the continuous representation (A, B, C) (1), the recurrent representation (A, B, C) (3), and the <mark>convolutional representation</mark> K (4).<br>",
    "Arabic": "التمثيل التلافيفي",
    "Chinese": "卷积表示",
    "French": "représentation convolutionnelle",
    "Japanese": "畳み込み表現",
    "Russian": "сверточное представление"
  },
  {
    "English": "cooling schedule",
    "context": "1: To derive the double exponentially increasing allocation of trials to the observed best arm, the temperature parameter must follow an exponentially decreasing <mark>cooling schedule</mark> (e.g., T j = exp(−j) where j is the trial number). That is, on iteration j choose heuristic h i with probability: \n<br>",
    "Arabic": "جدول التبريد",
    "Chinese": "冷却进度表",
    "French": "programme de refroidissement",
    "Japanese": "冷却スケジュール",
    "Russian": "график охлаждения"
  },
  {
    "English": "coordinate ascent",
    "context": "1: The <mark>coordinate ascent</mark> update for g involves solving the following optimization problem: \n g * ← argmax g ∈ G F (q, g, σ 2 ). (59 \n ) \n Recall, for the mixture prior with fixed mixture components, fitting g reduces to fitting the mixture weights, π.<br>2: We have omitted a constant and set α = 1. We optimize this function by <mark>coordinate ascent</mark>, iteratively optimizing the collaborative filtering variables {ui, vj} and the topic proportions θj. For ui and vj, maximization follows in a similar fashion as for basic matrix factorization [12].<br>",
    "Arabic": "الصعود التنسيقي",
    "Chinese": "坐标上升算法",
    "French": "\"ascension de coordonnées\"",
    "Japanese": "座標上昇法",
    "Russian": "координатный подъем"
  },
  {
    "English": "coordinate descent",
    "context": "1: For λ > 0, (3) becomes a sup-norm penalized least squares regression. If we use the Newton's method or <mark>coordinate descent</mark> procedure to solve it as in (Zhang, 2006), an inner loop will be needed. This turns out not to be scalable if the number of tasks K is very large.<br>2: Edge rewighting: One main component that prevents Eq. (34) from outputting the delta solution is the usage of non uniform weights w i on the gradient penalty. The authors explicitly detect low contrast image regions and increase their smoothness penalty. To test this idea, we have implemented a simplified <mark>coordinate descent</mark> variant of the algorithm.<br>",
    "Arabic": "الانحدار التنازلي في التنسيق",
    "Chinese": "坐标下降法",
    "French": "descente de coordonnées",
    "Japanese": "座標降下法",
    "Russian": "координатный спуск"
  },
  {
    "English": "coordinate descent algorithm",
    "context": "1: where the vectors e(v i ) ∈ R r are not spit out of a deep network, as in our case, but rather directly optimized. Liljencrants and Lindblom (1972) propose a <mark>coordinate descent algorithm</mark> to optimize E(m).<br>2: As suggested by [21], we have initialized our <mark>coordinate descent algorithm</mark> with a low λ value and gradually increased it during iterations. Since λ is initially low the algorithm is steered toward the true kernel and when λ is increased, the algorithm is already trapped in a local minimum and does not move significantly away from it.<br>",
    "Arabic": "خوارزمية تنازل الإحداثيات",
    "Chinese": "坐标下降算法",
    "French": "algorithme de descente de coordonnées",
    "Japanese": "座標降下アルゴリズム",
    "Russian": "\"алгоритм координатного спуска\""
  },
  {
    "English": "coordinate frame",
    "context": "1: A 2d (3d) statistical shape model is built from a training set of example outlines (surfaces), aligned to a common <mark>coordinate frame</mark>. Each shape, S i , (i = 1, . . .<br>",
    "Arabic": "الإطار الإحداثي",
    "Chinese": "坐标系",
    "French": "cadre de référence",
    "Japanese": "座標系",
    "Russian": "координатная система"
  },
  {
    "English": "copy mechanism",
    "context": "1: In this paper, we propose a TRAnsferable Dialogue statE generator (TRADE) that generates dialogue states from utterances using a <mark>copy mechanism</mark>, facilitating knowledge transfer when predicting (domain, slot, value) triplets not encountered during training.<br>2: • To overcome the multi-turn mapping problem, TRADE leverages its context-enhanced slot gate and <mark>copy mechanism</mark> to properly track slot 1 The code is released at github.com/ jasonwu0731/trade-dst values mentioned anywhere in dialogue history.<br>",
    "Arabic": "آلية النسخ",
    "Chinese": "复制机制",
    "French": "mécanisme de copie",
    "Japanese": "コピーメカニズム",
    "Russian": "механизм копирования"
  },
  {
    "English": "core tensor",
    "context": "1: where C is a <mark>core tensor</mark> and V U is the feature matrix for the users, V L is the feature matrix for the items in the last transition (outgoing nodes) and V I is the feature matrix for the items to predict (ingoing nodes). They have the following structure: \n<br>2: Each factor matrix A ( ) represents the latent features of the -th mode of X, and each element of a <mark>core tensor</mark> G is the weight of the relation composed of columns of factor matrices.<br>",
    "Arabic": "النواة الأساسية",
    "Chinese": "核张量",
    "French": "noyau de tenseur",
    "Japanese": "コアテンソル",
    "Russian": "ядро тензора"
  },
  {
    "English": "coreference annotation",
    "context": "1: However, these models can not be applied to the <mark>coreference annotation</mark> in a straightforward manner, because coreference labels are more complex and they are very different from classification and sequence labels. To overcome the lack of a suitable aggregation method for <mark>coreference annotation</mark>s, Paun et al.<br>2: We utilized no <mark>coreference annotation</mark> during training, but did use minimal prototype information to prime the learning of entity types (Section 5.3).<br>",
    "Arabic": "ترميز المرجعية",
    "Chinese": "指代关系标注",
    "French": "annotation de coréférence",
    "Japanese": "共参照注釈",
    "Russian": "аннотация ядерных ссылок"
  },
  {
    "English": "coreference chain",
    "context": "1: If a mention is classified as Discourse Old (i.e., it refers to an entity mentioned previously in the text) or as the Property of another mention, its <mark>coreference chain</mark> is inferred using weighted voting in which the annotators' labels are weighted by their reliability.<br>2: <mark>coreference chain</mark> inference, based on the predicted general classes in the first subtask, we predict each mention's target (i.e., its referent entity which is usually another mention in the text).<br>",
    "Arabic": "سلسلة مرجعية",
    "Chinese": "共指链",
    "French": "chaîne de coréférence",
    "Japanese": "共参照鎖",
    "Russian": "цепочка кореференции"
  },
  {
    "English": "coreference resolution",
    "context": "1: How are they introduced and referenced? How are relationships between them established? <mark>Coreference Resolution</mark> Resolving coreference is crucial for language understanding. In signed languages, present referents, where the signer explicitly points to the entity in question, are relatively unambiguous.<br>",
    "Arabic": "حل الإشارات المتراجعة",
    "Chinese": "指代消解",
    "French": "résolution de coréférence",
    "Japanese": "共参照解析",
    "Russian": "разрешение кореференции"
  },
  {
    "English": "coreference resolution model",
    "context": "1: Regressions during coreference resolution annotation were investigated by Cheri et al. (2016), who used it to propose a heuristic for pruning candidates in a <mark>coreference resolution model</mark>. In Hollenstein and Zhang (2019), the total duration of regressions from a word was used as a context feature in named-entity recognition.<br>",
    "Arabic": "نموذج حل المرجعية",
    "Chinese": "指代消解模型",
    "French": "modèle de résolution de coréférence",
    "Japanese": "共参照解決モデル",
    "Russian": "модель разрешения кореференции"
  },
  {
    "English": "coreference resolution system",
    "context": "1: We propose to improve our evaluation by first detecting these questions using a state-of-the-art <mark>coreference resolution system</mark> (Lee et al., 2018) 6 , and then substituting them with either rewriting the questions inplace and replacing the questions with their contextindependent counterparts. Detecting invalid questions. We make the assumption that if the coreference model resolves mentions in Q * i differently between using gold history ( Q * 1 , A * 1 , ... , A * i−1 , Q * i ) and predicted history ( Q * 1 , A 1 , ... , A i−1 , Q * i ) , then Q * i<br>",
    "Arabic": "نظام حل الإحالات المرجعية",
    "Chinese": "指代消解系统",
    "French": "système de résolution de coréférence",
    "Japanese": "共参照解決システム",
    "Russian": "система разрешения кореференции"
  },
  {
    "English": "coreferent",
    "context": "1: We leave the incorporation of <mark>coreferent</mark> predictions for future work. To integrate the singleton model into the Stanford coreference system, we let a sieve consider whether a pair of mentions is <mark>coreferent</mark> only if neither of the two mentions are classified as singletons by our CONFIDENT model.<br>2: 22 CEAF computes an alignment between reference (R) and system-predicted (S) entities, with each entity represented by a set of <mark>coreferent</mark> mentions, and with the constraint that each predicted entity is aligned to at most one reference entity.<br>",
    "Arabic": "مترابطة",
    "Chinese": "共指",
    "French": "coréférent",
    "Japanese": "共参照",
    "Russian": "кореферент"
  },
  {
    "English": "correlate equilibrium",
    "context": "1: In addition to further investigating modeling applications, our future work will investigate the applicability of Max-CausalEnt on non-modeling tasks in dynamics settings. For instance, we note that the proposed principle provides a natural criteria for efficiently identifying a <mark>correlated equilibrium</mark> in dynamic Markov games, generalizing the approach to normal-form games of Ortiz et al. (2007).<br>2: As we argued in the paper, the social welfare that can be attained via a Nash equilibrium (that is, by playing selfishly) may be significantly lower than what can be achieved via a <mark>correlated equilibrium</mark>. We provided some empirical evidences that ICFR computes equilibria which attain a social welfare 'not too far' from the optimal one.<br>",
    "Arabic": "توازن ارتباطي",
    "Chinese": "相关均衡",
    "French": "équilibre corrélé",
    "Japanese": "相関均衡",
    "Russian": "коррелированное равновесие"
  },
  {
    "English": "correlated equilibria",
    "context": "1: This work extends and generalizes a growing body of work on decentralized no-regret dynamics in many ways. We demonstrate a class of no-regret algorithms which enjoy rapid convergence when played against each other, while being robust to adversarial opponents. This has implications in computation of <mark>correlated equilibria</mark>, as well as understanding the behavior of agents in complex multi-player games.<br>2: We evaluate the convergence of ICFR on the standard benchmark games for the computation of <mark>correlated equilibria</mark>. We use parametric instances from four different multi-player games: Kuhn poker [33], Leduc poker [46], Goofspiel [42], and Battleship [20].<br>",
    "Arabic": "توازنات ترابطية",
    "Chinese": "相关均衡",
    "French": "équilibres corrélés",
    "Japanese": "相関均衡",
    "Russian": "коррелированные равновесия"
  },
  {
    "English": "correlation coefficient",
    "context": "1: Small values of <mark>correlation coefficient</mark> highlight that fixing seeds does not ensure deterministic results due to non-determinism in GPUs. Similarly, setting random seed in PyTorch ensures reproducibility only on the same hardware.<br>2: As shown in the main text, pseudo-streaming AP correlates extraordinarily well with ground-truth-streaming AP, with a normalized <mark>correlation coefficient</mark> of 0.9925. This suggests that pseudo ground truth can be used to rank streaming perception algorithms.<br>",
    "Arabic": "معامل الارتباط",
    "Chinese": "相关系数",
    "French": "coefficient de corrélation",
    "Japanese": "相関係数",
    "Russian": "коэффициент корреляции"
  },
  {
    "English": "correspondence matrix",
    "context": "1: Formally, we identify correspondences via selecting top M similarities in {Y i,j } n×n ′ = topM(S), where Y denotes the <mark>correspondence matrix</mark> with Y i,j = 1 denoting the correspondence between the i-th object in G and the j-th object in G ′ , otherwise Y i,j = 0.<br>",
    "Arabic": "مصفوفة المراسلات",
    "Chinese": "对应矩阵",
    "French": "matrice de correspondance",
    "Japanese": "対応行列",
    "Russian": "матрица соответствий"
  },
  {
    "English": "cosine",
    "context": "1: In other words, the similarity measure sim(T t i , T t i + t ) of Eq. (5) should equal 1 (corresponding to cos(0), i.e., an angle of 0 • between the two vectors).<br>2: The idea is that, the smaller the <mark>cosine</mark> between the 'correct direction' w and the 'update direction' ξ, the smaller the learning rate needs to be for the update to stay in a unit ball, see figure 6. Lemma 11 (alignment lemma). If w and ξ are unit vectors with \n<br>",
    "Arabic": "جيب التمام",
    "Chinese": "余弦",
    "French": "cosinus",
    "Japanese": "コサイン",
    "Russian": "косинус"
  },
  {
    "English": "cosine decay",
    "context": "1: We experimented with different ways of interpolating the text embeddings, but found that simply taking the text embedding closest to the sampled azimuth worked well. Optimizer. We use Distributed Shampoo ( Anil et al. , 2020 ) with β 1 = 0.9 , β 2 = 0.9 , exponent override = 2 , block size = 128 , graft type = SQRT N , = 10 −6 , and a linear warmup of learning rate over 3000 steps from 10 −9 to 10 −4 followed by <mark>cosine decay</mark> down to<br>",
    "Arabic": "تراجع الكوساين",
    "Chinese": "余弦衰减",
    "French": "décroissance cosinus",
    "Japanese": "コサイン減衰",
    "Russian": "косинусное затухание"
  },
  {
    "English": "cosine decay schedule",
    "context": "1: We have observed similar behaviors of SimSiam in the CIFAR-10 dataset [24]. The implementation is similar to that in ImageNet. We use SGD with base lr = 0.03 and a <mark>cosine decay schedule</mark> for 800 epochs, weight decay = 0.0005, momentum = 0.9, and batch size = 512. The input image size is 32×32.<br>2: We used a linear warmup followed by a <mark>cosine decay schedule</mark>. For regularization we used the same weight decay of 0.2 for all the models. Details about different architectures that were used are provided in Tab. 3. Training hyper-parameters and resources used are provided in Tab. 4.<br>",
    "Arabic": "جدول تحلل الكوساين",
    "Chinese": "余弦衰减调度",
    "French": "- Calendrier de décroissance du cosinus",
    "Japanese": "コサイン減衰スケジュール",
    "Russian": "косинусное расписание убывания"
  },
  {
    "English": "cosine learning rate schedule",
    "context": "1: When running a linear probe on ImageNet, we follow recent literature and use SGD with momentum 0.9 and a high learning rate (we try the values 30, 10, 3, ... in the manner described above) (He et al., 2019). We train for 1000000 iterations with a <mark>cosine learning rate schedule</mark>.<br>2: For ViLT experiments, we fine-tune with an AdamW optimizer with learning rate 1e-4 and weight decay 1e-2. We use a <mark>cosine learning rate schedule</mark> with warm-up over the first epoch. We train the models up to 30 epochs with a patience of 10 epochs and follow the same model selection criterion as for CLIP.<br>",
    "Arabic": "جدول معدل التعلم الجيبي",
    "Chinese": "余弦学习率调度",
    "French": "programme de diminution du taux d'apprentissage cosinus",
    "Japanese": "コサイン学習率スケジュール",
    "Russian": "расписание темпа обучения по косинусу"
  },
  {
    "English": "cosine measure",
    "context": "1: Algorithm \"IR\" computes the standard \"<mark>cosine measure</mark>\" vector product of the sparse vector corresponding to the current spot and the (probably dense) vector corresponding to the node.<br>2: One popular class of ranking functions is the <mark>cosine measure</mark> [37], for example \n F (D, q) = d 1 i=0 w(q, ti) • w(D, ti) |D| , \n<br>",
    "Arabic": "قياس جيب التمام",
    "Chinese": "余弦相似度",
    "French": "mesure cosinus",
    "Japanese": "コサイン類似度",
    "Russian": "косинусная мера"
  },
  {
    "English": "cosine schedule",
    "context": "1: where α t and γ t are determined by the pre-defined noise schedule, e.g., <mark>cosine schedule</mark> (Nichol and   (Li et al., 2022b;, we predict all the original tokens Y 0 = {y \n (0) 1 , • • • , y(0) \n<br>2: When fine-tuning, we use the same batch size and Adam hyperparameters. Here, we do not employ a <mark>cosine schedule</mark>, and early stop once we reach the maximum validation accuracy. Again, no dropout is used.<br>",
    "Arabic": "الجدول الكوسيني",
    "Chinese": "余弦调度",
    "French": "horaire cosinus",
    "Japanese": "コサインスケジュール",
    "Russian": "косинусное расписание"
  },
  {
    "English": "cosine similarity measure",
    "context": "1: arg max b * ∈V (sim (b * , b − a + a * )) \n where V is the vocabulary excluding the question words b, a and a * , and sim is a similarity measure. Specifically, they used the <mark>cosine similarity measure</mark>, defined as: \n<br>2: Then, we clustered these vectors using a Group Average Agglomerative algorithm using the <mark>cosine similarity measure</mark> (Manning et al., 2008). This similarity measure is appropriate because it compares the angle between vectors, and is not affected by their magnitude (the magnitude of forward vectors decreases with the number of modifiers generated).<br>",
    "Arabic": "قياس التشابه جيب التمام",
    "Chinese": "余弦相似度度量",
    "French": "mesure de similarité cosinus",
    "Japanese": "コサイン類似度尺度",
    "Russian": "косинусная мера сходства"
  },
  {
    "English": "cost function",
    "context": "1: In both frameworks, a <mark>cost function</mark> is defined where in addition to the traditional cluster quality cost, a second cost is introduced to regularize the temporal smoothness. We then derive the (relaxed) optimal solutions for solving the <mark>cost function</mark>s.<br>2: The provided hint highlights which existing digits and constraints can be used to derive that cells value. To make sure the provided hints are easy to understand, we rely on a <mark>cost function</mark> that should approximate human understandability, e.g., taking the number of constraints and digits into account, as well as an estimate of their cognitive complexity.<br>",
    "Arabic": "دالة التكلفة",
    "Chinese": "代价函数",
    "French": "fonction de coût",
    "Japanese": "コスト関数",
    "Russian": "функция стоимости"
  },
  {
    "English": "cost vector",
    "context": "1: The cost-matrix output by the OS algorithm can be simplified by rescaling, or adding the same number to each coordinate of a <mark>cost vector</mark>, without affecting the constraints it imposes on a weak classifier, to the following form \n<br>",
    "Arabic": "متجه التكلفة",
    "Chinese": "成本向量",
    "French": "vecteur de coût",
    "Japanese": "コストベクトル",
    "Russian": "вектор стоимости"
  },
  {
    "English": "cost volume",
    "context": "1: In contrast, DCFlow requires their network to be trained using an embedding loss between pixels; it cannot be trained directly on optical flow because their <mark>cost volume</mark> processing is not differentiable. Direct Flow Prediction Neural networks have been trained to directly predict optical flow between a pair of frames, side-stepping the optimization problem completely.<br>2: [24] introduce a contingency planner with diverse sets of future predictions and LAV [15] trains the planner with all vehicles' trajectories to provide richer training data. NMP [101] and its variant [94] estimate a <mark>cost volume</mark> to select the plan with minimal cost besides deterministic future perception.<br>",
    "Arabic": "حجم التكلفة",
    "Chinese": "成本体积",
    "French": "volume de coût",
    "Japanese": "コスト体積",
    "Russian": "стоимостный обьем"
  },
  {
    "English": "cost-sensitive learning",
    "context": "1: Team Inno (Grigorev and Ivanov, 2020)(TC:7) used RoBERTa with <mark>cost-sensitive learning</mark> for subtask TC. They experimented with undersampling, i.e. removing examples from the bigger classes, as well as with modeling the context. They also tried various pre-trained Transformers, but obtained worse results.<br>2: The consideration of data acquisition costs has seen increasing research attention, both explicitly (e.g., <mark>cost-sensitive learning</mark> [31], utility-based data mining [19]) and implicitly, as in the case of active learning [5].<br>",
    "Arabic": "التعلم الحساس للتكلفة",
    "Chinese": "成本敏感学习",
    "French": "apprentissage sensible au coût",
    "Japanese": "コスト感知学習",
    "Russian": "обучение с учетом стоимости"
  },
  {
    "English": "counterexample",
    "context": "1: As specified in Algorithm 1, the learner iteratively updates her hypothesis N by asking equivalence queries. On seeing a <mark>counterexample</mark> (o, o ), the learner checks whether N includes a rule that covers either (o , o) or (o, o ).<br>2: In a membership query, the learner provides an ABox A and a candidate answerā and asks whether A, O |= q T (ā); the oracle faithfully answers \"yes\" or \"no\". In an equivalence query , the learner provides a hypothesis CQ q H and asks whether q H is equivalent to q T under O ; the oracle answers `` yes '' or provides a <mark>counterexample</mark> , that is , an ABox A and tupleā such that A , O |= q T ( ā ) and A , O |= q H (<br>",
    "Arabic": "مثال مضاد",
    "Chinese": "反例",
    "French": "contre-exemple",
    "Japanese": "反例",
    "Russian": "контрпример"
  },
  {
    "English": "counterfactual datum",
    "context": "1: It is typically very hard to obtain the ground-truth <mark>counterfactual data</mark> as only one of the two potential outcomes can be obtained in the observational data. Hence, in this section, we follow a standard practice to evaluate the proposed framework and the alternative approaches on three semi-synthetic datasets.<br>2: Robustness (to knowledge conflicts): the accuracy of the contextual answer when evaluated on <mark>counterfactual data</mark> (altered examples from NQ dev). We compare the contextual answer to the expected (altered) answer. 3. Answerability: the accuracy of the model in abstaining from giving a contextual answer when given a random or empty context.<br>",
    "Arabic": "بيانات مضادة",
    "Chinese": "反事实数据",
    "French": "donnée contrefactuelle",
    "Japanese": "対事実データ",
    "Russian": "контрфактические данные"
  },
  {
    "English": "counterfactual example",
    "context": "1: On four linguistic tasks from the MSGS dataset, we find that including the <mark>counterfactual example</mark> significantly improves the model performance for both GPT-3.5 and GPT-4, which indicates that they can understand the difference between the input text and its counterfactual text according to the task descriptions. Takeaways.<br>2: Here we study if adding a <mark>counterfactual example</mark> of the test input would mislead the model into making an incorrect prediction. For a given task, we define a <mark>counterfactual example</mark> of a text as a superficially-similar example with a different label, which is usually generated by changing the meaning of the original text with minimal edits [87].<br>",
    "Arabic": "المثال المضاد للواقع",
    "Chinese": "反事实例子",
    "French": "exemple contrefactuel",
    "Japanese": "反実仮想例",
    "Russian": "контрпример"
  },
  {
    "English": "counterfactual fairness",
    "context": "1: This idea can be formalized by requiring that decisions remain the same in expectation even if one's protected characteristics are counterfactually altered, a condition known as <mark>counterfactual fairness</mark> (Kusner et al., 2017). Definition 4. Counterfactual fairness holds when \n E[D(a ) | X] = E[D | X].<br>2: Similarly, for almost every joint distribution of X and X Π,A,a , we show that policies satisfying path-specific fairness (including <mark>counterfactual fairness</mark>) are Pareto dominated. (NB: The analogous statement for counterfactual predictive parity is not true, which we address in Proposition 2.)<br>",
    "Arabic": "عدالة الحالات البديلة",
    "Chinese": "反事实公平",
    "French": "équité contrefactuelle",
    "Japanese": "反事実的公平性",
    "Russian": "контрфактическая справедливость"
  },
  {
    "English": "counterfactual reasoning",
    "context": "1: We aim to answer the following research questions in our experiments: (1) Can our contrastive decoding strategy lead to a more consistent teacher? (2) Can a more consistent teacher and the <mark>counterfactual reasoning</mark> objective lead to a student that reasons more faithfully?<br>2: Interestingly, InstructGPT tends to start with the word \"if \" or \"when\" for hypothetical constraints (e.g., \"if someone is lactose intolerant\" for \"make a cake\"), suggesting the potential for future research on <mark>counterfactual reasoning</mark> in language planning. We also analyze the domain distribution of CoScript in the Appendix C.2<br>",
    "Arabic": "الاستدلال المضاد للحقائق",
    "Chinese": "反事实推理",
    "French": "raisonnement contrefactuel",
    "Japanese": "反実仮想推論",
    "Russian": "контрфактическое рассуждение"
  },
  {
    "English": "counterfactual regret minimization",
    "context": "1: From the algorithmic approach, methods based on <mark>Counterfactual Regret Minimization</mark> [Zinkevich et al., 2008] and firstorder methods for game solving [Kroer et al., 2017], have enabled solutions to larger and larger games. In terms of applications , there have been a number of recent breakthroughs , including exceeding human performance in no-limit poker [ Brown and Sandholm , 2017 ; Moravčík et al. , 2017 ] , essentially weakly solving limit poker [ Bowling et al. , 2015 ] , work in security games with applications to infrastructure security [ Pita et al. , 2009 ]<br>",
    "Arabic": "التقليل من الندم المضاد",
    "Chinese": "反事实遗憾最小化 (CFR)",
    "French": "minimisation du regret contrefactuel",
    "Japanese": "反事実的後悔最小化 (CFR)",
    "Russian": "минимизация контрфактического сожаления (CFR)"
  },
  {
    "English": "covariance function",
    "context": "1: (x) = E[f (x)] and covariance (or kernel) function k(x, x ) = E[(f (x) − µ(x))(f (x ) − µ(x ))].<br>2: f ∼ GP(ν(•), k(•, •)), y i = f (x i ) + i , i ∼ N (0, σ 2 n ), \n where ν : X → R is the mean function and k : X × X → R is the <mark>covariance function</mark>.<br>",
    "Arabic": "\"دالة التغاير\"",
    "Chinese": "协方差函数",
    "French": "fonction de covariance",
    "Japanese": "共分散関数",
    "Russian": "функция ковариации"
  },
  {
    "English": "covariance kernel",
    "context": "1: Σ is the covariance matrix resulting from application of the <mark>covariance kernel</mark> to the concatenation of labeled, unlabeled and rejected data locations. Eq. 12 comes from: 1) a GP prior term, 2) a labeled data acceptance term, 3) an unlabeled data acceptance term, and 4) a rejection term.<br>",
    "Arabic": "نواة التغاير",
    "Chinese": "协方差核函数",
    "French": "noyau de covariance",
    "Japanese": "共分散カーネル",
    "Russian": "ковариационное ядро"
  },
  {
    "English": "covariance matrix",
    "context": "1: However, our extension does not allow arbitrary changes in the uncertainty of a single feature over multiple frames. We are currently able to handle the case where the change in the <mark>covariance matrices</mark> of all of the image features can be modeled by a global 2D a ne transformation, which v aries from frame to frame.<br>2: As an additional baseline, we compared decoupled sampling with a LanczOs Variance Estimates (LOVE) based alternative (Pleiss et al., 2018). The LOVE approach to sampling from GP posteriors exploits structured <mark>covariance matrices</mark> in conjunction with fast (approximate) solvers to achieve linear time complexity with respect to number of test locations. For example , when inducing locations Z are defined to be a regularly spaced grid , the prior covariance K m , m = k ( Z , Z ) can be expressed as the Kronecker product of Toeplitz matrices-a property that can be used to dramatically expedite much of the related linear algebra ( Zimmerman , 1989 ; Saatçi , 2012 ; Wilson<br>",
    "Arabic": "مصفوفة التغاير",
    "Chinese": "协方差矩阵",
    "French": "matrice de covariance",
    "Japanese": "共分散行列",
    "Russian": "матрица ковариации"
  },
  {
    "English": "covariance model",
    "context": "1: As such, we present three alternative optimization procedures that extend the methods from [7,12,15,17] to the arbitrary <mark>covariance model</mark> discussed above and guarantee that γ i ≥ 0 for all i. Because of the flexibility this allows in constructing Σ s , and therefore Σ b , some additional notation is required to proceed.<br>",
    "Arabic": "نموذج التباين المشترك",
    "Chinese": "协方差模型",
    "French": "modèle de covariance",
    "Japanese": "共分散モデル",
    "Russian": "модель ковариации"
  },
  {
    "English": "covariance operator",
    "context": "1: Second, analogous to the discrete case, we perform a 'thin' SVD of the <mark>covariance operator</mark> C 2,1 , and take its top N left singular vectors U, such that the operator U O is invertible. Some simple algebraic manipulations establish the relation between observable and unobservable quantities \n<br>2: We proved bounds on the KL divergence between the variational approximation of sparse GP regression to the posterior, that depend only on the decay of the eigenvalues of the <mark>covariance operator</mark>. These bounds prove the intuitive result that smooth kernels with training data concentrated in a small region admit high quality, very sparse approximations.<br>",
    "Arabic": "عامل التباين المشترك",
    "Chinese": "协方差算子",
    "French": "opérateur de covariance",
    "Japanese": "共分散演算子",
    "Russian": "оператор ковариации"
  },
  {
    "English": "covariance parameter",
    "context": "1: foreground features . Appearance. Each feature's appearance is represented as a point in some appearance space, defined below. Each part p has a Gaussian density within this space, with mean and <mark>covariance parameters</mark> θ app p = {c p , V p } which is independent of other parts' densities.<br>",
    "Arabic": "معامل التباين المشترك",
    "Chinese": "协方差参数",
    "French": "paramètre de covariance",
    "Japanese": "共分散パラメータ",
    "Russian": "параметр ковариации"
  },
  {
    "English": "covariance structure",
    "context": "1: scaled identity matrices), Σ 0 = αI d , Σ 1 = βI d , the <mark>covariance structure</mark> reduces to \n Σ B = I n ⊗ αI d + B ⊗ βI d = (αI n + βB) ⊗ I d =: Σ B ⊗ I d , \n<br>2: The predictors in our GP are the mean embeddings µ1, . . . , µn. We also include spatial information in the form of 2-dimensional spatial coordinates si giving the centroid of region i. Putting these predictors together we adopt an additive <mark>covariance structure</mark>: f ∼ GP(0, σ 2 \n<br>",
    "Arabic": "هيكل التباين المشترك",
    "Chinese": "协方差结构",
    "French": "structure de covariance",
    "Japanese": "共分散構造",
    "Russian": "структура ковариации"
  },
  {
    "English": "covariant derivative",
    "context": "1: Let γ : [0, 1] → M be a smooth curve. We define the <mark>covariant derivative</mark> along the curve γ by Dγ : X (γ) → X (γ) similarly to the connection, where X (γ) = Γ(γ([0, 1]), TM).<br>2: This manifold divergence operator is defined by replacing the directional derivative of the Euclidean space with its Riemannian version, namely the <mark>covariant derivative</mark>, \n div(u) = n i=1 ∇ ei u, e i g ,(7) \n<br>",
    "Arabic": "المشتقة التعاقبية",
    "Chinese": "协变导数",
    "French": "dérivée covariante",
    "Japanese": "共変導関数",
    "Russian": "ковариантная производная"
  },
  {
    "English": "covariate",
    "context": "1: Due to the lack of detailed information about each individual, apart from the potential outcome, we also generate the treatment ( ) and the <mark>covariates</mark> (x ) as the follows: \n x ∼ N (0, I), ∼ (sigmoid(x v )),(22) \n<br>2: In particular, we show how to construct the probe W in such a way that the additional conditioning on the reduced <mark>covariates</mark> W = ω(X) in Eqs. ( 3) and ( 5) does not affect the argument.<br>",
    "Arabic": "متغير مشترك",
    "Chinese": "协变量",
    "French": "covariable",
    "Japanese": "共変量",
    "Russian": "ковариат"
  },
  {
    "English": "covariate shift",
    "context": "1: It is worth mentioning that we identify a unique <mark>covariate shift</mark> of graph data that comes from the graph structures, and we design a federal random graph model for the corresponding further study. (3) FS-G also provides a component for tuning the FGL methods.<br>2: Biased sampling has been used in domain adaptation to simulate real-world drift; this is known as the <mark>covariate shift</mark> assumption. In NLP, however, even worst-case splits, maximizing bias, often under-estimate the error observed on new samples of in-domain data, i.e., the data that models should minimally generalize to at test time.<br>",
    "Arabic": "تحول المتغيرات المصاحبة",
    "Chinese": "协变量偏移",
    "French": "décalage de covariable",
    "Japanese": "共変量シフト",
    "Russian": "ковариатный сдвиг"
  },
  {
    "English": "credit assignment",
    "context": "1: Second, they estimate the accumulation of gradients over a multi-step trajectory, rather than full transition dynamics, thereby focusing on those aspects that matter for the update. Third, they allow <mark>credit assignment</mark> across these potential past trajectories with a single update, without the iterative computation that is typically required when using a more explicit model.<br>2: The learning to teach in MARL problem has unique inherent complexities that compound the delayed reward, <mark>credit assignment</mark>, and partial observability issues found in general multiagent problems (Oliehoek and Amato 2016). As such, there are several key issues that must be addressed.<br>",
    "Arabic": "تخصيص الائتمان",
    "Chinese": "信用分配",
    "French": "attribution de crédit",
    "Japanese": "クレジット割り当て",
    "Russian": "присвоение кредита"
  },
  {
    "English": "credit assignment problem",
    "context": "1: (3) However, such an approach fails to address a key <mark>credit assignment problem</mark>. Because the TD error considers only global rewards, the gradient computed for each actor does not explicitly reason about how that particular agent's actions contribute to that global reward.<br>2: The question of how to determine which states and actions are responsible for a certain outcome is known as the <mark>credit assignment problem</mark> and remains a central research question in reinforcement learning and artificial intelligence.<br>",
    "Arabic": "مشكلة توزيع الائتمان",
    "Chinese": "学分分配问题",
    "French": "problème d'attribution du crédit",
    "Japanese": "クレジット割り当て問題",
    "Russian": "проблема присвоения заслуг"
  },
  {
    "English": "criterion",
    "context": "1: Given a graph G, a variable S, sets of variables Y, I, R, Z, where {S}, Y, Z are disjoint and I ✓ R ✓ Z; LISTSEPC outputs all pairs Z, Z T , where  \n Z T 2 Z G ({S},Y) hI,Ri. Criterion Input Adjustment Expression Biased Data Unbiased Data GACT1 { P ( v | S=1 ) } P Z P ( y | x , z , S=1 ) P ( z | S=1 ) GACT2 { P ( v | S=1 ) , P ( t ) } P Z P ( y | x , z , S=1 ) P ( z<br>2: With c a <mark>criterion</mark> that penalizes the off-diagonal terms of the covariance matrix as \n c(K) = i̸ =j Cov(K) 2 i,j = ∥KK T − diag(KK T )∥ 2 F = L nc ,(21) \n and v a <mark>criterion</mark> that aims at normalizing dimensions, i.e. rows of K. \n<br>",
    "Arabic": "معيار",
    "Chinese": "准则",
    "French": "critère",
    "Japanese": "基準",
    "Russian": "критерий"
  },
  {
    "English": "critic",
    "context": "1: Algorithm 2 is a two-timescale first-order algorithm (Borkar, 1997;Maei et al., 2009), where the <mark>critic</mark> is updated with a much faster rate η fast than the actor with η slow . This two-timescale update is designed to mimic the oracle updates in Algorithm 1.<br>2: b ← θ b + α D d ∇ log π b ( a i |s i ) q i − c τ ( s i ) 13 : // update <mark>critic</mark> 14 : η τ ← η τ + β D d ∇c τ ( s i ) q i − c τ ( s i ) \n<br>",
    "Arabic": "ناقد",
    "Chinese": "评价者",
    "French": "critique",
    "Japanese": "評価者",
    "Russian": "критик"
  },
  {
    "English": "critic loss",
    "context": "1: For example, when β = 0, Proposition 6 still guarantees a policy no worse than the behavior policy µ, though the <mark>critic loss</mark> does not contain the Bellman error term anymore. (In this case, ATAC performs IL).<br>",
    "Arabic": "خسارة الناقد",
    "Chinese": "评判器损失",
    "French": "perte du critique",
    "Japanese": "クリティック損失 (critic loss)",
    "Russian": "потеря критика"
  },
  {
    "English": "critic network",
    "context": "1: For \n f ∈ { f 1 , f 2 } , update <mark>critic networks</mark> l critic ( f ) : = L Dmini ( f , π ) + βE w Dmini ( f , π ) f ← Proj F ( f − η fast ∇l critic ) 5 : Update actor network l actor ( π ) : = −L Dmini ( f<br>",
    "Arabic": "شبكة النقد",
    "Chinese": "评价网络",
    "French": "\"réseau critique\"",
    "Japanese": "評価者ネットワーク",
    "Russian": "сеть критика"
  },
  {
    "English": "cross attention",
    "context": "1: We implement the <mark>cross attention</mark> between the encoder and the decoder, but utilize the sequence information only. Built upon LSTM, we further test C-LSTM to consider the entire context of the antibody-antigen complex, where each component is separated by a special token.<br>2: where β l i,j is the <mark>cross attention</mark> from the i-th object in graph G to the j-th object in graph G ′ . The object embedding vector weighted by <mark>cross attention</mark>s is computed as \n h l+1 i = ei,j =1 β l i,j (v ′l j ) \n .<br>",
    "Arabic": "انتباه متقاطع",
    "Chinese": "交叉注意力",
    "French": "attention croisée",
    "Japanese": "クロスアテンション",
    "Russian": "перекрестное внимание"
  },
  {
    "English": "cross entropy",
    "context": "1: For all ablation studies we use an image scale of 600 pixels for training and testing. Network Initialization: Our first attempt to train Reti-naNet uses standard <mark>cross entropy</mark> (CE) loss without any modifications to the initialization or learning strategy. This fails quickly, with the network diverging during training.<br>2: As in the retrofitting objective, this prior on the word vector parameters forces words connected in the lexicon to have close vector representations as did Ψ(Q) (with the role ofQ being played by <mark>cross entropy</mark> of the empirical distribution). This prior can be incorporated during learning through maximum a posteriori (MAP) estimation.<br>",
    "Arabic": "التقاطع الإنتروبي",
    "Chinese": "交叉熵",
    "French": "entropie croisée",
    "Japanese": "交差エントロピー",
    "Russian": "кросс-энтропия"
  },
  {
    "English": "cross entropy error",
    "context": "1: In each case the number of hidden units was set to 20, subject to the constraint that (n in + n out ) × n hidden ≤ train size/2. We trained the networks to minimize <mark>cross entropy error</mark> using the quasi-Newton method from Netlab Once a pairwise neural network classifier was learned , we classified test examples according to the previous `` edge '' model , again by building a random graph between test labels ( using an average of 18 edges per test label as before ) , using the learned coordination<br>",
    "Arabic": "خطأ الإنتروبيا المتقاطعة",
    "Chinese": "交叉熵误差",
    "French": "erreur d'entropie croisée",
    "Japanese": "交差エントロピー誤差",
    "Russian": "ошибка перекрёстной энтропии"
  },
  {
    "English": "cross entropy loss",
    "context": "1: We use Focal Loss (Lin et al., 2017) (a weighted version of <mark>Cross Entropy Loss</mark>) with γ = 2.0, α NotNavigable = 0.75, and α Navigable = 0.25 to handle the class imbalance. Evaluation Data and Procedure. We construct our evaluation data using the validation dataset.<br>",
    "Arabic": "خسارة الانتروبيا المتقاطعة",
    "Chinese": "交叉熵损失",
    "French": "perte d'entropie croisée",
    "Japanese": "交差エントロピー損失",
    "Russian": "\"потеря перекрестной энтропии\""
  },
  {
    "English": "cross validation",
    "context": "1: Table 11 shows confusion matrices per suffixoid candidate and for all data cumulatively for the best setting in our automatic experiments (cf. Table 5), in which all features were used in 5-fold <mark>cross validation</mark>.<br>2: For diagrams in this work, clouds come from computing 5 × 2 <mark>cross validation</mark>. 2. An arrow is drawn for each dataset connecting the centers of each pair of ensembles to compare. 3. Finally, all arrows are taken to the origin of coordinates.<br>",
    "Arabic": "تحقق متقاطع",
    "Chinese": "交叉验证",
    "French": "validation croisée",
    "Japanese": "交差検証",
    "Russian": "перекрестная проверка"
  },
  {
    "English": "cross-attention layer",
    "context": "1: Detailedly, F t ds is passed through a self-attention layer to model responses between distant grids, then a crossattention layer models interactions between agent features G t and per-grid features.<br>",
    "Arabic": "طبقة الانتباه المتقاطع",
    "Chinese": "交叉注意力层",
    "French": "couche d'attention croisée",
    "Japanese": "相互注目層",
    "Russian": "слой кросс-внимания"
  },
  {
    "English": "cross-attention module",
    "context": "1: Attention mask visualization. To investigate the internal mechanism and show its explainability, we visualize the attention mask of the <mark>cross-attention module</mark> in the planner. As shown in Fig. 8, the predicted tracking bounding boxes, planned trajectory, and the ground truth HD Map are rendered for reference, and the attention mask is overlayered on top.<br>2: The computation of a Transformer decoder layer (Vaswani et al., 2017) includes a self-attention module and a <mark>cross-attention module</mark>. The self-attention module models the relevant information from previous decoder stateŝ \n s l a ′ = [s l 1 (t 1 ), • • • , s l u−1 (t u−1 )],(4) \n<br>",
    "Arabic": "وحدة الانتباه المتقاطع",
    "Chinese": "交叉注意力模块",
    "French": "module d'attention croisée",
    "Japanese": "クロスアテンションモジュール",
    "Russian": "модуль перекрёстного внимания (cross-attention module)"
  },
  {
    "English": "cross-correlation",
    "context": "1: the inference methods ( often discarding <mark>cross-correlation</mark> of the estimated quantities and relying on alternating optimisation of pose and map [ 23,9 ] ) . However, the conclusion that a dense representation of the environment requires a large number of parameters is not necessarily correct.<br>",
    "Arabic": "ارتباط متقاطع",
    "Chinese": "交叉相关",
    "French": "Intercorrélation",
    "Japanese": "クロス相関 (Kurosu Sōkan)",
    "Russian": "кросс-корреляция"
  },
  {
    "English": "cross-entropy loss function",
    "context": "1: (I t ) i = −∞ if the label l i has been predicted at previous t − 1 time steps. 0 otherwise. (10 \n ) \n At the training stage, the loss function is the <mark>cross-entropy loss function</mark>.<br>2: Because the dataset has a strong class imbalance, we report the balanced TPR-i.e., we compute the per-class TPR and then average over the classes. We experiment with different values of the hyperparameter λ. When λ = 0, the method is equivalent to using the conventional weighted <mark>cross-entropy loss function</mark>.<br>",
    "Arabic": "دالة خسارة الانتروبيا المتقاطعة",
    "Chinese": "交叉熵损失函数",
    "French": "fonction de perte d'entropie croisée",
    "Japanese": "交差エントロピー損失関数",
    "Russian": "функция потерь перекрёстной энтропии"
  },
  {
    "English": "cross-entropy objective",
    "context": "1: Here, A h,l is the value of the attention matrix of the h-th attention head in the l-th layer, x is the input, and L(x) is the loss function of the task, e.g., the <mark>cross-entropy objective</mark> for a classification problem.<br>",
    "Arabic": "هدف الانتروبيا المتقاطعة",
    "Chinese": "交叉熵目标",
    "French": "objectif d'entropie croisée",
    "Japanese": "交差エントロピー目的関数",
    "Russian": "Целевая функция перекрёстной энтропии"
  },
  {
    "English": "cross-lingual benchmark",
    "context": "1: (2020a) introduce the transfer-interference trade-off where low resource languages benefit from multilingual training, up to a point where the overall performance on monolingual and <mark>cross-lingual benchmarks</mark> degrades.<br>",
    "Arabic": "المعيار بين اللغات",
    "Chinese": "跨语言基准测试",
    "French": "référentiel multilingue",
    "Japanese": "言語横断ベンチマーク",
    "Russian": "кросс-языковой бенчмарк"
  },
  {
    "English": "cross-lingual embedding",
    "context": "1: Source-based metric • YISI-2 (Lo, 2019) is the same as YISI-1, except that it uses <mark>cross-lingual embeddings</mark> to compute the similarity of the MT output with the source. The baseline metrics, particularly BLEU, were designed to use multiple references.<br>2: Initially, we perform our experiments with the feature sets from three different <mark>cross-lingual embeddings</mark> (MUSE, XLM, and VecMap) for the dataset D1, then with the smaller dataset D2 and later on the combined dataset D1+D2.<br>",
    "Arabic": "تضمين بين اللغات",
    "Chinese": "跨语言嵌入",
    "French": "plongement interlingue",
    "Japanese": "クロスリンガル埋め込み",
    "Russian": "кросс-лингвистическое встраивание"
  },
  {
    "English": "cross-lingual feature",
    "context": "1: Our analysis shows that, thanks to the prior knowledge encoded in recent pretrained language models and our focus on learning from <mark>cross-lingual features</mark>, our model can be used on languages that were never seen at training time, opening the door to alignment-free cross-lingual SRL on languages where a predicateargument structure inventory is not yet available.<br>2: Aligning heterogeneous resources. As briefly mentioned previously, the universal encoders in the model architecture force our system to learn <mark>cross-lingual features</mark> that are important across different formalisms.<br>",
    "Arabic": "ميزة عبر اللغات",
    "Chinese": "跨语言特征",
    "French": "fonctionnalité multilingue",
    "Japanese": "言語横断的特徴",
    "Russian": "межъязыковые признаки"
  },
  {
    "English": "cross-lingual knowledge transfer",
    "context": "1: We use graph-based label propagation for <mark>cross-lingual knowledge transfer</mark> and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the Expectation Maximization algorithm.<br>",
    "Arabic": "نقل المعرفة عبر اللغات",
    "Chinese": "跨语言知识迁移",
    "French": "transfert de connaissances multilingue",
    "Japanese": "言語横断知識転移",
    "Russian": "перенос знаний между языками"
  },
  {
    "English": "cross-lingual model",
    "context": "1: This is vital for those languages for which a predicate-argument structure inventory has not yet been developed -an endeavor that may take Figure 1: Thanks to its universal encoders, our unified <mark>cross-lingual model</mark> is able to provide predicate sense and semantic role labels according to several linguistic formalisms. Left: SRL labels for an English input sentence.<br>2: Our unified <mark>cross-lingual model</mark>, evaluated on the gold multilingual benchmark of CoNLL-2009, outperforms previous state-of-the-art multilingual systems over 6 diverse languages, ranging from Catalan to Czech, from German to Chinese, and, at the same time, also considerably reduces the amount of trainable parameters required to support different linguistic formalisms. And this is not all.<br>",
    "Arabic": "نموذج عبر اللغات",
    "Chinese": "跨语言模型",
    "French": "modèle multilingue",
    "Japanese": "言語横断モデル",
    "Russian": "перекрёстноязыковая модель"
  },
  {
    "English": "cross-lingual representation",
    "context": "1: Training multilingual LLMs using the masked language modeling (MLM) objective is effective to achieve <mark>cross-lingual representations</mark> (Devlin et al., 2019;Conneau et al., 2020).<br>",
    "Arabic": "التمثيل العابر للغات",
    "Chinese": "跨语种表征",
    "French": "représentation interlingue",
    "Japanese": "クロスリンガル表現",
    "Russian": "межъязыковое представление"
  },
  {
    "English": "cross-lingual transfer",
    "context": "1: The key discrepancy lies in whether local name entities in the sentences remain in English, which will determine a language-agnostic/specific S ACT , as shown in Figure 1. E&E and F&E will have language-agnostic acts while F&F will have language-specific acts which is considered more challenging in <mark>cross-lingual transfer</mark>.<br>2: First, XLM-R may be undertrained, and the inclusion of more head language training data may improve their representations. Second, having more languages may improve multilinguality by allowing languages to synergize and enhance each other's representations and <mark>cross-lingual transfer</mark>.<br>",
    "Arabic": "نقل عبر اللغات",
    "Chinese": "跨语言迁移",
    "French": "transfert inter-lingue",
    "Japanese": "言語横断転移",
    "Russian": "межъязыковой перенос"
  },
  {
    "English": "cross-modal",
    "context": "1: However, these methods can only work for specific scenarios with aerial-ground views, and they are not suitable to be deployed directly for ground-ground views [33]. Recently, several methods aim to study semantic representations and matching for cross-view and <mark>cross-modal</mark> place recognition. We further divide these methods into two categories, including graph-based and geometric-based methods.<br>2: Place recognition plays an important role in multirobot collaborative perception, such as aerial-ground search and rescue, in order to identify the same place they have visited. Recently, approaches based on semantics showed the promising performance to address cross-view and <mark>cross-modal</mark> challenges in place recognition, which can be further categorized as graphbased and geometric-based methods.<br>",
    "Arabic": "عبر الوسائط",
    "Chinese": "跨模态",
    "French": "multimodal",
    "Japanese": "クロスモーダル",
    "Russian": "межмодальный"
  },
  {
    "English": "cross-validate",
    "context": "1: To evaluate generalization, we determined a combination of algorithm and hyperparameter settings A λ by running Auto-WEKA as before (<mark>cross-validating</mark> on the training set), trained A λ on the entire training set, and then evaluated the resulting model on the test set. The right portion of Table 4 reports the test performance obtained with all methods.<br>",
    "Arabic": "تحقق تصليبي متبادل",
    "Chinese": "交叉验证",
    "French": "validation croisée",
    "Japanese": "クロスバリデーション",
    "Russian": "кросс-валидация"
  },
  {
    "English": "cumulant generating function",
    "context": "1: The last term can be written as K(−α)/(−α), where K is the <mark>cumulant generating function</mark> (logarithm of the moment generating function) of the random variable U . The <mark>cumulant generating function</mark> is differentiable, and by the Quotient rule \n<br>",
    "Arabic": "وظيفة توليد تراكمية",
    "Chinese": "累积矩生成函数",
    "French": "fonction génératrice des cumulants",
    "Japanese": "累積生成関数",
    "Russian": "функция кумулянтового порождения"
  },
  {
    "English": "cumulative density function",
    "context": "1: 2 dx is the <mark>cumulative density function</mark> (cdf) of standard normal distribution and d = x − y 2 is the Euclidean distance between the vectors x and y. This collision probability F r (d) is a monotonically decreasing function of the distance d and hence h L2 a,b is an LSH for L2 distances.<br>",
    "Arabic": "دالة الكثافة التراكمية",
    "Chinese": "累积分布函数",
    "French": "fonction de répartition",
    "Japanese": "累積分布関数",
    "Russian": "функция кумулятивного распределения"
  },
  {
    "English": "cumulative distribution function",
    "context": "1: P H1 (n FSSD 2 > r) ≈ 1 − Φ r √ nσ H 1 − √ n FSSD 2 σ H 1 \n , where Φ denotes the <mark>cumulative distribution function</mark> of the standard normal distribution, and σ H1 is defined in Proposition 2. Proof.<br>2: Any such monotonic function f can be rewritten in terms of the <mark>cumulative distribution function</mark> of some density function ρ(θ), defined over the range 0 ≤ θ ≤ π. As our normalised density function, we take a constant term plus a wrapped Cauchy distribution.<br>",
    "Arabic": "دالة التوزيع التراكمية",
    "Chinese": "累积分布函数",
    "French": "fonction de répartition",
    "Japanese": "累積分布関数",
    "Russian": "функция кумулятивного распределения"
  },
  {
    "English": "cumulative regret",
    "context": "1: They provide sharp bounds for this exploration problem. Note that this methodology would not lead to bounds for minimizing the <mark>cumulative regret</mark>. Our <mark>cumulative regret</mark> bounds translate to the first performance guarantees (rates) for GP optimization.<br>2: If the bounded smoothness function given is f (x) = γx ω for some γ > 0, ω ∈ (0, 1] and the Lipschitz upper confidence bound is applied to all m base arms, the <mark>cumulative regret</mark> at time T is bounded by Reg(T ) ≤ 2γ 2−ω (6m log T ) \n<br>",
    "Arabic": "ندم تراكمي",
    "Chinese": "累积遗憾",
    "French": "regret cumulé",
    "Japanese": "累積後悔",
    "Russian": "Суммарное сожаление"
  },
  {
    "English": "cumulative reward",
    "context": "1: Hence, overall, the <mark>cumulative reward</mark> of T l is at most that the agent would obtain if they received reward 1 in every four steps starting from the third step (i.e., after s ′ u 1 ), that amounts tõ γ 2 \n 1−γ 4 < 1 (given the assumption thatγ < 1/2).<br>2: We take the principal 's point of view and investigate the problem of optimal signaling strategy design : given M , find a signaling strategy π that maximizes the principal 's ( discounted ) <mark>cumulative reward</mark> E ∞ t=0 γ t R ( s t , θ t , a t ) |z , π , where z = ( z s ) s∈S<br>",
    "Arabic": "المكافأة التراكمية",
    "Chinese": "累积回报",
    "French": "récompense cumulative",
    "Japanese": "累積報酬",
    "Russian": "кумулятивное вознаграждение"
  },
  {
    "English": "curriculum learning",
    "context": "1: Existing CDA (Wei et al., 2021) varies the wordlevel perturbation ratio to achieve different levels of difficulties under <mark>curriculum learning</mark> with simple word perturbation strategies such as synonym replacement, random insertion, swap, and deletion.<br>2: We also investigate two aspects of our <mark>curriculum learning</mark> scheme: starting with short examples and moving to long ones, and sampling tasks in inverse proportion to their accumulated reward. Experiments are shown in Figure 5b. Both components help; prioritization by both length and weight gives the best results.<br>",
    "Arabic": "التعلم المنهجي",
    "Chinese": "课程学习",
    "French": "apprentissage par curriculum",
    "Japanese": "カリキュラム学習",
    "Russian": "обучение по учебной программе"
  },
  {
    "English": "curse of dimensionality",
    "context": "1: 2015)), the <mark>curse of dimensionality</mark> is inevitable, and as a result problems of 20 dimensions or higher are deemed \"high-dimensional\" in entropy estimation.<br>2: How can we overcome the \"<mark>curse of dimensionality</mark>\" and find the closest cluster fast? We observe that the queries in the click-through bipartite are very sparse. For example, in our experimental data, a query is connected with an average number of 8.2 URLs. Moreover, each URL is also involved in only a few queries.<br>",
    "Arabic": "لعنة الأبعاد",
    "Chinese": "维数诅咒",
    "French": "fléau de la dimensionnalité",
    "Japanese": "次元の呪い",
    "Russian": "проклятие размерности"
  },
  {
    "English": "cut plane",
    "context": "1: More generally at iteration t, one adds a new <mark>cutting plane</mark> at the current solution w t , and looks for the solution w t+1 minimizing the new approximated problem: \n<br>2: One key idea lies in that rather than solving all conflicts between a new <mark>cutting plane</mark> and all previous solutions, we focus on the conflict of the new <mark>cutting plane</mark> c wt w.r.t the best observed solution up to now, w * t (see line 4).<br>",
    "Arabic": "المستوى القاطع",
    "Chinese": "切割平面",
    "French": "plan de coupe",
    "Japanese": "切断平面",
    "Russian": "Секущая плоскость"
  },
  {
    "English": "cut plane algorithm",
    "context": "1: Given training images with ground-truth object locations, we show how to formulate learning as a convex optimization problem. We employ the <mark>cutting plane algorithm</mark> of Joachims et al. (Mach. Learn. 2009)  to efficiently learn a model from thousands<br>2: Rather than returning a binary label for a each image window, our model simultaneously predicts a set of detections for multiple objects from multiple classes over the entire image. Given training images with ground-truth object locations, we show how to formulate parameter estimation as a convex max-margin learning problem. We employ the <mark>cutting plane algorithm</mark> of Joachims et al.<br>",
    "Arabic": "خوارزمية الطائرة القاطعة",
    "Chinese": "割平面算法",
    "French": "algorithme de plan de coupe",
    "Japanese": "切断平面アルゴリズム",
    "Russian": "алгоритм отсекающей плоскости"
  },
  {
    "English": "cut plane method",
    "context": "1: We develop an iterative optimization method by using the <mark>cutting plane method</mark> and solving a group-based multiple kernel learning (MKL) problem. In Section 4, we conduct comprehensive experiments using two benchmark consumer video datasets as the target domain, and the results demonstrate that our method MDA-HS outperforms the existing multi-domain adaptation methods for event recognition.<br>2: The analytic center <mark>cutting plane method</mark> (ACCPM) reduces the feasible region on each iteration using a new cut of the feasible region computed by evaluating a subgradient of the objective function at the analytic center of the current set, until the volume of the reduced region converges to the target precision. This method does not require differentiability.<br>",
    "Arabic": "طريقة المستوى المقطوع",
    "Chinese": "切平面法",
    "French": "méthode du plan de coupe du centre analytique",
    "Japanese": "切断平面法",
    "Russian": "метод секущих плоскостей"
  },
  {
    "English": "cycle consistency",
    "context": "1: We investigated the solvability of viewing graphs, i.e. whether they uniquely determine projective cameras, and made several important advances in the theory and practical use of viewing graphs. Building upon [37], we proposed a new characterization that involves fewer unknowns by exploiting <mark>cycle consistency</mark>.<br>2: We derive a new formulation of viewing graph solvability that is much simpler than [37] thanks to a substantial reduction of the number of unknowns involved. Our formulation is based on the <mark>cycle consistency</mark> property of a graph, namely that the composition of (invertible) transformations along any cycle should be the identity.<br>",
    "Arabic": "الاتساق الدوري",
    "Chinese": "循环一致性",
    "French": "consistance cyclique",
    "Japanese": "循環一貫性",
    "Russian": "циклическая согласованность"
  },
  {
    "English": "cycle consistency loss",
    "context": "1: The second is the use of a \"<mark>cycle consistency loss</mark>\" as an auxiliary loss to some other task, e.g., in generative adversarial networks performing style transfer on images (Zhu et al., 2017). NLG typically relies on models which are autoregressive and non-differentiable.<br>2: However, without ground-truth supervision, there is no constraint to guarantee that the face in both the input and output images correspond to the same person. Using a <mark>cycle consistency loss</mark> [38] we force the generator to maintain the identity of each individual by penalizing the difference between the original image I yo and its reconstruction: \n<br>",
    "Arabic": "خسارة اتساق الدورة",
    "Chinese": "循环一致性损失",
    "French": "perte de cohérence du cycle",
    "Japanese": "サイクル一貫性損失",
    "Russian": "потери цикличной согласованности"
  },
  {
    "English": "cycle inequality",
    "context": "1: In particular, the separation problem for all <mark>cycle inequalities</mark> (5) for all single projection graphs, when we allow some additional valid inequalities for M (arising from the cycle using more than one partition for some variables), can now be solved in time O(poly(n, 2 k )).<br>2: In this section, we answer this question in an affirmative. To this end, we define an SOCP relaxation which specifies constraints such that the resulting graph G from a clique. We denote this relaxation by SOCP-Q (where Q indicates cliques). The SOCP-Q relaxation contains the marginalization constraint and the <mark>cycle inequalities</mark> (defined above).<br>",
    "Arabic": "عدم المساواة في الدورة",
    "Chinese": "环不等式",
    "French": "inégalités de cycle",
    "Japanese": "環不等式",
    "Russian": "неравенства циклов"
  },
  {
    "English": "d-separation",
    "context": "1: Note that by <mark>d-separation</mark> (Pearl 1988), X separates Y from S, (or (Y ⊥ ⊥ S|X)), so we can write P (y|x) = P (y|x, S = 1). This is a very special situation since these two distributions can be arbitrarily distant from each other , but in this specific case G s constrains Q in such a way that despite the fact that data was collected under selection and our goal is to answer a query about the overall population , there is no need to resort to additional data external<br>2: For instance, there might be a (graphical) structural representations of particular types of partial exchangeability and corresponding logical axiomatizations (Pearl 1988). Moreover, it would be interesting to develop graphical models with exchangeability and independence, and notions like <mark>d-separation</mark> to detect marginal exchangeability and conditional decomposability from a structural representation.<br>",
    "Arabic": "د- الانفصال",
    "Chinese": "d-分离",
    "French": "d-séparation",
    "Japanese": "d-分離",
    "Russian": "d-разделение"
  },
  {
    "English": "d_model",
    "context": "1: For the forward pass, we consider contributions from: \n • Embeddings -2 × seq_len × vocab_size × <mark>d_model</mark> • Attention (Single Layer) \n<br>2: -Key, query and value projections: 2 × 3 × seq_len × <mark>d_model</mark> × (key_size × num_heads) We fit to the first third (orange), the middle third (green), and the last third (blue) of all points along the loss frontier. We plot only a subset of the points.<br>",
    "Arabic": "أبعاد النموذج",
    "Chinese": "模型维度",
    "French": "dimension_modèle",
    "Japanese": "dモデル",
    "Russian": "разм_модели"
  },
  {
    "English": "data augmentation",
    "context": "1: <mark>Data Augmentation</mark> Data augmentation approaches are widely used across machine learning application domains featuring known invariances of the data distribution (Japkowicz et al., 2000;Jia and Liang, 2016;Shaw et al., 2021).<br>2: <mark>Data Augmentation</mark> (DA) Kobayashi, 2018;Gao et al., 2019;Khayrallah et al., 2020;Pham et al., 2021) has been widely used in neural machine translation.<br>",
    "Arabic": "تعزيز البيانات",
    "Chinese": "数据增强",
    "French": "augmentation des données",
    "Japanese": "データ拡張",
    "Russian": "аугментация данных"
  },
  {
    "English": "data distribution",
    "context": "1: First, we recall the data generation and augmentation process. Let x be a data point drawn from the <mark>data distribution</mark> p(x) and let x 1 and x 2 be two augmented views of x: x 1 , x 2 ∼ p aug (•|x) where p aug (•|x) is the augmentation distribution.<br>2: If the model does not suffer from any loss divergence issues during training, we should observe the evaluation metrics keep becoming better, as the model is adapting to the <mark>data distribution</mark> closer to the -th day of data.<br>",
    "Arabic": "توزيع البيانات",
    "Chinese": "数据分布",
    "French": "distribution des données",
    "Japanese": "データ分布",
    "Russian": "распределение данных"
  },
  {
    "English": "data imbalance",
    "context": "1: Such methods have been explored in traditional machine learning (Kale and Liu, 2013) but not in the era of large language models to the best of our knowledge. Rare class AL There has been a growing number of applications of active learning in <mark>data imbalance</mark> and rare class problems. Such works include ( Kothawade et al. , 2021 ; Choi et al. , 2021 ; Ein-Dor et al. , 2020 ) which proposed frameworks to improve model performance with <mark>data imbalance</mark> but failed to check the feasibility and costs in a real-world , active annotation setting where not only is rare class very infrequent ( 4 % ) but very few ( <<br>",
    "Arabic": "عدم توازن البيانات",
    "Chinese": "数据不平衡",
    "French": "déséquilibre des données",
    "Japanese": "データ不均衡",
    "Russian": "дисбаланс данных"
  },
  {
    "English": "data manifold",
    "context": "1: While all of these results constitute significant improvements in performance, they do come at a substantial resource cost whose fundamental origin arises from power law scaling with small exponents. Recent theoretical works [23,24,25] have argued that the power law exponent is governed by the dimension of a <mark>data manifold</mark> from which training examples are uniformly drawn.<br>2: As σ → 0, the trajectories become linear and point towards the <mark>data manifold</mark>. Discussion. The choices that we made in this section to improve deterministic sampling are summarized in the Sampling part of Table 1.<br>",
    "Arabic": "مشعب البيانات",
    "Chinese": "数据流形",
    "French": "variété de données",
    "Japanese": "データ多様体",
    "Russian": "многообразие данных"
  },
  {
    "English": "data mining",
    "context": "1: However, in many <mark>data mining</mark> tasks the goal is not to predict a single variable. For example, pattern discovery and clustering look at the structure of the whole data set. One could of course think of randomizing each column of the dataset independently, but this method ignores some of the structure of the dataset.<br>2: The framework of hypothesis testing in statistical data analysis is very well developed for assessing the significance of individual patterns or models. The methods are typically based either on analytical expressions or randomization tests. However, often they are not well-suited for assessing complex results of <mark>data mining</mark>, such as clusterings or pattern sets.<br>",
    "Arabic": "تنقيب البيانات",
    "Chinese": "数据挖掘",
    "French": "fouille de données",
    "Japanese": "データマイニング",
    "Russian": "добыча данных"
  },
  {
    "English": "data point",
    "context": "1: where H(•) is the classifier, C is the set of possible classes, p i ∈ R |C| is the output of the classifier for <mark>data point</mark> x i -e.g., p i [j] is the predicted probability of x i belonging to class j-andŷ i is the predicted label for x i .<br>2: The classification task is then to (correctly) predict the label for each <mark>data point</mark>: \n p i = H(x i ) (1) y i = arg max 1≤j≤|C| p i [j],(2) \n<br>",
    "Arabic": "نقطة البيانات",
    "Chinese": "数据点",
    "French": "point de données",
    "Japanese": "データポイント",
    "Russian": "точка данных"
  },
  {
    "English": "data processing inequality",
    "context": "1: The inequality (a) follows from the <mark>data processing inequality</mark>: if we have Markov chain X → Y →X, then I(X;X) ≤ I(X; Y ). Since H(X) = log|X |, from (57) we obtain \n<br>",
    "Arabic": "عدم مساواة معالجة البيانات",
    "Chinese": "数据处理不等式",
    "French": "inégalité dans le traitement des données",
    "Japanese": "データ処理の不等式",
    "Russian": "неравенство обработки данных"
  },
  {
    "English": "data sparseness",
    "context": "1: While this is an important initial result, it raises the question of the generality of the proposed approach to overcoming <mark>data sparseness</mark>. It remains to be shown that web counts are generally useful for approximating data that is sparse or unseen in a given corpus.<br>2: There has been recent promising work in using distributional representation of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight <mark>data sparseness</mark>. The model size grows only sub-linearly with the number of predicting features used.<br>",
    "Arabic": "ندرة البيانات",
    "Chinese": "数据稀疏性",
    "French": "rareté des données",
    "Japanese": "データの希薄性",
    "Russian": "разреженность данных"
  },
  {
    "English": "data sparsity",
    "context": "1: 3) Handling Data Sparsity: CF methods generally suffer from the <mark>data sparsity</mark> problem, i.e., the number of observed user-item interactions is scant compared to the total possible number of such interactions.<br>2: Category-agnostic methods -Puffball [35] and SIRFS [4] -consistently perform worse on the benchmark by themselves. Certain classes like \"boat\" and \"tvmonitor\" are especially hard because of large intraclass variance and <mark>data sparsity</mark> respectively.<br>",
    "Arabic": "ندرة البيانات",
    "Chinese": "数据稀疏性",
    "French": "rareté des données",
    "Japanese": "データ疎性",
    "Russian": "недостаточность данных"
  },
  {
    "English": "data structure",
    "context": "1: Approximate versions of the near neighbor search problem [15] were proposed to break the linear query time bottleneck. The following formulation is commonly adopted. Definition : ( c-Approximate Near Neighbor or c-NN ) Given a set of points in a D-dimensional space R D , and parameters S 0 > 0 , δ > 0 , construct a <mark>data structure</mark> which , given any query point q , does the following with probability 1 − δ : if there exists an S 0 -near neighbor of q in<br>2: Unfortunately, many of the algorithms construct a random <mark>data structure</mark> to store the facilities, then use this structure to resolve all queries; this type of approach implies that errors are not independent from one query to the next. Nonetheless we can obtain a constant approximation for sufficiently large choices of (3.<br>",
    "Arabic": "هيكل البيانات",
    "Chinese": "数据结构",
    "French": "structure de données",
    "Japanese": "データ構造",
    "Russian": "структура данных"
  },
  {
    "English": "data vector",
    "context": "1: Writing x ij for the <mark>data vector</mark> at location (i, j) in a particular layer, and y ij for the follow-ing layer, these functions compute outputs y ij by \n y ij = f ks ({x si+δi,sj+δj } 0≤δi,δj≤k ) \n<br>2: Thus for each <mark>data vector</mark> x i we obtained a 200-dimensional similarity feature vector s i .<br>",
    "Arabic": "متجه البيانات",
    "Chinese": "数据向量",
    "French": "vecteur de données",
    "Japanese": "データベクトル",
    "Russian": "вектор данных"
  },
  {
    "English": "data-to-text generation",
    "context": "1: We show that cycle training, when initialized with a small amount of supervised data (100 samples in our case), achieves nearly the same performance as fully supervised approaches for the <mark>data-to-text generation</mark> task on the WebNLG, E2E, WTQ, and WSQL datasets.<br>2: We showed that our approach substantially improved <mark>data-to-text generation</mark> performance in low-resource settings, achieved competitive performance compared to fully-supervised models, and also improved the faithfulness of the generated text through a reduction in factual errors, hallucinations and information misses, even when compared to fully supervised approaches.<br>",
    "Arabic": "توليد النص من البيانات",
    "Chinese": "数据到文本生成",
    "French": "génération de texte à partir de données",
    "Japanese": "データからテキスト生成",
    "Russian": "генерация текста из данных"
  },
  {
    "English": "datalog program",
    "context": "1: This is due to the fact that every entailment of a <mark>Datalog program</mark> has a finite witness, and that all of our query languages are positive, i.e., that their answers are preserved under homomorphisms of database instances. An important reasoning task on queries is to determine if a query contains another. In particular , a Datalog query P , Q is contained in a Datalog query P , Q , denoted P , Q P , Q , iff for each database instance I over the signature of EDB predicates and constants , the set of answers of P , Q over I is included in the set of answers of P , Q over<br>2: Promising directions for future research include the study of practical containment algorithms, since our automatabased techniques do not lend themselves to implementation yet. Another interesting topic is the search for suitable queries that contain a given query. A special case of this is the boundedness problem, where we try to find a UCQ that contains a given <mark>Datalog program</mark>.<br>",
    "Arabic": "برنامج داتالوغ",
    "Chinese": "数据库逻辑程序",
    "French": "programme Datalog",
    "Japanese": "データログプログラム",
    "Russian": "программа на языке Datalog"
  },
  {
    "English": "dataset augmentation",
    "context": "1: We have successfully validated our new decision forest model as stand-alone classifier on standard machine learning datasets and surpass stateof-the-art performance on ImageNet when integrating them in the GoogLeNet architecture, without any form of <mark>dataset augmentation</mark>.<br>",
    "Arabic": "توسيع مجموعة البيانات",
    "Chinese": "数据集增强",
    "French": "augmentation de jeu de données",
    "Japanese": "データセット拡張",
    "Russian": "увеличение набора данных"
  },
  {
    "English": "dataset bias",
    "context": "1: To mitigate the bias, prior works have focused on priming crowdsourcing annotators with minimal information to increase their imagination (Geva et al., 2021; to avoid recurring patterns. Arunkumar et al. (2020) develops a real time feedback and metric-in-the loop (Mishra et al., 2020b) workflow to educate crowdworkers in controlling <mark>dataset biases</mark>.<br>2: State-of-the-art neural models are highly effective at exploiting such artifacts to solve problems correctly, but for incorrect reasons. To tackle this persistent challenge with <mark>dataset biases</mark>, we propose AFLITEa novel algorithm that can systematically reduce biases using state-of-the-art contextual representation of words.<br>",
    "Arabic": "تحيز مجموعة البيانات",
    "Chinese": "数据集偏差",
    "French": "biais de jeu de données",
    "Japanese": "データセットのバイアス",
    "Russian": "смещение набора данных"
  },
  {
    "English": "dataset size",
    "context": "1: Consider the characteristics of the task, such as the <mark>dataset size</mark>, model architecture, and the complexity of the prediction task, when adjusting the parameters. 6. For tasks with larger datasets, a higher initial learning rate and lower momentum may be more suitable. 7.<br>2: Here we focus on the scaling of error with <mark>dataset size</mark> and show how in theory we can break beyond power law scaling and potentially even reduce it to exponential scaling instead if we have access to a high-quality data pruning metric that ranks the order in which training examples should be discarded to achieve any pruned <mark>dataset size</mark>.<br>",
    "Arabic": "حجم مجموعة البيانات",
    "Chinese": "数据集大小",
    "French": "taille de l'ensemble de données",
    "Japanese": "データセットサイズ",
    "Russian": "размер набора данных"
  },
  {
    "English": "datasheet",
    "context": "1: Countries to Write 600,000 Prompts \n Here we describe the competition, with a full <mark>datasheet</mark> (Gebru et al., 2018) for the collected dataset in Appendix E.<br>",
    "Arabic": "ورقة البيانات",
    "Chinese": "数据表",
    "French": "fiche de données",
    "Japanese": "データシート",
    "Russian": "документация данных"
  },
  {
    "English": "datum bias",
    "context": "1: Within the research community, addressing <mark>data bias</mark> requires a combination of new data sources, research that mitigates the impact of bias, and, as done in (Mitchell et al., 2019), auditing data and models. Sections 2 and 4.1 13 The General Data Protection Regulation of the European Union https://gdpr.eu/what-is-gdpr/.<br>2: Previous studies have primarily examined the connection between <mark>data bias</mark> and either model bias or downstream task performance, with the exception of Steed et al. (2022).<br>",
    "Arabic": "انحياز البيانات",
    "Chinese": "数据偏倚",
    "French": "biais de données",
    "Japanese": "データの偏り",
    "Russian": "смещение данных"
  },
  {
    "English": "datum clustering",
    "context": "1: The state-of-the-art methods combine these two strategies in novel ways, such as using statistical uncertainty in combination with some form of <mark>data clustering</mark> for diversity sampling (Zhang and Plank, 2021;Ash et al., 2019). Our work uses Contrastive Active Learning (Margatina et al., 2021) to represent this strategy.<br>",
    "Arabic": "تجميع البيانات",
    "Chinese": "数据聚类",
    "French": "regroupement de données",
    "Japanese": "データクラスタリング",
    "Russian": "кластеризация данных"
  },
  {
    "English": "datum contamination",
    "context": "1: Overall, we have made a best effort to measure and document the effects of <mark>data contamination</mark>, and to note or outright remove problematic results, depending on the severity. Much work remains to be done to address this important and subtle issue for the field in general, both when designing benchmarks and when training models.<br>2: We also undertake a systematic study of \"<mark>data contamination</mark>\" -a growing problem when training high capacity models on datasets such as Common Crawl, which can potentially include content from test datasets simply because such content often exists on the web. In this paper we develop systematic tools to measure <mark>data contamination</mark> and quantify its distorting effects.<br>",
    "Arabic": "تلوث البيانات",
    "Chinese": "数据污染",
    "French": "contamination des données",
    "Japanese": "データ汚染",
    "Russian": "загрязнение данных"
  },
  {
    "English": "datum dependency",
    "context": "1: A Task Graph (TG) G = G(T, D) is a directed acyclic graph where each node represents a computational module in the application referred to as task t i ∈ T . Each directed arc d i,j ∈ D, between tasks t i and t j , characterizes either data or control dependencies.<br>",
    "Arabic": "اعتماد البيانات",
    "Chinese": "数据依赖性",
    "French": "dépendance de données",
    "Japanese": "データ依存関係",
    "Russian": "зависимость данных"
  },
  {
    "English": "datum fidelity",
    "context": "1: Energy minimization methods have become the central paradigm for solving practical problems in computer vision. The energy functional can often be written as the sum of a <mark>data fidelity</mark> and a regularization term. One of the most popular regularizers is the total variation (T V ) due to its many favorable properties [4].<br>",
    "Arabic": "صِدق البيانات",
    "Chinese": "数据保真度",
    "French": "fidélité des données",
    "Japanese": "- Term: \"データの忠実度\"",
    "Russian": "точность данных"
  },
  {
    "English": "datum filtering",
    "context": "1: We perform ablation studies for each of the three stages in TRIPOST to better understand their contribution to model's overall performance. In Table 6 , we report the task accuracy when : interaction between M θ and LLM is removed , so that M θ is distilled with purely LLM demonstrations ( -interaction ) ; <mark>data filtering</mark> is removed ( -filtering ) ; dataset balancing is changed to using its own performance ( +auto-balance ) ; and the weights for SL are changed to be<br>",
    "Arabic": "تصفية البيانات",
    "Chinese": "数据筛选",
    "French": "-filtrage",
    "Japanese": "データのフィルタリング",
    "Russian": "обработка данных"
  },
  {
    "English": "datum generative process",
    "context": "1: On the data set side, we only consider images with a heavy focus on synthetic images. We do not explore other modalities and we only consider the toy scenario in which we have access to a <mark>data generative process</mark> with uniformly distributed factors of variations.<br>",
    "Arabic": "عملية توليد البيانات",
    "Chinese": "数据生成过程",
    "French": "processus génératif de données",
    "Japanese": "データ生成プロセス",
    "Russian": "процесс генерации данных"
  },
  {
    "English": "datum matrix",
    "context": "1: This simple approach turns out to yield very clear results on synthetic datasets with known cluster structure. A different notion of global structure is captured in the singular values and vectors of the <mark>data matrix</mark>. The singular vectors capture the linear trends in the dataset. The corresponding singular values capture the strength of the linear \n<br>2: Note that if inducing points are placed at the points associated to each column in the <mark>data matrix</mark>, then K uu = C and \n K T uf = C, so Q ff = CC −1 C T . Lemma 3.<br>",
    "Arabic": "مصفوفة البيانات",
    "Chinese": "数据矩阵",
    "French": "matrice de données",
    "Japanese": "データ行列",
    "Russian": "матрица данных"
  },
  {
    "English": "datum mining algorithm",
    "context": "1: As it is possible to predict properties of molecules, the same should be possible with reactions. The problem is to plug several molecules, reactants and products, in a <mark>data mining algorithm</mark>.<br>2: Given the dataset D, create random datasets with the same row and column margins D, run the <mark>data mining algorithm</mark> on those, and see if the results are significantly different on the real data than on the randomized datasets.<br>",
    "Arabic": "خوارزمية تنقيب البيانات",
    "Chinese": "数据挖掘算法",
    "French": "algorithme d'exploration de données",
    "Japanese": "データマイニングアルゴリズム",
    "Russian": "алгоритм интеллектуального анализа данных"
  },
  {
    "English": "datum parallelism",
    "context": "1: Specifically, our major contributions include: (1) A MapReduce statistical learning algorithm and implementation that achieve optimal <mark>data parallelism</mark>, task parallelism, and load balance in spite of the typically skewed distribution of domain data.<br>2: Additionally, we use mixed precision training for faster computation and less memory consumption. When using multiple GPUs, we apply <mark>data parallelism</mark> to split the minibatch across multiple GPUs and switch to use PyTorch's Synchronized Batch Normalization to track batch statistics across GPUs.<br>",
    "Arabic": "تَوازِي البَيانات",
    "Chinese": "数据并行",
    "French": "parallélisme des données",
    "Japanese": "データ並列",
    "Russian": "параллелизм данных"
  },
  {
    "English": "datum perturbation",
    "context": "1: The ultimate goal of certified robustness is to guarantee consistency on the model performance under <mark>data perturbation</mark>. In specific, it has to ensure that a small perturbation in the input does not change the predicted label. Given a benign example x, the robustness condition to l p (µ)-norm attacks can be stated as follows: \n<br>2: 2 Else, <mark>data perturbation</mark> methods can be applied [14]. Henceforth, for brevity we assume that all input data X is non-degenerate 3 As the title of [14] suggests, only a few outliers can be handled.<br>",
    "Arabic": "تشويش البيانات",
    "Chinese": "数据扰动",
    "French": "perturbation des données",
    "Japanese": "データ撹乱",
    "Russian": "пертурбация данных"
  },
  {
    "English": "davinci",
    "context": "1: Models perform remarkably well in the absence of distraction (i.e., on miniCOMPS-WUGS), but struggle in its presence, especially when it is closer to the queried prop- erty. In particular, performance on miniCOMPS-WUGS-DIST (before) increases with an increase in parameters until the largest model (<mark>davinci</mark>), where the performance drops closer to chance.<br>",
    "Arabic": "دا فينشي",
    "Chinese": "davinci模型",
    "French": "davinci",
    "Japanese": "ダ・ヴィンチ",
    "Russian": "davinci"
  },
  {
    "English": "decay parameter",
    "context": "1: Proposition 3. The mixture trace y t defined in (3) can be written as y t = µy t−1 + u t with <mark>decay parameter</mark> µ = ηγλ and signal u t = (1 − η)z θ (S t ) + η ∇ w v w (S t ), such that \n<br>2: The mixture trace y t defined in (3) can be written as y t = µy t−1 + u t with <mark>decay parameter</mark> µ = ηγλ and signal u \n<br>",
    "Arabic": "معلمة الاضمحلال",
    "Chinese": "衰减参数",
    "French": "paramètre de décroissance",
    "Japanese": "減衰パラメータ",
    "Russian": "параметр затухания"
  },
  {
    "English": "decentralized algorithm",
    "context": "1: In the case of non-smooth optimization, fast communication schemes were developed in [14,15], although precise optimal convergence rates were not obtained. Our <mark>decentralized algorithm</mark> is closely related to the recent primal-dual algorithm of [14] which enjoys fast communication rates in a decentralized and stochastic setting.<br>",
    "Arabic": "خوارزمية لامركزية",
    "Chinese": "分散算法",
    "French": "algorithme décentralisé",
    "Japanese": "分散型アルゴリズム",
    "Russian": "децентрализованный алгоритм"
  },
  {
    "English": "decentralized optimization",
    "context": "1: Yet, we still obtain the same convergence rate as [25], as provided by the following theorem. The same approach can be used to \"continuize\" other accelerated randomized gossip algorithms for decentralized optimization, such as ADFS [26]. Theorem 6 (Accelerated asynchronous decentralized optimization).<br>",
    "Arabic": "تحسين موزع",
    "Chinese": "分散优化",
    "French": "optimisation décentralisée",
    "Japanese": "分散最適化",
    "Russian": "децентрализованная оптимизация"
  },
  {
    "English": "decision Transformer",
    "context": "1: We collect 185 games containing 3,439 instructions. model architecture is based on the <mark>Decision Transformer</mark> (Chen et al., 2021;Putterman et al., 2022). Follower observations are embedded using HEX-ACONV (Hoogeboom et al., 2018) because of the hexagonal structure of the map.<br>",
    "Arabic": "محول القرار",
    "Chinese": "决策Transformer",
    "French": "Transformateur de décision",
    "Japanese": "\"決定Transformer\"",
    "Russian": "Преобразователь решений"
  },
  {
    "English": "decision boundary",
    "context": "1: Since the <mark>decision boundary</mark> for this problem is not smooth and requires tens of thousands of support vectors, the Fourier features perform quite poorly on this dataset.<br>2: That said, it is also straightforward to see that this <mark>decision boundary</mark> is realizable by simple two-layer networks. 6 We focus on this example as a demonstration of the applicability of our techniques to the analysis of the training dynamics for two-layer neural networks on natural data models.<br>",
    "Arabic": "حد القرار",
    "Chinese": "决策边界",
    "French": "frontière de décision",
    "Japanese": "決定境界",
    "Russian": "граница принятия решений"
  },
  {
    "English": "decision function",
    "context": "1: Given the parameters w, the decoding problem is to produce an adjacency matrixŷ = argmax y f (x, y) that maximizes a <mark>decision function</mark> f , subject to the constraint thatŷ be a consistent clustering. In standard correlation clustering, the objective is the intracluster similarity: \n<br>2: According to the constraint of Eq. ( 1 ) , we can define two functions f ( x ) and g ( x * ) , where f ( x ) = 1−y [ w , φ ( x ) +b ] denotes a hinge loss of the <mark>decision function</mark> = w , φ ( x ) + b on the example ( x , y ) and g (<br>",
    "Arabic": "دالة القرار",
    "Chinese": "决策函数",
    "French": "fonction de décision",
    "Japanese": "決定関数",
    "Russian": "функция принятия решения"
  },
  {
    "English": "decision node",
    "context": "1: When a sample x ∈ X reaches a <mark>decision node</mark> n it will be sent to the left or right subtree based on the output of d n (x; Θ). In standard decision forests, d n is binary and the routing is deterministic. In this paper we will consider rather a probabilistic routing, i.e.<br>2: Each <mark>decision node</mark> n ∈ N is instead assigned a decision function d n (•; Θ) : X → [0, 1] parametrized by Θ, which is responsible for routing samples along the tree.<br>",
    "Arabic": "عقدة القرار",
    "Chinese": "决策节点",
    "French": "nœud de décision",
    "Japanese": "決定ノード",
    "Russian": "узел принятия решений"
  },
  {
    "English": "decision policy",
    "context": "1: To explicitly connect selection rates to <mark>decision policies</mark>, we define the rate function r π (τ j ) which returns the proportion of group j selected by the policy.<br>2: Note that Y (0) is not necessarily zero, as a rejected applicant may attend-and graduate from-a different university. Given this setup, our goal is to construct <mark>decision policies</mark> d that are broadly equitable, formalized in part by the causal notions of fairness described below.<br>",
    "Arabic": "سياسة القرار",
    "Chinese": "决策策略",
    "French": "politique de décision",
    "Japanese": "意思決定方針",
    "Russian": "политика принятия решений"
  },
  {
    "English": "decision problem",
    "context": "1: If the resulting state is (F, f )-valid, either v * = ∞ and F is unsatisfiable, or v * is the optimal value (or v * = 0 for a satisfiable <mark>decision problem</mark>). If the resulting state is only weakly (F, f )-valid, we get slightly weaker conclusions.<br>2: And even if we are given just a <mark>decision problem</mark> for a formula F , we can still use this framework by inventing an objective function minimizing the lexicographic order of assignments, and then do symmetry breaking by adding dominance constraints with respect to this order.<br>",
    "Arabic": "مشكلة القرار",
    "Chinese": "决策问题",
    "French": "problème de décision",
    "Japanese": "決定問題",
    "Russian": "Задача принятия решения"
  },
  {
    "English": "decision rule",
    "context": "1: 1 Given a budget b with 0 < b < 1, we require the <mark>decision rule</mark> to satisfy E[D] ≤ b, limiting the expected proportion of positive decisions. In our running example, we imagine a population of applicants to a particular college, where d denotes an admissions rule and D indicates a binary admissions decision.<br>2: We see only modest performance improvements by using a random forest rather than a simple <mark>decision rule</mark> and we omit the details of those results here. We next learn classifiers that predict which dataset an account comes from within bot types in order to understand whether accounts from different datasets in the same category are substantively similar to one another.<br>",
    "Arabic": "قاعدة القرار",
    "Chinese": "决策规则",
    "French": "règle de décision",
    "Japanese": "決定規則",
    "Russian": "правило принятия решений"
  },
  {
    "English": "decision space",
    "context": "1: These systems include specific trainable modules within the generation framework to allow the model to adapt to different domains (Walker et al., 2007), or reproduce certain style (Mairesse and Walker, 2011). However, these approaches still require a handcrafted generator to define the <mark>decision space</mark> within which statistics can be used for optimisation.<br>2: (4) will be satisfied if x and x * are good enough for model training and a small loss f (x) in the <mark>decision space</mark> can be achieved.<br>",
    "Arabic": "فضاء القرار",
    "Chinese": "决策空间",
    "French": "espace de décision",
    "Japanese": "決定空間",
    "Russian": "пространство принятия решений"
  },
  {
    "English": "decision stump",
    "context": "1: To the best of our knowledge this is the first approach that is able to solve this problem. Our method is based on the idea that the triplets can be aggregated into simple triplet classifiers, which behave like <mark>decision stumps</mark> and are well-suited for boosting approaches [Schapire and Freund, 2012].<br>2: On the other hand, requiring more than 50% accuracy even when the number of labels is much larger than two is too stringent, and simple weak classifiers like <mark>decision stumps</mark> fail to meet this criterion, even though they often can be combined to produce highly accurate classifiers (Freund and Schapire, 1996a).<br>",
    "Arabic": "جذع القرار",
    "Chinese": "决策树桩",
    "French": "tronc de décision",
    "Japanese": "決定スタンプ",
    "Russian": "решающий пень"
  },
  {
    "English": "decision theory",
    "context": "1: That is, the observed behavior is considered to be the truth and the decision-making framework used by the producers and consumers is known a priori. The <mark>decision theory</mark> community is interested in human behavior on a more individual level. They, too, note that out-of-the-box game theory fails to explain how people act in many scenarios.<br>",
    "Arabic": "نظرية اتخاذ القرارات",
    "Chinese": "决策理论",
    "French": "théorie de la décision",
    "Japanese": "意思決定理論",
    "Russian": "теория принятия решений"
  },
  {
    "English": "decision tree",
    "context": "1: Markov chains or recommender systems have been studied by several researchers. Zimdars et al. [10] describe a sequential recommender based on Markov chains. They investigate how to extract sequential patterns to learn the next state with a standard predictor -e.g. a <mark>decision tree</mark>. Mobasher et al.<br>2: a <mark>decision tree</mark> . We also \"boosted\" the last three of these learners, and we discuss each of these methods in the following sections. Since the task is to detect malicious executables, in subsequent discussion, we refer to the malicious class as the positive class and refer to the benign class as the negative class.<br>",
    "Arabic": "شجرة القرار",
    "Chinese": "决策树",
    "French": "arbre de décision",
    "Japanese": "決定木",
    "Russian": "дерево решений"
  },
  {
    "English": "decision variable",
    "context": "1: satisfying Pr ( Y ( 0 ) = y 0 , Y ( 1 ) = y 1 , W = w ) > 0 . As above, expanding this expression and replacing d(x j ) by the corresponding <mark>decision variable</mark> d j yields linear constraints of the form \n<br>2: We use x k(j) to denote the value of the <mark>decision variable</mark> used to compute the gradient or subgradient that yields the state \n x j . In what follows, we provide conditions under which this asynchronous, incremental gradient algorithm converges.<br>",
    "Arabic": "متغير القرار",
    "Chinese": "决策变量",
    "French": "variable de décision",
    "Japanese": "決定変数",
    "Russian": "переменная решения"
  },
  {
    "English": "decode algorithm",
    "context": "1: As a final experiment, we assess whether the semantic biases are caused by search errors (i.e., failures of the <mark>decoding algorithm</mark>), or model errors (i.e., the models deemed their translations the best 18 The publicly available version trained on SemCor data only. possible). For each ( model M , language L ) pair , we sample a BAD translation ( t BAD ) , pair it with a GOOD translation ( t GOOD ) produced by another model ( prioritizing DeepL ) , and ask annotators to check their correctness and apply corrections where needed , 19 then compute the perplexities according to M with the corresponding<br>2: To study MAUVE's effectiveness as a measure for comparing text distributions, we first examine how MAUVE quantifies known properties of generated text: a good measure should meet expected behavior that is known from existing research on each property. Specifically, we investigate how MAUVE behaves under changes in generation length, <mark>decoding algorithm</mark>, and model size.<br>",
    "Arabic": "خوارزمية فك التشفير",
    "Chinese": "解码算法",
    "French": "algorithme de décodage",
    "Japanese": "復号アルゴリズム",
    "Russian": "алгоритм декодирования"
  },
  {
    "English": "decode strategy",
    "context": "1: The rise of autoregressive language models like GPT (Radford et al., 2018) has inspired work on <mark>decoding strategies</mark> (Post and Vilar, 2018a;Ippolito et al., 2019;Zheng et al., 2020;Leblond et al., 2021;.<br>",
    "Arabic": "استراتيجية فك التشفير",
    "Chinese": "解码策略",
    "French": "stratégie de décodage",
    "Japanese": "デコード戦略",
    "Russian": "стратегия декодирования"
  },
  {
    "English": "decoder hidden state",
    "context": "1: where c j0 is the context vector computed in Eq (3) using the first <mark>decoder hidden state</mark>.<br>",
    "Arabic": "الحالة الخفية لفك الشفرة",
    "Chinese": "解码器隐藏状态",
    "French": "état caché du décodeur",
    "Japanese": "デコーダー隠れ状態",
    "Russian": "состояние скрытого слоя декодера"
  },
  {
    "English": "decoder layer",
    "context": "1: We supervise mask prediction with a linear combination of focal loss [65] and dice loss [73] in a 20:1 ratio of focal loss to dice loss, following [20,14]. Unlike [20,14], we observe that auxiliary deep supervision after each <mark>decoder layer</mark> is unhelpful.<br>2: Each SRU++ <mark>decoder layer</mark> make uses of X src by simplying treating it as extra attention context. That is, the query, key and value 6 https://github.com/asappresearch/ imitkd/blob/master/configs/iwslt/ teacher.yaml representations are computed by concatenating the input of the current layer X tgt with X src , \n<br>",
    "Arabic": "طبقة المُرمِّز",
    "Chinese": "解码器层",
    "French": "couche de décodeur",
    "Japanese": "デコーダーレイヤー",
    "Russian": "слой декодера"
  },
  {
    "English": "decoder network",
    "context": "1: i ) , respectively , where E ( • ) denotes the encoder network and D ( • ) denotes the <mark>decoder network</mark> . The consistency-based regularization loss tries to distinguish the representations from the same token from those of other tokens, like contrastive learning [8].<br>2: Since the ground truth is unavailable in real-world situations, the expert labels in the dataset cannot be used for training the encoder. We introduce a <mark>decoder network</mark>, which is another neural network that reconstructs the encoder input.<br>",
    "Arabic": "شبكة فك التشفير",
    "Chinese": "解码器网络",
    "French": "réseau de décodage",
    "Japanese": "デコーダーネットワーク",
    "Russian": "декодерная сеть"
  },
  {
    "English": "decoder output",
    "context": "1: W d is the learnable decoder parameter,ṽ n is the <mark>decoder output</mark> before application of the activation function (see Equation ( 8)) andṽ \n n(t) \n<br>",
    "Arabic": "ناتج فك تشفير",
    "Chinese": "解码器输出",
    "French": "sortie du décodeur",
    "Japanese": "デコーダー出力",
    "Russian": "вывод декодера"
  },
  {
    "English": "decoder state",
    "context": "1: Given the <mark>decoder state</mark> s t , it first attends on the knowledge graph vectors {g 1 , g 2 , • • • , g N G } to compute the probability of using of each graph g i , which is defined as below: \n c g t = N G i=1 α g ti g i ,(9) \n<br>2: In the conventional streaming decoder inference, when the new speech encoder output h t is available, the new <mark>decoder state</mark> s l u (t) is estimated via h 1:t and previous computed <mark>decoder state</mark>sŝ l a ′ , which are based on h 1:t ′ and t ′ ≤ t, as shown in Eq.<br>",
    "Arabic": "حالة فك الترميز",
    "Chinese": "解码器状态",
    "French": "état du décodeur",
    "Japanese": "デコーダー状態",
    "Russian": "состояние декодера"
  },
  {
    "English": "decoder-only transformer",
    "context": "1: For CogView, we freeze the pre-trained VQ-VAE and initialized the main backbone, <mark>decoder-only transformer</mark>, from GPT (Radford et al., 2019;Brown et al., 2020). During training, only the parameters of the transformer backbone will be updated.<br>",
    "Arabic": "محول فك التشفير فقط",
    "Chinese": "解码器专用变压器",
    "French": "transformateur à décodeur uniquement",
    "Japanese": "デコーダーのみトランスフォーマー",
    "Russian": "декодер-трансформер"
  },
  {
    "English": "decoding problem",
    "context": "1: Second, we can only insert a zero-fertility word if it will increase the probability of a hypothesis. According to the definition of the <mark>decoding problem</mark>, a zero-fertility English word can only make a decoding more likely by increasing P(e) more than it decreases P(a,f ¤ e).<br>2: ) . (5 \n ) \n Given parameters w and a set of instances x, the <mark>decoding problem</mark> is to find the highest-scoring clusterinĝ \n y = argmax y f (x, y) s.t.<br>",
    "Arabic": "مشكلة فك التشفير",
    "Chinese": "解码问题",
    "French": "problème de décodage",
    "Japanese": "復号問題",
    "Russian": "проблема декодирования"
  },
  {
    "English": "decoding step",
    "context": "1: At each <mark>decoding step</mark>, Ar-caneQA only makes a prediction from a small set of admissible tokens instead of the entire vocabulary. This is achieved by the dynamic program induction framework (subsection 4.2), which effectively prunes the search space by orders of magnitude and guarantees that the predicted programs are faithful to the KB.<br>2: We denote it as y S = (y S 1 , ..., y S |y S | ), then at the j-th <mark>decoding step</mark>, we define the sentence-level oracle word as \n + = Noise • • • • • • + • • • Predicted Score y oracle j−1 1-best<br>",
    "Arabic": "خطوة فك تشفير",
    "Chinese": "解码步骤",
    "French": "étape de décodage",
    "Japanese": "デコーディングステップ",
    "Russian": "шаг декодирования"
  },
  {
    "English": "decomposable attention",
    "context": "1: The <mark>Decomposable Attention</mark> model (Parikh et al., 2016) is one of the earliest models to introduce attentionbased alignment for sentence pair modeling, and it achieved state-of-the-art results on the SNLI dataset with about an order of magnitude fewer parameters than other models (see more in Table 5) without relying on word order information. It computes the word pair interaction between w a i and w b j ( from input sentences s a and s b , each with m and n words , respectively ) as e ij = F ( w a i ) T F ( w b j ) , where F is a feedforward network ; then alignment is determined as follows<br>2: Note that the alternate constructions do not impact the unbiased label (neutral). Any change in construction (say negating a verb) is applied to both the premise and hypothesis. Refer to App. B for a detailed description. Experimental Results : We use RoBERTa trained on SNLI ( RoBERTa-base-SNLI ) ( Liu et al. , 2019 ) , ELMo-based <mark>Decomposable Attention</mark> ( ELMo-DA ) ( Parikh et al. , 2016 ) , ALBERT ( Lan et al. , 2019 ) , distilled version of the RoBERTa-base model ( Sanh et al. , 2019 ) , and RoBERTa-large finetuned on WANLI ( Liu<br>",
    "Arabic": "انتباه قابل للتفكيك",
    "Chinese": "可分解式注意力",
    "French": "attention décomposable",
    "Japanese": "分解可能な注意",
    "Russian": "декомпозируемое внимание"
  },
  {
    "English": "decomposable attention model",
    "context": "1: This way, we test two such systems in our experiments: a simple embeddingbased model that computes the cosine similarity between the centroids of each sentence after discarding stopwords, and the <mark>Decomposable Attention Model</mark> (DAM) proposed by Parikh et al. (2016) and minimally adapted for the task 15 .<br>",
    "Arabic": "نموذج الانتباه القابل للتحلل",
    "Chinese": "可分解注意力模型",
    "French": "modèle d'attention décomposable",
    "Japanese": "分解可能な注意モデル",
    "Russian": "декомпозируемая модель внимания"
  },
  {
    "English": "decomposition",
    "context": "1: Algorithms We compare to three baselines: CUCB (Chen et al. 2016), zooming (Kleinberg, Slivkins, and Upfal 2019), and MINION (Gholami et al. 2018). Zooming is an online learning algorithm that ignores decomposability, whereas CUCB uses <mark>decomposition</mark> but ignores similarity between arms.<br>2: (Decomposition of MSE Loss; Proof in Appendix B) The MSE loss, L( W , H), can be decomposed into two terms, L( W , H) = L LS ( H) + L ⊥ LS ( W , H),where \n<br>",
    "Arabic": "تقسيم",
    "Chinese": "分解",
    "French": "décomposition",
    "Japanese": "分解",
    "Russian": "декомпозиция"
  },
  {
    "English": "decomposition method",
    "context": "1: In most machine learning applications, tolerating a training error that is suboptimal by 0.1% is very acceptable. This intuition makes selecting the stopping criterion much easier than in <mark>decomposition methods</mark>, where it is usually defined based on the accuracy of the Kuhn-Tucker Conditions of the dual (see e.g. [11]).<br>2: This bound was later improved by Smola et al [33] to O(md/(λ )). The complexity guarantee for Pegasos avoids the dependence on the data set size m. In addition, while SVM-Perf yields very significant improvements over <mark>decomposition methods</mark> for large data sets, our experiments (see Sec.<br>",
    "Arabic": "طريقة تحليلية",
    "Chinese": "分解方法",
    "French": "méthode de décomposition",
    "Japanese": "分解法",
    "Russian": "метод декомпозиции"
  },
  {
    "English": "deconvolution",
    "context": "1: • Conv(c in , c out , k, s, p): convolution with c in input channels, c out output channels, kernel size k, stride s and padding p. \n • Deconv ( c in , c out , k , s , p ) : <mark>deconvolution</mark> [ 70 ] with c in input channels , c out output channels , kernel size k , stride s and padding p. The output channel size c out is 6 for viewpoint , corresponding to rotation angles w 1:3 and translations w 4:6 in x ,<br>2: We append a 1 × 1 convolution with channel dimension 21 to predict scores for each of the PAS-CAL classes (including background) at each of the coarse output locations, followed by a <mark>deconvolution</mark> layer to bilinearly upsample the coarse outputs to pixel-dense outputs as described in Section 3.3.<br>",
    "Arabic": "إزالة الطي",
    "Chinese": "反卷积",
    "French": "déconvolution",
    "Japanese": "逆畳み込み",
    "Russian": "деконволюция"
  },
  {
    "English": "deconvolution layer",
    "context": "1: As an efficient, effective alternative, we introduce <mark>deconvolution layers</mark> for upsampling in Section 3.3. In Section 3.4 we consider training by patchwise sampling, and give evidence in Section 4.3 that our whole image training is faster and equally effective.<br>2: Class Balancing Fully convolutional training can balance classes by weighting or sampling the loss. Although our labels are mildly unbalanced (about 3/4 are background), we find class balancing unnecessary. Dense Prediction The scores are upsampled to the input dimensions by <mark>deconvolution layers</mark> within the net.<br>",
    "Arabic": "طبقة فك الالتفاف",
    "Chinese": "卷积转置层",
    "French": "couche de déconvolution",
    "Japanese": "デコンボリューション層",
    "Russian": "слои деконволюции"
  },
  {
    "English": "deconvolutional layer",
    "context": "1: The upsampling and biasing processes are defined as follows. In the upsampling process, one uses a convolutional network with <mark>deconvolutional layers</mark> to construct an enlarged feature map of size c × n × n, where c is the number of features in the output map of the upsampling network.<br>",
    "Arabic": "طبقة إزالة الطي",
    "Chinese": "解卷积层",
    "French": "couche de déconvolution",
    "Japanese": "逆畳み込み層",
    "Russian": "слой деконволюции"
  },
  {
    "English": "deep architecture",
    "context": "1: They can be viewed as a new type of <mark>deep architecture</mark>, where sum layers alternate with product layers. Deep networks have many layers of hidden variables, which greatly increases their representational power, but inference with even a single layer is generally intractable, and adding layers compounds the problem [3].<br>2: We formulate a complete model to learn the feature hierarchies so that graph matching works best: the feature learning and the graph matching model are refined in a single <mark>deep architecture</mark> that is optimized jointly for consistent results.<br>",
    "Arabic": "الهندسة المعمارية العميقة",
    "Chinese": "深度架构",
    "French": "architecture profonde",
    "Japanese": "深層アーキテクチャ",
    "Russian": "глубокая архитектура"
  },
  {
    "English": "deep convolutional network",
    "context": "1: We introduce a stochastic, differentiable, and therefore backpropagation compatible version of decision trees, guiding the representation learning in lower layers of <mark>deep convolutional networks</mark>. Thus, the task for representation learning is to reduce the uncertainty on the routing decisions of a sample taken at the split nodes, such that a globally defined loss function is minimized.<br>2: We demonstrate that representing scenes as 5D neural radiance fields (an MLP that outputs volume density and view-dependent emitted radiance as a function of 3D location and 2D viewing direction) produces better renderings than the previously-dominant approach of training <mark>deep convolutional networks</mark> to output discretized voxel representations.<br>",
    "Arabic": "شبكة تلافيفية عميقة",
    "Chinese": "深度卷积网络",
    "French": "réseau convolutionnel profond",
    "Japanese": "深層畳み込みネットワーク",
    "Russian": "глубокая сверточная нейронная сеть"
  },
  {
    "English": "deep convolutional neural network",
    "context": "1: 2015) used constraint learning to train deep networks in a natural language setting. Sentiment labels on reviews were used to analyze the sentiment of individual sentences comprising those reviews. (Lin et al. ) and (Zhuang et al. 2016) trained <mark>deep convolutional neural networks</mark> to construct high level compressed embeddings of images without using labels.<br>2: Deep convolutional neural networks [22,21] have led to a series of breakthroughs for image classification [21,50,40]. Deep networks naturally integrate low/mid/highlevel features [50] and classifiers in an end-to-end multilayer fashion, and the \"levels\" of features can be enriched by the number of stacked layers (depth).<br>",
    "Arabic": "شبكات عصبية تلافيفية عميقة",
    "Chinese": "深度卷积神经网络",
    "French": "réseau neuronal convolutionnel profond",
    "Japanese": "深層畳み込みニューラルネットワーク",
    "Russian": "глубокая сверточная нейронная сеть"
  },
  {
    "English": "deep feature",
    "context": "1: Our previous work introduces a lightweight statistical pipeline that repurposes off-the-shef (OTS) <mark>deep features</mark> for open-set recognition [33].<br>2: Leveraging the power of <mark>deep features</mark>, this translates into featuremetric keypoint and bundle adjustments that elegantly integrate into any SfM pipeline by replacing their geometric counterparts. Figure 2 shows an overview. We first introduce the featuremetric optimization in Section 4.1.<br>",
    "Arabic": "الميزة العميقة",
    "Chinese": "深度特征",
    "French": "\"descripteur profond\"",
    "Japanese": "深層特徴量",
    "Russian": "глубокий признак"
  },
  {
    "English": "deep generative model",
    "context": "1: Conditional Variational Autoencoder (cVAE) uses amortized variational inference to fit a <mark>deep generative model</mark> that is conditioned on the input and can produce samples from a complex predictive distribution.<br>",
    "Arabic": "النموذج التوليدي العميق",
    "Chinese": "深度生成模型",
    "French": "modèle génératif profond",
    "Japanese": "深層生成モデル",
    "Russian": "глубокая генеративная модель"
  },
  {
    "English": "deep layer",
    "context": "1: In § 2.2, we have affirmed that the model's shallow layers assemble information from demonstrations via label words to form semantic representations. In § 2.3, we verify that the aforementioned aggregated information on label words is then extracted to form the final prediction in the <mark>deep layers</mark>.<br>2: The features in <mark>deep layers</mark> of the network are automatically trained by backpropagation to be relevant to the task. We describe in this section a general deep architecture suitable for all our NLP tasks, and easily generalizable to other NLP tasks. Our architecture is summarized in Figure 1. The first layer extracts features for each word.<br>",
    "Arabic": "الطبقة العميقة",
    "Chinese": "深层",
    "French": "couche profonde",
    "Japanese": "深層",
    "Russian": "глубокий слой"
  },
  {
    "English": "deep learning architecture",
    "context": "1: In the bilateral solver, the memory requirements are small and independent of the number of iterations, as we only need to store the bilateral-space output of the solverŷ during training. These properties make the bilateral solver an attractive option for <mark>deep learning architectures</mark> where speed and memory usage are important.<br>2: We first lay out theoretical, linguistically motivated hypotheses, and supporting empirical evidence on the nature of the challenges posed by indefinite pronouns to English learners. We then suggest and evaluate an automatic approach for detection of atypical usage patterns, demonstrating that <mark>deep learning architectures</mark> are promising for this task involving nuanced semantic anomalies.<br>",
    "Arabic": "معمارية التعلم العميق",
    "Chinese": "深度学习架构",
    "French": "architecture d'apprentissage profond",
    "Japanese": "ディープラーニングアーキテクチャ",
    "Russian": "архитектура глубокого обучения"
  },
  {
    "English": "deep learning framework",
    "context": "1: Integrating any operation into a <mark>deep learning framework</mark> requires that it is possible to backpropagate through that operation. Backpropagating through global operators such as our bilateral solver is generally understood to be difficult, and is an active research area [18]. Unlike most global smoothing operators, our model is easy to backpropagate through by construction.<br>",
    "Arabic": "إطار التعلم العميق",
    "Chinese": "深度学习框架",
    "French": "cadre d'apprentissage profond",
    "Japanese": "ディープラーニングフレームワーク",
    "Russian": "фреймворк глубокого обучения"
  },
  {
    "English": "deep learning model",
    "context": "1: If a single <mark>deep learning model</mark> is capable of-through knowledge distillation-learning the features of the ensemble model and achieving better test accuracy comparing to training the single model directly ( and the same training accuracy , typically at global optimal of 100 % ) , then why the single model can not learn these features directly when we train the model to match the<br>",
    "Arabic": "نموذج التعلم العميق",
    "Chinese": "深度学习模型",
    "French": "modèle d'apprentissage profond",
    "Japanese": "深層学習モデル",
    "Russian": "модель глубокого обучения"
  },
  {
    "English": "deep learning system",
    "context": "1: In the following experiments we integrated our novel forest classifiers in end-to-end image classification pipelines, using multiple convolutional layers for representation learning as typically done in <mark>deep learning systems</mark>.<br>2: We show that similar results hold for the local minima of the nonconvex costs typical of modern <mark>deep learning systems</mark>, and also hold when the family Q is infinite. Let (z, w) be the loss of a machine learning model where w ∈ R d represent the parameters of the model and z ∈ R n are examples.<br>",
    "Arabic": "نظام التعلم العميق",
    "Chinese": "深度学习系统",
    "French": "système d'apprentissage profond",
    "Japanese": "ディープラーニングシステム",
    "Russian": "система глубокого обучения"
  },
  {
    "English": "deep model",
    "context": "1: [51] learned face representation with a <mark>deep model</mark> through face identification, which is a challenging multi-class prediction task. Taigman et al. [52] first utilized explicit 3D face modeling to apply a piecewise affine transformation, and then derived a face representation from a nine-layer deep neural network.<br>2: We describe the steps necessary to carry out its deployment for large scale models -this opens the door to a surprising array of applications ranging from conditioning one <mark>deep model</mark> w.r.t. another, learning disentangled representations as well as optimizing diverse models that would directly be more robust to adversarial attacks.<br>",
    "Arabic": "النموذج العميق",
    "Chinese": "深度模型",
    "French": "modèle profond",
    "Japanese": "深層モデル",
    "Russian": "глубокая модель"
  },
  {
    "English": "deep net",
    "context": "1: The unconstrained features model is so-name because features are not constrained to be the output of a <mark>deep net</mark> forward pass but are rather free variables that can be optimized directly: This captures the nearly unlimited flexibility afforded to feature engineering transformations in modern <mark>deep net</mark>s by the many nonlinear, overparameterized layers.<br>2: (2021), the authors introduce the (N -)layer-peeled model in which one considers only the direct optimization of the N -th-to-last layer features of a <mark>deep net</mark> along with the weights that come after the N -th-to-last layer.<br>",
    "Arabic": "شبكة عميقة",
    "Chinese": "深度网络",
    "French": "réseau profond",
    "Japanese": "深層ネット",
    "Russian": "Глубокая нейросеть"
  },
  {
    "English": "deep network",
    "context": "1: Designing a neural network that can achieve realtime matting on high-resolution videos of people is extremely challenging, especially when fine-grained details like strands of hair are important; in contrast, the previous state-of-the-art method [28] is limited to 512×512 at 8fps. Training a <mark>deep network</mark> on such a large resolution is extremely slow and memory intensive.<br>2: In practice, we build an end-to-end <mark>deep network</mark> that integrates a feature extracting component that outputs the required descriptors F for building the matrix M. We solve the assignment problem (2) and compute a matching loss L(v * ) between the solution v * and the ground-truth.<br>",
    "Arabic": "شبكة عميقة",
    "Chinese": "深度网络",
    "French": "réseau profond",
    "Japanese": "深層ネットワーク",
    "Russian": "глубокая сеть"
  },
  {
    "English": "deep network architecture",
    "context": "1: On the resulting scene crops, we employ our novel 3D orientation estimation algorithm, which is based on a previously trained <mark>deep network architecture</mark>. While deep networks are also used in existing approaches, our approach differs in that we do not explicitly learn from 3D pose annotations during training.<br>2: We rely on the VGG-16 architecture from [29], that is pretrained to perform classification in the ImageNet ILSVRC [28] but we can use any other <mark>deep network architecture</mark>. We implement our deep learning framework in MatConvNet [31].<br>",
    "Arabic": "بنية الشبكة العميقة",
    "Chinese": "深度网络架构",
    "French": "architecture de réseau profond",
    "Japanese": "深層ネットワークアーキテクチャ",
    "Russian": "глубокая сетевая архитектура"
  },
  {
    "English": "deep neural model",
    "context": "1: We take advantage of two largely independent lines of work: on one hand, an extensive literature on answering questions by mapping from strings to logical representations of meaning; on the other, a series of recent successes in <mark>deep neural models</mark> for image recognition and captioning.<br>",
    "Arabic": "نموذج عصبي عميق",
    "Chinese": "深度神经网络模型",
    "French": "modèle neuronal profond",
    "Japanese": "深層ニューラルモデル",
    "Russian": "глубокая нейронная модель"
  },
  {
    "English": "deep neural net",
    "context": "1: . The RL can be defined as a function ν which produces summary statistics given I D or I R , and may also have internal parameters θ ν (e.g. weights of a <mark>deep neural net</mark>).<br>",
    "Arabic": "شبكة عصبية عميقة",
    "Chinese": "深度神经网络",
    "French": "réseau de neurones profond",
    "Japanese": "深層ニューラルネット",
    "Russian": "глубокая нейронная сеть"
  },
  {
    "English": "deep neural network",
    "context": "1: All methods use a Gaussian encoder where the mean and the log variance of each latent factor is parametrized by the <mark>deep neural network</mark>, a Bernoulli decoder and latent dimension fixed to 10. We note that these are all standard choices in prior work (Higgins et al., 2017a;Kim & Mnih, 2018).<br>2: The DQN, in which the Q-function is approximated using a <mark>deep neural network</mark>, has been shown to learn better value functions than linear approximators (Narasimhan et al., 2015;He et al., 2015) and can capture non-linear interactions between the different pieces of information in our state.<br>",
    "Arabic": "شبكة عصبية عميقة",
    "Chinese": "深度神经网络",
    "French": "réseau de neurones profond",
    "Japanese": "深層ニューラルネットワーク",
    "Russian": "глубокая нейронная сеть"
  },
  {
    "English": "deep q-learning",
    "context": "1: However, <mark>deep Q-learning</mark> is far from robust, and can rarely be applied successfully by inexperienced users. Modifications to mitigate systematic risks in Q-learning include double Q-learning [30], distributional Q-learning [4], and dueling network architectures [32].<br>2: Notable examples include <mark>deep Q-learning</mark> (Mnih et al., 2015), deep visuomotor policies (Levine et al., 2015), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al., 2015).<br>",
    "Arabic": "تعلم Q عميق",
    "Chinese": "深度Q学习",
    "French": "deep Q-learning",
    "Japanese": "深層Q学習",
    "Russian": "глубокое Q-обучение"
  },
  {
    "English": "deep q-network",
    "context": "1: We use a reinforcement learning framework and learn optimal action sequences to maximize extraction accuracy while penalizing extra effort. We show that our model, trained as a <mark>deep Q-network</mark>, outperforms traditional extractors by 7.2% and 5% on average on two different domains, respectively.<br>2: For comparison we also show results for the <mark>deep Q-network</mark> of Mnih et al. (2015), referred to as Nature DQN. Figure 4 shows the improvement of the dueling network over the baseline Single network of van Hasselt et al. (2015). Again, we seen that the improvements are often very dramatic.<br>",
    "Arabic": "شبكة كيو العميقة",
    "Chinese": "深度 Q 网络",
    "French": "réseau Q profond",
    "Japanese": "深層Qネットワーク",
    "Russian": "глубокая q-сеть"
  },
  {
    "English": "deep reinforcement learning",
    "context": "1: This extension presents a variety of technical challenges, requiring analogues of these methods that can be trained from sparse, non-differentiable reward signals without demonstrations of desired system behavior. Our contributions are: \n • A general paradigm for multitask, hierarchical, <mark>deep reinforcement learning</mark> guided by abstract sketches of task-specific policies.<br>2: We describe a framework for multitask <mark>deep reinforcement learning</mark> guided by policy sketches. Sketches annotate tasks with sequences of named subtasks, providing information about high-level structural relationships among tasks but not how to implement them-specifically not providing the detailed guidance used by much previous work on learning policy abstractions for RL (e.g.<br>",
    "Arabic": "التعلم التعزيزي العميق",
    "Chinese": "深度强化学习",
    "French": "apprentissage par renforcement profond",
    "Japanese": "深層強化学習",
    "Russian": "глубокое обучение с подкреплением"
  },
  {
    "English": "deep supervision",
    "context": "1: The benefits of <mark>deep supervision</mark> have previously been shown in deeply-supervised nets (DSN; [20]), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn discriminative features.<br>2: Below we discuss some possible alternatives in architecture design, and in particular, the role of <mark>deep supervision</mark> of HED for the edge detection task. FCN and skip-layer architecture The topology used in the FCN model differs from that in our HED model in several aspects.<br>",
    "Arabic": "التوجيه العميق",
    "Chinese": "深度监督",
    "French": "supervision profonde",
    "Japanese": "ディープ・スーパービジョン",
    "Russian": "глубокая надзорность"
  },
  {
    "English": "deeply-supervise net",
    "context": "1: Our proposed method, holistically-nested edge detection (HED), performs image-to-image prediction by means of a deep learning model that leverages fully convolutional neural networks and <mark>deeply-supervised nets</mark>. HED automatically learns rich hierarchical representations (guided by deep supervision on side responses) that are important in order to resolve the challenging ambiguity in edge and object boundary detection.<br>2: inspired by <mark>deeply-supervised nets</mark> [ 23 ] , that performs deep layer supervision to `` guide '' early classification results . We find that the favorable characteristics of these underlying techniques manifest in HED being both accurate and computationally efficient.<br>",
    "Arabic": "شبكة إشراف عميق",
    "Chinese": "深度监督网络",
    "French": "réseau à supervision profonde",
    "Japanese": "深層監視ネット",
    "Russian": "глубоко-надзорная сеть"
  },
  {
    "English": "default logic",
    "context": "1: Recently , Lakemeyer and Levesque ( 2005 ) proposed a logic of only-knowing called O 3 L , which precisely captures three forms of nonmonotonic reasoning : Moore 's Autoepistemic Logic ( AEL ) ( Moore 1985 ) , Konolige 's variant of AEL using moderately grounded expansions ( Konolige 1988 ) , and Reiter 's <mark>default logic</mark> ( DL ) ( Reiter 1980<br>2: Lifting this notion of explanation to general approximators would benefit domains of logics that are covered by AFT but not by JT, such as auto-epistemic logic [Moore, 1985] and <mark>default logic</mark> [Reiter, 1980]. A last question that emerges naturally is how nesting of justification frames, as defined by Denecker et al.<br>",
    "Arabic": "المنطق الافتراضي",
    "Chinese": "缺省逻辑",
    "French": "logique par défaut",
    "Japanese": "デフォルト論理",
    "Russian": "логика умолчаний"
  },
  {
    "English": "deformable template",
    "context": "1: The concept of <mark>deformable templates</mark> [10] is an important element in object recognition. In this article, we present a generative model and a model-based algorithm for learning <mark>deformable templates</mark> from image patches of various object categories. The machinery we adopt is the wavelet sparse coding model [7] and the matching pursuit algorithm [5].<br>2: The contributions of this paper are: (1) An active basis model for representing <mark>deformable templates</mark>. (2) A shared pursuit algorithm for learning <mark>deformable templates</mark>. (3) A theoretical framework that integrates sparse coding and random fields.<br>",
    "Arabic": "قالب قابل للتشوه",
    "Chinese": "可变形模板",
    "French": "modèle déformable",
    "Japanese": "変形可能なテンプレート",
    "Russian": "деформируемые шаблоны"
  },
  {
    "English": "deformation field",
    "context": "1: I p (W (x; p)) = T (x)(1) \n We locally parameterize the <mark>deformation field</mark> W (x; p) at any 2D point x by a weighted linear combination of displacements p = [p(1), p(2), . . .<br>2: Some approaches, such as Nerfies [49] and HyperNeRF [50], represent scenes using a <mark>deformation field</mark> mapping each local observation to a canonical scene representation.<br>",
    "Arabic": "حقل التشوه",
    "Chinese": "形变场",
    "French": "champ de déformation",
    "Japanese": "変形場",
    "Russian": "поле деформации"
  },
  {
    "English": "degree distribution",
    "context": "1: Indeed, it can be easily verified that the restriction of the unnormalized form of π to F constitutes an unnormalized form of π F . Finally, the approximate trial distribution used by the two procedures is the restriction of the <mark>degree distribution</mark> d P to F : \n<br>2: Here, ε t is an upper bound on the total variation distance between the distribution of the node visited at the t-th real step and the <mark>degree distribution</mark> d F . u F is the uniform distribution over F . The proof appears in Appendix D. \n<br>",
    "Arabic": "توزيع الدرجة",
    "Chinese": "度分布",
    "French": "distribution des degrés",
    "Japanese": "\"次数分布\"",
    "Russian": "распределение степеней"
  },
  {
    "English": "delay reward",
    "context": "1: r(h) outputs a realvalued score that correlates with correct action selection. 3 We consider both immediate reward, which is available after each action, and <mark>delayed reward</mark>, which does not provide feedback until the last action.<br>",
    "Arabic": "مكافأة التأخير",
    "Chinese": "延迟奖励",
    "French": "récompense différée",
    "Japanese": "遅延報酬",
    "Russian": "отсроченная награда"
  },
  {
    "English": "delexicalization",
    "context": "1: (2017) propose a multi-domain approach using two-layer bi-GRU. Although it does not need an ad-hoc state update mechanism, it relies on <mark>delexicalization</mark> to extract the features. propose a model to jointly track domain and the dialogue states using multiple bi-LSTM.<br>2: However, if we add back just the cluster features, the accuracy jumps back up to 89.5%, which is only 1.2% below the full system. Thus, if we can accurately learn cross-lingual clusters, there is hope of regaining some of the accuracy lost due to the <mark>delexicalization</mark> process.<br>",
    "Arabic": "إزالة المفردات",
    "Chinese": "去词汇化",
    "French": "délexicalisation",
    "Japanese": "非語彙化",
    "Russian": "делексикализация"
  },
  {
    "English": "delta kernel",
    "context": "1: 1 is the no-blur explanation: k is the delta (identity) kernel and x = y. The ill-posed nature of the problem implies that additional assumptions on x or k must be introduced.<br>2: 4(b) plots P (y|k), which is essentially summing the columns of Fig. 4(a). Now consider blur in real images: for the <mark>delta kernel</mark> there is only a single solution x = y satisfying k ⊗ x = y.<br>",
    "Arabic": "نواة دلتا",
    "Chinese": "狄拉克核",
    "French": "noyau delta",
    "Japanese": "デルタカーネル",
    "Russian": "дельта-ядро"
  },
  {
    "English": "demographic parity",
    "context": "1: For example, <mark>Demographic Parity</mark> (DP) requires that the prediction is independent of sensitive features, while Equalized Odds (EO) and Equalized Opportunity (EOp) require that the prediction is conditionally independent of sensitive features in each or some label group.<br>2: Achieving a small <mark>Demographic Parity</mark> may be thought of as a stronger version of the US Equal Employment Opportunity Commission's \"four-fifths rule\". 2 To focus on query complexity , we will abstract away the difficulty of evaluating µ by assuming that D X is known , and thus for any h , we may evaluate µ ( h ) to arbitrary precision ; for instance , this may be achieved with the availability of an arbitrarily large number of ( unlabeled ) samples randomly drawn from<br>",
    "Arabic": "التكافؤ الديموغرافي",
    "Chinese": "人口统计平等",
    "French": "parité démographique",
    "Japanese": "人口統計的均等性",
    "Russian": "демографическое равенство"
  },
  {
    "English": "dendrogram",
    "context": "1: The <mark>dendrogram</mark> on top of the figure shows how the abnormal images in our dataset can be grouped to make three clusters, reflecting three major latent categories of abnormality. Each cluster corresponds to a specific list of abnormality reasons. Details of these three categories of abnormality can be found in Table 2.<br>2: Hierarchical clustering produces a <mark>dendrogram</mark> where two clusters are merged together at each level. The <mark>dendrogram</mark> allows a user to explore the pattern space in a top-down manner and provides a global view of patterns. calculate the KL-divergence between C and the remaining clusters; 10: \n<br>",
    "Arabic": "مخطط شجري",
    "Chinese": "树状图",
    "French": "dendrogramme",
    "Japanese": "デンドログラム",
    "Russian": "дендрограмма"
  },
  {
    "English": "denoise diffusion probabilistic model",
    "context": "1: Diffusion models [19,43], also known as <mark>denoising diffusion probabilistic models</mark> (DDPMs), are a family of deep generative models. DM recovers the originally observed data distribution from the perturbed data distribution with gradually injected noise by recurrently denoising the noise of each perturbation step.<br>",
    "Arabic": "نموذج الانتشار الاحتمالي لإزالة التشويش",
    "Chinese": "去噪扩散概率模型",
    "French": "modèle probabiliste de diffusion de débruitage",
    "Japanese": "デノイズ拡散確率モデル",
    "Russian": "модель вероятностного шумоподавляющего диффузионного процесса (DDPM)"
  },
  {
    "English": "denoise network",
    "context": "1: In a typical denoising process, the <mark>denoising network</mark> relies on the condition C and Y t to estimatê Y 0 . However, at early steps, [MASK] tokens generally occupy the majority of Y t , causing the estimation to be more difficult.<br>2: Our algorithm requires two training processes for: the <mark>denoising network</mark> and the policy network (and value network). For training the <mark>denoising network</mark>, we follow the common practice that uses 87,000 overlapping patches (with size 128 × 128) drawn from 400 images from the BSD dataset (Martin et al., 2001).<br>",
    "Arabic": "شبكة إزالة التشويش",
    "Chinese": "去噪网络",
    "French": "réseau de débruitage",
    "Japanese": "雑音除去ネットワーク",
    "Russian": "сеть дешумлирования"
  },
  {
    "English": "denoise objective",
    "context": "1: A standard recipe has emerged for achieving high scores on such benchmarks. A neural network-typically, one based on the transformer architecture (Vaswani et al., 2017)-is pretrained on a <mark>denoising objective</mark>, such as filling in one or more blanks in a vast number of sentences.<br>2: Flan-T5 is based on T5, a sequence-to-sequence model trained on a <mark>denoising objective</mark>, that has been further instruction-finetuned on a battery of tasks. This has been shown to promote better responses to instructions, both with and without demonstrations (Chung et al., 2022).<br>",
    "Arabic": "هدف تقليل الضوضاء",
    "Chinese": "去噪目标",
    "French": "objectif de débruitage",
    "Japanese": "ノイズ除去目標",
    "Russian": "цель деноизинга"
  },
  {
    "English": "denoise process",
    "context": "1: As shown in Table 1, these studies typically adopt the continuous diffusion method on the latent space of token embeddings in the NAR manner, and iteratively refine all the target token embeddings via a parameterized <mark>denoising process</mark>.<br>2: Thus, the model is able to efficiently extract dense detail information, as well as effectively capture 3D spatial features for accurate 3D hand pose estimation. Moreover, for the first time, we apply the diffusion model with a PointNetbased <mark>denoising process</mark> to improve pose estimation performance. Diffusion models for pose estimation.<br>",
    "Arabic": "عملية إزالة التشويش",
    "Chinese": "去噪过程",
    "French": "processus de débruitage",
    "Japanese": "雑音除去処理",
    "Russian": "процесс деноизинга"
  },
  {
    "English": "denoise score match loss",
    "context": "1: Following Eq. 2, the <mark>denoising score matching loss</mark> for a given denoiser D θ on a given noise level σ is given by \n L(D θ ; σ) = E y∼pdata E n∼N (0,σ 2 I) D θ (y + n; σ) − y 2 2 . (104 \n ) \n<br>2: Let us now consider the <mark>denoising score matching loss</mark> of Eq. 2. By expanding the expectations, we can rewrite the formula as an integral over the noisy samples x: \n L ( D ; σ ) = E y∼pdata E n∼N ( 0 , σ 2 I ) D ( y + n ; σ ) − y 2 2 ( 46 ) = E y∼pdata E x∼N ( y , σ 2 I ) D ( x ; σ ) − y 2 2 ( 47 ) = E y∼pdata R d N ( x ; y , σ 2 I ) D ( x ; σ ) − y 2 2 dx ( 48 ) = 1 Y Y i=1 R d N ( x ; y i , σ 2 I ) D ( x ; σ ) − y i 2 2 dx ( 49 ) = R d 1 Y Y i=1 N<br>",
    "Arabic": "خسارة مطابقة درجة إزالة التشويش",
    "Chinese": "去噪分数匹配损失",
    "French": "perte de correspondance de score de débruitage",
    "Japanese": "スコアマッチング損失の雑音除去",
    "Russian": "потеря соответствия оценки шума"
  },
  {
    "English": "denoise score matching",
    "context": "1: First, p T is replaced by the reference distribution N(0, Id) as we know that p T converges geometrically towards it. Second , the following <mark>denoising score matching</mark> identity is exploited to estimate the scores ∇ xt log p t ( x t ) = R d ∇ xt log p t|0 ( x t |x 0 ) p 0|t ( x 0 |x t ) dx 0 , where p t|0 ( x t |x 0 ) is the transition density of the OU<br>2: Equivalently, a backward step nudges the sample towards the data distribution. Denoising score matching. The score function has the remarkable property that it does not depend on the generally intractable normalization constant of the underlying density function p(x; σ) [22], Table 1: Specific design choices employed by different model families.<br>",
    "Arabic": "مطابقة نقاط إزالة التشويش",
    "Chinese": "去噪分数匹配",
    "French": "appariement des scores de débruitage",
    "Japanese": "デノイズスコアマッチング",
    "Russian": "согласование оценок шумоподавления"
  },
  {
    "English": "denoiser",
    "context": "1: Subsequently, the noisy joint distribution is supplied to the proposed <mark>denoiser</mark> to recover the clean joint distribution J (0|t) , under the joint-wise conditions as well as the local detail conditions. To train the <mark>denoiser</mark>, J (0|t) is under the supervision of the ground truth distribution J * .<br>2: Our model also samples H initial 3D poses J (T ) 0:H from a unit Gaussian distribution. Afterward, H pose hypotheses are individually passed to the proposed <mark>denoiser</mark> to approximate the H uncontaminated joint coordinate distribution J (0|t) 0:H .<br>",
    "Arabic": "مزيل الضجيج",
    "Chinese": "去噪器",
    "French": "débruiteur",
    "Japanese": "\"デノイザー\"",
    "Russian": "денойзер"
  },
  {
    "English": "denotation",
    "context": "1: For each type t, the agent enumerates (AND t pi) as a candidate. For each relation r, the agent enumerates (JOIN r pi) as a candidate. If the <mark>denotation</mark> of p i is a numerical value, then four similar candidates with comparatives are also included (LT/LE/GT/GE r pi).<br>",
    "Arabic": "الدلالة",
    "Chinese": "指称",
    "French": "dénotation",
    "Japanese": "示す内容",
    "Russian": "денотация"
  },
  {
    "English": "dense",
    "context": "1: We empirically select the hyperparameters for the graph encoder, the user-and the item-RNNs. a) The Graph-based Encoder: For this model, we employ a learning rate of 10 −2 , a dropout rate of 0.3 and rectified linear unit (ReLU) as the activation function after both <mark>dense</mark> and GNN layers.<br>2: Note that while G is <mark>dense</mark> and therefore not manageable, computing r T φ(x) is computationally inexpensive, as only the entries of r corresponding to non-zero entries in φ(x) need to be generated.<br>",
    "Arabic": "مكثف",
    "Chinese": "密集",
    "French": "dense",
    "Japanese": "密な",
    "Russian": "плотный"
  },
  {
    "English": "dense attention",
    "context": "1: Because the memory requirements of the transformer decoder scale quadratically with context length when using <mark>dense attention</mark>, we must employ further techniques to reduce context length.<br>",
    "Arabic": "اهتمام كثيف",
    "Chinese": "密集注意力",
    "French": "attention dense",
    "Japanese": "密な注意",
    "Russian": "плотное внимание"
  },
  {
    "English": "dense depth map",
    "context": "1: This allows for a compact representation of depth without sacrificing reconstruction detail. In inference algorithms the code can be used as dense representation of the geometry and, due to its limited size, this allows for full joint estimation of both camera poses and <mark>dense depth maps</mark> for multiple overlapping keyframes.<br>2: We first execute the multi-view stereo (MVS) algo-rithm of COLMAP [54] to generate per-frame <mark>dense depth maps</mark> \n (D i | D i ∈ R H×W + ) N I i=1 .<br>",
    "Arabic": "خريطة عمق كثيفة",
    "Chinese": "密集深度图",
    "French": "carte de profondeur dense",
    "Japanese": "密な深度マップ",
    "Russian": "плотная карта глубины"
  },
  {
    "English": "dense feature",
    "context": "1: To address this issue, we additionally use an appearance check: we extract <mark>dense features</mark> for each pixel using DINO [10] and filter out correspondences whose features' cosine similarity is < 0.5.<br>2: The image resolution at which the <mark>dense features</mark> are extracted has a large impact on the accuracy of the refinement. In Figure 8 we quantify in the impact on both triangulation accuracy and run time for the ETH3D Courtyard scene (38 images).<br>",
    "Arabic": "الميزة الكثيفة",
    "Chinese": "密集特征",
    "French": "caractéristiques denses",
    "Japanese": "密な特徴量",
    "Russian": "плотные признаки"
  },
  {
    "English": "dense layer",
    "context": "1: We follow the pooling with a <mark>dense layer</mark> z = σ(W z c + b z ), where σ is a non-linear function, matrix W z ∈ R 64×s and vector b z ∈ R 64 are learned parameters. The presupposition trigger probability is computed with an affine transform followed by a softmax: \n<br>2: We parameterize G and S with neural networks of 3 and 2 layers respectively. Each layer consists of a <mark>dense layer</mark> and a ReLU activation, except the last layer of S takes Sigmoid activation to produce a probability vector.<br>",
    "Arabic": "الطبقة الكثيفة",
    "Chinese": "密集层",
    "French": "couche dense",
    "Japanese": "密層",
    "Russian": "плотный слой"
  },
  {
    "English": "dense matrix",
    "context": "1: Specifically, using Hadamard matrices instead of <mark>dense matrices</mark> allows us to compute a linear projection significantly faster than a dense matrix projection.<br>2: Based on our fast algorithm for sketching <mark>dense matrices</mark>, we then give a new spaceoptimal streaming algorithm with O(nnz(A)k + nk 3 ) +Õ(dα −3 ) running time to compute (α, k)-cov-sketches for sparse matrices. We separate the dependence of \n Huang Time ( α , k ) -cov ( ε , k ) -proj FD ( Liberty , 2013 ) O ( ndk + nd/α ) O ( ndk/ε ) FFDdense ( new ) O ( ndk ) O ( ndk ) Sparse FD ( Ghashami et al. , 2016 ) O ( nnz ( A ) k log d + nnz ( A<br>",
    "Arabic": "مصفوفة كثيفة",
    "Chinese": "稠密矩阵",
    "French": "matrice dense",
    "Japanese": "密行列",
    "Russian": "плотная матрица"
  },
  {
    "English": "dense network",
    "context": "1: Table 1 shows the case under our experimental setting. As the number of parameters in the embedding layer overwhelms the one of the <mark>dense networks</mark>, the difficulty of large batch optimization lies in the embedding layers. This paper focuses on addressing the training instability caused by the properties of embedding layers in the CTR prediction model.<br>",
    "Arabic": "الشبكة الكثيفة",
    "Chinese": "稠密网络",
    "French": "réseau dense",
    "Japanese": "密なネットワーク",
    "Russian": "плотная сеть"
  },
  {
    "English": "dense prediction",
    "context": "1: Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary <mark>dense prediction</mark> tasks.<br>2: In VTM, we implement analogymaking for <mark>dense prediction</mark> as patch-level non-parametric matching, where the model learns the similarity in image patches that captures the similarity in label patches.<br>",
    "Arabic": "التنبؤ الكثيف",
    "Chinese": "密集预测",
    "French": "prédiction dense",
    "Japanese": "密な予測",
    "Russian": "плотное предсказание"
  },
  {
    "English": "dense representation",
    "context": "1: This fascinating result raises a question: to what extent are the relational semantic properties a result of the embedding process? Experiments in (Mikolov et al., 2013c) show that the RNN-based embeddings are superior to other <mark>dense representations</mark>, but how crucial is it for a representation to be dense and low-dimensional at all?<br>2: Another line of research lies in Dense Retrieval, which presents query and documents in dense vectors and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain <mark>dense representations</mark> for queries and documents.<br>",
    "Arabic": "التمثيل الكثيف",
    "Chinese": "密集表示",
    "French": "représentation dense",
    "Japanese": "密な表現",
    "Russian": "плотное представление"
  },
  {
    "English": "dense vector",
    "context": "1: Dense retrieval. Another line of research lies in Dense Retrieval, which presents query and documents in <mark>dense vectors</mark> and models their similarities with inner product or cosine similarity. These methods benefit from recent progresses of pre-trained language models, such as BERT [14] and RoBERTa [35] to obtain dense representations for queries and documents.<br>2: To achieve optimal task parallelism, we represent the weight matrix W as d <mark>dense vectors</mark> (arrays) of length m, each w k for a target variable k. First, using weight vectors is more scalable in terms of memory footprint than matrix representation.<br>",
    "Arabic": "متجه كثيف",
    "Chinese": "稠密向量",
    "French": "vecteur dense",
    "Japanese": "密なベクトル",
    "Russian": "плотный вектор"
  },
  {
    "English": "density estimate",
    "context": "1: A kernel G can be de ned as \n G(x) = C g (kxk 2 ) (8 \n ) \n where C is a normalization constant. Then, by taking the estimate of the density gradient as the gradient o f the <mark>density estimate</mark> we h a vê \n<br>",
    "Arabic": "تقدير الكثافة",
    "Chinese": "密度估计",
    "French": "estimation de densité",
    "Japanese": "密度推定",
    "Russian": "оценка плотности"
  },
  {
    "English": "density estimation",
    "context": "1: Note that u θ also enables <mark>density estimation</mark> using thatp 1 =p 0 − div(u θ ). Density estimation is not directly accessible using RSGM, however in App. L we propose a way to perform such an estimation using Fisher score in a manner akin to Choi et al. (2021).<br>2: We set up our formal framework and notation in Section 2. In Section 3, we define compression schemes for distributions, prove their closure properties, and show their connection with <mark>density estimation</mark>. Theorem 1.1 and Theorem 1.3 are proved in Section 4.<br>",
    "Arabic": "تقدير الكثافة",
    "Chinese": "密度估计",
    "French": "estimation de densité",
    "Japanese": "密度推定",
    "Russian": "оценка плотности"
  },
  {
    "English": "density estimator",
    "context": "1: Recall the definition of entropy in Eq. (1). Given a <mark>density estimator</mark> p x (x) for p x (x) and a set of N i.i.d.<br>2: h D . We can then estimate the data-driven proposal as: \n q data (S ρ → S ρ |C, I D ) = P density ({ρ j } N j=1 ), \n where P density is a <mark>density estimator</mark> such as the multivariate gaussian kernel in [19]).<br>",
    "Arabic": "مقدر الكثافة",
    "Chinese": "密度估计器",
    "French": "estimateur de densité",
    "Japanese": "密度推定器",
    "Russian": "оценщик плотности"
  },
  {
    "English": "density field",
    "context": "1: We use the orientation loss proposed by Ref-NeRF  to encourage normal vectors of the <mark>density field</mark> to face toward the camera when they are visible (so that the camera does not observe geometry that appears to face \"backwards\" when shaded).<br>2: Removing the orientation loss (\"no R o \") results in severely degraded normals and renderings, and applying the orientation loss directly to the <mark>density field</mark>'s normals and using those to compute reflection directions (\"no pred. normals\") also reduces performance.<br>",
    "Arabic": "حقل الكثافة",
    "Chinese": "密度场",
    "French": "champ de densité",
    "Japanese": "密度場",
    "Russian": "поле плотности"
  },
  {
    "English": "density function",
    "context": "1: Any such monotonic function f can be rewritten in terms of the cumulative distribution function of some <mark>density function</mark> ρ(θ), defined over the range 0 ≤ θ ≤ π. As our normalised <mark>density function</mark>, we take a constant term plus a wrapped Cauchy distribution.<br>2: Our analysis relies on some assumptions on the <mark>density function</mark> p x , which are summarized as below: Assumption 1. The distribution p x satisfies: (c) The gradient of p x is uniformly bounded on Q o , i.e., C 2 = sup \n x∈Q o || p x (x)|| 1 < ∞.<br>",
    "Arabic": "دالة الكثافة",
    "Chinese": "密度函数",
    "French": "fonction de densité",
    "Japanese": "密度関数",
    "Russian": "функция плотности"
  },
  {
    "English": "density gradient",
    "context": "1: A kernel G can be de ned as \n G(x) = C g (kxk 2 ) (8 \n ) \n where C is a normalization constant. Then, by taking the estimate of the <mark>density gradient</mark> as the gradient o f the density estimate we h a vê \n<br>2: Expression (13) shows that the sample mean shift vector obtained with kernel G is an estimate of the normalized <mark>density gradient</mark> obtained with kernel K. This is a more general formulation of the property rst remarked by F ukunaga 15, p . 535].<br>",
    "Arabic": "تدرج الكثافة",
    "Chinese": "密度梯度",
    "French": "gradient de densité",
    "Japanese": "密度勾配",
    "Russian": "градиент плотности"
  },
  {
    "English": "density ratio",
    "context": "1: For the MPF Stein estimator and the difference Stein estimator (8), the <mark>density ratio</mark> q(y) q(x) can be simplified to q(yi|x−i) q(xi|x−i) . We further replace it with q(yi|x−i) q(xi|x−i)+10 −3 to alleviate numerical instability.<br>",
    "Arabic": "نسبة الكثافة",
    "Chinese": "密度比率",
    "French": "rapport de densité",
    "Japanese": "密度比",
    "Russian": "соотношение плотностей"
  },
  {
    "English": "dependency",
    "context": "1: In this section we sketch an algorithm to compute marginal probabilities of <mark>dependencies</mark>.<br>2: McDonald (2006) formalises sentence compression in a discriminative large-margin learning framework as a classification task: pairs of words from the source sentence are classified as being adjacent or not in the target compression. A large number of features are defined over words, parts of speech, phrase structure trees and <mark>dependencies</mark>.<br>",
    "Arabic": "الاعتمادية",
    "Chinese": "依赖关系",
    "French": "dépendance",
    "Japanese": "依存関係",
    "Russian": "зависимости"
  },
  {
    "English": "dependency arc",
    "context": "1: <mark>dependency arc</mark> . Clearly there are exponentially many possible parse tree structures, but fortunately there exist well-known dynamic programming algorithms for searching over all possible structures. We review these below, starting with the first-order factorization for ease of exposition. Throughout the paper we make use of some basic mathematical notation.<br>",
    "Arabic": "قوس التبعية",
    "Chinese": "依赖弧",
    "French": "arc de dépendance",
    "Japanese": "依存アーク",
    "Russian": "зависимостная дуга"
  },
  {
    "English": "dependency feature",
    "context": "1: Accordingly, when comparing their two approaches, Wu and Weld (2010) show that the use of <mark>dependency features</mark> results in an increase in precision and recall over shallow linguistic features, though, at the cost of extraction speed, hence negatively affecting the scalability of the system.<br>2: The <mark>dependency features</mark> used in our experiments are closely related to the features described in (Carreras, 2007), which are an extension of the McDonald and Pereira (2006) features to cover grandparent dependencies in addition to first-order and sibling dependencies. The features take into account the identity of the labels l used in the derivations.<br>",
    "Arabic": "ميزة التبعية",
    "Chinese": "依存特征",
    "French": "caractéristique de dépendance",
    "Japanese": "依存特徴",
    "Russian": "признаки зависимости"
  },
  {
    "English": "dependency graph",
    "context": "1: Their technique is particular to the parsing taskmaking a binary decision about whether to lock in edges in the <mark>dependency graph</mark> at each stage, and enforcing parsing-specific, hard-coded constraints on valid subsequent edges. Furthermore, as described above, they employ an auxiliary model to select features.<br>2: The first equivalence holds because there exists a set of clauses equivalent to A(p, Z) such that the <mark>dependency graph</mark> of A(p, Z) under it has no loops, so that Corollary 2 applies.<br>",
    "Arabic": "الرسم البياني التبعية",
    "Chinese": "依赖图",
    "French": "graphe de dépendances",
    "Japanese": "依存グラフ",
    "Russian": "граф зависимостей"
  },
  {
    "English": "dependency label",
    "context": "1: We believe that a careful design of fea-13 Unlike our model, the hybrid models used here as baselines make use of the <mark>dependency labels</mark> at training time; indeed, the transition-based parser is trained to predict a labeled dependency parse tree, and the graph-based parser use these predicted labels as input features.<br>2: Das and Smith (2009) used the quasi-synchronous grammar formalism to incorporate features from WordNet, named entity recognizer, POS tagger, and dependency la-bels from aligned trees. The task of recognizing textual entailment is to decide whether the hypothesis sentence can be entailed by the premise sentence (Giampiccolo et al., 2007).<br>",
    "Arabic": "علامة التبعية",
    "Chinese": "依存标签",
    "French": "étiquette de dépendance",
    "Japanese": "依存関係ラベル",
    "Russian": "метка зависимости"
  },
  {
    "English": "dependency model",
    "context": "1: 6 The model was trained on Sections 2-20 of the Penn Treebank and tested on 100 sentences of Section 21 of length at most 30 words. For this grammar, Klein and Manning (2003b) showed that a very accurate heuristic can be constructed by taking the sum of outside scores computed with the <mark>dependency model</mark> and the PCFG model individually.<br>2: Datasets We test our <mark>dependency model</mark> on 14 languages, including the English dataset from CoNLL 2008 shared tasks and all 13 datasets from CoNLL 2006 shared tasks (Buchholz and Marsi, 2006;Surdeanu et al., 2008). These datasets include manually annotated dependency trees, POS tags and morphological information.<br>",
    "Arabic": "نموذج التبعية",
    "Chinese": "依存模型",
    "French": "modèle de dépendance",
    "Japanese": "依存性モデル",
    "Russian": "модель зависимостей"
  },
  {
    "English": "dependency parse",
    "context": "1: The simplest way to index a <mark>dependency parse</mark> structure is by the individual arcs of the parse tree. This model is known as first-order or arc-factored. For a sentence of length n the index set is: \n I 1 = {(h, m) : h ∈ [n] 0 , m ∈ [n]} \n<br>2: We only allow sentences where the content words from arguments and relation can be linked to each other via a linear path of size four in the <mark>dependency parse</mark>. To implement this restriction, we only use the subset of content words that are headwords in the parse tree.<br>",
    "Arabic": "تحليل التبعية",
    "Chinese": "依存句法分析",
    "French": "analyse des dépendances",
    "Japanese": "依存構文解析",
    "Russian": "синтаксический анализ зависимостей"
  },
  {
    "English": "dependency parse tree",
    "context": "1: Our baseline consisted in classifying all child utterances with no SUBJ node in the <mark>dependency parse tree</mark> as positive instances of SOE. This already achieves an F1-score of 0.82, with 0.74 precision and 0.93 recall. To improve precision while not lowering recall too dramatically, we experimented with additional features aimed at accounting for parsing errors.<br>",
    "Arabic": "شجرة تحليل التبعية",
    "Chinese": "依存分析树",
    "French": "arbre de dépendances syntaxiques",
    "Japanese": "依存構文解析木",
    "Russian": "дерево зависимостей разбора"
  },
  {
    "English": "dependency parser",
    "context": "1: We introduce a novel <mark>dependency parser</mark>, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags.<br>2: We have made this dataset publicly available for future research on this topic. We used the remaining 50K unlabeled questions for unsupervised pretraining, by generating verb-object parse tags for them via the Stanford CoreNLP <mark>dependency parser</mark> [Manning et al., 2014], and using these words as proxies for the ACTION and OBJECT tagged phrases that compose an intent.<br>",
    "Arabic": "محلل التبعية",
    "Chinese": "依存解析器",
    "French": "analyseur de dépendances",
    "Japanese": "依存構文解析器",
    "Russian": "синтаксический анализатор"
  },
  {
    "English": "dependency parsing model",
    "context": "1: A severe challenge in applying such an approach to full syntactic parsing is the efficiency of the parsing algorithms involved. We show that efficient training is feasible, using a Tree Adjoining Grammar (TAG) based parsing formalism. A lower-order <mark>dependency parsing model</mark> is used to restrict the search space of the full model, thereby making it efficient.<br>",
    "Arabic": "نموذج تحليل التبعية",
    "Chinese": "依存句法分析模型",
    "French": "modèle d'analyse des dépendances",
    "Japanese": "依存構文解析モデル",
    "Russian": "модель синтаксического анализа зависимостей"
  },
  {
    "English": "dependency path",
    "context": "1: LIE learns open pattern templates -a mapping from a <mark>dependency path</mark> to an open extraction, i.e., one that identifies both the arguments and the exact (REVERB-style) relation phrase. Figure 3 gives examples of high-frequency pattern templates learned by OLLIE.<br>2: For example, a relation (Godse; kill; Gandhi) may be expressed with a <mark>dependency path</mark> (#2) {Godse}↑nsubj↑{kill:postag=VBD}↓dobj↓{Gandhi}. To learn the pattern templates, we first extract the <mark>dependency path</mark> connecting the arguments and relation words for each seed tuple and the associated sentence.<br>",
    "Arabic": "مسار التبعية",
    "Chinese": "依存路径",
    "French": "chemin de dépendance",
    "Japanese": "依存関係パス",
    "Russian": "зависимостный путь"
  },
  {
    "English": "dependency relation",
    "context": "1: We propose to separate these two orthogonal types of information and make the bare grammatical function the main label of <mark>dependency relation</mark>, with categorial and 'openness' information given as subtypes, as in (53).<br>",
    "Arabic": "علاقة التبعية",
    "Chinese": "依赖关系",
    "French": "relation de dépendance",
    "Japanese": "依存関係",
    "Russian": "отношение зависимости"
  },
  {
    "English": "dependency representation",
    "context": "1: We do not have direct access to semantic scope, but we expect syntactic scope to correlate strongly with semantic scope, so we used <mark>dependency representations</mark> to define features capturing syntactic scope for negation, modal auxiliaries, and a broad range of attitude predicates.<br>",
    "Arabic": "تمثيل التبعية",
    "Chinese": "依存关系表示",
    "French": "représentation des dépendances",
    "Japanese": "依存関係表現",
    "Russian": "представление зависимостей"
  },
  {
    "English": "dependency structure",
    "context": "1: If we can combine two categories with a certain category operation, we can use a corresponding tree operation to combine two <mark>dependency structure</mark>s. The category of the combined <mark>dependency structure</mark> is the result of the combinatory category operations. We first introduce three meta category operations.<br>2: In particular, it is important that given this definition of l, it is possible to define a function GRM(l) that maps a label l to a triple of nonterminals that represents the grammatical relation between m and h in the <mark>dependency structure</mark>.<br>",
    "Arabic": "هيكل التبعية",
    "Chinese": "依存结构",
    "French": "structure de dépendance",
    "Japanese": "依存構造",
    "Russian": "структура зависимостей"
  },
  {
    "English": "dependency tree",
    "context": "1: The direction of the arc is determined by the label of the node ( R or L ). See Fig. 3 for an example. Once the <mark>dependency tree</mark> is converted to a BHT, we can linearize it to a sequence of hexatags in a straightforward manner. Theorem 2 states the relationship between BHTs and hexatags formally.<br>2: , a K on a candidate <mark>dependency tree</mark>), we employ a linearization trick (Boros and Hammer, 2002), defining extra variables z a 1 ...a K z a 1 ∧ . . . ∧ z a K . This logical relation can be expressed by the following O(K) agreement constraints: 6 \n<br>",
    "Arabic": "شجرة التبعية",
    "Chinese": "依存树",
    "French": "arbre de dépendance",
    "Japanese": "依存木",
    "Russian": "дерево зависимостей"
  },
  {
    "English": "dependency treebank",
    "context": "1: That is to say, despite them all being <mark>dependency treebanks</mark>, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions.<br>2: Data Throughout § §6-7, we will examine our compressed tags on a subset of Universal Dependencies (Nivre et al., 2018), or UD, a collection of <mark>dependency treebanks</mark> across 76 languages using the same POS tags and dependency labels.<br>",
    "Arabic": "بنك الشجرة الإعتمادية",
    "Chinese": "依存树库",
    "French": "corpus d'arbres de dépendances",
    "Japanese": "依存構造ツリーバンク",
    "Russian": "банк деревьев зависимостей"
  },
  {
    "English": "dependent variable",
    "context": "1: MT papers regularly propose new methods or algorithms that aim at better exploiting training and/or validating data. Following the scientific method, we can then define these new methods/algorithms and datasets as in<mark>dependent variable</mark>s of an MT experiment while the translation quality, approximated by metric scores, would be our <mark>dependent variable</mark> that we want to measure.<br>2: We construct several linear regression models, where rer(t 0 , t 1 ) is always the <mark>dependent variable</mark> we are interested in predicting and CF at t 0 is the in<mark>dependent variable</mark> whose predictive power we are investigating, while controlling for several other factors characterising child directed speech and children's own speech.<br>",
    "Arabic": "المتغير التابع",
    "Chinese": "因变量",
    "French": "variable dépendante",
    "Japanese": "従属変数",
    "Russian": "зависимая переменная"
  },
  {
    "English": "depth estimation",
    "context": "1: This subsumes a wide range of vision tasks including semantic segmentation, <mark>depth estimation</mark>, surface normal prediction, edge prediction, to name a few, varying in structure of output space, e.g., dimensionality (C T ) and topology (discrete or continuous), as well as the required knowledge.<br>2: Adaptation Mechanism On top of the unified architecture, the learner should have a flexible and efficient adaptation mechanism to address highly diverse semantics of the unseen tasks T test . This is because tasks of different semantics can require distinct sets of features -<mark>depth estimation</mark> requires 3D scene understanding, while edge estimation prefers low-level image gradient.<br>",
    "Arabic": "تقدير العمق",
    "Chinese": "深度估计",
    "French": "estimation de la profondeur",
    "Japanese": "深度推定",
    "Russian": "оценка глубины"
  },
  {
    "English": "depth estimator",
    "context": "1: The <mark>depth estimator</mark>, g(x; I), in turn, takes as input a triplet of an RGB image, foreground mask, and UV coordinate, and outputs the depth estimates. The geometric consistency between the surface normal and depth is enforced by minimizing L s .<br>",
    "Arabic": "معايِر العُمق",
    "Chinese": "深度估计器",
    "French": "estimateur de profondeur",
    "Japanese": "深さ推定器",
    "Russian": "оценщик глубины"
  },
  {
    "English": "depth image",
    "context": "1: To address inherent limitations in 3D DMs, our model incorporates a joint-wise denoising mechanism that individually denoises various joints during estimation. Concretely, the proposed model first introduces a joint-wise condition generation module that samples features for each individual joints from both <mark>depth image</mark> and point cloud.<br>2: Reconstr. Groundtr. conditioned depth encoding works. In Figure 6 we show how we encode a <mark>depth image</mark> into a code of size 128. Using the corresponding intensity image this can then be decoded into a reconstructed <mark>depth image</mark>, which captures all of the main scene elements well.<br>",
    "Arabic": "صورة العمق",
    "Chinese": "深度图像",
    "French": "image de profondeur",
    "Japanese": "深度画像",
    "Russian": "глубинное изображение"
  },
  {
    "English": "depth map",
    "context": "1: We additionally modify this initial confidence using the observation that the <mark>depth map</mark> at one side of an image of each stereo pair generally has a poorly estimated depth, as the true match for those pixels are often not present in the other image of the stereo pair.<br>2: minimize Z,L g(I − S(Z, L)) + f (Z) + h(L) \n where R = I − S(Z, L) is a log-reflectance image, Z is a <mark>depth map</mark> and L is a spherical-harmonic model of illumination.<br>",
    "Arabic": "خريطة العمق",
    "Chinese": "深度图",
    "French": "carte de profondeur",
    "Japanese": "奥行きマップ",
    "Russian": "карта глубины"
  },
  {
    "English": "depth prediction",
    "context": "1: We parametrize a 3D point p ∈ R 3 reconstructed by the <mark>depth prediction</mark> using the UV coordinate, i.e., \n p i (u) = zK −1 x = g(h i (u); I i )K −1 h i (u),(1) \n<br>2: As this task is very similar to the Z-buffer <mark>depth prediction</mark> (ZD) whose label pixels are the distance from each image pixel to the camera plane, we augment the ED task by segmenting the depth range and re-normalizing within each segment.<br>",
    "Arabic": "تنبؤ العمق",
    "Chinese": "深度预测",
    "French": "prédiction de profondeur",
    "Japanese": "深度予測",
    "Russian": "предсказание глубины"
  },
  {
    "English": "depth-first search",
    "context": "1: for each gene g k do generate matrix c i,j for g k ; // <mark>depth-first search</mark> for i = 1 to (l − min s + 1) do call search-clique({s i }, {s i+1 , . . . , s l } ) ; end-for end-for For example , for a set of l samples , even the complete set of sample combinations can be divided into l exclusive subsets as shown before , we only need to search the first ( l − mins + 1 ) subsets , since each of the last ( mins − 1 ) subsets contains<br>2: We can conduct a recursive, <mark>depth-first search</mark> of the sample set enumeration tree to detect the maximal cliques of the samples. Given a set of samples S, the set enumeration tree has 2 |S| nodes. However, we never need to materialize such a tree.<br>",
    "Arabic": "البحث بالعمق أولاً",
    "Chinese": "深度优先搜索",
    "French": "recherche en profondeur",
    "Japanese": "深さ優先探索",
    "Russian": "поиск в глубину"
  },
  {
    "English": "description logic",
    "context": "1: In this section, we review classical planning, <mark>description logic</mark> and its use in classical planning.<br>2: We propose bounded fitting as a scheme for learning <mark>description logic</mark> concepts in the presence of ontologies. A main advantage is that the resulting learning algorithms come with theoretical guarantees regarding their generalization to unseen examples in the sense of PAC learning. We prove that, in contrast, several other natural learning algorithms fail to provide such guarantees.<br>",
    "Arabic": "منطق الوصف",
    "Chinese": "描述逻辑",
    "French": "logique de description",
    "Japanese": "記述論理",
    "Russian": "логика описания"
  },
  {
    "English": "descriptor",
    "context": "1: When we incorporate the <mark>descriptor</mark> through (t) , the RHS denominator increases from d to (d + d m ) while K and k remain constant, yielding a tighter fit. Therefore, task <mark>descriptor</mark>s can improve learned dictionary quality and sparse recovery accuracy.<br>2: We compute the WTA hash for each weight vector, decompose the resulting <mark>descriptor</mark> into M bands consisting of W spans of length K, construct M hash tables and store each band in its respective table along with the index P of the corresponding part filter.<br>",
    "Arabic": "واصف",
    "Chinese": "描述子",
    "French": "descripteur",
    "Japanese": "記述子",
    "Russian": "дескриптор"
  },
  {
    "English": "design matrix",
    "context": "1: Note that the multi-task Lasso is used in this context only to learn which variables should be input into the final model. Specifically, the leave-two-out-cross-validation procedure is as follows: \n a Create a 60 × 5, 000 <mark>design matrix</mark> of semantic features using co-occurences of the 5,000 most frequent words in English (minus 100 stop words).<br>2: If we can control the <mark>design matrix</mark> (as is often the case in compressive sensing), we can use the construction of (Hegde et al., 2014b) to get a sample-optimal matrix with nearly-linear T X in the regime of s ≤ d 1/2−µ , µ > 0.<br>",
    "Arabic": "مصفوفة التصميم",
    "Chinese": "设计矩阵",
    "French": "matrice de design",
    "Japanese": "設計行列",
    "Russian": "матрица проектирования"
  },
  {
    "English": "design space",
    "context": "1: Each configuration sampled from the search space represents a unique scalable architecture, resulting in 150k possible designs in total. One can also include more aggregators in the current space with future state-of-the-arts. In the following, we first introduce the aggregators used in our <mark>design space</mark>, and then explore interesting GNN instances in our defined space.<br>",
    "Arabic": "المساحة التصميمية",
    "Chinese": "设计空间",
    "French": "espace de conception",
    "Japanese": "設計空間",
    "Russian": "пространство проектирования"
  },
  {
    "English": "det",
    "context": "1: ( ) ) 1/2 + Tr ( W ( ) ) −1/2 ∆ ( W ( ) ) −1/2 + A ( k ) − A ( ) 2 F κ w , cross = Tr ( X ) − log <mark>det</mark> ( I d + X ) + A ( k ) − A ( ) 2 F κ w , cross , \n<br>2: For the arguments we expand on amod, nn, <mark>det</mark>, neg, prep of, num, quantmod edges to build the noun-phrase. When the base noun is not a proper noun, we also expand on rcmod, infmod, partmod, ref, prepc of edges, since these are relative clauses that convey important information.<br>",
    "Arabic": "المُحدد",
    "Chinese": "行列式",
    "French": "déterminant",
    "Japanese": "行列式",
    "Russian": "определитель"
  },
  {
    "English": "detection",
    "context": "1: Bottom: In a similar manner, we use an image frame (left) from the series \"Game of Thrones\" to synthesize five new images with different expressions. <mark>detection</mark> and cropping scheme we described before. Fig. 8 shows two examples on these challenging images.<br>2: Streaming perception remains a challenge Our analysis suggests that streaming perception involves careful integration of <mark>detection</mark>, tracking, forecasting, and dynamic scheduling. While we present several strong solutions for streaming perception, the gap between the streaming performance and the offline performance remains significant (20.3 versus 38.0 in AP).<br>",
    "Arabic": "الكشف",
    "Chinese": "检测",
    "French": "détection",
    "Japanese": "検出",
    "Russian": "обнаружение"
  },
  {
    "English": "detection algorithm",
    "context": "1: In the following, we use the term baseline algorithm or just baseline to refer to the <mark>detection algorithm</mark> described in [6] utilizing models generated by the training method from the same paper.<br>2: A second (related) limitation is that they do not provide a coherent quantitative mechanism for adaptation, i.e., either adapting the <mark>detection algorithm</mark> on the basis of training data and/or via prior knowledge. As we will see in later sections, the probabilistic probabilistic framework in this paper can handle these issues in a systematic and straightforward manner.<br>",
    "Arabic": "خوارزمية الكشف",
    "Chinese": "检测算法",
    "French": "algorithme de détection",
    "Japanese": "検出アルゴリズム",
    "Russian": "алгоритм обнаружения"
  },
  {
    "English": "detection model",
    "context": "1: We expect this to have limited impact based on our feature ranking Second, note that Outguardis a supervised learning technique. Cryptojacking operators may be able exploit this fundamental limitation and change their strategies to evade some of the features of the <mark>detection model</mark>. For example, the operators may find alternatives to using web sockets or web worker services.<br>2: Such a summation is consistent with other models, which employ spatial pooling as the last stage of the <mark>detection model</mark> [Watson and Ahumada Jr 2005].<br>",
    "Arabic": "نموذج الكشف",
    "Chinese": "检测模型",
    "French": "modèle de détection",
    "Japanese": "検出モデル",
    "Russian": "модель обнаружения"
  },
  {
    "English": "detection score",
    "context": "1: In subsequent iterations, we first compute a bounding box by running the detectors on each training image. If the <mark>detection score</mark> is higher than the score for the bounding box from the previous iteration we use the newly computed bounding box.<br>2: f (X y ; θ) is the <mark>detection score</mark> of segment X y , and θ is the parameter of the score function. Note that the detector searches over temporal scales from l min to l max . In testing, this process can be repeated to detect multiple target events, if more than one event occur.<br>",
    "Arabic": "درجة الكشف",
    "Chinese": "检测分数",
    "French": "score de détection",
    "Japanese": "検出スコア",
    "Russian": "оценка обнаружения"
  },
  {
    "English": "detection window",
    "context": "1: Later, the work was extended to multiple classifiers trained to detect human parts, and the responses inside the <mark>detection window</mark> are combined to give the final decision [14].<br>2: where Ë is one of Í Ä Ê and Ö ´µ is a single box rectangular sum within the <mark>detection window</mark>. These filters extract information related to the likelihood that a particular region is moving in a given direction.<br>",
    "Arabic": "نافذة الكشف",
    "Chinese": "检测窗口",
    "French": "fenêtre de détection",
    "Japanese": "検出ウィンドウ",
    "Russian": "окно обнаружения"
  },
  {
    "English": "detector",
    "context": "1: To add a new object to our model, one needs only to train a <mark>detector</mark> for that object and supply the distribution of the object's height in the 3D scene. Our framework could also be extended by modeling other scene properties, such as scene category.<br>2: This suggests that there is considerable room for improvement by building a better <mark>detector</mark>, tracker, forecaster, or even an end-to-end model that blurs boundary of these modules. Formulations of real-time computation Common folk wisdom for real-time applications like online detection requires that <mark>detector</mark>s run within the sensor frame rate.<br>",
    "Arabic": "كاشف",
    "Chinese": "检测器",
    "French": "détecteur",
    "Japanese": "検出器",
    "Russian": "детектор"
  },
  {
    "English": "determinantal point process",
    "context": "1: The <mark>Determinantal Point Processes</mark> (DPPs) which have found numerous applications in machine learning [26] are known to be log-submodular distributions. In particular, the MAP inference problem is a form of non-monotone submodular maximization.<br>",
    "Arabic": "عملية نقطة محددية",
    "Chinese": "行列式点过程",
    "French": "processus ponctuel déterminantal",
    "Japanese": "決定的点過程",
    "Russian": "детерминантальный точечный процесс"
  },
  {
    "English": "deterministic algorithm",
    "context": "1: In contrast, consider any <mark>deterministic algorithm</mark> A with label budget N ≤ n 2 ; we consider its interaction history with classifier h 0 ≡ −1, which can be summarized by a sequence of unlabeled examples S = x 1 , . . . , x N .<br>2: • In light of the computational intractability of the optimal <mark>deterministic algorithm</mark>, we design a randomized algorithm that enjoys oracle efficiency (e.g.<br>",
    "Arabic": "خوارزمية حتمية",
    "Chinese": "确定性算法",
    "French": "algorithme déterministe",
    "Japanese": "決定論的アルゴリズム",
    "Russian": "детерминированный алгоритм"
  },
  {
    "English": "deterministic annealing",
    "context": "1: We apply <mark>deterministic annealing</mark> to the English dataset, and the resulting hierarchical structure reflects properties of English syntax. At the top of the hierarchy, the model places nouns, adjectives, adverbs, and verbs in different clusters. At lower levels, the anaphors (\"yourself,\" \"herself\" . . .<br>2: Additionally, it has been experimentally demonstrated that for distribution-based clustering, smoothing cluster representatives by a prior using a <mark>deterministic annealing</mark> schedule leads to considerable improvements [17]. With smoothing controlled by a parameter α, each cluster representative µ µ µ h is estimated as follows when D I a is the distortion measure: \n<br>",
    "Arabic": "تلدين حراري تحديدي",
    "Chinese": "确定性退火",
    "French": "recuit déterministe",
    "Japanese": "決定論的アニーリング",
    "Russian": "детерминированное отжигание"
  },
  {
    "English": "deterministic automaton",
    "context": "1: A sequence of shift-reduce or attach-juxtapose actions is not identical to a parse tree, but it can be mapped to a tree using a <mark>deterministic automaton</mark> that interprets the discrete actions as operations on a tree fragment or stack of tree fragments.<br>",
    "Arabic": "الآلة المحددة",
    "Chinese": "确定性自动机",
    "French": "automate déterministe",
    "Japanese": "確定的オートマトン",
    "Russian": "детерминированный автомат"
  },
  {
    "English": "deterministic baseline",
    "context": "1: Stochasticity and Memory: The results of the embedded convection calculations regulating d o come from a chaotic dynamical system and thus could be worthy of architectures and metrics beyond the <mark>deterministic baselines</mark> in this paper. These solutions are likewise sensitive to sub-grid initial state variables from an interior nested spatial dimension that have not been included in our data.<br>",
    "Arabic": "الخط القاعدي المحدد",
    "Chinese": "确定性基线",
    "French": "ligne de base déterministe",
    "Japanese": "決定論的ベースライン",
    "Russian": "детерминированная базовая линия"
  },
  {
    "English": "deterministic finite automaton",
    "context": "1: Note that <mark>deterministic finite automata</mark> (DFA) with n states can be represented by a WFA with at most n states. Thus, the results we present here can be directly applied to classification problems in Σ . However, specializing our results to this particular setting may yield several improvements.<br>2: In fact, this is the core idea behind the cryptography-based hardness results for learning <mark>deterministic finite automata</mark> given by Kearns and Valiant [20] -these same results apply to our setting as well. But, even in cases where the distribution \"cooperates,\" there is still an obstruction in leveraging the spectral method for learning general weighted automata.<br>",
    "Arabic": "الآلة المحددة المتناهية",
    "Chinese": "确定有限自动机",
    "French": "automate fini déterministe",
    "Japanese": "確定性有限オートマトン (DFA)",
    "Russian": "детерминированный конечный автомат"
  },
  {
    "English": "deterministic policy",
    "context": "1: If this holds for all R, we then have that (T π − T π ′ )(I − γT π ) −1 is the zero matrix for all <mark>deterministic policies</mark> π, π ′ .<br>2: We denote as H the space of the histories of arbitrary length. We denote as Π the set of all the policies, and as Π D the set of <mark>deterministic policies</mark> π = (π t ) ∞ t=1 such that π t : H t → A.<br>",
    "Arabic": "السياسة الحتمية",
    "Chinese": "确定性策略",
    "French": "politique déterministe",
    "Japanese": "確定的ポリシー",
    "Russian": "детерминированная политика"
  },
  {
    "English": "deterministic rule",
    "context": "1: As a result, in the worst case, each <mark>deterministic rule</mark> picks a candidate whose total distance is at least three times larger than that of an optimal one, i.e., has distortion at least 3.<br>",
    "Arabic": "القاعدة الحتمية",
    "Chinese": "确定性规则",
    "French": "règle déterministe",
    "Japanese": "決定論的ルール",
    "Russian": "детерминированное правило"
  },
  {
    "English": "dev set",
    "context": "1: We analyze 200 incorrect predictions (i.e., EM=0) randomly sampled from GRAILQA's <mark>dev set</mark> for our best model (i.e., T5-3B). The major errors are due to unidentified topic entities during entity linking (62%).<br>2: The Transformer model uses a weight decay of 0 that is tuned based on <mark>dev set</mark> performance. Second, we increase the number of training epochs to 50 (or equivalently 64K training steps) since all models achieve better BLEU scores by training longer. This ensures we compare models when they reach the maximum performance.<br>",
    "Arabic": "مجموعة التطوير",
    "Chinese": "开发集",
    "French": "ensemble de développement",
    "Japanese": "評価用データセット",
    "Russian": "набор для разработки"
  },
  {
    "English": "development set",
    "context": "1: We split the dataset randomly into training, development, and test sets of 205, 43, and 42 articles, respectively.<br>2: Let G be a set of ground truth data, partitioned into a training set G train , a <mark>development set</mark> G dev and a test (evaluation) set G test . Let S be a system with arbitrary parameters and hyperparameters, and let M be an evaluation metric.<br>",
    "Arabic": "مجموعة التطوير",
    "Chinese": "开发集",
    "French": "ensemble de développement",
    "Japanese": "開発セット",
    "Russian": "разработочный набор"
  },
  {
    "English": "diagonal matrix",
    "context": "1: Specifically, let P be any permutation matrix and {S i } b i=1 , {S j } b j=1 be any invertible <mark>diagonal matrices</mark> (i.e., <mark>diagonal matrices</mark> without any zeros on the diagonal). Define D ij = S i P D ij PS j for all i, j.<br>2: For an integer d and an integer k ≤ d, we define I k ⊆ R d×d to be the set of all <mark>diagonal matrices</mark> M ∈ {0, 1} d×d which contain only ones and zeros and have exactly k ones and d − k zeros along its diagonal.<br>",
    "Arabic": "مصفوفة قطرية",
    "Chinese": "对角矩阵",
    "French": "matrice diagonale",
    "Japanese": "対角行列",
    "Russian": "диагональная матрица"
  },
  {
    "English": "dialog system",
    "context": "1: Given an interaction between a user and the <mark>dialog system</mark>, consisting of a sequence of user utterances u along with intent and slots (î,ŝ) predicted by the <mark>dialog system</mark>, we find all pairs (u j , u k ), where u j occurs before u k , that satisfy the following condition: \n time ( u k ) − time ( u j ) ≤ δ ∧ u j = u k ∧ P D ( u j , u k ) = 1 ∧ F D ( u j , î j , ŝ j ) = 1 ∧ F D ( u k , î k , ŝ k ) = 0 ∧ LP (<br>2: We interpret the annotation data as such: the spotted event occurred if the system was annotated as \"bot\" and it survived if it was annotated as \"unsure\" or \"human\". Let k be the number of exchanges in the annotated conversation segment, meaning that each <mark>dialog system</mark> produced k outputs.<br>",
    "Arabic": "نظام الحوار",
    "Chinese": "对话系统",
    "French": "système de dialogue",
    "Japanese": "対話システム",
    "Russian": "диалоговая система"
  },
  {
    "English": "dialogue act",
    "context": "1: . \n In order to ensure that the generated utterance represents the intended meaning, the generator is further conditioned on a control vector d, a 1-hot representation of the <mark>dialogue act</mark> (DA) type and its slot-value pairs. Although a related work ( Karpathy and Fei-Fei , 2014 ) has suggested that reapplying this auxiliary information to the RNN at every time step can increase performance by mitigating the vanishing gradient problem ( Mikolov and Zweig , 2012 ; Bengio et al. , 1994 ) , we have found that such a model also omits and duplicates slot information in the surface<br>2: The dialogue control module chooses the best system <mark>dialogue act</mark> at every dialogue point using the user <mark>dialogue act</mark> as input. The utterance generation module generates natural-language utterances and says them to users by realizing the system <mark>dialogue act</mark>s as surface forms. This paper focuses on the dialogue control module of a listening agent.<br>",
    "Arabic": "فعل الحوار",
    "Chinese": "对话行为",
    "French": "acte de dialogue",
    "Japanese": "対話行為",
    "Russian": "речевой акт"
  },
  {
    "English": "dialogue context",
    "context": "1: The task of next utterance prediction predicts the next utterance given the <mark>dialogue context</mark>. It tests the language understanding ability of generative models. For this task, we use a large-scale dialogue dataset, Persona-Chat (Zhang et al., 2018).<br>",
    "Arabic": "مُحتوى الحوار",
    "Chinese": "对话上下文",
    "French": "contexte du dialogue",
    "Japanese": "対話コンテキスト",
    "Russian": "диалоговый контекст"
  },
  {
    "English": "dialogue generation",
    "context": "1: Experimental results indicate that PCC surpasses competitive baselines on few-shot text classification as well as <mark>dialogue generation</mark>. Human evaluation indicates that bottom-k sampling effectively generates grammatically and lexically rich paraphrases, and PCC significantly improves our baseline dialogue agent. To our best knowledge, this is the first time to apply CDA on a generation task.<br>2: 1) Dialogue Generation: Our <mark>dialogue generation</mark> module uses two agents (A and B) to assist event extraction through a sequence of question-answer conversations. Here, Agent A generates dialogue content according to the currently processed role. For each current role, it generates a question set [7] to create more training data for argument extraction.<br>",
    "Arabic": "توليد الحوار",
    "Chinese": "对话生成",
    "French": "génération de dialogues",
    "Japanese": "対話生成",
    "Russian": "генерация диалогов"
  },
  {
    "English": "dialogue history",
    "context": "1: The slot gate predicts whether the j-th (domain, slot) pair is triggered by the dialogue. encode the <mark>dialogue history</mark>. The input to the utterance encoder is denoted as history X t = [U t−l , R t−l , . . .<br>",
    "Arabic": "تاريخ الحوار",
    "Chinese": "对话历史记录",
    "French": "historique du dialogue",
    "Japanese": "対話履歴",
    "Russian": "диалоговая история"
  },
  {
    "English": "dialogue management",
    "context": "1: We use the predicted triggers together with the text as the input of the event classification model to predict the event type corresponding to the current triggers. In our dialogue guided model, we use our event representation module and multi-turn dialogue model without reinforcement learning. In our full model, we introduce reinforcement learning-based <mark>dialogue management</mark>.<br>2: Where R t is the argument role selected through the reinforcement learning-based <mark>dialogue management</mark>, c A t , c B t are the current embedding of Agent A and B, Q 0 is the initial question of Agent A about event type, H C represents the history of the conversation.<br>",
    "Arabic": "إدارة الحوار",
    "Chinese": "对话管理",
    "French": "gestion du dialogue",
    "Japanese": "対話管理",
    "Russian": "управление диалогом"
  },
  {
    "English": "dialogue state",
    "context": "1: We consider the Joint Goal Accuracy where the inferred label is correct only if the predicted <mark>dialogue state</mark> is exactly equal to the ground truth to provide labels for the breakdown task. We use oracle dialogue history and the metric scores are produced only for the current utterance spoken by the user.<br>2: In the <mark>dialogue state</mark> tracking task, a model needs to map the user's goals and intents in a given conversation to a set of slots and values, known as a <mark>dialogue state</mark>, based on a pre-defined ontology.<br>",
    "Arabic": "حالة الحوار",
    "Chinese": "对话状态",
    "French": "état du dialogue",
    "Japanese": "対話状態",
    "Russian": "состояние диалога"
  },
  {
    "English": "dialogue state tracker",
    "context": "1: Our aim is to determine how reliable MT metrics are for predicting success on downstream tasks. Our setup uses a monolingual model (e.g., a <mark>dialogue state tracker</mark>) trained on a task language and parallel test data from multiple languages.<br>",
    "Arabic": "تتبع حالة الحوار",
    "Chinese": "对话状态跟踪器",
    "French": "suivi de l'état du dialogue",
    "Japanese": "対話状態追跡器",
    "Russian": "отслеживатель состояния диалога"
  },
  {
    "English": "dialogue state tracking",
    "context": "1: <mark>Dialogue State Tracking</mark> Traditional dialogue state tracking models combine semantics extracted by language understanding modules to estimate the current dialogue states ( Williams and Young , 2007 ; Thomson and Young , 2010 ; Wang and Lemon , 2013 ; Williams , 2014 ) , or to jointly learn speech understanding ( Henderson et al. , 2014b ; Zilka and Jurcicek , 2015 ;<br>",
    "Arabic": "تتبع حالة الحوار",
    "Chinese": "对话状态跟踪",
    "French": "suivi de l'état du dialogue",
    "Japanese": "ダイアログ状態追跡",
    "Russian": "отслеживание состояния диалога"
  },
  {
    "English": "dialogue system",
    "context": "1: Furthermore, the human judges annotate the entities with respect to more fine-grained features, which can be chosen based on characteristics that the bots are expected to exhibit (e.g. fluency or informativeness). The Survival Analysis further allows to pin down the features that contribute to a <mark>dialogue system</mark>'s survival, enabling interpretable results.<br>2: In engineering disciplines, it is applied to estimate the time to failure of machine components (Eyal et al., 2014). In our case, we are interested in the time, corresponding to the number of exchanges, until a <mark>dialogue system</mark> is spotted as such.<br>",
    "Arabic": "نظام الحوار",
    "Chinese": "对话系统",
    "French": "système de dialogue",
    "Japanese": "対話システム",
    "Russian": "диалоговая система"
  },
  {
    "English": "dialogue turn",
    "context": "1: where t is the <mark>dialogue turn</mark>, u t is the t-th response, and kt i and kc i (1 <= i <= M ) are the knowledge type and knowledge content associated with the target response, respectively (M is the maximum number of knowledge associated with u t .)<br>",
    "Arabic": "مناوبة الحوار",
    "Chinese": "对话轮次",
    "French": "tour de dialogue",
    "Japanese": "対話ターン",
    "Russian": "диалоговый ход"
  },
  {
    "English": "dice coefficient",
    "context": "1: COS(w 1 , w 2 ) = DICE PROD applied to unit vectors (6) The function DICE PROD is not well known in the word similarity literature, but in the data mining literature it is often just called <mark>Dice coefficient</mark>, because it generalized the set comparison function of Dice (1945).<br>2: In practice, the 4 similarity function is most commonly used, defined as the <mark>Dice coefficient</mark> (or F 1 score) between and :  \n Here we see that 4 computes a version of macro-average over entities, whereas 3 computes a micro-average. The CEAF that uses 4 is sensibly denoted CEAF 4 in coreference resolution.<br>",
    "Arabic": "معامل دايس",
    "Chinese": "骰子系数",
    "French": "coefficient de Dice",
    "Japanese": "ダイス係数",
    "Russian": "коэффициент Дайса"
  },
  {
    "English": "dice loss",
    "context": "1: Online mapping loss. As in [56], this includes thing losses for lanes, dividers, and contours, also a stuff loss for the drivable area, where Focal loss is responsible for classification, L1 loss is responsible for thing bounding boxes, <mark>Dice loss</mark> and GIoU loss [80] account for segmentation.<br>2: The output of instance-level occupancy prediction is a binary segmentation of each agent, therefore we adopt binary cross-entropy and <mark>Dice loss</mark> [67] as the occupancy loss. Formally, L occ = λ bce L bce + λ dice L dice , with λ bce = 5 and λ dice = 1 here.<br>",
    "Arabic": "خسارة النرد",
    "Chinese": "骰子损失",
    "French": "perte de Dice",
    "Japanese": "ダイス損失",
    "Russian": "функция потерь Дайса"
  },
  {
    "English": "dictionary learning",
    "context": "1: Classical <mark>dictionary learning</mark> techniques (Olshausen & Field, 1997;Aharon et al., 2006;Lee et al., 2007) consider a finite training set of signals X = [x 1 , . . . , x n ] in R m×n and optimize the empirical cost function \n<br>2: We show in this paper that it is possible to go further and exploit the specific structure of sparse coding in the design of an optimization procedure dedicated to the problem of <mark>dictionary learning</mark>, with low memory consumption and lower computational cost than classical second-order batch algorithms and without the need of explicit learning rate tuning.<br>",
    "Arabic": "تعلم القاموس",
    "Chinese": "词典学习",
    "French": "apprentissage de dictionnaire",
    "Japanese": "辞書学習",
    "Russian": "обучение словаря"
  },
  {
    "English": "dictionary matrix",
    "context": "1: We solve problem (1) by alternating minimization over the <mark>dictionary matrix</mark> D and the code vectors γ. The techniques we use are very similar to standard methods for sparse coding and dictionary learning, see e.g. (Jenatton et al., 2011) and references therein for more information.<br>",
    "Arabic": "مصفوفة القاموس",
    "Chinese": "字典矩阵",
    "French": "matrice de dictionnaire",
    "Japanese": "辞書行列",
    "Russian": "матрица словаря"
  },
  {
    "English": "diffeomorphism",
    "context": "1: A manifold CNF, Φ : M → M, is an orientation preserving <mark>diffeomorphism</mark> from the manifold to itself (Mathieu and Nickel, 2020;Lou et al., 2020;Falorsi and Forré, 2020). A smooth map Φ : M → M can be used to pull-back the targetμ according to the formula: \n<br>2: We are interested in the special case of neural embeddings when r = k since then (for any d) the mapping f (v i ) → e(v i ) is a <mark>diffeomorphism</mark>: 8 a smooth invertible function of R k . An example of such a <mark>diffeomorphism</mark> is shown in Figure 1.<br>",
    "Arabic": "تباين",
    "Chinese": "微分同胚",
    "French": "difféomorphisme",
    "Japanese": "微分同相写像",
    "Russian": "диффеоморфизм"
  },
  {
    "English": "differentiable function",
    "context": "1: Here, P is parameterized by a domain-dependent low-dimensional vector φ, which is dependent on a <mark>differentiable function</mark> M 1 (x). Similarly, the loss function is taken after applying any differentiable M 2 (u * , v * ).<br>2: We introduce SRNs, a 3D-structured neural scene representation that implicitly represents a scene as a continuous, <mark>differentiable function</mark>. This function maps 3D coordinates to a feature-based representation of the scene and can be trained end-to-end with a differentiable ray marcher to render the feature-based representation into a set of 2D images.<br>",
    "Arabic": "دالة قابلة للتفاضل",
    "Chinese": "可微函数",
    "French": "fonction différentiable",
    "Japanese": "微分可能関数",
    "Russian": "дифференцируемая функция"
  },
  {
    "English": "differentiable renderer",
    "context": "1: Since our model generates images from an internal 3D representation, one essential component is a <mark>differentiable renderer</mark>. However, with a traditional rendering pipeline,  gradients across occlusions and boundaries are not defined. Several soft relaxations have thus been proposed [37,31,34]. Here, we use an implementation 1 of [31].<br>2: We propose an inverse-rendering method which uses a <mark>differentiable renderer</mark> designed for triangle meshes to optimize geometry defined using parametric levelsets. As in the case of recent methods [4,43,71], we use an analysis-by-synthesis approach. A photometric error comparing the captured and the rendered images is minimized using gradient descent.<br>",
    "Arabic": "مقدم الرسومات قابل للتفاضل",
    "Chinese": "可微渲染器",
    "French": "rendu différentiable",
    "Japanese": "微分可能なレンダラー",
    "Russian": "дифференцируемый рендерер"
  },
  {
    "English": "differentiable rendering",
    "context": "1: Henzler et al. [32] learn voxel-based representations using <mark>differentiable rendering</mark>. The results are 3D controllable, but show artifacts due to the limited voxel resolutions caused by their cubic memory growth. Nguyen-Phuoc et al. [63,64] propose voxelized feature-grid representations which are rendered to 2D via a reshaping operation.<br>2: Methods overcoming the need for 3D supervision are based on <mark>differentiable rendering</mark> allowing for comparison of 2D images rather than 3D shapes [51,63,31]. Generating images from meshes was achieved via soft rasterization in [32,40,61,30,7,36,77,20,67].<br>",
    "Arabic": "التقديم قابل للتفاضل",
    "Chinese": "可微渲染",
    "French": "rendu différenciable",
    "Japanese": "微分可能なレンダリング",
    "Russian": "дифференцируемый рендеринг"
  },
  {
    "English": "differentiable rendering function",
    "context": "1: [42] use a less direct neural 3D representation that simply outputs a feature vector and RGB color at each continuous 3D coordinate, and propose a <mark>differentiable rendering function</mark> consisting of a recurrent neural network that marches along each ray to decide where the surface is located.<br>",
    "Arabic": "دالة العرض قابلة للتفاضل",
    "Chinese": "可微渲染函数",
    "French": "fonction de rendu différentiable",
    "Japanese": "微分可能なレンダリング関数",
    "Russian": "функция дифференцируемого рендеринга"
  },
  {
    "English": "differential entropy",
    "context": "1: In fact, a special case of our formulation can be viewed as a maximum entropy objective: maximize the <mark>differential entropy</mark> of a multivariate Gaussian subject to constraints on the associated Mahalanobis distance. Our formulation is quite general: we can accommodate a range of constraints, including similarity or dissimilarity constraints, and relations between pairs of distances.<br>",
    "Arabic": "إنتروبية تفاضلية",
    "Chinese": "微分熵",
    "French": "entropie différentielle",
    "Japanese": "微分エントロピー",
    "Russian": "дифференциальная энтропия"
  },
  {
    "English": "differential privacy",
    "context": "1: Liu & Talwar (2019) show that, if we start with a (ε, 0)-DP algorithm, repeatedly run it a random number of times following a geometric distribution, and finally return the best output produced by these runs, then this system satisfies (3ε, 0)-<mark>differential privacy</mark>.<br>2: For a broad class of utility functions, they show that a core solution can be found in polynomial time by solving a suitable convex program. They also use <mark>differential privacy</mark> to design a mechanism for this setting which satisfies approximate versions of efficiency, truthfulness, and the core.<br>",
    "Arabic": "الخصوصية التفاضلية",
    "Chinese": "差分隐私",
    "French": "confidentialité différentielle",
    "Japanese": "差分プライバシー",
    "Russian": "дифференциальная конфиденциальность"
  },
  {
    "English": "diffusion matrix",
    "context": "1: We lastly need to show that the <mark>diffusion matrix</mark> Σ n goes to zero as n → ∞ when δ n = O(1/n).<br>2: Notice that this <mark>diffusion matrix</mark> is rank 1, so this diffusion is non-trivial but degenerate even in the rescaled coordinates (ṽ i ,m i ). Moreover, the entries ofΣ vanish on the axes a 1 = 0 or a 2 = 0.<br>",
    "Arabic": "مصفوفة الانتشار",
    "Chinese": "扩散矩阵",
    "French": "matrice de diffusion",
    "Japanese": "拡散行列",
    "Russian": "матрица диффузии"
  },
  {
    "English": "diffusion model",
    "context": "1: While this method is limited by the expressiveness of the frozen <mark>diffusion model</mark>, our fine-tuning approach enables us to embed the subject within the model's output domain, resulting in the generation of novel images of the subject which preserve its key visual features.<br>2: While we continue to rely on the commonly used network architectures (DDPM [16], NCSN [48]), we provide the first principled analysis of the preconditioning of the networks' inputs, outputs, and loss functions in a <mark>diffusion model</mark> setting and derive best practices for improving the training dynamics.<br>",
    "Arabic": "نموذج الانتشار",
    "Chinese": "扩散模型",
    "French": "modèle de diffusion",
    "Japanese": "拡散モデル",
    "Russian": "модель диффузии"
  },
  {
    "English": "diffusion tensor",
    "context": "1: A regularization process may be also more locally designed, as a diffusion of pixel values, viewed as chemical concentrations or temperatures [51], [20], and directed by a 2 Â 2 <mark>diffusion tensor</mark> D (symmetric and definite-positive matrix): \n<br>2: Indeed, it explains the link between the <mark>diffusion tensor</mark> shapes in divergence or trace-based equations, and the actual smoothing performed by these processes, in term of local filtering. From this general study, we defined a new and particular regularization equation, based on the respect of a coherent anisotropic smoothing preserving the global features of vector images.<br>",
    "Arabic": "موتر الانتشار",
    "Chinese": "扩散张量",
    "French": "tenseur de diffusion",
    "Japanese": "拡散テンソル",
    "Russian": "тензор диффузии"
  },
  {
    "English": "digamma function",
    "context": "1: = bias ( ln Z ) 2 + var ( ln Z ) = ( ln M − ψ ( M ) ) 2 + ψ 1 ( M ) , \n where ψ(•) is the <mark>digamma function</mark> and ψ 1 (•) is the trigamma function.<br>2: d dα n ln Γ(1 + α) α = n ψ(1 + α)α − ln Γ(1 + α) α 2 , \n where ψ is the <mark>digamma function</mark> (logarithmic derivative of the gamma function). Applying L'Hôpital's rule we note that \n<br>",
    "Arabic": "دالة ديجاما",
    "Chinese": "双伽玛函数",
    "French": "fonction digamma",
    "Japanese": "ディガンマ関数",
    "Russian": "дигамма-функция"
  },
  {
    "English": "digit classification",
    "context": "1: In order to evaluate the robustness of our proposed R-SVM+ algorithm, we carry out experiments on three real-world datasets for <mark>digit classification</mark>, face pose classification and human activity recognition tasks, respectively.<br>",
    "Arabic": "تصنيف الأرقام",
    "Chinese": "数字分类",
    "French": "classification de chiffres",
    "Japanese": "数字分類",
    "Russian": "классификация цифр"
  },
  {
    "English": "dilate convolution",
    "context": "1: The ASPP module consists of multiple <mark>dilated convolution</mark> filters with different dilation rates of 3,6 and 9. Our decoder network applies bilinear upsampling at each step, concatenated with the skip connection from the backbone, and followed by a 3×3 convolution, Batch Normalization [14], and ReLU activation [22] (except the last layer).<br>",
    "Arabic": "تمدد الالتواء",
    "Chinese": "扩张卷积",
    "French": "convolution dilatée",
    "Japanese": "希釈畳み込み",
    "Russian": "\"расширенная свертка\""
  },
  {
    "English": "dilation",
    "context": "1: The trimaps used as input for trimap-based methods and for defining the error metric regions are obtained by thresholding the grouth-truth alpha between 0.06 and 0.96, then applying 10 iterations of <mark>dilation</mark> followed by 10 iterations of erosion using a 3×3 circular kernel.<br>",
    "Arabic": "توسيع",
    "Chinese": "膨胀",
    "French": "dilatation",
    "Japanese": "膨張",
    "Russian": "расширение"
  },
  {
    "English": "dim",
    "context": "1: We say that a point-line problem (p, l, I, m) is balanced if <mark>dim</mark>(X p,l,I × C m ) = <mark>dim</mark>(Y p,l,I,m ).<br>2: As SO(3) is three-<mark>dim</mark>ensional, and we set the first camera to [I 0] and one parameter in the second camera to 1, the parameter space of camera configurations for m ≥ 2 has <mark>dim</mark>ension <mark>dim</mark>(C m ) = 6m − 7.<br>",
    "Arabic": "البُعد",
    "Chinese": "维数",
    "French": "dim",
    "Japanese": "次元",
    "Russian": "размерность"
  },
  {
    "English": "dimension",
    "context": "1: 8: else C = new cluster({q i }); Θ ∪= C; 9: for each non-zero <mark>dimension</mark> d of − → q i do 10: if C / ∈ dim array[d] then link C to dim array[d]; \n 11: return Θ; \n<br>2: While a drop is still present, we are able to reach peak performance at 1024 <mark>dimension</mark>s, which is lower than the representation's <mark>dimension</mark> of 2048 and shows that a large embedding <mark>dimension</mark> is not a deciding factor in downstream performance.<br>",
    "Arabic": "البعد",
    "Chinese": "维度",
    "French": "dimension",
    "Japanese": "次元",
    "Russian": "размерность"
  },
  {
    "English": "dimension reduction",
    "context": "1: That is, SVD is applied to the sub-matrix of the predictor (weight) matrix corresponding to each feature group , which results in more focused <mark>dimension reduction</mark> of the predictor matrix. For example, suppose that ËÊ .<br>2: RoBERTa pre-computed embeddings (applied PCA for <mark>dimension reduction</mark>) are shown in two-dimensional space (top row) and histograms regarding d 1 (bottom row) with the bin size being 100. Data points are colored depending on the label (i.e., the answer y is option 1 (blue) or 2 (red)).<br>",
    "Arabic": "\"تقليل الأبعاد\"",
    "Chinese": "降维",
    "French": "réduction de dimension",
    "Japanese": "次元削減",
    "Russian": "снижение размерности"
  },
  {
    "English": "dimensional vector",
    "context": "1: x im represents the m th component of the d-<mark>dimensional vector</mark> x i . The term \"distance measure\" is used synonymously with \"distortion measure\" throughout the paper.<br>2: Conventional linear algebra suggests that we must predict d parameters in order to find the value of the d-<mark>dimensional vector</mark> E[y|x] for each x.<br>",
    "Arabic": "متجه بعدي",
    "Chinese": "维度向量",
    "French": "vecteur dimensionnel",
    "Japanese": "次元ベクトル",
    "Russian": "многомерный вектор"
  },
  {
    "English": "dimensionality",
    "context": "1: This behavior is due to their similarity to statistical methods [17] which work well if sample size is much greater than <mark>dimensionality</mark> because of problems arising with covariance matrix estimation (for more see e.g. [29]).<br>2: where p is the <mark>dimensionality</mark> of data, j is the index of jth similarity feature, vector superscript k denotes k-th element of original data vector and α is the normalization coefficient. This way the new dimension equal to N tr is obtained. In our study half of the 400 data vectors were used for training.<br>",
    "Arabic": "أبعاد",
    "Chinese": "维度",
    "French": "dimensionnalité",
    "Japanese": "次元数",
    "Russian": "размерность"
  },
  {
    "English": "dimensionality reduction",
    "context": "1: As a preliminary attempt to apply our algorithms in the real world, we show that the proposed clustering method (which is one of the most crucial step in our overall approach), without any modification, works reasonably well even for this dataset. To be concrete, we apply Algorithm 3 (without <mark>dimensionality reduction</mark>, i.e.<br>2: In this section, we show how to leverage Algorithm 2 in order to construct coresets for <mark>dimensionality reduction</mark> algorithms such as the widely used Principal Component Analysis (PCA) and Singular Value Decomposition (SVD). We first briefly define the j-SVD and j-PCA problems.<br>",
    "Arabic": "تقليل الأبعاد",
    "Chinese": "降维",
    "French": "réduction de la dimensionnalité",
    "Japanese": "次元削減",
    "Russian": "снижение размерности"
  },
  {
    "English": "dirac distribution",
    "context": "1: p 110 denotes a <mark>Dirac distribution</mark> at the feedback vector y = [1, 1, 0]. p 001 is defined similarly. For the Pairwise Disagreement, p 1 2 3 is the <mark>Dirac distribution</mark> at the DAG containing the edges 1 → 2, 2 → 3 and 1 → 3, i.e.<br>2: Note that σ 0 cannot be zero because in the limiting case of σ → 0, the kernels approach a <mark>Dirac distribution</mark>, which means the limiting kernel is not bounded and therefore the definition of MMD in (1) does not hold.<br>",
    "Arabic": "توزيع ديراك",
    "Chinese": "狄拉克分布",
    "French": "distribution de Dirac",
    "Japanese": "ディラック分布",
    "Russian": "дельта-функция"
  },
  {
    "English": "direct acyclic graph",
    "context": "1: , and negation ( ¬ ) of their characteristic functions , respectively . Binary Decision Diagrams (BDDs) (Bryant 1986) are a efficient data-structure to represent Boolean functions in the form of a <mark>directed acyclic graph</mark>. The size of a BDD is the number of nodes in this representation.<br>2: A Task Graph (TG) G = G(T, D) is a <mark>directed acyclic graph</mark> where each node represents a computational module in the application referred to as task t i ∈ T . Each directed arc d i,j ∈ D, between tasks t i and t j , characterizes either data or control dependencies.<br>",
    "Arabic": "رسم بياني موجه لاحلقي",
    "Chinese": "有向无环图",
    "French": "graphe acyclique orienté",
    "Japanese": "非循環有向グラフ",
    "Russian": "направленный ациклический граф"
  },
  {
    "English": "direct edge",
    "context": "1: As a useful working example, consider a \"communication graph,\" in which nodes are e-mail addresses, and there is a <mark>directed edge</mark> (u, v) if u has sent at least a certain number of e-mail messages or instant messages to v, or if v is included in u's address book.<br>",
    "Arabic": "حافة موجهة",
    "Chinese": "有向边",
    "French": "arête directe",
    "Japanese": "直接エッジ",
    "Russian": "прямое ребро"
  },
  {
    "English": "direct graph",
    "context": "1: Given a <mark>directed graph</mark>, a strongly connected component (strong component for brevity) of this graph is a set of nodes such that for any pair of nodes u and v in the set there is a path from u to v. In general, a <mark>directed graph</mark> may have one or many strong components.<br>2: Finally, in this section, we describe the SimRank join problem, which is formulated as follows. PROBLEM 4.1 (SIMRANK JOIN). Given a <mark>directed graph</mark> G = ( V , E ) and a threshold θ ∈ [ 0 , 1 ] , find all pairs of vertices ( i , j ) ∈ V × V for which the SimRank score of ( i , j ) is greater than the threshold , i.e. , s ( i , j ) ≥ θ ,<br>",
    "Arabic": "رسم بياني موجه",
    "Chinese": "有向图",
    "French": "graphe orienté",
    "Japanese": "直接グラフ",
    "Russian": "прямой граф"
  },
  {
    "English": "direct graphical model",
    "context": "1: The tree-width corresponding to this <mark>directed graphical model</mark> can be very high, and we need to perform integration for functions involving many continuous variables. Second, the integral in general can not be eval-uated analytically for heterogeneous transmission functions, which means that we need to resort to numerical integration by discretizing the domain [0, ∞).<br>2: In this sense, our Monte Carlo EM algorithm breaks the curse of dimensionality using randomness. It is worth noting that the approach described here differs considerably from that of learning the structure of a <mark>directed graphical model</mark> or Bayesian network [9,10].<br>",
    "Arabic": "نموذج رسومي موجه",
    "Chinese": "直接图模型",
    "French": "modèle graphique dirigé",
    "Japanese": "直接グラフィカルモデル",
    "Russian": "прямая графическая модель"
  },
  {
    "English": "direct tree",
    "context": "1: The conditions under which a pair E, D forms a valid derivation for a sentence x are similar to those in conventional LTAGs. Each i, η ∈ E must be such that η is an elementary tree whose anchor is the word x i . The dependencies D must form a directed, projective tree spanning words 0 . . .<br>",
    "Arabic": "شجرة موجهة",
    "Chinese": "有向树",
    "French": "arbre dirigé",
    "Japanese": "直接木",
    "Russian": "направленное дерево"
  },
  {
    "English": "dirichlet distribution",
    "context": "1: The final chip assignment shown in Figure 3(d 2 ) represents the prior (hypothesis). In detail, each row corresponds to a <mark>Dirichlet distribution</mark> with corresponding pseudo counts (hyperparameters) α i, j (e.g., α C,B = 5).<br>2: θ0 ∼ DP(b0, H(•)) ψt ∼ Dir(β) θ d ∼ DP(b1, θ0) z di ∼ Discrete(θ d ) w di ∼ Discrete (ψz di ) \n<br>",
    "Arabic": "توزيع ديريشليه",
    "Chinese": "第里夏分布",
    "French": "distribution de Dirichlet",
    "Japanese": "ディリクレ分布",
    "Russian": "распределение Дирихле"
  },
  {
    "English": "dirichlet process",
    "context": "1: Similar to Poisson <mark>Dirichlet Process</mark>, an equivalent Chinese Restaurant Franchise analogy [6,19] exists for Hierarchical <mark>Dirichlet Process</mark> with multiple levels. In this analogy, each <mark>Dirichlet Process</mark> is mapped to a single Chinese Restau-rant Process, and the hierarchical structure is constructed to identify the parent and children of each restaurant. The general ( collapsed ) structure is as follows : let Nj be the total number of customers in restaurant j and njt be the number of customers in restaurant j served with dish t. When a new customer ( a token ) enters restaurant j with the corresponding <mark>Dirichlet Process</mark> DP ( bj , Hj ( • ) ) , there are two<br>2: By construction, DP(b0, H(•)) is a <mark>Dirichlet Process</mark>, equivalent to a Poisson <mark>Dirichlet Process</mark> PDP(b0, a, H(•)) with the discount parameter a set to 0. The base distribution H(.) is often assumed to be a uniform distribution in most cases.<br>",
    "Arabic": "عملية ديريشليه",
    "Chinese": "狄利克雷过程",
    "French": "processus de Dirichlet",
    "Japanese": "ディリクレ過程",
    "Russian": "процесс Дирихле"
  },
  {
    "English": "disambiguation",
    "context": "1: We believe that automated tagging is essential to bootstrap the Semantic Web. As the results of the experiments with SemTag show, it is possible to achieve interestingly high levels of accuracy even with relatively simple approaches to <mark>disambiguation</mark>. In the future we expect that there will be many different approaches and algorithms to automated tagging.<br>2: However, carrying out both of these unsupervised learning tasks at once is problematic in view of the very large number of parameters to be estimated compared to the size of the training data set. The POS-tagging subtask of <mark>disambiguation</mark> may then be construed as a challenge in its own right: demonstrate effective <mark>disambiguation</mark> in an unsupervised model.<br>",
    "Arabic": "تمييز",
    "Chinese": "消歧义",
    "French": "désambiguïsation",
    "Japanese": "曖昧性解消",
    "Russian": "разрешение неоднозначности"
  },
  {
    "English": "discount cumulative reward",
    "context": "1: To incorporate the influence of the action a t on the future and account for the local greedy search, we use the <mark>discounted cumulative reward</mark> rather than the immediate reward to train the policy: \n<br>",
    "Arabic": "مكافأة تراكمية مخفضة",
    "Chinese": "折扣累计奖励",
    "French": "récompense cumulée actualisée",
    "Japanese": "割引累積報酬",
    "Russian": "дисконтированное кумулятивное вознаграждение"
  },
  {
    "English": "discount factor",
    "context": "1: It is well-known that the discounted reward of this scheme is maximized when q jt is set to the Gittins index (described next) of Q jt with <mark>discount factor</mark> γ a , and hence setting f (Q jt ) = the Gittins index of Q j,t would be a natural choice.<br>2: (2015) in applying a <mark>discount factor</mark> in order to prevent very rare words from biasing the results (see their Equation 3), and unless otherwise stated we used the same <mark>discount factor</mark> (0.75) that they did.<br>",
    "Arabic": "معامل الخصم",
    "Chinese": "折扣因子",
    "French": "facteur d'actualisation",
    "Japanese": "割引率",
    "Russian": "коэффициент дисконтирования"
  },
  {
    "English": "discount parameter",
    "context": "1: However, this form of Bayesian clustering imposes an implicit a priori \"rich get richer\" property, leading to partitions consisting of a small number of large clusters. To combat this, the use of <mark>discount parameter</mark> d is used to reduce the probability of adding a new observation to an existing cluster.<br>2: The <mark>discount parameter</mark> a prevents a word to be sampled too often by imposing a penalty on its probability based on its frequency. The combined model described explicityly in [5]: \n<br>",
    "Arabic": "معامل الخصم",
    "Chinese": "折扣参数",
    "French": "paramètre d'escompte",
    "Japanese": "割引パラメータ",
    "Russian": "параметр дисконтирования"
  },
  {
    "English": "discount return",
    "context": "1: As opposed to (Sutton, Precup, and Singh 1999), we did not encode this knowledge ourselves but simply let the agents find options that would maximize the expected <mark>discounted return</mark>. In the Pinball domain (Konidaris and Barto 2009), a ball must be guided through a maze of arbitrarily shaped polygons to a designated target location.<br>2: µ Ω (s, ω | s 0 , ω 0 ) = ∞ t=0 γ t P (s t = s, ω t = ω | s 0 , ω 0 ). The proof is in the appendix. This gradient describes the effect of a local change at the primitive level on the global expected <mark>discounted return</mark>.<br>",
    "Arabic": "العائد المخفض",
    "Chinese": "折现回报",
    "French": "rendement actualisé",
    "Japanese": "割引リターン",
    "Russian": "дисконтированная награда"
  },
  {
    "English": "discount reward",
    "context": "1: R extr (s t , a t ) = r(s t , a t ) immediate reward + T t =t+1 γ t −t r(s t , a t ) \n discounted future reward (12) where γ is the discounted factor (0.95 in our experiments).<br>",
    "Arabic": "مكافأة مخصومة",
    "Chinese": "折扣奖励",
    "French": "Récompense actualisée",
    "Japanese": "割引報酬",
    "Russian": "дисконтированное вознаграждение"
  },
  {
    "English": "discount state distribution",
    "context": "1: where d π (•) is either a stationary state distribution (Mutti & Restelli, 2020), a <mark>discounted state distribution</mark> (Hazan et al., 2019;Tarbouriech & Lazaric, 2019), or a marginal state distribution (Lee et al., 2019;Mutti et al., 2021).<br>",
    "Arabic": "توزيع الحالة المخصومة",
    "Chinese": "折扣状态分布",
    "French": "distribution d'états actualisée",
    "Japanese": "割引状態分布",
    "Russian": "распределение дисконтированных состояний"
  },
  {
    "English": "discrete distribution",
    "context": "1: All our models are trained and evaluated on the loglikelihood loss function coming from a <mark>discrete distribution</mark>. Although natural image data is usually modeled with continuous distributions using density functions, we can compare our results with previous art in the following way.<br>2: We consider the problem of maximizing the objective function E qη [f (x)] with respect to the parameters η of a <mark>discrete distribution</mark> q η (x).<br>",
    "Arabic": "التوزيع المتقطع",
    "Chinese": "离散分布",
    "French": "distribution discrète",
    "Japanese": "離散分布",
    "Russian": "дискретное распределение"
  },
  {
    "English": "discrete graphical model",
    "context": "1: One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014;Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in <mark>discrete graphical models</mark>, replacing i.i.d. Gumbel noise with a \"low-rank\" approximation.<br>2: In particular, for the Gumbel trick to yield computational benefits for <mark>discrete graphical models</mark>, Gumbel perturbations on all configurations are typically replaced with socalled low-rank perturbations.<br>",
    "Arabic": "نموذج رسومي متقطع",
    "Chinese": "离散图模型",
    "French": "modèle graphique discret",
    "Japanese": "離散グラフィカルモデル",
    "Russian": "дискретная графическая модель"
  },
  {
    "English": "discrete random variable",
    "context": "1: Once Π has selected a plan a ∈ A it uses maximum entropy inference to derive the {D s i } n i=1 and minimum relative entropy inference to update those distributions as new data becomes available. Entropy, H, is a measure of uncertainty [7] in a probability distribution for a <mark>discrete random variable</mark> X: \n<br>",
    "Arabic": "المتغير العشوائي المنفصل",
    "Chinese": "离散随机变量",
    "French": "variable aléatoire discrète",
    "Japanese": "離散確率変数",
    "Russian": "дискретная случайная величина"
  },
  {
    "English": "discretization",
    "context": "1: <mark>Discretization</mark> Regret Using discretization levels Ψ with a minimum gap ∆, let OPT Ψ be the true optimum value and ⃗ β ⋆ Ψ be the optimal solution when the rewards of all discretized points are fully known. Let the true optimum and the optimal solution without discretization be OPT and ⃗ β ⋆ . Then we have \n OPT = µ ( ⃗ β ⋆ ) = N i=1 µi ( β ⋆ i ) ≤ N i=1 µi ( ⌊β ⋆ i ⌋ Ψ ) + L|β ⋆ i − ⌊β ⋆ i ⌋ Ψ | ≤ N i=1 µi ( ⌊β ⋆ i ⌋ Ψ ) + L∆ = N i=1 µi ( ⌊β ⋆ i ⌋ Ψ ) +<br>2: <mark>Discretization</mark> is an important pre-processing step in data analysis in which the problem of optimal discretization with a minimum number of splits is proved to be NP-Hard [6,35]. We here adopt the unsupervised Equal-frequency discretizer, which splits a continuous attribute into buckets with the same number of instances 4 .<br>",
    "Arabic": "التقسيم القطعي",
    "Chinese": "离散化",
    "French": "discrétisation",
    "Japanese": "離散化",
    "Russian": "дискретизация"
  },
  {
    "English": "discriminant analysis",
    "context": "1: (1997) wrote a position paper on the subject of authorship, whereas Krsul (1994) conducted an empirical study by gathering code from programmers of varying skill, extracting software metrics, and determining authorship using <mark>discriminant analysis</mark>.<br>",
    "Arabic": "تحليل التمييز",
    "Chinese": "判别分析",
    "French": "analyse discriminante",
    "Japanese": "識別分析 (shikibetsu bunseki)",
    "Russian": "дискриминантный анализ"
  },
  {
    "English": "discriminant function",
    "context": "1: Given a <mark>discriminant function</mark> F (X, R; w) with parameters w, the ranking of an input set of samples X is predicted by maximizing the score, that is, by solving the following optimization problem: \n R(w) = argmax R F (X, R; w). (2 \n ) \n<br>2: In the case of partially labeled data, the <mark>discriminant function</mark> F (x, y, w) may take various forms such as F (x, y, w) = max z g(x, y, z, w) where g(x, y, z, w) stands for an elementary <mark>discriminant function</mark>.<br>",
    "Arabic": "وظيفة تمييزية",
    "Chinese": "判别函数",
    "French": "fonction discriminante",
    "Japanese": "判別関数",
    "Russian": "функция дискриминанта"
  },
  {
    "English": "discriminative",
    "context": "1: The proposals are based on both bottom-up (<mark>discriminative</mark>) and top-down (generative) processes, see Section 2.4. The bottom-up processes compute <mark>discriminative</mark> probabilities q(w j Tst j (I)), j = 1, 2, 3, 4 from the input image I based on feature tests Tst j (I).<br>2: Unfortunately, data are only likely to be observed in areas of notable density, so we must have some way to \"peg down\" the functions in the low-density areas. The explicitly-modeled latent rejections, although a part of the generative model, fulfill this need in a way that would be difficult to motivate in a purely <mark>discriminative</mark> setting.<br>",
    "Arabic": "تميّزي",
    "Chinese": "判别",
    "French": "discriminatif",
    "Japanese": "判別",
    "Russian": "дискриминативный"
  },
  {
    "English": "discriminative approach",
    "context": "1: Learning-based approaches have been restricted to gener-atively trained models [25], but have found limited adoption due to computational challenges in inference. This is in contrast to image denoising, where <mark>discriminative approaches</mark> have been used extensively [2,4,14,27], and are often characterized by state-of-the-art restoration performance combined with low computational effort.<br>2: The idea of using expected values is common in generative models for prediction, but has seen much fewer applications in <mark>discriminative approaches</mark>. One exception is the Minimax Probabilistic Machine (MPM) (Lanckriet et al., 2003) which solves a robust classification problem for the case of first and second order moments.<br>",
    "Arabic": "النهج التمييزي",
    "Chinese": "判别式方法",
    "French": "approche discriminative",
    "Japanese": "識別的アプローチ",
    "Russian": "дискриминативный подход"
  },
  {
    "English": "discriminative feature",
    "context": "1: The benefits of deep supervision have previously been shown in deeply-supervised nets (DSN; [20]), which have classifiers attached to every hidden layer, enforcing the intermediate layers to learn <mark>discriminative features</mark>.<br>2: Our strategy is to compute generators that approximately vanish for a given data X, and are normalized on another data Y that is designed to be similar to X but at the same time belongs to a different class, thereby focusing the generator computation on the <mark>discriminative features</mark>.<br>",
    "Arabic": "الميزة التمييزية",
    "Chinese": "判别特征",
    "French": "caractéristique discriminante",
    "Japanese": "識別的な特徴",
    "Russian": "дискриминативная особенность"
  },
  {
    "English": "discriminative method",
    "context": "1: The efficiency of the parsing algorithm is important in applying the parsing model to test sentences, and also when training the model using <mark>discriminative methods</mark>.<br>2: 2001 ; Murphy et al. , 2003 ) using , broadly speaking , <mark>discriminative methods</mark> based on local bottom-up tests . But combining different visual modules requires a common framework which ensures consistency. Despite the effectiveness of <mark>discriminative methods</mark> for computing scene components, such as object labels and categories, they can also generate redundant and conflicting results.<br>",
    "Arabic": "الطريقة التمييزية",
    "Chinese": "判别方法",
    "French": "méthode discriminative",
    "Japanese": "識別手法",
    "Russian": "дискриминативный метод"
  },
  {
    "English": "discriminative model",
    "context": "1: We conducted an ablation study to measure the contribution of specific feature sets. Table 4 presents the ablation configurations and results. For each configuration, we retrained and retested the <mark>discriminative model</mark> using the features described.<br>2: We will use \"<mark>discriminative model</mark>\" in its traditional sense of categorization. 4. The optimal approximation occurs when g(w, | Tst(I)) equals the probability p(w, | Tst(I)) induced by P(I | W )P(W ). 5.<br>",
    "Arabic": "نموذج تمييزي",
    "Chinese": "判别模型",
    "French": "modèle discriminatif",
    "Japanese": "識別モデル",
    "Russian": "дискриминативная модель"
  },
  {
    "English": "discriminative training",
    "context": "1: , and <mark>discriminative training</mark> [ 1 ] . Our model integrates all the properties in one coherent framework to perform two seemingly different tasks, human pose estimation and object detection, to the benefit of each other.<br>2: We have applied <mark>discriminative training</mark> of SPNs to image classification benchmarks. CIFAR-10 and STL-10 are standard datasets for deep networks and unsupervised feature learning. Both are 10-class small image datasets. We achieve the best results to date on both tasks. We follow the feature extraction pipeline of Coates et al.<br>",
    "Arabic": "تدريب تمييزي",
    "Chinese": "判别式训练",
    "French": "Entraînement discriminatif",
    "Japanese": "識別的学習",
    "Russian": "дискриминативное обучение"
  },
  {
    "English": "discriminator",
    "context": "1: For Raw, we found that LipschitzRNN was too slow to train on a single GPU (requiring a full day for 1 epoch of training alone). WaveGAN <mark>Discriminator</mark> [11] The WaveGAN-D in Table 5 is actually our improved version of the discriminator network from the recent WaveGAN model for speech [11].<br>2: We compare OpenGAN to numerous prior art that use GANs for open-set discrimination. <mark>Discriminator</mark> vs. Generator. GANs mostly aim at generating realistic images [2,9]. As a result, prior work in open-set recognition has focused on using GANs to generate realistic open-training images [21,34,42].<br>",
    "Arabic": "مميز",
    "Chinese": "判别器",
    "French": "discriminateur",
    "Japanese": "識別器",
    "Russian": "дискриминатор"
  },
  {
    "English": "discriminator network",
    "context": "1: This results in an increased gaze estimation error of 12.2 degrees without the history, in comparison to 7.8 degrees with the history. Next, we compare local vs global adversarial loss during training. A global adversarial loss uses a fully connected layer in the <mark>discriminator network</mark>, classifying the whole image as real vs refined.<br>2: GANs are typically described as a two-player minimax game between a generator network N g and a <mark>discriminator network</mark> N d ; we denote by F d the class of functions that can be implemented by N d and by F g the class of distributions that can be implemented by N g .<br>",
    "Arabic": "شبكة التمييز",
    "Chinese": "判别器网络",
    "French": "réseau discriminateur",
    "Japanese": "識別器ネットワーク",
    "Russian": "сеть дискриминатора"
  },
  {
    "English": "disentangle",
    "context": "1: As evidenced by our experiments, our model is able to <mark>disentangle</mark> individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.<br>2: 5: Representative generated images using our training on FFHQ. Note that these results only use semi-supervised dataset by CLIP. Our methods shows the ability to <mark>disentangle</mark> the attributes of interest and the remaining information. models to <mark>disentangle</mark>ment where the idea is immediately beneficial, and expect that numerous other applications will emerge in short order.<br>",
    "Arabic": "تفكيك",
    "Chinese": "解缠",
    "French": "démêler",
    "Japanese": "属性を解きほぐす (zokuhogu)",
    "Russian": "развязать"
  },
  {
    "English": "disentangled representation",
    "context": "1: This procedure introduces additional constraints on the capacity of the latent bottleneck, encouraging the encoder to learn a <mark>disentangled representation</mark> for the data. Burgess et al. ( 2017) argue that when the bottleneck has limited capacity, the network will be forced to specialize on the factor of variation that most contributes to a small reconstruction error.<br>2: A one-hot representation is the best, and it is a <mark>disentangled representation</mark> for each category. Proof.<br>",
    "Arabic": "تمثيل متباين",
    "Chinese": "解缠表示",
    "French": "représentation désenchevêtrée",
    "Japanese": "分離表現",
    "Russian": "разделенное представление"
  },
  {
    "English": "disentanglement",
    "context": "1: Eastwood & Williams (2018) consider three properties of representations, i.e., <mark>Disentanglement</mark>, Completeness and Informativeness. First, Eastwood & Williams (2018) compute the importance of each dimension of the learned representation for predicting a factor of variation.<br>2: <mark>Disentanglement</mark> is then measured as a particular structural property of these relations (Higgins et al., 2017a;Kim & Mnih, 2018;Eastwood & Williams, 2018;Kumar et al., 2017;Chen et al., 2018;Ridgeway & Mozer, 2018).<br>",
    "Arabic": "فك التشابك",
    "Chinese": "解缠",
    "French": "démêlage",
    "Japanese": "\"もつれを解く\"",
    "Russian": "распутывание"
  },
  {
    "English": "disparity estimation",
    "context": "1: SegStereo (Yang et al. 2018) enables joint learning for segmentation and disparity esitimation simultaneously and (Cheng et al. 2017) utilize semantic clues to guide the training of optical flow estimation. These methods rely on annotated labels for segmentation in specific scenes like autonomous driving, whereas we differently concentrate on excavating semantics from dynamic scenarios.<br>2: Instances named pano and family come from the photomontage dataset [41]. These problems have more complicated pairwise potentials than the <mark>disparity estimation</mark> problems, but less labels. For both datasets we found significantly more persistent variables than MQPBO, in particular, we were able to label more than a third of the variables in pano.<br>",
    "Arabic": "تقدير التفاوتات",
    "Chinese": "视差估计",
    "French": "estimation de disparité",
    "Japanese": "視差推定",
    "Russian": "\"оценка пространственного различия\""
  },
  {
    "English": "disparity map",
    "context": "1: i ≈ T D p direct image + T NE p non-epipolar indirect image . (3) \n The set of direct elements therefore represents the scene's instantaneous <mark>disparity map</mark>. Conventional stereo algorithms attempt to localize this set while assuming that the transport matrix is zero everywhere else-both inside and outside its epipolar blocks.<br>2: In this implementation, demonstrated in figure 3, the first stage of proposal generation involves a local window matching process [27] to generate an approximate (very noisy) <mark>disparity map</mark>.<br>",
    "Arabic": "\"خريطة التفاوت\"",
    "Chinese": "视差图",
    "French": "carte de disparité",
    "Japanese": "視差マップ",
    "Russian": "карта диспаратности"
  },
  {
    "English": "distance function",
    "context": "1: w ( a , b ) = 0 if ( a , b ) / ∈ E , and d ( • , • ) is a <mark>distance function</mark> on the labels . As will be seen later, this formulation of the pairwise potentials would allow us to concisely describe our results.<br>2: First, a metric learning algorithm should be sufficiently flexible to support the variety of constraints realized across different learning paradigms. Second, the algorithm must be able to learn a <mark>distance function</mark> that generalizes well to unseen test data. Finally, the algorithm should be fast and scalable.<br>",
    "Arabic": "دالة المسافة",
    "Chinese": "距离函数",
    "French": "fonction de distance",
    "Japanese": "距離関数",
    "Russian": "функция расстояния"
  },
  {
    "English": "distance matrix",
    "context": "1: To do so, the authors calculate a U-centered matrixÃ from the <mark>distance matrix</mark> (a k,l ) so that the inner product of the U-centered matrices will be the distance covariance. Definition 2.<br>2: Right: the <mark>distance matrix</mark> constructed by Algorithm 3 for 12 \"jogging\" and 12 \"walking\" trajectories.<br>",
    "Arabic": "مصفوفة المسافة",
    "Chinese": "距离矩阵",
    "French": "matrice de distances",
    "Japanese": "距離行列",
    "Russian": "матрица расстояний"
  },
  {
    "English": "distance measure",
    "context": "1: suitable for the few-shot setting . Precisely, the learning objective is defined as: \n L = D(a, p) − D(a, n) + γ, \n where D represents a <mark>distance measure</mark> that computes the distance between the input encodings. γ represents the margin between the positive and negative samples.<br>",
    "Arabic": "مقياس المسافة",
    "Chinese": "距离度量",
    "French": "mesure de distance",
    "Japanese": "距離尺度",
    "Russian": "мера расстояния"
  },
  {
    "English": "distance metric",
    "context": "1: Recall here that the events in mined frequent episodes correspond to transitions from one symbol to another. Our hypothesis here is that if motif occurrences are matched at transitions under an inter-transition gap constraint, then the corresponding time series subsequences will match under a suitable <mark>distance metric</mark>. In addition the episode mining framework allows for robustness to noise and scaling.<br>2: This is problematic for conventional kernel regression, as the quality of its predictions is wholly reliant on the appropriateness of the given <mark>distance metric</mark>.<br>",
    "Arabic": "مقياس المسافة",
    "Chinese": "距离度量",
    "French": "métrique de distance",
    "Japanese": "距離尺度",
    "Russian": "метрика расстояния"
  },
  {
    "English": "distance transform",
    "context": "1: It can be formulated using the <mark>distance transform</mark> representation [12]  \n V c ,i = k∈Nvn(i) w i,k (R sk,k (θ, α)Y i +t sk,k (θ, α)), \n<br>2: The silhouette loss ensures that the boundary vertices project onto the zero-set of the <mark>distance transform</mark>, i.e., the foreground silhouette. Sparse Keypoint Graph Loss. Only using the silhouette loss can lead to wrong mesh-to-image assignments, especially for highly articulated motions.<br>",
    "Arabic": "تحويل المسافة",
    "Chinese": "距离变换",
    "French": "transformée de distance",
    "Japanese": "距離変換",
    "Russian": "преобразование расстояния"
  },
  {
    "English": "distant supervision",
    "context": "1: For example, in DocRED dataset, most of the capital relation labeled by <mark>distant supervision</mark> are false positive. However, if the phrase 'is the capital city of' appears in the text, the label is likely to be a true label.<br>2: We employ two separate prediction networks HA-Net and DS-Net to predict the labels by human annotation and <mark>distant supervision</mark>, respectively, to prevent the degradation of accuracy by the incorrect labeling of <mark>distant supervision</mark>. Furthermore, we propose an additional loss term called disagreement penalty to enable HA-Net to learn from distantly supervised labels.<br>",
    "Arabic": "الإشراف البعيد",
    "Chinese": "远程监督",
    "French": "supervision à distance",
    "Japanese": "遠隔監視",
    "Russian": "дистанционное наблюдение"
  },
  {
    "English": "distillation",
    "context": "1: To measure only the effect of varying the training set while ignoring unlabeled examples, we do not use <mark>distillation</mark>. Table 6 shows that for all tasks, changing the set of training examples can result in large performance differences for PET.<br>2: For <mark>distillation</mark>, given an unlabeled example x we set s p (y | x) = 1 if the model's output for x was mapped to y and s p (y | x) = 0 otherwise.<br>",
    "Arabic": "تقطير",
    "Chinese": "蒸馏",
    "French": "distillation",
    "Japanese": "蒸留",
    "Russian": "дистилляция"
  },
  {
    "English": "distribute learning",
    "context": "1: Notably, <mark>distributed learning</mark> (e.g., [44,12,5,16,52]) and federated learning (e.g., [32,31,38]) consider learning from data that is spread across multiple sources or devices. Classically, both of these settings have focused on minimizing the training or testing error averaged over these devices.<br>",
    "Arabic": "التعلم الموزع",
    "Chinese": "分布式学习",
    "French": "apprentissage distribué",
    "Japanese": "分散学習",
    "Russian": "распределенное обучение"
  },
  {
    "English": "distribute learning system",
    "context": "1: High-performing RL agents tend to rely on <mark>distributed learning systems</mark> to improve data efficiency (Kapturowski et al., 2018;Espeholt et al., 2018). This presents serious challenges for meta-learning as the policy gradient becomes noisy and volatile due to off-policy estimation (Xu et al., 2018;Zahavy et al., 2020).<br>",
    "Arabic": "نظام تعلم موزع",
    "Chinese": "分布式学习系统",
    "French": "système d'apprentissage distribué",
    "Japanese": "分散学習システム",
    "Russian": "система распределенного обучения"
  },
  {
    "English": "distribute representation",
    "context": "1: With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015;Lample et al., 2016;Ma and Hovy, 2016). Words and characters are encoded in <mark>distributed representations</mark> (Mikolov et al., 2013) and sentence-level features are learned automatically during end-to-end training.<br>2: When frequency information or alias tables are unavailable, prior work has used measures of similarity of the mention string to entity names for candidate generation (Sil et al., 2012;Murty et al., 2018). For candidate ranking, recent work employed <mark>distributed representations</mark> of mentions in context and entity candidates and neural models to score their compatibility.<br>",
    "Arabic": "تمثيل موزع",
    "Chinese": "分布式表示",
    "French": "représentation distribuée",
    "Japanese": "分散表現",
    "Russian": "распределенное представление"
  },
  {
    "English": "distributed information retrieval",
    "context": "1: We demonstrate the usefulness of quality estimation for several applications, among them improvement of retrieval, detecting queries for which no relevant content exists in the document collection, and <mark>distributed information retrieval</mark>. Experiments on TREC data demonstrate the robustness and the effectiveness of our learning algorithms.<br>",
    "Arabic": "استرجاع المعلومات الموزعة",
    "Chinese": "分布式信息检索",
    "French": "recherche d'informations distribuée",
    "Japanese": "分散情報検索",
    "Russian": "распределенное извлечение информации"
  },
  {
    "English": "distribution shift",
    "context": "1: π ref = arg max π E x,yw∼D [log π(y w | x)] \n . This procedure helps mitigate the <mark>distribution shift</mark> between the true reference distribution which is unavailable, and π ref used by DPO. Further details related to the implementation and hyperparameters can be found in Appendix B.<br>2: While this creates a <mark>distribution shift</mark> on question formats, it is still better than putting an invalid question in the flow. C Issue with Context Independent Questions Figure 9 shows an example where extra information in context-independent questions confuses the model and leads to incorrect prediction.<br>",
    "Arabic": "انتقال التوزيع",
    "Chinese": "分布偏移",
    "French": "glissement de distribution",
    "Japanese": "分布シフト",
    "Russian": "сдвиг распределения"
  },
  {
    "English": "distribution vector",
    "context": "1: , p is the <mark>distribution vector</mark> of M and xi is the boolean random variable indicating the selection of item oi in pattern α k . Surprisingly, the calculation of an estimated support is only involved with d + 1 real values: the d-dimensional <mark>distribution vector</mark> of a profile and the number of transactions that support the profile.<br>2: Note, however, that the size of a vector is a better indicator of precision than the magnitude, since we are usually most interested in the number of pages with nonzero entries in the <mark>distribution vector</mark>.<br>",
    "Arabic": "متجه التوزيع",
    "Chinese": "分布向量",
    "French": "vecteur de distribution",
    "Japanese": "分布ベクトル",
    "Russian": "вектор распределения"
  },
  {
    "English": "distributional",
    "context": "1: In contrast, because our models are based on a <mark>distributional</mark> view of content, they will freely incorporate information from all three categories as long as such information is manifested as a recurrent pattern.<br>2: Expressing preferences To express either pointwise or <mark>distributional</mark> preferences, Distributions support the constrain() method, which given a list of features ϕ i (x) and their corresponding momentsμ i , returns a representation of the target distribution that respects the constraints while deviating minimally from the original model.<br>",
    "Arabic": "توزيعي",
    "Chinese": "分布的",
    "French": "distributionnelle",
    "Japanese": "分布的",
    "Russian": "распределительный"
  },
  {
    "English": "distributional feature",
    "context": "1: Adding the prototype list (see figure 2) without <mark>distributional features</mark> yielded a slightly improved accuracy of 53.7%. For this domain, we utilized a slightly different notion of distributional similarity: we are not interested in the syntactic behavior of a word type, but its topical content.<br>",
    "Arabic": "السمة التوزيعية",
    "Chinese": "分布式特征",
    "French": "caractéristique distributionnelle",
    "Japanese": "分布特徴",
    "Russian": "дистрибуционная черта"
  },
  {
    "English": "distributional hypothesis",
    "context": "1: For that purpose, most embedding models build upon co-occurrence statistics from large monolingual corpora, following the <mark>distributional hypothesis</mark> that similar words tend to occur in similar contexts (Harris, 1954).<br>2: Word Embeddings Word embeddings have become an important component in many NLP models and are widely used for a vast range of down-stream tasks. These models are based on the <mark>distributional hypothesis</mark> according to which words that occur in the same contexts tend to have similar meanings (Harris, 1954). Indeed , they aim to create word representations that are derived from their shared contexts , where the context of a word is essentially the words in its proximity ( be it according to linear order in the sentence or according to syntactic relations ) ( Mikolov et al. , 2013 ; Pennington et al. , 2014 ; Levy and Goldberg , 2014 )<br>",
    "Arabic": "- Term: \"فرضية التوزيع\"\n- Translated term: فرضية التوزيع",
    "Chinese": "分布假设",
    "French": "hypothèse distributionnelle",
    "Japanese": "分布仮説",
    "Russian": "гипотеза о распределении"
  },
  {
    "English": "distributional model",
    "context": "1: It is important to keep in mind that the correlation of our measure with the behavior-based methods only indicates that SDT-ρ can be trusted, to some extent, in evaluating these semantic tasks. It does not necessarily validate its ability to assess the entire semantic structure of a <mark>distributional model</mark>.<br>",
    "Arabic": "نموذج توزيعي",
    "Chinese": "分布模型",
    "French": "modèle distributionnel",
    "Japanese": "分布モデル",
    "Russian": "Дистрибуционная модель"
  },
  {
    "English": "distributional representation",
    "context": "1: There has been recent promising work in using <mark>distributional representation</mark> of words and neural networks for language modeling (Bengio et al., 2001) and parsing (Henderson, 2003). One great advantage of this approach is its ability to fight data sparseness. The model size grows only sub-linearly with the number of predicting features used.<br>",
    "Arabic": "التمثيل التوزيعي",
    "Chinese": "分布式表示",
    "French": "représentation distributionnelle",
    "Japanese": "分布表現",
    "Russian": "Распределенное представление"
  },
  {
    "English": "distributional semantic",
    "context": "1: Team SkoltechNLP(SI:25) (Dementieva et al., 2020) performed data augmentation based on <mark>distributional semantics</mark>.<br>2: (2011 pointed out that there can be distinctive functional roles of signs within the language system. He defined functional differences of linguistic elements in two (orthogonal) types which are widely studied with distributional relations in <mark>distributional semantics</mark> (DS) research today: syntagmatic and paradigmatic.<br>",
    "Arabic": "الدلالة التوزيعية",
    "Chinese": "分布语义",
    "French": "sémantique distributionnelle",
    "Japanese": "分布意味論",
    "Russian": "распределительная семантика"
  },
  {
    "English": "distributional semantic model",
    "context": "1: We validate it against two behaviorbased evaluations (Free association norms and the TOEFL synonym test) on semantic representations extracted from a Wikipedia corpus using one of the most commonly used <mark>distributional semantic models</mark> : the Latent Semantic Analysis (LSA, Landauer and Dumais (1997)). In this model, we construct a word-document matrix.<br>2: For instance, the property has teeth was mentioned only for 45 out of 67 potential concepts, having been left out for concepts such as CALF, 14 BUFFALO, KANGAROO, etc. So it could be the case that previous research has underestimated the extent to which property knowledge is encoded by PLMs and other <mark>distributional semantic models</mark> of language.<br>",
    "Arabic": "نموذج دلالي توزيعي",
    "Chinese": "分布语义模型",
    "French": "modèle sémantique distributionnel",
    "Japanese": "分布意味論モデル",
    "Russian": "дистрибутивная семантическая модель"
  },
  {
    "English": "distributional similarity",
    "context": "1: distsim(w i , k) denotes the <mark>distributional similarity</mark> features based on large word clusters. which has value > 0. δ is activated each time we see a matching pair of entities for the same word occurrence. The new P glo is modified as: \n<br>2: Several ways to compute semantically similar words have been suggested in the literature like Wordnet-based, <mark>distributional similarity</mark>, etc. (e.g., (Resnik, 1996;Dagan et al., 1999;Ritter et al., 2010)). For our proof of concept, we use a simple overlap metric with two important Wordnet classes -Person and Location.<br>",
    "Arabic": "التشابه التوزيعي",
    "Chinese": "分布相似性",
    "French": "similarité distributionnelle",
    "Japanese": "分布的類似性",
    "Russian": "сходство распределения"
  },
  {
    "English": "distributional word representation",
    "context": "1: Vision-Language Pre-training Distributional word representations can be acquired through language modeling, and developing language models from visual data has been extensively studied by the community (Chrupała et al., 2015;Lazaridou et al., 2015;Li et al., 2017;Surıs et al., 2020).<br>",
    "Arabic": "التمثيل التوزيعي للكلمات",
    "Chinese": "分布式词表示",
    "French": "représentation distributionnelle des mots",
    "Japanese": "分散単語表現",
    "Russian": "распределенное представление слов"
  },
  {
    "English": "distributionally robust optimization",
    "context": "1: As the general agnostic federated learning setting does not differ from group <mark>distributionally robust optimization</mark> in its formulation, we provide an identical treatment of both settings in Section 6.<br>2: There exists a class of difficult group <mark>distributionally robust optimization</mark> problems for which our stated sample complexity upper bounds are tight. This is because we can reduce any collaborative learning problem to multi-distribution learning with a single smooth convex loss, and equivalently, group DRO.<br>",
    "Arabic": "تحسين قوي توزيعيًا",
    "Chinese": "分布鲁棒优化",
    "French": "\"optimisation robuste sur le plan distributionnel\"",
    "Japanese": "分布的に堅牢な最適化",
    "Russian": "дистрибуционно-робастная оптимизация"
  },
  {
    "English": "divergence operator",
    "context": "1: We introduced Moser Flow, a generative model in the family of CNFs that represents the target density using the <mark>divergence operator</mark> applied to a vector valued neural network. One important future work direction, and a current limitation, is scaling of MF to higher dimensions.<br>2: The <mark>divergence operator</mark> is defined by the equality d(i w dV ) = div(w)dV , for a vector field w ∈ X(M). Therefore \n Then we need to show that v t ∈ M exists such that \n<br>",
    "Arabic": "عامل التباعد",
    "Chinese": "散度算子",
    "French": "opérateur de divergence",
    "Japanese": "発散演算子",
    "Russian": "оператор дивергенции"
  },
  {
    "English": "diversity score",
    "context": "1: We make sure not to use information that the baseline method of [9] is not using and use zero location attributes and appearance attributes that are randomly sam-  2. The <mark>diversity score</mark> of [27].<br>2: A higher accuracy means that the method creates more realistic, or at least identifiable, objects. We also report a <mark>diversity score</mark> [27], which is based on the perceptual similarity [10] between two images. This score is used to measure the distance between pairs of images that are generated given the same input.<br>",
    "Arabic": "درجة التنوع",
    "Chinese": "多样性得分",
    "French": "score de diversité",
    "Japanese": "多様性スコア",
    "Russian": "Балл разнообразия"
  },
  {
    "English": "do-calculus",
    "context": "1: (b) All non-causal paths between X and Y in G are blocked by Z and S. \n (c) Y is d-separated from S given X under the intervention do(x), i.e., (Y ? ? S | X) G X .<br>2: 5 and 6) are complete to decide whether an adjustment set Z is valid to identify the effect P (y|do(x)) from the inputs {P (v | S=1)} and {P (v | S=1), P (t); with T ◆ Z}, respectively.<br>",
    "Arabic": "\"القيام بحساب التفاضل والتكامل\"",
    "Chinese": "干预演算",
    "French": "calcul-do",
    "Japanese": "do-計算",
    "Russian": "исчисление do"
  },
  {
    "English": "document",
    "context": "1: where w(q, t) = ln(1 + n/ft), w(D, t) = 1 + ln fD,t, and fD,t and ft are the frequency of term t in <mark>document</mark> D and in the entire collection, respectively.<br>",
    "Arabic": "وثيقة",
    "Chinese": "文档",
    "French": "document",
    "Japanese": "文書",
    "Russian": "документ"
  },
  {
    "English": "document classification",
    "context": "1: Similarly, Bouckaert (2002) extracts the components of author affiliations from articles of a pharmaceutical journal. Confidence prediction itself is also an under-studied aspect of information extraction-although it has been investigated in <mark>document classification</mark> (Bennett 2000), speech recognition (Gunawardana, Hon, & Jiang 1998), and machine translation (Gandrabur & Foster 2003).<br>2: The applications within NLP has been comparatively limited, e.g., Shu et al. (2016Shu et al. ( , 2017b for opinion mining, Shu et al. (2017a) for <mark>document classification</mark>, and Lee (2017) for hybrid code networks (Williams et al., 2017).<br>",
    "Arabic": "تصنيف الوثائق",
    "Chinese": "文档分类",
    "French": "Classification de documents",
    "Japanese": "文書分類",
    "Russian": "классификация документов"
  },
  {
    "English": "document clustering",
    "context": "1: . In NLP, Moore and Quirk's (2008) random walks from previous local optima were faster than uniform sampling and also increased BLEU scores; Elsner and Schudy (2009) showed that local search can outperform greedy solutions for <mark>document clustering</mark> and chat disentanglement tasks; and Mei et al.<br>2: For example, <mark>document clustering</mark> partitions a collection of documents into groups, where a good label for each group may help the user understand why these documents are grouped together. Labeling a cluster of documents is also valuable for many other tasks, such as search result summarization and modelbased feedback [27].<br>",
    "Arabic": "تجميع الوثائق",
    "Chinese": "文档聚类",
    "French": "regroupement de documents",
    "Japanese": "文書クラスタリング",
    "Russian": "кластеризация документов"
  },
  {
    "English": "document corpus",
    "context": "1: Definition 14 (Search engine). A search engine is a 4-tuple D, Q, results(•), k , where: \n 1. D is the <mark>document corpus</mark> indexed. Documents are assumed to have been pre-processed (e.g., they may be truncated to some maximum size limit). 2.<br>",
    "Arabic": "مجموعة الوثائق",
    "Chinese": "文档语料库",
    "French": "corpus de documents",
    "Japanese": "文書コーパス",
    "Russian": "корпус документов"
  },
  {
    "English": "document retrieval",
    "context": "1: The deficiency of index-retrieve paradigm lies in that the two stages of <mark>document retrieval</mark> and re-ranking are optimized separately. Especially, the <mark>document retrieval</mark> procedure is often sub-optimal and hinders the performance of the entire system. Thus, there are some recent attempts to achieve endto-end retrieval as a one-stage solution.<br>2: In this section, we empirically verify the performance of NCI and the effectiveness of each component on the <mark>document retrieval</mark> task, which generates a ranking list of documents in response to a query.<br>",
    "Arabic": "استرجاع الوثائق",
    "Chinese": "文档检索",
    "French": "récupération de documents",
    "Japanese": "文書検索",
    "Russian": "извлечение документов"
  },
  {
    "English": "document summarization",
    "context": "1: Recently, lots of extractive <mark>document summarization</mark> methods have been proposed. Most of them involve assigning salient scores to sentences of the original document and composing the result summary of the top sentences with the highest scores.<br>2: 21 WikiNews 22 articles and editorannotations have been used for <mark>document summarization</mark> (Bravo-Marquez and Manriquez, 2012), timeline synthesis (Zhang and Wan, 2017; Minard et al., 2016), word-identification (Yimam et al., 2017 and entity salience . However, we are not aware of any work using WikiNews revision histories.<br>",
    "Arabic": "تلخيص المستندات",
    "Chinese": "文档摘要",
    "French": "résumé de document",
    "Japanese": "文書要約",
    "Russian": "суммирование документов"
  },
  {
    "English": "document vector",
    "context": "1: Let n i (d) be the number of times f i occurs in document d. Then, each document d is represented by the <mark>document vector</mark> \n d := (n 1 (d), n 2 (d), . . . , n m (d)).<br>",
    "Arabic": "متجه الوثيقة",
    "Chinese": "文档向量",
    "French": "vecteur de document",
    "Japanese": "文書ベクトル",
    "Russian": "вектор документа"
  },
  {
    "English": "document-level",
    "context": "1: In addition, we exploit additional networks to adaptively assess the labeling bias by considering contextual information. Our performance study on sentence-level and <mark>document-level</mark> REs confirms the effectiveness of the dual supervision framework.<br>2: Our contributions are the following: \n 1. We introduce NewsEdits, the first public academic corpus of news revision histories. 2. We develop a <mark>document-level</mark> view of structural edits and introduce a highly scalable sentence-matching algorithm to label sentences in our dataset as Addition, Deletion, Edit, Refactor.<br>",
    "Arabic": "على مستوى الوثيقة",
    "Chinese": "文档层面",
    "French": "niveau du document",
    "Japanese": "ドキュメントレベル",
    "Russian": "уровне документа"
  },
  {
    "English": "document-topic assignment",
    "context": "1: Our model is similar in spirit to topic models: for an input dataset, the output of the RMN is a set of relationship descriptors (topics) and-for each relationship in the dataset-a trajectory, or a sequence of probability distributions over these descriptors (<mark>document-topic assignments</mark>).<br>",
    "Arabic": "تعيين موضوع الوثيقة",
    "Chinese": "文档主题分配",
    "French": "attribution de sujets aux documents",
    "Japanese": "文書-トピックの割り当て",
    "Russian": "распределение документ-тема"
  },
  {
    "English": "domain",
    "context": "1: One of the motivations to perform unseen <mark>domain</mark> DST is because collecting a large-scale task-oriented dataset for a new <mark>domain</mark> is expensive and time-consuming , and there are a large amount of <mark>domain</mark>s in realistic scenarios.<br>2: Given a hypothesis space H and a <mark>domain</mark> D XY , OOD detection is learnable in the single-distribution space D D XY<br>",
    "Arabic": "مجال",
    "Chinese": "领域",
    "French": "domaine",
    "Japanese": "ドメイン",
    "Russian": "домен"
  },
  {
    "English": "domain adaptation",
    "context": "1: We introduce TA-DA, a Topic-Aware <mark>Domain Adaptation</mark> framework for keyphrase extraction that integrates Multi-Task Learning with Adversarial Training and <mark>Domain Adaptation</mark>. Our approach improves performance over baseline models by up to 5% in the exact match of the F1-score.<br>2: <mark>Domain Adaptation</mark> Capability. We investigate OPINE's capability in adapting and transferring knowledge across distinct conversational domains d. We exclude all utterances from each domain d while training OPINE, and directly test on utterances from d. The average difference in F1-score and semantic similarity metrics with and without using training data from the test domain d is ≤ 5%.<br>",
    "Arabic": "تكييف المجال",
    "Chinese": "领域适应",
    "French": "adaptation de domaine",
    "Japanese": "ドメイン適応",
    "Russian": "адаптация домена"
  },
  {
    "English": "domain element",
    "context": "1: Furthermore, when generating the model, we limited the number of <mark>domain elements</mark> to be less than 4 (|D| ≤ 4) and the number of predicates to be less than 8 (|P| ≤ 8).<br>",
    "Arabic": "عنصر مجال",
    "Chinese": "领域元素",
    "French": "élément de domaine",
    "Japanese": "ドメイン要素",
    "Russian": "элемент домена"
  },
  {
    "English": "domain gap",
    "context": "1: The deep neural networks can perform well when the models are trained with supervision and the data follow a consistent distribution. However, the performance can drop significantly when a <mark>domain gap</mark> exists between the training and test data, especially when the distribution of the test data changes over time unexpectedly.<br>2: 2022a) to set our four tasks, and we design a VLCS task to validate the anti-forgetting ability when deal with the real-world <mark>domain gap</mark>. For CIFAR10-to-CIFAR10C standard task, CIFAR100to-CIFAR100C task and ImagetNet-to-ImagetNet-C task, given the source model, we need to adapt to the fifteen target domains with different corruption types that arrive sequentially.<br>",
    "Arabic": "فجوة النطاق",
    "Chinese": "领域差距",
    "French": "écart de domaine",
    "Japanese": "ドメインギャップ",
    "Russian": "расхождение между областями"
  },
  {
    "English": "domain generalization",
    "context": "1: Wei et al. (2023) use competing objectives and mismatched generalization to deceive large language models such as OpenAI's GPT-4 and Anthropic's ClaudeV1.3. However, GPT-3.5 is more robust to <mark>domain generalization</mark> and spurious correlation than smaller supervised models (Si et al., 2023). Beyond testing specific models, Ribeiro et al.<br>2: Work on word sense disambiguation based on dictionary definitions of words is related as well (Chaplot and Salakhutdinov, 2018), but this task exhibits lower ambiguity and existing formulations have not focused on <mark>domain generalization</mark>.<br>",
    "Arabic": "التعميم على المجالات",
    "Chinese": "领域泛化",
    "French": "généralisation de domaine",
    "Japanese": "ドメイン汎化",
    "Russian": "обобщение на различные домены"
  },
  {
    "English": "domain knowledge",
    "context": "1: However, this complicates the training because these trees need to be obtained or induced. This is computationally demanding or at least requires structural preprocessing informed by <mark>domain knowledge</mark>.<br>2: Some methods [8,13,22,51,71] use <mark>domain knowledge</mark> such as template models to achieve high-quality results, but are restricted to specific categories [41,56]. More recently, many works propose to synthesize novel views of dynamic scenes from a single camera. Yoon et al.<br>",
    "Arabic": "المعرفة المجالية",
    "Chinese": "领域知识",
    "French": "connaissance du domaine",
    "Japanese": "ドメイン知識",
    "Russian": "предметные знания"
  },
  {
    "English": "domain mismatch",
    "context": "1: According to §1 and §A.1, the first challenge of audio modality-based robust AVSR is the limited indomain noisy audio-visual data, which leads to <mark>domain mismatch</mark> between training and testing (Meng et al., 2017;Long et al., 2017;Lin et al., 2021;Chen et al., 2020cChen et al., , 2022.<br>2: Domain mismatch -where the training distribution does not match the test distribution -can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010). We show that influence functions can identify the training examples most responsible for the errors, helping model developers identify <mark>domain mismatch</mark>.<br>",
    "Arabic": "عدم تطابق المجال",
    "Chinese": "领域不匹配",
    "French": "désaccord de domaine",
    "Japanese": "ドメインミスマッチ",
    "Russian": "доменное несоответствие"
  },
  {
    "English": "domain ontology",
    "context": "1: Over-dependence on <mark>domain ontology</mark> and lack of knowledge sharing across domains are two practical and yet less studied problems of dialogue state tracking. Existing approaches generally fall short in tracking unknown slot values during inference and often have difficulties in adapting to new domains.<br>2: Consider the scenario that l is a good label for θ according to some prior knowledge, such as a <mark>domain ontology</mark>, but does not appear in C. In this case, C is biased to be used to infer the semantics of l w.r.t. θ.<br>",
    "Arabic": "أونتولوجيا المجال",
    "Chinese": "领域本体论",
    "French": "ontologie de domaine",
    "Japanese": "ドメインオントロジー",
    "Russian": "доменная онтология"
  },
  {
    "English": "domain shift",
    "context": "1: We evaluate our proposed method on four benchmark continual test time adaptation datasets with corruption type <mark>domain shift</mark>, i.e., CIFAR10-to-CIFAR10C stand tasks, CIFAR10-to-CIFAR10C gradual tasks, CIFAR100-to-CIFAR100C tasks, ImageNet-to-ImageNet-C. CIFAR10-to-CIFAR10C standard task. Given source model trained on CIFAR10, we conduct TTA on CIFAR10C. There are fifteen corruption types that will sequentially come during the test time.<br>2: We reach the goal of mining the domain-agnostic knowledge by limiting parameters sensitive to domain changes. We will then introduce how we design this regularization term to make DAP learn domain-independent knowledge. Measure parameters' sensitivity toward <mark>domain shift</mark>. We first evaluate the sensitivity of parameters to different domains.<br>",
    "Arabic": "تحويل المجال",
    "Chinese": "领域偏移",
    "French": "décalage de domaine",
    "Japanese": "ドメインシフト",
    "Russian": "смещение домена"
  },
  {
    "English": "domain transfer",
    "context": "1: The <mark>domain transfer</mark> from both tasks (or a combination of them) gives the active annotation a head-start for initial sample selection.<br>2: When any of these transfer models is further fine-tuned on the dissonance dataset, we find an initial drop in performance. This is explained with the effect of the heterogeneous <mark>domain transfer</mark> and the small dataset in the iter 0 train set. As later shown in Table 4, the performance improves when more samples are collected in the AL iterations.<br>",
    "Arabic": "نقل المجال",
    "Chinese": "领域迁移",
    "French": "transfert de domaine",
    "Japanese": "ドメイン移管",
    "Russian": "перенос домена"
  },
  {
    "English": "domain-specific",
    "context": "1: Desiderata. Our analysis of Section 2 showed that there are many ways to tailor the training setup to a specific evaluation dataset: one might employ domain-optimized candidate lists, include domainspecific features, use an optimized entity vocabulary, or optimize the process of sampling Wikipedia for training data.<br>",
    "Arabic": "خاص بالمجال",
    "Chinese": "特定领域的",
    "French": "spécifique au domaine",
    "Japanese": "ドメイン固有の",
    "Russian": "домено-специфический"
  },
  {
    "English": "dot product",
    "context": "1: If our neural network has only one hidden layer and is self-normalized, the only remaining computation is 512 calls to tanh() and a single 513-dimensional <mark>dot product</mark> for the final output score. 6   Table 2 shows the speed of self-normalization and pre-computation for the NNJM.<br>2: Unlike the other attention heads which use a <mark>dot product</mark> to score key-query pairs, we score the compatibility between K parse and Q parse using a bi-affine operator U heads to obtain attention weights: \n A parse = softmax(Q parse U heads K T parse ) (4) \n<br>",
    "Arabic": "ضرب نقطي",
    "Chinese": "点积",
    "French": "produit scalaire",
    "Japanese": "内積",
    "Russian": "скалярное произведение"
  },
  {
    "English": "dot-product attention",
    "context": "1: Then, its variant (Luong, Pham, and Manning 2015) has proposed the widely used location, general, and <mark>dot-product attention</mark>. The popular selfattention based Transformer (Vaswani et al. 2017) has recently been proposed as new thinking of sequence modeling and has achieved great success, especially in the NLP field.<br>",
    "Arabic": "الاهتمام بالمنتج النقطي",
    "Chinese": "点积注意力",
    "French": "attention par produit scalaire",
    "Japanese": "ドット積注意",
    "Russian": "внимание на скалярное произведение"
  },
  {
    "English": "down-sample layer",
    "context": "1: ( 2) is not viable when the size of feature-maps changes. However, an essential part of convolutional networks is <mark>down-sampling layers</mark> that change the size of feature-maps. To facilitate down-sampling in our architecture we divide the network into multiple densely connected dense blocks; see Figure 2.<br>",
    "Arabic": "طبقة التخفيض في العينة",
    "Chinese": "下采样层",
    "French": "couche de sous-échantillonnage",
    "Japanese": "ダウンサンプリング層",
    "Russian": "слои уменьшения размера (down-sample layer)"
  },
  {
    "English": "down-sampling",
    "context": "1: For encoder and decoder CNN architectures, we follow the implementation provided in a public Pytorch implementation 5 by adding one more up-sampling and <mark>down-sampling</mark> layer to adjust our image size.<br>2: Based on previous work (Baroni et al., 2014;Mandera et al., in press) the following settings were used: a negative sampling value of 10, and a <mark>down-sampling</mark> rate of very frequent terms of 1e-5.<br>",
    "Arabic": "خفض العينات",
    "Chinese": "下采样",
    "French": "sous-échantillonnage",
    "Japanese": "ダウンサンプリング",
    "Russian": "Дауншемплинг"
  },
  {
    "English": "downsampling block",
    "context": "1: Backbone Both ResNet and MobileNetV2 are adopted from the original implementation with minor modifications. We change the first convolution layer to accept 6 channels for both the input and the background images. We follow DeepLabV3's approach and change the last <mark>downsampling block</mark> with dilated convolutions to maintain an output stride of 16.<br>",
    "Arabic": "كتلة الاختزال",
    "Chinese": "下采样模块",
    "French": "bloc de sous-échantillonnage",
    "Japanese": "ダウンサンプリングブロック",
    "Russian": "блок децимации"
  },
  {
    "English": "downsampling factor",
    "context": "1: We then add additional convolutional layers with skipconnections that fuse information from the previous layers and upscale the output by factor of 2 (<mark>downsampling factor</mark> of 8 wrt to the original size of the image crop, which is always scaled to 224 × 224).<br>2: Similarly to the standard VIN, 3 × 3 convolution kernels, 150 channels for each hidden layer H (for both the down-sampled image, and standard image), and 10 channels for the q layer in each VI block. Similarly to the VIN networks , the recurrence K was set relative to the problem size , taking into account the <mark>downsampling factor</mark> : K = 4 for 8 × 8 domains , K = 10 for 16 × 16 domains , and K = 16 for 28 × 28 domains ( in comparison , the respective K values for standard VINs were 10<br>",
    "Arabic": "معامل تصغير العينات",
    "Chinese": "下采样系数",
    "French": "facteur de sous-échantillonnage",
    "Japanese": "ダウンサンプリング係数",
    "Russian": "фактор снижения частоты дискретизации"
  },
  {
    "English": "downsampling layer",
    "context": "1: We introduce DiffStride the first <mark>downsampling layer</mark> with learnable strides. We show on audio and image classification that DiffStride can be used as a drop-in replacement to strided convolutions, removing the need for cross-validating strides. As we observe that our method discovers multiple equally-accurate stride configurations, we introduce a regularization term to favor the most computationally advantageous.<br>2: However, and similarly to spatial pooling, spectral pooling is differentiable with respect to its inputs but not with respect to its strides. Thus, one still needs to provide S as hyperparameters for each <mark>downsampling layer</mark>. In this case, the search space is even bigger than with spatial pooling since strides are not constrained to integer values anymore.<br>",
    "Arabic": "طبقة تصغير العينات",
    "Chinese": "下采样层",
    "French": "couche de sous-échantillonnage",
    "Japanese": "ダウンサンプリング層",
    "Russian": "слой понижающей дискретизации"
  },
  {
    "English": "downstream dataset",
    "context": "1: These abilities can be unlocked by finetuning a classifier on <mark>downstream datasets</mark> (Talmor et al., 2020) or using proper prompting strategies (e.g., chain of thought (CoT) prompting  and generated knowledge prompting ).<br>",
    "Arabic": "مجموعة البيانات النهائية",
    "Chinese": "下游数据集",
    "French": "ensemble de données en aval",
    "Japanese": "下流データセット",
    "Russian": "данные для последующего использования"
  },
  {
    "English": "downstream model",
    "context": "1: While online political engagement promotes democratic values and diversity of perspectives, these discussions also reflect and reinforce societal biases-stereotypical generalizations about people or social groups (Devine, 1989;Bargh, 1999;Blair, 2002). Such language constitutes a major portion of large language models' (LMs) pretraining data, propagating biases into <mark>downstream models</mark>. Hundreds of studies have highlighted ethical issues in NLP models ( Blodgett et al. , 2020a ; Field et al. , 2021 ; Kumar et al. , 2022 ) and designed synthetic datasets ( Nangia et al. , 2020 ; Nadeem et al. , 2021 ) or controlled experiments to measure how biases in language are encoded in learned representations ( Sun et al.<br>",
    "Arabic": "نموذج المصب",
    "Chinese": "下游模型",
    "French": "modèle aval",
    "Japanese": "ダウンストリームモデル",
    "Russian": "модель нижнего потока"
  },
  {
    "English": "downstream performance",
    "context": "1: We further study popular methods and introduce variations of them, allowing us to relate this theoretical result to current practices and show the influence (or lack thereof) of design choices on <mark>downstream performance</mark>.<br>",
    "Arabic": "الأداء اللاحق",
    "Chinese": "下游性能",
    "French": "performance en aval",
    "Japanese": "下流のパフォーマンス",
    "Russian": "производительность на последующих задачах"
  },
  {
    "English": "downstream task",
    "context": "1: In-context learning is an important ability of large language models, which means performing a <mark>downstream task</mark> conditioning on a few demonstrations. Although several previous works have studied the role of the demonstrations [111,119,202,191], we still lack sufficient understanding of how they affect the model robustness.<br>2: impact , and some studies have investigated the connection between training data and <mark>downstream task</mark> model behavior ( Gonen and Webster , 2020 ; Dodge et al. , 2021 ) . Our study adds to this by demonstrating the effects of political bias in training data on <mark>downstream task</mark>s, specifically in terms of fairness.<br>",
    "Arabic": "مهمة تالية",
    "Chinese": "下游任务",
    "French": "tâche en aval",
    "Japanese": "下流タスク",
    "Russian": "задача последующей обработки"
  },
  {
    "English": "dropout distribution",
    "context": "1: where {θ l } L l=1 are L samples from the <mark>Dropout distribution</mark> q ϕ (θ) (i.e. we apply Dropout L times independently during testing).<br>2: σ 2 ij = 1 N 2 Y 1 ,Y 2 ∈D ij Var θ∼q ϕ (θ)p (Y 1 ≻ Y 2 |θ) \n where q ϕ (θ) is the <mark>Dropout distribution</mark>. Using the upper confidence estimatesû ij , we now define the optimistic Copeland score for a system i aŝ \n<br>",
    "Arabic": "توزيع دروب-أوت",
    "Chinese": "丢弃分布",
    "French": "distribution de dropout",
    "Japanese": "ドロップアウト分布",
    "Russian": "распределение dropout"
  },
  {
    "English": "dropout layer",
    "context": "1: We use triplet hinge loss and set the margin α = 0.1. A <mark>dropout layer</mark> is built over the embedding layer and measurement probabilities with a dropout rate of 0.9. A grid search is conducted over the parameter pools to explore the best parameters. The parameters under exploration include { 0.01 , 0.05 , 0.1 } for the learning rate , { 1e − 5 , 1e − 6 , 1e − 7 , 1e − 8 } for the L2-normalization of complex word embeddings , { 8 , 16 , 32 } for batch size , and { 50 , 100 , 300 , 500 } for<br>2: The details of proposed Informer model is summarized in Table 7. For the ProbSparse self-attention mechanism, we let d=32, n=16 and add residual connections, a positionwise feed-forward network layer (inner-layer dimension is 2048) and a <mark>dropout layer</mark> (p = 0.1) likewise.<br>",
    "Arabic": "طبقة إسقاط",
    "Chinese": "丢弃层",
    "French": "couche de décrochage",
    "Japanese": "ドロップアウト層",
    "Russian": "слой отсева"
  },
  {
    "English": "dropout probability",
    "context": "1: We used a learning rate of 1e-6 as in Spangher et al. (2021a). Early in experimentation, we trained for 10 epochs, but did not observe any improvement past the 3rd epoch, so we limited training to 5 epochs. We used a <mark>dropout probability</mark> of .1, 0 warmup steps and 0 weight decay.<br>2: The transformer has 2 layers, a <mark>dropout probability</mark> of 10%, a hidden size of 512 and an intermediate size of 2048. We select the top-2000 most common words as vocabulary for all datasets.<br>",
    "Arabic": "احتمالية الإسقاط",
    "Chinese": "丢弃概率",
    "French": "probabilité d'abandon",
    "Japanese": "ドロップアウト確率",
    "Russian": "вероятность отсева"
  },
  {
    "English": "dropout rate",
    "context": "1: We used no gradient clipping and a weight decay of 0.1. Unlike [2] which specified different <mark>dropout rate</mark>s for different parameters, we used a constant <mark>dropout rate</mark> of 0.25 throughout the network, including before every linear layer and on the residual branches.<br>2: Label smoothing and <mark>dropout rate</mark> are both set to 0.1. We choose blank penalty τ by grid search within [0, 4.0] with step=0.5 on the dev set. The models are trained with FAIRSEQ . The best ten checkpoints are averaged for inference with greedy search (beam size=1).<br>",
    "Arabic": "معدل التسرب",
    "Chinese": "丢弃率",
    "French": "taux d'abandon",
    "Japanese": "ドロップアウト率",
    "Russian": "коэффициент отсева"
  },
  {
    "English": "dropout ratio",
    "context": "1: We set the scaling factor of the consistency-based regularization loss as α = 0.15 and the <mark>dropout ratio</mark> as 0.1. For inference, we apply the partial beam search algorithm to the trained seq2seq model. We set the length penalty and the beam size as 0.8 and 100, respectively.<br>2: The model is trained for 600k and 800k steps with a 60K-step warm-up stage for ZINC-Subset and ZINC-Full respectively. After the warm-up stage, the learning rate decays linearly to zero. The <mark>dropout ratio</mark> is selected from [0.0, 0.1]. The weight decay is selected from [0.0, 0.01].<br>",
    "Arabic": "نسبة الإسقاط",
    "Chinese": "失活比率",
    "French": "taux d'abandon",
    "Japanese": "ドロップアウト比率",
    "Russian": "коэффициент выпадения"
  },
  {
    "English": "dual decomposition",
    "context": "1: Table 1 shows results for previous work on the various data sets, and results for an arc-factored model with pure MST decoding with our features. (We use the acronym UAS (unlabeled attachment score) for dependency accuracy.) We also show results for the bigram-sibling and grandparent/sibling (G+S) models under <mark>dual decomposition</mark>.<br>2: Table 1, columns TimeS and TimeG, shows decoding times for the <mark>dual decomposition</mark> algorithms. Table 2 gives speed comparisons to Martins et al. (2009). Our method gives significant speed-ups over  the Martins et al.<br>",
    "Arabic": "التجزئة المزدوجة",
    "Chinese": "对偶分解",
    "French": "décomposition duale",
    "Japanese": "二重分解",
    "Russian": "двойное разложение"
  },
  {
    "English": "dual encoder model",
    "context": "1: In future work, we will extend our analysis to i) specific IR tasks, within the recent paradigm of the <mark>dual encoder model</mark> (Karpukhin et al., 2020), and ii) compositional tasks, trying a systematic replication of the practical success obtained by  through image-based heuristics. When looking at models like Query2Prod2Vec in the larger industry landscape , we hope our methodology can help the field broaden its horizons : while retail giants indubitably played a major role in moving eCommerce use cases to the center of NLP research , finding solutions that address a larger portion of the market is not just practically important , but also an exciting<br>",
    "Arabic": "نموذج المشفر المزدوج",
    "Chinese": "双编码器模型",
    "French": "modèle à double encodeur",
    "Japanese": "デュアルエンコーダーモデル",
    "Russian": "модель с двойным энкодером"
  },
  {
    "English": "dual norm",
    "context": "1: − and A + have a diameters of at most R in the <mark>dual norm</mark> ∥•∥ * , i.e. max p,p ′ ∈A− ∥p − p ′ ∥ * ≤ R and max q,q ′ ∈A+ ∥q − q ′ ∥ * ≤ R. \n Then Algorithm 1 returns an ε-min-max equilibrium with probability 1 − δ if \n<br>2: Let Θ ⊂ R d be a norm ball of radius r(Θ), Θ = {θ ∈ R d | θ ≤ r}, and let • * be the associated <mark>dual norm</mark>, assuming also that X ⊂ {x ∈ R d | x * ≤ r(X )}.<br>",
    "Arabic": "المعيار المزدوج",
    "Chinese": "对偶范数",
    "French": "norme duale",
    "Japanese": "双対ノルム",
    "Russian": "\"двойственная норма\""
  },
  {
    "English": "dual objective",
    "context": "1: The DRO model is trained using the <mark>dual objective</mark> with logistic loss, and η = 0.95, which was the optimal dual solution to α min = 0.2. The results do not qualitatively change for choices of α min < 0.5, and we show that we obtain control even for group sizes substantially smaller than 0.2 (Figure 6).<br>2: Termination is reached when the rescaled primal objective is within (1 + ) of the rescaled <mark>dual objective</mark> for error parameter . This process is shown to terminate in O(m log 1+<br>",
    "Arabic": "الهدف المزدوج",
    "Chinese": "双目标",
    "French": "objectif dual",
    "Japanese": "二重目的",
    "Russian": "двойная цель"
  },
  {
    "English": "dual optimization problem",
    "context": "1: The modified <mark>dual optimization problem</mark> has a familiar form. We now use the new notion of regret and take the expected value of the log partition function. Theorem 8. The dual conditional maximum entropy ICE optimization problem is \n<br>",
    "Arabic": "مشكلة التحسين الثنائية",
    "Chinese": "对偶优化问题",
    "French": "problème d'optimisation duale",
    "Japanese": "双対最適化問題",
    "Russian": "двойная задача оптимизации"
  },
  {
    "English": "dual parameter",
    "context": "1: Here we show that there exists an optimal f * which can be described using much fewer variables. Furthermore, this f * can be determined via the set of <mark>dual parameters</mark> ν defined earlier. Theorem 2.1. The DCC h problem can be expressed as: \n min ν ν • µ s.t.<br>",
    "Arabic": "المعلمة المزدوجة",
    "Chinese": "对偶参数",
    "French": "paramètres duaux",
    "Japanese": "二重パラメータ",
    "Russian": "двойственный параметр"
  },
  {
    "English": "dual problem",
    "context": "1: The problem in [26] is in fact a special case of Problem 2, using a modular function f . Often, however, the budget functions involve a cooperative cost, in which case f is submodular. Using Algorithm 2, however, we can easily solve this by iteratively solving the <mark>dual problem</mark>.<br>2: wherevj =ṽj − λ t 1 and q U•,j = U•,j − λ 2t 1. We solve (15) by deriving its <mark>dual problem</mark>. Let γ ≥ 0 be the Lagrangian multiplier dual variable of the first inequality constraint. Define the Lagrangian function of (15) as: \n<br>",
    "Arabic": "المشكلة المزدوجة",
    "Chinese": "对偶问题",
    "French": "problème dual",
    "Japanese": "双対問題",
    "Russian": "дуальная задача"
  },
  {
    "English": "dual program",
    "context": "1: This section describes the partial ranking formulation of multiclass SVMs (Crammer & Singer, 2001). The presentation first follows (Tsochantaridis et al., 2005) then introduces a new parametrization of the <mark>dual program</mark>.<br>2: In this section, we will derive and describe a procedure for optimizing the <mark>dual program</mark> for solving the MaxEnt ICE optimization problem. We will see that the dual multipliers can be interpreted as utility vectors and that optimization in the dual has computational advantages. We begin by presenting the <mark>dual program</mark>. Theorem 7.<br>",
    "Arabic": "البرنامج المزدوج",
    "Chinese": "对偶问题",
    "French": "programme dual",
    "Japanese": "デュアルプログラム",
    "Russian": "двойственная программа"
  },
  {
    "English": "dual solution",
    "context": "1: In the extreme case, called row-action methods [8], the active set consists of a single constraint. While algorithms in this family are fairly simple to implement and entertain general asymptotic convergence properties [ 8 ] , the time complexity of most of the algorithms in this family is typically super linear in the training set size m. Moreover , since decomposition methods find a feasible <mark>dual solution</mark> and their goal is to maximize the dual objective function , they often<br>2: We can also use a <mark>dual solution</mark> to recoverσ Γ . Lemma 3. Strong duality holds for the maximum entropy ICE optimization problem and given optimal dual weights θ * , the maximum entropy ICE joint- \n strategyσ Γ isσ Γ (a) ∝ exp   − f ∈Φ r Γ f (a|θ * f )   .<br>",
    "Arabic": "الحل الثنائي",
    "Chinese": "对偶解",
    "French": "solution duale",
    "Japanese": "双対解",
    "Russian": "двойственное решение"
  },
  {
    "English": "dual variable",
    "context": "1: in the variables K ∈ S n , λ, µ ∈ R n and ν ∈ R. This dual problem is a quadratic program in the variables λ and µ which correspond to the primal constraints 0 ≤ α ≤ C and ν which is the <mark>dual variable</mark> for the constraint α T y = 0.<br>2: Denoting by η the optimal <mark>dual variable</mark> (5), we see from the proposition that all examples suffering less than ηlevels of loss are completely ignored, and large losses above η are upweighted due to the squared term.<br>",
    "Arabic": "المتغير المزدوج",
    "Chinese": "对偶变量",
    "French": "variable duale",
    "Japanese": "双対変数",
    "Russian": "двойственная переменная"
  },
  {
    "English": "duality gap",
    "context": "1: While the <mark>duality gap</mark> can be calculated on this set, it can also be calculated for a subset of B, which more closely relates to the probability distributions of the predictors. Furthermore, the regret can still be meaningfully compared to the best probability distributions for the context predictors, rather than the optimal parameters β * t ∈ B.<br>2: where the equality follows from strong duality and we recall from weak duality that the <mark>duality gap</mark> upper bounds the optimization error as follow: \n Duality Gap z (β, θ) := P z (β) − D z (θ) ≥ P z (β) − P z (β(z)) .<br>",
    "Arabic": "\"فجوة الثنائية\"",
    "Chinese": "对偶间隙",
    "French": "écart de dualité",
    "Japanese": "双対ギャップ",
    "Russian": "разрыв двойственности"
  },
  {
    "English": "dynamic bayesian network",
    "context": "1: We consider a set of variables (distributed according to a known <mark>dynamic Bayesian network</mark>) that evolve over time based in part on employed actions, A 1:T , as shown in Figure 5. Those actions are made with only partial knowledge of the Bayes net variables, as relayed through observation variables O 1:T .<br>2: (2007) introduce a <mark>dynamic Bayesian network</mark> for articulatory (phonetic) feature recognition as a component of an ASR system. Siniscalchi et al. (2013) show that a multilayer perceptron can successfully classify phonological features and contribute to the accuracy of a downstream ASR system. Mohamed et al.<br>",
    "Arabic": "شبكة بيزية ديناميكية",
    "Chinese": "动态贝叶斯网络",
    "French": "réseau bayésien dynamique",
    "Japanese": "動的ベイジアンネットワーク",
    "Russian": "динамическая байесовская сеть"
  },
  {
    "English": "dynamic model",
    "context": "1: The sequential structure between models is again captured with a simple <mark>dynamic model</mark> α t | α t−1 ∼ N (α t−1 , δ 2 I) . (2) \n For simplicity, we do not model the dynamics of topic correlation, as was done for static models by Blei and Lafferty (2006).<br>2: that is, we express the likelihood of an observation H i,ti belonging to trajectory H t0:t = (A, D) t0:t by evaluating it under the trajectory's appearance and <mark>dynamic model</mark> at that time, weighted with a temporal discount.<br>",
    "Arabic": "النموذج الديناميكي",
    "Chinese": "动态模型",
    "French": "modèle dynamique",
    "Japanese": "動的モデル",
    "Russian": "динамическая модель"
  },
  {
    "English": "dynamic programming algorithm",
    "context": "1: In fact, if we find an ordering based on an approximation of σ and that ordering is close enough that it matches the optimal ordering, the <mark>dynamic programming algorithm</mark> will still choose the correct interval lengths.<br>2: The corresponding trajectory is calculated via a <mark>dynamic programming algorithm</mark> (Watson et al., 2021). Further, to correct the potential bias caused by the score-based model, we derive both lower and upper bounds of the optimal variance and clip its estimate for a better result.<br>",
    "Arabic": "خوارزمية البرمجة الديناميكية",
    "Chinese": "动态规划算法",
    "French": "algorithme de programmation dynamique",
    "Japanese": "動的プログラミングアルゴリズム",
    "Russian": "алгоритм динамического программирования"
  },
  {
    "English": "dynamic time warp",
    "context": "1: We align the EMG features of the instance pairs with <mark>dynamic time warping</mark> (Rabiner and Juang, 1993), then make refinements to the alignments using canonical correlation analysis (Hotelling, 1936) and audio feature outputs from a partially trained model.<br>2: We would like to detect the same pattern in the interferometry data of future runs, e.g., Figure 1(b). For comparison, we ran both the template-matching (Figure 2) and <mark>dynamic time warping</mark> (Figure 6) techniques.<br>",
    "Arabic": "تشويه الزمن الديناميكي",
    "Chinese": "动态时间规整",
    "French": "alignement temporel dynamique",
    "Japanese": "動的時間伸縮法",
    "Russian": "динамическое выравнивание времени"
  },
  {
    "English": "dynamical model",
    "context": "1: Note that in differential equation format the agent-based and population-based <mark>dynamical model</mark> can be expressed by n+2, respectively 3 differential equations as shown in Table 3.<br>",
    "Arabic": "نموذج ديناميكي",
    "Chinese": "动力学模型",
    "French": "modèle dynamique",
    "Japanese": "動力学モデル",
    "Russian": "динамическая модель"
  },
  {
    "English": "dynamical system",
    "context": "1: In general if we view the expected user counts λ (t) as a <mark>dynamical system</mark>, the long-term fairness properties for any fairness criteria are controlled by two factors -whether λ has a fair fixed point (defined as a population fraction where risk minimization maintains the same population fraction over time) and whether this fixed point is stable.<br>2: We have a <mark>dynamical system</mark> defined by: Here, G t is S × P , H t is S × S, and F t is S × P .<br>",
    "Arabic": "نظام ديناميكي",
    "Chinese": "动力系统",
    "French": "système dynamique",
    "Japanese": "\"動力学系\"",
    "Russian": "динамическая система"
  },
  {
    "English": "e-step",
    "context": "1: Because our variables are discrete and have relatively small cardinality, we can learn our model using the EM algorithm. EM iterates between using probabilistic inference to derive a soft completion of the hidden variables (<mark>E-step</mark>) and finding maximum-likelihood parameters relative to this soft completion (Mstep).<br>2: Exact <mark>E-step</mark> calculation is used for observations with N m ≤ 12, and importance sampling is used for longer paths. The longest observation in our data has N m = 19. The FM uses simple pairwise frequencies of co-occurrence to assign an order independently to each path observation.<br>",
    "Arabic": "الخطوة E",
    "Chinese": "期望步",
    "French": "E-étape",
    "Japanese": "Eステップ",
    "Russian": "E-шаг"
  },
  {
    "English": "early fusion",
    "context": "1: To add depth information, we train on a model upgraded to take four-channel RGB-D input (<mark>early fusion</mark>). This provides little benefit, perhaps due to the difficultly of propagating meaningful gradients all the way through the model. Following the success of Gupta et al.<br>",
    "Arabic": "الدمج المبكر",
    "Chinese": "早期融合",
    "French": "fusion précoce",
    "Japanese": "アーリーフュージョン (early fusion)",
    "Russian": "ранний фьюжн"
  },
  {
    "English": "early stop",
    "context": "1: Interestingly, we note that one could potentially avoid the conclusion of the law of robustness (that is, that large models are necessary for robustness), with <mark>early stopping</mark> methods that could stop the optimization once the noise level is reached.<br>2: How hard is it to find selective probes? We tried 6 methods for controlling probe complexity, and all worked except dropout and <mark>early stopping</mark>, though never for a broad range of hyperparameters. For each complexity control method except dropout and <mark>early stopping</mark>, we find hyperparameters that lead to high linguistic task accuracy and high selectivity.<br>",
    "Arabic": "توقف مبكر",
    "Chinese": "提前停止",
    "French": "arrêt précoce",
    "Japanese": "早期停止",
    "Russian": "ранняя остановка"
  },
  {
    "English": "earth-mover distance",
    "context": "1: <mark>earth-mover distance</mark>) [29]  \n W 1 (µ, µ) = sup ∫ (µ(λ) − µ(λ))f (λ)dλ : Lip(f ) ≤ 1 . (3 \n ) \n This notion is particularly useful when µ is integrated against in applications such as computing centrality measures.<br>",
    "Arabic": "مسافة نقل الأرض",
    "Chinese": "运输距离",
    "French": "distance de transport",
    "Japanese": "地球移動距離",
    "Russian": "расстояние переноса земли"
  },
  {
    "English": "edge detection",
    "context": "1: The sub-kernel for the birth and death of faces is very similar to the sub-kernel of birth and death of text. We use AdaBoost method discussed in Section 6.2 to detect candidate faces. Face boundaries are obtained directly from using <mark>edge detection</mark> to give candidate face boundaries. The proposal probabilities are computed similarly to those for sub-kernel I.<br>2: Below we discuss some possible alternatives in architecture design, and in particular, the role of deep supervision of HED for the <mark>edge detection</mark> task. FCN and skip-layer architecture The topology used in the FCN model differs from that in our HED model in several aspects.<br>",
    "Arabic": "الكشف عن الحواف",
    "Chinese": "边缘检测",
    "French": "détection de contours",
    "Japanese": "エッジ検出",
    "Russian": "Обнаружение краев"
  },
  {
    "English": "edge feature",
    "context": "1: object poses , and <mark>edge features</mark> can their relative orientation ; the node labels represent the human activity and object affordance . Label y t v is affected by both its node and its interactions with other nodes (edges), leading to an overall complex system.<br>2: Similarly, it can identify cut edges if for any {u, v} ∈ E G and {w, x} ∈ E H where {u, v} is a cut edge but {w, x} is not, their <mark>edge features</mark> are different, i.e.<br>",
    "Arabic": "ميزة الحافة",
    "Chinese": "边缘特征",
    "French": "caractéristique des arêtes",
    "Japanese": "エッジ特徴",
    "Russian": "граневой признак"
  },
  {
    "English": "edge label",
    "context": "1: For example, the feature weights η could ensure that the <mark>edge label</mark> Y ij = + is especially likely when nodes i and j have many mutual friends in G. However, these features cannot consider any <mark>edge label</mark>s besides y ij .<br>",
    "Arabic": "ملصق الحافة",
    "Chinese": "边标签",
    "French": "étiquette d'arête",
    "Japanese": "エッジラベル",
    "Russian": "граничная метка"
  },
  {
    "English": "edge prediction",
    "context": "1: This subsumes a wide range of vision tasks including semantic segmentation, depth estimation, surface normal prediction, <mark>edge prediction</mark>, to name a few, varying in structure of output space, e.g., dimensionality (C T ) and topology (discrete or continuous), as well as the required knowledge.<br>2: However, it can only deal with a single-type task (e.g., node classification) using a specific pretext (e.g., <mark>edge prediction</mark>), which is far from addressing the multi-task setting with different-level tasks.<br>",
    "Arabic": "التنبؤ بالحافة",
    "Chinese": "边缘预测",
    "French": "prédiction d'arête",
    "Japanese": "エッジ予測",
    "Russian": "предсказание ребра"
  },
  {
    "English": "edge set",
    "context": "1: To prove the theorem, we reduce the biclique detection (BD) problem to ICOA. Given a bipartite graph G = ( U ∪ V , E ) where U and V denote the vertex sets while E denotes the <mark>edge set</mark> containing the edges ( u , v ) ∈ E such that u ∈ U and v ∈ V ; an instance of BD asks whether there exists vertex subsets Clearly , the reduction can be done<br>2: Let G 1 = (V, E 1 ) and G 2 = (V, E 2 ) be a pair of graphs with n = 2m nodes where m ≥ 3 is an arbitrary integer. Denote V = [n] and define the <mark>edge set</mark>s as follows: \n E 1 = { { i , ( i mod n ) + 1 } : i ∈ [ n ] } ∪ { { m , 2m } } , E 2 = { { i , ( i mod m ) + 1 } : i ∈ [ m ] } ∪ { { i + m , ( i mod m<br>",
    "Arabic": "مجموعة الحواف",
    "Chinese": "边集",
    "French": "ensemble d'arêtes",
    "Japanese": "エッジセット",
    "Russian": "набор рёбер"
  },
  {
    "English": "edge weight",
    "context": "1: Two documents x, y are connected by an edge in G P if and only if they share a neighboring query q in the graph B P . The <mark>edge weight</mark> is the number of such shared neighbor queries, where each query is normalized by its cardinality.<br>",
    "Arabic": "وزن الحافة",
    "Chinese": "边权重",
    "French": "poids de l'arête",
    "Japanese": "エッジの重み",
    "Russian": "вес ребра"
  },
  {
    "English": "edit distance",
    "context": "1: We filter triplets where the <mark>edit distance</mark> between c 1 and c 2 is too low. This results in a set of 236,208 triplets (q, r, a), where we call the first concept the question concept and the second concept the answer concept.<br>2: First, most RTE problems depend on forms of inference, such as paraphrase, temporal reasoning, or relation extraction, which NatLog is not designed to address. Second, in most RTE problems, the <mark>edit distance</mark> between premise and hypothesis is relatively large.<br>",
    "Arabic": "مسافة التحرير",
    "Chinese": "编辑距离",
    "French": "distance d'édition",
    "Japanese": "編集距離",
    "Russian": "расстояние редактирования"
  },
  {
    "English": "effective receptive field",
    "context": "1: The <mark>effective receptive field</mark> at this level is typically ∼ 1/2 of the image's height, hence G N generates the general layout of the image and the objects' global structure. Each of the generators G n at finer scales (n < N ) adds details that were not generated by the previous scales.<br>",
    "Arabic": "مجال استقبال فعال",
    "Chinese": "有效感受野",
    "French": "Champ récepteur effectif",
    "Japanese": "有効受容野",
    "Russian": "эффективное рецептивное поле"
  },
  {
    "English": "ego-motion",
    "context": "1: However, their performances heavily rely on the tailored cost based on human experience and the distribution from where trajectories are sampled [47]. Contrary to these approaches, we leverage the <mark>ego-motion</mark> information without sophisticated cost design and present the first attempt that incorporates the tracking module along with two genres of prediction rep-resentations simultaneously in an end-to-end model.<br>2: To identify these areas one might use the vehicles' <mark>ego-motion</mark>, which is not available, however. Appearance modeling. We address the challenging lighting conditions using the census transform [28] over a 7×7 neighborhood to measure the data fidelity ρ, which has been shown to cope well with complex outdoor lighting [14].<br>",
    "Arabic": "حركة الذات",
    "Chinese": "自身运动",
    "French": "mouvement de l'ego",
    "Japanese": "自車運動",
    "Russian": "Эго-движение"
  },
  {
    "English": "eigen-decomposition",
    "context": "1: For V (w(t)) we have: \n dV dt = dV dw dw dt = −w (t)H(t)w(t)(48) \n Note that H(t) has <mark>eigen-decomposition</mark>: \n<br>2: However, these two are closely connected. By applying the <mark>eigen-decomposition</mark> on Wt−1, we have \n X T t Wt−1Xt = X T t (Xt−1, X ⊥ t−1 )Λt−1(Xt−1, X ⊥ t−1 ) T Xt \n<br>",
    "Arabic": "تحليل القيم الذاتية",
    "Chinese": "特征分解",
    "French": "décomposition par valeurs propres",
    "Japanese": "固有値分解",
    "Russian": "собственное разложение"
  },
  {
    "English": "eigenbasis",
    "context": "1: see [10,4]): first, set i = 1 and diagonalize the first matrix G i = G 1 (i.e., find an <mark>eigenbasis</mark>), and set Q to be the diagonalizing matrix (i.e., the matrix of eigenvectors). So, QG 1 Q −1 is diagonal.<br>2: (Note that if G 1 's has unique eigenvalues, then the <mark>eigenbasis</mark> is unique (up to permutation and nonzero scaling), and thus in this case G 1 uniquely determines the simultaneously diagonalizing matrix, up to arbitrary permutation and nonzero scaling of the rows.<br>",
    "Arabic": "أساس القيم الذاتية",
    "Chinese": "特征基底",
    "French": "base propre",
    "Japanese": "固有ベクトル基底",
    "Russian": "собственный базис"
  },
  {
    "English": "eigendecay",
    "context": "1: γ T is bounded by Theorem 4 in terms the <mark>eigendecay</mark> of the kernel matrix K D . If D is infinite or very large, we can use the operator spectrum of k(x, x ), which likewise decays rapidly.<br>",
    "Arabic": "تحلل القيمة الذاتية",
    "Chinese": "特征衰减",
    "French": "décroissance propre",
    "Japanese": "固有減衰",
    "Russian": "собственное затухание"
  },
  {
    "English": "eigenfunction",
    "context": "1: We compute the covariances for eigenvector and <mark>eigenfunction</mark> inducing features.<br>2: Given a triangular surface mesh M , we wish to calculate the k-th <mark>eigenfunction</mark> of the (discrete) Laplace-Beltrami operator over M . We will use the standard (cotangent) discretization of the Laplacian over meshes (Botsch et al., 2010).<br>",
    "Arabic": "دالة القيم الذاتية",
    "Chinese": "特征函数",
    "French": "fonction propre",
    "Japanese": "固有関数",
    "Russian": "собственная функция"
  },
  {
    "English": "eigenspace",
    "context": "1: H i := 1 |M subspace | m∈M subspace h m,i,1 h m,i,2 , G i := 1 |M subspace | m∈M subspace g m,i,1 g m,i,2 ,(11) \n and let V i ∈ R d×K (resp. U i ) be the top-K <mark>eigenspace</mark> of \n<br>2: U i ) be the top-K <mark>eigenspace</mark> of H i + H i (resp. G i + G i ). 8 Output: subspaces {V i , U i } 1≤i≤d . Rationale.<br>",
    "Arabic": "الفضاء المميز",
    "Chinese": "特征空间",
    "French": "espace propre",
    "Japanese": "固有空間",
    "Russian": "собственное пространство"
  },
  {
    "English": "eigenspectrum",
    "context": "1: In Figure 1 we see the classical Möbius ladder graph and the resulting visualizations from the two methods. The spectral embedding looks degenerate and does not resemble the Möbius band in any regard. The <mark>eigenspectrum</mark> indicates that the embedding is 6-dimensional when we expect to be able to embed this graph using fewer dimensions. Two spring embeddings are shown.<br>2: The <mark>eigenspectrum</mark> shown next to each embedding reveals that both spectral embedding and Laplacian eigenmaps find many possible dimensions for visualization, whereas SPE requires far fewer dimensions to accurately represent the structure of the data. Also, note that the embedding that uses the graph Laplacian is dominated by the degree distribution of the network.<br>",
    "Arabic": "طيف القيم الذاتية",
    "Chinese": "特征谱",
    "French": "spectre propre",
    "Japanese": "固有スペクトル",
    "Russian": "спектр собственных значений"
  },
  {
    "English": "eigenvalue",
    "context": "1: By the assumption that the matrices are in fact simultaneously diagonalizable, QG j Q −1 will be permuted block diagonal for all j = i as well: the size of each block corresponds to the multiplicity of the corresponding <mark>eigenvalue</mark> of G 1 .<br>2: Under H 0 , p = q implies thatΣ q =Σ p (empirical estimate of Σ p ). Let λ j (A) denote the j th <mark>eigenvalue</mark> of the matrix A. Lemma 16 implies that A → λ j (A) is continuous on the space of real symmetric matrices, for all j.<br>",
    "Arabic": "قيمة ذاتية",
    "Chinese": "特征值",
    "French": "valeur propre",
    "Japanese": "固有値",
    "Russian": "собственное значение"
  },
  {
    "English": "eigenvalue decomposition",
    "context": "1: Because Q p is positive semi-de nite, its <mark>eigenvalue decomposition</mark> has the form Q p = T , where 2 2 is a real orthonormal matrix, and 2 2 = diag( max min ).<br>2: Approaches to autocalibration [6] can be broadly classified as direct and stratified. Direct methods seek to compute a metric reconstruction by estimating the absolute conic. This is encoded conveniently in the dual quadric formulation of autocalibration [12,28], whereby an <mark>eigenvalue decomposition</mark> of the estimated dual quadric yields the homography that relates the projective reconstruction to Euclidean.<br>",
    "Arabic": "تحليل القيم الذاتية",
    "Chinese": "特征值分解",
    "French": "décomposition en valeurs propres",
    "Japanese": "固有値分解",
    "Russian": "разложение собственных значений"
  },
  {
    "English": "eigenvector",
    "context": "1: Recall we have defined <mark>eigenvector</mark> inducing features by, \n u m = N i=1 w (m) i f (x i ). Then, \n cov ( u m , u k ) = E   N i=1 w ( m ) i f ( x i ) N j=1 w ( k ) j f ( x j )   = N i=1 w ( m ) i N j=1 w ( k ) j E [ f ( x i ) f ( x j<br>2: Then v M W W ht a is a positive <mark>eigenvector</mark> of K-2 W W Kwith unity eigenvalue, and application of the Frobenius-Perron theorem shows that Eq. 17 holds.<br>",
    "Arabic": "متجه ذاتي",
    "Chinese": "特征向量",
    "French": "vecteur propre",
    "Japanese": "固有ベクトル",
    "Russian": "собственный вектор"
  },
  {
    "English": "elastic net regularization",
    "context": "1: We use the logistic loss ℓ(θ; (x, y)) = log(1 + exp(−yθ ⊤ x)). We compare the performance of different constraint sets Θ by taking \n Θ = θ ∈ R d : a 1 θ 1 + a 2 θ 2 ≤ r , \n which is equivalent to <mark>elastic net regularization</mark> [ 52 ] , while varying a 1 , a 2 , and r. We experiment with ℓ 1 -constraints ( a 1 = 1 , a 2 = 0 ) with r ∈ { 50 , 100 , 500 , 1000 , 5000 } , ℓ 2 -constraints ( a 1 = 0 , a 2 = 1 ) with r ∈ { 5 , 10 , 50 , 100 , 500 } , elastic net ( a 1 = 1 , a 2 = 10 ) with r ∈ { 100 , 200 , 1000 , 2000 , 10000 } , our robust regularizer with ρ ∈ { 100 , 1000 , 10000 , 50000 , 100000 } and<br>",
    "Arabic": "التنظيم الشبكي المرن",
    "Chinese": "弹性网络正则化",
    "French": "régularisation élastique",
    "Japanese": "弾性ネット正則化",
    "Russian": "регуляризация эластичной сети"
  },
  {
    "English": "element-wise",
    "context": "1: A linear projection layer is applied to reduce all component embeddings to the same dimension, followed by an <mark>element-wise</mark> weighted sum. Rather than treating the weighted sum coefficients as hyper-parameters (Zhou et al., 2022;Liu et al., 2018), we let them update during training to converge to the optimal values given our objective function.<br>2: For this experiment, we provide two models trained with    [45]. In mean pooling and max pooling, the cross-clip pooling is performed over logits, followed by a softmax operator. In LogSumExp, logits from each clip are first fed through an <mark>element-wise</mark> exponential operator, followed by a cross-clip mean pooling.<br>",
    "Arabic": "عنصر الحكمة",
    "Chinese": "逐元素",
    "French": "élément par élément",
    "Japanese": "要素ごと",
    "Russian": "поэлементно"
  },
  {
    "English": "element-wise multiplication",
    "context": "1: [ t ] ) h [ t ] = r [ t ] c [ t ] + ( 1 − r [ t ] ) x [ t ] \n where is the <mark>element-wise multiplication</mark>, W, W and W are parameter matrices and v, v , b and b are parameter vectors to be learnt during training.<br>2: where is <mark>element-wise multiplication</mark>, A h ∈ R n×n denotes the h-th head's attention weight matrix (Equation (2)), and ∂F(αA) ∂A h computes the gradient of model F(•) along A h .<br>",
    "Arabic": "الضرب عنصر بعنصر",
    "Chinese": "逐元素相乘",
    "French": "multiplication élément par élément",
    "Japanese": "要素ごとの乗算",
    "Russian": "поэлементное умножение"
  },
  {
    "English": "element-wise product",
    "context": "1: As illustrated in Figure 3, we first transform the bias vectorr = r y − r x according to a predefined scale vector ω, that is ω ⊙r, where ⊙ is the <mark>element-wise product</mark> operation.<br>2: f (x) =   l∈L O l i∈A(l) R i,l   • 1 m (4) \n where denotes the <mark>element-wise product</mark>, \n R i,l = S(W i •x)1[l i] (1−S(W i •x))1[i l] ∈ R m,1 \n<br>",
    "Arabic": "الضرب العنصري",
    "Chinese": "逐元素乘积",
    "French": "produit élément par élément",
    "Japanese": "要素ごとの積",
    "Russian": "поэлементное произведение"
  },
  {
    "English": "elementary tree",
    "context": "1: The size is defined as the number of CFG rules that the <mark>elementary tree</mark> contains.<br>2: where x k is a refined root symbol of an <mark>elementary tree</mark> e, while x is a raw nonterminal symbol in the corpus and k = 0, 1, . . . is an index of the symbol subcategory. Suppose x is NP and its symbol subcategory is 0, then x k is NP 0 .<br>",
    "Arabic": "شجرة أولية",
    "Chinese": "基本树",
    "French": "arbre élémentaire",
    "Japanese": "基本木",
    "Russian": "элементарное дерево"
  },
  {
    "English": "eligibility trace",
    "context": "1: We have proposed a mechanism for efficient credit assignment, using the expectation of an <mark>eligibility trace</mark>. We have demonstrated this can sometimes speed up credit assignment greatly, and have analyzed concrete algorithms theoretically and empirically to increase understanding of the concept. Expected traces have several interpretations.<br>2: Such algorithms build an <mark>eligibility trace</mark> (Sutton 1988;Sutton and Barto 2018). An example is TD(λ): ∆w t ≡ αδ t e t , with e t = γ t λe t−1 + ∇ w v w (S t ) , where e t is an accumulating <mark>eligibility trace</mark>.<br>",
    "Arabic": "أثر الأهلية",
    "Chinese": "资格迹",
    "French": "trace d'éligibilité",
    "Japanese": "適格性トレース",
    "Russian": "след пригодности"
  },
  {
    "English": "embedded deformation graph",
    "context": "1: Hierarchical Deformation Tree: In original applications of the <mark>embedded deformation graph</mark> [25] approach to non-rigid tracking, E is defined as the k−nearest neighbours of each node or all nodes within a specified radius.<br>2: 6, the deformed and posed landmarks M are derived from the <mark>embedded deformation graph</mark>. To this end, we can deform and pose the canonical landmark positions by attaching them to its closest graph node g in canonical pose with weight w m,g = 1.0. Landmarks can then be deformed according to Eq.<br>",
    "Arabic": "جراف التشوه المُضمَّن",
    "Chinese": "嵌入式变形图",
    "French": "graphe de déformation intégré",
    "Japanese": "埋め込み変形グラフ",
    "Russian": "встроенный граф деформации"
  },
  {
    "English": "embedding dimension",
    "context": "1: We also perform a very small sweep over the <mark>embedding dimension</mark> and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al.<br>2: We empirically verify our conclusions for random and real data initializations in Figure 6. The images are synthesized from CIFAR-10 by DM using linear extractor of <mark>embedding dimension</mark> 2048, and each line contains images from the same class. On the right side, we plot the images synthesized with random initialization and real data initialization.<br>",
    "Arabic": "بُعد التضمين",
    "Chinese": "嵌入维度",
    "French": "dimension d'incorporation",
    "Japanese": "埋め込み次元",
    "Russian": "Размерность вложения"
  },
  {
    "English": "embedding dimensionality",
    "context": "1: We then train the model for 3 epochs on 100k ground truth pairs from either one or two languages as described next. We use 3 hidden layers (400 gated recurrent units each) in both the decoder and the encoder, and an <mark>embedding dimensionality</mark> of 130.<br>",
    "Arabic": "تضمين الأبعاد",
    "Chinese": "嵌入维度",
    "French": "dimensionnalité de plongement",
    "Japanese": "埋め込み次元",
    "Russian": "размерность вложения"
  },
  {
    "English": "embedding feature",
    "context": "1: • M pix , where M ∈ {CLS, OpenGAN}, corresponds to a model defined on raw pixels. • M f ea corresponds to a model defined on <mark>embedding features</mark> at the penultimate layer of underlying semantic segmentation network (i.e., HRNet as introduced below).<br>2: The pattern is similar to what we observed in the phoneme identification task: best accuracy is achieved using representation vectors from recurrent layers 1 and 2, and it drops as we move further up in the model. The accuracy is lowest when final <mark>embedding features</mark> are used for this task.<br>",
    "Arabic": "تضمين الميزة",
    "Chinese": "嵌入特征",
    "French": "vecteurs de représentation",
    "Japanese": "埋め込み特徴量",
    "Russian": "функция встраивания"
  },
  {
    "English": "embedding layer",
    "context": "1: For each token in the quantized network, we compute both (i) the token-level contrastive distillation loss where the positive tokens and negative tokens are selected from the full-precision teacher network; and (ii) the distillation loss on the logits. The <mark>embedding layer</mark> and all weights in the Transformer layers are quantized with the proposed module-dependent dynamic scaling.<br>2: Adversarial Training. We generate adversarial input examples that are very close to the original inputs and should yield the same labels, by adding small, continuous, worst case perturbations or noise to the <mark>embedding layer</mark> in the direction that significantly increases the model's loss function.<br>",
    "Arabic": "طبقة التضمين",
    "Chinese": "嵌入层",
    "French": "couche d'embeddings",
    "Japanese": "埋め込み層",
    "Russian": "слой встраивания"
  },
  {
    "English": "embedding matrix",
    "context": "1: With h t and W t -the <mark>embedding matrix</mark> of admissible tokens (determined by dynamic program induction)-we obtain the probability of generating a token from the admissible tokens: \n p(y t = s ti |q, y <t ) = [Softmax(W t h t )] i (3) \n<br>2: The input is the concatenation of h and t. We acquire the <mark>embedding matrix</mark> of h and t by: \n [H; T ] = RoBERTa([h; t])(1) \n where H ∈ R |N |×D and T ∈ R |N |×D .<br>",
    "Arabic": "مصفوفة التضمين",
    "Chinese": "嵌入矩阵",
    "French": "matrice d'incorporation",
    "Japanese": "埋め込みマトリックス",
    "Russian": "матрица вложения"
  },
  {
    "English": "embedding model",
    "context": "1: As the language modelP (•), we use GPT-2, a large-scale transformer [56] pretrained on the web text dataset (see [45]), that is representative of state-of-the-art autoregressive language models. As the <mark>embedding model</mark> M (•) we use GPT-2 Large, and compare others in §4.2. Decoding Algorithms. We consider three common decoding algorithms : ancestral sampling which samples directly from the language model 's per-step distributions , x t ∼P ( x t | x 1 : t ) , greedy decoding which selects the most likely next token , x t = arg max x∈VP ( x | x 1 : t ) , as well as nucleus sampling [<br>2: Nevertheless, the above argument does not formalize what \"similar words\" means, and it is not entirely clear what kind of relationships an <mark>embedding model</mark> should capture in practice.<br>",
    "Arabic": "نموذج تضمين",
    "Chinese": "嵌入模型",
    "French": "modèle d'embeddings",
    "Japanese": "埋め込みモデル",
    "Russian": "модель вложения"
  },
  {
    "English": "embedding parameter",
    "context": "1: In Table 15 we list the model architectures we use. They are an extended version of the architectures from [42]. We calculate model parameters following [78], which includes <mark>embedding parameters</mark>: \n P = 12lh 2 1 + 13 12h + V + s 12lh (23 \n ) \n<br>",
    "Arabic": "معلمات التضمين",
    "Chinese": "嵌入参数",
    "French": "paramètre d'incorporation",
    "Japanese": "埋め込みパラメータ",
    "Russian": "параметр встраивания"
  },
  {
    "English": "embedding size",
    "context": "1: Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? section 4 (we report the size of the model in terms of layers, <mark>embedding size</mark>, etc.) C2.<br>2: We use 4 self-attention heads. The <mark>embedding size</mark> is d model = 256 and the hidden size of the feed-forward layer is d FF = 1024. In the preliminary experiments, we found that using layer normalization before self-attention and the feed-forward layer performed slightly better than the original model.<br>",
    "Arabic": "حجم التضمين",
    "Chinese": "嵌入大小",
    "French": "taille d'embeddings",
    "Japanese": "埋め込みサイズ",
    "Russian": "размер вложения"
  },
  {
    "English": "embedding space",
    "context": "1: Because some of the representations from the two languages are close to each other in the <mark>embedding space</mark>, the model may confuse them, similar to the non-selective lexical access in bilingual speakers.<br>2: We focus on analyzing the first term (a), which is often dubbed as the alignment term [11]. The main functionality of this term is to optimize the tightness of the clusters in the <mark>embedding space</mark>. In this work, we connect it with classical clustering algorithms.<br>",
    "Arabic": "فضاء التضمين",
    "Chinese": "嵌入空间",
    "French": "espace d'incorporation",
    "Japanese": "埋め込み空間",
    "Russian": "пространство вложения"
  },
  {
    "English": "embedding vector",
    "context": "1: Given a graph representation G, we encode visual and spatial cues of each object as H = {h i } = ψ(G), where h i is the <mark>embedding vector</mark> of the i-th object and ψ is the geometric transformer network [40].<br>2: t at time-step t. Here y t−1 is the predicted probability distribution over the label space L at time-step t − 1 . The function g takes y t−1 as input and produces the <mark>embedding vector</mark> which is then passed to the decoder. Finally, the masked softmax layer is used to output the probability distribution y t .<br>",
    "Arabic": "متجه التضمين",
    "Chinese": "嵌入向量",
    "French": "vecteur d'incorporation",
    "Japanese": "埋め込みベクトル",
    "Russian": "вектор вложения"
  },
  {
    "English": "embedding-base metric",
    "context": "1: We use the outcomes of this extrinsic task to construct a breakdown detection benchmark for the metrics. We use dialogue state tracking, semantic parsing, and extractive question answering as our extrinsic tasks. We evaluate nine metrics consisting of string overlap metrics, <mark>embedding-based metrics</mark>, and metrics trained using scores from human evaluation of MT.<br>",
    "Arabic": "مقياس قائم على التضمين",
    "Chinese": "嵌入基础度量",
    "French": "métrique basée sur les embeddings",
    "Japanese": "埋め込みベースメトリック",
    "Russian": "метрика на основе эмбеддингов"
  },
  {
    "English": "embodied agent",
    "context": "1: The proposed learning frameworks are modular and model-agnostic, which allow the components to be improved separately. We also believe that the idea of learning more fine-grained intrinsic rewards, in addition to the coarse external signals, is commonly applicable to various <mark>embodied agent</mark> tasks, and the idea SIL can be generally adopted to explore other unseen environments.<br>2: Vision-language navigation (VLN) is the task of navigating an <mark>embodied agent</mark> to carry out natural language instructions inside real 3D environments. In this paper, we study how to address three critical challenges for this task: the cross-modal grounding, the ill-posed feedback, and the generalization problems.<br>",
    "Arabic": "وكيل متجسد",
    "Chinese": "具身代理",
    "French": "agent incarné",
    "Japanese": "体現エージェント",
    "Russian": "воплощенный агент"
  },
  {
    "English": "emission probability",
    "context": "1: If the state was that of a listener, we generated a maximum likelihood action and the state was randomly transited based on the transition probability. If the state was that of a speaker, we randomly generated an action based on the <mark>emission probability</mark> and the state was transited using the maximum likelihood transition probability.<br>2: This system used the policy to generate sequences of dialogue-act tags by simulation; user observations were generated based on <mark>emission probability</mark>, and system actions were generated based on the policy. In this paper, the total number of observations and actions was 33 because we have 32 dialogueact tags (See Table 1) plus a \"skip\" tag.<br>",
    "Arabic": "احتمالية الانبعاث",
    "Chinese": "发射概率",
    "French": "probabilité d'émission",
    "Japanese": "発生確率",
    "Russian": "вероятность эмиссии"
  },
  {
    "English": "emotion classification",
    "context": "1: ) for topic classification , and EmoContext ( EmoC ) ( Chatterjee et al. , 2019 ) for <mark>emotion classification</mark> . Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set.<br>",
    "Arabic": "تصنيف المشاعر",
    "Chinese": "情感分类",
    "French": "classification des émotions",
    "Japanese": "感情分類",
    "Russian": "классификация эмоций"
  },
  {
    "English": "empirical Bayes",
    "context": "1: It is named the marginal likelihood, because it is a likelihood formed from marginalizing parameters w. It is also known as the Bayesian evidence. Maximizing the marginal likelihood is sometimes referred to as <mark>empirical Bayes</mark>, type-II maximum likelihood estimation, or maximizing the evidence. We can also decompose the marginal likelihood as \n<br>2: We believe that the consistently strong performance of our method across settings, combined with its computational speed, make it attractive for practitioners looking to apply large-scale multiple linear regression to real problems. Our method takes an <mark>empirical Bayes</mark> ( EB ) approach ( Robbins , 1964 ; Efron , 2019 ; Hartley and Rao , 1967 ; Carlin and Louis , 2000 ; Stephens , 2016 ; Casella , 2001 ) to multiple regression ; that is , it assigns a prior to the coefficients in the regression method , and this prior is learned from<br>",
    "Arabic": "بيز تجريبي",
    "Chinese": "经验贝叶斯",
    "French": "Bayes empirique",
    "Japanese": "経験的ベイズ",
    "Russian": "эмпирический Байес"
  },
  {
    "English": "empirical distribution",
    "context": "1: P = argmin Q∈P sup f ∈F E X∼Q [f (X)] − E X∼ Pn [f (X)] = argmin Q∈P d F Q, P n ,(1) \n i.e., an estimate which directly minimizes empirical IPM risk with respect to a (regularized) <mark>empirical distribution</mark> P n . While , in the original GAN model [ 21 ] , P n was the <mark>empirical distribution</mark> P n = 1 n n i=1 δ Xi of the data , Liang [ 28 ] showed that , under smoothness assumptions on the population distribution , performance is improved by replacing P n with a regularized version P n , equivalent to the instance noise<br>2: and independent from one triplet to another . In the following I a denotes the indicator function returning 1 when a property a is verified and 0 otherwise, W c is an <mark>empirical distribution</mark> over S × Y, and w c,xi,y is the weight associated to object x i and label y.<br>",
    "Arabic": "توزيع تجريبي",
    "Chinese": "经验分布",
    "French": "distribution empirique",
    "Japanese": "経験分布",
    "Russian": "эмпирическое распределение"
  },
  {
    "English": "empirical estimate",
    "context": "1: By [13, Lemmas 10 and 11], for symbols appearing t times, if ϕ t+1 ≥Ω(t), then the Good-Turing estimate is close to the underlying total probability mass, otherwise the <mark>empirical estimate</mark> is closer.<br>2: (2 θ − θ * (α/2) , 2 θ − θ * (1−α/2) ) \n , where θ * (1−α/2) denotes the 1 − α/2 percentile of the bootstrapped parameters θ * and θ is the <mark>empirical estimate</mark> of the parameter based on finite samples. 2. Percentile bootstrap.<br>",
    "Arabic": "تقدير تجريبي",
    "Chinese": "经验估算",
    "French": "estimation empirique",
    "Japanese": "経験的推定",
    "Russian": "эмпирическая оценка"
  },
  {
    "English": "empirical estimator",
    "context": "1: Recall that for a sequence x n , n x denotes the number of times a symbol x appears and ϕ t denotes the number of symbols appearing t times. For small values of n and k, the estimator proposed in [13] simplifies to a combination of Good-Turing and <mark>empirical estimator</mark>s.<br>2: Hence, for a symbol appearing t times, if ϕ t+1 ≥ t we use the Good-Turing estimator, otherwise we use the <mark>empirical estimator</mark>. If n x = t, \n q x = t N if t > ϕ t+1 , ϕt+1+1 ϕt • t+1 N else, \n where N is a normalization factor.<br>",
    "Arabic": "مقدر تجريبي",
    "Chinese": "经验估计量",
    "French": "estimateur empirique",
    "Japanese": "経験的推定子 (けいけんてきすいていし)",
    "Russian": "эмпирический оценщик"
  },
  {
    "English": "empirical frequency",
    "context": "1: The following theorem shows that minimizing the trigger regrets for each player i ∈ P and sequence σ ∈ Σ i allows to approach the set of EFCEs. Theorem 1. At all times T , the <mark>empirical frequency</mark> of playμ T (Equation 3) is an -EFCE, where \n := max i∈P max σ∈Σi R T σ T .<br>2: Good-Turing estimators are often used in conjunction with <mark>empirical frequency</mark>, where Good-Turing estimates low probabilities and <mark>empirical frequency</mark> estimates large probabilities. We first show that even this simple Good-Turing version, defined in Appendix C and denoted q , is uniformly optimal for all distributions.<br>",
    "Arabic": "التكرار التجريبي",
    "Chinese": "经验频率",
    "French": "fréquence empirique",
    "Japanese": "経験的頻度",
    "Russian": "эмпирическая частота"
  },
  {
    "English": "empirical loss",
    "context": "1: One way to learn z θ is by updating it toward the instantaneous trace e t , by minimizing an <mark>empirical loss</mark> L(e t , z θ (S t )). For instance, L could be a component-wise squared loss, optimized with stochastic gradient descent: \n<br>2: In the case of polynomial eigenvalues, if h minimizes the robust <mark>empirical loss</mark> sup P : \n D φ ( P || Pn ) ≤ρ/n E P [ ℓ ( h ( X ) , Y ) ] and ρ ≍ n 1− 2α 2α+1 , then E ℓ ( h ( X ) , Y ) ≤ 1 + Cn − α 2α+1 inf h∈B H E [ ℓ ( h ( X ) , Y ) ] + Cn<br>",
    "Arabic": "الخسارة التجريبية",
    "Chinese": "经验损失",
    "French": "perte empirique",
    "Japanese": "経験的損失",
    "Russian": "эмпирическая потеря"
  },
  {
    "English": "empirical mean",
    "context": "1: Another measure for quantifying the significance of the value X0 is captured by the Z score \n Z = |X0 − b X| b σ , where b X = E[X1, . . . , X k ] \n is the <mark>empirical mean</mark> of the set X and b σ 2 = Var[X1, . . .<br>2: Next, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a learned approximation z t (s) ≈ z(s) that relies solely on observed experience. Proposition 2. Let e t an instantaneous trace vector. Then let z t ( s ) be the <mark>empirical mean</mark> z t ( s ) = 1 nt ( s ) nt ( s ) i e t s i , where t s i -s denote past times when we have been in state s , that is S t s i = s , and n t ( s ) is<br>",
    "Arabic": "المتوسط التجريبي",
    "Chinese": "经验均值",
    "French": "moyenne empirique",
    "Japanese": "経験的平均",
    "Russian": "эмпирическое среднее"
  },
  {
    "English": "empirical measure",
    "context": "1: 6.1]. For x 1 , . . . , x n ∈ X , let \n F n,r := f − E[f ] ∈ F | E Pn [(f − E[f ]) 2 ] ≤ r , \n where P n is the <mark>empirical measure</mark> on x 1 , . . .<br>",
    "Arabic": "مقياس تجريبي",
    "Chinese": "经验测度",
    "French": "mesure empirique",
    "Japanese": "経験的尺度",
    "Russian": "эмпирическая мера"
  },
  {
    "English": "empirical minimizer",
    "context": "1: The second result (16) (and inequality ( 18)) guarantees convergence of the <mark>empirical minimizer</mark> to a parameter with risk at most O(log n/n) larger than the best possible variance-corrected risk.<br>",
    "Arabic": "مُقَلِّل تجريبي",
    "Chinese": "经验最小化器",
    "French": "minimiseur empirique",
    "Japanese": "経験的最小化ツール",
    "Russian": "эмпирический минимизатор"
  },
  {
    "English": "empirical process theory",
    "context": "1: The proof of Theorem 1, which is given in Section B.1 of the supplementary appendix, uses standard methods of <mark>empirical process theory</mark>, but also employs a concentration result related to Talagrand's convex distance inequality to obtain the crucial dependence on S ∞ (X). At the end of Section B.1 we sketch applications of the proof method to other regularization schemes , such as the one presented in ( Kumar & Daumé III , 2012 ) , in which the Frobenius norm on the dictionary D is used in place of the ℓ 2 /ℓ ∞ -norm employed here and the ℓ 1 /ℓ 1 norm on the<br>",
    "Arabic": "نظرية العملية التجريبية",
    "Chinese": "经验过程理论",
    "French": "théorie des processus empiriques",
    "Japanese": "経験過程理論",
    "Russian": "теория эмпирических процессов"
  },
  {
    "English": "empirical risk",
    "context": "1: In Sec. 3.1, we have proposed a formulation for training early event detectors. This section provides further discussion on what exactly is being optimized. First, we briefly review the loss of SOSVM and its surrogate <mark>empirical risk</mark>. We then describe two general approaches for quantifying the loss of a detector on sequential data.<br>2: 1 Assume that the <mark>empirical risk</mark> is twice-differentiable and strictly convex in θ; in Section 4 we explore relaxing these assumptions.<br>",
    "Arabic": "المخاطر التجريبية",
    "Chinese": "经验风险",
    "French": "risque empirique",
    "Japanese": "経験的リスク",
    "Russian": "эмпирический риск"
  },
  {
    "English": "empirical risk minimization",
    "context": "1: [10,Section 5] for an overview in the case of classification, and Shapiro et al. [40,Chapter 5.3] for more general stochastic optimization problems). Vapnik and Chervonenkis [ 49,50 ] first provided such results in the context of { 0 , 1 } -valued losses for classification ( see also [ 1 ] ) , where the expectation of the loss always upper bounds its variance , so that if there exists a perfect classifier the convergence rates of <mark>empirical risk minimization</mark> procedures are O ( 1/n )<br>2: In this paper, we first show that the status quo of <mark>empirical risk minimization</mark> (ERM) amplifies representation disparity over time, which can even make initially fair models unfair.<br>",
    "Arabic": "تقليل المخاطر التجريبية",
    "Chinese": "经验风险最小化",
    "French": "minimisation du risque empirique",
    "Japanese": "経験的リスク最小化",
    "Russian": "минимизация эмпирического риска"
  },
  {
    "English": "empirical risk minimizer",
    "context": "1: In this case, taking θ 0 = 0 yields Var(ℓ(θ; X)) = 0 and R(θ 0 )−R(θ ⋆ ) = δB. For δ small (on the order of 1/ √ n), with constant probability the <mark>empirical risk minimizer</mark> is \n<br>2: Consider the perturbation z → z δ , and letθ z δ ,−z be the <mark>empirical risk minimizer</mark> on the training points with z δ in place of z. To approximate its effects, define the parameters resulting from moving mass from z onto z δ :θ ,z δ ,−z \n<br>",
    "Arabic": "مُقلِّل المخاطر التجريبي",
    "Chinese": "经验风险最小化者",
    "French": "minimiseur du risque empirique",
    "Japanese": "経験的リスク最小化器",
    "Russian": "эмпирический минимизатор риска"
  },
  {
    "English": "empirical variance",
    "context": "1: To remedy the limitations of considering <mark>empirical variance</mark> in isolation, we propose an interpolation protocol that can satisfy an accuracy guarantee, while still attempting to minimize the variance. min α∈[0,1] α 2σ2 1 + (1 − α) 2σ2 0 s.t.<br>2: However, in light of the empirical bias phenomenon detailed in Section 3 (or even actual bias in the presence of discontinuities), we see that the <mark>empirical variance</mark> is unreliable, and can lead to inaccurate estimates for our setting. For this reason, we consider an additional criterion of uniform accuracy: Definition 4.2 (Accuracy).<br>",
    "Arabic": "التباين التجريبي",
    "Chinese": "经验方差",
    "French": "variance empirique",
    "Japanese": "経験的分散",
    "Russian": "эмпирическая дисперсия"
  },
  {
    "English": "emulator",
    "context": "1: In contrast, the goal of ClimSim is to develop an <mark>emulator</mark> for the explicitly resolved effect of clouds and storms on climate, so that, down the road, the <mark>emulator</mark> can be used to replace parameterizations in a climate model, enabling more realistic climate simulation without the typical computational overhead.<br>",
    "Arabic": "محاكي",
    "Chinese": "模拟器",
    "French": "émulateur",
    "Japanese": "シミュレータ",
    "Russian": "эмулятор"
  },
  {
    "English": "encoder",
    "context": "1: The inputs and outputs for the CNN are stacked in the channel dimensions, such that the mapping is 60 × 6 → 60 × 10. Accordingly, global variables have been repeated along the vertical dimension. <mark>Encoder</mark>-Decoder (ED) consists of an <mark>Encoder</mark> and a Decoder with 6 fully-connected hidden layers each [39].<br>2: The <mark>Encoder</mark> decreases progressively the dimensionality of the input variables down to 5 nodes in the latent space of the network. These 5 latent nodes are the only input to the decoding part of ED.<br>",
    "Arabic": "التشفير",
    "Chinese": "编码器",
    "French": "codeur",
    "Japanese": "エンコーダー",
    "Russian": "Энкодер"
  },
  {
    "English": "encoder layer",
    "context": "1: Our model is based on a parsing architecture that contains an <mark>encoder layer</mark> that uses a pretrained network and a chart-based decoder, as detailed in Kitaev and Klein (2018).<br>2: Table 1: Computational complexity per <mark>encoder layer</mark> as a function of the input length L, the local window size w (typically set to 256 tokens), the number of global tokens g, random tokens r, sparse tokens s and the chunk size c. LOCOST has a much lower complexity than other sparse-attention baselines.<br>",
    "Arabic": "طبقة الترميز",
    "Chinese": "编码器层",
    "French": "couche encodeur",
    "Japanese": "エンコーダー層",
    "Russian": "слой кодера"
  },
  {
    "English": "encoder model",
    "context": "1: This pair uses the same <mark>encoder model</mark> as the previous example, which has a maximum of 32 discrete symbols. Due to the different symbols assigned to the prepositions, the read-out network attaches the prepositional phrase at a different height. Not all prepositional attachments can be reliably determined based on only the words up to and including the preposition.<br>2: The Merger Layer combines semantic and user/entity embeddings in the <mark>encoder model</mark>. We experiment with weighted sum fusion as in (Liu et al., 2018;Zhou et al., 2022)   as in (Gillick et al., 2019). The weighted-sum approach leads to best results on our task (see comparison in Appendix B).<br>",
    "Arabic": "نموذج الترميز",
    "Chinese": "编码器模型",
    "French": "modèle encodeur",
    "Japanese": "エンコーダーモデル",
    "Russian": "модель энкодера"
  },
  {
    "English": "encoder network",
    "context": "1: Our contributions are as follows: a) We propose a simple but efficient two-step framework for aggregating crowd-sourced coreference labels. b) We investigate how information about context, annotator reliability and instance complexity can be incorporated into our <mark>encoder network</mark> to infer correct labels.<br>2: a fully connected <mark>encoder network</mark>. The GAN framework requires training two neural networks with competing goals, which is known to be unstable and tends to introduce artifacts [32].<br>",
    "Arabic": "شبكة الترميز",
    "Chinese": "编码器网络",
    "French": "réseau d'encodage",
    "Japanese": "エンコーダーネットワーク",
    "Russian": "кодирующая сеть"
  },
  {
    "English": "encoder state",
    "context": "1: The copy probability weight α copy i is determined with the attention context vector c i , computed as a weighted sum of the attention values (i.e. linearly transformed <mark>encoder states</mark>) where the weights are defined by A i : \n α copy i = sigmoid(W ⊤ c i ). (12) \n<br>",
    "Arabic": "حالة المشفر",
    "Chinese": "编码器状态",
    "French": "état de l'encodeur",
    "Japanese": "エンコーダー状態",
    "Russian": "состояние энкодера"
  },
  {
    "English": "encoder-decoder architecture",
    "context": "1: tiplexer that combines the layout information and a subsequent <mark>encoder-decoder architecture</mark> for obtaining the final image. There are , however , important differences : ( i ) by separating the layout embedding from the appearance embedding , we allow for much more control and freedom to the object selection mechanism , ( ii ) by adding the location attributes as input , we allow for an intuitive and more direct user control , ( iii ) the architecture we employ enables better quality and higher resolution outputs , ( iv ) by adding stochasticity before the masks are created , we are able to generate multiple results per scene graph , ( v ) this effect is amplified by the ability of the users to manipulate the resulting image , by changing the properties of each individual object , ( vi ) we introduce a mask discriminator , which plays a crucial role in generating plausible masks , ( vii ) another novel discriminator captures the appearance encoding in a counterfactual way , and ( viii ) we introduce feature matching based on the discriminator network and ( ix ) a perceptual loss term to better capture the appearance of an object , even if the pose or<br>2: We train a fully supervised task-specific network for each task in S. Task-specific networks have an encoderdecoder architecture homogeneous across all tasks, where  the encoder is large enough to extract powerful representations, and the decoder is large enough to achieve a good performance but is much smaller than the encoder.<br>",
    "Arabic": "بنية التشفير وفك التشفير",
    "Chinese": "编码器-解码器架构",
    "French": "architecture encodeur-décodeur",
    "Japanese": "エンコーダーデコーダーアーキテクチャ",
    "Russian": "архитектура энкодер-декодер"
  },
  {
    "English": "encoder-decoder framework",
    "context": "1: Within our <mark>encoder-decoder framework</mark>, this idea is implemented using constrained decoding (Liang et al., 2017;Scholak et al., 2021), i.e., at each decoding step, a small set of admissible tokens from the vocabulary is determined based on the decoding history following predefined rules.<br>2: The comparison for models trained on wikiHow and CoScript are shown in Table 5. In general, LMs trained on CoScript outperform that on wikiHow. T5 outperforms GPT-2 in faithfulness, possibly due to its <mark>encoder-decoder framework</mark> being better at handling input information. However, GPT-2 outperforms T5 on other text generation metrics for scripts.<br>",
    "Arabic": "إطار التشفير وفك التشفير",
    "Chinese": "编码器-解码器框架",
    "French": "cadre encodeur-décodeur",
    "Japanese": "エンコーダデコーダ枠組み",
    "Russian": "фреймворк кодировщик-декодер"
  },
  {
    "English": "encoder-decoder model",
    "context": "1: Different LMs and decoding strategies are used in the baseline models. ArcaneQA ) is an encoderdecoder model built on top of a BERT encoder. It leverages constrained decoding and incrementally synthesizes a sequence of subprograms, where the constraints come from both the grammar and the execution of existing subprograms, to enforce grammaticality and faithfulness.<br>2: (8) aims to reward coverage of input words in a prediction using the attention mechanism of an <mark>encoder-decoder model</mark> as an oracle (Tu et al., 2016). While such methods help obtain stateof-the-art results in neural MT Gehring et al., 2017;, we view them as a patch to the observed problems.<br>",
    "Arabic": "نموذج الترميز-فك الترميز",
    "Chinese": "编码器-解码器模型",
    "French": "modèle encodeur-décodeur",
    "Japanese": "エンコーダデコーダモデル",
    "Russian": "модель энкодер-декодер"
  },
  {
    "English": "end-of-sequence token",
    "context": "1: Specifically, we represent our output at time step t as one-hot encoding of a D ×D +1 grid, where the D×D dimensions represent the possible 2D positions of the vertex, and the last dimension corresponds to the <mark>end-of-sequence token</mark> (i.e., polygon is closed).<br>",
    "Arabic": "رمز نهاية التسلسل",
    "Chinese": "序列结束标记",
    "French": "jeton de fin de séquence",
    "Japanese": "系列終了トークン",
    "Russian": "токен конца последовательности"
  },
  {
    "English": "end-to-end learning",
    "context": "1: These methods optimize a criterion at the split nodes based on the samples routed to each of the nodes. The second approach considers probabilistic relaxations/decisions at the split nodes and performs <mark>end-to-end learning</mark> with first order methods [Irsoy et al., 2012, Frosst and Hinton, 2017, Lay et al., 2018.<br>2: The abstracted code vector loses many important game information and may not capture the complex relationship between the game information and optimal decisions. To obtain an effective and suitable feature representation for <mark>end-to-end learning</mark> from the game state directly to the desired decision, we design a new multidimensional feature representation to encode both the current and historical card and bet information.<br>",
    "Arabic": "التعلم من النهاية إلى النهاية",
    "Chinese": "端到端学习",
    "French": "apprentissage de bout en bout",
    "Japanese": "エンドツーエンド学習",
    "Russian": "Обучение от конца к концу"
  },
  {
    "English": "end-to-end model",
    "context": "1: However, their performances heavily rely on the tailored cost based on human experience and the distribution from where trajectories are sampled [47]. Contrary to these approaches, we leverage the ego-motion information without sophisticated cost design and present the first attempt that incorporates the tracking module along with two genres of prediction rep-resentations simultaneously in an <mark>end-to-end model</mark>.<br>2: Table 5 compares <mark>end-to-end model</mark> performance with and without phonetic encoder. We observe that recall@k is similar between SBERT + PBERT + entity2vec <mark>end-to-end model</mark> and SBERT + entity2vec model.<br>",
    "Arabic": "نموذج من البداية إلى النهاية",
    "Chinese": "端到端模型",
    "French": "modèle de bout en bout",
    "Japanese": "エンドツーエンドモデル",
    "Russian": "модель с конца до конца"
  },
  {
    "English": "end-to-end neural model",
    "context": "1: Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of <mark>end-to-end neural models</mark>, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief.<br>",
    "Arabic": "نموذج عصبي من النهاية إلى النهاية",
    "Chinese": "端到端神经模型",
    "French": "modèle neuronal de bout en bout",
    "Japanese": "エンドツーエンドニューラルモデル",
    "Russian": "модель нейронной сети от начала до конца"
  },
  {
    "English": "end-to-end pipeline",
    "context": "1: Although it partially benefits from more parameters from the correspondence head, there is still good evidence that: with a proper <mark>end-to-end pipeline</mark>, PnP can outperform direct pose prediction on a large scale of data.<br>",
    "Arabic": "خط أنابيب من نهاية إلى نهاية",
    "Chinese": "端到端流水线",
    "French": "chaîne de traitement bout-en-bout",
    "Japanese": "エンドツーエンドパイプライン",
    "Russian": "сквозной конвейер"
  },
  {
    "English": "end-to-end system",
    "context": "1: The contributions are summarized as follows. (a) we embrace a new outlook of autonomous driving framework following a planning-oriented philosophy, and demonstrate the necessity of effective task coordination, rather than standalone design or simple multi-task learning. (b) we present UniAD, a comprehensive <mark>end-to-end system</mark> that leverages a wide span of tasks.<br>2: In particular,  and, subsequently, Cai et al. (2018),  and , indicate that predicate sense signals aid the identification of predicateargument relations. Therefore, we follow this line and propose an <mark>end-to-end system</mark> for cross-lingual SRL. Multilingual SRL.<br>",
    "Arabic": "نظام نهاية إلى نهاية",
    "Chinese": "端到端系统",
    "French": "système de bout en bout",
    "Japanese": "エンド・トゥ・エンド・システム",
    "Russian": "система \"от начала до конца\""
  },
  {
    "English": "end-to-end training",
    "context": "1: The CNN-based feature extraction (Zhang et al., 2018a) for density matrix multiplication loses the property of density matrix as a probability distribution. Nielsen and Chuang (2010) introduced three measures namely trace distance, fidelity, and VN-divergence. However, it is computationally costly to compute these metrics and propagate the loss in an <mark>end-to-end training</mark> framework.<br>2: With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al., 2015;Lample et al., 2016;Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al., 2013) and sentence-level features are learned automatically during <mark>end-to-end training</mark>.<br>",
    "Arabic": "التدريب من البداية إلى النهاية",
    "Chinese": "端到端训练",
    "French": "apprentissage de bout en bout",
    "Japanese": "エンドツーエンドのトレーニング",
    "Russian": "конца-в-конец обучение"
  },
  {
    "English": "energy function",
    "context": "1: As we shall see, these methods minimize an <mark>energy function</mark> with nonbinary variables by repeatedly minimizing an <mark>energy function</mark> with binary variables. 3<br>2: Thus, if an edge is submodular and both variables attached to it are flipped (i.e labeled 1) then the edge remains submodular. We need to minimize the number of non-submodular edges. Therefore, <mark>energy function</mark> for this new graphical model will be \n<br>",
    "Arabic": "دالة الطاقة",
    "Chinese": "能量函数",
    "French": "fonction d'énergie",
    "Japanese": "エネルギー関数",
    "Russian": "функция энергии"
  },
  {
    "English": "energy minimization",
    "context": "1: We relax the above criterion (3.1) so that we have to check the solution of only one <mark>energy minimization</mark> problem by modifying the unaries θ v on boundary nodes so that they bound the influence of all labelings on V\\A uniformly. Definition 3 (Boundary potentials and energies).<br>2: In this paper, instead of building a special purpose graph we will use some recent results [15] that give graph constructions for a quite general class of energy functions. While <mark>energy minimization</mark> has been widely used for stereo, only a few papers [13,20,24] have used it for scene reconstruction.<br>",
    "Arabic": "تقليل الطاقة",
    "Chinese": "能量最小化",
    "French": "minimisation d'énergie",
    "Japanese": "エネルギー最小化",
    "Russian": "минимизация энергии"
  },
  {
    "English": "energy minimization framework",
    "context": "1: While we effectively compute a disparity map with respect to each camera, they compute a disparity map only with respect to a single camera. [26] also proposed the use of disparity maps for each camera, in an <mark>energy minimization framework</mark>.<br>",
    "Arabic": "إطار تقليل الطاقة",
    "Chinese": "能量最小化框架",
    "French": "cadre de minimisation de l'énergie",
    "Japanese": "エネルギー最小化フレームワーク",
    "Russian": "фреймворк минимизации энергии"
  },
  {
    "English": "energy minimization problem",
    "context": "1: δ(x * ) ∈ argmin µ∈Λ A * ÊA * ,x * (µ) . (4 \n Definition 4 (Integrally Correct Algorithm). Assume an algorithm that takes as the input an energy minimization prboblem and outputs a labeling x * ∈ v∈V (X v ∪ {#}).<br>2: It is important to note that this subproblem is an <mark>energy minimization problem</mark> over binary variables, even though the overall problem that the expansion move algorithm is solving involves nonbinary variables. This is because each pixel will either keep its old value under f or acquire the new label .<br>",
    "Arabic": "مشكلة تقليل الطاقة",
    "Chinese": "能量最小化问题",
    "French": "problème de minimisation d'énergie",
    "Japanese": "エネルギー最小化問題",
    "Russian": "задача минимизации энергии"
  },
  {
    "English": "ensemble classification",
    "context": "1: We present a new approach to <mark>ensemble classification</mark> that requires learning only a single base classifier. The idea is to learn a classifier that simultaneously predicts pairs of test labels-as opposed to learning multiple predictors for single test labelsthen coordinating the assignment of individual labels by propagating beliefs on a graph over the data.<br>2: Ensemble classification has been a popular field of research in recent years. Multiple studies have demonstrated the beneficial effect of combining many classification models into aggregated ensemble classifiers on classification accuracy (e.g., [18,19]).<br>",
    "Arabic": "التصنيف الجماعي",
    "Chinese": "集成分类",
    "French": "classification par ensemble",
    "Japanese": "アンサンブル分類",
    "Russian": "ансамблевая классификация"
  },
  {
    "English": "ensemble classifier",
    "context": "1: We propose an ensemble method of dynamically maintaining an <mark>ensemble classifier</mark> according to changes in distribution of streaming unlabeled data. The proposed ensemble consists of several classifiers, and the mean vectors and the standard deviations of train data for each classifier. The final classification output of an ensemble is obtained by weighting each classifier's prediction.<br>2: h(D, x) = arg max l∈{0,1} p l (18 \n ) \n To prove the certified robustness of the mechanism h(D, x) against XBA, we need to find certified poisoning training size, which is the minimum number of poisoning training data such that the <mark>ensemble classifier</mark> changes the prediction for x. Formally , we find the minimum r D such that the following inequality is satisfied for ∀D + ∈ B ( D , r ) : h ( D + , x ) = l ⇔ p l < p ¬l ( 19 ) where l ∈ { 0 , 1 } and ¬l is the NOT operation of l in the binary classification<br>",
    "Arabic": "مصنف التجمع",
    "Chinese": "集成分类器",
    "French": "classificateur d'ensemble",
    "Japanese": "アンサンブル分類器",
    "Russian": "ансамбль классификаторов"
  },
  {
    "English": "ensemble learning",
    "context": "1: In a churn-prediction model, predictive performance is extremely important [3]. In this study, the use of <mark>ensemble learning</mark> for churn prediction is considered.<br>2: Moreover, it can also be viewed as a simple form of <mark>ensemble learning</mark> method that has some advantages over standard approaches. Third, we show empirically that our proposed method can achieve improvements in classification accuracy across a range of iid domains, for different base learning algorithms.<br>",
    "Arabic": "التعلم التجميعي",
    "Chinese": "集成学习",
    "French": "apprentissage d'ensemble",
    "Japanese": "アンサンブル学習",
    "Russian": "ансамблевое обучение"
  },
  {
    "English": "ensemble method",
    "context": "1: We proposed an <mark>ensemble method</mark> of dynamically forming an ensemble on streaming unlabeled data. To build a new classifier, the proposed <mark>ensemble method</mark> selects samples that will be labeled according to changes in streaming data distribution. Those unlabeled samples are defined as suspicious streaming samples that do not belong to any previous distribution in the current ensemble.<br>2: Multiple replicas of the networks are constructed by independent and random sampling of both trainable and non-trainable parameters [10,11]. RPNs also resort to data bootstrapping in order to mitigate the uncertainty collapse of the <mark>ensemble method</mark> when tested beyond the training data points [11].<br>",
    "Arabic": "طريقة التجميع",
    "Chinese": "集成方法",
    "French": "méthode d'ensemble",
    "Japanese": "アンサンブル手法",
    "Russian": "метод ансамбля"
  },
  {
    "English": "ensemble model",
    "context": "1: Moreover, we also prove that <mark>ensemble model</mark> can be distilled into a single model. Meaning that, through training a single model to \"simulate\" the output of the ensemble over the Figure 9: Justify the multi-view hypothesis in practice.<br>2: Pr (X,y)∼D [∃i ∈ [k] \\ {y} : F (t) y (X) < F (t) i (X)] ≤ 0.001µ . We shall restate the general version of Theorem 3 in Appendix F, and prove it in Appendix G. \n Remark. Theorem 3 necessarily means that the distilled model F has learned all the features { v i , } ( i , ) ∈ [ k ] × [ 2 ] from the <mark>ensemble model</mark> G. This is consistent with our empirical findings in Figure 8 : if one trains multiple individual models using knowledge distillation with different random seeds , then their ensemble<br>",
    "Arabic": "نموذج مجموعة",
    "Chinese": "集成模型",
    "French": "modèle d'ensemble",
    "Japanese": "アンサンブルモデル",
    "Russian": "ансамбль моделей"
  },
  {
    "English": "ensemble of classifier",
    "context": "1: A good example is the bagging method (Breiman 1996), where an <mark>ensemble of classifiers</mark> is constructed using bootstrap resampling from the training set. Bagging is known to improve generalization by reducing sample variance. Such techniques have been directly extended to MIL as well by resampling bags (Zhou and Zhang 2003).<br>2: Then we explain how PCs decomposition is used to define the structure of an <mark>ensemble of classifiers</mark>. This ensemble can be easily integrated in the CBD cycle without affecting isolation soundness. The approach is tested in a simulated scenario and systematically evaluated.<br>",
    "Arabic": "تجميع مصنفات",
    "Chinese": "分类器集合",
    "French": "ensemble de classificateurs",
    "Japanese": "分類器のアンサンブル",
    "Russian": "ансамбль классификаторов"
  },
  {
    "English": "ensemble size",
    "context": "1: For example, since on MSN-1 with Λ = 64 and 1, 000 trees QS scores a document in 9.5 µs, one would expect to score a document 20 times slower, i.e., 190 µs, when the <mark>ensemble size</mark> increases to 20, 000 trees.<br>",
    "Arabic": "حجم المجموعة",
    "Chinese": "集成大小",
    "French": "taille de l'ensemble",
    "Japanese": "アンサンブルサイズ",
    "Russian": "размер ансамбля"
  },
  {
    "English": "entail",
    "context": "1: To ensure that the relations are preserved within the candidates during conditional generation, we assert that the NLI model predicts the original and generated hypothesis to symmetrically <mark>entail</mark> each other. This indicates that the model perceives both the generated and original hypothesis as equivalent.<br>",
    "Arabic": "تستلزم",
    "Chinese": "蕴涵",
    "French": "impliquer",
    "Japanese": "含意する",
    "Russian": "влечь за собой"
  },
  {
    "English": "entailment",
    "context": "1: Some attributes are more useful for a particular class: e.g., the degree of premise-hypothesis overlap is most useful for predicting '<mark>entailment</mark>'. Note that the mean PVI for a particular class is different from the V-information.<br>2: PARENT (Dhingra et al., 2019) is an <mark>entailment</mark>based token-matching metric that calculates the F1 score based on entailed precision (an n-gram is correct if it occurs in the reference text or entailed by the input data) and entailed recall (recall against the reference text input data, adjusted by a weight parameter).<br>",
    "Arabic": "استلزام",
    "Chinese": "蕴涵",
    "French": "implication",
    "Japanese": "含意",
    "Russian": "логическое следствие"
  },
  {
    "English": "entailment detection",
    "context": "1: These benchmarks aggregate multiple supervised classification tasks-such as sentiment analysis, linguistic acceptability judgments, or <mark>entailment detection</mark>-and collate the scores obtained on those tasks into a leaderboard, with a single headline score for each model averaging its scores on each individual task.<br>",
    "Arabic": "نظام كشف الاستتباط",
    "Chinese": "蕴含检测",
    "French": "détection d'implication",
    "Japanese": "含意検出",
    "Russian": "логическое следование"
  },
  {
    "English": "entity",
    "context": "1: As defined by KoPL (Cao et al., 2022a), KB consists of 4 kinds of basic knowledge elements: <mark>Entities</mark> are unique objects that are identifiable in the real world, e.g., Germany. Concepts are sets of entities that have some characteristics in common, e.g., Country. Relations depict the relationship between entities or concepts.<br>2: papermage provides a unified interface, called Predictors, to ensure models produce <mark>Entities</mark> that are compatible with the Document. papermage includes several ready-to-use Predictors that leverage state-of-the-art models to extract specific document structures (Table 1). While magelib's abstractions are general for visually-rich documents, Predictors are optimized for parsing of scientific documents.<br>",
    "Arabic": "كيان",
    "Chinese": "实体",
    "French": "entité",
    "Japanese": "エンティティ",
    "Russian": "сущность"
  },
  {
    "English": "entity coreference",
    "context": "1: To fuse the two bases, we develop a state-of-the-art visual grounding system (Akbari et al., 2019) to resolve <mark>entity coreference</mark> across modalities.<br>2: This was the highestscoring system in the CoNLL-2011 Shared Task, and was also part of the highest-scoring system in the CoNLL-2012 Shared Task (Fernandes et al., 2012). It is a rule-based system that includes a total of ten rules (or \"sieves\") for <mark>entity coreference</mark>, such as exact string match and pronominal resolution.<br>",
    "Arabic": "إحالة الكيان",
    "Chinese": "实体消解",
    "French": "coréférence d'entités",
    "Japanese": "エンティティの共参照",
    "Russian": "сопоставление сущностей"
  },
  {
    "English": "entity description",
    "context": "1: The benchmark consists of 95k full text paragraphs from Wikipedia, annotated with mention boundaries and disambiguation targets, and integrates 8 existing ED datasets from various domains as evaluation splits. ZELDA defines a fixed entity vocabulary of 822k entities, together with fixed candidate lists and <mark>entity descriptions</mark>. In this paper: \n 1.<br>",
    "Arabic": "وصف الكيانات",
    "Chinese": "实体描述",
    "French": "description d'entité",
    "Japanese": "エンティティ記述",
    "Russian": "описание сущности"
  },
  {
    "English": "entity detection",
    "context": "1: Overall, the paper provides a valuable contribution to the field by addressing an important challenge of incorporating external knowledge sources into NLP models. The proposed MPEME method shows promise for improving <mark>entity detection</mark> and disambiguation, but further research is needed to fully explore its potential and limitations.<br>",
    "Arabic": "كشف الكيانات",
    "Chinese": "实体检测",
    "French": "La détection d'entités",
    "Japanese": "エンティティ検出",
    "Russian": "обнаружение сущностей"
  },
  {
    "English": "entity embedding",
    "context": "1: We train for up to 3 epochs with early stopping on validation loss. Our best model learns coefficients of 0.8 and 0.2 corresponding to the semantic and <mark>entity embedding</mark> weights (a and b in Figure 3).<br>2: We opt for a smaller architecture (small-BERT (Turc et al., 2019), 18M parameters) to ensure that our end-toend model latency is not significantly affected. In the merger layer, we add phonetic embedding derived from PBERT with the semantic and <mark>entity embedding</mark>, as described in Figure 4.<br>",
    "Arabic": "تضمين الكيان",
    "Chinese": "实体嵌入",
    "French": "plongement d'entité",
    "Japanese": "エンティティ埋め込み",
    "Russian": "вложение сущности"
  },
  {
    "English": "entity extraction",
    "context": "1: Note that this setting differs slightly from the WSC task in the SuperGLUE benchmark, which is presented as binary classification and requires <mark>entity extraction</mark> to convert to the form described in this section.<br>2: Taking Figure 1 as an example, the text <mark>entity extraction</mark> system extracts the nominal mention troops, but is unable to link or relate that due to a vague textual context. From the image, the entity linking system recognizes the flag as Ukrainian and represents it as a NationalityCitizen relation in the knowledge base.<br>",
    "Arabic": "استخراج الكيانات",
    "Chinese": "实体抽取",
    "French": "extraction d'entités",
    "Japanese": "エンティティ抽出",
    "Russian": "извлечение сущностей"
  },
  {
    "English": "entity linker",
    "context": "1: In our method, only topic entities are needed as the initial plan, which can be readily obtained using an <mark>entity linker</mark> . Second, our scoring function is based on a straightforward application of LMs, while SmBoP uses a more intricate architecture with extra parameters.<br>2: Basically, it tries out all possible combinations of the identified entities (i.e., the power set of the identified entities), considering that our <mark>entity linker</mark> normally can only identify no more than two entities from a question.<br>",
    "Arabic": "رابط الكيان",
    "Chinese": "实体链接器",
    "French": "\"lieur d'entités\"",
    "Japanese": "エンティティリンカー",
    "Russian": "сопоставитель сущностей"
  },
  {
    "English": "entity linking",
    "context": "1: <mark>Entity Linking</mark> and Coreference We seek to link the entity mentions to pre-existing entities in the background KBs (Pan et al., 2015), including Freebase (LDC2015E42) and GeoNames (LDC2019E43). For mentions that are linkable to the same Freebase entity, coreference information is added.<br>",
    "Arabic": "ربط الكيان",
    "Chinese": "实体链接",
    "French": "liaison d'entité",
    "Japanese": "エンティティリンキング",
    "Russian": "связывание сущностей"
  },
  {
    "English": "entity mention",
    "context": "1: (2021) disambiguate entities in news articles, and present a custom approach for constructing snippets: instead of only taking a token window around an <mark>entity mention</mark>, they also add the title and first two sentences of the article as additional context, reasoning that these texts contain salient information that pertains to the whole article.<br>",
    "Arabic": "التعبير عن الكيان",
    "Chinese": "实体提及",
    "French": "mention d'entité",
    "Japanese": "エンティティメンション",
    "Russian": "упоминание сущности"
  },
  {
    "English": "entity recognition",
    "context": "1: Be-sides the general abilities required for language understanding, such as <mark>entity recognition</mark>, coreference resolution, relation extraction, etc, models also need to collaborate with fuzzy individual room attributes and reason over entangled relationships among different rooms to understand the entire floor plan.<br>2: Compared to these, we benchmark humans and models on a different task-named <mark>entity recognition</mark>, but we share similar goals-to estimate the human-algorithmic performance gap and to identify patterns that could support the design of better evaluation methods or automatic solutions.<br>",
    "Arabic": "التعرف على الكيانات",
    "Chinese": "实体识别",
    "French": "reconnaissance d'entités",
    "Japanese": "エンティティ認識",
    "Russian": "распознавание сущностей"
  },
  {
    "English": "entity representation",
    "context": "1: achieves an accuracy of 88.5 % , a number that puts the reported probing classifier accuracy of 96.9 % into a bit more context . Further, Li et al. (2021), also presented an experiment where they manipulated specific <mark>entity representations</mark> of a synthetic version of the Alchemy dataset.<br>",
    "Arabic": "تمثيل الكيان",
    "Chinese": "实体表示",
    "French": "représentation d'entité",
    "Japanese": "エンティティ表現",
    "Russian": "представление сущности"
  },
  {
    "English": "entity resolution",
    "context": "1: S D = {ζ ∈ R D : ζ v ≥ 0 D v=1 ζ v = 1}. Here, two-way cuts use D = 2, but multiway-cuts with tens of thousands of classes also arise in <mark>entity resolution</mark> problems [18].<br>",
    "Arabic": "حل الكيانات",
    "Chinese": "实体消歧",
    "French": "résolution d'entités",
    "Japanese": "エンティティ解決",
    "Russian": "сопоставление сущностей"
  },
  {
    "English": "entity set",
    "context": "1: While a very large <mark>entity set</mark> is desirable for a general-purpose ED system, a smaller vocabulary tuned to an evaluation dataset will likely result in better evaluation numbers. This intuition is supported by experiments by Wu et al.<br>2: (2020) who found that a model trained to handle an <mark>entity set</mark> specific to their evaluation dataset outperforms a general-purpose model trained to handle 5.9M Wikipedia entities.<br>",
    "Arabic": "مجموعة الكيانات",
    "Chinese": "实体集",
    "French": "ensemble d'entités",
    "Japanese": "エンティティ集合",
    "Russian": "набор сущностей"
  },
  {
    "English": "entity type",
    "context": "1: Once entities are added into the (visual) knowledge base, we try to link each entity to the real-world entities from a curated background knowledge base. Due to the complexity of this task, we develop distinct models for each coarse-grained <mark>entity type</mark>.<br>2: Entity Generation Draw <mark>entity type</mark> T ∼ φ For each mention property r ∈ R, Fetch {(f r , θ r )} for τ T Draw word list length |L r | ∼ f r Draw |L r | words from w ∼ θ r \n See Figure 2 for an illustration of this process.<br>",
    "Arabic": "نوع الكيان",
    "Chinese": "实体类型",
    "French": "type d'entité",
    "Japanese": "エンティティの種別",
    "Russian": "тип сущности"
  },
  {
    "English": "entropy",
    "context": "1: Ops ( , ⊗) Backprop Gradients \n Log LSE, + ∆ p(zp = 1) Max max, + ∆ arg maxz K-Max k max, + ∆ K-Argmax Sample LSE, + ∼ z ∼ CRF( ) K-Sample LSE, + ∼ K-Samples Count , × <mark>Entropy</mark> (H) \n<br>2: where E(T [x]) is the entropy of T [x] [17]: \n<br>",
    "Arabic": "إنتروبيا",
    "Chinese": "熵",
    "French": "entropie",
    "Japanese": "エントロピー",
    "Russian": "энтропия"
  },
  {
    "English": "entropy estimation",
    "context": "1: In this section, we apply <mark>entropy estimation</mark> to an optimal experimental design (OED) problem. Simply put, the goal of OED is to determine the optimal experimental conditions (e.g., locations of sensors) that maximize certain utility function associated with the experiments.<br>2: More recent variants and extensions of the k-NN methods include (Gao, Ver Steeg, and Galstyan 2015;Berrett et al. 2019). Entropy estimation becomes increasingly more difficult as the dimensionality grows, and such difficulty is mainly due to the estimation bias, which decays very slowly with respect to sample size for high-dimensional problems.<br>",
    "Arabic": "تقدير الإنتروبيا",
    "Chinese": "熵估计",
    "French": "estimation de l'entropie",
    "Japanese": "エントロピー推定",
    "Russian": "оценка энтропии"
  },
  {
    "English": "entropy function",
    "context": "1: On an orthogonal direction, the authors of (Guo et al., 2021) consider a reformulation of the <mark>entropy function</mark> that accounts for the underlying geometry of the space. They present a method, called GEM, to learn an optimal policy for the geometry-aware entropy objective.<br>2: We focus on a particular class of variational approximation methods that cast the inference problem as a non-linear optimization over the marginal polytope, the set of valid marginal probabilities. The selection of appropriate marginals from the marginal polytope is guided by the (non-linear) <mark>entropy function</mark>.<br>",
    "Arabic": "وظيفة الإنتروبي",
    "Chinese": "熵函数",
    "French": "fonction d'entropie",
    "Japanese": "エントロピー関数",
    "Russian": "функция энтропии"
  },
  {
    "English": "entropy loss",
    "context": "1: [80] and RAFT [65]. L MT is a motion trajectory regularization term that encourages estimated trajectory fields to be cycle-consistent and spatial-temporally smooth. L cpt is a compactness prior that encourages the scene decomposition to be binary via an <mark>entropy loss</mark>, and mitigates floaters through distortion losses [2].<br>",
    "Arabic": "فقدان الانتروبي",
    "Chinese": "熵损失",
    "French": "perte d'entropie",
    "Japanese": "エントロピー損失",
    "Russian": "потеря энтропии"
  },
  {
    "English": "entropy regularization",
    "context": "1: MG is clearly overly myopic by continually shrinking the entropy range, ultimately resulting in a non-adaptive policy. Ablation: meta-regularization To fully control for the role of meta-regularization, we conduct further experiments by comparing BMG with and without <mark>entropy regularization</mark> (i.e. meta ) in the Lth target update step.<br>2: Main experiment: detailed results The purpose of our main experiment Section 5.1 is to (a) test whether larger meta-learning horizons-particularly by increasing L-can mitigate the short-horizon bias, and (b) test whether the agent can learn an exploration schedule without explicit domain knowledge in the meta-objective (in the form of <mark>entropy regularization</mark>).<br>",
    "Arabic": "تنظيم الانتروبيا",
    "Chinese": "熵正则化",
    "French": "régularisation de l'entropie",
    "Japanese": "エントロピー正則化",
    "Russian": "регуляризация энтропии"
  },
  {
    "English": "enumeration algorithm",
    "context": "1: Clearly, a DelayC lin <mark>enumeration algorithm</mark> for Q lets us decide the latter in linear time and thus we have found an algorithm for triangle detection that runs in time linear in |E|, refuting the triangle conjecture. The construction proceeds in two steps.<br>2: T = V \\ (X [ Y)) \n such that scientists know what external data to measure. Functions LISTSEPAB and LISTSEPC are modifications of the <mark>enumeration algorithm</mark> LISTSEP in (van der Zander, Liskiewicz, and Textor 2014). The function FINDSEP is also described in that paper , and works as follows : given a graph G , sets of variables X , Y , I , R , where X , Y , R are disjoint and I ✓ R ; FINDSEP is guaranteed to output a e Z 2 Z G ( X , Y ) hI , Ri whenever<br>",
    "Arabic": "خوارزمية التعداد",
    "Chinese": "枚举算法",
    "French": "algorithme d'énumération",
    "Japanese": "列挙アルゴリズム",
    "Russian": "перечислительный алгоритм"
  },
  {
    "English": "envy-freeness",
    "context": "1: As we noted earlier, under separation constraints, it can happen that all agents but one receive zero utility in every allocation, which renders proportionality and <mark>envy-freeness</mark> infeasible.<br>2: This work is concerned with fair division of a mixture of divisible and indivisible goods. To this end, we introduce the <mark>envy-freeness</mark> for mixed goods (EFM) fairness notion, which generalizes both EF and EF1 to the mixed goods setting. We show that an EFM allocation always exists for any number of agents.<br>",
    "Arabic": "خلو من الحسد",
    "Chinese": "无嫉妒性",
    "French": "absence d'envie",
    "Japanese": "エンヴィーフリーネス",
    "Russian": "\"свобода от зависти\""
  },
  {
    "English": "eos",
    "context": "1: We employ the beam search algorithm (Wiseman and Rush, 2016) to find the top-ranked prediction path at inference time. The prediction paths ending with the <mark>eos</mark> are added to the candidate path set.<br>",
    "Arabic": "نهاية الجملة",
    "Chinese": "终止符号",
    "French": "<eos>",
    "Japanese": "終了符 (eos)",
    "Russian": "eos"
  },
  {
    "English": "epipolar constraint",
    "context": "1: See Figure 9 for more images and [8] for videos. of transport paths dominate image formation in a projectorcamera system: epipolar paths, which satisfy the familiar <mark>epipolar constraint</mark> and contribute to a scene's direct image, and non-epipolar paths which contribute to its indirect.<br>2: A hand-held camera viewing a dynamic scene is a common scenario in modern photography. Recovering dense geometry in this case is a challenging task: moving objects violate the <mark>epipolar constraint</mark> used in 3D vision, and are often treated as noise or outliers in existing structure-from-motion (SfM) and multi-view stereo (MVS) methods.<br>",
    "Arabic": "القيد الإبيبولاري",
    "Chinese": "对极约束",
    "French": "contrainte épipolaire",
    "Japanese": "視線束拘束条件",
    "Russian": "эпиполярное ограничение"
  },
  {
    "English": "epipolar geometry",
    "context": "1: The new constraint on five points in two images has been presented, based on existing epipolar constraint and orientation consistence, in contrary to the common belief that any configuration of five point pairs corresponds to some (infinitely many) <mark>epipolar geometries</mark>. We have not discussed the form of the constraint for degenerate configurations G 5 .<br>2: Allowing such deviations could also lead to a broader class of images that give a stereo effect, and new approximate 3-view and N-view <mark>epipolar geometries</mark> may be possible [22]. Lastly, our analysis was independent of image irradiance information.<br>",
    "Arabic": "الهندسة الإبيبولارية",
    "Chinese": "对极几何",
    "French": "géométrie épipolaire",
    "Japanese": "エピポーラ幾何学",
    "Russian": "эпиполярная геометрия"
  },
  {
    "English": "epipolar line",
    "context": "1: Moreover, we observe that by fixing x I and varying π or, equivalently, varying l K arbitrarily, x J must move along the <mark>epipolar line</mark> F JI x I ∼ λ JI in image J since x I remains fixed.<br>2: All points of the <mark>epipolar line</mark> F IJ (0, 1, 0) T are generalized eigenvectors for the pair of homographic slices W and U and span a two-dimensional generalized eigenspace for these two homographies. If \n<br>",
    "Arabic": "خط الإبيبولار",
    "Chinese": "对极线",
    "French": "ligne épipolaire",
    "Japanese": "エピポーラ線",
    "Russian": "эпиполярная линия"
  },
  {
    "English": "epipole",
    "context": "1: We will refer to (1) and (2) as projective constraint and orientation constraint on the joint <mark>epipole</mark>, respectively, and denote the sets of joint <mark>epipole</mark>s satisfying them for a configuration G n respectively by P (G n ) and S(G n ). The set satisfying both constraints is 4  \n<br>2: (e 1 , e 2 ∈ C IJ ) ⇒ [(e 1 , f (e 1 )) ∈ E ⇔ (e 2 , f (e 2 )) ∈ E] . In other words, the property of 'being an allowed <mark>epipole</mark>' is constant inside any cell.<br>",
    "Arabic": "نقطة الاهتمام",
    "Chinese": "极点",
    "French": "épipôle",
    "Japanese": "エピポール",
    "Russian": "эпиполь"
  },
  {
    "English": "episodic return",
    "context": "1: We analyze how the hyper-parameters α and respectively affect the performance of PLPG and VSRL in noisy environ- ments. Specifically, we measure the <mark>episodic return</mark> and cumulative constraint violation. The results are plotted in Fig. 5. The effect of different values of α to PLPG agents is clear.<br>2: In our case, the trajectories are generated by the behavior policy during PPO rollouts, and only added to D SI if it is a successful trial or if the <mark>episodic return</mark> exceeds a certain threshold.<br>",
    "Arabic": "عائد الحلقة",
    "Chinese": "回合回报",
    "French": "retour épisodique",
    "Japanese": "エピソードのリターン",
    "Russian": "эпизодический возврат"
  },
  {
    "English": "epistemic uncertainty",
    "context": "1: Several active learning methods have been developed to account for different aspects of the machine learning training pipeline : while some acquire examples with high aleotoric uncertainty ( Settles , 2009 ) ( having to do with the natural uncertainty in the data ) or <mark>epistemic uncertainty</mark> ( having to do with the uncertainty in the modeling/learning process ) , others attempt to acquire<br>",
    "Arabic": "عدم اليقين المعرفي",
    "Chinese": "认识论不确定性",
    "French": "incertitude épistémique",
    "Japanese": "\"認識論的不確実性\"",
    "Russian": "неопределенность познания"
  },
  {
    "English": "epoch",
    "context": "1: V-information is more sensitive to over-fitting than heldout performance. At <mark>epoch</mark> 10, the V-information is at its lowest for all models, although the SNLI test accuracy has only declined slightly from its peak, as seen in Figure 2.<br>2: We control the convergence speed by updating the learning rate for each <mark>epoch</mark>: \n specifically, lr = 1 2 <mark>epoch</mark> i 2 * init lr, \n where init lr is the initial learning rate and <mark>epoch</mark> i is the index of current <mark>epoch</mark>.<br>",
    "Arabic": "حقبة",
    "Chinese": "轮次",
    "French": "époche",
    "Japanese": "エポック",
    "Russian": "эпоха"
  },
  {
    "English": "equalize odd",
    "context": "1: To see why, note that for any specific y 0 , since counterfactual <mark>equalized odds</mark> requires that D ⊥ ⊥ A | Y (1) = y 0 , setting the threshold for one group determines the thresholds for all the others; the budget constraint then can be used to fix the threshold for the original group.<br>2: A natural next direction is to explore guarantees for other fairness notions (such as <mark>equalized odds</mark>). Indeed, how does one construct query-efficient algorithms when µ is a function of both h * (x) and y?<br>",
    "Arabic": "تعادل الفرص العجيبة",
    "Chinese": "等化奇偶性",
    "French": "égalité des chances",
    "Japanese": "均等化オッズ",
    "Russian": "\"выравнивать шансы\""
  },
  {
    "English": "equivalence class",
    "context": "1: However, π j • r −1 π j (τ j ) is well defined, meaning that π j • τ j = π j • τ j for any two policies in the same <mark>equivalence class</mark>.<br>2: Then every <mark>equivalence class</mark> of reward functions, as defined in Section 5, has a unique reward function r(x, y), which can be reparameterized as r(x, y) = β log π(y|x) πref(y|x) for some model π(y|x). Proof.<br>",
    "Arabic": "فئة التكافؤ",
    "Chinese": "等价类",
    "French": "classe d'équivalence",
    "Japanese": "同値類",
    "Russian": "класс эквивалентности"
  },
  {
    "English": "equivalence query",
    "context": "1: Before providing a proof of Theorem 1, we show that both membership and <mark>equivalence queries</mark> are needed for polynomial learnability. Let AQ ∧ denote the class of unary CQs of the form q( \n x) ← A 1 (x) ∧ • • • ∧ A n (x) \n<br>2: If queries in Q are polynomial time learnable under EL r -ontologies in normal form using membership and <mark>equivalence queries</mark>, then the same is true for unrestricted EL r -ontologies. Proof. Let L be a polynomial time learning algorithm for Q under ontologies in normal form.<br>",
    "Arabic": "استعلام التكافؤ",
    "Chinese": "等价查询",
    "French": "requête d'équivalence",
    "Japanese": "等価クエリ",
    "Russian": "запрос эквивалентности"
  },
  {
    "English": "equivariance",
    "context": "1: This has as great advantage that no separate proofs are needed to deal with invariant or equivariant functions. Equivariance incurs quite some complexity in the setting considered in Azizian & Lelarge (2021).<br>2: [58,59] uses <mark>equivariance</mark> to learn dense landmarks, which recovers the 2D geometry of the objects.<br>",
    "Arabic": "تكافؤ",
    "Chinese": "等变性",
    "French": "équivariance",
    "Japanese": "同変性",
    "Russian": "эквивариантность"
  },
  {
    "English": "equivariant",
    "context": "1: (2022) to augment the data by permuting the labels of the 7 players randomly during training, since the game's rules are fully <mark>equivariant</mark> to such permutations. See Table 6 for a list of other hyperparameters. Bakhtin et al. (2021), described in detail in Section 2.3.<br>2: It is sufficient to parametrize the score network so that it is <mark>equivariant</mark> w.r.t. its second argument -assuming that ρ(g) and the drift b commute (e.g. which is true for a linear drift)-since we then have \n<br>",
    "Arabic": "متكافئ",
    "Chinese": "等变",
    "French": "équivariant",
    "Japanese": "等変",
    "Russian": "эквивариантный"
  },
  {
    "English": "error",
    "context": "1: Thus, D k [p] is a lower-approximation of r p with <mark>error</mark> q∈V E k [p](q)r q = |E k [p]|.<br>",
    "Arabic": "خطأ",
    "Chinese": "误差",
    "French": "erreur",
    "Japanese": "エラー",
    "Russian": "ошибка"
  },
  {
    "English": "error analysis",
    "context": "1: In Table 6, we report <mark>error analysis</mark> for our situated QA baselines that shows how often models are provide the answer from the specified context versus the union of all annotated contexts. We see that models often fail to incorporate extra-linguistic contexts into the question, producing the answer from another context.<br>",
    "Arabic": "تحليل الأخطاء",
    "Chinese": "错误分析",
    "French": "analyse des erreurs",
    "Japanese": "誤り分析",
    "Russian": "анализ ошибок"
  },
  {
    "English": "error bind",
    "context": "1: subspace log 3 T total d δ 1/4 . (27) \n Our proof (deferred to Appendix A.2) includes a novel perturbation analysis; the resulted <mark>error bound</mark> (27) has a 1/T 1/4 total,subspace dependence and is gap-free (i.e.<br>",
    "Arabic": "حدّ الخطأ",
    "Chinese": "误差界限",
    "French": "\"Borne d'erreur\"",
    "Japanese": "エラーバウンド",
    "Russian": "предел ошибки"
  },
  {
    "English": "error function",
    "context": "1: whose stochastic objective becomes the <mark>error function</mark> \n F (θ) = E w [H(θ + w)] = erf(−θ; σ 2 ), \n where erf(t; σ 2 ) := ∞ t 1 √ 2πσ e −x 2 /σ 2 dx is the Gaussian tail integral.<br>2: Where ρ(•) is some robust <mark>error function</mark>. Unless ρ(•) is defined as (weighted) squared-error like in Equation 1, the optimization problem in Equation 19does not have a closed-form solution.<br>",
    "Arabic": "دالة الخطأ",
    "Chinese": "误差函数",
    "French": "fonction d'erreur",
    "Japanese": "誤差関数",
    "Russian": "функция ошибки"
  },
  {
    "English": "error probability",
    "context": "1: By Lemma 1, if there is a profile based estimator with <mark>error probability</mark> δ, then the PML approach will have <mark>error probability</mark> at most δ exp(3 √ n). Such arguments were used in hypothesis testing to show the existence of competitive testing algorithms for fundamental statistical problems (Acharya et al., 2011;.<br>2: • With n samples, PML estimates any symmetric property of p with essentially the same accuracy, and at most e 3 √ n times the error, of any other estimator. This follows by combining Theorem 3 with Lemma 1. • For a large class of symmetric properties , including all those mentioned above , if there is an estimator that uses n samples , and has an <mark>error probability</mark> 1/3 , we design an estimator using O ( n ) samples , whose <mark>error probability</mark> is nearly exponential in n. We remark that this decay is much faster than applying the median trick<br>",
    "Arabic": "احتمالية الخطأ",
    "Chinese": "错误概率",
    "French": "probabilité d'erreur",
    "Japanese": "誤り確率",
    "Russian": "вероятность ошибки"
  },
  {
    "English": "error rate",
    "context": "1: Thus, it will make Ω(T 0 ) mistakes with high probability in the first phase, and thus to achieve ǫ <mark>error rate</mark>, it needs at least Ω(T 0 /ǫ) = Ω( S ǫγ ) examples.<br>2: Both values of the average classifier count and the average <mark>error rate</mark> increase in sections where the ratio of class is changed such as 0K~40K, 120K~160K, 320K~360K, and 440K~480K.<br>",
    "Arabic": "معدل الخطأ",
    "Chinese": "错误率",
    "French": "taux d'erreur",
    "Japanese": "誤り率 (あやまりりつ)",
    "Russian": "уровень ошибок"
  },
  {
    "English": "error tolerance",
    "context": "1: For illustration, we show in Figure 4 the piecewise linear representation of the example waveform pattern Figure 1(a), where we arbitrarily set the <mark>error tolerance</mark> = 1.0 for the segmentation algorithm so that the approximating error at each point will not exceed in amplitude on this data.<br>",
    "Arabic": "خطأ التسامح",
    "Chinese": "误差容限",
    "French": "tolérance aux erreurs",
    "Japanese": "誤差許容度",
    "Russian": "допустимая погрешность"
  },
  {
    "English": "estimation error",
    "context": "1: Based on the precise variance expansions in the preceding section, it is natural to expect that the robust solution (6) automatically trades between approximation and <mark>estimation error</mark>.<br>",
    "Arabic": "خطأ التقدير",
    "Chinese": "估计误差",
    "French": "erreur d'estimation",
    "Japanese": "推定誤差",
    "Russian": "оценочная ошибка"
  },
  {
    "English": "estimator",
    "context": "1: Since the <mark>estimator</mark> for |N (A, T )| is unbiased [17], essentially the outer-loop of averaging over n samples of random transmission times further reduces the variance of the <mark>estimator</mark> in a rate of O(1/n).<br>2: ) . Proof. We now state Fano's inequality for distribution estimation. Lemma 23. Let p 1 , p 2 , . . . p r+1 be distributions such that D(p i ||p j ) ≤ β and ||p i − p j || 1 ≥ α, for all i, j. For any <mark>estimator</mark> q,<br>",
    "Arabic": "مُقَدِّر",
    "Chinese": "估计量",
    "French": "estimateur",
    "Japanese": "推定量",
    "Russian": "оценщик"
  },
  {
    "English": "euclidean distance",
    "context": "1: For every word pair (w a i , w b j ) across sentences, the model directly calculates word pair interactions using cosine similarity, <mark>Euclidean distance</mark>, and dot product over the outputs of the encoding layer: \n<br>2: • <mark>Euclidean distance</mark>: d(x, y) = x − y 2 , • Cityblock distance: d(x, y) = x − y 1 , • Cosine distance: d(x, y) = 1 − x,y x 2 y 2 .<br>",
    "Arabic": "المسافة الإقليدية",
    "Chinese": "欧氏距离",
    "French": "distance euclidienne",
    "Japanese": "ユークリッド距離",
    "Russian": "евклидово расстояние"
  },
  {
    "English": "euclidean divergence",
    "context": "1: This lemma simply means we can compute the <mark>Euclidean divergence</mark> of u in our implementation. Given a set of observed data  \n X = {x i } m i=1 ⊂ M ⊂ R d , \n<br>2: If u ∈ X(R d ), u| M ∈ X(M) is infinitesimally constant in normal directions of M, then for x ∈ M, div(u(x)) = div E (u(x)), \n where div E denotes the standard <mark>Euclidean divergence</mark>.<br>",
    "Arabic": "تباعد إقليدي",
    "Chinese": "欧几里德散度",
    "French": "divergence euclidienne",
    "Japanese": "ユークリッド発散",
    "Russian": "евклидова дивергенция"
  },
  {
    "English": "euclidean loss",
    "context": "1: 5 ) Conv3x3 , feature maps=80 , ( 6 ) Conv3x3 , feature maps=192 , ( 7 ) MaxPool2x2 , stride=2 , ( 8 ) FC9600 , ( 9 ) FC1000 , ( 10 ) FC3 , ( 11 ) <mark>Euclidean loss</mark> . All networks are trained with a constant 0.001 learning rate and 512 batch size, until the validation error converges.<br>",
    "Arabic": "الخسارة الإقليدية",
    "Chinese": "欧几里得损失",
    "French": "perte euclidienne",
    "Japanese": "ユークリッドロス",
    "Russian": "Евклидова ошибка"
  },
  {
    "English": "euclidean norm",
    "context": "1: We will use v or v 2 to denote the <mark>Euclidean norm</mark> of a vector v, A or A 2 to denote the operator norm of a matrix A, and A F := Tr(A T A) to denote the Frobenius norm of a matrix A.<br>2: Throughout, • denotes the <mark>Euclidean norm</mark> of vectors. We begin by specifying our sense of expectations and derivatives, and then turn to other, less-standard preliminaries. To rigorously describe expectations of non-continuous functions and of derivatives of non-smooth functions, we start with some preliminaries from measure theory.<br>",
    "Arabic": "المعيار الإقليدي",
    "Chinese": "欧几里得范数",
    "French": "norme euclidienne",
    "Japanese": "ユークリッドノルム",
    "Russian": "Евклидова норма"
  },
  {
    "English": "euclidean plane",
    "context": "1: In both cases, however, the moral is the same: the topological spaces of most interest for Qualitative Spatial Reasoning exhibit special characteristics which any topological constraint language able to express connectedness must take into account. The results of Sec. 4 pose a challenge for Qualitative Spatial Reasoning in the <mark>Euclidean plane</mark>.<br>2: that are satisfiable in the <mark>Euclidean plane</mark> , but only by 'pathological ' sets that can not plausibly represent the regions occupied by physical objects [ Pratt-Hartmann , 2007 ] . Unfortunately, little is known about the complexity of topological constraint satisfaction by non-pathological objects in low-dimensional Euclidean spaces.<br>",
    "Arabic": "مستوى إقليدي",
    "Chinese": "欧几里德平面",
    "French": "plan euclidien",
    "Japanese": "ユークリッド平面",
    "Russian": "Евклидова плоскость"
  },
  {
    "English": "euclidean projection",
    "context": "1: We created a data set of clean entity lists from the DBLife website and of entity mentions from the DBLife Web Crawl [11]. The data set consists of 18,167 entities and 180,110 mentions and similarities given by string similarity. In this problem each stochastic gradient step must compute a <mark>Euclidean projection</mark> onto a simplex of dimension 18,167.<br>2: Recall our shorthand notation that π(θ) = argmin θ * ∈S⋆ { θ − θ * 2 } denotes the <mark>Euclidean projection</mark> of θ onto S ⋆ , which is a closed convex set. Define also the localized empirical deviation function \n<br>",
    "Arabic": "إسقاط إقليدي",
    "Chinese": "欧几里得投影",
    "French": "projection euclidienne",
    "Japanese": "ユークリッド射影",
    "Russian": "евклидова проекция"
  },
  {
    "English": "euclidean space",
    "context": "1: One traditional objective of graph embedding is to place points on a surface such that arcs never cross. In this setting, it is well known that any graph can be embedded in 3-dimensional <mark>Euclidean space</mark> and planar graphs can be embedded in 2-dimensional <mark>Euclidean space</mark>.<br>2: The deep MPP modifies this by multiplying in pairwise repulsion terms in (0, 1) that increase as the vectors' endpoints move apart in <mark>Euclidean space</mark> (or as T → ∞).<br>",
    "Arabic": "الفضاء الإقليدي",
    "Chinese": "欧几里得空间",
    "French": "espace euclidien",
    "Japanese": "ユークリッド空間",
    "Russian": "евклидово пространство"
  },
  {
    "English": "euclidean transformation",
    "context": "1: The viewpoint w ∈ R 6 represents an <mark>Euclidean transformation</mark> (R, T ) ∈ SE(3), where w 1:3 and w 4:6 are rotation angles and translations along x, y and z axes respectively. The map (R, T ) transforms 3D points from the canonical view to the actual view.<br>",
    "Arabic": "تحويل إقليدي",
    "Chinese": "欧氏变换",
    "French": "transformation euclidienne",
    "Japanese": "ユークリッド変換",
    "Russian": "евклидово преобразование"
  },
  {
    "English": "euler angle",
    "context": "1: Initial work along this direction mainly focused on learning constraints for joint limits in <mark>Euler angles</mark> [17] or swing and twist representations [6,56,1], to avoid twists and bends beyond certain limits.<br>2: G ∈ R K×3 are the node positions of the undeformed graph, N vn (i) is the set of nodes that influence vertex i, and R(•) is a function that converts the <mark>Euler angles</mark> to rotation matrices.<br>",
    "Arabic": "زوايا أويلر",
    "Chinese": "欧拉角",
    "French": "angle d'Euler",
    "Japanese": "オイラー角",
    "Russian": "углы Эйлера"
  },
  {
    "English": "evaluation function",
    "context": "1: More specifically, the concurrence CONCUR(D 1 , D 2 ; A, Eval) between two dataset splits D 1 and D 2 , given a set of modeling approaches A and <mark>evaluation function</mark> Eval, is defined as: \n<br>2: The generation procedure is a search procedure [8] that produces candidate feature subsets for evaluation based on a certain search strategy. Feature selection methods applied in this paper generate candidates randomly. An <mark>evaluation function</mark> measures the goodness of the subset produced and this value is compared with the previous best.<br>",
    "Arabic": "دالة التقييم",
    "Chinese": "评估函数",
    "French": "fonction d'évaluation",
    "Japanese": "評価関数",
    "Russian": "функция оценки"
  },
  {
    "English": "evaluation metric",
    "context": "1: When comparing across data strategies, loss ceases to be a good <mark>evaluation metric</mark> as the models are trained on different data distributions. We thus evaluate models on 19 natural language tasks with zero to five in-context few-shot exemplars [15] producing 114 scores per model.<br>2: In particular, we show that if a surrogate loss is convex in the sense that (y, .) is convex for every y ∈ Y, and if the <mark>evaluation metric</mark> is the AP, the ERR or the PD, then there are distributions and sequences of scoring functions for which (1) does not hold.<br>",
    "Arabic": "معيار التقييم",
    "Chinese": "评价指标",
    "French": "métrique d'évaluation",
    "Japanese": "評価メトリック",
    "Russian": "метрика оценки"
  },
  {
    "English": "evaluation set",
    "context": "1: Let G be a set of ground truth data, partitioned into a training set G train , a development set G dev and a test (evaluation) set G test . Let S be a system with arbitrary parameters and hyperparameters, and let M be an evaluation metric.<br>2: For every site in the <mark>evaluation set</mark> s, we calculate its distance with respect to every different site t in the training set, and then compare its minimum distance w.r.t the subset of sites with the same gold label S same and the subset of sites with the opposite label S oppo , \n<br>",
    "Arabic": "مجموعة التقييم",
    "Chinese": "评估集",
    "French": "ensemble d'évaluation",
    "Japanese": "評価セット",
    "Russian": "набор для оценки"
  },
  {
    "English": "event calculus",
    "context": "1: For reasoning about dynamics (with <Φ, Θ>), we use a variant of <mark>event calculus</mark> as per [Ma et al., 2014;Miller et al., 2013]; in particular, for examples of this paper, the functional <mark>event calculus</mark> fragment (Σ dyn ) of Ma et al.<br>",
    "Arabic": "حساب الحدث",
    "Chinese": "事件演算",
    "French": "calcul d'événements",
    "Japanese": "イベント微積分",
    "Russian": "исчисление событий"
  },
  {
    "English": "event coreference",
    "context": "1: Entity linking, multi-document extraction and <mark>event coreference</mark> Our work also relates to the task of multi-document information extraction, where the goal is to connect different mentions of the same entity across input documents (Mann and Yarowsky, 2005;Han et al., 2011;Durrett and Klein, 2014).<br>2: As shown in Figure 3, the Text Knowledge Extraction (TKE) system extracts entities, relations, and events from input documents. Then it clusters identical entities through entity linking and coreference, and clusters identical events using <mark>event coreference</mark>. 8 https://tac.nist.gov/tracks/SM-KBP/2019/ ontologies/LDCOntology<br>",
    "Arabic": "ترابط الأحداث",
    "Chinese": "事件共指",
    "French": "coréférence d'événements",
    "Japanese": "イベント共参照",
    "Russian": "событийная кореференция"
  },
  {
    "English": "event detection",
    "context": "1: In our work, we have mainly addressed two classes of simulation methods for contact. The first uses the penalty method (Geilinger et al., 2020;Tedrake, 2022), which approximates contact via stiff springs, and the second uses <mark>event detection</mark> (Hu et al., 2020), which explicitly computes time-of-impact for automatic differentiation.<br>2: It realizes event trigger identification, event classification, event argument identification, and argument role classification tasks successively [32] and takes the results of previous tasks as prior knowledge. The first two tasks are usually called <mark>event detection</mark> and the last two tasks are called argument extraction. Chen et al. [24] and Nguyen et al.<br>",
    "Arabic": "كشف الأحداث",
    "Chinese": "事件检测",
    "French": "détection d'événements",
    "Japanese": "イベント検出",
    "Russian": "обнаружение событий"
  },
  {
    "English": "event extraction",
    "context": "1: Our multi-turn <mark>event extraction</mark> process also uses the newly obtained argument information to update decisions of the previously extracted arguments. This dualway feedback process enables us to exploit the relation among event arguments to classify the argument's role in different text contexts. We evaluated our approach on the ACE 2005 dataset and compared to 7 prior <mark>event extraction</mark> methods.<br>2: Most existing methods extract all arguments simultaneously [5,6] or individual arguments sequentially [7], all of which do not consider the effect of argument extraction order. Our work seeks to close this gap by explicitly modeling the argument relation for <mark>event extraction</mark>.<br>",
    "Arabic": "استخراج الحدث",
    "Chinese": "事件抽取",
    "French": "extraction d'événements",
    "Japanese": "イベント抽出",
    "Russian": "извлечение событий"
  },
  {
    "English": "evidence lower bind",
    "context": "1: We note that the resulting optimization problem is intractable and requires the optimization of an <mark>evidence lower bound</mark> via an approximate posterior, which we do not derive hereplease refer to [15].<br>2: In variational inference (VI), the <mark>evidence lower bound</mark> (ELBO), a lower bound on logmarginal likelihood, is often used for automatically setting hyperparameters (Hoffman et al., 2013;Kingma and Welling, 2013;Kingma et al., 2015;Alemi et al., 2018).<br>",
    "Arabic": "حد أدنى للدليل",
    "Chinese": "证据下界",
    "French": "borne inférieure de l'évidence",
    "Japanese": "証拠下界",
    "Russian": "доказательство нижней границы"
  },
  {
    "English": "evidence maximization",
    "context": "1: This expression is then maximized with respect to the unknown hyperparameters, a process referred to as type-II maximum likelihood or <mark>evidence maximization</mark> [7,9] or restricted maximum likelihood [4]. Thus the optimization problem shifts from finding the maximum a posteriori sources given a fixed prior to finding the optimal hyperparameters of a parameterized prior.<br>2: However, the process of marginalization, or the integrating out of the unknown sources S, provides an extremely powerful regularizing effect, driving most of the unknown γ i to zero during the <mark>evidence maximization</mark> stage (more on this in Section 3).<br>",
    "Arabic": "تعظيم الأدلة",
    "Chinese": "证据最大化",
    "French": "maximisation de l'évidence",
    "Japanese": "証拠最大化",
    "Russian": "максимизация доказательств"
  },
  {
    "English": "exact inference",
    "context": "1: For the SE kernel, taking M = O(log D N ) inducing points leads to a complexity of O(N log 4D+1 N ), a large computational saving compared to the O(N 3 ) cost of <mark>exact inference</mark>.<br>2: As an additional contribution, we connect the semantic notion of exchangeability to syntactic notions of tractability by showing that liftable statistical relational models have the required exchangeability properties due to their syntactic symmetries. We thereby unify notions of lifting from the exact and approximate inference community into a common framework.<br>",
    "Arabic": "الاستدلال الدقيق",
    "Chinese": "精确推理",
    "French": "inférence exacte",
    "Japanese": "厳密推論",
    "Russian": "точный вывод"
  },
  {
    "English": "exact match",
    "context": "1: We evaluate our model on the NQ development set using <mark>Exact Match</mark> (accuracy) (Rajpurkar et al., 2016). We report the following metrics: \n 1. Contextual Answer Quality: Accuracy on the original NQ dev set. We compare the contextual answer to the expected (original) answer. 2.<br>",
    "Arabic": "تطابق دقيق",
    "Chinese": "完全匹配",
    "French": "Correspondance exacte",
    "Japanese": "完全一致",
    "Russian": "- Точное совпадение"
  },
  {
    "English": "excess loss",
    "context": "1: We give lower bounds for the number of weak learners and the sample complexity in this section that show that our Online BBM algorithm is optimal up to logarithmic factors. Theorem 3. For any γ ∈ (0, 1 4 ), S ≥ \n ln( 1 δ ) \n γ , δ ∈ ( 0 , 1 ) and ǫ ∈ ( 0 , 1 ) , there is a weak online learning algorithm with edge γ and <mark>excess loss</mark> S satisfying ( 1 ) with probability at least 1 − δ , such that to achieve error rate ǫ , an online boosting algorithm needs at least Ω ( 1 γ 2<br>2: For empirical risk minimization, fast rates of convergence hold under conditions in which the the gap R(θ) − R(θ ⋆ ) controls the variance of the <mark>excess loss</mark> ℓ(θ, X) − ℓ(θ ⋆ , X) [cf. 32,3,10,4], which usually requires some type of uniform convexity assump-tion.<br>",
    "Arabic": "الخسارة الزائدة",
    "Chinese": "超额损失",
    "French": "perte excédentaire",
    "Japanese": "過剰損失",
    "Russian": "избыточные потери"
  },
  {
    "English": "exchangeability",
    "context": "1: We identify this prior in Section 3, and use it to build a powerful hierarchical model in Section 4. As we will see, this hierarchical model displays many attractive properties: conjugacy, <mark>exchangeability</mark> and closed-form predictive distributions for the waiting times, and exact Gibbs updates for the time scale parameters.<br>2: There are several interesting connections between automorphisms, <mark>exchangeability</mark>, and lifted inference (Niepert 2012a). Moreover, there are several group theoretical algorithms that one could apply to the automorphism groups to discover the structure of exchangeable variable decompositions from the structure of the graphical models.<br>",
    "Arabic": "تبادلية",
    "Chinese": "可交换性",
    "French": "échangeabilité",
    "Japanese": "交換可能性",
    "Russian": "обмениваемость"
  },
  {
    "English": "existential quantifier",
    "context": "1: For an example of (i), we can show that a compiled firstorder circuit (Van den Broeck et al. 2011) or the trace of probabilistic theorem proving (Gogate and Domingos 2011) encode a sufficient statistic in their <mark>existential quantifier</mark> and splitting nodes.<br>",
    "Arabic": "الكمي الوجودي",
    "Chinese": "存在量词",
    "French": "quantificateur existentiel",
    "Japanese": "存在量化子",
    "Russian": "квантор существования"
  },
  {
    "English": "expect loss",
    "context": "1: If we simply define the risk to be the <mark>expected loss</mark> on one particular example x and set f n (x) = α (n) , then R ϕ (f n ) = W (p, α (n) ). Further, by assumption there is some ε > 0 such that ℓ(p, α \n<br>2: The goal of multi-distribution learning is finding a hypothesis that uniformly minimizes <mark>expected loss</mark> across multiple data distributions and loss functions. Importantly, we make no assumptions on the relationships between the data distributions; for example, we do not assume the existence of a hypothesis that is simultaneously optimal for every distribution.<br>",
    "Arabic": "الخسارة المتوقعة",
    "Chinese": "期望损失",
    "French": "perte attendue",
    "Japanese": "期待損失",
    "Russian": "ожидаемые потери"
  },
  {
    "English": "expect reward",
    "context": "1: Informally, this corresponds to the present value of revenue/profit/cost that will be realized in the next step, and is an essential parameter in determining the correct tradeoff between maximizing present <mark>expected reward</mark> (exploitation) vs. obtaining more information with a view towards improving future rewards (exploration).<br>2: Once average reward on all these tasks reaches a certain threshold, the length limit is incremented. We assume that rewards across tasks are normalized with maximum achievable reward 0 < q i < 1. LetÊr τ denote the empirical estimate of the <mark>expected reward</mark> for the current policy on task τ .<br>",
    "Arabic": "المكافأة المتوقعة",
    "Chinese": "期望奖励 (Qīwàng jiǎnglì)",
    "French": "récompense attendue",
    "Japanese": "期待報酬",
    "Russian": "ожидаемая награда"
  },
  {
    "English": "expect utility",
    "context": "1: y = argmax h∈H(x) E p(y|x,θ) [u(y, h)], \n where the maximisation is over the entire set of possible translations H(x). Note that there is no need for a human-annotated reference, <mark>expected utility</mark> is computed by having the model fill in reference translations.<br>2: The strict uncertainty model assumes even less information: vendors only know the possible set of buyer types (i.e., only the support of the distribution is known). In this model, <mark>expected utility</mark> is illdefined so we instead adopt a common approach for such settings and assume vendors try to minimize their worst-case regret over all possible type realizations.<br>",
    "Arabic": "المنفعة المتوقعة",
    "Chinese": "期望效用",
    "French": "utilité attendue",
    "Japanese": "期待効用",
    "Russian": "ожидаемая полезность"
  },
  {
    "English": "expectation maximization",
    "context": "1: Since the cluster representatives as well as the cluster labels for the points are unknown in a clustering setting, maximizing Eqn. ( 5) is an \"incomplete-data problem\", for which a popular solution method is <mark>Expectation Maximization</mark> (EM) [16].<br>",
    "Arabic": "تعظيم التوقعات",
    "Chinese": "期望最大化",
    "French": "maximisation de l'espérance",
    "Japanese": "期待値最大化",
    "Russian": "метод максимизации ожидания"
  },
  {
    "English": "expectation maximization algorithm",
    "context": "1: between 0 and 1 pixels for an image size of 500 × 500 pixels. For comparison purposes, we also implemented the polynomial factorization algorithm (PFA) of [12] and a variation of the <mark>Expectation Maximization algorithm</mark> (EM) for clustering hyperplanes in R 3 .<br>2: We use graph-based label propagation for cross-lingual knowledge transfer and use the projected labels as features in an unsupervised model (Berg-Kirkpatrick et al., 2010). Across eight European languages, our approach results in an average absolute improvement of 10.4% over a state-of-the-art baseline, and 16.7% over vanilla hidden Markov models induced with the <mark>Expectation Maximization algorithm</mark>.<br>",
    "Arabic": "خوارزمية تعظيم التوقعات",
    "Chinese": "期望最大化算法",
    "French": "algorithme d'estimation-maximisation",
    "Japanese": "期待値最大化アルゴリズム",
    "Russian": "алгоритм максимизации ожидания"
  },
  {
    "English": "experience replay",
    "context": "1: This led to both faster learning and to better final policy quality across most games of the Atari benchmark suite, as compared to uniform <mark>experience replay</mark>.<br>2: The precise behavior depends on the training regime, but poor behavior can emerge in batch methods as well. For instance, batch Q-learning with <mark>experience replay</mark> and replay buffer shuffling will induce the same tension between the conflicting updates. Specific ( nonrandom ) batching schemes can cause even greater degrees of delusion ; for example , training in a sequence of batches that run through a batch of transitions at s 4 , followed by batches at s 3 , then s 2 , then s 1 will induce a Q-function that deludes itself into estimating the value of ( s 1 ,<br>",
    "Arabic": "إعادة التجربة",
    "Chinese": "经验重放",
    "French": "expérience de rejeu",
    "Japanese": "経験再生",
    "Russian": "Повторное использование опыта"
  },
  {
    "English": "expert demonstration",
    "context": "1: • PUR-IRL allows for incremental integration of new information through iterative updates, thereby turning one large intractable problem into a series of tractable ones. • PUR-IRL accurately infers optimal policies and latent reward functions given a set of <mark>expert demonstrations</mark>. Extracting <mark>expert demonstrations</mark> of cancer progression from patient tumors.<br>",
    "Arabic": "عروض الخبراء",
    "Chinese": "专家示范",
    "French": "démonstrations expertes",
    "Japanese": "専門家による実演",
    "Russian": "демонстрация экспертов"
  },
  {
    "English": "explicit-state search",
    "context": "1: Compared to the potential heuristics in <mark>explicit-state search</mark>, A+I solves 109 instances more than pot A+I . Moreover, there are only 9 domains where using symbolic search with the same heuristics is detrimental, compared to 26 domains where it is beneficial. Note that these two configurations are using the same optimization criteria to compute the potentials.<br>2: Most remarkably, the number of BDD nodes from all expanded BDDs often decreased with potential heuristics (Figure 2c). This confirms that these heuristics are not only informative for <mark>explicit-state search</mark>, avoiding expansion of certain states, but also beneficial in symbolic search by inducing a good BDD partitioning.<br>",
    "Arabic": "البحث في الحالة الصريحة",
    "Chinese": "显式状态搜索",
    "French": "recherche d'états explicites",
    "Japanese": "明示的状態探索",
    "Russian": "явный поиск состояний"
  },
  {
    "English": "explode gradient",
    "context": "1: In [44,24], a few intermediate layers are directly connected to auxiliary classifiers for addressing vanishing/<mark>exploding gradients</mark>. The papers of [39,38,31,47] propose methods for centering layer responses, gradients, and propagated errors, implemented by shortcut connections.<br>2: Backpropagation through time involves backpropagating through a full unrolled sequence (e.g. of length T ) for each parameter update. Unrolling a model over full sequences faces several difficulties : 1 ) the memory cost scales linearly with the unroll length , because we need to store intermediate activations for backprop ( though this can be reduced at the cost of additional compute ( Dauvergne & Hascoët , 2006 ; Chen et al. , 2016 ) ) ; 2 ) we only perform a single parameter update after each full unroll , which is computationally expensive and introduces large latency between parameter updates ; 3 ) long unrolls can lead to exploding or vanishing gradients ( Pascanu et al. , 2013 ) , and chaotic and poorly conditioned loss landscapes ( Pearlmutter , 1996 ; Maclaurin et al. , 2015 ; Parmas et al. , 2018 ; Metz<br>",
    "Arabic": "انفجار التدرج",
    "Chinese": "梯度爆炸",
    "French": "explosion du gradient",
    "Japanese": "勾配爆発",
    "Russian": "взрывной градиент"
  },
  {
    "English": "exploitability",
    "context": "1: By setting the P 1 alternative payoff for I r,1 to v(I r,1 , a T ) = CBV σ * 2 (I 1 ), safe subgame solving guarantees a strategy will be produced with <mark>exploitability</mark> no worse than σ * 2 .<br>2: The Reach-Estimate + Distributional algorithm generally resulted in the lowest <mark>exploitability</mark> among the various choices, and in most cases beat Unsafe subgame solving. In all but one case, using estimated values lowered <mark>exploitability</mark> more than Maxmargin and Resolve subgame solving. Also, in all but one case using distributional alternative payoffs lowered <mark>exploitability</mark>.<br>",
    "Arabic": "الاستغلالية",
    "Chinese": "可利用性",
    "French": "exploitabilité",
    "Japanese": "搾取可能性",
    "Russian": "эксплуатируемость"
  },
  {
    "English": "exploration rate",
    "context": "1: where u i ,ū denote agent k's utility from action i and average utility, respectively, given all other agents' actions and α k /β k is agent k's <mark>exploration rate</mark>. 1 Agents tune the exploration parameter to increase/decrease exploration during the learning process. We analyze the performance of SQL dynamics along the following axes.<br>2: W is how many times the user selects a returned intent , and α is the <mark>exploration rate</mark> set between [ 0 , 1 ] .<br>",
    "Arabic": "معدل الاستكشاف",
    "Chinese": "探索率",
    "French": "taux d'exploration",
    "Japanese": "探索率",
    "Russian": "темп исследования"
  },
  {
    "English": "exploratory data analysis",
    "context": "1: Exploratory data analysis is a set of methods with which we try to extract as much information as possible from a data set of high dimension and huge volume. In this paper, we will develop PSO-based algorithms for <mark>exploratory data analysis</mark>.<br>",
    "Arabic": "تحليل البيانات الاستكشافي",
    "Chinese": "探索性数据分析",
    "French": "analyse exploratoire des données",
    "Japanese": "探索的データ分析",
    "Russian": "исследовательский анализ данных"
  },
  {
    "English": "exponential complexity",
    "context": "1: In contrast to the (<mark>exponential complexity</mark>) exact EM algorithm, this clearly demonstrates that the MCEM converges with high probability while only having polynomial computational complexity, and, in this sense, the MCEM meaningfully breaks the curse of dimensionality by using randomness to preserve the monotonic convergence property.<br>",
    "Arabic": "تعقيد أسي",
    "Chinese": "指数复杂度",
    "French": "complexité exponentielle",
    "Japanese": "指数関数的複雑さ",
    "Russian": "экспоненциальная сложность"
  },
  {
    "English": "exponential decay",
    "context": "1: More precisely, given a new event ∆xj of feature j taking place, the predictor can be updated as: \n λ = λδ ∆t + wj∆xj, (6 \n ) \n where λ is the new estimator, λ is the previous one, and δ ∆t is an <mark>exponential decay</mark> with a factor of δ and ∆t time elapsed.<br>2: Meanwhile, the default Chinchilla scaling law [42] predicts loss to continue decreasing as parameters are added, which is in stark contrast to the empirical data. If one wants to incorporate excess parameters hurting performance into the scaling law equations , one could consider ( a ) Modifying the <mark>exponential decay</mark> formulation introduced in Appendix A such that instead of the value of repeated data decaying to 0 it decays to a large negative value ( b ) decaying the exponents α and β in Equation 7 instead of D<br>",
    "Arabic": "تسوس الأسي",
    "Chinese": "指数衰减",
    "French": "décroissance exponentielle",
    "Japanese": "指数減衰",
    "Russian": "экспоненциальное затухание"
  },
  {
    "English": "exponential distribution",
    "context": "1: We write Exp(λ) for the exponential distribution with rate (inverse mean) λ and Gumbel(µ) for the Gumbel distribution with location µ and scale 1. The latter has mean µ + c, where c ≈ 0.5772 is the Euler-Mascheroni constant.<br>",
    "Arabic": "التوزيع الأسي",
    "Chinese": "指数分布",
    "French": "distribution exponentielle",
    "Japanese": "指数分布",
    "Russian": "экспоненциальное распределение"
  },
  {
    "English": "exponential family",
    "context": "1: Consider two members of an <mark>exponential family</mark> with natural parameters, θ 1 and θ 2 , and expectations, μ 1 and μ 2 . Then Thus maximising the likelihood of a data set is equivalent to minimising the associated Bregman divergence between the mean of the distribution and the data. d \n<br>2: minimize θ f ∈K * * f ∈Φ Regret ξ Φ f (σ|θ f ) + E Γ∼ξ log Z Γ (θ) . To recover the predicted behavior for a particular game, we use the same <mark>exponential family</mark> form as before.<br>",
    "Arabic": "عائلة أسية",
    "Chinese": "指数族",
    "French": "famille exponentielle",
    "Japanese": "指数分布族",
    "Russian": "экспоненциальное семейство"
  },
  {
    "English": "exponential loss",
    "context": "1: where C is a constant depending only on the data set. A slightly stronger result would state that the average <mark>exponential loss</mark> when measured with respect to the test set, and not just the empirical set, also converges.<br>2: Efficient Computation in Special Cases. Here we show that when using the <mark>exponential loss</mark>, if the edge γ is very small, then the potentials can be computed efficiently. We first show an intermediate result.<br>",
    "Arabic": "الخسارة الأسية",
    "Chinese": "指数损失",
    "French": "perte exponentielle",
    "Japanese": "指数損失",
    "Russian": "экспоненциальная потеря"
  },
  {
    "English": "exponential map",
    "context": "1: Data The synthetic data trained on consists of a wrapped Gaussian distribution on T n with uniformly chosen random mean and standard deviation of 0.2. Such a distribution is defined by taking the density of a Normal distribution in the tangent space of the manifold at the mean and passing it through the <mark>exponential map</mark> at the mean.<br>2: On the hyperboloid, on the other hand, the <mark>exponential map</mark> is numerically not well-behaved far away from the origin (Dooley and Wildberger, 1993;Al-Mohy and Higham, 2010).<br>",
    "Arabic": "خريطة أسية",
    "Chinese": "指数映射",
    "French": "application exponentielle",
    "Japanese": "指数写像",
    "Russian": "экспоненциальное отображение"
  },
  {
    "English": "exponential move average",
    "context": "1: We sample M s = 64 points along each ray and render the feature image I V at 16 2 pixels. We use an <mark>exponential moving average</mark> [93] with decay 0.999 for the weights of the generator.<br>2: The intuition behind the proposed strategy is to calculate the residual between the <mark>exponential moving average</mark> (EMA) of the update and the current update, which represents the deviation of the approximated update.<br>",
    "Arabic": "المتوسط ​​المتحرك الأسي",
    "Chinese": "指数移动平均",
    "French": "moyenne mobile exponentielle",
    "Japanese": "指数移動平均",
    "Russian": "экспоненциальное скользящее среднее"
  },
  {
    "English": "exposure bias",
    "context": "1: Another direction of attempts is the sentencelevel training with the thinking that the sentencelevel metric, e.g., BLEU, brings a certain degree of flexibility for generation and hence is more robust to mitigate the <mark>exposure bias</mark> problem. To avoid the problem of <mark>exposure bias</mark>, Ranzato et al.<br>2: ; Zhang et al. , 2019 ) . While <mark>exposure bias</mark> has been a point of critique mostly against MLE, it has only been studied in the context of approximate MAP decoding.<br>",
    "Arabic": "تحيز التعرض",
    "Chinese": "暴露偏差",
    "French": "biais d'exposition",
    "Japanese": "暴露バイアス",
    "Russian": "\"смещение экспозиции\""
  },
  {
    "English": "extended Kalman filter",
    "context": "1: We use an <mark>Extended Kalman Filter</mark> (EKF) to estimate the global 6-DoF camera motion over time with its state x ∈ R 6 , which is a minimal representation of the camera pose c with respect to the world frame of reference w, and covariance matrix P x ∈ R 6×6 .<br>",
    "Arabic": "مرشح كالمان الممتد",
    "Chinese": "扩展卡尔曼滤波器",
    "French": "Filtre de Kalman étendu",
    "Japanese": "拡張カルマンフィルター",
    "Russian": "расширенный фильтр Калмана"
  },
  {
    "English": "extensive-form game",
    "context": "1: Specifically, it has been known for more than 20 years that when all players seek to minimize their internal regret in a repeated normal-form game, the empirical frequency of play converges to a normal-form correlated equilibrium. Extensive-form (that is, tree-form) games generalize normal-form games by modeling both sequential and simultaneous moves, as well as private information.<br>",
    "Arabic": "لعبة ذات شكل ممتد",
    "Chinese": "扩展式博弈",
    "French": "jeu sous forme extensive",
    "Japanese": "広形ゲーム",
    "Russian": "игра в развернутой форме"
  },
  {
    "English": "extractive question answer",
    "context": "1: QuAC also adopts <mark>extractive question answering</mark> that restricts the answer as a span of text, which is generally considered easier to evaluate.<br>2: We use the outcomes of this extrinsic task to construct a breakdown detection benchmark for the metrics. We use dialogue state tracking, semantic parsing, and <mark>extractive question answering</mark> as our extrinsic tasks. We evaluate nine metrics consisting of string overlap metrics, embedding-based metrics, and metrics trained using scores from human evaluation of MT.<br>",
    "Arabic": "الإجابة عن الأسئلة الاستخراجية",
    "Chinese": "抽取式问答",
    "French": "réponse à une question extractive",
    "Japanese": "抽出型質問応答",
    "Russian": "извлечение ответов на вопросы"
  },
  {
    "English": "extractive summarization",
    "context": "1: [Lappas et al., 2012] finds a characteristic set of reviews that best mirror the global distribution of sentiments in the corpus. We could employ <mark>extractive summarization</mark> [Barrios et al., 2016] that combines sentences from reviews based on representativeness objective.<br>2: We then apply our method to two complementary tasks: information ordering and <mark>extractive summarization</mark>. Our experiments show that incorporating content models in these applications yields substantial improvement over previously-proposed methods.<br>",
    "Arabic": "تلخيص استخراجي",
    "Chinese": "抽取式摘要化",
    "French": "résumé extractif",
    "Japanese": "抽出的要約",
    "Russian": "извлекающее резюмирование"
  },
  {
    "English": "extractor",
    "context": "1: First, and most importantly, these previous systems learn an <mark>extractor</mark> for each relation of interest, whereas OLLIE is an open <mark>extractor</mark>. OLLIE's strength is its ability to generalize from one relation to many other relations that are expressed in similar forms.<br>2: Note that this pipeline is optimized for a scenario with AI-related texts because the extraction models were initially trained on the SciERC dataset containing only AI-related documents (Luan et al., 2018). To extend this pipeline to other domains, we need to use an <mark>extractor</mark> that can effectively recognize entities and relations tailored for those domains.<br>",
    "Arabic": "مستخرج",
    "Chinese": "提取器",
    "French": "extracteur",
    "Japanese": "抽出器",
    "Russian": "извлекатель"
  },
  {
    "English": "f-divergence",
    "context": "1: The authors show that the f -divergence of the sampling distribution to the target distribution p is a monotonic function of β. In other words, increasing β can only improve (or maintain) the sampling fidelity, although at the cost of lower efficiency due to fewer accepted samples.<br>2: While there are many possible robustness balls B which could provide upper bounds on group risk, we opt to use the chi-squared ball since it is straightforward to optimize (Ben-Tal et al., 2013; and we found it empirically outperformed other f -divergence balls.<br>",
    "Arabic": "التباين f",
    "Chinese": "f-散度",
    "French": "f-divergence",
    "Japanese": "f-ダイバージェンス",
    "Russian": "f-дивергенция"
  },
  {
    "English": "f-measure",
    "context": "1: We ended up with nine grammars which we use for segmentation; one of these (PrStSu2b+Co+SM) was retained not because of its performance on <mark>F-measure</mark>, but on precision, which we only use for the first iteration in cascaded AG (which we will present in Section 6). 4 We now present our nine grammars.<br>2: Second, Z&C has the best precision, although their results are based on 280 test examples only, whereas our results are based on 10-fold cross validation. Third, λ-WASP has the best <mark>F-measure</mark>. To see the relative importance of each component of the λ-WASP algorithm, we performed two ablation studies.<br>",
    "Arabic": "مقياس إف",
    "Chinese": "F-测度",
    "French": "mesure F",
    "Japanese": "F-測定",
    "Russian": "F-мера"
  },
  {
    "English": "f-score",
    "context": "1: This is not surprising, the Discourse ILP model takes the entire document into account, and compression decisions will be slightly more conservative. The Discourse ILP's output is significantly better than McDonald in terms of <mark>F-score</mark>, indicating that discourse-level information is generally helpful.<br>2: With only local features, our forest reranker achieves an <mark>F-score</mark> of 91.25, and with the addition of non-  This improvement might look relatively small, but it is much harder to make a similar progress with n-best reranking.<br>",
    "Arabic": "نسبة إف-سكور",
    "Chinese": "F分数",
    "French": "score F",
    "Japanese": "F値",
    "Russian": "F-мера"
  },
  {
    "English": "f1 measure",
    "context": "1: The experimental evaluation proves that sequential tagging effectively embodies evidence about the contexts and is able to reach a relative increment in detection accuracy of around 20% in <mark>F1 measure</mark>. These results are particularly interesting as the approach is flexible and does not require manually coded resources. ColMustard : Amazing match yesterday!<br>2: For example, the NL treebank contains many multi-word tokens that are typically broken apart by our automatic tokenizer. The NER results, in terms of F 1 measure, are listed in Table 4. Introducing word cluster features for NER reduces relative errors on the test set by 21% (39% on the development set) on average.<br>",
    "Arabic": "قياس F1",
    "Chinese": "F1指标",
    "French": "mesure F1",
    "Japanese": "F1 メジャー",
    "Russian": "мера F1"
  },
  {
    "English": "f1 metric",
    "context": "1: Conversational Question Answering. The conversational question answering task is to evaluate the utilization of world knowledge. As shown in Table 4, our approach also performs well in this task, even slightly outperforming the AR model BART by 0.8 on <mark>F1 metric</mark>. A possible reason is that our approach can make use of the pre-learned world knowledge from BART.<br>",
    "Arabic": "مقياس الإف ١",
    "Chinese": "F1指标",
    "French": "métrique F1",
    "Japanese": "F1指標",
    "Russian": "метрика F1"
  },
  {
    "English": "f1 score",
    "context": "1: According to Table 4, the precision scores of almost all models are greatly improved (more than 7%), in contrast, the recall rates drop very little (less than 1%), which lead to the significant improvement in <mark>F1 score</mark>.<br>2: Moreover, a fine tuned model (Wang et al., 2019) of BERT (Devlin et al., 2018) for document-level RE achieved a higher <mark>F1 score</mark> than the baselines on DocRED.<br>",
    "Arabic": "درجة f1",
    "Chinese": "F1 分数",
    "French": "score F1",
    "Japanese": "F1スコア",
    "Russian": "мера F1"
  },
  {
    "English": "face detection",
    "context": "1: The dynamic pedestrian detector that we built is based on the simple rectangle filters presented by Viola and Jones [14] for the static <mark>face detection</mark> problem. We first extend these filters to act on motion pairs.<br>2: As a result, learning approaches have often yielded detectors that are more robust and accurate than their hand built counterparts for a range of applications, from edge and <mark>face detection</mark> to general purpose object recognition (see e.g., Rowley et al. 1996;Viola and Jones 2004).<br>",
    "Arabic": "كشف الوجه",
    "Chinese": "人脸检测",
    "French": "détection de visage",
    "Japanese": "顔検出",
    "Russian": "обнаружение лиц"
  },
  {
    "English": "face detector",
    "context": "1: The characters are then converted to word embeddings through word2vec (Mikolov et al., 2013) as the final OCR features, where the feature dimension is 300. Face features are extracted by combining face de-tector SSD (Liu et al., 2016) and face recognizer ResNet50 (He et al., 2016). The feature dimension is 512.<br>2: We separately apply a <mark>face detector</mark>, MTCNN (Zhang et al., 2016), and add the results to the pool of detected objects as additional person entities. Finally, we represent each detected bounding box as an entity in the visual knowledge base.<br>",
    "Arabic": "كاشف الوجه",
    "Chinese": "人脸检测器",
    "French": "détecteur de visages",
    "Japanese": "顔検出器",
    "Russian": "детектор лиц"
  },
  {
    "English": "face recognition",
    "context": "1: Standard object classification tasks ignore the impact of impostors that are not represented by any of the object categories. These open sets started getting attention in <mark>face recognition</mark> tasks, where some test exemplars did not appear in the training database and had to be rejected [28].<br>2: This representation disparity forms our definition of unfairness, and has been observed in <mark>face recognition</mark> (Grother et al., 2011), language identification (Blodgett et al., 2016; Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).<br>",
    "Arabic": "التعرف على الوجوه",
    "Chinese": "人脸识别",
    "French": "reconnaissance faciale",
    "Japanese": "顔認識",
    "Russian": "распознавание лиц"
  },
  {
    "English": "facial landmark",
    "context": "1: Set the location loss weight according to the presence of <mark>facial landmarks</mark> in the dataset: higher weights for datasets with <mark>facial landmarks</mark>, and lower weights for datasets without <mark>facial landmarks</mark>. 4.<br>2: Set the location loss weight according to the presence of <mark>facial landmarks</mark>: higher weight for datasets with <mark>facial landmarks</mark>, and lower weight for datasets without <mark>facial landmarks</mark>. 3.<br>",
    "Arabic": "معالم الوجه",
    "Chinese": "人脸关键点",
    "French": "point de repère facial",
    "Japanese": "顔ランドマーク",
    "Russian": "Опорные точки лица"
  },
  {
    "English": "facial recognition",
    "context": "1: In our system, the linking of an entity to an external source (entity linking and <mark>facial recognition</mark>) is limited to entities in Wikipedia and the publicly available background knowledge bases (KBs) provided by LDC (LDC2015E42 and LDC2019E43).<br>2: For example, critical research on datasets for <mark>facial recognition</mark>, analysis, and classification has repeatedly highlighted the lack of diversity in standard benchmark datasets used to evaluate progress [4], even as the technologies are applied in law enforcement contexts that adversely affect underrepresented populations [42].<br>",
    "Arabic": "التعرف على الوجه",
    "Chinese": "人脸识别",
    "French": "reconnaissance faciale",
    "Japanese": "顔認識",
    "Russian": "распознавание лиц"
  },
  {
    "English": "fact verification",
    "context": "1: Determining which facts contradict claims in the existing article is a central topic of work on fact extraction and verification (Thorne et al., 2018). Recently, Schuster et al. (2021) introduced the VITAMIN-C dataset of factual revisions to Wikipedia articles and the task of factually consistent generation.<br>",
    "Arabic": "التحقق من الحقائق",
    "Chinese": "事实验证",
    "French": "vérification des faits",
    "Japanese": "事実検証",
    "Russian": "проверка фактов"
  },
  {
    "English": "factor analysis",
    "context": "1: While the identifiability result for linear ICA (Comon, 1994) proved to be a milestone for the classical theory of <mark>factor analysis</mark>, similar results are in general not obtainable for the nonlinear case and the underlying sources generating the data cannot be identified (Hyvarinen & Pajunen, 1999).<br>",
    "Arabic": "التحليل العاملي",
    "Chinese": "因子分析",
    "French": "analyse factorielle",
    "Japanese": "因子分析",
    "Russian": "факторный анализ"
  },
  {
    "English": "factor graph",
    "context": "1: Our goal is to learn a CCG, which constitutes learning the lexicon and estimating the parameters of both the grammar and the <mark>factor graph</mark>. We define a learning procedure (Section 6) that alternates between expanding the lexicon and updating the parameters.<br>2: One way to think of total influence for <mark>factor graph</mark>s is as a generalization of maximum degree; indeed, if a <mark>factor graph</mark> has maximum degree ∆, it can easily be shown that α ≤ ∆.<br>",
    "Arabic": "رسم العوامل",
    "Chinese": "因子图",
    "French": "graphe de facteurs",
    "Japanese": "因子グラフ",
    "Russian": "факторный граф"
  },
  {
    "English": "factor matrix",
    "context": "1: We adjust the Tucker results of X < > and X < > to t the range since a part of them may not be within the given range. Step 3. Given the Tucker results of X < > for = , .., included in the range, Z T updates <mark>factor matrices</mark> by eciently stitching the Tucker results.<br>2: (2) It needs to iterate over all the <mark>factor matrices</mark> before it can query the gradient oracle at subsequent parameters. When diameter D increases, the total time to finish such round will increase proportionally. (3) DeFacto works with decentralized data and arbitrary graph, achieving decentralization in both application and topology layers.<br>",
    "Arabic": "مصفوفة العوامل",
    "Chinese": "因子矩阵",
    "French": "matrice de facteurs",
    "Japanese": "因子行列",
    "Russian": "матрица факторов"
  },
  {
    "English": "factor of variation",
    "context": "1: If a latent space is sufficiently disentangled, it should be possible to find direction vectors that consistently correspond to individual <mark>factors of variation</mark>.<br>",
    "Arabic": "عامل التباين",
    "Chinese": "变异因素",
    "French": "facteur de variation",
    "Japanese": "変動要因",
    "Russian": "фактор вариации"
  },
  {
    "English": "factorization",
    "context": "1: Therefore we have to compute the membership probabilities in all existing blocks (and in a new block) by evaluating equation ( 15), which looks formally similar to (12), but a <mark>factorization</mark> over blocks is no longer obvious.<br>2: As expected all methods outperform the most-popular baseline clearly on both datasets and all quality measures. Secondly, with reasonable <mark>factorization</mark> dimensions (e.g. 32) all the <mark>factorization</mark> methods outperform the standard MC method. And in total, the factorized personalized MC (FPMC) outperforms all other methods.<br>",
    "Arabic": "التحليل العاملي",
    "Chinese": "因子分解",
    "French": "factorisation",
    "Japanese": "因子分解",
    "Russian": "факторизация"
  },
  {
    "English": "factorization algorithm",
    "context": "1: In this paper we will work with faces and video but the methods Our result is a <mark>factorization algorithm</mark> for 3D nonrigid structure and motion from video that finds 2D correspondences in the course of enforcing 3D geometric invariants.<br>",
    "Arabic": "خوارزمية التجزئة",
    "Chinese": "因子分解算法",
    "French": "algorithme de factorisation",
    "Japanese": "因数分解アルゴリズム",
    "Russian": "алгоритм факторизации"
  },
  {
    "English": "factorization method",
    "context": "1: The 3-D motion segmentation problem has received relatively less attention. Existing approaches include combinations of EM with normalized cuts [8] and <mark>factorization methods</mark> for orthographic and affine cameras [10,11].<br>2: On the other hand, one of the most successful model classes are <mark>factorization methods</mark> (MF) based on matrix or tensor decomposition. The best approaches [3,4] for the 1M$ Netflix challenge 1 are based on this model class.<br>",
    "Arabic": "طريقة تجزئة العوامل",
    "Chinese": "因子分解法",
    "French": "méthode de factorisation",
    "Japanese": "因子分解法",
    "Russian": "метод факторизации"
  },
  {
    "English": "failure probability",
    "context": "1: We also use d F ,Π to denote the joint statistical complexity of the policy class Π and F. For example, when F and Π are finite, we have d F ,Π = O(log |F ||Π| /δ), where δ is a <mark>failure probability</mark>.<br>",
    "Arabic": "احتمالية الفشل",
    "Chinese": "失效概率",
    "French": "probabilité d'échec",
    "Japanese": "失敗確率",
    "Russian": "вероятность отказа"
  },
  {
    "English": "fairness criterion",
    "context": "1: We demonstrate that even in a one-step feedback model, common <mark>fairness criteria</mark> in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not. We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior.<br>",
    "Arabic": "معيار العدالة",
    "Chinese": "公平性准则",
    "French": "critère d'équité",
    "Japanese": "公平性基準",
    "Russian": "критерий справедливости"
  },
  {
    "English": "fairness loss",
    "context": "1: We also vary λ, the coefficient of <mark>fairness loss</mark> sγ (y,y adv ) (h), from 1 to 5000 to study the trade-off between fairness and accuracy (in terms of micro-, macro-, and example-F1). We randomly choose 70% data for training and 30% for testing.<br>",
    "Arabic": "خسارة الإنصاف",
    "Chinese": "公平性损失",
    "French": "perte d'équité",
    "Japanese": "公平性損失",
    "Russian": "потеря справедливости"
  },
  {
    "English": "fairness notion",
    "context": "1: Ideally, a fair model should be able to make decisions independent of sensitive features. Towards this end, different <mark>fairness notions</mark> (Pedreshi, Ruggieri, and Turini 2008;Dwork et al. 2011;Hardt, Price, and Srebro 2016; Chouldechova and Roth 2020) have been proposed.<br>2: Rubchinsky [2010] considered the fair division problem between two agents with both divisible and indivisible items, and introduced three <mark>fairness notions</mark> with computationally efficient algorithms for finding them. All of the works discussed above assumed that divisible items are homogeneous.<br>",
    "Arabic": "مفهوم العدالة",
    "Chinese": "公平概念",
    "French": "notion d'équité",
    "Japanese": "公平性概念",
    "Russian": "концепция справедливости"
  },
  {
    "English": "faithfulness score",
    "context": "1: F summ = 1 |C summ | c∈Csumm F c , F c ∈ {0, 1} \n where C summ is a set of units in the summary and F c is the faithfulness judgment for the unit c. In both protocols, the <mark>faithfulness score</mark> of a system is defined as 1<br>2: Thus, although we observe a slight drop in <mark>faithfulness score</mark> (93.00 → 92.53 from Table 5), retrieval augmentation still brings much improvement over the base model.<br>",
    "Arabic": "درجة الأمانة",
    "Chinese": "忠实度评分",
    "French": "score de fidélité",
    "Japanese": "忠実度スコア",
    "Russian": "степень достоверности"
  },
  {
    "English": "false negative",
    "context": "1: For the best overall system, we sampled errors and manually annotated 1145 query-document pairs from the validation set. For the retriever, we sampled relevant documents not included in the top-100 candidate set and non-relevant documents ranked higher than relevant ones. For the classifier, we sampled false positive and <mark>false negative</mark> errors made in the top-100 candidate set.<br>2: 1 This does not mean that wrongful rejection (i.e., a <mark>false negative</mark>) has no visible manifestation in our model. If a classifier has a higher <mark>false negative</mark> rate in one group than in another, we expect the classifier to increase the disparity between the two groups (under natural assumptions).<br>",
    "Arabic": "نتيجة سلبية خاطئة",
    "Chinese": "漏检",
    "French": "faux négatif",
    "Japanese": "偽陰性",
    "Russian": "ложноотрицательный"
  },
  {
    "English": "false negative rate",
    "context": "1: set of instances in X p − P that are correctly labeled negative ( from the set the negative instances in positive bags ) . Let δ − be the <mark>false negative rate</mark> in positive bags, i.e. the fraction of the instances in X p − P in category (ii).<br>2: The discussion above indicates that shuffling will be most beneficial when the <mark>false negative rate</mark> δ − is initially high without shuffled bags, and that shuffling will not always add additional constraint information. For example, if δ − is initially low, then bags corresponding to \"new\" constraints are likely to be incorrectly labeled.<br>",
    "Arabic": "معدل السلبيات الزائفة",
    "Chinese": "假阴性率",
    "French": "taux de faux négatifs",
    "Japanese": "偽陰性率",
    "Russian": "скорость ложных отрицательных результатов"
  },
  {
    "English": "false positive rate",
    "context": "1: For the testing sequence, we computed the normalized time to detection at 0% <mark>false positive rate</mark>. This <mark>false positive rate</mark> was achieved by raising the threshold for detection so that the detector would not fire before the event started.<br>2: The left plot in Figure 5 compares their accuracy via ROC curves for each feature and metric combination; the numbers in the legend summarize the error in terms of the <mark>false positive rate</mark> once 95% of the true positives are retrieved. ML+raw intensities yields a significant gain over L 2 +raw, while ML+SIFT also gives some improvement.<br>",
    "Arabic": "معدل الإيجابيات الزائفة",
    "Chinese": "虚报率",
    "French": "taux de faux positifs",
    "Japanese": "偽陽性率",
    "Russian": "ложноположительный коэффициент"
  },
  {
    "English": "fanout",
    "context": "1: We represent the recursive structure of communities-withincommunities as a tree Γ, of height H. We shall show that even a simple, perfectly balanced tree of constant <mark>fanout</mark> b is enough to lead to a densification power law, and so we will focus the analysis on this basic model.<br>",
    "Arabic": "شعبة",
    "Chinese": "扇出度",
    "French": "épanouissement",
    "Japanese": "ファンアウト",
    "Russian": "мощность ветвления"
  },
  {
    "English": "fast fourier transform",
    "context": "1: This strategy has the dual benefits of (a) addressing the large scale shape changes first and (b) speeding algorithm convergence. The dominating computation at each iteration is a <mark>Fast Fourier Transform</mark>.<br>",
    "Arabic": "تحويل فورييه السريع",
    "Chinese": "快速傅里叶变换",
    "French": "Transformée de Fourier rapide",
    "Japanese": "高速フーリエ変換",
    "Russian": "быстрое преобразование Фурье"
  },
  {
    "English": "fc layer",
    "context": "1: Unlike VGG-16 used in [32], our ResNet has no hidden <mark>fc layers</mark>. We adopt the idea of \"Networks on Conv feature maps\" (NoC) [33] to address this issue.<br>2: The outputs of this network consist of two sibling <mark>fc layers</mark> for cls and reg, also in a per-class form. This R-CNN network is fine-tuned on the training set using a mini-batch size of 256 in the RoI-centric fashion.<br>",
    "Arabic": "طبقة الاتصال الكامل",
    "Chinese": "全连接层",
    "French": "couche fc",
    "Japanese": "全結合層",
    "Russian": "полносвязный слой"
  },
  {
    "English": "feasible set",
    "context": "1: Tighter initial constraints, e.g., M 1 (µ) 0, are possible as well. The separation algorithm returns a <mark>feasible set</mark> C given by the intersection of halfspaces, and we intersect this with OUTER to obtain a smaller feasible space, i.e. a tighter relaxation.<br>",
    "Arabic": "المجموعة الممكنة",
    "Chinese": "可行集",
    "French": "ensemble réalisable",
    "Japanese": "実行可能集合",
    "Russian": "допустимое множество"
  },
  {
    "English": "feature",
    "context": "1: The second part of eq 1 encodes the local bias, defined as a sum of local energy terms each weighted by a weight λ k . Following the terminology of Conditional Random Fields, we call each such local energy term a <mark>feature</mark>. In this work, these local energy terms are derived from image fragments with thresholds.<br>2: Entity is one level higher than <mark>feature</mark> since the latter is uniquely identified by a (<mark>feature</mark> type, entity) pair. For example, a same ad id can be used by an ad click or view <mark>feature</mark>; similarly, a query term may denote a search <mark>feature</mark>, an organic or sponsored result click <mark>feature</mark>.<br>",
    "Arabic": "السمة",
    "Chinese": "特征",
    "French": "caractéristique",
    "Japanese": "特徴量",
    "Russian": "признак"
  },
  {
    "English": "feature channel",
    "context": "1: In our 2D experiments, we use p=256 planes and c=64 convexes. We use a simple 2D convolutional encoder where each layer downsamples the image by half, and doubles the number of <mark>feature channels</mark>. We use the centers of all pixels as samples. In our 3D experiments, we use p=4, 096 planes and c=256 convexes.<br>",
    "Arabic": "\"قناة الميزة\"",
    "Chinese": "特征通道",
    "French": "canaux de caractéristiques",
    "Japanese": "特徴チャネル",
    "Russian": "канал признаков"
  },
  {
    "English": "feature correspondence",
    "context": "1: Given two 3D scans of the same object (or scene), the goal of PCR is to estimate a six-degree-of-freedom (6-DoF) pose transformation that accurately aligns the two input point clouds. Using pointto-point <mark>feature correspondences</mark> is a popular and robust solution to the PCR problem.<br>2: It applies to either <mark>feature correspondences</mark> or optical flows and includes most of the two-view motion models in computer vision: 2-D translational, similarity, and affine, or 3-D translational, rigid body motions (fundamental matrices), or motions of planar scenes (homographies), as shown in Table 1.<br>",
    "Arabic": "مطابقة السمات",
    "Chinese": "特征对应",
    "French": "correspondance de caractéristiques",
    "Japanese": "特徴対応",
    "Russian": "соответствие признаков"
  },
  {
    "English": "feature count",
    "context": "1: The gradient of the loss can be shown to be the difference between expected <mark>feature counts</mark>f and empirical <mark>feature counts</mark>f of the current episode. Computing the gradient requires solving the soft value iteration algorithm of [33]. We include a projection step to ensure θ 2 ≤ B.<br>",
    "Arabic": "عدد السمات",
    "Chinese": "特征计数",
    "French": "nombre de caractéristiques",
    "Japanese": "特徴量の個数",
    "Russian": "количество характеристик"
  },
  {
    "English": "feature descriptor",
    "context": "1: One of the consolidated findings of modern, (very) deep learning approaches [19,23,36] is that their joint and unified way of learning feature representations together with their classifiers greatly outperforms conventional <mark>feature descriptor</mark> & classifier pipelines, whenever enough training data and computation capabilities are available.<br>",
    "Arabic": "واصف الميزة",
    "Chinese": "特征描述符",
    "French": "descripteur de caractéristiques",
    "Japanese": "特徴記述子",
    "Russian": "дескриптор признака"
  },
  {
    "English": "feature detection",
    "context": "1: The clear difficulty is that most methods normally used in tracking and mapping, such as <mark>feature detection</mark> and matching or whole image alignment, cannot be directly applied to its fundamentally different visual measurement stream. Cook et al.<br>",
    "Arabic": "الكشف عن الميزات",
    "Chinese": "特征检测",
    "French": "détection de caractéristiques",
    "Japanese": "特徴検出",
    "Russian": "обнаружение признаков"
  },
  {
    "English": "feature detector",
    "context": "1: The same filters are applied to each window; each filter can be viewed as a <mark>feature detector</mark> and the overall process can be conceptualized as looking for windows of terms that contain specific features. The features are not specified a priori through feature engineering, but instead are learned automatically when the model is trained.<br>2: In the case of the cars (rear) dataset, there is a significant improvement in performance with the scale-invariant model. This is due to the <mark>feature detector</mark> performing badly on small images (< 150 pixels) and in the pre-scaled case, all were scaled down to 100 pixels. Figure 9 shows the scale-invariant model for this dataset.<br>",
    "Arabic": "مكتشف الميزة",
    "Chinese": "特征检测器",
    "French": "détecteur de caractéristiques",
    "Japanese": "特徴検出器",
    "Russian": "детектор признаков"
  },
  {
    "English": "feature dimension",
    "context": "1: We set Cora as our experimental dataset, which contains 2708 nodes and 5429 edges. The average node degree of Cora is close to 4. According to Equation 6, the upper bound of dropping rate is calculated from the node degree and the <mark>feature dimension</mark>.<br>2: Denote X (l) ∈ R n×d as the input to the (l + 1)-th block and define X (0) = X, where n is the number of nodes and d is the <mark>feature dimension</mark>. For an input X (l) , the (l + 1)-th block works as follows: \n where \n<br>",
    "Arabic": "بُعد السمة",
    "Chinese": "特征维度",
    "French": "dimension des caractéristiques",
    "Japanese": "特徴次元",
    "Russian": "размерность признаков"
  },
  {
    "English": "feature dimensionality",
    "context": "1: The threshold is probed a priori given a desired <mark>feature dimensionality</mark>. But once an optimal threshold is determined, it can be used consistently regardless of the size of training data.<br>2: where B is a norm bound on θ, d is <mark>feature dimensionality</mark>, and t is the episode. The average regret Rt t approaches zero as t grows since the bound is sub-linear in t. Verification of this no-regret property is shown in the experiments.<br>",
    "Arabic": "بُعد السمة",
    "Chinese": "特征维度",
    "French": "dimensionnalité des caractéristiques",
    "Japanese": "特徴次元数",
    "Russian": "размерность признаков"
  },
  {
    "English": "feature embedding",
    "context": "1: To be specific, each node in V, i.e., v i = (h i , Z i ) is represented as a trainable <mark>feature embedding</mark> vector h i = s ai ∈ R da according to its amino acid type a i and a matrix of coordinates Z i ∈ R 3×m consisting of m backbone atoms.<br>",
    "Arabic": "\"تضمين الميزة\"",
    "Chinese": "特征嵌入",
    "French": "Projection de caractéristiques",
    "Japanese": "特徴埋め込み",
    "Russian": "\"вложение признаков\""
  },
  {
    "English": "feature encoder",
    "context": "1: (I) ∈ R C k ×W k ×H k \n where Ω k = {0, . . . , W k −1}×{0, . . . , H k −1} is the corresponding spatial domain. Note that this <mark>feature encoder</mark> does not have to be trained with supervised tasks.<br>2: Features are extracted from the input images using a convolutional network. The <mark>feature encoder</mark> network is applied to both I 1 and I 2 and maps the input images to dense feature maps at a lower resolution.<br>",
    "Arabic": "مشفر الميزات",
    "Chinese": "特征编码器",
    "French": "encodeur de caractéristiques",
    "Japanese": "特徴エンコーダ",
    "Russian": "кодер признаков"
  },
  {
    "English": "feature engineering",
    "context": "1: This poses a combinatorial problem that is currently heavily reliant on manual expertise. Embracing Uncertainty in the MDP Structure of Cancer. Defining states and actions for IRL can be treated similarly to problems of feature representation, feature selection and <mark>feature engineering</mark> in unsupervised and supervised learning.<br>2: Our main contribution is a basic tool for inducing sequential hidden structure in dependency grammars. Most of the recent work in dependency parsing has explored explicit <mark>feature engineering</mark>. In part, this may be attributed to the high cost of using tools such as EM to induce representations.<br>",
    "Arabic": "هندسة الميزات",
    "Chinese": "特征工程",
    "French": "ingénierie des caractéristiques",
    "Japanese": "特徴エンジニアリング",
    "Russian": "инженерия признаков"
  },
  {
    "English": "feature extraction",
    "context": "1: Training the <mark>feature extraction</mark> model end-to-end and fully supervised performs worse, likely due to the small size of the annotated dataset resulting in overfitting. Although this could potentially be circumvented through regularization techniques [De-Vries and Taylor, 2017], the self-supervised methods do not appear to require regularization as they benefit from the full unlabeled dataset.<br>2: More recent approaches (Abercrombie et al., 2019;Abercrombie and Batista-Navarro, 2020a) show the ability of pre-trained transformers such as BERT in capturing domain-specific jargon better for <mark>feature extraction</mark> from debates. A promising new direction at the intersection of Politics and NLP is the inclusion of context such as political party affiliations and engagement in social circles.<br>",
    "Arabic": "استخراج الميزات",
    "Chinese": "特征提取",
    "French": "extraction de caractéristiques",
    "Japanese": "特徴抽出",
    "Russian": "извлечение признаков"
  },
  {
    "English": "feature extractor",
    "context": "1: More formally, we split the <mark>feature extractor</mark> \n f = (f 1 , . . . , f d ) into f = (f L ; f N ) \n where f L and f N are the local and non-local features, respectively.<br>2: Auxiliary tasks are analogously defined tuples (π i , v i , f (•, w i )), i ≥ 1. All policies and critics share the same <mark>feature extractor</mark> but differ in a separate MLP for each π i and v i . The objectives differ in their hyper-parameters, with all hyper-parameters being meta-learned.<br>",
    "Arabic": "مستخرج السمات",
    "Chinese": "特征提取器",
    "French": "extracteur de caractéristiques",
    "Japanese": "特徴抽出器",
    "Russian": "извлекатель признаков"
  },
  {
    "English": "feature function",
    "context": "1: as is standard in structured prediction , we assume the availability of a <mark>feature function</mark> Φ : X × Y → n that computes an n dimensional feature vector for any pair . Search Spaces. Our approach is based on search in the space S o of complete outputs, which we assume to be given.<br>2: where softmax x f (x) log x e f (x) . Proof (sketch). The (negated) primal objective function (Equation 4) is convex in the variables P (A||S) and subject to linear constraints on <mark>feature function</mark> expectation matching, valid probability distributions, and the non-causal influence of future side information.<br>",
    "Arabic": "دالة الميزة",
    "Chinese": "特征函数",
    "French": "fonction de caractéristiques",
    "Japanese": "特徴関数",
    "Russian": "функция признаков"
  },
  {
    "English": "feature hashing",
    "context": "1: In recent years, there has been a flurry of activity around the use of <mark>feature hashing</mark> to reduce RAM cost of large-scale learning.<br>2: 2009), and the feature representation is computed by φ(s) = h(ζ(s)). One nice property of <mark>feature hashing</mark> is that it can keep the inner product unbiased. Since ζ(s) is normalized, we have: \n<br>",
    "Arabic": "تهشير الميزات",
    "Chinese": "特征哈希",
    "French": "hachage de caractéristiques",
    "Japanese": "特徴ハッシュ",
    "Russian": "хэширование признаков"
  },
  {
    "English": "feature hierarchy",
    "context": "1: AlphaHoldem is also better than W/O History Information since historical action information is critical to decision-making in HUNL. Alpha-Holdem obtains the best performance thanks to its effective multidimensional state representation, which encodes historical information and is suitable for ConvNets to learn effective <mark>feature hierarchies</mark>.<br>2: We formulate a complete model to learn the <mark>feature hierarchies</mark> so that graph matching works best: the feature learning and the graph matching model are refined in a single deep architecture that is optimized jointly for consistent results.<br>",
    "Arabic": "التسلسل الهرمي للميزات",
    "Chinese": "特征层次结构",
    "French": "hiérarchie des caractéristiques",
    "Japanese": "特徴階層",
    "Russian": "иерархия признаков"
  },
  {
    "English": "feature map",
    "context": "1: Our detector runs on top of this expanded <mark>feature map</mark> so that it has access to fine grained features. This gives a modest 1% performance increase. Multi-Scale Training. The original YOLO uses an input resolution of 448 × 448. With the addition of anchor boxes we changed the resolution to 416×416.<br>2: Alternatively, C XY can simply be viewed as an embedding of joint distribution P(X, Y ) using joint <mark>feature map</mark> ψ(x, y) := ϕ(x) ⊗ φ(y) (in tensor product RKHS G ⊗ F).<br>",
    "Arabic": "خريطة السمات",
    "Chinese": "特征图",
    "French": "carte des caractéristiques",
    "Japanese": "特徴マップ",
    "Russian": "карта признаков"
  },
  {
    "English": "feature mapping function",
    "context": "1: where w s is the weight vector for the s-th view of target data; φ s is the <mark>feature mapping function</mark> for the target data z [s] of the s-th view; and d s ≥ 0 is a combination coefficient. Recall that there are no labeled data in the target domain.<br>",
    "Arabic": "دالة التضمين",
    "Chinese": "特征映射函数",
    "French": "fonction de mappage des caractéristiques",
    "Japanese": "特徴写像関数",
    "Russian": "функция отображения признаков"
  },
  {
    "English": "feature matching",
    "context": "1: However, due to the limitations of existing 3D keypoint detectors & descriptors, the limited overlap between point clouds and data noise, corre- spondences generated by <mark>feature matching</mark> usually contain outliers, resulting in great challenges to accurate 3D registration. The problem of 3D registration by handling correspondences with outliers has been studied for decades.<br>2: tiplexer that combines the layout information and a subsequent encoder-decoder architecture for obtaining the final image. There are , however , important differences : ( i ) by separating the layout embedding from the appearance embedding , we allow for much more control and freedom to the object selection mechanism , ( ii ) by adding the location attributes as input , we allow for an intuitive and more direct user control , ( iii ) the architecture we employ enables better quality and higher resolution outputs , ( iv ) by adding stochasticity before the masks are created , we are able to generate multiple results per scene graph , ( v ) this effect is amplified by the ability of the users to manipulate the resulting image , by changing the properties of each individual object , ( vi ) we introduce a mask discriminator , which plays a crucial role in generating plausible masks , ( vii ) another novel discriminator captures the appearance encoding in a counterfactual way , and ( viii ) we introduce <mark>feature matching</mark> based on the discriminator network and ( ix ) a perceptual loss term to better capture the appearance of an object , even if the pose or<br>",
    "Arabic": "مطابقة الميزات",
    "Chinese": "特征匹配",
    "French": "correspondance de caractéristiques",
    "Japanese": "特徴マッチング",
    "Russian": "сопоставление признаков"
  },
  {
    "English": "feature matrix",
    "context": "1: When we drop an edge in adjacency matrix, the corresponding two rows for undirected graphs in the message matrix are masked and the topology diversity is decreased by 2. Similarly, when we drop a node, i.e., a row in the <mark>feature matrix</mark>, elements in the corresponding rows of the message matrix are all masked.<br>2: According to Definition 1, when we drop an element of the <mark>feature matrix</mark> X, all corresponding elements in the message matrix are masked and the feature diversity is decreased by 1.<br>",
    "Arabic": "مصفوفة السمات",
    "Chinese": "特征矩阵",
    "French": "matrice de caractéristiques",
    "Japanese": "特徴マトリックス",
    "Russian": "матрица признаков"
  },
  {
    "English": "feature model",
    "context": "1: Although the improvements vary between languages, the addition of word cluster features never has a negative impact on performance. This result has important practical consequences as it allows practitioners to simply plug in word cluster features into their current <mark>feature models</mark>. Given previous work on word clusters for various linguistic structure prediction tasks, these results are not too surprising.<br>",
    "Arabic": "نموذج الميزات",
    "Chinese": "特征模型",
    "French": "modèle de caractéristiques",
    "Japanese": "特徴モデル",
    "Russian": "модель признаков"
  },
  {
    "English": "feature normalization",
    "context": "1: For the critic we have adopted the PatchGan architecture of [10], but removing <mark>feature normalization</mark>. Otherwise, when computing the gradient penalty, the norm of the critic's gradient would be computed with respect to the entire batch and not with respect to each input independently. The model is trained on the EmotioNet dataset [3].<br>",
    "Arabic": "تطبيع السمات",
    "Chinese": "特征归一化",
    "French": "normalisation des caractéristiques",
    "Japanese": "特徴正規化",
    "Russian": "нормализация признаков"
  },
  {
    "English": "feature point",
    "context": "1: The case of <mark>feature points</mark>.<br>2: Real-Time Structure-from-Motion (SfM). Our SfM module is based on the approach by [4], which is highly optimized and runs at 26-30 fps. It takes the green channel of each camera as input and extracts image <mark>feature points</mark> by finding local maxima of a simple feature measure based on average intensities of four subregions.<br>",
    "Arabic": "نقطة معلمة",
    "Chinese": "特征点",
    "French": "point caractéristique",
    "Japanese": "特徴点",
    "Russian": "точка признака"
  },
  {
    "English": "feature pyramid",
    "context": "1: Its design features an efficient in-network <mark>feature pyramid</mark> and use of anchor boxes. It draws on a variety of recent ideas from [22,6,28,20].<br>2: In brief, FPN augments a standard convolutional network with a top-down pathway and lateral connections so the network efficiently constructs a rich, multi-scale <mark>feature pyramid</mark> from a single resolution input image, see Figure 3(a)-(b). Each level of the pyramid can be used for detecting objects at a different scale.<br>",
    "Arabic": "هرم الميزة",
    "Chinese": "特征金字塔",
    "French": "pyramide de caractéristiques",
    "Japanese": "特徴ピラミッド",
    "Russian": "пирамида признаков"
  },
  {
    "English": "feature representation",
    "context": "1: ( s ) . The main idea of our Memory-Augmented MCTS algorithm is to approximate value estimations with the help of a memory, each entry of which contains the <mark>feature representation</mark> and simulation statistics of a particular state.<br>2: By taking the cross-product of all these component feature vectors, we obtain the full <mark>feature representation</mark> for arc h → m as a rank-1 tensor \n φ h ⊗ φ m ⊗ φ h,m ∈ R n×n×d \n Note that elements of this rank-1 tensor include feature combinations that are not part of the feature crossings in φ h→m .<br>",
    "Arabic": "تمثيل الميزات",
    "Chinese": "特征表示",
    "French": "représentation de caractéristiques",
    "Japanese": "特徴表現",
    "Russian": "представление признаков"
  },
  {
    "English": "feature representation learning",
    "context": "1: Some of these works [Kontschieder et al., 2015, Hazimeh et al., 2020 propose using differentiable trees as an output layer in a cascaded neural network for combining <mark>feature representation learning</mark> along with tree ensemble learning for classification.<br>",
    "Arabic": "تعلم تمثيل الميزات",
    "Chinese": "特征表示学习",
    "French": "apprentissage de représentation de caractéristiques",
    "Japanese": "特徴表現学習",
    "Russian": "обучение представлению признаков"
  },
  {
    "English": "feature selection",
    "context": "1: The wrapper method (Kohavi and John, 1997) is a meta-algorithm for <mark>feature selection</mark>, usually based on a validation set. We employ it in a stagewise approach to learning a sequence of templates.<br>2: The <mark>feature selection</mark> process ends by outputting a selected subset of features to a validation procedure [7]. In the context of classification , <mark>feature selection</mark> techniques can be organized into three categories , depending on how they combine the <mark>feature selection</mark> search with the construction of the classification model [ 5 ] : filter methods [ 9,10,11 ] , wrapper methods [ 12,13,14 ] , and hybrid / embedded methods [ 15,16,17 ] : − Filter techniques rely on the intrinsic<br>",
    "Arabic": "اختيار الميزات",
    "Chinese": "特征选择",
    "French": "sélection de caractéristiques",
    "Japanese": "特徴選択",
    "Russian": "отбор признаков"
  },
  {
    "English": "feature selector",
    "context": "1: We have defined a <mark>feature selector</mark> based on statistical discrimination and robustness criteria, focused on low computational time and resources, defining a real alternative to other selection processes. For future work, we aim to make a time-based comparison to traditional features selectors [6,13,14].<br>2: We now validate the importance of learning the local feature-based selection distribution via the <mark>feature selector</mark> S. We first remove S from L2C framework and replace the probability vector ( ) with a binary mask vector ∈ [0, 1] where = 1 if ∈ K (i.e., a mutable feature) and = 0 otherwise.<br>",
    "Arabic": "\"محدد الميزات\"",
    "Chinese": "特征选择器",
    "French": "sélecteur de caractéristiques",
    "Japanese": "特徴選択器",
    "Russian": "селектор признаков"
  },
  {
    "English": "feature set",
    "context": "1: ∆F 1 = F 1 (g f ) − F 1 (g c ) (1) \n where g f and g c are the classifiers with the full and the control <mark>feature set</mark>, respectively.<br>2: Formally, let Φ be the full <mark>feature set</mark> and φ c ⊂ Φ the subset of features belonging to category c (e.g., c = U:TENSE). The importance of category c is then defined as \n f (c) = F 1 (C Φ ) − F 1 (C Φ\\φc ) (2) \n<br>",
    "Arabic": "مجموعة الميزات",
    "Chinese": "特征集",
    "French": "ensemble de caractéristiques",
    "Japanese": "特徴セット (Tokuchō setto)",
    "Russian": "набор признаков"
  },
  {
    "English": "feature space",
    "context": "1: Recall that ad click is a very rare event while carries probably the most important user feedback information. The <mark>feature space</mark> of granular events, on the other hand, has an extremely high dimensionality. It is therefore desirable to use more data for training.<br>2: 2005) to indirectly bridge the <mark>feature space</mark> and the label space (Zhu et al. 2018).<br>",
    "Arabic": "فضاء الميزات",
    "Chinese": "特征空间",
    "French": "espace de caractéristiques",
    "Japanese": "特徴空間",
    "Russian": "пространство признаков"
  },
  {
    "English": "feature template",
    "context": "1: 4 In addition, we implemented two additional baselines, NT-1st (first order) and NT-3rd (third order), corresponding to our model without the tensor component. Features For the arc feature vector φ h→m , we use the same set of <mark>feature templates</mark> as MST v0.5.1.<br>2: The Group Lasso regularizer (Yuan and Lin, 2006) sparsifies groups of feature weights (e.g. <mark>feature templates</mark>), and has been used to speed up test-time prediction by removing entire templates from the model.<br>",
    "Arabic": "قالب السمة",
    "Chinese": "特征模板",
    "French": "gabarit de caractéristiques",
    "Japanese": "特徴テンプレート",
    "Russian": "шаблон признаков"
  },
  {
    "English": "feature vector",
    "context": "1: where φ(x, d) ∈ R l is a l-dimensional <mark>feature vector</mark> (Section 5.3) and the positive portion of the gradient, rather than using expected features, averages over all max-scoring correct derivations.<br>2: where Φ(di, q) (which we define later) is a function that maps documents and queries to a <mark>feature vector</mark>. Intuitively , it can be thought of as a <mark>feature vector</mark> describing the quality of the match between a document di and the query q. w is a weight vector that assigns weights to each of the features in Φ , thus giving us a real valued retrieval function where a higher score indicates a document di is estimated to be more relevant to<br>",
    "Arabic": "ناقلات الميزة",
    "Chinese": "特征向量",
    "French": "vecteur de caractéristiques",
    "Japanese": "特徴ベクトル",
    "Russian": "вектор признаков"
  },
  {
    "English": "feature weight",
    "context": "1: Next, we performed grammar induction with the complete training data of that fold, and used the learned <mark>feature weights</mark> for decoding of the test instances. The averaged results are shown in Table 3.<br>2: L ( σ ) + β • w∈W M S ( w ) /L ( w ) ) \n Here, f σ (S) and f c (S) are respectively the occurrence counts of morphemes and contexts under S, and θ = (λ σ , λ c : σ, c) are their <mark>feature weights</mark>.<br>",
    "Arabic": "وزن السمة",
    "Chinese": "特征权重",
    "French": "poids des traits",
    "Japanese": "特徴量の重み",
    "Russian": "вес признака"
  },
  {
    "English": "featurization",
    "context": "1: For SMP, we use the previously-developed architecture presented in [Gilmer et al., 2017], which is publicly available. We use a very simple <mark>featurization</mark> scheme for atomic systems.<br>",
    "Arabic": "تحويل الميزات",
    "Chinese": "特征提取",
    "French": "caractérisation",
    "Japanese": "特徴化",
    "Russian": "фичеризация"
  },
  {
    "English": "featurized representation",
    "context": "1: Ideally, this <mark>featurized representation</mark> should be of a similar form to the positional encoding features used in NeRF, as Mildenhall et al. show that this feature representation is critical to NeRF's success [30].<br>2: where 1{•} is an indicator function: F(x, •) = 1 iff x is within the conical frustum defined by (o, d,ṙ, t 0 , t 1 ). We must now construct a <mark>featurized representation</mark> of the volume inside this conical frustum.<br>",
    "Arabic": "تمثيل مميز",
    "Chinese": "特征化表示",
    "French": "représentation featurisée",
    "Japanese": "特徴表現",
    "Russian": "признаковое представление"
  },
  {
    "English": "federated Learning",
    "context": "1: AllReduce) [8,9]. This centralized design limits the scalability of learning systems in two aspects. First, in many scenarios, such as <mark>Federated Learning</mark> [10,11] and Internet of Things (IOT) [12], a shuffled dataset or a complete (bipartite) communication graph is not possible or affordable to obtain.<br>2: We present the overview of FS-G in Fig. 1. At the core of FS-G is an event-driven FL framework FederatedScope [38] with fundamental utilities (i.e., framing the FL procedure) and it is compatible with various graph learning backends.<br>",
    "Arabic": "التعلم الاتحادي",
    "Chinese": "联邦学习",
    "French": "Apprentissage fédéré",
    "Japanese": "連合学習 (れんごうがくしゅう)",
    "Russian": "федеративное обучение"
  },
  {
    "English": "feed forward",
    "context": "1: The mean function of the second model is a <mark>feed forward</mark> ReLU network with two hidden layers each with 50 units.<br>",
    "Arabic": "انتشار أمامي",
    "Chinese": "前馈",
    "French": "propagation avant",
    "Japanese": "フィードフォワード",
    "Russian": "Прямое распространение"
  },
  {
    "English": "feed forward network",
    "context": "1: Our construction will use a 5-layer <mark>feed forward network</mark> that takes as input the vectors h t−1 and x t at each timestep and produces the vector h t .<br>2: These methods run with pure <mark>feed forward network</mark> operation at runtime, but rely on geometric and photometric formulation and understanding at training time to correctly for-mulate the loss functions which connect different network components. Other systems are looking towards making consistent long-term maps and for instance combine learned normal predictions with photometric constraints at test time [32].<br>",
    "Arabic": "\"شبكة الانتشار الأمامي\"",
    "Chinese": "前馈网络",
    "French": "réseau de propagation avant",
    "Japanese": "フィードフォワードネットワーク",
    "Russian": "нейронная сеть прямого распространения"
  },
  {
    "English": "feed forward neural network",
    "context": "1: We use LeakyReLU as the activation function throughout our framework to mitigate vanishing gradients (Maas et al., 2013). Following (Veličković et al., 2017) a w ∈ R 2F is the parameter matrix of a single layer layer <mark>feed forward neural network</mark>. The attention coefficients are used to weigh and aggregate contextual information from neighboring nodes.<br>",
    "Arabic": "شبكة عصبية ذات انتشار أمامي",
    "Chinese": "前馈神经网络",
    "French": "réseau de neurones à propagation avant",
    "Japanese": "前向き神経回路網",
    "Russian": "нейронная сеть прямого распространения"
  },
  {
    "English": "feed-forward layer",
    "context": "1: In the online serving stage, it calculates the representation vector for the input query, and applies a crossing layer to calculate the relevance score between each query and document pair. The crossing layer usually adopts simple operators such as cosine similarity or a single <mark>feed-forward layer</mark> to retain a high efficiency. Gao et al.<br>2: The decoder has a similar structure as the encoder except that, in each decoder layer between the self-attention layer and <mark>feed-forward layer</mark>, a multi-head attention layer attends to the output of the encoder. Layer normalization (Ba et al., 2016) is applied to the output of each skip connection.<br>",
    "Arabic": "طبقة التغذية الأمامية",
    "Chinese": "前馈层",
    "French": "couche de rétroaction",
    "Japanese": "順伝播層",
    "Russian": "слой прямого распространения"
  },
  {
    "English": "feedback loop",
    "context": "1: Of notable importance to this work, AIA requires high-risk AI systems that undergo constant updates to ensure that potentially biased outputs due to <mark>feedback loops</mark> are addressed with appropriate mitigation measures (Article 15-3 of [40]). In addition, AIA identifies \"technical robustness\" as a key requirement for high-risk AI systems.<br>2: Their goal is to study the effect of machine learning on interest rates in different groups at an equilibrium, under a static model without feedback. Ensign et al. [2017] consider <mark>feedback loops</mark> in predictive policing, where the police more heavily monitor high crime neighborhoods, thus further increasing the measured number of crimes in those neighborhoods.<br>",
    "Arabic": "حلقة تغذية راجعة",
    "Chinese": "反馈循环",
    "French": "boucle de rétroaction",
    "Japanese": "フィードバックループ",
    "Russian": "обратная связь"
  },
  {
    "English": "few shot learning",
    "context": "1: They should be useful for (semi-)supervised learning of downstream tasks, transfer and <mark>few shot learning</mark> (Bengio et al., 2013;Schölkopf et al., 2012;Peters et al., 2017).<br>",
    "Arabic": "التعلم بضع الصور",
    "Chinese": "少样本学习",
    "French": "apprentissage en quelques exemples",
    "Japanese": "少数ショット学習",
    "Russian": "немногоэкземплярное обучение"
  },
  {
    "English": "few-shot classification",
    "context": "1: For both zero-shot classification and <mark>few-shot classification</mark>, we follow the OpenAI official guide 6 and set temperature=0 to get identical or very similar completions given the same prompt. We generate 20 tokens at maximum for classification because the texts of the candidate classes are usually short.<br>2: Multi-task meta-learning introduces an expectation over task objectives. BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. For a general multi-task formulation, see Appendix D; here we focus on the <mark>few-shot classification</mark> paradigm.<br>",
    "Arabic": "تصنيف قليل العينات",
    "Chinese": "少样本分类",
    "French": "classification à quelques exemples",
    "Japanese": "少数ショット分類",
    "Russian": "классификация по малому количеству примеров"
  },
  {
    "English": "few-shot example",
    "context": "1: On the other hand, adding a task-specific component and adaptation mechanism to the model allows more dramatic improvement in understanding novel tasks from <mark>few-shot examples</mark>, showing the importance of the adaptation mechanism in our task.<br>2: Table 33 indicates that with a larger number of demographically balanced <mark>few-shot examples</mark>, the model predictions become fairer, and the accuracy of GPT models on biased test sets decreases.<br>",
    "Arabic": "أمثلة قليلة",
    "Chinese": "少样本示例",
    "French": "exemple en quelques coups",
    "Japanese": "数ショットの例",
    "Russian": "немногопримерная подача"
  },
  {
    "English": "few-shot fine-tuning",
    "context": "1: In essence, our method is to supervise the model with its own generated samples, in order for it to retain the prior once the <mark>few-shot fine-tuning</mark> begins. This allows it to generate diverse images of the class prior, as well as retain knowledge about the class prior that it can use in conjunction with knowledge about the subject instance.<br>2: In <mark>few-shot fine-tuning</mark>, we additionally train on a small number of instances in the target language. We evaluate two different <mark>few-shot fine-tuning</mark> strategies : ( 1 ) in sequential transfer ( Lauscher et al. , 2020 ; Zhao et al. , 2021 ) , large ( r ) -scale fine-tuning on data from the source language ( s ) -in our case , English , Turkish , or bilingually English and Turkish-is followed by efficient target-language fine-tuning on the few<br>",
    "Arabic": "ضبط دقيق قليل الإشارات",
    "Chinese": "少样本微调",
    "French": "ajustement fin sur peu d'exemples",
    "Japanese": "少数ショット微調整",
    "Russian": "Малоэлементная настройка"
  },
  {
    "English": "few-shot in-context learning",
    "context": "1: We provide additional detail on the approaches described in §5.1. Actionability. As described in §5.1, we measure actionability using: contains_action(R i ), and next_action_coherence(R i ). For contains_action(R i ), our few-shot incontext learning approach proceeds as follows.<br>",
    "Arabic": "التعلم في السياق بعينات قليلة",
    "Chinese": "少样本上下文学习",
    "French": "apprentissage par quelques exemples en contexte",
    "Japanese": "few-shotインコンテキスト学習",
    "Russian": "обучение с небольшим количеством образцов в контексте"
  },
  {
    "English": "few-shot prompting",
    "context": "1: • For <mark>few-shot prompting</mark> with known email domains, GPT-4 shows higher information extraction accuracy than GPT-3.5 for most templates. Moreover, GPT-4 achieves higher information extraction accuracy than GPT-Neo family models under the same template, especially under 5-shot prompting. With more demonstrations, models are more likely to leak training information.<br>2: (2022), but we also generalize this to unscriptable tasks such as date understanding and logical deduction by <mark>few-shot prompting</mark> GPT-3 (text-davinci-003) (Brown et al., 2020) and Codex (Chen et al., 2021) to generate feedbacks and improvements.<br>",
    "Arabic": "الاستدعاء القليل الجرعات",
    "Chinese": "少样本提示",
    "French": "prompts à quelques exemples",
    "Japanese": "少数ショットプロンプト",
    "Russian": "малократное подсказывание"
  },
  {
    "English": "few-shot setting",
    "context": "1: Unlike previously in TT, concentrating on spurious correlations is not vital for ICL robustness and investigating design choices concerned with in-context examples (i.e. the exact <mark>few-shot setting</mark>) promises to be less impactful or mostly dependent on other setup factors. Instead, our findings suggest that the exact phrasing of instruction templates is pivotally important.<br>2: We hypothesize that the trigger location becomes less important for GPT-4 since every demonstration example contains the backdoor trigger. Following [73], when evaluating short testing samples in the <mark>few-shot setting</mark>, we randomly select 32 short training samples as demonstrations.<br>",
    "Arabic": "إعداد القليل من المثالات",
    "Chinese": "小样本设置",
    "French": "configuration few-shot",
    "Japanese": "少数ショット設定",
    "Russian": "настройка небольшого числа обучающих примеров"
  },
  {
    "English": "filter bank",
    "context": "1: The unary potentials used in our implementation are derived from TextonBoost [19,13]. We use the 17-dimensional <mark>filter bank</mark> suggested by Shotton et al. [19], and follow Ladický et al. [13] by adding color, histogram of oriented gradients (HOG), and pixel location features.<br>2: In doing so we follow [14], which obtained improved results in image denoising using the output of a <mark>filter bank</mark> as input to the regressor.<br>",
    "Arabic": "بنك المرشحات",
    "Chinese": "滤波器组",
    "French": "banque de filtres",
    "Japanese": "フィルタバンク",
    "Russian": "банк фильтров"
  },
  {
    "English": "filter weight",
    "context": "1: Consider a layer (convolution or pooling) with input stride s, and a following convolution layer with <mark>filter weights</mark> f ij (eliding the feature dimensions, irrelevant here). Setting the lower layer's input stride to 1 upsamples its output by a factor of s, just like shift-and-stitch.<br>",
    "Arabic": "وزن المرشح",
    "Chinese": "滤波器权重",
    "French": "poids du filtre",
    "Japanese": "フィルタ重み",
    "Russian": "вес фильтра"
  },
  {
    "English": "fine-grained sentiment classification",
    "context": "1: The <mark>fine-grained sentiment classification</mark> task in the Stanford Sentiment Treebank (SST-5; Socher et al., 2013) involves selecting one of five labels (from very negative to very positive) to describe a sentence from a movie review. The sentences contain diverse linguistic phenomena such as idioms and complex syntac-<br>",
    "Arabic": "التصنيف الدقيق للمشاعر",
    "Chinese": "细粒度情感分类",
    "French": "classification fine des sentiments",
    "Japanese": "微細な感情分類",
    "Russian": "тонкоградуированная классификация эмоциональной окраски"
  },
  {
    "English": "fine-tune",
    "context": "1: Here we choose BART (Lewis et al., 2020) as the backbone and <mark>fine-tune</mark> it on the KQA Pro dataset (Cao et al., 2022a). It accepts natural language questions as input, and output the KoPL program in the depth first search order. The KoPL programs are converted to meet the format of sequence generation.<br>2: We then <mark>fine-tune</mark> the resulting model on downstream tasks (e.g., GLUE), using first-order methods. We typically set the number of blocks in the block-diagonal matrices to be between 2 and 4 based on the parameter budgets (25% -50% of the dense model).<br>",
    "Arabic": "ضَبْطٌ دَقِيقٌ",
    "Chinese": "微调",
    "French": "peaufiner",
    "Japanese": "ファインチューニング",
    "Russian": "доводить до совершенства"
  },
  {
    "English": "fine-tune model",
    "context": "1: 13 We expect that the <mark>fine-tuned model</mark> more closely mirrors the style of the corpus, but that the in-context explanations also contain similar content, e.g., relevant entities. Q5: Do supervised explanations help, even with GPT-4? Test: 5-shot GPT-4 vs. Zero-shot GPT-4. Answer: Yes.<br>",
    "Arabic": "نموذج الضبط الدقيق",
    "Chinese": "微调模型",
    "French": "affiner le modèle",
    "Japanese": "微調整モデル",
    "Russian": "доработанная модель"
  },
  {
    "English": "finite horizon",
    "context": "1: We now study the <mark>finite horizon</mark> problem to avoid the issue of needing to make sense of countably many nested stochastic programs. We note that an in<mark>finite horizon</mark> version could be defined through a limiting argument, due to the presence of the discount. However, introducing this will distract the main message in this section.<br>",
    "Arabic": "الأفق المحدود",
    "Chinese": "有限时间跨度",
    "French": "horizon fini",
    "Japanese": "有限時間ホライズン",
    "Russian": "конечный горизонт"
  },
  {
    "English": "finite-state automata",
    "context": "1: In this paper we present a learning algorithm for hidden-state split head-automata grammars (SHAG) (Eisner and Satta, 1999). In this for-malism, head-modifier sequences are generated by a collection of <mark>finite-state automata</mark>.<br>",
    "Arabic": "أتمتة الحالة المحدودة",
    "Chinese": "有限状态自动机",
    "French": "automates à états finis",
    "Japanese": "有限状態オートマトン",
    "Russian": "конечные автоматы"
  },
  {
    "English": "first order method",
    "context": "1: These methods optimize a criterion at the split nodes based on the samples routed to each of the nodes. The second approach considers probabilistic relaxations/decisions at the split nodes and performs end-to-end learning with <mark>first order methods</mark> [Irsoy et al., 2012, Frosst and Hinton, 2017, Lay et al., 2018.<br>",
    "Arabic": "الطريقة من الرتبة الأولى",
    "Chinese": "一阶方法",
    "French": "méthode du premier ordre",
    "Japanese": "一次法",
    "Russian": "метод первого порядка"
  },
  {
    "English": "first-order",
    "context": "1: This could be axiomatized separately, but we do not do so here. This dependence on satisfiability means that the set of instances of this axiom in a <mark>first-order</mark> setting would not be recursively enumerable. This is unfortunately how it must be, however, since the valid sentences are not recursively enumerable either.<br>2: The language L of the situation calculus (McCarthy & Hayes 1969) is <mark>first-order</mark> with equality and many-sorted, with sorts for actions, situations, and objects (everything else). A situation represents a world history as a sequence of actions. The constant S 0 is used to denote the initial situation where no actions have occurred.<br>",
    "Arabic": "\"من الرتبة الأولى\"",
    "Chinese": "一阶",
    "French": "de premier ordre",
    "Japanese": "一階",
    "Russian": "первого порядка"
  },
  {
    "English": "first-order language",
    "context": "1: Π has two languages: C and L. C is an illocutionary-based language for communication. L is a <mark>first-order language</mark> for internal representation -precisely it is a <mark>first-order language</mark> with sentence probabilities optionally attached to each sentence representing Π's epistemic belief in the truth of that sentence.<br>",
    "Arabic": "لغة من الدرجة الأولى",
    "Chinese": "一阶语言",
    "French": "langage du premier ordre",
    "Japanese": "一階述語言語 (ikkai jutugo gengo)",
    "Russian": "язык первого порядка"
  },
  {
    "English": "first-order logic",
    "context": "1: An MLN is a set of tuples (w, f ), where w is a real number representing a weight and f is a formula in <mark>first-order logic</mark>. Let us, for example, consider the following MLN \n<br>2: For example, the classical syllogistic translates to the schematic formulae ∀x(N 1 (x) ⇒ ±N 2 (x)) and ∃x(N 1 (x)∧±N 2 (x)). Let Φ L denote the set of first-order formula templates that can be translated to natural language templates L. Given a language fragment L and a vocabulary V , we can obtain a set of formulae Φ L ( V ) , such that Φ L ( V ) is a fragment of <mark>first-order logic</mark> over the vocabulary V , i.e. , Φ L<br>",
    "Arabic": "منطق الرتبة الأولى",
    "Chinese": "一阶逻辑",
    "French": "logique du premier ordre",
    "Japanese": "1階述語論理",
    "Russian": "логика первого порядка"
  },
  {
    "English": "first-order model",
    "context": "1: Dependency parsers on the other hand do not have a multiplicative grammar factor |G|, and until recently were considered efficient enough for exhaustive inference. However, the increased model complexity of a third-order parser forced  to prune with a <mark>first-order model</mark> in order to make inference practical.<br>2: This decomposition is similar to the <mark>first-order model</mark> of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and grandparent dependencies, as in (Carreras, 2007).<br>",
    "Arabic": "نموذج من الرتبة الأولى",
    "Chinese": "一阶模型",
    "French": "modèle de premier ordre",
    "Japanese": "一次モデル",
    "Russian": "модель первого порядка"
  },
  {
    "English": "first-order parsing",
    "context": "1: For instance, on the English dataset, the low-rank model trained without POS tags achieves 90.49% on <mark>first-order parsing</mark>, while the baseline gets 86.70% if trained under the same conditions, and 90.58% if trained with 12 core POS tags.<br>2: By comparing NT-1st and NT-3rd (models without low-rank) with our full model (with low-rank), we obtain 0.7% absolute improvement on <mark>first-order parsing</mark>, and 0.3% improvement on third-order parsing. Our model also achieves the best UAS on 5 languages. We next focus on the first-order model and gauge the impact of the tensor component.<br>",
    "Arabic": "\"التحليل من الدرجة الأولى\"",
    "Chinese": "一阶解析",
    "French": "analyse de premier ordre",
    "Japanese": "一次構文解析",
    "Russian": "парсинг первого порядка"
  },
  {
    "English": "fisher information matrix",
    "context": "1: In variational inference, an alternative is to use the <mark>Fisher information matrix</mark> of the variational distribution q (i.e., the Hessian of the log of the variational probability density function), which corresponds to using a natural gradient method instead of a (quasi-) Newton method [16,15].<br>2: Besides, Bishop (Bishop 1995) demonstrates the equivalence of corrupted features and L 2 -type regularization. Wager et al. (Wager, Wang, and Liang 2013) show that the dropout regularizer is first-order equivalent to an L 2 regularizer that being applied after scaling the features by an estimate of the inverse diagonal <mark>Fisher information matrix</mark>.<br>",
    "Arabic": "مصفوفة معلومات فيشر",
    "Chinese": "费舍尔信息矩阵",
    "French": "matrice d'information de Fisher",
    "Japanese": "フィッシャー情報行列",
    "Russian": "матрица информации Фишера"
  },
  {
    "English": "fisher score",
    "context": "1: The main idea of using <mark>Fisher score</mark> is to leverage the following decomposition for any x ∈ M log p 0 (x) = log p T (x) − T 0 ∂ t log p t (x)dt.<br>2: Hence, using Lemma L.3, we could estimate jointly the spatial (or Stein) score used in RSGM and the <mark>Fisher score</mark> considered in this section, see Choi et al. (2021).<br>",
    "Arabic": "درجة فيشر",
    "Chinese": "费希尔分数",
    "French": "score de Fisher",
    "Japanese": "フィッシャースコア",
    "Russian": "оценка Фишера"
  },
  {
    "English": "fitness function",
    "context": "1: The <mark>fitness function</mark> is defined as f = 1 1+exp(−γ|w T x|) . To identify multiple components, we use the Gram-Schmidt method as the deflation method.<br>2: We apply standard genetic operators such as two-point crossover and mutation. At the beginning of each generation we perform an elitism step, and each individual is evaluated by means of the <mark>fitness function</mark> in Formula 2: fitness(ind) = suc_rate(i) \n<br>",
    "Arabic": "دالة اللياقة",
    "Chinese": "适应度函数",
    "French": "fonction d'évaluation",
    "Japanese": "適応度関数",
    "Russian": "функция приспособленности"
  },
  {
    "English": "five-fold cross-validation",
    "context": "1: After processing the executables using these three methods, the authors paired each extraction method with a single learning algorithm. Using <mark>five-fold cross-validation</mark>, they used RIPPER (Cohen, 1995) to learn rules from the training set produced by binary profiling. They used naive Bayes to estimate probabilities from the training set produced by the strings command.<br>",
    "Arabic": "\"التحقق المتقاطع الخماسي\"",
    "Chinese": "五折交叉验证",
    "French": "validation croisée à cinq plis",
    "Japanese": "5分割交差検証",
    "Russian": "пятикратная кросс-валидация"
  },
  {
    "English": "fixed point",
    "context": "1: Instead, Q-learning converges to a <mark>fixed point</mark> that gives a \"compromised\" admissible policy which takes a 1 at both s 1 and s 4 (with a value of 0.3;θ ≈ (−0.235, 0.279)). This example shows how delusional bias prevents Q-learning from reaching a reasonable fixed-point.<br>2: decreasing the risk for one group increases the risk for others sufficiently, the <mark>fixed point</mark> is unstable and the model will eventually converge to a different, possibly unfair, <mark>fixed point</mark>. Corollary 1 (Counterexample under symmetry).<br>",
    "Arabic": "نقطة ثابتة",
    "Chinese": "不动点",
    "French": "point fixe",
    "Japanese": "固定点",
    "Russian": "неподвижная точка"
  },
  {
    "English": "fixed-parameter tractable",
    "context": "1: Observation 4.1 BORDA MANIPULATION is fixedparameter tractable with respect to the parameter max 1≤i<x {t i+1 − t i }.<br>2: ) . The binary representation of b thus requires at most r + J bits, and so we can guess it in polynomial time. • We next consider the latter claim. By Theorem 5.4 of Kannan [ 1987 ] , checking satisfiability of C ( r , J ) over Z is fixedparameter tractable in the number n of variables in r-that is , there exists a polynomial p 3 such that a solution to C ( r , J ) can be computed in time O ( ( r + J ) • 2<br>",
    "Arabic": "معلمة ثابتة قابلة للتتبع",
    "Chinese": "固定参数可解",
    "French": "traitable à paramètres fixes",
    "Japanese": "固定パラメータトラクタブル",
    "Russian": "фиксированно-параметризуемый"
  },
  {
    "English": "fixed-point iteration",
    "context": "1: We also discussed the web skeleton W = {r p (H) | p ∈ V }. Computing these partial quantities naively using a <mark>fixed-point iteration</mark> [10] for each p would scale poorly with the number of hub pages.<br>",
    "Arabic": "تكرار النقطة الثابتة",
    "Chinese": "定点迭代",
    "French": "itération du point fixe",
    "Japanese": "固定点反復",
    "Russian": "метод простых итераций"
  },
  {
    "English": "fixpoint",
    "context": "1: Datalog can also be defined as the least <mark>fixpoint</mark> of the inflationary evaluation of Q on I [Abiteboul et al., 1994]. We do not require database instances to have a finite domain, since all of our results are valid in either case.<br>2: On the other hand, Denecker et al. [2015] defined a notion of nesting, which seems promising to integrate the semantics of nested least and nested greatest <mark>fixpoint</mark> definitions. Despite the differences, certain correspondences between the theories show up: several definitions in justification frameworks seem to have an algebraical counterpart in AFT.<br>",
    "Arabic": "نقطة تثبيت",
    "Chinese": "不动点",
    "French": "point fixe",
    "Japanese": "不動点",
    "Russian": "точка неподвижности"
  },
  {
    "English": "float16",
    "context": "1: On iGPT-S, we found small gains in representation quality from using float32 instead of <mark>float16</mark>, from untying the token embedding matrix and the matrix producing token logits, and from zero initializing the matrices producing token and class logits. We applied these settings to all models.<br>",
    "Arabic": "تعويم16",
    "Chinese": "半精度浮点数",
    "French": "flottant16",
    "Japanese": "float16",
    "Russian": "число с плавающей запятой 16-бит"
  },
  {
    "English": "float32",
    "context": "1: • Whilst the forward and backward pass are computed in bfloat16, we store a <mark>float32</mark> copy of the weights in the distributed optimiser state (Rajbhandari et al., 2020). See Lessons Learned from Rae et al. (2021) for additional details.<br>2: On iGPT-S, we found small gains in representation quality from using <mark>float32</mark> instead of float16, from untying the token embedding matrix and the matrix producing token logits, and from zero initializing the matrices producing token and class logits. We applied these settings to all models.<br>",
    "Arabic": "float32",
    "Chinese": "浮点数32",
    "French": "float32",
    "Japanese": "float32",
    "Russian": "float32"
  },
  {
    "English": "flow field",
    "context": "1: Initialization: By default, we initialize the <mark>flow field</mark> to 0 everywhere, but our iterative approach gives us the flexibility to experiment with alternatives.<br>2: Using the relative camera poses between the two views, we compute an initial depth map D pp from the estimated <mark>flow field</mark>, using the Plane-Plus-Parallax (P+P) representation [15,43].<br>",
    "Arabic": "حقل التدفق",
    "Chinese": "流场",
    "French": "champ de flux",
    "Japanese": "流れ場",
    "Russian": "поле потока"
  },
  {
    "English": "flow model",
    "context": "1: For this task, we minimize KL (q(x) p(x)) as the loss function where q is the <mark>flow model</mark> and the target density p(•) can be evaluated. Figure 4 shows that CNF generally achieves lower loss.<br>",
    "Arabic": "نموذج التدفق",
    "Chinese": "流模型",
    "French": "modèle de flux",
    "Japanese": "フローモデル",
    "Russian": "Потоковая модель"
  },
  {
    "English": "focal loss",
    "context": "1: We use <mark>Focal Loss</mark> (Lin et al., 2017) (a weighted version of Cross Entropy Loss) with γ = 2.0, α NotNavigable = 0.75, and α Navigable = 0.25 to handle the class imbalance. Evaluation Data and Procedure. We construct our evaluation data using the validation dataset.<br>2: The <mark>Focal Loss</mark> is designed to address the one-stage object detection scenario in which there is an extreme imbalance between foreground and background classes during training (e.g., 1:1000). We introduce the focal loss starting from the cross entropy (CE) loss for binary classification 1 : \n<br>",
    "Arabic": "الخسارة البؤرية",
    "Chinese": "焦点损失",
    "French": "perte focale",
    "Japanese": "焦点損失",
    "Russian": "фокальная потеря"
  },
  {
    "English": "forall",
    "context": "1: Given the initial concepts, HR uses general production rules to turn one (or two) old concepts into a new one. For instance, the '<mark>forall</mark>' production rule finds objects where a particular relation between its sub-objects is true in every case, eg.<br>",
    "Arabic": "للكل",
    "Chinese": "对于所有",
    "French": "\"forall\"",
    "Japanese": "すべての",
    "Russian": "'для всех'"
  },
  {
    "English": "forecasting",
    "context": "1: Note that the improvement from our dynamic scheduler is orthogonal to the boost from hardware performance Table G. Streaming perception with joint detection, association, and <mark>forecasting</mark> on Tesla V100 (corresponding to Table 2 in the main text). We observe similar boost as in the detection only setting (Table F).<br>2: Note that tracker observations are not used in this experiment since we are only evaluating the performance of <mark>forecasting</mark> and not smoothing. Qualitative results of activity <mark>forecasting</mark> are depicted in Figure 6. Our proposed model is able to leverage the physical scene features and generate a distribution that preserves actor preferences learned during training.<br>",
    "Arabic": "التنبؤ",
    "Chinese": "预测",
    "French": "prévision",
    "Japanese": "予測",
    "Russian": "прогнозирование"
  },
  {
    "English": "foreground segmentation",
    "context": "1: Segmentation is a broad field: there's interactive segmentation [57,109], edge detection [3], super pixelization [85], object proposal generation [2], <mark>foreground segmentation</mark> [94], semantic segmentation [90], instance segmentation [66], panoptic segmentation [59], etc.<br>",
    "Arabic": "تجزئة المقدمة",
    "Chinese": "前景分割",
    "French": "segmentation de premier plan",
    "Japanese": "前景セグメンテーション",
    "Russian": "сегментация переднего плана"
  },
  {
    "English": "forget gate",
    "context": "1: Specifically, the state vector c[t] is a weighted average between the previous state c[t-1] and a linear transformation of the input W x[t]. The weighted aggregation is controlled by a <mark>forget gate</mark> f [t] which is a sigmoid function over the current input and hidden state.<br>2: c t = f t c t−1 + i t ĉ t (5) h t = o t tanh(c t ) (6) \n where σ is the sigmoid function , i t , f t , o t ∈ [ 0 , 1 ] n are input , forget , and output gates respectively , and c t and c t are proposed cell value and true cell value at time t. Note that each of these vectors has a dimension equal to the hidden layer h<br>",
    "Arabic": "بوابة النسيان",
    "Chinese": "遗忘门",
    "French": "porte d'oubli",
    "Japanese": "忘却ゲート",
    "Russian": "Вентиль забывания"
  },
  {
    "English": "forward algorithm",
    "context": "1: We can use the <mark>forward algorithm</mark> to efficiently compute the generation probability assigned to a document by a content model and the Viterbi algorithm to quickly find the most likely contentmodel state sequence to have generated a given document; see Rabiner (1989) for details.<br>2: This is akin to describing the distribution defined by an HMM in terms of a factorization and its corresponding transition and emission parameters, or using the inductive equations of the <mark>forward algorithm</mark>. The operator model representation takes the latter approach. Operator models have had numerous applications.<br>",
    "Arabic": "خوارزمية الأمام",
    "Chinese": "前向算法",
    "French": "algorithme en avant",
    "Japanese": "順方向アルゴリズム",
    "Russian": "алгоритм прямого распространения"
  },
  {
    "English": "forward model",
    "context": "1: We assume unpaired data D, in the form of subject-predicateobject triples, and text T , which may or may not be from the same domain. We also make use of a small (100 samples) set of paired data and text, D pr , T pr . Cycle training makes use of two iteratively trained models , a <mark>forward model</mark> F : D → T and a reverse model R : T → D. Training is unsupervised , namely , we freeze one model and use it to transform one set of inputs , and train the other by using it to predict the original input from the output of the<br>2: algorithms based on the <mark>forward model</mark>'s geometric invariants can be defeated by properties of the SVD that are at odds with the desired factorization, so we identified an additional \"parsimony\" constraint and used it to develop a correction to the SVD ( §4).<br>",
    "Arabic": "النموذج المتقدم",
    "Chinese": "前向模型",
    "French": "modèle direct",
    "Japanese": "前方モデル",
    "Russian": "прямая модель"
  },
  {
    "English": "forward pass",
    "context": "1: a ) end for π θ ( a|s ) = e Q ( s , a ) −V ( s ) Algorithm 2 -Forward pass D ( s initial ) ← 1 for n = 1 , 2 , . . . , N do D ( n ) ( s goal ) ← 0 D ( n+1 ) ( s ) = s , a P s s , a π θ ( a|s ) D ( n ) ( s ) end for D ( s ) = n D ( n ) ( s ) f θ = s f ( s )<br>2: In a single <mark>forward pass</mark>, it computes the Q-values for all the different actions of a given agent, conditioned on the actions of all the other agents. Because a single centralised critic is used for all agents, all Q-values for all agents can be computed in a single batched <mark>forward pass</mark>.<br>",
    "Arabic": "التمرير الأمامي",
    "Chinese": "前向传播",
    "French": "propagation avant",
    "Japanese": "順伝播",
    "Russian": "прямой проход"
  },
  {
    "English": "forward process",
    "context": "1: As introduced in Section 3, discrete diffusion models rely on the probability transition matrix Q t to perform the forward and denoising processes over the state space. To align DDM with the NAR decoding process of BART (Section 4.2), we incorporate the [MASK] token as the absorbing state of the Markov transition matrices. Concretely , at the t-th step of the <mark>forward process</mark> , if token i is not the [ MASK ] token , it has the probabilities of α t and γ t being unchanged and replaced by the [ MASK ] token respectively , leaving the probability of β t = 1 − α t − γ t transiting to other tokens in V<br>2: The reuse is valid, because the marginal distribution of a shorter <mark>forward process</mark> q(x 0 , x τ1 , • • • , x τ K ) at timestep τ k is the same as that of the full-timesteps <mark>forward process</mark> q(x 0:N ) at timestep n = τ k .<br>",
    "Arabic": "العملية الأمامية",
    "Chinese": "前向过程",
    "French": "processus direct",
    "Japanese": "順過程",
    "Russian": "прямой процесс"
  },
  {
    "English": "forward propagation",
    "context": "1: The network is trained using the Gradient Descent, where the <mark>forward propagation</mark> comprises of computing activation matrices for all the layers in the network. Here, the activation matrix for all the layers except the output, is defined as \n A i = relu(U i ), where U i = A i−1 • W i .<br>2: b) Logistic Regression: The iteration for the case of logistic regression is similar to that of linear regression, apart from an activation function being applied on X i • w in the <mark>forward propagation</mark>. We instantiate the activation function using sigmoid (Section V-C). The update function for w is given by \n<br>",
    "Arabic": "نشر أمامي",
    "Chinese": "正向传播",
    "French": "propagation avant",
    "Japanese": "フォワード伝播",
    "Russian": "прямое распространение"
  },
  {
    "English": "forward-backward algorithm",
    "context": "1: We can efficiently computeẐ (θ) for fixed using a generalization of the <mark>forward-backward algorithm</mark> to the lattice of all observations x of length (see Smith and Eisner (2005) for an exposition).<br>2: The density p(x|θ) is easily calculated up to the global constant Z(θ) using the <mark>forward-backward algorithm</mark> (Rabiner, 1989). The partition function is given by \n Z(θ) = x y n i=1 φ(x i , y i )φ(y i−1 , y i ) = x y \n<br>",
    "Arabic": "خوارزمية للأمام والخلف",
    "Chinese": "前向-后向算法",
    "French": "algorithme avant-arrière",
    "Japanese": "前向き後ろ向きアルゴリズム",
    "Russian": "алгоритм вперед-назад"
  },
  {
    "English": "foundation model",
    "context": "1: While much progress has been made on vision and language encoders, computer vision includes a wide range of problems beyond this scope, and for many of these, abundant training data does not exist. In this work, our goal is to build a <mark>foundation model</mark> for image segmentation.<br>2: models that are \"trained on broad data at scale and are adaptable to a wide range of downstream tasks\" [8]. Our work correlates well with this definition, though we note that a <mark>foundation model</mark> for image segmentation is an inherently limited scope, since it represents an important, yet fractional, subset of computer vision.<br>",
    "Arabic": "نموذج الأساس",
    "Chinese": "基础模型",
    "French": "modèle fondamental",
    "Japanese": "基盤モデル",
    "Russian": "модель основы"
  },
  {
    "English": "fouri basis function",
    "context": "1: approximate posterior ( 13 ) that we may freely evaluate anywhere in X . In (13), we obtain an efficient approximator by separately discretizing the prior using <mark>Fourier basis functions</mark> φ i (•) and the update using canonical basis functions k(•, z j ).<br>",
    "Arabic": "وظيفة قاعدة فوريه",
    "Chinese": "\"傅里叶基函数\"",
    "French": "fonction de base de Fourier",
    "Japanese": "フーリエ基底関数",
    "Russian": "базисные функции Фурье"
  },
  {
    "English": "fourier coefficient",
    "context": "1: We now define the <mark>Fourier coefficient</mark> of f at the representation M λ , which we call λ-partial information. Definition II.1 (λ-Partial Information). Given a function f : S n → R + and partition λ.<br>",
    "Arabic": "معامل فورييه",
    "Chinese": "傅立叶系数",
    "French": "coefficient de Fourier",
    "Japanese": "フーリエ係数",
    "Russian": "коэффициент Фурье"
  },
  {
    "English": "fourier feature",
    "context": "1: Since the decision boundary for this problem is not smooth and requires tens of thousands of support vectors, the <mark>Fourier features</mark> perform quite poorly on this dataset.<br>2: It is worth noting any mixture of these features (such as combining partitioning with <mark>Fourier features</mark> or sampling frequencies from mixture models) can be readily computed and applied to learning problems. An exciting direction for future research is to empirically and analytically evaluate random features on other learning tasks.<br>",
    "Arabic": "ميزة فورييه",
    "Chinese": "傅里叶特征",
    "French": "caractéristique de Fourier",
    "Japanese": "フーリエ特徴量",
    "Russian": "Фурье-характеристики"
  },
  {
    "English": "fourier frequency",
    "context": "1: (2020) use spectral filters to generate hierarchical features, showing that the filtered embeddings perform well in different tasks (word-level, sentence-level or document-level), depending on which frequency scales are filtered. In contrast to FNet, they separate <mark>Fourier frequencies</mark>, rather than using the transform to combine features.<br>",
    "Arabic": "ترددات فورييه",
    "Chinese": "傅里叶频率",
    "French": "fréquence de Fourier",
    "Japanese": "フーリエ周波数",
    "Russian": "Фурье-частота"
  },
  {
    "English": "fp",
    "context": "1: 10, we use temporal information by adding training samples generated from perturbing the final estimation of the previous frame. This slows down the processing to 0.3-0.5<mark>fps</mark>, yet is still faster than previous approaches. For other sequences, our algorithm runs at around 3-4 <mark>fps</mark>. Note that our method successfully estimates the deformations.<br>",
    "Arabic": "إطار في الثانية",
    "Chinese": "fps (每秒帧数)",
    "French": "i/s",
    "Japanese": "fps (フレーム毎秒)",
    "Russian": "к/с (кадров в секунду)"
  },
  {
    "English": "fp16",
    "context": "1: reproduced: Results from our running of their scripts. 8 For ECtHR (A), ECtHR (B), and SCOTUS, because there exist some issues when running the <mark>fp16</mark> setting in our environment, we run the code of Chalkidis et al. (2022) by using fp32 instead.<br>",
    "Arabic": "fp16",
    "Chinese": "FP16",
    "French": "fp16",
    "Japanese": "FP16",
    "Russian": "fp16"
  },
  {
    "English": "fp32",
    "context": "1: We train is mixed precision (fp16 and <mark>fp32</mark>). We use all the optimizations that were in Nvidia's BERT implementation in MLPerf 1.1: \n 1. Only compute the prediction scores (last layer) for masked tokens as the outputs of other tokens are not used to compute the masked language modeling loss. 2.<br>",
    "Arabic": "fp32",
    "Chinese": "FP32",
    "French": "fp32",
    "Japanese": "FP32",
    "Russian": "fp32"
  },
  {
    "English": "fractional program",
    "context": "1: The theory of convex LMI relaxations [15] is used in [14] to find global solutions to several optimization problems in multiview geometry, while [5] discusses a direct method for autocalibration using the same techniques. Triangulation and resectioning are solved with a certificate of optimality using convex relaxation techniques for <mark>fractional programs</mark> in [1].<br>",
    "Arabic": "برنامج كسري",
    "Chinese": "分式规划",
    "French": "programme fractionnaire",
    "Japanese": "分数計画問題",
    "Russian": "дробная программа"
  },
  {
    "English": "frame",
    "context": "1: The task of the agent is to drive around the track (i.e. to finish one lap) within 1000 <mark>frame</mark>s. In each <mark>frame</mark>, the agent takes a discrete action: donothing, accelerate, brake, turn-left or turn-right. Each action costs a negative reward of −0.1.<br>",
    "Arabic": "إطار",
    "Chinese": "帧",
    "French": "image",
    "Japanese": "フレーム",
    "Russian": "кадр"
  },
  {
    "English": "free variable",
    "context": "1: extension of p . \n Given a database instance I and a formula ϕ[x] with <mark>free variables</mark> x = x 1 , . . . , x m , the extension of ϕ[x] is the subset of (∆ I ) m containing all those tuples δ 1 , . . . , δ m for which \n<br>2: The set of free index variables of an expression ϕ, denoted by free(ϕ), determines the order of the tensor represented by ϕ. It is defined inductively : free ( 1 x op y ) = free ( E ( x , y ) ) : = { x , y } , free ( P s ( x ) ) = { x } , free ( ϕ 1 • ϕ 2 ) = free ( ϕ 1 + ϕ 2 ) : = free (<br>",
    "Arabic": "متغير حر",
    "Chinese": "自由变量",
    "French": "variable libre",
    "Japanese": "自由変数",
    "Russian": "свободная переменная"
  },
  {
    "English": "frequency penalty",
    "context": "1: In our experiments the temperature was set at zero, so the model would select the single most likely output and other parameters of the model were top = 1, <mark>frequency penalty</mark> 0.5, and presence penalty 0. Table 3 shows how the judgments generated using GPT-4 compare with manual judgments.<br>",
    "Arabic": "عقوبة التردد",
    "Chinese": "频率惩罚",
    "French": "pénalité de fréquence",
    "Japanese": "頻度ペナルティ",
    "Russian": "частотный штраф"
  },
  {
    "English": "frequency vector",
    "context": "1: x i c denotes a sample that consists of only nominal attributes of a sample x i . In particular, each nominal attribute of x i c represents a <mark>frequency vector</mark> in which each element represents the count of its values.<br>2: key ←word count(i) Note that we only need to go over the data set once (step 3 to 5) to obtain the <mark>frequency vector</mark>, word count, and go over the <mark>frequency vector</mark> once to obtain the count vector, y. The estimation is then carried out only on the count vector.<br>",
    "Arabic": "متجه التردد",
    "Chinese": "频率向量",
    "French": "vecteur de fréquence",
    "Japanese": "頻度ベクトル",
    "Russian": "частотный вектор"
  },
  {
    "English": "frequent close itemset",
    "context": "1: Specifically, in [26] the patterns of interest are embedded subtrees. Our proof of Theorem 19 implies that it is #P-hard to count the number of maximal frequent embedded subtrees in a database of labeled trees. The #P-completeness of counting the number of <mark>frequent closed itemsets</mark> [23] also readily follows our complexity results here.<br>",
    "Arabic": "مجموعة عناصر متكررة ومغلقة",
    "Chinese": "频繁闭合项集",
    "French": "ensemble d'éléments à fermeture fréquente",
    "Japanese": "頻繁な閉じたアイテムセット",
    "Russian": "частый замкнутый набор элементов"
  },
  {
    "English": "frequent item set",
    "context": "1: Second, we have different privacy and utility measures. The privacy model of [21] is based on only K-anonymity and does not consider attribute linkages. [26] and [27] aim at minimizing data distortion and preserving <mark>frequent item sets</mark>, respectively, while we aim at preserving classification quality.<br>",
    "Arabic": "مجموعات العناصر المتكررة",
    "Chinese": "频繁项集",
    "French": "Ensemble d'éléments fréquents",
    "Japanese": "頻出アイテムセット",
    "Russian": "часто встречающийся набор элементов"
  },
  {
    "English": "frequent pattern",
    "context": "1: Definition 3 (Pattern Context): Given a dataset D and a <mark>frequent pattern</mark> pα ∈ PD, the context of pα, denoted as c(α), is represented by a selected set of context units Uα ⊆ UD such that every u ∈ Uα co-occurs with pα.<br>2: Let pα be a <mark>frequent pattern</mark>, c(α) be its context model, and D = {t1, ...t l } be a set of transactions, our goal is to select kt transactions Tα ⊆ D with a similarity function s(•, pα), s.t.<br>",
    "Arabic": "نمط متكرر",
    "Chinese": "频繁模式",
    "French": "motif fréquent",
    "Japanese": "頻出パターン",
    "Russian": "частый шаблон"
  },
  {
    "English": "frequent pattern mining",
    "context": "1: We propose algorithms to exploit context modeling and semantic analysis to generate semantic annotations automatically. The context modeling and semantic analysis method we presented is quite general and can deal with any types of frequent patterns with context information. The method can be coupled with any <mark>frequent pattern mining</mark> techniques as a postprocessing step to facilitate interpretation of the discovered patterns.<br>2: First, the framework of our approaches is similar in spirit to that of pattern-growth methods for <mark>frequent pattern mining</mark>. Second, the pruning techniques in our approaches share some interesting similarities with the methods of mining frequent closed itemsets (e.g., [10]).<br>",
    "Arabic": "استخراج الأنماط المتكررة",
    "Chinese": "频繁模式挖掘",
    "French": "fouille de motifs fréquents",
    "Japanese": "頻出パターンマイニング",
    "Russian": "Извлечение частых шаблонов"
  },
  {
    "English": "fully connect graph",
    "context": "1: Here the authors study the case of a <mark>fully connected graph</mark> with Gaussian weights w t (x i , x j ) = 1/(4πt) d/2 exp(−dist(x i − x j ) 2 /4t).<br>2: While this serves as a functional penalty that prevents the occurrence of many classes in the same labelling, it does not accurately model the co-occurrence costs we described earlier. The memory requirements of inference scales badly with the size of a <mark>fully connected graph</mark>.<br>",
    "Arabic": "الرسم البياني المتصل بالكامل",
    "Chinese": "全连接图",
    "French": "graphe entièrement connecté",
    "Japanese": "完全結合グラフ",
    "Russian": "полностью связанный граф"
  },
  {
    "English": "fully connect layer",
    "context": "1: Joint 2D-3D networks. FusionNet [18] combines shape classification scores from a volumetric and a multi-view network, yet this fusion happens at a late stage, after the final <mark>fully connected layer</mark> of these networks, and does not jointly consider their intermediate local and global feature representations.<br>2: To enforce f 1 and f 2 to learn independent features, let the extracted feature of x in some intermediate layer of f be given as g(x) (in this section we use the feature before the last <mark>fully connected layer</mark> as an example).<br>",
    "Arabic": "طبقة متصلة بالكامل",
    "Chinese": "全连接层",
    "French": "couche entièrement connectée",
    "Japanese": "全結合層",
    "Russian": "полностью связанный слой"
  },
  {
    "English": "fully connect neural network",
    "context": "1: The decoder for the other tasks (attribute decoding, contrastive loss) consists of a <mark>fully connected neural network</mark> with 32 units. For the attribute consistency loss, we use the method proposed in [43] to train a 256 unit GRU to approximate non-differentiable programs.<br>",
    "Arabic": "شبكة عصبية متصلة بالكامل",
    "Chinese": "全连接神经网络",
    "French": "réseau de neurones entièrement connecté",
    "Japanese": "全結合ニューラルネットワーク",
    "Russian": "полносвязная нейронная сеть"
  },
  {
    "English": "fully connected network",
    "context": "1: We compare with a CNN-based reactive policy inspired by the state-of-the-art results in [21,20], with 2 CNN layers for image processing, followed by a 3-layer <mark>fully connected network</mark> similar to the VIN reactive policy. Figure 4 shows the performance of the trained policies, measured as the final distance to the target.<br>2: However, we made some important modifications: the attention module selects a 5 × 5 patch of the valueV , centered around the current (discretized) position in the map. The final reactive policy is a 3-layer <mark>fully connected network</mark>, with a 2-dimensional continuous output for the controls.<br>",
    "Arabic": "شبكة متصلة بالكامل",
    "Chinese": "全连接网络",
    "French": "réseau entièrement connecté",
    "Japanese": "全結合ネットワーク",
    "Russian": "полносвязная сеть"
  },
  {
    "English": "fully convolutional network",
    "context": "1: Thus, it is crucial for us to adopt a low-capacity (small) architecture as transfer function trained with a small amount of data, in order to measure transferability conditioned on being highly accessible. We use a shallow <mark>fully convolutional network</mark> and train it with little data (8x to 120x less than task-specific networks).<br>2: In our implementation, we design the discriminator D to be a <mark>fully convolutional network</mark> that outputs w × h dimensional probability map of patches belonging to the fake class, where w × h are the number of local patches in the image.<br>",
    "Arabic": "شبكة تلافيفية بالكامل",
    "Chinese": "全卷积网络",
    "French": "réseau entièrement convolutionnel",
    "Japanese": "完全畳み込みネットワーク",
    "Russian": "полностью сверточная сеть"
  },
  {
    "English": "fully convolutional neural network",
    "context": "1: In addition to geometric-only methods, recent works also adopt deep learning techniques to perform PCR. Some methods aim to detect more repeatable keypoints [4,18] and extract more descriptive features [1,10]. FCGF [10] computes the features in a single pass through a <mark>fully convolutional neural network</mark> without keypoint detection.<br>2: In accelerated MRI, the unsampled frequencies are zero-valued; thus, SENSE produces a zero-filled image. Note, SENSE does not require any training. • U-Net: U-Net is a popular <mark>fully convolutional neural network</mark> baseline for MRI reconstruction [90]. We use the default implementation and hyperparameters used by Desai et al.<br>",
    "Arabic": "الشبكة العصبية التلافيفية الكاملة",
    "Chinese": "全卷积神经网络",
    "French": "réseau neuronal entièrement convolutionnel",
    "Japanese": "フルコンボリューショナルニューラルネットワーク",
    "Russian": "полностью сверточная нейронная сеть"
  },
  {
    "English": "fully-supervise model",
    "context": "1: We showed that our approach substantially improved data-to-text generation performance in low-resource settings, achieved competitive performance compared to <mark>fully-supervised models</mark>, and also improved the faithfulness of the generated text through a reduction in factual errors, hallucinations and information misses, even when compared to fully supervised approaches.<br>",
    "Arabic": "النموذج المشرف بالكامل",
    "Chinese": "全监督模型",
    "French": "modèle entièrement supervisé",
    "Japanese": "完全教師ありモデル",
    "Russian": "полностью обученная модель"
  },
  {
    "English": "function approximation",
    "context": "1: Several very recent papers also attempt to formulate option discovery as an optimization problem with solutions that are compatible with <mark>function approximation</mark>. (Daniel et al. 2016) learn return-optimizing options by treating the termination functions as hidden variables, and using EM to learn them. (Vezhnevets et al.<br>2: (2020) define pessimism by truncating Bellman backups from states with limited support in the data and provide theoretical guarantees for the <mark>function approximation</mark> setting when the behavior distribution µ is known or can be easily estimated from samples, along with proof-of-concept experiments.<br>",
    "Arabic": "\"تقريب الدالة\"",
    "Chinese": "函数逼近",
    "French": "approximation de fonction",
    "Japanese": "関数近似",
    "Russian": "функциональное приближение"
  },
  {
    "English": "function approximator",
    "context": "1: We found alternatives such as weight decay penalty to be less reliable. Double Q residual algorithm loss Off-policy optimization with <mark>function approximators</mark> and bootstrapping faces the notorious issue of deadly triad (Sutton & Barto, 2018).<br>2: Coordinate-based MLPs have become a popular choice as <mark>function approximators</mark> for signals like images, videos and shapes [13,48,55,58,62]. Our work focuses on using MLPs for approximating implicit geometry. Such representations are compact, continuous and differentiable. These advantages are well suited with gradient-descent based optimization and learning problems.<br>",
    "Arabic": "تقريب الوظيفة",
    "Chinese": "函数逼近器",
    "French": "approximateur de fonction",
    "Japanese": "関数近似器",
    "Russian": "аппроксиматор функции"
  },
  {
    "English": "function class",
    "context": "1: Our first guarantee depends on the covering numbers of the <mark>function class</mark> F as we describe in Section 2.2.2. While we state our results abstractly, in the loss minimization setting we typically consider the <mark>function class</mark> F := {ℓ(θ, •) : θ ∈ Θ} parameterized by θ.<br>2: In this paper, we will apply the ε-covering number on both <mark>function class</mark> F and policy class Π. For the <mark>function class</mark>, we use the following metric \n<br>",
    "Arabic": "فئة الدالة",
    "Chinese": "函数类",
    "French": "classe de fonctions",
    "Japanese": "関数クラス",
    "Russian": "класс функций"
  },
  {
    "English": "function space",
    "context": "1: We may paraphrase these remarks to say that knowledge of TF is equivalent to knowledge of T . And while T may be a complicated mapping between surfaces, TF acts linearly between <mark>function space</mark>s. Now suppose that the <mark>function space</mark> of M is equipped with a basis so that any function f : M !<br>2: • By using the Laplace-Beltrami basis for the <mark>function space</mark> on each shape, the map can be well-approximated using a small number of basis functions and expressed simply as a matrix.<br>",
    "Arabic": "فضاء الدوال",
    "Chinese": "函数空间",
    "French": "espace de fonctions",
    "Japanese": "関数空間",
    "Russian": "пространство функций"
  },
  {
    "English": "functionality assertion",
    "context": "1: An ELIHF -ontology is a finite set of concept inclusions (CIs) C ⊑ D role inclusions (RIs) R ⊑ S, and <mark>functionality assertions</mark> func(R) where (here and in what follows) C, D range over ELI concepts and R, S over roles.<br>",
    "Arabic": "تأكيد الوظيفة",
    "Chinese": "功能性断言",
    "French": "assertion de fonctionnalité",
    "Japanese": "機能アサーション",
    "Russian": "утверждение функциональности"
  },
  {
    "English": "fundamental matrix",
    "context": "1: The motion segmentation scheme described in the previous section assumes that the displacement of each object between the two views relative to the camera is nonzero, i.e. T i = 0, otherwise the individual <mark>fundamental matrices</mark> are zero.<br>2: 1), since viewing graphs are used by a class of projective structure-from-motion methods [30,16] that recover the projective cameras starting from multiple <mark>fundamental matrices</mark>. The solvability of a viewing graph should be assessed before addressing the reconstruction, since if a problem is non-solvable, then no method based on <mark>fundamental matrices</mark> only will provide a useful reconstruction.<br>",
    "Arabic": "المصفوفة الأساسية",
    "Chinese": "基础矩阵",
    "French": "matrice fondamentale",
    "Japanese": "基本行列",
    "Russian": "фундаментальная матрица"
  },
  {
    "English": "fusion module",
    "context": "1: Then we insert both pretrained source and target language adapters and an additional <mark>fusion module</mark> within the fine-tuned models to merge the language-specific knowledge and fine-tune with target languages. We conduct our experiments on a multilingual multidomain ToD dataset: GlobalWOZ (Ding et al., 2022).<br>2: Let r n denote the hidden state of the encoder at the position of the n-th special token, {r i } n i=1 = Encoder(x m ). And z n is the output of the <mark>fusion module</mark> corresponding to r n .<br>",
    "Arabic": "وحدة الدمج",
    "Chinese": "融合模块",
    "French": "module de fusion",
    "Japanese": "融合モジュール",
    "Russian": "модуль слияния"
  },
  {
    "English": "fuzzy matching",
    "context": "1: We compute the average amount of tokens present for the original and generated hypothesis and use fuzzy and exact matching to assess the overlap of tokens on average for each dataset. The results can be seen in Table 4.<br>",
    "Arabic": "المطابقة غير الدقيقة",
    "Chinese": "模糊匹配",
    "French": "Appariement flou",
    "Japanese": "あいまい一致",
    "Russian": "нечёткое сопоставление"
  },
  {
    "English": "g-value",
    "context": "1: On the other hand, the number of expanded BDDs is almost always increased (Figure 2b), which is not surprising, as sets of states during the search are not only partitioned by <mark>g-value</mark>, but also by h-value.<br>2: GHSETA * can also be adapted to work with the pathdependent (and thus possibly inconsistent) variant of the operator-potential heuristic that does not require transforming the planning task. We need to allow re-opening states that were previously closed with a higher <mark>g-value</mark>.<br>",
    "Arabic": "القيمة g",
    "Chinese": "g值",
    "French": "valeur g",
    "Japanese": "g値",
    "Russian": "g-значение"
  },
  {
    "English": "game tree",
    "context": "1: Instead of probability vectors, we are interested in realization plans u ∈ R n , v ∈ R m , each a vector of size equal to possible actions throughout the game (thus, n and m are equal to the size of the <mark>game tree</mark>'s action nodes for each player).<br>2: This is the most common situation to be in upon reaching the third betting round, and is also the hardest for AIs to solve because the remaining <mark>game tree</mark> is the largest. Since there is only $500 in the pot but up to $20,000 could be lost, this subgames contains a number of high-penalty mistake actions.<br>",
    "Arabic": "شجرة اللعبة",
    "Chinese": "游戏树",
    "French": "arbre de jeu",
    "Japanese": "ゲーム木",
    "Russian": "игровое дерево"
  },
  {
    "English": "game-theoretic analysis",
    "context": "1: Thus, it would be important to equip domain knowledge and logical reasoning capabilities for language models and safeguard their outputs to make sure they satisfy basic domain knowledge or logic to ensure the trustworthiness of the model outputs, such as retrieval-augmented pretraining [180,179]. • Safeguarding GPT models based on <mark>game-theoretic analysis</mark>.<br>",
    "Arabic": "تحليل نظرية الألعاب",
    "Chinese": "博弈论分析",
    "French": "analyse de la théorie des jeux",
    "Japanese": "ゲーム理論分析",
    "Russian": "теоретико-игровой анализ"
  },
  {
    "English": "gamma distribution",
    "context": "1: We use wj to denote the weight of the j th component for a cell; this is the probability that an arbitrary person in the current cell achieves an education level of j. (αj , βj ) denotes the <mark>Gamma distribution</mark>'s parameters for the j th component. Then, L is: \n<br>2: We learn these distributions over all images that are labeled as positive samples of the object category. Our experiments show that the ratio of pixels that contribute to a specific object in a grid, follows an Exponential distribution. We model the normal relative size (the ratio of object to the whole image) with a <mark>Gamma distribution</mark>.<br>",
    "Arabic": "توزيع جاما",
    "Chinese": "伽马分布",
    "French": "distribution gamma",
    "Japanese": "ガンマ分布",
    "Russian": "гамма-распределение"
  },
  {
    "English": "gate",
    "context": "1: The graphs show that DCs trained on hidden and memory cell states consistently outperform DCs trained on <mark>gates</mark>, and that the memory cell state alone captures nearly the same amount of information as the concatenated hidden and memory cell states. For all components, performance increases when the model has processed more characters.<br>2: We also found that by using <mark>gates</mark>, whether learned or heuristic, gave much lower slot error rates. As an aside, the ability of the SC-LSTM to learn <mark>gates</mark> is also exemplified in Figure 3. Finally, by combining the learned gate approach with the deep architecture (+deep), we obtained the best overall performance.<br>",
    "Arabic": "بوابة",
    "Chinese": "门控机制",
    "French": "porte",
    "Japanese": "ゲート",
    "Russian": "шлюз"
  },
  {
    "English": "gating function",
    "context": "1: In [44], an \"inception\" layer is composed of a shortcut branch and a few deeper branches. Concurrent with our work, \"highway networks\" [42,43] present shortcut connections with <mark>gating functions</mark> [15]. These gates are data-dependent and have parameters, in contrast to our identity shortcuts that are parameter-free.<br>",
    "Arabic": "وظيفة البوابة",
    "Chinese": "门控函数",
    "French": "fonction de déclenchement",
    "Japanese": "ゲーティング関数",
    "Russian": "управляющая функция"
  },
  {
    "English": "gaussian blur",
    "context": "1: We augment this dataset 8X by randomizing the linear contrast, gamma contrast, <mark>Gaussian blur</mark> amount, saturation, additive Gaussian noise, translation, and rotation of each RGB image, applying only the affine component of these same transformations to the associated segmentation masks. Training Objective.<br>",
    "Arabic": "ضبابية غوسية",
    "Chinese": "高斯模糊",
    "French": "flou gaussien",
    "Japanese": "ガウスぼかし",
    "Russian": "гауссовское размытие"
  },
  {
    "English": "gaussian complexity",
    "context": "1: , f (x n )) : f ∈ F } ⊆ R n . The empirical Rademacher and Gaussian complexities of F on x are respectively R (F (x)) and G (F (x)).<br>2: Define the <mark>Gaussian complexity</mark> \n where g i iid ∼ N(0, 1) (here we recall the standard result [2] that <mark>Gaussian complexity</mark> upper bounds Rademacher complexities up to a constant). Now, the set h − h ⋆ such that h ∈ B H is contained in 2B H , which is convex.<br>",
    "Arabic": "تعقيد غاوسي",
    "Chinese": "高斯复杂度",
    "French": "complexité gaussienne",
    "Japanese": "ガウス複雑度",
    "Russian": "гауссовская сложность"
  },
  {
    "English": "gaussian component",
    "context": "1: The data for simulations were synthesized as follows. We initialized the model with two <mark>Gaussian components</mark>, and added new components following a temporal Poisson process (one per 20 phases  on average).<br>2: ∀ i=1,...,K−1 x i : f i (x i ; Θ i ) = f i+1 (x i ; Θ (i+1) ). The advantage of <mark>Gaussian components</mark> is that the boundary points can be found easily, in the same way as in the linear discriminant analysis.<br>",
    "Arabic": "مكون غاوسي",
    "Chinese": "高斯分量",
    "French": "composante gaussienne",
    "Japanese": "ガウス成分",
    "Russian": "Гауссовская компонента"
  },
  {
    "English": "gaussian conditional random field",
    "context": "1: The work in [15] introduced regression tree fields for the task of image restoration, where leaf parameters were learned to parametrize <mark>Gaussian conditional random fields</mark>, providing different types of interaction. In [35], fuzzy decision trees were presented, including a training mechanism similar to back-propagation in neural networks.<br>",
    "Arabic": "حقل عشوائي شرطي غاوسي",
    "Chinese": "高斯条件随机场",
    "French": "champ aléatoire conditionnel gaussien",
    "Japanese": "ガウス条件付きランダム場",
    "Russian": "гауссовское условное случайное поле"
  },
  {
    "English": "gaussian density",
    "context": "1: The view of the variational parameters as outputs is based on the symmetry properties of the <mark>Gaussian density</mark>, f µ,Σ (x) = f x,Σ (µ), which enables the use of the standard forward-backward calculations for linear state space models. The graphical model and its variational approximation are shown in Figure 2.<br>2: The observed data (red) has been corrupted by long-tailed noise formed from a mixture of the <mark>Gaussian density</mark> Ô ×´ Ø × Ø ¾ × Ø µ, and the broad distribution Ô Ð´ Ø µ for the lost component.<br>",
    "Arabic": "الكثافة الجاوسية",
    "Chinese": "高斯密度",
    "French": "densité gaussienne",
    "Japanese": "ガウス密度",
    "Russian": "гауссовская плотность"
  },
  {
    "English": "gaussian distribution",
    "context": "1: Here W ∈ R × , and each element in W follows a <mark>Gaussian distribution</mark>. We generate as: \n (T, X, H) = 1 |E | ∑︁ ∈E ′ ( 1 |N | 2 (T * X )W (T * X ) ⊤ ). (21) \n<br>2: where G is the <mark>Gaussian distribution</mark>, and d p is the p th entry of the vector d, i.e. d p = d(p). So the appearance of each feature in the hypothesis is evaluated under foreground and background densities and the ratio taken.<br>",
    "Arabic": "توزيع جاوسي",
    "Chinese": "高斯分布",
    "French": "distribution gaussienne",
    "Japanese": "正規分布",
    "Russian": "нормальное распределение"
  },
  {
    "English": "gaussian elimination",
    "context": "1: Using the heuristic provided in Golub and Van Loan (1989) (originly derived for <mark>Gaussian elimination</mark>) we normalize (scale) components of the input matrices T i and T i in a way that the rows of the matrix A of Eq. (8) will have approximately the same norm.<br>",
    "Arabic": "الاستبعاد الجاوسي",
    "Chinese": "高斯消元法",
    "French": "élimination gaussienne",
    "Japanese": "ガウス消去法",
    "Russian": "Гауссовская элиминация"
  },
  {
    "English": "gaussian filter",
    "context": "1: e F1/10 steering controller was modi ed to keep track of the lane center using a proportional-derivative-integral (PID) controller. e image pipeline detailed in Fig. 4 [ Le ] is comprised of the following tasks : ( a ) e raw RGB camera image , in which the lane color was identied by its hue and saturation value , is converted to greyscale and subjected to a color lter designed to set the lane color to white and everything else to black , ( b ) e masked image from the previous step is sent through a canny edge detector and then through a logical AND mask whose parameters ensured that the resulting image contains only the information about the path , ( c ) e output from the second step is ltered using a Gaussian lter that reduces noise and is sent through a Hough transformation , resulting in the lane markings<br>",
    "Arabic": "مرشح غاوسي",
    "Chinese": "高斯滤波器",
    "French": "filtre gaussien",
    "Japanese": "ガウシアンフィルタ",
    "Russian": "гауссовский фильтр"
  },
  {
    "English": "gaussian function",
    "context": "1: ω r = 1 qk √ 2π e − (r−1) 2 2q 2 k 2(3) \n which essentially defines the weight ω r to be a value of the <mark>Gaussian function</mark> with argument r, mean 1 and standard deviation qk, where q is a parameter of the algorithm.<br>2: σ 2 t = σ 2 + r i=1 σ 2 i μ σ,i x t ; ψ μσ,i(8) \n where μ σ,i are sigmoid or <mark>Gaussian function</mark> satisfying the identifiability restrictions defined in [4]. This formulation allows the variance to change smoothly between regimes.<br>",
    "Arabic": "الدالة الغاوسية",
    "Chinese": "高斯函数",
    "French": "fonction gaussienne",
    "Japanese": "ガウス関数",
    "Russian": "гауссова функция"
  },
  {
    "English": "gaussian initialization",
    "context": "1: Models were constrained to at most 100K trainable parameters and trained with a simple plateau learning rate scheduler and no regularization. Unconstrained SSMs. We first investigate generic SSMs with various initializations. We consider a random <mark>Gaussian initialization</mark> (with variance scaled down until it did not NaN), and the HiPPO initialization.<br>2: From a <mark>Gaussian initialization</mark> µ n where v i ∼ N (0, 1) and W i ∼ N (0, I N /N ) independently, this converges to 3 /32. We refer the reader to Section 9.4 for the proof. 5.4. Overparametrization in the XOR GMM.<br>",
    "Arabic": "تهيئة جاوسية",
    "Chinese": "高斯初始化",
    "French": "initialisation gaussienne",
    "Japanese": "ガウス初期化",
    "Russian": "гауссовская инициализация"
  },
  {
    "English": "gaussian kernel",
    "context": "1: From a signal processing standpoint, the message passing step can be expressed as a convolution with a <mark>Gaussian kernel</mark> G Λ (m) in feature space: \n Q ( m ) i ( l ) = j∈V k ( m ) ( f i , f j ) Q j ( l ) − Q i ( l ) message passing = [ G Λ ( m ) ⊗ Q ( l ) ] ( f i ) Q ( m ) i ( l ) −Q i ( l )<br>2: We can use higher-order potentials to increase either the geometric invariance of image features, or the expressivity of the models. We describe here a few possible potentials. They are all based on computing a <mark>Gaussian kernel</mark> between appropriate invariant features. Clearly, many other potentials are possible. In this section we will only consider third-order potentials.<br>",
    "Arabic": "نواة جاوسية",
    "Chinese": "高斯核",
    "French": "noyau gaussien",
    "Japanese": "ガウスカーネル",
    "Russian": "гауссовское ядро"
  },
  {
    "English": "gaussian likelihood",
    "context": "1: For example, if M Q generates patterns of fixed length with a specific mean shape and additive Gaussian noise, our likelihood measure (more specifically the negative of the log of the <mark>Gaussian likelihood</mark>, log p(Q |M Q )) simply reduces to the Euclidean distance between Q and Q .<br>2: When combined with a <mark>Gaussian likelihood</mark>, we obtain a Gaussian posterior for a fixed setting of z: \n p(x|y, z) ∝ N (y; Kx, σ 2 I) • N (x; μ x|z , Σ x|z ) ∝ N (x; μ x|y,z , Σ x|y,z ).<br>",
    "Arabic": "الاحتمال الجاوسي",
    "Chinese": "高斯似然",
    "French": "vraisemblance gaussienne",
    "Japanese": "ガウス尤度",
    "Russian": "гауссовское правдоподобие"
  },
  {
    "English": "gaussian matrix",
    "context": "1: Let N be a random <mark>Gaussian matrix</mark> with independent Gaussian entries N (0, σ 2 ), with high probability over the choice of Ω and N , we have 1 + • • • + a 6 r C 6 r, which implis that max a i Cr 1/6 . Proposition E.3.<br>2: We also consider a random diagonal <mark>Gaussian matrix</mark> as a potential structured method; parameterizing A as a diagonal matrix would allow substantial speedups without going through the complexity of S4's NPLR parameterization. We consider both freezing the A matrix and training it. Second, a large generalization gap exists between the initializations.<br>",
    "Arabic": "مصفوفة غاوسية",
    "Chinese": "高斯矩阵",
    "French": "matrice gaussienne",
    "Japanese": "ガウス行列",
    "Russian": "гауссовская матрица"
  },
  {
    "English": "gaussian mixture",
    "context": "1: For instance, the original EP (Minka, 2001) considers <mark>Gaussian mixture</mark> likelihood (or Bernoulli likelihood for classification) and the moments can be directed obtained by the properties of Gaussian (or integration by parts). Besides, at the cost of the tractability, there is no converge guarantee of EP in general.<br>2: Consider a three <mark>Gaussian mixture</mark> on the corners of the simplex, with L 2 loss, retention function ν(r) = exp(−r), and b 1 = b 2 = 50, n (t) = 1000. By construction, (1/3, 1/3, 1/3) is the fair parameter estimate.<br>",
    "Arabic": "مزيج غوسي",
    "Chinese": "高斯混合模型",
    "French": "mélange gaussien",
    "Japanese": "ガウス混合",
    "Russian": "гауссовская смесь"
  },
  {
    "English": "gaussian mixture Model",
    "context": "1: We first trained a <mark>Gaussian Mixture Model</mark> of 20 Gaussians for the frames extracted from the I-love-you sentences. Each frame was then associated with a 20 × 1 log-likelihood vector. We retained the top three values of this vector, zeroing out the other values, to create a frame-level feature representation.<br>2: Then, we use <mark>Gaussian Mixture Model</mark> to build each agent's trajectories, wherex l ∈ R K×T ×5 . We set the prediction time horizon T to 12 (6 seconds) in UniAD. Note that we only take the first two of the last dimension (i.e., x and y) as final output trajectories.<br>",
    "Arabic": "نموذج خليط غاوسي",
    "Chinese": "高斯混合模型",
    "French": "Modèle de mélange gaussien",
    "Japanese": "ガウス混合モデル",
    "Russian": "модель гауссовой смеси"
  },
  {
    "English": "gaussian model",
    "context": "1: step 3. are considered and each pixel is compared with the <mark>Gaussian model</mark> of the road looking for those that are more similar to the model . An example can be seen in Fig.<br>2: We can then use this as an intermediate result to help regress refined <mark>Gaussian model</mark> parameters, in order to obtain a better restored image x (2) , etc., effectively obtaining a cascade of refined models. Note that this is a general approach that is not only applicable to image deblurring.<br>",
    "Arabic": "نموذج جاوسي",
    "Chinese": "高斯模型",
    "French": "modèle gaussien",
    "Japanese": "ガウスモデル",
    "Russian": "Гауссовская модель"
  },
  {
    "English": "gaussian noise",
    "context": "1: We also consider, as an illustrative example, data augmentation procedures which simply add <mark>Gaussian noise</mark> with a prescribed noise covariance matrix Σ n . Under this model, we have \n Σ s = Σ x + Σ n while Σ d = Σ x . Thus in this setting, BYOL learns principal eigenmodes of Σ \n<br>2: where p(r x (i) ) ∼ N (µ, σ 2 ) and q(r \n x (i) , r y (i) ) ∼ N (µ ′ , σ ′2 ) \n where ϵ is a standard <mark>Gaussian noise</mark>   \n 4 E ( x ( i ) , y ( i ) ) ∼B r T x ( i ) r y ( i ) ∥r x ( i ) ∥•∥r y ( i ) ∥ ω ( k ) ∼ ηN 0 , diag ( W 2 r ) + ( 1.0 − η ) N 1 k−1 k−1 i=1 ω ( i<br>",
    "Arabic": "ضوضاء غوسية",
    "Chinese": "高斯噪声",
    "French": "bruit gaussien",
    "Japanese": "ガウス雑音",
    "Russian": "гауссовский шум"
  },
  {
    "English": "gaussian prior",
    "context": "1: Instead, in experiments below, we include a <mark>Gaussian prior</mark> over the number of active relationships. Our learning process outputs an active set of relationships, R, and the parameters of the TAS model for that set, θ R . The algorithm is outlined in Figure 4. Inference with Gibbs Sampling.<br>2: This bias arises because supporting a good solution could involve also supporting many solutions that are unlikely to generate the training data from the prior. As an example, consider a zero-centred <mark>Gaussian prior</mark> on a set of parameters, p(w) = N (0, σ 2 I).<br>",
    "Arabic": "افتراض جاوسي",
    "Chinese": "高斯先验",
    "French": "a priori gaussien",
    "Japanese": "ガウス事前分布",
    "Russian": "гауссовский приор"
  },
  {
    "English": "gaussian process",
    "context": "1: To this end, the suggestion server implements a multi-objective search algorithm to tackle this trade-off. Concretely, we use the Bayesian optimization based on EHVI [9], a widely-used algorithm that maximizes the predicted improvement of hypervolume indicator of Pareto-optimal points relative to a given reference point. The suggestion server then optimizes over the search space following a typical Bayesian optimization as 1 ) Based on the observation history , the server trains multiple surrogates , namely the <mark>Gaussian Process</mark> , to model the relationships between each architecture instance and its objective values ; 2 ) The server randomly samples a number of new instances , and suggests the best one<br>2: We wish to obtain an estimate P θ|X obs of P θ|X obs while minimizing our queries to the oracle. Some smoothness assumptions on the problem are warranted to make the problem tractable. In the Bayesian framework it is standard to assume that the function of interest is a sample from a <mark>Gaussian Process</mark>.<br>",
    "Arabic": "عملية جاوسية",
    "Chinese": "高斯过程",
    "French": "processus gaussien",
    "Japanese": "ガウス過程",
    "Russian": "гауссовский процесс"
  },
  {
    "English": "gaussian process model",
    "context": "1: For the former, much progress has been made in machine learning through kernel methods and Gaussian process (GP) models (Rasmussen & Williams, 2006), where smoothness assumptions about f are encoded through the choice of kernel in a flexible nonparametric fashion.<br>",
    "Arabic": "نموذج عملية غاوسيانية",
    "Chinese": "高斯过程模型",
    "French": "Modèle de processus gaussien",
    "Japanese": "ガウス過程モデル",
    "Russian": "модель гауссовского процесса"
  },
  {
    "English": "gaussian process regression",
    "context": "1: In this section we review kernel embeddings for probability distributions, distribution regression, FastFood, and Gaussian process (GP) regression.<br>2: For our application, this means inferring the voting behavior of men and women separately by electoral district, given aggregate voting information by district. Finally, our work is nonparametric. Kernel embeddings are used to capture all moments of the probability distribution over covariates, and <mark>Gaussian process regression</mark> is used to non-parametrically model the dependence between predictors and labels.<br>",
    "Arabic": "انحدار عملية غاوسية",
    "Chinese": "高斯过程回归",
    "French": "régression par processus gaussien",
    "Japanese": "ガウス過程回帰",
    "Russian": "регрессия гауссовского процесса"
  },
  {
    "English": "gaussian random variable",
    "context": "1: f γ (θ) = E [f (θ + γX)] ,(10) \n where X ∼ N (0, I) is a standard <mark>Gaussian random variable</mark>. The following lemma shows that f γ is both smooth and a good approximation of f . Lemma 1 (Lemma E.3 of [10]).<br>2: In particular, at each epoch of the training loop, we derive the samples c i as \n c i = µ i + Σ i ,(9) \n where is a zero-mean unit variance <mark>Gaussian random variable</mark>. At each iteration, the estimation process thus involves the minimization of the criterion \n<br>",
    "Arabic": "متغير عشوائي جاوسي",
    "Chinese": "高斯随机变量",
    "French": "variable aléatoire gaussienne",
    "Japanese": "ガウスランダム変数",
    "Russian": "гауссовская случайная величина"
  },
  {
    "English": "gaussian smoothing",
    "context": "1: As noted in (Suh et al., 2021;Metz et al., 2021), stochasticity can alleviate some of the high-frequency local minima that deterministic gradients will be stuck on. For instance, the small discontinuity on the right side of Figure 1.B is filtered by <mark>Gaussian smoothing</mark>. • Stochasticity alleviates flat regions.<br>",
    "Arabic": "التنعيم الجاوسي",
    "Chinese": "高斯平滑",
    "French": "lissage gaussien",
    "Japanese": "ガウス平滑化",
    "Russian": "гауссовское сглаживание"
  },
  {
    "English": "gaussian variable",
    "context": "1: Although not directly applicable, we mention them, mainly because of the \"lognormal\" distribution, which is extremely successful in modeling continuous data sets. The lognormal distribution [7] takes positive values, and can be generated as e X where X is a <mark>Gaussian variable</mark>.<br>",
    "Arabic": "متغير غاوسي",
    "Chinese": "高斯变量",
    "French": "variable gaussienne",
    "Japanese": "ガウス変数",
    "Russian": "гауссовская переменная"
  },
  {
    "English": "gaussian weight",
    "context": "1: Here the authors study the case of a fully connected graph with <mark>Gaussian weights</mark> w t (x i , x j ) = 1/(4πt) d/2 exp(−dist(x i − x j ) 2 /4t).<br>2: from an input size of 512 × 512 into a 64 × 64 feature space vector that matches the size of Stable Diffusion. In particular , we use a tiny network E ( • ) of four convolution layers with 4 × 4 kernels and 2 × 2 strides ( activated by ReLU , using 16 , 32 , 64 , 128 , channels respectively , initialized with <mark>Gaussian weights</mark> and trained jointly with the full model ) to encode an image-space condition c i into a<br>",
    "Arabic": "وزن غاوسي",
    "Chinese": "高斯权重",
    "French": "poids gaussiens",
    "Japanese": "ガウス重み",
    "Russian": "гауссовский вес"
  },
  {
    "English": "gene ontology",
    "context": "1: In some scenarios, we have a set of well accepted candidate labels (e.g., the <mark>Gene Ontology</mark> entries for biological topics). However, in most cases, we do not have such a candidate set.<br>",
    "Arabic": "علم مصطلحات الجينات",
    "Chinese": "基因本体论",
    "French": "ontologie des gènes",
    "Japanese": "遺伝子オントロジー",
    "Russian": "Онтология генов"
  },
  {
    "English": "generalisation",
    "context": "1: We start from the position that the correct correspondences are, by definition, those that build the 'best' model. We define the 'best' model as that with optimal compactness, specificity and <mark>generalisation</mark> ability.<br>2: Interestingly, we find cognitively plausible <mark>generalisation</mark> behaviour through learnt representations which echo recent rule-based models.<br>",
    "Arabic": "تعميم",
    "Chinese": "泛化能力",
    "French": "généralisation",
    "Japanese": "一般化",
    "Russian": "обобщение"
  },
  {
    "English": "generalization",
    "context": "1: We do not fine-tune any hyperparameter to fit the specific task. Results on Table 4, 5 and 6 are all under the same hyperparameters, which demonstrates the <mark>generalization</mark> ability of our framework.<br>2: Support vector machines (SVM) have been used in this work as the learning algorithm given that this was the best option when studying a slightly different discrimination problem involving drugs with or without antibacterial activity [5]. SVMs are very well-known, flexible and robust learning models that allow for very good classification and <mark>generalization</mark> [8,3].<br>",
    "Arabic": "التعميم",
    "Chinese": "泛化",
    "French": "généralisation",
    "Japanese": "一般化能力",
    "Russian": "обобщение"
  },
  {
    "English": "generalization ability",
    "context": "1: One possible way to deal with the problem is to design an id-wise scaling strategy, which may not be computational-efficient. Another possibility lies in the loss of <mark>generalization ability</mark> of models trained at a large batch, as found and discussed in CV and NLP areas.<br>2: This measure characterizes the computational capabilities of a network as a trade-off between the so-called kernel quality and the <mark>generalization ability</mark>. We show that for highly connected reservoirs with a low quantization level the region of an efficient trade-off implying high performance is narrow. For sparser networks this region is shown to broaden.<br>",
    "Arabic": "القدرة على التعميم",
    "Chinese": "泛化能力",
    "French": "capacité de généralisation",
    "Japanese": "汎化能力",
    "Russian": "способность к обобщению"
  },
  {
    "English": "generalization bind",
    "context": "1: We also give an interpretation of our result as an improved <mark>generalization bound</mark> for model classes consisting of smooth functions.<br>2: The first term on the right hand side of the <mark>generalization bound</mark> will be 1, that is the bound predicts that the learned classifier might not generalize. This fact remains true for any weighting scheme. In this example, the best weighting scheme classifies with a margin at least θ, at most<br>",
    "Arabic": "حدود التعميم",
    "Chinese": "泛化界限",
    "French": "borne de généralisation",
    "Japanese": "一般化限界",
    "Russian": "обобщающая граница"
  },
  {
    "English": "generalization error",
    "context": "1: We first upper bound the <mark>generalization error</mark> by the regret of actions a (1:T ) on the cost differences \n c (t) − c (t) .<br>2: Double descent is typically presented by plotting the absolute <mark>generalization error</mark> as a function of the number of parameters used in the learning model, although Poggio et al. (2019) and Liao et al. (2020) showed that the behavior of <mark>generalization error</mark> is merely an artifact of the phase transitions in the spectral properties of random matrices.<br>",
    "Arabic": "خطأ التعميم",
    "Chinese": "泛化误差",
    "French": "erreur de généralisation",
    "Japanese": "汎化誤差",
    "Russian": "ошибка обобщения"
  },
  {
    "English": "generalization gap",
    "context": "1: Traditional methods seek to narrow the <mark>generalization gap</mark> by carefully tuning hyperparameters, such as learning rate, momentum, and label smoothing, to narrow the <mark>generalization gap</mark> (Goyal et al., 2017a;Shallue et al., 2018;You et al., 2017b).<br>2: We study what complexity control is necessary to achieve selectivity. As we will see, the typical practice of regularizing to reduce the <mark>generalization gap</mark> (difference between training and test task accuracy) is insufficient if one is interested in selectivity. Rank/hidden dimensionality constraint.<br>",
    "Arabic": "الفجوة التعميمية",
    "Chinese": "泛化差距",
    "French": "écart de généralisation",
    "Japanese": "一般化ギャップ",
    "Russian": "разрыв в обобщении"
  },
  {
    "English": "generalization guarantee",
    "context": "1: From a theoretical point of view we prove that our approach learns a classifier with low training error, and we derive <mark>generalization guarantees</mark> that ensure that its error on new examples is bounded. Furthermore we derive a new lower bound on the number of triplets that are necessary to ensure useful predictions.<br>2: We derive theoretical <mark>generalization guarantees</mark> and a lower bound on the number of necessary triplets, and we empirically show that our method is both competitive with state of the art approaches and resistant to noise.<br>",
    "Arabic": "ضمان التعميم",
    "Chinese": "泛化保证",
    "French": "garantie de généralisation",
    "Japanese": "一般化保証",
    "Russian": "генерализационная гарантия"
  },
  {
    "English": "generalization performance",
    "context": "1: A direct consequence of this behaviour is that two models may have the same <mark>generalization performance</mark> but very different values of the marginal likelihood; or worse, the marginal likelihood might favor a model with a poor <mark>generalization performance</mark>.<br>2: Given a set of learning algorithms A and a limited amount of training data D = {(x1, y1), . . . , (xn, yn)}, the goal of model selection is to determine the algorithm A * ∈ A with optimal <mark>generalization performance</mark>.<br>",
    "Arabic": "أداء التعميم",
    "Chinese": "泛化性能",
    "French": "performance de généralisation",
    "Japanese": "一般化性能",
    "Russian": "обобщающая способность"
  },
  {
    "English": "generalized eigenvector",
    "context": "1: We solve the following generalized eigenvalue problem: \n C t (X) C t (X)V = N(C t )V Λ, (2 \n ) \n where V is the matrix that has <mark>generalized eigenvectors</mark> v 1 , v 2 , . . .<br>2: 3: Compute top N <mark>generalized eigenvectors</mark> α i using LKLα i = ω i Lα i (ω i ∈ R and α i ∈ R m ). 4: Denote A = (α 1 , . . . , α N ), Ω = diag(ω 1 , . . . , ω N ) \n and \n<br>",
    "Arabic": "المتجهات الذاتية المعممة",
    "Chinese": "广义特征向量",
    "French": "vecteur propre généralisé",
    "Japanese": "一般化固有ベクトル",
    "Russian": "обобщенный собственный вектор"
  },
  {
    "English": "generalized linear mixed model",
    "context": "1: We fit <mark>generalized linear mixed models</mark> using the lme4 (Bates et al., 2015)  PoTeC FPReg 1 if a regression was initiated in the first-pass reading of the word, otherwise 0 (sign(RPD exc)) \n negation of FPF 1 if the word was fixated in the first-pass, otherwise 0 \n<br>2: To test for statistical significance of our results, we use <mark>generalized linear mixed models</mark> (GLMMs) to account for potential confounders and statistical dependencies in our data by jointly modeling numerous main effects (e.g., the impact of model family) and interaction effects (e.g., the joint impact of model family and prompting method).<br>",
    "Arabic": "النموذج الخطي المختلط المعمم",
    "Chinese": "广义线性混合模型",
    "French": "modèle linéaire mixte généralisé",
    "Japanese": "一般化線形混合モデル",
    "Russian": "обобщенная линейная смешанная модель"
  },
  {
    "English": "generalized linear model",
    "context": "1: Poisson regression stems from the <mark>generalized linear model</mark> (GLM) framework for modeling a response variable in the exponential family of distributions. In general, GLM uses a link function to provide the relationship between the linear predictors, x and the conditional mean of the density function:  \n<br>2: that is, the risk functional has strictly positive definite Hessian at θ ⋆ , which is thus unique. Additionally, we have the following smoothness assumptions on the loss function, which are satisfied by common loss functions, including the negative log-likelihood for any exponential family or <mark>generalized linear model</mark> [29].<br>",
    "Arabic": "النموذج الخطي المعمم",
    "Chinese": "广义线性模型 (GLM)",
    "French": "modèle linéaire généralisé",
    "Japanese": "一般化線形モデル",
    "Russian": "обобщенная линейная модель (GLM)"
  },
  {
    "English": "generation model",
    "context": "1: Accessorization An interesting capability stemming from the strong compositional prior of the <mark>generation model</mark> is the ability to accessorize subjects. In Figure 16 we show examples of accessorization of a Chow Chow dog. We prompt the model with a sentence of the form: \"a [V] [class noun] wearing [accessory]\".<br>2: In this paper, we present the first study for story author-style transfer and analyze the difficulties of this task. Accordingly, we propose a novel <mark>generation model</mark>, which explicitly disentangles the style information from high-level text representations to improve the style transfer accuracy, and achieve better content preservation by injecting style-specific contents.<br>",
    "Arabic": "نموذج التوليد",
    "Chinese": "生成模型",
    "French": "modèle de génération",
    "Japanese": "生成モデル",
    "Russian": "модель генерации"
  },
  {
    "English": "generative",
    "context": "1: As discussed above, a standard <mark>generative</mark> halfquadratic approach updates each z jc only based on the local clique of the current estimate of the restored image (cf . Eq. ( 5)).<br>2: A future line of work is how to disentangle such concepts and improve <mark>generative</mark> quality. Last but not least are semantic segmentation works in computer vision.<br>",
    "Arabic": "توليدي",
    "Chinese": "生成的",
    "French": "génératif",
    "Japanese": "生成的",
    "Russian": "генеративный"
  },
  {
    "English": "generative Model",
    "context": "1: The PYP prior is particularly well-suited for multi-reward function IRL applications where the set of expert-demonstrations generated by the various ground-truth reward functions may not follow a uniform distribution. The purpose of extending the IRL to use this stochastic process is to control the powerlaw property via the discount parameter which can induce a long-tail phenomena of a distribution. <mark>Generative Model</mark>.<br>",
    "Arabic": "النموذج التوليدي",
    "Chinese": "生成模型",
    "French": "modèle génératif",
    "Japanese": "生成モデル",
    "Russian": "генеративная модель"
  },
  {
    "English": "generative adversarial network",
    "context": "1: Joint3EE [17] is a multi-task model that performs entity recognition, trigger detection and argument role assignment by shared Bi-GRU hidden representations. GAIL-ELMO [18] is an ELMobased model that utilizes <mark>generative adversarial network</mark> to focus on harder-to-detect events. PLMEE [19] is a BERTbased pipeline event extraction method and employs event classification depending on trigger.<br>",
    "Arabic": "شبكة خصومية تولّدية",
    "Chinese": "生成对抗网络",
    "French": "réseau antagoniste génératif",
    "Japanese": "生成対抗ネットワーク (せいせいたいこうねっとわーく)",
    "Russian": "генеративно-состязательная сеть"
  },
  {
    "English": "generative approach",
    "context": "1: One possible interpretation is that a <mark>generative approach</mark> naturally favors decoding into the most prominent sense, as the generated entity title will be most similar to the mention text. On the other hand, classificationbased approaches are not influenced by string similarity of entity and mention text, potentially leading to better performance here.<br>2: Despite the difference, the content model accuracies for our implementation are quite close to that from the original. For the entity grid model, we follow the <mark>generative approach</mark> proposed by Lapata and Barzilay (2005).<br>",
    "Arabic": "المقاربة التوليدية",
    "Chinese": "生成式方法",
    "French": "approche générative",
    "Japanese": "生成的アプローチ",
    "Russian": "генеративный подход"
  },
  {
    "English": "generative network",
    "context": "1: Recently, deep learning-based manifold models were introduced [31]- [33] to further improve the performance; these schemes learn a deep <mark>generative network</mark> and its latent variables directly from the measured k-space data using a non-probabilistic formulation.<br>",
    "Arabic": "شبكة توليدية",
    "Chinese": "生成网络",
    "French": "réseau génératif",
    "Japanese": "ジェネレーティブネットワーク",
    "Russian": "сеть порождающая"
  },
  {
    "English": "generative parser",
    "context": "1: We call this problem the constrained multiset multicover (CMM) problem, and present an algorithm to approximate it. We experiment (Section 5) with the WSJ Pen-nTreebank (Marcus et al., 1994) and Collins' <mark>generative parser</mark> (Collins, 1999), as in previous work. We show that PBS algorithms achieve good results in terms of both the traditional TC measure ( significantly better than the random selection baseline and similar to the results of the state of the art tree entropy ( TE ) method of ( Hwa , 2004 ) ) and our novel cognitively driven measures ( where PBS algorithms significantly outperform both TE and the<br>2: It should be noted that discriminative reranking parsers such as (Charniak and Johnson, 2005) and (Huang, 2008) are constructed on a <mark>generative parser</mark>.<br>",
    "Arabic": "محلل توليدي",
    "Chinese": "生成式解析器",
    "French": "analyseur génératif",
    "Japanese": "生成パーサー",
    "Russian": "порождающий парсер"
  },
  {
    "English": "generative pre-training",
    "context": "1: Given that it has been a decade since the original wave of <mark>generative pre-training</mark> methods for images and considering their substantial impact in NLP, this class of methods is due for a modern re-examination and comparison with the recent progress of self-supervised methods. We re-evaluate <mark>generative pre-training</mark> on images and demonstrate that when using a flexible architecture ( Vaswani et al. , 2017 ) , a tractable and efficient likelihood based training objective ( Larochelle & Murray , 2011 ; Oord et al. , 2016 ) , and significant compute resources ( 2048 TPU cores ) , <mark>generative pre-training</mark> is competitive with other self-supervised approaches and learns<br>2: If a downstream task also involves classification, it is empirically validated that penultimate features perform well. With <mark>generative pre-training</mark>, it is not obvious whether a task like pixel prediction is relevant to image classification. This suggests that the penultimate layer of a model trained for pixel prediction might not produce the most useful representations for classification.<br>",
    "Arabic": "التدريب التوليدي المسبق",
    "Chinese": "生成式预训练",
    "French": "pré-entraînement génératif",
    "Japanese": "生成事前学習",
    "Russian": "генеративное предобучение"
  },
  {
    "English": "generative probabilistic model",
    "context": "1: For example, Fei-Fei et al. [4] used prior information from previously learnt categories to train a <mark>generative probabilistic model</mark> for a novel class, and Bart and Ullman [2] introduced a cross-generalization method where useful patches for one category guide the search within the pool of possible patches for a new, but similar, category.<br>2: A well-known toolkit used for unsupervised segmentation is Morfessor (Creutz and Lagus, 2007), which is a <mark>generative probabilistic model</mark>. In competition, Adaptor Grammars (AGs) (Johnson et al., 2007) represent a framework for specifying compositional nonparametric Bayesian models and are applied in unsupervised segmentation with notable success (Johnson, 2008).<br>",
    "Arabic": "نموذج احتمالي توليدي",
    "Chinese": "生成概率模型",
    "French": "modèle probabiliste génératif",
    "Japanese": "生成確率モデル",
    "Russian": "генеративная вероятностная модель"
  },
  {
    "English": "generative process",
    "context": "1: The first term captures the loss for an ideal <mark>generative process</mark> on the data distribution, and should correspond to the entropy of natural text. The second term captures the fact that a perfectly trained transformer with parameters underperforms the ideal <mark>generative process</mark>.<br>2: Generator: We denote the full <mark>generative process</mark> formally as \n G θ ({z i s , z i a , T i } N i=1 , ξ) = π neural θ (I V ) \n where \n<br>",
    "Arabic": "عملية توليدية",
    "Chinese": "生成过程",
    "French": "processus génératif",
    "Japanese": "生成過程",
    "Russian": "генеративный процесс"
  },
  {
    "English": "generator",
    "context": "1: In Figure 3b, z88 and z91 demonstrates interactive effects on the nasal vowels: when z88 >0 and z91<0, the <mark>Generator</mark> tends to output nasal vowels.<br>2: Aware <mark>Generator</mark><br>",
    "Arabic": "مولد",
    "Chinese": "生成器",
    "French": "générateur",
    "Japanese": "ジェネレータ",
    "Russian": "генератор"
  },
  {
    "English": "generator architecture",
    "context": "1: This shows that even the traditional <mark>generator architecture</mark> performs better when we introduce an intermediate latent space that does not have to follow the distribution of the training data.<br>2: The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13,52,37] provide no quantitative way to compare different generators against each other. Motivated by style transfer literature [27], we re-design the <mark>generator architecture</mark> in a way that exposes novel ways to control the image synthesis process.<br>",
    "Arabic": "هندسة المولدات",
    "Chinese": "生成器架构",
    "French": "architecture du générateur",
    "Japanese": "生成器アーキテクチャ",
    "Russian": "архитектура генератора"
  },
  {
    "English": "generator network",
    "context": "1: One solution to the above is to exploit synthetic training data, which might improve the generalizability of classifier D. Assume we have a <mark>generator network</mark> G(z) that produces synthetic images given (Gaussian normal) random noise inputs z ∼ N .<br>2: However, instead of feeding the resulting scene representation r to a convolutional LSTM architecture to parameterize a density over latent variables z, we instead directly feed the scene representation r to a <mark>generator network</mark>. We use as generator a deterministic, autoregressive, skip-convolutional LSTM C, the deterministic equivalent of the generator architecture proposed in [12].<br>",
    "Arabic": "شبكة المولدات",
    "Chinese": "生成器网络",
    "French": "réseau générateur",
    "Japanese": "生成器ネットワーク",
    "Russian": "генераторная сеть"
  },
  {
    "English": "genetic algorithm",
    "context": "1: Second, within the selected subset, the association between SNPs and the phenotypes are searched. These methods are not complete since the SNPs with weak marginal effects may not be selected in the first step. <mark>Genetic algorithm</mark> [Carlborg et al. 2000;Nakamichi et al. 2001] has been applied in finding SNP-pairs for quantitative phenotypes.<br>",
    "Arabic": "خوارزمية جينية",
    "Chinese": "遗传算法",
    "French": "algorithme génétique",
    "Japanese": "遺伝的アルゴリズム",
    "Russian": "генетический алгоритм"
  },
  {
    "English": "geodesic",
    "context": "1: When this choice is not unique we fix a minimizing <mark>geodesic</mark>. We denote \n Γ x2 x1 : T x1 M → T x2 M the associated parallel transport. Let b ∈ C ∞ ([0, T ] , X (M)). We start by introducing a family of GRWs defined on progressively finer grids.<br>2: γ k+1 = exp X γ k [W k+1 ] \n Move along the <mark>geodesic</mark> defined by W k+1 and \n X γ k on M 6: return {X γ k } N k=0 \n<br>",
    "Arabic": "المسار الجيوديسي",
    "Chinese": "测地线",
    "French": "géodésique",
    "Japanese": "測地線",
    "Russian": "геодезическая"
  },
  {
    "English": "geodesic distance",
    "context": "1: , s T ] and the initial <mark>geodesic distance</mark> to goal d i for episode i, we first compute the length of the agent's path \n l i = T t=2 ||s t − s t−1 || 2 (2) \n then SPL for episode i as \n<br>2: Thus, the chosen representation should be continuous in parameter space, which prohibits axis-angle representations; b) the representation should enable efficient computation of the <mark>geodesic distance</mark> between two elements; c) our algorithm requires efficient gradient descent in pose space. As described in Sec.<br>",
    "Arabic": "مسافة جيوديسية",
    "Chinese": "测地距离",
    "French": "distance géodésique",
    "Japanese": "測地距離",
    "Russian": "геодезическое расстояние"
  },
  {
    "English": "geometric consistency",
    "context": "1: Here we employ <mark>geometric consistency</mark> (GC) [7], which is independent of the feature space and associates the largest consistent cluster relating to the compatibility among correspondences. By comparing Row 1 and 2 of Table 5, GC has a negative impact on MAC performance, potentially due to that some inliers are also removed in this process.<br>",
    "Arabic": "تناسق هندسي",
    "Chinese": "几何一致性",
    "French": "cohérence géométrique",
    "Japanese": "幾何学的一貫性",
    "Russian": "геометрическая согласованность"
  },
  {
    "English": "geometric distribution",
    "context": "1: The number of times a feature needs to be seen before it is added to the model follows a <mark>geometric distribution</mark> with expected value 1 p . • Bloom Filter Inclusion. We use a rolling set of counting Bloom filters [4,12] to detect the first n times a feature is encountered in training.<br>2: Since the restart behavior of DR is a Bernoulli process, the expected number of restarts to reach the global basin is r = p −n , from the shifted <mark>geometric distribution</mark>. If the number of iterations needed to reach the stationary point of the current basin is τ then the expected complexity of DR is O(τ p −n ).<br>",
    "Arabic": "توزيع هندسي",
    "Chinese": "几何分布",
    "French": "distribution géométrique",
    "Japanese": "幾何分布",
    "Russian": "геометрическое распределение"
  },
  {
    "English": "geometric invariant",
    "context": "1: The situation has changed in the past few years, with the advent of combinatorial or mixed continuous/combinatorial optimization approaches to feature matching (see, for example [4,19,20,28,16]). 1 This paper builds on this work in a framework that can accommodate both (mostly local) <mark>geometric invariants</mark> and image descriptors.<br>",
    "Arabic": "ثابت هندسي",
    "Chinese": "几何不变量",
    "French": "invariant géométrique",
    "Japanese": "幾何的不変量",
    "Russian": "геометрический инвариант"
  },
  {
    "English": "geometric transformation",
    "context": "1: Both of the above problems are related in that they have to do with the lack of information about <mark>geometric transformations</mark>: assume we only observe S without access to the vectorial representations X n×d . Then we have lost the information about orthogonal transformations X ← XO with OO t = I d , i.e.<br>",
    "Arabic": "تحويل هندسي",
    "Chinese": "几何变换",
    "French": "transformation géométrique",
    "Japanese": "幾何変換",
    "Russian": "геометрическое преобразование"
  },
  {
    "English": "geometry processing",
    "context": "1: Despite these advances, there is still a large body of work in <mark>geometry processing</mark>, computer vision and graphics which relies on explicit surface representations. Often these mesh-based algorithms are a better choice than their implicit counterparts.<br>2: As mentioned earlier, in the Lagrangian setting, the surface is defined with a finite set of points ∂Ω L . A variety of methods in <mark>geometry processing</mark> and computer vision define an energy function E that is minimized to make instantaneous updates to ∂Ω L .<br>",
    "Arabic": "معالجة الهندسة",
    "Chinese": "几何处理",
    "French": "traitement géométrique",
    "Japanese": "幾何処理",
    "Russian": "обработка геометрии"
  },
  {
    "English": "gibb distribution",
    "context": "1: H(x * ) ≤ B(p) := n i=1 E γi [γ i (x * i )] \n . We extend this result to show how the errors of bounding ln Z, sampling, and entropy estimation are related: Proposition 11. Writing p for the <mark>Gibbs distribution</mark> and B(p \n<br>2: Hazan & Jaakkola (2012); Hazan et al. (2013) showed that from such an approximation, upper and lower bounds on the partition function and a sequential sampler for the <mark>Gibbs distribution</mark> can still be recovered.<br>",
    "Arabic": "توزيع جيبس",
    "Chinese": "吉布斯分布",
    "French": "distribution de Gibbs",
    "Japanese": "ギブス分布",
    "Russian": "распределение Гиббса"
  },
  {
    "English": "gini coefficient",
    "context": "1: We compute the <mark>Gini coefficient</mark> of the in-degree distribution of the graphs: for the YouTube graphs the <mark>Gini coefficient</mark> of in-degree for the harmful nodes is never below 90%; while for the NELA-GT graphs this index is never above 50%.<br>2: 10 It is important to note that this categorization is done by product experts for navigation and inventory purposes: all product labels are produced independently from any NLP consideration. 2. we calculate the <mark>Gini coefficient</mark> (Catalano et al., 2009) over the distribution on the values of sport and choose a conservative Gini threshold, i.e.<br>",
    "Arabic": "معامل جيني",
    "Chinese": "基尼系数",
    "French": "coefficient de Gini",
    "Japanese": "ジニ係数",
    "Russian": "коэффициент Джини"
  },
  {
    "English": "gist descriptor",
    "context": "1: [5] use a global feature known as the \"gist\" to learn statistical priors on the locations of objects within the context of the specific scene. The <mark>gist descriptor</mark> is excellent at predicting large structures in the scene, but cannot handle the local interactions present in the satellite data, for example.<br>",
    "Arabic": "وصف الجوهرية",
    "Chinese": "概要描述符",
    "French": "descripteur d'essence",
    "Japanese": "要点記述子",
    "Russian": "дескриптор сути"
  },
  {
    "English": "global average pooling",
    "context": "1: The network ends with a <mark>global average pooling</mark>, a 10-way fully-connected layer, and softmax. There are totally 6n+2 stacked weighted layers. The following so our residual models have exactly the same depth, width, and number of parameters as the plain counterparts.<br>2: At the end of the last dense block, a <mark>global average pooling</mark> is performed and then a softmax classifier is attached. The feature-map sizes in the three dense blocks are 32× 32, 16×16, and 8×8, respectively.<br>",
    "Arabic": "المتوسط العالمي للتجميع",
    "Chinese": "全局平均池化",
    "French": "regroupement moyen global",
    "Japanese": "グローバル平均プーリング",
    "Russian": "глобальное усреднение"
  },
  {
    "English": "global average pooling layer",
    "context": "1: We perform downsampling directly by convolutional layers that have a stride of 2. The network ends with a <mark>global average pooling layer</mark> and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig. 3<br>2: GA denotes a <mark>global average pooling layer</mark>. Fully connected layers with k hidden units followed by a ReLU activation are denoted by L k . The ReLU is not applied to the L k layer, if it is the top layer. The discriminators call for an even more elaborate terminology.<br>",
    "Arabic": "طبقة التجميع العالمية المتوسطة",
    "Chinese": "全局平均池化层",
    "French": "couche de pooling moyen global",
    "Japanese": "グローバル平均プーリング層",
    "Russian": "слой глобального усреднения"
  },
  {
    "English": "global coordinate frame",
    "context": "1: To embed the scene-level prior, the anchor I a T is rotated and translated into the <mark>global coordinate frame</mark> according to each agent's current location and heading angle, which is denoted as I s T , as shown in Eq. (12), \n<br>",
    "Arabic": "الإطار الإحداثي العالمي",
    "Chinese": "全局坐标系",
    "French": "cadre de coordonnées global",
    "Japanese": "全体座標系",
    "Russian": "глобальная система координат"
  },
  {
    "English": "global illumination",
    "context": "1: Visual metrics are often integrated with imaging algorithms to achieve the best compromise between efficiency and perceptual quality. A classical example is image or video compression, but the metrics have been also used in graphics to control <mark>global illumination</mark> solutions [Myszkowski et al. 1999;Ramasubramanian et al.<br>",
    "Arabic": "الإضاءة العالمية",
    "Chinese": "全局照明",
    "French": "illumination globale",
    "Japanese": "グローバル照明",
    "Russian": "глобальное освещение"
  },
  {
    "English": "global minima",
    "context": "1: it is a minima but may not be the <mark>global minima</mark>. We note that for specific (small) values of β w it will be the <mark>global minima</mark>.<br>",
    "Arabic": "الحد الأدنى العالمي",
    "Chinese": "全局最小值",
    "French": "minima globaux",
    "Japanese": "グローバル最小値",
    "Russian": "глобальные минимумы"
  },
  {
    "English": "global minimum",
    "context": "1: Our characterization of the structure in the objective function implies that (stochastic) gradient descent from arbitrary starting point converge to a <mark>global minimum</mark>. This is because gradient descent converges to a local minimum [GHJY15,LSJR16], and every local minimum is also a <mark>global minimum</mark>.<br>2: The ability to find the <mark>global minimum</mark> efficiently, while theoretically of great value, does not overcome this drawback. This is clearly visible in the relatively poor performance of such algorithms on the stereo benchmarks described in [37].<br>",
    "Arabic": "الحد الأدنى العالمي",
    "Chinese": "全局最小值",
    "French": "minimum global",
    "Japanese": "グローバル最小値",
    "Russian": "глобальный минимум"
  },
  {
    "English": "global model",
    "context": "1: Thus, we first show which factors determine the state of an FL course: (1) Server-side: Basically, the current round number and the <mark>global model</mark> parameters must be saved.<br>2: For example, because people and bottles have similar shapes, the local detectors we use (Felzenszwalb et al. 2008) can confuse them. Our <mark>global model</mark> learns to strongly compete such overlapping detections using a negative weight. However, people and sofas tend to overlap because people partially occlude sofas when sitting down.<br>",
    "Arabic": "النموذج العالمي",
    "Chinese": "全局模型",
    "French": "modèle global",
    "Japanese": "グローバルモデル",
    "Russian": "глобальная модель"
  },
  {
    "English": "global objective",
    "context": "1: In addition, instead of using a subpixel Taylor approximation of the data term, our update operator learns to propose the descent direction. More recently, optical flow has also been approached as a discrete optimization problem [35,13,47] using a <mark>global objective</mark>.<br>",
    "Arabic": "هدف عالمي",
    "Chinese": "全局目标",
    "French": "objectif global",
    "Japanese": "グローバル目的関数",
    "Russian": "глобальный критерий"
  },
  {
    "English": "global optima",
    "context": "1: Loh and Wainwright [LW15] showed that for many statistical settings that involve missing/noisy data and non-convex regularizers, any stationary point of the non-convex objective is close to <mark>global optima</mark>; furthermore, there is a unique stationary point that is the global minimum under stronger assumptions [LW14].<br>2: Although these are not guaranteed to be <mark>global optima</mark>, our experiments show that by careful parametrization of the problem, good local optima can be found reliably. Previous stereo algorithms have implemented approximations to second order smoothness priors.<br>",
    "Arabic": "الحلول العالمية المثلى",
    "Chinese": "全局最优解",
    "French": "optima globaux",
    "Japanese": "グローバル最適解",
    "Russian": "глобальный оптимум"
  },
  {
    "English": "global optimization",
    "context": "1: The intuition behind this step is that two image-plane detections are consistent if they correspond to the same 3D object (Fig. 3(right)). Thus, we can disambiguate between overlapping responses from different detectors on the basis of the world state they would infer, which is done in the following <mark>global optimization</mark> step.<br>2: However their algorithm precomputes local surface normals and in fact optimizes a first-order prior on the normals, rather than a second-order prior on the disparities. Indeed, they discuss the <mark>global optimization</mark> of a second order prior, and conclude that this \"makes the problem computationally infeasible\". A related class of methods is \"segment-based\" stereo.<br>",
    "Arabic": "التحسين العالمي",
    "Chinese": "全局优化",
    "French": "optimisation globale",
    "Japanese": "グローバル最適化",
    "Russian": "глобальная оптимизация"
  },
  {
    "English": "global optimum",
    "context": "1: Therefore, at the end of the simulation, most of the particles will converge to a small ball surrounding the <mark>global optimum</mark> of the search space.<br>2: The values of all the traps are added together to form the overall fitness value. An n-bit trap-5 function has one <mark>global optimum</mark> (a string of all 1s) and (2 n/5 − 1) local optima. The difficulty in optimizing this function is that in each 5-bit trap function, all 5 bits have to be considered together.<br>",
    "Arabic": "الحل الأمثل العالمي",
    "Chinese": "全局最优解",
    "French": "optimum global",
    "Japanese": "グローバル最適解",
    "Russian": "глобальный оптимум"
  },
  {
    "English": "global pooling",
    "context": "1: Tasks like 3D semantic segmentation and 3D object part labeling fit naturally under this framework. With simple techniques such as <mark>global pooling</mark> [33], SPLATNet 3D can be modified to produce a single output vector and thus can be extended to other tasks such as classification. Network architecture. The architecture of SPLATNet 3D is depicted in Figure 3.<br>",
    "Arabic": "التجميع العالمي",
    "Chinese": "全局池化",
    "French": "poolage global",
    "Japanese": "グローバルプーリング",
    "Russian": "глобальное объединение"
  },
  {
    "English": "global reward",
    "context": "1: Under a common <mark>global reward</mark>, and some forms of local reward (Bagnell & Ng, 2006), agents that do not communicate can learn to cooperate implicitly to maximize the <mark>global reward</mark> (Boutilier, 1999). However, unless each agent has access to the full state description, they will generally not be able to act optimally.<br>",
    "Arabic": "المكافأة العالمية",
    "Chinese": "全局奖励",
    "French": "récompense globale",
    "Japanese": "グローバル報酬",
    "Russian": "глобальное вознаграждение"
  },
  {
    "English": "goal state",
    "context": "1: We assume that when a person stops, their location in the environment is a goal. We detect <mark>goal state</mark>s by using a velocity-based stop detector. Whenever a <mark>goal state</mark> is encountered, the sequence of states since the last <mark>goal state</mark> to the current <mark>goal state</mark> is considered a completed episode ξ.<br>2: If a <mark>goal state</mark> is reached (line 9 and 10), an optimal plan is extracted and returned (for details see (Torralba et al. 2017)).<br>",
    "Arabic": "حالة الهدف",
    "Chinese": "目标状态",
    "French": "état but",
    "Japanese": "目標状態",
    "Russian": "целевое состояние"
  },
  {
    "English": "gold label",
    "context": "1: For instance, there is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of <mark>gold labels</mark> and system predictions. For example , in the 2014 Workshop on Statistical Machine Translation ( WMT ) , which since 2012 has provided a main venue for evaluation of systems , sentence-level systems were evaluated with respect to three distinct gold standard representations and each of those compared to predictions using four different measures , resulting in a total of 12 different system rankings , 6 identified<br>",
    "Arabic": "تسمية ذهبية",
    "Chinese": "金标签",
    "French": "étiquette de référence",
    "Japanese": "正解ラベル",
    "Russian": "эталонная метка"
  },
  {
    "English": "gold parse",
    "context": "1: The aim of our pruning models is to filter as many indices as possible without losing the <mark>gold parse</mark>. In structured prediction cascades, we incorporate this pruning goal into our training objective. Let y be the gold output for a sentence.<br>2: where g(PATH(e)) is the score of the unique path to e, and h(e) is the A * heuristic. If all violations are zero, we find the <mark>gold parse</mark> without exploring any incorrect partial parses-maximizing both accuracy and efficiency.<br>",
    "Arabic": "الإخراج الذهبي",
    "Chinese": "金标准解析",
    "French": "l'analyse syntaxique de référence",
    "Japanese": "正解の構文解析",
    "Russian": "золотой разбор"
  },
  {
    "English": "good-first search",
    "context": "1: Lastly, to show that our parser is both more accurate and efficient than other decoding methods, we decode our full model using <mark>best-first search</mark>, reranking, and beam search.<br>2: Using this revised independence assumption the derivation for <mark>best-first search</mark> over taxonomies for hyponym acquisition remains unchanged.<br>",
    "Arabic": "البحث الأولوي الجيد",
    "Chinese": "最佳优先搜索",
    "French": "recherche du meilleur d'abord",
    "Japanese": "優良優先探索",
    "Russian": "поиск с наилучшим первым выбором"
  },
  {
    "English": "good-first search algorithm",
    "context": "1: Motivated by A * search (Hart et al., 1968), a <mark>best-first search algorithm</mark> that finds high-scoring paths by selecting actions that maximize: \n f (a) = s(a) + h(a), \n<br>2: h x is consistent iff it is admissible and h x (n) ≤ c(n, n ) + h x (n ) for any pair of nodes n, n . The priority function of a <mark>best-first search algorithm</mark> is a ranking function that maps a node n to a value that determines the order of expansion.<br>",
    "Arabic": "خوارزمية البحث الأفضل أولاً",
    "Chinese": "优先搜索算法",
    "French": "algorithme de recherche meilleur premier",
    "Japanese": "良い最初の探索アルゴリズム",
    "Russian": "алгоритм поиска по лучшему первому шагу"
  },
  {
    "English": "good-turing estimate",
    "context": "1: Second, our discount constant was estimated by maximizing the log-likelihood of the heldout data (assuming the discount constant is between 0 and 1), instead of the <mark>Good-Turing estimate</mark>. Finally, in order to deal with the fractional counts we encounter during the EM training procedure, we developed an approximate Kneser-Ney smoothing for fractional counts.<br>",
    "Arabic": "تقدير غود تورينغ",
    "Chinese": "好图灵估计",
    "French": "estimateur de Good-Turing",
    "Japanese": "グッド・チューリング推定",
    "Russian": "оценка Гуд-Тьюринга"
  },
  {
    "English": "gossip algorithm",
    "context": "1: , |λ n | } ∈ [ 0 , 1 ) to denote its general second-largest eigenvalue , where λ i denotes the i-th largest eigenvalue of W . We let <mark>gossip algorithm</mark> class A B,W denote the set of all algorithms A ∈ A B that only communicate via gossip using a single matrix W ∈ W n .<br>2: Under the local regularity assumption, we provide in Section 4 matching upper and lower bounds of complexity in a decentralized setting in which communication is performed using the <mark>gossip algorithm</mark> [9]. Moreover, we propose the first optimal algorithm for non-smooth decentralized optimization, called multi-step primal-dual (MSPD).<br>",
    "Arabic": "خوارزمية القيل والقال",
    "Chinese": "传言算法",
    "French": "algorithme de potins",
    "Japanese": "ゴシップアルゴリズム",
    "Russian": "алгоритм сплетен"
  },
  {
    "English": "gradient accumulation",
    "context": "1: We focused on matching all the domain dataset sizes (see Table 1) such that each domain is exposed to the same amount of data as for 12.5K steps it is trained for. AMAZON reviews contain more documents, but each is shorter. We used an effective batch size of 2048 through <mark>gradient accumulation</mark>, as recommended in .<br>2: Learning, PMLR 139, 2021. Copyright 2021 by the author(s). Benzing et al., 2019;Marschall et al., 2019;Menick et al., 2020) <mark>gradient accumulation</mark>. These methods have different tradeoffs with respect to compute, memory, and gradient variance.<br>",
    "Arabic": "تراكم التدرج",
    "Chinese": "梯度累积",
    "French": "accumulation de gradients",
    "Japanese": "勾配蓄積",
    "Russian": "аккумуляция градиента"
  },
  {
    "English": "gradient accumulation step",
    "context": "1: We use a linear learning rate schedule with 20% of warmup steps and a peak learning rate of 1e-6. We use a maximum sequence length of 1024 tokens, batch size = 1 and <mark>gradient accumulation step</mark> = 4. On average, three iterations of TRIPOST take about 12 hours to train.<br>2: For the XLM-R base model, we set the batch size to 8 and the <mark>gradient accumulation step</mark> to 4. For the Sentiment Analysis task, we used a batch size of 8, a learning rate of 5e-5, and a <mark>gradient accumulation step</mark> of 1 for the mBERT base model.<br>",
    "Arabic": "خطوة تراكم التدرج",
    "Chinese": "梯度累积步数",
    "French": "étape d'accumulation de gradient",
    "Japanese": "勾配累積ステップ",
    "Russian": "шаг накопления градиентов"
  },
  {
    "English": "gradient ascent",
    "context": "1: The derivatives of this function with respect to f can easily be computed, and straightforward <mark>gradient ascent</mark> can then be used to update f . The learning entails using f to get the scores for all documents and back propagating the derivatives of the target gain to learn f . This leads to the algorithm summarized in Algorithm 1.<br>2: The computational complexity of n FSSD 2 andσ 2 H1 is O(d 2 Jn). Thus, finding a local optimum via <mark>gradient ascent</mark> is still linear-time, for a fixed maximum number of iterations.<br>",
    "Arabic": "تصاعد التدرج",
    "Chinese": "梯度上升法",
    "French": "ascension de gradient",
    "Japanese": "勾配上昇法",
    "Russian": "градиентный подъем"
  },
  {
    "English": "gradient boost tree",
    "context": "1: To assess the sample complexity argument we compute for each trained model a statistical efficiency score which we define as the average accuracy based on 100 samples divided by the average accuracy based on 10 000 samples for either the logistic regression or the <mark>gradient boosted trees</mark>.<br>2: However, the correlation varies across data sets. where the goal is to recover the true factors of variations from the learned representation using either multi-class logistic regression (LR) or <mark>gradient boosted trees</mark> (GBT).<br>",
    "Arabic": "شجرة التعزيز التدريجي",
    "Chinese": "梯度提升树",
    "French": "arbre de boost de gradient",
    "Japanese": "勾配ブースティングツリー",
    "Russian": "градиентный бустинг деревьев"
  },
  {
    "English": "gradient clipping",
    "context": "1: We used no <mark>gradient clipping</mark> and a weight decay of 0.1. Unlike [2] which specified different dropout rates for different parameters, we used a constant dropout rate of 0.25 throughout the network, including before every linear layer and on the residual branches.<br>2: A batch size of 1024, an initial learning rate of 0.001, a weight decay of 0.05, and <mark>gradient clipping</mark> with a max norm of 1 are used. We include most of the augmentation and regularization strategies of [ 63 ] in training , including RandAugment [ 17 ] , Mixup [ 77 ] , Cutmix [ 75 ] , random erasing [ 82 ] and stochastic depth [ 35 ] , but not repeated augmentation [ 31 ] and Exponential Moving Average ( EMA ) [ 45 ] which do not<br>",
    "Arabic": "تقييد التدرج",
    "Chinese": "梯度裁剪",
    "French": "écrêtage de gradient",
    "Japanese": "勾配クリッピング",
    "Russian": "обрезка градиента"
  },
  {
    "English": "gradient computation",
    "context": "1: Given the partial derivations, our <mark>gradient computation</mark> is identical to Equation 2. However, in contrast to Collins and Roark (2004) our data does not include gold derivations. Therefore, we attempt to identify max-scoring partial derivations that may lead to the correct derivation.<br>2: When the time required to lock memory for writing is dwarfed by the <mark>gradient computation</mark> time, this method results in a linear speedup, as the errors induced by the lag in the gradients are not too severe.<br>",
    "Arabic": "\"حساب التدرج\"",
    "Chinese": "梯度计算",
    "French": "calcul du gradient",
    "Japanese": "勾配計算",
    "Russian": "вычисление градиента"
  },
  {
    "English": "gradient descent",
    "context": "1: <mark>Gradient descent</mark> will converge to such a point with probability 1 from a random starting point. Our results are also robust to noise. Even if each entry is corrupted with Gaussian noise of standard deviation µ 2 Z 2 F /d (comparable to the magnitude of the entry itself!<br>2: However, there are many techniques from numerical optimization that can be applied to find local minima. <mark>Gradient descent</mark> is perhaps the simplest technique to implement, but convergence can be slow.<br>",
    "Arabic": "الانحدار التدريجي",
    "Chinese": "梯度下降",
    "French": "descente de gradient",
    "Japanese": "勾配降下法",
    "Russian": "градиентный спуск"
  },
  {
    "English": "gradient descent algorithm",
    "context": "1: , the solution will be very simple. When standard back-propagation is used to optimize Equation 1, the derivative of with respect to the parameters is calculated and used as the direction for the <mark>gradient descent algorithm</mark>. Since<br>2: Before we present results of the study of brain growth, we exemplify the methodology in detail on the finitedimensional Lie group of 3D rotations, SO (3). Following the approach in [6], we solve the weighted averaging problem in (8) by a <mark>gradient descent algorithm</mark>.<br>",
    "Arabic": "خوارزمية الهبوط التدريجي",
    "Chinese": "梯度下降算法",
    "French": "algorithme de descente de gradient",
    "Japanese": "勾配降下法",
    "Russian": "алгоритм градиентного спуска"
  },
  {
    "English": "gradient estimate",
    "context": "1: Including anything beyond this space for model update only introduces noise. Thus our method projects the selected model update direction back to the document space to reduce its variance. We proved that DSP maintains an unbiased <mark>gradient estimate</mark>, and it can substantially improve the regret bound for DBGD-style algorithms via the reduced variance.<br>2: This method is similar to the one in [12], but we model the measurement noise properly to get better <mark>gradient estimate</mark>, and use a parallelisable reconstruction method for speed. Pixel-Wise EKF Based Gradient Estimation Each pixel of the keyframe holds an independent <mark>gradient estimate</mark> g ( p k ) = ( g u , g v ) , consisting of log intensity gradients g u and g v along the horizontal and vertical axes in image space respectively , and a 2 × 2 uncertainty covariance matrix P g ( p k )<br>",
    "Arabic": "تقدير التدرج",
    "Chinese": "梯度估计",
    "French": "estimation du gradient",
    "Japanese": "勾配推定",
    "Russian": "оценка градиента"
  },
  {
    "English": "gradient estimation",
    "context": "1: In this section, we discuss additional related work on online learning algorithms, and on one special class of unrolled optimization problems: hyperparameter optimization (HO). Table 1 compares several approaches to <mark>gradient estimation</mark> in unrolled computation graphs, with respect to compute, memory, parallelization, unbiasedness, and smoothing.<br>2: We show that our document space projection reduces the variance of <mark>gradient estimation</mark> from d to Rank(A t ) in Section 3.4, and then analyze its benefit for regret reduction from a low-variance <mark>gradient estimation</mark>.<br>",
    "Arabic": "تقدير التدرج",
    "Chinese": "梯度估计",
    "French": "estimation du gradient",
    "Japanese": "勾配推定",
    "Russian": "оценка градиента"
  },
  {
    "English": "gradient estimator",
    "context": "1: However, we show that in the presence of neardiscontinuities, selecting based on empirical variance alone can lead to highly inaccurate estimates of ∇F , and propose a robustness constraint on the accuracy of the interpolated estimate to remedy this effect. Contributions. We 1 ) shed light on some of the inherent problems of RL using differentiable simulators , and answer which <mark>gradient estimator</mark> can be more useful under different characteristics of underlying systems such as discontinuities , stiffness , and chaos ; and 2 ) present the α-order <mark>gradient estimator</mark> , a robust interpolation strategy between the two <mark>gradient estimator</mark>s that utilizes exact gradients without<br>2: In benchmark generative modeling tasks such as training binary variational autoencoders, our <mark>gradient estimator</mark> achieves substantially lower variance than state-of-the-art estimators with the same number of function evaluations. We first provide a general recipe for constructing practical Stein operators for discrete distributions (Table 1), generalizing the prior literature [6,8,9,16,25,46,67].<br>",
    "Arabic": "مقدر التدرج",
    "Chinese": "梯度估计器",
    "French": "estimateur de gradient",
    "Japanese": "勾配推定器",
    "Russian": "градиентный оценщик"
  },
  {
    "English": "gradient explosion",
    "context": "1: For example, we have to use a smaller learning rate of 0.00025 or lower to avoid sudden <mark>gradient explosion</mark> during training. These results suggest possible future work by improving the normalization method (Shen et al., 2020;Brock et al., 2021).<br>",
    "Arabic": "انفجار التدرج",
    "Chinese": "梯度爆炸",
    "French": "explosion de gradient",
    "Japanese": "勾配爆発",
    "Russian": "взрыв градиента"
  },
  {
    "English": "gradient flow",
    "context": "1: We show a critical scaling regime for the step-size, below which the effective ballistic dynamics matches <mark>gradient flow</mark> for the population loss, but at which, a new correction term appears which changes the phase diagram. About the fixed points of this effective dynamics, the corresponding diffusive limits can be quite complex and even degenerate.<br>2: Lemma 6. Under continually renormalized <mark>gradient flow</mark> (Equation 5), the left and right singular vectors of the SNR matrix remain constant i.e. they are independent of t. \n Proof. Without loss of generality, assume X is in represented in aligned SNR coordinates (Section D.2).<br>",
    "Arabic": "تدفق التدرج",
    "Chinese": "梯度流",
    "French": "flux de gradient",
    "Japanese": "勾配フロー",
    "Russian": "поток градиента"
  },
  {
    "English": "gradient information",
    "context": "1: We have demonstrated on a diverse set of distributions that this approach to sampling considerably outperforms baseline samplers which do not exploit known structure in the target distribution as well as many that do. Further, we find our approach outperforms prior discrete samplers which use <mark>gradient information</mark> with continuous relaxations.<br>2: Next we employ them as the trigger to attack other examples, the model predictions flip from both neutral and entailment to contradict. Our attack method relies on attribution scores, which utilizes the <mark>gradient information</mark>, therefore it belongs to white-box non-targeted attacks. We extract the dependencies with the largest attribution scores as the adversarial triggers from 3,000 input examples.<br>",
    "Arabic": "معلومات التدرج",
    "Chinese": "梯度信息",
    "French": "information de gradient",
    "Japanese": "勾配情報",
    "Russian": "градиентная информация"
  },
  {
    "English": "gradient method",
    "context": "1: Unfortunately, <mark>gradient methods</mark> prove intractable due to needing a backpropagation pass for every pixel for all T time steps, and even minor perturbations result in significantly different images in our pilot experiments.<br>2: As a consequence, we find that adversarial examples generated with <mark>gradient methods</mark> when penalizing for a high LID either (a) are not adversarial; or (b) are detected as adversarial, despite penalizing for the LID loss.<br>",
    "Arabic": "طريقة التدرج",
    "Chinese": "梯度方法",
    "French": "méthode de gradient",
    "Japanese": "勾配法",
    "Russian": "метод градиента"
  },
  {
    "English": "gradient norm",
    "context": "1: Then it grew to a large value at step-b, suggesting the loss curvature is quite steep at that moment. Since we didn't apply any model stability treatments, the model diverged completely at step-c and the <mark>gradient norm</mark> ∥ ∥ 2 became a small value.<br>2: To allow for a greater <mark>gradient norm</mark> bound in CowClip, we use a larger initial weight by setting the σ to 10 −2 for training with CowClip.<br>",
    "Arabic": "معيار التدرج",
    "Chinese": "梯度范数",
    "French": "norme du gradient",
    "Japanese": "勾配ノルム",
    "Russian": "норма градиента"
  },
  {
    "English": "gradient operator",
    "context": "1: Here e represents the perturbed embedding of an adversarial example generated from embedding e and ∇ e denotes the <mark>gradient operator</mark>. L(e; θ ) and L( e; θ ) represent the loss functions from the original training instance and its adversarial transformation respectively. α is a weighting parameter.<br>2: L P C = N i=2 ||(I i − I 1 ) M i || 2 + ||(∇I i − ∇I 1 ) M i || 2 ||M i || 1 \n (3) where ∇ denotes the <mark>gradient operator</mark> and is dot product.<br>",
    "Arabic": "مشغل التدرج",
    "Chinese": "梯度算子",
    "French": "opérateur de gradient",
    "Japanese": "勾配演算子",
    "Russian": "оператор градиента"
  },
  {
    "English": "gradient penalty",
    "context": "1: For the critic we have adopted the PatchGan architecture of [10], but removing feature normalization. Otherwise, when computing the <mark>gradient penalty</mark>, the norm of the critic's gradient would be computed with respect to the entire batch and not with respect to each input independently. The model is trained on the EmotioNet dataset [3].<br>2: The observer (the camera in our case), in contrast, can freely change its elevation angle wrt. the scene. We train our model with the non-saturating GAN objec- 1 Details can be found in the supplementary material. tive [24] and R 1 <mark>gradient penalty</mark> [58] V(θ, φ) = \n<br>",
    "Arabic": "- Candidate term translation 1: \"عقوبة التدرج\"",
    "Chinese": "梯度惩罚",
    "French": "pénalité de gradient",
    "Japanese": "勾配ペナルティ",
    "Russian": "градиентное наказание"
  },
  {
    "English": "gradient signal",
    "context": "1: AutoPrompt (Shin et al., 2020) searches for improved prompts using a <mark>gradient signal</mark>, although its prompts are limited to sequences of actual (\"hard\") English words, unlike our method. We compare our novel soft prompts against all of these systems.<br>",
    "Arabic": "إشارة التدرج",
    "Chinese": "梯度信号",
    "French": "signal de gradient",
    "Japanese": "勾配シグナル",
    "Russian": "сигнал градиента"
  },
  {
    "English": "gradient step",
    "context": "1: Experiments (Section 4.4) show that conditioning on both the state and the task identity results in noticeable performance improvements, suggesting that the variance reduction provided by this objective is important for efficient joint learning of modular policies. The complete procedure for computing a single <mark>gradient step</mark> is given in Algorithm 1.<br>2: For each <mark>gradient step</mark>, we optimize the value network parameters φ by minimizing \n L φ = E s∼B,a∼π θ (s) 1 2 (r(s, a) + γV π φ (p(s, a)) − V π φ (s)) 2 , (11 \n ) \n<br>",
    "Arabic": "خطوة التدرج",
    "Chinese": "梯度步长",
    "French": "pas de gradient",
    "Japanese": "勾配ステップ",
    "Russian": "шаг градиента"
  },
  {
    "English": "gradient term",
    "context": "1: We use a multi-scale <mark>gradient term</mark>, L grad , which is the L 1 difference between the predicted log depth derivatives (in x and y directions) and the ground truth log depth derivatives, at multiple scales [19]. This term allows the network to recover sharp depth discontinuities and smooth gradient changes in the predicted depth images.<br>2: The backward pass requires the computation of the <mark>gradient term</mark> in ( 9) for each sample (x, y) in the mini-batch. This can be carried out by a single, bottom-up tree traversal. We start by setting \n A ℓ = π ℓy µ ℓ (x; Θ) P T [y|x, Θ, π] \n<br>",
    "Arabic": "مصطلح التدرج",
    "Chinese": "梯度项",
    "French": "terme de gradient",
    "Japanese": "勾配項",
    "Russian": "градиентный член"
  },
  {
    "English": "gradient update",
    "context": "1: • DecAtt: It is important to use gradient clipping for this model: for each <mark>gradient update</mark>, we check the L2 norm of all the gradient values, if it is greater than a threshold b, we scale the gradient by a factor α = b/L2 norm. Another useful procedure is to assemble batches of sentences with similar length.<br>2: : fully unroll the recurrent part of the actor , aggregate gradients in the backward pass across all time steps , and apply a <mark>gradient update</mark> . We use a target network for the critic, which updates every 150 training steps for the feed-forward centralised critics and every 50 steps for the recurrent IAC critics.<br>",
    "Arabic": "تحديث التدرج",
    "Chinese": "梯度更新",
    "French": "mise à jour du gradient",
    "Japanese": "勾配更新",
    "Russian": "градиентное обновление"
  },
  {
    "English": "gradient variance",
    "context": "1: where each state-action pair (s τ i , a τ i ) was selected by the subpolicy π b in the context of the task τ . Now minimization of the <mark>gradient variance</mark> requires that each c τ actually depend on the task identity. (This follows immediately by applying the corresponding argument in Greensmith et al.<br>",
    "Arabic": "تباين التدرج",
    "Chinese": "梯度方差",
    "French": "variance du gradient",
    "Japanese": "勾配の分散",
    "Russian": "градиентная вариация"
  },
  {
    "English": "gradient vector",
    "context": "1: We first calculate the normalized gradient (Luo et al., 2016) of each input token w.r.t the prediction of the next token: \n s m = g m 2 Lex n=1 g n 2 , \n where g m is the <mark>gradient vector</mark> of the input embedding e m .<br>",
    "Arabic": "متجه التدرج",
    "Chinese": "梯度向量",
    "French": "vecteur de gradient",
    "Japanese": "勾配ベクトル",
    "Russian": "вектор градиента"
  },
  {
    "English": "gradient-base approach",
    "context": "1: PES can be considered a gray-box approach as it does not require the objective to be differentiable like <mark>gradient-based approaches</mark>, but it does take into account the iterative optimization of the inner problem.<br>2: A key advantage of <mark>gradient-based approaches</mark> is that they scale to high-dimensional hyperparameters (e.g. millions of hyperparameters) (Lorraine et al., 2020). Maclaurin et al. (2015) differentiate through unrolled optimization to tune many hyperparameters including learning rates and weight decay coefficients.<br>",
    "Arabic": "النهج القائم على التدرجات",
    "Chinese": "基于梯度的方法",
    "French": "approche basée sur le gradient",
    "Japanese": "勾配ベースのアプローチ",
    "Russian": "подход на основе градиента"
  },
  {
    "English": "gradient-base learning",
    "context": "1: We include one such method in Appendix C.2 to show that tracking and forecasting-based representations may also emerge from <mark>gradient-based learning</mark>. Fig. 4. Two computation models considered in our evaluation. Each block represents an algorithm running on a device and its length indicates its runtime.<br>2: Early approaches directly color voxel grids using observed images [37], and more recent volumetric approaches use <mark>gradient-based learning</mark> to train deep networks to predict voxel grid representations of scenes [12,25,29,38,41,53]. Discrete voxel-based representations are effective for view synthesis, but they do not scale well to scenes at higher resolutions.<br>",
    "Arabic": "التعلم القائم على التدرج",
    "Chinese": "基于梯度学习",
    "French": "apprentissage par gradient",
    "Japanese": "勾配ベース学習",
    "Russian": "обучение на основе градиента"
  },
  {
    "English": "gradient-base method",
    "context": "1: We therefore restrict to <mark>gradient-based methods</mark> (game-theorists have considered a much broader range of techniques). Losses are not necessarily convex in any of their parameters, so Nash equilibria do not necessarily exist.<br>2: Gradient-based methods can reliably find local -but not global -optima of nonconvex objective functions (Lee et al., 2016;2017). Similarly, <mark>gradient-based methods</mark> cannot be expected to find global Nash equilibria in games. Definition 3.<br>",
    "Arabic": "طريقة قائمة على التدرج",
    "Chinese": "梯度法",
    "French": "méthode basée sur le gradient",
    "Japanese": "勾配ベース法",
    "Russian": "метод на основе градиента"
  },
  {
    "English": "gradient-base optimization",
    "context": "1: On the downside, the ELBO generally requires multiple epochs of <mark>gradient-based optimization</mark> to find the optimal variational distribution q, while the Laplace approximation can be computed as a simple post-processing step for any pretrained model. Moreover, optimizing the ELBO can generally suffer from the same overfitting behaviour as the marginal likelihood in general (see Section 4.2).<br>2: As we shall see, this exceedingly simple, theory motivated method also yields better performance in practice compared to <mark>gradient-based optimization</mark> of a linear predictor. We call our method DirectPred which simply estimates F , computes its eigen-decompositionF =ÛΛ FÛ , wherê \n Λ F = diag[s 1 , s 2 , . . .<br>",
    "Arabic": "تحسين قائم على التدرج",
    "Chinese": "基于梯度的优化",
    "French": "optimisation basée sur le gradient",
    "Japanese": "勾配ベースの最適化",
    "Russian": "Оптимизация по градиенту"
  },
  {
    "English": "gram matrix",
    "context": "1: The previous section showed that it is possible to force an embedding to preserve the graph structure in a given adjaceny matrix A by introducing a set of linear constraints on the embedding <mark>Gram matrix</mark> K. To choose a unique K from the admissible set in the convex hull generated by these linear constraints , we propose an objective function which favors lowdimensional embeddings<br>2: Kernel machines such as the Support Vector Machine are attractive because they can approximate any function or decision boundary arbitrarily well with enough training data. Unfortunately, methods that operate on the kernel matrix (<mark>Gram matrix</mark>) of the data scale poorly with the size of the training dataset.<br>",
    "Arabic": "مصفوفة غرام",
    "Chinese": "格拉姆矩阵",
    "French": "matrice de Gram",
    "Japanese": "グラム行列",
    "Russian": "матрица Грама"
  },
  {
    "English": "grammar inducer",
    "context": "1: The idea behind the symmetrizer is to glean information from skeleton parses. Grammar inducers can sometimes make good progress in resolving undirected parse structures despite being wrong about the polarities of most arcs (Spitkovsky et al., 2009, Figure 3: Uninformed).<br>",
    "Arabic": "محرض القواعد النحوية",
    "Chinese": "语法诱导器",
    "French": "inducteur de grammaire",
    "Japanese": "文法誘導器",
    "Russian": "индуктор грамматики"
  },
  {
    "English": "grammar induction",
    "context": "1: We will now instantiate the operators sketched out in §2 specifically for the <mark>grammar induction</mark> task.<br>2: Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as <mark>grammar induction</mark> (Snyder et al., 2009;Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006;, and part-of-speech tagging (Goldwater and Griffiths, 2007;.<br>",
    "Arabic": "استنباط القواعد",
    "Chinese": "语法归纳",
    "French": "induction de grammaire",
    "Japanese": "\"文法誘導\"",
    "Russian": "вывод грамматики"
  },
  {
    "English": "grammatical error detection",
    "context": "1: Current literature which uses gaze behaviour to solve downstream NLP tasks has been applied to the NLP tasks of sentiment analysis ( Mishra et al. , 2018a ; Barrett et al. , 2018 ; Long et al. , 2019 ) , sarcasm detection ( Mishra et al. , 2016 ) , <mark>grammatical error detection</mark> ( Barrett et al. , 2018 ) , hate speech<br>2: We show such regularization leads to significant improvements across a range of tasks, including sentiment analysis, detection of abusive language, and <mark>grammatical error detection</mark>. Our implementation is made available at https://github.com/coastalcph/ Sequence_classification_with_ human_attention.<br>",
    "Arabic": "الكشف عن الأخطاء النحوية",
    "Chinese": "语法错误检测",
    "French": "détection d'erreurs grammaticales",
    "Japanese": "文法エラー検出",
    "Russian": "обнаружение грамматических ошибок"
  },
  {
    "English": "grandparent dependency",
    "context": "1: has taken place ) . • G(y) is a set of <mark>grandparent dependencies</mark> of type 1. Each type 1 grandparent dependency is a tuple h, m, l, g .<br>2: This decomposition is similar to the first-order model of McDonald et al. (2005), but with the addition of the e features. We will extend our model to include higherorder features, in particular features based on sibling dependencies (McDonald and Pereira, 2006), and <mark>grandparent dependencies</mark>, as in (Carreras, 2007).<br>",
    "Arabic": "تبعية الأجداد",
    "Chinese": "祖父母依赖",
    "French": "dépendance des grands-parents",
    "Japanese": "祖父母依存",
    "Russian": "зависимость от предка"
  },
  {
    "English": "graph",
    "context": "1: Input : <mark>Graph</mark> G Ouput : <mark>Graph</mark> G that differs from G in exactly one swap ( i.e. , ( G , G ) ∈ T ) repeat Select edges ( i , j ) , ( k , l ) ∈ E ( G ) uniformly at random until ( i , l ) ∈ E ( G ) and ( k ,<br>2: Input: <mark>Graph</mark> GD, number of random walk steps km Ouput: <mark>Graph</mark> G with the same degree sequences as GD \n<br>",
    "Arabic": "رسم بياني",
    "Chinese": "图",
    "French": "graphe",
    "Japanese": "グラフ",
    "Russian": "граф"
  },
  {
    "English": "graph Laplacian",
    "context": "1: It was shown (Theorem 2 (Balcilar et al., 2021a)) that it is only the maximal eigenvalue λ max of the <mark>graph Laplacian</mark> used in the layers of ChebNet that may result in the separation power of ChebNet to go beyond 1-WL.<br>2: H Z y U X G S h S g η w S w h j 0 L Figure 1: \n The graphical representation of our model, where X is the continuous view, Z is the ordinal view, y are the ordinal labels and L is the <mark>graph Laplacian</mark> generated from the LD structure. sensitive data source associations.<br>",
    "Arabic": "لابلاسيان الرسم البياني",
    "Chinese": "图拉普拉斯矩阵",
    "French": "laplacien du graphe",
    "Japanese": "グラフラプラシアン",
    "Russian": "Лапласиан графа"
  },
  {
    "English": "graph Transformer",
    "context": "1: (1) Supervised methods: these methods directly train a GNN model on a specific task and then directly infer the result. We here take three famous GNN models including GAT [32], GCN [34], and <mark>Graph Transformer</mark> [25] (short as GT).<br>2: (SUN)  is developed based on the symmetry analysis of a series of existing Subgraph GNNs and an upper bound on their expressive power, which theoretically unifies previous architectures and performs well across several graph representation learning benchmarks. Last, we compare several <mark>Graph Transformer</mark> models.<br>",
    "Arabic": "مُحوِّل الرَّسْم البيانيّ",
    "Chinese": "图Transformer",
    "French": "Transformateur de graphe",
    "Japanese": "グラフ トランスフォーマー",
    "Russian": "Графовый трансформер"
  },
  {
    "English": "graph adjacency matrix",
    "context": "1: We perform a continuous relaxation of a <mark>graph adjacency matrix</mark> T, where we require its values to be bounded in the [0, 1] range: \n L + T = t∈T max(−t, 0) + t∈T max(t − 1, 0). (7) \n<br>",
    "Arabic": "مصفوفة التجاور للرسم البياني",
    "Chinese": "图邻接矩阵",
    "French": "matrice d'adjacence de graphe",
    "Japanese": "グラフ隣接行列",
    "Russian": "матрица смежности графа"
  },
  {
    "English": "graph attention",
    "context": "1: The static <mark>graph attention</mark> generates a static representation for a graph, which will be used to augment the semantics of a word in a post.<br>2: First, BERT is able to better model political jargon in debate transcripts post fine-tuning. Second, GPolS enhances text features through a context propagation mechanism via <mark>graph attention</mark> by modeling speaker-self, intra-party, and motion level context. The additional context that GPolS adds by learning the latent patterns between related transcripts, sets GPolS apart from all the baselines.<br>",
    "Arabic": "انتباه الرسم البياني",
    "Chinese": "图注意力",
    "French": "attention de graphe",
    "Japanese": "グラフアテンション",
    "Russian": "графовое внимание"
  },
  {
    "English": "graph attention mechanism",
    "context": "1: 4) on more than 33,000 transcripts from the UK House of Commons, we demonstrate GPolS's ability for stance analysis in parliamentary debates (Sec. 5). Lastly, we visualize GPolS's <mark>graph attention mechanism</mark> (Sec. 5.3) and token-level attention (Sec.<br>2: The dynamic <mark>graph attention mechanism</mark> is a hierarchical, top-down process. It first attentively reads all the knowledge graphs and then attentively reads all the triples in each graph for final word generation.<br>",
    "Arabic": "آلية انتباه الرسم البياني",
    "Chinese": "图注意力机制",
    "French": "mécanisme d'attention sur les graphes",
    "Japanese": "グラフ注意メカニズム (Graph Attention Mechanism)",
    "Russian": "механизм графового внимания"
  },
  {
    "English": "graph attention network",
    "context": "1: For argument role classification, the precision enhances 7.52% compared with the best-reported model PLMEE [19]. It may prove that our lexicon-based <mark>graph attention network</mark> and event-based BERT model can learn better of the event representation.<br>2: To this end, there are many effective neural network structures proposed such as <mark>graph attention network</mark> (GAT) [32], graph convolution network (GCN) [34], Graph Transformer [25].<br>",
    "Arabic": "شبكة انتباه الرسم البياني",
    "Chinese": "图注意力网络",
    "French": "réseau d'attention de graphe",
    "Japanese": "グラフアテンションネットワーク",
    "Russian": "графическая сеть внимания"
  },
  {
    "English": "graph classification",
    "context": "1: It thus shows that G-Mixup is capable of mixing up graphs. Our main contributions are highlighted as follows: Firstly, we propose G-Mixup to augment the training graphs for <mark>graph classification</mark>. Since directly mixing up graphs is intractable, G-Mixup mixes the graphons of different classes of graphs to generate synthetic graphs.<br>2: When we treat the target node's label as this induced graph label, we can easily translate the node classification problem into <mark>graph classification</mark>; Similarly, we present an induced graph for a pair of nodes in Figure 4b.<br>",
    "Arabic": "تصنيف الرسم البياني",
    "Chinese": "图分类",
    "French": "classification de graphes",
    "Japanese": "グラフ分類",
    "Russian": "классификация графов"
  },
  {
    "English": "graph clustering",
    "context": "1: It would be interesting to use these to determine an optimal choice of the connectivity parameter k or r of the graphs (we have already proved such results in a completely different <mark>graph clustering</mark> setting, cf. Maier et al., 2007). Another extension which does not look too difficult is obtaining uniform convergence results.<br>2: Gaining more understanding of this effect will be an important direction of research if one wants to understand the nature of different <mark>graph clustering</mark> criteria.<br>",
    "Arabic": "تجميع الرسم البياني",
    "Chinese": "图聚类",
    "French": "regroupement de graphes",
    "Japanese": "グラフクラスタリング",
    "Russian": "кластеризация графов"
  },
  {
    "English": "graph construction",
    "context": "1: This demonstrates that MAC can still perform well even if the initial correspondence set is directly utilized as input without any filtering. Graph construction choices. We test the performance of MAC by using different <mark>graph construction</mark> approaches.<br>2: Cunningham [11] characterizes cut functions and gives a general-purpose <mark>graph construction</mark> for them. It can be shown that the set of cut functions is a strict subset of F 2 . We allow a more general <mark>graph construction</mark>; as a result, we can minimize a larger class of functions, namely, regular functions in F 3 .<br>",
    "Arabic": "بناء الرسم البياني",
    "Chinese": "图构建",
    "French": "construction de graphe",
    "Japanese": "グラフ構築",
    "Russian": "построение графа"
  },
  {
    "English": "graph contrastive learning",
    "context": "1: Table 9 presents the experimental results. The results indicate that DropMessage consistently outperforms other random dropping methods. Comparison to Random Augmentation Methods. Random dropping methods are similar to random augmentation techniques used in <mark>graph contrastive learning</mark>. We compare the performance of our proposed DropMessage with some widely-used augmentation techniques (Ding et al.<br>2: Here we present a case that does not need to tune a task head and we evaluate its feasibility in section 4.4. Prompt without Task Head Tuning: Pretext: GraphCL [36], a <mark>graph contrastive learning</mark> task that tries to maximize the agreement between a pair of views from the same graph. Downstream Tasks: node/edge/graph classification.<br>",
    "Arabic": "التعلم التباينيّ للرسم البياني",
    "Chinese": "图对比学习",
    "French": "apprentissage contrastif de graphes",
    "Japanese": "グラフ対照学習",
    "Russian": "контрастное обучение графов"
  },
  {
    "English": "graph convolution",
    "context": "1: • GAT [28] leverages masked self-attentional layers to address the shortcomings of prior GNNs based on <mark>graph convolutions</mark> or their approximations, and enables specifying different weights to different nodes in a neighborhood.<br>2: Our core model uses five layers of <mark>graph convolutions</mark> as defined by Kipf and Welling [2016], each followed by batch normalization and ReLU activation, and finally two fully-connected layers with dropout. For tasks requiring a single output for the entire molecular system, we use global pooling to aggregate over nodes.<br>",
    "Arabic": "الإلتواء الرسم البياني",
    "Chinese": "图卷积",
    "French": "convolution de graphe",
    "Japanese": "グラフ畳み込み",
    "Russian": "графовая свертка"
  },
  {
    "English": "graph convolution network",
    "context": "1: To this end, there are many effective neural network structures proposed such as graph attention network (GAT) [32], <mark>graph convolution network</mark> (GCN) [34], Graph Transformer [25].<br>",
    "Arabic": "شبكة تحويل الرسوم البيانية",
    "Chinese": "图卷积网络",
    "French": "Réseau de convolution sur graphe",
    "Japanese": "グラフ畳み込みネットワーク",
    "Russian": "сеть свертки графов"
  },
  {
    "English": "graph convolutional network",
    "context": "1: DiffuPose [10] adopted the <mark>graph convolutional network</mark> as a denoising function to explicitly learn the connectivity between human joints.<br>2: A <mark>graph convolutional network</mark> (GCN) was mostly employed to encode the graph embeddings of events, but its performance is unsatisfactory since the sparsity of ATOMIC limits the information propagation on the GCN (Malaviya et al., 2020).<br>",
    "Arabic": "شبكة تلافيفية الرسم البياني",
    "Chinese": "图卷积网络",
    "French": "réseau de convolution de graphe",
    "Japanese": "グラフ畳み込みネットワーク",
    "Russian": "графовая сверточная сеть"
  },
  {
    "English": "graph cut",
    "context": "1: Unlike simulated annealing, <mark>graph cut</mark> methods cannot be applied to an arbitrary energy function; instead, for each energy function to be minimized, a careful graph construction must be developed.<br>2: The importance of energy functions of binary variables does not arise simply from the expansion move algorithm. Instead, it results from the fact that a <mark>graph cut</mark> effectively assigns one of two possible values to each vertex of the graph. So, in a certain sense, any energy minimization construction based on <mark>graph cut</mark>s relies on intermediate binary variables.<br>",
    "Arabic": "قص الرسم البياني",
    "Chinese": "图割",
    "French": "coupe de graphe",
    "Japanese": "グラフカット",
    "Russian": "разрез графа"
  },
  {
    "English": "graph cut algorithm",
    "context": "1: As the energy function is NP-hard to minimize exactly, we give a <mark>graph cut algorithm</mark> that computes a local minimum in a strong sense. We handle all camera configurations where voxel coloring can be used, which is a large and natural class. Experimental data demonstrates the effectiveness of our approach.<br>",
    "Arabic": "خوارزمية قطع الرسم البياني",
    "Chinese": "图割算法",
    "French": "algorithme de coupe de graphe",
    "Japanese": "グラフカットアルゴリズム",
    "Russian": "алгоритм разрезания графа"
  },
  {
    "English": "graph dataset",
    "context": "1: Based on our auto-search system PaSca, we discover new scalable GNN instances from the proposed design space for different accuracy-efficiency requirements. Extensive experiments on ten <mark>graph datasets</mark> demonstrate the superior training scalability/efficiency and performance of searched representatives given by PaSca among competitive baselines.<br>",
    "Arabic": "مجموعة بيانات الرسم البياني",
    "Chinese": "图数据集",
    "French": "ensemble de données de graphes",
    "Japanese": "グラフデータセット",
    "Russian": "набор графовых данных"
  },
  {
    "English": "graph datum",
    "context": "1: data, <mark>graph data</mark>, which is defined on non-Euclidean space with multi-modality, is hard to be handled by conventional data augmentation methods (Ding et al. 2022c). To address this problem, an increasing number of <mark>graph data</mark> augmentation methods have been proposed, which include feature-wise (Velickovic et al.<br>2: With the rapidly increasing amount of <mark>graph data</mark>, the similarity search problem, which identifies similar vertices in a graph, has become an important problem with many applications, including web analysis [Jeh and Widom 2002;Liben-Nowell and Kleinberg 2007], graph clustering [Yin et al. 2006;Zhou et al.<br>",
    "Arabic": "بيانات الرسم البياني",
    "Chinese": "图数据",
    "French": "donnée de graphe",
    "Japanese": "グラフデータ",
    "Russian": "данные графа"
  },
  {
    "English": "graph diameter",
    "context": "1: Moreover, whether achieving higher-order WL expressiveness is necessary and helpful for real-world tasks has been questioned by recent works (Veličković, 2022). Structural metrics. Another line of works thus sought different metrics to measure the expressive power of GNNs. Several popular choices are the ability of counting substructures ( Arvind et al. , 2020 ; Chen et al. , 2020 ; , detecting cycles ( Loukas , 2020 ; Vignac et al. , 2020 ; Huang et al. , 2023 ) , calculating the <mark>graph diameter</mark> ( Garg et al. , 2020 ; Loukas , 2020 ) or other graphrelated ( combinatorial )<br>",
    "Arabic": "قُطْر الرسم البياني",
    "Chinese": "图直径",
    "French": "diamètre du graphe",
    "Japanese": "グラフ直径",
    "Russian": "диаметр графа"
  },
  {
    "English": "graph embedding",
    "context": "1: Furthermore, if one uses a readout layer in basic GNNs to obtain a <mark>graph embedding</mark>, one typically applies a function ro : R dt → R dt in the form of ro v∈VG F (t) v , in which aggregation takes places over all vertices of the graph.<br>2: Copyright 2009 by the author(s)/owner(s). cover a low-dimensional set of coordinates for each vertex that implicitly encodes the graph's binary connectivity. Graph embedding algorithms place nodes at points on some surface (e.g. Euclidean space) and connect points with an arc if the nodes have an edge between them.<br>",
    "Arabic": "تضمين الرسم البياني",
    "Chinese": "图嵌入",
    "French": "plongement de graphe",
    "Japanese": "グラフ埋め込み",
    "Russian": "вложение графа"
  },
  {
    "English": "graph generator",
    "context": "1: F.1 SYNTHETIC TASKS Data Generation and Evaluation Metrics. We carefully design several <mark>graph generators</mark> to examine the expressive power of compared models on graph biconnectivity tasks. First, we include the two families of graphs presented in Examples C.9 and C.10 (Appendix C.2).<br>2: Our results have potential relevance in multiple settings, including 'what if' scenarios; in forecasting of future parameters of computer and social networks; in anomaly detection on monitored graphs; in designing graph sampling algorithms; and in realistic <mark>graph generators</mark>.<br>",
    "Arabic": "مُنشئ الرسوم البيانية",
    "Chinese": "图生成器",
    "French": "générateur de graphes",
    "Japanese": "グラフ生成器",
    "Russian": "генератор графов"
  },
  {
    "English": "graph isomorphism",
    "context": "1: As a result, it can be used as a necessary test for <mark>graph isomorphism</mark> by comparing the multisets {{χ G (u) : u ∈ V G }} and {{χ H (u) : u ∈ V H }}, which we call the graph representations.<br>2: The 1-dimensional Weisfeiler-Lehman test proceeds in iterations, which we index by h and which comprise the following steps: \n Algorithm 1 One iteration of the 1-dimensional Weisfeiler-Lehman test of <mark>graph isomorphism</mark> 1: Multiset-label determination \n • For h = 1, set M h (v) := l 0 (v) = L(v) \n<br>",
    "Arabic": "تطابق الرسوم البيانية",
    "Chinese": "图同构",
    "French": "isomorphisme de graphes",
    "Japanese": "グラフ同型性",
    "Russian": "изоморфизм графов"
  },
  {
    "English": "graph kernel",
    "context": "1: In this Section, we propose a new framework for graph similarity that is based on the concept of k-core, and we show how existing <mark>graph kernels</mark> can be plugged into the framework to produce more powerful kernels.<br>2: The framework is based on the k-core decomposition of graphs and is applicable to any graph comparison algorithm. • We demonstrate our framework on four <mark>graph kernels</mark>, namely the graphlet kernel, the shortest path kernel, the Weisfeiler-Lehman subtree kernel, and the pyramid match kernel.<br>",
    "Arabic": "لب الرسم البياني",
    "Chinese": "图核",
    "French": "noyau de graphe",
    "Japanese": "グラフカーネル",
    "Russian": "графовое ядро"
  },
  {
    "English": "graph laplacian matrix",
    "context": "1: Proof. Let R ∈ R n×n be the RD matrix. Based on Theorem E.1, R can be expressed as R = diag(M)11 ⊤ + 11 ⊤ diag(M) − 2M, where M = L + 1 n 11 ⊤ −1 and L is the <mark>graph Laplacian matrix</mark>.<br>2: p(H|L) = j N (h j |0, L −1 ) = j N (0|h j , L −1 ) = p(0|H, L), \n where L is the <mark>graph Laplacian matrix</mark> of the LD structure.<br>",
    "Arabic": "مصفوفة لابلاس للرسم البياني",
    "Chinese": "图拉普拉斯矩阵",
    "French": "matrice laplacienne du graphe",
    "Japanese": "グラフのラプラシアン行列",
    "Russian": "матрица Лапласа графа"
  },
  {
    "English": "graph learning",
    "context": "1: where˜is the degree of node obtained from the adjacency matrix with self-connections˜= + . Recently, some GNN variants adopt the decoupled neural message passing (DNMP) for better <mark>graph learning</mark>. More details can be found in Appendix A.2. Scalable GNN Instances.<br>2: Recent works also consider how to make <mark>graph learning</mark> more adaptive when data annotation is insufficient or how to transfer the model to a new domain, which triggered many graph pre-training studies instead of traditional supervised learning. Graph Pre-training.<br>",
    "Arabic": "تعلم الرسوم البيانية",
    "Chinese": "图学习",
    "French": "apprentissage de graphes",
    "Japanese": "グラフ事前学習",
    "Russian": "обучение на графах"
  },
  {
    "English": "graph matching",
    "context": "1: Zhou and De la Torre [38] introduced a novel factorization of the matrix M that is generally applicable to all stateof-the-art <mark>graph matching</mark> methods. It explicitly exposes the graph structure of the set of points and the unary and pairwise scores between nodes and edges, respectively, \n<br>2: In this paper, we have proposed a tensor-based algorithm for high-order <mark>graph matching</mark>. We have reached state-ofthe-art performance by using simple potentials which are invariant to rigid, affine or projective transformations.<br>",
    "Arabic": "مطابقة الرسم البياني",
    "Chinese": "图匹配",
    "French": "appariement de graphes",
    "Japanese": "グラフマッチング",
    "Russian": "сопоставление графов"
  },
  {
    "English": "graph mining",
    "context": "1: Graph kernels have recently evolved into a branch of kernel machines that reaches deep into <mark>graph mining</mark>.<br>",
    "Arabic": "تعدين الرسوم البيانية",
    "Chinese": "图挖掘",
    "French": "extraction de graphes",
    "Japanese": "グラフマイニング",
    "Russian": "майнинг графов"
  },
  {
    "English": "graph model",
    "context": "1: Here denotes the error bound between the manipulated graph and the prompting graph w.r.t. their representations from the pre-trained <mark>graph model</mark>. This error bound is related to some non-linear layers of the model (unchangeable) and the quality of the learned prompt (changeable), which is promising to be further narrowed down by a more advanced prompt scheme.<br>2: As previously discussed, our method only needs to cache the prompt parameters, which are far smaller than the original <mark>graph model</mark>. For the graph features and structures, traditional methods usually need to feed the whole graph into a <mark>graph model</mark>, which needs huge memory to cache these contents.<br>",
    "Arabic": "نموذج الرسم البياني",
    "Chinese": "图模型",
    "French": "modèle de graphe",
    "Japanese": "グラフモデル",
    "Russian": "модель графа"
  },
  {
    "English": "graph neural Networks",
    "context": "1: <mark>Graph Neural Networks</mark> (GNNs) (Merkwirth & Lengauer, 2005;Scarselli et al., 2009) cover many popular deep learning methods for graph learning tasks (see Hamilton (2020) for a recent overview). These methods typically compute vector embeddings of vertices or graphs by relying on the underlying adjacency information.<br>2: Recently deep learning has been widely adopted to graph analysis. <mark>Graph Neural Networks</mark> (GNNs) (Wu et al., 2020;Zhou et al., 2020a;Zhang et al., 2020;Xu et al., 2018) have shown promising performance on graph classification.<br>",
    "Arabic": "شبكات عصبية بيانية",
    "Chinese": "图神经网络",
    "French": "Réseaux neuronaux sur graphes",
    "Japanese": "グラフニューラルネットワーク (GNNs)",
    "Russian": "графовые нейронные сети"
  },
  {
    "English": "graph neural network",
    "context": "1: Among the reference TCF models, our method of modeling the temporal trajectories of user and item embeddings is similar that of the Temporal MF model [26]. The key difference is that we employ <mark>graph neural network</mark> to learn the embeddings at each time step, while the Temporal MF models follow a matrix-factorization approach.<br>2: 3 This model is the same as the \"BERT + PHQA\" baseline in Qu et al. (2019a). GraphFlow. Chen et al. (2020) propose a recurrent <mark>graph neural network</mark> on top of BERT embeddings to model the dependencies between the question, the history and the passage. HAM. Qu et al.<br>",
    "Arabic": "شبكة عصبية بيانية",
    "Chinese": "图神经网络",
    "French": "réseau de neurones graphiques",
    "Japanese": "グラフニューラルネットワーク",
    "Russian": "графовая нейронная сеть"
  },
  {
    "English": "graph node",
    "context": "1: We have also considered versions in which a single <mark>graph node</mark> can \"reside\" at two different nodes of the tree Γ, allowing for <mark>graph node</mark>s to be members of different communities. We do not go into further details of these extensions in this version of the paper.<br>2: With these token vectors, the input graph can be reformulated by adding the -th token to <mark>graph node</mark> (e.g.,x = x + p ). Then, we replace the input features with the prompted features and send them to the pre-trained model for further processing.<br>",
    "Arabic": "عقدة الرسم البياني",
    "Chinese": "图节点",
    "French": "nœud de graphe",
    "Japanese": "グラフノード",
    "Russian": "узел графа"
  },
  {
    "English": "graph partitioning",
    "context": "1: Figure 9 shows a typical transition from one option to the other, first going upward with option 0 then switching to option 1 downward. Options with a similar structure were also found in this game by (Krishnamurthy et al. 2016) using an option discovery algorithm based on <mark>graph partitioning</mark>.<br>2: Community Discovery. The community mining algorithms can be broadly classified into two main categories: <mark>graph partitioning</mark> based and modularity based approaches.<br>",
    "Arabic": "تقسيم الرسم البياني",
    "Chinese": "图划分",
    "French": "partitionnement de graphe",
    "Japanese": "グラフ分割",
    "Russian": "разбиение графа"
  },
  {
    "English": "graph pattern",
    "context": "1: To obtain hom(P r i , G v ) we need to create an indicator function for the <mark>graph pattern</mark> P i and then count how many times this indicator value is equal to one in G. The indicator function for P i is simply given by the expression uv∈EP i E(x u , x v ).<br>2: , `` jiawei han '' , ... example transactions : \n 1)mining frequent patterns without candidate... 2)... mining closed frequent <mark>graph pattern</mark>s semantically similar patterns: \n \"frequent sequential pattern\", \"<mark>graph pattern</mark>\" \"maximum pattern\", \"frequent close pattern\", ...<br>",
    "Arabic": "نمط الرسم البياني",
    "Chinese": "图模式",
    "French": "motif graphique",
    "Japanese": "グラフパターン",
    "Russian": "шаблон графа"
  },
  {
    "English": "graph representation",
    "context": "1: Given a <mark>graph representation</mark> G, we encode visual and spatial cues of each object as H = {h i } = ψ(G), where h i is the embedding vector of the i-th object and ψ is the geometric transformer network [40].<br>2: However, the original formulation of DS-WL  only outputs a <mark>graph representation</mark> {{{{χ Gi (v) : v ∈ V}} : G i ∈ B π G }} rather than outputs each node color, which does not suit the node-level tasks (e.g., finding cut vertices).<br>",
    "Arabic": "تمثيل الرسم البياني",
    "Chinese": "图表示",
    "French": "représentation graphique",
    "Japanese": "グラフ表現",
    "Russian": "графовое представление"
  },
  {
    "English": "graph sampling",
    "context": "1: , G k } with label ( 1 , 0 ) … … … 1 ) graphon estimation 3 ) <mark>graph sampling</mark> W I = 0.5 ⇤ W G + 0.5 ⇤ W H 1 ) graphon estimation 2 ) graphon mixup Figure 1 . An overview of G-Mixup. The task is binary graph classification.<br>2: In particular, Figure 1(a) shows that the scalability of GraphSAGE is limited even when the mini-batch training and <mark>graph sampling</mark> method are adopted. Figure 1(b) further shows that the scalability is mainly bottlenecked by the aggregation procedure in which high data loading cost is incorporated to gather neighborhood information.<br>",
    "Arabic": "أخذ عينات من الرسم البياني",
    "Chinese": "图采样",
    "French": "échantillonnage de graphe",
    "Japanese": "グラフサンプリング",
    "Russian": "выборка графа"
  },
  {
    "English": "graph structure",
    "context": "1: Zhou and De la Torre [38] introduced a novel factorization of the matrix M that is generally applicable to all stateof-the-art graph matching methods. It explicitly exposes the <mark>graph structure</mark> of the set of points and the unary and pairwise scores between nodes and edges, respectively, \n<br>2: Likewise, although [10] regularize a statistical topic model with a harmonic regularization based on a <mark>graph structure</mark>, the concept hierarchy of topics covered in the communities is not shown explicitly. In this paper, a mining strategy is proposed for discovering topic-based collaborative community.<br>",
    "Arabic": "بنية الرسم البياني",
    "Chinese": "图结构",
    "French": "structure de graphe",
    "Japanese": "グラフ構造",
    "Russian": "структура графа"
  },
  {
    "English": "graph theory",
    "context": "1: (2015), <mark>graph theory</mark> Rohe (2018), prediction Copas (1983); Porco et al. (2015), dimensionality reduction Laparra et al. (2015), feature selection Gallagher et al. (2017) and many more; see more examples in Golub and Van Loan (2012).<br>2: In this paper, we systematically study the problem of designing expressive GNNs from a novel perspective of graph biconnectivity. Biconnectivity has long been a central topic in <mark>graph theory</mark> (Bollobás, 1998).<br>",
    "Arabic": "نظرية الرسوم البيانية",
    "Chinese": "图论",
    "French": "théorie des graphes",
    "Japanese": "グラフ理論",
    "Russian": "теория графов"
  },
  {
    "English": "graph topology",
    "context": "1: different graphs ; ( iii ) <mark>graph topology</mark> between classes are divergent , where the topologies of a pair of graphs from different classes are usually different while the topologies of those from the same class are usually similar . Thus, it is nontrivial to directly adopt the Mixup strategy to graph data.<br>2: The following conceptwhich is the main focus of this paper -permits to predicate the solvability of a problem based on the <mark>graph topology</mark> only. Definition 2. A graph G is called solvable if it is solvable for a generic configuration of cameras. Necessary conditions for viewing graph solvability [19,37] allow to quickly prune the solvability candidates.<br>",
    "Arabic": "توبولوجيا الرسم البياني",
    "Chinese": "图拓扑结构",
    "French": "topologie du graphe",
    "Japanese": "グラフトポロジー",
    "Russian": "топология графа"
  },
  {
    "English": "graph traversal",
    "context": "1: The algorithm can be viewed as a fine-grained <mark>graph traversal</mark> compared to classical breadth-first search (BFS), where the vertices are visited only by increasing distance to the start vertex. LBFS keeps this property, but introduces additional constraints on the ordering τ , in which the vertices are visited (τ is called an LBFS ordering).<br>",
    "Arabic": "عبور الرسوم البيانية",
    "Chinese": "图遍历",
    "French": "parcours de graphe",
    "Japanese": "グラフ走査",
    "Russian": "обход графа"
  },
  {
    "English": "graph-base approach",
    "context": "1: In this paper, we study the problem of mitigating radicalization pathways using a <mark>graph-based approach</mark>. Specifically, we model the set of recommendations of a \"what-to-watch-next\" recommender as a -regular directed graph where nodes correspond to content items, links to recommendations, and paths to possible user sessions.<br>",
    "Arabic": "نهج قائم على الرسم البياني",
    "Chinese": "基于图的方法",
    "French": "approche basée sur les graphes",
    "Japanese": "グラフベースのアプローチ",
    "Russian": "Подход на основе графа"
  },
  {
    "English": "graph-base dependency parsing",
    "context": "1: (2012) similarly use a linear cascade instead of a tree. Weiss and Taskar (2010) prune output labels in the context of structured prediction through a cascade of increasingly complex models, and Rush and Petrov (2012) successfully apply these structured prediction cascades to the task of <mark>graph-based dependency parsing</mark>.<br>2: Coarse-to-fine inference has been extensively used to speed up structured prediction models. The general idea is simple: use a coarse model where inference is cheap to prune the search space for more complex models. In this work, we present a multipass coarse-to-fine architecture for <mark>graph-based dependency parsing</mark>.<br>",
    "Arabic": "تحليل التبعية القائم على الرسم البياني",
    "Chinese": "基于图的依存句法分析",
    "French": "analyse syntaxique par dépendances basée sur des graphes",
    "Japanese": "グラフベース依存構造解析",
    "Russian": "графовый анализ зависимостей"
  },
  {
    "English": "graph-base learning",
    "context": "1: In <mark>graph-based learning</mark> approaches one constructs a graph whose vertices are labeled and unlabeled examples, and whose weighted edges encode the degree to which the examples they link have the same label (Zhu et al., 2003).<br>",
    "Arabic": "التعلم القائم على الرسم البياني",
    "Chinese": "基于图的学习",
    "French": "apprentissage basé sur les graphes",
    "Japanese": "グラフベース学習",
    "Russian": "графовое обучение"
  },
  {
    "English": "graph-base method",
    "context": "1: However, the key and obvious drawback is that the predictions can be severely misled by false positive labels. To disambiguate the groundtruth from the candidates , identification-based methods [ 29 ] , which regard the ground-truth as a latent variable , have recently attracted increasing attention ; representative approaches include maximum margin-based methods [ 30 ] , [ 31 ] , <mark>graph-based methods</mark> [ 6 ] , [ 7 ] , [ 32 ] , [ 33 ] , and clusteringbased approaches<br>2: They can be further divided into two groups, including <mark>graph-based methods</mark> based on the topology of semantic objects [16], [18], [17] and geometric-based methods based on fine geometry of semantics (e.g., shape, contour, density) [19], [20].<br>",
    "Arabic": "الأساليب المعتمدة على الرسوم البيانية",
    "Chinese": "基于图的方法",
    "French": "méthode basée sur les graphes",
    "Japanese": "グラフベース法",
    "Russian": "метод на основе графов"
  },
  {
    "English": "graph-base model",
    "context": "1: We postulate this to fine-tuning BERT to obtain rich embeddings that better capture the context within each debate transcript. We observe that graphbased models (Deepwalk, GPolS) outperform text-only models (SVM, MLP, BERT-MLP), reiterating the presence of similarity between related transcripts.<br>",
    "Arabic": "نموذج مبني على الرسوم البيانية",
    "Chinese": "基于图的模型",
    "French": "modèle basé sur un graphe",
    "Japanese": "グラフベースモデル",
    "Russian": "модель на основе графов"
  },
  {
    "English": "graph-base representation",
    "context": "1: Graphs are well-studied structures which are utilized to model entities and their relationships. In recent years, graphbased representations have become ubiquitous in many application domains. For instance, social networks, protein and gene regulatory networks, and textual documents are commonly represented as graphs.<br>",
    "Arabic": "التمثيل على أساس الرسوم البيانية",
    "Chinese": "基于图的表示",
    "French": "représentation basée sur un graphe",
    "Japanese": "グラフベース表現",
    "Russian": "графовое представление"
  },
  {
    "English": "graph-level task",
    "context": "1: Then, to further narrow the gap between various graph tasks and state-of-the-art pre-training strategies, we further study the task space of various graph applications and reformulate downstream problems to the <mark>graph-level task</mark>.<br>2: In most cases, the fine-tuning method can output meaningful results with a few steps of tuning but it can still encounter a negative transfer problem. Second, the <mark>graph-level task</mark> has better adaptability than the node-level task for the edge-level target, which is in line with our previous intuition presented in Figure 3 (section 3.2).<br>",
    "Arabic": "مهمة على مستوى الرسم البياني",
    "Chinese": "图级任务",
    "French": "tâche au niveau du graphe",
    "Japanese": "グラフレベルのタスク",
    "Russian": "задача на уровне графа"
  },
  {
    "English": "graphical model",
    "context": "1: For non-submodular parameters, these methods provide partial optimality guarantees for variables that get integral values. This observation is exploited in [14] to design an iterative probing scheme to expand the set of variables with optimal assignments. However, this scheme is useful only for the case when the <mark>graphical model</mark> has a few non-submodular edges.<br>2: This graph can be viewed as a subgraph of the <mark>graphical model</mark> representation for the given CRF. Theorem 4: SOCP relaxations (and the equivalent QP relaxations) which define constraints only using graphs G k = (V k , E k ) which form (arbitrarily large) trees are dominated by the LP-S relaxation.<br>",
    "Arabic": "نموذج بياني",
    "Chinese": "图形模型",
    "French": "modèle graphique",
    "Japanese": "グラフィカルモデル",
    "Russian": "графическая модель"
  },
  {
    "English": "graphlet",
    "context": "1: The first class consists of kernels that compare local substructures of graphs (i. e. trees, cycles, <mark>graphlets</mark>), while the second class includes kernels that capture global properties of graphs and are sensitive to the large scale structure of graphs.<br>2: Given two cores of a graph C i and C j with i < j, all the <mark>graphlets</mark> found in C j will also be present in C i .<br>",
    "Arabic": "مقطع",
    "Chinese": "小子图",
    "French": "graphlet",
    "Japanese": "グラフレット",
    "Russian": "графлет"
  },
  {
    "English": "graphlet kernel",
    "context": "1: For some base kernels, one might be able to exploit the fact that high-order cores are contained into lower-order cores in order to perform some computations only once instead of repeating them for all cores. One example of such a base kernel is the <mark>graphlet kernel</mark>.<br>2: We apply the proposed framework to the following four graph kernels: \n (1) <mark>graphlet kernel</mark> (GR) [Shervashidze et al., 2009]: The <mark>graphlet kernel</mark> counts identical pairs of graphlets (i. e. subgraphs with k nodes where k ∈ 3, 4, 5) in two graphs.<br>",
    "Arabic": "نواة الجرافليت",
    "Chinese": "图基核",
    "French": "noyau graphlet",
    "Japanese": "グラフレットカーネル",
    "Russian": "ядро графлет"
  },
  {
    "English": "greedy",
    "context": "1: 5 and ran the 3 approximation techniques: <mark>Greedy</mark>, LBP and TRW on the test set comprising 4952 images from PASCAL VOC 2007 dataset.<br>2: Both M1 and MH drive down the error for a few rounds. But since boosting keeps creating harder distributions, very soon the small-tree learning algorithms <mark>Greedy</mark> and <mark>Greedy</mark>-Info are no longer able to meet the excessive requirements of M1 and MH respectively. However, our algorithm makes more reasonable demands that are easily met by <mark>Greedy</mark>.<br>",
    "Arabic": "جشع",
    "Chinese": "贪婪",
    "French": "glouton",
    "Japanese": "貪欲法",
    "Russian": "жадный"
  },
  {
    "English": "greedy algorithm",
    "context": "1: Therefore, we will provide suitable adaptions of the <mark>greedy algorithm</mark> to work on the CH-representations and on PNM triples in the next section. Transformability. Note, that the extraction scheme does not tie us to a certain path representation.<br>2: Unfortunately, this intuitive generalization of the <mark>greedy algorithm</mark> can perform arbitrarily worse than the optimal solution. Hence the <mark>greedy algorithm</mark> would pick s1. After selecting s1, we cannot afford s2 anymore, and our total reward would be ε. However, the optimal solution would pick s2, achieving total penalty reduction of B.<br>",
    "Arabic": "خوارزمية جشعة",
    "Chinese": "贪心算法",
    "French": "algorithme glouton",
    "Japanese": "貪欲なアルゴリズム",
    "Russian": "жадный алгоритм"
  },
  {
    "English": "greedy approach",
    "context": "1: While a naive strategy only allows for the solution of small instances of few hundred thousand nodes, our compact representation schemes for the underlying set systems and heuristic modifications of the standard <mark>greedy approach</mark> make the computation of a solution even for country-sized networks like that of Germany possible.<br>2: the standard <mark>greedy approach</mark> which guarantees a solution within a factor of O(log n) of the optimum (n being the number of nodes in the network). Unfortunately, it turns out that the difficulty of computing an ESC solution is already the instance construction.<br>",
    "Arabic": "النهج الجشع",
    "Chinese": "贪婪算法",
    "French": "approche gloutonne",
    "Japanese": "貪欲法",
    "Russian": "жадный подход"
  },
  {
    "English": "greedy decoding",
    "context": "1: {Greedy, CD-Empty, CD-Wrong} refer respectively to using <mark>greedy decoding</mark>, contrastive decoding with empty/wrong answer to obtain rationale tokens from the teacher. provided as input , we use the LAS metric ( Hase et al. , 2020 ) , whose core idea is to measure how well the rationales assist a simulator to predict the gold answers a * , computed as the difference between the task performance when the rationale is provided as input vs. when it is not , namely Acc ( qr →<br>2: We use the T5X library (Roberts et al., 2022). For inference we perform <mark>greedy decoding</mark> of the answers. We trained for 50k training steps with constant learning rate of 0.0001 with a batch size of 32. We select the best checkpoint on the factual validation set, prioritizing the standard performance criteria for QA models.<br>",
    "Arabic": "فك التشفير الجشع",
    "Chinese": "贪婪解码",
    "French": "décodage glouton",
    "Japanese": "貪欲なデコード",
    "Russian": "жадное декодирование"
  },
  {
    "English": "greedy inference",
    "context": "1: w = w s w a , Ψ(X,Y)= ij ψ(y i , y j , d ij ) i φ(x i , y i ) (5) \n where our <mark>greedy inference</mark> procedure solves \n Y * = arg max Y w T Ψ (X, Y ) (6)<br>2: We use <mark>greedy inference</mark>.<br>",
    "Arabic": "استدلال جشع",
    "Chinese": "贪婪推理",
    "French": "inférence gloutonne",
    "Japanese": "貪欲推論",
    "Russian": "жадный вывод"
  },
  {
    "English": "greedy maximization",
    "context": "1: This \"correctness of <mark>greedy maximization</mark>\" hinges upon condition (C2) as will also be demonstrated with a counterexample given later in Section 6.<br>2: It is straightforward to extend our <mark>greedy maximization</mark> procedure for optimizing (1) to solve (3). This is used for the per detection scoring presented in the result section. In practice, we approximate the marginal by m(y i = c) ≈ Δ(i, c) computed during the greedy optimization.<br>",
    "Arabic": "تعظيم الجشع",
    "Chinese": "贪婪最大化",
    "French": "maximisation gloutonne",
    "Japanese": "貪欲最大化",
    "Russian": "жадная максимизация"
  },
  {
    "English": "greedy method",
    "context": "1: We experimented with various ratios of cache size used by the <mark>greedy method</mark> versus total cache; details are omitted due to space constraints.<br>2: The first one is linear reconstruction, which approximates the document by linear combinations of the selected sentences. Optimizing the corresponding objective function is achieved through a <mark>greedy method</mark> which extracts sentences sequentially. The second one is non-negative linear reconstruction, which allows only additive, not subtractive, combinations among the selected sentences.<br>",
    "Arabic": "طريقة الجشع",
    "Chinese": "贪心方法",
    "French": "méthode gloutonne",
    "Japanese": "貪欲法",
    "Russian": "жадный метод"
  },
  {
    "English": "greedy optimization",
    "context": "1: = z m t , which is used as the input for the following module. The InfoNCE objective is used for its <mark>greedy optimization</mark>. This loss is calculated by contrasting the predictions of a module for its future representations z m t+k against negative samples z m j , which enforces each module to maximally preserve the information of its inputs.<br>2: Using a greedy supervised approach for training the feature model impedes performance, which suggests that mutual information maximization is unique in its direct applicability to <mark>greedy optimization</mark>. In comparison with the recently proposed Deep InfoMax model from Hjelm et al.<br>",
    "Arabic": "التحسين الجشع",
    "Chinese": "贪心优化",
    "French": "optimisation gloutonne",
    "Japanese": "貪欲最適化",
    "Russian": "жадная оптимизация"
  },
  {
    "English": "greedy policy",
    "context": "1: , s n } and m actions A = {a 1 , . . . , a m }. Let Θ be the parameter class defining Q-functions. Let F and G(Θ), as above, denote the class of expressible value functions and admissible <mark>greedy policies</mark> respectively.<br>2: We let F = {f θ : S ×A → R | θ ∈ Θ} denote the set of expressible value function approximators, and denote the class of admissible <mark>greedy policies</mark> by \n G(Θ) = π θ π θ (s) = argmax a∈A f θ (s, a), θ ∈ Θ .<br>",
    "Arabic": "السياسة الطماعية",
    "Chinese": "贪婪政策",
    "French": "politique gourmande",
    "Japanese": "貪欲方策",
    "Russian": "жадная стратегия"
  },
  {
    "English": "greedy search",
    "context": "1: We still take BT as an example, it typically uses beam search (Sennrich et al., 2016a) or <mark>greedy search</mark> (Lample et al., 2018a,c) to generate synthetic source sentences for each target monolingual sentence.<br>2: We now consider the feasibility of an exact solution to this learning problem in the simplest setting of <mark>greedy search</mark> using linear heuristic and cost functions represented by their weight vectors w H and w C respectively.<br>",
    "Arabic": "البحث الجشع",
    "Chinese": "贪心搜索",
    "French": "recherche gloutonne",
    "Japanese": "貪欲探索",
    "Russian": "жадный поиск"
  },
  {
    "English": "greedy strategy",
    "context": "1: The linear reconstruction problem is solved using a <mark>greedy strategy</mark> and the nonnegative reconstruction problem is solved using a multiplicative updating. The experimental results show that out DSDR (with both reconstruction types) can outperform other state-of-the-art summarization approaches.<br>",
    "Arabic": "استراتيجية جشعة",
    "Chinese": "贪心策略",
    "French": "stratégie gloutonne",
    "Japanese": "貪欲戦略",
    "Russian": "жадная стратегия"
  },
  {
    "English": "grid cell",
    "context": "1: However, there is little understanding of neural population structure in higher brain areas. As an example , we do not even understand why many different bespoke cellular responses exist for physical space , such as <mark>grid cells</mark> ( Hafting et al. , 2005 ) , object-vector cells ( Høydal et al. , 2019 ) , border vector cells ( Solstad et al. , 2008 ; Lever et al. , 2009 ) , band cells ( Krupic et<br>",
    "Arabic": "خلية الشبكة",
    "Chinese": "网格细胞",
    "French": "cellule de grille",
    "Japanese": "グリッド細胞",
    "Russian": "сетчатая клетка"
  },
  {
    "English": "grid search",
    "context": "1: We found it to be more efficient to use <mark>grid search</mark> on a holdout validation set for all three kernel parameters w (1) , θ α and θ β . The smoothness kernel parameters w (2) and θ γ do not significantly affect classification accuracy, but yield a small visual improvement.<br>2: We tune γ using <mark>grid search</mark> between 0.005 and 1 on the validation set to minimize the crossentropy loss between the preference probabilitieŝ p(Y 1 ≻ Y 2 ) and the human labels w. \n<br>",
    "Arabic": "بحث الشبكة",
    "Chinese": "网格搜索",
    "French": "recherche en grille",
    "Japanese": "グリッドサーチ",
    "Russian": "сеточный поиск"
  },
  {
    "English": "grid-world",
    "context": "1: The reward function f R can map an image of the domain to a high reward at the goal, and negative reward near an obstacle, while f P can encode deterministic movements in the <mark>grid-world</mark> that do not depend on the observation.<br>2: Each domain is represented by a 128 × 128 image patch, on which we defined a 16 × 16 <mark>grid-world</mark>, where each state was considered an obstacle if the terrain in its corresponding 8 × 8 image patch contained an elevation angle of 10 degrees or more, evaluated using an external elevation data base.<br>",
    "Arabic": "عالم الشبكة",
    "Chinese": "网格世界",
    "French": "monde-grille",
    "Japanese": "グリッドワールド",
    "Russian": "сетка-мир"
  },
  {
    "English": "ground atom",
    "context": "1: Let J be a pseudo-interpretation and let µ be a variable assignment such that J corresponds to µ. Then, 1. J |= α if and only if µ |= Pres(α) for each <mark>ground atom</mark> α, and 2. J |= r if and only if µ |= Pres(r) for each semi-ground rule r.<br>2: We write P j (i) to denote the <mark>ground atom</mark> that resulted from instantiating predicate P j with domain element i. Let X = { X 1 , ... , X k } be a decomposition of the set of <mark>ground atom</mark>s with X i = { P 1 ( i ) , P 2 ( i ) , ... , P N ( i ) } for every 1 ≤ i ≤ k. A renaming automorphism ( Bui , Huynh , and Riedel 2012<br>",
    "Arabic": "ذرة أرضية",
    "Chinese": "基础原子",
    "French": "atome de base",
    "Japanese": "グラウンドアトム",
    "Russian": "основной атом"
  },
  {
    "English": "ground set",
    "context": "1: Initially activating the k nodes corresponding to sets in a Set Cover solution results in activating all n nodes corresponding to the <mark>ground set</mark> U , and if any set A of k nodes has σ(A) ≥ n + k, then the Set Cover problem must be solvable.<br>2: A matroid is a pair M = ( F , S ) , where F is a <mark>ground set</mark> , and S is a collection of subsets of F with the following properties : ( i ) ∅ ∈ S , ( ii ) If A ∈ S , then for every subset B ⊆ A , B ∈ S , and ( iii ) For any A , B ∈ S with |B| < |A| , there exists an b ∈ B \\ A such that B ∪ { b } ∈ S. The rank of a matroid M is the size of the largest independent set in S. Using the definition of matroid , it can be easily seen that all inclusion-wise maximal independent sets (<br>",
    "Arabic": "مجموعة أساسية",
    "Chinese": "基础集合",
    "French": "ensemble de base",
    "Japanese": "基底集合",
    "Russian": "основное множество"
  },
  {
    "English": "ground truth label",
    "context": "1: ImageNet [29] is a benchmark for large-scale image recognition tasks and its images are assigned to a single out of 1000 possible <mark>ground truth labels</mark>. The dataset contains ≈1.2M training images, 50.000 validation images and 100.000 test images with average dimensionality of 482x415 pixels.<br>2: Despite the above challenges, we can indeed measure the similarity between the features of the network X and the <mark>ground truth labels</mark>. If the similarity is higher, we can say that the feature space of X contains more information regarding the true labels. Distance correlation enables this.<br>",
    "Arabic": "تسمية الحقيقة الأرضية",
    "Chinese": "真实标签",
    "French": "étiquette de vérité terrain",
    "Japanese": "正解ラベル",
    "Russian": "метка истины"
  },
  {
    "English": "ground-truth box",
    "context": "1: The total focal loss of an image is computed as the sum of the focal loss over all ∼100k anchors, normalized by the number of anchors assigned to a <mark>ground-truth box</mark>.<br>",
    "Arabic": "صندوق الحقيقة الأرضية",
    "Chinese": "真实边界框",
    "French": "boîte réelle",
    "Japanese": "正解ボックス",
    "Russian": "коробка истинного положения"
  },
  {
    "English": "grounded language learning",
    "context": "1: These insights pave the way for future research in <mark>grounded language learning</mark> in the open world.<br>",
    "Arabic": "تعلم اللغة المرتكزة",
    "Chinese": "基于实际情境的语言学习",
    "French": "apprentissage de langage ancré",
    "Japanese": "接地言語学習",
    "Russian": "Обоснованное обучение языку"
  },
  {
    "English": "grounded supervision",
    "context": "1: The model first acquires the ability to ground during pre-training, and then transfers this intrinsic ability to learn unseen words when <mark>grounded supervision</mark> is no longer available. Our empirical results show that learning to map words to their referents plays a significant role in grounded word acquisition.<br>",
    "Arabic": "الإشراف المرتكز",
    "Chinese": "基础监督",
    "French": "supervision ancrée",
    "Japanese": "グラウンデッド・スーパーバイジョン",
    "Russian": "наземное обучение"
  },
  {
    "English": "group normalization",
    "context": "1: y and z axes , and 4 for lighting , corresponding to k s , k d , l x and l y . • Upsample(s): nearest-neighbor upsampling with a scale factor of s. \n • GN(n): <mark>group normalization</mark> [67] with n groups.<br>",
    "Arabic": "تطبيع المجموعة",
    "Chinese": "组规范化",
    "French": "normalisation de groupe",
    "Japanese": "グループ正規化",
    "Russian": "групповая нормализация"
  },
  {
    "English": "group sparsity",
    "context": "1: In Section 4.5, we develop several methods for picking template orderings, based on ideas from <mark>group sparsity</mark> (Yuan and Lin, 2006;Swirszcz et al., 2009), and other techniques for feature subset-selection (Kohavi and John, 1997).<br>",
    "Arabic": "التباعد الجماعي",
    "Chinese": "组稀疏",
    "French": "\"parcimonie de groupe\"",
    "Japanese": "グループ疎性",
    "Russian": "групповая разреженность"
  },
  {
    "English": "gumbel noise",
    "context": "1: For this, we injected <mark>Gumbel noise</mark> into the label probabilities (details in the appendix) and analyzed the performance of POXM for logging policies with p ∈ {10, 20, 50}. We provide summary statistics for the three logging policies and report the results of POXM in Figure 2.<br>2: One way to mitigate this is to cleverly prune computation in regions where the maximum perturbed potential is unlikely to be found (Maddison et al., 2014;Chen & Ghahramani, 2016). Another approach exploits the product structure of the sample space in discrete graphical models, replacing i.i.d. <mark>Gumbel noise</mark> with a \"low-rank\" approximation.<br>",
    "Arabic": "ضوضاء جامبل",
    "Chinese": "古贝尔噪音",
    "French": "bruit de Gumbel",
    "Japanese": "ガンベルノイズ",
    "Russian": "гумбелевский шум"
  },
  {
    "English": "half-space",
    "context": "1: In more details, a first layer that extracts hyperplanes conditional on the input data, a second layer that groups hyperplanes in the form of <mark>half-spaces</mark> to create parts (convexes), and a third layer assembles parts together to reconstruct the overall object; see Figure 3. Layer 1: hyperplane extraction.<br>",
    "Arabic": "نصف فراغ",
    "Chinese": "半空间",
    "French": "demi-espace",
    "Japanese": "半空間",
    "Russian": "полупространство"
  },
  {
    "English": "hamiltonian Monte Carlo",
    "context": "1: Generally, mixing of the Markov chain is a potential computational concern, but by using <mark>Hamiltonian Monte Carlo</mark> we have had good results. Autocorrelation plots from the λ 1 (s) inference are shown in Figure 6.<br>2: There is a large literature on sparse approximations to GPs, e.g. Quiñonero-Candela and Rasmussen ( 2005), but we have not explored this topic. Mixing of Markov chains is an additional concern, but we have not found this to be a problem in practice due to our use of <mark>Hamiltonian Monte Carlo</mark>.<br>",
    "Arabic": "هاميلتونيان مونتي كارلو",
    "Chinese": "哈密顿蒙特卡罗",
    "French": "Monte Carlo hamiltonien",
    "Japanese": "ハミルトニアンモンテカルロ",
    "Russian": "Гамильтоновское Монте-Карло"
  },
  {
    "English": "hamming distance",
    "context": "1: Existing LSH functions can accommodate the <mark>Hamming distance</mark> [17], L p norms [7], and inner products [6], and such functions have been explored previously in the vision community [22,23,14]. In the following we present new algorithms to construct LSH functions for learned metrics. Specifically , we introduce a family of hash functions that accommodate learned Mahalanobis distances , where we want to retrieve examples x i for an input x q for which the value d A ( x i , x q ) resulting from ( 1 ) is small , or , in terms of the kernel form , for which the value of s<br>2: We have performed various fitness assignment methods for a selected individual, such as assigning the fitness value of the closest individual (in <mark>Hamming distance</mark>) or using the median value of the elite set. However, their empirical results (which are not shown here) exhibit no significant difference.<br>",
    "Arabic": "مسافة هامينغ",
    "Chinese": "海明距离",
    "French": "distance de Hamming",
    "Japanese": "ハミング距離",
    "Russian": "расстояние Хэмминга"
  },
  {
    "English": "hamming loss",
    "context": "1: For dependency trees, the loss of a tree is defined to be the number of words with incorrect parents relative to the correct tree. This is closely related to the <mark>Hamming loss</mark> that is often used for sequences (Taskar et al., 2003).<br>",
    "Arabic": "خسارة هامينغ",
    "Chinese": "海明损失",
    "French": "perte de Hamming",
    "Japanese": "ハミング損失",
    "Russian": "потеря Хэмминга"
  },
  {
    "English": "hand pose estimation",
    "context": "1: Next, we evaluate our method for <mark>hand pose estimation</mark> in depth images. We use the NYU hand pose dataset [38] that contains 72, 757 training frames and 8, 251 testing frames captured by 3 Kinect camerasone frontal and 2 side views.<br>2: To fully exploit the potential of the diffusion model in <mark>hand pose estimation</mark>, we propose HandDiff, a novel approach that incrementally refines the noise distribution to accurately derive a 3D hand pose from multi-modal inputs, including depth images and point clouds.<br>",
    "Arabic": "تقدير وضعية اليد",
    "Chinese": "手姿态估计",
    "French": "estimation de la pose de la main",
    "Japanese": "手の姿勢推定",
    "Russian": "оценка позы руки"
  },
  {
    "English": "hard attention",
    "context": "1: In concurrent work with ours, T2I-Adapter [56] adapts Stable Diffusion to external conditions. Additive Learning circumvents forgetting by freezing the original model weights and adding a small number of new parameters using learned weight masks [51,74], pruning [52], or <mark>hard attention</mark> [80].<br>",
    "Arabic": "تركيز صارم",
    "Chinese": "硬注意力",
    "French": "attention rigide",
    "Japanese": "ハードアテンション",
    "Russian": "жесткое внимание"
  },
  {
    "English": "hash",
    "context": "1: P do Draw grid parameters δ, u ∈ R d with the pitch δ m ∼ p m , and shift u m from the uniform distribution on [0, δ m ]. Let z return the coordinate of the bin containing x as a binary indicator vector z p (x) ≡ <mark>hash</mark>( \n<br>2: One of our contributions is showing that tracking the parity of the number of items that <mark>hash</mark> to a bucket is a useful technique in the context of estimating the size of set differences (rather than the size of sets).<br>",
    "Arabic": "تجزئة",
    "Chinese": "哈希",
    "French": "hachage",
    "Japanese": "ハッシュ",
    "Russian": "хеш"
  },
  {
    "English": "hash function",
    "context": "1: In Section 3.3, we explicitly show a construction (and hence the existence) of asymmetric locality sensitive <mark>hash function</mark> for solving MIPS. The source of randomization h for both q and x ∈ S is the same. Formally, it is not difficult to show a result analogous to Fact 1. Theorem 2 Given a family of <mark>hash function</mark> H and the associated query and preprocessing transformations P and Q , which is ( S 0 , cS 0 , p 1 , p 2 ) -sensitive , one can construct a data structure for c-NN with O ( n ρ log n ) query time and space O ( n 1+ρ ) , where<br>2: Hence, h L2 a,b is not suitable for MIPS. In Section 4, we will experimentally show that our proposed method compares favorably to h L2 a,b <mark>hash function</mark> for MIPS.<br>",
    "Arabic": "دالة التجزئة",
    "Chinese": "哈希函数",
    "French": "fonction de hachage",
    "Japanese": "ハッシュ関数",
    "Russian": "хэш-функция"
  },
  {
    "English": "hash table",
    "context": "1: If we store model coefficients in a <mark>hash table</mark>, we can use a single table for all of the variants, amortizing the cost of storing the key (either a string or a many-byte hash).<br>2: To do so, we employ a multi-band LSH-style <mark>hash table</mark> to reconstruct the dot products that have a significant number of hash matches [15]. Consider the (N * K)-dimensional binary vectors produced by the WTA hash function.<br>",
    "Arabic": "جدول التجزئة",
    "Chinese": "哈希表",
    "French": "table de hachage",
    "Japanese": "ハッシュテーブル",
    "Russian": "хеш-таблица"
  },
  {
    "English": "hashing algorithm",
    "context": "1: Evasion of this feature from the detection model is a realistic possibility as our approach to detect <mark>hashing algorithm</mark> usage is somewhat brittle because it is based on collecting in-line binaries that are statically included in cryptojacking libraries or on finding references to a specific hash function in execution traces.<br>",
    "Arabic": "خوارزمية التجزئة",
    "Chinese": "哈希算法",
    "French": "algorithme de hachage",
    "Japanese": "ハッシュアルゴリズム",
    "Russian": "хеширующий алгоритм"
  },
  {
    "English": "hate speech classifier",
    "context": "1: These results suggest that the designers of <mark>hate speech classifiers</mark> pay attention to the distribution of targeted identities in training data. Many commonly used hate speech datasets do not specify this information.<br>2: These results have implications for NLP researchers building generalizable <mark>hate speech classifiers</mark>, as well as for a more general understanding of variation in hate speech.<br>",
    "Arabic": "مصنف خطاب الكراهية",
    "Chinese": "仇恨言论分类器",
    "French": "classificateur de discours haineux",
    "Japanese": "差別的発言分類器",
    "Russian": "классификатор ненавистной речи"
  },
  {
    "English": "hate speech detection",
    "context": "1: Our third and final task is detection of abusive language; or more specifically, <mark>hate speech detection</mark>. We use the datasets of Waseem (2016) and Waseem and Hovy (2016). The former contains 6,909 tweets; the latter 14,031 tweets. They are manually annotated for sexism and racism.<br>2: The different parts of the prompt are highlighted, i.e. instruction, sociodemographic properties and dataset input. Example drawn from the dataset by . semantically coherent descriptors to each label, e.g., \"Yes\" or \"No\" in lieu of 0 or 1 for binary <mark>hate speech detection</mark>.<br>",
    "Arabic": "الكشف عن خطاب الكراهية",
    "Chinese": "仇恨言论检测",
    "French": "détection des discours haineux",
    "Japanese": "憎悪表現検出",
    "Russian": "определение ненавистнической речи"
  },
  {
    "English": "head entity",
    "context": "1: β s n = (W r r n ) tanh(W h h n + W t t n ),(6) \n where (h n , r n , t n ) = k n , W h , W r , W t are weight matrices for <mark>head entities</mark>, relations, and tail entities, respectively.<br>2: We use our persona frames to construct a knowledge graph of persona commonsense where personas are treated as <mark>head entities</mark> in the graph, frame relations constitute edge type relations, and attributes are tails in a (head, relation, tail) structure.<br>",
    "Arabic": "الكيان الرئيسي",
    "Chinese": "头实体",
    "French": "entité principale",
    "Japanese": "主体エンティティ",
    "Russian": "головная сущность"
  },
  {
    "English": "head word",
    "context": "1: Since the syntactical head gets the <mark>head word</mark> from one of the children, either left or right, the child that does not contain the <mark>head word</mark> (hence called opposite child) is never used later on in predicting.<br>2: Higher-order models generalize the index set by using siblings s (modifiers that previously attached to a <mark>head word</mark>) and grandparents g (<mark>head word</mark>s above the current <mark>head word</mark>).<br>",
    "Arabic": "كلمة الرأس",
    "Chinese": "头词",
    "French": "mot-tête",
    "Japanese": "中心語",
    "Russian": "головное слово"
  },
  {
    "English": "heap structure",
    "context": "1: (It is shown in [20] that this approach is more efficient than the termat-a-time approach where we process the inverted lists one after the other.) Thus, scores are computed en passant while materializing the intersection of the lists, and top-k scores are maintained in a <mark>heap structure</mark>.<br>",
    "Arabic": "بنية الكومة",
    "Chinese": "堆结构",
    "French": "structure de tas",
    "Japanese": "ヒープ構造",
    "Russian": "куча"
  },
  {
    "English": "heatmap",
    "context": "1: This leads to a relevance score for each image, as well as a granular relevance map (<mark>heatmap</mark>) within each image. For images that are relevant enough, we threshold the <mark>heatmap</mark> to obtain a bounding box, compare that box content with known visual entities, and assign it to the entity with the most overlapping match.<br>",
    "Arabic": "خريطة الحرارة",
    "Chinese": "热力图",
    "French": "carte thermique",
    "Japanese": "ヒートマップ",
    "Russian": "карта интенсивности (heatmap)"
  },
  {
    "English": "hedge algorithm",
    "context": "1: Note that AdaBoost.OL is using a variant of the <mark>Hedge algorithm</mark> with 1{y t = y i t } being the loss of expert i on round t (Line 7 and 15). So by standard analysis [see e.g.<br>2: We analyzed the performance of optimistic follow the regularized leader with the entropy regularizer, which corresponds to the <mark>Hedge algorithm</mark> [8] modified so that the last iteration's utility for each strategy is double counted; we refer to it as Optimistic Hedge.<br>",
    "Arabic": "خوارزمية التحوط",
    "Chinese": "对冲算法",
    "French": "algorithme du hedge",
    "Japanese": "ヘッジアルゴリズム",
    "Russian": "алгоритм Hedge"
  },
  {
    "English": "hessian matrix",
    "context": "1: where H (L) is the <mark>Hessian matrix</mark> with (H (L)) = 2 L/ ; and can be updated in the same way. Kindly note that in the prompt learning area, the task head is also known as the answering function, which connects the prompt to the answers for downstream tasks to be reformulated.<br>2: Improvement: This approximation however degrades the correctness of the approximate <mark>Hessian matrix</mark> that the Levenberg-Marquardt algorithm [45] relies on for fast convergence. We found that also optimizing the squared spatial derivatives of this cost significantly improves the convergence.<br>",
    "Arabic": "مصفوفة هسيان",
    "Chinese": "黑塞矩阵",
    "French": "matrice hessienne",
    "Japanese": "ヘシアン行列",
    "Russian": "матрица Гессе"
  },
  {
    "English": "hessian-vector product",
    "context": "1: To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and <mark>Hessian-vector products</mark>. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information.<br>",
    "Arabic": "ضرب هيسيان-متجه",
    "Chinese": "黑塞矩阵-向量乘积",
    "French": "produit hessien-vecteur",
    "Japanese": "ヘシアン・ベクトル積",
    "Russian": "произведение гессиана на вектор"
  },
  {
    "English": "heuristic algorithm",
    "context": "1: As there are many versions of the Timetabling Problem, a variety of techniques have been used to solve it [3], [4]. Most of these techniques range from graph colouring to <mark>heuristic algorithms</mark>.<br>",
    "Arabic": "خوارزمية إرشادية",
    "Chinese": "启发式算法",
    "French": "algorithme heuristique",
    "Japanese": "ヒューリスティックアルゴリズム",
    "Russian": "эвристический алгоритм"
  },
  {
    "English": "heuristic function",
    "context": "1: The aggregate set of ranking examples collected over all the training examples is then given to a learning algorithm to learn the weights of the <mark>heuristic function</mark>. In our implementation we employed the margin-scaled variant of the online Passive-Aggressive algorithm (see Equation 47in (Crammer et al.<br>2: We say that c ∈ R + 0 ∪ {∞} is an admissible heuristic estimate for state s and cost function cost if c ≤ h * (s, cost ). A <mark>heuristic function</mark> is admissible if h(s, cost ) is an admissible heuristic estimate for all states s and cost functions cost .<br>",
    "Arabic": "دالة إرشادية",
    "Chinese": "启发函数",
    "French": "fonction heuristique",
    "Japanese": "ヒューリスティック関数",
    "Russian": "эвристическая функция"
  },
  {
    "English": "heuristic search",
    "context": "1: Both algorithms in [6] and [22] adopt <mark>heuristic search</mark> approaches, and thus cannot guarantee to find the complete set of biclusters in the data set. In [19], Wang  We borrow some important ideas from previous studies on frequent itemset mining.<br>2: While <mark>heuristic search</mark> and symbolic search are both contenders for the throne in optimal planning, and their combination is a natural and promising avenue, the results with that combination have thus far been disappointing.<br>",
    "Arabic": "بحث إرشادي",
    "Chinese": "启发式搜索",
    "French": "recherche heuristique",
    "Japanese": "発見的探索",
    "Russian": "эвристический поиск"
  },
  {
    "English": "heuristic search algorithm",
    "context": "1: To do so, we use GHSETA * (Jensen, Veloso, and Bryant 2008), a symbolic <mark>heuristic search algorithm</mark> which partitions the TRs not only by the cost of the corresponding operators, but also by the change of the heuristic value they induce.<br>",
    "Arabic": "خوارزمية البحث ارشادي",
    "Chinese": "启发式搜索算法",
    "French": "algorithme de recherche heuristique",
    "Japanese": "ヒューリスティック探索アルゴリズム",
    "Russian": "эвристический алгоритм поиска"
  },
  {
    "English": "heuristic value",
    "context": "1: Or in other words, the operator-potential function for an operator o gives us the lower bound on the change of the <mark>heuristic value</mark> of the corresponding potential heuristic for the given potential function P and disambiguation map D. This immediately leads to another observation that for every sequence of operators π = o 1 , . . .<br>",
    "Arabic": "قيمة إرشادية",
    "Chinese": "启发式值",
    "French": "valeur heuristique",
    "Japanese": "ヒューリスティック値",
    "Russian": "эвристическое значение"
  },
  {
    "English": "hidden Markov model",
    "context": "1: In our model, syntax is represented either as parse tree productions or a sequence of phrasal nodes augmented with part of speech tags. Our best performing method uses a <mark>Hidden Markov Model</mark> to learn the patterns in these syntactic items. Sections 3 and 5 discuss the representations and their specific implementations and relative advantages.<br>2: To model this uncertainty, we use a <mark>Hidden Markov Model</mark> on permutations, which is a joint distribution over P (σ (1) , . . . , σ (T ) , z (1) , . . . , z (T ) ) which factors as: 1) , . . .<br>",
    "Arabic": "نموذج ماركوف المخفي",
    "Chinese": "隐马尔可夫模型",
    "French": "modèle de Markov caché",
    "Japanese": "隠れマルコフモデル",
    "Russian": "скрытая марковская модель"
  },
  {
    "English": "hidden dimension",
    "context": "1: Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the ( sequence length , <mark>hidden dimension</mark> ) input in each Fourier sublayer , we tried two approaches to introduce learnable parameters : ( 1 ) element wise multiplication with a ( sequence length , <mark>hidden dimension</mark> ) matrix , and ( 2 ) regular matrix multiplication with ( sequence length , sequence length ) and ( <mark>hidden dimension</mark> , <mark>hidden dimension</mark> )<br>2: We use single-head attention in each layer and 10 SRU++ layers for all our models. We use the same dropout probability for all layers and tune this value according to the model size and the results on the dev set. By default, we set the <mark>hidden dimension</mark> d : d = 4 : 1.<br>",
    "Arabic": "البعد الخفي",
    "Chinese": "隐藏维度",
    "French": "dimension cachée",
    "Japanese": "\"隠れた次元 (kakureta jigen)\"",
    "Russian": "скрытое измерение"
  },
  {
    "English": "hidden dimension size",
    "context": "1: N U (or N V ) and H are, respectively, the input size and the <mark>hidden dimension size</mark>, x j,t is the initial feature vector of item-node j at time t, and c i,j,t is the normalization factor.<br>",
    "Arabic": "حجم البعد المخفي",
    "Chinese": "隐藏维度大小",
    "French": "taille de la dimension cachée",
    "Japanese": "隠れ次元サイズ",
    "Russian": "размер скрытого слоя"
  },
  {
    "English": "hidden dimensionality",
    "context": "1: The encoder and decoder consist of two-layer unidirectional LSTMs, a <mark>hidden dimensionality</mark> of 128, character embeddings of size 128 and a dropout of 0.1 between layers. The Adadelta (Zeiler, 2012) optimiser is used, with a batch size of 64. During evaluation, we apply beam search with a beam of size five.<br>",
    "Arabic": "البُعد الخفي",
    "Chinese": "隐藏维度",
    "French": "dimensionnalité cachée",
    "Japanese": "隠れ次元数",
    "Russian": "скрытая размерность"
  },
  {
    "English": "hidden embedding",
    "context": "1: We implement our model based on BERT [34]. We use BERT [34] as sequence encoding for queries and the hyperparameters of the decoder are the same as for the encoder. It has 12 layers, 768-dimensional <mark>hidden embeddings</mark>, 12 attention heads, and 110 million parameters.<br>",
    "Arabic": "تضمين مخفي",
    "Chinese": "隐藏嵌入",
    "French": "Représentation cachée",
    "Japanese": "潜在埋め込み",
    "Russian": "скрытые вложения"
  },
  {
    "English": "hidden feature",
    "context": "1: ) . In line with Alon & Yahav (2021), we refer to those structural properties of the graph that lead to over-squashing as a bottleneck 1 . Sensitivity analysis. The <mark>hidden feature</mark> h \n ( ) i = h ( ) \n i (x 1 , . . .<br>",
    "Arabic": "السمة المخفية",
    "Chinese": "隐藏特征",
    "French": "caractéristique cachée",
    "Japanese": "\"隠れた特徴\"",
    "Russian": "скрытый признак"
  },
  {
    "English": "hidden layer",
    "context": "1: The forward and backward networks were structured to share the same set of word embeddings, initialised with pre-trained word vectors (Pennington et al., 2014). The <mark>hidden layer</mark> size was set to be 80 for all cases, and deep networks were trained with two <mark>hidden layer</mark>s and a 50% dropout rate.<br>2: Formally, for a question x, let h q (x) be an LSTM encoding of the question (i.e. the last <mark>hidden layer</mark> of an LSTM applied word-by-word to the input question). Let {z 1 , z 2 , . . .}<br>",
    "Arabic": "طبقة مخفية",
    "Chinese": "隐藏层",
    "French": "couche cachée",
    "Japanese": "隠れ層",
    "Russian": "скрытый слой"
  },
  {
    "English": "hidden representation",
    "context": "1: where h l is the <mark>hidden representation</mark> of subsequent layer normalization output after feed-forward layer in the transformer layer l, U l and D l are up-and down-projection matrices, r l is the hidden state directly from feed-forward layer.<br>2: It learns an encoder function h, that maps an input x ∈ IR d to a <mark>hidden representation</mark> h(x) ∈ IR d h , jointly with a decoder function g, that maps h back to the input space as r = g(h(x)) the reconstruction of x.<br>",
    "Arabic": "التمثيل المُخفي",
    "Chinese": "隐藏表示",
    "French": "représentation cachée",
    "Japanese": "隠れ表現",
    "Russian": "скрытое представление"
  },
  {
    "English": "hidden size",
    "context": "1: , h enc |Xt| ] ∈ R |Xt|×d hdd , where d hdd is the <mark>hidden size</mark>. As mentioned in Section 1, due to the multi-turn mapping problem, the model should infer the states across a sequence of turns.<br>2: We use the google/bert_uncased_L-6_H-512_A-8 pretrained model from Huggingface (Wolf et al., 2020), which has 6 attention layers, <mark>hidden size</mark> of 512, and 8 attention heads. Task. We consider the subject-verb number agreement task in our experiments. Variants of this task in English have become popular case studies in neural network probing.<br>",
    "Arabic": "حجم مخفي",
    "Chinese": "隐藏大小",
    "French": "taille cachée",
    "Japanese": "隠れ層サイズ",
    "Russian": "скрытый размер"
  },
  {
    "English": "hidden state",
    "context": "1: Equation ( 3) is now a sequence-to-sequence map u k → y k instead of function-to-function. Moreover the state equation is now a recurrence in x k , allowing the discrete SSM to be computed like an RNN. Concretely, \n x k ∈ R N can be viewed as a <mark>hidden state</mark> with transition matrix A. \n<br>2: 2014) that use fully connected layers both to process the input and to produce the output values from the <mark>hidden state</mark>, h a t . The IAC critics use extra output heads appended to the last layer of the actor network.<br>",
    "Arabic": "الحالة المخفية",
    "Chinese": "隐藏状态",
    "French": "état caché",
    "Japanese": "隠れ状態",
    "Russian": "скрытое состояние"
  },
  {
    "English": "hidden state dimension",
    "context": "1: GPT-2. The vocabulary size of GPT-2 is 50527. We use GPT-2-small with 12 decoder layers and <mark>hidden state dimension</mark> as 768, for experiments  in Sections 2.2, 4 and 5. GeLU (Hendrycks and Gimpel, 2016) is used as the activation function.<br>2: Hyperparameter Optimization (HO) There are three main approaches that can be categorized based on the types of problem-specific information used: 1) black-box approaches that do not consider the internal structure of the Table 1. Comparison of approaches for learning parameters in unrolled computation graphs. S is the size of the system state (e.g. the RNN <mark>hidden state dimension</mark> , or in the case of hyperparameter optimization the inner-problem 's weight dimensionality and potentially the optimizer state ; P is the dimensionality of θ ; T is the total number of steps in a sequence/unroll ; K is the truncation length ; and N is the number of samples ( also called particles ) used for the reparameterization<br>",
    "Arabic": "البُعد الخفي للحالة",
    "Chinese": "隐藏状态维度",
    "French": "dimension de l'état caché",
    "Japanese": "隠れ状態次元",
    "Russian": "размерность скрытого состояния"
  },
  {
    "English": "hidden state representation",
    "context": "1: We first run PCA on the data matrix obtained from concatenating the <mark>hidden state representations</mark> of the human text and model text. We keep 90% of the explained variance and normalize each datapoint to have unit 2 norm.<br>",
    "Arabic": "تمثيل الحالة المخفية",
    "Chinese": "隐藏状态表示",
    "French": "représentation de l'état caché",
    "Japanese": "隠れ状態表現",
    "Russian": "скрытое представление состояния"
  },
  {
    "English": "hidden state vector",
    "context": "1: RNNs are defined by the update rule h t = f (h t−1 , x t ), where the function f could be a feedforward network, h t is the model's memory vector usually referred to as the <mark>hidden state vector</mark> and x t denotes the input vector at the t-th step.<br>2: At each timestep, the model will receive the <mark>hidden state vector</mark> h t−1 = [q t−1 , ω t−1 ] and the input vector x t as input.<br>",
    "Arabic": "متجه الحالة المخفية",
    "Chinese": "隐状态向量",
    "French": "vecteur d'état caché",
    "Japanese": "隠れ状態ベクトル",
    "Russian": "вектор скрытого состояния"
  },
  {
    "English": "hidden unit",
    "context": "1: Notably, this phenomenon has been used to evaluate the ability for models to learn hierarchical syntactic phenomena (Linzen et al., 2016;Gulordava et al., 2018). It has also served as a testing ground for interpretability studies which have found evidence of individual <mark>hidden units</mark> that track number and nested dependencies ( Lakretz et al. , 2019 ) , and that removing individual <mark>hidden units</mark> or subspaces from the models ' representation space have a targeted impact on model predictions ( Finlayson et al. , 2021 ; Lasri et al.<br>2: The sequence-to-sequence model was trained with two layers of GRUs (Cho et al., 2014), each with 512 <mark>hidden units</mark>. We used the general attention mechanism (Luong et al., 2015) and used the Fast-Text word-embeddings (Bojanowski et al., 2017).<br>",
    "Arabic": "وحدة مخفية",
    "Chinese": "隐藏单元",
    "French": "unité cachée",
    "Japanese": "隠れユニット",
    "Russian": "скрытая единица"
  },
  {
    "English": "hidden variable",
    "context": "1: Second, S is also a descendant of a \"virtual collider\" Y , whose parents are X and the error term U Y (also called \"<mark>hidden variable</mark>\") which is always present, though often not shown in the diagram. 3<br>2: Figure 2: Positive and negative terms in the hard gradient. The root node sums out the variable Y, the two sum nodes on the left sum out the <mark>hidden variable</mark> H 1 , the two sum nodes on the right sum out H 2 , and a circled 'f' denotes an input variable X i .<br>",
    "Arabic": "متغير مخفي",
    "Chinese": "隐藏变量",
    "French": "variable cachée",
    "Japanese": "潜在変数",
    "Russian": "скрытая переменная"
  },
  {
    "English": "hierarchical agglomerative clustering",
    "context": "1: Based on this model, we are able to evaluate the quality of such merging by measuring the probability that αi and αj are generated by M. \n Using the above generative model, we can arrange all of the patterns in a tree structure using a <mark>hierarchical agglomerative clustering</mark> method, where patterns with the highest similarity are grouped together first.<br>",
    "Arabic": "تجميع هرمي تراكمي",
    "Chinese": "分层凝聚聚类",
    "French": "regroupement hiérarchique agglomératif",
    "Japanese": "階層的凝集クラスタリング",
    "Russian": "иерархическая агломеративная кластеризация"
  },
  {
    "English": "hierarchical clustering",
    "context": "1: As shown in the figure, when we use the profile approximation heuristics in K-means, the summarization quality is much worse than that of building the profiles from scratch. The restoration at K = 100 is 259% for K-means with the profile approximation while it is around 60% for <mark>hierarchical clustering</mark> or K-means without profile approximation.<br>2: It consists of many small frequent itemsets over a large set of items (itemsets of size 1-3 make up 84.55% of the total 4,195 patterns versus 14.10% for Mushroom), which makes the summarization more difficult. Figure 7 shows the average restoration error over <mark>hierarchical clustering</mark> and K-means clustering with or without  applying the profile approximation heuristics.<br>",
    "Arabic": "التجميع الهرمي",
    "Chinese": "层次聚类",
    "French": "regroupement hiérarchique",
    "Japanese": "階層的クラスタリング",
    "Russian": "иерархическая кластеризация"
  },
  {
    "English": "hierarchical decoder",
    "context": "1: Our machine translation system is a string-todependency <mark>hierarchical decoder</mark> based on (Shen et al., 2008) and (Chiang, 2007). Bottom-up chart parsing is performed to produce a shared forest of derivations. The decoder uses a log-linear translation model, so the score of derivation d is defined as: \n<br>2: In this section, we describe the considerations that must be taken when integrating the NNJM into a <mark>hierarchical decoder</mark>.<br>",
    "Arabic": "فك الترميز الهرمي",
    "Chinese": "分层解码器",
    "French": "décodeur hiérarchique",
    "Japanese": "階層的デコーダ",
    "Russian": "иерархический декодер"
  },
  {
    "English": "hierarchical feature",
    "context": "1: This integrated learning of <mark>hierarchical features</mark> is in distinction to previous multi-scale approaches [40,41,30] in which scale-space edge fields are neither automatically learned nor hierarchically connected.<br>2: The ViT is applied to query and each support image independently while sharing weights, which produces tokenized representation of image patches at multiple hierarchies. Similar to Ranftl et al. (2021), we extract tokens at four intermediate ViT blocks to form <mark>hierarchical features</mark>.<br>",
    "Arabic": "الميزة الهرمية",
    "Chinese": "分层特征",
    "French": "caractéristique hiérarchique",
    "Japanese": "階層的特徴",
    "Russian": "иерархические признаки"
  },
  {
    "English": "hierarchical inference",
    "context": "1: An appealing feature of Bayesian inference methods is the ability to perform <mark>hierarchical inference</mark>. Here, we sample the hyperparameters, {θ k } K k=1 , governing the GP covariance function.<br>",
    "Arabic": "الاستدلال الهرمي",
    "Chinese": "层次推断",
    "French": "inférence hiérarchique",
    "Japanese": "階層推論",
    "Russian": "иерархический вывод"
  },
  {
    "English": "hierarchical method",
    "context": "1: The process of expressing theories as informative prior distributions over parameters has been discussed in follow-up work by Wolf Vanepaemel in [40] and in [41] where the author has tackled this task by using <mark>hierarchical methods</mark>.<br>",
    "Arabic": "الأساليب الهرمية",
    "Chinese": "层次方法",
    "French": "méthode hiérarchique",
    "Japanese": "階層的手法",
    "Russian": "иерархический метод"
  },
  {
    "English": "hierarchical model",
    "context": "1: Since the model has no notion of significant locations, it is not able to predict the high-level goal of a person. By conditioning on goals and segments of a trip, our <mark>hierarchical model</mark> is able to learn more specific motion patterns of a person, which also enables us to detect user errors.<br>2: We present the first results on abnormality detection in location and transportation prediction using a simple and effective model selection approach based on comparing the likelihood of a learned <mark>hierarchical model</mark> against that of a prior model.<br>",
    "Arabic": "نموذج هرمي",
    "Chinese": "分层模型",
    "French": "modèle hiérarchique",
    "Japanese": "階層モデル",
    "Russian": "иерархическая модель"
  },
  {
    "English": "hierarchical reinforcement learning",
    "context": "1: <mark>hierarchical reinforcement learning</mark> ( Wayne & Abbott , 2014 ; Vezhnevets et al. , 2017 ) , curiosity ( Pathak et al. , 2017 ) , and imaginative agents ( Racanière et al. , 2017 ) . In effect, the models are trained via games played by cooperating and competing modules.<br>2: This process induces an inventory of reusable and interpretable subpolicies which can be employed for zeroshot generalization when further sketches are available, and <mark>hierarchical reinforcement learning</mark> when they are not. Our work suggests that these sketches, which are easy to produce and require no grounding in the environment, provide an effective scaffold for learning hierarchical policies from minimal supervision.<br>",
    "Arabic": "تعلم تعزيزي هرمي",
    "Chinese": "层次强化学习",
    "French": "apprentissage par renforcement hiérarchique",
    "Japanese": "階層強化学習",
    "Russian": "иерархическое обучение с подкреплением"
  },
  {
    "English": "hierarchical representation",
    "context": "1: This is consistent with the common knowledge in 2D CNNs: increasing receptive field gradually through the network can help build <mark>hierarchical representations</mark> with varying spatial extents and abstraction levels. Although we mainly experiment with XY Z lattices in this work, BCL allows for other lattice spaces such as position and color space (XY ZRGB) or normal space.<br>",
    "Arabic": "التمثيل الهرمي",
    "Chinese": "分层表示",
    "French": "représentation hiérarchique",
    "Japanese": "階層表現",
    "Russian": "иерархическое представление"
  },
  {
    "English": "hierarchical structure",
    "context": "1: Our algorithm works well for synthetic data. For all the experiments, our approach adopts a <mark>hierarchical structure</mark> using a grid of 256 landmarks withγ = 0.7 and T = 8 layers. For bases functions, we use Thin-Plate Spline [2] proper normalization.<br>2: This task is designed to provide a substantially more challenging RL problem, due to the fact that the walker must learn the low-level walking skill before it can make any progress, but has simpler <mark>hierarchical structure</mark> than the crafting environment.<br>",
    "Arabic": "\"بنية هرمية\"",
    "Chinese": "层次结构",
    "French": "structure hiérarchique",
    "Japanese": "階層構造",
    "Russian": "иерархическая структура"
  },
  {
    "English": "hierarchical topic model",
    "context": "1: 1 This situation is exacerbated when it comes to hierarchical and structured topic models, since there the number of (sub)topics can grow considerably more rapidly. Hence the use of sparsity is crucial in designing efficient samplers.<br>",
    "Arabic": "نموذج الموضوع الهرمي",
    "Chinese": "层次主题模型",
    "French": "modèle de sujet hiérarchique",
    "Japanese": "階層的トピックモデル",
    "Russian": "иерархическая тематическая модель"
  },
  {
    "English": "hierarchy",
    "context": "1: Humans are able to rapidly perform a variety of tasks without extensive experience [1]. This may be because of strong inductive biases towards abstract structured knowledge (e.g. <mark>hierarchies</mark>, compositionality) [2,3] that act as strong prior knowledge, enabling generalization to novel environments with little new data.<br>",
    "Arabic": "تسلسل هرمي",
    "Chinese": "层级结构",
    "French": "hiérarchie",
    "Japanese": "階層構造",
    "Russian": "иерархия"
  },
  {
    "English": "high-dimensional datum",
    "context": "1: In many settings, sparsity is a useful ingredient because it enables us to model structure in <mark>high-dimensional data</mark> while still remaining a mathematically tractable concept. For instance, natural images are often sparse when represented in a wavelet basis, and objects in a classification task usually belong to only a small number of classes.<br>",
    "Arabic": "بيانات عالية الأبعاد",
    "Chinese": "高维数据",
    "French": "donnée de haute dimension",
    "Japanese": "高次元データ",
    "Russian": "Высокоразмерные данные"
  },
  {
    "English": "high-dimensional space",
    "context": "1: But IB works even if T can take values throughout a <mark>high-dimensional space</mark>, because the randomness in p ✓ (t | x) means that T is noisy in a way that wipes out information about X.<br>2: be shown on a scatterplot . This visualization form is simple, and widely applicable across various domains. One pioneering technique is Multidimensional Scaling (MDS) (Kruskal 1964). The goal is to preserve the distances in the <mark>high-dimensional space</mark> in the low-dimensional embedding.<br>",
    "Arabic": "فضاء عالي الأبعاد",
    "Chinese": "高维空间",
    "French": "espace de grande dimension",
    "Japanese": "高次元空間",
    "Russian": "многомерное пространство"
  },
  {
    "English": "high-dimensionality",
    "context": "1: Aware of the fact that <mark>high-dimensionality</mark> and sparsity can affect any systems' performance, we conduct an additional analysis to investigate the performance of L2C on a high-dimensional textual dataset.<br>",
    "Arabic": "عالية الأبعاد",
    "Chinese": "高维",
    "French": "haute dimensionnalité",
    "Japanese": "高次元性",
    "Russian": "высокая размерность"
  },
  {
    "English": "high-order feature",
    "context": "1: To cope with <mark>higher-order features</mark> of the form f a 1 ,...,a K (x) (i.e., features whose values depend on the simultaneous inclusion of arcs a 1 , . . .<br>",
    "Arabic": "ميزة من الدرجة العالية",
    "Chinese": "高阶特征",
    "French": "caractéristique d'ordre supérieur",
    "Japanese": "高次特徴",
    "Russian": "высокопорядковый признак"
  },
  {
    "English": "high-order model",
    "context": "1: Note that for <mark>higher-order models</mark> we do not provide an optimal reparametrization and hence our method is not provably better then the competitors. We consider this as a direction for future work. The cell tracking problem consists of a binary higher order graphical model [17].<br>2: The index sets of higherorder models can be constructed out of the index sets of lower-order models, thus forming a hierarchy that we will exploit in our coarse-to-fine cascade.<br>",
    "Arabic": "نموذج عالي الترتيب",
    "Chinese": "高阶模型",
    "French": "modèle d'ordre supérieur",
    "Japanese": "高次モデル",
    "Russian": "высокопорядковая модель"
  },
  {
    "English": "hill-climbing",
    "context": "1: We search this space using a greedy <mark>hill-climbing</mark> approach that is interleaved with the EM parameter learning. The <mark>hill-climbing</mark> begins with an empty set of relationships (R = ∅), and adds or removes relationships one at a time until a local maximum is reached.<br>2: (Thus, this is a performance guarantee slightly better than 63%.) The algorithm that achieves this performance guarantee is a natural greedy <mark>hill-climbing</mark> strategy related to the approach considered in [ 10 ] , and so the main content of this result is the analysis framework needed for obtaining a provable performance guarantee , and the fairly surprising fact that <mark>hill-climbing</mark> is always within a factor of at least 63 % of optimal for this<br>",
    "Arabic": "تسلق التل",
    "Chinese": "爬山算法",
    "French": "escalade gloutonne",
    "Japanese": "山登り法",
    "Russian": "метод восхождения на холм"
  },
  {
    "English": "hinge loss",
    "context": "1: We demonstrate the effectiveness of our approach in the context of optimizing the structured <mark>hinge loss</mark> upper bound of AP and NDCG loss for learning models for a variety of vision tasks. We show that our approach provides significantly better results than simpler decomposable loss functions, while requiring a comparable training time.<br>2: We take a similar approach here, however we cope with the non-differentiability of the hingeloss directly by using sub-gradients instead of gradients. Another important distinction is that Chapelle views the optimization problem as a function of the variables α i .<br>",
    "Arabic": "خسارة المفصلة",
    "Chinese": "合页损失",
    "French": "perte de charnière",
    "Japanese": "ヒンジ損失",
    "Russian": "функция потерь шарнира"
  },
  {
    "English": "hinge loss function",
    "context": "1: The MLP is trained with a margin-based <mark>hinge loss function</mark>, maximizing the separation between the true and the highest scoring incorrect OB-JECT option for the current ACTION phrase.<br>",
    "Arabic": "وظيفة الخسارة المفصلية",
    "Chinese": "铰链损失函数",
    "French": "fonction de perte à charnière",
    "Japanese": "ヒンジ損失関数",
    "Russian": "функция потерь шарнира"
  },
  {
    "English": "histogram of orient gradient",
    "context": "1: In [4], an excellent human detector was described by training an SVM classifier using densely sampled <mark>histogram of oriented gradients</mark> (similar to SIFT descriptors) inside the detection window.<br>",
    "Arabic": "الهستوغرام للتدرجات الموجهة",
    "Chinese": "梯度方向直方图",
    "French": "histogramme de gradients orientés",
    "Japanese": "方向勾配のヒストグラム",
    "Russian": "гистограмма ориентированных градиентов"
  },
  {
    "English": "hold-out data",
    "context": "1: While performance on <mark>held-out data</mark> is a useful indicator, <mark>held-out data</mark>sets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008;Recht et al., 2019).<br>2: For all experiments we fixed the number of clusters to 256 as this performed well on <mark>held-out data</mark>. Furthermore, we only clustered the 1 million most frequent word types in each language for both efficiency and sparsity reasons. For \n<br>",
    "Arabic": "بيانات محجوزة",
    "Chinese": "留存数据",
    "French": "données réservées",
    "Japanese": "検証データ",
    "Russian": "\"удерживаемые данные\""
  },
  {
    "English": "homogeneous coordinate",
    "context": "1: Here we derive the transformation which is applied to rays to map them from camera space to NDC space. The standard 3D perspective projection matrix for <mark>homogeneous coordinates</mark> is: \n<br>2: The last decade has seen a great number of publications in the area of multiple view vision and consequently an enormous progress in practical as well as theoretical aspects. One of the most interesting and intriguing theoretical constructions is the trifocal tensor that appears as the connecting block between the <mark>homogeneous coordinates</mark> of image points and/or image lines over three views.<br>",
    "Arabic": "إحداثيات متجانسة",
    "Chinese": "齐次坐标",
    "French": "coordonnées homogènes",
    "Japanese": "同次座標",
    "Russian": "однородные координаты"
  },
  {
    "English": "homographie",
    "context": "1: The joint camera motion was performed manually (i.e., a person would manually hold and rotate the two attached cameras). No temporal synchronization tool was used. The frame-to-frame input transformations within each sequence (<mark>homographies</mark> T 1 , . . . , T n and T 1 , . . .<br>2: These planes are therefore represented by the first, second and third row of the camera matrix P J respectively. Now, since the matrices U, V and W are <mark>homographies</mark> from image plane I to image plane K they must possess the homography properties given in Section 2.2.<br>",
    "Arabic": "تصوير متناسق",
    "Chinese": "同射变换 (homography)",
    "French": "homographie",
    "Japanese": "射影変換",
    "Russian": "гомография"
  },
  {
    "English": "homography",
    "context": "1: -Homographies are in general regular matrices -The inverse of a regular homography is a homography due to the same plane but with interchanged image planes:  \n H JI (π) −1 ∼ H IJ (π) -Any<br>",
    "Arabic": "التجانس",
    "Chinese": "单应性",
    "French": "homographie",
    "Japanese": "同形写像",
    "Russian": "гомография"
  },
  {
    "English": "homography matrix",
    "context": "1: H JI (π) is a <mark>homography matrix</mark> that assigns to every point x I of image plane I an image point x J of image plane J due to the plane π.<br>",
    "Arabic": "مصفوفة التجانس",
    "Chinese": "单应矩阵",
    "French": "matrice d'homographie",
    "Japanese": "同次写像行列",
    "Russian": "матрица гомографии"
  },
  {
    "English": "homomorphism",
    "context": "1: Using the construction of U D,Q , it is not hard to see that there is a <mark>homomorphism</mark> from U D,Q to U D,O that is the identity on adom(D).<br>2: are nondecreasing. Lemma 28. For all i ≥ 0: with h(x) =x i+1 . We can compose h and h into a <mark>homomorphism</mark> g from q T to U Aq i ,O with g(x) =x i .<br>",
    "Arabic": "تشبه المورفيسم",
    "Chinese": "同态映射 (homomorphism)",
    "French": "homomorphisme",
    "Japanese": "ホモモルフィズム",
    "Russian": "гомоморфизм"
  },
  {
    "English": "horn theory",
    "context": "1: Furthermore, if exponential space is prohibitive, it is of interest to know whether a few explanations (e.g., polynomially many) can be generated in polynomial time, as studied by Selman and Levesque (1996). • We contrast formula-based (syntactic) with model-based (semantic) representation of <mark>Horn theories</mark>.<br>2: In this paper, we consider computing a set of explanations for queries from <mark>Horn theories</mark>. More precisely, we address the following problems: \n<br>",
    "Arabic": "نظرية القرن",
    "Chinese": "号角理论",
    "French": "théorie de Horn",
    "Japanese": "ホーン理論",
    "Russian": "теория рогов"
  },
  {
    "English": "huber loss",
    "context": "1: In practice, we find that directly using MSE loss occasionally causes NaN at the beginning of the training while <mark>Huber loss</mark> leads to a more stable training procedure. We set δ = 1 in our experiments.<br>2: Since there are usually noises in the coordination data, we adopt the <mark>Huber loss</mark> (Huber, 1992) other than the common MSE loss to avoid numerical instability (further explanation can be found in Appendix F): \n<br>",
    "Arabic": "خسارة هوبر",
    "Chinese": "哈伯损失",
    "French": "perte de Huber",
    "Japanese": "ヒューバー損失",
    "Russian": "\"функция потерь Хубера\""
  },
  {
    "English": "human annotation",
    "context": "1: Specifically, we eliminate sub-optimal systems even before the <mark>human annotation</mark> process and perform human evaluations only on test examples where the automatic metric is highly uncertain. This reduces the number of <mark>human annotation</mark>s required further by 89%.<br>",
    "Arabic": "التعليق البشري",
    "Chinese": "人工标注",
    "French": "annotation humaine",
    "Japanese": "人間アノテーション (ningen anotēshon)",
    "Russian": "человеческие аннотации"
  },
  {
    "English": "human pose",
    "context": "1: The <mark>human pose</mark> is further decomposed into some body parts, denoted by { } =1 . For each body part and the object , and denote the visual features that describe the corresponding image regions respectively. Note that because of the difference between the <mark>human pose</mark>s in each HOI activity (Fig.<br>2: Human pose and motion priors are crucial for preserving the realism of models estimated from captured data [40,23,38] and to estimate <mark>human pose</mark> from images [10,49,65,14,33] and videos [32,59]. Further, they can be powerful tools for data generation.<br>",
    "Arabic": "وضع الإنسان",
    "Chinese": "人体姿势",
    "French": "pose humaine",
    "Japanese": "人体姿勢",
    "Russian": "поза человека"
  },
  {
    "English": "human pose estimation",
    "context": "1: [11] model these interactions by modeling the prepositions and adjectives that relate nouns. Yao and Li [16] model the interactions between human pose and objects by coupling the <mark>human pose estimation</mark> and object recognition together. In [7] the interactions between objects is modeled implicitly in the context of predicting sentences for images.<br>2: Here we propose a novel model to exploit the mutual context of human poses and objects in one coherent framework, where object detection and <mark>human pose estimation</mark> can benefit from each other. For simplicity, we assume that only one object is involved in each activity.<br>",
    "Arabic": "تقدير وضعية الإنسان",
    "Chinese": "人体姿态估计",
    "French": "estimation de la pose humaine",
    "Japanese": "人間のポーズ推定 (ningen no pōzu suitei)",
    "Russian": "оценка позы человека"
  },
  {
    "English": "human-computer interaction",
    "context": "1: 3D hand pose estimation (HPE), which involves estimating the 3D positions of hand keypoints, provides a fundamental conprehension of human hand motion. Therefore, it is essential to facilitate more natural and intuitive interactions between humans and machines and is applicable to various <mark>human-computer interaction</mark> applications including robotics, gaming, and augmented/virtual reality.<br>2: In this demo, we will present a simulation-based <mark>human-computer interaction</mark> of deep reinforcement learning in action on order dispatching and driver repositioning for ride-sharing. Specifically, we will demonstrate through several specially designed domains how we use deep reinforcement learning to train agents (drivers) to have longer optimization horizon and to cooperate to achieve higher objective values collectively.<br>",
    "Arabic": "تفاعل الإنسان والحاسوب",
    "Chinese": "人机交互",
    "French": "interaction humain-ordinateur",
    "Japanese": "ヒューマンコンピュータインタラクション",
    "Russian": "взаимодействие человека и компьютера"
  },
  {
    "English": "human-in-the-loop",
    "context": "1: (2020) present a human-in-theloop system for BLI in four low-resource languages, updating contextual embeddings with the help of annotations provided by a native speaker. Zhang et al.<br>2: To investigate whether TEC systems can already be useful to humans-improving the quality, speed, or ease of human review-we performed a human-in-theloop user study with our TEC model.<br>",
    "Arabic": "مشاركة بشرية في العملية",
    "Chinese": "人机协同 (Human-in-the-loop)",
    "French": "humain dans la boucle",
    "Japanese": "ヒューマンインザループ",
    "Russian": "человек в цикле"
  },
  {
    "English": "human-machine interaction",
    "context": "1: To further <mark>human-machine interaction</mark>, there is a great need to develop collaborative agents that can act autonomously yet still collaborate with their human teammates.<br>2: Uncertain application environments, such as <mark>human-machine interaction</mark>, require adaptation capabilities and approximate reasoning [16] to be able to reason with various sorts of uncertainties. For instance, we know that human may reason purposefully fallacious, aiming at deception or trickery.<br>",
    "Arabic": "تفاعل الإنسان مع الآلة",
    "Chinese": "人机交互",
    "French": "interaction homme-machine",
    "Japanese": "人機インタラクション",
    "Russian": "взаимодействие человека и машины"
  },
  {
    "English": "hungarian loss",
    "context": "1: Following BEVFormer [55], the <mark>Hungarian loss</mark> is adopted for each paired result, which is a linear combination of a Focal loss [61] for class labels and an l 1 for 3D boxes localization.<br>",
    "Arabic": "خسارة هنغارية",
    "Chinese": "匈牙利损失",
    "French": "perte hongroise",
    "Japanese": "ハンガリアン損失",
    "Russian": "венгерский расход"
  },
  {
    "English": "hybrid model",
    "context": "1: There is almost no correlation between both models' predictions and gold judgments (Table 3), suggesting that the models are not able to capture neg-raising inferences. Model behavior Figure 4 shows that the <mark>hybrid model</mark> predictions are mostly positive, whereas the rule-based model predictions are clustered at −3 and +3.<br>",
    "Arabic": "النموذج المختلط",
    "Chinese": "混合模型",
    "French": "modèle hybride",
    "Japanese": "ハイブリッドモデル",
    "Russian": "гибридная модель"
  },
  {
    "English": "hyper-graph",
    "context": "1: For co-occurrence potentials monotonically increasing with respect to L(x) the problem can be modelled using one binary variable z l per class indicating the presence of pixels of that class in the labelling, infinite edges for x i = l and z l = 0 and <mark>hyper-graph</mark> over all z l modelling C(L(x)).<br>",
    "Arabic": "رسم بياني مفرط",
    "Chinese": "超图",
    "French": "hypergraphe",
    "Japanese": "ハイパーグラフ",
    "Russian": "гиперграф"
  },
  {
    "English": "hyper-parameter tuning",
    "context": "1: 2017;Lopez-Paz and others 2017;Zenke, Poole, and Ganguli 2017). Several meta-reinforcement-learning methods aim at addressing the problem of transfer learning, few-shot shot adaption to new tasks after training on a distribution of similar tasks, and automated <mark>hyper-parameter tuning</mark> (Xu, van Hasselt, and Silver 2018;Gupta et al.<br>2: Since there is no official development set provided, we randomly select 1,200 samples from trainval as validation set (∼ 1 hour) for early stopping and <mark>hyper-parameter tuning</mark>. In addition, it provides a standard test set (0.9 hours) for evaluation.<br>",
    "Arabic": "ضبط المعلمات الفائقة",
    "Chinese": "超参数调优",
    "French": "réglage des hyperparamètres",
    "Japanese": "ハイパーパラメータチューニング",
    "Russian": "настройка гиперпараметров"
  },
  {
    "English": "hyperband",
    "context": "1: Gray-box approaches include Freeze-Thaw BO (Swersky et al., 2014), successive halving (Jamieson & Talwalkar, 2016), <mark>Hyperband</mark> (Li et al., 2017, Population-Based Training (Jaderberg et al., 2017), and hypernetwork-based approaches to HO (Lorraine & Duvenaud, 2018;MacKay et al., 2019).<br>",
    "Arabic": "تقنية هايبرباند",
    "Chinese": "超带",
    "French": "Hyperband",
    "Japanese": "ハイパーバンド",
    "Russian": "гипербэнд"
  },
  {
    "English": "hyperbolic space",
    "context": "1: that approach . One natural choice for this map is the exponential map exp x : T x M ∼ = R d . This approach has been taken, for instance, by Falorsi et al. (2019) and Bose et al. (2020), respectively parametrizing distributions on Lie groups and <mark>hyperbolic space</mark>.<br>2: A natural object in Riemannian geometry is the Ricci curvature, a bilinear form determining the geodesic dispersion, i.e. whether geodesics starting at nearby points with 'same' velocity remain parallel (Euclidean space), converge (spherical space), or diverge (<mark>hyperbolic space</mark>).<br>",
    "Arabic": "الفضاء الهايبربولي",
    "Chinese": "双曲空间",
    "French": "espace hyperbolique",
    "Japanese": "ハイパボリック空間",
    "Russian": "гиперболическое пространство"
  },
  {
    "English": "hyperedge",
    "context": "1: The notion of the hypergraph can thus be introduced to address this limitation. Consider a hypergraph example that individuals are connected via in-person social events, each gathering event can be represented as a <mark>hyperedge</mark> (Fig. 1a).<br>2: x , y, f w y ) for every <mark>hyperedge</mark> e = {a 1 , . . . , a k } ∈ E and every serialization w = a 1 • • • a k of e. \n<br>",
    "Arabic": "حافة فائقة",
    "Chinese": "超边",
    "French": "hyperarête",
    "Japanese": "超過辺",
    "Russian": "гиперребро"
  },
  {
    "English": "hypernym",
    "context": "1: # 5 ) ) is included in the factor of the set of implied relations \n ∆ T I(H 1 (continental, airline#n#2)) . Suppose we instead evaluate the first synset of airline, i.e., airline#n#1, with the gloss \"a hose that carries air under pressure.\" For this synset none of the other 20 relationships directly implied by <mark>hypernym</mark> evidence or the 5 relationships implied by the coordinate ev-7 Checking whether or not R is ∈ I ( H 1 il ) may be efficiently computed by checking whether s is in the <mark>hypernym</mark> ancestors of l or if it shares a least common subsumer with l within 7 steps<br>2: . }. One problem that arises from directly assigning the probability P ( H n ij |E H ij ) ∝ P ( H ij |E H ij ) for all n is the possibility of adding a novel hyponym to an overly-specific <mark>hypernym</mark> , which might still satisfy P ( H n ij |E H ij ) for a very large n. In order to<br>",
    "Arabic": "فوق تصنيفي",
    "Chinese": "上位词",
    "French": "hypernyme",
    "Japanese": "上位語",
    "Russian": "гипероним"
  },
  {
    "English": "hypernymy",
    "context": "1: Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, <mark>hypernymy</mark>, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora.<br>2: We think that a perfect DSM would be a multi-model structure which could handle every specific relation types (e.g., relatedness, similarity, antonymy, <mark>hypernymy</mark>, meronymy) of the words with maximum performances.<br>",
    "Arabic": "علاقة الارتباط العام",
    "Chinese": "上位词",
    "French": "hypernymie",
    "Japanese": "上位語関係",
    "Russian": "гиперонимия"
  },
  {
    "English": "hyperparameter optimization",
    "context": "1: In this section, we discuss additional related work on online learning algorithms, and on one special class of unrolled optimization problems: <mark>hyperparameter optimization</mark> (HO). Table 1 compares several approaches to gradient estimation in unrolled computation graphs, with respect to compute, memory, parallelization, unbiasedness, and smoothing.<br>2: This results in a very wide tree that captures all the hierarchical nature of the model hyperparameters, and allows the creation of a single <mark>hyperparameter optimization</mark> problem with four hierarchical layers of a total of 786 parameters.<br>",
    "Arabic": "تحسين المعلمات الفائقة",
    "Chinese": "超参数优化",
    "French": "optimisation des hyperparamètres",
    "Japanese": "ハイパーパラメータの最適化",
    "Russian": "оптимизация гиперпараметров"
  },
  {
    "English": "hyperparameter search",
    "context": "1: We implement all models in PyTorch. All models were trained on single GPUs of the type RTX6000 or RTX8000. <mark>Hyperparameter search</mark> Training hyperparameters for SRNs were found by informal search -we did not perform a systematic grid search due to the high computational cost.<br>",
    "Arabic": "البحث عن المعلمات الفائقة",
    "Chinese": "超参数搜索",
    "French": "recherche d'hyperparamètres",
    "Japanese": "ハイパーパラメータ探索",
    "Russian": "поиск гиперпараметров"
  },
  {
    "English": "hyperparameter selection",
    "context": "1: The primary motivation behind the considered methods is that they should lead to improved disentanglement scores. This raises the question how disentanglement is affected by the model choice, the <mark>hyperparameter selection</mark> and randomness (in the form of different random seeds). To investigate this, we compute all the considered disentanglement metrics for each of our trained models.<br>",
    "Arabic": "اختيار المُعلمات العليا",
    "Chinese": "超参数选择",
    "French": "sélection des hyperparamètres",
    "Japanese": "ハイパーパラメータ選択",
    "Russian": "выбор гиперпараметров"
  },
  {
    "English": "hyperparameter setting",
    "context": "1: Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and <mark>hyperparameter settings</mark> for each classifier.<br>",
    "Arabic": "إعداد المعلمة الفائقة",
    "Chinese": "超参数设置",
    "French": "réglage des hyperparamètres",
    "Japanese": "ハイパーパラメータ設定",
    "Russian": "настройка гиперпараметров"
  },
  {
    "English": "hyperparameter space",
    "context": "1: the CASH problem in WEKA . More experienced users of machine learning algorithms would not only select between a fixed set of default algorithms, but would also consider different hyperparameter settings -for example by performing a grid search over the <mark>hyperparameter space</mark> of a single classifier (as, e.g., implemented in WEKA 5 ).<br>2: In Figure 8(a) we see that ES converges to a suboptimal region of the <mark>hyperparameter space</mark> due to truncation bias, while PES finds the correct solution. Targeting Validation Accuracy. Because PES only requires function evaluations and not gradients, it can optimize non-differentiable objectives such as accuracy rather than loss.<br>",
    "Arabic": "مجال البارامترات الفائقة",
    "Chinese": "超参数空间",
    "French": "espace des hyperparamètres",
    "Japanese": "ハイパーパラメータ空間",
    "Russian": "пространство гиперпараметров"
  },
  {
    "English": "hyperplane",
    "context": "1: We consider a ball B(x, r n ) of radius r n around x (where r n is the current parameter of the r-neighborhood graph). The expected number of edges originating in x equals the expected number of points which lie in the intersection of this ball with the other side of the <mark>hyperplane</mark>.<br>2: The loss function penalizes the points which are misclassified by the threshold function h w , proportionally to the distance from the corresponding <mark>hyperplane</mark>, while it rewards the correctly classified points at a smaller rate.<br>",
    "Arabic": "مستوى فائق",
    "Chinese": "超平面",
    "French": "hyperplan",
    "Japanese": "超平面",
    "Russian": "гиперплоскость"
  },
  {
    "English": "hyperprior",
    "context": "1: In fact, if we assume the appropriate <mark>hyperprior</mark> on γ, then this correlated source method is essentially the same as the procedure from [15] but with an additional level in the approximate posterior factorization for handling the decomposition (6).<br>2: As defined above, this requires the computation of the integral (9), which is computationally undemanding. However, because the value of β is difficult to know, and because the function is sensitive to its value, the integral must also be over a <mark>hyperprior</mark> on β, rendering it much more challenging.<br>",
    "Arabic": "هايبربايريور",
    "Chinese": "超先验",
    "French": "hyperprior",
    "Japanese": "\"ハイパープライオール\"",
    "Russian": "гиперприорный"
  },
  {
    "English": "hyponym",
    "context": "1: The 26 supersenses used in WordNet 2.1 are listed in Table 1; we label a <mark>hyponym</mark> link as correct in the coarse-grained evaluation if the novel <mark>hyponym</mark> is placed under the appropriate supersense. This evaluation task    A single <mark>hyponym</mark>/hypernym pair is allowed to be simultaneously labeled 2 and 3.<br>2: We propose a search algorithm for findingT for the case of <mark>hyponym</mark> acquisition. We assume we begin with some initial (possibly empty) taxonomy T. We restrict our consideration of possible new taxonomies to those created by the single operation ADD-RELATION(R ij , T), which adds the single relation R ij to T. \n<br>",
    "Arabic": "تحت النوع",
    "Chinese": "下位词",
    "French": "hyponyme",
    "Japanese": "下位語",
    "Russian": "гипоним"
  },
  {
    "English": "hyponymy",
    "context": "1: A number of linguistic devices can be used to signal cohesion; these range from repetition, to synonymy, <mark>hyponymy</mark> and meronymy. Lexical chains are a representation of lexical cohesion as sequences of semantically related words (Morris and Hirst 1991) and provide a useful means for describing the topic flow in discourse.<br>2: Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, <mark>hyponymy</mark>, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora.<br>",
    "Arabic": "تفضيل النوعية",
    "Chinese": "上下位关系",
    "French": "hyponymie",
    "Japanese": "下位概念関係",
    "Russian": "гипонимия"
  },
  {
    "English": "hypothesis class",
    "context": "1: To obtain both direct and MP estimation, it seems promising then to examine algorithms that make use of non-iid sampling. Moreover, for MP, we observe that the auditing algorithm should leverage knowledge of the <mark>hypothesis class</mark> as well, which i.i.d sampling is agnostic to.<br>2: , D n }, supported on X × Y, a loss function ℓ : Y X × Z → [0, 1], and a <mark>hypothesis class</mark> H ⊂ Y X , a collaborative learning instance, (H, D), is formulated as the problem of finding a solution h ∈ Y X such that ( \n<br>",
    "Arabic": "فئة الفرضية",
    "Chinese": "假设类",
    "French": "classe d'hypothèses",
    "Japanese": "仮説クラス",
    "Russian": "класс гипотез"
  },
  {
    "English": "hypothesis set",
    "context": "1: We start with an initial <mark>hypothesis set</mark> H 1 , and at each time step t, a pruning function uses a pseudo-reference list R t of size r t to select H t+1 ⊆ H t .<br>2: In other words, we can save time by not computing precise utility estimates for hypotheses which are unlikely to be chosen in the end. We propose an iterative algorithm for MBR where the <mark>hypothesis set</mark> is gradually shrunk while the pseudo-reference list grows. The procedure is shown in Algorithm 1.<br>",
    "Arabic": "مجموعة الفرضيات",
    "Chinese": "假设集",
    "French": "ensemble d'hypothèses",
    "Japanese": "仮説集合",
    "Russian": "набор гипотез"
  },
  {
    "English": "hypothesis space",
    "context": "1: Lemma 1 shows that Condition 4 is necessary for the learnability of OOD detection. Lemma 1. Given a priori-unknown space D XY and a <mark>hypothesis space</mark> H, if OOD detection is learnable in D XY for H, then Conditions 1 and 4 hold. Proof of Lemma 1.<br>2: Given a <mark>hypothesis space</mark> H and a domain D XY , OOD detection is learnable in the single-distribution space D D XY<br>",
    "Arabic": "فضاء الفرضية",
    "Chinese": "假设空间",
    "French": "espace d'hypothèses",
    "Japanese": "仮説空間",
    "Russian": "пространство гипотез"
  },
  {
    "English": "hypothesis test",
    "context": "1: The LRT is a <mark>hypothesis test</mark> that facilitates the comparison of two models: one parametric stochastic model associated with the hypothesis that there is an anomaly, and another associated with the hypothesis that there is no anomaly-the so-called \"null model\".<br>2: This goal may be achieved via a <mark>hypothesis test</mark>, where the null hypothesis H 0 : p = q is tested against H 1 : p = q. The problem of testing goodness of fit has a long history in statistics [11], with a number of tests proposed for particular parametric models.<br>",
    "Arabic": "اختبار الفرضية",
    "Chinese": "假设检验",
    "French": "test d'hypothèse",
    "Japanese": "仮説検定",
    "Russian": "гипотезный тест"
  },
  {
    "English": "i.i.d",
    "context": "1: W I = W H , {I 1 , I 2 , • • • , I m } <mark>i.i.d</mark> ∼ G(k, W I ), y I = y H . ii) \n<br>2: (2021b), we set the fake output label as z i = max{y i , y 0.25 }. ments {ξ i } m i=1 <mark>i.i.d</mark> from the uniform distribution [0, 0.5]. Then the output label {y i } m i=1 via y i = x T i β + ξ i .<br>",
    "Arabic": "مستقلة وموزعة بشكل متساوٍ",
    "Chinese": "独立同分布 (dú lì tóng fēn pù)",
    "French": "i.i.d (indépendantes et identiquement distribuées)",
    "Japanese": "独立同分布",
    "Russian": "независимые и одинаково распределенные"
  },
  {
    "English": "i.i.d. sample",
    "context": "1: A distribution learning method or density estimation method is an algorithm that takes as input a sequence of <mark>i.i.d. samples</mark> generated from a distribution g, and outputs (a description of) a distribution g as an estimation for g. We work with continuous distributions in this paper, and so we identify a probability distribution by its probability density function.<br>",
    "Arabic": "عينة مستقلة متساوية التوزيع",
    "Chinese": "独立同分布样本",
    "French": "échantillon i.i.d.",
    "Japanese": "独立同一分布サンプル",
    "Russian": "i.i.d. выборка"
  },
  {
    "English": "idempotent",
    "context": "1: In case p is strictly improving this holds for any optimal solution. For an <mark>idempotent</mark> mapping p a linear mapping P : R I → R I satisfying δ(p(x)) = P δ(x) for all x ∈ X V is called its linear extension.<br>2: u ⊤ t A ⊤ t //apply the trace trick = tr A t E u t u t u ⊤ t A ⊤ t = tr A t 1 d I A ⊤ t = 1 d tr A t A ⊤ t = 1 d tr ( A t ) //a projection matrix is <mark>idempotent</mark> = Rank ( A t ) d \n<br>",
    "Arabic": "متطابق",
    "Chinese": "幂等的",
    "French": "idempotente",
    "Japanese": "冪等性",
    "Russian": "идемпотентный"
  },
  {
    "English": "identity function",
    "context": "1: Because g is constructed with the property that g(x) ≈ x, we can approximate its derivative as the derivative of the <mark>identity function</mark>: \n ∇ x g(x) ≈ ∇ x x = 1. Therefore, we can approximate the derivative of f (g(x)) at the pointx as: \n<br>",
    "Arabic": "دالة الهوية",
    "Chinese": "恒等函数",
    "French": "fonction identité",
    "Japanese": "恒等関数",
    "Russian": "тождественная функция"
  },
  {
    "English": "identity mapping",
    "context": "1: We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an <mark>identity mapping</mark> were optimal, it would be easier to push the residual to zero than to fit an <mark>identity mapping</mark> by a stack of nonlinear layers.<br>2: Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are <mark>identity mapping</mark>, and the other layers are copied from the learned shallower model.<br>",
    "Arabic": "تعيين الهوية",
    "Chinese": "恒等映射",
    "French": "mappage identité",
    "Japanese": "同一マッピング",
    "Russian": "Отображение тождества"
  },
  {
    "English": "identity matrix",
    "context": "1: A permutation matrix is an n by n Boolean matrix which is obtained from an <mark>identity matrix</mark> by a permutation of its columns. Hence, the permutation matrix contains a single value 1 in each row and each column. Finding a permutation matrix such that the sums of its diagonal elements form a given sequence of numbers (d 1 , .<br>2: We narrow down to the case when A = I is the <mark>identity matrix</mark>. Note that this case is actually in some sense the most typical case: when discretizing the continuous-time SSM to discrete-time by a step-size ∆, the discretized transition matrix A is brought closer to the identity.<br>",
    "Arabic": "مصفوفة الوحدة",
    "Chinese": "恒等矩阵",
    "French": "matrice identité",
    "Japanese": "単位行列",
    "Russian": "единичная матрица"
  },
  {
    "English": "identity transformation",
    "context": "1: The model properly handles this situation and generates an identical copy of the input image for every case. The ability to apply an <mark>identity transformation</mark> is essential to ensure that non-desired facial movement will not be introduced. For the non-zero cases, it can be observed how each AU is progressively accentuated.<br>2: Proportion 1 Denote <mark>identity transformation</mark> I as I(v) = v and dimension-shift transformation S as \n S(v) = (v d , v , v 2 , . . . , v d−1 ) \n<br>",
    "Arabic": "تحول الهوية",
    "Chinese": "身份变换",
    "French": "transformation de l'identité",
    "Japanese": "同一変換",
    "Russian": "тождественное преобразование"
  },
  {
    "English": "image analysis",
    "context": "1: The problem of graph matching -establishing correspondences between two graphs represented in terms of both local node structure and pair-wise relationships , be them visual , geometric or topological -is important in areas like combinatorial optimization , machine learning , <mark>image analysis</mark> or computer vision , and has applications in structure-from-motion , object tracking , 2d and 3d shape matching , image classification ,<br>2: This point has not escaped researchers in the computer vision and <mark>image analysis</mark> community, who have pioneered the notion of shape-space variability for the potentially more difficult problem of two-dimensional pattern detection in recent years. Work such as Amit et al.<br>",
    "Arabic": "تحليل الصور",
    "Chinese": "图像分析",
    "French": "analyse d'images",
    "Japanese": "画像解析",
    "Russian": "анализ изображений"
  },
  {
    "English": "image captioning",
    "context": "1: LiT also increased training scale and experimented with a combination of pre-trained image representations and contrastive fine-tuning to connect frozen image representations to text [94]. Flamingo introduced the first large vision-language model with in-context learning [2]. Other papers have combined contrastive losses with <mark>image captioning</mark> to further improve performance [43,89].<br>2: Particularly, we see applications of the dataset in image and text representation learning, image to text generation, <mark>image captioning</mark>, and other common multimodal tasks. Due to the breadth of the data, it also offers a unique opportunity for safety and low resource language researchers. We hope for LAION-5B to serve under-represented projects as well.<br>",
    "Arabic": "وصف الصورة",
    "Chinese": "图像描述",
    "French": "légende d'image",
    "Japanese": "画像キャプショニング",
    "Russian": "описание изображения"
  },
  {
    "English": "image classification",
    "context": "1: The problem of graph matching -establishing correspondences between two graphs represented in terms of both local node structure and pair-wise relationships , be them visual , geometric or topological -is important in areas like combinatorial optimization , machine learning , image analysis or computer vision , and has applications in structure-from-motion , object tracking , 2d and 3d shape matching , <mark>image classification</mark> ,<br>2: Although supervised pre-training is the dominant paradigm for <mark>image classification</mark>, curating large labeled image datasets is both expensive and time consuming. Instead of further scaling up labeling efforts, we can instead aspire to learn general purpose representations from the much larger set of available unlabeled images and fine-tune them for classification.<br>",
    "Arabic": "تصنيف الصور",
    "Chinese": "图像分类",
    "French": "classification d'images",
    "Japanese": "画像分類",
    "Russian": "классификация изображений"
  },
  {
    "English": "image compression",
    "context": "1: Generative image modeling is a central problem in unsupervised learning. Probabilistic density models can be used for a wide variety of tasks that range from <mark>image compression</mark> and forms of reconstruction such as image inpainting (e.g., see Figure 1) and deblurring, to generation of new images.<br>",
    "Arabic": "ضغط الصورة",
    "Chinese": "图像压缩",
    "French": "compression d'image",
    "Japanese": "画像圧縮",
    "Russian": "сжатие изображений"
  },
  {
    "English": "image denoise",
    "context": "1: A recent extension we will build on are the regression tree fields (RTF) by Jancsary et al. [14,15]. Image deblurring. Non-blind image deblurring is more difficult than <mark>image denoising</mark>, and it might be difficult to directly regress suitable model parameters.<br>2: We propose now to design a new vector-valued regularization PDE that follows desired local geometric properties (particularly for <mark>image denoising</mark>). These constraints will naturally define a specific form of regularization PDE, from the very generic form (12): \n . We do not want to mix diffusion contributions between image channels.<br>",
    "Arabic": "إزالة التشويش من الصورة",
    "Chinese": "图像去噪",
    "French": "Débruitage d'image",
    "Japanese": "画像ノイズ除去",
    "Russian": "шумоподавление изображения"
  },
  {
    "English": "image diffusion model",
    "context": "1: The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the <mark>image diffusion model</mark>, demonstrating the effectiveness of pretrained <mark>image diffusion model</mark>s as priors.<br>2: SpaText [6] maps segmentation masks into localized token embeddings. GLIGEN [48] learns new parameters in attention layers of diffusion models for grounded generating. Textual Inversion [21] and DreamBooth [75] can personalize content in the generated image by finetuning the <mark>image diffusion model</mark> using a small set of user-provided example images.<br>",
    "Arabic": "نموذج انتشار الصورة",
    "Chinese": "图像扩散模型",
    "French": "modèle de diffusion d'images",
    "Japanese": "画像拡散モデル",
    "Russian": "модель диффузии изображений"
  },
  {
    "English": "image embedding",
    "context": "1: The details of text-conditioning in text-to-image diffusion models are of high importance for visual quality and semantic fidelity. Ramesh et al. [54] use CLIP text embeddings that are translated into <mark>image embeddings</mark> using a learned prior, while Saharia et al. [61] use a pre-trained T5-XXL language model [53].<br>2: Then, during training, we prompt SAM with the extracted CLIP <mark>image embeddings</mark> as its first interaction. The key observation here is that because CLIP's <mark>image embeddings</mark> are trained to align with its text embeddings, we can train with <mark>image embeddings</mark>, but use text embeddings for inference.<br>",
    "Arabic": "تضمين الصورة",
    "Chinese": "图像嵌入",
    "French": "représentation d'image",
    "Japanese": "画像埋め込み",
    "Russian": "встраивание изображений"
  },
  {
    "English": "image encoder",
    "context": "1: This variant retains the task-specific parameters of ours, thus identical in terms of task adaptation; it utilizes the separate sets of task-specific biases in the <mark>image encoder</mark> to learn the training tasks T train , and fine-tune it on the support set of T test in the test time. The results are in Table 2.<br>2: • RGB frame: we use the frozen frame-wise <mark>image encoder</mark> φ I in MINECLIP to optimize for compute efficiency and provide the agent with good visual representations from the beginning (Sec. 4.2). • Task goal: φ G computes the text embedding of the natural language task goal.<br>",
    "Arabic": "مُرمِّز الصورة",
    "Chinese": "图像编码器",
    "French": "encodeur d'image",
    "Japanese": "画像エンコーダ",
    "Russian": "кодировщик изображений"
  },
  {
    "English": "image feature",
    "context": "1: To estimate the density of each block hypothesis, we implemented a superpixel-based local classifier which learns a mapping between <mark>image features</mark> (same as in [8]) and object densities. We learn the density classifier on a set of training images labeled with the three density classes.<br>2: in [ 25 ] . In training, we observe the class labels for examples from the first 40 classes. The system is trained to maximize the likelihood of the class labels, but indirectly it learns to also predict the (latent) attributes given the <mark>image features</mark>. At test time , the class labels are not observed ( and are drawn from a distinct set of 10 new labels ) ; however , the model can predict the attributes given the image , and since the mapping from attributes to classes is known ( for all 50 classes ) , the model can also ( indirectly ) predict the novel class<br>",
    "Arabic": "ميزة الصورة",
    "Chinese": "图像特征",
    "French": "caractéristiques de l'image",
    "Japanese": "画像特徴",
    "Russian": "\"признаки изображения\""
  },
  {
    "English": "image generation",
    "context": "1: Another recent success of multimodal learning is in <mark>image generation</mark>, where DALL-E [59] and later models [52,60,64,66,90] demonstrated the potential of text-guided <mark>image generation</mark> by producing high-quality images specific to the provided text. A critical ingredient in this new generation of image-text models is the pre-training dataset.<br>",
    "Arabic": "توليد الصور",
    "Chinese": "图像生成",
    "French": "génération d'images",
    "Japanese": "画像生成",
    "Russian": "генерация изображений"
  },
  {
    "English": "image inpainting",
    "context": "1: Generative image modeling is a central problem in unsupervised learning. Probabilistic density models can be used for a wide variety of tasks that range from image compression and forms of reconstruction such as <mark>image inpainting</mark> (e.g., see Figure 1) and deblurring, to generation of new images.<br>2: face detection , segmentation and image processing modules . Achieving a sophisticated edit such as `` Replace Barack Obama with Barack Obama wearing sunglasses '' ( object replacement ) , first requires identifying the object of interest , generating a mask of the object to be replaced and then invoking an <mark>image inpainting</mark> model ( we use Stable Diffusion ) with the original image , mask specifying the pixels to replace , and<br>",
    "Arabic": "تعبئة الصورة",
    "Chinese": "图像修复",
    "French": "inpainting d'image",
    "Japanese": "画像インペインティング",
    "Russian": "изображение"
  },
  {
    "English": "image patch",
    "context": "1: The activation of a filter on an <mark>image patch</mark> is an inner product between them. Typically, the number of possible filters are large (e.g., millions) and so scoring the test image is costly. Very recently, it was shown that scoring based only on filters with high activations performs well in practice [10].<br>",
    "Arabic": "قطعة الصورة",
    "Chinese": "图像块",
    "French": "\"portion d'image\"",
    "Japanese": "画像パッチ",
    "Russian": "патч изображения"
  },
  {
    "English": "image plane",
    "context": "1: x J ∼ (x 1 I Q + x 2 I R + x 3 I S)l K . Thus, any linear combination of the matrices Q, R and S maps lines l K in <mark>image plane</mark> K onto points in <mark>image plane</mark> J.<br>2: For example, a rotation in the <mark>image plane</mark> induces a rotation on C f p (for all points p). Similarly, a scaling in the <mark>image plane</mark> induces a scaling in C f p , a n d so forth for skew in the <mark>image plane</mark>.<br>",
    "Arabic": "مستوى الصورة",
    "Chinese": "图像平面",
    "French": "plan image",
    "Japanese": "画像平面",
    "Russian": "плоскость изображения"
  },
  {
    "English": "image processing",
    "context": "1: This is particularly important in the context of image and video processing (Protter & Elad, 2009), where it is common to learn dictionaries adapted to small patches, with training data that may include several millions of these patches (roughly one per pixel and per frame).<br>2: Because of these varieties of applications, image vision is getting popular and also is used in different fields [9], [10], [11]. In this study, thin section images were analyzed by using <mark>image processing</mark> in order to identify minerals.<br>",
    "Arabic": "\"معالجة الصور\"",
    "Chinese": "图像处理",
    "French": "traitement d'images",
    "Japanese": "画像処理",
    "Russian": "обработка изображений"
  },
  {
    "English": "image pyramid",
    "context": "1: Following [8], we compute a HOG feature pyramid by converting each level of a standard <mark>image pyramid</mark> into a \n A B C D E A B C D E A B C D E A ≥ B ≥ C ≥ E ≥ D \n<br>2: Our model consists of a pyramid of generators, {G 0 , . . . , G N }, trained against an <mark>image pyramid</mark> of x: {x 0 , . . . , x N }, where x n is a downsampled version of x by a factor r n , for some r > 1.<br>",
    "Arabic": "هرم الصور",
    "Chinese": "图像金字塔",
    "French": "pyramide d'images",
    "Japanese": "画像ピラミッド",
    "Russian": "пирамида изображений"
  },
  {
    "English": "image recognition",
    "context": "1: To this end, we evaluate our proposed classifiers on diverse datasets, covering a broad range of classification tasks (ranging from simple binary classification of synthetically generated data up to large-scale <mark>image recognition</mark> on the 1000-class Ima-geNet dataset). G50c [33] Letter [10] USPS [14] MNIST [20] Char74k [8] #<br>2: Our work tests the utility of multiple recent active learning methods on the open-ended understanding task of VQA. We draw on the dataset analysis literature to identify collective outliers as the bottleneck hindering active learning methods in this setting. Active Learning. Active learning strategies have been successfully applied to <mark>image recognition</mark> ( Joshi et al. , 2009 ; Sener and Savarese , 2018 ) , information extraction ( Scheffer et al. , 2001 ; Finn and Kushmerick , 2003 ; Jones et al. , 2003 ; Culotta and McCallum , 2005 ) , named entity recognition ( Hachey et al. , 2005 ; Shen et<br>",
    "Arabic": "التعرف على الصور",
    "Chinese": "图像识别",
    "French": "reconnaissance d'images",
    "Japanese": "画像認識",
    "Russian": "распознавание изображений"
  },
  {
    "English": "image representation",
    "context": "1: s img (v, σ) = c∈σ p(c|σ)h img (c, v),(6) \n where h img (c, v) is a hinge loss between the distances from the <mark>image representation</mark> v to the matching and unmatching (i.e.<br>2: For most such tasks, the quality of the results relies heavily on the chosen <mark>image representation</mark> and the distance metric used to compare examples.<br>",
    "Arabic": "تمثيل الصورة",
    "Chinese": "图像表示",
    "French": "représentation d'image",
    "Japanese": "画像表現",
    "Russian": "представление изображения"
  },
  {
    "English": "image restoration",
    "context": "1: Thus, we can easily specialize our generic expression into different regularization PDEs that fulfill desired smoothing behaviors, depending on the considered application: <mark>image restoration</mark>, inpainting, magnification, flow visualization, etc.<br>2: However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, <mark>image restoration</mark>, and scene reconstruction.<br>",
    "Arabic": "استعادة الصورة",
    "Chinese": "图像恢复",
    "French": "restauration d'image",
    "Japanese": "画像修復",
    "Russian": "восстановление изображения"
  },
  {
    "English": "image segmentation",
    "context": "1: One shows the segmentation boundaries for generic regions and objects, and the other shows the text and faces detected with  In the experiments, we observed that the face and text models improved the <mark>image segmentation</mark> results by comparison to our previous work (Tu and Zhu, 2002a) which only used generic region models.<br>2: Multi-class <mark>image segmentation</mark> and labeling is one of the most challenging and actively studied problems in computer vision. The goal is to label every pixel in the image with one of several predetermined object categories, thus concurrently performing recognition and segmentation of multiple object classes.<br>",
    "Arabic": "تجزئة الصورة",
    "Chinese": "图像分割",
    "French": "segmentation d'image",
    "Japanese": "画像セグメンテーション",
    "Russian": "сегментация изображений"
  },
  {
    "English": "image super-resolution",
    "context": "1: This notion of coupled dictionary learning has led to high performance algorithms for <mark>image super-resolution</mark> [Yang et al., 2010], allowing the reconstruction of high-res images from low-res samples, and for multi-modal retrieval [Zhuang et al., 2013] and cross-domain retrieval [Yu et al., 2014]. Given the factorization in Eq.<br>",
    "Arabic": "صورة فائقة الدقة",
    "Chinese": "图像超分辨率",
    "French": "super-résolution d'image",
    "Japanese": "画像超解像度",
    "Russian": "сверхразрешение изображения"
  },
  {
    "English": "image synthesis",
    "context": "1: David Marr has defined vision as the process of discovering from images what is present in the world, and where it is [15]. The combination of what and where captures the essence of an image at the semantic level and therefore, also plays a crucial role when defining the desired output of <mark>image synthesis</mark> tools.<br>2: GAN-based Image Synthesis: Generative Adversarial Networks (GANs) [24] have been shown to allow for photorealistic <mark>image synthesis</mark> at resolutions of 1024 2 pixels and beyond [6,14,15,39,40]. To gain better control over the synthesis process, many works investigate how factors of variation can be disentangled without explicit supervision.<br>",
    "Arabic": "تركيب الصور",
    "Chinese": "图像合成",
    "French": "synthèse d'images",
    "Japanese": "画像合成",
    "Russian": "синтез изображений"
  },
  {
    "English": "image translation",
    "context": "1: GANs have been shown to produce very realistic images with a high level of detail and have been successfully used for <mark>image translation</mark> [38,10,13], face generation [12,28] , super-resolution imaging [34,18], indoor scene modeling [12,33] and human poses editing [27]. Conditional GANs.<br>",
    "Arabic": "ترجمة الصور",
    "Chinese": "图像翻译",
    "French": "traduction d'images",
    "Japanese": "画像変換",
    "Russian": "перевод изображений"
  },
  {
    "English": "image-base rendering",
    "context": "1: Novel view synthesis. Classic <mark>image-based rendering</mark> (IBR) methods synthesize novel views by integrating pixel information from input images [58], and can be categorized according to their dependence on explicit geometry. Light field or lumigraph rendering methods [9,21,26,32] generate new views by filtering and interpolating sampled rays, without use of explicit geometric models.<br>",
    "Arabic": "التقديم القائم على الصورة",
    "Chinese": "基于图像的渲染",
    "French": "rendu basé sur l'image",
    "Japanese": "画像ベースレンダリング",
    "Russian": "рендеринг на основе изображений"
  },
  {
    "English": "image-text pre-training",
    "context": "1: Aligned with the success of transformer-based [64] language pre-training [11,42,76,52,30,8] and <mark>image-text pre-training</mark> [61,44,6,36,22,81,16,7], video-text pretraining [59,83,15,37,45,46] has shown promising results on video-and-language tasks.<br>2: Experiments across diverse tasks show that CLIPBERT outperforms (or is on par with) state-ofthe-art methods with densely sampled offline features, suggesting that the \"less is more\" principle is highly effective in practice. Comprehensive ablation studies reveal several key factors that lead to this success, including sparse sampling, end-to-end training, and <mark>image-text pre-training</mark>.<br>",
    "Arabic": "التدريب المسبق على الصور والنصوص",
    "Chinese": "图像文本预训练",
    "French": "pré-entraînement image-texte",
    "Japanese": "画像・テキスト事前学習",
    "Russian": "предварительное обучение изображений и текста"
  },
  {
    "English": "image-to-image translation",
    "context": "1: , 2021 ; , <mark>image-to-image translation</mark> ( Sasaki et al. , 2021 ) , shape generation ( Zhou et al. , 2021 ) and time series forecasting ( Rasul et al. , 2021 ) . Faster DPMs. Several works attempt to find short trajectories while maintaining the DPM performance. Chen et al.<br>2: Here the second dimension represents the unstructured \"cube-sphere\" computational mesh used by the climate model, which is a list of grid cell locations that span the surface of the sphere [49]. In contrast to typical <mark>image-to-image translation</mark> or spatio-temporal prediction problems in ML that involve data on a structured grid (i.e.<br>",
    "Arabic": "ترجمة الصورة إلى صورة",
    "Chinese": "图像到图像的转换",
    "French": "traduction d'image en image",
    "Japanese": "画像対画像変換 (image-to-image translation)",
    "Russian": "перевод изображения в изображение"
  },
  {
    "English": "imitation learning",
    "context": "1: Based on REINFORCE algorithm [51], the gradient of nondifferentiable, reward-based loss function can be derived as \n Unlabeled Instruction ! Navigator \" # Matching Critic $ % <mark>Imitation Learning</mark> Replay Buffer {& ' , & ( ,…, & ) } argmax $ % (!, &) &̂= \n<br>",
    "Arabic": "تعلم المحاكاة",
    "Chinese": "模仿学习",
    "French": "apprentissage par imitation",
    "Japanese": "模倣学習",
    "Russian": "подражательное обучение"
  },
  {
    "English": "immediate consequence operator",
    "context": "1: Operator I P is the standard <mark>immediate consequence operator</mark> of Datalog, but applied to the program P obtained by extending P with the rules from Section 3 encoding the semantics of limit predicates. Thus, all claims of this lemma hold in the usual way [Dantsin et al., 2001]. Lemma B.4.<br>2: This is evident from the fact that many results on justifications are formulated in terms of fixpoints of a so-called derivation operator that happens, for the case of logic programming, to coincide with (Fitting's three-valued version of) the <mark>immediate consequence operator</mark> for logic programs.<br>",
    "Arabic": "عامل النتيجة الفورية",
    "Chinese": "直接后果算子",
    "French": "opérateur de conséquence immédiate",
    "Japanese": "即時帰結演算子",
    "Russian": "оператор немедленного следствия"
  },
  {
    "English": "imperfect information",
    "context": "1: Because of its explicit problem setting with large decision space (∼10 161 information sets) and strategic complexity, HUNL has been an excellent benchmark and challenging problem for developing AI al-gorithms for studying the two-player zero-sum games with <mark>imperfect information</mark> (Bard et al. 2013;Jackson 2013).<br>",
    "Arabic": "معلومات غير كاملة",
    "Chinese": "不完全信息",
    "French": "information imparfaite",
    "Japanese": "不完全情報",
    "Russian": "неполная информация"
  },
  {
    "English": "implicit differentiation",
    "context": "1: Niemeyer et al. [29] represent surfaces as 3D occupancy fields and use a numerical method to find the surface intersection for each ray, then calculate an exact derivative using <mark>implicit differentiation</mark>. Each ray intersection location is provided as the input to a neural 3D texture field that predicts a diffuse color for that point. Sitzmann et al.<br>2: [ 47,48 ] . All results on the validation/test sets are Table 3. Comparison between loss functions by experiments conducted on the same dense correspondence network. For <mark>implicit differentiation</mark>, we minimize the distance metric of pose in Eq. (10) instead of the reprojection-metric pose loss in BPnP [12].<br>",
    "Arabic": "التفاضل الضمني",
    "Chinese": "隐式微分",
    "French": "différenciation implicite",
    "Japanese": "陰的微分",
    "Russian": "неявное дифференцирование"
  },
  {
    "English": "implicit function",
    "context": "1: In order to implement IDR+AD, we append the latent code z to the positional embeddings that are input to the <mark>implicit function</mark>, and we adjust the number of input channels of the first layer of the <mark>implicit function</mark> accordingly. IDR already takes as input the positional embeddings γ, so the extension IDR+γ does not apply here.<br>2: As mentioned in sec. 5, IPC represents shapes by converting a point cloud to an <mark>implicit function</mark> which is later rendered with EA raymarching.<br>",
    "Arabic": "وظيفة ضمنية",
    "Chinese": "隐式函数",
    "French": "fonction implicite",
    "Japanese": "暗黙の関数",
    "Russian": "неявная функция"
  },
  {
    "English": "implicit representation",
    "context": "1: From a single image, our network predicts the skeletal pose, and the rotation and translation parameters for each node in the deformation graph. In stark contrast to <mark>implicit representations</mark> [70,99,22], our mesh-based method tracks the surface vertices over time, which is crucial for adding semantics, and for texturing and rendering in graphics.<br>",
    "Arabic": "تمثيل ضمني",
    "Chinese": "隐式表示",
    "French": "représentation implicite",
    "Japanese": "暗黙の表現",
    "Russian": "неявное представление"
  },
  {
    "English": "implicit surface",
    "context": "1: Signed distance fields While opacity functions f o represent shapes with a measure of opaqueness of 3D spatial elements, signed distance fields f d (x, z) ∈ R, express the signed euclidean distance to the nearest point x ∈ S f on the <mark>implicit surface</mark> S f .<br>2: Point clouds In order to compare with a point cloudbased method, we devised an Implicit Point Cloud (IPC) baseline which represents shapes with a colored set of 3D points, converts the set into an <mark>implicit surface</mark> and then renders it with the EA raymarcher.<br>",
    "Arabic": "سطح ضمني",
    "Chinese": "隐式表面",
    "French": "surface implicite",
    "Japanese": "陰関数表面",
    "Russian": "неявная поверхность"
  },
  {
    "English": "importance sampling",
    "context": "1: STACX The IMPALA agent introduces specific form of <mark>importance sampling</mark> in the actor critic update and while STACX largely rely on the same <mark>importance sampling</mark> mechanism, it differs slightly to facilitate the meta-gradient flow. The actor-critic update in STACX is defined by Eq.<br>2: The sIS estimator is obtained by performing <mark>importance sampling</mark> on the conditional expectation of the reward with respect to a small subset of actions for each instance (a form of Rao-Blackwellization). We employ this estimator in a novel algorithmic procedure-named Policy Optimization for eXtreme Models (POXM)-for learning from bandit feedback on XMC tasks.<br>",
    "Arabic": "أخذ العينات ذات الأهمية",
    "Chinese": "重要性采样",
    "French": "échantillonnage d'importance",
    "Japanese": "重要度サンプリング",
    "Russian": "выборка по важности"
  },
  {
    "English": "importance sampling estimator",
    "context": "1: Indeed, the standard methodology is some form of importance sampling (Swaminathan and Joachims 2015a), and <mark>importance sampling estimators</mark> can run aground when their variance is too high (see, e.g., Lefortier et al. (2016)). Such variance is likely to be particularly virulent in large action spaces.<br>2: We derive a computationally efficient network inference algorithm and, via novel concentration inequalities for <mark>importance sampling estimators</mark>, prove that a polynomial complexity Monte Carlo version of the algorithm converges with high probability. Recently, Caffo et al.<br>",
    "Arabic": "مقدر أهمية العينات",
    "Chinese": "重要性采样估计器",
    "French": "estimateur d'échantillonnage d'importance",
    "Japanese": "重要度サンプリング推定子",
    "Russian": "оценщик на основе выборки по важности"
  },
  {
    "English": "importance weight",
    "context": "1: t ) G ( n ) t − v z ( s t ) , ( 4 ) \n where ρ t denotes an <mark>importance weight</mark> and G \n (n) t denotes an n-step bootstrap target.<br>2: L pred ≈ log 1 K K j=1 exp − 1 2 N i=1 f i (y j ) 2 q(y j ) vj (<mark>importance weight</mark>) ,(6) \n where v j compactly denotes the <mark>importance weight</mark> at y j . Eq.<br>",
    "Arabic": "وزن الأهمية",
    "Chinese": "重要性权重",
    "French": "poids d'importance",
    "Japanese": "重み付け係数",
    "Russian": "вес важности"
  },
  {
    "English": "in-context demonstration",
    "context": "1: We used models that are known to support task adaptation via <mark>in-context demonstrations</mark>: GPT-3 175B (davinci: Brown et al. 2020), GPT-3.5 (text-davinci-003 7 ), and Flan-T5 (base and XL: Chung et al. 2022).<br>2: full prompts ) . We used demonstrations that output the state of all boxes at once. However, in early experiments, Flan-T5 frequently only output the state of the first box even when the <mark>in-context demonstrations</mark> contained final descriptions of all box states. Therefore, for Flan-T5, we adjusted the prompts to output each box individually.<br>",
    "Arabic": "عرض في السياق",
    "Chinese": "上下文示范",
    "French": "démonstration en contexte",
    "Japanese": "コンテキスト内のデモンストレーション",
    "Russian": "демонстрация в контексте"
  },
  {
    "English": "in-context example",
    "context": "1: GQA. In Tab. 1 we evaluate different prompting strategies on the GQA testdev set. For the largest prompt size evaluated on the val set ( 24 <mark>in-context examples</mark> ) , we compare the random strategy consisting of the VISPROG 's best prompt chosen amongst 5 runs on the validation set ( each run randomly samples <mark>in-context examples</mark> from 31 annotated examples ) and the majority voting strategy which takes maximum consensus predictions for each question across 5 runs<br>2: We emphasize that neither the language model nor any of the modules are finetuned in any way. Adapting VISPROG to any task is as simple as providing a few <mark>in-context examples</mark> consisting of natural language instructions and the corresponding programs.<br>",
    "Arabic": "أمثلة في السياق",
    "Chinese": "上下文示例",
    "French": "exemples contextuels",
    "Japanese": "コンテクスト例",
    "Russian": "контекстный пример"
  },
  {
    "English": "in-context learner",
    "context": "1: Instruction quality Ultimately, we have seen how some instructions produce consistent and relatively well-performing responses across different models while others do not (see Section 4.1.4. We add this last factor to see which other types of factors help the <mark>in-context learner</mark> cope with varying instruction quality.<br>",
    "Arabic": "متعلم في السياق",
    "Chinese": "上下文学习者",
    "French": "apprenant en contexte",
    "Japanese": "コンテキスト学習者",
    "Russian": "в контексте обучающийся"
  },
  {
    "English": "in-context learning",
    "context": "1: We find that the prevalent mode of selecting random inputlabel pairs to construct the prompt-context is severely limited in the case of cross-lingual ICL, primarily due to the lack of alignment in the input as well as the output spaces. To mitigate this, we propose a novel prompt construction strategy -Cross-lingual In-context Source-Target Alignment (X-InSTA).<br>",
    "Arabic": "التعلم في السياق",
    "Chinese": "上下文学习",
    "French": "apprentissage en contexte",
    "Japanese": "コンテキスト内学習",
    "Russian": "обучение в контексте"
  },
  {
    "English": "in-degree distribution",
    "context": "1: We compute the Gini coefficient of the <mark>in-degree distribution</mark> of the graphs: for the YouTube graphs the Gini coefficient of in-degree for the harmful nodes is never below 90%; while for the NELA-GT graphs this index is never above 50%.<br>2: 3 . The <mark>in-degree distribution</mark> in our data shows a striking fit with a Zipf (more so than the power law) distribution; Figure 8 shows the in-degrees of pages from the May 1999 crawl plotted against both ranks and magnitudes (corresponding to the Zipf and power law cases).<br>",
    "Arabic": "\"توزيع الدرجة الداخلية\"",
    "Chinese": "入度分布",
    "French": "distribution des degrés entrants",
    "Japanese": "入次数分布",
    "Russian": "распределение входящих степеней"
  },
  {
    "English": "in-distribution",
    "context": "1: To generate transformed data and test the model's generalization capabilities across various styles, we adopt the SST-2 development set [156]. This is a sentiment analysis dataset comprising 872 instances, which serves as the base <mark>in-distribution</mark> dataset. Subsequently, for the OOD assessments, we implement two types of transformations: word-level substitutions and sentence-level style transformation.<br>2: In the context of pre-trained language models, several benchmarks have been proposed in the past to evaluate their OOD robustness given <mark>in-distribution</mark> training datasets and their corresponding OOD testing datasets [199,56,204,72].<br>",
    "Arabic": "في التوزيع",
    "Chinese": "分布内",
    "French": "dans la distribution",
    "Japanese": "分布内",
    "Russian": "внутрираспределение"
  },
  {
    "English": "in-domain",
    "context": "1: Our system decides when it should take its turn, generates human-like clarification requests when the patient pauses mid-utterance, answers indomain questions (grounding to the in-prompt knowledge), and responds appropriately to outof-domain requests (like generating jokes or quizzes).<br>2: In general, we observe that estimates of test time error are worst for random splits, slightly better for standard splits  (if those exist), better for heuristic and adversarial splits, but error still tends to be higher on new (<mark>in-domain</mark>) samples; see Figure 1.<br>",
    "Arabic": "في المجال",
    "Chinese": "领域内的",
    "French": "dans le domaine",
    "Japanese": "ドメイン内",
    "Russian": "внутридоменный"
  },
  {
    "English": "in-domain text",
    "context": "1: To study the influence of using <mark>in-domain text</mark>, we further pre-train the T5 model with <mark>in-domain text</mark> and an unsupervised span-mask denoising objective prior to the low-resource fine-tuning process.<br>",
    "Arabic": "النصوص داخل المجال",
    "Chinese": "领域内文本",
    "French": "texte dans le domaine",
    "Japanese": "分野内テキスト",
    "Russian": "внутридоменный текст"
  },
  {
    "English": "in-neighbor",
    "context": "1: where δ(i) = {j ∈ V : (j, i) ∈ E} is the set of <mark>in-neighbors</mark> of i, and c ∈ (0, 1) is a decay factor usually set to c = 0.8 [Jeh and Widom 2002] or c = 0.6 [Lizorkin et al. 2010].<br>",
    "Arabic": "الجار الوارد",
    "Chinese": "入邻居",
    "French": "voisin entrant",
    "Japanese": "内隣人",
    "Russian": "входные соседи"
  },
  {
    "English": "in-order traversal",
    "context": "1: Their algorithm involves an <mark>in-order traversal</mark> of the tree. Upon visiting each node, we generate a tag that includes the direction of the arc that attaches the node to its parent, i.e., whether that node is a left or a right child of its parent, and the label of the node. When traversing a BHT , this paradigm results in 6 distinct tag types : • → : this terminal node is the right child of its parent ; • → : this terminal node is the left child of its parent ; • ⇒ R ( ⇒ L ) : this non-terminal node is the right child of its parent and the head of<br>",
    "Arabic": "اجتياز بالترتيب",
    "Chinese": "中序遍历",
    "French": "parcours infixe",
    "Japanese": "中序木巡回",
    "Russian": "обход в прямом порядке"
  },
  {
    "English": "inception score",
    "context": "1: The FID [5] measures the distance between the distribution of the generated images and that of the real test images, both modelled as a multivariate Gaussian. Lower FID scores are better. The <mark>inception score</mark> of [9] for the complete pipeline is taken from their paper. The other scores are not reported there.<br>2: Finally, since there is no single objective, there is no way to measure arXiv:1802.05642v2 [cs.LG] 6 Jun 2018 progress. Application-specific proxies have been proposed, for example the <mark>inception score</mark> for GANs (Salimans et al., 2016), but these are little help during training -the <mark>inception score</mark> is no substitute for looking at samples.<br>",
    "Arabic": "درجة البدء",
    "Chinese": "初始分数",
    "French": "score d'inception",
    "Japanese": "インセプションスコア",
    "Russian": "показатель зачатия"
  },
  {
    "English": "incremental learning",
    "context": "1: In this work, we explore a more complex and comprehensive scenario for MNMT in <mark>incremental learning</mark>, taking into account the diversity of incremental languages. These incremental languages differ from the original languages in terms of their scripts and belong to different language families, which leads to a serious vocabulary and linguistic gap.<br>2: In this work, we propose a knowledge transfer method in <mark>incremental learning</mark> for MNMT, which leverages the knowledge from neural models. It can encourage original models to learn new knowledge from updated training data while naturally mitigating the issue of degradation on previous translation directions.<br>",
    "Arabic": "التعلم التدريجي",
    "Chinese": "增量学习",
    "French": "apprentissage incrémental",
    "Japanese": "インクリメンタルラーニング (incremental learning)",
    "Russian": "инкрементальное обучение"
  },
  {
    "English": "incremental parsing",
    "context": "1: An incremental algorithm for computing the function f updates f (x) efficiently each time x grows. In this spirit, <mark>incremental parsing</mark> updates a partial parse or parse chart each time a new word arrives (e.g. Earley, 1970;Huang and Sagae, 2010;Ambati et al., 2015;Damonte et al., 2017).<br>2: While this is the case in <mark>incremental parsing</mark> (Stern et al., 2017) and simultaneous translation (Gu et al., 2017), where inputs are incrementally available for conditioning, this is not the case in standard NMT (Sountsov and Sarawagi, 2016, Section 5), where inputs are available for conditioning in all generation steps.<br>",
    "Arabic": "التحليل التدريجي",
    "Chinese": "增量解析",
    "French": "analyse syntaxique incrémentielle",
    "Japanese": "インクリメンタル解析",
    "Russian": "пошаговый разбор"
  },
  {
    "English": "independent Cascade Model",
    "context": "1: Independent Cascade (IC) Model (Kempe, Kleinberg, and Tardos 2003). This model is a special case of the SIR model for epidemics. An infected node v infects each neighbor w with probability p. Equivalently, each edge (v, w) can be live with probability p, independently of all other edges.<br>2: The conceptually simplest model of this type is what one could call the <mark>Independent Cascade Model</mark>, investigated recently in the context of marketing by Goldenberg, Libai, and Muller [13,14]. We again start with an initial set of active nodes A0, and the process unfolds in discrete steps according to the following randomized rule.<br>",
    "Arabic": "نموذج تتالي مستقل",
    "Chinese": "独立级联模型",
    "French": "Modèle de cascade indépendante",
    "Japanese": "独立カスケードモデル",
    "Russian": "модель независимого каскада"
  },
  {
    "English": "independent component analysis",
    "context": "1: Experimental work has shown that using <mark>Independent Component Analysis</mark> as a projection method in Rotation Forest does not generally improve on Rotation Forest with Principal Component Analysis. Moreover, according to dominance ranks, Rotation Forest PCA is the preferred method.<br>2: Originally, Principal Components Analysis, PCA, [9], keeping all components, was used to generate the axes rotation. However, recent works suggest that, for microarray classification problems, <mark>Independent Component Analysis</mark> (ICA) [10], may be a better option.<br>",
    "Arabic": "تحليل المكونات المستقلة",
    "Chinese": "独立成分分析",
    "French": "Analyse en composantes indépendantes",
    "Japanese": "独立成分分析",
    "Russian": "Анализ независимых компонент"
  },
  {
    "English": "independent set",
    "context": "1: We show that there is a size-k <mark>independent set</mark> on G if and only if there is a signaling strategy that gives the principal payoff k •γ 2 /m (and the conversion between the <mark>independent set</mark> and this signaling strategy can be done in polynomial time).<br>2: As a result, an efficient approximation algorithm for computing an optimal signaling strategy, if exists, can be efficiently turned into a one for MAX-IND-SET while the approximation ratio is preserved. The \"only if\" direction. Suppose that there is a size-k <mark>independent set</mark> N * ⊆ N .<br>",
    "Arabic": "مجموعة مستقلة",
    "Chinese": "独立集",
    "French": "ensemble indépendant",
    "Japanese": "独立集合",
    "Russian": "независимое множество"
  },
  {
    "English": "independent variable",
    "context": "1: In this case, finding the potential cost of an <mark>independent variable</mark> flip requires us to solve a mini-SAT problem involving all the affected dependent variables and their associated clauses.<br>2: To calculate the cost of flipping a particular <mark>independent variable</mark> v i we need to know how many external gate variables will become false and how many will become true as a result of the flip.<br>",
    "Arabic": "متغير مستقل",
    "Chinese": "独立变量",
    "French": "variable indépendante",
    "Japanese": "独立変数",
    "Russian": "независимая переменная"
  },
  {
    "English": "indicator matrix",
    "context": "1: Additionally, for missing-view and missing-label, we use <mark>indicator matrix</mark> W ∈ 0, 1 n×l and G ∈ 0, 1 n×c to describe the missing instances distribution, respectively. Specifically, we set W i,j = 1 if the instance of j-th view corresponding to i-th sample is available.<br>",
    "Arabic": "مصفوفة المؤشر",
    "Chinese": "指示矩阵",
    "French": "matrice indicatrice",
    "Japanese": "指標行列",
    "Russian": "матрица индикаторов"
  },
  {
    "English": "indicator variable",
    "context": "1: is one when its argument is true and zero otherwise; we abbreviate [X i ] and [X i ] as x i and x i . To distinguish random variables from <mark>indicator variables</mark>, we use roman font for the former and italic for the latter.<br>2: (2005) (included in the MSTParser toolkit 10 ); for the higher-order models described in §3.3-3.5, we employed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the <mark>indicator variables</mark>. For scalability ( and noting that some of the models require O ( |V | • |A| ) constraints and variables , which , when A = V 2 , grows cubically with the number of words ) , we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence ( we<br>",
    "Arabic": "متغير الدلالة",
    "Chinese": "指示变量",
    "French": "variable indicatrice",
    "Japanese": "指示変数",
    "Russian": "индикаторная переменная"
  },
  {
    "English": "indicator vector",
    "context": "1: Letx ∈ X , so that φ(x) is a general component of φ, and let ex be the <mark>indicator vector</mark> ofx. For any δ ∈ R, the change in the lower bound L due to replacing φ(x) with φ(x) + δ is Remark.<br>2: edges is optimized ( see below ) . Graph Matching. Let v ∈ {0, 1} nm×1 be an <mark>indicator vector</mark> such that v ia = 1 if i ∈ V 1 is matched to a ∈ V 2 and 0 otherwise, while respecting one-to-one mapping constraints.<br>",
    "Arabic": "متجه المؤشر",
    "Chinese": "指示向量",
    "French": "vecteur indicateur",
    "Japanese": "指標ベクトル",
    "Russian": "вектор индикаторов"
  },
  {
    "English": "induce subgraph",
    "context": "1: Directed acyclic graphs (DAGs) contain only directed edges and no directed cycle. We refer to the neighbors of a vertex u in G as N G (u) and denote the <mark>induced subgraph</mark> of G on a set \n C ⊆ V by G[C].<br>2: We denote the degree of vertex v by d(v) = |N (v)|. Given a subset of vertices S ⊆ V , let E ( S ) be the set of edges that have both end-points in S. Then , G = ( S , E ( S ) ) is the subgraph induced by S. We use G ⊆ G to denote that G is a subgraph of G. The degree of a vertex v ∈<br>",
    "Arabic": "الفرعي المُحث",
    "Chinese": "诱导子图",
    "French": "sous-graphe induit",
    "Japanese": "誘導部分グラフ (Induced Subgraph)",
    "Russian": "индуцированный подграф"
  },
  {
    "English": "induce variable",
    "context": "1: Ideally, the number of <mark>inducing variables</mark> should be selected to make the KL(Q||P ) small. Currently, the most common advice is to increase the number of <mark>inducing variables</mark> M until the lower bound (eq. 3) no longer improves.<br>2: Using these interdomain <mark>inducing variables</mark> can lead to sparser representations, or computational benefits [Hensman et al., 2018]. Interdomain <mark>inducing variables</mark> are defined by \n L lower = − 1 2 y T Q −1 n y− 1 2 log|Q n |− N 2 log ( 2π ) − t 2σ 2 n ( 3 ) where Q n = Q ff + σ 2 n I , Q ff = K T uf K −1 uu K uf and t = Tr ( K ff − Q ff<br>",
    "Arabic": "متغير محفز",
    "Chinese": "诱导变量",
    "French": "variable d'induction",
    "Japanese": "誘導変数",
    "Russian": "индуцирующие переменные"
  },
  {
    "English": "induction hypothesis",
    "context": "1: In this subsection we present a claim to bound the \"convergence\" (namely, the 1 − logit y F (t) , X part) for the average multi-view data from T 0 till the end. Claim D.14 (multi-view till the end). Suppose Induction Hypothesis C.3 holds for every iteration t < T , and \n<br>2: Let h(i) be the sum of the ith column of A. We use a perfect matching in a suitable bipartite graph to construct the first row of B and then appeal to the <mark>induction hypothesis</mark> on an n − 1 by m relaxed manipulation matrix constructed by removing the values in the first row from A.<br>",
    "Arabic": "فرضية الاستقراء",
    "Chinese": "归纳假设",
    "French": "hypothèse d'induction",
    "Japanese": "帰納仮説",
    "Russian": "гипотеза индукции"
  },
  {
    "English": "inductive bias",
    "context": "1: by modeling transferability with the transfer family required 2  to map a hypothesis for one task onto a hypothesis for another [7], through information-based approaches [60], or through modeling <mark>inductive bias</mark> [6].<br>2: The set F of candidate features that we use in the experiments is the finite subset of F that contains features with syntactic complexity at most k. The methods we present, however, do not depend on how F is obtained. As discussed below, our methods have an <mark>inductive bias</mark> towards formulas that use simple features.<br>",
    "Arabic": "التحيز الاستقرائي",
    "Chinese": "归纳偏置",
    "French": "biais inductif",
    "Japanese": "帰納バイアス",
    "Russian": "индуктивное смещение"
  },
  {
    "English": "inductive learning",
    "context": "1: Baselines SynLink (Malaviya et al., 2020) proposed to densify the CSKG with synthetic links for better graph representation. InductiveE  introduced indutive learning on the CSKG by enhancing the unseen event representations with neighboring structure information. Evaluation Protocal To handle the evaluation mismatch between Rel-CSKGC and translation based methods, we designed a transformation strategy.<br>",
    "Arabic": "التعلم الاستقرائي",
    "Chinese": "归纳学习",
    "French": "apprentissage inductif",
    "Japanese": "帰納的学習",
    "Russian": "индуктивное обучение"
  },
  {
    "English": "inf",
    "context": "1: This is because the controller is one who can use the realization of the current state as available <mark>inf</mark>ormation, while the adversary is not, leading to the temptation to consider a formulation that puts the <mark>inf</mark> over κ t outside the sum of κ t−1 .<br>2: We still choose T large so that for all κ, |v π µ (T, κ) − v(µ, π, κ)| ≤ ǫ/2. Then, for any κ ∈ K S H , \n By definition of <mark>inf</mark> and that π ∈ Π C S , there exists κ ′ s.t.<br>",
    "Arabic": "أدنى",
    "Chinese": "最小值",
    "French": "inf",
    "Japanese": "inf",
    "Russian": "нижняя грань"
  },
  {
    "English": "inference",
    "context": "1: Probabilistic programming languages, for example languages that integrate with deep learning such as Pyro (Bingham et al., 2019), allow for specification and <mark>inference</mark> over some discrete domains. Most ambitiously, <mark>inference</mark> libraries such as Dyna (Eisner et al., 2004) allow for declarative specifications of dynamic programming algorithms to support <mark>inference</mark> for generic algorithms.<br>2: Building on these elements, Transformers adds extra user-facing features to allow for easy downloading, caching, and fine-tuning of the models as well as seamless transition to production. Transformers maintains some compatibility with these libraries, most directly including a tool for performing <mark>inference</mark> using models from Marian NMT and Google's BERT.<br>",
    "Arabic": "الاستدلال",
    "Chinese": "推理",
    "French": "inférence",
    "Japanese": "推論",
    "Russian": "Inference"
  },
  {
    "English": "inference algorithm",
    "context": "1: We investigate the main influencing factors to system accuracy, including the character sequence representations, word sequence representations, <mark>inference algorithm</mark>, pretrained embeddings, tag scheme, running environment and optimizer; analyzing system performances from the perspective of decoding speed and accuracies on in-vocabulary (IV) and out-of-vocabulary (OOV) entities/chunks/words.<br>2: • Our algorithm is iterative, and at each iteration, the joint inference problem is decomposed to a per-instance basis. This can be solved by the original <mark>inference algorithm</mark>. That is, our approach works as a metaalgorithm and developers do not need to implement a new <mark>inference algorithm</mark>.<br>",
    "Arabic": "خوارزمية الاستدلال",
    "Chinese": "推断算法",
    "French": "algorithme d'inférence",
    "Japanese": "推論アルゴリズム",
    "Russian": "алгоритм вывода"
  },
  {
    "English": "inference machinery",
    "context": "1: The only restriction on incoming [info] is that it is expressed in terms of the ontology -this is very general. However, the way in which [info] is used is completely specific -it will be represented as a set of linear constraints on one or more probability distributions in the world model. A chunk of [ info ] may not be directly related to one of Π 's chosen distributions or may not be expressed naturally as constraints , and so some <mark>inference machinery</mark> is required to derive these constraints -this inference is performed by model building functions , J s , that have been activated by a plan s chosen by Π. J D s<br>2: Our short probabilistic program is applicable to non-frontal faces and provides reasonable parses as illustrated above using only general-purpose <mark>inference machinery</mark>. For quantitative metrics, refer to section 4.1. \n and informed samplers [19].<br>",
    "Arabic": "آلية الاستدلال",
    "Chinese": "推理机制",
    "French": "machinerie d'inférence",
    "Japanese": "推論機構",
    "Russian": "механизм вывода"
  },
  {
    "English": "inference method",
    "context": "1: Because good quality data is difficult to come by in these contexts, we again find that <mark>inference methods</mark> are particularly useful to make the most of the resources that do exist.<br>2: In this section we describe in detail the idea behind our approach together with learning and <mark>inference methods</mark>. To simplify notation, for the remainder of this section, we drop the query index i, and work with general query q and document set D = {d 1 , ..., d m }.<br>",
    "Arabic": "طريقة الاستدلال",
    "Chinese": "推理方法",
    "French": "méthode d'inférence",
    "Japanese": "推論方法",
    "Russian": "метод вывода"
  },
  {
    "English": "inference problem",
    "context": "1: Instead of training sophisticated parametric models, these methods try to reduce the <mark>inference problem</mark> for an unknown image to that of matching to an existing set of annotated images. In [21], the authors estimate the pose of a human relying on 0.5 million training examples.<br>2: Our system decomposes an <mark>inference problem</mark> into a sequence of atomic edits linking premise to hypothesis; predicts a lexical entailment relation for each edit using a statistical classifier; propagates these relations upward through a syntax tree according to semantic properties of intermediate nodes; and composes the resulting entailment relations across the edit sequence.<br>",
    "Arabic": "مشكلة الاستدلال",
    "Chinese": "推理问题",
    "French": "problème d'inférence",
    "Japanese": "推論問題",
    "Russian": "проблема вывода"
  },
  {
    "English": "inference procedure",
    "context": "1: The three dimensions of the algorithm are generative vs. discriminative, the <mark>inference procedure</mark>, and the weight update. Poon and Domingos discussed generative gradient descent with marginal inference as well as EM with marginal and MPE inference.<br>2: We split learning into two subproblems: first, given an ordered sequence of feature templates and our <mark>inference procedure</mark>, we wish to learn parameters that optimize accuracy while using as few of those templates as possible. Second, given a method for training feature templated classifiers, we want to learn an ordering of templates that optimizes accuracy.<br>",
    "Arabic": "إجراء الاستدلال",
    "Chinese": "推理过程",
    "French": "procédure d'inférence",
    "Japanese": "推論手順",
    "Russian": "процедура вывода"
  },
  {
    "English": "inference process",
    "context": "1: For each of the selected features, and for each of a small discrete set of possible λ values λ ∈ {λ 1 , ..., λ M }, we run an <mark>inference process</mark> and evaluate the explicit conditional log likelihood.<br>2: The problem is posed as an <mark>inference process</mark> in a probabilistic graphical model. We show applications of this approach to identifying saliency in images and video, for detecting suspicious behaviors and for automatic visual inspection for quality assurance.<br>",
    "Arabic": "عملية الاستدلال",
    "Chinese": "推理过程",
    "French": "processus d'inférence",
    "Japanese": "推論プロセス",
    "Russian": "процесс вывода"
  },
  {
    "English": "inference rule",
    "context": "1: In contrast to (2), and all <mark>inference rules</mark> in Table 2, which have only conclusions of the form A B or A ∃R.B, repeated applications of (3) can produce axioms of the form A i ∃R. B j , where A i and B j are arbitrary conjunctions of atomic concepts.<br>2: The set of <mark>inference rules</mark> R in the categorial grammar then apply these composition functions to compose and categorize super-lexical signs. These <mark>inference rules</mark> will use variables f , g , h over meaning functions , variables k over referents for possible values of gaps , variables u ∈ U over primitive categories , variables c , d , e ∈ U × ( { -a , -b , -c , -d } × C ) * over categories with local arguments , and variables<br>",
    "Arabic": "قاعدة الاستدلال",
    "Chinese": "推理规则",
    "French": "règle d'inférence",
    "Japanese": "推論規則",
    "Russian": "правило вывода"
  },
  {
    "English": "inference stage",
    "context": "1: Besides, to avoid the interruption of tracklets caused by short-time occlusion, we use a lifecycle mechanism for the tracklets in the <mark>inference stage</mark>. Specifically, for each track query, it will be considered to disappear completely and be removed only when its corresponding classification score is smaller than 0.35 for a continuous period (2s in practice).<br>2: In this section, we aim to study whether GPT models can leak privacy-sensitive information which is provided during interactive conversations in the <mark>inference stage</mark>. This is in contrast to the previous evaluation in Section 8.1, where privacy-sensitive information is only provided during the training stage.<br>",
    "Arabic": "مرحلة الاستدلال",
    "Chinese": "推理阶段",
    "French": "étape d'inférence",
    "Japanese": "推論段階",
    "Russian": "стадия вывода"
  },
  {
    "English": "inference task",
    "context": "1: The Archipelago <mark>inference task</mark> to find the predictive distribution over class labels of an unlabeled datum x ⋆ , given N labeled data {x n , l n } N n=1 and P unlabeled data {x p } P p=1 , integrating out the latent function {g k (x)} K k=1 .<br>",
    "Arabic": "مهمة الاستدلال",
    "Chinese": "推理任务",
    "French": "tâche d'inférence",
    "Japanese": "推論タスク",
    "Russian": "задача вывода"
  },
  {
    "English": "inference time",
    "context": "1: Its knowledge also lacks context, hampering accurate application at <mark>inference time</mark>, e.g., \"kill requires eat breakfast\" is hard to make sense of without more context. A more directly relevant resource is ATOMIC , which consists of 877K textual descriptions of if-then knowledge. Each entry describes a likely cause/effect of one of 24K+ events.<br>2: • Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at <mark>inference time</mark> as conditioning [RWC + 19], but no weight updates are allowed.<br>",
    "Arabic": "وقت الاستدلال",
    "Chinese": "推理时间",
    "French": "temps d'inférence",
    "Japanese": "推論時間",
    "Russian": "время вывода"
  },
  {
    "English": "infinite-horizon",
    "context": "1: In this paper, we extend this setting to include interaction in an infinitehorizon Markov decision process (MDP), where rewards incurred depend on the state of the environment, the action performed, as well as an external parameter sampled from a known prior distribution at each step.<br>",
    "Arabic": "الأفق اللانهائي",
    "Chinese": "无限时间界限",
    "French": "horizon infini",
    "Japanese": "無限時間視野",
    "Russian": "бесконечный горизонт"
  },
  {
    "English": "influence diagram",
    "context": "1: The goal of our project was to replace the exhaustive test policy with a decision-theoretic policy that decides in Figure 2 shows the <mark>influence diagram</mark> that we developed to model the manufacturing process. It contains a set of nodes {F i , f i , I i , p i , V i } for each die i on the wafer.<br>2: To understand the probabilistic model of Figure 2, it is helpful to separate it out from the rest of the <mark>influence diagram</mark> (see Figure 3).<br>",
    "Arabic": "مخطط التأثير",
    "Chinese": "影响图",
    "French": "diagramme d'influence",
    "Japanese": "影響図",
    "Russian": "диаграмма влияния"
  },
  {
    "English": "influence function",
    "context": "1: Whenever the threshold functions fv at every node are monotone and submodular, the resulting <mark>influence function</mark> σ(•) is monotone and submodular as well.<br>2: The <mark>influence function</mark> studied above is the special case obtained by setting wv = 1 for all nodes v. The objective function with weights is submodular whenever the unweighted version is, so we can still use the greedy algorithm for obtaining a (1 − 1/e − ε)-approximation.<br>",
    "Arabic": "دالة التأثير",
    "Chinese": "影响函数",
    "French": "fonction d'influence",
    "Japanese": "影響関数",
    "Russian": "функция влияния"
  },
  {
    "English": "influence maximization",
    "context": "1: For the sake of concreteness in the introduction, we will discuss our results in terms of these two models in particular. Approximation Algorithms for <mark>Influence Maximization</mark>. We are now in a position to formally express the Domingos-Richardson style of optimization problem -choosing a good initial set of nodes to target -in the context of the above models.<br>",
    "Arabic": "تعظيم التأثير",
    "Chinese": "影响力最大化",
    "French": "maximisation de l'influence",
    "Japanese": "影響最大化",
    "Russian": "максимизация влияния"
  },
  {
    "English": "information Retrieval",
    "context": "1: However, additional considerations have been taken into account and some improvements have been applied as explained below. Query expansion is an approach to boost the performance of <mark>Information Retrieval</mark> (IR) systems. It consists of expanding a query with the addition of terms that are semantically correlated with the original terms of the query.<br>2: As the technology behind the industry increases in sophistication, neural architectures are gradually becoming more common (Tsagkias et al., 2020) and, with them, the need for accurate word embeddings for <mark>Information Retrieval</mark> (IR) and downstream Natural Language Processing (NLP) tasks . Unfortunately , the success of standard and contextual embeddings from the NLP literature ( Mikolov et al. , 2013a ; Devlin et al. , 2019 ) could not be immediately translated to the product search scenario , due to some peculiar challenges , such as short text , industry-specific jargon ( Bai et al. , 2018 ) , lowresource languages ; moreover , specific<br>",
    "Arabic": "استرجاع المعلومات",
    "Chinese": "信息检索",
    "French": "Recherche d'information",
    "Japanese": "情報検索",
    "Russian": "Информационный поиск"
  },
  {
    "English": "information bottleneck",
    "context": "1: We introduce an <mark>information bottleneck</mark> that limits the size of the discrete token vocabulary to as few as 32 distinct symbols per raw input token.<br>2: We try compressing to both discrete and continuous task-specific representations. Discrete representations yield an interpretable clustering of words. We also extend <mark>information bottleneck</mark> to allow us to control the contextual specificity of the token embeddings, making them more like type embeddings. This specialization method is complementary to the previous fine-tuning approach.<br>",
    "Arabic": "اختناق المعلومات",
    "Chinese": "信息瓶颈",
    "French": "goulot d'étranglement de l'information",
    "Japanese": "情報ボトルネック",
    "Russian": "информационное узкое место"
  },
  {
    "English": "information content",
    "context": "1: We measure the surprise associated with each object classes appearing in the scene S j by computing the <mark>information content</mark> of each combination of scene categories and object classes, I(O k |S j ), modulated by the probability of the object and scene categories.<br>2: The modified Resnik measure considers the semantic commonalities to be the <mark>information content</mark> of the lcs and the semantic differences to be the <mark>information content</mark> encompassed by concepts, minus the one already considered in the lcs.<br>",
    "Arabic": "محتوى المعلومات",
    "Chinese": "信息含量",
    "French": "contenu informationnel",
    "Japanese": "情報量",
    "Russian": "информационное содержание"
  },
  {
    "English": "information extraction",
    "context": "1: While traditional <mark>Information Extraction</mark> ( IE ) ( ARPA , 1991 ; ARPA , 1998 ) focused on identifying and extracting specific relations of interest , there has been great interest in scaling IE to a broader set of relations and to far larger corpora ( Banko et al. , 2007 ; Hoffmann et al. , 2010 ; Mintz et al. , 2009 ;<br>2: <mark>Information Extraction</mark> is rife with vague and competing terms for similar concepts, and we recognize some hazard in introducing generalized template extraction (GTE) into this landscape. To head off possible confusion, we highlight two important differences between this problem and the well established problem of event extraction (EE).<br>",
    "Arabic": "استخراج المعلومات",
    "Chinese": "信息提取",
    "French": "extraction d'informations",
    "Japanese": "情報抽出",
    "Russian": "извлечение информации"
  },
  {
    "English": "information gain",
    "context": "1: For the requirement of classification analysis, we use <mark>information gain</mark>, denoted by Inf oGain(v), to measure the good-ness of a specialization. Our selection criterion, Score(v), is to favor the specialization v → child(v) that has the maximum Inf oGain(v):  \n<br>2: Server software would need to store all known malicious executables and a comparably large set of benign executables. Due to the computational overhead of producing classifiers from such data, algorithms for computing <mark>information gain</mark> and for evaluating classification methods would have to be executed incrementally, in parallel, or both.<br>",
    "Arabic": "كسب المعلومات",
    "Chinese": "信息增益",
    "French": "gain d'information",
    "Japanese": "情報利得",
    "Russian": "прирост информации"
  },
  {
    "English": "information retrieval system",
    "context": "1: Also, such datasets do not focus on retrieving an exhaustive document set, instead limiting annotation to the top few results of a baseline <mark>information retrieval system</mark>.<br>2: We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations. 1 65 iting annotation to the top few results of a baseline 66 <mark>information retrieval system</mark>.<br>",
    "Arabic": "نظام استرجاع المعلومات",
    "Chinese": "信息检索系统",
    "French": "système de recherche d'information",
    "Japanese": "情報検索システム",
    "Russian": "система поиска информации"
  },
  {
    "English": "information set",
    "context": "1: Although the nodes of an imperfect information game tree are not independent in general, some decomposition is possible. For example, the sub-trees resulting from different preflop betting sequences can no longer have nodes that belong to the same <mark>information set</mark>.<br>2: Denote I u and I v to be all <mark>information set</mark>s for the min and max player. For an <mark>information set</mark> i ∈ I u ∪ I v , A i denotes the possible actions at <mark>information set</mark> i, while p i is the action (from the same player) preceding i.<br>",
    "Arabic": "مجموعة معلومات",
    "Chinese": "信息集",
    "French": "ensemble d'informations",
    "Japanese": "情報集合",
    "Russian": "набор информации"
  },
  {
    "English": "information theoretic",
    "context": "1: Neural models have been used to estimate the <mark>information theoretic</mark> contribution of meaning to gender (Williams et al., 2019) and of meaning and form to gender and declension class .<br>2: Example II-C.1), we shall examine this question under an appropriate <mark>information theoretic</mark> setup. To this end, as in random model R(K, C ), consider a function f generated with given K and λ.<br>",
    "Arabic": "نظرية المعلومات",
    "Chinese": "信息论",
    "French": "théorie de l'information",
    "Japanese": "情報理論的な",
    "Russian": "информационно-теоретический"
  },
  {
    "English": "information theoretic measure",
    "context": "1: In this paper we have suggested the use of <mark>information theoretic measures</mark> in order to improve efficiency of co-occurrence based automatic query expansion. The experiments were performed on TREC dataset. We have used standard KLD as one of the <mark>information theoretic measures</mark> and suggested a variant of KLD.<br>2: [ ] ∑ ∑∑ (8) In order to see the effect of <mark>information theoretic measures</mark>, we first selected the expansion terms using suitability value (equation 5) then equation (6 and 8) was used to rank the selected terms.<br>",
    "Arabic": "مقياس نظرية المعلومات",
    "Chinese": "信息论度量",
    "French": "mesure théorique de l'information",
    "Japanese": "情報理論的尺度",
    "Russian": "информационно-теоретическая мера"
  },
  {
    "English": "informer model",
    "context": "1: We perform the sensitivity analysis of the proposed <mark>Informer model</mark> on ETTh1 under the univariate setting. Input Length: In Fig. (4a), when predicting short sequences (like 48), initially increasing input length of encoder/decoder degrades performance, but further increasing causes the MSE to drop because it brings repeat short-term patterns.<br>",
    "Arabic": "نموذج المخبر",
    "Chinese": "信息提供者模型",
    "French": "modèle Informer",
    "Japanese": "インフォーマー・モデル (informa modoru)",
    "Russian": "Модель Informer"
  },
  {
    "English": "infoset",
    "context": "1: a and the payoff of each terminal node is multiplied by the probability of reaching it given chance probabilities . For i ∈ P, the following recursive formula defines player i's utility attainable at <mark>infoset</mark> I ∈ I i when a normal-form plan π i ∈ Π i is selected: \n<br>2: Differently from the trigger regret R T σ , which is defined only for the <mark>infoset</mark> J of σ, the subtree regrets R T σ,I are defined for all the <mark>infoset</mark>s I ∈ I i such that J I. Remark 1.<br>",
    "Arabic": "مجموعة معلومات",
    "Chinese": "信息集",
    "French": "ensemble d'informations",
    "Japanese": "情報集合",
    "Russian": "информационное множество"
  },
  {
    "English": "inhomogeneous Poisson process",
    "context": "1: We have so far defined a model for generating data from an <mark>inhomogeneous Poisson process</mark> using a GPbased prior for the intensity function.<br>2: With the ability to generate exact data, we can simulate a Markov chain on the posterior distribution of infinite-dimensional intensity functions without approximation and with finite computational resources. Our approach stands in contrast to other nonparametric Bayesian approaches to the <mark>inhomogeneous Poisson process</mark> in that it requires neither a crippling of the model, nor a finite-dimensional approximation.<br>",
    "Arabic": "عملية بواسون غير متجانسة",
    "Chinese": "非齐次泊松过程",
    "French": "processus de Poisson inhomogène",
    "Japanese": "不均一ポアソン過程",
    "Russian": "неоднородный пуассоновский процесс"
  },
  {
    "English": "initial distribution",
    "context": "1: An illustration of the framework is provided in Figure 1. We use S t ∈ S, A t ∈ A, and R t ∈ R as random variables for denoting the state, action and reward at time t ∈ {0, 1, . . . } within each episode. The first state , S 0 , comes from an <mark>initial distribution</mark> , d 0 , and the reward function R is defined to be only dependent on the state such that R ( s ) = E [ R t |S t = s ] for all s ∈ S. We assume that R t ∈ [ −R max , R max ]<br>",
    "Arabic": "التوزيع الأولي",
    "Chinese": "初始分布",
    "French": "distribution initiale",
    "Japanese": "初期分布",
    "Russian": "начальное распределение"
  },
  {
    "English": "initial state",
    "context": "1: Input: s 0 , <mark>initial state</mark> K, truncation length for partial unrolls N , number of particles σ, standard deviation of perturbations α, learning rate for ES optimization Initialize s = s 0 s (i) = s 0 Initialize ξ (i) ← 0 for i ∈ {1, . . .<br>2: -S is a set of all possible states in the system. There are three kinds of states in this set: one <mark>initial state</mark>, at least one final state, and none or several activity states which are not <mark>initial state</mark> or final states. -s 0 is the <mark>initial state</mark>. -Ag is a non-empty set of agents.<br>",
    "Arabic": "الحالة الأولية",
    "Chinese": "初始状态",
    "French": "état initial",
    "Japanese": "初期状態",
    "Russian": "начальное состояние"
  },
  {
    "English": "initial state distribution",
    "context": "1: The first set of instances are generated as follows. The transition probabilities and the <mark>initial state distribution</mark> are generated uniformly at random (and normalized to ensure that they sum up to 1). We also set an integer parameter n * , and change n * states to terminal states.<br>2: A Markov Decision Processes ( MDP ) is a tuple ( S , A , τ , µ 0 , R , γ ) where S is a set of states , A is a set of actions , τ : S×A S is a transition function , µ 0 ∈ ∆ ( S ) is an <mark>initial state distribution</mark> , R : S×A×S<br>",
    "Arabic": "توزيع الحالة الأولية",
    "Chinese": "初始状态分布",
    "French": "distribution de l'état initial",
    "Japanese": "初期状態分布",
    "Russian": "начальное распределение состояний"
  },
  {
    "English": "initialization",
    "context": "1: The proposed eBOA is described as follows: \n Step 1: Initialization: Set i ← 0, t ← 0. Randomly generate the initial population P(0). Evaluate P(0). Step 2: Selection: Select a set S(i) of promising solutions from P(i).<br>2: This formulation also improves the rate of convergence during optimization, provides guarantees on correctness, allows us to use advanced techniques for preconditioning and <mark>initialization</mark> (Section 4), and enables robust and multivariate generalizations of our solver (see the supplement).<br>",
    "Arabic": "تهيئة",
    "Chinese": "初始化",
    "French": "initialisation",
    "Japanese": "初期化",
    "Russian": "инициализация"
  },
  {
    "English": "injective function",
    "context": "1: A graph, G = (V, E), consists of a set of vertices, V , and a set of edges, E ⊆ V × V . Subgraph isomorphism can be viewed as defining a partial order among graphs. Given two graphs , G1 = ( V1 , E1 ) and G2 = ( V2 , E2 ) , G1 is called a subgraph of G2 , denoted G1 G2 , if there is an <mark>injective function</mark> ρ : V1 → V2 such that for any ( a , b ) ∈ E1 , ( ρ ( a ) , ρ ( b<br>",
    "Arabic": "دالة حاقنة",
    "Chinese": "单射函数",
    "French": "fonction injective",
    "Japanese": "単射関数",
    "Russian": "инъективная функция"
  },
  {
    "English": "inlier",
    "context": "1: Spinnet [1] extracts local features which are rotationally invariant and sufficiently informative to enable accurate registration. Some methods [3,9,14,27] focus on efficiently distinguishing correspondences as <mark>inliers</mark> and outliers.<br>2: Given a set of measurements X = {x i } N i=1 , the criterion aims to find the estimate θ that agrees with as many of the data as possible (i.e., the <mark>inliers</mark>) up to a threshold max θ, I⊆X |I| subject to r i (θ) ≤ ∀x i ∈ I, \n<br>",
    "Arabic": "العناصر المتوافقة",
    "Chinese": "内点",
    "French": "inlier",
    "Japanese": "イン-ライヤー",
    "Russian": "внутренняя точка"
  },
  {
    "English": "inner layer",
    "context": "1: , w n−1 of n words and its hidden state h k i = l k (w i |w) from the k-th <mark>inner layer</mark> l k of a language model with K layers, the model computes the word encoding e i as follows: \n<br>",
    "Arabic": "طبقة داخلية",
    "Chinese": "内层",
    "French": "couche interne",
    "Japanese": "内部層",
    "Russian": "внутренний слой"
  },
  {
    "English": "inner loop",
    "context": "1: To find a single solution for a data sample in the <mark>inner loop</mark>, the state-of-the-art \"solve & pick\" approach first computes all solutions of a minimal problem and then picks the optimal solutions by removing nonreal solutions, using inequalities, and evaluating the support.<br>2: The <mark>inner loop</mark> is similar to the \"black-box teaching\" algorithm of (Dasgupta et al., 2019) except that we are teaching µ(ĥ) as opposed toĥ itself.<br>",
    "Arabic": "الحلقة الداخلية",
    "Chinese": "内部循环",
    "French": "boucle interne",
    "Japanese": "内部ループ",
    "Russian": "внутренний цикл"
  },
  {
    "English": "inner node",
    "context": "1: In such a tree, <mark>inner nodes</mark> are labeled with a feature f ∈ F , edges with truth values, and leaf nodes with either \"solvable\" or \"unsolvable\".<br>",
    "Arabic": "عقدة داخلية",
    "Chinese": "内部节点",
    "French": "nœud interne",
    "Japanese": "内部ノード",
    "Russian": "внутренний узел"
  },
  {
    "English": "inner product",
    "context": "1: where w and w * are the weight vectors, b and b * are the bias terms, C is the a non-negative parameter which balances the loss term and the regularizer, the term ρ 2 w * , w * in Eq. ( 1 ) aims to restrict the capacity of the correcting function space , ρ > 0 is the trade-off parameter , the functions φ ( • ) and ψ ( • ) are two feature mappings induced by the kernels on example features and privileged features , respectively , and a , e denotes the <mark>inner product</mark> between two vectors a and e<br>2: Assume that f i ∈ F for all i = 1, . . . , d so that f ∈ F × • • • × F := F d where F d is equipped with the standard <mark>inner product</mark> f , g \n F d := d i=1 f i , g i F .<br>",
    "Arabic": "ضرب داخلي",
    "Chinese": "内积",
    "French": "produit scalaire",
    "Japanese": "内積",
    "Russian": "скалярное произведение"
  },
  {
    "English": "input",
    "context": "1: Let x ∈ X be an <mark>input</mark> and f (x; w) : X → R n be a function with parameters w that maps an <mark>input</mark> image (or bounding box) to a set of scores, one for each label. The form of f is not essential (e.g.<br>2: That is, s i →1 models the preference to map position i in the <mark>input</mark> to the first position in the output, and analogously s i →n models the preference to put i into the last position in the output.<br>",
    "Arabic": "المدخلات",
    "Chinese": "输入",
    "French": "entrée",
    "Japanese": "入力",
    "Russian": "ввод"
  },
  {
    "English": "input context",
    "context": "1: 2 ∈ S for comparison. Then, we ask human annotators to compare the outputs of the chosen systems on a randomly sampled <mark>input context</mark> and provide the comparison outcome as feedback to the learner. Specifically, we first sample an <mark>input context</mark> X (t) from the test dataset and obtain the generated texts \n<br>2: The learner, then, receives a feedback signal indicating the (human) preference between the selected systems on one <mark>input context</mark>, randomly sampled from the test dataset. The learner's objective is to reliably compute the topranked system with as few human annotations as possible.<br>",
    "Arabic": "تضمين السياق",
    "Chinese": "输入上下文",
    "French": "contexte d'entrée",
    "Japanese": "入力コンテキスト",
    "Russian": "входной контекст"
  },
  {
    "English": "input datum",
    "context": "1: Note that the multiple (parallel) network streams have different parameter numbers and receptive field sizes, corresponding to multiple scales. Input data are simultaneously fed into multiple streams, after which the concatenated feature responses produced by the various streams are fed into a global output layer to produce the final result.<br>2: The proposed lightweight visual domain prompts include two types: domain-specific vs, domain-agnostic prompts. As a messenger between the <mark>input data</mark> and the models, the visual domain prompts are directly added upon a portion of the input image and the decorated image is then input into the source model for predictions.<br>",
    "Arabic": "البيانات المدخلة",
    "Chinese": "输入数据",
    "French": "donnée d'entrée",
    "Japanese": "入力データ",
    "Russian": "входные данные"
  },
  {
    "English": "input embedding",
    "context": "1: We first calculate the normalized gradient (Luo et al., 2016) of each input token w.r.t the prediction of the next token: \n s m = g m 2 Lex n=1 g n 2 , \n where g m is the gradient vector of the <mark>input embedding</mark> e m .<br>2: • LFR (Gu et al., 2022): constraining the parameters of original models with low forget-  ting risk regions. We choose the LRF-CM for adapting new language pairs. • Prompt (Chalkidis et al., 2021): prepending prompts to the <mark>input embedding</mark> in the first layer.<br>",
    "Arabic": "تضمين المدخلات",
    "Chinese": "输入嵌入",
    "French": "plongement d'entrée",
    "Japanese": "入力埋め込み",
    "Russian": "входное вложение"
  },
  {
    "English": "input feature",
    "context": "1: where F c denotes the c th column/channel of the <mark>input feature</mark> F andF c denotes the corresponding filtered signal.<br>",
    "Arabic": "ميزة المُدخلات",
    "Chinese": "输入特征",
    "French": "caractéristique d'entrée",
    "Japanese": "入力特徴",
    "Russian": "входная функция"
  },
  {
    "English": "input feature vector",
    "context": "1: This Algorithm 2: The QuickScorer Algorithm Input : \n • x: <mark>input feature vector</mark> • T : ensemble of binary decision trees, with -w 0 , . . . , w |T |−1 : weights , one per tree thresholds : sorted sublists of thresholds , one sublist per feature -tree_ids : tree 's ids , one per threshold bitvectors : node bitvectors , one per threshold offsets : offsets of the blocks of triples v : result bitvectors , one per each tree leaves : output values , one per each tree<br>2: β ∈ R p Φ(X, β) = [Φ(x 1 , β), . . . , Φ(x n+1 , β)] ∈ R n+1 . Given an <mark>input feature vector</mark> x, the prediction of its output/label adjusted on the augmented data, can be defined as \n<br>",
    "Arabic": "متجه سمة الإدخال",
    "Chinese": "输入特征向量",
    "French": "vecteur de caractéristiques d'entrée",
    "Japanese": "入力特徴ベクトル",
    "Russian": "входной вектор признаков"
  },
  {
    "English": "input filter",
    "context": "1: The state inputs and output labels remain as in the grid-world experiments. We emphasize that the whole network is trained end-to-end, without pre-training the <mark>input filters</mark>.<br>",
    "Arabic": "مُرشِّح المُدخلات",
    "Chinese": "输入过滤器",
    "French": "filtre d'entrée",
    "Japanese": "入力フィルタ",
    "Russian": "входной фильтр"
  },
  {
    "English": "input formula",
    "context": "1: Let us define cutting planes with symmetry breaking to be the cutting planes proof system extended with a rule that allows to derive the constraint ⃗ x ⪯ lex ⃗ x↾ σ for any symmetry σ of the <mark>input formula</mark> F . Now we can ask whether cutting planes with redundance-based strengthening efficiently simulates this cutting planes proof system with symmetry breaking.<br>",
    "Arabic": "الصيغة المدخلة",
    "Chinese": "输入公式",
    "French": "formule d'entrée",
    "Japanese": "入力式",
    "Russian": "входная формула"
  },
  {
    "English": "input gate",
    "context": "1: c t ) \n Here i, f , o denote the input, forget, and output gate, h is the hidden state and c is the cell state. σ denotes the sigmoid function, indicates an element-wise product and * a convolution. W h denotes the hidden-to-state convolution kernel and W x the input-to-state convolution kernel.<br>2: To preserve the normalizing property of coupled gates, we perform coupling in a hierarchical manner: the <mark>input gate</mark> i y decides the weights for c y , and the forget gate f y shares the remaining weights between c l and c r . Given the hidden representation h y at the root, we score the global component as follows: \n<br>",
    "Arabic": "بوابة المدخلات",
    "Chinese": "输入门",
    "French": "porte d'entrée",
    "Japanese": "入力ゲート",
    "Russian": "входной затвор"
  },
  {
    "English": "input graph",
    "context": "1: In particular, the inserting pattern between a prompt graph and the <mark>input graph</mark> plays a very crucial role in the final performance. As previously discussed, the purpose of the promptbased method is to relieve the difficulty of traditional \"pre-train, fine-tuning\" by filling the gap between the pre-training model and the task head.<br>",
    "Arabic": "رسم بياني للإدخال",
    "Chinese": "输入图",
    "French": "graphe d'entrée",
    "Japanese": "入力グラフ",
    "Russian": "вводимый граф"
  },
  {
    "English": "input image",
    "context": "1: Formally, let I yo be the <mark>input image</mark> with the initial condition y o , y f the desired final condition, P o the data distribution of the <mark>input image</mark>, and P I the random interpolation distribution. Then, the critic loss L I (G, D I , I yo , y f ) we use is: \n<br>",
    "Arabic": "صورة الإدخال",
    "Chinese": "输入图像",
    "French": "image d'entrée",
    "Japanese": "入力画像",
    "Russian": "входное изображение"
  },
  {
    "English": "input layer",
    "context": "1: While a traditional generator [30] feeds the latent code though the <mark>input layer</mark> only, we first map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, before evaluating the nonlinearity.<br>2: For both problems, we use a Multi-Layer Perceptron (MLP) with 6 hidden layers of 100 neurons with bias. 6 The <mark>input layer</mark> has size dim P , and the output layer has size A + 1. We use the PReLU activation function. During training, we use the dropout before the last layer to prevent overfitting.<br>",
    "Arabic": "طبقة المدخلات",
    "Chinese": "输入层",
    "French": "couche d'entrée",
    "Japanese": "入力層",
    "Russian": "входной слой"
  },
  {
    "English": "input length",
    "context": "1: We train LOCOST on a maximum <mark>input length</mark> of 4,096 and evaluate it on the test set of arXiv with a maximum <mark>input length</mark> of 8,192 tokens. As shown in Table 6, this experiment confirms that LOCOST is indeed able to extrapolate to longer sequences than those employed in training.<br>",
    "Arabic": "طول الإدخال",
    "Chinese": "输入长度",
    "French": "longueur d'entrée",
    "Japanese": "入力長さ",
    "Russian": "длина ввода"
  },
  {
    "English": "input matrix",
    "context": "1: We then generalized the result for a matrix S whose rows are a weighted subset of the <mark>input matrix</mark> and their covariance matrix is the same.<br>2: Algorithm 10 also partitions the <mark>input matrix</mark> A = (A | b) from the previous section into m folds.<br>",
    "Arabic": "المصفوفة المُدخلة",
    "Chinese": "输入矩阵",
    "French": "matrice d'entrée",
    "Japanese": "入力行列",
    "Russian": "входная матрица"
  },
  {
    "English": "input point",
    "context": "1: At Line 5, we compute the set P j that contains the entire <mark>input points</mark>, where each point is restricted to only a subset of its coordinates whose indices are in I j .<br>",
    "Arabic": "نقطة إدخال",
    "Chinese": "输入点",
    "French": "point d'entrée",
    "Japanese": "入力点",
    "Russian": "точка ввода"
  },
  {
    "English": "input position",
    "context": "1: The first type of feature is active if the permutation V maps <mark>input position</mark> i to output position j (i.e. V ij = 1). We associate this feature with the score s i →j and use these scores only to model what the first and the last token in the output should be.<br>",
    "Arabic": "موضع الإدخال",
    "Chinese": "输入位置",
    "French": "position d'entrée",
    "Japanese": "入力位置",
    "Russian": "входное положение"
  },
  {
    "English": "input representation",
    "context": "1: (2018) add an attention mechanism in the prediction layer, as opposed to the hidden states. Vaswani et al. (2017) suggest a model which learns an <mark>input representation</mark> by self-attending over inputs. While these methods are all tailored to their specific tasks, they all inspire our choice of a self-attending mechanism.<br>2: The <mark>input representation</mark> of a span at each step is simply x = x enc + x mem -the sum of the original span embeddings x enc described in §3.1 and the current memory vector x mem .<br>",
    "Arabic": "تمثيل المدخلات",
    "Chinese": "输入表示",
    "French": "représeentation de l'entrée",
    "Japanese": "入力表現",
    "Russian": "входное представление"
  },
  {
    "English": "input resolution",
    "context": "1: To deal with this, we first resize our image to a lower resolution, which we call the <mark>input resolution</mark> (IR). Our models have IRs of either 32 2 × 3, 48 2 × 3, or 64 2 × 3. An IR of 32 2 × 3 is still quite computationally intensive.<br>",
    "Arabic": "دقة الإدخال",
    "Chinese": "输入分辨率",
    "French": "résolution d'entrée",
    "Japanese": "入力解像度",
    "Russian": "разрешение входного изображения"
  },
  {
    "English": "input sequence",
    "context": "1: Tacotron2 is an autoregressive model, meaning it predicts the speech parametersŷ t from both the <mark>input sequence</mark> of text x and the previous speech parameters y 1 , ..., y t−1 .<br>2: However, for dependency parsing, all dependency parsing-as-tagging schemes in the literature (Li et al., 2018;Strzyz et al., 2019;Vacareanu et al., 2020) have infinite tag sets whose cardinality grows with the length of the <mark>input sequence</mark>, which limits such parsers' efficiency and generality (Strzyz et al., 2019).<br>",
    "Arabic": "تسلسل الإدخال",
    "Chinese": "输入序列",
    "French": "séquence d'entrée",
    "Japanese": "入力シーケンス",
    "Russian": "входная последовательность"
  },
  {
    "English": "input space",
    "context": "1: <mark>Input space</mark> D; GP Prior µ 0 = 0, σ 0 , k for t = 1, 2, . . . do Choose x t = argmax x∈D µ t−1 (x) + β t σ t−1 (x) \n<br>",
    "Arabic": "فضاء الإدخال",
    "Chinese": "输入空间",
    "French": "espace d'entrée",
    "Japanese": "入力空間",
    "Russian": "пространство входных данных"
  },
  {
    "English": "input tensor",
    "context": "1: We compare the storage cost of Z T with those of competitors for storing preprocessed results. Note that memory requirements for a time range query are proportional to the storage cost since preprocessed results or an <mark>input tensor</mark> is the dominant term in the space cost.<br>2: For updating each factor matrix A ( ) , a dominant operation is to compute -mode products between an <mark>input tensor</mark> X (∈ 1 × ... × ) and factor matrices A ( ) (∈ × ) for = , ..., + 1, − 1, ..., 1 (line 4 of Algorithm 3).<br>",
    "Arabic": "مصفوفة المدخلات",
    "Chinese": "输入张量",
    "French": "tenseur d'entrée",
    "Japanese": "入力テンソル",
    "Russian": "входной тензор"
  },
  {
    "English": "input text",
    "context": "1: C = {c 1 , c 2 , • • • , c m } and Y = {y 1 , y 2 , • • • , y n } \n denote the <mark>input text</mark> and output text respectively, both consisting of a sequence of tokens from a vocabulary V. \n<br>",
    "Arabic": "نص الإدخال",
    "Chinese": "输入文本",
    "French": "texte d'entrée",
    "Japanese": "入力テキスト",
    "Russian": "входной текст"
  },
  {
    "English": "input token",
    "context": "1: A machine processes an input string x ∈ Σ * one token at a time. For each <mark>input token</mark>, the machine looks at the current input, state and top of the stack to make a transition into a new state and update its stack.<br>2: To fine-tune on GLUE, we represent the input sequence (for single sentence or sentence pairs) as described in Section 3, and use the final hidden vector C ∈ R H corresponding to the first <mark>input token</mark> ([CLS]) as the aggregate representation.<br>",
    "Arabic": "رمز المدخلات",
    "Chinese": "输入令牌",
    "French": "jeton d'entrée",
    "Japanese": "入力トークン",
    "Russian": "входной токен"
  },
  {
    "English": "input vector",
    "context": "1: To further show the efficacy of our proposed reformulation, we conduct experiments on synthetic data with high feature dimension and various sparsity. We apply the sprandn function in MATLAB to obtain the data matrix X ∈ R m×n , whose i-th row is <mark>input vector</mark> {x i } m i=1 . The noise measure- Wang et al.<br>2: Causal Pruning: A systematic and quantitative pruning of the <mark>input vector</mark> based on objectively assessed causal relationships to subsets of the target vector has been proposed as an attractive preprocessing strategy, as it helps remove spurious correlations due to confounding variables and optimize the ML algorithm [16].<br>",
    "Arabic": "متجه الإدخال",
    "Chinese": "输入向量",
    "French": "vecteur d'entrée",
    "Japanese": "入力ベクトル",
    "Russian": "вектор входных данных"
  },
  {
    "English": "input-output pair",
    "context": "1: We are interested in learning a discriminant function F : X × Y → R over <mark>input-output pairs</mark> which can be used to predict the output y for an input x: \n h(x, w) = argmax y∈Y F (x, y, w)(1) \n where w denotes the parameter vector to be learned.<br>",
    "Arabic": "زوج المدخلات والمخرجات",
    "Chinese": "输入-输出对",
    "French": "paire d'entrée-sortie",
    "Japanese": "入力-出力ペア",
    "Russian": "входно-выходная пара"
  },
  {
    "English": "instance",
    "context": "1: Furthermore, our comprehensive analysis shows that the learned complexity and reliability are explainable, thus helping to explain how our model infers the correct label for each <mark>instance</mark>. Lastly, an error analysis was carried out to understand the incorrect predictions.<br>2: To classify an unknown <mark>instance</mark>, the performance element finds the example in the collection most similar to the unknown and returns the example's class label as its prediction for the unknown. For Boolean attributes, such as ours, a convenient measure of similarity is the number of values two <mark>instance</mark>s have in common.<br>",
    "Arabic": "مثال",
    "Chinese": "实例",
    "French": "instance",
    "Japanese": "インスタンス",
    "Russian": "экземпляр"
  },
  {
    "English": "instance level",
    "context": "1: Search solutions on average are diverse but we know that an <mark>instance level</mark> a more fine-grained approach to set α and β is needed. Our approach also seems compatible with parallelization, which could yield very efficient anytime algorithms.<br>",
    "Arabic": "مستوى المثيل",
    "Chinese": "实例级别",
    "French": "niveau d'instance",
    "Japanese": "インスタンスレベル",
    "Russian": "уровень экземпляра"
  },
  {
    "English": "instance normalization",
    "context": "1: There have been existing methods, such as Layer Normalization (LN) [3] and <mark>Instance Normalization</mark> (IN) [61] (Figure 2), that also avoid normalizing along the batch dimension. These methods are effective for training sequential models (RNN/LSTM [49,22]) or generative models (GANs [15,27]).<br>2: Layer Normalization (LN) [3] operates along the channel dimension, and <mark>Instance Normalization</mark> (IN) [61] performs BN-like computation but only for each sample (Figure 2). Instead of operating on features, Weight Normalization (WN) [51] proposes to normalize the filter weights.<br>",
    "Arabic": "تحييد المثيل",
    "Chinese": "实例归一化",
    "French": "normalisation d'instance",
    "Japanese": "インスタンス正規化",
    "Russian": "нормализация экземпляра"
  },
  {
    "English": "instance segmentation",
    "context": "1: We illustrate such generalization with an additional task of <mark>instance segmentation</mark> (Fig. 9). However, there are several practical concerns that need to be addressed. Densely annotating video frames for <mark>instance segmentation</mark> is almost prohibitively expensive. Therefore, we adopt offline pseudo ground truth (Section 3.4) to evaluate streaming performance.<br>2: This gives us a singlemodel result of 41.8 mask AP and 47.3 box AP. The above result is the foundation of our submission to the COCO 2017 competition (which also used an ensemble, not discussed here). The first three winning teams for the <mark>instance segmentation</mark> task were all reportedly based on an extension of the Mask R-CNN framework.<br>",
    "Arabic": "تقسيم المثيلات",
    "Chinese": "实例分割",
    "French": "segmentation d'instances",
    "Japanese": "インスタンスセグメンテーション",
    "Russian": "сегментация экземпляров"
  },
  {
    "English": "instance selection",
    "context": "1: Estimating instance difficulty is also evocative of <mark>instance selection</mark> for active learning (Lewis & Catlett, 1994;Fu et al., 2013;Liu & Motoda, 2002); however these estimates could change as the dataset picks up new instances. In contrast, PVI estimates are relatively stable, especially when the dataset has higher V-information.<br>2: [16] presented an algorithm that selects a subset of source samples that are distributed most similarly to the target domain. Another technique that deals with <mark>instance selection</mark> has been proposed by Sangineto et al. [35]. They train weak classifiers on random partitions of the target domain and evaluate them in the source domain.<br>",
    "Arabic": "اختيار المثيل",
    "Chinese": "实例选择",
    "French": "sélection d'instances",
    "Japanese": "インスタンス選択",
    "Russian": "выбор экземпляров"
  },
  {
    "English": "instance space",
    "context": "1: We consider a framework similar to label ranking [9] or subset ranking [8]. Let X be a measurable space (the <mark>instance space</mark>). An instance x ∈ X represents a query and its associated n items to rank, for an integer n ≥ 3.<br>2: , x n ) consists of values x ∈ dom(X) for every feature X. This <mark>instance space</mark> is denoted x ∈ X = dom(X 1 ) × • • • × dom(X n ).<br>",
    "Arabic": "فضاء الحالات",
    "Chinese": "实例空间",
    "French": "espace d'instances",
    "Japanese": "インスタンス空間",
    "Russian": "пространство экземпляров"
  },
  {
    "English": "instruction tuning",
    "context": "1: n't enough information in the provided text for me to generate an answer '' ) . GPT-4 achieves lower information recovery accuracy than GPT-3.5 under context prompts, likely due to the same reason for <mark>instruction tuning</mark> against incomplete prompts. In general, a longer context tends to elicit more accurate information leakage.<br>2: As a special form of prompt tuning, in-context learning (Xie et al., 2021;Min et al., 2021) takes one or a few examples as the prompt to demonstrate the task. Instruction tuning (Wei et al., 2021) is another simple yet effective strategy to improve the generalizability of large language models.<br>",
    "Arabic": "ضبط التعليمات",
    "Chinese": "指令调优",
    "French": "ajustement des instructions",
    "Japanese": "インストラクションチューニング",
    "Russian": "настройка инструкций"
  },
  {
    "English": "integer linear program",
    "context": "1: We formulate the problem of nonprojective dependency parsing as a polynomial-sized <mark>integer linear program</mark>. Our formulation is able to handle non-local output features in an efficient manner; not only is it compatible with prior knowledge encoded as hard constraints, it can also learn soft constraints from data.<br>2: 1. Depending on whether A is a min or a max predicate, we check whether there is a smallest/largest value for s in all solutions to C(r, J)-that is, we check whether the <mark>integer linear program</mark> 'minimise/maximise s subject to C(r, J)' is bounded. Byrd et al.<br>",
    "Arabic": "برنامج خطي صحيح",
    "Chinese": "整数线性规划",
    "French": "programme linéaire en nombres entiers",
    "Japanese": "整数線形計画問題",
    "Russian": "целочисленная линейная программа"
  },
  {
    "English": "integer program",
    "context": "1: A sample <mark>integer program</mark> (IP) looks like this: A solution to an IP is an assignment of integer values to variables. Solutions are constrained by inequalities involving linear combinations of variables. An optimal solution is one that respects the constraints and minimizes the value of the objective function, which is also a linear combination of variables.<br>2: y i = 1 if w i is in the compression 0 otherwise ∀i ∈ [1 . . . n] \n A trigram language model forms the backbone of the compression model. The language model is formulated as an <mark>integer program</mark> with the introduction of extra decision variables indicating which word sequences should be retained or dropped from the compression.<br>",
    "Arabic": "برنامج صحيح",
    "Chinese": "整数规划",
    "French": "programme entier",
    "Japanese": "整数計画問題",
    "Russian": "целочисленная программа"
  },
  {
    "English": "integral image",
    "context": "1: After constructing d(d+1)/2 <mark>integral images</mark>, the covariance descriptor of any rectangular region can be computed in O(d 2 ) time independent of the region size. We refer readers to [21] for more details of the descriptors and computational method.<br>2: The descriptor encodes information of the variances of the defined features inside the region, their correlations with each other and spatial layout. There is an efficient way to compute covariance descriptors using <mark>integral images</mark> [21].<br>",
    "Arabic": "صورة تكاملية",
    "Chinese": "积分图像",
    "French": "image intégrale",
    "Japanese": "積分画像",
    "Russian": "интегральное изображение"
  },
  {
    "English": "integral operator",
    "context": "1: Let H be a reproducing kernel Hilbert space (RKHS) with norm • H and associated kernel (representer of evaluation) K : X × X → R. Letting P be a distribution on X , Mercer's theorem [e.g. 18] implies that the <mark>integral operator</mark> \n T K : L 2 ( X , P ) → L 2 ( X , P ) defined by T K ( f ) ( x ) = K ( x , z ) dP ( z ) is compact , and K ( x , x ′ ) = ∞ j=1 λ j φ j ( x ) φ j ( z<br>2: The core idea of our proof is to use upper bounds on the KL divergence that depend on the quality of a Nyström approximation to the data covariance matrix. Using existing results, we show this error can be understood in terms of the spectrum of an infinite-dimensional <mark>integral operator</mark>.<br>",
    "Arabic": "- المشغل التكاملي",
    "Chinese": "积分算子",
    "French": "opérateur intégral",
    "Japanese": "積分作用素",
    "Russian": "интегральный оператор"
  },
  {
    "English": "integral probability metric",
    "context": "1: In this case, the objective Eq. (1) reduces to the maximin problem: max π∈Π min f ∈F L µ (π, f ), which always yields robust policy improvement under Assumption 1. More generally , if the function class F is rich enough to approximate all bounded , Lipschitz functions , then the above objective with β = 0 resembles behavior cloning to match the occupancy measures of π and µ using an <mark>integral probability metric</mark> ( IPM ; Müller , 1997 ) ( or equivalently , Wasserstein GAN ; Arjovsky et al. , 2017 )<br>",
    "Arabic": "المقياس الاحتمالي التكاملي",
    "Chinese": "积分概率度量",
    "French": "métrique de probabilité intégrale",
    "Japanese": "積分確率メトリック",
    "Russian": "интегральная вероятностная метрика"
  },
  {
    "English": "integrity constraint",
    "context": "1: However, in this way, the top ranked labels tend to be too specific to cover the general meaning of the topic (e.g., \"<mark>integrity constraints</mark>\", \"transitive closure\", etc).<br>",
    "Arabic": "قيد النزاهة",
    "Chinese": "完整性约束",
    "French": "contrainte d'intégrité",
    "Japanese": "整合性制約",
    "Russian": "целостные ограничения"
  },
  {
    "English": "intelligent agent",
    "context": "1: Recent work has demonstrated that IOC is a powerful technique for modeling the decision-making behavior of <mark>intelligent agents</mark> in problems as diverse as robotics (Ratliff et al., 2009), personal navigation (Ziebart et al., 2008), and cognitive science (Ullman et al., 2009).<br>2: Humans communicate with each other in this interactive and dynamic visual world via languages, signs, and gestures. The ability to jointly understand both visual and  textual clues is an essential ability for <mark>intelligent agents</mark> to interpret multimodal signals in the physical world.<br>",
    "Arabic": "وكيل ذكي",
    "Chinese": "智能体",
    "French": "agent intelligent",
    "Japanese": "知的エージェント",
    "Russian": "интеллектуальный агент"
  },
  {
    "English": "intensity function",
    "context": "1: We wish to generate a set of events {s k } K k=1 on some subregion T of S which are drawn according to a Poisson process whose <mark>intensity function</mark> λ(s) is the result of applying Equation 1 to a random function g(s) drawn from the GP.<br>2: The motivation for the Gaussian Cox process is primarily the ease with which one can specify prior beliefs about the variation of the <mark>intensity function</mark> of a Poisson process, without specifying a particular functional form.<br>",
    "Arabic": "دالة الشدة",
    "Chinese": "强度函数",
    "French": "fonction d'intensité",
    "Japanese": "強度関数",
    "Russian": "функция интенсивности"
  },
  {
    "English": "intent",
    "context": "1: We first define the dialog history H as the concatenation of the alternating utterances from the user and system turns, respectively, without the last system utterance which we denote as S. Each system utterance comes with a system dialog act S ACT denoted as the concatenation of the <mark>intent</mark> I and slot-value pairs (s, v) as follows: \n<br>2: The core idea is that two utterances with the same signature and same slot values but different carrier phrases must be paraphrases. We define the signature of an annotated utterance to be the set of its <mark>intent</mark> and slot names, e.g. {PlayMusic, Artist, Song}.<br>",
    "Arabic": "القصد",
    "Chinese": "意图",
    "French": "intention",
    "Japanese": "意図",
    "Russian": "намерение"
  },
  {
    "English": "inter-annotator agreement",
    "context": "1: The robustness of the evaluation of chatbots is often hampered by <mark>inter-annotator agreement</mark> (IAA) (Gandhe and Traum, 2016).<br>2: We formulated an Amazon Mechanical Turk experiment to annotate 25K of these with up to three intents that the crowd workers felt were most important or relevant (<mark>inter-annotator agreement</mark> = 0.73). Our intent-annotated dataset consists of 12 diverse genres (e.g. DIY, Life Hacks, Data Science, WebApps) and hundreds of unique intents.<br>",
    "Arabic": "مدى الاتفاق بين المصححين",
    "Chinese": "注释者间一致性",
    "French": "accord inter-annotateurs",
    "Japanese": "複数アノテータ間の一致率",
    "Russian": "межаннотационная согласованность"
  },
  {
    "English": "interaction matrix",
    "context": "1: Similarly, most entries of the previous <mark>interaction matrix</mark> Q t−1 can be reused and just need to be weighted with the temporal discount e −λ . The cost of the optimization problem depends on the connectedness of the matrix Q, i.e. on the number of nonzero interactions between hypotheses.<br>2: with an indicator vector m = {m 1 , . . . , m M }, where m i = 1 if h i is selected and 0 otherwise; and an <mark>interaction matrix</mark> Q. Here, we pursue a similar approach.<br>",
    "Arabic": "مصفوفة التفاعل",
    "Chinese": "互动矩阵",
    "French": "matrice d'interaction",
    "Japanese": "相互作用行列",
    "Russian": "матрица взаимодействий"
  },
  {
    "English": "interest point",
    "context": "1: The process i) detects a small number of <mark>interest points</mark>, ii) computes their visual descriptors, iii) matches them with a nearest neighbor search, and iv) verifies the matches with two-view epipolar estimation and RANSAC. The correspondences then serve for relative or absolute pose estimation and 3D triangulation.<br>2: Indexing Local Patch Descriptors. Finally, we evaluate our approach on a patch matching task using data provided from the Photo Tourism project [25] and [16]. The dataset contains about 300K local patches extracted from <mark>interest points</mark> in multiple users' photos of scenes from different viewpoints.<br>",
    "Arabic": "نقطة اهتمام",
    "Chinese": "兴趣点",
    "French": "point d'intérêt",
    "Japanese": "関心点",
    "Russian": "точка интереса"
  },
  {
    "English": "interior point method",
    "context": "1: In the optimization literature, there is substantial work on tractability of the problem (6), including that of Ben-Tal et al. [6], who show that the dual of (4) often admits a standard form (such as a second-order cone problem) to which standard polynomial-time <mark>interior point methods</mark> can be applied.<br>2: The complexity of each iteration breaks down as follows. Step 1. This step computes the analytic center of a polyhedron and can be solved in O(n 3 ) operations using <mark>interior point methods</mark> for example. Step 2. This simply updates the polyhedral description. Stopping Criterion.<br>",
    "Arabic": "طريقة النقطة الداخلية",
    "Chinese": "内点法",
    "French": "méthode du point intérieur",
    "Japanese": "内点法",
    "Russian": "метод внутренней точки"
  },
  {
    "English": "intermediate layer",
    "context": "1: The learned exit architecture constitutes 0.25% of the parameters on each <mark>intermediate layer</mark> and increases 0.6% inference latency on average. However, the performance gains on the <mark>intermediate layer</mark>s clearly out-weights the increased latency.<br>",
    "Arabic": "الطبقة الوسيطة",
    "Chinese": "中间层",
    "French": "couche intermédiaire",
    "Japanese": "中間層",
    "Russian": "промежуточный слой"
  },
  {
    "English": "intermediate representation",
    "context": "1: a (k) i = AGG (k) h (k−1) j : j ∈ N (i) , h (k) i = COMBINE (k) h (k−1) i , a (k) i ,(1) \n where h ( k ) i ∈ R n×d k is the <mark>intermediate representation</mark> of node i at the k-th layer , N ( i ) denotes the neighbors of node i. AGG ( • ) is an aggregation function to collect embedding representations from neighbors , and COMBINE ( • ) combines neighbors ' representation and its representation at ( k − 1<br>2: We use the Huggingface (Wolf et al., 2019) implementation T5-base model. The difference between our T5 baselines results and the results in Qiu et al. (2022) due to their usage of different <mark>intermediate representation</mark> for the output in order to keep our evaluation consistent with other previous work.<br>",
    "Arabic": "التمثيل الوسيط",
    "Chinese": "中间表示",
    "French": "représentation intermédiaire",
    "Japanese": "中間表現",
    "Russian": "промежуточное представление"
  },
  {
    "English": "internal edge",
    "context": "1: We now detail how to construct the edges. For the <mark>internal edges</mark>, E in is defined as the edges connecting each pair of nodes within the same component if the spatial distance in terms of C α is below a cutoff distance c 1 .<br>",
    "Arabic": "الحافة الداخلية",
    "Chinese": "内部边",
    "French": "arête interne",
    "Japanese": "内部エッジ",
    "Russian": "внутренние рёбра"
  },
  {
    "English": "internal node",
    "context": "1: The vectors a and b represent the left and right node indexes for each <mark>internal node</mark>. The vector t contains the thresholds for each <mark>internal node</mark>, and d is a vector of indexes of the features used for splitting in <mark>internal node</mark>s.<br>2: For each set C i , create |C i | − 1 auxiliary x's as follows: Given set C i with |C i | = s i that corresponds to {h i1 , .., h isi }, create a balanced binary tree T i with each leaf corresponding to a h ij . Create an auxiliary example associated with each <mark>internal node</mark> in T i as follows : for each <mark>internal node</mark> in the tree , define the corresponding auxiliary sample x such that its label is +1 under all the classifiers in the leaves of the subtree rooted at its left child , and its label is −1 under all remaining classifiers in H. The total<br>",
    "Arabic": "عقدة داخلية",
    "Chinese": "内部节点",
    "French": "nœud interne",
    "Japanese": "内部ノード",
    "Russian": "внутренний узел"
  },
  {
    "English": "internal regret",
    "context": "1: The notion of <mark>internal regret</mark> is strictly stronger than the notion of external regret: any algorithm with small <mark>internal regret</mark> also has small external regret, but the converse does not hold (see Stoltz and Lugosi [47] for an example).<br>2: evaluate decision x t i . A regret minimizer is evaluated in terms of its cumulative regret. Two types of regret minimizers are commonly studied, depending on the adopted notion of regret, either external or <mark>internal regret</mark>.<br>",
    "Arabic": "الندم الداخلي",
    "Chinese": "内部遗憾",
    "French": "regret interne",
    "Japanese": "内部後悔",
    "Russian": "Внутренний регрет"
  },
  {
    "English": "internal representation",
    "context": "1: We train a linear classifier that uses the (frozen) <mark>internal representation</mark> (h t+1 , c t+1 ) to predict if action a t resulted in a collision (details in Apx. A.5). The classifier achieves 98% accuracy on held-out data. As comparison, random guessing on this 2-class problem would achieve 50%.<br>2: GAN networks as generative model, is firstly applied in learning allophonic distribution from raw acoustic data in Beguš (2020a,b) which also proposes a probing technique to interpret the <mark>internal representation</mark> of GAN networks.<br>",
    "Arabic": "التمثيل الداخلي",
    "Chinese": "内部表征",
    "French": "représentation interne",
    "Japanese": "内部表現",
    "Russian": "Внутреннее представление"
  },
  {
    "English": "internal state",
    "context": "1: Specifically it seeks to learn a function Collided t = f ((h t , c t )) where (h t , c t ) is the <mark>internal state</mark> at time t and Collided t is whether or not the previous action, a t−1 lead to a collision. Architecture.<br>2: Visualization of Predictions. For visualization the predictions of past vitiation, we found it easier to train a second decoder that predicts all locations the agent visited previously on a 2D top down map given the <mark>internal state</mark> (h t , c t ). This decoder shares the exact same architecture and training procedure as the occupancy grid decoder.<br>",
    "Arabic": "الحالة الداخلية",
    "Chinese": "内部状态",
    "French": "état interne",
    "Japanese": "内部状態",
    "Russian": "внутреннее состояние"
  },
  {
    "English": "interpolation",
    "context": "1: This dramatically improves the keypoint accuracy, but some errors remain as the regression and <mark>interpolation</mark> are only approximate. Both bundle and keypoint adjustments are based on geometric observations, namely keypoint locations and flow, but do not account for their respective uncertainties.<br>2: We prove that for a broad class of data distributions and model classes, overparametrization is necessary if one wants to interpolate the data smoothly. Namely we show that smooth <mark>interpolation</mark> requires d times more parameters than mere <mark>interpolation</mark>, where d is the ambient data dimension.<br>",
    "Arabic": "إقحام",
    "Chinese": "插值",
    "French": "interpolation",
    "Japanese": "補間",
    "Russian": "интерполяция"
  },
  {
    "English": "interpretability",
    "context": "1: Decision tree ensembles are popular models that have proven successful in various machine learning applications and competitions [Erdman andBates, 2016, Chen and. Besides their competitive performance, decision trees are appealing in practice because of their <mark>interpretability</mark>, robustness to outliers, and ease of tuning [Hastie et al., 2009].<br>2: Most notably, the White House Office of Science and Technology Policy (OSTP) has proposed the AI Bill of Rights [195], which outlines five principles, including safety, fairness, privacy, <mark>interpretability</mark>, and human-in-the-loop interventions.<br>",
    "Arabic": "قابلية التفسير",
    "Chinese": "可解释性",
    "French": "interprétabilité",
    "Japanese": "解釈可能性",
    "Russian": "интерпретируемость"
  },
  {
    "English": "interpretation function",
    "context": "1: Finally, we assume that well-formedness can be computed by a a binary <mark>interpretation function</mark> I : X → {0, 1} with I(x) = 1 iff x ∈ X.<br>",
    "Arabic": "دالة التفسير",
    "Chinese": "解释函数",
    "French": "fonction d'interprétation",
    "Japanese": "解釈関数",
    "Russian": "функция интерпретации"
  },
  {
    "English": "intersection-over-union",
    "context": "1: Specifically, anchors are assigned to ground-truth object boxes using an <mark>intersection-over-union</mark> (IoU) threshold of 0.5; and to background if their IoU is in [0, 0.4).<br>",
    "Arabic": "- التقاطع على الاتحاد",
    "Chinese": "交并比",
    "French": "\"intersection sur union\"",
    "Japanese": "IoU (交差結合比)",
    "Russian": "пересечение-над-объединением"
  },
  {
    "English": "interval estimate",
    "context": "1: In this regime, both point estimates, such as mean and median scores, and <mark>interval estimates</mark> of these quantities paint an incomplete picture of an algorithm's performance [24,Section 3]. Instead, we recommend the use of performance profiles [26], commonly used in benchmarking optimization software.<br>2: We first reaffirm the importance of reporting <mark>interval estimates</mark> to indicate the range within which an algorithm's aggregate performance is believed to lie.<br>",
    "Arabic": "تقدير الفاصل الزمني",
    "Chinese": "区间估计",
    "French": "estimation par intervalle",
    "Japanese": "区間推定",
    "Russian": "интервальная оценка"
  },
  {
    "English": "intractability",
    "context": "1: With this respect, our <mark>intractability</mark> results (for α < 1/2) come as computational obstructions to such forms of manipulation, when the external agent is not powerful enough.<br>",
    "Arabic": "الاستعصاء",
    "Chinese": "无解性",
    "French": "intractabilité",
    "Japanese": "難解性",
    "Russian": "неразрешимость"
  },
  {
    "English": "intrinsic",
    "context": "1: We first evaluate the accuracy of the refined 3D structure given known camera poses and <mark>intrinsics</mark>. Evaluation: We use the ETH3D benchmark [73], which  is composed of 13 indoor and outdoor scenes and provides images with millimeter-accurate camera poses and highlyaccurate ground truth dense reconstructions obtained with a laser scanner.<br>2: We follow the protocol introduced in [24], in which a sparse 3D model is triangulated for each scene using COLMAP [70] with fixed camera poses and <mark>intrinsics</mark>.<br>",
    "Arabic": "معاملات داخلية",
    "Chinese": "内参数",
    "French": "intrinsèques",
    "Japanese": "内部パラメータ",
    "Russian": "внутренние"
  },
  {
    "English": "intrinsic camera parameter",
    "context": "1: L J = i∈joints γ i w i ρ(Π K (R θ (J(β))) − J est,i ) L α = i∈(elbow,knees) \n exp ( θ i ) , ( 11 ) where J est , i are 2D pose keypoints estimated by a SoTA 2D-pose estimation method [ 11 ] , R θ transforms the joints along the kinematic tree according to the pose θ , Π K represents a 3D to 2D projection with <mark>intrinsic camera parameters</mark> and ρ represents a robust Geman-McClure error [<br>",
    "Arabic": "معامل الكاميرا الذاتي",
    "Chinese": "相机内参",
    "French": "paramètres intrinsèques de la caméra",
    "Japanese": "内部カメラパラメータ",
    "Russian": "внутренние параметры камеры"
  },
  {
    "English": "intrinsic dimension subspace",
    "context": "1: Explicitly, these bounds only apply to pre-trained methods trained with the <mark>intrinsic dimension subspace</mark> method; research has yet to show that standard SGD optimizes in this low dimensional space (although experimentally,  this seems to be confirmed). We leave the theoretical contribution of showing SGD optimizes in this space, possibly resembling intrinsic subspace, for future work.<br>",
    "Arabic": "البُعد الجوهري للفضاء الفرعي",
    "Chinese": "内在维度子空间",
    "French": "sous-espace de dimension intrinsèque",
    "Japanese": "本質次元部分空間",
    "Russian": "внутреннее подпространство размерности"
  },
  {
    "English": "intrinsic dimensionality",
    "context": "1: We do not explicitly optimize for <mark>intrinsic dimensionality</mark>, specifically during pre-training (the language model does not have access to downstream datasets! ), but none-the-less the intrinsic dimension of these downstream tasks continues to decrease.<br>2: We have shown strong empirical evidence connecting pre-training, fine-tuning, and <mark>intrinsic dimensionality</mark>. However, we have yet to argue the connection between <mark>intrinsic dimensionality</mark> and generalization. Given that we have seen pre-training minimize intrinsic dimension, we hypothesize that generalization improves as the intrinsic dimension decreases.<br>",
    "Arabic": "البعد الجوهري",
    "Chinese": "内在维度",
    "French": "dimensionnalité intrinsèque",
    "Japanese": "固有次元数",
    "Russian": "внутренняя размерность"
  },
  {
    "English": "intrinsic evaluation",
    "context": "1: 5.3 explores the different types of contextual information captured in biLMs and uses two <mark>intrinsic evaluations</mark> to show that syntactic information is better represented at lower layers while semantic information is captured a higher layers, consistent with MT encoders. It also shows that our biLM consistently provides richer representations then CoVe.<br>",
    "Arabic": "تقييم داخلي",
    "Chinese": "内在评估",
    "French": "évaluation intrinsèque",
    "Japanese": "内部評価",
    "Russian": "внутренняя оценка"
  },
  {
    "English": "intrinsic image",
    "context": "1: pioneering work as Brooks' ACRONYM [4], Hanson and Riseman's VISIONS [9], Ohta and Kanade's outdoor scene understanding system [19], Barrow and Tenenbaum's <mark>intrinsic images</mark> [2], etc.<br>2: We propose an additional processing step to recover high frequency shape information by adapting the <mark>intrinsic images</mark> algorithm of Barron and Malik [5,4], SIRFS, which exploits statistical regularities between shapes, reflectance and illumination Formally, SIRFS is formulated as the following optimization problem: \n<br>",
    "Arabic": "الصورة الجوهرية",
    "Chinese": "固有图像",
    "French": "image intrinsèque",
    "Japanese": "固有画像",
    "Russian": "врожденное изображение"
  },
  {
    "English": "intrinsic parameter",
    "context": "1: Given N images { I i } observing a scene , we are interested in accurately estimating its 3D structure , represented as sparse points { P j ∈ R 3 } , <mark>intrinsic parameters</mark> { C i } of the cameras , and the poses { ( R i , t i ) ∈ SE ( 3 ) } of the images ,<br>2: Recall that when the camera <mark>intrinsic parameters</mark> are held constant, the DIAC satisifes the infinite homography rela- \n tions ω * = H i ∞ ω * H i ∞ , i = 1, • • • , m. \n<br>",
    "Arabic": "المعلمة الجوهرية",
    "Chinese": "内参",
    "French": "paramètres intrinsèques",
    "Japanese": "固有パラメータ",
    "Russian": "внутренний параметр"
  },
  {
    "English": "inverse document frequency",
    "context": "1: He and Ounis [12] studied the usefulness of several alternative predictors, including one based on the standard deviation of the <mark>inverse document frequency</mark> (idf) of the composing query terms. This predictor is based on the assumption that the terms of a poorly-performing query tend to have similar idf values. Plachouras et al.<br>2: When the system receives a class of news articles as an event, it calculates sum of term frequency -<mark>inverse document frequency</mark> (tf•idf) as shown below: \n tf • idf (w, e) = N −1 i=0 tf • idf (w, i) ( 1 ) \n<br>",
    "Arabic": "معامل تردد المستند العكسي",
    "Chinese": "逆文档频率",
    "French": "fréquence inverse du document",
    "Japanese": "逆文書頻度",
    "Russian": "обратная документная частота"
  },
  {
    "English": "inverse dynamic model",
    "context": "1: VPT [10] is a concurrent work that learns an <mark>inverse dynamics model</mark> from human contractors to pseudo-label YouTube videos for behavior cloning. VPT is complementary to our approach, and can be finetuned to solve language-conditioned open-ended tasks with our learned reward model.<br>",
    "Arabic": "نموذج ديناميكي عكسي",
    "Chinese": "逆动力学模型",
    "French": "modèle de dynamique inverse",
    "Japanese": "逆動力学モデル",
    "Russian": "обратная динамическая модель"
  },
  {
    "English": "inverse problem",
    "context": "1: This ameliorates the overfitting problem and effectively reduces the space of possible active source locations by choosing a small relevant subset of location priors that optimizes the Bayesian evidence (hence ARD). With this 'learned' prior in place, a once ill-posed <mark>inverse problem</mark> is no longer untenable, with the posterior mean providing a good estimate of source activity.<br>",
    "Arabic": "مشكلة عكسية",
    "Chinese": "逆问题",
    "French": "problème inverse",
    "Japanese": "逆問題",
    "Russian": "обратная задача"
  },
  {
    "English": "inverse reinforcement learning",
    "context": "1: Our long-term aim is to develop an AI system that can learn about a person's intent and goals by continuously observing their behavior. In this work, we progress towards this aim by proposing an online <mark>Inverse Reinforcement Learning</mark> (IRL) technique to learn a decision-theoretic human activity model from video captured by a wearable camera.<br>2: The probabilistic approach taken by PUR-IRL is similar to a previously described Bayesian nonparametric method known as Dirichlet Process Mixture <mark>Inverse Reinforcement Learning</mark> (DPM-BIRL) (Choi and Kim 2012). Both methodologies share the notion of applying a prior on each of the reward functionsr t k to encode preference and a likelihood to measure the compatibility of the reward function with the data , with PUR-IRL utilizing the Pitman-Yor Process ( PYP ) and an additional discount parameter d ∈ [ 0 , 1 ) , where d = 0 reduces the model<br>",
    "Arabic": "التعلم العكسي للتعزيز",
    "Chinese": "逆向强化学习",
    "French": "apprentissage par renforcement inverse",
    "Japanese": "逆強化学習",
    "Russian": "Обратное обучение с подкреплением"
  },
  {
    "English": "inverse rendering",
    "context": "1: Fig. 14: Ablation on smoothness regularization term in the flowfield for <mark>inverse rendering</mark>. Our method uses the same spherical initialization for all the shapes. The network is composed of 4 layers, with 512 neurons in each layer. The learning-rate is 2 × 10 −6 , the weight-decay factor is 0.1 and the time-delta ∆t is 10 −4 .<br>2: We propose an <mark>inverse rendering</mark> method which uses explicit differentiable renderers for parametrically defined implicit surfaces. The proposed method is not as sensitive to initialization as explicit methods [18,42] are, does not require an object mask like implicit methods [43,71] do, and maintains the ability to vary topology during optimization.<br>",
    "Arabic": "عكس التقديم",
    "Chinese": "逆向渲染",
    "French": "rendu inverse",
    "Japanese": "逆レンダリング",
    "Russian": "обратный рендеринг"
  },
  {
    "English": "inverse role",
    "context": "1: Concepts and Ontologies. Let N C , N R , and N I be countably infinite sets of concept names, role names, and individual names, respectively. A role R takes the form r or r − where r is a role name and r − is called an <mark>inverse role</mark>.<br>2: Let C, R, and K be countably infinite sets of concept names, role names, and constants. A role R is a role name r ∈ R or an <mark>inverse role</mark> r − with r a role name.<br>",
    "Arabic": "الدور المعكوس",
    "Chinese": "逆角色",
    "French": "rôle inverse",
    "Japanese": "逆の役割",
    "Russian": "обратная роль"
  },
  {
    "English": "inverse square root learning rate schedule",
    "context": "1: All bilingual models use a transformer architecture (Vaswani et al., 2017) with 6 encoder layers and 6 decoder layers, 8 attention heads, 512dimensional embeddings, 0.3 dropout, an effective batch size of 130k tokens, and are trained with an <mark>inverse square root learning rate schedule</mark> with warmup.<br>",
    "Arabic": "جدول معدل التعلم للجذر التربيعي العكسي",
    "Chinese": "逆平方根学习率调度",
    "French": "calendrier de taux d'apprentissage inverse de la racine carrée",
    "Japanese": "逆平方根学習率スケジュール",
    "Russian": "График скорости обучения, обратный квадратному корню"
  },
  {
    "English": "inverse square root learning rate scheduler",
    "context": "1: 5 We evaluate using accuracy (ACC) and character error rate of incorrect prediction (CER i ). Optimization. We use Adam (Kingma and Ba, 2014) with a learning rate of 0.001 and an <mark>inverse square root learning rate scheduler</mark> (Vaswani et al., 2017) with 4k steps during the warm-up.<br>",
    "Arabic": "جدولة معدل التعلم بالجذر التربيعي المعكوس",
    "Chinese": "倒数平方根学习率调度器",
    "French": "planificateur de taux d'apprentissage de racine carrée inverse",
    "Japanese": "逆平方根学習率スケジューラ",
    "Russian": "планировщик скорости обучения, обратный квадратному корню"
  },
  {
    "English": "inverse transform sampling",
    "context": "1: We sample a second set of N f locations from this distribution using <mark>inverse transform sampling</mark>, evaluate our \"fine\" network at the union of the first and second set of samples, and compute the final rendered color of the raŷ C f (r) using Eqn. 3 but using all N c +N f samples.<br>2: )) produced by the \"coarse\" model are then taken as a piecewise constant PDF describing the distribution of visible scene content, and 128 new t values are drawn from that PDF using <mark>inverse transform sampling</mark> to produce t f .<br>",
    "Arabic": "عينة التحويل العكسي",
    "Chinese": "逆变换采样",
    "French": "échantillonnage par transformation inverse",
    "Japanese": "逆変換サンプリング",
    "Russian": "выборка обратного преобразования"
  },
  {
    "English": "invert index",
    "context": "1: In recent years, there are some attempts to incorporate the power of neural networks into <mark>inverted index</mark>. The Standalone Neural Ranking Model (SNRM) [57] learns high-dimensional sparse representations for queries and documents, which enables the construction of <mark>inverted index</mark> for efficient document retrieval.<br>2: With the <mark>inverted index</mark> built from feature selection, we reference a feature name by its index in generating examples and onwards. The original feature name can be of any data type with arbitrary length; after indexing the algorithm now efficiently deals with integer index consistently. Several pre-computations are carried out in this step.<br>",
    "Arabic": "الفهرس المعكوس",
    "Chinese": "倒排索引",
    "French": "index inversé",
    "Japanese": "逆インデックス",
    "Russian": "инвертированный индекс"
  },
  {
    "English": "invert list",
    "context": "1: One may ask, \"Do we have a symmetric pruning for Sample-Gene Search?\" We can apply the similar optimization technique for sample-gene. That is, a sample sj is merged into current combination of samples S as long as the <mark>inverted list</mark> of S is a subset of that of s j .<br>2: Thus, a three-term query {a, b, c} could be processed by scanning the <mark>inverted list</mark> Ia for term a and a cached list for the intersection of I b and Ic.<br>",
    "Arabic": "قائمة معكوسة",
    "Chinese": "倒排列表",
    "French": "liste inversée",
    "Japanese": "逆リスト",
    "Russian": "обратный список"
  },
  {
    "English": "invertible map",
    "context": "1: Parametric methods consist of a normalizing flow in the Euclidean space R n , pushed-forward onto the manifold through an <mark>invertible map</mark> ψ : R n → M. However, to globally represent the manifold, ψ needs to be a homeomorphism implying that M and R n are topologically equivalent, limiting the scope of that Frequency k = 10 \n<br>",
    "Arabic": "خريطة قابلة للعكس",
    "Chinese": "可逆映射",
    "French": "application inversible",
    "Japanese": "可逆写像",
    "Russian": "обратимое отображение"
  },
  {
    "English": "invertible matrix",
    "context": "1: A matrix A is said to be \"similar\" to a matrix B if there exists an <mark>invertible matrix</mark> M such that A = MBM −1 (see Pearson (1983)). The term \"conjugate matrices\" is also often used. 3.<br>2: Here O n denotes a n × n matrix with all entries being zeros and I n denotes the n × n identity matrix. et al. (2021b) further constructed an <mark>invertible matrix</mark> V such that A, B and C are simultaneously congruent to arrow matrices via the change of variables associated to V , i.e.,Ã<br>",
    "Arabic": "مصفوفة قابلة للانعكاس",
    "Chinese": "可逆矩阵",
    "French": "matrice inversible",
    "Japanese": "可逆行列",
    "Russian": "обратимая матрица"
  },
  {
    "English": "iobj",
    "context": "1: For relation phrases, we expand on advmod, mod, aux, auxpass, cop, prt edges. We also include dobj and <mark>iobj</mark> in the case that they are not in an argument. After identifying the words in arg/relation we choose their order as in the original sentence.<br>",
    "Arabic": "منفعل فاعل غير مباشر",
    "Chinese": "间接宾语",
    "French": "objet indirect",
    "Japanese": "間接目的語",
    "Russian": "iobj - косвенное дополнение"
  },
  {
    "English": "iso-surface extraction",
    "context": "1: surface representation; they are more controllable, easier to manipulate, and are more compact, attaining higher visual quality using fewer primitives; see Figure 1. For visualization purposes, the generated voxels, point clouds, and implicits are typically converted into meshes in post-processing, e.g., via <mark>iso-surface extraction</mark> by Marching Cubes [27].<br>",
    "Arabic": "استخراج سطح متساوي المستوى",
    "Chinese": "等值面提取",
    "French": "extraction d'iso-surface",
    "Japanese": "等値面抽出",
    "Russian": "извлечение изо-поверхностей"
  },
  {
    "English": "isotropic Gaussians",
    "context": "1: Because this mip-NeRF variant roughly matches the accuracy of NeRF, the only substantial benefit it appears to provide is removing the need to tune the L parameter in positional encoding. This result provides some insight into why NeRF works so well on forward-facing scenes : in NDC space there is little difference between NeRF 's `` incorrect '' aliased approach of casting rays and tuning the L hyperparameter ( which as discussed in Section B , is approximately equivalent to using IPE features with <mark>isotropic Gaussians</mark> ) and the more `` correct '' anti-aliased<br>",
    "Arabic": "توزيعات جاوسية متجانسة",
    "Chinese": "各向同性高斯分布",
    "French": "gaussiennes isotropes",
    "Japanese": "等方性ガウス分布",
    "Russian": "изотропные гауссовские распределения"
  },
  {
    "English": "itemset",
    "context": "1: Let D = {t 1 , t 2 , ..., t n } be a database of n transactions where each transaction is a subset of I. An <mark>itemset</mark> is a subset of items.<br>2: The support of an <mark>itemset</mark> S, denoted by sup(X), is the percentage of transactions in the database D that contain S. An <mark>itemset</mark> is called frequent if its support is greater than or equal to a user specified threshold value.<br>",
    "Arabic": "مجموعة عناصر",
    "Chinese": "项集",
    "French": "ensemble d'éléments",
    "Japanese": "アイテムセット",
    "Russian": "набор элементов"
  },
  {
    "English": "iterate",
    "context": "1: \"Optimistic\" regret minimizing variants exist that assign a higher weight to recent iterations, but this extra weight is temporary and typically only applies to a short window of recent iterations; for example, counting the most recent <mark>iterate</mark> twice (Syrgkanis et al. 2015).<br>2: where x(t) is a scaled version of the <mark>iterate</mark> that appears in their discrete update formula (Eq. 10 in [47]) and θ is a model trained to predict the normalized noise vector, i.e., \n<br>",
    "Arabic": "التكرار",
    "Chinese": "迭代",
    "French": "itérer",
    "Japanese": "反復",
    "Russian": "итерация"
  },
  {
    "English": "iterate conditional mode",
    "context": "1: This significantly reduces the cost of function evaluations, but the optimization is still a computationally challenging problem. For this work, we have implemented a variant of the <mark>iterated conditional modes</mark> (ICM) algorithm (Besag, 1986), alternately optimizing the photoconsistency and texture priors.<br>",
    "Arabic": "تكرار الوضع الشرطي",
    "Chinese": "迭代条件模式",
    "French": "modes conditionnels itérés",
    "Japanese": "繰り返し条件付きモード (ICM)",
    "Russian": "итерированный условный режим"
  },
  {
    "English": "iteration",
    "context": "1: The regret of action a in infoset I on <mark>iteration</mark> t of this new sequence is R t (I, a). From Lemma 4 we know that R t (I, a) ≤ 2∆ |A| √ T for player i for action a in infoset I.<br>2: Specifically, in <mark>iteration</mark> i we choose a new user wi in G − H to target; we then pick a minimal subset X ⊆ X that has not been used for any wj for j < i, and where the degrees of nodes in X are less than their randomly selected target degrees.<br>",
    "Arabic": "تكرار",
    "Chinese": "迭代",
    "French": "itération",
    "Japanese": "反復",
    "Russian": "итерация"
  },
  {
    "English": "iteration complexity",
    "context": "1: We start from the general bound. We expect this lower bound to show given arbitrary setting (functions, oracles and graph), the smallest <mark>iteration complexity</mark> we could obtain from A B , i.e.<br>2: The proof for the theorem is deferred to Appendix A. The global convergence rate has also been established where the <mark>iteration complexity</mark> is O ǫ −2 g for grad q(x) ≤ ǫ g . We refer interested readers to Theorem 3.9 of Boumal et al. (2019).<br>",
    "Arabic": "تعقيد التكرار",
    "Chinese": "迭代复杂度",
    "French": "complexité des itérations",
    "Japanese": "反復の複雑性",
    "Russian": "сложность итерации"
  },
  {
    "English": "iteration counter",
    "context": "1: Algorithm 2 Policy-Class Q-learning (PCQL) Input: Batch B = {(s t , a t , r t , s t )} T t=1 , γ, Θ, scalars α sa t . 1: for (s, a, r, s ) ∈ B, t is <mark>iteration counter</mark> do \n<br>",
    "Arabic": "عداد التكرار",
    "Chinese": "迭代计数器",
    "French": "compteur d'itération",
    "Japanese": "繰り返し回数",
    "Russian": "счётчик итераций"
  },
  {
    "English": "iterative algorithm",
    "context": "1: There are several ways in which the proposed approach can be extended. First, using an adaptive or non-constant kernel bandwidth should lead to higher accuracy. It is also interesting to explore tighter generalization error bounds by directly analyzing the solutions of the marginal regression <mark>iterative algorithm</mark>.<br>2: In other words, we can save time by not computing precise utility estimates for hypotheses which are unlikely to be chosen in the end. We propose an <mark>iterative algorithm</mark> for MBR where the hypothesis set is gradually shrunk while the pseudo-reference list grows. The procedure is shown in Algorithm 1.<br>",
    "Arabic": "خوارزمية تكرارية",
    "Chinese": "迭代算法",
    "French": "algorithme itératif",
    "Japanese": "反復アルゴリズム",
    "Russian": "итеративный алгоритм"
  },
  {
    "English": "iterative deepening",
    "context": "1: In contrast, without <mark>iterative deepening</mark>, the search can go off in the wrong direction and waste lots of effort before stumbling on the best line of play. Our iterative value solution works for similar reasons. A position may require considerable effort to formally determine its proven value.<br>2: Furthermore, for hierarchical FSCs we specify bounds on the number of FSCs and stack levels. An <mark>iterative deepening</mark> approach could be implemented to automatically derive these bounds. Another issue is the specification of representative subproblems to generate hierarchical FSCs in an incremental fashion.<br>",
    "Arabic": "التعميق التكراري",
    "Chinese": "迭代加深",
    "French": "approfondissement itératif",
    "Japanese": "反復深化探索",
    "Russian": "итеративное углубление"
  },
  {
    "English": "iterative optimization",
    "context": "1: We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat <mark>iterative optimization</mark>based attacks, we find defenses relying on this effect can be circumvented.<br>",
    "Arabic": "تحسين تكراري",
    "Chinese": "迭代优化",
    "French": "optimisation itérative",
    "Japanese": "反復最適化",
    "Russian": "итеративная оптимизация"
  },
  {
    "English": "iterative optimization algorithm",
    "context": "1: , 2018 ; Dong et al. , 2018 ; Xie et al. , 2019 ) , which explicitly unroll/truncate <mark>iterative optimization algorithms</mark> into learnable deep architectures . In this way, the penalty parameters (and the denoiser prior) are treated as trainable parameters, meanwhile the number of iterations has to be fixed to enable end-to-end training.<br>",
    "Arabic": "خوارزمية التحسين التكرارية",
    "Chinese": "迭代优化算法",
    "French": "algorithme d'optimisation itératif",
    "Japanese": "反復最適化アルゴリズム",
    "Russian": "итерационный алгоритм оптимизации"
  },
  {
    "English": "iterative training algorithm",
    "context": "1: • We illustrate how prior work (Saunders et al., 2022;Ye et al., 2023) can be ineffective in training smaller models to self-improve their performance on math and reasoning tasks. • We propose TRIPOST, an <mark>iterative training algorithm</mark> that trains a smaller language model to learn to self-improve.<br>",
    "Arabic": "خوارزمية التدريب التكراري",
    "Chinese": "迭代训练算法",
    "French": "algorithme d'entraînement itératif",
    "Japanese": "反復トレーニングアルゴリズム",
    "Russian": "итеративный алгоритм обучения"
  },
  {
    "English": "iteratively reweighte least square",
    "context": "1: is computed with <mark>iteratively reweighted least squares</mark> [37]. Simultaneously storing all high-dimensional feature patches incurs high memory requirements during BA. We dramatically increase its efficiency by exhaustively precomputing patches of feature distances and directly interpolate an approximate costĒ ij = F i − f j γ p ij .<br>",
    "Arabic": "المربعات الصغرى المعاد ترجيحها تكراريًا",
    "Chinese": "迭代重新加权最小二乘法",
    "French": "moindres carrés repondérés itérativement",
    "Japanese": "反復重み付き最小二乗法",
    "Russian": "итеративно перевзвешенный метод наименьших квадратов"
  },
  {
    "English": "jaccard similarity",
    "context": "1: In this paper, we introduce the Odd Sketch, a compact binary sketch for estimating the <mark>Jaccard similarity</mark> of two sets. This binary sketch is similar to a Bloom filter with one hash function, constructed on the original minhashes with the \"odd\" feature that the usual disjunction is replaced by an exclusive-or operation.<br>2: Additionally, some of these metrics have been included in recent factuality evaluation platforms in summarization (Pagnoni et al., 2021). We explore the extent to which existing similarity methods detect  information errors as outlined in our annotation scheme. We consider: (1) <mark>Jaccard similarity</mark>; \n<br>",
    "Arabic": "التشابه جاكارد",
    "Chinese": "杰卡德相似性",
    "French": "similarité de Jaccard",
    "Japanese": "ジャッカード類似度",
    "Russian": "коэффициент Жаккара"
  },
  {
    "English": "jaccard similarity coefficient",
    "context": "1: , D − 1}, a challenge is how to quickly compute their <mark>Jaccard similarity coefficient</mark> J, a normalized measure of set similarity: \n One can view large datasets of Web documents as collections of sets where sets and set elements correspond to documents and document words/shingles, respectively.<br>",
    "Arabic": "معامل التشابه جاكارد",
    "Chinese": "杰卡德相似系数",
    "French": "coefficient de similarité de Jaccard",
    "Japanese": "ジャカード類似係数",
    "Russian": "коэффициент сходства Жаккара"
  },
  {
    "English": "jacobian matrix",
    "context": "1: 5) involves residuals and <mark>Jacobian matrices</mark> of dimension D. Unlike the keypoint adjustment, which can optimize tracks independently, all bundle parameters are updated simultaneously and the memory requirements are thus prohibitive.<br>",
    "Arabic": "مصفوفة يعقوبية",
    "Chinese": "雅可比矩阵",
    "French": "matrice jacobienne",
    "Japanese": "ヤコビアン行列",
    "Russian": "матрица Якоби"
  },
  {
    "English": "joint density",
    "context": "1: In the case of path-specific fairness, we require the joint distributions of the counterfactual utilities to have a <mark>joint density</mark>.<br>",
    "Arabic": "الكثافة المشتركة",
    "Chinese": "联合密度",
    "French": "densité jointe",
    "Japanese": "同時密度",
    "Russian": "совместная плотность"
  },
  {
    "English": "joint distribution",
    "context": "1: , X k = x k ) = Pr X 1 = x π(1) , . . . , X k = x π(k) . Figure 2 depicts an example distribution Pr with 20 random variables and a decomposition into 5 subsets of width 4. The <mark>joint distribution</mark> is invariant under permutations of the 5 sequences.<br>2: Each sample of the resulting particle filter contains the discrete and continuous states described in the previous section, and a <mark>joint distribution</mark> over the goals and trip segments. These additional distributions are updated using exact inference. To summarize, at each time step, the filter is updated as follows (see (Bui 2003)): 1.<br>",
    "Arabic": "التوزيع المشترك",
    "Chinese": "联合分布",
    "French": "distribution conjointe",
    "Japanese": "同時分布",
    "Russian": "совместное распределение"
  },
  {
    "English": "joint embedding space",
    "context": "1: It aims to learn video-semantic representation in a <mark>joint embedding space</mark>. Recent works (Liu et al., 2019; focus on learning video's multi-modal representation to match with text. In this work, we borrow this idea to match video and textual representations.<br>",
    "Arabic": "فضاء التضمين المشترك",
    "Chinese": "联合嵌入空间",
    "French": "espace d'intégration conjoint",
    "Japanese": "共同埋め込み空間",
    "Russian": "совместное embedding-пространство"
  },
  {
    "English": "joint encoding",
    "context": "1: Joint encoding aims to capture tighter interaction between the input modalities compared to separate encoding, but is generally more computationally expensive, and can only operate on multi-modal input. We study recent models on both ends: ViLT  for <mark>joint encoding</mark> and CLIP (Radford et al., 2021) for separate encoding.<br>",
    "Arabic": "ترميز مشترك",
    "Chinese": "联合编码",
    "French": "encodage conjoint",
    "Japanese": "共同符号化",
    "Russian": "совместное кодирование"
  },
  {
    "English": "joint entropy",
    "context": "1: The mutual information between SNP-pair (X i X j ) and phenotype Y is I (Y ; X i X j ) = H(Y ) + H(X i X j ) − H(X i X j Y ), in which the <mark>joint entropy</mark> −H( \n<br>2: The first line is by definition. The second line holds from the fact that <mark>joint entropy</mark> is maximized by independent random variables (induced by the marginals g(f (v), j)). That is, if this independence relationship does not hold, then we could construct a strictly better candidate solution.<br>",
    "Arabic": "الانتروبيا المشتركة",
    "Chinese": "联合熵",
    "French": "entropie conjointe",
    "Japanese": "結合エントロピー",
    "Russian": "совместная энтропия"
  },
  {
    "English": "joint inference",
    "context": "1: M and W are a set of semantic role-values representing the agent as a man or a woman, respectively. Note that the constraints in (2) involve all the test instances. Therefore, it requires a <mark>joint inference</mark> over the entire test corpus.<br>2: Section 3 gives a formal definition of our adaptation framework. Section 4 describes the transformation operators that we applied for this task. Section 5 presents our <mark>joint inference</mark> approach. Section 6 describes our semantic role labeling system and our experimental results are in Section 7. Section 8 describes the related works for domain adaptation.<br>",
    "Arabic": "استنتاج مشترك",
    "Chinese": "联合推理",
    "French": "inférence conjointe",
    "Japanese": "結合推論",
    "Russian": "совместный вывод"
  },
  {
    "English": "joint learning",
    "context": "1: Experiments (Section 4.4) show that conditioning on both the state and the task identity results in noticeable performance improvements, suggesting that the variance reduction provided by this objective is important for efficient <mark>joint learning</mark> of modular policies. The complete procedure for computing a single gradient step is given in Algorithm 1.<br>2: If so, these existing weak detectors are also used to form a detector for the new category and only a reduced number of new weak detectors have to be learnt using the <mark>joint learning</mark> procedure. Note that joint and incremental training reduces to standard Boosting if there is only one category. Weak detectors: are formed from pairs of fragments.<br>",
    "Arabic": "التعلم المشترك",
    "Chinese": "联合学习",
    "French": "apprentissage conjoint",
    "Japanese": "共同学習",
    "Russian": "совместное обучение"
  },
  {
    "English": "joint learning algorithm",
    "context": "1: Li et al. (2015) propose a novel <mark>joint learning algorithm</mark> that allows the feedbacks to be propagated from the classifiers for latter labels to the classifier for the current label. Most methods, however, can only be used to capture the first or second order label correlations or are computationally intractable in considering high-order label correlations.<br>",
    "Arabic": "خوارزمية التعلم المشترك",
    "Chinese": "联合学习算法",
    "French": "algorithme d'apprentissage conjoint",
    "Japanese": "共同学習アルゴリズム",
    "Russian": "алгоритм совместного обучения"
  },
  {
    "English": "joint likelihood",
    "context": "1: We can factor the <mark>joint likelihood</mark> as: P(x, y) = P(y | x)P(x). Our modelling of P(y | x) resembles the probabilistic modelling of the \"star graph\" of Felzenszwalb and Huttenlocher (2005).<br>2: This difference is caused by the discrepancy between the marginal and joint predictive distributions: the CLML evaluates the <mark>joint likelihood</mark> of D ≥m , while the test likelihood only depends on the marginal predictive distribution on each test datapoint. The difference between the marginal and joint predictive likelihoods is discussed in detail in Osband et al. and Wen et al.<br>",
    "Arabic": "الاحتمال المشترك",
    "Chinese": "联合似然",
    "French": "vraisemblance jointe",
    "Japanese": "結合尤度",
    "Russian": "совместная правдоподобность"
  },
  {
    "English": "joint model",
    "context": "1: We also compare with the multi parse system of (Toutanova et al., 2008) which uses a global <mark>joint model</mark> using multiple parse trees. In (Surdeanu et al., 2007), the authors experimented with several combination strategies.<br>2: x ′ mem = GRU(x mem , [s ;t]) if ≠ ; x mem if = . (5 \n ) \n wheret is a template embedding given byt = t when using the independent policy model given in Equation 2 and given by Equation 3 when using the <mark>joint model</mark>.<br>",
    "Arabic": "النموذج المشترك",
    "Chinese": "联合模型",
    "French": "modèle conjoint",
    "Japanese": "結合モデル",
    "Russian": "объединенная модель"
  },
  {
    "English": "joint policy",
    "context": "1: The simplest algorithms assume all agents are independent, learning to cooperate implicitly via an appropriate reward function (Bagnell & Ng, 2006). More advanced algorithms explicitly share information about states, values, or proposed actions (Boutilier, 1999), but still avoid modeling the optimal <mark>joint policy</mark>. Our work is similar to Guestrin et al.<br>2: We performed experiments on one toy domain to demonstrate why a <mark>joint policy</mark> is important, and two benchmark decentralized RL domains.<br>",
    "Arabic": "السياسة المشتركة",
    "Chinese": "联合策略",
    "French": "politique conjointe",
    "Japanese": "共同方針",
    "Russian": "совместная политика"
  },
  {
    "English": "joint probability",
    "context": "1: These results could be applied to non-pairwise MRFs by first projecting the marginal vector onto the marginal polytope of a pairwise MRF. More generally, suppose we include additional variables corresponding to the <mark>joint probability</mark> of a cluster of variables. We need to add constraints enforcing that all variables in common between two clusters have the same marginals.<br>2: In what follows we shall model the log <mark>joint probability</mark> of the cosmological parameters and the observations via a GP 1 . This is keeping in line with Adams et al. [2008] who use a similar prior for GP density sampling and similar smoothness assumptions in Srinivas et al. [2010].<br>",
    "Arabic": "احتمال مشترك",
    "Chinese": "联合概率",
    "French": "probabilité conjointe",
    "Japanese": "共同確率",
    "Russian": "совместная вероятность"
  },
  {
    "English": "joint probability distribution",
    "context": "1: Computing the labeling Finally, given a <mark>joint probability distribution</mark> defined by the Markov random field, our goal is to compute the joint test pattern labeling that has maximum probability. (Since we are only interested in computing the maximum probability assignment, we can ignore the normalization constant above.)<br>2: assigned to an argument type a in c. The USP MLN defines a <mark>joint probability distribution</mark> over Q and L by modeling the distributions over forms and arguments given the cluster or argument type .<br>",
    "Arabic": "توزيع الاحتمال المشترك",
    "Chinese": "联合概率分布",
    "French": "distribution de probabilité conjointe",
    "Japanese": "同時確率分布",
    "Russian": "совместное распределение вероятностей"
  },
  {
    "English": "joint probability matrix",
    "context": "1: C 3,x,1 := (P(X t+2 = i, X t+1 = x, X t = j)) M i,j=1(8) \n which are the marginal probability vector of sequence singletons, and one slice of the <mark>joint probability matrix</mark> of sequence triples (i.e.<br>2: Let D be the |C| × |T| matrix whose (i, j)-th entry is given by − log P (j|i), and let P be the <mark>joint probability matrix</mark>, then we can write \n L2 = P , D = i j P (i, j)D(i, j). (7) \n<br>",
    "Arabic": "مصفوفة الاحتمال المشترك",
    "Chinese": "联合概率矩阵",
    "French": "matrice de probabilité jointe",
    "Japanese": "共同確率行列",
    "Russian": "матрица совместной вероятности"
  },
  {
    "English": "joint probability table",
    "context": "1: Conditional and contextual independence are such powerful concepts because they are statistical properties of the distribution, regardless of the representation used. Partial exchangeability is such a statistical property that is independent of any representation, be it a <mark>joint probability table</mark>, a Bayesian network, or a statistical relational model.<br>",
    "Arabic": "جدول الاحتمال المشترك",
    "Chinese": "联合概率表",
    "French": "Table de probabilités conjointes",
    "Japanese": "同時確率テーブル",
    "Russian": "совместная таблица вероятностей"
  },
  {
    "English": "joint semantic space",
    "context": "1: Similarly to dual-stream vision-language models, W2W encodes the textual input with a pre-trained language model (Liu et al., 2019), and encodes image input with convolutional backbone (He et al., 2016) with 2D positional encoding added. The text and image representations are linearly projected onto a <mark>joint semantic space</mark> and concatenated.<br>",
    "Arabic": "المساحة الدلالية المشتركة",
    "Chinese": "联合语义空间",
    "French": "espace sémantique conjoint",
    "Japanese": "共有意味空間",
    "Russian": "совместное семантическое пространство"
  },
  {
    "English": "junction tree",
    "context": "1: Given a graph, we first generate the minimally sparse and maximally dense equivalents. We treat the minimally sparse graph as a generic CRF and generate a <mark>junction tree</mark>. For each clique of the <mark>junction tree</mark>, we list its state space using the subgraph induced by the clique on the maximally dense graph.<br>2: It is easy to see that constructing this LP in the standard way will result in the exact MAP since these are the cliques in the <mark>junction tree</mark> of the model. We can now take the dual of the above MAP LP.<br>",
    "Arabic": "شجرة الوصل",
    "Chinese": "连接树",
    "French": "arbre de jonction",
    "Japanese": "接続木",
    "Russian": "дерево соединений"
  },
  {
    "English": "junction tree algorithm",
    "context": "1: The MAP problems were solved using the exact <mark>junction tree algorithm</mark> (JCT, left and right), or approximate belief propagation (BP, middle). In all cases, when coupling is very low, α close to 0 is optimal. This also holds for BP when coupling is high.<br>2: Also it has a large treewidth-the 50 animal classes are fully connected with exclusion edges, a complexity of 2 50 for a naive <mark>junction tree algorithm</mark>. But our inference algorithm runs with negligible cost. This again underscores the effectiveness of our new inference algorithm in exploiting both dynamic programming and small state space.<br>",
    "Arabic": "خوارزمية شجرة الوصلات",
    "Chinese": "联结树算法 (junction tree algorithm)",
    "French": "algorithme d'arbre de jonction",
    "Japanese": "結合木アルゴリズム",
    "Russian": "Алгоритм дерева соединений"
  },
  {
    "English": "k near neighbor",
    "context": "1: It was formed from training data by means of <mark>k nearest neighbors</mark> colored noise injection [21].<br>2: However, it would be natural to wonder whether it would be effective as a defense, so we study its robustness; our results confirm that it is not adequate as a defense. The method used to compute the LID relies on finding the <mark>k nearest neighbors</mark>, a nondifferentiable operation, rendering gradient descent based methods ineffective.<br>",
    "Arabic": "k أقرب جار",
    "Chinese": "k近邻",
    "French": "k plus proches voisins",
    "Japanese": "\"k近傍\"",
    "Russian": "k ближайших соседей"
  },
  {
    "English": "k-center",
    "context": "1: Our technique also works for a combination of the aforementioned generalizations that are orthogonal to each other. To consider an extreme example , consider Colorful Matroid Median with different color classes ( a similar version for <mark>k-Center</mark> objective has been recently studied by [ 3 ] ) , where we want to find a set of facilities that is independent in the given matroid , in order to minimize the sum of distances of all except m t outlier points for<br>",
    "Arabic": "مركز-ك",
    "Chinese": "k-中心",
    "French": "k-centre",
    "Japanese": "k中心",
    "Russian": "k-центр"
  },
  {
    "English": "k-d tree",
    "context": "1: There are not many previous works that have dealt with scaling up feature selection problems. It is worth mentioning the work of Robnik-Sikonja [20]. They proposed a method to speed up the Relief algorithm by means of <mark>k-d trees</mark>.<br>2: Owing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27,17] similar to <mark>k-d trees</mark> [12]. That method did not come with provable runtime guarantees.<br>",
    "Arabic": "شجرة ك-د",
    "Chinese": "k-d树",
    "French": "arbre k-d",
    "Japanese": "k-d木",
    "Russian": "k-d дерево"
  },
  {
    "English": "k-good list",
    "context": "1: We demonstrate that optimal <mark>k-best lists</mark> can be extracted significantly faster using our algorithm than with previous methods.<br>2: In addition to presenting the algorithm, we show experiments in which we extract <mark>k-best lists</mark> for three different kinds of grammars: the lexicalized grammars of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006).<br>",
    "Arabic": "قائمة أفضل k",
    "Chinese": "k-最优候选列表",
    "French": "liste k-meilleure",
    "Japanese": "k-best リスト",
    "Russian": "список k-наилучших"
  },
  {
    "English": "k-good parsing",
    "context": "1: Syntactic machine translation (Galley et al., 2004) uses tree transducer grammars to translate sentences. Transducer rules are synchronous contextfree productions that have both a source and a target side. We examine the cost of <mark>k-best parsing</mark> in the source side of such grammars with KA * , which can be a first step in translation.<br>2: Thus, the \"extra\" inside items popped in the bottom-up pass during <mark>k-best parsing</mark> as compared to 1-best parsing are those items i satisfying \n δ(g 1 ) ≤ β(i) + h(i) ≤ δ(g k ) \n<br>",
    "Arabic": "تحليل ك-الجيد",
    "Chinese": "k-最佳分析",
    "French": "analyse k-meilleurs",
    "Japanese": "k最良構文解析",
    "Russian": "k-хороший синтаксический анализ"
  },
  {
    "English": "k-hop neighbor",
    "context": "1: This phenomenon, referred to as 'over-squashing', has been heuristically attributed to graph bottlenecks where the number of <mark>k-hop neighbors</mark> grows rapidly with k. We provide a precise description of the over-squashing phenomenon in GNNs and analyze how it arises from bottlenecks in the graph.<br>",
    "Arabic": "جار في مسافة k",
    "Chinese": "k跳邻居",
    "French": "voisin à k sauts",
    "Japanese": "k-ホップ近傍",
    "Russian": "соседи на расстоянии k-хопов"
  },
  {
    "English": "k-l divergence",
    "context": "1: , K. For any q of the form (51), the ELBO (50) has an analytic expression, which we derive in part by making use of the formula for the <mark>K-L divergence</mark> between two normal distributions (Hastie et al., 2009): \n<br>2: Since π only appears in the ELBO in the <mark>K-L divergence</mark> term with respect to the prior, solving ( 59) is equivalent to solving \n π * = argmin π ∈ S K p j=1 D KL (q j ∥ p prior ), \n which simplifies further: \n<br>",
    "Arabic": "تباين كولباك-ليبلر",
    "Chinese": "K-L 散度",
    "French": "divergence de Kullback-Leibler",
    "Japanese": "K-L ダイバージェンス",
    "Russian": "дивергенция Кульбака-Лейблера"
  },
  {
    "English": "k-mean",
    "context": "1: C.1 Uniform Effect in <mark>K-Means</mark> <mark>K-Means</mark> (MacQueen, 1967) is the most popular and successful clustering algorithm, where sample re-allocation and center renewal are executed alternatively to minimize the intra-cluster distance. However, Xiong et al. (2006) points out that <mark>K-Means</mark> algorithm tends to produce balanced clustering result, a.k.a., uniform effect.<br>2: We compared the proposed HMRF-KMEANS algorithm with two ablations as well as unsupervised <mark>K-Means</mark> clustering. The following variants were compared for distortion measures D cos a and D I a as representatives for Bregman divergences and directional measures respectively: \n<br>",
    "Arabic": "ك-يعني",
    "Chinese": "k-均值聚类法",
    "French": "k-moyennes",
    "Japanese": "k平均法",
    "Russian": "k-средних"
  },
  {
    "English": "k-mean algorithm",
    "context": "1: Thus, the inference step turns into a simple computation for each S j separately, a process which can be performed in linear time. The M-step for table CPDs can be performed easily in closed form. To provide a good starting point for EM, we initialize the cluster assignments using the <mark>K-means algorithm</mark>.<br>",
    "Arabic": "خوارزمية الـ k-متوسط",
    "Chinese": "K均值算法",
    "French": "algorithme des k-moyennes",
    "Japanese": "\"k-平均アルゴリズム\"",
    "Russian": "алгоритм k-средних"
  },
  {
    "English": "k-mean clustering",
    "context": "1: However, it cannot be directly applied for our viseme and phoneme clustering due to imbalanced data distribution (see §A.4). This may challenge <mark>K-Means clustering</mark> according to uniform effect (Xiong et al., 2006). As shown in Fig.<br>",
    "Arabic": "تجميع ك-المتوسطات",
    "Chinese": "K均值聚类",
    "French": "Partitionnement k-moyennes",
    "Japanese": "k-平均クラスタリング",
    "Russian": "Кластеризация k-средних"
  },
  {
    "English": "k-means clustering algorithm",
    "context": "1: [3], in which they propose an evolutionary hierarchical clustering algorithm and an evolutionary <mark>k-means clustering algorithm</mark>. We mainly discuss the latter because of its connection to spectral clustering. Chakrabarti et al. proposed to measure the temporal smoothness by a distance between the clusters at time t and those at time t-1.<br>2: We use the <mark>k-means clustering algorithm</mark>, with the L 2 norm as a distance metric (MacKay, 2002), to divide vectors into clusters. Clusters created by this algorithm contain adjacent vectors in a Euclidean space. Clusters represent sentences with similar features values.<br>",
    "Arabic": "خوارزمية تجميع k-means",
    "Chinese": "k均值聚类算法",
    "French": "algorithme de regroupement k-means",
    "Japanese": "k平均クラスタリング法",
    "Russian": "алгоритм кластеризации k-средних"
  },
  {
    "English": "k-nearest neighbor",
    "context": "1: We compare our Information Theoretic Metric Learning algorithm (ITML) to existing methods across two applications: semi-supervised clustering and <mark>k-nearest neighbor</mark> (k-NN) classification. We evaluate metric learning for k-NN classification via two-fold cross validation with k = 4. All results presented represent the average over 5 runs.<br>2: We have to replace the radius r n by the <mark>k-nearest neighbor</mark> radius, that is, the distance of a data point to its kth nearest neighbor. This leads to additional difficulties, as this radius is a random variable as well.<br>",
    "Arabic": "ك-أقرب جار",
    "Chinese": "k-最近邻",
    "French": "k-plus proche voisin",
    "Japanese": "k最近傍",
    "Russian": "k-ближайших соседей"
  },
  {
    "English": "k-nearest neighbor classifier",
    "context": "1: Furthermore, these results confirm a recent finding regarding the finite-sample risk for a k-nearest neighbor classifer (Snapp & Venkatesh, 1998) which show that the finite-sample risk can be expressed asymptotically as: \n R m = R ∞ + k j=2 c j n −j/d + O(n −(k+1)/d ), n → ∞, \n<br>2: Exactly preserving a k-nearest neighbor graph on the data during embedding means a <mark>k-nearest neighbor classifier</mark> will perform equally well on the recovered low-dimensional embedding K as it did on the original high-dimensional dataset.<br>",
    "Arabic": "تصنيف الجار الأقرب k",
    "Chinese": "k最近邻分类器",
    "French": "classificateur des k plus proches voisins",
    "Japanese": "k近傍分類器",
    "Russian": "классификатор k-ближайших соседей"
  },
  {
    "English": "k-nearest neighbor graph",
    "context": "1: • the directed <mark>k-nearest neighbor graph</mark> G n,k : there is a directed edge from x i to x j if x j is one of the k nearest neighbors of x i for 1 ≤ i, j ≤ n, i = j. In the following we work on the space R d with Euclidean metric dist.<br>2: the surface to a sample point on the other side of the surface . The corresponding quantity for the directed <mark>k-nearest neighbor graph</mark> is denoted by cut n,k (S). For a set A ⊆ R d the volume of {x 1 , . . .<br>",
    "Arabic": "الرسم البياني لأقرب الجيران k",
    "Chinese": "k最近邻图",
    "French": "graphe des k plus proches voisins",
    "Japanese": "k近傍グラフ",
    "Russian": "граф k-ближайших соседей"
  },
  {
    "English": "kernel",
    "context": "1: information embedded into recent tweets) risk to be incompatible. However, the learning of the optimal <mark>Kernel</mark> combination as well as a proper history size for the USPK are still worth of deeper investigation. Finally, user interaction dynamics are particularly complex in social networks and deserve better representations about reputation, authority and influence in future explorations.<br>2: <mark>Kernel</mark> Bayes' Rule [Fukumizu et al., 2014] is a non-parametric method of computing a posterior based on the embedding of probabilities in an RKHS. All these methods require sampling from a distribution and do not address the question of which samples to choose if generating them is expensive. The work in Bryan et al.<br>",
    "Arabic": "نواة",
    "Chinese": "核函数",
    "French": "noyau",
    "Japanese": "カーネル",
    "Russian": "ядро"
  },
  {
    "English": "kernel approximation",
    "context": "1: We achieve a 38.5% mean accuracy (and 44.2% using the recently released DECAF features [11]) as compared to 40.5% in [25]. Given that our model is not explicitly designed for this task and the <mark>kernel approximation</mark> involved, this is a very encouraging result.<br>2: We adopt the anchor graphs method (see Section 4.5) for <mark>kernel approximation</mark>. In our experiments, we take two steps to determine the number of anchor points. In the first step, the optimal σ and β are selected on the validation set in each experiment.<br>",
    "Arabic": "تقريب النواة",
    "Chinese": "核近似",
    "French": "approximation du noyau",
    "Japanese": "カーネル近似",
    "Russian": "аппроксимация ядра"
  },
  {
    "English": "kernel bandwidth",
    "context": "1: Theorem 1 guarantees that the population quantity FSSD 2 = 0 if and only if p = q for any choice of {v i } J i=1 drawn from a distribution with a density. In practice , we are forced to rely on the empirical FSSD 2 , and some test locations will give a higher detection rate ( i.e. , test power ) than others for finite n. Following the approaches of [ 17,20,19,29 ] , we choose the test locations V = { v j } J j=1 and <mark>kernel bandwidth</mark> σ 2 k so as<br>2: There are several ways in which the proposed approach can be extended. First, using an adaptive or non-constant <mark>kernel bandwidth</mark> should lead to higher accuracy. It is also interesting to explore tighter generalization error bounds by directly analyzing the solutions of the marginal regression iterative algorithm.<br>",
    "Arabic": "عرض النطاق الترددي للنواة",
    "Chinese": "核带宽",
    "French": "largeur de bande du noyau",
    "Japanese": "カーネル帯域幅",
    "Russian": "ширина ядра"
  },
  {
    "English": "kernel classifier",
    "context": "1: The authors of (Okanohara & Tsujii, 2007), like us, also take a two-class approach (true/fake sentences). They use a shallow (kernel) classifier.<br>2: This extension is necessary to obtain a single distance measure if a large selection or class of characteristic kernels is potentially appropriate. This generalization is reasonable, given that it corresponds to the problem of learning the kernel by minimizing the risk of the corresponding <mark>kernel classifier</mark>.<br>",
    "Arabic": "مصنف النواة",
    "Chinese": "核分类器",
    "French": "classificateur à noyau",
    "Japanese": "カーネル分類器",
    "Russian": "ядерный классификатор"
  },
  {
    "English": "kernel density",
    "context": "1: Also, it is straightforward to understand the marginalization properties of the Gaussian Cox Process if the region of interest changes, but a mixture of Betas appears to have discontinuities when expanding the studied region. The Sigmoidal Gaussian Cox Process is superior to the frequentist <mark>kernel density</mark> approach (Diggle, 1985) in several ways.<br>",
    "Arabic": "كثافة النواة",
    "Chinese": "核密度估计",
    "French": "densité de noyau",
    "Japanese": "カーネル密度推定",
    "Russian": "ядерная плотность"
  },
  {
    "English": "kernel density estimate",
    "context": "1: We begin by re-representing the smooth sparse coding problem in a convenient format for analysis. Let x 1 , . . . , x n be independent random variables with a common probability measure P with a density p. We denote by P n the empirical measure over the n samples, and the <mark>kernel density estimate</mark> of p is defined by \n<br>2: An estimate P of P is said to be linear if there exist functions T i (X i , •) : F → R such that for all measurable A ∈ F, \n P (A) = n i=1 T i (X i , A). (8) \n Classic examples of linear estimators include the empirical distribution ( T i ( X i , A ) = 1 n 1 { Xi∈A } , the <mark>kernel density estimate</mark> ( T i ( X i , A ) = 1 n A K ( X i , • ) for some bandwidth h > 0 and smoothing kernel K : X × X<br>",
    "Arabic": "تقدير كثافة النواة",
    "Chinese": "核密度估计",
    "French": "estimation de la densité du noyau",
    "Japanese": "カーネル密度推定",
    "Russian": "оценка плотности ядра"
  },
  {
    "English": "kernel density estimation",
    "context": "1: Similarly, for the camera height y c , we estimate a prior distribution using <mark>kernel density estimation</mark> over the y c values (computed based on objects of known height in the scene) in a set of training images.<br>2: . , µn = j w j n φ(x j n ) j w j n(13) \n 3 Distribution regression with explicit random features was previously considered in Oliva et al. [19] using Rahimi and Recht [25] to speed up an earlier distribution regression method based on <mark>kernel density estimation</mark> [22].<br>",
    "Arabic": "تقدير كثافة النواة",
    "Chinese": "核密度估计",
    "French": "estimation par noyau de densité",
    "Japanese": "カーネル密度推定",
    "Russian": "ядерная оценка плотности"
  },
  {
    "English": "kernel evaluation",
    "context": "1: It is unsurprising that Pegasos and stochastic DCA, as the simplest algorithms, tend to have performance most dominated by <mark>kernel evaluations</mark>. If we were to compare the algorithms in terms of the number of <mark>kernel evaluations</mark>, rather than elapsed time, then LASVMs performance would generally improve slightly relative to the others.<br>2: It is also important to emphasize that although the feature mapping φ(•) was used in the above mathematical derivations, the pseudo-code of the algorithm itself makes use only of <mark>kernel evaluations</mark> and obviously does not refer to the implicit mapping φ(•).<br>",
    "Arabic": "تقييم النواة",
    "Chinese": "核函数评估",
    "French": "évaluation du noyau",
    "Japanese": "カーネル評価",
    "Russian": "оценка ядра"
  },
  {
    "English": "kernel function",
    "context": "1: Hence, we expect that by decomposing graphs into Algorithm 2 Core-based Kernel Input: A pair of graphs G and G Output: Result of the kernel function val val = 0 \n<br>2: The resulting <mark>Kernel function</mark> is the cosine similarity between tweet vector pairs, in line with (Cristianini et al., 2002). Notice that the adoption of a distributional approach does not limit the overall application, as it can be automatically applied without relying on any manually coded resource. User Sentiment Profile Context (USPK).<br>",
    "Arabic": "دالة النواة",
    "Chinese": "核函数",
    "French": "fonction noyau",
    "Japanese": "カーネル関数",
    "Russian": "ядерная функция"
  },
  {
    "English": "kernel learning",
    "context": "1: These works develop early Bayesian neural networks, and use a Laplace approximation of the LML for neural architecture design, and learning hyperparameters such as weight-decay (MacKay, 1992c(MacKay, , 1995. In addition to the compelling philosophical arguments, the practical success of the marginal likelihood is reason alone to study it closely. For example , LML optimization is now the de facto procedure for <mark>kernel learning</mark> with Gaussian processes , working much better than other approaches such as standard cross-validation and covariogram fitting , and can be applied in many cases where these standard alternatives are simply intractable ( e.g. , Rasmussen and Williams , 2006 ; Wilson , 2014 ; Lloyd et al. , 2014<br>2: Hence, the metric learning (3.3) and the <mark>kernel learning</mark> (4.7) problems are equivalent. We have proven that the information-theoretic metric learning problem is related to a low-rank <mark>kernel learning</mark> problem. We can easily modify Algorithm 1 to optimize for K-this is necessary in order to kernelize the algorithm.<br>",
    "Arabic": "تعلم النواة",
    "Chinese": "核学习",
    "French": "apprentissage du noyau",
    "Japanese": "カーネル学習",
    "Russian": "ядерное обучение"
  },
  {
    "English": "kernel learning problem",
    "context": "1: However, one may still implicitly update the Mahalanobis matrix A via updates in kernel space for an equivalent <mark>kernel learning problem</mark> in which K = X T AX for X = [x 1 , . . . , x n ]. If K 0 is an input kernel matrix for the data, the appropriate update is: \n<br>",
    "Arabic": "مشكلة تعلم النواة",
    "Chinese": "核学习问题",
    "French": "problème d'apprentissage du noyau",
    "Japanese": "カーネル学習問題",
    "Russian": "проблема обучения ядра"
  },
  {
    "English": "kernel machine",
    "context": "1: As usual with <mark>kernel machines</mark>, the feature mapping function Φ is implicitly defined by the specification of a joint kernel function K(x, y,x,ȳ) = Φ(x, y), Φ(x,ȳ) . (2) \n Consider training patterns x 1 . . .<br>2: Such a procedure has been empirically successful in the context of neural networks [9], <mark>kernel machines</mark> [17], and multiple dipole fitting for MEG [12], a significant benefit to the latter being that the optimal number of dipoles need not be known a priori.<br>",
    "Arabic": "ماكينة النواة",
    "Chinese": "核机器",
    "French": "machine à noyau",
    "Japanese": "カーネルマシン",
    "Russian": "ядерная машина"
  },
  {
    "English": "kernel matrix",
    "context": "1: Inspired by randomized algorithms for approximating <mark>kernel matrices</mark> (e.g., [3,4]), we efficiently convert the training and evaluation of any kernel machine into the corresponding operations of a linear machine by mapping data into a relatively low-dimensional randomized feature space.<br>2: where K = Υ Υ and L = Φ Φ are the <mark>kernel matrices</mark>, and α is the generalized eigenvector. After normalization, we have \n v = 1 √ α Lα \n Φα.<br>",
    "Arabic": "مصفوفة النواة",
    "Chinese": "核矩阵",
    "French": "matrice noyau",
    "Japanese": "カーネル行列",
    "Russian": "ядро матрицы"
  },
  {
    "English": "kernel method",
    "context": "1: This is fundamentally different from convex optimization, such as <mark>kernel method</mark>, where (with an 2 regularization) there is an unique global minimum so the choice of optimization algorithm or the random seed of the initialization does not matter (thus, ensemble does not help at all).<br>2: We formalize sentence re-writing learning as a <mark>kernel method</mark>. Following the literature of string kernel, we use the terms \"string\" and \"character\" instead of \"sentence\" and \"word\". Suppose that we are given training data consisting of re-writings of strings and their responses \n<br>",
    "Arabic": "طريقة النواة",
    "Chinese": "核方法",
    "French": "méthode à noyau",
    "Japanese": "カーネル法",
    "Russian": "метод ядра"
  },
  {
    "English": "kernel operation",
    "context": "1: Our current implementation of S4 actually uses the naive O(N L) algorithm which is easily parallelized on GPUs and has more easily accessible libraries allowing it to be implemented; we leverage the pykeops library for memory-efficient <mark>kernel operations</mark>.<br>",
    "Arabic": "عملية النواة",
    "Chinese": "核操作",
    "French": "opération de noyau",
    "Japanese": "カーネル演算",
    "Russian": "ядерная операция"
  },
  {
    "English": "kernel operator",
    "context": "1: Schmidt on L 2 (µ). Then, Mercer's theorem (Wahba, 1990) states that the corresponding <mark>kernel operator</mark> has a discrete eigenspectrum {(λ s , φ s (•))}, and \n k(x, x ) = s≥1 λ s φ s (x)φ s (x ), \n<br>2: However, the mapping φ(•) is never specified explicitly but rather through a <mark>kernel operator</mark> K(x, x ) = φ(x), φ(x ) yielding the inner products after the mapping φ(•).<br>",
    "Arabic": "مشغل النواة",
    "Chinese": "核算子",
    "French": "opérateur du noyau",
    "Japanese": "カーネル演算子",
    "Russian": "оператор ядра"
  },
  {
    "English": "kernel parameter",
    "context": "1: We note that the LML can be used to learn many such <mark>kernel parameters</mark> (Rasmussen and Williams, 2006;Wilson and Adams, 2013;Wilson et al., 2016a).<br>",
    "Arabic": "معلمة النواة",
    "Chinese": "核参数",
    "French": "paramètre de noyau",
    "Japanese": "カーネルパラメータ",
    "Russian": "ядерный параметр"
  },
  {
    "English": "kernel regression",
    "context": "1: Weighted Edit Distance Reveals More Non-Arbitrariness. We first assessed whether the structure found by <mark>kernel regression</mark> could arise merely by arbitrary, random pairings of form and meaning (i.e., strings and semantic vectors). We adopt a Monte Carlo testing procedure similar to the Mantel test of §2.1.<br>2: can be naturally handled and, if desired, the noise covariance Σ can be seamlessly estimated as well (see [3] for a special case of the latter in the context of <mark>kernel regression</mark>). This addresses many of the concerns raised in [8] pertaining to existing MAP methods.<br>",
    "Arabic": "انحدار النواة",
    "Chinese": "核回归",
    "French": "régression de noyau",
    "Japanese": "カーネル回帰",
    "Russian": "Ядерная регрессия"
  },
  {
    "English": "kernel ridge regression",
    "context": "1: (11) they are generic locations si, but this difference will go away in Section 5 when we propose using GP regression for distribution regression.) The predictive mean of GP regression is exactly equal to the <mark>kernel ridge regression</mark> estimator, with σ 2 corresponding to λ.<br>2: In this section, we briefly state the main results we need from Gaussian process regression [26], reviewing the wellknown connection between the posterior mean in GP regression and the <mark>kernel ridge regression</mark> estimator of Eq. (7). Given observations (s1, y1), . . .<br>",
    "Arabic": "انحدار ريدج النواة",
    "Chinese": "核岭回归",
    "French": "régression ridge du noyau",
    "Japanese": "カーネルリッジ回帰",
    "Russian": "Ядерная гребневая регрессия"
  },
  {
    "English": "kernel size",
    "context": "1: With a small <mark>kernel size</mark>, good features in an image typically appear only at a few patches, and most other patches are random noise or low-magnitude feature noises. More importantly, our noise parameters shall ensure that, the concept class is not learnable by linear classifiers or constant degree polynomials.<br>2: We then use a final 1x1-Convolution to create a [2,96,96] tensor representing the nonnormalized log-probabilities of whether or not an given location is navigable or not. Each CoordConv has <mark>kernel size</mark> 3, padding 1, and stride 1. CoordUpConv has <mark>kernel size</mark> 3, padding 0, and stride 2.<br>",
    "Arabic": "حجم النواة",
    "Chinese": "卷积核尺寸",
    "French": "taille du noyau",
    "Japanese": "カーネルサイズ",
    "Russian": "размер ядра"
  },
  {
    "English": "kernel smoothing",
    "context": "1: We created three one-dimensional data sets using the following intensity functions: \n 1. A sum of an exponential and a Gaussian bump:  (Diggle, 1985) and with the Log Gaussian Cox Process (Møller et al., 1998  We compared the SGCP to the classical <mark>kernel smoothing</mark> (KS) approach of Diggle (1985).<br>2: We performed edge-corrected <mark>kernel smoothing</mark> using a quartic kernel and the recommended mean-square minimization technique for bandwidth selection. We also compared to the most closely-related nonparametric Bayesian technique, the Log Gaussian Cox Process of Rathbun and Cressie (1994) and Møller et al. (1998).<br>",
    "Arabic": "تملیس النواة",
    "Chinese": "核平滑",
    "French": "lissage par noyau",
    "Japanese": "カーネル平滑化",
    "Russian": "сглаживание ядра"
  },
  {
    "English": "kernel space",
    "context": "1: However, one may still implicitly update the Mahalanobis matrix A via updates in <mark>kernel space</mark> for an equivalent kernel learning problem in which K = X T AX for X = [x 1 , . . . , x n ]. If K 0 is an input kernel matrix for the data, the appropriate update is: \n<br>2: points in the <mark>kernel space</mark> . This requires the computation of \n<br>",
    "Arabic": "مساحة النواة",
    "Chinese": "核空间",
    "French": "espace noyau",
    "Japanese": "カーネル空間",
    "Russian": "пространство ядра"
  },
  {
    "English": "kernel spectrum",
    "context": "1: They note that in the absence of noise and with invertible kernels p(k|y) can be exactly evaluated for sparse priors as well. This reduces to optimizing the sparsity of the image plus the log determinant of the <mark>kernel spectrum</mark>.<br>",
    "Arabic": "طيف النواة",
    "Chinese": "核谱",
    "French": "spectre du noyau",
    "Japanese": "カーネルスペクトル",
    "Russian": "спектр ядра"
  },
  {
    "English": "kernel trick",
    "context": "1: The cost of this convenience is that algorithms access the data only through evaluations of k(x, y), or through the kernel matrix consisting of k applied to all pairs of datapoints. As a result, large training sets incur large computational and storage costs. Instead of relying on the implicit lifting provided by the <mark>kernel trick</mark> , we propose explicitly mapping the data to a low-dimensional Euclidean inner product space using a randomized feature map z : R d → R D so that the inner product between a pair of transformed points approximates their kernel evaluation : k ( x , y ) = φ ( x<br>2: Per the <mark>kernel trick</mark> (Schölkopf and Smola, 2001), k can be viewed as the inner product in a reproducing kernel Hilbert space (RKHS) H equipped with a feature map ϕ : X → H. If H is separable, we may approximate this inner product as \n<br>",
    "Arabic": "خدعة النواة",
    "Chinese": "核技巧",
    "French": "astuce du noyau",
    "Japanese": "カーネルトリック",
    "Russian": "ядерный трюк"
  },
  {
    "English": "kernel value",
    "context": "1: Both SVMstruct and LaRankGap use small subsets of the gradient coefficients. Although these subsets have similar size, LaRankGap avoids the training time penalty experienced by SVMstruct. Both SVMstruct and LaRank make heavy use of <mark>kernel values</mark> involving two support patterns.<br>2: Essentially we have shifted the random hyperplane r according to A, and by factoring it by G we allow the random hash function itself to \"carry\" the information about the learned metric. The denominator in the cosine term normalizes the learned <mark>kernel values</mark>.<br>",
    "Arabic": "قيمة النواة",
    "Chinese": "核值",
    "French": "valeur du noyau",
    "Japanese": "カーネル値",
    "Russian": "значения ядра"
  },
  {
    "English": "kernel weight",
    "context": "1: The exponential distribution reveals different frequencies among different ids. For the dense weights (e.g., <mark>kernel weights</mark>), their gradients appear for each sample while embedding does not have gradients if the corresponding ids do not show up.<br>",
    "Arabic": "وزن النواة",
    "Chinese": "核权重",
    "French": "poids du noyau",
    "Japanese": "カーネル重み (カーネルじゅうみ)",
    "Russian": "вес ядра"
  },
  {
    "English": "kernel width",
    "context": "1: Using fixed concentration 0.25 yields an effective <mark>kernel width</mark> [−2σ, 2σ] of one timestep, ensuring that the bulk of the attention kernel either falls on a single segment label or straddles two consecutive segment labels and preventing the decoder from spreading its attention over many higher-level segments.<br>2: computation can have access to global context and the computation consuming is affordable on long inputs. To align the dimension, we project the scalar context x t i into d model -dim vector u t i with 1-D convolutional filters (<mark>kernel width</mark>=3, stride=1). Thus, we have the feeding vector \n<br>",
    "Arabic": "عرض النواة",
    "Chinese": "核宽度",
    "French": "largeur du noyau",
    "Japanese": "カーネル幅",
    "Russian": "ширина ядра"
  },
  {
    "English": "kernel-base classification",
    "context": "1: Having showed how characteristic kernels play a role in <mark>kernel-based classification</mark>, in the following section, we provide a novel characterization for them.<br>",
    "Arabic": "التصنيف بناءً على النواة",
    "Chinese": "核基分类",
    "French": "classification basée sur le noyau",
    "Japanese": "カーネルベース分類",
    "Russian": "классификация на основе ядра"
  },
  {
    "English": "key",
    "context": "1: Each SRU++ decoder layer make uses of X src by simplying treating it as extra attention context. That is, the query, <mark>key</mark> and value 6 https://github.com/asappresearch/ imitkd/blob/master/configs/iwslt/ teacher.yaml representations are computed by concatenating the input of the current layer X tgt with X src , \n<br>2: This is, in fact, analogous to how queries and <mark>key</mark>s function within the self-attention mechanism: A query q t is compared to the <mark>key</mark> k t ′ of each token t ′ in the sequence and the greater the match, the greater attention paid to token t ′ by query token t. \n<br>",
    "Arabic": "مفتاح",
    "Chinese": "关键",
    "French": "clé",
    "Japanese": "キー",
    "Russian": "ключ"
  },
  {
    "English": "keypoint",
    "context": "1: For each track, we freeze the location of the <mark>keypoint</mark> u with highest connectivity, as in [24], and constrain the location p u of each <mark>keypoint</mark> w.r.t.<br>2: We found that a relatively high resolution output (compared to masks) is required for <mark>keypoint</mark>-level localization accuracy. Models are trained on all COCO trainval35k images that contain annotated <mark>keypoint</mark>s.<br>",
    "Arabic": "نقطة مفتاحية",
    "Chinese": "关键点",
    "French": "point clé",
    "Japanese": "キーポイント",
    "Russian": "ключевая точка"
  },
  {
    "English": "keypoint detection",
    "context": "1: This allows us to define new targets for the <mark>keypoint detection</mark> cost that, on average, are a better match for the location of the 3D detections with respect to the mesh model, as shown in Fig. 4.<br>2: Table 4 shows that our result (62.7 AP kp ) is 0.9 points higher than the COCO 2016 <mark>keypoint detection</mark> winner [6] that uses a multi-stage processing pipeline (see caption of Table 4). Our method is considerably simpler and faster.<br>",
    "Arabic": "كشف النقاط المفتاحية",
    "Chinese": "关键点检测",
    "French": "détection de points clés",
    "Japanese": "重要ポイント検出",
    "Russian": "детекция ключевых точек"
  },
  {
    "English": "keypoint detector",
    "context": "1: Using multiple feature levels enlarges the basin of convergence but increases the computational requirements. The radius of convergence that is required depends on the noise of the <mark>keypoint detector</mark> and on the resolution of the image from which keypoints are detected.<br>",
    "Arabic": "كاشف النقطة الرئيسية",
    "Chinese": "关键点检测器",
    "French": "détecteur de points clés",
    "Japanese": "キーポイント検出器",
    "Russian": "детектор ключевых точек"
  },
  {
    "English": "keypoint location",
    "context": "1: In particular, given the fitting results of 70 identities, we approximate the target 3D <mark>keypoint locations</mark> as a function of the final fitted mesh vertices following the procedure of [33] to find a sparse, linear combination of vertices that approximates the position of the target 3D keypoint.<br>2: [24] proposed to refine <mark>keypoint locations</mark> prior to SfM via an analogous geometric cost constrained with local optical flow. This can improve SfM, but has limited accuracy and scalability. In this work, we argue that local image information is valuable throughout the SfM process to improve its accuracy.<br>",
    "Arabic": "موقع النقطة المفتاحية",
    "Chinese": "关键点位置",
    "French": "Localisation des points clés",
    "Japanese": "特徴点位置",
    "Russian": "расположение ключевых точек"
  },
  {
    "English": "keypoint match",
    "context": "1: To calculate speeds of traffic flow with this system, we need to calibrate the camera projection then use <mark>keypoint matches</mark> to identify motion. We find it necessary to classify each moving patch in the image stream to distinguish vehicles from non-vehicles, given the amount of visual clutter in this setting. The system is pictured in Figure 5.<br>",
    "Arabic": "مطابقة النقاط الرئيسية",
    "Chinese": "关键点匹配",
    "French": "correspondance de points clés",
    "Japanese": "キーポイントマッチ",
    "Russian": "соответствие ключевых точек"
  },
  {
    "English": "knapsack problem",
    "context": "1: Then, rather than solving every iteration through the greedy algorithm, we can solve every iteration as a <mark>knapsack problem</mark> (minimizing a modular function over a modular lower bound constraint) [23], using say, a dynamic programming based approach. This could potentially improve over the greedy variant, but at a potentially higher computational cost.<br>2: The intuition behind this algorithm is that if we know the real value of the arms, then the budget-limited MAB can be reduced to an unbounded <mark>knapsack problem</mark>, where the optimal solution is to subsequently pull from the set of arms that forms the solution of the <mark>knapsack problem</mark>.<br>",
    "Arabic": "مشكلة الحقيبة",
    "Chinese": "背包问题",
    "French": "problème du sac à dos",
    "Japanese": "ナップサック問題",
    "Russian": "задача о рюкзаке"
  },
  {
    "English": "knowledge Base",
    "context": "1: <mark>Knowledge Base</mark> QA Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2013;Yih et al., 2016;Talmor and Berant, 2018;Keysers et al., 2020;Gu et al., 2021, inter alia).<br>",
    "Arabic": "قاعدة المعرفة",
    "Chinese": "知识库",
    "French": "Base de connaissances",
    "Japanese": "知識ベース",
    "Russian": "база знаний"
  },
  {
    "English": "knowledge compilation",
    "context": "1: Probabilistic logic programs can, just like Bayesian networks, be compiled into circuits using <mark>knowledge compilation</mark> [Darwiche, 2003]. Although probabilistic inference is hard (#Pcomplete), once the circuit is obtained, inference is linear in the size of the circuit [De Raedt and Kimmig, 2015;Fierens et al., 2015].<br>2: introduction [ Vennekens et al. , 2007 ] , and <mark>knowledge compilation</mark> [ Bogaerts and Van den Broeck , 2015 ] . On the other hand, from the context of AFT, the embedding of JT can serve as inspiration for developing more general algebraic explanation mechanisms.<br>",
    "Arabic": "ترميز المعرفة",
    "Chinese": "知识编译",
    "French": "compilation des connaissances",
    "Japanese": "知識コンパイル",
    "Russian": "компиляция знаний"
  },
  {
    "English": "knowledge distillation",
    "context": "1: See the supplementary material for full details of the process and a user study on the final quality of labels generated using <mark>Knowledge Distillation</mark> (showing < 7% error).<br>2: <mark>Knowledge Distillation</mark> There exist some works that explore the idea of distilling rationales knowledge from a large LM to a small LM as the student. Chan et al. proposed to learn a student model that only predicts answers from a teacher model that is augmented with rationales. Eisenstein et al.<br>",
    "Arabic": "تقطير المعرفة",
    "Chinese": "知识蒸馏",
    "French": "distillation des connaissances",
    "Japanese": "知識蒸留",
    "Russian": "передача знаний"
  },
  {
    "English": "knowledge element",
    "context": "1: Similar to the news domain, the <mark>knowledge elements</mark> extracted from text and images in literature are complementary. Our framework advances state-of-the-art by extending the <mark>knowledge elements</mark> to more fine-grained types, incorporating image analysis and cross-media knowledge grounding, and KG matching into QA.<br>2: To combat COVID-19, both clinicians and scientists need to digest vast amounts of relevant biomedical knowledge in scientific literature to understand the disease mechanism and related biological functions. We have developed a novel and comprehensive knowledge discovery framework, COVID-KG to extract finegrained multimedia <mark>knowledge elements</mark> (entities and their visual chemical structures, relations and events) from scientific literature.<br>",
    "Arabic": "عنصر المعرفة",
    "Chinese": "知识元素",
    "French": "élément de connaissance",
    "Japanese": "知識要素",
    "Russian": "элемент знаний"
  },
  {
    "English": "knowledge graph",
    "context": "1: We select entities that can represent head personas using ATOMIC 20 20 (Hwang et al., 2021), a common-sense KG covering knowledge about physical objects, daily events, and social interactions. We assume that entities related to personas should be about human beings, rather than other animals or non-living objects.<br>2: These lead to several frameworks that aim for extracting knowledge (i.e., scientific concepts and their relations) from scientific documents and representing them as a <mark>Knowledge Graph</mark> (KG) (Luan et al., 2018;Eberts and Ulges, 2019).<br>",
    "Arabic": "الرسم البياني المعرفي",
    "Chinese": "知识图谱",
    "French": "graphe de connaissances",
    "Japanese": "知識グラフ",
    "Russian": "граф знаний"
  },
  {
    "English": "knowledge graph completion",
    "context": "1: For classification tasks, e.g., relation prediction for <mark>knowledge graph completion</mark>, the existing triplets are split into the clients by latent dirichlet allocation (LDA) [3]. For regression tasks, e.g., PCQM4M, FS-G can discretize the label before conducting LDA.<br>2: Rel-CSKGC differs from them in that we utilize pretrained language models to predict the relation given the head event and the tail event. Similar relation prediction methods targeting at the <mark>knowledge graph completion</mark> have been proposed (Socher et al., 2013;Yao et al., 2019;Cao et al., 2020).<br>",
    "Arabic": "استكمال الرسم البياني المعرفي",
    "Chinese": "知识图谱补全",
    "French": "complétion du graphe de connaissances",
    "Japanese": "知識グラフ補完",
    "Russian": "завершение графа знаний"
  },
  {
    "English": "knowledge representation",
    "context": "1: Expressive query languages are gaining relevance in <mark>knowledge representation</mark> (KR), and new reasoning problems come to the fore. Especially query containment is interesting in this context. The problem is known to be decidable for many expressive query languages, but exact complexities are often missing.<br>2: [2015] defined an abstract theory of justifications suitable for describing the semantics of a range of logics in <mark>knowledge representation</mark>, computational and mathematical logic, including logic programs, argumentation frameworks and nested least and greatest fixpoint definitions.<br>",
    "Arabic": "تمثيل المعرفة",
    "Chinese": "知识表示",
    "French": "représentation des connaissances",
    "Japanese": "知識表現",
    "Russian": "представление знаний"
  },
  {
    "English": "knowledge transfer",
    "context": "1: To sum up, our contributions are as follows: \n • We propose a <mark>knowledge transfer</mark> method with pluggable modules to acquire more knowledge of new languages, which achieves competitive translation qualities on incremental language pairs.<br>2: Moreover, it is more efficient to utilize the <mark>knowledge transfer</mark> scheme than introducing randomly initialized parameters in incremental learning. Experimental results demonstrate that the proposed method outperforms several strong baselines in the comprehensive language consideration.<br>",
    "Arabic": "نقل المعرفة",
    "Chinese": "知识转移",
    "French": "transfert de connaissances",
    "Japanese": "知識転移",
    "Russian": "передача знаний"
  },
  {
    "English": "l 1 -norm",
    "context": "1: Both these formulations do result in convex subproblems, for which efficient solvers exist, however this does not guarantee that global optimality is obtained for the original problem in the <mark>L 1 -norm</mark>. The excellent work by [6] also needs mentioning.<br>2: It was also shown here, that one can also solve the Huber-norm, an approximation of the <mark>L 1 -norm</mark>, in a similar fashion, with the difference that each subproblem now is a quadratic problem.<br>",
    "Arabic": "النورم L1",
    "Chinese": "L1范数",
    "French": "norme L1",
    "Japanese": "L1ノルム",
    "Russian": "л₁-норма"
  },
  {
    "English": "l 1 distance",
    "context": "1: In certain restricted situations, it is possible to efficiently compute the global minimum. If there are only two labels, [18] showed how to compute the global minimum of E. This is also possible for an arbitrary number of labels as long as the labels are consecutive integers and V is the <mark>L 1 distance</mark>.<br>",
    "Arabic": "مسافة L1",
    "Chinese": "L1距离",
    "French": "distance L1",
    "Japanese": "L1距離",
    "Russian": "L1 расстояние"
  },
  {
    "English": "l 2 -norm",
    "context": "1: The Wiberg algorithm is initially based on the observation that, for a fixed U in, (1)the <mark>L 2 -norm</mark> becomes a linear, least-squares minimization problem in V , \n min v ||W y − W (I n ⊗ U )v|| 2 2 ,(3) \n<br>",
    "Arabic": "مُعيار l2",
    "Chinese": "L2范数",
    "French": "norme L2",
    "Japanese": "L2ノルム",
    "Russian": "L2-норма"
  },
  {
    "English": "l 2 distance",
    "context": "1: If β > 1, the importance of large q i is reduced which gives more weights to the outliers. Special cases include the <mark>L 2 distance</mark> (i.e., β = 2) and KL divergence (i.e., β → 1). AB-Divergences.<br>2: • Temporal Coherence (TC) evaluates the temporal coherence of the tracks by measuring the <mark>L 2 distance</mark> between the acceleration of groundtruth tracks and predicted tracks. The acceleration is measured as the flow difference between two adjacent frames for visible points.<br>",
    "Arabic": "المسافة L 2",
    "Chinese": "L2距离",
    "French": "distance L2",
    "Japanese": "L2距離",
    "Russian": "расстояние L2"
  },
  {
    "English": "l 2 loss",
    "context": "1: Nemirovski [41] first noticed that, over classes of regression functions with inhomogeneous (i.e., spatially-varying) smoothness, many widely-used regression estimators, called \"linear\" estimators (defined precisely in Section 4.2), are provably unable to converge at the minimax optimal rate, in <mark>L 2 loss</mark>. Donoho et al.<br>",
    "Arabic": "خسارة إل ٢",
    "Chinese": "L2损失",
    "French": "perte L2",
    "Japanese": "L 2損失",
    "Russian": "потери L2"
  },
  {
    "English": "l 2 regularization",
    "context": "1: Here we show that truncation bias can also arise for regularization hyperparameters such as the <mark>L 2 regularization</mark> coefficient. We tune <mark>L 2 regularization</mark> for linear regression on the Yacht data from the UCI collection (Asuncion & Newman, 2007). We found the optimal L 2 coefficient using a fine-trained grid search.<br>2: We alternate between improving the model p ✓ (t|x) on even epochs and the variational distributions q (y|t), r (t), s ⇠ (t i |x i ) on odd epochs. We train for 50 epochs with minibatches of size 20 and <mark>L 2 regularization</mark>.<br>",
    "Arabic": "التنظيم L 2",
    "Chinese": "L 2 正则化",
    "French": "régularisation L2",
    "Japanese": "L2正則化",
    "Russian": "L 2 регуляризация"
  },
  {
    "English": "l ∞ norm",
    "context": "1: A simplified approach is numerically integrating µ over small intervals of equal size to generate a spectral histogram. The advantage is the error is now easily measured and visualized in the <mark>L ∞ norm</mark>. For example, Figure 1 shows the exact and approximated spectral histogram for the normalized adjacency matrix of an Internet topology.<br>2: In our approach the number of parameters enters in bounding the covering number of F in the rather strict L ∞ (R d ; R) norm, which seems difficult to control by other means.<br>",
    "Arabic": "المعيار اللامتناهي (L ∞)",
    "Chinese": "L∞范数",
    "French": "norme L ∞",
    "Japanese": "L ∞ ノルム",
    "Russian": "L ∞ норма"
  },
  {
    "English": "l1 bind",
    "context": "1: In the preprocess phase, we precompute γ in (3.10) for the L2 bound as described in Algorithm 8. Note that we compute α, β in (3.6), (3.7) for the <mark>L1 bound</mark> in query phase.<br>",
    "Arabic": "حد l1",
    "Chinese": "l1约束",
    "French": "liaison L1",
    "Japanese": "L1バインド",
    "Russian": "\"L1 связь\""
  },
  {
    "English": "l1 difference",
    "context": "1: Self-regularization in Feature Space: When the synthetic and real images have significant shift in the distribution, a pixel-wise <mark>L1 difference</mark> may be restrictive. In such cases, we can replace the identity map with an alternative feature transform. For example, in Figure 6, we use the mean of RGB channels for color image refinement.<br>",
    "Arabic": "الفارق L1",
    "Chinese": "L1差异",
    "French": "différence L1",
    "Japanese": "L1 差異",
    "Russian": "разница L1"
  },
  {
    "English": "l1 loss",
    "context": "1: We randomly crop the images in every minibatch so that the height and width are each uniformly distributed between 1024 and 2048 to support inference at any resolution and aspect ratio. To learn α w.r.t. ground-truth α * , we use an <mark>L1 loss</mark> over the whole alpha matte and its (Sobel) gradient: \n<br>2: For each patch, we add white Gaussian noise with noise level sampled from [1,50]. The denoising networks are trained with 50 epoch using L 1 loss and Adam optimizer (Kingma & Ba, 2014) with batch size 32.<br>",
    "Arabic": "الخسارة L1",
    "Chinese": "L1损失",
    "French": "perte L1",
    "Japanese": "L1損失",
    "Russian": "L1 потеря"
  },
  {
    "English": "l1 penalty",
    "context": "1: With an appropriate optimization method, an <mark>L1 penalty</mark> could also be used for learning with marginal inference on dense SPN architectures. However, sparsity is not as important for SPNs as it is for Markov random fields, where a non-zero weight can have outsize impact on inference time; with SPNs inference is always linear with respect to model size.<br>2: Note that in fused Lasso based approach, in addition to the standard L 1 penalty, an additional L 1 penalty on the difference between the neighboring frames for each dimensions is used. This tries to enforce the assumption that in a video sequence, neighboring frames are more related to one another as compared to frames that are farther apart.<br>",
    "Arabic": "عقوبة L1",
    "Chinese": "L1 惩罚",
    "French": "pénalité L1",
    "Japanese": "L1 ペナルティ",
    "Russian": "l1 штраф"
  },
  {
    "English": "l1 regularization",
    "context": "1: Without regularization, this algorithm is identical to standard online gradient descent, but because it uses an alternative lazy representation of the model coefficients w, <mark>L1 regularization</mark> can be implemented much more effectively. The FTRL-Proximal algorithm has previously been framed in a way that makes theoretical analysis convenient [24].<br>2: in terms of accuracy . One family of methods achieves sparsity in training via an implementation of <mark>L1 regularization</mark> that doesn't need to track any statistics for features with a coefficient of zero (e.g., [20]). This allows less informative features to be removed as training progresses.<br>",
    "Arabic": "تنظيم L1",
    "Chinese": "L1 正则化",
    "French": "régularisation L1",
    "Japanese": "L1 正則化",
    "Russian": "L1-регуляризация"
  },
  {
    "English": "l1 term",
    "context": "1: At the beginning of the training, the deviation of the predicted structure and ground truth is large and the <mark>L1 term</mark> makes the loss less sensitive to outliers than MSE loss. When the training is almost done, the deviation is small and the MSE loss provides smoothness near 0.<br>",
    "Arabic": "مصطلح L1",
    "Chinese": "L1项",
    "French": "terme L1",
    "Japanese": "L1項",
    "Russian": "L1 член"
  },
  {
    "English": "l2 error",
    "context": "1: ŝ t−k = f k (h t , c t ) + s t , k ∈ [1, 256]. Given ground truth location s t+k , we evaluate the decoder via relative <mark>L2 error</mark> ||ŝ t+k −s t+k ||/||s t+k −s t || (refer to Apx. A.4 for details).<br>",
    "Arabic": "خطأ المربعات",
    "Chinese": "L2误差",
    "French": "erreur L2",
    "Japanese": "L2誤差",
    "Russian": "ошибка l2"
  },
  {
    "English": "l2 regularisation",
    "context": "1: log p(x n ,C n |y) ∝ log p(C n |y) = T t K k C n tk logC n tk . (12) \n Regularisation: To prevent overfitting, we apply L1 and <mark>L2 regularisation</mark> to parameters as: \n<br>",
    "Arabic": "تنظيم إل2",
    "Chinese": "L2 正则化",
    "French": "régularisation L2",
    "Japanese": "L2正則化",
    "Russian": "l2 регуляризация"
  },
  {
    "English": "l2 regularizer",
    "context": "1: We tried network depths d ∈ {0, 1, 2, 3}. We sweep the coefficient for an L 2 regularizer on the neural network parameters.<br>2: Lasso imposes an L1 regularizer on β, while ridge regression imposes an <mark>L2 regularizer</mark> on β. Elastic net combines 2 After tuning on a validation set for one task, we fix alpha=1e− 5 and l1 ratio=0.5. both penalties: \n<br>",
    "Arabic": "منظم L2",
    "Chinese": "L2 正则化器",
    "French": "régulariseur L2",
    "Japanese": "L2正則化項",
    "Russian": "регуляризатор L2"
  },
  {
    "English": "l2 weight decay",
    "context": "1: For training, we use Adam with learning rate of 1e-4, β 1 =0.9, β 2 =0.999, <mark>L2 weight decay</mark> of 0.01, learning rate warmup over the first 10, 000 steps, and linear decay of the learning rate. We use dropout probability of 0.1 on all layers and training batch size of 256.<br>2: ) . Early stopping is used for all the training and Adam optimizer [24] is used to optimize all the parameters. To avoid overfitting, we use an <mark>L2 weight decay</mark> of 0.0005 and a dropout ratio of 0.5. The discounted factor γ of our cumulative reward is 0.95.<br>",
    "Arabic": "تحلل الوزن L2",
    "Chinese": "L2权重衰减",
    "French": "décroissance de poids l2",
    "Japanese": "L2 重み減衰",
    "Russian": "L2-регуляризация весов"
  },
  {
    "English": "l2-normalization",
    "context": "1: These plots intuitively demonstrate the benefit of <mark>L2-normalization</mark> and using OTS features rather than the highly-invariant logits. parametric models such as class centroids [40] and classconditional Gaussian models [35,27], non-parametric models such as NN [8,32], and mixture models such as (classconditional) GMMs and k-means [11].<br>",
    "Arabic": "التطبيع L2",
    "Chinese": "L2-归一化",
    "French": "normalisation L2",
    "Japanese": "L2正規化",
    "Russian": "L2-нормализация"
  },
  {
    "English": "label",
    "context": "1: Where it simplifies notation, we write s l − → s to denote a transition s, l, s from s to s with <mark>label</mark> l, and we may write s l − → s ∈ Θ for s l − → s ∈ T .<br>2: In other words, the overlap of a <mark>label</mark> is the number of other non-superset and non-subset <mark>label</mark>s you can additionally apply to the same object. We can now use the maximum overlap of a graph to bound the size of its state space: \n Theorem 3.<br>",
    "Arabic": "تسمية",
    "Chinese": "标签",
    "French": "étiquette",
    "Japanese": "ラベル",
    "Russian": "метка"
  },
  {
    "English": "label datum",
    "context": "1: Let F be a set consisting of all finite sequences, whose coordinates are <mark>labeled data</mark>, i.e., F = {S = (S 1 , ..., S i , ..., S m ) : ∀i = 1, ..., m and ∀ <mark>labeled data</mark> S i }.<br>2: Though effective, these methods are usually faced with two practical challenges: 1) lack of sufficient labeled noisy audio-visual training data in some real-world scenarios and 2) less optimal model generality to unseen testing noises.<br>",
    "Arabic": "البيانات المصنفة",
    "Chinese": "标注数据",
    "French": "donnée étiquetée",
    "Japanese": "ラベル付きデータ",
    "Russian": "маркированные данные"
  },
  {
    "English": "label distribution",
    "context": "1: Say we wish to compare two predictive families, V and U, such that U ⊆ V. Assuming both families can model the <mark>label distribution</mark>, the task will at least as easy for the larger family. This provably obviates the need to evaluate simpler function families (e.g., linear functions) when estimating dataset difficulty.<br>",
    "Arabic": "توزيع التسميات",
    "Chinese": "标签分布",
    "French": "distribution des étiquettes",
    "Japanese": "ラベル分布",
    "Russian": "распределение меток"
  },
  {
    "English": "label embedding",
    "context": "1: More recently, Chen et al. (2019b) proposed a top-k off-policy correction method for a real-world recommender system. Their approach deals with millions of actions although it treats <mark>label embeddings</mark> as given, whereas this problem is in general a hard problem for XMC.<br>2: • Label embeddings of the form ϕ(x i ) := P s (x i ), with x i ∈ X, and defined by ϕ(G, v) := (col G (v i )) s , are k-MPNNs; \n<br>",
    "Arabic": "تضمين الوسم",
    "Chinese": "标签嵌入",
    "French": "intégration d'étiquettes",
    "Japanese": "ラベル埋め込み",
    "Russian": "векторное представление меток"
  },
  {
    "English": "label example",
    "context": "1: We will show the following claim: at any stage of A, if the set of <mark>labeled examples</mark> L shown so far induces a version V = H[L], then A will subsequently query at most Cost(V ) more labels before exiting the while loop.<br>2: By inductive hypothesis, there exists some h ∈ H L ∪ (x, y) , such that when A interacts with h subsequently (with obtained <mark>labeled examples</mark> L ∪ (x, y) and label budget < n), the final unlabeled dataset S A,h satisfies \n<br>",
    "Arabic": "مثال موسوم",
    "Chinese": "标签示例",
    "French": "exemple étiqueté",
    "Japanese": "ラベル例",
    "Russian": "метка примера"
  },
  {
    "English": "label graph",
    "context": "1: We should point out that our complexity results can be readily extended to prove the #P-hardness of counting maximal frequent embedded subtrees [26] in a database of labeled trees. Moreover, we can also immediately derive the following corollary for <mark>labeled graphs</mark>.<br>2: Key to this efficiency is the observation that the Weisfeiler-Lehman test of isomorphism from graph theory elegantly computes a subtree kernel as a byproduct. Our fast subtree kernels can deal with <mark>labeled graphs</mark>, scale up easily to large graphs and outperform state-of-the-art graph kernels on several classification benchmark datasets in terms of accuracy and runtime.<br>",
    "Arabic": "الرسم البياني الموسوم",
    "Chinese": "标注图",
    "French": "graphe étiqueté",
    "Japanese": "ラベル付きグラフ",
    "Russian": "помеченный граф"
  },
  {
    "English": "label noise",
    "context": "1: We give many more empirical evidences to show that the variance (either from <mark>label noise</mark> or from the non-convex landscape) is usually not the cause for why ensemble works in deep learning, see Section 5.<br>2: Suppose a set of shuffled bags is generated from an MI dataset without <mark>label noise</mark> on the original bags. Then if each shuffled bag is of size s ≥ 1 C log 1 s , where \n C = log |X p | − log (|X p | − |B p |), 1 − s of \n<br>",
    "Arabic": "ضوضاء التصنيف",
    "Chinese": "标签噪声",
    "French": "bruit d'étiquette",
    "Japanese": "ラベルノイズ",
    "Russian": "шум метки"
  },
  {
    "English": "label sequence",
    "context": "1: p(y|x) = n i=1 p(y i |y 1 , y 2 , • • • , y i−1 , x) (1) \n An overview of our proposed model is shown in Figure 1. First, we sort the <mark>label sequence</mark> of each sample according to the frequency of the labels in the training set.<br>2: T is an observation sequence and y i = (y i 1 , y i 2 , ..., y i L ) ∈ L L is the corresponding <mark>label sequence</mark> (with L ≤ T ).<br>",
    "Arabic": "تسلسل التسمية",
    "Chinese": "标签序列",
    "French": "séquence d'étiquettes",
    "Japanese": "ラベルシーケンス",
    "Russian": "последовательность меток"
  },
  {
    "English": "label smoothing",
    "context": "1: The final training sets have 5.8 million, 1.9 million, and 207 thousand sentence pairs respectively. All models are transformers of the base model size from Vaswani et al. (2017) and are trained without <mark>label smoothing</mark> until convergence.<br>2: When we use multilingual data, the encoder is shared in the {Hindi, Nepali}-English direction, and the decoder is shared in the English-{Hindi, Nepali}direction. We regularize our models with dropout, <mark>label smoothing</mark> and weight decay, with the corresponding hyper-parameters tuned independently for each language pair.<br>",
    "Arabic": "التلطيخ بالتسمية",
    "Chinese": "标签平滑",
    "French": "lissage des étiquettes",
    "Japanese": "ラベル平滑化",
    "Russian": "сглаживание меток"
  },
  {
    "English": "label space",
    "context": "1: Moreover, PiCO consistently achieves superior results as the size of the candidate set increases, while the baselines demonstrate a significant performance drop. Besides, it is worth pointing out that previous works [18], [19] are typically evaluated on datasets with a small <mark>label space</mark> (C = 10).<br>2: t at time-step t. Here y t−1 is the predicted probability distribution over the <mark>label space</mark> L at time-step t − 1 . The function g takes y t−1 as input and produces the embedding vector which is then passed to the decoder. Finally, the masked softmax layer is used to output the probability distribution y t .<br>",
    "Arabic": "فضاء التصنيف",
    "Chinese": "标签空间",
    "French": "espace d'étiquettes",
    "Japanese": "ラベル空間",
    "Russian": "пространство меток"
  },
  {
    "English": "label token",
    "context": "1: Similar to the image encoder, the label encoder is applied to each support label independently, with a major difference that it sees one channel at a time since we treat each channel as an independent task as discussed in Section 2. Then the <mark>label tokens</mark> are extracted from multiple hierarchies that matches the image encoder.<br>2: Specifically, we use a randomly initialized ViT-B (Dosovitskiy et al., 2020) as label encoder g and extract features from 3, 6, 9, 12-th layers of the encoder to form multi-level label features (<mark>label tokens</mark>).<br>",
    "Arabic": "رمز التسمية",
    "Chinese": "标签令牌",
    "French": "jeton d'étiquette",
    "Japanese": "ラベルトークン",
    "Russian": "токен метки"
  },
  {
    "English": "label training datum",
    "context": "1: 1 We assume that absolutely no <mark>labeled training data</mark> is available for the foreign language of interest, but that we have access to parallel data with a resource-rich language.<br>2: A standard approach is to replace the pretrained model's output layer with a task-specific head and finetune the entire model on a set of <mark>labeled training data</mark>. However , language modeling is not only a powerful pretraining objective , but many tasks can be reformulated as cloze questions ( e.g. , by appending phrases such as `` the correct answer is __ '' ) , allowing pretrained LMs to solve them without any or with only very few labeled examples ( Radford et al. , 2019 ; Schick and Schütze ,<br>",
    "Arabic": "بيانات التدريب الموسومة",
    "Chinese": "标注训练数据",
    "French": "donnée d'entraînement étiquetée",
    "Japanese": "ラベル付きトレーニングデータ",
    "Russian": "метка обучающего набора данных"
  },
  {
    "English": "label vector",
    "context": "1: Assume we are given a collection of training images X i and labels Y i . We want to find a model w that, given a new image X i , tends to produce the true <mark>label vector</mark> Y * i = Y i . We formulate this as a regularized learning problem: \n<br>2: We write I for a particular set of instanced window-class pairs {(i, c)} and write Y (I ) for the associated <mark>label vector</mark> where y i = c for all pairs in I and y i = 0 otherwise.<br>",
    "Arabic": "متجه التسمية",
    "Chinese": "标签向量",
    "French": "vecteur d'étiquettes",
    "Japanese": "ラベルベクトル",
    "Russian": "вектор меток"
  },
  {
    "English": "lagrange multiplier",
    "context": "1: 2 2 : Initialize <mark>Lagrange multiplier</mark> α ← 1 3 : for k = 1 , 2 , . . . , K do 4: \n Sample minibatch D mini from dataset D.<br>2: Let α ij be the <mark>Lagrange multiplier</mark> for constraint a ij ≥ 0 and A = [a ij ], the Lagrange L is: \n L = J + Tr[αA T ], α = [α ij ]. The derivative of L with respect to A is: \n<br>",
    "Arabic": "مضاعف لاغرانج",
    "Chinese": "拉格朗日乘数",
    "French": "multiplicateur de Lagrange",
    "Japanese": "ラグランジュ乗数",
    "Russian": "множитель Лагранжа"
  },
  {
    "English": "lagrangian duality",
    "context": "1: AA = {v,w}∈E W {v,w} = L. \n Then, introducing Lagrange multipliers λ, we obtain through <mark>Lagrangian duality</mark> that Problem ( 53) is equivalent to: max λ∈R E×d −F * (Aλ), \n with F * the convex conjugate of F . Following the approach of Hendrikx et al.<br>",
    "Arabic": "الثنائية اللاغرانجية",
    "Chinese": "拉格朗日对偶",
    "French": "dualité lagrangienne",
    "Japanese": "ラグランジュ双対性",
    "Russian": "дуальность Лагранжа"
  },
  {
    "English": "lagrangian multiplier",
    "context": "1: The optimality condition of problem ( 12) is characterized as follows (adapted from Chapter 7 in Conn et al. (2000)) \n (H + λ * I)r * = −g, H + λ * I 0, r * = 1,(15) \n where λ * is the corresponding <mark>Lagrangian multiplier</mark>.<br>2: It is shown that there always exists an optimal solution r * and a unique <mark>Lagrangian multiplier</mark> λ * , because different λ * s yield different values for r * , contradicting r * = 1.<br>",
    "Arabic": "معامل لاغرانج",
    "Chinese": "拉格朗日乘数",
    "French": "multiplicateur de Lagrange",
    "Japanese": "ラグランジュの未定乗数",
    "Russian": "множитель Лагранжа"
  },
  {
    "English": "lagrangian relaxation",
    "context": "1: A dual-decomposition algorithm for models that satisfy the GSD assumption is shown in Figure 2. The algorithm can be justified as an instance of <mark>Lagrangian relaxation</mark> applied to the problem \n argmax z∈Z,y∈Y f (z) + h(y)(10) \n<br>2: For example, the activity cooking is over 33% more likely to involve females than males in a training set, and a trained model further amplifies the disparity to 68% at test time. We propose to inject corpus-level constraints for calibrating existing structured prediction models and design an algorithm based on <mark>Lagrangian relaxation</mark> for collective inference.<br>",
    "Arabic": "الاسترخاء اللاغرانجي",
    "Chinese": "拉格朗日松弛法",
    "French": "relaxation lagrangienne",
    "Japanese": "ラグランジュ緩和",
    "Russian": "Лагранжева релаксация"
  },
  {
    "English": "lambda calculus",
    "context": "1: To our best knowledge, this is the first probabilistic model for generating sentences from the <mark>lambda calculus</mark> encodings of their underlying formal meaning representations, that concerns both surface realization and lexical acquisition. We demonstrate the effectiveness of our model in Section 5.<br>2: The simply-typed <mark>lambda calculus</mark> logical form in the category represents its semantic meaning. The typing system includes basic types (e.g., entity e, truth value t) and functional types (e.g., e, t is the type of a function from e to t).<br>",
    "Arabic": "حساب لامبدا",
    "Chinese": "λ演算",
    "French": "calcul lambda",
    "Japanese": "ラムダ計算",
    "Russian": "лямбда-исчисление"
  },
  {
    "English": "lambertian reflectance",
    "context": "1: Due to the complex and often unknown nature of the bidirectional reflectance distribution function (BRDF) that determines material behavior, simplifying assumptions like brightness constancy and <mark>Lambertian reflectance</mark> are often employed. However, psychophysical studies have established that complex reflectance does not impede shape perception [16].<br>2: Shape from shading [25,71] assumes a shading model such as <mark>Lambertian reflectance</mark>, and reconstructs the surface by exploiting the non-uniform illumination. Category-specific reconstruction. Learning-based methods have recently been leveraged to reconstruct objects from a single view, either in the form of a raw image or 2D keypoints (see also Table 1).<br>",
    "Arabic": "انعكاس لامبرتي",
    "Chinese": "朗伯反射率",
    "French": "réflectance lambertienne",
    "Japanese": "ランベルト反射",
    "Russian": "ламбертово отражение"
  },
  {
    "English": "landmark",
    "context": "1: This includes the state equation heuristic (Bonet and van den Briel 2014), which was previously thought (Bonet 2013) to fall outside the four main categories of heuristics for classical planning: abstractions, <mark>landmarks</mark>, delete-relaxations and critical paths (Helmert and Domshlak 2009).<br>2: This is done by minimizing the distance between the rotated <mark>landmarks</mark> R T c P c ,m and the corresponding rays cast from the camera origin o c to the 2D joint detections: \n c m σ c,m (R T c P c ,m + t − o c ) × d c,m 2 , (2 \n<br>",
    "Arabic": "معلم",
    "Chinese": "关键点",
    "French": "repère",
    "Japanese": "特徴点",
    "Russian": "ориентир"
  },
  {
    "English": "landmark point",
    "context": "1: (1991); Mardia and Dryden (1998) is seminal in this context, using the notion of identifying <mark>landmark points</mark> to represent the shape of an outline in 2d, and then characterizing shape variability among a population of such outlines in terms of a shape-space distributions on the vector of landmarks.<br>",
    "Arabic": "نقطة معلمية",
    "Chinese": "关键点",
    "French": "point de repère",
    "Japanese": "ランドマークポイント",
    "Russian": "Опорная точка"
  },
  {
    "English": "language drift",
    "context": "1: In our experience, the best results for maximum subject fidelity are achieved by fine-tuning all layers of the model. This includes fine-tuning layers that are conditioned on the text embeddings, which gives rise to the problem of <mark>language drift</mark>.<br>2: We use a class descriptor in the sentence in order to tether the prior of the class to our unique subject and find that using a wrong class descriptor, or no class descriptor increases training time and <mark>language drift</mark> while decreasing performance.<br>",
    "Arabic": "الانجراف اللغوي",
    "Chinese": "语言漂移",
    "French": "déviation linguistique",
    "Japanese": "言語ドリフト",
    "Russian": "языковой дрейф"
  },
  {
    "English": "language encoder",
    "context": "1: It's important to note that the unseen words may not be completely new, e.g., the models may have encountered these words in its <mark>language encoder</mark> initialized with pre-trained language models. We consider them \"unseen\" because the model never sees these words paired with their referent, i.e., the grounded meanings of the words are unknown.<br>2: l task = L task (G(p τ1 , p τ2 , ..., p τ N train ), q),(4) \n We use a trainable word embedding layer as our <mark>language encoder</mark> F l to encode language tokens and add trainable position embeddings to encode positional information of the tokens.<br>",
    "Arabic": "مُشفر اللغة",
    "Chinese": "语言编码器",
    "French": "encodeur de langage",
    "Japanese": "言語エンコーダー",
    "Russian": "языковой кодировщик"
  },
  {
    "English": "language generation",
    "context": "1: Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010;Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly , graph-based semi-supervised learning ( Zhu , 2005 ; Talukdar and Pereira , 2010 ) has been applied to machine translation ( Alexandrescu and Kirchhoff , 2009 ) , unsupervised semantic role induction ( Lang and Lapata , 2011 ) , semantic document modeling ( Schuhmacher and Ponzetto , 2014 ) , <mark>language generation</mark> ( Krahmer et al. , 2003 ) and sentiment<br>2: Specifically, we are not aware of any prior work that handles both automatic unsupervised lexical acquisition and surface realization for generation from logical forms in a single framework. Another line of research efforts focused on the task of <mark>language generation</mark> from other meaning representation formalisms.<br>",
    "Arabic": "توليد اللغة",
    "Chinese": "语言生成",
    "French": "génération de langage",
    "Japanese": "言語生成",
    "Russian": "генерация языка"
  },
  {
    "English": "language generation model",
    "context": "1: We probe <mark>language generation models</mark> by conducting text generation based on the following prompt: \"Please respond to the following statement: [STATEMENT] \\n Your response:\". We then use an off-the-shelf stance detector  to determine whether the generated response agrees or disagrees with the given statement.<br>2: The goal of this work is to expose a possible inductive bias of beam search. We now exhibit our primary hypothesis Hypothesis 4.2. Beam search is a cognitively motivated search heuristic for decoding language gen-eration models.<br>",
    "Arabic": "نموذج توليد اللغة",
    "Chinese": "语言生成模型",
    "French": "modèle de génération de langage",
    "Japanese": "言語生成モデル",
    "Russian": "модель генерации языка"
  },
  {
    "English": "language identification",
    "context": "1: This representation disparity forms our definition of unfairness, and has been observed in face recognition (Grother et al., 2011), <mark>language identification</mark> (Blodgett et al., 2016; Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).<br>2: For example, analysis agents that scan each web page and recognize geographic locations, or proper names, or weights and measures, or indications that the page contains pornographic content, are all annotators. Similarly, analysis agents that perform complex tokenization, summarization, or <mark>language identification</mark>, or that automatically translate between languages, are also annotators.<br>",
    "Arabic": "تحديد اللغة",
    "Chinese": "语言识别",
    "French": "identification de la langue",
    "Japanese": "言語識別",
    "Russian": "определение языка"
  },
  {
    "English": "language model",
    "context": "1: We show results for different encoding sizes of the word in lower case: wsz = 15, 50 and 100. Results: <mark>Language Model</mark> Because the language model was trained on a huge database we first trained it alone. It takes about a week to train on one computer.<br>",
    "Arabic": "نموذج لغة",
    "Chinese": "语言模型",
    "French": "modèle de langage",
    "Japanese": "言語モデル",
    "Russian": "языковая модель"
  },
  {
    "English": "language model pre-training",
    "context": "1: Language model pre-training has been shown to be effective for improving many natural language processing tasks (Dai and Le, 2015;Peters et al., 2018a;Radford et al., 2018;Howard and Ruder, 2018). These include sentence-level tasks such as natural language inference ( Bowman et al. , 2015 ; Williams et al. , 2018 ) and paraphrasing ( Dolan and Brockett , 2005 ) , which aim to predict the relationships between sentences by analyzing them holistically , as well as token-level tasks such as named entity recognition and question answering , where models are required to<br>2: Recent advances in natural language processing ( NLP ) through deep learning have been largely enabled by vector representations ( or embeddings ) learned through <mark>language model pre-training</mark> ( Bengio et al. , 2003 ; Mikolov et al. , 2013 ; Pennington et al. , 2014 ; Bojanowski et al. , 2017 ; Peters et al. , 2018 ; Devlin et al. , 2019<br>",
    "Arabic": "التدريب المسبق لنماذج اللغة",
    "Chinese": "语言模型预训练",
    "French": "pré-entraînement des modèles de langage",
    "Japanese": "言語モデルの事前学習",
    "Russian": "предобучение языковой модели"
  },
  {
    "English": "language modeling toolkit",
    "context": "1: The decoder is implemented in Python, an interpreted language, with C++ code from the SRI <mark>Language Modeling Toolkit</mark> (Stolcke, 2002). Using the settings described above, on a 2.4 GHz Pentium IV, it takes about 20 seconds to translate each sentence (average length about 30).<br>",
    "Arabic": "مجموعة أدوات نمذجة اللغة",
    "Chinese": "语言建模工具包",
    "French": "boîte à outils de modélisation linguistique",
    "Japanese": "言語モデリングツールキット",
    "Russian": "инструментарий языкового моделирования"
  },
  {
    "English": "language pair",
    "context": "1: Given that the sample size is small to begin with (typically 10-15 MT systems per <mark>language pair</mark>), we believe that we do not have enough data to use this method to assess whether metric reliability decreases with the quality of MT systems.<br>2: (2021) suggested learning a binary mask for every model parameter and every <mark>language pair</mark>, both requiring further training after the base multilingual model converges. Li and Gong (2021) used per language gradients geometry to rescale gradients of different <mark>language pair</mark> to improve performance on low resource languages.<br>",
    "Arabic": "زوج لغوي",
    "Chinese": "语言对",
    "French": "paire de langues",
    "Japanese": "言語ペア",
    "Russian": "языковая пара"
  },
  {
    "English": "language representation",
    "context": "1: There is a long history of pre-training general <mark>language representations</mark>, and we briefly review the most widely-used approaches in this section.<br>",
    "Arabic": "تمثيل اللغة",
    "Chinese": "语言表征",
    "French": "représentation linguistique",
    "Japanese": "言語表現",
    "Russian": "языковое представление"
  },
  {
    "English": "language transfer",
    "context": "1: data Within the area of low-resource translation , describe the development of translation systems for low-resource languages without using any parallel data at all , relying instead on crawled monolingual data and <mark>language transfer</mark> . Methods which don't require parallel data are likely complementary to the seed data approach proposed in this paper.<br>2: We observe limited and inconsistent gains only in zero-shot downstream <mark>language transfer</mark>: further analyses reveal that (1) intermediate LM training yields comparable gains and (2) IPT only marginally changes representation spaces of transformers exposed to sufficient amount of language data in LM-pretraining.<br>",
    "Arabic": "انتقال اللغة",
    "Chinese": "语言迁移",
    "French": "transfert de langue",
    "Japanese": "言語転移",
    "Russian": "языковой перенос"
  },
  {
    "English": "language understanding",
    "context": "1: We propose to leverage implicit user feedback to reduce the need for manual annotation and thereby scale <mark>language understanding</mark> in dialog systems to more long-tail utterances.<br>2: It allows us to evaluate whether current speaker commitment models achieve robust <mark>language understanding</mark>, by analyzing their performance on specific challenging linguistic constructions.<br>",
    "Arabic": "فهم اللغة",
    "Chinese": "语言理解",
    "French": "compréhension du langage",
    "Japanese": "言語理解",
    "Russian": "понимание языка"
  },
  {
    "English": "laplace approximation",
    "context": "1: Recently, the <mark>Laplace approximation</mark> (LA) and its use in marginal likelihood model selection has quickly regained popularity in Bayesian deep learning (Kirkpatrick et al., 2017;Ritter et al., 2018;Daxberger et al., 2021;Immer et al., 2021Immer et al., , 2022a. Notably, Immer et al.<br>2: We follow the standard approach for GP classification and logistic Gaussian processes and use the <mark>Laplace approximation</mark> [36,27]. The <mark>Laplace approximation</mark> gives an approximate posterior distribution for f , from which we can calculate a posterior distribution over the ki of Eq. ( 14) as explained in detail in [26,Section 3.4.2].<br>",
    "Arabic": "تقريب لابلاس",
    "Chinese": "拉普拉斯近似",
    "French": "approximation de Laplace",
    "Japanese": "ラプラス近似",
    "Russian": "приближение Лапласа"
  },
  {
    "English": "laplace distribution",
    "context": "1: This allows the network to attenuate the cost of difficult regions and to focus on reconstructing parts which can be well explained. At test time, the learned uncertainties can also serve to gauge the reliability of the reconstruction. In the present work we employ a <mark>Laplace distribution</mark> which has heavier tails than the traditional Gaussian distribution: \n<br>",
    "Arabic": "توزيع لابلاس",
    "Chinese": "拉普拉斯分布",
    "French": "distribution de Laplace",
    "Japanese": "ラプラス分布",
    "Russian": "распределение Лапласа"
  },
  {
    "English": "laplace noise",
    "context": "1: The base algorithm Q(x) will simply pick an index j ∈ [m] uniformly at random and the privately estimate u(x, j) by adding Laplace or Gaussian noise ξ and output the pair (j, u(x, j) + ξ).<br>2: The total order on the output space [m] × R simply selects for the highest estimated utility (breaking ties arbitrarily). If we take ξ to be <mark>Laplace noise</mark> with scale 1/ε, then Q is (ε, 0)-DP.<br>",
    "Arabic": "ضجيج لابلاس",
    "Chinese": "拉普拉斯噪声",
    "French": "bruit de Laplace",
    "Japanese": "ラプラスノイズ",
    "Russian": "шум Лапласа"
  },
  {
    "English": "laplacian distribution",
    "context": "1: The loss can be interpreted as the negative log-likelihood of a factorized <mark>Laplacian distribution</mark> on the reconstruction residuals. Optimizing likelihood causes the model to selfcalibrate, learning a meaningful confidence map [32].<br>",
    "Arabic": "توزيع لابلاسي",
    "Chinese": "拉普拉斯分布",
    "French": "distribution laplacienne",
    "Japanese": "ラプラス分布",
    "Russian": "распределение Лапласа"
  },
  {
    "English": "laplacian matrix",
    "context": "1: Ω RL ℓ ε τ γ(W ) + RL ℓ ε 2 . (20 \n ) \n The proof of Theorem 3 relies on linear graphs (whose diameter is proportional to 1/ γ(L) where L is the <mark>Laplacian matrix</mark>) and Theorem 2.<br>2: Ae { v , w } = P { v , w } ( e v − e w ) . Matrix A then satisfies AA = L the <mark>Laplacian matrix</mark> of graph G with weights P {v,w} .<br>",
    "Arabic": "مصفوفة لابلاسية",
    "Chinese": "拉普拉斯矩阵",
    "French": "matrice laplacienne",
    "Japanese": "ラプラシアン行列",
    "Russian": "матрица Лапласа"
  },
  {
    "English": "laplacian smoothing",
    "context": "1: One set is composed of 100 most frequent such queries, while the second set contain frequent queries that followed the target query in query logs candidate query is then scored by multiplying its smoothed frequency by its smoothed frequency of following the target query in past search sessions, using <mark>Laplacian smoothing</mark>.<br>",
    "Arabic": "التنعيم اللابلاسياني",
    "Chinese": "拉普拉斯平滑",
    "French": "lissage laplacien",
    "Japanese": "ラプラシアン平滑化",
    "Russian": "сглаживание Лапласа"
  },
  {
    "English": "large language model",
    "context": "1: Since this setting allows a system to search for and condition on text which potentially contains the answer it is denoted \"open-book\". [RRS20] recently demonstrated that a <mark>large language model</mark> can perform surprisingly well directly answering the questions without conditioning on auxilliary information. They denote this more restrictive evaluation setting as \"closed-book\".<br>2: The energy cost of a <mark>large language model</mark> is amortized through its usage for inference an fine-tuning. The benefits of a more optimally trained smaller model, therefore, extend beyond the immediate benefits of its improved performance. Table 1 | Current LLMs.<br>",
    "Arabic": "نموذج لغة كبير",
    "Chinese": "大型语言模型",
    "French": "modèle de langage de grande taille",
    "Japanese": "大規模言語モデル",
    "Russian": "большая языковая модель"
  },
  {
    "English": "large-margin learning",
    "context": "1: To learn these structures we used online <mark>large-margin learning</mark> (McDonald et al., 2005) that empirically provides state-of-the-art performance for Czech. A major advantage of our models is the ability to naturally model non-projective parses. Nonprojective parsing is commonly considered more difficult than projective parsing.<br>2: We evaluate these methods on the Prague Dependency Treebank using online <mark>large-margin learning</mark> techniques McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies.<br>",
    "Arabic": "التعلم بهامش كبير",
    "Chinese": "大边界学习",
    "French": "apprentissage à grande marge",
    "Japanese": "大マージン学習",
    "Russian": "обучение с большим зазором"
  },
  {
    "English": "latent code",
    "context": "1: Prompt-to-prompt [23] allows for local and global editing without an input mask. These methods fall short of identitypreserving novel sample generation of a subject. In the context of GANs, Pivotal Tuning [57] allows for real image editing by finetuning the model with an inverted <mark>latent code</mark> anchor, and Nitzan et al.<br>2: At inference time, a random noise <mark>latent code</mark> goes through the backward diffusion process and the pre-trained decoder is used to generate the final image. Our method can be naturally applied to this scenario, where the U-Net (and possibly the text encoder) are trained, and the decoder is fixed.<br>",
    "Arabic": "الرمز الكامن",
    "Chinese": "潜在编码",
    "French": "code latent",
    "Japanese": "潜在コード",
    "Russian": "Латентный код"
  },
  {
    "English": "latent dimension",
    "context": "1: That is, we compute the SVD of the ratings matrix \n R = W ΣV T \n where W is n users × f matrix and V is n item × f matrix for some appropriately chosen rank f (which is also called the <mark>latent dimension</mark>).<br>2: First, we estimate the variance of each <mark>latent dimension</mark> by embedding 10 000 random samples from the data set and we exclude collapsed dimensions with variance smaller than 0.05. Second, we generate the votes for the majority vote classifier by sampling a batch of 64 points, all with a factor fixed to the same random value.<br>",
    "Arabic": "البعد الكامن",
    "Chinese": "隐藏维度",
    "French": "dimension latente",
    "Japanese": "潜在次元",
    "Russian": "латентное измерение"
  },
  {
    "English": "latent dirichlet allocation",
    "context": "1: The unsupervised <mark>Latent Dirichlet Allocation</mark> topic model (Blei et al., 2003) which can capture topics of words and documents was extended by Felt et al. (2015) to be able to handle crowdsourced noisy labels.<br>2: When z is a vector of topics associated with individual words, this leads to <mark>Latent Dirichlet Allocation</mark> [3]. Likewise, whenever z indicates a term in a hierarchy, it leads to structured and mixed-content annotations [19,2,4,12]. with the data.<br>",
    "Arabic": "تخصيص ديريتشليت الكامنة",
    "Chinese": "隐狄利克雷分布",
    "French": "allocation latente de Dirichlet",
    "Japanese": "潜在ディリクレ配分",
    "Russian": "латентное распределение Дирихле"
  },
  {
    "English": "latent distribution",
    "context": "1: The objective is to learn, for each d n , a <mark>latent distribution</mark> over Z topics {P(z|d n )} Z z=1 . Each topic z is associated with a parameter θ z , which is a probability distribution {P(w|θ z )} w∈W over words in the vocabulary W .<br>",
    "Arabic": "توزيع كامن",
    "Chinese": "隐分布",
    "French": "distribution latente",
    "Japanese": "潜在分布",
    "Russian": "латентное распределение"
  },
  {
    "English": "latent dynamic model",
    "context": "1: The results presented in this paper are the new state-of-theart in this popular domain. Watter, M., Springenberg, J. T., Boedecker, J., and Riedmiller, M. A. Embed to control: A locally linear <mark>latent dynamics model</mark> for control from raw images. In NIPS, 2015.<br>2: We achieve this by training a multi-headed <mark>latent dynamics model</mark> with the following (Fig. 4 ) : ( 1 ) An encoder q ( z t |M ≤t , a ≤t−1 ) compressing high-dimensional segmented images M t to compressed latent states z t , ( 2 ) A transition function over the latent states p ( z τ |z τ −1 , h k τ −1 ) with which to imagine rollouts , and ( 3 )<br>",
    "Arabic": "نموذج ديناميكي كامن",
    "Chinese": "潜在动态模型",
    "French": "modèle dynamique latent",
    "Japanese": "潜在動的モデル",
    "Russian": "латентная динамическая модель"
  },
  {
    "English": "latent embedding",
    "context": "1: Baselines For each method we evaluate the ability to represent the shape space of training object instances by turning it into an autodecoder [4] which, in an encoder-less manner, learns a separate <mark>latent embedding</mark> z scene (sequence ID ) for each train scene as a free training parameter.<br>2: A crucial part of a category-centric reconstructor is the <mark>latent embedding</mark> z. Early methods [18,70,58,61,40] predicted a global scene encoding z global = Φ CNN (I src ) with a deep convolutional network Φ CNN that solely analyzed the colors of source image pixels.<br>",
    "Arabic": "التضمين الكامن",
    "Chinese": "隐式嵌入",
    "French": "plongement latent",
    "Japanese": "潜在埋め込み",
    "Russian": "латентное вложение"
  },
  {
    "English": "latent factor",
    "context": "1: A popular generalization of this framework, which combines neighborhood information with <mark>latent factor</mark> approach [18], leads to the following model: \n r i,j = µ + b i + b j + u T i v j (3 \n ) \n<br>2: For EFM 2 , as in the original work, the <mark>latent factor</mark> and explicit factor dimensions are 60 and 40. For MTER, we adopt the default setting of the author's implementation 3 . It is not our intention to compare these two, as our model works with any compatible base recommendation method. Evaluation Metrics.<br>",
    "Arabic": "عامل كامن",
    "Chinese": "潜在因子",
    "French": "facteur latent",
    "Japanese": "潜在因子",
    "Russian": "скрытый фактор"
  },
  {
    "English": "latent feature",
    "context": "1: Note that u i is a k dimensional <mark>latent feature</mark> for the i-th subject. In a Bayesian framework, we assign a Gaussian prior over U, p(U) = i N (u i |0, I), and specify the rest of the model (see Figure 1) as follows. Continuous data distribution.<br>",
    "Arabic": "سمة كامنة",
    "Chinese": "潜在特征",
    "French": "caractéristique latente",
    "Japanese": "潜在特徴",
    "Russian": "латентная характеристика"
  },
  {
    "English": "latent function",
    "context": "1: From these we can approximate the predictive distribution via a mixture of softmax functions. We might also be interested in the class-conditional predictive distributions. These K distributions are the ones that arise on data space, conditioning on membership in class k, but integrating out the <mark>latent function</mark> and hyperparameters.<br>2: A sigmoid function π(•) is imposed to squash the output of the <mark>latent function</mark> into [0, 1], π(f i ) = p(y i = 1|f i ). Assuming the data set is i.i.d, then the joint likelihood factorizes to \n<br>",
    "Arabic": "الدالة الكامنة",
    "Chinese": "潜在函数",
    "French": "fonction latente",
    "Japanese": "潜在関数",
    "Russian": "латентная функция"
  },
  {
    "English": "latent group",
    "context": "1: We denote the expected loss as the risk R(θ) = E Z∼P [ (θ; Z)]. The observations Z are assumed to arise from one of K <mark>latent groups</mark> such that Z ∼ P := k∈[K] α k P k .<br>",
    "Arabic": "المجموعات الكامنة",
    "Chinese": "潜在群体",
    "French": "groupe latent",
    "Japanese": "潜在グループ",
    "Russian": "скрытая группа"
  },
  {
    "English": "latent parameter",
    "context": "1: A better way is a joint approach that builds both reductions into a single, consistent whole that produces topic distributions and visualization coordinates simultaneously. The joint approach was attempted by PLSV (Iwata, Yamada, and Ueda 2008), which derives the <mark>latent parameters</mark> by maximizing the likelihood of observing the documents.<br>2: L(Ψ|D, Ω) = L(Ψ|D) − λ 2 • R(Ψ|Ω)(4) \n The first component L is the log-likelihood function in Equation 2 , which reflects the global consistency between the <mark>latent parameters</mark> Ψ and the observation D. The second component R is a regularization function , which reflects the local consistency between the <mark>latent parameters</mark> Ψ of neighboring documents in the manifold Ω. λ is the regularization parameter , commonly found in manifold learning algorithms<br>",
    "Arabic": "المعامل الكامن",
    "Chinese": "隐含参数",
    "French": "paramètre latent",
    "Japanese": "潜在パラメータ",
    "Russian": "латентный параметр"
  },
  {
    "English": "latent representation",
    "context": "1: Specifically in the generative story, the rule probability π r is estimated by the model g with a <mark>latent representation</mark> z for each sentence σ, which is in turn drawn from a prior p(z): \n π r = g r (z; θ), z ∼ p(z). (1 \n ) \n<br>2: In fact, {c i } C i=1 can be regarded as a codebook generated by our model. For any un-seen pair of face images, we also first compute its joint feature vector x * for each pair of patches, and estimate its <mark>latent representation</mark> z * . Then we compute its first-order and second-order statistics to the centers.<br>",
    "Arabic": "التمثيل الكامن",
    "Chinese": "潜在表征",
    "French": "représentation latente",
    "Japanese": "潜在表現",
    "Russian": "латентное представление"
  },
  {
    "English": "latent reward function",
    "context": "1: Once the <mark>latent reward function</mark> describing the explicit values of various state and action pairs, and optimal policy defining the general (nonsurjective and non-injective) mapping from states to actions are inferred, implicit causal relationships encoded within the data can be extrapolated for subsequent predictive and mechanistic modeling tasks.<br>",
    "Arabic": "وظيفة المكافأة الكامنة",
    "Chinese": "潜在奖励函数",
    "French": "fonction de récompense latente",
    "Japanese": "潜在報酬関数",
    "Russian": "скрытая функция вознаграждения"
  },
  {
    "English": "latent semantic",
    "context": "1: We view this work as a preliminary step towards predictive theories of <mark>latent semantics</mark>, beyond purely descriptive models. Despite ample practical evidence that interventions such as stoplist curation can have significant effects, most previous work has focused on algorithms for identifying a single \"optimal\" low-dimensional semantic representation.<br>",
    "Arabic": "الدلالات الكامنة",
    "Chinese": "潜在语义",
    "French": "sémantique latente",
    "Japanese": "潜在的意味",
    "Russian": "латентная семантика"
  },
  {
    "English": "latent semantic analysis",
    "context": "1: We show that it enables to predict two behaviorbased measures across a range of parameters in a <mark>Latent Semantic Analysis</mark> model.<br>2: Despite being rather simple, ICSI produces strong and still close to state-of-the-art summaries (Hong et al., 2014). Different but similar words may refer to the same topic and should not be counted separately. This observation gave rise to a set of important techniques based on topic models (Allahyari et al., 2017). These approaches cover sentence clustering ( McKeown et al. , 1999 ; Radev et al. , 2000 ; Zhang et al. , 2015 ) , lexical chains ( Barzilay and Elhadad , 1999 ) , <mark>Latent Semantic Analysis</mark> ( Deerwester et al. , 1990 ) or Latent Dirichlet Allocation ( Blei et al. , 2003 ) adapted to summarization ( Hachey et al. ,<br>",
    "Arabic": "التحليل الكامن للدلالات",
    "Chinese": "潜在语义分析",
    "French": "analyse sémantique latente",
    "Japanese": "潜在意味解析",
    "Russian": "Латентный семантический анализ"
  },
  {
    "English": "latent space",
    "context": "1: This is true in terms of established quality metrics, and we further believe that our investigations to the separation of high-level attributes and stochastic effects, as well as the linearity of the intermediate <mark>latent space</mark> will prove fruitful in improving the understanding and controllability of GAN synthesis.<br>2: If W is indeed a disentangled and \"flattened\" mapping of Z, it may contain regions that are not on the input manifold -and are thus badly reconstructed by the generator -even between points that are mapped from the input manifold, whereas the input <mark>latent space</mark> Z has no such regions by definition.<br>",
    "Arabic": "الفضاء الكامن",
    "Chinese": "潜在空间",
    "French": "espace latent",
    "Japanese": "潜在空間",
    "Russian": "скрытое пространство"
  },
  {
    "English": "latent state",
    "context": "1: For state dimension N and sequence length L, computing the <mark>latent state</mark> requires O(N 2 L) operations and O(N L) space -compared to a Ω(L + N ) lower bound for both. Thus for reasonably sized models (e.g. N = 256 in Gu et al.<br>2: The state space model is defined by the simple equation (1). It maps a 1-D input signal u(t) to an N -D <mark>latent state</mark> x(t) before projecting to a 1-D output signal y(t).<br>",
    "Arabic": "حالة كامنة",
    "Chinese": "潜在状态",
    "French": "état latent",
    "Japanese": "潜在状態",
    "Russian": "скрытое состояние"
  },
  {
    "English": "latent topic",
    "context": "1: The number of <mark>latent topics</mark>, K, is a free parameter in each of the models; here we explore this with K = 50, 100 and 150. The remaining parameters -β k , the topic multinomial distribution for topic k; and θ d , the topic mixture proportions for document d -are inferred from data.<br>2: Probabilistic topic models have become popular tools for the unsupervised analysis of large document collections [1]. These models posit a set of <mark>latent topics</mark>, multinomial distributions over words, and assume that each document can be described as a mixture of these topics.<br>",
    "Arabic": "موضوع كامن",
    "Chinese": "隐含主题",
    "French": "sujet latent",
    "Japanese": "潜在トピック",
    "Russian": "латентная тема"
  },
  {
    "English": "latent variable",
    "context": "1: VAEs are models with a joint density p(y, x) = p(y|x)p(x), where x denotes the <mark>latent variable</mark>. x is assigned a uniform factorized Bernoulli prior. The likelihood p θ (y|x) is parameterized by the output of a neural network with x as input and parameters θ.<br>2: Now we want to recover Q = P (y|do(x)) in Fig. 3(c) (U is a <mark>latent variable</mark>) with T = {W 2 }. Condition (iii) of the s-backdoor fails since (S ⊥ ⊥ Y |{X, W 2 }) does not hold.<br>",
    "Arabic": "المتغير الكامن",
    "Chinese": "潜在变量",
    "French": "variable latente",
    "Japanese": "潜在変数",
    "Russian": "скрытая переменная"
  },
  {
    "English": "latent variable model",
    "context": "1: The generative procedure did not require integrating an infinite-dimensional random function, nor did it require knowledge of g(s) or λ(s) at more than a finite number of locations. By considering the procedure as a <mark>latent variable model</mark>, we inherit these convenient properties for inference.<br>",
    "Arabic": "نموذج المتغير الكامن",
    "Chinese": "潜变量模型",
    "French": "modèle de variable latente",
    "Japanese": "潜在変数モデル",
    "Russian": "модель скрытых переменных"
  },
  {
    "English": "latent vector",
    "context": "1: For each item j, draw item <mark>latent vector</mark> vj ∼ N (0, λ −1 v IK ). 3. For each user-item pair (i, j), draw the response \n rij ∼ N (u T i vj, c −1 ij ),(3) \n where cij is the precision parameter for rij.<br>2: In matrix factorization, we represent users and items in a shared latent low-dimensional space of dimension K-user i is represented by a <mark>latent vector</mark> ui ∈ R K and item j by a <mark>latent vector</mark> vj ∈ R K . We form the prediction of whether user i will like item j with the inner product between their latent representations, \n<br>",
    "Arabic": "المتجه الكامن",
    "Chinese": "隐向量",
    "French": "vecteur latent",
    "Japanese": "潜在ベクトル",
    "Russian": "латентный вектор"
  },
  {
    "English": "layer",
    "context": "1: We can tune the prompt by additively perturbing each v ( ) i by a small vector ∆ ( ) i before it is used in further computations. The ∆ vectors for a given hard prompt are initialized to 0 and then tuned. Perturbing only <mark>layer</mark> 0 is equivalent to tuning v i directly as in §3.1.<br>2: is the parameter matrix in the ( +1)-th <mark>layer</mark>, where ( ) and ( +1) refer to the dimensionality of the interference representations in the -th and ( +1)-th <mark>layer</mark>s, respectively. Modeling interference with different significance.<br>",
    "Arabic": "طبقة",
    "Chinese": "层",
    "French": "couche",
    "Japanese": "レイヤー",
    "Russian": "слой"
  },
  {
    "English": "layer activation",
    "context": "1: As can be seen, the model can discriminate between phonemes with high accuracy across all the layers, and the <mark>layer activations</mark> are more informative for this task than the MFCC features.<br>",
    "Arabic": "تنشيط الطبقة",
    "Chinese": "层激活",
    "French": "activation des couches",
    "Japanese": "層活性化",
    "Russian": "активации слоя"
  },
  {
    "English": "layer normalization",
    "context": "1: There have been existing methods, such as <mark>Layer Normalization</mark> (LN) [3] and Instance Normalization (IN) [61] (Figure 2), that also avoid normalizing along the batch dimension. These methods are effective for training sequential models (RNN/LSTM [49,22]) or generative models (GANs [15,27]).<br>2: of Large and further adds more complexity by having DCN-v2 layers [31] on inputs, followed by a standard <mark>Layer Normalization</mark> [4].<br>",
    "Arabic": "تنميط الطبقات",
    "Chinese": "层归一化",
    "French": "normalisation des couches",
    "Japanese": "層正規化",
    "Russian": "нормализация слоя"
  },
  {
    "English": "layer-wise learning rate decay",
    "context": "1: We use a <mark>layer-wise learning rate decay</mark> [5] (ld) of 0.8. No data augmentation is applied. We initialize SAM from an MAE [47] pre-trained ViT-H. We distribute training across 256 GPUs, due to the large image encoder and 1024×1024 input size.<br>",
    "Arabic": "انحسار معدل التعلم طبقة بطبقة",
    "Chinese": "逐层学习率衰减",
    "French": "décroissance du taux d'apprentissage par couche",
    "Japanese": "層ごとの学習率減衰",
    "Russian": "убывание скорости обучения по слоям"
  },
  {
    "English": "lazy grounding",
    "context": "1: Lazy grounding turns out to be very efficient in solving grounding bottleneck problem but obtain bad search performance. In order to improve the performance of lazy-grounding approach, the concept of laziness has been relaxed and different strategies have been proposed [25]. Recently, a different solution was proposed in order to solve this issue.<br>2: Albeit <mark>lazy grounding</mark> techniques obtained good preliminary results, their performance is still not competitive with state-of-the-art systems [17]. Lazy grounding has been also extended to support aggregates [5]. To the best of my knowledge, this normalization strategies is limited to monotone aggregates with a lower bound.<br>",
    "Arabic": "التأريض البطيء",
    "Chinese": "惰性实例化",
    "French": "instanciation paresseuse",
    "Japanese": "遅延グラウンディング",
    "Russian": "ленивое обоснование"
  },
  {
    "English": "leaf node",
    "context": "1: Each document is associated with one <mark>leaf node</mark> with a deterministic routing path l = {r 0 , r 1 , ..., r m } from the root, where r i ∈ [0, k) represents the internal cluster index for level i, and r m ∈ [0, c) is the <mark>leaf node</mark>.<br>2: f (v, I) = ⎧ ⎨ ⎩f (v, I), if v is a <mark>leaf node</mark> v ∈Z(v)f (v , I), if v is an internal node (3) \n Where Z ( v ) is the set of all <mark>leaf node</mark>s under node v and f ( v , I ) is the output of a Platt-scaled decision value from a linear SVM trained for the category corresponding to input <mark>leaf node</mark> v. Each linear SVM is trained on sift features with locally-constrained linear coding and spatial pooling on a regular 3x3 grid<br>",
    "Arabic": "العقدة الورقية",
    "Chinese": "叶子节点",
    "French": "nœud feuille",
    "Japanese": "葉ノード",
    "Russian": "листовой узел"
  },
  {
    "English": "learn agent",
    "context": "1: [33] find that human-provided rewards tend to depend on a <mark>learning agent</mark>'s entire policy, rather than just the current state. Further, work by Hadfield-Menell et al. [15] and Kumar et al.<br>2: representation of the game and instead looks for merely <mark>learning agent</mark> strategies that will perform well [ Letchford et al. , 2009 ; Vorobeychik et al. , 2007 ; Fearnley et al. , 2015 ] .<br>",
    "Arabic": "تعلم الوكيل",
    "Chinese": "学习智能体",
    "French": "agent apprenant",
    "Japanese": "学習エージェント",
    "Russian": "обучающий агент"
  },
  {
    "English": "learn algorithm",
    "context": "1: Support vector machines (SVM) have been used in this work as the <mark>learning algorithm</mark> given that this was the best option when studying a slightly different discrimination problem involving drugs with or without antibacterial activity [5]. SVMs are very well-known, flexible and robust learning models that allow for very good classification and generalization [8,3].<br>2: Consequently, our <mark>learning algorithm</mark> ingests a number of single-view images of a deformable object category and produces as output a deep network that can estimate the 3D shape of any instance given a single image of it (Fig. 1).<br>",
    "Arabic": "خوارزمية التعلم",
    "Chinese": "学习算法",
    "French": "algorithme d'apprentissage",
    "Japanese": "学習アルゴリズム",
    "Russian": "алгоритм обучения"
  },
  {
    "English": "learn method",
    "context": "1: In this paper, we have demonstrated that query chains can be used to extract useful information from search engine log files. After presenting an algorithm to infer preference judgments from log files, we showed that the preferences judgments are valid, independent of the <mark>learning method</mark>.<br>2: We discuss related work in Section 2 and introduce the problem setting in Section 3. In Section 4, we derive a <mark>learning method</mark> starting from a relaxed clustering variant. In Section 5, we exploit the temporal nature of the data and devise a sequential clustering algorithm with an appropriate learning variant.<br>",
    "Arabic": "طريقة التعلم",
    "Chinese": "学习方法",
    "French": "méthode d'apprentissage",
    "Japanese": "学習方法",
    "Russian": "метод обучения"
  },
  {
    "English": "learn model",
    "context": "1: These utility weights are then used to mimic the behavior in similar situations through a decision-making algorithm. Unlike the other two communities, it is the predictive performance of the <mark>learned model</mark> that is most pivotal and noisy observations are expected and managed by those techniques.<br>2: Our general framework can work with different types of parsers and executable semantic representations. In future work, the subgraph selection decisions could be made by a <mark>learned model</mark> that considers the cost and benefit of each call, instead of using a fixed threshold. The parser could also condition on the execution status, instead of operating separately.<br>",
    "Arabic": "نموذج متعلم",
    "Chinese": "学习模型",
    "French": "modèle appris",
    "Japanese": "モデルを学習する",
    "Russian": "модель обучения"
  },
  {
    "English": "learn paradigm",
    "context": "1: We achieve this isolation by judiciously designing the agent's perceptual system and the <mark>learning paradigm</mark> such that these alternative mechanisms are rendered implausible. Our agents are effectively 'blind'; they possess a minimal perceptual system capable of sensing only egomotion, i.e.<br>2: We then give a learning algorithm under a reasonable <mark>learning paradigm</mark>, together with a self contained proof in elementary terms (not presupposing any extensive knowledge of lattice theory), of the correctness of this algorithm.<br>",
    "Arabic": "نموذج التعلم",
    "Chinese": "学习范式",
    "French": "paradigme d'apprentissage",
    "Japanese": "学習パラダイム",
    "Russian": "парадигма обучения"
  },
  {
    "English": "learn problem",
    "context": "1: And while no individual member of the family is necessary for boostability, we also show that the entire family taken together is necessary in the sense that for every boostable <mark>learning problem</mark>, there exists one member of the family that is satisfied.<br>2: This shows, that in the worst case the hardness of our <mark>learning problem</mark> is not simply a result of the hardness of discovering good outputs.<br>",
    "Arabic": "مشكلة التعلم",
    "Chinese": "学习问题",
    "French": "problème d'apprentissage",
    "Japanese": "学習問題",
    "Russian": "проблема обучения"
  },
  {
    "English": "learn representation",
    "context": "1: A change in a single underlying factor of variation z i should lead to a change in a single factor in the <mark>learned representation</mark> r(x). This assumption can be extended to groups of factors as, for instance, in Bouchacourt et al. (2018) or Suter et al. (2018).<br>2: We evaluate TREBA trained using different decoder losses on supervised behavior classification. The procedure is the same as described in the main paper. We evaluate performance given both our <mark>learned representation</mark> and one of either (1) raw keypoints or (2) domain-specific features designed by experts.<br>",
    "Arabic": "تعلم التمثيل",
    "Chinese": "学习表示",
    "French": "représentation apprise",
    "Japanese": "表現を学習する",
    "Russian": "обученное представление"
  },
  {
    "English": "learn-to-rank algorithm",
    "context": "1: Search engines using <mark>learning-to-rank algorithms</mark> could consider the nature of the terminology in captions associated with clicks and downweight clicks that appear to be driven by known biases, e.g., those associated with health anxiety.<br>2: In other words, we show that <mark>learning-to-rank algorithms</mark> that define their objective through a convex surrogate loss cannot provably optimize any of these evaluation metrics.<br>",
    "Arabic": "خوارزمية تعلم الترتيب",
    "Chinese": "学习排序算法",
    "French": "algorithme d'apprentissage du classement",
    "Japanese": "順位付け学習アルゴリズム",
    "Russian": "алгоритм обучения ранжированию"
  },
  {
    "English": "learnability",
    "context": "1: Then, according to the definition of <mark>learnability</mark> of OOD detection, we have an algorithm A and a monotonically decreasing sequence cons (n) → 0, as n → +∞, such that for any α ∈ [0, 1), \n E S∼D n X I Y I R D α ( A ( S ) ) ≤ inf h∈H R D α ( h ) + cons ( n ) , ( by the property of priori-unknown space ) where R D α ( A ( S ) ) = X ×Y all ( A ( S ) ( x ) , y )<br>2: f j − g 1 ≤ 3 min i∈[M ] f i − g 1 + 4ε, with probability at least 1 − δ/3. The proof of the following theorem appears in the full version [2]. Theorem 3.5 (compressibility implies <mark>learnability</mark>) Suppose F admits (τ, t, m) compression.<br>",
    "Arabic": "قابلية التعلم",
    "Chinese": "可学习性",
    "French": "apprenabilité",
    "Japanese": "学習可能性",
    "Russian": "обучаемость"
  },
  {
    "English": "learnable parameter",
    "context": "1: Let w HA be a <mark>learnable parameter</mark> of HA-Net which predicts relations in the test time. We investigate the effect of the disagreement penalty by comparing the gradients of loss functions with respect to w HA for a human annotated label and a distantly supervised label.<br>2: where the complexity d n is computed using Equation ( 1) and W e is the learnable encoder parameter.<br>",
    "Arabic": "المعلمة القابلة للتعلم",
    "Chinese": "可学习参数",
    "French": "paramètre apprenable",
    "Japanese": "学習可能なパラメータ",
    "Russian": "обучаемый параметр"
  },
  {
    "English": "learnable vector",
    "context": "1: The function ϕ 1 and ϕ 2 can simply be parameterized by two <mark>learnable vectors</mark> v 1 and v 2 , so that ϕ 1 (D SPD uv ) is a learnable scalar corresponding to v 1 D SPD uv (and similarly for ϕ 2 ).<br>",
    "Arabic": "متجهات قابلة للتعلم",
    "Chinese": "可学习向量",
    "French": "vecteur apprenable",
    "Japanese": "学習可能なベクトル",
    "Russian": "обучаемый вектор"
  },
  {
    "English": "learner",
    "context": "1: The regret [Hazan, 2016] at step t is the cumulative difference between the loss incurred by the <mark>learner</mark> with its time varying parameters β i , i = 1, 2, . . . , t, and the loss when using the optimum parameters in hindsight β * t : \n<br>2: Intuitively, the minimizing player can be interpreted as a <mark>learner</mark> who proposes candidate solutions while the maximizing player can be interpreted as an auditor who tries to pick a data distribution and loss function for which the <mark>learner</mark>'s hypothesis performs poorly.<br>",
    "Arabic": "المتعلم",
    "Chinese": "学习者",
    "French": "apprenant",
    "Japanese": "学習者",
    "Russian": "обучающийся"
  },
  {
    "English": "learning rate",
    "context": "1: Relevant hyperparameters for training the dual encoder are: \n • <mark>Learning Rate</mark>: 1e-3 For the T5 input we concatenated the query and truncated document text. The T5 output is the string \"relevant\" or \"not relevant\".<br>",
    "Arabic": "معدل التعلم",
    "Chinese": "学习率",
    "French": "taux d'apprentissage",
    "Japanese": "学習率",
    "Russian": "Коэффициент обучения"
  },
  {
    "English": "learning rate decay",
    "context": "1: One key assumption is made on the cosine cycle length and the corresponding learning rate drop (we use a 10× <mark>learning rate decay</mark> in line with Rae et al. (2021)).<br>2: and 3x schedule ( 36 epochs with the <mark>learning rate decay</mark>ed by 10× at epochs 27 and 33 ) . For system-level comparison , we adopt an improved HTC [ 9 ] ( denoted as HTC++ ) with instaboost [ 22 ] , stronger multi-scale training [ 7 ] ( resizing the input such that the shorter side is between 400 and 1400 while the longer side is at most 1600 ) , 6x schedule ( 72 epochs with the <mark>learning rate decay</mark>ed at<br>",
    "Arabic": "معدل تناقص التعلم",
    "Chinese": "学习率衰减",
    "French": "diminution du taux d'apprentissage",
    "Japanese": "学習率減衰",
    "Russian": "убывание скорости обучения"
  },
  {
    "English": "learning rate decay schedule",
    "context": "1: Next, we add in the <mark>learning rate decay schedule</mark> hand-engineered by He et al. (2016): the step size is divided by 10 at epochs 100 and 150. We compare this with a hyperoptimizer initialized with the same starting hyperparameters, training both variants for 500 epochs. Our results are shown in Figure 2b.<br>2: The hyperoptimizer not only matches the final test loss of the hand-engineered <mark>learning rate decay schedule</mark>, but also learns a decay schedule strikingly similar to one hand-engineered by He et al. Of course, both networks significantly outperform the baseline trained with a fixed step size.<br>",
    "Arabic": "جدول تناقص معدل التعلم",
    "Chinese": "学习率衰减时间表",
    "French": "Programme de décroissance du taux d'apprentissage",
    "Japanese": "学習率減衰スケジュール",
    "Russian": "график снижения скорости обучения"
  },
  {
    "English": "learning rate schedule",
    "context": "1: On the other hand, if we look at this as a single learning problem, the standard <mark>learning rate schedule</mark> ηt = 1 √ t is applied to all coordinates: that is, we decrease the learning rate for coin i even when it is not being flipped. This is clearly not the optimal behavior.<br>2: (2021), Chinchilla stored a higher-precision copy of the weights in the sharded optimiser state. We show comparisons of models trained with Adam and AdamW in Figure A6 and Figure A7. We find that, independent of the <mark>learning rate schedule</mark>, AdamW trained models outperform models trained with Adam.<br>",
    "Arabic": "معدل التعلم الجدول الزمني",
    "Chinese": "学习率计划",
    "French": "taux d'apprentissage planifié",
    "Japanese": "学習率スケジュール",
    "Russian": "график скорости обучения"
  },
  {
    "English": "learning rate scheduler",
    "context": "1: The <mark>learning rate scheduler</mark> we use is identical to T5, employing an inverse square root function, with the warm-up steps set to 10,000. We set the GSG-ratio α = 0.2 and do not employ dropout during this phase. We follow closely the same pre-training as LongT5 (Guo et al., 2022).<br>2: 6 Note that the model of Wu and Cotterell has 8.66M parameters, 17% more than the transformer model. To get an apples-to-apples comparison, we apply the same <mark>learning rate scheduler</mark> to Wu and Cotterell; this does not yield similar improvements and underperforms with respect to the traditional <mark>learning rate scheduler</mark>.<br>",
    "Arabic": "جدولة معدل التعلم",
    "Chinese": "学习率调度器",
    "French": "programmateur de taux d'apprentissage",
    "Japanese": "学習率スケジューラ",
    "Russian": "планировщик скорости обучения"
  },
  {
    "English": "learning rate warmup",
    "context": "1: We set a <mark>learning rate warmup</mark> over the first 10% stepsexcept on arXiv-Lay where it is set to 10k consistently with , and use a square root decay of the learning rate. All our experiments have been run on four Nvidia V100 with 32GB each.<br>2: [14] confirmed that this prediction is still accurate enough. In terms of techniques, there are some methods widely used in language and vision models, such as activation clipping [20], gradient clipping [24], <mark>learning rate warmup</mark> [16], and various normalization techniques [4,18].<br>",
    "Arabic": "معدل التعلم المبدئي",
    "Chinese": "学习率热身",
    "French": "montée en puissance du taux d'apprentissage",
    "Japanese": "学習率ウォームアップ",
    "Russian": "скорость обучения разогрева"
  },
  {
    "English": "least square",
    "context": "1: The best-fit plane was computed by fitting a plane to the reconstructed depths at those pixels using <mark>least squares</mark>.<br>2: Note that step (i) corresponds to regression with L 1 constraints and step (ii) corresponds to <mark>least squares</mark> with L 2 constraints. In this section we show how marginal regression could be used to obtain better codes faster (step (i)).<br>",
    "Arabic": "مربعات الصغرى",
    "Chinese": "最小二乘",
    "French": "moindres carrés",
    "Japanese": "最小二乗法",
    "Russian": "метод наименьших квадратов"
  },
  {
    "English": "least square criterion",
    "context": "1: In this article, we choose to adopt the more general probabilistic formulation, where the <mark>least squares criterion</mark> is a special case of the log-likelihood.<br>",
    "Arabic": "معيار المربع الأصغر",
    "Chinese": "最小二乘准则",
    "French": "critère des moindres carrés",
    "Japanese": "最小二乗基準",
    "Russian": "критерий наименьших квадратов"
  },
  {
    "English": "least square minimization",
    "context": "1: For each pair of temporally corresponding transformations T i and T i in sequences S and S , we first compute their eigenvalues eig(T i ) and eig(T i ). The scale factor s i which relates them is then estimated from Eq. (4) using <mark>least squares minimization</mark> (three equations, one unknown).<br>",
    "Arabic": "التقليل من المربعات الصغرى",
    "Chinese": "最小二乘法",
    "French": "moindres carrés",
    "Japanese": "最小二乗法",
    "Russian": "минимизация методом наименьших квадратов"
  },
  {
    "English": "least square problem",
    "context": "1: Consider the general problem of solving a linear system: \n Ay = b (9) \n Where A is an invertible square matrix, and y and b are vectors. We can solve forŷ as a simple <mark>least squares problem</mark>: \n y = A −1 b (10) \n<br>2: In this set of experiments, we trained regressors and classifiers by solving the <mark>least squares problem</mark> min w Z w − y 2 2 + λ w 2 2 , where y denotes the vector of desired outputs and Z denotes the matrix of random features.<br>",
    "Arabic": "مشكلة المربعات الصغرى",
    "Chinese": "最小二乘问题",
    "French": "problème des moindres carrés",
    "Japanese": "最小二乗問題",
    "Russian": "проблема наименьших квадратов"
  },
  {
    "English": "least square regression",
    "context": "1: , d}), over-parameterized regime for <mark>least squares regression</mark> [52], function interpolation and gossip algorithms [8]. For a symmetric non-negative matrix A and a vector x, we denote x \n 2 A = x Ax. Let H = E[aa ] \n be the Hessian of f .<br>2: In the first set of experiments, summarized in Table 1, we show that <mark>least squares regression</mark> on our random features is a fast way to approximate the training of supervised kernel machines.<br>",
    "Arabic": "انحدار المربعات الصغرى",
    "Chinese": "最小二乘回归",
    "French": "régression des moindres carrés",
    "Japanese": "最小二乗回帰",
    "Russian": "регрессия методом наименьших квадратов"
  },
  {
    "English": "least square solution",
    "context": "1: Marginal regression proceeds as follows: \n • Calculate the <mark>least squares solution</mark>α (j) = x T j y. • Threshold the least-square coefficientsβ (j) =α (j) 1 {|α (j) |>t} , j = 1, . . . , p. \n<br>2: If the regularization parameter λ = 0, the problem in (3) decouples and the <mark>least squares solution</mark> α \n ( k ) j is α ( k ) j = R ( k ) j , X ( k ) j , for ∀j , k. ( 4 ) Since R ( k ) j , X ( k ) j = Y ( k ) , X ( k ) j − =j β ( k ) X ( k ) ,<br>",
    "Arabic": "حل المربعات الصغرى",
    "Chinese": "最小二乘解",
    "French": "solution des moindres carrés",
    "Japanese": "最小二乗解",
    "Russian": "решение методом наименьших квадратов"
  },
  {
    "English": "leave-one-out",
    "context": "1: Other approaches rank the features accordingly to their variances or accordingly to their contribution to the entropy calculated on a <mark>leave-one-out</mark> basis [15,3]. Unlike most of the filter approaches, wrapper methods evaluate feature subsets and not simple features. These approaches perform better since the evaluation is based on the exploratory analysis method employed for data analysis.<br>2: As we have seen in Section 2, there is a long history of designing CVs for REINFORCE estimators using \"baselines\" [7,37,45,64]. Recent progress is mostly driven by <mark>leave-one-out</mark> [30,38,48,49] and sample-dependent baselines [20,22,43,60,62].<br>",
    "Arabic": "ترك واحد خارج",
    "Chinese": "留一法",
    "French": "validation croisée simple",
    "Japanese": "1つ抜き法",
    "Russian": "исключая один элемент"
  },
  {
    "English": "left-to-right model",
    "context": "1: In particular, we can reverse the translation direction of the languages, as well as the direction of the language model. We denote our original formulation as a sourceto-target, <mark>left-to-right model</mark> (S2T/L2R). We can train three variations using target-to-source (T2S) and right-to-left (R2L) models: \n S2T/R2L Π |T | i=1 P ( t i |t i+1 , t i+2 , • • • , s a i , s a i −1 , s a i +1 , • • • ) T2S/L2R Π |S| i=1 P ( s i |s i−1 , s i−2 , • • • , t a i , t a i −1 ,<br>",
    "Arabic": "نموذج من اليسار إلى اليمين",
    "Chinese": "从左到右模型",
    "French": "modèle de gauche à droite",
    "Japanese": "左から右のモデル",
    "Russian": "модель слева направо"
  },
  {
    "English": "lemmatization",
    "context": "1: We experimented with different schemes for each language, measuring their success at removing gender bias of inanimate nouns with respect to English. 8 For German, we found <mark>lemmatization</mark> to work better than gender change. In Italian gender change got better results.<br>2: If we select a consistent lemma for each word type, and end up selecting a different lemma for each of \"usato\" and \"usata\", we again leak signal regarding the original gender. One solution would be to use context-sensitive <mark>lemmatization</mark>, that chooses the correct analysis in context.<br>",
    "Arabic": "تقنين الكلمات",
    "Chinese": "词形还原",
    "French": "lemmatisation",
    "Japanese": "単語の基本形への変換",
    "Russian": "лемматизация"
  },
  {
    "English": "length normalization",
    "context": "1: Although Okapi satisfies some constraints conditionally, unlike in the pivoted normalization method, the conditions do not provide any bound for the parameter b. Therefore, the performance of Okapi can be expected to be less sensitive to the <mark>length normalization</mark> parameter than the pivoted normalization method, which is confirmed by our experiments.<br>2: regularization, the optional <mark>length normalization</mark> of embeddings, the resulting difficulty of the optimization problem...), and explain the variations observed in our experiments, this shows that typical downstream systems are able to adjust the similarity order themselves.<br>",
    "Arabic": "تطبيع الطول",
    "Chinese": "长度归一化",
    "French": "normalisation de la longueur",
    "Japanese": "長さ正規化",
    "Russian": "нормализация длины"
  },
  {
    "English": "length penalty",
    "context": "1: We use the default setup of K = 40 for all three tasks, η = 0.6 for both Zh→En and En→De while η = 0.45 for En→Fr. For evaluation, the beam size and <mark>length penalty</mark> are set to 4 and 0.6 for the En→De as well as En→Fr, while 5 and 1.0 for the Zh→En task.<br>2: Following ; , we set the number of beams to 8 for Pegasus-based models, and 5 for BigBird-Pegasus-based models. For the non-English datasets, we set it to 5 for all models, for fair comparison. For all experiments, we use a <mark>length penalty</mark> of 0.8. For more implementation details, see Section B.1 in the Appendix.<br>",
    "Arabic": "عقوبة الطول",
    "Chinese": "长度惩罚",
    "French": "pénalité de longueur",
    "Japanese": "長さペナルティ",
    "Russian": "штраф за длину"
  },
  {
    "English": "lexeme",
    "context": "1: In this example, the word is derived from the root word (i.e., <mark>lexeme</mark>) \"maymun\" (monkey) by getting the affix \"cHk\" with a valid state transition but its one sense's meaning shifts to an entirely different space. It is a very challenging problem for compositional DSMs. Vecchi et al.<br>2: Following Artzi and Zettlemoyer (2013b), we constrain the set of derivations to include only those that use at most one <mark>lexeme</mark> from G gen . If generating new <mark>lexeme</mark>s is sufficient to derive z from x, D + will contain these derivations and we return their lexical entries to be added to the lexicon Λ (lines 5-7).<br>",
    "Arabic": "وحدة معجمية",
    "Chinese": "词元",
    "French": "lexème",
    "Japanese": "語彙素",
    "Russian": "лексема"
  },
  {
    "English": "lexical acquisition",
    "context": "1: Specifically, we are not aware of any prior work that handles both automatic unsupervised <mark>lexical acquisition</mark> and surface realization for generation from logical forms in a single framework. Another line of research efforts focused on the task of language generation from other meaning representation formalisms.<br>2: Many early approaches to generation from logical forms make use of rule-based methods (Wang, 1980;Shieber et al., 1990), which concern surface realization (ordering and inflecting of words) but largely ignore <mark>lexical acquisition</mark>.<br>",
    "Arabic": "اكتساب معجمي",
    "Chinese": "词汇获取",
    "French": "acquisition lexicale",
    "Japanese": "語彙獲得",
    "Russian": "лексическое приобретение"
  },
  {
    "English": "lexical acquisition algorithm",
    "context": "1: To see why the current <mark>lexical acquisition algorithm</mark> can be problematic, consider the word alignment in Figure 5 (for the sentence pair in Figure 1(b)). No rules can be extracted for the state predicate, because the shortest NL substring that covers the word states and the argument string Texas, i.e.<br>",
    "Arabic": "خوارزمية اكتساب المفردات",
    "Chinese": "词汇获取算法",
    "French": "algorithme d'acquisition lexicale",
    "Japanese": "語彙獲得アルゴリズム",
    "Russian": "алгоритм лексического приобретения"
  },
  {
    "English": "lexical ambiguity",
    "context": "1: (2017) put forward ContraWSD, a dataset which includes 7,200 instances of <mark>lexical ambiguity</mark> for German → English, and 6,700 for German → French. This dataset pairs every reference translation with a set of contrastive examples which contain incorrect translations of a polysemous target word.<br>2: This benchmark allows the community not only to better explore the described phenomena, but also to devise innovative MT systems which better deal with <mark>lexical ambiguity</mark>. Specifically, the contributions of the present work are threefold: \n<br>",
    "Arabic": "الغموض المعجمي",
    "Chinese": "词汇歧义",
    "French": "ambiguïté lexicale",
    "Japanese": "語彙的曖昧さ",
    "Russian": "лексическая многозначность"
  },
  {
    "English": "lexical entry",
    "context": "1: and lexicon Λ. LEX ( d ) is the set of <mark>lexical entries</mark> used in the derivation d. COMPUTEGRAD ( x , z , θ , Λ ) computes the gradient for sentence x and logical form z , given the parameters θ and lexicon Λ , and it described in Section 6.2 .<br>2: 1 The drawback is that the complexity in syntactic processing is coupled with semantic parsing and makes the latter even harder. For example, when applying their approach to a different domain with somewhat less rigid syntax, Zettlemoyer and Collins (2007) need to introduce new combinators and new forms of candidate <mark>lexical entries</mark>.<br>",
    "Arabic": "مدخل لغوي",
    "Chinese": "词条",
    "French": "entrée lexicale",
    "Japanese": "語彙項目",
    "Russian": "лексическая запись"
  },
  {
    "English": "lexical exposure",
    "context": "1: Were this to happen, a misalignment in the evaluation between pretrained and nonpretrained models would contribute to variation in the concurrence values, where the performance of pretrained models is overestimated due to <mark>lexical exposure</mark> in pretraining. To test for possible effects of <mark>lexical exposure</mark> , we extend the experiment from -who conducted it for COGS -to the TMCD and Std split of GeoQueory , and the TurnLeft split of SCAN 6 In both cases , we swap out lexical items with strings of similar length that act as `` wug words '' ( Berko , 1958 ) , or ,<br>",
    "Arabic": "تعرّض لغوي",
    "Chinese": "词汇暴露",
    "French": "exposition lexicale",
    "Japanese": "語彙的露出",
    "Russian": "лексическая экспозиция"
  },
  {
    "English": "lexical feature",
    "context": "1: • We could use <mark>lexical features</mark>, which fire if a certain lexical relationship (f, e) occurs: \n h(f J 1 , e I 1 ) =   J j=1 δ(f, f j )   • I i=1 δ(e, e i ) \n<br>2: An interesting perspective, where a kind of contextual information is studied, is presented in (Mukherjee and Bhattacharyya, 2012): the sentiment detection of tweets is here modeled according to <mark>lexical features</mark> as well as discourse relations like the presence of connectives, conditionals and semantic operators like modals and negations.<br>",
    "Arabic": "الميزة المعجمية",
    "Chinese": "词汇特征",
    "French": "caractéristique lexicale",
    "Japanese": "語彙的特徴",
    "Russian": "лексическая характеристика"
  },
  {
    "English": "lexical functional grammar",
    "context": "1: On the constructive side, we have demonstrated that a relatively modest modification of some assumptions of <mark>Lexical Functional Grammar</mark> (LFG; Bresnan 1982, Dalrymple 2001, Bresnan et al.<br>",
    "Arabic": "النحو الوظيفي المعجمي",
    "Chinese": "词汇功能语法",
    "French": "grammaire lexicale fonctionnelle",
    "Japanese": "語彙機能文法",
    "Russian": "лексико-функциональная грамматика"
  },
  {
    "English": "lexical head",
    "context": "1: We have taken the immediate-head parser described in [3] as our starting point. This parsing model assigns a probability to a parse by a topdown process of considering each constituent c in and , for each c , first guessing the pre-terminal of c , t ( c ) ( t for `` tag '' ) , then the <mark>lexical head</mark> of c , h ( c ) , and then the expansion of c into further<br>",
    "Arabic": "رأس معجمي",
    "Chinese": "词头",
    "French": "tête lexicale",
    "Japanese": "語彙的主要部",
    "Russian": "лексическая вершина"
  },
  {
    "English": "lexical item",
    "context": "1: , given the same training . This may be attributed to implicit tradeoffs in many categorial frameworks that minimize the number of composition operations at the expense of large numbers of possible categories for each <mark>lexical item</mark>, which may lead to sparse data effects in training.<br>2: COGS is a synthetic benchmark for compositional generalization introduced by Kim and Linzen  (2020). Models are tested for 21 different cases of generalization, 18 of which focus on using a <mark>lexical item</mark> in new contexts (Lex). There are 1000 instances per generalization case. Seq2seq models struggle in particular with the structural generalization tasks ( Yao and Koller , 2022 ) , and we focus on those : ( i ) generalization to deeper PP recursion than seen during training ( `` Emma saw a hedgehog on a chair in the garden beside the tree ... '' ) , ( ii ) deeper CP recursion ( `` Olivia<br>",
    "Arabic": "مدخل لغوي",
    "Chinese": "词汇项",
    "French": "unité lexicale",
    "Japanese": "語彙項目",
    "Russian": "лексическая единица"
  },
  {
    "English": "lexical knowledge",
    "context": "1: Thus, we use <mark>lexical knowledge</mark> to concatenate characters capturing the local composition and a global relay node to capture long-range dependency. We convert the sentence to a directed graph (as shown in Fig.<br>",
    "Arabic": "المعرفة المعجمية",
    "Chinese": "词汇知识",
    "French": "connaissances lexicales",
    "Japanese": "語彙知識",
    "Russian": "лексические знания"
  },
  {
    "English": "lexical model",
    "context": "1: The effectiveness gap between the best and the worst model of a group can be substantial on some corpora (e.g., <mark>lexical models</mark> on Args.me: 0.14 vs. 0.57), while being negligible on others (e.g., <mark>lexical models</mark> on NFCorpus).<br>",
    "Arabic": "نموذج معجمي",
    "Chinese": "词汇模型",
    "French": "modèle lexical",
    "Japanese": "語彙モデル",
    "Russian": "лексическая модель"
  },
  {
    "English": "lexical overlap",
    "context": "1: Appendix D investigates whether these similarity effects can be described in terms of <mark>lexical overlap</mark> or matches in low-level syntactic features between the prefix and test content; we find no clear relationship between these low-level features and models' acceptability judgment performance.<br>2: Up to now, the most successful approaches have used fairly shallow semantic representations, relying on measures of lexical or semantic overlap (Jijkoun and de Rijke, 2005), pattern-based relation extraction (Romano et al., 2006), or approximate matching of predicate-argument structure (Hickl et al., 2006).<br>",
    "Arabic": "التداخل المعجمي",
    "Chinese": "词汇重叠",
    "French": "chevauchement lexical",
    "Japanese": "語彙の重複",
    "Russian": "лексическое перекрытие"
  },
  {
    "English": "lexicalization",
    "context": "1: These statistics lead us to a straightforward, but important, conclusion: only in a limited number of cases is a <mark>lexicalization</mark> belonging to a given synset to be considered as a suitable translation equivalent for the provided target word and its context.<br>2: Rather, we have shown ways to improve parsing, some easier than <mark>lexicalization</mark>, and others of which are orthogonal to it, and could presumably be used to benefit lexicalized parsers as well.<br>",
    "Arabic": "المعجمية",
    "Chinese": "词汇化",
    "French": "lexicalisation",
    "Japanese": "語彙化",
    "Russian": "лексикализация"
  },
  {
    "English": "lexicalized grammar",
    "context": "1: In addition to presenting the algorithm, we show experiments in which we extract k-best lists for three different kinds of grammars: the <mark>lexicalized grammars</mark> of Klein and Manning (2003b), the state-split grammars of Petrov et al. (2006), and the tree transducer grammars of Galley et al. (2006).<br>",
    "Arabic": "القواعد المعجمية",
    "Chinese": "词汇化语法",
    "French": "grammaire lexicalisée",
    "Japanese": "語彙化された文法",
    "Russian": "лексикализованная грамматика"
  },
  {
    "English": "lexicalized parsing model",
    "context": "1: We also experimented with the <mark>lexicalized parsing model</mark> described in Klein and Manning (2003b). This model is constructed as the product of a dependency model and the unlexicalized PCFG model in Klein and Manning (2003a). We<br>",
    "Arabic": "نموذج التحليل المعجمي",
    "Chinese": "词汇化解析模型",
    "French": "modèle d'analyse syntaxique lexicalisé",
    "Japanese": "語彙化構文解析モデル",
    "Russian": "лексикализованная модель синтаксического анализа"
  },
  {
    "English": "lexicon",
    "context": "1: where {r τ k } assigns different types to color words, colors, and remaining tokens. The homomorphic transformations of this <mark>lexicon</mark> exchange color words and colors but preserve mixing relations.<br>2: The main learning algorithm (Algorithm 1) starts by initializing the <mark>lexicon</mark> (line 1) and then Algorithm 1 The main learning algorithm.<br>",
    "Arabic": "معجم",
    "Chinese": "词汇表",
    "French": "lexique",
    "Japanese": "語彙",
    "Russian": "лексикон"
  },
  {
    "English": "lexicon induction",
    "context": "1: In both tables, we also report the impact of using a simple copy mechanism instead of the more complex <mark>lexicon induction</mark> mechanism (-Lex). Our model outperforms all other non-treebased models by a considerable margin. Structural generalization without trees. All previous methods that obtain high accuracy on recursion generalization on COGS use trees.<br>2: For modules that accept a vector parameter, we associate these parameters with words rather than semantic tokens, and thus turn the combinatorial optimization problem associated with <mark>lexicon induction</mark> into a continuous one.<br>",
    "Arabic": "تحريض المعجم",
    "Chinese": "词汇诱导",
    "French": "induction du lexique",
    "Japanese": "語彙誘導",
    "Russian": "индукция лексикона"
  },
  {
    "English": "lie algebra",
    "context": "1: When d = 3, the <mark>Lie algebra</mark> can be represented as vectors (three numbers specify a 3 × 3 antisymmetric matrix) with the ×-product as Lie bracket. In general, the antisymmetric matrix A captures the infinitesimal tendency of ξ to rotate at each point in the parameter space.<br>2: In claiming A is analogous to curl, we are simply dropping the Hodge-star operator. Finally, recall that the <mark>Lie algebra</mark> of infinitesimal rotations in d-dimensions is given by antisymmetric matrices.<br>",
    "Arabic": "جبر لي",
    "Chinese": "李代数",
    "French": "algèbre de Lie",
    "Japanese": "リー代数",
    "Russian": "алгебра Ли"
  },
  {
    "English": "lifelong learning",
    "context": "1: Our algorithm, Task Descriptors for <mark>Lifelong Learning</mark> (TaDeLL), encodes the task descriptions as feature vectors that identify each task, treating these descriptors as side information in addition to training data on the individual tasks. This idea of using task features for knowledge transfer has been explored previously by Bonilla et al.<br>",
    "Arabic": "تعلم مدى الحياة",
    "Chinese": "终身学习",
    "French": "apprentissage tout au long de la vie",
    "Japanese": "生涯学習",
    "Russian": "\"обучение на протяжении всей жизни\""
  },
  {
    "English": "light field",
    "context": "1: View synthesis and image-based rendering Given a dense sampling of views, photorealistic novel views can be reconstructed by simple <mark>light field</mark> sample interpolation techniques [21,5,7]. For novel view synthesis with sparser view sampling, the computer vision and graphics communities have made significant progress by predicting traditional geometry and appearance representations from observed images.<br>",
    "Arabic": "مجال الضوء",
    "Chinese": "光场",
    "French": "champ lumineux",
    "Japanese": "光場",
    "Russian": "световое поле"
  },
  {
    "English": "light field interpolation",
    "context": "1: When images of the scene are captured densely, <mark>light field interpolation</mark> techniques [9,14,22] can be used to render novel views without reconstructing an intermediate representation of the scene. Issues related to sampling and aliasing have been thoroughly studied within this setting [7].<br>",
    "Arabic": "تقنيات تداخل حقل الضوء",
    "Chinese": "光场插值",
    "French": "interpolation de champ lumineux",
    "Japanese": "光場補間",
    "Russian": "интерполяция светового поля"
  },
  {
    "English": "likelihood function",
    "context": "1: It remains to specify the prior p(W ) and the <mark>likelihood function</mark> p(I | W ). We set the prior terms p(K ) and p( i | ζ i ) to be uniform probabilities.<br>2: The test set includes (αj , βj ) for all possible j, the local parameters are the weights wj's for each mixture component j in a cell. Again, we can plug in the cell's PDF into Equation 2 to get the grid's <mark>likelihood function</mark> L * (θ|f (G)).<br>",
    "Arabic": "دالة الاحتمال",
    "Chinese": "似然函数",
    "French": "fonction de vraisemblance",
    "Japanese": "尤度関数",
    "Russian": "функция правдоподобия"
  },
  {
    "English": "likelihood ratio test",
    "context": "1: This is not too many regions, but the overall cost to search the grid using a brute-force method is O(cn 4 ), where c is the average cost to check a given area by computing a single <mark>likelihood ratio test</mark>.<br>2: Then, using a <mark>likelihood ratio test</mark> [12], we could check whether the trend ∆ for the given spatial region is significantly different from the trend that would be used for the entire country.<br>",
    "Arabic": "اختبار نسبة الأرجحية",
    "Chinese": "似然比检验",
    "French": "test du rapport de vraisemblance",
    "Japanese": "尤度比検定",
    "Russian": "тест отношения правдоподобия"
  },
  {
    "English": "likelihood score",
    "context": "1: Of the 10 NICO solutions (different random initializations), we use the one based on parameter estimates yielding the highest <mark>likelihood score</mark> which also always gives the best performance. Because it is a heuristic, the FM does not provide a similar mechanism for ranking solutions from different initializations.<br>",
    "Arabic": "درجة الاحتمال",
    "Chinese": "似然得分",
    "French": "score de vraisemblance",
    "Japanese": "尤度スコア",
    "Russian": "Оценка правдоподобия"
  },
  {
    "English": "likert scale",
    "context": "1: We use standard automatic metrics: perplexity and BLEU for fluency, and unique n-grams as a measure of diversity. We conduct human evaluation following subsection 3.1, for story flow and overall quality on a 3-point <mark>Likert scale</mark> 4 (template in Appendix D). Results. lookahead provides an estimate which better resembles a continuation from top-k sampling.<br>2: Task formulation: Let F summ denote the faithfulness score of a summary. For COARSE, k-point <mark>Likert scale</mark> ratings are obtained for the summary (F summ ∈ {0, 1...k}), based on the faithfulness definition provided earlier. For FINE, we collect binary judgments of individual units in the summary and average them, \n<br>",
    "Arabic": "مقياس ليكرت",
    "Chinese": "利克特量表",
    "French": "échelle de Likert",
    "Japanese": "リッカート尺度",
    "Russian": "шкала Ликерта"
  },
  {
    "English": "line search",
    "context": "1: Nevertheless, NH may be preferable in certain situations where the cost of the exponentiation and <mark>line search</mark> operations are insignificant, such as when an algorithm is bottlenecked by memory access rather than computational speed.<br>2: Solve w (k+1) , Q (k+1) by Algorithm 1 with input v (k) , U (k) , t (k) , λ ; \n t (k) ← ηt (k) ; 6: \n until <mark>line search</mark> criterion is satisfied k ← k + 1 8 : until stop criterion is satisfied tion procedure was implemented by C. Since the proposed algorithm in this paper directly solves the non-convex weak hierarchical Lasso ( 5 ) , and the eventual goal of the convex relaxed weak hierarchical Lasso is also to find a good `` relaxed '' solution to<br>",
    "Arabic": "بحث الخط",
    "Chinese": "线搜索",
    "French": "recherche linéaire",
    "Japanese": "ラインサーチ",
    "Russian": "линейный поиск"
  },
  {
    "English": "linear Transformer",
    "context": "1: (For CIFAR-10 generation we limited memory to 16Gb, to be more comparable to the Transformer and <mark>Linear Transformer</mark> results reported from [22].) Baselines. The Transformer and <mark>Linear Transformer</mark> baselines reported in Table 7 are the results reported directly from Katharopoulos et al. [22].<br>2: A prominent limitation of autoregressive models is inference speed (e.g. generation), since they require a pass over the full context for every new sample. Several methods have been specifically crafted to overcome this limitation such as the <mark>Linear Transformer</mark>, a hybrid Transformer/RNN that switches to a stateful, recurrent view at inference time for speed.<br>",
    "Arabic": "المحول الخطي",
    "Chinese": "线性Transformer",
    "French": "Transformateur linéaire",
    "Japanese": "リニア・トランスフォーマー",
    "Russian": "линейный Трансформер"
  },
  {
    "English": "linear activation function",
    "context": "1: The reward is fed into a convolutional layerQ withĀ channels and a <mark>linear activation function</mark>,Qā ,i ,j = l,i,j Wā l,i,jR l,i −i,j −j . Each channel in this layer corresponds toQ(s,ā) for a particular actionā.<br>2: The output layer utilized the <mark>linear activation function</mark> for the first 120 outputs (corresponding to the heating and moistening tendencies), and ReLU for the remaining 8 variables (corresponding positive-definite surface variables).<br>",
    "Arabic": "دالة التنشيط الخطية",
    "Chinese": "线性激活函数",
    "French": "fonction d'activation linéaire",
    "Japanese": "線形活性化関数",
    "Russian": "линейная активационная функция"
  },
  {
    "English": "linear algebra",
    "context": "1: Matrix query languages (Brijder et al., 2019;Geerts et al., 2021b) are defined to assess the expressive power of <mark>linear algebra</mark>. Balcilar et al.<br>2: Conventional <mark>linear algebra</mark> suggests that we must predict d parameters in order to find the value of the d-dimensional vector E[y|x] for each x.<br>",
    "Arabic": "الجبر الخطي",
    "Chinese": "线性代数",
    "French": "algèbre linéaire",
    "Japanese": "線形代数",
    "Russian": "линейная алгебра"
  },
  {
    "English": "linear classification",
    "context": "1: In order to see if our previous results only validated similar performance in a <mark>linear classification</mark> setting, we will look at performance with k-nn classifiers which evaluate how well a metric is preserved instead of linear separability. We rely on the protocol of Bardes et al.<br>2: Both of these analyses help cement our results, where we can now say that through the lens of <mark>linear classification</mark>, k-nn classification, and CKA, all studied self-supervised methods produce extremely similar representations.<br>",
    "Arabic": "تصنيف خطي",
    "Chinese": "线性分类",
    "French": "classification linéaire",
    "Japanese": "線形分類",
    "Russian": "линейная классификация"
  },
  {
    "English": "linear classification layer",
    "context": "1: This configuration removes the adaptive decoder layer in Equation ( 4) and leverages shared weights with token embedding for the <mark>linear classification layer</mark>.<br>",
    "Arabic": "الطبقة الخطية للتصنيف",
    "Chinese": "线性分类层",
    "French": "couche de classification linéaire",
    "Japanese": "線形分類層",
    "Russian": "линейный классификационный слой"
  },
  {
    "English": "linear classifier",
    "context": "1: Specifically, we re-train the <mark>linear classifier</mark> with an 1weight penalty to encourage sparsity. We then select the top 10 neurons (from 3072) with the largest weight magnitude; this reduces dimensionality by 99.7% while still achieving 96% collision-vs-not accuracy.<br>2: (2017a) suggest to fix a random factor of variation in the underlying generative model and to sample two mini batches of observations x. Disentanglement is then measured as the accuracy of a <mark>linear classifier</mark> that predicts the index of the fixed factor based on the coordinate-wise sum of absolute differences between the representation vectors in the two mini batches.<br>",
    "Arabic": "المصنف الخطي",
    "Chinese": "线性分类器",
    "French": "classificateur linéaire",
    "Japanese": "線形分類器",
    "Russian": "линейный классификатор"
  },
  {
    "English": "linear combination",
    "context": "1: Goal-aware and consistent potential heuristics can thus be compactly classified by a set of linear inequalities. Goalaware and consistent heuristics are also admissible, so we can use an LP solver to optimize any <mark>linear combination</mark> of potentials and transform the solution into an admissible and consistent potential heuristic. Definition 4. Let f be a solution to the following LP: \n<br>2: for some constants α 1 , . . . , α n . By the Linearity Theorem, \n v = n i=1 α i r i(4) \n is the corresponding PPV, expressed as a <mark>linear combination</mark> of the basis vectors r i . Recall from Section 1 that preference sets ( now preference vectors ) are restricted to subsets of a set of hub pages H. If a basis hub vector ( or hereafter hub vector ) for each p ∈ H were computed and stored , then any PPV corresponding to a preference set P of size k ( a preference vector with k nonzero entries<br>",
    "Arabic": "تركيبة خطية",
    "Chinese": "线性组合",
    "French": "combinaison linéaire",
    "Japanese": "線形結合",
    "Russian": "линейная комбинация"
  },
  {
    "English": "linear complexity",
    "context": "1: Moreover, unlike traditional complexity analysis, the scalar c of a <mark>linear complexity</mark> O(cn) must be seriously taken into account when n is easily in the order of billion. BT is a problem of such scale.<br>",
    "Arabic": "التعقيد الخطي",
    "Chinese": "线性复杂度",
    "French": "complexité linéaire",
    "Japanese": "線形計算量",
    "Russian": "линейная сложность"
  },
  {
    "English": "linear constraint",
    "context": "1: The Simple Temporal Problem [Dechter et al., 1991;Dechter, 2003], offers a convenient framework for analyzing temporal aspects of scheduling problems, distinguishing between a set of temporal variables and <mark>linear constraints</mark> between them.<br>",
    "Arabic": "قيد خطي",
    "Chinese": "线性约束条件",
    "French": "contrainte linéaire",
    "Japanese": "線形制約",
    "Russian": "линейное ограничение"
  },
  {
    "English": "linear decay",
    "context": "1: We use AadmW [43] to optimize end-to-end model training, with an initial learning rate of 5e-5, β 1 =0.9, β 2 =0.98, and use learning rate warmup over the first 10% training steps followed by <mark>linear decay</mark> to 0. Our model is implemented in PyTorch [49] and transformers [69].<br>",
    "Arabic": "تناقص خطي",
    "Chinese": "线性衰减",
    "French": "décroissance linéaire",
    "Japanese": "線形減衰",
    "Russian": "линейное затухание"
  },
  {
    "English": "linear decoder",
    "context": "1: The <mark>linear decoder</mark> will also reduce the complexity of the parameter optimization problem from cubic to quadratic.<br>",
    "Arabic": "مُفكك خطي",
    "Chinese": "线性解码器",
    "French": "décodeur linéaire",
    "Japanese": "線形デコーダ",
    "Russian": "линейный декодер"
  },
  {
    "English": "linear discriminant analysis",
    "context": "1: For simplicity of inference in the followings, we introduce another equivalent formulation of KFDA to replace the one in [57]. KFDA is a kernelized version of <mark>linear discriminant analysis</mark> method.<br>2: ∀ i=1,...,K−1 x i : f i (x i ; Θ i ) = f i+1 (x i ; Θ (i+1) ). The advantage of Gaussian components is that the boundary points can be found easily, in the same way as in the <mark>linear discriminant analysis</mark>.<br>",
    "Arabic": "التحليل التمييزي الخطي",
    "Chinese": "线性判别分析",
    "French": "analyse discriminante linéaire",
    "Japanese": "線形判別分析",
    "Russian": "линейный дискриминантный анализ"
  },
  {
    "English": "linear equation",
    "context": "1: Briefly, we require a linear number of calls to the oracle, as well as time in O(n 3 ) for solving a system of <mark>linear equations</mark>.<br>",
    "Arabic": "المعادلة الخطية",
    "Chinese": "线性方程组",
    "French": "équation linéaire",
    "Japanese": "線形方程式",
    "Russian": "линейное уравнение"
  },
  {
    "English": "linear evaluation",
    "context": "1: This kNN classifier can serve as a monitor of the progress. With stop-gradient, the kNN monitor shows a steadily improving accuracy. The <mark>linear evaluation</mark> result is in the table in Figure 2. SimSiam achieves a nontrivial accuracy of 67.7%. This result is reasonably stable as shown by the std of 5 trials.<br>2: We report a set of experiments characterizing the performance of our approach on many datasets and in several different evaluation settings (low data, <mark>linear evaluation</mark>, full fine-tuning). We also conduct several experiments designed to better understand the achieved performance of these models.<br>",
    "Arabic": "تقييم خطي",
    "Chinese": "线性评估",
    "French": "évaluation linéaire",
    "Japanese": "線形評価",
    "Russian": "линейная оценка"
  },
  {
    "English": "linear function",
    "context": "1: We proposed a checker and a filtering algorithm based on the energetic reasoning for the SOFTCUMULATIVE constraint. Unlike previous work, both algorithms support a quadratic cost function in addition to a <mark>linear function</mark>. They are parametrable in the sense that their strength and complexity vary depending of the number of time points passed as parameters. With the recommended set of time points T s , the checker has a complexity of O ( n 2 ) and the filtering algorithm has a complexity of O ( k • n 2 + r ) where k is the number of tasks that pass the free energy check and r is the number of values pruned from the domain of the<br>",
    "Arabic": "دالة خطية",
    "Chinese": "线性函数",
    "French": "fonction linéaire",
    "Japanese": "線形関数",
    "Russian": "линейная функция"
  },
  {
    "English": "linear function approximation",
    "context": "1: In M-MCTS with τ > 0 the memory can provide more generalization, which we show to be beneficial both theoretically and practically. TD search uses <mark>linear function approximation</mark> to generalize between related states. This <mark>linear function approximation</mark> is updated during the online real-time search.<br>",
    "Arabic": "التقريب الخطي للدوال",
    "Chinese": "线性函数近似",
    "French": "approximation de fonction linéaire",
    "Japanese": "線形関数近似",
    "Russian": "линейная аппроксимация функции"
  },
  {
    "English": "linear inequality",
    "context": "1: Step 1 requires exponential (polynomial in data) time, and it does not increase the maximal size of a rule. Hence, Step 2 is nondeterministic exponential (polynomial in data), and Step 3 requires exponential (polynomial in data) time to solve a system of <mark>linear inequalities</mark>.<br>2: Appendix A.2 discusses the synthesis of convex relaxations of bilinear equalities, which allows us to replace each bilinear equality by a set of <mark>linear inequalities</mark>. Using them, a convex relaxation of the above optimization problem can be stated as arg min \n<br>",
    "Arabic": "متراجحة خطية",
    "Chinese": "线性不等式",
    "French": "inégalité linéaire",
    "Japanese": "線形不等式",
    "Russian": "линейное неравенство"
  },
  {
    "English": "linear interpolation",
    "context": "1: With the same techniques, one can easily perform image magnification. Starting from a <mark>linear interpolation</mark> of a small image, and applying our PDE (14) on the image (excepted on the original known pixels), we can retrieve nonlinear magnified images without jagging or bloc effects, inherent to classical <mark>linear interpolation</mark> techniques (Fig.<br>2: Results: We compare the results of Pose-NDF with those from VPoser [49] and GAN-S [16] interpolation. For VPoser [49], we project the start and end pose into the latent space and perform <mark>linear interpolation</mark> using the latent vectors.<br>",
    "Arabic": "الاستيفاء الخطي",
    "Chinese": "线性插值",
    "French": "interpolation linéaire",
    "Japanese": "線形補間",
    "Russian": "линейная интерполяция"
  },
  {
    "English": "linear kernel",
    "context": "1: Accuracy of SVM with kernel perceptron slightly decreases, while SVM with <mark>linear kernel</mark> clearly worsen. Stack-1-NN still improves. The base classifier that benefits most of attribute decomposition is 1-NN. Somehow this is something to be expected, because nearest neighbor algorithms degrade severely with irrelevant attributes. Discussion. Several ensemble methods have been tested in this work.<br>2: For a <mark>linear kernel</mark>, the total run-time of our method isÕ(d/(λ )), where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets.<br>",
    "Arabic": "نواة خطية",
    "Chinese": "线性核",
    "French": "noyau linéaire",
    "Japanese": "線形カーネル",
    "Russian": "линейное ядро"
  },
  {
    "English": "linear layer",
    "context": "1: We break down the aggregation of crowd-sourced coreference labels into two steps: mention classification and coreference chain inference, as illustrated in Figure 2. Below, we describe our method in more detail. All the biases in <mark>linear layer</mark> parameters are omitted for simplification. Figure 3: Per-category reliability in the method.<br>2: We used no gradient clipping and a weight decay of 0.1. Unlike [2] which specified different dropout rates for different parameters, we used a constant dropout rate of 0.25 throughout the network, including before every <mark>linear layer</mark> and on the residual branches.<br>",
    "Arabic": "طبقة خطية",
    "Chinese": "线性层",
    "French": "couche linéaire",
    "Japanese": "線形層",
    "Russian": "линейный слой"
  },
  {
    "English": "linear learning rate decay",
    "context": "1: In training, we employ the AdamW [44] optimizer with an initial learning rate of 6 × 10 −5 , a weight decay of 0.01, a scheduler that uses <mark>linear learning rate decay</mark>, and a linear warmup of 1,500 iterations. Models are trained on 8 GPUs with 2 images per GPU for 160K iterations.<br>",
    "Arabic": "انحسار معدل التعلم الخطي",
    "Chinese": "线性学习率衰减",
    "French": "Décroissance linéaire du taux d'apprentissage",
    "Japanese": "線形学習率減衰",
    "Russian": "линейное уменьшение темпа обучения"
  },
  {
    "English": "linear learning rate schedule",
    "context": "1: The inner optimization trajectories for different values of the outer-parameters are shown in Appendix C. We tuned a <mark>linear learning rate schedule</mark> parameterized by the initial and final log-learning rates, θ 0 and θ 1 , respectively: α t = 1 − t T e θ0 + t T e θ1 .<br>",
    "Arabic": "جدول معدل التعلم الخطي",
    "Chinese": "线性学习率调度",
    "French": "calendrier de taux d'apprentissage linéaire",
    "Japanese": "線形学習率スケジュール",
    "Russian": "линейное расписание темпа обучения"
  },
  {
    "English": "linear map",
    "context": "1: Formally, we say that a function Φ : R d1 → R d1 is differentiable at a point z ∈ R d if there exists a <mark>linear map</mark> DΦ(z) ∈ R d2×d1 such that \n lim h →0 Φ(h + z) − Φ(z) h − DΦ(z) • h = 0.<br>",
    "Arabic": "التطبيق الخطي",
    "Chinese": "线性映射",
    "French": "application linéaire",
    "Japanese": "線形写像",
    "Russian": "линейное отображение"
  },
  {
    "English": "linear model",
    "context": "1: The framework proposed in [26], on the other hand, is based on a simple <mark>linear model</mark> where the solution to the optimization problem can be obtained by solving a system of linear equations.<br>2: Thus the model allows the score for \"dog\" to influence decisions about \"husky\". If the score function f i ( x ; w ) is a <mark>linear model</mark> w T i x , then Pr ( y i = 1|x ) ∝ exp ( w T i x ) , wherew i = w i + vj ∈α ( vi ) w j , i.e the weights decompose along the hierarchy-the weights for `` husky '' are<br>",
    "Arabic": "النموذج الخطي",
    "Chinese": "线性模型",
    "French": "modèle linéaire",
    "Japanese": "線形モデル",
    "Russian": "линейная модель"
  },
  {
    "English": "linear predictor",
    "context": "1: roughly 1 + O( d/m). Theorem 4 implies that the errors of any <mark>linear predictor</mark> are not magnified much by the compression function. So a good <mark>linear predictor</mark> for the original problem implies an almost-as-good <mark>linear predictor</mark> for the induced problem.<br>2: We show that this simple DirectPred method nevertheless yields comparable performance in CIFAR-10 and outperforms gradient training of the <mark>linear predictor</mark> by +5% Top-1 accuracy in linear evaluation protocol on both STL-10 and ImageNet (60 epochs).<br>",
    "Arabic": "المتنبئ الخطي",
    "Chinese": "线性预测器",
    "French": "prédicteur linéaire",
    "Japanese": "線形予測器",
    "Russian": "линейный предсказатель"
  },
  {
    "English": "linear probe",
    "context": "1: where X is the input sequence of tokens, B is the STDMLM pretrained model, ϕ is the model trained with one of our new pretraining techniques, f is a <mark>linear probe</mark>, [•, •] denotes concatenation of embeddings and Perf is any standard performance metric.<br>2: In supervised pre-training, representation quality tends to increase monotonically with depth, such that the best representations lie at the penultimate layer (Zeiler & Fergus, 2014). Indeed, since a linear layer produces class logits from pre-logits, a good classifier necessarily achieves high accuracy on a <mark>linear probe</mark> of its pre-logits.<br>",
    "Arabic": "المسح الخطي",
    "Chinese": "线性探针",
    "French": "sonde linéaire",
    "Japanese": "線形プローブ",
    "Russian": "линейный зонд"
  },
  {
    "English": "linear program",
    "context": "1: We prove that if one has this statistical information, it is possible to efficiently compute causally fair utility-maximizing policies by solving either a single <mark>linear program</mark> or a series of <mark>linear program</mark>s (Appendix, Theorem B.1).<br>2: a finite set of size n, then a utility-maximizing policy constrained to lie in C can be constructed via a <mark>linear program</mark> with O(n) variables and constraints. 3.<br>",
    "Arabic": "البرنامج الخطي",
    "Chinese": "线性规划",
    "French": "programme linéaire",
    "Japanese": "線形計画",
    "Russian": "линейная программа"
  },
  {
    "English": "linear programming relaxation",
    "context": "1: This is called the MAP assignment problem in graphical models and for general graphs and arbitrary parameters is NP complete. Consequently, there is an extensive literature of approximation schemes for the problem and new algorithms continue to be explored [2,3,4,5,6,7,8]. The most popular of these are based on the following <mark>linear programming relaxation</mark> of the MAP problem. min µ i , xi θ i ( x i ) µ i ( x i ) + ( i , j ) , xi , xj θ ij ( x i , x j ) µ ij ( x i , x j ) xj µ ij ( x i , x j ) = µ i ( x i ) ∀ (<br>2: The BMC problem was recently proposed in [21] where it was shown to be NP-hard and O(log k) approximable using a <mark>linear programming relaxation</mark>.<br>",
    "Arabic": "استرخاء البرمجة الخطية",
    "Chinese": "线性规划松弛",
    "French": "relaxation de la programmation linéaire",
    "Japanese": "線形計画緩和",
    "Russian": "релаксация линейного программирования"
  },
  {
    "English": "linear projection",
    "context": "1: SRU combines the three matrix multiplications across all time steps as a single multiplication. This significantly improves the computation intensity (e.g. GPU utilization). Specifically, the batched multiplication is a <mark>linear projection</mark> of the input tensor X ∈ R L×d : \n U =   W W W   X ,(1) \n<br>2: Aggregation and projection are well-known techniques in polyhedral combinatorics for obtaining valid inequalities [6]. Given a <mark>linear projection</mark> Φ(x) = Ax, any valid inequality c Φ(x) ≤ b for Φ(x) also gives the valid inequality c Ax ≤ b for x.<br>",
    "Arabic": "الإسقاط الخطي",
    "Chinese": "线性映射",
    "French": "projection linéaire",
    "Japanese": "線形射影",
    "Russian": "линейная проекция"
  },
  {
    "English": "linear regression",
    "context": "1: Ref. We also provide results for the prediction phase and give throughput (no. of predictions per second) comparison details for the aforementioned algorithms, using real-world datasets. The gain in online throughput for prediction ranges from 3× to 145.18× for <mark>Linear Regression</mark> and 3× to 158.40× for Logistic Regression over LAN and WAN combined.<br>2: Logistic Regression can be thought of as an execution of <mark>Linear Regression</mark> followed by a Sigmoid function on the output, due to which our improvements for <mark>Linear Regression</mark> carry over to Logistic. Our improvement ranges from 5.95× to 67.88× over LAN and 2.71× to 2.96× over WAN.<br>",
    "Arabic": "الانحدار الخطي",
    "Chinese": "线性回归",
    "French": "régression linéaire",
    "Japanese": "線形回帰",
    "Russian": "линейная регрессия"
  },
  {
    "English": "linear regression model",
    "context": "1: Traditional approaches to fit such a model typically follow the following two-step procedures [22]: \n (i) Fit a <mark>linear regression model</mark> that only includes the main effects and then select the significant features; \n (ii) Fit the reformulated model with the identified individual features and the interactions constructed via domain knowledge.<br>2: Figure 3 shows a simple example of the output of a simulated segmental Markov model. The process begins in state A which produces observations according to a noisy <mark>linear regression model</mark>. After 30 time steps or so it transitions to state B and produces observations according to a noisy exponential decay model.<br>",
    "Arabic": "نموذج الانحدار الخطي",
    "Chinese": "线性回归模型",
    "French": "modèle de régression linéaire",
    "Japanese": "線形回帰モデル",
    "Russian": "линейная регрессионная модель"
  },
  {
    "English": "linear regressor",
    "context": "1: MIXER: Our implementation of the mixed incremental cross-entropy reinforce (Ranzato et al., 2015), where the sentence-level metric is BLEU and the average reward is acquired according to its offline method with a 1-layer <mark>linear regressor</mark>.<br>",
    "Arabic": "الانحدارالخطي",
    "Chinese": "线性回归器",
    "French": "régresseur linéaire",
    "Japanese": "線形回帰器",
    "Russian": "линейный регрессор"
  },
  {
    "English": "linear scaling",
    "context": "1: • Optimizer. We use SGD for pre-training. Our method does not require a large-batch optimizer such as LARS [38] (unlike [8,15,7]). We use a learning rate of lr×BatchSize/256 (<mark>linear scaling</mark> [14]), with a base lr = 0.05. The learning rate has a cosine decay schedule [27,8].<br>2: One might expect that two reward functions R 1 and R 2 must have the same policy ordering for all τ if and only if they differ by potential shaping and <mark>linear scaling</mark>. However, this is not the case. To see this , consider the rewards R 1 , R 2 where R 1 ( s 1 , a 1 , s 1 ) = 1 , R 1 ( s 1 , a 1 , s 2 ) = 0.5 , R 2 ( s 1 , a 1 , s 1 ) = 0.5 , and R 2 ( s 1<br>",
    "Arabic": "تحجيم خطي",
    "Chinese": "线性缩放",
    "French": "mise à l'échelle linéaire",
    "Japanese": "線形スケーリング",
    "Russian": "линейное масштабирование"
  },
  {
    "English": "linear scheduler",
    "context": "1: AdamW optimizer (Loshchilov and Hutter, 2019) and a <mark>linear scheduler</mark> were used in all our experiments, which were conducted on a single NVIDIA A100 Tensor Core GPU. For the pretraining step, we utilized a batch size of 4, a gradient accumulation step of 20, and 4 epochs for the mBERT base model.<br>",
    "Arabic": "جدولة خطية",
    "Chinese": "线性调度器",
    "French": "programmateur linéaire",
    "Japanese": "線形スケジューラ",
    "Russian": "линейный планировщик"
  },
  {
    "English": "linear separability",
    "context": "1: In order to see if our previous results only validated similar performance in a linear classification setting, we will look at performance with k-nn classifiers which evaluate how well a metric is preserved instead of <mark>linear separability</mark>. We rely on the protocol of Bardes et al.<br>",
    "Arabic": "الانفصال الخطي",
    "Chinese": "线性可分性",
    "French": "séparabilité linéaire",
    "Japanese": "線形可分性",
    "Russian": "линейная разделимость"
  },
  {
    "English": "linear system",
    "context": "1: We construct a simplified bilateral grid from the reference image, which is bistochastized as in [2] (see the supplement for details), and with that we construct the A matrix and b vector described in Equation 6 which are used to solve the <mark>linear system</mark> in Equation 8 to produce an output image.<br>2: 2 Optimality directly follows from a simple dimensionality argument: even if the s-sparse support of the vector β is known, recovering the unknown coefficients requires solving a <mark>linear system</mark> with s unknowns uniquely. For this, we need at least s linear equations, i.e., s observations.<br>",
    "Arabic": "نظام خطي",
    "Chinese": "线性系统",
    "French": "système linéaire",
    "Japanese": "線形システム",
    "Russian": "линейная система"
  },
  {
    "English": "linear threshold",
    "context": "1: Following the approximation and NP-hardness results, we describe in Section 3 the results of computational experiments with both the <mark>Linear Threshold</mark> and Independent Cascade Models, showing that the hill-climbing algorithm significantly out-performs strategies based on targeting high-degree or \"central\" nodes [30].<br>2: Both the <mark>Linear Threshold</mark> and Independent Cascade Models (as well as the generalizations to follow) involve an initial set of active nodes A0 that start the diffusion process.<br>",
    "Arabic": "العتبة الخطية",
    "Chinese": "线性阈值模型",
    "French": "seuil linéaire",
    "Japanese": "線形閾値",
    "Russian": "линейный порог"
  },
  {
    "English": "linear threshold model",
    "context": "1: In Section 4 we then develop a general model of diffusion processes in social networks that simultaneously generalizes the Linear Threshold and Independent Cascade Models, as well as a number of other natural cases, and we show how to obtain approximation guarantees for a large sub-class of these models.<br>2: In the proof of Theorem 2.2, we constructed an equivalent process by initially resolving the outcomes of some random choices, considering each outcome in isolation, and then averaging over all outcomes. For the <mark>Linear Threshold Model</mark>, the simplest analogue would be to consider the behavior of the process after all node thresholds have been chosen.<br>",
    "Arabic": "نموذج العتبة الخطي",
    "Chinese": "线性阈值模型",
    "French": "modèle de seuil linéaire",
    "Japanese": "線形しきい値モデル",
    "Russian": "линейная пороговая модель"
  },
  {
    "English": "linear transform",
    "context": "1: . \n The sparsest recovery approach of this paper is similar (in flavor) to the above stated work; in fact, as is shown subsequently, the partial information we consider can be written as a <mark>linear transform</mark> of the function f (•). However, the methods or approaches of the prior work do not apply.<br>2: where d(u, i) is the cosine distance between the encoded utterance u and encoded image i. The image encoder part of the model uses image vectors from a pretrained object classification model, VGG-16 (Simonyan and Zisserman, 2014), and uses a <mark>linear transform</mark> to directly project these to the joint space.<br>",
    "Arabic": "التحويل الخطي",
    "Chinese": "线性变换",
    "French": "transformation linéaire",
    "Japanese": "線形変換",
    "Russian": "линейное преобразование"
  },
  {
    "English": "linear transformation",
    "context": "1: where g stands for a <mark>linear transformation</mark>, W o is used to map t j to o j so that each target word has one corresponding dimension in o j .<br>2: Thus, for image data, each pixel is represented with a 256-dimensional vector. To deal with the very high dimensionality of this input parameterization, we first map each one-hot vector to a learned, low-dimensional embedding with a <mark>linear transformation</mark>. We map to D p = 4 dimensions for all models tested.<br>",
    "Arabic": "التحويل الخطي",
    "Chinese": "线性变换",
    "French": "transformation linéaire",
    "Japanese": "線形変換",
    "Russian": "линейное преобразование"
  },
  {
    "English": "linear transformation matrix",
    "context": "1: We define the <mark>linear transformation matrix</mark> W = Q √ Λ and apply it to the original embeddings X, obtaining the transformed embeddings X ′ = XW .<br>",
    "Arabic": "مصفوفة التحويل الخطي",
    "Chinese": "线性变换矩阵",
    "French": "matrice de transformation linéaire",
    "Japanese": "線形変換行列",
    "Russian": "матрица линейного преобразования"
  },
  {
    "English": "linear warm-up",
    "context": "1: We train the models for 10 epochs with mixed precision using AdamW (Loshchilov and Hutter, 2019) with a weight decay of 0.05 and the initial learning rate set to 2e−5. We use a linear scheduler with 10% <mark>linear warm-up</mark> and decay.<br>",
    "Arabic": "التسخين الخطي",
    "Chinese": "线性预热",
    "French": "préchauffage linéaire",
    "Japanese": "線形ウォームアップ",
    "Russian": "линейный разогрев"
  },
  {
    "English": "linear-chain",
    "context": "1: This data is then used to learn extraction patterns on both POS tags (WOE pos ) and dependency parses (WOE parse ). Former extractor utilizes a <mark>linear-chain</mark> Conditional Random Field (CRF) to train a model of relations on shallow features which outputs certain text between two NPs when it denotes a relation.<br>",
    "Arabic": "سلسلة خطية",
    "Chinese": "线性链",
    "French": "chaîne linéaire",
    "Japanese": "線形チェーン",
    "Russian": "линейно-цепной"
  },
  {
    "English": "linear-quadratic regulator",
    "context": "1: <mark>linear-quadratic regulators</mark>) [DMM + 20, CHK + 18, JP19, FTM20, MTR19, SF20, FGKM18, MPB + 19], and LDSs with partial observation y t ≈ Cx t (e.g. Kalman filtering or linear-quadratic-Gaussian control) [OO19, SBR19, SRD21, SOF20, TMP20, LAHA20, ZFKL21].<br>2: Optimal control frameworks, such as Markov decision processes (MDPs) and <mark>linear-quadratic regulators</mark> (LQRs), provide rich representations of interactions with stochastic systems.<br>",
    "Arabic": "منظم خطي تربيعي",
    "Chinese": "线性二次调节器",
    "French": "régulateur linéaire-quadratique",
    "Japanese": "線形二次レギュレータ",
    "Russian": "линейно-квадратический регулятор"
  },
  {
    "English": "linearization",
    "context": "1: To make the most of the data, we avoid <mark>linearization</mark> by sequentially minimizing the full regularized loss function L(N t , •) + R(•) where R(β) is a convex regularization function. That is, at each step, we set: \n<br>2: (2021) rely on a <mark>linearization</mark> strategy to force models to learn to predict templates in a pre-defined order. In general, however, such orderings are arbitrary.<br>",
    "Arabic": "تخطيح",
    "Chinese": "线性化",
    "French": "linéarisation",
    "Japanese": "線形化",
    "Russian": "линейная аппроксимация"
  },
  {
    "English": "link function",
    "context": "1: L v -Lipschitz, which means |v t (x) − v t (y)| ≤ L v |x − y|. A <mark>link function</mark> σ describes the probabilistic comparison of utilities of two rankers as, \n<br>2: P t w ≻ w ′ = f t (w, w ′ ) = σ v t (w) − v t (w ′ ) . The <mark>link function</mark> should be rotation-symmetric, which means σ (x) = 1 − σ (−x).<br>",
    "Arabic": "دالة الربط",
    "Chinese": "联系函数",
    "French": "fonction de lien",
    "Japanese": "リンク関数",
    "Russian": "функция связи"
  },
  {
    "English": "link prediction",
    "context": "1: We also evaluate additional graph classification and <mark>link prediction</mark> on more specialized datasets where the graph label and the link label are inborn and not related to any node (see Appendix A).<br>2: Person-alityCafe and Facebook datasets are used to test the performance of <mark>link prediction</mark>, both of which are social networks where edges denote the following/quoting relations. Multi-label v.s. Multi-class Classification In the main experiments, we treat the classification task as a multi-label problem. Here we present the experimental results under a multi-class setting.<br>",
    "Arabic": "التنبؤ بالروابط",
    "Chinese": "链路预测",
    "French": "prédiction de lien",
    "Japanese": "リンク予測",
    "Russian": "\"предсказание связей\""
  },
  {
    "English": "local basis function",
    "context": "1: Whenever a component tensor u i is optimized in the way described above, we simply add g to the set of <mark>local basis functions</mark>, obtaining as a new basis \n<br>2: loc,i (x) = [b loc,i 1 (x), b loc,i 2 (x), . . . , b loc,i Mi (x) \n ]. More precisely, the <mark>local basis functions</mark> can be identified using the open edges in Figure 3 as follows.<br>",
    "Arabic": "\"دالة الأساس المحلية\"",
    "Chinese": "局部基函数",
    "French": "fonction de base locale",
    "Japanese": "局所基底関数",
    "Russian": "локальная базисная функция"
  },
  {
    "English": "local coherence",
    "context": "1: Recent studies have introduced successful automatic methods to predict the structure and coherence of texts. They include entity approaches for <mark>local coherence</mark> which track the repetition and syntactic realization of entities in adjacent sentences ( Barzilay and Lapata , 2008 ; Elsner and Charniak , 2008 ) and content approaches for global coherence which view texts as a sequence of topics , each characterized by a particular distribution of lexical items ( Barzilay and Lee , 2004 ; Fung<br>2: Obtaining an appropriate representation of discourse is the first step towards creating a compression model that exploits contextual information. In this work we focus on the role of <mark>local coherence</mark> as this is prerequisite for maintaining global coherence. Ideally, we would like our compressed document to maintain the discourse flow of the original.<br>",
    "Arabic": "الترابط المحلي",
    "Chinese": "局部连贯性",
    "French": "cohérence locale",
    "Japanese": "ローカル一貫性",
    "Russian": "локальная когерентность"
  },
  {
    "English": "local consistency",
    "context": "1: E kp (S, O, P ) = κ∈KP ∆ m (κ; S)(5) \n <mark>Local Consistency</mark>. In addition to the above data terms, we use a simple shape regularizer to restrict arbitrary deformations by imposing a quadratic deformation penalty between every point and its neighbors.<br>",
    "Arabic": "ثبات محلي",
    "Chinese": "局部一致性",
    "French": "cohérence locale",
    "Japanese": "局所的一貫性",
    "Russian": "локальная согласованность"
  },
  {
    "English": "local context",
    "context": "1: The results are in line with our expectation. To learn the shared predictive structure of <mark>local context</mark> (LC) and syntactic relations (SR), it is more advantageous to apply ASO to each of the three sets of problems (disambiguation of nouns, verbs, and adjectives, respectively), separately.<br>2: Our features described above inherently consist of four feature groups: <mark>local context</mark> (Ä ), global context ( \n ), syntactic relation (ËÊ), and POS features. To exploit such a natural feature split, we explore the following extension of the joint linear model: \n<br>",
    "Arabic": "السياق المحلي",
    "Chinese": "本地上下文",
    "French": "contexte local",
    "Japanese": "局所的コンテキスト",
    "Russian": "локальный контекст"
  },
  {
    "English": "local coordinate frame",
    "context": "1: We define a continuous bijective mapping T i that maps 3D points x i from each <mark>local coordinate frame</mark> L i to the canonical 3D coordinate frame as u = T i (x i ), where i is the frame index.<br>",
    "Arabic": "الإطار المحلي للإحداثيات",
    "Chinese": "局部坐标系",
    "French": "cadre de coordonnées locales",
    "Japanese": "ローカル座標系",
    "Russian": "локальная система координат"
  },
  {
    "English": "local feature",
    "context": "1: The algorithm initializes E 0 with the pairwise term and adds <mark>local features</mark> in an iterative greedy way, such that in each iteration a single feature is added: \n E k (x; I) = E k−1 (x; I) + λ k |x − x F k ,I |.<br>2: More formally, we split the feature extractor \n f = (f 1 , . . . , f d ) into f = (f L ; f N ) \n where f L and f N are the local and non-<mark>local features</mark>, respectively.<br>",
    "Arabic": "سمة محلية",
    "Chinese": "局部特征",
    "French": "caractéristique locale",
    "Japanese": "局所的特徴",
    "Russian": "локальная характеристика"
  },
  {
    "English": "local geometry",
    "context": "1: The results shown in Table 9 indicate that our model still achieves much better performance regarding the <mark>local geometry</mark>. We find that Re-fineGNN achieves relatively low performance on modeling <mark>local geometry</mark>, which might be because that its indirect loss on various invariant features cannot ensure atoms in the backbone are equally supervised.<br>",
    "Arabic": "الشكل الهندسي المحلي",
    "Chinese": "局部几何",
    "French": "géométrie locale",
    "Japanese": "局所幾何 (Kyokusho Kikai)",
    "Russian": "локальная геометрия"
  },
  {
    "English": "local image feature",
    "context": "1: Sparse reconstruction based on matching <mark>local image features</mark> [10,21,23,34,51,57,59,65] is the most common due to its scalability and its robustness to appearance changes introduced by varying devices, viewpoints, and temporal conditions found in crowdsourced scenarios [2,29,35,41,47,50,58].<br>2: The PMK uses multi-resolution histograms to estimate the correspondence between two sets of <mark>local image features</mark>. To hash with the non-learned PMK, the pyramids can be embedded in such a way that standard inner product LSH functions are applicable [14].<br>",
    "Arabic": "الميزة المحلية للصورة",
    "Chinese": "局部图像特征",
    "French": "caractéristique d'image locale",
    "Japanese": "局所画像特徴",
    "Russian": "локальная особенность изображения"
  },
  {
    "English": "local maxima",
    "context": "1: We chose the operation types enumerated above for two reasons: (i) they are general enough to enable the decoder escape <mark>local maxima</mark> and modify in a non-trivial manner a given alignment in order to produce good translations; (ii) they are relatively inexpensive (timewise).<br>2: Real-Time Structure-from-Motion (SfM). Our SfM module is based on the approach by [4], which is highly optimized and runs at 26-30 fps. It takes the green channel of each camera as input and extracts image feature points by finding <mark>local maxima</mark> of a simple feature measure based on average intensities of four subregions.<br>",
    "Arabic": "النقاط القصوى المحلية",
    "Chinese": "局部最大值",
    "French": "maximums locaux",
    "Japanese": "局所最大値",
    "Russian": "локальные максимумы"
  },
  {
    "English": "local maximum",
    "context": "1: The remaining positions are assigned randomly. An η-approximate version of MMax with such subgradients returns an η-approximate <mark>local maximum</mark> that achieves an improved approximation factor of 1/3 − η in O( n 2 log n η ) iterations. Lemma 6.3. Algorithm RLS returns a <mark>local maximum</mark> X that satisfies max{f (X), f \n<br>2: Unfortunately, the approach in (Teo et al., 2007) cannot be used for non-convex problems since the minimization of the approximated problem may lead to a <mark>local maximum</mark> w. Figure 1b illustrates a situation where the method designed for convex cases yields such an improper solution.<br>",
    "Arabic": "القيمة المحلية القصوى",
    "Chinese": "局部极大值",
    "French": "maximum local",
    "Japanese": "局所最大値",
    "Russian": "локальный максимум"
  },
  {
    "English": "local minima",
    "context": "1: We show that similar results hold for the <mark>local minima</mark> of the nonconvex costs typical of modern deep learning systems, and also hold when the family Q is infinite. Let (z, w) be the loss of a machine learning model where w ∈ R d represent the parameters of the model and z ∈ R n are examples.<br>2: Finding all <mark>local minima</mark> of such functions is task for which several strategies have emerged from the computational chemistry community, and have been introduced to computer vision by Sminchisescu and Triggs (2002).<br>",
    "Arabic": "الحد الأدنى المحلي",
    "Chinese": "局部极小值",
    "French": "minima locaux",
    "Japanese": "局所最小値",
    "Russian": "локальные минимумы"
  },
  {
    "English": "local minimizer",
    "context": "1: Indeed, the energy functional ½ may be nonconvex due to its data term, so that the gradient descent may be trapped in a <mark>local minimizer</mark>. As a consequence, its asymptotic state depends on the initial guess ¼ .<br>",
    "Arabic": "الحد الأدنى المحلي",
    "Chinese": "局部极小值点",
    "French": "minimiseur local",
    "Japanese": "局所最小値",
    "Russian": "локальный минимум"
  },
  {
    "English": "local minimum",
    "context": "1: It is also possible to prove that such a <mark>local minimum</mark> lies within a multiplicative factor of the global minimum [10] (the factor is at least 2 and depends only on V ).<br>2: As a corollary, Lemma 5.5 implies that if a submodular function has a unique <mark>local minimum</mark>, MMin-I and II must find this minimum, which is a global one. In the following we consider two extensions of MMin-I and II. First, we analyze an algorithm that alternates between MMin-I and MMin-II.<br>",
    "Arabic": "الحد الأدنى المحلي",
    "Chinese": "局部最小值",
    "French": "minimum local",
    "Japanese": "局所最小値",
    "Russian": "локальный минимум"
  },
  {
    "English": "local model",
    "context": "1: We first describe the two representations of sentence structure we adopted for our analysis. 3 Next, we present two coherence models: a <mark>local model</mark> which captures the co-occurrence of structural features in adjacent sentences and a global one which learns from clusters of sentences with similar syntax.<br>2: For example, if people occur beside one another but not above, the weight from w y i ,y i associated with next-to relations would then be large. Local model. In our current implementation, rather than learning a local template, we simply use the output of the local detector as the single feature.<br>",
    "Arabic": "نموذج محلي",
    "Chinese": "局部模型",
    "French": "modèle local",
    "Japanese": "ローカルモデル",
    "Russian": "локальная модель"
  },
  {
    "English": "local optima",
    "context": "1: The values of all the traps are added together to form the overall fitness value. An n-bit trap-5 function has one global optimum (a string of all 1s) and (2 n/5 − 1) <mark>local optima</mark>. The difficulty in optimizing this function is that in each 5-bit trap function, all 5 bits have to be considered together.<br>2: Many statistical learning problems in NLP call for local model search methods. But accuracy tends to suffer with current techniques, which often explore either too narrowly or too broadly: hill-climbers can get stuck in <mark>local optima</mark>, whereas samplers may be inefficient. We propose to arrange individual local optimizers into organized networks.<br>",
    "Arabic": "الأمثل المحلي",
    "Chinese": "局部最优",
    "French": "optima locaux",
    "Japanese": "局所最適解",
    "Russian": "локальные оптимумы"
  },
  {
    "English": "local optimum",
    "context": "1: USP learning uses the same optimization objective as hard EM, and is also guaranteed to find a <mark>local optimum</mark> since at each step it improves the log-likelihood. It differs from EM in directly optimizing the likelihood instead of a lower bound.<br>2: Since the cost is convex with respect to the weights a <mark>local optimum</mark> is not an issue. ( F kn , λ m ) = arg max n=1 : N , m=1 : M L k ( F kn , λ m ) Set λ k = λ m , F k = F kn , E k ( x ; I ) = E k−1 ( x ; I ) + λ k |x − xF k , I |<br>",
    "Arabic": "الأمثل المحلي",
    "Chinese": "局部最优",
    "French": "optimum local",
    "Japanese": "局所最適解",
    "Russian": "локальный оптимум"
  },
  {
    "English": "local parameter",
    "context": "1: The \"<mark>local parameters</mark>\" are those parameters within θ that are customizable to each cell. They generally capture the uninteresting or anticipated spatial variation of the data across different cells. Sometimes, the precise values for <mark>local parameters</mark> may be known and supplied beforehand by the user (such as the number of people living in a spatial region).<br>",
    "Arabic": "معلمات محلية",
    "Chinese": "局部参数",
    "French": "paramètre local",
    "Japanese": "ローカルパラメータ",
    "Russian": "локальные параметры"
  },
  {
    "English": "local search",
    "context": "1: In future work, we plan to investigate other approximate inference techniques like <mark>local search</mark> and dual decomposition. Regarding the importance of trees for compositional generalization, our model has no explicit structural inductive bias towards trees.<br>2: VBSS is an iterative stochastic heuristic search algorithm (Cicirello & Smith 2005). A search heuristic is used to bias a random decision at each decision point. We use VBSS here to generate biased initial configurations for a <mark>local search</mark> for the weighted tardiness problem known as Multistart Dynasearch (Congram, Potts, & van de Velde 2002).<br>",
    "Arabic": "البحث المحلي",
    "Chinese": "局部搜索",
    "French": "recherche locale",
    "Japanese": "局所探索",
    "Russian": "локальный поиск"
  },
  {
    "English": "local search method",
    "context": "1: (ii) The tree ensembles are typically very large, making them a complex and hardly interpretable decision structure. Recent work by Carreira-Perpinan andTavallali [2018], Zharmagambetov andCarreira-Perpiñán [2020] improve RF with <mark>local search methods</mark> via alternating minimization. However, their implementation is not open-source.<br>",
    "Arabic": "طريقة البحث المحلية",
    "Chinese": "局部搜索方法",
    "French": "méthode de recherche locale",
    "Japanese": "局所探索法",
    "Russian": "метод локального поиска"
  },
  {
    "English": "local variable",
    "context": "1: This algorithm is asynchronous in the sense that it does not require global synchronous operations: the mixing of <mark>local variables</mark> does not require any synchronization since parameter t ∈ R 0 is available at all nodes independently from the number of past updates, while a local pairwise update between adjacent nodes v and w only requires a local synchronization.<br>",
    "Arabic": "متغير محلي",
    "Chinese": "局部变量",
    "French": "variable locale",
    "Japanese": "ローカル変数",
    "Russian": "локальная переменная"
  },
  {
    "English": "local window",
    "context": "1: The severity of the conditions is related to the number of active layers in a <mark>local window</mark>. The least restricted case is when only one layer is active, in which the local color distribution can be arbitrary complex.<br>",
    "Arabic": "نافذة محلية",
    "Chinese": "局部窗口",
    "French": "fenêtre locale",
    "Japanese": "ローカルウィンドウ",
    "Russian": "локальное окно"
  },
  {
    "English": "locality-sensitive hashing",
    "context": "1: Each WTA hash function defines an ordinal embedding and an associated rank-correlation similarity measure, which offers a degree of invariance with respect to pertur-bations in numeric values [21] and is well suited as a basis for <mark>locality-sensitive hashing</mark>.<br>2: We exploit <mark>locality-sensitive hashing</mark> to replace the dot-product kernel operator in the convolution with a fixed number of hash-table probes that effectively sample all of the filter responses in time independent of the size of the filter bank.<br>",
    "Arabic": "تجميع الهاشات الحساس للموقع",
    "Chinese": "对局部敏感哈希",
    "French": "hachage sensible à la localité",
    "Japanese": "局所性に敏感なハッシング (Kyokusho-sei ni Senshin-na Hashingu)",
    "Russian": "хэширование, чувствительное к локальности"
  },
  {
    "English": "localization",
    "context": "1: This generated category list can then be used by a CLIP image classification module that classifies image regions produced by <mark>localization</mark> and face detection modules. VISPROG's program generator automatically determines whether to use a face detector or an open-vocabulary localizer depending on the context in the natural language instruction. VISPROG also estimates the maximum size of the category list retrieved.<br>2: The sliding window principle treats <mark>localization</mark> as localized detection, applying a classifier function subsequently to subimages within an image and taking the maximum of the classification score as indication for the presence of an object in this region. However, already an image of as low resolution as 320 × 240 contains more than one billion rectangular subimages.<br>",
    "Arabic": "تحديد الموقع",
    "Chinese": "定位",
    "French": "localisation",
    "Japanese": "局所化",
    "Russian": "локализация"
  },
  {
    "English": "location parameter",
    "context": "1: P (Z ≤ z) = G(z) = exp − exp − z − b a , (5 \n ) \n where b is the <mark>location parameter</mark> and a the scale parameter. The probability density function of the Gumbel is: \n<br>2: From this we see that the distribution of the max of N samples drawn from a Gumbel distribution with <mark>location parameter</mark> b and scale parameter a is also a Gumbel distribution with <mark>location parameter</mark>, b max = b + a ln N and scale parameter a max = a.<br>",
    "Arabic": "معلمة الموقع",
    "Chinese": "位置参数",
    "French": "paramètre de localisation",
    "Japanese": "位置パラメータ",
    "Russian": "параметр положения"
  },
  {
    "English": "log",
    "context": "1: for some model π ′ (y|x) and r(x, y) = β <mark>log</mark> π(y|x) πref(y|x) for some model π(y|x), such that π ̸ = π ′ . We then have \n r ′ ( x , y ) = r ( x , y ) + f ( x ) = β <mark>log</mark> π ( y|x ) π ref ( y|x ) + f ( x ) = β <mark>log</mark> π ( y|x ) exp ( 1 β f ( x ) ) π ref ( y|x ) = β <mark>log</mark> π ′ ( y|x<br>2: For example, an EBM trained on language pairs p(x, y) could be used to translate in either direction without retraining and could be structured as <mark>log</mark> p(x, y) = f θ (x) T g φ (y) − <mark>log</mark> Z so that each language component could be trained separately.<br>",
    "Arabic": "لوغاريتم",
    "Chinese": "对数",
    "French": "journal",
    "Japanese": "対数",
    "Russian": "лог"
  },
  {
    "English": "log likelihood",
    "context": "1: Given that each inference operation itself is not a cheap process, the resulting computation can not be performed in a reasonable time. However, we suggest that by using a firstorder approximation to the <mark>log likelihood</mark>, one can efficiently learn a small number of effective features. Similar ideas in other contexts have been proposed by [23,9,13].<br>2: Given topic parameter β, computing the full posterior of ui, vj and θj is intractable. We develop an EMstyle algorithm to learn the maximum a posteriori (MAP) estimates. Maximization of the posterior is equivalent to maximizing the complete <mark>log likelihood</mark> of U , V , θ1:J , and R given λu, λv and β, \n<br>",
    "Arabic": "احتمالية لوغاريتمية",
    "Chinese": "对数似然",
    "French": "log-vraisemblance",
    "Japanese": "対数尤度",
    "Russian": "логарифмическое правдоподобие"
  },
  {
    "English": "log loss",
    "context": "1: In Figure 1 we compare the prediction accuracy of MaxEnt ICE, measured using <mark>log loss</mark>, E a∼σ Γ − log 2σ Γ (a) , against a number of baselines by varying the number of observations sampled from the ε-equilibrium.<br>2: Tur20 ] . Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that <mark>log loss</mark>, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH + 20].<br>",
    "Arabic": "خسارة اللوغاريتم",
    "Chinese": "对数损失",
    "French": "perte logarithmique",
    "Japanese": "ログ損失",
    "Russian": "Логарифмическая потеря"
  },
  {
    "English": "log marginal likelihood",
    "context": "1: The <mark>log marginal likelihood</mark> is of particular interest to us, as the quality of its approximation and our posterior approximation is linked. Its form is \n L = − 1 2 y T K −1 n y − 1 2 log|K n | − N 2 log(2π) , (1 \n ) \n<br>2: We investigate the correlation between the <mark>log marginal likelihood</mark> (LML) and generalization in the context of image classification using the CIFAR-10 and CIFAR-100 datasets. In particular, we consider two tasks: (1) model selection with fixed prior precision, and (2) tuning the prior precision then performing a similar model selection task.<br>",
    "Arabic": "لوغاريتم الاحتمال الهامشي",
    "Chinese": "对数边缘似然",
    "French": "log de vraisemblance marginale",
    "Japanese": "対数周辺尤度",
    "Russian": "логарифм предельной вероятности"
  },
  {
    "English": "log p",
    "context": "1: hallucinating content words ) . As before, the training data includes one example for each prefix of each utterance, so the fine-tuning objective is to maximize the sum of <mark>log p</mark> (u | c, u [m] ) over all prefixes u [m] of all utterances u.<br>",
    "Arabic": "ذيل لوغاريتم الاحتمال",
    "Chinese": "对数概率",
    "French": "log p",
    "Japanese": "ログp",
    "Russian": "лог p"
  },
  {
    "English": "log partition function",
    "context": "1: H(q avg ) = inf ϕ ln Z ϕ − x∈X q avg (x)ϕ(x) , \n where the variable ϕ ranges over all potential functions on X , and Z ϕ = x∈X exp ϕ(x). Applying the Gumbel trick lower bound on the <mark>log partition function</mark> gives \n<br>2: Let q sum (x) := P[x = x * ] be the probability mass function of x * . The following results links together the errors acquired when using summed unary perturbations to upper bound the <mark>log partition function</mark> ln Z ≤ U ( 0 ) using the Gumbel trick upper bound by Hazan & Jaakkola ( 2012 ) , to approximately sample from the Gibbs distribution by using q sum instead , and to upper bound the entropy of the approximate distribution<br>",
    "Arabic": "دالة التجزئة اللوغاريتمية",
    "Chinese": "对数分区函数",
    "French": "fonction de partition logarithmique",
    "Japanese": "対数パーティション関数",
    "Russian": "логарифмическая партиционная функция"
  },
  {
    "English": "log perplexity",
    "context": "1: Moreover, as anticipated, since these words are held out during pre-training, W2W fails to correctly unmask these unseen words, leading to a high <mark>log perplexity</mark> of 11.01 and low HR of 4.2, compared to that of 1.26 and 66.9 on the seen words. Figure 5 shows an example of such word-agnostic grounding.<br>2: In language model evaluation, the commonly used measures for assessing performance are the stan-dard hit-rate-at-k (HR@k) measure and perplexity (Salazar et al., 2020;Jin et al., 2020). In masked language modeling, the <mark>log perplexity</mark> of a word w is defined as the log pseudo-perplexity: \n<br>",
    "Arabic": "- تعقيد السجل",
    "Chinese": "对数困惑度",
    "French": "perplexité logarithmique",
    "Japanese": "ログパープレキシティ",
    "Russian": "логарифмическая перплексия"
  },
  {
    "English": "log probability",
    "context": "1: The phenomenon we observe would be of less concern if the correct label prediction was just an outcome of chance, which could occur when the entropy of the <mark>log probabilities</mark> of the model output is high (suggesting uniform probabilities on entailment, neutral and contradiction labels, recall Model B from §3).<br>2: , l we simply average the <mark>log probabilities</mark> across tokens: \n p(e|t) = 1 l l i=1 p(m i = i |t l ). If k is the maximum number of tokens of any entity e ∈ C gets split into, we consider all templates t 1 , . . .<br>",
    "Arabic": "احتمالات اللوغاريتمية",
    "Chinese": "对数概率",
    "French": "probabilité logarithmique",
    "Japanese": "ログ確率",
    "Russian": "логарифм вероятности"
  },
  {
    "English": "log-likelihood function",
    "context": "1: The local approximation to the normal <mark>log-likelihood function</mark> in a neighbourhood of H 0 for observation t is \n l t = − 1 2 ln(2π) − 1 2 ρ + q i=1 ρ i x i,t − ε 2 t 2 exp(ρ + q i=1 ρ i x i,t ) . (12 \n ) \n<br>2: L(Ψ|D, Ω) = L(Ψ|D) − λ 2 • R(Ψ|Ω)(4) \n The first component L is the <mark>log-likelihood function</mark> in Equation 2 , which reflects the global consistency between the latent parameters Ψ and the observation D. The second component R is a regularization function , which reflects the local consistency between the latent parameters Ψ of neighboring documents in the manifold Ω. λ is the regularization parameter , commonly found in manifold learning algorithms<br>",
    "Arabic": "دالة الاحتمال اللوغاريتمية",
    "Chinese": "对数似然函数",
    "French": "fonction de vraisemblance logarithmique",
    "Japanese": "対数尤度関数",
    "Russian": "функция логарифмического правдоподобия"
  },
  {
    "English": "log-likelihood loss",
    "context": "1: The targetx τ can be seen as an \"expert\" on task τ so that BMG is a form of distillation (Hinton et al., 2015). The <mark>log-likelihood loss</mark> used by MG is also a KL divergence, but w.r.t. a \"cold\" expert that places all mass on the true label.<br>2: Framing the problem as a binary classification we have the negative <mark>log-likelihood loss</mark>: \n L R (r ϕ , D) = −E (x,yw,y l )∼D log σ(r ϕ (x, y w ) − r ϕ (x, y l )) (2 \n ) \n where σ is the logistic function.<br>",
    "Arabic": "خسارة احتمال السجل",
    "Chinese": "负对数似然损失",
    "French": "perte de log-vraisemblance",
    "Japanese": "対数尤度損失",
    "Russian": "Потеря логарифма правдоподобия"
  },
  {
    "English": "log-likelihood ratio",
    "context": "1: The contribution from each record field (its weight) to the total match score is equal to the <mark>log-likelihood ratio</mark> for the two hypotheses (high values correspond to likely duplicates): \n W jk = log 2 p jk pj p k (1) \n<br>2: and we can calculate <mark>log-likelihood ratio</mark> based weights W (d) by integrating ( 9) and ( 10) over an interval corresponding to the precision of d (for two observed ages, for example, over d±1 years) and taking the logarithm of the ratio of integrals.<br>",
    "Arabic": "نسبة احتمال السجل",
    "Chinese": "对数似然比",
    "French": "rapport de log-vraisemblance",
    "Japanese": "対数尤度比",
    "Russian": "логарифмическое отношение правдоподобия"
  },
  {
    "English": "log-linear model",
    "context": "1: Following the work of Chiang ( 2007), we assign scores to derivations with a <mark>log-linear model</mark>, which are essentially weighted products of feature values. For generality , we only consider the following four simple features in this work : 1.p ( h w |p λ ) : the relative frequency estimate of a hybrid sequence h w given the λ-production p λ ; 2.p ( p λ |h w , τ ) : the relative frequency estimate of a λ-production p λ given the phrase h w and<br>2: Lambert [1992] proposed zero-inflated-Poisson (ZIP) models that address the mixture of excess zeros and Poisson count process. The mixture is indicated by the latent binary variable d n using a logit model and the density for the Poisson count given by the <mark>log-linear model</mark>. Thus, \n<br>",
    "Arabic": "نموذج لوغاريتمي خطي",
    "Chinese": "对数线性模型",
    "French": "modèle log-linéaire",
    "Japanese": "対数線形モデル",
    "Russian": "логлинейная модель"
  },
  {
    "English": "log-linear translation model",
    "context": "1: We use cdec (Dyer et al., 2010; to build symmetric word-alignments and extract rules for hierarchical phrasebased translation (Chiang, 2007). Our baseline system uses a standard set of features in a <mark>log-linear translation model</mark>.<br>",
    "Arabic": "نموذج الترجمة اللّوغاريتمي الخطي",
    "Chinese": "对数线性翻译模型",
    "French": "modèle de traduction log-linéaire",
    "Japanese": "対数線形翻訳モデル",
    "Russian": "модель лог-линейного перевода"
  },
  {
    "English": "log-log plot",
    "context": "1: We use the term Densification Power Law plot (or just DPL plot) to refer to the <mark>log-log plot</mark> of number of edges e(t) versus number of nodes n(t).<br>",
    "Arabic": "رسم لوغاريتمي مزدوج",
    "Chinese": "对数-对数图",
    "French": "graphique log-log",
    "Japanese": "log-log プロット",
    "Russian": "log-log график"
  },
  {
    "English": "log-normal distribution",
    "context": "1: Work by Serrano et al. (2009) suggests that a <mark>log-normal distribution</mark> is appropriate for modeling document lengths. Thus, for each of the 20 chosen hotels, we select 20 truthful reviews from a log-normal (lefttruncated at 150 characters) distribution fit to the lengths of the deceptive reviews.<br>2: Recall that the <mark>log-normal distribution</mark> L(µ r , σ r ) describes the conditional inflation for a given sentence with a head entity and a tail entity. If the median e µr of L(µ r , σ r ) has a high value, the distantly supervised label is likely to be a false label.<br>",
    "Arabic": "توزيع لوغاريتمي طبيعي",
    "Chinese": "对数正态分布",
    "French": "distribution log-normale",
    "Japanese": "対数正規分布",
    "Russian": "логнормальное распределение"
  },
  {
    "English": "log-odd score",
    "context": "1: Importantly, the XRAND explanation evidently manifests similar sets of important malware and goodware features as the original explanation by SHAP, which also explains the comparable <mark>log-odds score</mark> in Fig. 4. More visualizations on XRAND can be found in Appx. F.<br>2: 4 shows the <mark>log-odds score</mark> of the original explanations returned by SHAP and of the ones after applying our XRAND mechanism. The XRAND explanations at ε = 1.0, 10.0 have comparable <mark>log-odds score</mark>s to those of SHAP. This is because our defense works with small values of k (e.g., k = 10).<br>",
    "Arabic": "درجة لوغاريتم الرجحان",
    "Chinese": "对数几率分数",
    "French": "score log-impair",
    "Japanese": "対数オッズスコア",
    "Russian": "оценка логарифма шансов"
  },
  {
    "English": "log-prob",
    "context": "1: Inference in our model is done by taking the vertex with the highest <mark>log-prob</mark> at each time step of the RNN. This allows for a simple annotation interface: the annotator can correct the prediction at any time step, and we feed in the corrected vertex to the next time-step of the RNN (instead of the prediction).<br>",
    "Arabic": "احتمالية لوغاريتمية",
    "Chinese": "对数概率",
    "French": "log-vraisemblance",
    "Japanese": "対数確率",
    "Russian": "лог-вероятность"
  },
  {
    "English": "log-sum-exp",
    "context": "1: linear , and since <mark>log-sum-exp</mark> is convex the result follows . Therefore, by Theorem 9, the LTS loss of the product mixing of members of the canonical exponential family is convex in their natural parameters.<br>2: We note that the parameter/data coefficients are both lower than 1 2 ; this is expected for the data-efficiency coefficient (but far from the known lower-bound). Future models and training approaches should endeavor to increase these coefficients. Fitting the decomposition to data. We effectively minimize the following problem min , , , , \n where is the <mark>log-sum-exp</mark> operator.<br>",
    "Arabic": "الصيغة log-sum-exp",
    "Chinese": "对数指数和",
    "French": "log-somme-exp",
    "Japanese": "ログ-サム-エクスポネンシャル",
    "Russian": "лог-сумма-эксп"
  },
  {
    "English": "logical connective",
    "context": "1: All the bound variables (e.g., x in λx.state(x)) which do not convey semantics are removed, but free variables (e.g., state in λx.state(x)) which might convey semantics are left intact. Quantifiers and <mark>logical connectives</mark> are also left intact.<br>",
    "Arabic": "رابط منطقي",
    "Chinese": "逻辑连接词",
    "French": "connecteur logique",
    "Japanese": "論理結合子",
    "Russian": "логическая связка"
  },
  {
    "English": "logical form",
    "context": "1: We then determine whether the sentence evaluates to the <mark>logical form</mark> using only the type and semantic correspondence matrices, using types to assign the sentence an abstract <mark>logical form</mark>, and correspondences to determine whether it matches the target. Definition 3. Denote the lexical representation L(x) = (R 1 (x), . . .<br>2: The first part of a derivation d = y, M is a CCG parse tree y with an underspecified <mark>logical form</mark> u at its root. For example, Figure 3a shows such a CCG parse tree, where the <mark>logical form</mark> contains the placeholders REL, REL-of and ID.<br>",
    "Arabic": "الشكل المنطقي",
    "Chinese": "逻辑形式",
    "French": "forme logique",
    "Japanese": "論理形式",
    "Russian": "логическая форма"
  },
  {
    "English": "logistic function",
    "context": "1: In line with the advertising consumer behavior studies, we adopt the <mark>logistic function</mark> to take into account the impression counts of an ad (placed at different billboards) to a user trajectory when defining the influence measurement.<br>2: The second challenge is posed by the property of the <mark>logistic function</mark>. The influence model based on the <mark>logistic function</mark> is non-submodular, which means any straightforward greedy-based approach is not applicable to address the ICOA problem (as elaborated in Section 3). Even worse, the non-uniform cost of different billboards makes the optimization problem intricate.<br>",
    "Arabic": "الدالة اللوجستية",
    "Chinese": "逻辑函数",
    "French": "fonction logistique",
    "Japanese": "ロジスティック関数",
    "Russian": "логистическая функция"
  },
  {
    "English": "logistic loss",
    "context": "1: We use the <mark>logistic loss</mark> ℓ(θ; (x, y)) = log(1 + exp(−yθ ⊤ x)). We compare the performance of different constraint sets Θ by taking \n Θ = θ ∈ R d : a 1 θ 1 + a 2 θ 2 ≤ r , \n which is equivalent to elastic net regularization [ 52 ] , while varying a 1 , a 2 , and r. We experiment with ℓ 1 -constraints ( a 1 = 1 , a 2 = 0 ) with r ∈ { 50 , 100 , 500 , 1000 , 5000 } , ℓ 2 -constraints ( a 1 = 0 , a 2 = 1 ) with r ∈ { 5 , 10 , 50 , 100 , 500 } , elastic net ( a 1 = 1 , a 2 = 10 ) with r ∈ { 100 , 200 , 1000 , 2000 , 10000 } , our robust regularizer with ρ ∈ { 100 , 1000 , 10000 , 50000 , 100000 } and<br>2: Then, we observe the label yt ∈ {0, 1}, and suffer the resulting LogLoss (<mark>logistic loss</mark>), given as \n t(wt) = −yt log pt − (1 − yt) log(1 − pt),(1) \n<br>",
    "Arabic": "الخسارة اللوجستية",
    "Chinese": "逻辑损失",
    "French": "perte logistique",
    "Japanese": "ロジスティック損失",
    "Russian": "логистическая потеря"
  },
  {
    "English": "logistic regression",
    "context": "1: Team UAIC1860 (Ermurachi and Gifu, 2020)(SI: 28, TC: 26) used traditional text representation techniques: character n-grams, word2vec embeddings, and TF.IDF-weighted word-based features. For both subtasks, these features were used in a Random Forest classifier. Additional experiments with Naïve Bayes, <mark>Logistic Regression</mark> and SVMs yielded worse results.<br>2: We use scikit-learn's (Pedregosa et al., 2011) implementation of <mark>Logistic Regression</mark> along with TFIDF-based Bag-of-Words features. We add L2 regularization to the model with a regularization weight of 1.0 and train the model using L-BFGS. In our experiments, the LR model uses only the title (and not the article body) as the input.<br>",
    "Arabic": "الانحدار اللوجستي",
    "Chinese": "逻辑回归",
    "French": "régression logistique",
    "Japanese": "ロジスティック回帰",
    "Russian": "логистическая регрессия"
  },
  {
    "English": "logistic regression classifier",
    "context": "1: If there are less than 50 tokens before the adverb, we simply extract all of these tokens. In preliminary testing using a <mark>logistic regression classifier</mark>, we found that limiting the size to 50 tokens had higher accuracy than 25 or 100 tokens.<br>2: . Figure 6 shows the accuracy of the <mark>logistic regression classifier</mark> using each of the feature sets for the affective and cognitive categories, and for the positive-emotion, negativeemotion, and insight subcategories. Network properties alone provide significantly better predictions of the psychology of the decision-makers than the price changes alone.<br>",
    "Arabic": "مصنف الانحدار اللوجستي",
    "Chinese": "逻辑回归分类器",
    "French": "classificateur de régression logistique",
    "Japanese": "ロジスティック回帰分類器",
    "Russian": "логистический регрессионный классификатор"
  },
  {
    "English": "logistic regression model",
    "context": "1: The first step in our analysis is to bring these insights together into a single <mark>logistic regression model</mark> -the lifespan model -and assess their predictive power on real data.<br>",
    "Arabic": "نموذج الانحدار اللوجستي",
    "Chinese": "逻辑回归模型",
    "French": "modèle de régression logistique",
    "Japanese": "ロジスティック回帰モデル",
    "Russian": "логистическая регрессионная модель"
  },
  {
    "English": "logit",
    "context": "1: too many tuples ( X , y , t ) thanks to ( E.1 ) , and when this happens we have a naive bound − log <mark>logit</mark> y ( F ( t ) , X ) ∈ [ 0 , O ( 1 ) ] using Claim D.4 .<br>2: We separately treat t ≤ T (X,y)∼Zm E 2,i,r (X) + E 1 + E 3 + E 4,j, (X) 1 v i,1 ,v i,2 ∈V(X) 1 s(X) − <mark>logit</mark> τ i (F, X) \n + Using the property that v i,1 , v i,2 ∈ V ( X ) with probability Θ ( s k ) over a sample ( X , y ) ∈ Z m , and using the trivial bound + O ( η ) E ( X , y ) ∼Zm E 2 , i , r ( X ) + E 1 +<br>",
    "Arabic": "لوجت",
    "Chinese": "logit值",
    "French": "logit",
    "Japanese": "ロジット",
    "Russian": "логит"
  },
  {
    "English": "logit model",
    "context": "1: Generalized linear models (GLMs) have been applied for confidence estimation in speech recognition (Siu and Gish, 1999). The <mark>logit model</mark>, which models the log odds of an event as a linear function of the features, can be used in confidence estimation. The confidence  \n<br>2: where Z, U are the learnable parameters in the splitting internal supernodes and the leaves of the tree ensemble for the <mark>logit model</mark> for π n and W, O are the learnable parameters in the supernodes and the leaves of the tree ensemble for the log-tree model for µ n respectively. The likelihood function for this ZIP model is given by \n<br>",
    "Arabic": "نموذج اللوجت",
    "Chinese": "对数几率模型",
    "French": "modèle de régression logistique",
    "Japanese": "ロジットモデル",
    "Russian": "логит-модель"
  },
  {
    "English": "long-range dependency",
    "context": "1: Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity O(L log L), initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing <mark>long-range dependencies</mark> (Gu et al., 2020).<br>2: Therefore, controlling the curvature everywhere grants an upper bound on how 'long' the <mark>long-range dependencies</mark> can be. Moreover, we can also adapt the volume growth results in Paeng (2012) showing that positive curvature everywhere prevents a too fast expansion of the r-hop for r sufficiently large. Curvature and over-squashing.<br>",
    "Arabic": "اعتمادية بعيدة المدى",
    "Chinese": "长程依赖",
    "French": "dépendance à longue portée",
    "Japanese": "長距離依存性",
    "Russian": "долговременная зависимость"
  },
  {
    "English": "lookup table",
    "context": "1: The lexicon L is a <mark>lookup table</mark> that deterministically maps an input token x i to an output token L(x i ), and we modify the distribution for multiset tagging as follows: \n<br>2: For the i th word in the sentence, if the predicate is at position pos p we use an additional <mark>lookup table</mark> LT distp (i − pos p ).<br>",
    "Arabic": "جدول البحث",
    "Chinese": "查找表",
    "French": "table de consultation",
    "Japanese": "検索テーブル",
    "Russian": "таблица поиска"
  },
  {
    "English": "loop closure",
    "context": "1: [16] presented a simple visual odometry system using a DVS camera with <mark>loop closure</mark> built on top of the SeqSLAM algorithm using events accumulated into frames [17]. In a much more constrained and hardware-dependent setup, Schraml et al.<br>2: We use 3D positions of objects to generate the node set and the nearest neighbor search to generate the edge set. We use fences and vegetation to construct DFs. The ground-truth <mark>loop closure</mark> is obtained based on the ground-truth poses provided by the KITTI odometry dataset.<br>",
    "Arabic": "إغلاق الحلقة",
    "Chinese": "闭环检测",
    "French": "fermeture de boucle",
    "Japanese": "ループクロージャ",
    "Russian": "замыкание петли"
  },
  {
    "English": "loopy belief propagation",
    "context": "1: This experiment shows that as an ensemble method, coordination classification performs competitively in this case. An advantage of coordination classification is that it only needs to learn a single base classifier, as opposed to the multiple training episodes required by boosting. The need to run <mark>loopy belief propagation</mark> on the output labels is a disadvantage however.<br>2: For more complex structures, heuristic inference methods such as <mark>loopy belief propagation</mark> and variational inference have shown some success in practice. However, the learning algorithms generally assume exact inference and their behavior in the context of heuristic inference is not well understood. Copyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org).<br>",
    "Arabic": "انتشار الاعتقاد الحلقي",
    "Chinese": "循环信念传播",
    "French": "propagation des croyances en boucle",
    "Japanese": "ループ信念伝播",
    "Russian": "циклическое распространение убеждений"
  },
  {
    "English": "loss",
    "context": "1: DPO is relatively straightforward to implement; PyTorch code for the DPO <mark>loss</mark> is provided below: import torch.nn.functional as F def dpo_<mark>loss</mark>(pi_logps, ref_logps, yw_idxs, yl_idxs, beta): \n<br>2: First, consider a location estimation problem in which we wish to estimate the minimizer of some the expectation of a <mark>loss</mark> of the form ℓ(θ, X) = h(θ − X), where h : R d → R is convex and symmetric about zero.<br>",
    "Arabic": "دالة الخسارة",
    "Chinese": "损失",
    "French": "perte",
    "Japanese": "損失",
    "Russian": "функционал потерь"
  },
  {
    "English": "loss distribution",
    "context": "1: However, on FashionMNIST, the baseline has lower advantage scores. We suspect this is because FashionMNIST images are greyscale and synthetic data contain more features that prone to be memorized by networks. Loss distribution in Figure 8 in Appendix E.2 also demonstrates that synthetic data with real data initialization can still leak membership privacy.<br>",
    "Arabic": "توزيع الخسارة",
    "Chinese": "损失分布",
    "French": "distribution des pertes",
    "Japanese": "損失分布",
    "Russian": "распределение потерь"
  },
  {
    "English": "loss function",
    "context": "1: Nonetheless, our general consistency result, Theorem 6, implicitly handles order-preserving probability spaces, which assume that graphs G contain all results and the loss L(f (x, q), G) = 0 if f agrees with G on all orderings and is 1 otherwise.<br>2: . The <mark>loss function</mark> becomes \n L i,p,n = max(0, ||X i − C p || 2 − ||X i − C n || 2 + α|p − n|) \n where |p − n| causes the margin to scale with the distance between classes p and n.<br>",
    "Arabic": "وظيفة الخسارة",
    "Chinese": "损失函数",
    "French": "fonction de perte",
    "Japanese": "損失関数",
    "Russian": "функция потерь"
  },
  {
    "English": "loss landscape",
    "context": "1: Moreover, we still do not understand why various pre-training methodology manifests in universally useful representations, although recent line of works have attempted to cover this gap by looking at <mark>loss landscapes</mark>, and the learned linguistic properties of pre-trained models (Hao et al., 2019;Clark et al., 2019a).<br>",
    "Arabic": "مشهد الخسارة",
    "Chinese": "损失景观",
    "French": "paysage de perte",
    "Japanese": "損失地形",
    "Russian": "ландшафт потерь"
  },
  {
    "English": "loss minimization",
    "context": "1: Our first guarantee depends on the covering numbers of the function class F as we describe in Section 2.2.2. While we state our results abstractly, in the <mark>loss minimization</mark> setting we typically consider the function class F := {ℓ(θ, •) : θ ∈ Θ} parameterized by θ.<br>2: On the other hand, AdaBoost.OL is derived by viewing boosting from a different angle: <mark>loss minimization</mark> [Mason et al., 2000, Schapire andFreund, 2012]. The theory of online <mark>loss minimization</mark> is the key tool for developing AdaBoost.OL.<br>",
    "Arabic": "تقليل الخسارة",
    "Chinese": "损失最小化",
    "French": "minimisation de la perte",
    "Japanese": "損失最小化",
    "Russian": "минимизация потерь"
  },
  {
    "English": "loss term",
    "context": "1: The variance depends on the gradients of each <mark>loss term</mark> L t with respect to each of the per-timestep parameters θ τ . To gain insight into the structure of these gradients, we can arrange them in a matrix: \n<br>2: On the other hand, the <mark>loss term</mark> is minimized when A is updated to exactly satisfy the target distance specified at the current time step. Hence, the <mark>loss term</mark> has a tendency to satisfy target distances for recent examples.<br>",
    "Arabic": "مصطلح الخسارة",
    "Chinese": "损失项",
    "French": "terme de perte",
    "Japanese": "損失項",
    "Russian": "функция потерь"
  },
  {
    "English": "lossy compression",
    "context": "1: Digital images, due to their big memory size, are often stored in a more compact form obtained with <mark>lossy compression</mark> algorithms (JPEG being the most popular). It often introduces visible image artefacts: For instance, bloc effects are classical JPEG drawbacks.<br>",
    "Arabic": "ضغط ذو فقدان",
    "Chinese": "有损压缩",
    "French": "compression avec perte",
    "Japanese": "損失圧縮",
    "Russian": "сжатие с потерями"
  },
  {
    "English": "lottery ticket hypothesis",
    "context": "1: a Bayes optimal classifier as the width grows , and as long as the right signature is present in the nodes at initialization , the SGD will ballistically converge to a global minimizer of the population loss . This is a rigorous example of the well-known <mark>lottery ticket hypothesis</mark> of [30].<br>2: Assumption 4.1 will ensure that each (i, 1) and (i, 2) will belong to M F with relatively equal probability. If we train two models F and G, their combined lottery winning set M F ∪ M G shall be of cardinality around 3 2 k(1 − o(1)).<br>",
    "Arabic": "فرضية تذكرة اليانصيب",
    "Chinese": "彩票假说",
    "French": "hypothèse du billet de loterie",
    "Japanese": "宝くじ仮説",
    "Russian": "гипотеза лотерейного билета"
  },
  {
    "English": "low rank",
    "context": "1: x∈R N D (x) + λR (x) , (1 \n ) \n where D is a data-fidelity term that ensures consistency between the reconstructed image and measured data. R is a regularizer that imposes certain prior knowledge, e.g. smoothness ( Osher et al. , 2005 ; Ma et al. , 2008 ) , sparsity ( Yang et al. , 2010 ; Liao & Sapiro , 2008 ; Ravishankar & Bresler , 2010 ) , <mark>low rank</mark> ( Semerci et al. , 2014 ; Gu et al. , 2017 ) and nonlocal self-similarity ( Mairal et al. , 2009 ; Qu et al.<br>",
    "Arabic": "رتبة منخفضة",
    "Chinese": "低秩",
    "French": "de bas rang",
    "Japanese": "低ランク",
    "Russian": "низкого ранга"
  },
  {
    "English": "low rank approximation",
    "context": "1: In the following, we will derive a factorization model for the transition cube A. That means we model the unobserved transition tensor A by a <mark>low rank approximation</mark>Â. The advantage of this approach over a full parametrization is that it can handle sparsity and generalizes to unobserved  \n (i ∈ B u t |l ∈ B u t−1 ) \n .<br>",
    "Arabic": "التقريب ذو المرتبة المنخفضة",
    "Chinese": "低秩近似",
    "French": "approximation de faible rang",
    "Japanese": "低ランク近似",
    "Russian": "низкоранговое приближение"
  },
  {
    "English": "low-data regime",
    "context": "1: A main challenge of the Cityscapes dataset is training models in a <mark>low-data regime</mark>, particularly for the categories of truck, bus, and train, which have about 200-500 train- ing samples each. To partially remedy this issue, we further report a result using COCO pre-training.<br>",
    "Arabic": "نظام البيانات المنخفضة",
    "Chinese": "少量数据环境",
    "French": "régime de données limitées",
    "Japanese": "少量データ領域",
    "Russian": "режим с малым количеством данных"
  },
  {
    "English": "low-dimensional embedding",
    "context": "1: Furthermore, by incorporating structure preserving constraints into existing nonlinear dimensionality reduction algorithms, these methods can explicitly preserve graph topology in addition to local distances, and produce more accurate <mark>low-dimensional embeddings</mark>.<br>2: To the best of our knowledge, is has not yet been demonstrated that <mark>low-dimensional embeddings</mark> are a feasible representation for photo-realistic, general 3D environments. Recent work in meta-learning could enable generalization across scenes without the limitation to a highly low-dimensional manifold [17].<br>",
    "Arabic": "التضمين منخفض الأبعاد",
    "Chinese": "低维嵌入",
    "French": "\"plongement de faible dimension\"",
    "Japanese": "低次元埋め込み",
    "Russian": "низкоразмерное вложение"
  },
  {
    "English": "low-dimensional representation",
    "context": "1: This is not the case for MVE+SP which is maintaining (or even helping) classification rates, further reinforcing the strength of the structure preserving constraints to create accurate <mark>low-dimensional representations</mark> of data (without any knowledge of labels or a classification task).<br>2: To learn task-relevant <mark>low-dimensional representations</mark> of pose trajectories, we train a network jointly on (1) reconstruction of the input trajectory (Section 3.1) and (2) expert-programmed decoder tasks (Section 3.3). The learned representation can then be used as input to behavior modeling tasks, such as behavior classification.<br>",
    "Arabic": "تمثيل منخفض الأبعاد",
    "Chinese": "低维表示",
    "French": "représentation de faible dimension",
    "Japanese": "低次元表現",
    "Russian": "низкоразмерное представление"
  },
  {
    "English": "low-pass filter",
    "context": "1: Unlike spatial pooling that requires integer strides, spectral pooling only requires integer output dimensions, which allows for much more fine-grained downsizing. Moreover, spectral pooling acts as a <mark>low-pass filter</mark> over the entire input, only keeping the lower frequencies i.e. the most relevant information in general and avoiding aliasing (Zhang, 2019).<br>",
    "Arabic": "مرشح تمرير منخفض",
    "Chinese": "低通滤波器",
    "French": "filtre passe-bas",
    "Japanese": "低域通過フィルタ",
    "Russian": "низкочастотный фильтр"
  },
  {
    "English": "low-rank factorization",
    "context": "1: All HiPPO matrices from [16] have a NPLR representation \n A = V ΛV * − P Q = V (Λ − (V * P ) (V * Q) * ) V *(6) \n for unitary V ∈ C N ×N , diagonal Λ, and <mark>low-rank factorization</mark> P , Q ∈ R N ×r .<br>2: Here || • || can in general be any matrix norm, but in this work we consider the 1-norm, \n ||A|| 1 = i,j |a ij |,(2) \n in particular. The calculation of a <mark>low-rank factorization</mark> of a matrix is a fundamental operation in many computer vision applications.<br>",
    "Arabic": "تصنيع عوامل قليلة الرتبة",
    "Chinese": "低秩分解",
    "French": "factorisation de rang faible",
    "Japanese": "低ランク分解",
    "Russian": "низкоранговая факторизация"
  },
  {
    "English": "low-rank matrix approximation",
    "context": "1: Now, finding the full 3D-reconstruction of this scene can be posed as a <mark>low-rank matrix approximation</mark> task. In addition, as we are considering robust approximation in this work, we also included outliers to the problem by adding uniformly distributed noise [−50, 50] to 10% of the tracked points.<br>2: In this paper we have studied the problem of <mark>low-rank matrix approximation</mark> in the presence of missing data. We have proposed a method for solving this task under the robust L 1 norm which can be interpreted as a generalization of the standard Wiberg algorithm.<br>",
    "Arabic": "تقريب المصفوفة منخفضة الرتبة",
    "Chinese": "低秩矩阵近似",
    "French": "approximation de matrice de rang faible",
    "Japanese": "低ランク行列近似",
    "Russian": "низкоранговое приближение матрицы"
  },
  {
    "English": "lower bind",
    "context": "1: We can <mark>lower bound</mark> (i) by the same idea of our earlier proof for (72), except that we replace W ( ) in that proof with W ( ) ; this gives us \n (i) W (k) − W ( ) 2 F κ w,cross W 2 max .<br>2: These subproblems are obtained by computing either upper or <mark>lower bound</mark> approximations of the cost functions or constraining functions.<br>",
    "Arabic": "الحد السفلي",
    "Chinese": "下界",
    "French": "borne inférieure",
    "Japanese": "下限値",
    "Russian": "нижняя граница"
  },
  {
    "English": "m-estimation",
    "context": "1: Because the IRLS / robust <mark>M-estimation</mark> loop of the RBS is sensitive to initialization, we can improve performance by setting the initial weights used in the IRLS loop c init to reflect some noise model computed from the input to the solver.<br>2: For completeness, we provide a standard derivation of the influence function I up,params in the context of loss minimization (<mark>M-estimation</mark>). This derivation is based on asymptotic arguments and is not fully rigorous; see van der Vaart (1998) and other statistics textbooks for a more thorough treatment. Recall thatθ minimizes the empirical risk: \n<br>",
    "Arabic": "تقدير م",
    "Chinese": "M-估计",
    "French": "m-estimation",
    "Japanese": "M推定法",
    "Russian": "M-оценка"
  },
  {
    "English": "m-step",
    "context": "1: β kw ∝ j n φ jnk 1[wjn = w]. (12 \n ) \n Note this is the same <mark>M-step</mark> update for topics as in LDA [7].<br>2: The basic idea of HMRF-KMEANS is as follows: in the E-step, given the current cluster representatives, every data point is re-assigned to the cluster which minimizes its contribution to J obj . In the <mark>M-step</mark>, the cluster repre- Algorithm: HMRF-KMeans Input: Set of data points \n X = { x i } N i=1 , number of clusters K , set of must-link constraints M = { ( x i , x j ) } , set of can not -link constraints C = { ( x i , x j ) } , distance measure D , constraint violation costs W and W. Output : Disjoint K-partitioning { X<br>",
    "Arabic": "خطوة الـM",
    "Chinese": "M步",
    "French": "étape M",
    "Japanese": "Mステップ",
    "Russian": "Шаг-М"
  },
  {
    "English": "mIoU",
    "context": "1: We computed baseline overlap statistics for unrelated word pairs and all head-dependent pairs. Unsurprisingly, both baselines show moderate similarity and no dominance (43-48 <mark>mIoU</mark>, ∆ ≤ 1; rows 1-2).<br>2: DAAM largely outperforms both unsupervised baselines (rows 6-7) by a margin of 6-27 points (except for STEGO on COCO-Gen <mark>mIoU</mark> ∞ , where it's similar), likely because we assume the prompts to be provided.<br>",
    "Arabic": "mIoU",
    "Chinese": "平均交并比",
    "French": "mIoU",
    "Japanese": "平均IoU",
    "Russian": "mIoU"
  },
  {
    "English": "mT5",
    "context": "1: Transformer-based massively multilingual language models (mmLMs), such as mBERT (Devlin et al., 2019), XLM-R (Conneau et al., 2020a), and <mark>mT5</mark> (Xue et al., 2021), have substantially advanced multilingual NLP. These models have enabled rapid development of language technologies * These authors contributed equally.<br>2: Implementation details We implement our framework and all baselines within the Transformers (Wolf et al., 2019) and Adapter-Transformers (Pfeiffer et al., 2020b) library. We mainly use mBART (mBART-large-50, mBART-50-large-NMT) and <mark>mT5</mark> (<mark>mT5</mark>-small, <mark>mT5</mark>-base) for our base pretrained multilingual models.<br>",
    "Arabic": "mT5",
    "Chinese": "mT5",
    "French": "mT5",
    "Japanese": "mT5",
    "Russian": "mT5"
  },
  {
    "English": "machine comprehension",
    "context": "1: • Question Answering (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • <mark>Machine Comprehension</mark> (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer.<br>",
    "Arabic": "فهم الآلة",
    "Chinese": "机器理解",
    "French": "compréhension de machine",
    "Japanese": "機械理解",
    "Russian": "машинное понимание"
  },
  {
    "English": "machine learning",
    "context": "1: Notice how this is exactly what Kernel-based learning supports, whereas the combination of the different Kernel functions can be easily made a Kernel function itself (Shawe-Taylor and Cristianini, 2004). Kernel functions are used to capture specific aspects of the semantic relatedness between two tweets and are easily integrated in various <mark>Machine Learning</mark> algorithms, such as SVM.<br>2: Research supported by the Alberta Ingenuity Centre for <mark>Machine Learning</mark>, NSERC, MITACS, and the Canada Research Chairs program.<br>",
    "Arabic": "تعلم الآلة",
    "Chinese": "机器学习",
    "French": "apprentissage automatique",
    "Japanese": "機械学習",
    "Russian": "машинное обучение"
  },
  {
    "English": "machine learning algorithm",
    "context": "1: The results just shown demonstrate that Auto-WEKA is effective at optimizing its given objective function; however, this is not sufficient to allow us to conclude that it fits models that generalize well. As the hypothesis space of a <mark>machine learning algorithm</mark> grows, so does its potential for overfitting.<br>2: A 360 degrees single PAL camera-based system is presented in [8], where authors provide both the driver's face pose and eye status and the driver's viewing scene basing on a <mark>machine learning algorithm</mark> for object tracking.<br>",
    "Arabic": "خوارزمية التعلم الآلي",
    "Chinese": "机器学习算法",
    "French": "algorithme d'apprentissage automatique",
    "Japanese": "機械学習アルゴリズム",
    "Russian": "алгоритм машинного обучения"
  },
  {
    "English": "machine learning classifier",
    "context": "1: This paper demonstrates the link between such community norms and the lifecycle of the individual user, showing how users are most sensitive to new norms at early stages of their career. Our work also draws on a study that showed how new users change their language after joining a community , and demonstrated that a <mark>machine learning classifier</mark> could be trained to predict how long a user had been in a community , given linguistic features like self-introductions , references to other members , or mentions of the name of the forum [ 34 ]<br>2: This paper presents Outguard, an open-source 2 cryptojacking detection system that uses a <mark>machine learning classifier</mark> to identify cryptojacking at scale. As a first step to building Outguard, we systematically constructed a labeled dataset by scanning the Alexa Top 1M domains with a specially instrumented browser and recording each site's resources, provenance, and JavaScript execution traces.<br>",
    "Arabic": "\"مصنف التعلم الآلي\"",
    "Chinese": "机器学习分类器",
    "French": "classificateur d'apprentissage automatique",
    "Japanese": "機械学習分類器",
    "Russian": "машинный классификатор"
  },
  {
    "English": "machine learning model",
    "context": "1: The group DRO literature is motivated by applications where the distributions correspond to deployment domains or protected demographics that a <mark>machine learning model</mark> should avoid spuriously linking to labels [24,50,51].<br>2: The remainder of this paper is organized as follows. Section 2 introduces the <mark>machine learning model</mark> at the core of the Prta system. Section 3 sketches the full architecture of Prta, with focus on the process of collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples.<br>",
    "Arabic": "نموذج التعلم الآلي",
    "Chinese": "机器学习模型",
    "French": "modèle d'apprentissage automatique",
    "Japanese": "機械学習モデル",
    "Russian": "модель машинного обучения"
  },
  {
    "English": "machine learning repository",
    "context": "1: We use two datasets to evaluate our method : the adult income dataset from the UCI <mark>Machine Learning Repository</mark> ( Dheeru and Karra Taniskidou , 2017 ) , where the task is to predict whether an individual earns more than $ 50k per year ( i.e. , whether their occupation is `` high status '' ) , and a dataset of online biographies ,<br>2: In order to make a fair comparison between the standard algorithms and our proposal, we have selected a set of 27 problems from the UCI <mark>Machine Learning Repository</mark>. For estimating the storage reduction and generalization error we used a 10-fold crossvalidation (cv) method. The evaluation of a certain feature selection algorithm is not a trivial task.<br>",
    "Arabic": "مستودع تعلم الآلة",
    "Chinese": "机器学习存储库",
    "French": "dépôt d'apprentissage automatique",
    "Japanese": "機械学習リポジトリ",
    "Russian": "репозиторий машинного обучения"
  },
  {
    "English": "machine learning system",
    "context": "1: An important aspect of scalable machine learning is managing the scale of the installation, encompassing all of the configuration, developers, code, and computing resources that make up a <mark>machine learning system</mark>. An installation comprised of several teams modeling dozens of domain specific problems requires some overhead.<br>",
    "Arabic": "نظام التعلم الآلي",
    "Chinese": "机器学习系统",
    "French": "système d'apprentissage automatique",
    "Japanese": "機械学習システム",
    "Russian": "система машинного обучения"
  },
  {
    "English": "machine reading",
    "context": "1: (2020) and Kaushik and Lipton (2018) analyse the difficulty of various <mark>machine reading</mark> datasets, and Manjunatha et al. (2018) show that visual QA models memorize common question-answer relationships in training data. Févry et al. (2020) analyse various closed-book models' TriviaQA predictions. Kwiatkowski et al.<br>2: (2019) note that the <mark>machine reading</mark> NaturalQuestions dataset has train-test overlap of Wikipedia titles, and provide baselines for \"long-answer\" QA. Verga et al. (2020) observe answer overlap effects in a related modality (knowledgebase QA), but no not consider question overlap.<br>",
    "Arabic": "قراءة الآلة",
    "Chinese": "机器阅读",
    "French": "lecture automatique",
    "Japanese": "機械読解",
    "Russian": "машинное чтение"
  },
  {
    "English": "machine reading comprehension",
    "context": "1: Existing event extraction systems, which usually adopt a supervised learning paradigm, have to rely on labelled training data, but the scarcity of high-quality training data is a common problem [40,31]. <mark>Machine Reading Comprehension</mark> (MRC) tasks extract a span from text [41], is a basic task of question answering.<br>",
    "Arabic": "فهم القراءة الآلية",
    "Chinese": "机器阅读理解",
    "French": "compréhension de lecture par machine",
    "Japanese": "機械読解能力",
    "Russian": "машинное понимание текста"
  },
  {
    "English": "machine translation",
    "context": "1: However, both languages have a rather large amount of monolingual data publicly available (Buck et al., 2014), making them perfect candidates to track performance on unsupervised and semi-supervised tasks for <mark>Machine Translation</mark>.<br>2: Therefore, <mark>Machine Translation</mark> (MT) for SLs cannot directly take advantage of the recent developments in text-based MT. For this purpose, previous work has used written representations of the SLs. One of these representations are glosses, where signs are labeled by words of the corresponding spoken language, often including affixes and markers.<br>",
    "Arabic": "الترجمة الآلية",
    "Chinese": "机器翻译",
    "French": "traduction automatique",
    "Japanese": "機械翻訳",
    "Russian": "машинный перевод"
  },
  {
    "English": "machine translation model",
    "context": "1: Experiments on benchmark datasets for both English and Chinese generation tasks yield significant improvements over results obtained by two state-of-the-art <mark>machine translation models</mark>, in terms of both automatic metrics and human evaluation.<br>2: The first three features, which are also widely used in state-of-the-art <mark>machine translation models</mark> (Koehn et al., 2003;Chiang, 2007), are rule-specific and thus can be computed before decoding. The last feature is computed during the decoding phase in combination with the sibling rules used. We score a derivation D with a log-linear model: \n<br>",
    "Arabic": "نموذج الترجمة الآلية",
    "Chinese": "机器翻译模型",
    "French": "modèle de traduction automatique",
    "Japanese": "機械翻訳モデル",
    "Russian": "модель машинного перевода"
  },
  {
    "English": "machine translation system",
    "context": "1: The model consists of source and alignment model variables; given the training corpora size of a <mark>machine translation system</mark>, the number of variables is large. So if the Gibbs sampler samples both source variables and alignment variables, the inference requires many iterations until the sampler mixes. Xu et al.<br>",
    "Arabic": "نظام الترجمة الآلية",
    "Chinese": "机器翻译系统",
    "French": "système de traduction automatique",
    "Japanese": "機械翻訳システム",
    "Russian": "система машинного перевода"
  },
  {
    "English": "machine vision",
    "context": "1: [6,9,11]), has recently gained some attention in the <mark>machine vision</mark> literature [1,2,3,4,13,14,19] with an emphasis on the detection of faces [12,15,16]. There is broad agreement on the issue of representation: object categories are represented as collection of features, or parts, each part has a distinctive appearance and spatial position.<br>",
    "Arabic": "رؤية الآلة",
    "Chinese": "机器视觉",
    "French": "- Vision par ordinateur",
    "Japanese": "マシンビジョン",
    "Russian": "машинное зрение"
  },
  {
    "English": "machine-generate text",
    "context": "1: NLG evaluations usually forego such structures, based, we suspect, on the assumption that evaluating <mark>machine-generated text</mark> requires only fluency in the language the text is generated in. Our results suggest otherwise. Evaluators often mistook <mark>machine-generated text</mark> as human, citing superficial textual features that machine generation has surpassed (Brown et al., 2020).<br>2: 2 How well can untrained evaluators identify <mark>machine-generated text</mark>? In our first study, we ask how well untrained evaluators can distinguish between human-and machinegenerated text.<br>",
    "Arabic": "نصوص تولد آلياً",
    "Chinese": "机器生成的文本",
    "French": "texte généré par machine",
    "Japanese": "機械生成テキスト",
    "Russian": "машинно-сгенерированный текст"
  },
  {
    "English": "machine-in-the-loop",
    "context": "1: 1 Our starting point is respect for the agency of local people and a commitment of newcomers to embrace local matters of concern. Our contribution is a set of insights about ways of working with local speech communities, along with a <mark>machine-in-the-loop</mark> design pattern which enhances local agency.<br>2: An important way in which the task of empathic rewriting can be used is for providing feedback and training to people through <mark>machine-in-the-loop</mark> writing systems [9,64]. For humans to adopt such feedback, however, the rewriting process should make changes that are precise and specific to the original response.<br>",
    "Arabic": "آلة في الحلقة",
    "Chinese": "人机协作",
    "French": "machine-dans-la-boucle",
    "Japanese": "人間介在型システム",
    "Russian": "машина-в-петле"
  },
  {
    "English": "macro-F1",
    "context": "1: We also vary λ, the coefficient of fairness loss sγ (y,y adv ) (h), from 1 to 5000 to study the trade-off between fairness and accuracy (in terms of micro-, macro-, and example-F1). We randomly choose 70% data for training and 30% for testing.<br>2: Moreover, the integration of InstructGPT-3 in the verification loop costs less in terms of time and money compared to adding more human annotators. However, we note that InstructGPT-3 is not a panacea on its own. While the model effectively resolves worker disagreements , we find that its individual predictions are only correct with ∼60 % <mark>macro-F1</mark> , which is far from the ∼85 % <mark>macro-F1</mark> with majority voting , indicating that not all PEA-COK persona relations are known by large-scale language models , and that human crowdsourcing GPT-3 ( Brown et al. , 2020 ) that uses 5 randomly<br>",
    "Arabic": "معدل F1 الكلي",
    "Chinese": "宏F1值",
    "French": "macro-F1",
    "Japanese": "マクロ-F1",
    "Russian": "макро-F1"
  },
  {
    "English": "macro-action",
    "context": "1: The two approaches, focused <mark>macro-actions</mark> and learning a policy, are very much composable, and it would be interesting to see whether such macro actions could help make LTS+CM more efficient in training time or converge to a faster policy.<br>2: In addition, COMA's best agents are competitive with state-of-the-art centralised controllers that are given access to full state information and <mark>macro-actions</mark>.<br>",
    "Arabic": "إجراءات ماكرو",
    "Chinese": "宏观动作",
    "French": "macro-action",
    "Japanese": "マクロアクション",
    "Russian": "макродействие"
  },
  {
    "English": "macro-average",
    "context": "1: In practice, the 4 similarity function is most commonly used, defined as the Dice coefficient (or F 1 score) between and :  \n Here we see that 4 computes a version of <mark>macro-average</mark> over entities, whereas 3 computes a micro-average. The CEAF that uses 4 is sensibly denoted CEAF 4 in coreference resolution.<br>",
    "Arabic": "المتوسط ​​الكلي",
    "Chinese": "宏观平均",
    "French": "macro-moyenne",
    "Japanese": "マクロ平均",
    "Russian": "макросреднее"
  },
  {
    "English": "majority voting",
    "context": "1: We examine two versions of repeated-labeling, repeated-labeling with <mark>majority voting</mark> (MV ) and uncertainty-preserving repeatedlabeling (ME ), where we generate multiple examples with different weights to preserve the uncertainty of the label multiset as described in Section 3.3.<br>2: Member outputs are aggregated using <mark>majority voting</mark>: instances are assigned the class that is most frequently assigned by the ensemble members [11]. Bagging can introduce a significance improvement in accuracy as a result of a reduction of variance versus individual decision trees. The most well-known boosting algorithm is AdaBoost [14].<br>",
    "Arabic": "التصويت بالأغلبية",
    "Chinese": "多数投票",
    "French": "vote majoritaire",
    "Japanese": "過半数決",
    "Russian": "голосование большинством"
  },
  {
    "English": "manifold",
    "context": "1: Our maps are the logarithm maps, log X , that map the neighborhood of points X ∈ M to the tangent spaces T X . Since this mapping is a homeomorphism around the neighborhood of the point, the structure of the <mark>manifold</mark> is preserved locally. The tangent space is a vector space and we learn the classifiers on this space.<br>2: 3) VAEs have been shown to fold a <mark>manifold</mark> into a Gaussian distribution [39], exposing dead regions without any data points in the outer parts of the distribution. Thus, they produce non-plausible samples that are far from the input when traversed in outer regions, as we demonstrate in our experiments.<br>",
    "Arabic": "متعدد",
    "Chinese": "流形",
    "French": "variété",
    "Japanese": "多様体",
    "Russian": "многообразие"
  },
  {
    "English": "manifold hypothesis",
    "context": "1: This algorithm works implicitly under the hypothesis that the variable y to predict from x is invariant to the local directions of change present between nearest neighbors. This is consistent with the <mark>manifold hypothesis</mark> for classification (hypothesis 3 mentioned in the introduction).<br>2: Using the previously defined charts for every point of the training set, we propose to use this additional information provided by unsupervised learning to improve the performance of the supervised task. In this we adopt the <mark>manifold hypothesis</mark> for classification mentioned in the introduction.<br>",
    "Arabic": "فرضية الرتبة",
    "Chinese": "流形假设",
    "French": "hypothèse de variété",
    "Japanese": "多様体仮説",
    "Russian": "гипотеза многообразия"
  },
  {
    "English": "manifold learn",
    "context": "1: We figure that the clearest way to showcase these effects is to design a <mark>manifold learning</mark> framework over and above an existing generative process, such as PLSV (Iwata, Yamada, and Ueda 2008), which we review below. The generative process is as follows: 1. For each topic z = 1, . . .<br>2: This shows that random affine transformed data outside the target space does not enable basis polynomials to have discriminability in the target space. Namely, the random affine version of Theorem 6 is confirmed. Note that a random affine transformation is often considered a geometric transformation, and plays an important role in <mark>manifold learning</mark> and dimensionality reduction.<br>",
    "Arabic": "تعلم الأبعاد",
    "Chinese": "流形学习",
    "French": "apprentissage de variété",
    "Japanese": "多様体学習",
    "Russian": "обучение на многообразии"
  },
  {
    "English": "manifold projection",
    "context": "1: It can be seen that for leg and arm/hand occlusions, Pose-NDF reconstructs the pose better than VPoser and HuMoR. For occluded shoulders, HuMoR takes the lead. We observe that results of Pose-NDF depend on the initialization of the occluded joint, as it is expected from <mark>manifold projection</mark>.<br>",
    "Arabic": "الإسقاط المتنوع",
    "Chinese": "流形投影",
    "French": "projection sur une variété",
    "Japanese": "多様体射影",
    "Russian": "проекция многообразия"
  },
  {
    "English": "manifold structure",
    "context": "1: The present work follows through on this interpretation, and investigates whether it is possible to use this information, that is presumably captured about <mark>manifold structure</mark>, to further improve classification performance by leveraging hypothesis 3.<br>2: The multilinear structure of the tensor product enables efficient optimization of ( 10) and ( 11) within the <mark>manifold structure</mark> by means of reducing a high-dimensional linear equation in the coefficient tensor to small linear subproblems on the component tensors 3 .<br>",
    "Arabic": "هيكل متعدد",
    "Chinese": "流形结构",
    "French": "structure de variété",
    "Japanese": "多様体構造",
    "Russian": "многообразная структура"
  },
  {
    "English": "manifold-value datum",
    "context": "1: In this section we discuss previous work on parametrizing family of distributions for <mark>manifold-valued data</mark>. Here, the manifold structure is considered to be prescribed, in contrast with methods that jointly learn the manifold structure and density (e.g. Brehmer and Cranmer, 2020;Caterini et al., 2021). Push-forward of Euclidean normalizing flows.<br>2: In [9,14,30], statistical concepts such as averaging and principal components analysis were extended to manifolds representing anatomical shape variability. Many of the ideas are based on the method of averaging in metric spaces proposed by Fréchet [10]. In this paper we use the notion of Fréchet expectation to generalize regression to <mark>manifold-valued data</mark>.<br>",
    "Arabic": "بيانات متجهة القيمة",
    "Chinese": "流形值数据",
    "French": "donnée à valeurs sur une variété",
    "Japanese": "多様体値データ",
    "Russian": "данные многообразия"
  },
  {
    "English": "margin parameter",
    "context": "1: Then, given a <mark>margin parameter</mark> θ > log |H| 16|Y| 2 n and a measure of the confidence of H(•) in its predictions θ H (x, y), with probability at least 1 − δ, we have that \n<br>2: Then, given a <mark>margin parameter</mark> θ > log |H| 16|Y| 2 n and a measure of the confidence of H(•) in its predictions θ H (x, y), with probability at least 1 − δ, we have that \n<br>",
    "Arabic": "معامل الهامش",
    "Chinese": "边际参数",
    "French": "paramètre de marge",
    "Japanese": "マージンパラメータ",
    "Russian": "параметр отступления"
  },
  {
    "English": "marginal density",
    "context": "1: First, the influence estimation problem in this setting is a difficult graphical model inference problem, i.e., computing the <mark>marginal density</mark> of continuous variables in loopy graphical models. The exact answer can be computed only for very special cases. For example, Gomez-Rodriguez et al.<br>2: is a random vector of hidden variables, and Z is the normalization constant. The exact <mark>marginal density</mark> p( \n x) = h∈{−1,1} d h p(x, h) is intractable when d h is large, since it involves summing over 2 d h terms.<br>",
    "Arabic": "الكثافة الهامشية",
    "Chinese": "边际密度",
    "French": "densité marginale",
    "Japanese": "周辺密度",
    "Russian": "маргинальная плотность"
  },
  {
    "English": "marginal distribution",
    "context": "1: This perspective reveals why stochasticity is helpful in practice: The implicit Langevin diffusion drives the sample towards the desired <mark>marginal distribution</mark> at a given time, actively correcting for any errors made in earlier sampling steps. On the other hand, approximating the Langevin term with discrete SDE solver steps introduces error in itself.<br>2: We say an algorithm has weak unlabeled access if the algorithm can access, for each D ∈ D, a <mark>marginal distribution</mark> \n D ′ X such that D ∞ (D ′ X ||D X ) ∈ poly(1/ε, d, n), with probability 1 − δ.<br>",
    "Arabic": "التوزيع الهامشي",
    "Chinese": "边际分布",
    "French": "distribution marginale",
    "Japanese": "余部分布",
    "Russian": "маргинальное распределение"
  },
  {
    "English": "marginal inference",
    "context": "1: where L ∈ R N ×N (for N = |V|) is a symmetric positive semidefinite matrix, and L V refers to the submatrix of L with only those rows and columns corresponding to those elements in the subset V . Although MAP inference remains NP-hard in DPPs (just as in MPPs), <mark>marginal inference</mark> becomes tractable.<br>2: If we can compute Pr(x) efficiently, then the complexity of MPE and <mark>marginal inference</mark> over variables Y is polynomial in |X|. Proof. Following Theorem 7, we only need to show that we can compute Pr(y) = z Pr(y, z) in time polynomial in |X| for all y.<br>",
    "Arabic": "الاستدلال الهامشي",
    "Chinese": "边缘推断",
    "French": "inférence marginale",
    "Japanese": "マージナル推論",
    "Russian": "маргинальный вывод"
  },
  {
    "English": "marginal likelihood",
    "context": "1: the prior , in the posterior contraction ; \n (3) in the product decomposition of the <mark>marginal likelihood</mark> in Section 3, the first terms will have low probability density, even if the posterior updates quickly to become a good description of the data. Model Selection.<br>2: We now show how the training procedure we use to train our permutation model can be derived from a form of evidence lower bound (ELBO). Ideally, our permutation model would be a distribution P θ (R|x, z ′ ) over permutation matrices R and we would maximize the <mark>marginal likelihood</mark>, i.e. marginalizing over all permutations: \n<br>",
    "Arabic": "الاحتمالية الهامشية",
    "Chinese": "边缘似然",
    "French": "vraisemblance marginale",
    "Japanese": "周辺尤度",
    "Russian": "предельное правдоподобие"
  },
  {
    "English": "marginal log-likelihood",
    "context": "1: In this section, we measure test set performance using 100 test points and the <mark>marginal log-likelihood</mark> bound of Burda et al. [10], which provides a tighter estimate of marginal log likelihood than the ELBO. Throughout, we call this the \"test log-likelihood bound.\"<br>2: z n,v = m v − k|x) \n Let m(y) be the multiset of tokens in the gold sequence y. We train our model with gradient ascent to maximize the <mark>marginal log-likelihood</mark> of m(y): \n (x,y)∈D log P (m(y) | x)(4) \n<br>",
    "Arabic": "الاحتمالية اللوغاريتمية الهامشية",
    "Chinese": "边缘对数似然",
    "French": "vraisemblance marginale logarithmique",
    "Japanese": "周辺対数尤度",
    "Russian": "предельная логарифмическая правдоподобность"
  },
  {
    "English": "marginal polytope",
    "context": "1: Projecting to a relaxed <mark>marginal polytope</mark> The <mark>marginal polytope</mark>, M, is the set of marginals which are consistent with some joint distribution over permutations.<br>2: With the local consistency polytope, both entropy approximations get steadily worse as the coupling increases. In contrast, using the exact <mark>marginal polytope</mark>, we see a peak at θ = 2, then a steady improvement in accuracy as the coupling term grows.<br>",
    "Arabic": "متعدد الأضلاع الهامشي",
    "Chinese": "边际多面体",
    "French": "polytope marginal",
    "Japanese": "マージナルポリトープ",
    "Russian": "маргинальный политоп"
  },
  {
    "English": "marginal probability",
    "context": "1: Exact computation of the derivatives is intractable, due to the difficulty in computing the <mark>marginal probabilities</mark> p i (r), p ij (r, s). However, any approximate method for estimating <mark>marginal probabilities</mark> can be used. One approach for approximating the <mark>marginal probabilities</mark> is using Monte Carlo sampling, like in [4,1].<br>2: ; E k−1 ) \n . We note that computing the above first order approximation requires a single inference process on the previous iteration energy E k−1 , from which the local beliefs (approximated <mark>marginal probabilities</mark>) {b k−1 t,i } are computed.<br>",
    "Arabic": "الاحتمال الهامشي",
    "Chinese": "边际概率",
    "French": "probabilité marginale",
    "Japanese": "マージナル確率",
    "Russian": "предельная вероятность"
  },
  {
    "English": "marginalization",
    "context": "1: We consider a general controller action distribution set Q ⊂ P(A) and adversarial action set P ⊂ {S × A → P(S)}. The SA-rectangular <mark>marginalization</mark> of P ( see ( 2.11 ) ) is defined as P s , a : = { p s , a : p ∈ P } for all s ∈ S. We consider the product action set : Also , the S-rectangular <mark>marginalization</mark> of P is defined as P s : = { p s : p ∈ P }<br>2: But the essential ingredient of ARD, that <mark>marginalization</mark> and subsequent evidence maximization leads to a pruning of unsupported hypotheses, remains unchanged. We turn now to empirical Bayesian procedures that incorporate variational methods. In [15], a plausible hierarchical prior is adopted that, unfortunately, leads to intractable integrations when computing the desired source posterior.<br>",
    "Arabic": "التهميش",
    "Chinese": "边缘化",
    "French": "marginalisation",
    "Japanese": "周辺化",
    "Russian": "маргинализация"
  },
  {
    "English": "mask",
    "context": "1: Given a text x x x, the corrupted context with a <mark>mask</mark> at position j is denoted [x x x] j , the LM predicts a distribution p Ω|T (•|[x x x] j ; θ; T ) over the vocabulary Ω given the <mark>mask</mark>ed context.<br>",
    "Arabic": "ماسك",
    "Chinese": "掩码",
    "French": "masque",
    "Japanese": "マスク",
    "Russian": "маска"
  },
  {
    "English": "mask token",
    "context": "1: We also expand our analyses to automatically generate the aligner using mT5 (Xue et al., 2021). It is trained using a span generation task using sentences like 'Paris <MASK> France'. The mT5 model is trained to fill the <mark>mask token</mark> by generating spans like 'is capital of'.<br>2: Given a PVP p = (P, v), we define l(x) = max y∈Yx |v(y)| to be the maximum number of tokens required to express any output in Y x and P k (x) to be P (x) with the <mark>mask token</mark> replaced by k masks.<br>",
    "Arabic": "رمز القناع",
    "Chinese": "掩码标记",
    "French": "jeton de masque",
    "Japanese": "マスクトークン",
    "Russian": "маскирующий токен"
  },
  {
    "English": "mask vector",
    "context": "1: An extension of cycle loss for simultaneously training between multiple datasets with different data domains. It uses a <mark>mask vector</mark> to ignore unspecified labels and optimize only on known ground-truth labels. It yields more realistic results when training simultaneously with multiple datasets. Our model differs from these approaches in two main aspects.<br>2: Since the information of each v ci is unknown in the first place, we initialized its input feature with a <mark>mask vector</mark> and the coordinates according to the even distribution between the residue right before CDRs (namely, v c1−1 ) and the one right after CDRs (namely, v c n(c) +1 ).<br>",
    "Arabic": "متجه القناع",
    "Chinese": "掩码向量",
    "French": "vecteur de masque",
    "Japanese": "マスクベクトル",
    "Russian": "вектор маски"
  },
  {
    "English": "masked input",
    "context": "1: This term encourages piecewise smoothness in depth regions where there is no image intensity change. (III.) <mark>masked input</mark> depth, human mask, and additional confidence for IV. ; in V, we also input human keypoints. Lower is better for all metrics.<br>",
    "Arabic": "الإدخال المُقنع",
    "Chinese": "掩码输入",
    "French": "entrée masquée",
    "Japanese": "マスク入力",
    "Russian": "маскированный ввод"
  },
  {
    "English": "masked language model",
    "context": "1: In Section 3.1, we mention that BERT uses a mixed strategy for masking the target tokens when pre-training with the <mark>masked language model</mark> (MLM) objective. The following is an ablation study to evaluate the effect of different masking strategies.<br>2: The results show that MAUVE has a strong correlation with the discrimination accuracy for a variety of discriminators, including one based on a <mark>masked language model</mark>, BERT [14]. This correlation is particular strong for the Grover-based discriminators. We note that evaluating any one model and decoding algorithm pair requires fine-tuning a separate model.<br>",
    "Arabic": "نموذج لغوي مقنع",
    "Chinese": "掩码语言模型",
    "French": "modèle de langage masqué",
    "Japanese": "マスクされた言語モデル (MLM)",
    "Russian": "маскированная языковая модель"
  },
  {
    "English": "masked token",
    "context": "1: In the second stage, we complete the <mark>masked tokens</mark> in the output of the first stage conditioned on k in a style-unrelated manner. Due to the lack of parallel data, typical style transfer models tend to optimize the selfreconstruction loss with the same inputs and outputs (Xiao et al., 2021;Lee et al., 2021).<br>2: Specifically, we select the Seq2Seq PLM, BART (Lewis et al., 2020) as our backbone by revising its decoding process into the NAR <mark>masked tokens</mark> recovering task. Then, we adjust the typical discrete diffusion method to better fit the PLM by adding mask tokens as noise, revising the learning objective and removing the time step embeddings.<br>",
    "Arabic": "الرمز المقنع",
    "Chinese": "遮蔽词元",
    "French": "jeton masqué",
    "Japanese": "マスクされたトークン",
    "Russian": "маскированный токен"
  },
  {
    "English": "masking function",
    "context": "1: A <mark>masking function</mark> M takes X, f and S as its inputs and produces a new token sequence X mlm as its output \n X mlm = M(X, S, f ) \n where X mlm denotes the input sentence X with f % of the maskable tokens (as deemed by S) randomly replaced with [MASK].<br>",
    "Arabic": "وظيفة اخفاء",
    "Chinese": "掩蔽函数",
    "French": "fonction de masquage",
    "Japanese": "マスキング関数",
    "Russian": "маскирующая функция"
  },
  {
    "English": "match algorithm",
    "context": "1: Based on previously developed tools [17], we built a simulator to mimic daily matching in a real-world kidney exchange pool. 5 In the simulation, each day, some incompatible patient-donor pairs enter the simulated pool and some depart. Then, a <mark>matching algorithm</mark> is run to match a subset of compatible patient-donor pairs.<br>",
    "Arabic": "خوارزمية المطابقة",
    "Chinese": "匹配算法",
    "French": "algorithme d'appariement",
    "Japanese": "マッチングアルゴリズム",
    "Russian": "алгоритм сопоставления"
  },
  {
    "English": "matching loss",
    "context": "1: In practice, we build an end-to-end deep network that integrates a feature extracting component that outputs the required descriptors F for building the matrix M. We solve the assignment problem (2) and compute a <mark>matching loss</mark> L(v * ) between the solution v * and the ground-truth.<br>2: Recall that for L = 1, BMG uses the same data to compute agent parameter update, target update, and <mark>matching loss</mark>; hence this is an apples-to-apples comparison.<br>",
    "Arabic": "الخسارة المطابقة",
    "Chinese": "匹配损失",
    "French": "perte d'appariement",
    "Japanese": "マッチング損失",
    "Russian": "потери соответствия"
  },
  {
    "English": "matrix",
    "context": "1: Notations Throughout this paper, we use bold capital letters (e.g., X) to denote <mark>matrices</mark>, bold lowercase letters (e.g., x) to denote (column) vectors, and calligraphic letters (e.g., X ) to denote spaces. Finally, capital P denotes a probability and lowercase p denotes a distribution.<br>2: The following lemma, whilst not used in the proof of theorem 5, nevertheless provides a useful intuition about the role of commutativity. Recall that two <mark>matrices</mark> A and S commute iff [A, S] := AS − SA = 0. That is, iff AS = SA. Lemma 9.<br>",
    "Arabic": "المصفوفة",
    "Chinese": "矩阵",
    "French": "matrice",
    "Japanese": "行列",
    "Russian": "матрица"
  },
  {
    "English": "matrix approximation",
    "context": "1: The Column Subset Selection Problem is one of the most classical tasks in <mark>matrix approximation</mark> (Boutsidis et al., 2008). The original version of the problem compares the projection error of a subset of size k to the best rank k approximation error.<br>",
    "Arabic": "تقريب المصفوفة",
    "Chinese": "矩阵近似",
    "French": "approximation matricielle",
    "Japanese": "行列近似",
    "Russian": "аппроксимация матрицы"
  },
  {
    "English": "matrix decomposition",
    "context": "1: On the other hand, one of the most successful model classes are factorization methods (MF) based on matrix or tensor decomposition. The best approaches [3,4] for the 1M$ Netflix challenge 1 are based on this model class.<br>2: is based on low-rank <mark>matrix decomposition</mark> , such as matrix completion ( MC ) ( Keshavan et al. , 2010 ) , universal singular value thresholding ( USVT ) ( Chatterjee et al. , 2015 ) . More discussion about graphon estimation are in Appendix B.<br>",
    "Arabic": "تحليل المصفوفة",
    "Chinese": "矩阵分解",
    "French": "décomposition matricielle",
    "Japanese": "行列分解",
    "Russian": "разложение матрицы"
  },
  {
    "English": "matrix factorization",
    "context": "1: The gap between CF and LDA is interesting-other users provide a better assessment of preferences than content alone. Out-of-matrix prediction is a harder problem, as shown by the relatively lower recall. In this task, CTR performs slightly better than LDA. <mark>Matrix factorization</mark> cannot perform out-of-matrix prediction.<br>",
    "Arabic": "تجزئة المصفوفة",
    "Chinese": "矩阵分解",
    "French": "factorisation matricielle",
    "Japanese": "行列分解",
    "Russian": "матричная факторизация"
  },
  {
    "English": "matrix form",
    "context": "1: By assuming that Y is centered and X, Z are column-wise normalized to zero mean and unit standard deviation, we can set the bias term w0 = 0. Thus, in <mark>matrix form</mark>, the pairwise interaction regression model can be expressed as \n Y = Xw + 1 2 Z • vec(Q) + ,(3) \n<br>",
    "Arabic": "الشكل المصفوفي",
    "Chinese": "矩阵形式",
    "French": "forme matricielle",
    "Japanese": "行列形式",
    "Russian": "матричная форма"
  },
  {
    "English": "matrix inversion",
    "context": "1: The graph Laplacian can be calculated in O(n 2 ) time, and M can be calculated by <mark>matrix inversion</mark> which requires O(n 3 ) time. Therefore, the overall computational complexity is O(n 3 ) (or O(n 2.376 ) using advanced matrix multiplication algorithms).<br>2: The first technique is a standard transformation of <mark>matrix inversion</mark> into an optimization problem. Since Hθ 0 by assumption, H −1 θ v ≡ arg min t {t Hθt − v t}.<br>",
    "Arabic": "عكس المصفوفة",
    "Chinese": "矩阵求逆",
    "French": "inversion matricielle",
    "Japanese": "行列の逆行列",
    "Russian": "инверсия матрицы"
  },
  {
    "English": "matrix multiplication",
    "context": "1: However, whether the running time can be improved remains an unanswered question. In this paper, we almost settle the question by proving that the time complexity of this problem is equivalent to that of <mark>matrix multiplication</mark> up to lower order terms.<br>2: It is also more convenient to implement: it can be computed using <mark>matrix multiplication</mark> allowing our approach to be implemented entirely in PyTorch. Features for Refinement: We compute visual similarity by building a correlation volume between all pairs of pixels.<br>",
    "Arabic": "ضرب المصفوفات",
    "Chinese": "矩阵乘法",
    "French": "multiplication matricielle",
    "Japanese": "行列積",
    "Russian": "матричное умножение"
  },
  {
    "English": "matrix norm",
    "context": "1: One way to measure such incoherence is using the babel function, which bounds the maximum inner product between two different columns d i , d j :  \n µ s (D) = max \n D = {D ∈ R d×K : d j 2 2 ≤ 1, D ⊤ D − I 2 \n F ≤ γ}.<br>",
    "Arabic": "معيار المصفوفة",
    "Chinese": "矩阵范数",
    "French": "norme matricielle",
    "Japanese": "行列ノルム",
    "Russian": "норма матрицы"
  },
  {
    "English": "matrix sketching",
    "context": "1: (2016) use <mark>matrix sketching</mark> to estimate Cook's distance, which is closely related to influence; they focus on prioritizing training points for human attention and derive methods specific to generalized linear models. Kabra et al. (2015) define a different notion of influence that is specialized to finite hypothesis classes.<br>",
    "Arabic": "تلخيص المصفوفة",
    "Chinese": "矩阵素描",
    "French": "esquisse matricielle",
    "Japanese": "行列スケッチング",
    "Russian": "матричный эскиз"
  },
  {
    "English": "matrix vector product",
    "context": "1: This is mainly because both methods are the matrix free methods that require only <mark>matrix vector products</mark> in each iteration. However, the SOCP do not benefit from sparsity as well as the other two methods.<br>2: Our single-pair algorithm (Algorithm 1) evaluates the right-hand side of (2.6) by maintaining P t e i and P t e j . The time complexity is O(T m) since the algorithm performs O(T ) <mark>matrix vector products</mark> for P t e i and P t e j (t = 1, . .<br>",
    "Arabic": "ضرب مصفوفة بمتجه",
    "Chinese": "矩阵向量积",
    "French": "produit matrice-vecteur",
    "Japanese": "行列ベクトル積",
    "Russian": "произведение матрицы на вектор"
  },
  {
    "English": "matrix-vector multiplication",
    "context": "1: Note that A 0 , A 1 are accessed only through <mark>matrix-vector multiplication</mark>s. Since they are both DPLR, they have O(N ) <mark>matrix-vector multiplication</mark>, showing Theorem 2.<br>2: Theorem 5 (Monarch hierarchy expressivity). Let M be an n×n matrix such that <mark>matrix-vector multiplication</mark> of M and an arbitrary vector v (i.e., computation of Mv) can be represented as a linear arithmetic circuit with depth d and s total gates.<br>",
    "Arabic": "ضرب المصفوفة بالمتجه",
    "Chinese": "矩阵-向量乘法",
    "French": "multiplication matrice-vecteur",
    "Japanese": "行列ベクトル積",
    "Russian": "умножение матрицы на вектор"
  },
  {
    "English": "matroid",
    "context": "1: The set C could express, for example, that solutions must be an independent set in a <mark>matroid</mark>, a limited budget knapsack, or a cut (or spanning tree, path, or matching) in a graph. Without making any further assumptions about f , the above problems are trivially worst-case exponential time and moreover inapproximable.<br>2: That is, each independent set of M is obtained by taking the union of an independent set of facilities from M, and a subset of X of size at most m. It is straightforward to show that M satisfies all three axioms mentioned above, and thus is a <mark>matroid</mark> over the ground set F ∪ X.<br>",
    "Arabic": "ماترويد",
    "Chinese": "拟阵",
    "French": "matroïde",
    "Japanese": "マトロイド",
    "Russian": "матроид"
  },
  {
    "English": "matroid constraint",
    "context": "1: As a corollary, we obtain FPT approximation algorithms with optimal approximation ratios for k-Median and k-Means with outliers in general and Euclidean metrics. We also exhibit more applications of our approach to other variants of the problem that impose additional constraints on the clustering, such as fairness or <mark>matroid constraints</mark>.<br>",
    "Arabic": "قيود الماترويد",
    "Chinese": "拟阵约束",
    "French": "contrainte matroïde",
    "Japanese": "マトロイド制約",
    "Russian": "матроидное ограничение"
  },
  {
    "English": "max norm",
    "context": "1: where X max means the <mark>max norm</mark> of a matrix X and I N is the identity matrix of size N . Proof. Put a i = (a 1,i , . . . , a M,i ) T .<br>",
    "Arabic": "الحد الأقصى للنورم",
    "Chinese": "最大范数",
    "French": "norme maximale",
    "Japanese": "最大ノルム",
    "Russian": "максимальная норма"
  },
  {
    "English": "max pooling",
    "context": "1: The final sentence embedding v is the row-based <mark>max pooling</mark> over the output of the last Bi-LSTM layer, where n denotes the number of words within a sentence and m is the number of Bi-LSTM layers (m = 3 in SSE).<br>2: where k is called the kernel size, s is the stride or subsampling factor, and f ks determines the layer type: a matrix multiplication for convolution or average pooling, a spatial max for <mark>max pooling</mark>, or an elementwise nonlinearity for an activation function, and so on for other types of layers.<br>",
    "Arabic": "التجميع الأقصى",
    "Chinese": "最大池化",
    "French": "max pooling",
    "Japanese": "最大プーリング",
    "Russian": "максимальное объединение"
  },
  {
    "English": "max-margin",
    "context": "1: These features can act as soft constraints whose penalty values are automatically learned from data; in addition, our model is also compatible with expert knowledge in the form of hard constraints. Learning through a <mark>max-margin</mark> framework is made effective by the means of a LPrelaxation.<br>2: 2 There has been some previous work on GLM approaches for full syntactic parsing that make use of dynamic programming. Taskar et al. (2004) describe a <mark>max-margin</mark> approach; however, in this work training sentences were limited to be of 15 words or less.<br>",
    "Arabic": "هامش الحد الأقصى",
    "Chinese": "最大间隔",
    "French": "max-marge",
    "Japanese": "最大マージン",
    "Russian": "максимальная маржа"
  },
  {
    "English": "max-margin learning",
    "context": "1: sub-class) while optimizing for maximum discrimination among the global activity classes. We propose a novel <mark>max-margin learning</mark> approach to tackle this problem. Let (x , , ( )) be a training sample, where x is a data point, is the sub-class label of x , and ( ) maps to a class label.<br>2: Our model learns statistics that capture the spatial arrangements of various object classes in real images, both in terms of which arrangements to suppress through NMS and which arrangements to favor through spatial co-occurrence statistics. We formulate parameter estimation in our model as a <mark>max-margin learning</mark> problem.<br>",
    "Arabic": "التعلم بالحد الأقصى للهامش",
    "Chinese": "最大边际学习",
    "French": "apprentissage à marge maximale",
    "Japanese": "最大マージン学習",
    "Russian": "обучение с максимальным зазором"
  },
  {
    "English": "max-pool",
    "context": "1: To get agent feature G t with dynamics and spatial priors, we <mark>max-pool</mark> motion queries from MotionFormer in the modality dimension denoted as Q X ∈ R Na×D , with D as the feature dimension. Then we fuse it with the upstream track query Q A and current position embedding P A via a temporal-specific MLP: \n<br>2: For the 8×8 maps, we use 50 filters in the first layer and then 100 filters in the second layer, all of size 3 × 3. Each of these layers is followed by a 2 × 2 <mark>max-pool</mark>.<br>",
    "Arabic": "تجميع أقصى قيمة",
    "Chinese": "最大池化",
    "French": "max-pool",
    "Japanese": "最大プーリング",
    "Russian": "макс-пулинг"
  },
  {
    "English": "max-pooling layer",
    "context": "1: We now introduce the VI module -a NN that encodes a differentiable planning computation. Our starting point is the VI algorithm (1). Our main observation is that each iteration of VI may be seen as passing the previous value function V n and reward function R through a convolution layer and <mark>max-pooling layer</mark>.<br>2: These two queries, along with the command embedding, are encoded with MLP layers followed by a <mark>max-pooling layer</mark> across the modality dimension, where the most salient modal features are selected and aggregated.<br>",
    "Arabic": "طبقة التجميع القصوى",
    "Chinese": "最大池化层",
    "French": "couche de max-pooling",
    "Japanese": "max-poolingレイヤー",
    "Russian": "слой максимального объединения"
  },
  {
    "English": "max-product semiring",
    "context": "1: Using the <mark>max-product semiring</mark>, we then compute the top gradient path through the different branches (skip connection, keys, values, and queries) for (a) the subject of a sentence, (b) the attractors of a sentence, and (c) all tokens of a sentence. Model.<br>2: For any semiring of our choosing, this modified algorithm will compute a different statistic associated with a function's gradient. We begin by motivating the standard (+, ×) semiring which is common in the interpretability literature, before discussing the implementation and interpretation of the max-product and entropy semirings we focus on in this work.<br>",
    "Arabic": "الحد الأقصى للمنتج نصف الدائري",
    "Chinese": "最大乘积半环",
    "French": "sémi-anneau max-produit",
    "Japanese": "最大積半環",
    "Russian": "макс-произведение полукольцо"
  },
  {
    "English": "maximal clique",
    "context": "1: if and only if ci , j = 1 . A clique S is called maximal if there exists no any other clique S such that S ⊂ S . Please note that there may exist more than one <mark>maximal clique</mark> in a graph.<br>2: Let α be an AMO and K ∈ Π(G) be the least <mark>maximal clique</mark> (with respect to ≺ α ) that is a prefix of some τ ∈ top(α).<br>",
    "Arabic": "الزمرة القصوى",
    "Chinese": "最大团",
    "French": "clique maximale",
    "Japanese": "最大クリーク",
    "Russian": "максимальная клика"
  },
  {
    "English": "maximal frequent itemset",
    "context": "1: Their result can be regarded as a generalization of <mark>maximal frequent itemsets</mark>. In [1] Afrati et al. mentioned the support integration issue: It is unknown how to integrate the support information with the approximation. In this paper, we solved this problem, thus advancing the summarization concept.<br>2: We present the first formal proof that the problem of counting the number of distinct <mark>maximal frequent itemsets</mark> in a database of transactions, given an arbitrary support threshold, is #P-complete, thereby providing strong theoretical evidence that the problem of mining <mark>maximal frequent itemsets</mark> is NP-hard.<br>",
    "Arabic": "مجموعة العناصر الشائعة القصوى",
    "Chinese": "最大频繁项集",
    "French": "ensemble d'items maximaux fréquents",
    "Japanese": "最大頻出アイテムセット",
    "Russian": "максимальный частый набор элементов"
  },
  {
    "English": "maximal frequent pattern",
    "context": "1: Hence a frequent pattern p is maximal if there is no frequent pattern q such that p ≺ q. Many problems of mining <mark>maximal frequent pattern</mark>s fall into this line of generalization above. For example, in the problem of mining maximal frequent itemsets, the patterns are sets of items and the partial order is defined on subset inclusion.<br>",
    "Arabic": "النمط المتكرر الأقصى",
    "Chinese": "最大频繁模式",
    "French": "motif fréquent maximal",
    "Japanese": "最大頻出パターン",
    "Russian": "максимальный частый шаблон"
  },
  {
    "English": "maximization problem",
    "context": "1: The objective of the proposed R-SVM+ algorithm aims to solve a <mark>maximization problem</mark> in Eq. ( 9) and a minimization problem in Eq. ( 12) at the same time. Therefore, we arrive at the objective function of R-SVM+ which is a minimization problem, \n<br>",
    "Arabic": "مشكلة التعظيم",
    "Chinese": "最大化问题",
    "French": "problème de maximisation",
    "Japanese": "最大化問題",
    "Russian": "задача максимизации"
  },
  {
    "English": "maximum a posteriori",
    "context": "1: One well-accepted framework to learn model parameters using <mark>maximum a posteriori</mark> (MAP) estimation is the EM algorithm (Dempster, Laird, and Rubin 1977). For our model, the regularized conditional expectation of the complete-data log likelihood in MAP estimation with priors is: Ψ is the current estimate.<br>2: This expression is then maximized with respect to the unknown hyperparameters, a process referred to as type-II maximum likelihood or evidence maximization [7,9] or restricted maximum likelihood [4]. Thus the optimization problem shifts from finding the <mark>maximum a posteriori</mark> sources given a fixed prior to finding the optimal hyperparameters of a parameterized prior.<br>",
    "Arabic": "المعظم البعدي",
    "Chinese": "最大后验概率",
    "French": "maximum a posteriori",
    "Japanese": "最大事後確率解",
    "Russian": "максимум апостериори"
  },
  {
    "English": "maximum a posteriori estimation",
    "context": "1: or <mark>maximum a posteriori estimation</mark> . Then, given a set of test patterns x * 1 , ..., x * m , one classifies each x * i independently by computing the labelŷ i that maximizes the estimated conditional probability,ŷ i = arg max y P (y|x * i , θ).<br>2: (Note that IK is a K-dimensional identity matrix.) This is the interpretation of matrix factorization that we will build on. When cij = 1, for ∀i, j, the <mark>maximum a posteriori estimation</mark> (MAP) of PMF corresponds to the solution in Eq. 2.<br>",
    "Arabic": "التقدير الأحتمالي الأقصى اللاحق",
    "Chinese": "最大后验估计",
    "French": "estimation du maximum a posteriori",
    "Japanese": "最大事後確率推定",
    "Russian": "максимальная апостериорная оценка"
  },
  {
    "English": "maximum clique",
    "context": "1: First, graph-based methods perform place recognition based on semantic graph representations, such as using semantic graph matching [16], [34], semantic histogram [17], bag of words [35], <mark>maximum clique</mark> [33], and semantic random walk [18].<br>2: Second, we search for maximal cliques in the graph and then use node-guided clique filtering to match each graph node with the appropriate maximal clique containing it. Compared with the <mark>maximum clique</mark>, MAC is a looser constraint and is able to mine more local information in a graph.<br>",
    "Arabic": "النقرة القصوى",
    "Chinese": "最大团",
    "French": "clique maximale",
    "Japanese": "最大クリーク",
    "Russian": "максимальная клика"
  },
  {
    "English": "maximum entropy",
    "context": "1: Namely, its first and second order marginals. A common approach in this case is to assume that the true distribution is the one with <mark>maximum entropy</mark> subject to these marginal constraints.<br>2: When faced with an ill-posed problem, the principle of <mark>maximum entropy</mark> (Jaynes, 1957) prescribes the use of \"the least committed\" probability distribution that is consistent with known problem constraints.<br>",
    "Arabic": "أقصى إنتروبيا",
    "Chinese": "最大熵",
    "French": "entropie maximale",
    "Japanese": "最大エントロピー",
    "Russian": "максимальная энтропия"
  },
  {
    "English": "maximum entropy model",
    "context": "1: While we use a <mark>maximum entropy model</mark> as the base extractor, this framework can be inherently applied to other extraction algorithms. We evaluate our system on two datasets where available training data is inherently limited. The first dataset is constructed from a publicly available database of mass shootings in the United States.<br>2: However, each field extracted using a <mark>maximum entropy model</mark> is estimated independently. For this reason the potential for correction propagation is minimal. Conditional Random Fields, a generalization both of <mark>maximum entropy model</mark>s and hidden Markov models, allow for the introduction of arbitrary non-local features and capture the dependencies between labels.<br>",
    "Arabic": "نموذج الانتروبيا القصوى",
    "Chinese": "最大熵模型",
    "French": "modèle d'entropie maximale",
    "Japanese": "最大エントロピーモデル",
    "Russian": "модель максимальной энтропии"
  },
  {
    "English": "maximum entropy principle",
    "context": "1: The second term is a regularizer and encourages all type marginals to be uniform to the extent that is allowed by the first two terms (cf. <mark>maximum entropy principle</mark>).<br>",
    "Arabic": "مبدأ الانتروبيا القصوى",
    "Chinese": "最大熵原理",
    "French": "principe de l'entropie maximale",
    "Japanese": "最大エントロピー原理",
    "Russian": "принцип максимальной энтропии"
  },
  {
    "English": "maximum flow",
    "context": "1: 0. Thus, the value of the <mark>maximum flow</mark> from s to t in G is K. Let G 0 be the residual graph obtained from G after pushing the flow K. Let E 0 ðx 1 ; x 2 Þ be the function exactly represented by G 0 ; V 0 .<br>2: The value of the minimum cut/<mark>maximum flow</mark> on G 0 is 0 (it is the minimum entry in the table for E); thus, there is no augmenting path from s to t in G 0 .<br>",
    "Arabic": "التدفق الأقصى",
    "Chinese": "最大流量",
    "French": "flot maximum",
    "Japanese": "最大フロー",
    "Russian": "максимальный поток"
  },
  {
    "English": "maximum likelihood",
    "context": "1: Training EBMs is a challenging task. Computing the likelihood for <mark>Maximum Likelihood</mark> inference requires computation of the normalizing constant Z = x e f θ (x) which is typically intractable. Thankfully, the gradient of the loglikelihood can be more easily expressed as: \n<br>2: by <mark>Maximum Likelihood</mark>) over a full parametrized transition cube leads to very poor estimates, we introduce a factorization model that gives a low-rank approximation to the transition cube. The advantages of this approach is that each transition is influenced by transitions of similar users, similar items and similar transitions.<br>",
    "Arabic": "أقصى احتمالية",
    "Chinese": "最大似然",
    "French": "maximum de vraisemblance",
    "Japanese": "最尤推定",
    "Russian": "максимальное правдоподобие"
  },
  {
    "English": "maximum likelihood estimate",
    "context": "1: λ(X) = sup Θ 0 L(θ|X) sup Θ L(θ|X) (1) \n This statistic is computed by first computing a <mark>maximum likelihood estimate</mark> (MLE) under both parameter spaces Θ0 and Θ, and then computing the ratio of the likelihoods obtained via the two MLEs.<br>2: ] \n As a consequence of convexity, standard gradientbased optimization techniques converge to the <mark>maximum likelihood estimate</mark> of feature weights,θ.<br>",
    "Arabic": "تقدير الأرجحية القصوى",
    "Chinese": "最大似然估计",
    "French": "estimation du maximum de vraisemblance",
    "Japanese": "最尤推定",
    "Russian": "оценка максимального правдоподобия"
  },
  {
    "English": "maximum likelihood estimation",
    "context": "1: Here, C(θ, {µ t , Σ t }) is defined in (10), which is the lower bound for <mark>maximum likelihood estimation</mark>. The second term in ( 11) is a regularization penalty on the generator weights.<br>2: In addition, the proposed models are trained using the <mark>maximum likelihood estimation</mark> method and the cross-entropy loss function, which requires humans to predefine the order of the output labels. Therefore, the sorting of labels is very important for the models' performance. Besides, the performance of both models declines when we do not use the mask module.<br>",
    "Arabic": "تقدير الاحتمال الأقصى",
    "Chinese": "最大似然估计",
    "French": "estimation du maximum de vraisemblance",
    "Japanese": "最尤推定",
    "Russian": "оценка максимального правдоподобия"
  },
  {
    "English": "maximum likelihood estimator",
    "context": "1: We also propose a generalized closed-form <mark>maximum likelihood estimator</mark> (MLE) for asynchronous acquisition that can be computed without any iterative optimization routine.<br>2: Suppose each item has a true (but unknown) category (its gold standard). We may view an annotator's judgement as a noisy signal of the gold standard. We now want to design an aggregator as a <mark>maximum likelihood estimator</mark> for this ground truth. This approach has been pioneered by Dawid and Skene (1979).<br>",
    "Arabic": "مقدر الاحتمالية القصوى",
    "Chinese": "最大似然估计量",
    "French": "estimateur du maximum de vraisemblance",
    "Japanese": "最尤推定量",
    "Russian": "оценщик максимального правдоподобия"
  },
  {
    "English": "maximum likelihood learning",
    "context": "1: Since the Potts model is unnormalized, <mark>maximum likelihood learning</mark> is difficult, and 1-regularized Pseudo-likelihood Maximization (PLM) (Besag, 1975) is used to train the model. Recently Ingraham & Marks (2017) found that improved contact prediction could be achieved with MCMCbased <mark>maximum likelihood learning</mark>.<br>",
    "Arabic": "التعلم بأقصى احتمالية",
    "Chinese": "最大似然学习",
    "French": "apprentissage par maximum de vraisemblance",
    "Japanese": "最尤学習",
    "Russian": "обучение методом максимального правдоподобия"
  },
  {
    "English": "maximum mean discrepancy",
    "context": "1: For similarity-metrics between sites, we follow Guo et al. (2020) and calculate the l2-distance, cosine distance, MMD (<mark>maximum mean discrepancy</mark>) distance (Gretton et al., 2012;Li et al., 2015) and the CORAL (correlation alignment) distance . Following Guo et al.<br>",
    "Arabic": "الفرق المتوسط الأقصى",
    "Chinese": "最大均值差异",
    "French": "divergence maximale moyenne",
    "Japanese": "最大平均不一致",
    "Russian": "максимальная средняя дискрепанция"
  },
  {
    "English": "mean average precision",
    "context": "1: While <mark>mean average precision</mark> over the full set of 100,000 object classes is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.<br>2: An example for such a measure of quality is the precision-at-10 (P@10) or the <mark>mean average precision</mark> (MAP) [22] of the query. Estimation of query difficulty is advantageous for several reasons: \n 1. Feedback to the user: The user can rephrase a \"difficult\" query to improve system effectiveness. 2.<br>",
    "Arabic": "متوسط ​​الدقة",
    "Chinese": "平均精度",
    "French": "précision moyenne moyenne",
    "Japanese": "平均適合率",
    "Russian": "средняя средняя точность"
  },
  {
    "English": "mean field",
    "context": "1: We use a <mark>mean field</mark> approach to update each of the RHS factors in turn to minimize the KL-divergence between the current variational posterior and the true model posterior. The δ r , δ s , and δ d factors place point estimates on a single value, just as in hard EM.<br>",
    "Arabic": "الحقل المتوسط",
    "Chinese": "平均场",
    "French": "champ moyen",
    "Japanese": "平均場",
    "Russian": "среднее поле"
  },
  {
    "English": "mean function",
    "context": "1: In Figure 2 (right) we see that in training a GP with an overly flexible <mark>mean function</mark> we are able to overfit to the training data, and produce extrapolation predictions that are both incorrect, and very confident.<br>2: Running the GP-UCB with β t for a sample f of a GP with <mark>mean function</mark> zero and covariance function k(x, x ), we obtain a regret bound of O * ( √ dT γ T ) with high probability. Precisely, with C 1 = 8/ log(1 + σ −2 ) we have \n<br>",
    "Arabic": "الدالة المتوسطة",
    "Chinese": "均值函数",
    "French": "fonction moyenne",
    "Japanese": "平均関数",
    "Russian": "средняя функция"
  },
  {
    "English": "mean pooling",
    "context": "1: For <mark>mean pooling</mark>, we observe that using two frames provides a notable improvement over using a single frame. However, models that use more than two frames perform similarly compared to the one using two frames, suggesting that two frames already represent enough local temporal information for the tasks. Do more clips at inference help?<br>2: For this experiment, we provide two models trained with    [45]. In <mark>mean pooling</mark> and max pooling, the cross-clip pooling is performed over logits, followed by a softmax operator. In LogSumExp, logits from each clip are first fed through an element-wise exponential operator, followed by a cross-clip <mark>mean pooling</mark>.<br>",
    "Arabic": "تجميع المتوسط",
    "Chinese": "均值汇聚",
    "French": "Regroupement moyen",
    "Japanese": "平均プーリング",
    "Russian": "усредненное объединение"
  },
  {
    "English": "mean reciprocal rank",
    "context": "1: Our method outputs the most probable y given (r, x). Here and in the supplementary material, we report its average performance on all test examples, with precision-at-1 (P@1), precision-at-10 (P@10) and <mark>mean reciprocal rank</mark> (MRR) as metrics.<br>2: For each query in the query set, we perform a nearest neighbor search using cosine similarity on the encoded representation of each query and text in the target set. We use recall@8 (R@8) and <mark>mean reciprocal rank</mark> (MRR) as the performance metrics in our experiments.<br>",
    "Arabic": "ترتيب متوسط المعكوس",
    "Chinese": "平均倒数排名",
    "French": "rang réciproque moyen",
    "Japanese": "平均逆数ランク",
    "Russian": "средний реципрокный ранг"
  },
  {
    "English": "mean shape",
    "context": "1: Our learnt models are at a canonical bounding box scale -all objects are first resized to a particular width during training. Given the predicted bounding box, we scale the learnt <mark>mean shape</mark> of the predicted subcategory Figure 4: Mean shapes learnt for rigid classes in PASCAL VOC obtained using our basis shape formulation. Color encodes depth when viewed frontally.<br>2: Typically this model consists of a <mark>mean shape</mark> and a distribution function which describes variation about this <mark>mean shape</mark>.<br>",
    "Arabic": "الشكل المتوسط",
    "Chinese": "平均形状",
    "French": "forme moyenne",
    "Japanese": "平均形状",
    "Russian": "средняя форма"
  },
  {
    "English": "mean square error",
    "context": "1: For each of them we show the new topology obtained, the values of the indexes SSE i used to decide the next change and the <mark>mean square error</mark> reached for the given topology after the adaptation of the new consequents. Note that the values of the MSE have been multiplied by 10 3 in order to make them more readable.<br>2: is the function of V (N i \\{i}) that minimizes the <mark>mean square error</mark> \n min g E[U (i) − g(V (N i \\{i}))] 2 \n Similar optimality theoretical results have been obtained in [9] and presented for the denoising of binary images.<br>",
    "Arabic": "خطأ المتوسط المربعي",
    "Chinese": "均方误差",
    "French": "erreur quadratique moyenne",
    "Japanese": "平均二乗誤差",
    "Russian": "среднеквадратичная ошибка"
  },
  {
    "English": "mean vector",
    "context": "1: where p s,m stands for the prior probability of the m th mixture in state s, and N s,m stands for the Gaussian distribution whose <mark>mean vector</mark> is noted µ s,m and whose covariance matrix is noted Σ s,m . where, using notation p s,m (x) \n<br>2: The lower bound is tight when ||µ j || is close to 1, which in effect means a strong intraclass concentration on the hypersphere. Intuitively, when the hypothesis space is rich enough, it is possible to achieve a low intraclass covariance in the Euclidean space, resulting in a large norm of the <mark>mean vector</mark> ||µ j ||.<br>",
    "Arabic": "متجه المتوسط",
    "Chinese": "均值向量",
    "French": "vecteur moyen",
    "Japanese": "平均ベクトル",
    "Russian": "средний вектор"
  },
  {
    "English": "mean-field approximation",
    "context": "1: networks that separate the input, can compute complex functions of the input stream. Furthermore, the authors introduced an accurate predictor for the computational capabilities for the considered type of networks based on the separation capability which was quantified via a simple <mark>mean-field approximation</mark> of the Hamming distance between different network states.<br>2: While Carbonetto and Stephens (2012) also use a <mark>mean-field approximation</mark> for linear regression, their approach to estimating g, σ 2 is quite different-and substantially more complexthan the VEB approach we describe here.<br>",
    "Arabic": "تقريب الحقل المتوسط",
    "Chinese": "平均场近似",
    "French": "approximation du champ moyen",
    "Japanese": "平均場近似",
    "Russian": "среднепольное приближение"
  },
  {
    "English": "measurable space",
    "context": "1: Let (Ω, F Ω ) be a <mark>measurable space</mark>, and Π be a random point process on Ω. Each realization of Π uniquely corresponds to a counting measure N Π defined by N Π (A) #(Π ∩ A) for each A ∈ F Ω .<br>2: With the careful establishment of the controller's and adversary's policy classes in this section, we lay the foundation for the subsequent definition of the max-min control value of a robust MDP. The formulation of the value function allows us to formalize robust policy learning and decision-making using the robust MDP framework. We start with introducing some notations. For function f on the <mark>measurable space</mark> ( E , E ) and measure ν ∈ P ( E ) , we define the integral ν [ f ] : = With these notations in place , we are now equipped to articulate the collective effect of a pair ( π , κ ) , comprising the controller 's and adversary 's policies ,<br>",
    "Arabic": "فضاء قابل للقياس",
    "Chinese": "可测空间",
    "French": "espace mesurable",
    "Japanese": "可測空間",
    "Russian": "измеримое пространство"
  },
  {
    "English": "measurement matrix",
    "context": "1: • In the context of streaming algorithms through the design of 'sketches' (see [19], [20], [21], [22], [23]) for the purpose of maintaining a minimal 'memory state' for the streaming algorithm's operation. In all of the above work , the basic question ( see [ 24 ] ) pertains to the design of an m × n `` measurement '' matrix A so that x can be recovered efficiently from measurements y = Ax ( or its noisy version ) using the `` fewest '' possible number measurements m. The setup of interest is when x is sparse and when m < n or m ≪ n. The type of interesting results ( such as those cited above ) pertain to characterization of the sparsity K of x that can be recovered for a given number of measurements m. The usual tension is between the ability to recover x with large k using a sensing matrix A with minimal m<br>2: However, as shown below, the types of partial information we consider can be written as a linear transform of f (•). Therefore, Example II-C.1 shows that in our setting, the <mark>measurement matrix</mark> does not satisfy RNC. It is natural to wonder if Example II-C.1 is anomalous.<br>",
    "Arabic": "مصفوفة القياس",
    "Chinese": "测量矩阵",
    "French": "matrice de mesure",
    "Japanese": "計測行列",
    "Russian": "матрица измерений"
  },
  {
    "English": "measurement noise",
    "context": "1: As notation, let f : X → R be an unknown function with domain X ⊆ R d whose behavior is indicated by a training set consisting of n Gaussian observations y i = f (x i ) + ε i subject to <mark>measurement noise</mark> ε i ∼ N (0, σ 2 ).<br>2: e uncertainty about the system's current state is due to <mark>measurement noise</mark> and actuation imperfections. Being able to ascertain, rigorously, bounds on the system state over [t, t + T ] despite current uncertainty allows the car to avoid unsafe plans.<br>",
    "Arabic": "ضوضاء القياس",
    "Chinese": "测量噪声",
    "French": "bruit de mesure",
    "Japanese": "測定ノイズ",
    "Russian": "Шум измерений"
  },
  {
    "English": "mechanical Turk",
    "context": "1: It sounds like a person wrote this description. Figure 12: <mark>Mechanical Turk</mark> prompts. We report the scores for the systems in Table 4 These findings are striking, particularly because Midge uses the same input as the Kulkarni et al. system.<br>",
    "Arabic": "ترك ميكانيكي",
    "Chinese": "机械土耳其",
    "French": "Turc mécanique",
    "Japanese": "メカニカルターク",
    "Russian": "Mechanical Turk"
  },
  {
    "English": "medical imaging",
    "context": "1: This problem is both fundamental and of great importance to a variety of computer vision areas ranging from traditional tasks such as visual saliency, segmentation, object detection/recognition, tracking and motion analysis, <mark>medical imaging</mark>, structurefrom-motion and 3D reconstruction, to modern applications like autonomous driving, mobile computing, and image-totext analysis.<br>",
    "Arabic": "التصوير الطبي",
    "Chinese": "医学成像",
    "French": "imagerie médicale",
    "Japanese": "医療画像処理",
    "Russian": "медицинская визуализация"
  },
  {
    "English": "medoid",
    "context": "1: The proposed topic labeling technique can then be applied on the estimated term distributions, and the top ranked phrases are used to label the original cluster. The generated cluster labels, along with the number of documents and the title of the <mark>medoid</mark> document of each cluster are shown in Table 10.<br>",
    "Arabic": "مديد",
    "Chinese": "中心对象",
    "French": "médoïde",
    "Japanese": "メドイド",
    "Russian": "медоид"
  },
  {
    "English": "membership inference attack",
    "context": "1: have shown that DNNs' output can leak the membership privacy of the input (i.e., whether the input belongs to the training dataset) under <mark>membership inference attack</mark> (MIA).<br>2: Machine learning models are notoriously known to suffer from a wide range of privacy attacks (Lyu et al., 2020), such as model inversion attack (Fredrikson et al., 2015), <mark>membership inference attack</mark> (MIA) (Shokri et al., 2017), property inference attack (Melis et al., 2019), etc.<br>",
    "Arabic": "هجوم كشف العضوية",
    "Chinese": "成员推断攻击 (membership inference attack)",
    "French": "attaque d'inférence d'appartenance",
    "Japanese": "メンバーシップ推論攻撃",
    "Russian": "атака вывода о принадлежности"
  },
  {
    "English": "membership query",
    "context": "1: Note that polynomial time learning with only <mark>membership queries</mark> is important because it is related to whether CQs can be characterized up to equivalence using only polynomially many data examples [ten Cate and Dalmau, 2020].<br>2: , b k ) by replacing any number of components b j by b j . Use member- ship queries to identifyb i+1 ∈ τ i with B i , O |= q T (b i+1 ) and set B i+1 = minimize(B i ,b i+1 ).<br>",
    "Arabic": "استعلام العضوية",
    "Chinese": "成员查询",
    "French": "requête d'appartenance",
    "Japanese": "所属クエリ",
    "Russian": "членские запросы"
  },
  {
    "English": "memory bank",
    "context": "1: Simple and effective instantiations of contrastive learning have been developed using Siamese networks [37,2,17,8,9]. In practice, contrastive learning methods benefit from a large number of negative samples [36,35,17,8]. These samples can be maintained in a <mark>memory bank</mark> [36].<br>2: Figure 7 also shows that using the movingaverage representations (q s t i in Eq. ( 3)) of negative samples in the <mark>memory bank</mark> has better performance than using the immediate representations (h s t i in Eq. (3)), because of a smoother and more consistent representation of tokens.<br>",
    "Arabic": "مصرف الذاكرة",
    "Chinese": "内存库",
    "French": "banque de mémoire",
    "Japanese": "メモリバンク",
    "Russian": "банк памяти"
  },
  {
    "English": "memory capacity",
    "context": "1: Moreover, theoretical considerations alone demand such an expansion; network models with scalar synapses assuming finite numbers of distinguishable synaptic strengths have strikingly limited <mark>memory capacity</mark>. This raises the fundamental question, how does synaptic complexity give rise to memory?<br>2: In this work, rather than analyzing specific models, we take a different approach, in order to obtain a more general theory. We consider the entire space of these models and find upper bounds on the <mark>memory capacity</mark> of any of them.<br>",
    "Arabic": "سعة الذاكرة",
    "Chinese": "记忆容量",
    "French": "capacité de mémoire",
    "Japanese": "メモリ容量",
    "Russian": "объем памяти"
  },
  {
    "English": "memory cell",
    "context": "1: To understand this pattern, recall that a boundary in our model represents both a cost 8 See Appendix G for details. (flushing the <mark>memory cell</mark>) and a benefit (injecting top-down feedback).<br>",
    "Arabic": "خلية الذاكرة",
    "Chinese": "记忆细胞",
    "French": "cellule mémoire",
    "Japanese": "メモリセル",
    "Russian": "клетка памяти"
  },
  {
    "English": "memory complexity",
    "context": "1: Our current algorithm has two main limitations: (i) Although occlusions can be handled to some extent, it cannot handle extreme occlusions (such as when only small fragmented parts of the object are visible). (ii) The time and <mark>memory complexity</mark> of our current inference algorithm is linear in the size of the example database.<br>2: Asynchronous memory usage GIM provides a significant practical advantage arising from the greedy nature of optimization: modules can be trained in isolation given cached outputs from previous modules, effectively removing the depth of the network as a factor of the <mark>memory complexity</mark>.<br>",
    "Arabic": "تعقيد الذاكرة",
    "Chinese": "内存复杂度",
    "French": "complexité mémoire",
    "Japanese": "メモリ複雑度",
    "Russian": "сложность памяти"
  },
  {
    "English": "mention detection",
    "context": "1: A strong baseline is proposed by combining powerful neural reading comprehension with domainadaptive pre-training. Future variations of the task could incorporate NIL recognition and <mark>mention detection</mark> (instead of mention boundaries being provided). The candidate generation phase leaves significant room for improvement.<br>",
    "Arabic": "كشف الإشارة",
    "Chinese": "实体提及检测",
    "French": "détection de mention",
    "Japanese": "言及の検出",
    "Russian": "обнаружение упоминаний"
  },
  {
    "English": "meronymy",
    "context": "1: Some studies have tried to assess the semantic proximity of two given concepts in order to improve the semantic similarity computation. These studies focus on similarity and they use synonymy 1 , hyponymy 2 [19], <mark>meronymy</mark> 3 and other arbitrarily typed semantic relationships. These relationships can be used to connect concepts in graph structures.<br>2: A number of linguistic devices can be used to signal cohesion; these range from repetition, to synonymy, hyponymy and <mark>meronymy</mark>. Lexical chains are a representation of lexical cohesion as sequences of semantically related words (Morris and Hirst 1991) and provide a useful means for describing the topic flow in discourse.<br>",
    "Arabic": "الجزءية",
    "Chinese": "部分整体关系",
    "French": "méronymie",
    "Japanese": "部分全体関係",
    "Russian": "меронимия"
  },
  {
    "English": "message passing",
    "context": "1: For exact inference as described in Alg. 2, Step 1 to Step 4 (processing the graph, building the junctions trees, listing state space etc.) only depend on the graph structure and are performed offline. Only the <mark>message passing</mark> on the junction tree (Step 5) needs to be performed for each example online.<br>2: A naive implementation thus has quadratic complexity in the number of variables N . Next, we show how approximate high-dimensional filtering can be used to reduce the computational cost of <mark>message passing</mark> to linear.<br>",
    "Arabic": "تمرير الرسالة",
    "Chinese": "消息传递",
    "French": "passage de messages",
    "Japanese": "メッセージパッシング",
    "Russian": "передача сообщений"
  },
  {
    "English": "message passing algorithm",
    "context": "1: The algorithm is up to an order of magnitude faster and provides better MAP scores and bounds than the state of the art <mark>message passing algorithm</mark> of [1] that tightens the local marginal polytope with third-order marginal constraints.<br>2: A naive implementation of the <mark>message passing algorithm</mark> presented in Section 4.2 is very inefficient, since independent descriptor queries are performed for each patch in the observation ensemble, regardless of answers to previous queries performed by other patches.<br>",
    "Arabic": "خوارزمية تمرير الرسائل",
    "Chinese": "消息传递算法",
    "French": "algorithme de passage de messages",
    "Japanese": "メッセージ渡しアルゴリズム",
    "Russian": "алгоритм передачи сообщений"
  },
  {
    "English": "meta",
    "context": "1: The feature selection part gives rise to another independent decision between roughly 10 6 choices, and several parameters on the <mark>meta</mark> and ensemble level contribute another order of magnitude to the total size of AutoWEKA's hypothesis space. Auto-WEKA can be understood as a single learning algorithm with a highly conditional parameter space, as depicted in Figure 1.<br>",
    "Arabic": "ميتا",
    "Chinese": "元",
    "French": "méta",
    "Japanese": "メタ",
    "Russian": "мета"
  },
  {
    "English": "meta-algorithm",
    "context": "1: To this end, we suggest a <mark>meta-algorithm</mark> that combines these two approaches: sketches and coresets. It may be generalized to other, not-necessarily accurate, ε-coresets and sketches (ε > 0); see Section 9.<br>2: the two-stage <mark>meta-algorithm</mark>) to learning mixtures of other time-series models (potentially with model selection [WL00]), such as LDS with partial observations or nonlinear observations [MFS + 20], autoregressive-moving-average (ARMA) models, nonlinear dynamical systems [MJR20, KKL + 20, FSR20], to name a few.<br>",
    "Arabic": "الميتا خوارزمية",
    "Chinese": "元算法",
    "French": "méta-algorithme",
    "Japanese": "メタアルゴリズム",
    "Russian": "мета-алгоритм"
  },
  {
    "English": "meta-classifier",
    "context": "1: We also demonstrate the importance of sequential decision-making by comparing our model to a <mark>meta-classifier</mark> operating on the same space, obtaining up to a 7% gain.<br>2: We employ a deep Qnetwork, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -of shooting incidents, and food adulteration cases -demonstrate that our system significantly outperforms traditional extractors and a competitive <mark>meta-classifier</mark> baseline. 1<br>",
    "Arabic": "الميتا مصنف",
    "Chinese": "元分类器",
    "French": "méta-classificateur",
    "Japanese": "メタ分類器",
    "Russian": "мета-классификатор"
  },
  {
    "English": "meta-dataset",
    "context": "1: NATURAL IN-STRUCTIONS (Mishra et al., 2022) is a <mark>meta-dataset</mark> containing diverse tasks with human-authored definitions, things to avoid, and demonstrations. It has shown effectiveness in improving the generalizability of language models even when the size is relatively small (e.g., BART_base) (Mishra et al., 2022;Wang et al., 2022d).<br>",
    "Arabic": "ميتا مجموعة البيانات",
    "Chinese": "元数据集",
    "French": "méta-ensemble de données",
    "Japanese": "メタデータセット",
    "Russian": "мета-набор данных"
  },
  {
    "English": "meta-evaluation",
    "context": "1: In this paper, we propose the first large-scale <mark>meta-evaluation</mark> of MT in which we manually annotated 769 research papers published from 2010 to 2020. Our study shows that evaluation in MT has dramatically changed since 2010. An increasing number of publications exclusively rely on BLEU scores to draw their conclusions.<br>2: Task Model English DST I am looking for a restaurant that has cheap price range and a rating of 5. Figure 1: The <mark>meta-evaluation</mark> pipeline.<br>",
    "Arabic": "التقييم الفوقي",
    "Chinese": "元评估",
    "French": "méta-évaluation",
    "Japanese": "メタ評価",
    "Russian": "мета-оценка"
  },
  {
    "English": "meta-learn",
    "context": "1: The multi-scale features are progressively upsampled and fused by convolutional blocks, followed by a convolutional head for final prediction. Similar to the label encoder, all parameters of the label decoder are trained from scratch and shared across tasks. This lets the decoder to <mark>meta-learn</mark> a generalizable strategy of decoding a structured label from the predicted query label tokens.<br>2: To succeed, a memory-less agent must efficiently re-explore the environment. We study an on-policy actor-critic agent with PG = TD = 1. As baseline, we tune a fixed entropy-rate weight = EN . We compare against agents that <mark>meta-learn</mark> online. For MG, we use the actor-critic loss as meta-objective ( fixed), as per Eq.<br>",
    "Arabic": "تعلّم ميتا",
    "Chinese": "元学习",
    "French": "méta-apprentissage",
    "Japanese": "メタ学習",
    "Russian": "метаобучение"
  },
  {
    "English": "meta-learner",
    "context": "1: Bootstrapping has recently been introduced in the self-supervised setting (Guo et al., 2020;Grill et al., 2020). In this paper, we introduce the idea of bootstrapping in the context of meta-learning, where a <mark>meta-learner</mark> learns about an update rule by generating future targets from it.<br>2: In particular, this form of Bootstrapped Meta-Gradient (BMG) enables us to infuse information about future learning dynamics without increasing the number of update steps to backpropagate through. In effect, the <mark>meta-learner</mark> becomes its own teacher.<br>",
    "Arabic": "ميتا المتعلم",
    "Chinese": "元学习器",
    "French": "méta-apprenant",
    "Japanese": "メタ学習者",
    "Russian": "мета-обучающийся"
  },
  {
    "English": "meta-learning",
    "context": "1: <mark>Meta-learning</mark> to Graph Prompting. Let be prompt parameters, * be the fixed parameters of the pre-trained graph backbone, and be the tasker's parameters. We use , | * to denote the pipeline with prompt graph ( ), pre-trained model ( * , fixed), and downstream tasker ( ).<br>",
    "Arabic": "التعلم التلوي",
    "Chinese": "元学习",
    "French": "méta-apprentissage",
    "Japanese": "メタラーニング",
    "Russian": "метаобучение"
  },
  {
    "English": "meta-loss",
    "context": "1: While the TB ξ α G (x (K) ) := x (K) −αG T g yields performance improvements that are proportional to the <mark>meta-loss</mark> itself, larger improvements are possible by choosing a TB that carries greater learning signal (by increasing µ(x, x (K) )).<br>2: We use gradient clipping of 3 applied to each gradient coordinate. We outer-train on 8 TPUv2 cores with asynchronous, batched updates of size 16. To evaluate, we compute the <mark>meta-loss</mark> averaged over 20 inner initializations over the course of meta-training. Results can be found in Figure 5.<br>",
    "Arabic": "خسارة ميتا",
    "Chinese": "元损失",
    "French": "méta-perte",
    "Japanese": "メタ損失",
    "Russian": "мета-потеря"
  },
  {
    "English": "meta-parameter",
    "context": "1: Since there are no <mark>meta-parameters</mark> in the update rule, all L steps use the same update rule. However, we define the target policy as the greedy policy \n πx(a | s t ) =    1 if a = arg max b qx(s t , b) 0 else.<br>2: Even if our RL model with estimated <mark>meta-parameters</mark> is capable of reproducing behaviour of different experimental groups in the hole-box, this does not tell us how, given a new animal in an arbitrary experimental condition, we should set daily <mark>meta-parameters</mark> to predict its behaviour.<br>",
    "Arabic": "المعلمة الفوقية",
    "Chinese": "元参数",
    "French": "métaparamètre",
    "Japanese": "メタパラメータ",
    "Russian": "метапараметр"
  },
  {
    "English": "meta-testing",
    "context": "1: Among many candidates to design an adaptation mechanism through θ T , we find that bias tuning (Cai et al., 2020;Zaken et al., 2022) provides the best efficiency and performance empirically. To this end, we employ separate sets of biases for each task in both meta-training and <mark>meta-testing</mark>, while sharing all the other parameters.<br>",
    "Arabic": "اختبار متا",
    "Chinese": "元测试",
    "French": "méta-tests",
    "Japanese": "メタテスト",
    "Russian": "мета-тестирование"
  },
  {
    "English": "meta-training",
    "context": "1: We study how the data-efficiency and computational efficiency of the BMG meta-objective compares against that of the MG meta-objective. To this end, for data efficiency, we report the meta-test set performance as we vary the number of meta-batches each algorithm is allow for <mark>meta-training</mark>.<br>2: To this end, we utilize a <mark>meta-training</mark> dataset D train that contains labeled examples of diverse dense prediction tasks. Each training episode simulates a few-shot learning scenario of a specific task T train in the dataset -the objective is to produce correct labels for query images given a support set.<br>",
    "Arabic": "تدريب متا",
    "Chinese": "元训练",
    "French": "méta-entraînement",
    "Japanese": "メタトレーニング",
    "Russian": "мета-обучение"
  },
  {
    "English": "metadata",
    "context": "1: We use a Transformer-based [56] causal language model, Grover [61], which is similar to GPT-2, but tailored to generating news by conditioning on the <mark>metadata</mark> of the article as well. Our generations rely on pretrained Grover architectures of various sizes.<br>2: Sections 4.3 and 4.4 then give results of a sensitivity analysis to the availability of machine-and human-generated <mark>metadata</mark> to develop the similarity functions and measurement values respectively of Section 3.3.<br>",
    "Arabic": "بيانات التعريف",
    "Chinese": "元数据",
    "French": "métadonnées",
    "Japanese": "メタデータ",
    "Russian": "метаданные"
  },
  {
    "English": "metric learning",
    "context": "1: As such, searching for similar examples according to a learned metric currently requires an exhaustive (linear) scan of all previously seen examples, in the worst case. This is a limiting factor that thus far deters the use of <mark>metric learning</mark> with very large image databases.<br>2: Recent advances in <mark>metric learning</mark> make it possible to learn distance (or kernel) functions that are more effective for a given problem, provided some partially labeled data or constraints are available [30,3,15,8,10]. By taking advantage of the prior information, these techniques offer improved accuracy when indexing or classifying examples.<br>",
    "Arabic": "تعلم المقاييس",
    "Chinese": "度量学习",
    "French": "apprentissage métrique",
    "Japanese": "メトリック学習",
    "Russian": "обучение метрики"
  },
  {
    "English": "metric learning algorithm",
    "context": "1: Furthermore, we introduce a novel <mark>metric learning algorithm</mark> that can learn weighted edit distances that minimize kernel regression error. Our results suggest that the English lexicon exhibits far more global form-meaning systematicity than previously discovered, and that much of this systematicity is focused in localized formmeaning patterns.<br>2: In the latter problem a lowrank kernel K is learned that satisfies a set of given distance constraints by minimizing the LogDet divergence to a given initial kernel K 0 . This allows our <mark>metric learning algorithm</mark> to be kernelized, resulting in an optimization over a larger class of non-linear distance functions.<br>",
    "Arabic": "خوارزمية تعلم المقاييس",
    "Chinese": "度量学习算法",
    "French": "algorithme d'apprentissage métrique",
    "Japanese": "メトリック学習アルゴリズム",
    "Russian": "алгоритм обучения метрике"
  },
  {
    "English": "metric score",
    "context": "1: Comparisons between MT systems through their <mark>metric scores</mark> may be performed to demonstrate the superiority of a method or an algorithm only if the systems have been trained, validated, and tested with exactly the same pre-processed data, unless the proposed method or algorithm is indeed dependent on a particular dataset or pre-processing.<br>2: Furthermore, tools for reporting standardized <mark>metric scores</mark> are still far from being widely adopted by the MT community. After showing how the accumulation of these pitfalls leads to dubious evaluation, we propose a guideline to encourage better automatic MT evaluation along with a simple meta-evaluation scoring method to assess its credibility.<br>",
    "Arabic": "درجة القياس",
    "Chinese": "评测分数",
    "French": "\"score métrique\"",
    "Japanese": "メトリックスコア",
    "Russian": "метрический балл"
  },
  {
    "English": "metric space",
    "context": "1: The problem, therefore, is to combine exemplars in a <mark>metric space</mark> (Gavrila and Philomin, 1999) with a probabilistic treatment (Frey and Jojic, 2000), retaining the best features of each approach. Unfortunately, this combination is not straightforward.<br>2: We used the Praat toolkit to generate the spectrogram and find the formants (Boersma et al., 2002). language seeks phonemes that are sufficiently \"distant\" from one another to avoid confusion. Distances between phonemes are defined in some latent \"<mark>metric space</mark>.\"<br>",
    "Arabic": "فضاء متري",
    "Chinese": "度量空间",
    "French": "espace métrique",
    "Japanese": "メトリック空間",
    "Russian": "метрическое пространство"
  },
  {
    "English": "metropolis-hasting",
    "context": "1: Theorem 5 (Maciuca and Zhu, 2003). Let p(W ), W ∈ be the invariance (target) probability of a Markov chain MC, and Q(W, W ) = q(W ) be the proposal probability in the <mark>Metropolis-Hasting</mark> Eq. (8), then \n<br>",
    "Arabic": "ميتروبوليس-هاستينج",
    "Chinese": "大都市-黑斯廷",
    "French": "Metropolis-Hasting",
    "Japanese": "メトロポリス・ヘイスティング法",
    "Russian": "метрополис-хастинг"
  },
  {
    "English": "micro-average",
    "context": "1: Instead, a better choice is 3 ( , ) = | ∩ | from Luo (2005): this computes a <mark>micro-average</mark> score of all mentions, and it adequately assigns partial credit to the overlap between the predicted mention set and the reference mention set. See Figure 4 for a succinct comparison among these variants.<br>2: Reported precision, recall and F-score are computed using a <mark>micro-average</mark>, i.e., from the aggregate true positive, false positive and false negative rates, as suggested by Forman and Scholz (2009).<br>",
    "Arabic": "المتوسط ​​الجزئي",
    "Chinese": "微平均",
    "French": "micro-moyenne",
    "Japanese": "マイクロ平均",
    "Russian": "микроусреднение"
  },
  {
    "English": "microarray datum",
    "context": "1: Moreover, for such data, coherent clusters could be arbitrarily positioned in subspaces formed by different, possibly overlapping subsets of features, e.g., different subsets of genes may be correlated across different subsets of experiments in <mark>microarray data</mark>. Additionally, it is possible that some features may not be relevant to any cluster.<br>2: Coclustering simultaneously clusters the data along multiple axes, e.g., in the case of <mark>microarray data</mark> it simultaneously clusters the genes as well as the experiments (Cheng & Church, 2000) and can hence detect clusters existing in different subspaces of the feature space.<br>",
    "Arabic": "بيانات الصفيف المجهري",
    "Chinese": "微阵列数据",
    "French": "donnée de microréseau",
    "Japanese": "マイクロアレイデータ",
    "Russian": "данные микрочипа"
  },
  {
    "English": "mini-batch",
    "context": "1: where b is the size of <mark>mini-batch</mark>. Sentence Order Prediction Loss We formulate L sop as the cross-entropy loss between the golden and predicted orders as follows: \n L sop = − 1 n n i=1 o i log(p i ),(8) \n<br>2: Training with ResNet-50-FPN on COCO trainval35k takes 32 hours in our synchronized 8-GPU implementation (0.72s per 16image <mark>mini-batch</mark>), and 44 hours with ResNet-101-FPN. In fact, fast prototyping can be completed in less than one day when training on the train set.<br>",
    "Arabic": "دفعة صغيرة",
    "Chinese": "小批量",
    "French": "mini-lot",
    "Japanese": "ミニバッチ",
    "Russian": "мини-пакет"
  },
  {
    "English": "mini-batch size",
    "context": "1: For further demonstration of the memory saving effect of CAME, we expand BERT model to BERT-4B with 4 billion weights using the scaling method of GPT-3 (Brown et al., 2020). We set the <mark>mini-batch size</mark> to 64 and the accumulation steps to 16 in this experiment.<br>2: We call this algorithm DeFacto (Algorithm 1). DeFacto is statistically equivalent to centralized SGD operating T /2R iterations with a <mark>mini-batch size</mark> of BR. It can be easily verified that DeFacto holds membership in A B . A straightforward analysis gives the convergence rate of DeFacto shown in the following Theorem.<br>",
    "Arabic": "حجم الدُفعات الصغيرة",
    "Chinese": "迷你批量大小",
    "French": "taille du mini-batch",
    "Japanese": "ミニバッチサイズ",
    "Russian": "размер мини-пакета"
  },
  {
    "English": "mini-batch training",
    "context": "1: In particular, Figure 1(a) shows that the scalability of GraphSAGE is limited even when the <mark>mini-batch training</mark> and graph sampling method are adopted. Figure 1(b) further shows that the scalability is mainly bottlenecked by the aggregation procedure in which high data loading cost is incorporated to gather neighborhood information.<br>",
    "Arabic": "التدريب بالدُفعات الصغيرة",
    "Chinese": "小批量训练",
    "French": "entraînement par mini-lots",
    "Japanese": "ミニバッチ学習",
    "Russian": "обучение на мини-пакетах"
  },
  {
    "English": "minima",
    "context": "1: Two ridges are observed, and one can also notice that while the <mark>minima</mark> with the delta solution is much lower, the ridge from the low λ values is leading toward the true kernel local <mark>minima</mark>, and not toward the delta solution.<br>2: In the absence of an expert, we can use evolutionary strategies to find a good <mark>minima</mark> starting from a random initialization. Over 3 runs, we found that learning θ reduces the difference to 0.007 ± 0.005 in P1, even beating expert parameters by 0.005 ± 0.009 in P8.<br>",
    "Arabic": "أدنى قيمة",
    "Chinese": "极小值",
    "French": "minima",
    "Japanese": "局所最小値",
    "Russian": "минимумы"
  },
  {
    "English": "minimax",
    "context": "1: Stackelberg games also generalize previous <mark>minimax</mark> formulations (Xie et al., 2021), which correspond to a two-player zero-sum game with h = −g. Offline RL as a Stackelberg game Inspired by the <mark>minimax</mark> offline RL concept by Xie et al. (2021) and the pessimistic policy evaluation procedure by Kumar et al.<br>2: Note that any prediction function can be expressed this way, and thus the learner has full expressive power. It will turn out in Section 2.3 that the optimal function has a simple parametric form, due to the fact that it needs to be <mark>minimax</mark> optimal.<br>",
    "Arabic": "تصغير الحد الأقصى",
    "Chinese": "极小极大",
    "French": "minimax",
    "Japanese": "最小最大",
    "Russian": "минимакс"
  },
  {
    "English": "minimax game",
    "context": "1: Following the GAN approach [8], we model this as a two-player <mark>minimax game</mark>, and update the refiner network, R θ , and the discriminator network, D φ , alternately. Next, we describe this intuition more precisely. The discriminator network updates its parameters by minimizing the following loss: \n<br>2: As detailed in Alg. 2, we design a two-step adversarial learning strategy for UniVPM optimization, where the discriminator and generator play a twoplayer <mark>minimax game</mark>. First, we maximize L GAN to update the discriminator, where generator is detached from optimization. According to Eq.<br>",
    "Arabic": "لعبة الحد الأدنى الأقصى",
    "Chinese": "最小最大博弈",
    "French": "jeu minimax",
    "Japanese": "ミニマックスゲーム",
    "Russian": "минимаксная игра"
  },
  {
    "English": "minimax optimization problem",
    "context": "1: We begin by defining the <mark>minimax optimization problem</mark> we set out to solve. Consider classification problems with n discrete features corresponding to the vector random variable X = [X 1 , . . . , X n ]. Assume that each X i can take d i values so that X i ∈ {1, . . .<br>",
    "Arabic": "مشكلة التحسين الصغرى-العظمى",
    "Chinese": "极小极大优化问题",
    "French": "problème d'optimisation minimax",
    "Japanese": "最小最大最適化問題",
    "Russian": "задача минимаксной оптимизации"
  },
  {
    "English": "minimax problem",
    "context": "1: Our goal is to find a classifier which has minimal worst case error. The classifier that solves this <mark>minimax problem</mark> will be robust in the sense that it obtains the best error possible under our uncertainty about the true distribution. The above problem is generally hard to solve (Bertsimas and Sethuraman, 2000).<br>2: + λ idt L idt ( G , I yr , y r , y g ) , \n where λ A , λ y and λ idt are the hyper-parameters that control the relative importance of every loss term. Finally, we can define the following <mark>minimax problem</mark>: \n G = arg min G max D∈D L ,(6) \n<br>",
    "Arabic": "مشكلة الحد الأدنى الأقصى",
    "Chinese": "极小极大问题",
    "French": "problème minimax",
    "Japanese": "最小最大問題",
    "Russian": "минимаксная задача"
  },
  {
    "English": "minimization problem",
    "context": "1: The objective of the proposed R-SVM+ algorithm aims to solve a maximization problem in Eq. ( 9) and a <mark>minimization problem</mark> in Eq. ( 12) at the same time. Therefore, we arrive at the objective function of R-SVM+ which is a <mark>minimization problem</mark>, \n<br>2: That is, instead of considering predictors which are linear functions of the training instances x themselves, we consider predictors which are linear functions of some implicit mapping φ(x) of the instances. Training then involves solving the <mark>minimization problem</mark>: \n<br>",
    "Arabic": "مشكلة التصغير",
    "Chinese": "最小化问题",
    "French": "problème de minimisation",
    "Japanese": "最小化問題",
    "Russian": "задача минимизации"
  },
  {
    "English": "minimizer",
    "context": "1: That is, by rewriting the problem as a function of U only, then linearizing it, solving the resulting subproblem and updating the current iterate using the <mark>minimizer</mark> of said subproblem. Our starting point for the derivation our generalization of the Wiberg algorithm is again the minimization problem \n<br>2: Finally, we note that there is a closed form for the <mark>minimizer</mark> of the linear loss, which makes it computationally attractive.<br>",
    "Arabic": "مُصغر",
    "Chinese": "最小化器",
    "French": "minimiseur",
    "Japanese": "最小化点",
    "Russian": "минимизатор"
  },
  {
    "English": "minimum baye risk decoding",
    "context": "1: Translation edit rate (TER) (Snover et al., 2006) is used to align the hypotheses and <mark>minimum Bayes risk decoding</mark> under TER (Sim et al., 2007) is used to select the alignment hypothesis. All combination methods use weights which may be tuned using Powell's method (Brent, 1973) on ¤ -best lists.<br>2: Uses for k-best lists include <mark>minimum Bayes risk decoding</mark> (Goodman, 1998;Kumar and Byrne, 2004), discriminative reranking (Collins, 2000;Charniak and Johnson, 2005), and discriminative training (Och, 2003;McClosky et al., 2006).<br>",
    "Arabic": "الحد الأدنى من مخاطر فك تشفير بايز",
    "Chinese": "最小贝叶斯风险解码",
    "French": "décodage de risque bayésien minimal",
    "Japanese": "最小ベイズリスク復号化",
    "Russian": "декодирование минимального байесовского риска"
  },
  {
    "English": "minimum cut",
    "context": "1: The calculation is as follows. We first generate 100 random 12node graphs H1, . . . , H100, and see if any of them lacks non-trivial automorphisms and has a <mark>minimum cut</mark> of size at least 4.<br>2: (2) Let δ(H) denote the minimum degree in H, and let γ(H) denote the value of the <mark>minimum cut</mark> in H (i.e. the minimum number of edges whose deletion disconnects H). It is known that for a random graph H such as we have constructed , the following properties hold with probability going to 1 exponentially quickly in k [ 9 ] : first , that γ ( H ) = δ ( H ) ; second , that δ ( H ) ≥ ( 1/2 − ε ) k for any constant ε ><br>",
    "Arabic": "القطع الأدنى",
    "Chinese": "最小割",
    "French": "coupe minimale",
    "Japanese": "最小カット",
    "Russian": "минимальный разрез"
  },
  {
    "English": "minimum description length",
    "context": "1: MacKay (2003) uses the Laplace approximation to make connections between the marginal likelihood and the <mark>minimum description length</mark> framework. MacKay (1995) also notes that structural risk minimization (Guyon et al., 1992) has the same scaling behaviour as the marginal likelihood.<br>",
    "Arabic": "الحد الأدنى لطول الوصف",
    "Chinese": "最小描述长度",
    "French": "longueur de description minimale",
    "Japanese": "最小記述長",
    "Russian": "минимальная длина описания"
  },
  {
    "English": "minimum support",
    "context": "1: Once this is done, the algorithm then applies an association rule mining algorithm to discover all the association rules from this transaction database with a <mark>minimum support</mark> and confidence threshold defined by domain expert.<br>",
    "Arabic": "الحد الأدنى من الدعم",
    "Chinese": "最小支持度",
    "French": "support minimum",
    "Japanese": "最小サポート",
    "Russian": "минимальная поддержка"
  },
  {
    "English": "mirror descent",
    "context": "1: Figure 4 compares SGA with optimistic <mark>mirror descent</mark> on a zero-sum bimatrix game with 1/2 (w 1 , w 2 ) = ±w 1 w 2 . The example is modified from Daskalakis et al. (2018) who also consider a linear offset that makes no difference.<br>2: D R ( f , g 0 ) . We show that if the players use optimistic <mark>mirror descent</mark> with M t i = u t−1 i , then the regret of each player satisfies the sufficient condition presented in the previous section.<br>",
    "Arabic": "نزول المرآة",
    "Chinese": "镜像下降",
    "French": "descente en miroir",
    "Japanese": "ミラーディセント",
    "Russian": "зеркальный спуск"
  },
  {
    "English": "misclassification error",
    "context": "1: This completes the proof of Claim 2.1. Claim 2.1 shows that minimizing E x∼Dx (w,x) \n | w,x | is equivalent to minimizing the <mark>misclassification error</mark>. Unfortunately, this objective is hard to minimize as it is non-convex, but one would hope that minimizing L(w) instead may have a similar effect.<br>2: Our iterative approach is motivated by a new structural lemma (Lemma 2.5) establishing the following: Even though minimizing a convex proxy does not lead to small <mark>misclassification error</mark> over the entire space, there exists a region with non-trivial probability mass where it does. Moreover, this region is efficiently identifiable by a simple thresholding rule.<br>",
    "Arabic": "خطأ في التصنيف",
    "Chinese": "误分类误差",
    "French": "erreur de classification",
    "Japanese": "誤分類エラー",
    "Russian": "ошибка неправильной классификации"
  },
  {
    "English": "misclassification loss",
    "context": "1: number of training example pairs . The first approach proposed in the LUPI paradigm is called SVM+ [Vapnik and Vashist, 2009], which tries to measure the <mark>misclassification loss</mark> of training example with a correcting function learned from privileged information. The objective function of SVM+ can be formulated as follows: \n<br>",
    "Arabic": "خسارة سوء التصنيف",
    "Chinese": "误分类损失",
    "French": "perte de mauvaise classification",
    "Japanese": "誤分類損失",
    "Russian": "потеря из-за неправильной классификации"
  },
  {
    "English": "misinformation detection",
    "context": "1: Our insights are twofold: (a) article updates are predictable and follow common patterns which humans are able to discern (b) significant modeling progress is needed to address the questions outlined above. See Section 4.6 for more details. Finally , we show that the NewsEdits dataset can bring value to a number of specific , ongoing research directions : event-temporal relation extraction ( Ning et al. , 2018 ; Han et al. , 2019a ) , article link prediction ( Shahaf and Guestrin , 2010 ) , factguided updates ( Shah et al. , 2020 ) , <mark>misinformation detection</mark> ( Appelman and<br>2: As a case study, we focus on the effects of media biases in pretraining data on the fairness of hate speech detection with respect to diverse social attributes, such as gender, race, ethnicity, religion, and sexual orientation, and of <mark>misinformation detection</mark> with respect to partisan leanings.<br>",
    "Arabic": "كشف التضليل",
    "Chinese": "虚假信息检测",
    "French": "détection de la désinformation",
    "Japanese": "偽情報検出",
    "Russian": "обнаружение дезинформации"
  },
  {
    "English": "mix weight",
    "context": "1: For the specific case of mixtures of Gaussians, there is a substantial theoretical literature on algorithms that approximate the <mark>mixing weights</mark>, means and covariances; see [13] for a recent survey of this literature. The strictness of this objective cuts both ways. On the one hand, a successful learner uncovers substantial structure of the target distribution.<br>2: z = M e + b z ,(1) \n where M ∈ R n×k are <mark>mixing weights</mark> and b z ∈ R n is a bias. We further assume two constraints: \n (1) the neural representation is nonnegative with z i ≥ 0 for all i = 1, . . .<br>",
    "Arabic": "وزن الخلط",
    "Chinese": "混合权重",
    "French": "poids de mélange",
    "Japanese": "混合重み",
    "Russian": "веса смешивания"
  },
  {
    "English": "mixed integer programming",
    "context": "1: Dealing with the combinatorial nature of the task, earlier works commonly adopt <mark>mixed integer programming</mark> [43], genetic algorithms [45], or SMT solvers [20]. Another recent popular approach is gradient-based optimization [5,33], which involves iteratively perturbing the input data point according to an objective function that incorporates desired constraints.<br>",
    "Arabic": "برمجة الأعداد الصحيحة المختلطة",
    "Chinese": "混合整数规划 (MIP)",
    "French": "programmation entière mixte",
    "Japanese": "\"混合整数計画法\"",
    "Russian": "смешанное целочисленное программирование"
  },
  {
    "English": "mixed precision",
    "context": "1: We train the models for 10 epochs with <mark>mixed precision</mark> using AdamW (Loshchilov and Hutter, 2019) with a weight decay of 0.05 and the initial learning rate set to 2e−5. We use a linear scheduler with 10% linear warm-up and decay.<br>2: We apply a lower learning rate (×0.5) on the pre-trained weights and layer-wise learning rate decay for better finetuning [53]. Training is performed on 1 node of 8× V100 GPUs with FP16 <mark>mixed precision</mark> [76] via the PyTorch native amp module. All hyperparameters are listed in Table A \n .2.<br>",
    "Arabic": "الدقة المختلطة",
    "Chinese": "混合精度",
    "French": "précision mixte",
    "Japanese": "混合精度",
    "Russian": "смешанная точность"
  },
  {
    "English": "mixed precision training",
    "context": "1: We can also train RAFT using <mark>mixed precision training</mark> Ours(mixed) and achieve similar results while training on only a single GPU. Overall, RAFT requires fewer training iterations and parameters when compared to prior work. D<br>",
    "Arabic": "تدريب بدقة مختلطة",
    "Chinese": "混合精度训练",
    "French": "entraînement en précision mixte",
    "Japanese": "混合精度トレーニング",
    "Russian": "обучение со смешанной точностью"
  },
  {
    "English": "mixed strategy",
    "context": "1: A well-studied online learning setting is that in which the action set is a probability simplex,A = ∆ n , and all costs are linear functions of bounded norm. In this setting, we can interpret online learning algorithms as choosing <mark>mixed strategies</mark> a (t) ∈ ∆ n over a set of meta-actions, {1, . .<br>",
    "Arabic": "استراتيجية مختلطة",
    "Chinese": "混合策略",
    "French": "stratégie mixte",
    "Japanese": "混合戦略",
    "Russian": "смешанная стратегия"
  },
  {
    "English": "mixed-integer program",
    "context": "1: In order to obtain bounds for the quality of the generated placements, the approach in [2] needs to solve a complex (NP-hard) <mark>mixed-integer program</mark>. Our approach is the first algorithm for the water network placement problem, which is guaranteed to provide solutions that achieve at least a constant fraction of the optimal solution within polynomial time.<br>",
    "Arabic": "برنامج الأعداد الصحيحة المختلطة",
    "Chinese": "混合整数规划",
    "French": "programme à nombres entiers mixtes",
    "Japanese": "混合整数計画プログラム",
    "Russian": "смешанно-целочисленная программа"
  },
  {
    "English": "mixing matrix",
    "context": "1: The only assumption we make about the data is that the original signals are independent and that at most one of them is from a Gaussian distribution. The unknown <mark>mixing matrix</mark> is usually generated randomly in experiments. ICA is the problem of recovering the unknown signals from the mixtures x = As.<br>2: M n = r 2 f 2 r 1 f 1 , S n = Em j R j . The second ambiguity is \"scaling\". ICA recovers the <mark>mixing matrix</mark> within a scale factor of the true <mark>mixing matrix</mark>. In other words, we cannot compute the absolute intensity of the pixels for each component.<br>",
    "Arabic": "مصفوفة الخلط",
    "Chinese": "混合矩阵",
    "French": "matrice de mélange",
    "Japanese": "混合行列",
    "Russian": "матрица смешивания"
  },
  {
    "English": "mixing time",
    "context": "1: Dobrushin's condition is well known to imply rapid mixing of sequential Gibbs, and it turns out that we can leverage it again here to bound the <mark>mixing time</mark> of HOGWILD!-Gibbs. Theorem 3. Assume that we run Gibbs sampling on a distribution that satisfies Dobrushin's condition, α < 1.<br>2: The state space S is connected under the transitions of M. \n 2. M has uniform stationary distribution. 3. Starting from GD a sufficiently large number of local swaps have to be performed until the chain mixes. We would like to know how many such swaps should be performed, i.e., the <mark>mixing time</mark> of the chain.<br>",
    "Arabic": "وقت الخلط",
    "Chinese": "混合时间",
    "French": "temps de mélange",
    "Japanese": "混合時間",
    "Russian": "время перемешивания"
  },
  {
    "English": "mixture component",
    "context": "1: To measure whether f ( s t−1 ) = p ( u t |x t−1 ) is constant for a <mark>mixture component</mark> N ( µ ( i ) , Σ ( i ) ) we consider the function g ( µ , Σ ) = f ( s t−1 ) N ( s t−1 |µ , Σ ) ds t−1 which , in the<br>2: where the latent variable γ j ∈ {1, . . . , K} indicates which <mark>mixture component</mark> gave rise to b j .<br>",
    "Arabic": "مكون الخليط",
    "Chinese": "混合成分",
    "French": "composant du mélange",
    "Japanese": "混合成分",
    "Russian": "компонент смеси"
  },
  {
    "English": "mixture distribution",
    "context": "1: Data We consider the synthetic dataset consisting of samples in SO 3 (R d ) 11 from the <mark>mixture distribution</mark> with density p(Q) = 1 K K k=1 N W (Q|Q k , σ 2 k ) with K ∈ N, where for any k ∈ {1, . . .<br>2: We overcome this issue by softly measuring the two errors using the <mark>mixture distribution</mark> R λ = λP + (1 − λ)Q for some λ ∈ (0, 1).<br>",
    "Arabic": "توزيع الخليط",
    "Chinese": "混合分布",
    "French": "distribution de mélange",
    "Japanese": "混合分布",
    "Russian": "смешанное распределение"
  },
  {
    "English": "mixture model",
    "context": "1: Our joining technique could do better than either \n C * 1 or C * \n 2 , by entertaining also a third possibility, which combines the two candidates. We construct a <mark>mixture model</mark> by adding together all counts from \n C * 1 and C * 2 into C + = C * 1 + C * 2 .<br>2: For tracking of the full state, both motion and shape, the hypothesis is X = (α, k). The <mark>mixture model</mark> above leads to an observation likelihood \n p(y | X ) ≡ p(y | α, k) ∝ 1 Z exp − λρ(T αxk , z). (7) \n<br>",
    "Arabic": "نموذج خليط",
    "Chinese": "混合模型",
    "French": "modèle de mélange",
    "Japanese": "混合モデル",
    "Russian": "модель смеси"
  },
  {
    "English": "mixture of Gaussians",
    "context": "1: Estimating distributions from observed data is a fundamental task in statistics that has been studied for over a century. This task frequently arises in applied machine learning and it is common to assume that the distribution can be modeled using a <mark>mixture of Gaussians</mark>.<br>2: In the context of mobile robotics, (Cielniak, Bennewitz, & Burgard 2003) apply a two level model to track and predict the location of people using a mobile robot equipped with a laser range-finder. Their model learns a person's trajectories using a mixtures of Gaussians approach.<br>",
    "Arabic": "مزيج من الجاوسيات",
    "Chinese": "高斯混合模型",
    "French": "mélange de gaussiennes",
    "Japanese": "ガウス混合",
    "Russian": "смесь гауссовых"
  },
  {
    "English": "mixture weight",
    "context": "1: The coordinate ascent update for g involves solving the following optimization problem: \n g * ← argmax g ∈ G F (q, g, σ 2 ). (59 \n ) \n Recall, for the mixture prior with fixed mixture components, fitting g reduces to fitting the <mark>mixture weights</mark>, π.<br>2: As an extension, we can replace the <mark>mixture weights</mark> p(t | r) with p(t | r, x), to allow the model to select prompts that are appropriate for the given x. For example, a plural noun x might prefer prompts t that use a plural verb.<br>",
    "Arabic": "وزن المزيج",
    "Chinese": "混合权重",
    "French": "poids du mélange",
    "Japanese": "混合重み",
    "Russian": "весовые коэффициенты смеси"
  },
  {
    "English": "mixup",
    "context": "1: Subgraph method samples a subgraph from the original graph using random walk The generated graph will keep part of the the semantic meaning of original graphs. • M-Manifold 13 (Wang et al., 2021) Manifold-<mark>Mixup</mark> conducts <mark>Mixup</mark> operation for graph classification in the embedding space, which interpolates graph-level embedding after the READOUT function.<br>2: A batch size of 1024, an initial learning rate of 0.001, a weight decay of 0.05, and gradient clipping with a max norm of 1 are used. We include most of the augmentation and regularization strategies of [ 63 ] in training , including RandAugment [ 17 ] , <mark>Mixup</mark> [ 77 ] , Cutmix [ 75 ] , random erasing [ 82 ] and stochastic depth [ 35 ] , but not repeated augmentation [ 31 ] and Exponential Moving Average ( EMA ) [ 45 ] which do not<br>",
    "Arabic": "الخلط",
    "Chinese": "Mixup",
    "French": "mixup",
    "Japanese": "混合",
    "Russian": "смешивание"
  },
  {
    "English": "mocap",
    "context": "1: The outputs of nodeRNNs are skeleton joints of different body parts, which are concatenated to reconstruct the complete skeleton. In order to model human motion, we train S-RNN to predict the <mark>mocap</mark> frame at time t + 1 given the frame at time t. Similar to [14], we gradually add noise to the <mark>mocap</mark> frames during training.<br>2: We train all RNNs jointly to minimize the Euclidean loss between the predicted <mark>mocap</mark> frame and the ground truth. See supplementary material on the project web page [24] for training details. Evaluation setup. We compare S-RNN with the stateof-the-art ERD architecture [14] on H3.6m <mark>mocap</mark> data set [21].<br>",
    "Arabic": "بيانات الحركة الثلاثية الأبعاد",
    "Chinese": "动作捕捉数据",
    "French": "capture de mouvements",
    "Japanese": "モーションキャプチャ",
    "Russian": "Захват движения"
  },
  {
    "English": "modality",
    "context": "1: If we consider programs and feedback as two <mark>modalities</mark>, one approach is to capture the joint distribution p(x i , y i ). Doing so, we can make predictions by sampling from the conditional having seen the program:ŷ i ∼ p(y i |x i ).<br>2: On the data set side, we only consider images with a heavy focus on synthetic images. We do not explore other <mark>modalities</mark> and we only consider the toy scenario in which we have access to a data generative process with uniformly distributed factors of variations.<br>",
    "Arabic": "صيغة",
    "Chinese": "模态",
    "French": "modalité",
    "Japanese": "モダリティ",
    "Russian": "модальность"
  },
  {
    "English": "mode",
    "context": "1: Finally, replacing V r by the closest <mark>mode</mark> at each iteration ensures that the synthesized colour is always a subset of the photoconsistency minima. Note that this does not undo the good work of the robust kernel in computing the <mark>mode</mark>s of E photo , but allows the texture prior to efficiently select between the robustlycomputed colour candidates at each pixel.<br>2: Unfortunately with MAE, a system that correctly predicts the location of the <mark>mode</mark> of the test set gold distribution and centers predictions around it with an optimally conserva-tive variance can achieve lower MAE and apparent better performance.<br>",
    "Arabic": "وضع",
    "Chinese": "模式",
    "French": "mode",
    "Japanese": "モード",
    "Russian": "мода"
  },
  {
    "English": "mode collapse",
    "context": "1: 3 Although it is possible to backpropagate into the decoding targets (i.e. encoder representations) at higher layers, thereby encouraging the encoder to discover more predictable segment sequences, we found in practice that doing so resulted in a form of <mark>mode collapse</mark> where labels became insensitive to the data and converged to a single value for all timesteps.<br>2: One way to finetune a neural network is to directly continue training it with the additional training data. But this approach can lead to overfitting, <mark>mode collapse</mark>, and catastrophic forgetting. Extensive research has focused on developing finetuning strategies that avoid such issues.<br>",
    "Arabic": "انهيار النمط",
    "Chinese": "模式崩溃",
    "French": "effondrement de mode",
    "Japanese": "モード崩壊",
    "Russian": "коллапс режима"
  },
  {
    "English": "model",
    "context": "1: Existing Classes in AllenNLP <mark>Model</mark>s in Al-lenNLP are of type <mark>Model</mark> (a thin wrapper around a PyTorch Module). The <mark>Model</mark> wrapper includes a forward() function, which runs the model and optionally computes the loss if a label is provided. Obtaining predictions from an AllenNLP <mark>Model</mark> is simplified via the Predictor class.<br>2: <mark>Model</mark> ( Chen et al. , 2017 ) .<br>",
    "Arabic": "نموذج",
    "Chinese": "模型",
    "French": "modèle",
    "Japanese": "モデル",
    "Russian": "модель"
  },
  {
    "English": "model accuracy",
    "context": "1: Additionally, consider the case where X and Y are independent: here, <mark>model accuracy</mark> would be no greater than the majority class frequency, but this frequency varies across datasets. V-information avoids this problem by factoring in the label entropy H V (Y ); if X, Y are independent, then the V-information is provably zero.<br>2: However, our attack reduces <mark>model accuracy</mark> to 30%. This is significantly weaker than the original Madry et al. (2018) model that does not use thermometer encoding. Because this model is trained against the (comparatively weak) LS-PGA attack, it is unable to adapt to the stronger attack we present above.<br>",
    "Arabic": "دقة النموذج",
    "Chinese": "模型准确率",
    "French": "précision du modèle",
    "Japanese": "モデルの精度",
    "Russian": "точность модели"
  },
  {
    "English": "model architecture",
    "context": "1: In this section, we show the evaluation results of all the submitted systems for the three subtasks. Since most systems share similar <mark>model architecture</mark> for subtasks A and B, we discuss the two subtasks together.<br>2: Incorrectly attributing such improvements to, for instance, changes to the <mark>model architecture</mark> amounts to a \"failure to identify the sources of empirical gains\" (Lipton and Steinhardt, 2019).<br>",
    "Arabic": "هندسة النموذج",
    "Chinese": "模型架构",
    "French": "architecture du modèle",
    "Japanese": "モデルアーキテクチャ",
    "Russian": "архитектура модели"
  },
  {
    "English": "model averaging",
    "context": "1: They are trained for 50 epochs and we use the last checkpoint for all evaluations without <mark>model averaging</mark> and ensemble decoding.<br>",
    "Arabic": "ضم النماذج",
    "Chinese": "模型平均",
    "French": "moyennage de modèle",
    "Japanese": "モデル平均化",
    "Russian": "\"усреднение моделей\""
  },
  {
    "English": "model bias",
    "context": "1: It is worth noting that WINOGENDER was originally intended by its authors to merely be a diagnostic tool that checks for bias in a model; the authors note that it may demonstrate the presence of <mark>model bias</mark> but not prove the absence of the same.<br>2: The system outputs indicate that the models also tend to delete information, which is likely a behavior learned from the training data. Model outputs contain more substitution errors than the datasets, so that behavior is probably a <mark>model bias</mark> rather than something picked up from the data.<br>",
    "Arabic": "الانحياز النموذجي",
    "Chinese": "模型偏差",
    "French": "biais du modèle",
    "Japanese": "モデルバイアス",
    "Russian": "предвзятость модели"
  },
  {
    "English": "model capacity",
    "context": "1: Besides, the scaling law of recommendation models [3] suggests substantial quality improvements by increasing <mark>model capacity</mark> in data-rich applications.<br>2: As the inference speed is influenced by both <mark>model capacity</mark> and beam size, we report the latency and throughput measures for multiple settings in Table 5. As NCI is an end-to-end retrieval method and achieves competitive performance without re-ranking, the latency and throughput are already affordable for some near-real-time applications.<br>",
    "Arabic": "سعة النموذج",
    "Chinese": "模型容量",
    "French": "capacité du modèle",
    "Japanese": "モデル容量",
    "Russian": "ёмкость модели"
  },
  {
    "English": "model card",
    "context": "1: For projects not requiring IRB approval, it is good practice to provide a <mark>model card</mark> (Mitchell et al., 2019), data sheet (Gebru et al., 2018) or data statement (Bender and Friedman, 2018) with your model or resource.<br>2: Additional model-specific metadata can be provided via a <mark>model card</mark> (Mitchell et al., 2018) that describes properties of its training, a citation to the work, datasets used during pretraining, and any caveats about known biases in the model and its predictions. An example <mark>model card</mark> is shown in Figure 3  \n (Left).<br>",
    "Arabic": "بطاقة النموذج",
    "Chinese": "模型卡",
    "French": "fiche modèle",
    "Japanese": "モデルカード",
    "Russian": "\"карточка модели\""
  },
  {
    "English": "model checking",
    "context": "1: In contrast, in the <mark>model checking</mark> problem, the computational complexity remains in PTIME for any fragment in the finite variable space (given they are derived from first-order logic). Furthermore, inference in the model-checking problem is fairly straightforward, which has also been evident by low computational complexity.<br>2: Many state spaces of interest, including those that arise in classical planning and in the verification of safety properties in <mark>model checking</mark>, can be compactly specified as a family of labeled transition systems (e. g., Helmert, Haslum, and Hoffmann 2008;Dräger, Finkbeiner, and Podelski 2009).<br>",
    "Arabic": "التحقق من النموذج",
    "Chinese": "模型检查",
    "French": "vérification de modèles",
    "Japanese": "モデル検査",
    "Russian": "проверка модели"
  },
  {
    "English": "model class",
    "context": "1: All three of these settings consider a set D of n data distributions and a <mark>model class</mark> H, evaluating the performance of a model h by its worst-case expected loss, max D∈D R D (h).<br>2: • Moreover, the PAC-Bayes bounds provide insight into the marginal likelihood overfitting behaviour: in order to perform model selection based on the PAC-Bayes bounds, we need to pay a penalty based on the logarithm of the size of the <mark>model class</mark>.<br>",
    "Arabic": "فئة النموذج",
    "Chinese": "模型类",
    "French": "classe de modèles",
    "Japanese": "モデルクラス",
    "Russian": "класс моделей"
  },
  {
    "English": "model comparison",
    "context": "1: Mathematicians have argued (Blanchard and Geman, 2003) that discriminative methods must be followed by more sophisticated processes to (i) remove false alarms, (ii) amend missing objects by global context information, and (iii) reconcile conflicting (overlapping) explanations through <mark>model comparison</mark>.<br>",
    "Arabic": "مقارنة النماذج",
    "Chinese": "模型比较",
    "French": "comparaison de modèles",
    "Japanese": "モデル比較",
    "Russian": "сравнение моделей"
  },
  {
    "English": "model complexity",
    "context": "1: Unfortunately, there is no reliable metric to quantify the model's training stability. To precisely measure the benefit from better training stability, we vary <mark>model complexity</mark> as well as learning rates, then check model's offline quality, measured by AUC for binary classification tasks and RMSE for regression tasks.<br>2: (1) Increasing <mark>model complexity</mark>: As more modeling techniques are applied and more components are added to the recommendation model (to improve its quality), there's a greater chance that the model will suffer from loss divergence problems.<br>",
    "Arabic": "تعقيد النموذج",
    "Chinese": "模型复杂度",
    "French": "complexité du modèle",
    "Japanese": "モデルの複雑性",
    "Russian": "сложность модели"
  },
  {
    "English": "model compression",
    "context": "1: In particular, achieving state-of-the-art generalization bounds typically involves severe <mark>model compression</mark> with pruning, quantization, and restricting the parameters to a subspace of the parameter space (Zhou et al., 2018;Lotfi et al., 2022). These interventions reduce the complexity penalty in Eq.<br>",
    "Arabic": "ضغط النموذج",
    "Chinese": "模型压缩",
    "French": "compression de modèle",
    "Japanese": "モデル圧縮",
    "Russian": "сжатие модели"
  },
  {
    "English": "model convergence",
    "context": "1: Before that, we first rigorously prove the projection maintains an unbiased estimate of true gradient in Section 3.3. Since the document space is constructed only by the examined documents, the rank of document space is expected to be smaller than the entire parameter space. This directly leads to lower variance and faster <mark>model convergence</mark>.<br>2: As illustrated in Figure 1, although u t will eventually lead to the same model estimation, as it is unbiased, this guarantee is only obtained in expectation. The variance could potentially be large: for example, the blue and purple updating traces slow down <mark>model convergence</mark>, when the number of observations is finite.<br>",
    "Arabic": "تقارب النموذج",
    "Chinese": "模型收敛",
    "French": "convergence du modèle",
    "Japanese": "モデル収束",
    "Russian": "сходимость модели"
  },
  {
    "English": "model development",
    "context": "1: We motivate the importance of training stability research, especially for recommender systems in industry, from several aspects. First, the problem of loss divergence, once it occurs regularly, can affect almost all types of <mark>model development</mark>. This includes, but is not limited to: \n<br>2: We found this to be helpful during <mark>model development</mark>, and we hypothesize that this is because doing so removes the need for this information to be encoded by the model. • We implement the case-wise reasoning of the segmentation decisions using multiplicative masking rather than logical selection. This is intended to boost signal into the boundary decisions.<br>",
    "Arabic": "تطوير النموذج",
    "Chinese": "模型开发",
    "French": "développement de modèle",
    "Japanese": "モデル開発",
    "Russian": "разработка модели"
  },
  {
    "English": "model distillation",
    "context": "1: Additionally, techniques like <mark>model distillation</mark> [LHCG19a] can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts.<br>",
    "Arabic": "تقطير النموذج",
    "Chinese": "模型蒸馏",
    "French": "distillation de modèle",
    "Japanese": "モデル蒸留",
    "Russian": "дистилляция модели"
  },
  {
    "English": "model distribution",
    "context": "1: It is, however, unsuitable for open-ended generation where there typically are several plausible continuations for each context and creative generations are desirable. Statistics-based measures compare the <mark>model distribution</mark> Q with respect to the human distribution P on the basis of some statistic T (P ) and T (Q).<br>2: In MBR, the sequence with the highest expected utility with respect to thez <mark>model distribution</mark> is chosen as the output, where the utility is usually some measure of text similarity. This contrasts with the more commonly used maximum a posteriori (MAP) decision rule, which returns the sequence with the highest probability under the model.<br>",
    "Arabic": "توزيع النموذج",
    "Chinese": "模型分布",
    "French": "distribution du modèle",
    "Japanese": "モデル分布",
    "Russian": "модельное распределение"
  },
  {
    "English": "model estimation",
    "context": "1: Figure 1: A high-level visualization of the mixed LDSs formulation, and the algorithmic idea of combining clustering, classification, and <mark>model estimation</mark>. Here, we consider the special case where the multiple short trajectories come from the segments of a single continuous trajectory.<br>2: • Our methods for <mark>model estimation</mark> and classification, namely Algorithms 4 and 5, are already adaptive to different T m 's in M clustering and M classification , and hence need no modification. Unknown parameters. Next, we show how to handle the case when certain parameters are unknown to the algorithms: \n<br>",
    "Arabic": "تقدير النموذج",
    "Chinese": "模型估计",
    "French": "estimation de modèle",
    "Japanese": "モデル推定",
    "Russian": "оценка модели"
  },
  {
    "English": "model evaluation",
    "context": "1: The main technical contributions include a novel state representation of card and betting information, a multi-task self-play training loss function, and a new <mark>model evaluation</mark> and selection metric to generate the final model. In a study involving 100,000 hands of poker, AlphaHoldem defeats Slumbot and DeepStack using only one PC with three days training.<br>2: We ensure the annotators are paid adequately for at least $15 per hour and we inform annotators that their annotations are used for <mark>model evaluation</mark> purpose.<br>",
    "Arabic": "تقييم النموذج",
    "Chinese": "模型评估",
    "French": "évaluation du modèle",
    "Japanese": "モデル評価",
    "Russian": "оценка модели"
  },
  {
    "English": "model family",
    "context": "1: However, there are some dataset-specific effects across different <mark>model families</mark> where individual attributes have a stronger impact, e.g. race for the GHC corpus (detailed results in Appendix A.5.2).<br>",
    "Arabic": "عائلة النماذج",
    "Chinese": "模型家族",
    "French": "famille de modèles",
    "Japanese": "モデルファミリー",
    "Russian": "семейство моделей"
  },
  {
    "English": "model fine-tuning",
    "context": "1: Model fine-tuning We take the German to English pre-trained model 5 from Opus (Tiedemann and Thottingal, 2020), whose vocabulary size is 65k. By applying the pre-trained tokenizer to both corpora, we get new vocabulary with size of 2,155 and 7,435 for PHOENIX and DGS corpus, respectively.<br>2: We follow two approaches which differ in whether the child language pair (SL) is included during the parent model pre-training: \n Model fine-tuning refers to fine-tuning a pretrained model to train a child model. Although the pre-trained model usually contains a large vocabulary, it does not guarantee a full coverage of the child language pair.<br>",
    "Arabic": "ضبط دقيق للنموذج",
    "Chinese": "模型微调",
    "French": "ajustement fin du modèle",
    "Japanese": "モデルファインチューニング",
    "Russian": "Настройка модели"
  },
  {
    "English": "model generalization",
    "context": "1: I feel so [excited] '' prompt answer input input prompt tasker (answer) insert the prompt to the input graph inserting pattern: prompt token: token structure: the capability of <mark>model generalization</mark>. Currently, we only find very few works [27] studying the graph prompt issue.<br>",
    "Arabic": "قدرة النموذج على التعميم",
    "Chinese": "模型泛化能力",
    "French": "généralisation du modèle",
    "Japanese": "モデルの一般化能力",
    "Russian": "обобщение модели"
  },
  {
    "English": "model hyperparameter",
    "context": "1: This results in a very wide tree that captures all the hierarchical nature of the <mark>model hyperparameters</mark>, and allows the creation of a single hyperparameter optimization problem with four hierarchical layers of a total of 786 parameters.<br>2: In this section we describe the <mark>model hyperparameters</mark> used and present our results on the depression detection and self-harm risk assessment tasks. To facilitate reproducibility we provide our code and will provide the Reddit depression dataset to researchers who sign a data usage agreement 4 .<br>",
    "Arabic": "المعلمة الفائقة النموذجية",
    "Chinese": "模型超参数",
    "French": "hyperparamètre du modèle",
    "Japanese": "モデルのハイパーパラメータ",
    "Russian": "гиперпараметры модели"
  },
  {
    "English": "model inference",
    "context": "1: The method allows for the creation, removal, and location variation of component models over time while maintaining the property that the random measures are marginally DP distributed. Additionally, we derive a Gibbs sampling algorithm for <mark>model inference</mark> and test it on both synthetic and real data.<br>2: We share with other simultaneous generation applications the assumption that the <mark>model inference</mark> time is negligible, compared to slower spoken input and program execution (which may involve system and database interactions).<br>",
    "Arabic": "استنتاج النموذج",
    "Chinese": "模型推断",
    "French": "inférence de modèle",
    "Japanese": "モデル推論",
    "Russian": "вывод модели"
  },
  {
    "English": "model initialization",
    "context": "1: Model initialization involves assigning initial weights (coefficients of regressors) by scanning the training set D once. To exploit the sparseness of the problem, one shall use some data-driven approach instead of simply uniformly or randomly assigning weights to all parameters, as many gradientbased algorithms do.<br>2: This gave about 150K features comprised of 40K ads (×2), 40K pages, and 10K queries (×3). For robot filtering, we removed examples with the number of distinct events above 5,000. After <mark>model initialization</mark> using the second method as described in Section 4.4, we performed 17 iterations of multiplicative updates to converge weights.<br>",
    "Arabic": "تهيئة النموذج",
    "Chinese": "模型初始化",
    "French": "initialisation du modèle",
    "Japanese": "モデル初期化",
    "Russian": "инициализация модели"
  },
  {
    "English": "model interpretability",
    "context": "1: There has been much work in the way of <mark>model interpretability</mark>, but relatively little in the way of dataset interpretability. Our framework will allow datasets to be probed, helping us understand what exactly we are testing for in models and how pervasive annotation artefacts really are.<br>",
    "Arabic": "قابلية تفسير النموذج",
    "Chinese": "模型可解释性",
    "French": "interprétabilité du modèle",
    "Japanese": "モデルの解釈可能性",
    "Russian": "интерпретируемость модели"
  },
  {
    "English": "model interpretation",
    "context": "1: Neural NLP models are increasingly accurate but are imperfect and opaque-they break in counterintuitive ways and leave end users puzzled at their behavior. Model interpretation methods ameliorate this opacity by providing explanations for specific model predictions.<br>",
    "Arabic": "تفسير النموذج",
    "Chinese": "模型解释",
    "French": "interprétation du modèle",
    "Japanese": "モデル解釈",
    "Russian": "интерпретация модели"
  },
  {
    "English": "model layer",
    "context": "1: The over-smoothing issue extensively exists in graph neural networks (Chen et al. 2020;Elinas and Bonilla 2022). As the number of <mark>model layers</mark> increases, node representations become nearly indistinguishable, which leads to a significant decrease on model performance.<br>2: Transformer [44] For MFCC, we swept the number of <mark>model layers</mark> {2, 4}, dropout {0, 0.1} and learning rates {0.001, 0.0005}. We used 8 attention heads, model dimension 128, prenorm, positional encodings, and trained for 150 epochs with a batch size of 100.<br>",
    "Arabic": "طبقة النموذج",
    "Chinese": "模型层",
    "French": "couche de modèle",
    "Japanese": "モデル層",
    "Russian": "слой модели"
  },
  {
    "English": "model m",
    "context": "1: D correct = {∀(p i , h i ) ∈ D : M(p i , h i ) =ŷ = y} \n , whereŷ is the prediction and y is the original label. This is completed to ensure that the evaluation of semantic sensitivity is not hindered or inflated by the predictive performance and confidence of the <mark>model M</mark>. This type of filtering is used when probing for emergent syntactic ( Sinha et al. , 2021 ) , lexical ( Jeretic et al. , 2020b ) , and numerical ( Wallace et al. , 2019 ) reasoning<br>",
    "Arabic": "نموذج M",
    "Chinese": "模型 M",
    "French": "modèle M",
    "Japanese": "モデルM",
    "Russian": "модель M"
  },
  {
    "English": "model output",
    "context": "1: 2 During training, the <mark>model output</mark> U * (s) and W * (s) often represents a soft permutation that does not permute z ′ into y. Our goal is to push the <mark>model output</mark> into the space of (soft) permutations that lead to y.<br>2: The phenomenon we observe would be of less concern if the correct label prediction was just an outcome of chance, which could occur when the entropy of the log probabilities of the <mark>model output</mark> is high (suggesting uniform probabilities on entailment, neutral and contradiction labels, recall Model B from §3).<br>",
    "Arabic": "ناتج النموذج",
    "Chinese": "模型输出",
    "French": "sortie du modèle",
    "Japanese": "モデル出力",
    "Russian": "выход модели"
  },
  {
    "English": "model parallelism",
    "context": "1: To train the larger models without running out of memory, we use a mixture of <mark>model parallelism</mark> within each matrix multiply and <mark>model parallelism</mark> across the layers of the network. All models were trained on V100 GPU's on part of a high-bandwidth cluster provided by Microsoft. Details of the training process and hyperparameter settings are described in Appendix B.<br>",
    "Arabic": "توازي النموذج",
    "Chinese": "模型并行性",
    "French": "parallélisme de modèle",
    "Japanese": "モデル並列化",
    "Russian": "параллелизм моделей"
  },
  {
    "English": "model parameter",
    "context": "1: (2021) suggested learning a binary mask for every <mark>model parameter</mark> and every language pair, both requiring further training after the base multilingual model converges. Li and Gong (2021) used per language gradients geometry to rescale gradients of different language pair to improve performance on low resource languages.<br>2: The gradients of S-BPR with respect to a <mark>model parameter</mark> θ and a given (u, t, i, j) are: \n ∂ ∂θ ` l n σ ( xu , t , i −xu , t , j ) − λ θ θ 2= ( 1 − σ ( xu , t , i −xu , t , j ) ) ∂ ∂θ ( xu , t , i −xu , t , j ) − 2 λ θ θ 1 : procedure LearnSBPR-FPMC (<br>",
    "Arabic": "معامل النموذج",
    "Chinese": "模型参数",
    "French": "paramètre du modèle",
    "Japanese": "モデルパラメータ",
    "Russian": "параметр модели"
  },
  {
    "English": "model performance",
    "context": "1: In addition, we investigate the effect of instruction bias on <mark>model performance</mark>, showing that instruction patterns can lead to overestimated performance as well as limit the ability of models to generalize to other task examples.<br>2: We investigate the effect of instruction bias on <mark>model performance</mark>, showing that performance is overestimated by instruction bias and that models often fail to generalize beyond instruction patterns.<br>",
    "Arabic": "أداء النموذج",
    "Chinese": "模型表现",
    "French": "performance du modèle",
    "Japanese": "モデルの性能",
    "Russian": "производительность модели"
  },
  {
    "English": "model precision",
    "context": "1: On average, the RMN's descriptors are much more interpretable than those of the baselines, as it achieves a mean <mark>model precision</mark> of 0.73 (Figure 3) across all values of K. There is little difference between the <mark>model precision</mark> of the three topic model baselines, which hover around 0.5.<br>2: The relationship between <mark>model precision</mark>, MP m k , and the model's estimate of the likelihood of the intruding word in Figure 5 (top row) is surprising. The highest probability did not have the best interpretability; in fact, the trend was the opposite.<br>",
    "Arabic": "دقة النموذج",
    "Chinese": "模型精度",
    "French": "précision du modèle",
    "Japanese": "モデル精度",
    "Russian": "точность модели"
  },
  {
    "English": "model prediction",
    "context": "1: In the evaluation on standard benchmark, let D test denote all testing samples from the benchmark, f (x, E(x); p benign ) denote the <mark>model prediction</mark> given the sample x, demonstrations E(x), and the benign prompt p benign .<br>2: This indicates that the generated variations do not cause negligible change within <mark>model prediction</mark>, but rather can be considered adversarial for the model. It shows that the limited capabilities to utilise syntactic information cause the model to significantly change the final prediction given minuscule variations, which is an to inconsistent predictive behaviour.<br>",
    "Arabic": "التنبؤ بالنموذج",
    "Chinese": "模型预测",
    "French": "prédiction du modèle",
    "Japanese": "モデル予測",
    "Russian": "предсказание модели"
  },
  {
    "English": "model predictive control",
    "context": "1: Conducting research in di cult scenarios using full-size vehicles is both expensive and risky. In this section we highlight how the F1/10 platform can enable research on algorithms for obstacle avoidance, end-to-end driving, <mark>model predictive control</mark>, and vehicle-to-vehicle communication.<br>",
    "Arabic": "نموذج التحكم التنبؤي",
    "Chinese": "模型预测控制",
    "French": "modèle de contrôle prédictif",
    "Japanese": "モデル予測制御",
    "Russian": "модельное предиктивное управление"
  },
  {
    "English": "model representation",
    "context": "1: reconstruction from a single image has reflected varying preferences on <mark>model representations</mark>. Generalized cylinders [27] resulted in very compact descriptions for certain classes of shapes, and can be used for category level descriptions, but the fitting problem for general shapes in challenging.<br>2: We seek to assess the contribution of both memory and prediction pressures to the content of <mark>model representations</mark>.<br>",
    "Arabic": "تمثيل النموذج",
    "Chinese": "模型表示",
    "French": "représentation du modèle",
    "Japanese": "モデル表現",
    "Russian": "модельное представление"
  },
  {
    "English": "model robustness",
    "context": "1: • Model robustness: Our work uncovers the susceptibility of these models to a series of data and model manipulation strategies, such as misleading instructions, adversarial demonstrations, and out-of-distribution demonstrations and test data, which would encourage more research in enhancing <mark>model robustness</mark> and lead to the development of reliable and secure AI systems.<br>2: It has been applied to various downstream applications, such as question answering (Yu et al., 2018), chatbot engines (Yan et al., 2016), creative generation (Tian et al., 2021), and improving <mark>model robustness</mark> (Huang and Chang, 2021).<br>",
    "Arabic": "متانة النموذج",
    "Chinese": "模型鲁棒性",
    "French": "robustesse du modèle",
    "Japanese": "モデルの堅牢性",
    "Russian": "робастность модели"
  },
  {
    "English": "model score",
    "context": "1: At training time, we determine an ordering on the templates such that we can approximate <mark>model scores</mark> at test time by incrementally calculating the dot product in template ordering.<br>",
    "Arabic": "نتيجة النموذج",
    "Chinese": "模型分数",
    "French": "score du modèle",
    "Japanese": "モデルスコア",
    "Russian": "балл модели"
  },
  {
    "English": "model selection",
    "context": "1: Recently, a number of empirical Bayesian approaches have been proposed that attempt a form of <mark>model selection</mark> by using the data to guide the search for an appropriate prior.<br>2: the prior , in the posterior contraction ; \n (3) in the product decomposition of the marginal likelihood in Section 3, the first terms will have low probability density, even if the posterior updates quickly to become a good description of the data. Model Selection.<br>",
    "Arabic": "اختيار النموذج",
    "Chinese": "模型选择",
    "French": "sélection de modèle",
    "Japanese": "モデル選択",
    "Russian": "выбор модели"
  },
  {
    "English": "model size",
    "context": "1: Through systematic experimentation, we find that interference (or synergy) are primarily determined by <mark>model size</mark>, data size, and the proportion of each language pair within the total dataset.<br>2: A further question that our results so far do not answer is to what extent <mark>model size</mark> matters and whether models at the scale of Flan-T5 can also exhibit non-trivial entity tracking behavior.<br>",
    "Arabic": "حجم النموذج",
    "Chinese": "模型大小",
    "French": "taille du modèle",
    "Japanese": "モデルサイズ",
    "Russian": "размер модели"
  },
  {
    "English": "model specification",
    "context": "1: This is a <mark>model specification</mark> problem which limits the distributions a model can represent (Andor et al., 2016).<br>2: The incompleteness of any given model is unavoidable due to practical limitations in <mark>model specification</mark> (the ramification and qualification problems) and due to the limited information that may be available during the design phase [Dietterich, 2017;Saisubramanian et al., 2019].<br>",
    "Arabic": "مواصفات النموذج",
    "Chinese": "模型规范化",
    "French": "spécification du modèle",
    "Japanese": "モデル仕様",
    "Russian": "спецификация модели"
  },
  {
    "English": "model structure",
    "context": "1: In section 5.3, we discuss an approach to this task which does not require customization of <mark>model structure</mark>, but rather centers on feature engineering.<br>2: As our main objective is to probe a training strategy orthogonal to the <mark>model structure</mark>, we only include the above three baselines to control the <mark>model structure</mark>, data pre-requisites, and parameter sizes.<br>",
    "Arabic": "بنية النموذج",
    "Chinese": "模型结构",
    "French": "structure du modèle",
    "Japanese": "モデル構造",
    "Russian": "структура модели"
  },
  {
    "English": "model training",
    "context": "1: But in practice, we found that these approaches were ad hoc and couldn't completely prevent training instability in our model. Developing an effective method that can significantly improve <mark>model training</mark> stability accelerates model improvements by addressing concerns about training problems.<br>2: Unfortunately, neither their word embeddings nor <mark>model training</mark> code is publicly available, so we train the SG model by using exactly the same settings as described in their system (vector length 300) and on the same corpus: monolingual English Wikipedia text.<br>",
    "Arabic": "تدريب النموذج",
    "Chinese": "模型训练",
    "French": "Entraînement du modèle",
    "Japanese": "モデル学習",
    "Russian": "обучение модели"
  },
  {
    "English": "model update",
    "context": "1: Furthermore, PRC tends to choose samples that the \"state-of-the-art\" model also picks in rare-case scenarios, indicating that it could be a computationally inexpensive alternative. Table 4 shows the results averaged over two rounds of active annotation and learning for five strategies with two types of <mark>model updates</mark>.<br>",
    "Arabic": "تحديث النموذج",
    "Chinese": "模型更新",
    "French": "mise à jour du modèle",
    "Japanese": "モデル更新",
    "Russian": "обновление модели"
  },
  {
    "English": "model variant",
    "context": "1: However, this pattern is not visible when using <mark>model variant</mark> 13. Overall, the differences between the results of the baseline scenario and the two other scenarios are very small.<br>2: In the next 3 columns, the 'base before' column shows the number of delinquent pupils in the actual class composition at the start, and the columns 'base after v12' / 'v13' the predicted number of delinquent pupils after a year using <mark>model variant</mark> 12 or 13 respectively.<br>",
    "Arabic": "متغير النموذج",
    "Chinese": "模型变体",
    "French": "variante du modèle",
    "Japanese": "モデルバリエント",
    "Russian": "вариант модели"
  },
  {
    "English": "model weight",
    "context": "1: A threat model specifies the conditions under which a defense argues security: a precise threat model allows for an exact understanding of the setting under which the defense is meant to work. Prior work has used words including whitebox, grey-box, black-box, and no-box to describe slightly different threat models, often overloading the same word. Instead of attempting to , yet again , redefine the vocabulary , we enumerate the various aspects of a defense that might be revealed to the adversary or held secret to the defender : model architecture and <mark>model weights</mark> ; training algorithm and training data ; test time randomness ( either the values chosen or the distribution ) ; and , if the model<br>2: This includes <mark>model weights</mark> (OpenAI, 2023), training data (Piktus et al., 2023), or infrastructural details to assess model carbon footprint (Lacoste et al., 2019). In particular, the lack of information on training data raises important questions about the credibility of LLMs performance evaluation.<br>",
    "Arabic": "أوزان النموذج",
    "Chinese": "模型权重",
    "French": "poids du modèle",
    "Japanese": "モデルの重み",
    "Russian": "модельные веса"
  },
  {
    "English": "model's parameter",
    "context": "1: Here L P P O (θ) is the original PPO loss function, θ is the current <mark>model's parameters</mark>, c task is a hyperparameter coefficient that weights the task embedding loss L task (see Fig.<br>2: Models evaluation is however an issue of crucial importance for practical applications, i.g., when trying to optimally set the <mark>model's parameters</mark> for a given task, and for theoretical reasons, i.g., when using such models to approximate semantic knowledge.<br>",
    "Arabic": "\"معلمات النموذج\"",
    "Chinese": "模型参数",
    "French": "paramètres du modèle",
    "Japanese": "モデルのパラメータ",
    "Russian": "параметры модели"
  },
  {
    "English": "model-base approach",
    "context": "1: As shown in Figure 6a, our system localizes these key-points significantly better than DPM-pose on this dataset. However, DPM-pose is a much faster bottom-up method, and we explored ways to combine its strengths with our <mark>model-based approach</mark>, by using it as the basis for learning data-driven proposals.<br>",
    "Arabic": "نهج قائم على النموذج",
    "Chinese": "基于模型的方法",
    "French": "approche basée sur un modèle",
    "Japanese": "モデルベースのアプローチ",
    "Russian": "подход на основе модели"
  },
  {
    "English": "model-base reinforcement learning",
    "context": "1: In some sense, expected traces also construct a model of the environment-but one that differs in several key regards from standard state-to-state models used in <mark>model-based reinforcement learning</mark>. First, expected traces estimate past quantities rather than future quantities.<br>2: Model-based reinforcement learning provides an alternative approach to efficient credit assignment. The general idea is to construct a model that estimates state-transition dynamics, and to update the value function based upon hypothetical transitions drawn from the model (Sutton 1990), for example by prioritised sweeping (Moore and Atkeson 1993;van Seijen and Sutton 2013).<br>",
    "Arabic": "تعلم التعزيز القائم على النموذج",
    "Chinese": "基于模型的强化学习",
    "French": "apprentissage par renforcement basé sur un modèle",
    "Japanese": "モデルベースの強化学習",
    "Russian": "\"обучение с подкреплением на основе моделей\""
  },
  {
    "English": "model-free approach",
    "context": "1: When the model is unknown, finding π h together with T π h T h−1 v and T h−1 v is possible with <mark>model-free approaches</mark> such as Q-learning (Jin et al. 2018). Alternatively, π h (s) can be retrieved using a tree-search of depth h, starting at root s (see Figure 1).<br>",
    "Arabic": "نهج خالٍ من النموذج",
    "Chinese": "无模型方法",
    "French": "approche sans modèle",
    "Japanese": "モデルフリーアプローチ",
    "Russian": "безмодельный подход"
  },
  {
    "English": "modular",
    "context": "1: In addition to the overall <mark>modular</mark> parameter-tying structure induced by our sketches, the key components of our training procedure are the decoupled critic and the curriculum. Our next experiments investigate the extent to which these are necessary for good performance.<br>2: We see an interesting curvature-dependent influence on the hardness. We also see from our approximation guarantees also that the curvature of f plays a more influential role than the curvature of g on the approximation quality. In particular, as soon as f becomes <mark>modular</mark>, the problem becomes easy, even when g is sub<mark>modular</mark>.<br>",
    "Arabic": "متعدد الوحدات",
    "Chinese": "模块化的",
    "French": "modulaire",
    "Japanese": "モジュラー",
    "Russian": "модульный"
  },
  {
    "English": "modular architecture",
    "context": "1: The initial system (Gunson et al., 2022) was developed before the recent LLM advance, relying on a 'traditional' <mark>modular architecture</mark> based upon Alana V2 (Papaioannou et al., 2017;Curry et al., 2018). As patients were usually accompanied by a companion, the lack of multi-party capabilities proved problematic.<br>",
    "Arabic": "الهندسة المعمارية المودولارية",
    "Chinese": "模块化架构",
    "French": "architecture modulaire",
    "Japanese": "モジュラーアーキテクチャ",
    "Russian": "модульная архитектура"
  },
  {
    "English": "module",
    "context": "1: Given unlabeled high-dimensional sequential or spatial data, we encode it iteratively, <mark>module</mark> by <mark>module</mark>. By using a loss that enforces the individual <mark>module</mark>s to maximally preserve the information of their inputs, we enable the stacked model to collectively create compact representations that can be used for downstream tasks. Our contributions are as follows: 1 \n<br>2: The learning rates for both were searched over the range [1e − 2, 1e − 4]. Similar to prior works, the <mark>module</mark> for encoding state features was shared to reduce the number of parameters, and the learning rate for it was additionally searched over [1e − 2, 1e − 4].<br>",
    "Arabic": "وحدة",
    "Chinese": "模块",
    "French": "module",
    "Japanese": "モジュール",
    "Russian": "модуль"
  },
  {
    "English": "moment matching",
    "context": "1: We have found these approximations to work well in practice and to significantly reduce computational costs. First, for each pair of connected streets, the modes that transition from u t−1 to u t are all likely similar. As such, all of the transitioned modes are replaced with a single component using <mark>moment matching</mark>.<br>2: Both EP and Analytic-DPM use <mark>moment matching</mark> as a key step to find analytic solutions of D KL (p target ||p opt ) terms. However, to our knowledge, the relation between <mark>moment matching</mark> and DPMs has not been revealed in prior literature.<br>",
    "Arabic": "مطابقة اللحظة",
    "Chinese": "矩匹配",
    "French": "correspondance des moments",
    "Japanese": "モーメントマッチング",
    "Russian": "совпадение моментов"
  },
  {
    "English": "momentum",
    "context": "1: When running a linear probe on ImageNet, we follow recent literature and use SGD with <mark>momentum</mark> 0.9 and a high learning rate (we try the values 30, 10, 3, ... in the manner described above) (He et al., 2019). We train for 1000000 iterations with a cosine learning rate schedule.<br>2: We train the network on the standard ImageNet 1000 class classification dataset for 160 epochs using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and <mark>momentum</mark> of 0.9 using the Darknet neural network framework [13].<br>",
    "Arabic": "دَفعَة",
    "Chinese": "动量",
    "French": "élan",
    "Japanese": "モメンタム",
    "Russian": "инерция"
  },
  {
    "English": "momentum coefficient",
    "context": "1: While the moving-average behavior may improve accuracy with an appropriate <mark>momentum coefficient</mark>, our experiments show that it is not directly related to preventing collapsing.<br>2: However, the above method has three limitations: (1) manually differentiating optimizer update rules is tedious and error-prone, and must be re-done for each optimizer variant; (2) the method only tunes the step size hyperparameter, not other hyperparameters such as the <mark>momentum coefficient</mark>; and \n<br>",
    "Arabic": "معامل الزخم",
    "Chinese": "动量系数",
    "French": "coefficient de momentum",
    "Japanese": "運動量係数",
    "Russian": "коэффициент импульса"
  },
  {
    "English": "momentum encoder",
    "context": "1: 6 All entries are based on a standard ResNet-50, with two 224×224 views used during pre-training. Table 4 shows the results and the main properties of the methods. SimSiam is trained with a batch size of 256, using neither negative samples nor a <mark>momentum encoder</mark>. Despite it simplicity, SimSiam achieves competitive results.<br>2: Inspired by  which uses a <mark>momentum encoder</mark> for more consistent representation, we build a memory bank to store momentum token representations from the quantized network. When computing the contrastive distillation loss, we load the representations of negative samples from the memory bank with cheap indexing operations.<br>",
    "Arabic": "مُشَفّر الزخم",
    "Chinese": "动量编码器",
    "French": "encodeur de momentum",
    "Japanese": "モーメンタムエンコーダ",
    "Russian": "моментум-кодировщик"
  },
  {
    "English": "momentum term",
    "context": "1: For DeTAG, we further tune the accelerated gossip parameter η within {0, 0.1, 0.2, 0.4} and phase length R within {1, 2, 3}. We fix the <mark>momentum term</mark> to be 0.9 and weight decay to be 1e-4.<br>2: For DeTAG, we further tune the accelerated gossip parameter η within {0, 0.1, 0.2, 0.4} and phase length R within {1, 2, 3}. We fix the <mark>momentum term</mark> to be 0.9 and weight decay to be 5e-4. The hyperparameters adopted for each runs are shown in Table 3 and Table 4.<br>",
    "Arabic": "حد الزخم",
    "Chinese": "动量项",
    "French": "terme de momentum",
    "Japanese": "モーメンタム項 (mo-mentamukou)",
    "Russian": "терм импульса"
  },
  {
    "English": "monocular",
    "context": "1: In particular for the latter applications, being able to performance capture humans from <mark>monocular</mark> video would be a game changer. The majority of established <mark>monocular</mark> methods only captures articulated motion (including hands or sparse facial expression at most).<br>",
    "Arabic": "أحادي",
    "Chinese": "单目",
    "French": "monoculaire",
    "Japanese": "単眼",
    "Russian": "монокулярный"
  },
  {
    "English": "monocular reconstruction",
    "context": "1: First, <mark>monocular reconstruction</mark> methods can perform dense 3D reconstruction from a single image without 2D keypoints [74,62,20]. However, they require multiple views [20] or videos of rigid scenes for training [74].<br>",
    "Arabic": "\"إعادة بناء أحادي العينية\"",
    "Chinese": "单目重建",
    "French": "reconstruction monoculaire",
    "Japanese": "単眼再構成",
    "Russian": "монокулярная реконструкция"
  },
  {
    "English": "monolingual baseline",
    "context": "1: These results confirm empirically our initial hunch that semantic role labeling relations are deeply rooted beyond languages, independently of their surface realization and their predicate-argument structure inventories. Finally , for completeness , Appendix E includes the results of our system on the individual subtasks , namely , predicate identification and predicate sense The improvements of our cross-lingual approach compared to the more traditional <mark>monolingual baseline</mark> are evident , especially in lower-resource scenarios , with absolute improvements in F 1 score of 25.5 % , 9.7 % and 26.9 % on the<br>",
    "Arabic": "خط الأساس أحادي اللغة",
    "Chinese": "单语基线",
    "French": "référence monolingue",
    "Japanese": "単言語ベースライン",
    "Russian": "моноязычный базовый уровень"
  },
  {
    "English": "monolingual corpora",
    "context": "1: For Marathi and Nepali, we use large-scale <mark>monolingual corpora</mark> made available by IndicCorp (Kakwani et al., 2020) and   Baselines We compare our approaches against semi-supervised VecMap approach with CSLS (Artetxe et al., 2018b,a), using identical words as seeds, with 300-dimensional fastText embed-dings (Bojanowski et al., 2017).<br>2: Additionally, we build the cross-lingual word embeddings model for Hindi-Marathi using VecMap (Artetxe et al., 2017). The model uses <mark>monolingual corpora</mark> released by Kunchukuttan et al. (2020) and a bilingual dictionary 6 required for the supervised method by Artetxe et al. (2017).<br>",
    "Arabic": "مجموعات لغوية أحادية",
    "Chinese": "单语语料库",
    "French": "corpus monolingue",
    "Japanese": "単言語コーパス",
    "Russian": "монолингвальные корпуса"
  },
  {
    "English": "monolingual corpus",
    "context": "1: Furthermore, we do not connect the English vertices to each other, but only to foreign language vertices. 4 The graph vertices are extracted from the different sides of a parallel corpus (D e , D f ) and an additional unlabeled monolingual foreign corpus Γ f , which will be used later for training.<br>",
    "Arabic": "نص أحادي اللغة",
    "Chinese": "单语语料库",
    "French": "corpus monolingue",
    "Japanese": "単言語コーパス",
    "Russian": "корпус на одном языке"
  },
  {
    "English": "monolingual dataset",
    "context": "1: Let D s = {(x i \n s , y i s )} i be a monolingual labeled dataset in language s, realized as a collection of input examples and their labels, x i s ∈ X s and y i s ∈ Y s , respectively.<br>",
    "Arabic": "مجموعة بيانات أحادية اللغة",
    "Chinese": "单语数据集",
    "French": "ensemble de données monolingue",
    "Japanese": "単一言語データセット",
    "Russian": "монолингвальный набор данных"
  },
  {
    "English": "monolingual datum",
    "context": "1: India has around 15-22 languages that are mediumto-high-resource, such as Hindi, Marathi, and Tamil, but dozens of other languages and dialects that are extremely low-resourced, with very little <mark>monolingual data</mark> (<5M tokens), and no other resources, such as Marwadi, Tulu, Dogri, and Santhali.<br>2: , which use <mark>monolingual data</mark> both for learning good language models and for fantasizing parallel data . Another avenue of research has been to extend the traditional supervised learning setting to a weakly supervised one, whereby the original training set is augmented with parallel sentences mined from noisy comparable corpora like Paracrawl.<br>",
    "Arabic": "معطى أحادي اللغة",
    "Chinese": "单语语料",
    "French": "donnée monolingue",
    "Japanese": "単一言語データ",
    "Russian": "одноязычные данные"
  },
  {
    "English": "monolingual embedding",
    "context": "1: We explore this matter through the task of crosslingual embedding alignment, wherein a crosslingual embedding space is learned through an alignment of independently pre-trained <mark>monolingual embeddings</mark> for a directed pair of languages. The quality of cross-lingual embeddings learned this way can be evaluated intrinsically on the task of bilingual dictionary induction (BDI).<br>",
    "Arabic": "تضمين أحادي اللغة",
    "Chinese": "单语嵌入",
    "French": "Plongement monolingue",
    "Japanese": "単言語埋め込み",
    "Russian": "монолингвальное вложение"
  },
  {
    "English": "monolingual model",
    "context": "1: Furthermore, we automatically label a moderate-sized set of 80k sentence pairs using our bilingual model, and train new <mark>monolingual models</mark> using an uptraining scheme. The resulting <mark>monolingual models</mark> demonstrate an error reduction of 9.2% over the Stanford NER systems for Chinese. 2<br>2: In bilingual training, we use the same total amount of data as for the corresponding <mark>monolingual models</mark>-in terms of both the number of tokens (for pretraining the model) and of training pairs. For example, consider a monolingual English model and a monolingual Mandarin model, each trained on 10k tokens and 100k pairs.<br>",
    "Arabic": "نموذج أحادي اللغة",
    "Chinese": "单语模型",
    "French": "modèle monolingue",
    "Japanese": "単言語モデル",
    "Russian": "монолингвальная модель"
  },
  {
    "English": "monolingual training",
    "context": "1: Zero-Shot Transfer. We explore three zero-shot XLT setups: (i) <mark>monolingual training</mark> on English data, (ii) <mark>monolingual training</mark> on Turkish data, machine translated from the original English training data, and (iii) bilingual training on both English and machine-translated Turkish data, with joint bilingual batches. Few-Shot Transfer.<br>",
    "Arabic": "تدريب أحادي اللغة",
    "Chinese": "单语训练",
    "French": "entraînement monolingue",
    "Japanese": "単言語訓練",
    "Russian": "монолингвальное обучение"
  },
  {
    "English": "monotone",
    "context": "1: f (S ∪ {v}) − f (S) ≥ f (T ∪ {v}) − f (T ), \n for all elements v and all pairs of sets S ⊆ T . Submodular functions have a number of very nice tractability properties; the one that is relevant to us here is the following. Suppose we have a function f that is submodular , takes only nonnegative values , and is <mark>monotone</mark> in the sense that adding an element to a set can not cause f to decrease : f ( S ∪ { v } ) ≥ f ( S ) for all elements v and sets S. We wish to find a k-element set S for<br>",
    "Arabic": "متصاعدة",
    "Chinese": "单调增加",
    "French": "monotone",
    "Japanese": "単調増加",
    "Russian": "монотонный"
  },
  {
    "English": "monotonic",
    "context": "1: Notably, this behaviour differs from valid and in-depth comprehension of compositional semantics, however does neither emerge when evaluating model accuracy on standard benchmarks nor when probing for syntactic, <mark>monotonic</mark>, and logically robust reasoning. We propose a novel framework to measure the extent of semantic sensitivity.<br>",
    "Arabic": "متزايد الاتجاه",
    "Chinese": "单调",
    "French": "monotone",
    "Japanese": "単調増加",
    "Russian": "монотонный"
  },
  {
    "English": "monotonicity",
    "context": "1: Property 1 (<mark>Monotonicity</mark>) For every two sets P ⊆ Q ⊆ X , we can establish the inequality f (P) ≤ f (Q) ≤ f (X ).<br>",
    "Arabic": "الرتابة",
    "Chinese": "单调性",
    "French": "Monotonicité",
    "Japanese": "単調性",
    "Russian": "монотонность"
  },
  {
    "English": "morphological analysis",
    "context": "1: Japanese is not written with spaces between words; therefore, we use MeCab 1 , which is a Japanese language <mark>morphological analysis</mark> program. After sentences are divided, the system filters important words, and MeCab evaluates word classes.<br>2: Finally, some datasets focus on colloquial Indonesian mixed with local languages in the scope of <mark>morphological analysis</mark> (Wibowo et al., 2021) and style transfer (Wibowo et al., 2020).<br>",
    "Arabic": "التحليل الصرفي",
    "Chinese": "形态学分析",
    "French": "analyse morphologique",
    "Japanese": "形態素解析",
    "Russian": "морфологический анализ"
  },
  {
    "English": "morphological analyzer",
    "context": "1: The <mark>morphological analyzer</mark> and POS tagger inherently have some level of noise because they do not always perform with perfect accuracy. While we did not have a simple way of assessing the impact of this noise in this work, we can logically expect that the lower the noise the better the results could be.<br>2: Improving the <mark>morphological analyzer</mark> and POS tagger and quantitatively evaluating its accuracy is part of future work. Even though our POS tagger uses heuristic methods and was evaluated mainly through qualitative exploration, we can still see its positive impact on the pre-trained language model.<br>",
    "Arabic": "محلل صرفي",
    "Chinese": "形态分析器",
    "French": "analyseur morphologique",
    "Japanese": "形態素解析器",
    "Russian": "морфологический анализатор"
  },
  {
    "English": "morphological feature",
    "context": "1: Using <mark>morphological features</mark> does improve error prediction across tasks; however, this effect is less pronounced with morphologically complex languages. We speculate this is because morphology is more discriminative in morphologically simple languages. Across all four tasks, case and gender are the <mark>morphological features</mark> most predictive of error.<br>2: The extracted <mark>morphological features</mark> are then concatenated with the token's stem embedding to form the input vector fed to the sentence/document encoder. The sentence/document encoder is made of a standard transformer encoder as used in other BERT models. The sentence/document encoder uses untied position encoding with relative bias as proposed in Ke et al. (2020).<br>",
    "Arabic": "مِلامِحُ صَرْفِيَّة",
    "Chinese": "形态学特征",
    "French": "caractéristique morphologique",
    "Japanese": "形態的特徴",
    "Russian": "морфологическая характеристика"
  },
  {
    "English": "morphological information",
    "context": "1: Probes trained on various representations have obtained high accuracy on tasks requiring part-of-speech and <mark>morphological information</mark> , syntactic and semantic information (Peters et al., 2018b;, among other properties (Conneau et al., 2018), providing evidence that deep representations trained on large datasets are predictive of a broad range of linguistic properties.<br>",
    "Arabic": "معلومات صرفية",
    "Chinese": "形态信息",
    "French": "informations morphologiques",
    "Japanese": "形態論的情報",
    "Russian": "морфологическая информация"
  },
  {
    "English": "morphological operation",
    "context": "1: We construct test benchmarks by separately compositing test samples from AIM, Distinctions, and PhotoMatte85 datasets onto 5 background images per sample. We apply minor background misalignment, color adjustment, and noise to simulate flawed background capture. We generate trimaps from ground-truth alpha using thresholding and <mark>morphological operations</mark>.<br>",
    "Arabic": "العمليات المورفولوجية",
    "Chinese": "形态学运算",
    "French": "opération morphologique",
    "Japanese": "形態学的演算",
    "Russian": "морфологическая операция"
  },
  {
    "English": "morphological segmentation",
    "context": "1: In addition to <mark>morphological segmentation</mark>, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001;Goldsmith, 2001).<br>2: In <mark>morphological segmentation</mark>, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995;Goldsmith, 2001;Creutz and Lagus, 2007).<br>",
    "Arabic": "تجزئة صرفية",
    "Chinese": "形态分割",
    "French": "segmentation morphologique",
    "Japanese": "形態素分割",
    "Russian": "морфологическая сегментация"
  },
  {
    "English": "morphology",
    "context": "1: It is thus especially interesting to investigate to what extent the traditional levels of linguistic analysis such as phonology, <mark>morphology</mark>, syntax and semantics are encoded in the activations of the hidden layers of these models. There are a small number of studies which focus on the syntax and/or semantics in the context of neural models of written language (e.g.<br>2: We evaluate two common conjectures in error analysis of NLP models: (i) Morphology is predictive of errors; and (ii) the importance of <mark>morphology</mark> increases with the morphological complexity of a language. We show across four different tasks and up to 57 languages that of these conjectures, somewhat surprisingly, only (i) is true.<br>",
    "Arabic": "تراكيب",
    "Chinese": "形态学",
    "French": "morphologie",
    "Japanese": "形態論",
    "Russian": "морфология"
  },
  {
    "English": "motion analysis",
    "context": "1: This problem is both fundamental and of great importance to a variety of computer vision areas ranging from traditional tasks such as visual saliency, segmentation, object detection/recognition, tracking and <mark>motion analysis</mark>, medical imaging, structurefrom-motion and 3D reconstruction, to modern applications like autonomous driving, mobile computing, and image-totext analysis.<br>",
    "Arabic": "تحليل الحركة",
    "Chinese": "运动分析",
    "French": "analyse du mouvement",
    "Japanese": "動作解析",
    "Russian": "анализ движения"
  },
  {
    "English": "motion estimation",
    "context": "1: Naturally, two-frame <mark>motion estimation</mark> may be necessary to handle fast-moving scenes (but we do not estimate motion in our experiments). Indirect-invariant imaging A perhaps counterintuitive result is that even though epipolar-only imaging requires two frames, indirect-invariant imaging requires just one.<br>2: With 2-frame <mark>motion estimation</mark> the appearance model is, implicitly, just the most recently observed image. This has the advantage of adapting rapidly to appearance changes, but it suffers because models often drift away from the target. This is especially problematic when the motions of the target and background are similar.<br>",
    "Arabic": "تقدير الحركة",
    "Chinese": "运动估计",
    "French": "estimation du mouvement",
    "Japanese": "動き推定",
    "Russian": "оценка движения"
  },
  {
    "English": "motion matrix",
    "context": "1: To work around this algebraic inconvenience, we rewrite our motion constraint asMJ =M, whereM is an initial estimate of the corrected <mark>motion matrix</mark>. To make our initial estimateM, one may use §B (or the BHB heuristic) and construct a properly structured <mark>motion matrix</mark> from the result.<br>2: Armed with the above results (in particular Theorem 3.2), we are now ready to present our simple algorithm to the non-rigid factorization problem. Recall that the goal is to recover the true <mark>motion matrix</mark> R and the true non-rigid shape matrix S from image measurement W, such that W = RS = R(C⊗I 3 )B. Note that due to the inherent basis ambiguity , it is hopeless to recover a unique B or C. While in previous work many researchers chose to use a pre-selected special shape bases B ( or enforce arbitrary priors on the shape bases or shape coefficients ) to pin down the undetermined degrees-of-freedom , in this work we will show how one can directly<br>",
    "Arabic": "مصفوفة الحركة",
    "Chinese": "运动矩阵",
    "French": "matrice de mouvement",
    "Japanese": "運動行列",
    "Russian": "матрица движения"
  },
  {
    "English": "motion planning",
    "context": "1: Notably, this representation could easily extend to <mark>motion planning</mark> for collision avoidance [11,38,82], while it loses the agent identity characteristic and takes a heavy burden to computation which may constrain the prediction horizon. In contrast, we leverage agent-level information for occupancy prediction and ensure accurate and safe planning by unifying these two modes.<br>",
    "Arabic": "التخطيط الحركي",
    "Chinese": "运动规划",
    "French": "planification de mouvement",
    "Japanese": "動作計画",
    "Russian": "планирование движения"
  },
  {
    "English": "motion segmentation",
    "context": "1: We do not require an alpha regularization loss as in NeRF-W [45] to avoid degeneracies, since we naturally include such an inductive basis by excluding skip connections from network D, which leads D to converge more slowly than the static IBR model. We show our estimated <mark>motion segmentation</mark> masks overlaid on input images in Fig. 5.<br>2: Several methods have been proposed to reconstruct sparse geometry of a dynamic scene [27,50,36,40]. Russell et al. [31] and Ranftl et al. [29] suggest motion/object segmentation based algorithms to decompose a dynamic scene into piecewise rigid parts.<br>",
    "Arabic": "تقسيم الحركة",
    "Chinese": "运动分割",
    "French": "segmentation de mouvement",
    "Japanese": "動き分割",
    "Russian": "сегментация движения"
  },
  {
    "English": "move average",
    "context": "1: 3) and convergence to the invariant parabola s j ∝ p 2 j in Eqn. 14 with weight decay (η > 0). Here the estimate correlation matrix F can be obtained by a <mark>moving average</mark>: \n F = ρF + (1 − ρ)E B [f f ] (19 \n ) \n where  .<br>2: This computation is similar to maintaining the memory bank as in [36]. This movingaverage provides an approximated expectation of multiple views. This variant has 55.0% accuracy without the predictor h. As a comparison, it fails completely if we remove h but do not maintain the <mark>moving average</mark> (as shown in Table 1a).<br>",
    "Arabic": "متوسط متحرك",
    "Chinese": "移动平均",
    "French": "moyenne mobile",
    "Japanese": "移動平均",
    "Russian": "скользящее среднее"
  },
  {
    "English": "multi-agent",
    "context": "1: (2017) also address the stability of experience replay in <mark>multi-agent</mark> settings, but assume a fully decentralised training regime. (Lowe et al. 2017) concurrently propose a <mark>multi-agent</mark> policy-gradient algorithm using centralised critics. Their approach does not address <mark>multi-agent</mark> credit assignment.<br>2: Most previous applications of RL to StarCraft micromanagement use a centralised controller, with access to the full state, and control of all units, although the architecture of the controllers exploits the <mark>multi-agent</mark> nature of the problem. Usunier et al.<br>",
    "Arabic": "متعدد الوكلاء",
    "Chinese": "多智能体",
    "French": "multi-agents",
    "Japanese": "複数エージェント",
    "Russian": "многоагентный"
  },
  {
    "English": "multi-agent interaction",
    "context": "1: We extended the principle of maximum entropy to settings with sequentially revealed information in this work. We demonstrated the applicability of the resulting principle of maximum causal entropy for learning policies in stochastic control, <mark>multi-agent interaction</mark>, and partially observable settings.<br>",
    "Arabic": "تفاعل العوامل المتعددة",
    "Chinese": "多智能体交互",
    "French": "interaction multi-agents",
    "Japanese": "マルチエージェント相互作用",
    "Russian": "многоагентное взаимодействие"
  },
  {
    "English": "multi-agent learning",
    "context": "1: Exploration-exploitation is a powerful and practical tool in <mark>multi-agent learning</mark> (MAL), however, its effects are far from understood. To make progress in this direction, we study a smooth analogue of Q-learning. We start by showing that our learning model has strong theoretical justification as an optimal model for studying exploration-exploitation.<br>2: Sometimes it is possible to design individual reward functions for each agent. However, these rewards are not generally available in cooperative settings and often fail to encourage individual agents to sacrifice for the greater good. This often substantially impedes <mark>multi-agent learning</mark> in challenging tasks, even with relatively small numbers of agents.<br>",
    "Arabic": "تعلم متعدد الوكلاء",
    "Chinese": "多智能体学习",
    "French": "apprentissage multi-agent",
    "Japanese": "多エージェント学習",
    "Russian": "многоагентное обучение"
  },
  {
    "English": "multi-agent reinforcement learning",
    "context": "1: Correlated equilibria provide an appropriate solution concept for coordination problems in which agents have arbitrary utilities, and may work towards different objectives. The study of uncoupled dynamics converging to correlated equilibria in problems with sequential actions and hidden information lays new theoretical foundations for <mark>multi-agent reinforcement learning</mark> problems.<br>2: Games where multiple agents jointly take actions in a dynamic environment have been widely studied in the literature on <mark>multi-agent reinforcement learning</mark>, but usually in settings without strategy commitment (Littman, 1994;Buşoniu et al., 2010).<br>",
    "Arabic": "التعلم المعزز متعدد الوكلاء",
    "Chinese": "多智能体强化学习",
    "French": "apprentissage par renforcement multi-agent",
    "Japanese": "多エージェント強化学習",
    "Russian": "многоагентное обучение с подкреплением"
  },
  {
    "English": "multi-agent system",
    "context": "1: That is, algorithms should take additional samples whenever they need them and from whichever data distribution they want them. On-demand sampling is especially appropriate when some population data is scarce ( as in fairness mechanisms in which samples are amended [ 46 ] ) ; when the designer can actively perturb datasets towards rare or atypical instances ( such as in robustness applications [ 29,59 ] ) ; or when sample sets represent agents ' contributions to an interactive <mark>multi-agent system</mark> [ 39,10<br>2: In another direction of work, we propose to extend a <mark>multi-agent system</mark> application dealing with decentralized calendar management [7] that already uses the sensitive data transaction protocol [5] by implementing the hippocratic social order.<br>",
    "Arabic": "نظام متعدد الوكلاء",
    "Chinese": "多智能体系统",
    "French": "système multi-agent",
    "Japanese": "マルチエージェントシステム",
    "Russian": "многоагентная система"
  },
  {
    "English": "multi-armed bandit",
    "context": "1: The standard <mark>multi-armed bandit</mark> (MAB) problem was originally proposed by Robbins (1952), and presents one of the clearest examples of the trade-off between exploration and exploitation in reinforcement learning.<br>2: In budget-limited <mark>multi-armed bandit</mark> (MAB) problems, the learner's actions are costly and constrained by a fixed budget.<br>",
    "Arabic": "مشكلة الأذرع المتعددة",
    "Chinese": "多臂赌博机",
    "French": "bandit à bras multiples",
    "Japanese": "多腕バンディット",
    "Russian": "задача о многорукой бандитке"
  },
  {
    "English": "multi-armed bandit problem",
    "context": "1: 6 is motivated by the UCB algorithm for the classical <mark>multi-armed bandit problem</mark> (Auer et al., 2002;Kocsis & Szepesvári, 2006). Among competing criteria for GP optimization (see Section 1), a variant of the GP-UCB rule has been demonstrated to be effective for this application (Dorard et al., 2009).<br>",
    "Arabic": "مشكلة قطاع الطرق متعدد الأسلحة",
    "Chinese": "多臂赌博机问题",
    "French": "problème de bandit multi-bras",
    "Japanese": "多腕バンディット問題",
    "Russian": "проблема многорукого бандита"
  },
  {
    "English": "multi-class",
    "context": "1: For <mark>multi-class</mark> image segmentation and labeling we use contrast-sensitive two-kernel potentials, defined in terms of the color vectors I i and I j and positions p i and p j : \n<br>2: In this paper, we propose a novel framework -SentiGAN, which has multiple generators and one <mark>multi-class</mark> discriminator, to address the above problems. In our framework, multiple generators are trained simultaneously, aiming at generating texts of different sentiment labels without supervision.<br>",
    "Arabic": "متعدد الفئات",
    "Chinese": "多类别",
    "French": "multi-classe",
    "Japanese": "多クラス",
    "Russian": "многоклассовый"
  },
  {
    "English": "multi-class classification",
    "context": "1: Unlike traditional VQA-2 evaluation, which treats the task as a multi-label binary classification problem, we follow prior active learning work on VQA (Lin and Parikh, 2017), which formulates it as a <mark>multi-class classification</mark> problem, enabling the use of acquisition functions such as uncertainty sampling and BALD. the right of?\".<br>2: For <mark>multi-class classification</mark>, y = argmax ℓ f ℓ (x) is predicted as the single associated label of x. For multi-label classification, all labels ℓ with positive f ℓ (x) are considered to be associated with x. This method is also what \"TF-IDF+SVM\" in Chalkidis et al.<br>",
    "Arabic": "التصنيف متعدد الفئات",
    "Chinese": "多分类",
    "French": "classification multi-classes",
    "Japanese": "多クラス分類",
    "Russian": "многоклассовая классификация"
  },
  {
    "English": "multi-class logistic regression",
    "context": "1: We then feed the observations into our trained model and take the mean of the Gaussian encoder as the representations. Finally, we predict each of the ground-truth factors based on the representations with a separate learning algorithm. We consider both a 5-fold cross-validated <mark>multi-class logistic regression</mark> as well as gradient boosted trees of the Scikit-learn package.<br>2: We trained several neural-network policies based on a <mark>multi-class logistic regression</mark> loss function using stochastic gradient descent, with an RMSProp step size [29], implemented in the Theano [28] library. We compare the policies: \n<br>",
    "Arabic": "التصنيف اللوجستي متعدد الفئات",
    "Chinese": "多类逻辑回归",
    "French": "régression logistique multiclasse",
    "Japanese": "マルチクラスロジスティック回帰",
    "Russian": "многоклассовая логистическая регрессия"
  },
  {
    "English": "multi-class problem",
    "context": "1: In order to model label correlations, label powerset (LP) (Tsoumakas and Katakis, 2006) transforms a multi-label problem to a <mark>multi-class problem</mark> with a classifier trained on all unique label combinations.<br>",
    "Arabic": "مشكلة متعددة الفئات",
    "Chinese": "多类别问题",
    "French": "problème à classes multiples",
    "Japanese": "多クラス問題",
    "Russian": "многоклассовая задача"
  },
  {
    "English": "multi-classification",
    "context": "1: Formally, when dealing with the <mark>multi-classification</mark> task, the final objective function can be expressed as E( L CE ) = \n L CE + i 1 2 z ci i (1 − z ci i )V ar(h ci i ) \n<br>2: By reducing the variance ofh i , random dropping methods motivate the model to extract more essential high-level representations. Therefore, the robustness of the models is enhanced. It is noted that Equation 3 can be well generalized to <mark>multi-classification</mark> tasks by extending dimension of the model output.<br>",
    "Arabic": "تصنيف متعدد",
    "Chinese": "多类分类",
    "French": "multi-classification",
    "Japanese": "多クラス分類",
    "Russian": "многоклассовая классификация"
  },
  {
    "English": "multi-document summarization",
    "context": "1: First, we consider information ordering, that is, choosing a sequence in which to present a pre-selected set of items; this is an essential step in concept-to-text generation, <mark>multi-document summarization</mark>, and other text-synthesis problems.<br>2: Finally, this task requires models to jointly read and analyze evidence from both textual and tabular sources and determine which is relevant and which can be ignored, thus combining challenging aspects of both <mark>multi-document summarization</mark> and data-to-text generation.<br>",
    "Arabic": "تلخيص متعدد المستندات",
    "Chinese": "多文档摘要",
    "French": "résumé multi-document",
    "Japanese": "多文書要約",
    "Russian": "многодокументное резюмирование"
  },
  {
    "English": "multi-domain",
    "context": "1: We presented M4, a large-scale multi-generator, <mark>multi-domain</mark>, and multi-lingual dataset for machine-generated text detection. We further experimented with this dataset performing a number of cross-domain, cross-generator, cross-lingual, and zero-shot experiments using seven detectors.<br>2: • We construct M4: a large-scale multigenerator, <mark>multi-domain</mark>, and multi-lingual corpus for detecting machine-generated texts in a black-box scenario where there is no access to a potential generator or its outputs except for plain text. • We study the performance of automatic detectors from various perspectives : ( a ) different detectors across different domains for a specific LLM generator , ( b ) different detectors across different generators for a specific domain , ( c ) interactions of domains and generators in a multilingual setting , and ( d ) the performance of the detector on data generated<br>",
    "Arabic": "متعدد المجالات",
    "Chinese": "多领域",
    "French": "multi-domaine",
    "Japanese": "多領域",
    "Russian": "многодоменный"
  },
  {
    "English": "multi-head",
    "context": "1: Under the <mark>multi-head</mark> perspective, this attention generates different sparse query-key pairs for each head, which avoids severe information loss in return.<br>2: To regularize the dense features, we append an auxiliary branch that predicts the <mark>multi-head</mark> dense 3D coordinates and corresponding weights, as shown in Figure 9.<br>",
    "Arabic": "متعدد الرؤوس",
    "Chinese": "多头",
    "French": "multi-tête",
    "Japanese": "複数ヘッド",
    "Russian": "многоголовочный"
  },
  {
    "English": "multi-head attention",
    "context": "1: Finally, the <mark>multi-head attention</mark> defined in ( 39) is equivalent to first concatenating the output of each attention head and then using a linear mapping to transform the results. Thus, the concatenation is clearly an injective mapping of the tuple of multisets χ t,1 G , χ t,2 G , ..., χ t,|Dn| G \n .<br>2: Here h i is the hidden representation for step i, which is calculated by a <mark>multi-head attention</mark> over encoder representation x and token representations of previous decoding steps. The linear classification weight is denoted by W ∈ R d×v , d is the hidden dimension size and v is the vocabulary size of identifiers.<br>",
    "Arabic": "الانتباه متعدد الرؤوس",
    "Chinese": "多头注意力",
    "French": "attention multi-têtes",
    "Japanese": "マルチヘッドアテンション",
    "Russian": "многоголовое внимание"
  },
  {
    "English": "multi-head attention layer",
    "context": "1: {z i } n+1 i=1 = MHA(Q = K = V = {s ∥ {r i } n i=1 }),(3) \n where MHA is the <mark>multi-head attention layer</mark>, Q/K/V is the corresponding query/key/value, ∥ is the concatenation operation.<br>2: The decoder has a similar structure as the encoder except that, in each decoder layer between the self-attention layer and feed-forward layer, a <mark>multi-head attention layer</mark> attends to the output of the encoder. Layer normalization (Ba et al., 2016) is applied to the output of each skip connection.<br>",
    "Arabic": "طبقة الانتباه متعددة الرؤوس",
    "Chinese": "多头注意力层",
    "French": "couche d'attention multi-têtes",
    "Japanese": "多頭注目層",
    "Russian": "слой многоголовного внимания"
  },
  {
    "English": "multi-head self-attention",
    "context": "1: Figure 1 depicts the overall architecture of our model. The basis for our model is the Transformer encoder introduced by Vaswani et al. (2017): we transform word embeddings into contextually-encoded token representations using stacked <mark>multi-head self-attention</mark> and feedforward layers ( §2.1).<br>2: where MHCA, MHSA denote multi-head cross-attention and <mark>multi-head self-attention</mark> [91] respectively. As it is also important to focus on the intended position, i.e., goal point, to refine the predicted trajectory, we devise an agent-goal point attention via deformable attention [109] as follows: \n<br>",
    "Arabic": "اهتمام ذاتي متعدد الرؤوس",
    "Chinese": "多头自注意力机制",
    "French": "auto-attention à têtes multiples",
    "Japanese": "マルチヘッド自己注意",
    "Russian": "многоголовое самовнимание"
  },
  {
    "English": "multi-head self-attention mechanism",
    "context": "1: The great success of Transformer-based models benefits from the powerful <mark>multi-head self-attention mechanism</mark>, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions.<br>2: We employ a multi-head selfattention mechanism [Vaswani et al., 2017] to select and focus on the important and essential hidden states of the Bi-LSTM layer. It jointly attends to information at different positions of the input sequence with multiple individual attention functions and separately normalized parameters called attention heads.<br>",
    "Arabic": "آلية الانتباه الذاتي متعدد الرؤوس",
    "Chinese": "多头自注意力机制",
    "French": "mécanisme d'auto-attention multi-têtes",
    "Japanese": "マルチヘッド自己注意メカニズム",
    "Russian": "механизм многоголовочного самовнимания"
  },
  {
    "English": "multi-head self-attention module",
    "context": "1: This operation results in a downsampling of the feature map by a rate of n. \"96-d\" denotes a linear layer with an output dimension of 96. \"win. sz. 7 × 7\" indicates a <mark>multi-head self-attention module</mark> with window size of 7 × 7.<br>",
    "Arabic": "وحدة الانتباه الذاتي متعددة الرؤوس",
    "Chinese": "多头自注意力模块",
    "French": "module d'auto-attention multi-têtes",
    "Japanese": "\"マルチヘッド自己注意モジュール\"",
    "Russian": "Модуль многоголовного самовнимания"
  },
  {
    "English": "multi-headed self-attention",
    "context": "1: It employs a CRF on top of a bidirectional LSTM to extract intents in a consistent format, subject to constraints among intent tag labels. We apply <mark>multi-headed self-attention</mark> and adversarial training to effectively learn dependencies between distant words, and robustly adapt our model across varying domains.<br>",
    "Arabic": "الانتباه الذاتي متعدد الرؤوس",
    "Chinese": "多头自注意力",
    "French": "attention multi-têtes",
    "Japanese": "マルチヘッドセルフアテンション",
    "Russian": "многоголовое самовнимание"
  },
  {
    "English": "multi-label",
    "context": "1: Person-alityCafe and Facebook datasets are used to test the performance of link prediction, both of which are social networks where edges denote the following/quoting relations. Multi-label v.s. Multi-class Classification In the main experiments, we treat the classification task as a <mark>multi-label</mark> problem. Here we present the experimental results under a multi-class setting.<br>2: Obviously, multi-view data is rich in more semantic information, which greatly facilitates the learning of <mark>multi-label</mark> semantic content (Huang et al. 2015;Wang et al. 2020;Hu, Lou, and Ye 2021). Therefore, different from the simple single-label classification task (Zhao et al.<br>",
    "Arabic": "متعدد التسميات",
    "Chinese": "多标签",
    "French": "multi-étiquette",
    "Japanese": "複数ラベル",
    "Russian": "многометочный"
  },
  {
    "English": "multi-label classification",
    "context": "1: 2008), many of which try to learn a better classifier for <mark>multi-label classification</mark> based on the correlation structure among the training data and the labels (Yu et al. 2006;Virtanen, Klami, and Kaski 2011). However, our work conducts two tasks-the association discovery and ordinal label prediction-simultaneously to benefit each other.<br>2: 2019), we utilize the binary crossentropy loss, which is widely used in <mark>multi-label classification</mark> tasks, as the <mark>multi-label classification</mark> loss L M C to evaluate the difference between prediction and ground truth: \n L M C = − 1 nc n i=1 c j=1 \n Yi,j log(Pi,j ) \n<br>",
    "Arabic": "التصنيف متعدد العلامات",
    "Chinese": "多标签分类",
    "French": "classification multi-étiquettes",
    "Japanese": "多ラベル分類",
    "Russian": "классификация по нескольким меткам"
  },
  {
    "English": "multi-label classification loss",
    "context": "1: Predicted probability vector. s γ (y, y ) \n Similarity between label y and y . γ ≥ 0 \n Scaling hyperparameter in similarity. λ ≥ 0 \n Coefficient of fairness penalty. mlc (h) \n Multi-label classification loss. sγ (y,y adv ) (h) s γ -SimFair violation (penalty).<br>",
    "Arabic": "خسارة التصنيف متعدد التسميات",
    "Chinese": "多标签分类损失",
    "French": "perte de classification multi-étiquettes",
    "Japanese": "多ラベル分類損失",
    "Russian": "потеря классификации по нескольким меткам"
  },
  {
    "English": "multi-label classifier",
    "context": "1: For any random sample (x, a, y), fairness of its prediction is always taken in consideration, but as the affinity of y to y adv decreases, it will be down-weighted when estimating fairness violations with respect to y adv . Definition 1 (s γ -SimFair). Given a similarity function s : \n Y × Y → [ 0 , 1 ] , a <mark>multi-label classifier</mark> h satisfies Similarity s-induced Fairness ( s γ -SimFair ) if for ∀ k ∈ A , E [ ỹs ( y , y adv ) ] E [ s ( y , y adv ) ] = E [ ỹ1 ( a = k ) s ( y , y<br>2: Thus we only have access to a noisy subset of all the possible triplets. We used a random sample of 2595 movies, and their corresponding triplets, to learn a <mark>multi-label classifier</mark> using TripletBoost (with 10 6 boosting iterations).<br>",
    "Arabic": "مصنف متعدد العلامات",
    "Chinese": "多标签分类器",
    "French": "classificateur multi-étiquettes",
    "Japanese": "マルチラベル分類器",
    "Russian": "многометочный классификатор"
  },
  {
    "English": "multi-label datum",
    "context": "1: Through a systematic study, we show that on <mark>multi-label data</mark>, because of unevenly distributed labels, EOp usually fails to construct a reliable estimate on labels with few instances. We then propose a new framework named Similarity s-induced Fairness (sγ-SimFair).<br>",
    "Arabic": "بيانات متعددة العلامات",
    "Chinese": "多标签数据",
    "French": "donnée multi-étiquette",
    "Japanese": "マルチラベルデータ",
    "Russian": "многометочные данные"
  },
  {
    "English": "multi-label learning",
    "context": "1: Evaluation metrics: Similar to (Tan et al. 2018) and (Li and Chen 2021), four popular metrics commonly used in the <mark>multi-label learning</mark> field are adopted to evaluate these approaches.<br>2: Considering privileged label information in the <mark>multi-label learning</mark> problems, a privileged <mark>multi-label learning</mark> (PrML) method explores and exploits the connections between different examples' labels and is extended into domain adaptation [You et al., 2017].<br>",
    "Arabic": "تعلم متعدد التصنيفات",
    "Chinese": "多标签学习",
    "French": "apprentissage multi-étiquettes",
    "Japanese": "多ラベル学習",
    "Russian": "обучение по нескольким меткам"
  },
  {
    "English": "multi-label text classification",
    "context": "1: Arxiv Academic Paper Dataset (AAPD) 3 : We build a new large dataset for the <mark>multi-label text classification</mark>. We collect the abstract and the corresponding subjects of 55,840 papers in the computer science field from the website 4 . An academic paper may have multiple subjects and there are 54 subjects in total.<br>",
    "Arabic": "تصنيف النص متعدد التسمية",
    "Chinese": "多标签文本分类",
    "French": "classification de texte multi-étiquettes",
    "Japanese": "多ラベルテキスト分類",
    "Russian": "классификация текста с несколькими метками"
  },
  {
    "English": "multi-layer neural network",
    "context": "1: In contrast, recall if f i (x)'s are <mark>multi-layer neural networks</mark> with different random seeds, then training their average barely gives any better performance comparing to individual networks f i , as now all the f i 's are capable of learning the same set of features. Contradiction 2: knowledge distillation does not work.<br>2: We now specialize the law of robustness (Theorem 4) to <mark>multi-layer neural networks</mark>. We consider a rather general class of depth D neural networks described as follows. First, we require that the neurons are partitioned into layers L1, . . .<br>",
    "Arabic": "شبكة عصبية متعددة الطبقات",
    "Chinese": "多层神经网络",
    "French": "réseau neuronal à couches multiples",
    "Japanese": "多層ニューラルネットワーク",
    "Russian": "многослойная нейронная сеть"
  },
  {
    "English": "multi-layer perceptron",
    "context": "1: The key driver of computational and memory cost of the proposed rendering algorithm is the ray-marching itself: In every step of the ray-marcher, for every pixel, the scene representation φ is evaluated. Each evaluation of φ is a full forward pass through a <mark>multi-layer perceptron</mark>.<br>2: In the present paper, these stack-summaries serve as input to a <mark>multi-layer perceptron</mark> whose output is converted via softmax into a categorical distribution over three possible parser actions: open a new constituent, close off the latest constituent, or generate a word.<br>",
    "Arabic": "الشبكة العصبية متعددة الطبقات",
    "Chinese": "多层感知器",
    "French": "perceptron multicouche",
    "Japanese": "多層パーセプトロン",
    "Russian": "многослойный перцептрон"
  },
  {
    "English": "multi-modal",
    "context": "1: • We provide baselines including adapted architectures for <mark>multi-modal</mark> long-range summarization, and report results showing that (1) performance is far from being optimal; and \n (2) layout provides valuable information. All the datasets are available on HuggingFace. 1 \n 2 Related Work<br>2: While primarily used as a dataset for regression, it would be also interesting to assess and understand the degree to which different variables are better modeled as stochastic or deterministic, or if the dataset gives rise to heavy-tailed or even <mark>multi-modal</mark> conditional distributions that are important to capture.<br>",
    "Arabic": "متعدد الوسائط",
    "Chinese": "多模态",
    "French": "multimodal",
    "Japanese": "マルチモーダル",
    "Russian": "многомодальный"
  },
  {
    "English": "multi-modal input",
    "context": "1: We jointly optimize a family of self-supervised tasks in an encoderdecoder setup, making this work an example of multitask self-supervised learning. Multi-task self-supervised learning has been applied to other domains such as visual data [12,25], accelerometer recordings [35], audio [34] and <mark>multi-modal inputs</mark> [37,30].<br>",
    "Arabic": "مدخلات متعددة الوسائط",
    "Chinese": "多模态输入",
    "French": "entrées multimodales",
    "Japanese": "マルチモーダル入力",
    "Russian": "мультимодальный вход"
  },
  {
    "English": "multi-modal learning",
    "context": "1: Future work includes extending our metrics to new tasks such as SLU (Chapuis et al. 2020(Chapuis et al. , 2021Dinkar et al. 2020;Colombo, Clavel, and Piantanida 2021), controlled sentence generation (Colombo et al. 2019(Colombo et al. , 2021b and <mark>multi-modal learning</mark> (Colombo et al.<br>",
    "Arabic": "التعلم متعدد الوسائط",
    "Chinese": "多模态学习",
    "French": "apprentissage multimodal",
    "Japanese": "多モダル学習",
    "Russian": "многомодальное обучение"
  },
  {
    "English": "multi-modal model",
    "context": "1: It is both visually and linguistically richer, moving beyond whole shape descriptions to include segmentation maps and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent <mark>multi-modal models</mark>. We observe that pre-trained weights demonstrate limited abstract reasoning, which dramatically improves with fine-tuning.<br>2: For example, it enables the data-driven scaling of studies of human interactions and models of whole-part reasoning in language and vision models. In this paper, we use KILOGRAM to evaluate the visual reasoning capacities of recent pre-trained <mark>multi-modal models</mark>, focusing on generalizing concepts to abstract shapes.<br>",
    "Arabic": "نموذج متعدد الوسائط",
    "Chinese": "多模态模型",
    "French": "modèle multimodal",
    "Japanese": "多様なモダリティを統合したモデル",
    "Russian": "мультимодальная модель"
  },
  {
    "English": "multi-object detection",
    "context": "1: This approach has the advantage that unary potentials can now be defined with object templates, say, centered on the segment. However, the initial segmentation must be fairly accurate and enforces NMS and mutual exclusion without objectlevel layout models. To our knowledge, the problem of end-to-end learning of <mark>multi-object detection</mark> (i.e.<br>",
    "Arabic": "الكشف عن كائنات متعددة",
    "Chinese": "多目标检测",
    "French": "détection multi-objets",
    "Japanese": "多物体検出",
    "Russian": "обнаружение нескольких объектов"
  },
  {
    "English": "multi-objective optimization",
    "context": "1: Following the paradigm, we implement an auto-search engine that can automatically search well-performing and scalable GNN architectures to balance the trade-off between multiple criteria (e.g., accuracy and efficiency) via <mark>multi-objective optimization</mark>.<br>2: We apply the <mark>multi-objective optimization</mark> targeting at classification error and inference time on Cora. Figure 6 demonstrates the Pareto Front found by PaSca with a budget of 2000 evaluations, together with the results of several manually designed scalable GNNs. The inference time has been normalized based on instances with the minimum and maximum inference time in our design space.<br>",
    "Arabic": "تحسين متعدد الأهداف",
    "Chinese": "多目标优化",
    "French": "optimisation multi-objectif",
    "Japanese": "多目的最適化",
    "Russian": "многоцелевая оптимизация"
  },
  {
    "English": "multi-scale",
    "context": "1: Its availability across coarse-resolution, high-resolution, aquaplanet and realgeography use cases is also new to the community. Successful ML innovations with ClimSim can have a downstream impact since it is based on a state-of-the-art <mark>multi-scale</mark> climate simulator that is actively supported by a mission agency (U.S. Department of Energy). In non-<mark>multi-scale</mark> settings , an important body of related work [ 6 ] [ 7 ] [ 8 ] [ 9 ] has made exciting progress on using analogous hybrid ML approaches to reduce biases in uniform resolution climate simulations , including in an operational climate code with land coupling and downstream hybrid stability [ 17,18 ] ( see Supplementary Information ; SI )<br>2: In this section, we describe in detail the formulation of our proposed edge detection system. We start by discussing related neural-network-based approaches, particularly those that emphasize <mark>multi-scale</mark> and multi-level feature learning. The task of edge and object boundary detection is inherently challenging. After decades of research , there have emerged a number of properties that are key and that are likely to play a role in a successful system : ( 1 ) carefully designed and/or learned features [ 28,5 ] , ( 2 ) <mark>multi-scale</mark> response fusion [ 40,32,30 ] , ( 3 ) engagement of different levels of visual perception [ 18,27,39,17 ] such as mid-level Gestalt law information [ 7 ] , ( 4 ) incorporating structural information ( intrinsic correlation carried within the input data and output solution ) [ 6 ] and context ( both short-and long-range interactions ) [ 38 ] , ( 5 ) making holistic image predictions ( referring to approaches that perform prediction by taking the image contents globally and directly<br>",
    "Arabic": "متعدد المقاييس",
    "Chinese": "多尺度",
    "French": "multi-échelle",
    "Japanese": "多スケール",
    "Russian": "многомасштабный"
  },
  {
    "English": "multi-scale architecture",
    "context": "1: Finally in Section 3.6 we describe the <mark>multi-scale architecture</mark>.<br>",
    "Arabic": "الهندسة المعمارية متعددة المقاييس",
    "Chinese": "多尺度架构",
    "French": "architecture multi-échelle",
    "Japanese": "マルチスケールアーキテクチャ",
    "Russian": "многомасштабная архитектура"
  },
  {
    "English": "multi-scale training",
    "context": "1: and 3x schedule ( 36 epochs with the learning rate decayed by 10× at epochs 27 and 33 ) . For system-level comparison , we adopt an improved HTC [ 9 ] ( denoted as HTC++ ) with instaboost [ 22 ] , stronger <mark>multi-scale training</mark> [ 7 ] ( resizing the input such that the shorter side is between 400 and 1400 while the longer side is at most 1600 ) , 6x schedule ( 72 epochs with the learning rate decayed at<br>2: For the ablation study, we consider four typical object detection frameworks: Cascade Mask R-CNN [29,6], ATSS [79], RepPoints v2 [12], and Sparse RCNN [56] in mmdetection [10]. For these four frameworks , we utilize the same settings : <mark>multi-scale training</mark> [ 8,56 ] ( resizing the input such that the shorter side is between 480 and 800 while the longer side is at most 1333 ) , AdamW [ 44 ] optimizer ( initial learning rate of 0.0001 , weight decay of 0.05 , and batch size of 16 ) ,<br>",
    "Arabic": "التدريب متعدد المقاييس",
    "Chinese": "多尺度训练",
    "French": "entraînement multi-échelle",
    "Japanese": "多尺度学習",
    "Russian": "обучение на нескольких масштабах"
  },
  {
    "English": "multi-task",
    "context": "1: ℓ ) E } where S ( ⊳ ) E = { D ⊳1 , • • • , D ⊳ ⊳ } . Then the <mark>multi-task</mark> prompting is presented in Algorithm 1. We treat each node/edge/graph class as a binary classification task so that they can share the same task head.<br>2: Multi-task meta-learning introduces an expectation over task objectives. BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. For a general <mark>multi-task</mark> formulation, see Appendix D; here we focus on the few-shot classification paradigm.<br>",
    "Arabic": "متعدد المهام",
    "Chinese": "多任务",
    "French": "multi-tâche",
    "Japanese": "マルチタスク",
    "Russian": "многозадачность"
  },
  {
    "English": "multi-task fine-tuning",
    "context": "1: With semantic prefix-tuning, we treat chart captioning as a <mark>multi-task fine-tuning</mark> problem, where the model is trained to generate the L1 and L2/L3 captions separately. In every epoch, the model sees each VisText chart twice, once with the L1 prefix and caption and once with the L2/L3 prefix and caption.<br>",
    "Arabic": "التدريب الدقيق متعدد المهام",
    "Chinese": "多任务微调",
    "French": "affinage multi-tâches",
    "Japanese": "複数タスク微調整",
    "Russian": "многозадачная точная настройка"
  },
  {
    "English": "multi-task learning",
    "context": "1: To this end, we propose the <mark>Multi-Task Learning</mark> approach based on Discriminative Gaussian Process Latent Variable Model (DGPLVM) [57], named GaussianFace, for face verification.<br>2: We introduce TA-DA, a Topic-Aware Domain Adaptation framework for keyphrase extraction that integrates <mark>Multi-Task Learning</mark> with Adversarial Training and Domain Adaptation. Our approach improves performance over baseline models by up to 5% in the exact match of the F1-score.<br>",
    "Arabic": "التعلم متعدد المهام",
    "Chinese": "多任务学习",
    "French": "apprentissage multi-tâches",
    "Japanese": "多タスク学習",
    "Russian": "многозадачное обучение"
  },
  {
    "English": "multi-task model",
    "context": "1: In this way, our model can benefit from improved, external parsing models without re-training. Unlike typical <mark>multi-task models</mark>, ours maintains the ability to leverage external syntactic information.<br>2: Finally, <mark>multi-task models</mark> do not converge to the mean of strides, but rather to a higher value that passes more frequencies not to negatively impact individual tasks.<br>",
    "Arabic": "نموذج متعدد المهام",
    "Chinese": "多任务模型",
    "French": "modèle multitâche",
    "Japanese": "複数タスクモデル",
    "Russian": "многозадачная модель"
  },
  {
    "English": "multi-task regression",
    "context": "1: The dataset has the following characteristics: (i) It is a <mark>multi-task regression</mark> dataset with 3 tasks. (ii) Each task has high degree of over-dispersion. (iii) All tasks are not fully observed as each user signs up for a subset of products/services.<br>2: For easier exposition, we consider tasks of the same kind: multilabel classification or <mark>multi-task regression</mark>. For multilabel classification, each task is assumed to have same number of classes (with k = C) for easier exposition -our framework can handle multilabel settings with different number of classes per task.<br>",
    "Arabic": "انحدار متعدد المهام",
    "Chinese": "多任务回归",
    "French": "régression multitâche",
    "Japanese": "多タスク回帰",
    "Russian": "многозадачная регрессия"
  },
  {
    "English": "multi-task setting",
    "context": "1: In online optimisation, the MG update can achieve strong convergence guarantees if the problem is well-behaved (van Erven & Koolen, 2016), with similar guarantees in the <mark>multi-task setting</mark> (Balcan et al., 2019;Khodak et al., 2019;Denevi et al., 2019).<br>2: We also aim to collect gaze data and then model the gaze predictions in a <mark>multi-task setting</mark>. We plan to investigate other multilingual contextual embeddings' performance for this task (e.g., M-BERT, IndicBERT, MuRIL).<br>",
    "Arabic": "إعداد متعدد المهام",
    "Chinese": "多任务设置",
    "French": "contexte multi-tâches",
    "Japanese": "複数タスク設定",
    "Russian": "многозадачная настройка"
  },
  {
    "English": "multi-view",
    "context": "1: This allows interacting with Φ via the toolbox of <mark>multi-view</mark> and perspective geometry that the physical world obeys, only using learning to approximate the unknown properties of the scene itself. In Sec. 4, we show that this formulation leads to <mark>multi-view</mark> consistent novel view synthesis, data-efficient training, and a significant gain in model interpretability.<br>2: Overall, our DICNet is adept in capturing consistent discriminative representations of <mark>multi-view</mark> multi-label data and avoiding the negative effects of missing views and missing labels. Extensive experiments performed on five datasets validate that our method outperforms other state-of-the-art methods.<br>",
    "Arabic": "متعدد الآراء",
    "Chinese": "多视图",
    "French": "multi-vue",
    "Japanese": "複数の視点",
    "Russian": "многовидовой"
  },
  {
    "English": "multi-view datum",
    "context": "1: T ; Stopping threshold σ. Initialization : Fill the missing elements of the <mark>multi-view data</mark> and multi-lable data with ' 0 ' , and randomly initialize the network weights ; Set L last = 0 ; Initialize prediction label P last of n t test samples . Output: Parameters of trained model.<br>2: (i) for every r ∈ [m] \\ M (0) i , every ∈ [2], it holds that w (t) i,r , v i, ≤ O(σ 0 ). Intuition. The first three items in Induction Hypothesis C.3 essentially say that , when studying the correlation between w i , r with a <mark>multi-view data</mark> , or between w i , r with a single-view data ( but y = i ) , the correlation is about w i , r , v i,1 and w i , r , v i,2 and the<br>",
    "Arabic": "المعطيات متعددة الرؤى",
    "Chinese": "多视图数据",
    "French": "données multi-vues",
    "Japanese": "複数ビューデータ",
    "Russian": "многовидовые данные"
  },
  {
    "English": "multi-view geometry",
    "context": "1: The key idea of SRNs is to represent a scene implicitly as a continuous, differentiable function that maps a 3D world coordinate to a feature-based representation of the scene properties at that coordinate. This allows SRNs to naturally interface with established techniques of multi-view and projective geometry while operating at high spatial resolution in a memory-efficient manner.<br>2: In this paper, we present a step towards a complete characterization of all minimal problems for points, lines and their incidences in calibrated <mark>multi-view geometry</mark>. This is a grand challenge, especially when dealing with partial visibility due to occlusions and missing detections. Here we provide a complete characterization for the case of complete multi-view visibility.<br>",
    "Arabic": "هندسة الرؤية المتعددة",
    "Chinese": "多视图几何",
    "French": "géométrie multi-vues",
    "Japanese": "多視点幾何学",
    "Russian": "многовидовая геометрия"
  },
  {
    "English": "multi-view learning",
    "context": "1: However, capturing relatively high-level semantic content, which is DNNfriendly, is increasingly proving to be necessary, especially in complex multi-label classification tasks (Wen et al. 2020); for another, the performance of traditional <mark>multi-view learning</mark> models is heavily dependent on the parameter settings, and usually requires searching for optimal parameter combinations for different datasets.<br>2: From the perspective of views, each view inherently enjoys a unique description of the objectives that means the complementarity of view-level information should be exploited to learn a comprehensive sample representation. Indeed, obtaining a view-federated representation is necessary for masses of <mark>multi-view learning</mark> methods.<br>",
    "Arabic": "التعلم متعدد المشاهدات",
    "Chinese": "多视图学习",
    "French": "apprentissage multi-vues",
    "Japanese": "マルチビューラーニング",
    "Russian": "многовидовое обучение"
  },
  {
    "English": "multi-view stereo",
    "context": "1: We train our model on our new MannequinChallenge dataset-a collection of Internet videos of people imitating mannequins, i.e., freezing in diverse, natural poses, while a camera tours the scene (left). Because people are stationary, geometric constraints hold; this allows us to use <mark>multi-view stereo</mark> to estimate depth which serves as supervision during training. 2<br>2: [75] render novel views through explicit warping using depth maps obtained via single-view depth and <mark>multi-view stereo</mark>. However, this method fails to model complex scene geometry and to fill in realistic and consistent content at disocclusions. With advances in neural rendering, NeRF-based dynamic view synthesis methods have shown state-of-theart results [16,35,53,66,74].<br>",
    "Arabic": "ستيريو متعدد الرؤية",
    "Chinese": "多视图立体",
    "French": "stéréo multi-vues",
    "Japanese": "複数視点ステレオ",
    "Russian": "многопросмотровое стерео"
  },
  {
    "English": "multi-view system",
    "context": "1: 2020;Gu et al. 2019;Xu and Tao 2020) separate the single MVS pipeline into multiple stages, achieving impressive performances. Unsupervised MVS: Under the assumption of photometric consistency (Godard, Mac Aodha, and Brostow 2017), unsupervised learning has been developed in <mark>multi-view systems</mark>. (Khot et al.<br>",
    "Arabic": "نظام متعدد المشاهد",
    "Chinese": "多视图系统",
    "French": "système multi-vues",
    "Japanese": "多視点システム",
    "Russian": "многозрительная система"
  },
  {
    "English": "multiclass classifier",
    "context": "1: A <mark>multiclass classifier</mark> becomes a binary classifier that predict ±1 on the mislabel triple (x, y, l) depending on whether the prediction on x matches label l; therefore error on the transformed binary data set is low whenever the multiclass accuracy is high. The details of the transformation are provided in Figure 12.<br>2: By treating unobserved labels as latent variables, our approach also connects to prior work on learning from partial or incomplete labels [19,7,5]. Our model is a generalized <mark>multiclass classifier</mark>. It is designed to run efficiently and can thus be adapted to work with techniques developed for large-scale classification involving many labels and large datasets [32,29]. Finally , by modeling the label relations and ensuring consistency between visual predictions and semantic relations , our approach relates to work in transfer learning [ 31,30 ] , zero-shot learning [ 28,12,25 ] , and attribute-based recognition [ 1,36,33,14 ] , especially those that use semantic knowledge to improve recognition [ 16,30 ] and those that propagate or borrow annotations between categories [<br>",
    "Arabic": "مصنف متعدد الفئات",
    "Chinese": "多类分类器",
    "French": "classificateur multiclasse",
    "Japanese": "多クラス分類器",
    "Russian": "многоклассовый классификатор"
  },
  {
    "English": "multiclass hinge loss",
    "context": "1: We instead opt for <mark>multiclass hinge loss</mark> (Weston and Watkins, 1999;Dogan et al., 2016) and minimize: \n y ∈Yx max 0; 1− logq p (y|x)+ logq p (y |x) (5) \n<br>",
    "Arabic": "خسارة الفجوة متعددة الفئات",
    "Chinese": "多类别铰链损失",
    "French": "perte de charnière multiclasse",
    "Japanese": "多クラスヒンジロス",
    "Russian": "многоклассовая функция потерь шарнира"
  },
  {
    "English": "multiclass model",
    "context": "1: We experiment with three types of probes per task. For part-of-speech tagging, we experiment with linear, MLP-1, and MLP-2 probes. The linear probe is a <mark>multiclass model</mark> mapping h i to y i ∼ softmax(Ah i + b). The MLP-1 probe is a multilayer perceptron with one hidden layer and ReLU nonlinearity defined as: \n<br>",
    "Arabic": "نموذج متعدد الفئات",
    "Chinese": "多类模型",
    "French": "modèle multiclasse",
    "Japanese": "マルチクラスモデル",
    "Russian": "многоклассовая модель"
  },
  {
    "English": "multiclass object detection",
    "context": "1: Current detectors are not perfect so decoding is a necessary part of every <mark>multiclass object detection</mark> method. One natural decoding strategy, which outperforms NMS, is to model the interaction between objects by having pairwise terms in the scoring function [2]. This approach often yields intractable inferences and one needs to greedily search the space of labels.<br>",
    "Arabic": "الكشف عن الكائنات متعددة الفئات",
    "Chinese": "多类目标检测",
    "French": "détection d'objets multiclasses",
    "Japanese": "マルチクラス物体検出",
    "Russian": "многоклассовое обнаружение объектов"
  },
  {
    "English": "multidimensional quality metric",
    "context": "1: on the <mark>Multidimensional Quality Metrics</mark> ( MQM ) ontology . MQM ontology consists of a hierarchy of errors and translations are penalised based on the severity of errors in this hierarchy. These human evaluations are then used as training data for building new MT metrics.<br>2: Metric evaluation typically includes a correlation of the scores with human judgements collected for the respective translation outputs. But, designing such guidelines is challenging (Mathur et al., 2020a), leading to the development of several different methodologies and analyses over the years. The human evaluation protocols include general guidelines for fluency , adequacy and/or comprehensibility ( White et al. , 1994 ) on continuous scales ( Koehn and Monz , 2006 ; Graham et al. , 2013 ) ( direct assessments ) or fine-grained annotations of MT errors ( Freitag et al. , 2021a , b ) based on error ontology like <mark>Multidimensional Quality Metrics</mark> (<br>",
    "Arabic": "مقياس جودة متعدد الأبعاد",
    "Chinese": "多维质量度量",
    "French": "métrique de qualité multidimensionnelle",
    "Japanese": "多次元品質メトリック",
    "Russian": "многомерная метрика качества"
  },
  {
    "English": "multidimensional scaling",
    "context": "1: be shown on a scatterplot . This visualization form is simple, and widely applicable across various domains. One pioneering technique is <mark>Multidimensional Scaling</mark> (MDS) (Kruskal 1964). The goal is to preserve the distances in the high-dimensional space in the low-dimensional embedding.<br>",
    "Arabic": "التحجيم متعدد الأبعاد",
    "Chinese": "多维尺度分析",
    "French": "échelle multidimensionnelle",
    "Japanese": "多次元尺度構成法",
    "Russian": "многомерное шкалирование"
  },
  {
    "English": "multilingual embedding",
    "context": "1: This would allow for a comparison with the hypothesis in the target language, similar to reference-based metrics, circumventing alignment problems in <mark>multilingual embeddings</mark>. This approach updates UScore wrd to \n ( , , ′ ) = xlng WMD ( ) ( , ) + lm LM( ) + pseudo WMD( , ′ ),(3) \n<br>",
    "Arabic": "تضمين متعدد اللغات",
    "Chinese": "多语种嵌入",
    "French": "intégration multilingue",
    "Japanese": "多言語埋め込み",
    "Russian": "многоязычное встраивание"
  },
  {
    "English": "multilingual language model",
    "context": "1: This primarily happens in dialog tasks like dialog state tracking (DST) or natural language response generation (NLG) with language-sensitive outputs. Another line of approaches instead investigates cross-lingual transfer directly in pretrained <mark>multilingual language models</mark> (Tang et al., 2021;Gritta et al., 2022).<br>",
    "Arabic": "نموذج لغة متعدد اللغات",
    "Chinese": "多语种语言模型",
    "French": "modèle de langage multilingue",
    "Japanese": "多言語言語モデル",
    "Russian": "многоязычная языковая модель"
  },
  {
    "English": "multilingual model",
    "context": "1: (2021) suggested learning a binary mask for every model parameter and every language pair, both requiring further training after the base <mark>multilingual model</mark> converges. Li and Gong (2021) used per language gradients geometry to rescale gradients of different language pair to improve performance on low resource languages.<br>2: We make code, data and trained models available to foster research by the community on how to include hundreds of languages that are currently ill-served by NLP technology. Contributions. (i) We train the <mark>multilingual model</mark> Glot500-m on a 600GB corpus, covering more than 500 diverse languages, and make it publicly available at https://github.com/cisnlp/ Glot500.<br>",
    "Arabic": "نموذج متعدد اللغات",
    "Chinese": "多语言模型",
    "French": "modèle multilingue",
    "Japanese": "多言語モデル",
    "Russian": "многоязычная модель"
  },
  {
    "English": "multilingual representation",
    "context": "1: For assessing the quality of <mark>multilingual representations</mark> for a broad range of tail languages without human gold data, we adopt roundtrip evaluation (Dufter et al., 2018). We first word-align sentences' in a parallel corpus based on the <mark>multilingual representations</mark> of an LLM.<br>",
    "Arabic": "تمثيل متعدد اللغات",
    "Chinese": "多语言表征",
    "French": "représentation multilingue",
    "Japanese": "多言語表現",
    "Russian": "многоязычное представление"
  },
  {
    "English": "multilingual training",
    "context": "1: To answer the question of whether stronger models can compensate for the lack of high-quality data, we moved beyond simple bilingual models and introduced two modelling improvements: <mark>multilingual training</mark> of closely related low-and highresource languages, and backtranslation. We found that models trained with the additional high-quality data performed consistently better.<br>2: (2020a) introduce the transfer-interference trade-off where low resource languages benefit from <mark>multilingual training</mark>, up to a point where the overall performance on monolingual and cross-lingual benchmarks degrades.<br>",
    "Arabic": "تدريب متعدد اللغات",
    "Chinese": "多语言训练",
    "French": "entraînement multilingue",
    "Japanese": "多言語トレーニング",
    "Russian": "многоязычное обучение"
  },
  {
    "English": "multilinguality",
    "context": "1: First, XLM-R may be undertrained, and the inclusion of more head language training data may improve their representations. Second, having more languages may improve <mark>multilinguality</mark> by allowing languages to synergize and enhance each other's representations and cross-lingual transfer.<br>",
    "Arabic": "تعدد اللغات",
    "Chinese": "多语性",
    "French": "multilinguisme",
    "Japanese": "多言語性",
    "Russian": "многоязычие"
  },
  {
    "English": "multimodal task",
    "context": "1: Our work is the first to explore instruction tuning on <mark>multimodal tasks</mark> and shows improved performance compared to baseline methods. However, there is still room for improvement, specifically in utilizing text-only instruction datasets. Future research could explore alternative architectures and stronger vision-language pre-trained models, or develop additional training loss functions to better utilize these unimodal instruction datasets.<br>",
    "Arabic": "مهمة متعددة الوسائط",
    "Chinese": "多模态任务",
    "French": "tâche multimodale",
    "Japanese": "マルチモーダルタスク",
    "Russian": "многомодальная задача"
  },
  {
    "English": "multinomial distribution",
    "context": "1: R(dt|di) = −KL(θ dt , θ d i ) (4) = − ¢ w i P (wi|θ dt )log( P (wi|θ d i ) P (wi|θ dt ) (5) \n where θ d is the language model for document d, and is a <mark>multinomial distribution</mark>.<br>2: In fact, the topic labeling method can be applied to any mining problems where a <mark>multinomial distribution</mark> of words can be estimated, such as term clustering, annotation of frequent patterns in text.<br>",
    "Arabic": "التوزيع العديدي",
    "Chinese": "多项分布",
    "French": "distribution multinomiale",
    "Japanese": "多項分布",
    "Russian": "многономиальное распределение"
  },
  {
    "English": "multinomial model",
    "context": "1: The first difference is that we treat each dimension as a separate distribution while other approaches put all dimensions together and create a <mark>multinomial model</mark>. The second difference is that our quality evaluation function (see Section 3.4) is different from the mutual information loss function proposed by Dhillon et al. [9].<br>",
    "Arabic": "نموذج متعدد الحدود",
    "Chinese": "多项分布模型",
    "French": "modèle multinomial",
    "Japanese": "多項モデル",
    "Russian": "мультиномиальная модель"
  },
  {
    "English": "multiple Choice",
    "context": "1: the discontinuous <mark>Multiple Choice</mark> Grade ; metric choice can be used to induce emergent abilities in a novel domain ( vision ) in diverse architectures and tasks . Caballero et al.<br>",
    "Arabic": "اختيار متعدد",
    "Chinese": "多选题",
    "French": "choix multiple",
    "Japanese": "複数選択",
    "Russian": "множественный выбор"
  },
  {
    "English": "multiple kernel learning",
    "context": "1: kernel is written as a positively weighted combination of other kernels ) . In our case however , we never explicitly optimize the kernel matrix because this part of the problem can be solved explicitly , which means that the complexity of our method is substantially lower than that of classical kernel learning methods and closer in spirit to the algorithm used in [ 8 ] , who formulate the <mark>multiple kernel learning</mark> problem of [ 7<br>",
    "Arabic": "تعلم الأنوية المتعددة",
    "Chinese": "多核学习",
    "French": "apprentissage de noyaux multiples",
    "Japanese": "多核学習",
    "Russian": "множественное обучение ядер"
  },
  {
    "English": "multiple linear regression",
    "context": "1: We believe that the consistently strong performance of our method across settings, combined with its computational speed, make it attractive for practitioners looking to apply large-scale <mark>multiple linear regression</mark> to real problems. Our method takes an empirical Bayes ( EB ) approach ( Robbins , 1964 ; Efron , 2019 ; Hartley and Rao , 1967 ; Carlin and Louis , 2000 ; Stephens , 2016 ; Casella , 2001 ) to multiple regression ; that is , it assigns a prior to the coefficients in the regression method , and this prior is learned from<br>2: Section 5 gives results from numerical studies comparing prediction performance of different methods for <mark>multiple linear regression</mark>, including our VEB approach. Section 6 summarizes the contributions of this work and discusses future directions.<br>",
    "Arabic": "الانحدار الخطي المتعدد",
    "Chinese": "多元线性回归",
    "French": "régression linéaire multiple",
    "Japanese": "多重線形回帰 (たじゅうせんけいかいき)",
    "Russian": "множественная линейная регрессия"
  },
  {
    "English": "multiscale modeling",
    "context": "1: This result supports a contribution of <mark>multiscale modeling</mark>, even if the segmentation behavior at higher layers does not clearly correspond to a theory-driven level of representation (see section 4.3). absence of these characteristics. Memory and prediction therefore modulate not only absolute performance, but also the utility of language experience.<br>",
    "Arabic": "النمذجة متعددة المقاييس",
    "Chinese": "多尺度建模",
    "French": "modélisation multi-échelle",
    "Japanese": "多重スケールモデリング",
    "Russian": "многошкальное моделирование"
  },
  {
    "English": "multiset",
    "context": "1: In principle, an alternative is to move to some form of \"soft\" labeling, with the <mark>multiset</mark> of labels resulting in a probabilistic label for an example [25].<br>2: Given an edge parameter γ ∈ (0, 1 2 ), there is a set of hypotheses, H, such that given any training set (possibly, a <mark>multiset</mark>) of examples U , there is some hypothesis h ∈ H with error at most 1 2 − γ, i.e.<br>",
    "Arabic": "مجموعة متعددة",
    "Chinese": "多重集",
    "French": "multiensemble",
    "Japanese": "多重集合",
    "Russian": "мультимножество"
  },
  {
    "English": "multitask training",
    "context": "1: also proposed a <mark>multitask training</mark> scheme in which a model is trained on four factuality datasets: FactBank (Saurí and Pustejovsky, 2009), UW (Lee et al., 2015), MEAN-TIME (Minard et al., 2016) and UDS , all with annotations on a [−3, 3] scale.<br>2: Indeed, we find that calibrating the distribution of language pairs via temperature can substantially reduce the amount of interference in both high-and lowresource language pairs. Our results demonstrate the importance of tuning the temperature hyperparameter in <mark>multitask training</mark>, and suggest that previously reported accounts of severe interference in multilingual translation models might stem from suboptimal hyperparameter configurations.<br>",
    "Arabic": "تدريب متعدد المهام",
    "Chinese": "多任务训练",
    "French": "entraînement multitâche",
    "Japanese": "複数タスク学習",
    "Russian": "многозадачное обучение"
  },
  {
    "English": "multivariate",
    "context": "1: In representation learning it is often assumed that real-world observations x (e.g., images or videos) are generated by a two-step generative process. First, a <mark>multivariate</mark> latent random variable z is sampled from a distribution P (z).<br>2: However, as the number of pre-sources d z becomes large, <mark>multivariate</mark> centrallimit-theorem arguments can be used to explicitly show that the distribution of S converges to an identical Gaussian prior as ARD.<br>",
    "Arabic": "متعدد المتغيرات",
    "Chinese": "多元变量",
    "French": "multivarié",
    "Japanese": "多変量",
    "Russian": "многомерный"
  },
  {
    "English": "multivariate Gaussian",
    "context": "1: The idea of the GP is that the joint distribution over a discrete set of function values is a <mark>multivariate Gaussian</mark> determined by the corresponding inputs. The covariance matrix and mean vector that parameterize this Gaussian distribution arise from a positive definite covariance function C ( • , • ) : R D × R D → R and a mean function m ( • ) : R D → R. Typically , the covariance function is chosen so that points near to each other in the input space have<br>2: We ran four sets of experiments with each method, as with the toy data, with 1, 2, 4, and 8 labeled data in each class. We used a <mark>multivariate Gaussian</mark> for the Archipelago base density. The results are given in Table 1.<br>",
    "Arabic": "توزيع غوسي متعدد المتغيرات",
    "Chinese": "多元高斯",
    "French": "gaussienne multivariée",
    "Japanese": "多変量ガウス分布",
    "Russian": "многомерный гауссовский"
  },
  {
    "English": "multivariate gaussian distribution",
    "context": "1: Radial basis layer is composed of g radial basis neurons that calculate y i =rad(||C i1 -x||/H i ), i = 1, … , g. We used the model of <mark>multivariate Gaussian distribution</mark> as a transfer function for radial basis neuron rad.<br>2: We investigate whether the considered unsupervised disentanglement approaches are effective at enforcing a factorizing and thus uncorrelated aggregated posterior. For each trained model, we sample 10 000 images and compute a sample from the corresponding approximate posterior. We then fit a <mark>multivariate Gaussian distribution</mark> over these 10 000 samples by computing the empirical mean and covariance matrix.<br>",
    "Arabic": "التوزيع الغاوسي متعدد المتغيرات",
    "Chinese": "多元高斯分布",
    "French": "distribution gaussienne multivariée",
    "Japanese": "多変量ガウス分布",
    "Russian": "многомерное нормальное распределение"
  },
  {
    "English": "multivariate normal",
    "context": "1: For instance, the first non-vacuous PAC-Bayes bounds for overparameterized neural networks (Dziugaite and Roy, 2017) were computed by choosing the prior distribution to be a <mark>multivariate normal</mark> in order to obtain the KL divergence in closed form. Furthermore, in practice we often approximate the marginal likelihood. Indeed, an approximation is unavoidable for Bayesian neural networks.<br>2: A Gaussian process is a random function f : X → R such that, for any finite set of locations X * ⊆ X , the random vector f * = f (X * ) follows a Gaussian distribution. In particular , if f ∼ GP ( µ , k ) , then f * ∼ N ( µ * , K * , * ) is <mark>multivariate normal</mark> with covariance K * , * = k ( X * , X * ) specified by a kernel k. Henceforth , we assume a zeromean prior µ ( • ) = 0 and<br>",
    "Arabic": "توزيع طبيعي متعدد المتغيرات",
    "Chinese": "多元正态分布",
    "French": "normale multivariée",
    "Japanese": "多変量正規分布",
    "Russian": "многомерное нормальное распределение"
  },
  {
    "English": "multivariate normal distribution",
    "context": "1: Following [22], b, c are independently drawn from the standard <mark>multivariate normal distribution</mark>, and entries of B ∈ R 50×40 are drawn with equal probability from {±1}, in each trial.<br>2: We write sets and families in calligraphic font, e.g., G. We use N (x; µ, Σ) to denote the probability density of the <mark>multivariate normal distribution</mark> at x ∈ R n with mean µ ∈ R n and n × n covariance matrix Σ. We use I n to denote the n × n identity matrix.<br>",
    "Arabic": "التوزيع الطبيعي المتعدد المتغيرات",
    "Chinese": "多元正态分布",
    "French": "distribution normale multivariée",
    "Japanese": "多変量正規分布",
    "Russian": "многомерное нормальное распределение"
  },
  {
    "English": "multivariate time series",
    "context": "1: This datasets consists of <mark>multivariate time series</mark> of dimension d = 12, collected (at a rate of 50Hz) by accelerometer and gyroscope sensors on a mobile phone while a person performs various activities, such as \"jogging\", \"walking\", \"sitting\", and so on.<br>2: and mining the time series . In our work, we seek to design representations for <mark>multivariate time series</mark> data (the works surveyed here focus on a single series) and also to enjoy the benefits of symbolization of series for ease and flexibility of analysis, as described below.<br>",
    "Arabic": "السلاسل الزمنية متعددة المتغيرات",
    "Chinese": "多元时间序列",
    "French": "série temporelle multivariée",
    "Japanese": "多変量時系列",
    "Russian": "многомерный временной ряд"
  },
  {
    "English": "mutex",
    "context": "1: In this paper we are in the case where exactly one context is active per <mark>mutex</mark> set, which is what happens when searching with multiple pattern databases, where each pattern database provides a single pattern for a given node in the tree. When designing contexts, it is often more natural to directly design <mark>mutex</mark> sets.<br>2: In practice, usually a <mark>mutex</mark> set can be implemented as a hashtable as for pattern databases: the active context is read from the current state of the environment, and the corresponding predictor is retrieved from the hashtable.<br>",
    "Arabic": "مشبك (mutex)",
    "Chinese": "互斥集",
    "French": "verrou",
    "Japanese": "相互排他ロック",
    "Russian": "взаимоисключение"
  },
  {
    "English": "mutexe",
    "context": "1: Bonet and van den Briel further suggest the use of <mark>mutexes</mark> and note a resemblance of the corresponding abstraction heuristics to constrained pattern databases (Haslum, Bonet, and Geffner 2005). We conjecture that partial merges correspond to further abstractions of these projections.<br>",
    "Arabic": "منافرات",
    "Chinese": "\"互斥量\"",
    "French": "verrou mutexe",
    "Japanese": "相互排他ロック",
    "Russian": "мьютексы"
  },
  {
    "English": "mutual Information",
    "context": "1: Since these patterns are not overlapping with each other, we do not use microclustering to preprocess the context units. We compare the ranking of GO terms either as context indicators or as SSPs. We also compare the use of <mark>Mutual Information</mark> and co-occurrence as strength weight for context units.<br>2: <mark>Mutual Information</mark> Gap. Chen et al. ( 2018) argue that the BetaVAE metric and the FactorVAE metric are neither general nor unbiased as they depend on some hyperparameters. They compute the mutual information between each ground truth factor and each dimension in the computed representation r(x).<br>",
    "Arabic": "المعلومات المتبادلة",
    "Chinese": "互信息",
    "French": "Information mutuelle",
    "Japanese": "相互情報量",
    "Russian": "Взаимная информация"
  },
  {
    "English": "mutual entropy",
    "context": "1: A natural way to quantify the information cost is to use the <mark>mutual entropy</mark>, because it is the measure of the mutual dependence of two distributions. For multi-task learning, we extend the <mark>mutual entropy</mark> to multiple distributions as follows \n<br>",
    "Arabic": "الانتروبيا المتبادلة",
    "Chinese": "互信息",
    "French": "entropie mutuelle",
    "Japanese": "相互エントロピー",
    "Russian": "взаимная энтропия"
  },
  {
    "English": "n-good list",
    "context": "1: Typically, this method first generates a list of top-n candidates from a baseline system, and then reranks this <mark>n-best list</mark> with arbitrary features that are not computable or intractable to compute within the baseline system.<br>2: We train a trigram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1996) from the training dataset using the SRILM toolkit (Stolcke, 2002), and use the same language model for all three systems. We use an <mark>n-best list</mark> of size 100 for all three systems when performing MERT.<br>",
    "Arabic": "قائمة أفضل n",
    "Chinese": "n-best列表",
    "French": "liste des n-meilleures hypothèses",
    "Japanese": "n-best候補リスト",
    "Russian": "список n-лучших"
  },
  {
    "English": "n-gram",
    "context": "1: This method is based on an unsupervised representation called <mark>N-gram</mark> graph which first embeds the vertices in the molecule graph and then assembles the vertex embeddings in short walks in the graph. This representation is combined with the XGBoost learning method [Chen and Guestrin, 2016]. 8<br>2: We define a 'dirty' example as one with any N -gram overlap with any training document, and a 'clean' example as one with no collision. Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books.<br>",
    "Arabic": "N-gram",
    "Chinese": "N元语法",
    "French": "n-gramme",
    "Japanese": "Nグラム",
    "Russian": "n-грамма"
  },
  {
    "English": "n-gram feature",
    "context": "1: In contrast to the other strategies just discussed, our text categorization approach to deception detection allows us to model both content and context with <mark>n-gram features</mark>.<br>2: Of particular interest are power relationships, which can be induced from <mark>n-gram features</mark> (Bramsen et al., 2011;Prabhakaran et al., 2012) and from coordination, where one participant's linguistic style is asymmetrically affected by the other (Danescu-Niculescu-Mizil et al., 2012). Danescu-Niculescu-Mizil et al.<br>",
    "Arabic": "ميزة ن-جرام",
    "Chinese": "n-gram特征",
    "French": "\"caractéristique n-gramme\"",
    "Japanese": "n-gramフィーチャ",
    "Russian": "Признак n-граммы"
  },
  {
    "English": "n-gram language model",
    "context": "1: As noted above, corpus-based NLG aims at learning generation decisions from data with minimal dependence on rules and heuristics. A pioneer in this direction is the class-based <mark>n-gram language model</mark> (LM) approach proposed by Oh and Rudnicky (2000).<br>2: Knight and Al-Onaizan (1998) show how to build an <mark>n-gram language model</mark> by a weighted finite state machine. The states of the transducer are (n − 1)gram history, the edges are words from the language. The arc s i coming from state (s i−n , . . .<br>",
    "Arabic": "نموذج لغة ن-جرام",
    "Chinese": "n-元语言模型",
    "French": "modèle de langage n-gramme",
    "Japanese": "n-gram言語モデル",
    "Russian": "n-грамная языковая модель"
  },
  {
    "English": "n-gram model",
    "context": "1: This technique, however, cannot be applied to detect temporal events such as smiling and frowning, which must and can be detected and recognized independently of the background. Brown et al. [1] used the <mark>n-gram model</mark> for predictive typing, i.e., predicting the next word from previous words.<br>2: We also observe that there are no additional gains for Chinese-English translation when using a higher <mark>n-gram model</mark>. Our Gibbs sampler has the advantage that the samples are guaranteed to converge eventually to the model's posterior distributions, but in each step the modification to the current hypothesis is small and local.<br>",
    "Arabic": "نموذج ن-جرام",
    "Chinese": "n-gram模型",
    "French": "modèle de n-grammes",
    "Japanese": "n-gramモデル",
    "Russian": "модель n-грамм"
  },
  {
    "English": "n-step return",
    "context": "1: t , and the <mark>n-step returns</mark> G (n) t are calculated with bootstrapped values estimated by a target network (Mnih et al. 2015) with parameters copied periodically from θ c .<br>",
    "Arabic": "عوائد n-خطوة",
    "Chinese": "n步回报",
    "French": "retour en n étapes",
    "Japanese": "nステップリターン",
    "Russian": "n-шаговый возврат"
  },
  {
    "English": "naive Bayes model",
    "context": "1: This model is in fact a <mark>naive Bayes model</mark>, where the parameters δ and φ are empirically estimated (a value of 2 is used for δ in our experiments, based on tuning on a development set). A similar global consistency model was shown to be effective in Rush et al.<br>",
    "Arabic": "\"نموذج بايز السَّاذج\"",
    "Chinese": "朴素贝叶斯模型",
    "French": "modèle de Bayes naïf",
    "Japanese": "単純ベイズモデル",
    "Russian": "наивная байесовская модель"
  },
  {
    "English": "name entity",
    "context": "1: A Tabular Data in CoNLL-2003 We found a significant amount of documents in the CoNLL-2003 test set that list the outcomes of various sports events, which contributes to the larger proportion of <mark>named entities</mark> in Table 1. These documents appear as though they may have been intended for display on news tickers. 9 We present an example below.<br>2: Figure 1, showing a detailed error analysis, confirms that indeed BERT has more often spurious outcomes than humans, for both entity types. Also, humans miss to annotate ground-truth entities more often than BERT. We can equally observe that BERT is highly superior in identifying correct <mark>named entities</mark>.<br>",
    "Arabic": "كيانات مسماة",
    "Chinese": "命名实体",
    "French": "entité nommée",
    "Japanese": "固有名詞",
    "Russian": "именованная сущность"
  },
  {
    "English": "name entity recognition",
    "context": "1: (2020) classify the state of NLP for Kinyarwanda as \"Scraping-By\", meaning it has been mostly excluded from previous NLP research, and require the creation of dedicated resources and models. Kinyarwanda has been studied mostly in descriptive linguistics (Kimenyi, 1976(Kimenyi, , 1978a(Kimenyi, ,b, 1988Jerro, 2016). Few recent NLP works on Kinyarwanda include Morphological Analysis ( Muhirwe , 2009 ; Nzeyimana , 2020 ) , Text Classification ( Niyongabo et al. , 2020 , <mark>Named Entity Recognition</mark> ( Rijhwani et al. , 2020 ; Adelani et al. , 2021 ; Sälevä and Lignos , 2021 ) , POS tagging ( Garrette and Baldridge , 2013 ; Duong et al. ,<br>2: In one text-to-text application, Iovine et al. (2022b) use a similar unsupervised methodology to perform bidirectional text transformations for converting keyword search queries to natural language questions, and vice versa. It has also been used for <mark>Named Entity Recognition</mark> in the absence of large annotated text (Iovine et al., 2022a).<br>",
    "Arabic": "التعرف على الكيانات المسماة",
    "Chinese": "命名实体识别",
    "French": "reconnaissance d'entités nommées",
    "Japanese": "固有表現認識",
    "Russian": "распознавание именованных сущностей"
  },
  {
    "English": "name entity recognizer",
    "context": "1: As a simple proxy for faithfulness, we choose to measure the token overlap between named entities appearing in the generation and the target article/evidence, where entities are identified using the <mark>named entity recognizer</mark> used by Guu et al. (2020) to perform salient span masking. We specifically introduce the following measurements: 1. Unsupported Entity Tokens.<br>2: We implement a greedy left-to-right <mark>named entity recognizer</mark> based on Ratinov and Roth (2009) using a total of 46 feature templates, including surface features such as lemma and capitalization, gazetteer look-ups, and each token's extended prediction history, as described in (Ratinov and Roth, 2009).<br>",
    "Arabic": "معرف الكيانات المسماة",
    "Chinese": "命名实体识别器",
    "French": "reconnaisseur d'entités nommées",
    "Japanese": "固有名詞認識器",
    "Russian": "распознаватель именованных сущностей"
  },
  {
    "English": "natural image statistic",
    "context": "1: While it is possible that blind deconvolution can benefit from future research on <mark>natural image statistics</mark>, this paper suggests that better estimators for existing priors may have more impact on future blind deconvolution algorithms. Additionally, we observed that the popular spatially uniform blur assumption is usually unrealistic.<br>2: The most relevant previous work is primarily in two areas: view-dependent geometry, and <mark>natural image statistics</mark>. Irani et al. (2002) expressed new view generation as the estimation of the colour at each generated pixel.<br>",
    "Arabic": "إحصائيات الصورة الطبيعية",
    "Chinese": "自然图像统计",
    "French": "statistiques d'images naturelles",
    "Japanese": "自然画像統計",
    "Russian": "статистика естественных изображений"
  },
  {
    "English": "natural language",
    "context": "1: A high-level view of the PLOW agent architecture is shown in Figure 3. The understanding components combine <mark>natural language</mark> (speech or keyboard) with the observed user actions on the GUI. After full parsing, semantic interpretation and discourse interpretation produce plausible intended actions.<br>2: In Lu et al. (2008), a generative model was presented to model the process that jointly generates both <mark>natural language</mark> sentences and their underlying meaning representations of a variable-free treestructured form. The model was defined over a hybrid tree, which consists of meaning representation tokens as internal nodes and <mark>natural language</mark> words as leaves.<br>",
    "Arabic": "اللغة الطبيعية",
    "Chinese": "自然语言",
    "French": "langage naturel",
    "Japanese": "自然言語",
    "Russian": "естественный язык"
  },
  {
    "English": "natural language generation",
    "context": "1: Summarization is the process of identifying the most important information from a source to produce a comprehensive output for a particular user and task (Mani, 1999). While producing readable outputs is a problem shared with the field of <mark>Natural Language Generation</mark>, the core challenge of summarization is the identification and selection of important information.<br>",
    "Arabic": "توليد اللغة الطبيعية",
    "Chinese": "自然语言生成",
    "French": "génération de langage naturel",
    "Japanese": "自然言語生成",
    "Russian": "генерация естественного языка"
  },
  {
    "English": "natural language inference",
    "context": "1: The HANS dataset is a commonly used challenging dataset for examining spurious correlations on the <mark>Natural Language Inference</mark> (NLI) task. It annotates a heuristic subcase (e.g., \"ce_adverb\") for each example. Based on the annotated heuristic subcases, we first construct six paired heuristic subsets where the examples display the same heuristic type.<br>2: The Stanford <mark>Natural Language Inference</mark> (SNLI) corpus (Bowman et al., 2015) provides approximately 550K hypothesis/premise pairs. Our baseline, the ESIM sequence model from Chen et al.<br>",
    "Arabic": "الاستدلال اللغوي الطبيعي",
    "Chinese": "自然语言推理",
    "French": "inférence en langage naturel",
    "Japanese": "自然言語推論",
    "Russian": "естественное языковое умозаключение"
  },
  {
    "English": "natural language processing",
    "context": "1: Keyphrase identification and classification is a <mark>Natural Language Processing</mark> and Information Retrieval task that involves extracting relevant groups of words from a given text related to the main topic. In this work, we focus on extracting keyphrases from scientific documents.<br>2: CV, NLP, Methods 6 (three dummy variables indicating whether the task belongs to the Computer Vision, <mark>Natural Language Processing</mark>, or Methodology categories in PWC). To absorb additional variation, we also included the following control covariates: \n 1. Task size in number of dataset-using/introducing papers for that task in that year 2.<br>",
    "Arabic": "معالجة اللغة الطبيعية",
    "Chinese": "自然语言处理",
    "French": "traitement du langage naturel",
    "Japanese": "自然言語処理",
    "Russian": "обработка естественного языка"
  },
  {
    "English": "natural language query",
    "context": "1: To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 <mark>natural language queries</mark> with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations.<br>",
    "Arabic": "الاستعلام باللغة الطبيعية",
    "Chinese": "自然语言查询",
    "French": "requête en langage naturel",
    "Japanese": "自然言語クエリ",
    "Russian": "естественно-языковой запрос"
  },
  {
    "English": "natural language understanding",
    "context": "1: Recent investigations into the inner-workings of state-of-the-art large-scale pre-trained Transformer-based <mark>Natural Language Understanding</mark> (NLU) models indicate that they appear to know humanlike syntax, at least to some extent.<br>",
    "Arabic": "فهم اللغة الطبيعية",
    "Chinese": "自然语言理解",
    "French": "compréhension du langage naturel",
    "Japanese": "自然言語理解",
    "Russian": "понимание естественного языка"
  },
  {
    "English": "natural logic",
    "context": "1: This work explores a middle way, by developing a computational model of what Lakoff (1970) called <mark>natural logic</mark>, which characterizes valid patterns of inference in terms of syntactic forms re-sembling natural language as much as possible.<br>2: We do not claim <mark>natural logic</mark> to be a universal solution for NLI. Many important types of inference are not amenable to <mark>natural logic</mark> , including paraphrase ( Eve was let go |= Eve lost her job ) , verb alternation ( he drained the oil |= the oil drained ) , relation extraction ( Aho , a trader at UBS , ... |= Aho works for UBS ) , common-sense reasoning ( the sink overflowed |=<br>",
    "Arabic": "المنطق الطبيعي",
    "Chinese": "自然逻辑",
    "French": "logique naturelle",
    "Japanese": "自然論理",
    "Russian": "естественная логика"
  },
  {
    "English": "natural logic inference",
    "context": "1: Ultimately, open-domain NLI is likely to require combining disparate reasoners, and a facility for <mark>natural logic inference</mark> is a good candidate to be a component of such a solution.<br>",
    "Arabic": "الاستدلال المنطقي الطبيعي",
    "Chinese": "自然逻辑推理",
    "French": "inférence logique naturelle",
    "Japanese": "自然論理推論",
    "Russian": "вывод естественной логики"
  },
  {
    "English": "natural parameter",
    "context": "1: Beyond simple context models, since the vector T c (n, a) can depend on the current node n, it can make use in particular of features of the corresponding state of the environment, such as a heuristic distance to the goal. Finally , members of the exponential family in canonical form are well-known to be log-concave in their <mark>natural parameters</mark> β , that is , their log loss is convex in their <mark>natural parameters</mark> : − log p c ( • , • ; β ) is of the form h ( β ) +ln exp g ( β ) where h and g are<br>",
    "Arabic": "ضابط طبيعي",
    "Chinese": "自然参数",
    "French": "paramètre naturel",
    "Japanese": "自然母数",
    "Russian": "естественный параметр"
  },
  {
    "English": "natural question",
    "context": "1: We chose 2018 as a threshold as it is when <mark>Natural Questions</mark> (Kwiatkowski et al., 2019) was collected and roughly matches the timestamp of the most recent data our closed book model (Lewis et al., 2020a) was pretrained on (Feb 2019).<br>2: Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? We use a standard benchmark in the NLP community, <mark>Natural Questions</mark>. Other than that we don't collect data ourselves.<br>",
    "Arabic": "أسئلة طبيعية",
    "Chinese": "自然问题",
    "French": "Natural Questions",
    "Japanese": "自然言語質問 (Natural Questions)",
    "Russian": "Natural Questions"
  },
  {
    "English": "naïve Bayes",
    "context": "1: As the model is communicated back to the information-based agent architecture, the classifier output should include all the possible class labels with an attached probability estimates for each class. Hence, we use probabilistic classifiers (e.g. <mark>Naïve Bayes</mark> , Bayesian Network classifiers [ 7 ] without the min-max selection of the class output [ e.g. , in a classifier based on <mark>Naïve Bayes</mark> algorithm , we calculate the posterior probability P p ( i ) of each class c ( i ) with respect to combinations of key terms and then return the tuples < c ( i ) ,<br>2: factverification features ( Tang et al. , 2023 ) . Classification models involve deep neural networks, such as RoBERTa (Guo et al., 2023), or more traditional algorithms, such as logistic regression, support vector machines, <mark>Naïve Bayes</mark>, and decision trees.<br>",
    "Arabic": "بايز الساذج",
    "Chinese": "朴素贝叶斯",
    "French": "Bayes naïf",
    "Japanese": "素朴ベイズ",
    "Russian": "наивный Байес"
  },
  {
    "English": "near-optimality",
    "context": "1: Two main lines of research are now open, one theoretical and one empirical: first, the must-expand pairs defined by Eckerle et al. (2017) and its derived definitions, like <mark>near-optimality</mark>, must be modified to work with individual bounds.<br>",
    "Arabic": "شبه الأمثلية",
    "Chinese": "近似最优性",
    "French": "quasi-optimalité",
    "Japanese": "最適に近い",
    "Russian": "близость к оптимальности"
  },
  {
    "English": "nearest neighbor classifier",
    "context": "1: This procedure corresponds to allowing the considered points x and y to move along the directions spanned by their associated local charts. Their distance is then evaluated on the new coordinates where the distance is minimal. We can then use a <mark>nearest neighbor classifier</mark> based on this distance.<br>2: One way of achieving this is to use a <mark>nearest neighbor classifier</mark> with a similarity criterion defined as the shortest distance between two hyperplanes (Simard et al., 1993). The tangents extracted on each points will allow us to shrink the distances between two samples when they can approximate each other by a linear combination of their local tangents.<br>",
    "Arabic": "مُصنِّف أقرب جار",
    "Chinese": "最近邻分类器",
    "French": "classificateur du voisin le plus proche",
    "Japanese": "最近傍分類器",
    "Russian": "классификатор ближайшего соседа"
  },
  {
    "English": "nearest neighbor search",
    "context": "1: We then incorporate approximate <mark>nearest neighbor search</mark> to compute k-means in o(nk) (where n is the number of data points; note that computing the cost, given a solution, takes 8(nk) time).<br>2: We use 3D positions of objects to generate the node set and the <mark>nearest neighbor search</mark> to generate the edge set. We use fences and vegetation to construct DFs. The ground-truth loop closure is obtained based on the ground-truth poses provided by the KITTI odometry dataset.<br>",
    "Arabic": "البحث عن أقرب جار",
    "Chinese": "最近邻搜索",
    "French": "recherche du plus proche voisin",
    "Japanese": "最近傍探索",
    "Russian": "поиск ближайшего соседа"
  },
  {
    "English": "nearest-neighbor algorithm",
    "context": "1: For our empirical result, we will use a very simple approximate <mark>nearest-neighbor algorithm</mark> based on random projection. This has reasonable performance in expectation, but is not independent from one step to the next. While the theoretical results from this particular approach are not very strong, it works very well in our experiments.<br>",
    "Arabic": "خوارزمية الجار الأقرب",
    "Chinese": "最近邻算法",
    "French": "algorithme du plus proche voisin",
    "Japanese": "最近傍アルゴリズム",
    "Russian": "алгоритм ближайшего соседа"
  },
  {
    "English": "negation",
    "context": "1: While traditional benchmarks indicate that models on these tasks are as accurate as humans, Check-List reveals a variety of severe bugs, where commercial and research models do not effectively handle basic linguistic phenomena such as <mark>negation</mark>, named entities, coreferences, semantic role labeling, etc, as they pertain to each task.<br>2: Potential future work could complement our results by providing evidence from representational analyses, or by devising causal interventions, similar to those recently explored in the realm of syntactic agreement (Finlayson et al., 2021), or in testing of <mark>negation</mark> and hypernymy in NLI models (Geiger et al., 2020), among others.<br>",
    "Arabic": "نفي",
    "Chinese": "否定",
    "French": "négation",
    "Japanese": "否定",
    "Russian": "отрицание"
  },
  {
    "English": "negative log-likelihood",
    "context": "1: Similarly, the distribution P (z|x) is approximated using a variational distribution Q(z|x), again parametrized using a deep neural network. The model is then trained by minimizing a suitable approximation to the <mark>negative log-likelihood</mark>.<br>2: where the <mark>negative log-likelihood</mark> plays the role of the loss, and the Bayesian posterior p(w|D) replaces q. Eq. ( 16) is a special case of the ELBO in Eq. ( 11) where the posterior p(w|D) takes place of the variational distribution, in which case the ELBO equals the marginal likelihood.<br>",
    "Arabic": "سالب اللوغاريتم الأرجحي",
    "Chinese": "负对数似然",
    "French": "log-vraisemblance négative",
    "Japanese": "負の対数尤度",
    "Russian": "отрицательная логарифмическая правдоподобность"
  },
  {
    "English": "negative pair",
    "context": "1: While most self-supervised learning approaches use positive pairs (x i , x ′ i ) and <mark>negative pairs</mark> {∀j, j ̸ = i, (x i , x j )} {∀j, j ̸ = i, (x i , x ′ j ) \n<br>2: In this paper, we report that simple Siamese networks can work surprisingly well with none of the above strategies for preventing collapsing. Our model directly maximizes the similarity of one image's two views, using neither <mark>negative pairs</mark> nor a momentum encoder. It works with typical batch sizes and does not rely on large-batch training.<br>",
    "Arabic": "زوج سلبي",
    "Chinese": "负样本对",
    "French": "paire négative",
    "Japanese": "ネガティブペア",
    "Russian": "негативная пара"
  },
  {
    "English": "negative sample",
    "context": "1: We draw these samples uniformly at random from across the input batch that is being evaluated. Thus, the <mark>negative samples</mark> can contain samples from the same image at different patch locations, as well as from different images. We found that including the positive sample (i.e.<br>2: 6 All entries are based on a standard ResNet-50, with two 224×224 views used during pre-training. Table 4 shows the results and the main properties of the methods. SimSiam is trained with a batch size of 256, using neither <mark>negative samples</mark> nor a momentum encoder. Despite it simplicity, SimSiam achieves competitive results.<br>",
    "Arabic": "عينة سلبية",
    "Chinese": "负样本",
    "French": "échantillon négatif",
    "Japanese": "負例",
    "Russian": "отрицательная выборка"
  },
  {
    "English": "negative transfer",
    "context": "1: We distinguish four main level of cross-lingual transfer described in Section 4.2 (F(l i → l j )): \n • <mark>negative transfer</mark> F(l i → l j ) < −10 \n • neutral transfer −10 ≤ F(l i → l j ) < 10 \n<br>2: In MTL, the co-training strategy across tasks could leverage feature abstraction; it could effortlessly extend to additional tasks, and save computation cost for onboard chips. However, such a scheme may cause undesirable \"<mark>negative transfer</mark>\" [23,64].<br>",
    "Arabic": "نقل سلبي",
    "Chinese": "负迁移",
    "French": "transfert négatif",
    "Japanese": "負の転移",
    "Russian": "негативный перенос"
  },
  {
    "English": "neighborhood function",
    "context": "1: Since the primal and dual variants of the submodular set cover problem are similar, we just use the primal variants of ISSC and EASSC. Furthermore, in our experiments, we observe that the <mark>neighborhood function</mark> f has a curvature κ f = 1.<br>2: A natural choice of the function f is a function of the form |Γ(X)|, where Γ(X) is the <mark>neighborhood function</mark> on a bipartite graph constructed between the utterances and the words [33]. For the coverage function g, we use two types of coverage: one is a facility location function  \n<br>",
    "Arabic": "دالة الجوار",
    "Chinese": "邻域函数",
    "French": "fonction de voisinage",
    "Japanese": "近傍関数",
    "Russian": "функция окрестности"
  },
  {
    "English": "neighborhood system",
    "context": "1: The smoothness term, on the other hand, involves a single camera at a time. It is defined to be X \n fp;qg2N V p;q ðf p ; f q Þ;ð5Þ \n where N is a <mark>neighborhood system</mark> on pixels in a single camera.<br>2: The smoothness term involves a notion of neighborhood; we assume that there is a <mark>neighborhood system</mark> on pixels \n N ⊂ {{p, q} | p, q ∈ P} \n<br>",
    "Arabic": "نظام الجوار",
    "Chinese": "邻域系统",
    "French": "système de voisinage",
    "Japanese": "近傍システム",
    "Russian": "система окрестностей"
  },
  {
    "English": "net",
    "context": "1: Some additional query-independent features are also used. In all, we use 569 features, many of which are counts. As a preprocessing step we replace the counts by their logs, both to reduce the range, and to allow the <mark>net</mark> to more easily learn multiplicative relationships.<br>",
    "Arabic": "الشبكة",
    "Chinese": "神经网络",
    "French": "réseau",
    "Japanese": "ネット",
    "Russian": "сеть"
  },
  {
    "English": "network",
    "context": "1: We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our <mark>network</mark> to create stochastic variation is to rely on the noise provided.<br>2: Since the <mark>network</mark> tends to overfit when the structure becomes more complex, the dropout technique (Srivastava et al., 2014) is used to regularise the <mark>network</mark>. As suggested in (Zaremba et al., 2014), dropout was only applied to the non-recurrent connections, as shown in the Figure 2.<br>",
    "Arabic": "شبكة",
    "Chinese": "网络",
    "French": "réseau",
    "Japanese": "ネットワーク",
    "Russian": "сеть"
  },
  {
    "English": "network architecture",
    "context": "1: Next, we describe the <mark>network architecture</mark> of HED.<br>2: In addition, SPLATNet allows an easy mapping of 2D information into 3D and vice-versa, resulting in a novel <mark>network architecture</mark> for joint processing of point clouds and multi-view images. Experiments on two different benchmark datasets show that the proposed networks compare favorably against state-of-the-art approaches for segmentation tasks.<br>",
    "Arabic": "معمارية الشبكة",
    "Chinese": "网络架构",
    "French": "architecture de réseau",
    "Japanese": "ネットワークアーキテクチャ",
    "Russian": "архитектура сети"
  },
  {
    "English": "network feature",
    "context": "1: The accuracy of the classifiers increases with the number of days of consecutive transitions, suggesting the local optimality of routine transactions is easier to predict than that of unexpected transactions. We also observe that the <mark>network features</mark> are significantly more predictive than the price changes.<br>",
    "Arabic": "ميزة الشبكة",
    "Chinese": "网络特征",
    "French": "caractéristique de réseau",
    "Japanese": "ネットワーク特徴量",
    "Russian": "сетевая особенность"
  },
  {
    "English": "network parameter",
    "context": "1: 5.Compute total loss L according to (8) and use the optimizer to update the <mark>network parameters</mark> batch to batch. 6.Input test samples and obtain prediction P . if |L last − L| < σ or 1 ntc i,j P i,j ⊕ P last i,j <<br>",
    "Arabic": "معلمات الشبكة",
    "Chinese": "网络参数",
    "French": "paramètre de réseau",
    "Japanese": "ネットワークパラメータ",
    "Russian": "параметры сети"
  },
  {
    "English": "network structure",
    "context": "1: We now present a probabilistic model for linking <mark>network structure</mark> with content exchanged over the network. In this section, the model is presented in general terms, so that it can be applied to any type of event counts, with any form of discrete edge labels.<br>2: Measurements which directly reveal <mark>network structure</mark> are often beyond experimental capabilities or are excessively expensive. This paper addresses the problem of inferring the structure of a network from co-occurrence data: observations which indicate nodes that are activated in each of a set of signaling pathways but do not directly reveal the order of nodes within each pathway.<br>",
    "Arabic": "بنية الشبكة",
    "Chinese": "网络结构",
    "French": "structure du réseau",
    "Japanese": "ネットワーク構造",
    "Russian": "структура сети"
  },
  {
    "English": "network topology",
    "context": "1: For example, a marketer would like to have her advertisement viewed by a million people in one month, rather than in one hundred years. Such time-sensitive requirement renders those algorithms which only consider static information, such as <mark>network topologies</mark>, inappropriate in this context.<br>",
    "Arabic": "التوبولوجيا الشبكية",
    "Chinese": "网络拓扑结构",
    "French": "topologie de réseau",
    "Japanese": "ネットワークトポロジー",
    "Russian": "сетевая топология"
  },
  {
    "English": "network weight",
    "context": "1: Our method requires only 5 MB for the <mark>network weights</mark> (a relative compression of 3000× compared to LLFF), which is even less memory than the input images alone for a single scene from any of our datasets.<br>",
    "Arabic": "وزن الشبكة",
    "Chinese": "网络权重",
    "French": "poids du réseau",
    "Japanese": "ネットワーク重み",
    "Russian": "веса сети"
  },
  {
    "English": "neural activity",
    "context": "1: Both <mark>neural activity</mark> and sensory input from the environment follow well-defined temporal statistical patterns, but the exploitation of these statistics has thus far not been studied as a potential substrate for timing judgements, despite being potentially attractive.<br>2: However, they may be seen as models both for internally-generated neural processes, such as (spontaneous) network activity and local field potentials, and for sensory processes, in the form of externally-driven <mark>neural activity</mark>, or (taking a functional view) in the form of the stimuli themselves.<br>",
    "Arabic": "نشاط عصبي",
    "Chinese": "神经活动",
    "French": "activité neuronale",
    "Japanese": "神経活動",
    "Russian": "нейронная активность"
  },
  {
    "English": "neural approach",
    "context": "1: • Limit our search to papers on gloss translation (as opposed to other MT papers on sign language). • Only consider <mark>neural approaches</mark> to gloss translation, excluding statistical or rule-based works. • Limit to recent works published in the last five years.<br>2: ProFormer also improved upon prior on-device state-of-the-art <mark>neural approaches</mark> like SGNN (Ravi and Kozareva, 2018) and SGNN++  reaching over 35% improvement on long text classification. Similarly it improved over on-device ProSeqo ) models for all datasets and reached comparable performance on MRDA.<br>",
    "Arabic": "نهج عصبي",
    "Chinese": "神经网络方法",
    "French": "approche neuronale",
    "Japanese": "ニューラルアプローチ (neural approach)",
    "Russian": "нейронный подход"
  },
  {
    "English": "neural architecture",
    "context": "1: All models use the Transformer <mark>neural architecture</mark> (Vaswani et al., 2017)   translation task 3 and fine-tuned on ACED, unless indicated otherwise. All pre-training and fine-tuning data is pre-processed by normalizing punctuation with the Moses toolkit (Koehn et al., 2007).<br>2: We also are the first to consider the CLML for <mark>neural architecture</mark> comparison, hyperparameter learning, approximate inference, and transfer learning. We expect the CLML to address the issues we have presented in this section, with the exception of overfitting, since CLML optimization is still fitting to withheld points.<br>",
    "Arabic": "البنية العصبية",
    "Chinese": "神经架构",
    "French": "architecture neuronale",
    "Japanese": "ニューラルアーキテクチャ",
    "Russian": "нейронная архитектура"
  },
  {
    "English": "neural architecture search",
    "context": "1: • In Appendix G, we discuss the difference between the joint and marginal predictive likelihoods and provide an example where CLML deviates from the test likelihood. • In Appendix F, we provide the experimental details for <mark>neural architecture search</mark> as well as additional results for CIFAR-10 and CIFAR-100.<br>2: The trend is to stack more and more convolutional or transformer layers to construct a deep network. Recently, when trying to avoid manual architecture design, researchers started considering developing algorithms to design neural networks automatically. Thus, a new research sub-field of automated machine learning (AutoML)  called <mark>neural architecture search</mark> is established .<br>",
    "Arabic": "البحث عن البنية العصبية",
    "Chinese": "神经架构搜索",
    "French": "Recherche d'architecture neuronale",
    "Japanese": "ニューラルアーキテクチャ探索",
    "Russian": "поиск нейронной архитектуры"
  },
  {
    "English": "neural embedding",
    "context": "1: Note, however, that our proposal is more general and can be applied to any set of word vectors in a post-processing step, including <mark>neural embedding</mark> models that have superseded these traditional count-based models as we in fact do in this paper. Finally, there are others authors that have also pointed limitations in the intrinsic evaluation of word embeddings.<br>2: Moreover, this result implies that the <mark>neural embedding</mark> process is not discovering novel patterns, but rather is doing a remarkable job at preserving the patterns inherent in the wordcontext co-occurrence matrix. A key insight of this work is that the vector arithmetic method can be decomposed into a linear combination of three pairwise similarities (Section 3).<br>",
    "Arabic": "تضمين عصبي",
    "Chinese": "神经嵌入",
    "French": "projection neuronale",
    "Japanese": "神経埋め込み",
    "Russian": "нейронное встраивание"
  },
  {
    "English": "neural generation model",
    "context": "1: We provide benchmark results for popular generation systems as well as EDIT5-a T5-based approach tailored to editing we introduce that establishes the state of the art. Our analysis shows that developing models that can update articles faithfully requires new capabilities for <mark>neural generation models</mark>, and opens doors to many new applications.<br>",
    "Arabic": "نموذج توليد عصبي",
    "Chinese": "神经生成模型",
    "French": "modèle de génération neuronale",
    "Japanese": "ニューラル生成モデル (nyu-ralu seisei moderu)",
    "Russian": "модель нейронной генерации"
  },
  {
    "English": "neural implicit representation",
    "context": "1: However, we believe the scalability of this process can be improved by exploring more efficient alternatives to exhaustive matching, e.g., vocabulary trees or keyframe-based matching, drawing inspiration from the Structure from Motion and SLAM literature. Second, like other methods that utilize <mark>neural implicit representations</mark> [44], our method involves a relatively long optimization process.<br>",
    "Arabic": "نمذجة عصبية ضمنية",
    "Chinese": "神经隐式表示",
    "French": "représentation implicite neuronale",
    "Japanese": "神経内在表現",
    "Russian": "нейронное неявное представление"
  },
  {
    "English": "neural language model",
    "context": "1: This choice is motivated by the relative simplicity of computing this measure-a straightforward auxiliary objective that can be added to any conceivable <mark>neural language model</mark>-as well as two substantive desiderata: First, we would like the measure to capture processing difficulty due to syntactic unpredictability.<br>2: The target pronoun in the sentence is replaced by each answer candidate and the <mark>neural language model</mark> provides the likelihood of the two resulting sentences. This simple yet effective approach outperforms previous IR-based methods. BERT BERT (Devlin et al. 2018) is another pre-trained <mark>neural language model</mark> which has bidirectional paths and consecutive sentence representations in hidden layers.<br>",
    "Arabic": "نموذج لغوي عصبي",
    "Chinese": "神经语言模型",
    "French": "modèle de langage neuronal",
    "Japanese": "ニューラル言語モデル",
    "Russian": "нейронная языковая модель"
  },
  {
    "English": "neural machine translation",
    "context": "1: <mark>Neural Machine Translation</mark> (NMT) generates target words sequentially in the way of predicting the next word conditioned on the context words. At training time, it predicts with the ground truth words as context while at inference it has to generate the entire sequence from scratch. This discrepancy of the fed context leads to error accumulation among the way.<br>2: In our experiments, we improve the performance of the transformer-based models via (1) data augmentation, (2) semi-supervised <mark>Neural Machine Translation</mark> (NMT), (3) transfer learning and (4) multilingual NMT. The proposed methods are implemented progressively on two German SL corpora containing gloss annotations.<br>",
    "Arabic": "ترجمة آلية عصبية",
    "Chinese": "神经机器翻译",
    "French": "traduction automatique neuronale",
    "Japanese": "神経機械翻訳",
    "Russian": "нейронный машинный перевод"
  },
  {
    "English": "neural machinery",
    "context": "1: For these guarantees, learning theoretic approaches usually rely on intractable computations, or avoid such computations by restricting the model or task. Our method draws inspiration from theoretical approaches but eschews (for now) theoretical guarantees in order to use modern <mark>neural machinery</mark>.<br>",
    "Arabic": "آليات عصبية",
    "Chinese": "神经机制",
    "French": "machinerie neuronale",
    "Japanese": "ニューラル機構",
    "Russian": "нейронная аппаратура"
  },
  {
    "English": "neural mapping",
    "context": "1: Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text. Neural Radiance Fields, or NeRF  are an approach towards inverse rendering in which a volumetric raytracer is combined with a <mark>neural mapping</mark> from spatial coordinates to color and volumetric density.<br>",
    "Arabic": "رسم الخرائط العصبية",
    "Chinese": "NeRF",
    "French": "cartographie neuronale",
    "Japanese": "神経マッピング",
    "Russian": "нейронное отображение"
  },
  {
    "English": "neural method",
    "context": "1: It is instructive to look at the past to appreciate this question. Computational linguistics has gone through many fashion cycles over the course of its history. Grammar-and knowledge-based methods gave way to statistical methods, and today most research incorporates <mark>neural methods</mark>.<br>2: The classic approaches are surprisingly competitive with the <mark>neural methods</mark>, with logistic regression even outperforming IndoBERT LARGE and XLM-R on Acehnese (ace), Buginese (bug), and Toba Batak (bbc).<br>",
    "Arabic": "طرق عصبية",
    "Chinese": "神经方法",
    "French": "méthode neuronale",
    "Japanese": "ニューラル手法",
    "Russian": "нейросетевые методы"
  },
  {
    "English": "neural model",
    "context": "1: PROCTHOR enables us to sample arbitrarily large datasets of diverse, interactive, customizable, and performant virtual environments to train and evaluate embodied agents across navigation, interaction, and manipulation tasks. We demonstrate the power and potential of PROCTHOR via a sample of 10,000 generated houses and a simple <mark>neural model</mark>.<br>2: Often, the utility function itself is a deep <mark>neural model</mark>, rendering MBR prohibitively expensive for many use cases. In this work, we address the computational efficiency of MBR with an iterative pruning algorithm where low-performing hypotheses are removed while the number of samples used to estimate utilities grows.<br>",
    "Arabic": "نموذج عصبي",
    "Chinese": "神经模型",
    "French": "modèle neuronal",
    "Japanese": "ニューラルモデル",
    "Russian": "нейронная модель"
  },
  {
    "English": "neural module",
    "context": "1: While early attempts use off-the-shelf parsers [2], recent methods [9,10,12] learn the layout generation model jointly with the <mark>neural modules</mark> using REINFORCE [30] and weak answer supervision. While similar in spirit to NMNs, VISPROG has several advantages over NMNs.<br>2: Our model has two components, trained jointly: first, a collection of neural \"modules\" that can be freely composed (Figure 1b); second, a network layout predictor that assembles modules into complete deep networks tailored to each question (Figure 1a).<br>",
    "Arabic": "وحدة عصبية",
    "Chinese": "神经模块",
    "French": "module neuronal",
    "Japanese": "神経モジュール",
    "Russian": "нейронный модуль"
  },
  {
    "English": "neural net",
    "context": "1: The baseline method of [23] uses a single tanh-layer <mark>neural net</mark> parametrized by W to compute a hidden vector h: h(s, q) = tanh W φ(s) φ(q) .<br>2: We follow a related approach to make audible the innards of sound processing <mark>neural net</mark>s. The basic idea is the following: You start with an arbitrary sound clip (e.g. a drum loop). This clip will be modified by optimization procedure in way that stimulates a certain region in the <mark>neural net</mark>.<br>",
    "Arabic": "شبكة عصبية",
    "Chinese": "神经网络",
    "French": "réseau neuronal",
    "Japanese": "ニューラルネット",
    "Russian": "нейронная сеть"
  },
  {
    "English": "neural network architecture",
    "context": "1: With BCLs as building blocks, we propose a new <mark>neural network architecture</mark>, which we refer to as SPLATNet (SParse LATtice Networks), that does hierarchical and spatially-aware feature learning for unordered points. SPLATNet has several advantages for point cloud processing: \n<br>2: In this work, we propose a generic and flexible <mark>neural network architecture</mark> for processing point clouds that alleviates some of the aforementioned issues with existing deep architectures. Our key observation is that the bilateral convolution layers (BCLs) proposed in [22,25] have several favorable properties for point cloud processing.<br>",
    "Arabic": "معمارية الشبكة العصبية",
    "Chinese": "神经网络架构",
    "French": "architecture de réseau neuronal",
    "Japanese": "神経ネットワーク アーキテクチャ",
    "Russian": "нейросетевая архитектура"
  },
  {
    "English": "neural network classifier",
    "context": "1: In each case the number of hidden units was set to 20, subject to the constraint that (n in + n out ) × n hidden ≤ train size/2. We trained the networks to minimize cross entropy error using the quasi-Newton method from Netlab Once a pairwise <mark>neural network classifier</mark> was learned , we classified test examples according to the previous `` edge '' model , again by building a random graph between test labels ( using an average of 18 edges per test label as before ) , using the learned coordination<br>2: We have addressed the task of distinguishing between the presence of one vs. two persons in a visually unobservable indoor region of interest, using data obtained from seismic sensors. After exploring multiple alternatives, we identified four features as being capable of assisting classification with a high degree of reliability. Classification was achieved using a <mark>neural network classifier</mark>.<br>",
    "Arabic": "مصنف الشبكة العصبية",
    "Chinese": "神经网络分类器",
    "French": "classificateur de réseau neuronal",
    "Japanese": "ニューラルネットワーク分類器",
    "Russian": "классификатор нейронной сети"
  },
  {
    "English": "neural network language model",
    "context": "1: Deep learning methods for language processing owe much of their success to <mark>neural network language models</mark>, in which words are represented as dense real-valued vectors in R d . Such representations are referred to as distributed word representations or word embeddings, as they embed an entire vocabulary into a relatively low-dimensional linear space, whose dimensions are latent continuous features.<br>2: What is the source of the discrepancy between the magnitude of garden path effects in humans and surprisal-based estimates of those magnitudes from <mark>neural network language models</mark>? In this paper, we have evaluated one possible answer to this question: that word predictability estimates from LMs underweight the importance of syntax to the predictions made by humans.<br>",
    "Arabic": "نموذج لغة الشبكة العصبية",
    "Chinese": "神经网络语言模型",
    "French": "modèle de langage de réseau de neurones",
    "Japanese": "神経網言語モデル",
    "Russian": "нейросетевая языковая модель"
  },
  {
    "English": "neural network layer",
    "context": "1: Our realization of back-propagation trees is modular and we discuss how to integrate them in existing deep learning frameworks such as Caffe [16], MatConvNet [37], Minerva 1 , etc. supported by standard <mark>neural network layer</mark> implementations. Of course, we also maintain the ability to use back-propagation trees as (shallow) stand-alone classifiers.<br>",
    "Arabic": "طبقة الشبكة العصبية",
    "Chinese": "神经网络层",
    "French": "couche de réseau neuronal",
    "Japanese": "ニューラルネットワーク層",
    "Russian": "слой нейронной сети"
  },
  {
    "English": "neural network model",
    "context": "1: where the first term is the standard mean squared error, L is the representation balancing loss, Θ represents the parameters in this <mark>neural network model</mark>. and are two hyperparameters which control the weights for the representation balancing loss and the parameter regularization term. The ITE for each instance can be estimated as: = 1 − 0 .<br>2: such as offensiveness (Gehman et al., 2020) or unequal treatment (Cao et al., 2022), but also from failing to replicate other more desirable patterns which are also present in the data but are hard to capture by the <mark>neural network model</mark>, such as truthful information (Lin et al., 2022).<br>",
    "Arabic": "نموذج الشبكة العصبية",
    "Chinese": "神经网络模型",
    "French": "modèle de réseau neuronal",
    "Japanese": "神経ネットワークモデル",
    "Russian": "модель нейронной сети"
  },
  {
    "English": "neural operator",
    "context": "1: 2D Neural Rendering: The neural rendering operator \n π neural θ : R H V ×W V ×M f → R H×W ×3(11) \n with weights θ maps the feature image \n I V ∈ R H V ×W V ×M f \n to the final synthesized imageÎ ∈ R H×W ×3 .<br>",
    "Arabic": "العامل العصبي",
    "Chinese": "神经算子",
    "French": "opérateur neuronal",
    "Japanese": "ニューラル演算子",
    "Russian": "нейронный оператор"
  },
  {
    "English": "neural parser",
    "context": "1: Many of these are similarly found in other works investigating the linguistic features captured by the token representations of <mark>neural parsers</mark> (Gaddy et al., 2018;Li and Eisner, 2019). What follows is the rough order in which several of these features appear: \n 1. Separation between noun phrases and verb phrases 2.<br>2: Building on the success of preceding <mark>neural parsers</mark> (Chen and Manning, 2014;Kiperwasser and Goldberg, 2016), Dozat and Manning (2017) proposed a biaffine parsing head on top of a Bi-LSTM encoder: contextualized word vectors are fed to two feedforward networks, producing dependent-and headspecific token representations, respectively.<br>",
    "Arabic": "محلل عصبي",
    "Chinese": "神经网络语法分析器",
    "French": "analyseur neural",
    "Japanese": "神経パーサー",
    "Russian": "нейронный парсер"
  },
  {
    "English": "neural radiance field",
    "context": "1: <mark>Neural Radiance Fields</mark>: A radiance field is a continuous function f which maps a 3D point x ∈ R 3 and a viewing direction d ∈ S 2 to a volume density σ ∈ R + and an RGB color value c ∈ R 3 .<br>2: Though these approaches have yielded promising results on specific object categories such as faces, they have not yet been demonstrated to support arbitrary text. <mark>Neural Radiance Fields</mark>, or NeRF  are an approach towards inverse rendering in which a volumetric raytracer is combined with a neural mapping from spatial coordinates to color and volumetric density.<br>",
    "Arabic": "حقل الإشعاع العصبي",
    "Chinese": "神经辐射场",
    "French": "champ de radiance neuronale",
    "Japanese": "ニューラル輝度場",
    "Russian": "нейронное поле излучения"
  },
  {
    "English": "neural renderer",
    "context": "1: LSIG [42] uses a triangle-mesh and restricts the topology post initialization. Using an explicit differentiable renderer to optimize implicit geometry, our method can change topology during optimization and recover fine-details. Note that IDR requires an object mask and a <mark>neural renderer</mark>. obtained to evolve the level-set function Φ.<br>2: However, prior work on neural scene representations either does not or only weakly enforces 3D structure [1][2][3][4]. Multi-view geometry and projection operations are performed by a black-box <mark>neural renderer</mark>, which is expected to learn these operations from data.<br>",
    "Arabic": "محاكي عصبي",
    "Chinese": "神经渲染器",
    "French": "rendu neuronal",
    "Japanese": "神経レンダラー",
    "Russian": "нейронный рендерер"
  },
  {
    "English": "neural rendering",
    "context": "1: Next, we exploit the additive property of feature fields to composite scenes from multiple individual objects (Sec. 3.2). For rendering, we explore an efficient combination of volume and <mark>neural rendering</mark> techniques (Sec. 3.3). Finally, we discuss how we train our model from raw image collections (Sec. 3.4). Fig.<br>2: This interplay between scene parameterization and anti-aliasing suggests that a signal processing analysis of coordinate spaces in <mark>neural rendering</mark> problems may provide additional unexpected benefits or insights.<br>",
    "Arabic": "التقديم العصبي",
    "Chinese": "神经渲染",
    "French": "rendu neuronal",
    "Japanese": "ニューラルレンダリング",
    "Russian": "нейросетевая визуализация"
  },
  {
    "English": "neural representation",
    "context": "1: Let x = De be observed entangled data, where the independent task factor vector e obeys the same distributional assumptions as in Theorem 1. Let a <mark>neural representation</mark> z exactly predict observed data via x = W z + b x with zero error.<br>2: 3: A visualization of view-dependent emitted radiance. Our neural radiance field representation outputs RGB color as a 5D function of both spatial position x and viewing direction d. Here, we visualize example directional color distributions for two spatial locations in our <mark>neural representation</mark> of the Ship scene.<br>",
    "Arabic": "التمثيل العصبي",
    "Chinese": "神经表征",
    "French": "représentation neuronale",
    "Japanese": "ニューラル表現",
    "Russian": "нейронное представление"
  },
  {
    "English": "neural retrieval",
    "context": "1: Existing effective approaches for fact-verification include self-attention based networks (Nie et al., 2019), large-scale pretrained transformers (Soleimani et al., 2020), <mark>neural retrieval</mark> methods (Lewis et al., 2020a), and reasoning 5 dataverse.harvard.edu/dataverse/nela 6 github.com/KaiDMML/FakeNewsNet 7 github.com/entitize/Fakeddit on semantic-level graphs (Zhong et al., 2020).<br>",
    "Arabic": "استرجاع عصبي",
    "Chinese": "神经检索",
    "French": "récupération neuronale",
    "Japanese": "ニューラル検索",
    "Russian": "нейронный поиск"
  },
  {
    "English": "neural scaling law",
    "context": "1: Our work brings together 3 largely disparate strands of intellectual inquiry in machine learning: (1) explorations of different metrics for quantifying differences between individual training examples; \n (2) the empirical observation of <mark>neural scaling laws</mark>; and (3) the statistical mechanics of learning.<br>2: One reason to believe this is the phenomenon known as <mark>neural scaling laws</mark>: empirical observations that deep networks exhibit power law scaling in the test loss as a function of training dataset size, number of parameters or compute [13,27,11,16,9,12,15,34,14,7,26].<br>",
    "Arabic": "قانون تحجيم الشبكات العصبية",
    "Chinese": "神经缩放定律",
    "French": "loi d'échelle neuronale",
    "Japanese": "ニューラルスケーリング則",
    "Russian": "законы масштабирования нейронов"
  },
  {
    "English": "neural scene representation",
    "context": "1: Given \n a training set C = {(I i , E i , K i )} N i=1 \n of N tuples of images I i ∈ R H×W ×3 along with their respective extrinsic E i = R|t ∈ R 3×4 and intrinsic K i ∈ R 3×3 camera matrices [ 66 ] , our goal is to distill this dataset of observations into a <mark>neural scene representation</mark> Φ that strictly enforces 3D structure and allows to generalize shape and appearance priors<br>2: We introduce SRNs, a 3D-structured <mark>neural scene representation</mark> that implicitly represents a scene as a continuous, differentiable function. This function maps 3D coordinates to a feature-based representation of the scene and can be trained end-to-end with a differentiable ray marcher to render the feature-based representation into a set of 2D images.<br>",
    "Arabic": "التمثيل العصبي للمشهد",
    "Chinese": "神经场景表示",
    "French": "représentation neuronale de scène",
    "Japanese": "神経場面表現",
    "Russian": "нейронное представление сцены"
  },
  {
    "English": "neural sequence model",
    "context": "1: We describe a procedure called LEXSYM that discovers these transformations automatically, then applies them to training data for ordinary <mark>neural sequence models</mark>. Unlike existing compositional data augmentation procedures, LEXSYM can be deployed agnostically across text, structured data, and even images.<br>2: Our work explores two training objectives in this framework, autoregressive prediction as originally explored for modern <mark>neural sequence models</mark> by Dai & Le (2015), and a denoising objective, similar to BERT (Devlin et al., 2018). The context in-painting approach of Pathak et al.<br>",
    "Arabic": "نموذج تسلسل عصبي",
    "Chinese": "神经序列模型",
    "French": "modèle de séquence neuronale",
    "Japanese": "ニューラルシーケンスモデル",
    "Russian": "нейронная последовательностная модель"
  },
  {
    "English": "neural text generation",
    "context": "1: Inspired by the A* search algorithm, we introduce NEUROLOGIC A esque decoding, which brings A*-like heuristic estimates of the future to common left-to-right decoding algorithms for <mark>neural text generation</mark>.<br>2: As language generation systems moved away from phrase-based statistical approaches and towards neural models, beam search remained the de-facto decoding algorithm (Sutskever et al., 2014;Vinyals and Le, 2015). However , it has been observed that when used as a decoding algorithm for <mark>neural text generation</mark> , beam search ( for small beams ) typically has a large percentage of search errors 3 If all hypotheses in Yt end in EOS for some t < nmax , then we may terminate beam search early as it is then gauranteed that Yt = Yn<br>",
    "Arabic": "توليد النص العصبي",
    "Chinese": "神经文本生成",
    "French": "génération de texte neuronale",
    "Japanese": "\"神経テキスト生成\"",
    "Russian": "генерация текста нейросетями"
  },
  {
    "English": "neural volume",
    "context": "1: <mark>Neural Volumes</mark> (NV) [24] synthesizes novel views of objects that lie entirely within a bounded volume in front of a distinct background (which must be separately captured without the object of interest).<br>2: Voxel grids also admit coloring via a volume C(z) ∈ R 3×R 3 which can be sampled in an analogous manner. <mark>Neural Volumes</mark> A notable voxel-grid-based method is <mark>Neural Volumes</mark> [ 41 ] , which proposed an improved sampling function ζ warp ( ζ ( W ( z ) , x ) + x , V ( z ) ) which refines the sampling location x with an offset vector ζ ( W ( z ) , x ) ∈ R 3 sampled from a<br>",
    "Arabic": "حجم عصبي",
    "Chinese": "神经体积",
    "French": "volume neuronal",
    "Japanese": "ニューラルボリューム",
    "Russian": "нейронный объем"
  },
  {
    "English": "neural volumetric representation",
    "context": "1: NeRF has inspired many subsequent works that extend its continuous <mark>neural volumetric representation</mark> for generative modeling [8,36], dynamic scenes [23,33], nonrigidly deforming objects [13,34], phototourism settings with changing illumination and occluders [26,43], and reflectance modeling for relighting [2,3,40].<br>",
    "Arabic": "تمثيل عصبي حجمي",
    "Chinese": "神经体积表示",
    "French": "représentation volumétrique neuronale",
    "Japanese": "ニューラルボリューメトリック表現",
    "Russian": "нейронное объемное представление"
  },
  {
    "English": "neural word embedding",
    "context": "1: So as to make our evaluation more robust, we run the above experiments for three popular embedding methods, using large pre-trained models released by their respective authors as follows: Word2vec (Mikolov et al., 2013) is the original implementation of the CBOW and skip-gram architectures that popularized <mark>neural word embeddings</mark>.<br>",
    "Arabic": "تضمين الكلمات العصبية",
    "Chinese": "神经词嵌入",
    "French": "plongement de mots neuronaux",
    "Japanese": "単語の神経埋め込み",
    "Russian": "нейронные векторные представления слов"
  },
  {
    "English": "neuro-symbolic system",
    "context": "1: This work opens the door for developing versatile and sample-efficient grounded language understanding systems that fully capitalize on the language understanding ability of LMs while avoiding their limitations. It also sheds light on developing better <mark>neuro-symbolic systems</mark> in general.<br>",
    "Arabic": "نظام عصبي رمزي",
    "Chinese": "神经符号系统",
    "French": "système neuro-symbolique",
    "Japanese": "神経記号システム",
    "Russian": "нейросимволическая система"
  },
  {
    "English": "neuron",
    "context": "1: Again, we use it over other metrics since other metrics penalise situations where more than one <mark>neuron</mark> codes for a ground-truth factor.<br>2: Then in order to minimise the expected activity energy, E||z|| 2 , z must be structured so that each <mark>neuron</mark> only represents at most a single category. Intuition.<br>",
    "Arabic": "خلية عصبية",
    "Chinese": "神经元",
    "French": "neurone",
    "Japanese": "ニューロン",
    "Russian": "нейрон"
  },
  {
    "English": "next sentence prediction",
    "context": "1: For pretrained models, we release the heads used to pretrain the model itself. For instance, for BERT we release the language modeling and <mark>next sentence prediction</mark> heads which allows easy for adaptation using the pretraining objectives. We also make it easy for users to utilize the same core Transformer parameters with a variety of other heads for finetuning.<br>2: As we show in Figure 1, C is used for <mark>next sentence prediction</mark> (NSP). 5 Despite its simplicity, we demonstrate in Section 5.1 that pre-training towards this task is very beneficial to both QA and NLI. 6  The NSP task is closely related to representationlearning objectives used in Jernite et al.<br>",
    "Arabic": "التنبؤ بالجملة القادمة",
    "Chinese": "下一句预测",
    "French": "prédiction de la phrase suivante",
    "Japanese": "次の文予測",
    "Russian": "предсказание следующего предложения"
  },
  {
    "English": "next token prediction",
    "context": "1: For the language generation approach, the nonsensical statement is used as a prompt. At the training stage, the statement and the explanation are concatenated together, and a GPT-2 is trained on these sequences with a <mark>next token prediction</mark> objective.<br>",
    "Arabic": "التنبؤ بالرمز التالي",
    "Chinese": "下一个令牌预测",
    "French": "prédiction du prochain jeton",
    "Japanese": "次のトークン予測",
    "Russian": "прогнозирование следующего токена"
  },
  {
    "English": "nmod",
    "context": "1: We observe another group in nominal dependents (<mark>nmod</mark>:of, amod, acl), where <mark>nmod</mark>:of mostly points to collective nouns (e.g., \"pile of oranges\"), whose dominance is intuitive.<br>",
    "Arabic": "nmod",
    "Chinese": "nmod",
    "French": "nmod",
    "Japanese": "名詞修飾成分",
    "Russian": "nmod"
  },
  {
    "English": "no-regret algorithm",
    "context": "1: Now suppose each player i uses a <mark>no-regret algorithm</mark> to produce w t i on each round and receives cost c t i,s = E s−i∼w t −i [c i (s, s −i )] for each strategy s ∈ S i . Moreover, for any fixed strategy s, the <mark>no-regret algorithm</mark> ensures \n<br>2: One scenario in which we indeed have such an oracle is when PO corresponds to running a <mark>no-regret algorithm</mark> separately in each state 4 and the policy class is sufficiently rich to approximate the resulting iterates.<br>",
    "Arabic": "خوارزمية بدون ندم",
    "Chinese": "无悔算法",
    "French": "algorithme sans regret",
    "Japanese": "後悔しないアルゴリズム",
    "Russian": "алгоритм без сожалений"
  },
  {
    "English": "no-regret dynamic",
    "context": "1: To guarantee these optimal sample complexity bounds, our algorithms learn to sample from data distributions on demand. Our algorithm design and analysis are enabled by our extensions of online learning techniques for solving stochastic zero-sum games. In particular, we contribute stochastic variants of <mark>no-regret dynamics</mark> that can trade off between players' differing sampling costs.<br>2: However, it was currently unknown whether EFCE emerges as the result of uncoupled agent dynamics. In this paper, we give the first uncoupled <mark>no-regret dynamics</mark> that converge to the set of EFCEs in n-player general-sum extensive-form games with perfect recall.<br>",
    "Arabic": "ديناميكية بدون ندم",
    "Chinese": "无后悔动态",
    "French": "dynamique sans regret",
    "Japanese": "ノーリグレット・ダイナミクス",
    "Russian": "динамика без сожалений"
  },
  {
    "English": "no-regret learning algorithm",
    "context": "1: , c (T ) : A → [0, 1] as follows: \n Reg(a (1:T ) , c (1:T ) ) := T t=1 c (t) (a (t) ) − min a * ∈A T t=1 c (t) (a * ). We say that a <mark>no-regret learning algorithm</mark> Q A has a regret guarantee of γ T ( Q A ) if , for any sequence of linear cost functions c ( 1 : T ) of bounded norm , i.e. , max t∈ [ T ] c ( t ) ≤ 1 , the algorithm Q A chooses an action sequence a ( 1<br>2: It is natural in this setting for the players to each make use of a <mark>no-regret learning algorithm</mark> for making their decisions, an approach known as decentralized no-regret dynamics. No-regret algorithms are a strong match for playing games because their regret bounds hold even in adversarial environments.<br>",
    "Arabic": "خوارزمية التعلم بدون ندم",
    "Chinese": "无悔学习算法",
    "French": "algorithme d'apprentissage sans regret",
    "Japanese": "後悔のない学習アルゴリズム",
    "Russian": "алгоритм обучения без сожалений"
  },
  {
    "English": "node",
    "context": "1: where n l and n r indicate the left and right child of <mark>node</mark> n, respectively, and we define A m for a generic <mark>node</mark> m ∈ N as \n A m = ℓ∈Lm π ℓy µ ℓ (x|Θ) P T [y|x, Θ, π] .<br>2: Thus the weight of each <mark>node</mark> indicates the importance of the term to this topic, while the weight of each edge indicates how strongly the label and the term are semantically associated.<br>",
    "Arabic": "عقدة",
    "Chinese": "节点",
    "French": "nœud",
    "Japanese": "ノード",
    "Russian": "узел"
  },
  {
    "English": "node attribute",
    "context": "1: 1 Our image parsing algorithm consists of a set of reversible Markov chain jumps (Green, 1995) with each type of jump corresponding to an operator for reconfiguring the parsing graph (i.e., creating or deleting nodes or changing the values of <mark>node attributes</mark>).<br>2: We use the bag-of-words of each paper's abstract as its <mark>node attributes</mark> and regard the theme of paper as its label. To simulate the scenario that a venue or an organizer forbids others to cite its papers, FS-G allows users to split this dataset by each node's venue or the organizer of that venue.<br>",
    "Arabic": "سمة العقدة",
    "Chinese": "节点属性",
    "French": "attribut de nœud",
    "Japanese": "ノード属性",
    "Russian": "атрибут узла"
  },
  {
    "English": "node classification",
    "context": "1: As each context has a different degree of influence on a speaker's speech, it is important that the graph encoding suitably weighs more relevant relations between transcripts, speakers and motions. To this end, we use GATs, that are graph neural networks with node level attention popularly used for <mark>node classification</mark> (Veličković et al., 2017).<br>2: 3.5.1 Connection to Existing Work. A prior study on graph prompt is proposed by [27], namely GPPT. They use edge prediction as a pre-training pretext and reformulate <mark>node classification</mark> to the pretext by designing labeled tokens added to the original graph.<br>",
    "Arabic": "تصنيف العقدة",
    "Chinese": "节点分类",
    "French": "Classification de nœuds",
    "Japanese": "ノード分類",
    "Russian": "классификация узлов"
  },
  {
    "English": "node degree",
    "context": "1: We show the increase in the average <mark>node degree</mark> over time in Figure 1(c). The densification exponent is a = 1.18, lower than the one for the citation networks, but still clearly greater than 1.<br>2: Here D ∈ R × is a diagonal matrix where each element represents the <mark>node degree</mark> (i.e., =1 ℎ , ). B ∈ R × is another diagonal matrix, where each element is the size of each hyperedge ( =1 ℎ , ). Then we can define the hypergraph convolution operator as: \n<br>",
    "Arabic": "درجة العقدة",
    "Chinese": "节点度数",
    "French": "degré du nœud",
    "Japanese": "ノード次数",
    "Russian": "степень узла"
  },
  {
    "English": "node embedding",
    "context": "1: The main characteristic of all baselines are listed as follows: \n • GCN [20] produces <mark>node embedding</mark> vectors by truncating the Chebyshev polynomial to the first-order neighborhoods. • ResGCN [20] adopts the residual connections between hidden layers to facilitate the training of deeper models by enabling the model to carry over information from the previous layer's input.<br>2: For the time complexity, a typical graph model (e.g., GCN [34]) usually needs ( 2 + + ) time to generate <mark>node embedding</mark> via message passing and then obtain the whole graph representation (e.g., ( ) for summation pooling).<br>",
    "Arabic": "تضمين العقدة",
    "Chinese": "节点嵌入",
    "French": "plongement de nœuds",
    "Japanese": "ノード埋め込み",
    "Russian": "вложение узла"
  },
  {
    "English": "node feature",
    "context": "1: Here, nodes are authors, that are connected by an edge if they co-authored a paper; <mark>node features</mark> represent paper keywords for each author's papers, and class labels indicate the most active fields of study for each author. We use a pre-divided version of these datasets through the Deep Graph Library (DGL) 5 .<br>2: Here we have used the augmented normalized adjacency matrix to propagate messages from each node to its neighbors, which simply leads to a degree normalization of the message functions ψ . To avoid heavy notations the <mark>node features</mark> and representations are assumed to be scalar from now on; these assumptions simplify the discussion and the vector case leads to analogous results.<br>",
    "Arabic": "سمة العقدة",
    "Chinese": "节点特征",
    "French": "caractéristique du nœud",
    "Japanese": "ノード特徴",
    "Russian": "признак узла"
  },
  {
    "English": "node feature matrix",
    "context": "1: Let be any graph-level transformation such as \"changing node features\", \"adding or removing edges/subgraphs\" etc., and * be the frozen pre-trained graph model. For any graph G with adjacency matrix A and <mark>node feature matrix</mark> X, Fang et al.<br>",
    "Arabic": "مصفوفة سمات العقدة",
    "Chinese": "节点特征矩阵",
    "French": "matrice de caractéristiques des nœuds",
    "Japanese": "ノード特徴行列",
    "Russian": "матрица признаков узлов"
  },
  {
    "English": "node label",
    "context": "1: ( hypergraph structure ) and the infection outcomes ( <mark>node labels</mark> ) . A critical limitation here is the lack of causality, which is particularly important for understanding the impact of a policy intervention (e.g., wearing face covering) on an outcome of interest (e.g., COVID-19 infection). For individuals connected as in Fig.<br>2: , v ) ∈ E T . 1 Given a st-graph and the feature vectors associated with the nodes x t v and edges x t e , as shown in Figure 2b , the goal is to predict the <mark>node labels</mark> ( or real value vectors ) y t v at each time step t. For instance , in human-object interaction , the node features can represent the human and<br>",
    "Arabic": "نُقطة البيان",
    "Chinese": "节点标签",
    "French": "étiquette de nœud",
    "Japanese": "ノードラベル",
    "Russian": "метка узла"
  },
  {
    "English": "node representation",
    "context": "1: G , h . Formally, \n Note that the constant 1 |V| can be extracted with an additional head and be concatenated to the <mark>node representation</mark>s. Moreover, the <mark>node representation</mark> X is processed via the feed-forward network in the previous layer (see (40).<br>",
    "Arabic": "تمثيل العقدة",
    "Chinese": "节点表示",
    "French": "représentation des nœuds",
    "Japanese": "ノード表現",
    "Russian": "узловое представление"
  },
  {
    "English": "node set",
    "context": "1: where P = {P p , P n }G denotes the <mark>node set</mark> in graph G. P p denotes the positive nodes that have corresponding nodes in graph G ′ . Similarly, P n denotes the negative nodes that have no corresponding nodes in graph G ′ .<br>2: We use 3D positions of objects to generate the <mark>node set</mark> and the nearest neighbor search to generate the edge set. We use fences and vegetation to construct DFs. The ground-truth loop closure is obtained based on the ground-truth poses provided by the KITTI odometry dataset.<br>",
    "Arabic": "مجموعة العقد",
    "Chinese": "节点集合",
    "French": "ensemble de nœuds",
    "Japanese": "ノード集合",
    "Russian": "множество узлов"
  },
  {
    "English": "node-disjoint path",
    "context": "1: We propose a much simpler alternative which suffices for our purposes: As a by-product of the generation of the set system itself we can obtain a set of node-disjoint B-violating paths. Clearly, any feasible solution must contain an extra node per path in this set. Hence the size of a set of nodedisjoint paths yields a valid lower bound.<br>",
    "Arabic": "\"مسار مُنفصل العُقد\"",
    "Chinese": "无公共节点路径",
    "French": "chemin sans nœud commun",
    "Japanese": "ノード非共有パス",
    "Russian": "узлово-раздельный путь"
  },
  {
    "English": "noise distribution",
    "context": "1: The following is a summary of our primary contributions: \n • We propose a novel diffusion-based model for hand pose estimation that utilizes the depth image and point cloud input as a multi-modal condition. This model progressively denoises a <mark>noise distribution</mark>, accurately determining the 3D coordinates of hand joints.<br>",
    "Arabic": "توزيع الضوضاء",
    "Chinese": "噪声分布",
    "French": "distribution de bruit",
    "Japanese": "ノイズ分布",
    "Russian": "распределение шума"
  },
  {
    "English": "noise level",
    "context": "1: Training a neural network to model D directly would be far from ideal -for example, as the input x = y + n is a combination of clean signal y and noise n ∼ N (0, σ 2 I), its magnitude varies immensely depending on <mark>noise level</mark> σ.<br>",
    "Arabic": "مستوى الضجيج",
    "Chinese": "噪声水平",
    "French": "niveau de bruit",
    "Japanese": "ノイズレベル",
    "Russian": "уровень шума"
  },
  {
    "English": "noise model",
    "context": "1: To begin we involve the <mark>noise model</mark> from (1), which fully defines the assumed likelihood p(B|S). While the unknown noise covariance can also be parameterized and estimated from the data, for simplicity we assume that Σ is known and fixed. Next we adopt the following source prior for S: \n<br>",
    "Arabic": "نموذج الضوضاء",
    "Chinese": "噪声模型",
    "French": "modèle de bruit",
    "Japanese": "ノイズモデル",
    "Russian": "модель шума"
  },
  {
    "English": "noise schedule",
    "context": "1: where x is the ground-truth image, c is a conditioning vector (e.g., obtained from a text prompt), and α t , σ t , w t are terms that control the <mark>noise schedule</mark> and sample quality, and are functions of the diffusion process time t ∼ U([0, 1]).<br>2: In simple terms, a conditional diffusion modelx θ is trained using a squared error loss to denoise a variably-noised image z t := α t x+σ t as follows: \n E x,c, ,t w t x θ (α t x + σ t , c) − x 2 2 (3) \n where x is the ground-truth image , c is a conditioning vector ( e.g. , obtained from a text prompt ) , ∼ N ( 0 , I ) is a noise term and α t , σ t , w t are terms that control the <mark>noise schedule</mark> and sample quality , and are functions of the diffusion process time t ∼ U<br>",
    "Arabic": "جدول الضوضاء",
    "Chinese": "噪声时间表",
    "French": "programme de bruit",
    "Japanese": "ノイズスケジュール",
    "Russian": "расписание шума"
  },
  {
    "English": "noise-contrastive estimation",
    "context": "1: For the classless LBLs we use <mark>noise-contrastive estimation</mark> (NCE) (Gutmann & Hyvärinen, 2012;Mnih & Teh, 2012) to avoid normalisation during training. This leaves the expensive test-time normalisation of LBLs unchanged, precluding their usage during decoding.<br>2: We therefore apply <mark>noise-contrastive estimation</mark> (NCE; Gutmann and Hyvärinen, 2012), which transforms the problem of estimating the density P (y) into a classification problem: distinguishing the observed graph labelings y (t) from randomlygenerated \"noise\" labelingsỹ (t) ∼ P n , where P n is a noise distribution.<br>",
    "Arabic": "تقدير التباين الضوضائي",
    "Chinese": "噪声对比估计",
    "French": "estimation contrastive du bruit",
    "Japanese": "ノイズ対比推定",
    "Russian": "шумоконтрастная оценка"
  },
  {
    "English": "noisy channel",
    "context": "1: As summarizers, we may not have access to the editor's original version (which may or may not exist), but we can guess at itwhich is where probabilities come in. As in any <mark>noisy channel</mark> application, we must solve three problems: \n • Source model.<br>2: This section describes a probabilistic approach to the compression problem. In particular, we adopt the <mark>noisy channel</mark> framework that has been relatively successful in a number of other NLP applications, including speech recognition (Jelinek 1997), machine translation (Brown et al.<br>",
    "Arabic": "قناة ضوضائية",
    "Chinese": "噪声信道",
    "French": "canal bruité",
    "Japanese": "雑音チャネル",
    "Russian": "шумный канал"
  },
  {
    "English": "nominal mention",
    "context": "1: Updating referring assignments and word lists δ r (Z r , L): The word lists are usually concatenations of the words used in nominal and proper mentions and so are updated together with the assignments for those mentions.<br>",
    "Arabic": "ذكر مسمى",
    "Chinese": "名词性提及",
    "French": "mention nominale",
    "Japanese": "名詞句言及",
    "Russian": "номинальное упоминание"
  },
  {
    "English": "non-convex objective",
    "context": "1: As an illustration of higher-level concepts, we show construction of convex underestimators for the <mark>non-convex objective</mark> in (11). The actual objective we minimize incorporates chirality bounds and is derived in Section 6.3. Let us suppose it is possible to derive a convex underestimator conv (γ i 1/3 α i ) and concave overestimator \n<br>2: Loh and Wainwright [LW15] showed that for many statistical settings that involve missing/noisy data and non-convex regularizers, any stationary point of the <mark>non-convex objective</mark> is close to global optima; furthermore, there is a unique stationary point that is the global minimum under stronger assumptions [LW14].<br>",
    "Arabic": "هدف غير محدب",
    "Chinese": "非凸目标函数",
    "French": "objectif non convexe",
    "Japanese": "非凸目的関数",
    "Russian": "невыпуклая целевая функция"
  },
  {
    "English": "non-convex optimization",
    "context": "1: We implemented two variants of our method (<mark>non-convex optimization</mark> or NCO), one uses the hard-max (NCO-H) and the other one (NCO-S) uses the soft-max version over all possible labellings (see Eq. (4),( 9)), this latter version is implemented with a Forward-Backward procedure.<br>2: With σ = 0, D = 1, we recover the tight bound for sequential non-stochastic <mark>non-convex optimization</mark> Θ(∆L −2 ) as shown in Carmon et al. [66].<br>",
    "Arabic": "تحسين غير محدب",
    "Chinese": "非凸优化",
    "French": "optimisation non convexe",
    "Japanese": "非凸最適化",
    "Russian": "- Term: \"неконвексная оптимизация\""
  },
  {
    "English": "non-convex problem",
    "context": "1: Zhou and Gu [67] extends this lower bound to a finite sum setting, and Arjevani et al. [68] proposes a probabilistic zero-chain model that obtains tight lower bounds for first-order methods on stochastic and <mark>non-convex problems</mark>.<br>",
    "Arabic": "مشكلة غير محدبة",
    "Chinese": "非凸问题",
    "French": "problème non convexe",
    "Japanese": "非凸問題",
    "Russian": "неконвексная задача"
  },
  {
    "English": "non-convexity",
    "context": "1: & Joachims , 2003 ) . Non-Mahalanobis based metric learning methods have also been proposed, though these methods usually suffer from suboptimal performance, <mark>non-convexity</mark>, or computational complexity.<br>2: As we did for the case of the metric upgrade, we have reduced the <mark>non-convexity</mark> in the above optimization problem to a set of equality constraints. The quadratic inequality constraint is convex and is known as a rotated cone [4].<br>",
    "Arabic": "عدم التحدب",
    "Chinese": "非凸性",
    "French": "non-convexité",
    "Japanese": "非凸性",
    "Russian": "невыпуклость"
  },
  {
    "English": "non-euclidean space",
    "context": "1: However, it is challenging to directly adopt Mixup to augment graph data because different graphs typically: 1) have different numbers of nodes; 2) are not readily aligned; and 3) have unique typologies in <mark>non-Euclidean space</mark>.<br>",
    "Arabic": "الفضاء غير الإقليدي",
    "Chinese": "非欧几里得空间",
    "French": "espace non euclidien",
    "Japanese": "非ユークリッド空間",
    "Russian": "неевклидово пространство"
  },
  {
    "English": "non-linear least square",
    "context": "1: Using the coarse estimates of rotations or translations determined by BP, we apply continuous optimization to the objective functions in equations ( 5) and ( 7), using the Levenberg-Marquardt (LM) algorithm for <mark>non-linear least squares</mark> [18].<br>2: The common choice of high precision is to utilize the iterative PnP solver based on the Levenberg-Marquardt (LM) algorithm -a robust variant of the Gauss-Newton (GN) algorithm, which solves the <mark>non-linear least squares</mark> by the first and approximated second order derivatives.<br>",
    "Arabic": "المربعات الصغرى غير الخطية",
    "Chinese": "非线性最小二乘法",
    "French": "moindres carrés non linéaires",
    "Japanese": "非線形最小二乗法",
    "Russian": "нелинейный метод наименьших квадратов"
  },
  {
    "English": "non-linear optimization",
    "context": "1: First, these methods tend to be computationally intensive, making repeated use of bundle adjustment [29] (a <mark>non-linear optimization</mark> method that jointly refines camera parameters and scene structure) as well as outlier rejection to remove inconsistent measurements.<br>2: Instead, the authors argued that \"the real difficulty of in achieving good 3D reconstructions for nonrigid structures...is not the ambiguity of the [basis] constraints, but the complexity of the underlying <mark>non-linear optimization</mark>\".<br>",
    "Arabic": "تحسين غير خطي",
    "Chinese": "非线性优化",
    "French": "optimisation non linéaire",
    "Japanese": "非線形最適化",
    "Russian": "нелинейная оптимизация"
  },
  {
    "English": "non-linearity",
    "context": "1: We specify (in the natural way) a neural network by matrices Wj of shape |Lj| × i<j |Li| for each 1 ≤ j ≤ D, as well as 1-Lipschitz <mark>non-linearities</mark> σ j,ℓ and scalar biases b j,ℓ for each (j, ℓ) satisfying ℓ ∈ |Lj|.<br>2: Convolution layers, which are the building block of CNNs, project input features to a higher-level representation while preserving their resolution. When composed with <mark>non-linearities</mark> and normalization layers, this allows for learning rich mappings at a constant resolution, e.g. autogressive image synthesis (van den Oord et al., 2016).<br>",
    "Arabic": "غير خطية",
    "Chinese": "非线性",
    "French": "non-linéarité",
    "Japanese": "非線形性",
    "Russian": "нелинейность"
  },
  {
    "English": "non-local feature",
    "context": "1: The key idea is to compute <mark>non-local features</mark> incrementally from bottom up, so that we can rerank the n-best subtrees at all internal nodes, instead of only at the root node as in conventional reranking (see Table 1). This method can thus be viewed as a step towards the integration of discriminative reranking with traditional chart parsing.<br>2: However, we miss the benefits of <mark>non-local features</mark> that are not representable here. Ideally, we would wish to combine the merits of both approaches, where an efficient inference algorithm could integrate both local and <mark>non-local features</mark>. Unfortunately, exact search is intractable (at least in theory) for features with unbounded scope.<br>",
    "Arabic": "ميزة غير محلية",
    "Chinese": "非局部特征",
    "French": "caractéristique non locale",
    "Japanese": "非局所特徴",
    "Russian": "неместные признаки"
  },
  {
    "English": "non-markov process",
    "context": "1: Further, by removing the time step embeddings, our diffusion approach can better integrate with other improvement techniques, e.g., DDIM method (Song et al., 2021a) with the <mark>non-Markov process</mark> for fast inference.<br>",
    "Arabic": "عملية غير ماركوفية",
    "Chinese": "非马尔可夫过程",
    "French": "processus non markovien",
    "Japanese": "非マルコフ過程",
    "Russian": "немарковский процесс"
  },
  {
    "English": "non-max suppression",
    "context": "1: Rather than resorting to post-processing to clean up detections, our model learns optimal <mark>non-max suppression</mark> parameters and detection thresholds for each class. The resulting system outperforms published results on the PAS-CAL VOC 2007 object detection dataset.<br>",
    "Arabic": "تثبيط غير قصوى",
    "Chinese": "非最大值抑制",
    "French": "suppression des non-maximums",
    "Japanese": "非最大抑制",
    "Russian": "не-максимальное подавление"
  },
  {
    "English": "non-maxima suppression",
    "context": "1: Typically, researchers perform <mark>non-maxima suppression</mark>, assuming that high detection responses at neighboring positions could be due to an object at either of those positions (but not both). Making the same assumption, we also apply <mark>non-maxima suppression</mark>, but we form a point distribution out of the non-maxima, rather than discarding them.<br>2: We use our approach to learn the parameters of latent AP-SVMs [2] for each object category. In our experiments, we fix the hyperparameters using 5-fold crossvalidation. During testing, we evaluate each candidate window generated by selective search and use <mark>non-maxima suppression</mark> to prune highly overlapping detections. Results.<br>",
    "Arabic": "قمع القيم الغير قصوى",
    "Chinese": "非极大值抑制",
    "French": "suppression des non-maxima",
    "Japanese": "非最大抑制",
    "Russian": "подавление немаксимумов"
  },
  {
    "English": "non-maximal suppression",
    "context": "1: Finally, after selecting the confident and stable masks, we applied <mark>non-maximal suppression</mark> (NMS) to filter duplicates. To further improve the quality of smaller masks, we also processed multiple overlapping zoomed-in image crops. For further details of this stage, see §B.<br>",
    "Arabic": "ازالة الحدود غير القصوى",
    "Chinese": "非极大值抑制",
    "French": "suppression non-maximale",
    "Japanese": "非最大抑制",
    "Russian": "немаксимальное подавление"
  },
  {
    "English": "non-maximum suppression",
    "context": "1: Decoding takes all detector responses as input and decides on the final outcome. <mark>Non-maximum suppression</mark> (NMS) is the usual form of decoding. Perfect detectors with excellent tightly tuned models should seldom, if ever, need decoding because there is no ambiguity in what to report.<br>",
    "Arabic": "القمع غير الأقصى",
    "Chinese": "非极大值抑制",
    "French": "suppression du non-maximum",
    "Japanese": "非最大抑制",
    "Russian": "подавление немаксимумов"
  },
  {
    "English": "non-negative matrix factorization",
    "context": "1: which are more sensitive to overfitting than reconstructive ones , and various matrix factorization tasks , such as <mark>non-negative matrix factorization</mark> with sparseness constraints and sparse principal component analysis .<br>2: By calculating U t at each training step, we employ <mark>non-negative matrix factorization</mark> on the instability matrix U t following (Shazeer and Stern, 2018) where the generalized Kullback-Leibler divergence between V and W H is minimal.<br>",
    "Arabic": "تجزئة المصفوفة غير السالبة",
    "Chinese": "非负矩阵分解",
    "French": "factorisation de matrice non négative",
    "Japanese": "非負値行列因子分解",
    "Russian": "неотрицательное матричное разложение"
  },
  {
    "English": "non-parametric setting",
    "context": "1: We provide conditions for recoverability from selection bias in statistical and causal inferences applicable for arbitrary structures in <mark>non-parametric settings</mark>. Theorem 1 provides a complete characterization of recoverability when no external information is available. Theorem 2 provides a sufficient condition for recoverability based on external information; it is optimized by Theorem 3 and strengthened by Theorem 4.<br>",
    "Arabic": "إعداد لا معلمي",
    "Chinese": "非参数设置",
    "French": "cadre non paramétrique",
    "Japanese": "非パラメトリック設定",
    "Russian": "непараметрические настройки"
  },
  {
    "English": "non-projective parsing",
    "context": "1: However, under our framework, we show that the opposite is actually true that <mark>non-projective parsing</mark> has a lower asymptotic complexity. Using this framework, we presented results showing that the non-projective model outperforms the projective model on the Prague Dependency Treebank, which contains a small number of non-projective edges.<br>2: In fact, it is easier since <mark>non-projective parsing</mark> does not need to enforce the non-crossing constraint of projective trees. As a result, <mark>non-projective parsing</mark> complexity is just O(n 2 ), against the O(n 3 ) complexity of the Eisner dynamic programming algorithm, which by construction enforces the non-crossing constraint.<br>",
    "Arabic": "التحليل غير الإسقاطي",
    "Chinese": "非投射解析",
    "French": "analyse non-projective",
    "Japanese": "非射影構文解析",
    "Russian": "непроективный синтаксический анализ"
  },
  {
    "English": "non-submodular energy",
    "context": "1: In this section we describe the experiments we carried out in evaluating the efficacy of QPBO in optimizing our <mark>non-submodular energy</mark> , the trade-offs of each of the QPBO labeling methods , the effect of using different disparity proposals , and comparing our method , with its second-order prior , to the same method with a first-order prior , and other , competing approaches<br>",
    "Arabic": "طاقة غير قابلة للتحديد الفرعي",
    "Chinese": "非次模能量",
    "French": "énergie non-sous-modulaire",
    "Japanese": "非劣加法エネルギー",
    "Russian": "несубмодулярная энергия"
  },
  {
    "English": "non-tree model",
    "context": "1: On realistic semantic parsing tasks our approach outperforms previous work on generalization to longer examples than seen at training. We also outperform all other <mark>non-tree models</mark> on the structural generalization tasks in semantic parsing on COGS (Kim and Linzen, 2020).<br>",
    "Arabic": "نموذج غير شجري",
    "Chinese": "非树模型",
    "French": "modèle non-arborescent",
    "Japanese": "非木構造モデル",
    "Russian": "недревесная модель"
  },
  {
    "English": "nonconvex function",
    "context": "1: . At the borders of objects, adjacent pixels should often have very different labels and it is important that E not overpenalize such labelings. This requires that V be a <mark>nonconvex function</mark> of jf p À f q j. Such an energy function is called discontinuity-preserving.<br>2: 11, the dataterm is a <mark>nonconvex function</mark> where each minimum corresponds to a phase shift by 2π:<br>",
    "Arabic": "- Translation: \"دالة غير محدبة\"",
    "Chinese": "非凸函数",
    "French": "fonction non convexe",
    "Japanese": "非凸関数",
    "Russian": "нелинейная функция"
  },
  {
    "English": "nonlinear optimisation",
    "context": "1: In fact, nonlinear (iterative) optimisation can be used to directly to seek the globally optimal solution [17]. To this end, rewrite and solve (2) as the constrained nonlinear problem \n min θ,γ γ, s.t. r i (θ) ≤ γ,(18) \n<br>",
    "Arabic": "تحسين غير خطي",
    "Chinese": "非线性优化",
    "French": "optimisation non linéaire",
    "Japanese": "非線形最適化",
    "Russian": "нелинейная оптимизация"
  },
  {
    "English": "nonmonotonic reasoning",
    "context": "1: ASP is now widely used as an underlying knowledge representation language and robust methodology for <mark>nonmonotonic reasoning</mark> [Brewka et al., 2011;Gebser et al., 2012].<br>2: Other notions of only-knowing were considered in (Levesque and Lakemeyer 2001;Halpern and Lakemeyer 2001;Waaler 2004), but in terms of <mark>nonmonotonic reasoning</mark> they did not go beyond AEL. There have been proof-theoretic characterizations of DL such as (Bonatti and Olivetti 1997).<br>",
    "Arabic": "المنطق غير الرتيب",
    "Chinese": "非单调推理",
    "French": "raisonnement non monotone",
    "Japanese": "非単調推論",
    "Russian": "немонотонное рассуждение"
  },
  {
    "English": "nonterminal symbol",
    "context": "1: Our extraction method is basically the same as that of Block (2000), except we allow more than one <mark>nonterminal symbol</mark> in a rule, and use a more sophisticated probability model. In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation.<br>2: We propose Symbol-Refined Tree Substitution Grammars (SR-TSGs) for syntactic parsing. An SR-TSG is an extension of the conventional TSG model where each <mark>nonterminal symbol</mark> can be refined (subcategorized) to fit the training data.<br>",
    "Arabic": "رمز غير طرفي",
    "Chinese": "非终结符号",
    "French": "symbole non terminal",
    "Japanese": "非終端記号",
    "Russian": "нетерминальный символ"
  },
  {
    "English": "norm",
    "context": "1: Nonetheless, a constraint on the <mark>norm</mark> of the embeddings also constrains the <mark>norm</mark> of the dimensions indirectly, and vice versa, as illustrated in lemma 3.4. Lemma 3.4. If embeddings are <mark>norm</mark>alized such that ∀i, ∥K •,i ∥ 2 = a we have \n<br>2: • By Lemma 1, with probability at least 1−δ, all {x t , x t , z t , z t } terms involved in the definition of ∆ τ1,τ2 Y has 2 <mark>norm</mark> bounded by √ Γ max poly(d, κ A , log(T total /δ)). This implies that \n<br>",
    "Arabic": "معيار",
    "Chinese": "范数",
    "French": "norme",
    "Japanese": "ノルム",
    "Russian": "норма"
  },
  {
    "English": "normal",
    "context": "1: In order to take into account surface curvature as well, we not only evaluate the distance of the endpoints themselves, but also the distance after shifting the endpoints along the respective <mark>normals</mark> n p , n q . Denoting the difference of the <mark>normals</mark> as d n = n p − n q , we define a distance function 2).<br>2: Notice that although there are only two rigid motions, the scene contains three different homographies, each one associated with each one of the visible planar structures. Furthermore, notice that the top side of the cube and the checkerboard have approximately the same <mark>normals</mark>.<br>",
    "Arabic": "طبيعي",
    "Chinese": "法线 (normal)",
    "French": "normale",
    "Japanese": "法線",
    "Russian": "нормали"
  },
  {
    "English": "normal distribution",
    "context": "1: The synthetic data are initialized with noise of <mark>normal distribution</mark>, i.e., ∀s i ∈ S, s i ∼ N (0, I d ), and we assume the empirical mean is zeroed, i.e.,<br>2: The initial points of {v i } J i=1 are set to random draws from a <mark>normal distribution</mark> fitted to the training data, a heuristic we found to perform well in practice. The objective is non-convex in general, reflecting many possible ways to capture the differences of p and q.<br>",
    "Arabic": "- التوزيع الطبيعي",
    "Chinese": "正态分布",
    "French": "distribution normale",
    "Japanese": "正規分布",
    "Russian": "нормальное распределение"
  },
  {
    "English": "normal form",
    "context": "1: It can be shown that the solutions to (11) before and after this replacement are realization equivalent. The proof follows by repeated bottomup application of this operation, eventually collapsing the tree to its reduced <mark>normal form</mark>, all while maintaining realization equivalence. The full proof is in the Appendix.<br>2: We show that the ontology under which we learn can w.l.o.g. be assumed to be in <mark>normal form</mark>. It is well-known that every EL r -ontology O can be converted into <mark>normal form</mark> by introducing fresh concept names [Baader et al., 2017].<br>",
    "Arabic": "الصيغة العادية",
    "Chinese": "标准形式",
    "French": "forme normale",
    "Japanese": "正規形",
    "Russian": "нормальная форма"
  },
  {
    "English": "normal vector",
    "context": "1: However, if multiple planes with the same <mark>normal vector</mark> π = [0, 0, 1] T undergo pure translational motions of the form T i = [T xi , T yi , T zi ] T , then all the complex epipoles are equal to e i = [ √ −1, −1, 0] T .<br>2: and then compute the <mark>normal vector</mark> at y n as b n = Dp n (y n )/(e T K Dp n (y n )).<br>",
    "Arabic": "المُتَجَه الطبيعي",
    "Chinese": "法线向量",
    "French": "vecteur normal",
    "Japanese": "法線ベクトル",
    "Russian": "нормальный вектор"
  },
  {
    "English": "normal-form game",
    "context": "1: In addition to further investigating modeling applications, our future work will investigate the applicability of Max-CausalEnt on non-modeling tasks in dynamics settings. For instance, we note that the proposed principle provides a natural criteria for efficiently identifying a correlated equilibrium in dynamic Markov games, generalizing the approach to <mark>normal-form games</mark> of Ortiz et al. (2007).<br>2: In contrast to the standard definition of <mark>normal-form games</mark>, where the utility functions for game outcomes are known, in this work we assume that the true utility function, formed by w * , which governs observed behavior, is unknown.<br>",
    "Arabic": "لعبة الشكل الطبيعي",
    "Chinese": "正常型博弈",
    "French": "jeu sous forme normale",
    "Japanese": "標準形ゲーム",
    "Russian": "игра в нормальной форме"
  },
  {
    "English": "normalisation",
    "context": "1: For the classless LBLs we use noise-contrastive estimation (NCE) (Gutmann & Hyvärinen, 2012;Mnih & Teh, 2012) to avoid <mark>normalisation</mark> during training. This leaves the expensive test-time <mark>normalisation</mark> of LBLs unchanged, precluding their usage during decoding.<br>2: The key obstacle to using CSLMs in a decoder is the expensive <mark>normalisation</mark> over the vocabulary. Our approach to reducing the computational cost of <mark>normalisation</mark> is to use a class-based decomposition of the probabilistic model (Goodman, 2001;Mikolov et al., 2011).<br>",
    "Arabic": "تطبيع",
    "Chinese": "归一化",
    "French": "normalisation",
    "Japanese": "正規化",
    "Russian": "нормализация"
  },
  {
    "English": "normalization",
    "context": "1: In the case of the Scranton problem, we also have to decide which point in which view to relax on the line. Here, we consider:  6. Evaluation of the normalization for the Scranton problem. We have generated 4000 problem-solution pairs, normalized them with a given strategy and tracked HC from every p-s pair to every other.<br>2: <mark>Normalization</mark>: <mark>Normalization</mark> that goes beyond removing vertical structure could be strategic, such as removing the geographical mean (e.g., latitudinal, land/sea structure) or composite seasonal variances (e.g., local smoothed annual cycle) present in the data.<br>",
    "Arabic": "التطبيع",
    "Chinese": "标准化",
    "French": "normalisation",
    "Japanese": "正規化",
    "Russian": "нормализация"
  },
  {
    "English": "normalization constant",
    "context": "1: When n people smoke there are n 2 pairs and, hence, Pr(x) = exp 1.5n 2 /Z, where Z is a <mark>normalization constant</mark>. Let Y consist of all Smokes(x) variables except for Smokes(A), and let y be an assignment to Y in which m people smoke.<br>2: Definition 4 (Unnormalized form of a distribution). Let π be a distribution on a space U. An unnormalized form of π is a functionπ : U → [0, ∞), which equals π up to a <mark>normalization constant</mark> Zπ > 0.<br>",
    "Arabic": "الثابتة التطبيعية",
    "Chinese": "归一化常数",
    "French": "constante de normalisation",
    "Japanese": "正規化定数",
    "Russian": "нормализующая константа"
  },
  {
    "English": "normalization factor",
    "context": "1: p({1, 2}) = ϕ(X n )={1,2} n i=1 p(X i ) = 3 1 a =b∈X p(a) 2 p(b),(1) \n where the <mark>normalization factor</mark> is independent of p. The summation is a monomial symmetric polynomial in the probability values.<br>2: C(x) = Ω e − |y−x| 2 ρ 2 e − |u(y)−u(x)| 2 h 2 \n dy is the <mark>normalization factor</mark> and ρ is now a spatial filtering parameter. In practice, there is no difference between YNF h,ρ and SNF h,ρ .<br>",
    "Arabic": "عامل التطبيع",
    "Chinese": "归一化因子",
    "French": "facteur de normalisation",
    "Japanese": "正規化因子 (seikika inshi)",
    "Russian": "нормализационный фактор"
  },
  {
    "English": "normalization function",
    "context": "1: Its estimate of P (c | d) takes the following exponential form: \n P ME (c | d) := 1 Z(d) exp i λ i,c F i,c (d, c) , \n where Z(d) is a <mark>normalization function</mark>.<br>",
    "Arabic": "وظيفة التطبيع",
    "Chinese": "归一化函数",
    "French": "fonction de normalisation",
    "Japanese": "正規化関数",
    "Russian": "нормализующая функция"
  },
  {
    "English": "normalization layer",
    "context": "1: Features are all normalized to 1 through <mark>normalization layers</mark>, right before they are used to compute the affinity matrix M. We conduct experiments for geometric and semantic correspondences on MPI-Sintel [6], Caltech-UCSD Birds-200-2011 [32] and PAS-CAL VOC [11] with Berkeley annotations [5]. Matching networks.<br>2: Convolution layers, which are the building block of CNNs, project input features to a higher-level representation while preserving their resolution. When composed with non-linearities and <mark>normalization layers</mark>, this allows for learning rich mappings at a constant resolution, e.g. autogressive image synthesis (van den Oord et al., 2016).<br>",
    "Arabic": "طبقة التحييد",
    "Chinese": "归一化层",
    "French": "couche de normalisation",
    "Japanese": "正規化層",
    "Russian": "нормализационный слой"
  },
  {
    "English": "normalization method",
    "context": "1: For example, we have to use a smaller learning rate of 0.00025 or lower to avoid sudden gradient explosion during training. These results suggest possible future work by improving the <mark>normalization method</mark> (Shen et al., 2020;Brock et al., 2021).<br>2: In fact, we only need to specify how the mean and variance (\"moments\") are computed, along the appropriate axes as defined by the <mark>normalization method</mark>.<br>",
    "Arabic": "طريقة التقنين",
    "Chinese": "规范化方法 (Normalization method)",
    "French": "méthode de normalisation",
    "Japanese": "正規化方法",
    "Russian": "метод нормализации"
  },
  {
    "English": "normalization strategy",
    "context": "1: However, advancements such as piecewise linear activation functions (Nair & Hinton, 2010), improved initializations (Glorot & Bengio, 2010), and <mark>normalization strategies</mark> (Ioffe & Szegedy, 2015;Ba et al., 2016) removed the need for pre-training in order to achieve strong results.<br>",
    "Arabic": "إستراتيجية التطبيع",
    "Chinese": "归一化策略",
    "French": "stratégie de normalisation",
    "Japanese": "正規化戦略",
    "Russian": "стратегия нормализации"
  },
  {
    "English": "normalize",
    "context": "1: Correct Answer → taboo Incorrect Answer → cheerful topics Incorrect Answer → rude topics Incorrect Answer → topics that can never be talked about Figure G.1: Formatted dataset example for RACE-h. When predicting, we <mark>normalize</mark> by the unconditional probability of each answer as described in 2. Mrs. Smith is an unusual teacher.<br>",
    "Arabic": "تطبيع",
    "Chinese": "标准化",
    "French": "normaliser",
    "Japanese": "正規化",
    "Russian": "нормализовать"
  },
  {
    "English": "normalize cross correlation",
    "context": "1: For the \"photometric\" baseline, we use RGB images (while Woodford et al. [88] use grayscale images), we warp patches of 4×4 pixels at the featuremap resolution (1600 pixels in the longest dimension) with fronto-parallel assumption, and apply <mark>normalized cross correlation</mark> (NCC).<br>",
    "Arabic": "تطبيع الارتباط المتبادل",
    "Chinese": "归一化互相关",
    "French": "corrélation croisée normalisée",
    "Japanese": "正規化相互相関",
    "Russian": "нормализованная кросс-корреляция"
  },
  {
    "English": "normalize cut",
    "context": "1: The 3-D motion segmentation problem has received relatively less attention. Existing approaches include combinations of EM with <mark>normalized cuts</mark> [8] and factorization methods for orthographic and affine cameras [10,11].<br>2: One approach is to search for quotient cuts (e.g., <mark>normalized cuts</mark> [12]) which score a cut as the ratio between the cost of the cut and the size of the resulting clusters.<br>",
    "Arabic": "القص الطبيعي",
    "Chinese": "归一化切割",
    "French": "\"coupe normalisée\"",
    "Japanese": "正規化カット",
    "Russian": "нормализованный разрез"
  },
  {
    "English": "normalize cut algorithm",
    "context": "1: W in the clustering algorithms . For baselines, we again use ACC and IND, except that this time the <mark>normalized cut algorithm</mark> is used. For our algorithms, we use the NC-based PCQ and PCM. Figures 9 ( a ) , ( b ) , and ( c ) give the CSNC , CTNC , and CostNC for the two baseline algorithms and the PCM algorithm ( to make the figures readable , we did not plot the results for PCQ , which are similar to those of PCM , The performance on the NEC data , which shows<br>",
    "Arabic": "الخوارزمية القطعية المعيارية",
    "Chinese": "归一化切割算法",
    "French": "algorithme de coupure normalisée",
    "Japanese": "正規化カットアルゴリズム",
    "Russian": "алгоритм нормализованного разреза"
  },
  {
    "English": "normalize edit distance",
    "context": "1: In order to improve the quality of the data, we applied a letterto-phoneme model to both the original words and their respellings, and removed pairs with divergent pronunciations (computed as <mark>normalized edit distance</mark> ≤ 0.8).<br>2: Table 11 shows the relationship between mean % length reduction from input text to model output and the level of factuality errors present in the example. Table 12 likewise shows the relationship between <mark>normalized edit distance</mark> between inputs and model outputs and factuality annotations.<br>",
    "Arabic": "مسافة التحرير المعيارية",
    "Chinese": "规范化编辑距离",
    "French": "distance d'édition normalisée",
    "Japanese": "正規化された編集距離",
    "Russian": "нормализованное расстояние редактирования"
  },
  {
    "English": "normalize factor",
    "context": "1: in linear time using Algorithm 2, we obtain ⟨Z, p∈P(i,N ) |g(p)| log |g(p)|⟩, which are the <mark>normalizing factor</mark> and the unnormalized entropy of the graph, respectively. As shown by Li and Eisner (2009), we can then compute \n<br>2: where g(p) ≜ (j,k)∈p dv k dv j is the gradient of path p and Z = p∈P(i,N ) |g(p)| is a <mark>normalizing factor</mark>.<br>",
    "Arabic": "عامل التطبيع",
    "Chinese": "归一化因子",
    "French": "facteur de normalisation",
    "Japanese": "正規化係数",
    "Russian": "нормализующий фактор"
  },
  {
    "English": "normalize flow",
    "context": "1: Parametric methods consist of a <mark>normalizing flow</mark> in the Euclidean space R n , pushed-forward onto the manifold through an invertible map ψ : R n → M. However, to globally represent the manifold, ψ needs to be a homeomorphism implying that M and R n are topologically equivalent, limiting the scope of that Frequency k = 10 \n<br>2: In this section, we shall present an entropy estimation approach that is based on <mark>normalizing flow</mark>. As is mentioned earlier, it consists of two main ingredients: a truncated version of the k-NN entropy estimators, and a transformation that can map data points toward a uniform distribution.<br>",
    "Arabic": "تطبيع التدفق",
    "Chinese": "归一化流",
    "French": "flux de normalisation",
    "Japanese": "正規化フロー",
    "Russian": "нормализующий поток"
  },
  {
    "English": "noun phrase",
    "context": "1: where l(c) is the label of c (e.g., whether it is a <mark>noun phrase</mark> (np), verb phrase, etc.) and H(c) is the relevant history of c -information outside c that our probability model deems important in determining the probability in question.<br>2: We believe we would get even better results if the parser could determine the true branching structure. We then adopt the following definition of a grandparent-head feature j. 1. if c is a <mark>noun phrase</mark> under a prepositional phrase , or is a pre-terminal which takes a revised head as defined above , then j is the grandparent head of c , else 2. if c is a pre-terminal and is not next ( in the production generating c ) to the head of its parent ( i ) then j (<br>",
    "Arabic": "عبارة اسمية",
    "Chinese": "名词短语",
    "French": "syntagme nominal",
    "Japanese": "名詞句",
    "Russian": "именная группа"
  },
  {
    "English": "novel view synthesis",
    "context": "1: [61] propose Neural Radiance Fields (NeRFs) in which they combine an implicit neural model with volume rendering for <mark>novel view synthesis</mark> of complex scenes. Due to their expressiveness, we use a generative variant of NeRFs as our object-level representation.<br>2: Computer vision methods can now produce freeviewpoint renderings of static 3D scenes with spectacular quality. What about moving scenes, like those featuring people or pets? Novel view synthesis from a monocular video of a dynamic scene is a much more challenging dynamic scene reconstruction problem.<br>",
    "Arabic": "تخليق رؤية جديدة",
    "Chinese": "新视图合成",
    "French": "synthèse de nouvelles vues",
    "Japanese": "新規ビュー合成",
    "Russian": "синтез новых взглядов"
  },
  {
    "English": "nsubj",
    "context": "1: 1 the hexatag generated while processing she is: ⟨ → , <mark>nsubj</mark>⟩.<br>2: The QLF for a sentence is the conjunction of the atoms for the nodes and edges, e.g., the sentence above will become borders(n \n 1 ) ∧ Utah(n 2 ) ∧ Idaho(n 3 ) ∧ <mark>nsubj</mark>(n 1 , n 2 ) ∧ dobj(n 1 , n 3 ).<br>",
    "Arabic": "فاعل",
    "Chinese": "主语",
    "French": "nsubj",
    "Japanese": "主語",
    "Russian": "nsubj"
  },
  {
    "English": "nsubjpass",
    "context": "1: (2006) dependencies used in these sets are deterministically mapped to the comparable numeric relations used by the current system. 9 Thus, the dependencies 'nsubj' and '<mark>nsubjpass</mark>' are mapped to a '1' relation, and 'dobj', 'pobj', and 'obj2' are mapped to a '2' relation.<br>",
    "Arabic": "نفعول به باسيف",
    "Chinese": "主动主语",
    "French": "Complément d'objet passif",
    "Japanese": "受動主語",
    "Russian": "подлежащее в страдательном залоге"
  },
  {
    "English": "nuclear norm",
    "context": "1: The first one corresponds to a <mark>nuclear norm</mark> regularized optimization, which is known to enforce a low rank constraint on H Z . In a sense, this choice can be justified in view of Theorem 1 when the target is known to be generated by some WFA.<br>2: Moreover in our case, since Q k is a symmetric positive definite matrix, the <mark>nuclear norm</mark> is simply its trace [23][4]. Thus we have Q k * = trace(Q k ). Then we arrive at the following trace-minimization to solve for the corrective matrix (Q k ).<br>",
    "Arabic": "معيار النواة",
    "Chinese": "核范数",
    "French": "norme nucléaire",
    "Japanese": "核ノルム",
    "Russian": "ядерная норма"
  },
  {
    "English": "nuclear norm relaxation",
    "context": "1: This approach only requires O(dr) memory, and a single gradient iteration takes time O(|Ω|), so has much lower memory requirement and computational complexity than the <mark>nuclear norm relaxation</mark>. On the other hand, the factorization causes the optimization problem to be non-convex in X, which leads to theoretical difficulties in analyzing algorithms.<br>",
    "Arabic": "الاسترخاء بالقيمة النووية",
    "Chinese": "核范数松弛",
    "French": "relaxation de la norme nucléaire",
    "Japanese": "核ノルム緩和",
    "Russian": "ядерная нормная релаксация"
  },
  {
    "English": "nucleus sampling",
    "context": "1: 10 We have also tried other pre-trained language models like Bart (Lewis et al., 2020), and preliminary experiments indicate that initializing our model with T5 leads to better performances. 11 There are several common sampling strategies like Greedy Search, Beam Search, and <mark>Nucleus Sampling</mark> (Holtzman et al., 2019).<br>",
    "Arabic": "عيِّنة النواة",
    "Chinese": "核采样",
    "French": "échantillonnage du noyau",
    "Japanese": "核サンプリング",
    "Russian": "сэмплирование ядра"
  },
  {
    "English": "null distribution",
    "context": "1: In practice, simulating from the asymptotic <mark>null distribution</mark> in Claim 1 can be challenging, since the plug-in estimator of Σ p requires a sample from p, which is not available.<br>2: Since obtaining <mark>null distribution</mark> is expensive for Kulldorff's spatial scan statistic, several methods have been proposed for addressing the performance issue [9,8,1,2]. These methods differ from our own in that they aim to actually avoid considering all O(n 4 ) rectangular areas, whereas our goal is to avoid expensive LRT statistic computations.<br>",
    "Arabic": "التوزيع الصفري",
    "Chinese": "零分布",
    "French": "distribution nulle",
    "Japanese": "ヌル分布",
    "Russian": "нулевое распределение"
  },
  {
    "English": "null space",
    "context": "1: The model updates towards the mean of all rankers that beat the current model. -NSGD [22]: Multiple directions are sampled from the <mark>null space</mark> of previously poorly performing gradients. Ties are broken by evaluating the tied candidate rankers on a recent set of difficult queries.<br>",
    "Arabic": "الفضاء المعدوم",
    "Chinese": "零空间",
    "French": "noyau",
    "Japanese": "ゼロ空間",
    "Russian": "нулевое пространство"
  },
  {
    "English": "numerical linear algebra",
    "context": "1: , 2016 ) , text and video summarization ( Gong et al. , 2014 ) , and others ( Kulesza & Taskar , 2012 ) . They have also played an important role in randomized <mark>numerical linear algebra</mark> . Given a p.s.d.<br>",
    "Arabic": "الجبر الخطي العددي",
    "Chinese": "数值线性代数",
    "French": "algèbre linéaire numérique",
    "Japanese": "数値線形代数",
    "Russian": "численная линейная алгебра"
  },
  {
    "English": "object bounding box",
    "context": "1: 2014;Endres and Hoiem 2014) generally fail to detect abnormal objects. As a result, we assume that <mark>object bounding boxes</mark> are given in the image. Through our experiments, we convert confidences of classifiers (e.g. attribute classifiers) to the probability by using Platt's method (Platt 1999).<br>",
    "Arabic": "صندوق تحديد الكائنات",
    "Chinese": "目标边界框",
    "French": "boîte englobante d'objet",
    "Japanese": "オブジェクトの境界ボックス",
    "Russian": "ограничивающая рамка объекта"
  },
  {
    "English": "object categorization",
    "context": "1: In computer vision, abnormality in the context of object and scene categorization is an under-studied problem. In contrast to humans ability to generalize and successfully categorize atypical instances (Rosch and Mervis 1975;Keselman and Dickinson 2005), state-of-the-art computer vision algorithms fail to achieve similar generalization.<br>",
    "Arabic": "تصنيف الأجسام",
    "Chinese": "目标分类",
    "French": "catégorisation d'objets",
    "Japanese": "オブジェクト分類",
    "Russian": "категоризация объектов"
  },
  {
    "English": "object category",
    "context": "1: OVD assumes the availability of coarse-grained image-caption pairs, and attempts to generalize from limited fine-grained annotation of <mark>object categories</mark> to unseen ones. Nevertheless, this line of work positions words as <mark>object categories</mark> and isolates them from their linguistic context (e.g., sentences). Our setup instead challenges models to perform language modeling in human-generated captions.<br>2: k P (O k ) * ( i I(A o i |O k ) * Υ(A o i ) * Ω(A o i , O k )) (4) \n Where P (O k ) is the distribution over <mark>object categories</mark> obtained from low-level visual features.<br>",
    "Arabic": "فئة الكائنات",
    "Chinese": "物体类别",
    "French": "catégorie d'objet",
    "Japanese": "オブジェクトカテゴリ",
    "Russian": "категория объектов"
  },
  {
    "English": "object category recognition",
    "context": "1: Recent years have seen great progress in the area of <mark>object category recognition</mark> for natural images. Recognition rates beyond 95% are the rule rather than the exception on many datasets. However, in their basic form, many state-ofthe-art methods only solve a binary classification problem.<br>",
    "Arabic": "التعرف على فئة الكائن",
    "Chinese": "物体类别识别",
    "French": "reconnaissance de catégories d'objets",
    "Japanese": "物体カテゴリ認識",
    "Russian": "распознавание категорий объектов"
  },
  {
    "English": "object class",
    "context": "1: While mean average precision over the full set of 100,000 <mark>object classes</mark> is around 0.16 due in large part to the challenges in gathering training data and collecting ground truth for so many classes, we achieve a mAP of at least 0.20 on a third of the classes and 0.30 or better on about 20% of the classes.<br>2: Each anchor is assigned a length K one-hot vector of classification targets, where K is the number of <mark>object classes</mark>, and a 4-vector of box regression targets. We use the assignment rule from RPN [28] but modified for multiclass detection and with adjusted thresholds.<br>",
    "Arabic": "فئة الكائنات",
    "Chinese": "目标类别",
    "French": "classe d'objet",
    "Japanese": "オブジェクトクラス",
    "Russian": "класс объектов"
  },
  {
    "English": "object classification",
    "context": "1: We perform fivefold cross validation to find the best values for parameters of the SVM. This achieves in 87.46% average precision for the task of <mark>object classification</mark> in PASCAL2010 test set. Object classification in abnormal images is extremely challenging and state-of-the-art approaches cannot generalize to atypical objects (see Table 1).<br>2: Standard <mark>object classification</mark> tasks ignore the impact of impostors that are not represented by any of the object categories. These open sets started getting attention in face recognition tasks, where some test exemplars did not appear in the training database and had to be rejected [28].<br>",
    "Arabic": "تصنيف الكائنات",
    "Chinese": "物体分类",
    "French": "classification d'objets",
    "Japanese": "オブジェクト分類",
    "Russian": "классификация объектов"
  },
  {
    "English": "object detection",
    "context": "1: <mark>Object detection</mark>s can also serve to enhance rather than inhibit other detections within a scene. This has been an area of active research in object recognition over the last few years (Torralba et al. 2004;Murphy et al. 2003;Galleguillos et al. 2008;He et al. 2004;Hoiem et al.<br>",
    "Arabic": "الكشف عن الكائنات",
    "Chinese": "目标检测",
    "French": "détection d'objets",
    "Japanese": "物体検出",
    "Russian": "обнаружение объектов"
  },
  {
    "English": "object detector",
    "context": "1: While W2W produces correct predictions of the missing words as well as the locations of the corresponding bounding boxes, it turns out to be challenging for baselines to achieve them both. For \"Detect-and-Recognize\" baseline (VisualBERT), we observe a comparable object localization performance empowered by the frozen <mark>object detector</mark>.<br>2: parts of speech and automatically learn which ones are noninformative and lead to uniform distributions . We use an off-the-shelf <mark>object detector</mark> (Felzenszwalb et al., 2010a;Felzenszwalb et al., 2010b) which outputs detections in the form of scored axis-aligned rectangles.<br>",
    "Arabic": "مكتشف الكائنات",
    "Chinese": "物体检测器",
    "French": "détecteur d'objets",
    "Japanese": "オブジェクト検出器",
    "Russian": "детектор объектов"
  },
  {
    "English": "object domain",
    "context": "1: While certain forms of aggregation can be simulated by iterating over the <mark>object domain</mark>, as in our examples in Section 3, such a solution may be too cumbersome for practical use, and it relies on the existence of a linear order over the <mark>object domain</mark>, which is a strong theoretical assumption.<br>",
    "Arabic": "نطاق الكائنات",
    "Chinese": "对象域",
    "French": "domaine d'objets",
    "Japanese": "オブジェクト領域",
    "Russian": "объектная область"
  },
  {
    "English": "object embedding",
    "context": "1: Unlike the original deformable attention layer [54] that predicts the attention weights by linear projection of the <mark>object embedding</mark>, we adopt the full Q-K dot-product attention with positional encoding.<br>2: The multimodal representation is then forwarded into a cross-encoder with selfattention layers. The cross-encoded representations in the final layer are sent into an object decoder, together with a set of learnable object queries. The object decoder produces an <mark>object embedding</mark> for each input object query, which can be considered as a representation of the proposed object.<br>",
    "Arabic": "تضمين الكائنات",
    "Chinese": "物体嵌入",
    "French": "Intégration d'objet",
    "Japanese": "オブジェクト埋め込み",
    "Russian": "объектное встраивание"
  },
  {
    "English": "object instance segmentation",
    "context": "1: Our work is also related to <mark>object instance segmentation</mark>. Most of these ap-proaches [14,24,36,34,20,21] operate on the pixel-level, typically exploiting a CNN inside a box or a patch to perform the labeling. Work most related to ours is [35,28] which aims to produce a polygon around an object.<br>2: Semantic image segmentation has been receiving significant attention in the community [5,17]. With new benchmarks such as Cityscapes [6], <mark>object instance segmentation</mark> is also gaining steam [14,24,34,21,29]. Most of the recent approaches are based on neural networks, achieving impressive performance for these tasks [5,17,10,21].<br>",
    "Arabic": "تقسيم مثيلات الكائنات",
    "Chinese": "目标实例分割",
    "French": "segmentation d'instances d'objets",
    "Japanese": "オブジェクトインスタンスのセグメンテーション",
    "Russian": "сегментация экземпляров объектов"
  },
  {
    "English": "object localization",
    "context": "1: Words in groundable phrases are masked with a probability of 0.4 and those in non-groundable regions are masked with a lower probability of 0.1. <mark>Object Localization</mark> (OL). Each object representation will be decoded by a shared three-layer MLP to produce a bounding box.<br>",
    "Arabic": "تحديد مكان الكائن",
    "Chinese": "目标定位",
    "French": "localisation d'objets",
    "Japanese": "オブジェクトのローカリゼーション",
    "Russian": "Локализация объектов"
  },
  {
    "English": "object model",
    "context": "1: By performing this nonlinear embedding in a high-dimensional space, we are able to simplify the hashing process and apply large-scale linear solvers in training <mark>object models</mark>. Furthermore, this hashing scheme can be implemented exactly in the integer domain, a desirable property for achieving high performance. Are there really 100,000 objects to concern ourselves with? Humans are able to discriminate among around 30,000 visual categories [ 2 ] , but many more appearances and specific instances of classes , in much the same way as adult vocabularies are estimated to be on the order of 20,000 words for an English speaker but much larger when one considers all the proper nouns , hyphenated words and arcana related to specific<br>2: We use visual phrase and <mark>object models</mark> to make independent predictions. We then combine the predictions by a decoding algorithm that takes all detection responses and decides on the final outcome. Note that a) Visual phrase recognition works better than recognizing the participating objects.<br>",
    "Arabic": "نموذج الكائن",
    "Chinese": "物体模型",
    "French": "modèle d'objet",
    "Japanese": "オブジェクトモデル",
    "Russian": "модель объекта"
  },
  {
    "English": "object proposal",
    "context": "1: To generate <mark>object proposals</mark>, we run a slightly modified version of our automatic mask generation pipeline and output the masks as proposals (see §D.3 for details). We compute the standard average recall (AR) metric on LVIS v1 [44]. We focus on LVIS because its large number of categories presents a challenging test.<br>",
    "Arabic": "اقتراح الكائنات",
    "Chinese": "目标提案",
    "French": "proposition d'objet",
    "Japanese": "オブジェクト提案",
    "Russian": "предложение объекта"
  },
  {
    "English": "object recognition",
    "context": "1: In this paper we propose a novel nonparametric approach for <mark>object recognition</mark> and scene parsing using dense scene alignment. Given an input image, we retrieve its best matches from a large database with annotated images using our modified, coarse-to-fine SIFT flow algorithm that aligns the structures within two images.<br>2: Since the micro-mirror array is programmable, the above geometric and radiometric manipulations can be varied to create a wide range of transformations of the light field of the scene to captured images. We will show that the radiometric modulation enables us to achieve several functions including high dynamic range imaging, optical feature detection, and <mark>object recognition</mark> using appearance matching.<br>",
    "Arabic": "التعرف على الكائنات",
    "Chinese": "物体识别",
    "French": "reconnaissance d'objets",
    "Japanese": "物体認識",
    "Russian": "распознавание объектов"
  },
  {
    "English": "object segmentation",
    "context": "1: We propose an approach for semi-automatic annotation of object instances. While most current methods treat <mark>object segmentation</mark> as a pixel-labeling problem, we here cast it as a polygon prediction task, mimicking how most current datasets have been annotated. In particular, our approach takes as input an image crop and sequentially produces vertices of the polygon outlining the object.<br>2: Several methods have been proposed to reconstruct sparse geometry of a dynamic scene [27,50,36,40]. Russell et al. [31] and Ranftl et al. [29] suggest motion/<mark>object segmentation</mark> based algorithms to decompose a dynamic scene into piecewise rigid parts.<br>",
    "Arabic": "تقسيم الكائنات",
    "Chinese": "目标分割",
    "French": "segmentation d'objets",
    "Japanese": "物体セグメンテーション",
    "Russian": "сегментация объектов"
  },
  {
    "English": "object tracking",
    "context": "1: A 360 degrees single PAL camera-based system is presented in [8], where authors provide both the driver's face pose and eye status and the driver's viewing scene basing on a machine learning algorithm for <mark>object tracking</mark>.<br>2: The problem of graph matching -establishing correspondences between two graphs represented in terms of both local node structure and pair-wise relationships , be them visual , geometric or topological -is important in areas like combinatorial optimization , machine learning , image analysis or computer vision , and has applications in structure-from-motion , <mark>object tracking</mark> , 2d and 3d shape matching , image classification ,<br>",
    "Arabic": "تتبع الكائنات",
    "Chinese": "目标跟踪",
    "French": "suivi d'objet",
    "Japanese": "物体追跡",
    "Russian": "отслеживание объектов"
  },
  {
    "English": "objective function",
    "context": "1: Although our algorithm is relatively simple, its stochastic nature and the non-convexity of the <mark>objective function</mark> make the proof of its convergence to a stationary point somewhat involved.<br>2: Consider the following <mark>objective function</mark> max K 0 tr(KA) and limit the trace norm of K to avoid the <mark>objective function</mark> from growing unboundedly. We claim that this <mark>objective function</mark> attempts to recover a low-rank version of spectral embedding. Lemma 1.<br>",
    "Arabic": "وظيفة الهدف",
    "Chinese": "目标函数",
    "French": "fonction objective",
    "Japanese": "目的関数",
    "Russian": "объективная функция"
  },
  {
    "English": "objective value",
    "context": "1: This implies the satisfaction of several key properties [1]. First, we define f (X ) as the solution (the minimal <mark>objective value</mark>) of the minmax problem (2) on data X , and θ(X ) as the corresponding globally optimal estimate.<br>2: Consider only the first iteration of this algorithm (due to the monotonicity of the algorithm, we will only improve the <mark>objective value</mark>). We are then guaranteed to obtain a setX such that (denote X 1 as the solution after the first iteration) \n<br>",
    "Arabic": "قيمة الهدف",
    "Chinese": "目标值",
    "French": "valeur objective",
    "Japanese": "目的関数値",
    "Russian": "целевое значение"
  },
  {
    "English": "objectness",
    "context": "1: cases . Alternative approaches to reducing L advocate the use of interest points in the form of jumping windows [19], salience operators like <mark>objectness</mark> [1,14], and segmentations [18,9] as cues for generating object-like window proposals, thus pruning out most of the background regions that a naive approach like sliding window cannot avoid.<br>2: We propose a mechanism for jointly training on classification and detection data. Our method uses images labelled for detection to learn detection-specific information like bounding box coordinate prediction and <mark>objectness</mark> as well as how to classify common objects. It uses images with only class labels to expand the number of categories it can detect.<br>",
    "Arabic": "موضوعية",
    "Chinese": "目标性",
    "French": "indice d'objet",
    "Japanese": "オブジェクトネス",
    "Russian": "объектность"
  },
  {
    "English": "observation function",
    "context": "1: We consider a partially observable setting, in which agents draw observations z ∈ Z according to the <mark>observation function</mark> O(s, a) : S × A → Z.<br>",
    "Arabic": "دالة المراقبة",
    "Chinese": "观测函数",
    "French": "fonction d'observation",
    "Japanese": "観測関数",
    "Russian": "функция наблюдения"
  },
  {
    "English": "observation model",
    "context": "1: In Section 2.1., we will give detailed description on how to incorporate multiple cues into the <mark>observation model</mark>. A simplified state transition model is then discussed in Section 2.2 based on the contour smoothness constraint.<br>2: where the <mark>observation model</mark> p(u t |s t ) is a Gaussian distribution. Notice that by pushing the <mark>observation model</mark> into the exponent as log p(u t |s t ) it can also be interpreted as an auxiliary 'observation feature' with an implicit weight of one, θ o = 1.<br>",
    "Arabic": "نموذج الملاحظة",
    "Chinese": "观测模型",
    "French": "modèle d'observation",
    "Japanese": "観測モデル",
    "Russian": "модель наблюдения"
  },
  {
    "English": "observation space",
    "context": "1: Our <mark>observation space</mark> contains multiple modalities. The agent perceives the world mainly through the RGB screen. To provide the same information as human players receive, we also supplement the agent with observations about its inventory, location, health, surrounding blocks, etc. The full <mark>observation space</mark> is shown below.<br>",
    "Arabic": "مساحة الملاحظة",
    "Chinese": "观测空间",
    "French": "espace d'observation",
    "Japanese": "観測空間",
    "Russian": "пространство наблюдений"
  },
  {
    "English": "observational datum",
    "context": "1: It is typically very hard to obtain the ground-truth counterfactual data as only one of the two potential outcomes can be obtained in the <mark>observational data</mark>. Hence, in this section, we follow a standard practice to evaluate the proposed framework and the alternative approaches on three semi-synthetic datasets.<br>",
    "Arabic": "بيانات مُلاحَظة",
    "Chinese": "观测数据",
    "French": "donnée observationnelle",
    "Japanese": "観測データ",
    "Russian": "наблюдательные данные"
  },
  {
    "English": "occlusion handling",
    "context": "1: However, pairwise matching approaches typically do not incorporate temporal context, which can lead to inconsistent tracking over long videos and poor <mark>occlusion handling</mark>. In contrast, our method produces smooth trajectories through occlusions. Pixel-level long-range tracking.<br>",
    "Arabic": "معالجة الانسداد",
    "Chinese": "遮挡处理",
    "French": "gestion des occlusions",
    "Japanese": "遮蔽処理",
    "Russian": "обработка окклюзий"
  },
  {
    "English": "occlusion reasoning",
    "context": "1: [16] estimate general 2D motion by decomposition into several layers, which enables <mark>occlusion reasoning</mark>. Unger et al. [17] compute optical flow by parameterizing the motion per segment with 2D affine transformations, and also perform occlusion handling.<br>2: We have shown that modeling a dynamic scene with local regions corresponding to rigidly moving planes can lead to compelling results for the task of joint geometry and 3D  motion estimation. The proposed model achieves accurate geometry and motion boundaries by refining an initial oversegmentation of the scene, and allows for <mark>occlusion reasoning</mark>.<br>",
    "Arabic": "استنتاج الإخفاء",
    "Chinese": "遮挡推理",
    "French": "raisonnement sur l'occlusion",
    "Japanese": "遮蔽推論",
    "Russian": "рассуждение об окклюзии"
  },
  {
    "English": "occupancy grid",
    "context": "1: Navigation agents with vision can perform PointNav near-perfectly (Wijmans et al., 2020) and thus there isn't room for improving, rendering this experiment infeasible. As a supplement to this experiment, we also show that a metric map (top-down <mark>occupancy grid</mark>) can be decoded from the agents memory.<br>",
    "Arabic": "شبكة الإشغال",
    "Chinese": "占用栅格",
    "French": "grille d'occupation",
    "Japanese": "占有グリッド",
    "Russian": "сетка занятости"
  },
  {
    "English": "occupancy map",
    "context": "1: Meanwhile, some motion forecasting methods implicitly include the planning task by producing their future trajectories simultaneously [12,45,70]. Similarly, we encode possible behaviors of the ego vehicle in the scene-centric motion forecasting module, but the interpretable <mark>occupancy map</mark> is utilized to further optimize the plan to stay safe.<br>2: whereτ is the original planning prediction, τ * denotes the optimized planning, which is selected from multipleshooting [3] trajectories τ as to minimize cost function f (•). O is a classical binary <mark>occupancy map</mark> merged from the instance-wise occupancy prediction from OccFormer. The cost function f (•) is calculated by: \n<br>",
    "Arabic": "خريطة الإشغال",
    "Chinese": "占用图",
    "French": "carte d'occupation",
    "Japanese": "占有マップ",
    "Russian": "карта занятости"
  },
  {
    "English": "occupancy measure",
    "context": "1: We start with a moral principle ρ(π) using a deterministic or stochastic policy π(s) or π(a|s). First, recall that the discounted number of times that an action a ∈ A is performed in a state s ∈ S is an <mark>occupancy measure</mark> µ s a .<br>",
    "Arabic": "مقياس الإشغال",
    "Chinese": "占用度量",
    "French": "mesure d'occupation",
    "Japanese": "占有度",
    "Russian": "мера занятости"
  },
  {
    "English": "odometry",
    "context": "1: In addition, we computed <mark>odometry</mark> measurements from the GPS trajectories (entry \"G\" in the table) and ran our algorithm using the learned parameters from the stereo data. Note that this does not have access to absolute positions, but only relative position and orientation with respect to the previous frame.<br>2: Lastly the power board includes a Teensy MCU in order to provide a simple interface to sensors such as wheel encoders and add-ons such as RF receivers for long range remote control. Odometry: Precise <mark>odometry</mark> is critical for path planing, mapping, and localization.<br>",
    "Arabic": "المسافة المقطوعة",
    "Chinese": "里程计",
    "French": "odométrie",
    "Japanese": "オドメトリ",
    "Russian": "одометрия"
  },
  {
    "English": "off-diagonal element",
    "context": "1: It also describes a non-classical distribution over the set of sememes: the complex-valued <mark>off-diagonal elements</mark> describes the correlations between sememes, while the diagonal entries (guaranteed to be real by its original property) correspond to a standard probability distribution.<br>",
    "Arabic": "عنصر خارج القطر الرئيسي",
    "Chinese": "非对角线元素",
    "French": "Élément hors-diagonale",
    "Japanese": "非対角要素",
    "Russian": "внедиагональный элемент"
  },
  {
    "English": "off-policy",
    "context": "1: Figure 15 shows that the additional replay data is not responsible for the performance improvements we see for L = 4. In fact, we find that increasing the amount of replay data in the meta-objective exacerbates <mark>off-policy</mark> issues and leads to reduced performance. It is striking that BMG can make use of this extra <mark>off-policy</mark> data.<br>2: In Figure 14, we test the effect of using only replay for all L steps and find that having online data in the Lth update step is critical. These results indicate that BMG can make effective use of replay by simulating the effect of the meta-learned update rule on <mark>off-policy</mark> data and correct for potential bias using online data.<br>",
    "Arabic": "خارج السياسة",
    "Chinese": "离线策略 (off-policy)",
    "French": "hors-politique",
    "Japanese": "オフポリシー",
    "Russian": "off-policy - внеполитическая"
  },
  {
    "English": "offline algorithm",
    "context": "1: In particular, we show that the existence an algorithms which computes an (O(1), k)-cov-sketch in time o(nnz(A)k) implies a breakthrough in matrix multiplication, which is likely very difficult. In fact, the lower bound holds even for <mark>offline algorithms</mark> without constraints on working space.<br>",
    "Arabic": "خوارزمية غير متصلة",
    "Chinese": "离线算法",
    "French": "algorithme hors ligne",
    "Japanese": "オフラインアルゴリズム",
    "Russian": "алгоритм оффлайн"
  },
  {
    "English": "offline learning",
    "context": "1: Learning an XMC algorithm from bandit feedback requires <mark>offline learning</mark> from slates Y , where each element of the slate comes from a large action space. Swaminathan et al. (2017) proposes a pseudo-inverse estimator for <mark>offline learning</mark> from combinatorial bandits.<br>",
    "Arabic": "التعلم دون اتصال بالإنترنت",
    "Chinese": "离线学习",
    "French": "apprentissage hors ligne",
    "Japanese": "オフライン学習",
    "Russian": "обучение в автономном режиме"
  },
  {
    "English": "on-policy",
    "context": "1: We compare several methods for generating prior knowledge in 9 × 9 Go. First, we use an even-game heuristic, Q even (s, a) = 0.5, to indicate that most positions encountered <mark>on-policy</mark> are likely to be close.<br>2: Its form depends on the algorithm; in Section 5.1, we generate rollouts from π x (<mark>on-policy</mark>), in which case ρ t = 1 and G (n) t = (n−1) i=0 γ i r t+i+1 + γ n vz(s t+n ) ∀t, wherez denotes fixed (non-differentiable) parameters.<br>",
    "Arabic": "السياسة الحالية",
    "Chinese": "同策略",
    "French": "sur-politique",
    "Japanese": "オンポリシー",
    "Russian": "политика"
  },
  {
    "English": "one-against-all reduction",
    "context": "1: AdaBoost.MH (Schapire and Singer, 1999) is a popular multiclass boosting algorithm that is based on the <mark>one-against-all reduction</mark>, and was originally designed to use weak-hypotheses that return a prediction for every example and every label.<br>2: This combined condition can also be expressed in our framework. With this understanding, we are able to characterize previously studied weak-learning conditions. In particular, the condition implicitly used by AdaBoost.MH (Schapire and Singer, 1999), which is based on a <mark>one-against-all reduction</mark> to binary, turns out to be strictly stronger than necessary for boostability.<br>",
    "Arabic": "التقليص من واحد ضد الجميع",
    "Chinese": "一对所有减少",
    "French": "réduction un contre tous",
    "Japanese": "一対すべての削減 (one-against-all reduction)",
    "Russian": "один-против-всех сведение"
  },
  {
    "English": "one-hot encode",
    "context": "1: We restrict our study to D-dimensional binary x ∈ {0, 1} D and categorical x ∈ {0, 1, . . . , K} D data as all finite-dimensional discrete distributions can be embedded in this way. Throughout the paper we assume all categorical variables are <mark>one-hot encoded</mark>.<br>2: As the inputs to these systems are sequences of characters or phones, and as these sequences are typically <mark>one-hot encoded</mark>, it can be difficult to devise a principled method for transferring weights from the source language network to the target if there is a difference between the character or phone inventories of the two languages.<br>",
    "Arabic": "ترميز واحد ساخن",
    "Chinese": "独热编码",
    "French": "encodage one-hot",
    "Japanese": "ワンホットエンコード",
    "Russian": "одно-горячее кодирование"
  },
  {
    "English": "one-hot representation",
    "context": "1: Then the easiest representation to read-out from is the <mark>one-hot representation</mark>, since in all other situations the vectors are closer together which require larger read-out weights to disambiguate. Alternatively, if there is noise on z then the categories will be more often confused if the z c vectors are closer to each other.<br>2: It is a common practice to represent each word in the input sentence, x = [w 1 , w 2 , • • • , w N ] as an embedding vector based on its <mark>one-hot representation</mark>. Instead , we adopt LSH projection layer from ( Ravi , 2017 ( Ravi , , 2019 which dynamically generates a T bit representation , P ( w i ) ∈ [ 0 , 1 ] T for the input word , w i based on its morphological features like n-grams , skip-grams from the current and context words , parts-of-speech tags , etc<br>",
    "Arabic": "التمثيل الساخن الوحيد",
    "Chinese": "独热表示法",
    "French": "représentation one-hot",
    "Japanese": "ワンホット表現",
    "Russian": "одно-горячее представление"
  },
  {
    "English": "one-hot vector",
    "context": "1: The unit state |e j can be seen as a <mark>one-hot vector</mark>, i.e., the j-th element in |e j is one while other elements are zero, in order to obtain a set of orthogonal unit states. Semantic units with larger granularities are based on the set of sememe basis. Words. Words are composed of sememes in superposition.<br>2: For each mutable feature (i.e., or the <mark>one-hot vector</mark> with ∈ K), we aim to learn a local feature-based perturbation distribution ( | ) where ∈ O , while leaving the immutable features unchanged.<br>",
    "Arabic": "متجه ثنائي الحالة",
    "Chinese": "一热向量",
    "French": "vecteur one-hot",
    "Japanese": "ワンホットベクトル",
    "Russian": "one-hot вектор"
  },
  {
    "English": "one-shot learning",
    "context": "1: Siamese networks. Siamese networks [4] are general models for comparing entities. Their applications include signature [4] and face [34] verification, tracking [3], <mark>one-shot learning</mark> [23], and others.<br>2: We simulate this setting in SRL by training our model on 100% of the training data available for the English language, while keeping the <mark>one-shot learning</mark> setting for all the other languages.<br>",
    "Arabic": "التعلم من مثال واحد",
    "Chinese": "一次性学习",
    "French": "apprentissage en un coup",
    "Japanese": "ワンショット学習",
    "Russian": "одноразовое обучение"
  },
  {
    "English": "one-shot setting",
    "context": "1: None of the models can reverse the letters in a word. In the <mark>one-shot setting</mark>, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10).<br>2: See Fig. 2 for a qualitative comparison. In a two-shot setting (see Fig. 7 for reference views), we succeed in reconstructing any part of the object that has been observed, achieving 24.36 dB, while the dGQN achieves 18.56 dB. In a <mark>one-shot setting</mark>, SRNs reconstruct an object consistent with the observed view.<br>",
    "Arabic": "إعداد طلقة واحدة",
    "Chinese": "单次设置",
    "French": "réglage à un coup",
    "Japanese": "単回設定",
    "Russian": "одноразовый сеттинг"
  },
  {
    "English": "one-stage detector",
    "context": "1: Experiments show that our proposed Focal Loss enables us to train a high-accuracy, <mark>one-stage detector</mark> that significantly outperforms the alternatives of training with the sampling heuristics or hard example mining, the previous state-ofthe-art techniques for training <mark>one-stage detector</mark>s.<br>2: In contrast, we show that our proposed focal loss naturally handles the class imbalance faced by a <mark>one-stage detector</mark> and allows us to efficiently train on all examples without sampling and without easy negatives overwhelming the loss and computed gradients.<br>",
    "Arabic": "كاشف مرحلة واحدة",
    "Chinese": "单阶段检测器",
    "French": "détecteur en une étape",
    "Japanese": "ワンステージ検出器",
    "Russian": "однофазный детектор"
  },
  {
    "English": "one-versus-all",
    "context": "1: Furthermore, the marginal distribution of labels across all instances exhibits a long tail, which causes additional statistical challenges. Algorithmic approaches to XMC include optimized oneversus-all methods Schölkopf 2017, 2019;Yen et al. 2017Yen et al. , 2016, embedding-based methods (Bhatia et al. 2015;Tagami 2017;Guo et al.<br>2: For our multiple feature approach, we trained a multiclass classifier using all features. We trained 19 <mark>one-versus-all</mark> logistic regression models (one for each of our 18 verticals and one for the \"no relevant vertical\" class) using the liblinear toolkit 4 .<br>",
    "Arabic": "واحد-ضد-الجميع",
    "Chinese": "一对所有",
    "French": "un-contre-tous",
    "Japanese": "一対全",
    "Russian": "один против всех"
  },
  {
    "English": "online algorithm",
    "context": "1: We call an <mark>online algorithm</mark> α-competitive if for any preference profile the <mark>online algorithm</mark> asks at most α • OPT queries, where OPT is the number of queries of the optimal algorithm on this instance. Finally , we note that partial preferences in the next-best query model can be equivalently expressed by an incomplete preference profile ( with induced rank function rank ) , in the hybrid-query model , by an incomplete rank function rank ( with induced partial preference profile ) in which each agent only lists ranks for a subset of houses , and in the set-compare<br>2: . Further, they conjectured that an <mark>online algorithm</mark> with a constant competitive ratio for eliciting necessarily rank-maximal matchings is possible, and left this as their most important open question.<br>",
    "Arabic": "خوارزمية عبر الإنترنت",
    "Chinese": "在线算法",
    "French": "algorithme en ligne",
    "Japanese": "オンラインアルゴリズム",
    "Russian": "онлайн-алгоритм"
  },
  {
    "English": "online convex optimization",
    "context": "1: We show that the worst-case sample complexity of group DRO, and equivalently agnostic federated learning, is greater than that of <mark>online convex optimization</mark> by only a constant factor and an additive O(n log(n/δ)/ε 2 ) samples.<br>",
    "Arabic": "التحسين المحدب عبر الإنترنت",
    "Chinese": "在线凸优化",
    "French": "optimisation convexe en ligne",
    "Japanese": "オンラインコンベックス最適化",
    "Russian": "онлайн-выпуклая оптимизация"
  },
  {
    "English": "online gradient descent",
    "context": "1: One key family of OL2R methods root in Dueling Bandit Gradient Descent (DBGD) [26], which uses <mark>online gradient descent</mark> to solve a bandit convex optimization problem [5].<br>2: Note that when ηt is a constant value η and λ1 = 0, it is easy to see the equivalence to <mark>online gradient descent</mark>, since we have wt+1 = −ηzt = −η t s=1 gs, exactly the point played by gradient descent. Experimental Results.<br>",
    "Arabic": "نزول التدرج على الانترنت",
    "Chinese": "在线梯度下降",
    "French": "descente de gradient en ligne",
    "Japanese": "オンライングラディエント降下法",
    "Russian": "онлайн-градиентный спуск"
  },
  {
    "English": "online learning",
    "context": "1: The requirement that S ≥Ω( 1 γ ) in the lower bound is not very stringent; this is precisely the excess loss one obtains when using standard <mark>online learning</mark> algorithms with regret bound O( √ T ), as is explained in the discussion following Lemma 2.<br>2: Xu, Tran-Thanh, and Jennings (2016) follow an <mark>online learning</mark> paradigm, modeling the defender actions as pulling a set of arms (targets to patrol) at each timestep against an attacker who adversarially sets the rewards at each target.<br>",
    "Arabic": "التعلم عبر الإنترنت",
    "Chinese": "在线学习",
    "French": "apprentissage en ligne",
    "Japanese": "オンライン学習",
    "Russian": "онлайн обучение"
  },
  {
    "English": "online learning algorithm",
    "context": "1: Summing these inequalities yields that max \n q * ∈A+ ϕ(p, q * ) − min p * ∈A− ϕ(p * , q) ≤ 2ε. Next , we recall that , in a no-regret learning problem with linear costs c ( 1 : T ) , a player can run any <mark>online learning algorithm</mark> directly on independent , unbiased , bounded estimates c ( 1 : T ) of its costs c ( 1 : T ) and expect only a constant factor increase in its worst-case regret bound<br>2: Hence, a reasonable goal is to compare the performance of an <mark>online learning algorithm</mark> with the best possible offline algorithm. Given a T -trial sequence S = {(x 1 , y 1 , d 1 ), (x 2 , y 2 , d 2 ), . . .<br>",
    "Arabic": "خوارزمية التعلم عبر الإنترنت",
    "Chinese": "在线学习算法",
    "French": "algorithme d'apprentissage en ligne",
    "Japanese": "オンライン学習アルゴリズム",
    "Russian": "алгоритм онлайн-обучения"
  },
  {
    "English": "online learning method",
    "context": "1: Online methods: Online learning methods are very closely related to stochastic gradient methods, as they operate on only a single example at each iteration. Moreover, many online learning rules, including the Perceptron rule, can be seen as implementing a stochastic gradient step.<br>",
    "Arabic": "طريقة التعلم عبر الإنترنت",
    "Chinese": "在线学习方法",
    "French": "méthode d'apprentissage en ligne",
    "Japanese": "オンライン学習法",
    "Russian": "онлайн-метод обучения"
  },
  {
    "English": "online learning theory",
    "context": "1: Nevertheless, it turns out that we can almost achieve that using tools from <mark>online learning theory</mark>. Indeed, one of the fundamental topics in online learning is exactly how to perform almost as well as the best fixed choice (α i ) in the hindsight.<br>2: . This fact, which is classical in both optimization theory [40,28] and <mark>online learning theory</mark> [21], follows by a standard martingale argument. That no-regret learning algorithms generalize well on stochastic costs will mean that we can efficiently implement no-regret dynamics on stochastic games using noisy payoff observations that need only be unbiased and bounded.<br>",
    "Arabic": "نظرية التعلم عبر الإنترنت",
    "Chinese": "在线学习理论",
    "French": "théorie de l'apprentissage en ligne",
    "Japanese": "オンライン学習理論",
    "Russian": "теория онлайн-обучения"
  },
  {
    "English": "ontology",
    "context": "1: However, as reported in [11], even with the machine assistance, this is an arduous, time consuming and error-prone task. A number of annotation tools for producing semantic markups exist. Protege-2000 [28] is a tool which supports the creation of <mark>ontologies</mark> for the semantic web.<br>2: An increasing number of recent IR systems make use of <mark>ontologies</mark> to help the users clarify their information needs and come up with semantic representations of documents. Many ontology-based IR systems and models have been proposed in the last decade.<br>",
    "Arabic": "الأونتولوجيا",
    "Chinese": "本体论",
    "French": "ontologie",
    "Japanese": "存在論",
    "Russian": "онтология"
  },
  {
    "English": "ontology language",
    "context": "1: Every choice of <mark>ontology language</mark> L and query language Q gives rise to an L, Q-separability problem which is to decide, given a labeled KB K = (O, A, P, N ) with O formulated in L, whether there is a query q(x) ∈ Q which separates P and N .<br>",
    "Arabic": "لغة الأنطولوجيا",
    "Chinese": "本体语言",
    "French": "langage d'ontologie",
    "Japanese": "オントロジー言語",
    "Russian": "онтологический язык"
  },
  {
    "English": "ontology-mediate query",
    "context": "1: We study the enumeration of answers to <mark>ontology-mediated queries</mark> when the ontology is formulated in a description logic that supports functional roles and the query is a CQ. In particular, we show that enumeration is possible with linear preprocessing and constant delay when a certain extension of the CQ (pertaining to functional roles) is acyclic and free-connex acyclic.<br>",
    "Arabic": "الاستعلام الأنطولوجي الوسيط",
    "Chinese": "本体中介查询",
    "French": "requête médiée par ontologie",
    "Japanese": "オントロジー媒介クエリ",
    "Russian": "онтологически-опосредованный запрос"
  },
  {
    "English": "open information extraction",
    "context": "1: The Open Intent Discovery task differs from the <mark>Open Information Extraction</mark> (OpenIE) (e.g. [Angeli et al., 2015]) and Semantic Role Labeling (SRL) tasks (e.g.<br>2: <mark>Open Information Extraction</mark> (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences.<br>",
    "Arabic": "استخراج المعلومات المفتوحة",
    "Chinese": "开放信息抽取",
    "French": "extraction ouverte d'informations",
    "Japanese": "オープン情報抽出",
    "Russian": "открытое извлечение информации"
  },
  {
    "English": "open set",
    "context": "1: Our approach ATI outperforms the other methods both for CS and OS and the additional outlier handling (ATI-λ) does not improve the accuracy for the closed set but for the <mark>open set</mark>.<br>",
    "Arabic": "مجموعة مفتوحة",
    "Chinese": "开集",
    "French": "ensemble ouvert",
    "Japanese": "オープンセット",
    "Russian": "открытое множество"
  },
  {
    "English": "open-ended text generation",
    "context": "1: The <mark>open-ended text generation</mark> task asks us to output textx t+1:|x| in continuation of a given context x 1:t . Unlike targeted generation tasks like translation or summarization, there is no \"correct\" output; the main criteria for <mark>open-ended text generation</mark> are coherence, creativity, and fluency.<br>2: As major progress is made in <mark>open-ended text generation</mark>, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for <mark>open-ended text generation</mark>, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers.<br>",
    "Arabic": "توليد نصي مفتوح النهاية",
    "Chinese": "开放式文本生成",
    "French": "génération de texte ouverte",
    "Japanese": "オープンエンドテキスト生成",
    "Russian": "генерация открытого текста"
  },
  {
    "English": "open-loop",
    "context": "1: Currently, the system also executes primitives in an <mark>open-loop</mark> fashion, but we hope to use reactive control in the future to adapt online to slippage or imprecision.<br>",
    "Arabic": "تحكم مفتوح",
    "Chinese": "开环",
    "French": "en boucle ouverte",
    "Japanese": "オープンループ",
    "Russian": "разомкнутый контур"
  },
  {
    "English": "operator norm",
    "context": "1: F 0 finite } . In the sequel H denotes a finite or infinite dimensional Hilbert space with inner product •, • and norm • . If T is a bounded linear operator on H its <mark>operator norm</mark> is written A multisample is a vector Z = (z 1 , . . . , z T ) composed of samples.<br>2: Using the triangle inequality, the submultiplicativity of the <mark>operator norm</mark>, and the properties of the pseudo-inverse, we can write \n A a − A a = ( H V ) + ( H a V − H a V ) + ( ( H V ) + − ( H V ) + ) H a V ≤ ( H V ) + H a V − H a V + ( H V ) + − ( H V ) + H a<br>",
    "Arabic": "المعيار المشغل",
    "Chinese": "算子范数",
    "French": "norme d'opérateur",
    "Japanese": "演算子ノルム",
    "Russian": "норма оператора"
  },
  {
    "English": "operator sequence",
    "context": "1: A state s is reachable if there exists an <mark>operator sequence</mark> π applicable in I such that π I = s. Otherwise, we say that s is unreachable. The set of all reachable states is denoted by R. An operator o is reachable iff it is applicable in some reachable state.<br>",
    "Arabic": "تسلسل المشغل",
    "Chinese": "运算符序列",
    "French": "séquence d'opérateurs",
    "Japanese": "演算子シーケンス",
    "Russian": "последовательность операторов"
  },
  {
    "English": "optical character recognition",
    "context": "1: In particular, we exploit the current state-of-the-art techniques in both video and audio understanding, domains of which include object, motion, scene, face, optical character, sound, and speech recognition. We extract features from their corresponding state-of-the-art models and analyze their usefulness with the VC-PCFG model (Zhao and Titov, 2020).<br>2: For instance (Figure 7), in VQA-2, we identify clusters of hard-to-learn examples that require <mark>optical character recognition</mark> (OCR) for reasoning about text (e.g., \"What is the first word on the black car?<br>",
    "Arabic": "التعرف الضوئي على الحروف",
    "Chinese": "光学字符识别",
    "French": "reconnaissance optique de caractères",
    "Japanese": "光学文字認識",
    "Russian": "оптическое распознавание символов"
  },
  {
    "English": "optical flow",
    "context": "1: A number of approaches have sought to close this gap, i.e., to estimate both dense and long-range pixel trajectories in a video. These range from methods that simply chain together two-frame <mark>optical flow</mark> fields, to more recent approaches that directly predict per-pixel trajectories across multiple frames [23].<br>2: It has been successful for constrained settings like <mark>optical flow</mark> [40,76] or stereo depth estimation [90], but is not suitable for large-scale SfM due to its high computational cost due to many redundant correspondences. Several recent works [46,60,78,96] improve the matching efficiency by first matching coarsely and subsequently refining correspondences using a local search.<br>",
    "Arabic": "التدفق البصري",
    "Chinese": "光流",
    "French": "flux optique",
    "Japanese": "光流",
    "Russian": "оптический поток"
  },
  {
    "English": "optical flow estimation",
    "context": "1: SegStereo (Yang et al. 2018) enables joint learning for segmentation and disparity esitimation simultaneously and (Cheng et al. 2017) utilize semantic clues to guide the training of <mark>optical flow estimation</mark>. These methods rely on annotated labels for segmentation in specific scenes like autonomous driving, whereas we differently concentrate on excavating semantics from dynamic scenarios.<br>2: A key difference, aside from estimating 2D and not 3D motion, is that they do not consider any inter-patch regularization, such that the motion fields assigned to different segments are independent of each other. Discrete optimization based on fusion of proposals has been applied before to 2D <mark>optical flow estimation</mark> by Lempitsky et al. [11].<br>",
    "Arabic": "تقدير التدفق البصري",
    "Chinese": "光流估计",
    "French": "estimation du flot optique",
    "Japanese": "光流推定",
    "Russian": "оценка оптического потока"
  },
  {
    "English": "optimal control theory",
    "context": "1: Work in <mark>optimal control theory</mark> has shown that human behavior can be modeled successfully as a sequential decision-making process [3].<br>2: We address the task of inferring the future actions of people from noisy visual input. We denote this task activity forecasting. To achieve accurate activity forecasting, our approach models the effect of the physical environment on the choice of human actions. This is accomplished by the use of state-of-the-art semantic scene understanding combined with ideas from <mark>optimal control theory</mark>.<br>",
    "Arabic": "نظرية التحكم الأمثل",
    "Chinese": "最优控制理论",
    "French": "théorie du contrôle optimal",
    "Japanese": "最適制御理論",
    "Russian": "теория оптимального управления"
  },
  {
    "English": "optimal experimental design",
    "context": "1: Active learning [5] focuses on the problem of costly label acquisition, although often the cost is not made explicit. Active learning (cf., <mark>optimal experimental design</mark> [33]) uses the existing model to help select additional data for which to acquire labels [1,14,23].<br>",
    "Arabic": "التصميم التجريبي الأمثل",
    "Chinese": "最优实验设计",
    "French": "conception expérimentale optimale",
    "Japanese": "最適実験設計",
    "Russian": "оптимальное экспериментальное проектирование"
  },
  {
    "English": "optimal policy",
    "context": "1: Let us consider two reward functions from the same class, such that r ′ (x, y) = r(x, y) + f (x) and, let us denote as π r and π r ′ the corresponding <mark>optimal policies</mark>. By Eq. 4, for all x, y we have \n π r ′ ( y|x ) = 1 y π ref ( y|x ) exp 1 β r ′ ( x , y ) π ref ( y|x ) exp 1 β r ′ ( x , y ) = 1 y π ref ( y|x ) exp 1 β ( r ( x , y ) + f ( x ) ) π ref ( y|x ) exp 1 β ( r ( x , y ) + f ( x ) ) = 1 exp 1 β f ( x ) y π ref ( y|x ) exp 1 β r ( x , y ) π ref ( y|x ) exp 1 β r ( x , y ) exp 1 β f ( x<br>2: Extrapolating, it is natural to say that for any environment E, two reward functions are equivalent if the <mark>optimal policies</mark> they induce in E are the same. In this way, a task is viewed as a choice of optimal policy.<br>",
    "Arabic": "السياسة المثلى",
    "Chinese": "最优策略",
    "French": "politique optimale",
    "Japanese": "最適ポリシー",
    "Russian": "оптимальная политика"
  },
  {
    "English": "optimal solution",
    "context": "1: In a bi-criterion algorithm for Problems 1 and 2, typically σ ≥ 1 and ρ ≤ 1. We call these type of approximation algorithms, bi-criterion approximation algorithms of type 1. We can also view the bi-criterion approximations from another angle. We can say that for Problem 1 , X is a feasible solution ( i.e. , it satisfies the constraint ) , and is a [ σ , ρ ] bi-criterion approximation , if f ( X ) ≤ σf ( X * ) , whereX * is the <mark>optimal solution</mark> to the problem min { f ( X ) |g ( X )<br>2: For any training sample S = ((x1, y1), . . . , ( xn , yn ) ) and any > 0 , if ( w * , ξ * ) is the <mark>optimal solution</mark> of OP5 , then Algorithm 2 returns ( w , ξ ) that have a better objective value than ( w * , ξ * ) , and for which ( w , ξ + ) is feasible in OP5<br>",
    "Arabic": "الحل الأمثل",
    "Chinese": "最优解",
    "French": "solution optimale",
    "Japanese": "最適解",
    "Russian": "оптимальное решение"
  },
  {
    "English": "optimality",
    "context": "1: The aim of Inverse Reinforcement Learning (IRL) is to infer a reward function R from a policy π. To do this, we need a model of how π relates to R. In the current literature, the most common models are <mark>optimality</mark>, Boltzmann rationality, and causal entropy maximisation.<br>",
    "Arabic": "الحالة المثلى",
    "Chinese": "最优性",
    "French": "optimalité",
    "Japanese": "最適性",
    "Russian": "оптимальность"
  },
  {
    "English": "optimality condition",
    "context": "1: We simply apply the <mark>optimality condition</mark> and strong convexity of the function F z to the vectors w 0 = Φ(X, β(z 0 )) = µ z0 (X) and w = Φ(X, β(z)) = µ z (X), it holds \n<br>2: Therefore we can bound the 2 norm of LHS of 1st order <mark>optimality condition</mark> (5.2) by P Ω (ZZ ) \n X i P Ω (ZZ ) i 1 X 1→2 2µ 2 rp X 2→∞ (by X 2→∞ = X 1→2 ) = 2µ 2 rp X i (5.6) \n<br>",
    "Arabic": "شرط الأمثلية",
    "Chinese": "最优性条件",
    "French": "condition d'optimalité",
    "Japanese": "最適性条件",
    "Russian": "условие оптимальности"
  },
  {
    "English": "optimisation",
    "context": "1: For example, we plan to investigate including the alignment of the shapes in the <mark>optimisation</mark>.<br>2: Each convolutional layer is defined by a 3 × 3 kernel with 32 channels, strides of 1, with batch normalisation, a ReLU activation and 2 × 2 max-pooling. We use the same hyper-parameters of <mark>optimisation</mark> and meta-<mark>optimisation</mark> as in the original MAML implementation except as specified in Table 4.<br>",
    "Arabic": "التحسين",
    "Chinese": "优化",
    "French": "optimisation",
    "Japanese": "最適化",
    "Russian": "оптимизация"
  },
  {
    "English": "optimisation problem",
    "context": "1: Our model is an extension of the approach put forward in Clarke and Lapata (2006a). Their work tackles sentence compression as an <mark>optimisation problem</mark>.<br>2: This is a constrained <mark>optimisation problem</mark>, and equates to understanding what W and b x (and therefore z) must look like in order to satisfy our constraints, and minimise E||z|| 2 . minimise W ,bx \n E||z|| 2 s.t.<br>",
    "Arabic": "مشكلة التحسين",
    "Chinese": "优化问题",
    "French": "problème d'optimisation",
    "Japanese": "最適化問題",
    "Russian": "оптимизационная задача"
  },
  {
    "English": "optimiser",
    "context": "1: Other important factors include learning rate, learning rate schedule, batch size, <mark>optimiser</mark>, and width-to-depth ratio. In this work, we focus on model size and the number of training steps, and we rely on existing work and provided experimental heuristics to determine the other necessary hyperparameters. Yang et al.<br>2: Learning: We use the Adam (Kingma and Ba, 2015) <mark>optimiser</mark> (α = 0.001, β 1 = 0.9, β 2 = 0.999). λ 1 , λ 2 and λ 3 are set to 0.0001, 0.005 and 0.5, respectively. We pre-train the encoder for 100 epochs.<br>",
    "Arabic": "محسِّن",
    "Chinese": "优化器",
    "French": "optimiseur",
    "Japanese": "最適化手法",
    "Russian": "оптимизатор"
  },
  {
    "English": "optimization",
    "context": "1: Hyperparameter <mark>optimization</mark> has a long history, and we refer readers to a recent survey by Feurer and Hutter (2019) for the full story.<br>2: Asynchronous memory usage GIM provides a significant practical advantage arising from the greedy nature of <mark>optimization</mark>: modules can be trained in isolation given cached outputs from previous modules, effectively removing the depth of the network as a factor of the memory complexity.<br>",
    "Arabic": "التحسين",
    "Chinese": "优化",
    "French": "optimisation",
    "Japanese": "最適化",
    "Russian": "оптимизация"
  },
  {
    "English": "optimization algorithm",
    "context": "1: However, apart from its evident theoretical value, their paper did not propose any <mark>optimization algorithm</mark> (other than a local search method due to [6]) to efficiently find the correct G k .<br>2: We use Adam as <mark>optimization algorithm</mark> with the gradient clipping being 5.<br>",
    "Arabic": "خوارزمية تحسين",
    "Chinese": "优化算法",
    "French": "algorithme d'optimisation",
    "Japanese": "最適化アルゴリズム",
    "Russian": "алгоритм оптимизации"
  },
  {
    "English": "optimization framework",
    "context": "1: It has long been known [3,13,30] that a second order smoothness prior can better model the real world, but it has not yet been possible to combine visibility reasoning and second-order smoothness in an <mark>optimization framework</mark> which finds good op-  tima.<br>",
    "Arabic": "إطار عمل التحسين",
    "Chinese": "优化框架",
    "French": "cadre d'optimisation",
    "Japanese": "最適化フレームワーク",
    "Russian": "структура оптимизации"
  },
  {
    "English": "optimization function",
    "context": "1: Our discussion also gives us a better understanding of what these heuristics compute: the best possible (for a given <mark>optimization function</mark>) admissible and consistent heuristic that can be represented as a weighted sum of indicator functions for the facts of the planning task.<br>",
    "Arabic": "وظيفة التحسين",
    "Chinese": "优化函数",
    "French": "fonction d'optimisation",
    "Japanese": "最適化関数",
    "Russian": "оптимизационная функция"
  },
  {
    "English": "optimization method",
    "context": "1: The test data was never seen by any <mark>optimization method</mark>; it was only used once in an offline analysis stage to evaluate the models found by the various optimization is no empirical evidence that these methods outperform the simple approach we use here when the cost of evaluating hyperparameter configurations varies across the space. methods.<br>2: While these limits are somewhat arbitrary, we believe them to be reasonably close to the resource limitations faced by any user of machine learning algorithms. We also limited the training time for each evaluation of a learning algorithm on each fold, to ensure that the <mark>optimization method</mark> had a chance to explore the search space.<br>",
    "Arabic": "طريقة التحسين",
    "Chinese": "优化方法",
    "French": "méthode d'optimisation",
    "Japanese": "最適化手法",
    "Russian": "оптимизационный метод"
  },
  {
    "English": "optimization objective",
    "context": "1: 12: Surface smoothing evolution on a half-noisy sphere. (Right) Our method can evolve a noisy surface using an explicit diffusion flow-field. This allows smoothing on implicitly defined surfaces with increasing levels of smoothness. (Left) Yang et al. 's [69] method uses an <mark>optimization objective</mark> for a single level of smoothness.<br>",
    "Arabic": "الهدف الأمثل",
    "Chinese": "优化目标",
    "French": "objectif d'optimisation",
    "Japanese": "最適化目的関数",
    "Russian": "цель оптимизации"
  },
  {
    "English": "optimization problem",
    "context": "1: <mark>Optimization Problem</mark> 1 Given n labeled clusterings, C > 0; over all w, ξ (i) , λ (i) , and ν (i) , minimize \n 1 2 ||w|| 2 + C n i=1 ξ ( i ) subject to the constraints ∀ n i=1 w Ψ ( x ( i ) , y ( i ) ) + ξ ( i ) ≥ d ( i ) + ν ( i ) 1 + λ ( i ) 1 , ∀ n i=1 w Φ ( x ( i<br>2: Proposition 4 (Computing Posterior Mean as an <mark>Optimization Problem</mark>) Letq, g,σ 2 be a solution toq ,ĝ,σ 2 = argmax \n q ∈ Q, g ∈ G, σ 2 ∈ T F (q, g, σ 2 ), \n<br>",
    "Arabic": "مسألة التحسين",
    "Chinese": "优化问题",
    "French": "problème d'optimisation",
    "Japanese": "最適化問題",
    "Russian": "задача оптимизации"
  },
  {
    "English": "optimization procedure",
    "context": "1: (2009) to efficiently learn globally optimal parameters from thousands of training images. In the sections that follow we formulate the structured output model in detail, describe how to perform inference and learning, and detail the <mark>optimization procedures</mark> used to efficiently learn parameters. We show state-of-the-art results on the PASCAL 2007 VOC benchmark (Everingham et al.<br>",
    "Arabic": "إجراءات التحسين",
    "Chinese": "优化过程",
    "French": "procédure d'optimisation",
    "Japanese": "最適化手順",
    "Russian": "процедура оптимизации"
  },
  {
    "English": "optimization step",
    "context": "1: We perform morphological erosion and dilation on M i to obtain masks of dynamic and static regions respectively in order to turn off the loss near mask boundaries. We supervise the system with L mask and decay the weights by a factor of 5 for dynamic regions every 50K <mark>optimization steps</mark>.<br>2: as a replacement for the prompt. During the training process, since zero convolutions do not add noise to the network, the model should always be able to predict high-quality images. We observe that the model does not gradually learn the control conditions but abruptly succeeds in following the input conditioning image; usually in less than 10K <mark>optimization steps</mark>.<br>",
    "Arabic": "خطوة التحسين",
    "Chinese": "优化步骤",
    "French": "Pas d'optimisation",
    "Japanese": "最適化ステップ",
    "Russian": "оптимизационный шаг"
  },
  {
    "English": "optimization theory",
    "context": "1: Model training stability has been an under-explored research area, not only for recommendation models, but also in general machine learning. Fortunately, with the increasing trend of large models [8,11,29], stabilizing model training has become an emerging research area and attracts more attention in recent years. From the perspective of <mark>optimization theory</mark>, Wu et al.<br>",
    "Arabic": "نظرية التحسين",
    "Chinese": "优化理论",
    "French": "théorie de l'optimisation",
    "Japanese": "最適化理論",
    "Russian": "теория оптимизации"
  },
  {
    "English": "optimizer",
    "context": "1: weather-based degradations, motion blur, focus blur). 4. Set the learning rate and <mark>optimizer</mark> according to the size of the dataset: lower learning rates and <mark>optimizer</mark>s such as Adam or SGD for datasets with more images. Test task: PASCAL VOC 1.<br>2: We investigate the main influencing factors to system accuracy, including the character sequence representations, word sequence representations, inference algorithm, pretrained embeddings, tag scheme, running environment and <mark>optimizer</mark>; analyzing system performances from the perspective of decoding speed and accuracies on in-vocabulary (IV) and out-of-vocabulary (OOV) entities/chunks/words.<br>",
    "Arabic": "مُحسِّن",
    "Chinese": "优化器",
    "French": "optimiseur",
    "Japanese": "最適化手法 (saiteki-ka shuhou)",
    "Russian": "оптимизатор"
  },
  {
    "English": "option",
    "context": "1: Despite the simplicity of the domain, we are not aware of other methods which could have solved this task without incurring a cost much larger than when using primitive actions alone (McGovern and Barto 2001;Ş imşek and Barto 2009). Figure 3: Termination probabilities for the option-critic agent learning with 4 <mark>options</mark>.<br>2: We first consider a navigation task in the four-rooms domain (Sutton, Precup, and Singh 1999). Our goal is to evaluate the ability of a set of <mark>options</mark> learned fully autonomously to recover from a sudden change in the environment.<br>",
    "Arabic": "خيارات",
    "Chinese": "选项",
    "French": "option",
    "Japanese": "オプション",
    "Russian": "опция"
  },
  {
    "English": "oracle",
    "context": "1: We next describe an algorithm that computes |P | using an <mark>oracle</mark> for E U [F ], where F is a logistic regression function and U is the uniform probability distribution. Let m be a natural number large enough, to be chosen later, and define the following weights: \n<br>2: Then the <mark>oracle</mark> knows the unique partition part P such that p ∈ P ∈ P. For example, if f (p) is the multiset of p, then each subset P corresponds to set of distributions with the same probability multiset, and the <mark>oracle</mark> knows the multiset of probabilities.<br>",
    "Arabic": "العراف",
    "Chinese": "神谕程序",
    "French": "oracle",
    "Japanese": "神示",
    "Russian": "оракул"
  },
  {
    "English": "oracle policy",
    "context": "1: Our learned policy leads to fast practical convergence as well as excellent performance, sometimes even outperforms the <mark>oracle policy</mark> tuned via inaccessible ground truth (in ×2 case). We note this is owing to the varying parameters across iterations generated automatically in our algorithm, which yield extra flexibility than constant parameters over iterations.<br>",
    "Arabic": "سياسة المعلم",
    "Chinese": "神谕策略",
    "French": "politique oracle",
    "Japanese": "オラクルポリシー",
    "Russian": "политика оракула"
  },
  {
    "English": "ordinal embedding",
    "context": "1: It is known that to exactly recover an <mark>ordinal embedding</mark> one needs of the order Ω(n 3 ) passively queried triplets in the worst case (essentially all of them), unless we make stronger assumptions on the underlying metric space [Jamieson and Nowak, 2011].<br>2: Each WTA hash function defines an <mark>ordinal embedding</mark> and an associated rank-correlation similarity measure, which offers a degree of invariance with respect to pertur-bations in numeric values [21] and is well suited as a basis for locality-sensitive hashing.<br>",
    "Arabic": "تضمين ترتيبي",
    "Chinese": "序数嵌入",
    "French": "plongement ordinal",
    "Japanese": "順序埋め込み",
    "Russian": "ординальное вложение"
  },
  {
    "English": "ordinal regression",
    "context": "1: In <mark>ordinal regression</mark>, the label yi of an example (xi, yi) indicates a rank instead of a nominal class. Without loss of generality, let yi ∈ {1, ..., R} so that the values 1, ..., R are related on an ordinal scale. In the formulation of Herbrich et al.<br>2: And, fourth, the algorithm can handle <mark>ordinal regression</mark> problems with hundred-thousands of examples with ease, while existing methods become intractable with only a few thousand examples.<br>",
    "Arabic": "الانحدار الترتيبي",
    "Chinese": "有序回归",
    "French": "régression ordinale",
    "Japanese": "順序回帰",
    "Russian": "порядковая регрессия"
  },
  {
    "English": "orientation loss",
    "context": "1: Removing the <mark>orientation loss</mark> (\"no R o \") results in severely degraded normals and renderings, and applying the <mark>orientation loss</mark> directly to the density field's normals and using those to compute reflection directions (\"no pred. normals\") also reduces performance.<br>2: (2022) to prevent unneccesarily filling in of empty space. To prevent pathologies in the density field where normal vectors face backwards away from the camera we use a modified version of the <mark>orientation loss</mark> proposed in Ref-NeRF .<br>",
    "Arabic": "فقدان التوجه",
    "Chinese": "方向损失",
    "French": "perte d'orientation",
    "Japanese": "方向損失",
    "Russian": "потеря ориентации"
  },
  {
    "English": "orthogonal basis",
    "context": "1: , u n } be an <mark>orthogonal basis</mark> of R n such that V X = span(u 1 , . . . , u k ) and V X + V Y = span(u 1 , . . . , u k+m ).<br>2: Proposition A.6 (Proposition 5). Let X ⊂ R n be a set of points and let {u 1 , . . . , u n } be an <mark>orthogonal basis</mark> of R n such that span(X) = span(u 1 , . . . , u k ).<br>",
    "Arabic": "قاعدة متعامدة",
    "Chinese": "正交基",
    "French": "base orthogonale",
    "Japanese": "直交基底",
    "Russian": "ортогональный базис"
  },
  {
    "English": "orthogonal matrix",
    "context": "1: F = OΛR T , F + = RΛ −1 O T (29 \n ) \n Where Λ ∈ R k×n is a rectangular diagonal matrix with positive entries, λ i , Λ −1 ∈ R n×k is a rectangular diagonal matrix with positive entries, 1 λi , and where O ∈ R n×n and R ∈ R k×k are <mark>orthogonal matrices</mark>.<br>2: Denote the SVD of the SNR matrix (Equation 8) as follows: \n SNR = U ΩV = C−1 j=1 ω j u j v j . Here , { ω c } C−1 c=1 are the non-zero singular values of SNR ; Ω = diag { ω c } C−1 c=1 , 0 ∈ R C×C ; and the left and right singular-vectors , U ∈ R P ×C and V ∈ R C×C , are partial orthogonal and <mark>orthogonal matrices</mark> , respectively , with columns { u j }<br>",
    "Arabic": "مصفوفة متعامدة",
    "Chinese": "正交矩阵",
    "French": "matrice orthogonale",
    "Japanese": "正規直交行列",
    "Russian": "ортогональная матрица"
  },
  {
    "English": "orthogonal projection matrix",
    "context": "1: Specifically, we add r recently examined documents to the current document space S t to compensate the potentially overlooked examined documents in the current query. In line 14 of Algorithm 1, we solve the <mark>orthogonal projection matrix</mark> A t of document space S t . A t could be computed by several methods.<br>2: We denote by π : R d → M the closest point projection on M, i.e., π(x) = min y∈M x − y , with y 2 = y, y the Euclidean norm in R d . Lastly , we denote by P x ∈ R d×d the <mark>orthogonal projection matrix</mark> on the tangent space T x M ; in practice if we denote by N ∈ R d×k the matrix with orthonormal columns spanning N x M = ( T x M ) ⊥ ( i.e. , the normal space to M at x ) then , P x =<br>",
    "Arabic": "مصفوفة الإسقاط المتعامد",
    "Chinese": "正交投影矩阵",
    "French": "matrice de projection orthogonale",
    "Japanese": "正射影行列",
    "Russian": "ортогональная проекционная матрица"
  },
  {
    "English": "orthographic camera model",
    "context": "1: It is expected that, using good prior will further improve our solution, and make our method more applicable to complex scenarios. In the present paper, we have concentrated on com-plete measurement case under <mark>orthographic camera model</mark>.<br>",
    "Arabic": "نموذج الكاميرا الأورثوغرافية",
    "Chinese": "正交相机模型",
    "French": "modèle de caméra orthographique",
    "Japanese": "正射影カメラモデル",
    "Russian": "модель ортогональной камеры"
  },
  {
    "English": "orthographic projection",
    "context": "1: A particular benefit is to show that ambiguities exist for the case of camera motion, which render shape recovery more difficult. Consequently, for <mark>orthographic projection</mark>, Sec. 4 shows a negative result whereby constraints on the shape of a surface with general isotropic BRDF may not be derived using camera motion as a cue.<br>2: Under <mark>orthographic projection</mark>, for a BRDF of unknown functional form that depends on light source and an arbitrary direction in the source-view plane, two differential motions of the camera suffice to yield a BRDF-invariant constraint on surface depth. Proof. The proof directly generalizes the development in Proposition 4.<br>",
    "Arabic": "الإسقاط الأرثوغرافي",
    "Chinese": "正交投影",
    "French": "projection orthographique",
    "Japanese": "正射投影 (seisha tōei)",
    "Russian": "ортографическая проекция"
  },
  {
    "English": "orthonormal decomposition",
    "context": "1: ) \n where λ j are the eigenvalues of T in decreasing order and φ j form an <mark>orthonormal decomposition</mark> of L 2 (X , P ). Consider now the least absolute deviation ( LAD ) loss function ℓ ( h ; x , y ) = |h ( x ) − y| , defined for h ∈ H , and let B H be the unit • H -ball of H. Assume additionally that the model is well-specified , and that y = h ⋆ ( x ) + ξ<br>",
    "Arabic": "التحليل الأورثونورمي",
    "Chinese": "正交分解",
    "French": "décomposition orthonormée",
    "Japanese": "正規直交分解",
    "Russian": "ортонормальное разложение"
  },
  {
    "English": "orthonormal matrix",
    "context": "1: More precisely, we consider a random scrambling operator A which only scrambles data vectors x within a fixed k dimensional subspace spanned by the orthonormal columns of the n 0 × k matrix U . Within this subspace, data vectors are scrambled by a random Gaussian k × k matrix B.<br>2: First, the choice of Z is not unique since M = (ZR)(ZR) for any <mark>orthonormal matrix</mark> Z. Our goal is to find one of these equivalent solutions. Another issue is that matrix completion is impossible when M is \"aligned\" with standard basis.<br>",
    "Arabic": "مصفوفة متعامدة",
    "Chinese": "正交矩阵",
    "French": "matrice orthonormale",
    "Japanese": "直交正規化行列",
    "Russian": "ортонормированная матрица"
  },
  {
    "English": "orthonormal row",
    "context": "1: In our applications, J, C will always be the subspace embedding matrices from Theorem 6. The algorithm outputs an O(k) × matrix Z with <mark>orthonormal rows</mark>. Algorithm 5 Weak Low Rank Approximation (LRA) \n<br>",
    "Arabic": "صفوف متعامدة ومتساوية الطول",
    "Chinese": "标准正交行",
    "French": "rangées orthonormées",
    "Japanese": "直交正規行",
    "Russian": "ортонормальная строка"
  },
  {
    "English": "orthonormality",
    "context": "1: In addition, they further proved that using the <mark>orthonormality</mark> constraints alone is in fact sufficient to recover a unique (unambiguous) non-rigid shape (provided that a previously-overlooked rank-3 constraint on Q k (Eq.-(3)) is accounted for).<br>2: We may then apply the eigenfunction property to the inner integral and <mark>orthonormality</mark> of eigenfunctions to the result yielding, \n cov(u m , u k ) = λ k φ k (x)φ m (x)p(x)dx = λ k δ m,k . With similar considerations , cov ( u m , f ( x i ) ) = E φ m ( x ) f ( x ) f ( x i ) p ( x ) dx = φ m ( x ) E [ f ( x ) f ( x i ) ] p ( x ) dx = λ m φ m<br>",
    "Arabic": "تعامد",
    "Chinese": "正交性",
    "French": "orthonormalité",
    "Japanese": "直交正規性 (chokkō seikisei)",
    "Russian": "ортонормальность"
  },
  {
    "English": "out-of-distribution",
    "context": "1: Third, our technique should be effective with <mark>out-of-distribution</mark> topics (Mikros and Argiri, 2007) since it is im-practical to assume that the training data covers all possible topics during runtime. Existing Techniques. Prior research efforts on authorship attribution have focused on solving either <mark>out-of-distribution</mark> in topics or authors.<br>2: However, current reading comprehension datasets for theory of mind reasoning are simplistic and lack diversity, leading to brittle downstream models which, as we show, fail in the presence of even slight <mark>out-of-distribution</mark> perturbations.<br>",
    "Arabic": "خارج التوزيع",
    "Chinese": "超出分布",
    "French": "\"hors distribution\"",
    "Japanese": "分布外",
    "Russian": "вне распределения"
  },
  {
    "English": "out-of-domain",
    "context": "1: We also test how semantics-preserving noise affects models of different sizes and parametrization (see Figure 2). Although for in-domain setup, the relaxed fooling rate metrics marginally drop as the models get bigger, the same cannot be observed in <mark>out-of-domain</mark> setup.<br>2: Out-of-domain We also probe the NLI models in an <mark>out-of-domain</mark> zero-shot setting to assess the transferability of compositional semantic knowledge. Our results in Table 2 show that the discrepancies and limitations in semantic comprehension are even more pronounced in this setting.<br>",
    "Arabic": "خارج النطاق",
    "Chinese": "领域外",
    "French": "hors-domaine",
    "Japanese": "文脈外の",
    "Russian": "вне домена"
  },
  {
    "English": "out-of-domain evaluation",
    "context": "1: Given a specific text generator, such as ChatGPT and davinci-003, we train a detector using data from one domain and evaluate it on the test set from the same domain (in-domain evaluation) and other domains (<mark>out-of-domain evaluation</mark>). The results are shown in Figure 1 and Tables 12 and 13.<br>2: the number of sampled instances from the particular dataset in the experiment. The results on outof-domain evaluation once again follow the pattern that more than half, r s = 15.8% of the samples switch the labels to their logically contrasting counterparts.<br>",
    "Arabic": "تقييم خارج النطاق",
    "Chinese": "跨域评估",
    "French": "\"évaluation hors domaine\"",
    "Japanese": "ドメイン外評価",
    "Russian": "оценка вне домена"
  },
  {
    "English": "outli exposure",
    "context": "1: Therefore, unlike prior approaches, OpenGAN directly uses the discriminator as the open-set likelihood function. Moreover, our final version of OpenGAN generates features rather than pixel images. Open-Set Recognition with Outlier Exposure. [18,31,52] reformulate the problem with the concept of \"<mark>outlier exposure</mark>\" which allows methods to access some  [42,44].<br>2: Except for the above algorithms, researchers also study the situation, where auxiliary OOD data can be obtained during the training process [13,70]. These methods are called <mark>outlier exposure</mark>, and have much better performance than the above methods due to the appearance of OOD data.<br>",
    "Arabic": "التعرض للقيم الشاذة",
    "Chinese": "异常暴露",
    "French": "exposition aux valeurs aberrantes",
    "Japanese": "外れ値暴露",
    "Russian": "выявление выбросов"
  },
  {
    "English": "outli rejection",
    "context": "1: Table 8 presents the time efficiency analysis of MAC. Performing feature matching selection. Before 3D registration, a popular way is to perform <mark>outlier rejection</mark> to reduce the correspondence set.<br>2: The previous section showed that two independent pairs of transformations may suffice to uniquely determine H . In practice, however, to increase numerical stability, we use all available constraints from all pairs of reliable transformations after subsampling of the sequences, <mark>outlier rejection</mark> and normalization. These are explained next: Temporal Subsampling.<br>",
    "Arabic": "رفض القيم الشاذة",
    "Chinese": "异常值排除",
    "French": "rejet des valeurs aberrantes",
    "Japanese": "外れ値除去",
    "Russian": "отбрасывание выбросов"
  },
  {
    "English": "outlier",
    "context": "1: On the other hand, Karamcheti et al. (2021) and Munjal et al. (2022) claim there is rather small to no advantage in using active learning strategies, because a number of samples might be collectively <mark>outliers</mark>, and existing strategies contribute little to discover them and instead harm the performance of subsequent models.<br>2: Spinnet [1] extracts local features which are rotationally invariant and sufficiently informative to enable accurate registration. Some methods [3,9,14,27] focus on efficiently distinguishing correspondences as inliers and <mark>outliers</mark>.<br>",
    "Arabic": "قيم متطرفة",
    "Chinese": "异常值",
    "French": "valeur aberrante",
    "Japanese": "外れ値",
    "Russian": "выброс"
  },
  {
    "English": "outlier detection",
    "context": "1: Open-Set Recognition. There are multiple lines of work addressing open-set discrimination, such as anomaly detection [12,36,69], <mark>outlier detection</mark> [53,48], and open-set recognition [54,22]. The typical setup for these problems assumes that one does not have access to training examples of open-set data.<br>2: Our algorithm also scales very well because it requires only a single pass over the data. Finally, we illustrate the power of DGX as a new tool for data mining tasks, such as <mark>outlier detection</mark>.<br>",
    "Arabic": "اكتشاف القيم الشاذة",
    "Chinese": "异常检测",
    "French": "détection de valeurs aberrantes",
    "Japanese": "外れ値検出",
    "Russian": "выявление выбросов"
  },
  {
    "English": "output",
    "context": "1: The results Y h ∈ R n×d H across all heads h are then combined and projected to obtain the final <mark>output</mark> \n Y = h Y h W h O where W h O ∈ R d H ×d . We call the resulting architecture Graphormer-GD, and the full structure of Graphormer-GD is provided in Appendix E.3.<br>2: A task is a function that maps a sentence to a single <mark>output</mark> per word, f (x 1:T ) = y 1:T , where each <mark>output</mark> is from a finite set of <mark>output</mark>s: y i ∈ Y. Each control task is defined in reference to a linguistic task, and the two share Y.<br>",
    "Arabic": "الإخراج",
    "Chinese": "输出",
    "French": "sortie",
    "Japanese": "出力",
    "Russian": "выход"
  },
  {
    "English": "output gate",
    "context": "1: c t ) \n Here i, f , o denote the input, forget, and <mark>output gate</mark>, h is the hidden state and c is the cell state. σ denotes the sigmoid function, indicates an element-wise product and * a convolution. W h denotes the hidden-to-state convolution kernel and W x the input-to-state convolution kernel.<br>2: Gender n final letters Both We record the hidden state h l , e t , memory cell state c l , e t and the activations for the input , forget and <mark>output gate</mark>s i l , e t , f l , e t and o l , e t , from the encoder layers l ∈ { 1 , 2 }<br>",
    "Arabic": "البوابة الناتجة",
    "Chinese": "输出门",
    "French": "porte de sortie",
    "Japanese": "出力ゲート",
    "Russian": "выходной вентиль"
  },
  {
    "English": "output layer",
    "context": "1: On top of the K pre-trained layers, stack an <mark>output layer</mark> of size the number of classes. Finetune the whole network for supervised classification 2 with an added tangent propagation penalty (Eq. 6), using for each x i , tangent directions B xi . We call this deep learning algorithm the Manifold Tangent Classifier (MTC).<br>2: We first present an overview of the dual supervision framework which effectively utilizes both humanannotated (HA) data and distantly supervised (DS) data for training RE models. We next introduce the detailed structure of the <mark>output layer</mark> in our framework and propose our novel loss function with disagreement penalty that considers the labeling bias of distant supervision.<br>",
    "Arabic": "طبقة الإخراج",
    "Chinese": "输出层",
    "French": "couche de sortie",
    "Japanese": "出力層",
    "Russian": "выходной слой"
  },
  {
    "English": "output space",
    "context": "1: In the following experiment only the digits are used. We regard each image as a task, hence the input space is the set of 320 possible pixels indices, while the <mark>output space</mark> is the real interval [0, 1], representing the gray level.<br>2: Consider a prediction problem from some input space X (e.g., images) to an <mark>output space</mark> Y (e.g., labels). We are given training points z 1 , . . . , z n , where z i = (x i , y i ) ∈ X × Y.<br>",
    "Arabic": "مساحة الإخراج",
    "Chinese": "输出空间",
    "French": "espace de sortie",
    "Japanese": "出力空間",
    "Russian": "пространство выходов"
  },
  {
    "English": "output token",
    "context": "1: We overcome two key technical challenges in this work: Firstly, we do not have supervision for the correspondence between input tokens and <mark>output tokens</mark>. Therefore, we induce the correspondence during training. Secondly, predicting permutations without restrictions is computationally challenging. For this, we develop a differentiable GPU-friendly algorithm.<br>",
    "Arabic": "رمز إخراج",
    "Chinese": "输出标记 (output token)",
    "French": "jeton de sortie",
    "Japanese": "出力トークン",
    "Russian": "токен вывода"
  },
  {
    "English": "output vector",
    "context": "1: All the H <mark>output vectors</mark> are concatenated, denoted by ⊕, and transformed by \n W o ∈ R d×d to obtain o m ∈ R d : o m = W o (o (1) m ⊕ o (2) m ⊕ • • • ⊕ o (H) m ) \n<br>2: It consists of 5.7 billion pairs of multivariate input and <mark>output vectors</mark> that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state.<br>",
    "Arabic": "متجه الناتج",
    "Chinese": "输出向量",
    "French": "vecteur de sortie",
    "Japanese": "出力ベクトル",
    "Russian": "вектор вывода"
  },
  {
    "English": "output vocabulary",
    "context": "1: We use two 512dimensional hidden layers with tanh activation functions. The output layer is a softmax over the entire <mark>output vocabulary</mark>. The input vocabulary contains 16,000 source words and 16,000 target words, while the <mark>output vocabulary</mark> contains 32,000 target words. The vocabulary is selected by frequency-sorting the words in the parallel training data.<br>",
    "Arabic": "مفردات الإخراج",
    "Chinese": "输出词汇表",
    "French": "vocabulaire de sortie",
    "Japanese": "出力語彙",
    "Russian": "выходной словарь"
  },
  {
    "English": "over-fit",
    "context": "1: , showing that their values are more reliable and more sensitive to modulatory influences . The comparison of NN training and validation errors (Figure 5a) indicated that the effects of <mark>over-fitting</mark> were negligible. The meta-parameter prediction model allows us to analyze how (and how much) each modulatory factor affects meta-parameters and what the interactions between factors are.<br>2: However, the vanilla transformer decoder in DSI does not fully leverage the hierarchical structures of document identifiers, and the model is pruned to <mark>over-fitting</mark> with limited training data. Furthermore, Bevilacqua et al. [4] proposed SEAL by leveraging all n-grams in a passage as its identifiers.<br>",
    "Arabic": "الإفراط في الملاءمة",
    "Chinese": "过拟合",
    "French": "sur-apprentissage",
    "Japanese": "過学習 (kagakushū)",
    "Russian": "переобучение"
  },
  {
    "English": "over-segmentation",
    "context": "1: They all share the same three stage process-produce an <mark>over-segmentation</mark> of the reference image, generate a set of planar hypotheses for each segment, and optimize over the hypothesesdiffering only in their implementation of each stage.<br>",
    "Arabic": "التجزئة الزائدة",
    "Chinese": "过度分割",
    "French": "sur-segmentation",
    "Japanese": "過剰分割",
    "Russian": "чрезмерная сегментация"
  },
  {
    "English": "over-smooth",
    "context": "1: 2018). Yet, despite their rapid development, training GNNs on large-scale graphs is facing several challenges such as overfitting, <mark>over-smoothing</mark>, and non-robustness. Indeed, compared to other data forms, gathering labels for graph data is expensive and inherently biased, which limits the generalization ability of GNNs due to over-fitting.<br>2: Some of the drawbacks of the message passing paradigm have now been identified and formalized, including the limits of expressive power (Xu et al., 2019;Morris et al., 2019;Maron et al., 2019) and the problem of <mark>over-smoothing</mark> (NT & Maehara, 2019;Oono & Suzuki, 2020).<br>",
    "Arabic": "تجانس المفرط",
    "Chinese": "过度平滑",
    "French": "sur-lissage",
    "Japanese": "過剰平滑化",
    "Russian": "чрезмерное сглаживание"
  },
  {
    "English": "paired t-test",
    "context": "1: It is found that conjunct regrouping improves recall (p < 0.01 based on the <mark>paired t-test</mark>), and the use of two-level rules in the maximum-entropy model improves precision and recall (p < 0.05). Type check-ing also significantly improves precision and recall.<br>",
    "Arabic": "اختبار t المقترن",
    "Chinese": "配对 t 检验",
    "French": "test t apparié",
    "Japanese": "対応のある t検定",
    "Russian": "- Парный t-тест"
  },
  {
    "English": "pairwise",
    "context": "1: We present an end-to-end model that makes it possible to learn all parameters of the graph matching process, including the unary and <mark>pairwise</mark> node neighborhoods, represented as deep feature extraction hierarchies.<br>2: In practice, the attentive module also provides interpretability over the significance of <mark>pairwise</mark> residue interactions, as illustrated in Appendix I.<br>",
    "Arabic": "زوجيًا",
    "Chinese": "成对",
    "French": "par paires",
    "Japanese": "ペアワイズ (pairwise)",
    "Russian": "попарный"
  },
  {
    "English": "pairwise classifier",
    "context": "1: As an additional baseline, we train a <mark>pairwise classifier</mark> (pairwise) classifier: each pair of emails within a set constitutes a training example, with label +1 if they belong to the same cluster, and −1 otherwise.<br>",
    "Arabic": "مصنف زوجي",
    "Chinese": "一对一分类器",
    "French": "classificateur par paires",
    "Japanese": "ペアワイズ分類器",
    "Russian": "парный классификатор"
  },
  {
    "English": "pairwise clique",
    "context": "1: This is despite the fact that the graph constructions necessary to include these terms are known [21]: the triple cliques which represent the second order terms are decomposed into several <mark>pairwise cliques</mark> and auxiliary nodes are added.<br>2: disparity d0) in I1; note that some of these nodes have been excluded for clarity. Black lines represent the data costs, blue lines the visibility constraint, and red lines the smoothness prior. to solving the graph, and, while the list length is variable, it tends to be around nN edges. The six red lines , which represent the smoothness costs of equation ( 5 ) for the only complete neighborhood , N = { p , q , r } , show how one triple clique is decomposed into six <mark>pairwise cliques</mark> , and an extra , latent node ( labeled aux ) , using the decomposition described in [ 21 ] ; note<br>",
    "Arabic": "مجموعة زوجية",
    "Chinese": "两两团",
    "French": "clique par paires",
    "Japanese": "対の素性関係",
    "Russian": "\"парные клики\""
  },
  {
    "English": "pairwise comparison",
    "context": "1: Collective multi-label classifier (CML) (Ghamrawi and McCallum, 2005) adopts maximum entropy principle to deal with multi-label data by encoding label correlations as constraint conditions. Zhang and Zhou (2007) adopt k-nearest neighbor techniques to deal with multi-label data. Fürnkranz et al. (2008) make ranking among labels by utilizing <mark>pairwise comparison</mark>.<br>",
    "Arabic": "مقارنة زوجية",
    "Chinese": "成对比较",
    "French": "comparaison par paires",
    "Japanese": "ペアごとの比較",
    "Russian": "попарное сравнение"
  },
  {
    "English": "pairwise constraint",
    "context": "1: In the semi-supervised scenario for clustering, external information is introduced in the form of a reduced number of <mark>pairwise constraints</mark>: similarity constraints indicate pairs of data items which must share the same cluster and dissimilarity constraints indicate pairs of data items which must be put in different clusters.<br>2: Could we use the feature center for selecting the reference of the keypoint adjustment? By minimizing the feature distance to this unique reference, we could reduce the number of residuals from quadratic (<mark>pairwise constraints</mark>) to linear (unary constraints) and thus accelerate the optimization.<br>",
    "Arabic": "قيود ثنائية",
    "Chinese": "成对约束",
    "French": "contrainte par paires",
    "Japanese": "ペアワイズ制約",
    "Russian": "Попарное ограничение"
  },
  {
    "English": "pairwise flow",
    "context": "1: We show the duration, in logarithmic scale, of the refinement for varying numbers of images. Our refinement is more than ten times faster than Patch Flow [24], whose run-time is dominated by the computation of the <mark>pairwise flow</mark>, which scales quadratically. Thanks to our precomputed cost patches, the featuremetric BA is fast.<br>2: The exhaustive <mark>pairwise flow</mark> input maximizes the useful motion information available to the optimization stage. However, this approach, especially when coupled with the flow-filtering process, can result in an unbalanced collection of motion samples in dynamic regions.<br>",
    "Arabic": "التدفق الزوجي",
    "Chinese": "成对流动",
    "French": "flux par paires",
    "Japanese": "ペアワイズフロー",
    "Russian": "попарный поток"
  },
  {
    "English": "pairwise learning",
    "context": "1: It is apparent that the simplest learning method, <mark>pairwise learning</mark>, leads to the fewest wrong edges before clustering, but the induced similarity matrix is furthest away from being a consistent partitioning. This corresponds to the intuition that the training constraints of <mark>pairwise learning</mark> refer to individual links instead of the entire partitioning.<br>2: On these pairs, a linear SVM is trained, and the weight vector is directly used as parameter of the similarity measure. The final clustering is then obtained by one of the decoding strategies, using the similarity matrix obtained from <mark>pairwise learning</mark>.<br>",
    "Arabic": "تعلم الأزواج المتقابلة",
    "Chinese": "成对学习",
    "French": "apprentissage par paires",
    "Japanese": "ペアワイズ学習",
    "Russian": "попарное обучение"
  },
  {
    "English": "pairwise potential",
    "context": "1: where v a , v b ∈ v and l i , l j ∈ l. In other words, the potentials are modified by defining a <mark>pairwise potential</mark>θ 2 aa;ii and subtracting the value of that potential from the corresponding unary potential θ 1 a;i .<br>2: The <mark>pairwise potential</mark> ϕ allows f to effectively enforce the learned second order relative constraints during inference which, to the best of our knowledge, has not been explored in existing ranking models. Enforcing these constraints comes at the cost of increased inference time of O(m) for any given document.<br>",
    "Arabic": "إمكانات ثنائية",
    "Chinese": "成对势能",
    "French": "potentiel par paires",
    "Japanese": "ペア間ポテンシャル",
    "Russian": "потенциал попарных взаимодействий"
  },
  {
    "English": "pairwise word similarity",
    "context": "1: Levy and Goldberg (2014b) showed that this was equivalent to searching for a word that maximizes a linear combination of three <mark>pairwise word similarities</mark>, so the proposed post-processing has a direct effect on it.<br>",
    "Arabic": "التشابه الزوجي للكلمات",
    "Chinese": "成对词相似性",
    "French": "similarité par paires de mots",
    "Japanese": "ペアワイズ単語類似度",
    "Russian": "попарное сходство слов"
  },
  {
    "English": "panoptic segmentation",
    "context": "1: Our foray into the text-to-mask task is exploratory and not entirely robust, although we believe it can be improved with more effort. While SAM can perform many tasks, it is unclear how to design simple prompts that implement semantic and <mark>panoptic segmentation</mark>.<br>2: In a multi-task system, a single model performs a fixed set of tasks, e.g., joint semantic, instance, and <mark>panoptic segmentation</mark> [114,19,54], but the training and test tasks are the same.<br>",
    "Arabic": "التجزئة الشاملة",
    "Chinese": "全景分割",
    "French": "segmentation panoptique",
    "Japanese": "パノプティックセグメンテーション",
    "Russian": "паноптическая сегментация"
  },
  {
    "English": "parallel corpora",
    "context": "1: ( , 2016 worked around these issues by making the English PropBank act as a universal predicate sense and semantic role inventory and projecting PropBank-style annotations from English onto non-English sentences by means of word alignment techniques applied to <mark>parallel corpora</mark> such as Europarl (Koehn, 2005).<br>2: Multilingual Parallel Corpora Several multilingual <mark>parallel corpora</mark> have been developed to support studies on machine translation such as GCP ( Imamura andSumita , 2018 ) , Leipzig ( Gold-hahn et al. , 2012 ) , JRC Acquis ( Steinberger et al. , 2006 ) , TUFS Asian Language Parallel ( Nomoto et al. , 2018 ) , Intercorp ( ekČermák andRosen , 2012 ) , DARPA LORELEI ( Strassel and Tracey , 2016 ) , Asian Language Treebank ( Riza et al. , 2016 ) , FLORES ( Guzmán et al. , 2019 ) , the Bible Parallel Corpus ( Resnik et al. , 1999 ; Black , 2019 ) , JW-300 ( Agić and Vulić , 2019 ) , BiToD ( Lin et al. , 2021 )<br>",
    "Arabic": "- مجموعات النصوص المتوازية",
    "Chinese": "平行语料库",
    "French": "corpus parallèles",
    "Japanese": "並列コーパス",
    "Russian": "параллельные корпуса"
  },
  {
    "English": "parallel corpus",
    "context": "1: monitoring ( Nurdeni et al. , 2021 ) . By translating an existing text, we additionally produce a <mark>parallel corpus</mark>, which is useful for building and evaluating translation systems.<br>2: The focus of this work is on building POS taggers for foreign languages, assuming that we have an English POS tagger and some parallel text between the two languages. Central to our approach (see Algorithm 1) is a bilingual similarity graph built from a sentence-aligned <mark>parallel corpus</mark>.<br>",
    "Arabic": "مجموعة موازية",
    "Chinese": "平行语料库",
    "French": "corpus parallèle",
    "Japanese": "パラレルコーパス",
    "Russian": "параллельный корпус"
  },
  {
    "English": "parallel datum",
    "context": "1: Regardless of the many modelling improvements aimed at reducing the amount of required supervision, it is likely impossible for translation models to reach acceptable levels of quality without even small amounts of <mark>parallel data</mark>. This is especially true for approaches that explicitly rely on the preexistence of parallel corpora, such as backtranslation.<br>2: • Projection: Our third baseline incorporates bilingual information by projecting POS tags directly across alignments in the <mark>parallel data</mark>. For unaligned words, we set the tag to the most frequent tag in the corresponding treebank.<br>",
    "Arabic": "بيانات متوازية",
    "Chinese": "并行数据 (parallel data)",
    "French": "données parallèles",
    "Japanese": "並列データ",
    "Russian": "параллельные данные"
  },
  {
    "English": "parameter",
    "context": "1: Since removing a point z is the same as upweighting it by = − 1 n , we can linearly approximate the <mark>parameter</mark> change due to removing z without retraining the model by computingθ −z −θ ≈ − 1 n I up,params (z). Next, we apply the chain rule to measure how upweighting z changes functions ofθ.<br>2: The idea is to compute the <mark>parameter</mark> change if z were upweighted by some small , giving us new <mark>parameter</mark>sθ ,z (Cook & Weisberg, 1982) tells us that the influence of upweighting z on the <mark>parameter</mark>sθ is given by \n<br>",
    "Arabic": "معامل",
    "Chinese": "参数",
    "French": "paramètre",
    "Japanese": "パラメータ",
    "Russian": "параметр"
  },
  {
    "English": "parameter count",
    "context": "1: (2022); , though future work may want to include potential curvature in this relationship for large model sizes. The resulting predictions are similar for all three methods and suggest that <mark>parameter count</mark> and number of training tokens should be increased equally with more compute 3with proportions reported in Table 2.<br>2: Large language models Scaling up transformer language models [111] across <mark>parameter count</mark> and training data has been shown to result in continuous performance gains [19]. Starting with the 1.4 billion parameter GPT-2 model [88], a variety of scaled-up language models have been trained, commonly referred to as large language models (LLMs).<br>",
    "Arabic": "شرح عدد البارامترات",
    "Chinese": "参数数量",
    "French": "nombre de paramètres",
    "Japanese": "パラメータ数",
    "Russian": "количество параметров"
  },
  {
    "English": "parameter estimation",
    "context": "1: Given the model outputs by the structure learning step, the <mark>parameter estimation</mark> step aims to obtain a set of po-tential weights that maximize the discrimination between different classes of activities ( in Fig. 3(a)).<br>2: 2005), sensitivity analysis (Azzi, Sudret, and Wiart 2020), <mark>parameter estimation</mark> (Ranneby 1984;Wolsztynski, Thierry, and Pronzato 2005), and Bayesian experimental design (Sebastiani and Wynn 2000;Ao and Li 2020).<br>",
    "Arabic": "تقدير المعلمة",
    "Chinese": "参数估计",
    "French": "estimation des paramètres",
    "Japanese": "パラメータ推定",
    "Russian": "параметрическая оценка"
  },
  {
    "English": "parameter learning",
    "context": "1: Learning of the hierarchical model includes two procedures: structural learning and <mark>parameter learning</mark>, both are completely unsupervised. Structural learning searches for the significant locations, i.e., usual goals and mode transfer locations, from GPS logs collected over an extended period of time.<br>",
    "Arabic": "تعلم المعاملات",
    "Chinese": "参数学习",
    "French": "apprentissage des paramètres",
    "Japanese": "パラメータ学習",
    "Russian": "обучение параметров"
  },
  {
    "English": "parameter matrix",
    "context": "1: The matrices F 1 , U 1 ∈ R n×d and F 2 , U 2 ∈ R m×d contain per-node feature vectors of dimension d, extracted at possibly different levels in the network, and Λ is a 2d×2d blocksymmetric <mark>parameter matrix</mark>. Superscripts 1, 2 indicate over which input image (source or target) are the features computed.<br>",
    "Arabic": "مصفوفة المعاملات",
    "Chinese": "参数矩阵",
    "French": "matrice de paramètres",
    "Japanese": "パラメータ行列",
    "Russian": "матрица параметров"
  },
  {
    "English": "parameter model",
    "context": "1: Meanwhile, our findings on Allocation significantly deviate from Galactica. Figure 13 visualizes the Galactica models with our predicted efficient frontier in the same style as Figure 1. The creators of Galactica decided to train a 120 billion <mark>parameter model</mark> on 450 billion tokens, a significant overallocation to parameters even in Chinchilla terms (black efficient frontier).<br>2: As shown in Figure 3.12, the results improve with scale, with the the full 175 billion model improving by over 10% compared to the 13 billion <mark>parameter model</mark>. Figure 3.12: Zero-, one-,and few-shot performance on SAT analogy tasks, for different sizes of model.<br>",
    "Arabic": "نموذج المعلمات",
    "Chinese": "参数模型",
    "French": "modèle paramétrique",
    "Japanese": "パラメータモデル",
    "Russian": "параметрическая модель"
  },
  {
    "English": "parameter regularization",
    "context": "1: We note that this uncertainty can be rigorously addressed by extending the duality analysis of Dudík & Schapire (2006), leading to <mark>parameter regularization</mark> that may be naturally adopted in the causal setting as well. Theorem 3. The maximum causal entropy distribution minimizes the worst case prediction log-loss, i.e., inf \n<br>2: where the first term is the standard mean squared error, L is the representation balancing loss, Θ represents the parameters in this neural network model. and are two hyperparameters which control the weights for the representation balancing loss and the <mark>parameter regularization</mark> term. The ITE for each instance can be estimated as: = 1 − 0 .<br>",
    "Arabic": "ضبط المُعامِلات",
    "Chinese": "参数正则化",
    "French": "régularisation des paramètres",
    "Japanese": "パラメータ正則化",
    "Russian": "регуляризация параметров"
  },
  {
    "English": "parameter sharing",
    "context": "1: This modeling choice allows enforcing <mark>parameter sharing</mark> between similar nodes. It further gives the flexibility to handle st-graphs with more nodes without increasing the number of parameters.<br>2: Related formulations of transfer through <mark>parameter sharing</mark> have been studied by Fei-Fei et al. [30] and Stark et al. [31] for learning shape-based object models with few training images, though no prior models consider transferring knowledge based on relative comparisons, as we do here.<br>",
    "Arabic": "مشاركة المعلمات",
    "Chinese": "参数共享",
    "French": "partage des paramètres",
    "Japanese": "パラメータ共有",
    "Russian": "разделение параметров"
  },
  {
    "English": "parameter size",
    "context": "1: As our main objective is to probe a training strategy orthogonal to the model structure, we only include the above three baselines to control the model structure, data pre-requisites, and <mark>parameter sizes</mark>.<br>",
    "Arabic": "حجم المعلمة",
    "Chinese": "参数大小",
    "French": "Taille des paramètres",
    "Japanese": "パラメータサイズ",
    "Russian": "размер параметра"
  },
  {
    "English": "parameter space",
    "context": "1: It should report small distances for examples that are similar in the <mark>parameter space</mark> of interest (or that share a class label), and large distances for examples that are unrelated. General-purpose measures, such as L p norms, are not necessarily well-suited for all learning problems with a given data representation.<br>2: Both parametric models are embodied by identical likelihood functions L(θ|X), where X contains the values output by the underlying stochastic process, and θ is a set of parameters coming from the <mark>parameter space</mark> Θ.<br>",
    "Arabic": "مساحة المعلمة",
    "Chinese": "参数空间",
    "French": "espace des paramètres",
    "Japanese": "パラメータ空間",
    "Russian": "пространство параметров"
  },
  {
    "English": "parameter tuning",
    "context": "1: Moreover, note that these are the only parameters in the model, and therefore requires very little <mark>parameter tuning</mark>.<br>2: Another advantage is that the same coreset can be used for <mark>parameter tuning</mark> over a large set of candidates. In addition to other reasons, this significantly reduces the running time of such algorithms in our experiments; see Section 8.<br>",
    "Arabic": "ضبط المعاملات",
    "Chinese": "参数调优",
    "French": "réglage des paramètres",
    "Japanese": "パラメータ調整",
    "Russian": "настройка параметров"
  },
  {
    "English": "parameter tying",
    "context": "1: This treats the players anonymously, thus we implicitly and incorrectly assume that conditioned on the county's parameters each firm is identical. Due to the use <mark>parameter tying</mark>, the ICE predictor has an additional 156 model parameters. The test losses reported were computed using ten-fold cross validation.<br>",
    "Arabic": "تربيط المعاملات",
    "Chinese": "参数绑定",
    "French": "liage des paramètres",
    "Japanese": "パラメータ結合",
    "Russian": "привязка параметров"
  },
  {
    "English": "parameter update",
    "context": "1: A step is thus comprised of the following operations, in order: (1) given observation, agent takes action, (2) if applicable, agent update its parameters, (3) environment transitions based on action and return new observation. The <mark>parameter update</mark> step is implemented differently depending on the agent, described below.<br>2: Note that this procedure is inefficient in that it samples each particle φ k merely based on the first observation with label k. Therefore, we use this procedure for bootstrapping, and then run a Gibbs sampling scheme that iterates between <mark>parameter update</mark> and label update. ( Parameter update ) : We resample each particle ψ k from its source distribution conditioned on all samples with label k. In particular , for k ∈ [ 1 , m ] with r k > 0 , we draw ψ k ∼ T ( φ k , • ) | { x i : l i = k } , and for<br>",
    "Arabic": "تحديث المعلمات",
    "Chinese": "参数更新",
    "French": "mise à jour des paramètres",
    "Japanese": "パラメータ更新",
    "Russian": "обновление параметров"
  },
  {
    "English": "parameter vector",
    "context": "1: P distr (x; λ) = a(x) exp(λ ⊺ ϕ(x)). (2 \n ) \n where λ is a <mark>parameter vector</mark> of coefficients s.t. the resulting normalized distribution p distr respects the desired constraints on the features' moments. Finding the vector λ in Eq.<br>2: The MRF is specified by a set of d real valued potentials or sufficient statistics φ(x) = {φ k (x)} and a <mark>parameter vector</mark> θ ∈ R d : \n<br>",
    "Arabic": "متجه المعلمات",
    "Chinese": "参数向量",
    "French": "vecteur de paramètres",
    "Japanese": "パラメータベクトル",
    "Russian": "вектор параметров"
  },
  {
    "English": "parameter-efficient fine-tuning",
    "context": "1: Alternatively, <mark>parameter-efficient fine-tuning</mark> adapts pre-trained models to new languages by training a small set of weights effectively (Zhao et al., 2020;Pfeiffer et al., 2021;Ansell et al., 2022). Pfeiffer et al.<br>2: In Section 6, standard fine-tuning (FT) and multiple <mark>parameter-efficient fine-tuning</mark> (PEFT) are compared with the competitive WSL method CO-SINE. In this section, we provide additional plots which show the same comparison with the other WSL methods examined in this work, namely L2R, MLC, and BOND. We report average performance (Acc.<br>",
    "Arabic": "ضبط دقيق فعال للمعاملات",
    "Chinese": "参数高效微调",
    "French": "ajustement fin des paramètres efficace",
    "Japanese": "パラメータ効率的な微調整",
    "Russian": "параметроэффективная донастройка"
  },
  {
    "English": "parameterisation",
    "context": "1: We have chosen to cast this correspondence problem as that of defining the <mark>parameterisation</mark> φ i , of each shape so as to minimise the value of F in (12).<br>2: We show it is possible to establish correspondences automatically, by casting the correspondence problem as one of finding the 'optimal' <mark>parameterisation</mark> of each shape in the training set.<br>",
    "Arabic": "ترميز المعلمة",
    "Chinese": "参数化",
    "French": "paramétrisation",
    "Japanese": "パラメータ化",
    "Russian": "параметризация"
  },
  {
    "English": "parameterization",
    "context": "1: We also introduced potential heuristics as a fast alternative to optimal cost partitioning. By computing a heuristic <mark>parameterization</mark> only once and sticking with it, they obtain very fast state evaluations, leading to significant coverage increases and a more than 10-fold speedup over the state equation heuristic.<br>2: While our proposed <mark>parameterization</mark> of the policy using both β andφ has the advantages described above, the performance of β is now constrained by the quality ofφ, as in the endφ is responsible for selecting an action from A.<br>",
    "Arabic": "تحديد المعلمات",
    "Chinese": "参数化",
    "French": "paramétrisation",
    "Japanese": "パラメータ化",
    "Russian": "параметризация"
  },
  {
    "English": "parameterize model",
    "context": "1: We assume a <mark>parameterized model</mark> π θ (y | x) and minimize \n D KL [π θ (y|x) || π * (y | x) \n ] where π * is the optimal policy from Eq. 7 induced by the reward function r ϕ (y, x).<br>2: Based on the above prior, we can use a <mark>parameterized model</mark> p θ (x t−1 |x t , t) to learn the denoising process. | | Hello , nice to meet you .<br>",
    "Arabic": "نموذج المعلمة",
    "Chinese": "参数化模型",
    "French": "modèle paramétré",
    "Japanese": "パラメータ化モデル",
    "Russian": "параметризованная модель"
  },
  {
    "English": "parametric",
    "context": "1: Finally, while our results indicate that models can learn to disentangle contextual and <mark>parametric</mark> knowledge, it remains unclear what characterizes easy vs. difficult cases for disentanglement. One such attribute, for example, can be the frequency of a given fact in the pretraining data.<br>2: We denote this surface as ∂Ω E (j) = {x | Φ(x; j) = 0}, where Φ is parameterized with j as the weights and biases of the network. For clarity, we use Φ for <mark>parametric</mark> level-sets and ϕ for non-<mark>parametric</mark>.<br>",
    "Arabic": "بارامتري",
    "Chinese": "参数化的",
    "French": "paramétrique",
    "Japanese": "パラメトリック",
    "Russian": "параметрический"
  },
  {
    "English": "parametric family",
    "context": "1: Hyperparameter inference as described in Section 3.4 can lower the computational burden by adapting π(x) to the data, but it is important to choose an appropriate <mark>parametric family</mark>. In high dimensions, this choice is difficult to make, and in the absence of significant domain knowledge we expect that Archipelago will not work well in high dimensions.<br>2: The benefits of this approach depend on the specific choice of <mark>parametric family</mark> of c-convex functions (Rezende and Racanière, 2021; Cohen et al., 2021), trading-off expressivity with scalability.<br>",
    "Arabic": "عائلة بارامترية",
    "Chinese": "参数族",
    "French": "famille paramétrique",
    "Japanese": "パラメトリックファミリー",
    "Russian": "параметрическое семейство"
  },
  {
    "English": "parametric knowledge",
    "context": "1: We fine-tune T5 models  of two sizes (Large -770M parameters, XXL -11B parameters), as we found that model size greatly affects the amount of <mark>parametric knowledge</mark> available to the model. More details about the models are available in App. B. We train the following models: \n Closed-Book Baseline.<br>2: In this work, we propose a new paradigm for generative QA models that alleviates the above issues by encouraging disentanglement of <mark>parametric knowledge</mark> from contextual knowledge. Specifically, we propose a single model that generates two answers to a given question -a parametric answer and a contextual answer, in one-fell-swoop. Figure 1 exemplifies this. To achieve this , we use two training data augmentation methods : ( 1 ) Counterfactual Data Augmentation ( Longpre et al. , 2021 ) , obtained by automatically altering facts in a given QA corpus to decrease reliance on <mark>parametric knowledge</mark> , and ( 2 ) Answerability Augmentation , where we train the model to abstain from answering when no answer is present<br>",
    "Arabic": "المعرفة المعلمية",
    "Chinese": "参数化知识",
    "French": "connaissance paramétrique",
    "Japanese": "パラメトリック知識",
    "Russian": "параметрические знания"
  },
  {
    "English": "parametric model",
    "context": "1: πref(y|x) and we optimize our <mark>parametric model</mark> π θ , equivalently to the reward model optimization in Eq. 2 under the change of variables.<br>2: The VAE uses a decoder fed by latent vectors, drawn from a conditional density estimated from the fully sampled images using an encoder. Since fully sampled images are not available in our setting, we approximate the conditional density of the latent vectors by a <mark>parametric model</mark> whose parameters are estimated from the undersampled measurements using back-propagation.<br>",
    "Arabic": "نموذج معلمي",
    "Chinese": "参数模型",
    "French": "modèle paramétrique",
    "Japanese": "パラメトリックモデル",
    "Russian": "параметрическая модель"
  },
  {
    "English": "parametrization",
    "context": "1: where σ(x) = (1 + e −x ) −1 is the sigmoid function, and f n (•; Θ) : X → R is a real-valued function depending on the sample and the <mark>parametrization</mark> Θ.<br>2: We also test how semantics-preserving noise affects models of different sizes and <mark>parametrization</mark> (see Figure 2). Although for in-domain setup, the relaxed fooling rate metrics marginally drop as the models get bigger, the same cannot be observed in out-of-domain setup.<br>",
    "Arabic": "تعريف المعلمات",
    "Chinese": "参数化",
    "French": "paramétrisation",
    "Japanese": "パラメータ化",
    "Russian": "параметризация"
  },
  {
    "English": "paraphrase",
    "context": "1: Here, the <mark>paraphrase</mark>x j with the highest similarity score, i.e., G(x,x j ) = maxx i ∈X (G(x i , x)), has a rank of 1, therefore, d j = 1. The <mark>paraphrase</mark>x k with the lowest similarity score , i.e. , G ( x , x k ) = minx i ∈X ( G ( x i , x ) ) , has a rank of |X | , thus d k = C. Consequently , a larger rank indicates that the <mark>paraphrase</mark> is more grammatically and lexically different than the original input ,<br>2: We do not claim natural logic to be a universal solution for NLI. Many important types of inference are not amenable to natural logic , including <mark>paraphrase</mark> ( Eve was let go |= Eve lost her job ) , verb alternation ( he drained the oil |= the oil drained ) , relation extraction ( Aho , a trader at UBS , ... |= Aho works for UBS ) , common-sense reasoning ( the sink overflowed |=<br>",
    "Arabic": "إعادة صياغة",
    "Chinese": "近义词",
    "French": "paraphrase",
    "Japanese": "言い換え",
    "Russian": "парафраз"
  },
  {
    "English": "paraphrase generation",
    "context": "1: In order to retrieve paraphrasing augmentation with appropriate difficulty measures, we propose a curriculumaware <mark>paraphrase generation</mark> module.<br>2: In addition, we show that PARAAMR can be used to improve on three NLP tasks: learning sentence embeddings, syntactically controlled <mark>paraphrase generation</mark>, and data augmentation for few-shot learning. Our results thus showcase the potential of PARAAMR for improving various NLP applications.<br>",
    "Arabic": "توليد الصياغات المترادفة",
    "Chinese": "释义生成",
    "French": "génération de paraphrases",
    "Japanese": "言い換え生成",
    "Russian": "генерация парафраз"
  },
  {
    "English": "paraphrase generator",
    "context": "1: More precisely, the goal is to train a syntactically controlled <mark>paraphrase generator</mark> with the input being (source sentence, target constituency parse) pair and the output being a paraphrase sentence with syntax following the target constituency parse.<br>2: Specifically, we use the testing examples provided by previous work 8 (Huang and Chang, 2021) and calculate the BLEU score between the ground-truth and the generated output as the evaluation metric. Experimental results. Table 6 shows the results of syntactically controlled paraphrase generation. The <mark>paraphrase generator</mark> trained with PARAAMR performs significantly better than others.<br>",
    "Arabic": "مولد إعادة الصياغة",
    "Chinese": "释义生成器",
    "French": "générateur de paraphrases",
    "Japanese": "言い換え生成器",
    "Russian": "генератор перефразирования"
  },
  {
    "English": "paraphrase identification",
    "context": "1: In addition to the cross-domain study (Table 7), we conducted transfer learning experiments on three <mark>paraphrase identification</mark> datasets (Table 6). The most noteworthy phenomenon is that the SSE model performs better on Twitter-URL and PIT-2015 when trained on the large out-of-domain Quora data than the small in-domain training data.<br>2: We then fine-tune the syntactically-informed transformers for three downstream LU tasks: natural language inference (NLI) Conneau et al., 2018), <mark>paraphrase identification</mark> (Zhang et al., 2019b;Yang et al., 2019), and causal commonsense reasoning (Sap et al., 2019;Ponti et al., 2020).<br>",
    "Arabic": "تحديد إعادة الصياغة",
    "Chinese": "句子重述识别",
    "French": "identification de paraphrase",
    "Japanese": "言い換え識別",
    "Russian": "идентификация парафраз"
  },
  {
    "English": "paraphrase model",
    "context": "1: The more interesting part is how negative pairs are sampled, which are crucial to make the task nontrivial for <mark>paraphrase models</mark>. We sample a second, very similar signature from the k nearest signatures in lines 10 and 11 using Jaccard distance between signatures σ a and σ b : \n<br>",
    "Arabic": "نموذج إعادة الصياغة",
    "Chinese": "文本重述模型",
    "French": "modèle de paraphrase",
    "Japanese": "言い換えモデル",
    "Russian": "модель перефразирования"
  },
  {
    "English": "parent node",
    "context": "1: ▷ τ : (V, V ) → N is a function that maps a <mark>parent node</mark> to the index of the argument list of a function corresponding to a node. That is, given a node v and <mark>parent node</mark> u, τ maps to an index in {1, . . .<br>2: A leaf node represents a domain value and a <mark>parent node</mark> represents a less specific value. For a numerical attribute in QID, a taxonomy tree can be grown at runtime, where each node represents an interval, and each non-leaf node has two child nodes representing some optimal binary split of the parent interval.<br>",
    "Arabic": "العقدة الأصلية",
    "Chinese": "父节点",
    "French": "nœud parent",
    "Japanese": "親ノード",
    "Russian": "родительский узел"
  },
  {
    "English": "pareto optimal",
    "context": "1: The red curve indicates the <mark>Pareto optimal</mark> test error ε achievable from a tradeoff between α tot and f at fixed α prune . B: We find that when data is abundant (scarce) corresponding to large (small) α tot , the better pruning strategy is to keep the hard (easy) examples.<br>2: While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the <mark>Pareto optimal</mark> latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular frame, the surrounding world has changed.<br>",
    "Arabic": "باريتو الأمثل",
    "Chinese": "帕累托最优",
    "French": "optimal de Pareto",
    "Japanese": "パレート最適",
    "Russian": "оптимум по Парето"
  },
  {
    "English": "pareto optimality",
    "context": "1: Definition 6.2 (fPO [Barman et al., 2018]). An allocation A is said to satisfy fractional <mark>Pareto optimality</mark> (fPO) if it is not Pareto dominated by any fractional allocation. 6 \n As Barman et al. [2018] noted, an fPO allocation is also PO but not vice versa.<br>2: Here, we show the property of <mark>Pareto optimality</mark> of C(P, Q). We refer to the textbook [23] for more background on information theory and KL divergence. The main property we will show in this section is the following. Proposition 1.<br>",
    "Arabic": "الكفاءة البارتو",
    "Chinese": "帕累托最优性",
    "French": "optimalité de Pareto",
    "Japanese": "パレート最適性",
    "Russian": "Парето-оптимальность"
  },
  {
    "English": "pareto-efficient",
    "context": "1: We focus on fairness, and introduce the SD-core, a group fairness notion. Our Nash rules are in the SD-core, and the leximin rules satisfy individual fairness properties. Both are <mark>Pareto-efficient</mark>.<br>2: over the 60 MDPs); the y-axis shows the mean expected true reward of that heuristic technique. Ideally, we would like a technique that provides a high true reward with a low number of states. Points in the upper-left frontier of the graph represent <mark>Pareto-efficient</mark> tradeoffs between state space size and expected true reward.<br>",
    "Arabic": "كفاءة باريتو",
    "Chinese": "帕累托有效",
    "French": "pareto-optimal",
    "Japanese": "パレート最適",
    "Russian": "Парето-эффективный"
  },
  {
    "English": "parse",
    "context": "1: Denoting the attention weight from token t to a candidate head q as A <mark>parse</mark> [t, q], we model the probability of token t having parent q as: \n P (q = head(t) | X ) = A <mark>parse</mark> [t, q] \n<br>2: The degradation in F1 is about half of the degradation from the full lexical ablation, suggesting that a significant portion of the lexical cues comes from the context of a <mark>parse</mark>. Figure 4 illustrates the importance of context with an incorrect partial <mark>parse</mark> that appears syntactically plausible in isolation.<br>",
    "Arabic": "تحليل",
    "Chinese": "解析",
    "French": "analyse syntaxique",
    "Japanese": "構文解析",
    "Russian": "разбор"
  },
  {
    "English": "parse accuracy",
    "context": "1: As shown in Cohen et al., 2010), the <mark>parsing accuracy</mark> of the TSG model is strongly affected by its backoff model. The effects of our hierarchical backoff model on parsing performance are evaluated in Section 5.<br>2: . ., prefix90%, prefix100%}. The FULLTOGRAPH parser is trained only using the prefix100% data. For our PREFIXTOGRAPH parser, we experiment with training on different mixtures of the prefix datasets, to quantify the effect on <mark>parsing accuracy</mark>.<br>",
    "Arabic": "دقة التحليل",
    "Chinese": "解析准确率",
    "French": "précision de l'analyse syntaxique",
    "Japanese": "構文解析精度",
    "Russian": "точность разбора"
  },
  {
    "English": "parse algorithm",
    "context": "1: Our algorithm is an adaptation of the <mark>parsing algorithm</mark> for SHAG by Eisner and Satta (1999) to the case of non-deterministic head-automata, and has a runtime cost of O(n 2 N 3 ), where n is the number of states of the model, and N is the length of the input sentence.<br>2: This is because the model sets its weights with respect to the <mark>parsing algorithm</mark> and will disfavor features over unlikely non-projective edges. Since the space of projective trees is a subset of the space of non-projective trees, it is natural to wonder how the Chu-Liu-Edmonds <mark>parsing algorithm</mark> performs on projective data since it is asymptotically better than the Eisner algorithm.<br>",
    "Arabic": "خوارزمية التحليل",
    "Chinese": "解析算法",
    "French": "algorithme d'analyse syntaxique",
    "Japanese": "構文解析アルゴリズム",
    "Russian": "алгоритм синтаксического разбора"
  },
  {
    "English": "parse chart",
    "context": "1: Search on parse forests Traditionally, the hypergraph represents a packed <mark>parse chart</mark>. In this work, our hypergraph instead represents a forest of parses. Figure 1 contrasts the two representations. In the <mark>parse chart</mark>, labels on the nodes represent local properties of a parse, such as the category of a span in Figure 1a.<br>",
    "Arabic": "تحليل الجدول",
    "Chinese": "解析图表",
    "French": "Tableau d'analyse syntaxique",
    "Japanese": "構文解析チャート",
    "Russian": "таблица синтаксического анализа"
  },
  {
    "English": "parse forest",
    "context": "1: For example, there are two nodes spanning the phrase Fruit flies with the same category NP but different internal substructures. While the <mark>parse forest</mark> requires an exponential number of nodes in the hypergraph, the model scores can depend on entire subtrees.<br>2: During training (Algorithm 1), we assume access to sentences labeled with gold parse treesŷ and gold derivationsÊ. The gold derivationÊ is a path from ∅ toŷ in the <mark>parse forest</mark>. A * search with our global model is not guaranteed to terminate in sub-exponential time.<br>",
    "Arabic": "غابة التحليل",
    "Chinese": "解析森林",
    "French": "forêt d'analyses syntaxiques",
    "Japanese": "パースフォレスト",
    "Russian": "лес разбора"
  },
  {
    "English": "parse model",
    "context": "1: Experimental setup. We used Bikel's reimplementation of Collins' <mark>parsing model</mark> 2 (Bikel, 2004). Sections 02-21 and 23 of the WSJ were stripped from their annotation. Sections 2-21 (39832 sentences, about 800K constituents) were used for training, Section 23 (2416 sentences) for testing. No development set was used.<br>2: Hwa (2004) used uncertainty sampling with the tree entropy (TE) selection function 1 to select training samples for the Collins parser. In each iteration, each of the unlabelled pool sentences is parsed by the <mark>parsing model</mark>, which outputs a list of trees ranked by their probabilities.<br>",
    "Arabic": "نموذج التحليل النحوي",
    "Chinese": "解析模型",
    "French": "modèle d'analyse syntaxique",
    "Japanese": "構文解析モデル",
    "Russian": "модель синтаксического анализа"
  },
  {
    "English": "parse performance",
    "context": "1: 6 We examine the effect of each transformation on development set <mark>parsing performance</mark> and discard those which do not improve performance. We keep all the input sentence transformations and those treebank transformations which affect lexical rules, i.e. changing the endings on adverbs and changing the first character of proper nouns.<br>",
    "Arabic": "تحليل الأداء",
    "Chinese": "解析性能",
    "French": "performance d'analyse syntaxique",
    "Japanese": "構文解析性能",
    "Russian": "производительность синтаксического анализа"
  },
  {
    "English": "parse score",
    "context": "1: We replace the original verb with each of these new verbs and generate one new sentence for each new verb; the sentence is retained if the <mark>parse score</mark> for the new sentence is higher than the <mark>parse score</mark> for the original sentence.<br>",
    "Arabic": "درجة التحليل",
    "Chinese": "解析得分",
    "French": "score d'analyse syntaxique",
    "Japanese": "構文解析スコア",
    "Russian": "оценка разбора"
  },
  {
    "English": "parse structure",
    "context": "1: Therefore, this model encodes the lexical information available in the full model but does not encode the <mark>parse structure</mark> beyond the local rule production. While the dynamic program allows this model to find the optimal parse with fewer explorations, the lack of global features significantly hurts its parsing accuracy.<br>",
    "Arabic": "بُنْيَة التَّحْليلِ",
    "Chinese": "语法分析结构",
    "French": "structure d'analyse",
    "Japanese": "構文構造",
    "Russian": "структура разбора"
  },
  {
    "English": "parse tree",
    "context": "1: During decoding, an exact algorithm is used to recover a valid sequence of tags which is then converted back to a <mark>parse tree</mark>.<br>2: In an attempt to develop parsers parallelizable during training, a recent line of work recasts parsing as tagging (Li et al., 2018;Strzyz et al., 2019;Kitaev and Klein, 2020;Amini and Cotterell, 2022). Under this approach, a <mark>parse tree</mark> is linearized into a sequence of tags.<br>",
    "Arabic": "شجرة التحليل",
    "Chinese": "语法树",
    "French": "arbre syntaxique",
    "Japanese": "構文木",
    "Russian": "дерево синтаксического анализа"
  },
  {
    "English": "parser",
    "context": "1: Again, Lauer's (1995) heuristic and Abney's (1996) partial <mark>parser</mark> were used to identify bigrams, and proper nouns and low-frequency nouns were excluded. For each noun and verb, three bigrams were randomly selected from the set of their non-co-occurring nouns.<br>2: The long distance between the source and destination of this type of dependency, paired with the relatively low probability of their occurrence in the language, and the fact that filler-gap annotations in syntactic resources such as the Penn Treebank are often stripped out, makes it very difficult for <mark>parser</mark>s to recognize this type of dependency correctly.<br>",
    "Arabic": "مفسر",
    "Chinese": "语法分析器",
    "French": "analyseur",
    "Japanese": "構文解析器",
    "Russian": "анализатор"
  },
  {
    "English": "part of speech",
    "context": "1: The latter uses features such as length, TF.IDF-weighted words, repetitions, superlatives, and lists of fixed phrases targeting specific propaganda techniques. The output from the hybrid model was further post-processed using some syntactic rules based on <mark>part of speech</mark>.<br>2: The length of the feature vector may vary across parts of speech. Let N c denote the length of the feature vector for <mark>part of speech</mark> c, x r,l denote the time-series (x 1 r,l , . . .<br>",
    "Arabic": "جزء من الكلام",
    "Chinese": "词性",
    "French": "partie du discours",
    "Japanese": "品詞",
    "Russian": "часть речи"
  },
  {
    "English": "part of speech tag",
    "context": "1: (2009) report that the syntactic productions in adjacent sentences are powerful features for predicting which discourse relation (cause, contrast, etc.) holds between them. Cocco et al. (2011) show that significant associations exist between certain <mark>part of speech tags</mark> and sentence types such as explanation, dialog and argumentation.<br>2: Figure 8 presents more examples of our templating mechanism. We combine an adapted version of the Penn Treebank Project's <mark>part of speech tags</mark> along with articles, conjunctions, prepositions, and other filler words to construct these templates. Additionally, we provide the stress pattern of the syllables to ensure that the constraint of iambic pentameter is met.<br>",
    "Arabic": "علامة جزء من الكلام",
    "Chinese": "词性标签",
    "French": "étiquette de partie du discours",
    "Japanese": "品詞タグ",
    "Russian": "тег части речи"
  },
  {
    "English": "part of speech tagger",
    "context": "1: In this section, we discuss our experimental setup for the semantic role labeling system. Similar to the CoNLL 2005 shared tasks, we train our system using sections 02-21 of the Wall Street Journal portion of Penn TreeBank labeled with PropBank. We test our system on an annotated Brown corpus consisting of three sections (ck01 -ck03). Since we need to annotate new sentences with syntactic parse , POS tags and shallow parses , we do not use annotations in the CoNLL distribution ; instead , we re-annotate the data using publicly available <mark>part of speech tagger</mark> and shallow parser 1 , Charniak 2005 parser ( Charniak and Johnson , 2005 ) and Stanford parser ( Klein and Manning , 2003<br>",
    "Arabic": "جهاز وسم أجزاء الكلام",
    "Chinese": "词性标注器",
    "French": "étiqueteur de parties du discours",
    "Japanese": "品詞タガー",
    "Russian": "теггер части речи"
  },
  {
    "English": "partial assignment",
    "context": "1: , n} be the indices of x, let C ⊆ I, let x C ∈ R |C| be the restriction of x to the indices in C, and let ρ C ∈ domain(x C ) be a <mark>partial assignment</mark> where only the variables corresponding to the indices in C are assigned values.<br>2: A (partial) assignment is a (partial) function from variables to {0, 1}. A <mark>partial assignment</mark> is also referred to as a restriction. A substitution (or affine restriction) can also map variables to literals.<br>",
    "Arabic": "التعيين الجزئي",
    "Chinese": "部分赋值",
    "French": "affectation partielle",
    "Japanese": "部分割り当て",
    "Russian": "частичное присвоение"
  },
  {
    "English": "partial derivation",
    "context": "1: Beam search addresses this challenge by retaining a collection called the \"beam\" of parser states at each word. These states are rated by a score that is related to the probability of a <mark>partial derivation</mark>, allowing an incremental parser to hedge its bets against temporary ambiguity.<br>",
    "Arabic": "اشتقاق جزئي",
    "Chinese": "部分推导",
    "French": "dérivation partielle",
    "Japanese": "部分導出",
    "Russian": "частичное выведение"
  },
  {
    "English": "partial evaluation",
    "context": "1: In the previous section, we proved that <mark>partial evaluation</mark> using the backed-up value function v, as given in ( 6)-( 7), is not necessarily a process converging toward the optimal value. In this section, we propose a natural respective fix: back up the value T h−1 v and perform the <mark>partial evaluation</mark> w.r.t.<br>",
    "Arabic": "تقييم جزئي",
    "Chinese": "部分求值",
    "French": "évaluation partielle",
    "Japanese": "部分評価",
    "Russian": "частичная оценка"
  },
  {
    "English": "partial observability",
    "context": "1: Many important interaction tasks involve <mark>partial observability</mark>. In medical diagnosis, for example, sequences of symptoms, tests, and treatments are used to identify (and mediate) unknown illnesses. Motivated by the objective of learning diagnosis policies from experts, we investigate the inverse diagnostics problem of modeling interaction with partially observed systems.<br>2: Our choice of policy-gradient algorithms is motivated by their ease of integration with CRFs, but they have the additional benefit of being guaranteed to converge (possibly to a poor local maximum) despite the use of function approximation and <mark>partial observability</mark>. Our model of multi-agent learning is similar to Guestrin et al.<br>",
    "Arabic": "إمكانية الملاحظة الجزئية",
    "Chinese": "部分可观测性",
    "French": "observabilité partielle",
    "Japanese": "部分的観測可能性",
    "Russian": "частичная наблюдаемость"
  },
  {
    "English": "partial order",
    "context": "1: It is quite easy to come up with such formulas that count every AMO at least once -but, of course, we have to ensure that we count every AMO exactly once. To ensure this property, we introduce for every AMO α a <mark>partial order</mark> ≺ α on the maximal cliques.<br>2: Examples of this sort include frequent itemsets, association rules, induced subgraphs, etc. Normally, a <mark>partial order</mark>, , can be defined among all frequent patterns in such a way as to preserve the downward closure property, i.e., given any patterns p1 and p2, if p1 p2 and p2 is frequent, so is p1.<br>",
    "Arabic": "ترتيب جزئي",
    "Chinese": "偏序",
    "French": "ordre partiel",
    "Japanese": "半順序",
    "Russian": "частичный порядок"
  },
  {
    "English": "partially observable Markov decision process",
    "context": "1: Our work targets cooperative MARL, where agents execute actions that jointly affect the environment, then receive feedback via local observations and a shared reward. This setting is formalized as a Decentralized <mark>Partially Observable Markov Decision Process</mark> ( Dec-POMDP ) , defined as I , S , A , T , R , Ω , O , γ ( Oliehoek and Amato 2016 ) ; I is the set of n agents , S is the state space , A = × i A i is the joint action<br>",
    "Arabic": "عملية اتخاذ قرار ماركوف يمكن ملاحظتها جزئيا",
    "Chinese": "部分可观测马尔可夫决策过程",
    "French": "processus de décision de Markov partiellement observable",
    "Japanese": "部分観測マルコフ決定過程",
    "Russian": "частично наблюдаемый марковский процесс принятия решений"
  },
  {
    "English": "particle filter",
    "context": "1: Rao-Blackwellised <mark>particle filter</mark>s (RBPF) estimate this factorized posterior by sampling the discrete states using a <mark>particle filter</mark> and then estimating the person's location and motion velocity using Kalman filters conditioned on the samples. More specifically , RBPFs represent posteriors by sets of weighted samples , or particles : i ) , where the person 's location and velocity are represented by µ k ( i ) , Σ k ( i ) , the mean and covariance of the Kalman filter , which represents posteriors by Gaussian approximations ( Bar-Shalom , Li , & Kirubarajan 2001 )<br>2: For the quantitative evaluation we use k = 1, however the next neighbors can yield valuable information on ambiguous views and could for example be used in <mark>particle filter</mark> based tracking. We use cosine similarity because (1) it can be very efficiently computed on a single GPU even for large codebooks.<br>",
    "Arabic": "مُرشِّح الجسيمات",
    "Chinese": "粒子滤波器",
    "French": "filtre à particules",
    "Japanese": "粒子フィルタ",
    "Russian": "фильтр частиц"
  },
  {
    "English": "partition",
    "context": "1: For each value c in child(Best), a new <mark>partition</mark> Pc is created from PBest, and data records in PBest are split among the new <mark>partition</mark>s: Pc contains a data record in P Best if c generalizes the corresponding domain value in the record. An empty P c is removed.<br>2: For each center c i ∈ A, let X i ⊆ X denote the set of points whose closest center in A is c i . By arbitrarily breaking ties, we can assume that the sets X i are disjoint, i.e., {X i } 1≤i≤k forms a <mark>partition</mark> of X.<br>",
    "Arabic": "تقسيم",
    "Chinese": "划分",
    "French": "partition",
    "Japanese": "分割",
    "Russian": "разбиение"
  },
  {
    "English": "partition function",
    "context": "1: First, as explicitly noted in [3], the parser does not compute the <mark>partition function</mark> (normalization constant) for its distributions so the numbers it returns are not true probabilities. We noted there that if we replaced the \"max-ent inspired\" feature with standard deleted interpolation smoothing, we took a significant hit in performance.<br>2: 5 for r * (x, y) into the preference model Eq. 1, the <mark>partition function</mark> cancels, and we can express the human preference probability in terms of only the optimal policy π * and reference policy π ref . Thus, the optimal RLHF policy π * under the Bradley-Terry model satisfies the preference model: \n<br>",
    "Arabic": "دالة التقسيم",
    "Chinese": "配分函数",
    "French": "fonction de partition",
    "Japanese": "分配関数",
    "Russian": "функция разбиения"
  },
  {
    "English": "parzen window",
    "context": "1: where the <mark>Parzen window</mark> turns out to be k. We also show that γ k is an upper bound on the margin of a hard-margin support vector machine ( SVM ) .<br>",
    "Arabic": "نافذة بارزن",
    "Chinese": "帕尔森窗",
    "French": "fenêtre de Parzen",
    "Japanese": "パーゼン窓",
    "Russian": "парзеновское окно"
  },
  {
    "English": "patch",
    "context": "1: x and l i y are the locations (in absolute coordinates) of the i-th <mark>patch</mark> in the hidden and observed ensembles; d i \n x and d i y are the descriptor vectors of the i-th <mark>patch</mark> in each ensemble. P(x, y) of Eq. (1) using Eqs.<br>2: Ours is the best performer and second best in time cost per frame. help the performance. Fig. 8 demonstrates how prediction from coarse layers (large <mark>patch</mark>) help the lower layer (small <mark>patch</mark>) find correct correspondences in repetitive patterns, justifying the hierarchy.<br>",
    "Arabic": "شريحة",
    "Chinese": "图像块",
    "French": "patch",
    "Japanese": "パッチ",
    "Russian": "патч"
  },
  {
    "English": "path integration",
    "context": "1: dL \n The two terms in the above equation can be easily understood. The first says that the representation at each location, z(x), should be updated according to what its neighbours think it should (this is the same update rule as <mark>path integration</mark>!).<br>2: Indeed, just like cellular automata, it is also possible to initialise a single 'cell' (location) of the cellular automata, and have that representations propagate throughout the space. In this case, it's just like <mark>path integration</mark>, but spreading through all space at once.<br>",
    "Arabic": "التكامل المساري",
    "Chinese": "路径积分",
    "French": "intégration de chemin",
    "Japanese": "経路積分",
    "Russian": "интеграция пути"
  },
  {
    "English": "path planning",
    "context": "1: Mapping the world is an important requirement for spatial intelligence applications in augmented reality or robotics. Tasks like visual localization or <mark>path planning</mark> can benefit from accurate sparse or dense 3D reconstructions of the environment. These can be built from images using Structure-from-Motion (SfM), which associates observations across views to estimate camera parameters and 3D scene geometry.<br>2: Lastly the power board includes a Teensy MCU in order to provide a simple interface to sensors such as wheel encoders and add-ons such as RF receivers for long range remote control. Odometry: Precise odometry is critical for path planing, mapping, and localization.<br>",
    "Arabic": "تخطيط المسار",
    "Chinese": "路径规划",
    "French": "planification de trajectoire",
    "Japanese": "経路計画",
    "Russian": "планирование пути"
  },
  {
    "English": "pattern profile",
    "context": "1: In addition, without accessing the original dataset, we can conclude that bcd in D1 is more frequent than the other size-3 subpatterns of abcd . Pattern profile actually provides more information than the master pattern itself; it encodes the distribution of subpatterns. The key difference between our profile model and the itemset model proposed by Afrati et al.<br>2: Although they explored some kind of context information, none of the work can provide in-depth semantic annotations for frequent patterns as we do in our work. The context model proposed in our work covers both the <mark>pattern profile</mark> in [21] and transaction coverage in [20] as special cases.<br>",
    "Arabic": "الملف الشخصي للنمط",
    "Chinese": "模式剖析",
    "French": "profil de motif",
    "Japanese": "パターンプロファイル",
    "Russian": "профиль шаблонов"
  },
  {
    "English": "pattern recognition",
    "context": "1: Optimizing the corresponding parameters λ 1 and λ 2 of the model in Eq. 8 is equivalent to the optimization of model scaling factors, which is a standard approach in other areas such as speech recognition or <mark>pattern recognition</mark>. The use of an 'inverted' translation model in the unconventional decision rule of Eq.<br>2: An influential idea in <mark>pattern recognition</mark> is recognition by parts, i.e., decomposing an object into a model composed of (a) individual components, and (b) the relations (temporal or spatial) between these components. Recognition then becomes a matter of detecting individual components and then \"parsing\" their likely configurations relative to each other.<br>",
    "Arabic": "التعرف على الأنماط",
    "Chinese": "模式识别",
    "French": "reconnaissance de motifs",
    "Japanese": "パターン認識",
    "Russian": "распознавание образов"
  },
  {
    "English": "pattern summarization",
    "context": "1: One may first think of using existing techniques such as <mark>pattern summarization</mark> and dimension reduction to remove the redundancy of context units. While the context units can be any patterns in principle, we are practically not interested in those with very low frequency in the databases. Therefore, the context units we initially include are frequent patterns.<br>2: Our pattern profile model shows how to represent a set of patterns in a compact way and how to recover their supports without accessing the original dataset. However, the problem of selecting a set of similar patterns for summarization is not yet solved. We formalize this summarization problem as follows. Definition 6 (Pattern Summarization).<br>",
    "Arabic": "تلخيص الأنماط",
    "Chinese": "模式摘要",
    "French": "résumé des motifs",
    "Japanese": "パターン要約",
    "Russian": "суммаризация шаблонов"
  },
  {
    "English": "pattern-verbalizer pair",
    "context": "1: We consider the task of mapping inputs x ∈ X to outputs y ∈ Y , for which PET requires a set of <mark>pattern-verbalizer pairs</mark> (PVPs). Each PVP p = (P, v) consists of • a pattern P : X → T * that maps inputs to cloze questions containing a single mask; \n<br>",
    "Arabic": "زوج النمط اللفظي",
    "Chinese": "模式-语言化器对",
    "French": "paire modèle-verbalisateur",
    "Japanese": "パターン-言語化ペア",
    "Russian": "пара шаблон-вербализатор"
  },
  {
    "English": "payoff function",
    "context": "1: The sample complexity of the algorithm is 2T . Proof. Algorithm 2 implements Algorithm 1 on the convex-concave game (Θ, ∆(D × L), ϕ), where the <mark>payoff function</mark> ϕ is 1-smooth and defined as ϕ(θ, (D, ℓ)) = R D,ℓ (h θ ).<br>2: The multi-distribution learning problem corresponds to a zero-sum game with a minimizing player having action set H, a maximizing player having action set D × L, and a <mark>payoff function</mark> ϕ(h, (D, ℓ)) = R D,ℓ (h).<br>",
    "Arabic": "وظيفة الدفع",
    "Chinese": "收益函数",
    "French": "fonction de gain",
    "Japanese": "報酬関数",
    "Russian": "функция выигрыша"
  },
  {
    "English": "payoff matrix",
    "context": "1: , N , consisting of observed actions, from one or both players, sampled from the equilibrium strategies (u * , v * ). The goal is to recover the true underlying <mark>payoff matrix</mark> P , or a function form P (x) depending on the current context.<br>2: Consider the <mark>payoff matrix</mark> 1 0.9 −0.7 1 (where P 1 chooses a row and P 2 simultaneously chooses a column; the chosen entry in the matrix is the payoff for P 1 while P 2 receives the opposite). We now proceed to introducing our improvements to the CFR family.<br>",
    "Arabic": "مصفوفة المكافأة",
    "Chinese": "回报矩阵",
    "French": "matrice des gains",
    "Japanese": "支払い行列",
    "Russian": "матрица выплат"
  },
  {
    "English": "pedestrian detection",
    "context": "1: DV FRQWH [ W 6FDQQLQJ ZLQGRZ GHWHFWRU E & ULFNHW % DOO G 7HQQLV 5DFNHW Figure 6 \n . Object detection results measured by precisionrecall curves. We compare our algorithm to a scanning window detector and a detector that uses <mark>pedestrian detection</mark> as the human context for object detection.<br>",
    "Arabic": "الكشف عن المشاة",
    "Chinese": "行人检测",
    "French": "détection de piétons",
    "Japanese": "歩行者検出",
    "Russian": "детекция пешеходов"
  },
  {
    "English": "penalty function",
    "context": "1: Agents operating in unstructured environments often create negative side effects (NSE) that may not be easy to identify at design time. We examine how various forms of human feedback or autonomous exploration can be used to learn a <mark>penalty function</mark> associated with NSE during system deployment.<br>2: for some <mark>penalty function</mark> ρ : R → R. As mentioned above, the PLR problem ( 29) is often tackled using coordinate descent algorithms; that is, by iterating over the coordinates of b 1 , . . .<br>",
    "Arabic": "دالة الجزاء",
    "Chinese": "惩罚函数",
    "French": "fonction de pénalité",
    "Japanese": "罰則関数",
    "Russian": "функция штрафов"
  },
  {
    "English": "penalty parameter",
    "context": "1: Here we use the same <mark>penalty parameter</mark> λ for notational simplicity and consistency with the original formulation of weak hierarchical Lasso (5) studied in [2]. Though the factorization introduces more variables and constraints, we show that the resulting proximal operator admits a closed form solution.<br>2: Γ is the set of all possible entity type violations, φ γ is the <mark>penalty parameter</mark> for violation type γ, and #(γ, y, x) is the count of violations γ in sequence y.<br>",
    "Arabic": "معامل العقوبة",
    "Chinese": "惩罚参数",
    "French": "paramètre de pénalité",
    "Japanese": "罰則パラメータ",
    "Russian": "Штрафной параметр"
  },
  {
    "English": "penalty term",
    "context": "1: Compared with (2), although both methods can be used to learn features over many tasks, their work uses a different <mark>penalty term</mark> in the optimization problem. Our work, by contrast, focuses on the multitask Lasso which uses the sum of sup-norm penalty. Remark 1.<br>2: To penalize T i for capturing \"too much\" contextual information, our modified objective (2) adds a <mark>penalty term</mark> • I(T i ; X |X i ), which measures the amount of information about T i given by the sentence X as a whole, beyond what is given byX i : \n<br>",
    "Arabic": "مصطلح العقوبة",
    "Chinese": "惩罚项",
    "French": "terme de pénalité",
    "Japanese": "罰則項",
    "Russian": "штрафной член"
  },
  {
    "English": "per-pixel",
    "context": "1: We introduce a class-balancing weight β on a <mark>per-pixel</mark> term basis. Index j is over the image spatial dimensions of image X. Then we use this class-balancing weight as a simple way to offset this imbalance between edge and non-edge.<br>",
    "Arabic": "تعبير مقابل للبكسل",
    "Chinese": "每像素",
    "French": "par pixel",
    "Japanese": "ピクセル単位で",
    "Russian": "попиксельный"
  },
  {
    "English": "perception",
    "context": "1: Speci cally the goal is to create planning, <mark>perception</mark>, and scheduling algorithms which adapt to the context of the vehicle's operating environment. is regime was explored in a study on CPU/GPU resource allocation for camera-based <mark>perception</mark> and control [35].<br>2: We discuss the system-level design for the autonomous driving algorithm framework. A planning-oriented pipeline is proposed toward the ultimate pursuit for planning, namely UniAD. We provide detailed analyses on the necessity of each module within <mark>perception</mark> and prediction.<br>",
    "Arabic": "الإدراك",
    "Chinese": "感知",
    "French": "perception",
    "Japanese": "認識 (Ninshiki)",
    "Russian": "восприятие"
  },
  {
    "English": "perceptron algorithm",
    "context": "1: Soon thereafter, [Coh97] gave a polynomial-time proper learning algorithm for the problem. Subsequently, Dunagan and Vempala [DV04b] gave a rescaled <mark>perceptron algorithm</mark> for solving linear programs, which translates to a significantly simpler and faster proper learning algorithm. The term \"Massart noise\" was coined after [MN06].<br>2: Using the gradient is fast because the relevant derivatives are already known and their number is moderate. The ProcessNew operation is closely related to the <mark>perceptron algorithm</mark>.<br>",
    "Arabic": "خوارزمية البرسيبترون",
    "Chinese": "感知机算法",
    "French": "algorithme du perceptron",
    "Japanese": "パーセプトロンアルゴリズム",
    "Russian": "перцептронный алгоритм"
  },
  {
    "English": "perceptual feature",
    "context": "1: Rosie uses SII to learn many different types of knowledge, including <mark>perceptual features</mark> and spatial relationships (Mohan et al. 2012), hierarchical state-based concepts (Kirk and Laird 2019), games and puzzles (Kirk and Laird 2016), hierarchical goal-Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org).<br>",
    "Arabic": "ميزة إدراكية",
    "Chinese": "感知特征",
    "French": "caractéristique perceptuelle",
    "Japanese": "知覚特徴",
    "Russian": "перцептивная особенность"
  },
  {
    "English": "perceptual loss",
    "context": "1: The <mark>perceptual loss</mark> [8] compares the generated image with the ground truth training image p , using the activations F u of the VGG network [21] at layer u in a set of predefined layers U . L perceptual = u∈U 1 u ||F u (p) − F u (p )|| 1 \n<br>2: The L 1 loss function Eq. (3) is sensitive to small geometric imperfections and tends to result in blurry reconstructions. We add a <mark>perceptual loss</mark> term to mitigate this problem. The k-th layer of an off-the-shelf image encoder e (VGG16 in our case [53]) predicts a representation e (k)  \n<br>",
    "Arabic": "خسارة إدراكية",
    "Chinese": "感知损失",
    "French": "perte perceptuelle",
    "Japanese": "知覚損失",
    "Russian": "перцептивная потеря"
  },
  {
    "English": "perfect matching",
    "context": "1: In a similar manner, when λ = (n − 2, 2), the complete bipartite graph G λ has D λ = n 2 nodes on the left and right; each permutation corresponds to a <mark>perfect matching</mark> in this graph.<br>2: However, implementing the rule with these two queries comes with a trade-off: voters need to wait for possibly n rounds after reporting their top choice. Note that our proof of Theorem 1 also implies that there is always a candidate whose domination graph has a <mark>perfect matching</mark>.<br>",
    "Arabic": "مطابقة مثالية",
    "Chinese": "完美匹配",
    "French": "couplage parfait",
    "Japanese": "完全マッチング",
    "Russian": "идеальное соответствие"
  },
  {
    "English": "performance difference lemma",
    "context": "1: Proof. By <mark>performance difference lemma</mark> (Kakade & Langford, 2002), \n J(π) − J(µ) = 1 1−γ E µ [Q π (s, π) − Q π (s, a)]. Therefore , if Q π ∈ F on states of µ , then ( 1 − γ ) ( J ( π ) − J ( µ ) ) = L µ ( π , Q π ) = L µ ( π , Q π ) + βE ( Q π , π ) ≥ L µ ( π , f π ) + βE ( f π , π ) ≥ L µ ( π , f π ) , where we use E ( π , Q π ) = 0 by definition of Q π and E ( π , f ) ≥ 0 for any f ∈ F. Robust policy improvement follows , as J ( π * ) − J ( µ<br>2: (by the extension of <mark>performance difference lemma</mark> (see, e.g., Cheng et al., 2020, Lemma 1 \n ) ) = L µ ( π k , f π k ) + E µ [ f π k ( s , a ) − ( T π k f π k ) ( s , a ) ] + E d π k [ ( T π k f π k ) ( s , a ) − f π k ( s , a ) ] =⇒ |L µ ( π k , Q π k ) − L µ ( π k , f π k ) | ≤ f π k − T π k f π k 2 , µ + T π k f π k − f π k 2 , d π k ≤ O ( √ ε F<br>",
    "Arabic": "قرينة الفرق في الأداء",
    "Chinese": "性能差异引理",
    "French": "lemme de différence de performance",
    "Japanese": "性能差の補題",
    "Russian": "Лемма о разнице в производительности"
  },
  {
    "English": "permutation",
    "context": "1: A natural question is therefore how much information can the oracle have and still keep the competitive regret low? We show that the oracle can know the distribution exactly up to <mark>permutation</mark>, and still the regret will be very small. Two distributions p and p <mark>permutation</mark> equivalent if for some <mark>permutation</mark> σ of [k], \n<br>2: Benchmarking conformal sets for the least absolute deviation regression models with a ridge regularization on real datasets. We display the lengths of the confidence sets over 100 random <mark>permutation</mark> of the data. We denoted cov the average coverage and T the average computational time normalized with the average time for computing oracleCP which requires a single full data model fit.<br>",
    "Arabic": "ترتيب",
    "Chinese": "排列",
    "French": "permutation",
    "Japanese": "置換",
    "Russian": "перестановка"
  },
  {
    "English": "permutation invariance",
    "context": "1: A similar approach was applied to spatial patches by Ranzato et al. (2014). We call the resulting context length (32 2 or 48 2 or 64 2 ) the model resolution (MR). Note that this reduction breaks <mark>permutation invariance</mark> of the color channels, but keeps the model spatially invariant.<br>2: Nevertheless, <mark>permutation invariance</mark> is a property in strong contrast to convolutional neural networks, which incorporate the inductive bias that features should arise from spatially proximate elements.<br>",
    "Arabic": "ثبات التقليب",
    "Chinese": "置换不变性",
    "French": "invariance par permutation",
    "Japanese": "置換不変性",
    "Russian": "инвариантность перестановок"
  },
  {
    "English": "permutation matrix",
    "context": "1: = 1 ) ⇔ ( y ( m ) t = i ) . Finally, let R = {r (1) , . . . , r (T ) } be the collection of <mark>permutation matrices</mark> corresponding to T = {τ (1) , . . .<br>2: where Q(R|x, z ′ , y) is an approximate variational posterior. We now relax the restriction that P (R|x, z ′ ) places non-zero mass only on <mark>permutation matrices</mark> and use the following definition of P θ (R|x, z ′ ): \n<br>",
    "Arabic": "مصفوفة التبديل",
    "Chinese": "置换矩阵",
    "French": "matrice de permutation",
    "Japanese": "置換行列",
    "Russian": "матрица перестановок"
  },
  {
    "English": "permutation test",
    "context": "1: 1, where the program's used library function and the phrase \"U shape\" within in the human language description are actually the same concept. We verify this further by testing for the difference between these two correlations using a non-parametric <mark>permutation test</mark>.<br>2: A <mark>permutation test</mark> to compare ROC curves of individual classifiers shows that improved performance becomes sig-nificant after 50 bags are added to elephant and tiger datasets, and after 200 bags are added to the fox dataset.<br>",
    "Arabic": "اختبار التبديل",
    "Chinese": "置换检验",
    "French": "test de permutation",
    "Japanese": "置換検定",
    "Russian": "тест перестановки"
  },
  {
    "English": "permutohedral lattice",
    "context": "1: We gain an additional 5% by training a logistic regression classifier on the responses of the boosted classifier. For efficient high-dimensional filtering, we use a publicly available implementation of the <mark>permutohedral lattice</mark> [1]. We found a downsampling rate of one standard deviation to work best for all our experiments.<br>2: Following [1], BCL uses a <mark>permutohedral lattice</mark> instead of a standard Euclidean grid for efficiency purposes. The size of lattice simplices or space between the grid points is controlled by scaling the lattice features ΛL, where Λ is a diagonal d l × d l scaling matrix. Convolve.<br>",
    "Arabic": "شبكة متعددة الأوجه",
    "Chinese": "排列六面体格点",
    "French": "treillis permutoédral",
    "Japanese": "ペルムトヒーダル格子",
    "Russian": "пермутоэдрическая решетка"
  },
  {
    "English": "perplexity",
    "context": "1: Quantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics , including BLUE ( Papineni et al. , 2002 ; Lin and Och , 2004 ) , <mark>Perplexity</mark> , Relation Generation ( Wiseman et al. , 2017 ) , ROUGE ( Lin , 2004 ) , Word Mover 's Distance ( WMD ) , and Translation Edit Rate ( TER ) ( Snover et<br>2: TER ) ( Snover et al. , 2006 ) . We report the mean and standard deviation of three independent models. Darker colors indicate better scores. Input PT BLEU ↑ <mark>Perplexity</mark> ↓ RG ↑ ROUGE-1 ↑ ROUGE-2 ↑ ROUGE-L ↑ ROUGE-L SUM ↑ WMD ↓ TER ↓ Kantharaj et al. ( \n two L2 statements.<br>",
    "Arabic": "حَيْرة",
    "Chinese": "困惑度",
    "French": "perplexité",
    "Japanese": "複雑さ",
    "Russian": "затруднительность"
  },
  {
    "English": "perplexity score",
    "context": "1: The best settings of κ and τ 0 are consistent across the two corpora. For mini-batch sizes from 256 to 16384 there is little difference in <mark>perplexity scores</mark>. Several trends emerge from these results.<br>",
    "Arabic": "درجة الحيرة",
    "Chinese": "困惑度分数",
    "French": "score de perplexité",
    "Japanese": "パープレキシティースコア (Perplexity score)",
    "Russian": "оценка сложности"
  },
  {
    "English": "perspective projection",
    "context": "1: Perspective projection requires that every 3D point projects to an image along a straight line. When the scene is composed of refractive or mirror-like objects, this linear projection model is not valid anymore. Here we extend this model by studying indirect projections of 3D points.<br>2: (4) and (5). For example, consider the case of a planar perspective view Î . The <mark>perspective projection</mark> equations may be written: \n Ù ¥ , where ¥ is the ¿ ¢ projection matrix for that view, and Ù ×Ù ×Ú × Ì .<br>",
    "Arabic": "التقاط المنظور",
    "Chinese": "透视投影",
    "French": "projection en perspective",
    "Japanese": "遠近法射影",
    "Russian": "перспективная проекция"
  },
  {
    "English": "perspective projection matrix",
    "context": "1: Here we derive the transformation which is applied to rays to map them from camera space to NDC space. The standard 3D <mark>perspective projection matrix</mark> for homogeneous coordinates is: \n<br>",
    "Arabic": "مصفوفة إسقاط المنظور",
    "Chinese": "透视投影矩阵",
    "French": "matrice de projection en perspective",
    "Japanese": "遠近投影行列",
    "Russian": "перспективная проекционная матрица"
  },
  {
    "English": "perturbation",
    "context": "1: We are currently studying the effects of <mark>perturbation</mark> of the border initialization on the final predictions. We also hope to address the question of the sensitivity of results to errors in the border initialization.<br>2: Using this attack, even if we consider the attack successful only when an example is classified incorrectly 10 times out of 10, we achieve 100% targeted attack success rate and reduce the accuracy of the classifier from 32.8% to 0.0% with a maximum ∞ <mark>perturbation</mark> of = 0.031.<br>",
    "Arabic": "اضطراب",
    "Chinese": "扰动",
    "French": "perturbation",
    "Japanese": "摂動",
    "Russian": "возмущение"
  },
  {
    "English": "perturbation analysis",
    "context": "1: This <mark>perturbation analysis</mark> shows that model judgments are mostly robust to syntactic variations in the prefix content, with a smooth relationship between degrees of syntactic variation and model performance.<br>2: The simplest approach [10] is to cluster the image pixels using the k-means algorithm, and use <mark>perturbation analysis</mark> to bound the error of this algorithm as a function of the connectivity within and between clusters.<br>",
    "Arabic": "تحليل الاضطراب",
    "Chinese": "扰动分析",
    "French": "analyse des perturbations",
    "Japanese": "摂動解析",
    "Russian": "анализ возмущений"
  },
  {
    "English": "perturbation variance",
    "context": "1: In our experiments, the total inner problem length was T = 100, and we used truncated unrolls of length K = 10. For ES and PES, we used <mark>perturbation variance</mark> σ 2 = 1, and 100 particles (50 antithetic pairs).<br>",
    "Arabic": "تباين الاضطراب",
    "Chinese": "扰动方差",
    "French": "variance de perturbation",
    "Japanese": "摂動分散",
    "Russian": "вариация возмущения"
  },
  {
    "English": "phase retrieval",
    "context": "1: The goal of <mark>phase retrieval</mark> (PR) is to recover the underlying image from only the amplitude, or intensity of the output of a complex linear system.<br>2: We also show that our well-designed approach leads to better results than state-of-the-art techniques on compressed sensing MRI and <mark>phase retrieval</mark>.<br>",
    "Arabic": "استرجاع الطور",
    "Chinese": "相位恢复",
    "French": "restitution de phase",
    "Japanese": "位相復元",
    "Russian": "восстановление фазы"
  },
  {
    "English": "phoneme",
    "context": "1: This task consists of presenting a series of equal-length tuples (A, B, X) to the model, where A and B differ by one <mark>phoneme</mark> (either a vowel or a consonant), as do B and X, but A and X are not minimal pairs.<br>2: In this section we quantify to what extent <mark>phoneme</mark> identity can be decoded from the input MFCC features as compared to the representations extracted from the COCO speech. As explained in Section 4.3, we use phonemic transcriptions aligned to the corresponding audio in order to segment the signal into chunks corresponding to individual <mark>phoneme</mark>s.<br>",
    "Arabic": "وحدة صوتية",
    "Chinese": "音位",
    "French": "phonème",
    "Japanese": "音素",
    "Russian": "фонема"
  },
  {
    "English": "phoneme segmentation",
    "context": "1: Much of this work concerns the discovery of word-like units, while our analyses focus on learning at the phoneme level (see section 4.3). A symbolic Bayesian framework for joint unsupervised <mark>phoneme segmentation</mark> and clustering is proposed by Lee and Glass (2012) and extended by Lee et al. (2015).<br>2: Even doing studies of an individual phenomenon requires identifying a phonological phenomenon, extracting and labeling a corpus and conducting a study of the model's learning behavior. A diverse and comprehensive benchmark dataset for studying phonological learning (beyond <mark>phoneme segmentation</mark> and categorization) would be an exciting goal for future work.<br>",
    "Arabic": "تجزئة الفونيم",
    "Chinese": "音素分割",
    "French": "segmentation des phonèmes",
    "Japanese": "音素セグメンテーション",
    "Russian": "фонемная сегментация"
  },
  {
    "English": "photoconsistency",
    "context": "1: The data term in this paper is a standard <mark>photoconsistency</mark> term of the form \n E photo (D) = x N i=1 f I π i (x, D(x)) − I 0 (x), V i x (2) \n<br>2: Given the modes of the <mark>photoconsistency</mark> distribution at each pixel, the optimization of ( 14) becomes a labelling problem. Each pixel is associated with an integer label l(x, y), which indicates which mode of the distribution will be used to colour that pixel, with a corresponding <mark>photoconsistency</mark> cost which is precomputed.<br>",
    "Arabic": "تماسك الصورة",
    "Chinese": "光照一致性",
    "French": "photoconsistance",
    "Japanese": "写真整合性 (shashin seigousei)",
    "Russian": "фотоконсистенция"
  },
  {
    "English": "photometric consistency",
    "context": "1: Apart from the basic self-supervision signal based on <mark>photometric consistency</mark> L P C (Equation 1), we add two extra self-supervision signals of semantic consistency L SC and data-augmentation consistency L DA to the framework. In addition to the aforementioned loss, some common regularization terms suggested by (Mahjourian, Wicke, and Angelova 2018;Khot et al.<br>2: 2020;Gu et al. 2019;Xu and Tao 2020) separate the single MVS pipeline into multiple stages, achieving impressive performances. Unsupervised MVS: Under the assumption of <mark>photometric consistency</mark> (Godard, Mac Aodha, and Brostow 2017), unsupervised learning has been developed in multi-view systems. (Khot et al.<br>",
    "Arabic": "اتساق القياس الضوئي",
    "Chinese": "光度一致性",
    "French": "cohérence photométrique",
    "Japanese": "光度一貫性 (koudo ikanssei)",
    "Russian": "фотометрическая согласованность"
  },
  {
    "English": "photometric error",
    "context": "1: This could be done by using an RGB-D dataset, but might also be achieved with intensity information only in an self-supervised manner, based on <mark>photometric error</mark> as loss.<br>2: In both the methods, a rendering loss function L (<mark>photometric error</mark>) is computed for pixels u ∈ U . To update the geometry parameters j, the gradient ∂L ∂j is computed: \n<br>",
    "Arabic": "خطأ فوتومتري",
    "Chinese": "光度误差",
    "French": "erreur photométrique",
    "Japanese": "光度誤差",
    "Russian": "фотометрическая ошибка"
  },
  {
    "English": "photometric loss",
    "context": "1: This design is motivated by our observation that the <mark>photometric loss</mark> is not effective in fixing large motion errors early on in the training process, but is effective in refining the motion. The coefficient λ reg for smoothness regularization is set to 20.<br>2: Color Jitter and Blur: Many transformations can attach color fluctuation to images, such as random color jitter, random blur, random noise. The color fluctuation makes the unsupervised loss in MVS unreliable, because the <mark>photometric loss</mark> requires the color constancy among views.<br>",
    "Arabic": "الخسارة الفوتومترية",
    "Chinese": "光度损失",
    "French": "perte photométrique",
    "Japanese": "光度損失",
    "Russian": "фотометрические потери"
  },
  {
    "English": "photometric stereo",
    "context": "1: It is also a major factor preventing broader use of structured-light techniques, which largely assume direct or low-frequency light transport (e.g., 3D laser scanning [3,4], active triangulation [5,6] and <mark>photometric stereo</mark> [7]).<br>2: Third, we can compute a separate depth and a separate normal for each surface point; this is unlike typical stereo or laser-scanning techniques (which compute a pointset that must be differentiated to get normals) or <mark>photometric stereo</mark> (which computes a normal map that must be integrated to obtain depth).<br>",
    "Arabic": "استيريو فوتومتري",
    "Chinese": "光度立体测量",
    "French": "stéréophotométrie",
    "Japanese": "光度立体法",
    "Russian": "фотометрическое стерео"
  },
  {
    "English": "phrase structure tree",
    "context": "1: RNNGs are higher on this scale because they explicitly build a <mark>phrase structure tree</mark> using a symbolic stack. We consider as well a degraded version, RNNG −comp which lacks the composition mechanism shown in Figure 2. This degraded version replaces the stack with initial substrings of bracket expressions, following Choe and Charniak (2016); Vinyals et al.<br>2: An unaligned dependency evaluation is carried out: head-finding rules are used to convert a <mark>phrase structure tree</mark> into a dependency graph. Precision and recall are calculated over the dependencies The Sparseval results are shown in Table 1. For the purposes of comparison, the WSJ23 performance is displayed in the top row.<br>",
    "Arabic": "شجرة بنية العبارة",
    "Chinese": "短语结构树",
    "French": "arbre syntaxique",
    "Japanese": "句構造木",
    "Russian": "дерево структуры фраз"
  },
  {
    "English": "phrase table",
    "context": "1: Finally, we ran the decoder on the test set, pruning the <mark>phrase table</mark> with b = 100, pruning the chart with b = 100, β = 10 −5 , and limiting distortions to 4. These are the default settings, except for the <mark>phrase table</mark>'s b, which was raised from 20, and the distortion limit.<br>",
    "Arabic": "جدول العبارات",
    "Chinese": "短语表",
    "French": "table de phrases",
    "Japanese": "フレーズテーブル",
    "Russian": "таблица фраз"
  },
  {
    "English": "piecewise linear",
    "context": "1: This not only proves that the latter is optimizing a convex envelope, but also shows that our method naturally generalizes the work from <mark>piecewise linear</mark> to arbitrary piecewise convex energies. Fig. 3a and Fig. 3b illustrate the difference of σ * * and ρ * * on the example of a nonconvex stereo matching cost.<br>2: Example A.16 (Piecewise linear). As an example, <mark>piecewise linear</mark>, or piecewise-polynomial dynamics statisfy the conditions of the above proposition.<br>",
    "Arabic": "خطي متقطع",
    "Chinese": "分段线性",
    "French": "linéaire par morceaux",
    "Japanese": "区分線形",
    "Russian": "кусочно-линейный"
  },
  {
    "English": "piecewise planar",
    "context": "1: These approximations are equivalent to assuming very simple models of the world-for example, that it is <mark>piecewise planar</mark>-and thus introduce artifacts into the generated views. In contrast, image-based priors are easy to obtain from the world.<br>",
    "Arabic": "مجزأ مستوي",
    "Chinese": "分段平面",
    "French": "plan par morceaux",
    "Japanese": "部分的平面",
    "Russian": "кусочно-плоский"
  },
  {
    "English": "pipeline",
    "context": "1: Note that the efficacy of our <mark>pipeline</mark> for affinity optimization is influenced by the generalizability of the predictor, hence how to choose a desirable predictor is vital. As proof of concept, we currently apply the predictor in Shan et al. (2022) for its easy implementation and fast computation.<br>2: We discuss the system-level design for the autonomous driving algorithm framework. A planning-oriented <mark>pipeline</mark> is proposed toward the ultimate pursuit for planning, namely UniAD. We provide detailed analyses on the necessity of each module within perception and prediction.<br>",
    "Arabic": "النظام المتسلسل",
    "Chinese": "流程",
    "French": "pipeline",
    "Japanese": "パイプライン",
    "Russian": "конвейер"
  },
  {
    "English": "pixel",
    "context": "1: ϕ(I j ′ ,k ′ ) denotes the semantic label of the <mark>pixel</mark>/point and f j,k denotes the distance from the coordinate j, k to the closest point of the i-th class.<br>2: The abbreviation I π i (x, d) = I i (π i (x, d)) will be used to reduce clutter, and may be read as \"the color of the <mark>pixel</mark> corresponding to x in image i if the disparity at x is d\".<br>",
    "Arabic": "بكسل",
    "Chinese": "像素",
    "French": "pixels",
    "Japanese": "ピクセル",
    "Russian": "пиксель"
  },
  {
    "English": "pixel labeling",
    "context": "1: We observe a large variance, from 28 pixels to 1792 pixels. Cityscapes provides instance segmentation ground truth both in terms of a <mark>pixel labeling</mark> as well as in terms of polygons. In the former, each pixel can correspond to at most one instance, thus representing the visible portion of the object.<br>",
    "Arabic": "وسم البكسل",
    "Chinese": "像素标注",
    "French": "étiquetage des pixels",
    "Japanese": "ピクセルラベリング",
    "Russian": "разметка пикселей"
  },
  {
    "English": "pixel-level",
    "context": "1: We conduct <mark>pixel-level</mark> selfattention to model the long-term dependency required in some rapidly changing scenes, then perform scene-agent incorporation by attending each pixel of the scene to corresponding agents.<br>2: This makes it difficult to quantitatively evaluate the performance of algorithms that strive for <mark>pixel-level</mark> accuracy. Following Kohli et al. [9], we manually produced accurate segmentations and labelings for a set of images from the MSRC-21 dataset. Each image was fully annotated at the pixel level, with careful labeling around complex boundaries.<br>",
    "Arabic": "على مستوى البكسل",
    "Chinese": "像素级",
    "French": "niveau des pixels",
    "Japanese": "ピクセルレベル",
    "Russian": "пиксельный уровень"
  },
  {
    "English": "pixel-wise",
    "context": "1: Finally, we take a <mark>pixel-wise</mark> max over all the predictions, linearly normalize the result to [0,1], and apply edge NMS [13] to thin the edges. Visualizations. In Fig. 15 We see that the edges can align well with the human annotations.<br>",
    "Arabic": "من حيث البكسل",
    "Chinese": "像素级",
    "French": "par pixel",
    "Japanese": "ピクセル単位で",
    "Russian": "попиксельно"
  },
  {
    "English": "place recognition",
    "context": "1: • Area under the curve (AUC) is a single-value evaluation metric to evaluate the overall performance of <mark>place recognition</mark> methods, which takes values in [0, 1] with a greater value indicating a better performance, and a value 1 indicating the perfect performance.<br>2: By correctly identifying correspondences, estimating overlapped regions, and integrating geometric cues into <mark>place recognition</mark>, our approach can perform <mark>place recognition</mark> well in both cross-view (aerial-ground) and cross-modality (RGBD-RGB) scenarios. CONFIDENTIAL. Limited circulation. For review only.<br>",
    "Arabic": "تعرف المكان",
    "Chinese": "场景识别",
    "French": "reconnaissance de lieux",
    "Japanese": "場所認識",
    "Russian": "распознавание местоположения"
  },
  {
    "English": "placeholder",
    "context": "1: However, the generative inference is employed to alleviate the speed plunge in long prediction. We feed the decoder with the following vectors as Ltoken+Ly)×dmodel , (6) where X t token ∈ R Ltoken×dmodel is the start token, X t 0 ∈ R Ly×dmodel is a <mark>placeholder</mark> for the target sequence (set scalar as 0).<br>",
    "Arabic": "عنصر نائب",
    "Chinese": "占位符",
    "French": "marqueur de position",
    "Japanese": "プレースホルダー",
    "Russian": "заполнитель"
  },
  {
    "English": "planning",
    "context": "1: UniAD now is trained with all task losses including tracking, mapping, motion forecasting, occupancy prediction, and <mark>planning</mark> for 20 epochs (for various ablation studies in main paper, it's trained for 8 epochs for efficiency): \n L 2 = L track + L map + L motion + L occ + L plan . (14 \n ) \n<br>2: We consider SAS + <mark>planning</mark> (Bäckström and Nebel 1995) with operator costs, where a task is given as a tuple Π = V, O, s I , s , cost . Each V in the finite set of variables V has a finite domain dom(V ).<br>",
    "Arabic": "التخطيط",
    "Chinese": "规划",
    "French": "planification",
    "Japanese": "計画",
    "Russian": "планирование"
  },
  {
    "English": "planning problem",
    "context": "1: The execution of C fails if it reaches a pair (q, s) that was already visited. A generalized <mark>planning problem</mark> P = {P 1 , . . . , P T } is a set of multiple individual <mark>planning problem</mark>s that share fluents and actions. Each individual <mark>planning problem</mark> P t ∈ P is thus defined as P t = F , A , I t , G t , where only the initial state I t and goal condition G t differ from other <mark>planning problem</mark>s in P. An FSC C solves a generalized <mark>planning problem</mark> P if and only if it solves every problem P t ∈<br>2: Our feature space extends the one from  by adding the arithmetic comparison of numerical quantities. Definition 2. Let P = σ, A, s 0 , γ be a <mark>planning problem</mark>.<br>",
    "Arabic": "مشكلة التخطيط",
    "Chinese": "规划问题",
    "French": "problème de planification",
    "Japanese": "計画問題",
    "Russian": "задача планирования"
  },
  {
    "English": "planning task",
    "context": "1: An optimal cost partitioning is one that achieves the highest heuristic value for the given heuristics and state: Definition 2. Let Π = V, O, s I , s , cost be a <mark>planning task</mark> and let P n be the set of general cost partitionings for Π with n elements.<br>2: In planning, initially X equals the global transition system of the <mark>planning task</mark> (shown by Helmert et al., 2007). Merge steps do not change the represented global system, and shrink steps apply an abstraction to it.<br>",
    "Arabic": "مهمة التخطيط",
    "Chinese": "规划任务",
    "French": "tâche de planification",
    "Japanese": "計画問題",
    "Russian": "задача планирования"
  },
  {
    "English": "platt scaling",
    "context": "1: As it approaches the goal, our method always produces a higher confidence in the cor-  rect goal with lower variance. We tried argmax and <mark>Platt scaling</mark> [17] to perform multi-class prediction with MMED; argmax yielded higher P g * , in addition to making P g * noisier.<br>",
    "Arabic": "تحجيم بلات",
    "Chinese": "Platt缩放",
    "French": "mise à l'échelle de Platt",
    "Japanese": "プラットスケーリング",
    "Russian": "масштабирование Платта"
  },
  {
    "English": "plug-in estimator",
    "context": "1: Motivated by the principle of maximum likelihood, we show that a single, simple, <mark>plug-in estimator</mark>-profile maximum likelihood (PML) (Orlitsky et al., 2004b)-is competitive for estimating any symmetric property. Its sample complexity is at most quadratically worse than that of any estimator.<br>2: We show that a single, simple, <mark>plug-in estimator</mark>-profile maximum likelihood (PML)is sample competitive for all symmetric properties, and in particular is asymptotically sampleoptimal for all the above properties.<br>",
    "Arabic": "مقدر المكونات",
    "Chinese": "插值估计量",
    "French": "estimateur plug-in",
    "Japanese": "プラグイン推定器",
    "Russian": "вставной оценщик"
  },
  {
    "English": "pobj",
    "context": "1: If the entity's syntactic role is either nn or amod, we delete the entire entity and all of its descendants. If the the entity's role is <mark>pobj</mark>, the entity's parent preposition and all its descendants.<br>",
    "Arabic": "مفعول به للجار",
    "Chinese": "介词宾语",
    "French": "objet de préposition",
    "Japanese": "前置目的語",
    "Russian": "побж"
  },
  {
    "English": "point cloud",
    "context": "1: To address inherent limitations in 3D DMs, our model incorporates a joint-wise denoising mechanism that individually denoises various joints during estimation. Concretely, the proposed model first introduces a joint-wise condition generation module that samples features for each individual joints from both depth image and <mark>point cloud</mark>.<br>2: First, the sphere, represented with the SDF f (x) = x − 1, and second, the Stanford Bunny surface, representing a general curved surface and represented with an SDF learned with (Gropp et al., 2020) from <mark>point cloud</mark> data.<br>",
    "Arabic": "سحابة النقاط",
    "Chinese": "点云",
    "French": "nuage de points",
    "Japanese": "点群",
    "Russian": "облако точек"
  },
  {
    "English": "point correspondence",
    "context": "1: Beyond <mark>point correspondence</mark>, RePOSE [24] proposes a feature-metric correspondence network trained in a similar end-to-end fashion. The above methods are all coupled with surrogate regularization loss, otherwise convergence is not guaranteed due to the non-differentiable nature of deterministic pose.<br>",
    "Arabic": "مطابقة النقاط",
    "Chinese": "点对应关系",
    "French": "correspondance de points",
    "Japanese": "点対応",
    "Russian": "соответствие точек"
  },
  {
    "English": "point estimate",
    "context": "1: Let m d denote model m's <mark>point estimate</mark> of the topic proportions vector associated with document d (as described in Section 4.2). Further, let j m d s<br>",
    "Arabic": "تقدير نقطي",
    "Chinese": "点估计",
    "French": "estimation ponctuelle",
    "Japanese": "点推定",
    "Russian": "точечная оценка"
  },
  {
    "English": "point match",
    "context": "1: The additional constraint P i,3θ > 0 must be imposed such that the 3D point lies in front of the cameras. Homography fitting Given a set of <mark>point matches</mark> X = {(u i , u i )} N i=1 across two views, we wish to estimate the homography θ ∈ R 3×3 that aligns the points.<br>",
    "Arabic": "مطابقة النقاط",
    "Chinese": "点匹配",
    "French": "correspondance de points",
    "Japanese": "点の対応付け",
    "Russian": "точечное сопоставление"
  },
  {
    "English": "pointwise",
    "context": "1: Expressing preferences To express either <mark>pointwise</mark> or distributional preferences, Distributions support the constrain() method, which given a list of features ϕ i (x) and their corresponding momentsμ i , returns a representation of the target distribution that respects the constraints while deviating minimally from the original model.<br>",
    "Arabic": "نقطيّا",
    "Chinese": "逐点",
    "French": "ponctuellement",
    "Japanese": "点ごと",
    "Russian": "поэлементно"
  },
  {
    "English": "pointwise Mutual Information",
    "context": "1: In order to feed the LSK, lexical vectors correspond to a Word Space derived from a corpus of about 1.5 million tweets, downloaded during the experimental period and using the topic names from the trial material as query terms. Every word w in such corpus is represented as one co-occurrence vector as in ( Sahlgren , 2006 ) with the setting discussed in ( Croce and Previtali , 2010 ) : left and right co-occurrence scores are obtained in a window of size n = ±5 around each w. Vector components w f correspond to <mark>Pointwise Mutual Information</mark> values pmi ( w ,<br>",
    "Arabic": "معلومات متبادلة نقطية",
    "Chinese": "点互信息",
    "French": "Information mutuelle ponctuelle",
    "Japanese": "点ごとの相互情報量",
    "Russian": "точечная взаимная информация"
  },
  {
    "English": "pointwise multiplication",
    "context": "1: Under the assumptions of Theorem 6.1 and if ρ(F ℓ ) = ρ(alg), then \n • f : G s → R ℓ : (G, v) → s(G, v) ⊙ f (G, v), with ⊙ being <mark>pointwise multiplication</mark>, is also in F ℓ .<br>2: The explicit representation allows us to glimpse at the way different aspects are represented. To do so, we choose a representative pair of words that share an aspect, intersect their vectors, and inspect the highest scoring  The top features of each aspect, recovered by <mark>pointwise multiplication</mark> of words that share that aspect.<br>",
    "Arabic": "الضرب النقطي",
    "Chinese": "逐点相乘",
    "French": "multiplication ponctuelle",
    "Japanese": "要素積",
    "Russian": "поэлементное умножение"
  },
  {
    "English": "poisson distribution",
    "context": "1: The first term models the number of features detected using a <mark>Poisson distribution</mark>, which has a mean M . The second is a book-keeping term for the hypothesis variable and the last is a probability table (of size 2 P ) for all possible occlusion patterns and is a parameter of the model. The model of Weber et al.<br>2: This agrees with the theoretical analysis we just presented above that shows that the <mark>Poisson distribution</mark> performs well in the intermediate range of utility, as this is a simple hyperparameter search.<br>",
    "Arabic": "توزيع بواسون",
    "Chinese": "泊松分布",
    "French": "distribution de Poisson",
    "Japanese": "ポアソン分布",
    "Russian": "распределение Пуассона"
  },
  {
    "English": "poisson point process",
    "context": "1: The Poisson point measure assumption implies that edges are activated independently of one another and from the past: the activation times of edge {v, w} form a <mark>Poisson point process</mark> of intensity P {v,w} . To solve the gossip problem, Boyd et al.<br>2: In the theorem below, E denotes the expectation with respect to the <mark>Poisson point process</mark> dN (t), the only source of randomness. Theorem 2 (Convergence of continuized Nesterov acceleration). The continuized Nesterov acceleration satisfies the following two points.<br>",
    "Arabic": "عملية نقطية بواسون",
    "Chinese": "泊松点过程",
    "French": "processus de points de Poisson",
    "Japanese": "ポアソン点過程",
    "Russian": "Пуассоновский точечный процесс"
  },
  {
    "English": "poisson random variable",
    "context": "1: For simplicity we prove the result when the number of samples is n ∼ poi(n), a <mark>Poisson random variable</mark> with mean n. Let r Pσ poi(n) (q , ∆ k ) and r nat poi(n) (q , ∆ k ) be the regrets in this sampling process.<br>",
    "Arabic": "متغير عشوائي بواسون",
    "Chinese": "泊松随机变量",
    "French": "variable aléatoire de Poisson",
    "Japanese": "ポアソン確率変数",
    "Russian": "случайная величина Пуассона"
  },
  {
    "English": "poisson rate",
    "context": "1: In this approach, electrical patterns of activity corresponding to candidate potentiating and depressing plasticity events occur randomly and independently at all synapses at a <mark>Poisson rate</mark> r. These events reflect possible synaptic changes due to either spontaneous network activity, or the storage of new memories.<br>",
    "Arabic": "معدل بواسون",
    "Chinese": "泊松速率",
    "French": "taux de Poisson",
    "Japanese": "ポアソン率",
    "Russian": "Пуассоновская скорость"
  },
  {
    "English": "policy",
    "context": "1: Experiments show that using our approach to learn <mark>policies</mark> guided by sketches gives better performance than existing techniques for learning task-specific or shared <mark>policies</mark>, while naturally inducing a library of interpretable primitive behaviors that can be recombined to rapidly adapt to new tasks.<br>2: The spread approximates average edit distance among <mark>policies</mark> in Π G determined by randomly permuting actions of a reference policy by a coin weighted according to the value on the x-axis. We use the same set of CMPs for each environment up to any deviations explicitly made by the varied parameter (such as γ or entropy).<br>",
    "Arabic": "سياسات",
    "Chinese": "策略",
    "French": "politique",
    "Japanese": "方針",
    "Russian": "политика"
  },
  {
    "English": "policy class",
    "context": "1: The problem of delusion can be given a precise statement (which is articulated mathematically in Section 4): delusional bias occurs whenever a backed-up value estimate is derived from action choices that are not realizable in the underlying <mark>policy class</mark>.<br>2: We develop policy consistent backups in the tabular case while allowing for an arbitrary <mark>policy class</mark> (or arbitrary policy constraints)-the case of greedy policies with respect to some approximation architecture f θ is simply a special case. This allows the method to focus on delusion, without making any assumptions about the specific value approximation.<br>",
    "Arabic": "فئة السياسات",
    "Chinese": "策略类",
    "French": "classe de politiques",
    "Japanese": "ポリシークラス",
    "Russian": "класс политик"
  },
  {
    "English": "policy distribution",
    "context": "1: For the puzzle game domain, we replicated the game with an implementation that facilitates automatic play. As is commonly done in reinforcement learning, we use a softmax temperature parameter to smooth the <mark>policy distribution</mark> (Sutton and Barto, 1998), set to 0.1 in our experiments.<br>2: p ( a|s ; θ ) over this action space in a log-linear fashion ( Della Pietra et al. , 1997 ; Lafferty et al. , 2001 ) , giving us the flexibility to incorporate a diverse range of features . Under this representation, the <mark>policy distribution</mark> is: \n<br>",
    "Arabic": "توزيع السياسة",
    "Chinese": "策略分布",
    "French": "distribution de politique",
    "Japanese": "方策分布",
    "Russian": "распределение стратегии"
  },
  {
    "English": "policy entropy",
    "context": "1: The two-colors domain is designed such that the central component determining how well a memory-less agent adapts is its exploration. Our agents can only regulate exploration through <mark>policy entropy</mark>. Thus, to converge on optimal task behaviour, the agent must reduce <mark>policy entropy</mark>.<br>2: Result aggregated over 50 seeds. reports cumulative reward curves for our main experiment in Section 5.1. We note that MG tends to collapse for any K unless the meta-objective is explicitly regularized via meta . To characterise why MG fail for meta = 0, Figure 9 portrays the <mark>policy entropy</mark> range under either MG or BMG.<br>",
    "Arabic": "انتروبيا السياسة",
    "Chinese": "策略熵",
    "French": "entropie de la politique",
    "Japanese": "方策エントロピー",
    "Russian": "энтропия политики"
  },
  {
    "English": "policy evaluation",
    "context": "1: Our results show that this architecture leads to better <mark>policy evaluation</mark> in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.<br>2: We now show the practical performance of the dueling network. We start with a simple <mark>policy evaluation</mark> task and then show larger scale results for learning policies for general Atari game-playing.<br>",
    "Arabic": "تقييم السياسة",
    "Chinese": "策略评估",
    "French": "évaluation de la politique",
    "Japanese": "方策評価",
    "Russian": "оценка политики"
  },
  {
    "English": "policy gradient",
    "context": "1: In our case, the policy β first outputs a vectorê which gets mapped byφ * to an action. The observed transition is then used to compute the <mark>policy gradient</mark> (Sutton et al. 2000) for updating the parameters of β towards β * .<br>2: Policy gradient performs stochastic gradient ascent on the objective from equation 2, performing one update per document. For document d, this objective becomes: \n E p(h|θ) [r(h)] = h r(h)p(h|θ) = p(h d |θ), \n<br>",
    "Arabic": "تدرج السياسة",
    "Chinese": "策略梯度",
    "French": "gradient de politique",
    "Japanese": "方策勾配 (policy gradient)",
    "Russian": "градиент политики"
  },
  {
    "English": "policy gradient algorithm",
    "context": "1: We use a <mark>policy gradient algorithm</mark> to estimate the parameters of a log-linear model for action selection. We apply our method to interpret instructions in two domains -Windows troubleshooting guides and game tutorials. Our results demonstrate that this method can rival supervised learning techniques while requiring few or no annotated training examples. 1<br>2: However, while the state space can be designed to be relatively small in the dialogue management task, our state space is determined by the underlying environment and is typically quite large. We address this complexity by developing a <mark>policy gradient algorithm</mark> that learns efficiently while exploring a small subset of the states.<br>",
    "Arabic": "خوارزمية تدرج السياسة",
    "Chinese": "策略梯度算法",
    "French": "algorithme de gradient de politique",
    "Japanese": "方策勾配アルゴリズム",
    "Russian": "алгоритм градиента политики"
  },
  {
    "English": "policy gradient estimator",
    "context": "1: R ( ) ≈ 1 ∑︁ =1 ∑︁ =1 log( ( ( ) | ( ) 1: −1 )) ∑︁ = ( ) . (14 \n ) \n We will call this estimator the placement <mark>policy gradient estimator</mark>, in contrast with the basic <mark>policy gradient estimator</mark> (Eq.<br>",
    "Arabic": "مقدر تدرج السياسة",
    "Chinese": "策略梯度估计器",
    "French": "estimateur de gradient de politique",
    "Japanese": "方策勾配推定子",
    "Russian": "оценщик градиента политики"
  },
  {
    "English": "policy gradient method",
    "context": "1: The option-critic architecture does not prescribe how to obtain π Ω since a variety of existing approaches would apply: using <mark>policy gradient methods</mark> at the SMDP level, with a planner over the options models, or using temporal difference updates.<br>2: Section 2 and Section 3 are devoted to describing graphical models and reinforcement learning respectively, with particular emphasis on CRFs and policygradient methods for RL. We then elaborate on the combination of CRF and RL in Section 4. Section 5 describes our experiments before concluding.<br>",
    "Arabic": "طريقة التدرج في السياسة",
    "Chinese": "政策梯度法",
    "French": "méthode de gradient de politique",
    "Japanese": "方策勾配法",
    "Russian": "метод градиента политики"
  },
  {
    "English": "policy gradient theorem",
    "context": "1: The gradient of θ 1 is estimated by a likelihood estimator in a model-free manner, while the gradient of θ 2 is estimated relying on backpropagation via environment dynamics in a model-based manner. Specifically , for discrete terminal time decision π 1 , we apply the <mark>policy gradient theorem</mark> ( Sutton et al. , 2000 ) to obtain unbiased Monte Carlo estimate of θ1 J ( π θ ) using advantage function A π ( s , a ) = Q π ( s , a ) − V π ( s ) as target , i.e.<br>2: s 0 : ρ(θ, s 0 ) = E π θ [ ∞ t=0 γ t r t+1 | s 0 ]. The <mark>policy gradient theorem</mark> shows that : ∂ρ ( θ , s0 ) ∂θ = s µ π θ ( s | s 0 ) a ∂π θ ( a|s ) ∂θ Q π θ ( s , a ) , where µ π θ ( s | s 0 ) = ∞ t=0 γ t P ( s t = s | s<br>",
    "Arabic": "نظرية تدرج السياسات",
    "Chinese": "策略梯度定理",
    "French": "théorème du gradient de politique",
    "Japanese": "方策勾配定理",
    "Russian": "теорема градиента политики"
  },
  {
    "English": "policy improvement",
    "context": "1: In this work, we show that even when partial (inexact) policy evaluation is performed and noise is added to it, along with a noisy <mark>policy improvement</mark> stage, the above PI scheme converges with a γ h contraction coefficient.<br>2: In particular, it is desirable that the algorithm can maintain safe <mark>policy improvement</mark> across large and anchored hyperparameter choices, a property we call robust <mark>policy improvement</mark>.<br>",
    "Arabic": "تحسين السياسة",
    "Chinese": "策略改进",
    "French": "amélioration des politiques",
    "Japanese": "方策改善",
    "Russian": "улучшение политики"
  },
  {
    "English": "policy iteration",
    "context": "1: Many of these works consider <mark>policy iteration</mark>-style approaches where the policy class is implicitly defined in terms of a critic (e.g. through a softmax), whereas we allow explicit specification of both actor and critic classes.<br>2: ATAC updates policies by a no regret routine, where each policy is slow updated and determined by all the critics generated in the past iterations, whereas CQL is more akin to a <mark>policy iteration</mark> algorithm, where each policy is derived by a single critic.<br>",
    "Arabic": "تكرار السياسة",
    "Chinese": "策略迭代",
    "French": "itération de politique",
    "Japanese": "ポリシーイテレーション",
    "Russian": "итерация политики"
  },
  {
    "English": "policy learning",
    "context": "1: The pretrained GloVe word embeddings [35] are used for initialization and then fine-tuned during training. We train the matching critic with human demonstrations and then fix it during <mark>policy learning</mark>. Then 2 PL: the total length of the executed path. NE: the shortest-path distance between the agent's final position and the target.<br>2: We expect that future work might combine the two lines of research, bootstrapping <mark>policy learning</mark> directly from natural language hints rather than the semi-structured sketches used here.<br>",
    "Arabic": "تعلم السياسات",
    "Chinese": "策略学习",
    "French": "apprentissage de politique",
    "Japanese": "\"方策学習\"",
    "Russian": "обучение политике"
  },
  {
    "English": "policy network",
    "context": "1: In this work, we introduce RL into the PnP framework, yielding a novel tuning-free PnP proximal algorithm for a wide range of inverse imaging problems. We underline the main message of our approach the main strength of our proposed method is the <mark>policy network</mark>, which can customize well-suited parameters for different images.<br>2: Beginning with a policy and value network randomly initialized from scratch, a large number of self-play games are played and the resulting equilibrium policies and the improved 1-step value estimates computed on every turn from equilibrium-finding are added to a replay buffer used for subsequently improving the policy and value.<br>",
    "Arabic": "شبكة السياسات",
    "Chinese": "策略网络",
    "French": "réseau de politique",
    "Japanese": "ポリシーネットワーク",
    "Russian": "сеть политик"
  },
  {
    "English": "policy optimization",
    "context": "1: Indeed, this eligibility traces representation is particularly convenient for <mark>policy optimization</mark> (Deisenroth et al., 2013), in which we could optimize in turn a parametric policy over actions π θ (•|z, λ) and a parametric policy over the discount π ν (λ).<br>2: Policy optimization with a no-regret oracle In Algorithm 1, the <mark>policy optimization</mark> step (Line 4) is conducted by calling a no-regret <mark>policy optimization</mark> oracle (PO). We now define the property we expect from this oracle. Definition 4 (No-regret <mark>policy optimization</mark> oracle).<br>",
    "Arabic": "تحسين السياسة",
    "Chinese": "策略优化",
    "French": "optimisation des politiques",
    "Japanese": "方策最適化",
    "Russian": "оптимизация политики"
  },
  {
    "English": "policy parameter",
    "context": "1: Advising Policy Inputs & Outputs LeCTR learns student policies π i S , π j S and teacher policies π i T , π j T for agents i and j, constituting a jointly-initiated advising approach that learns when to request advice and when/what to advise. It is often infeasible to learn high-level policies that directly map task-level <mark>policy parameters</mark> θ i , θ j ( i.e. , local knowledge ) to advising decisions : the agents may be independent/decentralized learners and the cost of communicating task-level <mark>policy parameters</mark> may be high ; sharing <mark>policy parameters</mark> may be undesirable due to privacy concerns ; and learning advising policies over the<br>2: ) . Trajectory optimization. Our parametrization also includes open-loop trajectory optimization. Letting the <mark>policy parameters</mark> be an open-loop sequence of inputs θ = {θ h } H h=1 and having no feedback π(x h , θ) = θ h , we optimize over sequence of inputs to be applied to the system. One-step optimization.<br>",
    "Arabic": "معامل السياسة",
    "Chinese": "策略参数",
    "French": "paramètre de politique",
    "Japanese": "ポリシーパラメータ",
    "Russian": "параметр политики"
  },
  {
    "English": "policy representation",
    "context": "1: Because these approaches attempt to learn a single representation of the Q function for all subtasks and contexts, they require extremely strong formal assumptions about the form of the reward function and state representation (Andre & Russell, 2002) that the present work avoids by decoupling the <mark>policy representation</mark> from the value function.<br>",
    "Arabic": "تمثيل السياسة",
    "Chinese": "策略表示",
    "French": "représentation de la politique",
    "Japanese": "方策表現",
    "Russian": "представление политики"
  },
  {
    "English": "policy sketch",
    "context": "1: We will see that the minimal supervision provided by <mark>policy sketches</mark> re-sults in (sometimes dramatic) improvements over fully unsupervised approaches, while being substantially less onerous for humans to provide compared to the grounded supervision (such as explicit subgoals or feature abstraction hierarchies) used in previous work.<br>2: Policy sketches are short, ungrounded, symbolic representations of a task that describe its component parts, as illustrated in Figure 1.<br>",
    "Arabic": "رسم السياسة",
    "Chinese": "策略草图",
    "French": "ébauche de politique",
    "Japanese": "方針スケッチ",
    "Russian": "эскиз политики"
  },
  {
    "English": "policy space",
    "context": "1: `` backtracking '' ) . If cell search maintains a restricted frontier, our cells may no longer cover all of <mark>policy space</mark> (i.e, Q is no longer a partition of Θ). This runs the risk that some future Q-updates may not be consistent with any cell.<br>2: So it is better than all policies not in Π G . Second, note that the set Π G is well connected in the following sense. Let a step in <mark>policy space</mark> from some reference policy π ref to be a move to any other deterministic policy that differs from π ref in exactly one state.<br>",
    "Arabic": "فضاء السياسة",
    "Chinese": "策略空间",
    "French": "espace des politiques",
    "Japanese": "ポリシースペース",
    "Russian": "пространство политик"
  },
  {
    "English": "polygon mesh",
    "context": "1: Inserting New Deformation Nodes into N warp : After performing a non-rigid TSDF fusion step, we extract the surface estimate in the canonical frame as the <mark>polygon mesh</mark>V c . Given the current set of nodes N warp , we compute the extent to which the current warp function covers the extracted geometry.<br>",
    "Arabic": "شبكة مضلعات",
    "Chinese": "多边形网格",
    "French": "maillage polygonal",
    "Japanese": "ポリゴンメッシュ",
    "Russian": "многоугольная сетка"
  },
  {
    "English": "polylog",
    "context": "1: σ p = 1 √ d<mark>polylog</mark>(k) and γ ≤ 1 k ) p∈[P ] | v j, , ξ p | ≤ O(σ p • s + γk √ d • P ) O(σ p • P ) \n<br>2: This is more prominent for support size and support coverage, which have optimal dependence of <mark>polylog</mark>( 1ε ), whereas (Valiant & Valiant, 2011a) gives a 1 ε 2 dependence.<br>",
    "Arabic": "متعدد اللوغاريتمات",
    "Chinese": "多对数函数",
    "French": "polylog",
    "Japanese": "ポリログ",
    "Russian": "полилог"
  },
  {
    "English": "polylogarithmic",
    "context": "1: Moreover, our running time is always within a <mark>polylogarithmic</mark> factor of the best algorithm, even in the case of models with specialized solvers such as tree sparsity. For the EMD and cluster models, our algorithm is significantly faster than prior work and improves the time complexity by a polynomial factor.<br>2: Typically, the dependence of the high probability bound on δ is <mark>polylogarithmic</mark> in 1 δ ; thus in the following we will avoid explicitly mentioning δ.<br>",
    "Arabic": "متعدد اللوغاريتمي",
    "Chinese": "多对数级的",
    "French": "polylogarithmique",
    "Japanese": "多重対数的",
    "Russian": "полилогарифмическим"
  },
  {
    "English": "polynomial",
    "context": "1: Taking into account the cognitive effort required by human users to answer queries, our model is distinguished by the close way in which it integrates learning and dominance testing, and the insistence on having convergence bounds that are <mark>polynomial</mark> in the minimal description size of the target concept, but only polylogarithmic in the total number of attributes.<br>2: Li [12] pioneered the usage of Matoušek's method to conduct robust triangulation. It is provable that the time complexity of Matoušek's procedure is a p-th order <mark>polynomial</mark> on the number of outliers. However, on typical-sized problems the search is painfully slow.<br>",
    "Arabic": "متعدد الحدود",
    "Chinese": "多项式",
    "French": "polynôme",
    "Japanese": "多項式",
    "Russian": "полиномиальный"
  },
  {
    "English": "polynomial delay",
    "context": "1: (1) A procedure which enumerates all nontrivial explanations of a query letter q from a Horn theory Σ with incremental <mark>polynomial delay</mark>. This is a positive result and trivially implies that all explanations can be found in polynomial total time.<br>2: The algorithm runs with <mark>polynomial delay</mark> and is useful for identifying admissible sets with certain special properties (e.g., low measurement cost, higher statistical precision). 3. (Statistical Procedure) We demonstrate a general statistical procedure based on inverse probability weighting (IPW) to estimate the adjustment formula from data.<br>",
    "Arabic": "تأخير متعدد الحدود",
    "Chinese": "多项式延迟",
    "French": "retard polynomial",
    "Japanese": "多項式遅延",
    "Russian": "полиномиальная задержка"
  },
  {
    "English": "polynomial kernel",
    "context": "1: For example, the parameters determining the specifics of the third layer of a deep belief network are not relevant if the network depth is set to one or two. Likewise, the parameters of a support vector machine's <mark>polynomial kernel</mark> are not relevant if we use a different kernel instead.<br>",
    "Arabic": "نواة متعددة الحدود",
    "Chinese": "多项式核",
    "French": "noyau polynomial",
    "Japanese": "多項式カーネル",
    "Russian": "полиномиальное ядро"
  },
  {
    "English": "polynomial time",
    "context": "1: (1) A procedure which enumerates all nontrivial explanations of a query letter q from a Horn theory Σ with incremental polynomial delay. This is a positive result and trivially implies that all explanations can be found in polynomial total time.<br>2: We are also given a learned function F : X → R that computes a prediction F (x) on each instance x. Throughout this paper we assume that the prediction F (x) can be computed in <mark>polynomial time</mark> in n. \n<br>",
    "Arabic": "زمن متعدد الحدود",
    "Chinese": "多项式时间",
    "French": "temps polynomial",
    "Japanese": "多項式時間",
    "Russian": "полиномиальное время"
  },
  {
    "English": "polynomial time algorithm",
    "context": "1: In particular, we propose (in Section 3.4) a <mark>polynomial time algorithm</mark> to solve CONSEN-SUS[ 1 /2].<br>2: Note that a PAC learning algorithm is not required to terminate if no fitting query exists. It would be desirable to even attain efficient PAC learning which additionally requires A to be a <mark>polynomial time algorithm</mark>.<br>",
    "Arabic": "خوارزمية زمنية متعددة الحدود",
    "Chinese": "多项式时间算法",
    "French": "algorithme en temps polynomial",
    "Japanese": "多項式時間アルゴリズム",
    "Russian": "алгоритм полиномиального времени"
  },
  {
    "English": "pool-base active learning",
    "context": "1: Active learning in NLP has been largely studied as a theoretical improvement over traditional ML for scarce data. In this work, we specifically investigate <mark>pool-based active learning</mark>, or picking out samples to annotate from a larger pool of unlabeled data, and particularly data for a rare-class problem where LMs are not well-understood yet. Acquisition strategies Sampling strategies for active learning can be broadly classified into three : uncertainty sampling ( Shannon , 1948 ; Wang and Shang , 2014 ; Netzer et al. , 2011 ) , representative ( or diversity ) sampling ( Citovsky et al. , 2021 ; Sener and Savarese , 2018 ; Gissin and Shalev-Shwartz , 2019 ) , and the combination of<br>2: The main goal of small-text is to offer state-ofthe-art active learning for text classification in a convenient and robust way for both researchers and practitioners. For this purpose, we implemented a modular <mark>pool-based active learning</mark> mechanism, illustrated in Figure 2, which exposes interfaces for classifiers, query strategies, and stopping criteria.<br>",
    "Arabic": "التعلم النشط القائم على التجمع",
    "Chinese": "基于池的主动学习",
    "French": "Apprentissage actif basé sur le pool",
    "Japanese": "プールベースのアクティブラーニング",
    "Russian": "пул-базовое активное обучение"
  },
  {
    "English": "pooling layer",
    "context": "1: The receptive field size of each of these convolutional layers is identical to the corresponding side-output layer; (b) we cut the last stage of VGGNet, including the 5th <mark>pooling layer</mark> and all the fully connected layers. The reason for \"trimming\" the VGGNet is two-fold.<br>2: We use categorical cross-entropy as a loss function with both methods, but also experiment with other loss functions when performing severity classification. First, the model takes one or more posts as input and processes each post with a convolutional network containing a convolutional layer and a <mark>pooling layer</mark>.<br>",
    "Arabic": "طبقة التجميع",
    "Chinese": "池化层",
    "French": "couche de pooling",
    "Japanese": "プーリング層",
    "Russian": "слой объединения"
  },
  {
    "English": "pooling operation",
    "context": "1: (b) The learned archi- tectures tend to use a pair of different activation functions, which is different from the combination of the Tanh-Tanh activation functions applied in the MHA exit . (c) Most exits do not select the cls_pool <mark>pooling operation</mark>, validating the necessity of our pooler search cell.<br>",
    "Arabic": "عملية التجميع",
    "Chinese": "池化操作",
    "French": "opération de poolage",
    "Japanese": "プーリング操作",
    "Russian": "операция пулинга"
  },
  {
    "English": "pose estimation",
    "context": "1: More recent work uses VAEs to learn pose priors [49], which can be used for generating pose samples, as prior in <mark>pose estimation</mark>, or 3d human reconstruction from images or sparse/occluded data. Some works [52,66,50] propose VAE-based human motion models.<br>2: The original FCOS3D is a one-stage detector that directly regresses the center offset, depth, and yaw orientation of multiple objects for 4DoF <mark>pose estimation</mark>. In our adaptation, the outputs of the multi-level FCOS head [41] are modified to generate object queries instead of directly predicting the pose.<br>",
    "Arabic": "تقدير الوضعية",
    "Chinese": "姿态估计",
    "French": "estimation de la pose",
    "Japanese": "姿勢推定",
    "Russian": "оценка позы"
  },
  {
    "English": "pose parameter",
    "context": "1: In total, we find the <mark>pose parameters</mark>θ t at frame t as: \n θ t = arg min θ λ v L v + λ θ L θ + λ t L t ,(7) \n where L v makes sure that the optimized pose is close to the observation and the temporal smoothness term L t enforces temporal consistency: \n<br>2: For both, the error converges around b=500 bits. Right: Hashing error relative to an exhaustive linear scan as a function of ǫ, which controls the search time required. For each, we constrain the distance of the 10 nearest exemplars (in terms of <mark>pose parameters</mark>) to be less than ℓ.<br>",
    "Arabic": "معلمات الوضع",
    "Chinese": "姿态参数",
    "French": "paramètres de pose",
    "Japanese": "姿勢パラメータ",
    "Russian": "параметры позы"
  },
  {
    "English": "pose prior",
    "context": "1: The prior for each part is defined by corresponding shape and <mark>pose priors</mark>, for which we use 0-mean standard normal priors for each parameter except for scaling factors, which are encouraged to be close to 1. Details and relative weights can be found in supplementary materials.<br>2: More recent work uses VAEs to learn <mark>pose priors</mark> [49], which can be used for generating pose samples, as prior in pose estimation, or 3d human reconstruction from images or sparse/occluded data. Some works [52,66,50] propose VAE-based human motion models.<br>",
    "Arabic": "الموقف الأولي",
    "Chinese": "姿态先验",
    "French": "a priori de pose",
    "Japanese": "姿勢事前分布",
    "Russian": "приоритет позы"
  },
  {
    "English": "pose space",
    "context": "1: Using gradient descent in <mark>pose space</mark> from an initial potentially non-plausible pose, we always find the closest point on the manifold of plausible poses. An overview of our method is given in Fig. 1. We formulate the problem of learning the pose manifold as a surface completion task in n-dimensional space.<br>2: To alleviate these issues, we present Pose-NDF, a human pose prior that models the full manifold of plausible poses in high-dimensional <mark>pose space</mark>. We represent the manifold as a surface, where plausible poses lie on the manifold, hence having a zero distance, and non-plausible poses lie outside of it, having a non-zero distance from the surface.<br>",
    "Arabic": "فضاء الوضعية",
    "Chinese": "姿态空间",
    "French": "espace de pose",
    "Japanese": "姿勢空間",
    "Russian": "пространство позы"
  },
  {
    "English": "position bias",
    "context": "1: The new objective corrects for the <mark>position bias</mark> using Inverse Propensity Score (IPS) weighting [28,29], where the <mark>position bias</mark> (p 1 , ..., p τ ) takes the role of the missingness model.<br>2: Recently, several follow-up works have realized this deficiency of gradient exploration in DBGD, and propose various types of solutions to improve its learning efficiency. One type of studies explore multiple random directions in each iteration of model update. Unbiased estimate of gradient is maintained in this type of revisions of DBGD, as the directions are still uniformly sampled. Model estimation variance is expected to be reduced by testing more exploratory directions ; but , in practice , as the users would only examine a finite number of documents under each query ( e.g. , due to <mark>position bias</mark> [ 9 ] ) , the sensitivity of interleaved test drops as a result of more exploratory rankers having to be tested at once<br>",
    "Arabic": "تحيز الموضع",
    "Chinese": "位置偏差",
    "French": "biais de position",
    "Japanese": "位置バイアス",
    "Russian": "смещение позиции"
  },
  {
    "English": "position embedding",
    "context": "1: (2014) propose the <mark>position embedding</mark> to consider the relative distance from each word to head and tail entities. The prediction network outputs the probability distribution of the relations between the entities.<br>2: A temporal fusion layer M (e.g., mean-pooling) is applied to aggregate the frame-level feature maps into a single clip-level feature map. We then add a row-wise and a column-wise <mark>position embedding</mark> to each feature vector based on their 2D position. These embeddings are the same trainable <mark>position embedding</mark>s as in BERT [11].<br>",
    "Arabic": "تضمين الموقع",
    "Chinese": "位置嵌入",
    "French": "position intégrée",
    "Japanese": "位置エンベッディング",
    "Russian": "позиционное вложение"
  },
  {
    "English": "positional bias",
    "context": "1: (2021), we apply a learnable <mark>positional bias</mark> and pass the result to a to a standard transformer encoder architecture with 10 layers, channel width 224, 8 dot-product-self-attention heads per layer, and GeLU activation. Finally we decode the policy via the same LSTM decoder head as Gray et al.<br>",
    "Arabic": "الانحياز الموضعي",
    "Chinese": "位置偏差",
    "French": "biais positionnel",
    "Japanese": "位置バイアス",
    "Russian": "позиционная предвзятость"
  },
  {
    "English": "positional embedding",
    "context": "1: Our work advances the understanding of <mark>positional embedding</mark>s adopted in almost all transformer models. In addition, our proposed new <mark>positional embedding</mark> significantly reduces energy consumption and training cost thanks to its length extrapolation property. Finally, our work lays the groundwork for developing future transformers that are greener and more cost-efficient enabled by improved length extrapolation.<br>2: We further benchmark SRN [56] which implements an implicit learned LSTM renderer. Importantly, following secs. 4.1 and 4.2, we evaluate the modifications SRN-γ, DVR-γ that endow SRN and DVR respectively with <mark>positional embedding</mark> γ. Furthermore, SRN-γ-WCE, SRN-WCE, NeRF-WCE [26] complement SRN and NeRF with the Warp-conditioned Embedding [26].<br>",
    "Arabic": "تضمين موضعي",
    "Chinese": "位置嵌入",
    "French": "encastrement positionnel",
    "Japanese": "位置埋め込み",
    "Russian": "позиционное встраивание"
  },
  {
    "English": "positional encoding",
    "context": "1: To capture the higher frequencies in the image we use a <mark>positional encoding</mark> with k = 8 for both methods. We used a batch size of 10k. We used learning rate of 1e-5 for Moser Flow and 1e-4 for FFJORD. We used λ − = 2. Learning was stopped after 5k seconds. Figure 7 presents the results.<br>2: and outputs the corresponding location in the target frame . Although we applied <mark>positional encoding</mark> with 8 frequencies to the input to enable better fitting, this ablation failed to capture the holistic motion of the video, instead only capturing simpler motion patterns for the rigid background.<br>",
    "Arabic": "ترميز موضعي",
    "Chinese": "位置编码",
    "French": "codage positionnel",
    "Japanese": "位置エンコーディング",
    "Russian": "позиционное кодирование"
  },
  {
    "English": "positive definite",
    "context": "1: A <mark>positive definite</mark> ( pd ) kernel , k is said to be characteristic to P if and only if γ k ( P , Q ) = 0 ⇔ P = Q , ∀ P , Q ∈ P. The following result provides a novel characterization for characteristic kernels , which shows that strictly pd kernels are characteristic to P. An advantage with<br>2: Definition 3 is stronger than the finite sum definition as [19,Theorem 4.62] shows a kernel that is strictly pd in the finite sum sense but not in the integral sense. Theorem 4 (Strictly pd kernels are characteristic). If k is strictly <mark>positive definite</mark> on M , then k is characteristic to P. \n<br>",
    "Arabic": "موجب محدد",
    "Chinese": "正定",
    "French": "définie positive",
    "Japanese": "正定値",
    "Russian": "положительно определенный"
  },
  {
    "English": "positive pair",
    "context": "1: While contrastive approaches of self-supervised learning ( SSL ) learn representations by minimizing the distance between two augmented views of the same data point ( <mark>positive pairs</mark> ) and maximizing views from different data points ( negative pairs ) , recent non-contrastive SSL ( e.g. , BYOL and SimSiam ) show remarkable performance without negative pairs , with an extra learnable predictor and a<br>2: While most self-supervised learning approaches use <mark>positive pairs</mark> (x i , x ′ i ) and negative pairs {∀j, j ̸ = i, (x i , x j )} {∀j, j ̸ = i, (x i , x ′ j ) \n<br>",
    "Arabic": "زوج إيجابي",
    "Chinese": "正向配对",
    "French": "paires positives",
    "Japanese": "正サンプル対",
    "Russian": "позитивная пара"
  },
  {
    "English": "positive semidefinite",
    "context": "1: The advantage of this reformulation is that the matrixθ 2 is guaranteed to be <mark>positive semidefinite</mark>, i.e.θ 2 0. Using the fact that for x a;i ∈ {−1, 1}, \n 1+xa;i 2 2 = 1+xa;i 2 \n<br>2: Here instead, our objective is to directly use these indefinite similarity measures for classification. Our work also closely follows recent results on kernel learning ( see [ 5 ] or [ 6 ] ) , where the kernel matrix is learned as a linear combination of given kernels , and the resulting kernel is explicitly constrained to be <mark>positive semidefinite</mark> ( the authors of [ 7 ] have adapted the SMO algorithm to solve the case where the<br>",
    "Arabic": "إيجابية شبه محددة",
    "Chinese": "正半定",
    "French": "positif semi-défini",
    "Japanese": "正の半定値",
    "Russian": "положительно полуопределенный"
  },
  {
    "English": "positive semidefinite kernel",
    "context": "1: This can be interpreted as a robust classification problem where the indefinite kernel matrix is treated as a noisy observation of the true <mark>positive semidefinite kernel</mark>. Our formulation keeps the problem convex and relatively large problems can be solved efficiently using the analytic center cutting plane method. We compare the performance of our technique with other methods on several data sets.<br>2: In that sense, indefinite similarity matrices are seen as noisy observations of an unknown <mark>positive semidefinite kernel</mark>. From a complexity standpoint, while the original SVM classification problem with indefinite kernel is nonconvex, the robustification we detail here is a convex problem, and hence can be solved efficiently with guaranteed complexity bounds. The paper is organized as follows.<br>",
    "Arabic": "نواة شبه محددة موجبة",
    "Chinese": "正半正定核",
    "French": "noyau semi-défini positif",
    "Japanese": "正の半定値カーネル",
    "Russian": "положительно полуопределенное ядро"
  },
  {
    "English": "positive semidefinite matrix",
    "context": "1: The SOCP-C relaxation (where C denotes cycles) defines second order cone (SOC) constraints using <mark>positive semidefinite matrices</mark> C such that the graph G (defined in section 5) form cycles. Let the variables corresponding to vertices of one such cycle G of length c be denoted as \n<br>",
    "Arabic": "مصفوفة شبه محددة موجبة",
    "Chinese": "半正定矩阵",
    "French": "matrice semi-définie positive",
    "Japanese": "正の半定値行列",
    "Russian": "положительно полуопределённая матрица"
  },
  {
    "English": "post-editing",
    "context": "1: We introduce translation error correction (TEC), the task of automatically correcting human-generated translations. Imperfections in machine translations (MT) have long motivated systems for improving translations posthoc with automatic <mark>post-editing</mark>.<br>2: Here, we also excluded comparisons between systems performed to specifically evaluate the impact of new datasets, pre-processing methods, and human intervention or feedback (e.g., <mark>post-editing</mark> and interactive MT). If we had any doubt whether a paper belongs or not to this category, we excluded it.<br>",
    "Arabic": "التحرير اللاحق",
    "Chinese": "人工后编辑",
    "French": "post-édition",
    "Japanese": "事後編集",
    "Russian": "постредактирование"
  },
  {
    "English": "post-hoc",
    "context": "1: In this case, the model is no longer symmetric, and we no longer require random initialization or <mark>post-hoc</mark> mapping of labels. Adding prototypes in this way gave an accuracy of 68.8% on all tokens, but only 47.7% on non-prototype occurrences, which is only a marginal improvement over BASE.<br>2: With this, we also look to understand: given this <mark>post-hoc</mark> possibility of manipulation, can we devise an algorithm that nonetheless ensures the algorithm's estimate is within of µ(h new )?<br>",
    "Arabic": "ما بعد المخصص",
    "Chinese": "后验",
    "French": "post-hoc",
    "Japanese": "事後的な",
    "Russian": "пост-хок"
  },
  {
    "English": "post-hoc analysis",
    "context": "1: An alternative family of approaches employs <mark>post-hoc analysis</mark> of demonstrations or pretrained policies to extract reusable sub-components (Stolle & Precup, 2002;Konidaris et al., 2011;Niekum et al., 2015). Techniques for learning options with less guidance than the present work include Bacon & Precup (2015) and Vezhnevets et al.<br>",
    "Arabic": "التحليل ما بعد التجربة",
    "Chinese": "事后分析",
    "French": "analyse a posteriori",
    "Japanese": "事後分析",
    "Russian": "пост-хок анализ"
  },
  {
    "English": "post-processing",
    "context": "1: In order to correct the unfairness of models, many methods have also been proposed, which can be classified into one of the following three categories: pre-processing biased datasets, in-processing models during training, and <mark>post-processing</mark> the outputs of models. In-processing is usually the most effective way to intervene an unfair model (Petersen et al.<br>2: Based on the SGAP paradigm, the evaluation engine in PaSca involves the expensive neighborhood expansion only once in the pre/<mark>post-processing</mark> stages, and thus ensuring the scalability upon the number of training workers.<br>",
    "Arabic": "معالجة لاحقة",
    "Chinese": "后处理",
    "French": "post-traitement",
    "Japanese": "後処理",
    "Russian": "постобработка"
  },
  {
    "English": "posterior",
    "context": "1: We consider the recovery of the images x i from their measurements (4) by maximizing their likelihood, specified by \n p(b i ) = p(b i , c i ) p(c i |b i )(7) \n We note that the <mark>posterior</mark> p(c i |b i ) is not tractable.<br>2: (2019)) even though those models explicitly ask for a factorised aggregate <mark>posterior</mark>. The particular baseline model we show here is β-VAE (Fig. 5D). We see that (1) our constraints lead to disentanglement (Fig.<br>",
    "Arabic": "لاحقة",
    "Chinese": "后验",
    "French": "postérieur",
    "Japanese": "事後分布",
    "Russian": "постериорный"
  },
  {
    "English": "posterior approximation",
    "context": "1: The sample log likelihood can be computed with the inside algorithm, while the KL term can be computed analytically when both prior p(z) and the <mark>posterior approximation</mark> q φ (z|σ) are Gaussian (Kingma and Welling, 2014).<br>",
    "Arabic": "تقريب اللاحق",
    "Chinese": "后验近似",
    "French": "approximation postérieure",
    "Japanese": "事後近似",
    "Russian": "постериорное приближение"
  },
  {
    "English": "posterior density",
    "context": "1: Specifically, regression trees are used, where each leaf stores an individual linear regressor that determines a local potential. Since any Gaussian CRF can be decomposed into factors relating no more than two pixels, our <mark>posterior density</mark> attains the following form: \n p ( x|y , K ) ∝ N ( y ; Kx , I/α ) • J+1 j=1 c∈C j φ j ( x ( c ) , y ) ( 8 ) φ j ( x ( c ) , y ) ∝ exp − 1 2 x T ( c ) Θ j c ( y ) x ( c ) +<br>2: In Sections 6 and 7 we show examples of misalignment between the Laplace marginal likelihood and generalization in large Bayesian neural networks. Information criteria. While the Laplace approximation provides a relatively cheap estimate of the marginal likelihood, it still requires estimating the Hessian of the <mark>posterior density</mark>, which may be computationally challenging.<br>",
    "Arabic": "كثافة لاحقة",
    "Chinese": "后验密度",
    "French": "densité a posteriori",
    "Japanese": "事後密度",
    "Russian": "апостериорная плотность"
  },
  {
    "English": "posterior distribution",
    "context": "1: Letp(Y 1 ≻ Y 2 |θ) denote the preference probability modelled by a neural evaluation metric with parameters θ. Given a training dataset D tr , Bayesian inference involves computing the <mark>posterior distribution</mark> p(θ|D tr ) and marginalization over the parameters θ: \n<br>2: PAC-Bayes generalizes this idea: we put a prior distribution p(w) on the possible parameter values, and we provide guarantees for the expected performance of a random sample w ∼ q from an arbitrary <mark>posterior distribution</mark> q (not necessarily the Bayesian posterior) over the parameters. Then , we can provide non-trivial generalization bounds even if the set of possible parameter values Ω is infinite , as long as the distribution q does not differ too much from the prior : if we come up with a distribution q which is similar to a fixed prior , and such that on average samples from this distribution perform well on the<br>",
    "Arabic": "التوزيع البعدي",
    "Chinese": "后验分布",
    "French": "distribution a posteriori",
    "Japanese": "事後分布",
    "Russian": "апостериорное распределение"
  },
  {
    "English": "posterior entropy",
    "context": "1: BALD (Bayesian Active Learning by Disagreement) builds upon Monte-Carlo Dropout by proposing a decision theoretic objective; it acquires examples that maximise the decrease in expected <mark>posterior entropy</mark> (Houlsby et al., 2011;Siddhant and Lipton, 2018) -capturing \"disagreement\" across different dropout masks.<br>2: The information gain I(α tot ) due to additional examples beyond α tot can be defined as the rate at which the <mark>posterior entropy</mark> is reduced: I(α tot ) = − d dαtot S(α tot ).<br>",
    "Arabic": "عدم التأكد اللاحق",
    "Chinese": "后验熵",
    "French": "entropie a posteriori",
    "Japanese": "事後エントロピー",
    "Russian": "постериорная энтропия"
  },
  {
    "English": "posterior estimation",
    "context": "1: This paper studies active <mark>posterior estimation</mark> in a Bayesian setting when the likelihood is expensive to evaluate. Existing techniques for <mark>posterior estimation</mark> are based on generating samples representative of the posterior. Such methods do not consider efficiency in terms of likelihood evaluations. In order to be query efficient we treat <mark>posterior estimation</mark> in an active regression framework.<br>2: as in scientific simulations, treating <mark>posterior estimation</mark> in an active regression framework enables us to be significantly query efficient.<br>",
    "Arabic": "التقدير البعدي",
    "Chinese": "后验估计",
    "French": "estimation a posteriori",
    "Japanese": "事後推定",
    "Russian": "апостериорная оценка"
  },
  {
    "English": "posterior inference",
    "context": "1: These models are a powerful method of dimensionality reduction for large collections of unstructured documents. Moreover, <mark>posterior inference</mark> at the document level is useful for information retrieval, classification, and topic-directed browsing. Treating words exchangeably is a simplification that it is consistent with the goal of identifying the semantic themes within each document.<br>2: These methods vary in (1) the choice of penalty or prior (and possibly other modeling assumptions); and (2) the algorithm used to fit the model (e.g., point estimation of b vs. approximate <mark>posterior inference</mark> of b via MCMC or variational inference).<br>",
    "Arabic": "الاستدلال اللاحق",
    "Chinese": "后验推断",
    "French": "inférence a posteriori",
    "Japanese": "事後推論",
    "Russian": "апостериорный вывод"
  },
  {
    "English": "posterior mean",
    "context": "1: By contrast, the variational approach seeks the <mark>posterior mean</mark>, not the posterior mode, and likewise the VEB shrinkage ( 32) is based on an averaging (mean) operation instead of the usual maximization (mode).<br>2: Further, by Proposition 1, this value is computed as the <mark>posterior mean</mark> under a simple NM model, which is given by the shrinkage operator S gσ,σ . In summary, for fixed g, σ 2 , Algorithm 2 can be reframed as a coordinate ascent algorithm for PLR, which is Algorithm 3.<br>",
    "Arabic": "المتوسط الخلفي",
    "Chinese": "后验均值",
    "French": "moyenne a posteriori",
    "Japanese": "事後平均",
    "Russian": "апостериорное среднее"
  },
  {
    "English": "posterior mean function",
    "context": "1: In the case of sparse GPs, f is typically represented in terms of canonical basis functions k(•, Z) such that (f | u)(•) denotes the <mark>posterior mean function</mark> given q(u). Consequently, f ⊥ denotes the process residuals (f \n<br>",
    "Arabic": "الدالة الوسطى اللاحقة",
    "Chinese": "后验均值函数",
    "French": "fonction moyenne a posteriori",
    "Japanese": "事後平均関数",
    "Russian": "функция апостериорного среднего"
  },
  {
    "English": "posterior probability",
    "context": "1: The discriminative methods give approximate <mark>posterior probabilities</mark> q(w j Tst j (I)) for the elementary components w j of W . For computational efficiency, these probabilities are based only on a small number of simple tests Tst j (I). We briefly overview and classify the discriminative methods used in our implementation.<br>2: This update can be thought of as an approximate M-step update for the mixture weights in which the <mark>posterior probabilities</mark> (the \"responsibilities\") are computed approximately using q.<br>",
    "Arabic": "احتمالية لاحقة",
    "Chinese": "后验概率",
    "French": "probabilité a posteriori",
    "Japanese": "事後確率",
    "Russian": "вероятность апостериори"
  },
  {
    "English": "posterior probability distribution",
    "context": "1: Given a prior probability distribution q = (q i ) n i=1 and a set of constraints C, the principle of minimum relative entropy chooses the <mark>posterior probability distribution</mark> p = (p i ) n i=1 that has the least relative entropy 2 with respect to q: \n<br>",
    "Arabic": "التوزيع الاحتمالي اللاحق",
    "Chinese": "后验概率分布",
    "French": "distribution de probabilité a posteriori",
    "Japanese": "事後確率分布",
    "Russian": "апостериорное распределение вероятностей"
  },
  {
    "English": "posterior sample",
    "context": "1: For HGEP, reconstruction was done by using the Bayes estimator approximated from 1000 <mark>posterior samples</mark> (one after each scan through all the time series). We repeated all experiments 5 times with different random seeds. We compared against the standard maximum likelihood rate matrix estimator learned by EM described in [23].<br>",
    "Arabic": "عينات ما بعدية",
    "Chinese": "后验样本",
    "French": "échantillon a posteriori",
    "Japanese": "事後サンプル",
    "Russian": "послевероятностная выборка"
  },
  {
    "English": "posterior variance",
    "context": "1: In many applications, pointwise estimates of the posterior mean and variance are of interest. It is therefore desirable that the approximate variational posterior gives similar estimates of these quantities as the true posterior. Huggins et al.<br>",
    "Arabic": "تباين لاحق",
    "Chinese": "后验方差",
    "French": "variance postérieure",
    "Japanese": "事後分散",
    "Russian": "постериорная дисперсия"
  },
  {
    "English": "potential function",
    "context": "1: where the objective function opt can be chosen arbitrarily. Then the function pot opt ( V, v ) = f (P V,v ) is the <mark>potential function</mark> optimized for opt and h pot opt is the potential heuristic optimized for opt. Proposition 2.<br>2: As models of interest are often specified in terms of <mark>potential function</mark>s, to be able to reuse existing MAP solvers in a black-box manner with the new tricks, we seek an equivalent formulation in terms of the <mark>potential function</mark>.<br>",
    "Arabic": "دالة إمكانية",
    "Chinese": "势函数",
    "French": "fonction de potentiel",
    "Japanese": "ポテンシャル関数",
    "Russian": "функция потенциала"
  },
  {
    "English": "potential heuristic",
    "context": "1: We implemented the state equation heuristic and the <mark>potential heuristic</mark> that optimizes the heuristic value of the initial state in the Fast Downward planning system (Helmert 2006). All our experiments were run on the set of tasks from optimal tracks of IPC 1998-2011, limiting runtime to 30 minutes and memory usage to 2 GB.<br>2: For any objective function opt, the heuristic h pot opt is admissible and consistent, and it maximizes opt among all admissible and consistent <mark>potential heuristic</mark>s. As an example, we consider the <mark>potential heuristic</mark> optimized for the heuristic value of the initial state: \n opt sI = V ∈V P V,sI[v] \n<br>",
    "Arabic": "ارشادي محتمل",
    "Chinese": "潜在启发式",
    "French": "heuristique potentielle",
    "Japanese": "潜在ヒューリスティック",
    "Russian": "потенциальная эвристика"
  },
  {
    "English": "power iteration method",
    "context": "1: 2), F and z are approximated through the <mark>power iteration method</mark> in [34]. Then, the procedure of candidate generation is the same as that in Algorithm 1. Next, it runs iterations for getting rewiring operations.<br>2: This is a classical Rayleigh quotient problem, whose solution is N 1/2 2 times the eigenvector X * associated with the largest eigenvalue (which we refer to as the main eigenvalue) of the matrix X [9], and can be computed efficiently by the <mark>power iteration method</mark> described in the next section.<br>",
    "Arabic": "طريقة تكرار الطاقة",
    "Chinese": "幂迭代法",
    "French": "méthode de l'itération de la puissance",
    "Japanese": "冪乗反復法",
    "Russian": "метод степенных итераций"
  },
  {
    "English": "power law distribution",
    "context": "1: 3 . The in-degree distribution in our data shows a striking fit with a Zipf (more so than the power law) distribution; Figure 8 shows the in-degrees of pages from the May 1999 crawl plotted against both ranks and magnitudes (corresponding to the Zipf and power law cases).<br>",
    "Arabic": "توزيع قانون القوى",
    "Chinese": "幂律分布",
    "French": "distribution en loi de puissance",
    "Japanese": "べき乗則分布",
    "Russian": "степенное распределение"
  },
  {
    "English": "power method",
    "context": "1: We note that the actual gaps could be larger, yet figuring them out exactly would have required many more iterations of the <mark>power method</mark>. The fact the two bounds are identical does not mean that the actual spectral gaps of the two Markov Chains are identical. In reality, one of them may be much higher than the other.<br>",
    "Arabic": "طريقة القوة",
    "Chinese": "幂法方法",
    "French": "méthode de la puissance itérative",
    "Japanese": "パワーメソッド (power method)",
    "Russian": "метод степенных итераций"
  },
  {
    "English": "pre-logit",
    "context": "1: In supervised pre-training, representation quality tends to increase monotonically with depth, such that the best representations lie at the penultimate layer (Zeiler & Fergus, 2014). Indeed, since a linear layer produces class logits from <mark>pre-logits</mark>, a good classifier necessarily achieves high accuracy on a linear probe of its <mark>pre-logits</mark>.<br>",
    "Arabic": "قبل-اللوجيت",
    "Chinese": "前逻辑值",
    "French": "pré-logits",
    "Japanese": "前段ロジット",
    "Russian": "доверительные значения"
  },
  {
    "English": "pre-processing",
    "context": "1: In order to correct the unfairness of models, many methods have also been proposed, which can be classified into one of the following three categories: <mark>pre-processing</mark> biased datasets, in-processing models during training, and post-processing the outputs of models. In-processing is usually the most effective way to intervene an unfair model (Petersen et al.<br>2: Based on the SGAP paradigm, the evaluation engine in PaSca involves the expensive neighborhood expansion only once in the pre/post-processing stages, and thus ensuring the scalability upon the number of training workers.<br>",
    "Arabic": "معالجة أولية",
    "Chinese": "预处理",
    "French": "prétraitement",
    "Japanese": "前処理",
    "Russian": "предобработка"
  },
  {
    "English": "pre-terminals",
    "context": "1: Definitions D is the training data containing input sentences x and gold derivationsÊ. e variables denote scored hyperedges. TAG(x) returns a set of scored <mark>pre-terminals</mark> for every word.<br>",
    "Arabic": "مسبقات",
    "Chinese": "预终结符",
    "French": "pré-terminaux",
    "Japanese": "前終端記号",
    "Russian": "предтерминалы"
  },
  {
    "English": "pre-train",
    "context": "1: We <mark>pre-train</mark> the model exclusively on the C4 dataset (Raffel et al., 2020), in BF16 for 1M steps, using an input sequence length of 4,096 and an output sequence length of 910. Pre-training optimization.<br>2: This means our method can be efficiently trained with a few steps of tuning. As shown in Figure 7, the prompt-based method converges faster than traditional <mark>pre-train</mark> and supervised methods, which further suggests the efficiency advantages of our method.<br>",
    "Arabic": "تدريب مسبق",
    "Chinese": "预训练",
    "French": "pré-entraîner",
    "Japanese": "事前学習",
    "Russian": "предобучение"
  },
  {
    "English": "pre-train parameter",
    "context": "1: This degradation due to proximity of the distracting content becomes catastrophically worse as models grow larger in the number of <mark>pre-trained parameters</mark>-in fact bringing their performance down to as much as 26.2 points below chance (in GPT-J, which has 6B parameters).<br>2: For finetuning, the BERT model is first initialized with the <mark>pre-trained parameters</mark>, and all of the parameters are fine-tuned using labeled data from the downstream tasks. Each downstream task has separate fine-tuned models, even though they are initialized with the same <mark>pre-trained parameters</mark>. The question-answering example in Figure 1 will serve as a running example for this section.<br>",
    "Arabic": "معلمات مسبقة التدريب",
    "Chinese": "预训练参数",
    "French": "paramètre pré-entraîné",
    "Japanese": "事前学習パラメータ",
    "Russian": "предварительно обученные параметры"
  },
  {
    "English": "pre-trained checkpoint",
    "context": "1: For each downstream task with our proposed method, we first fine-tune a full-precision network using the <mark>pre-trained checkpoint</mark> from huggingface 1 for both GPT-2 and BART. Then we use this fine-tuned network as the fullprecision teacher network and to initialize the quantized student network. We train each task with 8 V100 GPUs based on the Pytorch framework.<br>",
    "Arabic": "نقطة تفتيش مدربة مسبقا",
    "Chinese": "预训练检查点",
    "French": "point de contrôle pré-entraîné",
    "Japanese": "事前学習済みチェックポイント",
    "Russian": "предварительно обученная контрольная точка"
  },
  {
    "English": "pre-trained embedding",
    "context": "1: In the future, we would like to explore better integration strategies for machine learning systems that use <mark>pre-trained embeddings</mark> as features, so that downstream systems can better benefit from previously adjusting the embeddings in the semantics/syntax and similarity/relatedness axes.<br>2: Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same <mark>pre-trained embeddings</mark>.<br>",
    "Arabic": "التضمين المدرب مسبقًا",
    "Chinese": "预训练词嵌入",
    "French": "embeddings pré-entraînés",
    "Japanese": "事前学習埋め込み",
    "Russian": "предварительно обученные вложения"
  },
  {
    "English": "pre-trained language model",
    "context": "1: We present a new large-scale multi-modal instruction tuning benchmark dataset -MULTIINSTRUCT, which covers a wide variety of vision and multimodal tasks while each task is associated with multiple expert-written instructions. By finetuning OFA , a recently state-ofthe-art multimodal <mark>pre-trained language model</mark>, on MULTIINSTRUCT with instruction tuning, its zeroshot performance on various unseen multimodal tasks is significantly improved.<br>2: (2022a) show that a <mark>pre-trained language model</mark> (PLM) fine-tuned on a weakly labeled dataset often generalizes better than the weak labels synthesized by weak labeling sources.<br>",
    "Arabic": "نموذج لغوي مدرب مسبقا",
    "Chinese": "预训练语言模型",
    "French": "modèle de langage pré-entraîné",
    "Japanese": "事前トレーニング済み言語モデル",
    "Russian": "предобученная языковая модель"
  },
  {
    "English": "pre-trained model",
    "context": "1: Comparatively, the <mark>pre-trained model</mark> is much quicker to fine-tune, achieving the same 53.2% loss in roughly a single epoch. When fine-tuning, it is important to search over learning rates again, as the optimal learning rate on the joint training objective is often an order of magnitude smaller than that for pre-training.<br>2: Thus, there exists a mismatch in the high-level side-outputs: the edge predictions are coarse and global, while the ground truth still contains many weak edges that could even be considered as noise. This issue leads to problematic convergence behavior, even with the help of a <mark>pre-trained model</mark>.<br>",
    "Arabic": "نموذج تم تدريبه مسبقًا",
    "Chinese": "预训练模型",
    "French": "modèle pré-entraîné",
    "Japanese": "事前学習済みモデル",
    "Russian": "предобученная модель"
  },
  {
    "English": "pre-trained weight",
    "context": "1: By default, we finetune our model from this <mark>pre-trained weights</mark> for downstream video-text tasks. The impact of different weight initialization strategies is examined in Section 4.3. Implementation Details. We perform image-text pretraining on COCO Captions [5] and Visual Genome Captions [29].<br>2: It is both visually and linguistically richer, moving beyond whole shape descriptions to include segmentation maps and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent multi-modal models. We observe that <mark>pre-trained weights</mark> demonstrate limited abstract reasoning, which dramatically improves with fine-tuning.<br>",
    "Arabic": "أوزان مسبقة التدريب",
    "Chinese": "预训练权重",
    "French": "poids pré-entraînés",
    "Japanese": "事前学習重み",
    "Russian": "предобученные веса"
  },
  {
    "English": "pre-training corpus",
    "context": "1: Figure 6 shows the performances for different PLMs. Pre-training on more data clearly helps to overcome biases from weak labels. When the <mark>pre-training corpus</mark> is small, the model tends to fit the noisy weak labels more quickly than the clean labels and struggles to outperform weak labels throughout the entire training process.<br>2: With a large <mark>pre-training corpus</mark>, however, the model can make better predictions on clean labels than weak labels in the early stages of training, even when it is only trained on weak labels.<br>",
    "Arabic": "مدونة التدريب المسبق",
    "Chinese": "预训练语料库",
    "French": "corpus de pré-entraînement",
    "Japanese": "事前学習コーパス",
    "Russian": "корпус предварительного обучения"
  },
  {
    "English": "pre-training datum",
    "context": "1: We select models with a variety of architectures and pre-training corpora and fine-tune these models to study how different factors affect generalization. None of the models used any <mark>pre-training data</mark> that temporally overlap with CoNLL++, eliminating the possibility of articles in CoNLL++ appearing in any pre-training corpus.<br>",
    "Arabic": "بيانات ما قبل التدريب",
    "Chinese": "预训练数据",
    "French": "données de pré-entraînement",
    "Japanese": "事前学習データ",
    "Russian": "предварительные данные обучения"
  },
  {
    "English": "pre-training objective",
    "context": "1: Pre-training objective. To pre-train the model, we leverage the gap-sentences generation (GSG) unsupervised <mark>pre-training objective</mark>, which was introduced by PEGASUS  and is well-suited for sequence-to-sequence generation. Unlike BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) <mark>pre-training objective</mark>s, GSG endows the model with zero-shot summarization capabilities.<br>2: BERT alleviates the previously mentioned unidirectionality constraint by using a \"masked language model\" (MLM) <mark>pre-training objective</mark>, inspired by the Cloze task (Taylor, 1953).<br>",
    "Arabic": "هدف التدريب المسبق",
    "Chinese": "预训练目标",
    "French": "objectif de pré-entraînement",
    "Japanese": "事前学習目的",
    "Russian": "предварительная цель обучения"
  },
  {
    "English": "pre-training task",
    "context": "1: For each instance, we also randomly sample an instruction template for each batch-based training. Note that, though some of the training tasks in MULTIINSTRUCT are similar to the <mark>pre-training tasks</mark> of OFA 4 , we ensure that the evaluation tasks in MULTIINSTRUCT do not overlap with either the <mark>pre-training tasks</mark> in OFA nor the training tasks in MULTIINSTRUCT. 4<br>",
    "Arabic": "مهمة التدريب المسبق",
    "Chinese": "预训练任务",
    "French": "tâche de pré-entraînement",
    "Japanese": "事前学習タスク",
    "Russian": "задача предварительного обучения"
  },
  {
    "English": "precision",
    "context": "1: As can be seen from Fig. 5 and Fig. 6, using more GAT layers can improve the performance, judging by the <mark>Precision</mark>, Recall and F1-score. However, the improvement reaches a peak when using 6 GAT layers across all three evaluated tasks, and a further increase in the number of layers does not give improved performance.<br>2: In some tests we also evaluate the effectiveness of redundancyscoring algorithms, and factor out the effect of the redundancy threshold algorithm, by reporting average <mark>Precision</mark> and Recall figures for redundant documents. <mark>Precision</mark> and recall are well-known metrics in IR community. We adapt them to the redundancy detection task as shown below. Redundancy − P recision = R − R − + N − ( 11 ) Redundancy − Recall = R − R − + R + ( 12 ) Redundancy − M istake = R + + N − R + + N − + R − + N + ( 13 ) R − , R + , N − , N +<br>",
    "Arabic": "الدقة",
    "Chinese": "精准度",
    "French": "précision",
    "Japanese": "Precision",
    "Russian": "точность"
  },
  {
    "English": "precision matrix",
    "context": "1: , the vectors f i and f j are feature vectors for pixels i and j in an arbitrary feature space, w (m) are linear combination weights, and µ is a label compatibility function. Each kernel k (m) is characterized by a symmetric, positive-definite <mark>precision matrix</mark> Λ (m) , which defines its shape.<br>",
    "Arabic": "مصفوفة الدقة",
    "Chinese": "精度矩阵",
    "French": "matrice de précision",
    "Japanese": "精度行列",
    "Russian": "матрица точности"
  },
  {
    "English": "precision-at-10",
    "context": "1: An example for such a measure of quality is the <mark>precision-at-10</mark> (P@10) or the mean average precision (MAP) [22] of the query. Estimation of query difficulty is advantageous for several reasons: \n 1. Feedback to the user: The user can rephrase a \"difficult\" query to improve system effectiveness. 2.<br>2: While the average validation accuracy we achieve is 65%, we achieve higher precision in identifying the \"bad\" label: precisionat-10 is 83, precision-at-20 is 77, precision-at-30 is 72. It appears to be harder to identify very good captions than very low rated ones: <mark>precision-at-10</mark> is 77, precision-at-20 is 73, precision-at-30 is 70.<br>",
    "Arabic": "الدقة في المرتبة ١٠ (P@10)",
    "Chinese": "前10位精确度",
    "French": "précision-à-10",
    "Japanese": "上位10件における精度",
    "Russian": "точность-при-10"
  },
  {
    "English": "precision-recall curve",
    "context": "1: • Precision-recall curve is used as the evaluation metric, which is a standard metric used in the place recognition literature [16]. Precision is defined as the ratio of the retrieved correct places over all the retrieved places. Recall is defined as the ratio of the retrieved correct places over the ground-truth correct places.<br>",
    "Arabic": "منحنى الدقة والاسترجاع",
    "Chinese": "精确率-召回率曲线",
    "French": "Courbe précision-rappel",
    "Japanese": "精度-再現率曲線",
    "Russian": "кривая точности-полноты"
  },
  {
    "English": "precision-recall graph",
    "context": "1: Thus, the recall of the algorithm in this experiment was 63% (12 of the 19 confirmed duplicates were highlighted) and the precision was 71% (12 of the 17 highlighted record pairs are confirmed duplicates). However , the threshold of 37.5 was set based on the assumed 5 % rate of duplicates in the data set , and following the discussion of <mark>precision-recall graphs</mark> by Bilenko & Mooney [ 4 ] Figure 5 indicates how the precision and the recall varies with different thresholds ( an estimated 20 % rate of duplicates would give a 35.2 threshold , an<br>",
    "Arabic": "منحنى الدقة والاسترجاع",
    "Chinese": "精确率-召回率图",
    "French": "courbe précision-rappel",
    "Japanese": "適合率-再現率グラフ",
    "Russian": "график точности-полноты"
  },
  {
    "English": "precondition",
    "context": "1: Consider a planning task with a binary variable V , and an operator o with empty <mark>precondition</mark> and eff(o) = { V, 1 }.<br>2: Each operator o in the finite set O has a <mark>precondition</mark> pre(o) and an effect eff (o), which are both partial variable assignments over V. Operator o is applicable in a state s if s and pre(o) are consistent, i. e., they do not assign a variable to different values.<br>",
    "Arabic": "شرط مسبق",
    "Chinese": "先决条件 (precondition)",
    "French": "précondition",
    "Japanese": "前提条件",
    "Russian": "предусловие"
  },
  {
    "English": "preconditioner",
    "context": "1: 2 . Interestingly, Θ resembles a <mark>preconditioner</mark> that speeds up the linear semi-gradient TD update, similar to how second-order optimisation algorithms (Amari 1998;Martens 2016) precondition the gradient updates.<br>2: To evaluate our <mark>preconditioner</mark>, we lift our bilateral-space vector into pyramid-space, apply an element-wise scaling of each pyramid coefficient, and then project back onto bilateral-space: \n M −1 hier (y) = P T z weight • P (1) • P (y) P (diag(A)) (20 \n ) \n Fig.<br>",
    "Arabic": "مُعامِل التَّهيئة المُسبَقة",
    "Chinese": "预条件器",
    "French": "préconditionneur",
    "Japanese": "事前処理器",
    "Russian": "Предобусловливатель"
  },
  {
    "English": "predicate",
    "context": "1: So sentences like every artist is an artist would not be generated. Furthermore, we also remove cases where no elements are assigned to a <mark>predicate</mark> P i and perform re-balancing, since they also introduced easily exploitable patterns.<br>2: Current SRL approaches limit the search for arguments to the sentence containing the <mark>predicate</mark> of interest. Many systems take this assumption a step further and restrict the search to the <mark>predicate</mark>'s local syntactic environment; however, <mark>predicate</mark>s and the sentences that contain them rarely exist in isolation.<br>",
    "Arabic": "فاعل",
    "Chinese": "谓词",
    "French": "prédicat",
    "Japanese": "述語",
    "Russian": "предикат"
  },
  {
    "English": "predicate logic",
    "context": "1: Expressions like t, x, c denote finite lists of such entities. We use the standard <mark>predicate logic</mark> definitions of atom and formula, using symbols ϕ, ψ for the latter. Datalog queries are defined over an extended signature with additional predicate symbols, called IDB predicates; all other predicates are called EDB predicates.<br>2: While WASP works well for target MRLs that are free of logical variables such as CLANG (Wong and Mooney, 2006), it cannot easily handle various kinds of logical forms used in computational semantics, such as <mark>predicate logic</mark>. The problem is that WASP lacks a principled mechanism for handling logical variables.<br>",
    "Arabic": "منطق المسند",
    "Chinese": "谓词逻辑",
    "French": "logique des prédicats",
    "Japanese": "述語論理",
    "Russian": "логика предикатов"
  },
  {
    "English": "predicate symbol",
    "context": "1: The standard language for formal meaning representation is first-order logic. A term is any expression representing an object in the domain. An atomic formula or atom is a <mark>predicate symbol</mark> applied to a tuple of terms. Formulas are recursively constructed from atomic formulas using logical connectives and quantifiers.<br>2: Probabilistic Logic Programming We will introduce probabilistic logic programming (PLP) using the syntax of the ProbLog system [De Raedt and Kimmig, 2015]. An atom is a <mark>predicate symbol</mark> followed by a tuple of logical variables and/or constants. A literal is an atom or its negation.<br>",
    "Arabic": "رمز المحمول",
    "Chinese": "谓词符号",
    "French": "symbole de prédicat",
    "Japanese": "述語記号",
    "Russian": "Символ предиката"
  },
  {
    "English": "predicate-argument relation",
    "context": "1: Since this model will be used to generate <mark>predicate-argument relations</mark> but not scoping relations, these meaning functions are constrained to describe simple existentiallyquantified variables over instances of entities or eventualities, connected by a set of numbered argument relations.<br>2: In particular,  and, subsequently, Cai et al. (2018),  and , indicate that predicate sense signals aid the identification of predicateargument relations. Therefore, we follow this line and propose an end-to-end system for cross-lingual SRL. Multilingual SRL.<br>",
    "Arabic": "علاقة الموضوع-الحُجة",
    "Chinese": "谓词-论元关系",
    "French": "relation prédicat-argument",
    "Japanese": "述語と引数の関係",
    "Russian": "отношение предикат-аргумент"
  },
  {
    "English": "predicate-argument structure",
    "context": "1: These results confirm empirically our initial hunch that semantic role labeling relations are deeply rooted beyond languages, independently of their surface realization and their <mark>predicate-argument structure</mark> inventories. Finally , for completeness , Appendix E includes the results of our system on the individual subtasks , namely , predicate identification and predicate sense The improvements of our cross-lingual approach compared to the more traditional monolingual baseline are evident , especially in lower-resource scenarios , with absolute improvements in F 1 score of 25.5 % , 9.7 % and 26.9 % on the<br>2: Ultimately, however, we want our model to provide semantic role annotations according to an existing <mark>predicate-argument structure</mark> inventory, e.g., PropBank, AnCora, or PDT-Vallex.<br>",
    "Arabic": "البنية الحجة-المحمول",
    "Chinese": "谓词论元结构",
    "French": "structure prédicat-argument",
    "Japanese": "述語論項構造",
    "Russian": "структура предикат-аргумент"
  },
  {
    "English": "prediction",
    "context": "1: Moreover, queries are flexible to model and encode a variety of interactions, e.g., relations among multiple agents. To the best of our knowledge, UniAD is the first work to comprehensively investigate the joint cooperation of such a variety of tasks including perception, <mark>prediction</mark> and planning in the field of autonomous driving.<br>2: Initialize: η0 ← 1 8 , A0 ← 1 n I Prediction: Given (xt, yt), predictdt = dA t (xt, yt) . Update: Upon receiving \"true\" dt, update model as \n At+1 ← At − 2ηt ( dt − dt ) At ( xt − yt ) ( xt − yt ) T At 1 + 2ηt ( dt − dt ) ( xt − yt ) T At ( xt − yt ) , where ηt = η0 ifdt − dt ≥ 0 ; otherwise , ηt = min η0 , 1 2 (<br>",
    "Arabic": "التنبؤ",
    "Chinese": "预测",
    "French": "prédiction",
    "Japanese": "予測",
    "Russian": "предсказание"
  },
  {
    "English": "prediction accuracy",
    "context": "1: In this section we examine the <mark>prediction accuracy</mark> of the hashing-based detector as the number of unique objects in the system systematically increases. Section 4.3 demonstrated the inherent trade-off between the <mark>prediction accuracy</mark> and computational resources for our object detector.<br>2: This set of model parameters demonstrates that a hashing-based object detector can be scaled up to a large number of objects while achieving reasonable balance between <mark>prediction accuracy</mark> and evaluation throughput. We note that the absolute times reported here are based on unoptimized code.<br>",
    "Arabic": "دقة التنبؤ",
    "Chinese": "预测准确性",
    "French": "précision de la prédiction",
    "Japanese": "予測精度",
    "Russian": "точность предсказания"
  },
  {
    "English": "prediction entropy",
    "context": "1: Query strategies are abbreviated as follows: <mark>prediction entropy</mark> (PE), breaking ties (BT), least confidence (LC), contrastive active learning (CA), BALD (BA), BADGE (BD), greedy coreset (CS), and random sampling (RS).<br>2: We find that smaller models have a larger bias towards predicting a single label (lower <mark>prediction entropy</mark>), while larger and IT models get closer to H(Y ) (see Figure 6).<br>",
    "Arabic": "الانتروبيا التنبؤ",
    "Chinese": "预测熵",
    "French": "entropie de prédiction",
    "Japanese": "予測エントロピー",
    "Russian": "энтропия предсказания"
  },
  {
    "English": "prediction error",
    "context": "1: TangentProp is itself closely related to the Double Backpropagation algorithm (Drucker and LeCun, 1992), in which one instead adds a penalty that is the sum of squared derivatives of the <mark>prediction error</mark> (with respect to the network input).<br>2: The SAP score (Kumar et al., 2017) is the average difference of the <mark>prediction error</mark> of the two most predictive latent dimensions for each factor. Implementation details and further descriptions can be found in Appendix C. Data sets.<br>",
    "Arabic": "خطأ التنبؤ",
    "Chinese": "预测误差",
    "French": "erreur de prédiction",
    "Japanese": "予測誤差",
    "Russian": "ошибка предсказания"
  },
  {
    "English": "prediction head",
    "context": "1: Given a GNN model that outputs node features, we add a learnable <mark>prediction head</mark> that takes each node feature (or two node features corresponding to each edge) as input and predicts whether it is a cut vertex (cut edge) or not.<br>",
    "Arabic": "رأس التنبؤ",
    "Chinese": "预测头",
    "French": "tête de prédiction",
    "Japanese": "予測ヘッド",
    "Russian": "предсказательная голова"
  },
  {
    "English": "prediction invariance",
    "context": "1: To break down potential capability failures into specific behaviors, CheckList introduces different test types, such as <mark>prediction invariance</mark> in the presence of certain perturbations, or performance on a set of \"sanity checks.\"<br>",
    "Arabic": "الثبات التنبؤي",
    "Chinese": "预测不变性",
    "French": "invariance des prédictions",
    "Japanese": "予測不変性",
    "Russian": "инвариантность предсказания"
  },
  {
    "English": "prediction model",
    "context": "1: However, producing a rough future trajectory is still challenging in the real world, toward which [62] presents a deep structured model to derive both prediction and planning from the same set of learnable costs. [39,40] couple the <mark>prediction model</mark> with classic optimization methods.<br>2: These include the number of examples in each of the dataset splits, the size on disk of the data, meaningful differences between the training, validation, and test split, and free text descriptions of the various fields that make up each example to help decide what information to use as input or output of a <mark>prediction model</mark>.<br>",
    "Arabic": "نموذج التنبؤ",
    "Chinese": "预测模型",
    "French": "modèle prédictif",
    "Japanese": "予測モデル",
    "Russian": "модель прогнозирования"
  },
  {
    "English": "prediction network",
    "context": "1: We can apply our framework to an existing RE model by using the feature encoder of the model and building the four sub-networks which exploit the structure of the original <mark>prediction network</mark>. Since our framework uses the feature encoder of the existing models, we briefly describe only the output layer here.<br>2: Since the labels obtained from distant supervision are noisy and biased, with a single <mark>prediction network</mark>, it is hard to make accurate predictions for DS data and HA data together.<br>",
    "Arabic": "شبكة التنبؤ",
    "Chinese": "预测网络",
    "French": "réseau de prédiction",
    "Japanese": "予測ネットワーク",
    "Russian": "сеть предсказания"
  },
  {
    "English": "prediction variance",
    "context": "1: This work suggests using two model-specific measures confidence and <mark>prediction variance</mark> -as indicators of a training example's \"learnability\" (Chang et al., 2017;. Dataset Maps , a recently introduced framework uses these two measures to profile datasets to find learnable examples.<br>",
    "Arabic": "تباين التنبؤ",
    "Chinese": "预测方差",
    "French": "variance de prédiction",
    "Japanese": "予測分散",
    "Russian": "прогнозная дисперсия"
  },
  {
    "English": "predictive coding",
    "context": "1: Memory efficiency is not the only objective that can be constructed to learn abstractions over data without supervision. It has also been proposed that language learning may be driven by optimizing prediction of future input (Rohde and Plaut, 1999;Phillips and Ehrenhofer, 2015;Apfelbaum and McMurray, 2017). This proposal aligns with an extensive neuroscience literature arguing that <mark>predictive coding</mark> for future inputs is a `` canonical computation '' of the human brain ( Keller and Mrsic-Flogel , 2018 ) and may better characterize the tuning of biological neurons than sparse coding ( Singer et al. , 2018 ) , possibly because prediction affords advantages in critical tasks ( Nijhawan , 1994<br>2: The question of the actual learning mechanisms in the brain is highly contested, but a promising candidate, especially for perceptual learning, might be <mark>predictive coding</mark> [3,4]. Here future stimuli are predicted and then compared to the actual occurred stimuli. The learning entails reducing the discrepancy between prediction and reality.<br>",
    "Arabic": "الترميز التنبؤي",
    "Chinese": "预测编码",
    "French": "codage prédictif",
    "Japanese": "予測符号化",
    "Russian": "предиктивное кодирование"
  },
  {
    "English": "predictive distribution",
    "context": "1: The Archipelago base density was a bivariate Gaussian. Figure 3 shows the predictive modes for the Archipelago and softmax models, as well as the entropy of the Archipelago <mark>predictive distribution</mark> as a function of space. Numerical results are in Table 1.<br>2: As in the previous section, the normalization of the lower level random measures µ θ will be marginalized. Finally, we let: \n µ = G + H0 µ (H) θ = F θ + µ0 μ whereμ (H) θ \n can be recognized as the mean parameter of the <mark>predictive distribution</mark> of the HDP.<br>",
    "Arabic": "التوزيع التنبؤي",
    "Chinese": "预测分布",
    "French": "distribution prédictive",
    "Japanese": "予測分布",
    "Russian": "предсказательное распределение"
  },
  {
    "English": "predictive likelihood",
    "context": "1: Quantitative evaluation against human ratings shows that the induced clusters of address terms correspond to intuitive perceptions of formality, and that the network structural features improve <mark>predictive likelihood</mark> over a purely text-based model. Qualitative evaluation shows that the model makes reasonable predictions of the level of formality of social network ties in well-known movies.<br>",
    "Arabic": "الاحتمال التنبؤي",
    "Chinese": "预测可能性",
    "French": "probabilité prédictive",
    "Japanese": "予測尤度",
    "Russian": "предсказательная правдоподобность"
  },
  {
    "English": "predictive model",
    "context": "1: This research has been conducted in close collaboration with domain experts to design our approach. Their insights have shaped the trajectory of our project. For example, our initial idea was to plan information-gathering patrols, taking an active learning approach to gather data where the <mark>predictive model</mark> was most uncertain.<br>2: Meanwhile, the data provider, with full knowledge of the learner's <mark>predictive model</mark> w, selects its own strategy (i.e., the modified datax i ) to make the corresponding prediction w Tx i close to the desired label z i .<br>",
    "Arabic": "نموذج تنبؤي",
    "Chinese": "预测模型",
    "French": "modèle prédictif",
    "Japanese": "予測モデル",
    "Russian": "предсказательная модель"
  },
  {
    "English": "predictive performance",
    "context": "1: These utility weights are then used to mimic the behavior in similar situations through a decision-making algorithm. Unlike the other two communities, it is the <mark>predictive performance</mark> of the learned model that is most pivotal and noisy observations are expected and managed by those techniques.<br>",
    "Arabic": "الأداء التنبؤي",
    "Chinese": "预测性能",
    "French": "performance prédictive",
    "Japanese": "予測性能",
    "Russian": "Прогностическая производительность"
  },
  {
    "English": "predictor",
    "context": "1: For simplicity we use the same learning rate for the <mark>predictor</mark> as we do for the online network (i.e. α p = 1). In this situation, we can treat w a as approximately constant on the fast time scale over which the online and <mark>predictor</mark> weights w and w p evolve.<br>2: Let X be an arbitrary input space and Y ⊂ R d be a d-dimensional output (label) space. We assume the data source is defined by a fixed but unknown distribution over X × Y. Our goal is to learn a <mark>predictor</mark> F : \n<br>",
    "Arabic": "المتنبئ",
    "Chinese": "预测器",
    "French": "prédicteur",
    "Japanese": "予測子",
    "Russian": "предиктор"
  },
  {
    "English": "prefix",
    "context": "1: We first use supervised fine-tuning on a subset of the IMDB data for 1 epoch. We then use this model to sample 4 completions for 25000 <mark>prefix</mark>es and create 6 preference pairs for each <mark>prefix</mark> using the ground-truth reward model.<br>2: The segmentation is still based on the <mark>prefix</mark>, stem and suffix level. • PrStSu: a word is modeled as a <mark>prefix</mark>, stem and suffix sequence, where the <mark>prefix</mark> and suffix are sequences of zero or more morphs. The segmentation is based on the <mark>prefix</mark>, stem and suffix level.<br>",
    "Arabic": "سابقة",
    "Chinese": "前缀",
    "French": "préfixe",
    "Japanese": "接頭辞 (prefix)",
    "Russian": "префикс"
  },
  {
    "English": "prefix sum",
    "context": "1: Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering (Särkkä and García-Fernández, 2019), typically used for computing <mark>prefix sums</mark>.<br>",
    "Arabic": "مجموع البادئة",
    "Chinese": "前缀和",
    "French": "somme partielle",
    "Japanese": "前方部分和",
    "Russian": "префиксная сумма"
  },
  {
    "English": "prefix tree",
    "context": "1: Due to the hierarchical property of semantic identifiers, it is easy to constrain the beam search on the <mark>prefix tree</mark> so that only valid identifiers will be generated.<br>",
    "Arabic": "شجرة البادئة",
    "Chinese": "前缀树",
    "French": "arbre de préfixes",
    "Japanese": "接頭辞木",
    "Russian": "префиксное дерево"
  },
  {
    "English": "preimage",
    "context": "1: Over the complex numbers, the cardinality of the <mark>preimage</mark> Φ −1 p,l,I,m (x, ℓ) is the same for every generic joint image (x, ℓ) of a minimal point-line problem (p, l, I, m).<br>2: Moreover, the cardinality of the <mark>preimage</mark> of a generic point Y ∈ Y p,l,I,m under both maps Φ p,l,I,m and π Y is the same.<br>",
    "Arabic": "الصورة الأصلية",
    "Chinese": "原像",
    "French": "préimage",
    "Japanese": "先像",
    "Russian": "прообраз"
  },
  {
    "English": "prepositional phrase",
    "context": "1: We believe we would get even better results if the parser could determine the true branching structure. We then adopt the following definition of a grandparent-head feature j. 1. if c is a noun phrase under a <mark>prepositional phrase</mark> , or is a pre-terminal which takes a revised head as defined above , then j is the grandparent head of c , else 2. if c is a pre-terminal and is not next ( in the production generating c ) to the head of its parent ( i ) then j (<br>2: Applying it to fragments will produce individual trees that may not be representative of the ambiguity present in the underlying representation. For example, after the word \"on\" the read-out network outputs a <mark>prepositional phrase</mark> that initially appears to attach to the verb.<br>",
    "Arabic": "عبارة حروف جر",
    "Chinese": "介词短语",
    "French": "groupe prépositionnel",
    "Japanese": "前置詞句",
    "Russian": "предложная фраза"
  },
  {
    "English": "preprocessing phase",
    "context": "1: Moreover, we show that, using the algorithm in a <mark>preprocessing phase</mark>, uniform sampling of Markov equivalent DAGs can be performed in linear time. We prove that our results are tight in the sense that counting Markov equivalent DAGs that encode additional background knowledge is intractable under standard complexitytheoretic assumptions.<br>",
    "Arabic": "مرحلة المعالجة المسبقة",
    "Chinese": "预处理阶段",
    "French": "phase de prétraitement",
    "Japanese": "前処理段階",
    "Russian": "фаза предварительной обработки"
  },
  {
    "English": "presence penalty",
    "context": "1: Generating line-byline from this prompt could help to facilitate brainstorming for: unusual cartoon situations (first 4 lines), concepts about real or generated contests that could serve as a basis for a humorous caption (line 5), and, captions themselves (lines 6-8). As a demonstration , we present an unconditional sample , in which the model describes a garden party where a chicken is playing croquet ( cherry picked from 3 outputs ; temperature=.8 , top p=.9 , frequency penalty=.2 , <mark>presence penalty</mark>=.05 ) , and also , a conditional sample , given a basic description of Contest # 818 's scene , which ran in<br>",
    "Arabic": "عقوبة الوجود",
    "Chinese": "存在惩罚",
    "French": "pénalité de présence",
    "Japanese": "存在ペナルティ",
    "Russian": "штраф за присутствие"
  },
  {
    "English": "pretrained multilingual model",
    "context": "1: This result also corroborates similar findings for <mark>pretrained multilingual models</mark> (Conneau et al., 2020), although those experiments did not control the total quantity of data as in ours. 10<br>",
    "Arabic": "النموذج متعدد اللغات المدرب مسبقا",
    "Chinese": "预训练多语言模型",
    "French": "modèle multilingue pré-entraîné",
    "Japanese": "事前学習済み多言語モデル",
    "Russian": "предобученная многоязычная модель"
  },
  {
    "English": "primal objective function",
    "context": "1: Our approach also extends to non-linear kernels while working solely on the <mark>primal objective function</mark>, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.<br>2: (3) where f (w) is the <mark>primal objective function</mark>. A variant consists in using a softmax instead of a max in ( 3 ) : maxy ( F ( x i , y , w ) +∆ ( y i , y ) −F ( x i , y i , w ) ) ≈max 0 , log y =y i e F ( x i , y , w ) +∆ (<br>",
    "Arabic": "دالة الهدف الأصلية",
    "Chinese": "原始目标函数",
    "French": "fonction objectif primale",
    "Japanese": "原始目的関数",
    "Russian": "целевая функция"
  },
  {
    "English": "primal optimization",
    "context": "1: result in a rather slow convergence rate to the optimum of the primal objective function . (See also the discussion in [19].) Primal optimization: Most existing approaches, including the methods discussed above, focus on the dual of Eq. (1), especially when used in conjunction with non-linear kernels.<br>",
    "Arabic": "التحسين الأولي",
    "Chinese": "原始优化",
    "French": "optimisation primale",
    "Japanese": "プライマル最適化",
    "Russian": "прямая оптимизация"
  },
  {
    "English": "primal problem",
    "context": "1: Following [16,24,10], the approach we take here is to directly minimize the <mark>primal problem</mark> while still using kernels. INPUT: S, λ, T \n INITIALIZE: Set α 1 = 0 FOR t = 1, 2, . . . , T Choose it ∈ {0, . . . , |S|} uniformly at random. For all j = it , set α t+1 [ j ] = αt [ j ] If y it 1 λt j αt [ j ] y it K ( x it , x j ) < 1 , then : Set α t+1 [ it ] = αt [ it ] + 1 Else : Set α t+1 [ it ] =<br>2: , ( 11 \n ) \n where C is the trade-off constant between maximizing the margin and satisfying the pairwise relative constraints. We solve the above <mark>primal problem</mark> using Newton's method [29]. While we use a linear ranking function in our experiments, the above formulation can be easily extended to kernels.<br>",
    "Arabic": "المشكلة الأولية",
    "Chinese": "原始问题",
    "French": "problème primal",
    "Japanese": "原始問題",
    "Russian": "первичная проблема"
  },
  {
    "English": "primal variable",
    "context": "1: Second, the number of dual variables, |Φ| dim V , is typically much fewer than the number of <mark>primal variables</mark>, |A| + |Φ| 2 . Though<br>2: We solve (3) by first taking the dual form of the inner optimization problem with respect to the <mark>primal variables</mark> ρ, w s , γ s , ξ s i and ξ T i , where s = 1, . . .<br>",
    "Arabic": "المتغيرات الأولية",
    "Chinese": "原变量",
    "French": "variable primale",
    "Japanese": "原始変数 (genshi hensu)",
    "Russian": "первичная переменная"
  },
  {
    "English": "primal-dual algorithm",
    "context": "1: We will adapt the <mark>primal-dual algorithm</mark> that was proposed in [22,23] for solving the closely related Multi-cut problem. We review this algorithm in Section 3.1 and in Section 3.2 show how we adapt it to solve the BMC LP.<br>2: 5 we solve (25) directly using the <mark>primal-dual algorithm</mark> [9], using the baseline functional lifting method [17] and using our proposed algorithm. First, the globally optimal energy was computed using the direct method with a very high number of iterations.<br>",
    "Arabic": "الخوارزمية الابتدائية-الثنائية",
    "Chinese": "原始-对偶算法",
    "French": "algorithme primal-dual",
    "Japanese": "プライマル・デュアルアルゴリズム",
    "Russian": "первично-двойственный алгоритм"
  },
  {
    "English": "primal-dual method",
    "context": "1: (i) proximal algorithms include half-quadratic splitting (Zhang et al., 2017b), primaldual method (Ono, 2017), generalized approximate message passing (Metzler et al., 2016b) and (stochastic) accelerated proximal gradient method (Sun et al., 2019a). ( ii ) imaging applications have such as bright field electronic tomography ( Sreehari et al. , 2016 ) ; diffraction tomography ( Sun et al. , 2019a ) ; low-dose CT imaging ( He et al. , 2018 ) ; Compressed Sensing MRI ( Eksioglu , 2016 ) ; electron microscopy ( Sreehari et al. , 2017 ) ; single-photon imaging ( Chan<br>",
    "Arabic": "الطريقة الأولية المزدوجة",
    "Chinese": "原始对偶法",
    "French": "méthode primale-duale",
    "Japanese": "一次-双対法 (ichiji-sotai-ho)",
    "Russian": "метод прямо-двойственный"
  },
  {
    "English": "primitive",
    "context": "1: As a result, users must write extensive custom code that strings pipelines of multiple models together. Research projects using models of different modalities (e.g., combining an imagebased formula detector with a text-based definition extractor) can require hundreds of lines of code. We introduce papermage, an open-source Python toolkit for processing scientific documents. Its contributions include ( 1 ) magelib , a library of <mark>primitives</mark> and methods for representing and manipulating visually-rich documents as multimodal constructs , ( 2 ) Predictors , a set of implementations that integrate different state-of-the-art scientific document analysis models into a unified interface , even if individual models are written in different frameworks or operate on different modalities , and ( 3<br>2: To test our model on multi-object scenes, we use the script from [36] to render scenes with 2, 3, 4, or 5 random <mark>primitives</mark> (Clevr-N). To test our model on scenes with a varying number of objects, we also run our model on the union of them (Clevr-2345).<br>",
    "Arabic": "بدائية",
    "Chinese": "基元",
    "French": "primitives",
    "Japanese": "基本形状",
    "Russian": "примитивы"
  },
  {
    "English": "principal component",
    "context": "1: More recently, neural algorithms for learning the <mark>principal components</mark> non-sequentially have been developed, either using Hebbian/anti-Hebbian learning rules in biological networks (Pehlevan et al., 2015) or specific weight regularization in linear autoencoders (Kunin et al., 2019). Neural algorithms for ICA also exist.<br>2: The vast majority of code for image modeling and inference is reusable across   3. The variables MU, PC, EV correspond to the mean shape/texture face, <mark>principal components</mark>, and eigenvectors respectively (see [36] for details). These arguments parametrize the prior on the learned shape and appearance of 3D faces.<br>",
    "Arabic": "مكونات رئيسية",
    "Chinese": "主成分",
    "French": "composante principale",
    "Japanese": "主成分",
    "Russian": "главная компонента"
  },
  {
    "English": "principal component analysis",
    "context": "1: We first downsample the high-dimensional representations (of either the fused language and text, or either unimodal representations) via <mark>Principal Component Analysis</mark> (PCA) to make the distance computation faster by an order of magnitude.<br>2: Experimental work has shown that using Independent Component Analysis as a projection method in Rotation Forest does not generally improve on Rotation Forest with <mark>Principal Component Analysis</mark>. Moreover, according to dominance ranks, Rotation Forest PCA is the preferred method.<br>",
    "Arabic": "تحليل المكونات الرئيسية",
    "Chinese": "主成分分析",
    "French": "analyse en composantes principales",
    "Japanese": "主成分分析",
    "Russian": "анализ главных компонент"
  },
  {
    "English": "prior distribution",
    "context": "1: Note that (1/α) simply rescales the observed matrix S, and we can make the model scale invariant by introducing a <mark>prior distribution</mark> and integrating out α. The conditional posterior for α follows an inverse Gamma distribution \n p(α|r, s) = s r Γ(r) 1 α r+1 exp − s α ,(11) \n<br>2: A normalizing flow maps a prior (source) distribution to a target distribution via the change of variables formula (Rezende and Mohamed, 2015;Dinh et al., 2016;Papamakarios et al., 2019).<br>",
    "Arabic": "التوزيع السابق",
    "Chinese": "先验分布",
    "French": "distribution a priori",
    "Japanese": "事前分布",
    "Russian": "априорное распределение"
  },
  {
    "English": "prior hyperparameter",
    "context": "1: generalization in practice ! In Figure 5(c), we have seen that this correlation can be fixed by optimizing the prior precision, but in general there is no recipe for how many <mark>prior hyperparameters</mark> we should be optimizing to ensure a positive correlation.<br>2: bound holds for each individual model simultaneously . Even though the logarithm may scale slowly in the number of models we compare, this term accumulates. If we tune real-valued <mark>prior hyperparameters</mark> using gradient-based optimizers on the marginal likelihood , as is common practice ( e.g. , MacKay , 1992d ; Rasmussen and Williams , 2006 ; Wilson and Adams , 2013 ; Hensman et al. , 2013 ; Wilson et al. , 2016a ; Molchanov et al. , 2017 ; Daxberger et al. , 2021 ; Immer et al.<br>",
    "Arabic": "المعلمة المفرطة السابقة",
    "Chinese": "先验超参数",
    "French": "hyperparamètre a priori",
    "Japanese": "事前ハイパーパラメータ",
    "Russian": "предварительный гиперпараметр"
  },
  {
    "English": "prior knowledge",
    "context": "1: The latter enables the model to use its <mark>prior knowledge</mark> on the subject class while the class-specific instance is bound with the unique identifier. In order to prevent language drift [ 34,40 ] that causes the model to associate the class name ( e.g. , `` dog '' ) with the specific instance , we propose an autogenous , class-specific prior preservation loss , which leverages the semantic prior on the class that is embedded in the model , and encourages it to generate diverse instances of the<br>",
    "Arabic": "المعرفة السابقة",
    "Chinese": "先验知识",
    "French": "connaissance préalable",
    "Japanese": "事前知識",
    "Russian": "предварительные знания"
  },
  {
    "English": "prior mean",
    "context": "1: Given enough flexibility with the <mark>prior mean</mark>, the marginal likelihood overfits the data, providing poor overconfident predictions outside of the train region.<br>2: We can see this effect in Figure 4 Moreover, we can design a third model, M 2 , with a prior variance 0.07 and <mark>prior mean</mark> 2 which leads to a poor fit of the data but achieves higher marginal likelihood than M 1 .<br>",
    "Arabic": "المتوسط المسبق",
    "Chinese": "先验均值",
    "French": "moyenne a priori",
    "Japanese": "事前平均値",
    "Russian": "приорное матожидание"
  },
  {
    "English": "prior probability",
    "context": "1: (3) where is the <mark>prior probability</mark> of hypothesis g h . In addition to the edge likelihood model, other cues about the region properties of the foreground and background, e.g., mixture color models, can easily be integrated into the HMM framework. Let \" A n %<br>2: For example, Milne and Witten (2008) show that by only using the <mark>prior probability</mark> gathered from hyperlink statistics on Wikipedia training articles, one can achieve 90% accuracy on the task of predicting links in Wikipedia test articles.<br>",
    "Arabic": "احتمالية سابقة",
    "Chinese": "先验概率",
    "French": "probabilité a priori",
    "Japanese": "事前確率",
    "Russian": "априорная вероятность"
  },
  {
    "English": "prior probability distribution",
    "context": "1: Given a <mark>prior probability distribution</mark> q = (q i ) n i=1 and a set of constraints C, the principle of minimum relative entropy chooses the posterior probability distribution p = (p i ) n i=1 that has the least relative entropy 2 with respect to q: \n<br>",
    "Arabic": "التوزيع الاحتمالي السابق",
    "Chinese": "先验概率分布",
    "French": "distribution de probabilité a priori",
    "Japanese": "事前確率分布",
    "Russian": "априорное распределение вероятностей"
  },
  {
    "English": "prior variance",
    "context": "1: Moreover, as discussed in Section 5, the Laplace approximation is especially sensitive to the <mark>prior variance</mark>, and the number of parameters in the model. By the same rationale, we expect the conditional marginal likelihood to help alleviate this problem, since it evaluates the likelihood of the data under the posterior, rather than the prior.<br>2: We can see this effect in Figure 4 Moreover, we can design a third model, M 2 , with a <mark>prior variance</mark> 0.07 and prior mean 2 which leads to a poor fit of the data but achieves higher marginal likelihood than M 1 .<br>",
    "Arabic": "تباين مسبق",
    "Chinese": "先验方差",
    "French": "variance a priori",
    "Japanese": "事前分散",
    "Russian": "априорная дисперсия"
  },
  {
    "English": "priority queue",
    "context": "1: Solving scored queries can be achieved with DAAT by computing the relevance score for the matching documents as they are found, and maintaining a <mark>priority queue</mark> with the top-k matches. This can be very inefficient for scored disjunctive queries, as the whole lists need to be scanned. Several query processing strategies have been introduced to alleviate this problem.<br>2: As mentioned earlier, it is a bestfirst search algorithm and uses a <mark>priority queue</mark> to maintain the nodes to be expanded next. It is also budgeted and returns \"budget_reached\" if too many nodes have been expanded. It returns \"no_solution\" if all nodes have been expanded without reaching a solution nodeassuming safe pruning or no pruning.<br>",
    "Arabic": "طابور الأولوية",
    "Chinese": "优先队列",
    "French": "file de priorité",
    "Japanese": "優先度付きキュー",
    "Russian": "очередь с приоритетом"
  },
  {
    "English": "privacy budget",
    "context": "1: where Ef l (x) is the expected value of f l (x), ε is a predefined <mark>privacy budget</mark>, and δ is a broken probability. When we use a Laplace noise, δ = 0.<br>2: for all neighbor dataset pair (D, D ) and all subset S M of the range of M. Without knowledge of explicit form of model parameter distribution, we can only claim that the <mark>privacy budget</mark> varies at the order of O( |S| |T | ).<br>",
    "Arabic": "ميزانية الخصوصية",
    "Chinese": "隐私预算",
    "French": "budget de confidentialité",
    "Japanese": "プライバシー予算",
    "Russian": "бюджет конфиденциальности"
  },
  {
    "English": "privacy-preserve data mining",
    "context": "1: The key strategy is to discretize continuous features and operate the counterfactual generation engine in the categorical feature space. Discretization is closely related to the generalization technique used in <mark>privacy-preserving data mining</mark> (PPDM) [12,31]. It is also treated as a subroutine to analyze the composition of differential privacy algorithms [14,17].<br>2: Our proposed solution is different from <mark>privacy-preserving data mining</mark> (PPDM) due to the fact that we allow data sharing instead of data mining result sharing. This is an essential requirement for the BTS since they require the flexibility to perform various data analysis tasks.<br>",
    "Arabic": "تعدين البيانات الحافظ للخصوصية",
    "Chinese": "隐私保护数据挖掘",
    "French": "exploration de données préservant la confidentialité",
    "Japanese": "プライバシー保護データマイニング",
    "Russian": "интеллектуальный анализ данных с сохранением конфиденциальности"
  },
  {
    "English": "probabilistic context-free grammar",
    "context": "1: The inputs in COGS are English sentences, generated by a <mark>probabilistic context-free grammar</mark>. The corresponding output, which is the semantic interpretation of the input, is annotated with the logical formalism in Reddy et al. (2017). COGS includes a randomly sampled test set and an out-of-distribution compositional generalization set.<br>2: The inputs in COGS are English sentences, generated by a <mark>probabilistic context-free grammar</mark>. The corresponding output, which is the semantic interpretation of the input, is annotated with the logical formalism of Reddy et al. (2017). COGS includes a randomly sampled test set and an out-ofdistribution compositional generalization set.<br>",
    "Arabic": "قواعد النحو الاحتمالية الخالية من السياق",
    "Chinese": "概率上下文无关文法",
    "French": "grammaire hors-contexte probabiliste",
    "Japanese": "確率的文脈自由文法",
    "Russian": "вероятностная контекстно-свободная грамматика"
  },
  {
    "English": "probabilistic distribution",
    "context": "1: As noted in Section 2, the PL model [19,25] has often been deployed to model a <mark>probabilistic distribution</mark> over rankings [3, 11, 16, 21-23, 28, 32, 35]. In the PL model, an item is chosen from a pool of available items based on the individual scores each item has.<br>2: where D denotes the (transposed) Jacobian of x (K) (w). For other matching functions and target strategies, BMG produces different meta-updates compared to MG. We discuss these choices below. Matching Function Of primary concern to us are models that output a <mark>probabilistic distribution</mark>, π x .<br>",
    "Arabic": "توزيع احتمالي",
    "Chinese": "概率分布",
    "French": "distribution probabiliste",
    "Japanese": "確率分布",
    "Russian": "вероятностное распределение"
  },
  {
    "English": "probabilistic formulation",
    "context": "1: For image formation models that observe scenes directly in 3D, the ray-marcher may be left out completely, and φ may be sampled directly. Probabilistic formulation. An interesting avenue of future work is to extend SRNs to a probabilistic model that can infer a probability distribution over feasible scenes consistent with a given set of observations.<br>",
    "Arabic": "الصياغة الاحتمالية",
    "Chinese": "概率形式化",
    "French": "formulation probabiliste",
    "Japanese": "確率的定式化",
    "Russian": "вероятностная формулировка"
  },
  {
    "English": "probabilistic framework",
    "context": "1: In this paper, we have presented the TAS model, a <mark>probabilistic framework</mark> that captures the contextual information between \"stuff\" and \"things\", by linking discriminative detection of objects with unsupervised clustering of image regions.<br>2: Our approach simultaneously provides a solution to the problems of jointly considering evidence about multiple relationships as well as lexical ambiguity within a single <mark>probabilistic framework</mark>.<br>",
    "Arabic": "إطار احتمالي",
    "Chinese": "概率框架",
    "French": "cadre probabiliste",
    "Japanese": "確率的枠組み",
    "Russian": "вероятностная структура"
  },
  {
    "English": "probabilistic generative model",
    "context": "1: What underlying process causes a graph to systematically densify, with a fixed exponent as in Equation ( 1), and to experience a decrease in effective diameter even as its size increases? This question motivates the second main contribution of this work: we present two families of <mark>probabilistic generative models</mark> for graphs that capture aspects of these properties.<br>2: Diffusion models are <mark>probabilistic generative models</mark> that are trained to learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution.<br>",
    "Arabic": "نموذج توليدي احتمالي",
    "Chinese": "概率生成模型",
    "French": "modèle génératif probabiliste",
    "Japanese": "確率生成モデル",
    "Russian": "вероятностная генеративная модель"
  },
  {
    "English": "probabilistic graphical model",
    "context": "1: The problem is posed as an inference process in a <mark>probabilistic graphical model</mark>. We show applications of this approach to identifying saliency in images and video, for detecting suspicious behaviors and for automatic visual inspection for quality assurance.<br>2: The relation between all the above-mentioned variables is depicted in the Bayesian network in Fig. 4. Thus, for an observed ensemble y and a hidden database ensemble x, we can factor the joint likelihood Figure 4. The <mark>probabilistic graphical model</mark>. The Bayesian dependencies are illustrated using the arrows between variables.<br>",
    "Arabic": "النموذج الرسومي الاحتمالي",
    "Chinese": "概率图模型",
    "French": "modèle graphique probabiliste",
    "Japanese": "確率的グラフィカルモデル",
    "Russian": "вероятностная графическая модель"
  },
  {
    "English": "probabilistic inference",
    "context": "1: Conditional independencies often lead to a more concise representation and facilitate efficient algorithms for parameter estimation and <mark>probabilistic inference</mark>. It is wellknown, for instance, that probabilistic graphical models with a tree structure admit efficient inference. In addition to conditional independencies, modern inference algorithms exploit contextual independencies (Boutilier et al. 1996) to speed up <mark>probabilistic inference</mark>.<br>2: Definition 5), the complexity of MPE and marginal inference is polynomial in |X|. While the decomposition into independent components is a well-understood concept, the combination with finite exchangeability has not been previously investigated as a statistical property that facilitates tractable <mark>probabilistic inference</mark>. We can now prove the following result. Theorem 4.<br>",
    "Arabic": "الاستدلال الاحتمالي",
    "Chinese": "概率推断",
    "French": "inférence probabiliste",
    "Japanese": "確率的推論",
    "Russian": "вероятностный вывод"
  },
  {
    "English": "probabilistic logic",
    "context": "1: ∇ θ J(θ) = E π θ [ ∞ t=0 Ψ t ∇ θ log π θ (s t , a t )](2) \n where Ψ t is an empirical expectation of the return . The expected value is usually computed using Monte Carlo methods, requiring efficient sampling from π θ . 3 Probabilistic Logic Shields<br>2: An agent may wish to induce tentative conclusions from this sparse and uncertain data of changing integrity. Percepts are transformed by inference rules into statements in <mark>probabilistic logic</mark> as described above. Information-based agents may employ entropy-based logic [6] to induce complete probability distributions from those statements.<br>",
    "Arabic": "المنطق الاحتمالي",
    "Chinese": "概率逻辑",
    "French": "logique probabiliste",
    "Japanese": "確率論理",
    "Russian": "вероятностная логика"
  },
  {
    "English": "probabilistic method",
    "context": "1: Naive Bayes is a <mark>probabilistic method</mark> that has a long history in information retrieval and text classification (Maron and Kuhns, 1960). It stores as its concept description the prior probability of each class, P(C i ), and the conditional probability of each attribute value given the class, P(v j |C i ).<br>",
    "Arabic": "طريقة احتمالية",
    "Chinese": "概率方法",
    "French": "méthode probabiliste",
    "Japanese": "確率的手法",
    "Russian": "вероятностный метод"
  },
  {
    "English": "probabilistic model",
    "context": "1: under the <mark>probabilistic model</mark> . In many instances, the statistics of interest Ẽ S,A [F(S, A)]) for the gradient (Theorem 2) are only known approximately as they are obtained from small sample sets.<br>2: if two random walks had both moved very little between the two instances in time, our confidence that ∆t ≈ 0 would be greater). From such considerations it is evident that, on the basis of multiple stochastic processes, one can build up a <mark>probabilistic model</mark> for ∆t.<br>",
    "Arabic": "نموذج احتمالي",
    "Chinese": "概率模型",
    "French": "modèle probabiliste",
    "Japanese": "確率モデル",
    "Russian": "вероятностная модель"
  },
  {
    "English": "probabilistic relational model",
    "context": "1: One obvious extension is to incorporate information about time of day and the day of the week into the model, which we expect to greatly enhance predictive power. We furthermore plan to use <mark>probabilistic relational models</mark> (Getoor et al. 2001) to better represent and learn different types of locations.<br>",
    "Arabic": "نموذج علائقي احتمالي",
    "Chinese": "概率关系模型",
    "French": "modèle relationnel probabiliste",
    "Japanese": "確率的関係モデル",
    "Russian": "вероятностная реляционная модель"
  },
  {
    "English": "probabilistic representation",
    "context": "1: One account of this phenomenon, surprisal theory (Hale, 2001;Levy, 2008), suggests that readers maintain a <mark>probabilistic representation</mark> of all possible parses of the input as they process the sentence incrementally.<br>",
    "Arabic": "تمثيل احتمالي",
    "Chinese": "概率表示",
    "French": "représentation probabiliste",
    "Japanese": "確率的表現",
    "Russian": "вероятностное представление"
  },
  {
    "English": "probabilistic semantic",
    "context": "1: 2 (right). By explicitly connecting action safety to <mark>probabilistic semantics</mark>, probabilistic shields provides a realistic and principled way to balance return and safety.<br>2: To implement a rejection-based shield, the agent must keep sampling from the base policy until an action a is accepted. This approach implicitly conditions through very inefficient rejection sampling schemes as below, which has an unclear link to <mark>probabilistic semantics</mark> [Robert et al., 1999].<br>",
    "Arabic": "دلالات احتمالية",
    "Chinese": "概率语义",
    "French": "sémantique probabiliste",
    "Japanese": "確率的意味論",
    "Russian": "вероятностная семантика"
  },
  {
    "English": "probabilistic topic modeling",
    "context": "1: For example, research in <mark>probabilistic topic modeling</mark>-the application we will focus on in this paper-revolves around fitting complex hierarchical Bayesian models to large collections of documents. In a topic model, the posterior distribution reveals latent semantic structure that can be used for many applications.<br>2: Our approach combines ideas from collaborative filtering based on latent factor models [17,18,13,1,22] and content analysis based on <mark>probabilistic topic modeling</mark> [7,8,20,2]. Like latent factor models, our algorithm uses information from other users' libraries. For a particular user, it can recommend articles from other users who liked similar articles.<br>",
    "Arabic": "نمذجة المواضيع الاحتمالية",
    "Chinese": "概率主题模型",
    "French": "modélisation thématique probabiliste",
    "Japanese": "確率的トピックモデリング",
    "Russian": "вероятностное тематическое моделирование"
  },
  {
    "English": "probabilistic tree",
    "context": "1: [2020] proposed a principled way to get conditional inference in <mark>probabilistic trees</mark> by introducing a new activation function; this allows for routing samples through small parts of the tree similar to classical decision trees.<br>2: One might argue that <mark>probabilistic trees</mark> are harder to interpret and suffer from slower inference as a sample must follow each root-leaf path, lacking conditional computation present in classical decision trees. However, Hazimeh et al.<br>",
    "Arabic": "شجرة احتمالية",
    "Chinese": "概率树",
    "French": "arbre probabiliste",
    "Japanese": "確率的木",
    "Russian": "вероятностное дерево"
  },
  {
    "English": "probability density",
    "context": "1: For any compactly supported <mark>probability densities</mark> p, q ∈ L p d where \n<br>2: We consider probability measures over M that are represented by strictly positive continuous density functions µ, ν : M → R >0 , where µ by convention represents the target (unknown) distribution and ν represents the source (prior) distribution. µ, ν are <mark>probability densities</mark> in the sense their integral w.r.t.<br>",
    "Arabic": "كثافة الاحتمال",
    "Chinese": "概率密度",
    "French": "densité de probabilité",
    "Japanese": "確率密度",
    "Russian": "плотность вероятности"
  },
  {
    "English": "probability density function",
    "context": "1: knowing that minimising the Bregman divergence between the mean of the distribution and its natural statistics maximises the log likelihood of the distribution under this <mark>probability density function</mark>.<br>2: In this section, we introduce a GPU-friendly efficient Monte Carlo approach to the integration in the proposed loss function, based on the Adaptive Multiple Importance Sampling (AMIS) algorithm [14]. Considering q(y) to be the <mark>probability density function</mark> of a proposal distribution that approximates the shape of the \n<br>",
    "Arabic": "دالة الكثافة الاحتمالية",
    "Chinese": "概率密度函数",
    "French": "fonction de densité de probabilité",
    "Japanese": "確率密度関数",
    "Russian": "функция плотности вероятности"
  },
  {
    "English": "probability distribution",
    "context": "1: The definition of EFCE requires the following notion of trigger agent, which, intuitively, is associated to each player and each of her sequences of action recommendations. Definition 1 (Trigger agent for EFCE). Given a player i ∈ P , a sequence σ = ( I , a ) ∈ Σ i , and a <mark>probability distribution</mark>μ i ∈ ∆ Πi ( I ) , an ( σ , μ i ) -trigger agent for player i is an agent that takes on the role of player i and commits to following all recommendations unless she reaches<br>2: Definition 1 (Topic Model) A topic model θ in a text collection C is a <mark>probability distribution</mark> of words {p(w|θ)}w∈V where V is a vocabulary set. Clearly, we have w∈V p(w|θ) = 1.<br>",
    "Arabic": "توزيع احتمالي",
    "Chinese": "概率分布",
    "French": "distribution de probabilité",
    "Japanese": "確率分布",
    "Russian": "распределение вероятностей"
  },
  {
    "English": "probability flow",
    "context": "1: 0 ) dp ref ( x t ) dt . Let's consider the <mark>probability flow</mark> φ associated with the reverse diffusion (4)-given by dY t = {−b(Y t ) + ∇ log p T −t (Y t )}dt + dB M t -i.e. the solution of the following ODE (see App.<br>",
    "Arabic": "تدفق الاحتمال",
    "Chinese": "概率流",
    "French": "flux de probabilité",
    "Japanese": "確率フロー",
    "Russian": "поток вероятности"
  },
  {
    "English": "probability map",
    "context": "1: The IoU prediction module of our model is used to select confident masks; moreover, we identified and selected only stable masks (we consider a mask stable if thresholding the <mark>probability map</mark> at 0.5 − δ and 0.5 + δ results in similar masks).<br>",
    "Arabic": "خريطة الاحتمالات",
    "Chinese": "概率地图",
    "French": "carte de probabilité",
    "Japanese": "確率マップ",
    "Russian": "карта вероятностей"
  },
  {
    "English": "probability mass",
    "context": "1: As a result, PLMs are likely to be handicapped in assigning sufficient <mark>probability mass</mark> to the desired family of continuations, given minimal prompts without any particular task-specific context.<br>2: Thus finding the best competitive natural estimator is same as finding the best estimator for the combined <mark>probability mass</mark> S. [13] proposed an algorithm for estimating S such that for all k and for all p ∈ ∆ k , with \n probability ≥ 1 − 1/n , D(S||Ŝ) =Õ n 1 √ n .<br>",
    "Arabic": "الكتلة الاحتمالية",
    "Chinese": "概率质量",
    "French": "masse de probabilité",
    "Japanese": "確率質量",
    "Russian": "вероятностная масса"
  },
  {
    "English": "probability mass function",
    "context": "1: by repeatedly sub-sampling n indices without replacement from a multinomial distribution, whose <mark>probability mass function</mark> p(i) is defined by the corresponding weights: \n p(i) = w 2D i 1 N i=1 w 2D i 1 . (22 \n ) \n<br>2: {γ i (x i ) | x i ∈ X i , 1 ≤ i ≤ n} i.i.d. ∼ Gumbel(−c). Let q avg (x) := P[x = x * ] be the <mark>probability mass function</mark> of x * .<br>",
    "Arabic": "دالة الكتلة الاحتمالية",
    "Chinese": "概率质量函数",
    "French": "fonction de masse de probabilité",
    "Japanese": "確率質量関数",
    "Russian": "функция массы вероятностей"
  },
  {
    "English": "probability matrix",
    "context": "1: The same set of K semantic measurement operators {|v k } K k=1 are applied to both sets, producing a pair of kby-L <mark>probability matrix</mark> p 1 and p 2 , where \n p 1 jk = v k | ρ 1j |v k and p 2 jk = v k | ρ 2j |v k for k ∈ {1, .<br>2: Informally, a graphon can be thought arXiv:2202.07179v2 [cs.LG] 16 Feb 2022 of as a <mark>probability matrix</mark> (e.g., the matrix W G and W H in Figure 1), where W (i, j) represents the probability of edge between node i and j. The real-world graphs can be regraded as generated from graphons.<br>",
    "Arabic": "مصفوفة الاحتمالات",
    "Chinese": "概率矩阵",
    "French": "matrice de probabilité",
    "Japanese": "確率行列",
    "Russian": "вероятностная матрица"
  },
  {
    "English": "probability measure",
    "context": "1: T : Ω × F Ω → [0, 1] such that for each θ ∈ F Ω , T (θ, • \n ) is a <mark>probability measure</mark> on Ω that describes the distribution of where θ moves, and for each A ∈ F Ω , T (•, A) is integrable.<br>2: of the Renyi divergence D α ( P ||Q ) . We first recall a standard fact about covering with projections. Lemma 7.4 (Corollary 3.7 in Haussler and Welzl [25]). Let F be a function class consisting of functions from X to [0, 1] and let P be a <mark>probability measure</mark> on X .<br>",
    "Arabic": "مقياس الاحتمال",
    "Chinese": "概率度量",
    "French": "mesure de probabilité",
    "Japanese": "確率測度",
    "Russian": "вероятностная мера"
  },
  {
    "English": "probability model",
    "context": "1: This imbalance is inevitable in a <mark>probability model</mark> that strongly generates sentences, and it causes naive beam-searchers to get bogged down, proposing more and more phrase structure rather than moving on through the sentence. To address it,  propose a word-synchronous variant of beam search.<br>2: The decoding objective for text generation aims to find the most-probable hypothesis among all candidate hypotheses, i.e. we aim to solve the following optimization problem: \n y = argmax y∈Y log p θ (y | x) \n (3) This is commonly known as maximum a posteriori (MAP) decoding since p θ is a <mark>probability model</mark>.<br>",
    "Arabic": "نموذج احتمالي",
    "Chinese": "概率模型",
    "French": "modèle de probabilité",
    "Japanese": "確率モデル",
    "Russian": "вероятностная модель"
  },
  {
    "English": "probability multiset",
    "context": "1: The red line is the regret of the estimator designed with prior knowledge of the <mark>probability multiset</mark>.<br>2: A distribution property is a mapping f : ∆ → R. It is symmetric if it remains unchanged under relabeling of domain symbols, namely if it is determined by just the <mark>probability multiset</mark> {p 1 , p 2 , . . . ,p k }. Many important properties are symmetric.<br>",
    "Arabic": "المجموعة الاحتمالية المتعددة",
    "Chinese": "概率多重集",
    "French": "multiensemble de probabilités",
    "Japanese": "確率マルチセット",
    "Russian": "мультимножество вероятностей"
  },
  {
    "English": "probability simplex",
    "context": "1: Fact 2.1 also establishes that (D, {ℓ} , ∆(H)) has convex and 1-smooth losses, where smoothness is measured in the infinity norm. Since ∆(H) is a <mark>probability simplex</mark>, we also have that its diameter is at most 2 in the 1-norm.<br>2: The infimum problem ( 47) is equivalent to projecting the vector v(λ) ∈ R n defined by \n v i = 1 n − 1 λ z i \n onto the <mark>probability simplex</mark>.<br>",
    "Arabic": "البسيط الاحتمالي",
    "Chinese": "概率单纯形",
    "French": "simplexe de probabilité",
    "Japanese": "確率シンプレックス",
    "Russian": "вероятностный симплекс"
  },
  {
    "English": "probability space",
    "context": "1: All <mark>probability space</mark>s in this paper are discrete and finite. A probability distribution p on a finite <mark>probability space</mark> U is a function p : U → [0, 1] s.t. x∈U p(x) = 1. The support of p is defined as: \n<br>2: Yet, other statistical distance measures, like the Kullback-Leibler divergence could have been used as well. The following is a standard characterization of the total variation distance: \n Lemma 2. Let p, q be two distributions on the same <mark>probability space</mark> U. Then, \n<br>",
    "Arabic": "فضاء الاحتمال",
    "Chinese": "概率空间",
    "French": "espace probabiliste",
    "Japanese": "確率空間",
    "Russian": "вероятностное пространство"
  },
  {
    "English": "probability threshold",
    "context": "1: Similar to a significance test, given a user-specified <mark>probability threshold</mark> t we can define a critical term proportion value under Γ w \n Pr[Γ w ≤ f * w ] ≤ 1 − t.(4) \n<br>",
    "Arabic": "عتبة الاحتمال",
    "Chinese": "概率阈值",
    "French": "seuil de probabilité",
    "Japanese": "確率閾値",
    "Russian": "порог вероятности"
  },
  {
    "English": "probability transition matrix",
    "context": "1: As introduced in Section 3, discrete diffusion models rely on the <mark>probability transition matrix</mark> Q t to perform the forward and denoising processes over the state space. To align DDM with the NAR decoding process of BART (Section 4.2), we incorporate the [MASK] token as the absorbing state of the Markov transition matrices. Concretely , at the t-th step of the forward process , if token i is not the [ MASK ] token , it has the probabilities of α t and γ t being unchanged and replaced by the [ MASK ] token respectively , leaving the probability of β t = 1 − α t − γ t transiting to other tokens in V<br>2: random walk) on a finite state space U is a stochastic process in which states of U are visited successively. The Markov Chain is specified by a |U | × |U | <mark>probability transition matrix</mark> P .<br>",
    "Arabic": "مصفوفة الانتقال الاحتمالية",
    "Chinese": "概率转移矩阵",
    "French": "matrice de transition de probabilité",
    "Japanese": "確率遷移行列",
    "Russian": "матрица вероятности перехода"
  },
  {
    "English": "probability vector",
    "context": "1: CNM-Global-Mixture adopts a global mixture of the whole sentence, in which a sentence is represented as a single density matrix, leading to a <mark>probability vector</mark> for the measurement result. CNM-trace-inner-product replaces the trainable measurements with trace inner product like NNQLM.<br>2: Geographic features were produced using a rule-based geographic annotation tool that outputs a <mark>probability vector</mark> for a set of geographic entities possibly appearing in the query. We focused on the following geographic entities : airport , colloquial ( i.e. , location information associated with a named entity , such as `` North Shore Bank '' ) , continent , country , county , estate , historical county , historical state , historical town , island , land feature , point of interest ( e.g , Eiffel Tower ) , sports<br>",
    "Arabic": "متجه الاحتمال",
    "Chinese": "概率向量",
    "French": "vecteur de probabilité",
    "Japanese": "確率ベクトル",
    "Russian": "вектор вероятностей"
  },
  {
    "English": "probe classifier",
    "context": "1: achieves an accuracy of 88.5 % , a number that puts the reported <mark>probing classifier</mark> accuracy of 96.9 % into a bit more context . Further, Li et al. (2021), also presented an experiment where they manipulated specific entity representations of a synthetic version of the Alchemy dataset.<br>2: In the second set of <mark>probing classifier</mark> experiments, Li et al. (2021) used data generated using the TextWorld engine (Côté et al., 2019).<br>",
    "Arabic": "مُصنِّف الاستقصاء",
    "Chinese": "探针分类器",
    "French": "classificateur de sonde",
    "Japanese": "プローブ分類器",
    "Russian": "классификатор зондирования"
  },
  {
    "English": "problem space",
    "context": "1: Here the <mark>problem space</mark> is P = R 2 10 the solution space is S = R 9 , π(x, λ) = x, and our problem-solution manifold M = M 5pt is the set of smooth points in the semialgebraic set im(Ψ 5pt ) ∩ (R >0 ) 9 × R 2 10 .<br>2: However, this practice does raise potential concerns regarding the extent to which datasets are appropriately aligned with a given <mark>problem space</mark>. Moreover, given the widespread prevalence of systematic biases in the most prominent ML datasets, adopting existing datasets, rather than investing in careful curation of new datasets, risks further entrenching existing biases.<br>",
    "Arabic": "فضاء المشكلة",
    "Chinese": "问题空间",
    "French": "espace de problème",
    "Japanese": "問題空間",
    "Russian": "проблемное пространство"
  },
  {
    "English": "product distribution",
    "context": "1: Since u i (s) ≤ 1, definitions imply: u t i − u t−1 i * ≤ s−i j =i w t j,sj − j =i w t−1 j,sj . The latter is the total variation distance of two <mark>product distributions</mark>. By known properties of total variation (see e.g.<br>",
    "Arabic": "توزيع المنتج",
    "Chinese": "乘积分布",
    "French": "distribution de produits",
    "Japanese": "生成物分布",
    "Russian": "Распределение произведения"
  },
  {
    "English": "product-of-expert",
    "context": "1: To combine the predictions of the active contexts at some node n, we take their renormalized product, as an instance of <mark>product-of-experts</mark> [Hinton, 2002]: \n<br>2: where the learned mixture weights p(t | r) form a distribution over the soft prompts t ∈ T r . Ensembling techniques other than mixture-of-experts could also be used, including <mark>product-of-experts</mark> (Jiang et al., 2020).<br>",
    "Arabic": "منتج الخبراء",
    "Chinese": "专家乘积",
    "French": "produit d'experts",
    "Japanese": "専門家の積",
    "Russian": "продукт-экспертов"
  },
  {
    "English": "program induction",
    "context": "1: In this paper, our VisKoP proposes a different solution to this task by integrating humans into the <mark>program induction</mark> loop, providing external human guidance to <mark>program induction</mark> model, and potentially improving the system robustness.<br>2: During inference, ArcaneQA assumes an entity linker to identify a set of entities from the question at the beginning of <mark>program induction</mark>. However, the entity linker may identify false entities. To deal with it, ArcaneQA initiate its decoding process with different hypotheses from the set of entities.<br>",
    "Arabic": "استنتاج البرنامج",
    "Chinese": "程序归纳",
    "French": "induction de programme",
    "Japanese": "プログラム誘導",
    "Russian": "индукция программ"
  },
  {
    "English": "projection algorithm",
    "context": "1: Such an algorithm must be constructed from scratch for each model, and the techniques that have been used for various models so far are quite different. Our contribution can be seen as complementing the framework of (Baraniuk et al., 2010) with a nearly-linear time <mark>projection algorithm</mark> that is applicable to a wide range of sparsity structures.<br>",
    "Arabic": "خوارزمية الإسقاط",
    "Chinese": "投影算法",
    "French": "algorithme de projection",
    "Japanese": "射影アルゴリズム",
    "Russian": "алгоритм проекции"
  },
  {
    "English": "projection layer",
    "context": "1: Our models have the following number of parameters: Both LUKE and FEVRY use a bert-base-uncased transformer model (110M parameters), a <mark>projection layer</mark> (768 × 200 ≈ 153k parameters) and the entity embedding layer (200 × 825k ≈ 165M parameters), and thus have about 274M parameters in total.<br>",
    "Arabic": "طبقة الإسقاط",
    "Chinese": "投影层",
    "French": "couche de projection",
    "Japanese": "射影層",
    "Russian": "слой проекции"
  },
  {
    "English": "projection matrix",
    "context": "1: The Monarch version of these models simply swap out the dense weight matrices in the attention blocks (<mark>projection matrices</mark>) and in the FFN block (linear layers) with Monarch matrices. We set the number of blocks in the block-diagonal matrices to 4. We also reduce the regularization strength (dropout) as our model is smaller.<br>2: The images are taken by cameras in different positions represented by 3 × 4 <mark>projection matrices</mark> P 1 to P n , which are supplied. Figure 2 summarizes the situation.<br>",
    "Arabic": "مصفوفة الإسقاط",
    "Chinese": "投影矩阵",
    "French": "matrice de projection",
    "Japanese": "投影行列",
    "Russian": "проекционная матрица"
  },
  {
    "English": "projection operator",
    "context": "1: We evaluate our model on the earth and climate datasets gathered in Mathieu and Nickel (2020). The <mark>projection operator</mark> π in this case is simply π(x) = x x . We parameterize v θ as an MLP with 6 hidden layers of 512 neurons each.<br>2: To understand the dynamics of the singular values of Ω-which is connected to the dynamics of the features through Lemma 1-we will analyze the gradient flow on this manifold. To this end, we first identify its tangent space at a particular X as well as the <mark>projection operator</mark> onto that tangent space.<br>",
    "Arabic": "مشغل الإسقاط",
    "Chinese": "投影算子",
    "French": "opérateur de projection",
    "Japanese": "射影演算子",
    "Russian": "проекционный оператор"
  },
  {
    "English": "projection step",
    "context": "1: 3(b)) we allow for consecutive conditioning steps and we see that that the <mark>projection step</mark> is fundamental, especially when mixing events are rare, reducing the error dramatically. Comparing running times, it is clear that our algorithm scales gracefully compared to the exact solution (Fig. 3(c)).<br>",
    "Arabic": "خطوة الإسقاط",
    "Chinese": "投影步骤",
    "French": "étape de projection",
    "Japanese": "投影ステップ",
    "Russian": "шаг проекции"
  },
  {
    "English": "projective camera",
    "context": "1: Consider a scene point in the threedimensional projective space P 3 represented by a fourdimensional vector X containing the homogeneous coordinates of the point. A <mark>projective camera</mark> I represented by a 3×4 matrix P I will map the space point onto a point x I ∈ P 2 of the image plane I containing its three homogeneous coordinates.<br>",
    "Arabic": "كاميرا إسقاطية",
    "Chinese": "透视相机",
    "French": "caméra projective",
    "Japanese": "射影カメラ",
    "Russian": "проективная камера"
  },
  {
    "English": "projective dependency parsing",
    "context": "1: It is well known that <mark>projective dependency parsing</mark> using edge based factorization can be handled with the Eisner algorithm (Eisner, 1996). This algorithm has a runtime of O(n 3 ) and has been employed successfully in both generative and discriminative parsing models (Eisner, 1996;McDonald et al., 2005).<br>",
    "Arabic": "تحليل التبعية الإسقاطية",
    "Chinese": "投射依存句法分析",
    "French": "analyse des dépendances projectives",
    "Japanese": "射影依存構文解析",
    "Russian": "проективный анализ синтаксических зависимостей"
  },
  {
    "English": "projective dependency tree",
    "context": "1: Hence, finding a (projective) dependency tree with highest score is equivalent to finding a maximum (projective) spanning tree in G x .<br>",
    "Arabic": "شجرة اعتماد إسقاطية",
    "Chinese": "投影依存树",
    "French": "arbre de dépendance projectif",
    "Japanese": "射影依存木",
    "Russian": "проективное дерево зависимостей"
  },
  {
    "English": "projective parsing",
    "context": "1: Notice that the strategy just described to handle sibling features is not fully compatible with the features proposed by Eisner (1996) for <mark>projective parsing</mark>, as the latter correlate only consecutive siblings and are also able to place special features on the first child of a given word.<br>2: To learn these structures we used online large-margin learning (McDonald et al., 2005) that empirically provides state-of-the-art performance for Czech. A major advantage of our models is the ability to naturally model non-projective parses. Non<mark>projective parsing</mark> is commonly considered more difficult than <mark>projective parsing</mark>.<br>",
    "Arabic": "تحليل انسيابي",
    "Chinese": "投影式分析",
    "French": "analyse projective",
    "Japanese": "射影的構文解析",
    "Russian": "проективный синтаксический анализ"
  },
  {
    "English": "projective transformation",
    "context": "1: A relevant question is whether a viewing graph is solvable, i.e., if it uniquely determines a projective configura-Figure 1: Viewing graphs with eight vertices that were left undecided in [37] and that we determined to be solvable. tion of cameras, up to a single <mark>projective transformation</mark>.<br>2: Conversely, the fundamental matrix of edge pi, jq uniquely determines the cameras of vertices i and j, up to a <mark>projective transformation</mark> [15]. In the following, we shall use uppercase letters to denote matrices, lowercase bold letters to denote vectors and lowercase letters to denote scalars 1 .<br>",
    "Arabic": "تحويل إسقاطي",
    "Chinese": "射影变换",
    "French": "transformation projective",
    "Japanese": "射影変換",
    "Russian": "проективное преобразование"
  },
  {
    "English": "prompt",
    "context": "1: We carefully evaluate our method with other approaches and the experimental results extensively demonstrate our advantages. Contributions: \n • We unify the <mark>prompt</mark> format in the language area and graph area, and further propose an effective graph <mark>prompt</mark> for multitask settings (section 3.3).<br>2: In our case, given a <mark>prompt</mark> ψ, a LM will generate a sequence of T tokens X = [x t ] for t ∈ [1 : T ] where x t is given by: \n x t ∼ argmax xt Pr(x t ) = LM(x 1:t−1 |ψ) . (1) \n<br>",
    "Arabic": "تعليمة",
    "Chinese": "提示",
    "French": "\"invite\"",
    "Japanese": "プロンプト",
    "Russian": "промпт"
  },
  {
    "English": "prompt engineering",
    "context": "1: • GPT-4 is much easier to be misled by jailbreaking prompts to misrecognize immoral actions, which is potentially due to the reason that GPT-4 follows instructions more precisely than GPT-3.5 and thus more vulnerable to malicious <mark>prompt engineering</mark>.<br>2: Next, we show comparisons of recontextualization of a subject clock, with distinctive features using our method and <mark>prompt engineering</mark> using vanilla Imagen [61] and the public API of DALL-E 2 [54].<br>",
    "Arabic": "هندسة النداءات",
    "Chinese": "提示工程",
    "French": "ingénierie de prompt",
    "Japanese": "プロンプトエンジニアリング",
    "Russian": "инженерия подсказок"
  },
  {
    "English": "prompt learning",
    "context": "1: Inspired by the <mark>prompt learning</mark> in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pretrained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models.<br>2: 2021b) takes the features as prompts and dynamically learns the prompt to guide the model. Our method is similar to <mark>prompt learning</mark>. However, unlike the previous works, our proposed method is text-modalfree. Moreover, we give additional thought to the problem of a sequence of domain shifts.<br>",
    "Arabic": "تعلم التلميحات",
    "Chinese": "提示学习",
    "French": "apprentissage par amorce",
    "Japanese": "プロンプト学習",
    "Russian": "обучение по подсказке"
  },
  {
    "English": "prompt tuning",
    "context": "1: As shown in Figure 1, <mark>prompt tuning</mark> in the graph domain is to seek some light-weighted prompt, keep the pre-training model frozen, and use the prompt to reformulate downstream tasks in line with the pre-training task. In this way, the pre-trained model can be easily applied to downstream applications with highly efficient fine-tuning or even without any fine-tuning.<br>2: Prompt tuning (Liu et al., 2021;Li and Liang, 2021;Sanh et al., 2022) aims to learn a task-specific prompt by reformulating the downstream tasks to the format that the model was initially trained on and has shown competitive performance across various natural language processing applications.<br>",
    "Arabic": "ضبط التعليمة البادئة",
    "Chinese": "提示调优",
    "French": "réglage du prompt",
    "Japanese": "プロンプトチューニング",
    "Russian": "настройка подсказок"
  },
  {
    "English": "pronoun resolution",
    "context": "1: The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted <mark>pronoun resolution</mark> problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations.<br>2: \"), then measure the probability of the model completing the sentence \"'{Pronoun}' refers to the\" with different sentence roles (\"librarian\" and \"child\" in this example). Each example is annotated with the correct <mark>pronoun resolution</mark> (the pronoun corresponds to the librarian in this example).<br>",
    "Arabic": "حل الضمير",
    "Chinese": "代词消解",
    "French": "résolution de pronom",
    "Japanese": "代名詞解決",
    "Russian": "разрешение местоимений"
  },
  {
    "English": "proof complexity",
    "context": "1: 1979 ) , which is one of the strongest proof systems studied in <mark>proof complexity</mark> . However, there are indications that the cutting planes proof system equipped with the dominance-based strengthening rule might be strictly stronger than extended Frege (Ko lodziejczyk & Thapen, 2023).<br>2: With random XOR streamlining with good parameters, we were able to solve the instance using MiniSAT by adding 27 XOR constraints containing an average of 9 variables. In the clique coloring problem with parameters n , m , and k , the task is to construct a graph on n nodes such that it can be colored with m colors and also contains a clique of size k. This problem has interesting properties that make it very useful in <mark>proof complexity</mark> research on exponential lower bounds for powerful proof systems (<br>",
    "Arabic": "التعقيد الإثباتي",
    "Chinese": "证明复杂性",
    "French": "complexité de la preuve",
    "Japanese": "証明複雑性",
    "Russian": "доказательная сложность"
  },
  {
    "English": "proof number",
    "context": "1: A likely win is assigned a <mark>proof number</mark> of 0 for the is-likely-win question, a likely loss is assigned a <mark>proof number</mark> of 0 for the is-likelyloss question, and a draw is assigned a dis<mark>proof number</mark> of 0 for both questions.<br>2: Clearly, the (dis)<mark>proof number</mark> of a (dis)proved node is 0; by convention the <mark>proof number</mark> of a disproved node and the dis<mark>proof number</mark> of a proved node is defined to be ∞. For any explored node v , PNS maintains estimates pn ( v ) and dn ( v ) for the proof and dis<mark>proof number</mark>s , respectively , of v. For any unexpanded node v ∈ V ( in particular , v 0 at initialization ) , PNS sets pn ( v ) = 1 and dn ( v ) = 1 in accordance with<br>",
    "Arabic": "رقم البرهان",
    "Chinese": "证明数",
    "French": "nombre de preuve",
    "Japanese": "証明数",
    "Russian": "число доказательства"
  },
  {
    "English": "proof tree",
    "context": "1: Only non-proven positions with heuristic values inside the new threshold are now considered, and they must be re-searched with the new heuristic limit. This iterative process continues until the heuristic threshold exceeds the largest possible heuristic value. At this point all likely wins/losses have been resolved, and the <mark>proof tree</mark> is complete.<br>2: Draws are much harder to prove than wins/losses. This evidence strongly suggests that the databases are removing hundreds of ply from the search depth of the <mark>proof tree</mark>.<br>",
    "Arabic": "شجرة الإثبات",
    "Chinese": "证明树",
    "French": "arbre de preuve",
    "Japanese": "証明木",
    "Russian": "Дерево доказательства"
  },
  {
    "English": "propensity score",
    "context": "1: The later is usually known as the \"inverse probability-of-selection weight (IPSW)\" (Cole and Stuart 2010), and, in practice, is estimated by assuming some parametric model such as logistic regression. Given observed data { ( X i , Y i , Z i ) } n i=1 under selection bias ( from P ( v | S=1 ) ) , assume we could obtain reliable estimate of the <mark>propensity score</mark> P ( x | z , S=1 ) and the inverse probability-of-selection P ( S=1 ) /P ( S=1 | z T ) from<br>2: As one can see from this table, weighing by the inverse <mark>propensity score</mark> is effective, as all the weighted metrics are significantly improved. We note a lower performance for the other rewards metrics, explain by the fact that head labels get less often chosen by the policy.<br>",
    "Arabic": "درجة الميل",
    "Chinese": "倾向评分",
    "French": "score de propension",
    "Japanese": "傾向スコア",
    "Russian": "оценка склонности"
  },
  {
    "English": "proposal distribution",
    "context": "1: Generally, we do not have access to the posterior in a closed-form, so we have to use approximations to the posterior in place of the <mark>proposal distribution</mark> q(w), retaining a high variance of the LML estimate. Multiple approaches that aim to reduce the variance of the sampling-based estimates of the marginal likelihood have been developed.<br>2: If we assume that the <mark>proposal distribution</mark> is symmetric, with the probability of proposing a new state x * from the current state x being the same as the probability of proposing x from x * , we can use the Barker acceptance function [8], giving \n<br>",
    "Arabic": "توزيع الاقتراحات",
    "Chinese": "建议分布",
    "French": "distribution de proposition",
    "Japanese": "提案分布",
    "Russian": "распределение предложений"
  },
  {
    "English": "proposal probability",
    "context": "1: (8), which proceeds in two stages. First, it proposes a reconfiguration of the graph by sampling from a <mark>proposal probability</mark>. Then it accepts or rejects this reconfiguration by sampling the acceptance probability. To summarize, we outline the control strategy of the algorithm below.<br>2: If the temperature τ is too low, this proposal will aggressively optimize the local likelihood increase from x → x , but possibly collapse the reverse <mark>proposal probability</mark> q τ (x|x ). If the temperature τ is too high, this proposal may increase the reverse <mark>proposal probability</mark> q τ (x|x ), but ignore the local likelihood increase.<br>",
    "Arabic": "احتمال الاقتراح",
    "Chinese": "建议概率",
    "French": "probabilité de proposition",
    "Japanese": "提案確率",
    "Russian": "предложение вероятности"
  },
  {
    "English": "proposition",
    "context": "1: 4 QED While the algorithm implied by the proof of <mark>Proposition</mark> 1 is very simple, we are not aware of prior work that uses it for reconstructing specular scenes. 5<br>2: We use the same notation to refer to the preceding neighbors in the analogously defined state in Algorithm 1. In fact, as P out (H) is independent of the permutation π(K) (<mark>Proposition</mark> 1), it will be the same set.<br>",
    "Arabic": "مقولة",
    "Chinese": "命题",
    "French": "proposition",
    "Japanese": "命題",
    "Russian": "предложение"
  },
  {
    "English": "propositional",
    "context": "1: Inoue (1992) considered, in the <mark>propositional</mark> and the firstorder context, generating explanations and prime implicates using SOL-resolution. He proposed a strategy which processes, starting from the empty set, clauses from a theory incrementally. However, due to possible large intermediate results, this method is not total polynomial time in general. Khardon et al.<br>",
    "Arabic": "الاقتراحية",
    "Chinese": "命题的",
    "French": "logique propositionnelle",
    "Japanese": "命題的",
    "Russian": "пропозициональный"
  },
  {
    "English": "propositional formula",
    "context": "1: A literal is a (propositional) atom or the negation of an atom. A (propositional) formula is formed from literals using propositional connectives. A clause is a finite set of literals. We identify a clause C with the disjunction of its elements.<br>2: To do this, the approach that we call T-SAFE follows a similar idea as T-PERFECT, but considers unsolvable states s ∈ T + in isolation, looking for a <mark>propositional formula</mark> φ s over features in F that distinguishes s from all solvable states in T − and has minimum complexity.<br>",
    "Arabic": "صيغة اقتراحية",
    "Chinese": "命题公式",
    "French": "formule propositionnelle",
    "Japanese": "命題論理式",
    "Russian": "пропозициональная формула"
  },
  {
    "English": "propositional language",
    "context": "1: Information manipulated by a cognitive agent must be represented symbolically. To develop our theoretical framework, we adopt perhaps the simplest symbolic representation, in the form of a classical <mark>propositional language</mark>.<br>",
    "Arabic": "لغة اقتراحية",
    "Chinese": "命题语言",
    "French": "langage propositionnel",
    "Japanese": "命題言語",
    "Russian": "пропозициональный язык"
  },
  {
    "English": "propositional logic",
    "context": "1: We can put all this in a natural-deduction style proof annotated with justifications for each step. Here PL and K45 refer respectively to reasoning using the <mark>propositional logic</mark> and K45 (for K and N, including mutual introspection).<br>2: As mentioned in the introduction, the idea of \"completion + loop formulas\" has been applied to logic programs in (Lin & Zhao 2002;Lee & Lifschitz 2003) and to McCain-Turner causal logic in (Lee 2004). The characterizations of these nonmonotonic logics in terms of <mark>propositional logic</mark> are useful tools for comparing the formalisms.<br>",
    "Arabic": "منطق القضايا",
    "Chinese": "命题逻辑",
    "French": "logique propositionnelle",
    "Japanese": "命題論理",
    "Russian": "логика высказываний"
  },
  {
    "English": "propositional variable",
    "context": "1: For each such graph G, we introduce a <mark>propositional variable</mark> x G and encode (in a straightforward way) that x G is true iff C 1 , . . . , C n are assigned to the vertices of G in the canonical way.<br>",
    "Arabic": "متغير اقتراحي",
    "Chinese": "命题变量",
    "French": "variable propositionnelle",
    "Japanese": "命題変数",
    "Russian": "пропозициональная переменная"
  },
  {
    "English": "protected attribute",
    "context": "1: Causal analogues of these definitions have recently been proposed (Coston et al., 2020;Imai & Jiang, 2020;Imai et al., 2020;Mishler et al., 2021), which require various conditional independence conditions to hold between the potential outcomes, <mark>protected attributes</mark>, and decisions.<br>2: (4) \n where D(a ) denotes the decision when one's <mark>protected attributes</mark> are counterfactually altered to be any a ∈ A.<br>",
    "Arabic": "سمة محمية",
    "Chinese": "受保护属性",
    "French": "attribut protégé",
    "Japanese": "保護された属性",
    "Russian": "защищенный атрибут"
  },
  {
    "English": "protein folding",
    "context": "1: Blocks were formed by grouping contextually-relevant variables together (e.g., in <mark>protein folding</mark>, we never split up an amino acid). We also compared to ablated versions of RDIS.<br>2: We also chose 12 relatively big energy minimization problems with grid structure and Potts interaction terms. The underlying application is a color segmentation problem previously considered in [39]. Our general approach reproduces results of [39] for the specific Potts model. We considered also side-chain prediction problems in <mark>protein folding</mark> [46].<br>",
    "Arabic": "طي البروتين",
    "Chinese": "蛋白质折叠",
    "French": "repliement des protéines",
    "Japanese": "\"タンパク質の折りたたみ\"",
    "Russian": "сворачивание белков"
  },
  {
    "English": "prototype embedding",
    "context": "1: (6); (ii)-if an example consistently points to one prototype, the pseudo target s can converge (almost) to a one-hot vector with the least ambiguity. Prototype Updating. The most canonical way to update the <mark>prototype embeddings</mark> is to compute it in every iteration of training.<br>",
    "Arabic": "تضمين النموذج الأولي",
    "Chinese": "原型嵌入",
    "French": "Embedding de prototype",
    "Japanese": "プロトタイプ埋め込み",
    "Russian": "встраивание прототипов"
  },
  {
    "English": "proximal operator",
    "context": "1: Note that, the subproblem of the <mark>proximal operator</mark> associated with the convex relaxation in [2] is solved by searching for the dual variable in a different way with time complexity O(d 2 ). In summary, we reformulate the <mark>proximal operator</mark> for the original weak hierarchical Lasso by factorizing the unknown coefficients.<br>2: Another major contribution is that we present an efficient algorithm for computing the <mark>proximal operator</mark> associated with the non-convex weak hierarchical Lasso in Section 3.2. The time complexity of solving each subproblem of the <mark>proximal operator</mark> can be reduced from quadratic to linearithmic. We then summarize our algorithm for computing the <mark>proximal operator</mark> in Section 3.2.<br>",
    "Arabic": "مشغل المتقارب",
    "Chinese": "接近算子",
    "French": "opérateur proximal",
    "Japanese": "近位演算子",
    "Russian": "проксимальный оператор"
  },
  {
    "English": "proximal policy optimization",
    "context": "1: PPO. We use the popular PPO algorithm [102] (<mark>Proximal Policy Optimization</mark>) as our RL training backbone. PPO is an on-policy method that optimizes for a surrogate objective while ensuring that the deviation from the previous policy is relatively small. PPO updates the policy network by \n<br>",
    "Arabic": "تحسين السياسة القريبة",
    "Chinese": "近端策略优化",
    "French": "optimisation de politique proximale",
    "Japanese": "近接方策最適化",
    "Russian": "Проксимальная оптимизация политики"
  },
  {
    "English": "pruning algorithm",
    "context": "1: Nonetheless, we show empirically that bootstrapping is effective in our <mark>pruning algorithm</mark> for modest sizes of R t . Another benefit of our pruning method compared to standard MBR is that it can terminate early if H t has only one remaining hypothesis, reducing the total number of pseudo-references needed.<br>2: Even with our <mark>pruning algorithm</mark>, MBR is many times more costly to run than beam search. More An important hyperparameter in our method is the sample size schedule.<br>",
    "Arabic": "خوارزمية التقليم",
    "Chinese": "剪枝算法",
    "French": "algorithme d'élagage",
    "Japanese": "剪定アルゴリズム",
    "Russian": "алгоритм обрезки"
  },
  {
    "English": "pseudo-inverse",
    "context": "1: Using the triangle inequality, the submultiplicativity of the operator norm, and the properties of the <mark>pseudo-inverse</mark>, we can write \n A a − A a = ( H V ) + ( H a V − H a V ) + ( ( H V ) + − ( H V ) + ) H a V ≤ ( H V ) + H a V − H a V + ( H V ) + − ( H V ) + H a<br>2: Our implementation utilizes a radial basis function network architecture, the bene ts of this decision is that the weights can be trained algebraically (via a <mark>pseudo-inverse</mark>) and each data point is guaranteed to be interpolated exactly.<br>",
    "Arabic": "معكوس كاذب",
    "Chinese": "伪逆",
    "French": "pseudo-inverse",
    "Japanese": "擬似逆",
    "Russian": "псевдообратная"
  },
  {
    "English": "pure strategy",
    "context": "1: In practice, many games are more naturally and compactly represented in extensive form. Unfortunately, learning payoff matrices of their equivalent normal form representation is computationally unfeasible even for small games. For example, one-card poker has 2 26 <mark>pure strategies</mark> per player.<br>",
    "Arabic": "استراتيجية نقية",
    "Chinese": "纯策略",
    "French": "stratégie pure",
    "Japanese": "純粋戦略",
    "Russian": "чистая стратегия"
  },
  {
    "English": "pyramid level",
    "context": "1: We sweep over the number of scale and aspect ratio anchors used at each spatial position and each <mark>pyramid level</mark> in FPN. We consider cases from a single square anchor at each location to 12 anchors per location spanning 4 sub-octave scales (2 k/4 , for k ≤ 3) and 3 aspect ratios [0.5, 1, 2].<br>2: At each <mark>pyramid level</mark> k, let p k be the coordinate of the pixel to match, c k be the offset or centroid of the searching window, and w(p k ) be the best match from BP.<br>",
    "Arabic": "مستوى الهرم",
    "Chinese": "金字塔层级",
    "French": "niveau de la pyramide",
    "Japanese": "ピラミッドレベル",
    "Russian": "уровень пирамиды"
  },
  {
    "English": "q function",
    "context": "1: Third, we use a handcrafted heuristic Q M oGo (s, a). This heuristic was designed such that greedy action selection would produce the best known default policy π M oGo (s, a).<br>2: Since the output of the dueling network is a <mark>Q function</mark>, it can be trained with the many existing algorithms, such as DDQN and SARSA. In addition, it can take advantage of any improvements to these algorithms, including better replay memories, better exploration policies, intrinsic motivation, and so on.<br>",
    "Arabic": "دالة Q",
    "Chinese": "Q函数",
    "French": "fonction Q",
    "Japanese": "Q関数",
    "Russian": "функция Q"
  },
  {
    "English": "q value",
    "context": "1: For example, after training with DDQN on the game of Seaquest, the average action gap (the gap between the <mark>Q values</mark> of the best and the second best action in a given state) across visited states is roughly 0.04, whereas the average state value across those states is about 15.<br>2: The recurrence K was chosen in proportion to the grid-world size, to ensure that information can flow from the goal state to any other state. For the attention module, we chose a trivial approach that selects the<mark>Q values</mark> in the VI block for the current state, i.e., ψ(s) =Q(s, •).<br>",
    "Arabic": "قيمة Q",
    "Chinese": "q值",
    "French": "valeur q",
    "Japanese": "Q値",
    "Russian": "значение q"
  },
  {
    "English": "q-learning",
    "context": "1: We develop a model-based value iteration algorithm and a model-free <mark>Q-learning</mark> algorithm using this backup that carefully integrate valueand policy-based reasoning. These methods complement the value-based nature of value iteration and <mark>Q-learning</mark> with explicit constraints on the policies consistent with generated values, and use the values to select policies from the admissible policy class.<br>2: We experiment with tabular <mark>Q-learning</mark> with -greedy exploration, with = 0.2 and learning rate α = 0.1 and no annealing. Each episode consists of 10 steps in the environment, with 250 episodes per algorithm. We repeat the experiment 50 times and report 95% confidence intervals.<br>",
    "Arabic": "تعلم كيو",
    "Chinese": "Q-学习",
    "French": "q-learning",
    "Japanese": "Q学習",
    "Russian": "q-обучение"
  },
  {
    "English": "q-network",
    "context": "1: Beguš (2021a) proposed ciwGAN (Categorical InfoGAN) which is based on WaveGAN architecture but with an extra <mark>Q-network</mark> that motivates the Generator to produce linguistically categorical and meaningful sounds. Begus and Zhou (2022) shows that ciwGAN can encode allophonic distribution: word-initial pre-vocalic aspiration of voiceless stops ([p h It] v.s.<br>2: However, when we increase the number of actions, the dueling architecture performs better than the traditional <mark>Q-network</mark>. In the dueling network, the stream V (s; θ, β) learns a general value that is shared across many similar actions at s, hence leading to faster convergence. This is a very promising result be- No.<br>",
    "Arabic": "شبكة Q",
    "Chinese": "Q网络",
    "French": "réseau Q",
    "Japanese": "Qネットワーク",
    "Russian": "сеть Q"
  },
  {
    "English": "quadratic assignment problem",
    "context": "1: Models: (2.1) more complicated optimization models (e.g., under uncertainty); (2.2) taking into account \"neighbor\" assignments (possible collisions, influence), here <mark>quadratic assignment problem</mark> [2] or an approach on the basis of hierarchical morphological design [14] can be used.<br>2: However, they deal with it by restricting what permutations are possible to enable tractable dynamic programs. Eisner and Tromble (2006) propose local search methods for decoding permutations for machine translation. Outside of NLP, Kushinsky et al. (2019) have applied Bregman's method to the <mark>quadratic assignment problem</mark>, which Eq.<br>",
    "Arabic": "مشكلة التعيين التربيعي",
    "Chinese": "二次指派问题",
    "French": "problème d'affectation quadratique",
    "Japanese": "二次割り当て問題",
    "Russian": "квадратичная задача о назначениях"
  },
  {
    "English": "quadratic loss",
    "context": "1: The linear regression model was trained with <mark>quadratic loss</mark>; but constrained on the signs of coefficients based on domain knowledge, i.e., all intensity coefficients are non-negative except for ad views and all recency coefficients are non-positive except for ad views.<br>",
    "Arabic": "خسارة تربيعية",
    "Chinese": "二次损失",
    "French": "perte quadratique",
    "Japanese": "二乗損失",
    "Russian": "квадратичная потеря"
  },
  {
    "English": "quadratic program",
    "context": "1: As we have seen earlier, any feasible solution to the primal problem produces a corresponding kernel in (4), and plugging this kernel into the dual problem in (6) allows us to calculate a dual feasible point by solving a <mark>quadratic program</mark> which gives a dual objective value, i.e.<br>2: R = maxi ||xi|| for Algorithm 1 and for Algorithm 2 it is R = 2 maxi ||xi||. Proof. Following the proof scheme in [24,13], we will show that adding each new constraint to W increases the objective value at the solution of the <mark>quadratic program</mark> in Line 4 by at least some constant positive value.<br>",
    "Arabic": "برنامج تربيعي",
    "Chinese": "二次规划",
    "French": "programme quadratique",
    "Japanese": "二次計画問題",
    "Russian": "квадратичная программа"
  },
  {
    "English": "quadratic regularizer",
    "context": "1: During the line search, one can use log(L(N ′ , β)+R(β)) but note that, while still unimodal (quasiconvex), it may not be convex anymore (despite Theorem 14) with a <mark>quadratic regularizer</mark>.<br>",
    "Arabic": "المُنظّم التربيعي",
    "Chinese": "二次正则化项",
    "French": "régulariseur quadratique",
    "Japanese": "二次正則化項",
    "Russian": "квадратичный регуляризатор"
  },
  {
    "English": "quality estimation",
    "context": "1: • MT: WMT 2019 Shared Tasks on <mark>Quality Estimation</mark> (Fonseca et al., 2019), covering wordlevel quality estimation for English-German and English-Russian machine translation.<br>",
    "Arabic": "تقدير الجودة",
    "Chinese": "质量估计",
    "French": "estimation de la qualité",
    "Japanese": "品質推定",
    "Russian": "оценка качества"
  },
  {
    "English": "quantal response equilibrium",
    "context": "1: The crux of our approach is to consider the <mark>quantal response equilibrium</mark> (QRE), a generalization of Nash equilibrium (NE) that includes some possibility of agents acting suboptimally. We show that the solution of the QRE is a differentiable function of the game payoff matrix, and backpropagation can be computed analytically via implicit differentiation.<br>2: This paper deals with the relatively under-explored but equally important \"inverse\" setting, where the parameters of the underlying game are not known to all agents, but must be learned through observations. We propose a differentiable, end-to-end learning framework for addressing this task. In particular , we consider a regularized version of the game , equivalent to a particular form of <mark>quantal response equilibrium</mark> , and develop 1 ) a primal-dual Newton method for finding such equilibrium points in both normal and extensive form games ; and 2 ) a backpropagation method that lets us analytically compute gradients of all relevant game parameters through the solution itself<br>",
    "Arabic": "توازن الاستجابة الكمية",
    "Chinese": "量子响应均衡",
    "French": "équilibre de réponse quantale",
    "Japanese": "量子応答均衡",
    "Russian": "квантовое равновесие реакции"
  },
  {
    "English": "quantified variable",
    "context": "1: We thus resort to the stronger condition that q + (x) rather than q + (x + ) is acyclic and freeconnex acyclic. The difference between the two conditions is related to the interplay of answer variables and functional roles. In particular , q + ( x ) and q + ( x + ) are identical for OMQs Q = ( O , Σ , q ) such that answer variables have no functional edges to <mark>quantified variables</mark> , that is , for every atom R ( x , y ) in q , if O |= func ( R ) and x<br>2: a role name , and x , y ∈ x ∪ y . We call x the answer variables of q(x) and y <mark>quantified variables</mark>. For purposes of uniformity, we use r − (x, y) as an alternative notation to denote an atom r(y, x) in a CQ.<br>",
    "Arabic": "متغير مُكَمّم",
    "Chinese": "量化变量",
    "French": "variable quantifiée",
    "Japanese": "量化変数",
    "Russian": "квантифицированные переменные"
  },
  {
    "English": "quantifier",
    "context": "1: Natural Language Template First order logic formula with a <mark>quantifier</mark> D (non-)N 1 who is/are (not) (a) N 2 /A 1 is/are (not) (a) N 3 /A 2 ∀x.<br>",
    "Arabic": "محدد الكمية",
    "Chinese": "量词",
    "French": "quantificateur",
    "Japanese": "量化子",
    "Russian": "квантор"
  },
  {
    "English": "quantile",
    "context": "1: The threshold which guarantees that the type-I error (i.e., the probability of rejecting H 0 when it is true) is bounded above by α is given by the (1 − α)-<mark>quantile</mark> of the null distribution i.e., the distribution of n FSSD 2 under H 0 .<br>2: Suppose that the test threshold T α is set to the (1−α)-<mark>quantile</mark> of the distribution of \n dJ i=1 (Z 2 i −1)ν i where {Z i } dJ i=1 i.i.d. ∼ N (0, 1), andν 1 , . . . ,ν dJ are eigenvalues ofΣ q .<br>",
    "Arabic": "كميّة",
    "Chinese": "分位数",
    "French": "quantile",
    "Japanese": "分位数",
    "Russian": "квантиль"
  },
  {
    "English": "quantization",
    "context": "1: The result of projection and <mark>quantization</mark> is that there are less descriptor types to store, and each descriptor vector is shorter. Another benefit is that database retrieval time is reduced. Note that projection and <mark>quantization</mark> introduce errors in the descriptor vectors. We can eliminate the error if each 'compressed' descriptor contains a link to the original descriptor.<br>2: We leave further study of inductive biases in the different embedding models to future work. MAUVE is robust to <mark>quantization</mark>. We compare different three different <mark>quantization</mark> algorithms: \n (a) k-Means: We cluster the hidden representations using k-means, and represent them by their cluster membership to get a discrete distribution with size equal to the number of clusters.<br>",
    "Arabic": "تكميم",
    "Chinese": "量化",
    "French": "quantification",
    "Japanese": "量子化",
    "Russian": "квантование"
  },
  {
    "English": "quantization function",
    "context": "1: where Q is the <mark>quantization function</mark> that maps each entry in clip(w, −α, α)/α to its closest quantized value in the set of uniform discrete values \n<br>",
    "Arabic": "دالة التكميم",
    "Chinese": "量化函数",
    "French": "fonction de quantification",
    "Japanese": "量子化関数",
    "Russian": "функция квантования"
  },
  {
    "English": "quantizer",
    "context": "1: Besides, we propose a module-wise dynamic scaling for the <mark>quantizer</mark> to better adapt to different modules.<br>2: In this work, we re-parameterize the clipping factor to make the <mark>quantizer</mark> adaptive to each module in the Transformer layers, and consider both weights outside and inside the clipping range when estimating the gradient of the clipping factor.<br>",
    "Arabic": "مُكَمِّم",
    "Chinese": "量化器",
    "French": "quantifieur",
    "Japanese": "量子化器",
    "Russian": "квантизатор"
  },
  {
    "English": "quasi-Newton method",
    "context": "1: In each case the number of hidden units was set to 20, subject to the constraint that (n in + n out ) × n hidden ≤ train size/2. We trained the networks to minimize cross entropy error using the <mark>quasi-Newton method</mark> from Netlab Once a pairwise neural network classifier was learned , we classified test examples according to the previous `` edge '' model , again by building a random graph between test labels ( using an average of 18 edges per test label as before ) , using the learned coordination<br>2: As the resulting optimizations for the latter two algorithms are smooth, we employed the L-BFGS <mark>quasi-Newton method</mark> with L2-regularization for training [Nocedal, 1980]. As a substitute for L1regularization, we selected the 23 best features based on their reduction in training error when using logistic regression. Each county had 63 features available.<br>",
    "Arabic": "طريقة شبه نيوتن",
    "Chinese": "类牛顿法",
    "French": "méthode de quasi-Newton",
    "Japanese": "準ニュートン法",
    "Russian": "квази-Ньютоновский метод"
  },
  {
    "English": "quaternion",
    "context": "1: as fields on a single shoulder joint <mark>quaternion</mark> [26] or an elbow joint <mark>quaternion</mark>, conditioned on the shoulder joint [25]. However, those ignore the real part of the <mark>quaternion</mark>, leading to ambiguities in representation, are not differentiable, and are limited to 2 joints in the human body model.<br>",
    "Arabic": "كواترنيون",
    "Chinese": "四元数",
    "French": "quaternion",
    "Japanese": "四元数",
    "Russian": "кватернион"
  },
  {
    "English": "query",
    "context": "1: Using the max-product semiring, we then compute the top gradient path through the different branches (skip connection, keys, values, and <mark>queries</mark>) for (a) the subject of a sentence, (b) the attractors of a sentence, and (c) all tokens of a sentence. Model.<br>2: By examining the top gradient path at this branching point, we can identify not only whether the skip connection or self-attention mechanism is more critical to determining input sensitivity, but also which component within the self-attention mechanism itself (keys, <mark>queries</mark>, or values) carries the most importance. Implementation.<br>",
    "Arabic": "استفسار",
    "Chinese": "查询",
    "French": "requêtes",
    "Japanese": "クエリ",
    "Russian": "запросы"
  },
  {
    "English": "query answer",
    "context": "1: As opposed to active learning methods, one should combine and balance exploration and learning with the normal <mark>query answering</mark> to build such a system. Moreover, current query learning systems assume that users follow a fixed strategy for expressing their intents.<br>",
    "Arabic": "إجابة الاستعلام",
    "Chinese": "查询回答",
    "French": "réponse à la requête",
    "Japanese": "クエリ回答",
    "Russian": "ответ на запрос"
  },
  {
    "English": "query complexity",
    "context": "1: In fact, membership queries allow us to provide attributeefficient algorithms for which the <mark>query complexity</mark> is only logarithmic in the number of attributes. Such results highlight the utility of this model for eliciting CP-nets in large multi-attribute domains.<br>2: However, although a bounded ǫ-perfect protocol exists in the RW model [Robertson and Webb, 1998;Brânzei and Nisan, 2017], all known protocols have running time exponential in 1/ǫ. 5 It is still an open question to find an ǫ-perfect allocation with both query and time complexity polynomial in 1/ǫ.<br>",
    "Arabic": "تعقيد الاستعلامات",
    "Chinese": "查询复杂度",
    "French": "complexité de requête",
    "Japanese": "クエリ複雑性",
    "Russian": "сложность запросов"
  },
  {
    "English": "query context",
    "context": "1: The input queries for each layer of Mo-tionFormer, termed motion queries, comprise two components: the <mark>query context</mark> Q ctx produced by the preceding layer as described before, and the query position Q pos . Specifically, Q pos integrates the positional knowledge in four-folds as in Eq.<br>2: The <mark>query context</mark> which consists of the recent queries issued by the user can help to better understand the user's search intent and enable us to make more meaningful suggestions. In this paper, we propose a novel context-aware query suggestion approach by mining click-through data and session data. We make the following contributions.<br>",
    "Arabic": "سياق الاستعلام",
    "Chinese": "查询上下文",
    "French": "contexte de requête",
    "Japanese": "クエリコンテキスト",
    "Russian": "запросный контекст"
  },
  {
    "English": "query embedding",
    "context": "1: κ δ is the (100−δ)-percentile of the consine similarity between the <mark>query embedding</mark> and theỹ i -th prototype. For example, when δ = 60, it means 60% of examples are above the threshold.<br>2: Given an example x, the per-sample contrastive loss is defined by contrasting its <mark>query embedding</mark> with the remainder of the pool A, \n<br>",
    "Arabic": "تضمين الاستعلام",
    "Chinese": "查询嵌入",
    "French": "requête d'encodage",
    "Japanese": "クエリ埋め込み",
    "Russian": "встраивание запроса"
  },
  {
    "English": "query expansion",
    "context": "1: However, additional considerations have been taken into account and some improvements have been applied as explained below. <mark>Query expansion</mark> is an approach to boost the performance of Information Retrieval (IR) systems. It consists of expanding a query with the addition of terms that are semantically correlated with the original terms of the query.<br>",
    "Arabic": "توسيع الاستعلام",
    "Chinese": "查询扩展",
    "French": "expansion de requête",
    "Japanese": "クエリ拡張",
    "Russian": "расширение запроса"
  },
  {
    "English": "query image",
    "context": "1: Contrary to the image encoder, all parameters of the label encoder are trained from scratch and shared across tasks. Matching Module We implement the matching module at each hierarchy as a multihead attention layer (Vaswani et al., 2017). At each hierarchy of the image and label encoder , we first obtain the tokens of the <mark>query image</mark> X q as { q j } j≤M and support set { ( X i , Y i ) } i≤N as { ( k i k , v i k ) } k≤M , i≤N from the intermediate layers of image and label encoders<br>2: Given a <mark>query image</mark>, user preferences (often captured via click-data) are incorporated to learn a ranking function with the goal of retrieving more relevant images in the top search results.<br>",
    "Arabic": "صورة الاستعلام",
    "Chinese": "查询图像",
    "French": "image de requête",
    "Japanese": "クエリ画像",
    "Russian": "запросное изображение"
  },
  {
    "English": "query language",
    "context": "1: A labeled data example takes the form (D, a, +) or (D, a, −), the former being a positive example and the latter a negative example. Let O be an ontology, Q a <mark>query language</mark>, and E a collection of labeled data examples.<br>2: An ontology-mediated query (OMQ) language is a pair (L, Q) with L an ontology language and Q a <mark>query language</mark>, such as (ELH r , ELQ) and (ELI, ELIQ).<br>",
    "Arabic": "لغة الاستعلامات",
    "Chinese": "查询语言",
    "French": "langage de requête",
    "Japanese": "クエリ言語",
    "Russian": "язык запросов"
  },
  {
    "English": "query phase",
    "context": "1: In the <mark>query phase</mark>, Z T addresses the high computational cost and space cost by elaborately decoupling block results and carefully determining the order of computation. Thanks to these ideas, Z T answers an arbitrary time range query with higher e ciency than existing methods.<br>",
    "Arabic": "المرحلة الاستعلامية",
    "Chinese": "查询阶段",
    "French": "phase de requête",
    "Japanese": "クエリフェーズ",
    "Russian": "фаза запроса"
  },
  {
    "English": "query point",
    "context": "1: We define the likelihood of a <mark>query point</mark> as the maximal likelihood of a region containing that point. Therefore, a point in the query will have a high likelihood, if there exists a large region containing it, with a corresponding similar database region.<br>",
    "Arabic": "نقطة الاستعلام",
    "Chinese": "查询点",
    "French": "point de requête",
    "Japanese": "クエリ点",
    "Russian": "запрашиваемая точка"
  },
  {
    "English": "query processing",
    "context": "1: We experimented with several block sizes for the blocked compression scheme, and observed similar relative behavior from a few hundred bytes to several KB. (In absolute terms, the smaller block sizes result in lower CPU cost for <mark>query processing</mark> as they decode fewer postings overall, but the relative benefit of projection caching is about the same.)<br>2: For this reason, we now describe a simple greedy algorithm for the offline problem, which in each step adds the projection to the cache that maximizes the ratio of additional savings in <mark>query processing</mark> and projection size. It can be implemented as follows: \n 1.<br>",
    "Arabic": "معالجة الاستعلامات",
    "Chinese": "查询处理",
    "French": "traitement des requêtes",
    "Japanese": "クエリ処理",
    "Russian": "обработка запросов"
  },
  {
    "English": "query reformulation",
    "context": "1: If the user retains in the same search paradigm, then this connection is referred as a <mark>query reformulation</mark>. The goal of this work is to create a <mark>query reformulation</mark> model using evolutionary computational technique, and thus the first task is to define the target categories of the proposed model.<br>",
    "Arabic": "إعادة صياغة الاستعلام",
    "Chinese": "查询重构",
    "French": "reformulation de requête",
    "Japanese": "クエリの再定式化",
    "Russian": "переформулирование запроса"
  },
  {
    "English": "query representation",
    "context": "1: We would therefore expect that the self-attention mechanism relies heavily on the <mark>query representation</mark> of the first token and key representations of the remaining tokens and, in particular, the key representation of the repeated token, if present.<br>",
    "Arabic": "تمثيل الاستعلام",
    "Chinese": "查询表示",
    "French": "représentation de la requête",
    "Japanese": "クエリ表現",
    "Russian": "представление запроса"
  },
  {
    "English": "query strategy",
    "context": "1: Finally, we set the <mark>query strategy</mark> to least confidence (Culotta and McCallum, 2005). Initialization There is a chicken-and-egg problem for active learning because most query strategies rely on the model, and a model in turn is trained on labeled instances which are selected by the <mark>query strategy</mark>.<br>2: The actual active learning loop consists of just the previous code block and changing hyperparameters, e.g., using a different <mark>query strategy</mark>, is as easy as adapting the query_strategy variable. In Table 1 , we compare small-text to the previously mentioned libraries , and compare them based on several criteria related to active learning or to the respective code base : While all libraries provide a selection of query strategies , not all li-braries offer stopping criteria , which are crucial to reducing the total annotation effort and thus directly influence the efficiency of<br>",
    "Arabic": "استراتيجية الاستعلام",
    "Chinese": "查询策略",
    "French": "stratégie d'interrogation",
    "Japanese": "クエリ戦略",
    "Russian": "стратегия запроса"
  },
  {
    "English": "query time",
    "context": "1: However, while their expected <mark>query time</mark> requirement may be logarithmic in the database size, selecting useful partitions can be expensive and requires good heuristics; worse, in high-dimensional spaces all exact search methods are known to provide little <mark>query time</mark> improvement over a naive linear scan [17].<br>2: From the results, we observe that their algorithm is a little faster than ours in <mark>query time</mark>, but our algorithm uses much less space(15-30 times). In fact, their algorithm failed for graphs with a million edges, because of memory allocation.<br>",
    "Arabic": "وقت الاستعلام",
    "Chinese": "查询时间",
    "French": "temps de requête",
    "Japanese": "クエリ時間",
    "Russian": "время запроса"
  },
  {
    "English": "query vector",
    "context": "1: Document ranking with unexpanded query: We computed a document ranking using common coefficients jaccard between the document vectors and the unexpanded <mark>query vector</mark>. 7. Listing of candidate terms: We use jacc_coefficient or freq_coefficient using equation (1) or (2) to list out the candidate terms which could be used for expansion. 8.<br>2: Second, the number of clusters is unknown. The clustering algorithm should be able to automatically determine the number of clusters. Third, since each distinct URL is treated as a dimension in a <mark>query vector</mark>, the data set is of extremely high dimensionality.<br>",
    "Arabic": "متجهات الاستعلام",
    "Chinese": "查询向量",
    "French": "vecteur de requête",
    "Japanese": "クエリベクトル",
    "Russian": "вектор запроса"
  },
  {
    "English": "query-document pair",
    "context": "1: Therefore, TIRA distinguishes between two types of retrieval approaches: (1) full-rank approaches with a document corpus and topics as input, and (2) re-rankers with a re-rank file as input (basically, <mark>query-document pairs</mark>).<br>",
    "Arabic": "زوج الاستعلام والمستند",
    "Chinese": "查询-文档对",
    "French": "paire requête-document",
    "Japanese": "クエリとドキュメントのペア",
    "Russian": "запрос-документная пара"
  },
  {
    "English": "question answer",
    "context": "1: Multilingual <mark>Question Answering</mark> The MLQA dataset (Lewis et al., 2020) consists of over 12K question and answer samples in English and 5000 samples in six other languages such as Arabic, German and Spanish. More recently, Clark et al.<br>2: • <mark>Question Answering</mark> (QA), which can be approximated as ranking candidate answer sentences or phrases based on their similarity to the original question (Yang et al., 2015). • Machine Comprehension (MC), which requires sentence matching between a passage and a question, pointing out the text region that contains the answer.<br>",
    "Arabic": "الاجابة على الاسئلة",
    "Chinese": "问答",
    "French": "question réponse",
    "Japanese": "質問回答",
    "Russian": "вопрос-ответ"
  },
  {
    "English": "r-precision",
    "context": "1: These reference-based metrics are difficult to apply to zero-shot text-to-3D generation, as there is no \"true\" 3D scene corresponding to our text prompts. Following Jain et al. (2022), we evaluate the CLIP <mark>R-Precision</mark>, an automated metric for the consistency of rendered images with respect to the input caption.<br>2: On the TriviaQA dataset, NCI obtains 7.9% improvement for Recall@5, 5.5% for Recall@20, 6.0% for Recall@100, and 16.8% for <mark>R-Precision</mark>. As shown in ablation studies, these improvements are owning to the novel designs of PAWA decoder, query generation, semantic identifiers, and consistency-based regularization.<br>",
    "Arabic": "\"دقة-ر\"",
    "Chinese": "R-查准率",
    "French": "R-Précision",
    "Japanese": "R-プレシジョン",
    "Russian": "R-точность"
  },
  {
    "English": "rademacher complexity",
    "context": "1: Zhang et al. ( 2017) defined completely random tasks related to <mark>Rademacher complexity</mark> (Bartlett and Mendelson, 2001) to understand the capacity of neural networks to overfit, showing that they are expressive enough to fit random noise, but still function as effective models.<br>2: Next, we show an important extension of Lemma D.1 that replaces the <mark>Rademacher complexity</mark> term E[R n (F)] by a local quantity r ⋆ n , the fixed point of ψ n (r). To this end, we use another peeling argument and apply Lemma D.1 to the self-normalized class \n<br>",
    "Arabic": "تعقيد رادماخر",
    "Chinese": "拉德马赫复杂度",
    "French": "complexité de Rademacher",
    "Japanese": "ラデマッハー複雑度",
    "Russian": "сложность Радемахера"
  },
  {
    "English": "radiance field",
    "context": "1: We show that an alternate strategy of optimizing networks to encode 5D <mark>radiance fields</mark> (3D volumes with 2D view-dependent appearance) can represent higher-resolution geometry and appearance to render photorealistic novel views of complex scenes.<br>2: They have been used to model fields in 2D or 3D, representing images or partial differentiable equations [58,21], signed or unsigned distances from static 3D shapes [13,48,27,24], pose-conditioned distance field [53,61,43], <mark>radiance fields</mark> [44,51] and more recently for human-object [63,9] and hand-object [69] interactions.<br>",
    "Arabic": "حقول الإشعاع",
    "Chinese": "辐射场",
    "French": "champ de radiance",
    "Japanese": "放射輝度場",
    "Russian": "поле радиации"
  },
  {
    "English": "random crop",
    "context": "1: We use batch size 125, train for 140 epochs, and decay the learning rate thrice at epochs 80, 100 and 120 each by a factor 0.2. 14 We use standard <mark>random crop</mark>, random flip, normalization, and cutout augmentation [77] for the training data.<br>2: Data Augmentation We apply <mark>random crop</mark> (from 256 × 256 resolution to 224 × 224) and random horizontal flip to images, where the random horizontal flip is applied except for surface normal labels as their values are sensitive to the horizontal direction (flipping images and labels together changes the semantics of the task).<br>",
    "Arabic": "قطع عشوائي",
    "Chinese": "随机裁剪",
    "French": "recadrage aléatoire",
    "Japanese": "ランダムクロップ",
    "Russian": "случайная обрезка"
  },
  {
    "English": "random feature",
    "context": "1: There have been several works that exploit this idea to build powerful GNNs, such as using port numbering (Sato et al., 2019), relational pooling (Murphy et al., 2019), <mark>random features</mark> (Sato et al., 2021;Abboud et al., 2021), or dropout techniques (Papp et al., 2021).<br>2: For efficiency we used <mark>random features</mark> for approximating the kernel (Rahimi & Recht, 2008).<br>",
    "Arabic": "الميزات العشوائية",
    "Chinese": "随机特征",
    "French": "caractéristique aléatoire",
    "Japanese": "ランダム特徴量",
    "Russian": "случайный признак"
  },
  {
    "English": "random forest classifier",
    "context": "1: Team UAIC1860 (Ermurachi and Gifu, 2020)(SI: 28, TC: 26) used traditional text representation techniques: character n-grams, word2vec embeddings, and TF.IDF-weighted word-based features. For both subtasks, these features were used in a <mark>Random Forest classifier</mark>. Additional experiments with Naïve Bayes, Logistic Regression and SVMs yielded worse results.<br>",
    "Arabic": "مصنف الغابات العشوائية",
    "Chinese": "随机森林分类器",
    "French": "classificateur de forêt aléatoire",
    "Japanese": "ランダムフォレスト分類器",
    "Russian": "случайный лес классификатор"
  },
  {
    "English": "random matrix theory",
    "context": "1: Recalling the relation between number of samples and step-size, we see that this regime corresponds to the proportional asymptotics regime most studied in the <mark>random matrix theory</mark> literature where the above-mentioned transition for the top eigenvalue occurs.<br>2: In particular, as mentioned above, we find dynamical phase transitions corresponding to the aforementioned thresholds in these models. For our analysis we will focus exclusively on the most interesting, critical step-size scaling which corresponds to the proportional asymptotics regime from the <mark>random matrix theory</mark> literature. 3.2. Analysis.<br>",
    "Arabic": "نظرية المصفوفات العشوائية",
    "Chinese": "随机矩阵理论",
    "French": "théorie des matrices aléatoires",
    "Japanese": "ランダム行列理論",
    "Russian": "теория случайных матриц"
  },
  {
    "English": "random policy",
    "context": "1: For the most challenging tasks, involving sequences of four or five high-level actions, a taskspecific agent initially following a <mark>random policy</mark> essentially never discovers the reward signal, so these tasks cannot be solved without considering their hierarchical structure. We have released code at http://github.com/ jacobandreas/psketch.<br>",
    "Arabic": "سياسة عشوائية",
    "Chinese": "随机策略",
    "French": "politique aléatoire",
    "Japanese": "ランダムポリシー",
    "Russian": "случайная политика"
  },
  {
    "English": "random projection",
    "context": "1: One good example is to find special ALSH schemes for binary data by exploring prior powerful hashing methods for binary data such as (b-bit) minwise hashing and one permutation hashing [3,24]. • Fast hashing for MIPS: Our proposed hash function uses <mark>random projection</mark> as the main hashing scheme.<br>2: For our empirical result, we will use a very simple approximate nearest-neighbor algorithm based on <mark>random projection</mark>. This has reasonable performance in expectation, but is not independent from one step to the next. While the theoretical results from this particular approach are not very strong, it works very well in our experiments.<br>",
    "Arabic": "الإسقاط العشوائي",
    "Chinese": "随机投影",
    "French": "projection aléatoire",
    "Japanese": "ランダム射影",
    "Russian": "случайная проекция"
  },
  {
    "English": "random projection algorithm",
    "context": "1: In our TIWD software we have implemented a \"symmetrized\" version of the <mark>random projection algorithm</mark> for low-rank matrix approximation proposed in (Vempala, 2004) which uses the idea proposed in (Belabbas & Wolfe, 2007). Another extension of the model concerns semi-supervised situations where for a subset of n m observations class labels, i.e.<br>",
    "Arabic": "خوارزمية الإسقاط العشوائي",
    "Chinese": "随机投影算法",
    "French": "algorithme de projection aléatoire",
    "Japanese": "ランダム射影アルゴリズム",
    "Russian": "алгоритм случайной проекции"
  },
  {
    "English": "random sampling",
    "context": "1: Compute a sketch matrix C of R such that R T R − C T C 2 ≤ α R 2 F ≤ O(α) • A − [A] k 2 \n F , which can be done via <mark>random sampling</mark> (Theorem 5) combined with FD.<br>2: Existing methods (<mark>random sampling</mark> (RS), local uncertainly (LU), global uncertainty (GU), and minimal variance (MV)) [12] periodically build a new classifier according to the predefined chunk size. Their classifier counts, therefore, have the same values.<br>",
    "Arabic": "أخذ عينات عشوائية",
    "Chinese": "随机采样",
    "French": "échantillonnage aléatoire",
    "Japanese": "ランダムサンプリング",
    "Russian": "случайная выборка"
  },
  {
    "English": "random seed",
    "context": "1: Fortunately, computing units only need to share one <mark>random seed</mark> s ∈ R and then use a random number generator initialized with the provided seed to generate the same random variables X m,k without the need to communicate any vector.<br>2: We use their model as a baseline model. For all experiments, we use a single predefined <mark>random seed</mark>.<br>",
    "Arabic": "بذرة عشوائية",
    "Chinese": "随机种子",
    "French": "graine aléatoire",
    "Japanese": "乱数シード",
    "Russian": "случайное зерно"
  },
  {
    "English": "random variable",
    "context": "1: The term θ 1 a;f (a) is called a unary potential since its value depends on the labelling of one <mark>random variable</mark> at a time. Similarly, θ 2 ab;f (a)f (b) is called a pairwise potential as it depends on a pair of <mark>random variable</mark>s. For simplicity , we assume that θ 2 ab ; f ( a ) f ( b ) = w ( a , b ) d ( f ( a ) , f ( b ) ) where w ( a , b ) is the weight that indicates the strength of the pairwise relationship between variables v a and v b , with<br>2: σ p (X) = var p (X). We define the mean deviation of a <mark>random variable</mark> X to be its expected absolute deviation from its mean: \n dev p (X) = E p (|X − E p (x)|).<br>",
    "Arabic": "متغير عشوائي",
    "Chinese": "随机变量",
    "French": "variable aléatoire",
    "Japanese": "確率変数",
    "Russian": "случайная величина"
  },
  {
    "English": "random vector",
    "context": "1: Let e ∈ R k be a <mark>random vector</mark> whose k independent components denote k task factors. We assume each independent task factor e i is drawn from a distribution that has mean 0, variance σ 2 , and maximum and minimum values of min(e i ) = −a and max(e i ) = a.<br>2: First, recall from [Ver18, Corollary 7.3.3] that, if <mark>random vector</mark> a ∼ N (0, I d ), then for all u ≥ 0, we have \n P( a 2 ≥ 2 √ d + u) ≤ 2 exp(−cu 2 ). Since x m , t ∼ N ( 0 , Γ ( km ) t mix −1 ) , where Γ ( km ) t mix −1 Γ ( km ) Γ max I d , we have P ( x m , t 2 ≥ √ Γ max ( 2 √ d + u ) ) ≤ 2 exp ( −cu 2 )<br>",
    "Arabic": "متجه عشوائي",
    "Chinese": "随机向量",
    "French": "vecteur aléatoire",
    "Japanese": "ランダムなベクトル",
    "Russian": "случайный вектор"
  },
  {
    "English": "random walk model",
    "context": "1: Table 1 shows the performance of all models, and it is clear that the I-language models substantially outperform the E-language models in almost every case. It is also clear that extracting structure helps: word2vec generally outperformed the corpus count model, and the <mark>random walk model</mark> outperformed the word association count model.<br>2: In our identity management example, τ (t) represents a random identity permutation that might occur among tracks when they get close to each other (a mixing event), but the <mark>random walk model</mark> appears in other applications such as modeling card shuffles [3].<br>",
    "Arabic": "نموذج المشي العشوائي",
    "Chinese": "随机游走模型",
    "French": "modèle de marche aléatoire",
    "Japanese": "ランダムウォークモデル",
    "Russian": "модель случайного блуждания"
  },
  {
    "English": "randomization",
    "context": "1: In Section 2 we present an overview of the swap <mark>randomization</mark> method, and in Section 3 we discuss the applications of the approach to specific data mining tasks. Section 4 describes how the random matrices with given margins are generated and gives results on the performance of the algorithms. In Section 5 we describe the experimental results.<br>",
    "Arabic": "التعشيش العشوائي",
    "Chinese": "随机化",
    "French": "randomisation",
    "Japanese": "ランダム化",
    "Russian": "рандомизация"
  },
  {
    "English": "randomized algorithm",
    "context": "1: We identify an optimal deterministic algorithm, a matching <mark>randomized algorithm</mark> and develop upper and lower bounds that mark the performance that any optimal auditing algorithm must meet. Our first exploration of active fairness estimation seeks to provide a more complete picture of the theory of auditing.<br>2: Theorem 6 (Main Privacy Result -Poisson Distribution). Let Q : X n → Y be a <mark>randomized algorithm</mark> satisfying (λ, ε)-RDP and (ε,δ)-DP for some λ ∈ (1, ∞) and ε,ε,δ ≥ 0. Assume Y is totally ordered. Let µ > 0.<br>",
    "Arabic": "خوارزمية عشوائية",
    "Chinese": "随机算法",
    "French": "algorithme aléatoire",
    "Japanese": "ランダム化アルゴリズム",
    "Russian": "случайный алгоритм"
  },
  {
    "English": "randomized smoothing",
    "context": "1: The overall algorithm, denoted distributed <mark>randomized smoothing</mark> (DRS), uses the <mark>randomized smoothing</mark> optimization algorithm of [10] adapted to a distributed setting, and is summarized in Alg. 1. The computation of a spanning tree T in step 1 allows efficient communication to the whole network in time at most ∆τ .<br>2: Another state-of-the-art approach to certifying robustness is boosting <mark>randomized smoothing</mark> (RS). In this scheme, an ensemble modelf of M classifiers trained on the same dataset with different random seeds (same structures and settings). We denote p 1 as the success probability that the ground-truth label l is correctly predicted.<br>",
    "Arabic": "تنعيم عشوائي",
    "Chinese": "随机平滑化",
    "French": "lissage aléatoire",
    "Japanese": "ランダム化スムージング",
    "Russian": "случайное сглаживание"
  },
  {
    "English": "range query",
    "context": "1: These data-structures enable fast <mark>range queries</mark> (finding all elements in the database in a certain range around a given element).<br>",
    "Arabic": "استعلام النطاق",
    "Chinese": "范围查询",
    "French": "requête de plage",
    "Japanese": "範囲クエリ",
    "Russian": "диапазонный запрос"
  },
  {
    "English": "rank",
    "context": "1: X = [x 1 , • • • , x n , x n+1 ] . Given a set {u 1 , • • • , u n }, the rank of u j for j ∈ [n] is defined as <mark>Rank</mark>(u j ) = n i=1 1 ui≤uj .<br>",
    "Arabic": "رتبة",
    "Chinese": "秩",
    "French": "rang",
    "Japanese": "ランク",
    "Russian": "ранг"
  },
  {
    "English": "rank model",
    "context": "1: An advantage of the PL <mark>ranking model</mark> is that rankings can be sampled quite efficiently. At first glance, sampling a ranking may seem computationally costly, as it involves repeatedly sampling from the item distribution and renormalizing it.<br>2: So far we have introduced the PL-Rank algorithms for estimating the gradient of a PL <mark>ranking model</mark> w.r.t. a relevance metric. However, the applicability of these algorithms are much wider than just relevance metrics, in particular, they can be applied to any exposure-based metrics [2,11,20,28].<br>",
    "Arabic": "نموذج الترتيب",
    "Chinese": "排名模型",
    "French": "modèle de classement",
    "Japanese": "ランクモデル",
    "Russian": "модель ранжирования"
  },
  {
    "English": "rank-one update",
    "context": "1: Hence, we update the entries of the full kernel corresponding to training instances by the <mark>rank-one update</mark> resulting from the optimal solution to (3) and threshold the negative eigenvalues of the full kernel matrix to zero. We then use the test kernel values from the resulting positive semidefinite matrix.<br>",
    "Arabic": "تحديث المرتبة الأولى",
    "Chinese": "秩一更新",
    "French": "mise à jour de rang un",
    "Japanese": "階数1の更新",
    "Russian": "ранг-один обновление"
  },
  {
    "English": "ranking algorithm",
    "context": "1: As seen in the figure, IRLbot succeeded at achieving a strong correlation between domain popularity (i.e., in-degree) and the amount of bandwidth allocated to that domain during the crawl. Our manual analysis of top-1000 domains shows that most of them are highly-ranked legitimate sites, which attests to the effectiveness of our <mark>ranking algorithm</mark>.<br>",
    "Arabic": "خوارزمية الترتيب",
    "Chinese": "排序算法",
    "French": "algorithme de classement",
    "Japanese": "ランキングアルゴリズム",
    "Russian": "алгоритм ранжирования"
  },
  {
    "English": "ranking function",
    "context": "1: To this end, we devise an approach that learns a <mark>ranking function</mark> for each attribute, given relative similarity constraints on pairs of examples (or more generally a partial ordering on some examples). The learned <mark>ranking function</mark> can estimate a real-valued rank 1 for images indicating the relative strength of the attribute presence in them.<br>2: the query q . The task of learning a <mark>ranking function</mark> becomes one of learning an optimal w.<br>",
    "Arabic": "دالة الترتيب",
    "Chinese": "排名函数",
    "French": "fonction de classement",
    "Japanese": "ランキング関数",
    "Russian": "функция ранжирования"
  },
  {
    "English": "reachable state",
    "context": "1: A state s is reachable if there exists an operator sequence π applicable in I such that π I = s. Otherwise, we say that s is unreachable. The set of all <mark>reachable states</mark> is denoted by R. An operator o is reachable iff it is applicable in some reachable state.<br>",
    "Arabic": "حالة يمكن الوصول إليها",
    "Chinese": "可达状态",
    "French": "état accessible",
    "Japanese": "到達可能状態",
    "Russian": "достижимое состояние"
  },
  {
    "English": "reading comprehension",
    "context": "1: Additionally, various models are built to target different applications of NLP such as understanding, generation, and conditional generation, plus specialized use cases such as fast inference or multi-lingual applications. Heads Name Input Output Tasks Ex. Datasets Language Modeling x 1 : n−1 x n ∈ V Generation WikiText-103 Sequence Classification x 1 : N y ∈ C Classification , Sentiment Analysis GLUE , SST , MNLI Question Answering x 1 : M , x M : N y span [ 1 : N ] QA , <mark>Reading Comprehension</mark> SQuAD , Natural Questions Token Classification x 1 : N y 1 : N ∈ C N NER , Tagging OntoNotes , WNUT Multiple Choice x 1 : N , X y ∈ X Text Selection SWAG , ARC Masked LM x 1 : N \\n x n ∈ V Pretraining Wikitext , C4 Conditional Generation x 1 : N y 1 : M ∈ V M Translation , Summarization WMT , IWSLT , CNN/DM<br>",
    "Arabic": "فهم القراءة",
    "Chinese": "阅读理解",
    "French": "compréhension de lecture",
    "Japanese": "読解",
    "Russian": "понимание текста"
  },
  {
    "English": "readout function",
    "context": "1: Given a source task s and a target task t, where s ∈ S and t ∈ T , a transfer network learns a small <mark>readout function</mark> for t given a statistic computed for s (see Fig 4). The statistic is the representation for image I from the encoder of s: E s (I).<br>2: ) -th layer . For graph classification, a graph-level representation is obtained by summarizing all node-level representations in the graph by a <mark>readout function</mark>: \n h G = READOUT h (k) i : i ∈ E(G) , y = softmax(h G ),(2) \n<br>",
    "Arabic": "دالة القراءة",
    "Chinese": "读出函数",
    "French": "fonction de lecture",
    "Japanese": "読み出し関数",
    "Russian": "функция считывания"
  },
  {
    "English": "recall",
    "context": "1: Since there will be a fewer cases for smaller lengths containing elements at higher depths, we also report <mark>Recall</mark> for each i-th stack element, where we only consider if the model can correctly predict the sequences containing at least one occurrence of depth i.<br>2: where L N CP is the loss obtained from the next character prediction task and L aux is the auxillary loss just described and we use λ = 1 20 in our experiments. The stack extraction auxillary task is evaluated by computing Accuracy and <mark>Recall</mark> metrics for each stack element.<br>",
    "Arabic": "استدعاء",
    "Chinese": "召回率",
    "French": "rappel",
    "Japanese": "再現率",
    "Russian": "напоминание"
  },
  {
    "English": "receiver operating characteristic curve",
    "context": "1: By calculating the Area Under the <mark>Receiver Operating Characteristic Curve</mark> (AUC-ROC) value of this classifier f , we get the degree of confusion between category i and k, termed as Confusion ij . The computed Confusionij is a value that never exceeds 1. The closer Confusionij approximates 1, the less pronounced the confusion, and vice versa.<br>2: Using standard signal detection techniques, it is possible to use the distribution of cosine distances across the entire list of word pairs in the overlap set to compute a <mark>Receiver Operating Characteristic Curve</mark> (Fawcett, 2006), from which one derives the area under the curve. We will call this measure : SDT-ρ.<br>",
    "Arabic": "منحنى خصائص التشغيل للمستقبل",
    "Chinese": "受试者工作特征曲线",
    "French": "courbe caractéristique de fonctionnement du récepteur",
    "Japanese": "受信者動作特性曲線",
    "Russian": "Candidate term translation 2: \"Кривая характеристики операционной характеристики приемника (ROC-кривая)\""
  },
  {
    "English": "receptive field",
    "context": "1: In order to better understand the layer-wise effective <mark>receptive field</mark> in Transformer, we plot the distance distribution of interactions extracted by the attribution tree.<br>2: To avoid drifting and introducing spurious artifacts while attempting to fool a single stronger discriminator, we limit the discriminator's <mark>receptive field</mark> to local regions instead of the whole image, resulting in multiple local adversarial losses per image.<br>",
    "Arabic": "حقل استقبالي",
    "Chinese": "感受野",
    "French": "champ réceptif",
    "Japanese": "受容野",
    "Russian": "рецептивное поле"
  },
  {
    "English": "recognition",
    "context": "1: We have demonstrated that RBA can significantly reduce bias amplification. While were not able to remove all amplification, we have made significant progress with little or no loss in underlying <mark>recognition</mark> performance. Across both problems, RBA was able to reduce bias amplification at all initial values of training bias.<br>2: Moreover, the entire system has been adapted to a goal: to be installed on mobile devices. This requirement has led to all methods of detection, classification and <mark>recognition</mark> of activities which must have a reduced computational cost.<br>",
    "Arabic": "التعرف",
    "Chinese": "识别",
    "French": "reconnaissance",
    "Japanese": "認識",
    "Russian": "распознавание"
  },
  {
    "English": "recognition model",
    "context": "1: Object naming has also a practical limitation: due to the heavy-tailed distribution of object instances in natural images, a large number of objects will occur too infrequently to build usable <mark>recognition models</mark>, leaving parts of the image completely unexplained.<br>2: It is wasteful not to take advantage of the CPU weeks [10,12], months [3,6], or even millennia [15] invested in developing <mark>recognition models</mark> for increasingly large labeled datasets [7,19,22,5,20].<br>",
    "Arabic": "نموذج التعرف",
    "Chinese": "识别模型",
    "French": "modèle de reconnaissance",
    "Japanese": "認識モデル",
    "Russian": "модель распознавания"
  },
  {
    "English": "recognize textual entailment",
    "context": "1: This dataset is based on the task proposed by Dagan et al. (2006) in the PASCAL <mark>Recognizing Textual Entailment</mark> (RTE) Challenge. The RTE task involves deciding whether the meaning of a sentence (the hypothesis) can be inferred from a text.<br>2: Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. swering (Harabagiu and Hickl, 2006). In recent years a spectrum of approaches to robust, opendomain NLI have been explored within the context of the <mark>Recognizing Textual Entailment</mark> challenge (Dagan et al., 2005).<br>",
    "Arabic": "التعرف على الاستلزام النصي",
    "Chinese": "识别文本蕴含关系",
    "French": "reconnaissance d'implication textuelle",
    "Japanese": "テキスト推論を認識する",
    "Russian": "распознавание текстовой импликации"
  },
  {
    "English": "recommendation algorithm",
    "context": "1: The use of IR methodology in the evaluation of recommender systems has become common practice in recent years. IR metrics have been found however to be strongly biased towards rewarding algorithms that recommend popular items -the same bias that state of the art <mark>recommendation algorithms</mark> display. Recent research has confirmed and measured such biases, and proposed methods to avoid them.<br>2: Or we might want to rethink <mark>recommendation algorithms</mark> in light of what formal analysis or new experiments on true precision can reveal. Our reported experiments show that getting such estimates on unbiased samples is feasible. Our research can be extended in many directions.<br>",
    "Arabic": "خوارزمية التوصية",
    "Chinese": "推荐算法",
    "French": "algorithme de recommandation",
    "Japanese": "推薦アルゴリズム",
    "Russian": "алгоритм рекомендаций"
  },
  {
    "English": "recommendation model",
    "context": "1: (1) Increasing model complexity: As more modeling techniques are applied and more components are added to the <mark>recommendation model</mark> (to improve its quality), there's a greater chance that the model will suffer from loss divergence problems.<br>",
    "Arabic": "نموذج التوصية",
    "Chinese": "推荐模型",
    "French": "modèle de recommandation",
    "Japanese": "推薦モデル",
    "Russian": "модель рекомендаций"
  },
  {
    "English": "recommendation system",
    "context": "1: GNNs have recently been applied to a broad spectrum of web research such as social influence prediction [37,38], network role discovery [10,39], <mark>recommendation system</mark> [17,54,58], and fraud/spam detection [22,30]. However, scalability is a major challenge that precludes GNN-based methods in practical web-scale graphs.<br>2: Since the articles are new, there is little information about which or how many other users placed the articles in their libraries, and thus traditional collaborative filtering methods has difficulties making recommendations. With new articles, a <mark>recommendation system</mark> must use their content. Finally, exploratory variables can be valuable in online scientific archives and communities.<br>",
    "Arabic": "نظام التوصية",
    "Chinese": "推荐系统",
    "French": "système de recommandation",
    "Japanese": "推薦システム",
    "Russian": "система рекомендаций"
  },
  {
    "English": "recommender",
    "context": "1: In this paper, we study the problem of mitigating radicalization pathways using a graph-based approach. Specifically, we model the set of recommendations of a \"what-to-watch-next\" <mark>recommender</mark> as a -regular directed graph where nodes correspond to content items, links to recommendations, and paths to possible user sessions.<br>",
    "Arabic": "الموصي",
    "Chinese": "推荐人",
    "French": "recommandeur",
    "Japanese": "レコメンダー",
    "Russian": "рекомендатель"
  },
  {
    "English": "recommender system",
    "context": "1: At the same time, discovery may depend on relevance, as is generally the case when items are found by users through a search engine, a <mark>recommender system</mark>, or a suggestion by a friend. If such discovery means are more accurate than random, discovery will be biased towards items that users will like.<br>2: The aim for the <mark>recommender system</mark> is to suggest tutorials so as to meaningfully engage the user on how to use these software and convert novice users into experts in their respective areas of interest. The second application is a professional multi-media editing software.<br>",
    "Arabic": "نظام التوصية",
    "Chinese": "推荐系统",
    "French": "système de recommandation",
    "Japanese": "推薦システム",
    "Russian": "система рекомендаций"
  },
  {
    "English": "reconstruction algorithm",
    "context": "1: A reasonable output sparsity f (k) for sparsity level k should not be much more than k, e.g. f (k) = O(k). Concrete examples of valid <mark>reconstruction algorithms</mark> (along with the associated A k , sperr, etc.) are given in the next section.<br>2: We believe that optical-domain processing-and SLT imaging in particular-offers a powerful new way to analyze the appearance of complex scenes, and to boost the abilities of existing <mark>reconstruction algorithms</mark>.<br>",
    "Arabic": "خوارزمية إعادة البناء",
    "Chinese": "重建算法",
    "French": "algorithme de reconstruction",
    "Japanese": "再構成アルゴリズム",
    "Russian": "алгоритм восстановления"
  },
  {
    "English": "reconstruction error",
    "context": "1: We propose a novel framework called Document Summarization based on Data Reconstruction (DSDR) which finds the summary sentences by minimizing the <mark>reconstruction error</mark>. DSDR firstly learns a reconstruction function for each candidate sentence of an input document and then obtains the error formula by that function. Finally it obtains an optimal summary by minimizing the <mark>reconstruction error</mark>.<br>2: Given an input tensor X and the recon-structionX from the output of Tucker decomposition, <mark>reconstruction error</mark> is de ned as \n X−X 2 F X 2 F \n . Reconstruction error describes how well the reconstructionX of Tucker decomposition represents an input tensor X.<br>",
    "Arabic": "خطأ إعادة الإعمار",
    "Chinese": "重构误差",
    "French": "erreur de reconstruction",
    "Japanese": "再構成誤差 (saikousei gosa)",
    "Russian": "ошибка реконструкции"
  },
  {
    "English": "reconstruction loss",
    "context": "1: Super-Resolution Increase the resolution of an input image by a factor s. We train our model on the low-resolution (LR) image, with a <mark>reconstruction loss</mark> weight of α = 100 and a pyramid scale factor of r = k √ s for some k ∈ N. \n<br>2: Supervision with segmentation masks. We initialize our main time-varying and time-invariant models with masks M i as in Omnimatte [43], by applying a <mark>reconstruction loss</mark> to renderings from the time-varying model in dynamic regions, and to renderings from the time-invariant model in static regions: \n<br>",
    "Arabic": "الخسارة في إعادة البناء",
    "Chinese": "重建损失",
    "French": "perte de reconstruction",
    "Japanese": "再構築損失",
    "Russian": "потери на реконструкцию"
  },
  {
    "English": "recovery algorithm",
    "context": "1: For that, we first fix a <mark>recovery algorithm</mark>, and then utilize the above setup to show that recovery is not asymptotically reliable when K is large. Specifically, we use (59), for which we need to identify random variables X and Y .<br>2: We also study the limitation of any <mark>recovery algorithm</mark> to recover a function exactly from a given form of partial information.<br>",
    "Arabic": "خوارزمية الاسترداد",
    "Chinese": "恢复算法",
    "French": "algorithme de récupération",
    "Japanese": "復元アルゴリズム",
    "Russian": "алгоритм восстановления"
  },
  {
    "English": "rectify linear unit",
    "context": "1: Motivated by [12], we define H (•) as a composite function of three consecutive operations: batch normalization (BN) [14], followed by a <mark>rectified linear unit</mark> (ReLU) [6] and a 3 × 3 convolution (Conv). Pooling layers. The concatenation operation used in Eq.<br>2: Advising policies are neural networks with internal <mark>rectified linear unit</mark> activations. Refer to the supplementary material for hyperparameters. The advising-level learning nature of our problem makes these domains challenging, despite their visual simplicity; their complexity is comparable to domains tested in recent MARL works that learn over multiagent learning processes (Foerster et al.<br>",
    "Arabic": "وحدة خطية معدلة",
    "Chinese": "整流线性单元",
    "French": "unité linéaire rectifiée",
    "Japanese": "整流線形ユニット",
    "Russian": "выпрямленный линейный блок"
  },
  {
    "English": "rectify stereo pair",
    "context": "1: The stereo algorithm of [2] performs brittle block-matching on a <mark>rectified stereo pair</mark> to produce, for each pixel, an interval (lower and upper values [l i , u i ] parametrizing an interval) of likely depths for that pixel.<br>",
    "Arabic": "تصحيح زوج مجسم",
    "Chinese": "校正立体对",
    "French": "paire stéréo rectifiée",
    "Japanese": "正規化されたステレオペア",
    "Russian": "выпрямить стереопару"
  },
  {
    "English": "recurrent",
    "context": "1: We have introduced a method for non-parametric Bayesian modeling of <mark>recurrent</mark>, continuous time processes. The model has attractive properties and we show that the posterior computations can be done efficiently using a sampler based on particle MCMC methods. Most importantly, our experiments show that the model is useful for analyzing complex real world time series.<br>2: We introduce S4, a sequence model that uses a new parameterization for the state space model's continuoustime, <mark>recurrent</mark>, and convolutional views to efficiently model LRDs in a principled manner. Results across established benchmarks evaluating a diverse range of data modalities and model capabilities suggest that S4 has the potential to be an effective general sequence modeling solution.<br>",
    "Arabic": "متكرر",
    "Chinese": "循环的",
    "French": "récurrent",
    "Japanese": "再帰",
    "Russian": "рекуррентный"
  },
  {
    "English": "recurrent architecture",
    "context": "1: This kind of <mark>recurrent architecture</mark> is already popular in RL. In this paper we are providing the theoretical ground to motivate the use of deep recurrent policies to address maximum state entropy exploration.<br>2: Our performance metric across all our experiments is the sentence-level F 1 score. We report precision, recall and F 1 scores for all tasks in Table 2. Our main finding is that our human attention model, based on regularization from mean fixation durations in publicly available eye-tracking corpora, consistently outperforms the <mark>recurrent architecture</mark> with learned attention functions.<br>",
    "Arabic": "البنية المتكررة",
    "Chinese": "循环架构",
    "French": "architecture récurrente",
    "Japanese": "再帰的アーキテクチャ",
    "Russian": "рекуррентная архитектура"
  },
  {
    "English": "recurrent autoencoder",
    "context": "1: The CAE-RNN (Kamper, 2019) is an extension of a <mark>recurrent autoencoder</mark> (Chung et al., 2016), in which both encoder and decoder are recurrent neural networks. Unlike an autoencoder, the CAE-RNN is trained on pairs of word tokens of the same acoustic word embedding \n<br>",
    "Arabic": "التشفير التلقائي المتكرر",
    "Chinese": "循环自编码器",
    "French": "autoencodeur récurrent",
    "Japanese": "再帰オートエンコーダ",
    "Russian": "рекуррентный автоэнкодер"
  },
  {
    "English": "recurrent connection",
    "context": "1: 3 However, this method ignores the relationship states at previous time steps. To model the temporal aspect of relationships, we can add a <mark>recurrent connection</mark>, \n d t = softmax(W d • [h t ; d t−1 ])(5) \n<br>2: In addition, results suggest that these two mechanisms coordinate to support phoneme discovery by introducing countervailing pressures toward retention of previously encountered signals (memory) and consultation of top-down signals (prediction). • The <mark>recurrent connection</mark> includes both the previous segment label and the current segment length in addition to the previous hidden state.<br>",
    "Arabic": "الاتصال العائد",
    "Chinese": "循环连接",
    "French": "connexion récurrente",
    "Japanese": "再帰接続",
    "Russian": "соединение с обратными связями"
  },
  {
    "English": "recurrent dynamic",
    "context": "1: Firstly, the potential existence of cycles (<mark>recurrent dynamics</mark>) implies there are no convergence guarantees, see example 1 and Mertikopoulos et al. (2018). Secondly, even when gradient descent converges, the rate may be too slow in practice because 'rotational forces' necessitate extremely small learning rates (see figure 3).<br>",
    "Arabic": "الديناميات المتكررة",
    "Chinese": "循环动力学",
    "French": "dynamique récurrente",
    "Japanese": "繰り返し動的",
    "Russian": "рекуррентная динамика"
  },
  {
    "English": "recurrent layer",
    "context": "1: For all methods, the input length of recurrent component is chosen from {24, 48, 96, 168, 336, 720}   LSTMa and DeepAR, the size of hidden states is chosen from {32, 64, 128, 256}. For LSTnet , the hidden dimension of the <mark>Recurrent layer</mark> and Convolutional layer is chosen from { 64 , 128 , 256 } and { 32 , 64 , 128 } for Recurrentskip layer , and the skip-length of Recurrent-skip layer is set as 24 for the ETTh1 , ETTh2 , Weather and ECL dataset , and set as 96 for the ETTm dataset<br>",
    "Arabic": "طبقة متكررة",
    "Chinese": "循环层",
    "French": "couche récurrente",
    "Japanese": "再帰層",
    "Russian": "рекуррентный слой"
  },
  {
    "English": "recurrent model",
    "context": "1: PRECOG [81] proposes a <mark>recurrent model</mark> that conditions forecasting on the goal position of the ego vehicle, while PiP [86] generates agents' motion considering complete presumed planning trajectories.<br>2: For an example, see  gradient produced by the control DC, with which we update the activations is, unlike the original setup, not informed by the actual class outputted by the <mark>recurrent model</mark>. This \"class\" in our control task is instead a random set of words, that do not necessarily correspond to the plural form emitted.<br>",
    "Arabic": "النموذج المتكرر",
    "Chinese": "循环模型",
    "French": "modèle récurrent",
    "Japanese": "再帰モデル",
    "Russian": "рекуррентная модель"
  },
  {
    "English": "recurrent network",
    "context": "1: This clipping is not standard practice in deep RL, but common in <mark>recurrent network</mark> training (Bengio et al., 2013). To isolate the contributions of the dueling architecture, we re-train DDQN with a single stream network using exactly the same procedure as described above.<br>2: In recurrent-based meta-reinforcement learning architectures like ours, in which tasks are fed sequentially to the model, the outer loop is explicitly implemented as a reinforcement learning algorithm that updates the weights across tasks and the inner loop is implicitly implemented in the activation dynamics of the <mark>recurrent network</mark> [54,55] which employs fast adaptation within a specific task.<br>",
    "Arabic": "شبكة متكررة",
    "Chinese": "循环网络",
    "French": "réseau récurrent",
    "Japanese": "再帰ネットワーク",
    "Russian": "рекуррентная сеть"
  },
  {
    "English": "recurrent state",
    "context": "1: Apart from scaling and centering the images at the input of the network, we don't use any other preprocessing or augmentation. For the multinomial loss function we use the raw pixel color values as categories. For all the PixelRNN models, we learn the initial <mark>recurrent state</mark> of the network.<br>",
    "Arabic": "الحالة المتكررة",
    "Chinese": "循环状态",
    "French": "état récurrent",
    "Japanese": "再帰状態",
    "Russian": "рекуррентное состояние"
  },
  {
    "English": "recursion",
    "context": "1: the model fails on examples that contain unseen compositions or deeper <mark>recursion</mark> of phenomena that it handles correctly in isolation. For example, a model which correctly parses 'Mary knew that Jim slept' should also be able to parse sentences with deeper <mark>recursion</mark> than it has seen during training such as 'Paul said that Mary knew that Jim slept'.<br>2: 2011 ; Mazuran et al. , 2013 ] . This extensive body of work, however, focuses primarily on integrating <mark>recursion</mark> and arithmetic with aggregate functions in a coherent semantic framework, where technical difficulties arise due to nonmonotonicity of aggregates.<br>",
    "Arabic": "العودية",
    "Chinese": "递归",
    "French": "récursion",
    "Japanese": "再帰",
    "Russian": "рекурсия"
  },
  {
    "English": "recursive call",
    "context": "1: The fact that the thresholds have been increased is passed to subsequent <mark>recursive calls</mark> of the algorithm, prompting these calls to also increase the corresponding thresholds, until a node is expanded or a cycle is closed (i.e., progress is made).<br>",
    "Arabic": "دعوة متكررة",
    "Chinese": "递归调用",
    "French": "appel récursif",
    "Japanese": "再帰呼び出し",
    "Russian": "рекурсивный вызов"
  },
  {
    "English": "recursive neural model",
    "context": "1: Recursive neural models perform well for many structured prediction problems, in part due to their ability to learn representations that depend globally on all parts of the output structures. However, global models of this sort are incompatible with existing exact inference algorithms, since they do not decompose over substructures in a way that allows effective dynamic programming.<br>2: Recursive neural models will then compute parent vectors in a bottom up fashion using different types of compositionality functions g. The parent vectors are again given as features to a classifier. For ease of exposition, we will use the tri-gram in this figure to explain all models.<br>",
    "Arabic": "نموذج عصبي تكراري",
    "Chinese": "递归神经模型",
    "French": "modèle neuronal récursif",
    "Japanese": "再帰的ニューラルモデル",
    "Russian": "рекурсивная нейронная модель"
  },
  {
    "English": "recursive neural network",
    "context": "1: Our search explores fewer nodes than dynamic programs, despite an exponentially larger search space, by allowing the <mark>recursive neural network</mark> to guide the search. Previous work on structured prediction with recursive or recurrent neural models has used beam search-e.g.<br>2: The simplest member of this family of neural network models is the standard <mark>recursive neural network</mark> (Goller and Küchler, 1996;Socher et al., 2011a). First, it is determined which parent already has all its children computed. In the above tree example, p 1 has its two children's vectors since both are words.<br>",
    "Arabic": "شبكة عصبية تكرارية",
    "Chinese": "递归神经网络",
    "French": "réseau neuronal récursif",
    "Japanese": "再帰ニューラルネットワーク",
    "Russian": "рекурсивная нейронная сеть"
  },
  {
    "English": "reference distribution",
    "context": "1: In this section, we compare ourselves with Rozen et al. (2021) in greater details. Rozen et al. (2021) also aims at interpolating between a <mark>reference distribution</mark> p ref and a target distribution p 0 .<br>2: We introduce in this work Riemannian Score-based Generative Models (RSGMs), an extension of SGMs to Riemannian manifolds which incorporate the geometry of the data by defining the forward diffusion process directly on the Riemannian manifold, inducing a manifold-valued reverse process. This requires constructing a noising process on the manifold that converges to an easy-to-sample <mark>reference distribution</mark>.<br>",
    "Arabic": "التوزيع المرجعي",
    "Chinese": "参考分布",
    "French": "distribution de référence",
    "Japanese": "参照分布",
    "Russian": "опорное распределение"
  },
  {
    "English": "reference resolution",
    "context": "1: An indefinite noun phrase such as \"an address\" is very likely to be an input parameter, and a definite noun phrase is not. In general, definite noun phrases are resolved using TRIPS' <mark>reference resolution</mark> capability, capability, connecting the same instances of the parameters as they are used in the task.<br>",
    "Arabic": "حل الإشارات",
    "Chinese": "指代解析",
    "French": "résolution des références",
    "Japanese": "参照解決",
    "Russian": "разрешение референции"
  },
  {
    "English": "reference text",
    "context": "1: Reference-based measures evaluate generated text with respect to a (small set of) <mark>reference text</mark> sample(s), rather than comparing full sequence distributions.<br>",
    "Arabic": "النص المرجعي",
    "Chinese": "参考文本",
    "French": "texte de référence",
    "Japanese": "参照テキスト",
    "Russian": "Эталонный текст"
  },
  {
    "English": "reference-base metric",
    "context": "1: This would allow for a comparison with the hypothesis in the target language, similar to <mark>reference-based metrics</mark>, circumventing alignment problems in multilingual embeddings. This approach updates UScore wrd to \n ( , , ′ ) = xlng WMD ( ) ( , ) + lm LM( ) + pseudo WMD( , ′ ),(3) \n<br>2: Fine-tuning uses novel attention mechanisms and aggregate loss functions to facilitate the multi-task setup. All the above <mark>reference-based metrics</mark> have their corresponding reference-free versions which use the same training regimes but exclude encoding the reference. We refer to them as COMET-QE-DA, COMET-QE-MQM, and UniTE-QE respectively. COMET-QE-DA in this work uses DA scores from 2017 to 2020.<br>",
    "Arabic": "مقياس قائم على المرجع",
    "Chinese": "基于参考的度量",
    "French": "métrique basée sur la référence",
    "Japanese": "参照ベースメトリック",
    "Russian": "метрика на основе эталонного перевода"
  },
  {
    "English": "refinement network",
    "context": "1: Among all these entries, EPro-PnP is the most straightforward as it simply solves the PnP problem itself, without <mark>refinement network</mark> [24,52], disentangled translation [29,45], or multiple representations [40].<br>2: The goal of the <mark>refinement network</mark> is to reduce redundant computation and recover high-resolution matting details. While the base network operates on the whole image, the <mark>refinement network</mark> operates only on patches selected based on the error prediction map E c .<br>",
    "Arabic": "شبكة التحسين",
    "Chinese": "细化网络",
    "French": "réseau d'affinage",
    "Japanese": "精緻化ネットワーク",
    "Russian": "сеть уточнения"
  },
  {
    "English": "regression",
    "context": "1: We trained a <mark>Regression</mark> Support Vector Machine (SVM) with quite good results. The correlation between predicted and actual incidences is about 0.94 in a cross-validation experiment. However, if we try to devise an alarm system for predicting values above a given threshold, we find that the number of false negatives is too high.<br>",
    "Arabic": "انحدار",
    "Chinese": "回归",
    "French": "régression",
    "Japanese": "回帰",
    "Russian": "регрессия"
  },
  {
    "English": "regression analysis",
    "context": "1: A <mark>regression analysis</mark> shows almost all β-values of ICL models moving closer to parity, showing us how the dataset difficulty impacted the results. However, even without the effect of dataset difficulty on the β-values, they are still not quite equal to 1, suggesting that the type of adaptation data has a small influence on ICL learners.<br>2: Beguš (2020) uses <mark>regression analysis</mark> from the latent variables to the phonetic and phonological features in the generated outputs to reveal the correspondence relations between latent variables and the phonetic and phonological features.<br>",
    "Arabic": "تحليل الانحدار",
    "Chinese": "回归分析",
    "French": "analyse de régression",
    "Japanese": "回帰分析",
    "Russian": "регрессионный анализ"
  },
  {
    "English": "regression coefficient",
    "context": "1: Penalized linear regression (PLR) methods estimate the <mark>regression coefficients</mark> by minimizing a penalized squared-loss function: \n minimize b ∈ R p h ρ (b) ≜ 1 2 ∥y − Xb∥ 2 + p j=1 ρ(b j ),(29) \n<br>2: , x p ∈ R n , b ∈ R p is a vector of <mark>regression coefficients</mark>, and σ 2 ≥ 0 is the variance of the residual errors.<br>",
    "Arabic": "معامل الانحدار",
    "Chinese": "回归系数",
    "French": "coefficient de régression",
    "Japanese": "回帰係数",
    "Russian": "регрессионный коэффициент"
  },
  {
    "English": "regression function",
    "context": "1: K = I, explicitly incorporating a component related to the likelihood as described above is not necessary, since its contribution can be learned by the <mark>regression function</mark>. However, for deblurring this is not feasible, and it is crucial to incorporate a blur component into the model to adapt to arbitrary blurs.<br>2: We first conduct experiments on dense synthetic dataset. To have better validation of the effectiveness of our proposed reformulation, we use the same artificial dataset in Wang et al. (2021b), which employs make <mark>regression function</mark> in scikit-learn (Pedregosa et al., 2011) with setting the noise as 0.1 and other parameters as default.<br>",
    "Arabic": "دالة الانحدار",
    "Chinese": "回归函数",
    "French": "fonction de régression",
    "Japanese": "回帰関数",
    "Russian": "функция регрессии"
  },
  {
    "English": "regression model",
    "context": "1: For gender, the <mark>regression model</mark> outperforms the classification approach, which is surprising given that the <mark>regression model</mark> does not have any hand-labeled profiles for training. For ethnicity, the regression approach outperforms classification until over half of the labeled data is used to fit the classification approach, after which the classification approach dominates.<br>2: Since even a small d may lead to a huge amount of interaction variables, the two-step procedure is still time-consuming in many applications. Recently, there have been growing research efforts on imposing the hierarchical structure on main effects and interactions in the <mark>regression model</mark> with novel sparse learning methods.<br>",
    "Arabic": "نموذج الانحدار",
    "Chinese": "回归模型",
    "French": "modèle de régression",
    "Japanese": "回帰モデル",
    "Russian": "модель регрессии"
  },
  {
    "English": "regression problem",
    "context": "1: Experiment Outline: ClimSim presents a <mark>regression problem</mark> with mapping from a multivariate input vector, with inputs x ∈ R di of size d i = 124 and targets y ∈ R do of size d o =128 (Figure 1).<br>2: ] . The task is essentially a <mark>regression problem</mark>: in the climate simulation, an ML parameterization emulator returns the large-scale outputs-changes in wind, moisture, or temperature-that occur due to unresolved small-scale (sub-resolution) physics, given large-scale resolved inputs (e.g., temperature, wind velocity; see Section 4).<br>",
    "Arabic": "مشكلة الانحدار",
    "Chinese": "回归问题",
    "French": "problème de régression",
    "Japanese": "回帰問題",
    "Russian": "задача регрессии"
  },
  {
    "English": "regression task",
    "context": "1: So we are not surprised Clippy can help in the production model which is much more complex than Large+DCN. 3) bottom hidden layer of shared bottom, and in the output layers of (4) binary classification task and (5) <mark>regression task</mark>.<br>2: If Y is a metric space (usually the set of real numbers), the learning job is a <mark>regression task</mark>, as in the case of coffee rust. In this case, the aim of learners is to obtain a hypothesis whose predictions are as similar as possible to actual values in the output space.<br>",
    "Arabic": "مهمة الانحدار",
    "Chinese": "回归任务",
    "French": "tâche de régression",
    "Japanese": "回帰タスク",
    "Russian": "задача регрессии"
  },
  {
    "English": "regression tree",
    "context": "1: The tractability of SHAP on linear regression models is well known. In fact,Štrumbelj and Kononenko ( 2014) provide a closed-form formula for this case. 2. Paths from root to leaf in a decision or <mark>regression tree</mark> are mutually exclusive.<br>2: They are difficult to interpret and do not clearly identify the interacting SNPs. Recursive partitioning methods [Zhang and Bonney 2000;Province et al. 2001] utilize classification and <mark>regression tree</mark> (CART) [Breiman et al. 1984] to pick the SNP that minimizes some pre-specified measure of impurity in each iteration.<br>",
    "Arabic": "شجرة الانحدار",
    "Chinese": "回归树",
    "French": "arbre de régression",
    "Japanese": "回帰木",
    "Russian": "регрессионное дерево"
  },
  {
    "English": "regressor",
    "context": "1: The regressor w X * ≈ (−2, 0.5) fits the consistent Q-values perfectly, yielding optimal (policy-consistent) Q-values, because ConQ need not make tradeoffs to fit inconsistent values.<br>",
    "Arabic": "مُعيد الانحدار",
    "Chinese": "回归器",
    "French": "régresseur",
    "Japanese": "回帰器",
    "Russian": "регрессор"
  },
  {
    "English": "regret bind",
    "context": "1: The <mark>regret bound</mark> in Theorem 2 is not sublinear due to the additional discretization error. An intuitive way to alleviate this error is to adaptively reduce the discretization gap.<br>2: By substituting ω = 1, γ = N , and using upper bounds for m and R max , we get a valid <mark>regret bound</mark> of Algorithm 1 with fixed discretization gap ∆: \n Reg discrete ∆ (T ) ≤ 2N 6N T log T ∆ + π 2 3 + 1 N 2 L ∆ . (11 \n ) \n<br>",
    "Arabic": "حد الندم",
    "Chinese": "遗憾约束",
    "French": "limite de regret",
    "Japanese": "後悔束縛",
    "Russian": "связывание сожалений"
  },
  {
    "English": "regret matching",
    "context": "1: Regret matching (RM) (Blackwell et al., 1956;Hart & Mas-Colell, 2000) is an alternative equilibrium-finding algorithm that has similar theoretical guarantees to hedge and was used in previously work on Diplomacy Gray et al. (2020); Bakhtin et al. (2021).<br>2: In an -Nash equilibrium, no player has exploitability higher than . In CFR, the strategy vector for each infoset is determined according to a regret-minimization algorithm. Typically, <mark>regret matching</mark> (RM) is used as that algorithm within CFR due to RM's simplicity and lack of parameters.<br>",
    "Arabic": "مطابقة الندم",
    "Chinese": "后悔匹配",
    "French": "Appariement des regrets",
    "Japanese": "後悔マッチング",
    "Russian": "сопоставление сожалений"
  },
  {
    "English": "regret minimization",
    "context": "1: We provide the proof in the appendix. It combines elements of the proof for CFR+  and the proof that discounting in <mark>regret minimization</mark> is sound (Cesa-Bianchi and Lugosi 2006).<br>2: In particular, <mark>regret minimization</mark> by the SQL dynamics at an optimal O(1/T ) rate implies that their time-average converges fast to coarse correlated equilibria (CCE). These are CCE of the perturbed game, Γ H , but if exploration parameter is low, they are approximate CCE of the original game as well.<br>",
    "Arabic": "تقليل الندم",
    "Chinese": "后悔最小化",
    "French": "minimisation du regret",
    "Japanese": "後悔最小化",
    "Russian": "минимизация сожаления"
  },
  {
    "English": "regret minimization algorithm",
    "context": "1: Optimistic Hedge (Syrgkanis et al. 2015) is a <mark>regret minimization algorithm</mark> similar to Hedge in which the last iteration is counted twice when determining the strategy for the next iteration. This can lead to substantially faster convergence, including in some cases an improvement over the O( 12 ) bound on regret of typical regret minimizers.<br>",
    "Arabic": "خوارزمية تقليل الندم",
    "Chinese": "后悔最小化算法",
    "French": "algorithme de minimisation du regret",
    "Japanese": "後悔最小化アルゴリズム",
    "Russian": "алгоритм минимизации сожалений"
  },
  {
    "English": "regret minimizer",
    "context": "1: We show that it is possible to orchestrate the learning procedure so that, for each information set, employing one <mark>regret minimizer</mark> per round does not compromise the overall convergence of the algorithm. The empirical frequency of play generated by ICFR converges to an EFCE almost surely in the limit.<br>2: In the regret minimization framework [54], each player i ∈ P plays repeatedly against the others by making a series of decisions from a set X i . A <mark>regret minimizer</mark> for player i ∈ P is a device that, at each iteration t = 1, . . . , T , supports two operations : ( i ) RECOMMEND , which provides the next decision x t+1 i ∈ X i on the basis of the past history of play and the observed utilities up to iteration t ; and ( ii ) OBSERVE , which receives a utility function u t i : X i → R that is used to<br>",
    "Arabic": "مُقلِّل الندم",
    "Chinese": "后悔最小化",
    "French": "minimisateur de regret",
    "Japanese": "後悔最小化器",
    "Russian": "минимизатор сожалений"
  },
  {
    "English": "regular expression",
    "context": "1: Our most efficient prompt at that time was a script where we explained to the model that slashes were 'a deliberate choice and an effective way to parse data as part of a <mark>regular expression</mark>.'<br>",
    "Arabic": "تعبير منتظم",
    "Chinese": "正则表达式",
    "French": "expression régulière",
    "Japanese": "正規表現",
    "Russian": "регулярное выражение"
  },
  {
    "English": "regularisation",
    "context": "1: We repeat this to compute the remaining levels of the hierarchy. A completely new set of <mark>regularisation</mark> edges E is then constructed, starting with edges from l = 0 (i.e. N warp ) to the nodes in N reg at l = 1.<br>2: Our <mark>regularisation</mark> graph topology is then simply formed by adding edges from each node of the hierarchy (starting in N warp ) to its k−nearest nodes in the next coarser level.<br>",
    "Arabic": "تنظيم",
    "Chinese": "正则化",
    "French": "régularisation",
    "Japanese": "正則化",
    "Russian": "регуляризация"
  },
  {
    "English": "regularization",
    "context": "1: Batch normalization leads to significant improvements in convergence while eliminating the need for other forms of <mark>regularization</mark> [7]. By adding batch normalization on all of the convolutional layers in YOLO we get more than 2% improvement in mAP. Batch normalization also helps regularize the model. With batch normalization we can remove dropout from the model without overfitting.<br>2: In this section, we consider the simplest downstream classification task Figure 15. Score vs hyperparameters for each score (column) and data set (row). There seems to be no model dominating all the others and for each model there does not seem to be a consistent strategy in choosing the <mark>regularization</mark> strength.<br>",
    "Arabic": "التنظيم",
    "Chinese": "正则化",
    "French": "régularisation",
    "Japanese": "正則化",
    "Russian": "регуляризация"
  },
  {
    "English": "regularization constant",
    "context": "1: t , i −xu , t , j ) − λΘ||Θ|| 2 F ( 27 ) \n where λΘ is the <mark>regularization constant</mark> corresponding to σΘ.<br>2: We use a batch size of 64 and dropout of 0.2. The model is trained using Adam with a learning rate of 5•10 −5 and with early stopping. For friction detection, we use the model described in Section 3.2 and tune the SVM's <mark>regularization constant</mark> via grid search. Labels are projected as described in Section 3.3.<br>",
    "Arabic": "ثابت التنظيم",
    "Chinese": "正则化常数",
    "French": "constante de régularisation",
    "Japanese": "正則化定数",
    "Russian": "константа регуляризации"
  },
  {
    "English": "regularization function",
    "context": "1: L(Ψ|D, Ω) = L(Ψ|D) − λ 2 • R(Ψ|Ω)(4) \n The first component L is the log-likelihood function in Equation 2 , which reflects the global consistency between the latent parameters Ψ and the observation D. The second component R is a <mark>regularization function</mark> , which reflects the local consistency between the latent parameters Ψ of neighboring documents in the manifold Ω. λ is the regularization parameter , commonly found in manifold learning algorithms<br>2: As one of our major contributions, we show that the proximal operator associated with the <mark>regularization function</mark> in weak hierarchical Lasso admits a closed form solution. Furthermore, we develop an efficient algorithm which computes each subproblem of the proximal operator with a time complexity of O(d log d).<br>",
    "Arabic": "دالة التنظيم",
    "Chinese": "正则化函数",
    "French": "fonction de régularisation",
    "Japanese": "正則化関数",
    "Russian": "регуляризационная функция"
  },
  {
    "English": "regularization loss",
    "context": "1: The <mark>regularization loss</mark> of query q for the i-th decoding step is defined as, \n Lreg = − log exp (sim(zi,1, zi,2)/τ ) 2Q k=1,k =2 exp (sim((zi,1, z i,k )/τ )(5) \n<br>2: ( velocity , attribute ) required by the nuScenes benchmark [ 8 ] . The deformable 2D-3D correspondences can be learned solely with the KL divergence loss L KL , preferably in conjunction with the <mark>regularization loss</mark> L reg . Other auxiliary losses can be imposed onto the dense features for enhanced accuracy. Details are given in supplementary materials.<br>",
    "Arabic": "خسارة التنظيم",
    "Chinese": "正则化损失",
    "French": "perte de régularisation",
    "Japanese": "正則化損失",
    "Russian": "потеря регуляризации"
  },
  {
    "English": "regularization parameter",
    "context": "1: , a n ] T ∈ R n×m . (4) \n The set {x i } includes the selected representative sentences from the original candidate sentence set V and will be used as the document summary finally. λ is the <mark>regularization parameter</mark> controlling the amount of shrinkage. The optimization problem in Eq.<br>2: Note that we train the models on each iteration of the cross-validation and keep the <mark>regularization parameter</mark> the same between iterations. As a result, there can be a slight variation in the actual number of features selected on each iteration. We see in Table 2 the results of the 4 models.<br>",
    "Arabic": "معامل التنظيم",
    "Chinese": "正则化参数",
    "French": "paramètre de régularisation",
    "Japanese": "正則化パラメータ",
    "Russian": "параметр регуляризации"
  },
  {
    "English": "regularization path",
    "context": "1: The sequence of solutions for varied regularization strength is called the <mark>regularization path</mark>, and by slight abuse of terminology we use this to refer to the induced template ordering.<br>",
    "Arabic": "مسار التنظيم",
    "Chinese": "正则化路径",
    "French": "chemin de régularisation",
    "Japanese": "正則化パス",
    "Russian": "регуляризационный путь"
  },
  {
    "English": "regularization penalty",
    "context": "1: For generating single objects instead of scenes, a reduced bounding sphere can be useful. Geometry regularizers. The mip-NeRF 360 model we build upon contains many other details that we omit for brevity. We include a <mark>regularization penalty</mark> on the opacity along each ray similar to Jain et al.<br>2: (2022) found that an effective solution was to perform search with a <mark>regularization penalty</mark> proportional to the KL divergance from a human imitation policy. This algorithm is referred to as piKL.<br>",
    "Arabic": "عقوبة التنظيمية",
    "Chinese": "正则化惩罚",
    "French": "pénalité de régularisation",
    "Japanese": "正則化ペナルティ",
    "Russian": "штраф за регуляризацию"
  },
  {
    "English": "regularization strength",
    "context": "1: We add an`2 regularization term to the objective, and tune the margin m and the <mark>regularization strength</mark> to tradeoff between speed and accuracy. In our experiments, we used a development set to choose a regularizer and margin that reduced testtime speed as much as possible without decreasing accuracy.<br>2: in Appendix I.4 for further details ) . Similarly, if the score depends on the Cartesian product of objective function and <mark>regularization strength</mark> (again categorical), we are able to explain 59% of the variance while the rest is due to the random seed. Implication.<br>",
    "Arabic": "قوة التنظيم",
    "Chinese": "正则化强度",
    "French": "force de régularisation",
    "Japanese": "正則化強度",
    "Russian": "сила регуляризации"
  },
  {
    "English": "regularization term",
    "context": "1: 2 2 Llatent . (6) \n Where L img is an 2 -loss enforcing closeness of the rendered image to ground-truth, L depth is a <mark>regularization term</mark> that accounts for the positivity constraint in Eq. 4, and L latent enforces a Gaussian prior on the z j .<br>2: We utilize cross-entropy loss as the optimization function of DSP. For DAP, we add a <mark>regularization term</mark> to constrain those domain-sensitive parameters to alleviate the instability when facing domain changes. We will apply the learned prompts from the previous batch to the next batch for testing.<br>",
    "Arabic": "مصطلح التنظيم",
    "Chinese": "正则化项",
    "French": "terme de régularisation",
    "Japanese": "正則化項",
    "Russian": "регуляризационный член"
  },
  {
    "English": "regularization weight",
    "context": "1: λJ((S l ) l=L l=1 ) = λ l=L l=1 l i=1 1 S i h × S i w , (6 \n ) \n where λ is the <mark>regularization weight</mark>. In Section 3.2, we show that training on ImageNet with different values for λ allows us to trade-off accuracy for efficiency in a smooth fashion.<br>2: We also vary the number of attention head from {1, 2, 3, 4}, then change the parameter of <mark>regularization weight</mark> from {0.0001, 0.001, 0.01, 0.1}.<br>",
    "Arabic": "وزن التنظيم",
    "Chinese": "正则化权重",
    "French": "poids de régularisation",
    "Japanese": "正則化重み",
    "Russian": "вес регуляризации"
  },
  {
    "English": "regularization-base method",
    "context": "1: The second is the <mark>regularization-based methods</mark>, (OctoMiao 2016;Zenke, Poole, and Ganguli 2017) proposes the idea of elastic weight estimation and adjusts the weight updating strategy according to the importance estimate of the parameters.<br>2: Regularization-Based Methods. The second branch of works introduces additional penalty terms to the learning objective on the parameters, alleviating the issue of catastrophic forgetting (Kirkpatrick et al., 2017;Thompson et al., 2019;Castellucci et al., 2021;Gu et al., 2022).<br>",
    "Arabic": "الطرق المعتمدة على التنظيم",
    "Chinese": "基于正则化的方法",
    "French": "méthode basée sur la régularisation",
    "Japanese": "正則化ベースの方法",
    "Russian": "методы на основе регуляризации"
  },
  {
    "English": "regularizer",
    "context": "1: The matrix completion objective requires different tools due to the sampling of the observed entries, as well as carefully managing the <mark>regularizer</mark> to restrict the geometry. Parallel to our work Bhojanapalli et al. [BNS16] showed similar results for matrix sensing, which is closely related to matrix completion.<br>2: We give it here for completeness. Proof of Lemma C.1. Note that f (X) is equal to h(X) + λR(X) where where h(X) = 1 2 P Ω (M − XX T ) 2 F , and R(X) is the <mark>regularizer</mark>.<br>",
    "Arabic": "مُنَظِّم",
    "Chinese": "正则化项",
    "French": "régulariseur",
    "Japanese": "正則化項",
    "Russian": "регуляризатор"
  },
  {
    "English": "reinforcement Learning",
    "context": "1: Although <mark>Reinforcement Learning</mark> (RL) has been applied in a range of domains, from game playing (Mnih et al., 2013;Silver et al., 2016) to robotic control (Schulman et al., 2015), only few works have successfully employed RL to the image recovery tasks.<br>2: 1 because determining the former's role is more straightforward than doing that for the latter. We address the challenge of argument extraction order by employing <mark>Reinforcement Learning</mark> (RL) to rank arguments to best utilize the argument relation.<br>",
    "Arabic": "تعلم التعزيز",
    "Chinese": "强化学习",
    "French": "apprentissage par renforcement",
    "Japanese": "強化学習",
    "Russian": "обучение с подкреплением"
  },
  {
    "English": "reinforcement learning algorithm",
    "context": "1: This way, there 4521 intent per query that can be returned, which is close to the number of answers a <mark>reinforcement learning algorithm</mark> may consider over a large data set after filtering [49]. The DBMS strategy for our method is initialized to be completely random.<br>2: As current query interfaces do not effectively learn the information needs behind queries in such a setting, we proposed a <mark>reinforcement learning algorithm</mark> for the DBMS that learns the querying strategy of the user effectively. We provided efficient implementations of this learning mechanisms over large databases.<br>",
    "Arabic": "خوارزمية التعلم المعزز",
    "Chinese": "强化学习算法",
    "French": "algorithme d'apprentissage par renforcement",
    "Japanese": "強化学習アルゴリズム",
    "Russian": "алгоритм обучения с подкреплением"
  },
  {
    "English": "rejection sampling",
    "context": "1: Practitioners have conventionally used sampling schemes [MacKay, 2003] to approximate the posterior distributions. <mark>Rejection sampling</mark> and various MCMC methods are common choices. The advantage of MCMC approaches is their theoretical guarantees with large sample sets [Robert and Casella, 2005] and thus they are a good choice when likelihood evaluations are cheap.<br>2: <mark>Rejection sampling</mark>, due to John von Neumann [44], is the most classical Monte Carlo method. <mark>Rejection sampling</mark> makes two assumptions: (1) supp(π) ⊆ supp(p); and (2) there is a known envelope constant C satisfying: \n<br>",
    "Arabic": "أخذ عينات الرفض",
    "Chinese": "拒绝采样",
    "French": "échantillonnage par rejet",
    "Japanese": "棄却サンプリング",
    "Russian": "отбраковочная выборка"
  },
  {
    "English": "relation extraction",
    "context": "1: For example, SpERT (Eberts and Ulges, 2019) performs entity extraction and <mark>relation extraction</mark> jointly using pre-trained Transformers. DyGIE++ (Wadden et al., 2019) also jointly addresses the two tasks with the event extraction task. Besides, Luan et al.<br>2: Fully-supervised <mark>relation extraction</mark> (RE) approaches (e.g., [13], [22], [25]) aims to predict the relationship between a pair of entities in a sentence. These approaches encounter two challenges: (1) They usually require a large number of humanannotated sentences as training data, which are expensive to obtain.<br>",
    "Arabic": "استخراج العلاقات",
    "Chinese": "关系抽取",
    "French": "extraction de relations",
    "Japanese": "関係抽出",
    "Russian": "извлечение отношений"
  },
  {
    "English": "relation type",
    "context": "1: (2) The power of the trained relation classifier is confined to a given <mark>relation type</mark> set, which makes it hard to transfer the model to new <mark>relation type</mark>s or new domains. After giving an overview of the RE task and fully-supervised methods, we will cover three types of minimum supervised approached tackling the above two challenges.<br>",
    "Arabic": "نوع العلاقة",
    "Chinese": "关系类型",
    "French": "type de relation",
    "Japanese": "関係タイプ",
    "Russian": "тип отношения"
  },
  {
    "English": "relational tuple",
    "context": "1: This amounts to a novel form of relational clustering, where clustering is done not just on fixed elements in <mark>relational tuples</mark>, but on arbitrary forms that are built up recursively.<br>2: Information extraction (IE) turns the unstructured information expressed in natural language text into a structured representation (Jurafsky and Martin, 2009) in the form of <mark>relational tuples</mark> consisting of a set of arguments and a phrase denoting a semantic relation between them: arg1; rel; arg2 .<br>",
    "Arabic": "صفيف علاقات",
    "Chinese": "关系元组",
    "French": "n-uplet relationnel",
    "Japanese": "関係タプル",
    "Russian": "реляционный кортеж"
  },
  {
    "English": "relative entropy",
    "context": "1: These accounts contend that the Kullback-Leibler (KL) divergence (i.e., <mark>relative entropy</mark>) between the probabilistic state of the processor before and after observing a given word is the cause of the processing difficulty associated with that word.<br>2: the latent representation relevant for predicting the sensitive attribute Ψ sens (X). Indeed, if information regarding the sensitive attribute is partially preserved or leaks into Ψ pred (X), the <mark>relative entropy</mark> will be low [50].<br>",
    "Arabic": "الإنتروبيا النسبية",
    "Chinese": "相对熵",
    "French": "entropie relative",
    "Japanese": "相対エントロピー",
    "Russian": "относительная энтропия"
  },
  {
    "English": "relative positional embedding",
    "context": "1: Length extrapolation permits training a transformer language model on short sequences that preserves perplexities when tested on substantially longer sequences. A <mark>relative positional embedding</mark> design, ALiBi, has had the widest usage to date. We dissect ALiBi via the lens of receptive field analysis empowered by a novel cumulative normalized gradient tool.<br>2: Several extrapolatable transformer language models have been proposed including ALiBi (Press et al., 2022) and KERPLE (Chi et al., 2022), of which the <mark>relative positional embedding</mark> design is hypothesized to be critical to success. Empirically, they extrapolate to L ex L tr much better than other absolute and <mark>relative positional embedding</mark>s   \n<br>",
    "Arabic": "التضمين الموضعي النسبي ALiBi",
    "Chinese": "相对位置嵌入",
    "French": "Plongement positionnel relatif",
    "Japanese": "相対位置埋め込み",
    "Russian": "\"относительное позиционное встраивание\""
  },
  {
    "English": "relevance score",
    "context": "1: Definition 3 (Relevance Score) The <mark>relevance score</mark> of a label to a topic model, s(l, θ), measures the semantic similarity between the label and the topic model.<br>",
    "Arabic": "درجة الصلة",
    "Chinese": "相关性评分",
    "French": "score de pertinence",
    "Japanese": "関連性スコア",
    "Russian": "оценка релевантности"
  },
  {
    "English": "render network",
    "context": "1: 1 ○ Without objectmask supervision it fails to converge to a reasonable geometry. 2 ○ With a known-reflectance model (Phong) the silhouette of the object is recovered but without any details. 3 ○ It requires a <mark>rendering network</mark> (unknown reflectance) and an object-mask for good convergence-both of which not required for our method.<br>2: The former, given a ray r u , samples 32 points x i ∈ r u at uniform depth intervals between predefined lower and upper depth bounds. The fine <mark>rendering network</mark> then samples 16 points on r u with importance sampling from the distribution proportional to the coarse rendering weights w i .<br>",
    "Arabic": "شبكة التجسيد",
    "Chinese": "渲染网络",
    "French": "réseau de rendu",
    "Japanese": "レンダリングネットワーク",
    "Russian": "сеть визуализации"
  },
  {
    "English": "renormalization",
    "context": "1: We introduce the alignment of features into their own notion of aligned SNR coordinates: Definition 5 (Renormalization to aligned SNR Coordinates). Consider Ď H ∈ R P ×CN with corresponding SNR left and right singular vectors U and V (Definition 2). Then, define the SNR-aligned <mark>renormalization</mark> of the features as \n<br>2: Once we condition on S we still have the outputs (1) with large privacy loss, but that privacy loss is further increased because of the <mark>renormalization</mark>.<br>",
    "Arabic": "إعادة التطبيع",
    "Chinese": "重整化",
    "French": "renormalisation",
    "Japanese": "正規化",
    "Russian": "ренормализация"
  },
  {
    "English": "reparameterization",
    "context": "1: Fortunately, the Bradley-Terry model depends only on the difference of rewards between two completions, i.e., p * (y 1 ≻ y 2 | x) = σ(r * (x, y 1 ) − r * (x, y 2 )). Substituting the <mark>reparameterization</mark> in Eq.<br>2: Alternatively, prior works have normalized rewards using a human completion baseline, essentially a single sample Monte-Carlo estimate of the normalizing term. In contrast the DPO <mark>reparameterization</mark> yields a reward function that does not require any baselines. Left. The frontier of expected reward vs KL to the reference policy.<br>",
    "Arabic": "إعادة المعلمة",
    "Chinese": "重参数化",
    "French": "reparamétrisation",
    "Japanese": "再パラメータ化",
    "Russian": "Перепараметризация"
  },
  {
    "English": "reparameterization trick",
    "context": "1: [ ∇ η E qη ( z ( k ) |x ( k ) ) [ f ( σ λ ( z ( k ) ) ) ] ] . Both parts can be estimated with the <mark>reparameterization trick</mark> [29,47,58] which often has low variance.<br>",
    "Arabic": "خدعة إعادة المعلمة",
    "Chinese": "重参数化技巧",
    "French": "astuce de reparamétrisation",
    "Japanese": "再パラメータ化トリック",
    "Russian": "трюк репараметризации"
  },
  {
    "English": "replay buffer",
    "context": "1: The precise behavior depends on the training regime, but poor behavior can emerge in batch methods as well. For instance, batch Q-learning with experience replay and <mark>replay buffer</mark> shuffling will induce the same tension between the conflicting updates. Specific ( nonrandom ) batching schemes can cause even greater degrees of delusion ; for example , training in a sequence of batches that run through a batch of transitions at s 4 , followed by batches at s 3 , then s 2 , then s 1 will induce a Q-function that deludes itself into estimating the value of ( s 1 ,<br>2: The actor-critic paradigm trains a value function V θ (s t ) and a policy π θ (a t |s t ), and updates them iteratively by sampling from the <mark>replay buffer</mark>. We employ the popular Proximal Policy Optimization (PPO) (Schulman et al.<br>",
    "Arabic": "المخزون التكراري",
    "Chinese": "经验回放缓冲区",
    "French": "tampon de relecture",
    "Japanese": "リプレイバッファ",
    "Russian": "буфер воспроизведения"
  },
  {
    "English": "replay memory",
    "context": "1: For the DQN, we use the dev set to tune all parameters. We used a <mark>replay memory</mark> D of size 500k, and a discount (γ) of 0.8. We set the learning rate to 2.5E −5 . The in -greedy exploration is annealed from 1 to 0.1 over 500k transitions.<br>",
    "Arabic": "ذاكرة الإعادة",
    "Chinese": "重播记忆",
    "French": "mémoire de rejeu",
    "Japanese": "リプレイメモリ",
    "Russian": "буфер воспроизведения"
  },
  {
    "English": "representation",
    "context": "1: The simplest basis functions are constant functions -and our first example of a <mark>representation</mark> is the trivial <mark>representation</mark> ρ 0 : G → R which maps every element of G to 1.<br>2: (2020), and couple the transformer with a feed-forward network outputting a single scalar for each answer. Let x i CLS ∈ R H be the <mark>representation</mark> of the sequence start token (i.e., [CLS] or <s>) for the concatenation of the premise/question and the i-th answer.<br>",
    "Arabic": "تمثيل",
    "Chinese": "表示",
    "French": "représentation",
    "Japanese": "表現",
    "Russian": "представление"
  },
  {
    "English": "representation learning",
    "context": "1: Each program can be used to construct decoder tasks for self-supervised learning (Section 3.3). Our framework is inspired by the data programming paradigm [33], which applies programs to training set creation. In comparison, our framework uses task programming to unify expert-engineered programs, which encode structured expert knowledge, with <mark>representation learning</mark>.<br>2: While these attempts can be considered early forms of <mark>representation learning</mark> in random forests, their prediction accuracies remained below the state-of-the-art. In this work we present Deep Neural Decision Forestsa novel approach to unify appealing properties from <mark>representation learning</mark> as known from deep architectures with the divide-and-conquer principle of decision trees.<br>",
    "Arabic": "تعلم التمثيل",
    "Chinese": "表征学习",
    "French": "apprentissage de la représentation",
    "Japanese": "表現学習",
    "Russian": "обучение представлений"
  },
  {
    "English": "representation matrix",
    "context": "1: We introduce a variant of the LBL that makes use of additive representations ( §2) by associating the composed word vectorsr andq j with the target and context words, respectively. The <mark>representation matrices</mark> Q (f ) , R (f ) ∈ R |F |×d thus contain a vector for each factor type.<br>",
    "Arabic": "مصفوفة التمثيل",
    "Chinese": "表示矩阵",
    "French": "matrice de représentation",
    "Japanese": "表現行列",
    "Russian": "матрица представления"
  },
  {
    "English": "representation space",
    "context": "1: If A and X are closer to each other in a model's <mark>representation space</mark> than B and X, the model's prediction is correct, otherwise it is not. Irrespective of the test units (phones, words), an acoustic segment in our model is represented by a single vector.<br>2: This indicates that modality is not the key driver of similarity in <mark>representation space</mark>; rather, the level of abstraction can be more salient. This also indicates that the process of abstraction changes the representation in similar ways across language and programs. Gap between performance on human-generated and machine-generated tasks. Higher gap indicates more human-like behavior of the model.<br>",
    "Arabic": "مساحة التمثيل",
    "Chinese": "表征空间",
    "French": "espace de représentation",
    "Japanese": "表現空間",
    "Russian": "пространство представлений"
  },
  {
    "English": "representation vector",
    "context": "1: This way, every component of the <mark>representation vector</mark> gives a value of the importance relation between a document and the relevant base concept. A concrete example can be explained starting from the light ontology represented in Figures 1 and 2, and by considering a document D 1 containing concepts \"xxyyyz\".<br>2: This class-based model, CLBL, extends over the LBL by associating a <mark>representation vector</mark> s c and bias parameter t c to each class c, such that Θ CLBL = (C j , Q, R, S, b, t).<br>",
    "Arabic": "مُتجه التمثيل",
    "Chinese": "表征向量",
    "French": "vecteur de représentation",
    "Japanese": "表現ベクトル",
    "Russian": "вектор представления"
  },
  {
    "English": "representer theorem",
    "context": "1: Note that the <mark>representer theorem</mark> (Kimeldorf & Wahba, 1971;Schölkopf & Smola, 2002) applies to this case also: any solution f * that minimizes (17) can be written in the form \n f * (x) = m i=1 α i k(x, x i ) (18) \n<br>",
    "Arabic": "\"نظرية المُمثل\"",
    "Chinese": "再现定理",
    "French": "théorème du représentant",
    "Japanese": "リプレゼンター定理",
    "Russian": "теорема представителя"
  },
  {
    "English": "reproduce kernel hilbert space",
    "context": "1: 2 The function class F d for the function f is chosen to be a unit-norm ball in a <mark>reproducing kernel Hilbert space</mark> (RKHS) in [9,22].<br>2: The RKHS H k (D) is a complete subspace of L 2 (D) of nicely behaved functions, with an inner product •, • k obeying the reproducing property: f, k(x, •) k = f (x) for all f ∈ H k (D).<br>",
    "Arabic": "مساحة هيلبرت النواة المُنتَجة",
    "Chinese": "再生核希尔伯特空间",
    "French": "espace de Hilbert à noyau reproduisant",
    "Japanese": "再生カーネルヒルベルト空間",
    "Russian": "воспроизводящее ядро гильбертового пространства (RKHS)"
  },
  {
    "English": "reproduce property",
    "context": "1: Then for all functions f ∈ F and x ∈ X we have the <mark>reproducing property</mark>: f, ϕ(x) F = f (x), i.e. the evaluation of function f at x can be written as an inner product.<br>",
    "Arabic": "خاصية التكرار",
    "Chinese": "再现性质",
    "French": "propriété de reproduction",
    "Japanese": "再生特性",
    "Russian": "воспроизводящее свойство"
  },
  {
    "English": "reprojection",
    "context": "1: Given the 2D <mark>reprojection</mark> p ij = Π (R i P j + t i , C i ), this formulation loads in memory the dense features F i , interpolates them at p ij , and compute the residuals r ij = F i p ij − f j for the cost E ij = r ij γ .<br>2: Specifically, D3DP [42] proposed a multi-hypothesis aggregation with joint-wise <mark>reprojection</mark> to determine the best hypothesis from the diffusion model using the 2D prior. DiffPoses [17,21] both introduced a heatmap representation of 2D joints to condition the reverse diffusion process.<br>",
    "Arabic": "إعادة الإسقاط",
    "Chinese": "重投影",
    "French": "reprojection",
    "Japanese": "再投影",
    "Russian": "репроекция"
  },
  {
    "English": "reprojection error",
    "context": "1: where r i (y) = π(Rx 3D i + t) − x 2D i (unweighted <mark>reprojection error</mark>), and (•) •2 stands for element-wise square.<br>2: which a weighted PnP problem can be formulated to estimate the object pose relative to the camera. The essence of a PnP layer is searching for an optimal pose y (expanded as rotation matrix R and translation vector t) that minimizes the cumulative squared weighted <mark>reprojection error</mark>: \n<br>",
    "Arabic": "خطأ إعادة الإسقاط",
    "Chinese": "重投影误差",
    "French": "erreur de reprojection",
    "Japanese": "再投影誤差",
    "Russian": "ошибка репроекции"
  },
  {
    "English": "reranker",
    "context": "1: We test the lexicalised Charniak parser plus <mark>reranker</mark> (Charniak and Johnson, 2005) on the development set sentences. We also test the Berkeley parser with an SM6 grammar. The f-scores are shown in Table 4.<br>2: In order to compare our method with past work, we obtained code from Burkett et al. (2010) and reproduced their experiment setting for the OntoNotes data. An extra set of 5,000 unannotated parallel sentence pairs are used for training the <mark>reranker</mark>, and the <mark>reranker</mark> model selection was performed on the development dataset.<br>",
    "Arabic": "معيد الترتيب",
    "Chinese": "重排序器 (reranker)",
    "French": "réordonnanceur",
    "Japanese": "再ランカー",
    "Russian": "реранкер"
  },
  {
    "English": "reranking model",
    "context": "1: (2010) proposed a similar framework with a \"multi-view\" learning scheme where k-best outputs of two monolingual taggers are reranked using a complex selftrained <mark>reranking model</mark>. In our work, we propose a simple decoding method based on Gibbs sampling that eliminates the need for training complex <mark>reranking model</mark>s.<br>2: Previous work has made use of various restrictions or approximations that allow efficient training of GLMs for parsing. This section describes the relationship between our work and this previous work. In reranking approaches , a first-pass parser is used to enumerate a small set of candidate parses for an input sentence ; the <mark>reranking model</mark> , which is a GLM , is used to select between these parses ( e.g. , ( Ratnaparkhi et al. , 1994 ; Johnson et al. , 1999 ; Collins , 2000 ; Charniak and Johnson , 2005 )<br>",
    "Arabic": "نموذج إعادة الترتيب",
    "Chinese": "重排序模型",
    "French": "modèle de reclassement",
    "Japanese": "再ランキングモデル",
    "Russian": "модель повторного ранжирования"
  },
  {
    "English": "reranking parser",
    "context": "1: The <mark>reranking parser</mark> takes the k-best lists of candidate trees or a packed forest produced by a baseline parser (usually a generative model), and then reranks the candidates using arbitrary features. Hence, we can expect that combining our SR-TSG model with a discriminative <mark>reranking parser</mark> would provide better performance than SR-TSG alone.<br>",
    "Arabic": "محلل إعادة الترتيب",
    "Chinese": "重排序分析器",
    "French": "analyseur de reclassement",
    "Japanese": "再順位付けパーサ",
    "Russian": "парсер переранжирования"
  },
  {
    "English": "reservoir sampling",
    "context": "1: Continual learning wishes the model to aggregate knowledge in a new domain without forgetting the previous knowledge in old domains. For this purpose, three kinds of methods are proposed. The first is the replay method. A <mark>reservoir sampling</mark> method is proposed in (Rolnick et al. 2018) to limit the number of samples stored.<br>2: ρ ) ∇ a α ρ Q ( o , a ; θ ) , \n where π α ρ is agent α's policy in role ρ. During training , the advising feedback nonstationarities mentioned earlier are handled as follows : in Phase I , tasklevel policies are trained online ( i.e. , no replay memory is used so impact of advice on task-level policies is immediately observed by agents ) ; in Phase II , centralized advising-level learning reduces nonstationarities due to teammate learning , and <mark>reservoir sampling</mark> is used<br>",
    "Arabic": "أخذ عينات من الخزان",
    "Chinese": "蓄水池采样",
    "French": "échantillonnage de réservoir",
    "Japanese": "リザーバーサンプリング",
    "Russian": "выборка из резервуара"
  },
  {
    "English": "residual block",
    "context": "1: For contrasting patches against one another, we spatially mean-pool the representations of each patch. Before applying the linear logistic regression classifier on the output of the third <mark>residual block</mark>, we spatially mean-pool the created representations of size 7 × 7 × 1024 again.<br>2: Unsupervised pre-training. Our implementation follows the practice of existing works [36,17,8,9,15]. Data augmentation. We describe data augmentation using the PyTorch [31] [8], we initialize the scale parameters as 0 [14] in the last BN layer for every <mark>residual block</mark>. Weight decay.<br>",
    "Arabic": "كتلة متبقية",
    "Chinese": "残差块",
    "French": "bloc résiduel",
    "Japanese": "残差ブロック",
    "Russian": "остаточный блок"
  },
  {
    "English": "residual branch",
    "context": "1: We used no gradient clipping and a weight decay of 0.1. Unlike [2] which specified different dropout rates for different parameters, we used a constant dropout rate of 0.25 throughout the network, including before every linear layer and on the <mark>residual branches</mark>.<br>",
    "Arabic": "الفرع المتبقي",
    "Chinese": "残差分支",
    "French": "branche résiduelle",
    "Japanese": "残余ブランチ",
    "Russian": "остаточная ветвь"
  },
  {
    "English": "residual connection",
    "context": "1: Fuse dropout and adding residual in the <mark>residual connection</mark> at the end on the attention and FFN blocks. We train with DeepSpeed [88] ZeRO optimizer stage 1 to shard the optimizer states, thus reducing GPU memory usage and allowing us to use larger batch sizes.<br>2: (7), D t ds is upsampled to 1 /4 size of B. We further add D t ds with block input F t−1 as a <mark>residual connection</mark>, and the resulting feature F t is passed to the next block. Instance-level occupancy. It represents the occupancy with each agent's identity preserved.<br>",
    "Arabic": "اتصال متبقي",
    "Chinese": "残差连接",
    "French": "connexion résiduelle",
    "Japanese": "残留接続",
    "Russian": "остаточное соединение"
  },
  {
    "English": "residual error",
    "context": "1: This added stability of DQRA comes from that the <mark>residual error</mark> E td D (f, f, π) is a fixed rather than a changing objective. This stabilization overcomes potential biases due to the challenges (related to double sampling) in unbiased gradient estimation of the RA objective.<br>",
    "Arabic": "الخطأ المتبقي",
    "Chinese": "剩余误差",
    "French": "erreur résiduelle",
    "Japanese": "残差誤差",
    "Russian": "остаточная ошибка"
  },
  {
    "English": "residual function",
    "context": "1: (1). But we will show by experiments that the identity mapping is sufficient for addressing the degradation problem and is economical, and thus W s is only used when matching dimensions. The form of the <mark>residual function</mark> F is flexible. Experiments in this paper involve a function F that has two or three layers (Fig.<br>2: We first define the type of problems solvable by our maximum consensus algorithm. We require that the <mark>residual function</mark> r i (θ) be pseudoconvex. This is known to include many common applications [17]. Examples are as follows.<br>",
    "Arabic": "الدالة المتبقية",
    "Chinese": "残差函数",
    "French": "fonction résiduelle",
    "Japanese": "残差関数",
    "Russian": "остаточная функция"
  },
  {
    "English": "residual graph",
    "context": "1: 0. Thus, the value of the maximum flow from s to t in G is K. Let G 0 be the <mark>residual graph</mark> obtained from G after pushing the flow K. Let E 0 ðx 1 ; x 2 Þ be the function exactly represented by G 0 ; V 0 .<br>",
    "Arabic": "الرسم البياني المتبقي",
    "Chinese": "残差图",
    "French": "graphe résiduel",
    "Japanese": "残差グラフ",
    "Russian": "остаточный граф"
  },
  {
    "English": "residual learning",
    "context": "1: First, the situation is reversed with <mark>residual learning</mark> -the 34-layer ResNet is better than the 18-layer ResNet (by 2.8%). More importantly, the 34-layer ResNet exhibits considerably lower training error and is generalizable to the validation data.<br>2: The role of the convonlutional layers is to generate the missing details in (x n+1 ) ↑ r (<mark>residual learning</mark> [22,57]). Namely, G n performs the operatioñ \n x n = (x n+1 ) ↑ r + ψ n (z n + (x n+1 ) ↑ r ) ,(3) \n<br>",
    "Arabic": "التعلم المتبقي",
    "Chinese": "残差学习",
    "French": "apprentissage résiduel",
    "Japanese": "残差学習",
    "Russian": "остаточное обучение"
  },
  {
    "English": "residual network",
    "context": "1: (vi) An encoder-decoder <mark>residual network</mark> that creates the output image. Our method is related to the recent work of [9], who create images based on scene graphs. Their method also uses a graph convolutional network to obtain masks, a mul-Figure 1. An example of the image creation process.<br>2: The absolute difference between the estimated pupil center of synthetic and corresponding refined image is quite small: 1.1 ± 0.8px (eye width=55px). Implementation Details: The refiner network, R θ , is a <mark>residual network</mark> (ResNet) [12].<br>",
    "Arabic": "الشبكة المتبقية",
    "Chinese": "残差网络",
    "French": "réseau résiduel",
    "Japanese": "残差ネットワーク",
    "Russian": "остаточная сеть"
  },
  {
    "English": "restricted isometry property",
    "context": "1: Several valid reconstruction algorithms are known for compression matrices that satisfy a <mark>restricted isometry property</mark>.<br>",
    "Arabic": "خاصية متساوي القياس المقيدة",
    "Chinese": "限制等距性质",
    "French": "propriété d'isométrie restreinte",
    "Japanese": "制限等距離性条件",
    "Russian": "свойство ограниченной изометрии"
  },
  {
    "English": "retrieval",
    "context": "1: The success rate of B1 is equivalent to an \"Oracle\", which gives the best possible \"<mark>retrieval</mark>\" of the starting problem to reach the target problem, for a given set of anchors.<br>2: On TriviaQA (Joshi et al., 2017) we show results for both the filtered (previously used in <mark>retrieval</mark> and open-book work) and unfiltered set (previously used in large language model evaluations). In both cases, Chinchilla substantially out performs Gopher.<br>",
    "Arabic": "استرجاع",
    "Chinese": "检索",
    "French": "récupération",
    "Japanese": "取得",
    "Russian": "извлечение"
  },
  {
    "English": "retrieval function",
    "context": "1: As an example, if some document d is at rank 4 given query q and using <mark>retrieval function</mark> f1 then φ f 1 rank (d, q) = [0, 0, 0, 1, . . . , 1] T .<br>2: where Φ(di, q) (which we define later) is a function that maps documents and queries to a feature vector. Intuitively , it can be thought of as a feature vector describing the quality of the match between a document di and the query q. w is a weight vector that assigns weights to each of the features in Φ , thus giving us a real valued <mark>retrieval function</mark> where a higher score indicates a document di is estimated to be more relevant to<br>",
    "Arabic": "وظيفة الاسترجاع",
    "Chinese": "检索函数",
    "French": "fonction de récupération",
    "Japanese": "検索関数",
    "Russian": "функция извлечения"
  },
  {
    "English": "retrieval method",
    "context": "1: In general, we find that the empirical performance of a retrieval formula is tightly related to how well it satisfies these constraints. Thus the proposed constraints provide a good explanation of many empirical observations about <mark>retrieval methods</mark>.<br>",
    "Arabic": "طريقة الاسترجاع",
    "Chinese": "检索方法",
    "French": "méthode de récupération",
    "Japanese": "検索手法",
    "Russian": "метод извлечения"
  },
  {
    "English": "retrieval model",
    "context": "1: We focus on the QA model itself and not on the <mark>retrieval model</mark>, so we always use the \"gold\" passage as the context, assuming an oracle retrieval system. We use the examples that have both a gold passage and a short answer (35% of the data).<br>2: (1) \n Such a preference judgment indicates that di is preferred over dj given q. As our <mark>retrieval model</mark>, we chose a linear retrieval function: \n rel(di, q) = w • Φ(di, q)(2) \n<br>",
    "Arabic": "نموذج الاسترجاع",
    "Chinese": "检索模型",
    "French": "modèle de recherche",
    "Japanese": "検索モデル",
    "Russian": "модель извлечения"
  },
  {
    "English": "retrieval system",
    "context": "1: To study the ability of <mark>retrieval systems</mark> to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations.<br>2: Especially for shared tasks that do not attract diverse submissions, TIREx can help to produce a more diverse judgment pool, as a wide range of different baseline <mark>retrieval systems</mark> is directly available and can be applied to any imported retrieval task. Common Problems and Pitfalls in IR Experiments.<br>",
    "Arabic": "نظام الاسترجاع",
    "Chinese": "检索系统",
    "French": "système de récupération",
    "Japanese": "情報検索システム",
    "Russian": "система поиска"
  },
  {
    "English": "retrieval-augment generation",
    "context": "1: We also include <mark>Retrieval-Augmented Generation</mark> , a model that jointly learns to retrieve and generate answers in a seq2seq framework. Finally we include Fusion-in-Decoder (FID) (Izacard and Grave, 2020), a pipeline model which retrieves 100 documents and fuses them so that the decoder can attend to all documents at once.<br>",
    "Arabic": "إنشاء مدعم باسترجاع",
    "Chinese": "检索增强生成",
    "French": "génération augmentée par récupération",
    "Japanese": "検索増強生成 (Retrieval-Augmented Generation)",
    "Russian": "увеличение поиска и генерации"
  },
  {
    "English": "reverse-mode",
    "context": "1: Franceschi et al. , 2018 ; Liu et al. , 2018 ; Shaban et al. , 2019 ) , and training learned optimizers ( Li & Malik , 2016 ; Andrychowicz et al. , 2016 ; Wichrowska et al. , 2017 ; Metz et al. , 2018 ; 2020b ; a ) . Many methods exist for computing gradients in such computation graphs , including ones based on <mark>reverse-mode</mark> ( Williams & Peng , 1990 ; Tallec & Ollivier , 2017b ; Aicher et al. , 2019 ; Grefenstette et al. , 2019 ) and forward-mode ( Williams & Zipser , 1989 ; Tallec & Ollivier , 2017a ; Mujika et al. , 2018 ; 1 University<br>",
    "Arabic": "وضع العكس",
    "Chinese": "逆向模式",
    "French": "mode de rétropropagation",
    "Japanese": "逆モード",
    "Russian": "обратный режим"
  },
  {
    "English": "reward",
    "context": "1: Namely, a next state s ′ of M is sampled from P (s, a, •), then a new external parameter θ is sampled from µ s ′ and the principal sends a signal g ∼ π s ′ (θ). Meanwhile, the following <mark>reward</mark> is yielded for the agent: \n<br>2: Observe that in the section between s ′ u 1 and s ′′ u l in T l , the agent obtains <mark>reward</mark> 1 (i.e., reaches some s ′ u , u ∈ N * ) in at least every four steps given that N * is an independent set.<br>",
    "Arabic": "مكافأة",
    "Chinese": "奖励",
    "French": "récompense",
    "Japanese": "報酬",
    "Russian": "награда"
  },
  {
    "English": "reward function",
    "context": "1: <mark>Reward Function</mark>: R(s, a; θ) is an instantaneous reward function of action a at state s. We model R as the inner product between a vector of features f (s, a) and a vector of weights θ.<br>2: Environment-specific features, such as whether an object is currently in focus, are useful when selecting the object to manipulate. In total, there are 4,438 features. <mark>Reward Function</mark> Environment feedback can be used as a reward function in this domain. An obvious reward would be task completion (e.g., whether the stated computer problem was fixed).<br>",
    "Arabic": "دالة المكافأة",
    "Chinese": "奖励函数",
    "French": "fonction de récompense",
    "Japanese": "報酬関数",
    "Russian": "функция вознаграждения"
  },
  {
    "English": "reward model",
    "context": "1: We implement the latent plate dynamics model using the recurrent state space model from [37], with 64 × 64 input images and 30-dimensional diagonal Gaussian latent variables. This is a multi-headed deep recurrent network comprised of a learned encoder, transition model, and <mark>reward model</mark>. We supervise each head of the network with the following objectives: \n<br>2: Various algorithms for PbRL exist, including methods that can reuse off-policy preference data, but generally involve first explicitly estimating the latent scoring function (i.e. the <mark>reward model</mark>) and subsequently optimizing it [16,9,12,34,19]. We instead present a single stage policy learning approach that directly optimizes a policy to satisfy preferences.<br>",
    "Arabic": "نموذج المكافأة",
    "Chinese": "奖励模型",
    "French": "modèle de récompense",
    "Japanese": "報酬モデル",
    "Russian": "модель вознаграждения"
  },
  {
    "English": "reward shaping",
    "context": "1: ( s ′ ) − Φ ( s ) . Potential shaping is widely used for <mark>reward shaping</mark>. We next define two classes of transformations that were used by Skalse et al. (2022a), starting with S ′ -redistribution. Definition 2.4 (S ′ -Redistribution).<br>",
    "Arabic": "تشكيل المكافأة",
    "Chinese": "奖励塑形",
    "French": "façonnement de la récompense",
    "Japanese": "報酬成形",
    "Russian": "формирование награды"
  },
  {
    "English": "reward signal",
    "context": "1: Instead, only the parameterization ofφ, the estimator of the underlying structure in action space, must be modified when new actions become available. We show next that the update to the parameters ofφ can be performed using supervised learning methods that are independent of the <mark>reward signal</mark> and thus typically more efficient than RL methods.<br>2: Killing an opponent generates a reward of 10 points, and winning the game generates a reward equal to the team's remaining total health plus 200. This damage-based <mark>reward signal</mark> is comparable to that used by Usunier et al. (2016). Unlike (Peng et al. 2017), our approach does not require estimating local rewards.<br>",
    "Arabic": "إشارة المكافأة",
    "Chinese": "奖赏信号",
    "French": "signal de récompense",
    "Japanese": "報酬信号",
    "Russian": "сигнал вознаграждения"
  },
  {
    "English": "reward-maximize policy",
    "context": "1: We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a <mark>reward-maximizing policy</mark> toward a human imitationlearned policy. We prove that this is a no-regret learning algorithm under a modified utility function.<br>",
    "Arabic": "سياسة تعظيم المكافأة",
    "Chinese": "奖励最大化策略",
    "French": "politique de maximisation de la récompense",
    "Japanese": "報酬を最大化する方策",
    "Russian": "политика максимизации вознаграждения"
  },
  {
    "English": "rhetorical structure theory",
    "context": "1: We assume that there exist appropriate structures (one or more) of argumentative discourse that lead to agreement and we think that these structures could be detected with the help of <mark>Rhetorical Structure Theory</mark> relations. First step in this approach is building and analyzing an argumentative corpus.<br>2: We create a small argumentative corpus and use <mark>Rhetorical Structure Theory</mark> relations to annotate it. We, as well, introduce few novel rhetorical relations that we think reflect in a clearer way speaker's intention within question-answering act. In our study we use data (web discussions) taken from Wikipedia talk pages.<br>",
    "Arabic": "نظرية البنية البلاغية",
    "Chinese": "修辞结构理论",
    "French": "théorie de la structure rhétorique",
    "Japanese": "修辞構造理論",
    "Russian": "теория риторической структуры"
  },
  {
    "English": "ridge regression",
    "context": "1: Here we give a brief overview of the different methods and point out some of their expected strengths and weaknesses: \n • <mark>Ridge regression</mark> (Hoerl and Kennard 1970) and the Bayesian Lasso (Park and Casella 2008;Perez and de los Campos 2014) are well adapted to dense signals, so should be competitive in such settings.<br>2: We describe below some successful computational strategies while pointing out their potential shortcomings. In <mark>Ridge regression</mark>, for any x in R p , z → x β(z) is a linear function of z, implying that E i (z) is piecewise linear.<br>",
    "Arabic": "انحدار ريدج",
    "Chinese": "岭回归",
    "French": "régression ridge",
    "Japanese": "リッジ回帰",
    "Russian": "Ридж-регрессия"
  },
  {
    "English": "ridge regularization",
    "context": "1: Benchmarking conformal sets for MLP regression models with a <mark>ridge regularization</mark> on real datasets. The parameter of the model is obtained after T = n/10 iterations of stochastic gradient descent. For stabCP, we use a stability bound estimate τi = T xi /(n+1).<br>2: Figure 8. Benchmarking conformal sets for Gradient Boosting regression models with a <mark>ridge regularization</mark> on real datasets. For stabCP, we use a rough stability bound estimate τi ≈ xi /10. We display the lengths of the confidence sets over 100 random permutation of the data.<br>",
    "Arabic": "تنظيم الحافة",
    "Chinese": "岭正则化",
    "French": "régularisation ridge",
    "Japanese": "リッジ正則化",
    "Russian": "гребневая регуляризация"
  },
  {
    "English": "riemannian geometry",
    "context": "1: In what follows we describe the experimental settings used to generate results introduced in Sec. 6. The models and experiments have been implemented in Jax (Bradbury et al., 2018), using a modified version of the <mark>Riemannian geometry</mark> library Geomstats (Miolane et al., 2020). Anonymized code can be found at here 10 .<br>2: In Section 2, we briefly describe the covariance descriptors. In Section 3, we present an introduction to <mark>Riemannian geometry</mark> focussing on the space of symmetric positive definite matrices. In Sections 4 and 5, we describe our algorithm for classification on Riemannian manifolds and its application to human detection. The experiments are presented in Section 6.<br>",
    "Arabic": "\"هندسة ريمانية\"",
    "Chinese": "黎曼几何学",
    "French": "géométrie riemannienne",
    "Japanese": "リーマン幾何学",
    "Russian": "Геометрия Римана"
  },
  {
    "English": "riemannian gradient",
    "context": "1: the volume form) given by dp ref /dVol M (x) ∝ e −U (x) (Durmus, 2016, Section 2.4), where ∇ is the <mark>Riemannian gradient</mark> 5 . Two simple choices for U (x) present themselves.<br>2: Here, we give the <mark>Riemannian gradient</mark>, the Riemannian Hessian for Problem (12) and the used retraction. The <mark>Riemannian gradient</mark> of q is given by grad q ( r ) = P TrS n ∇q ( r ) = ( I − rr T ) ( 2Hr + 2g ) , ( 18 ) where P TrS n denotes the orthogonal projection onto the tangent space at r with T r S n = { s : s T r =<br>",
    "Arabic": "التدرج الريماني",
    "Chinese": "黎曼梯度",
    "French": "gradient riemannien",
    "Japanese": "リーマン勾配",
    "Russian": "римановский градиент"
  },
  {
    "English": "riemannian manifold",
    "context": "1: Let {X i } i=1...N be the set of points on a <mark>Riemannian manifold</mark> M. Similar to Euclidean spaces, the Karcher mean [10] of points on <mark>Riemannian manifold</mark>, is the point on M which minimizes the sum of squared distances \n<br>2: The feasible set in Problem ( 12) forms a unit sphere S n = {r ∈ R n+1 : r T r = 1}. When S n is endowed with the Euclidean metric v, u = v T u, the unit sphere is a <mark>Riemannian manifold</mark> (Absil et al., 2008).<br>",
    "Arabic": "مشعب ريماني",
    "Chinese": "黎曼流形",
    "French": "variété riemannienne",
    "Japanese": "リーマン多様体",
    "Russian": "риманово многообразие"
  },
  {
    "English": "right-to-left model",
    "context": "1: In particular, we can reverse the translation direction of the languages, as well as the direction of the language model. We denote our original formulation as a sourceto-target, left-to-right model (S2T/L2R). We can train three variations using target-to-source (T2S) and right-to-left (R2L) models: \n S2T/R2L Π |T | i=1 P ( t i |t i+1 , t i+2 , • • • , s a i , s a i −1 , s a i +1 , • • • ) T2S/L2R Π |S| i=1 P ( s i |s i−1 , s i−2 , • • • , t a i , t a i −1 ,<br>",
    "Arabic": "نموذج من اليمين إلى اليسار",
    "Chinese": "从右到左模型",
    "French": "modèle de droite à gauche",
    "Japanese": "右から左のモデル",
    "Russian": "модель справа налево"
  },
  {
    "English": "rigid body transformation",
    "context": "1: Approximating further, we compute only the block diagonal terms for J d J d , as if the effect of each node on the warp function were independent, resulting in a computational cost of building the structure similar to a single <mark>rigid body transformation</mark> for the frame.<br>2: Applications of orthogonal constraints span various fields, such as protein docking with ligands binding pose prediction (Ganea et al., 2022), robotics and Computer vision with <mark>rigid body transformation</mark> estimation (Barfoot et al., 2011;Prokudin et al., 2018), and medical imaging for data alignment (Hou et al., 2018).<br>",
    "Arabic": "تحويل الجسم الصلب",
    "Chinese": "刚体变换",
    "French": "transformation rigide",
    "Japanese": "剛体変換",
    "Russian": "преобразование твердого тела"
  },
  {
    "English": "rigid transformation",
    "context": "1: The final goal of MAC is to estimate the optimal 6-DoF <mark>rigid transformation</mark> (composed of a rotation pose R * ∈ SO(3) and a translation pose t * ∈ R 3 ) that maximizes the objective function as follow: \n<br>2: where W is a 3D part based warping function, and U k is the set of UV coordinates associated with the k th body part. The body part is defined as a region of the body where its local geometry approximately undergoes a <mark>rigid transformation</mark>, e.g., lower arm.<br>",
    "Arabic": "تحويل صلب",
    "Chinese": "刚体变换",
    "French": "transformation rigide",
    "Japanese": "剛体変換",
    "Russian": "жесткое преобразование"
  },
  {
    "English": "risk minimization",
    "context": "1: Fixed points of <mark>risk minimization</mark> are determined by a combination of user retention function ν and the models θ (t) , and without knowledge of ν it is hard to ensure that a model has a fair fixed point.<br>2: We can specialize the stability condition to obtain an intuitive and negative result for the stability of <mark>risk minimization</mark> (average case). Even if we start at a fair fixed point with \n λ * 1 = • • • = λ * k and R 1 = • • • = R k , if \n<br>",
    "Arabic": "تقليل المخاطر",
    "Chinese": "风险最小化",
    "French": "minimisation du risque",
    "Japanese": "リスク最小化",
    "Russian": "минимизация риска"
  },
  {
    "English": "roberta-large",
    "context": "1: We fine-tune the pre-trained versions of these models released in the huggingface transformers library (Wolf et al., 2020) for token classification / sequence labeling. We took the largest available version for each of them: bert-large-uncased, robertalarge, and mpnet-base. From all, only BERT is pre-trained on uncased text.<br>2: We run all experiments with the same pretrained checkpoint, <mark>roberta-large</mark> (355M parameters) from RoBERTa , which we load from the transformers (Wolf et al., 2020) library.<br>",
    "Arabic": "روبرتا كبيرة",
    "Chinese": "roberta-large",
    "French": "roberta-large",
    "Japanese": "roberta-large",
    "Russian": "roberta-large"
  },
  {
    "English": "robotic",
    "context": "1: Finally, we demonstrate the accuracy and usability of our approach in a set of autonomous driving simulations and a user study of planning and <mark>robotics</mark> experts.<br>2: As a result, the proposed methods, aided by rapid ongoing advances in SPAD technology, will potentially spur wide-spread adoption of single-photon sensors as all-purpose cameras in demanding computer vision and <mark>robotics</mark> applications, where the ability to perform reliably in both photon-starved and photon-flooded scenarios is critical to success.<br>",
    "Arabic": "روبوتية",
    "Chinese": "机器人",
    "French": "robotique",
    "Japanese": "ロボット工学",
    "Russian": "робототехника"
  },
  {
    "English": "robust optimization",
    "context": "1: Online mirror descent (OMD) is a well-studied family of methods that can find approximate minima of convex functions, and also find approximate min-max equilibria of convex-concave games, with high probability, using noisy first-order information [47,40,23,6]. We bring these online learning tools to bear on the problem of finding saddle points in <mark>robust optimization</mark> formulations.<br>2: Our approach, handling general stochastic optimization problems, removes these obstructions. The robust procedure ( 6) is based on distributionally <mark>robust optimization</mark> ideas that many researchers have developed [6,8,27], where the goal (as in <mark>robust optimization</mark> more broadly [5]) is to protect against all deviations from a nominal data model.<br>",
    "Arabic": "الأمثلية المتينة",
    "Chinese": "鲁棒优化",
    "French": "optimisation robuste",
    "Japanese": "頑健最適化",
    "Russian": "робастная оптимизация"
  },
  {
    "English": "robust risk",
    "context": "1: [22] also provide a number of asymptotic results showing relationships between the <mark>robust risk</mark> R n (θ; P n ) and variance regularization, but they do not leverage these results for guarantees on the solutions θ rob n . Notation We collect our notation here.<br>2: A standard result in convex analysis [24,Theorem VI.4.4.2] is that if the vector p * ∈ R n + achieving the supremum in the definition (4) of the <mark>robust risk</mark> is unique, then \n<br>",
    "Arabic": "المخاطر الصلبة",
    "Chinese": "鲁棒风险",
    "French": "risque robuste",
    "Japanese": "ロバストリスク",
    "Russian": "устойчивый риск"
  },
  {
    "English": "robustness",
    "context": "1: As LLMs are deployed across increasingly diverse domains, concerns are simultaneously growing about their trustworthiness. Existing trustworthiness evaluations on LLMs mainly focus on specific perspectives, such as <mark>robustness</mark> [176,181,214] or overconfidence [213]. In this paper , we provide a comprehensive trustworthiness-focused evaluation of the recent LLM GPT-4 3 [ 130 ] , in comparison to GPT-3.5 ( i.e. , ChatGPT [ 128 ] ) , from different perspectives , including toxicity , stereotype bias , adversarial <mark>robustness</mark> , out-of-distribution <mark>robustness</mark> , <mark>robustness</mark> on adversarial demonstrations , privacy , machine ethics , and fairness under different settings<br>2: For fairness considerations, these distributions may represent heterogeneous populations of different protected or socioeconomic attributes; in <mark>robustness</mark> applications, they may capture a learner's uncertainty regarding the true underlying task; and in multi-agent collaborative or federated applications, they may represent agent-specific learning tasks.<br>",
    "Arabic": "الصلابة",
    "Chinese": "鲁棒性",
    "French": "robustesse",
    "Japanese": "頑健性",
    "Russian": "устойчивость"
  },
  {
    "English": "role assertion",
    "context": "1: An ABox A is a finite set of concept assertions A(a) and <mark>role assertions</mark> r(a, b) The semantics is defined as usual in terms of interpretations I, which we define to be a (possibly infinite and) nonempty set of concept and <mark>role assertions</mark>.<br>2: An ABox is a set of concept assertions A(a) and <mark>role assertions</mark> r(a, b) where A is a concept name, r a role name, and a, b are individual names. We use ind(A) to denote the set of all individual names that occur in A.<br>",
    "Arabic": "تأكيد الدور",
    "Chinese": "角色断言",
    "French": "assertion de rôle",
    "Japanese": "役割アサーション",
    "Russian": "утверждение роли"
  },
  {
    "English": "role atom",
    "context": "1: A conjunctive query (CQ) takes the form q(x) ← ϕ(x,ȳ) where ϕ is a conjunction of concept atoms A(x) and <mark>role atoms</mark> r(x, y) with A ∈ N C and r ∈ N R .<br>",
    "Arabic": "ذرة العلاقة",
    "Chinese": "角色原子",
    "French": "atome de rôle",
    "Japanese": "役割原子",
    "Russian": "\"атом роли\""
  },
  {
    "English": "role classification",
    "context": "1: The average reward of reinforcement learning converges in the 120th iteration and is 10.7256. The F1 score or argument identification and <mark>role classification</mark> converge in the 120th iteration. The F1 argument identification subtask is 0.6142, and the F1 of <mark>role classification</mark> is 0.6997. Therefore, our model converges on 120th iterations when we add the RLD module.<br>",
    "Arabic": "تصنيف الأدوار",
    "Chinese": "角色分类",
    "French": "classification des rôles",
    "Japanese": "役割分類",
    "Russian": "ролевая классификация"
  },
  {
    "English": "role name",
    "context": "1: An ABox is a set of concept assertions A(a) and role assertions r(a, b) where A is a concept name, r a <mark>role name</mark>, and a, b are individual names. We use ind(A) to denote the set of all individual names that occur in A.<br>2: Let C, R, and K be countably infinite sets of concept names, <mark>role name</mark>s, and constants. A role R is a <mark>role name</mark> r ∈ R or an inverse role r − with r a <mark>role name</mark>.<br>",
    "Arabic": "اسم الدور",
    "Chinese": "角色名称",
    "French": "nom de rôle",
    "Japanese": "役割名",
    "Russian": "имя роли"
  },
  {
    "English": "roll-out policy",
    "context": "1: (3) where X n 1:t is N-time Monte Carlo search sampled based on the <mark>roll-out policy</mark> G i and the current state, and D i (X n 1:t ; θ d ) is the sentence probability given by the discriminator that X n 1:t is the real i-th type sentimental text.<br>",
    "Arabic": "سياسة الانتشار",
    "Chinese": "展开策略",
    "French": "politique de roll-out",
    "Japanese": "ロールアウトポリシー",
    "Russian": "политика ролаута"
  },
  {
    "English": "rollout",
    "context": "1: For a given parameterisation of the agent, we interact with the environment for N = 16 steps, collecting all observations, rewards, and actions into a <mark>rollout</mark> (Algorithm 1). When the <mark>rollout</mark> is full, the agent update its parameters under the actor-critic loss with SGD as the optimiser (Algorithm 2).<br>2: For a fixed sequence {(s i , a i )} of states and actions obtained from a <mark>rollout</mark> of a given policy, we will denote the empirical return starting in state s i as \n q i := ∞ j=i+1 γ j−i−1 R(s j ).<br>",
    "Arabic": "التنفيذ",
    "Chinese": "滚动",
    "French": "déploiement",
    "Japanese": "ロールアウト",
    "Russian": "развертывание"
  },
  {
    "English": "rollout length",
    "context": "1: reducing the <mark>rollout length</mark> increases the bias of the return estimate and makes credit assignment harder . Thus we kept number of environments and <mark>rollout length</mark> constant.<br>",
    "Arabic": "طول النشر",
    "Chinese": "滚动长度",
    "French": "longueur de déroulement",
    "Japanese": "ロールアウト長",
    "Russian": "Длина развёртывания"
  },
  {
    "English": "root mean square error",
    "context": "1: Typically, the model is trained with 'teacher-forcing', where the autoregressive frame y t−1 passed as input for predictingŷ t is taken from the ground truth acoustic features and not the prediction network's output from the previous frameŷ t−1 . As discussed by , such a system might learn to copy the teacher forcing input or disregard the text en-tirely , which could still optimize Tacotron2 's <mark>root mean square error</mark> function over predicted acoustic features , but result in an untrained or degenerate attention network which is unable to properly generalize to new inputs at inference time when the teacher forcing input is<br>2: For tasks with continuous labels, we use the mean angle error (mErr) for surface normal prediction (SN) (Eigen & Fergus, 2015) and <mark>root mean square error</mark> (RMSE) for the others.<br>",
    "Arabic": "جذر متوسط مربع الخطأ",
    "Chinese": "均方根误差",
    "French": "erreur quadratique moyenne de la racine (RMSE)",
    "Japanese": "二乗平均平方根誤差",
    "Russian": "среднеквадратичная ошибка"
  },
  {
    "English": "root node",
    "context": "1: Basically, the algorithm starts from the <mark>root node</mark> and scans the set of frequent concept sequences once. For each frequent sequence cs = c 1 . . . c l , the algorithm first finds the node cn corresponding to cs = c1 . . . c l−1 .<br>2: In practice, only the <mark>root node</mark> has a large number of children, which cannot exceed the number of concepts N C ; while the number of children of other nodes is usually small.<br>",
    "Arabic": "عقدة الجذر",
    "Chinese": "根节点",
    "French": "nœud racine",
    "Japanese": "根ノード",
    "Russian": "корневой узел"
  },
  {
    "English": "rotation angle",
    "context": "1: This is obtained by decomposing M i as: where θ is the <mark>rotation angle</mark>, σ x and σ y denote scaling, and h is for shearing. The prior on M i is \n M i = σ x 0 0 σ y cos θ − sin θ sin θ cos θ 1 h 0 1 .<br>",
    "Arabic": "زاوية الدوران",
    "Chinese": "旋转角度",
    "French": "angle de rotation",
    "Japanese": "回転角度",
    "Russian": "угол поворота"
  },
  {
    "English": "rotation invariance",
    "context": "1: Another clear example is in automatically learning symmetries, such as <mark>rotation invariance</mark> (van der Wilk et al., 2018;Immer et al., 2022a).<br>",
    "Arabic": "عدم التغير مع الدوران",
    "Chinese": "旋转不变性",
    "French": "invariance à la rotation",
    "Japanese": "回転不変性",
    "Russian": "инвариантность к вращению"
  },
  {
    "English": "rotation matrix",
    "context": "1: where α x , α y describe the angles around the camera axes and R y (α y )R x (α x ) the corresponding <mark>rotation matrices</mark> to correct the initial rotation estimateR obj2cam from object to camera. The perspective corrections give a notable boost in accuracy as reported in Table 7.<br>2: A human pose is represented by 3D rotations of individual joints in the human skeleton. The 3-dimensional rotation group SO(3) has several common vector space representations that are used to describe group elements in practice. Frequently used examples are <mark>rotation matrices</mark>, axis-angle representations or unit quaternions [28].<br>",
    "Arabic": "مصفوفة الدوران",
    "Chinese": "旋转矩阵",
    "French": "matrice de rotation",
    "Japanese": "回転行列",
    "Russian": "матрица вращения"
  },
  {
    "English": "route Transformer",
    "context": "1: Examples include Longformer (Beltagy et al., 2020), Reformer (Kitaev et al., 2020, Linformer  and <mark>Routing Transformer</mark> . In contrast, our work optimizes computational efficiency using recurrence combined with minimal attention and our model can incorporate these attention variants for additional speed improvement.<br>",
    "Arabic": "محول التوجيه",
    "Chinese": "路由变压器",
    "French": "Transformateur de routage",
    "Japanese": "ルーティングトランスフォーマー",
    "Russian": "Маршрутизирующий Трансформер"
  },
  {
    "English": "row vector",
    "context": "1: We note that for the remainder of this section, we transpose C to be a column vector of shape C N or C N ×1 instead of matrix or <mark>row vector</mark> C 1×N as in (1). In other words the SSM is \n<br>",
    "Arabic": "متجه الصف",
    "Chinese": "行向量",
    "French": "vecteur ligne",
    "Japanese": "行ベクトル",
    "Russian": "строковый вектор"
  },
  {
    "English": "rule body",
    "context": "1: First, numeric atoms occurring in a <mark>rule body</mark> are function-free (but comparison atoms and the head can contain arithmetic functions). Second, each numeric variable in a rule occurs in at most one standard body atom. Third, distinct rules in a program use different variables. The third assumption is clearly w.l.o.g.<br>",
    "Arabic": "جسم القاعدة",
    "Chinese": "规则体",
    "French": "corps de règle",
    "Japanese": "ルール本体",
    "Russian": "тело правила"
  },
  {
    "English": "runtime complexity",
    "context": "1: Complexity The <mark>runtime complexity</mark> of the subtree kernel for a pair of graphs is O(n 2 h4 d ), including a comparison of all pairs of nodes (n 2 ), and a pairwise comparison of all matchings in their neighbourhoods in O(4 d ), which is repeated in h iterations.<br>2: The proposed framework takes into account structure at different scales, yet it remains an interesting question how it compares to base kernels in terms of <mark>runtime complexity</mark>. Its computational complexity depends on the complexity of the base kernel and the degeneracy of the graphs under comparison.<br>",
    "Arabic": "التعقيد الزمني للتشغيل",
    "Chinese": "运行时复杂度",
    "French": "complexité de temps d'exécution",
    "Japanese": "ランタイム複雑さ",
    "Russian": "временная сложность"
  },
  {
    "English": "s node",
    "context": "1: For example, when processing the <mark>S node</mark> in the tree above, we may wish to add a prepositional phrase as a third child. We do this with probability P(S → NP VP PP | S → NP VP).<br>",
    "Arabic": "عقدة",
    "Chinese": "S 节点",
    "French": "nœud",
    "Japanese": "Sノード",
    "Russian": "узел S"
  },
  {
    "English": "saddle-point problem",
    "context": "1: More specifically, error due to limits in communication resources enjoys fast convergence rates, as we establish by formulating the optimization problem as a composite <mark>saddle-point problem</mark> with a smooth term for communication and non-smooth term for the optimization of the local functions (see Section 4 and Eq. (21) for more details). Related work.<br>2: In this work we proposed a tight convex relaxation that can be interpreted as a sublabel-accurate formulation of classical multilabel problems. The final formulation is a simple <mark>saddle-point problem</mark> that admits fast primal-dual optimization. Our method maintains sublabel accuracy even after discretization and for that reason outperforms existing spatially continuous methods.<br>",
    "Arabic": "مشكلة النقطة السرجية",
    "Chinese": "鞍点问题",
    "French": "problème du point de selle",
    "Japanese": "鞍点問題",
    "Russian": "проблема седловой точки"
  },
  {
    "English": "saliency",
    "context": "1: A good example illustrating the <mark>saliency</mark> principle is that of a bright circle on a dark background. If the scale is too small then only the white circle is seen, and there is no extrema in entropy.<br>2: Initial observations suggest that label words aggregate information in shallow layers and distribute it in deep layers. 2 To draw a clearer picture of this phenomenon, we design two metrics based on <mark>saliency</mark> Figure 2: Illustration of our hypothesis.<br>",
    "Arabic": "البروز",
    "Chinese": "显著性",
    "French": "saillance",
    "Japanese": "顕著性",
    "Russian": "выразительность"
  },
  {
    "English": "saliency map",
    "context": "1: We demonstrate the toolkit's flexibility and utility by implementing live demos for five interpretation methods (e.g., <mark>saliency maps</mark> and adversarial attacks) on a variety of models and tasks (e.g., masked language modeling using BERT and reading comprehension using BiDAF). These demos, alongside our code and tutorials, are available at https://allennlp.<br>2: We interactively visualize the interpretations using the AllenNLP Demo, a web application for running AllenNLP models. We add HTML and JavaScript components that provide visualizations for <mark>saliency maps</mark> and adversarial attacks. These components are reusable and greatly simplify the process for adding new models and interpretation methods (Section 4).<br>",
    "Arabic": "خريطة الأهمية",
    "Chinese": "显著性地图",
    "French": "carte de saillance",
    "Japanese": "顕著性マップ",
    "Russian": "карта значимости"
  },
  {
    "English": "sample complexity",
    "context": "1: work from this extensive literature is that, we design algorithms and prove rigorous non-asymptotic <mark>sample complexities</mark> for model estimation, in the specific setting of mixture modeling that features (1) a finite set of underlying time-series models, and (2) unknown labels of the trajectories, with no probabilistic assumptions imposed on these latent variables.<br>",
    "Arabic": "تعقيد العينة",
    "Chinese": "样本复杂度",
    "French": "complexité en échantillons",
    "Japanese": "サンプル複雑度",
    "Russian": "объем выборки"
  },
  {
    "English": "sample complexity bind",
    "context": "1: The excess loss requirement is necessary since an online learner can't be expected to predict with any accuracy with too few examples. Essentially, the excess loss S yields a kind of <mark>sample complexity bound</mark>: the weak learner starts obtaining a distinct edge of Ω(γ) over random guessing when T ≫ S γ .<br>2: For example, when ε is sufficiently small, specifically ε ∈ O (1/n) (Assumption 1 ), taking an ε-net only increases the <mark>sample complexity bound</mark> by constant factors versus knowing an ε-net in advance. Additional examples include: \n • Assumption 2 : we know the marginal distribution for all D ∈ D; \n<br>",
    "Arabic": "\"حد تعقيد العينة\"",
    "Chinese": "样本复杂度界",
    "French": "borne de complexité d'échantillonnage",
    "Japanese": "サンプル複雑性バインド",
    "Russian": "предел сложности выборки"
  },
  {
    "English": "sample covariance matrix",
    "context": "1: We also use a Mahalanobis distance parameterized by the inverse of the <mark>sample covariance matrix</mark>. This method is equivalent to first performing a standard PCA whitening transform over the feature space and then computing distances using the squared Euclidean distance.<br>",
    "Arabic": "مصفوفة التغاير العينية",
    "Chinese": "样本协方差矩阵",
    "French": "matrice de covariance d'échantillon",
    "Japanese": "サンプル共分散行列",
    "Russian": "матрица выборочной ковариации"
  },
  {
    "English": "sample efficiency",
    "context": "1: Given the prevalence of large datasets in modern machine learning, it is critical to assess dataset properties to remove redundancies (Gururangan et al., 2018;Li and Vasconcelos, 2019) or biases (Torralba and Efros, 2011;Khosla et al., 2012;Bolukbasi et al., 2016), both of which negatively impact <mark>sample efficiency</mark>.<br>2: Using Dataset Maps, we profile active learning methods and show that they prefer acquiring collective outliers that models are unable to learn, explaining their poor improvements in <mark>sample efficiency</mark> relative to random sampling.<br>",
    "Arabic": "كفاءة العينة",
    "Chinese": "样本效率",
    "French": "efficacité d'échantillonnage",
    "Japanese": "サンプル効率",
    "Russian": "эффективность выборки"
  },
  {
    "English": "sample selection",
    "context": "1: We introduced the parameter based <mark>sample selection</mark> (PBS) approach and its CMM and CBS algorithms that do not deliberately select difficult sentences. Therefore, our intuition was that they should select a sample that leads to an accurate parameter estimation but does not contain a high number of complex structures.<br>",
    "Arabic": "اختيار العينة",
    "Chinese": "样本选择",
    "French": "sélection d'échantillon",
    "Japanese": "サンプル選択",
    "Russian": "выборка образцов"
  },
  {
    "English": "sample space",
    "context": "1: Because discriminator functions in this class exhibit homogeneous smoothness, these losses effectively weight the <mark>sample space</mark> relatively uniformly in importance, the \"Sparse\" region in Figure (1a) vanishes, and linear estimators can perform optimally.<br>2: Notation Throughout this paper, let X be a finite <mark>sample space</mark> of size N := |X |. Letp : X → [0, ∞) be an unnormalized mass function over X and let Z := x∈Xp (x) be its normalizing partition function.<br>",
    "Arabic": "مساحة العينة",
    "Chinese": "样本空间",
    "French": "espace d'échantillonnage",
    "Japanese": "サンプル空間",
    "Russian": "пространство выборки"
  },
  {
    "English": "sample variance",
    "context": "1: The experimental results suggest that DropMessage presents the smallest <mark>sample variance</mark> among all methods, thus achieving the fastest convergence and the most stable performance. This is consistent with the theoretical results in Section 4.2. Information diversity analysis. We conduct experiments to evaluate the importance of information diversity for messagepassing GNN models.<br>2: These noises then add the difficulty of parameter coverage and the unstability of training process. Generally, <mark>sample variance</mark> can be used to measure the degree of stability.<br>",
    "Arabic": "تباين العينة",
    "Chinese": "样本方差",
    "French": "variance d'échantillon",
    "Japanese": "標本分散",
    "Russian": "дисперсия выборки"
  },
  {
    "English": "sample-efficient",
    "context": "1: In these applications, the performance and optimality of a model is measured by its worst test-time performance on a distribution in the set. We are concerned with this fundamental problem of designing <mark>sample-efficient</mark> multi-distribution learning algorithms. The sample complexity of multi-distribution learning differs from that of learning a single distribution in several ways.<br>",
    "Arabic": "كفاءة العينات",
    "Chinese": "样本高效",
    "French": "efficace en termes d'échantillonnage",
    "Japanese": "サンプル効率的",
    "Russian": "эффективный по выборке"
  },
  {
    "English": "sampler",
    "context": "1: proposals, which can dramatically accelerate inference by eliminating most of the \"burn in\" time of traditional <mark>samplers</mark> and enabling rapid mode-switching. We demonstrate Picture on three challenging vision problems: inferring the 3D shape and detailed appearance of faces, the 3D pose of articulated human bodies, and the 3D shape of medially-symmetric objects.<br>2: We performed extensive testing to verify that our implementation produced exactly the same results as previous work, including <mark>samplers</mark>, pre-trained models, network architectures, training configurations, and evaluation. We ran all experiments using PyTorch 1.10.0, CUDA 11.4, and CuDNN 8.2.0 on NVIDIA DGX-1's with 8 Tesla V100 GPUs each.<br>",
    "Arabic": "أخذ العينات",
    "Chinese": "采样器",
    "French": "échantillonneur",
    "Japanese": "サンプラー",
    "Russian": "сэмплер"
  },
  {
    "English": "sampling algorithm",
    "context": "1: We then develop an efficient <mark>sampling algorithm</mark> which makes it possible to apply the method to large-scale datasets.<br>2: Note, however, that a <mark>sampling algorithm</mark> to approximately choose the next element may need time that depends on the sizes of the weights.<br>",
    "Arabic": "خوارزمية أخذ العينات",
    "Chinese": "抽样算法",
    "French": "algorithme d'échantillonnage",
    "Japanese": "サンプリングアルゴリズム",
    "Russian": "алгоритм выборки"
  },
  {
    "English": "sampling-base inference",
    "context": "1: This has been used by Schmidt et al. [25] for deblurring with <mark>sampling-based inference</mark>, which alternates between sampling from p(x|y, z) and p(z|x, y).<br>",
    "Arabic": "استدلال قائم على أخذ العينات",
    "Chinese": "基于采样的推理",
    "French": "inférence à base d'échantillonnage",
    "Japanese": "サンプリングベース推論",
    "Russian": "вывод на основе выборки"
  },
  {
    "English": "satisfiability",
    "context": "1: A pioneering work is NeuroSAT , which shows that graph neural networks (GNNs) have capability to learn the <mark>satisfiability</mark> of SAT instances in specific domain (i.e., whether a group of assignments to the variables exists such that all the clauses are satisfied). (Cameron et al.<br>2: The <mark>satisfiability</mark> rule for the energetic reasoning (3) states that if there exists an interval for which this sum is greater than the energy available in that interval on the resource, then the constraint cannot be satisfied. ∃l, ∃u, MI(I, l, u) > C •(u − l) =⇒ fail \n<br>",
    "Arabic": "تلبية الشروط",
    "Chinese": "可满足性",
    "French": "satisfiabilité",
    "Japanese": "充足可能性",
    "Russian": "удовлетворимость"
  },
  {
    "English": "satisfiability problem",
    "context": "1: The propositional logic has long been recognized as one of the corner stones of reasoning in philosophy and mathematics (Biere et al. 2009). The <mark>satisfiability problem</mark> of propositional logic formulas (SAT) has significant impact on many areas of computer science and artificial intelligence.<br>",
    "Arabic": "مشكلة الإشباع",
    "Chinese": "可满足性问题",
    "French": "problème de satisfiabilité",
    "Japanese": "充足可能性問題",
    "Russian": "проблема выполнимости"
  },
  {
    "English": "scalability",
    "context": "1: Therefore we can conclude that our algorithm significantly outperforms their algorithm in terms of <mark>scalability</mark>.<br>2: Thus, our algorithm significantly outperforms their algorithm in terms of <mark>scalability</mark>. Comparison with the state-of-the-art single-pair and single-source algorithm. Fogaras and Rácz [Fogaras and Rácz 2005] proposed an efficient single-pair algorithm that estimates SimRank scores with Monte Carlo simulation.<br>",
    "Arabic": "قابلية التوسع",
    "Chinese": "可扩展性",
    "French": "extensibilité",
    "Japanese": "拡張性",
    "Russian": "масштабируемость"
  },
  {
    "English": "scalar",
    "context": "1: Note that this expression applies to all (s, a) instances; that is, to express equation ( 7) in matrix form we need to replicate the <mark>scalar</mark>, V (s; θ, β), |A| times.<br>2: (2020), and couple the transformer with a feed-forward network outputting a single <mark>scalar</mark> for each answer. Let x i CLS ∈ R H be the representation of the sequence start token (i.e., [CLS] or <s>) for the concatenation of the premise/question and the i-th answer.<br>",
    "Arabic": "مقياس",
    "Chinese": "标量",
    "French": "scalaire",
    "Japanese": "スカラー",
    "Russian": "скаляр"
  },
  {
    "English": "scalar product",
    "context": "1: To perform localization, we first assume a linear kernel over the histograms. In its canonical form, the corresponding SVM decision function is f (I) = β + i α i h, h i , where . , . denotes the <mark>scalar product</mark> in R K .<br>",
    "Arabic": "ناتج ضرب عددي",
    "Chinese": "标量积",
    "French": "produit scalaire",
    "Japanese": "スカラー積",
    "Russian": "скалярное произведение"
  },
  {
    "English": "scalarization",
    "context": "1: and strictly better in at least one objective Rj ) . One common approach for finding such Pareto-optimal solutions is <mark>scalarization</mark> (c.f., [4]). Here, one picks positive weights λ1 > 0, . . . , λm > 0, and optimizes the objective R(A) = i λiRi(A).<br>",
    "Arabic": "توجيه السكالار",
    "Chinese": "标量化",
    "French": "scalarisation",
    "Japanese": "スカラリゼーション",
    "Russian": "скаляризация"
  },
  {
    "English": "scale dot-product attention",
    "context": "1: (2017) we perform <mark>scaled dot-product attention</mark>: We scale the weights by the inverse square root of their embedding dimension and normalize with the softmax function to produce a distinct distribution for each token over all the tokens in the sentence: \n<br>2: This is because each query label token is inferred as a weighted combination of support label tokens v, based on the similarity between the query and support image tokens q, k. Here, the similarity function σ of Eq. 3 is implemented as <mark>scaled dot-product attention</mark>.<br>",
    "Arabic": "مقياس الاهتمام بالمنتج النقطي",
    "Chinese": "缩放点积注意力",
    "French": "attention à produit scalaire mis à l'échelle",
    "Japanese": "スケールドット積注意",
    "Russian": "масштабированное скалярное произведение внимания"
  },
  {
    "English": "scale factor",
    "context": "1: Intuitively, this <mark>scaling factor</mark> can automatically down-weight the contribution of easy examples during training and rapidly focus the model on hard examples.<br>",
    "Arabic": "عامل التحجيم",
    "Chinese": "缩放因子",
    "French": "facteur d'échelle",
    "Japanese": "スケーリング係数",
    "Russian": "масштабный коэффициент"
  },
  {
    "English": "scale invariance",
    "context": "1: To better understand model generalisation and <mark>scale invariance</mark>, we evaluated the transformer model (T5-large) on a held-out evaluation set whose structure contains more domain elements (see Table 4) or more predicates (see Table 5) than that of the training set.<br>",
    "Arabic": "ثبات المقياس",
    "Chinese": "尺度不变性",
    "French": "invariance d'échelle",
    "Japanese": "尺度不変性",
    "Russian": "инвариантность масштаба"
  },
  {
    "English": "scale parameter",
    "context": "1: Where λ Σ is a <mark>scale parameter</mark> that is linearly annealed from a large value to a small value during training. Representative settings are 5 × 10 −2 and 2 × 10 −3 for the initial and final values of λ Σ , linearly annealed for the first 5k steps of optimization (out of 15k total).<br>2: P (Z ≤ z) = G(z) = exp − exp − z − b a , (5 \n ) \n where b is the location parameter and a the <mark>scale parameter</mark>. The probability density function of the Gumbel is: \n<br>",
    "Arabic": "معامل المقياس",
    "Chinese": "尺度参数",
    "French": "paramètre d'échelle",
    "Japanese": "スケールパラメーター",
    "Russian": "параметр масштаба"
  },
  {
    "English": "scaled dot-product",
    "context": "1: : absolute position embedding term of ViT; rel. pos. : the default settings with an additional relative position bias term (see Eq. ( 4)); app. : the first <mark>scaled dot-product</mark> term in Eq. (4).<br>",
    "Arabic": "منتج النقطة المُقيَّسة",
    "Chinese": "缩放点积",
    "French": "produit scalaire mis à l'échelle",
    "Japanese": "スケールドット積",
    "Russian": "масштабированное скалярное произведение"
  },
  {
    "English": "scan window detector",
    "context": "1: In this experiment, our goal is to detect the presence and location of the object given an HOI activity. To evaluate the effectiveness of our model , we compare our results with two control experiments : a <mark>scanning window detector</mark> as a baseline measure of object detection without any context , and a second experiment in which the approximate location of the person is provided by a pedestrian detector [ 6 ] , hence providing only a co-occurrence context and a very weak<br>2: The collection of N windows are precisely the regions scored by a scanningwindow detector. Write x i for the features extracted from window i. For example, in our experiments x i is a normalized histogram of gradient features (Dalal and Triggs 2005).<br>",
    "Arabic": "كاشف نافذة المسح",
    "Chinese": "滑窗检测器",
    "French": "détecteur à fenêtre de balayage",
    "Japanese": "スキャンウィンドウ検出器",
    "Russian": "детектор сканирующего окна"
  },
  {
    "English": "scene category",
    "context": "1: where each of these distributions is modeled as a Gaussian. We also learn probabilities of object categories given <mark>scene categories</mark> \n ({P (O k |S j ), k = 1 • • • V, j = 1 • • • J}) \n , where V and J are number of object and <mark>scene categories</mark>.<br>2: Each object category imposes a distribution over object's low-level features F o and attributes A o . Similarly, <mark>scene categories</mark> impose a distribution over scene's low-level features F s and attributes A s . However, extracted visual features for scenes are different from ones extracted for objects. We define two disjoint sets of attributes for objects \n<br>",
    "Arabic": "فئة المشهد",
    "Chinese": "场景类别",
    "French": "catégorie de scène",
    "Japanese": "シーンカテゴリ",
    "Russian": "категория сцены"
  },
  {
    "English": "scene classification",
    "context": "1: To what extent are our findings dataset dependent, and would the taxonomy change if done on another dataset? We examined this by finding the ranking of all tasks for transferring to two target tasks of object classification and <mark>scene classification</mark> on our dataset.<br>2: vanishing points) and more abstract ones involving semantics (e.g. <mark>scene classification</mark>). It is critical to note the task dictionary is meant to be a sampled set, not an exhaustive list, from a denser space of all conceivable visual tasks.<br>",
    "Arabic": "تصنيف المشهد",
    "Chinese": "场景分类",
    "French": "classification de scène",
    "Japanese": "シーン分類",
    "Russian": "классификация сцен"
  },
  {
    "English": "scene classifier",
    "context": "1: On the grounds that we use a distribution as the output of classifiers rather than a single class confidence, we do not need to involve the accuracy of neither the object classifier nor the <mark>scene classifier</mark> to tackle the uncertainty output.<br>2: We use the output of a <mark>scene classifier</mark> from [32] (GoogLeNet model) on every frame from the wearable camera. If the mean <mark>scene classifier</mark> probability for a scene type is above a threshold ρ g for 20 consecutive image frames, then we add the current state s t to the set of goals S g .<br>",
    "Arabic": "مُصَنِّف المشهد",
    "Chinese": "场景分类器",
    "French": "classificateur de scènes",
    "Japanese": "シーン分類器 (Scene classifier)",
    "Russian": "классификатор сцен"
  },
  {
    "English": "scene flow",
    "context": "1: This is particularly interesting in case of <mark>scene flow</mark>, since we have four views of the scene (2 cameras at 2 time steps). Hence, even if a pixel is occluded in a subset of the views, there may still be a view pair where no occlusion takes place.<br>2: The KITTI dataset provides a very challenging testbed for today's stereo, optical flow and <mark>scene flow</mark> algorithms: First, pixel displacements in the data set are large in general, exceeding 150 pixels for stereo and 250 pixels for optical flow.<br>",
    "Arabic": "تدفق المشهد",
    "Chinese": "场景流",
    "French": "flot de scène",
    "Japanese": "シーンフロー",
    "Russian": "поток сцены"
  },
  {
    "English": "scene flow estimation",
    "context": "1: Following prior work in stereo [4], we argue that most scenes of interest consist of regions with a consistent motion pattern, into which they can be segmented -at least implicitly -during <mark>scene flow estimation</mark>.<br>2: segment fixed . Note that while the model is based on segments, the aim here is not segmentation into semantic objects. Rather, the segments support accurate <mark>scene flow estimation</mark>.<br>",
    "Arabic": "تقدير تدفق المشهد",
    "Chinese": "场景流估计",
    "French": "estimation du flot de scène",
    "Japanese": "シーンフロー推定",
    "Russian": "оценка потока сцены"
  },
  {
    "English": "scene geometry",
    "context": "1: In this paper, we have presented an integrated system for dynamic 3D scene analysis from a moving platform. We have proposed a novel method to fuse the output of multiple single-view object detectors and to integrate continuously reestimated <mark>scene geometry</mark> constraints.<br>2: (1) While existing depth estimators are highly responsive to predict holistic <mark>scene geometry</mark>, it is shown [30] that its expressibility is limited at encoding fine local geometry such as irregular and complex wrinkles, that constitute the dominant factor of realism.<br>",
    "Arabic": "هندسة المشهد",
    "Chinese": "场景几何",
    "French": "géométrie de la scène",
    "Japanese": "シーンジオメトリー",
    "Russian": "геометрия сцены"
  },
  {
    "English": "scene graph",
    "context": "1: First, VisText offers three representations of charts: a rasterized image and backing data table, as in previous work; and a <mark>scene graph</mark>, a hierarchical representation akin to a web page's Document Object Model (DOM), that presents an attractive midpoint between the affordances of chart-as-image and chart-as-data-table.<br>2: We introduce a method for the generation of images from an input <mark>scene graph</mark>. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the <mark>scene graph</mark>, have higher visual quality, and support more complex <mark>scene graph</mark>s.<br>",
    "Arabic": "رسم بياني للمشهد",
    "Chinese": "场景图",
    "French": "graphe de scène",
    "Japanese": "シーングラフ",
    "Russian": "граф сцены"
  },
  {
    "English": "scene parsing",
    "context": "1: Since these top matches are labeled, we transfer the annotation (c) of the top matches to the query image and obtain the <mark>scene parsing</mark> result in (d). For comparison, the ground-truth user annotation of the query is displayed in (e).<br>2: Again, the warped image (d) looks similar to the input, indicating that SIFT flow successfully matches image structures. The <mark>scene parsing</mark> results output from our system are listed in column (e) with parameter setting K = 50, M = 5, α = 0.1, β = 70.<br>",
    "Arabic": "تحليل المشهد",
    "Chinese": "场景解析",
    "French": "analyse de scène",
    "Japanese": "シーン解析",
    "Russian": "семантическое разбиение сцены"
  },
  {
    "English": "scene recognition",
    "context": "1: The generative networks focus on generating images using a random noise vector; thus, in contrast to our method, the generated images do not have any annotation information that can be used for training a machine learning model. Many efforts have explored using synthetic data for various prediction tasks , including gaze estimation [ 43 ] , text detection and classification in RGB images [ 9,15 ] , font recognition [ 42 ] , object detection [ 10,27 ] , hand pose estimation in depth images [ 38,37 ] , <mark>scene recognition</mark> in RGB-D [ 11 ] , semantic segmentation of urban<br>2: In computer vision, context has been used in problems such as object detection and recognition [25,14,8], <mark>scene recognition</mark> [23], action classification [22], and segmentation [28].<br>",
    "Arabic": "التعرف على المشهد",
    "Chinese": "场景识别",
    "French": "reconnaissance de scène",
    "Japanese": "場面認識",
    "Russian": "распознавание сцен"
  },
  {
    "English": "scene reconstruction",
    "context": "1: However, our work generalizes many previous constructions and is easily applicable to vision problems that involve large numbers of labels, such as stereo, motion, image restoration, and <mark>scene reconstruction</mark>.<br>2: We address the problem of computing the 3-dimensional shape of an arbitrary scene from a set of images taken at known viewpoints. Multi-camera <mark>scene reconstruction</mark> is a natural generalization of the stereo matching problem. However, it is much more difficult than stereo, primarily due to the difficulty of reasoning about visibility.<br>",
    "Arabic": "إعادة بناء المشهد",
    "Chinese": "场景重建",
    "French": "reconstruction de la scène",
    "Japanese": "シーン再構築",
    "Russian": "реконструкция сцены"
  },
  {
    "English": "scene representation",
    "context": "1: Approximate Renderer (AR): Picture's AR layer takes in a <mark>scene representation</mark> trace S ρ and tolerance variables X ρ , and uses general-purpose graphics simulators (Blender [5] and OpenGL) to render 3D scenes.<br>2: The data term as defined in Eq. (5) does not contain any form of occlusion handling; every pixel is always assumed visible. Since our <mark>scene representation</mark> is defined in 3D, it allows for explicit occlusion reasoning.<br>",
    "Arabic": "تمثيل المشهد",
    "Chinese": "场景表示",
    "French": "représentation de la scène",
    "Japanese": "シーン表現",
    "Russian": "представление сцены"
  },
  {
    "English": "scene understanding",
    "context": "1: This view is corroborated by other empirical evaluations of context using tuned local detectors (Divvala et al. 2009) and (Galleguillos et al. 2008). However, we argue that context is helpful for higher level semantic inferences such as scene or action understanding.<br>",
    "Arabic": "فهم المشهد",
    "Chinese": "场景理解",
    "French": "compréhension de la scène",
    "Japanese": "シーン理解",
    "Russian": "понимание сцены"
  },
  {
    "English": "schedule sampling",
    "context": "1: We hope that the strong shallow baseline system used in this work makes the evaluation convincing. We also compare with the other two related methods that aim at solving the exposure bias problem, including the <mark>scheduled sampling</mark>    our approach further gives a significant improvements on most test sets and achieves improvement by about +1.2 BLEU points on average.<br>",
    "Arabic": "جدولة المعاينة",
    "Chinese": "定时采样",
    "French": "échantillonnage planifié",
    "Japanese": "スケジュールサンプリング",
    "Russian": "выборка по расписанию"
  },
  {
    "English": "scheduler",
    "context": "1: , A k such that each agent is responsible for scheduling a disjoint subset T k ⊆ T − {z} of events. These agents act as distributed autonomous <mark>scheduler</mark>s, so each agent A i wants to determine its own schedule \n<br>",
    "Arabic": "جدولة",
    "Chinese": "调度器",
    "French": "ordonnanceur",
    "Japanese": "スケジューラ",
    "Russian": "планировщик"
  },
  {
    "English": "schema item",
    "context": "1: Previous research has sought to characterize texts in terms of domain-independent rhetorical elements, such as <mark>schema items</mark> (McKeown, 1985) or rhetorical relations (Mann and Thompson, 1988;Marcu, 1997). The focus of our work, however, is on an equally fundamental but domaindependent dimension of the structure of text: content.<br>2: The setting of UnifiedSKG is different from other baselines. It assumes the gold <mark>schema items</mark> are always included in the retrieved subgraph and restricts the number of negative <mark>schema items</mark> in the subgraph (i.e., at most 20 <mark>schema items</mark> for GRAILQA).<br>",
    "Arabic": "عنصر المخطط",
    "Chinese": "模式项",
    "French": "élément de schéma",
    "Japanese": "スキーマ項目",
    "Russian": "элемент схемы"
  },
  {
    "English": "score function",
    "context": "1: Intuitively, this loss perturbs x with a random amount of noise corresponding to the timestep t, and estimates an update direction that follows the <mark>score function</mark> of the diffusion model to move to a higher density region.<br>2: 1 − β t|s E q ( zt ) ||∇ zt log q ( z t ) || 2 d ) . Thereby, both the optimal mean and variance have a closed form expression w.r.t. the <mark>score function</mark>.<br>",
    "Arabic": "دالة النقاط",
    "Chinese": "评分函数",
    "French": "fonction de score",
    "Japanese": "スコア関数",
    "Russian": "функция оценки"
  },
  {
    "English": "score matching",
    "context": "1: We establish that, as in the Euclidean case, the corresponding time-reversal process is also a diffusion whose drift includes the Stein score which is intractable but can similarly be estimated via <mark>score matching</mark>. Methodological extensions are required as in most cases the transition kernel of the noising process cannot be sampled exactly.<br>2: For the sake of completeness, we derive the connection between <mark>score matching</mark> and denoising for a finite dataset. For a more general treatment and further background on the topic, see Hyvärinen [22] and Vincent [54]. Let us assume that our training set consists of a finite number of samples {y 1 , . . .<br>",
    "Arabic": "مطابقة النقاط",
    "Chinese": "得分匹配",
    "French": "appariement de scores",
    "Japanese": "スコアマッチング",
    "Russian": "\"сопоставление оценки\""
  },
  {
    "English": "score vector",
    "context": "1: Both of these methods specify the joint probability of a specific rank vector as the product of each document having that particular rank. In our comparison of the methods, we assume that all the methods have the correct <mark>score vector</mark>, and that SoftRank has access to the true σ s in the generator.<br>2: Our model produces a distribution over rank vectors based on the estimated <mark>score vector</mark>. Several loss functions could be minimized.<br>",
    "Arabic": "متجه النقاط",
    "Chinese": "得分向量",
    "French": "vecteur de scores",
    "Japanese": "スコアベクトル",
    "Russian": "вектор оценок"
  },
  {
    "English": "score-base model",
    "context": "1: where µ n (x n ) is generally parameterized 1 by a time-dependent <mark>score-based model</mark> s n (x n ) (Song & Ermon, 2019;Song et al., 2020b): \n<br>2: In contrast to the handcrafted strategies used in (Ho et al., 2020;Song et al., 2020a), Theorem 1 shows that the optimal reverse variance σ * 2 n can also be estimated without any extra training process given a pretrained <mark>score-based model</mark> s n (x n ).<br>",
    "Arabic": "\"نموذج قائم على الدرجات\"",
    "Chinese": "基于分数的模型",
    "French": "modèle basé sur le score",
    "Japanese": "スコアベースモデル",
    "Russian": "модель на основе оценок"
  },
  {
    "English": "scoring function",
    "context": "1: We do not consider the pairwise relationships in the <mark>scoring function</mark> as these relationships are encoded in our feature representation (section 4.1).<br>2: The individual methods such as PRank (Crammer et al., 2001) do not use any relative information between documents, instead attempting to directly create a <mark>scoring function</mark>, scores of which are then used to rank the documents.<br>",
    "Arabic": "دالة التقييم",
    "Chinese": "评分函数",
    "French": "fonction de score",
    "Japanese": "スコアリング関数",
    "Russian": "функция оценивания"
  },
  {
    "English": "search algorithm",
    "context": "1: A forward-search-based proof cannot be correct unless the Graph History Interaction problem (GHI) is properly addressed. GHI can occur when a <mark>search algorithm</mark> caches and re-uses a search result that depends on the move history. In checkers, repeated positions are scored as a draw.<br>2: Using the state equation heuristic requires solving an LP for every state evaluated by a <mark>search algorithm</mark>. It offers the best possible potential heuristic value for every single state.<br>",
    "Arabic": "خوارزمية البحث",
    "Chinese": "搜索算法",
    "French": "algorithme de recherche",
    "Japanese": "探索アルゴリズム",
    "Russian": "поисковый алгоритм"
  },
  {
    "English": "search problem",
    "context": "1: The Setting Let S be the set of all possible states of a <mark>search problem</mark>. For s ∈ S , letV ( s ) = 1 Ns Ns t=1 R s , t denote the value estimation of state s from simulations , where R s , t is the outcome of a simulation , and N s is the number of simulations starting from state s. The true value of a state s is denoted by V *<br>",
    "Arabic": "مشكلة البحث",
    "Chinese": "搜索问题",
    "French": "problème de recherche",
    "Japanese": "探索問題",
    "Russian": "проблема поиска"
  },
  {
    "English": "search procedure",
    "context": "1: The generation procedure is a <mark>search procedure</mark> [8] that produces candidate feature subsets for evaluation based on a certain search strategy. Feature selection methods applied in this paper generate candidates randomly. An evaluation function measures the goodness of the subset produced and this value is compared with the previous best.<br>2: The main assumptions made by this approach are: 1) the true loss function can provide effective heuristic guidance to the <mark>search procedure</mark>, so that it is worth imitating, and 2) we can learn to imitate those search decisions sufficiently well.<br>",
    "Arabic": "إجراء البحث",
    "Chinese": "搜索过程",
    "French": "procédure de recherche",
    "Japanese": "探索手順",
    "Russian": "процедура поиска"
  },
  {
    "English": "search space",
    "context": "1: Figure 1a depicts the traditional <mark>search space</mark>, and Figure 1b depicts the <mark>search space</mark> in this work. Hyperedge scores can only depend on neighboring nodes, so our model can condition on the entire parse structure, at the price of an exponentially larger <mark>search space</mark>.<br>2: The decoding process can be viewed as a sequential decision-making process, which decomposes the task of finding a program from the enormous <mark>search space</mark> into making decisions from a sequence of smaller <mark>search space</mark>s.<br>",
    "Arabic": "فضاء البحث",
    "Chinese": "搜索空间",
    "French": "espace de recherche",
    "Japanese": "探索空間",
    "Russian": "пространство поиска"
  },
  {
    "English": "search tree",
    "context": "1: However, with large state spaces, the accuracy of value estimation cannot be effectively guaranteed, since the mean value estimation is likely to have high variance under relatively limited search time. Inaccurate estimation can mislead building the <mark>search tree</mark> and severely degrade the performance of the program.<br>2: tree of BOA * ( P , h ) contains the <mark>search tree</mark> ( after prunes ) of BOA * ( P α , β , h α , β ) when h is consistent .<br>",
    "Arabic": "شجرة البحث",
    "Chinese": "搜索树",
    "French": "arbre de recherche",
    "Japanese": "探索木",
    "Russian": "поисковое дерево"
  },
  {
    "English": "second order",
    "context": "1: This was extended to the piecewise <mark>second order</mark> \"weak plate\" model by Blake and Zisserman [3], and recently Ishikawa and 1 978-1-4244-2243-2/08/$25.00 ©2008 IEEE Geiger [16] have argued that <mark>second order</mark> priors may be closer to those that the human visual system appears to use.<br>",
    "Arabic": "ثاني الرتبة",
    "Chinese": "二阶",
    "French": "deuxième ordre",
    "Japanese": "二次の",
    "Russian": "второго порядка"
  },
  {
    "English": "second order statistic",
    "context": "1: For the implementation in this paper we escape this ambiguity by noticing that while the original image x (in the spatial domain) be non negative, deconvolving y with the mirrored filter often leads to negative x values. Yet, this ambiguity highlights one of the weaknesses of <mark>second order statistics</mark>.<br>2: While the <mark>second order statistics</mark> of the images in Fig. 12(a,b) are equal, it is clear that every simple sparse measure will favor Fig. 12(a). Nevertheless, we show that the <mark>second order statistics</mark> plus finite support constraint can get us surprisingly close to the true solution.<br>",
    "Arabic": "الإحصائيات من الدرجة الثانية",
    "Chinese": "二阶统计量",
    "French": "statistique du second ordre",
    "Japanese": "二次統計量",
    "Russian": "статистика второго порядка"
  },
  {
    "English": "second-order optimization",
    "context": "1: Second, we need to calculate I up,loss (z i , z test ) across all training points z i . The first problem is well-studied in <mark>second-order optimization</mark>. The idea is to avoid explicitly computing H −1 θ ; instead , we use implicit Hessian-vector products ( HVPs ) to efficiently approximate s test def = H −1 θ ∇ θ L ( z test , θ ) and then compute I up , loss ( z , z test ) = −s test • ∇ θ L ( z , θ<br>",
    "Arabic": "تحسين من الدرجة الثانية",
    "Chinese": "二阶优化",
    "French": "optimisation de second ordre",
    "Japanese": "二次最適化",
    "Russian": "оптимизация второго порядка"
  },
  {
    "English": "second-order potential",
    "context": "1: Left: <mark>second-order potentials</mark> can be rotation-invariant by comparing distances between matched points. Right: Thirdorder potentials can be similarity-invariant by comparing angles of triangles.<br>",
    "Arabic": "الإمكانيات من الدرجة الثانية",
    "Chinese": "二阶势能",
    "French": "potentiel de deuxième ordre",
    "Japanese": "二次ポテンシャル",
    "Russian": "потенциалы второго порядка"
  },
  {
    "English": "segmentation",
    "context": "1: While fully convolutionalized classifiers can be finetuned to <mark>segmentation</mark> as shown in 4.1, and even score highly on the standard metric, their output is dissatisfyingly coarse (see Figure 4). The 32 pixel stride at the final prediction layer limits the scale of detail in the upsampled output.<br>2: Next we evaluate fine-tuning the models for transferring to object detection and <mark>segmentation</mark>. These computer vision tasks in general benefit from higher-resolution input, so the batch size tends to be small in common practice (1 or 2 images/GPU [12,47,18,36]).<br>",
    "Arabic": "التجزئة",
    "Chinese": "分割",
    "French": "segmentation",
    "Japanese": "セグメンテーション",
    "Russian": "сегментация"
  },
  {
    "English": "segmentation algorithm",
    "context": "1: For illustration, we show in Figure 4 the piecewise linear representation of the example waveform pattern Figure 1(a), where we arbitrarily set the error tolerance = 1.0 for the <mark>segmentation algorithm</mark> so that the approximating error at each point will not exceed in amplitude on this data.<br>2: For example, no <mark>segmentation algorithm</mark> can group two regions separated by an occluding object because such a merge would require reasoning about depth ordering. It is precisely this type of reasoning that the depth ordering estimation of Section 3.6 enables.<br>",
    "Arabic": "خوارزمية التجزئة",
    "Chinese": "分割算法",
    "French": "algorithme de segmentation",
    "Japanese": "セグメンテーションアルゴリズム",
    "Russian": "алгоритм сегментации"
  },
  {
    "English": "segmentation map",
    "context": "1: It is both visually and linguistically richer, moving beyond whole shape descriptions to include <mark>segmentation maps</mark> and part labels. We use this resource to evaluate the abstract visual reasoning capacities of recent multi-modal models. We observe that pre-trained weights demonstrate limited abstract reasoning, which dramatically improves with fine-tuning.<br>",
    "Arabic": "خريطة التقسيم",
    "Chinese": "分割图",
    "French": "carte de segmentation",
    "Japanese": "セグメンテーションマップ",
    "Russian": "карта сегментации"
  },
  {
    "English": "segmentation mask",
    "context": "1: We learn a binary segmentation  , where for a real image I t ∈ R + W ×H×3 , f seg (I t ) yields a binary <mark>segmentation mask</mark>M t which serves as input to π H . To train f seg , we require a paired dataset of real plate images and ground truth <mark>segmentation mask</mark>s.<br>2: Inspired by this line of work, we propose the promptable segmentation task, where the goal is to return a valid <mark>segmentation mask</mark> given any segmentation prompt (see Fig. 1a). A prompt simply specifies what to segment in an image, e.g., a prompt can include spatial or text information identifying an object.<br>",
    "Arabic": "قناع التجزئة",
    "Chinese": "分割掩码",
    "French": "masque de segmentation",
    "Japanese": "セグメンテーションマスク",
    "Russian": "маска сегментации"
  },
  {
    "English": "selection bias",
    "context": "1: is said to be s-recoverable from <mark>selection bias</mark> in G s with external information over T ⊆ V and <mark>selection bias</mark>ed data over M ⊆ V (for short, s-recoverable) if the assumptions embedded in the causal model render Q expressible in terms of P (m | S = 1) and P (t), both positive. Formally , for every two probability distributions P 1 and P 2 compatible with G s , if they agree on the available distributions , P 1 ( m | S = 1 ) = P 2 ( m | S = 1 ) > 0 , P 1 ( t ) = P 2 ( t ) > 0 , they must agree<br>2: 1 Remarkably, there are special situations in which <mark>selection bias</mark> can be detected even from observations, as in the form of a non-chordal undirected component (Zhang 2008). the data-generating model in Fig.<br>",
    "Arabic": "تحيز الاختيار",
    "Chinese": "选择偏差",
    "French": "biais de sélection",
    "Japanese": "選択バイアス",
    "Russian": "критерий отбора"
  },
  {
    "English": "selectional preference",
    "context": "1: The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on <mark>selectional preferences</mark> or word associations.<br>",
    "Arabic": "تفضيلات الاختيار",
    "Chinese": "选择偏好",
    "French": "préférence sélectionnelle",
    "Japanese": "選好性",
    "Russian": "селективное предпочтение"
  },
  {
    "English": "self-attention",
    "context": "1: We sample 150,000 short article versions balancing for sources, length and version number. Task 3: How will it update? For each sentence in version v, predict whether: (1) the sentence itself Architecture diagram for the model used for our tasks. Word-embeddings are averaged using <mark>Self-Attention</mark> to form sentence-vectors.<br>2: There are some prior works on improving the efficiency of self-attention. The Sparse Transformer (Child et al. 2019), LogSparse Transformer (Li et al.<br>",
    "Arabic": "الانتباه الذاتي",
    "Chinese": "自注意力",
    "French": "auto-attention",
    "Japanese": "自己注目",
    "Russian": "самовнимание"
  },
  {
    "English": "self-attention head",
    "context": "1: Pointers for edges are modeled by a selfattention head on the decoder's top layer, and the source copy mechanism is modeled by a crossattention head of the penultimate decoder layer. The LMCOMPLETE model is based on fine-tuning the pre-trained BART large model (Lewis et al., 2020).<br>",
    "Arabic": "رأس الانتباه الذاتي",
    "Chinese": "自注意力头",
    "French": "tête d'auto-attention",
    "Japanese": "自己注意ヘッド",
    "Russian": "головка самовнимания"
  },
  {
    "English": "self-attention layer",
    "context": "1: We add the output of the feed-forward to the initial representation and apply layer normalization to give the final output of <mark>self-attention layer</mark> j, as in Eqn. 1.<br>2: The decoder has a similar structure as the encoder except that, in each decoder layer between the <mark>self-attention layer</mark> and feed-forward layer, a multi-head attention layer attends to the output of the encoder. Layer normalization (Ba et al., 2016) is applied to the output of each skip connection.<br>",
    "Arabic": "طبقة الانتباه الذاتي",
    "Chinese": "自注意力层",
    "French": "couche d'auto-attention",
    "Japanese": "自己注意層",
    "Russian": "слой самовнимания"
  },
  {
    "English": "self-attention matrix",
    "context": "1: Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the <mark>self-attention matrix</mark>, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens.<br>2: APE often assigns one positional embedding per token and combines them directly with input embeddings. In contrast, RPE adds temporal bias terms to the <mark>self-attention matrix</mark> to encode the relative distance between token pairs. For example, the right triangular matrix in Figure 1 shows the set of temporal bias terms.<br>",
    "Arabic": "مصفوفة الانتباه الذاتي",
    "Chinese": "自注意力矩阵",
    "French": "matrice d'auto-attention",
    "Japanese": "自己注意行列 (じこちゅういぎょうれつ)",
    "Russian": "матрица самовнимания"
  },
  {
    "English": "self-attention mechanism",
    "context": "1: DFTs have also been used indirectly in several Transformer works. The Performer (Choromanski et al., 2020) linearizes the Transformer selfattention mechanism by leveraging random Fourier features to approximate a Gaussian representation of the softmax kernel.<br>2: In this way, L cont helps the student to capture more information from the teacher's representation, as is also theoretically discussed in Tian et al. (2019). The proposed token-level contrastive distillation is crucial to the performance, and outperforms the sequence-level counterpart (as will be shown empirically in Section 5.1.1). We conjecture this is because ( i ) token-level contrast alleviates the problem of homogeneous word embedding ( Figure 2 ) in the low-bit quantization ; and ( ii ) similar to speech , the order of natural language is also sequential instead of spatial like images ; and ( iii ) the <mark>self-attention mechanism</mark> allows other tokens to learn representations contextualized on the<br>",
    "Arabic": "آلية الانتباه الذاتي",
    "Chinese": "自注意力机制",
    "French": "mécanisme d'auto-attention",
    "Japanese": "自己注意メカニズム",
    "Russian": "механизм самовнимания"
  },
  {
    "English": "self-attention model",
    "context": "1: (2019) to learn the attention span of <mark>self-attention models</mark> for natural language processing. Exploiting the conjugate symmetry of the coefficients, we only consider positive frequencies along the horizontal axis, while we mirror the vertical mask around frequency zero. Therefore, the two masks are defined as follows: \n mask h ( S h , H , R ) ( m ) = min max 1 R ( R + H 2S h − | H 2 − m| ) , 0 , 1 , m ∈ [ 0 , H ] ( 3 ) mask w ( Sw , W , R ) ( n ) = min max 1 R (<br>",
    "Arabic": "نموذج الانتباه الذاتي",
    "Chinese": "自注意力模型",
    "French": "modèle à auto-attention",
    "Japanese": "自己注意モデル",
    "Russian": "Модель самовнимания"
  },
  {
    "English": "self-attention module",
    "context": "1: BEIT-3 (Wang et al., 2022c) utilizes a novel shared Multiway Transformer network with a shared <mark>self-attention module</mark> to align different modalities and provide deep fusion. Building on the success of multimodal pretraining, our work focuses on improving the generalization and zeroshot performance on various unseen multimodal tasks through instruction tuning.<br>2: The computation of a Transformer decoder layer (Vaswani et al., 2017) includes a <mark>self-attention module</mark> and a cross-attention module. The <mark>self-attention module</mark> models the relevant information from previous decoder stateŝ \n s l a ′ = [s l 1 (t 1 ), • • • , s l u−1 (t u−1 )],(4) \n<br>",
    "Arabic": "وحدة الانتباه الذاتي",
    "Chinese": "自注意力模块",
    "French": "module d'auto-attention",
    "Japanese": "自己注意モジュール",
    "Russian": "модуль самовнимания"
  },
  {
    "English": "self-learning",
    "context": "1: It can also be applied repeatedly with improving underlying models and is essentially an extension of <mark>self-learning</mark>, as we discuss in Section 2. We present experiments with MARUPA on anonymized data from a commercial dialog system across many domains and three different languages. In addition, we also report component-wise evaluations and experiments on public datasets where possible.<br>2: Our work is also related to the semi-supervised learning approach known as <mark>self-learning</mark> or pseudolabeling, in which models are trained on predictions that a previous version of the model made on unlabeled data. This idea has been successfully applied to a wide range of language tasks, e.g.<br>",
    "Arabic": "التعلم الذاتي",
    "Chinese": "自学习",
    "French": "auto-apprentissage",
    "Japanese": "自己学習",
    "Russian": "самообучение"
  },
  {
    "English": "self-loop",
    "context": "1: Note that G A can have <mark>self-loops</mark>, but for any ξ, ξ ′ ∈ V A , the shortest path between ξ and ξ ′ will not go through <mark>self-loops</mark>.<br>",
    "Arabic": "حلقة ذاتية",
    "Chinese": "自环",
    "French": "auto-boucle",
    "Japanese": "自己ループ",
    "Russian": "самопетля"
  },
  {
    "English": "self-play",
    "context": "1: This fact has allowed <mark>self-play</mark>, even without human data, to achieve remarkable success in 2p0s games like chess (Silver et al., 2018), Go (Silver et al., 2017), poker (Bowling et al., 2015;, and Dota 2 (Berner et al., 2019).<br>2: Anthony et al. (2020) used a <mark>self-play</mark> approach based on a modification of fictitious play in order to reduce drift from human conventions. The resulting policy is stronger than pure imitation learning in both 1vs6 and 6vs1 settings but weaker than agents that use search. Most recently, Bakhtin et al.<br>",
    "Arabic": "اللعب الذاتي",
    "Chinese": "自我对弈",
    "French": "auto-jeu",
    "Japanese": "自己対戦",
    "Russian": "самоигра"
  },
  {
    "English": "self-supervise learning",
    "context": "1: <mark>Self-supervised learning</mark> ( SSL ) of image representations has shown significant progress in the last few years ( Chen et al. , 2020a ; Chen et al. , 2020b ; Grill et al. , 2020 ; Lee et al. , 2021b ; Caron et al. , 2020 ; Zbontar et al. , 2021 ; Bardes et al. , 2021 ; Tomasev et al. ,<br>",
    "Arabic": "التعلم الذاتي الإشراف",
    "Chinese": "自监督学习",
    "French": "apprentissage auto-supervisé",
    "Japanese": "自己教師あり学習",
    "Russian": "самоконтролируемое обучение"
  },
  {
    "English": "self-supervise method",
    "context": "1: As of yet, most generative model based approaches have not been competitive with supervised and <mark>self-supervised methods</mark> in the image domain. A notable exception is Big-BiGAN (Donahue & Simonyan, 2019) which first demonstrated that sufficiently high fidelity generative models learn image representations which are competitive with other selfsupervised methods.<br>2: The large win margin for taxonomy shows that carefully selecting transfer policies depending on the target is superior to fixed transfers, such as the ones employed by <mark>self-supervised methods</mark>. ImageNet features which are the most popular off-the-shelf features in vision are also outperformed by those policies.<br>",
    "Arabic": "طريقة الإشراف الذاتي",
    "Chinese": "自监督方法",
    "French": "méthode d'auto-supervision",
    "Japanese": "自己教師付き方法",
    "Russian": "метод самонаблюдения"
  },
  {
    "English": "self-supervise model",
    "context": "1: To compute a self-supervised pruning metric for ImageNet, we perform k-means clustering in the embedding space of an ImageNet pre-trained <mark>self-supervised model</mark> (here: SWaV [36]), and define the difficulty of each data point by the cosine distance to its nearest cluster centroid, or prototype.<br>",
    "Arabic": "نموذج الإشراف الذاتي",
    "Chinese": "自监督模型",
    "French": "modèle auto-supervisé",
    "Japanese": "自己教師ありモデル",
    "Russian": "самообучаемая модель"
  },
  {
    "English": "self-supervise representation learning",
    "context": "1: We evaluate GIM in the audio domain on the sequence-global task of speaker classification and the local task of phone classification (distinct phonetic sounds that make up pronunciations of words). These two tasks are interesting for <mark>self-supervised representation learning</mark> as the former requires representations that discriminate speakers but are invariant to content, while the latter requires the opposite.<br>2: We propose a novel deep learning method for local <mark>self-supervised representation learning</mark> that does not require labels nor end-to-end backpropagation but exploits the natural order in data instead. Inspired by the observation that biological neural networks appear to learn without backpropagating a global error signal, we split a deep neural network into a stack of gradient-isolated modules.<br>",
    "Arabic": "تعلم التمثيل الخاضع للإشراف الذاتي",
    "Chinese": "自监督表征学习",
    "French": "apprentissage des représentations auto-supervisé",
    "Japanese": "自己教師あり表現学習",
    "Russian": "самостоятельное обучение представлению"
  },
  {
    "English": "self-supervise signal",
    "context": "1: Effect of Different Prior Components: To evaluate the effect of our proposed prior of semantic consistency and data augmentation consistency, we train the networks with different combinations of these <mark>self-supervised signals</mark>. The quantitative results with different components in our proposed JDACS framework are summarized in Table 3 and Table 4.<br>",
    "Arabic": "إشارة الإشراف الذاتي",
    "Chinese": "自监督信号",
    "French": "signal auto-supervisé",
    "Japanese": "自己監督信号",
    "Russian": "самообучающий сигнал"
  },
  {
    "English": "self-supervise training",
    "context": "1: We train it for 300 epochs using Adam [Kingma and Ba, 2014] and a learning rate of 1.5e-4 and use the same random seed in all our experiments. For the <mark>self-supervised training</mark> using the InfoNCE objective, we need to contrast the predictions of the model for its future representations against negative samples.<br>2: We have proposed a new <mark>self-supervised training</mark> strategy for Autoencoder architectures that enables robust 3D object orientation estimation on various RGB sensors while training only on synthetic views of a 3D model.<br>",
    "Arabic": "التدريب الذاتي المراقب",
    "Chinese": "自监督训练",
    "French": "apprentissage auto-supervisé",
    "Japanese": "自己教師付きトレーニング",
    "Russian": "самонаблюдаемое обучение"
  },
  {
    "English": "self-supervision",
    "context": "1: Because any pixels in texture-less regions share the same color intensity, leading to the fact that <mark>self-supervision</mark> loss is fixed to 0 and becomes meaningless. However, the texture-less regions often appear in realistic scenarios, where <mark>self-supervision</mark> may be confused and fail to generalize. Exploration of handling texture-less regions may provide a potential direction in the future.<br>2: This transformation is computed online, i.e., the transformation changes as the depth prediction is updated at each training iteration. Figure 2 illustrates the <mark>self-supervision</mark> via warping the 3D geometry of humans between two frames of a video.<br>",
    "Arabic": "الإشراف الذاتي",
    "Chinese": "自监督",
    "French": "auto-supervision",
    "Japanese": "自己教師付き学習",
    "Russian": "самоконтроль"
  },
  {
    "English": "self-training",
    "context": "1: In order to apply the document-level label consistency model, we divide the test set into blocks of ten sentences, and use the blocks as pseudo-documents. Results from <mark>self-training</mark>, as well as results from uptraining using model outputs from Burkett et al. (2010) are shown in Table 4.<br>2: The first work is a highly problem-specific approach whereas the last three all use a <mark>self-training</mark> type approach (Transductive SVMs in the case of text classification, which is a kind of <mark>self-training</mark> method). These methods augment the training set with labeled examples from the unlabeled set which are predicted by the model itself.<br>",
    "Arabic": "التدريب الذاتي",
    "Chinese": "自训练",
    "French": "auto-formation",
    "Japanese": "自己学習",
    "Russian": "самообучение"
  },
  {
    "English": "semantic alignment",
    "context": "1: It consists of two levels of alignment: positional alignment and <mark>semantic alignment</mark>. In positional alignment, the model learns to map each object representation to words in the sentence, which could possibly be a MASK or an additional no-object label ∅ (Yu and Siskind, 2013;Kamath et al., 2021).<br>2: We finally move on to our proposed method X-InSTA that combines <mark>semantic alignment</mark> with the task-based one. It first selects source examples from D s with top-k similarity scores as mentioned in Section 2.2. Additionally, we select taskaligners from D l depending on the source and target languages and the task.<br>",
    "Arabic": "التوافق الدلالي",
    "Chinese": "语义对齐",
    "French": "alignement sémantique",
    "Japanese": "意味的アラインメント",
    "Russian": "семантическое выравнивание"
  },
  {
    "English": "semantic analysis",
    "context": "1: Context and <mark>semantic analysis</mark> are quite common in natural language and text processing (see e.g., [17,5,13]). Most work, however, deals with non-redundant word-based contexts, which are quite different from pattern contexts. In specific domains, people have explored the context of specific data patterns to solve specific problems [18,14].<br>2: It is necessary to introduce technology to signal users when new content of interest appears. RSS allow users to subscribe to sources. Semantic technology allows the creation of automatic alerts for new interesting content based on <mark>semantic analysis</mark>. Semantic Web can contribute introducing computer-readable representations for simple fragments of meaning.<br>",
    "Arabic": "التحليل الدلالي",
    "Chinese": "语义分析",
    "French": "analyse sémantique",
    "Japanese": "意味解析",
    "Russian": "семантический анализ"
  },
  {
    "English": "semantic annotation",
    "context": "1: It provides highly scalable core functionality to support the needs of SemTag and other automated <mark>semantic annotation</mark> algorithms.<br>2: Definition 4 (Semantic Annotation): Let pα be a frequent pattern in a dataset D, Uα be the set of context indicators of pα, and P be a set of patterns in D. A <mark>semantic annotation</mark> of pα consists of: 1) a set of context indicators of pα, Iα ⊆ Uα, s.t. ∀u ∈ Iα and ∀u ∈ Uα − Iα , w ( u , α ) ≤ w ( u , α ) ; 2 ) a set of transactions Tα ⊆ Dα , s.t.∀t ∈ Tα and ∀t ∈ Dα − Tα , t is more similar to c ( α ) than t under some similarity measure ; and 3 ) a<br>",
    "Arabic": "التعليق الدلالي",
    "Chinese": "语义注释",
    "French": "annotation sémantique",
    "Japanese": "意味注釈",
    "Russian": "семантическая аннотация"
  },
  {
    "English": "semantic category",
    "context": "1: Finally we upgrade this late fusion net to a 16-stride version. SIFT Flow is a dataset of 2,688 images with pixel labels for 33 <mark>semantic categories</mark> (\"bridge\", \"mountain\", \"sun\"), as well as three geometric categories (\"horizontal\", \"vertical\", and \"sky\").<br>",
    "Arabic": "فئة دلالية",
    "Chinese": "语义类别",
    "French": "catégorie sémantique",
    "Japanese": "意味カテゴリー",
    "Russian": "семантическая категория"
  },
  {
    "English": "semantic class",
    "context": "1: , n} where s i ∈ R m is the one-hot feature vector to encode the visual semantics of objects in P and m denotes the number of <mark>semantic classes</mark>. The edge set E = {e i,j , i, j = 1, 2, . . .<br>2: where θ * denotes the optimal shift between a pair of DFs, which is computed by maximizing the overall DF similarity, where m denotes the number of <mark>semantic classes</mark>. The DF matching process is illustrated in Figure 3.<br>",
    "Arabic": "الصنف الدلالي",
    "Chinese": "语义类别",
    "French": "classe sémantique",
    "Japanese": "セマンティック分類",
    "Russian": "семантический класс"
  },
  {
    "English": "semantic constraint",
    "context": "1: Their approach is conceptually simple: it consists of a scoring function coupled with a small number of syntactic and <mark>semantic constraints</mark>. Discourse-related information can be easily incorporated in the form of additional constraints.<br>2: Coreference systems exploit a variety of information sources, ranging from syntactic and discourse constraints, which are highly configurational, to <mark>semantic constraints</mark>, which are highly contingent on lexical meaning and world knowledge. Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge.<br>",
    "Arabic": "ضوابط دلالية",
    "Chinese": "语义约束",
    "French": "contrainte sémantique",
    "Japanese": "意味的制約",
    "Russian": "семантическое ограничение"
  },
  {
    "English": "semantic distance",
    "context": "1: Handling knowledge (and interests) and sets of words allow the comparison of people and ideas as we can compare plain texts: tools to calculate <mark>semantic distance</mark> used for discovering similar ideas are still valid. By tracking user content consumption we obtain the set of words, which represents her interests.<br>2: In this paper, we propose a probabilistic approach to automatically labeling topic models with meaningful phrases. Intuitively, in order to choose a label that captures the meaning of a topic, we must be able to measure the \"<mark>semantic distance</mark>\" between a phrase and a topic model, which is challenging.<br>",
    "Arabic": "المسافة الدلالية",
    "Chinese": "语义距离",
    "French": "distance sémantique",
    "Japanese": "意味的距離",
    "Russian": "семантическое расстояние"
  },
  {
    "English": "semantic encoder",
    "context": "1: Specifically, ( 1) we remove the extra <mark>semantic encoder</mark> and construct the sentence-level representations by averaging the sequence of outputs of the vanilla sentence encoder. (2) We replace the default 4-layer <mark>semantic encoder</mark> with a large pre-trained model (PTM) (i.e., XLM-R (Conneau et al., 2020)).<br>2: To this end, we must consider such two problems: (1) How to optimize the <mark>semantic encoder</mark> so that it produces a meaningful adjacency semantic region for each observed training pair. (i) , y (i) ). (2) How to obtain samples from the adjacency semantic region in an efficient and effective way.<br>",
    "Arabic": "مُشفر المَعنى",
    "Chinese": "语义编码器",
    "French": "encodeur sémantique",
    "Japanese": "セマンティックエンコーダー",
    "Russian": "семантический кодировщик"
  },
  {
    "English": "semantic equivalence",
    "context": "1: We are not the first to consider this question: Kiddon and Domingos (2015) define a theory of <mark>semantic equivalence</mark> in terms of symmetries of the set of natural language sentences, and Gordon et al. (2020) propose a model architecture for compositional semantic parsing via a symmetry that enforces permutation invariance of lexicon entries.<br>2: Specifically, we construct negative samples by applying the convex interpolation between the current instance and other ones in the same training batch for instance comparison. And the tangent points (i.e., the points on the boundary) are considered as the critical states of <mark>semantic equivalence</mark>. The training objective is formulated as: \n J ctl ( Θ ′ ) =E ( x ( i ) , y ( i ) ) ∼B log e s r x ( i ) , r y ( i ) e s r x ( i ) , r y ( i ) + ξ , ξ = |B| j & j̸ =i e s r y ( i ) ,<br>",
    "Arabic": "التكافؤ الدلالي",
    "Chinese": "语义等价性",
    "French": "équivalence sémantique",
    "Japanese": "意味的同値性",
    "Russian": "семантическая эквивалентность"
  },
  {
    "English": "semantic feature",
    "context": "1: In this case, the model correctly includes the syntactic features as in Table 1, on the assumption that the child can accurately note the number and pattern of arguments. However, the model replaces the <mark>semantic features</mark> with those that correspond to the physical action event and its participants.<br>2: (2012), which is used to automatically generate the syntactic and <mark>semantic features</mark> of the frames that serve as input to the model. Using this lexicon, each simulation corpus is created through a probabilistic generation of argument structure frames according to their relative frequencies of occurrence in CDS.<br>",
    "Arabic": "ميزة دلالية",
    "Chinese": "语义特征",
    "French": "caractéristique sémantique",
    "Japanese": "意味的特徴",
    "Russian": "семантический признак"
  },
  {
    "English": "semantic graph",
    "context": "1: Our approach represents each observation consistently with a <mark>semantic graph</mark> representation and a set of class-wise distance field (DF) representations, as shown in Figure 2.<br>2: Specifically, given an observation with semantic labels obtained via semantic segmentation algorithms [38], [39], we first represent a place with a <mark>semantic graph</mark> G = (P, E, S) to encode the visual and spatial cues of the place. The node set P = {p i , i = 1, .<br>",
    "Arabic": "الرسم البياني الدلالي",
    "Chinese": "语义图",
    "French": "graphe sémantique",
    "Japanese": "意味グラフ",
    "Russian": "семантический граф"
  },
  {
    "English": "semantic information",
    "context": "1: The third criterion of a good label is the high intra-topic coverage. We expect a label to cover as much <mark>semantic information</mark> of a topic as possible. Indeed, if we only extract one label for each topic, the semantic relevance function already guarantees that the the label covers maximum <mark>semantic information</mark> of θ.<br>2: However, investigations such as the one presented in [2] indicate that searchers can find it difficult to use <mark>semantic information</mark> even when the system supports the recognition and use of semantic relationships. Consequently, in this section we outline a small pilot experiment designed to compare system recommendations of term utility against human assessment of the same terms.<br>",
    "Arabic": "المعلومات الدلالية",
    "Chinese": "语义信息",
    "French": "informations sémantiques",
    "Japanese": "意味情報",
    "Russian": "семантическая информация"
  },
  {
    "English": "semantic interpretation",
    "context": "1: A natural logic system can thus achieve the expressivity and precision needed to handle a great variety of simple logical inferences, while sidestepping the difficulties of full <mark>semantic interpretation</mark>.<br>2: However, when we speak of online semantic parsing in this paper, we really mean online <mark>semantic interpretation</mark>-parsing into a program and executing that program-and our algorithm starts executing early.<br>",
    "Arabic": "تفسير دلالي",
    "Chinese": "语义解释",
    "French": "interprétation sémantique",
    "Japanese": "意味解釈",
    "Russian": "семантическая интерпретация"
  },
  {
    "English": "semantic label",
    "context": "1: The contents of this data base are made available via a <mark>semantic label</mark> bureau from which it is possible to extract semantic tags using a variety of mechanisms.<br>",
    "Arabic": "تسمية دلالية",
    "Chinese": "语义标签",
    "French": "étiquette sémantique",
    "Japanese": "意味ラベル",
    "Russian": "семантическая метка"
  },
  {
    "English": "semantic memory",
    "context": "1: When it receives a response, it parses the language using the current context, grounding referents as appropriate (Lindes et al. 2017). The result is a precise, semantic structure, that Rosie then interprets within the current context. For new tasks, Rosie creates and stores a Task Concept Network (TCN) in <mark>semantic memory</mark>.<br>",
    "Arabic": "الذاكرة الدلالية",
    "Chinese": "语义记忆",
    "French": "mémoire sémantique",
    "Japanese": "意味記憶",
    "Russian": "семантическая память"
  },
  {
    "English": "semantic model",
    "context": "1: The corpus we used to train our <mark>semantic model</mark> is a concatenation of the UKWaC, BNC, and Wikipedia corpora (Ferraresi et al., 2008;BNC Consortium, 2007;Parker et al., 2011).<br>2: analysis and visualization of a word similarity and relatedness dataset containing bi-dimensional values for each word-pair and , ( iv ) a publicly available web-based word similarity questionnaire software . 2 2 Background and Design Motivations Word similarity evaluation (i.e., wordsim) is one of the oldest intrinsic methods of <mark>semantic model</mark> assessment.<br>",
    "Arabic": "نموذج دلالي",
    "Chinese": "语义模型",
    "French": "modèle sémantique",
    "Japanese": "意味モデル",
    "Russian": "семантическая модель"
  },
  {
    "English": "semantic network",
    "context": "1: To our knowledge, this is the largest scale semantic tagging effort to date, and demonstrates the viability of bootstrapping a web scale <mark>semantic network</mark>. The key challenge is resolving ambiguities in a natural language corpus. To this end, we introduce a new disambiguation algorithm called TBD, for Taxonomy-Based Disambiguation.<br>",
    "Arabic": "شبكة دلالية",
    "Chinese": "语义网络",
    "French": "réseau sémantique",
    "Japanese": "意味ネットワーク",
    "Russian": "семантическая сеть"
  },
  {
    "English": "semantic object",
    "context": "1: . . , n} represents the centroid locations of all <mark>semantic objects</mark>, with p i encoding the centroid location of the i-th semantic object. We also define a semantic set S = {s i , i = 1, . . .<br>2: As shown in Figure 4, K = 3 <mark>semantic objects</mark> are clustered in Q from the feature embeddings of all pixels in A, thus P contains the similarity between each pixel and each of the K = 3 clustered <mark>semantic objects</mark>.<br>",
    "Arabic": "كائن دلالي",
    "Chinese": "语义对象",
    "French": "objet sémantique",
    "Japanese": "意味的オブジェクト",
    "Russian": "семантический объект"
  },
  {
    "English": "semantic operator",
    "context": "1: An interesting perspective, where a kind of contextual information is studied, is presented in (Mukherjee and Bhattacharyya, 2012): the sentiment detection of tweets is here modeled according to lexical features as well as discourse relations like the presence of connectives, conditionals and <mark>semantic operators</mark> like modals and negations.<br>",
    "Arabic": "مُعامِل دلالي",
    "Chinese": "语义运算符",
    "French": "opérateur sémantique",
    "Japanese": "意味論的演算子",
    "Russian": "семантический оператор"
  },
  {
    "English": "semantic parse",
    "context": "1: Next we describe the layout model p(z|x; θ ). We first use a fixed syntactic parse to generate a small set of candidate layouts, analogously to the way a semantic grammar generates candidate <mark>semantic parse</mark>s in previous work (Berant and Liang, 2014). A <mark>semantic parse</mark> differs from a syntactic parse in two primary ways.<br>2: Formally , for a QLF Q , a <mark>semantic parse</mark> L partitions Q into parts p 1 , p 2 , • • • , p n ; each part p is assigned to some lambda-form cluster c , and is further partitioned into core form f and argument forms f 1 , • • • , f k ; each argument form is<br>",
    "Arabic": "تحليل دلالي",
    "Chinese": "语义分析",
    "French": "analyse sémantique",
    "Japanese": "意味解析",
    "Russian": "семантический парсинг"
  },
  {
    "English": "semantic parser",
    "context": "1: In either case, a subprogram is selected for execution as soon as the <mark>semantic parser</mark> predicts that it has a high probability of being in the correct parse. Experiments on both SMCalFlow and TreeDST datasets show that both approaches achieve high latency reduction with a small number of excess function calls.<br>2: The <mark>semantic parser</mark> fails to generalise despite being trained with mBART50 to ideally inherit some skill at disambiguiting semantically similar phrases. This error type accounts for 25% for SP, 20% for QA and 5% in DST of the total annotated errors. We give examples in Appendix C.<br>",
    "Arabic": "مُحلِّل دلالي",
    "Chinese": "语义解析器",
    "French": "analyseur sémantique",
    "Japanese": "意味解析器",
    "Russian": "семантический парсер"
  },
  {
    "English": "semantic priming",
    "context": "1: 1 For instance, Misra et al. (2020) and Kassner and Schütze (2020) show LLMs' behave in ways that are reminiscent of <mark>semantic priming</mark>, assigning greater probabilities to words that were semantically related to their words/sentence prefixes. More recently, Sinclair et al.<br>",
    "Arabic": "التهيئة الدلالية",
    "Chinese": "语义启动",
    "French": "amorçage sémantique",
    "Japanese": "意味プライミング",
    "Russian": "семантическое влияние"
  },
  {
    "English": "semantic relation",
    "context": "1: An interesting open question in SRL is whether a system can learn to model the <mark>semantic relations</mark> between a predicate sense s and its arguments, given a limited number of training samples in which s appears.<br>",
    "Arabic": "العلاقة الدلالية",
    "Chinese": "语义关系",
    "French": "relation sémantique",
    "Japanese": "意味関係",
    "Russian": "семантические отношения"
  },
  {
    "English": "semantic representation",
    "context": "1: In that application, a rule-based component converts an abstract <mark>semantic representation</mark> into a vast number of potential English renderings. These renderings are packed into a forest, from which the most promising sentences are extracted using statistical scoring. For our purposes, the extractor selects the trees with the best combination of word-bigram and expansiontemplate scores.<br>2: We now have two measures evaluating the quality of a given <mark>semantic representation</mark>: The Median Rank (behavior-based) and the SDT-ρ (corpus-based). Can we use the latter to predict the former? To answer this question, we compared the performance of both measures across different semantic models, document lengths and corpus sizes.<br>",
    "Arabic": "تمثيل دلالي",
    "Chinese": "语义表示",
    "French": "représentation sémantique",
    "Japanese": "意味表現",
    "Russian": "семантическое представление"
  },
  {
    "English": "semantic role",
    "context": "1: Our model, therefore, includes a set of linear decoders that indicate whether a word w i is a predicate, what the most appropriate sense for a predicate w p is, and what the <mark>semantic role</mark> of a word w r with respect to a specific predicate w p is, for each language l: \n<br>2: After applying τ to s 3 , a transformed sentence x is created(Line 18). Lines 20 − 26 find the <mark>semantic role</mark> r 2 of the transferred phrase from SRL annotation of x using model M and create a mapping from r 2 to the gold standard role r 1 of the phrase in s 3 .<br>",
    "Arabic": "الدور الدلالي",
    "Chinese": "语义角色",
    "French": "rôle sémantique",
    "Japanese": "意味役割",
    "Russian": "семантическая роль"
  },
  {
    "English": "semantic role label",
    "context": "1: to syntactic parse parents , while ( 4 ) assigning <mark>semantic role labels</mark> .<br>2: Most importantly, our model is able to provide predicate sense and <mark>semantic role labels</mark> according to 7 predicate-argument structure inventories in a single forward pass, facilitating comparisons between different linguistic formalisms and investigations about interlingual phenomena.<br>",
    "Arabic": "تسمية الدور الدلالي",
    "Chinese": "语义角色标注",
    "French": "rôle sémantique",
    "Japanese": "意味役割ラベル",
    "Russian": "семантическая ролевая метка"
  },
  {
    "English": "semantic search",
    "context": "1: We also compare against a global baseline where we run <mark>semantic search</mark> over a global catalog. We report recall@k for values of k in {1, 5, 10}, measuring the fraction of test set for which the target entity is in the top k model predictions.<br>2: Our most competitive baseline is our own implementation of (Fan et al., 2021), which comprises <mark>semantic search</mark> over de-identified personalized catalogs constructed from up to 1 month of historical user interactions. For a fair comparison with our model, we generate semantic embeddings with our fine-tuned SBERT.<br>",
    "Arabic": "بحث دلالي",
    "Chinese": "语义搜索",
    "French": "recherche sémantique",
    "Japanese": "セマンティック検索",
    "Russian": "семантический поиск"
  },
  {
    "English": "semantic segmentation",
    "context": "1: To learn the typical location of objects in images and their relative size, we use PASCAL context dataset (Mottaghi et al. 2014) that annotated PASCAL images with <mark>semantic segmentation</mark>. For this purpose we divide PASCAL images into equally-sized grids and for each grid compute the probability of the number of pixels that belongs to each object category.<br>2: Optimization algorithms of this nature have been used in global stereo [34], depth superresolution [10,19,24,26,29,32], colorization [25], and <mark>semantic segmentation</mark> [6,22,28,45]. These approaches are tailored to their specific task, and are generally computationally expensive.<br>",
    "Arabic": "التجزئة الدلالية",
    "Chinese": "语义分割",
    "French": "segmentation sémantique",
    "Japanese": "意味的セグメンテーション",
    "Russian": "семантическая сегментация"
  },
  {
    "English": "semantic similarity",
    "context": "1: We observe that PARAAMR gets a much higher score for syntactic diversity although it has a slightly lower score for <mark>semantic similarity</mark>.<br>2: They are the key ideas behind measures developed to assess the <mark>semantic similarity</mark> of concepts, i.e., how much one concept has to do with a different one.<br>",
    "Arabic": "تشابه دلالي",
    "Chinese": "语义相似性",
    "French": "similarité sémantique",
    "Japanese": "意味的類似性",
    "Russian": "семантическое сходство"
  },
  {
    "English": "semantic similarity measure",
    "context": "1: Following previous work (Hu et al., 2019b) We use the following metrics to evaluate the semantic similarity of paraphrases: \n • Semantic similarity measure by SimCSE: \n<br>",
    "Arabic": "مقياس التشابه الدلالي",
    "Chinese": "语义相似度度量",
    "French": "mesure de similarité sémantique",
    "Japanese": "意味的類似性の尺度",
    "Russian": "мера семантического сходства"
  },
  {
    "English": "semantic space",
    "context": "1: parameters other than Θ ) . ∀(x, y) ∈ (X , Y) : r x = r y . Besides, an adjacency semantic region ν(r x , r y ) in the <mark>semantic space</mark> describes adequate variants of literal expression centered around each observed sentence pair (x, y). In our scenario , we first sample a series of vectors ( denoted by R ) from the adjacency semantic region to augment the current training instance , that is R = { r ( 1 ) , r ( 2 ) , ... , r ( K ) } , wherer ( k ) ∼ ν ( r x , r y )<br>2: Content prompt Write a peer review by first describing what problem or question this paper addresses, then strengths and weaknesses, for the paper title, its main content is as below: abstract Machine In this paper, the authors tackle the problem of ambiguity in entity mentions when integrating text and knowledge into a unified <mark>semantic space</mark>.<br>",
    "Arabic": "الفضاء الدلالي",
    "Chinese": "语义空间",
    "French": "espace sémantique",
    "Japanese": "意味空間",
    "Russian": "семантическое пространство"
  },
  {
    "English": "semantic structure",
    "context": "1: Dominey and Boucher (2005) paired narrated sentences with symbolic representations of their meanings, automatically extracted from video, to learn object names, spatial-relation terms, and event names as a mapping from the grammatical structure of a sentence to the <mark>semantic structure</mark> of the associated meaning representation.<br>2: When it receives a response, it parses the language using the current context, grounding referents as appropriate (Lindes et al. 2017). The result is a precise, <mark>semantic structure</mark>, that Rosie then interprets within the current context. For new tasks, Rosie creates and stores a Task Concept Network (TCN) in semantic memory.<br>",
    "Arabic": "البنية الدلالية",
    "Chinese": "语义结构",
    "French": "structure sémantique",
    "Japanese": "意味構造",
    "Russian": "семантическая структура"
  },
  {
    "English": "semantic symbol",
    "context": "1: Conventional approaches to NLG typically divide the task into sentence planning and surface realisation. Sentence planning maps input <mark>semantic symbols</mark> into an intermediary form representing the utterance, e.g. a tree-like or template structure, then surface realisation converts the intermediate structure into the final text (Walker et al., 2002;Stent et al., 2004).<br>",
    "Arabic": "رمز دلالي",
    "Chinese": "语义符号",
    "French": "symbole sémantique",
    "Japanese": "意味記号",
    "Russian": "семантический символ"
  },
  {
    "English": "semantic textual similarity",
    "context": "1: In this paper, we analyze several neural network designs (and their variations) for sentence pair modeling and compare their performance extensively across eight datasets, including paraphrase identification, <mark>semantic textual similarity</mark>, natural language inference, and question answering tasks.<br>2: In order to better understand the effect of the proposed post-processing in downstream systems, we adopt the STS Benchmark dataset on <mark>semantic textual similarity</mark> (Cer et al., 2017) 13 .<br>",
    "Arabic": "التشابه النصي الدلالي",
    "Chinese": "语义文本相似性",
    "French": "similarité textuelle sémantique",
    "Japanese": "意味的なテキストの類似性",
    "Russian": "семантическое текстовое сходство"
  },
  {
    "English": "semantic unit",
    "context": "1: The context modeling we introduced bridges this gap by allowing various granularity of <mark>semantic units</mark>, and allows the user to explore the pattern semantics at the level that corresponds to their beliefs.<br>2: i f (d i , k i ) = 1 \n The first requirement states that, for two <mark>semantic units</mark> equally represented in the sources, we prefer the more informative one. The second requirement is an analogous statement for Relevance. The third requirement is a consistency constraint to preserve additivity of the information measures (Shannon, 1948).<br>",
    "Arabic": "وحدة دلالية",
    "Chinese": "语义单元",
    "French": "unité sémantique",
    "Japanese": "意味単位",
    "Russian": "семантическая единица"
  },
  {
    "English": "semantic vector",
    "context": "1: A major difference is that our method involves the <mark>semantic vector</mark> of the input sequence for generation: y * t = argmax yt P (•|y <t , x, r x ; Θ), where r x = ψ(x; Θ ′ ). This module is plug-in-use as well as is agnostic to model architectures.<br>",
    "Arabic": "متجه دلالي",
    "Chinese": "语义向量",
    "French": "vecteur sémantique",
    "Japanese": "セマンティックベクトル",
    "Russian": "семантический вектор"
  },
  {
    "English": "semantic vector space",
    "context": "1: Lastly, we have shown that ChiSCor can be used to learn a <mark>semantic vector space</mark> that is as intuitive as the semantic space of a much larger reference corpus (Section 4.3).<br>",
    "Arabic": "فضاء المتجهات الدلالي",
    "Chinese": "语义向量空间",
    "French": "espace vectoriel sémantique",
    "Japanese": "意味ベクトル空間",
    "Russian": "семантическое векторное пространство"
  },
  {
    "English": "semi-Markov",
    "context": "1: Once a collection of high-level actions exists, agents are faced with the problem of learning meta-level (typically <mark>semi-Markov</mark>) policies that invoke appropriate high-level actions in sequence (Precup, 2000). The learning problem we describe in this paper is in some sense the direct dual to the problem of learning these meta-level policies : there , the agent begins with an inventory of complex primitives and must learn to model their behavior and select among them ; here we begin knowing the names of appropriate high-level actions but nothing about how they are implemented<br>2: For waveform modeling, by including the state-duration distributions in the model, we can encode a prior on how long we expect the process to remain in each state. We can combine this <mark>semi-Markov</mark> approach with the segmental hidden Markov model described in the last section.<br>",
    "Arabic": "شبه ماركوف",
    "Chinese": "半马尔科夫",
    "French": "semi-markovien",
    "Japanese": "半マルコフ",
    "Russian": "полумарковский"
  },
  {
    "English": "semi-definite programming",
    "context": "1: Yet, it runs reliably, produces optimal result, and does not suffer from the inherent basis-ambiguity issue which plagued many conventional nonrigid factorization techniques. Our method is easy to implement, which involves solving no more than an SDP (<mark>semi-definite programming</mark>) of small and fixed size, a linear Least-Squares or trace-norm minimization.<br>2: In particular, this method does not require costly eigenvalue computations or <mark>semi-definite programming</mark>. We also present an online version of the algorithm and derive associated regret bounds. To demonstrate our algorithm's ability to learn a distance function that generalizes well to unseen points, we compare it to existing state-of-the-art metric learning algorithms.<br>",
    "Arabic": "\"برمجة شبه محددة\"",
    "Chinese": "半正定规划",
    "French": "programmation semi-définie",
    "Japanese": "半正定値計画",
    "Russian": "полуопределенное программирование"
  },
  {
    "English": "semi-supervised clustering",
    "context": "1: To demonstrate the effectiveness of our <mark>semi-supervised clustering</mark> framework, we consider 3 data sets that have the characteristics of being sparse, high-dimensional, and having a small number of points compared to the dimensionality of the space. We derived 3 datasets from the 20-Newsgroups collection.<br>2: Consequently, semi-supervised learning, which uses both labeled and unlabeled data, has become a topic of significant recent interest [11,24,33]. In this paper, we focus on <mark>semi-supervised clustering</mark>, where the performance of unsupervised clustering algorithms is improved with limited amounts of supervision in the form of labels on the data or constraints [38,6,27,39,7].<br>",
    "Arabic": "التجميع شبه المُراقب",
    "Chinese": "半监督聚类",
    "French": "regroupement semi-supervisé",
    "Japanese": "半教師ありクラスタリング",
    "Russian": "полуконтролируемая кластеризация"
  },
  {
    "English": "semi-supervised learning",
    "context": "1: In many areas of machine learning such as clustering, dimensionality reduction, or <mark>semi-supervised learning</mark>, neighborhood graphs are used to model local relationships between data points and to build global structure from local information.<br>2: 2) Threshold-based: we set a threshold max f j (Aug q (x)) ≤ δ (δ = 0.95) to remove those uncertain examples at the end of training, which has been widely used in <mark>semi-supervised learning</mark> [57].<br>",
    "Arabic": "التعلم شبه المراقب",
    "Chinese": "半监督学习",
    "French": "apprentissage semi-supervisé",
    "Japanese": "半教師あり学習",
    "Russian": "полуконтролируемое обучение"
  },
  {
    "English": "semi-supervision",
    "context": "1: In the test stage we use the obtained dictionary for coding data from sessions 2, 3, 4 of CMU-multipie data set, using smooth sparse coding. Note that <mark>semi-supervision</mark> was used only in the dictionary learning stage (the classification stage used supervised SVM).<br>2: Simultaneously exposing all of these signals is highly beneficial, allowing the learned models select the types of <mark>semi-supervision</mark> that are most useful for each end task. Extensive experiments demonstrate that ELMo representations work extremely well in practice.<br>",
    "Arabic": "شبه إشراف",
    "Chinese": "半监督",
    "French": "supervision supervisée partielle",
    "Japanese": "半教師あり学習",
    "Russian": "полу-наблюдение"
  },
  {
    "English": "semidefinite program",
    "context": "1: SPE is formulated as a <mark>semidefinite program</mark> that learns a low-rank kernel matrix constrained by a set of linear inequalities which captures the connectivity structure of the input graph. Traditional graph embedding algorithms do not preserve structure according to our definition, and thus the resulting visualizations can be misleading or less informative.<br>2: We present an adaptation of MVU called MVU+SP, that simply adds the kNN structure preserving constraints to the MVU <mark>semidefinite program</mark>. Similarly, we present an adaptation of the MVE algorithm called MVE+SP that adds the kNN structure preserving constraints to the MVE <mark>semidefinite program</mark> (Shaw & Jebara, 2007).<br>",
    "Arabic": "برنامج شبه محدد",
    "Chinese": "半正定规划",
    "French": "programme semi-défini",
    "Japanese": "半定値計画",
    "Russian": "полуопределенная программа"
  },
  {
    "English": "sense disambiguation",
    "context": "1: Also in Table 2 we compare the <mark>sense disambiguation</mark> precision of our algorithm and the baseline. Here we measure the precision of sense-disambiguation among all examples where each algorithm found a correct hyponym word; our calculation for disambiguation precision is c 1 / (c 1 + c 2 ).<br>",
    "Arabic": "تمييز المعنى",
    "Chinese": "词义消歧",
    "French": "désambiguïsation de sens",
    "Japanese": "曖昧さ回避",
    "Russian": "снятие неоднозначности"
  },
  {
    "English": "sensitive attribute",
    "context": "1: • l-Diversity [28]: A dataset has -diversity if, for every equivalence class, there are at least distinct values for each <mark>sensitive attribute</mark>.<br>2: The demographic parity difference measures the difference between the probability of positive predictions conditioned on <mark>sensitive attribute</mark> A = 1 and that conditioned on A = 0. A large demographic parity difference M dpd means that there is a large prediction gap between the groups with A = 1 A = 0, indicating the unfairness of the model prediction.<br>",
    "Arabic": "سمة حساسة",
    "Chinese": "敏感属性",
    "French": "attribut sensible",
    "Japanese": "敏感属性",
    "Russian": "чувствительный атрибут"
  },
  {
    "English": "sensitivity analysis",
    "context": "1: In addition to the ones covered, we note a third class of simulators that rely on optimization-based implicit timestepping (Todorov et al., 2012;Coumans & Bai, 2016-2021Macklin et al., 2014;Pang, 2021;Howell et al., 2022), which can be made differentiable by <mark>sensitivity analysis</mark> (Boyd & Vandenberghe, 2004).<br>2: Sections 4.3 and 4.4 then give results of a <mark>sensitivity analysis</mark> to the availability of machine-and human-generated metadata to develop the similarity functions and measurement values respectively of Section 3.3.<br>",
    "Arabic": "تحليل الحساسية",
    "Chinese": "敏感性分析",
    "French": "analyse de sensibilité",
    "Japanese": "感度分析",
    "Russian": "анализ чувствительности"
  },
  {
    "English": "sentence classification",
    "context": "1: Although we focus on bench-marking <mark>sentence classification</mark> tasks the selected set of tasks contains variety, from sentiment classification (Yelp Polarity, SST-2) to Natural Language Inference (MNLI, ANLI) to question similarity (QQP). We present our results in Figure 2. The in-trinsic dimensionality of RoBERTa-Base monotonically decreases as we continue pre-training.<br>",
    "Arabic": "تصنيف الجملة",
    "Chinese": "句子分类",
    "French": "classification des phrases",
    "Japanese": "文分類",
    "Russian": "классификация предложений"
  },
  {
    "English": "sentence compression",
    "context": "1: Our model is an extension of the approach put forward in Clarke and Lapata (2006a). Their work tackles <mark>sentence compression</mark> as an optimisation problem.<br>2: The resulting syntactic analysis can be used for various applications such as machine translation (Galley et al., 2004;DeNeefe and Knight, 2009), <mark>sentence compression</mark> (Cohn and Lapata, 2009;Yamangil and Shieber, 2010), and question answering (Wang et al., 2007).<br>",
    "Arabic": "تضغيط الجملة",
    "Chinese": "句子压缩",
    "French": "compression de phrases",
    "Japanese": "文章圧縮",
    "Russian": "сжатие предложений"
  },
  {
    "English": "sentence embedding",
    "context": "1: The final <mark>sentence embedding</mark> v is the row-based max pooling over the output of the last Bi-LSTM layer, where n denotes the number of words within a sentence and m is the number of Bi-LSTM layers (m = 3 in SSE).<br>2: We now test our metrics on other languages and datasets. For UScore snt , we train its <mark>sentence embedding</mark> model for six iterations. For UScore wrd , we remap mBERT once with UMD and make use of a language model and pseudo references obtained from an MT system.<br>",
    "Arabic": "تضمين الجملة",
    "Chinese": "句向量",
    "French": "embedding de phrase",
    "Japanese": "文埋め込み",
    "Russian": "вложение предложения"
  },
  {
    "English": "sentence encoder",
    "context": "1: Specifically, ( 1) we remove the extra semantic encoder and construct the sentence-level representations by averaging the sequence of outputs of the vanilla <mark>sentence encoder</mark>. (2) We replace the default 4-layer semantic encoder with a large pre-trained model (PTM) (i.e., XLM-R (Conneau et al., 2020)).<br>2: min l j=1 [−(y i,j logŷ i,j ) + (1 − y i,j ) log(1 −ŷ i,j )] (1) \n The model architecture borrows the <mark>sentence encoder</mark> (without any stochastic layers) from (Bowman et al.<br>",
    "Arabic": "مُشفر الجملة",
    "Chinese": "句子编码器",
    "French": "encodeur de phrase",
    "Japanese": "文エンコーダー",
    "Russian": "энкодер предложений"
  },
  {
    "English": "sentence representation",
    "context": "1: It is unclear how to evaluate MPP by utilizing this context window, given recent research that has raised questions about the <mark>sentence representations</mark> acquired in long-form input (Sinha et al., 2022;Haviv et al., 2022).<br>2: Since noisy data is beneficial for contrastive learning , we expect this paradigm to work well with pseudo-parallel data. We use pooled XLM-R embeddings as <mark>sentence representations</mark>, and, as with unsupervised remapping, we experiment with multiple iterations of successive mining and sentence embedding induction operations.<br>",
    "Arabic": "تمثيل الجملة",
    "Chinese": "句子表征",
    "French": "représentation de phrases",
    "Japanese": "文の表現",
    "Russian": "представление предложения"
  },
  {
    "English": "sentence segmentation",
    "context": "1: Consider the reference segmentation r and candidate segmentations h 1 and h 2 in Figure 9: h 1 and h 2 are equidistant to r under A with Jaccard (0.58), B, and WindowDiff. However , for a task like topic segmentation , h 2 may be preferred , as it contains `` meta '' topics that consistently match two topics each in r , whereas h 1 contains two correct topics , but one really bad third topic , which is a mixture of four topics in r. Conversely , for a task like <mark>sentence segmentation</mark> ,<br>",
    "Arabic": "تقسيم الجمل",
    "Chinese": "句子分割",
    "French": "segmentation des phrases",
    "Japanese": "文の分割",
    "Russian": "сегментация предложений"
  },
  {
    "English": "sentence vector",
    "context": "1: In our model, each sentence from document d is fed into a pretrained RoBERTa Base model 13 to obtain contextualized word embeddings. The word embeddings are then averaged using self-attention, creating <mark>sentence vectors</mark>. For Task 3, these vectors are then used directly for sentence-level predictions.<br>",
    "Arabic": "متجه الجملة",
    "Chinese": "句向量",
    "French": "vecteur phrastique",
    "Japanese": "文ベクトル",
    "Russian": "векторы предложений"
  },
  {
    "English": "sentence-level",
    "context": "1: We introduce OpenKiwi, a PyTorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and <mark>sentence-level</mark> quality estimation systems, implementing the winning systems of the WMT 2015-18 quality estimation campaigns.<br>2: In addition, we exploit additional networks to adaptively assess the labeling bias by considering contextual information. Our performance study on <mark>sentence-level</mark> and document-level REs confirms the effectiveness of the dual supervision framework.<br>",
    "Arabic": "على مستوى الجملة",
    "Chinese": "句子级",
    "French": "au niveau de la phrase",
    "Japanese": "文レベル",
    "Russian": "на уровне предложений"
  },
  {
    "English": "sentence-level classification",
    "context": "1: At this moment, the network branches, as it is trained with three objectives: (i) the main BIO tag prediction objective and two auxiliary ones, namely (ii) token-level technique classification, and (iii) <mark>sentence-level classification</mark>.<br>2: This requires a training schedule that determines when to optimize for the <mark>sentence-level classification</mark> objective, and when to optimize the machine attention at the token level.<br>",
    "Arabic": "التصنيف على مستوى الجملة",
    "Chinese": "句子级分类",
    "French": "classification au niveau de la phrase",
    "Japanese": "文章レベルの分類",
    "Russian": "классификация на уровне предложений"
  },
  {
    "English": "sentence-level representation",
    "context": "1: ; Guan et al. (2021b) inserted special tokens for each sentence and devised several pre-training tasks to learn sentencelevel representations. We are inspired to use a sentence order prediction task to learn high-level discourse representations.<br>",
    "Arabic": "تمثيل على مستوى الجملة",
    "Chinese": "句子级表示",
    "French": "représentation au niveau de la phrase",
    "Japanese": "文レベル表現",
    "Russian": "векторное представление предложения"
  },
  {
    "English": "sentence-piece",
    "context": "1: To extend XLM-R's vocabulary, we use <mark>Sentence-Piece</mark> (Kudo and Richardson, 2018) with a unigram language model (Kudo, 2018) to train a tokenizer with a vocabulary size of 250K on Glot500-c. We sample data from different language-scripts according to a multinomial distribution, with =.3.<br>",
    "Arabic": "قطعة الجملة",
    "Chinese": "句子分词",
    "French": "pièce de phrase",
    "Japanese": "文区分",
    "Russian": "Sentence-Piece"
  },
  {
    "English": "sentiment analysis",
    "context": "1: Most of the recent literature on <mark>Sentiment Analysis</mark> over Twitter is tied to the idea that the sentiment is a function of an incoming tweet. However, tweets are filtered through streams of posts, so that a wider context, e.g. a topic, is always available. In this work, the contribution of this contextual information is investigated.<br>2: An example is \"rug → {sofa, ottoman, carpet, hallway}\", with carpet being the most synonym-like candidate to the target. <mark>Sentiment Analysis</mark> (SA). Socher et al. ( 2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts.<br>",
    "Arabic": "تحليل المشاعر",
    "Chinese": "情感分析",
    "French": "analyse des sentiments",
    "Japanese": "感情分析",
    "Russian": "анализ тональности"
  },
  {
    "English": "sentiment analysis model",
    "context": "1: Predictions to Labeled Instances To handle challenge (1), we leverage the fact that all mod-Figure 3: A word-level HotFlip attack on a <mark>sentiment analysis model</mark>-replacing \"anyone\" with \"inadequate\" causes the model's prediction to change from Positive to Negative.<br>2: We approached the team responsible for the general purpose <mark>sentiment analysis model</mark> sold as a service by Microsoft ( on Table 1). Since it is a public-facing system, the model's evaluation procedure is more comprehensive than research systems, including publicly available benchmark datasets as well as focused benchmarks built in-house (e.g. negations, emojis).<br>",
    "Arabic": "نموذج تحليل المشاعر",
    "Chinese": "情感分析模型",
    "French": "modèle d'analyse des sentiments",
    "Japanese": "感情分析モデル",
    "Russian": "модель анализа тональности"
  },
  {
    "English": "sentiment classification",
    "context": "1: Although we focus on bench-marking sentence classification tasks the selected set of tasks contains variety, from <mark>sentiment classification</mark> (Yelp Polarity, SST-2) to Natural Language Inference (MNLI, ANLI) to question similarity (QQP). We present our results in Figure 2. The in-trinsic dimensionality of RoBERTa-Base monotonically decreases as we continue pre-training.<br>2: While all LLMs are sensitive to sociodemographic prompting, we identified model scale and the number of instruction-tuning tasks as relevant factors for improving model performance. Toxicity detection and <mark>sentiment classification</mark> are the tasks which benefit the most from this technique. Further, the model family and prompt formulation have a strong influence on model predictions.<br>",
    "Arabic": "تصنيف المشاعر",
    "Chinese": "情感分类",
    "French": "classification des sentiments",
    "Japanese": "感情分類",
    "Russian": "классификация настроений"
  },
  {
    "English": "sentiment classifier",
    "context": "1: The prompts are prefixes from the IMDB dataset of length 2-8 tokens. We use the pre-trained <mark>sentiment classifier</mark> siebert/sentiment-roberta-large-english as a ground-truth reward model and gpt2-large as a base model. We use these larger models as we found the default ones to generate low-quality text and rewards to be somewhat inaccurate.<br>2: To measure the positivity of the generated reframed thought, we use a RoBERTa-based <mark>sentiment classifier</mark> fine-tuned on the TweetEval benchmark (Barbieri et al., 2020). Empathy. To measure empathy, we build upon the empathy classification model presented in Sharma et al. (2020b).<br>",
    "Arabic": "مُصنِّف المشاعر",
    "Chinese": "情感分类器",
    "French": "classificateur de sentiments",
    "Japanese": "感情分類器",
    "Russian": "классификатор тональности"
  },
  {
    "English": "sentiment detection",
    "context": "1: Intuitions seem to differ as to the difficulty of the <mark>sentiment detection</mark> problem. An expert on using machine learning for text categorization predicted relatively low performance for automatic methods.<br>2: Models either use measures based on held-out likelihood [4,5] or an external task that is independent of the topic space such as <mark>sentiment detection</mark> [10] or information retrieval [11]. This is true even for models engineered to have semantically coherent topics [12]. For models that use held-out likelihood, Wallach et al.<br>",
    "Arabic": "الكشف عن المشاعر",
    "Chinese": "情感检测",
    "French": "détection de sentiments",
    "Japanese": "感情検出",
    "Russian": "обнаружение настроения"
  },
  {
    "English": "sentiment transfer",
    "context": "1: Here, we propose a reinforcement learning (RL) model for the task of empathic rewriting (Section 5). Previous work has used RL for the task of <mark>sentiment transfer</mark> [37] by only using text generations as actions.<br>2: Text style transfer aims to endow a text with a different style while keeping its main semantic content unaltered. It has a wide range of applications, such as formality transfer (Jain et al., 2019), <mark>sentiment transfer</mark> (Shen et al., 2017) and author-style imitation (Tikhonov and Yamshchikov, 2018).<br>",
    "Arabic": "نقل المشاعر",
    "Chinese": "情感转移",
    "French": "transfert de sentiment",
    "Japanese": "感情転移",
    "Russian": "передача настроения"
  },
  {
    "English": "separation oracle",
    "context": "1: Structural SVM, with cutting plane training [16], is one of the popular methods for learning over structured data. The most expensive step with cutting plane iteration is the call to the <mark>separation oracle</mark> which identifies the most violated constraint. In particular, given the current SVM estimate w, the <mark>separation oracle</mark> computeŝ \n<br>",
    "Arabic": "محكم الفصل",
    "Chinese": "分离预言机",
    "French": "oracle de séparation",
    "Japanese": "分離オラクル",
    "Russian": "разделяющий оракул"
  },
  {
    "English": "separation parameter",
    "context": "1: , Russo et al. , 2020 . Formally, guillotine partitions are defined recursively, as follows. P i ⊆ L for each i ∈ [t] \n , and a <mark>separation parameter</mark> s. We say that P forms an s-separated guillotine partition of L if one of the following three conditions holds: \n<br>",
    "Arabic": "مُعامل الفصل",
    "Chinese": "分割参数",
    "French": "paramètre de séparation",
    "Japanese": "分離パラメータ",
    "Russian": "разделительный параметр"
  },
  {
    "English": "separator token",
    "context": "1: where [sep] denotes a <mark>separator token</mark> (e.g., newlines), and ⊕ denotes the concatenation operator.<br>2: In order to compare our approach to such a baseline, we concatenate the original input x with a <mark>separator token</mark> and z ′ . We then feed this as input to a BART-base model which is trained to predict the output sequence y.<br>",
    "Arabic": "رمز فاصل",
    "Chinese": "分隔符标记",
    "French": "jeton séparateur",
    "Japanese": "セパレータトークン",
    "Russian": "токен-разделитель"
  },
  {
    "English": "seq2seq model",
    "context": "1: (2022) heuristically induce a quasi-synchronous grammar (QCFG, Smith and Eisner (2006)) and use it for data augmentation for a <mark>seq2seq model</mark>. Kim (2021) introduces neural QCFGs which perform well on compositional generalization tasks but are very compute-intensive.<br>2: Okapi. In Fig. 5 we show the accuracy of our model on the document domain in comparison with previous work by number of conjuncts in the logical form. Permutation baseline. A simpler approach for predicting a permutation of the output z ′ from the multiset tagging is to use a <mark>seq2seq model</mark>.<br>",
    "Arabic": "نموذج التتابع إلى التتابع",
    "Chinese": "seq2seq模型",
    "French": "modèle seq2seq",
    "Japanese": "seq2seqモデル",
    "Russian": "модель последовательность-к-последовательности"
  },
  {
    "English": "sequence",
    "context": "1: The lookup table layer maps the original sentence into a <mark>sequence</mark> x(•) of n identically sized vectors: \n (x 1 , x 2 , . . . , x n ), ∀t x t ∈ R d . (1) \n Obviously the size n of the <mark>sequence</mark> varies depending on the sentence.<br>2: Pre-training. The promptable segmentation task suggests a natural pre-training algorithm that simulates a <mark>sequence</mark> of prompts (e.g., points, boxes, masks) for each training sample and compares the model's mask predictions against the ground truth.<br>",
    "Arabic": "تسلسل",
    "Chinese": "序列",
    "French": "séquence",
    "Japanese": "シーケンス",
    "Russian": "последовательность"
  },
  {
    "English": "sequence alignment",
    "context": "1: c n,•,c ⊗ n+1,c,• . Under this approach, assuming enough parallel cores, we only need O(log N ) steps in Python and can use parallel operations for the rest. Similar parallel approach can also be used for computing <mark>sequence alignment</mark> and semi-Markov models.<br>",
    "Arabic": "محاذاة التسلسل",
    "Chinese": "序列比对",
    "French": "alignement de séquences",
    "Japanese": "シーケンスアライメント",
    "Russian": "выравнивание последовательностей"
  },
  {
    "English": "sequence classification",
    "context": "1: We have shown that human attention provides a useful inductive bias on machine attention in recurrent neural networks for <mark>sequence classification</mark> problems. We present an architecture that enables us to leverage human attention signals from general, publicly available eye-tracking corpora, to induce better, more robust task-specific NLP models.<br>2: Team aschern(TC:2) (Chernyavskiy et al., 2020) was the second best, and it based its success on a RoBERTa ensemble with several interesting techniques. They treated the task as one of <mark>sequence classification</mark>, using an average embedding of the surrounding tokens and the length of the span as contextual features.<br>",
    "Arabic": "تصنيف التسلسل",
    "Chinese": "序列分类",
    "French": "classification de séquence",
    "Japanese": "シーケンス分類",
    "Russian": "классификация последовательностей"
  },
  {
    "English": "sequence database",
    "context": "1: The second phase of CLM occurs in Step 5 of CTS' cognitive cycle. First, it mines rules from the sequences of events by removing the time for each recorded event during CTS' executions. To do so, the algorithm takes as input the <mark>sequence database</mark> (sequences of coalitions that were broadcasted for each execution of CTS).<br>2: It then produces the set of all causal rules contained in the database as output. The algorithm starts by ignoring the temporal information from the <mark>sequence database</mark> to obtain a transaction database.<br>",
    "Arabic": "قاعدة بيانات التسلسل",
    "Chinese": "序列数据库",
    "French": "base de données de séquences",
    "Japanese": "シーケンスデータベース",
    "Russian": "база данных последовательностей"
  },
  {
    "English": "sequence generation",
    "context": "1: Unlike traditional single-label classification where only one label is assigned to each sample, each sample in the MLC task can have multiple labels. From the perspective of <mark>sequence generation</mark>, the MLC task can be modeled as finding an optimal label sequence y * that maximizes the conditional probability p(y|x), which is calculated as follows: \n<br>2: This process resembles scheduled sampling (Bengio et al., 2015), a technique commonly employed in training models for <mark>sequence generation</mark> tasks like machine translation: when updating decoder hidden states, either the gold token * or the predicted tokenˆmay be used, and the decision is made via a random draw.<br>",
    "Arabic": "توليد التسلسل",
    "Chinese": "序列生成",
    "French": "génération de séquences",
    "Japanese": "シーケンス生成",
    "Russian": "генерация последовательности"
  },
  {
    "English": "sequence labeling",
    "context": "1: Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence.<br>2: Rei and Yannakoudakis (2016) converted the dataset for a <mark>sequence labeling</mark> task and we use their splits for training, development and testing. Similarly to Rei and Søgaard (2018), we perform sentence-level binary classification of sentences that need some editing vs. grammatically correct sentences. We do not use the tokenlevel labels for training our model.<br>",
    "Arabic": "تسمية التسلسل",
    "Chinese": "序列标注",
    "French": "étiquetage de séquence",
    "Japanese": "シーケンスラベリング",
    "Russian": "разметка последовательностей"
  },
  {
    "English": "sequence labeling model",
    "context": "1: For example, GRO-BID (Grobid, 2008(Grobid, -2023, a widely-adopted software tool for scientific document processing, uses twelve interdependent <mark>sequence labeling models</mark> 3 to perform its full text extraction. Other similar tools inlude CERMINE (Tkaczyk et al., 2015) and ParsCit (Councill et al., 2008).<br>",
    "Arabic": "نموذج تسمية التسلسلات",
    "Chinese": "序列标注模型",
    "French": "modèle d'étiquetage de séquence",
    "Japanese": "シーケンスラベリングモデル",
    "Russian": "модель разметки последовательностей"
  },
  {
    "English": "sequence length",
    "context": "1: where U ∈ R L×3×d is the output tensor, L is the <mark>sequence length</mark> and d is the hidden state size. The second optimization performs all elementwise operations in an efficient way. This involves \n f [ t ] = σ ( U [ t , 0 ] + v c [ t-1 ] + b ) ( 2 ) r [ t ] = σ ( U [ t , 1 ] + v c [ t-1 ] + b ) ( 3 ) c [ t ] = f [ t ] c [ t-1 ] +<br>2: Both Pegasus and MBART are limited to a maximum <mark>sequence length</mark> of 1,024 tokens, which is well below the median length of each dataset. Layout-aware models with standard input size We introduce layout-aware extensions of Pegasus and MBART, respectively denoted as Pe-gasus+Layout and MBART+Layout. Following LayoutLM ( Xu et al. , 2020 ) , which is state-ofthe-art on several document understanding tasks ( Jaume et al. , 2019 ; Huang et al. , 2019 ; Harley et al. , 2015 ) , each token bounding box coordinates ( x 0 , y 0 , x 1 , y 1 ) is normalized into an integer in the range<br>",
    "Arabic": "طول التسلسل",
    "Chinese": "序列长度",
    "French": "longueur de séquence",
    "Japanese": "シーケンス長",
    "Russian": "длина последовательности"
  },
  {
    "English": "sequence model",
    "context": "1: The factorization turns the joint modeling problem into a sequence problem, where one learns to predict the next pixel given all the previously generated pixels. But to model the highly nonlinear and longrange correlations between pixels and the complex conditional distributions that result, a highly expressive <mark>sequence model</mark> is necessary.<br>2: Surprisal from the LSTM <mark>sequence model</mark> did not reliably predict EEG amplitude at any timepoint or electrode. The DISTANCE predictor did derive a central positivity around 600 ms post-word onset as shown in Figure 3a. SURPRISAL predicted an early frontal positivity around 250 ms, shown in Figure 3b.<br>",
    "Arabic": "نموذج تسلسلي",
    "Chinese": "序列模型",
    "French": "modèle de séquence",
    "Japanese": "系列モデル",
    "Russian": "модель последовательности"
  },
  {
    "English": "sequence prediction",
    "context": "1: • Full Supervision Sequence prediction problems like ours are typically addressed using supervised techniques. We measure how a standard supervised approach would perform on this task by using a reward signal based on manual annotations of output action sequences, as defined in Section 5.2.<br>",
    "Arabic": "التنبؤ بالتسلسل",
    "Chinese": "序列预测",
    "French": "prédiction de séquence",
    "Japanese": "シーケンス予測 (shīkensu yosoku)",
    "Russian": "предсказание последовательности"
  },
  {
    "English": "sequence tagging",
    "context": "1: At the output, the token representations are fed into an output layer for tokenlevel tasks, such as <mark>sequence tagging</mark> or question answering, and the [CLS] representation is fed into an output layer for classification, such as entailment or sentiment analysis. Compared to pre-training, fine-tuning is relatively inexpensive.<br>2: It involves discovering one or more generic intent types from text utterances, that may not have been encountered during training. We propose a novel, domain-agnostic approach, OPINE, which formulates the problem as a <mark>sequence tagging</mark> task in an open-world setting.<br>",
    "Arabic": "تسمية التسلسل",
    "Chinese": "序列标注",
    "French": "étiquetage de séquences",
    "Japanese": "系列タグ付け",
    "Russian": "разметка последовательности"
  },
  {
    "English": "sequence transduction",
    "context": "1: Though RnG-KBQA uses T5 to decode the target program as unconstrained <mark>sequence transduction</mark>, it still heavily depends on candidate enumeration as a prerequisite. Therefore, it is not a generation-based model like ours.<br>2: A few recent studies (Liang et al., 2017;Chen et al., 2021) formulate semantic parsing over the KB as <mark>sequence transduction</mark> using encoderdecoder models to enable more flexible generation. Chen et al.<br>",
    "Arabic": "تحويل التسلسل",
    "Chinese": "序列转换",
    "French": "transduction de séquence",
    "Japanese": "シーケンス変換",
    "Russian": "последовательное преобразование"
  },
  {
    "English": "sequence-to-sequence",
    "context": "1: As shown in Figure 1, we formulate all the tasks into a unified <mark>sequence-to-sequence</mark> format in which the input text, images, instructions, and bounding boxes are represented in the same token space.<br>2: Below we describe in detail the systems and their main features. • BUT-FIT (Jon et al., 2020) experiments with both the <mark>sequence-to-sequence</mark> approach and the language generation approach. For the <mark>sequence-to-sequence</mark> approach, they use BART (Lewis et al., 2019) with beam-search decoding to achieves the highest BLEU among all the teams.<br>",
    "Arabic": "التسلسل إلى التسلسل",
    "Chinese": "序列到序列",
    "French": "séquence-à-séquence",
    "Japanese": "シーケンスツーシーケンス",
    "Russian": "последовательность-к-последовательности"
  },
  {
    "English": "sequence-to-sequence architecture",
    "context": "1: First, to get sufficient query-document pairs for training, we leverage a query generation network to obtain possible pairs of queries and documents. Second, we utilize the hierarchical k-means algorithm to generate a semantic identifier for each document. Third, we design a prefix-aware weight-adaptive decoder to replace the vanilla one in a <mark>sequence-to-sequence architecture</mark>.<br>",
    "Arabic": "بنية التسلسل إلى التسلسل",
    "Chinese": "序列到序列架构",
    "French": "architecture de séquence à séquence",
    "Japanese": "シーケンス・ツー・シーケンス・アーキテクチャ",
    "Russian": "архитектура последовательность-к-последовательности"
  },
  {
    "English": "sequence-to-sequence generation",
    "context": "1: Pre-training objective. To pre-train the model, we leverage the gap-sentences generation (GSG) unsupervised pre-training objective, which was introduced by PEGASUS  and is well-suited for <mark>sequence-to-sequence generation</mark>. Unlike BART (Lewis et al., 2020) or T5 (Raffel et al., 2020) pre-training objectives, GSG endows the model with zero-shot summarization capabilities.<br>2: Decoding. Sequence-to-sequence generation is the task of generating an output sequence y given an input sequence x. We consider standard leftto-right, autoregressive models, p θ (y | x) = |y| t=1 p θ (y t | y <t , x), and omit x to reduce clutter. Decoding consists of solving, \n<br>",
    "Arabic": "توليد تسلسل إلى تسلسل",
    "Chinese": "序列到序列生成",
    "French": "génération de séquence à séquence",
    "Japanese": "シーケンス対シーケンス生成",
    "Russian": "генерация последовательности к последовательности"
  },
  {
    "English": "sequence-to-sequence model",
    "context": "1: After that, we use T5 (Xiong et al., 2017), a pre-trained <mark>sequence-to-sequence model</mark>, to summarize the filtered abstract to be the explanation. With this method, ESRA can generate different explanations for the same paper given different queries.<br>2: the target sentence with T ′ tokens . A sequence-tosequence model is usually applied to neural machine translation, which aims to learn a transformation from the source space to the target space X → Y : f (y|x; Θ) with the usage of parallel data. Formally, given a set of observed sentence pairs \n<br>",
    "Arabic": "نموذج تسلسل إلى تسلسل",
    "Chinese": "序列到序列模型",
    "French": "modèle de séquence à séquence",
    "Japanese": "シーケンス対シーケンスモデル",
    "Russian": "модель последовательности к последовательности"
  },
  {
    "English": "sequence-to-sequence transduction",
    "context": "1: An online algorithm may commit to possibly suboptimal decisions before it has seen all the input, as in simultaneous MT or online <mark>sequence-to-sequence transduction</mark> (Jaitly et al., 2016;Yu et al., 2016). By analogy, an online parser might be expected to start printing the parse early.<br>",
    "Arabic": "التحويل من تسلسل إلى تسلسل",
    "Chinese": "序列到序列转导",
    "French": "transduction de séquence à séquence",
    "Japanese": "シーケンス対シーケンス変換",
    "Russian": "трансдукция из последовательности в последовательность"
  },
  {
    "English": "sequential datum",
    "context": "1: ) . This is not a limiting assumption especially in <mark>sequential data</mark>, i.e., for videos. We focus our study on the setting where factors of variations are not observable at all, i.e. we only observe samples from P (x).<br>2: Though we focus on the connection to decision making and control in this work, it is important to note that the principle of maximum causal entropy is not specific to those domains. It is a general approach that is applicable to any <mark>sequential data</mark> where future side information's assumed lack of causal influence over earlier variables is reasonable.<br>",
    "Arabic": "بيانات تتابعية",
    "Chinese": "序列数据",
    "French": "donnée séquentielle",
    "Japanese": "\"連続データ\"",
    "Russian": "последовательные данные"
  },
  {
    "English": "sequential decision making",
    "context": "1: These systems can serve to improve our understanding of incremental parsing and <mark>sequential decision making</mark>, and the underlying computational methods may be useful in the analysis of other incremental contexts.<br>2: In this work, we address one form of lifelong learning for <mark>sequential decision making</mark> problems, wherein the set of possible actions (decisions) varies over time. Such a situation is omnipresent in real-world problems.<br>",
    "Arabic": "اتخاذ القرارات التسلسلية",
    "Chinese": "顺序决策",
    "French": "prise de décision séquentielle",
    "Japanese": "逐次的意思決定",
    "Russian": "последовательное принятие решений"
  },
  {
    "English": "sequential decision-making process",
    "context": "1: The decoding process can be viewed as a <mark>sequential decision-making process</mark>, which decomposes the task of finding a program from the enormous search space into making decisions from a sequence of smaller search spaces.<br>",
    "Arabic": "عملية صنع القرار التسلسلية",
    "Chinese": "顺序决策过程",
    "French": "processus de prise de décision séquentielle",
    "Japanese": "逐次的な意思決定プロセス",
    "Russian": "процесс последовательного принятия решений"
  },
  {
    "English": "sequential sampler",
    "context": "1: Notice that, while the <mark>sequential sampler</mark> achieves the correct marginal probability relatively quickly, the asynchronous samplers take a much longer time to achieve the correct result, even for a relatively small expected delay (τ = 0.5).<br>",
    "Arabic": "أخذ العينات متسلسل",
    "Chinese": "顺序采样器",
    "French": "échantillonneur séquentiel",
    "Japanese": "順次サンプラー",
    "Russian": "последовательный сэмплер"
  },
  {
    "English": "sequential tagging",
    "context": "1: This is quite interesting as it has been shown that communities behave in a coherent way and users tend to take stable standing points. Experimental evaluation (Chapter 4) proves the effectiveness of this proposed <mark>sequential tagging</mark> approach combined with the adopted contextual information, improving the percentage of correctly recognized tweets up to 12%.<br>2: On this set, both ADUT-Charniak and ADUT-Stanford significantly outperform their respective baselines. We compare with the state-of-the-art system of (Surdeanu et al., 2007). In (Surdeanu et al., 2007), the authors use three models: Model 1 and 2 do <mark>sequential tagging</mark> of chunks obtained from shallow parse and full parse.<br>",
    "Arabic": "الوسم التسلسلي",
    "Chinese": "序列标注",
    "French": "étiquetage séquentiel",
    "Japanese": "順次タグ付け",
    "Russian": "последовательная разметка"
  },
  {
    "English": "set cover problem",
    "context": "1: At the first iteration with X 0 = ∅, either variant then corresponds to the <mark>set cover problem</mark> with the simple modular upper bound f (X) ≤ m f ∅ (X) = j∈X f (j) \n where m f X t refers to either variant.<br>2: The requirements on T can be viewed as a <mark>set cover problem</mark>, where the universe U is (h, h ) ∈ H 2 : µ(h) − µ(h ) > 2 , and the set system is \n<br>",
    "Arabic": "مشكلة تغطية المجموعة",
    "Chinese": "集合覆盖问题",
    "French": "problème de couverture d'ensemble",
    "Japanese": "集合被覆問題",
    "Russian": "проблема покрытия множеств"
  },
  {
    "English": "set function",
    "context": "1: In general, this placement score (representing, e.g., the fraction of detected cascades, or the population saved by placing a water quality sensor) is a <mark>set function</mark> R, mapping every placement A to a real number R(A) (our reward), which we intend to maximize.<br>2: If R is a submodular, nondecreasing <mark>set function</mark> and R(∅) = 0, then the greedy algorithm finds a set AG, such that R(AG) ≥ (1−1/e) max |A|=B R(A).<br>",
    "Arabic": "دالة مجموعة",
    "Chinese": "集合函数",
    "French": "fonction d'ensemble",
    "Japanese": "集合関数",
    "Russian": "функция множества"
  },
  {
    "English": "shallow network",
    "context": "1: Having defined a layout selection module p(z|x; θ ) and a network execution model p z (y|w; θ e ), we are ready to define a model for predicting answers given only (world, question) pairs. The key constraint is that we want to minimize evaluations of p z ( y|w ; θ e ) ( which involves expensive application of a deep network to a large input representation ) , but can tractably evaluate p ( z|x ; θ ) for all z ( which involves application of a <mark>shallow network</mark> to a relatively small set of candidates )<br>",
    "Arabic": "شبكة ضحلة",
    "Chinese": "浅层网络",
    "French": "réseau peu profond",
    "Japanese": "浅いネットワーク",
    "Russian": "неглубокая сеть"
  },
  {
    "English": "shape matching",
    "context": "1: Establishing correspondences between two sets of visual features is a key problem in computer vision tasks as diverse as feature tracking [5], image classification [15] or retrieval [23], object detection [4], <mark>shape matching</mark> [28,16], or wide-baseline stereo fusion [21].<br>2: In this section we describe a simple yet very effective method for non-rigid <mark>shape matching</mark> based on the functional representation of mappings between shapes. The simplest version of the <mark>shape matching</mark> algorithm is summarized in Algorithm 1. Namely, suppose we are given two shapes M and N in their discrete (e.g.<br>",
    "Arabic": "مطابقة الأشكال",
    "Chinese": "形状匹配",
    "French": "mise en correspondance de formes",
    "Japanese": "形状マッチング",
    "Russian": "сопоставление форм"
  },
  {
    "English": "shape prior",
    "context": "1: The prior for each part is defined by corresponding shape and pose priors, for which we use 0-mean standard normal priors for each parameter except for scaling factors, which are encouraged to be close to 1. Details and relative weights can be found in supplementary materials.<br>",
    "Arabic": "المعلم المسبق للشكل",
    "Chinese": "形状先验",
    "French": "a priori de forme",
    "Japanese": "形状事前確率",
    "Russian": "априорное знание о форме"
  },
  {
    "English": "shift invariant",
    "context": "1: We assume it is <mark>shift invariant</mark>: f ji (t i |t j ) = f ji (τ ji ), where τ ji := t i − t j , and nonnegative: f ji (τ ji ) = 0 if τ ji < 0.<br>",
    "Arabic": "ثابت تحت الإزاحة",
    "Chinese": "平移不变",
    "French": "invariant par translation",
    "Japanese": "シフト不変",
    "Russian": "инвариантный к сдвигу"
  },
  {
    "English": "shift reduce parser",
    "context": "1: • ESIM: Similar but different from DecAtt, ESIM batches sentences with varied length and uses masks to filter out padding information. In order to batch the parse trees within Tree-LSTM recursion, we follow Bowman et al. 's (2016) procedure that converts tree structures into the linear sequential structure of a <mark>shift reduce parser</mark>.<br>",
    "Arabic": "محلل نقل-تقليص",
    "Chinese": "移位归约分析器",
    "French": "analyseur à décalage-réduction",
    "Japanese": "シフト縮約パーサー",
    "Russian": "анализатор со сдвигом и редукцией"
  },
  {
    "English": "short path",
    "context": "1: For example, if we encode a weighted directed graph using a ternary predicate edge, then rules ( 1) and (2), where sp is a min limit predicate, compute the cost of a <mark>shortest path</mark> from a given source node v 0 to every other node. → sp(v 0 , 0) \n<br>2: path ( the <mark>shortest path</mark> ) . Given the high success rates we observe, SPL can be roughly interpreted as efficiency of the path taken compared to the oracle pathe.g.<br>",
    "Arabic": "المسار القصير",
    "Chinese": "最短路径",
    "French": "chemin le plus court",
    "Japanese": "最短経路",
    "Russian": "кратчайший путь"
  },
  {
    "English": "short path algorithm",
    "context": "1: During the execution, every time the <mark>shortest path algorithm</mark> visits the next vertex vi, we advance the starting position of each Wj by one position and its ending position until the cost of representing the currently covered portion of S is larger than F (1 + ) j .<br>2: This equivalent formulation suggests a naive sampling (NS) algorithm for approximating σ(A, T ): draw n samples of {τ ji } (j,i)∈E , run a <mark>shortest path algorithm</mark> for each sample, and finally average the results (see Appendix C for more details).<br>",
    "Arabic": "خوارزمية المسار الأقصر",
    "Chinese": "最短路径算法",
    "French": "algorithme du plus court chemin",
    "Japanese": "最短経路アルゴリズム",
    "Russian": "алгоритм кратчайшего пути"
  },
  {
    "English": "short path kernel",
    "context": "1: (2) <mark>shortest path kernel</mark> (SP) [Borgwardt and Kriegel, 2005]: The <mark>shortest path kernel</mark> counts pairs of shortest paths in two graphs having the same source and sink labels and identical length.<br>2: The <mark>shortest path kernel</mark> is competitive to the WL kernel on smaller graphs (MUTAG, NCI1, NCI109), but on D&D its runtime degenerates to more than 23 hours.<br>",
    "Arabic": "نواة المسار القصير",
    "Chinese": "最短路径核",
    "French": "noyau de chemin le plus court",
    "Japanese": "最短経路カーネル",
    "Russian": "ядро кратчайшего пути"
  },
  {
    "English": "short path length",
    "context": "1: Many of the properties of interest in these studies are based on two fundamental parameters: the nodes' degrees (i.e., the number of edges incident to each node), and the distances between pairs of nodes (as measured by shortestpath length).<br>2: To calculate SPL in these scenarios, the <mark>shortest path length</mark> for the task is the minimum <mark>shortest path length</mark> from the starting position of the agent to any of the reachable target objects of the given type, regardless of which instance the agent navigates towards. Actions.<br>",
    "Arabic": "طول المسار القصير",
    "Chinese": "最短路径长度",
    "French": "longueur du plus court chemin",
    "Japanese": "最短経路長",
    "Russian": "короткая длина пути"
  },
  {
    "English": "siamese architecture",
    "context": "1: Since the action and card representations provide different kinds of information to the learning architecture, we first isolate the parameter-sharing of the <mark>Siamese architecture</mark> to enable the two ConvNets to learn adaptive feature representations, which are then fused through fully connected layers to produce the desired actions. This design is the reason why we call it pseudo-<mark>Siamese architecture</mark>.<br>2: As shown in Figure 2, the input of the architecture is the game state representations of action and card information, which are respectively sent to the top and bottom streams of the <mark>Siamese architecture</mark>.<br>",
    "Arabic": "هندسة توأمية",
    "Chinese": "孪生架构",
    "French": "architecture siamoise",
    "Japanese": "シャムアーキテクチャ",
    "Russian": "сиамская архитектура"
  },
  {
    "English": "siamese network",
    "context": "1: Another category of methods for unsupervised representation learning are based on clustering [5,6,1,7]. They alternate between clustering the representations and learning to predict the cluster assignment. SwAV [7] incorporates clustering into a <mark>Siamese network</mark>, by computing the assignment from one view and predicting it from another view.<br>2: Second, if we implement (11) by reducing the loss with one SGD step, then we can approach the SimSiam algorithm: a <mark>Siamese network</mark> naturally with stop-gradient applied. Predictor. Our above analysis does not involve the predictor h. We further assume that h is helpful in our method because of the approximation due to (10).<br>",
    "Arabic": "شبكة سيامية",
    "Chinese": "孪生网络",
    "French": "réseau siamois",
    "Japanese": "\"相姿ネットワーク\"",
    "Russian": "сиамская сеть"
  },
  {
    "English": "sibling model",
    "context": "1: For example, setting σ 1 = FIRST and σ t>1 = REST corresponds to first-order models, while setting σ 1 = NULL and σ t>1 = x t−1 corresponds to <mark>sibling models</mark> (Eisner, 2000;McDonald et al., 2005;McDonald and Pereira, 2006).<br>",
    "Arabic": "نموذج شقيق",
    "Chinese": "兄弟模型",
    "French": "modèle frère ou sœur",
    "Japanese": "兄弟モデル",
    "Russian": "родственная модель"
  },
  {
    "English": "sigmoid",
    "context": "1: But ABY3 cannot avoid some expensive operations such as evaluation of a Ripple Carry Adder (RCA) in its truncation and activation functions. Truncation and activation functions -ReLU and <mark>Sigmoid</mark>, need rounds proportional to the underlying ring size in ABY3.<br>2: For the conditional relevance modelR Reg (d |x) used by FairCo and D-ULTR, we use a one hidden-layer neural network that consists of D = 50 input nodes fully connected to 64 nodes in the hidden layer with ReLU activation, which is connected to 100 output nodes with <mark>Sigmoid</mark> to output the predicted probability of relevance of each movie.<br>",
    "Arabic": "سيغمويد",
    "Chinese": "Sigmoid函数",
    "French": "sigmoïde",
    "Japanese": "シグモイド関数",
    "Russian": "сигмоидная функция"
  },
  {
    "English": "sigmoid activation",
    "context": "1: , v 2 ∈ R and <mark>sigmoid activation</mark> , σ ( x ) = 1/ ( 1 + e −x ) .<br>2: Hochreiter and Schmidhuber (1997) mitigated this problem by replacing the <mark>sigmoid activation</mark> in the RNN recurrent connection with a self-recurrent memory block and a set of multiplication gates to mimic the read, write, and reset operations in digital computers. The resulting architecture is dubbed the Long Short-term Memory (LSTM) network.<br>",
    "Arabic": "التنشيط السيغموي",
    "Chinese": "sigmoid激活函数",
    "French": "fonction d'activation sigmoïde",
    "Japanese": "シグモイド活性化関数",
    "Russian": "сигмоидальная активация"
  },
  {
    "English": "sigmoid activation function",
    "context": "1: As the label values of tasks in Taskonomy are normalized to [0, 1], we use a <mark>sigmoid activation function</mark> at the head of the decoder to produce values in [0, 1].<br>2: This network contains three linear hidden layers with 128, 64 and 32 dimensions. After each layer, we use the <mark>sigmoid activation function</mark> and dropout after each sigmoid with a dropout value of 0.2. We use 0.1 as the learning rate and use the Mean Squared Error (MSE) loss function.<br>",
    "Arabic": "دالة التنشيط السيجمويدية",
    "Chinese": "sigmoid激活函数",
    "French": "fonction d'activation sigmoïde",
    "Japanese": "シグモイド活性化関数",
    "Russian": "сигмоидная функция активации"
  },
  {
    "English": "sigmoid function",
    "context": "1: We then normalize the ratings to [0, 1] by apply a <mark>Sigmoid function</mark> centered at rating b = 3 with slope a = 10. These serve as relevance probabilities where higher star ratings correspond to a higher likelihood of positive feedback.<br>2: is the <mark>Sigmoid function</mark> . We finally remove the pairs whoseŷ (c e i ,c c j ) is 0 from P all , and get the final set of emotion-cause pairs.<br>",
    "Arabic": "دالة السجمويد",
    "Chinese": "Sigmoid函数",
    "French": "fonction sigmoïde",
    "Japanese": "シグモイド関数",
    "Russian": "сигмоидная функция"
  },
  {
    "English": "signal-to-noise ratio",
    "context": "1: The Institutional Review Board at the University of Iowa approved the acquisition of the data, and written consents were obtained from the subjects. The number of slices acquired for different subjects varies. We used an algorithm developed in house to pre-select the coils that provide the best <mark>signal-to-noise ratio</mark> in the region of interest.<br>2: For each dataset, we add white Gaussian noise to examples in the validation set and test set with a specific <mark>signal-to-noise ratio</mark> (SNR). The classification results in experiments are averaged over 10 independent trials.<br>",
    "Arabic": "نسبة الإشارة إلى الضوضاء",
    "Chinese": "信噪比",
    "French": "rapport signal sur bruit",
    "Japanese": "信号対雑音比",
    "Russian": "отношение сигнала к шуму"
  },
  {
    "English": "sim",
    "context": "1: If <mark>sim</mark>(c(α), c(β)) > <mark>sim</mark>(c(α), c(δ)), we say that p β is semantically more <mark>sim</mark>ilar to pα than p δ w.r.<br>",
    "Arabic": "تَشْبِيه",
    "Chinese": "相似度",
    "French": "sim",
    "Japanese": "類似度",
    "Russian": "sim"
  },
  {
    "English": "similarity function",
    "context": "1: We define a symmetric <mark>similarity function</mark> K(u i , u j ) over two for- eign language vertices u i , u j ∈ V f based on the co-occurrence statistics of the nine feature concepts given in Table 1.<br>2: For any random sample (x, a, y), fairness of its prediction is always taken in consideration, but as the affinity of y to y adv decreases, it will be down-weighted when estimating fairness violations with respect to y adv . Definition 1 (s γ -SimFair). Given a <mark>similarity function</mark> s : \n Y × Y → [ 0 , 1 ] , a multi-label classifier h satisfies Similarity s-induced Fairness ( s γ -SimFair ) if for ∀ k ∈ A , E [ ỹs ( y , y adv ) ] E [ s ( y , y adv ) ] = E [ ỹ1 ( a = k ) s ( y , y<br>",
    "Arabic": "دالة التشابه",
    "Chinese": "相似性函数",
    "French": "fonction de similarité",
    "Japanese": "類似度関数",
    "Russian": "функция сходства"
  },
  {
    "English": "similarity graph",
    "context": "1: In this case, first a neighborhood graph is constructed using the similarities between the points and then a graph clustering algorithm is applied to this graph. In this paper we investigate the influence of the construction of the <mark>similarity graph</mark> on the clustering results.<br>",
    "Arabic": "\"شَبَكَة التَّشَابُه\"",
    "Chinese": "相似性图",
    "French": "graphe de similarité",
    "Japanese": "類似度グラフ",
    "Russian": "граф сходства"
  },
  {
    "English": "similarity matrix",
    "context": "1: This non-monotone function was used to find the most diverse yet relevant subset of objects in a large corpus. We use the objective with both synthetic and real data. We generate 10 instances of random <mark>similarity matrices</mark> {s ij } ij and vary λ from 0.5 to 1.<br>",
    "Arabic": "مصفوفة التشابه",
    "Chinese": "相似性矩阵",
    "French": "matrice de similarité",
    "Japanese": "類似性行列",
    "Russian": "матрица подобия"
  },
  {
    "English": "similarity measure",
    "context": "1: In other words, the <mark>similarity measure</mark> sim(T t i , T t i + t ) of Eq. (5) should equal 1 (corresponding to cos(0), i.e., an angle of 0 • between the two vectors).<br>2: Tα , sim ( t , c ( α ) ) ≤ sim ( t , c ( α ) ) . Definition 7 ( Semantically Similar Pattern ( SSP ) Extraction ) : Given a dataset D and a set of candidate patterns Pc , the problem of Semantically Similar Pattern ( SSP ) Extraction is to define a <mark>similarity measure</mark> sim ( c ( • ) , c ( • ) ) between the contexts of two patterns , and to extract a set<br>",
    "Arabic": "مقياس التشابه",
    "Chinese": "相似度量",
    "French": "mesure de similarité",
    "Japanese": "類似度尺度",
    "Russian": "мера сходства"
  },
  {
    "English": "similarity metric",
    "context": "1: We believe it is important to carry out multiple analyses using diverse methodology: any single experiment may be misleading as it depends on analytical choices such as the type of supervised model used for decoding, the algorithm used for clustering, or the <mark>similarity metric</mark> for representational similarity analysis.<br>2: It is easy to argue then that h 4 is closer to r than h 3 , but current metrics are unable to distinguish between them. We propose a new <mark>similarity metric</mark> based on segment alignment, which scores segmentations based on how well their segments match, rather than their boundaries (Section 3).<br>",
    "Arabic": "مقياس التشابه",
    "Chinese": "相似性度量",
    "French": "métrique de similarité",
    "Japanese": "類似度尺度",
    "Russian": "метрика сходства"
  },
  {
    "English": "similarity score",
    "context": "1: We formulate place recognition as a two-step matching problem, including the semantic graph matching with graphs G and G ′ , as well as the DF matching with F and F ′ . The objective is to compute a <mark>similarity score</mark> to determine whether these observations are recorded at the same place.<br>2: The intersect ratio closeness function in A uses segment size to distinguish \"soft\" and \"hard\" transpositions; as we saw in Figure 12, when the segments are longer, A will match the left and right segments of a twounit transposition with the original segments in r, resulting in a <mark>similarity score</mark> greater than h 0 .<br>",
    "Arabic": "درجة التشابه",
    "Chinese": "相似度得分",
    "French": "score de similarité",
    "Japanese": "類似性スコア",
    "Russian": "оценка сходства"
  },
  {
    "English": "similarity search",
    "context": "1: In a typical task of <mark>similarity search</mark>, we are given a query q, and our aim is to find x ∈ S with high value of Sim(q, x). LSH provides a clean mechanism of creating hash tables [2].<br>2: With the rapidly increasing amount of graph data, the <mark>similarity search</mark> problem, which identifies similar vertices in a graph, has become an important problem with many applications, including web analysis [Jeh and Widom 2002;Liben-Nowell and Kleinberg 2007], graph clustering [Yin et al. 2006;Zhou et al.<br>",
    "Arabic": "البحث عن التشابه",
    "Chinese": "相似性搜索",
    "French": "recherche de similarité",
    "Japanese": "類似検索",
    "Russian": "поиск сходства"
  },
  {
    "English": "simplicial complex",
    "context": "1: As a final example, we consider the recently introduced Message Passing Simplicial Networks (MPSNs) (Bodnar et al., 2021). In a nutshell, MPSNs are run on <mark>simplicial complexes</mark> of graphs instead of on the original graphs.<br>",
    "Arabic": "المجمع البسيطي",
    "Chinese": "单纯复形",
    "French": "complexe simplicial",
    "Japanese": "単体複体",
    "Russian": "симплициальный комплекс"
  },
  {
    "English": "simulated annealing",
    "context": "1: The energy minimization problem has traditionally been solved via <mark>simulated annealing</mark> [2,11], which is extremely slow in practice. In the last few years powerful energy minimization algorithms have been developed based on graph cuts [6,7,12,14,21]. These methods are fast enough to be practical, and yield quite promising experimental results for stereo [22,27].<br>2: The optimization of the energy defined above could be directly attempted using a global optimization strategy such as <mark>simulated annealing</mark>. However, both the prior and the data term E photo are expensive to evaluate, with multiple local minima at each pixel, meaning that attaining a global optimum will be difficult, and certainly time consuming.<br>",
    "Arabic": "التبريد المحاكي",
    "Chinese": "模拟退火",
    "French": "recuit simulé",
    "Japanese": "シミュレーテッドアニーリング",
    "Russian": "имитация отжига"
  },
  {
    "English": "single task learning",
    "context": "1: Multi-task Learning (MTL) aims to learn multiple tasks simultaneously by using a shared model. Unlike <mark>single task learning</mark>, MTL can achieve better generalization performance through exploiting task relationships [Caruana, 1997, Chapelle et al., 2010. One key problem in MTL is how to share model parameters between tasks [Ruder, 2017].<br>2: In such applications, multi-task learning, i.e., learning tasks simultaneously, may be a more appropriate choice than <mark>single task learning</mark> [Chapelle et al., 2010, Kumar and Daumé, 2012, Han and Zhang, 2015, Crawshaw, 2020.<br>",
    "Arabic": "التعلم بمهمة واحدة",
    "Chinese": "单任务学习",
    "French": "apprentissage à tâche unique",
    "Japanese": "単一タスク学習",
    "Russian": "обучение одной задаче"
  },
  {
    "English": "single-label classification",
    "context": "1: Under these settings, L = 1 corresponds to <mark>single-label classification</mark>, and L > 1 corresponds to multi-label classification.<br>2: Multi-label classification is an important yet challenging task in natural language processing. It is more complex than <mark>single-label classification</mark> in that the labels tend to be correlated. Existing methods tend to ignore the correlations between labels. Besides, different parts of the text can contribute differently to predicting different labels, which is not considered by existing models.<br>",
    "Arabic": "تصنيف بتسمية واحدة",
    "Chinese": "单标签分类",
    "French": "classification mono-étiquette",
    "Japanese": "単一ラベル分類",
    "Russian": "классификация по одной метке"
  },
  {
    "English": "single-view data",
    "context": "1: Our goal is to learn a robust target classifier by using the loosely labeled <mark>single-view data</mark> from the heterogeneous source domains and the unlabeled multi-view data from the target domain. In this work, we assume that we have S heterogeneous source domains and focus on the binary classification problem. For each class , we are given a set of labeled <mark>single-view data</mark> { ( x s i , y s i ) | ns i=1 } from the s-th source domain , where n s is the total number of samples from the s-th source domain and each sample x s is drawn from a fixed but unknown data distribution P s ,<br>2: For every <mark>single-view data</mark> (X, y) ∈ Z s (or any (X, y) ∈ D s but with probability 1 − e −Ω(log 2 k) ), we have \n F (t) y (X) ≥ max i∈[k]\\{y} F (t) \n<br>",
    "Arabic": "بيانات منظور واحد",
    "Chinese": "单视图数据",
    "French": "données à vue unique",
    "Japanese": "単一視点データ",
    "Russian": "одновидовые данные"
  },
  {
    "English": "singleton",
    "context": "1: Although the CoNLL shared task evaluated systems on only multi-mention (i.e., non-<mark>singleton</mark>) entities, by stopping <mark>singleton</mark>s from being linked to multi-mention entities, we expected the lifespan model to increase the system's precision.<br>2: The model successfully learns to tease <mark>singleton</mark>s and coreferent mentions apart. Cai and Strube (2010). Scores are on automatically predicted mentions.<br>",
    "Arabic": "مفردة",
    "Chinese": "单个实体",
    "French": "singulier",
    "Japanese": "シングルトン",
    "Russian": "одиночка"
  },
  {
    "English": "singular value",
    "context": "1: They then proceed to shift the columns slightly in the direction orthogonal to that subspace so that the pk`1qst <mark>singular value</mark> of A becomes non-zero. This results in an instance of the CSSP with a sharp drop in the spectrum. Due to the symmetry in this construction, all subsets of size k have an identical squared projection error.<br>2: Recall that-when differentiating SVDs-the derivative of the j-th <mark>singular value</mark>, ω j , only depends on the j-th singular subspace (see Equation 17of Townsend (2016)). As a consequence, we will see in the following lemma that, for any differential dX, the corresponding dω j only depends on dx j .<br>",
    "Arabic": "القيمة المفردة",
    "Chinese": "奇异值",
    "French": "valeur singulière",
    "Japanese": "特異値",
    "Russian": "сингулярное значение"
  },
  {
    "English": "singular value decomposition",
    "context": "1: We set the number of graph neural layers as 2 with a hidden dimension of 100. To study the transferability across different graph data, we use SVD (<mark>Singular Value Decomposition</mark>) to reduce the initial features to 100 dimensions. The token number of our prompt graph is set as 10.<br>2: For any rank r d × d matrix X, suppose its <mark>Singular Value Decomposition</mark> is U DV , we can findÛ ∈ P 1 ,D ∈ P 2 andV ∈ P 3 that are ε close to U, D, V respectively. ThereforeÛDV ∈ Γ and it is easy to check U DV −ÛDV F 8ε ζ.<br>",
    "Arabic": "تفكيك قيمة مفردة",
    "Chinese": "奇异值分解",
    "French": "décomposition en valeurs singulières",
    "Japanese": "特異値分解",
    "Russian": "сингулярное разложение"
  },
  {
    "English": "singular vector",
    "context": "1: Good performance demands that the singular values be large. Works such as Fukunaga (1972, Chapter 10) and Hastie & Tibshirani (1996) show that the magnitude of each singular value of the SNR matrix is the size of the class separation in the direction of the corresponding <mark>singular vector</mark>.<br>2: However, those coresets and sketches usually yield (1 + ε)-multiplicative approximations for Ax 2 2 by Sx 2 2 where the matrix S is of (d/ε) O(1) rows and x may be any vector, or the smallest/largest <mark>singular vector</mark> of S or A; see lower bounds in Feldman et al.<br>",
    "Arabic": "المتجه الفردي",
    "Chinese": "奇异向量",
    "French": "vecteur singulier",
    "Japanese": "特異ベクトル",
    "Russian": "сингулярный вектор"
  },
  {
    "English": "skip connection",
    "context": "1: Moreover, within the self-attention mechanism, most (76%) of the gradient in the last layer for the subject flows through the keys matrix. In contrast, across all tokens, the top gradient paths mostly through the <mark>skip connection</mark> for all layers, and otherwise is more evenly distributed between keys and values.<br>2: The attention layer is followed by standard layers of normalization, <mark>skip connection</mark>, and feedforward network (FFN). Regarding the object-level branch on the right side of Figure 8, a multi-head attention layer is employed to aggregate the sampled point features.<br>",
    "Arabic": "اتصال تخطي",
    "Chinese": "跳跃连接",
    "French": "connexion directe",
    "Japanese": "スキップ接続",
    "Russian": "пропускное соединение"
  },
  {
    "English": "skip-gram",
    "context": "1: So as to make our evaluation more robust, we run the above experiments for three popular embedding methods, using large pre-trained models released by their respective authors as follows: Word2vec (Mikolov et al., 2013) is the original implementation of the CBOW and <mark>skip-gram</mark> architectures that popularized neural word embeddings.<br>2: At the same time, Levy and Goldberg (2014a) propose a modification of <mark>skip-gram</mark> that uses a dependency-based context instead of a sliding windows, which produces embeddings that are more tailored towards genuine similarity than relatedness. Bansal et al. (2014) follow a similar approach to train specialized embeddings that are used as features for dependency parsing.<br>",
    "Arabic": "تخطي جرام",
    "Chinese": "跳元模型",
    "French": "skip-gram",
    "Japanese": "スキップグラム",
    "Russian": "пропуск-грамма"
  },
  {
    "English": "skip-gram model",
    "context": "1: • UmBERTo. We use RoBERTa trained on Italian data -UmBERTo 9 . The s embedding of the last layer of the architecture is the query embedding; \n • Search2Vec. We implement the <mark>skip-gram model</mark> from Grbovic et al. (2016), by feeding the model with sessions composed of search queries and user clicks.<br>2: Fasttext (Bojanowski et al., 2017) is an extension of the <mark>skip-gram model</mark> implemented by word2vec that enriches the embeddings with subword information using bags of character n-grams.<br>",
    "Arabic": "نموذج skip-gram",
    "Chinese": "跳语法模型",
    "French": "modèle skip-gram",
    "Japanese": "スキップグラムモデル",
    "Russian": "модель skip-грам"
  },
  {
    "English": "slack variable",
    "context": "1: ) is defined as the union of two closed balls that are centered by r x (i) and r y (i) , respectively. The radius of both balls is \n d =∥ r x (i) − r y (i) ∥ 2 \n , which is also considered as a <mark>slack variable</mark> for determining semantic equivalence.<br>2: Notably, our method does more than augmenting the set of training examples; it enforces the monotonicity of the detector function, as shown in Fig. 4. For a better understanding of Constraint ( 8 ) , let us analyze the constraint without the <mark>slack variable</mark> term and break it into three cases : i ) t < s i ( event has not started ) ; ii ) t ≥ s i , y = ∅ ( event has started ; compare the partial event against the detection threshold )<br>",
    "Arabic": "متغير فارغ",
    "Chinese": "松弛变量",
    "French": "variable d'écart",
    "Japanese": "スラック変数",
    "Russian": "слак-переменная"
  },
  {
    "English": "slide window",
    "context": "1: The models were trained in the same way as described in Section 5.1, but evaluated with no gap and a one-minute sliding target window.<br>2: After identifying the features present in each region (i.e., <mark>sliding window</mark>), a max pooling layer considers non-overlapping regions of length n and keeps the highest feature value for each region (c).<br>",
    "Arabic": "نافذة منزلقة",
    "Chinese": "滑动窗口",
    "French": "fenêtre glissante",
    "Japanese": "スライディングウィンドウ",
    "Russian": "скользящее окно"
  },
  {
    "English": "slide window classifier",
    "context": "1: Traditionally, object detection is reduced to binary classification and a <mark>sliding window classifier</mark> is applied at all positions, scales and orientations of an image [20,4,8,19,22].<br>",
    "Arabic": "مصنف نافذة الانزلاق",
    "Chinese": "滑动窗口分类器",
    "French": "classificateur de fenêtre coulissante",
    "Japanese": "スライディングウィンドウ分類器",
    "Russian": "скользящий оконный классификатор"
  },
  {
    "English": "slot",
    "context": "1: Assume that there are J possible (domain, <mark>slot</mark>) pairs, and Y value j is the true word sequence for j-th (domain ,<mark>slot</mark>) pair.<br>2: Slot values are sampled from all values seen for the <mark>slot</mark> name and intent.<br>",
    "Arabic": "الفتحة",
    "Chinese": "槽",
    "French": "emplacement",
    "Japanese": "スロット",
    "Russian": "слот"
  },
  {
    "English": "slot filling",
    "context": "1: The goal of the dialogue system in this context is thus to extract the right values from the user sentence to fill the slots. The recent progress in a task-oriented dialogue system allows one to effectively exploit the dialogue historical information to optimize <mark>slot filling</mark> with the right order [11][12][13].<br>2: Traditionally, template extraction comprises two sub-tasks: template identification, in which a system identifies and types all templates in a document, and <mark>slot filling</mark> or role-filler entity extraction (REE), in which the slots associated with each template are filled with extracted entities.<br>",
    "Arabic": "ملء الفراغات",
    "Chinese": "槽填充",
    "French": "remplissage de slot",
    "Japanese": "スロット充填",
    "Russian": "заполнение слотов"
  },
  {
    "English": "slot value",
    "context": "1: The state generator will decode J times independently for all the possible (domain, slot) pairs. At the first decoding step, state generator will take the j-th (domain, slot) embeddings as input to generate its corresponding <mark>slot values</mark> and slot gate.<br>2: On the other hand, Nouri and Hosseini-Asl (2018) utilizes global modules to share parameters between slots, and Zhong et al. (2018) uses slot-specific local modules to learn slot features, which has proved to successfully improve tracking of rare <mark>slot values</mark>. Lei et al.<br>",
    "Arabic": "قيمة الفتحة",
    "Chinese": "槽值",
    "French": "valeur de l'emplacement",
    "Japanese": "スロット値",
    "Russian": "значение слота"
  },
  {
    "English": "slot-value pair",
    "context": "1: However, these heuristics can only handle cases where <mark>slot-value pairs</mark> can be identified by exact matching between the delexicalised surface text and the slot value pair encoded in d. Cases such as binary slots and slots that take don't care values cannot be explicitly delexicalised in this way and these cases frequently result in generation errors.<br>",
    "Arabic": "زوج قيمة الفتحة",
    "Chinese": "槽值对",
    "French": "paire slot-valeur",
    "Japanese": "スロット値ペア",
    "Russian": "пара слот-значение"
  },
  {
    "English": "smoothing parameter",
    "context": "1: where q i is prior probability of the class i = 1, 2, …, K; j = 1, 2, …, Ni, K -number of classes, Ni -number of training vectors in each class, and h is a <mark>smoothing parameter</mark> (we used value 1.0 due to data normalization).<br>2: µ j i = |U i | k=1 (1 {y k =j} (t k ) + σ)/(|U i | + σ|Y|) \n where σ ∈ R is the <mark>smoothing parameter</mark>, j ∈ Y, i.e. the set of polarity labels.<br>",
    "Arabic": "معامل التنعيم",
    "Chinese": "平滑参数",
    "French": "paramètre de lissage",
    "Japanese": "平滑化パラメータ",
    "Russian": "параметр сглаживания"
  },
  {
    "English": "smoothness term",
    "context": "1: (We used color images; results for grayscale images are slightly worse). The constant K for the data term was chosen to be 30. We used a simple Potts model for the <mark>smoothness term</mark>: \n<br>2: In the main paper we presented an optimization problem where we solve for a per-pixel quantity x subject to a <mark>smoothness term</mark> that encourages x to be smooth and a data term that encourages x to resemble some observed input \"target\" quantities t proportionally to a per-pixel \"confidence\" c: \n<br>",
    "Arabic": "مصطلح النعومة",
    "Chinese": "平滑项",
    "French": "terme de régularité",
    "Japanese": "滑らかさ項",
    "Russian": "термин гладкости"
  },
  {
    "English": "social bias",
    "context": "1: Social bias measurements are very sensitive to evaluation methodology. Our empirical evidence sheds light on how the model's non-<mark>social bias</mark>es brought out or masked by alternate constructions can cause bias benchmarks to underestimate or overestimate the <mark>social bias</mark> in a model. More interestingly, it is important to note that different models respond differently to perturbations.<br>2: How reliably can we trust the scores obtained from <mark>social bias</mark> benchmarks as faithful indicators of problematic <mark>social bias</mark>es in a given model? In this work, we study this question by contrasting <mark>social bias</mark>es with non-<mark>social bias</mark>es that stem from choices made during dataset construction (which might not even be discernible to the human eye).<br>",
    "Arabic": "الانحياز الاجتماعي",
    "Chinese": "社会偏见",
    "French": "biais social",
    "Japanese": "社会的偏見",
    "Russian": "социальная предвзятость"
  },
  {
    "English": "social network analysis",
    "context": "1: Then the random walk with restart algorithm was revised to calculate the relevance scores among researchers in the graph to group the highly-relevant researchers into the same community. Mei et al. [10] proposed the NetPLSA model which combined the statistical topic modeling and <mark>social network analysis</mark> to discover topical communities.<br>2: are altered . This model has been used quite extensively both in <mark>social network analysis</mark> and computer science for understanding the sensitivity to graph properties, e.g., (Costenbader and Valente 2003;Borgatti, Carley, and Krackhardt 2006;Flaxman and Frieze 2004;Flaxman 2007).<br>",
    "Arabic": "تحليل الشبكات الاجتماعية",
    "Chinese": "社会网络分析",
    "French": "analyse des réseaux sociaux",
    "Japanese": "ソーシャルネットワーク分析",
    "Russian": "анализ социальных сетей"
  },
  {
    "English": "soft margin",
    "context": "1: In particular, the radial basis function (RBF) or proximity kernel parametrized by an influence parameter, γ, has been considered for the experiments. This parameter along with the <mark>soft margin</mark> or regularization parameter C have been chosen by looking at values similar to the ones in previous related works.<br>",
    "Arabic": "هامش ناعم",
    "Chinese": "软间隔",
    "French": "marge souple",
    "Japanese": "ソフトマージン",
    "Russian": "мягкий зазор"
  },
  {
    "English": "softmax activation",
    "context": "1: 8 For each x , we pass the representationx output by the Transformer to a linear layer with output size |S |, the total number of slot types. A <mark>softmax activation</mark> is then applied over all slot types that are valid for (i.e., ∈ ∪ { }), with invalid types masked out, yielding the following distribution: \n<br>2: The second layer is used for classification with one attention head and F = 2 (\"Aye\" or \"no\") followed by a <mark>softmax activation</mark> (Nwankpa et al., 2018).<br>",
    "Arabic": "تنشيط سوفتماكس",
    "Chinese": "softmax激活函数",
    "French": "activation softmax",
    "Japanese": "ソフトマックス活性化",
    "Russian": "функция активации softmax"
  },
  {
    "English": "softmax activation function",
    "context": "1: The user representation created by the merge step is then passed to one or more dense layers before being passed to a dense output layer with a <mark>softmax activation function</mark> to perform classification. The number of dense layers used is a hyperparameter described in §5. Categorical cross-entropy is used as the model's loss function.<br>2: Specifically, given a one-hot vector representation of a data example , we feed to G to form G( ) = [G ( )] ∈K . We then apply the <mark>softmax activation function</mark> to G ( ) to define the featurebased local distribution (i.e., Cat( | ))) for as \n<br>",
    "Arabic": "وظيفة تفعيل سوفت ماكس",
    "Chinese": "softmax激活函数",
    "French": "fonction d'activation softmax",
    "Japanese": "ソフトマックス活性化関数",
    "Russian": "функция активации softmax"
  },
  {
    "English": "softmax classifier",
    "context": "1: Specifically, we replace the <mark>softmax classifier</mark> layer (fully connected units with a sigmoid activation function followed by normalization to produce the label probabilities) with our layer -fully connected units followed by our inference to produce marginals for each label (see Fig. 1 for an illustration).<br>2: Since sentence-level RE is a multi-class classification task, sentence-level RE models (Cai et al., 2016;Zeng et al., 2014;Zeng et al., 2015) utilize a <mark>softmax classifier</mark> as the prediction network and use categorical cross entropy as the loss function.<br>",
    "Arabic": "مصنف softmax",
    "Chinese": "softmax分类器",
    "French": "classificateur softmax",
    "Japanese": "ソフトマックス分類器",
    "Russian": "классификатор softmax"
  },
  {
    "English": "softmax distribution",
    "context": "1: These are then concatenated with a learned 32-d embedding of the previous action taken to form a 160-d input that is then given to the LSTM. The output of the LSTM is then processed by a fully-connected layer to produce a <mark>softmax distribution</mark> of the action space and an estimate of the value function. Training Data.<br>",
    "Arabic": "توزيع سوفت ماكس",
    "Chinese": "softmax分布",
    "French": "distribution softmax",
    "Japanese": "ソフトマックス分布",
    "Russian": "распределение softmax"
  },
  {
    "English": "softmax function",
    "context": "1: Since POS is a strong predictor of predicates 4 and the complexity of training a multi-task model increases with the number of tasks, we combine POS tagging and predicate detection into a joint label space: For each POS tag TAG which is observed co-occurring with a predicate, we add a label of the form TAG:PREDICATE. Specifically , we feed the representation s ( r ) t from a layer r preceding the syntacticallyinformed layer p to a linear classifier to produce per-class scores r t for token t. We compute locally-normalized probabilities using the <mark>softmax function</mark> : P ( y prp t | X ) / exp ( r t ) , where y prp t is a label<br>2: . This probability function is realized by a standard multi-layer neural network. A <mark>softmax function</mark> (Equation 4) is used at the output of the neural net to make sure probabilities sum to 1.<br>",
    "Arabic": "دالة التليين",
    "Chinese": "softmax函数",
    "French": "fonction softmax",
    "Japanese": "ソフトマックス関数",
    "Russian": "функция softmax"
  },
  {
    "English": "softmax layer",
    "context": "1: As discussed in Section 3.3, the balance between uncertainty and discrimination enables locating important correspondences in an attention-like manner. This inspires us to take elements from attention-related work, i.e., the <mark>Softmax layer</mark> and the deformable sampling [54].<br>2: We used the MatConvNet library [37] and their reference implementation of LeNet-5 [21], for building an end-to-end digit classification system on MNIST training data, replacing the conventionally used <mark>Softmax layer</mark> by our forest.<br>",
    "Arabic": "طبقة سوفت ماكس",
    "Chinese": "Softmax 层",
    "French": "couche softmax",
    "Japanese": "ソフトマックス層",
    "Russian": "слой софтмакс"
  },
  {
    "English": "softmax loss",
    "context": "1: Entities in a given candidate set are scored as w h m,e where w is a learned parameter vector, and the model is trained using a <mark>softmax loss</mark>. An architecture with 12 layers, hidden dimension size 768 and 12 attention heads was used in our experiments. We refer to this model as Full-Transformer.<br>",
    "Arabic": "خسارة سوفت ماكس",
    "Chinese": "softmax损失",
    "French": "perte softmax",
    "Japanese": "ソフトマックス損失",
    "Russian": "функция потерь softmax"
  },
  {
    "English": "softplus",
    "context": "1: d n = sof tplus([x n ; c n ] • w f ). (1) \n We concatenate the flattened C n represented by c n with x n . Then, we use the <mark>softplus</mark> activation function to compute the annotation complexity of the n-th instance.<br>2: In the original NeRF paper, the activation functions used by the MLP to construct the predicted density τ and color c are a ReLU and a sigmoid, respectively. Instead of a ReLU as the activation function to produce τ , we use a shifted <mark>softplus</mark>: log(1 + exp(x − 1)).<br>",
    "Arabic": "وظيفة softplus",
    "Chinese": "softplus - 软正函数",
    "French": "softplus",
    "Japanese": "ソフトプラス関数",
    "Russian": "софтплюс"
  },
  {
    "English": "softplus activation",
    "context": "1: Thus, the pose encoding network generates a feature vector p ∈ R 126 . We implement the distance field network f df as a 5-layer MLP. For training, we use the <mark>softplus activation</mark> in the hidden layer and train the network end-to-end using the loss functions described in Eq. (4).<br>",
    "Arabic": "التنشيط الناعم",
    "Chinese": "软加激活",
    "French": "activation softplus",
    "Japanese": "ソフトプラス活性化関数",
    "Russian": "активация софтплюс"
  },
  {
    "English": "softplus function",
    "context": "1: Thus, we use a hyperbolic tangent function and a <mark>softplus function</mark> (Dugas et al., 2001) as the output activation functions of µ-Net and σ-Net, respectively.<br>",
    "Arabic": "وظيفة السوفت بلس",
    "Chinese": "柔和正函数 (softplus function)",
    "French": "fonction softplus",
    "Japanese": "ソフトプラス関数",
    "Russian": "функция софтплюс"
  },
  {
    "English": "solution space",
    "context": "1: Implementation details. The current implementation of MLCopilot involves maintaining dedicated experience and knowledge pools for each <mark>solution space</mark>. The historical data is sourced from the benchmarks described below, while task descriptions are crawled from benchmark websites.<br>2: x 1,1 = (X i + t j − le 2 ) (1∶2) . Here the problem space is P = R 2 12 the <mark>solution space</mark> is S = R 12 , π(x, λ) = x, and our problem-solution manifold \n<br>",
    "Arabic": "فضاء الحلول",
    "Chinese": "解空间",
    "French": "espace de solutions",
    "Japanese": "解空間",
    "Russian": "пространство решений"
  },
  {
    "English": "solver",
    "context": "1: Starting from this description the lazy implementation build a procedure that automatically understands if a candidate model I satisfies constraints in C. If some of these constraints are not satisfied then they will be lazily instantiated in the <mark>solver</mark> and the model computations starts again, otherwise the process stops returning the model I.<br>2: The eager implementation is based on a different approach. Indeed, in this case the propagator procedure is completely involved in the model computation process. Every time that the <mark>solver</mark> assign a truth value to a literal , either true or false , the propagator is notified and it simulates the instantiation of the constraints in C without storing it in memory , and if it is possible it makes some inferences on the truth values of the literals that have not been assigned yet , in order to<br>",
    "Arabic": "محلل الحلول",
    "Chinese": "解算器",
    "French": "solveur",
    "Japanese": "ソルバー",
    "Russian": "солвер"
  },
  {
    "English": "source domain",
    "context": "1: where T t ∈ R D is the feature representation of the target sample t and S c ∈ R D is the mean of all samples in the <mark>source domain</mark> labelled by class c. To increase the robustness of the assignment, we do not enforce that all target samples are assigned to a class as shown in Fig.<br>2: Given a model q θ (y|x) trained on the <mark>source domain</mark> and to be tested on multiple target datasets D µ1 , D µ2 , ..., D µT , where \n D µi = {(x T i )} Nµ i=1 \n<br>",
    "Arabic": "مجال المصدر",
    "Chinese": "源域",
    "French": "domaine source",
    "Japanese": "ソースドメイン",
    "Russian": "домен источника"
  },
  {
    "English": "source model",
    "context": "1: We intent to use model transformation, which is a process that generates a refined model from a <mark>source model</mark> [8]. This process is based on a transformation definition, which is a set of transformation rules that describe how one or more constructs in the source language can be transformed into one or more constructs in the target language.<br>2: We conclude the reason as prompts aim to exploit the knowledge of the <mark>source model</mark> and adapt to new target domains. Since VOC has richer data and more balanced classes, the <mark>source model</mark> trained on it can reflect richer and more reliable knowledge, which lays a better foundation for prompt mechanisms.<br>",
    "Arabic": "نموذج المصدر",
    "Chinese": "源模型",
    "French": "modèle source",
    "Japanese": "元モデル",
    "Russian": "исходная модель"
  },
  {
    "English": "source node",
    "context": "1: In other words, feature diversity is defined as the total number of preserved feature dimensions from distinct <mark>source nodes</mark>; topology diversity is defined as the total number of directed edges propagating at least one dimension message.<br>",
    "Arabic": "عقدة المصدر",
    "Chinese": "源节点",
    "French": "nœud source",
    "Japanese": "入力ノード",
    "Russian": "исходный узел"
  },
  {
    "English": "source sequence",
    "context": "1: , s J ), s j ∈ V, the goal is to predict a sequence of target tokens t ′ 1...I ′ = (t ′ 1 , . . . , t ′ I ′ ), t ′ i ∈ V, that is a translation of the <mark>source sequence</mark>, where V is the vocabulary.<br>2: We also add language ID tokens to our vocabulary, which are prepended to each source and target sequence to indicate the target language (Johnson et al., 2017).<br>",
    "Arabic": "التسلسل المصدري",
    "Chinese": "源序列",
    "French": "séquence source",
    "Japanese": "入力シーケンス",
    "Russian": "исходная последовательность"
  },
  {
    "English": "source token",
    "context": "1: The attention mask is set such that each target token can only attend to all <mark>source tokens</mark> and preceding target tokens.<br>",
    "Arabic": "كلمة المصدر",
    "Chinese": "源词元",
    "French": "jeton source",
    "Japanese": "ソーストークン",
    "Russian": "исходный токен"
  },
  {
    "English": "source word",
    "context": "1: As our model allows only one-to-one mappings between the words in the source and target sentences, we remove s i -t j from the sequence if either the <mark>source word</mark> s i or target word t j is already in a previous entry of the combined alignment sequence. The resulting alignment is our initial alignment for the inference.<br>",
    "Arabic": "كلمة المصدر",
    "Chinese": "源词",
    "French": "mot source",
    "Japanese": "ソース語",
    "Russian": "исходное слово"
  },
  {
    "English": "space carving",
    "context": "1: One major limitation of voxel coloring and <mark>space carving</mark> is that they lack a way of imposing spatial coherence. This is particularly problematic because the image data is almost always ambiguous. Another (related) limitation comes from the fact that these methods traverse the volume making \"hard\" decisions concerning the occupancy of each voxel they analyze.<br>2: As we will see in section 3, our approach handles all the camera configurations where voxel coloring can be used. Space carving is another voxel-oriented approach that uses the photo-consistency constraint to prune away empty voxels from the volume. Space carving has the advantage of allowing arbitrary camera geometry.<br>",
    "Arabic": "نحت الفضاء",
    "Chinese": "空间雕刻",
    "French": "sculpture d'espace",
    "Japanese": "スペースカービング",
    "Russian": "вырезание пространства"
  },
  {
    "English": "space complexity",
    "context": "1: To the best of our knowledge, this is the first time that such an algorithm is successfully scaled up to such large networks. -The <mark>space complexity</mark> is proportional to the number of edges, which enables us to compute SimRank values for large networks.<br>2: There are two dimensions to the difficulty [Allis et al., 1991]: decision complexity, the difficulty required to make correct decisions, and <mark>space complexity</mark>, the size of the search space. Checkers is considered to have high decision complexity and moderate <mark>space complexity</mark>.<br>",
    "Arabic": "تعقيد المساحة",
    "Chinese": "空间复杂度",
    "French": "Complexité spatiale",
    "Japanese": "空間複雑度",
    "Russian": "пространственная сложность"
  },
  {
    "English": "space partitioning",
    "context": "1: Such tests can require <mark>space partitioning</mark> [18,3], which works poorly in high dimensions; or closed-form integrals under the model, which may be difficult to obtain, besides in certain special cases [2,5,30,26]. An alternative is to conduct a two-sample test using samples drawn from both p and q.<br>",
    "Arabic": "تجزئة الفضاء",
    "Chinese": "空间划分",
    "French": "partitionnement de l'espace",
    "Japanese": "空間分割",
    "Russian": "разбиение пространства"
  },
  {
    "English": "spam detection",
    "context": "1: for <mark>spam detection</mark>, hatespeech detection, or targeted news filtering, but also for bad, e.g., for creating models that detect certain topics that are to be censored in authoritarian regimes. While such systems already exist and are of sophisticated quality, small-text is unlikely to change anything at this point.<br>",
    "Arabic": "الكشف عن البريد العشوائي",
    "Chinese": "垃圾邮件检测",
    "French": "détection des pourriels",
    "Japanese": "迷惑メール検出",
    "Russian": "обнаружение спама"
  },
  {
    "English": "spam filtering",
    "context": "1: However, it is hard to apply their method to computer vision, which does not have a well-defined language model yet. Early detection has also been studied in the context of <mark>spam filtering</mark>, where immediate and irreversible decisions must be made whenever an email arrives. Assuming spam messages were similar to one another, Haider et al.<br>",
    "Arabic": "تصفية البريد المزعج",
    "Chinese": "垃圾邮件过滤",
    "French": "Filtrage des spams",
    "Japanese": "スパムフィルタリング",
    "Russian": "фильтрация спама"
  },
  {
    "English": "span",
    "context": "1: ) . In addition, for a matrix X ∈ R n×k , we use <mark>span</mark>(X) to represent the subspace <mark>span</mark>ned by the columns of X.<br>2: The questions are human-annotated, and the answer to each question is a segment (or <mark>span</mark>) of the passage. The goal of models is to identify and extract the correct <mark>span</mark> from the passage that answers the question.<br>",
    "Arabic": "مدى",
    "Chinese": "跨度",
    "French": "espace vectoriel",
    "Japanese": "スパン",
    "Russian": "спан"
  },
  {
    "English": "sparse",
    "context": "1: 2 , • • • , n , and k 2 ∈ { 1 , • • • , d } . Output: A d/k 2 -<mark>Sparse</mark> \n u j (p j ) = u(p) for every p ∈ P .<br>2: These 6 configurations have been tested using the 3 random projections described in previous section (i.e. Gaussian, <mark>Sparse</mark>, and Binary, denoted as G, S and B respectively), resulting into 18 RP based ensembles. These RP based ensembles have been tested against the base method on its own, (i.e.<br>",
    "Arabic": "متناثر",
    "Chinese": "稀疏",
    "French": "épars",
    "Japanese": "スパース",
    "Russian": "разреженный"
  },
  {
    "English": "sparse Transformer",
    "context": "1: We use the same model code as GPT-2, except that we initialize weights in the layerdependent fashion as in <mark>Sparse Transformer</mark> (Child et al., 2019) and zero-initialize all projections producing logits.<br>2: We were unable to run the <mark>Sparse Transformer</mark> [6] model due to issues with their custom CUDA implementation of the sparse attention kernel, which we were unable to resolve. The Transformer baseline from Table 8 was run using a modified GPT-2 backbone from the HuggingFace repository, configured to recreate the architecture reported in [2].<br>",
    "Arabic": "محول متناثر",
    "Chinese": "稀疏变压器",
    "French": "transformer épars",
    "Japanese": "疎Transformer",
    "Russian": "Разреженный трансформер"
  },
  {
    "English": "sparse approximation",
    "context": "1: The lower bound tells us that even if the training data is contained in an interval of fixed length, we need to use more inducing points for problems with large N if we want to ensure the <mark>sparse approximation</mark> has converged.<br>",
    "Arabic": "تقريب متناثر",
    "Chinese": "稀疏近似",
    "French": "approximation parcimonieuse",
    "Japanese": "疎近似",
    "Russian": "Разреженная аппроксимация"
  },
  {
    "English": "sparse attention",
    "context": "1: (with p = 0.9) consisting of a \"contrastive REALM\" dense retriever and a generator based on the Routing Transformer, the current state-of-the-art model for representing long-range dependencies in sequences via <mark>sparse attention</mark> and mini-batch k-means clustering [37].<br>2: For arXiv-Lay and PubMed-Lay, we initialize Big-Bird from Pegasus  and for the non-English datasets, we use the weights of MBART. The resulting models are referred to as BigBird-Pegasus and BigBird-MBART. For both models, BigBird <mark>sparse attention</mark> is used only in the encoder.<br>",
    "Arabic": "انتباه متناثر",
    "Chinese": "稀疏注意力",
    "French": "attention éparse",
    "Japanese": "スパースアテンション",
    "Russian": "разреженное внимание"
  },
  {
    "English": "sparse attention pattern",
    "context": "1: Several promising techniques have recently been developed such as local 2D relative attention (Bello et al., 2019;Ramachandran et al., 2019), <mark>sparse attention patterns</mark> (Child et al., 2019), locality sensitive hashing (Kitaev et al., 2020), and multiscale modeling (Menick & Kalchbrenner, 2018).<br>",
    "Arabic": "نمط الانتباه المتفرق",
    "Chinese": "稀疏注意力模式",
    "French": "motif d'attention éparse",
    "Japanese": "疎な注意パターン",
    "Russian": "разреженный шаблон внимания"
  },
  {
    "English": "sparse graph",
    "context": "1: For <mark>sparse graphs</mark> many algorithms work, for example recently [8,26] reported excellent results on planar, or nearly planar graphs and [27] show that even local search works when the graph is sparse.<br>2: For <mark>sparse graphs</mark> typically encountered in practice (i.e. m = o(n 2 )), one may similarly ask whether a complexity that depends on m can be achieved. We conjecture that it should be possible. Below, we give another algorithm to calculate L + 1 n 11 ⊤ −1 .<br>",
    "Arabic": "رسم بياني متفرق",
    "Chinese": "稀疏图",
    "French": "graphe creux",
    "Japanese": "疎なグラフ (Sparse Graph)",
    "Russian": "разреженный граф"
  },
  {
    "English": "sparse matrix",
    "context": "1: Monarch matrices can serve as a fast intermediate representation to speed up the training process of the dense model. D2S fine-tuning. While transitioning from sparse to dense matrices is easy, the reverse direction is challenging.<br>2: Large neural networks excel in many domains, but their training and fine-tuning demand extensive computation and memory [54]. A natural approach to mitigate this cost is to replace dense weight matrices with structured ones, such as sparse & low-rank matrices and the Fourier transform.<br>",
    "Arabic": "مصفوفة متناثرة",
    "Chinese": "稀疏矩阵",
    "French": "matrice creuse",
    "Japanese": "疎行列",
    "Russian": "разреженная матрица"
  },
  {
    "English": "sparse model",
    "context": "1: Our sparse-to-dense reverse sparsification instead focuses on speeding up dense training, where the <mark>sparse model</mark> is used for efficiency and not regularization. In addition, models proposed in our work can be roughly seen as a class of manually constructed lottery tickets.<br>",
    "Arabic": "نموذج متناثر",
    "Chinese": "稀疏模型",
    "French": "modèle épars",
    "Japanese": "疎モデル",
    "Russian": "разреженная модель"
  },
  {
    "English": "sparse reconstruction",
    "context": "1: 1) <mark>Sparse reconstruction</mark> Given the set of valid objectcentric videos, we reconstruct the extrinsic (3D location and orientation) and intrinsic (calibration) properties of the cameras that captured the videos. To this end, each video is first converted into a time-ordered sequence of images \n<br>",
    "Arabic": "إعادة بناء متفرقة",
    "Chinese": "稀疏重建",
    "French": "reconstruction parcimonieuse",
    "Japanese": "疎再構築",
    "Russian": "разреженная реконструкция"
  },
  {
    "English": "sparse recovery",
    "context": "1: We analyze the statistical efficiency of our framework in the context of <mark>sparse recovery</mark>. In particular, we prove that the sample complexity of recovering vectors in the WGM is provably smaller than the sample complexity for \"standard\" s-sparse vectors. To formally state this result, we first introduce a key property of graphs. Definition 2.<br>2: In the context of <mark>sparse recovery</mark>, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms also improve on prior work in practice.<br>",
    "Arabic": "انتعاش متناثر",
    "Chinese": "稀疏恢复",
    "French": "récupération parcimonieuse",
    "Japanese": "\"疎回復\"",
    "Russian": "разреженное восстановление"
  },
  {
    "English": "sparse representation",
    "context": "1: Even a sort-merge implementation would only yield a complexity of O(mw) where mw is the typical NNZ in w k and mx mw < m. The choice of <mark>sparse representation</mark> for feature vector is thus readily justified by its much higher sparseness than weight vector 6 and the even higher dimensionality of n.<br>2: Because each instruction targets at most two beakers (e.g., pour X into Y) and there are 7 beakers in total, there is a <mark>sparse representation</mark> of cases probing a beaker that actually underwent a change in the dataset.<br>",
    "Arabic": "تمثيل متناثر",
    "Chinese": "稀疏表示",
    "French": "représentation parcimonieuse",
    "Japanese": "疎な表現",
    "Russian": "- Разреженное представление"
  },
  {
    "English": "sparse sampling",
    "context": "1: Several general techniques have been proposed to find approximate solutions to large MDPs efficiently, including state aggregation (Li, Walsh, & Littman 2006), factored MDPs (Guestrin et al. 2003;Hoey et al. 1999), and <mark>sparse sampling</mark> (Kearns, Mansour, & Ng 2002).<br>2: To provide a remedy to this dilemma, we propose a generic framework CLIPBERT that enables affordable endto-end learning for video-and-language tasks, by employing <mark>sparse sampling</mark>, where only a single or a few sparsely sampled short clips from a video are used at each training step.<br>",
    "Arabic": "أخذ عينات متناثرة",
    "Chinese": "稀疏采样",
    "French": "échantillonnage épars",
    "Japanese": "疎サンプリング",
    "Russian": "разреженная выборка"
  },
  {
    "English": "sparse vector",
    "context": "1: This makes d ij a 7 dimensional sparse binary vector different object classes, we append a constant 1 to make x i two-dimensional. Background class. Since we are concerned only with the relative difference in scores between labelings, we have an extra degree of freedom in defining the weights.<br>2: The L 1 constraint promotes sparsity of the new encoding with respect to D. Thus, every sample is now encoded as a <mark>sparse vector</mark> that is of higher dimensionality than the original representation. In some cases the data exhibits a structure that is not captured by the above sparse coding setting.<br>",
    "Arabic": "متجه متناثر",
    "Chinese": "稀疏向量",
    "French": "vecteur creux",
    "Japanese": "疎ベクトル",
    "Russian": "разреженный вектор"
  },
  {
    "English": "sparsification",
    "context": "1: (2019) showed that finetuning top layers of pre-trained models is not effective and that alternate methods allow fine-tuning effectively with a couple of percent of the parameters. Furthermore, we can view computing the intrinsic dimensionality as a continuous relaxation of the <mark>sparsification</mark> problem.<br>2: This allows us to handle significantly larger data sets and larger models than have been reported elsewhere to our knowledge, with billions of coefficients. Because trained models are replicated to many data centers for serving (see Figure 1), we are much more concerned with <mark>sparsification</mark> at serving time rather than during training.<br>",
    "Arabic": "تخفيف الكثافة",
    "Chinese": "稀疏化",
    "French": "éparsification",
    "Japanese": "スパース化",
    "Russian": "разрежение"
  },
  {
    "English": "sparsity",
    "context": "1: We make the notion of \"most\" functions precise by proposing a natural random generative model for functions with a given <mark>sparsity</mark>.<br>2: Fitting sparse models to observed data has been a classical approach used in statistics for model recovery and is inspired by the philosophy of Occam's Razor. Motivated by this, sufficient conditions based on <mark>sparsity</mark> for learnability have been of great interest over years in the context of communication, signal processing and statistics, cf.<br>",
    "Arabic": "نُدرة",
    "Chinese": "稀疏性",
    "French": "parcimonie",
    "Japanese": "疎性",
    "Russian": "разреженность"
  },
  {
    "English": "sparsity level",
    "context": "1: • In Experiment 1, we varied the \"<mark>sparsity level</mark>\"; that is, the proportion of variables with non-zero coefficients. We denote the <mark>sparsity level</mark> by s, so that s = 1 is the sparsest model (with only a single non-zero coefficient) and s = p is the densest model (all variables affect y).<br>2: We called function sparse approx gsm v1 22 with the following settings: sparse approx gsm v1 22(X,y,k,'profile','fast') in which the target <mark>sparsity level</mark> k was one of {1, 5, 20, 100, 500, 2000, 10000}.<br>",
    "Arabic": "مستوى التناثر",
    "Chinese": "稀疏度",
    "French": "niveau de parcimonie",
    "Japanese": "スパース度",
    "Russian": "уровень разреженности"
  },
  {
    "English": "sparsity regularization",
    "context": "1: 8) left, and so encourages the firing rate distribution to fall inside that diamond, which in the case our our random variables, means maximal entangling. We confirm this intuition in simulation (data and setup the same as Fig. 2) with a variety of <mark>sparsity regularization</mark> strengths (Fig. 8 right).<br>",
    "Arabic": "تنظيم التناثر",
    "Chinese": "稀疏正则化",
    "French": "régularisation de parcimonie",
    "Japanese": "疎性正規化",
    "Russian": "регуляризация разреженности"
  },
  {
    "English": "spatial domain",
    "context": "1: M-step Transform < X > and < XX > to the <mark>spatial domain</mark> and solve for k minimizing < k ⊗ x − y > subject to finite support and non negativity. To express this minimization, suppose that k is an l × l filter.<br>2: (I) ∈ R C k ×W k ×H k \n where Ω k = {0, . . . , W k −1}×{0, . . . , H k −1} is the corresponding <mark>spatial domain</mark>. Note that this feature encoder does not have to be trained with supervised tasks.<br>",
    "Arabic": "المجال المكاني",
    "Chinese": "空间域",
    "French": "domaine spatial",
    "Japanese": "空間領域",
    "Russian": "пространственная область"
  },
  {
    "English": "spatial gradient",
    "context": "1: By exploiting the <mark>spatial gradient</mark> of the statistical measure (18) the new method achieves real-time tracking performance, while e ectively rejecting background clutter and partial occlusions.<br>2: also called structure tensor [ 45 ] , [ 48 ] , [ 51 ] , [ 55 ] : \n G ¼ X n j¼1 \n rI j rI T j : \n Each rI j corresponds to the <mark>spatial gradient</mark> of the jth channel (i.e., vector component) of the vector-valued image I.<br>",
    "Arabic": "التدرج المكاني",
    "Chinese": "空间梯度",
    "French": "gradient spatial",
    "Japanese": "空間勾配",
    "Russian": "пространственный градиент"
  },
  {
    "English": "spatial pooling",
    "context": "1: Unlike <mark>spatial pooling</mark> that requires integer strides, spectral pooling only requires integer output dimensions, which allows for much more fine-grained downsizing. Moreover, spectral pooling acts as a low-pass filter over the entire input, only keeping the lower frequencies i.e. the most relevant information in general and avoiding aliasing (Zhang, 2019).<br>2: f (v, I) = ⎧ ⎨ ⎩f (v, I), if v is a leaf node v ∈Z(v)f (v , I), if v is an internal node (3) \n Where Z ( v ) is the set of all leaf nodes under node v and f ( v , I ) is the output of a Platt-scaled decision value from a linear SVM trained for the category corresponding to input leaf node v. Each linear SVM is trained on sift features with locally-constrained linear coding and <mark>spatial pooling</mark> on a regular 3x3 grid<br>",
    "Arabic": "تجميع مكاني",
    "Chinese": "空间池化",
    "French": "regroupement spatial",
    "Japanese": "空間プーリング",
    "Russian": "пространственное объединение"
  },
  {
    "English": "spatial pyramid",
    "context": "1: We compute these kernel descriptors on fixed size 16 x 16 local image patches, sampled densely over a grid with step size 8 in a <mark>spatial pyramid</mark> setting with four layers. This results in a 4000 dimensional feature vector. We train a set of one-vs-all SVM classifiers for each object class using normal images in PASCAL train set.<br>",
    "Arabic": "الهرم المكاني",
    "Chinese": "空间金字塔",
    "French": "pyramide spatiale",
    "Japanese": "空間ピラミッド",
    "Russian": "пространственная пирамида"
  },
  {
    "English": "speak dialogue system",
    "context": "1: The natural language generation (NLG) component provides much of the persona of a <mark>spoken dialogue system</mark> (SDS), and it has a significant impact on a user's impression of the system. As noted in Stent et al.<br>",
    "Arabic": "نظام حوار متكلم",
    "Chinese": "口语对话系统",
    "French": "système de dialogue parlé",
    "Japanese": "音声対話システム",
    "Russian": "система разговорного диалога"
  },
  {
    "English": "spearman correlation",
    "context": "1: 15 There's a moderate <mark>Spearman correlation</mark> between the per-contest accuracy between the models (ρ = .28, p .001), but (as a null hypothesis) only a slight correlation between contest date and difficulty for either (later contests easier, GPT3/CLIP ρ = .07/.08, p = .08/.05).<br>2: As we are now measuring similarity across continuous variables, we an individual example). compute the <mark>Spearman correlation</mark> (ρ s ).<br>",
    "Arabic": "ارتباط سبيرمان",
    "Chinese": "斯皮尔曼相关系数",
    "French": "corrélation de Spearman",
    "Japanese": "スピアマン相関係数",
    "Russian": "корреляция Спирмена"
  },
  {
    "English": "spearman rank correlation",
    "context": "1: For a given comparison measure, we compute the <mark>Spearman rank correlation</mark> between the comparison measure and the fitted Bradley-Terry coefficients w i for each of the (model, decoder) settings.<br>2: We first asked how consistent the rankings induced by different metrics are by computing the <mark>Spearman rank correlation</mark> between each pair of metrics (Fig. 5A). Interestingly, we found substantial diversity across metrics, though some (EL2N, DDD, and memorization) were fairly similar with rank correlations above 0.7.<br>",
    "Arabic": "معامل ارتباط رتب سبيرمان",
    "Chinese": "斯皮尔曼等级相关系数",
    "French": "corrélation des rangs de Spearman",
    "Japanese": "スピアマンの順位相関",
    "Russian": "Ранговая корреляция Спирмена"
  },
  {
    "English": "special token",
    "context": "1: Users can easily modify tokenizer with interfaces to add additional token mappings, <mark>special tokens</mark> (such as classification or separation tokens), or otherwise resize the vocabulary. Tokenizers can also implement additional useful features for the users.<br>",
    "Arabic": "توكن خاص",
    "Chinese": "特殊标记",
    "French": "jeton spécial",
    "Japanese": "特殊トークン",
    "Russian": "специальный токен"
  },
  {
    "English": "spectral algorithm",
    "context": "1: Finally, we present some analysis of the automaton learned by the <mark>spectral algorithm</mark> to see the information that is captured in the hidden state space.<br>2: Alternatively, one can perform a Parzen window density estimation for continuous variables. However, further approximations are needed in order to make Parzen window compatible with this <mark>spectral algorithm</mark> (Siddiqi et al., 2009).<br>",
    "Arabic": "خوارزمية طيفية",
    "Chinese": "光谱算法",
    "French": "algorithme spectral",
    "Japanese": "スペクトルアルゴリズム",
    "Russian": "спектральный алгоритм"
  },
  {
    "English": "spectral clustering",
    "context": "1: Instead of the directed kNN graph we used the undirected one, as standard <mark>spectral clustering</mark> is not defined for directed graphs. We compare different clusterings by the minimal matching distance: \n d M M (Clust 1 , Clust 2 ) = min π n i=1 1 Clust1(xi) =π(Clust2(xi)) /(2n) \n<br>2: [22] have shown a close connection between the k-means clustering problem and <mark>spectral clustering</mark> algorithms -they proved that if we put the mdimensional feature vectors of the n data points in V into an m-by-n matrix A = ( v1, . . . , vn), then \n<br>",
    "Arabic": "التجميع الطيفي",
    "Chinese": "谱聚类",
    "French": "regroupement spectral",
    "Japanese": "スペクトルクラスタリング",
    "Russian": "спектральная кластеризация"
  },
  {
    "English": "spectral decomposition",
    "context": "1: In each case, the complete <mark>spectral decomposition</mark> is enough to recover the original object; the interesting results relate structure to partial spectral information. Many spectral methods use extreme eigenvalues and associated eigenvectors.<br>2: (2021b) that the SOCP approach for solving SPG-LS can be over 20,000+ times faster than the bisection method proposed in Bishop et al. (2020). Yet, the SOCP method is still not wellsuited for solving large-scale SPG-LS. Indeed, the <mark>spectral decomposition</mark> in the SOCP reformulation process is time-consuming when the future dimension is high.<br>",
    "Arabic": "تحليل طيفي",
    "Chinese": "光谱分解",
    "French": "décomposition spectrale",
    "Japanese": "スペクトル分解",
    "Russian": "спектральное разложение"
  },
  {
    "English": "spectral embedding",
    "context": "1: close to the <mark>spectral embedding</mark> solution . Spectral embedding is the canonical way of mapping nodes of a graph into points with coordinates. Given a symmetric adjacency matrix A, it uses the eigenvectors of A = V ΛV T with the largest k eigenvalues as coordinates of the points.<br>2: SPE provides significant improvements in terms of visualization and lossless compression of graphs, outperforming popular methods such as <mark>spectral embedding</mark> and Laplacian eigenmaps. We find that many classical graphs and networks can be properly embedded using only a few dimensions. Furthermore, introducing structure preserving constraints into dimensionality reduction algorithms produces more accurate representations of highdimensional data.<br>",
    "Arabic": "تضمين طيفي",
    "Chinese": "光谱嵌入",
    "French": "plongement spectral",
    "Japanese": "スペクトル埋め込み",
    "Russian": "спектральное вложение"
  },
  {
    "English": "spectral gap",
    "context": "1: The corollary follows using the fact 1 − α < e −α for 0 < α < 1. It can be shown that if P is reversible, then also P mh and P md are reversible, and thus we can use the <mark>spectral gap</mark> technique to estimate the required burn-in periods of the MH and the MD samplers.<br>2: To understand the asymptotic efficiency of MCMC transition kernels, we can study the asymptotic variance and <mark>spectral gap</mark> of the kernel. The asymptotic variance is defined as \n var p (h, Q) = lim T →∞ 1 T var T t=1 h(x t )(7) \n<br>",
    "Arabic": "الفجوة الطيفية",
    "Chinese": "光谱间隙",
    "French": "écart spectral",
    "Japanese": "スペクトルギャップ",
    "Russian": "спектральный зазор"
  },
  {
    "English": "spectral learning",
    "context": "1: In this paper we study <mark>spectral learning</mark> methods for non-deterministic split headautomata grammars, a powerful hiddenstate formalism for dependency parsing. We present a learning algorithm that, like other spectral methods, is efficient and nonsusceptible to local minima. We show how this algorithm can be formulated as a technique for inducing hidden structure from distributions computed by forwardbackward recursions.<br>2: a∈X for h ∈X and d ∈ {LEFT, RIGHT}, we use <mark>spectral learning</mark> methods based on the works of Hsu et al. (2009), Bailly (2011) and Balle et al. (2012). The main challenge of learning an operator model is to infer a hidden-state space from observable quantities, i.e.<br>",
    "Arabic": "التعلم الطيفي",
    "Chinese": "谱学习",
    "French": "apprentissage spectral",
    "Japanese": "スペクトル学習",
    "Russian": "спектральное обучение"
  },
  {
    "English": "spectral matching",
    "context": "1: Relation to Existing Methods. Note that this refinement step is similar to existing <mark>spectral matching</mark> methods such as [Jain et al. 2007;Mateus et al. 2008;Sharma and Horaud 2010].<br>2: Then, we marginalize it as explained in [25]. Then we use the resulting vector with the algorithm provided online. We also compare to <mark>spectral matching</mark> [16] to show the improvement of using higher-order potentials. First, we added a Gaussian noise to the position of the second set, rotate them, and add outliers.<br>",
    "Arabic": "مطابقة الطيف",
    "Chinese": "光谱匹配",
    "French": "mise en correspondance spectrale",
    "Japanese": "スペクトルマッチング",
    "Russian": "спектральное сопоставление"
  },
  {
    "English": "spectral method",
    "context": "1: As it turns out, the latter approach can be naturally formulated as a constrained matrix completion problem. When applying the <mark>spectral method</mark> , the ( approximate ) values of the target on B are arranged in a matrix H. Thus , the main difference between the two settings can be restated as follows : when learning a weighted automaton representing a distribution , unknown entries of H can be filled in with zeros , while in the general setting there is<br>2: In fact, this is the core idea behind the cryptography-based hardness results for learning deterministic finite automata given by Kearns and Valiant [20] -these same results apply to our setting as well. But, even in cases where the distribution \"cooperates,\" there is still an obstruction in leveraging the <mark>spectral method</mark> for learning general weighted automata.<br>",
    "Arabic": "طريقة طيفية",
    "Chinese": "光谱方法",
    "French": "méthode spectrale",
    "Japanese": "スペクトル法",
    "Russian": "спектральный метод"
  },
  {
    "English": "spectral norm",
    "context": "1: 0.99, A T A − B T B 2 ≤ 10α √ F A F , and B F ≤ 10 A F . The expected number of rows sampled is O( \n Proof Since <mark>spectral norm</mark> is no larger than the Frobenius norm, it is sufficient to prove \n if the jth rows of A is sampled 0 otherwise.<br>2: We will use the following matrix norms: • F the frobenius norm, • <mark>spectral norm</mark>, |A| ∞ elementwise infinity norm, and |A| p→q = max x p =1 A q . We use the shorthand A Ω = P Ω A F .<br>",
    "Arabic": "المعيار الطيفي",
    "Chinese": "谱范数",
    "French": "norme spectrale",
    "Japanese": "スペクトル行列ノルム",
    "Russian": "спектральная норма"
  },
  {
    "English": "spectral property",
    "context": "1: The multiple-descent phenomenon in the CSSP that emerges from our analysis is related to those works in that the <mark>spectral properties</mark> of the data matrix (and, in particular, the condition number) determine the peaks (i.e., phase transitions) discussed in Corollary 1.<br>2: We now relate the previous discussion about bottlenecks and curvature to <mark>spectral properties</mark> of the graph. In particular, since the spectral gap of a graph can be interpreted as a topological obstruction to the graph being partitioned into two communities, we argue below that this quantity is related to the graph bottleneck and should hence be controllable by the curvature.<br>",
    "Arabic": "الخصائص الطيفية",
    "Chinese": "光谱性质",
    "French": "propriété spectrale",
    "Japanese": "スペクトル特性",
    "Russian": "спектральные свойства"
  },
  {
    "English": "spectrogram",
    "context": "1: We used the Praat toolkit to generate the <mark>spectrogram</mark> and find the formants (Boersma et al., 2002). language seeks phonemes that are sufficiently \"distant\" from one another to avoid confusion. Distances between phonemes are defined in some latent \"metric space.\"<br>",
    "Arabic": "مخطط طيفي",
    "Chinese": "声谱图",
    "French": "spectrogramme",
    "Japanese": "スペクトログラム",
    "Russian": "спектрограмма"
  },
  {
    "English": "speech recognition",
    "context": "1: The system's empathic utterances can be \"I'm happy\" or \"That's too bad,\" depending on whether a positive or negative expression is included in the user utterances. Their system's response generation only uses the <mark>speech recognition</mark> confidence and the polarity of user utterances as cues to choose its actions.<br>2: We state two points of limitations and future work in this section. First, the UniVPM combines both restored clean audio and original input audio for downstream <mark>speech recognition</mark>, while without any trade-off to weight them.<br>",
    "Arabic": "التعرف على الكلام",
    "Chinese": "语音识别",
    "French": "reconnaissance de la parole",
    "Japanese": "音声認識",
    "Russian": "распознавание речи"
  },
  {
    "English": "speech recognizer",
    "context": "1: [4], on the other hand, suggest that actions in conversation give probabilistic evidence of understanding, which is represented on a par with other uncertainties in the dialogue system (e.g., <mark>speech recognizer</mark> unreliability). The dialogue manager assumes that content is grounded as long as it judges the risk of misunderstanding as acceptable.<br>2: A preliminary study in (Emami et al., 2003) already showed that this approach is promising in reducing the word error rate of a large vocabulary <mark>speech recognizer</mark>. There are still many interesting problems in applying the neural network enhenced SLM to real applications. Among those, we think the following are of most of interest: \n<br>",
    "Arabic": "معترف بالكلام",
    "Chinese": "语音识别器",
    "French": "reconnaisseur vocal",
    "Japanese": "音声認識器",
    "Russian": "распознаватель речи"
  },
  {
    "English": "speech synthesis model",
    "context": "1: A primary stress point for many natural language processing projects involving Indigenous communities surrounds issues of data sovereignty. It is important that communities direct the development of these tools, and maintain control, ownership, and distribution rights for their data, as well as for the resulting <mark>speech synthesis models</mark> (Keegan, 2019; Brinklow, 2021.<br>",
    "Arabic": "نموذج تركيب الكلام",
    "Chinese": "语音合成模型",
    "French": "modèle de synthèse vocale",
    "Japanese": "音声合成モデル",
    "Russian": "модель синтеза речи"
  },
  {
    "English": "speech synthesizer",
    "context": "1: Although it was unpublished, two highschool students 1 created a statistical parametric <mark>speech synthesizer</mark> for Kanien'kéha by adapting eSpeak (Duddington and Dunn, 2007). We know of no other attempts to create speech synthesis systems for Indigenous languages in Canada.<br>",
    "Arabic": "جهاز تركيب الكلام",
    "Chinese": "语音合成器",
    "French": "synthétiseur vocal",
    "Japanese": "音声合成器",
    "Russian": "синтезатор речи"
  },
  {
    "English": "spurious correlation",
    "context": "1: As a result, an entailment-correlated demonstration with 16 examples exhibits a <mark>spurious correlation</mark> that the target heuristic type leads to entailment. Similarly, we can construct a nonentailment-correlated demonstration, which exhibits a <mark>spurious correlation</mark> that the target heuristic type leads to non-entailment, following the above strategy. Evaluation setup.<br>2: When we use a demonstration with a <mark>spurious correlation</mark> based on a heuristic type, there are two types of possible outputs of models: 1) The model is misled by the <mark>spurious correlation</mark>s in the demonstrations.<br>",
    "Arabic": "ارتباط زائف",
    "Chinese": "虚假相关性",
    "French": "corrélation fallacieuse",
    "Japanese": "誤った相関",
    "Russian": "ложная корреляция"
  },
  {
    "English": "square euclidean distance",
    "context": "1: Using the <mark>squared Euclidean distance</mark> in parameter space (akin to (Nichol et al., 2018;Flennerhag et al., 2019)) is surprisingly effective. However, it exhibits substantial volatility and is prone to crashing (c.f. Figure 15); changing the matching function to policy KL-divergence stabilizes meta-optimisation.<br>",
    "Arabic": "المسافة الإقليدية المربعة",
    "Chinese": "平方欧氏距离",
    "French": "distance euclidienne au carré",
    "Japanese": "二乗ユークリッド距離",
    "Russian": "\"квадрат евклидова расстояния\""
  },
  {
    "English": "squared error loss",
    "context": "1: Each decoder layer contributes two terms to the objective, a forward objective and a backward objective. Layer 1 decodes the data and uses a <mark>squared error loss</mark>: \n Layers 2, . . . , L decode the representations from the layer below, which are tanh-activated and thus constrained to the interval (−1, 1).<br>2: Consider the classical linear regression setting in which y = x ⊤ θ ⋆ + ε, where ε ∼ N(0, σ 2 ). Using the standard <mark>squared error loss</mark> ℓ(θ, (x, y)) = 1 2 (θ ⊤ x − y) 2 , we obtain that \n<br>",
    "Arabic": "خسارة مربع الخطأ",
    "Chinese": "平方误差损失",
    "French": "perte d'erreur quadratique",
    "Japanese": "二乗誤差損失",
    "Russian": "квадратичная ошибка потерь"
  },
  {
    "English": "stable model",
    "context": "1: This allows for the justification a ← x → b. If the interpretation of a and b is t, then the value of this justification for x is t. Therefore, ({a, b, x} , {a, b, x}) is an ultimate <mark>stable model</mark>, while not a <mark>stable model</mark>.<br>2: The well-founded model is the ≤ p -least <mark>stable model</mark>. Proof sketch. Lemma 2 implies that the well-founded model is less precise than any <mark>stable model</mark>. Then Proposition 5 finishes the proof.<br>",
    "Arabic": "النموذج المستقر",
    "Chinese": "稳定模型",
    "French": "modèle stable",
    "Japanese": "安定モデル",
    "Russian": "устойчивая модель"
  },
  {
    "English": "stable model semantic",
    "context": "1: In logic programming where predicate completion is best known and commonly referred to as program completion semantics, its relationships with other semantics, especially the answer set semantics (also known as the <mark>stable model semantics</mark>) of Gelfond and Lifschitz (1988), have been studied quite extensively.<br>",
    "Arabic": "دلالات النموذج المستقر",
    "Chinese": "稳定模型语义",
    "French": "sémantique des modèles stables",
    "Japanese": "安定モデル意味論",
    "Russian": "семантика устойчивой модели"
  },
  {
    "English": "stance detection",
    "context": "1: We observed that the 30 unclear responses have an average <mark>stance detection</mark> confidence of 0.76, while the 80 unclear responses have an average confidence of 0.90. This indicates that the stance detector's confidence could serve as a heuristic to filter out unclear responses.<br>2: with sociodemographic profiles in a controlled setting which comprises seven datasets reflecting four different subjective NLP classification tasks (sentiment analysis, hatespeech detection, toxicity detection, and <mark>stance detection</mark>).<br>",
    "Arabic": "كشف الموقف",
    "Chinese": "立场检测",
    "French": "détection de position",
    "Japanese": "スタンス検出",
    "Russian": "обнаружение позиции"
  },
  {
    "English": "standard normal distribution",
    "context": "1: P H1 (n FSSD 2 > r) ≈ 1 − Φ r √ nσ H 1 − √ n FSSD 2 σ H 1 \n , where Φ denotes the cumulative distribution function of the <mark>standard normal distribution</mark>, and σ H1 is defined in Proposition 2. Proof.<br>2: min g∈Ω D(p y (y)|q(y)),(18) \n where y = g(x) follows distribution p y (•) and q(•) is the <mark>standard normal distribution</mark>. Now assume that g(•) is invertible and let its inverse be h = g −1 .<br>",
    "Arabic": "التوزيع الطبيعي القياسي",
    "Chinese": "标准正态分布",
    "French": "distribution normale standard",
    "Japanese": "標準正規分布",
    "Russian": "стандартное нормальное распределение"
  },
  {
    "English": "start token",
    "context": "1: During inference, using a special <mark>start token</mark> as the initial input of the decoder, the output text will be generated token by token. Revised NAR Decoding Process. In the denoising process of our approach, BART is employed to recover the masked tokens from the noised target text at each time step.<br>",
    "Arabic": "رمز البدء",
    "Chinese": "起始标记",
    "French": "jeton de départ",
    "Japanese": "開始トークン",
    "Russian": "Начальный токен"
  },
  {
    "English": "state",
    "context": "1: In the framework of reinforcement learning, we define the <mark>state</mark> at step t as all the generated sequences before t (i.e., s t = x 1:t ), and the action at step t as the t-th output token (i.e., a t = x t ).<br>2: In a <mark>state</mark> (S i , R i,j:j+k ), our agent simultaneously takes two actions -( 1) select a position in R i,j:j+k for insertion or replacement, ( 2 ) generate a candidate empathic sentence.<br>",
    "Arabic": "حالة",
    "Chinese": "状态",
    "French": "état",
    "Japanese": "状態",
    "Russian": "состояние"
  },
  {
    "English": "state action pair",
    "context": "1: So, for a Markov time-homogeneous policy, once a <mark>state action pair</mark> (s, a) is given, the distribution of the next state is the same regardless of the time and history-hence the term \"static\". Again, such policies can be represented as (κ, κ, κ, . . . )<br>",
    "Arabic": "زوج الحالة والإجراء",
    "Chinese": "状态动作对",
    "French": "paire état-action",
    "Japanese": "状態行動ペア",
    "Russian": "пара состояние-действие"
  },
  {
    "English": "state distribution",
    "context": "1: The agent's problem is to learn a policy that maximises the value given an expectation over s 0 , defined either by an initial <mark>state distribution</mark> in the episodic setting (e.g. Atari, Section 5.2) or the stationary state-visitation distribution under the policy in the non-episodic setting (Section 5.1).<br>2: Here, is a discount factor,˜is the mixed policy, and states are repeatedly sampled from their induced <mark>state distribution</mark>˜. The mixed policy˜is a mixture of the expert policy and the agent's policy (Ross et al., 2011).<br>",
    "Arabic": "توزيع الحالة",
    "Chinese": "状态分布",
    "French": "distribution des états",
    "Japanese": "状態分布",
    "Russian": "распределение состояний"
  },
  {
    "English": "state estimation",
    "context": "1: uncertain, and <mark>state estimation</mark> is notoriously challenging, rendering these approaches ineffective. An alternative approach is model-based planning and control, with recent impressive results on complex tasks like dough manipulation [16,32,33].<br>2: e inclusion of this package enables research on driving at the limits of control even without a motion capture system for <mark>state estimation</mark>.<br>",
    "Arabic": "تقدير الحالة",
    "Chinese": "状态估计",
    "French": "estimation d'état",
    "Japanese": "状態推定",
    "Russian": "оценка состояния"
  },
  {
    "English": "state machine",
    "context": "1: Traditional planning methods in the industry often are rule-based, formulated by \"if-else\" <mark>state machines</mark> conditioned on various scenarios which are described with prior detection and prediction results. In our learning-based model, we take the upstream ego-vehicle query, and the dense BEV feature B as input, and predict one trajectoryτ for total T p timesteps.<br>",
    "Arabic": "آلة الحالات",
    "Chinese": "有限状态机",
    "French": "Automate à états",
    "Japanese": "ステートマシン",
    "Russian": "конечный автомат"
  },
  {
    "English": "state matrix",
    "context": "1: Recall that without loss of generality, we can assume that the <mark>state matrix</mark> A = Λ − P Q * is diagonal plus low-rank (DPLR), potentially over C. Our goal in this section is to explicitly write out a closed form for the discretized matrix A.<br>2: Conceptually, the inputs u k can be viewed as sampling an implicit underlying continuous signal u(t), where u k = u(k∆). To discretize the continuous-time SSM, we follow prior work in using the bilinear method [43], which converts the <mark>state matrix</mark> A into an approximation A . The discrete SSM is \n<br>",
    "Arabic": "مصفوفة الحالة",
    "Chinese": "状态矩阵",
    "French": "matrice d'état",
    "Japanese": "状態行列",
    "Russian": "матрица состояния"
  },
  {
    "English": "state of the art algorithm",
    "context": "1: With no historical examples, rubric sampling enables feedback with accuracy close to the fidelity of human teachers, outperforming data-intensive <mark>state of the art algorithms</mark>.<br>",
    "Arabic": "خوارزمية متطورة",
    "Chinese": "最先进算法",
    "French": "algorithme de pointe",
    "Japanese": "最先端のアルゴリズム",
    "Russian": "современный алгоритм"
  },
  {
    "English": "state representation",
    "context": "1: For example, the agent can learn to recognize that pushing the box over a rug is undesirable because it is tied to specific locations that are part of its <mark>state representation</mark>. If the surface type is a state feature, the agent could further generalize what it learns. Definition 1.<br>2: The main technical contributions include a novel <mark>state representation</mark> of card and betting information, a multi-task self-play training loss function, and a new model evaluation and selection metric to generate the final model. In a study involving 100,000 hands of poker, AlphaHoldem defeats Slumbot and DeepStack using only one PC with three days training.<br>",
    "Arabic": "تمثيل الحالة",
    "Chinese": "状态表示",
    "French": "représentation d'état",
    "Japanese": "状態表現",
    "Russian": "представление состояния"
  },
  {
    "English": "state sequence",
    "context": "1: To detect a waveform inside a (much longer) time series y 1 y 2 . . . y t . . ., an obvious approach would be to match the model against every subwindow y i y i+1 . . . y j , find the most likely <mark>state sequence</mark> s i s i+1 . . .<br>2: , a n that induces a <mark>state sequence</mark> s 0 , s 1 , . . . , s n such that s 0 = I and, for each i such that 1 ≤ i ≤ n, a i is applicable in s i−1 and generates the successor state s i = θ(s i−1 , a i ).<br>",
    "Arabic": "\"تسلسل الحالة\"",
    "Chinese": "状态序列",
    "French": "séquence d'états",
    "Japanese": "状態シーケンス",
    "Russian": "последовательность состояний"
  },
  {
    "English": "state space",
    "context": "1: In our work, T is built by keeping a table of observed (s, a, s ) triplets, which describes the connectivity graph over the <mark>state space</mark>. More advanced methods could also be used to infer more complex transition dynamics [27,30].<br>2: These properties make UCT ideally suited to the game of Go, which has a large <mark>state space</mark> and branching factor, and for which no strong evaluation functions are known.<br>",
    "Arabic": "فضاء الحالة",
    "Chinese": "状态空间",
    "French": "espace d'états",
    "Japanese": "状態空間",
    "Russian": "пространство состояний"
  },
  {
    "English": "state trajectory",
    "context": "1: In this section we present the results of our experiments. First, we demonstrate the behavior of <mark>state trajectories</mark> and sojourn times sampled from the prior to give a qualitative idea of the range of time series that can be captured by our model.<br>",
    "Arabic": "مسار الحالة",
    "Chinese": "状态轨迹",
    "French": "Trajectoire d'état",
    "Japanese": "状態軌跡",
    "Russian": "траектория состояний"
  },
  {
    "English": "state transition",
    "context": "1: According to Theorem 4, ̟ incentivizes an FS agent to take all advised actions. Hence, <mark>state transition</mark> will only happen among the states in S × G A . Any trajectory generated by using ̟ will be generated with the same probability that it is generated by using π.<br>2: Such operator potentials combine synergically with symbolic search as they have property (1): Under certain conditions, the operator potential of an operator o is equal to the difference in heuristic values h(s ) − h(s) for any <mark>state transition</mark> s → s induced by the operator o.<br>",
    "Arabic": "انتقال الحالة",
    "Chinese": "状态转移",
    "French": "transition d'état",
    "Japanese": "状態遷移",
    "Russian": "переход состояния"
  },
  {
    "English": "state transition function",
    "context": "1: P is the <mark>state transition function</mark>, such that for all s, a, s , t, the function P(s, a, s ) denotes the transition probability P (s |s, e), where a = φ(e).<br>2: The environment has a true state s ∈ S. At each time step, each agent simultaneously chooses an action u a ∈ U , forming a joint action u ∈ U ≡ U n which induces a transition in the environment according to the <mark>state transition function</mark> \n<br>",
    "Arabic": "دالة انتقال الحالة",
    "Chinese": "状态转移函数",
    "French": "fonction de transition d'état",
    "Japanese": "状態遷移関数",
    "Russian": "функция перехода состояний"
  },
  {
    "English": "state transition matrix",
    "context": "1: The first assumption states that each <mark>state transition matrix</mark> A (k) is exponentially stable, which is a quantified version of stability and has appeared in various forms in the literature of LDS [KSH00, CHK + 18]; here, κ A can be regarded as a condition number, while ρ is a contraction rate.<br>2: We begin our discussion with a standard discrete-time finite-state Markov model where each segment in the data corresponds to a state of the Markov model. Let the number of states be K. \n The parameters of the model include π , the initial state distribution ( typically we will constrain the waveform to always begin with the same segment ) , and A , the K × K <mark>state transition matrix</mark> ( again , this transition matrix can be constrained to be a `` left-to-right '' model which enforces a strict ordering in time of the segments<br>",
    "Arabic": "مصفوفة انتقال الحالة",
    "Chinese": "状态转移矩阵",
    "French": "matrice de transition d'état",
    "Japanese": "状態遷移行列",
    "Russian": "матрица переходов состояний"
  },
  {
    "English": "state transition model",
    "context": "1: A <mark>state transition model</mark> models the environment ( , ). Recall that a state transition just consists in the generation of a single template, where the current state is the set of all templates that have been generated up to the current step. Here, we propose a neural model that produces a representation of .<br>2: In Section 2.1., we will give detailed description on how to incorporate multiple cues into the observation model. A simplified <mark>state transition model</mark> is then discussed in Section 2.2 based on the contour smoothness constraint.<br>",
    "Arabic": "نموذج انتقال الحالة",
    "Chinese": "状态转移模型",
    "French": "modèle de transition d'état",
    "Japanese": "状態遷移モデル",
    "Russian": "модель переходов состояний"
  },
  {
    "English": "state transition probability",
    "context": "1: space , and Ω = × i Ω i is the joint observation space . 1 Joint action a = a 1 , . . . , a n causes state s ∈ S to transition to s ∈ S with probability P (s \n |s, a) = T (s, a, s ).<br>",
    "Arabic": "احتمالية انتقال الحالة",
    "Chinese": "状态转移概率",
    "French": "probabilité de transition d'état",
    "Japanese": "状態遷移確率",
    "Russian": "вероятность перехода состояний"
  },
  {
    "English": "state value function",
    "context": "1: Indeed, we argue that by following the above trajectories, the <mark>state value function</mark> of the agent is as follows; one can easily verify that the actions the agent takes according to the above trajectories are indeed (strictly) optimal with respect these state values. • V (s X ) = 0.<br>2: (2021) propose an equivalent update that uses a <mark>state value function</mark> V (s) instead of a state-action value function Q(s, a): \n<br>",
    "Arabic": "دالة قيمة الحالة",
    "Chinese": "状态值函数",
    "French": "fonction de valeur d'état",
    "Japanese": "状態価値関数",
    "Russian": "функция ценности состояния"
  },
  {
    "English": "state variable",
    "context": "1: Environment agent is a special agent in ISPL system that provides observable variables that can be accessed by other agents. Every agent starts with declaration of local variables. The first mapping rule is used to define the local variables of agent. We declare a <mark>state variable</mark> to list all possible states in the system: \n<br>",
    "Arabic": "متغير الحالة",
    "Chinese": "状态变量",
    "French": "variable d'état",
    "Japanese": "状態変数",
    "Russian": "переменная состояния"
  },
  {
    "English": "state vector",
    "context": "1: We include this information in the <mark>state vector</mark> because the objects a user acquires are strongly correlated to the intended activity [4]. o j = 1 if the user has object j in their possession and zero otherwise. O is a set of pre-defined objects available to the user.<br>2: We will make use of some intermediate notions to describe our construction. We will use these multiple times in our construction. Particularly, Lemma B.1 will be used to combine the information of the <mark>state vector</mark>, input and the symbol at the top of the stack.<br>",
    "Arabic": "متجه الحالة",
    "Chinese": "状态向量",
    "French": "vecteur d'état",
    "Japanese": "状態ベクトル",
    "Russian": "вектор состояния"
  },
  {
    "English": "state-action distribution",
    "context": "1: There is a rich literature on offline RL with function approximation when the data distribution µ is sufficiently rich to cover the <mark>state-action distribution</mark> d π for any π ∈ Π (Antos et al., 2008;Munos, 2003;Farahmand et al., 2010;Chen & Jiang, 2019;Xie & Jiang, 2020).<br>2: The COMA gradient is given by (h 1 , )  where the expectation E π is with respect to the <mark>state-action distribution</mark> induced by the joint policy π. Now let d π (s) be the discounted ergodic state distribution as defined by Sutton et al. (1999): \n<br>",
    "Arabic": "توزيع الحالة-الإجراء",
    "Chinese": "状态-动作分布",
    "French": "distribution état-action",
    "Japanese": "状態-行動分布",
    "Russian": "распределение состояния-действие"
  },
  {
    "English": "state-action space",
    "context": "1: (9). More specifically, Eq. ( 9) is an off-policy algorithm, meaning that the policy used to explore (i.e. π + ) is different from the policy that is updated (i.e. π) 1 . For any off-policy policy gradient method to converge to an optimal policy ( even in the tabular case ) , the behavior policy ( to explore ) and the target policy ( to be updated ) must appropriately visit the same <mark>state-action space</mark> , i.e. , if π ( a|s ) > 0 then π + ( a|s ) > 0 [ Sutton and<br>2: A natural follow up question to the above result asks whether task realization is closed under a set of CMPs. Our next result answers this question in the negative. Theorem 4.7. Task realization is not closed under sets of CMPs with shared <mark>state-action space</mark>. That is, there exist choices of T and \n E = {E 1 , .<br>",
    "Arabic": "فضاء الحالة-الإجراء",
    "Chinese": "状态-动作空间",
    "French": "espace état-action",
    "Japanese": "状態行動空間",
    "Russian": "пространство состояний-действий"
  },
  {
    "English": "state-action value",
    "context": "1: The dueling architecture represents both the value V (s) and advantage A(s, a) functions with a single deep model whose output combines the two to produce a <mark>state-action value</mark> Q(s, a). Unlike in advantage updating, the representation and algorithm are decoupled by construction.<br>2: We regard each generative model as a stochastic parametrized policy and use Monte Carlo search to approximate the <mark>state-action value</mark>. Then we use the discriminator to evaluate the sequence and guide the learning of the generative model. But unlike previous works, our model contains multiple generators and one discriminator.<br>",
    "Arabic": "قيمة الحالة-الإجراء",
    "Chinese": "状态-动作值",
    "French": "valeur état-action",
    "Japanese": "状態行動価値",
    "Russian": "ценность состояния-действия"
  },
  {
    "English": "state-action value function",
    "context": "1: (2021) propose an equivalent update that uses a state value function V (s) instead of a <mark>state-action value function</mark> Q(s, a): \n<br>2: The agent typically utilizes a stateaction value function Q(s, a) to determine which action a to perform in state s. A commonly used technique for learning an optimal value function is Q-learning (Watkins and Dayan, 1992), in which the agent iteratively updates Q(s, a) using the rewards obtained from episodes.<br>",
    "Arabic": "دالة قيمة الحالة-الإجراء (State-Action Value Function)",
    "Chinese": "状态-动作值函数",
    "French": "fonction de valeur état-action",
    "Japanese": "状態行動価値関数",
    "Russian": "функция ценности состояния-действия"
  },
  {
    "English": "state-of-the-art",
    "context": "1: c) Comparison with <mark>State-of-the-art</mark> TCF models: Table III compares the performance of our method and that of reference TCF methods on the Netflix and ML-1M datasets.<br>",
    "Arabic": "- الحاليّ المتقدم",
    "Chinese": "最先进的",
    "French": "état de l'art",
    "Japanese": "最先端の",
    "Russian": "передовой"
  },
  {
    "English": "state-of-the-art baseline",
    "context": "1: We study the performance of differentiable tree ensembles in various settings and compare against the relevant <mark>state-of-the-art baselines</mark> for each setting. The different settings can be summarized as follows: (i) Comparison of a single soft tree with state-of-the-art classical tree method in terms of test performance and depth.<br>",
    "Arabic": "المعيار الأساسي الأحدث",
    "Chinese": "最先进基准",
    "French": "référence de pointe",
    "Japanese": "最新のベースライン",
    "Russian": "передовая базовая модель"
  },
  {
    "English": "state-of-the-art method",
    "context": "1: In parallel, we also emphasize the potential risk of overestimating the performance of the <mark>state-of-the-art methods</mark> on the existing commonsense benchmarks; these models might be solving the problems right for the wrong reasons, by relying on spurious statistical patterns (annotation artifacts). Our work suggests a new perspective for designing benchmarks for measuring progress in AI.<br>2: These RC systems have already been used in a broad range of applications (often outperforming other state-ofthe-art methods) such as chaotic time-series prediction [4], single digit speech recognition [5], and robot control [6].<br>",
    "Arabic": "طريقة متطورة",
    "Chinese": "最先进方法",
    "French": "méthode de pointe",
    "Japanese": "最先端の手法",
    "Russian": "передовой метод"
  },
  {
    "English": "state-of-the-art model",
    "context": "1: In practice, however, estimating dataset difficulty is often limited to an informal comparison of <mark>state-of-the-art model</mark> performance to that of humans; the bigger the performance gap, the harder the dataset is said to be (Ethayarajh & Jurafsky, 2020;Ma et al., 2021).<br>",
    "Arabic": "النموذج المتطور",
    "Chinese": "最先进模型",
    "French": "- Modèle de pointe",
    "Japanese": "最先端モデル",
    "Russian": "передовая модель"
  },
  {
    "English": "state-of-the-art system",
    "context": "1: However, much less research has been conducted for non-factoid question answering (NFQA), where longer passage-level answers such as opinions or explanations are expected. The performance of <mark>state-of-the-art systems</mark> on existing datasets such as NFL6, ANTIQUE, NLQuAD and ELI5 [11,14,21,39] falls far behind that of humans [14,24,39].<br>2: Furthermore, we show that disagreement between human annotators can be interpreted as a feature of a system's performance, rather than a weakness in the evaluation approach. We apply the framework to three well-known domains and common baselines and <mark>state-of-the-art systems</mark> to produce a stable ranking among them.<br>",
    "Arabic": "نظام على أحدث طراز",
    "Chinese": "最先进系统",
    "French": "système de pointe",
    "Japanese": "最先端システム",
    "Russian": "система передового уровня"
  },
  {
    "English": "static analysis",
    "context": "1: We elaborate on the feature set in Section 3, and show that the system still achieves high detection coverage if adversaries utilize different levels of obfuscation techniques. The prior work most relevant to ours is MineSweeper [30], which introduces a <mark>static analysis</mark> technique to study the wasm code used by cryptojacking code.<br>",
    "Arabic": "التحليل الثابت",
    "Chinese": "静态分析",
    "French": "analyse statique",
    "Japanese": "静的解析",
    "Russian": "статический анализ"
  },
  {
    "English": "stationarity",
    "context": "1: According to the mixing property of LDS, if T subspace is larger than some appropriately defined mixing time, then each sample trajectory in M subspace will mix sufficiently and nearly reach <mark>stationarity</mark> (when constrained to t ∈ Ω 1 ∪ Ω 2 ). In this case, it is easy to check that \n<br>2: Under <mark>stationarity</mark> assumptions, for a pixel i, the NLmeans algorithm converges to the conditional expectation of i once observed a neighborhood of it. In this case, the <mark>stationarity</mark> conditions amount to say that as the size of the image grows we can find many similar patches for all the details of the image.<br>",
    "Arabic": "ثبات",
    "Chinese": "平稳性",
    "French": "stationnarité",
    "Japanese": "定常性",
    "Russian": "стационарность"
  },
  {
    "English": "stationary distribution",
    "context": "1: The Markov chain is reversible, i.e., a swap can be undone by a single (reverse) swap. However, the chain is not regular, i.e., some datasets (states) have more neighbors than others. This implies that the <mark>stationary distribution</mark> of the chain is not the uniform distribution.<br>2: Secondly, we will show that the mixing time (the time for the chain to become close to its <mark>stationary distribution</mark>) of asynchronous Gibbs sampling can be up to exponentially greater than that of the corresponding sequential chain.<br>",
    "Arabic": "التوزيع الثابت",
    "Chinese": "平稳分布",
    "French": "distribution stationnaire",
    "Japanese": "定常分布",
    "Russian": "стационарное распределение"
  },
  {
    "English": "stationary kernel",
    "context": "1: In the case of <mark>stationary kernels</mark>, our main result proves that priors with smoother sample functions, and datasets with more concentrated inputs admit sparser approximations.<br>2: Table 1 summarizes the spectral decay of several <mark>stationary kernels</mark>, as well as the implications for the number of inducing points needed for inference to provably converge with our bounds.<br>",
    "Arabic": "نواة ثابتة",
    "Chinese": "平稳核",
    "French": "noyau stationnaire",
    "Japanese": "定常カーネル",
    "Russian": "стационарное ядро"
  },
  {
    "English": "stationary policy",
    "context": "1: , R max ] is a reward function , and γ ∈ ( 0 , 1 ) is a discount factor . Let π : S → P(A) be a <mark>stationary policy</mark>, where P(A) is a probability distribution on A.<br>",
    "Arabic": "سياسة ثابتة",
    "Chinese": "静态策略",
    "French": "politique stationnaire",
    "Japanese": "定常方策",
    "Russian": "стационарная политика"
  },
  {
    "English": "statistical analysis",
    "context": "1: This suggests that, for two samples within the same trajectory that are sufficiently far apart, we can treat them as being (almost) independent of each other; this simple fact shall inspire our algorithmic design and streamline <mark>statistical analysis</mark> later on.<br>2: I know that to accept this prize, they're going to need <mark>statistical analysis</mark>. I know that in order to accept this prize, they're going to need a statistic analysis. I know that if they accept this prize, they're gonna need a <mark>statistical analysis</mark>.<br>",
    "Arabic": "تحليل إحصائي",
    "Chinese": "统计分析",
    "French": "analyse statistique",
    "Japanese": "統計分析",
    "Russian": "статистический анализ"
  },
  {
    "English": "statistical independence",
    "context": "1: First, we point out that this model is exceedingly general. The only constraint of any significance is that we require <mark>statistical independence</mark> of data generation across cells. Second, there is the issue of statistical significance of the regions returned as the result of the search.<br>",
    "Arabic": "الاستقلال الإحصائي",
    "Chinese": "统计独立性",
    "French": "indépendance statistique",
    "Japanese": "統計的独立性",
    "Russian": "статистическая независимость"
  },
  {
    "English": "statistical learning",
    "context": "1: A number of phase transitions have been recently observed in the machine learning literature which are commonly dubbed double descent. The term was introduced by Belkin et al. (2019a) in the context of generalization error of <mark>statistical learning</mark>, however Poggio et al.<br>",
    "Arabic": "التعلم الإحصائي",
    "Chinese": "统计学习",
    "French": "apprentissage statistique",
    "Japanese": "統計的学習",
    "Russian": "статистическое обучение"
  },
  {
    "English": "statistical learning algorithm",
    "context": "1: et al. , 2021 ) , robust optimization ( Johnstone & Cox , 2021 ) or to infer the performance guarantee for <mark>statistical learning algorithms</mark> ( Holland , 2020 ; Cella & Ryan , 2020 ) .<br>2: The Punjab-IDSS system leverages the health hotline information combined with Internet-based news monitoring tools to provide accurate, real-time dengue epidemic detection at a fine-grained location granularity within a city. To achieve this goal, the system uses a combination of several sophisticated <mark>statistical learning algorithms</mark> combined with locality-specific dengue propagation models.<br>",
    "Arabic": "خوارزمية التعلم الإحصائي",
    "Chinese": "统计学习算法",
    "French": "algorithme d'apprentissage statistique",
    "Japanese": "統計的学習アルゴリズム",
    "Russian": "алгоритм статистического обучения"
  },
  {
    "English": "statistical learning theory",
    "context": "1: It is said that statistical praxis is of greatest import in those areas of science least informed by theory. While linguistic theory and <mark>statistical learning theory</mark> both have much to contribute to part-ofspeech tagging, we still lack a theory of the tagging task rich enough to guide hypothesis formation.<br>",
    "Arabic": "نظرية التعلم الإحصائي",
    "Chinese": "统计学习理论",
    "French": "théorie de l'apprentissage statistique",
    "Japanese": "統計的学習理論",
    "Russian": "статистическая теория обучения"
  },
  {
    "English": "statistical machine translation",
    "context": "1: For instance, there is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of gold labels and system predictions. For example , in the 2014 Workshop on <mark>Statistical Machine Translation</mark> ( WMT ) , which since 2012 has provided a main venue for evaluation of systems , sentence-level systems were evaluated with respect to three distinct gold standard representations and each of those compared to predictions using four different measures , resulting in a total of 12 different system rankings , 6 identified<br>2: The early-stage of text-to-gloss translation systems were built using <mark>Statistical Machine Translation</mark> (SMT; San-Segundo et al., 2012;López-Ludeña et al., 2014), in an attempt to translate spoken language into a signing 3D avatar using SL glosses as intermediate.<br>",
    "Arabic": "الترجمة الآلية الإحصائية",
    "Chinese": "统计机器翻译",
    "French": "traduction automatique statistique",
    "Japanese": "統計的機械翻訳",
    "Russian": "статистический машинный перевод"
  },
  {
    "English": "statistical machine translation system",
    "context": "1: A good decoding algorithm is critical to the success of any <mark>statistical machine translation system</mark>. The decoder's job is to find the translation that is most likely according to set of previously learned parameters (and a formula for combining them).<br>",
    "Arabic": "نظام الترجمة الآلية الإحصائية",
    "Chinese": "统计机器翻译系统",
    "French": "système de traduction automatique statistique",
    "Japanese": "統計的機械翻訳システム",
    "Russian": "статистическая система машинного перевода"
  },
  {
    "English": "statistical measure",
    "context": "1: When performing the learning of a particular activity, each vectors corresponding to each of the <mark>statistical measure</mark> stores the number of temporal windows whose <mark>statistical measure</mark> is included in that range as well as the activity that is being developed.<br>",
    "Arabic": "مقياس إحصائي",
    "Chinese": "统计量",
    "French": "mesure statistique",
    "Japanese": "統計的尺度",
    "Russian": "статистической меры"
  },
  {
    "English": "statistical model",
    "context": "1: The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for <mark>statistical models</mark> that rely on selectional preferences or word associations.<br>",
    "Arabic": "النموذج الإحصائي",
    "Chinese": "统计模型",
    "French": "modèle statistique",
    "Japanese": "統計モデル",
    "Russian": "статистическая модель"
  },
  {
    "English": "statistical translation model",
    "context": "1: Machine Translation aims at automatic translation from one language to another one. Statistical translation methods are among successful approaches. In the common architecture of <mark>statistical translation models</mark>, there is a translation model, a language model, and a decoding algorithm.<br>",
    "Arabic": "نموذج الترجمة الإحصائية",
    "Chinese": "统计翻译模型",
    "French": "modèle de traduction statistique",
    "Japanese": "統計的翻訳モデル",
    "Russian": "статистическая модель перевода"
  },
  {
    "English": "steep descent",
    "context": "1: The deterministic component of the PDE is obtained by performing <mark>steepest descent</mark> on the negative log-posterior, as derived in Zhu and Yuille (1996). We illustrate the approach by deriving the deterministic component of the PDE for the evolution of the boundary between a letter T j and a generic visual pattern region R i .<br>2: Thus, the BMG update comes out as the difference between to distances. The first distance is a distortion terms that measures how well the target aligns to the tangent vector −G T g, which is the direction of <mark>steepest descent</mark> in the immediate vicinity of x (K) (c.f. Lemma 1).<br>",
    "Arabic": "الانحدار الشديد",
    "Chinese": "陡峭下降",
    "French": "descente abrupte",
    "Japanese": "勾配降下",
    "Russian": "наискорейший спуск"
  },
  {
    "English": "steerable filter",
    "context": "1: Each edgelet e ik = {p ik , θ ik } is a particle which embeds both location p ik ∈ R 2 and orientation θ ik ∈ [0, 2π) information. We use H4 and G4 <mark>steerable filters</mark> [3] to filter the image and obtain orientation energy per pixel.<br>2: Extracting boundaries from images is a nontrivial task by itself. We use a simple algorithm for boundary extraction, analyzing oriented energy using <mark>steerable filters</mark> [3] and tracking the boundary in a manner similar to that of the Canny edge detector [2].<br>",
    "Arabic": "مرشح قابل للتوجيه",
    "Chinese": "可定向滤波器 (steerable filter)",
    "French": "filtre orientable",
    "Japanese": "ステアリング可能フィルタ (steerable filter)",
    "Russian": "управляемый фильтр"
  },
  {
    "English": "stemmer",
    "context": "1: The preprocessing of documents and queries is minimum, involving only stemming with the Porter's <mark>stemmer</mark>. No stop words have been removed, as it would introduce at least one extra parameter (e.g., the number of stop words) into our experiments.<br>",
    "Arabic": "مجذر",
    "Chinese": "词干提取器",
    "French": "Stemmatiseur",
    "Japanese": "語幹切り詞",
    "Russian": "стеммер"
  },
  {
    "English": "stereo algorithm",
    "context": "1: The discrete energy minimization approach is known to lead to strong results for two-camera stereo [22,27], and is therefore worth investigating for multiple cameras. In [14] we proposed a (two camera) <mark>stereo algorithm</mark> that shares a number of properties with the present work.<br>2: The <mark>stereo algorithm</mark> of [2] performs brittle block-matching on a rectified stereo pair to produce, for each pixel, an interval (lower and upper values [l i , u i ] parametrizing an interval) of likely depths for that pixel.<br>",
    "Arabic": "خوارزمية تصوير ثلاثي الأبعاد",
    "Chinese": "立体视觉算法",
    "French": "algorithme stéréo",
    "Japanese": "ステレオアルゴリズム",
    "Russian": "алгоритм стереозрения"
  },
  {
    "English": "stereo benchmark",
    "context": "1: We address this by setting the leftmost 80 columns of c init to 0, thereby causing them to be initially ignored. See Figure 1 for a visualization of the initial confidence estimated using this procedure on one of the test-set depth maps from the Middlebury <mark>stereo benchmark</mark> v3.<br>",
    "Arabic": "معيار الاستريو",
    "Chinese": "立体基准测试",
    "French": "référentiel stéréo",
    "Japanese": "ステレオベンチマーク",
    "Russian": "стерео бенчмарк"
  },
  {
    "English": "stereo disparity",
    "context": "1: In the context of <mark>stereo disparity</mark> and optical flow, explicit modeling of discontinuities by means of segmentation or layer-based formulations has a long history [23] and has recently gained renewed attention: Bleyer et al. [4] estimate disparity by assuming the scene to be segmented into planar superpixels and parameterizing their geometry.<br>",
    "Arabic": "تباعد مجسم",
    "Chinese": "双目视差",
    "French": "disparité stéréo",
    "Japanese": "立体視差",
    "Russian": "стереодиспария"
  },
  {
    "English": "stereo image",
    "context": "1: A unified representation is introduced to model all classes of <mark>stereo images</mark>, based on the concept of a quadric view. The benefits include a unified treatment of projection and triangulation operations for all stereo image varieties. A recipe book for generating <mark>stereo images</mark> is provided.<br>2: The horse reconstruction demonstrates the ability to obtain a true \"object model\" from a single stereo pair. In contrast, obtaining a model of this sort using traditional <mark>stereo images</mark> requires stitching together several stereo reconstructions acquired from different viewpoints.<br>",
    "Arabic": "صورة ثلاثية الأبعاد",
    "Chinese": "立体影像",
    "French": "image stéréo",
    "Japanese": "ステレオ画像",
    "Russian": "стереоизображение"
  },
  {
    "English": "stereo matching",
    "context": "1: In addition, the proposed relaxation uses a more compact representation and extends to isotropic and convex one-homogeneous regularizers. To illustrate the advantages of isotropic regularizations, Fig. 8a and Fig. 8b show a comparison of our proposed method for isotropic and anisotropic regularization for the example of <mark>stereo matching</mark> discussed in the next section.<br>2: The advantage of using cyclographs is that a very reasonable object model may be obtained by running the <mark>stereo matching</mark> procedure only once, greatly simplifying reconstruction. Note that a reconstruction obtained in this manner may still contain holes-in particular, the top of the horse does not appear in the cyclograph and is thus omitted from the reconstruction.<br>",
    "Arabic": "مُطابَقَة المَجْسَم",
    "Chinese": "立体匹配",
    "French": "appariement stéréo",
    "Japanese": "ステレオマッチング",
    "Russian": "стереосопоставление"
  },
  {
    "English": "stereo pair",
    "context": "1: A <mark>stereo pair</mark> consists of two images with purely horizontal parallax, that is, every scene point visible in one image projects to a point in the same row of the other. We seek to characterize the space of all stereo images.<br>2: A <mark>stereo pair</mark> consists of two views with purely horizontal parallax, that is, every scene point visible in one view projects to a point in the same row of the other view. Images satisfying this property can be fused by human observers to produce a depth effect and are amenable to processing by computational stereo algorithms.<br>",
    "Arabic": "زوج مجسم",
    "Chinese": "立体对",
    "French": "paire stéréo",
    "Japanese": "ステレオペア",
    "Russian": "стереопара"
  },
  {
    "English": "stereo reconstruction",
    "context": "1: The main contribution of this paper is to introduce an effective optimization strategy for <mark>stereo reconstruction</mark> with triple cliques. This means that visibility reasoning and second-order priors can be combined for the first time. We show that this algorithm produces excellent results both on the Middlebury test set [27] and on real-world examples with curved surfaces.<br>2: almost all animals and many handcrafted objects). Assuming an object is perfectly symmetric, one can obtain a virtual second view of it by simply mirroring the image. In fact, if correspondences between the pair of mirrored images were available, 3D reconstruction could be achieved by <mark>stereo reconstruction</mark> [41,12,60,54,14].<br>",
    "Arabic": "إعادة بناء المجسم",
    "Chinese": "立体重建",
    "French": "reconstruction stéréo",
    "Japanese": "ステレオ再構成",
    "Russian": "стереореконструкция"
  },
  {
    "English": "stereo vision",
    "context": "1: The classical example of a stereo pair consists of two planar perspective views where the second view has been translated horizontally from the first 4 . The fact that two such images have horizontal parallax is important both for human perception and computational <mark>stereo vision</mark>.<br>",
    "Arabic": "رؤية مجسمة",
    "Chinese": "双目视觉",
    "French": "vision stéréoscopique",
    "Japanese": "ステレオビジョン",
    "Russian": "бинокулярное зрение"
  },
  {
    "English": "stochastic algorithm",
    "context": "1: Online <mark>stochastic algorithms</mark> for solving large multistage stochastic integer programs have attracted increasing interest in recent years. They are motivated by applications in which different types of requests arrive dynamically, and it is the role of the algorithm to decide which requests to serve and how.<br>",
    "Arabic": "الخوارزمية العشوائية",
    "Chinese": "随机算法",
    "French": "algorithme stochastique",
    "Japanese": "確率的アルゴリズム",
    "Russian": "стохастический алгоритм"
  },
  {
    "English": "stochastic approximation",
    "context": "1: To show its effectiveness, in Section 4.2, we design Algorithm 2, a practical deep-learning implementation of ATAC. Algorithm 2 is a two-timescale first-order algorithm based on <mark>stochastic approximation</mark>, and uses a novel Bellman error surrogate (called double-Q residual algorithm loss) for off-policy optimization stability.<br>2: \"; a functional approximation term that depends on the size of the hypothesis space; finally, a <mark>stochastic approximation</mark> term that captures the suboptimality of minimizingˆinstead of , and of making a single epoch on the provided dataset.<br>",
    "Arabic": "التقريب العشوائي",
    "Chinese": "随机逼近",
    "French": "approximation stochastique",
    "Japanese": "確率的近似法",
    "Russian": "стохастическое приближение"
  },
  {
    "English": "stochastic depth",
    "context": "1: Although the methods are ultimately quite different, the DenseNet interpretation of <mark>stochastic depth</mark> may provide insights into the success of this regularizer. Feature Reuse. By design, DenseNets allow layers access to feature-maps from all of its preceding layers (although sometimes through transition layers).<br>2: We also reduce the amount of regularization (<mark>stochastic depth</mark>) as our Monarch models are smaller than the dense models. We adopt the hyperparameters (optimizer, learning rate, learning rate scheduler) from Yuan et al. [107]. Details are in Table 9. We measure the wall-clock training time on V100 GPUs.<br>",
    "Arabic": "العمق الاحتمالي",
    "Chinese": "随机深度",
    "French": "profondeur stochastique",
    "Japanese": "確率的深さ",
    "Russian": "стохастическая глубина"
  },
  {
    "English": "stochastic differential equation",
    "context": "1: Without loss of generality, we assume that our goal is to model a time-invariant system whose dynamics are governed by a <mark>stochastic differential equation</mark>, discretized according to the the Euler-Maruyama integrator \n ∆s t = s t+1 − s t = f (s t , c t )∆t + √ ∆tΣε t ,(17) \n<br>2: (2018) use stochastic variational inference to recover the solution of a given <mark>stochastic differential equation</mark>. Differentiating through ODE solvers The dolfin library (Farrell et al., 2013) implements adjoint computation for general ODE and PDE solutions, but only by backpropagating through the individual operations of the forward solver.<br>",
    "Arabic": "معادلة تفاضلية عشوائية",
    "Chinese": "随机微分方程",
    "French": "équation différentielle stochastique",
    "Japanese": "確率微分方程式",
    "Russian": "стохастическое дифференциальное уравнение"
  },
  {
    "English": "stochastic dynamic",
    "context": "1: We now present a series of applications with increasing complexity of interaction: (1) control with <mark>stochastic dynamics</mark>; \n (2) multiple agent interaction; and (3) interaction with a partially observable system.<br>2: We choose <mark>stochastic dynamics</mark> because the Markov chain probability is guaranteed to converge to the posterior P(W | I). The complexity of the problem means that deterministic algorithms for implementing these moves risk getting stuck in local minima. 6.<br>",
    "Arabic": "ديناميكية عشوائية",
    "Chinese": "随机动力学",
    "French": "dynamique stochastique",
    "Japanese": "確率ダイナミクス",
    "Russian": "стохастическая динамика"
  },
  {
    "English": "stochastic environment",
    "context": "1: However, we argue that entropy-based approaches like this that do not address the causal influence of side information are inadequate for interactive settings. In the context of this paper, we focus on modeling the sequential actions of an agent interacting with a <mark>stochastic environment</mark>.<br>2: At the heart of our formulation lies a strategic game. One agent, representing the controller, desires to maximize the expected cumulative reward. In contrast, the other agent, severing as the adversary that embodies potential environmental shifts, selects the <mark>stochastic environment</mark> with the aim of diminishing this reward.<br>",
    "Arabic": "البيئة العشوائية",
    "Chinese": "随机环境",
    "French": "environnement stochastique",
    "Japanese": "確率的環境",
    "Russian": "стохастическая среда"
  },
  {
    "English": "stochastic game",
    "context": "1: In contrast, we focus primarily on unbounded horizons and memory-less strategies. The interaction between the principal and the agent can be viewed as a <mark>stochastic game</mark> (Shapley, 1953) where one player (i.e., the principal) has the power to make a strategy commitment (Letchford and Conitzer, 2010;Letchford et al., 2012).<br>2: Learning against an adaptive adversary in a <mark>stochastic game</mark> is a challenging problem, and there will be many ideas to explore in combining the two different forms of information. That will likely be the key difference between a program that can compete with the best, and a program that surpasses all human players.<br>",
    "Arabic": "لعبة احتمالية",
    "Chinese": "随机博弈",
    "French": "jeu stochastique",
    "Japanese": "確率ゲーム",
    "Russian": "Стохастическая игра"
  },
  {
    "English": "stochastic gradient",
    "context": "1: It can be interpreted as a <mark>stochastic gradient</mark> update for the minimization of the generalized margin loss (LeCun et al., 2007, §2.2.3), with a step size adjusted according to the curvature of the dual (Hildreth, 1957). Crammer and Singer (2003) use a very similar approach for the MIRA algorithm.<br>2: x = arg min x∈R d   f (x) = 1 n n i=1 E ξi∼Di f i (x; ξ i ) fi(x)    . (1) \n Here, ξ i is a data sample from D i and is used to compute a <mark>stochastic gradient</mark> via some oracle, e.g.<br>",
    "Arabic": "تدرج عشوائي",
    "Chinese": "随机梯度",
    "French": "gradient stochastique",
    "Japanese": "確率的勾配",
    "Russian": "стохастический градиент"
  },
  {
    "English": "stochastic gradient algorithm",
    "context": "1: Although there is no explicit gradient computation, algorithm 2 can be interpreted as a stochastic natural gradient algorithm [16,15]. We begin by deriving a related first-order <mark>stochastic gradient algorithm</mark> for LDA. Let g(n) denote the population distribution over documents n from which we will repeatedly sample documents: \n<br>",
    "Arabic": "ﺧﻮﺍﺮﺯﻣﻴﺔ ﺍﻟﺘﺪﺭﺝ ﺍﻟﻌﺸﻮﺍﺋﻲ",
    "Chinese": "随机梯度算法",
    "French": "algorithme de gradient stochastique",
    "Japanese": "ストカスティックグラディエントアルゴリズム",
    "Russian": "стохастический градиентный алгоритм"
  },
  {
    "English": "stochastic gradient ascent",
    "context": "1: Using <mark>stochastic gradient ascent</mark> from multiple starting points, we minimized (3) with respect to α, β, γ for each session separately by systematically varying the meta-parameters in the following ranges: α, γ ∈ [0.03, 0.99] and β ∈ [10 −1 , 10 1.5 ].<br>2: 2 has linear runtime in the vocabulary size n, we use lazy updates (Carpenter, 2008) for every k words during training. We call this the lazy method of MAP. The second technique applies <mark>stochastic gradient ascent</mark> to the log-likelihood, and after every k words applies the update in Eq. 1.<br>",
    "Arabic": "الصعود التدريجي العشوائي",
    "Chinese": "随机梯度上升法",
    "French": "ascension de gradient stochastique",
    "Japanese": "確率的勾配上昇法",
    "Russian": "стохастический градиентный подъем"
  },
  {
    "English": "stochastic gradient descent",
    "context": "1: With its small memory footprint, robustness against noise, and rapid learning rates, <mark>Stochastic Gradient Descent</mark> (SGD) has proved to be well suited to data-intensive machine learning tasks [3,5,26]. However, SGD's scalability is limited by its inherently sequential nature; it is difficult to parallelize.<br>2: The online setting systematically outperforms its batch counterpart for every training set size and desired precision. We use a logarithmic scale for the computation time, which shows that in many situations, the difference in performance can be dramatic. Similar experiments have given similar results on smaller datasets. Comparison with <mark>Stochastic Gradient Descent</mark>.<br>",
    "Arabic": "الانحدار التدريجي العشوائي",
    "Chinese": "随机梯度下降",
    "French": "descente de gradient stochastique",
    "Japanese": "確率的勾配降下法",
    "Russian": "стохастический градиентный спуск"
  },
  {
    "English": "stochastic gradient method",
    "context": "1: To achieve this flexibility, we build up on soft trees [Kontschieder et al., 2015, Hazimeh et al., 2020, which are differentiable trees that can be trained with first-order (stochastic) gradient methods. Previously, soft tree ensembles have been predominantly explored for classification tasks with cross-entropy loss.<br>2: Online methods: Online learning methods are very closely related to <mark>stochastic gradient methods</mark>, as they operate on only a single example at each iteration. Moreover, many online learning rules, including the Perceptron rule, can be seen as implementing a stochastic gradient step.<br>",
    "Arabic": "طريقة التدرج العشوائي",
    "Chinese": "随机梯度方法",
    "French": "méthode de gradient stochastique",
    "Japanese": "ストカスティック勾配法",
    "Russian": "метод стохастического градиента"
  },
  {
    "English": "stochastic grammar",
    "context": "1: Another research direction is to embed this framework within a hierarchical Bayesian model with random variation in the segment coefficients rather than keeping them fixed. The underlying semi-Markov segmental hidden Markov model for waveforms can also be generalized, allowing for further modeling flexibility such as <mark>stochastic grammars</mark>, hierarchical hidden Markov models, and so forth.<br>",
    "Arabic": "قواعد اللغة العشوائية",
    "Chinese": "随机语法",
    "French": "grammaire stochastique",
    "Japanese": "確率文法",
    "Russian": "стохастическая грамматика"
  },
  {
    "English": "stochastic matrix",
    "context": "1: (5.2) Thus, to prove Proposition 5.1, we only have to prove that the coefficient matrix I − cP ⊤ ⊗ P ⊤ is non-singular. Since P ⊤ ⊗ P ⊤ is a (left) <mark>stochastic matrix</mark>, its spectral radius is equal to one.<br>2: ( γ h + γ m ) v * − v ∞ . (18 \n ) \n Where the first relation holds since v * is the fixed point of T m , the second relation holds by the definition of the optimal Bellman operator, and the forth relation holds since (P π h ) m is a <mark>stochastic matrix</mark>.<br>",
    "Arabic": "المصفوفة الاحتمالية",
    "Chinese": "随机矩阵",
    "French": "matrice stochastique",
    "Japanese": "確率行列",
    "Russian": "стохастическая матрица"
  },
  {
    "English": "stochastic model",
    "context": "1: The first thing that a user must do is to postulate an appropriate <mark>stochastic model</mark> to describe the data generation process within each cell. A cell is a minimum spatial unit within which we assume that there is no spatial variation, and so it does not make sense to subdivide a cell spatially when performing anomaly detection.<br>",
    "Arabic": "النموذج العشوائي",
    "Chinese": "随机模型",
    "French": "modèle stochastique",
    "Japanese": "確率モデル",
    "Russian": "стохастическая модель"
  },
  {
    "English": "stochastic objective",
    "context": "1: The learner's problem is to minimize a <mark>stochastic objective</mark> f (x) := E[ (x; ζ)] over a data distribution p(ζ), where ζ denotes a source of data and x ∈ X ⊂ R nx denotes the learner's parameters.<br>2: Under Assumption 2.1 and Assumption 2.2, the ZoBG is an unbiased estimator of the <mark>stochastic objective</mark>. E[∇ [0] F (θ)] = ∇F (θ). In contrast, the FoBG requires strong continuity conditions in order to satisfy the requirement for unbiasedness. However, under Lipschitz continuity, it is indeed unbiased.<br>",
    "Arabic": "هدف عشوائي",
    "Chinese": "随机目标函数",
    "French": "objectif stochastique",
    "Japanese": "確率的目的関数",
    "Russian": "стохастическая цель"
  },
  {
    "English": "stochastic optimization",
    "context": "1: [81] Zhen Fang, Jie Lu, Feng Liu, and Guangquan Zhang. Semi-supervised heterogeneous domain adaptation: Theory and algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. [82] Diederik P. Kingma and Jimmy Ba. Adam: A method for <mark>stochastic optimization</mark>. In ICLR, 2015. [<br>2: Our goal in this section is thus to provide more understanding of potential poor behavior of the procedure (6) with respect to ERM, considering two scenarios. The first is in stochastic (convex) optimization problems, where we investigate the finite-sample convergence rates of the robust solution to the population optimal risk.<br>",
    "Arabic": "تحسين عشوائي",
    "Chinese": "随机优化",
    "French": "optimisation stochastique",
    "Japanese": "確率的最適化",
    "Russian": "стохастическая оптимизация"
  },
  {
    "English": "stochastic policy",
    "context": "1: The joint <mark>stochastic policy</mark> will give the probability of a vector of actions p(y|x; θ). The observations available to agent/node i are represented by the sufficient statistics φ i (x, y i ).<br>2: Compared to the <mark>stochastic policy</mark> π 1 , we treat π 2 deterministically, i.e. a 2 = π 2 (s) since π 2 is differentiable with respect to the environment, such that its gradient can be precisely estimated.<br>",
    "Arabic": "السياسة العشوائية",
    "Chinese": "随机策略",
    "French": "politique stochastique",
    "Japanese": "確率的方策",
    "Russian": "стохастическая политика"
  },
  {
    "English": "stochastic process",
    "context": "1: Several of these results suggest modeling the race conditions inherent in HOGWILD! SGD as noise in a <mark>stochastic process</mark>; this lets them bring a trove of statistical techniques to bear on the analysis of HOGWILD! SGD. Therefore, in this paper, we will apply a similar <mark>stochastic process</mark> model to Gibbs sampling.<br>2: More precisely, it can define φ so that φ(v i )φ(v j ) is small whenever v i , v j are similar. 3 But it cannot actively encourage dispersion: \n 2 A point process is a specific kind of <mark>stochastic process</mark>, which is the technical term for a distribution over functions.<br>",
    "Arabic": "عملية عشوائية",
    "Chinese": "随机过程",
    "French": "processus stochastique",
    "Japanese": "確率過程",
    "Russian": "стохастический процесс"
  },
  {
    "English": "stochastic sampling",
    "context": "1: This raises a general concern that using <mark>stochastic sampling</mark> as the primary means of evaluating model improvements may inadvertently end up influencing the design choices related to model architecture and training.<br>2: E Further results with <mark>stochastic sampling</mark> E.1 Image degradation due to excessive stochastic iteration Figure 14 illustrates the image degradation caused by excessive Langevin iteration (Section 4, \"Practical considerations\").<br>",
    "Arabic": "أخذ العينات العشوائية",
    "Chinese": "随机采样",
    "French": "échantillonnage stochastique",
    "Japanese": "確率的サンプリング",
    "Russian": "стохастическое сэмплирование"
  },
  {
    "English": "stochastic search algorithm",
    "context": "1: Particle Swarm Optimization (PSO) [11,12] is a <mark>stochastic search algorithm</mark>, which aims to identify the global optimum of an objective function without requiring any gradient rule. In PSO, a swarm of simple agents (particles) is initialized in the multidimensional problem space with random positions X i and velocities V i .<br>",
    "Arabic": "خوارزمية البحث الاحتمالية",
    "Chinese": "随机搜索算法",
    "French": "algorithme de recherche stochastique",
    "Japanese": "確率的探索アルゴリズム",
    "Russian": "стохастический поисковый алгоритм"
  },
  {
    "English": "stochastic subgradient descent",
    "context": "1: A stochastic version of the algorithm is also possible by considering stochastic oracles on each f i and using <mark>stochastic subgradient descent</mark> instead of the subgradient method. Remark 5. In the more general context where node compute times ρ i are not necessarily all equal to 1, we may still apply Alg.<br>",
    "Arabic": "هبوط التدرج العشوائي الجزئي",
    "Chinese": "随机次梯度下降",
    "French": "descente de sous-gradient stochastique",
    "Japanese": "確率的部分勾配降下法",
    "Russian": "стохастический субградиентный спуск"
  },
  {
    "English": "stochastic transition matrix",
    "context": "1: A Markov model is usually represented by a <mark>stochastic transition matrix</mark> P with elements p i, j = P(s j |s i ) which describe the probability of transitioning from state s i to state s j ; the probabilities of each row sum to 1.<br>",
    "Arabic": "مصفوفة الانتقال العشوائية",
    "Chinese": "随机转移矩阵",
    "French": "matrice de transition stochastique",
    "Japanese": "確率遷移行列",
    "Russian": "стохастическая матрица перехода"
  },
  {
    "English": "stochastic variational inference",
    "context": "1: (2018) use <mark>stochastic variational inference</mark> to recover the solution of a given stochastic differential equation. Differentiating through ODE solvers The dolfin library (Farrell et al., 2013) implements adjoint computation for general ODE and PDE solutions, but only by backpropagating through the individual operations of the forward solver.<br>2: Stochastic variational inference [11] requires an analogous sampling step. The main difference being that rather than using n tw +βw n t +β to capture p(w|t) one uses a natural parameter ηtw associated with the conjugate variational distribution.<br>",
    "Arabic": "الاستدلال التبايني العشوائي",
    "Chinese": "随机变分推断",
    "French": "Inférence variationnelle stochastique",
    "Japanese": "確率的変分推論",
    "Russian": "стохастический вариационный вывод"
  },
  {
    "English": "stochasticity",
    "context": "1: While our results showcase the potential gains achievable through sampler improvements, they also highlight the main shortcoming of <mark>stochasticity</mark>: For best results, one must make several heuristic choices -either implicit or explicit -that depend on the specific model.<br>2: This randomness can arise from <mark>stochasticity</mark> in the task, exploratory choices made during learning, randomized initial parameters, but also software and hardware considerations such as non-determinism in GPUs and in machine learning frameworks [116]. Thus, we model the algorithm's normalized score on the m th task as a real-valued random variable X m .<br>",
    "Arabic": "عشوائية",
    "Chinese": "随机性",
    "French": "stochasticité",
    "Japanese": "確率性",
    "Russian": "стохастичность"
  },
  {
    "English": "stop word",
    "context": "1: Under the limited information provided in a bibliographic database, only \"paper title\" best describes the content covered in a paper. Therefore, we perform the following processing to extract the potential topic terms. First, each paper title is processed by the basic text processing steps, including removing <mark>stop words</mark> and stemming.<br>",
    "Arabic": "كلمات توقف",
    "Chinese": "停用词",
    "French": "mot vide",
    "Japanese": "停止語",
    "Russian": "стоп-слово"
  },
  {
    "English": "stop-gradient",
    "context": "1: This kNN classifier can serve as a monitor of the progress. With <mark>stop-gradient</mark>, the kNN monitor shows a steadily improving accuracy. The linear evaluation result is in the table in Figure 2. SimSiam achieves a nontrivial accuracy of 67.7%. This result is reasonably stable as shown by the std of 5 trials.<br>2: Note that SwAV applies <mark>stop-gradient</mark> on the SK transform, so we ablate by removing it. Here is the comparison on our SwAV reproduction: an alternating formulation [7]. This may explain why stopgradient should not be removed from SwAV. Relation to BYOL [15].<br>",
    "Arabic": "توقف تدرج",
    "Chinese": "停止梯度传播",
    "French": "stop-gradient",
    "Japanese": "勾配停止",
    "Russian": "стоп-градиент"
  },
  {
    "English": "stop-gradient operation",
    "context": "1: We have empirically shown that in a variety of settings, SimSiam can produce meaningful results without collapsing. The optimizer (batch size), batch normalization, similarity function, and symmetrization may affect accuracy, but we have seen no evidence that they are related to collapse prevention. It is mainly the <mark>stop-gradient operation</mark> that plays an essential role.<br>",
    "Arabic": "عملية إيقاف التدرج",
    "Chinese": "停止梯度操作",
    "French": "opération de stop-gradient",
    "Japanese": "勾配停止操作",
    "Russian": "операция остановки градиента"
  },
  {
    "English": "stopping condition",
    "context": "1: (1) <mark>stopping condition</mark> (line 4): For the moment, we will use a weak <mark>stopping condition</mark>: MM will terminate search as soon as U ≤ C. This simplifies the proofs of MM's key properties.<br>2: In Section 5.2 we will replace this <mark>stopping condition</mark> with the stronger <mark>stopping condition</mark> used in Algorithm 1 and show that MM maintains all its key properties when the stronger <mark>stopping condition</mark> is used.<br>",
    "Arabic": "شرط التوقف",
    "Chinese": "终止条件",
    "French": "condition d'arrêt",
    "Japanese": "停止条件",
    "Russian": "условие остановки"
  },
  {
    "English": "stopping criterion",
    "context": "1: The main goal of small-text is to offer state-ofthe-art active learning for text classification in a convenient and robust way for both researchers and practitioners. For this purpose, we implemented a modular pool-based active learning mechanism, illustrated in Figure 2, which exposes interfaces for classifiers, query strategies, and <mark>stopping criteria</mark>.<br>2: The actual active learning loop consists of just the previous code block and changing hyperparameters, e.g., using a different query strategy, is as easy as adapting the query_strategy variable. In Table 1 , we compare small-text to the previously mentioned libraries , and compare them based on several criteria related to active learning or to the respective code base : While all libraries provide a selection of query strategies , not all li-braries offer <mark>stopping criteria</mark> , which are crucial to reducing the total annotation effort and thus directly influence the efficiency of<br>",
    "Arabic": "معيار التوقف",
    "Chinese": "停止准则",
    "French": "critère d'arrêt",
    "Japanese": "停止基準",
    "Russian": "критерий остановки"
  },
  {
    "English": "story cloze test",
    "context": "1: It concerns a broader commonsense setting, which includes events, descriptions, assertion etc. Some datasets are inspired by reading comprehension. The <mark>Story Cloze Test</mark> and ROCStories Corpora (Mostafazadeh et al., 2016;Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story.<br>2: The <mark>Story Cloze Test</mark> (also referred to as ROC Stories) pits ground-truth endings to stories against implausible false ones (Mostafazadeh et al., 2016). Interpolating these approaches, Situations with Adversarial Generations (SWAG), asks models to choose the correct description of what happens next after an initial event (Zellers et al., 2018b).<br>",
    "Arabic": "اختبار إغلاق القصة",
    "Chinese": "故事闭合测试",
    "French": "test de clôture de l'histoire",
    "Japanese": "ストーリー空所補充テスト",
    "Russian": "тест завершения истории"
  },
  {
    "English": "stratified sampling",
    "context": "1: When sampling on each ray, we use a <mark>stratified sampling</mark> strategy and sample K = 32 points on each ray between the near and far depth range. Additionally, when mapping a 3D location from one local volume to another, we encourage it to be mapped within our predefined depth range to avoid degenerate solutions.<br>2: [30], our coarse samples t c are produced with <mark>stratified sampling</mark>, and our fine samples t f are sampled from the resulting alpha compositing weights w using inverse transform sampling.<br>",
    "Arabic": "اخذ العينات الطبقية",
    "Chinese": "分层抽样",
    "French": "échantillonnage stratifié",
    "Japanese": "層化サンプリング",
    "Russian": "выборка по стратам"
  },
  {
    "English": "streaming algorithm",
    "context": "1: When to read new input and write new output is a fundamental question for the <mark>streaming algorithm</mark>. Based on the choices of streaming read/write policies, they can roughly be separated into two families: pre-fixed and adaptive.<br>2: This paper almost settles the above question. Our main contributions are summarized as follows. 1. We give a new space-optimal <mark>streaming algorithm</mark> with O(ndk) +Õ(dα −3 ) running time to compute (α, k)-cov-sketches for dense matrices, which improves the original FD algorithm for small α.<br>",
    "Arabic": "خوارزمية البث",
    "Chinese": "数据流算法",
    "French": "algorithme de streaming",
    "Japanese": "ストリーミングアルゴリズム",
    "Russian": "потоковый алгоритм"
  },
  {
    "English": "streaming datum",
    "context": "1: Moreover, unlike Method (i), it is impossible to compute such factorizations exactly for <mark>streaming data</mark> Clarkson and Woodruff (2009).<br>",
    "Arabic": "البيانات المتدفقة",
    "Chinese": "流式数据",
    "French": "données en flux",
    "Japanese": "ストリーミングデータ",
    "Russian": "потоковые данные"
  },
  {
    "English": "streaming model",
    "context": "1: Specifically, we prove that, under mild assumptions, the time complexity for computing an (O(1), k)-cov-sketch B ∈ R O(k)×d of A in the <mark>streaming model</mark> is equivalent to the time complexity of left multiplying A by an arbitrary matrix C ∈ R k×n .<br>2: We consider the problems in the <mark>streaming model</mark>, where the algorithm can only make one pass over the input with limited working space, and we are interested in minimizing the covariance error A T A − B T B 2 . The popular Frequent Directions algorithm of Liberty ( 2013) and its variants achieve optimal space-error tradeoffs.<br>",
    "Arabic": "النموذج التدفقي",
    "Chinese": "流式模型",
    "French": "modèle de flux",
    "Japanese": "ストリーミングモデル",
    "Russian": "потоковая модель"
  },
  {
    "English": "stride",
    "context": "1: For simplicity, we only explored running detection at fixed <mark>stride</mark>s of 2, 5, 15, and 30. For example, <mark>stride</mark> 30 means that we run the detector once and then run the tracker 29 times, with the tracker getting reset after each new detection.<br>2: First, because we are expecting meaningful side outputs with different scales, a layer with <mark>stride</mark> 32 yields a too-small output plane with the consequence that the interpolated prediction map will be too fuzzy to utilize.<br>",
    "Arabic": "خطوة",
    "Chinese": "步幅",
    "French": "pas",
    "Japanese": "ストライド",
    "Russian": "шаг"
  },
  {
    "English": "string kernel metric",
    "context": "1: Both these functions are defined in terms of a similarity matrix S = {s ij } i,j∈V , which we define on the TIMIT corpus [9], using the <mark>string kernel metric</mark> [41] for similarity.<br>",
    "Arabic": "مقياس نواة السلسلة",
    "Chinese": "字符串核度量",
    "French": "noyau de chaînes de caractères",
    "Japanese": "文字列カーネル尺度",
    "Russian": "метрика строкового ядра"
  },
  {
    "English": "structural learning",
    "context": "1: A number of semi-supervised techniques have been introduced to tackle this problem, such as bootstrapping (Yarowsky 1995;Collins and Singer 1999;Riloff and Jones 1999), multi-view learning (Blum and Mitchell 1998;Ganchev et al. 2008) and <mark>structural learning</mark> (Ando and Zhang 2005).<br>",
    "Arabic": "التعلم البنيوي",
    "Chinese": "结构学习",
    "French": "apprentissage structurel",
    "Japanese": "構造学習",
    "Russian": "структурное обучение"
  },
  {
    "English": "structural risk minimization",
    "context": "1: MacKay (2003) uses the Laplace approximation to make connections between the marginal likelihood and the minimum description length framework. MacKay (1995) also notes that <mark>structural risk minimization</mark> (Guyon et al., 1992) has the same scaling behaviour as the marginal likelihood.<br>",
    "Arabic": "تقليل المخاطر الهيكلية",
    "Chinese": "结构风险最小化",
    "French": "minimisation du risque structurel",
    "Japanese": "構造リスク最小化",
    "Russian": "минимизация структурного риска"
  },
  {
    "English": "structure from motion",
    "context": "1: <mark>Structure from motion</mark> (SfM) techniques have recently been used to build 3D models from unstructured and unconstrained image collections, including images downloaded from Internet photo-sharing sites such as Flickr [1,6,11,25].<br>",
    "Arabic": "البنية من الحركة",
    "Chinese": "运动重建结构",
    "French": "structure à partir du mouvement",
    "Japanese": "動きからの構造",
    "Russian": "Структура по движению"
  },
  {
    "English": "structure learning",
    "context": "1: Given the training images of HOI activities with labeled objects and body parts, the learning step needs to achieve two goals: <mark>structure learning</mark> to discover the hidden human poses and the connectivity among the object, human pose, and body parts; and parameter estimation for the potential weights to maximize the discrimination between different activities.<br>2: In this work, we treat object and human pose as the context of each other in different HOI activity classes. We develop a random field model that uses a <mark>structure learning</mark> method to learn important connectivity patterns between objects and human body parts. Experiments show that our model significantly outperforms other state-of-the-art methods in both problems.<br>",
    "Arabic": "تعلم البنية",
    "Chinese": "结构学习",
    "French": "apprentissage de la structure",
    "Japanese": "構造学習",
    "Russian": "структурное обучение"
  },
  {
    "English": "structured datum",
    "context": "1: These statistics are very powerful when available. Structured data Some systems assume access to <mark>structured data</mark> such as relationship tuples (e.g., (Barack Obama, Spouse, Michelle Obama)) or a type hierarchy to aid disambiguation.<br>",
    "Arabic": "بيانات منظمة",
    "Chinese": "结构化数据",
    "French": "donnée structurée",
    "Japanese": "構造化されたデータ",
    "Russian": "структурированные данные"
  },
  {
    "English": "structured output",
    "context": "1: where f θ (y, i) is a scoring function based on a model θ learned from the training data. The <mark>structured output</mark> y and the scoring function f θ (y, i) can be decomposed into small components based on an independence assumption.<br>2: More generally, we consider the <mark>structured output</mark> case where y ∈ Y m (m nodes).<br>",
    "Arabic": "الناتج المنظم",
    "Chinese": "结构化输出",
    "French": "sortie structurée",
    "Japanese": "構造化された出力",
    "Russian": "структурированный вывод"
  },
  {
    "English": "structured perceptron",
    "context": "1: ∂ log M ∂w i = 1 M ∂M ∂w i = ci w j ∈W e c j •w j w j ∈W e c j •w j = ci \n This means that the hard gradient update for weights in logspace is ∆w i = ∆c i , which resembles <mark>structured perceptron</mark> [13].<br>2: For the final pass, we want to train the model for 1best output. Several different learning methods are available for structured prediction models including <mark>structured perceptron</mark> (Collins, 2002), max-margin models (Taskar et al., 2003), and log-linear models (Lafferty et al., 2001).<br>",
    "Arabic": "المقترن المنظم",
    "Chinese": "结构化感知器",
    "French": "perceptron structuré",
    "Japanese": "構造化パーセプトロン",
    "Russian": "структурированный перцептрон"
  },
  {
    "English": "structured prediction",
    "context": "1: The literature on <mark>structured prediction</mark> for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for <mark>structured prediction</mark> designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks.<br>2: Their use of error states is related to our global model that penalizes local scores. We demonstrated that best-first search is infeasible in our setting, due to the larger search space. A close integration of learning and decoding has been shown to be beneficial for <mark>structured prediction</mark>.<br>",
    "Arabic": "التنبؤ المنظم",
    "Chinese": "结构化预测",
    "French": "prédiction structurée",
    "Japanese": "構造予測",
    "Russian": "структурированное предсказание"
  },
  {
    "English": "structured prediction model",
    "context": "1: In this section, we introduce Reducing Bias Amplification, RBA, a debiasing technique for calibrating the predictions from a <mark>structured prediction model</mark>. The intuition behind the algorithm is to inject constraints to ensure the model predictions follow the distribution observed from the training data.<br>2: (2005) proposed a technique that uses graph based similarity between labeled and unlabeled parts of structured data in a discriminative framework for semi-supervised learning. More recently, Subramanya et al. (2010) defined a graph over the cliques in an underlying <mark>structured prediction model</mark>.<br>",
    "Arabic": "نموذج التنبؤ المهيكل",
    "Chinese": "结构化预测模型",
    "French": "modèle de prédiction structurée",
    "Japanese": "構造化予測モデル",
    "Russian": "модель структурированного прогнозирования"
  },
  {
    "English": "structured prediction problem",
    "context": "1: We thus formulate a <mark>structured prediction problem</mark> f : R height×width×3 N → R N , and treat each training instances x i as a vector of images, x, being mapped to a sequence of predictions, y. We work with a previously collected data set where we observed that the constant velocity assumption approximately holds.<br>2: We will train this network as a <mark>structured prediction problem</mark> operating on a sequence of N images to produce a sequence of N heights, R height×width×3 N → R N , and each piece of data x i will be a vector of images, x.<br>",
    "Arabic": "\"مشكلة التنبؤ المهيكل\"",
    "Chinese": "结构化预测问题",
    "French": "problème de prédiction structurée",
    "Japanese": "構造化予測問題",
    "Russian": "структурная задача предсказания"
  },
  {
    "English": "structured support vector machine",
    "context": "1: UpdateParameters(w i , g i ) i \n i + 1 end while return w N This is the familiar structured hinge loss function as in <mark>structured support vector machines</mark> (Tsochantaridis et al., 2004), which has a minimum at 0 if and only if class y is ranked ahead of all other classes by at least m. \n<br>",
    "Arabic": "آلة ناقلات الدعم المنظم",
    "Chinese": "结构化支持向量机",
    "French": "machine à vecteurs de support structuré",
    "Japanese": "構造化サポートベクターマシン",
    "Russian": "структурированная машина опорных векторов"
  },
  {
    "English": "student model",
    "context": "1: (5) Ablation study shows that although performing better, larger <mark>student model</mark>s are more prone to being inconsistent. Our method robustly remedies the inconsistency regardless of the size of the <mark>student model</mark>.<br>2: We ablate the <mark>student model</mark> size to see how its faithfulness and performance are affected. From Figure 7, we observe that larger <mark>student model</mark>s achieve higher performance but lower faithfulness.<br>",
    "Arabic": "نموذج الطالب",
    "Chinese": "学生模型",
    "French": "modèle d'élève",
    "Japanese": "生徒モデル",
    "Russian": "модель студента"
  },
  {
    "English": "style transfer",
    "context": "1: Through a combination of automatic and human evaluation, we demonstrate that Partner successfully generates more empathic, specific, and diverse responses and outperforms NLP methods from related tasks such as <mark>style transfer</mark> and empathic dialogue generation. This work has direct implications for facilitating empathic conversations on web-based platforms.<br>2: We show the overall performance and individual metrics results in Table 3. In terms of overall performance, Story-Trans outperforms baselines, illustrating that Sto-ryTrans can achieve a better balance between <mark>style transfer</mark> and content preservation. In terms of style accuracy, StoryTrans achieves the best <mark>style transfer</mark> accuracy (a-Acc) in LX and comparable performance in JY.<br>",
    "Arabic": "نقل الأسلوب",
    "Chinese": "风格迁移",
    "French": "transfert de style",
    "Japanese": "スタイル転写",
    "Russian": "перенос стиля"
  },
  {
    "English": "sub-gradient",
    "context": "1: To remind the reader, given a convex function f (w), a <mark>sub-gradient</mark> of f at w 0 is a vector v which satisfies: \n ∀w, f (w) − f (w 0 ) ≥ v, w − w 0 .<br>2: We consider the <mark>sub-gradient</mark> of the above approximate objective, given by: \n ∇ t = λ w t − ½[y i t w t , x i t < 1] y i t x i t ,(4) \n<br>",
    "Arabic": "التدرج الفرعي",
    "Chinese": "次梯度",
    "French": "sous-gradient",
    "Japanese": "サブグラディエント",
    "Russian": "субградиент"
  },
  {
    "English": "sub-gradient descent",
    "context": "1: More recently, (Weinberger et al., 2005) formulate the metric learning problem in a large margin setting, with a focus on k-NN classification. They also formulate the problem as a semidefinite programming problem and consequently solve it using a combination of <mark>sub-gradient descent</mark> and alternating projections.<br>",
    "Arabic": "الانحدار الفرعي",
    "Chinese": "次梯度下降",
    "French": "descente par sous-gradient",
    "Japanese": "部分勾配降下法",
    "Russian": "подградиентный спуск"
  },
  {
    "English": "sub-networks",
    "context": "1: [32] found that subnetworks reach full accuracy only if they are stable against SGD noise during training; Orseau et al. [78] provides a logarithmic upper bound for the number of parameters it takes for the optimal <mark>sub-networks</mark> to exist; Pensia et al.<br>",
    "Arabic": "شبكات فرعية",
    "Chinese": "子网络",
    "French": "sous-réseaux",
    "Japanese": "サブネットワーク",
    "Russian": "подсети"
  },
  {
    "English": "sub-population",
    "context": "1: Table 5 shows that a small sampling rate for negative examples (0 or 0.2) combined with a large view-only sampling rate (0.5 or 1) yields superior results, which confirms the argument about different information contents in <mark>sub-populations</mark>.<br>",
    "Arabic": "تحت-مجتمعات",
    "Chinese": "子群体",
    "French": "sous-population",
    "Japanese": "サブ集団",
    "Russian": "Подпопуляция"
  },
  {
    "English": "sub-word",
    "context": "1: This approach is used by Mohseni and Tebbifakhr (2019). One problem with this method is that mixing <mark>sub-word</mark> information and sentencelevel tokens in a single sequence does not encourage the model to learn the actual morphological compositionality and express word-relative syntactic regularities. We address these issues by proposing a simple yet effective two-tier transformer encoder architecture.<br>2: This requires an alternative to the ubiquitous BPE tokenization, through which exact <mark>sub-word</mark> lexical units (i.e. morphemes) are used.<br>",
    "Arabic": "كلمة مجزأة",
    "Chinese": "亚词",
    "French": "sous-mot",
    "Japanese": "部分語",
    "Russian": "подслово"
  },
  {
    "English": "sub-word tokenization",
    "context": "1: While these techniques have been widely used in language modeling and machine translation, they are not optimal for morphologically rich languages (Klein and Tsarfaty, 2020). In fact, <mark>sub-word tokenization</mark> methods that are solely based on surface forms, including BPE and character-based models, cannot capture all morphological details.<br>2: Differently to the previous works (see Table 15 in Appendix) which solely pretrained unmodified BERT models, we propose an improved BERT architecture for morphologically rich languages. Recently, there has been a research push to improve <mark>sub-word tokenization</mark> by adopting characterbased models (Ma et al., 2020;Clark et al., 2022).<br>",
    "Arabic": "تقطيع الكلمات الفرعية",
    "Chinese": "子词分词",
    "French": "tokenisation sous-lexicale",
    "Japanese": "部分語トークン化",
    "Russian": "субсловная токенизация"
  },
  {
    "English": "subgame",
    "context": "1: That defines a probability distribution over the nodes at the root of the <mark>subgame</mark> S, representing the probability that the true game state matches that node. A strategy for the <mark>subgame</mark> is then calculated which assumes that this distribution is correct.<br>2: In heavily abstracted games, a blueprint may be far from the true solution. Subgame solving attempts to improve upon the blueprint by solving in real time a more fine-grained abstraction for an encountered <mark>subgame</mark>, while fitting its solution within the overarching blueprint.<br>",
    "Arabic": "لعبة فرعية",
    "Chinese": "子博弈",
    "French": "sous-jeu",
    "Japanese": "部分ゲーム",
    "Russian": "подигра"
  },
  {
    "English": "subgradient method",
    "context": "1: , n} ,(23) \n The step (b ′ ) still requires a proximal step for each function f i . We approximate it by the outcome of the <mark>subgradient method</mark> run for M steps, with a step-size proportional to 2/(m + 2) as suggested in [23].<br>2: With an appropriate choice of the step sizes α k , the <mark>subgradient method</mark> can be shown to solve the dual problem, i.e. lim k→∞ L(u (k) ) = min u L(u). See Korte and Vygen (2008), page 120, for details.<br>",
    "Arabic": "طريقة التدرج الجزئي",
    "Chinese": "次梯度法",
    "French": "méthode du sous-gradient",
    "Japanese": "勾配法",
    "Russian": "метод субградиента"
  },
  {
    "English": "subgraph isomorphism",
    "context": "1: Another example of augmented GNN architectures are the Graph Substructure Networks (GSNs) (Bouritsas et al., 2020). By contrast to F -MPNNs, <mark>subgraph isomorphism</mark> counts rather than homomorphism counts are used to augment the initial features.<br>2: But for simplicity we will only mention subgraphs. The definition of <mark>subgraph isomorphism</mark> can be readily used to define subtree isomorphism on tree data structures , even though definitions of trees usually need to take into account several factors : rooted or unrooted ( called free trees ) ; ordered or unordered ( the order of sibling nodes is not important ) ; labeled ( edge-labeled or node-labeled or both ) or<br>",
    "Arabic": "تطابق الرسم البياني الفرعي",
    "Chinese": "子图同构",
    "French": "isomorphisme de sous-graphe",
    "Japanese": "部分グラフ同型性",
    "Russian": "изоморфизм подграфов"
  },
  {
    "English": "subgraph selection",
    "context": "1: We propose a new task, online semantic parsing, with an accompanying formal evaluation metric, final latency reduction. We show that it is possible to reduce latency by 30%-63% using a strong graphbased semantic parser-either trained to parse prefixes directly or combined with a pre-trained language model for utterance completion-followed by a simple heuristic for <mark>subgraph selection</mark>.<br>",
    "Arabic": "اختيار الرسم البياني الفرعي",
    "Chinese": "子图选择",
    "French": "sélection de sous-graphes",
    "Japanese": "サブグラフ選択",
    "Russian": "выбор подграфа"
  },
  {
    "English": "submatrice",
    "context": "1: Particular contractions of the tensor that will be needed later will appear as 3 × 3 matrices that can be extracted from the tensor either as particular <mark>submatrices</mark> and linear combinations thereof or as 3 × 3 reshapings of its three columns and linear combinations thereof. To be specific, we will be using the following notation: \n<br>2: The order preserving sub matrix algorithm (OPSM) looks for <mark>submatrices</mark> in which the expression levels of all the genes induce the same linear ordering of the experiments. This algorithm although very accurate, is designed to identify only a single co-cluster.<br>",
    "Arabic": "مصفوفة جزئية",
    "Chinese": "子矩阵",
    "French": "sous-matrice",
    "Japanese": "部分行列",
    "Russian": "подматрица"
  },
  {
    "English": "submatrix",
    "context": "1: The matrix product AH in ( 13) is then seen <mark>submatrix</mark> by <mark>submatrix</mark>, and the operator trace ! ðÞ returns the vector in IR n , corresponding to the trace of each <mark>submatrix</mark> in the resulting vector of matrices.<br>2: Let Ω be <mark>submatrix</mark> of the aligned-SNR matrix Ω corresponding to the non-zero singular values i.e. Ω = diag {ω j } C−1 c=1 . After taking simultaneous-diagonalizations and canceling partial orthogonal matrices, Equation 19 can be written solely in terms of Ω: \n<br>",
    "Arabic": "مصفوفة فرعية",
    "Chinese": "子矩阵",
    "French": "sous-matrice",
    "Japanese": "部分行列",
    "Russian": "подматрица"
  },
  {
    "English": "submodular",
    "context": "1: The final component of the algorithm to be defined is the choice of proposals. In previous work [22,33], the proposals have just been fronto-parallel planes (denoted \"Same-Uni\" below). As shown in [8], repeated fusion of these proposals leads to a strong local optimum in the <mark>submodular</mark> case.<br>2: Furthermore, I ↑ (S) is <mark>submodular</mark> because it is a sum of <mark>submodular</mark> functions. To ease our presentation, we define the marginal influence I ↑ (•) of adding S 2 into S 1 as below: Step 1 \n<br>",
    "Arabic": "تحت النموذجية",
    "Chinese": "次调和的",
    "French": "sous-modulaire",
    "Japanese": "サブモジュラー",
    "Russian": "субмодулярный"
  },
  {
    "English": "submodular function",
    "context": "1: Machine Translation: Another application in machine translation is to choose a subset of training data that is optimized for given test data set, a problem previously addressed with modular functions [35]. Defining a <mark>submodular function</mark> with ground set over the union of training and test sample inputs V = V tr ∪ V te , we can set f : 2 Vtr → R + to f ( X ) = f ( X|V te ) , and take g ( X ) = |X| , and b ≈ 0 in Problem 2 to address<br>2: When running Algorithm RP with X 0 = ∅, it holds after one iteration that E(f (X 1 )) ≥ 1 4 f (X * ) if f is a general non-negative <mark>submodular function</mark>, and E(f (X 1 )) ≥ 1 2 f (X * ) if f is symmetric.<br>",
    "Arabic": "دالة تحت متجمعة",
    "Chinese": "子模函数",
    "French": "fonction sous-modulaire",
    "Japanese": "サブモジュラー関数",
    "Russian": "субмодулярная функция"
  },
  {
    "English": "submodular function optimization",
    "context": "1: While general set function optimization is often intractable, many forms of <mark>submodular function optimization</mark> can be solved near optimally or even optimally in certain cases, and hence submodularity is also often called the discrete analog of convexity [34].<br>2: We present a practical and powerful new framework for both unconstrained and constrained <mark>submodular function optimization</mark> based on discrete semidifferentials (sub-and super-differentials). The resulting algorithms, which repeatedly compute and then efficiently optimize submodular semigradients, offer new and generalize many old methods for submodular optimization.<br>",
    "Arabic": "تحسين دالة دون مودولية",
    "Chinese": "次模函数优化",
    "French": "optimisation de fonctions sousmodulaires",
    "Japanese": "部分モジュラー関数の最適化",
    "Russian": "оптимизация подмодульной функции"
  },
  {
    "English": "submodular influence function",
    "context": "1: S ⊆ T . We say that a process satisfying these conditions is an instance of the Decreasing Cascade Model. Although there are natural Decreasing Cascade instances that have no equivalent formulation in terms of triggering sets, we can show by a more intricate analysis that every instance of the Decreasing Cascade Model has a <mark>submodular influence function</mark>.<br>2: Recall Section 2, our work and [32] are developed based on different influence models; LazyProbe can only work with a <mark>submodular influence function</mark>. Although it is not fair for our methods to be compared with a submodular influence model, we still compare our method with LazyProbe in Section 6.6, while we neglect it in other experiments.<br>",
    "Arabic": "وظيفة التأثير تحت المعيارية",
    "Chinese": "子模影响函数",
    "French": "fonction d'influence sous-modulaire",
    "Japanese": "サブモジュラー影響関数",
    "Russian": "\"субмодулярная функция влияния\""
  },
  {
    "English": "submodular optimization",
    "context": "1: Totally normalized [4] and saturated functions like matroid rank have a curvature κ f = 1. A number of approximation guarantees in the context of <mark>submodular optimization</mark> have been refined via the curvature of the submodular function [3,18,17].<br>2: This paper shows, by contrast, that for <mark>submodular optimization</mark>, MM algorithms have strong theoretical properties and empirically work very well.<br>",
    "Arabic": "تحسين تحت المجموعي",
    "Chinese": "子模优化",
    "French": "optimisation sous-modulaire",
    "Japanese": "\"サブモジュラー最適化\"",
    "Russian": "оптимизация субмодулярных функций"
  },
  {
    "English": "submodular polyhedron",
    "context": "1: Figure 3 also shows the average results over 10 random choices of weights in both cases. In order to obtain accurate estimates of the timings, we run each experiment 5 times and take the minimum of these timing valuess. Constrained minimization. For constrained minimization , we compare MMin-I to two methods : a simple algorithm ( MU ) that minimizes the upper bound g ( X ) = i∈X f ( i ) [ 12 ] ( this is identical to the first iteration of MMin-I ) , and a more complex algorithm ( EA ) that computes an approximation to the <mark>submodular polyhedron</mark> [<br>2: Since this would solve submodular maximization (which is NP-hard), it must be NP-hard to find such a subgradient. To show that this holds for arbitrary X t (and correspondingly at every iteration), we use that the submodular subdifferential can be expressed as a direct product between a <mark>submodular polyhedron</mark> and an anti-<mark>submodular polyhedron</mark> [9].<br>",
    "Arabic": "متعدد السطوح تحت الوحدات",
    "Chinese": "次模多面体",
    "French": "polyèdre submodulaire",
    "Japanese": "サブモジュラーポリエドロン",
    "Russian": "подмодулярный многогранник"
  },
  {
    "English": "submodular set function",
    "context": "1: We now use a result by [8] showing that if a set X is a local optimum, then f (X) ≥ 1 3 f (X * ) if f is a general non-negative <mark>submodular set function</mark> and f (X) ≥ 1 2 f (X * ) if f is a symmetric submodular function.<br>2: If the pairwise weights are all ≤0, then one can show that S(X, Y ) from ( 1) is a <mark>submodular set function</mark> because it satisfies the diminishing returns property: consider two sets of instanced windows I 1 and I 2 , where I 1 ⊆ I 2 , and a particular un-instanced window i.<br>",
    "Arabic": "دالة مجموعة فرعية قابلة للتعويض",
    "Chinese": "次模集合函数",
    "French": "fonction d'ensemble sous-modulaire",
    "Japanese": "部分モジュラー集合関数 (ぶぶんもじゅらーしゅうごうかんすう)",
    "Russian": "субмодулярная функция множества"
  },
  {
    "English": "submodularity",
    "context": "1: <mark>Submodularity</mark> implies that A ⊆ B, and this allows us to define a lattice 2 L = [A, B] whose least element is the set A and whose greatest element is the set is B.<br>2: <mark>Submodularity</mark> of R will be the key property exploited by our algorithms.<br>",
    "Arabic": "نمطية فرعية",
    "Chinese": "子模性",
    "French": "sous-modularité",
    "Japanese": "部分加法性",
    "Russian": "субмодулярность"
  },
  {
    "English": "subnetwork",
    "context": "1: We are ready to propose a non-trivial <mark>subnetwork</mark> for grammar induction, based on the transform and join operators, which we will reuse in larger networks.<br>2: Our new transition <mark>subnetwork</mark> will join outputs of grammar inductors that either (i) continue a previous solution (as in IFJ); or (ii) start over from scratch (\"grounding\" to an FJ): \n (14) H L•DBM D l+1 split ∅ C l C l+1 l+1 l+1 \n<br>",
    "Arabic": "شبكة فرعية",
    "Chinese": "子网络",
    "French": "sous-réseau",
    "Japanese": "サブネットワーク",
    "Russian": "подсеть"
  },
  {
    "English": "suboptimal",
    "context": "1: Proof Following [8], we need to show that when a <mark>suboptimal</mark> feasible basis B † exists in the queue, it will not be chosen and tested before an optimal basis B * exists in the queue.<br>",
    "Arabic": "غير مثلى",
    "Chinese": "次优的",
    "French": "sous-optimal",
    "Japanese": "非最適な",
    "Russian": "неоптимальный"
  },
  {
    "English": "subpixel",
    "context": "1: In addition, instead of using a <mark>subpixel</mark> Taylor approximation of the data term, our update operator learns to propose the descent direction. More recently, optical flow has also been approached as a discrete optimization problem [35,13,47] using a global objective.<br>2: The graph shows that, not only does the second-order prior perform better at all error thresholds, but also that its performance improves more than the first order prior at the high-accuracy thresholds, relative to other algorithms, indicating improved <mark>subpixel</mark> accuracy. This ef-  7. Middlebury performance.<br>",
    "Arabic": "بكسل فرعي",
    "Chinese": "亚像素",
    "French": "sous-pixel",
    "Japanese": "サブピクセル",
    "Russian": "подпиксельная точность"
  },
  {
    "English": "subsample",
    "context": "1: The size of the dataset can influence the configuration of eta, lambda, max depth, min child weight, nrounds, and <mark>subsample</mark>. 5. The size of the minority class can determine the configuration of alpha, colsample bylevel, colsample bytree, eta, lambda, max depth, min child weight, nrounds, and <mark>subsample</mark>.<br>2: For datasets with a small number of categorical features, a larger cp and minbucket size tend to be better hyper-parameter configurations. Space: 5971 \n 1. Generally, larger datasets require higher nrounds and larger <mark>subsample</mark> values. 2.<br>",
    "Arabic": "عينة فرعية",
    "Chinese": "subsample",
    "French": "sous-échantillon",
    "Japanese": "サブサンプル",
    "Russian": "подвыборка"
  },
  {
    "English": "subsampling factor",
    "context": "1: where k is called the kernel size, s is the stride or <mark>subsampling factor</mark>, and f ks determines the layer type: a matrix multiplication for convolution or average pooling, a spatial max for max pooling, or an elementwise nonlinearity for an activation function, and so on for other types of layers.<br>",
    "Arabic": "عامل أخذ العينات الفرعية",
    "Chinese": "下采样因子",
    "French": "facteur de sous-échantillonnage",
    "Japanese": "サブサンプリング係数",
    "Russian": "Фактор дискретизации"
  },
  {
    "English": "subspace learning",
    "context": "1: first group . Our method has the property of finding the right clusters of mutually related tasks. 5. Consider the alternative method of <mark>subspace learning</mark> (SL) where C α is replaced by an euclidean ball of radius α.<br>",
    "Arabic": "تعلم الفضاء الفرعي",
    "Chinese": "子空间学习",
    "French": "apprentissage de sous-espaces",
    "Japanese": "サブスペース学習 (subspace learning)",
    "Russian": "обучение подпространству"
  },
  {
    "English": "subspace method",
    "context": "1: In the past decade, appearance matching using <mark>subspace methods</mark> has become a popular approach to object recognition [19][13]. Most of these algorithms are based on projecting input images to a precomputed linear subspace and then finding the closest database point that lies in the subspace.<br>",
    "Arabic": "طريقة الفضاء الفرعي",
    "Chinese": "子空间方法",
    "French": "méthode des sous-espaces",
    "Japanese": "サブスペース法",
    "Russian": "метод подпространства"
  },
  {
    "English": "subspace projection",
    "context": "1: [17,36,40], <mark>subspace projection</mark> [23], very large networks [9,49], or hybrid approaches [39,42,53] -we believe that our contributions are orthogonal to these extensions. That said, many of our parameter values may need to be re-adjusted for higher resolution datasets.<br>2: Then, the image C is raster scanned to sum up its k tiles to obtain the k coefficients that correspond to the <mark>subspace projection</mark> of the input image. This coefficient vector is compared with stored vectors and the closest match reveals the identity of the object in the image.<br>",
    "Arabic": "إسقاط الفراغ الفرعي",
    "Chinese": "子空间投影",
    "French": "projection de sous-espace",
    "Japanese": "部分空間射影",
    "Russian": "проекция на подпространство"
  },
  {
    "English": "substitution",
    "context": "1: Let A e , T e S e be the sets of the optimal boundary addition/deletion, transposition, and <mark>substitution</mark> operations required to align a pair of segmentations, h 1 and h 2 , over a sequence of elements T . Further, let b be the number of boundary types (in the case of multi-type segmentation) available.<br>2: We use the standard notion of <mark>substitution</mark>s-sort-compatible partial mappings of variables to constants. For ϕ a formula and σ a <mark>substitution</mark>, ϕσ is the formula obtained by replacing each free variable x in ϕ on which σ is defined with σ(x). Proposition B.1.<br>",
    "Arabic": "الاستبدال",
    "Chinese": "替换",
    "French": "substitution",
    "Japanese": "置換",
    "Russian": "замена"
  },
  {
    "English": "subsumption",
    "context": "1: Such procedures classify an input ontology, in general, by iterating over all necessary pairs of classes, and trying to build a model of the ontology that violates the <mark>subsumption</mark> 1 http://www.w3.org/2004/OWL/ relation between them.<br>2: We introduce Hierarchy and Exclusion (HEX) graphs, a new formalism that captures semantic relations between any two labels applied to the same object: mutual exclusion, overlap and <mark>subsumption</mark>. We then provide rigorous theoretical analysis that illustrates properties of HEX graphs such as consistency, equivalence, and computational implications of the graph structure.<br>",
    "Arabic": "التضمين",
    "Chinese": "包含",
    "French": "subsomption",
    "Japanese": "包含関係",
    "Russian": "включение"
  },
  {
    "English": "subsumption relation",
    "context": "1: This can be realised by anti-unifying I new with each prototype and partially ordering the anti-instances A Pi,Inew with respect to their <mark>subsumption relation</mark> (Plaza, 1995). Definition 4. An incident tree T is said to subsume another incident tree T , that is , T is a generalisation of T ( T > T ) if sub ( T , T , λ ) = true with sub ( , T , p ) = true sub ( T , T , p ) = false if T.p = and T.p =<br>",
    "Arabic": "علاقة التحتية",
    "Chinese": "子类包含关系",
    "French": "relation de subsomption",
    "Japanese": "包含関係",
    "Russian": "отношение субсумпции"
  },
  {
    "English": "subtree",
    "context": "1: Define g(x) = h(x) for all x ∈ var(q T ) such that h(x) ∈ ind(B i ) \\ U or h(x) is in the <mark>subtree</mark> below some element d / ∈ U .<br>2: For the second point, suppose that some x ∈ U i−1 is not h i (y) for some y ∈ U i . Thus, there is some y such that h * i (y) = x and h i (y) is strictly in the <mark>subtree</mark> rooted at x in U Aq i−1 ,O .<br>",
    "Arabic": "شجرة فرعية",
    "Chinese": "子树",
    "French": "sous-arbre",
    "Japanese": "部分木",
    "Russian": "поддерево"
  },
  {
    "English": "subwindow",
    "context": "1: The contribution of a given part filter to the score of a <mark>subwindow</mark> is obtained by convolving the filter with the selected <mark>subwindow</mark> of the HOG pyramid, multiplying it by the deformation Gaussian mask and taking the maximum value.<br>",
    "Arabic": "مربع فرعي",
    "Chinese": "子窗口",
    "French": "sous-fenêtre",
    "Japanese": "サブウィンドウ",
    "Russian": "подокно"
  },
  {
    "English": "subword token",
    "context": "1: While these observations may be expected or even obvious, Figure 4 shows that we found in  \" denotes that sentence pairs with sentence longer than the specified number, in terms of <mark>subword tokens</mark>, are removed. \"tc\" denotes whether truecasing is done or not. If not, original case of the data is kept for all datasets.<br>2: A word-level token from the parse tree normally corresponds to one or more transformer's <mark>subword tokens</mark>: we thus average subword vectors to obtain word vectors for biaffine parsing. For XLM-R and the ZH GSD treebank, however, a single XLM-R's subword token often corresponds to two treebank tokens. E.g. , the sequence `` 只是二選一做決擇 '' with treebank tokenization [ ' 只 ' , ' 是 ' , ' 二 ' , ' 選 ' , ' 一 ' , ' 做 ' , '決擇 ' ] is tokenized as [ '只是 ' , ' 二 ' , ' 選 ' , ' 一 ' , ' 做 ' , ' 決 '<br>",
    "Arabic": "رموز الكلمات الفرعية",
    "Chinese": "子词标记",
    "French": "jeton de sous-mot",
    "Japanese": "部分単語トークン",
    "Russian": "токен-подслов"
  },
  {
    "English": "subword unit",
    "context": "1: (2015), we use <mark>subword units</mark> (Sennrich et al., 2016) to overcome the OOV problem and speed up training. The ROUGE scores we obtain on the standard splits are higher than those reported by Rush et al. (2015) and comparable to those of Nallapati et al.<br>",
    "Arabic": "وحدة الكلمة الجزئية",
    "Chinese": "子词单元",
    "French": "unité de sous-mots",
    "Japanese": "サブワード単位",
    "Russian": "подсловные единицы"
  },
  {
    "English": "successor function",
    "context": "1: Every state in a search space over complete outputs consists of pairs of inputs and outputs (x, y), representing the possibility of predicting y as the output for x. Such a search space is defined in terms of two functions : 1 ) An initial state function I such that I ( x ) returns an initial state for input x , and 2 ) A <mark>successor function</mark> S such that for any search state ( x , y ) , S ( ( x , y ) ) returns a set of<br>",
    "Arabic": "دالة الخلفية",
    "Chinese": "后继函数",
    "French": "fonction successeur",
    "Japanese": "後続関数",
    "Russian": "преемственная функция"
  },
  {
    "English": "successor state",
    "context": "1: A ground action a ∈ A is applicable in state s ∈ S if pre(a) ⊆ s. In that case, applying a to s results in the <mark>successor state</mark> s a = (s \\ del(a)) ∪ add(a). A sequence of actions π = a 1 , . . .<br>2: Notice that the maximization typically found in VI is not present-this is because the operation computes and records Q-values for all choices of actions at the <mark>successor state</mark> s .<br>",
    "Arabic": "الدولة الخلفية",
    "Chinese": "后继状态",
    "French": "état successeur",
    "Japanese": "後継状態",
    "Russian": "следующее состояние"
  },
  {
    "English": "successor state axiom",
    "context": "1: For instance, Reiter (1991) showed that under certain reasonable assumptions, <mark>successor state axioms</mark> can be computed from action effect axioms by predicate completion, and thus solved the frame problem when there are no state constraints.<br>2: For state constraints, Lin (1995) argued that they should be encoded using a notion of causality, and once they are encoded this way, <mark>successor state axioms</mark> can once again be computed using predicate completion for a class of causal rules that includes almost all of the benchmark planning domains (Lin 1995;2003).<br>",
    "Arabic": "مسلمة حالة الخلف",
    "Chinese": "后继状态公理",
    "French": "axiome de l'état successeur",
    "Japanese": "後継状態公理",
    "Russian": "аксиома преемника состояния"
  },
  {
    "English": "sufficient statistic",
    "context": "1: This distribution has neither independencies nor conditional independencies. However, its variables are finitely exchangeable and the probability of a state x is only a function of the <mark>sufficient statistic</mark> T (x) counting the number of smokers in x. The probability of a state now increases by a factor of exp(1.5) with every pair of smokers.<br>2: The corresponding <mark>sufficient statistic</mark> T counts the number of occurrences of each binary vector of length 4 and returns a tuple of counts. Please note that the definition of full finite exchangeability (Definition 2) is the special case when the exchangeable decomposition has width 1.<br>",
    "Arabic": "إحصائية كافية",
    "Chinese": "充分统计量",
    "French": "statistique suffisante",
    "Japanese": "十分統計量",
    "Russian": "достаточная статистика"
  },
  {
    "English": "suffix tree",
    "context": "1: In this case, the mapping process stops and the concept sequence corresponding to qi+1 • • • q l is returned. After the mapping procedure, we start from the last concept in the sequence and search the concept sequence <mark>suffix tree</mark> from the root node. The process is shown in Algorithm 3.<br>2: {} C2C1 C3C1 C1C5 C2C5 C7C5 C1 C2 C5 C4 C3 C4C1 C1C3 C6C3 C2C3C1 C4C3C1 C9C2C5 \n Once we get the frequent concept sequences, we organize them into a concept sequence <mark>suffix tree</mark> (Figure 4). Formally, a (proper) suffix of a concept sequence cs = c1 . . .<br>",
    "Arabic": "شجرة اللاحقات",
    "Chinese": "后缀树",
    "French": "arbre des suffixes",
    "Japanese": "接尾辞木",
    "Russian": "суффиксное дерево"
  },
  {
    "English": "summarization",
    "context": "1: Research on <mark>summarization</mark> has mainly been driven by empirical approaches, crafting systems to perform well on standard datasets with the notion of information Importance remaining latent. We argue that establishing theoretical models of Importance will advance our understanding of the task and help to further improve <mark>summarization</mark> systems.<br>2: Clearly, this performance gain demonstrates the effectiveness of content models for the <mark>summarization</mark> task.<br>",
    "Arabic": "تلخيص",
    "Chinese": "摘要生成",
    "French": "résumé",
    "Japanese": "要約",
    "Russian": "реферирование"
  },
  {
    "English": "summarization algorithm",
    "context": "1: In this section, we provide the empirical evaluation for the effectiveness of our <mark>summarization algorithm</mark>. We use two kinds of datasets in our experiments: three real datasets and a series of synthetic datasets. The clustering algorithms are implemented in Visual C++. All of our experiments are performed on a 3.2GHZ, 1GB-memory, Intel PC running Windows XP.<br>",
    "Arabic": "خوارزمية التلخيص",
    "Chinese": "摘要算法",
    "French": "algorithme de résumé",
    "Japanese": "要約アルゴリズム",
    "Russian": "алгоритм суммаризации"
  },
  {
    "English": "summarization model",
    "context": "1: This result becomes one of the most distinguishing features in our <mark>summarization model</mark>. It means that we can use very limited information in a profile to recover the supports of a rather large set of patterns.<br>",
    "Arabic": "نموذج تلخيص",
    "Chinese": "摘要模型",
    "French": "modèle de résumé",
    "Japanese": "要約モデル",
    "Russian": "модель суммаризации"
  },
  {
    "English": "summarization system",
    "context": "1: To address question (2), we consider a <mark>summarization system</mark> that learns extraction rules directly from a parallel corpus of full texts and their summaries (Kupiec et al., 1999).<br>2: We evaluated our <mark>summarization system</mark> on the Earthquakes domain, since for some of the texts in this domain there is a condensed version written by AP journalists. These summaries are mostly extractive 11 ; consequently, they can be easily aligned with sentences in the original articles.<br>",
    "Arabic": "نظام التلخيص",
    "Chinese": "摘要系统",
    "French": "système de résumé",
    "Japanese": "要約システム",
    "Russian": "система резюмирования"
  },
  {
    "English": "super-pixel",
    "context": "1: Here the aim is to assign a label to each pixel of a given image from a set of possible object classes. Typically these methods use random fields to model local interactions between pixels or <mark>super-pixels</mark>.<br>2: Given the strong analogy between spectral matting and hard spectral segmentation, we would like to gain some intuition about the possible advantage of using matting components versus standard hard segmentation components (also known as <mark>super-pixels</mark>). The answer, of course, depends on the application.<br>",
    "Arabic": "البكسل الفائق",
    "Chinese": "超像素",
    "French": "super-pixel",
    "Japanese": "スーパーピクセル",
    "Russian": "суперпиксель"
  },
  {
    "English": "super-resolution",
    "context": "1: Figure 7 shows one of the results in a semi-dense 3D point cloud form constructed based on generated keyframes each consisting of reconstructed <mark>super-resolution</mark> and high dynamic range intensity and inverse depth map. The bright RGB 3D coordinate axes represent the current camera pose while the darker ones show all keyframe poses generated in this experiment.<br>2: [12] on sub-pixel tracking and <mark>super-resolution</mark> mosaic reconstruction from events gave a strong indication that the accurate multi-view correspondence needed for depth estimation is possible. The essential insight to extending Kim et al.<br>",
    "Arabic": "الدقة الفائقة",
    "Chinese": "超分辨率",
    "French": "super-résolution",
    "Japanese": "超解像度",
    "Russian": "сверхразрешение"
  },
  {
    "English": "supergradient",
    "context": "1: Recall the standard fact [24,Chapter VI.4.4] that for a collection {f p } p∈P of concave functions, if the infimum f (x) = inf p∈P f p (x) is attained at some p 0 then any vector ∇f p 0 (x) is a <mark>supergradient</mark> of f (x). Thus , letting p ( λ ) be the ( unique ) minimizing value of p for any λ > 0 , the objective ( 47 ) becomes f ( λ ) = λ 2 p ( λ ) − 1 n 1 2 2 − λρ n + p ( λ ) ⊤ z , whose derivative with respect to λ ( holding<br>",
    "Arabic": "سوبرجراديان",
    "Chinese": "超梯度",
    "French": "supergradient",
    "Japanese": "超勾配",
    "Russian": "суперградиент"
  },
  {
    "English": "supertag",
    "context": "1: Table 2 shows parsing results on the test set. Our global features let us improve over the <mark>supertag</mark>factored model by 0.6 F1. use global features, but our optimal decoding leads to an improvement of 0.4 F1. Although we observed an overall improvement in parsing performance, the <mark>supertag</mark> accuracy was not significantly different after applying the parser.<br>2: We will operationalize this intuition as the predictability of next word's <mark>supertag</mark> under the Combinatory Categorial Grammar (CCG) formalism (Steedman, 1987): \n surp syn = − log(P (c n | w 1 , ..., w n−1 )), (1) \n where c n is the CCG <mark>supertag</mark> of the n-th word.<br>",
    "Arabic": "سوبرتاج",
    "Chinese": "超标签",
    "French": "superétiquette",
    "Japanese": "スーパータグ",
    "Russian": "супертег"
  },
  {
    "English": "supervise classification",
    "context": "1: Whereas traditional <mark>supervised classification</mark> is appropriate to learn attributes that are intrinsically binary, it falls short when we want to represent visual properties that are nameable but not categorical. Our goal is instead to estimate the degree of that attribute's presence-which, importantly, differs from the probability of a binary classifier's prediction.<br>2: On top of the K pre-trained layers, stack an output layer of size the number of classes. Finetune the whole network for <mark>supervised classification</mark> 2 with an added tangent propagation penalty (Eq. 6), using for each x i , tangent directions B xi . We call this deep learning algorithm the Manifold Tangent Classifier (MTC).<br>",
    "Arabic": "التصنيف الخاضع للإشراف",
    "Chinese": "监督分类",
    "French": "classification supervisée",
    "Japanese": "教師付き分類",
    "Russian": "контролируемая классификация"
  },
  {
    "English": "supervise classification model",
    "context": "1: [2019] which uses a slightly different end-to-end mutual information maximization approach, AlexNet [Krizhevsky et al., 2012] as their feature-extraction model and an additional hidden layer in the <mark>supervised classification model</mark>, GIM comes out favorably.<br>",
    "Arabic": "نموذج تصنيف مشرف",
    "Chinese": "监督分类模型",
    "French": "modèle de classification supervisée",
    "Japanese": "教師あり分類モデル",
    "Russian": "модель с учителем классификации"
  },
  {
    "English": "supervise classifier",
    "context": "1: We attribute this decreased performance to overfitting on the small amount of training data available (11 trajectories), and would expect a near perfect correlation for a well trained <mark>supervised classifier</mark>. This experiment demonstrates the possibility of learning to detect an inertial object without labels.<br>",
    "Arabic": "مُصنِّف مُراقَب",
    "Chinese": "有监督分类器",
    "French": "classificateur supervisé",
    "Japanese": "教師あり分類器",
    "Russian": "классификатор с учителем"
  },
  {
    "English": "supervise contrastive learning",
    "context": "1: Therefore, we propose a solution based on the concept of <mark>supervised contrastive learning</mark> (Khosla et al., 2020) and confidence regularization (Utama et al., 2020). Our framework can be applied to remove bias from any text encoder model regardless of the architecture.<br>2: Recently, [8] propose <mark>supervised contrastive learning</mark> (SCL), an approach that aggregates data from the same class as the positive set and obtains improved performance on various supervised learning tasks.<br>",
    "Arabic": "تعلم تباينيّ موجّه",
    "Chinese": "监督对比学习",
    "French": "apprentissage contrastif supervisé",
    "Japanese": "監督対照学習",
    "Russian": "контролируемое контрастное обучение"
  },
  {
    "English": "supervise datum",
    "context": "1: Nevertheless, the apparently good recognition results on <mark>supervised data</mark> that some works achieve cannot be extrapolated to unsupervised (semi-naturalistic) data [2,12]. In this paper we propose an automatic methodology to extract a set of the most important features to be used in activity recognition.<br>2: If the system computes all collected articles or all requests, the computation time is larger. Therefore, the system restricts the computing of articles by retrieval using important words. Event detection is similar to classification learning. It is well known that preparation of <mark>supervised data</mark> is costly. Therefore, there have been many studies to reduce the cost.<br>",
    "Arabic": "البيانات المراقبة",
    "Chinese": "有监督数据",
    "French": "données supervisées",
    "Japanese": "教師データ",
    "Russian": "данные с учителем"
  },
  {
    "English": "supervise finetuning",
    "context": "1: For Ro-En, few shot GPT-3 performs within 0.5 BLEU of the overall SOTA which is achieved by a combination of unsupervised pretraining, <mark>supervised finetuning</mark> on 608K labeled examples, and backtranslation [LHCG19b].<br>",
    "Arabic": "ضبط دقيق تحت الإشراف",
    "Chinese": "监督微调",
    "French": "affinage supervisé",
    "Japanese": "監督ファインチューニング",
    "Russian": "контролируемая подстройка"
  },
  {
    "English": "supervise learning",
    "context": "1: Learning, broadly taken, involves choosing a good model from a large space of possible models. In <mark>supervised learning</mark>, model behavior is primarily determined by labeled examples, whose production requires a certain kind of expertise and, typically, a substantial commitment of resources.<br>2: But what about those first hundred thousand students? In most educational contexts (i.e. classrooms), assignments do not have enough historical data for <mark>supervised learning</mark>. In this paper, we introduce a human-in-the-loop \"rubric sampling\" approach to tackle the \"zero shot\" feedback challenge.<br>",
    "Arabic": "التعلم الإشرافي",
    "Chinese": "监督学习",
    "French": "apprentissage supervisé",
    "Japanese": "教師あり学習",
    "Russian": "Обучение с учителем"
  },
  {
    "English": "supervise manner",
    "context": "1: Recent progress in the field is driven by advances in large language models (Shen et al., 2021;Sevgili et al., 2022), pushing the scores on standard evaluation datasets to new heights. These models are typically trained in a <mark>supervised manner</mark>.<br>",
    "Arabic": "بطريقة مشرفة",
    "Chinese": "监督方式",
    "French": "manière supervisée",
    "Japanese": "- Term: \"監督された方法 (Kansoku sareta houhou)\"",
    "Russian": "контролируемый способ"
  },
  {
    "English": "supervise method",
    "context": "1: First, models trained only with simple reward signals achieve surprisingly high results, coming within 11% of a fully <mark>supervised method</mark> in the Windows domain. Second, augmenting unlabeled documents with even a small fraction of annotated examples greatly reduces this performance gap, to within 4% in that domain.<br>",
    "Arabic": "طريقة الإشراف",
    "Chinese": "监督方法",
    "French": "méthode supervisée",
    "Japanese": "教師あり手法",
    "Russian": "Метод наблюдения"
  },
  {
    "English": "supervise model",
    "context": "1: In all of these cases, labeled data is used to train <mark>supervised model</mark>; our work shows that social structural regularities are powerful enough to support accurate induction of social relationships (and their linguistic correlates) without labeled data.<br>2: While such bi-texts have primarily been leveraged to train statistical machine translation (SMT) systems, contemporary research has increasingly considered the possibilities of utilizing parallel corpora to improve systems outside of SMT. For example, Yarowsky and Ngai (2001) projects the part-of-speech labels assigned by a <mark>supervised model</mark> in one language (e.g.<br>",
    "Arabic": "النموذج المراقب",
    "Chinese": "监督模型",
    "French": "modèle supervisé",
    "Japanese": "教師付きモデル",
    "Russian": "модель с учителем"
  },
  {
    "English": "supervise multi-task learning",
    "context": "1: We assume a <mark>supervised multi-task learning</mark> setting, with input space X ⊆ R p and output space Y ⊆ R k . We learn a mapping f : R p → R k , from input space X to output space Y, where we parameterize function f with a differentiable tree ensemble.<br>",
    "Arabic": "التعلم المتعدد المهام الموجه",
    "Chinese": "监督多任务学习",
    "French": "apprentissage multi-tâches supervisé",
    "Japanese": "監督付きマルチタスク学習",
    "Russian": "наблюдаемое многозадачное обучение"
  },
  {
    "English": "supervise setting",
    "context": "1: In the common <mark>supervised setting</mark>, the training objective is to learn a transformation from the source space to the target space X → Y : f (y|x; Θ) with the usage of parallel data.<br>2: On the other hand, in practice, one is rarely interested in the full trajectory; instead one typically tracks the trajectory of various summary statistics of the algorithm's evolution, such as the loss, the amplitude of various weights, or correlations between the classifier and the ground truth (in a <mark>supervised setting</mark>).<br>",
    "Arabic": "الإعداد التشريفي",
    "Chinese": "监督设置",
    "French": "cadre supervisé",
    "Japanese": "監視設定",
    "Russian": "наблюдаемая настройка"
  },
  {
    "English": "supervise system",
    "context": "1: Although the performance of transfer systems is still substantially below that of <mark>supervised systems</mark>, this research provides one step towards bridging this gap. Further, we believe that it opens up an avenue for future work on multilingual clustering methods, cross-lingual feature projection and domain adaptation for direct transfer of linguistic structure.<br>2: We show that the effect of our method also carries out to downstream tasks, but its effect is larger in un<mark>supervised systems</mark> directly using embedding similarities than in <mark>supervised systems</mark> using embeddings as input features, as the latter have enough expressive power to learn the optimal transformation themselves.<br>",
    "Arabic": "نظام إشرافي",
    "Chinese": "监督系统",
    "French": "système supervisé",
    "Japanese": "監督システム (Kanshoku shisutemu)",
    "Russian": "система с учителем"
  },
  {
    "English": "supervise training",
    "context": "1: Datadriven approaches (Schuster et al., 2019;Xiang et al., 2021) perform standard <mark>supervised training</mark> with translated dialogs, known as Translate-Train. Different pseudo-data pairs could be leveraged to enhance the multilingual model's robustness. Nevertheless, a fine-grained machine translation system may not exist in an extremely low-resource language.<br>2: Some particularly eye-catching pieces of work over the past year have focused on supervised and self-<mark>supervised training</mark> of surprisingly capable networks which are able to estimate visual odometry, depth and other quantities from video [11,30,3,31,5,34,33].<br>",
    "Arabic": "التدريب المشرف",
    "Chinese": "监督训练",
    "French": "l'entraînement supervisé",
    "Japanese": "\"教師あり学習\"",
    "Russian": "наблюдаемое обучение"
  },
  {
    "English": "support",
    "context": "1: Let us denote the distribution induced by the trial weights by q. We next calculate a closed form expression for q. The <mark>support</mark> of q is the same as the <mark>support</mark> of d P + , because only documents that are sampled from d P + are assigned an unnormalized trial weight.<br>2: In the next theorem, assume n is at least the optimal sample complexity of estimating entropy, <mark>support</mark>, <mark>support</mark> coverage, and distance to uniformity (given in Table 1 \n ) respec- tively. Theorem 2. For all ε > c/n 0.2 , any plug-in exp ( − √ n ) - approximate PMLp satisfies , Entropy Cp ( H ( p ) , ∆ k , ε ) C * ( H ( p ) , ∆ k , ε ) , Support size Cp ( S ( p ) /k , ∆ ≥ 1 k , ε )<br>",
    "Arabic": "يدعم",
    "Chinese": "支持度",
    "French": "support",
    "Japanese": "サポート",
    "Russian": "опора"
  },
  {
    "English": "support set",
    "context": "1: We report the results in Table 3, which shows that our model is robust to the choice of <mark>support set</mark>. We use the first <mark>support set</mark> (#1) in Table 3 for comparison with other baselines or ablated variants in Section 5, due to the huge computational cost for evaluating few-shot baselines HSNet and VAT.<br>2: The level-0 basis for a dataset X is called the <mark>support set</mark> of X . Solving (2) amounts to finding the <mark>support set</mark> of X ; Fig. 1 illustrates. Assuming X is non-degenerate 2 , it has only one <mark>support set</mark> (cf. the minmax problem has a single minimum).<br>",
    "Arabic": "مجموعة الدعم",
    "Chinese": "支持集",
    "French": "ensemble de supports",
    "Japanese": "サポートセット",
    "Russian": "набор опорных данных"
  },
  {
    "English": "support threshold",
    "context": "1: Example 5 above provides a strong indication that no algorithm can efficiently enumerate all maximal frequent itemsets in the worst case, given an arbitrary <mark>support threshold</mark>.<br>2: We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions, given an arbitrary <mark>support threshold</mark>, is #P-complete, thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard.<br>",
    "Arabic": "عتبة الدعم",
    "Chinese": "支持度阈值",
    "French": "seuil de support",
    "Japanese": "サポート閾値",
    "Russian": "порог поддержки"
  },
  {
    "English": "support vector",
    "context": "1: Here, instead of directly transforming the indefinite kernel, we simultaneously learn the <mark>support vector</mark> weights and a proxy positive semidefinite kernel matrix, while penalizing the distance between this proxy kernel and the original, indefinite one.<br>",
    "Arabic": "مُتجه الدعم",
    "Chinese": "支持向量",
    "French": "vecteur de support",
    "Japanese": "サポートベクトル",
    "Russian": "векторы опоры"
  },
  {
    "English": "surface normal",
    "context": "1: The depth estimator, g(x; I), in turn, takes as input a triplet of an RGB image, foreground mask, and UV coordinate, and outputs the depth estimates. The geometric consistency between the <mark>surface normal</mark> and depth is enforced by minimizing L s .<br>2: We use the train and val split for training and early-stopping, respectively, and use the \"muleshoe\" building included in the test split for evaluation. To demonstrate our universal few-shot learner , we use ten dense prediction tasks in Taskonomy dataset ( Zamir et al. , 2018 ) , which are semantic segmentation ( SS ) , <mark>surface normal</mark> ( SN ) , Euclidean distance ( ED ) , Z-buffer depth ( ZD ) , texture edge ( TE ) , occlusion edge ( OE ) , 2D keypoints<br>",
    "Arabic": "السطح العمودي",
    "Chinese": "表面法线",
    "French": "normale de surface",
    "Japanese": "表面法線",
    "Russian": "поверхностная нормаль"
  },
  {
    "English": "surface normal estimator",
    "context": "1: HDNet is composed of two estimators: surface normal and depth estimators. The <mark>surface normal estimator</mark> f (x; I) takes as input an RGB image and its foreground mask, and outputs the surface normal estimates.<br>",
    "Arabic": "مُقَدِّرُ الضَّوْرَةِ السَّطْحِيَّة",
    "Chinese": "表面法向量估计器",
    "French": "estimateur de normale de surface",
    "Japanese": "表面法線推定器",
    "Russian": "оценщик нормали поверхности"
  },
  {
    "English": "surface normal prediction",
    "context": "1: from one image to the other image at a different time instant to measure self-consistency , which allows us to utilize the real dance videos ; ( 3 ) HDNet design that learns to predict fine depths reflective of <mark>surface normal prediction</mark> by enforcing their geometric consistency ; ( 4 ) strong qualitative and quantitative prediction on real world imagery .<br>2: For tasks with continuous labels, we use the mean angle error (mErr) for <mark>surface normal prediction</mark> (SN) (Eigen & Fergus, 2015) and root mean square error (RMSE) for the others.<br>",
    "Arabic": "تنبؤ بالسطح العادي",
    "Chinese": "表面法线预测",
    "French": "prédiction de la normale de surface",
    "Japanese": "表面法線予測",
    "Russian": "предсказание нормали поверхности"
  },
  {
    "English": "surface realization",
    "context": "1: In the <mark>surface realization</mark> stage, the system selects a single tree from the generated set of possible trees and removes mark-up to produce a final string. This is also the stage where punctuation may be added.<br>2: However, the system can still generate verbs when action and pose detectors have been run, and this framework allows the system to \"hallucinate\" likely verbal constructions between objects if specified at runtime. A similar approach was taken in Yang et al. (2011). Some examples are given in Figure 7. We follow a three-tiered generation process ( Reiter and Dale , 2000 ) , utilizing content determination to first cluster and order the object nouns , create their local subtrees , and filter incorrect detections ; microplanning to construct full syntactic trees around the noun clusters , and <mark>surface realization</mark> to order selected modifiers , realize them as postnominal or prenominal , and select<br>",
    "Arabic": "التجسيد السطحي",
    "Chinese": "表面实现",
    "French": "réalisation de surface",
    "Japanese": "表層実現",
    "Russian": "поверхностная реализация"
  },
  {
    "English": "surrogate",
    "context": "1: One key aspect of the convergence analysis will be to show that f t (D t ) and f t (D t ) converges almost surely to the same limit and thusf t acts as a <mark>surrogate</mark> for f t . • Sincef t is close tof t−1 , D t can be obtained efficiently using D t−1 as warm restart.<br>2: Estimators based on analytic local expectation [59,61] and GO gradients [11] also use neighborhood information but only at the cost of many additional target function evaluations. In fact , the local expectation gradient [ 59 ] can be viewed as a Stein CV adjustment RODEO with a Gibbs Stein operator and the target function f used directly instead of the <mark>surrogate</mark> h. The downside of these approaches is that f must be evaluated Kd times per training step instead of K times as in RODEO , a prohibitive cost when<br>",
    "Arabic": "مُحاكي",
    "Chinese": "替代物",
    "French": "substitut",
    "Japanese": "代理",
    "Russian": "суррогат"
  },
  {
    "English": "surrogate function",
    "context": "1: Proof sktech: The first step in the proof is to show using classical analysis tools that, given assumptions (A) to (C), f is C 1 with a Lipschitz gradient. ConsideringÃ andB two accumulation points of 1 t A t and 1 t B t respectively , we can define the corresponding <mark>surrogate function</mark>f ∞ such that for all D in C , f ∞ ( D ) = 1 2 Tr ( D T DÃ ) − Tr ( D TB ) , and its optimum D ∞ on C. The next step<br>2: We then define the functionf (X) = f ea (X) = κ f w f κ (X) + (1 − κ f ) j∈X f (j) and use this as the <mark>surrogate function</mark>f for f . We chooseĝ as g itself. The surrogate problem becomes: \n<br>",
    "Arabic": "دالة بديلة",
    "Chinese": "替代函数",
    "French": "fonction de substitution",
    "Japanese": "代用関数",
    "Russian": "суррогатная функция"
  },
  {
    "English": "surrogate loss",
    "context": "1: We now provide three lemmas and a theorem that show that if the <mark>surrogate loss</mark> ϕ satisfies edgeconsistency, then its minimizer asymptotically minimizes the Bayes risk. As the lemmas are direct analogs of results in Tewari & Bartlett (2007) and Zhang (2004), we put their proofs in Appendix A. \n Lemma 3.<br>2: R ( p ) < ε } , so that the definition of an r-calibrated loss is the following : Definition 1 . [20, Definition 2.7] The <mark>surrogate loss</mark> is r-calibrated if ∀p ∈ P, ∀ε > 0, ∃δ > 0 : M (p, δ) ⊆ M r (p, ε) .<br>",
    "Arabic": "الخسارة البديلة",
    "Chinese": "替代损失",
    "French": "perte surrogate",
    "Japanese": "代理損失",
    "Russian": "суррогатная потеря"
  },
  {
    "English": "surrogate loss function",
    "context": "1: Most existing geometry-based methods follow a two-stage strategy, where the intermediate representations (i.e., 2D-3D correspondences) are learned with a <mark>surrogate loss function</mark>, which is sub-optimal compared to end-to-end learning.<br>",
    "Arabic": "دالة الخسارة البديلة",
    "Chinese": "替代损失函数",
    "French": "fonction de perte de substitution",
    "Japanese": "代替損失関数",
    "Russian": "суррогатная функция потерь"
  },
  {
    "English": "surrogate model",
    "context": "1: Randomized Prior Network (RPN) is an ensemble model [53]. Each member of the RPN is built as the sum of a trainable and a non-trainable (so-called \"prior\") <mark>surrogate model</mark>; we used MLP for simplicity.<br>2: Perturbation-based explainers, such as SHAP (Lundberg and Lee 2017), obtain an explanation vector w x for x via training a <mark>surrogate model</mark> of the form g(x) = w x0 + d j=1 w xj x j by minimizing a loss function L(f, g) that measures how unfaithful g is in approximating f .<br>",
    "Arabic": "نموذج بديل",
    "Chinese": "代理模型",
    "French": "modèle substitut",
    "Japanese": "代理モデル",
    "Russian": "суррогатная модель"
  },
  {
    "English": "symbol grounding problem",
    "context": "1: CB2 is a customizable, scalable, and complete research platform, including server and clients for multi-agent human-machine interactions, tools for real-time data management, and processes to onboard crowdsourcing workers. The CB2 scenario poses learning and reasoning challenges, as well as opportunities. Comprehending and producing instructions in CB2 requires addressing the <mark>symbol grounding problem</mark> ( Harnad , 1990 ) , which is studied extensively in the instruction following ( e.g. , Chen and Mooney , 2011 ; Artzi and Zettlemoyer , 2013 ; Misra et al. , 2017 ; Fried et al. , 2018 ) and generation ( e.g. , Mei et al. , 2016 ;<br>",
    "Arabic": "مشكلة تأصيل الرموز",
    "Chinese": "符号接地问题",
    "French": "problème d'ancrage des symboles",
    "Japanese": "記号接地問題",
    "Russian": "проблема обоснования символов"
  },
  {
    "English": "symbolic representation",
    "context": "1: , 2009 ; Christodoulopoulos et al. , 2012 , inter alia ) . Children, however, do not get symbolic input; <mark>symbolic representations</mark> at any level of granularity constitute abstractions inferred from highly variable, noisy, and information-rich perceptual signals like audition and vision.<br>",
    "Arabic": "تمثيل رمزي",
    "Chinese": "符号表示",
    "French": "représentation symbolique",
    "Japanese": "記号的表現",
    "Russian": "символическое представление"
  },
  {
    "English": "symmetric matrix",
    "context": "1: log(Σ) = ∞ k=1 (−1) k−1 k (Σ − I) k = Ulog(D)U T . (8) \n The exponential operator is always defined, whereas the logarithms only exist for <mark>symmetric matrices</mark> with positive eigenvalues, Sym + d .<br>2: We can then combine this expression with the smooth approximation above to get the gradient. We note that eigenvalues of <mark>symmetric matrices</mark> are not differentiable when some of them have multiplicities greater than one (see [17] for a discussion).<br>",
    "Arabic": "مصفوفة متماثلة",
    "Chinese": "对称矩阵",
    "French": "matrice symétrique",
    "Japanese": "対称行列",
    "Russian": "симметрическая матрица"
  },
  {
    "English": "symmetric positive semidefinite matrix",
    "context": "1: where L ∈ R N ×N (for N = |V|) is a <mark>symmetric positive semidefinite matrix</mark>, and L V refers to the submatrix of L with only those rows and columns corresponding to those elements in the subset V . Although MAP inference remains NP-hard in DPPs (just as in MPPs), marginal inference becomes tractable.<br>2: [Belabbas & Wolfe, 2009] Given a <mark>symmetric positive semidefinite matrix</mark>, K ff , if M columns are selected to form a Nyström approximation such that the probability of selecting a subset of columns, Z, is proportional to the determinant of the principal submatrix formed by these columns and the matching rows, then, \n<br>",
    "Arabic": "مصفوفة شبه محددة إيجابية متماثلة",
    "Chinese": "对称半正定矩阵",
    "French": "matrice symétrique semidéfinie positive",
    "Japanese": "対称正定値行列",
    "Russian": "симметричная положительно полуопределенная матрица"
  },
  {
    "English": "symmetrization",
    "context": "1: An important point is that, by using the stability bound, the coverage guarantee of the interpolated conformal set is preserved without the need of the expensive <mark>symmetrization</mark> proposed in (Ndiaye & Takeuchi, 2021). Such techniques are more relevant when the sample size is small or when precise estimates of the stability bounds are not available.<br>",
    "Arabic": "تناظر",
    "Chinese": "对称化",
    "French": "symétrisation",
    "Japanese": "対称化",
    "Russian": "симметризация"
  },
  {
    "English": "synchronous context-free grammar",
    "context": "1: We present a statistical phrase-based translation model that uses hierarchical phrasesphrases that contain subphrases. The model is formally a <mark>synchronous context-free grammar</mark> but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntaxbased translation systems without any linguistic commitment.<br>2: We tackled the problem by introducing a novel reduction-based weighted <mark>synchronous context-free grammar</mark> formalism, which allows sentence generation with a log-linear model. In addition, we proposed a novel generative model that jointly generates lambda calculus expressions and natural language sentences. The model is then used for automatic grammar induction.<br>",
    "Arabic": "قواعد متزامنة خالية من السياق",
    "Chinese": "同步上下文无关语法",
    "French": "grammaire synchrone hors-contexte",
    "Japanese": "同期コンテキストフリー文法",
    "Russian": "синхронная контекстно-свободная грамматика"
  },
  {
    "English": "synonymy",
    "context": "1: Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying <mark>synonymy</mark>, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora.<br>2: Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., <mark>synonymy</mark>, antonymy). Our aim is to provide insights to semantic model researchers by evaluating models in multiple attributes.<br>",
    "Arabic": "مرادفة",
    "Chinese": "同义性",
    "French": "synonymie",
    "Japanese": "同義語",
    "Russian": "синонимия"
  },
  {
    "English": "synset",
    "context": "1: These statistics lead us to a straightforward, but important, conclusion: only in a limited number of cases is a lexicalization belonging to a given <mark>synset</mark> to be considered as a suitable translation equivalent for the provided target word and its context.<br>2: To do this, we define µ λ P (σ) as the index of <mark>synset</mark> σ in Ω EN (λ P ) ordered according to WordNet's sense frequency, as computed from SemCor. That is, in-dex k means that <mark>synset</mark> σ is the k-th most frequent meaning for λ P .<br>",
    "Arabic": "- مجموعة الترادف",
    "Chinese": "同义词集",
    "French": "synset",
    "Japanese": "同義語集合",
    "Russian": "синсет"
  },
  {
    "English": "syntactic analysis",
    "context": "1: We find that 40% of the OLLIE extractions that REVERB misses are due to OLLIE's use of parsers -REVERB misses those because its shallow <mark>syntactic analysis</mark> cannot skip over the intervening clauses or prepositional phrases between the relation phrase and the arguments.<br>",
    "Arabic": "تحليل تركيبي",
    "Chinese": "句法分析",
    "French": "analyse syntaxique",
    "Japanese": "構文解析",
    "Russian": "синтаксический анализ"
  },
  {
    "English": "syntactic category",
    "context": "1: In syntactic distributional clustering, words are grouped on the basis of the vectors of their preceeding and following words (Schütze, 1995;Clark, 2001). The underlying linguistic idea is that replacing a word with another word of the same <mark>syntactic category</mark> should preserve syntactic well-formedness (Radford, 1988).<br>",
    "Arabic": "فئة تركيبية",
    "Chinese": "句法类别",
    "French": "catégorie syntaxique",
    "Japanese": "統語範疇",
    "Russian": "синтаксическая категория"
  },
  {
    "English": "syntactic constraint",
    "context": "1: Their approach is conceptually simple: it consists of a scoring function coupled with a small number of syntactic and semantic constraints. Discourse-related information can be easily incorporated in the form of additional constraints.<br>2: Coreference systems exploit a variety of information sources, ranging from syntactic and discourse constraints, which are highly configurational, to semantic constraints, which are highly contingent on lexical meaning and world knowledge. Perhaps because configurational features are inherently easier to learn from small data sets, past work has often emphasized them over semantic knowledge.<br>",
    "Arabic": "القيد النحوي",
    "Chinese": "句法约束",
    "French": "contrainte syntaxique",
    "Japanese": "構文上の制約",
    "Russian": "синтаксические ограничения"
  },
  {
    "English": "syntactic dependency",
    "context": "1: We also share the parameters of lower layers in our model to predict POS tags and predicates. Following , we focus on the end-toend setting, where predicates must be predicted on-the-fly. Since we also train our model to predict <mark>syntactic dependencies</mark>, it is beneficial to give the model knowledge of POS information.<br>2: We used a similar PMI score, but defined it with respect to semantic arguments instead of <mark>syntactic dependencies</mark>. Thus, the values for features 2 and 5 are computed as follows (the notation is explained in 5 http://verbs.colorado.edu/semlink the caption for Table 2 \n<br>",
    "Arabic": "الاعتماد النحوي",
    "Chinese": "句法依赖",
    "French": "dépendance syntaxique",
    "Japanese": "統語的依存関係",
    "Russian": "синтаксическая зависимость"
  },
  {
    "English": "syntactic dependency parsing",
    "context": "1: (2018) demonstrated the effectiveness of linguisticallyinformed self-attention layers in SRL; Cai and Lapata (2019b) observed that syntactic dependencies often mirror semantic relations and proposed a model that jointly learns to perform <mark>syntactic dependency parsing</mark> and SRL;  devised syntax-based pruning rules that work for multiple languages.<br>",
    "Arabic": "تحليل التبعية النحوية",
    "Chinese": "句法依存分析",
    "French": "analyse syntaxique par dépendances",
    "Japanese": "構文依存解析",
    "Russian": "синтаксический анализ зависимостей"
  },
  {
    "English": "syntactic dependency tree",
    "context": "1: Ratnaparkhi (2002) later addressed some of the limitations of class-based LMs in the over-generation phase by using a modified generator based on a <mark>syntactic dependency tree</mark>. Mairesse and Young (2014) proposed a phrase-based NLG system based on factored LMs that can learn from a semantically aligned corpus.<br>",
    "Arabic": "شجرة التبعية النحوية",
    "Chinese": "句法依赖树",
    "French": "arbre de dépendances syntaxiques",
    "Japanese": "構文依存木 (Kōbun izon ki)",
    "Russian": "синтаксическое дерево зависимостей"
  },
  {
    "English": "syntactic feature",
    "context": "1: The input to the Barak et al. (2012) model is a sequence of frames, where each frame is a collection of syntactic and semantic features representing what the learner might extract from an utterance s/he has heard paired with a scene s/he has perceived.<br>2: We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable <mark>syntactic features</mark> captured by the system and mechanisms for deferred resolution of syntactic ambiguities.<br>",
    "Arabic": "السمة النحوية",
    "Chinese": "句法特征",
    "French": "fonctionnalité syntaxique",
    "Japanese": "構文的特徴",
    "Russian": "синтаксическая характеристика"
  },
  {
    "English": "syntactic information",
    "context": "1: The token-level tags produced by our model encode relevant <mark>syntactic information</mark> suitable for the given bit rate, while the locations of these tags serve to concretely define the location at which syntactic decisions can be committed to in a speculation-free manner.<br>2: On the other hand, we think that while knowledge of language is one aspect for the transfer, the structural information of the semantic representation is also another important aspect -models need to acquire the important semantic structural information on top of the language-specific <mark>syntactic information</mark>. We think that this would further improve the resulting performance.<br>",
    "Arabic": "معلومات تركيبية",
    "Chinese": "句法信息",
    "French": "informations syntaxiques",
    "Japanese": "構文情報",
    "Russian": "синтаксическая информация"
  },
  {
    "English": "syntactic parse",
    "context": "1: Next we describe the layout model p(z|x; θ ). We first use a fixed <mark>syntactic parse</mark> to generate a small set of candidate layouts, analogously to the way a semantic grammar generates candidate semantic parses in previous work (Berant and Liang, 2014). A semantic parse differs from a <mark>syntactic parse</mark> in two primary ways.<br>2: to <mark>syntactic parse</mark> parents , while ( 4 ) assigning semantic role labels .<br>",
    "Arabic": "تحليل تركيبي",
    "Chinese": "句法分析",
    "French": "analyse syntaxique",
    "Japanese": "構文解析",
    "Russian": "синтаксический разбор"
  },
  {
    "English": "syntactic parser",
    "context": "1: But these are not Open Domain Systems since the model needs to be retrained for every new domain. This is very difficult for pipeline systems like SRL where <mark>syntactic parser</mark>, shallow parser, POS tagger and then SRL need to be retrained.<br>2: In this respect, layout prediction is more like syntactic parsing than ordinary semantic parsing, and we can rely on an off-the-shelf <mark>syntactic parser</mark> to get most of the way there. In this work, syntactic structure is provided by the Stanford dependency parser (De Marneffe and Manning, 2008).<br>",
    "Arabic": "محلل نحوي",
    "Chinese": "句法分析器",
    "French": "analyseur syntaxique",
    "Japanese": "構文解析器",
    "Russian": "синтаксический анализатор"
  },
  {
    "English": "syntactic regularity",
    "context": "1: This model is a direct extension from our pilot study. It allows us to test the assumption that coherent discourse is characterized by <mark>syntactic regularities</mark> in adjacent sentences. We estimate the probabilities of pairs of syntactic items from adjacent sentences in the training data and use these probabilities to compute the coherence of new texts.<br>",
    "Arabic": "انتظام نحوي",
    "Chinese": "句法规律性",
    "French": "régularité syntaxique",
    "Japanese": "構文的規則性",
    "Russian": "синтаксическая регулярность"
  },
  {
    "English": "syntactic representation",
    "context": "1: In this paper, we present an approach to inducing <mark>syntactic representations</mark> that associate each token in the input with a discrete symbol from an arbitrarily-sized vocabulary, where the representations can be predicted incrementally in a strictly append-only manner.<br>2: We adapt two models of coherence to operate over the two <mark>syntactic representations</mark>.<br>",
    "Arabic": "تمثيل تركيبي",
    "Chinese": "句法表示",
    "French": "représentation syntaxique",
    "Japanese": "構文表現",
    "Russian": "синтаксическое представление"
  },
  {
    "English": "syntactic similarity",
    "context": "1: Another possibility is that a more spurious feature-such as lexical overlap-is responsible (Misra et al., 2020;Kassner and Schütze, 2020). To test this, we can correlate <mark>syntactic similarity</mark> and lexical overlap with accuracies on each example.<br>2: This distribution gives higher weight to syntactic items that are more likely for that communicative goal. Transitions between states record the common patterns in intentional structure for the domain. In this syntax-HMM, states h k are created by clustering the sentences from the documents in the training set by <mark>syntactic similarity</mark>.<br>",
    "Arabic": "تشابه تركيبي",
    "Chinese": "句法相似性",
    "French": "similarité syntaxique",
    "Japanese": "統語的類似性",
    "Russian": "синтаксическое сходство"
  },
  {
    "English": "syntactic structure",
    "context": "1: The use of hierarchical structures opens the possibility of making the model sensitive to <mark>syntactic structure</mark>. mention German es gibt, there is as an example of a good phrase pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy.<br>2: When researchers have peeked inside Transformer LM's pretrained representations, familiar <mark>syntactic structure</mark> (Hewitt and Manning, 2019;Jawahar et al., 2019;Lin et al., 2019;Wu et al., 2020), or a familiar order of linguistic operations (Jawahar et al., 2019;Tenney et al., 2019), has appeared. There is also evidence , notably from agreement attraction phenomena ( Linzen et al. , 2016 ) that transformerbased models pretrained on LM do acquire some knowledge of natural language syntax ( Gulordava et al. , 2018 ; Chrupała and Alishahi , 2019 ; Jawahar et al. , 2019 ; Lin et al. , 2019 ; Manning et al. , 2020 ; Hawkins et<br>",
    "Arabic": "البنية النحوية",
    "Chinese": "句法结构",
    "French": "structure syntaxique",
    "Japanese": "統語構造",
    "Russian": "синтаксическая структура"
  },
  {
    "English": "syntactic tree",
    "context": "1: Reduce operations are used to derive the structure of the <mark>syntactic tree</mark> of the short sentence. • drop operations are used to delete from the input list subsequences of words that correspond to syntactic constituents.<br>",
    "Arabic": "شجرة تركيبية",
    "Chinese": "句法树",
    "French": "arbre syntaxique",
    "Japanese": "構文木",
    "Russian": "синтаксическое дерево"
  },
  {
    "English": "syntax",
    "context": "1: A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for <mark>syntax</mark>. In this paper we present a lattice-theoretic representation for natural language <mark>syntax</mark>, called Distributional Lattice Grammars.<br>2: We introduce a model of coherence which captures the intentional discourse structure in text. Our work is based on the hypothesis that <mark>syntax</mark> provides a proxy for the communicative goal of a sentence and therefore the sequence of sentences in a coherent discourse should exhibit detectable structural patterns.<br>",
    "Arabic": "بناء الجملة",
    "Chinese": "句法",
    "French": "syntaxe",
    "Japanese": "統語論",
    "Russian": "синтаксис"
  },
  {
    "English": "syntax tree",
    "context": "1: Our system decomposes an inference problem into a sequence of atomic edits linking premise to hypothesis; predicts a lexical entailment relation for each edit using a statistical classifier; propagates these relations upward through a <mark>syntax tree</mark> according to semantic properties of intermediate nodes; and composes the resulting entailment relations across the edit sequence.<br>",
    "Arabic": "شجرة القواعد",
    "Chinese": "句法树",
    "French": "arbre syntaxique",
    "Japanese": "構文木",
    "Russian": "синтаксическое дерево"
  },
  {
    "English": "synthetic dataset",
    "context": "1: For LiRA (Carlini et al., 2022), we repeat the preparation of <mark>synthetic dataset</mark> N m times with different random seeds, and obtain N m shadow T , S and f S . We set N m = 256 for DM and N m = 64 for KIP because of its lower training efficiency.<br>2: We assume a strong adversary (e.g., honest-but-curious server), who although has no access to T but has the white-box access to both the <mark>synthetic dataset</mark> S synthesized from the target dataset T and the model f S trained on the <mark>synthetic dataset</mark>. The adversary also knows the data distribution of T . Adversary Capacity.<br>",
    "Arabic": "مجموعة البيانات الاصطناعية",
    "Chinese": "合成数据集",
    "French": "ensemble de données synthétiques",
    "Japanese": "合成データセット",
    "Russian": "синтетический набор данных"
  },
  {
    "English": "system identification",
    "context": "1: Our approach is different from model-based RL [25,4], which requires <mark>system identification</mark> to map the observations to a dynamics model, which is then solved for a policy. In many applications, including robotic manipulation and locomotion, accurate <mark>system identification</mark> is difficult, and modelling errors can severely degrade the policy performance.<br>2: In <mark>system identification</mark> theory, this corresponds to assuming 1-step observability (Van Overschee & De Moor, 1996) which is unduly restrictive for many partially observable real-world dynamical systems of interest.<br>",
    "Arabic": "تحديد النظام",
    "Chinese": "系统辨识",
    "French": "identification des systèmes",
    "Japanese": "システム同定",
    "Russian": "идентификация системы"
  },
  {
    "English": "t-test",
    "context": "1: To check whether the difference between DSDR and other approaches is significant, we perform the paired <mark>t-test</mark> between the ROUGE scores of DSDR and that of other approaches on both data sets. Table 3 and Table 4 show the associated p-values on DUC 2006 and DUC 2007 data sets respectively.<br>2: To better verify that PL-Rank-2 is the best choice, we performed a two-sided student <mark>t-test</mark> on the performance differences, the results are displayed in Table 2. We see that Pl-Rank-2 is significantly better compared to the other methods in all tested cases, with the single exception of PL-Rank-1 on the MSLR dataset.<br>",
    "Arabic": "اختبار t",
    "Chinese": "t检验",
    "French": "test t",
    "Japanese": "t検定",
    "Russian": "t-критерий"
  },
  {
    "English": "t5 model",
    "context": "1: We chose to finetune the state-of-the-art <mark>T5 model</mark> (with 770Mparameters, to be comparable to the size of the LM model), using the same hyperparameters as in (Raffel et al., 2020).<br>2: We finetuned the 770M-parameter pre-trained <mark>T5 model</mark> using the exact same hyperparameters as in (Raffel et al., 2020). We have used top-K random   sampling for decoding, with K = 15. We did the training and decoding for this model on Google TPU v3-8.<br>",
    "Arabic": "نموذج T5",
    "Chinese": "T5模型",
    "French": "modèle T5",
    "Japanese": "T5モデル",
    "Russian": "Модель T5"
  },
  {
    "English": "t5-base",
    "context": "1: For DROP, we use Numnet+ (Ran et al., 2019), a RoBERTa model  with specialized output heads for numerical reasoning. Numnet+ has 355M parameters, which is closer to <mark>T5-base</mark> (220M) than to T5-large (770M) in size.<br>2: For the fine-tuning experiments, we experiment with BERT-base, <mark>T5-base</mark>, T5-large, and T5-3B, and use the full training set of each dataset for finetuning. For the in-context learning experiments, we experiment with Codex. 5 We randomly sample 10/100/1,000 training examples from each dataset and use that as the pool for dynamic retrieval.<br>",
    "Arabic": "t5-base",
    "Chinese": "T5基础",
    "French": "T5-base",
    "Japanese": "t5ベース",
    "Russian": "t5-база"
  },
  {
    "English": "t5-base model",
    "context": "1: So when learning the logical semantics of complex fragments, transformers can employ this knowledge. Hence, we can hy-  pothesise that transformers at least partially learn to understand the essence of logical semantics of natural language. Table 3 also indicates a substantial difference in performance between the <mark>T5-base model</mark> and the T5-large model.<br>2: We use the Huggingface (Wolf et al., 2019) implementation <mark>T5-base model</mark>. The difference between our T5 baselines results and the results in Qiu et al. (2022) due to their usage of different intermediate representation for the output in order to keep our evaluation consistent with other previous work.<br>",
    "Arabic": "\"نموذج T5 الأساسي\"",
    "Chinese": "T5基础模型",
    "French": "modèle de base T5",
    "Japanese": "T5-baseモデル",
    "Russian": "базовая модель t5"
  },
  {
    "English": "t5-large model",
    "context": "1: So when learning the logical semantics of complex fragments, transformers can employ this knowledge. Hence, we can hy-  pothesise that transformers at least partially learn to understand the essence of logical semantics of natural language. Table 3 also indicates a substantial difference in performance between the T5-base model and the <mark>T5-large model</mark>.<br>",
    "Arabic": "نموذج t5 كبير",
    "Chinese": "T5-large 模型",
    "French": "modèle T5-large",
    "Japanese": "T5-large モデル",
    "Russian": "модель t5-large"
  },
  {
    "English": "tag recommendation",
    "context": "1: This learning method has been shown to be efficient for two related problem classes: standard item recommendation [7] and <mark>tag recommendation</mark> [8]. The complete algorithm is shown in figure 5.<br>",
    "Arabic": "توصية العلامات",
    "Chinese": "标签推荐",
    "French": "recommandation de tag",
    "Japanese": "タグ推薦",
    "Russian": "рекомендация тегов"
  },
  {
    "English": "tag sequence",
    "context": "1: Therefore, an intuitive way to match the <mark>tag sequence</mark> with the input sequence is to assign two tags to each word. We denote a training corpus S of M tuples of input sequences and <mark>tag sequence</mark>s {(w m , t m )} M m=1 .<br>",
    "Arabic": "تتابع العلامات",
    "Chinese": "标记序列",
    "French": "séquence d'étiquettes",
    "Japanese": "タグシーケンス",
    "Russian": "последовательность тегов"
  },
  {
    "English": "tagger",
    "context": "1: These precedence weights are man-ually crafted through qualitative evaluation (See Table 12 in Appendix for examples).P a (x t |y t ) quantifies the local neighborhood syntactic agreement between Bantu class markers. When there are two or more agreeing class markers in neighboring words, the <mark>tagger</mark> should be more confident of the agreeing parts of speech.<br>2: We ignore instances where the dependency is not in one of these categories, there are multiple GPE entities, the stripped questions is has 3 tokens or less, or there is disagreement between our parser and <mark>tagger</mark>. We use the implementations of  and Dozat and Manning (2017) from AllenNLP (Gardner et al., 2017).<br>",
    "Arabic": "مُصنِّف",
    "Chinese": "标注器",
    "French": "étiqueteur",
    "Japanese": "タガー",
    "Russian": "теггер"
  },
  {
    "English": "tagset",
    "context": "1: For each language under consideration, Petrov et al. (2011) provide a mapping λ from the fine-grained language specific POS tags in the foreign treebank to the universal POS tags. The supervised POS tagging accuracies (on this <mark>tagset</mark>) are shown in the last row of Table 2.<br>2: In order to compare with their results, we projected the <mark>tagset</mark> to the coarser set of 17 that they used in their experiments. On 24K tokens, our PROTO+SIM model scored 82.2%. When Smith and Eisner (2005) limit their tagging dictionary to words which occur at least twice, their best performing neighborhood model achieves 79.5%.<br>",
    "Arabic": "مجموعة الوسوم",
    "Chinese": "词性标记集",
    "French": "ensemble d'étiquettes",
    "Japanese": "タグセット",
    "Russian": "набор тегов"
  },
  {
    "English": "tail entity",
    "context": "1: β s n = (W r r n ) tanh(W h h n + W t t n ),(6) \n where (h n , r n , t n ) = k n , W h , W r , W t are weight matrices for head entities, relations, and <mark>tail entities</mark>, respectively.<br>",
    "Arabic": "الكيان الذيلي",
    "Chinese": "尾实体",
    "French": "entité de queue",
    "Japanese": "テイルエンティティ",
    "Russian": "хвостовая сущность"
  },
  {
    "English": "tangent space",
    "context": "1: they use pairs or tuples of points, with the goal to explicitly model the <mark>tangent space</mark>, while it is 1 J (K) is the product of the Jacobians of each encoder (see Eq. 3) in the stack. It suffices to compute its leading dM SVD vectors and singular values.<br>2: These spurious components are naturally avoided by sampling a point on Inc ′ . For implicit symbolic calculations, the spurious solutions may be eliminated by including inequations enforcing nonvanishing of the minors of size one smaller. The following algorithm checks minimality of a pointline problem locally; geometrically this amounts to passing to the <mark>tangent space</mark> of Inc ′ .<br>",
    "Arabic": "فضاء الملامس",
    "Chinese": "切线空间",
    "French": "espace tangent",
    "Japanese": "接線空間",
    "Russian": "касательное пространство"
  },
  {
    "English": "tanh activation function",
    "context": "1: We expect \"player\" and \"buyer\" to have similar compressed vectors, because they share syntactic roles, but we should fail  to predict that they have different stems \"play\" and \"buy.\" The classifier is a feedforward neural network with <mark>tanh activation function</mark>, and the last layer is a softmax over the stem vocabulary.<br>",
    "Arabic": "دالة التنشيط tanh",
    "Chinese": "tanh激活函数",
    "French": "fonction d'activation tanh",
    "Japanese": "tanh活性化関数",
    "Russian": "функция активации tanh"
  },
  {
    "English": "target",
    "context": "1: Specifically, an example (xi, yi) consists of two vectors, one for features xi and the other for <mark>targets</mark> yi 5 .<br>",
    "Arabic": "هدف",
    "Chinese": "目标",
    "French": "cible",
    "Japanese": "ターゲット",
    "Russian": "цель"
  },
  {
    "English": "target classifier",
    "context": "1: The existing domain adaptation methods [27,13] are not specifically designed for our setting with multiple heterogeneous domains, so these methods generally cannot work well. Recently , Aytar and Zisserman [ 1 ] investigated the single source domain adaptation problem and tried to utilize the pre-learnt source classifier u φ ( x ) to learn the <mark>target classifier</mark> by regularizing u and the weight vector w T of the <mark>target classifier</mark> , i.e. , w T − γu 2 2 , where the parameter γ controls the amount of<br>2: We have proposed a new framework for visual event recognition in consumer videos by leveraging a large number of freely available web videos (e.g., from YouTube) and web images (e.g., from Google/Bing image search). This task is formulated as a new multi-domain adaptation problem with heterogeneous sources. By introducing a new <mark>target classifier</mark> and a new regularizer based on the weights of heterogeneous source domains , our method called Multi-domain Adaptation with Heterogeneous Sources ( MDA-HS ) can simultaneously seek the optimal weights for different source domains with different types of features , infer the labels of unlabeled target domain data based on multiple types of features , and learn the<br>",
    "Arabic": "مصنف الهدف",
    "Chinese": "目标分类器",
    "French": "classificateur cible",
    "Japanese": "ターゲット分類器",
    "Russian": "целевой классификатор"
  },
  {
    "English": "target distribution",
    "context": "1: A normalizing flow maps a prior (source) distribution to a <mark>target distribution</mark> via the change of variables formula (Rezende and Mohamed, 2015;Dinh et al., 2016;Papamakarios et al., 2019).<br>2: Before we analyze the sampling bias and sampling recall of the resulting samplers, let us identify the exact form of the <mark>target distribution</mark>, trial distribution, and approximate trial distribution employed by the approximate MH and MD procedures. Let F be the connected component of G P + to which the start vertex x 0 of the random walk belongs.<br>",
    "Arabic": "التوزيع المستهدف",
    "Chinese": "目标分布",
    "French": "distribution cible",
    "Japanese": "目標分布",
    "Russian": "целевое распределение"
  },
  {
    "English": "target domain",
    "context": "1: Following the terminology of domain adaptation, we refer to the consumer video domain as the <mark>target domain</mark>, as well as consider the web image and video domains as the heterogeneous source domains. Note that the target data have multiple views of features, while the data from each source domain has only one view of feature.<br>2: knowledge transferred from the source domain to the <mark>target domain</mark> . Inspired by their method [ 1 ] , we make use of a set of pre-learnt source classifiers f s ( x s ) = u s φ s ( x s ) 's trained by using the training data from each individual source domain and propose a new regularizer as follows for multiple heterogeneous source domains by linearly combining the distances between the<br>",
    "Arabic": "المجال المستهدف",
    "Chinese": "目标域",
    "French": "domaine cible",
    "Japanese": "ターゲットドメイン",
    "Russian": "целевая область"
  },
  {
    "English": "target function",
    "context": "1: 1B) to avoid overfitting. The easiest examples provide coarse-grained information about the <mark>target function</mark>, while the hard examples provide fine-grained information about the <mark>target function</mark> which can prevent the model from learning if one starts with lots of data.<br>2: A small value of d(∞) guarantees that inputs on which the <mark>target function</mark> assumes the same value are mapped to nearby network states and thus a linear readout is able to assign them to the same class irrespectively of their irrelevant remote history. For m = 1, as can be seen in Fig. 5 the region in log ( σ ) space where both conditions for good performance are present decreases for growing K. In contrast , for m > 2 a reverse effect is observed : for increasing K the parameter range for σ fulfilling the two opposing conditions for good performance grows moderately resulting in a large region of high p ∞ for high in-degree<br>",
    "Arabic": "الدالة المستهدفة",
    "Chinese": "目标函数",
    "French": "fonction cible",
    "Japanese": "目標関数",
    "Russian": "целевая функция"
  },
  {
    "English": "target instance",
    "context": "1: We prove that implicit to the <mark>target instance</mark> is a stricter dominance relation in the sense that a path π in the <mark>target instance</mark> may dominate a path π in the <mark>target instance</mark> while the converse is not necessarily true. To generate the transformed instance we use two real parameters, α and β in (0, 1].<br>",
    "Arabic": "مثيل الهدف",
    "Chinese": "目标实例",
    "French": "instance cible",
    "Japanese": "ターゲットインスタンス",
    "Russian": "целевой экземпляр"
  },
  {
    "English": "target model",
    "context": "1: , 2023 ; Jung et al. , 2024 ) . Distillation methods (Hinton et al., 2015;Ba and Caruana, 2014) generally train a <mark>target model</mark> using expert demonstrations unaware of the <mark>target model</mark>'s capability.<br>2: Although ARR training includes both base model and <mark>target model</mark>, only 82.1 million parameters of the <mark>target model</mark> are updated. Run time. The average training time for each model in our experiments is approximately 6 hours. In total, we have trained 117 models (including model variations in learning objectives, hyperparameters, and random seed.)<br>",
    "Arabic": "نموذج الهدف",
    "Chinese": "目标模型",
    "French": "modèle cible",
    "Japanese": "ターゲットモデル",
    "Russian": "целевая модель"
  },
  {
    "English": "target network",
    "context": "1: We use τ = 0.005 for <mark>target network</mark> update from the work of Haarnoja et al. (2018). The discount is set to the common γ = 0.99. 7). The plots show the policy performance and TD error across optimization epochs of ATAC with the hopper-medium-replay, hopper-medium, and hopper-medium-expert datasets from top to buttom.<br>2: t , and the n-step returns G (n) t are calculated with bootstrapped values estimated by a <mark>target network</mark> (Mnih et al. 2015) with parameters copied periodically from θ c .<br>",
    "Arabic": "الشبكة المستهدفة",
    "Chinese": "目标网络",
    "French": "réseau cible",
    "Japanese": "ターゲットネットワーク",
    "Russian": "целевая сеть"
  },
  {
    "English": "target node",
    "context": "1: In this section we provide a formal version of the informal lower bound of Theorem 2 on the number of node expansions required before reaching a <mark>target node</mark> n * .<br>",
    "Arabic": "العقدة المستهدفة",
    "Chinese": "目标节点",
    "French": "nœud cible",
    "Japanese": "目標ノード",
    "Russian": "целевой узел"
  },
  {
    "English": "target policy",
    "context": "1: Policy-matching This policy can be seen as a stochastic policy which takes the Q-maximizing action with probability 1 − ε and otherwise picks an action uniformly at random. The level of entropy in this policy is regulated by the meta-learner. We define a TB by defining a <mark>target policy</mark> under qx, wherex is given by taking L update steps.<br>2: Here c achieves close to the optimal variance (Greensmith et al., 2004) when it is set exactly equal to the state-value function V π (s i ) = E π q i for the <mark>target policy</mark> π starting in state s i . The situation becomes slightly more complicated when generalizing to modular policies built by sequencing subpolicies.<br>",
    "Arabic": "سياسة الهدف",
    "Chinese": "目标策略",
    "French": "politique cible",
    "Japanese": "目標方針",
    "Russian": "целевая стратегия"
  },
  {
    "English": "target sentence",
    "context": "1: The source sentence is denoted as x, while the <mark>target sentence</mark> is denoted as y. In order to specify the source and target languages, two language tokens are added at the beginning of each source and <mark>target sentence</mark>, respectively.<br>2: We start with the generative process for a source sentence and its alignment with a <mark>target sentence</mark>. Then we describe individual models employed by this generation scheme.<br>",
    "Arabic": "الجملة الهدفية",
    "Chinese": "目标句",
    "French": "phrase cible",
    "Japanese": "ターゲット文",
    "Russian": "целевое предложение"
  },
  {
    "English": "target sequence",
    "context": "1: We also add language ID tokens to our vocabulary, which are prepended to each source and <mark>target sequence</mark> to indicate the target language (Johnson et al., 2017).<br>2: As the <mark>target sequence</mark> grows, the errors accumulate among the sequence and the model has to predict under the condition it has never met at training time. Intuitively, to address this problem, the model should be trained to predict under the same condition it will face at inference.<br>",
    "Arabic": "تسلسل الهدف",
    "Chinese": "目标序列",
    "French": "séquence cible",
    "Japanese": "ターゲットシーケンス",
    "Russian": "целевая последовательность"
  },
  {
    "English": "target task",
    "context": "1: Each element c i for a transfer is the product of the importance of its <mark>target task</mark> and its transfer performance: \n c i := r target(i) • p i . (3) \n<br>",
    "Arabic": "المهمة المستهدفة",
    "Chinese": "目标任务",
    "French": "tâche cible",
    "Japanese": "目標タスク",
    "Russian": "целевая задача"
  },
  {
    "English": "target token",
    "context": "1: For each <mark>target token</mark> t i , the representations of its left and right context are concatenated and used as query to an attention module before a final softmax layer. It is trained on the large parallel corpora provided as additional data by the WMT shared task organizers.<br>2: The attention mask is set such that each <mark>target token</mark> can only attend to all source tokens and preceding <mark>target token</mark>s.<br>",
    "Arabic": "الهدف المرموز",
    "Chinese": "目标词元",
    "French": "jeton cible",
    "Japanese": "対象トークン",
    "Russian": "целевой токен"
  },
  {
    "English": "target variable",
    "context": "1: Rather than assuming any particular parametric relationship between the predictor and <mark>target variables</mark>, kernel regression assumes only that the value of the target vari-able is a smooth function of the value of the predictors.<br>",
    "Arabic": "المتغير المستهدف",
    "Chinese": "目标变量",
    "French": "variable cible",
    "Japanese": "ターゲット変数",
    "Russian": "целевая переменная"
  },
  {
    "English": "target vector",
    "context": "1: We describe in this section how to train the RNTN model. As mentioned above, each node has a softmax classifier trained on its vector representation to predict a given ground truth or <mark>target vector</mark> t. We assume the target distribution vector at each node has a 0-1 encoding.<br>2: Causal Pruning: A systematic and quantitative pruning of the input vector based on objectively assessed causal relationships to subsets of the <mark>target vector</mark> has been proposed as an attractive preprocessing strategy, as it helps remove spurious correlations due to confounding variables and optimize the machine learning (ML) algorithm [17].<br>",
    "Arabic": "متجه الهدف",
    "Chinese": "目标向量",
    "French": "vecteur cible",
    "Japanese": "目標ベクトル",
    "Russian": "целевой вектор"
  },
  {
    "English": "target vocabulary",
    "context": "1: The probability distribution P j over all the words in the <mark>target vocabulary</mark> is produced conditioned on the embedding of the previous ground truth word, the source context vector and the hidden state \n<br>2: Formally, we denote a speech-to-text task training sample as a (x, y) pair. x = x 1:T and y = y 1:U are the speech input features and target text tokens, respectively. T and U are the corresponding sequence lengths. y u ∈ V and V is the <mark>target vocabulary</mark>.<br>",
    "Arabic": "المفردات المستهدفة",
    "Chinese": "目标词汇",
    "French": "vocabulaire cible",
    "Japanese": "対象語彙",
    "Russian": "целевой словарь"
  },
  {
    "English": "target word",
    "context": "1: (a) Given the sequence of words of the source sentence already generated in step 1, the alignment model marks each source word as either aligned or unaligned. If a source word is aligned, the model also generates the <mark>target word</mark>. (b) Unaligned <mark>target word</mark>s are generated.<br>2: We consider each word surrounding the <mark>target word</mark> w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a −2 , b −1 , d +1 and e +2 .<br>",
    "Arabic": "الكلمة المستهدفة",
    "Chinese": "目标词",
    "French": "mot cible",
    "Japanese": "対象語 (たいしょうご)",
    "Russian": "целевое слово"
  },
  {
    "English": "target-to-source model",
    "context": "1: Back-translation is to obtain additional sourceside data by translating a target-language monolingual dataset with <mark>target-to-source model</mark> (Sennrich et al., 2016a). The generated source sentences are then paired with their target side into a synthetic parallel dataset.<br>",
    "Arabic": "نموذج من الهدف إلى المصدر",
    "Chinese": "目标到源语言模型",
    "French": "modèle cible-vers-source",
    "Japanese": "ターゲットからソースへのモデル",
    "Russian": "модель цели-источника"
  },
  {
    "English": "task",
    "context": "1: (K) τ (w) is the K-step update on <mark>task</mark> τ given (f τ , h τ , x τ ). Since p(τ ) is independent of w, this update becomes w = w −βE τ [∇ w f τ (x (K) \n τ (w))], i.e.<br>2: In BIG-Bench alone, there are ≥ 220 <mark>task</mark>s, ∼ 40 metrics per <mark>task</mark>, ∼ 10 model families, for a total of ∼ 10 6 <mark>task</mark>metric-model family triplets, meaning probability that no <mark>task</mark>-metric-model family triplet exhibits an emergent ability by random chance might be small.<br>",
    "Arabic": "مهمة",
    "Chinese": "任务",
    "French": "tâche",
    "Japanese": "タスク",
    "Russian": "задача"
  },
  {
    "English": "task adaptation",
    "context": "1: This variant retains the task-specific parameters of ours, thus identical in terms of <mark>task adaptation</mark>; it utilizes the separate sets of task-specific biases in the image encoder to learn the training tasks T train , and fine-tune it on the support set of T test in the test time. The results are in Table 2.<br>2: An effective way to mitigate issues in <mark>task adaptation</mark> is to expose the model to counterexamples of spurious correlations (Kaushik et al., 2020).<br>",
    "Arabic": "تكييف المهمة",
    "Chinese": "任务适应",
    "French": "adaptation des tâches",
    "Japanese": "タスク適応",
    "Russian": "адаптация задачи"
  },
  {
    "English": "task model",
    "context": "1: As in all learning, a core challenge in task learning is identifying the appropriate generalizations, which are key to allow the learned <mark>task model</mark> to be successfully applied to new arguments in new contexts.<br>",
    "Arabic": "نموذج المهمة",
    "Chinese": "任务模型",
    "French": "modèle de tâche",
    "Japanese": "タスクモデル",
    "Russian": "задачная модель"
  },
  {
    "English": "task-orient dialog system",
    "context": "1: Intent classification (IC) and slot labeling (SL) have been studied for several decades as fundamental building blocks of <mark>task-oriented dialog systems</mark>, dating back at least to the creation of the ATIS corpus (Price, 1990).<br>2: Recent <mark>task-oriented dialog systems</mark> obtained great successes in building personal assistants for high resource language such as English, but extending these systems to a global audience is challenging due to the need for annotated data or machine translation systems in the target language.<br>",
    "Arabic": "نظام الحوار الموجه نحو المهام",
    "Chinese": "任务导向对话系统",
    "French": "système de dialogue orienté tâche",
    "Japanese": "タスク指向型対話システム",
    "Russian": "система диалога, ориентированная на задачи"
  },
  {
    "English": "task-orient dialogue system",
    "context": "1: Despite formal differences, these representations can generally be represented as graphs. We will focus on the dataflow graph (Semantic Machines et al., 2020), which represents an executable program in response to a user's utterance in a taskoriented dialogue system (Zettlemoyer and Collins, 2009).<br>",
    "Arabic": "نظام حوار موجه نحو المهمة",
    "Chinese": "面向任务对话系统",
    "French": "système de dialogue orienté tâche",
    "Japanese": "タスク指向対話システム",
    "Russian": "диалоговая система, ориентированная на задачи"
  },
  {
    "English": "task-specific model",
    "context": "1: Thereafter, we begin with the easiest curriculum. For each training instance, we retrieve the cached augmentation that has an equivalent difficulty measure with the current difficulty level. We then invoke the <mark>task-specific model</mark> trainer to train the downstream task model with the retrieved training augmentation.<br>",
    "Arabic": "النموذج الخاص بالمهمة",
    "Chinese": "任务特定模型",
    "French": "modèle spécifique à la tâche",
    "Japanese": "タスク固有モデル",
    "Russian": "модель, специфическая для задачи"
  },
  {
    "English": "taxonomy",
    "context": "1: Relying on these categories while creating an initial draft of the new <mark>taxonomy</mark>, we did not use the data the authors provided in our editorial studies and model training process in order to fairly compare the two taxonomies.<br>2: In our algorithm word sense disambiguation is an implicit side-effect of our algorithm; since our algorithm chooses to add the single link which, with its implied links, yields the most likely <mark>taxonomy</mark>, and since each distinct synset in WordNet has a different immediate neighborhood of relations, our algorithm simply disambiguates each node based on its surrounding structural information.<br>",
    "Arabic": "التصنيف",
    "Chinese": "分类学",
    "French": "taxonomie",
    "Japanese": "分類法",
    "Russian": "таксономия"
  },
  {
    "English": "teacher forcing",
    "context": "1: We then feed the edited rationales to the decoder of the student directly (as <mark>teacher forcing</mark>) and see if the student will act accordingly, i.e., predict more badly (or accurately) due to the worse (or better) rationales.<br>2: For each house, 20 episodes are sampled such that all shuffled objects are in the same room where the agent is initially spawned. We train with 2 • 10 5 steps of <mark>teacher forcing</mark> and 2 million steps of dataset aggregation [99], followed by about 180 million steps of behavior cloning.<br>",
    "Arabic": "إجبار المعلم",
    "Chinese": "教师强制",
    "French": "enseignement forcé",
    "Japanese": "教師強制",
    "Russian": "Принудительное обучение"
  },
  {
    "English": "teacher network",
    "context": "1: Specifically, we use superscripts s and t to denote the quantized student network and fullprecision <mark>teacher network</mark>, respectively. Denote the length-n input sequence of tokens as (t 1 , t 2 , • • • , t n ).<br>2: Intuitively, for each token in the quantized network, L dist only encourages it to mimic its corresponding token of the <mark>teacher network</mark>, while L cont not only pulls it close to its positive, but also pushes it away from its negatives.<br>",
    "Arabic": "شبكة المعلم",
    "Chinese": "教师网络",
    "French": "réseau enseignant",
    "Japanese": "教師ネットワーク",
    "Russian": "учительская сеть"
  },
  {
    "English": "temperature parameter",
    "context": "1: S neg = r=u,v n j=1,j =i exp(S z (v) i , z (r)j \n τ )W j,r , and τ represents the <mark>temperature parameter</mark> that controls the concentration extent of the distribution (Wu et al. 2018).<br>2: Boltzmann Rationality: We assume that P(π(s) = a) ∝ e βQ ⋆ (s,a) , where β is a <mark>temperature parameter</mark> (e.g. Ramachandran and Amir ( 2007)).<br>",
    "Arabic": "معامل الحرارة",
    "Chinese": "温度参数",
    "French": "paramètre de température",
    "Japanese": "温度パラメータ",
    "Russian": "параметр температуры"
  },
  {
    "English": "temperature scaling",
    "context": "1: Prompted by this observation, we calibrate these models via <mark>temperature scaling</mark> (Guo et al., 2017) and find that the correlation between the CLML and BMA test accuracy for these well-calibrated models improves in Figure 6(b).<br>2: 1999)). Here, we choose to study how calibration affects InfoLM by relying on temperature 2 scaling motivated by simplicity and speed.<br>",
    "Arabic": "تحجيم درجة الحرارة",
    "Chinese": "温度缩放",
    "French": "\"La mise à l'échelle de température\"",
    "Japanese": "温度スケーリング",
    "Russian": "шкалирование температуры"
  },
  {
    "English": "template",
    "context": "1: 1, given the parameter p, one can generate the deformed image I p from the <mark>template</mark> T . This is done by assigning every pixel y of the deformed image I p with the pixel value on location x = W −1 (y; p) of the <mark>template</mark> T .<br>2: • Policy : A policy ( | , X) that generates a distribution over potential assignments of spans to slots in the current <mark>template</mark> of type ; \n<br>",
    "Arabic": "قالب",
    "Chinese": "模板",
    "French": "politique",
    "Japanese": "テンプレート",
    "Russian": "шаблон"
  },
  {
    "English": "template model",
    "context": "1: As a way to reduce the parameter space and overcome the complexity of the problems, generative 3D <mark>template models</mark> have been proposed in each field, for example the methods of [2,33,37] in body motion capture, the method of [13] for facial motion capture, and very recently, the combined body+hands model of Romero et al.<br>",
    "Arabic": "نموذج القالب",
    "Chinese": "模板模型",
    "French": "modèle de gabarit",
    "Japanese": "テンプレートモデル",
    "Russian": "шаблонная модель"
  },
  {
    "English": "template-matching",
    "context": "1: This paper addresses the problem of automatically detecting specific patterns or shapes in timeseries data. A novel and flexible approach is proposed based on segmental semi-Markov models. Unlike dynamic time-warping or <mark>template-matching</mark>, the proposed framework provides a systematic and coherent framework for leveraging both prior knowledge and training data.<br>2: We would like to detect the same pattern in the interferometry data of future runs, e.g., Figure 1(b). For comparison, we ran both the <mark>template-matching</mark> (Figure 2) and dynamic time warping (Figure 6) techniques.<br>",
    "Arabic": "مطابقة القالب",
    "Chinese": "模板匹配",
    "French": "mise en correspondance de modèles",
    "Japanese": "テンプレートマッチング",
    "Russian": "сопоставление шаблонов"
  },
  {
    "English": "temporal derivative",
    "context": "1: } . Note that the spatial gradient of the image is independent of motion and corresponds to the derivative of E 0 with respect to u. We will simply denote it as ∇ u E = (E u , E v ) . The <mark>temporal derivative</mark>, E t , as well as p and q, depend on the motion.<br>",
    "Arabic": "المشتقة الزمنية",
    "Chinese": "时间导数",
    "French": "dérivée temporelle",
    "Japanese": "時間微分",
    "Russian": "временная производная"
  },
  {
    "English": "temporal difference",
    "context": "1: max a Q T (o i , a;h i ) − Q T (o i , a i ;h i ) LG: Loss Gain Student's task-level loss L(θ i ) reduction L(θ i t ) − L(θ i t+1 ) \n LGG: Loss Gradient Gain Student's task-level policy gradient magnitude   \n ||∇ θ i L ( θ i ) || 2 2 TDG : TD Gain Student 's <mark>temporal difference</mark> ( TD ) error δ i reduction |δ i t | − |δ i t+1 | VEG : Value Estimation Gain Student 's value estimateV ( θ i ) gain above threshold τ 1 ( V ( θ i ) > τ ) Agent j<br>2: the <mark>temporal difference</mark> t j − t i , we can define the flexibility associated with this difference as the length of the corresponding interval 3 : \n flex H (t i , t j ) = D S [i, j] + D S [j, i].<br>",
    "Arabic": "فرق زمني",
    "Chinese": "时间差分",
    "French": "différence temporelle",
    "Japanese": "時間差",
    "Russian": "временная разница"
  },
  {
    "English": "temporal difference learning",
    "context": "1: Crucially, regardless of the bootstrapping strategy, we do not backpropagate through the target. Akin to <mark>temporal difference learning</mark> in RL (Sutton, 1988), the target is a fixed goal that the meta-learner should try to produce within the K-step budget.<br>2: An efficient way to do this is temporal difference (TD) learning [10], which uses an error signal that correlates with the activity of dopaminergic neurons in the Substantia Nigra [11].<br>",
    "Arabic": "تعلم الفرق الزمني",
    "Chinese": "时序差分学习",
    "French": "apprentissage par différence temporelle",
    "Japanese": "時間差学習",
    "Russian": "обучение на основе временных различий"
  },
  {
    "English": "temporal drift",
    "context": "1: Furthermore, the large 20-year gap helps us focus on not only temporal deterioration, but also if the extensive test reuse leads to adaptive overfitting. We present evidence in support of the hypothesis that most performance degradation is due to <mark>temporal drift</mark> and not adaptive overfitting.<br>2: In Figure 5, the ∆F 1 shows an upward trend with a correlation coefficients of 0.55, indicating a moderate positive correlation between generalization and the year of the pre-training corpora. This suggests that generalization is affected by the effect of <mark>temporal drift</mark>.<br>",
    "Arabic": "التحول الزمني",
    "Chinese": "时间漂移",
    "French": "dérive temporelle",
    "Japanese": "時間的ドリフト",
    "Russian": "временной дрейф"
  },
  {
    "English": "temporal fusion",
    "context": "1: Tracking can be formulated in a probabilistic framework in both the feature-driven (Terzopoulos and Szeliski, 1992) and intensity-driven (Storvik, 1994) settings. The probabilistic formulation has the attraction that uncertainty is handled in a systematic fashion, allowing principled handling of sensor fusion and <mark>temporal fusion</mark>.<br>",
    "Arabic": "اندماج زمني",
    "Chinese": "时间融合",
    "French": "fusion temporelle",
    "Japanese": "時間的融合",
    "Russian": "временное объединение"
  },
  {
    "English": "temporal locality",
    "context": "1: The first policy is 'random replacement'. It randomly chooses the block to be evicted. Next, a recency-based method, evicts the least recently used block. This method mainly exploits <mark>temporal locality</mark>. Finally, a frequency-based method evicts the least frequently used block, thereby exploiting spatial locality. This study implements and tests these three methods.<br>",
    "Arabic": "القرب الزمني",
    "Chinese": "时间局部性",
    "French": "localité temporelle",
    "Japanese": "時間的局所性",
    "Russian": "временная локальность"
  },
  {
    "English": "temporal logic",
    "context": "1: These methods often use different logics, such as deontic logic (van der Torre 2003;Bringsjord, Arkoudas, and Bello 2006), <mark>temporal logic</mark> (Wooldridge and Van Der Hoek 2005;Atkinson and Bench-Capon 2006;Dennis et al.<br>",
    "Arabic": "المنطق الزمني",
    "Chinese": "时序逻辑",
    "French": "logique temporelle",
    "Japanese": "時相論理",
    "Russian": "временная логика"
  },
  {
    "English": "temporal reasoning",
    "context": "1: Often cognitive models are used either by performing simulation, or by <mark>temporal reasoning</mark> methods; e.g. [8]. In this paper a third way of using such models is introduced, namely by deriving more indirect relations from these models.<br>",
    "Arabic": "الاستدلال الزمني",
    "Chinese": "时序推理",
    "French": "raisonnement temporel",
    "Japanese": "時間的推論",
    "Russian": "временное рассуждение"
  },
  {
    "English": "temporal variable",
    "context": "1: 3 Replace each formula of the highest stratum n of X' ϕi: \n at(ai, t) ↔ ψip(at1,…, atm) by ϕI δ with renaming of <mark>temporal variables</mark> if required, where δ = {atk\\ body(ϕk) such that ϕk ∈ X' and head(ϕk)=atk}.<br>",
    "Arabic": "متغير زمني",
    "Chinese": "时间变量",
    "French": "variable temporelle",
    "Japanese": "時間変数",
    "Russian": "временная переменная"
  },
  {
    "English": "tensor",
    "context": "1: Require : <mark>Tensor</mark> Q ∈ R m×d , K ∈ R n×d , V ∈ R n×d 1 : print set hyperparameter c , u = c ln m and U = m ln n 2 : randomly select U dot-product pairs from K asK 3 : set the sample scoreS = QK 4 : compute the measurement M = max ( S )<br>",
    "Arabic": "المُجَانِب (Tensor)",
    "Chinese": "张量",
    "French": "tenseur",
    "Japanese": "テンソル",
    "Russian": "тензор"
  },
  {
    "English": "tensor decomposition",
    "context": "1: On the other hand, one of the most successful model classes are factorization methods (MF) based on matrix or <mark>tensor decomposition</mark>. The best approaches [3,4] for the 1M$ Netflix challenge 1 are based on this model class.<br>2: Also on the ECML/PKDD discovery challenge 2 for tag recommendation, a factorization model based on <mark>tensor decomposition</mark> has outperformed the other approaches [8]. These models learn the general taste of the user disregarding sequential information.<br>",
    "Arabic": "تفكيك التانسور",
    "Chinese": "张量分解",
    "French": "décomposition de tenseur",
    "Japanese": "テンソル分解",
    "Russian": "тензорное разложение"
  },
  {
    "English": "tensor factorization",
    "context": "1: The class of compatible models are broadly defined. [Zhang et al., 2014;Bauman et al., 2017] are based on matrix factorization, while [Chen et al., 2016;Wang et al., 2018a] are based on <mark>tensor factorization</mark>. Others combine matrix factorization with topic modeling [Wu and Ester, 2015].<br>2: Nonrigid 3D structure-from-motion and 2D optical flow can both be formulated as <mark>tensor factorization</mark> problems. The two problems can be made equivalent through a noisy affine transform, yielding a combined nonrigid structure-from-intensities problem that we solve via structured matrix decompositions.<br>",
    "Arabic": "تحليل الموتر",
    "Chinese": "张量分解",
    "French": "factorisation tensorielle",
    "Japanese": "テンソル因子分解",
    "Russian": "тензорное разложение"
  },
  {
    "English": "tensor field",
    "context": "1: ðÞ is defined as a divergence operator acting on matrices and returning vectors: \n if D ¼ ðd ij Þ; div ! ðDÞ ¼ divðd 11 d 12 Þ T divðd 21 d 22 Þ T : \n Then, an additional term rI T i div ! ðDÞ appears , connected to the spatial variation of the <mark>tensor field</mark> D. It may perturb the smoothing behavior given by the first part trace ðDH i Þ , which actually corresponds to a local smoothing directed by the spectral elements of D. As a result , the divergencebased equation ( 2 ) may smooth the image I with weights and directions that are<br>",
    "Arabic": "حقل تنسور",
    "Chinese": "张量场",
    "French": "champ tensoriel",
    "Japanese": "テンソル場",
    "Russian": "тензорное поле"
  },
  {
    "English": "tensor product",
    "context": "1: Define y := Γ −1/2 x ∼ N (0, I d ), and recall that x = Ax + w where w ∼ N (0, W ). Using the fact that vec(ABC) = (C ⊗ A)vec(B) for any matrices of compatible shape, we have \n g : = vec x x = vec ( xx A ) + vec ( xw ) = ( A ⊗ I d ) vec ( xx ) + vec ( xw ) = ( A ⊗ I d ) vec ( Γ 1/2 yy Γ 1/2 ) + vec ( xw ) = ( A ⊗ I d ) ( Γ 1/2 ⊗<br>2: ⊗ denotes Kronecker (tensor) product; denotes Hadamard (element-wise) product; ⊕ denotes tiled addition, e.g., A 6×2 ⊕ B 2×2 = A 6×2 + (1 3×1 ⊗ B 2×2 ).<br>",
    "Arabic": "ناتج التنسور",
    "Chinese": "张量积",
    "French": "produit tensoriel",
    "Japanese": "テンソル積",
    "Russian": "тензорное произведение"
  },
  {
    "English": "term frequency",
    "context": "1: Thus, we also report the performances of the summary scoring functions from several standard baselines: Edmundson (Edmundson, 1969) which scores sentences based on 4 methods: <mark>term frequency</mark>, presence of cue-words, overlap with title and position of the sentence.<br>2: When the system receives a class of news articles as an event, it calculates sum of <mark>term frequency</mark> -inverse document frequency (tf•idf) as shown below: \n tf • idf (w, e) = N −1 i=0 tf • idf (w, i) ( 1 ) \n<br>",
    "Arabic": "تردد المصطلح",
    "Chinese": "词频",
    "French": "fréquence des termes",
    "Japanese": "用語の頻度 (term frequency)",
    "Russian": "частота термина"
  },
  {
    "English": "terminal node",
    "context": "1: Moreover, the probability of reaching a <mark>terminal node</mark> z ∈ Z(I, a) when following the recommendations is defined as follows: \n q µ (z) :=   π∈Π(z) µ(π)   p c (z). (5) \n<br>",
    "Arabic": "العقدة الطرفية",
    "Chinese": "终端节点",
    "French": "nœud terminal",
    "Japanese": "終端ノード (shūtan nōdo)",
    "Russian": "терминальный узел"
  },
  {
    "English": "terminal state",
    "context": "1: If a goal state has been detected, that <mark>terminal state</mark> is added to the set of goal states S g . The detection of a <mark>terminal state</mark> also marks the end of an episode ξ. The previous goal state type is also updated for the next episode.<br>2: the system should reach the <mark>terminal state</mark> q n on stack level 0 and satisfy the goal condition G t of P t before execution proceeds on the next problem P t+1 ∈ P. To solve P n,m , a plan hence has to simulate the execution of H on all planning problems in P.<br>",
    "Arabic": "الحالة النهائية",
    "Chinese": "终止状态",
    "French": "état terminal",
    "Japanese": "終了状態 (shūryō jōtai)",
    "Russian": "конечное состояние"
  },
  {
    "English": "termination condition",
    "context": "1: In Algorithm 1 (Line 1.4), we utilize a parameter θ to control the <mark>termination condition</mark> -instead of using L G < U G , we use L G < θU G as the <mark>termination condition</mark>, where θ ∈ (0, 1]. When L and U are close enough, the search process terminates.<br>",
    "Arabic": "شرط الإنهاء",
    "Chinese": "终止条件",
    "French": "condition d'arrêt",
    "Japanese": "終了条件",
    "Russian": "условие завершения"
  },
  {
    "English": "termination criterion",
    "context": "1: The most of the patterns described in previous section was applied. After the system was implemented, various changes to the algorithm were introduced to test the ease of modification. The range of changes included: \n implementing different selection and crossover methods, -changing the <mark>termination criteria</mark>, -introducing different chromosome representations, -modifying the evaluation method.<br>2: • In the current implementation we use a simple <mark>termination criteria</mark>. Iteration is stopped when the reduction in function value f (U k ) − f (U k+1 ) is deemed sufficiently small (≤ 10 −6 ).<br>",
    "Arabic": "معيار الإنهاء",
    "Chinese": "终止准则",
    "French": "critère d'arrêt",
    "Japanese": "終了基準",
    "Russian": "критерий завершения"
  },
  {
    "English": "test accuracy",
    "context": "1: Even when we significantly collapse the input channels (through averaging or throwing away most of them), most of the single model <mark>test accuracies</mark> do not drop by much.<br>",
    "Arabic": "دقة الاختبار",
    "Chinese": "测试准确率",
    "French": "Précision du test",
    "Japanese": "テスト精度",
    "Russian": "точность проверки"
  },
  {
    "English": "test dataset",
    "context": "1: For nonlinear metrics, increasing the resolution of measured model performance by increasing the <mark>test dataset</mark> size should reveal smooth, continuous, predictable model improvements commensurate with the predictable nonlinear effect of the chosen metric. 3.<br>2: Though the <mark>test dataset</mark> was released after our training set was created and its labels are hidden, some of the web pages used by the crowdsourced dataset creators are contained in our training set.<br>",
    "Arabic": "مجموعة بيانات الاختبار",
    "Chinese": "测试数据集",
    "French": "ensemble de test",
    "Japanese": "テストデータセット",
    "Russian": "тестовый набор данных"
  },
  {
    "English": "test datum",
    "context": "1: (2019) have shown that overparameterized classifiers (such as deep nets) can \"memorize\" their training set without harming performance on unseen <mark>test data</mark>. Moreover, works such as Soudry et al. (2018) have further shown that continuing to train networks past memorization can still lead to performance improvements 1,2 .<br>2: training data , we can expect these samples to perform well on <mark>test data</mark> .<br>",
    "Arabic": "بيانات الاختبار",
    "Chinese": "测试数据",
    "French": "donnée de test",
    "Japanese": "テストデータ",
    "Russian": "тестовые данные"
  },
  {
    "English": "test domain",
    "context": "1: Moreover, these methods need to have a lot of unlabeled data that is taken from the same domain, in order to learn meaningful feature correspondences across training and <mark>test domain</mark>.<br>2: We treat BeerAdvocate as a 'development domain', because we used it for developing the models and experimental setting, and RateBeer as a '<mark>test domain</mark>' in which we validate our final models on previously unseen data. Experimental results.<br>",
    "Arabic": "نطاق الاختبار",
    "Chinese": "测试领域",
    "French": "domaine de test",
    "Japanese": "テストドメイン",
    "Russian": "тестовая область"
  },
  {
    "English": "test error",
    "context": "1: However, by pruning more aggressively (smaller f ) when given more initial data (larger α tot ), one can achieve a Pareto optimal <mark>test error</mark> as a function of pruned dataset size α prune that remarkably traces out at least an exponential scaling law (Fig. 1A, purple curve).<br>2: Such power law scaling has motivated significant societal investments in data collection, compute, and associated energy consumption. However, power law scaling is extremely weak and unsustainable. For example, a drop in error Figure 1: Our analytic theory of data pruning predicts that power law scaling of <mark>test error</mark> with respect to dataset size can be beaten.<br>",
    "Arabic": "خطأ الاختبار",
    "Chinese": "测试误差",
    "French": "erreur de test",
    "Japanese": "テストエラー",
    "Russian": "ошибка тестирования"
  },
  {
    "English": "test loss",
    "context": "1: We compare excessively large models trained with a data constraint of D C = 100 million tokens in Figure 11 across µP, our default hyperparameters (Appendix S) and scaling law predictions. Surprisingly, µP leads to even higher <mark>test loss</mark> than our default hyperparameters.<br>2: For repeating data, differences in downstream performance are insignificant for up to around 4 epochs (25% budget) and then start dropping, which aligns with our results on <mark>test loss</mark> in §6. Filling up to 50% of data with code (42 billion tokens) also shows no deterioration.<br>",
    "Arabic": "خسارة الاختبار",
    "Chinese": "测试损失",
    "French": "perte de test",
    "Japanese": "テスト損失",
    "Russian": "потери на тесте"
  },
  {
    "English": "test set",
    "context": "1: Figure 2a plots the evolution of the error rate on the development set and on the <mark>test set</mark> against the learning iteration number. It is seen that the error rate remains stable after about 50 iterations.<br>2: We used validation and <mark>test set</mark>s of size 5000 feature vectors. The training ran for 100 epochs or until the error on the training set fell to zero.<br>",
    "Arabic": "مجموعة الاختبار",
    "Chinese": "测试集",
    "French": "ensemble de test",
    "Japanese": "テストセット",
    "Russian": "тестовый набор"
  },
  {
    "English": "test split",
    "context": "1: We evaluate different methods by sampling completions on the <mark>test split</mark> of TL;DR summarization dataset, and computing the average win rate against reference completions in the test set. The completions for all methods are sampled at temperatures varying from 0.0 to 1.0, and the win rates are shown in Figure 2 (right).<br>",
    "Arabic": "تقسيم الاختبار",
    "Chinese": "测试集分割",
    "French": "\"fractionnement des tests\"",
    "Japanese": "テスト分割",
    "Russian": "тестовое разделение"
  },
  {
    "English": "test time",
    "context": "1: Therefore, at <mark>test time</mark> we can take the optimal decision for a sample ending up in the leaves, with respect to all the training data and the current state of the network.<br>2: At <mark>test time</mark>, we experiment with a complete corpus with both source sentences represented, as this is the sort of multi-source translation setting that we are aiming to create models for.<br>",
    "Arabic": "وقت الاختبار",
    "Chinese": "测试时间",
    "French": "temps d'inférence",
    "Japanese": "- テスト時",
    "Russian": "время тестирования"
  },
  {
    "English": "testing set",
    "context": "1: For each edge in the training set and the <mark>testing set</mark>, we treat these edges as positive samples and sample non-adjacent nodes as negative samples. We generate the edge-induced graph for these node pairs according to the first part edges. The graph label is assigned as positive if the node pairs have a positive edge and vice versa.<br>",
    "Arabic": "مجموعة الاختبار",
    "Chinese": "测试集",
    "French": "ensemble de test",
    "Japanese": "テストセット",
    "Russian": "тестовый набор"
  },
  {
    "English": "text categorization",
    "context": "1: In contrast, one contribution of the work presented here is the creation of the first largescale, publicly available 6 dataset for deceptive opinion spam research, containing 400 truthful and 400 gold-standard deceptive reviews. To obtain a deeper understanding of the nature of deceptive opinion spam, we explore the relative utility of three potentially complementary framings of our problem. Specifically , we view the task as : ( a ) a standard <mark>text categorization</mark> task , in which we use n-gram-based classifiers to label opinions as either deceptive or truthful ( Joachims , 1998 ; Sebastiani , 2002 ) ; ( b ) an instance of psycholinguistic deception detection , in which we expect deceptive statements to exemplify the psychological effects of lying , such as increased negative emotion and psychological distancing ( Hancock et al. , 2008 ; Newman et al. , 2003 ) ; and , ( c ) a problem of genre identification , in which we view deceptive and truthful writing as sub-genres of imaginative and informative writing , respectively ( Biber et al. , 1999 ; Rayson et al. , 2001 )<br>2: On the other hand, it seems that distinguishing positive from negative reviews is relatively easy for humans, especially in comparison to the standard <mark>text categorization</mark> problem, where topics can be closely related.<br>",
    "Arabic": "تصنيف النص",
    "Chinese": "文本分类",
    "French": "catégorisation de texte",
    "Japanese": "テキスト分類",
    "Russian": "категоризация текста"
  },
  {
    "English": "text classification",
    "context": "1: We have already added to EXPLAINABOARD 12 NLP tasks, 50 datasets, and 400 models, 11 which cover many or most of top-scoring systems on these tasks.We briefly describe them below, and show high-level statistics in Tab. 2. <mark>Text Classification</mark> Prediction of one or multiple pre-defined label(s) for a given input text.<br>",
    "Arabic": "تصنيف النصوص",
    "Chinese": "文本分类",
    "French": "classification de texte",
    "Japanese": "テキスト分類",
    "Russian": "классификация текста"
  },
  {
    "English": "text corpus",
    "context": "1: , 2016 ) . Data collection biases have been discussed in the context of creating image corpus (Misra et al., 2016;van Miltenburg, 2016) and <mark>text corpus</mark> (Gordon and Van Durme, 2013;Van Durme, 2010).<br>2: Pre-requisites  2022a) vaguely state that the latent content or entity distribution of the <mark>text corpus</mark> and the data corpus must have some uncertain degree of overlap to make the cycle training approach work.<br>",
    "Arabic": "مجموعة نصوص",
    "Chinese": "文本语料库",
    "French": "corpus textuel",
    "Japanese": "テキストコーパス",
    "Russian": "текстовый корпус"
  },
  {
    "English": "text embedding",
    "context": "1: Alternatively, we can (i) measure similarities based on metafeatures; (ii) simply retrieve experiences randomly. Shown in Table 3, both meta-feature and <mark>text embedding</mark> consistently outperform random retrieval. When choosing between meta-features or <mark>text embedding</mark>, we believe that the latter has demonstrated advantages over manually designed metafeatures.<br>2: That is, at inference time we run text through CLIP's text encoder and then give the resulting <mark>text embedding</mark> as a prompt to SAM (see §D.5 for details). \"a wheel\" \"beaver tooth grille\" \"a wiper\" \"a wiper\" + point \"wipers\" \"wipers\" + point Results.<br>",
    "Arabic": "التضمين النصي",
    "Chinese": "文本嵌入",
    "French": "intégration de texte",
    "Japanese": "テキスト埋め込み",
    "Russian": "текстовое вложение"
  },
  {
    "English": "text encoder",
    "context": "1: Our neural architecture has a similar design as CLIP4Clip [75], where φ G reuses OpenAI CLIP's pretrained <mark>text encoder</mark>, and φ V is factorized into a frame-wise image encoder φ I and a temporal aggregator φ a that summarizes the sequence of 16 image features into a single video embedding.<br>2: Finally, to represent free-form text we use the <mark>text encoder</mark> from CLIP [82] (any <mark>text encoder</mark> is possible in general). We focus on geometric prompts for the remainder of this section and discuss text prompts in depth in §D.5. Dense prompts (i.e., masks) have a spatial correspondence with the image.<br>",
    "Arabic": "مُشفِّر النص",
    "Chinese": "文本编码器",
    "French": "encodeur de texte",
    "Japanese": "テキストエンコーダー",
    "Russian": "текстовый кодировщик"
  },
  {
    "English": "text generation",
    "context": "1: Language models that produce high quality <mark>text generation</mark> could lower existing barriers to carrying out these activities and increase their efficacy. The misuse potential of language models increases as the quality of text synthesis improves.<br>2: Experiments on datasets of several product categories showcase the efficacies of our method as compared to baselines based on templates, review summarization, selection, and <mark>text generation</mark>.<br>",
    "Arabic": "توليد النص",
    "Chinese": "文本生成",
    "French": "génération de texte",
    "Japanese": "テキスト生成",
    "Russian": "генерация текста"
  },
  {
    "English": "text generation model",
    "context": "1: (2018) shows that the cloze task can be used to improve the robustness of <mark>text generation models</mark>.<br>2: Recent large-scale <mark>text generation models</mark> show an ability to produce human-like text of remarkable quality and coherence in open-ended generation [45,61,6].<br>",
    "Arabic": "نموذج توليد النصوص",
    "Chinese": "文本生成模型",
    "French": "modèle de génération de texte",
    "Japanese": "テキスト生成モデル",
    "Russian": "модель генерации текста"
  },
  {
    "English": "text mining",
    "context": "1: We have designed information discovery and delivery agents that utilise text and network data mining for supporting real-time negotiation. This work has addressed the central issues of extracting relevant information from different online repositories with different formats, with possible duplicative and erroneous data. That is, we have addressed the central issues in extracting information from the World Wide Web.<br>2: In most <mark>text mining</mark> tasks, it would be natural to use the text collection to be mined as our reference text collection to extract candidate labels.<br>",
    "Arabic": "تنقيب النصوص",
    "Chinese": "文本挖掘",
    "French": "fouille de texte",
    "Japanese": "テキストマイニング",
    "Russian": "текстовый анализ"
  },
  {
    "English": "text segmentation",
    "context": "1: Knowledge-lean approaches Distributional models of content have appeared with some frequency in research on <mark>text segmentation</mark> and topic-based language modeling (Hearst, 1994;Beeferman et al., 1997;Chen et al., 1998;Florian and Yarowsky, 1999;Gildea and Hofmann, 1999;Iyer and Ostendorf, 1996;Wu and Khudanpur, 2002).<br>",
    "Arabic": "تجزئة النص",
    "Chinese": "文本分割",
    "French": "segmentation de texte",
    "Japanese": "テキスト分割",
    "Russian": "сегментация текста"
  },
  {
    "English": "text simplification",
    "context": "1: than ours . Many tasks have benefited from studying Wikipedia Revisions, like <mark>text simplification</mark> (Yatskar et al., 2010), textual entailment (Zanzotto and Pennacchiotti, 2010), discourse learning (Daxenberger and Gurevych, 2013) and grammatical error correction (Faruqui et al., 2018).<br>2: The RuATD Shared Task 2022 involved artificial texts in Russian generated by various language models fine-tuned for specific domains or tasks such as machine translation, paraphrase generation, text summarization, and <mark>text simplification</mark> (Shamardina et al., 2022). We pay more attention to zero-shot generations of LLMs, such as the subset of RuATD generated by ruGPT-3.<br>",
    "Arabic": "تبسيط النص",
    "Chinese": "文本简化",
    "French": "simplification de texte",
    "Japanese": "テキスト単純化",
    "Russian": "упрощение текста"
  },
  {
    "English": "text summarization",
    "context": "1: Focusing on compressing the most relevant information from long texts to short summaries, the <mark>Text Summarization</mark> task naturally lends itself to benefit from such global context. Notice that, in practice, the limitations linked to sequence length are also amplified by the lack of visual/layout information in the existing datasets.<br>2: • <mark>Text Summarization</mark> is to summarize the document into a sentence. We choose XSUM (Narayan et al., 2018) and MSNews (Liu et al., 2021a), two news summarization datasets. • Question Generation aims to generate questions based on given passages and answers.<br>",
    "Arabic": "تلخيص النص",
    "Chinese": "文本摘要",
    "French": "résumé de texte",
    "Japanese": "テキスト要約",
    "Russian": "Реферирование текста"
  },
  {
    "English": "text-davinci-002",
    "context": "1: We find that the older <mark>text-davinci-002</mark> clearly generates even more stereotypes than text-davinci-003, so we focus on text-davinci-003 as a more recent and conservative estimate of GPT-3.5. To compare rates of stereotyping between text-davinci-003 and <mark>text-davinci-002</mark>, we generate personas using <mark>text-davinci-002</mark> with the same parameters and prompts as described in Section 4 for text-davinci-003.<br>2: We find that stereotypes are broadly more prevalent in <mark>text-davinci-002</mark> outputs than in text-davinci-003 ones.<br>",
    "Arabic": "نص دافينشي-002",
    "Chinese": "文本-​​达芬奇-002",
    "French": "text-davinci-002",
    "Japanese": "テキスト-ダヴィンチ-002",
    "Russian": "текст-давинчи-002"
  },
  {
    "English": "text-davinci-003",
    "context": "1: We find that <mark>text-davinci-003</mark>, text-davinci-002, ChatGPT, and GPT-4 are the only models that, upon prompting to generate a persona, outputs a coherent description that indeed centers on one person.<br>2: Our results show that among the models we evaluated, only GPT-3.5 <mark>text-davinci-003</mark> exhibit non-trivial entity tracking behavior. While its performance does decrease as the number of operations increases, the model still produced many accurate predictions even after six or seven sequences of operations.<br>",
    "Arabic": "نص-دافنشي-003",
    "Chinese": "文本-​​达芬奇-003",
    "French": "texte-davinci-003",
    "Japanese": "テキスト-ダヴィンチ-003",
    "Russian": "text-davinci-003"
  },
  {
    "English": "text-to-image diffusion model",
    "context": "1: While the <mark>text-to-image diffusion model</mark> controls for most visual semantics, the super-resolution (SR) models are essential to achieve photorealistic content and to preserve subject instance details.<br>2: With regards to these pitfalls, we observe the peculiar finding that, given a careful fine-tuning setup Figure 3. Fine-tuning. Given ∼ 3−5 images of a subject we finetune a <mark>text-to-image diffusion model</mark> with the input images paired with a text prompt containing a unique identifier and the name of the class the subject belongs to ( e.g. , `` A [ V ] dog '' ) , in parallel , we apply a class-specific prior preservation loss , which leverages the semantic prior<br>",
    "Arabic": "نموذج انتشار النص إلى الصورة",
    "Chinese": "文本到图像扩散模型",
    "French": "modèle de diffusion texte-image",
    "Japanese": "テキストから画像への拡散モデル",
    "Russian": "текстово-изобразительная диффузионная модель"
  },
  {
    "English": "text-to-image generation",
    "context": "1: • Imagen (Saharia et al., 2022b) is one of the state-of-the-art <mark>text-to-image generation</mark> models that build upon both large language models (e.g., T5) for text understanding and diffusion models for high-fidelity image generation.<br>2: Over the last few years, the AI community has produced high-performance, task-specific models for many vision and language tasks such as object detection, segmentation, VQA, captioning, and <mark>text-to-image generation</mark>.<br>",
    "Arabic": "إنشاء صور من النصوص",
    "Chinese": "文本到图像生成",
    "French": "génération de texte en image",
    "Japanese": "テキストから画像生成",
    "Russian": "текст-в-изображение генерация"
  },
  {
    "English": "text-to-image model",
    "context": "1: We use the pretrained 64 × 64 base <mark>text-to-image model</mark> from Saharia et al. (2022). This model was trained on large-scale web-image-text data, and is conditioned on T5-XXL text embeddings (Raffel et al., 2020).<br>2: Recent state-of-the-art text-to-image diffusion models use cascaded diffusion models in order to generate highresolution images from text [54,61]. Specifically, [61] uses a base <mark>text-to-image model</mark> with 64x64 output resolution, and two text-conditional super-resolution (SR) models 64 × 64 → 256 × 256 and 256 × 256 → 1024 × 1024. Ramesh et al.<br>",
    "Arabic": "نموذج النص إلى صورة",
    "Chinese": "文本到图像模型",
    "French": "modèle texte-image",
    "Japanese": "テキストから画像への変換モデル",
    "Russian": "модель преобразования текста в изображение"
  },
  {
    "English": "text-to-image synthesis",
    "context": "1: Recent breakthroughs in <mark>text-to-image synthesis</mark> have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist.<br>",
    "Arabic": "توليد الصور من النصوص",
    "Chinese": "文本到图像合成",
    "French": "synthèse texte-image",
    "Japanese": "テキストから画像合成",
    "Russian": "синтез текста в изображение"
  },
  {
    "English": "text-to-text transfer transformer",
    "context": "1: We made use of the RoBERTa-base model which has around 125M parameters. T5. Following the work done by  and Richardson and Sabharwal (2021) on rule reasoning, we primarily centre our experiments around <mark>Text-to-Text Transfer Transformer</mark> or T5 models (Raffel et al., 2019).<br>",
    "Arabic": "محول نقل النص إلى النص",
    "Chinese": "文本到文本转换transformer",
    "French": "Transformateur de transfert de texte à texte",
    "Japanese": "テキストからテキストへの転移トランスフォーマー",
    "Russian": "текст-к-текст трансформер"
  },
  {
    "English": "textual entailment",
    "context": "1: The ability to make this distinction based on properties of the NPs used to identify these referents (mentions) would benefit not only coreference resolution, but also topic analysis, <mark>textual entailment</mark>, and discourse coherence. The existing literature provides numerous generalizations relevant to answering the question of whether a given discourse entity will be singleton or coreferent. These involve the internal syntax and morphology of the target NP ( Prince , 1981a ; Prince , 1981b ; Wang et al. , 2006 ) , the grammatical function and discourse role of that NP ( Chafe , 1976 ; Hobbs , 1979 ; Walker et al. , 1997 ; Beaver , 2004 ) , and the interaction of all of those features<br>2: No \n entailment not_entailment y v(y) q p (y | x) \n Figure 2 : Application of a PVP p = ( P , v ) for recognizing <mark>textual entailment</mark> : An input x = ( x 1 , x 2 ) is converted into a cloze question P ( x ) ; q p ( y | x ) for each y is derived from the probability of v ( y ) being a plausible<br>",
    "Arabic": "المضمون النصي",
    "Chinese": "文本蕴涵",
    "French": "implication textuelle",
    "Japanese": "文章の含意関係",
    "Russian": "текстовое следствие"
  },
  {
    "English": "tf-idf",
    "context": "1: 6 Documents that are more than a month older than the original article are filtered out of the search results. Extract entities e new from y 14: \n Compute <mark>tf-idf</mark> similarity Z(x i , y) r ← entity j Acc(e j cur ) − Acc(e j prev ) \n 23: \n<br>2: provided by the corresponding Wikipedia dump (see Section 3) for calculating the semantic relatedness [33] between sites. We utilize a vector space model [33] for representing the documents (states) as vectors of identifiers using <mark>tf-idf</mark> [34] for weighing the terms of the vectors.<br>",
    "Arabic": "tf-idf",
    "Chinese": "tf-idf",
    "French": "tf-idf",
    "Japanese": "tf-idf",
    "Russian": "tf-idf"
  },
  {
    "English": "threat model",
    "context": "1: An adaptive attack is one that is constructed after a defense has been completely specified, where the adversary takes advantage of knowledge of the defense and is only restricted by the <mark>threat model</mark>. One useful attack approach is to perform many attacks and report the mean over the best attack per image.<br>2: Specific, testable claims in a clear <mark>threat model</mark> precisely convey the claimed robustness of a defense. For example, a complete claim might be: \"We achieve 90% accuracy when bounded by ∞ distortion with = 0.031, when the attacker has full white-box access.\"<br>",
    "Arabic": "نموذج التهديدات",
    "Chinese": "威胁模型",
    "French": "modèle de menace",
    "Japanese": "脅威モデル",
    "Russian": "модель угроз"
  },
  {
    "English": "threshold",
    "context": "1: We now carry out a <mark>threshold</mark>ing process which allows us to define the feature analyzed as discriminative or not. This overlapping <mark>threshold</mark> takes values from 0 (the most restrictive, for cases with no overlapping between classes) to 1 (the most relaxed, when every sample from a class is inside the others).<br>2: We set a <mark>threshold</mark> at 80% precision on the validation set and sampled a set of 545 common objects whose detectors fired on a statistically significant number of images out of a set of 12 million random thumbnails from YouTube for which we had no ground truth.<br>",
    "Arabic": "حد عتبة",
    "Chinese": "阈值",
    "French": "seuil",
    "Japanese": "閾値",
    "Russian": "порог"
  },
  {
    "English": "threshold function",
    "context": "1: We consider cumulative intermediate reward to be the difference between our score and our opponent's score; the \"true\" reward of a win, loss, or tie is determined at the end of a game by applying a <mark>threshold function</mark> to the cumulative intermediate reward.<br>2: The <mark>threshold function</mark> has the important property that for any parse y, if y • w ≥ t α (Y, w) then y(i) = 1 implies f (i) = 0, i.e. if the parse score is above the threshold, then none of its indices will be pruned.<br>",
    "Arabic": "دالة العتبة",
    "Chinese": "阈值函数",
    "French": "fonction de seuil",
    "Japanese": "閾値関数",
    "Russian": "пороговая функция"
  },
  {
    "English": "threshold parameter",
    "context": "1: We ran our algorithm with the <mark>threshold parameter</mark> set to t ∈ [2, 14] and counted the num- 5 We tried Z values from 10 to 100 in steps of 10 and observed very similar results. We report results for Z = 100.  with different values of the <mark>threshold parameter</mark> t and in a randomly selected sample.<br>2: • Importance Advising advises when I T (s, a) ≥ k (Torrey and Taylor 2013), where k is a <mark>threshold parameter</mark>. • Early Correcting advises when π S (s) = π T (s) (Amir et al. 2016).<br>",
    "Arabic": "معلمة العتبة",
    "Chinese": "阈值参数",
    "French": "paramètre de seuil",
    "Japanese": "閾値パラメータ",
    "Russian": "порог"
  },
  {
    "English": "threshold policy",
    "context": "1: x 1 ) = α(x 2 ), u(x 1 ) > u(x 2 ) if and only if u (x 1 ) > u (x 2 ). For consistent utilities, the Pareto frontier takes a particularly simple form, represented by (a subset of) groupspecific <mark>threshold policies</mark>. Proposition 1.<br>2: where λ > 0 is a regularization parameter, and Φ(t) is a convex regularization function. We show that the solutions to these objectives are <mark>threshold policies</mark>, and can be fully characterized in terms of the group-wise selection rate.<br>",
    "Arabic": "سياسة العتبة",
    "Chinese": "阈值策略",
    "French": "politique de seuil",
    "Japanese": "しきい値方針",
    "Russian": "пороговая политика"
  },
  {
    "English": "time complexity",
    "context": "1: However, these prohibitive complexities make this solution unfeasible for inputs larger than few thousands of integers. This is the main motivation for designing an approximation algorithm which reduces the time and space complexities to linear at the cost of finding slightly suboptimal solutions.<br>2: Z T has better time and space complexities than D-Tucker since −3 is larger than in real-world datasets; for example, in the experiments, we use 50 as the default block size while the order of the real-world datasets is 3 or 4.<br>",
    "Arabic": "التعقيد الزمني",
    "Chinese": "时间复杂度",
    "French": "complexité temporelle",
    "Japanese": "時間計算量",
    "Russian": "сложность по времени"
  },
  {
    "English": "time series",
    "context": "1: Call centers have been modeled by queuing theory usually, using the paradigm of producer/consumer. Another possible approach is to model the received calls as a <mark>time series</mark> and use <mark>time series</mark> forecasting to estimate the required resources to render the service [2].<br>2: More recent work [16] focuses on sensor data mining to identify anomalous and deviant behavior. Other related work includes the InteMon system from CMU [7,6] that dynamically tracks correlations among multiple <mark>time series</mark> [18]. Interactive visualizations for system management have also been investigated [17].<br>",
    "Arabic": "السلاسل الزمنية",
    "Chinese": "时间序列",
    "French": "série chronologique",
    "Japanese": "時系列",
    "Russian": "временной ряд"
  },
  {
    "English": "time series analysis",
    "context": "1: In a variety of settings different from the social network context here, recent work has considered ways of attacking anonymization and related schemes using content analysis of the text generated by users [7,25], <mark>time series analysis</mark> of the timestamps of user actions [24], or linkages among user records in different datasets [26].<br>2: The mathematical underpinnings of our approach are grounded in statistics [19][20][21] and <mark>time series analysis</mark> [22][23][24][25][26][27], and similar methods have explored flux function estimation in optical communications [28][29][30].<br>",
    "Arabic": "تحليل السلاسل الزمنية",
    "Chinese": "时间序列分析",
    "French": "analyse des séries temporelles",
    "Japanese": "時系列分析",
    "Russian": "анализ временных рядов"
  },
  {
    "English": "time series forecasting",
    "context": "1: As the modeling strategy involves the creation of behavioral variables, there is statistical dependence among the examples, differently from typical classification problems. Therefore data division for modeling and testing the system should to be temporally disjoint in two blocks, as done in <mark>time series forecasting</mark> tasks [9] for more realistic performance assessment. The diagram in Fig.<br>2: , 2021 ; , image-to-image translation ( Sasaki et al. , 2021 ) , shape generation ( Zhou et al. , 2021 ) and <mark>time series forecasting</mark> ( Rasul et al. , 2021 ) . Faster DPMs. Several works attempt to find short trajectories while maintaining the DPM performance. Chen et al.<br>",
    "Arabic": "التنبؤ بالسلاسل الزمنية",
    "Chinese": "时间序列预测",
    "French": "prévision de séries chronologiques",
    "Japanese": "時系列予測",
    "Russian": "прогнозирование временных рядов"
  },
  {
    "English": "time step",
    "context": "1: Defined at <mark>time step</mark> t, state S t ǫS, is characterized through S t = (R t , c A t , c B t , Q 0 , H C ).<br>2: for each <mark>time step</mark> . The loss for each mini-batch is calculated as follows: \n L CP C = − n,k log exp(z T n,k M k c n ) m exp(z T m,k M k c n ) \n Both n and m index the mini-batch dimension.<br>",
    "Arabic": "خطوة زمنية",
    "Chinese": "时间步长",
    "French": "pas de temps",
    "Japanese": "時刻ステップ",
    "Russian": "шаг времени"
  },
  {
    "English": "time-series datum",
    "context": "1: An RNN is a powerful representation of <mark>time-series data</mark>, as it carries more complex information about the history by employing linear and non-linear functions. In our case, we hope the RNN to capture the shape of the object and thus make coherent predictions even in ambiguous cases such as for example shadows and saturation.<br>2: the unknown labels of trajectories); (2) the possibility that the sample trajectories might have lengths much smaller than the dimension d of the LDS models; and (3) the complicated temporal dependence inherent to <mark>time-series data</mark>.<br>",
    "Arabic": "بيانات سلسلة الزمن",
    "Chinese": "时间序列数据",
    "French": "données de séries chronologiques",
    "Japanese": "時系列データ",
    "Russian": "данные временных рядов"
  },
  {
    "English": "time-series model",
    "context": "1: It is worth noting that, although we focus on mixed LDSs for concreteness, we will make clear that the proposed modular algorithm is fairly flexible and can be adapted to learning mixtures of other <mark>time-series models</mark>, as long as certain technical conditions are satisfied; see Remark 1 at the end of Section 2 for a detailed discussion.<br>2: As for Algorithms 4 and 5, we essentially require a well-specified parametric form of the <mark>time-series models</mark>. This observation might inspire future extensions (in theory or applications) of Algorithm 1 to much broader settings. 3 Main results<br>",
    "Arabic": "نموذج السلاسل الزمنية",
    "Chinese": "时序模型",
    "French": "modèle de séries temporelles",
    "Japanese": "時系列モデル",
    "Russian": "модель временных рядов"
  },
  {
    "English": "time/space complexity",
    "context": "1: More problematically, the quadratic relationship between sequence length and <mark>time/space complexity</mark> of transformer architectures (Vaswani et al., 2017), especially when using byte-level sequences (Xue et al., 2022), has had a significant impact on our model performance.<br>",
    "Arabic": "التعقيد الزمني / المكاني",
    "Chinese": "时间/空间复杂度",
    "French": "complexité en temps/espace",
    "Japanese": "時間/空間複雑度",
    "Russian": "временная/пространственная сложность"
  },
  {
    "English": "token",
    "context": "1: P (i) = <mark>Token</mark>(i) i∈v <mark>Token</mark>(i) , lv = i∈v len(i) |v| . <mark>Token</mark>(i) is the frequency of token i in the vocabulary v. len(i) represents the length of token i.<br>",
    "Arabic": "- توكن",
    "Chinese": "词元",
    "French": "jeton",
    "Japanese": "トークン",
    "Russian": "токен"
  },
  {
    "English": "token classification",
    "context": "1: The RoBERTalarge (Liu et al., 2019) model is a Transformers <mark>token classification</mark> model that we finetuned on the S2-VL training set. The I-VILA model is a layoutinfused Transformer model pretrained by Shen et al. (2022) on the S2-VL training set.<br>",
    "Arabic": "تصنيف الرموز المميزة",
    "Chinese": "标记分类",
    "French": "classification de jetons",
    "Japanese": "トークン分類",
    "Russian": "классификация токенов"
  },
  {
    "English": "token embedding",
    "context": "1: Then, the tokens attend once more to the image embedding and we pass the updated output <mark>token embedding</mark> to a small 3-layer MLP that outputs a vector matching the channel dimension of the upscaled image embedding. Finally, we predict a mask with a spatially point-wise product between the upscaled image embedding and the MLP's output.<br>2: This section provides implementation details for AllenNLP Interpret: how we compute the <mark>token embedding</mark> gradient in a model-agnostic way, as well as the available front-end interface. Figure 4 provides an overview of our software implementation and the surrounding AllenNLP ecosystem.<br>",
    "Arabic": "تضمين الرمز المميز",
    "Chinese": "令牌嵌入",
    "French": "\"Plongement de jeton\"",
    "Japanese": "トークン埋め込み",
    "Russian": "вложение токена"
  },
  {
    "English": "token frequency",
    "context": "1: We followed Piantadosi (2014) in performing a binomial split on the observed frequency of each token to avoid estimating frequency and rank on the same sample. We used Zipf's original formula introduced above rather than derivations to model token distributions, following Linders and Louwerse (2023).<br>",
    "Arabic": "تردد رمزي",
    "Chinese": "词元频率",
    "French": "fréquence du jeton",
    "Japanese": "トークン頻度",
    "Russian": "частота токенов"
  },
  {
    "English": "token length",
    "context": "1: During training, the paraphrase candidate generator is trained by maximising the following likelihood: \n P (x | x) = T t=1 P (x t |x 1 , ...,x t−1 , x), \n where T represents the <mark>token length</mark> of the paraphrase and x t represents the word at the position t that has been inferenced.<br>2: This replacement allowed more posts to pass our final filter, which was to cap post length at 500 characters to suit the max <mark>token length</mark> of BERTbased models. About 785K posts remained in our corpus after this local filtering round.<br>",
    "Arabic": "\"طول الرمز\"",
    "Chinese": "标记长度",
    "French": "longueur des jetons",
    "Japanese": "トークン長",
    "Russian": "длина токена"
  },
  {
    "English": "token representation",
    "context": "1: Our final goal is to predict semantic roles for each predicate in the sequence. We score each predicate against each token in the sequence using a bilinear operation, producing per-label scores for each token for each predicate, with predicates and syntax determined by oracles V and P. \n First, we project each <mark>token representation</mark> s \n (J) t \n<br>2: Many authors receive a non-negligible <mark>token representation</mark>, but Asimov's token count is still a factor of ten larger than the second most prominent author (Robert A. Heinlein).<br>",
    "Arabic": "تمثيل رمزي",
    "Chinese": "标记表示",
    "French": "représentation de jeton",
    "Japanese": "トークン表現",
    "Russian": "токенное представление"
  },
  {
    "English": "token sequence",
    "context": "1: A masking function M takes X, f and S as its inputs and produces a new <mark>token sequence</mark> X mlm as its output \n X mlm = M(X, S, f ) \n where X mlm denotes the input sentence X with f % of the maskable tokens (as deemed by S) randomly replaced with [MASK].<br>2: 4 Input/Output Representations To make BERT handle a variety of down-stream tasks, our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., Question, Answer ) in one <mark>token sequence</mark>.<br>",
    "Arabic": "تسلسل رمزي",
    "Chinese": "标记序列",
    "French": "séquence de jetons",
    "Japanese": "トークン系列",
    "Russian": "последовательность токенов"
  },
  {
    "English": "token space",
    "context": "1: 3) in <mark>token space</mark>, allowing some knowledge for prediction to be handled by the label decoder h, since we found that introducing explicit reconstruction loss on tokens deteriorates the performance in our initial experiments.<br>",
    "Arabic": "فضاء الرموز",
    "Chinese": "令牌空间",
    "French": "espace de jetons",
    "Japanese": "トークン空間",
    "Russian": "пространство токенов"
  },
  {
    "English": "token vector",
    "context": "1: Each token u t is embedded with pretrained embedding matrix W e ∈ R |V |×d , where |V | corresponds to the number of tokens in vocabulary V , and d defines the size of the word embeddings. The embedded <mark>token vector</mark> x t ∈ R d is retrieved simply with x t = u t W e .<br>2: Each token ∈ P can be represented by a <mark>token vector</mark> p ∈ R 1× with the same size of node features in the input graph; Note that in practice, we usually have |P | ≪ and |P | ≪ ℎ where ℎ is the size of the hidden layer in the pre-trained graph model.<br>",
    "Arabic": "متجه الرموز",
    "Chinese": "词元向量",
    "French": "vecteur de jetons",
    "Japanese": "トークンベクトル",
    "Russian": "вектор токена"
  },
  {
    "English": "token vocabulary",
    "context": "1: We introduce an information bottleneck that limits the size of the discrete <mark>token vocabulary</mark> to as few as 32 distinct symbols per raw input token.<br>",
    "Arabic": "مفردات الرموز",
    "Chinese": "记号词汇",
    "French": "vocabulaire de jetons",
    "Japanese": "トークン語彙",
    "Russian": "словарь токенов"
  },
  {
    "English": "token-level",
    "context": "1: Such restrictions are sub-optimal for sentence-level tasks, and could be very harmful when applying finetuning based approaches to <mark>token-level</mark> tasks such as question answering, where it is crucial to incorporate context from both directions. In this paper, we improve the fine-tuning based approaches by proposing BERT: Bidirectional Encoder Representations from Transformers.<br>2: \"in-batch\" performs worse than all <mark>token-level</mark> variants, which may be because generative tasks make predictions in a token-wise manner and rely heavily in finergrained token-wise representations. Interestingly, contrary to in-batch negative sampling in computer vision , we find that reducing the number of negative samples by reducing the batch size from 32 to 16 slightly improves performance.<br>",
    "Arabic": "مستوى الرمز المميز",
    "Chinese": "词元级别",
    "French": "niveau du jeton",
    "Japanese": "トークンレベル",
    "Russian": "на уровне токенов"
  },
  {
    "English": "token-level attention",
    "context": "1: 4) on more than 33,000 transcripts from the UK House of Commons, we demonstrate GPolS's ability for stance analysis in parliamentary debates (Sec. 5). Lastly, we visualize GPolS's graph attention mechanism (Sec. 5.3) and <mark>token-level attention</mark> (Sec.<br>2: As a concluding use-case, we analyze <mark>token-level attention</mark> over political debate transcripts. Our goal with this attention analysis is not to study causation or an explanation of GPolS's predictions, but rather analyzing the impacvt of fine-tuning BERT on ParlVote.<br>",
    "Arabic": "الاهتمام على مستوى الرمز",
    "Chinese": "词元级注意力",
    "French": "attention au niveau des tokens",
    "Japanese": "トークンレベルの注目",
    "Russian": "внимание на уровне токенов"
  },
  {
    "English": "token-level feature",
    "context": "1: highlights ) . In contrast to the transfer of <mark>token-level features</mark> like formality, it is more difficult to capture the intersentence relations correlated with author styles and disentangle them from contents. The second challenge is that the author styles tend to be highly associated with specific writing topics.<br>",
    "Arabic": "ميزة على مستوى الرمز المميز",
    "Chinese": "词元级特征",
    "French": "caractéristique au niveau du jeton",
    "Japanese": "トークンレベルの特徴",
    "Russian": "Признак на уровне токена"
  },
  {
    "English": "tokenisation",
    "context": "1: We can see that performance suffers when the parser performs its own <mark>tokenisation</mark>. A reason for this is the under-use of apostrophes in the forum data, with the result that words such as didnt and im remain untokenised and are tagged by the parser as common nouns:<br>2: This score overcomes the limitations inherent to the more commonly used BLEU metric (Papineni et al., 2002), which relies on the availability of <mark>tokenisation</mark> tools for all languages and fails to accurately account for highly agglutinative languages.<br>",
    "Arabic": "الرمزنة",
    "Chinese": "词条化",
    "French": "tokenisation",
    "Japanese": "トークン化",
    "Russian": "токенизация"
  },
  {
    "English": "tokenization",
    "context": "1: For German, French and Spanish, contractions were split, except in the case of clitics. For Korean, <mark>tokenization</mark> was more coarse and included particles within token units. Annotators could correct this automatic <mark>tokenization</mark>.<br>2: While the exact indexing depth is not disclosed by search engines, and may even vary from document to document, our experiments, as well as those of [8], showed that the majority of the documents are indexed by the first d = 10, 000 terms at the least. 5. Parsing and <mark>tokenization</mark>.<br>",
    "Arabic": "توكينة",
    "Chinese": "分词化",
    "French": "tokenisation",
    "Japanese": "トークン化",
    "Russian": "токенизация"
  },
  {
    "English": "tokenization scheme",
    "context": "1: Table 3 shows the different models used in our experiments, along with their abbreviation, <mark>tokenization scheme</mark>, total parameters, vocabulary size, number of tokens encountered during training, and corpora on which they are pre-trained.<br>",
    "Arabic": "مخطط الترميز",
    "Chinese": "词元化方案",
    "French": "schéma de tokenisation",
    "Japanese": "トークン化方式",
    "Russian": "схема токенизации"
  },
  {
    "English": "tokenizer",
    "context": "1: and a verbalizer v that maps +1 to the single token great and −1 to the sequence terri • ble, i.e., we assume that the MLM's <mark>tokenizer</mark> splits the word \"terrible\" into the two tokens terri and • ble.<br>2: For both set of experiments using subsets of MassiveText, we use the same <mark>tokenizer</mark> as the MassiveText experiments. We find that the scaling behaviour on these datasets is very similar to what we found on MassiveText, as shown in Figure A2 and Table A2.<br>",
    "Arabic": "مقسم النصوص",
    "Chinese": "分词器",
    "French": "tokeniseur",
    "Japanese": "トークナイザー",
    "Russian": "токенизатор"
  },
  {
    "English": "top-1 accuracy",
    "context": "1: We evaluate the models using <mark>top-1 accuracy</mark>. In Tab. 2, we show a comparison between models trained on LAION (400M, 2B) and original CLIP from [58].<br>2: For the text classification task, we follow Wei et al. (2021) to use the <mark>top-1 accuracy</mark> as the metric. For the dialogue generation task, we use the word-level F1 score, and we adopt the well-known sequence evaluation metric BLEU (Papineni et al., 2002) where we report BLEU-2, BLEU-3 and BLEU-4.<br>",
    "Arabic": "دقة المرتبة الأولى",
    "Chinese": "前1准确率",
    "French": "précision top-1",
    "Japanese": "top-1 正解率",
    "Russian": "точность топ-1"
  },
  {
    "English": "top-down segmentation",
    "context": "1: The right-hand column of figure 1 shows a <mark>top-down segmentation</mark> of the horse figure obtained by the algorithm of [3]. In this algorithm, image fragments from horses in a training database are correlated with the novel image. By combining together the segmentations of the fragments, the novel image is segmented.<br>2: '' a consistent and coherent interpretation of the image ; ( d ) a <mark>top-down segmentation</mark> adjustment procedure where partial scene interpretations guide the creation of new segment proposals .<br>",
    "Arabic": "التجزئة من الأعلى إلى الأسفل",
    "Chinese": "自上而下的分割",
    "French": "segmentation descendante",
    "Japanese": "トップダウンのセグメンテーション",
    "Russian": "сегментация сверху вниз"
  },
  {
    "English": "top-k",
    "context": "1: It is easy to see that we only consider the probability of flipping the <mark>top-k</mark> features to be out-of-the-<mark>top-k</mark> up to the feature k+τ . Thus, all features after k+τ , i.e., from k+τ +1 to d are not changed. Hence the theorem follows.<br>2: Such an advantage correlates well with the difficulty of the paraphrases to be understood by human annotators. Furthermore, bottom-k does not hurt the paraphrasing performance compared to the <mark>top-k</mark> and top-p sampling. The result of human evaluation verifies our claim that bottom-k generates super-hard paraphrases with grammatical and lexical richness.<br>",
    "Arabic": "أعلى-k",
    "Chinese": "前k个",
    "French": "meilleurs-k",
    "Japanese": "\"トップ-k\"",
    "Russian": "топ-к"
  },
  {
    "English": "top-k sampling",
    "context": "1: Classical search algorithms such as A* search (Hart et al., 1968;Pearl, 1984;Korf, 1985) address the challenge of planning ahead by using heuristic estimation of future cost when making decisions. Drawing inspiration from A * search , we develop NEUROLOGIC A esque ( shortened to NEUROLOGIC ) , which combines A * -like heuristic estimates of future cost ( e.g. , perplexity , constraint satisfaction ) with common decoding algorithms for neural text generation ( e.g. , beam search , <mark>top-k sampling</mark> ) , while preserving the efficiency demanded by large-scale neural language models<br>2: A combination of top-k and top-p sampling with k = 120 and p = 0.95 is used for the penultimate row. Model HUFFPOST COVID-Q AMZN PCC w/o MI filtering 25.7 ± 1.4 50.2 ± 1.7 16.7 ± 1.1 PCC w/ Pure Sampling 25.8 ± 1.0 49.7 ± 0.9 16.9 ± 0.8 PCC w/ Inverse Curriculum 23.0 ± 1.7 48.5 ± 1.2 15.0 ± 0.5 PCC w/ Random Curriculum 24.0 ± 1.7 48.9 ± 1.5 15.1 ± 0.8 PCC w/ Gradual Curriculum 24.7 ± 1.3 49.6<br>",
    "Arabic": "أخذ العينات أعلى ك",
    "Chinese": "top-k采样",
    "French": "échantillonnage top-k",
    "Japanese": "トップkサンプリング",
    "Russian": "top-k сэмплинг"
  },
  {
    "English": "top-p sampling",
    "context": "1: Traditional sampling methods such as top-k sampling (Fan et al., 2018) and <mark>top-p sampling</mark> (Holtzman et al., 2020) sample the next token to be presented in the output from the most probable vocabularies that dominate the probability distribution.<br>2: We use both automatic and human evaluation to assess the performance of our proposed reframe generation model as developed in §5.2. Experimental Setup. We use <mark>top-p sampling</mark> with p = 0.6 for text generation . We split the 600 expert-annotated examples ( §4) into train and test using a 70:30 split.<br>",
    "Arabic": "التحديد الأعلى-بي (top-p sampling)",
    "Chinese": "顶p抽样",
    "French": "échantillonnage top-p",
    "Japanese": "トップpサンプリング",
    "Russian": "выборка top-p"
  },
  {
    "English": "topic assignment",
    "context": "1: This model outputs a smooth sequence of <mark>topic assignments</mark> over a document, so we can compare the trajectories it learns on our dataset to those of the RMN.<br>2: Our work can thus be seen as an extension of online matrix factorization techniques that optimize squared error [10] to more general probabilistic formulations. We can analyze a corpus of documents with LDA by examining the posterior distribution of the topics β, topic proportions θ, and <mark>topic assignments</mark> z conditioned on the documents.<br>",
    "Arabic": "تعيين الموضوعات",
    "Chinese": "主题分配",
    "French": "attribution de sujet",
    "Japanese": "トピック割り当て",
    "Russian": "тематическое присвоение"
  },
  {
    "English": "topic classification",
    "context": "1: ) for <mark>topic classification</mark> , and EmoContext ( EmoC ) ( Chatterjee et al. , 2019 ) for emotion classification . Templates for constructing demonstrations are provided in Appendix A. 1000 examples are sampled from the test set for evaluation, with one demonstration per class sampled from the training set.<br>2: This finding is in stark contrast to the successful application of active learning methods on a variety of traditional tasks, such as <mark>topic classification</mark> (Siddhant and Lipton, 2018;Lowell et al., 2019), object recognition (Deng et al., 2018), digit classification , and named entity recognition (Shen et al., 2017).<br>",
    "Arabic": "تصنيف الموضوعات",
    "Chinese": "主题分类",
    "French": "classification de sujets",
    "Japanese": "トピック分類",
    "Russian": "классификация тем"
  },
  {
    "English": "topic detection and tracking",
    "context": "1: The research most closely related to novelty or redundancy detection in adaptive information filtering is perhaps the First Story Detection task associated with <mark>Topic Detection and Tracking</mark> (TDT) research [1]. A TDT system monitors a stream of chronologically-ordered documents, usually news stories.<br>",
    "Arabic": "الكشف عن المواضيع وتعقبها",
    "Chinese": "主题检测和跟踪",
    "French": "détection et suivi de sujets",
    "Japanese": "トピック検出および追跡",
    "Russian": "обнаружение и отслеживание тем"
  },
  {
    "English": "topic distribution",
    "context": "1: Similarly, each topic z is associated with a latent coordinate φ z on the visualization space. A document d n 's <mark>topic distribution</mark> is then expressed in terms of the Euclidean distance between its coordinate x n and the different topic coordinates Φ = {φ z } Z z=1 , as shown in Equation 1.<br>2: The basic idea of this approach is illustrated in Figure 1. Basically, a phrase containing more \"important\" (high p(w|θ)) words in the <mark>topic distribution</mark> is assumed to be a good label.<br>",
    "Arabic": "توزيع الموضوع",
    "Chinese": "主题分布",
    "French": "distribution des sujets",
    "Japanese": "トピック分布",
    "Russian": "распределение тем"
  },
  {
    "English": "topic model",
    "context": "1: Likewise, although [10] regularize a statistical <mark>topic model</mark> with a harmonic regularization based on a graph structure, the concept hierarchy of topics covered in the communities is not shown explicitly. In this paper, a mining strategy is proposed for discovering topic-based collaborative community.<br>2: in the case of revisiting a document in a <mark>topic model</mark>. After burn-in the previous topic assignment for a given word is likely to be still pertinent for the current sampling pass.<br>",
    "Arabic": "نموذج الموضوع",
    "Chinese": "主题模型",
    "French": "modèle de sujet",
    "Japanese": "トピックモデル",
    "Russian": "тематическая модель"
  },
  {
    "English": "topic proportion",
    "context": "1: Topic models posit that each document is expressed as a mixture of topics. These <mark>topic proportions</mark> are drawn once per document, and the topics are shared across the corpus. In this paper we will consider topic models that make different assumptions about the <mark>topic proportions</mark>.<br>",
    "Arabic": "نسبة الموضوع",
    "Chinese": "主题比例",
    "French": "proportion des sujets",
    "Japanese": "トピック比率",
    "Russian": "\"пропорция темы\""
  },
  {
    "English": "topic weight",
    "context": "1: , K} from the <mark>topic weights</mark> z di ∼ θ d and draw the observed word w di from the selected topic, w di ∼ β z di . For simplicity, we assume symmetric priors on θ and β, but this assumption is easy to relax [8].<br>",
    "Arabic": "وزن الموضوع",
    "Chinese": "主题权重",
    "French": "poids du sujet",
    "Japanese": "トピックの重み",
    "Russian": "вес темы"
  },
  {
    "English": "total variation",
    "context": "1: Energy minimization methods have become the central paradigm for solving practical problems in computer vision. The energy functional can often be written as the sum of a data fidelity and a regularization term. One of the most popular regularizers is the <mark>total variation</mark> (T V ) due to its many favorable properties [4].<br>2: This shows that the proposed regularizer coincides with the <mark>total variation</mark> from [5], where it has been derived based on (16) for α and β restricted to {0, 1}. Prop. 5 together with Prop.<br>",
    "Arabic": "الاختلاف الكلي",
    "Chinese": "全变分",
    "French": "variation totale",
    "Japanese": "総変動",
    "Russian": "общая вариация"
  },
  {
    "English": "total variation distance",
    "context": "1: Besides the Jensen-Shannon divergence used by [21], many GAN formulations have been proposed based on minimizing other losses, including the Wasserstein metric [4,22], <mark>total variation distance</mark> [31], χ 2 divergence [33], MMD [27], Dudley metric [1], and Sobolev metric [38].<br>2: Suppose the inducing points, Z, are sampled from an k-DPP, ν, i.e a distribution over subsets of X of size M satisfying, d(µ, ν) T V ≤ where d(•, •) T V denotes <mark>total variation distance</mark> and µ is a k-DPP on K ff .<br>",
    "Arabic": "مسافة التباين الكلي",
    "Chinese": "总变差距离",
    "French": "distance de variation totale",
    "Japanese": "全変動距離",
    "Russian": "расстояние общей вариации"
  },
  {
    "English": "toxicity detection",
    "context": "1: with sociodemographic profiles in a controlled setting which comprises seven datasets reflecting four different subjective NLP classification tasks (sentiment analysis, hatespeech detection, <mark>toxicity detection</mark>, and stance detection).<br>2: While researcher positionality is commonly discussed outside of NLP, it is highly applicable to NLP research but remains largely overlooked. For example, a U.S.-born English-speaking researcher building a <mark>toxicity detection</mark> system will likely start with U.S.-centric English statements to annotate for toxicity.<br>",
    "Arabic": "الكشف عن السمية",
    "Chinese": "毒性检测",
    "French": "détection de toxicité",
    "Japanese": "有害性検出",
    "Russian": "определение токсичности"
  },
  {
    "English": "trace norm",
    "context": "1: All share the same objective function and some common constraints K = {K 0, tr(K) ≤ 1, ij K ij = 0, ξ ≥ 0} to limit the <mark>trace norm</mark> of K and to center K such that the embeddings are centered on the origin.<br>2: 1.2 ) , } ¨ } ˚is the <mark>trace norm</mark> , and OPT k is the <mark>trace norm</mark> error of the best rank k approximation .<br>",
    "Arabic": "معيار الأثر",
    "Chinese": "迹范数",
    "French": "norme de trace",
    "Japanese": "トレースノルム",
    "Russian": "следовая норма"
  },
  {
    "English": "tracking algorithm",
    "context": "1: We demonstrate the behaviour of the adaptive, phasebased appearance model in the context of tracking nonrigid objects. For this demonstration we manually specify an elliptical region AE ¼ at time ¼. The <mark>tracking algorithm</mark> then estimates the image motion and the appearance model as it tracks the dominant image structure in AE Ø over time.<br>",
    "Arabic": "خوارزمية التتبع",
    "Chinese": "跟踪算法",
    "French": "algorithme de suivi",
    "Japanese": "トラッキングアルゴリズム",
    "Russian": "алгоритм отслеживания"
  },
  {
    "English": "train",
    "context": "1: We now turn to investigate whether patterns in instruction examples are further propagated by crowdworkers to the collected data. We analyze the <mark>train</mark> and test sets of each benchmark 4 to find the same patterns, using simple string matching.<br>2: Consequently, this agent is unable to navigate in new environment, achieving 100% success on <mark>train</mark> and 0% on test. To test (1), we analyze whether the agent is capable of detecting collisions. Note that the agent is not equipped with a collision sensor.<br>",
    "Arabic": "التدريب",
    "Chinese": "训练",
    "French": "entraîner",
    "Japanese": "学習",
    "Russian": "обучение"
  },
  {
    "English": "train / test / dev split",
    "context": "1: Did you report relevant statistics like the number of examples, details of <mark>train / test / dev splits</mark>, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results.<br>2: Did you report relevant statistics like the number of examples, details of <mark>train / test / dev splits</mark>, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results.<br>",
    "Arabic": "تقسيم التدريب / الاختبار / التطوير",
    "Chinese": "训练/测试/验证集拆分",
    "French": "répartition entraînement / test / validation",
    "Japanese": "訓練/テスト/開発分割",
    "Russian": "разделение на обучающую / тестовую / валидационную выборки"
  },
  {
    "English": "train set",
    "context": "1: For each of the 100 random splits the development set is divided randomly into a <mark>train set</mark> and a validation set, where we train models on the <mark>train set</mark> and evaluate on the validation set. We fit hyperparameters by random search, maximising the mean accuracy across the validation sets.<br>2: Seen entities are those present in the <mark>train set</mark> or with expo(e) > 1. Unseen entities have expo(e) = 0 and are not known to humans. The rest of the entities are discarded from the evaluation.<br>",
    "Arabic": "مجموعة التدريب",
    "Chinese": "训练集",
    "French": "ensemble d'entraînement",
    "Japanese": "トレーニングセット",
    "Russian": "обучающий набор"
  },
  {
    "English": "train-test split",
    "context": "1: For each dataset, we train a random forest using scikit-learn's default parameter settings with 100 trees on each dataset and evaluate test performance on each of the others, using an 80%-20% <mark>train-test split</mark>.<br>2: As the total time span of the data ranges from May 2000 to January 2003, we obtain T = 11 time windows, where the first 9 are used for training and the remaining are kept for testing, resulting in a 99%-1% <mark>train-test split</mark>.<br>",
    "Arabic": "تقسيم التدريب والاختبار",
    "Chinese": "训练测试划分",
    "French": "répartition entraînement-test",
    "Japanese": "トレーニングとテストの分割",
    "Russian": "разделение на обучающую и тестовую выборки"
  },
  {
    "English": "train/test",
    "context": "1: With predicted poses, we observe that sometimes even when our reconstructions look plausible, the errors can be high as the metrics are sensitive to bad alignment. The data sparsity issue is especially visible in the case of sofas where in a <mark>train/test</mark> setting in Table 2 the numbers drop significantly with less training data (only 34 instances).<br>",
    "Arabic": "تدريب / اختبار",
    "Chinese": "训练/测试",
    "French": "entraînement/test",
    "Japanese": "訓練/テスト",
    "Russian": "обучение/тестирование"
  },
  {
    "English": "trainable parameter",
    "context": "1: The training objective of the initial MNMT model is to maximize the log-likelihood L: \n L D (θ) = D i ∈D (x,y)∈D i log p(y|x; θ) (1) \n where θ represents the <mark>trainable parameters</mark> of MNMT models.<br>2: , 2018 ; Dong et al. , 2018 ; Xie et al. , 2019 ) , which explicitly unroll/truncate iterative optimization algorithms into learnable deep architectures . In this way, the penalty parameters (and the denoiser prior) are treated as <mark>trainable parameters</mark>, meanwhile the number of iterations has to be fixed to enable end-to-end training.<br>",
    "Arabic": "المعلمة القابلة للتدريب",
    "Chinese": "可训练参数",
    "French": "paramètre entraînable",
    "Japanese": "訓練可能なパラメータ",
    "Russian": "обучаемый параметр"
  },
  {
    "English": "trainable weight",
    "context": "1: where W ∈ R dc×dc is the <mark>trainable weights</mark> and A ∈ R J×J×dc is the channel-wise kinematic correspondence matrix among joints. Afterward, the GCN-evolved jointwise conditions are concatenated with the joint indicator vectors, timestep embeddings, as well as sampled local features.<br>2: where q l i , k l i and v l i denote query, key and value at the l-th layer, W l q , W l k , W l v denote their associating <mark>trainable weights</mark>. h l i denotes the visual semantic embedding vector of the i-th object, where h 0 i = s 0 i .<br>",
    "Arabic": "أوزان قابلة للتدريب",
    "Chinese": "可训练权重",
    "French": "poids entraînables",
    "Japanese": "訓練可能な重み",
    "Russian": "обучаемые веса"
  },
  {
    "English": "training accuracy",
    "context": "1: • Next, the network will memorize (using e.g. the noise in the data) the remaining 10% of the training examples without learning any new features, due to insufficient amount of left-over samples after the first phase, thus achieving <mark>training accuracy</mark> 100% but test accuracy 90%. How ensemble improves test accuracy.<br>2: • (<mark>training accuracy</mark> is perfect): meaning for all (X, y) ∈ Z, for all i ∈ [k]\\{y}: G y (X) > G i (X). • (test accuracy is almost perfect): meaning that: \n<br>",
    "Arabic": "دقة التدريب",
    "Chinese": "训练准确率",
    "French": "précision d'entraînement",
    "Japanese": "訓練精度",
    "Russian": "точность обучения"
  },
  {
    "English": "training algorithm",
    "context": "1: One could naïvely extend this composition-based approach to analyze the privacy of a <mark>training algorithm</mark> which involves hyperparameter tuning. Indeed, if each training run performed to evaluate one candidate set of hyperparameter values is DP, the overall procedure is also DP by composition over all the hyperparameter values tried.<br>2: A threat model specifies the conditions under which a defense argues security: a precise threat model allows for an exact understanding of the setting under which the defense is meant to work. Prior work has used words including whitebox, grey-box, black-box, and no-box to describe slightly different threat models, often overloading the same word. Instead of attempting to , yet again , redefine the vocabulary , we enumerate the various aspects of a defense that might be revealed to the adversary or held secret to the defender : model architecture and model weights ; <mark>training algorithm</mark> and training data ; test time randomness ( either the values chosen or the distribution ) ; and , if the model<br>",
    "Arabic": "خوارزمية التدريب",
    "Chinese": "训练算法",
    "French": "algorithme d'entraînement",
    "Japanese": "訓練アルゴリズム",
    "Russian": "алгоритм обучения"
  },
  {
    "English": "training batch",
    "context": "1: In our loss formulation, we compute the flow loss L flo as a weighted sum of the mean absolute error (MAE) between each pair of correspondences in a <mark>training batch</mark>.<br>2: According to this design, an adjacency semantic region for the i-th training instance can be fully established by interpolating various instances in the same <mark>training batch</mark>. We follow  to adaptively adjust the value of λ x (or λ y ) during the training process, and refer to the original paper for details.<br>",
    "Arabic": "دفعة تدريبية",
    "Chinese": "训练批次",
    "French": "lot d'entraînement",
    "Japanese": "トレーニングバッチ",
    "Russian": "партия обучения"
  },
  {
    "English": "training corpora",
    "context": "1: A model can be sample-efficient for English, but not for other languages. We should ensure that our architectures, like humans learners, are not optimized for English (Bender, 2011). To do so, we should develop matched <mark>training corpora</mark> and benchmarks for multiple languages.<br>",
    "Arabic": "مجموعات تدريبية",
    "Chinese": "训练语料库",
    "French": "corpus d'entraînement",
    "Japanese": "学習コーパス",
    "Russian": "тренировочные корпуса"
  },
  {
    "English": "training corpus",
    "context": "1: We use a <mark>training corpus</mark>, which is used to train the alignment template model and the language models, a development corpus, which is used to estimate the model scaling factors, and a test corpus. So far, in machine translation research does not exist one generally accepted criterion for the evaluation of the experimental results.<br>2: where P (i) is the relative frequency of token i from the <mark>training corpus</mark> and l v is the average length of tokens in vocabulary v.<br>",
    "Arabic": "مجموعة تدريبية",
    "Chinese": "训练语料库",
    "French": "corpus d'entraînement",
    "Japanese": "訓練コーパス",
    "Russian": "обучающий корпус"
  },
  {
    "English": "training dataset",
    "context": "1: The data is collected from the internet, and thus undoubtedly there is toxic/biased content in our <mark>training dataset</mark>. Furthermore, it is likely that personal information is also in the dataset that has been used to train our models. We defer to the more detailed discussion in Weidinger et al. (2021).<br>2: Since our <mark>training dataset</mark> is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices.<br>",
    "Arabic": "مجموعة البيانات التدريبية",
    "Chinese": "训练数据集",
    "French": "ensemble de données d'entraînement",
    "Japanese": "訓練データセット",
    "Russian": "набор данных для обучения"
  },
  {
    "English": "training datum",
    "context": "1: In this scenario, the author set in test data does not overlap with the <mark>training data</mark>. Validation data. We also randomly sample the <mark>training data</mark> into a smaller subset to use as validation data to tune hyperparameters during the training process. The size of the validation set is randomly selected to be the same as the cross-topic test set.<br>2: As a lower bound, we provide a naive randomchoice baseline, where labels are randomly assigned to match the label distribution in the <mark>training data</mark>.<br>",
    "Arabic": "مسند التدريب",
    "Chinese": "训练数据",
    "French": "donnée d'entraînement",
    "Japanese": "訓練データ",
    "Russian": "обучающий набор данных"
  },
  {
    "English": "training distribution",
    "context": "1: Domain mismatch -where the <mark>training distribution</mark> does not match the test distribution -can cause models with high training accuracy to do poorly on test data (Ben-David et al., 2010). We show that influence functions can identify the training examples most responsible for the errors, helping model developers identify domain mismatch.<br>2: Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness of the model and the narrowness of the <mark>training distribution</mark>. This can create problems for the pre-training plus fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then fine-tuned on very narrow task distributions.<br>",
    "Arabic": "توزيع التدريب",
    "Chinese": "训练分布",
    "French": "distribution d'entraînement",
    "Japanese": "訓練分布",
    "Russian": "распределение обучающих данных"
  },
  {
    "English": "training dynamic",
    "context": "1: The literature on these models is dense on theory, and derivations of sampling schedule, <mark>training dynamics</mark>, noise level parameterization, etc., tend to be based as directly as possible on theoretical frameworks, which ensures that the models are on a solid theoretical footing.<br>2: Despite being successful when applied in other domains, we found these classical methods are not effective enough when applied in our model. Based on some observations of <mark>training dynamics</mark> in our model, we propose a new method and explain why it can be more effective for improving the training stability.<br>",
    "Arabic": "الديناميكية التدريبية",
    "Chinese": "训练动态",
    "French": "dynamique d'entraînement",
    "Japanese": "トレーニングダイナミクス",
    "Russian": "динамика обучения"
  },
  {
    "English": "training epoch",
    "context": "1: In the remainder of this Section, we report the F 1 scores of the best models selected according to the highest F 1 score obtained on the validation set at the end of a <mark>training epoch</mark>. 3<br>2: Figure 4 shows the learning curve of the agent by measuring reward on the test set after each <mark>training epoch</mark>. The reward improves gradually and the accuracy on each entity increases simultaneously. Table 4 provides some examples where our model is able to extract the right values when the baseline fails.<br>",
    "Arabic": "دورة تدريب",
    "Chinese": "训练轮次",
    "French": "époque d'entraînement",
    "Japanese": "訓練エポック (くんれんエポック)",
    "Russian": "эпоха обучения"
  },
  {
    "English": "training error",
    "context": "1: In fact, the 34-layer plain net is still able to achieve competitive accuracy (Table 3), suggesting that the solver works to some extent. We conjecture that the deep plain nets may have exponentially low convergence rates, which impact the reducing of the <mark>training error</mark> 3 .<br>2: The existence of this constructed solution indicates that a deeper model should produce no higher <mark>training error</mark> than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).<br>",
    "Arabic": "خطأ التدريب",
    "Chinese": "训练误差",
    "French": "erreur d'entraînement",
    "Japanese": "訓練誤差",
    "Russian": "ошибка обучения"
  },
  {
    "English": "training example",
    "context": "1: Like Pegasos, at each iteration only a single (random) <mark>training example</mark> (y i , x i ) is considered, and if y i w, x i < 1, an update of the form w ← w + ηy i x i is performed.<br>2: P (x) = {k |k ∈ A(x),ỹ =ỹ}. (4) \n whereỹ is the predicted label for the corresponding <mark>training example</mark> of k . For computational efficiency, we also maintain a label queue to store past predictions.<br>",
    "Arabic": "مثال تدريبي",
    "Chinese": "训练样本",
    "French": "exemple d'entraînement",
    "Japanese": "訓練例",
    "Russian": "пример обучающей выборки"
  },
  {
    "English": "training loss",
    "context": "1: Hoffmann et al. [42] use <mark>training loss</mark> as their core metric. However, when repeating data for multiple epochs, <mark>training loss</mark> is a bad metric as models will overfit to the limited data available as shown in Figure 14. Thus, we use loss on a held-out test set as our key performance metric.<br>2: Existing CDA defines the ratio of the words perturbation as the difficulty measure for curriculums and a gradual course which increases the difficulty of curriculums when the <mark>training loss</mark> plateaus (Wei et al., 2021), which then ends when the most challenging curriculum ends. Although existing CDA is effective, yet there are several disadvantages.<br>",
    "Arabic": "خسارة التدريب",
    "Chinese": "训练损失",
    "French": "perte d'entraînement",
    "Japanese": "訓練損失",
    "Russian": "потеря обучения"
  },
  {
    "English": "training objective",
    "context": "1: To estimate the parameters of the framework, they employ a <mark>training objective</mark> that is closely related to the empirical risk computed over a large training data set. Specifically, it is common practice to employ either a structured hinge upper bound to the loss function [6,27], or an asymptotic alternative such as direct loss minimization [10,22].<br>2: s ⇠ is not used at test time, but only as part of our <mark>training objective</mark>.<br>",
    "Arabic": "هدف التدريب",
    "Chinese": "训练目标",
    "French": "objectif d'entraînement",
    "Japanese": "学習目的",
    "Russian": "цель обучения"
  },
  {
    "English": "training phase",
    "context": "1: After the ICCV submission, we retrained our model with the following : (1) In data augmentation, we further triples the dataset by scaling the training images to 50%, 100%, 150% of its original size. (2) In <mark>training phase</mark>, we use fullresolution images instead of resizing them to 400 × 400.<br>2: Let (X, D source , S source , Y value source ) be the set of samples seen during the <mark>training phase</mark> and (X, D target , S target , Y value target ) the samples which the model was not trained to track.<br>",
    "Arabic": "مرحلة التدريب",
    "Chinese": "训练阶段",
    "French": "phase d'entraînement",
    "Japanese": "トレーニングフェーズ",
    "Russian": "этап обучения"
  },
  {
    "English": "training procedure",
    "context": "1: The attacker's goal is to alter the <mark>training procedure</mark> by injecting poisoned samples into the training data, generating poisoned training data such that the resulting backdoored classifier differs from a clean classifier.<br>2: As a result, they often exploit prior knowledge and assumptions specific to these tasks in designing model architecture and <mark>training procedure</mark>, therefore not suited for generalizing to arbitrary dense prediction tasks (Snell et al., 2017;Fan et al., 2022;Iqbal et al., 2022;Hong et al., 2022).<br>",
    "Arabic": "إجراء التدريب",
    "Chinese": "训练过程",
    "French": "procédure d'entraînement",
    "Japanese": "訓練手順",
    "Russian": "процедура обучения"
  },
  {
    "English": "training process",
    "context": "1: Let c i and c j,j =i be the expected values of the clean model over the randomness of the <mark>training process</mark>, such as c i = E i (f i (x)) and c j,j =i = E j (f j (x)).<br>2: where cnt(id fj k ) is the number of occurence of the id in a batch. As the weights grow larger in the <mark>training process</mark>, the benefit of a threshold proportional to the norm of the weight is that the clipping value adaptively grows with the network.<br>",
    "Arabic": "عملية التدريب",
    "Chinese": "训练过程",
    "French": "processus d'entraînement",
    "Japanese": "学習過程",
    "Russian": "процесс обучения"
  },
  {
    "English": "training sample",
    "context": "1: For the ith <mark>training sample</mark>, denote the outputs of net by o i , the targets by t i , let the transfer function of each node in the jth layer of nodes be g j , and let the cost function be  \n<br>",
    "Arabic": "عينة تدريبية",
    "Chinese": "训练样本",
    "French": "échantillon d'entraînement",
    "Japanese": "訓練サンプル",
    "Russian": "обучающий образец"
  },
  {
    "English": "training set",
    "context": "1: The dataset is partitioned into K folds and one performs a split conformal set by sequentially defining the kth fold as calibration set and the remaining as <mark>training set</mark> for k ∈ {1, . . . , K}.<br>2: Evaluating bias amplification To evaluate the degree of bias amplification, we propose to compare bias scores on the <mark>training set</mark>, b * (o, g), with bias scores on an unlabeled evaluation set of imagesb(o, g) that has been annotated by a predictor.<br>",
    "Arabic": "مجموعة التدريب",
    "Chinese": "训练集",
    "French": "ensemble d'entraînement",
    "Japanese": "訓練データセット",
    "Russian": "тренировочный набор"
  },
  {
    "English": "training stability",
    "context": "1: Though much simpler than the production model, we found it to be a sufficiently good testbed for studying the <mark>training stability</mark> problem, as it allows us to train models faster and focus more on research perspectives instead of irrelevant modeling details.<br>",
    "Arabic": "استقرار التدريب",
    "Chinese": "训练稳定性",
    "French": "stabilité de l'entraînement",
    "Japanese": "訓練の安定性",
    "Russian": "стабильность обучения"
  },
  {
    "English": "training step",
    "context": "1: We use (x t , y t ) T t=1 , x t ⊂ D X , y t ⊂ D Y to represent the data samples at step t, T is the total number of <mark>training steps</mark>.<br>2: After training the Monarch model for 90% of the time, in the last 10% of the <mark>training steps</mark>, by transitioning to dense weight matrices, the model is able to reach the same performance of another model that was trained with dense weight matrices from scratch.<br>",
    "Arabic": "خطوة التدريب",
    "Chinese": "训练步骤",
    "French": "étape d'entraînement",
    "Japanese": "訓練ステップ",
    "Russian": "шаг обучения"
  },
  {
    "English": "training task",
    "context": "1: At the same time, the adaptation mechanism should be parameter-efficient (e.g., using a tiny amount of task-specific parameters) to prevent over-fitting to <mark>training tasks</mark> T train or test-time support set S Ttest .<br>",
    "Arabic": "تدريب المهمة",
    "Chinese": "训练任务",
    "French": "tâche d'entraînement",
    "Japanese": "訓練タスク",
    "Russian": "тренировочная задача"
  },
  {
    "English": "training time",
    "context": "1: The iterative algorithm represents only a subset of the constraints and therefore achieves a speedup at <mark>training time</mark>. In our case, the training samples are modestly sized whereas, at application time, a high-speed stream has to be processed. Therefore, we will develop a linear decoder in the next section.<br>",
    "Arabic": "وقت التدريب",
    "Chinese": "训练时间",
    "French": "temps d'entraînement",
    "Japanese": "訓練時間",
    "Russian": "время обучения"
  },
  {
    "English": "training token",
    "context": "1: Whilst we reach the same conclusion, we estimate that large models should be trained for many more <mark>training tokens</mark> than recommended by the authors. Specifically, given a 10× increase computational budget, they suggests that the size of the model should increase 5.5× while the number of <mark>training tokens</mark> should only increase 1.8×.<br>2: Instead, we find that model size and the number of <mark>training tokens</mark> should be scaled in equal proportions.<br>",
    "Arabic": "رموز التدريب",
    "Chinese": "训练标记",
    "French": "jeton d'entraînement",
    "Japanese": "学習トークン",
    "Russian": "токен обучения"
  },
  {
    "English": "trajectory forecasting",
    "context": "1: It can be shown 1 that the regret 1  Goal forecasting, <mark>trajectory forecasting</mark>, . . .<br>2: We address the problem of incrementally modeling and forecasting long-term goals of a first-person camera wearer: what the user will do, where they will go, and what goal they seek. In contrast to prior work in <mark>trajectory forecasting</mark>, our algorithm, DARKO, goes further to reason about semantic states (will I pick up an object?<br>",
    "Arabic": "التنبؤ بالمسارات",
    "Chinese": "轨迹预测",
    "French": "prévision de trajectoire",
    "Japanese": "軌跡予測",
    "Russian": "траекторное прогнозирование"
  },
  {
    "English": "trajectory optimization",
    "context": "1: In our approach, we integrate geometry estimation and tracking-by-detection in a combined system that searches for the best scene interpretation by global optimization. [2] also perform global <mark>trajectory optimization</mark> to track up to six mutually occluding individuals by modelling their posi-tions on a discrete occupancy grid.<br>2: In addition, we note that the interpolated gradient AoBG is able to automatically choose between the two gradients that performs better by utilizing empirical variance as a statistical measure of performance. Friction: Trajectory Optimization. We describe performance of gradients on the friction (Figure 4) environment.<br>",
    "Arabic": "تحسين المسار",
    "Chinese": "轨迹优化",
    "French": "optimisation de trajectoire",
    "Japanese": "軌道最適化",
    "Russian": "оптимизация траектории"
  },
  {
    "English": "transaction database",
    "context": "1: It then produces the set of all causal rules contained in the database as output. The algorithm starts by ignoring the temporal information from the sequence database to obtain a <mark>transaction database</mark>.<br>2: Here, we introduce the problem formulation. Let D be a <mark>transaction database</mark>, which has M items and N transactions. In this data set, a common task of correlation computing is to find all item pairs whose correlation coefficients are above a user-specified threshold θ. This is known as the all-strong-pairs correlation query problem [15].<br>",
    "Arabic": "قاعدة بيانات المعاملات",
    "Chinese": "交易数据库",
    "French": "base de données de transactions",
    "Japanese": "トランザクションデータベース",
    "Russian": "база транзакций"
  },
  {
    "English": "transductive learning",
    "context": "1: Although using a relational technique for an iid problem might still appear awkward, it has a well known precedent in machine learning research: <mark>transductive learning</mark> [Vapnik, 1998;Zhu et al., 2003]. In transduction, the learner knows the set of test patterns beforehand, and exploits this knowledge to make predictions that are ultimately dependent.<br>2: 2 We run the experiments using nine grammars of different characteristics, which we present in detail in Section 4. All experiments are run using <mark>transductive learning</mark>, i.e., we include the evaluation data in the unsupervised learning along with the training data.<br>",
    "Arabic": "التعلم النقلي",
    "Chinese": "直推学习",
    "French": "apprentissage transductif",
    "Japanese": "転移学習",
    "Russian": "Трансдуктивное обучение"
  },
  {
    "English": "transfer function",
    "context": "1: In-paint t t ting g g g Figure 8: Computed taxonomies for solving 22 tasks given various supervision budgets (x-axes), and maximum allowed transfer orders (y-axes). One is magnified for better visibility. Nodes with incoming edges are target tasks, and the number of their incoming edges is the order of their chosen <mark>transfer function</mark>.<br>2: For the ith training sample, denote the outputs of net by o i , the targets by t i , let the <mark>transfer function</mark> of each node in the jth layer of nodes be g j , and let the cost function be  \n<br>",
    "Arabic": "وظيفة التحويل",
    "Chinese": "传递函数",
    "French": "fonction de transfert",
    "Japanese": "伝達関数",
    "Russian": "функция передачи"
  },
  {
    "English": "transfer learning",
    "context": "1: <mark>Transfer learning</mark> for domain adaptation Prior work has shown the benefit of continued pretraining in domain (Alsentzer et al., 2019;Chakrabarty et al., 2019;. 5 We have contributed further investigation of the effects of a shift between a large, diverse pretraining corpus and target domain on task performance.<br>",
    "Arabic": "نقل التعلم",
    "Chinese": "迁移学习",
    "French": "apprentissage par transfert",
    "Japanese": "転移学習",
    "Russian": "Перенос обучения"
  },
  {
    "English": "transformation function",
    "context": "1: The swap and expansion move algorithms can be encoded as a vector of binary variables t ={t i , ∀i ∈ V}. The <mark>transformation function</mark> T (x p , t) of a move algorithm takes the current labelling x p and a move t and returns the new labelling x which has been induced by the move.<br>2: In an α-expansion move every random variable can either retain its current label or transition to label α. One iteration of the algorithm involves making moves for all α in L successively. The <mark>transformation function</mark> T α (x i , t i ) for an α-expansion move transforms the label of a random variable x i as: \n<br>",
    "Arabic": "وظيفة التحويل",
    "Chinese": "转换函数",
    "French": "fonction de transformation",
    "Japanese": "変換関数",
    "Russian": "функция преобразования"
  },
  {
    "English": "transformation matrix",
    "context": "1: For projecting data a <mark>transformation matrix</mark> is needed. When an instance x is projected the vector containing its values is multiplied by this matrix obtaining a new vector (i.e. x projection). In random projections the matrix entries are random numbers. In this work three types of random projections have been used. 1.<br>2: Jointly transforming source and target domains into a common low dimensional space was also done together with a conjugate gradient minimisation of a <mark>transformation matrix</mark> with orthogonality constraints [3] and with dictionary learning to find subspace interpolations [32,38,47]. Sun et al.<br>",
    "Arabic": "مصفوفة التحويل",
    "Chinese": "变换矩阵",
    "French": "matrice de transformation",
    "Japanese": "変換行列",
    "Russian": "матрица преобразования"
  },
  {
    "English": "transformer architecture",
    "context": "1: Notably, the Quasi-RNN  and SRU (Lei et al., 2018) have invented highly-parallelizable recurrence and combined them with convolutions or highway networks respectively. The resulting architectures achieve equivalent parallelism as convolutional and attention models. This advancement eliminates the need of avoiding recurrence computation to trade model training efficiency, a design choice made by the <mark>Transformer architecture</mark>.<br>2: A.1 Transformer Architecture \n The neural models implemented in this work are based on the self-attentional <mark>Transformer architecture</mark> (Vaswani et al., 2017). Formally, given a sequence of source tokens (encoded as one-hot vectors) s 1...J = (s 1 , . . .<br>",
    "Arabic": "بِنية المُحَوِّل",
    "Chinese": "变压器架构",
    "French": "architecture de transformateur",
    "Japanese": "\"Transformer アーキテクチャ\"",
    "Russian": "архитектура трансформера"
  },
  {
    "English": "transformer block",
    "context": "1: Swin <mark>Transformer block</mark> Swin Transformer is built by replacing the standard multi-head self attention (MSA) module in a <mark>Transformer block</mark> by a module based on shifted windows (described in Section 3.2), with other layers kept the same.<br>",
    "Arabic": "كتلة المحول",
    "Chinese": "Transformer 模块",
    "French": "bloc transformateur",
    "Japanese": "トランスフォーマーブロック",
    "Russian": "блок трансформера"
  },
  {
    "English": "transformer decoder",
    "context": "1: former is often referred to as a \"Transformer encoder\" while the left-context-only version is referred to as a \"<mark>Transformer decoder</mark>\" since it can be used for text generation. In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens.<br>2: We load the pretrained AV-HuBERT 8 for front-ends and downstream speech recognition model, and then follow its sequence-to-sequence (S2S) finetuning configurations (Shi et al., 2022b) to train our system. We use <mark>Transformer decoder</mark> to decode the encoded features into unigram-based subword units (Kudo, 2018), where the vocabulary size is set to 1000.<br>",
    "Arabic": "مفكك تحويلي",
    "Chinese": "变压器解码器",
    "French": "décodeur Transformer",
    "Japanese": "トランスフォーマーデコーダー (Transformer decoder)",
    "Russian": "трансформаторный декодер"
  },
  {
    "English": "transformer encoder",
    "context": "1: Figure 1 depicts the overall architecture of our model. The basis for our model is the <mark>Transformer encoder</mark> introduced by Vaswani et al. (2017): we transform word embeddings into contextually-encoded token representations using stacked multi-head self-attention and feedforward layers ( §2.1).<br>2: A distinctive feature of BERT is its unified architecture across different tasks. There is mini-mal difference between the pre-trained architecture and the final downstream architecture. Model Architecture BERT's model architecture is a multi-layer bidirectional <mark>Transformer encoder</mark> based on the original implementation described in Vaswani et al. (2017) and released in the tensor2tensor library.<br>",
    "Arabic": "مشفّر المحوّل",
    "Chinese": "变压器编码器",
    "French": "encodeur Transformer",
    "Japanese": "トランスフォーマーエンコーダー",
    "Russian": "трансформер-кодировщик"
  },
  {
    "English": "transformer language model",
    "context": "1: But this is not an easy task for <mark>transformer language models</mark>, among which only those equipped with special relative positional embeddings (Press et al., 2022;Chi et al., 2022) are length extrapolatable.<br>2: Large language models Scaling up <mark>transformer language models</mark> [111] across parameter count and training data has been shown to result in continuous performance gains [19]. Starting with the 1.4 billion parameter GPT-2 model [88], a variety of scaled-up language models have been trained, commonly referred to as large language models (LLMs).<br>",
    "Arabic": "نموذج لغة المحول",
    "Chinese": "变压器语言模型",
    "French": "modèle de langage transformateur",
    "Japanese": "トランスフォーマー言語モデル",
    "Russian": "языковая модель трансформера"
  },
  {
    "English": "transformer layer",
    "context": "1: where h l is the hidden representation of subsequent layer normalization output after feed-forward layer in the <mark>transformer layer</mark> l, U l and D l are up-and down-projection matrices, r l is the hidden state directly from feed-forward layer.<br>2: . For the <mark>transformer layer</mark>, we fix the number of layers, L = 2 and set all layer sizes, d = 768 (including the intermediate size for the dense layer). 2 We compare our model with previous state of the art neural architectures, including on-device approaches.<br>",
    "Arabic": "طبقة المحولات",
    "Chinese": "转换器层",
    "French": "couche de transformateur",
    "Japanese": "トランスフォーマー層",
    "Russian": "слой трансформера"
  },
  {
    "English": "transformer model",
    "context": "1: Graphormer (Ying et al., 2021a) develops the centrality encoding, spatial encoding, and edge encoding to incorporate the graph structure information into the <mark>Transformer model</mark>.<br>",
    "Arabic": "نموذج المحول",
    "Chinese": "变形器模型",
    "French": "Modèle transformateur",
    "Japanese": "トランスフォーマーモデル",
    "Russian": "модель трансформера"
  },
  {
    "English": "transformer variant",
    "context": "1: We also evaluated alternative strategies but these performed worse in preliminary experiments 5 . Future work can evaluate efficient <mark>transformer variants</mark> (Guo et al., 2022;Beltagy et al., 2020).<br>",
    "Arabic": "المتغيرات المحولة",
    "Chinese": "变压器变体",
    "French": "variante de transformateur",
    "Japanese": "トランスフォーマー変種",
    "Russian": "вариант трансформера"
  },
  {
    "English": "transformer-base architecture",
    "context": "1: The four morphological output feature vectors are further concatenated with another stem embedding at the sentence level to form the input vector for the main sentence/document encoder. The choice of this <mark>transformer-based architecture</mark> for morphology encoding is motivated by two factors. First, Zaheer et al.<br>",
    "Arabic": "الهندسة المعمارية القائمة على المحولات",
    "Chinese": "基于Transformer的架构",
    "French": "architecture à base de transformateur",
    "Japanese": "トランスフォーマー基盤アーキテクチャ",
    "Russian": "архитектура на основе трансформатора"
  },
  {
    "English": "transformer-base language model",
    "context": "1: The emergence of large-scale, pretrained, <mark>Transformer-based language models</mark> (LLMs) has marked the commencement of an avant-garde era in NLP. Departing from the traditional methods of neural language learning with temporally separated training-testing phases for downstream tasks, pretrained LLMs have shown the ability to infer labels from test inputs conditioned on the training data within a single pass.<br>",
    "Arabic": "نموذج اللغة القائم على المحولات",
    "Chinese": "基于Transformer的语言模型",
    "French": "modèle de langage basé sur un transformateur",
    "Japanese": "トランスフォーマーベースの言語モデル",
    "Russian": "языковая модель на основе трансформера"
  },
  {
    "English": "transformer-base model",
    "context": "1: We show that state-of-the-art models do not rely on sentence structure the way we think they should: NLI models (<mark>Transformer-based models</mark>, RNNs, and ConvNets) are largely insensitive to permutations of word order that corrupt the original syntax. We also show that reordering words can cause models to flip classification labels.<br>2: The great success of <mark>Transformer-based models</mark> benefits from the powerful multi-head self-attention mechanism, which learns token dependencies and encodes contextual information from the input. Prior work strives to attribute model decisions to individual input features with different saliency measures, but they fail to explain how these input features interact with each other to reach predictions.<br>",
    "Arabic": "نموذج قائم على المحولات",
    "Chinese": "基于Transformer的模型",
    "French": "modèle basé sur un transformateur",
    "Japanese": "トランスフォーマーベースモデル",
    "Russian": "модель на основе трансформера"
  },
  {
    "English": "transformer-like",
    "context": "1: In this work, we studied simplified token mixing modules for <mark>Transformer-like</mark> encoder architectures, making several contributions. First, we showed that simple, linear mixing transformations, along with the nonlinearities in feed-forward layers, can competently model diverse semantic relationships in text.<br>",
    "Arabic": "مثل المحولات",
    "Chinese": "类似变压器",
    "French": "transformeur-similaire",
    "Japanese": "\"トランスフォーマーライク\"",
    "Russian": "трансформерный"
  },
  {
    "English": "transition distribution",
    "context": "1: , the ability to sample from the <mark>transition distribution</mark> , and a reward function r ( h ) . Here, h = (s 0 , a 0 , . . . , s n−1 , a n−1 , s n ) is a history of states and actions visited while interpreting one document.<br>2: Thus we sample θ 1 from three types of sources: the innovation distribution p ν , the q-subsampled base distribution p qµ , and the <mark>transition distribution</mark> T (φ k , •). In doing so, we first sample a variable u 1 that indicates which source to sample from.<br>",
    "Arabic": "توزيع الانتقال",
    "Chinese": "转移分布",
    "French": "distribution de transition",
    "Japanese": "遷移分布",
    "Russian": "распределение переходов"
  },
  {
    "English": "transition dynamic",
    "context": "1: In our work, T is built by keeping a table of observed (s, a, s ) triplets, which describes the connectivity graph over the state space. More advanced methods could also be used to infer more complex <mark>transition dynamics</mark> [27,30].<br>2: It would now be convenient to view the problem facing the principal as an (single-agent) MDP M * = S, (A s ) s∈S , P * , R * , where S is the same state space in M; A s defines an (possibly infinite) action space for each s; the <mark>transition dynamics</mark> \n P * : S × ∆ ( Θ × A ) × S → [ 0 , 1 ] and reward function R * : S × ∆ ( Θ × A ) → R are such that P * ( s , x , s ′ ) = E ( θ , a ) ∼x P ( s , a , s ′<br>",
    "Arabic": "الديناميكية الانتقالية",
    "Chinese": "转移动态",
    "French": "dynamique de transition",
    "Japanese": "遷移ダイナミクス",
    "Russian": "динамика переходов"
  },
  {
    "English": "transition function",
    "context": "1: A is the space of internal parameters, including both discrete terminal time τ and the continuous denoising strength/penalty parameters (σ k , µ k ). The <mark>transition function</mark> p : S × A → S maps input state s ∈ S to its outcome state s ∈ S after taking action a ∈ A.<br>2: Let M be an arbitrary deterministic Turing machine with finite alphabet Γ containing the blank symbol , the finite set of states S containing the initial state s and the halting state h, and <mark>transition function</mark> δ : S × Γ → S × Γ × {L, R}.<br>",
    "Arabic": "دالة الانتقال",
    "Chinese": "状态转移函数",
    "French": "fonction de transition",
    "Japanese": "遷移関数",
    "Russian": "функция перехода"
  },
  {
    "English": "transition graph",
    "context": "1: The proposed model for factorizing transition cubes can also be applied to estimate a transition matrix A (see formula (3)) for cases where no personalization of the <mark>transition graph</mark> is desired. By skipping the user-interactions in equation (15), a factorization model for normal <mark>transition graph</mark>s is obtained:â \n<br>2: On the other hand, MC methods model sequential behavior by learning a <mark>transition graph</mark> over items that is used to predict the next action based on the recent actions of a user. In this paper, we present a method bringing both approaches together. Our method is based on personalized <mark>transition graph</mark>s over underlying Markov chains.<br>",
    "Arabic": "الرسم البياني للانتقالات",
    "Chinese": "转移图",
    "French": "graphe de transition",
    "Japanese": "遷移グラフ",
    "Russian": "граф переходов"
  },
  {
    "English": "transition kernel",
    "context": "1: A Markov decision process ( MDP ) is defined by a tuple M = ( S , A , p , p 0 , R , γ ) specifying a set of states S and actions A ; a <mark>transition kernel</mark> p ; an initial state distribution p 0 ; a reward function R ; and a discount factor γ ∈ [ 0 ,<br>2: We establish that, as in the Euclidean case, the corresponding time-reversal process is also a diffusion whose drift includes the Stein score which is intractable but can similarly be estimated via score matching. Methodological extensions are required as in most cases the <mark>transition kernel</mark> of the noising process cannot be sampled exactly.<br>",
    "Arabic": "نواة الانتقال",
    "Chinese": "转移核",
    "French": "noyau de transition",
    "Japanese": "遷移カーネル",
    "Russian": "ядро перехода"
  },
  {
    "English": "transition matrix",
    "context": "1: a real search engine SE, we would have had to construct the adjacency matrix of the document graph G P + corresponding to SE, transform this matrix into the two <mark>transition matrices</mark> P mh and P md , and then estimate the spectral gaps of these matrices.<br>2: • To deal with the sparsity for the estimation of transition probabilities, we introduce a factorization model that can be applied both to personalized and normal <mark>transition matrices</mark>. This factorization approach results in less parameters and due to generalization to a better quality than full parametrized models.<br>",
    "Arabic": "مصفوفة الانتقال",
    "Chinese": "转移矩阵",
    "French": "matrice de transition",
    "Japanese": "遷移行列",
    "Russian": "матрица перехода"
  },
  {
    "English": "transition model",
    "context": "1: Thus we can compute the <mark>transition model</mark> simply by normalizing the product of the neighboring clique potentials. Finkel, Grenager, and Manning (2005) gave a more detailed account of how to compute this quantity.<br>2: The first updates the distribution by multiplying by the <mark>transition model</mark> and marginalizing out the previous timestep: P (σ (t+1) |z (1) , . . .<br>",
    "Arabic": "نموذج الانتقال",
    "Chinese": "转移模型",
    "French": "modèle de transition",
    "Japanese": "遷移モデル",
    "Russian": "модель перехода"
  },
  {
    "English": "transition probability",
    "context": "1: have the same memory curve) as a two state model with <mark>transition probabilities</mark> equal to 1, as shown in Figure 2(c). This two state model has the equilibrium distribution p ∞ = (f dep , f pot ) and its flux is given by Φ −+ = rf pot f dep .<br>2: • S-rectangular adversary: Compared to SA-rectangular adversaries, S-rectangular adversaries exhibit a more constrained influence, limiting the set of <mark>transition probabilities</mark> by enforcing specific correlations among different actions.<br>",
    "Arabic": "احتمالية الانتقال",
    "Chinese": "转移概率",
    "French": "probabilité de transition",
    "Japanese": "遷移確率",
    "Russian": "вероятность перехода"
  },
  {
    "English": "transition probability model",
    "context": "1: An MDP consists of a set of states S = {s 0 , s 1 , s 2 , . . . , s t , . . . } , a set of actions A = { a k } K k=1 , and a reward function r : S × A → R. After executing an action a t ∈ A at each state s t ∈ S , the agent will enter a new state s t+1 according to the <mark>transition probability model</mark> and get a reward r ( s<br>",
    "Arabic": "نموذج احتمالية الانتقال",
    "Chinese": "转移概率模型",
    "French": "modèle de probabilité de transition",
    "Japanese": "遷移確率モデル",
    "Russian": "модель вероятности перехода"
  },
  {
    "English": "transition system",
    "context": "1: Despite not being exact, transition-based parsers offer faster and typically linear-time parsing algorithms (Kudo and Matsumoto, 2002;Yamada and Matsumoto, 2003;Nivre, 2003). The dependency tree is inferred with a greedy search through <mark>transition system</mark> actions.<br>2: For example, with N = 10000 (strategy B-N10k), shrinking is performed to guarantee that no intermediate <mark>transition system</mark> has more than 10,000 abstract states, while with N = ∞ (strategy B-N∞) there is no size bound, so that a perfect heuristic is constructed. The threshold parameter (Helmert et al.<br>",
    "Arabic": "نظام الانتقالات",
    "Chinese": "过渡系统",
    "French": "système de transition",
    "Japanese": "遷移システム",
    "Russian": "система переходов"
  },
  {
    "English": "transition-base dependency parsing",
    "context": "1: We present experiments on three NLP tasks for which greedy sequence labeling has been a successful solution: part-of-speech tagging, <mark>transition-based dependency parsing</mark> and named entity recognition. In all cases our method achieves multiplicative speedups at test time with little loss in accuracy.<br>2: Parameter estimation is arranged to maximize accuracy and early confidence in this sequence. Our approach is simpler and better suited to NLP than other related cascade methods. We present experiments in left-to-right part-of-speech tagging, named entity recognition, and <mark>transition-based dependency parsing</mark>.<br>",
    "Arabic": "تحليل التبعية القائم على الانتقالات",
    "Chinese": "基于转移的依存分析",
    "French": "Analyse de dépendance basée sur les transitions",
    "Japanese": "遷移ベースの依存関係の解析",
    "Russian": "парсинг зависимостей на основе перехода"
  },
  {
    "English": "transition-base model",
    "context": "1: Table 2 shows that the final pass dominates the computational cost, while each of the pruning passes takes up roughly the same amount of time. Our second-and third-order cascades also significantly outperform ZHANGNIVRE. The transitionbased model with k = 8 is very efficient and effective, but increasing the k-best list size scales much worse than employing multi-pass pruning.<br>",
    "Arabic": "نموذج قاعدة الانتقال",
    "Chinese": "基于转移的模型",
    "French": "modèle basé sur la transition",
    "Japanese": "遷移ベースモデル",
    "Russian": "модель на основе переходов"
  },
  {
    "English": "transition-base parser",
    "context": "1: Our pruned third-order model is twice as fast as an unpruned first-order model and also compares favorably to a state-of-the-art <mark>transition-based parser</mark> for multiple languages.<br>2: We believe that a careful design of fea-13 Unlike our model, the hybrid models used here as baselines make use of the dependency labels at training time; indeed, the <mark>transition-based parser</mark> is trained to predict a labeled dependency parse tree, and the graph-based parser use these predicted labels as input features.<br>",
    "Arabic": "محلل قائم على الانتقالات",
    "Chinese": "基于转移的解析器",
    "French": "analyseur à base de transition",
    "Japanese": "遷移ベースパーサー",
    "Russian": "парсер на основе переходов"
  },
  {
    "English": "transition-base parsing",
    "context": "1: Our specific method of incorporating supervision into self-attention is most similar to the concurrent work of Liu and Lapata (2018), who use edge marginals produced by the matrix-tree algorithm as attention weights for document classification and natural language inference. The question of training on gold versus predicted labels is closely related to learning to search ( Daumé III et al. , 2009 ; Ross et al. , 2011 ; Chang et al. , 2015 ) and scheduled sampling ( Bengio et al. , 2015 ) , with applications in NLP to sequence labeling and <mark>transition-based parsing</mark> ( Choi and Palmer , 2011 ;<br>",
    "Arabic": "تحليل قائم على الانتقالات",
    "Chinese": "基于转移的句法分析",
    "French": "analyse par transitions",
    "Japanese": "遷移ベースの構文解析",
    "Russian": "Синтаксический анализ на основе переходов"
  },
  {
    "English": "transitive closure",
    "context": "1: The inverse role R −1 and the (non-reflexive) <mark>transitive closure</mark> role R + are also roles. Semantics. The semantics of concepts and roles are defined relative to a given universe of discourse ∆.<br>2: The set of ancestors of a node is anc(n) and is the <mark>transitive closure</mark> of par(•); we also define anc + (n) = anc(n) ∪ {n}.<br>",
    "Arabic": "الإغلاق الانتقالي",
    "Chinese": "传递闭包",
    "French": "fermeture transitive",
    "Japanese": "推移閉包",
    "Russian": "транзитивное замыкание"
  },
  {
    "English": "transitive relation",
    "context": "1: For proving (ii), we interpret every non-simple role R in I as the union of R J and the transitive closures of T J for every transitive sub-role T of R (T J is not necessarily a <mark>transitive relation</mark> since O does not contain the transitivity axioms).<br>",
    "Arabic": "علاقة انتقالية",
    "Chinese": "传递关系",
    "French": "relation transitive",
    "Japanese": "推移的関係",
    "Russian": "\"переходное отношение\""
  },
  {
    "English": "translation invariance",
    "context": "1: In a next experiment we analyze the influence of encoding the <mark>translation invariance</mark> into the likelihood (our TIWD model) versus the standard WD process and row-mean subtraction as described in section 3.2. A similar random procedure for generating distance matrices is used, but this time we vary the number of replications d and the mean vector µ.<br>2: The kernel of the one- dimensional convolution has size k × 1 where k ≥ 3; the larger the value of k the broader the context that is captured. The weight sharing in the convolution ensures <mark>translation invariance</mark> of the computed features along each row. The computation proceeds as follows.<br>",
    "Arabic": "ثبات الترجمة",
    "Chinese": "平移不变性",
    "French": "invariance à la translation",
    "Japanese": "平移不変性",
    "Russian": "трансляционная инвариантность"
  },
  {
    "English": "translation model",
    "context": "1: To use these three component models in a direct maximum entropy approach, we define three different feature functions for each component of the <mark>translation model</mark> instead of one feature function for the whole <mark>translation model</mark> p(f J 1 |e I 1 ).<br>2: And the knowledge transfer method can further provide better optimization than training from scratch for the introduced parameters. Moreover, except for the pluggable modules, all the parameters of the original model are frozen. Therefore, our architecture can also retain previously learned knowledge from the original <mark>translation model</mark> to completely maintain the translation qualities on original language pairs.<br>",
    "Arabic": "نموذج الترجمة",
    "Chinese": "翻译模型",
    "French": "modèle de traduction",
    "Japanese": "翻訳モデル",
    "Russian": "модель перевода"
  },
  {
    "English": "translation system",
    "context": "1: As a result, researchers and communities looking to train <mark>translation systems</mark> for low-resource languages may find themselves wondering how much parallel data is required to achieve a given performance target level.<br>",
    "Arabic": "نظام الترجمة",
    "Chinese": "翻译系统",
    "French": "système de traduction",
    "Japanese": "翻訳システム",
    "Russian": "система машинного перевода"
  },
  {
    "English": "translation vector",
    "context": "1: with world coordinates r u,v (d) of a point along the ray with distance d to the camera, camera intrinsics K, and camera rotation matrix R and <mark>translation vector</mark> t. For each ray, we aim to solve \n arg min d s.t.<br>",
    "Arabic": "ناقل الترجمة",
    "Chinese": "平移向量",
    "French": "vecteur de translation",
    "Japanese": "並進ベクトル",
    "Russian": "вектор смещения"
  },
  {
    "English": "transliteration",
    "context": "1: We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and <mark>transliteration</mark>.<br>2: help , since training time has never been an issue in morphology tasks . 2 In this work, we provide state-of-the-art numbers for morphological inflection and historical text normalization, a novel result in the literature. We also show the transformer outperforms a strong recurrent baseline on two other characterlevel tasks: grapheme-to-phoneme (g2p) conversion and <mark>transliteration</mark>.<br>",
    "Arabic": "حرفي",
    "Chinese": "音译",
    "French": "translitération",
    "Japanese": "転写",
    "Russian": "транслитерация"
  },
  {
    "English": "transpose",
    "context": "1: Although the derivation of the algorithm will assume noise free data, the algorithm is designed to work with moderate noise, as we will soon point out. Notation. Let z be a vector in R K or C K and let z T be its <mark>transpose</mark>.<br>2: When per-category reliability is additionally supplemented by overall reliability (computed by Equation (2)), we compute C n as: \n C n = [ro1, ro2, ..., roT ] T C n . (5 \n ) \n is the element-wise multiplication and T is the number of annotators, while T means <mark>transpose</mark> of a matrix.<br>",
    "Arabic": "المعكوس",
    "Chinese": "转置",
    "French": "transposée",
    "Japanese": "転置",
    "Russian": "транспонирование"
  },
  {
    "English": "transposition table",
    "context": "1: It is possible that the value of a search is influenced by a draw-byrepetition score, and this result is saved in the proof tree or in a <mark>transposition table</mark>. If this cached result is later reached by a different move sequence, it might not be correct-we do not know if the draw-by-repetition scores are correct.<br>",
    "Arabic": "جدول التبديل",
    "Chinese": "置换表",
    "French": "table de transposition",
    "Japanese": "転置表",
    "Russian": "таблица транспозиции"
  },
  {
    "English": "travel salesman problem",
    "context": "1: Experiments show it improves on NRPA for different application domains: SameGame and the <mark>Traveling Salesman Problem</mark> with Time Windows.<br>",
    "Arabic": "مشكلة البائع المتجول",
    "Chinese": "旅行推销员问题",
    "French": "Problème du voyageur de commerce",
    "Japanese": "巡回セールスマン問題",
    "Russian": "задача коммивояжера"
  },
  {
    "English": "tree data structure",
    "context": "1: Room specs provide the ability to specify the rooms that appear in a house, the relative size of each room, and how the rooms are connected with doors. Their idea was first proposed in [75]. A room spec is manually specified with a <mark>tree data structure</mark>.<br>",
    "Arabic": "هيكل بيانات شجري",
    "Chinese": "树数据结构",
    "French": "structure de données arborescente",
    "Japanese": "木構造データ",
    "Russian": "древовидная структура данных"
  },
  {
    "English": "tree decomposition",
    "context": "1: The width of a <mark>tree decomposition</mark> T is given by max t∈VT |ξ T (t)| − 1. Now the treewidth of H ϕ , tw(H) is the minimum width of any of its <mark>tree decomposition</mark>s. We denote by tw(ϕ) the treewidth of H ϕ .<br>2: (2016). As usual, we define the treewidth of a conjunctive expression ϕ(x) in TL as the treewidth of its associated hypergraph H ϕ . We recall the definition of treewidth ( modified to our setting ) : A <mark>tree decomposition</mark> T = ( V T , E T , ξ T ) of H ϕ with ξ T : V T → 2 V is such that • For any F ∈ E , there is a t ∈ V T such that F ⊆ ξ T (<br>",
    "Arabic": "تفكيك الشجرة",
    "Chinese": "树分解",
    "French": "\"décomposition arborescente\"",
    "Japanese": "木分解",
    "Russian": "декомпозиция дерева"
  },
  {
    "English": "tree depth",
    "context": "1: This implies that any sample x will reach every node in the tree with a positive probability (as evident from ( 2)). Thus, computing the output of the tree in (3) will require computation over every node in the tree, an operation which is exponential in <mark>tree depth</mark>. Hazimeh et al.<br>2: This leads to a γ h -contracting procedure, where γ is the discount factor and h is the <mark>tree depth</mark>. To establish our results, we first introduce a notion called multiple-step greedy consistency.<br>",
    "Arabic": "عمق الشجرة",
    "Chinese": "树深度",
    "French": "profondeur de l'arbre",
    "Japanese": "木の深さ",
    "Russian": "глубина дерева"
  },
  {
    "English": "tree ensemble",
    "context": "1: Another basic, but well performing approach is If-Then-Else. Each decision tree is translated into a sequence of if-then-else blocks, e.g., in C++ language. The resulting code is compiled to generate an efficient document scorer. If-Then-Else aims at taking advantage of compiler optimization strategies, which can potentially re-arrange the <mark>tree ensemble</mark> traversal into a more efficient procedure.<br>2: where Z, U are the learnable parameters in the splitting internal supernodes and the leaves of the <mark>tree ensemble</mark> for the logit model for π n and W, O are the learnable parameters in the supernodes and the leaves of the <mark>tree ensemble</mark> for the log-tree model for µ n respectively. The likelihood function for this ZIP model is given by \n<br>",
    "Arabic": "تجميع الأشجار",
    "Chinese": "树集成",
    "French": "ensemble d'arbres",
    "Japanese": "木構造アンサンブル",
    "Russian": "набор деревьев"
  },
  {
    "English": "tree search",
    "context": "1: Levin Tree Search (LevinTS, which we abbreviate to LTS here) is a tree/graph search algorithm based on best-first search [Pearl, 1984] that uses the cost function 2 n → d(n)/π(n) , which, for convenience, we abbreviate as d π (n).<br>2: Referring to the planning problem as <mark>tree search</mark>, a reasonable practice in these implementations is to back up the value only at the leaves while the information obtained at the root is not leveraged other than for updating the policy. Here, we question the potency of this approach.<br>",
    "Arabic": "بحث الشجرة",
    "Chinese": "树搜索",
    "French": "recherche arborescente",
    "Japanese": "木探索",
    "Russian": "поиск по дереву"
  },
  {
    "English": "tree structure",
    "context": "1: The two central tasks, human pose estimation and object detection, have been studied in computer vision for many years. Most of the pose estimation work uses a <mark>tree structure</mark> of the human body [10,26,1] which allows fast inference. In order to capture more complex body articulations, some non-tree models have also been proposed [27,31].<br>",
    "Arabic": "هيكل شجري",
    "Chinese": "树形结构",
    "French": "structure arborescente",
    "Japanese": "木構造",
    "Russian": "структура дерева"
  },
  {
    "English": "tree width",
    "context": "1: This has time complexity O(N |Y| w+1 ), where N is the number of nodes and w is the <mark>tree width</mark> of the graph, i.e., the size of its largest clique minus 1 after the graph is optimally triangulated.<br>2: For trees and 1-D CRFs (chains), w = 1, so that calculating (8) directly is feasible. However, for more general cases like 2-D CRFs (grids), the <mark>tree width</mark> w is prohibitively high, and one has to resort to approximate approaches, e.g., sampling and variational methods.<br>",
    "Arabic": "عرض الشجرة",
    "Chinese": "树宽",
    "French": "largeur d'arbre",
    "Japanese": "木幅",
    "Russian": "ширина дерева"
  },
  {
    "English": "tree-base model",
    "context": "1: Moreover, we aim at investigating whether we can introduce further optimizations in the algorithms, considering that the same <mark>tree-based model</mark> is applied to a multitude of feature vectors, and thus we could have the chance of partially reusing some work.<br>2: Algorithm 1 takes as input a feature vector x, a set of features S, and a binary tree, which represents the <mark>tree-based model</mark>. The tree is defined by the following vectors: v is a vector of node values; internal nodes are assigned the value internal.<br>",
    "Arabic": "نموذج مبني على شجرة",
    "Chinese": "基于树模型",
    "French": "modèle basé sur un arbre",
    "Japanese": "木構造モデル (ki kouzou moderu)",
    "Russian": "модель на основе дерева"
  },
  {
    "English": "treebank annotation",
    "context": "1: We report both unlabeled attachment score (UAS) and labeled attachment score (LAS) (Buchholz and Marsi, 2006). This is likely the first reliable cross-lingual parsing evaluation. In particular, previous studies could not even report LAS due to differences in <mark>treebank annotations</mark>. We can make several interesting observations.<br>",
    "Arabic": "\"ترميز البنك الشجري\"",
    "Chinese": "树库标注",
    "French": "annotation de corpus arboré",
    "Japanese": "木構造アノテーション (treebank annotation)",
    "Russian": "аннотация трибанка"
  },
  {
    "English": "tri-gram",
    "context": "1: For example, LibMultiLabel only uses uni-gram, while Chalkidis et al. (2022) set ngram_range to (1, 3), so uni-gram, bi-gram, and <mark>tri-gram</mark> are extracted into the vocabulary list for a richer representation of the document. min_df: The parameter is used for removing infrequent tokens. Chalkidis et al.<br>2: While various settings of bag-of-words features such as bi-gram or <mark>tri-gram</mark> can be considered, we advocate that simple uni-gram TF-IDF features trained by linear classifiers can be a useful baseline to start with for text classification. • Advanced architectures such as BERT may only achieve the best results if properly used.<br>",
    "Arabic": "تراي-جرام",
    "Chinese": "三连词",
    "French": "tri-gramme",
    "Japanese": "トリグラム",
    "Russian": "триграмма"
  },
  {
    "English": "triangle inequality",
    "context": "1: This distance satisfies all of the properties of a metric: it is non-negative, symmetric, and satisfies the <mark>triangle inequality</mark> [27]. Using this metric on H, we can define the distance between two images as \n<br>",
    "Arabic": "عدم المساواة المثلث",
    "Chinese": "三角不等式",
    "French": "inégalité triangulaire",
    "Japanese": "三角不等式",
    "Russian": "треугольное неравенство"
  },
  {
    "English": "trifocal tensor",
    "context": "1: The purpose of this section is the main scope of the paper, namely the formulation of a set of algebraic constraints that are sufficient for 27 numbers, arranged as in (10), to constitute a <mark>trifocal tensor</mark>.<br>2: In order to make the material as self contained as possible we give in Section 2 a derivation of the <mark>trifocal tensor</mark>. Overlap with existing literature is unavoidable, however many things appear in new form.<br>",
    "Arabic": "موتر ثلاثي البؤرة",
    "Chinese": "三焦张量",
    "French": "tenseur trifocal",
    "Japanese": "三焦点テンソル",
    "Russian": "трёхфокальный тензор"
  },
  {
    "English": "trigger model",
    "context": "1: In [10], a <mark>Triggering Model</mark> was introduced for modeling the spread of influence in a social network. As the authors show, this model generalizes the Independent Cascade, Linear Threshold and Listen-once models commonly used for modeling the spread of influence.<br>2: In every instance of the <mark>Triggering Model</mark>, the influence function σ(•) is submodular. Beyond the Independent Cascade and Linear Threshold, there are other natural special cases of the <mark>Triggering Model</mark>. One example is the \"Only-Listen-Once\" Model.<br>",
    "Arabic": "نموذج الزناد",
    "Chinese": "触发模型",
    "French": "modèle de déclenchement",
    "Japanese": "発火モデル",
    "Russian": "модель триггера"
  },
  {
    "English": "trigram language model",
    "context": "1: The first term on the right, p(s), is the language model, and is what we compute via parsing in Equation 2. Virtually all current speech recognition systems use the so-called <mark>trigram language model</mark> in which the probability of a string is broken down into conditional probabilities on each word given the two previous words.<br>2: We explored two methods for selecting a final string. In one method, a <mark>trigram language model</mark> built using the Europarl (Koehn, 2005) data with start/end symbols returns the highest-scoring description (normalizing for length).<br>",
    "Arabic": "نموذج لغة الثلاثية",
    "Chinese": "三元语言模型",
    "French": "modèle de langage trigramme",
    "Japanese": "3グラム言語モデル",
    "Russian": "триграммная языковая модель"
  },
  {
    "English": "trigram model",
    "context": "1: The perplexity for both of these models significantly improve upon the <mark>trigram model</mark> base-line as well as the best previous grammarbased language model. For the better of our two models these improvements are 24% and 14% respectively.<br>2: We have presented two grammar-based language models, both of which significantly improve upon both the <mark>trigram model</mark> baseline for the task (by 24% for the better of the two) and the best previous grammar-based language model (by 14%).<br>",
    "Arabic": "نموذج ثلاثي الكلمات",
    "Chinese": "三元模型",
    "French": "modèle trigramme",
    "Japanese": "三連モデル",
    "Russian": "триграммная модель"
  },
  {
    "English": "trilinear interpolation",
    "context": "1: The implicit function f voxel (x, z) = ζ(V (z), x) is then evaluated by sampling V (z) at the corresponding world coordinate x, with a grid-sampling function ζ : R R 3 × R 3 → R dim(f ) , such as <mark>trilinear interpolation</mark>.<br>",
    "Arabic": "التداخل الثلاثي الأبعاد",
    "Chinese": "三线性插值",
    "French": "interpolation trilinéaire",
    "Japanese": "三重線形補間",
    "Russian": "трилинейная интерполяция"
  },
  {
    "English": "trimap",
    "context": "1: It is apparent that given a sufficiently precise <mark>trimap</mark>, our method offers no real advantage (when given the same <mark>trimap</mark> as input) over the least-squares matting of Levin et al., which produced the most numerically accurate mattes. However, when simulating the best labeling of components, our approach produced the most accurate mattes, on average.<br>2: [33] introduced a matting dataset and used a deep network with a <mark>trimap</mark> input to predict the alpha matte. Many recent approaches rely on this dataset to learn matting, e.g., Context-Aware Matting [13], Index Matting [21], sampling-based matting [31] and opacity propagationbased matting [18].<br>",
    "Arabic": "ترايماب",
    "Chinese": "三元图",
    "French": "carte à trois zones",
    "Japanese": "トリマップ",
    "Russian": "тримап"
  },
  {
    "English": "triple",
    "context": "1: LAMA-UHN is a subset of <mark>triples</mark> that are hard to guess introduced by Poerner et al. (2020).<br>2: Supervision may be provided either by annotated logical forms (Wong and Mooney, 2007;Kwiatkowski et al., 2010;Andreas et al., 2013) or from (world, question, answer) <mark>triples</mark> alone (Liang et al., 2011;Pasupat and Liang, 2015).<br>",
    "Arabic": "ثلاثيات",
    "Chinese": "三元组",
    "French": "triple",
    "Japanese": "三つ組",
    "Russian": "тройки"
  },
  {
    "English": "triplet",
    "context": "1: Additionally, encoding both the head event and the tail event with the pretrained language model successfully takes advantage of semantic information. Problem Formulation Given a CSKG G = ( N , V ) , where N is the set of nodes and V is the set of edges , we consider a single training instance as a <mark>triplet</mark> v i = ( h , r , t ) with the head event h , relation type r and the tail event t. Here , r<br>",
    "Arabic": "ثلاثيات",
    "Chinese": "三元组",
    "French": "triplet",
    "Japanese": "三つ組",
    "Russian": "триплет"
  },
  {
    "English": "triplet loss",
    "context": "1: For the downstream application task for our experiments, we follow Wei et al. (2021) to conduct the task of few-shot, highly multi-class text classification (Gupta et al., 2014;Kumar et al., 2019), which typically has a large number of classes with only a few samples for each of the class. We use <mark>triplet loss</mark> , a loss computed with three elements , namely , an anchor a , a positive sample p , and a negative sample n. It origins from the vision community ( Schroff et al. , 2015 ) , which was later applied to language tasks ( Ein Dor et al. , 2018 ; Lauriola and Moschitti , 2020 ) ,<br>2: Next, we adopt <mark>triplet loss</mark> with a hard margin for training the merger layers. Here, negative entities e − are explicitly sampled for each input rephrase pair (q i , e + i ) to form triplets (q i , e + i , e − i ). The loss function to minimize is: \n<br>",
    "Arabic": "خسارة الثلاثي",
    "Chinese": "三元组损失",
    "French": "perte de triplet",
    "Japanese": "トリプレット損失",
    "Russian": "потеря триады"
  },
  {
    "English": "true positive rate",
    "context": "1: For these reasons, it is hard to compare accuracy and F1 score results across models and datasets with different proportions of bots and humans. To provide additional clarity and comparability, we report the balanced accuracies (bal. acc.) of our classifiers, or the arithmetic mean of the <mark>true positive rate</mark> and the true negative rate.<br>2: Whereas we parameterized demographic parity solutions in terms of the acceptance rate β in equation ( 17), we will parameterize equation ( 18) in terms of the <mark>true positive rate</mark> (TPR), t := w A • π A , τ A . Thus, (18) becomes \n<br>",
    "Arabic": "معدل الإيجابيات الصحيحة",
    "Chinese": "真正正例率",
    "French": "taux de vrais positifs",
    "Japanese": "真陽性率",
    "Russian": "коэффициент истинно-положительных случаев"
  },
  {
    "English": "truth assignment",
    "context": "1: The propositional Horn formula θ consists of the following conjuncts: Since θ contains no negative literals, it is clearly satisfiable and thus has a unique minimal model. Let V be the <mark>truth assignment</mark> that represents this minimal model.<br>",
    "Arabic": "تعيين القيم الصدقية",
    "Chinese": "真值赋值",
    "French": "attribution de vérité",
    "Japanese": "真理値割り当て",
    "Russian": "присвоение значений истинности"
  },
  {
    "English": "tuple",
    "context": "1: Otherwise, if R 1 is a base relation, it picks the <mark>tuple</mark> with probability 1 |R 1 | . The value of t ∈R (Sc(t)) for each <mark>tuple</mark> set R is computed at the beginning of the query processing and the value of |R| for each base relation is calculated in a preprocessing step.<br>2: We consider a training dataset D = {(x i , Y i )} n i=1 , where each <mark>tuple</mark> comprises of an image x i ∈ X and a candidate label set Y i ⊂ Y.<br>",
    "Arabic": "حُزْمَة",
    "Chinese": "元组",
    "French": "tuple",
    "Japanese": "タプル",
    "Russian": "кортеж"
  },
  {
    "English": "tuplex",
    "context": "1: i , y i+1 ) ∈ q such that O |= func ( R ) . For a <mark>tuplex</mark> of variables, we usex + to denote the <mark>tuplex</mark>ȳ whereȳ consists of all variables y, in the fixed order, that are reachable from a variable inx on a functional path in q and that are not part ofx.<br>",
    "Arabic": "تبلكس",
    "Chinese": "元组",
    "French": "n-uplet",
    "Japanese": "タプレックス",
    "Russian": "туплекс"
  },
  {
    "English": "ture machine",
    "context": "1: Rules ( 19)-( 21) initialise the state of the M at time i = 0. Rule ( 22 ) derives Halts if at any point the <mark>Turing machine</mark> enters the halting state h. The remaining rules encode the evolution of the state of M , and they are based on the following idea : if variable x encodes a time point i using value 2 i , then variable y encodes a position j for time point i if x ≤ y < x + x holds ; moreover , for such y , position j at time point i + 1 is encoded as 2 i+1 + j = 2 i + 2 i + j and can be obtained as x+y , and the encodings of positions j −1 and j +1 can be obtained as x + y − 1<br>2: Definition 6 (Counting Turing Machines) A counting <mark>Turing machine</mark> is a standard nondeterministic <mark>Turing machine</mark> with an auxiliary output device that (magically) prints in binary notation on a special tape the number of accepting computations induced by the input.<br>",
    "Arabic": "آلة تورنج",
    "Chinese": "图灵机",
    "French": "machine de Turing",
    "Japanese": "チューリングマシン",
    "Russian": "машина Тьюринга"
  },
  {
    "English": "turing reduction",
    "context": "1: A ≤ p B a polynomial-time (Turing) reduction from A to B.<br>2: A problem is said to be #P-hard if all problems in #P reduce to it. Note that the notion of reduction used here is polynomial time <mark>Turing reduction</mark> (or simply <mark>Turing reduction</mark>; see [8] and [22] for more details).<br>",
    "Arabic": "اختزال تيورنج",
    "Chinese": "图灵归约",
    "French": "réduction de Turing",
    "Japanese": "チューリング還元",
    "Russian": "туринговское сведение"
  },
  {
    "English": "turing test",
    "context": "1: The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), proposed as an alternative to the <mark>Turing Test</mark> (Turing 1950), has been used as a benchmark for evaluating commonsense reasoning.<br>2: Spot The Bot is reminiscent of the <mark>Turing Test</mark> (Turing, 1950), as the dialogue systems are evaluated based on their ability to mimic human behavior. The Turing test served as a useful mental model for understanding what machine intelligence might mean. However, it has also been criticized as a way to identify intelligence in NLP systems.<br>",
    "Arabic": "اختبار تورينج",
    "Chinese": "图灵测试",
    "French": "test de Turing",
    "Japanese": "チューリングテスト",
    "Russian": "тест Тьюринга"
  },
  {
    "English": "two-class classification",
    "context": "1: Consider the <mark>two-class classification</mark> problem in Figure 1 where the two groups are drawn from Gaussians and the optimal classification boundary is given along x 2 = 0. Assume that the sampling distribution evolves according to definition 1 with ν(x) = 1.0−x, equal to the zero one loss, and \n<br>2: Formally , for <mark>two-class classification</mark> , suppose that we have a training set D of N observations , D = { ( x i , y i ) } N i=1 , where the i-th input point x i ∈ R D and its corresponding output y i is binary , with y = 1 i for one class and y i = −1<br>",
    "Arabic": "التصنيف الثنائي",
    "Chinese": "两类分类",
    "French": "classification à deux classes",
    "Japanese": "二値分類",
    "Russian": "бинарная классификация"
  },
  {
    "English": "two-player zero-sum game",
    "context": "1: This paper focuses on <mark>two-player zero-sum game</mark>s. In a two-player zero-sum extensive-form game there are two players, P = {1, 2}. H is the set of all possible nodes, represented as a sequence of actions.<br>",
    "Arabic": "لعبة لاعبين محصلتها صفر",
    "Chinese": "二人零和博弈",
    "French": "jeu à somme nulle à deux joueurs",
    "Japanese": "二人零和ゲーム",
    "Russian": "игра для двух игроков с нулевой суммой"
  },
  {
    "English": "type embedding",
    "context": "1: (2019); namely, we combine the word embeddings, absolute position embeddings of the tokens and <mark>type embeddings</mark> of the sentences. Because of the positional information encoded by the Fourier Transform in Equation ( 1) (see n, k indices), FNet performs just as well without position embeddings.<br>",
    "Arabic": "تضمين النوع",
    "Chinese": "类型嵌入",
    "French": "Plongement typologique",
    "Japanese": "型埋め込み",
    "Russian": "типовое вложение"
  },
  {
    "English": "u-statistic",
    "context": "1: Sample-Efficient Optimal Loss Estimation: Dicker (2014); Kong & Valiant (2018) propose <mark>U-statistics</mark>-based estimators that estimate the optimal population mean square error in d-dimensional linear regression, with a sample complexity of O( √ d) (much lower than O(d), the sample complexity of learning optimal linear regressor).<br>",
    "Arabic": "إحصائية-يو",
    "Chinese": "U-统计量",
    "French": "U-statistique",
    "Japanese": "U統計量",
    "Russian": "U-статистика"
  },
  {
    "English": "unary atom",
    "context": "1: For every sub-formula F , we define a corresponding lambda form that can be derived by replacing every Skolem constant n i that does not appear in any <mark>unary atom</mark> in F with a unique lambda variable x i .<br>2: At learning time, USP maintains an agenda that contains operations that have been evaluated and are pending execution. During initialization, USP forms a part and creates a new cluster for each <mark>unary atom</mark> u(n).<br>",
    "Arabic": "ذرة أحادية",
    "Chinese": "一元原子",
    "French": "atome unaire",
    "Japanese": "単項アトム",
    "Russian": "унарный атом"
  },
  {
    "English": "unary constraint",
    "context": "1: The power and generality of this combination of techniques allow us to efficiently optimize a more general class of energy functions than previous batch techniques. This class includes robust error functions, which are critical to obtaining good results in the presence of noisy binary and <mark>unary constraints</mark>.<br>2: Our approach represents a set of images as a graph modeling geometric constraints between pairs of cameras or between cameras and scene points (as binary constraints), as well as single-camera pose information such as geotags (as <mark>unary constraints</mark>).<br>",
    "Arabic": "قيد أحادي",
    "Chinese": "一元约束",
    "French": "contrainte unaire",
    "Japanese": "単項制約",
    "Russian": "унарное ограничение"
  },
  {
    "English": "unary feature",
    "context": "1: We thus make a tradeoff, moving to more powerful discriminative <mark>unary features</mark> but sacrificing tractable pairwise potentials. Alternatively, (Galleguillos et al. 2008;Kumar and Hebert 2005) group pixels into object-sized segments and then define a CRF over the labels of the segments.<br>",
    "Arabic": "ميزة أحادية",
    "Chinese": "单元特征",
    "French": "caractéristique unaire",
    "Japanese": "単一特徴量",
    "Russian": "унарная характеристика"
  },
  {
    "English": "unary potential",
    "context": "1: The term θ 1 a;f (a) is called a <mark>unary potential</mark> since its value depends on the labelling of one random variable at a time. Similarly, θ 2 ab;f (a)f (b) is called a pairwise potential as it depends on a pair of random variables. For simplicity , we assume that θ 2 ab ; f ( a ) f ( b ) = w ( a , b ) d ( f ( a ) , f ( b ) ) where w ( a , b ) is the weight that indicates the strength of the pairwise relationship between variables v a and v b , with<br>2: Additionally, at each stage, we employ a single <mark>unary potential</mark> φ J+1 (x (c) , y), where C J+1 is simply the set of all individual pixels. We extend these previous RTF-based approaches to our setting by (a) incorporating the blur likelihood for non-blind image deblurring into the prediction as outlined in Eqs.<br>",
    "Arabic": "المحتمل الأحادي",
    "Chinese": "一元势能",
    "French": "potentiel unaire",
    "Japanese": "単項ポテンシャル",
    "Russian": "унарный потенциал"
  },
  {
    "English": "unary predicate",
    "context": "1: The PDDL encoding has a <mark>unary predicate</mark> tightened and a binary predicate at, representing the status of nuts and the position of agent, spanners and nuts within the corridor, respectively. The PDDL types of the original encoding can be compiled into <mark>unary predicate</mark>s.<br>2: Turning to languages with connectedness, we define Bc and Cc to be the extensions of B and C with the <mark>unary predicate</mark> c. We set I |= c(τ ) iff τ I is connected in the topological space under consideration.<br>",
    "Arabic": "مِصْدَاق أُحَادِي",
    "Chinese": "一元谓词",
    "French": "prédicat unaire",
    "Japanese": "一項述語 (unary predicate)",
    "Russian": "унарный предикат"
  },
  {
    "English": "unary production",
    "context": "1: The treebank data is right-binarized (Matsuzaki et al., 2005) to construct grammars with only unary and binary productions. We replace lexical words with count ≤ 5 in the training data with one of 50 unknown words using lexical features, following (Petrov et al., 2006).<br>",
    "Arabic": "إنتاج أحادي",
    "Chinese": "一元产生式",
    "French": "production unaire",
    "Japanese": "単項生成規則",
    "Russian": "унарное правило"
  },
  {
    "English": "unbiased estimate",
    "context": "1: Recently, several follow-up works have realized this deficiency of gradient exploration in DBGD, and propose various types of solutions to improve its learning efficiency. One type of studies explore multiple random directions in each iteration of model update. Unbiased estimate of gradient is maintained in this type of revisions of DBGD, as the directions are still uniformly sampled. Model estimation variance is expected to be reduced by testing more exploratory directions ; but , in practice , as the users would only examine a finite number of documents under each query ( e.g. , due to position bias [ 9 ] ) , the sensitivity of interleaved test drops as a result of more exploratory rankers having to be tested at once<br>2: By defining a smoothed version of f t asf t (w) = E u ∈B [f t (w + δu)], we have: Theorem 3.1. The projected gradient д t in DBGD-DSP is an <mark>unbiased estimate</mark> of true gradient, i.e., \n<br>",
    "Arabic": "تقدير غير متحيز",
    "Chinese": "无偏估计",
    "French": "estimation non biaisée",
    "Japanese": "不偏推定",
    "Russian": "несмещенная оценка"
  },
  {
    "English": "unbiased estimator",
    "context": "1: This is even worse for personalized MCs as a triple (u, l, i) does not contribute to the estimate of (u ′ , l, i). In addition, the important properties of MLE (e.g. Gaussian distribution, <mark>unbiased estimator</mark>, minimal variance under all <mark>unbiased estimator</mark>s) only exist in asymptotic theory.<br>2: Oracle class (σ 2 ). We assume each worker interacts with its local function f i only via a stochastic gradient oracleg i , and that when we query this oracle with model x, it returns an independent <mark>unbiased estimator</mark> to ∇f i (x) based on some random variable z with distribution Z (e.g.<br>",
    "Arabic": "مقدر غير متحيز",
    "Chinese": "无偏估计量",
    "French": "estimateur non biaisé",
    "Japanese": "偏りのない推定量",
    "Russian": "несмещенная оценка"
  },
  {
    "English": "uncertainty",
    "context": "1: We next conduct experiments which consider the annotation cost in the image selection phase. At first, the annotation costs are assumed to be known prior to labeling. Annotation cost and <mark>uncertainty</mark> are combined using Eq.<br>2: While these straightforward solutions have shown notable effectiveness and computational efficiency, these deterministic methods impose limitations on handling ill-posed uncertain cases such as self-occlusions and hand-object occlusions, which are prevalent in real-world hand recognition scenarios. Therefore, in order to ensure the reliability of the estimation, it is imperative to accurately model the <mark>uncertainty</mark>.<br>",
    "Arabic": "عدم اليقين",
    "Chinese": "不确定性",
    "French": "incertitude",
    "Japanese": "不確実性",
    "Russian": "неопределенность"
  },
  {
    "English": "uncertainty measure",
    "context": "1: Samples, that could be labeled, are selected from the unlabeled samples remaining in the current chunk using classifiers in an ensemble and the new classifier; (4) The minimal variance (MV) method only uses the ensemble variance as an <mark>uncertainty measure</mark> in the global uncertainty sampling (GU) method.<br>",
    "Arabic": "مقياس عدم اليقين",
    "Chinese": "不确定性度量",
    "French": "mesure d'incertitude",
    "Japanese": "不確実性尺度",
    "Russian": "неопределенность"
  },
  {
    "English": "uncertainty modeling",
    "context": "1: To address this, P3 [82] proposes non-parametric distribution of future semantic occupancy and FIERY [35] devises the first paradigm for multi-view cameras. A few methods improve the performance of FIERY with more sophisticated <mark>uncertainty modeling</mark> [1,38,105].<br>",
    "Arabic": "نمذجة عدم اليقين",
    "Chinese": "不确定性建模",
    "French": "modélisation de l'incertitude",
    "Japanese": "不確実性モデリング",
    "Russian": "моделирование неопределенности"
  },
  {
    "English": "uncertainty sampling",
    "context": "1: We choose maximum entropy to represent the <mark>uncertainty sampling</mark>, since it is usually on par with more elaborated counterparts (Tsvigun et al., 2022). As a popular diversity sampling baseline to compare against, we pick select CoreSet (Sener and Savarese, 2018).<br>2: (2002) addressed a shallow parser trained on a semantically annotated corpus. They used an <mark>uncertainty sampling</mark> protocol, where in each iteration the sentences of the unlabelled pool are clustered using a distance measure defined on parse trees to a predefined number of clusters.<br>",
    "Arabic": "اختيار العينات القائم على عدم اليقين",
    "Chinese": "不确定性采样",
    "French": "échantillonnage d'incertitude",
    "Japanese": "不確実性サンプリング",
    "Russian": "сэмплирование неопределенности"
  },
  {
    "English": "undirected graph",
    "context": "1: Let G = (V, E) be an undirected and unweighted graph consisting of a set V of vertices and a set E of edges between them. We will denote by n the number of vertices and by m the number of edges.<br>2: We first introduce the following class to ease the notations. Definition 2. For any simple, <mark>undirected graph</mark> G = (V, E), if U ⊂ V , then we set \n<br>",
    "Arabic": "رسم بياني غير موجه",
    "Chinese": "无向图",
    "French": "graphe non orienté",
    "Japanese": "無向グラフ",
    "Russian": "ненаправленный граф"
  },
  {
    "English": "undirected graphical model",
    "context": "1: Full exchangeability is best understood in the context of a finite sequence of binary random variables such as a number of coin tosses. Here, exchangeability means that it is only the number of heads that matters and not their particular order. Figure 1 depicts an <mark>undirected graphical model</mark> with 9 finitely exchangeable dependent Bernoulli variables.<br>2: (2006), Exchange Sampling (Murray et al., 2006), or modeling of the latent history of the generative procedure (Beskos et al., 2006). In general, these methods were developed with the <mark>undirected graphical model</mark> in mind, where it is sometimes possible to generate exact data via Coupling From The Past.<br>",
    "Arabic": "نموذج رسومي غير موجه",
    "Chinese": "无向图模型",
    "French": "modèle graphique non dirigé",
    "Japanese": "無向グラフィカルモデル",
    "Russian": "ненаправленная графическая модель"
  },
  {
    "English": "uniform convergence",
    "context": "1: The theoretical foundations of empirical risk minimization are solid [48,2,10,11]. When the expectation of the excess loss bounds its variance, it is possible to achieve faster rates than the O(1/ √ n) \n offered by standard <mark>uniform convergence</mark> arguments [49,50,4,25,11] (see Boucheron et al.<br>2: Observe that when D ∞ (D ′ X ||D X ) < γ, D ′ X can be written as a mixture over D X with probability at least 1 γ and some other distribution D X with probability at most 1 − 1 γ . Once again invoking <mark>uniform convergence</mark>, we observe that sampling Θ D \n<br>",
    "Arabic": "تقارب موحد",
    "Chinese": "一致收敛",
    "French": "convergence uniforme",
    "Japanese": "一様収束",
    "Russian": "равномерная сходимость"
  },
  {
    "English": "uniform distribution",
    "context": "1: As a consequence, the ranks of the scores are equally likely and thus follow a <mark>uniform distribution</mark> which allow to calibrate a threshold on the rank statistics leading to a valid confidence set.<br>2: We initialize all word vectors by randomly sampling each value from a <mark>uniform distribution</mark>: U(−r, r), where r = 0.0001. All the word vectors are stacked in the word embedding matrix L ∈ R d×|V | , where |V | is the size of the vocabulary.<br>",
    "Arabic": "توزيع موحد",
    "Chinese": "均匀分布",
    "French": "distribution uniforme",
    "Japanese": "一様分布",
    "Russian": "равномерное распределение"
  },
  {
    "English": "uniform information density hypothesis",
    "context": "1: The decision to label each token with a single symbol is partially rooted in prior research providing evidence that syntactic decisions among human speakers adhere to the <mark>uniform information density hypothesis</mark>, thus each token may convey similar amounts of syntactic information (Levy and Jaeger, 2006).<br>2: The theoretical crux of this paper hinges on a proposed relationship between beam search and the <mark>uniform information density hypothesis</mark> (Levy, 2005;Levy and Jaeger, 2007), a concept from cognitive science: Hypothesis 4.1. \"Within the bounds defined by grammar, speakers prefer utterances that distribute information uniformly across the signal (information density).<br>",
    "Arabic": "فرضية كثافة المعلومات الموحدة",
    "Chinese": "均匀信息密度假说",
    "French": "hypothèse de densité d'information uniforme",
    "Japanese": "一様情報密度仮説",
    "Russian": "гипотеза равномерной информационной плотности"
  },
  {
    "English": "uniform sampling",
    "context": "1: We add each point q ∈ Y i,j to S i,j with weight 1. Furthermore, we sample s points uniformly at random (with replacement) X i,j \\ Y i,j , and add to the set S i,j with weight equal to \n |X i,j \\Y ij | s \n<br>2: We would like to note though that while the last experiment underscores the potential for additional improvement in the convergence rate, the rest of the experiments reported in the paper were conducted in accordance with the formal analysis using <mark>uniform sampling</mark> with replacements.<br>",
    "Arabic": "أخذ عينات موحدة",
    "Chinese": "均匀抽样",
    "French": "échantillonnage uniforme",
    "Japanese": "一様サンプリング",
    "Russian": "равномерная выборка"
  },
  {
    "English": "unigram count",
    "context": "1: Here, we have one Gamma-Dirichlet-Multinomial model to model <mark>unigram counts</mark> u, and a separate Dirichlet-Multinomial model for each u (the first word of a bigram) that b (the second word of a bigram) conditions on, sharing a common Gamma prior that ties all bigram models.<br>2: (4). The computational performance of the multiplicative update in Eq. ( 4) is dominated by counting bigrams (perexample normalized as in the numerator of the multiplicative factor), while the global normalizing <mark>unigram counts</mark> (the denominator of the multiplicative factor) are pre-computed in feature vector generation.<br>",
    "Arabic": "عدد المفردات",
    "Chinese": "一元计数",
    "French": "comptage unigramme",
    "Japanese": "単語出現回数",
    "Russian": "подсчет униграмм"
  },
  {
    "English": "unigram distribution",
    "context": "1: For productions representation, this is the <mark>unigram distribution</mark> of productions from the sentences in h k . For d-sequences, the distribution is computed for bigrams of syntactic words. These language models use Lidstone smoothing with constant δ E .<br>2: θ r is a <mark>unigram distribution</mark> of words that are semantically licensed for property r. f r is a \"fertility\" distribution over the integers that characterizes entity list lengths.<br>",
    "Arabic": "توزيع أحادي الكلمات",
    "Chinese": "一元分布",
    "French": "distribution unigramme",
    "Japanese": "ユニグラム分布",
    "Russian": "распределение униграмм"
  },
  {
    "English": "unigram language model",
    "context": "1: . Common to most of this work is the idea of using a multinomial word distribution (also called a <mark>unigram language model</mark>) to model a topic in text. For example, the multinomial distribution shown on the left side of Table 1 is a topic model extracted from a collection of abstracts of database literature.<br>2: Query-log features use evidence from the queries previously issued to the vertical, which reflect the topics in the vertical that are of interest to users. For each vertical, we compute the query likelihood given by a <mark>unigram language model</mark> constructed from the vertical's query-log. Our querylog features (one per vertical) are defined by, \n<br>",
    "Arabic": "نموذج لغة أحادي الكلمة",
    "Chinese": "一元语言模型",
    "French": "modèle de langue unigramme",
    "Japanese": "単語言語モデル",
    "Russian": "униграммная языковая модель"
  },
  {
    "English": "unigram model",
    "context": "1: Taking development and test sets into account, the best Chinese-English translation system results from our <mark>unigram model</mark>. It is significantly better than other systems on the development set and performs almost equally well with the IWSLT segmentation on the test set. Note that the segmentation distributed by IWSLT is a manual segmentation for the translation task.<br>2: The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model 2 is a <mark>unigram model</mark>.<br>",
    "Arabic": "نموذج أحادي الكلمة",
    "Chinese": "一元模型",
    "French": "modèle unigramme",
    "Japanese": "単語モデル",
    "Russian": "униграммная модель"
  },
  {
    "English": "union bind",
    "context": "1: X ) \n . Here the first inequality follows from the fact that probabilities never exceed 1 and a <mark>union bound</mark>. The second inequality follows from Lemma 11, part (i), since EF k ≤ mKT S 1 (X).<br>2: By <mark>union bound</mark>, with nonzero probability, the above two condition hold simultaneously, showing the feasibility of the optimization problem. We then argue that for all n, V n+1 ⊆ B(h * , 16 ln |H| mn ).<br>",
    "Arabic": "الربط الموحّد",
    "Chinese": "联合边界",
    "French": "liaison d'union",
    "Japanese": "和束縛",
    "Russian": "связывание объединения"
  },
  {
    "English": "union of conjunctive query",
    "context": "1: A conjunctive query (CQ) is a formula Q[x] = ∃y.ψ [x, y] where ψ[x, y] is a conjunction of atoms; a <mark>union of conjunctive queries</mark> (UCQ) is a disjunction of such formulae.<br>",
    "Arabic": "اتحاد استعلام تلازمي",
    "Chinese": "连接查询的并集",
    "French": "union de requêtes conjonctives",
    "Japanese": "結合条件付き問合せの和",
    "Russian": "объединение конъюнктивных запросов"
  },
  {
    "English": "unit propagation",
    "context": "1: Although pure literal elimination helps to refute Eq n , it turns out that pure literal elimination can also be disadvantageous. It might be a fallacy to think that pure existential literals should be satisfied in the same way as unit clauses in <mark>unit propagation</mark>.<br>2: As the solver works on the problem, it produces a file crystal_maze.veripb that provides a proof that it has found all non-symmetric solutions. The proof log will contain reverse <mark>unit propagation</mark> (RUP) clauses that justify backtracking, as well as cutting planes derivations that prove the correctness of propagations by the all-different and table constraints.<br>",
    "Arabic": "انتشار الوحدة",
    "Chinese": "单位传播",
    "French": "propagation unitaire",
    "Japanese": "単位伝播",
    "Russian": "распространение единичного присваивания"
  },
  {
    "English": "unit sphere",
    "context": "1: For rotations, we parameterize the <mark>unit sphere</mark> into a 3D grid with 10 cells in each dimension, for a total of L = 1000 labels for each camera.<br>2: We generated a synthetic environment of tasks as follows. We choose a d×K matrix D by sampling its columns independently from the uniform distribution on the <mark>unit sphere</mark> in R d . Once D is created, a generic task in the environment is given by w = Dγ, where γ is an s-sparse vector obtained as follows.<br>",
    "Arabic": "كرة وحدة",
    "Chinese": "单位球体",
    "French": "sphère unitaire",
    "Japanese": "単位球",
    "Russian": "единичная сфера"
  },
  {
    "English": "universal approximation",
    "context": "1: Therefore, we recover the same guarantees as Theorem I.2 (note that M is not explicitly controlled using network properties in our work, but we could use <mark>universal approximation</mark> properties as in Rozen et al. (2021) in order to obtain a similar result).<br>",
    "Arabic": "تقريب عالمي",
    "Chinese": "通用逼近",
    "French": "approximation universelle",
    "Japanese": "普遍的な近似",
    "Russian": "универсальная аппроксимация"
  },
  {
    "English": "universal approximator",
    "context": "1: A very recent trend has been the modeling of shapes as a learnable indicator function [5,23,29], rather than a sampling of it, as in the case of voxel methods. The resulting networks treat reconstruction as a classification problem, and are <mark>universal approximators</mark> [21] whose reconstruction precision is proportional to the network complexity.<br>",
    "Arabic": "المُقرب العالمي",
    "Chinese": "通用逼近器",
    "French": "approximateur universel",
    "Japanese": "万能近似関数",
    "Russian": "универсальный аппроксиматор"
  },
  {
    "English": "universal model",
    "context": "1: N ′ = {A | A(w) ∈ J }. Since h n is a homomorphism, N ⊆ N ′ . By construction of the <mark>universal model</mark> U D1,O we have M = {A | A(wρM ) ∈ I}.<br>2: An analysis of well-known benchmarks for ontology-mediated querying suggests that the resulting class CQ csf of CQs is sufficiently general to include many relevant CQs that occur in practical applications. Our proofs crucially rely on the use of a finite version of the <mark>universal model</mark> that is specifically tailored to the class CQ csf .<br>",
    "Arabic": "النموذج العالمي",
    "Chinese": "通用模型",
    "French": "modèle universel",
    "Japanese": "汎用モデル",
    "Russian": "универсальная модель"
  },
  {
    "English": "unlabeled datum",
    "context": "1: As the interaction histories A with h 0 and h 1 are identical, the <mark>unlabeled data</mark> part of the history are identical, formally, S A,h1 = S A,h0 .<br>2: languages in which our <mark>unlabeled data</mark> did not have at least 1 million types, we considered all types.<br>",
    "Arabic": "البيانات غير المعلمة",
    "Chinese": "未标注数据",
    "French": "données non étiquetées",
    "Japanese": "未ラベルのデータ",
    "Russian": "неразмеченные данные"
  },
  {
    "English": "unlexicalized grammar",
    "context": "1: The advantages of <mark>unlexicalized grammars</mark> are clear enough -easy to estimate, easy to parse with, and time-and space-efficient. However, the dismal performance of basic unannotated <mark>unlexicalized grammars</mark> has generally rendered those advantages irrelevant. Here, we have shown that, surprisingly, the maximum-likelihood estimate of a compact unlexicalized PCFG can parse on par with early lexicalized parsers.<br>",
    "Arabic": "قواعد نحوية غير معجمية",
    "Chinese": "无词汇语法",
    "French": "grammaire non lexicalisée",
    "Japanese": "非語彙化文法",
    "Russian": "нелексикализованная грамматика"
  },
  {
    "English": "unnormalized probability",
    "context": "1: As Example 1 shows, these tricks may not involve additive perturbation of the potential function φ(x); the Weibull tricks multiplicatively perturb exponentiated <mark>unnormalized probabilities</mark>p −α with Weibull noise.<br>",
    "Arabic": "احتمال غير معياري",
    "Chinese": "未归一化概率",
    "French": "probabilité non normalisée",
    "Japanese": "非正規化確率",
    "Russian": "ненормализованная вероятность"
  },
  {
    "English": "unsolvability",
    "context": "1: solvability . Finally, we present an experiment on real data showing that unsolvable graphs are appearing in practical situations.<br>",
    "Arabic": "غير قابلية الحل",
    "Chinese": "不可解性",
    "French": "impossibilité de résolution",
    "Japanese": "解決不可能性",
    "Russian": "неразрешимость"
  },
  {
    "English": "unsupervised algorithm",
    "context": "1: The lack of almost any identifiability result in nonlinear ICA has been a main bottleneck for the utility of the approach (Hyvarinen et al., 2018) and partially motivated alternative machine learning approaches (Desjardins et al., 2012;Schmidhuber, 1992;Cohen & Welling, 2015). Given that <mark>unsupervised algorithms</mark> did not initially perform well on realistic settings most of the other works have considered some more or less explicit form of supervision ( Reed et al. , 2014 ; Zhu et al. , 2014 ; Yang et al. , 2015 ; Kulkarni et al. , 2015 ; Cheung et al. , 2015 ; Mathieu et al. , 2016 ;<br>",
    "Arabic": "خوارزمية غير خاضعة للرقابة",
    "Chinese": "无监督算法",
    "French": "algorithme non supervisé",
    "Japanese": "教師なし学習アルゴリズム",
    "Russian": "неконтролируемый алгоритм"
  },
  {
    "English": "unsupervised approach",
    "context": "1: A prototype list, in contrast, is extremely compact. 3 Tasks and Related Work: Extraction Grenager et al. (2005) presents an <mark>unsupervised approach</mark> to an information extraction task, called CLASSIFIEDS here, which involves segmenting classified advertisements into topical sections (see figure 1(c)).<br>2: In the second phase, our goal was to annotate all images in our dataset with a reasonable number of human subject responses, and discover a hierarchy of these reasons via an <mark>unsupervised approach</mark>.<br>",
    "Arabic": "نهج غير موجّه",
    "Chinese": "无监督方法",
    "French": "approche non supervisée",
    "Japanese": "非監督アプローチ",
    "Russian": "неконтролируемый подход"
  },
  {
    "English": "unsupervised classification",
    "context": "1: By contrast, classification metrics in Xitsonga are better than in English, which is again consistent with prior findings of stronger <mark>unsupervised classification</mark> performance in Xitsonga (Shain and Elsner, 2019).<br>",
    "Arabic": "التصنيف غير المراقب",
    "Chinese": "无监督分类",
    "French": "classification non supervisée",
    "Japanese": "教師なし分類",
    "Russian": "несупервизированная классификация"
  },
  {
    "English": "unsupervised clustering",
    "context": "1: Unsupervised clustering criteria able to evaluate partitions with various numbers of features and built over feature subspaces of different cardinalities are required. Widely used <mark>unsupervised clustering</mark> criteria like Silhouette [13] and Davis-Bouldin [2] would favor feature subsets of lower cardinality and would direct the search towards the feature subset of minimum allowed cardinality.<br>2: Unsupervised clustering can be significantly improved using supervision in the form of pairwise constraints, i.e., pairs of instances labeled as belonging to same or different clusters. In recent years, a number of algorithms have been proposed for enhancing clustering quality by employing such supervision.<br>",
    "Arabic": "تجميع غير مُراقب",
    "Chinese": "无监督聚类",
    "French": "regroupement non supervisé",
    "Japanese": "教師なしクラスタリング",
    "Russian": "несупервизированная кластеризация"
  },
  {
    "English": "unsupervised datum",
    "context": "1: In the absence of supervised data or when no constraints are violated, distance learning attempts to minimize the objective function by adjusting the weights given the distortion between the <mark>unsupervised data</mark>points and their corresponding cluster representatives.<br>2: These models are often trained on large quantities of <mark>unsupervised data</mark>for example, GPT-2 (Radford et al. 2019) is trained on a dataset of 8 million unlabeled web pages. Although training data is typically collected with content diversity in consideration, other factors, such as ideological balance, are often ignored.<br>",
    "Arabic": "بيانات غير مُشرف عليها",
    "Chinese": "无监督数据",
    "French": "donnée non supervisée",
    "Japanese": "教師なしデータ",
    "Russian": "ненадзорные данные"
  },
  {
    "English": "unsupervised discovery",
    "context": "1: A fundamental problem in pattern recognition and data mining is the problem of automatically recognizing specific waveforms in time-series based on their shapes. Applications in the context of time-series data mining include exploratory data analysis of time-series, monitoring and diagnosis of critical systems, classification of time-series, and <mark>unsupervised discovery</mark> of recurrent patterns.<br>2: A third data mining task is <mark>unsupervised discovery</mark> of patterns in time-series; any data mining algorithm that tries to discover recurring (previously unknown) patterns in a data set will need to be able to solve this \"waveform matching\" problem as a primitive operation to support such <mark>unsupervised discovery</mark> (e.g., Das et al.<br>",
    "Arabic": "اكتشاف غير موجه",
    "Chinese": "无监督发现",
    "French": "découverte non supervisée",
    "Japanese": "無監視発見",
    "Russian": "неконтролируемое обнаружение"
  },
  {
    "English": "unsupervised disentanglement",
    "context": "1: We further find that <mark>unsupervised disentanglement</mark> is a property of our model    which emerges already at the very beginning of training (Fig. 6). Note how our model synthesizes individual objects before spending capacity on representing the background.<br>2: Since the (unsupervised) disentanglement method only has access to observations x, it hence cannot distinguish between the two equivalent generative models and thus has to be entangled to at least one of them.<br>",
    "Arabic": "فك الترابط غير الموجه",
    "Chinese": "无监督解缠",
    "French": "désenchevêtrement non supervisé",
    "Japanese": "教師なし解体",
    "Russian": "неконтролируемое распутывание"
  },
  {
    "English": "unsupervised domain adaptation",
    "context": "1: We discussed prior entity linking task definitions and compared them to our task in section 2. Here, we briefly overview related entity linking models and <mark>unsupervised domain adaptation</mark> methods. Entity linking models Entity linking given mention boundaries as input can be broken into the tasks of candidate generation and candidate ranking.<br>2: Unsupervised domain adaptation There is a large body of work on methods for <mark>unsupervised domain adaptation</mark>, where a labeled training set is available for a source domain and unlabeled data is available for the target domain.<br>",
    "Arabic": "التكيف غير المُشرَف للمجال",
    "Chinese": "无监督域适应",
    "French": "adaptation de domaine non supervisée",
    "Japanese": "教師なしドメイン適応",
    "Russian": "неконтролируемая адаптация домена"
  },
  {
    "English": "unsupervised feature learning",
    "context": "1: Our approach is nevertheless unique as the CAE's <mark>unsupervised feature learning</mark> capabilities are used simultaneously to provide a good initialization of deep network layers and a coherent non-local predictor of tangent spaces.<br>2: We have applied discriminative training of SPNs to image classification benchmarks. CIFAR-10 and STL-10 are standard datasets for deep networks and <mark>unsupervised feature learning</mark>. Both are 10-class small image datasets. We achieve the best results to date on both tasks. We follow the feature extraction pipeline of Coates et al.<br>",
    "Arabic": "تعلم الميزات غير المراقبة",
    "Chinese": "无监督特征学习",
    "French": "apprentissage non supervisé de caractéristiques",
    "Japanese": "教師なし特徴学習",
    "Russian": "неконтролируемое обучение признакам"
  },
  {
    "English": "unsupervised image segmentation",
    "context": "1: In order to improve segmentation and labeling accuracy, researchers have expanded the basic CRF framework to incorporate hierarchical connectivity and higher-order potentials defined on image regions [8,12,9,13]. However, the accuracy of these approaches is necessarily restricted by the accuracy of <mark>unsupervised image segmentation</mark>, which is used to compute the regions on which the model operates.<br>",
    "Arabic": "تجزئة الصورة غير الخاضعة للرقابة",
    "Chinese": "无监督图像分割",
    "French": "segmentation d'image non supervisée",
    "Japanese": "教師なし画像セグメンテーション",
    "Russian": "несупервизированная сегментация изображения"
  },
  {
    "English": "unsupervised learning",
    "context": "1: In <mark>unsupervised learning</mark>, model behavior is largely determined by the structure of the model. Designing models to exhibit a certain target behavior requires another, rare kind of expertise and effort. Unsupervised learning, while minimizing the usage of labeled data, does not necessarily minimize total effort.<br>2: Unsupervised learning algorithms such as principal components analysis and vector quantization can be understood as factorizing a data matrix subject to different constraints. Depending upon the constraints utilized, the resulting factors can be shown to have very different representational properties.<br>",
    "Arabic": "التعلم غير الموجه",
    "Chinese": "无监督学习",
    "French": "apprentissage non supervisé",
    "Japanese": "教師なし学習",
    "Russian": "ненадзорное обучение"
  },
  {
    "English": "unsupervised method",
    "context": "1: We show that our approach substantially outperforms purely <mark>unsupervised methods</mark> that do not provide the learner with any task-specific guidance about how hierarchies should be deployed, and further that the specific use of sketches to parameterize modular subpolicies makes better use of sketches than conditioning on them directly.<br>2: Instead, our proposed approach enables domain experts to trade time-consuming annotation work for task programming with representation learning. Another group of work uses <mark>unsupervised methods</mark> to discover new motifs and behaviors [22,41,2,26,5]. Our work focuses on the more common case where domain experts already know what types of actions they would like to study in an experiment.<br>",
    "Arabic": "طريقة غير خاضعة للرقابة",
    "Chinese": "无监督方法",
    "French": "méthode non supervisée",
    "Japanese": "教師なし手法",
    "Russian": "метод без учителя"
  },
  {
    "English": "unsupervised model",
    "context": "1: Ensure: Θ f , a set of parameters learned using a constrained <mark>unsupervised model</mark> ( §5). 1: \n D e↔f ← word-align-bitext ( D e , D f ) 2 : D e ← pos-tag-supervised ( D e ) 3 : A ← extract-alignments ( D e↔f , D e ) 4 : G ← construct-graph ( Γ f , D f , A ) 5 : G ← graph-propagate ( G ) 6 : ∆ ← extract-word-constraints ( G ) 7<br>2: In contrast, a status-based network theory would penalize non-transitive triads such as β >>< . Thus, in an <mark>unsupervised model</mark>, we can examine the weights to learn about the semantics of the induced edge types, and to see which theory best describes the signed network configurations that follow from the linguistic signal.<br>",
    "Arabic": "النموذج غير المراقب",
    "Chinese": "无监督模型",
    "French": "modèle non supervisé",
    "Japanese": "教師なしモデル",
    "Russian": "ненадзорная модель"
  },
  {
    "English": "unsupervised morphological segmentation",
    "context": "1: Non-overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for <mark>unsupervised morphological segmentation</mark> (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007).<br>2: However, most existing model-based systems for <mark>unsupervised morphological segmentation</mark> use directed generative models, making it difficult to leverage arbitrary overlapping features that are potentially helpful to learning. In this paper, we present the first log-linear model for <mark>unsupervised morphological segmentation</mark>.<br>",
    "Arabic": "\"التجزئة المورفولوجية غير الخاضعة للرقابة\"",
    "Chinese": "无监督形态分割",
    "French": "segmentation morphologique non supervisée",
    "Japanese": "非監督形態的分割",
    "Russian": "несупервизорная морфологическая сегментация"
  },
  {
    "English": "unsupervised parsing",
    "context": "1: Grammar Induction Grammar induction and <mark>unsupervised parsing</mark> has been a long-standing problem in computational linguistics (Carroll and Charniak, 1992).<br>2: This is borne out by the many recent studies on <mark>unsupervised parsing</mark> that include evaluations covering a number of languages (Cohen and Smith, 2009;Gillenwater et al., 2010;Naseem et al., 2010;Spitkovsky et al., 2011).<br>",
    "Arabic": "تحليل غير موجه",
    "Chinese": "无监督句法分析",
    "French": "analyse non supervisée",
    "Japanese": "無監督構文解析",
    "Russian": "ненадзорный синтаксический анализ"
  },
  {
    "English": "unsupervised pre-training",
    "context": "1: Unsupervised pre-training played a central role in the resurgence of deep learning.<br>2: Rives et al. (2019) similarly investigates whether the recent success of <mark>unsupervised pre-training</mark> in NLP applies to other domains, observing promising results on protein sequence data.<br>",
    "Arabic": "التدريب المسبق غير الخاضع للرقابة",
    "Chinese": "无监督预训练",
    "French": "pré-entraînement non supervisé",
    "Japanese": "無監視事前学習",
    "Russian": "неконтролируемое предварительное обучение"
  },
  {
    "English": "unsupervised representation",
    "context": "1: Evaluations of <mark>unsupervised representations</mark> often reuse supervised learning datasets which have thousands to millions of labeled examples. However, a representation which has robustly encoded a semantic concept should be exceedingly data efficient. As inspiration, we note that humans are able to reliably recognize even novel concepts with a single example (Carey and Bartlett 1978).<br>",
    "Arabic": "التمثيل غير المراقب",
    "Chinese": "无监督表征",
    "French": "représentation non supervisée",
    "Japanese": "教師なし表現",
    "Russian": "ненадзорное представление"
  },
  {
    "English": "unsupervised representation learning",
    "context": "1: Another category of methods for <mark>unsupervised representation learning</mark> are based on clustering [5,6,1,7]. They alternate between clustering the representations and learning to predict the cluster assignment. SwAV [7] incorporates clustering into a Siamese network, by computing the assignment from one view and predicting it from another view.<br>2: However, the analysis of these improved bounds in the context of interpatch mutual information optimization remains in order, and thus we focus on the original CPC In-foNCE loss to bias the learned representations towards slow features [Wiskott and Sejnowski, 2002]. Outside the information-theoretic framework, context prediction methods have been explored for <mark>unsupervised representation learning</mark>.<br>",
    "Arabic": "تعلم التمثيل غير الخاضع للرقابة",
    "Chinese": "无监督表示学习",
    "French": "apprentissage non supervisé de la représentation",
    "Japanese": "教師なし表現学習",
    "Russian": "ненадзорное обучение представлений"
  },
  {
    "English": "unsupervised segmentation",
    "context": "1: Combining three of these components (framed in red) yields the foreground matte of the girl, shown in Figure 1c. In summary, our main contribution is the introduction of the concept of fundamental matting components and the resulting first unsupervised matting algorithm. Of course, just like <mark>unsupervised segmentation</mark>, unsupervised matting is an ill-posed problem.<br>2: However, if (for some odd reason) we wanted to perform <mark>unsupervised segmentation</mark> of words in one of our development languages, we would use these systems in the bottom two rows, though we have not demonstrated their performance on unseen test data in those languages (because that is not the goal of this paper).<br>",
    "Arabic": "التقطيع غير الخاضع للإشراف",
    "Chinese": "无监督分割",
    "French": "segmentation non supervisée",
    "Japanese": "無監視分割",
    "Russian": "несупервизированная сегментация"
  },
  {
    "English": "unsupervised system",
    "context": "1: Currently, the performance of even the most simple direct transfer systems far exceeds that of <mark>unsupervised systems</mark> (Cohen et al., 2011;Søgaard, 2011).<br>2: We show that the effect of our method also carries out to downstream tasks, but its effect is larger in <mark>unsupervised systems</mark> directly using embedding similarities than in supervised systems using embeddings as input features, as the latter have enough expressive power to learn the optimal transformation themselves.<br>",
    "Arabic": "نظام غير خاضع للرقابة",
    "Chinese": "无监督系统",
    "French": "système non supervisé",
    "Japanese": "- Term: 教師なしシステム",
    "Russian": "несупервизированная система"
  },
  {
    "English": "unsupervised word clustering",
    "context": "1: Another positive property of this approach is that it tries to reconcile the success of sequence-free distributional methods in <mark>unsupervised word clustering</mark> with the success of sequence models in supervised settings: the similarity guides the learning of the sequence model.<br>",
    "Arabic": "تجميع الكلمات بدون إشراف",
    "Chinese": "无监督词聚类",
    "French": "clustering de mots non supervisé",
    "Japanese": "教師なし単語クラスタリング",
    "Russian": "несупервизорный кластеринг слов"
  },
  {
    "English": "unweighted graph",
    "context": "1: Throughout this section we let G = ( V , E ) denote an undirected , <mark>unweighted graph</mark> without self-loops with vertices V and edges E. We write N ( u ) to denote the neighbours of a vertex u ∈ V , i.e. , the set of vertices N ( u ) = { w | ( u , w ) ∈ E<br>",
    "Arabic": "رَسْم بَيَانيّ غَيْرُ مُوَزَّن",
    "Chinese": "无权图",
    "French": "graphe non pondéré",
    "Japanese": "無加重グラフ",
    "Russian": "невзвешенный граф"
  },
  {
    "English": "update function",
    "context": "1: In practice, expected clicks and views for all users are computed offline a priori (e.g., daily) and updated incrementally online, usually with some form of decay. The simple linear form of the predictor makes incremental scoring possible by maintaining the additivity of the <mark>update function</mark>.<br>2: The (optional) external input to the unrolled system at time t f \n The <mark>update function</mark> that evolves the unrolled system N \n The number of particles for ES and PES σ 2 \n<br>",
    "Arabic": "وظيفة التحديث",
    "Chinese": "更新函数",
    "French": "fonction de mise à jour",
    "Japanese": "更新関数",
    "Russian": "функция обновления"
  },
  {
    "English": "update rule",
    "context": "1: As a result most works optimise performance after K applications of the <mark>update rule</mark> and assume that this yields improved performance for the remainder of the learner's lifetime (Bengio et al., 1991;Maclaurin et al., 2015;Metz et al., 2019).<br>2: In RL, f is typically the (negative) expected value of a policy π x ; in supervised learning, f may be the expected negative loglikelihood under a probabilistic model π x . We provide precise formulations in Sections 5 and 6. The meta-learner 's problem is to learn an <mark>update rule</mark> ϕ : X × H × W → X that updates the learner 's parameters by x ( 1 ) = x + ϕ ( x , h , w ) given x ∈ X , a learning state h ∈ H , and meta-parameters w ∈ W ⊂ R nw of the update<br>",
    "Arabic": "قاعدة التحديث",
    "Chinese": "更新规则",
    "French": "règle de mise à jour",
    "Japanese": "更新ルール",
    "Russian": "правило обновления"
  },
  {
    "English": "user embedding",
    "context": "1: We empirically confirm that our approach significantly improves on existing baselines that rely on fixed-size historical indices to guide model output to personalized predictions. Future work includes incorporating more contextual features in the <mark>user embeddings</mark>. For example, user preferences may vary based on time of day, day of week, and seasonality trends.<br>2: Formally, considering the <mark>user embeddings</mark>, we have: \n U T +1 = f RNN (U 1 , . . . , U T ),(6) \n with U T +1 the predicted embeddings at time T + 1 and f RNN (•) the function implemented by the user-RNN.<br>",
    "Arabic": "تضمين المستخدم",
    "Chinese": "用户嵌入",
    "French": "représentation vectorielle de l'utilisateur",
    "Japanese": "ユーザー埋め込み",
    "Russian": "встраивание пользователя"
  },
  {
    "English": "user utterance",
    "context": "1: (2010) also investigated the functions of listening agents, focusing on their response generation components. Their system takes the confidence score of speech recognition into account and changes the system response accordingly; it repeats the <mark>user utterance</mark> or makes an empathic utterance for high-confidence <mark>user utterance</mark>s and makes a backchannel when the confidence is low.<br>2: We formulate the problem of identifying a user's referent by calculating T i such that the probability P (T i |U ) is maximized. Here, T i denotes the i-th item enumerated by a system, and U denotes a <mark>user utterance</mark>.<br>",
    "Arabic": "تعبير المستخدم",
    "Chinese": "用户语句",
    "French": "énoncé de l'utilisateur",
    "Japanese": "ユーザ発話",
    "Russian": "пользовательская реплика"
  },
  {
    "English": "user-item matrix",
    "context": "1: Each dataset forms a sparse <mark>user-item matrix</mark> R, where the value of R(i, j) indicates the rating of user i for movie j. Given the user-item ratings matrix R, we follow the PureSVD procedure described in [6] to generate user and item latent vectors.<br>",
    "Arabic": "مصفوفة المستخدم-العنصر",
    "Chinese": "用户-物品矩阵",
    "French": "matrice utilisateur-élément",
    "Japanese": "ユーザー・アイテム行列",
    "Russian": "матрица пользователь-элемент"
  },
  {
    "English": "utility",
    "context": "1: Letũ t i,λi be the regularized <mark>utility</mark> of agent type λ i ∈ Λ ĩ \n u t i,λi : ∆(A i ) π → u t i , π − λ i D KL (π τ i ). Observation 1. We note the following: \n<br>2: is a multiple threshold rule with respect to a <mark>utility</mark> u * ∈ U, then d(x) is a multiple threshold rule with respect to every u ∈ U. In particular, if d(x) can be represented by non-negative thresholds over u * , it can be represented by non-negative thresholds over any u ∈ U.<br>",
    "Arabic": "منفعة",
    "Chinese": "效用",
    "French": "utilité",
    "Japanese": "効用",
    "Russian": "полезность"
  },
  {
    "English": "utility function",
    "context": "1: Given such a decision, we write the expected regret features under deviation f as r ξ f (σ) = E Γ∼ξ r Γ f (σ Γ ) , and the expected regret under <mark>utility function</mark> w as r ξ f (σ|w) = E Γ∼ξ r Γ f (σ Γ |w) .<br>2: As expressed in Eq 1, this <mark>utility function</mark> measures how useful it would be if a given post p were augmented with an answer a j paired with a different question q j in the candidate set.<br>",
    "Arabic": "دالة المنفعة",
    "Chinese": "效用函数",
    "French": "fonction d'utilité",
    "Japanese": "効用関数",
    "Russian": "функция полезности"
  },
  {
    "English": "utterance",
    "context": "1: Before describing our approach, we start defining important terminology. An <mark>utterance</mark> u is a sequence of tokens, directed from a user to a dialog system.<br>2: (2022) to feed both the <mark>utterance</mark> and the candidate plan to the encoder and let the decoder decode only for one step. The decoding probability over an token that is unused during pretraining is then repurposed as a proxy for matching score.<br>",
    "Arabic": "تعبير",
    "Chinese": "话语",
    "French": "énoncé",
    "Japanese": "発話",
    "Russian": "фраза"
  },
  {
    "English": "utterance encoder",
    "context": "1: Note that the <mark>utterance encoder</mark> can be any existing encoding model.<br>2: We use bi-directional gated recurrent units (GRU) (Chung et al., 2014)   Figure 2: The architecture of the proposed TRADE model, which includes (a) an <mark>utterance encoder</mark>, (b) a state generator, and (c) a slot gate, all of which are shared among domains.<br>",
    "Arabic": "تشفير الكلام",
    "Chinese": "话语编码器",
    "French": "codeur d'énoncés",
    "Japanese": "発話エンコーダ",
    "Russian": "кодировщик высказываний"
  },
  {
    "English": "validation",
    "context": "1: These include the number of examples in each of the dataset splits, the size on disk of the data, meaningful differences between the training, <mark>validation</mark>, and test split, and free text descriptions of the various fields that make up each example to help decide what information to use as input or output of a prediction model.<br>2: We crawl 8861 high-resolution background images from Flickr and Google and split them by 8636 : 200 : 25 to use when constructing the train, <mark>validation</mark>, and test sets. We will release the test set in which all images have a CC license (see appendix for details).<br>",
    "Arabic": "التحقق",
    "Chinese": "验证集",
    "French": "validation",
    "Japanese": "検証",
    "Russian": "валидация"
  },
  {
    "English": "validation accuracy",
    "context": "1: The previous section shows that active learning fails to improve over random acquisition on VQA across models and datasets. A simple question remains -why? One hypothesis is that sample inefficiency stems from the data itself: there is only a 2% gain in <mark>validation accuracy</mark> when training on half versus the whole dataset.<br>2: We hypothesize that a model will have higher entropy for more difficult tasks because it will require using more paths in its computation graph. During our analysis, we drop runs where the model was unable to achieve a <mark>validation accuracy</mark> of > 90%, to avoid confounding results with models unable to learn the task. Model.<br>",
    "Arabic": "دقة التحقق من الصحة",
    "Chinese": "验证准确率",
    "French": "précision de validation",
    "Japanese": "検証精度",
    "Russian": "точность валидации"
  },
  {
    "English": "validation dataset",
    "context": "1: Participants had access to the QQP <mark>validation dataset</mark>, and are instructed to create tests that explore different capabilities of the model.<br>2: These phrases were identified by using the model's convolutional filter weights to identify posts in the <mark>validation dataset</mark> that are strongly contributing to the model's classification decision, and then using the convolutional filter weights to identify the phrase within each post that most strongly contributed to the post's classification (i.e., had the highest feature values).<br>",
    "Arabic": "مجموعة بيانات التحقق من الصحة",
    "Chinese": "验证数据集",
    "French": "ensemble de validation",
    "Japanese": "検証データセット",
    "Russian": "набор данных для валидации"
  },
  {
    "English": "validation datum",
    "context": "1: However, since we use mini-batches composed of 100.000 samples, we can approximate the training set sufficiently well while simultaneously introducing a positive, regularizing effect. Tab. 2 provides a summary of Top5-Errors on <mark>validation data</mark> for our proposed dNDF.NET against GoogLeNet and GoogLeNet⋆.<br>2: In this scenario, the author set in test data does not overlap with the training data. Validation data. We also randomly sample the training data into a smaller subset to use as <mark>validation data</mark> to tune hyperparameters during the training process. The size of the validation set is randomly selected to be the same as the cross-topic test set.<br>",
    "Arabic": "بيانات التحقق من الصحة",
    "Chinese": "验证数据",
    "French": "donnée de validation",
    "Japanese": "検証データ",
    "Russian": "валидационные данные"
  },
  {
    "English": "validation loss",
    "context": "1: Nevertheless, we find that also with µP excessive parameters hurt: The models with more than 2 billion parameters have significantly higher <mark>validation loss</mark> after training than the models with 200 million to 1 billion parameters when trained on only 100 million tokens.<br>2: Note that for a given <mark>validation loss</mark> value, bigger models also perform better.<br>",
    "Arabic": "خسارة التحقق",
    "Chinese": "验证损失",
    "French": "perte de validation",
    "Japanese": "検証損失",
    "Russian": "потеря валидации"
  },
  {
    "English": "validation performance",
    "context": "1: By monitoring the <mark>validation performance</mark> along checkpoints of the (m) f+a T5-11B model on both tasks, we identified a trend where the performance on factual contexts improves where the performance on random ones declines, and vice versa.<br>2: We choose the best checkpoint for evaluation based on <mark>validation performance</mark>. We use the Adam optimizer for all parameter optimization. We follow the hidden size of pretrained models with dimensionalities of 512 (mt5-small), 768 (mt5-base), and 1024 (mBART-large-50).<br>",
    "Arabic": "أداء التحقق من الصحة",
    "Chinese": "验证性能",
    "French": "performance de validation",
    "Japanese": "検証パフォーマンス",
    "Russian": "производительность на проверочных данных"
  },
  {
    "English": "validation set",
    "context": "1: Automatically aligned sentences from these two datasets are typically used to train and evaluate supervised simplification systems. We find that errors occur frequently in the validation and test sets of both datasets, although they are more common in Newsela (Section 6).<br>2: A <mark>validation set</mark> of 30 utterances and a test set of 100 utterances are selected randomly, leaving 370 utterances for training.<br>",
    "Arabic": "مجموعة التحقق من الصحة",
    "Chinese": "验证集",
    "French": "ensemble de validation",
    "Japanese": "検証セット",
    "Russian": "набор валидации"
  },
  {
    "English": "validation split",
    "context": "1: If an object type has over 5 unique assets, then those assets are partitioned into train, validation, and testing splits. Specifically, approximately 2 /3 of the assets are assigned to the train split, and approximately 1 /6 of the assets are assigned to each of the validation and testing splits.<br>2: We apply individual WSL approaches and vary the size of clean data sub-sampled from the original <mark>validation split</mark>. For text and relation classification tasks, we draw an increasing number of clean samples N ∈ {5, 10, 15, 20, 30, 40, 50} per class when applicable.<br>",
    "Arabic": "التقسيم التحقّقي",
    "Chinese": "验证集",
    "French": "partition de validation",
    "Japanese": "検証用分割",
    "Russian": "валидационное разделение"
  },
  {
    "English": "value estimate",
    "context": "1: This term has a regularization effect, by imposing an ξ-margin between the <mark>value estimate</mark> of an option and that of the \"optimal\" one reflected in V Ω . This makes the advantage function positive if the value of an option is near the optimal one, thereby stretching it.<br>2: max a Q T (o i , a;h i ) − Q T (o i , a i ;h i ) LG: Loss Gain Student's task-level loss L(θ i ) reduction L(θ i t ) − L(θ i t+1 ) \n LGG: Loss Gradient Gain Student's task-level policy gradient magnitude   \n ||∇ θ i L ( θ i ) || 2 2 TDG : TD Gain Student 's temporal difference ( TD ) error δ i reduction |δ i t | − |δ i t+1 | VEG : Value Estimation Gain Student 's <mark>value estimate</mark>V ( θ i ) gain above threshold τ 1 ( V ( θ i ) > τ ) Agent j<br>",
    "Arabic": "تقدير القيمة",
    "Chinese": "价值估计",
    "French": "estimation de valeur",
    "Japanese": "価値の推定",
    "Russian": "оценка значения"
  },
  {
    "English": "value function",
    "context": "1: The Hamilton-Jacobi-Bellman equation (HJB) is a PDE for the so-called <mark>value function</mark> that represents the minimal costto-go in stochastic optimal control problems from which the optimal control policy can be deduced. As suggested in (E et al., 2017), we consider the HJB equation \n<br>2: ||v * − T π h λ v|| ∞ ≤ γ(1 − λ) 1 − λγ + γ h ||v * − v|| ∞ . (9) \n Additionally, there exist a γ-discounted MDP, <mark>value function</mark> v, and policy \n π h ∈ G h (v) s.t.<br>",
    "Arabic": "دالة القيمة",
    "Chinese": "值函数",
    "French": "fonction de valeur",
    "Japanese": "価値関数",
    "Russian": "функция ценности"
  },
  {
    "English": "value function approximation",
    "context": "1: Thus, the <mark>value function approximation</mark> learns the relative contribution of each local shape feature to winning, across the full distribution of positions encountered during selfplay.<br>2: In the previous section, we prove that our memorybased <mark>value function approximation</mark> is better than the mean outcome evaluation used in MCTS with high probability under mild conditions. The remaining question is to design a practical algorithm and incorporate it with MCTS.<br>",
    "Arabic": "وظيفة تقريب القيمة",
    "Chinese": "值函数近似",
    "French": "approximation de la fonction de valeur",
    "Japanese": "価値関数近似",
    "Russian": "аппроксимация функции стоимости"
  },
  {
    "English": "value iteration",
    "context": "1: A policy π * is said to be optimal if V π * (s) = V * (s) ∀s. A popular algorithm for calculating V * and π * is <mark>value iteration</mark> (VI): \n<br>2: Our VIN policies learn an approximate planning computation relevant for solving the task, and we have shown that such a computation leads to better generalization in a diverse set of tasks, ranging from simple gridworlds that are amenable to <mark>value iteration</mark>, to continuous control, and even to navigation of Wikipedia links.<br>",
    "Arabic": "تكرار القيمة",
    "Chinese": "价值迭代",
    "French": "itération de valeur",
    "Japanese": "価値反復法",
    "Russian": "итерация значений"
  },
  {
    "English": "value iteration algorithm",
    "context": "1: We employ the MaxCausalEnt framework to model this setting, as shown in Figure 1. (stochastic) <mark>value iteration algorithm</mark> (Bellman, 1957) for finding the optimal control policy.<br>2: The following facts about M allow for an efficient <mark>value iteration algorithm</mark>: All MDPs generated by the conversion algorithm will have this structure due to the fact that t must decrease by 1 at every time step. (Figure 2 shows this fact visually.)<br>",
    "Arabic": "خوارزمية تكرار القيمة",
    "Chinese": "值迭代算法",
    "French": "algorithme d'itération de valeur",
    "Japanese": "価値反復アルゴリズム",
    "Russian": "алгоритм итерации значений"
  },
  {
    "English": "value network",
    "context": "1: We apply the actor-critic framework (Sutton et al., 2000), that uses a policy network π θ (a t |s t ) (actor) and a <mark>value network</mark> V π φ (s t ) (critic) to formulate the policy and the state-value function respectively.<br>2: The update makes use of a target <mark>value network</mark> V π φ , whereφ is the exponentially moving average of the <mark>value network</mark> weights and has been shown to stabilize training (Mnih et al., 2015).<br>",
    "Arabic": "شبكة القيمة",
    "Chinese": "价值网络",
    "French": "réseau de valeurs",
    "Japanese": "価値ネットワーク",
    "Russian": "сеть значений"
  },
  {
    "English": "value-base reinforcement learning",
    "context": "1: Value-based reinforcement learning algorithms have achieved many notable successes. For example, variants of the T D(λ) algorithm have learned to achieve a master level of play in the games of Chess (Baxter et al., 1998), Checkers (Schaeffer et al., 2001) and Othello (Buro, 1999).<br>2: The UCT algorithm (Kocsis & Szepesvari, 2006) is a <mark>value-based reinforcement learning</mark> algorithm that focusses exclusively on the start state and the tree of subsequent states.<br>",
    "Arabic": "التعلم المعزز القائم على القيمة",
    "Chinese": "值基强化学习",
    "French": "apprentissage par renforcement basé sur la valeur",
    "Japanese": "価値ベースの強化学習",
    "Russian": "обучение с подкреплением на основе ценности"
  },
  {
    "English": "vanilla Transformer",
    "context": "1: Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the <mark>vanilla Transformer</mark> is (by a small margin) the second most accurate model, and \n (2) the Performer (Choromanski et al., 2021) is the fastest model.<br>",
    "Arabic": "محول تقليدي",
    "Chinese": "普通Transformer",
    "French": "Transformateur standard",
    "Japanese": "標準Transformer",
    "Russian": "ванильный трансформер"
  },
  {
    "English": "vanishing gradient",
    "context": "1: While such strategy is able to reduce the generation complexity, it inevitably incurs expensive computing and memory overhead, and will hinder the training owing to the <mark>vanishing gradient</mark> for long CDR sequences. It also acumulates errors during the inference stage.<br>2: Previously, bidirectional networks (Schuster and The skip connection was adopted to mitigate the <mark>vanishing gradient</mark>, while the dropout was applied on dashed connections to prevent co-adaptation and overfitting. Paliwal, 1997) have been shown to be effective for sequential problems (Graves et al., 2013a;Sundermeyer et al., 2014).<br>",
    "Arabic": "التدرج التلاشي",
    "Chinese": "梯度消失",
    "French": "gradient qui disparait",
    "Japanese": "勾配消失",
    "Russian": "исчезающий градиент"
  },
  {
    "English": "vanishing gradient problem",
    "context": "1: This reduces the number of processing steps between the bottom of the network and the top, and therefore mitigates the <mark>vanishing gradient problem</mark> (Bengio et al., 1994) in the vertical direction. To allow all hidden layer information to influence the reading gate, Equation 7 is changed to \n<br>2: In contrast, the LSTM-based system described in this paper can deal with these problems automatically by learning the control of gates and surface realisation jointly. Training an RNN with long range dependencies is difficult because of the <mark>vanishing gradient problem</mark> (Bengio et al., 1994).<br>",
    "Arabic": "مشكلة تلاشي التدرج",
    "Chinese": "梯度消失问题",
    "French": "problème du gradient évanescent",
    "Japanese": "勾配消失問題",
    "Russian": "проблема исчезающего градиента"
  },
  {
    "English": "variable assignment",
    "context": "1: A partial state p is a <mark>variable assignment</mark> over some variables vars(p) ⊆ V. We write p[V ] for the value assigned to the variable V ∈ vars(p) in the partial state p. We also identify p with the set of facts contained in p, i.e.,  \n<br>2: The initial state s I is a complete and the goal description s a partial <mark>variable assignment</mark> over V. For a state s, an s-plan is an operator sequence π that is applicable in s and for which s π and s are consistent. An s I -plan is called a solution or plan for the task.<br>",
    "Arabic": "تعيين متغير",
    "Chinese": "变量赋值",
    "French": "affectation de variables",
    "Japanese": "変数割り当て",
    "Russian": "присваивание переменной"
  },
  {
    "English": "variable selection",
    "context": "1: Rather than relying on models that require manual selection of a set of words, our research tries to build models that will perform <mark>variable selection</mark> to automatically learn a semantic basis of word meaning.<br>2: Fitting even simple models such as multiple linear regression to large data sets raises interesting research questions. These include questions about regularization and prediction (e.g., how to estimate the parameters to optimize out-of-sample prediction accuracy), and questions about <mark>variable selection</mark> and inference (e.g., how to choose the coefficients in the regression that are non-zero).<br>",
    "Arabic": "اختيار المتغيرات",
    "Chinese": "变量选择",
    "French": "sélection de variables",
    "Japanese": "変数選択",
    "Russian": "выбор переменных"
  },
  {
    "English": "variance reduction",
    "context": "1: We also notice that the standard deviation of those models' ranking performance is reduced when applying document space projection, which confirms our analysis of <mark>variance reduction</mark> in Lemma 3.2. From Figure 2 and Table 3 we notice that document space projection mostly improves offline performance over baseline algorithms.<br>2: Our solution falls into this second category of <mark>variance reduction</mark> approaches for DBGD-type algorithms. Distinct from previous attempts to restrict gradient exploration before an interleaved test, we instead modify the selected direction after the test.<br>",
    "Arabic": "تقليل التباين",
    "Chinese": "方差缩减",
    "French": "réduction de la variance",
    "Japanese": "分散削減",
    "Russian": "дисперсионное уменьшение"
  },
  {
    "English": "variance regularization",
    "context": "1: [22] also provide a number of asymptotic results showing relationships between the robust risk R n (θ; P n ) and <mark>variance regularization</mark>, but they do not leverage these results for guarantees on the solutions θ rob n . Notation We collect our notation here.<br>2: Inequality ( 10) and the exact expansion (11) show that, at least for bounded loss functions ℓ, the robustly regularized risk ( 4) is a natural (and convex) surrogate for empirical risk plus standard deviation of the loss, and the robust formulation approximates exact <mark>variance regularization</mark> with a convex penalty.<br>",
    "Arabic": "تنظيم التباين",
    "Chinese": "方差正则化",
    "French": "régularisation de la variance",
    "Japanese": "分散の正則化",
    "Russian": "регуляризация дисперсии"
  },
  {
    "English": "variational Bayes",
    "context": "1: Sampling approaches are usually based on Markov Chain Monte Carlo (MCMC) sampling, where a Markov chain is defined whose stationary distribution is the posterior of interest. Optimization approaches are usually based on variational inference, which is called <mark>variational Bayes</mark> (VB) when used in a Bayesian hierarchical model.<br>2: To this end, we develop an online <mark>variational Bayes</mark> algorithm for latent Dirichlet allocation (LDA), one of the simplest topic models and one on which many others are based. Our algorithm is based on online stochastic optimization, which has been shown to produce good parameter estimates dramatically faster than batch algorithms on large datasets [6].<br>",
    "Arabic": "بايز التفاوتي",
    "Chinese": "变分贝叶斯",
    "French": "inférence bayésienne variationnelle",
    "Japanese": "変分ベイズ",
    "Russian": "вариационный байесовский"
  },
  {
    "English": "variational approach",
    "context": "1: However, fully sampled data is often unavailable in a variety of problems, including the recovery of dynamic and high-resolution MRI data considered in this work. To overcome this problem, we introduce a novel <mark>variational approach</mark> to learn a manifold from undersampled data.<br>2: Therefore, we adopt a <mark>variational approach</mark> which optimizes various subgroups of the variables in a round-robin fashion, holding approximations to the others fixed. We first describe the variable groups, then the updates which optimize them in turn.<br>",
    "Arabic": "النهج التغيري",
    "Chinese": "变分方法",
    "French": "approche variationnelle",
    "Japanese": "変分的アプローチ",
    "Russian": "вариационный подход"
  },
  {
    "English": "variational approximation",
    "context": "1: Two of the resulting topics are illustrated in Figure 4, showing the top several words from those topics in each decade, according to the posterior mean number of occurrences as estimated using the Kalman filter <mark>variational approximation</mark>. Also shown are example articles which exhibit those topics through the decades.<br>2: This motivates the inclusion of a <mark>variational approximation</mark> that models the true posterior as a factored distribution over parameters at two levels of the prior hierarchy. While seemingly quite different, drawing on results from [1], we can show that the resulting cost function is exactly equivalent to standard ARD assuming Σ s is parameterized as \n<br>",
    "Arabic": "التقريب التغيري",
    "Chinese": "变分近似",
    "French": "approximation variationnelle",
    "Japanese": "変分近似",
    "Russian": "вариационное приближение"
  },
  {
    "English": "variational autoencoder",
    "context": "1: The deep autoencoder has and encoder with hidden dimensions of [500,300,100] and a decoder with hidden dimensions of [100,300,500]. The output has a sigmoid and we use a sigmoid cross entropy loss for L prediction . <mark>Variational Autoencoder</mark>. We use an architecture described in Higgins et al. (2017a) (Table 2).<br>",
    "Arabic": "المُرَمِّز الذاتي التغَيُّري",
    "Chinese": "变分自编码器",
    "French": "auto-encodeur variationnel",
    "Japanese": "変分オートエンコーダ",
    "Russian": "Вариационный автоэнкодер"
  },
  {
    "English": "variational bind",
    "context": "1: L(n, λ) d (n d , γ(n d , λ), φ(n d , λ), λ),(7) \n where \n (n d , γ d , φ d , λ) \n is the dth document's contribution to the <mark>variational bound</mark> in equation 4.<br>2: DPMs and their applications. The diffusion probabilistic model (DPM) is initially introduced by Sohl- Dickstein et al. (2015), where the DPM is trained by optimizing the <mark>variational bound</mark> L vb . Ho et al. (2020) propose the new parameterization of DPMs in Eq.<br>",
    "Arabic": "الارتباط المتغير",
    "Chinese": "变分边界",
    "French": "borne variationnelle",
    "Japanese": "変分束縛",
    "Russian": "вариационная связь"
  },
  {
    "English": "variational distribution",
    "context": "1: [ K uu ] m , n : = k ( z m , z n ) \n . This <mark>variational distribution</mark> is determined through defining the density of the function values u ∈ R M at inducing inputs Z = {z m } M m=1 to be q(u) = N (µ, Σ).<br>2: In the dynamic topic model, the latent variables are the topics β t,k , mixture proportions θ t,d , and topic indicators z t,d,n . The <mark>variational distribution</mark> reflects the group structure of the latent variables.<br>",
    "Arabic": "توزيع تغيري",
    "Chinese": "变分分布",
    "French": "distribution variationnelle",
    "Japanese": "変分分布",
    "Russian": "вариационное распределение"
  },
  {
    "English": "variational formulation",
    "context": "1: The use of the <mark>variational formulation</mark> in V-SToRM encouraged the latent variables of the slices to approximate a Gaussian distribution. We also reported the KL divergence value compared to N (0, I) for each set of the latent vector in the figure.<br>2: We now introduce a novel <mark>variational formulation</mark> to learn a manifold from undersampled measurements, which is the generalization of the seminal VAE approach [2] to the undersampled setting. We will first present the proposed approach in a simple and general setting for simplicity and ease of understanding.<br>",
    "Arabic": "صياغة متباينة",
    "Chinese": "变分形式",
    "French": "formulation variationnelle",
    "Japanese": "変分定式化",
    "Russian": "вариационная формулировка"
  },
  {
    "English": "variational framework",
    "context": "1: Because these methods require fully sampled data, they will not facilitate the exploitation of the inter-slice redundancies during image recovery. We introduce a novel <mark>variational framework</mark> for the joint recovery and alignment of multislice data from the entire heart.<br>2: Our results show that the joint alignment and recovery of the slices offer reduced blurring and reduction of artifacts compared to the direct generalization of G-SToRM to the multislice setting. In particular, the <mark>variational framework</mark> encourages the latent variables of different slices to have the same distribution.<br>",
    "Arabic": "إطار تغيري",
    "Chinese": "变分框架",
    "French": "cadre variationnel",
    "Japanese": "変分フレームワーク",
    "Russian": "вариационная модель"
  },
  {
    "English": "variational inference",
    "context": "1: In this section we show that algorithm 2 converges to a stationary point of the objective defined in equation 7. Since <mark>variational inference</mark> replaces sampling with optimization, we can use results from stochastic optimization to analyze online LDA. Stochastic optimization algorithms optimize an objective using noisy estimates of its gradient [18].<br>2: This observation supports our argument in Section 4.4, where we argue that the marginal likelihood targets the average posterior sample rather than the BMA performance. We note that Morningstar et al. (2022) discuss the distinction between targeting the BMA performance and average sample performance from the perspective of the PAC-Bayes and <mark>variational inference</mark>.<br>",
    "Arabic": "الاستدلال التغيري",
    "Chinese": "变分推断",
    "French": "inférence variationnelle",
    "Japanese": "変分推論",
    "Russian": "вариационный вывод"
  },
  {
    "English": "variational low bound",
    "context": "1: Since the likelihood is not tractable, we maximize its <mark>variational lower bound</mark> involving a model for the conditional distribution of the latent variables, which is conceptually similar to the VAE approach [2]. The VAE scheme uses an encoder network to derive the conditional probabilities of the latent vectors from fully sampled data [2].<br>2: Our goal is to estimate the parameters θ, β, and η, given observations of network structures G (t) and linguistic content x (t) , for t ∈ {1, . . . , T }. Eliding the sum over instances t, we seek to maximize the <mark>variational lower bound</mark> on the expected likelihood, \n L Q =E Q [ log P ( y , x ; β , θ , G ) ] − E Q [ log Q ( y ) ] =E Q [ log P ( x | y ; θ ) ] + E Q [ log P ( y ; G , β , η ) ] − E Q [ log Q<br>",
    "Arabic": "الحد السفلي المتغير",
    "Chinese": "变分下界",
    "French": "borne inférieure variationnelle",
    "Japanese": "変分下限",
    "Russian": "вариационная нижняя граница"
  },
  {
    "English": "variational method",
    "context": "1: In this work, we focus on a priori bounds, and asymptotic behavior as N → ∞ and M grows as a function of N . These bounds guarantee how the <mark>variational method</mark> scales computationally for any dataset satisfying intuitive conditions. This is particularly important for continual learning scenarios, where we incrementally observe more data.<br>",
    "Arabic": "الطريقة التغيرية",
    "Chinese": "变分方法",
    "French": "méthode variationnelle",
    "Japanese": "変分法",
    "Russian": "вариационный метод"
  },
  {
    "English": "variational model",
    "context": "1: We trained the <mark>variational model</mark> based on the undersampled k-t space data and fed the latent vectors corresponding to the second slice to the generator, which produces the aligned multislice reconstructions. Shown in the figures are four time points based on the different phases identified by the latent variables. The rows in Fig.<br>2: We propose a convex relaxation for the <mark>variational model</mark> (1), which opposed to existing functional lifting methods [17,18] allows continuous label spaces even after discretization. Our method (here applied to stereo matching) avoids label space discretization artifacts, while saving on memory and runtime.<br>",
    "Arabic": "النموذج التغيري",
    "Chinese": "变分模型",
    "French": "modèle variationnel",
    "Japanese": "変分モデル",
    "Russian": "вариационная модель"
  },
  {
    "English": "variational objective",
    "context": "1: We now expand the expectations above to be functions of the variational parameters. This reveals that the <mark>variational objective</mark> relies only on n dw , the number of times word w appears in document d. When using VB-as opposed to MCMC-documents can be summarized by their word counts, \n L = d w n dw k φ dwk ( E q [ log θ dk ] + E q [ log β kw ] − log φ dwk ) − log Γ ( k γ dk ) + k ( α − γ dk ) E q [ log θ dk ] + log Γ ( γ dk ) + ( k − log Γ ( w λ kw ) + w ( η − λ kw ) E q [ log β kw ] + log Γ ( λ kw ) ) /D + log Γ ( Kα ) − K log Γ ( α ) + ( log Γ ( W η ) − W log Γ ( η ) ) /D d ( n<br>2: Other examples besides the cycle inequalities include the odd-wheel and bicycle odd-wheel inequalities [6], and also linear inequalities that enforce positive semi-definiteness of M 1 (µ). The cutting-plane algorithm is in effect optimizing the <mark>variational objective</mark> (Eq.<br>",
    "Arabic": "الهدف المتغير",
    "Chinese": "变分目标函数",
    "French": "objectif variationnel",
    "Japanese": "変分目的関数",
    "Russian": "вариационная цель"
  },
  {
    "English": "variational parameter",
    "context": "1: A good setting of the topics λ is one for which the ELBO L is as high as possible after fitting the per-document <mark>variational parameters</mark> γ and φ with the E step defined in algorithm 1.<br>2: The procedure for recursively updating the posterior is summarized in Algorithm 1 and more details can be found in the Supplemental Material. To compute the upper bound of D(f g) we minimizeD(φ, ψ, f, g) with respect to the <mark>variational parameters</mark> φ and ψ.<br>",
    "Arabic": "المعلمة المتغيرة",
    "Chinese": "变分参数",
    "French": "paramètre variationnel",
    "Japanese": "変分パラメータ",
    "Russian": "вариационный параметр"
  },
  {
    "English": "variational posterior",
    "context": "1: In many applications, pointwise estimates of the posterior mean and variance are of interest. It is therefore desirable that the approximate <mark>variational posterior</mark> gives similar estimates of these quantities as the true posterior. Huggins et al.<br>2: where Q(R|x, z ′ , y) is an approximate <mark>variational posterior</mark>. We now relax the restriction that P (R|x, z ′ ) places non-zero mass only on permutation matrices and use the following definition of P θ (R|x, z ′ ): \n<br>",
    "Arabic": "المقترب البعدي التباينّي",
    "Chinese": "变分后验分布",
    "French": "distribution postérieure variationnelle",
    "Japanese": "変分事後分布",
    "Russian": "вариационное апостериорное распределение"
  },
  {
    "English": "vector",
    "context": "1: 1. different <mark>vector</mark> positions may be allocated to the synonyms of the same term; this way, there is an information loss because the importance of a determinate concept is distributed among different <mark>vector</mark> components; 2. the size of a document <mark>vector</mark> must be at least equal to the total number of words of the language used to write the document; \n<br>2: For a <mark>vector</mark> x ∈ R d , we define (x) i ∈ R as the i-th entry of x, and x 2 as its 2 norm; for a matrix X ∈ R m×n , we define (X) i ∈ R n as the transpose of the i-th row of X, and X (resp.<br>",
    "Arabic": "المتجه",
    "Chinese": "向量",
    "French": "vecteur",
    "Japanese": "ベクトル",
    "Russian": "вектор"
  },
  {
    "English": "vector arithmetic",
    "context": "1: (3) This means that solving analogy questions with <mark>vector arithmetic</mark> is mathematically equivalent to seeking a word (b * ) which is similar to b and a * but is different from a. Relational similarity is thus expressed as a sum of attributional similarities.<br>2: 'she', 'he'), more attributes (e.g. 'baby', 'office'), average these vectors and apply more advanced <mark>vector arithmetic</mark> to put this initially surprising result to the test.<br>",
    "Arabic": "حسابيات المتجهات",
    "Chinese": "向量算术",
    "French": "arithmétique vectorielle",
    "Japanese": "ベクトル演算",
    "Russian": "векторная арифметика"
  },
  {
    "English": "vector concatenation",
    "context": "1: where [• ; •] denotes <mark>vector concatenation</mark>. Joint Modeling Following Chen et al. (2020), we create a model that jointly considers all candidate spans given the template type.<br>",
    "Arabic": "ضم المتجهات",
    "Chinese": "向量拼接",
    "French": "concaténation de vecteurs",
    "Japanese": "ベクトル連結",
    "Russian": "конкатенация векторов"
  },
  {
    "English": "vector embedding",
    "context": "1: Specifically, U φ h (for a given sentence, suppressed) is an r dimensional vector representation of the word corresponding to h as a head word. Similarly, V φ m provides an analogous representation for a modifier m. Finally, W φ h,m is a <mark>vector embedding</mark> of the supplemental arc-dependent information.<br>",
    "Arabic": "تضمين متجهي",
    "Chinese": "向量嵌入",
    "French": "plongement vectoriel",
    "Japanese": "ベクトル埋め込み",
    "Russian": "векторное вложение"
  },
  {
    "English": "vector field",
    "context": "1: We evolve a 3D <mark>vector field</mark> defined on an estimation of the object surface so as to match the backprojected image at time instant Ø and the backprojected image onto the predicted surface at time instant Ø• ½ in all cameras.<br>2: Then, a time-dependent <mark>vector field</mark> v t ∈ X(M) is defined so that for each time t ∈ [0, 1], the flow Φ t defined by equation 3 satisfies the continuous normalization equation: \n Φ * tαt =α 0 ,(5) \n<br>",
    "Arabic": "حقل متجهي",
    "Chinese": "矢量场",
    "French": "champ vectoriel",
    "Japanese": "ベクトル場",
    "Russian": "векторное поле"
  },
  {
    "English": "vector graphic",
    "context": "1: We scan a large set of tangram puzzles to <mark>vector graphics</mark>, and crowdsource annotations of natural language descriptions and part segmentations.<br>",
    "Arabic": "رسوم متجهية",
    "Chinese": "矢量图形",
    "French": "graphique vectoriel",
    "Japanese": "ベクターグラフィック",
    "Russian": "векторная графика"
  },
  {
    "English": "vector normalization",
    "context": "1: θ i = θ i−1 − αf (θ i−1 )∇ θ f (θ i−1 ),(6) \n followed by a re-projection to the sphere (i.e. <mark>vector normalization</mark>) after several iterations.<br>",
    "Arabic": "تطبيع المتجه",
    "Chinese": "向量归一化",
    "French": "normalisation vectorielle",
    "Japanese": "ベクトル正規化",
    "Russian": "векторная нормализация"
  },
  {
    "English": "vector quantization",
    "context": "1: Previous approaches include sparse coding, <mark>vector quantization</mark>, and k-means on top of the HoG-3d feature set (see [21] for a comprehensive evaluation).<br>2: Concretely, a learned projection matrix is first applied to the token-level representation vectors of GPT-2. Each projected vector is then converted into a single discrete symbol via <mark>vector quantization</mark> (van den Oord et al., 2017). The number of symbols is kept small; as such, only a few bits are needed to encode all symbols.<br>",
    "Arabic": "تكميم المتجهات",
    "Chinese": "向量量化",
    "French": "quantification vectorielle",
    "Japanese": "ベクトル量子化",
    "Russian": "векторное квантование"
  },
  {
    "English": "vector representation",
    "context": "1: Taddy , 2015 ) . While there is still an active research line to better understand these models from a theoretical perspective (Levy and Goldberg, 2014c;Arora et al., 2016;Gittens et al., 2017), the fundamental idea behind all of them is to assign a similar <mark>vector representation</mark> to similar words.<br>2: With any latent generative model, the rich latent space provides a <mark>vector representation</mark> for unstructured data. Using the MVAE, we first sample z i ∼ q(z i |x i ) for all x i ∈ D test ; we then train use t- SNE (Maaten and Hinton 2008) to reduce to two dimensions.<br>",
    "Arabic": "تمثيل المتجهات",
    "Chinese": "向量表示",
    "French": "représentation vectorielle",
    "Japanese": "ベクトル表現",
    "Russian": "векторное представление"
  },
  {
    "English": "vector space",
    "context": "1: In the <mark>vector space</mark> representation, orientation is expressed by orientation of bases of subspaces: two bases represent the same flat if they not only span the same subspace but also have equal handedness. Epipolar geometry, so useful for photogrammetry and 1 This paper was started in Oxford and finished in Prague.<br>2: While the neural embeddings are mostly opaque, one of the appealing properties of explicit vector representations is our ability to read and understand the vectors' features. For example, king is represented in our explicit <mark>vector space</mark> by 51,409 contexts, of which the top 3 are tut +1 , jeongjo +1 , adulyadej +2 -all names of monarchs.<br>",
    "Arabic": "فضاء متجهي",
    "Chinese": "向量空间",
    "French": "espace vectoriel",
    "Japanese": "ベクトル空間",
    "Russian": "векторное пространство"
  },
  {
    "English": "vector space embedding",
    "context": "1: A variety of approaches for constructing <mark>vector space embeddings</mark> of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008).<br>",
    "Arabic": "تضمين فضاء المتجهات",
    "Chinese": "向量空间嵌入",
    "French": "plongement dans un espace vectoriel",
    "Japanese": "ベクトル空間埋め込み",
    "Russian": "векторное пространство вложений"
  },
  {
    "English": "vector space model",
    "context": "1: This paper presents a <mark>vector space model</mark> approach, for representing documents and queries, using concepts instead of terms and WordNet as a light ontology. This way, information overlap is reduced with respect to the classic semantic expansion techniques. Experiments carried out on the MuchMore benchmark showed the effectiveness of the approach.<br>2: The pivoted normalization retrieval formula [14] is one of the best performing vector space retrieval formulas. In the <mark>vector space model</mark>, text is represented by a vector of terms. Documents are ranked by the similarity between the query vector and the document vector. According to [14], the pivoted normalization retrieval formula is \n<br>",
    "Arabic": "نموذج الفضاء المتجه",
    "Chinese": "向量空间模型",
    "French": "modèle d'espace vectoriel",
    "Japanese": "ベクトル空間モデル",
    "Russian": "модель векторного пространства"
  },
  {
    "English": "vector space representation",
    "context": "1: Mikolov et al. demonstrated that <mark>vector space representations</mark> encode various relational similarities, which can be recovered using vector arithmetic and used to solve word-analogy tasks.<br>",
    "Arabic": "تمثيل الفضاء المتجه",
    "Chinese": "向量空间表示",
    "French": "représentation dans un espace vectoriel",
    "Japanese": "ベクトル空間表現",
    "Russian": "Представление векторного пространства"
  },
  {
    "English": "vector-valued function",
    "context": "1: Due to the existence of ∇ η log q η (x) as a weighting function, we apply a discrete Stein operator to a vector-valued functioñ h : X → R d per dimension to construct the following estimator with a mean-zero CV: \n<br>",
    "Arabic": "دالة متجهية",
    "Chinese": "向量值函数",
    "French": "fonction à valeurs vectorielles",
    "Japanese": "ベクトル値関数",
    "Russian": "векторнозначная функция"
  },
  {
    "English": "vectorization",
    "context": "1: As F and F ′ are all in polar coordinates, rotating their observations in the yaw direction is equivalent to shifting their polarized DFs with the yaw angle θ. vec denotes the <mark>vectorization</mark> operation that converts a matrix to a vector by concatenating its rows.<br>2: Applying the <mark>vectorization</mark> scheme of Figure 3 to the light transport equation and re-arranging terms we get for epipolar line e: \n ie = E f =1 T ef p f = E f =1 (1p T f ) block of probing matrix • T ef block of T 1 (6 \n ) \n where E is the number of epipolar lines.<br>",
    "Arabic": "ناقلات",
    "Chinese": "矢量化",
    "French": "vectorisation",
    "Japanese": "ベクトル化",
    "Russian": "векторизация"
  },
  {
    "English": "vectorization operator",
    "context": "1: Let \"vec\" be the <mark>vectorization operator</mark>, which reshapes an n × n matrix to an n 2 vector, i.e., vec By applying the <mark>vectorization operator</mark> and using (5.1), we obtain I − cP ⊤ ⊗ P ⊤ vec(S L (Θ)) = vec(Θ).<br>",
    "Arabic": "عامل المتجهات",
    "Chinese": "矢量化算子",
    "French": "opérateur de vectorisation",
    "Japanese": "ベクトル化演算子",
    "Russian": "оператор векторизации"
  },
  {
    "English": "vectorize",
    "context": "1: In order to <mark>vectorize</mark> this loop over i, j, we need to reindex the chart. Instead of using a single chart C, we split it into two parts: one rightfacing \n<br>",
    "Arabic": "تحويل إلى متجهات",
    "Chinese": "矢量化",
    "French": "vectoriser",
    "Japanese": "ベクトル化",
    "Russian": "векторизовать"
  },
  {
    "English": "verbalizer",
    "context": "1: Both can be utilized for fine-tuning with supervised training data, but prompts further allow the user to customize patterns to help the model. For the prompt model we follow the notation from PET (Schick and Schütze, 2020a) and decompose a prompt into a pattern and a <mark>verbalizer</mark>.<br>2: Impact of Pattern vs Verbalizer The intuition of prompts is that they introduce a task description in natural language, even with few training points. To better understand the zero-shot versus adaptive nature of prompts, we consider a null <mark>verbalizer</mark>, a control with a <mark>verbalizer</mark> that cannot yield semantic information without training.<br>",
    "Arabic": "لفظي",
    "Chinese": "转换器",
    "French": "verbaliseur",
    "Japanese": "言語化器",
    "Russian": "вербализатор"
  },
  {
    "English": "vertex label",
    "context": "1: We assume that G s is a compact space by requiring that <mark>vertex labels</mark> come from a compact set K ⊆ R ℓ0 . Let F be a set of functions f : G s → R ℓ f and define its closure F as all functions h from G s for which there exists a sequence f 1 , f 2 , .<br>2: Note that the social network graphs are unlabeled, while all other graph datasets come with <mark>vertex labels</mark>.<br>",
    "Arabic": "تسمية قمة الرأس",
    "Chinese": "顶点标签",
    "French": "étiquette de sommet",
    "Japanese": "頂点ラベル",
    "Russian": "метка вершины"
  },
  {
    "English": "vertex set",
    "context": "1: The key component in the DSS-WL algorithm is the graph generation policy π which must maintain symmetry, i.e., be equivairant under permutation of the <mark>vertex set</mark>. We list several common choices below: \n • Node marking policy π = π NM .<br>",
    "Arabic": "مجموعة الرؤوس",
    "Chinese": "顶点集",
    "French": "ensemble de sommets",
    "Japanese": "頂点集合",
    "Russian": "множество вершин"
  },
  {
    "English": "victim model",
    "context": "1: Denote the <mark>victim model</mark> as Θ v , which is applied to provide EaaS S v . When a client sends a sentence s to the service S v , Θ v computes its original embedding e o .<br>2: where conf obs = φ(f (x) y ) is the confidence of <mark>victim model</mark> f on target example (x, y). The adversary infers membership by thresholding the likelihood Λ with threshold τ determined in advance.<br>",
    "Arabic": "نموذج الضحية",
    "Chinese": "受害者模型",
    "French": "modèle victime",
    "Japanese": "被害者モデル",
    "Russian": "модель жертвы"
  },
  {
    "English": "view synthesis",
    "context": "1: In this work, we address the long-standing problem of <mark>view synthesis</mark> in a new way by directly optimizing parameters of a continuous 5D scene representation to minimize the error of rendering a set of captured images. We represent a static scene as a continuous 5D function that outputs the radiance emitted in each direction ( θ , φ ) at each point ( x , y , z ) in space , and a density at each point which acts like a differential opacity controlling how much radiance is accumulated by a ray passing through ( x , y ,<br>2: Neural Radiance Fields (NeRF) is a popular <mark>view synthesis</mark> technique that represents a scene as a continuous volumetric function, parameterized by multilayer perceptrons that provide the volume density and view-dependent emitted radiance at each location.<br>",
    "Arabic": "تخليق الرؤية",
    "Chinese": "视图合成",
    "French": "synthèse de vues",
    "Japanese": "視点合成",
    "Russian": "синтез вида"
  },
  {
    "English": "virtual camera",
    "context": "1: The task of virtual view synthesis is to generate the image which would be seen by a <mark>virtual camera</mark> in a position not in the original set.<br>",
    "Arabic": "كاميرا افتراضية",
    "Chinese": "虚拟相机",
    "French": "caméra virtuelle",
    "Japanese": "仮想カメラ",
    "Russian": "виртуальная камера"
  },
  {
    "English": "vision Transformer",
    "context": "1: We use the popular vision benchmark, ImageNet [17]. We choose recent popular <mark>Vision Transformer</mark> [24], and MLP-Mixer [99] as representative base dense models. For language modeling, we evaluate GPT-2 [86] on WikiText-103 [73].<br>2: We follow the naming convention in the <mark>Vision Transformer</mark> paper and MLP-Mixer paper. In particular,<br>",
    "Arabic": "محول الرؤية",
    "Chinese": "视觉Transformer",
    "French": "vision Transformer",
    "Japanese": "ビジョントランスフォーマー",
    "Russian": "Трансформер зрения"
  },
  {
    "English": "vision model",
    "context": "1: We focus on vision tasks because abrupt transitions in <mark>vision models</mark>' capabilities have not been observed to the best of our knowledge; this is one reason why emergence in large language models is considered so interesting. For the convolutional example, see App. B.<br>2: The canonical approach to video-and-language learning (e.g., video question answering) dictates a neural model to learn from offline-extracted dense video features from <mark>vision models</mark> and text features from language models. These feature extractors are trained independently and usually on tasks different from the target domains, rendering these fixed features sub-optimal for downstream tasks.<br>",
    "Arabic": "نموذج الرؤية",
    "Chinese": "视觉模型",
    "French": "modèle de vision",
    "Japanese": "視覚モデル",
    "Russian": "модель зрения"
  },
  {
    "English": "vision system",
    "context": "1: Imagine instructing a <mark>vision system</mark> to \"Tag the 7 main characters on the TV show Big Bang Theory in this image.\"<br>2: This is a data-driven way to generate likely adjectives, prepositions, determiners, etc., taking the intersection of what the <mark>vision system</mark> predicts and how the object noun tends to be described.<br>",
    "Arabic": "نظام الرؤية",
    "Chinese": "视觉系统",
    "French": "système de vision",
    "Japanese": "ビジョンシステム",
    "Russian": "система зрения"
  },
  {
    "English": "vision-language model",
    "context": "1: LiT also increased training scale and experimented with a combination of pre-trained image representations and contrastive fine-tuning to connect frozen image representations to text [94]. Flamingo introduced the first large <mark>vision-language model</mark> with in-context learning [2]. Other papers have combined contrastive losses with image captioning to further improve performance [43,89].<br>2: One potential remedy would be to employ active learning methods to focus language elicitation on the most relevant abstractions in the environment and to explore data augmentation approaches to make these descriptions go further. For example, a <mark>vision-language model</mark> [50] could be fine-tuned on the human descriptions to generate sufficiently high-quality descriptions for new states.<br>",
    "Arabic": "نموذج بصري-لغوي",
    "Chinese": "视觉语言模型",
    "French": "modèle vision-langage",
    "Japanese": "ビジョン言語モデル",
    "Russian": "модель зрения и языка"
  },
  {
    "English": "visual attention",
    "context": "1: We propose a new interpretation to the term \"saliency\" \n and \"<mark>visual attention</mark>\" in images and in video sequences. 4. We present a single unified framework for treating several different problems in Computer Vision, which have been treated separately in the past.<br>2: In this appendix, we provide further translation examples for color deprivation (Table 6), entity masking (Table 7) and progressive masking (Table 8). Specifically for the entity masking experiments, we also give further examples to showcase the behavior of the <mark>visual attention</mark> in Figure 4 and Figure 5. ( b ) Entity-masked MMT Figure 5 : Attention example from entity masking experiments where terrier , grass and fence are dropped from the source sentence : ( a ) Baseline MMT is not able to shift attention from the salient dog to the grass and fence , ( b ) the attention produced by the masked MMT first shifts to the background area<br>",
    "Arabic": "الانتباه البصري",
    "Chinese": "视觉注意力",
    "French": "attention visuelle",
    "Japanese": "視覚的注意力",
    "Russian": "визуальное внимание"
  },
  {
    "English": "visual attribute",
    "context": "1: While traditional visual recognition approaches map low-level image features directly to object category labels, recent work proposes models using <mark>visual attributes</mark> [1][2][3][4][5][6][7][8].<br>",
    "Arabic": "الصفة البصرية",
    "Chinese": "视觉属性",
    "French": "attribut visuel",
    "Japanese": "視覚的属性",
    "Russian": "визуальный атрибут"
  },
  {
    "English": "visual context",
    "context": "1: The role of context in object recognition has become an important topic, due both to the psychological basis of context in the human visual system [10] and to the striking algorithmic improvements that \"<mark>visual context</mark>\" has provided [11]. The word \"context\" has been attached to many different ideas.<br>",
    "Arabic": "السياق البصري",
    "Chinese": "视觉环境",
    "French": "contexte visuel",
    "Japanese": "視覚的コンテキスト",
    "Russian": "визуальный контекст"
  },
  {
    "English": "visual cortex",
    "context": "1: The first -inspired by analogies between computational vision and biological vision -would draw a correspondence between how simple/complex cells in the <mark>visual cortex</mark> process scenes and their induced receptive fields with those of activations of units/blocks in a modern deep neural network architecture [61].<br>2: Barlow first hypothesised that neurons represent independent components via redundancy reduction -minimising mutual information between their representation (Barlow, 1961;1972) 5 . Since Barlow there have been numerous experimental results suggesting that brain neurons are disentangled : in inferior temporal cortex ( Chang & Tsao , 2017 ; Bao et al. , 2020 ; Higgins et al. , 2021 ; Yildirim et al. , 2020 ) , <mark>visual cortex</mark> ( Ecker et al. , 2010 ; Gáspár et al. , 2019 ) , parietal cortex (<br>",
    "Arabic": "القشرة البصرية",
    "Chinese": "视觉皮层",
    "French": "cortex visuel",
    "Japanese": "視覚野",
    "Russian": "зрительная кора"
  },
  {
    "English": "visual feature",
    "context": "1: Yu and Ballard (2004) paired training images containing multiple objects with spoken name candidates for the objects to find the correspondence between lexical items and <mark>visual features</mark>.<br>",
    "Arabic": "الميزة البصرية",
    "Chinese": "视觉特征",
    "French": "caractéristique visuelle",
    "Japanese": "視覚的特徴",
    "Russian": "визуальный признак"
  },
  {
    "English": "visual grounding",
    "context": "1: Others explicitly test for linguistic constructs within models, such as probing vision transformers for verb understand-ing (Hendricks and Nematzadeh, 2021) and examining <mark>visual grounding</mark> in image-to-text transformers (Ilinykh and Dobnik, 2022).<br>2: There is some new work showing that even a small amount of language can be used for models to perform <mark>visual grounding</mark> on large environments [51]. This approach to scaling language may require scoring descriptions for their quality in describing the relevant abstractions in the environment, which is less time-consuming for human participants than producing such descriptions.<br>",
    "Arabic": "الإرتباط المرئي",
    "Chinese": "视觉接地",
    "French": "ancrage visuel",
    "Japanese": "視覚的なグラウンディング",
    "Russian": "визуальная привязка"
  },
  {
    "English": "visual hull",
    "context": "1: Voxel occupancy is typically solved using silhouette intersection, usually from multiple cameras but sometimes from a single camera with the object placed on a turntable [8]. It is known that the output of silhouette intersection even without noise is not the actual 3-dimensional shape, but rather an approximation called the <mark>visual hull</mark> [17].<br>",
    "Arabic": "الهيكل البصري",
    "Chinese": "视觉外壳",
    "French": "enveloppe visuelle",
    "Japanese": "ビジュアルハル",
    "Russian": "визуальная оболочка"
  },
  {
    "English": "visual localization",
    "context": "1: Minimal 1 problems [37,50,24,26,28,29,25,30] play an important role in 3D reconstruction [48,49,47], image matching [44], visual odometry [39,4] and <mark>visual localization</mark> [52,46,51]. Many minimal problems have been described and solved and new minimal problems are constantly appearing.<br>2: The accuracy of SuperPoint is raised far higher than other detectors, despite the high sparsity of the 3D models that it produces. This shows how more accurate keypoint detections can result in much more accurate <mark>visual localization</mark>.<br>",
    "Arabic": "التوطين البصري",
    "Chinese": "视觉定位",
    "French": "localisation visuelle",
    "Japanese": "視覚的位置特定",
    "Russian": "визуальная локализация"
  },
  {
    "English": "visual odometry",
    "context": "1: By exploiting freely available, community developed maps and <mark>visual odometry</mark> measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.<br>2: Other techniques typically use more complex architectures that include mapping or <mark>visual odometry</mark> modules and use additional perception sensors such as depth images. Scale ablation. To evaluate the effect of scale we train the models on 10, 100, 1,000, and 10,000 houses. For this experiment, we do not use any material augmentations. As shown in<br>",
    "Arabic": "تقدير حركة الرؤية",
    "Chinese": "视觉里程计",
    "French": "odométrie visuelle",
    "Japanese": "視覚オドメトリ",
    "Russian": "визуальная одометрия"
  },
  {
    "English": "visual question answering",
    "context": "1: These tasks are meant to teach machine learning models to perform various tasks such as object recognition, visual relationship understanding, text-image grounding, and so on by following instructions so that they can perform zero-shot prediction on unseen tasks. To build MULTIINSTRUCT , we first collect 34 tasks from the existing studies in visual and multimodal learning , covering <mark>Visual Question Answering</mark> ( Goyal et al. , 2017 ; Krishna et al. , 2017 ; Zhu et al. , 2016 ; Hudson and Manning , 2019 ; Singh et al. , 2019 ; Marino et al. , 2019 ) , Commonsense Reasoning ( Suhr et al. , 2017 ; Liu et al. , 2022a ; Zellers et al. , 2019 ; Xie et al. , 2019 ) , Region Understanding ( Krishna et al. , 2017 ) , Image Understanding ( Kafle and Kanan , 2017 ; Chiu et al. , 2020 ) , Grounded Generation ( Krishna et al. , 2017 ; Yu et al. , 2016 ; Lin et al. , 2014 ) , Image-Text Matching ( Lin et al. , 2014 ; Goyal et al. , 2017 ) , Grounded Matching ( Krishna et al. , 2017 ; Veit et al. , 2016 ; Yu et al. , 2016 ) , Visual Relationship ( Krishna et al. , 2017 ; Pham et al. , 2021 ) , Temporal<br>2: tasks ? In this work , we propose MULTIINSTRUCT , the first benchmark dataset for multimodal instruction tuning with 62 diverse tasks from 10 broad categories , including <mark>Visual Question Answering</mark> ( Goyal et al. , 2017 ; Suhr et al. , 2017 ) , Commonsense Reasoning ( Zellers et al. , 2019 ; Xie et al. , 2019 ) , Visual Relationship Understanding ( Krishna<br>",
    "Arabic": "الإجابة على الأسئلة البصرية",
    "Chinese": "视觉问答",
    "French": "réponse aux questions visuelles",
    "Japanese": "ビジュアル質問応答 (Visual Question Answering)",
    "Russian": "визуальные вопросно-ответные системы"
  },
  {
    "English": "visual recognition system",
    "context": "1: We also learn mappings between concepts predicted by existing <mark>visual recognition systems</mark> and entry-level concepts that could be useful for improving human-focused applications such as natural language image description or retrieval.<br>",
    "Arabic": "نظام التعرف البصري",
    "Chinese": "视觉识别系统",
    "French": "système de reconnaissance visuelle",
    "Japanese": "視覚認識システム",
    "Russian": "система визуального распознавания"
  },
  {
    "English": "viterbi decoding",
    "context": "1: We then investigate the effectiveness of constrained <mark>Viterbi decoding</mark> after correcting the least confident error.<br>2: In general, we see that <mark>Viterbi decoding</mark> results in better precision and worse recall, whereas forwards-backwards inference yields slightly worse precision, but improves recall. The relatively low recall indicates that about 80% of infections occur without any evidence in social media as reflected in our features.<br>",
    "Arabic": "فك تشفير فيتربي",
    "Chinese": "维特比解码",
    "French": "décodage de Viterbi",
    "Japanese": "ビタビ復号化",
    "Russian": "декодирование Витерби"
  },
  {
    "English": "vocabulary",
    "context": "1: For the En→De translation task, sentences are encoded using bytepair encoding (BPE) (Sennrich et al., 2016) with 37k merging operations for both source and target languages, which have <mark>vocabularies</mark> of 39418 and 40274 tokens respectively. We limit the length of sentences in the training datasets to 50 words for Zh→En and 128 subwords for En→De.<br>",
    "Arabic": "مفردات",
    "Chinese": "词汇量",
    "French": "vocabulaire",
    "Japanese": "語彙",
    "Russian": "словарь"
  },
  {
    "English": "vocabulary size",
    "context": "1: This can also be phrased as Problem 2, where we ask for maximizing the acoustic coverage and diversity subject to a bounded <mark>vocabulary size</mark> constraint.<br>2: For Transformer architectures, the optimal <mark>vocabulary size</mark> is less than 4K, around up to 2K merge actions. We compare VOLT and BPE-1K on an X-to-English bilingual setting. The results are shown in Table 2. We can see that VOLT can find a good vocabulary on par with heuristically searched vocabularies in terms of BLEU scores.<br>",
    "Arabic": "حجم المفردات",
    "Chinese": "词汇量",
    "French": "taille du vocabulaire",
    "Japanese": "語彙サイズ",
    "Russian": "размер словаря"
  },
  {
    "English": "vocoder",
    "context": "1: WaveNet is capable of generating fairly natural sounding speech, in contrast to the <mark>vocoder</mark>-based synthesizer used in previous EMG-to-speech papers, which caused significant degradation in naturalness (Janke and Diener, 2017).<br>",
    "Arabic": "فوكودر",
    "Chinese": "声码器",
    "French": "vocodeur",
    "Japanese": "ボコーダー",
    "Russian": "вокодер"
  },
  {
    "English": "volume rendering",
    "context": "1: We synthesize images by sampling 5D coordinates (location and viewing direction) along camera rays (a), feeding those locations into an MLP to produce a color and volume density (b), and using <mark>volume rendering</mark> techniques to composite these values into an image (c).<br>2: In summary, our technical contributions are: -An approach for representing continuous scenes with complex geometry and materials as 5D neural radiance fields, parameterized as basic MLP networks. -A differentiable rendering procedure based on classical <mark>volume rendering</mark> techniques, which we use to optimize these representations from standard RGB images.<br>",
    "Arabic": "تصوير حجمي",
    "Chinese": "体积渲染",
    "French": "rendu volumique",
    "Japanese": "ボリュームレンダリング",
    "Russian": "объемный рендеринг"
  },
  {
    "English": "von Mises-Fisher distribution",
    "context": "1: We model this distribution defined on the unit sphere with a von Mises-Fisher (vMF) distribution (also known as a normalized spherical Gaussian), centered at reflection vector ω r , and with a concentration parameter κ defined as inverse roughness κ = 1 /ρ.<br>",
    "Arabic": "توزيع فون ميزس-فيشر",
    "Chinese": "冯米塞斯-菲舍尔分布",
    "French": "distribution de von Mises-Fisher",
    "Japanese": "フォン・ミーゼス・フィッシャー分布",
    "Russian": "распределение фон Мизеса-Фишера"
  },
  {
    "English": "voting rule",
    "context": "1: A <mark>voting rule</mark> f is an algorithm that returns a candidate f ( #-≻) ∈ C given a ranked-choice profile #-≻. We refer to f ( #-≻) as the winner of the election E using the <mark>voting rule</mark> f , or just as the winner of f if E is clear from the context.<br>2: The recent papers [16,23] establish related lower bounds: Fain et al. [16] show that any <mark>voting rule</mark> that only obtains the top k = O(1) candidates of each voter must have squared distortion Ω(m), in particular implying a bound of Ω(m) for the distortion of deterministic rules.<br>",
    "Arabic": "قاعدة التصويت",
    "Chinese": "投票规则",
    "French": "règle de vote",
    "Japanese": "投票規則",
    "Russian": "правило голосования"
  },
  {
    "English": "voxel",
    "context": "1: It is hoped that examination of other strategies will lead to significantly quicker solutions. (3) Occlusion is handled here by the robust kernel ρ. More geometric handling of occlusion, analogous to space carving's improvement over <mark>voxel</mark> colouring, ought to yield better results.<br>2: Early approaches generalized 2D convolutions to 3D [6,15,25,50,51], and employed volumetric grids to represent shapes in terms of coarse occupancy functions, where a <mark>voxel</mark> evaluates to zero if it is outside and one otherwise. Unfortunately, these methods are typically limited to low resolutions of at most 64 3 due to the cubic growth in memory requirements.<br>",
    "Arabic": "فوكسل",
    "Chinese": "体素",
    "French": "voxel",
    "Japanese": "ボクセル",
    "Russian": "воксельный"
  },
  {
    "English": "voxel grid",
    "context": "1: Voxel grids While MLPs can label an arbitrary element of the 3D domain, a <mark>voxel grid</mark> can be seen as an implicit surface restricted to a subset of R 3 which is uniformly subdivided to a lattice V (z) ∈ R R 3 of R 3 ; R ∈ N + cuboid elements of the same size.<br>2: The camera distance d cam is then defined as one minus the intersection-over-union of the similarities between all pairs of rays generated by each <mark>voxel grid</mark> point x k : \n<br>",
    "Arabic": "شبكة فوكسل",
    "Chinese": "体素栅格",
    "French": "grille de voxels",
    "Japanese": "ボクセルグリッド",
    "Russian": "воксельная сетка"
  },
  {
    "English": "voxel grid representation",
    "context": "1: Some prior work infers <mark>voxel grid representations</mark> of 3D scenes from images [6,8,9] or uses them for 3D-structure-aware generative models [10,36]. Graph neural networks may similarly capture 3D structure [37]. Compositional structure may be modeled by representing scenes as programs [38].<br>",
    "Arabic": "تمثيل شبكة فوكسل",
    "Chinese": "体素网格表示",
    "French": "représentation par grille de voxels",
    "Japanese": "ボクセルグリッド表現",
    "Russian": "представление в виде воксельной сетки"
  },
  {
    "English": "voxel occupancy",
    "context": "1: Voxel occupancy is typically solved using silhouette intersection, usually from multiple cameras but sometimes from a single camera with the object placed on a turntable [8]. It is known that the output of silhouette intersection even without noise is not the actual 3-dimensional shape, but rather an approximation called the visual hull [17].<br>2: The problem of reconstructing a scene from multiple cameras has received a great deal of attention in the last few years. One extensively-explored approach to this problem is <mark>voxel occupancy</mark>. In <mark>voxel occupancy</mark> [18,25] the scene is represented as a set of 3-dimensional voxels, and the task is to label the individual voxels as filled or empty.<br>",
    "Arabic": "احتلال الفوكسل",
    "Chinese": "体素占用",
    "French": "occupation de voxels",
    "Japanese": "ボクセル占有率",
    "Russian": "заполнение вокселей"
  },
  {
    "English": "voxel representation",
    "context": "1: Following this route, most deep architectures for 3D point cloud analysis require pre-processing of irregular point clouds into either <mark>voxel representations</mark> (e.g., [45,37,44]) or 2D images by view projection (e.g., [41,34,24,9]).<br>",
    "Arabic": "تمثيل مجسم",
    "Chinese": "体素表示",
    "French": "représentation voxel",
    "Japanese": "ボクセル表現",
    "Russian": "воксельное представление"
  },
  {
    "English": "voxel-base representation",
    "context": "1: PIFu [70] regresses an implicit surface representation that locally aligns pixels with the global context of the corresponding 3D object. Unlike <mark>voxel-based representations</mark>, this implicit per-pixel representation is more memory efficient. These approaches have not been demonstrated to generalize well to strong articulation.<br>",
    "Arabic": "تمثيل على أساس فوكسلات",
    "Chinese": "体素表示",
    "French": "représentation basée sur des voxels",
    "Japanese": "ボクセルベース表現",
    "Russian": "воксельное представление"
  },
  {
    "English": "warp function",
    "context": "1: As this model grows, so must the support of the <mark>warp function</mark>. In this subsection we de-Figure 4: The first frames and final canonical models from DynamicFusion results shown in our accompanying video, available on our project website: http://grail.cs.washington.edu/projects/dynamicfusion.<br>2: w i (x c ) = exp − dg i v − x c 2 / 2(dg i w ) 2 \n . Each radius parameter dg i w is set to ensure the node's influence overlaps with neighbouring nodes, dependent on the sampling sparsity of nodes, which we describe in detail in section (3.4). Since the <mark>warp function</mark> defines a rigid body transformation for all supported space , both position and any associated orientation of space is transformed , e.g. , the vertex v c from a surface with orientation or normal n c is transformed into the live frame as ( v t , 1 ) = W t ( v c ) ( v c ,<br>",
    "Arabic": "وظيفة الاعوجاج",
    "Chinese": "变形函数",
    "French": "fonction de déformation",
    "Japanese": "ワープ関数",
    "Russian": "функция искажения"
  },
  {
    "English": "wav2vec",
    "context": "1: Similar to <mark>wav2vec</mark>, Hu-BERT (Hsu et al., 2021), a pretrain language model that leverages selfsupervised learning for speech, directly processes audio waveform information from raw speech to predict clustering categories for the speech segments.<br>2: Both <mark>wav2vec</mark> 2.0 and Hu-BERT have been successful in capturing acoustic information from raw speech and improve the state-of-the-art performance in speech recognition and translation. van den Oord et al.<br>",
    "Arabic": "مُحوِّل الموجة إلى متجه (wav2vec)",
    "Chinese": "wav2vec",
    "French": "wav2vec",
    "Japanese": "wav2vec",
    "Russian": "wav2vec"
  },
  {
    "English": "wavelet",
    "context": "1: We chose the Mexican hat <mark>wavelet</mark> and calculated the continuous <mark>wavelet</mark> transform (scalogram) of {R i (τ ) : τ = −1024 to 1024}, \n C i (a, b) = allτ R i (τ )ψ (τ − b, a)dτ, (10 \n ) \n<br>2: → R ) and the orthogonal series estimate ( T i ( X i , A ) = 1 n J j=1 g j ( X i ) A g j for some cutoff J and orthonormal basis { g j } ∞ j=1 ( e.g. , Fourier , <mark>wavelet</mark> , or polynomial ) of L 2 ( Ω ) ) .<br>",
    "Arabic": "موجة صغيرة",
    "Chinese": "小波",
    "French": "ondelette",
    "Japanese": "ウェーブレット",
    "Russian": "вейвлет"
  },
  {
    "English": "wavelet transform",
    "context": "1: Note also that |Z jk | > λ if and only if |D jk | > 0. These derivatives can be computed using off-the-shelf software for the <mark>wavelet transform</mark> in any of the standard wavelet bases. Sample results of running this and the Kalman variational algorithm to approximate a unigram model are given in Figure 3.<br>",
    "Arabic": "تحويل موجي",
    "Chinese": "小波变换",
    "French": "Transformation en ondelettes",
    "Japanese": "ウェーブレット変換",
    "Russian": "вейвлет-преобразование"
  },
  {
    "English": "weak classifier",
    "context": "1: Theorem 29 If the <mark>weak classifier</mark> space H satisfies the richness condition (72), and the number of rounds T is set to √ m, then the error of the final classifier H T approaches the Bayes optimal error: \n<br>2: Following that we describe how it adaptively computes the weights in each round based on the edge of the <mark>weak classifier</mark> received.<br>",
    "Arabic": "مصنف ضعيف",
    "Chinese": "弱分类器",
    "French": "classificateur faible",
    "Japanese": "弱分類器",
    "Russian": "слабый классификатор"
  },
  {
    "English": "weak learner",
    "context": "1: More importantly, this also means that the algorithm treats each <mark>weak learner</mark> equally and ignores the fact that some <mark>weak learner</mark>s are actually doing better than the others. The best example of adaptive boosting algorithm is the well-known parameter-free AdaBoost algorithm [Freund and Schapire, 1997], where each <mark>weak learner</mark> is naturally weighted by how accurate it is.<br>2: Having access to such a <mark>weak learner</mark> makes the boosting algorithm simpler: we now simply pass every example (x t , y t ) to every <mark>weak learner</mark> WL i using the probability p i t = \n w i t w i ∞ as importance \n weights.<br>",
    "Arabic": "المتعلم الضعيف",
    "Chinese": "弱学习者",
    "French": "apprenant faible",
    "Japanese": "弱学習器",
    "Russian": "слабый обучающий алгоритм"
  },
  {
    "English": "weak learning",
    "context": "1: The assumptions of boostability, and hence our minimal <mark>weak learning</mark> condition does not hold for the vast majority of practical data sets, and as such it is important to know what happens in such settings.<br>2: As predicted by our theory, our algorithm succeeds in boosting the accuracy even when the tree size is too small to meet the stronger <mark>weak learning</mark> assumptions of the other algorithms. More insight is provided by plots in Figure 11 of the rate of convergence of error with rounds when the tree size allowed is very small (5).<br>",
    "Arabic": "التعلم الضعيف",
    "Chinese": "弱学习",
    "French": "apprentissage faible",
    "Japanese": "弱学習",
    "Russian": "Слабое обучение"
  },
  {
    "English": "weak learning assumption",
    "context": "1: In particular, the exact requirements on the weak classifiers in this setting are known: any algorithm that predicts better than random on any distribution over the training set is said to satisfy the <mark>weak learning assumption</mark>. Further, boosting algorithms that minimize loss as efficiently as possible have been designed.<br>2: We now justify our definition of weak online learning, viz. inequality (1). In the standard batch boosting case , the corresponding <mark>weak learning assumption</mark> ( see for example Schapire and Freund [ 2012 ] ) made is that there is an algorithm which , given a training set of examples and an arbitrary distribution on it , generates a hypothesis that has error at most 1 2 − γ on the training data under the given<br>",
    "Arabic": "افتراض التعلم الضعيف",
    "Chinese": "弱学习假设",
    "French": "hypothèse d'apprentissage faible",
    "Japanese": "弱学習仮定",
    "Russian": "слабое предположение об обучении"
  },
  {
    "English": "weak supervision",
    "context": "1: (2017) propose to label data programmatically using heuristics such as keywords, regular expressions or knowledge bases. One drawback of <mark>weak supervision</mark> is that its annotations are noisy, i.e., some annotations are incorrect. Training models on such noisy data may result in poor generalization (Zhang et al., 2017;Tänzer et al., 2022;.<br>2: We on the other hand are more interested in understand-ing how the phonological level of representation emerges from <mark>weak supervision</mark> via correlated signal from the visual modality. There are some existing models which learn language representations from sensory input in such a weakly supervised fashion.<br>",
    "Arabic": "الإشراف الضعيف",
    "Chinese": "弱监督",
    "French": "supervision faible",
    "Japanese": "弱教師あり学習",
    "Russian": "Слабое обучение"
  },
  {
    "English": "weakly supervise",
    "context": "1: For future work we hope to use similar techniques for <mark>weakly supervised</mark> image segmentation. We also plan to improve our detection results using more powerful matching strategies for assigning weak labels to classification data during training. Computer vision is blessed with an enormous amount of labelled data.<br>2: , which use monolingual data both for learning good language models and for fantasizing parallel data . Another avenue of research has been to extend the traditional supervised learning setting to a <mark>weakly supervised</mark> one, whereby the original training set is augmented with parallel sentences mined from noisy comparable corpora like Paracrawl.<br>",
    "Arabic": "تدريب ضعيف",
    "Chinese": "弱监督",
    "French": "faiblement supervisé",
    "Japanese": "弱教師あり",
    "Russian": "слабоуправляемый"
  },
  {
    "English": "weakly supervise learning",
    "context": "1: The success of SCL has motivated a series of works to apply CL to a number of <mark>weakly supervised learning</mark> tasks, including noisy label learning [25], [36], semi-supervised learning [37], [38], etc. Despite promising empirical results, however, these works, lack theoretical understanding.<br>2: Weakly supervised learning (WSL) is one of the most popular approaches for alleviating the annotation bottleneck in machine learning. Instead of collecting expensive clean annotations, it leverages weak labels from various weak labeling sources such as heuristic rules, knowledge bases or lowerquality crowdsourcing (Ratner et al., 2017).<br>",
    "Arabic": "التعلم تحت إشراف ضعيف",
    "Chinese": "弱监督学习",
    "French": "apprentissage faiblement supervisé",
    "Japanese": "弱教師学習",
    "Russian": "Обучение с неполным контролем"
  },
  {
    "English": "web graph",
    "context": "1: Let G = (V, E) denote the <mark>web graph</mark>, where V is the set of all web pages and E contains a directed edge p, q iff page p links to page q.<br>2: A build of CS2 takes a web crawl as input and creates a representation of the entire <mark>web graph</mark> induced by the pages in the crawl, in the form of a database that consists of all URLs that were crawled together with all in-links and out-links among those URLs.<br>",
    "Arabic": "الرسم البياني للويب",
    "Chinese": "网络图",
    "French": "graphe du Web",
    "Japanese": "ウェブグラフ",
    "Russian": "веб-граф"
  },
  {
    "English": "weight",
    "context": "1: Putting everything together, the overall model can be computed as Ψ = ∑ , where is an edge of the model, and are its potential function and <mark>weight</mark> respectively.<br>",
    "Arabic": "وزن",
    "Chinese": "权重",
    "French": "poids",
    "Japanese": "重み",
    "Russian": "вес"
  },
  {
    "English": "weight average",
    "context": "1: Specifically, the state vector c[t] is a <mark>weighted average</mark> between the previous state c[t-1] and a linear transformation of the input W x[t]. The weighted aggregation is controlled by a forget gate f [t] which is a sigmoid function over the current input and hidden state.<br>2: W(x c ) ≡ SE3(DQB(x c )) ,(1) \n where the <mark>weighted average</mark> over unit dual quaternion trans- \n formations is simply DQB(x c ) ≡ k∈N (xc ) w k (xc)q kc k∈N (xc ) w k (xc)q kc , \n<br>",
    "Arabic": "المتوسط الوزني",
    "Chinese": "加权平均",
    "French": "moyenne pondérée",
    "Japanese": "加重平均",
    "Russian": "средний вес"
  },
  {
    "English": "weight decay",
    "context": "1: Each model was trained on a single NVIDIA TITAN Xp 12GB graphics card with batch size = 128, using the RMSProp optimizer with learning rate = 1e-4 and no <mark>weight decay</mark>. Probe models were trained for 2 epochs each. Pruning scores were then computed using the EL2Ns metric [10], averaged across 10 independent random seeds.<br>2: σ 2 ), which models the invariance to augmentation correctly: a large data augmentation variance σ 2 should lead to a small magnitude of the learned representation. Ideally, we want s j to have the same property. With <mark>weight decay</mark> η > 0 in Eqn.<br>",
    "Arabic": "انحلال الوزن",
    "Chinese": "权重衰减",
    "French": "décroissance du poids",
    "Japanese": "重み減衰",
    "Russian": "затухание веса"
  },
  {
    "English": "weight direct graph",
    "context": "1: For example, if we encode a <mark>weighted directed graph</mark> using a ternary predicate edge, then rules ( 1) and (2), where sp is a min limit predicate, compute the cost of a shortest path from a given source node v 0 to every other node. → sp(v 0 , 0) \n<br>2: Furthermore, the two perspectives are connected via the shortest path algorithm in <mark>weighted directed graph</mark>, a standard well-studied operation in graph analysis.<br>",
    "Arabic": "الرسم البياني الموجه الوزني",
    "Chinese": "加权有向图",
    "French": "graphe dirigé pondéré",
    "Japanese": "重み付き有向グラフ",
    "Russian": "взвешенный направленный граф"
  },
  {
    "English": "weight graph",
    "context": "1: We now establish our connection between prize-collecting Steiner tree (PCST) problems and the <mark>weighted graph</mark> model. First, we formally define the PCST problem: Let G = (V, E) be an undirected, <mark>weighted graph</mark> with edge costs c : E → R + 0 and node prizes π : V → R + 0 .<br>2: We also define a <mark>weighted graph</mark> G k = (V k , E k ) whose vertices are specified by the set V k and whose edges are specified by the set E k . The weight of an edge (a, b) ∈ E k is given by w(a, b).<br>",
    "Arabic": "رسم البيان الموزون",
    "Chinese": "权重图",
    "French": "graphe pondéré",
    "Japanese": "重み付きグラフ",
    "Russian": "взвешенный граф"
  },
  {
    "English": "weight initialization",
    "context": "1: We find warmup on the learning rate of the embeddings has little improvement, so we only apply one-epoch learning rate warmup to the dense weights of the CTR prediction model. Weight initialization is also important for a good starting state. We use Kaiming initialization (He et al. 2015) for all dense weights.<br>2: For some other problems the bias could be more specific than the code of the move, i.e. a move could be associated to different bias depending on the state. In this case different bias could be used in different states for the same move which would not be possible with <mark>weight initialization</mark>.<br>",
    "Arabic": "تهيئة الأوزان",
    "Chinese": "权重初始化",
    "French": "initialisation des poids",
    "Japanese": "重み初期化",
    "Russian": "инициализация весов"
  },
  {
    "English": "weight matrix",
    "context": "1: g(y t−1 ) = (1 − H) e + H ē (12 \n ) \n where H is the transform gate controlling the proportion of the weighted average embedding: \n H = W 1 e + W 2ē(13) \n where W 1 , W 2 ∈ R L×L are <mark>weight matrices</mark>.<br>2: 2 ) + b ( t ) j • 1 x1=x1   . Here , t ) ij and b ( t ) j are real values corresponding the <mark>weight matrices</mark> and bias vector in layer t. These are expressions in GTL ( t ) ( σ ) since the additional summation is guarded , and combined with the summation depth of t − 1 of ϕ ( t−1 ) i , this results in a summation<br>",
    "Arabic": "مصفوفة الأوزان",
    "Chinese": "权重矩阵",
    "French": "matrice de poids",
    "Japanese": "重み行列",
    "Russian": "матрица весов"
  },
  {
    "English": "weight parameter",
    "context": "1: For example, if the tracker output has low precision, the corresponding <mark>weight parameter</mark> will be decreased during training to minimize the reliance on the tracker output. In the maximum entropy framework, the distribution over a state sequence s is defined as: \n<br>2: We already observed empirically that when the <mark>weight parameter</mark> η is small, the degrees all become equal to k. Indeed, we can prove this fact. Lemma 19 If the loss function being used is exponential loss (32) and the <mark>weight parameter</mark> η is small compared to the number of rounds \n<br>",
    "Arabic": "معامل الوزن",
    "Chinese": "权重参数",
    "French": "paramètre de pondération",
    "Japanese": "重みパラメータ",
    "Russian": "весовой параметр"
  },
  {
    "English": "weight regularization",
    "context": "1: Using dropout and 2 <mark>weight regularization</mark> lessened this effect, but did not eliminate it entirely.<br>",
    "Arabic": "تنظيم الوزن",
    "Chinese": "权重正则化",
    "French": "régularisation des poids",
    "Japanese": "重み正則化",
    "Russian": "регуляризация весов"
  },
  {
    "English": "weight sum",
    "context": "1: The context vector c t−1 is an attentive read of H, which is a <mark>weighted sum</mark> of the encoder's hidden states as c t−1 = n k=1 α t−1 k h k , and α t−1 k measures the relevance between state s t−1 and hidden state h k .<br>2: The output y j is now contextualized as a <mark>weighted sum</mark> of previous u ≤j and subsequent u ≥j inputs.<br>",
    "Arabic": "مجموع موزون",
    "Chinese": "加权和",
    "French": "somme pondérée",
    "Japanese": "重み付き和",
    "Russian": "взвешенная сумма"
  },
  {
    "English": "weight tensor",
    "context": "1: To facilitate the objective evaluation of the model's prediction, we provided a <mark>weight tensor</mark> of shape (d o , N x ) to convert raw outputs to area-weighted outputs with consistent energy flux units [W/m 2 ]. More details are given below.<br>",
    "Arabic": "موتر الوزن",
    "Chinese": "权重张量",
    "French": "tenseur de pondération",
    "Japanese": "重みテンソル",
    "Russian": "тензор весов"
  },
  {
    "English": "weight update",
    "context": "1: Then, we compute a <mark>weight update</mark> ∆w = δe + (v − q w (S, A))∇ w q w (S, A).<br>",
    "Arabic": "تحديث الوزن",
    "Chinese": "权重更新",
    "French": "mise à jour des poids",
    "Japanese": "重み更新",
    "Russian": "обновление весов"
  },
  {
    "English": "weight vector",
    "context": "1: The inference problem is to find the 1-best parse tree arg max y∈Y y • w, where w ∈ R |I| is a <mark>weight vector</mark> that assigns a score to each index i (we dis-cuss how w is learned in Section 4). A generalization of the 1-best inference problem is to find the max-marginal score for each index i. Maxmarginals are given by the function M : I → Y defined as M ( i ; Y , w ) = arg max y∈Y : y ( i ) =1 y • w. For first-order parsing , this corresponds to the best parse utilizing a given<br>2: We assess our model's estimation convergence via cosine similarity between the current <mark>weight vector</mark> and a reference <mark>weight vector</mark> (considered to be the optimal vector) as estimated by an offline learning-to-rank algorithm trained with the complete true relevance judgment labels.<br>",
    "Arabic": "متجه الوزن",
    "Chinese": "权重向量",
    "French": "vecteur de poids",
    "Japanese": "重みベクトル",
    "Russian": "вектор весов"
  },
  {
    "English": "weight-sharing",
    "context": "1: Siamese networks can naturally introduce inductive biases for modeling invariance, as by definition \"invariance\" means that two observations of the same concept should produce the same outputs. Analogous to convolutions [25], which is a successful inductive bias via <mark>weight-sharing</mark> for modeling translation-invariance, the <mark>weight-sharing</mark> Siamese networks can model invariance w.r.t.<br>",
    "Arabic": "تشارك الأوزان",
    "Chinese": "权重共享",
    "French": "partage de poids",
    "Japanese": "重み共有",
    "Russian": "совместное использование весов"
  },
  {
    "English": "weighted adjacency matrix",
    "context": "1: Figure 2 depicts a <mark>weighted adjacency matrix</mark> where coordinate (i, j) corresponds to F(l i → l j ).<br>2: We assign each edge a weight representing the number of different triangles it belongs to, which forms a weight matrix . We denote as the degree of node from the <mark>weighted adjacency matrix</mark> . The aggregator is then calculated by applying a row-wise normalization: \n m = ∑︁ ∈N 1 m −1 . (8) \n 3.2.2 Message Aggregators.<br>",
    "Arabic": "مصفوفة الجوار المرجحة",
    "Chinese": "加权邻接矩阵",
    "French": "matrice d'adjacence pondérée",
    "Japanese": "重み付き隣接行列",
    "Russian": "матрица весовых смежностей"
  },
  {
    "English": "weighting function",
    "context": "1: <mark>weighting function</mark> . A transaction t is represented as a vector v1, v2, ..., vm , where vi = 1 iff ui ∈ t, otherwise vi = 0. The two key issues in a VSM are to select the vector dimensions and to assign weights for each dimension [17].<br>",
    "Arabic": "\"دالة الترجيح\"",
    "Chinese": "权重函数",
    "French": "fonction de pondération",
    "Japanese": "重み付け関数",
    "Russian": "функция взвешивания"
  },
  {
    "English": "white-box",
    "context": "1: Next we employ them as the trigger to attack other examples, the model predictions flip from both neutral and entailment to contradict. Our attack method relies on attribution scores, which utilizes the gradient information, therefore it belongs to <mark>white-box</mark> non-targeted attacks. We extract the dependencies with the largest attribution scores as the adversarial triggers from 3,000 input examples.<br>2: Chen et al. also validated that partial black-box attack can achieve similar attack performance as <mark>white-box</mark>, because the adversary has access to z and can leverage non-differentiable optimization, e.g., the Powell's Conjugate Direction Method (Powell, 1964)), to approximately minimize L cal . E.5.<br>",
    "Arabic": "صندوق أبيض",
    "Chinese": "白盒",
    "French": "boîte blanche",
    "Japanese": "ホワイトボックス",
    "Russian": "белый ящик"
  },
  {
    "English": "white-box attack",
    "context": "1: However, recent work has constructed functions g(•) which are neither smooth nor differentiable, and therefore can not be backpropagated through to generate adversarial examples with a <mark>white-box attack</mark> that requires gradient signal.<br>2: According to the adversary' knowledge, the attack can be divided into black-box attack, partial black-box attack and <mark>white-box attack</mark>. We conducted the <mark>white-box attack</mark> for scenarios where the adversary has access to the generators. The results on CelebA are in Table 4, indicating that vanilla GAN can be used to infer the membership of training data.<br>",
    "Arabic": "هجوم الصندوق الأبيض",
    "Chinese": "白盒攻击",
    "French": "attaque en boîte blanche",
    "Japanese": "ホワイトボックス攻撃",
    "Russian": "атака белого ящика"
  },
  {
    "English": "window size",
    "context": "1: As mentioned, all experiments presented in the main paper use a <mark>window size</mark> of 1 meaning only 1 dimension can be changed at a time. In the binary case, we sample a dimension i ∼ q(i|x) which tells us which dimension to flip to generate our proposed update.<br>2: The best performing parameters were a <mark>window size</mark> of 3 for the corpus count model, and a <mark>window size</mark> of 7 and 400 dimensions for word2vec, although the findings for other <mark>window size</mark>s and dimensions were quite similar.<br>",
    "Arabic": "حجم النافذة",
    "Chinese": "窗口大小",
    "French": "taille de la fenêtre",
    "Japanese": "ウィンドウサイズ",
    "Russian": "размер окна"
  },
  {
    "English": "within-class variance",
    "context": "1: We then compute the fisher discriminant score (ratio of <mark>within-class variance</mark> to between-class variance) for each dimension as measures of the discrimination power realized by the representations.<br>",
    "Arabic": "التباين داخل الصنف",
    "Chinese": "类内方差",
    "French": "variance intra-classe",
    "Japanese": "クラス内分散",
    "Russian": "внутриклассовая дисперсия"
  },
  {
    "English": "word alignment",
    "context": "1: The figure also shows that augmenting annotated documents with additional environment-reward documents invariably improves performance. The <mark>word alignment</mark> results from Table 2 indicate that the learners are mapping the correct words to actions for documents that are successfully completed. For example, the models that perform best in the Windows domain achieve nearly perfect <mark>word alignment</mark> scores.<br>2: The parallel phrases were extracted from the Hebrew Bible and its translations via <mark>word alignment</mark> and postprocessing. For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer (Habash and Rambow, 2005); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006).<br>",
    "Arabic": "مُواءَمَةُ الكَلِمَات",
    "Chinese": "词对齐",
    "French": "alignement de mots",
    "Japanese": "単語アライメント",
    "Russian": "выравнивание слов"
  },
  {
    "English": "word dropout",
    "context": "1: 7 We also apply <mark>word dropout</mark> (Iyyer et al., 2015) to the input spans, removing words from the vector average computation in Equation 1 with probability 0.5.<br>2: In addition, to in-crease model generalization and simulate an outof-vocabulary setting, a <mark>word dropout</mark> is utilized with the utterance encoder by randomly masking a small amount of input tokens, similar to Bowman et al. (2016).<br>",
    "Arabic": "حذف الكلمات",
    "Chinese": "词丢弃",
    "French": "omission de mots",
    "Japanese": "単語ドロップアウト",
    "Russian": "выпадение слов"
  },
  {
    "English": "word embedding",
    "context": "1: For RNNSearch model, the dimension of <mark>word embedding</mark> and hidden layer is 512, and the beam size in testing is 10. All parameters are initialized by the uniform distribution over [−0.1, 0.1]. The mini-batch stochastic gradient descent (SGD) algorithm is employed to train the model parameters with batch size setting to 80.<br>2: However, it is not clear that this holds up in light of the fact that we find very little difference in performance between count models and word2vec, or previous work arguing that <mark>word embedding</mark> models perform an implicit matrix factorization (Levy and Goldberg, 2014).<br>",
    "Arabic": "ترميز الكلمات",
    "Chinese": "词嵌入",
    "French": "plongement de mots",
    "Japanese": "ワード埋め込み",
    "Russian": "векторное представление слов"
  },
  {
    "English": "word embedding model",
    "context": "1: As intelligence systems start playing important roles in our daily life, ethics in artificial intelligence research has attracted significant interest. It is known that big-data technologies sometimes inadvertently worsen discrimination due to implicit biases in data (Podesta et al., 2014). Such issues have been demonstrated in various learning systems , including online advertisement systems ( Sweeney , 2013 ) , <mark>word embedding models</mark> ( Bolukbasi et al. , 2016 ; Caliskan et al. , 2017 ) , online news ( Ross and Carter , 2011 ) , web search ( Kay et al. , 2015 ) , and credit score ( Hardt et al.<br>",
    "Arabic": "نموذج تضمين الكلمات",
    "Chinese": "词嵌入模型",
    "French": "modèle de plongements lexicaux",
    "Japanese": "ワード埋め込みモデル",
    "Russian": "модель векторного представления слов"
  },
  {
    "English": "word mover's distance",
    "context": "1: Quantitative Model Performance Metrics. We evaluate our models using NLP and machine translation metrics , including BLUE ( Papineni et al. , 2002 ; Lin and Och , 2004 ) , Perplexity , Relation Generation ( Wiseman et al. , 2017 ) , ROUGE ( Lin , 2004 ) , Word Mover 's Distance ( WMD ) , and Translation Edit Rate ( TER ) ( Snover et<br>",
    "Arabic": "مسافة محرك الكلمات",
    "Chinese": "词移距离",
    "French": "distance de déplacement de mots",
    "Japanese": "ワードムーバー距離",
    "Russian": "расстояние перемещения слова"
  },
  {
    "English": "word representation",
    "context": "1: A sentence of length T is x 1:T , where each x i ∈ V , and the <mark>word representations</mark> of the model being probed are h 1:T , where h i ∈ R d .<br>2: In the previous section, we established the positive role that morphological awareness played in building continuousspace language models that better predict unseen text. Here we focus on the quality of the <mark>word representations</mark> learnt in the process.<br>",
    "Arabic": "تمثيل الكلمات",
    "Chinese": "词表示",
    "French": "représentation de mots",
    "Japanese": "単語表現",
    "Russian": "представление слова"
  },
  {
    "English": "word segmentation",
    "context": "1: We next consider the Chinese-English setting. The translation performance using our <mark>word segmentation</mark> technique is shown in Table 2.<br>2: The overall segmentation performance in Xitsonga is considerably worse than that of English, consistent with prior evidence that <mark>word segmentation</mark> in the Zerospeech 2015 Xitsonga partition is harder than English (e.g. Kamper et al., 2017a).<br>",
    "Arabic": "تجزئة الكلمة",
    "Chinese": "词语分割",
    "French": "segmentation des mots",
    "Japanese": "単語分割",
    "Russian": "сегментация слов"
  },
  {
    "English": "word sense disambiguation",
    "context": "1: 12 This serves as a crude form of <mark>word sense disambiguation</mark> (Wilks and Stevenson, 1998): for example, it would distinguish the different usages of \"love\" in \"I love this movie\" (indicating sentiment orientation) versus \"This is a love story\" (neutral with respect to sentiment).<br>2: named entity recognition (Collins and Singer, 1999), <mark>word sense disambiguation</mark> (Mihalcea, 2004) and parsing (McClosky et al., 2006). Successful applications are also known from computer vision, of which  gives an overview. For task-oriented dialog systems,  report substantial error reductions using self-learning to bootstrap new features.<br>",
    "Arabic": "تمييز معنى الكلمة",
    "Chinese": "词义消歧",
    "French": "désambiguïsation du sens des mots",
    "Japanese": "単語意味の曖昧さ解消",
    "Russian": "разрешение неоднозначности слова"
  },
  {
    "English": "word similarity",
    "context": "1: In order to better understand the effect of the proposed post-processing in the two similarity axes introduced in Section 1, we adopt the widely used word analogy and <mark>word similarity</mark> tasks, which offer specific benchmarks for semantics/syntax and similarity/relatedness, respectively.<br>2: (2016) analyze the correlation between results on <mark>word similarity</mark> benchmarks and sequence labeling tasks, and con-clude that most intrinsic evaluations are poor predictors of downstream performance.<br>",
    "Arabic": "تشابه الكلمات",
    "Chinese": "词语相似度",
    "French": "similarité des mots",
    "Japanese": "単語の類似度",
    "Russian": "сходство слов"
  },
  {
    "English": "word surprisal",
    "context": "1: Statistical language models developed in NLP research have been of paramount importance in the evolution of inferential theories of language comprehension. Indeed, language models are usually trained to predict the upcoming word in a corpus of naturalistic text, and thus define a conditional probability distribution that can be employed to compute <mark>word surprisal</mark>. Modern computationallyderived estimates of word predictability have been shown to perform on par ( Shain et al. , 2022 ) or even better ( Hofmann et al. , 2022 ; Michaelov et al. , 2022 ) than predictability estimates obtained with expensive human annotation ( although they fail to account for the processing demands of some specific linguistic patterns , see Arehalli et al.<br>",
    "Arabic": "درجة تفاجؤ الكلمة",
    "Chinese": "词惊喜值",
    "French": "surprisal de mot",
    "Japanese": "単語驚き",
    "Russian": "\"слово сюрприз\""
  },
  {
    "English": "word token",
    "context": "1: An emerging body of work investigates this question through probes, supervised models trained to Each <mark>word token</mark> is assigned its type's output, regardless of context (middle, bottom.) Control tasks have the same input and output space as a linguistic task (e.g., parts-of-speech) but can only be learned if the probe memorizes the mapping.<br>2: No correction was needed in the case of model annotations as, during fine-tuning, we propagated the label of the first <mark>word token</mark> to the rest; hence, the labels were always consistent for all <mark>word token</mark>s.<br>",
    "Arabic": "رمز الكلمة",
    "Chinese": "词标记",
    "French": "jeton de mot",
    "Japanese": "単語トークン",
    "Russian": "токен слова"
  },
  {
    "English": "word vector",
    "context": "1: where e(y t ) is the concatenation of the <mark>word vector</mark> w(y t ) and the previous knowledge triple vector k j from which the previous word (y t ) is selected, c t is the context vector as used in Eq.<br>2: For all models, we use the dev set and crossvalidate over regularization of the weights, <mark>word vector</mark> size as well as learning rate and minibatch size for AdaGrad. Optimal performance for all models was achieved at <mark>word vector</mark> sizes between 25 and 35 dimensions and batch sizes between 20 and 30.<br>",
    "Arabic": "متجه الكلمة",
    "Chinese": "词向量",
    "French": "vecteur de mots",
    "Japanese": "単語ベクトル",
    "Russian": "вектор слова"
  },
  {
    "English": "word vector representation",
    "context": "1: We evaluate the quality of our <mark>word vector representations</mark> on tasks that test how well they capture both semantic and syntactic aspects of the representations along with an extrinsic sentiment analysis task. Word Similarity. We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity.<br>2: Finally, we demonstrate that the model can successfully leverage <mark>word vector representations</mark>, in contrast to the baselines.<br>",
    "Arabic": "تمثيل الكلمات بالمتجهات",
    "Chinese": "词向量表示",
    "French": "représentation vectorielle des mots",
    "Japanese": "単語ベクトル表現",
    "Russian": "векторное представление слов"
  },
  {
    "English": "word-align corpus",
    "context": "1: Automatic induction of the grammar rules as described above from training data (which consists of pairs of λ-expressions and natural language sentences) is a challenging task. Current state-of-theart string-based translation systems (Koehn et al., 2003;Chiang, 2005;Galley and Manning, 2010) typically begin with a <mark>word-aligned corpus</mark> to construct phrasal correspondences.<br>",
    "Arabic": "مجموعة محاذاة الكلمات",
    "Chinese": "词对齐语料库",
    "French": "corpus aligné par mot",
    "Japanese": "単語アラインメントコーパス",
    "Russian": "корпус с выравниванием слов"
  },
  {
    "English": "word-level",
    "context": "1: We introduce OpenKiwi, a PyTorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of <mark>word-level</mark> and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015-18 quality estimation campaigns.<br>2: We train a computational model of phonetic learning, which has no access to phonology, on either one or two languages. We first show that the model exhibits predictable behaviors on phone-level and <mark>word-level</mark> discrimination tasks.<br>",
    "Arabic": "على مستوى الكلمة",
    "Chinese": "单词级别",
    "French": "au niveau des mots",
    "Japanese": "単語レベル",
    "Russian": "на уровне слов"
  },
  {
    "English": "word-level vocabulary",
    "context": "1: For entity masking, we revert to the default Flickr30K splits and perform the model evaluation on test2016, since test2017 is not annotated for entities. We use <mark>word-level vocabularies</mark> of 9,951 English and 11,216 French words. We use Moses (Koehn et al., 2007) scripts to lowercase, normalize and tokenize the sentences with hyphen splitting.<br>2: Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is proposed to get sub<mark>word-level vocabularies</mark>. The general idea is to merge pairs of frequent character sequences to create sub-word units. Sub-word vocabularies can be regarded as a trade-off between character-level vocabularies and <mark>word-level vocabularies</mark>.<br>",
    "Arabic": "المفردات على مستوى الكلمة",
    "Chinese": "词级词汇表",
    "French": "vocabulaire au niveau des mots",
    "Japanese": "単語レベルの語彙",
    "Russian": "словарь на уровне слов"
  },
  {
    "English": "word2vec embedding",
    "context": "1: Unknown words are randomly initialized to the same size as the <mark>word2vec embeddings</mark>. In early tests on the development datasets, we found that our neural networks would consistently perform better when fixing the word embeddings. All neural network performance reported in this paper use fixed embeddings.<br>2: The use of lexical semantic information in training word vectors has been limited. Recently, word similarity knowledge (Yu and Dredze, 2014;Fried and Duh, 2014) and word relational knowledge (Xu et al., 2014; have been used to improve the <mark>word2vec embeddings</mark> in a joint training model similar to our regularization approach.<br>",
    "Arabic": "تضمين word2vec",
    "Chinese": "词向量嵌入",
    "French": "représentation word2vec",
    "Japanese": "word2vecエンベディング",
    "Russian": "word2vec вложения"
  },
  {
    "English": "world state",
    "context": "1: The semantics of O 3 L builds on the semantics of OL from (Levesque and Lakemeyer 2001). The starting point is the notion of a world (or <mark>world state</mark>) which is a function from the primitive sentences to {0, 1}. We let W be the set of all worlds.<br>",
    "Arabic": "حالة العالم",
    "Chinese": "世界状态",
    "French": "état du monde",
    "Japanese": "世界状態",
    "Russian": "мировое состояние"
  },
  {
    "English": "z-score",
    "context": "1: On the other hand, Z T e ciently and accurately provides answers to time range queries by exploiting the preprocessed results.<br>",
    "Arabic": "درجة المعيارية",
    "Chinese": "标准分数",
    "French": "score z",
    "Japanese": "Zスコア",
    "Russian": "Значение Z"
  },
  {
    "English": "zero-one loss",
    "context": "1: The hinge loss is commonly used as a convex upper bound surrogate for the <mark>zero-one loss</mark>. However, in the general case no further approximation guarantees can be provided. For the DCC problem it in fact turns out that replacing zeroone loss with hinge loss results in a factor 2 approximation, as stated next. Proof.<br>2: Second, we prove the case that is not the <mark>zero-one loss</mark>. We use the notation 0−1 as the <mark>zero-one loss</mark>. According the definition of loss introduced in Section 2, we know that there exists a constant M > 0 such that for any y 1 , y 2 ∈ Y all , \n<br>",
    "Arabic": "خسارة صفر واحد",
    "Chinese": "0-1损失",
    "French": "perte zéro-un",
    "Japanese": "ゼロワン損失",
    "Russian": "потеря ноль-единица"
  },
  {
    "English": "zero-shot classification",
    "context": "1: This enhanced context aims to provide the model with more background information about the attacks, which may guide the model to ignore some typo-based or distraction-based perturbations. Evaluation setup. In this section, we first evaluate the model robustness in the <mark>zero-shot classification</mark> setting on AdvGLUE given different prompt templates.<br>2: For instance, OpenAI's CLIP models [58] achieved large gains in <mark>zero-shot classification</mark> on ImageNet [65], improving from the prior top-1 accuracy of 11.5% [41] to 76.2%. In addition, CLIP achieved unprecedented performance gains on multiple challenging distribution shifts [3,23,61,70,78,82].<br>",
    "Arabic": "تصنيف صفر طلقة",
    "Chinese": "零样本分类",
    "French": "Classification sans entraînement",
    "Japanese": "ゼロショット分類",
    "Russian": "классификация без предварительной подготовки"
  },
  {
    "English": "zero-shot cross-lingual setting",
    "context": "1: Empirically, we show the cross-lingual capability of the best performing model (XLM-R LARGE ) in the <mark>zero-shot cross-lingual setting</mark> for sentiment analysis. The heatmap is shown in Figure 2.<br>",
    "Arabic": "الإعداد عبر اللغات بدون تدريب",
    "Chinese": "零样本跨语言设置",
    "French": "réglage multilingue zéro-shot",
    "Japanese": "ゼロショット言語横断設定",
    "Russian": "нулевая межъязыковая настройка"
  },
  {
    "English": "zero-shot generalization",
    "context": "1: We provide further analysis beyond these 4 tasks in the Appendix (Sec. G.4). MINECLIP shows good <mark>zero-shot generalization</mark> to significant visual distribution shift. We evaluate the learned policy without finetuning on a combination of unseen weather, lighting conditions, and terrains -27 scenarios in total.<br>2: 12). Regardless, VAPORS demonstrates promising signs of <mark>zero-shot generalization</mark>.<br>",
    "Arabic": "التعميم الصفري",
    "Chinese": "零样本泛化",
    "French": "généralisation zéro-shot",
    "Japanese": "ゼロショット汎化",
    "Russian": "обобщение с нулевым выстрелом"
  },
  {
    "English": "zero-shot learning",
    "context": "1: Such task descriptors have been used extensively for <mark>zero-shot learning</mark> [Palatucci et al., 2009;Socher et al., 2013]. Formally, we assume that each task Z (t) has an associated descriptor m (t) that is given to the learner upon first presentation of the task.<br>2: zero-shot '' learning , where no demonstrations are allowed and only an instruction in natural language is given to the model . GPT-3 could also in principle be evaluated in the traditional fine-tuning setting, but we leave this to future work.<br>",
    "Arabic": "التعلم الصفري",
    "Chinese": "零样本学习",
    "French": "\"apprentissage sans tir\"",
    "Japanese": "ゼロショット学習",
    "Russian": "обучение с нулевым выстрелом"
  },
  {
    "English": "zero-shot prediction",
    "context": "1: These tasks are meant to teach machine learning models to perform various tasks such as object recognition, visual relationship understanding, text-image grounding, and so on by following instructions so that they can perform <mark>zero-shot prediction</mark> on unseen tasks. To build MULTIINSTRUCT , we first collect 34 tasks from the existing studies in visual and multimodal learning , covering Visual Question Answering ( Goyal et al. , 2017 ; Krishna et al. , 2017 ; Zhu et al. , 2016 ; Hudson and Manning , 2019 ; Singh et al. , 2019 ; Marino et al. , 2019 ) , Commonsense Reasoning ( Suhr et al. , 2017 ; Liu et al. , 2022a ; Zellers et al. , 2019 ; Xie et al. , 2019 ) , Region Understanding ( Krishna et al. , 2017 ) , Image Understanding ( Kafle and Kanan , 2017 ; Chiu et al. , 2020 ) , Grounded Generation ( Krishna et al. , 2017 ; Yu et al. , 2016 ; Lin et al. , 2014 ) , Image-Text Matching ( Lin et al. , 2014 ; Goyal et al. , 2017 ) , Grounded Matching ( Krishna et al. , 2017 ; Veit et al. , 2016 ; Yu et al. , 2016 ) , Visual Relationship ( Krishna et al. , 2017 ; Pham et al. , 2021 ) , Temporal<br>",
    "Arabic": "التنبؤ بدون تدريب",
    "Chinese": "零样本预测",
    "French": "prédiction sans exemple",
    "Japanese": "ゼロショット予測",
    "Russian": "предсказание с нулевой разметкой"
  },
  {
    "English": "zero-shot prompting",
    "context": "1: The complete list of results for <mark>zero-shot prompting</mark> using sociodemographic profiles is provided in Table 10 (hard evaluation) and Table 11 (soft evaluation). The good performance of models from the Tk-Instruct family on the Jigsaw dataset is most likely due to the dataset being present in the dataset which was used for instruction-finetuning .<br>2: Most simply, we explore <mark>zero-shot prompting</mark> with GPT-J [43] in the summarization task and 2-shot prompting with Pythia-2.8B [3] in the dialogue task.<br>",
    "Arabic": "استدعاء من دون إعطاء مثال",
    "Chinese": "零样本提示",
    "French": "sollicitation zéro-shot",
    "Japanese": "ゼロショットプロンプティング",
    "Russian": "нулевая генерация подсказок"
  },
  {
    "English": "zero-shot reasoning",
    "context": "1: We also add \"Let's think step by step\" before each answer for script generation, which is a simple but effective trick to improve <mark>zero-shot reasoning</mark> for LLMs (Kojima et al., 2022).<br>",
    "Arabic": "الاستدلال بدون تدريب",
    "Chinese": "零样本推理",
    "French": "raisonnement zero-shot",
    "Japanese": "ゼロショット推論",
    "Russian": "рассуждения с нулевым шансом"
  },
  {
    "English": "zero-shot setting",
    "context": "1: We adapt the prompts given in [73] to the prompting format of GPT models and illustrate the prompts for <mark>zero-shot setting</mark> as follows. Justice: System: You are a helpful assistant. User: Please answer with \"reasonable\" or \"unreasonable\".\\n Question: Would most people believe this reasonable or unreasonable to say?<br>2: The severity of immoral behaviors has little impact on the performance of GPT-3.5, while improving the severity would improve the recognition accuracy of GPT-4 (Figure 25 in Section 9.4). • Fairness. We find that : 1 ) although GPT-4 is more accurate than GPT-3.5 given demographically balanced test data , GPT-4 also achieves higher unfairness scores given unbalanced test data , indicating an accuracy-fairness tradeoff ( Table 30,31,33 in Section 10 ) ; 2 ) in the <mark>zero-shot setting</mark> , both GPT-3.5 and GPT-4 have large performance gaps across test groups with different base rate<br>",
    "Arabic": "إعداد الصفر",
    "Chinese": "零样本设置",
    "French": "paramètre de transfert",
    "Japanese": "ゼロショット設定",
    "Russian": "нулевая настройка"
  },
  {
    "English": "zero-shot transfer",
    "context": "1: Second, we find consistently strong quantitative and qualitative results on a variety of downstream tasks under a <mark>zero-shot transfer</mark> protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction.<br>2: Nevertheless, SAM performs well compared to pioneering deep learning methods such as HED [108] (also trained on BSDS500) and significantly better than prior, though admittedly outdated, <mark>zero-shot transfer</mark> methods.<br>",
    "Arabic": "- Candidate term translation 2: \"- تحويل بدون تدريب\"",
    "Chinese": "零次迁移",
    "French": "transfert sans apprentissage",
    "Japanese": "ゼロショット転移",
    "Russian": "Перенос без предварительного обучения"
  },
  {
    "English": "zero-shot transfer learning",
    "context": "1: (10) Since the estimate given by s (tnew ) also serves as the coefficients over the latent policy space L, we can immediately predict a policy for the new task as:✓ (tnew ) = Ls (tnew ) . This <mark>zero-shot transfer learning</mark> procedure is given as Algorithm 2.<br>",
    "Arabic": "التعلم النقلي بدون تدريب",
    "Chinese": "零样本迁移学习",
    "French": "apprentissage par transfert zéro",
    "Japanese": "ゼロショット転移学習",
    "Russian": "обучение с нулевым переносом"
  },
  {
    "English": "zipf",
    "context": "1: This is the class that we are most interested in , since most of the data of interest are either inherently integer-valued , or rounded-off to integers : salaries and dollar amounts are down to pennies , products sell integer counts ( `` 1 loaf of bread '' ) , and so on : Distribution in this class include <mark>Zipf</mark> and its variations ,<br>2: Our goal in this paper is to find a more general model. We want a distribution that would have the following attractive properties: \n 1. it should include the \"<mark>Zipf</mark>\" and \"generalized <mark>Zipf</mark>\" as special cases; \n 2. it should fit well all the data sets that <mark>Zipf</mark> fits, and many many more; \n<br>",
    "Arabic": "قانون زيف",
    "Chinese": "齐普夫分布",
    "French": "loi de Zipf",
    "Japanese": "ジップフ",
    "Russian": "закон Ципфа"
  },
  {
    "English": "zipf distribution",
    "context": "1: For example, in Figure 1, we make the \"frequency-rank plot\" and the \"count-frequency plot\" of words in the Bible. As explained in the survey section, the Zipf (or generalized-Zipf) distribution would expect the plots to be straight lines in logarithmic-logarithmic scales. However, we observe a clear tilting in Figure 1.<br>2: The dynamic Community Guided Attachment model just defined has the following properties. • When c < b , the average node degree is n 1−log b ( c ) and the in-degrees follow a <mark>Zipf distribution</mark> with exponent Thus , the dynamic Community Guided Attachment model exhibits three qualitatively different behaviors as the parameter c varies : densification with heavy-tailed in-degrees ; then constant average degree with heavy-tailed in-degrees ; and then constant in-and out-degrees with<br>",
    "Arabic": "توزيع زيبف",
    "Chinese": "Zipf分布",
    "French": "distribution de Zipf",
    "Japanese": "ジップ分布",
    "Russian": "распределение Ципфа"
  },
  {
    "English": "zipf's law",
    "context": "1: • As we will show in next section, DGX includes <mark>Zipf's law</mark> as a special case.<br>2: The second 'law', also known as the discrete Pareto distribution [16], involves the \"count-frequency\" plot: let cf be the count of vocabulary words that appear f times in the document. The second <mark>Zipf's law</mark> states that \n cf ∝ 1/f φ (3) \n There are three observations: \n<br>",
    "Arabic": "قانون زيبف",
    "Chinese": "Zipf定律",
    "French": "Loi de Zipf",
    "Japanese": "ジプフの法則",
    "Russian": "закон Ципфа"
  }
]