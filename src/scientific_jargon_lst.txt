integration
training epochs
pre-training
COCO images
OCR-VQA
RoBERTa text encoder
generation-based classification approach
Cloud Provider
captioning
maskrcnn-benchmark
capacity
bias
misuse
generativeimage2text
Code of Conduct
training
pixels
Azure
MSRVTT-QA
post-processing
error types
MSVD-QA
qualitative analysis
generative task
performance characteristics
linear layer
evaluation benchmarks
concatenated image features
ethical concerns
Mishra et al
NPD
TextVQA
Ref 2
tokenization
CLIP
capabilities
text data
Software Type
arXiv
ST-VQA
carbon intensity
model
GPUs
associated text description
language generation
aspect ratio
image-text paired data
license
few-shot
performance metrics
caption prefix
trademarks
auto-regressively
characteristics
resizing
performance
dataset
Reference 3
hyperparameters
generative nature
Code snippets
microsoft/git-base-textvqa
VQA benchmarks
Azure Machine Learning
potential vulnerabilities
VizWiz-VQA
Generative Image-to-Text (GIT)
training data
dataset size
occluded
object detectors
cosine decay
training times
fine-tuning dataset
sociotechnical impacts
Performance
explainability
visual question answering (VQA)
A100
VQAv2
PyTorch
iterations
energy consumption
legal
interpretability
project
state-of-the-art performance
visual question answering
compute resources
video QA
fluent sentences
language modeling loss
discriminative approach
GIT models
preprocessing
contrastive pre-trained model
robustness
projected
Ref
poison the output
analysis
COCO
pre-trained model
fine-tuning
reference
large-scale data
data
nvidia-docker
generate
unreliable
checkpoint sizes
evaluating
visual content
hardware
model card
throughput
Finetuned
model explainability
text decoder
'inference'
Transformers
training time
layernorm layer
Generative Image-to-text Transformer
GPU
pre-trained
learning rate
candidate answers
model size
exploit
demographic groups
optical character recognition (OCR)
fine-tuned
normalized performance difference (NPD)
ViT
question answering
METEOR
contrastive
downstream tasks
Microsoft/git-base-textvqa
quantitative performance metrics
large-scale
recognizing
visual knowledge
concatenated
Training Hyperparameters
evaluated
datasets
metrics
EOS
skin tone biases
sociological
subgroups
language modeling objective
microsoft/git-base-textvqa model
data domains
image captioning
noisy data
pipeline
trained
larger systems
failure modes
interdisciplinary
Robustness
manipulation
gender
BLEU
encoder
visual concepts
model components
VirTex
natural language
handwritten text
dimensions
vision-language
base model
Bias
large-scale image-text pairs
text patterns
carbon emitted
Open Source
extracting features
feature map
Microsoft
ground truth
image encoder
CIDEr
TextVQA dataset
JPEG format
scaling up
initialization
'prefix'
embedded
transformer module
opencode@microsoft
8B data
classification approach
flattened
performance trends
training images
'test_git_inference_single_image'
concatenating
input image
token
VizWiz-QA
potential misunderstandings
explainable AI
classification-based approaches
ethical
carbon footprint
logos
Training Data
decoder
malicious actors
Goyal et al
scene text
image-text pairs
TextVQA evaluation
object recognition
error tradeoffs
benchmarks
contrastive pretraining
visual context
vocabulary knowledge
BOS
technical limitations
discriminative
tradeoffs
DeepSpeed
OSCAR
TextCaps
terabytes
tokenized
Reference 5
classifying images
privacy
discriminative models
leveraging
EOS token
textual context
category likelihood
preprocessed
evaluation metrics
 fine-tuning
pre-training data
discriminative approaches
SPICE
toxic language
fairness
skin type
quantitative results
accuracy
inferring
generative
git-base-textvqa
text embeddings
out-of-domain data
electricity
generative approach
sizes
contrastive pretraining model
ecosystems
LM task
optimizer
Azure Blob Storage
ipc
py38pt19u20cu11
nonsensical
vision-language tasks
generating captions
proactively
architecture
misleading
zero-shot
performance difference
VQA
image features
carbon emission data
model scaling
 VQA datasets
transformer
pre-training data sources
captions
sampled frames
pre-defined answer candidates
quality
video representation
batch size
memory
benchmark
auto-regressive
image types
disaggregated evaluation
discriminative methods
surveillance
A100 GPUs
pre-defined candidate answers
language modeling
Visual Question Answering (VQA)
evaluation datasets
performance disparities
transformer architecture
image classification
