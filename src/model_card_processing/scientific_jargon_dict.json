{
    "Chinese": {
        "biases": "偏见",
        "pre-trained": "预先训练过",
        "Network size": "网络的大小",
        "model outputs": "模型输出",
        "probability": "概率",
        "ALBERT models": "阿尔伯特模型",
        "post-processing": "后处理",
        "length": "长度",
        "sensitive use cases": "敏感的使用案例",
        "feed-forward/filter size": "前进/过滤器的大小",
        "training": "培训",
        "vocabulary size": "词汇量",
        "SOP loss": "SOP损失",
        "tokens": "令牌",
        "GitHub": "在GitHub",
        "author": "作者",
        "self-supervised loss": "自我监督的损失",
        "scientific text": "科学文本",
        "downstream": "下游的",
        "Reading Comprehension": "阅读理解能力",
        "Attention heads": "注意头部",
        "ALBERT project": "ALBERT 项目",
        "analyzing personal data": "分析个人数据",
        "Masked inputs": "隐藏的输入",
        "SQuAD v1": "SQuAD v1",
        "model": "模型",
        "filter size": "<unk>滤器的大小",
        "fine-tune": "精度调节",
        "memory consumption": "记忆的消耗",
        "input length": "输入长度",
        "model memorizing": "记忆模型",
        "BOOKCORPUS": "书籍库",
        "state-of-the-art performance": "先进的性能",
        "ALBERT model": "阿尔伯特模型",
        "parameter-reduction techniques": "参数减少技术",
        "nuances": "不同的色彩",
        "diminishing returns": "减少的回报",
        "sentence order prediction (SOP)": "句子顺序预测 (SOP)",
        "target task": "目标任务",
        "Wikipedia": "维基百科",
        "larger structure": "更大的结构",
        "CLS": "CLS",
        "gender": "性別",
        "General Language Understanding Evaluation (GLUE) benchmark": "一般语言理解评估 (GLUE) 基准",
        "Models": "模型",
        "Probability": "概率",
        "pretrained": "预训练过",
        "ALBERT-xxlarge": "ALBERT-xx大",
        "reading comprehension": "阅读理解",
        "RACE benchmarks": "RACE 参考标准",
        "self-supervised learning": "自我监督的学习",
        "input data": "输入数据",
        "discourse-level coherence": "话语层面的连贯性",
        "pre-trained model": "预先训练的模型",
        "multi-sentence encoding tasks": "多句编码任务",
        "embedding matrix": "嵌入矩阵",
        "arXiv": "arXiv",
        "memory constraints": "记忆限制",
        "Sentence Order Prediction": "句子顺序预测",
        "cloud provider": "云提供商",
        "Finetuned": "精调的",
        "Hidden size": "隐藏的尺寸",
        "tokenized": "标记化",
        "dimensional embedding space": "维度嵌入空间",
        "repo": "收回交易",
        "performance decline": "性能下降",
        "block attention": "阻碍注意力",
        "training data": "培训数据",
        "parameter-efficiency techniques": "参数效率技术",
        "vocabulary embedding size": "词汇嵌入大小",
        "sentence classification": "句子分类",
        "efficiency": "效率",
        "model size": "模型大小",
        "fine-grained": "细粒的",
        "computational expense": "计算费用",
        "hardware": "硬件",
        "hidden space": "隐藏的空间",
        "language representation model": "语言表示模型",
        "multi-sentence encoding": "多句编码",
        "postprocessing": "后处理",
        "Transformer": "变压器",
        "self-supervised": "自我监督的",
        "inter-sentence coherence": "句子之间的连贯性",
        "dropout": "辍学",
        "architecture": "建筑",
        "feed-forward": "前进式供给",
        "preprocessing": "预处理",
        "model distillation": "模型蒸<unk>",
        "ALBERT configurations": "ALBERT 配置",
        "corpora": "机构",
        "hidden sizes": "隐藏的尺寸",
        "learned representations": "学到的表述",
        "General Language Understanding Evaluation": "一般语言理解评估",
        "Training steps": "培训步骤",
        "multi-sentence inputs": "多句输入",
        "transformer encoder": "变压器编码器",
        "question answering": "回答问题",
        "memory limitations": "记忆的限制",
        "inter-sentence coherence modeling": "句子间连贯性建模",
        "albert-large-v2": "阿尔伯特-大-v2",
        "SentencePiece": "句子片",
        "Diminishing returns": "收益下降",
        "models": "模型",
        "pre-training tasks": "培训前的任务",
        "training details": "培训细节",
        "downstream performance": "下游性能",
        "attention heads": "注意力头部",
        "deployment": "部署",
        "parameter sharing": "参数共享",
        "pretraining": "预训练",
        "GELU": "GELU",
        "Dropout": "辍学",
        "discourse-level coherence properties": "话语层面的连贯性特性",
        "network parameters": "网络参数",
        "training speed": "训练速度",
        "parameter efficiency": "参数效率",
        "benchmarks": "参考标准",
        "pipeline": "管道",
        "sociotechnical limitations": "社会技术限制",
        "next sentence prediction": "下一句预测",
        "parameters": "参数",
        "ALBERT": "阿尔伯特",
        "repository": "存储库",
        "pretrained models": "预先训练的模型",
        "legal": "法律上的",
        "pretraining data": "预训练数据",
        "MLM": "跨国公司",
        "GLUE": "粘合剂",
        "natural language understanding": "自然语言理解",
        "comparisons": "比较",
        "Radu Soricut": "拉杜·索里库特",
        "computationally expensive": "计算成本高昂",
        "hidden size": "隐藏的大小",
        "GLUE benchmark": "GLUE 参考值",
        "pre-existing model": "现有的模型",
        "corpus": "语料",
        "n-gram masking": "n-gram 掩盖",
        "encoder": "编码器",
        "rewriting": "重新写作",
        "learning rate": "学习率",
        "downstream tasks": "下游任务",
        "TPUs": "TPU 系统",
        "factorized embedding parameterization": "分数化嵌入参数化",
        "Factorizes": "分解为因子",
        "private information": "私人信息",
        "evaluating": "评估",
        "evaluation": "评估",
        "NLP benchmarks": "NLP 基准",
        "More Information Needed": "需要更多的信息",
        "embedding parameters": "嵌入参数",
        "self-supervised training": "自我监督的培训",
        "interpretability": "解释能力",
        "Masked Language Model (MLM)": "掩盖语言模型 (MLM)",
        "Input sequences": "输入序列",
        "randomly generated": "随机生成的",
        "Wikipedia corpora": "维基百科的体",
        "SOP": "标准操作程序",
        "foreseeable harms": "可预见的伤害",
        "informing decisions": "通知决定",
        "embedding dimension": "嵌入维度",
        "Tokenized": "标记化",
        "labeled data": "标记的数据",
        "XLNet": "XLNet",
        "Sentence-order prediction": "句子顺序预测",
        "pooled output": "聚合的产出",
        "steps": "步骤",
        "model's performance": "模型性能",
        "LAMB optimizer": "LAMB优化器",
        "Next Sentence Prediction (NSP)": "下一句预测 (NSP)",
        "interpretable": "可以解释的",
        "N-gram": "N-克",
        "benchmark": "参考标准",
        "development sets": "发展套件",
        "ALBERT architecture": "ALBERT 架构",
        "embedding": "嵌入",
        "sparse attention": "稀疏的注意力",
        "input sequences": "输入序列",
        "performance increases": "性能提高",
        "task-specific": "具体任务",
        "Cross-layer parameter sharing": "跨层参数共享",
        "Accuracy": "精度",
        "hidden states": "隐藏的状态",
        "performance": "性能",
        "SEP": "SEP",
        "performance gains": "绩效收益",
        "Piyush Sharma": "皮尤什·夏尔马",
        "fine-tuned": "精度调节",
        "Google Research": "谷歌研究",
        "Sebastian Goodman": "塞巴斯蒂安·古德曼",
        " SQuAD ": "队伍",
        "SOP task": "标准操作程序任务",
        "Cloud": "云层",
        "RACE dataset": "RACE数据集",
        "layers": "层次",
        "batch normalization": "批次正常化",
        "SQuAD": "队伍",
        "next sentence prediction (NSP)": "下一句预测 (NSP)",
        "language understanding component": "语言理解部分",
        "Computational efficiency": "计算效率",
        "distillation": "蒸<unk>",
        "3-gram": "三克",
        "age": "年龄",
        "RACE": "种族",
        "medical": "医疗",
        "inference latency": "推断延迟",
        "losses": "损失",
        "task-specific output layer": "特定任务的输出层",
        "preprocess": "预处理",
        "n-gram": "n-克",
        "computational cost": "计算成本",
        "false negatives": "错误的负数",
        "Feed-forward": "前进式",
        "variant": "变体",
        "State-of-the-art performance": "最先进的性能",
        "Stanford Question Answering Dataset": "斯坦福问题解答数据集",
        "Examinations": "考试",
        "GLUE score": "GLUE分数",
        "masking probability distribution": "掩盖概率分布",
        "text classification": "文字分类",
        "highly specialized domains": "高度专业化领域",
        "ALBERT-large-v2 model": "ALBERT-大v2模型",
        "explainability": "可解释性",
        "Training Data": "培训数据",
        "model architecture": "模型架构",
        "BERT": "BERT",
        "resizing": "调整大小",
        "Reference": "参考资料",
        "ALBERT-large": "ALBERT-大",
        "English Wikipedia": "英语维基百科",
        "datasets": "数据集",
        "coherence": "连贯性",
        "analysis": "分析",
        "one-hot vectors": "一个热向量",
        "Computational cost": "计算成本",
        "misc": "各种各样的",
        "inference": "推断",
        "common sense understanding": "常识的理解",
        "uncompressed text": "未压缩的文本",
        "sociotechnical impacts": "社会技术影响",
        "Early stopping": "提前停止",
        "Σ (sigma)": "(西格玛)",
        "human-level reasoning abilities": "人类水平的推理能力",
        "performance metrics": "绩效指标",
        "Zhenzhong Lan": "郑中兰",
        "language representations": "语言表示",
        "pre-trained language understanding capabilities": "预先训练的语言理解能力",
        "API": "API 应用程序",
        "sentence ordering pretraining loss": "命令训练前损失的判决",
        "negative impacts": "负面影响",
        "fairness": "公平性",
        "distilled": "蒸<unk>的",
        "ALBERT-large configurations": "ALBERT-大配置",
        "Sentence Order Prediction (SOP)": "判决顺序预测 (SOP)",
        "raw text": "原文",
        "training time": "训练时间",
        "development set scores": "发展设置分数",
        "pre-training": "预培训",
        "mask": "掩面",
        "sentence ordering prediction (SOP)": "句子排序预测 (SOP)",
        "hyperparameters": "超级参数",
        "Mingda Chen": "明达·陈",
        "natural language processing": "自然语言处理",
        "metrics": "指标",
        "TPU": "TPU",
        "Cloud TPU V3": "云TPU V3",
        "encoder layers": "编码层",
        "nonlinearities": "非线性",
        "bias": "偏见",
        "Fatorized embedding parameterization": "因素化嵌入参数化",
        "ALBERT-large-v2": "ALBERT-大v2",
        "computational efficiency": "计算效率",
        "structure": "结构",
        "state-of-the-art results": "最新的结果",
        "Kevin Gimpel": "凯文·吉姆佩尔",
        "number of layers": "层的数量",
        "representations": "代表",
        "BERT-large": "BERT-大型的",
        "Stanford Question Answering Dataset (SQuAD)": "斯坦福问题回答数据集 (SQuAD)",
        "performance returns": "绩效回报",
        "cross-layer parameter sharing": "跨层参数共享",
        "pre-training task": "培训前的任务",
        "SQuAD 2": "第二队",
        "architecture choices": "建筑的选择",
        "fine-tuning": "精度调节",
        "MLM targets": "多边市场的目标",
        "model configuration": "模型配置",
        "albert-large-v2 model": "阿尔伯特-拉奇-v2模型",
        "race": "种族",
        "subpopulations": "亚种群"
    },
    "Russian": {
        "biases": "предвзятость",
        "pre-trained": "предварительно обученные",
        "Network size": "Размер сети",
        "model outputs": "выходные данные модели",
        "probability": "Вероятность",
        "ALBERT models": "Модели Альберта",
        "post-processing": "Послеработка",
        "length": "длина",
        "sensitive use cases": "чувствительные случаи использования",
        "feed-forward/filter size": "Перенаправление/размер фильтра",
        "training": "обучение",
        "vocabulary size": "размер словарного запаса",
        "SOP loss": "Потеря SOP",
        "tokens": "жетоны",
        "GitHub": "GitHub (англ.)",
        "author": "Автор",
        "self-supervised loss": "самоконтролируемые потери",
        "scientific text": "научный текст",
        "downstream": "вниз по течению",
        "Reading Comprehension": "Понимание чтения",
        "Attention heads": "Внимание, главы.",
        "ALBERT project": "Проект ALBERT",
        "analyzing personal data": "анализ персональных данных",
        "Masked inputs": "Маскированные входы",
        "SQuAD v1": "SQuAD v1",
        "model": "модель",
        "filter size": "размер фильтра",
        "fine-tune": "тонкая настройка",
        "memory consumption": "потребление памяти",
        "input length": "входная длина",
        "model memorizing": "запоминание модели",
        "BOOKCORPUS": "Книжный корпус",
        "state-of-the-art performance": "современные характеристики",
        "ALBERT model": "Модель АЛБЕРТА",
        "parameter-reduction techniques": "методы снижения параметров",
        "nuances": "нюансы",
        "diminishing returns": "уменьшающиеся доходы",
        "sentence order prediction (SOP)": "Предсказание порядка предложений (SOP)",
        "target task": "целевая задача",
        "Wikipedia": "Википедия",
        "larger structure": "более крупная структура",
        "CLS": "CLS",
        "gender": "пол",
        "General Language Understanding Evaluation (GLUE) benchmark": "Базовый показатель оценки общего понимания языка (GLUE)",
        "Models": "Модели",
        "Probability": "Вероятность",
        "pretrained": "предварительно обученный",
        "ALBERT-xxlarge": "Альберт-Хкс-Большой",
        "reading comprehension": "понимание чтения",
        "RACE benchmarks": "Исходные показатели RACE",
        "self-supervised learning": "самонаблюдаемое обучение",
        "input data": "входные данные",
        "discourse-level coherence": "согласованность на уровне дискурса",
        "pre-trained model": "предварительно обученная модель",
        "multi-sentence encoding tasks": "задачи кодирования нескольких предложений",
        "embedding matrix": "матрица встраивания",
        "arXiv": "Арксив",
        "memory constraints": "ограничения памяти",
        "Sentence Order Prediction": "Предсказание порядка приговора",
        "cloud provider": "поставщик облачных услуг",
        "Finetuned": "Тонко настроенный",
        "Hidden size": "Скрытый размер",
        "tokenized": "Токенизированный",
        "dimensional embedding space": "пространство размеренного встраивания",
        "repo": "Репо",
        "performance decline": "снижение производительности",
        "block attention": "блокировать внимание",
        "training data": "данные о подготовке",
        "parameter-efficiency techniques": "Методы эффективности параметров",
        "vocabulary embedding size": "размер встраивания словарного запаса",
        "sentence classification": "классификация предложений",
        "efficiency": "эффективность",
        "model size": "Размер модели",
        "fine-grained": "мелкозернистые",
        "computational expense": "расход на вычисления",
        "hardware": "аппаратное обеспечение",
        "hidden space": "Скрытое пространство",
        "language representation model": "модель языкового представления",
        "multi-sentence encoding": "кодирование нескольких предложений",
        "postprocessing": "Послепроцессуальная обработка",
        "Transformer": "Трансформатор",
        "self-supervised": "под самонадзором",
        "inter-sentence coherence": "согласованность между предложениями",
        "dropout": "Отсев.",
        "architecture": "архитектура",
        "feed-forward": "перенаправление",
        "preprocessing": "предварительная обработка",
        "model distillation": "модель дистилляции",
        "ALBERT configurations": "Конфигурации ALBERT",
        "corpora": "Корпусы",
        "hidden sizes": "скрытые размеры",
        "learned representations": "изученные представления",
        "General Language Understanding Evaluation": "Оценка общего понимания языка",
        "Training steps": "Шаги обучения",
        "multi-sentence inputs": "многосложные входы",
        "transformer encoder": "трансформаторный кодер",
        "question answering": "Отвечать на вопросы",
        "memory limitations": "ограничения памяти",
        "inter-sentence coherence modeling": "моделирование межреченной согласованности",
        "albert-large-v2": "Альберт-Ларж-В2",
        "SentencePiece": "Предложение",
        "Diminishing returns": "Уменьшение доходов",
        "models": "модели",
        "pre-training tasks": "задания по подготовке",
        "training details": "подробности обучения",
        "downstream performance": "производительность вниз по течению",
        "attention heads": "Главы внимания.",
        "deployment": "развертывание",
        "parameter sharing": "совместное использование параметров",
        "pretraining": "предварительная подготовка",
        "GELU": "GELU",
        "Dropout": "Отсев",
        "discourse-level coherence properties": "свойства согласованности на уровне дискурса",
        "network parameters": "параметры сети",
        "training speed": "скорость обучения",
        "parameter efficiency": "эффективность параметра",
        "benchmarks": "контрольные показатели",
        "pipeline": "трубопровод",
        "sociotechnical limitations": "социально-технические ограничения",
        "next sentence prediction": "предсказание следующего предложения",
        "parameters": "параметры",
        "ALBERT": "Альберт.",
        "repository": "хранилище",
        "pretrained models": "предварительно обученные модели",
        "legal": "юридическое",
        "pretraining data": "данные предварительной подготовки",
        "MLM": "МЛМ",
        "GLUE": "КЛЮК",
        "natural language understanding": "понимание естественного языка",
        "comparisons": "сопоставления",
        "Radu Soricut": "Раду Сорикут",
        "computationally expensive": "вычислительно дорогостоящий",
        "hidden size": "скрытый размер",
        "GLUE benchmark": "Исходный показатель GLUE",
        "pre-existing model": "ранее существующая модель",
        "corpus": "Корпус",
        "n-gram masking": "Маскировка n-грамм",
        "encoder": "кодер",
        "rewriting": "Переписывание",
        "learning rate": "скорость обучения",
        "downstream tasks": "Задачи на последующей стадии",
        "TPUs": "TPU",
        "factorized embedding parameterization": "факторизированная параметризация встраивания",
        "Factorizes": "Факторизация",
        "private information": "частная информация",
        "evaluating": "Оценка",
        "evaluation": "Оценка",
        "NLP benchmarks": "Исходные показатели NLP",
        "More Information Needed": "Необходимо больше информации",
        "embedding parameters": "параметры встраивания",
        "self-supervised training": "самоконтролируемое обучение",
        "interpretability": "интерпретабельность",
        "Masked Language Model (MLM)": "Модель маскированного языка (MLM)",
        "Input sequences": "Последовательности ввода",
        "randomly generated": "произвольно сгенерированные",
        "Wikipedia corpora": "Корпусы Википедии",
        "SOP": "СОП",
        "foreseeable harms": "предсказуемый вред",
        "informing decisions": "информирование решений",
        "embedding dimension": "размер встраивания",
        "Tokenized": "Токенные",
        "labeled data": "маркированные данные",
        "XLNet": "XLNet",
        "Sentence-order prediction": "Предсказание порядка предложений",
        "pooled output": "объединенная продукция",
        "steps": "шаги",
        "model's performance": "производительность модели",
        "LAMB optimizer": "Оптимизатор LAMB",
        "Next Sentence Prediction (NSP)": "Предсказание следующего предложения (NSP)",
        "interpretable": "интерпретируемый",
        "N-gram": "N-грамм",
        "benchmark": "эталонный показатель",
        "development sets": "наборы развития",
        "ALBERT architecture": "Архитектура ALBERT",
        "embedding": "встраивание",
        "sparse attention": "редкое внимание",
        "input sequences": "входные последовательности",
        "performance increases": "повышение производительности",
        "task-specific": "специфические задачи",
        "Cross-layer parameter sharing": "Совместное использование параметров между уровнями",
        "Accuracy": "Точность",
        "hidden states": "скрытые состояния",
        "performance": "производительность",
        "SEP": "SEP",
        "performance gains": "повышение производительности",
        "Piyush Sharma": "Пиюш Шарма",
        "fine-tuned": "тонко настроенный",
        "Google Research": "Исследования Google",
        "Sebastian Goodman": "Себастьян Гудман",
        " SQuAD ": "Команды",
        "SOP task": "Задача SOP",
        "Cloud": "Облако",
        "RACE dataset": "Набор данных RACE",
        "layers": "слои",
        "batch normalization": "нормализация партии",
        "SQuAD": "Команды",
        "next sentence prediction (NSP)": "предсказание следующего предложения (NSP)",
        "language understanding component": "компонент понимания языка",
        "Computational efficiency": "Вычислительная эффективность",
        "distillation": "дистилляция",
        "3-gram": "3 грамма",
        "age": "возраст",
        "RACE": "РАСА",
        "medical": "медицинский",
        "inference latency": "латентность вывода",
        "losses": "потери",
        "task-specific output layer": "уровень выхода, специфичный для задачи",
        "preprocess": "предварительная обработка",
        "n-gram": "n-грамм",
        "computational cost": "вычислительные затраты",
        "false negatives": "ложные отрицательные",
        "Feed-forward": "Передача",
        "variant": "вариант",
        "State-of-the-art performance": "Современные характеристики",
        "Stanford Question Answering Dataset": "Сбор данных ответов на вопросы Стэнфорда",
        "Examinations": "Экзамены",
        "GLUE score": "Оценка GLUE",
        "masking probability distribution": "распределение вероятности маскирования",
        "text classification": "классификация текста",
        "highly specialized domains": "высокоспециализированные области",
        "ALBERT-large-v2 model": "Модель ALBERT-large-v2",
        "explainability": "объяснимость",
        "Training Data": "Данные о подготовке",
        "model architecture": "архитектура модели",
        "BERT": "БЕРТ",
        "resizing": "Изменение размера",
        "Reference": "Ссылка",
        "ALBERT-large": "ALBERT-большой",
        "English Wikipedia": "Английская Википедия",
        "datasets": "Наборы данных",
        "coherence": "согласованность",
        "analysis": "анализ",
        "one-hot vectors": "одногорячие векторы",
        "Computational cost": "Расходы на вычисления",
        "misc": "Разные",
        "inference": "Вывод",
        "common sense understanding": "здравый смысл",
        "uncompressed text": "некомпрессированный текст",
        "sociotechnical impacts": "социально-технические последствия",
        "Early stopping": "Раннее прекращение",
        "Σ (sigma)": "(Сигма)",
        "human-level reasoning abilities": "способности рассуждения на человеческом уровне",
        "performance metrics": "показатели эффективности",
        "Zhenzhong Lan": "Чжэнчжонг Лан",
        "language representations": "языковые представления",
        "pre-trained language understanding capabilities": "предварительно обученные способности понимания языка",
        "API": "АПИ",
        "sentence ordering pretraining loss": "приговор, предписывающий потерю до обучения",
        "negative impacts": "отрицательное воздействие",
        "fairness": "справедливость",
        "distilled": "дистиллированный",
        "ALBERT-large configurations": "Конфигурации ALBERT-large",
        "Sentence Order Prediction (SOP)": "Предсказание порядка вынесения приговора (SOP)",
        "raw text": "необработанный текст",
        "training time": "время обучения",
        "development set scores": "оценки набора развития",
        "pre-training": "предварительная подготовка",
        "mask": "Маска.",
        "sentence ordering prediction (SOP)": "Предсказание порядка предложений (SOP)",
        "hyperparameters": "гиперпараметры",
        "Mingda Chen": "Мингда Чен",
        "natural language processing": "обработка естественного языка",
        "metrics": "метрики",
        "TPU": "TPU",
        "Cloud TPU V3": "Cloud TPU V3 (облачный TPU)",
        "encoder layers": "слои кодера",
        "nonlinearities": "нелинейности",
        "bias": "Предвзятость",
        "Fatorized embedding parameterization": "Факторизированная параметризация встраивания",
        "ALBERT-large-v2": "ALBERT-large-v2",
        "computational efficiency": "вычислительная эффективность",
        "structure": "структура",
        "state-of-the-art results": "последние результаты",
        "Kevin Gimpel": "Кевин Гимпель",
        "number of layers": "количество слоев",
        "representations": "представительства",
        "BERT-large": "BERT-большой",
        "Stanford Question Answering Dataset (SQuAD)": "Сборник данных Stanford Question Answering (SQuAD)",
        "performance returns": "возвраты на производительность",
        "cross-layer parameter sharing": "совместное использование параметров между уровнями",
        "pre-training task": "задача предварительной подготовки",
        "SQuAD 2": "Отряд 2",
        "architecture choices": "выбор архитектуры",
        "fine-tuning": "тонкая настройка",
        "MLM targets": "Цели MLM",
        "model configuration": "конфигурация модели",
        "albert-large-v2 model": "модель albert-large-v2",
        "race": "раса",
        "subpopulations": "субпопуляции"
    },
    "Japanese": {
        "biases": "偏見について",
        "pre-trained": "予備訓練を受けた",
        "Network size": "ネットワークのサイズ",
        "model outputs": "モデル出力",
        "probability": "確率について",
        "ALBERT models": "ALBERTモデル",
        "post-processing": "後の処理",
        "length": "長さ",
        "sensitive use cases": "敏感な使用例",
        "feed-forward/filter size": "フィードフォワード/フィルターのサイズ",
        "training": "トレーニング",
        "vocabulary size": "語の大きさ",
        "SOP loss": "SOP損失について",
        "tokens": "トークン",
        "GitHub": "GitHub (ギットハブ)",
        "author": "著者",
        "self-supervised loss": "自己監督された損失",
        "scientific text": "科学的なテキスト",
        "downstream": "下流に",
        "Reading Comprehension": "読み解き力",
        "Attention heads": "注意して",
        "ALBERT project": "ALBERTプロジェクト",
        "analyzing personal data": "個人データの分析",
        "Masked inputs": "マスクされた入力",
        "SQuAD v1": "SQuAD v1について",
        "model": "モデル",
        "filter size": "フィルターのサイズ",
        "fine-tune": "フェインチューン",
        "memory consumption": "メモリ消費",
        "input length": "入力長さ",
        "model memorizing": "モデルを記憶する",
        "BOOKCORPUS": "ブックコープス",
        "state-of-the-art performance": "最新のパフォーマンス",
        "ALBERT model": "ALBERTモデル",
        "parameter-reduction techniques": "パラメータ削減技術",
        "nuances": "ニュアンス",
        "diminishing returns": "減少する収益",
        "sentence order prediction (SOP)": "句順予測 (SOP)",
        "target task": "ターゲットタスク",
        "Wikipedia": "ウィキペディア",
        "larger structure": "大きな構造",
        "CLS": "CLSについて",
        "gender": "性別について",
        "General Language Understanding Evaluation (GLUE) benchmark": "一般言語理解評価 (GLUE) のベンチマーク",
        "Models": "モデル",
        "Probability": "確率について",
        "pretrained": "予備訓練を受けた",
        "ALBERT-xxlarge": "ALBERT-xxlarge",
        "reading comprehension": "読み解く能力",
        "RACE benchmarks": "RACEのベンチマーク",
        "self-supervised learning": "自己監督された学習",
        "input data": "入力データ",
        "discourse-level coherence": "ディスカッションレベルの連携",
        "pre-trained model": "予備訓練されたモデル",
        "multi-sentence encoding tasks": "多文のエンコーディングタスク",
        "embedding matrix": "埋め込みマトリックス",
        "arXiv": "アーXIV",
        "memory constraints": "メモリの制限",
        "Sentence Order Prediction": "句順予測",
        "cloud provider": "クラウドプロバイダー",
        "Finetuned": "細かく調整された",
        "Hidden size": "隠されたサイズ",
        "tokenized": "トークン化された",
        "dimensional embedding space": "寸法的な埋め込み空間",
        "repo": "レポ",
        "performance decline": "パフォーマンスの低下",
        "block attention": "注意を遮断する",
        "training data": "トレーニングデータ",
        "parameter-efficiency techniques": "パラメータ効率の技術",
        "vocabulary embedding size": "語埋め込みサイズ",
        "sentence classification": "句の分類",
        "efficiency": "効率性について",
        "model size": "モデルサイズ",
        "fine-grained": "細粒子",
        "computational expense": "計算費",
        "hardware": "ハードウェア",
        "hidden space": "隠された空間",
        "language representation model": "言語表現モデル",
        "multi-sentence encoding": "多文エンコーディング",
        "postprocessing": "後の処理",
        "Transformer": "トランスフォーマー",
        "self-supervised": "自己監督された",
        "inter-sentence coherence": "句間連結性",
        "dropout": "退学する",
        "architecture": "建築学",
        "feed-forward": "フィードフォワード",
        "preprocessing": "前処理",
        "model distillation": "モデル蒸留",
        "ALBERT configurations": "ALBERT 構成",
        "corpora": "コルパス",
        "hidden sizes": "隠されたサイズ",
        "learned representations": "学んだ表現",
        "General Language Understanding Evaluation": "一般的な言語理解評価",
        "Training steps": "トレーニングステップ",
        "multi-sentence inputs": "多文入力",
        "transformer encoder": "トランスフォーマーエンコーダー",
        "question answering": "質問に答える",
        "memory limitations": "メモリの制限",
        "inter-sentence coherence modeling": "句間連結モデリング",
        "albert-large-v2": "アルバート・ラージュ・v2",
        "SentencePiece": "句子の部分",
        "Diminishing returns": "収益が減る",
        "models": "モデル",
        "pre-training tasks": "予備訓練のタスク",
        "training details": "訓練の詳細",
        "downstream performance": "ダウングラムの性能",
        "attention heads": "注意の頭",
        "deployment": "配備する",
        "parameter sharing": "パラメータ共有",
        "pretraining": "予備訓練",
        "GELU": "GELU (ゲルー)",
        "Dropout": "退学する",
        "discourse-level coherence properties": "ディスカッションレベルの連結性の特性",
        "network parameters": "ネットワークパラメータ",
        "training speed": "トレーニング速度",
        "parameter efficiency": "パラメータ効率",
        "benchmarks": "ベンチマーク",
        "pipeline": "パイプライン",
        "sociotechnical limitations": "社会技術的な制限",
        "next sentence prediction": "次の文の予測",
        "parameters": "パラメーター",
        "ALBERT": "アルバート",
        "repository": "貯蔵庫",
        "pretrained models": "訓練済みのモデル",
        "legal": "法律的",
        "pretraining data": "予備訓練データ",
        "MLM": "MLM (マルチメディア・マーケティング)",
        "GLUE": "グルー",
        "natural language understanding": "自然言語の理解",
        "comparisons": "比較する",
        "Radu Soricut": "ラドゥ・ソリクット",
        "computationally expensive": "計算的に高価な",
        "hidden size": "隠されたサイズ",
        "GLUE benchmark": "GLUE ベンチマーク",
        "pre-existing model": "既存のモデル",
        "corpus": "コルプス",
        "n-gram masking": "nグラムのマッキング",
        "encoder": "エンコーダー",
        "rewriting": "書き直すこと",
        "learning rate": "学習率について",
        "downstream tasks": "ダウングラムタスク",
        "TPUs": "TPU (トーピーユニット)",
        "factorized embedding parameterization": "因数化された埋め込みパラメトリ化",
        "Factorizes": "ファクタリズ",
        "private information": "個人情報",
        "evaluating": "評価する",
        "evaluation": "評価する",
        "NLP benchmarks": "NLPのベンチマーク",
        "More Information Needed": "必要 な 情報",
        "embedding parameters": "埋め込みパラメーター",
        "self-supervised training": "自己監督訓練",
        "interpretability": "解釈性について",
        "Masked Language Model (MLM)": "マスクされた言語モデル (MLM)",
        "Input sequences": "入力シーケンス",
        "randomly generated": "ランダムに生成された",
        "Wikipedia corpora": "ウィキペディア・コーパス",
        "SOP": "SOP",
        "foreseeable harms": "予測可能な害",
        "informing decisions": "情報提供の決定",
        "embedding dimension": "埋め込み次元",
        "Tokenized": "トークン化された",
        "labeled data": "ラベル付けされたデータ",
        "XLNet": "XLNet (XLネット)",
        "Sentence-order prediction": "句順予測",
        "pooled output": "統合された生産",
        "steps": "ステップ",
        "model's performance": "モデルのパフォーマンス",
        "LAMB optimizer": "LAMBオプティマイザー",
        "Next Sentence Prediction (NSP)": "次の文の予測 (NSP)",
        "interpretable": "解釈可能である",
        "N-gram": "Nグラム",
        "benchmark": "ベンチマーク",
        "development sets": "開発セット",
        "ALBERT architecture": "ALBERTアーキテクチャ",
        "embedding": "埋め込む",
        "sparse attention": "注意が薄い",
        "input sequences": "入力シーケンス",
        "performance increases": "パフォーマンスの増加",
        "task-specific": "特定のタスク",
        "Cross-layer parameter sharing": "クラスレイヤーパラメータの共有",
        "Accuracy": "正確さ",
        "hidden states": "隠された状態",
        "performance": "パフォーマンス",
        "SEP": "SEPについて",
        "performance gains": "パフォーマンスゲイン",
        "Piyush Sharma": "ピユシュ・シャーマ",
        "fine-tuned": "細かく調整された",
        "Google Research": "グーグル・リサーチ",
        "Sebastian Goodman": "セバスチャン・グッドマン",
        " SQuAD ": "スクアッド",
        "SOP task": "SOPタスク",
        "Cloud": "クラウド",
        "RACE dataset": "RACEデータセット",
        "layers": "層である",
        "batch normalization": "バッチの正常化",
        "SQuAD": "スクアッド",
        "next sentence prediction (NSP)": "次の文の予測 (NSP)",
        "language understanding component": "言語理解コンポーネント",
        "Computational efficiency": "計算効率",
        "distillation": "蒸留",
        "3-gram": "3グラム",
        "age": "年齢について",
        "RACE": "ラス",
        "medical": "医療について",
        "inference latency": "推論の遅延時間",
        "losses": "損失について",
        "task-specific output layer": "タスク特有の出力層",
        "preprocess": "前処理",
        "n-gram": "nグラム",
        "computational cost": "計算コスト",
        "false negatives": "偽陰性",
        "Feed-forward": "フィードフォワード",
        "variant": "変形",
        "State-of-the-art performance": "最新のパフォーマンス",
        "Stanford Question Answering Dataset": "スタンフォードの質問と答えのデータセット",
        "Examinations": "検査について",
        "GLUE score": "GLUEスコア",
        "masking probability distribution": "マッキング確率分布",
        "text classification": "テキスト分類",
        "highly specialized domains": "高度に専門的な分野",
        "ALBERT-large-v2 model": "ALBERT-large-v2モデル",
        "explainability": "説明性について",
        "Training Data": "トレーニングデータ",
        "model architecture": "モデルアーキテクチャ",
        "BERT": "BERT (ベルト)",
        "resizing": "サイズ変更",
        "Reference": "参照する",
        "ALBERT-large": "ALBERT-large",
        "English Wikipedia": "英語のウィキペディア",
        "datasets": "データセット",
        "coherence": "一貫性について",
        "analysis": "分析する",
        "one-hot vectors": "単一ホットベクトル",
        "Computational cost": "計算コスト",
        "misc": "様々な",
        "inference": "推論する",
        "common sense understanding": "常識的な理解",
        "uncompressed text": "圧縮されていないテキスト",
        "sociotechnical impacts": "社会技術的な影響",
        "Early stopping": "早めに止める",
        "Σ (sigma)": "(シグマ)",
        "human-level reasoning abilities": "人間のレベルの推論能力",
        "performance metrics": "パフォーマンスメトリック",
        "Zhenzhong Lan": "ジンジョングラン",
        "language representations": "言語表現",
        "pre-trained language understanding capabilities": "言語理解能力の予備訓練",
        "API": "API (アピー)",
        "sentence ordering pretraining loss": "訓練前の損失を命じる判決",
        "negative impacts": "否定的な影響",
        "fairness": "公平さについて",
        "distilled": "蒸留された",
        "ALBERT-large configurations": "ALBERT-large 構成",
        "Sentence Order Prediction (SOP)": "判決順序予測 (SOP)",
        "raw text": "原文",
        "training time": "トレーニング時間",
        "development set scores": "開発セットスコア",
        "pre-training": "予備訓練について",
        "mask": "マスク",
        "sentence ordering prediction (SOP)": "句の順序予測 (SOP)",
        "hyperparameters": "ハイパーパラメータ",
        "Mingda Chen": "ミンダ・チェン",
        "natural language processing": "自然言語処理",
        "metrics": "メトリック",
        "TPU": "TPU (トーピュ)",
        "Cloud TPU V3": "クラウドTPU V3",
        "encoder layers": "エンコーダー層",
        "nonlinearities": "非線形性",
        "bias": "偏見について",
        "Fatorized embedding parameterization": "ファクタリズドインベディングパラメタライズ",
        "ALBERT-large-v2": "ALBERT-large-v2",
        "computational efficiency": "計算効率",
        "structure": "構造について",
        "state-of-the-art results": "最新の結果",
        "Kevin Gimpel": "ケヴィン・ギンペル",
        "number of layers": "層の数",
        "representations": "代表する",
        "BERT-large": "BERT-large (BERT-大規模)",
        "Stanford Question Answering Dataset (SQuAD)": "スタンフォードの質問回答データセット (SQuAD)",
        "performance returns": "パフォーマンスリターン",
        "cross-layer parameter sharing": "クラスレイヤーパラメータ共有",
        "pre-training task": "予備訓練のタスク",
        "SQuAD 2": "スクアッド2",
        "architecture choices": "建築の選択",
        "fine-tuning": "フェインチューニング",
        "MLM targets": "MLMのターゲット",
        "model configuration": "モデル構成",
        "albert-large-v2 model": "アルバート・ラージュ・v2モデル",
        "race": "人種",
        "subpopulations": "サブポピュレーション"
    },
    "French": {
        "biases": "les biais",
        "pre-trained": "préformés",
        "Network size": "Taille du réseau",
        "model outputs": "résultats du modèle",
        "probability": "probabilité",
        "ALBERT models": "Les modèles ALBERT",
        "post-processing": "traitement postérieur",
        "length": "longueur",
        "sensitive use cases": "cas d'utilisation sensibles",
        "feed-forward/filter size": "taille de l'alimentation en avant/filtre",
        "training": "la formation",
        "vocabulary size": "taille du vocabulaire",
        "SOP loss": "Perte de SOP",
        "tokens": "les jetons",
        "GitHub": "GitHub est disponible sur GitHub.",
        "author": "auteur",
        "self-supervised loss": "perte sous auto-surveillance",
        "scientific text": "texte scientifique",
        "downstream": "en aval",
        "Reading Comprehension": "Compréhension de la lecture",
        "Attention heads": "Attention aux têtes.",
        "ALBERT project": "Projet ALBERT",
        "analyzing personal data": "analyse des données personnelles",
        "Masked inputs": "Entrées masquées",
        "SQuAD v1": "SQuAD v1",
        "model": "le modèle",
        "filter size": "taille du filtre",
        "fine-tune": "à régler finement",
        "memory consumption": "consommation de mémoire",
        "input length": "longueur d'entrée",
        "model memorizing": "mémorisation de modèles",
        "BOOKCORPUS": "Le corps de la livre",
        "state-of-the-art performance": "performance de pointe",
        "ALBERT model": "Le modèle ALBERT",
        "parameter-reduction techniques": "techniques de réduction des paramètres",
        "nuances": "les nuances",
        "diminishing returns": "rendements décroissants",
        "sentence order prediction (SOP)": "prédiction de l'ordre des phrases (SOP)",
        "target task": "tâche cible",
        "Wikipedia": "Wikipédia",
        "larger structure": "structure plus grande",
        "CLS": "CLS",
        "gender": "le sexe",
        "General Language Understanding Evaluation (GLUE) benchmark": "Évaluation de la compréhension générale des langues (GLUE)",
        "Models": "Les modèles",
        "Probability": "La probabilité",
        "pretrained": "préentraîné",
        "ALBERT-xxlarge": "ALBERT-xxlarge",
        "reading comprehension": "compréhension de la lecture",
        "RACE benchmarks": "Les critères de référence RACE",
        "self-supervised learning": "apprentissage auto-supervisé",
        "input data": "données d'entrée",
        "discourse-level coherence": "cohérence au niveau du discours",
        "pre-trained model": "modèle préentraîné",
        "multi-sentence encoding tasks": "tâches de codage de plusieurs phrases",
        "embedding matrix": "matrice d'intégration",
        "arXiv": "Le fichier arXiv",
        "memory constraints": "contraintes de mémoire",
        "Sentence Order Prediction": "Prédiction de l'ordre des phrases",
        "cloud provider": "fournisseur de cloud",
        "Finetuned": "Aiguisé",
        "Hidden size": "Taille cachée",
        "tokenized": "avec des jetons",
        "dimensional embedding space": "espace d'intégration dimensionnelle",
        "repo": "les opérations de rachat",
        "performance decline": "baisse de performance",
        "block attention": "blocage de l'attention",
        "training data": "données de formation",
        "parameter-efficiency techniques": "techniques d'efficacité des paramètres",
        "vocabulary embedding size": "taille d'intégration du vocabulaire",
        "sentence classification": "classification des phrases",
        "efficiency": "l'efficacité",
        "model size": "taille du modèle",
        "fine-grained": "à grains fins",
        "computational expense": "dépenses de calcul",
        "hardware": "le matériel",
        "hidden space": "espace caché",
        "language representation model": "modèle de représentation linguistique",
        "multi-sentence encoding": "codage à plusieurs phrases",
        "postprocessing": "traitement postérieur",
        "Transformer": "Le transformateur",
        "self-supervised": "auto-supervisé",
        "inter-sentence coherence": "cohérence entre les phrases",
        "dropout": "abandon scolaire",
        "architecture": "l'architecture",
        "feed-forward": "à l'avance",
        "preprocessing": "pré-traitement",
        "model distillation": "distillation de modèle",
        "ALBERT configurations": "Configurations ALBERT",
        "corpora": "les corps",
        "hidden sizes": "tailles cachées",
        "learned representations": "représentations apprises",
        "General Language Understanding Evaluation": "Évaluation de la compréhension générale des langues",
        "Training steps": "Étapes de formation",
        "multi-sentence inputs": "entrées à plusieurs phrases",
        "transformer encoder": "encodeur de transformateur",
        "question answering": "répondre aux questions",
        "memory limitations": "limitations de la mémoire",
        "inter-sentence coherence modeling": "modélisation de la cohérence entre les phrases",
        "albert-large-v2": "Il s'agit de l'albert-large-v2",
        "SentencePiece": "Une phrase.",
        "Diminishing returns": "Les rendements diminuent",
        "models": "les modèles",
        "pre-training tasks": "tâches de préformation",
        "training details": "détails de la formation",
        "downstream performance": "performance en aval",
        "attention heads": "les têtes d'attention",
        "deployment": "déploiement",
        "parameter sharing": "partage de paramètres",
        "pretraining": "pré-formation",
        "GELU": "GELU",
        "Dropout": "Abandon scolaire",
        "discourse-level coherence properties": "propriétés de cohérence au niveau du discours",
        "network parameters": "paramètres du réseau",
        "training speed": "vitesse d'entraînement",
        "parameter efficiency": "efficacité du paramètre",
        "benchmarks": "les points de référence",
        "pipeline": "le pipeline",
        "sociotechnical limitations": "les limitations sociotechniques",
        "next sentence prediction": "prédiction de la prochaine phrase",
        "parameters": "les paramètres",
        "ALBERT": "Je vous en prie.",
        "repository": "dépôt",
        "pretrained models": "modèles préentraînés",
        "legal": "légale",
        "pretraining data": "données de pré-formation",
        "MLM": "Le MLM",
        "GLUE": "La colle",
        "natural language understanding": "compréhension du langage naturel",
        "comparisons": "les comparaisons",
        "Radu Soricut": "Radu Soricut",
        "computationally expensive": "coûteux en calcul",
        "hidden size": "taille cachée",
        "GLUE benchmark": "Indice de référence GLUE",
        "pre-existing model": "modèle préexistant",
        "corpus": "le corpus",
        "n-gram masking": "masquage de n-grammes",
        "encoder": "codeur",
        "rewriting": "réécriture",
        "learning rate": "taux d'apprentissage",
        "downstream tasks": "tâches en aval",
        "TPUs": "Les TPU",
        "factorized embedding parameterization": "paramétrisation d'intégration factorisée",
        "Factorizes": "Facteurisé",
        "private information": "des informations privées",
        "evaluating": "à évaluer",
        "evaluation": "évaluation",
        "NLP benchmarks": "Les critères de référence de la PNL",
        "More Information Needed": "Plus d'informations nécessaires",
        "embedding parameters": "les paramètres d'intégration",
        "self-supervised training": "formation sous auto-supervision",
        "interpretability": "interprétabilité",
        "Masked Language Model (MLM)": "Modèle de langage masqué (MLM)",
        "Input sequences": "Sequences d'entrée",
        "randomly generated": "généré aléatoirement",
        "Wikipedia corpora": "Les corpus de Wikipédia",
        "SOP": "SOP",
        "foreseeable harms": "dommages prévisibles",
        "informing decisions": "décisions d'information",
        "embedding dimension": "dimension d'intégration",
        "Tokenized": "Tokenisé",
        "labeled data": "données étiquetées",
        "XLNet": "XLNet est",
        "Sentence-order prediction": "Prédiction de l'ordre des phrases",
        "pooled output": "production regroupée",
        "steps": "les étapes",
        "model's performance": "la performance du modèle",
        "LAMB optimizer": "Optimiseur LAMB",
        "Next Sentence Prediction (NSP)": "Prédiction de la prochaine phrase (NSP)",
        "interpretable": "à interpréter",
        "N-gram": "N-grammes",
        "benchmark": "le niveau de référence",
        "development sets": "ensembles de développement",
        "ALBERT architecture": "L'architecture ALBERT",
        "embedding": "intégration",
        "sparse attention": "une attention rare",
        "input sequences": "séquences d'entrée",
        "performance increases": "augmentation de la performance",
        "task-specific": "spécifique à une tâche",
        "Cross-layer parameter sharing": "Partage de paramètres entre couches",
        "Accuracy": "Précision",
        "hidden states": "états cachés",
        "performance": "la performance",
        "SEP": "SEP",
        "performance gains": "gains de performance",
        "Piyush Sharma": "Piyush Sharma est un homme de droit.",
        "fine-tuned": "réglé avec précision",
        "Google Research": "Recherche effectuée par Google",
        "Sebastian Goodman": "Je suis Sebastian Goodman.",
        " SQuAD ": "Squad",
        "SOP task": "Tâche de la SOP",
        "Cloud": "Le nuage",
        "RACE dataset": "Ensemble de données RACE",
        "layers": "les couches",
        "batch normalization": "normalisation du lot",
        "SQuAD": "Squad",
        "next sentence prediction (NSP)": "prédiction de la phrase suivante (NSP)",
        "language understanding component": "composante de compréhension des langues",
        "Computational efficiency": "Efficacité informatique",
        "distillation": "la distillation",
        "3-gram": "3 grammes",
        "age": "l'âge",
        "RACE": "La race",
        "medical": "médicale",
        "inference latency": "latence d'inférence",
        "losses": "les pertes",
        "task-specific output layer": "couche de sortie spécifique à la tâche",
        "preprocess": "pré-processus",
        "n-gram": "n-gramme",
        "computational cost": "coût de calcul",
        "false negatives": "faux négatifs",
        "Feed-forward": "À l'avance",
        "variant": "variante",
        "State-of-the-art performance": "Des performances de pointe",
        "Stanford Question Answering Dataset": "Ensemble de données de réponse aux questions de Stanford",
        "Examinations": "Examens effectués",
        "GLUE score": "Score GLUE",
        "masking probability distribution": "distribution de probabilité de masquage",
        "text classification": "classification du texte",
        "highly specialized domains": "domaines hautement spécialisés",
        "ALBERT-large-v2 model": "Le modèle ALBERT-large-v2",
        "explainability": "explication",
        "Training Data": "Données sur la formation",
        "model architecture": "architecture de modèle",
        "BERT": "Le BERT",
        "resizing": "redimensionnement",
        "Reference": "Référence",
        "ALBERT-large": "ALBERT-grand",
        "English Wikipedia": "Wikipédia en anglais",
        "datasets": "ensembles de données",
        "coherence": "la cohérence",
        "analysis": "analyse",
        "one-hot vectors": "vecteurs un-chauds",
        "Computational cost": "Coût de calcul",
        "misc": "Misc.",
        "inference": "la déduction",
        "common sense understanding": "compréhension du bon sens",
        "uncompressed text": "texte non comprimé",
        "sociotechnical impacts": "les impacts sociotechniques",
        "Early stopping": "Arrêt précoce",
        "Σ (sigma)": "(Σsigma)",
        "human-level reasoning abilities": "capacités de raisonnement au niveau humain",
        "performance metrics": "les mesures de performance",
        "Zhenzhong Lan": "Zhenzhong Lan est une ville de Chine.",
        "language representations": "représentations linguistiques",
        "pre-trained language understanding capabilities": "capacités de compréhension linguistique pré-formées",
        "API": "L'API est",
        "sentence ordering pretraining loss": "peine ordonnant la perte préalable à la formation",
        "negative impacts": "les impacts négatifs",
        "fairness": "l'équité",
        "distilled": "distillés",
        "ALBERT-large configurations": "Configurations de grande taille ALBERT",
        "Sentence Order Prediction (SOP)": "Prédiction de l'ordre de la peine (SOP)",
        "raw text": "texte brut",
        "training time": "temps de formation",
        "development set scores": "notes de développement définies",
        "pre-training": "pré-formation",
        "mask": "le masque",
        "sentence ordering prediction (SOP)": "prédiction de l'ordre des phrases (SOP)",
        "hyperparameters": "les hyperparamètres",
        "Mingda Chen": "Mingda Chen est une femme.",
        "natural language processing": "traitement du langage naturel",
        "metrics": "les mesures",
        "TPU": "Le TPU",
        "Cloud TPU V3": "Cloud TPU V3",
        "encoder layers": "couches d'encodeur",
        "nonlinearities": "les non-linéaires",
        "bias": "biaisé",
        "Fatorized embedding parameterization": "Paramétrisation d'intégration factorialisée",
        "ALBERT-large-v2": "ALBERT-large-v2",
        "computational efficiency": "efficacité informatique",
        "structure": "la structure",
        "state-of-the-art results": "résultats à la pointe de la technologie",
        "Kevin Gimpel": "Il s'agit de Kevin Gimpel.",
        "number of layers": "nombre de couches",
        "representations": "les représentations",
        "BERT-large": "BERT-large",
        "Stanford Question Answering Dataset (SQuAD)": "Ensemble de données de réponse aux questions de Stanford (SQuAD)",
        "performance returns": "rendements de performance",
        "cross-layer parameter sharing": "partage de paramètres entre couches",
        "pre-training task": "tâche de préformation",
        "SQuAD 2": "Squad 2",
        "architecture choices": "choix d'architecture",
        "fine-tuning": "réglage précis",
        "MLM targets": "Objectifs du MLM",
        "model configuration": "configuration du modèle",
        "albert-large-v2 model": "modèle albert-large-v2",
        "race": "la race",
        "subpopulations": "sous-populations"
    },
    "Arabic": {
        "biases": "التحيزات",
        "pre-trained": "مدرب مسبقاً",
        "Network size": "حجم الشبكة",
        "model outputs": "مخرجات النموذج",
        "probability": "الاحتمالية",
        "ALBERT models": "نماذج ألبرت",
        "post-processing": "بعد المعالجة",
        "length": "الطول",
        "sensitive use cases": "حالات الاستخدام الحساسة",
        "feed-forward/filter size": "حجم التغذية الأمامية/المرشح",
        "training": "التدريب",
        "vocabulary size": "حجم المفردات",
        "SOP loss": "خسارة SOP",
        "tokens": "الرموز",
        "GitHub": "جيت هاب",
        "author": "المؤلف",
        "self-supervised loss": "الخسارة الخاضعة للإشراف الذاتي",
        "scientific text": "النص العلمي",
        "downstream": "أسفل النهر",
        "Reading Comprehension": "الفهم القراء",
        "Attention heads": "انتبهوا",
        "ALBERT project": "مشروع ألبرت",
        "analyzing personal data": "تحليل البيانات الشخصية",
        "Masked inputs": "مدخلات مغطاة",
        "SQuAD v1": "SQuAD v1",
        "model": "النموذج",
        "filter size": "حجم المرشح",
        "fine-tune": "ضبط دقيق",
        "memory consumption": "استهلاك الذاكرة",
        "input length": "طول المدخل",
        "model memorizing": "حفظ النموذج",
        "BOOKCORPUS": "مجموعة الكتب",
        "state-of-the-art performance": "أداء متطور",
        "ALBERT model": "نموذج ألبرت",
        "parameter-reduction techniques": "تقنيات خفض المعلمات",
        "nuances": "الفروق الدقيقة",
        "diminishing returns": "انخفاض العائدات",
        "sentence order prediction (SOP)": "التنبؤ بترتيب الجملة (SOP)",
        "target task": "المهمة المستهدفة",
        "Wikipedia": "ويكيبيديا",
        "larger structure": "هيكل أكبر",
        "CLS": "CLS",
        "gender": "الجنس",
        "General Language Understanding Evaluation (GLUE) benchmark": "معيار تقييم الفهم العام للغة (GLUE)",
        "Models": "النماذج",
        "Probability": "الاحتمال",
        "pretrained": "مدرب مسبقاً",
        "ALBERT-xxlarge": "ألبرت-xxlarge",
        "reading comprehension": "الفهم القراء",
        "RACE benchmarks": "معايير RACE",
        "self-supervised learning": "التعلم تحت الإشراف الذاتي",
        "input data": "بيانات الإدخال",
        "discourse-level coherence": "التماسك على مستوى الخطاب",
        "pre-trained model": "نموذج مدرب مسبقاً",
        "multi-sentence encoding tasks": "مهام ترميز عدة جمل",
        "embedding matrix": "مصفوفة التضمين",
        "arXiv": "أركسيف",
        "memory constraints": "قيود الذاكرة",
        "Sentence Order Prediction": "التنبؤ بترتيب الجملة",
        "cloud provider": "مزود السحابة",
        "Finetuned": "ضبط دقيق",
        "Hidden size": "الحجم المخفي",
        "tokenized": "رمزية",
        "dimensional embedding space": "مساحة التضمين الأبعاد",
        "repo": "إعادة شراء",
        "performance decline": "انخفاض الأداء",
        "block attention": "منع الانتباه",
        "training data": "بيانات التدريب",
        "parameter-efficiency techniques": "تقنيات كفاءة المعلمات",
        "vocabulary embedding size": "حجم تضمين المفردات",
        "sentence classification": "تصنيف الجمل",
        "efficiency": "الكفاءة",
        "model size": "حجم النموذج",
        "fine-grained": "ذرة دقيقة",
        "computational expense": "نفقات الحوسبة",
        "hardware": "الأجهزة",
        "hidden space": "مساحة مخفية",
        "language representation model": "نموذج تمثيل اللغة",
        "multi-sentence encoding": "ترميز عدة جمل",
        "postprocessing": "المعالجة اللاحقة",
        "Transformer": "المحول",
        "self-supervised": "تحت الإشراف الذاتي",
        "inter-sentence coherence": "الاتساق بين الجمل",
        "dropout": "التسرب",
        "architecture": "الهندسة المعمارية",
        "feed-forward": "التغذية إلى الأمام",
        "preprocessing": "المعالجة المسبقة",
        "model distillation": "التقطير النموذجي",
        "ALBERT configurations": "تكوينات ألبرت",
        "corpora": "الكوربا",
        "hidden sizes": "الأحجام المخفية",
        "learned representations": "تمثيلات تعلمت",
        "General Language Understanding Evaluation": "تقييم الفهم العام للغة",
        "Training steps": "خطوات التدريب",
        "multi-sentence inputs": "مدخلات متعددة الجمل",
        "transformer encoder": "رمز تحويل",
        "question answering": "الإجابة على الأسئلة",
        "memory limitations": "قيود الذاكرة",
        "inter-sentence coherence modeling": "نموذج التماسك بين الجمل",
        "albert-large-v2": "ألبرت-لارج-في2",
        "SentencePiece": "جملة قطعة",
        "Diminishing returns": "انخفاض العائدات",
        "models": "النماذج",
        "pre-training tasks": "مهام ما قبل التدريب",
        "training details": "تفاصيل التدريب",
        "downstream performance": "الأداء في الممر اللاحق",
        "attention heads": "رؤساء الانتباه",
        "deployment": "الانتشار",
        "parameter sharing": "مشاركة المعلمات",
        "pretraining": "التدريب المسبق",
        "GELU": "GELU",
        "Dropout": "التسرب",
        "discourse-level coherence properties": "خصائص التماسك على مستوى الخطاب",
        "network parameters": "معلمات الشبكة",
        "training speed": "سرعة التدريب",
        "parameter efficiency": "كفاءة المعلمة",
        "benchmarks": "المعايير المرجعية",
        "pipeline": "خط الأنابيب",
        "sociotechnical limitations": "القيود الاجتماعية التقنية",
        "next sentence prediction": "التنبؤ بالجملة التالية",
        "parameters": "المعلمات",
        "ALBERT": "(ألبرت)",
        "repository": "المستودع",
        "pretrained models": "نماذج مدربة مسبقاً",
        "legal": "قانوني",
        "pretraining data": "بيانات التدريب المسبق",
        "MLM": "MLM",
        "GLUE": "الغراء",
        "natural language understanding": "فهم اللغة الطبيعية",
        "comparisons": "المقارنات",
        "Radu Soricut": "رادو سوركوت",
        "computationally expensive": "مكلفة من الناحية الحسابية",
        "hidden size": "الحجم المخفي",
        "GLUE benchmark": "معيار GLUE",
        "pre-existing model": "نموذج موجود مسبقاً",
        "corpus": "الكوربوس",
        "n-gram masking": "إخفاء n-gram",
        "encoder": "مُشعر",
        "rewriting": "إعادة الكتابة",
        "learning rate": "معدل التعلم",
        "downstream tasks": "المهام اللاحقة",
        "TPUs": "TPUs",
        "factorized embedding parameterization": "تحديد معلمات التضمين",
        "Factorizes": "تصنف",
        "private information": "معلومات خاصة",
        "evaluating": "تقييم",
        "evaluation": "التقييم",
        "NLP benchmarks": "معايير NLP",
        "More Information Needed": "مزيد من المعلومات المطلوبة",
        "embedding parameters": "معلمات التضمين",
        "self-supervised training": "التدريب تحت الإشراف الذاتي",
        "interpretability": "قابلية التفسير",
        "Masked Language Model (MLM)": "نموذج اللغة المقنعة (MLM)",
        "Input sequences": "تسلسلات الإدخال",
        "randomly generated": "تم إنشاؤه عشوائياً",
        "Wikipedia corpora": "فيكيديا كوربا",
        "SOP": "SOP",
        "foreseeable harms": "الأضرار المتوقعة",
        "informing decisions": "قرارات إعلامية",
        "embedding dimension": "بعد التضمين",
        "Tokenized": "رمزية",
        "labeled data": "بيانات ملصقة",
        "XLNet": "XLNet",
        "Sentence-order prediction": "التنبؤ بترتيب الجمل",
        "pooled output": "الناتج المجمع",
        "steps": "الخطوات",
        "model's performance": "أداء النموذج",
        "LAMB optimizer": "محسن LAMB",
        "Next Sentence Prediction (NSP)": "التنبؤ بالجملة التالية (NSP)",
        "interpretable": "قابلة للتفسير",
        "N-gram": "N-غرام",
        "benchmark": "المعيار",
        "development sets": "مجموعات التنمية",
        "ALBERT architecture": "بنية ألبرت",
        "embedding": "التضمين",
        "sparse attention": "اهتمام قليل",
        "input sequences": "تسلسلات الإدخال",
        "performance increases": "زيادة الأداء",
        "task-specific": "مهمة محددة",
        "Cross-layer parameter sharing": "مشاركة المعلمات عبر الطبقات",
        "Accuracy": "الدقة",
        "hidden states": "الحالات المخفية",
        "performance": "الأداء",
        "SEP": "SEP",
        "performance gains": "مكاسب الأداء",
        "Piyush Sharma": "بييوش شارما",
        "fine-tuned": "ضبط دقيق",
        "Google Research": "بحث جوجل",
        "Sebastian Goodman": "سيباستيان غودمان",
        " SQuAD ": "فرقة",
        "SOP task": "مهمة SOP",
        "Cloud": "السحابة",
        "RACE dataset": "مجموعة بيانات RACE",
        "layers": "طبقات",
        "batch normalization": "تطبيع الدفعة",
        "SQuAD": "فرقة",
        "next sentence prediction (NSP)": "التنبؤ بالجملة التالية (NSP)",
        "language understanding component": "عنصر فهم اللغة",
        "Computational efficiency": "الكفاءة الحاسوبية",
        "distillation": "التقطير",
        "3-gram": "3 جرام",
        "age": "العمر",
        "RACE": "السباق",
        "medical": "طبية",
        "inference latency": "تأخير الاستنتاج",
        "losses": "الخسائر",
        "task-specific output layer": "طبقة المخرجات الخاصة بالمهمة",
        "preprocess": "ما قبل العملية",
        "n-gram": "n-غرام",
        "computational cost": "تكلفة الحوسبة",
        "false negatives": "سلبيات كاذبة",
        "Feed-forward": "التغذية إلى الأمام",
        "variant": "المتغير",
        "State-of-the-art performance": "أداء أحدث",
        "Stanford Question Answering Dataset": "مجموعة بيانات استجابة أسئلة ستانفورد",
        "Examinations": "الفحوصات",
        "GLUE score": "درجة GLUE",
        "masking probability distribution": "توزيع احتمالية التستر",
        "text classification": "تصنيف النص",
        "highly specialized domains": "مجالات متخصصة للغاية",
        "ALBERT-large-v2 model": "نموذج ALBERT-large-v2",
        "explainability": "قابلية التفسير",
        "Training Data": "بيانات التدريب",
        "model architecture": "بنية النموذج",
        "BERT": "(بيرت)",
        "resizing": "تغيير الحجم",
        "Reference": "المرجع",
        "ALBERT-large": "ألبيرت الكبير",
        "English Wikipedia": "ويكيبيديا الإنجليزية",
        "datasets": "مجموعات البيانات",
        "coherence": "الاتساق",
        "analysis": "التحليل",
        "one-hot vectors": "المتجهات الساخنة الواحدة",
        "Computational cost": "تكلفة الحوسبة",
        "misc": "مختلط",
        "inference": "الاستنتاج",
        "common sense understanding": "فهم الحس السليم",
        "uncompressed text": "النص غير المضغوط",
        "sociotechnical impacts": "التأثيرات الاجتماعية التقنية",
        "Early stopping": "التوقف المبكر",
        "Σ (sigma)": "(سيغما)",
        "human-level reasoning abilities": "قدرات التفكير على مستوى الإنسان",
        "performance metrics": "مقاييس الأداء",
        "Zhenzhong Lan": "(زينجونغ لان)",
        "language representations": "تمثيلات اللغة",
        "pre-trained language understanding capabilities": "قدرات فهم اللغة المدربة مسبقاً",
        "API": "API",
        "sentence ordering pretraining loss": "الحكم الذي يأمر بالخسارة قبل التدريب",
        "negative impacts": "التأثيرات السلبية",
        "fairness": "الإنصاف",
        "distilled": "مقطورة",
        "ALBERT-large configurations": "تكوينات ALBERT الكبيرة",
        "Sentence Order Prediction (SOP)": "التنبؤ بترتيب الجملة (SOP)",
        "raw text": "النص الخام",
        "training time": "وقت التدريب",
        "development set scores": "وضع درجات التنمية",
        "pre-training": "التدريب المسبق",
        "mask": "قناع",
        "sentence ordering prediction (SOP)": "التنبؤ بترتيب الجملة (SOP)",
        "hyperparameters": "المعلمات الفائقة",
        "Mingda Chen": "مينغدا تشن",
        "natural language processing": "معالجة اللغة الطبيعية",
        "metrics": "المقاييس",
        "TPU": "TPU",
        "Cloud TPU V3": "سحابة TPU V3",
        "encoder layers": "طبقات التشفير",
        "nonlinearities": "غير الخطية",
        "bias": "التحيز",
        "Fatorized embedding parameterization": "تحديد معلمات التضمين المعاملية",
        "ALBERT-large-v2": "ألبرت-لارج-في2",
        "computational efficiency": "الكفاءة الحاسوبية",
        "structure": "الهيكل",
        "state-of-the-art results": "أحدث النتائج",
        "Kevin Gimpel": "كيفن غيمبل",
        "number of layers": "عدد الطبقات",
        "representations": "تمثيلات",
        "BERT-large": "BERT-large",
        "Stanford Question Answering Dataset (SQuAD)": "مجموعة بيانات استجابة أسئلة ستانفورد (SQuAD)",
        "performance returns": "عوائد الأداء",
        "cross-layer parameter sharing": "مشاركة المعلمات عبر الطبقات",
        "pre-training task": "مهمة ما قبل التدريب",
        "SQuAD 2": "فرقة 2",
        "architecture choices": "خيارات الهندسة المعمارية",
        "fine-tuning": "ضبط دقيق",
        "MLM targets": "أهداف MLM",
        "model configuration": "تكوين النموذج",
        "albert-large-v2 model": "نموذج albert-large-v2",
        "race": "العرق",
        "subpopulations": "السكان الفرعيين"
    }
}