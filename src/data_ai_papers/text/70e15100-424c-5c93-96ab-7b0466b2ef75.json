{"title": "OpenKiwi: An Open Source Framework for Quality Estimation", "authors": "F\u00e1bio Kepler; Jonay Tr\u00e9nous; Marcos Treviso; Miguel Vera; Andr\u00e9 F T Martins", "pub_date": "", "abstract": "We introduce OpenKiwi, a PyTorch-based open source framework for translation quality estimation. OpenKiwi supports training and testing of word-level and sentence-level quality estimation systems, implementing the winning systems of the WMT 2015-18 quality estimation campaigns. We benchmark OpenKiwi on two datasets from WMT 2018 (English-German SMT and NMT), yielding state-of-the-art performance on the word-level tasks and near state-of-the-art in the sentencelevel tasks.", "sections": [{"heading": "Introduction", "text": "Quality estimation (QE) provides the missing link between machine and human translation: its goal is to evaluate a translation system's quality without access to reference translations (Specia et al., 2018b). Among its potential usages are: informing an end user about the reliability of automatically translated content; deciding if a translation is ready for publishing or if it requires human post-editing; and highlighting the words that need to be post-edited.\nWhile there has been tremendous progress in QE in the last years (Martins et al., 2016(Martins et al., , 2017Wang et al., 2018), the ability of researchers to reproduce state-of-the-art systems has been hampered by the fact that these are either based on complex ensemble systems, complicated architectures, or require not well-documented pretraining and fine-tuning of some components. Existing open-source frameworks such as WCE-LIG (Servan et al., 2015), QuEST++ (Specia et al., 2015), Marmot (Logacheva et al., 2016), or Deep-Quest (Ive et al., 2018), while helpful, are currently behind the recent best systems in WMT QE shared tasks. To address the shortcoming * Work done during an internship at Unbabel in 2018.\nabove, this paper presents OpenKiwi, 1 a new open source framework for QE that implements the best QE systems from WMT 2015-18 shared tasks, making it easy to combine and modify their key components, while experimenting under the same framework.\nThe main features of OpenKiwi are:\n\u2022 Implementation of four QE systems: QUETCH (Kreutzer et al., 2015), NUQE (Martins et al., 2016, Predictor-Estimator Wang et al., 2018), and a stacked ensemble with a linear system (Martins et al., 2016(Martins et al., , 2017;\n\u2022 Easy to use API: can be imported as a package in other projects or run from the command line;\n\u2022 Implementation in Python using PyTorch as the deep learning framework;\n\u2022 Ability to train new QE models on new data;\n\u2022 Ability to run pre-trained QE models on data from the WMT 2018 campaign;\n\u2022 Easy to track and reproduce experiments via YAML configuration files and (optionally) MLflow;\n\u2022 Open-source license (Affero GPL).\nThis project is hosted at https://github. com/Unbabel/OpenKiwi. We welcome and encourage contributions from the research community. 2", "publication_ref": ["b13", "b7", "b8", "b14", "b9", "b12", "b6", "b1", "b5", "b14", "b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Quality Estimation", "text": "The goal of word-level QE (Figure 1) is to assign quality labels (OK or BAD) to each machinetranslated word, as well as to gaps between words Figure 1: Example from the WMT 2018 word-level QE training set. Shown are the English source sentence (top), the German machine translated text (bottom), and its manual post-edition (middle). We show also the three types of word-level quality tags: MT (or target) tags account for words that are replaced or deleted, gap tags account for words that need to be inserted, and source tags indicate what are the source words that were omitted or mistranslated. For this example, the HTER sentence-level score (number of edit operations to produce PE from MT normalized by the length of PE) is 8/12 = 66.7%, corresponding to 4 insertions, 1 deletion, and 3 replacements out of 12 reference words.\n(to account for context that needs to be inserted), and source words (to denote words in the original sentence that have been mistranslated or omitted in the target). In the last years, the most accurate systems that have been developed for this task combine linear and neural models (Kreutzer et al., 2015;Martins et al., 2016), use automatic post-editing as an intermediate step (Martins et al., 2017), or develop specialized neural architectures Wang et al., 2018).\nSentence-level QE, on the other hand, aims to predict the quality of the whole translated sentence, for example based on the time it takes for a human to post-edit it, or on how many edit operations are required to fix it, in terms of HTER (Human Translation Error Rate) (Specia et al., 2018b). The most successful approaches to sentence-level QE to date are based on conversions from wordlevel predictions (Martins et al., 2017) or joint training with multi-task learning Wang et al., 2018).", "publication_ref": ["b5", "b7", "b8", "b14", "b13", "b8", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Implemented Systems", "text": "OpenKiwi implements four popular systems that have been proposed in the last years, which we now describe briefly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "QUETCH.", "text": "The \"QUality Estimation from scraTCH\" system (Kreutzer et al., 2015) is designed as a multilayer perceptron with one hidden layer, non-linear tanh activation functions and a lookup-table layer mapping words to continuous dense vectors. For each position in the MT, a window of fixed size surrounding that position, as well as a windowed representation of aligned words from the source text, are concatenated as model input. 3 The output layer scores OK/BAD probabilities for each word with a softmax activation. The model is trained independently to predict source tags, gap tags, and target tags. QUETCH is a very simple model and does not rely on any kind of external auxiliary data for training, only the shared task datasets.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "NuQE.", "text": "OpenKiwi also implements the NeUral Quality Estimation system proposed by Martins et al. (2016). Its architecture consists of a lookup layer containing embeddings for target words and their source-aligned words, in the same fashion as QUETCH. These embeddings are concatenated and fed into two consecutive sets of two feedforward layers and a bi-directional GRU layer. The output contains a softmax layer that produces the final OK/BAD decisions. Like QUETCH, training is also carried independently for source tags, gap tags, and target tags. NuQE is also a blackbox system, meaning it is trained with the shared task data only (i.e., no auxiliary parallel or roundtrip data).\nPredictor-Estimator. Our implementation follows closely the architecture proposed by , which consists of two modules:\n\u2022 a predictor, which is trained to predict each token of the target sentence given the source and the left and right context of the target sentence;\n\u2022 an estimator, which takes features produced by the predictor and uses them to classify each word as OK or BAD.\nOur predictor uses a bidirectional LSTM to encode the source, and two unidirectional LSTMs processing the target in left-to-right (LSTM-L2R) and right-to-left (LSTM-R2L) order. For each target token t i , the representations of its left and right context are concatenated and used as query to an attention module before a final softmax layer. It is trained on the large parallel corpora provided as additional data by the WMT shared task organizers. The estimator takes as input a sequence of features: for each target token t i , the final layer before the softmax (before processing t i ), and the concatenation of the i-th hidden state of LSTM-L2R and LSTM-R2L (after processing t i ). In addition, we train this system with a multi-task architecture that allows us to predict sentence-level HTER scores. Overall, this system is capable to predict sentence-level scores and all word-level labels (for MT words, gaps, and source words)-the source word labels are produced by training a predictor in the reverse direction.\nStacked Ensemble. The systems above can be ensembled by using a stacked architecture with a feature-based linear system, as described by Martins et al. (2017). The features are the ones described there, including lexical and part-of-speech tags from words, their contexts, and their aligned words and contexts, as well as syntactic features and features provided by a language model (as provided by the shared task organizers). This system is only used to produce word-level labels for MT words.", "publication_ref": ["b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Design, Implementation and Usage", "text": "OpenKiwi is designed and implemented in a way that allows new models to be easily added and run, without requiring much concern about input data processing and output generation and evaluation. That means the focus can be almost exclusively put in adding or changing a torch.nn.Module based class. If new flags or options are required, all that is needed is to add them to the CLI parsing module.\nDesign. As a general architecture example, the training pipeline follows these steps:\n\u2022 Each input data, like source text and MT text, is defined as a Field, which holds information about how data should be tokenized, how the inner vocabulary is built, how the mapping to IDs is done, and how a list of samples is padded into a tensor;\n\u2022 A Dataset holds a set of input and output fields, and builds minibatches of samples, each containing their respective input and output data;\n\u2022 A training loop iterates over epochs and steps, calling the model with each minibatch, computing the loss, backpropagating, evaluating on the validation set, and saving snapshots as requested;\n\u2022 By default, the best model is kept and predictions on the validation set are saved as probabilities.\nThe flow rarely needs to be changed for the QE task, so all that is needed for quick experimentation is changing configuration parameters (check the Usage part below) or the model class.\nImplementation. OpenKiwi supports Python 3.5 and later. Since reproducibility is important, it uses Poetry 4 for deterministic dependency management. To decrease the risk of introducing breaking changes with new code, a set of tests are also implemented and currently provide a code coverage close to 80%.\nOpenKiwi offers support for tracking experiments with MLflow, 5 which allows comparing different runs and searching for specific metrics and parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Usage.", "text": "Training an OpenKiwi model is as simple as running the following command:\n$ python kiwi train --config config.yml \u2192\nwhere config.yml is a configuration file with training and model options.\nOpenKiwi can also be installed as a Python package by running pip install openkiwi. In this case, the above command can be switched by $ kiwi train --config config.yml  Figure 2 shows an example of QE predictions using the framework.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Benchmark Experiments", "text": "Datasets. To benchmark OpenKiwi, we use the following datasets from the WMT 2018 quality estimation shared task, all English-German (En-De):\n\u2022 Two quality estimation datasets of sentence triplets, each consisting of a source sentence (SRC), its machine translation (MT) and a human post-edition (PE) of the machine translation: a larger dataset of 26,273 training and 1,000 development triplets, where the MT is generated by a phrase-based statistical machine translation (SMT); and a smaller dataset of 13,442 training and 1,000 development triplets, where the MT is generated by a neural machine translation system (NMT). The data also contains word-level quality labels and sentencelevel scores that are obtained from the posteditions using TERCOM (Snover et al., 2006).\n\u2022 A corpus of 526,368 artificially generated sentence triplets, obtained by first cross-entropy filtering a much larger monolingual corpus for indomain sentences, then using round-trip translation and a final stratified sampling step.\n\u2022 A parallel dataset of 3,396,364 in-domain sentences used for pre-training of the predictorestimator model.\nSystems. In addition to the models that are part of OpenKiwi, in the experiments below, we also use Automatic Post-Editing (APE) adapted for QE (APE-QE). APE-QE has been used by Martins et al. (2017) as an intermediate step for quality estimation, where an APE system is trained on the human post-edits and its outputs are used as pseudo-post-editions to generate word-level quality labels and sentence-level scores in the same way that the original labels were created. Since OpenKiwi's focus is not on implementing a sequence-to-sequence model, we used an external software, OpenNMT-py (Klein et al., 2017), to train two separate translation models:\n\u2022 SRC \u2192 PE: trained first on the in-domain corpus provided, then fine-tuned on the shared task data.    Ive et al. (2018), UNQE is the unpublished system from Jiangxi Normal University, described by Specia et al. (2018a), and QE Brain is the system from Alibaba described by Wang et al. (2018). Reported numbers for the OpenKiwi system correspond to best models in the development set: the STACKED model for prediction of MT tags, and the ENSEMBLED model for the rest.\n\u2022 MT \u2192 PE: trained on the concatenation of the corpus of artificially created sentence triplets and the shared task data oversampled by a factor of 20.\nThese predictions are then combined in the ensemble and stacked systems as explained below.\nExperiments. We show benchmark numbers on the two English-German WMT 2018 datasets. In Table 1, we compare different configurations of OpenKiwi on the development datasets. For the single systems, we can see that the predictorestimator has the best performance, except for predicting the source and the gap word-level tags, where APE-QE is superior. Overall, ensembled versions of these systems perform the best, with a stacked architecture being very effective for predicting word-level MT labels, confirming the findings of Martins et al. (2017). Finally, in Table 2, we report numbers on the official test set. We compare OpenKiwi against the best systems in WMT 2018 (Specia et al., 2018a) and another existing open-source tool, deepQuest (Ive et al., 2018). Overall, OpenKiwi outperforms deepQuest for all wordlevel and sentence-level tasks, and attains the best results for all the word-level tasks. Since its release, OpenKiwi was adopted as the baseline system for the WMT 2019 QE shared task 6 , Moreover, all the winning systems of the word-, sentence-and document-level tasks of the WMT 2019 QE shared task 7 (Kepler et al., 2019) used OpenKiwi as their building foundation.", "publication_ref": ["b10", "b8", "b4", "b1", "b11", "b14", "b8", "b11", "b1", "b2"], "figure_ref": [], "table_ref": ["tab_2", "tab_3"]}, {"heading": "Conclusions", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The authors would like to thank Eduardo Fierro, Thomas Reynaud, and the Unbabel AI and Engineering teams for their invaluable contributions to OpenKiwi.\nThey would also like to thank the support provided by the European Union in the context of the PT2020 projects 027767 and 038510.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A simple, fast, and effective reparameterization of IBM model 2", "journal": "", "year": "2013", "authors": "Chris Dyer; Victor Chahuneau; Noah A Smith"}, {"ref_id": "b1", "title": "DeepQuest: a framework for neural-based Quality Estimation", "journal": "", "year": "2018", "authors": "Julia Ive; Fr\u00e9d\u00e9ric Blain; Lucia Specia"}, {"ref_id": "b2", "title": "Unbabel's Participation in the WMT19 Translation Quality Estimation Shared Task", "journal": "", "year": "2019", "authors": "F\u00e1bio Kepler; Jonay Tr\u00e9nous; Marcos Treviso; Miguel Vera; Ant\u00f3nio G\u00f3is; M Amin Farajian; Ant\u00f3nio V Lopes; Andr\u00e9 F T Martins"}, {"ref_id": "b3", "title": "Predictor-Estimator using Multilevel Task Learning with Stack Propagation for Neural Quality Estimation", "journal": "", "year": "2017", "authors": "Hyun Kim; Jong-Hyeok Lee; Seung-Hoon Na"}, {"ref_id": "b4", "title": "Opennmt: Open-source toolkit for neural machine translation", "journal": "", "year": "2017", "authors": "Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander M Rush"}, {"ref_id": "b5", "title": "QUality Estimation from ScraTCH (QUETCH): Deep Learning for Word-level Translation Quality Estimation", "journal": "", "year": "2015", "authors": "Julia Kreutzer; Shigehiko Schamoni; Stefan Riezler"}, {"ref_id": "b6", "title": "Marmot: A toolkit for translation quality estimation at the word level", "journal": "", "year": "2016", "authors": "Varvara Logacheva; Chris Hokamp; Lucia Specia"}, {"ref_id": "b7", "title": "Unbabel's Participation in the WMT16 Word-Level Translation Quality Estimation Shared Task", "journal": "", "year": "2016", "authors": "F T Andr\u00e9; Ramon Martins; Chris Astudillo; F\u00e1bio Hokamp;  Kepler"}, {"ref_id": "b8", "title": "Pushing the limits of translation quality estimation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "F T Andr\u00e9; Marcin Martins; Fabio Junczys-Dowmunt; Ramon Kepler; Chris Astudillo; Roman Hokamp;  Grundkiewicz"}, {"ref_id": "b9", "title": "An Open Source Toolkit for Word-level Confidence Estimation in Machine Translation", "journal": "", "year": "2015", "authors": "Christophe Servan; Ngoc-Tien Le; Ngoc Quang Luong; Benjamin Lecouteux; Laurent Besacier"}, {"ref_id": "b10", "title": "A study of translation edit rate with targeted human annotation", "journal": "", "year": "2006", "authors": "Matthew Snover; Bonnie Dorr; Richard Schwartz; Linnea Micciulla; John Makhoul"}, {"ref_id": "b11", "title": "Findings of the wmt 2018 shared task on quality estimation", "journal": "", "year": "2018", "authors": "Lucia Specia; Fr\u00e9d\u00e9ric Blain; Varvara Logacheva; Ram\u00f3n Astudillo; Andr\u00e9 F T Martins"}, {"ref_id": "b12", "title": "Multi-level translation quality prediction with quest++", "journal": "", "year": "2015", "authors": "Lucia Specia; Gustavo Paetzold; Carolina Scarton"}, {"ref_id": "b13", "title": "Quality estimation for machine translation", "journal": "Synthesis Lectures on Human Language Technologies", "year": "2018", "authors": "Lucia Specia; Carolina Scarton; Gustavo Henrique Paetzold"}, {"ref_id": "b14", "title": "Alibaba Submission for WMT18 Quality Estimation Task", "journal": "", "year": "2018", "authors": "Jiayi Wang; Kai Fan; Bo Li; Fengming Zhou; Boxing Chen; Yangbin Shi; Luo Si"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: Interactive visualization of the system output. Words tagged as BAD as shown in red, and BAD gaps are denoted as red underscores (\" \"). The Jupyter Notebook producing this output is available at https:// github.com/Unbabel/OpenKiwi/blob/master/demo/KiwiViz.ipynb.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "We presented OpenKiwi, a new open source framework for QE. OpenKiwi is implemented in PyTorch and supports training of word-level and sentence-level QE systems on new data. It outperforms other open source toolkits on both wordlevel and sentence-level, and yields new state-ofthe-art word-level QE results.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "QUETCH 39.90 17.10 36.10 48.32 51.31 29.18 13.26 28.91 42.84 49.59 NUQE 50.04 35.53 42.08 59.62 60.89 32.49 15.01 30.19 43.41 50.87 PRED-EST 57.29 43.68 33.02 70.95 74.49 39.25 21.54 29.52 50.18 55.66 APE-QE 55.12 47.04 51.11 58.01 60.58 37.60 21.78 34.46 35.23 38.88 ENSEMBLED 61.33 53.05 51.11 72.89 76.37 43.04 24.74 34.46 52.34 56.98", "figure_data": "ModelMTEn-De SMT gaps sourcer\u03c1MTEn-De NMT gaps sourcer\u03c1STACKED62.40----43.88----"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Benchmarking of the different models implemented in OpenKiwi on the WMT 2018 development set, OpenKiwi 62.70 52.14 48.88 71.08 72.70 44.77 22.89 36.53 46.72 58.51", "figure_data": "along with an ensembled system (ENSEMBLED) that averages the predictions of the NUQE, APE-QE, and PRED-EST systems, as well as a stacked architecture (STACKED) which stacks their predictions into a linear feature-basedmodel, as described by Martins et al. (2017). For each system, we report the five official scores used in WMT2018: word-level F mult 1for MT, gaps, and source tokens, and sentence-level Pearson's r and Spearman's \u03c1 rankcorrelations.ModelMTEn-De SMT gaps sourcer\u03c1MTEn-De NMT gaps sourcer\u03c1deepQUEST 42.98 28.24 33.97 48.72 50.97 30.31 11.93 28.59 38.08 48.00UNQE---70.00 72.44---51.29 60.52QE Brain62.46 49.99-73.97 75.43 43.61--50.12 60.49"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Final results on the WMT 2018 test set. The first three systems are the official WMT18-QE winners (underlined): deepQUEST is the open source system developed by", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "$ python kiwi train --config config.yml \u2192", "formula_coordinates": [3.0, 307.28, 594.98, 183.27, 21.23]}], "doi": ""}