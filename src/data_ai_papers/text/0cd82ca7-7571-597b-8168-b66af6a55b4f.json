{"title": "Generalising to German Plural Noun Classes, from the Perspective of a Recurrent Neural Network", "authors": "Verna Dankers; Anna Langedijk; Kate Mccurdy; Adina Williams; Dieuwke Hupkes", "pub_date": "", "abstract": "Inflectional morphology has since long been a useful testing ground for broader questions about generalisation in language and the viability of neural network models as cognitive models of language. Here, in line with that tradition, we explore how recurrent neural networks acquire the complex German plural system and reflect upon how their strategy compares to human generalisation and rule-based models of this system. We perform analyses including behavioural experiments, diagnostic classification, representation analysis and causal interventions, suggesting that the models rely on features that are also key predictors in rule-based models of German plurals. However, the models also display shortcut learning, which is crucial to overcome in search of more cognitively plausible generalisation behaviour.", "sections": [{"heading": "Introduction", "text": "Language is a complex and mysterious system, which requires that speakers systematically generalise but also that they admit exceptions (Jackendoff and Audring, 2018;Bybee and Hopper, 2001;Pinker, 1998). A clear illustration of this is the domain of morphology, where suffixes and affixes can be productively used to express a particular grammatical property, but where there are also several words that follow irregular patterns for the same grammatical function. For example, while the past tense for most English verbs is formed by affixing -ed (walk \u2192 walked), the past tense of break is not breaked but broke. If an English speaker encounters an unknown form such as treak, they must decide whether to attach -ed, or instead go with the irregular form troke. Precisely because of such intricacies, the computational task of acquiring a Equal contribution. morphological system capable of generalisation has a strong historical connection to cognitive science (e.g. Seidenberg and Plaut, 2014).\nMaking progress on how human minds process this interesting subdomain of language, however, is a challenging enterprise: probing the internal representations of human minds is difficult and, in some cases, potentially unethical. Several researchers have therefore opted to instead investigate neural models that show generalisation behaviour that is similar to humans in key aspects and use them to learn more about human language processing (for a prime example of this cycle, see Lakretz et al., 2019Lakretz et al., , 2021Baroni, 2021). Using neural models in such a fashion starts with an accurate understanding of how they approach the phenomena of interest: without that, we are constrained in our attempts to use them to devise hypotheses about human language processing and compare them to existing theories (Hupkes, 2020;Baroni, 2021).\nIn that vein, we present a detailed examination of how recurrent neural networks (RNNs) process the complex German plural system which -contrary to the classical example of English past tense -features generalisation of multiple classes (Mar-cus et al., 1995;McCurdy et al., 2020;Belth et al., 2021). We ask what kind of representations such models learn, whether they support human-like generalisation, and we make a start with comparing their learnt solutions with existing models of German plural inflection. We train RNNs to predict the form of a German plural noun (or its plural class) from its singular form and grammatical gender and present an elaborate investigation of the resulting models. We perform behavioural analyses ( \u00a73) of the models' predictions as well as structural analyses of their internal representations, aimed at identifying the features that the models have learnt to associate with each of the classes ( \u00a74). For the latter, we use diagnostic classifiers  that we afterwards use to intervene in the model to establish causal connections between the internal encodings and the predictions the model generates from these encodings ( \u00a75).\nWe find that our networks show a mixture of cognitively plausible generalisation and reliance upon 'shortcuts' or heuristics (McCoy et al., 2019;Geirhos et al., 2020) (see Figure 1). On the negative side, the model's ability to cope with nouns in low-frequency plural classes is very brittle. Our behavioural analyses reveal that the models overrely on length as a heuristic to predict the rare class -s. However, we also find that our models correctly learn that key predictors of plural class include grammatical gender and the last few letters of a word, which are the same features considered by the recent decision-tree-based cognitive model of Belth et al. (2021). Our diagnostic classifiers additionally show how these predictors are encoded in the model's recurrent hidden representations. This interesting overlap between neural and rule-based models raises questions as to what neural models might teach us about the cognitive implementation of rule-based domains. 1", "publication_ref": ["b30", "b8", "b49", "b54", "b37", "b36", "b3", "b27", "b3", "b43", "b42", "b19"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Methods", "text": "German plural inflection comprises multiple plural classes with different frequencies (Clahsen et al., 1992;Marcus et al., 1995;Clahsen, 1999;McCurdy et al., 2020;Zaretsky and Lange, 2016). Most plural nouns are inflected with one of five suffixes (Clahsen et al., 1992): -(e)n, -e, -\u00f8, -er and -s (Table 1 shows some examples). The suffixes -e, -er, 1 The data and implementation are available here.  and -\u00f8 sometimes combine with umlaut. 2 The literature on German plural generalisation has measured success rates based on correct suffixation (as opposed to a combination of the suffix and umlaut), and for simplicity, we keep this focus in the current study (McCurdy et al., 2020;Belth et al., 2021).", "publication_ref": ["b11", "b40", "b10", "b43", "b63", "b11", "b43"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Dataset", "text": "In our experiments, we use the Wiktionary dataset, 3 which contains orthographic representations of pairs of singular and plural forms of German nouns in the nominative case. 4 Given a control token indicating grammatical gender and a singular form (a sequence of discrete characters followed by a stop token, e.g. <f> f r a u </s>), the model is trained to predict the plural form (f r a u e n). Nouns that did not have a gender listed were excluded. The training, validation, and test splits consist of 46k, 6.5k, and 6.6k instances, respectively. Masculine, feminine and neuter nouns appear 23k, 25k and 11k times, respectively. Table 1 indicates the frequency of each of the plural classes, with the average length of the inputs per class. Notice that the nouns that take the -s class are, on average, 8 characters long, while the overall average length is 11 characters.\nTo label plural forms predicted by the model, we consider whether the plural form has one of the five acceptable suffixes (-(e)n, -e, -\u00f8, -er or -s) and whether the singular form appears in the plural form (after removing umlauts). Predictions that belong to one of these five classes are considered well-formed. Otherwise, the input belongs to the unknown inflection class -?.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Model", "text": "We study a recurrent encoder-decoder model implemented with the OpenNMT library (Klein et al., 2017). Modern RNNs trained to perform sequenceto-sequence tasks, including morphological inflection, typically have a bidirectional encoder and an attention mechanism (Corkery et al., 2019;Mc-Curdy et al., 2020). While effective in terms of task accuracy, that setup is further away from how humans process the task at hand (incrementally and in one go) than unidirectional models (see also Hupkes, 2020;Christiansen and Chater, 2016;Baroni, 2021). Furthermore, in models with attention, there is no bottleneck between the encoder and decoder that forces the model to create one localised representation of the input and its potential plural class. We, therefore, study unidirectional models without attention. The encoder and decoder consist of two-layer unidirectional LSTMs, a hidden dimensionality of 128, character embeddings of size 128 and a dropout of 0.1 between layers. The Adadelta (Zeiler, 2012) optimiser is used, with a batch size of 64. During evaluation, we apply beam search with a beam of size five. We train five models with different random initialisations. All results presented are averaged over those models. Belth et al. (2021) propose a cognitive model of morphological learning which uses recursive application of the frequency threshold defined by the Tolerance Principle (Yang, 2016) to identify productive rules, resulting in a decision tree. The model checks at each node whether to keep traversing the tree, apply a learnt rule, or match the input form to a stored exception.For German plural inflection, their model relies upon grammatical gender and the last few characters of the input word as features in the decision tree. We train their model on our dataset for comparison. 5", "publication_ref": ["b33", "b13", "b27", "b9", "b3", "b64", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "Rule-based comparison", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Behavioural results", "text": "In Table 2, we summarise the models' performance after 25 epochs. With 92.3%, our suffix accuracies are competitive, despite the unidirectionality and removal of attention. The RNNs outperform the rule-based model of Belth et al. (2021) on unseen data. 6 Figure 2a shows the RNNs' training curve per plural class, with all classes undergoing rapid 5 We make examples of these decision trees available here. A part of the model is visualised in Appendix E. 6 The Belth et al. model does not handle stem changes such as umlaut, which negatively impacts its full noun accuracy.   increases in performance during the first 5 epochs and less rapid but still substantial increases between epochs 5 and 10. Particularly notable is the curve for samples from the -s class, which is learnt more slowly than the other classes. Figure 2b details the training curve for the -s class by separating inputs ending in a consonant, and those ending in a vowel, which suggests that mostly the inputs from the former class are learnt later during training.\nOvergeneralisation Throughout training, samples can be assigned suffixes different from their target suffix. This rarely happens for majority classes (-(e)n, -e, -\u00f8) (with the exception of -e, which, during one epoch, is predicted as -(e)n in 5% of the cases), but is more frequent for the rarer, minority classes -er and -s. The models tend to generalise the suffixes of majority classes to the minority classes, a phenomenon traditionally referred to as overgeneralisation (Feldman, 2005). Maximum overgeneralisation typically occurs early on during training, as shown in Figure 3 (c.f. Korrel et al., 2019;Dankers et al., 2021).\nWug testing Next, we apply the trained models to 24 nonce word stimuli from Marcus et al. (1995). Of these stimuli, 12 are rhymes -i.e. phonologically familiar words rhyming with an existing word -and 12 non-rhymes -i.e. phonotactically atypical words. We feed them to the network following  2020) though, -s predictions are more frequent than -er, and are also more frequent for non-rhymes than for rhymes. 7 Figure 4a also illustrates that gender impacts the models' predictions: the feminine tag seems related to -(e)n predictions, and the neuter tag to -er predictions.\nEnforcing gender In wug testing, changing the gender changes the model's predictions. Is this the case for Wiktionary data as well? To find out, we compare the model's predictions for samples from the validation set with its predictions for the same samples force-fed with new gender control tokens. Figure 5 visualises the corresponding results. Introducing the feminine control token has the most prominent effect: the vast majority of model predictions changes to -(e)n. Providing the masculine or 7 We trained 95 further model instances with the same setup, to approach the sample size of speakers tested by Mc-Curdy et al. (2020), and found that the pattern of increased -s production on non-rhymes is not statistically reliable.  neuter one reduces the amount of -(e)n predictions, increasing -e, -\u00f8 and -s predictions. The plural class -er only appears associated with the neuter grammatical gender. Taken together, the impact of gender on both wug data and Wiktionary data suggests the model has learnt to strongly rely on the gender markers.", "publication_ref": ["b17", "b34", "b15", "b40"], "figure_ref": ["fig_1", "fig_1", "fig_2", "fig_4"], "table_ref": ["tab_3"]}, {"heading": "Enforcing length", "text": "The high frequency of -s predictions for nonce words is remarkable, given the relative rarity of the -s plural. We observe, however, that the nonce words overall are rather short (4.6 characters), and that the nouns from the -s class are the shortest in Wiktionary (see Table 1). To investigate whether the model has learnt a causal connection between input length and emitting -s, we sample an equal number of nouns from the Wiktionary validation set of each gender that are balanced for whether their singular form ends in a vowel or a consonant. We then lengthen them by prepending nouns of three lengths (\"See\", \"Haupt\" or \"Lieblings\") to form compounds, which simulates a length increase without altering the target's plural class (which is generally determined by the second noun in a compound in German). Our results confirm that the models emit -s less often for longer inputs (see Figure 4b), suggesting that they rely on length as a shortcut for predicting -s. 8", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Diagnostic classification", "text": "We now look into how and where the plural classes are encoded by the model. To do so, we use diagnostic, or probing classifiers (DCs, Adi et al., 2017;Conneau et al., 2018), commonly used to estimate the extent to which hidden representations of a", "publication_ref": ["b0", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Class", "text": "Gender n final letters Both   We record the hidden state h l,e t , memory cell state c l,e t and the activations for the input, forget and output gates i l,e t , f l,e t and o l,e t , from the encoder layers l \u2208 {1, 2}, with 1 \u2264 t \u2264 m; m being the length of the input. The hidden state and memory cell state from the ultimate time step of the encoder form the initialisation of the decoding LSTM. In the absence of an attention mechanism, these representations form the information bottleneck between the encoder and the decoder.\nn = 1 n = 2 n = 1 n = 2 -(e)\nWe train DCs to predict the plural class a model will assign to an input from intermediate time steps. If the to be predicted suffix can accurately be inferred by the DCs, and this generalises to unseen examples, that strengthens the hypothesis that the suffixes are distinctly encoded in the hidden representations of the model. The targets used to train the DCs are the plural class of a model's prediction, rather than the true target class. We only train and evaluate DCs on well-formed model predictions, which means the amount of samples available for training and evaluation changes across epochs. Training lasts for 50 epochs, with a batch size of 16, a learning rate of .00025 and Adam as optimiser. We train five DCs per model and evaluate the DCs using the Wiktionary validation data, with F 1 -scores per plural class and the macro average across classes.\nWe compare with rule-based baselines, where the plural class is estimated from the grammatical gender or the final characters of singular nouns. The F 1 -scores are provided in Table 3. Less wellinformed baselines predicting one class only, or predicting at random according to the frequencies of the different classes, obtain macro-averaged F 1scores of 12.5 and 19.6, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "DC results", "text": "We first consider the difference between different model components (i.e. hidden states, gates) and processing steps. For every input token, we consider the first and the last three time steps. At time step 1, the model processes the gender tag, followed by the first and second character of the noun in time steps 2 and 3. Time steps -3 and -2 correspond to the last two characters of the noun; time step -1 is the end-of-sequence (EOS) token. We train separate DCs for every time step, using representations from the 25 th (i.e., final) epoch. Figure 6 reports the F 1 -scores of the DCs, per plural class; the macro-averaged F 1 -score is shown in black.\nIn Figure 6a, we visualise results for DCs trained on the concatenated hidden and memory cell states, for multiple time steps in the encoder. Figure 6b summarises the performance in the last time step for the remaining model components (full figures are in Appendix B). The graphs show that DCs trained on hidden and memory cell states consistently outperform DCs trained on gates, and that the memory cell state alone captures nearly the same amount of information as the concatenated hidden and memory cell states. For all components, performance increases when the model has processed more characters. A remarkable exception to this is the -(e)n class, for which performance immedi-  ately peaks at the first time step. Considering the behavioural analyses indicating a large impact of the feminine gender tag on this class, the DC may have learnt that this is a strong predictor for that category. The gender tag is fed during the first time step, and may be encoded still towards the end. Focusing on the DC performance for the concatenations of the hidden and memory cell states, these F 1 -scores are either similar to the highest baseline performance in Table 3 (for -(e)n, -\u00f8, -e) or even sub-par compared to those scores (for -s and -er). Taken together with the impact of gender observed for -(e)n, and the fact that the scores increase as the word is being processed, this suggests that the last few letters and the grammatical gender are essential features in predicting the plural class.\nGeneralisability across time steps Following Giulianelli et al. (2018), we now test how well DCs generalise across time steps to get an indication of when consistent representations of the plural classes are formed. Considering again epoch 25, we test our DCs trained on the concatenation of the hidden and memory cell states from one time step and evaluate on another, for time steps 1, 2, 3, -3, -2 and -1. We show the results in Figure 7, where the diagonal corresponds to results in Figure 6a, and the off-diagonal entries represent generalisation across time steps. Classifiers trained on early time steps do not generalise to representations close to the end. This is unsurprising, considering that features learnt by early DCs cannot be based on the noun, since it has not been processed yet. Given the average input length of 11, time steps 3 and -3 will typically be far apart, which is why time step 3 need not generalise to time step -3 for the majority of the inputs. Yet, generalisation is not good even among early or late time steps, with the exception of the -(e)n class (see Figure 7b), for which there are blocks visible in the upper left and bottom right corners, suggesting that the DC relies on the same feature in multiple time steps. The absence of blocks in the lower left and upper right corner implies that that feature is differently encoded at the beginning than at end of processing -e.g. because the hidden representations store more information later on. For the remaining classes, even the last two time steps do not generalise perfectly to one another, which either means that the plural class is not decided until the end or that the decision is encoded in multiple ways, with the DCs in different time steps picking up on different features.\nPerformance over epochs Figure 8 shows the F 1 -scores for DCs trained on the hidden and memory cell states for different training epochs. Because the number of well-formed predictions changes over the course of training, the size of the dataset available for training DCs increases over time. Nonetheless, the DCs' performance on evaluation data remains stable, or even slightly decreases over time. A potential cause could be that inputs for which the model learns to emit a class after the initial epochs are atypical nouns for which the model memorises a suffix to emit, but whose features do not generalise towards new inputs.", "publication_ref": ["b20"], "figure_ref": ["fig_6", "fig_6", "fig_6", "fig_7", "fig_6", "fig_7", "fig_8"], "table_ref": ["tab_5"]}, {"heading": "Dissecting the representation space", "text": "To better understand the features that the DCs rely on, we train sparse DCs by applying L 0 regularisation to the DCs' parameters, that reduces the number of non-zero weights in the classifier. These DCs are trained on representations from epoch five, when performance of the DC peaked in Figure 8. The L 0 -norm is not differentiable and cannot simply be added to the training objective as a regularisation term, but is incorporated through a mask that is multiplied with the weights of the linear layer, where a collection of non-negative stochastic gates determine which weights are set to zero. We refer the reader to Louizos et al. (2018) for a detailed explanation of this regularisation technique. The sparse DCs are trained for 50 epochs, with a learning rate of .001; the L 0 component in the loss is weighted by a hyperparameter \u03bb = .005. We use sparse DCs with, approximately, 95% of the hidden dimensions excluded, to visualise the non-zero weighted dimensions per output class, using t-SNE projections. We only include dimensions that are not pruned by five DCs trained with different random seeds. 10 We then visually inspect the representations by considering features such as the RNN's predicted plural class, the class the DC predicts, the grammatical gender, the singular noun length, and the last few letters of a word. We have four main observations: (1) The gender tags are grouped for -(e)n and -er -e.g. see Figure 9b for -er. This corresponds to the fact that in Figure 5, masculine and neuter proved predictive of -(e)n and -er, respectively. Furthermore, the feminine tag is grouped for -e and -\u00f8, but the three tags are scattered for -s, as shown in Figure 9a. (2) For all classes, the length is an important organisational feauture in the representation space, but for all classes except -s, the DC still predicts that class for nearly all input lengths -e.g. compare Figures 9c  and 9d. (3) The final letter of the singular noun is a prominent organisational feature too; there are clusters of 'e', 't' and 'r', in particular, that are among the top five most frequent final letters of nouns in Wiktionary (see, for example, Figure 9e). Vowels other than 'e' are typically scattered across the representation space, except for -s, for which they cluster (Figure 9f). (4) Lastly, while all predicted classes cluster together when we select the dimensions from the sparse DC for that class, -e cannot easily be localised, potentially due to the fact that two other suffixes can involve adding an 'e' to the singular noun (i.e. -(e)n and -er).", "publication_ref": ["b38"], "figure_ref": ["fig_8", "fig_4"], "table_ref": []}, {"heading": "Interventions", "text": "Until now, we have only been able to relate DC features to models' behaviour by making adaptations in the inputs fed to the model. To strengthen these results, we now ask: can we also change the models' behaviour without changing the input, by changing the input's hidden representation? To do so, we use interventions (Giulianelli et al., 2018) that halt the model while it processes inputs and change its representations using the DC. We moni- tor the effect to establish a causal link between the DC's results and the model's predictions.\nSetup Following Giulianelli et al. (2018), the DC findings are linked to models' behaviour by adapting the hidden representations through the signal provided by DCs, while monitoring the impact on the models' predictions. We perform interventions on the hidden and memory cell state from the final encoder time step, by running the RNN, halting it after the encoder processed the input and intervening on the decoder's initialisation before it predicts an output. Assuming h is the hidden representation, we use the DC as follows: h \u2190 h \u2212 \u03b1\u2207 h L DC ( h).\nWe perform interventions with respect to the true plural classes of samples from the validation set for which the prediction is well-formed but not correct, with \u03b1 empirically set to 2.0. 11\nResults Figure 10a summarises the impact of interventions during the fifth epoch. For wellformed model predictions that have been assigned the wrong plural class, we can change the model's prediction to the right class in up to 43% of the samples with target -(e)n confirming that the information detected by DCs is partially also used by the model. For -e, -\u00f8 and -er, a smaller, yet still substantial percentage can be corrected (18-25%). However, this comes at a cost; some previously well-formed predictions are no longer well-formed (-? in the figure). In most of these cases, though, it is not the plural class that was corrupted, but the rest of the noun -i.e. the intervention sometimes negatively impacts the decoder's ability to recover all of the noun's characters. That the DC has picked up on class-specific features can be deduced from the fact that the interventions either change the class to the correct one, or make the predictions less well-formed, but hardly ever cause the model to emit a different incorrect plural class. In many cases, intervening does not lead to any changes in the models ' predictions, as shown in Figure 10b. Predictions belonging to the plural class -e are typically immune to interventions, which may, again, be due to the fact that two other classes (i.e.\n-(e)n and -er) contain 'e' as part of their suffix.", "publication_ref": ["b20", "b20"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Related work", "text": "Our analysis draws upon current research investigating neural model representations. We apply these techniques to German plural generalisation, a challenging domain with an extensive cognitive and linguistic literature.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Morphological inflection in neural networks", "text": "Recently, others have explored the potential linguistic and cognitive implications of morphological generalisation in neural networks. Malouf (2017) visualised the representation space learnt by RNNs to draw connections with more traditional linguistic categories. King et al. (2020) and Gorman et al. (2019) grouped sequence-to-sequence model errors into linguistically meaningful categories. Neural models have been used to estimate the information theoretic contribution of meaning to gender (Williams et al., 2019) and of meaning and form to gender and declension class .  used grammatical gender classes to track phylogenetic relationships between related languages, while others used them to model morphological learnability (Elsner et al., 2019;Forster et al., 2021).\nProbing has been used to investigate how linguistic information is encoded in neural model representations (Alain and Bengio, 2017;Goodwin et al., 2020;Ravichander et al., 2020), including morphological structure (Torroba Hennigen et al., 2020). Much recent debate has focused on appropriate methods for probing (Belinkov, 2021;Hewitt and Liang, 2019;Hall Maudslay et al., 2020;Pimentel et al., 2020a;Ravichander et al., 2021;White et al., 2021). Our work applies probing to German plural inflection and bolsters it using causal interventions.\nGerman plurals and the past tense debate In a wider context, our work fits within the (in)famous past tense debate, one of the longest and most vigorous conflicts in cognitive science (e.g. Seidenberg and Plaut, 2014), which contrasted neural network models of English past tense inflection (Rumelhart and McClelland, 1986) against theories of generalisation which emphasised a need for symbolic rules (Pinker and Prince, 1988).\nGerman plurals have been an important phenomenon for this debate. Dual-route theorists argued that German speakers show rule-based generalisation for one plural class -the numerically rare -s class -and analogical generalisation of the other classes (Clahsen et al., 1992;Marcus et al., 1995;Clahsen, 1999). This account has been contested by schema theories of German plural generalisation (K\u00f6pcke, 1988;Bybee, 1995). Later experiments also cast doubt on the dual-route account of speaker preference for -s (Hahn and Nakisa, 2000;Zaretsky and Lange, 2016;McCurdy et al., 2020), and recent rule-based models of German plural inflection model all plural classes with a unified approach (Yang, 2016;Belth et al., 2021). The speaker preference for -s on unusual inputs, such as the non-rhyme words developed by Marcus et al. (1995), has been claimed as a key signature of human-like generalisation in contrast to neural network models (Clahsen, 1999). Similar to Goebel and Indefrey (2000), our neural model shows this behaviour, although one should be careful in interpreting this given the length shortcut observed for -s. The question of how rules might be represented neurally is still open to debate and investigation. Our work continues to weaken the original empirical objections to connectionist models (see also Kirov and Cotterell 2018).", "publication_ref": ["b39", "b31", "b23", "b60", "b16", "b18", "b1", "b22", "b52", "b56", "b4", "b26", "b25", "b47", "b51", "b59", "b53", "b50", "b11", "b40", "b10", "b35", "b7", "b24", "b63", "b43", "b62", "b40", "b10", "b21", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion & Conclusion", "text": "For more than 30 years, the field of inflectional morphology has been a testing ground for broader questions about generalisation in language, centred around the extent to which explicit rules are required. In this discussion, neural networks are traditionally considered as an alternative to the explicit representation of rules. However, recent studies have shown that such models show interesting generalisation patterns -sometimes comparable to behaviour observed in humans (Corkery et al., 2019;Kirov and Cotterell, 2018). This raises the question of what kind of solution is implemented by neural networks to process language in seemingly rule-governed domains, how these solutions relate to rule-based models, and what it teaches us about human processing of inflectional morphology. Our study takes a step in this direction by exploring how an RNN encodes generalisation behaviour.\nWe find that an RNN shows a mixture of humanlike generalisation and reliance upon 'shortcuts'. The models correctly learn that key predictors of plural class include grammatical gender and the last few letters of a word, which are the same features used by the recent rule-based cognitive model of Belth et al. (2021). Our DCs show how these predictors are largely encoded in the hidden representations of the encoder. Variation in the classifiers' performance may reflect that some plural classes are encoded more consistently than others; for instance, feminine gender is highly predictive of the -(e)n class. Alternatively, the decoder may decide the plural class for some inputs.\nOn the other hand, the models' ability to cope with nouns in low-frequency plural classes is very brittle. The DCs perform worst for the minority classes, it proved hard to change the model's predictions to -s in the interventions, and behavioural analyses suggested the model overly relies on length as a shortcut to predict this class. By contrast, we see model bias for the frequent class -e in overgeneralisation behaviour ( \u00a73), in representation space dispersion ( \u00a74), and in resistance to interventions ( \u00a75). We speculate that the characterbased RNN may conflate the -e class with the 'e' character that appears in the -(e)n and -er classes.\nIn summary, we contribute a detailed analysis of how an RNN processes the complex task of plural inflection in German. Interestingly, we find cognitively plausible generalisation behaviour through learnt representations which echo recent rule-based models. Future work could address the broader questions raised by these findings, such as what constitutes a rule given overlap in strategy between neural and rule-based models, and how a mechanistic understanding of how neural networks approach seemingly rule-governed domains might contribute to understanding how such generalisation is instantiated in the human brain.", "publication_ref": ["b13", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Additional behavioural analyses", "text": "Here, we present two additional analyses for the nonce word stimuli from Marcus et al. (1995). Firstly, we present them to the model as compounds, to investigate whether these longer inputs change the model predictions too (see \u00a73). We form a novel noun-noun compound with the nonce word in the second position while keeping the neuter tag (e.g. presenting <n> t i e r b r a l </s> to the model instead of <n> b r a l </s>), using three nouns of different genders (\"der Zahn\", \"die Hand\", \"das Tier\"). Generally, the plural class is determined by the second noun in a compound in German, but it is possible that our models might be biased by the first noun of the compound to emit a different plural class. There is only a small impact of the specific noun used to form a compound (Figure 11). A pattern that is more pronounced is that, overall, there are fewer -s predictions, and many more -er predictions. Considering that by creating a compound we increased the length of the nonce word (from 4.6 to 8.6), this suggests a correlation between input length and plural class emitted, as has been previously observed in the main paper.\nSecondly, we investigate how models' predictions for nonce words change during training, as shown in Figure 12. Small fluctuations aside, the nonce predictions do not appear to change substantially after the point of overgeneralisation shown in Figure 3, even though the model's training accuracy increased until the end of training. This pattern is consistent in the remaining productions for which Figure 4 ", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "B Additional DC analyses", "text": "In Figure 13a, we visualise results for DCs trained on the concatenated hidden and memory cell states, for multiple time steps in the encoder. The remaining graphs present the same performance measures for DCs trained on (13b) the hidden states, (13c) memory cell states, and the (13d) input, (13e) forget and (13f) output gates.\nC Additional analyses interventions \u00a75 presented causal interventions with \u03b1 = 2.0.\nThat hyperparameter controls the size of the update of the hidden representation. A large \u03b1 yields more successful interventions, but also more interventions that are not well-formed. As \u03b1 increases, so does the frequency of these errors. We summarise this trend for each of the plural classes in Figure 14.   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Control tasks", "text": "In order to assess the ability of our probes to learn a linguistically meaningless control mapping, we train a DC in a control setting (Hewitt and Liang, 2019), by randomly reassigning labels to each input. Note that we cannot use word identity, as is used by Hewitt and Liang (2019), as a basis for our label shuffling in our morphological task: words do not reappear. Instead, we randomly assign classes based on two features of the input word: gender tag and final two letters. For an example, see  gradient produced by the control DC, with which we update the activations is, unlike the original setup, not informed by the actual class outputted by the recurrent model. This \"class\" in our control task is instead a random set of words, that do not necessarily correspond to the plural form emitted. Successful interventions are therefore coincidental. The results for all interventions are listed in Figure 16. The impact of the hyperparameter \u03b1 on the control DC is visualised in Figure 17. Some outputs can be corrected using an effectively meaningless hidden state update: the maximum percentage of corrected predictions never reaches above 20% for any particular plural class, as can be seen in Figure 17a (compare Figure 14a).\n- ", "publication_ref": ["b26", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "E Rule-based model", "text": "We train rule-based models in the manner suggested by Belth et al. (2021). A part of the model can be seen in Figure 18. We make visualisations of the full models available here.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Fine-grained analysis of sentence embeddings using auxiliary prediction tasks", "journal": "", "year": "2017", "authors": "Yossi Adi; Einat Kermany; Yonatan Belinkov; Ofer Lavi; Y Goldberg"}, {"ref_id": "b1", "title": "Understanding intermediate layers using linear classifier probes", "journal": "", "year": "2017", "authors": "Guillaume Alain; Yoshua Bengio"}, {"ref_id": "b2", "title": "Class features as probes", "journal": "Oxford University Press", "year": "2008", "authors": "Artemis Alexiadou; Gereon M\u00fcller"}, {"ref_id": "b3", "title": "On the gap between theoretical and computational linguistics", "journal": "", "year": "2021", "authors": "Marco Baroni"}, {"ref_id": "b4", "title": "Probing classifiers: Promises, shortcomings, and alternatives", "journal": "CoRR", "year": "2021", "authors": "Yonatan Belinkov"}, {"ref_id": "b5", "title": "What do neural machine translation models learn about morphology?", "journal": "Long Papers", "year": "2017", "authors": "Yonatan Belinkov; Nadir Durrani; Fahim Dalvi; Hassan Sajjad; James Glass"}, {"ref_id": "b6", "title": "2021. The Greedy and Recursive Search for Morphological Productivity", "journal": "", "year": "", "authors": "Caleb Belth; Sarah Payne; Deniz Beser; Jordan Kodner; Charles Yang"}, {"ref_id": "b7", "title": "Regular morphology and the lexicon", "journal": "Language and Cognitive Processes", "year": "1995", "authors": "Joan Bybee"}, {"ref_id": "b8", "title": "Frequency and the emergence of linguistic structure", "journal": "John Benjamins Publishing", "year": "2001", "authors": "L Joan; Paul J Bybee;  Hopper"}, {"ref_id": "b9", "title": "The now-or-never bottleneck: A fundamental constraint on language", "journal": "Behavioral and brain sciences", "year": "2016", "authors": "H Morten; Nick Christiansen;  Chater"}, {"ref_id": "b10", "title": "Lexical entries and rules of language: A multidisciplinary study of German inflection", "journal": "Behavioral and Brain Sciences", "year": "1999", "authors": "Harald Clahsen"}, {"ref_id": "b11", "title": "Regular and irregular inflection in the acquisition of German noun plurals", "journal": "Cognition", "year": "1992", "authors": "Harald Clahsen; Monika Rothweiler; Andreas Woest; Gary F Marcus"}, {"ref_id": "b12", "title": "What you can cram into a single $&!#* vector: Probing sentence embeddings for linguistic properties", "journal": "", "year": "2018", "authors": "Alexis Conneau; Germ\u00e1n Kruszewski; Guillaume Lample; Lo\u00efc Barrault; Marco Baroni"}, {"ref_id": "b13", "title": "Are we there yet? Encoder-decoder neural networks as cognitive models of English past tense inflection", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Maria Corkery; Yevgen Matusevych; Sharon Goldwater"}, {"ref_id": "b14", "title": "On the complexity and typology of inflectional morphological systems", "journal": "", "year": "2019", "authors": "Ryan Cotterell; Christo Kirov; Mans Hulden; Jason Eisner"}, {"ref_id": "b15", "title": "The paradox of the compositionality of natural language: a neural machine translation case study", "journal": "CoRR", "year": "2021", "authors": "Verna Dankers; Elia Bruni; Dieuwke Hupkes"}, {"ref_id": "b16", "title": "Modeling morphological learning, typology, and change: What can the neural sequence-tosequence framework contribute", "journal": "Journal of Language Modelling", "year": "2019", "authors": "Micha Elsner; Andrea D Sims; Alexander Erdmann; Antonio Hernandez; Evan Jaffe; Lifeng Jin; Martha Booker Johnson; Shuan Karim; David L King; Luana Lamberti Nunes; Byung-Doh Oh; Nathan Rasmussen; Cory Shain; Stephanie Antetomaso; Kendra V Dickinson; Noah Diewald; Michelle Mckenzie; Symon Stevens-Guille"}, {"ref_id": "b17", "title": "Learning and overgeneralization patterns in a connectionist model of the German plural", "journal": "", "year": "2005", "authors": "Naomi Feldman"}, {"ref_id": "b18", "title": "Searching for search errors in neural morphological inflection", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Martina Forster; Clara Meister; Ryan Cotterell"}, {"ref_id": "b19", "title": "Shortcut learning in deep neural networks", "journal": "Nature Machine Intelligence", "year": "2020", "authors": "Robert Geirhos; J\u00f6rn-Henrik Jacobsen; Claudio Michaelis; Richard Zemel; Wieland Brendel; Matthias Bethge; Felix A Wichmann"}, {"ref_id": "b20", "title": "Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information", "journal": "", "year": "2018", "authors": "Mario Giulianelli; Jack Harding; Florian Mohnert; Dieuwke Hupkes; Willem Zuidema"}, {"ref_id": "b21", "title": "A recurrent network with short-term memory capacity learning the German-s plural. Models of language acquisition: Inductive and deductive approaches", "journal": "", "year": "2000", "authors": "Rainer Goebel; Peter Indefrey"}, {"ref_id": "b22", "title": "Probing linguistic systematicity", "journal": "", "year": "2020", "authors": "Emily Goodwin; Koustuv Sinha; Timothy J O'donnell"}, {"ref_id": "b23", "title": "Weird inflects but ok: Making sense of morphological generation errors", "journal": "", "year": "2019", "authors": "Kyle Gorman; D Arya; Ryan Mccarthy;  Cotterell"}, {"ref_id": "b24", "title": "German Inflection: Single Route or Dual Route?", "journal": "Cognitive Psychology", "year": "2000", "authors": "Ulrike Hahn; Ramin Charles Nakisa"}, {"ref_id": "b25", "title": "A tale of a probe and a parser", "journal": "", "year": "2020", "authors": "Josef Rowan Hall Maudslay; Tiago Valvoda; Adina Pimentel; Ryan Williams;  Cotterell"}, {"ref_id": "b26", "title": "Designing and interpreting probes with control tasks", "journal": "", "year": "2019", "authors": "John Hewitt; Percy Liang"}, {"ref_id": "b27", "title": "Hierarchy and interpretability in neural models of language processing", "journal": "", "year": "2020", "authors": "Dieuwke Hupkes"}, {"ref_id": "b28", "title": "Compositionality decomposed: how do neural networks generalise", "journal": "Journal of Artificial Intelligence Research", "year": "2020", "authors": "Dieuwke Hupkes; Verna Dankers; Mathijs Mul; Elia Bruni"}, {"ref_id": "b29", "title": "Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure", "journal": "Journal of Artificial Intelligence Research", "year": "2018", "authors": "Dieuwke Hupkes; Sara Veldhoen; Willem Zuidema"}, {"ref_id": "b30", "title": "Morphology and memory: toward an integrated theory", "journal": "Topics in cognitive science", "year": "2018", "authors": "Ray Jackendoff; Jenny Audring"}, {"ref_id": "b31", "title": "Interpreting sequence-to-sequence models for Russian inflectional morphology", "journal": "", "year": "2020", "authors": "David King; Andrea Sims; Micha Elsner"}, {"ref_id": "b32", "title": "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Christo Kirov; Ryan Cotterell"}, {"ref_id": "b33", "title": "OpenNMT: open-source toolkit for neural machine translation", "journal": "", "year": "2017", "authors": "Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander M Rush"}, {"ref_id": "b34", "title": "Transcoding compositionally: Using attention to find more generalizable solutions", "journal": "", "year": "2019", "authors": "Kris Korrel; Dieuwke Hupkes; Verna Dankers; Elia Bruni"}, {"ref_id": "b35", "title": "Schemas in German plural formation", "journal": "Lingua", "year": "1988", "authors": "Klaus-Michael K\u00f6pcke"}, {"ref_id": "b36", "title": "Mechanisms for handling nested dependencies in neural-network language models and humans", "journal": "Cognition", "year": "2021", "authors": "Yair Lakretz; Dieuwke Hupkes; Alessandra Vergallito; Marco Marelli; Marco Baroni; Stanislas Dehaene"}, {"ref_id": "b37", "title": "The emergence of number and syntax units in LSTM language models", "journal": "", "year": "2019", "authors": "Yair Lakretz; German Kruszewski; Theo Desbordes; Dieuwke Hupkes; Stanislas Dehaene; Marco Baroni"}, {"ref_id": "b38", "title": "Learning sparse neural networks through L 0 regularization", "journal": "", "year": "2018", "authors": "Christos Louizos; Max Welling; Diederik P Kingma"}, {"ref_id": "b39", "title": "Abstractive morphological learning with a recurrent neural network", "journal": "Morphology", "year": "2017", "authors": "Robert Malouf"}, {"ref_id": "b40", "title": "German inflection: The exception that proves the rule", "journal": "Cognitive psychology", "year": "1995", "authors": "F Gary; Ursula Marcus; Harald Brinkmann; Richard Clahsen; Steven Wiese;  Pinker"}, {"ref_id": "b41", "title": "Measuring the similarity of grammatical gender systems by comparing partitions", "journal": "", "year": "2020", "authors": "D Arya; Adina Mccarthy; Shijia Williams; David Liu; Ryan Yarowsky;  Cotterell"}, {"ref_id": "b42", "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"ref_id": "b43", "title": "Inflecting when there's no majority: Limitations of encoder-decoder neural networks as cognitive models for German plurals", "journal": "", "year": "2020", "authors": "Kate Mccurdy; Sharon Goldwater; Adam Lopez"}, {"ref_id": "b44", "title": "Remarks on nominal inflection in German", "journal": "Akademie Verlag", "year": "2015", "authors": "Gereon M\u00fcller"}, {"ref_id": "b45", "title": "The relation of vowel letters to phonological syllables in English and German", "journal": "", "year": "2004", "authors": "Martin Neef"}, {"ref_id": "b46", "title": "Die Graphematik des Deutschen", "journal": "Walter de Gruyter", "year": "2011", "authors": "Martin Neef"}, {"ref_id": "b47", "title": "Pareto probing: Trading off accuracy for complexity", "journal": "", "year": "2020", "authors": "Tiago Pimentel; Naomi Saphra; Adina Williams; Ryan Cotterell"}, {"ref_id": "b48", "title": "Information-theoretic probing for linguistic structure", "journal": "", "year": "2020", "authors": "Tiago Pimentel; Josef Valvoda; Rowan Hall Maudslay; Ran Zmigrod; Adina Williams; Ryan Cotterell"}, {"ref_id": "b49", "title": "Words and rules", "journal": "Lingua", "year": "1998", "authors": "Steven Pinker"}, {"ref_id": "b50", "title": "On language and connectionism: Analysis of a parallel distributed processing model of language acquisition. Cognition", "journal": "", "year": "1988", "authors": "Steven Pinker; Alan Prince"}, {"ref_id": "b51", "title": "Probing the probing paradigm: Does probing accuracy entail task relevance?", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Abhilasha Ravichander; Yonatan Belinkov; Eduard Hovy"}, {"ref_id": "b52", "title": "On the systematicity of probing contextualized word representations: The case of hypernymy in BERT", "journal": "", "year": "2020", "authors": "Abhilasha Ravichander; Eduard Hovy; Kaheer Suleman; Adam Trischler; Jackie Chi Kit Cheung"}, {"ref_id": "b53", "title": "On Learning the Past Tenses of English Verbs. In Parallel distributed processing: Explorations in the microstructure of cognition", "journal": "MIT Press", "year": "1986", "authors": "J D E Rumelhart;  Mcclelland"}, {"ref_id": "b54", "title": "Quasiregularity and Its Discontents: The Legacy of the Past Tense Debate", "journal": "Cognitive Science", "year": "2014", "authors": "S Mark; David C Seidenberg;  Plaut"}, {"ref_id": "b55", "title": "Masked language modeling and the distributional hypothesis: Order word matters pre-training for little", "journal": "CoRR", "year": "2021", "authors": "Koustuv Sinha; Robin Jia; Dieuwke Hupkes; Joelle Pineau; Adina Williams; Douwe Kiela"}, {"ref_id": "b56", "title": "Intrinsic probing through dimension selection", "journal": "", "year": "2020", "authors": "Adina Lucas Torroba Hennigen; Ryan Williams;  Cotterell"}, {"ref_id": "b57", "title": "The subsegmental structure of German plural allomorphy. Natural Language & Linguistic Theory", "journal": "", "year": "2020", "authors": "Jochen Trommer"}, {"ref_id": "b58", "title": "Informationtheoretic probing with minimum description length", "journal": "", "year": "2020", "authors": "Elena Voita; Ivan Titov"}, {"ref_id": "b59", "title": "A non-linear structural probe", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Jennifer C White; Tiago Pimentel; Naomi Saphra; Ryan Cotterell"}, {"ref_id": "b60", "title": "Quantifying the semantic core of gender systems", "journal": "", "year": "2019", "authors": "Adina Williams; Damian Blasi; Lawrence Wolf-Sonkin; Hanna Wallach; Ryan Cotterell"}, {"ref_id": "b61", "title": "Predicting declension class from form and meaning", "journal": "", "year": "2020", "authors": "Adina Williams; Tiago Pimentel; Hagen Blix; Arya D Mccarthy; Eleanor Chodroff; Ryan Cotterell"}, {"ref_id": "b62", "title": "The price of linguistic productivity: how children learn to break the rules of language", "journal": "The MIT Press", "year": "2016", "authors": "Charles D Yang"}, {"ref_id": "b63", "title": "No matter how hard we try: Still no default plural marker in nonce nouns in Modern High German", "journal": "University of Bamberg Press", "year": "2016", "authors": "Eugen Zaretsky; P Benjamin;  Lange"}, {"ref_id": "b64", "title": "ADADELTA: an adaptive learning rate method", "journal": "CoRR", "year": "2012", "authors": "Matthew D Zeiler"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Illustration of how our models predict plural nouns. Line thickness indicates performance per plural class; colour gradients show how that performance increases while the encoder processes the word.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Training accuracy across epochs for the five plural classes, and for the -s nouns with stems ending in vowels and consonants, separately.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Cumulative distribution of the predicted suffix for training samples with -s and -er as plural class.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Predicted plural classes change when new gender control tokens (<f>, <m>, <n>) are enforced for nouns from the Wiktionary validation data that normally would not have that grammatical gender.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "4. 11Setup At the end of a training epoch, we extract the model's representations for a subset of the training data and the validation data. The training data subset contains an equal number of samples for each plural class.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Performance for DCs trained and evaluated with data per time step, separately. Negative time steps are relative to the position of the EOS token (position -1). In (a), performance is shown for the concatenation of the hidden and memory cell states. (b) shows the final time step for the remaining model components.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: DC F 1 -score when training on representations from one time step, and testing on representations from another, (a) averaged over all plural classes, and (b) shown for -(e)n only.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: DCs trained on the concatenated hidden and memory cell states for seven training epochs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 9: T-SNE visualisations of hidden and memory cell states. The dimensions t-SNE uses vary per figure and are those most relevant to the plural class in the caption. Colour schemes show (a, b) gender tags, (c, d) lengths of singular nouns, (e) the most frequent last letters of singular nouns, (f) vowels occurring as the last letter. Grey approximately marks the area in which the DC predictions match the plural class in the caption.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 10 :10Figure 10: The results of interventions, showing target class distribution for (a) interventions that changed the suffix of the prediction, and (b) those that did not.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": The German plural system as represented inthe Wiktionary dataset with examples, along with in-flection class frequency and average (singular form)word length."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "] 97.8\u00b1.3 93.9\u00b1.2 94.0\u00b1.1 Unidirectional noun 95.8\u00b1.5 87.8\u00b1.6 87.9\u00b1.5 noun[-1] 97.4\u00b1.2 92.2\u00b1.2 92.3\u00b1.3 Belth et al.", "figure_data": "ModelMeasure Train ValidationTestBidirec. & att.noun 97.4\u00b1.3 92.9\u00b1.2 93.0\u00b1.2noun[-1noun 99.9\u00b1078.8\u00b1078.2\u00b10noun[-1] 99.9\u00b1089.2\u00b1089.0\u00b10"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Accuracy (noun) and final letter accuracy (noun[-1]), with standard deviations over seeds.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance (F 1 ) of the plural class of the models' outputs for the validation set, for several ma-", "figure_data": "jority baselines, conditioned on 1) gender tag, 2) finalletter(s) of the singular form, or 3) both.neural model reflect a specific linguistic property.DCs are simple classifiers that are trained to predictthat property from the representation. The DC'sperformance on new data is assumed indicative ofwhether the linguistic property was, in fact, en-coded."}], "formulas": [{"formula_id": "formula_0", "formula_text": "n = 1 n = 2 n = 1 n = 2 -(e)", "formula_coordinates": [5.0, 77.5, 85.74, 204.99, 23.52]}], "doi": "10.18653/v1/P17-1080"}