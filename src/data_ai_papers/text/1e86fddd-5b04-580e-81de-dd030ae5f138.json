{"title": "HC-Search: Learning Heuristics and Cost Functions for Structured Prediction", "authors": "Janardhan Rao; Alan Fern; Prasad Tadepalli", "pub_date": "", "abstract": "Structured prediction is the problem of learning a function from structured inputs to structured outputs. Inspired by the recent successes of search-based structured prediction, we introduce a new framework for structured prediction called HC-Search. Given a structured input, the framework uses a search procedure guided by a learned heuristic H to uncover high quality candidate outputs and then uses a separate learned cost function C to select a final prediction among those outputs. We can decompose the regret of the overall approach into the loss due to H not leading to high quality outputs, and the loss due to C not selecting the best among the generated outputs. Guided by this decomposition, we minimize the overall regret in a greedy stagewise manner by first training H to quickly uncover high quality outputs via imitation learning, and then training C to correctly rank the outputs generated via H according to their true losses. Experiments on several benchmark domains show that our approach significantly outperforms the state-of-the-art methods.", "sections": [{"heading": "Introduction", "text": "We consider the problem of structured prediction, where the predictor must produce a structured output given a structured input. For example, in part of speech tagging, the inputs and outputs are sequences of words and part of speech tags respectively. A standard approach to structured prediction is to learn a cost function C(x, y) for scoring a potential structured output y given a structured input x. Given such a cost function and a new input x, the output computation involves solving the so-called \"Argmin\" problem, which is to find the minimum cost output for a given input. Unfortunately exactly solving the Argmin problem is intractable except in limited cases such as when the dependency structure among features forms a tree (Lafferty, McCallum, and Pereira 2001;Taskar, Guestrin, and Koller 2003;Tsochantaridis et al. 2004). For more complex structures, heuristic inference methods such as loopy belief propagation and variational inference have shown some success in practice. However, the learning algorithms generally assume exact inference and their behavior in the context of heuristic inference is not well understood.\nCopyright c 2013, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nAnother approach to dealing with the Argmin problem is to eliminate it altogether and instead attempt to learn a classifier that can greedily produce a structured output by making a series of discrete decisions (Dietterich, Hild, and Bakiri 1995;Hal Daum\u00e9 III, Langford, and Marcu 2009;Ross, Gordon, and Bagnell 2011). Unfortunately, in many problems, some decisions are difficult to make by a greedy classifier, but are crucial for good performance. Cascade models (Felzenszwalb and McAllester 2007;Weiss, Sapp, and Taskar 2010) achieve efficiency by performing inference on a sequence of models from coarse to finer levels. However, cascading places strong restrictions on the form of the cost functions.\nWe are inspired by the recent successes of output-space search approaches, which place few restrictions on the form of the cost function (Doppa, Fern, and Tadepalli 2012;Wick et al. 2011). These methods learn and use a cost function to direct a combinatorial search through a space of outputs, and return the least cost output uncovered. While these approaches have achieved state-of-the-art performance on a number of benchmark problems, a primary contribution of this paper is to highlight a fundamental deficiency that they share. In particular, prior work uses a single cost function to serve the dual roles of both: 1) guiding the search toward good outputs, and 2) scoring generated outputs in order to select the best one. Serving these dual roles often means that the cost function needs to make unclear tradeoffs, increasing the difficulty of learning.\nIn this paper, we study a new framework for structured prediction called HC-Search that closely follows the traditional search literature. The key idea is to learn distinct functions for each of the above roles: 1) a heuristic function H to guide the search and generate a set of high-quality candidate outputs, and 2) a cost function C to score the outputs generated by the heuristic H. Given a structured input, predictions are made by using H to guide a search strategy (e.g., greedy search or beam search) until a time bound and then returning the generated output of least cost according to C.\nWhile the move to HC-Search might appear to be relatively small, there are significant implications in terms of both theory and practice. First, the regret of the HC-Search approach can be decomposed into the loss due to H not leading to high quality outputs, and the loss due to C not selecting the best among the generated outputs. This de-composition helps us target our training to minimize each of these losses individually in a greedy stage-wise manner. Second, as we will show, the performance of the approaches with a single function can be arbitrarily bad when compared to that of HC-Search in the worst case. Finally, we show that in practice HC-Search performs significantly better than the single cost function search and other state-of-the-art approaches to structured prediction. In addition to providing overall empirical results, we provide an in-depth analysis in terms of our loss decomposition that more precisely identifies the advantages over prior work and the reasons for our own failures.\nFinally, we note that our methodology can be viewed as similar in spirit to Re-Ranking (Collins 2000), which uses a generative model to propose a k-best list of outputs, which are then ranked by a separate ranking function. In contrast, we search in efficient search spaces guided by a learned heuristic that has minimal restrictions on its representation.", "publication_ref": ["b14", "b18", "b19", "b3", "b17", "b5", "b22", "b4", "b24", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "HC-Search", "text": "Problem Setup. A structured prediction problem specifies a space of structured inputs X , a space of structured outputs Y, and a non-negative loss function L : X \u00d7 Y \u00d7 Y \u2192 + such that L(x, y , y * ) is the loss associated with labeling a particular input x by output y when the true output is y * . We are provided with a training set of input-output pairs {(x, y * )} drawn from an unknown target distribution D. The goal is to return a function/predictor from structured inputs to outputs whose predicted outputs have low expected loss with respect to the distribution D. Since our algorithms will be learning heuristics and cost functions over input-output pairs, as is standard in structured prediction, we assume the availability of a feature function \u03a6 : X \u00d7 Y \u2192 n that computes an n dimensional feature vector for any pair.\nSearch Spaces. Our approach is based on search in the space S o of complete outputs, which we assume to be given. Every state in a search space over complete outputs consists of pairs of inputs and outputs (x, y), representing the possibility of predicting y as the output for x. Such a search space is defined in terms of two functions: 1) An initial state function I such that I(x) returns an initial state for input x, and 2) A successor function S such that for any search state (x, y), S((x, y)) returns a set of next states {(x, y 1 ), \u2022 \u2022 \u2022 , (x, y k )} that share the same input x as the parent. For example, in a sequence labeling problem, such as part-of-speech tagging, (x, y) is a sequence of words and corresponding part-of-speech (POS) labels. The successors of (x, y) might correspond to all ways of changing one of the output labels in y (the so-called flip-bit space).\nA variety of search spaces, such as the above flip-bit space, Limited Discrepancy Search (LDS) space (Doppa, Fern, and Tadepalli 2012), and those defined based on handdesigned proposal distributions (Wick et al. 2011) have been used in past research. While our work applies to any such space, we will focus on the LDS space in our experiments. The LDS space is defined in terms of a recurrent classifier which uses the next input token, e.g. word, and output tokens in a small preceding window, e.g. POS labels, to predict the next output token. The successors of (x, y) here consist of the results of running a recurrent classifier after changing one more label (introducing a discrepancy) in the current sequence y. In previous work, the LDS space is shown to be effective in uncovering high-quality outputs at relatively shallow search depths (Doppa, Fern, and Tadepalli 2012).\nOur Approach. Our approach is parameterized by a search space, a heuristic search strategy A (e.g., greedy search), a learned heuristic function H : X \u00d7 Y \u2192 , and a learned cost function C : X \u00d7 Y \u2192 . Given an input x and a time bound \u03c4 , we generate an output by running A starting at I(x) and guided by H until the time bound is exceeded. Each output uncovered by the search is scored according to C and we return the least-cost one as the final output.\nMore formally, let Y H (x) be the set of candidate outputs generated using heuristic H for a given input x. Further let y * H be the best loss output in this set, and\u0177 be the best cost output according to C, i.e.,\ny * H = arg min y\u2208Y H (x) L(x, y, y * ) y = arg min y\u2208Y H (x) C(x, y)\nHere, y * H is the best output that our approach could possibly return when using H and\u0177 is the output that it will actually return. The expected loss of the HC-Search approach E(H, C) for a given heuristic H and C can be defined as\nE (H, C) = E (x,y * )\u223cD L (x,\u0177, y * ) (1)\nOur goal is to learn a heuristic function H * and corresponding cost function C * from their respective spaces H and C that minimizes the expected loss, i.e.,\n(H * , C * ) = arg min (H,C)\u2208H\u00d7C E (H, C)(2)\nLearning Complexity. We now consider the feasibility of an exact solution to this learning problem in the simplest setting of greedy search using linear heuristic and cost functions represented by their weight vectors w H and w C respectively. In particular, we consider the HC-Search Consistency Problem, where the input is a training set of structured examples, and we must decide whether or not there exists w H and w C such that HC-Search using greedy search will achieve zero loss on the training set. We first note, that this problem can be shown to be NP-Hard by appealing to results on learning for beam search (Xu, Fern, and Yoon 2009). In particular, results there imply that in all but trivial cases, simply determining whether or not there is a linear heuristic w H that uncovers a zero loss node is NP-Hard. Since HC-Search can only return zero loss outputs when the heuristic is able to uncover them, we see that our problem is also hard.\nHere we prove a stronger result that provides more insight into the HC-Search framework. In particular, we show that even when it is \"easy\" to learn a heuristic that uncovers all zero loss outputs, the consistency problem is still hard. This shows, that in the worst case the hardness of our learning problem is not simply a result of the hardness of discovering good outputs. Rather our problem is additionally complicated by the potential interaction between H and C. Intuitively, when learning H in the worst case there can be ambiguity about which of many good outputs to generate, and for only some of those will we be able to find an effective C to return the best one. We prove the following theorem.\nTheorem 1. The HC-Search Consistency Problem for greedy search and linear heuristic and cost functions is NP-Hard even when we restrict to problems for which all possible heuristic functions uncover all zero loss outputs.\nProof. (Sketch) We reduce from the Minimum Disagreement problem for linear binary classifiers, which was proven to be NP-complete by (Hoffgen, Simon, and Horn 1995). In one statement of this problem we are given as input a set of N , m-dimensional vectors T = {x 1 , . . . , x N } and a positive integer k. The problem is to decide whether or not there is an m-dimensional real-valued weight vector w such that w \u2022 x i < 0 for at most k of the vectors.\nThe reduction is relatively detailed and here we sketch the main idea. Given an instance of Minimum Disagreement, we construct an HC-Search consistency problem with only a single structured training example. The search space corresponding to the training example is designed such that there is a single node n * that has a loss of zero and all other nodes have a loss of 1. Further for all linear heuristic functions all greedy search paths terminate at n * , while generating some other set of nodes/outputs on the path there. The search space is designed such that each possible path from the initial node to n * corresponds to selecting k or fewer vectors from T , which we will denote by T \u2212 . By traversing the path, the set of nodes generated (and hence must be scored by C), say N , includes feature vectors corresponding to those in T \u2212 T \u2212 along with the negation of the feature vectors in T \u2212 . We further define n * to be assigned the zero vector, so that the cost of that node is 0 for any weight vector.\nIn order to achieve zero loss given the path in consideration, there must be a weight vector w C such that w C \u2022 x \u2265 0 for all x \u2208 N . By our construction this is equivalent to w C \u2022 x < 0 for x \u2208 T \u2212 . If this is possible then we have found a solution to the Minimum Disagreement problem since |T \u2212 | \u2264 k. The remaining details show how to construct this space so that there is a setting of the heuristic weights that can generate paths corresponding to all possible T \u2212 in a way that all paths end at n * . Loss Decomposition. The above suggests that in general learning the optimal (H * , C * ) pair is impractical due to their potential interdependence. However, we observe a decomposition of the above error that leads to an effective training approach. In particular, the expected loss E (H, C) can be decomposed into two parts: 1) the generation loss H , due to the heuristic H not generating high-quality outputs, and 2) the selection loss C|H , the additional loss (conditional on H) due to the cost function C not selecting the best loss output generated by the heuristic.\nE (H, C) = E (x,y * )\u223cD L (x, y * H , y * ) H + E (x,y * )\u223cD L (x,\u0177, y * ) \u2212 L (x, y * H , y * ) C|H (3)\nAs described in the next section, this decomposition allows us to specifically target our training to minimize each of Figure 1: An example that illustrates that C-Search can suffer arbitrarily large loss compared to HC-Search.\nthese errors separately. In addition, as we show in our experiments, the terms of this decomposition can be easily measured for a learned (H, C) pair, which allows for an assessment of which function is more responsible for the loss.\nIn contrast to our approach, existing approaches for output space search (Doppa, Fern, and Tadepalli 2012;Wick et al. 2011) use a single function (say C) to serve the dual purpose of heuristic and cost function. This raises the question of whether HC-Search, which uses two different functions, is strictly more powerful in terms of its achievable losses. It turns out that the expected loss of HC-Search can be arbitrarily smaller than when restricting to using a single function C as the following proposition shows. Proof. The first part of the proposition follows from the fact that the first minimization is over a subset of the choices considered by the second. To see the second part, consider a problem with a single training instance with search space shown in Figure 1. We assume greedy search and linear C and H functions of features \u03a6(n). Node 7 is the least loss output. To find it, the heuristic H needs to satisfy, H(3)<H(2), and H(7)<H(6), which are easily satisfied. To return node 7 as the final output, the cost function must satisfy, C(7)<C(1), C(7)<C(2), C(7)<C(3), and C(7)<C(6). Again, it is possible to satisfy these, and HC-Search can achieve zero loss on this problem. However, it can be verified that there is no single set of weights that simultaneously satisfies both sets of constraints (C(3)<C(2) and C( 7)<C(1) in particular) as required by the C-Search. By scaling the losses by constant factors we can make the loss suffered arbitrarily high.", "publication_ref": ["b4", "b24", "b4", "b25", "b11", "b4", "b24"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Learning Approach", "text": "Guided by the error decomposition in Equation 3, we optimize the overall error of the HC-Search approach in a greedy stage-wise manner. We first train a heuristic\u0124 in order to optimize the heuristic loss component H and then train a cost function\u0108 to optimize the cost function loss C|\u0124 con-\nditioned on\u0124.\u0124 \u2248 arg min H\u2208H \u0124 C \u2248 arg min C\u2208C C|\u0124\nIn what follows, we first describe a generic approach for heuristic function learning that is applicable for a wide range of search spaces and search strategies, and then explain our cost function learning algorithm.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Heuristic Function Learning", "text": "Most generally, learning a heuristic can be viewed as a Reinforcement Learning (RL) problem where the heuristic is viewed as a policy for guiding \"search actions\" and rewards are received for uncovering high quality outputs. In fact, this approach has been explored for structured prediction in the case of greedy search (Wick et al. 2009) and was shown to be effective given a carefully designed reward function and action space. While this is a viable approach, general purpose RL can be quite sensitive to the algorithm parameters and specific definition of the reward function and actions, which can make designing an effective learner quite challenging. Indeed, recent work (Jiang et al. 2012), has shown that generic RL algorithms can struggle for some structured prediction problems, even with significant effort put forth by the designer. Rather, in this work, we follow an approach based on imitation learning, that makes stronger assumptions, but has nevertheless been very effective and easy to apply across a variety of problem.\nOur heuristic learning approach is based on the observation that for many structured prediction problems, we can quickly generate very high-quality outputs by guiding the search procedure using the true loss function L as a heuristic (only known for the training data). This suggests formulating the heuristic learning problem in the framework of imitation learning by attempting to learn a heuristic that mimics the search decisions made by the true loss function on training examples. The learned heuristic need not approximate the true loss function uniformly over the output space, but need only make the distinctions that were important for guiding the search. The main assumptions made by this approach are: 1) the true loss function can provide effective heuristic guidance to the search procedure, so that it is worth imitating, and 2) we can learn to imitate those search decisions sufficiently well.\nThis imitation learning approach is similar to prior work on learning single cost functions for output-space search (Doppa, Fern, and Tadepalli 2012). However, a key distinction here is that learning is focused on only making distinctions necessary for uncovering good outputs (the purpose of the heuristic) and hence requires a different formulation. As in prior work, in order to avoid the need to approximate the loss function arbitrarily closely, we restrict ourselves to \"rank-based\" search strategies. A search strategy is called rank-based if it makes all its search decisions by comparing the relative values of the search nodes (their ranks) assigned by the heuristic, rather than being sensitive to absolute values of heuristic. Most common search procedures such as greedy search, beam search, and best-first search fall under this category.\nImitating Search Behavior. Given a search space over complete outputs S o , a rank-based search procedure A, and a search time bound \u03c4 , our learning procedure generates imitation training data for each training example (x, y * ) as follows. We run the search procedure A for a time bound of \u03c4 for input x using a heuristic equal to the true loss function, i.e. H(x, y) = L(x, y, y * ). During the search process we observe all of the pair-wise ranking decisions made by A using this oracle heuristic and record those that are sufficient (see below) for replicating the search. If the state (x, y 1 ) has smaller loss than (x, y 2 ), then a ranking example is generated in the form of the constraint H(x, y 1 )<H(x, y 2 ). Ties are broken using a fixed arbitrator. The aggregate set of ranking examples collected over all the training examples is then given to a learning algorithm to learn the weights of the heuristic function. In our implementation we employed the margin-scaled variant of the online Passive-Aggressive algorithm (see Equation 47in (Crammer et al. 2006)) as our base learner to train the heuristic function, which we have found to be quite effective yet efficient.\nIf we can learn a function H from hypothesis space H that is consistent with these ranking examples, then the learned heuristic is guaranteed to replicate the oracle-guided search on the training data. Further, given assumptions on the base learning algorithm (e.g. PAC), generic imitation learning results can be used to give generalization guarantees on the performance of search on new examples (Khardon 1999;Fern, Yoon, and Givan 2006;Ross and Bagnell 2010). Our experiments show, that the simple approach described above, performs extremely well on our problems.\nAbove we noted that we only need to collect and learn to imitate the \"sufficient\" pairwise decisions encountered during search. We say that a set of constraints is sufficient for a structured training example (x, y * ), if any heuristic that is consistent with the constraints causes the search to generate the same set of outputs for input x. The precise specification of these constraints depends on the search procedure. For space reasons, here we only describe the generation of sufficient constraints for greedy search, which we use in our experiments. Similar formulations for other rank-based search strategies is straightforward.\nGreedy search simply starts at the initial state, selects the best successor state according to the heuristic, and then repeats this process for the selected state. At every search step, the best node (x, y best ) from the candidate set C (i.e. successors of current search state) needs to be ranked better (lower H-value) than every other node. Therefore, we include one ranking constraint for every node (x, y) \u2208 C \\(x, y best ) such that H(x, y best )<H(x, y).", "publication_ref": ["b23", "b12", "b4", "b2", "b13", "b6", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Cost Function Learning", "text": "Given a learned heuristic H, we now want to learn a cost function that correctly ranks the potential outputs generated using H. More formally, let l best be the loss of the best output among those outputs as evaluated by the true loss function L, i.e., l best = min y\u2208Y H (x) L(x, y, y * ). The specific goal of learning is, then, to find the parameters of a cost function C such that for every training example (x, y * ), the loss of the minimum cost output\u0177 equals l best , i.e., L(x,\u0177, y * ) = l best , where\u0177 = arg min y\u2208Y H (x) C(x, y).\nWe formulate the cost function training problem similarly to traditional learning to rank problems (Agarwal and Roth 2005). More specifically, we want all the best loss outputs in Y H (x) to be ranked better than all the non-best loss outputs according to our cost function, which is a bi-partite ranking problem. Let Y best be the set of all best loss outputs from Y H (x), i.e., Y best = {y \u2208 Y H (x)|L(x, y, y * ) = l best }. We generate one ranking example for every pair of outputs (y best , y) \u2208 Y best \u00d7 Y H (x) \\ Y best , requiring that C(x, y best )<C(x, y).\nIt is important to note that both in theory and practice, the distribution of outputs generated by the learned heuristic H on the testing data may be slightly different from the one on training data. Thus, if we train C on the training examples used to train H, then C is not necessarily optimized for the test distribution. To mitigate this effect, we train our cost function via cross validation (see Algorithm 1) by training the cost function on the data, which was not used to train the heuristic. This training methodology is commonly used in Re-ranking style algorithms (Collins 2000) among others. We use the online Passive-Aggressive algorithm (Crammer et al. 2006) as our rank learner, exactly as configured for the heuristic learning. \nC = Rank-Learner(\u222a k i=1 R i )", "publication_ref": ["b0", "b1", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments and Results", "text": "Datasets. We evaluate our approach on the following four structured prediction problems (three benchmark sequence labeling problems and a 2D image labeling problem): 1) Handwriting Recognition (HW). The input is a sequence of binary-segmented handwritten letters and the output is the corresponding character sequence [a\u2212z] + . This dataset contains roughly 6600 examples divided into 10 folds (Taskar, Guestrin, and Koller 2003). We consider two different variants of this task as in (Hal Daum\u00e9 III, Langford, and Marcu 2009); in HW-Small version, we use one fold for training and remaining 9 folds for testing, and vice-versa in HW-Large. 2) NETtalk Stress. The task is to assign one of the 5 stress labels to each letter of a word. There are 1000 training words and 1000 test words in the standard dataset. We use a sliding window of size 3 for observational features.\n3) NETtalk Phoneme. This is similar to NETtalk Stress ex-cept that the task is to assign one of the 51 phoneme labels to each letter of the word. 4) Scene labeling. This dataset contains 700 images of outdoor scenes (Vogel and Schiele 2007). Each image is divided into patches by placing a regular grid of size 10 \u00d7 10 and each patch takes one of the 9 semantic labels (sky, water, grass, trunks, foliage, field, rocks, flowers, sand). Simple appearance features like color, texture and position are used to represent each patch. Training was performed with 600 images and the remaining 100 images were used for testing.\nExperimental setting. For our HC-Search experiments, we use the Limited Discrepancy Space (LDS) exactly as described in (Doppa, Fern, and Tadepalli 2012) as our search space over structured outputs. Prior work (Doppa, Fern, and Tadepalli 2012) and our own experience with HC-Search has shown that greedy search works quite well for most structured prediction tasks, particularly when using the LDS space. Hence, we consider only greedy search in our experiments, noting that experiments not shown using beam search and best first search produce similar results. During training and testing we set the search time bound \u03c4 to be 15 search steps for all domains except for scene labeling, which has a much larger search space and uses \u03c4 = 150. For all domains, we learn linear heuristic and cost functions over second order features unless otherwise noted. In this case, the feature vector measures features over neighboring label pairs and triples along with features of the structured input. We measure error with Hamming loss in all cases.\nComparison to state-of-the-art. We compare the results of our HC-Search approach with other structured prediction algorithms including CRFs (Lafferty, McCallum, and Pereira 2001), SVM-Struct (Tsochantaridis et al. 2004), Searn (Hal Daum\u00e9 III, Langford, andMarcu 2009), Cascades  and C-Search, the most similar approach to ours, which uses a single-function for output space search (Doppa, Fern, and Tadepalli 2012). We also show the performance of Recurrent, which is a simple recurrent classifier trained exactly as in (Doppa, Fern, and Tadepalli 2012). The top section of Table 1 shows the error rates of the different algorithms. For scene labeling it was not possible to run CRFs, SVM-Struct, and Cascades due to the complicated grid structure of the outputs (hence the '-' in the table). We report the best published results of CRFs, SVM-Struct, Searn and Cascades (see (Doppa, Fern, and Tadepalli 2012)). Across all benchmarks we see that results of HC-Search are comparable or significantly better than the state-of-the-art including C-Search. The results in the scene labeling domain are the most significant improving the error rate from 27.05 to 19.71. These results show that HC-Search is a state-of-the-art approach across these problems and that learning separate heuristic and cost functions can significantly improve output-space search.\nHigher-Order Features. One of the advantages of our approach compared to many frameworks for structured prediction is the ability to use more expressive feature spaces without paying a huge computational price. The bottom part of Table 1 shows results using third-order features (compared to second-order above) for HC-Search, C-Search and  Cascades 1 . Note that it is not practical to run the other methods using third-order features due to the substantial increase in inference time. The overall error of HC-Search with higher-order features slightly improved compare to using second-order features across all benchmarks and is still better than the error-rates of C-Search and Cascades with third-order features. In fact, HC-Search using only secondorder features is still outperforming the third-order results of the other methods across the board. Finally, we note that while Cascades is able to significantly improve performance in two of its four data sets by using third-order features, in the other two benchmarks performance degrades. Loss Decomposition Analysis. We now examine HC-Search and C-Search in terms their loss decomposition (see Equation 3) into generation loss H and selection loss C|H . Both of these quantities can be easily measured for both HC-Search and C-Search by keeping track of the best loss output generated by the search (guided either by a heuristic or the cost function for C-Search) across the testing examples. Table 2 shows these results, giving the overall error and its decomposition across our benchmarks for both HC-Search and C-Search.\nWe first see that generation loss H is very similar for C-Search and HC-Search across the benchmarks with the exception of scene labeling, where HC-Search generates slightly better outputs. This shows that at least for the LDS search space the difference in performance between C-Search and HC-Search cannot be explained by C-Search generating lower quality outputs. Rather, the difference between the two methods is most reflected by the difference in selection loss C|H , meaning that C-Search is not as effective 1 http://code.google.com/p/structured-cascades/ at ranking the outputs generated during search compared to HC-Search. This result clearly shows the advantage of separating the roles of C and H and is understandable in light of the training mechanism for C-Search. In that approach, the cost function is trained to satisfy constraints related to both the generation loss and selection loss. It turns out that there are many more generation-loss constraints, which we hypothesize biases C-Search toward low generation loss at the expense of selection loss.\nThese results also show that for both methods the selection loss C|H contributes significantly more to the overall error compared to H . This shows that both approaches are able to uncover very high-quality outputs, but are unable to correctly rank the generated outputs according to their losses. This suggests that a first avenue for improving the results of HC-Search would be to improve the cost function learning component, e.g. by using non-linear cost functions.\nOracle Heuristic Results. Noting that there is room to improve the generation loss, the above results do not indicate whether improving this loss via a better learned heuristic would lead to better results overall. To help evaluate this we ran an experiment where we gave HC-Search the true loss function to use as a heuristic (an oracle heuristic), i.e., H(x, y) = L(x, y, y * ), during both training of the cost function and testing. This provides an assessment of how much better we might be able to do if we could improve heuristic learning. The results in Table 2, which we label as LC-Search (Oracle H) show that when using the oracle heuristic, H is negligible as we might expect and smaller than observed for HC-Search. This shows that it may be possible to further improve our heuristic learning via better imitation. We note that we have experimented with more sophisticated imitation learning algorithms (e.g., Dagger (Ross, Gordon, and Bagnell 2011)), but did not see significant improvements.\nWe also see from the oracle results that the overall error is better than that of HC-Search. This indicates that our cost function learner is able to leverage, to varying degrees, the better outputs produced by the oracle heuristic. This suggests that improving the heuristic learner in order to reduce the generation loss could be a viable way of further reducing the overall loss of HC-Search, even without altering the current cost learner. However, as we saw above there is much less room to improve the heuristic learner for these data sets and hence the potential gains are less than for directly trying to improve the cost learner.", "publication_ref": ["b18", "b20", "b4", "b4", "b14", "b10", "b4", "b4", "b4", "b17"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_1"]}, {"heading": "Conclusions and Future Work", "text": "We introduced the HC-Search framework for structured prediction whose principal feature is the separation of the cost function from search heuristic. We showed that our framework yields significantly superior performance to state-ofthe-art results, and allows an informative error analysis and diagnostics. Our investigation showed that the main source of error of existing output-space approaches including our own approach (HC-Search) is the inability of cost function to correctly rank the candidate outputs produced by the heuristic. This analysis suggests that learning more powerful cost functions, e.g., Regression trees (Mohan, Chen, and Weinberger 2011), with an eye towards anytime performance (Grubb and Bagnell 2012;Xu, Weinberger, and Chapelle 2012) would be productive. Our results also suggested that there is also room to improve overall performance with better heuristic learning. Thus, another direction to pursue is heuristic function learning to speed up the process of generating high-quality outputs (Fern 2010).", "publication_ref": ["b15", "b8", "b26", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported in part by NSF grants IIS 1219258, IIS 1018490 and in part by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL) under Contract No. FA8750-13-2-0033. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF, the DARPA, the Air Force Research Laboratory (AFRL), or the US government.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learnability of bipartite ranking functions", "journal": "", "year": "2005", "authors": "S Agarwal; Roth ; D "}, {"ref_id": "b1", "title": "Discriminative reranking for natural language parsing", "journal": "", "year": "2000", "authors": "M Collins"}, {"ref_id": "b2", "title": "Online passive-aggressive algorithms", "journal": "JMLR", "year": "2006", "authors": "K Crammer; O Dekel; J Keshet; S Shalev-Shwartz; Y Singer"}, {"ref_id": "b3", "title": "A comparison of ID3 and backpropagation for english text-to-speech mapping", "journal": "MLJ", "year": "1995", "authors": "T G Dietterich; H Hild; G Bakiri"}, {"ref_id": "b4", "title": "Output space search for structured prediction", "journal": "", "year": "2012", "authors": "J R Doppa; A Fern; P Tadepalli"}, {"ref_id": "b5", "title": "The generalized A* architecture", "journal": "JAIR", "year": "2007", "authors": "P F Felzenszwalb; D A Mcallester"}, {"ref_id": "b6", "title": "Approximate policy iteration with a policy language bias: Solving relational Markov decision processes", "journal": "JAIR", "year": "2006", "authors": "A Fern; S W Yoon; R Givan"}, {"ref_id": "b7", "title": "Speedup learning", "journal": "", "year": "2010", "authors": "A Fern"}, {"ref_id": "b8", "title": "Speedboost: Anytime prediction with uniform near-optimality", "journal": "JMLR Proceedings Track", "year": "2012", "authors": "A Grubb; D Bagnell"}, {"ref_id": "b9", "title": "", "journal": "", "year": "", "authors": "Hal Daum\u00e9; Iii "}, {"ref_id": "b10", "title": "Searchbased structured prediction", "journal": "MLJ", "year": "2009", "authors": "J Langford; D Marcu"}, {"ref_id": "b11", "title": "Robust trainability of single neurons", "journal": "Journal of Computer and System Sciences", "year": "1995", "authors": "K.-U Hoffgen; H.-U Simon; K S V Horn"}, {"ref_id": "b12", "title": "Learned prioritization for trading off accuracy and speed", "journal": "", "year": "2012", "authors": "J Jiang; A Teichert; Iii Daum\u00e9; H Eisner; J "}, {"ref_id": "b13", "title": "Learning to take actions", "journal": "Machine Learning", "year": "1999", "authors": "R Khardon"}, {"ref_id": "b14", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "", "year": "2001", "authors": "J Lafferty; A Mccallum; F Pereira"}, {"ref_id": "b15", "title": "Websearch ranking with initialized gradient boosted regression trees", "journal": "JMLR Proceedings Track", "year": "2011", "authors": "A Mohan; Z Chen; K Q Weinberger"}, {"ref_id": "b16", "title": "Efficient reductions for imitation learning", "journal": "", "year": "2010", "authors": "S Ross; D Bagnell"}, {"ref_id": "b17", "title": "A reduction of imitation learning and structured prediction to no-regret online learning", "journal": "", "year": "2011", "authors": "S Ross; G Gordon; D Bagnell"}, {"ref_id": "b18", "title": "Max-margin markov networks", "journal": "", "year": "2003", "authors": "B Taskar; C Guestrin; D Koller"}, {"ref_id": "b19", "title": "Support vector machine learning for interdependent and structured output spaces", "journal": "", "year": "2004", "authors": "I Tsochantaridis; T Hofmann; T Joachims; Y Altun"}, {"ref_id": "b20", "title": "Semantic modeling of natural scenes for content-based image retrieval", "journal": "IJCV", "year": "2007", "authors": "J Vogel; B Schiele"}, {"ref_id": "b21", "title": "Structured prediction cascades", "journal": "", "year": "2010", "authors": "D Weiss; B Taskar"}, {"ref_id": "b22", "title": "Sidestepping intractable inference with structured ensemble cascades", "journal": "", "year": "2010", "authors": "D Weiss; B Sapp; B Taskar"}, {"ref_id": "b23", "title": "Training factor graphs with reinforcement learning for efficient map inference", "journal": "", "year": "2009", "authors": "M L Wick; K Rohanimanesh; S Singh; A Mccallum"}, {"ref_id": "b24", "title": "Samplerank: Training factor graphs with atomic gradients", "journal": "", "year": "2011", "authors": "M L Wick; K Rohanimanesh; K Bellare; A Culotta; A Mccallum"}, {"ref_id": "b25", "title": "Learning linear ranking functions for beam search with application to planning", "journal": "JMLR", "year": "2009", "authors": "Y Xu; A Fern; S Yoon"}, {"ref_id": "b26", "title": "The greedy miser: Learning under test-time budgets", "journal": "", "year": "2012", "authors": "Z Xu; K Weinberger; O Chapelle"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Proposition 1 .1Let H and C be functions from the same function space. Then for all learning problems, min C E(C, C) \u2265 min (H,C) E(H, C). Moreover there exist learning problems for which min C E(C, C) is arbitrarily larger (i.e. worse) than min (H,C) E(H, C).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Algorithm 11Cost Function Learning via Cross Validation Input: training examples T = { x, y * }, search strategy A, time bound \u03c4 , loss function L. 1: Divide the training set T into k folds such that T = \u222a k i=1 f old i 2: Learn k different heuristics H 1 , \u2022 \u2022 \u2022 , H k : H i = LearnH(T i , L, A, \u03c4 ), where T i = \u222a j =i f old j 3: Generate ranking examples for cost function training using each heuristic H i : R i = Generate-CL-Examples(f old i , H i , A, \u03c4, L) 4: Train cost function on all the ranking examples:", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Error rates of different structured prediction algorithms.", "figure_data": "ALGORITHMSDATASETSHW-SmallHW-LargeStressPhonemeScene labelingHC-Search13.353.7817.4416.0519.71C-Search17.417.4121.1520.9127.05CRF19.9713.1121.4821.09-SVM-Struct19.6412.4922.0121.7-Recurrent34.3325.1327.1826.4243.36Searn17.889.4223.8522.7437.69Cascades30.3812.0522.8230.23-Third-Order FeaturesHC-Search11.243.0316.6114.7518.25C-Search14.154.9219.7918.3125.79Cascades18.826.2426.5231.02-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "HC-Search vs. C-Search: Error decomposition of heuristic and cost function.", "figure_data": "DATASETSHW-SmallHW-LargeStressPhonemeSceneERRORHC|HHC|HHC|HHC|HHC|HHC-Search13.355.477.883.781.342.4417.443.1314.3116.053.9812.0719.715.8213.89C-Search17.415.5911.827.411.515.921.153.2417.9120.914.3816.5327.057.8319.22LC-Search10.120.229.93.070.532.5414.190.2613.9312.250.5111.7416.390.3716.02(Oracle H)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "y * H = arg min y\u2208Y H (x) L(x, y, y * ) y = arg min y\u2208Y H (x) C(x, y)", "formula_coordinates": [2.0, 367.08, 246.56, 266.29, 27.84]}, {"formula_id": "formula_1", "formula_text": "E (H, C) = E (x,y * )\u223cD L (x,\u0177, y * ) (1)", "formula_coordinates": [2.0, 369.57, 326.91, 188.43, 12.03]}, {"formula_id": "formula_2", "formula_text": "(H * , C * ) = arg min (H,C)\u2208H\u00d7C E (H, C)(2)", "formula_coordinates": [2.0, 353.77, 381.1, 204.24, 12.03]}, {"formula_id": "formula_3", "formula_text": "E (H, C) = E (x,y * )\u223cD L (x, y * H , y * ) H + E (x,y * )\u223cD L (x,\u0177, y * ) \u2212 L (x, y * H , y * ) C|H (3)", "formula_coordinates": [3.0, 77.83, 615.22, 214.67, 58.22]}, {"formula_id": "formula_4", "formula_text": "ditioned on\u0124.\u0124 \u2248 arg min H\u2208H \u0124 C \u2248 arg min C\u2208C C|\u0124", "formula_coordinates": [4.0, 54.0, 72.83, 233.73, 44.19]}, {"formula_id": "formula_5", "formula_text": "C = Rank-Learner(\u222a k i=1 R i )", "formula_coordinates": [5.0, 70.94, 480.91, 121.56, 12.32]}], "doi": ""}