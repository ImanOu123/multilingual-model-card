{"title": "Stable Conformal Prediction Sets", "authors": "Eugene Ndiaye", "pub_date": "", "abstract": "When one observes a sequence of variables (x 1 , y 1 ), . . . , (x n , y n ), Conformal Prediction (CP) is a methodology that allows to estimate a confidence set for y n+1 given x n+1 by merely assuming that the distribution of the data is exchangeable. CP sets have guaranteed coverage for any finite population size n. While appealing, the computation of such a set turns out to be infeasible in general, e.g., when the unknown variable y n+1 is continuous. The bottleneck is that it is based on a procedure that readjusts a prediction model on data where we replace the unknown target by all its possible values in order to select the most probable one. This requires computing an infinite number of models, which often makes it intractable. In this paper, we combine CP techniques with classical algorithmic stability bounds to derive a prediction set computable with a single model fit. We demonstrate that our proposed confidence set does not lose any coverage guarantees while avoiding the need for data splitting as currently done in the literature. We provide some numerical experiments to illustrate the tightness of our estimation when the sample size is sufficiently large, on both synthetic and real datasets.", "sections": [{"heading": "Introduction", "text": "Modern machine learning algorithms can predict the label of an object based on its observed characteristics with impressive accuracy. They are often trained on historical datasets sampled from the same distribution and it is important to quantify the uncertainty of their predictions. Conformal prediction is a versatile and simple method introduced in (Vovk et al., 2005;Shafer & Vovk, 2008) that provides a finite sample and distribution free 100(1 \u2212 \u03b1)% confidence region on the predicted object based on past observations. The Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). main idea can be subsumed as a hypothesis testing between H 0 : y n+1 = z and H 1 :\ny n+1 = z ,\nwhere z is any replacement candidate for the unknown response y n+1 .\nThe conformal prediction set will consist of the collection of candidates whose tests are not rejected. The construction of a p-value function is simple. We start by fitting a model with training set {(x 1 , y 1 ), . . . , (x n , y n ), (x n+1 , z)} and sort the prediction scores/errors for each instance in ascending order. A candidate z will be considered as conformal or typical if the rank of its score is sufficiently small compared to the others. The key assumption is that the predictive model and the joint probability distribution of the sequence {(x i , y i )} n+1 i=1 are invariant w.r.t. permutation of the data. As a consequence, the ranks of the scores are equally likely and thus follow a uniform distribution which allow to calibrate a threshold on the rank statistics leading to a valid confidence set. This method has a strong coverage guarantee without any further assumptions on the distribution and is valid for any finite sample size n; see more details in Section 2.\nConformal prediction technique has been applied for designing uncertainty sets in active learning (Ho & Wechsler, 2008), anomaly detection (Laxhammar & Falkman, 2015;Bates et al., 2021), few shot learning (Fisch et al., 2021), time series (Chernozhukov et al., 2018;Xu & Xie, 2021;Chernozhukov et al., 2021), robust optimization (Johnstone & Cox, 2021) or to infer the performance guarantee for statistical learning algorithms (Holland, 2020;Cella & Ryan, 2020). Currently, we are seeing a growing interest in these approaches due to their flexibility and ease of deployment even for very complex problems where classical approaches offer limited performance (Efron, 2021). We refer to (Balasubramanian et al., 2014) for other AI applications.\nDespite its nice properties, the computation of conformal prediction sets requires fitting a model on a new augmented dataset where the unknown quantity y n+1 is replaced by a set of candidates. In a regression setting where an object can take an uncountable possible value, the set of candidates is infinite. Therefore, computing the conformal prediction is infeasible without additional structural assumptions about the underlying model fit, and even so, the current computational costs remain very high. Hence the prevailing recommendation to use less efficient data splitting methods.", "publication_ref": ["b42", "b39", "b23", "b28", "b7", "b18", "b13", "b43", "b14", "b26", "b25", "b11", "b16", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "arXiv:2112.10224v2 [stat.ML] 7 Dec 2022", "text": "Contribution. We leverage algorithmic stability to bound the variation of the predictive model w.r.t. to changes in the input data. This results in a circumvention of the computational bottleneck induced by the necessary readjustment of the model each time we want to assess the typicalness of a candidate replacement of the target variable. As such, we can provide a tight estimation of the confidence sets without loss in the coverage guarantee. Our method is computationally and statistically efficient since it requires only a single model fit and does not involve any data splitting.\nNotation. For a nonzero integer n, we denote [n] to be the set {1, \u2022 \u2022 \u2022 , n}. The dataset of size n is denoted\nD n = (x i , y i ) i\u2208[n]\n, the row-wise input feature matrix\nX = [x 1 , \u2022 \u2022 \u2022 , x n , x n+1 ] . Given a set {u 1 , \u2022 \u2022 \u2022 , u n }, the rank of u j for j \u2208 [n] is defined as Rank(u j ) = n i=1 1 ui\u2264uj .\nWe denote u (i) the i-th order statistics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conformal Prediction", "text": "Conformal prediction (Vovk et al., 2005) is a framework for constructing online confidence sets, with the remarkable properties of being distribution free, having a finite sample coverage guarantee, and being able to be adapted to any estimator under mild assumptions. We recall the arguments in (Shafer & Vovk, 2008;Lei et al., 2018) to construct a conformity/typicalness function based on rank statistics that yields to distribution-free inference methods. The main tool is that the rank of one variable among an exchangeable and identically distributed sequence follows a (sub)-uniform distribution (Br\u00f6cker & Kantz, 2011). Lemma 2.1. Let U 1 , . . . , U n , U n+1 be an exchangeable and identically distributed sequence of random variables. Then for any \u03b1 \u2208 (0, 1), we have\nP n+1 (Rank(U n+1 ) \u2264 (n + 1)(1 \u2212 \u03b1)) \u2265 1 \u2212 \u03b1 .\nWe remind that y n+1 is the unknown target variable. We introduce a learning problem with the augmented training data D n+1 (z) := D n \u222a {(x n+1 , z)} for z \u2208 R and with the augmented vector of labels y\n(z) = (y 1 , \u2022 \u2022 \u2022 , y n , z): \u03b2(z) \u2208 arg min \u03b2\u2208R p L(y(z), \u03a6(X, \u03b2)) + \u2126(\u03b2) , (1\n)\nwhere \u03a6 is a feature map and for any parameter\n\u03b2 \u2208 R p \u03a6(X, \u03b2) = [\u03a6(x 1 , \u03b2), . . . , \u03a6(x n+1 , \u03b2)] \u2208 R n+1 .\nGiven an input feature vector x, the prediction of its output/label adjusted on the augmented data, can be defined as\n\u00b5 z (x) := \u03a6(x, \u03b2(z)) .\nFor example in case of empirical risk minimization, we have\nL(y(z), \u03a6(X, \u03b2)) = n i=1 (y i , \u03a6(x i , \u03b2))+ (z, \u03a6(x n+1 , \u03b2)) .\nThere are many examples of cost functions in the literature.\nA popular example is the power norm regression, where (a, b) = |a \u2212 b| q . When q = 2, this corresponds to the classical linear regression. The cases where q = (1, 2) are frequent in robust statistics where the case q = 1 is known as the least absolute deviation. The loss logcosh (a, b) = \u03b3 log(cosh(a \u2212 b)/\u03b3) is a differentiable alternative to the \u221e norm (Chebychev approximation). One can also have the loss function Linex (Gruber, 2010;Chang & Hung, 2007) which provides an asymmetric loss function\n(a, b) = exp(\u03b3(a \u2212 b)) \u2212 \u03b3(a \u2212 b) \u2212 1, for \u03b3 = 0.\nAny convex regularization function \u2126 e.g., Ridge (Hoerl & Kennard, 1970) or norm inducing sparsity (Bach et al., 2012) can be considered. Also the feature map \u03a6 can be parameterized and learned\u00e0 la neural network. Equation ( 1) includes many modern formulations of statistical learning estimators. The only requirement on these is to be invariant with respect to the data permutation; this leaves a very large degree of freedom on their choice. For example, \u03b2(z) can be the output of an iterative model e.g., proximal gradient descent, with early stopping.\nLet us define the conformity measure for D n+1 (z) as\n\u2200i \u2208 [n], E i (z) = S(y i , \u00b5 z (x i )) ,(2)\nE n+1 (z) = S(z, \u00b5 z (x n+1 )) , (3\n)\nwhere S is a real-valued function e.g., in a linear regression problem, one can take s(a, b) = |a \u2212 b|. The main idea for constructing a conformal confidence set is to consider the typicalness/conformity of a candidate point z measured as\n\u03c0(z) := 1 \u2212 1 n + 1 Rank(E n+1 (z)) .(4)\nThe conformal set gathers all the real values z such that \u03c0(z) \u2265 \u03b1, if and only if, the score E n+1 (z) is ranked no higher than\n(n + 1)(1 \u2212 \u03b1) , among {E i (z)} i\u2208[n+1] i.e., \u0393 (\u03b1) (x n+1 ) := {z \u2208 R : \u03c0(z) \u2265 \u03b1} .(5)\nA direct application of Lemma 2.1 to U i = E i (y n+1 ) reads P(\u03c0(y n+1 ) \u2264 \u03b1) \u2264 \u03b1 i.e., the random variable \u03c0(y n+1 ) takes small values with small probability and it reads the coverage guarantee \nP(y n+1 \u2208 \u0393 (\u03b1) (x n+1 )) \u2265 1 \u2212 \u03b1 .", "publication_ref": ["b42", "b39", "b30", "b9", "b19", "b12", "b24", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Computational Limitations and Previous Works", "text": "For regression problems where y n+1 lies in a subset of R, obtaining the conformal set \u0393 (\u03b1) (x n+1 ) in Equation ( 5) is computationally challenging. It requires re-fitting the prediction model \u03b2(z) for infinitely many candidates z in order to compute the map of conformity measure such as z \u2192 E i (z) = |y i \u2212 x i \u03b2(z)|. Except for a few examples, the computation of a conformal prediction set is infeasible in general. We describe below some successful computational strategies while pointing out their potential shortcomings.\nIn Ridge regression, for any x in R p , z \u2192 x \u03b2(z) is a linear function of z, implying that E i (z) is piecewise linear. Exploiting this fact, an exact conformal set \u0393 (\u03b1) (x n+1 ) for Ridge regression was efficiently constructed in (Nouretdinov et al., 2001). Similarly, using the piecewise linearity w.r.t. sparsity level of the Lasso path provided by the Lars algorithm (Efron et al., 2004), (Hebiri, 2010) builds a sequence of conformal sets for the Lasso associated to the transition points of the Lars with the observed data D n . Nevertheless, such procedure breaks the proof technique for the coverage guarantee as the exchangeability of the sequence\n(E i (y n+1 )) i\u2208[n+1]\nis not necessarily maintained. However, a slight adaptation can fix the previous problem. Indeed using the piecewise linearity in z of the Lasso solution, (Lei, 2019) proposed a piecewise linear homotopy under mild assumptions, when a single input sample point is perturbed. This finally allows to compute the whole solution path z \u2192 \u03b2(z) and successfully provides a conformal set for the Lasso and Elastic Net. These processes are however limited to quadratic loss function. Later,  proposed an adaptation using approximate solution path  instead of exact solution. This results in a careful discretization of the set of candidates restricted into a preselected compact [z min , z max ]. Assuming that the optimization problem in Equation ( 1) is convex and that the loss function is smooth, this leads to a computational complexity of O(1/ \u221a ) where > 0 is a prescribed optimization error. All these previous methods are at best restricted to convex optimization formulations. A different road consists in merely assuming that the conformal set \u0393 (\u03b1) (x n+1 ) in Equation ( 5) itself is a bounded interval. As such, its endpoints can be estimated by approximating the roots of the function z \u2192 \u03c0(z)\u2212\u03b1. Under slight additional assumptions, a direct bisection search can then compute a conformal set with a complexity of O(log 2 (1/ r )) (Ndiaye & Takeuchi, 2021) where r > 0 is the tolerance error w.r.t. to exact root.\nCross-conformal Predictors was initially introduced in its one split version in (Papadopoulos et al., 2002).The idea is to separate the data into two independent parts, fit the model on one part and rank the scores on the other part where Lemma 2.1 remains applicable and thus preserves the coverage guarantee. Although this approach avoids the computational bottleneck by requiring only one data adjustment, the statistical efficiency of the model may be reduced due to a much smaller sample size available during the training and calibration phases. In general, the proportion of the training set to the calibration set is a hyperparameter that requires appropriate tuning: a small calibration set leads to highly variable conformational scores and a small training set leads to poor model fit. Such trade-off is very recurrent in machine learning and often appears in the debate between bias reduction and variance reduction. It is often decided by the cross-validation method with several folds (Arlot & Celisse, 2010). Cross-conformal predictors (Vovk, 2015) follow the same ideas and exploit the full dataset for calibration and significant proportions for training the model. The dataset is partitioned into K folds and one performs a split conformal set by sequentially defining the kth fold as calibration set and the remaining as training set for k \u2208 {1, . . . , K}. The leave-one-out aka Jackknife CP set, requires K = n model fit which is prohibitive even when n is moderately large. On the other hand, the K-fold version will require K model fit but will come at the cost of fitting on a lower sample size and will leads to an additional excess coverage of O( 2/n) and requires a subtle aggregation of the different pi-values obtained; see (Carlsson et al., 2014;Linusson et al., 2017). (Barber et al., 2021) shown that the confidence level attained is 1 \u2212 2\u03b1 instead of 1 \u2212 \u03b1 and can only approximately reaches the target coverage 1 \u2212 \u03b1 under additional stability assumption.\nAlthough these recent advances have drastically improved the tractability of the calculations, in practice multiple re-adjustments of the data are required. This remains very expensive especially for complex models. Imagine having to re-train a neural network from scratch ten or twenty times to get a reasonable estimate. In this paper, we actually show that a single model fit is enough to tightly approximate the conformal set when the underlying model fitting is stable.", "publication_ref": ["b36", "b17", "b21", "b29", "b34", "b37", "b2", "b41", "b10", "b32", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Approximation via Algorithmic Stability", "text": "The Section 2 guarantees that \u03c0(y n+1 ) \u2265 \u03b1 with high probability. Therefore, since y n+1 is unknown, the conformal set just selects all z that satisfies the same inequality i.e., \u0393 (\u03b1) (x n+1 ) = {z : \u03c0(z) \u2265 \u03b1}. This leads to fitting a new model for any z. Here, we take a different strategy. The main remark is that only one element of the dataset changes at a time, then with mild stability assumptions, one can expect that the model prediction will not change drastically. Instead of inverting \u03c0(\u2022), we will bound it with quantities independent of the model fit \u00b5 z for any z.\nDefinition 3.1 (Algorithmic Stability). A prediction func- tion \u00b5 \u2022 is stable if for any observed features x i , i \u2208 [n + 1], we have |S(q, \u00b5 z (x i )) \u2212 S(q, \u00b5 z0 (x i ))| \u2264 \u03c4 i \u2200z, z 0 , q \u2208 R . (6)\nIn the literature, it is common to make assumptions about the stability of a predictive model to obtain upper bounds on its generalization error and thus ensure that it does not overfit the training data e.g., (Bousquet & Elisseeff, 2002).\nAlthough the conformal prediction framework applies even when the underlying model is not stable, we show that this additional assumption allows for efficient evaluation of confidence sets. We also add that in cases where the generalization capabilities of the model are poor, the size of the confidence intervals can become very large; even unbounded, and not at all informative. Proposition 3.2. Assume that the model fit \u00b5 \u2022 is stable as in Definition 3.1. Then, we have:\n\u2200z,\u1e91, \u03c0 lo (z,\u1e91) \u2264 \u03c0(z) \u2264 \u03c0 up (z,\u1e91) , with \u03c0 lo (z,\u1e91) := 1 \u2212 1 n + 1 n+1 i=1 1 Li(z,\u1e91)\u2264Un+1(z,\u1e91) , \u03c0 up (z,\u1e91) := 1 \u2212 1 n + 1 n+1 i=1 1 Ui(z,\u1e91)\u2264Ln+1(z,\u1e91) ,\nwhere, we define, for any index i in [n],\nL i (z,\u1e91) = E i (\u1e91) \u2212 \u03c4 i , U i (z,\u1e91) = E i (\u1e91) + \u03c4 i , L n+1 (z,\u1e91) = S(z, \u00b5\u1e91(x n+1 )) \u2212 \u03c4 n+1 , U n+1 (z,\u1e91) = S(z, \u00b5\u1e91(x n+1 )) + \u03c4 n+1 .\nProof. By stability, for any q, we have:\n|S(q, \u00b5 z (x i )) \u2212 S(q, \u00b5\u1e91(x i ))| \u2264 \u03c4 i .\nApplying the previous inequality to q = y i for any index i in [n + 1], we have L i (z,\u1e91) \u2264 E i (z) \u2264 U i (z,\u1e91) and it holds:\nU i (z,\u1e91) \u2264 L n+1 (z,\u1e91) =\u21d2 E i (z) \u2264 E n+1 (z) =\u21d2 L i (z,\u1e91) \u2264 U n+1 (z,\u1e91) .\nTaking the indicator of the corresponding sets, we obtain the result.\nA direct consequence of Proposition 3.2 is that the exact conformal set can be wrapped as follows. Corollary 3.3 (Stable Conformal Sets). Under the assumption of Proposition 3.2, the conformal prediction set is lower and upper approximated as\n\u0393 (\u03b1) lo (x n+1 ) \u2282 \u0393 (\u03b1) (x n+1 ) \u2282 \u0393 (\u03b1) up (x n+1 ) ,where\n\u0393 (\u03b1) lo (x n+1 ) = {z : \u03c0 lo (z,\u1e91) \u2265 \u03b1} , \u0393 (\u03b1) up (x n+1 ) = {z : \u03c0 up (z,\u1e91) \u2265 \u03b1} .\nSince our proposal arises from a combination of the conformal prediction sets with a correction from the stability bounds, we call the resulting (upper) confidence set \u0393 (\u03b1) up (x n+1 ) stabCP for stable conformal set. By construction, it contains the exact confidence set \u0393 (\u03b1) (x n+1 ) and therefore enjoys at least the same statistical benefits displayed in the following result. Proposition 3.4 (Coverage guarantee). Assume that the model fit \u00b5 \u2022 is stable as in Definition 3.1. Then the stabCP set is an upper envelope of the exact conformal prediction set in Equation (5) and is thus valid i.e.,\nP(y n+1 \u2208 \u0393 (\u03b1) up (x n+1 )) \u2265 1 \u2212 \u03b1 .\nAs promised in the abstract, our proposed method suffers no loss of statistical coverage, requires only one model adjustment to the data at an arbitrary candidate point\u1e91, and fully uses all the data (no splitting). Thus we can benefit both from statistical efficiency with a smaller confidence interval as in the case of the exact calculation; but also we completely break the computational difficulty as in the case of splitting methods. To our knowledge, there is no equivalent method that can benefit from such a double performance.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Practical Computation of stabCP sets", "text": "By construction, the computation of stable conformal sets is equivalent to collecting all z such that \u03c0 up (z,\u1e91) \u2265 \u03b1.\nLet's begin by noting that U n+1 (z,\u1e91) > L n+1 (z,\u1e91) when \u03c4 n+1 > 0 which we will assume for simplicity. We have Approximation Gap This means that a candidate z is selected, if at most\n\u03c0 up (z,\u1e91) \u2265 \u03b1 \u21d4 n i=1 1 Ui(z,\u1e91)\u2264Ln+1(z,\u1e91) \u2264 (1\u2212\u03b1)(n+1) .\nsup z\u2208Z Gap(z, 0) 1 |Z| z\u2208Z Gap(z, 0) max i\u2208[n+1] \u03c4i 1 n + 1 i\u2208[n+1] \u03c4i (d) Convergence of the approximation gap Gap(z,\u1e91)\n(1 \u2212 \u03b1)(n + 1) elements of {U i (z,\u1e91)} i\u2208[n] are smaller than L n+1 (z,\u1e91). Which is equivalent to 1 L n+1 (z,\u1e91) \u2264 U ( (1\u2212\u03b1)(n+1) ) (z,\u1e91) =: Q 1\u2212\u03b1 (\u1e91) .\nHence, we can conclude that\n\u0393 (\u03b1) up (x n+1 ) = {z : S(z, \u00b5\u1e91(x n+1 )) \u2264 Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 }.\nFor the absolute value score, it reduces to the interval\n\u0393 (\u03b1) up (x n+1 ) = [\u00b5\u1e91(x n+1 ) \u00b1 (Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 )] .\nFor the sake of clarity, we summarize the computations for this simplest case in Algorithm 1 and discuss the generalization in the appendix. In general terms, stabCP sets are convex sets when the score function z \u2192 S(z, \u00b5\u1e91(x n+1 )) has convex level sets. This presumes that our strategy will also facilitate the calculations in cases where the target y n+1 is multi-dimensional.\n1 For i \u2208 [n], Ui(z,\u1e91) and Li(z,\u1e91) are independent of z.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Stable Conformal Prediction Set", "text": "Input: data {(x 1 , y 1 ), . . . , (x n , y n )} and x n+1 Coverage level \u03b1 \u2208 (0, 1), any estimate\u1e91 \u2208 R Stability bounds \u03c4 1 , . . . , \u03c4 n+1 of the learning algorithm Output:\nprediction interval at x n+1 Fit a model \u00b5\u1e91 on the training data D n+1 (\u1e91) Compute the quantile Q 1\u2212\u03b1 (\u1e91) = U ( (1\u2212\u03b1)(n+1) ) (z,\u1e91)\nwhere the U i s are defined in Proposition 3.2\nReturn: [\u00b5\u1e91(x n+1 ) \u00b1 (Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 )]", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Batch Approximation", "text": "The stable conformal sets require a single model fit \u00b5\u1e91 for an arbitrary candidate\u1e91. The approximation gaps are computable as\nmax{\u03c0(z) \u2212 \u03c0 lo (z,\u1e91), \u03c0 up (z,\u1e91) \u2212 \u03c0(z)} \u2264 Gap(z,\u1e91) ,\nwhere Gap(z,\u1e91) := \u03c0 up (z,\u1e91) \u2212 \u03c0 lo (z,\u1e91) .\nSince the above upper and lower bounds hold for any\u1e91, tighter approximations are obtained with a batch of candi-\ndates Z =\u1e91 1 , \u2022 \u2022 \u2022 ,\u1e91 d as \u03c0 up (z, Z) = inf z\u2208Z \u03c0 up (z,\u1e91) and \u03c0 lo (z, Z) = sup z\u2208Z \u03c0 lo (z,\u1e91) .\nAnother possibility is to build an interpolation of z \u2192 \u00b5 z (\u2022) based on query points\u1e91 1 , \u2022 \u2022 \u2022 ,\u1e91 d \u2208 (z min , z max ) \u2282 R. For example, one can consider as predictive model the following piecewise linear interpolatio\u00f1\n\u00b5 z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3\u1e91 1\u2212\u1e91 z1\u2212zmin \u00b5 zmin + zmin\u2212\u1e91 z1\u2212zmin \u00b5\u1e91 1 if z \u2264 z min , z\u2212\u1e91t+1 zt\u2212\u1e91t+1 \u00b5\u1e91 t + z\u2212\u1e91t zt+1\u2212\u1e91t \u00b5\u1e91 t+1 if z \u2208 [\u1e91 t ,\u1e91 t+1 ] , z\u2212\u1e91 d zmax\u2212\u1e91 d \u00b5 zmax + zmax\u2212z zmax\u2212\u1e91 d \u00b5\u1e91 d if z \u2265 z max ,\nAn important point is that, by using the stability bound, the coverage guarantee of the interpolated conformal set is preserved without the need of the expensive symmetrization proposed in (Ndiaye & Takeuchi, 2021). Such techniques are more relevant when the sample size is small or when precise estimates of the stability bounds are not available. The corresponding conformity function is defined in a similar way as the previous versions, where we simply plugin the interpolated model. We refer to the appendix for more details. Remark 3.5 (Categorical Variables). In this article, we have essentially limited ourselves to regression problems which, in general, pose intractable computational difficulties. However, the methods remain applicable for classification problems where the set of candidates can only take a finite number of values in C := c 1 , . . . , c m . In this case, an additional precaution of encoding the categories in real numbers is necessary. Considering the leave-one-out score function, our proposal is therefore an alternative to the approximations via influence function used in (Alaa & Schaar, 2020;Abad et al., 2022) when an exact computation (Cherubin et al., 2021) would be unusable or too costly.", "publication_ref": ["b34", "b1", "b0", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Stability Bounds", "text": "In this section, we recall some stability bounds. The proof techniques rely on regularity assumptions on the function to be minimized and are relatively standard in optimization (Shalev-Shwartz & Ben-David, 2014, Chapter 13). Stability is a widely used assumption to provide generalization bounds for machine learning algorithms (Bousquet & Elisseeff, 2002;Hardt et al., 2016). We specify that here the notion of stability that we require is related to the variation of the score and not of the loss function in the optimization objective. However, the ideas for establishing the stability bounds are essentially the same and we recall the core strategies here for the sake of completeness.\nLet us start with the unregularized model where \u2126 = 0 i.e., \u03b2(z) \u2208 arg min\n\u03b2\u2208R p L(y(z), \u03a6(X, \u03b2)) = F z (\u03a6(X, \u03b2)) . (7\n)\nDefinition 3.6. A function f is \u03bb-strongly convex if for any w 0 , w and \u03c2 \u2208 (0, 1)\nf (\u03c2w 0 + (1 \u2212 \u03c2)w) \u2264 \u03c2f (w 0 ) + (1 \u2212 \u03c2)f (w) \u2212 \u03bb 2 \u03c2(1 \u2212 \u03c2) w 0 \u2212 w 2 .\nProposition 3.7. Assume that for any z, F z is \u03bb-strongly convex and \u03c1-Lipschitz. It holds\n\u00b5 z (X) \u2212 \u00b5 z0 (X) \u2264 2\u03c1 \u03bb .\nProof. By optimality of \u03b2(z), we have\nF z (\u03a6(X, \u03b2(z))) \u2264 F z (\u03a6(X, \u03b2)) \u2200\u03b2 .(8)\nWe simply apply the optimality condition and strong convexity of the function F z to the vectors w 0 = \u03a6(X, \u03b2(z 0 )) = \u00b5 z0 (X) and w = \u03a6(X, \u03b2(z)) = \u00b5 z (X), it holds\n0 (8) \u2264 F z (\u03c2w 0 + (1 \u2212 \u03c2)w) \u2212 F z (w) \u03c2 (3.6) \u2264 F z (w 0 ) \u2212 F z (w) \u2212 \u03bb 2 (1 \u2212 \u03c2) w 0 \u2212 w 2 .\nSince F z is \u03c1-Lipschitz, we have\n\u03bb 2 w 0 \u2212 w 2 \u2264 F z (w 0 ) \u2212 F z (w) \u2264 \u03c1 w \u2212 w 0 .\nTherefore, \u03bb 2 w 0 \u2212 w \u2264 \u03c1, hence the result.\nThe Proposition 3.7 does not assume that the optimization problem in Equation ( 7) is convex in the model parameter \u03b2. We can now easily deduce a stability bound according to the Definition 3.1. Corollary 3.8. If the score function S(q, \u2022) is \u03b3-Lipschitz for any q, then\n\u03c4 i = 2\u03b3\u03c1 \u03bb , \u2200i \u2208 [n + 1] .\nWhen the loss function is not strongly convex, it is known that adding a strongly convex regularization can stabilize the algorithm (Shalev-Shwartz & Ben-David, 2014, Chapter 13). The proof technique is similar to the previous one with the difference that now the bound is on the arg min of the optimization problem and not the predictions of the model. This requires stronger assumptions. Proposition 3.9. Assume the optimization problem Equation (1) is convex, \u2126 is \u03bb-strongly convex. If the loss L is convex-\u03c1-Lipschitz, then\n\u03b2(z) \u2212 \u03b2(z 0 ) \u2264 2\u03c1 \u03bb .\nWhen the loss function L is convex-\u03bd-smooth with \u03bd < \u03bb and L(y(z), \u00b5 z (X)) \u2264 C for any z, then\n\u03b2(z) \u2212 \u03b2(z 0 ) \u2264 2 \u221a 2\u03bdC \u03bb \u2212 \u03bd .\nThese optimization error bounds also imply the following stability bounds.\nCorollary 3.10. Assume that the score function S(q, \u2022) is \u03b3-Lipschitz for any q, and that the prediction model \u00b5 \u2022 (x) := \u03a6(x, \u03b2(\u2022)) satisfies for any x \u2208 R p , z, z 0 \u2208 R,\n|\u00b5 z (x) \u2212 \u00b5 z0 (x)| \u2264 L \u03a6 |x \u03b2(z) \u2212 x \u03b2(z 0 )| .\nIf the loss is \u03c1-Lipschitz, then\n\u03c4 i = 2\u03b3\u03c1L \u03a6 x i \u03bb , \u2200i \u2208 [n + 1] .\nIf the loss is \u03bd-smooth with \u03bd < \u03bb and bounded by C, then\n\u03c4 i = 2\u03b3L \u03a6 x i \u221a 2\u03bdC \u03bb \u2212 \u03bd , \u2200i \u2208 [n + 1] .\nAnother way to understand such regularized bounds, is to leverage duality. A smoothness assumption in the primal space will translate into a strongly concave assumption in the dual space (Hiriart-Urruty & Lemar\u00e9chal, 1993, Theorem 4.2.2, p. 83). The dual formulation (Rockafellar, 1997, Chapter 31) of Equation ( 1) reads:\n\u03b8(z) \u2208 arg max \u03b8\u2208R n+1 \u2212L * (y(z), \u2212\u03b8) \u2212 \u2126 * (X \u03b8) ,(9)\nwhere, given a proper, closed and convex function f : R n \u2192 R \u222a {+\u221e}, we denoted its Fenchel-Legendre transform as f * : R n \u2192 R \u222a {+\u221e} defined by f * (x * ) = sup x\u2208dom f x * , x \u2212 f (x) with dom f = {x \u2208 R n : f (x) < +\u221e}.\nLet P z and D z denote the primal and dual objective functions. We have the following classical error bounds for the dual optimization problem. If the loss function L is \u03bd-smooth, then L * is 1/\u03bd-strongly convex and we have for\n\u2200(\u03b2, \u03b8) \u2208 dom P z \u00d7 dom D z 1 2\u03bd \u03b8(z) \u2212 \u03b8 2 \u2264 D z (\u03b8(z)) \u2212 D z (\u03b8) = P z (\u03b2(z)) \u2212 D z (\u03b8) \u2264 Duality Gap z (\u03b2, \u03b8) ,\nwhere the equality follows from strong duality and we recall from weak duality that the duality gap upper bounds the optimization error as follow:\nDuality Gap z (\u03b2, \u03b8) := P z (\u03b2) \u2212 D z (\u03b8) \u2265 P z (\u03b2) \u2212 P z (\u03b2(z)) .\nThis readily leads to several possible bounds. If the dual function D z (\u2022) is \u03c1 * -Lipschitz for any z, then\n\u03b8(z) \u2212 \u03b8 \u2264 2\u03bd\u03c1 * .\nIf the duality gap can be assumed to be bounded by C for any z \u2208 [z min , z max ], then\n\u03b8(z) \u2212 \u03b8 \u2264 \u221a 2\u03bdC .\nWe obtain stability bounds when one uses the dual solution (which is a function of the residual) as a conformity score S(y(z), \u00b5 z (X)) = |\u03b8(z)| where the absolute value is taken coordinate wise. For example, these dual based score functions were used in . Remark 3.11 (Bound on the loss). The assumption of a bounded loss function that we make, is not rigorously feasible and some adaptations are necessary. For simplicity, let us consider that \u03a6(x, 0) = 0 and \u2126(0) = 0. Using the optimality of \u03b2(z), we obtain for any candidate z L(y(z), \u00b5 z (X)) \u2264 L(y(z), \u00b5 z (X)) + \u2126(\u03b2(z))\n\u2264 L(y(z), 0) .\nUnfortunately, for common examples such as least squares, the right hand side is unbounded. Nevertheless, since the data are assumed to be exchangeable, we have\nP(y n+1 \u2208 [y (1) , y (n) ]) \u2265 1 \u2212 2 n + 1 .\nHence it is reasonable to restrict the range of candidates as\nz \u2208 [y (1) , y (n) ], which implies L(y(z), \u00b5 z (X)) \u2264 sup z\u2208[y (1) ,y (n) ]\nL(y(z), 0) =: C .", "publication_ref": ["b8", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical Experiments", "text": "We conduct all the experiments with a coverage level of 0.9 i.e., \u03b1 = 0.1. For comparisons, we run the evaluations on 100 repetitions of examples and display the average of the following performance statistics for different methods: the empirical coverage i.e., the percentage of times the prediction set contains the held-out target y n+1 , the length of the confidence intervals, and the execution time. We compare the method we propose stabCP with the conformal prediction set computed with an oracle method defined below, with a splitting strategy splitCP (Papadopoulos et al., 2002;Lei et al., 2018), and finally with an estimation of the \u03b1-level set of the conformity function rootCP (Ndiaye & Takeuchi, 2021) by root-finding solvers. Note that, when the conformal set is a bounded interval, stabCP approximates rootCP as in Figure 2. In all experiments conducted, we observed that the exact conformal prediction set is indeed an interval. Although this is often the case, we recall that it might not be in general. Just for the comparisons, we therefore estimated the stabCP sets with a root-finding solver as well, as if a closed form solution was not available.\nA python package with our implementation is available at https://github.com/EugeneNdiaye/stable_ conformal_prediction where additional numerical experiments (e.g., using large pre-trained neural net) and benchmarks will be provided. Figure 3. Benchmarking conformal sets for the least absolute deviation regression models with a ridge regularization on real datasets. We display the lengths of the confidence sets over 100 random permutation of the data. We denoted cov the average coverage and T the average computational time normalized with the average time for computing oracleCP which requires a single full data model fit. The full and exact CP set can always be approximated with a fine (costly) grid discretization of the output space and can then be used as a default baseline. Here, it is represented by rootCP since in the examples displayed the full CP set turns out to be an interval and then rootCP is equal to the full CP up to r digit precision on the decimals; we used a default value of r = 10 \u22124 . oracleCP. To define an oracle prediction set as reference, we follow in 2021) and assume that the unavailable target variable y n+1 is observed by the algorithm. Hence, we define the oracle scores\n\u2200i \u2208 [n], E or i = S(y i , \u00b5 yn+1 (x i )) , E or n+1 (z) = S(z, \u00b5 yn+1 (x n+1 )) ,\nand the oracle conformal set as\n\u0393 (\u03b1) oracle (x n+1 ) := {z : \u03c0 oracle (z) \u2265 \u03b1} , \u03c0 oracle (z) = 1 \u2212 1 n + 1 n+1 i=1 1 E or i \u2264E or n+1 (z) .\nsplitCP. A popular and classical estimation of conformal prediction sets relies on splitting the dataset. The split conformal prediction set introduced in (Papadopoulos et al., 2002), separates the model fitting and the calibration steps.", "publication_ref": ["b37", "b30", "b34", "b16", "b37"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Let us define", "text": "\u2022 the training set\nD tr = {(x 1 , y 1 ), \u2022 \u2022 \u2022 , (x m , y m )} with m < n ,\n\u2022 the calibration set\nD cal = {(x m+1 , y m+1 ), \u2022 \u2022 \u2022 , (x n , y n )} .\nThen the model is fitted on the training set D tr to get \u00b5 tr (\u2022) and define the score function on the calibration set D cal :\n\u2200i \u2208 [m + 1, n], E cal i = S(y i , \u00b5 tr (x i )) , E cal n+1 (z) = S(z, \u00b5 tr (x n+1 )) .\nThus, we obtain the split conformal set as\n\u0393 (\u03b1) split (x n+1 ) = {z : \u03c0 split (z) \u2265 \u03b1} , \u03c0 split (z) = 1 \u2212 1 n \u2212 m + 1 n+1 i=m+1 1 E cal i \u2264E cal n+1 (z) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "The data splitting approach does not use all the data in the training phase. It is often less statistically efficient, and its interval length can vary greatly depending on the additional randomness of the split. On the contrary, our approach does not use any splitting, provides an approximation of the exact conformal set that is pretty accurate depending on the stability of the model as can be observed on Figure 3. All this requires one and only one data fitting of the underlying learning model. You will notice that splitCP and stabCP have the same structure and are simple intervals if the score functions are reasonably simple. The presence of data splitting in the former is replaced by an additional stability term in the latter. So if the predictive model is very stable, stabCP benefits from all the data, and very little regularization to get closer to the oracle version that includes the unknown target y n+1 . To date, we are not aware of any other method that can obtain a full conformal prediction set with such computational efficiency while ensuring no loss on the coverage guarantee. We observe on the benchmarks with real data Figure 3 that the stabCP is often very similar to the rootCP which approximates with a very fine precision the exact set (under the assumption that the latter is a bounded interval). Our proposal has the net advantage of being twenty to thirty times faster and can often be computed in closed form.\nHowever, as can be seen in Figure 2, our proposed method loses precision when the sample size is small. This reflects the difficulty of estimating a reliable confidence set in the absence of algorithmic stability. At the same time, it is difficult to have an algorithm that generalizes well with so little training data. Otherwise, when the size of the data is important, the influence of the stability bound is very little felt because they are often of the order of magnitude O(1/n).\nFinally, a notorious limitation is that one needs to know explicitly the stability bounds. This can be difficult to estimate for some models. The bounds we presented in Section 3.3 cover a wide range of examples and can be completed by bounds displayed in (Hardt et al., 2016;Bassily et al., 2020;Lei et al., 2021;Klochkov & Zhivotovskiy, 2021) for stochastic gradient descent. Even if the notion of stability required here is slightly different, any error bound on the estimator can be naturally converted into a stability bound for conformal prediction sets. So we don't lose much generality as long as we make the assumption that the score function is sufficiently regular e.g., Lipschitz. This is precisely what allowed us to obtain the bounds presented in this article. Yet, if the parameter of the predictive model is defined iteratively by a gradient descent process on a non-convex objective function, obtaining stability bounds becomes quite delicate. Moreover, the Lipschitz constant of neural network objectives can be poorly estimated. In this case, our approach could not be applied safely or could lead to uninformative confidence intervals. The splitting strategy remains more flexible. It would be interesting to study fine combinations of data splitting and inclusion of stability bounds to reduce the size of the confidence intervals and their variance while being pivotal to explicit stability bounds.", "publication_ref": ["b20", "b6", "b31", "b27"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Appendix", "text": "In these supplementary notes, we complete some proofs and bring algorithmic precisions of our approach as well as additional numerical experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "StabCP Set with General Score Function", "text": "We explain a simple procedure to approximate the set prediction with an arbitrary pre-defined accuracy. We recall that \u0393 (\u03b1) up (x n+1 ) = {z : \u03c0 up (z,\u1e91) \u2265 \u03b1} = {z : S(z, \u00b5\u1e91(x n+1 )) \u2264 Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 } , which is a convex set when the level-set of the score function is convex. By simplicity, we assume that the score function is such that \u0393 1. find z min < z 0 < z max such that \u03c0 up (z min ,\u1e91) < \u03b1 < \u03c0 up (z 0 ,\u1e91) and \u03b1 > \u03c0 up (z max ,\u1e91) . ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stability of the Linear Interpolation", "text": "We discussed in Section 3.2 the potential gain in accuracy when approximating the conformity function using not a single point but a batch of points. Here we justify the interpolation approach when the score function S is sufficiently regular. Proposition 6.1. Let us assume that the score function S(q, \u2022) is \u03b3-Lipschitz for any q, and consider the interpolated prediction model defined as\u03bc\nz = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3\u1e91 1\u2212\u1e91 z1\u2212zmin \u00b5 zmin + zmin\u2212\u1e91 z1\u2212zmin \u00b5\u1e91 1 if z \u2264 z min , z\u2212\u1e91t+1 zt\u2212\u1e91t+1 \u00b5\u1e91 t + z\u2212\u1e91t zt+1\u2212\u1e91t \u00b5\u1e91 t+1 if z \u2208 [\u1e91 t ,\u1e91 t+1 ] , z\u2212\u1e91 d zmax\u2212\u1e91 d \u00b5 zmax + zmax\u2212z zmax\u2212\u1e91 d \u00b5\u1e91 d if z \u2265 z max ,(11)\nwhere \u00b5 \u2022 is stable according to Definition 3.1. It holds\n|S(q,\u03bc z (x i )) \u2212 S(q,\u03bc z0 (x i ))| \u2264 3\u03b3\u03c4 i .(12)\nProof. Using the triangle inequality, we hav\u1ebd  Figure 6. Benchmarking conformal sets for MLP regression models with a ridge regularization on real datasets. The parameter of the model is obtained after T = n/10 iterations of stochastic gradient descent. For stabCP, we use a stability bound estimate \u03c4i = T xi /(n+1). We display the lengths of the confidence sets over 100 random permutation of the data. We denoted cov the average coverage and T the average computational time normalized with the average time for computing oracleCP which requires a single full data model fit. Figure 7. Benchmarking conformal sets for Gradient Boosting regression models with a ridge regularization on real datasets. For stabCP, we use a stability bound estimate \u03c4i = xi /(n + 1). We display the lengths of the confidence sets over 100 random permutation of the data. We denoted cov the average coverage and T the average computational time normalized with the average time for computing oracleCP which requires a single full data model fit. Figure 8. Benchmarking conformal sets for Gradient Boosting regression models with a ridge regularization on real datasets. For stabCP, we use a rough stability bound estimate \u03c4i \u2248 xi /10. We display the lengths of the confidence sets over 100 random permutation of the data. We denoted cov the average coverage and T the average computational time normalized with the average time for computing oracleCP which requires a single full data model fit. This example shows that for unstable models such as decision trees, a coarse estimation of the stability bound can result in an overestimation of the confidence interval, which is a notable limitation of the proposed method.\nstab := |S(q,\u03bc z (x i )) \u2212 S(q,\u03bc z0 (x i ))| \u2264 |S(q,\u03bc z (x i )) \u2212 S(q, \u00b5 z (x i ))| + |S(q, \u00b5 z (x i )) \u2212 S(q, \u00b5 z0 (x i ))| + |S(q, \u00b5 z0 (x i )) \u2212 S(q,\u03bc z0 (x i ))| .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We warmly thank the reviewers for their insightful comments and contributions to improve the presentation of this paper. We also thank Elvis Dohmatob and Xiaoming Huo for proofreading and for pointing out mistakes in notations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "If \u00b5 \u2022 is stable, then the second term of the right hand side of the previous inequality is bounded by \u03c4 i . Now, assuming that S is \u03b3-Lipschitz in its second argument, for any q, we have:\nwhere\nis the scaling of interpolation points. Thus, we obtai\u00f1\nThe upper and lower approximation of the conformity function obtained with the interpolated model fit along with stability bounds are defined as:\u03c0\nwhere for any index i in [n + 1], using the stability bound in Equation ( 12), we defin\u1ebd\nIn general, approximating the entire model path with respect to output/label changes using finite grid points is not always safe for calculating the conformal prediction set because it breaks the exchangeability assumptions of the data set. Incorporating the stability bound will regularize the conformity function to restore the validity of the method. However, the procedure proves to be quite robust to wrong estimation of the stability bounds. The experiments in (Ndiaye & Takeuchi, 2021) are conducted with estimates \u03c4 i = 0 and the prediction sets obtained are essentially the same as the exact one. More detailed experiments will be proposed in our github implementation.", "publication_ref": ["b34"], "figure_ref": [], "table_ref": []}, {"heading": "Additional Experiments", "text": "In this appendix, we add some numerical experiments to illustrate how stabCP can behave when using an estimator that is not defined as an argmin but rather as an output of an iterative process. In this case, we use a Multi-Layer Perceptron regressor trained with T = n iter number of gradient descent iterations. Recent analyses (Hardt et al., 2016) have shown that any model trained with the stochastic gradient method in a reasonable amount of time achieves low generalization error. The proof of these results consists in showing that the estimator verifies a stability condition when the input data are slightly perturbed. The bounds on the iterates of stochastic gradient methods are often proportional to T n . They also depend on the Lipschitz regularity constants which unfortunately can be hard to estimate in practice. Here, we will be satisfied with the order of magnitude and evaluate the behavior of the conformity function according to the number of iterations performed. We run the experiments on two different datasets with a sample size of 442 and 20640.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Approximating full conformal prediction at scale via influence functions", "journal": "", "year": "2022", "authors": "J Abad; U Bhatt; A Weller; G Cherubin"}, {"ref_id": "b1", "title": "Quantifying uncertainty in deep learning via higher-order influence functions. International Conference on Machine Learning", "journal": "", "year": "2020", "authors": "A Alaa; M V D Schaar;  Discriminative Jackknife"}, {"ref_id": "b2", "title": "A survey of cross-validation procedures for model selection", "journal": "Statistics surveys", "year": "2010", "authors": "S Arlot; A Celisse"}, {"ref_id": "b3", "title": "Optimization with sparsity-inducing penalties. Foundations and Trends in Machine Learning", "journal": "", "year": "2012", "authors": "F Bach; R Jenatton; J Mairal; G Obozinski"}, {"ref_id": "b4", "title": "Conformal prediction for reliable machine learning: theory, adaptations and applications", "journal": "Elsevier", "year": "2014", "authors": "V Balasubramanian; S.-S Ho; V Vovk"}, {"ref_id": "b5", "title": "Predictive inference with the jackknife+", "journal": "The Annals of Statistics", "year": "2021", "authors": "R F Barber; E J Candes; A Ramdas; R J Tibshirani"}, {"ref_id": "b6", "title": "Stability of stochastic gradient descent on nonsmooth convex losses", "journal": "", "year": "2020", "authors": "R Bassily; V Feldman; C Guzm\u00e1n; K Talwar"}, {"ref_id": "b7", "title": "Testing for outliers with conformal p-values", "journal": "", "year": "2021", "authors": "S Bates; E Cand\u00e8s; L Lei; Y Romano; M Sesia"}, {"ref_id": "b8", "title": "Stability and generalization", "journal": "The Journal of Machine Learning Research", "year": "2002", "authors": "O Bousquet; A Elisseeff"}, {"ref_id": "b9", "title": "The concept of exchangeability in ensemble forecasting", "journal": "", "year": "2011", "authors": "J Br\u00f6cker; H Kantz"}, {"ref_id": "b10", "title": "IFIP International Conference on Artificial Intelligence Applications and Innovations", "journal": "", "year": "2014", "authors": "L Carlsson; M Eklund; U Norinder"}, {"ref_id": "b11", "title": "Valid distribution-free inferential models for prediction", "journal": "", "year": "2020", "authors": "L Cella; R Ryan"}, {"ref_id": "b12", "title": "Linex loss functions with applications to determining the optimum process parameters", "journal": "Quality & Quantity", "year": "2007", "authors": "Y.-C Chang; W.-L Hung"}, {"ref_id": "b13", "title": "Exact and robust conformal inference methods for predictive machine learning with dependent data", "journal": "", "year": "2018", "authors": "V Chernozhukov; K W\u00fcthrich; Y Zhu"}, {"ref_id": "b14", "title": "An exact and robust conformal inference method for counterfactual and synthetic controls", "journal": "Journal of the American Statistical Association", "year": "2021", "authors": "V Chernozhukov; K W\u00fcthrich; Y Zhu"}, {"ref_id": "b15", "title": "Exact optimization of conformal predictors via incremental and decremental learning", "journal": "", "year": "2021", "authors": "G Cherubin; K Chatzikokolakis; M Jaggi"}, {"ref_id": "b16", "title": "Resampling plans and the estimation of prediction error", "journal": "Stats", "year": "2021", "authors": "B Efron"}, {"ref_id": "b17", "title": "Least angle regression. The Annals of Statistics", "journal": "", "year": "2004", "authors": "B Efron; T Hastie; I M Johnstone; R Tibshirani"}, {"ref_id": "b18", "title": "Fewshot conformal prediction with auxiliary tasks", "journal": "ICML", "year": "2021", "authors": "A Fisch; T Schuster; T Jaakkola; R Barzilay"}, {"ref_id": "b19", "title": "Regression estimators: A comparative study", "journal": "JHU Press", "year": "2010", "authors": "M Gruber"}, {"ref_id": "b20", "title": "Train faster, generalize better: Stability of stochastic gradient descent", "journal": "", "year": "2016", "authors": "M Hardt; B Recht; Y Singer"}, {"ref_id": "b21", "title": "Sparse conformal predictors", "journal": "Statistics and Computing", "year": "2010", "authors": "M Hebiri"}, {"ref_id": "b22", "title": "Convex analysis and minimization algorithms. II", "journal": "Springer-Verlag", "year": "1993", "authors": "J.-B Hiriart-Urruty; C Lemar\u00e9chal"}, {"ref_id": "b23", "title": "Query by transduction", "journal": "", "year": "2008", "authors": "S.-S Ho; H Wechsler"}, {"ref_id": "b24", "title": "Ridge regression: Biased estimation for nonorthogonal problems", "journal": "", "year": "1970", "authors": "A E Hoerl; R W Kennard"}, {"ref_id": "b25", "title": "Making learning more transparent using conformalized performance prediction", "journal": "", "year": "2020", "authors": "M J Holland"}, {"ref_id": "b26", "title": "Conformal uncertainty sets for robust optimization. Conformal and Probabilistic Prediction and Applications", "journal": "", "year": "2021", "authors": "C Johnstone; B Cox"}, {"ref_id": "b27", "title": "Stability and deviation optimal risk bounds with convergence rate o(1/n)", "journal": "", "year": "2021", "authors": "Y Klochkov; N Zhivotovskiy"}, {"ref_id": "b28", "title": "Inductive conformal anomaly detection for sequential detection of anomalous sub-trajectories", "journal": "Annals of Mathematics and Artificial Intelligence", "year": "2015", "authors": "R Laxhammar; G Falkman"}, {"ref_id": "b29", "title": "Fast exact conformalization of lasso using piecewise linear homotopy", "journal": "Biometrika", "year": "2019", "authors": "J Lei"}, {"ref_id": "b30", "title": "Distribution-free predictive inference for regression", "journal": "Journal of the American Statistical Association", "year": "2018", "authors": "J Lei; M Sell; A Rinaldo; R J Tibshirani; L Wasserman"}, {"ref_id": "b31", "title": "Stability and generalization of stochastic gradient methods for minimax problems", "journal": "", "year": "2021", "authors": "Y Lei; Z Yang; T Yang; Ying ; Y "}, {"ref_id": "b32", "title": "On the calibration of aggregated conformal predictors", "journal": "", "year": "2017", "authors": "H Linusson; U Norinder; H Bostr\u00f6m; U Johansson; T L\u00f6fstr\u00f6m"}, {"ref_id": "b33", "title": "Computing full conformal prediction set with approximate homotopy", "journal": "NeurIPS", "year": "2019", "authors": "E Ndiaye; I Takeuchi"}, {"ref_id": "b34", "title": "Root-finding approaches for computing conformal prediction set", "journal": "", "year": "2021", "authors": "E Ndiaye; I Takeuchi"}, {"ref_id": "b35", "title": "Safe grid search with optimal complexity", "journal": "ICML", "year": "2019", "authors": "E Ndiaye; T Le; O Fercoq; J Salmon; I Takeuchi"}, {"ref_id": "b36", "title": "", "journal": "", "year": "2001", "authors": "I Nouretdinov; T Melluish; V Vovk"}, {"ref_id": "b37", "title": "Inductive confidence machines for regression", "journal": "", "year": "2002", "authors": "H Papadopoulos; K Proedrou; V Vovk; A Gammerman"}, {"ref_id": "b38", "title": "Reprint of the 1970 original", "journal": "Princeton University Press", "year": "1997", "authors": "R T Rockafellar"}, {"ref_id": "b39", "title": "A tutorial on conformal prediction", "journal": "Journal of Machine Learning Research", "year": "2008", "authors": "G Shafer; V Vovk"}, {"ref_id": "b40", "title": "Understanding machine learning: From theory to algorithms", "journal": "Cambridge university press", "year": "2014", "authors": "S Shalev-Shwartz; S Ben-David"}, {"ref_id": "b41", "title": "Cross-conformal predictors", "journal": "Annals of Mathematics and Artificial Intelligence", "year": "2015", "authors": "V Vovk"}, {"ref_id": "b42", "title": "Algorithmic learning in a random world", "journal": "Springer", "year": "2005", "authors": "V Vovk; A Gammerman; G Shafer"}, {"ref_id": "b43", "title": "Conformal prediction interval for dynamic time-series", "journal": "ICML", "year": "2021", "authors": "C Xu; Y Xie"}], "figures": [{"figure_label": "11", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 Figure 1 .11Figure1illustrates the candidates selected for inclusion in the confidence set as the most likely variables.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure2. Illustration of the evolution of the conformity function as a function of sample size. The underlying model fit is \u03b2(z) \u2208 arg min \u03b2\u2208R p y(z) \u2212 X\u03b2 1 /(n + 1) + \u03bb \u03b2 2 where y(z) = (y1, \u2022 \u2022 \u2022 , yn, z) and we use sklearn synthetic dataset make regression(n, p = 100, noise = 1). We fixed\u1e91 = 0 and \u03bb = 0.5. The set Z is a linear grid in the interval [y (1) , y (n) ]. We also illustrate the batch approximation for different values of\u1e91.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "n+1 ) is a bounded interval. Algorithm 2 summarizes the process. Algorithm 2 Stable conformal prediction set for score function with convex level-set Input: data {(x 1 , y 1 ), . . . , (x n , y n )} and x n+1 Coverage level \u03b1 \u2208 (0, 1), any estimate\u1e91 \u2208 R Stability bounds \u03c4 1 , . . . , \u03c4 n+1 of the learning algorithm Output: prediction interval at x n+1 Fit a model \u00b5\u1e91 on the training data D n+1 (\u1e91) Compute the quantile Q 1\u2212\u03b1 (\u1e91) = U ( (1\u2212\u03b1)(n+1) ) (z,\u1e91) where the U i s are defined in Proposition 3.2 Compute \u0393 (\u03b1) up (x n+1 ) = [ \u03b1 (x n+1 ), u \u03b1 (x n+1 )] up to r > 0 tolerance error as follow:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": ") 2 .2Perform a bisection search in [z min , z 0 ]. It will output a point\u02c6 such that \u03b1 (x n+1 ) belongs to [\u02c6 \u00b1 r ] after at most log 2 ( z0\u2212zmin r ) iterations. 3. Perform a bisection search in [z 0 , z max ]. It will output a point\u00fb such that u \u03b1 (x n+1 ) belongs to [\u00fb \u00b1 r ] after at most log 2 (", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 4 .Figure 5 .45Figure4. Illustration of different conformity functions with respect to a sequence of stability bounds. We observe that by merely staking an order of magnitude O(1/n) as stability bound, gives a good estimate of the conformal prediction set even if the bound is not safe. These experiments are conducted with a Multi-Layer Perceptron regressor on the Diabetes (442, 10) dataset, trained with T = n iter iterations of Stochastic Gradient Descent.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "y n+1 = z ,", "formula_coordinates": [1.0, 472.89, 196.27, 46.5, 10.32]}, {"formula_id": "formula_1", "formula_text": "D n = (x i , y i ) i\u2208[n]", "formula_coordinates": [2.0, 55.44, 224.99, 81.57, 17.29]}, {"formula_id": "formula_2", "formula_text": "X = [x 1 , \u2022 \u2022 \u2022 , x n , x n+1 ] . Given a set {u 1 , \u2022 \u2022 \u2022 , u n }, the rank of u j for j \u2208 [n] is defined as Rank(u j ) = n i=1 1 ui\u2264uj .", "formula_coordinates": [2.0, 55.44, 237.99, 234.0, 60.42]}, {"formula_id": "formula_3", "formula_text": "P n+1 (Rank(U n+1 ) \u2264 (n + 1)(1 \u2212 \u03b1)) \u2265 1 \u2212 \u03b1 .", "formula_coordinates": [2.0, 71.0, 529.88, 202.88, 18.44]}, {"formula_id": "formula_4", "formula_text": "(z) = (y 1 , \u2022 \u2022 \u2022 , y n , z): \u03b2(z) \u2208 arg min \u03b2\u2208R p L(y(z), \u03a6(X, \u03b2)) + \u2126(\u03b2) , (1", "formula_coordinates": [2.0, 83.82, 592.18, 201.75, 36.81]}, {"formula_id": "formula_5", "formula_text": ")", "formula_coordinates": [2.0, 285.57, 612.7, 3.87, 8.64]}, {"formula_id": "formula_6", "formula_text": "\u03b2 \u2208 R p \u03a6(X, \u03b2) = [\u03a6(x 1 , \u03b2), . . . , \u03a6(x n+1 , \u03b2)] \u2208 R n+1 .", "formula_coordinates": [2.0, 72.84, 636.85, 203.44, 37.24]}, {"formula_id": "formula_7", "formula_text": "\u00b5 z (x) := \u03a6(x, \u03b2(z)) .", "formula_coordinates": [2.0, 126.5, 707.59, 91.88, 10.32]}, {"formula_id": "formula_8", "formula_text": "L(y(z), \u03a6(X, \u03b2)) = n i=1 (y i , \u03a6(x i , \u03b2))+ (z, \u03a6(x n+1 , \u03b2)) .", "formula_coordinates": [2.0, 307.44, 89.51, 243.17, 30.32]}, {"formula_id": "formula_9", "formula_text": "(a, b) = exp(\u03b3(a \u2212 b)) \u2212 \u03b3(a \u2212 b) \u2212 1, for \u03b3 = 0.", "formula_coordinates": [2.0, 311.59, 249.19, 208.76, 17.29]}, {"formula_id": "formula_10", "formula_text": "\u2200i \u2208 [n], E i (z) = S(y i , \u00b5 z (x i )) ,(2)", "formula_coordinates": [2.0, 351.07, 408.72, 190.37, 17.29]}, {"formula_id": "formula_11", "formula_text": "E n+1 (z) = S(z, \u00b5 z (x n+1 )) , (3", "formula_coordinates": [2.0, 377.62, 423.91, 159.95, 10.32]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [2.0, 537.57, 424.9, 3.87, 8.64]}, {"formula_id": "formula_13", "formula_text": "\u03c0(z) := 1 \u2212 1 n + 1 Rank(E n+1 (z)) .(4)", "formula_coordinates": [2.0, 348.5, 501.49, 192.94, 23.78]}, {"formula_id": "formula_14", "formula_text": "(n + 1)(1 \u2212 \u03b1) , among {E i (z)} i\u2208[n+1] i.e., \u0393 (\u03b1) (x n+1 ) := {z \u2208 R : \u03c0(z) \u2265 \u03b1} .(5)", "formula_coordinates": [2.0, 347.96, 557.44, 193.49, 41.26]}, {"formula_id": "formula_15", "formula_text": "P(y n+1 \u2208 \u0393 (\u03b1) (x n+1 )) \u2265 1 \u2212 \u03b1 .", "formula_coordinates": [2.0, 355.26, 666.2, 138.37, 18.44]}, {"formula_id": "formula_16", "formula_text": "(E i (y n+1 )) i\u2208[n+1]", "formula_coordinates": [3.0, 54.28, 588.04, 74.27, 10.62]}, {"formula_id": "formula_17", "formula_text": "Definition 3.1 (Algorithmic Stability). A prediction func- tion \u00b5 \u2022 is stable if for any observed features x i , i \u2208 [n + 1], we have |S(q, \u00b5 z (x i )) \u2212 S(q, \u00b5 z0 (x i ))| \u2264 \u03c4 i \u2200z, z 0 , q \u2208 R . (6)", "formula_coordinates": [4.0, 55.08, 264.15, 236.01, 58.8]}, {"formula_id": "formula_18", "formula_text": "\u2200z,\u1e91, \u03c0 lo (z,\u1e91) \u2264 \u03c0(z) \u2264 \u03c0 up (z,\u1e91) , with \u03c0 lo (z,\u1e91) := 1 \u2212 1 n + 1 n+1 i=1 1 Li(z,\u1e91)\u2264Un+1(z,\u1e91) , \u03c0 up (z,\u1e91) := 1 \u2212 1 n + 1 n+1 i=1 1 Ui(z,\u1e91)\u2264Ln+1(z,\u1e91) ,", "formula_coordinates": [4.0, 55.44, 494.6, 214.69, 99.57]}, {"formula_id": "formula_19", "formula_text": "L i (z,\u1e91) = E i (\u1e91) \u2212 \u03c4 i , U i (z,\u1e91) = E i (\u1e91) + \u03c4 i , L n+1 (z,\u1e91) = S(z, \u00b5\u1e91(x n+1 )) \u2212 \u03c4 n+1 , U n+1 (z,\u1e91) = S(z, \u00b5\u1e91(x n+1 )) + \u03c4 n+1 .", "formula_coordinates": [4.0, 91.85, 619.5, 161.18, 55.4]}, {"formula_id": "formula_20", "formula_text": "|S(q, \u00b5 z (x i )) \u2212 S(q, \u00b5\u1e91(x i ))| \u2264 \u03c4 i .", "formula_coordinates": [4.0, 98.22, 707.34, 148.45, 17.29]}, {"formula_id": "formula_21", "formula_text": "U i (z,\u1e91) \u2264 L n+1 (z,\u1e91) =\u21d2 E i (z) \u2264 E n+1 (z) =\u21d2 L i (z,\u1e91) \u2264 U n+1 (z,\u1e91) .", "formula_coordinates": [4.0, 319.44, 111.78, 210.0, 32.23]}, {"formula_id": "formula_22", "formula_text": "\u0393 (\u03b1) lo (x n+1 ) \u2282 \u0393 (\u03b1) (x n+1 ) \u2282 \u0393 (\u03b1) up (x n+1 ) ,where", "formula_coordinates": [4.0, 307.44, 250.15, 204.78, 30.43]}, {"formula_id": "formula_23", "formula_text": "\u0393 (\u03b1) lo (x n+1 ) = {z : \u03c0 lo (z,\u1e91) \u2265 \u03b1} , \u0393 (\u03b1) up (x n+1 ) = {z : \u03c0 up (z,\u1e91) \u2265 \u03b1} .", "formula_coordinates": [4.0, 350.22, 287.29, 148.44, 36.42]}, {"formula_id": "formula_24", "formula_text": "P(y n+1 \u2208 \u0393 (\u03b1) up (x n+1 )) \u2265 1 \u2212 \u03b1 .", "formula_coordinates": [4.0, 355.26, 472.19, 138.37, 18.44]}, {"formula_id": "formula_25", "formula_text": "\u03c0 up (z,\u1e91) \u2265 \u03b1 \u21d4 n i=1 1 Ui(z,\u1e91)\u2264Ln+1(z,\u1e91) \u2264 (1\u2212\u03b1)(n+1) .", "formula_coordinates": [4.0, 307.44, 690.07, 236.18, 30.32]}, {"formula_id": "formula_26", "formula_text": "sup z\u2208Z Gap(z, 0) 1 |Z| z\u2208Z Gap(z, 0) max i\u2208[n+1] \u03c4i 1 n + 1 i\u2208[n+1] \u03c4i (d) Convergence of the approximation gap Gap(z,\u1e91)", "formula_coordinates": [5.0, 55.14, 228.84, 160.68, 131.94]}, {"formula_id": "formula_27", "formula_text": "(1 \u2212 \u03b1)(n + 1) elements of {U i (z,\u1e91)} i\u2208[n] are smaller than L n+1 (z,\u1e91). Which is equivalent to 1 L n+1 (z,\u1e91) \u2264 U ( (1\u2212\u03b1)(n+1) ) (z,\u1e91) =: Q 1\u2212\u03b1 (\u1e91) .", "formula_coordinates": [5.0, 54.28, 457.12, 235.16, 54.17]}, {"formula_id": "formula_28", "formula_text": "\u0393 (\u03b1) up (x n+1 ) = {z : S(z, \u00b5\u1e91(x n+1 )) \u2264 Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 }.", "formula_coordinates": [5.0, 56.52, 542.7, 231.84, 18.44]}, {"formula_id": "formula_29", "formula_text": "\u0393 (\u03b1) up (x n+1 ) = [\u00b5\u1e91(x n+1 ) \u00b1 (Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 )] .", "formula_coordinates": [5.0, 71.57, 592.56, 201.74, 18.44]}, {"formula_id": "formula_30", "formula_text": "prediction interval at x n+1 Fit a model \u00b5\u1e91 on the training data D n+1 (\u1e91) Compute the quantile Q 1\u2212\u03b1 (\u1e91) = U ( (1\u2212\u03b1)(n+1) ) (z,\u1e91)", "formula_coordinates": [5.0, 317.4, 494.85, 225.2, 34.53]}, {"formula_id": "formula_31", "formula_text": "Return: [\u00b5\u1e91(x n+1 ) \u00b1 (Q 1\u2212\u03b1 (\u1e91) + \u03c4 n+1 )]", "formula_coordinates": [5.0, 317.4, 542.43, 170.31, 17.29]}, {"formula_id": "formula_32", "formula_text": "max{\u03c0(z) \u2212 \u03c0 lo (z,\u1e91), \u03c0 up (z,\u1e91) \u2212 \u03c0(z)} \u2264 Gap(z,\u1e91) ,", "formula_coordinates": [5.0, 310.73, 643.62, 227.42, 17.29]}, {"formula_id": "formula_33", "formula_text": "dates Z =\u1e91 1 , \u2022 \u2022 \u2022 ,\u1e91 d as \u03c0 up (z, Z) = inf z\u2208Z \u03c0 up (z,\u1e91) and \u03c0 lo (z, Z) = sup z\u2208Z \u03c0 lo (z,\u1e91) .", "formula_coordinates": [6.0, 55.44, 69.3, 238.45, 37.65]}, {"formula_id": "formula_34", "formula_text": "\u00b5 z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3\u1e91 1\u2212\u1e91 z1\u2212zmin \u00b5 zmin + zmin\u2212\u1e91 z1\u2212zmin \u00b5\u1e91 1 if z \u2264 z min , z\u2212\u1e91t+1 zt\u2212\u1e91t+1 \u00b5\u1e91 t + z\u2212\u1e91t zt+1\u2212\u1e91t \u00b5\u1e91 t+1 if z \u2208 [\u1e91 t ,\u1e91 t+1 ] , z\u2212\u1e91 d zmax\u2212\u1e91 d \u00b5 zmax + zmax\u2212z zmax\u2212\u1e91 d \u00b5\u1e91 d if z \u2265 z max ,", "formula_coordinates": [6.0, 56.02, 170.48, 231.64, 55.84]}, {"formula_id": "formula_35", "formula_text": "\u03b2\u2208R p L(y(z), \u03a6(X, \u03b2)) = F z (\u03a6(X, \u03b2)) . (7", "formula_coordinates": [6.0, 99.24, 701.53, 186.33, 17.51]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [6.0, 285.57, 702.76, 3.87, 8.64]}, {"formula_id": "formula_37", "formula_text": "f (\u03c2w 0 + (1 \u2212 \u03c2)w) \u2264 \u03c2f (w 0 ) + (1 \u2212 \u03c2)f (w) \u2212 \u03bb 2 \u03c2(1 \u2212 \u03c2) w 0 \u2212 w 2 .", "formula_coordinates": [6.0, 319.78, 99.18, 209.32, 38.52]}, {"formula_id": "formula_38", "formula_text": "\u00b5 z (X) \u2212 \u00b5 z0 (X) \u2264 2\u03c1 \u03bb .", "formula_coordinates": [6.0, 372.4, 168.78, 109.07, 23.78]}, {"formula_id": "formula_39", "formula_text": "F z (\u03a6(X, \u03b2(z))) \u2264 F z (\u03a6(X, \u03b2)) \u2200\u03b2 .(8)", "formula_coordinates": [6.0, 343.92, 221.05, 197.52, 17.29]}, {"formula_id": "formula_40", "formula_text": "0 (8) \u2264 F z (\u03c2w 0 + (1 \u2212 \u03c2)w) \u2212 F z (w) \u03c2 (3.6) \u2264 F z (w 0 ) \u2212 F z (w) \u2212 \u03bb 2 (1 \u2212 \u03c2) w 0 \u2212 w 2 .", "formula_coordinates": [6.0, 324.98, 280.04, 198.92, 50.25]}, {"formula_id": "formula_41", "formula_text": "\u03bb 2 w 0 \u2212 w 2 \u2264 F z (w 0 ) \u2212 F z (w) \u2264 \u03c1 w \u2212 w 0 .", "formula_coordinates": [6.0, 321.6, 349.9, 206.88, 23.78]}, {"formula_id": "formula_42", "formula_text": "\u03c4 i = 2\u03b3\u03c1 \u03bb , \u2200i \u2208 [n + 1] .", "formula_coordinates": [6.0, 363.89, 481.32, 121.11, 23.78]}, {"formula_id": "formula_43", "formula_text": "\u03b2(z) \u2212 \u03b2(z 0 ) \u2264 2\u03c1 \u03bb .", "formula_coordinates": [6.0, 380.58, 640.06, 92.71, 23.78]}, {"formula_id": "formula_44", "formula_text": "\u03b2(z) \u2212 \u03b2(z 0 ) \u2264 2 \u221a 2\u03bdC \u03bb \u2212 \u03bd .", "formula_coordinates": [6.0, 369.82, 689.48, 114.23, 39.29]}, {"formula_id": "formula_45", "formula_text": "|\u00b5 z (x) \u2212 \u00b5 z0 (x)| \u2264 L \u03a6 |x \u03b2(z) \u2212 x \u03b2(z 0 )| .", "formula_coordinates": [7.0, 78.48, 141.81, 187.92, 17.29]}, {"formula_id": "formula_46", "formula_text": "\u03c4 i = 2\u03b3\u03c1L \u03a6 x i \u03bb , \u2200i \u2208 [n + 1] .", "formula_coordinates": [7.0, 95.09, 182.73, 154.71, 23.78]}, {"formula_id": "formula_47", "formula_text": "\u03c4 i = 2\u03b3L \u03a6 x i \u221a 2\u03bdC \u03bb \u2212 \u03bd , \u2200i \u2208 [n + 1] .", "formula_coordinates": [7.0, 83.5, 226.01, 177.89, 39.29]}, {"formula_id": "formula_48", "formula_text": "\u03b8(z) \u2208 arg max \u03b8\u2208R n+1 \u2212L * (y(z), \u2212\u03b8) \u2212 \u2126 * (X \u03b8) ,(9)", "formula_coordinates": [7.0, 72.57, 351.39, 216.87, 19.14]}, {"formula_id": "formula_49", "formula_text": "\u2200(\u03b2, \u03b8) \u2208 dom P z \u00d7 dom D z 1 2\u03bd \u03b8(z) \u2212 \u03b8 2 \u2264 D z (\u03b8(z)) \u2212 D z (\u03b8) = P z (\u03b2(z)) \u2212 D z (\u03b8) \u2264 Duality Gap z (\u03b2, \u03b8) ,", "formula_coordinates": [7.0, 55.44, 505.39, 200.71, 77.16]}, {"formula_id": "formula_50", "formula_text": "Duality Gap z (\u03b2, \u03b8) := P z (\u03b2) \u2212 D z (\u03b8) \u2265 P z (\u03b2) \u2212 P z (\u03b2(z)) .", "formula_coordinates": [7.0, 83.3, 631.28, 178.28, 32.23]}, {"formula_id": "formula_51", "formula_text": "\u03b8(z) \u2212 \u03b8 \u2264 2\u03bd\u03c1 * .", "formula_coordinates": [7.0, 129.36, 707.34, 91.15, 17.29]}, {"formula_id": "formula_52", "formula_text": "\u03b8(z) \u2212 \u03b8 \u2264 \u221a 2\u03bdC .", "formula_coordinates": [7.0, 383.14, 90.54, 87.59, 26.21]}, {"formula_id": "formula_53", "formula_text": "P(y n+1 \u2208 [y (1) , y (n) ]) \u2265 1 \u2212 2 n + 1 .", "formula_coordinates": [7.0, 348.2, 312.58, 152.48, 23.78]}, {"formula_id": "formula_54", "formula_text": "z \u2208 [y (1) , y (n) ], which implies L(y(z), \u00b5 z (X)) \u2264 sup z\u2208[y (1) ,y (n) ]", "formula_coordinates": [7.0, 307.44, 351.48, 140.19, 35.8]}, {"formula_id": "formula_55", "formula_text": "\u2200i \u2208 [n], E or i = S(y i , \u00b5 yn+1 (x i )) , E or n+1 (z) = S(z, \u00b5 yn+1 (x n+1 )) ,", "formula_coordinates": [9.0, 91.4, 125.2, 162.07, 27.64]}, {"formula_id": "formula_56", "formula_text": "\u0393 (\u03b1) oracle (x n+1 ) := {z : \u03c0 oracle (z) \u2265 \u03b1} , \u03c0 oracle (z) = 1 \u2212 1 n + 1 n+1 i=1 1 E or i \u2264E or n+1 (z) .", "formula_coordinates": [9.0, 76.06, 181.44, 192.77, 48.56]}, {"formula_id": "formula_57", "formula_text": "D tr = {(x 1 , y 1 ), \u2022 \u2022 \u2022 , (x m , y m )} with m < n ,", "formula_coordinates": [9.0, 87.66, 339.24, 189.5, 17.29]}, {"formula_id": "formula_58", "formula_text": "D cal = {(x m+1 , y m+1 ), \u2022 \u2022 \u2022 , (x n , y n )} .", "formula_coordinates": [9.0, 100.2, 385.45, 164.41, 17.29]}, {"formula_id": "formula_59", "formula_text": "\u2200i \u2208 [m + 1, n], E cal i = S(y i , \u00b5 tr (x i )) , E cal n+1 (z) = S(z, \u00b5 tr (x n+1 )) .", "formula_coordinates": [9.0, 80.5, 442.45, 183.88, 29.22]}, {"formula_id": "formula_60", "formula_text": "\u0393 (\u03b1) split (x n+1 ) = {z : \u03c0 split (z) \u2265 \u03b1} , \u03c0 split (z) = 1 \u2212 1 n \u2212 m + 1 n+1 i=m+1 1 E cal i \u2264E cal n+1 (z) .", "formula_coordinates": [9.0, 61.49, 501.89, 221.91, 52.63]}, {"formula_id": "formula_62", "formula_text": "z = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3\u1e91 1\u2212\u1e91 z1\u2212zmin \u00b5 zmin + zmin\u2212\u1e91 z1\u2212zmin \u00b5\u1e91 1 if z \u2264 z min , z\u2212\u1e91t+1 zt\u2212\u1e91t+1 \u00b5\u1e91 t + z\u2212\u1e91t zt+1\u2212\u1e91t \u00b5\u1e91 t+1 if z \u2208 [\u1e91 t ,\u1e91 t+1 ] , z\u2212\u1e91 d zmax\u2212\u1e91 d \u00b5 zmax + zmax\u2212z zmax\u2212\u1e91 d \u00b5\u1e91 d if z \u2265 z max ,(11)", "formula_coordinates": [12.0, 188.3, 567.31, 353.14, 55.84]}, {"formula_id": "formula_63", "formula_text": "|S(q,\u03bc z (x i )) \u2212 S(q,\u03bc z0 (x i ))| \u2264 3\u03b3\u03c4 i .(12)", "formula_coordinates": [12.0, 217.07, 644.85, 324.37, 17.29]}, {"formula_id": "formula_64", "formula_text": "stab := |S(q,\u03bc z (x i )) \u2212 S(q,\u03bc z0 (x i ))| \u2264 |S(q,\u03bc z (x i )) \u2212 S(q, \u00b5 z (x i ))| + |S(q, \u00b5 z (x i )) \u2212 S(q, \u00b5 z0 (x i ))| + |S(q, \u00b5 z0 (x i )) \u2212 S(q,\u03bc z0 (x i ))| .", "formula_coordinates": [12.0, 81.56, 688.81, 433.75, 32.23]}], "doi": ""}