{"title": "Hexatagging: Projective Dependency Parsing as Tagging", "authors": "Afra Amini; Tianyu Liu; Ryan Cotterell", "pub_date": "", "abstract": "We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser's linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding.", "sections": [{"heading": "Introduction", "text": "The combination of parallel computing hardware and highly parallelizable neural network architectures (Vaswani et al., 2017) has enabled the pretraining of language models on increasingly large amounts of data. In order to apply pretrained language models to downstream NLP tasks, many practitioners fine-tune the pretrained model while the task-specific architecture is jointly trained from scratch. Typically, the task-specific architecture is built upon the hidden representations generated by the final layer of a pretrained model. Exploiting pretrained language models in this manner has boosted the performance considerably on many NLP tasks (Devlin et al., 2019;Clark et al., 2020; * Equal contribution. From bottom to top, the figure shows the dependency tree, the hexatags, and the binary head tree for the sentence \"She reads fascinating papers.\" Aghajanyan et al., 2021). However, for the end-toend fine-tuning process to be fully parallelizable, it is also necessary to parallelize the training of the task-specific architecture. Unfortunately, due to the complexity of the output in many structured prediction tasks in natural language, e.g., in dependency parsing, state-of-the-art models still use architectures with limited parallelization during training (Mrini et al., 2020;Yang and Tu, 2022).\nIn an attempt to develop parsers parallelizable during training, a recent line of work recasts parsing as tagging (Li et al., 2018;Strzyz et al., 2019;Kitaev and Klein, 2020;Amini and Cotterell, 2022). Under this approach, a parse tree is linearized into a sequence of tags. 1 The benefit of such a paradigm is that tagging can be done by only adding a linear classifier on top of a pretrained language model and the tags can, thus, be predicted independently. This leads to a parser that is highly parallelizable and whose training can be easily harmonized with the (parallelizable) fine-tuning of pretrained language models. During decoding, an exact algorithm is used to recover a valid sequence of tags which is then converted back to a parse tree. Kitaev and Klein (2020) were the first to propose a parsing-as-tagging scheme with a constant tag space for constituency parsing and, additionally, the first to achieve results competitive with the stateof-the-art non-parallelizable constituency parsers using such a tagger. However, for dependency parsing, all dependency parsing-as-tagging schemes in the literature (Li et al., 2018;Strzyz et al., 2019;Vacareanu et al., 2020) have infinite tag sets whose cardinality grows with the length of the input sequence, which limits such parsers' efficiency and generality (Strzyz et al., 2019). Moreover, in some cases, this growth hinders generalization to sentences longer than the longest training sentence. Furthermore, tagging-based dependency parsers still do not perform competitively with the bestperforming parsers in the literature (Li et al., 2018).\nIn this paper, we propose a novel way of framing projective dependency parsing as a tagging task. Our approach makes use of 6 distinct tags, motivating us to naming the scheme hexatagger. In our experiments, hexatagger achieves state-of-the-art performance on the English Penn Treebank (PTB; Marcus et al., 1993) test set. Notably, it outperforms parsers with more computationally expensive training procedures and extra constituency annotations, e.g., the parser developed by Mrini et al. (2020). Furthermore, hexatagger achieves results competitive to Yang and Tu's (2022) parser on the Chinese Penn Treebank (CTB; Xue et al., 2005) test set and 12 languages on the pseudo-projectivized data from the Universal Dependencies (UD2.2; Nivre et al., 2018) benchmark. In terms of efficiency, our experiments suggest that hexatagger is 10 times faster than previous top-performing parsers, and consumes significantly less memory, despite using an exact dynamic program for decoding.", "publication_ref": ["b30", "b7", "b4", "b0", "b23", "b34", "b19", "b28", "b15", "b1", "b15", "b19", "b28", "b29", "b28", "b19", "b20", "b23", "b34", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Hexatagging", "text": "In this section, we introduce hexatagging, a tagging scheme that consists of 6 unique tag types. We further prove by construction that there exists an injective mapping between valid sequences of hexatags and dependency trees.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Binary Head Trees", "text": "Before going into the details on how to represent dependency trees with a sequence of tags, we introduce binary head trees (BHTs), a simple formalism that serves as a useful intermediary between dependency trees and sequence of hexatags. Intuitively, a BHT is a special form of a constituency tree where each internal node is either labeled L when the head of the derived constituent is in the left subtree or R when the head is in the right subtree. See Fig. 1 for a visual depiction of a BHT. In the next theorem, we formally state the relationship between the set of dependency trees and BHTs.\nTheorem 1. There exists a bijective 2 function that maps every projective dependency tree to a BHT.\nIn the following two paragraphs, we sketch a construction that such a function exists, i.e., we describe how to map any dependency tree to a BHT and then how to map back any BHT to a dependency tree and back again.\nProjective Dependency Trees to BHTs. To convert a dependency tree to a BHT, we start from the root and do a depth-first traversal of the dependency tree. To avoid spurious ambiguity (Eisner and Satta, 1999), we canonically order arcs of the tree by processing the arcs left to right and inside out. 3 Algorithmically, converting a dependency tree to a BHT proceeds as follows. When we first visit a word, we push it onto a stack and proceed with visiting its dependents. When there is no dependent word left to visit, we create a new node ( L or R ) and attach the top two elements in the stack as the left and right child of this node. A stepby-step demonstration of this algorithm is shown in Fig. 2 and pseudocode is provided in Alg. 1.\nBHTs to Projective Dependency Trees. To convert a BHT back to the dependency tree we follow Alg. 2. Algorithmically, we process BHT in a depth-first fashion. Upon visiting R or L nodes, we combine the top two elements in the stack by creating a dependency arc between them. The direction of the arc is determined by the label of the node ( R or L ). See Fig. 3 for an example. Once the dependency tree is converted to a BHT, we can linearize it to a sequence of hexatags in a straightforward manner. Theorem 2 states the relationship between BHTs and hexatags formally.\nTheorem 2. There exists a total and injective function that maps every BHT to a valid hexatag sequence, i.e., in other words, every BHT can be mapped to a unique hexatag sequence. However, some hexatag sequences do not correspond to BHTs, i.e., the function is not surjective.\nIn the following subsections, we prove by construction that such a function exists. Throughout the rest of the paper, we refer to those haxatagging sequences that do correspond to BHTs as valid.", "publication_ref": ["b11"], "figure_ref": ["fig_1", "fig_3"], "table_ref": []}, {"heading": "From BHT to Hexatags", "text": "To transform a given BHT to a sequence of hexatags, we enumerate the action sequence that a left-corner shift-reduce parser would take when parsing this BHT (Johnson, 1998). Left-corner parsers have actions that align more closely with the input sequence than top-down or bottom-up shift-reduce actions and, thus, offer a better linearization for tagging tasks (Amini and Cotterell, 2022). A simple explanation of this linearization process is given by Kitaev and Klein (2020, \u00a73.1). Their algorithm involves an in-order traversal of the tree. Upon visiting each node, we generate a tag that includes the direction of the arc that attaches the node to its parent, i.e., whether that node is a left or a right child of its parent, and the label of the node. When traversing a BHT, this paradigm results in 6 distinct tag types: \u2022 \u2192 : this terminal node is the right child of its parent; \u2022 \u2192 : this terminal node is the left child of its parent; \u2022 \u21d2 R ( \u21d2 L ): this non-terminal node is the right child of its parent and the head of the corresponding constituent is on the right (respectively, left) subtree; \u2022 \u21d2 R ( \u21d2 L ): this non-terminal node is the left child of its parent and the head of the corresponding constituent is on the right (respectively, left) subtree. For an input sequence w = w 1 \u2022 \u2022 \u2022 w N , this process gives us a hexatag sequence of length 2N \u2212 1. Fig. 1 depicts tree-to-tags transformation through an example.\nLabeled Dependency Trees. When converting a labeled dependency tree to a sequence of hexatags, the arc labels must be encoded in the tags. Therefore, while reading a terminal node, we concatenate the label of the arc that connects the node to its parent with the hexatag. In this case, the number of distinct tags would be O(|A|), where |A| is the number of unique arc labels. For example, in Fig. 1 the hexatag generated while processing she is: \u27e8 \u2192 , nsubj\u27e9.", "publication_ref": ["b12", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "From Hexatags to Dependency Trees", "text": "To transform a sequence of hexatags back to a dependency tree, we again go through a two-step process. First, we again interpret hexatags as actions in a left-corner shift-reduce transition system to construct a BHT. The actions in such a transition system are as follows:\n\u2022 \u2192 : shift the leaf node into the stack; \u2022 \u21d2 R ( \u21d2 L ): create a new node labeled R (respectively, L ), attach the top element in the stack as its left child, and attach a dummy node as its right child (\u2205 in step 2 in Fig. 3 (1) \n\u2192 $ A R B L $ A R C D A (2) \u21d2 R R \u2205 $ A R B L $ A R C D A B (3) \u2192 R B $ A R B L $ A R C D A B (4) \u21d2 L R B L $ \u2205 A R B L $ A R C D A B C (5) \u2192 R B L $ \u2205 A $ C R B L $ A R C D A B C D (6) \u21d2 R R B L $ A R C \u2205 R B L $ A R C D A B C D (7) \u2192 R B L $ A R C D R B L $ A R C D A B C D", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Probability Model", "text": "In this section, we explain how to predict hexatags in parallel. Our tagging model predicts two hexatags for each word in the input sequence with the exception of that last word, for which we only predict one tag. As discussed in \u00a72.1, a hexatagger produces a sequence of 2N \u2212 1 tags t = [t 1 , t 2 , . . . , t 2N \u22121 ] for an input sequence of length N , w = w 1 w 2 \u2022 \u2022 \u2022 w N . Therefore, an intuitive way to match the tag sequence with the input sequence is to assign two tags to each word. We denote a training corpus S of M tuples of input sequences and tag sequences {(w m , t m )} M m=1 . To learn the scoring function over tags, we follow the same independence assumption as in (Kitaev and Klein, 2020), i.e., the probability of predicting each tag is independent of other tags given the input sequence. This assumption barely harms model performance (see Amini and Cotterell, 2022, Table 3), but significantly speeds up the training process by enabling each tag to be predicted in parallel, and complexity reduces by a factor of O(N ). The training objective is to minimize the negative log-likelihood of the gold-standard tag sequences, i.e.\nL(\u03b8) = \u2212 (w,t)\u2208S log p \u03b8 (t | w) (1a) = \u2212 (w,t)\u2208S log 2N \u22121 n=1 p \u03b8 (t n | w) (1b) = \u2212 (w,t)\u2208S N n=1 log p \u03b8 (t 2n\u22121 | w) (1c) + N \u22121 n=1 log p \u03b8 (t 2n | w)\nwhere \u03b8 refers collectively to the parameters of the two linear projections and the parameters of the pretrained model. To obtain p \u03b8 (t 2n | w) and p \u03b8 (t 2n+1 | w), we apply two independent linear projections on the contextualized representation of w n 4 given by a pretrained model and convert that to a probability distribution using softmax.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Decoding", "text": "Our goal in this section is to develop an efficient algorithm to find the highest-scoring hexatag sequence under the model developed in \u00a73. As stated in Theorem 2, the transformation function between BHTs and hexatag sequences is not surjective, i.e., not all the tag sequences can be transformed back into a BHT. Therefore, we need to find a valid hexatag sequence with the maximum probability under the model that can be transformed back to a BHT. Once such hexatag sequence is found, we can follow the two-step algorithm described in \u00a72.3 to obtain the corresponding dependency tree.\nTo find the highest-scoring valid hexatag sequence, we follow the linear-time algorithm developed by Kitaev and Klein (2020). For a hexatag sequence to be valid, we should be able to interpret it as actions in a left-corner shift-reduce transitions system, described in \u00a72.3. Concretely:\n\u2022  \u2022 After performing all the actions, the stack should contain a single element. The above shows that the validity of a hexatag sequence only depends on the number of elements in the stack at each point of the derivation. 5 ", "publication_ref": ["b15", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We conduct experiments on the English Penn Treebank (PTB; Marcus et al., 1993), the Chinese Penn Treebank (CTB; Xue et al., 2005), and the Universal Dependencies 2.2 (UD2.2; Nivre et al., 2018). For UD2.2, we adopt the pseudo-projective transformation (Nivre and Nilsson, 2005) to convert non-projective trees into projective trees following previous work (Wang and Tu, 2020;Yang and Tu, 2022). We report dataset statistics in App. E and hyperparameter settings in App. F.", "publication_ref": ["b20", "b32", "b27", "b31", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy.", "text": "We train the hexatagger model based on XLNet (Yang et al., 2019) and report the results on PTB and CTB in Table 2. Furthermore, we eval- 5 Specifically, The decoding algorithm can be thought of as constructing a lattice where each node corresponds to the number of elements in the stack for each transition step (N \u00d7d nodes for maximum stack size of d, d \u2264 N ). Each transition corresponds to performing a valid action. The score of the tag at step n is set to the negative log probability \u2212 log p \u03b8 (tn | w) of the corresponding hexatag given by the model. Finally, we remark that our decoding algorithm is essentially a shortestpath dynamic program that finds the highest-scoring valid hexatag sequence. See Amini and Cotterell (2022, \u00a75.1)   uate hexatagger in a set of 12 topologically diverse languages on UD corpus, where we use Multilingual BERT (Devlin et al., 2019) as the underlying model (see Table 1). In PTB, we observe that hexatagger achieves state-of-the-art results, compared to models with custom architectures and even in some cases with extra annotation. In CTB and UD, hexatagger follows the best performance closely.\nEfficiency. We compare the efficiency of hexatagger with biaffine modules, 6 which are the backbone of many neural graph-based parsers (Kiperwasser and Goldberg, 2016;Mrini et al., 2020;Yang and Tu, 2022). As depicted in Table 3, we observe that our hexatagger is an order of magnitude faster and consumes less memory. Further analysis is included in App. C.", "publication_ref": ["b35", "b42", "b7", "b14", "b23", "b34"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Conclusion", "text": "In summary, hexatagging, our novel scheme, offers a parallelizable and efficiently decodable backbone for dependency parsing. Without relying on custom architecture for dependency parsing, the hexatagger achieves state-of-the-art accuracy on several datasets using no more than a pretrained language model and linear classifiers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Non-projectivity. The primary theoretical limitation of hexatagger is that it can only produce projective dependency trees. We would like to explore the possibility of extending hexatagger to non-projective parsing for future work.\nInterpretibility. As a trade-off for efficiency, hexatagger does not model dependency arcs directly. Compared to graph-based models that explicitly score arc scores between pairs of words, it is more difficult to interpret the output of hexatagger.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "We do not believe the work presented here further amplifies biases already present in the datasets. Therefore, we foresee no ethical concerns in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Algorithms", "text": "Algorithm 1 Create a BHT from a dependency tree. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Related Work", "text": "Traditionally, approaches to dependency parsing have been taxonomized into graph-based and transition-based parsers. The authors of this paper take the stance that this distinction is misleading because the difference lies not in the models themselves, but rather in whether exact or approximate inference algorithms are employed. For instance, Kuhlmann et al. (2011) gives exact algorithms for transition-based dependency parsers, which exposes the inability to formally distinguish graph-based and transition-based parsers. Thus, we classify our related work into sections: exact and approximate decoding. Further, we review works on tagging-based parsing which is the most relevant line of work to this paper.\nExact Decoding. Most exact algorithms for projective dependency parsing models apply a modified form of the CKY algorithm on nested dependency trees. The best runtime among the commonly deployed algorithms O N 3 (Eisner, 1996), but algorithms based on fast matrix multiplication exist and can achieve a lower runtime bound (Cohen and Gildea, 2016). However, exact decoding of non-projective parsers is intractable unless under independence assumptions, e.g., edge factored assumption (McDonald and Satta, 2007). Edgefactored parsers (McDonald et al., 2005; construct graphs by scoring all possible arcs between each pair of words. They then use the maximum spanning tree (MST) finding algorithms for decoding to build the valid dependency trees with maximum score in O N 2 (Zmigrod et al., 2020). The discussed algorithms are exact in inferring the dependency structure, however, they are neither fast nor parallelizable.\nApproximate Decoding. Despite not being exact, transition-based parsers offer faster and typically linear-time parsing algorithms (Kudo and Matsumoto, 2002;Yamada and Matsumoto, 2003;Nivre, 2003). The dependency tree is inferred with a greedy search through transition system actions. Following this approach, actions are not predicted in parallel and the configuration of the transition system (stack and buffer) needs to be modeled with a neural network (Chen and Manning, 2014), which prevents using pretrained models out of the box.\nTagging-based parsing. Inspired by Bangalore and Joshi's (1999) seminal work supertagging, a recent line of work aims to utilize pretrained models and parse dependency trees by inferring tags for each word in the input sequence. Li et al. (2018); Kiperwasser and Ballesteros (2018) predict the relative position of the dependent with respect to its parent as the tag. They then use beam tree constraints (Lee et al., 2016) to infer valid dependency trees. Strzyz et al. (2019) provides a framework for analyzing similar tagging schemes. Although these works have demonstrated potential in this area, none achieved state-of-the-art results compared to custom architectures and algorithms developed for dependency parsing. Additionally, the output space, or size of the tag set, is unrestricted, which limits the efficiency of this approach.", "publication_ref": ["b17", "b10", "b5", "b22", "b21", "b39", "b16", "b33", "b24", "b3", "b2", "b19", "b13", "b18", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "C Analysis", "text": "LEFT-FIRST vs. RIGHT-FIRST. We examine the effect of the two orders of binarization of Alg. 1 in  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Efficiency Evaluation", "text": "For efficiency comparison, we use BERT-large as the base feature encoder for both Hexatagger and Biaffine. We use the English PTB test set and truncate or pad the input sentences to the control length. The results are averaged over 3 random runs on the same server with one Nvidia A100-80GB GPU. The other experimental settings are kept the same (i.e., the version of PyTorch and Transformer, FP32 precision, batching).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Datasets", "text": "Preprocessing. Following previous work (Kiperwasser and Goldberg, 2016;, the dependency annotations are derived by the Stanford Dependency converter v3.3.0 (de Marneffe and Manning, 2008) from the treebank annotations. Punctuation is omitted for evaluation. Gold part-of-speech tags are provided to the model both during training and evaluation following the code released by Mrini et al. (2020). Some other authors use system-predicted partof-speech tags (Zhou and Zhao, 2019) or use mixed configurations. E.g., Yang and Tu (2022) uses gold part-of-speech tags on CTB and UD, while not using any on PTB,  uses gold part-of-speech tags on CTB but systempredicted ones on PTB. Our preliminary experiments show that removing the usage of part-ofspeech information barely affects the UAS metric, and gives us a performance of 97.4 UAS and 95.8 LAS on PTB.\nSplits. All the datasets splits are consistent with previous work. For PTB, we follow the standard split of Marcus et al. (1993), resulting in 39,832 sentences for training, 1,700 for development, and 2,416 for testing. For CTB, we follow the split of Zhang and Clark (2008), resulting in 16,091 sentences for training, 803 for development, and 1,910 for testing. For UD2.2, we follow Yang and Tu (2022) and use the standard splits of the following corpora for experiments: BG-btb, CA-ancora, CSpdt, DE-gsd, EN-ewt, ES-ancora, FR-gsd, IT-isdt, NL-alpino, NO-rrt, RO-rrt, RU-syntagrus.\nLicenses. The PTB and CTB datasets are licensed under LDC User Agreement. The UD2.2 dataset is licensed under the Universal Dependencies License Agreement.", "publication_ref": ["b14", "b23", "b38", "b34", "b20", "b37", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "F Hyperparameter Settings", "text": "We use the Python NLTK package to process the datasets, i.e., converting CoNLL-U formatted data to dependency trees, extracting dependency arcs from dependency trees for evaluation, implementing Alg. 1 and 2. For UD, we apply MaltParser v1.9.2 7 to pseudo-projectivize the non-projective trees (Nivre and Nilsson, 2005).\nWe use xlnet-large-cased 8 for English PTB, chinese-xlnet-mid 9 for CTB, and bert-multilingualcased 10 for UD.\nThe dimension of POS tag embedding is set to 256 for all experiments. On top of concatenated pretrained representations and POS embedding, we use a 3-layer BiLSTM with a hidden size of 768 for base-sized models (bert-multilingual-cased on UD) and 1024 for large-sized models (xlnet-large-cased on PTB and chinese-xlnet-mid on CTB).\nDropout layers with a rate of 0.33 are applied after the concatenated embedding layer, between LSTM layers, and before the MLP projection layer to hexatags.\nFor training, we used AdamW with a learning rate of 2e\u22125 for pretrained LMs and 1e\u22124 for POS embedding, BiLSTM, and MLP. The gradient clipping threshold is set to 1.0. The batch size is set to 32. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? App D B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? App D B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. App D C Did you run computational experiments? 5 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? App. E", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank Tim Vieira for his invaluable feedback throughout the process of this paper. Afra Amini is supported by ETH AI Center doctoral fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Muppet: Massive multi-task representations with pre-finetuning", "journal": "", "year": "2021", "authors": "Armen Aghajanyan; Anchit Gupta; Akshat Shrivastava; Xilun Chen; Luke Zettlemoyer; Sonal Gupta"}, {"ref_id": "b1", "title": "On parsing as tagging", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Afra Amini; Ryan Cotterell"}, {"ref_id": "b2", "title": "Supertagging: An approach to almost parsing", "journal": "Computational Linguistics", "year": "1999", "authors": "Srinivas Bangalore; Aravind K Joshi"}, {"ref_id": "b3", "title": "A fast and accurate dependency parser using neural networks", "journal": "", "year": "2014", "authors": "Danqi Chen; Christopher Manning"}, {"ref_id": "b4", "title": "ELECTRA: pretraining text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; Quoc V Le; Christopher D Manning"}, {"ref_id": "b5", "title": "Parsing Linear Context-Free Rewriting Systems with Fast Matrix Multiplication", "journal": "Computational Linguistics", "year": "2016", "authors": "B Shay; Daniel Cohen;  Gildea"}, {"ref_id": "b6", "title": "The Stanford typed dependencies representation", "journal": "", "year": "2008", "authors": "Marie-Catherine De Marneffe; Christopher D Manning"}, {"ref_id": "b7", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "Deep biaffine attention for neural dependency parsing", "journal": "", "year": "2017-04-24", "authors": "Timothy Dozat; Christopher D Manning"}, {"ref_id": "b9", "title": "Stanford's graph-based neural dependency parser at the CoNLL 2017 shared task", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Timothy Dozat; Peng Qi; Christopher D Manning"}, {"ref_id": "b10", "title": "Efficient normal-form parsing for combinatory categorial grammar", "journal": "", "year": "1996", "authors": "Jason Eisner"}, {"ref_id": "b11", "title": "Efficient parsing for bilexical context-free grammars and head automaton grammars", "journal": "Association for Computational Linguistics", "year": "1999", "authors": "Jason Eisner; Giorgio Satta"}, {"ref_id": "b12", "title": "Finite-state approximation of constraint-based grammars using left-corner grammar transforms", "journal": "Association for Computational Linguistics", "year": "1998", "authors": "Mark Johnson"}, {"ref_id": "b13", "title": "Scheduled multi-task learning: From syntax to translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Eliyahu Kiperwasser; Miguel Ballesteros"}, {"ref_id": "b14", "title": "Simple and accurate dependency parsing using bidirectional LSTM feature representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Eliyahu Kiperwasser; Yoav Goldberg"}, {"ref_id": "b15", "title": "Tetra-tagging: Word-synchronous parsing with linear-time inference", "journal": "", "year": "2020", "authors": "Nikita Kitaev; Dan Klein"}, {"ref_id": "b16", "title": "Japanese dependency analysis using cascaded chunking", "journal": "", "year": "2002", "authors": "Taku Kudo; Yuji Matsumoto"}, {"ref_id": "b17", "title": "Dynamic programming algorithms for transition-based dependency parsers", "journal": "Association for Computational Linguistics", "year": "2011", "authors": "Marco Kuhlmann; Carlos G\u00f3mez-Rodr\u00edguez; Giorgio Satta"}, {"ref_id": "b18", "title": "Global neural CCG parsing with optimality guarantees", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Kenton Lee; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b19", "title": "Seq2seq dependency parsing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Zuchao Li; Jiaxun Cai; Shexia He; Hai Zhao"}, {"ref_id": "b20", "title": "Building a large annotated corpus of English: The Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz"}, {"ref_id": "b21", "title": "Non-projective dependency parsing using spanning tree algorithms", "journal": "", "year": "2005", "authors": "Ryan Mcdonald; Fernando Pereira; Kiril Ribarov"}, {"ref_id": "b22", "title": "On the complexity of non-projective data-driven dependency parsing", "journal": "", "year": "2007", "authors": "Ryan Mcdonald; Giorgio Satta"}, {"ref_id": "b23", "title": "Rethinking self-attention: Towards interpretability in neural parsing", "journal": "", "year": "2020", "authors": "Khalil Mrini; Franck Dernoncourt; Trung Quan Hung Tran; Walter Bui; Ndapa Chang;  Nakashole"}, {"ref_id": "b24", "title": "An efficient algorithm for projective dependency parsing", "journal": "", "year": "2003", "authors": "Joakim Nivre"}, {"ref_id": "b25", "title": "Aur\u00e9lie Collomb, \u00c7agr\u0131 \u00c7\u00f6ltekin, Miriam Connor, Marine Courtin, Elizabeth Davidson, Marie-Catherine de Marneffe, Valeria de Paiva, Arantza Diaz de Ilarraza, Carly Dickerson", "journal": "C\u0203t\u0203lina M\u0203r\u0203nduc", "year": "", "authors": "Joakim Nivre; Mitchell Abrams; \u017deljko Agi\u0107; Lars Ahrenberg; Lene Antonsen; Maria Jesus Aranzabe; Gashaw Arutie; Masayuki Asahara; Luma Ateyah; Mohammed Attia; Aitziber Atutxa; Liesbeth Augustinus; Elena Badmaeva; Miguel Ballesteros; Esha Banerjee; Sebastian Bank; Barbu Verginica; John Mititelu; Sandra Bauer; Kepa Bellato; Riyaz Ahmad Bengoetxea; Erica Bhat; Eckhard Biagetti; Rogier Bick; Victoria Blokland; Carl Bobicev; Cristina B\u00f6rstell; Gosse Bosco; Sam Bouma; Adriane Bowman; Aljoscha Boyd; Marie Burchardt; Bernard Candito; Gauthier Caron; G\u00fcl\u015fen Caron; Giuseppe G A Cebiroglu Eryigit; Savas Celano; Fabricio Cetin; Jinho Chalub; Yongseok Choi; Jayeol Cho; Silvie Chun; Kaja Cinkov\u00e1 ; Peter Dirix; Timothy Dobrovoljc; Kira Dozat; Puneet Droganova; Marhaba Dwivedi; Ali Eli; Binyam Elkahky; Toma\u017e Ephrem; Aline Erjavec; Rich\u00e1rd Etienne; Hector Farkas; Jennifer Fernandez Alcalde; Cl\u00e1udia Foster; Katar\u00edna Freitas; Daniel Gajdo\u0161ov\u00e1; Marcos Galbraith; Moa Garcia; Kim G\u00e4rdenfors; Filip Gerdes; Iakes Ginter; Koldo Goenaga; Memduh Gojenola; Yoav G\u00f6k\u0131rmak; Xavier G\u00f3mez Goldberg; Berta Gonz\u00e1les Guinovart; Matias Saavedra; Normunds Grioni; Bruno Gr\u016bz\u012btis; Fredrik Guillaume ; Anders Johannsen; H\u00fcner J\u00f8rgensen;  Ka\u015f\u0131kara; Hiroshi Sylvain Kahane; Jenna Kanayama; Tolga Kanerva; V\u00e1clava Kayadelen; Jesse Kettnerov\u00e1; Natalia Kirchner; Simon Kotsyba;  Krek ; Yuji; Ryan Matsumoto; Gustavo Mcdonald; Niko Mendon\u00e7a; Anna Miekka; C\u0203t\u0203lin Missil\u00e4; Yusuke Mititelu; Simonetta Miyao; Amir Montemagni; Laura Moreno More; Shinsuke Romero; Bjartur Mori; Bohdan Mortensen; Kadri Moskalevskyi;  Muischnek"}, {"ref_id": "b26", "title": "Sumire Uematsu, Zde\u0148ka Ure\u0161ov\u00e1, Larraitz Uria, Hans Uszkoreit, Sowmya Vajjala, Daniel van Niekerk, Gertjan van Noord, Viktor Varga, Veronika Vincze", "journal": "", "year": "2018", "authors": "Hanna Nurmi; Ad\u00e9day\u00f2 Stina Ojala; Mai Ol\u00fa\u00f2kun; Petya Omura; Robert Osenova; Lilja \u00d6stling; Niko \u00d8vrelid; Elena Partanen; Marco Pascual; Agnieszka Passarotti; Siyao Patejuk; Cenel-Augusto Peng; Guy Perez; Slav Perrier; Jussi Petrov; Emily Piitulainen; Barbara Pitler; Thierry Plank; Martin Poibeau; Lauma Popel; Sophie Pretkalnin; Tiina Pr\u00e9vost ; Adam Przepi\u00f3rkowski; Sampo Puolakainen; Andriela Pyysalo; Georg R\u00e4\u00e4bis ; Siva Reddy; Michael Rehm; Larissa Rie\u00dfler; Laura Rinaldi; Luisa Rituma; Mykhailo Rocha; Rudolf Romanenko; Davide Rosa; Valentin Rovati; Olga Ros; Shoval Rudina; Shadi Sadde; Tanja Saleh; Stephanie Samard\u017ei\u0107; Manuela Samson; Baiba Sanguinetti; Yanin Saul\u012bte; Nathan Sawanakunanon; Sebastian Schneider; Djam\u00e9 Schuster; Wolfgang Seddah; Mojgan Seeker; Mo Seraji; Atsuko Shen; Muh Shimada; Dmitry Shohibussirri; Chunxiao Sichinava ; Sum Wong;  Yan; M Marat; Zhuoran Yavrumyan; Zden\u011bk Yu; Amir \u017dabokrtsk\u00fd; Daniel Zeldes; Manying Zeman; Hanzhi Zhang;  Zhu"}, {"ref_id": "b27", "title": "Pseudoprojective dependency parsing", "journal": "", "year": "2005", "authors": "Joakim Nivre; Jens Nilsson"}, {"ref_id": "b28", "title": "Viable dependency parsing as sequence labeling", "journal": "", "year": "2019", "authors": "Michalina Strzyz; David Vilares; Carlos G\u00f3mez-Rodr\u00edguez"}, {"ref_id": "b29", "title": "Parsing as tagging", "journal": "European Language Resources Association", "year": "2020", "authors": "Robert Vacareanu; George Caique Gouveia Barbosa; Marco A Valenzuela-Esc\u00e1rcega; Mihai Surdeanu"}, {"ref_id": "b30", "title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"ref_id": "b31", "title": "Second-order neural dependency parsing with message passing and end-to-end training", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Xinyu Wang; Kewei Tu"}, {"ref_id": "b32", "title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "journal": "Natural Language Engineering", "year": "2005", "authors": "Naiwen Xue; Fei Xia; Fu-Dong Chiou; Marta Palmer"}, {"ref_id": "b33", "title": "Statistical dependency analysis with support vector machines", "journal": "", "year": "2003", "authors": "Hiroyasu Yamada; Yuji Matsumoto"}, {"ref_id": "b34", "title": "Headed-span-based projective dependency parsing", "journal": "Long Papers", "year": "2022", "authors": "Songlin Yang; Kewei Tu"}, {"ref_id": "b35", "title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; R Russ; Quoc V Salakhutdinov;  Le"}, {"ref_id": "b36", "title": "Efficient second-order TreeCRF for neural dependency parsing", "journal": "", "year": "2020", "authors": "Yu Zhang; Zhenghua Li; Min Zhang"}, {"ref_id": "b37", "title": "A tale of two parsers: Investigating and combining graph-based and transition-based dependency parsing", "journal": "Association for Computational Linguistics", "year": "2008", "authors": "Yue Zhang; Stephen Clark"}, {"ref_id": "b38", "title": "Head-Driven Phrase Structure Grammar parsing on Penn Treebank", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Junru Zhou; Hai Zhao"}, {"ref_id": "b39", "title": "Please mind the root: Decoding arborescences for dependency parsing", "journal": "", "year": "2020", "authors": "Ran Zmigrod; Tim Vieira; Ryan Cotterell"}, {"ref_id": "b40", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "App. E", "year": "", "authors": ""}, {"ref_id": "b41", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b42", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b43", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b44", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b45", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b46", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b47", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b48", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure1: From bottom to top, the figure shows the dependency tree, the hexatags, and the binary head tree for the sentence \"She reads fascinating papers.\"", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: The example shows how to derive the BHT for a dependency tree A B C D . The top of the stack is on the right.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "); \u2022 \u2192 : pop the subtree on the top of the stack. Replace the dummy node in the subtree with the terminal node. Push the subtree back to the stack; \u2022 \u21d2 R ( \u21d2 L ): create a new node labeled R (respectively, L ). Pop the top element of the stack, attach it as the new node's left child, and set a dummy node as the node's right child. Pop another subtree of the stack, identify the dummy node in the subtree and replace it with the newly created subtree. Push the subtree back to the stack (step 6 in Fig. 2); hexatags \u2192 BHT BHT \u2192 Dep. Tree Action BHT Stack BHT Dep. Stack", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: The example shows how to derive BHT from hexatags and how to transform BHT to a dependency tree. The top of the stacks is on the right.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "you describe the limitations of your work? Limitations A2. Did you discuss any potential risks of your work? Ethics Statement A3. Do the abstract and introduction summarize the paper's main claims? Abstract and Sec. 1 A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? 5, App. D B1. Did you cite the creators of artifacts you used? 5 B2. Did you discuss the license or terms for use and / or distribution of any artifacts? App D", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "LAS scores on the test sets of 12 languages in UD 2.2. Hexatagger achieves competitive performance in all languages and is state-of-the-art in 4 languages.", "figure_data": "PTBCTBModelUAS LAS UAS LASZhou and Zhao (2019)  *  Mrini et al. (2020)  *97.0 95.4 91.2 89.2 97.4 96.3 94.6 89.3Chen and Manning (2014) 91.8 89.6 83.9 82.4Dozat and Manning (2017) 95.7 94.1 89.3 88.2Yang and Tu (2022) #97.4 95.8 93.5 92.5Hexatagger97.4 96.4 93.2 91.9"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Comparison of parsing speed and memory consumption on PTB test set. Results are averaged over 3 random runs on the same server with one Nvidia A100-80GB GPU using BERT-large as encoder. We use a batch size of 128 sentences, except for \u22c6 that uses 64, which otherwise results in an out-of-memory error."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ". In our experiments, the choice of left-firstor right-first order has little to no effect on parsingperformance."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Comparison of left-first and right-first binarization.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2192 $ A R B L $ A R C D A (2) \u21d2 R R \u2205 $ A R B L $ A R C D A B (3) \u2192 R B $ A R B L $ A R C D A B (4) \u21d2 L R B L $ \u2205 A R B L $ A R C D A B C (5) \u2192 R B L $ \u2205 A $ C R B L $ A R C D A B C D (6) \u21d2 R R B L $ A R C \u2205 R B L $ A R C D A B C D (7) \u2192 R B L $ A R C D R B L $ A R C D A B C D", "formula_coordinates": [4.0, 70.86, 152.8, 209.95, 290.15]}, {"formula_id": "formula_1", "formula_text": "L(\u03b8) = \u2212 (w,t)\u2208S log p \u03b8 (t | w) (1a) = \u2212 (w,t)\u2208S log 2N \u22121 n=1 p \u03b8 (t n | w) (1b) = \u2212 (w,t)\u2208S N n=1 log p \u03b8 (t 2n\u22121 | w) (1c) + N \u22121 n=1 log p \u03b8 (t 2n | w)", "formula_coordinates": [4.0, 314.75, 165.27, 210.41, 148.15]}], "doi": "10.18653/v1/2021.emnlp-main.468"}