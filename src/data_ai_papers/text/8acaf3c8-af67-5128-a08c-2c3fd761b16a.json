{"title": "Efficient, correct, unsupervised learning of context-sensitive languages", "authors": "Alexander Clark", "pub_date": "", "abstract": "A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for syntax. In this paper we present a lattice-theoretic representation for natural language syntax, called Distributional Lattice Grammars. These representations are objective or empiricist, based on a generalisation of distributional learning, and are capable of representing all regular languages, some but not all context-free languages and some noncontext-free languages. We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm.", "sections": [{"heading": "Introduction", "text": "Grammar induction, or unsupervised learning of syntax, no longer requires extensive justification and motivation. Both from engineering and cognitive/linguistic angles, it is a central challenge for computational linguistics. However good algorithms for this task are thin on the ground. There are numerous heuristic algorithms, some of which have had significant success in inducing constituent structure (Klein and Manning, 2004). There are algorithms with theoretical guarantees as to their correctness -such as for example Bayesian algorithms for inducing PCFGs (Johnson, 2008), but such algorithms are inefficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers. The efficient algorithms that are actually used are heuristic approximations to the true posteriors. There are algorithms like the Inside-Outside algorithm (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum. There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size (Horning, 1969). There are no correct and efficient algorithms, as there are for parsing, for example.\nThere is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy. Abe and Warmuth (1992) showed that training stochastic regular grammars is hard; Angluin and Kharitonov (1995) showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and so on. However, these results do not necessarily apply to other representations. Regular grammars are not learnable, but deterministic finite automata are learnable under various paradigms (Angluin, 1987). Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are \"objective\"; there is a correspondence between the structure of the language, (the residual languages) and the representational primitives of the formalism (the states) which is expressed by the Myhill-Nerode theorem.\nIn this paper we study the learnability of a class of representations that we call distributional lattice grammars (DLGs). Lattice-based formalisms were introduced by Clark et al. (2008) and Clark (2009) as context sensitive formalisms that are potentially learnable. Clark et al. (2008) established a similar learnability result for a limited class of context free languages. In Clark (2009), the approach was extended to a significantly larger class but without an explicit learning algorithm. Most of the building blocks are however in place, though we need to make several modifications and ex-tensions to get a clean result. Most importantly, we need to replace the representation used there, which naively could be exponential, with a lazy, exemplar based model.\nIn this paper we present a simple algorithm for the inference of these representations and prove its correctness under the following learning paradigm: we assume that as normal there is a supply of positive examples, and additionally that the learner can query whether a string is in the language or not (an oracle for membership queries). We also prove that the algorithm is efficient in the sense that it will use a polynomial amount of computation and makes a polynomial number of queries at each step.\nThe contributions of this paper are as follows: after some basic discussion of distributional learning in Section 2, we define in Section 3 an exemplar-based grammatical formalism which we call Distributional Lattice Grammars. We then give a learning algorithm under a reasonable learning paradigm, together with a self contained proof in elementary terms (not presupposing any extensive knowledge of lattice theory), of the correctness of this algorithm.", "publication_ref": ["b17", "b15", "b18", "b14", "b0", "b1", "b2", "b7", "b8", "b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Basic definitions", "text": "We now define our notation; we have a finite alphabet \u03a3; let \u03a3 * be the set of all strings (the free monoid) over \u03a3, with \u03bb the empty string. A (formal) language is a subset of \u03a3 * . We can concatenate two languages A and B to get AB = {uv|u \u2208 A, b \u2208 B}.\nA context or environment, as it is called in structuralist linguistics, is just an ordered pair of strings that we write (l, r) where l and r refer to left and right; l and r can be of any length. We can combine a context (l, r) with a string u with a wrapping operation that we write : so (l, r) u is defined to be lur. We will sometimes write f for a context (l, r). There is a special context (\u03bb, \u03bb): (\u03bb, \u03bb) w = w. We will extend this to sets of contexts and sets of strings in the natural way. We will write Sub(w) = {u|\u2203(l, r) : lur = w} for the set of substrings of a string, and Con(w) = {(l, r)|\u2203u \u2208 \u03a3 * : lur = w}.\nFor a given string w we can define the distribution of that string to be the set of all contexts that it can appear in:\nC L (w) = {(l, r)|lwr \u2208 L}, equiv- alently {f |f w \u2208 L}. Clearly (\u03bb, \u03bb) \u2208 C L (w) iff w \u2208 L.\nDistributional learning (Harris, 1954) as a technical term refers to learning techniques which model directly or indirectly properties of the distribution of strings or words in a corpus or a language. There are a number of reasons to take distributional learning seriously: first, historically, CFGs and other PSG formalisms were intended to be learnable by distributional means. Chomsky (2006) says (p. 172, footnote 15):\nThe concept of \"phrase structure grammar\" was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus.\nSecond, empirically we know they work well at least for lexical induction, (Sch\u00fctze, 1993;Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996). These arguments together suggest that distributional learning has a somewhat privileged status.\n3 Lattice grammars Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices. Here we will largely ignore this, and start from a straightforward computational treatment. We start by defining the representation.\nDefinition 1. Given a non-empty finite alphabet, \u03a3, a distributional lattice grammar (DLG) is a 3tuple consisting of K, D, F , where F is a finite subset of \u03a3 * \u00d7 \u03a3 * , such that (\u03bb, \u03bb) \u2208 F , K is a finite subset of \u03a3 * which contains \u03bb and \u03a3, and D is a subset of (F KK). K here can be thought of as a finite set of exemplars, which correspond to substrings or fragments of the language. F is a set of contexts or features, that we will use to define the distributional properties of these exemplars; finally D is a set of grammatical strings, the data; a finite subset of the language. F KK using the notation above is {luvr|u, v \u2208 K, (l, r) \u2208 F }. This is the finite part of the language that we examine. If the language we are modeling is L, then D = L \u2229 (F KK). Since \u03bb \u2208 K, K \u2286 KK.\nWe define a concept to be an ordered pair S, C where S \u2286 K and C \u2286 F , which satisfies the following two conditions: first C S \u2286 D; that is to say every string in S can be combined with any context in C to give a grammatical string, and secondly they are maximal in that neither K nor F can be increased without violating the first condition.\nWe define B(K, D, F ) to be the set of all such concepts. We use the B symbol (Begriff ) to bring out the links to Formal Concept Analysis (Ganter and Wille, 1997;Davey and Priestley, 2002). This lattice may contain exponentially many concepts, but it is clearly finite, as the number of concepts is less than min(2\n|F | , 2 |K| ).\nThere is an obvious partial order defined by\nS 1 , C 1 \u2264 S 2 , C 2 iff S 1 \u2286 S 2 , Note that S 1 \u2286 S 2 iff C 2 \u2286 C 1 .\nGiven a set of strings S we can define a set of contexts S to be the set of contexts that appear with every element of S. S = {(l, r) \u2208 F : \u2200w \u2208 S, lwr \u2208 D} Dually we can define for a set of contexts C the set of strings C that occur with all of the elements of C:\nC = {w \u2208 K : \u2200(l, r) \u2208 C, lwr \u2208 D}\nThe concepts S, C are just the pairs that satisfy S = C and C = S; the two maps denoted by are called the polar maps. For any S \u2286 K, S = S and for any C \u2286 F , C = C . Thus we can form a concept from any set of strings S \u2286 K by taking S , S ; this is a concept as S = S . We will write this as C(S), and for any C \u2286 F , we will write C(C) = C , C .\nIf S \u2286 T then T \u2286 S , and S \u2286 T . For any set of strings S \u2286 K, S \u2286 S .\nOne crucial concept here is the concept defined by (\u03bb, \u03bb) or equivalently by the set K \u2229 D which corresponds to all of the elements in the language. We will denote this concept by\nL = C({(\u03bb, \u03bb)}) = C(K \u2229 D).\nWe also define a meet operation by\nS 1 , C 1 \u2227 S 2 , C 2 = S 1 \u2229 S 2 , (S 1 \u2229 S 2 )\nThis is the greatest lower bound of the two concepts; this is a concept since if S 1 = S 1 and\nS 2 = S 2 then (S 1 \u2229 S 2 ) = (S 1 \u2229 S 2 ).\nNote that this operation is associative and commutative. We can also define a join operation dually; with these operations B(K, D, D) is a complete lattice. So far we have only used strings in F K; we now define a concatenation operation as follows.\nS 1 , C 1 \u2022 S 2 , C 2 = (S 1 S 2 ) , (S 1 S 2 )\nSince S 1 and S 2 are subsets of K, S 1 S 2 is a subset of KK, but not necessarily of K. (S 1 S 2 ) is the set of contexts shared by all elements of S 1 S 2 and (S 1 S 2 ) is the subset of K, not KK, that has all of the contexts of (S 1 S 2 ) . (S 1 S 2 ) might be larger than (S 1 S 2 ) . We can also write this as C((S 1 S 2 ) ).\nBoth \u2227 and \u2022 are monotonic in the sense that if\nX \u2264 Y then X \u2022 Z \u2264 Y \u2022 Z, Z \u2022 X \u2264 Z \u2022 Y and X \u2227 Z \u2264 Y \u2227 Z.\nNote that all of these operations can be computed efficiently; using a perfect hash, and a naive algorithm, we can do the polar maps and \u2227 operations in time O(|K||F |), and the concatenation in time O(|K| 2 |F |).\nWe now define the notion of derivation in this representation. Given a string w we recursively compute a concept for every substring of w; this concept will approximate the distribution of the string. We define \u03c6 G as a function from \u03a3 * to B(K, D, F ); we define it recursively:\n\u2022 If |w| \u2264 1, then \u03c6 G (w) = {w} , {w} \u2022 If |w| > 1 then \u03c6 G (w) = u,v\u2208\u03a3 + :uv=w \u03c6 G (u) \u2022 \u03c6 G (v)\nThe first step is well defined because all of the strings of length at most 1 are already in K so we can look them up directly. To clarify the sec-\nond step, if w = abc then \u03c6 G (abc) = \u03c6 G (a) \u2022 \u03c6 G (bc) \u2227 \u03c6 G (ab) \u2022 \u03c6 G (c)\n; we compute the string from all possible non-trivial splits of the string into a prefix and a suffix. By using a dynamic programming table that stores the values of \u03c6(u) for all u \u2208 Sub(w) we can compute this in time O(|K| 2 |F ||w| 3 ); this is just an elementary variant of the CKY algorithm. We define the language defined by the DLG G to be\nL(G) = {w|\u03c6 G (w) \u2264 C({(\u03bb, \u03bb)})}\nThat is to say, a string is in the language if we predict that a string has the context (\u03bb, \u03bb). We now consider a trivial example: the Dyck language.\nExample 1. Let L be the Dyck language (matched parenthesis language) over \u03a3 = {a, b}, where a corresponds to open bracket, and b to close bracket. Define\n\u2022 K = {\u03bb, a, b, ab} \u2022 F = {(\u03bb, \u03bb), (\u03bb, b), (a, \u03bb)}.\n\u2022 D = {\u03bb, ab, abab, aabb} G = K, D, F is a DLG. We will now write down the 5 elements of the lattice:\n\u2022 = K, \u2205 \u2022 \u22a5 = \u2205, F \u2022 L = {\u03bb, ab}, {(\u03bb, \u03bb)} \u2022 A = {a}, {(\u03bb, b)} \u2022 B = {b}, {(a, \u03bb)}\nTo compute the concatenation A \u2022 B we first compute {a}{b} = {ab}; we then compute {ab} which is {(\u03bb, \u03bb)}, and {(\u03bb, \u03bb)} = {\u03bb, ab}, so A \u2022 B = L. Similarly to compute L \u2022 L, we first take {\u03bb, ab}{\u03bb, ab} = {\u03bb, ab, abab}. These all have the context (\u03bb, \u03bb), so the result is the concept L. If we compute A \u2022 A we get {a}{a} which is {aa} which has no contexts so the result is .\nWe have \u03c6 G (\u03bb) = L, \u03c6 G (a) = A, \u03c6 G (b) = B.\nApplying the recursive computation we can verify that \u03c6 G (w) = L iff w \u2208 L and so L(G) = L. We can also see that D = L \u2229 (F KK).", "publication_ref": ["b13", "b5", "b21", "b9", "b16", "b4", "b20", "b8", "b11", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Search", "text": "In order to learn these grammars we need to find a suitable set of contexts F , a suitable set of strings K, and then work out which elements of F KK are grammatical. So given a choice for K and F it is easy to learn these models under a suitable regime: the details of how we collect information about D depend on the learning model.\nThe question is therefore whether it is easy to find suitable sets, K and F . Because of the way the formalism is designed, it transpires that the search problem is entirely tractable. In order to analyse the search space, we define two maps between the lattices as K and F are increased. We are going to augment our notation slightly; we will write B(K, L, F ) for B(K, L \u2229 (F KK), F ) and similarly K, L, F for K, L\u2229 (F KK), F . When we use the two polar maps (such as C , S ), though we are dealing with more than one lattice, there is no ambiguity as the maps agree; we will when necessary explicitly restrict the output (e.g. C \u2229 J) to avoid confusion. Definition 2. For any language L and any set of contexts F \u2286 G, and any sets of strings J \u2286 K \u2286 \u03a3 * . We define a map g from B(J, L, F ) to B(K, L, F ) (from the smaller lattice to the larger lattice) as g( S, C ) = C , C .\nWe also define a map f from B(K, L, G) to B(K, L, F ), (from larger to smaller) as\nf ( S, C ) = (C \u2229 F ) , C \u2229 F .\nThese two maps are defined in opposite directions: this is because of the duality of the lattice. By defining them in this way, as we will see, we can prove that these two maps have very similar properties. We can verify that the outputs of these maps are in fact concepts.\nWe now need to define two monotonicity lemmas: these lemmas are crucial to the success of the formalism. We show that as we increase K the language defined by the formalism decreases monotonically, and that as we increase F the language increases monotonically. There is some duplication in the proofs of the two lemmas; we could prove them both from more abstract properties of the maps f, g which are what are called residual maps, but we will do it directly.\nLemma 1. Given two lattices B(K, L, F ) and B(K, L, G) where F \u2286 G; For all X, Y \u2208 B(K, L, G) we have that\n1. f (X) \u2022 f (Y ) \u2265 f (X \u2022 Y ) 2. f (X) \u2227 f (Y ) \u2265 f (X \u2227 Y )\nProof. The proof is elementary but difficult to read. We write X = S X , C X and similarly for Y . For part 1 of the lemma: Clearly (S X \u2229 F ) \u2286 S X , so (S X \u2229 F ) \u2287 S X = S X and the same for\nS Y . So (S X \u2229 F ) (S Y \u2229 F ) \u2287 S X S Y (as subsets of KK). So ((S X \u2229F ) (S Y \u2229F ) ) \u2286 (S X S Y ) \u2286 (S X S Y ) . Now by definition, f (X) \u2022 f (Y ) is C(Z) where Z = ((S X \u2229 F ) (S Y \u2229 F ) ) \u2229 F and f (X \u2022 Y ) has the set of contexts ((S X S Y ) \u2229 F ). Therefore f (X \u2022 Y ) has a bigger set of contexts than f (X) \u2022 f (Y ) and is thus a smaller concept. For part 2: by definition f (X \u2227 Y ) = ((S X \u2229 S y ) \u2229 F ) , (S X \u2229 S y ) \u2229 F and f (X) \u2227 f (Y ) = (S X \u2229F ) \u2229(S y \u2229F ) , ((S X \u2229F ) \u2229(S y \u2229F ) ) \u2229F Now S X \u2229 F \u2286 S X , so (since S X = S X ) S X \u2286 (S X \u2229F ) , and so S X \u2229S y \u2286 (S X \u2229F ) \u2229(S y \u2229F ) . So (S X \u2229 S y ) \u2287 ((S X \u2229 F ) \u2229 (S y \u2229 F ) ) which\ngives the result by comparing the context sets of the two sides of the inequality.\nLemma 2. For any language L, and two sets of contexts F \u2286 G, and any K, if we have two DLGs K, L, F with map \u03c6 F : \u03a3 * \u2192 B(K, L, F ) and K, L, G with map \u03c6 G : \u03a3 * \u2192 B(K, L, G) then for all w, f (\u03c6 G (w)) \u2264 \u03c6 F (w).\nProof. By induction on the length of w; clearly if |w| \u2264 1, f (\u03c6 G (w)) = \u03c6 F (w). We now take the inductive step; by definition, (suppressing the definition of u, v in the meet)\nf (\u03c6 G (w)) = f ( u,v \u03c6 G (u) \u2022 \u03c6 G (v)) By Lemma 1, part 2: f (\u03c6 G (w)) \u2264 u,v f (\u03c6 G (u) \u2022 \u03c6 G (v))\nBy Lemma 1, part 1:\nf (\u03c6 G (w)) \u2264 u,v f (\u03c6 G (u)) \u2022 f (\u03c6 G (v))\nBy the inductive hypothesis we have f (\u03c6 G (u)) \u2264 \u03c6 F (u) and similarly for v and so by the monotonicity of \u2227 and \u2022:\nf (\u03c6 G (w)) \u2264 u,v \u03c6 F (u) \u2022 \u03c6 F (v)\nSince the right hand side is equal to \u03c6 F (w), the proof is done.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "It is then immediate that", "text": "Lemma 3. If F \u2286 G then L( K, L, F ) \u2286 L( K, L, G ),\nProof. If w \u2208 L( K, L, F ), then \u03c6 F (w) \u2264 L, and so f (\u03c6 G (w)) \u2264 L and so \u03c6 G (w) has the context (\u03bb, \u03bb) and is thus in L( K, L, G ).\nWe now prove the corresponding facts about g. Lemma 4. For any J \u2286 K and any concepts X, Y in B(J, L, F ), we have that\n1. g(X) \u2022 g(Y ) \u2265 g(X \u2022 Y ) 2. g(X) \u2227 g(Y ) \u2265 g(X \u2227 Y )\nProof. For the first part: Write X = S X , C X as before. Note that S\nX = C X \u2229 J . S X \u2286 C X , so S X S Y \u2286 C X C Y , and so (S X S Y ) \u2286 (C X C Y ) , and ((S X S Y ) \u2229 J) \u2287 (C X C Y ) . By calcu- lation g(X) \u2022 g(Y ) = (C X C Y ) , (C X C Y )\nOn the other hand, g(X \u2022 Y ) = ((S X S Y ) \u2229 J) , ((S X S Y ) \u2229 J) and so g(X \u2022 Y ) is smaller as it has a larger set of contexts.\nFor the second part:\ng(X) \u2227 g(Y ) = C X \u2229 C Y , (C X \u2229 C Y ) and g(X \u2227 Y ) = (S X \u2229 S Y ) , (S X \u2229 S Y ) . Since S X = C X \u2229 J, S X \u2286 C X , so (S X \u2229 S Y ) \u2286 C X \u2229 C Y , and therefore (S X \u2229 S Y ) \u2286 (C X \u2229 C Y ) = C X \u2229 C Y .\nWe now state and prove the monotonicity lemma for g. Lemma 5. For all J \u2286 K \u2286 \u03a3 * \u00d7 \u03a3 * , and for all strings w; we have that g(\u03c6 J (w)) \u2264 \u03c6 K (w).\nProof. By induction on length of w. Both J and K include the basic elements of \u03a3 and \u03bb.\nFirst suppose |w| \u2264 1, then \u03c6 J (w) = (C L (w) \u2229 F ) \u2229 J, C L (w) \u2229 F , and g(\u03c6 J (w)) = (C L (w) \u2229 F ) , C L (w) \u2229 F which is equal to \u03c6 K (w).\nNow suppose true for all w of length at most k, and take some w of length k + 1. By definition of \u03c6 J :\ng(\u03c6 J (w)) = g u,v \u03c6 J (u) \u2022 \u03c6 J (v) Next by Lemma 4, Part 2 g(\u03c6 J (w)) \u2264 u,v g(\u03c6 J (u) \u2022 \u03c6 J (v)) By Lemma 4, Part 1 g(\u03c6 J (w)) \u2264 u,v g(\u03c6 J (u)) \u2022 g(\u03c6 J (v))\nBy the inductive hypothesis and monotonicity of \u2022 and \u2227:\ng(\u03c6 J (w)) \u2264 u,v \u03c6 K (u) \u2022 \u03c6 K (v) = \u03c6 K (w) Lemma 6. If J \u2286 K then L( J, L, F ) \u2287 L( K, L, F )\nProof. Suppose w \u2208 L( K, L, F ). this means that \u03c6 K (w) \u2264 L K . therefore g(\u03c6 J (w)) \u2264 L k ; which means that (\u03bb, \u03bb) is in the concept g(\u03c6 J (w)), which means it is in the concept \u03c6 J (w), and therefore w \u2208 L( J, L, F ).\nGiven these two lemmas we can make the following observations. First, if we have a fixed L and F , then as we increase K, the language will decrease until it reaches a limit, which it will attain after a finite limit.\nLemma 7. For all L, and finite context sets F , there is a finite K such that for all K 2 , K \u2282 K 2 , L( K, L, F ) = L( K 2 , L, F ).\nProof. We can define the lattice B(\u03a3 * , L, F ). Define the following equivalence relation between pairs of strings, where\n(u 1 , v 1 ) \u223c (u 2 , v 2 ) iff C(u 1 ) = C(u 2 ) and C(v 1 ) = C(v 2 ) and C(u 1 v 1 ) = C(u 2 v 2 ).\nThe number of equivalence classes is clearly finite. If K is sufficiently large that there is a pair of strings (u, v) in K for each equivalence class, then clearly the lattice defined by this K will be isomorphic to B(\u03a3 * , L, F ). Any superset of K will not change this lattice. Moreover this language is unique for each L, F . We will call this the limit language of L, F , and we will write it as L( \u03a3 * , L, F ).\nIf\nF \u2286 G, then L( \u03a3 * , L, F ) \u2286 L( \u03a3 * , L, G ).\nFinally, we will show that the limit languages never overgeneralise.\nLemma 8. For any L, and for any F , L( \u03a3 * , L, F ) \u2286 L.\nProof. Recall that C(w) = {w} , {w} is the real concept. If G is a limit grammar, we can show that we always have \u03c6 G (w) > C(w), which will give us the result immediately. First note that C(u) \u2022 C(v) \u2265 C(uv), which is immediate by the definition of \u2022. We proceed, again, by induction on the length of w. For |w| \u2264 1, \u03c6 G (w) = C(w). For the inductive step we have\n\u03c6 G (w) = u,v \u03c6 G (u) \u2022 \u03c6 G (v)\n; by inductive hypothesis we have that this must be more than\nu,v C(u) \u2022 C(v) > u,v C(uv) = C(w)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Weak generative power", "text": "First we make the following observation: if we consider an infinite variant of this, where we set K = \u03a3 * and F = \u03a3 * \u00d7 \u03a3 * and D = L, we can prove easily that, allowing infinite \"representations\", for any L, L( K, D, F ) = L. In this infinite data limit, \u2022 becomes associative, and the structure of B(K, D, F ) becomes a residuated lattice, called the syntactic concept lattice of the language L, B(L). This lattice is finite iff the language is regular. The fact that this lattice now has residuation operations suggest interesting links to the theory of categorial grammar. It is the finite case that interests us.\nWe will use L DLG to refer to the class of languages that are limit languages in the sense defined above.\nL DLG = {L|\u2203F, L( \u03a3 * , L, F ) = L}\nOur focus in this paper is not on the language theory: we present the following propositions. First L DLG properly contains the class of regular languages. Secondly L DLG contains some noncontext-free languages (Clark, 2009). Thirdly it does not contain all context-free languages.\nA natural question to ask is how to convert a CFG into a DLG. This is in our view the wrong question, as we are not interested in modeling CFGs but modeling natural languages, but given the status of CFGs as a default model for syntactic structure, it will help to give a few examples, and a general mechanism. Consider a nonterminal N in a CFG with start symbol S. We can define C(N ) = {(l, r)|S * \u21d2 lN r} and the yield Y (N ) = {w|N * \u21d2 w}. Clearly C(N ) Y (N ) \u2286 L, but these are not necessarily maximal, and thus C(N ), Y (N ) is not necessarily a concept. Nonetheless in most cases, we can construct a grammar where the non-terminals will correspond to concepts, in this way.\nThe basic approach is this: for each nonterminal, we identify a finite set of contexts that will pick out only the set of strings generated from that non-terminal: we find some set of contexts F N typically a subset of C(N ) such that Y (N ) = {w|\u2200(l, r) \u2208 F N , lwr \u2208 L}. We say that we can contextually define this non-terminal if there is such a finite set of contexts F N . If a CFG in Chomsky normal form is such that every non-terminal can be contextually defined then the language defined by that grammar is in L DLG . If we can do that, then the rest is trivial. We take any set of features F that includes all of these F N ; probably just F = N F N ; we then pick a set of strings K that is sufficiently large to rule out all incorrect generalisations, and then define D to be L \u2229 (F KK).\nConsider the language\nL = {a n b n c m |n, m \u2265 0} \u222a {a m b n c n |n, m \u2265 0}.\nL is a classic example of an inherently ambiguous and thus nondeterministic language.\nThe natural CFG in CNF for L has non-terminals that generate the following sets:\n{a n b n |n \u2265 0}, {a n+1 b n |n \u2265 0}, {b n c n |n \u2265 0}, {b n c n+1 |n \u2265 0}, {a * } and {c * }.\nWe note that the six contexts (aa, bbc), (aa, bbbc), (abb, cc)(abbb, cc), (\u03bb, a) and (c, \u03bb) will define exactly these sets, in the sense that the set of strings that occur in each context will be exactly the corresponding set.\nWe can also pick out \u03bb, a, b, c with individual contexts. Let F = {(\u03bb, \u03bb), (aaabb, bccc), (aaabbc, \u03bb), (\u03bb, abbccc), (aaab, bccc), (aa, bbc), (aa, bbbc), (abb, cc), (abbb, cc), (\u03bb, a), (c, \u03bb)}. If we take a sufficiently large set K, say \u03bb, a, b, c, ab, aab, bc, bcc, abc, and set D = L \u2229 F KK, then we will have a DLG for the language L. In this example, it is sufficient to have one context per non-terminal. This is not in general the case.\nConsider L = {a n b n |n \u2265 0} \u222a {a n b 2n |n \u2265 0}.\nHere we clearly need to identify sets of strings corresponding to the two parts of this language, but it is easy to see that no one context will suffice. However, note that the first part is defined by the two contexts (\u03bb, \u03bb), (a, b) and the second by the two contexts (\u03bb, \u03bb), (a, bb). Thus it is sufficient to have a set F that includes these four contexts, as well as similar pairs for the other non-terminals in the grammar, and some contexts to define a and b.\nWe can see that we will not always be able to do this for every CFG. One fixable problem is if the CFG has two separate non-terminals, M, N such that C(M ) \u2287 C(N ). If this is the case, then we must have that Y (N ) \u2287 Y (M ), If we pick a set of contexts to define Y (N ), then clearly any string in Y (M ) will also be picked out by the same contexts. If this is not the case, then we can clearly try to rectify it by adding a rule N \u2192 M which will not change the language defined. However, we cannot always pick out the nonterminals with a finite set of contexts. Consider the language L = {a n b|n > 0} \u222a {a n c m |m > n > 0} defined in Clark et al. (2008). Suppose wlog that F contains no context (l, r) such that |l| + |r| \u2265 k. Then it is clear that we will not be able to pick out b without also picking out\nc k+1 , since C L (c k+1 ) \u2229 F \u2287 C L (b) \u2229 F . Thus L, which is clearly context-free, is not in L DLG .\nLuckily, this example is highly artificial and does not correspond to any phenomena we are aware of in linguistics.\nIn terms of representing natural languages, we clearly will in many cases need more than one context to pick out syntactically relevant groups of strings. Using a very simplified example from English, if we want to identify say singular noun phrases, a context like (that is, \u03bb) will not be sufficient since as well as noun phrases we will also have some adjective phrases. However if we include multiple contexts such as (\u03bb, is over there) and so on, eventually we will be able to pick out exactly the relevant set of strings. One of the reasons we need to use a context sensitive representation, is so that we can consider every possible combination of contexts simultaneously: this would require an exponentially large context free grammar.", "publication_ref": ["b8", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Model", "text": "In order to prove correctness of the learning algorithm we will use a variant of Gold-style inductive inference (Gold, 1967). Our choice of this rather old-fashioned model requires justification. There are two problems with learning -the information theoretic problems studied under VC-dimension etc., and the computational complexity issues of constructing a hypothesis from the data. In our view, the latter problems are the key ones. Accordingly, we focus entirely on the efficiency issue, and allow ourself a slightly unrealistic model; see (Clark and Lappin, 2009) for arguments that this is a plausible model.\nWe assume that we have a sequence of positive examples, and that we can query examples for membership. Given a language L a presentation for L is an infinite sequence of strings w 1 , w 2 , . . . such that {w i |i \u2208 N } = L. An algorithm receives a sequence T and an oracle, and must produce a hypothesis H at every step, using only a polynomial number of queries to the membership oracle -polynomial in the total size of the presentation. It identifies in the limit the language L iff for every presentation T of L there is a N such that for all n > N H n = H N , and L(H N ) = L. We say it identifies in the limit a class of languages L iff it identifies in the limit all L in L. We say that it identifies the class in polynomial update time iff there is a polynomial p, such that at each step the model uses an amount of computation (and thus also a number of queries) that is less than p(n, l), where n is the number of strings and l is the maximum length of a string in the observed data. We note that this is slightly too weak. It is possible to produce vacuous enumerative algorithms that can learn anything by only processing a logarithmically small prefix of the string (Pitt, 1989).", "publication_ref": ["b12", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Algorithm", "text": "We now define a simple learning algorithm, that establishes learnability under this paradigm.\nThere is one minor technical detail we need to deal with. We need to be able to tell when adding a string to a lazy DLG will leave the grammar unchanged. We use a slightly weaker test. Given G 1 = K, D, F we define as before the equivalence relation between pairs of strings of K, where\n(u 1 , v 1 ) \u223c G 1 (u 2 , v 2 ) iff C D (u 1 ) = C D (u 2 ) and C D (v 1 ) = C D (v 2 ) and C D (u 1 v 1 ) = C D (u 2 v 2 ). Note that C D (u) = {(l, r)|lur \u2208 D}.\nGiven two grammars G 1 = K, D, F and G 2 = K 2 , D 2 , F where K \u2286 K 2 and D \u2286 D 2 but F is unchanged, we say that these two are indistinguishable iff the number of equivalence classes of K \u00d7 K under \u223c G 1 is equal to the number of equivalence classes of K 2 \u00d7 K 2 under \u223c G 2 . This can clearly be computed efficiently using a union-find algorithm, in time polynomial in |K| and |F |. If they are indistinguishable then they define the same language.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm", "text": "Algorithm 1 presents the basic algorithm. At various points we compute sets of strings like (F KK) \u2229 L; these can be computed using the membership oracle.\nFirst we prove that the program is efficient in the sense that it runs in polynomial update time.\nLemma 9. There is a polynomial p, such that Algorithm 1, for each w n , runs in time bounded by p(n, l) where l is the maximum length of a string in w 1 , . . . w n .\nProof. First we note that K, K 2 and F are always subsets of Sub(E)\u222a\u03a3 and Con(E), and thus both |K| and |F | are bounded by nl(l + 1)/2 + |\u03a3| + 1. Computing D is efficient as |F KK| is bounded by |K| 2 |F |. We can compute \u03c6 G as mentioned above in time |K| 2 |F |l 3 ; distinguishability is as observed earlier also polynomial.\nBefore we prove the correctness of the algorithm we make some informal points. First, we are learning under a rather pessimistic model -the positive examples may be chosen to confuse us, so we cannot make any assumptions. Accordingly we have to very crudely add all substrings and all Algorithm 1: DLG learning algorithm Data:\nInput strings S = {w 1 , w 2 . . . , }, membership oracle O Result: A sequence of DLGs G 1 , G 2 , . . . K \u2190 \u03a3 \u222a {\u03bb}, K 2 = K ; F \u2190 {(\u03bb, \u03bb)}, E = {} ; D = (F KK) \u2229 L ; G = K, D, F ; for w i do E \u2190 E \u222a {w i } ; K 2 \u2190 K 2 \u222a Sub(w i ) ; if there is some w \u2208 E that is not in L(G) then F \u2190 Con(E) ; K \u2190 K 2 ; D = (F KK) \u2229 L ; G = K, D, F ; end else D 2 \u2190 (F K 2 K 2 ) \u2229 L ; if K 2 , D 2 , F not indistinguishable from K, D, F then K \u2190 K 2 ; D = (F KK) \u2229 L ; G = K, D, F ; end end\nOutput G; end contexts, rather than using sensible heuristics to select frequent or likely ones.\nIntuitively the algorithm works as follows: if we observe a string not in our current hypothesis, then we increase the set of contexts which will increase the language defined. Since we only see positive examples, we will never explicitly find out that our hypothesis overgenerates, accordingly we always add strings to a tester set K 2 and see if this gives us a more refined model. If this seems like it might give a tighter hypothesis, then we increase K.\nIn what follows we will say that the hypothesis at step n, G n = K n , D n , F n , and the language defined is L n . We will assume that the target language is some L \u2208 L DLG and w 1 , . . . is a presentation of L.\nLemma 10. Then there is a point n, and a finite set of contexts F such that for all N > n, F N = F ., and L( \u03a3 * , L, F ) = L.\nProof. Since L \u2208 L DLG there is some set of con-texts G \u2282 Con(L), such that L = L( \u03a3 * , L, G ). Any superset of G will define the correct limit language. Let n be the smallest n such that G is a subset of Con({w 1 , . . . , w n }). Consider F n . If F n defines the correct limit language, then we will never change F as the hypothesis will be a superset of the target. Otherwise it must define a subset of the correct language. Then either there is some N > n at which it has converged to the limit language which will cause the first condition in the loop to be satisfied and F will be increased to a superset of G, or F will be increased before it converges, and thus the result holds.\nLemma 11. After F converges according to the previous lemma, there is some n, such that for all N > n, K N = K n and L( K n , L, F n ) = L.\nProof. let n 0 be the convergence point of F ; for all n > n 0 the hypothesis will be a superset of the target language; therefore the only change that can happen is that K will increase. By definition of the limit language, it must converge after a finite number of examples.\nTheorem 1. For every language L \u2208 L DLG , and every presentation of L, Algorithm 1 will converge to a grammar G such that L(G) = L. This result is immediate by the two preceding lemmas.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have presented an efficient, correct learning algorithm for an interesting class of languages; this is the first such learning result for a class of languages that is potentially large enough to describe natural language.\nThe results presented here lack a couple of technical details to be completely convincing. In particular we would like to show that given a representation of size n, we can learn once we have seen a set of examples that is polynomially bounded by n. This will be challenging, as the size of the K we need to converge can be exponentially large in F . We can construct DFAs where the number of congruence classes of the language is an exponential function of the number of states. In order to learn languages like this, we will need to use a more efficient algorithm that can learn even with \"insufficient\" K: that is to say when the lattice B(K, L, F ) has fewer elements that B(KK, L, F ). This algorithm can be implemented directly and functions as expected on synthetic examples, but would need modification to run efficiently on natural languages. In particular rather than considering whole contexts of the form (l, r) it would be natural to restrict them just to a narrow window of one or two words or tags on each side. Rather than using a membership oracle, we could probabilistically cluster the data in the table of counts of strings in F K. In practice we will have a limited amount of data to work with and we can control over-fitting in a principled way by controlling the relative size of K and F . This formalism represents a process of analogy from stored examples, based on distributional learning -this is very plausible in terms of what we know about cognitive processes, and is compatible with much non-Chomskyan theorizing in linguistics (Blevins and Blevins, 2009). The class of languages is a good fit to the class of natural languages; it contains, as far as we can tell, all standard examples of context free grammars, and includes non-deterministic and inherently ambiguous grammars. It is hard to say whether the class is in fact large enough to represent natural languages; but then we don't know that about any formalism, context-free or context-sensitive. All we can say is that there are no phenomena that we are aware of that don't fit. Only large scale empirical work can answer this question.\nIdeologically these models are empiricist -the structure of the representation is based on the structure of the data: this has to be a good thing for computational modeling. By minimizing the amount of hidden, unobservable structure, we can improve learnability. Languages are enormously complex, and it would be simplistic to try to reduce their acquisition to a few pages of mathematics; nonetheless, we feel that the representations and grammar induction algorithms presented in this paper could be a significant piece of the puzzle.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the computational complexity of approximating distributions by probabilistic automata", "journal": "", "year": "1992", "authors": "N Abe; M K Warmuth"}, {"ref_id": "b1", "title": "When won't membership queries help?", "journal": "J. Comput. Syst. Sci", "year": "1995", "authors": "D Angluin; M Kharitonov"}, {"ref_id": "b2", "title": "Learning regular sets from queries and counterexamples. Information and Computation", "journal": "", "year": "1987", "authors": "D Angluin"}, {"ref_id": "b3", "title": "Analogy in grammar: Form and acquisition", "journal": "Oxford University Press", "year": "2009", "authors": "P James; Juliette Blevins;  Blevins"}, {"ref_id": "b4", "title": "Constituent structure. Oxford University Press", "journal": "", "year": "2008", "authors": "A Carnie"}, {"ref_id": "b5", "title": "Language and mind", "journal": "Cambridge University Press", "year": "2006", "authors": "Noam Chomsky"}, {"ref_id": "b6", "title": "Another look at indirect negative evidence", "journal": "", "year": "2009-03", "authors": "Alexander Clark; Shalom Lappin"}, {"ref_id": "b7", "title": "A polynomial algorithm for the inference of context free languages", "journal": "Springer", "year": "2008-09", "authors": "Alexander Clark; R\u00e9mi Eyraud; Amaury Habrard"}, {"ref_id": "b8", "title": "A learnable representation for syntax using residuated lattices", "journal": "", "year": "2009", "authors": "Alexander Clark"}, {"ref_id": "b9", "title": "From distributional to semantic similarity", "journal": "", "year": "2003", "authors": "J R Curran"}, {"ref_id": "b10", "title": "Introduction to Lattices and Order", "journal": "Cambridge University Press", "year": "2002", "authors": "B A Davey; H A Priestley"}, {"ref_id": "b11", "title": "Formal Concept Analysis: Mathematical Foundations", "journal": "Springer-Verlag", "year": "1997", "authors": "B Ganter; R Wille"}, {"ref_id": "b12", "title": "Language identification in the limit", "journal": "Information and control", "year": "1967", "authors": "E M Gold"}, {"ref_id": "b13", "title": "Distributional structure. Word", "journal": "", "year": "1954", "authors": "Zellig Harris"}, {"ref_id": "b14", "title": "A Study of Grammatical Inference", "journal": "", "year": "1969", "authors": "J J Horning"}, {"ref_id": "b15", "title": "Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure", "journal": "", "year": "2008", "authors": "M Johnson"}, {"ref_id": "b16", "title": "Distributional phrase structure induction", "journal": "", "year": "2001", "authors": "Dan Klein; Chris Manning"}, {"ref_id": "b17", "title": "Corpus-based induction of syntactic structure: Models of dependency and constituency", "journal": "", "year": "2004", "authors": "Dan Klein; Chris Manning"}, {"ref_id": "b18", "title": "The estimation of stochastic context-free grammars using the insideoutside algorithm", "journal": "Computer Speech and Language", "year": "1990", "authors": "K Lari; S J Young"}, {"ref_id": "b19", "title": "Inductive inference, dfa's, and computational complexity", "journal": "Springer-Verglag", "year": "1989", "authors": "L Pitt"}, {"ref_id": "b20", "title": "Statistical learning by eight month old infants", "journal": "Science", "year": "1996", "authors": "J R Saffran; R N Aslin; E L Newport"}, {"ref_id": "b21", "title": "Part of speech induction from scratch", "journal": "", "year": "1993", "authors": "Hinrich Sch\u00fctze"}], "figures": [], "formulas": [{"formula_id": "formula_0", "formula_text": "C L (w) = {(l, r)|lwr \u2208 L}, equiv- alently {f |f w \u2208 L}. Clearly (\u03bb, \u03bb) \u2208 C L (w) iff w \u2208 L.", "formula_coordinates": [2.0, 72.0, 728.42, 218.27, 36.91]}, {"formula_id": "formula_1", "formula_text": "|F | , 2 |K| ).", "formula_coordinates": [3.0, 140.48, 281.16, 41.92, 11.52]}, {"formula_id": "formula_2", "formula_text": "S 1 , C 1 \u2264 S 2 , C 2 iff S 1 \u2286 S 2 , Note that S 1 \u2286 S 2 iff C 2 \u2286 C 1 .", "formula_coordinates": [3.0, 72.0, 310.22, 218.27, 24.18]}, {"formula_id": "formula_3", "formula_text": "C = {w \u2208 K : \u2200(l, r) \u2208 C, lwr \u2208 D}", "formula_coordinates": [3.0, 92.85, 465.11, 176.56, 9.57]}, {"formula_id": "formula_4", "formula_text": "L = C({(\u03bb, \u03bb)}) = C(K \u2229 D).", "formula_coordinates": [3.0, 72.0, 665.77, 218.27, 23.15]}, {"formula_id": "formula_5", "formula_text": "S 1 , C 1 \u2227 S 2 , C 2 = S 1 \u2229 S 2 , (S 1 \u2229 S 2 )", "formula_coordinates": [3.0, 87.49, 717.44, 184.5, 10.63]}, {"formula_id": "formula_6", "formula_text": "S 2 = S 2 then (S 1 \u2229 S 2 ) = (S 1 \u2229 S 2 ).", "formula_coordinates": [3.0, 307.28, 66.32, 173.93, 11.92]}, {"formula_id": "formula_7", "formula_text": "S 1 , C 1 \u2022 S 2 , C 2 = (S 1 S 2 ) , (S 1 S 2 )", "formula_coordinates": [3.0, 326.71, 159.87, 172.01, 10.63]}, {"formula_id": "formula_8", "formula_text": "X \u2264 Y then X \u2022 Z \u2264 Y \u2022 Z, Z \u2022 X \u2264 Z \u2022 Y and X \u2227 Z \u2264 Y \u2227 Z.", "formula_coordinates": [3.0, 307.28, 294.06, 215.85, 23.36]}, {"formula_id": "formula_9", "formula_text": "\u2022 If |w| \u2264 1, then \u03c6 G (w) = {w} , {w} \u2022 If |w| > 1 then \u03c6 G (w) = u,v\u2208\u03a3 + :uv=w \u03c6 G (u) \u2022 \u03c6 G (v)", "formula_coordinates": [3.0, 318.66, 467.24, 187.02, 49.58]}, {"formula_id": "formula_10", "formula_text": "ond step, if w = abc then \u03c6 G (abc) = \u03c6 G (a) \u2022 \u03c6 G (bc) \u2227 \u03c6 G (ab) \u2022 \u03c6 G (c)", "formula_coordinates": [3.0, 307.28, 569.06, 218.27, 24.23]}, {"formula_id": "formula_11", "formula_text": "L(G) = {w|\u03c6 G (w) \u2264 C({(\u03bb, \u03bb)})}", "formula_coordinates": [3.0, 337.59, 702.94, 157.64, 10.69]}, {"formula_id": "formula_12", "formula_text": "\u2022 K = {\u03bb, a, b, ab} \u2022 F = {(\u03bb, \u03bb), (\u03bb, b), (a, \u03bb)}.", "formula_coordinates": [4.0, 83.38, 129.38, 132.75, 32.04]}, {"formula_id": "formula_13", "formula_text": "\u2022 = K, \u2205 \u2022 \u22a5 = \u2205, F \u2022 L = {\u03bb, ab}, {(\u03bb, \u03bb)} \u2022 A = {a}, {(\u03bb, b)} \u2022 B = {b}, {(a, \u03bb)}", "formula_coordinates": [4.0, 83.38, 232.72, 111.16, 99.47]}, {"formula_id": "formula_14", "formula_text": "We have \u03c6 G (\u03bb) = L, \u03c6 G (a) = A, \u03c6 G (b) = B.", "formula_coordinates": [4.0, 72.0, 453.44, 218.27, 10.69]}, {"formula_id": "formula_15", "formula_text": "f ( S, C ) = (C \u2229 F ) , C \u2229 F .", "formula_coordinates": [4.0, 307.28, 219.34, 143.97, 9.57]}, {"formula_id": "formula_16", "formula_text": "1. f (X) \u2022 f (Y ) \u2265 f (X \u2022 Y ) 2. f (X) \u2227 f (Y ) \u2265 f (X \u2227 Y )", "formula_coordinates": [4.0, 315.93, 507.25, 134.25, 32.08]}, {"formula_id": "formula_17", "formula_text": "S Y . So (S X \u2229 F ) (S Y \u2229 F ) \u2287 S X S Y (as subsets of KK). So ((S X \u2229F ) (S Y \u2229F ) ) \u2286 (S X S Y ) \u2286 (S X S Y ) . Now by definition, f (X) \u2022 f (Y ) is C(Z) where Z = ((S X \u2229 F ) (S Y \u2229 F ) ) \u2229 F and f (X \u2022 Y ) has the set of contexts ((S X S Y ) \u2229 F ). Therefore f (X \u2022 Y ) has a bigger set of contexts than f (X) \u2022 f (Y ) and is thus a smaller concept. For part 2: by definition f (X \u2227 Y ) = ((S X \u2229 S y ) \u2229 F ) , (S X \u2229 S y ) \u2229 F and f (X) \u2227 f (Y ) = (S X \u2229F ) \u2229(S y \u2229F ) , ((S X \u2229F ) \u2229(S y \u2229F ) ) \u2229F Now S X \u2229 F \u2286 S X , so (since S X = S X ) S X \u2286 (S X \u2229F ) , and so S X \u2229S y \u2286 (S X \u2229F ) \u2229(S y \u2229F ) . So (S X \u2229 S y ) \u2287 ((S X \u2229 F ) \u2229 (S y \u2229 F ) ) which", "formula_coordinates": [4.0, 307.28, 606.47, 218.27, 161.27]}, {"formula_id": "formula_18", "formula_text": "f (\u03c6 G (w)) = f ( u,v \u03c6 G (u) \u2022 \u03c6 G (v)) By Lemma 1, part 2: f (\u03c6 G (w)) \u2264 u,v f (\u03c6 G (u) \u2022 \u03c6 G (v))", "formula_coordinates": [5.0, 72.0, 265.99, 186.67, 85.42]}, {"formula_id": "formula_19", "formula_text": "f (\u03c6 G (w)) \u2264 u,v f (\u03c6 G (u)) \u2022 f (\u03c6 G (v))", "formula_coordinates": [5.0, 96.1, 393.74, 170.06, 21.54]}, {"formula_id": "formula_20", "formula_text": "f (\u03c6 G (w)) \u2264 u,v \u03c6 F (u) \u2022 \u03c6 F (v)", "formula_coordinates": [5.0, 111.17, 485.45, 139.93, 21.54]}, {"formula_id": "formula_21", "formula_text": "Lemma 3. If F \u2286 G then L( K, L, F ) \u2286 L( K, L, G ),", "formula_coordinates": [5.0, 72.0, 579.64, 218.27, 23.19]}, {"formula_id": "formula_22", "formula_text": "1. g(X) \u2022 g(Y ) \u2265 g(X \u2022 Y ) 2. g(X) \u2227 g(Y ) \u2265 g(X \u2227 Y )", "formula_coordinates": [5.0, 80.66, 727.92, 131.49, 34.43]}, {"formula_id": "formula_23", "formula_text": "X = C X \u2229 J . S X \u2286 C X , so S X S Y \u2286 C X C Y , and so (S X S Y ) \u2286 (C X C Y ) , and ((S X S Y ) \u2229 J) \u2287 (C X C Y ) . By calcu- lation g(X) \u2022 g(Y ) = (C X C Y ) , (C X C Y )", "formula_coordinates": [5.0, 307.28, 79.86, 218.27, 52.88]}, {"formula_id": "formula_24", "formula_text": "g(X) \u2227 g(Y ) = C X \u2229 C Y , (C X \u2229 C Y ) and g(X \u2227 Y ) = (S X \u2229 S Y ) , (S X \u2229 S Y ) . Since S X = C X \u2229 J, S X \u2286 C X , so (S X \u2229 S Y ) \u2286 C X \u2229 C Y , and therefore (S X \u2229 S Y ) \u2286 (C X \u2229 C Y ) = C X \u2229 C Y .", "formula_coordinates": [5.0, 307.28, 174.71, 218.27, 66.43]}, {"formula_id": "formula_25", "formula_text": "First suppose |w| \u2264 1, then \u03c6 J (w) = (C L (w) \u2229 F ) \u2229 J, C L (w) \u2229 F , and g(\u03c6 J (w)) = (C L (w) \u2229 F ) , C L (w) \u2229 F which is equal to \u03c6 K (w).", "formula_coordinates": [5.0, 307.28, 327.11, 218.27, 50.98]}, {"formula_id": "formula_26", "formula_text": "g(\u03c6 J (w)) = g u,v \u03c6 J (u) \u2022 \u03c6 J (v) Next by Lemma 4, Part 2 g(\u03c6 J (w)) \u2264 u,v g(\u03c6 J (u) \u2022 \u03c6 J (v)) By Lemma 4, Part 1 g(\u03c6 J (w)) \u2264 u,v g(\u03c6 J (u)) \u2022 g(\u03c6 J (v))", "formula_coordinates": [5.0, 307.28, 438.49, 190.95, 131.3]}, {"formula_id": "formula_27", "formula_text": "g(\u03c6 J (w)) \u2264 u,v \u03c6 K (u) \u2022 \u03c6 K (v) = \u03c6 K (w) Lemma 6. If J \u2286 K then L( J, L, F ) \u2287 L( K, L, F )", "formula_coordinates": [5.0, 307.28, 615.41, 218.27, 75.18]}, {"formula_id": "formula_28", "formula_text": "(u 1 , v 1 ) \u223c (u 2 , v 2 ) iff C(u 1 ) = C(u 2 ) and C(v 1 ) = C(v 2 ) and C(u 1 v 1 ) = C(u 2 v 2 ).", "formula_coordinates": [6.0, 72.0, 214.52, 218.27, 37.73]}, {"formula_id": "formula_29", "formula_text": "F \u2286 G, then L( \u03a3 * , L, F ) \u2286 L( \u03a3 * , L, G ).", "formula_coordinates": [6.0, 72.0, 370.0, 218.27, 25.07]}, {"formula_id": "formula_30", "formula_text": "\u03c6 G (w) = u,v \u03c6 G (u) \u2022 \u03c6 G (v)", "formula_coordinates": [6.0, 72.0, 560.8, 137.28, 12.22]}, {"formula_id": "formula_31", "formula_text": "u,v C(u) \u2022 C(v) > u,v C(uv) = C(w)", "formula_coordinates": [6.0, 81.09, 587.9, 167.86, 12.22]}, {"formula_id": "formula_32", "formula_text": "L DLG = {L|\u2203F, L( \u03a3 * , L, F ) = L}", "formula_coordinates": [6.0, 335.88, 150.4, 161.07, 12.96]}, {"formula_id": "formula_33", "formula_text": "L = {a n b n c m |n, m \u2265 0} \u222a {a m b n c n |n, m \u2265 0}.", "formula_coordinates": [6.0, 307.28, 685.82, 218.27, 25.07]}, {"formula_id": "formula_34", "formula_text": "{a n b n |n \u2265 0}, {a n+1 b n |n \u2265 0}, {b n c n |n \u2265 0}, {b n c n+1 |n \u2265 0}, {a * } and {c * }.", "formula_coordinates": [7.0, 72.0, 64.36, 218.27, 38.86]}, {"formula_id": "formula_35", "formula_text": "Consider L = {a n b n |n \u2265 0} \u222a {a n b 2n |n \u2265 0}.", "formula_coordinates": [7.0, 72.0, 297.63, 218.27, 25.07]}, {"formula_id": "formula_36", "formula_text": "c k+1 , since C L (c k+1 ) \u2229 F \u2287 C L (b) \u2229 F . Thus L, which is clearly context-free, is not in L DLG .", "formula_coordinates": [7.0, 72.0, 669.33, 218.27, 25.97]}, {"formula_id": "formula_37", "formula_text": "(u 1 , v 1 ) \u223c G 1 (u 2 , v 2 ) iff C D (u 1 ) = C D (u 2 ) and C D (v 1 ) = C D (v 2 ) and C D (u 1 v 1 ) = C D (u 2 v 2 ). Note that C D (u) = {(l, r)|lur \u2208 D}.", "formula_coordinates": [8.0, 72.0, 233.82, 218.27, 37.78]}, {"formula_id": "formula_38", "formula_text": "Input strings S = {w 1 , w 2 . . . , }, membership oracle O Result: A sequence of DLGs G 1 , G 2 , . . . K \u2190 \u03a3 \u222a {\u03bb}, K 2 = K ; F \u2190 {(\u03bb, \u03bb)}, E = {} ; D = (F KK) \u2229 L ; G = K, D, F ; for w i do E \u2190 E \u222a {w i } ; K 2 \u2190 K 2 \u222a Sub(w i ) ; if there is some w \u2208 E that is not in L(G) then F \u2190 Con(E) ; K \u2190 K 2 ; D = (F KK) \u2229 L ; G = K, D, F ; end else D 2 \u2190 (F K 2 K 2 ) \u2229 L ; if K 2 , D 2 , F not indistinguishable from K, D, F then K \u2190 K 2 ; D = (F KK) \u2229 L ; G = K, D, F ; end end", "formula_coordinates": [8.0, 318.19, 83.98, 194.68, 342.84]}], "doi": ""}