{"title": "AnlamVer: Semantic Model Evaluation Dataset for Turkish -Word Similarity and Relatedness", "authors": "G\u00f6khan Ercan; Olcay Taner Y\u0131ld\u0131z", "pub_date": "", "abstract": "In this paper, we present AnlamVer, which is a semantic model evaluation dataset for Turkish designed to evaluate word similarity and word relatedness tasks while discriminating those two relations from each other. Our dataset consists of 500 word-pairs annotated by 12 human subjects, and each pair has two distinct scores for similarity and relatedness. Word-pairs are selected to enable the evaluation of distributional semantic models by multiple attributes of words and word-pair relations such as frequency, morphology, concreteness and relation types (e.g., synonymy, antonymy). Our aim is to provide insights to semantic model researchers by evaluating models in multiple attributes. We balance dataset word-pairs by their frequencies to evaluate the robustness of semantic models concerning out-of-vocabulary and rare words problems, which are caused by the rich derivational and inflectional morphology of the Turkish language.", "sections": [{"heading": "Introduction", "text": "Unsupervised semantic modeling has recently gained a lot of attention in the NLP community due to the notion of high reusability of pre-trained models across a variety of higher level NLP tasks such as machine translation, word sense disambiguation and named entity recognition. Increasing computability of unsupervised distributional semantic modeling (DSM) methods enable researchers to increase the performance of NLP tasks by leveraging extracted semantic information from a high volume of unstructured texts at low costs. However, there are very few available methods and resources to evaluate semantic models intrinsically regardless of the higher level tasks' dynamics. Presenting word similarity and word relatedness (i.e., association) dataset AnlamVer, we aim at providing the semantic modeling field for Turkish with an intrinsic evaluation resource targeting morphology driven issues caused by the rich agglutinative nature of the language. We are not aware of the existence of such word similarity or relatedness evaluation resources constructed for Turkish. In this paper, we describe design considerations and data collection guidelines we followed in the construction of such dataset as well as dataset statistics.\nThe main contributions of this paper include: (i) an introduction of a first word similarity and word relatedness evaluation dataset for a low-resource language Turkish 1 , (ii) design considerations on the construction of a dataset where the main objective is balancing the words and the words-pairs by multiple morphological and semantic attributes, (iii) a novel analysis and visualization of a word similarity and relatedness dataset containing bi-dimensional values for each word-pair and, (iv) a publicly available web-based word similarity questionnaire software. 2 2 Background and Design Motivations Word similarity evaluation (i.e., wordsim) is one of the oldest intrinsic methods of semantic model assessment. For example, RG dataset Rubenstein and Goodenough (1965) is still being used today as one of the gold standards in the DSM research. Wordsim datasets are constructed by asking human subjects to assign numeric scores from 0 to 10 for every pre-selected word-pairs. In this section, we describe some issues of word similarity evaluation and design decisions we made to overcome such issues in our study.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "Similarity and Relatedness Confusion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Linguistic Background", "text": "Linguists have been studying on statistical distributions of linguistic items (words) for a century. Although the distributional hypothesis \"words that occur in similar contexts, tend to have similar meanings\" is commonly traced back to Harris (1954), according to Sahlgren (2006), theoretical foundations of his distributional methodology go back to structuralist linguist Bloomfield (1887-1949) and Ferdinand de Saussure (1857-1913). De Saussure et al. (2011 pointed out that there can be distinctive functional roles of signs within the language system. He defined functional differences of linguistic elements in two (orthogonal) types which are widely studied with distributional relations in distributional semantics (DS) research today: syntagmatic and paradigmatic. Briefly, \"words have a syntagmatic relation if they cooccur, and a paradigmatic relation if they share same neighbors\" (Sahlgren, 2006). Paradigmatic words represent similar concepts or entities of the real world which are most likely substitutional in the context. One example is the synonyms like \"clever\" and \"smart\" in the sentence \"She is very [clever | smart].\" where two words are less likely to occur at the same sentence.", "publication_ref": ["b9", "b16", "b3", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Lack of Distinction in Word Evaluation", "text": "Following the theoretical distinction of paradigmatic and syntagmatic relation types, one can easily apply such distinction to word evaluation by making the assumption that \"similarity represents paradigmatic and relatedness represents syntagmatic type of relations.\". However, semantic research has not paid as much attention to this distinction as necessary. Two most comprehensive DSM benchmark studies, Baroni et al. (2014) and Levy et al. (2015), reported model performances based on wordsim datasets such as RG (Rubenstein and Goodenough, 1965), WordSim-353 (Finkelstein et al., 2001) and MEN (Bruni et al., 2012). In their study,  thoroughly describe the distinction between similarity and relatedness (i.e., sim-rel) caused limitation problems of such datasets.  also define the criteria for evaluation datasets in three: representative, clearly-defined, consistent and reliable. Most wordsim datasets such as RG, MC (Miller and Charles, 1991), WordSim-353, and MEN do not satisfy clearly-defined criteria since their screen guidelines use both \"similarity\" and \"relatedness\", and \"association\" words in place of each others. One good example for guideline ambiguity is the following instructions from the WordSim-353 study: \"...please assign a numerical similarity score between 0 and 10 (0 = words are totally unrelated), 10 = words are VERY closely related...\". Since our study aims to collect both similarity and relatedness scores from participants, we provided clearly-defined detailed instructions in the questionnaire screens (Figure 2).\nOne Model Does Not Fit All Agirre et al. (2009) detected the aforementioned sim-rel confusion and split the original WordSim-353 dataset into two datasets (WS-Rel and WS-Sim) by classifying word-pairs based on their relationship types. Thus, they solved the dataset's sim-rel distinction problem without re-scoring word-pairs. 3 They proposed two separate models for similarity and relatedness evaluation tasks. For example, they reported that the context-windows-based approach is better at capturing similarity (evaluated on WS-Sim) while the bag-of-words approach is at relatedness (evaluated on WS-Rel). Capturing the similarity seems arguably harder for the distributional hypothesis based unsupervised models compared to relatedness models. Examining the DSM benchmark study of Levy et al. (2015), average performances of all model configurations consistently perform the worst on the SimLex-999 similarity dataset (\u2248 0.39) compared to relatedness (traditional wordsims) (\u2248 0.70) datasets. 3 4 Similarly,  focus only on similarity evaluation while clearly informing annotators about the sim-rel distinction in their SimLex-999 dataset work. In another dataset study, SimVerb-3500 (Gerz et al., 2016), only distributional verb semantics with a large scale (3,500 word-pairs) of verb similarity evaluation is considered. We observe that DSMs have been starting to be divided into more specific models (e.g., relatedness, similarity, antonymy), motivated by the better performance requirements of the higher-level tasks. As Faruqui et al. (2016) point out, intrinsic wordsim evaluation does not correlate well with extrinsic NLP tasks' evaluation results. Wordsim sim-rel confusion might be one of the reasons for this inconsistency. It is an open question whether a single pre-trained DSM can represent the semantics of a domain consistently across multiple higher-level tasks. We think that a perfect DSM would be a multi-model structure which could handle every specific relation types (e.g., relatedness, similarity, antonymy, hypernymy, meronymy) of the words with maximum performances. In this study, our dataset targets Turkish language specific DSMs with two main types of semantic relations (similarity and relatedness) by evaluating word-pairs on both at the same time. We are not aware of the existence of such dataset study for any language.", "publication_ref": ["b1", "b11", "b15", "b6", "b2", "b0", "b11", "b7", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Sim-Rel Vector Space", "text": "Instead of splitting word-pairs into two distinct groups, we decided to get two scores for every wordpair: similarity and relatedness. By doing so, we can keep the dataset as a single unit while evaluating semantic models in two relation types. Two-dimensional structure of our evaluation data structure allows us to visualize the semantic space of the dataset through a scatter plot diagram we named \"Sim-Rel vector space\" (Figure 1).\nGiven x and y axes represent relatedness r and similarity s scores of each word-pair in the dataset respectively, and variables r and s (orthogonality of paradigmatic and syntagmatic relations) range from 0 and 10. Let SU similar-unrelated, SR similar-related, DU dissimilar-unrelated, DR dissimilar-related are categorical labels of possible semantic sub-spaces ss, ss = f 1 (r, s) function would be,\nss = f 1 (r, s) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3\nSU, if s \u2265 5 and r < 5 SR, if s \u2265 5 and r \u2265 5 DU, if s < 5 and r < 5 DR, if s < 5 and r \u2265 5 And let t = 2 denotes a threshold variable that represents boundary point of relation-type-spaces where synonym, antonym, irrelevant are categorical labels of possible semantic relation-types rt, rt = f 2 (r, s) function would be,\nrt = f 2 (r, s) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3\nsynonym, if 10 \u2212 t \u2264 s and 10 \u2212 t \u2264 r antonym, if 10 \u2212 t \u2264 r and s \u2264 t irrelevant, if t \u2265 r and t \u2265 s  If we assume that participants are asked to score lower for similarity s (closer to 0) in the case of their antonym judgements for word-pairs, and asked to score higher (closer to 10) in the case of their synonym judgements 5 , following the definition of the Sim-Rel vector space functions f 1 and f 2 above, followings can be inferred:\n\u2022 A perfect DSM could assign word-pairs to every semantic sub-space ss with 100% accuracy.\n\u2022 No word-pair instance is expected to be assigned to a similar-unrelated SU sub-space. Semantically, all highly similar word-pairs should also be highly related. For example, \"car -automobile\" wordpair is highly similar. Since they are very likely to share many common neighbors in their contexts, their relatedness score should be high, too.\n\u2022 Word-pairs could be accepted as synonyms if their rt value is assigned to synonym varying by the t parameter. The same rule applies to the irrelevant value, too. We intuitively chose the threshold t = 2 value for Sim-Rel semantic space. We kept t = 2 same for all axes and relations for the sake of model and visualization simplicity. We leave the theoretical or empirical investigation of selection strategies of such threshold values for future work.\n\u2022 Antonym-DR-overlapping problem: No DSM could perfectly assign rt as an antonym. Boundaries between the antonyms and dissimilar-related DR word-pairs are semantically ambiguous. Our bidimensional evaluation model cannot differentiate between the two. For example, word-pairs \"tense -loose\" and \"red -rose\" could get closer r and s scores while the first one can semantically be an antonym but the latter is obviously not. Asking to score lower for antonym judgements is a common method in similarity datasets ) (Gerz et al., 2016). Storing two inherently different relation types' scores (synonym, antonym) in a single s variable is the root cause of the problem. It is the downside of our Sim-Rel vector space model. We leave the resolution of this problem for future work.", "publication_ref": ["b7"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Out-Of-Vocabulary and Rare Words Problems", "text": "Turkish is an agglutinative language which has a highly inflectional and derivational morphology. The agglutinative nature allows forming new words by stringing stem, morphemes and suffixes together. In Turkish, words are bound-morphemes, which means that there can be only one lexical stem (root) of a word. Since Turkish has many productional affixes (e.g., CHk, CA, CI, lHK, SHz, HmsH), theoretically unlimited surface words can be generated. Table 1 shows inflections and derivations of various words in morphologically decomposed forms where every word share the same lexeme \"maymun\" (monkey).\nIn this study, all morphological decompositions are performed by the toolkit from G\u00f6rg\u00fcn and Yildiz (2011).  Simple word-based models ignore the internal structure of words which reduces the model's capabilities and qualities. The main problem is zero (i.e., unseen, out-of-vocabulary) or low occurrence (i.e., rare words) of a testing word in the training corpus. Distributional semantics (DS) community has been developing more complex subword-level (i.e., compositional) models to overcome out-of-vocabulary (OOV) and rare words problems. DSMs for morphologically-rich languages must alleviate OOV and rare words problem to generalize better. RW dataset (Luong et al., 2013) provides word frequency (i.e., rareness) based evaluation strategy to compositional model developers. Similarly, we aim to balance our dataset's word-pool by words' frequencies to assess generalization powers of such models.\nIn addition to the traditional OOV and rare words evaluation strategy, we applied another concept that we named made-up words by injecting novel (i.e., made-up, fake) words into the word-pool of our dataset. Vecchi et al. (2017) applied this concept to their phrase-level models to test the model's creative capabilities (i.e., generalization power). The main idea is as follows: even if people hear a word for the first time and it might sound odd to them, people have the intuition to make sense of the intended meaning. Could DSMs do the same? In our subword-level case, we assume that Turkish affixes can change the meanings of the words in a consistent manner, which is called acceptable semantic deviance. For example, the word \"maymungilleri\" (family of monkeys) is a made-up word and it sounds odd to any native Turkish speaker (Table 1) in the first place. But almost every native speaker can understand what it's meant to some extent. This productivity feature of a language can be seen as a substantial model generalization potential for a researcher. However, the downside of the concept is that the assumption does not always hold as in the word \"maymuncuk\" (skeleton key, a tool) (Table 1). In this example, the word is derived from the root word (i.e., lexeme) \"maymun\" (monkey) by getting the affix \"cHk\" with a valid state transition but its one sense's meaning shifts to an entirely different space. It is a very challenging problem for compositional DSMs. Vecchi et al. (2017) name this type of semantically-lossy derivations as deviants.", "publication_ref": ["b8", "b12", "b20", "b20"], "figure_ref": [], "table_ref": ["tab_1", "tab_1", "tab_1"]}, {"heading": "Dataset Translation Issues", "text": "Before starting the dataset construction phase, we have considered translating the existing well-known wordsim datasets to Turkish. After completing MC dataset's translation, we conclude that constructing a new dataset would be more meaningful and reliable than translating the existing ones. The main reasons behind our decision can be summarized as follows:\n1. Both words in the source word-pair map to the same single word in the target language: \"footballsoccer\" \u2192 \"futbol -futbol\".", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "A word in the source word-pair maps to a multi-word phrase: \"asylum -madhouse\" \u2192 \"t\u0131marhane -ak\u0131l hastanesi\". Traditional wordsim datasets and DSMs ignore phrases for the sake of model and evaluation simplicity. We left phrases out of the scope of this study.\n3. Meaning shifts in translations require re-annotation from human resources for every word-pair. The human annotation stage is one of the costly parts of the study. 6\n4. We aim to balance words and word-pairs in as many attributes as possible such as word frequency, derivations, inflections, concreteness and relation types. Word frequencies and morphological features are pretty much language-dependent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Design and Methodology", "text": "Design motivations we borrowed from the previous section can be summarized as follows: (i) collecting two-dimensional relatedness and similarity scores from participants while clearly-defining distinctions of such concepts, (ii) making it language-specific morphological dataset which can evaluate DSMs' generalization power concerning OOV, rare words and semantic deviance scenarios, and (iii) balancing the dataset by multiple morphological and semantic attributes as much as possible. Due to the time and budget limitations, we set the target dataset size of the project to 1.000 scores (500 word-pairs) as most of the wordsim datasets include fewer scores (SimLex-999=999, RG=65, M30, WordSim-353=353, RW=2,034, MEN=3,000). We planned dataset construction process in three stages: word candidates, word pool and word-pairs selections (Table 2).  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Word Candidates Selections TKN Dataset", "text": "Since Turkish is a low-resource language in NLP research, we aimed to re-use existing resources as much as possible. We investigated word candidates that already have some informative attributes. We used word norms \"T\u00fcrk\u00e7e Kelime Normlar\u0131\" (TKN) dataset (Tekcan and G\u00f6z, 2005) which is constructed for a psycholinguistics study. TKN consists of 600 Turkish words which are balanced in terms of concreteness (half concrete, half abstract) values. TKN's concreteness values are annotated by 100 voluntary university students. As in the English USF word norms dataset (Nelson et al., 2004), TKN words range between 1 and 7. Lower values denote more abstract and higher values denote more concrete concepts. For instance, the word \"mutluluk\" (happiness) takes 1.85 while the word \"g\u00fcl\" (rose) 6.79. By choosing candidates from TKN, we enabled model developers to evaluate their models on various concreteness levels. Unfortunately, TKN contains very frequent and root-formed word dataset with 480 words in lexical root form where none of the 120 non-root form words are inflected. 7 Those limitations led us to add 99 words (with no concreteness values) manually on the next stage in order to achieve the balancing goal of the dataset.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Word-pool Selections", "text": "Having 600 candidate words that are transferred from the first stage, our goal was to narrow down them to 320 words (word-pair candidates) while preserving our dataset balancing requirements. Table 3 shows word-pool grouping attributes along with the number of words and percentages.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Frequency-based Balancing", "text": "Considering the importance of OOV and rare words problems in modeling for morphologically rich and productive languages, our priority was to balance our word-pool based on word frequencies. RareWords dataset (Luong et al., 2013) addresses this issue by grouping words by their frequencies into four groups (5 \u2212 10, 10 \u2212 100, 100 \u2212 1000, 1000 \u2212 10000). Since the RareWords dataset is designed for the English language (which is relatively less inflectional and productive than Turkish), researchers may assume that words with frequencies lower than five are most likely to be junk or non-English words. In our case, a single Turkish lexicon can take thousands of surface forms. Our own corpus coverage analysis shows that 47% of word types (277K) occur only once in the corpus, which is compatible with Boun Corpus word coverage statistics (Sak et al., 2011). Therefore, we couldn't ignore the words that have zero or less than five frequencies. We applied a different grouping strategy, where the first group is OOV (zero frequency) and rare words in five groups RW 1, RW 2, RW 3, RW 4, RW 5. Table 3 displays how OOV and rare words (RW) groups are represented in the word-pool. We made the frequency analysis on Boun Corpus (Sak et al., 2011) which consists of roundly 3.2 million token types (i.e., vocabulary size). We defined frequency groups (0 \u2212 32, 32 \u2212 320, 320 \u2212 3200, 3200 \u2212 32000, 32000 \u2212 \u221e) by using the gr(n, voc, g) function below, where g is the number of groupings, n is the index of each group varying between 1 to g, and voc is the vocabulary size of the given corpus. The only exception is the minimum and maximum values of the first and last groups are fixed to 0 and \u221e respectively. Ampersand symbols (&) denotes string concatenations:\ngr(n, voc, g) = (voc \u00d7 10 \u2212(g\u2212n+3) ) & \"-\" & (voc \u00d7 10 \u2212(g\u2212n+2) )", "publication_ref": ["b12", "b17", "b17"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Word Pair Selections", "text": "In the word-pair selection stage, we matched words from word-pool with each other to form new pairs. We defined a constraint that every word in the word-pool should occur in the matching word-pairs up to five times. The primary goal of this mapping stage was to find, 500 word-pair relations, which are balanced explicitly by new type attributes: estimated semantics relations are synonym, antonym, hypernym, meronym etc. We manually matched and estimated the semantic type of the relationships. For example, we manually picked two potentially similar words \"otomobil\" (automobile) and \"araba\" (car) from the pool and flagged them as a strong synonym potential. We defined 50 synonym, 50 antonym, 50 meronym, 50 hypernym relations. Similarly, we grouped word-pairs by estimated magnitudes (low, medium, high) of relatedness relations too. Table 4 shows the number of actual instances and percentages of such estimation-based groupings and morphological groupings of word-pairs. Finally, we ended up with 500 manually-selected, grouped word-pairs. Table 5 shows some sample annotated word-pair instances from the final dataset.\n4 Questionnaire Design", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_8"]}, {"heading": "Platform", "text": "We built a web-based application to collect data from human annotators. Participants were asked to score similarity and relatedness relationship for every 500 word-pairs. In total, every participant scored 1,000 answers for 500 word-pairs. We split the questionnaire into two sections. The first section starts with describing what similarity relation is, along with five examples with detailed descriptions. Similar to the Simlex-999 guidelines, we asked users to score low for antonyms and score high for synonyms. We described similarity as follows (showing first two sentences only): \"Two words are similar if they refer to the same thing, person, concept or action. Similar things share common concrete or abstract attributes. For example, 'tea' and 'coffee' are quite similar because both   are relaxing hot drinks gathered from nature and irreplaceable beverages for friendly conversations.\"\nFigure 2 shows a snapshot from the initial guideline screen for similarity annotation. When a participant presses the \"ileri\" (next) button, first word-pair page appears, asking to score 20 word-pairs per screen (Figure 3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Participants", "text": "All 12 participants are native Turkish speakers who voluntarily participated in the questionnaire. Eight participants are female, and four participants are male. Both mean and median values of ages are 33.5\nFigure 2: Similarity instructions page. Figure 3: Word-pair annotation page.\nwith the standard deviation of 9.3. Nine participants are university graduates (seven participants with the master's degree), two participants are undergraduate, and a participant holds a high school degree.\nParticipants were asked to join the questionnaire remotely using their web browsers by following the invitation link sent to their mailboxes. Questionnaire software WSQuest's 8 responsive layout support, allowed users to score quickly from mobile and tablet devices. They are asked to read user guidelines carefully since no participant had the prior knowledge about the flow of the annotation process, and word similarity and relatedness concepts. Initial guideline screen informed users that they could score the questionnaire freely at any time of the day in three days since questionnaire software allows users to continue their sessions easily as long as they keep the last completed URL of the software. Without giving any breaks, it took participants' 75 minutes on average to complete the entire questionnaire.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Analysis", "text": "Final (actual) similarity s and relatedness r values of the dataset seems consistent with our estimations. Under the aforementioned Sim-Rel vector space model assumptions and configuration, scatter plot of the average values of s and r yielded a visual similar to our expectations (Figure 4). Our observations about the actual data distribution as follows:\n\u2022 SU subspace remain empty as expected. Participants proved that word-pairs cannot be similar and unrelated at the same time.\n\u2022 Antonym-DR-overlapping problem holds. We observed very close word-pair values for antonym and DR word-pairs. For example, average s and r scores of \"k\u0131rm\u0131z\u0131 -g\u00fcl\" (red -rose) word-pair are 1.16 and 7.16 respectively. On the other hand, an antonym-estimated word-pair \"\u015feffaf -opak\" (transparent -opaque) has exactly the same scores as the former one (see Table 5).\n\u2022 Participants scored word-pairs that include made-up words normally as regular word-pairs. For example, annotators scored the word-pair \"atat\u00fcrkist -kemalci\" (atat\u00fcrk+\u0130ST -kemal+CH) as 8.75 similar and 9.63 related (see Table 5) where neither of surface forms has the common usage in Turkish (OOV and RW1 respectively). Both\u0130ST and CH suffixes have usages to change words' meaning to \"ideological adherence to a person/thing\" where both names \"Atat\u00fcrk\" and \"Kemal\" are parts of the name \"Mustafa Kemal Atat\u00fcrk\" who is the founder of the Republic of Turkey.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_8", "tab_8"]}, {"heading": "Post-processing and Inter-annotator Argument", "text": "Since questionnaire includes many uncommon OOV and rare words pairs, it allows users to skip that word-pair empty if they don't have any idea about the meanings. However, null answering rate (0.1%) is quite lower than we expected. In order to calculate ranking correlations properly, we replaced null answers with the average score of all users' answers for that question. Among 16 participants, we do some post analysis on collected data. We detected that one participant achieved marginally low (0.32 min, 0.57 max) Spearman ranking correlation score compared to the other participants. After a little further investigation, we noticed that this participant had completed the test only in 25 minutes. It is three times faster than what we estimated for a high-quality annotation. Similarly, we increased the overall data quality by removing three more participant's answers too. After post-processing, we computed 0.748 average pairwise inter-annotator (apia) score where the highest pairwise correlation of users is 0.847, and the lowest one is 0.474. Even though dataset's apia score is lower than we expected, 0.748 is still higher than the most word similarity datasets (WS-Sim=0.667, MEN=0.68, 0.67=SimLex-999).\nBased on the study of (Snow et al., 2008), more than ten annotators are statistically acceptable for word similarity evaluation task's reliability.  (adh = adherent, conc = concreteness, ss = semantic sub-space, syn = synonyms, ant = antonyms, irr = irrelevant, der# = total derivations, inf# = total inflections)", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We presented a semantic model evaluation dataset for the Turkish language. Turkish morphology requires complex semantic models to alleviate OOV and rare words problems. Since the dataset includes 13% OOV and 26% rare-word-pairs (RW1 and RW2), we think that it will be a challenging intrinsic evaluation task for DSM researchers. Hopefully, AnlamVer-evaluated distinct similarity and relatedness models correlate better with the higher level NLP tasks. For future work, we are planning to construct a bigger dataset, leveraging existing lexical resources such as Turkish WordNet (Ehsani et al., 2018) which already includes manually-annotated synonymy (i.e., synsets), antonymy and hypernymy relations.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was supported by T\u00fcbitak project 116E104.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": ". Appendices", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A study on similarity and relatedness using distributional and wordnet-based approaches", "journal": "Association for Computational Linguistics", "year": "2009", "authors": "Eneko Agirre; Enrique Alfonseca; Keith Hall; Jana Kravalova; Marius Pa\u015fca; Aitor Soroa"}, {"ref_id": "b1", "title": "Don't count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors", "journal": "", "year": "2014", "authors": "Marco Baroni; Georgiana Dinu; Germ\u00e1n Kruszewski"}, {"ref_id": "b2", "title": "Distributional semantics in technicolor", "journal": "Association for Computational Linguistics", "year": "2012", "authors": "Elia Bruni; Gemma Boleda; Marco Baroni; Nam-Khanh Tran"}, {"ref_id": "b3", "title": "Course in general linguistics", "journal": "Columbia University Press", "year": "2011", "authors": "Ferdinand De Saussure; Wade Baskin; Perry Meisel"}, {"ref_id": "b4", "title": "Constructing a wordnet for turkish using manual and automatic annotation", "journal": "ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)", "year": "2018", "authors": "Razieh Ehsani; Ercan Solak; Olcay Taner Yildiz"}, {"ref_id": "b5", "title": "Problems with evaluation of word embeddings using word similarity tasks", "journal": "", "year": "2016", "authors": "Manaal Faruqui; Yulia Tsvetkov; Pushpendre Rastogi; Chris Dyer"}, {"ref_id": "b6", "title": "Placing search in context: The concept revisited", "journal": "ACM", "year": "2001", "authors": "Lev Finkelstein; Evgeniy Gabrilovich; Yossi Matias; Ehud Rivlin; Zach Solan; Gadi Wolfman; Eytan Ruppin"}, {"ref_id": "b7", "title": "Simverb-3500: A large-scale evaluation set of verb similarity", "journal": "", "year": "2016", "authors": "Daniela Gerz; Ivan Vuli\u0107; Felix Hill; Roi Reichart; Anna Korhonen"}, {"ref_id": "b8", "title": "A novel approach to morphological disambiguation for turkish", "journal": "Springer", "year": "2011", "authors": "Onur G\u00f6rg\u00fcn; Olcay Taner; Yildiz "}, {"ref_id": "b9", "title": "Distributional structure. Word", "journal": "", "year": "1954", "authors": "S Zellig;  Harris"}, {"ref_id": "b10", "title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "journal": "", "year": "2016", "authors": "Felix Hill; Roi Reichart; Anna Korhonen"}, {"ref_id": "b11", "title": "Improving distributional similarity with lessons learned from word embeddings", "journal": "Transactions of the Association for Computational Linguistics", "year": "2015", "authors": "Omer Levy; Yoav Goldberg; Ido Dagan"}, {"ref_id": "b12", "title": "Better word representations with recursive neural networks for morphology", "journal": "", "year": "2013", "authors": "Thang Luong; Richard Socher; Christopher D Manning"}, {"ref_id": "b13", "title": "Contextual correlates of semantic similarity", "journal": "Language and cognitive processes", "year": "1991", "authors": "A George;  Miller; G Walter;  Charles"}, {"ref_id": "b14", "title": "The university of south florida free association, rhyme, and word fragment norms", "journal": "Behavior Research Methods, Instruments, & Computers", "year": "2004", "authors": "Cathy L Douglas L Nelson; Thomas A Mcevoy;  Schreiber"}, {"ref_id": "b15", "title": "Contextual correlates of synonymy", "journal": "Communications of the ACM", "year": "1965", "authors": "Herbert Rubenstein; B John;  Goodenough"}, {"ref_id": "b16", "title": "The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in high-dimensional vector spaces", "journal": "", "year": "2006", "authors": "Magnus Sahlgren"}, {"ref_id": "b17", "title": "Resources for turkish morphological processing. Language resources and evaluation", "journal": "", "year": "2011", "authors": "Ha\u015fim Sak; Tunga G\u00fcng\u00f6r; Murat Sara\u00e7lar"}, {"ref_id": "b18", "title": "Cheap and fast-but is it good?: evaluating non-expert annotations for natural language tasks", "journal": "Association for Computational Linguistics", "year": "2008", "authors": "Rion Snow; O' Brendan; Daniel Connor; Andrew Y Jurafsky;  Ng"}, {"ref_id": "b19", "title": "Ali\u0130 Tekcan and\u0130lyas G\u00f6z", "journal": "\u0130stanbul Bogazi\u00e7i \u00dcniversitesi", "year": "2005", "authors": ""}, {"ref_id": "b20", "title": "Spicy adjectives and nominal donkeys: Capturing semantic deviance using compositionality in distributional spaces", "journal": "Cognitive science", "year": "2017", "authors": "M Eva; Marco Vecchi; Roberto Marelli; Marco Zamparelli;  Baroni"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Sim-Rel vector space of word-pairs.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Scatter plot of the final (actual) dataset. Data-points denote participants' avg. Sim-Rel score of each word-pair where y axis is s and x axis is r. Member counts of ss{SU, SR, SU, DR} semantic sub-spaces are in the parenthesis.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Morphological decomposition of various words sharing the same lexeme.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Three stages of dataset construction.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Groupings of the word-pool.G0G1G2G3G4G5 TotalEst. Synonymssynonym antonym other505040050010%10%80%100%Est. Relatedness highmediumlow20015015050040%30%30%100%Est. Rel. Typehyponym meronym other505040050010%10%80%100%OOVno oovany oovtwo oov434664250086.8%13.2%8.4%100%Min. Derivations no der.der1der2+23116610350046.2%33.2%20.6%100%Min. Inflectionsno infinf1inf2+424324450084.8%6.4%8.8%100%Min. RareWordrw0 (oov) rw1rw2rw3 rw4rw5666562130 1423550013.2%13%12.4%26% 28.4% 7% 100%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Groupings of the word-pairs.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Sample word-pairs from the final dataset. Words with asterisks (*) are made-up words.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "ss = f 1 (r, s) = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3", "formula_coordinates": [3.0, 201.78, 560.47, 82.15, 62.62]}, {"formula_id": "formula_1", "formula_text": "rt = f 2 (r, s) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3", "formula_coordinates": [3.0, 166.83, 687.69, 81.09, 42.98]}, {"formula_id": "formula_2", "formula_text": "gr(n, voc, g) = (voc \u00d7 10 \u2212(g\u2212n+3) ) & \"-\" & (voc \u00d7 10 \u2212(g\u2212n+2) )", "formula_coordinates": [7.0, 152.56, 398.4, 292.42, 12.3]}], "doi": ""}