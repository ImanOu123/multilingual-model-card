{"title": "InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation", "authors": "Pierre Colombo; Chlo\u00e9 Clavel; Pablo Piantanida", "pub_date": "", "abstract": "Assessing the quality of natural language generation systems through human annotation is very expensive. Additionally, human annotation campaigns are time-consuming and include non-reusable human labour. In practice, researchers rely on automatic metrics as a proxy of quality. In the last decade, many string-based metrics (e.g., BLEU) have been introduced. However, such metrics usually rely on exact matches and thus, do not robustly handle synonyms. In this paper, we introduce InfoLM a family of untrained metrics that can be viewed as a string-based metric that addresses the aforementioned flaws thanks to a pre-trained masked language model. This family of metrics also makes use of information measures allowing the adaptation of InfoLM to various evaluation criteria. Using direct assessment, we demonstrate that InfoLM achieves statistically significant improvement and over 10 points of correlation gains in many configurations on both summarization and data2text generation.", "sections": [{"heading": "Introduction", "text": "A plethora of applications of natural language processing (NLP) performs text-to-text transformation (Mellish and Dale 1998;Belz and Reiter 2006;Specia, Scarton, and Paetzold 2018). Given an input, these systems are required to produce an output text that is coherent, readable and informative. Due to both high annotation costs and time, researchers tend to rely on automatic evaluation to compare the outputs of such systems. Reference-based automatic evaluation relies on comparing a candidate text produced by the NLG system and one or multiple reference texts ('gold standard') created by a human annotator. Generic automatic evaluation of NLG is a huge challenge as it requires building a metric that evaluates the similarity between a candidate and one or several gold-standard reference texts. However, the definition of success criteria is task-specific: as an example, evaluation of text summarization focuses on content, coherence, grammatically, conciseness, and readability (Mani 2001;Colombo et al. 2022), whereas machine translation focuses on fidelity, fluency and adequacy of the translation (Hovy 1999) and data2text generation (Gardent et al. 2017) consider criteria such as data coverage, correctness and text structure.\nAutomatic text evaluation metrics fall into two categories: metrics that are trained to maximise their correlations using human annotation (e.g., BLEND (Ma et al. 2017)) and untrained metrics (e.g., BLEU (Papineni et al. 2002), ROUGE (Lin 2004), BERTSCORE (Zhang et al. 2019), DepthScore (Staerman et al. 2021), BaryScore (Colombo et al. 2021c), MOVERSCORE (Zhao et al. 2019)). In this work, we focus on untrained metrics as trained metrics may not generalize well to new data (existing labelled corpora are of small size). Two categories of untrained metrics can be distinguished: word or character based-metrics that compute a score based on string representation and embedding-based metrics that rely on a continuous representation. String-based metrics (e.g., BLEU) often fail to robustly match paraphrases (Reiter and Belz 2009) as they mainly focus on the surface form as opposed to embeddingbased metrics relying on continuous representations.\nIn this paper, we introduce InfoLM a family of new untrained metrics to evaluate text summarization and data2text generation. At the highest level InfoLM key components include: (1) a pre-trained masked language model (PMLM) that is used to compute two discrete probability distributions over the vocabulary. They represent the probability of observing each token of the vocabulary given the candidate and the reference sentence, respectively. (2) A contrast function I that is used to measure the dissimilarity between aforementioned probability distributions. InfoLM differs from existing BERT-based metrics (e.g. BERTSCORE, MOVERSCORE) as it directly relies on the PMLM which outputs discrete probability distributions. Thus InfoLM does neither require to arbitrarily select one or several specific layers ( e.g. BERTSCORE relies on the 9th layer for bert-base-uncased), nor involves selecting arbitrary aggregations technics (e.g. see MOVERSCORE). As InfoLM relies on statistics on tokens it can also be seen as a string-based metric. However, it does not suffer from common pitfalls of string-based metrics (e.g. synonyms, need of an exact string-match) as the PMLM also allows ones to assign a high score to paraphrases and to capture distant dependencies. Contributions Our contributions are summarized below: arXiv:2112.01589v3 [cs.CL] 25 Mar 2022\n(1) A set of novel metrics to automatically evaluate summarization and data2text generation. In this work, we introduce InfoLM which overcomes the common pitfall of string matching metrics and does not require to select a layer, nor to rely on a arbitrary aggregation function. InfoLM combines a pre-trained model and a contrast function denoted by I between two discrete probability distributions. We explore the use of different choices of contrast functions such as f -divergences, L p distances or Fisher-Rao distances.\n(2) Tasks. First, we demonstrate on both summarization and data2text that InfoLM is better suited than concurrent metrics. A comparison is conducted, using multiple correlation measures with human judgment both at the text and system level. Second, we dissect InfoLM to better understand the relative importance of each component (e.g. calibration, sensibility to the change of information measures).", "publication_ref": ["b40", "b3", "b56", "b37", "b15", "b26", "b22", "b35", "b44", "b33", "b62", "b57", "b16", "b63", "b52"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Statement and Related Work", "text": "In this section, we start by introducing notations and formulate the problem of both evaluating text generation and metrics. Then, we identify and present the most relevant related work and the existing approaches for the studied tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Statement", "text": "NLG evaluation. Given a dataset D = {x i x i x i , {y s i y s i y s i , h(x i x i x i , y s i y s i y s i )} S s=1 } N i=1\nwhere x x x i is the i-th reference text; y y y s i is the i-th candidate text generated by the s-th NLG system; N is the number of texts in the dataset and S the number of systems available. The vector\nx i x i x i = (x 1 , \u2022 \u2022 \u2022 , x M ) is composed of M tokens (e.g., words or subwords) and y s i y s i y s i = (y s 1 , \u2022 \u2022 \u2022 , y s L\n) is composed of L tokens 1 . The set of tokens (vocabulary) is denoted as \u2126, T denotes the set of possible texts. h(\nx i x i x i , y s i y s i y s i ) \u2208 R +\nis the score associated by a human annotator to the candidate text y y y s i when comparing it with the reference text x i x i x i . We aim at building an evaluation metric f such that f (x i\nx i x i , y i y i y i ) \u2208 R + . Evaluating evaluation metrics. To assess the relevance of an evaluation metric f , correlation with human judgment is considered to be one of the most important criteria (Koehn 2009;Chatzikoumi 2020). Debate on the relative merits of different correlations for the evaluation of automatic metrics is ongoing but classical correlation measures are Pearson (Leusch et al. 2003), Spearman (Melamed, Green, and Turian 2003) or Kendall (Kendall 1938) tests. Two metaevaluation strategies are commonly used: (1) text-level correlation or (2) system-level correlation. Formally, the textlevel correlation C t,f is computed as follows:\nC t,f 1 N N i=1 K(F t i , H t i ),(1)\nwhere\nF i = f (x i x i x i , y 1 i y 1 i y 1 i ), \u2022 \u2022 \u2022 , f (x i x i x i , y S i y S i y S i ) and H i = h(x i x i x i , y 1 i y 1 i y 1 i ), \u2022 \u2022 \u2022 , h(x i x i x i , y S i y S i y S i )\nare the vectors composed of scores assigned by the automatic metric f and the human respectively. and K : R N \u00d7 R N \u2192 [\u22121, 1] is the chosen correlation measure (e.g., Pearson, Kendall or Spearman). Similarly, the system level correlation C sy,f is obtained by C sy,f K(F sy , H sy ),\n(2)\nF sy = 1 N N i=1 f (x i x i x i , y 1 i y 1 i y 1 i ), . . . , 1 N N i=1 f (x i x i x i , y S i y S i y S i ) H sy = 1 N N i=1 h(x i x i x i , y 1 i y 1 i y 1 i ), . . . , 1 N N i=1 h(x i x i x i , y S i y S i y S i ) ,\nwhere the latter are the vectors composed of the averaged scores assigned by f and the human, respectively. For the significance analysis, we follow Graham and Baldwin (2014) and use a William test to validate a significant improvement for dependent correlations (Steiger 1980).  (Mikolov et al. 2013) or contextualized embeddings such as ELMO (Peters et al. 2018), BERT (Devlin et al. 2018) and its variants (Sanh et al. 2019;Liu et al. 2019). Among the most popular metrics, we can mention MOVERSCORE, BERTSCORE, WMD (Kusner et al. 2015) , WMDO (Chow, Specia, and Madhyastha 2019). Different from these approaches InfoLM relies on a language model and work with discrete probability distributions instead of continuous representations. PMLM as a Metric. To the best of our knowledge, using a PMLM (i.e, without further training) as a reference-based automatic metric remains overlooked. The closest use we found was to rely on autoregressive models, such as GPT-2 (Radford et al. 2019), to compute the generated sentence perplexity and assess its fluency. Researchers mainly focused on the use of the learnt embedding of the PMLM. However, it remains an open question to find a reliable layer aggregation mechanism (e.g, BERTSCORE arbitrary selects a layer based on a chosen dataset, MOVERSCORE uses the 5 last layers). InfoLM addresses this aggregation issue by relying on the PMLM.", "publication_ref": ["b29", "b8", "b31", "b39", "b28", "b23", "b59", "b41", "b45", "b20", "b54", "b34", "b10", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Existing Metrics", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning-based Metrics", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Masked Language Modeling", "text": "Language models based on masked language pre-training objectives Devlin et al. (2018); Liu et al. (2019) aim at reconstructing a corrupt version [x x x] of an input text x x x by minimizing a cross-entropy loss. This corrupted context corresponds to a \"local view\" view of the sentence. To ensure fair comparison, we do not use existing alternatives (e.g. GPT-2 based models (Radford et al. 2019)) as concurrent works (Zhang et al. 2019;Zhao et al. 2019) rely on PMLM.", "publication_ref": ["b20", "b34", "b49", "b62", "b63"], "figure_ref": [], "table_ref": []}, {"heading": "InfoLM", "text": "In this section, we first introduce a novel family of metrics called InfoLM and then detail the different components of these novel metrics.\nNotations We denote by \u03b8 \u2208 \u0398, the parameter of the PMLM, T its temperature and I : [0, 1] |\u2126| \u00d7 [0, 1] |\u2126| an information measure (see 3.3) which quantifies the similarity between two discrete distributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Motivations & Definitions", "text": "A PMLM has learnt the empirical distribution of a large text corpus. Given a text x x x, the corrupted context with a mask at position j is denoted [x x x] j , the LM predicts a distribution p \u2126|T (\u2022|[x x x] j ; \u03b8; T ) over the vocabulary \u2126 given the masked context. As an example, for a masked input sentence [x x x] 1 = \"The [MASK] was delicious\", a pretrained model could place high probabilities on tokens \"food\", \"meal\" and low probability on \"the\". It is worth noting that p \u2126|T (\u2022|[x x x] 1 ; \u03b8; T ) represents the probability of observing each token of the vocabulary given the masked input [x x x] 1 . Definition 3.1 (Equivalence for masked contexts). Given I, two masked contexts [x x x] j , [y y y] k from input texts x x x, y y y, with masks at positions j and k respectively, are equivalent (denoted\n[x x x] j I \u223c [y y y] k ) if the two predicted discrete distributions given by the PMLM, namely p \u2126|T (\u2022|[x x x] j ; \u03b8; T ) and p \u2126|T (\u2022|[y y y] k ; \u03b8; T ), are similar. Formally, [x x x] j I \u223c [y y y] k if I p \u2126|T (\u2022|[x x x] j ; \u03b8; T ), p \u2126|T (\u2022|[y y y] k ; \u03b8; T ) \u2248 0.\nRemark. We have the intuition that two similar sentences will share several pairs of equivalent masked contexts. At this point, we make no claim on the relationship between equivalence and the masked context similarity.\nIn this work, we make the hypothesis that two similar sentences x x x, y y y will share multiple equivalent masked contexts.  \np \u2126|T (\u2022|x x x; \u03b8; T ) M k=1 \u03b3 k \u00d7 p \u03b8 (\u2022|[x x x] k ; \u03b8; T ), p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) N k=1\u03b3 k \u00d7 p \u03b8 (\u2022|[y s i y s i y s i ] k ; \u03b8; T ),\nwhere\u03b3 k and \u03b3 k are measures of the importance of the k-th token in the candidate and reference text, respectively, i.e., satisfying M j=1 \u03b3 j = N j=1\u03b3 j = 1. These are computed using the inverse document frequency scores determined at the corpus level (Zhao et al. 2019;Kusner et al. 2015).\nLM Calibration. Modern deep neural networks are overconfident (Guo et al. 2017). To re-calibrate language models several techniques have been proposed (e.g temperature scaling (Platt et al. 1999)). Here, we choose to study how calibration affects InfoLM by relying on temperature 2 scaling motivated by simplicity and speed.", "publication_ref": ["b63", "b30", "b24", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "Information Measures", "text": "In this work, we focus on comparing a pair of discrete probability distributions through information measures (see Basseville ( 2013) for an exhaustive study). We rely on two types of information measures: divergences and distances. The divergence is a measure of dissimilarity that is always positive or equal to zero if (and only if) the two considered distributions are strictly identical. We call distance, a function that is symmetric, positive, respects the triangle inequality and is equal to zero if (and only if) the two considered distributions are strictly identical. We will use information measures that belong to either Csiszar f -divergences or that are distances.\nDivergence Measures Various divergence measures have been proposed for a large variety of applications (Basseville 2013). The full expression of the studied divergences can be found in 1. We focus here on three families of divergences \u03b1 Divergences, \u03b3 Divergences and AB Divergences. Note that there exist other families of divergences such as Bregman divergence (Bregman 1967), \u03b2 divergences (Basu et al. 1998), Chernoff divergence (Chernoff et al. 1952). \u03b1-Divergences. This divergence was introduced by R\u00e9nyi et al. (1961) and are a special case of the f -divergences (Csisz\u00e1r 1967). They are widely used in variational inference (Li and Turner 2016) and closely related to R\u00e9nyi divergences but are not a special case. From 1 we note special cases of \u03b1-Divergences: (i) Kullback-Leiber (KL) is recovered by letting \u03b1 \u2192 1, (ii) Hellinger distance (Hellinger 1909) follows by choosing \u03b1 = 0.5. For this family, \u03b1 weights the influence of p q . \u03b3-Divergences. This divergence has been introduced by (Fujisawa and Eguchi 2008) as a scale-invariant modification of the robust \u03b2-divergences. 3 For the \u03b3 divergences the parameter \u03b2 is used to control the importance of the element of small probabilities (e.g., outliers in some scenarios, tokens with low probability in our case). If \u03b2 > 1, the importance of large q i is reduced which gives more weights to the outliers. Special cases include the L 2 distance (i.e., \u03b2 = 2) and KL divergence (i.e., \u03b2 \u2192 1). AB-Divergences. The family of AB-divergences is flexible and allows to respectively control the mass coverage or the robustness. Cichocki, Cruces, and Amari (2011) propose to use AB divergences. As can be seen in 1 these divergences have two parameters \u03b1, \u03b2. It allows to tune the mass coverage and the robustness independently. The \u03b2-divergence is obtained by choosing \u03b1 = 1, \u03b2 \u2208 R. From information divergences to discrimination. For our application, we would like to produce a metric between two texts regardless of the source (system or human). Thus we are interested in symmetric divergence: such divergences are called discrimination. To obtain discrimination two tricks are commonly applied either the Jeffrey's symmetrization, which averages KL(p q) and KL(q p)), or the Jensen's symmetrization, which averages KL(p p+q 2 ) and KL(q p+q 2 ). We choose to use Jeffreys symmetrization as it does not require computing p+q 2 . The symmetric KL with Jeffrey's symmetrization is denoted JS.\nDistances L p distances. The L p distances p \u2208 R + can be used to measure the similarity between two distributions and we restrict ourselves to p \u2208 {1, 2, +\u221e}. Fisher-Rao distance. The Fisher-Rao distance (R) represents the Geodesic Distance (Rao 1987) between two distributions. Interestingly, this distance remains overlooked in the ML community but has been recently used to achieve robustness against adversarial attacks (Picot et al. 2021).\nConnection with String Matching Metrics. We adopt the following notations p \u2126|T (\u2022|x i\nx i x i ; \u03b8; T ) = [p 0 , \u2022 \u2022 \u2022 , p |\u2126| ] and p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) = [q 0 , \u2022 \u2022 \u2022 , q |\u2126| ]. First, let us con- sider two texts x i x i x i , y s i y s i y s i such that x i x i x i I \u2248 y s i y s i y s i . InfoLM with L \u221e is closed to 0 if \u2200i \u2208 [1, |\u2126|] p i \u2248 q i .\nIt means that all likely tokens (according to the PMLM) when considering x i\nx i x i are also likely when considering y s i y s i y s i . For string matching metrics, it corresponds to a perfect match between x i\nx i x i and y s i\ny s i y s i . Second, let us consider x i x i x i , y s i y s i y s i such that I p \u2126|T (\u2022|x i x i x i ; \u03b8; T ), p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) \u2248 1 (dissimilar texts\n) and a measure of information that relies on product of p i and q i (e.g Fisher-Rao). In this case, \u2200i \u2208 [1, |\u2126|] p i \u00d7 q i \u2248 0 thus all likely tokens when considering x i\nx i x i are unlikely when considering y s i y s i y s i (the converse it true as well). For string matching metrics this corresponds to no match among the sub-strings of x i\nx i x i and y s i y s i y s i .", "publication_ref": ["b1", "b5", "b2", "b9", "b53", "b32", "b25", "b11", "b50", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Frameworks", "text": "In this section, we describe our experimental setting. We present the tasks and the baselines metrics use for each task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Text Summarization", "text": "Text summarization aims at compressing long texts into fluent, short sentences that preserve the salient information. Datasets. To compare the different metrics previous work (Bhandari et al. 2020) either relies on the TAC datasets (Dang and Owczarzak 2008;McNamee and Dang 2009) or on new summarization datasets extracted from CNN/Dai-lyMail (Nallapati et al. 2016). As pointed out by Peyrard (2019); Bhandari et al. (2020), TAC datasets are old and contain flaws (e.g systems used to generate summaries were of poor quality), we choose to work with the newly assemble dataset from CNN/Daily News proposed in Bhandari et al. (2020). This dataset gathers 11,490 summaries and annotations are carried using the pyramid method (Nenkova and Passonneau 2004). Metrics. For text summarization, perhaps the most known metrics are ROUGE and its extensions (Ng and Abrecht 2015), or METEOR and its variants (Denkowski and Lavie 2014). Recently, a new set of metrics (e.g BERTSCORE, MOVERSCORE) have been applied to text summarization.", "publication_ref": ["b4", "b18", "b38", "b42", "b46", "b4", "b4", "b43", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Data2Text Generation", "text": "Prior works mainly rely on two task-oriented dialogue datasets (i.e., BAGEL (Mairesse et al. 2010), SFHOTEL (Wen et al. 2015)). As sentence generated in these data-sets (Cichocki, Cruces, and Amari 2011 Metrics. For this task, organisers rely on untrained metrics (e.g. BLEU, METEOR, TER, BERTSCORE) to compare the performance of the candidate systems. Thus, we will focus on system-level correlation.\nName Notation Domain Expression \u03b1-divergence (Csisz\u00e1r 1967) D \u03b1 \u03b1 \u2208 {0, 1} 1 \u03b1(\u03b1\u22121) (1 \u2212 q 1\u2212\u03b1 i p \u03b1 i ) \u03b3 divergence (Fujisawa and Eguchi 2008) D \u03b2 \u03b3 \u03b2 \u2208 {0, \u22121} 1 \u03b2(\u03b2+1) log p \u03b2+1 i + 1 \u03b2+1 log q \u03b2+1 i \u2212 1 \u03b2 log p i q \u03b2 i AB Divergence\n) D \u03b1,\u03b2 sAB (\u03b1, \u03b2) \u2208 (R * ) 2 \u03b2 + \u03b1 = 0 1 \u03b2(\u03b2+\u03b1) log p \u03b2+\u03b1 i + 1 \u03b2+\u03b1 log q \u03b2+\u03b1 i \u2212 1 \u03b2 log p \u03b1 i q \u03b2 i L 1 distance L 1 |p i \u2212 q i | L 2 distance L 2 (p i \u2212 q i ) 2 L \u221e distance L \u221e max i |p i \u2212 q i | Fisher-Rao distance R 2 \u03c0 arccos \u221a p i \u00d7 q i", "publication_ref": ["b36", "b61"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Numerical Results", "text": "In this section, we study the performance of InfoLM on both text summarization and data2text generation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results on Text Summarization", "text": "General Analysis. 1 gathers the results of the correlation study between scores produced by different metrics and human judgement (i.e. pyramid score). We can reproduce results from Bhandari et al. (2020). We observe a different behavior depending on the type of systems to be evaluated (e.g., abstractive or extractive) and the chosen correlation coefficient. We observe that InfoLM with D AB or with R outperforms other BERT-based metrics such as MOVERSCORE or BERTSCORE (e.g., it is worth noting that both metrics perform poorly at the text or system-level when considering outputs from extractive systems). D AB largely outperforms n-gram matching metrics (e.g., ROUGE metrics) on all datasets when measuring correlation with the Kendall \u03c4 and in almost all configurations (except when considering abstractive outputs at the system level) when using the Pearson r. It is worth noting the overall good performance of the parameter-free Fisher-Rao distance.\nChoice of information geometric measure for InfoLM.\nIn 1, we can observe two different types of groups depending on the global behaviour. First we notice that using L p , p \u2208 {1, 2, . . . , +\u221e} leads to poor performances in many configurations. Good performance of L \u221e in some configurations is surprising as L \u221e is extremely selective (i.e. L \u221e computes max i |p i \u2212 q i |). As output produced by the PMLM is sparse, max i |p i \u2212 q i | correspond to one likely word in one sentence and not likely at all in the other. The second group gathers JS, R, D \u03b1 , D \u03b2 and D AB and achieves the best performance overall. D \u03b1 and D AB achieve similar performance suggesting that the flexibility (e.g., robustness to outliers) introduced by the \u03b2 parameter in D AB is not useful in our task. This observation is strengthened by the lower performance of D \u03b2 . The difference of results between the two measures is due to the flexibility introduced by \u03b1 (i.e., \u03b1 controls the relative importance of the ration pi qi ) which can be interpreted in our case as the ability to control the importance attributed to less likely words (Jalalzai *). Takeaways. The best performing metric is obtained with D AB . The Fisher-Rao distance, denoted by R, achieves good performance in many scenarios and has the advantage to be parameter-free.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Results on Data2Text", "text": "Global Analysis: 2 gathers results of the correlation analysis of the metrics with human judgements following the five different axes. We observe that the five considered criteria of annotations are not independent: text structure and fluency achieve a strong correlation coefficient (> 98). Additionally, all metrics achieve similar results when the correlation is computed on these two criteria. We observe that the best performing group of metric is based on InfoLM followed by metrics based on continuous representation from BERT (i.e., MOVERSCORE and BERTSCORE) followed by N-gram matching metrics. Regarding correctness, data coverage and relevance, we observe that both D AB and D \u03b1 achieve the best results on almost all correlation coefficients. On data coverage, InfoLM achieves improvement up to 17 points in correlation compared to both BERT based or N-gram matching metrics. Regarding fluency and text structure, Fisher-Rao distance works better and slightly outperforms the   second-best performing metric, namely BERTSCORE. Takeaways. Similar to summarisation, we observe very low correlation for L p , p \u2208 {1, 2, . . . , +\u221e}. We also observe that \u03b2-divergences achieve lower results than both \u03b1 and AB divergences suggesting that, as noticed for summarisation, robustness to unlikely words (i.e., outliers) is less relevant for our task.\nD AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS\nD AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS\nD AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS Ber tS Mo verS BLE U R-1 R-2 R-W E 0.5 0.6 0.7 0.8 (f) Ext -Text D AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Further Analysis", "text": "Correlation Between Metrics In this experiment, we complete our global analysis by comparing the scores obtained by the different metrics with each other. We want to gain an understanding of how different our metric is from other metrics and how the choice of information geometric measures affects the predictions. 2 gathers the results of the experiment. We observe a high correlation (r > 88) between D \u03b1 , D \u03b2 , D AB and R 4 . Interestingly, we observe a lower correlation (r \u2248 70) with BERTSCORE and N-gram matching metrics, e.g., ROOGE) whereas BERTSCORE achieves a stronger correlation with ROUGE (r \u2248 80). Takeaways. Through the correlation analysis in 2, we observe the impact of different geometry on InfoLM predictions. The correlation analysis shows that the prediction of InfoLM when using D \u03b1 , D \u03b2 , D AB and R are highly correlated and as illustrated by previous experience achieve high correlation scores which we believe validate our approach.\nIt is worth noting that R requires no tuning as it is parameter free.\nFigure 2: Pearson correlation at the system level between metrics when considering abstractive system outputs.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Score Distributions", "text": "In 3, we study the text score distribution of different metrics on abstractive summary. The ideal metric would mimic the human score distribution (i.e. P yr.) and be able to distinguish between good and bad quality summaries. The results show that ROUGE and BLEU struggle to distinguish between between good quality (P yr. \u2265 0.5) low quality summaries (P yr. \u2264 0.5) which has been reported in Peyrard (2019). We observe that D AB , R and JS metrics are able to make the distinction. Interestingly, as p increases and the L p distances become more selective (i.e. focus on one word solely), the L p distances struggle to distinguish low from high scoring summaries.  Takeaways. InfoLM when combined with D AB , R and JS is able to distinguish high-scoring from low scoring summaries.\nFigure 3: Score distribution of text score when considering abstractive system outputs. P yr. stands for pyramide score.\nTemperature Calibration To study the impact of calibration, we choose to work on system-level correlation and report in 4 the achieved the correlation measured by the different coefficients. We limit our study to the Fisher-Rao distance as it is a parameter-free metric and is among the bestperforming metrics of InfoLM. Due to space constraints, we report the result on extractive systems only. Takeaways. Fisher-Rao only considers product p i \u00d7 q i thus as T increases and the predicted probability of the PMLM becomes more uniform more words are considered and the aggregated distributions become richer in terms of considered words. It is worth noting that when changing the temperature we observe a smooth change in correlation and observe an optimal temperature T which is reached for T \u2208 [1, 2]. It suggests that InfoLM benefits from a PMLM that is not too selective (case T 1). For a specific application, the temperature of InfoLM can be tuned to improve correlation and InfoLM will likely benefit from well-calibrated PMLM. ", "publication_ref": ["b46"], "figure_ref": [], "table_ref": []}, {"heading": "Summary and Concluding Remarks", "text": "In this work, we presented InfoLM that does not require training and it is among the first metrics computing the similarity between two discrete probability distributions over the vocabulary (which is similar to string-based metrics) but also leverages the recent advance in language modeling thanks to a PMLM. Our experiments on both summarization and data2text generation demonstrate the validity of our approach. Among available contrast measures, the Fisher-Rao distance is parameter-free and thus, it is easy to use in practice while the AB-Divergence achieves better results but requires to select \u03b1 and \u03b2. Future work includes extending our metrics to new tasks such as SLU (Chapuis et al. 2020(Chapuis et al. , 2021Dinkar et al. 2020;Colombo, Clavel, and Piantanida 2021), controlled sentence generation (Colombo et al. 2019(Colombo et al. , 2021b and multi-modal learning (Colombo et al. 2021a;Garcia et al. 2019).", "publication_ref": ["b7", "b6", "b13", "b17", "b14", "b12", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was also granted access to the HPC resources of IDRIS under the allocation 2021-AP010611665 as well as under the project 2021-101838 made by GENCI.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "METEOR: An automatic metric for MT evaluation with improved correlation with human judgments", "journal": "", "year": "2005", "authors": "S Banerjee; A Lavie"}, {"ref_id": "b1", "title": "Divergence measures for statistical data processing-An annotated bibliography. Signal Processing", "journal": "", "year": "2013", "authors": "M Basseville"}, {"ref_id": "b2", "title": "Robust and efficient estimation by minimising a density power divergence", "journal": "Biometrika", "year": "1998", "authors": "A Basu; I R Harris; N L Hjort; M Jones"}, {"ref_id": "b3", "title": "Comparing automatic and human evaluation of NLG systems", "journal": "", "year": "2006", "authors": "A Belz; E Reiter"}, {"ref_id": "b4", "title": "Re-evaluating Evaluation in Text Summarization", "journal": "", "year": "2020", "authors": "M Bhandari; P N Gour; A Ashfaq; P Liu; G Neubig"}, {"ref_id": "b5", "title": "The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming", "journal": "", "year": "1967", "authors": "L M Bregman"}, {"ref_id": "b6", "title": "Code-switched inspired losses for generic spoken dialog representations", "journal": "EMNLP", "year": "2021", "authors": "E Chapuis; P Colombo; M Labeau; C Clavel"}, {"ref_id": "b7", "title": "Hierarchical pre-training for sequence labelling in spoken dialog", "journal": "", "year": "2020", "authors": "E Chapuis; P Colombo; M Manica; M Labeau; C Clavel"}, {"ref_id": "b8", "title": "How to evaluate machine translation: A review of automated and human metrics", "journal": "Natural Language Engineering", "year": "2020", "authors": "E Chatzikoumi"}, {"ref_id": "b9", "title": "A measure of asymptotic efficiency for tests of a hypothesis based on the sum of observations", "journal": "The Annals of Mathematical Statistics", "year": "1952", "authors": "H Chernoff"}, {"ref_id": "b10", "title": "WMDO: Fluency-based Word Mover's Distance for Machine Translation Evaluation", "journal": "ACL", "year": "2019", "authors": "J Chow; L Specia; P Madhyastha"}, {"ref_id": "b11", "title": "Generalized alpha-beta divergences and their application to robust nonnegative matrix factorization", "journal": "Entropy", "year": "2011", "authors": "A Cichocki; S Cruces; S Amari"}, {"ref_id": "b12", "title": "Improving multimodal fusion via mutual dependency maximisation", "journal": "EMNLP", "year": "2021", "authors": "P Colombo; E Chapuis; M Labeau; C Clavel"}, {"ref_id": "b13", "title": "A Novel Estimator of Mutual Information for Learning to Disentangle Textual Representations", "journal": "ACL", "year": "2021", "authors": "P Colombo; C Clavel; P Piantanida"}, {"ref_id": "b14", "title": "Beam Search with Bidirectional Strategies for Neural Response Generation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "P Colombo; C Clavel; C Yack; G Varni"}, {"ref_id": "b15", "title": "What are the best systems? New perspectives on NLP Benchmarking", "journal": "", "year": "2022", "authors": "P Colombo; N Noiry; E Irurozki; S Clemencon"}, {"ref_id": "b16", "title": "Automatic text evaluation through the lens of wasserstein barycenters", "journal": "EMNLP", "year": "2021", "authors": "P Colombo; G Staerman; C Clavel; P Piantanida"}, {"ref_id": "b17", "title": "Affect-driven dialog generation. NAACL. Csisz\u00e1r, I. 1967. Information-type measures of difference of probability distributions and indirect observation", "journal": "studia scientiarum Mathematicarum Hungarica", "year": "2019", "authors": "P Colombo; W Witon; A Modi; J Kennedy; M Kapadia"}, {"ref_id": "b18", "title": "Overview of the TAC 2008 Update Summarization Task", "journal": "", "year": "2008", "authors": "H T Dang; K Owczarzak"}, {"ref_id": "b19", "title": "Meteor universal: Language specific translation evaluation for any target language", "journal": "", "year": "2014", "authors": "M Denkowski; A Lavie"}, {"ref_id": "b20", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova"}, {"ref_id": "b21", "title": "Robust parameter estimation with a small bias against heavy contamination", "journal": "", "year": "2008", "authors": "H Fujisawa; S Eguchi"}, {"ref_id": "b22", "title": "From the Token to the Review: A Hierarchical Multimodal Approach to Opinion Mining", "journal": "", "year": "2017", "authors": "A Garcia; P Colombo; F Buc; S Essid; C Clavel;  Acl; C Gardent; A Shimorina; S Narayan; L Perez-Beltrachini"}, {"ref_id": "b23", "title": "Testing for significance of increased correlation with human judgment", "journal": "", "year": "2014", "authors": "Y Graham; T Baldwin"}, {"ref_id": "b24", "title": "On calibration of modern neural networks", "journal": "", "year": "2017", "authors": "C Guo; G Pleiss; Y Sun; K Q Weinberger"}, {"ref_id": "b25", "title": "Neue begr\u00fcndung der theorie quadratischer formen von unendlichvielen ver\u00e4nderlichen", "journal": "Journal f\u00fcr die reine und angewandte Mathematik", "year": "1909", "authors": "E Hellinger"}, {"ref_id": "b26", "title": "Toward finely differentiated evaluation metrics for machine translation", "journal": "", "year": "1999", "authors": "E H Hovy"}, {"ref_id": "b27", "title": "Heavy-tailed Representations, Text Polarity Classification & Data Augmentation", "journal": "", "year": "2020", "authors": "(*) Jalalzai; H Colombo(*); P Clavel; C Gaussier; E Varni; G Vignon; E Sabourin; A "}, {"ref_id": "b28", "title": "A new measure of rank correlation", "journal": "Biometrika", "year": "1938", "authors": "M G Kendall"}, {"ref_id": "b29", "title": "Statistical machine translation", "journal": "Cambridge University Press", "year": "2009", "authors": "P Koehn"}, {"ref_id": "b30", "title": "CDER: Efficient MT Evaluation Using Block Movements", "journal": "ACL", "year": "2006", "authors": "M Kusner; Y Sun; N Kolkin; K Weinberger; G Leusch; N Ueffing; H Ney"}, {"ref_id": "b31", "title": "A novel string-to-string distance measure with applications to machine translation evaluation", "journal": "", "year": "2003", "authors": "G Leusch; N Ueffing; H Ney"}, {"ref_id": "b32", "title": "R\\'enyi Divergence Variational Inference", "journal": "", "year": "2016", "authors": "Y Li; R E Turner"}, {"ref_id": "b33", "title": "ROUGE: A Package for Automatic Evaluation of Summaries", "journal": "ACL", "year": "2004", "authors": "C.-Y Lin"}, {"ref_id": "b34", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Y Liu; M Ott; N Goyal; J Du; M Joshi; D Chen; O Levy; M Lewis; L Zettlemoyer; V Stoyanov"}, {"ref_id": "b35", "title": "Blend: a novel combined MT metric based on direct assessment-casict-dcu submission to WMT17 metrics task", "journal": "", "year": "2017", "authors": "Q Ma; Y Graham; S Wang; Q Liu"}, {"ref_id": "b36", "title": "Phrase-based statistical language generation using graphical models and active learning", "journal": "", "year": "2010", "authors": "F Mairesse; M Gasic; F Jurcicek; S Keizer; B Thomson; K Yu; S Young"}, {"ref_id": "b37", "title": "Automatic summarization", "journal": "John Benjamins Publishing", "year": "2001", "authors": "I Mani"}, {"ref_id": "b38", "title": "Overview of the TAC 2009 knowledge base population track", "journal": "", "year": "2009", "authors": "P Mcnamee; H T Dang"}, {"ref_id": "b39", "title": "Precision and recall of machine translation", "journal": "", "year": "2003", "authors": "I D Melamed; R Green; J Turian"}, {"ref_id": "b40", "title": "Evaluation in the context of natural language generation", "journal": "Computer Speech & Language", "year": "1998", "authors": "C Mellish; R Dale"}, {"ref_id": "b41", "title": "Efficient Estimation of Word Representations in Vector Space", "journal": "", "year": "2013", "authors": "T Mikolov; K Chen; G Corrado; J Dean"}, {"ref_id": "b42", "title": "Abstractive text summarization using sequence-to-sequence rnns and beyond", "journal": "", "year": "2004", "authors": "R Nallapati; B Zhou; C Gulcehre; B Xiang"}, {"ref_id": "b43", "title": "Better summarization evaluation with word embeddings for rouge", "journal": "", "year": "2015", "authors": "J.-P Ng; V Abrecht"}, {"ref_id": "b44", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "journal": "", "year": "2002", "authors": "K Papineni; S Roukos; T Ward; W.-J Zhu"}, {"ref_id": "b45", "title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "M E Peters; M Neumann; M Iyyer; M Gardner; C Clark; K Lee; L Zettlemoyer"}, {"ref_id": "b46", "title": "Studying summarization evaluation metrics in the appropriate scoring range", "journal": "", "year": "2019", "authors": "M Peyrard"}, {"ref_id": "b47", "title": "Adversarial Robustness via Fisher-Rao Regularization", "journal": "", "year": "2021", "authors": "M Picot; F Messina; M Boudiaf; F Labeau; I Ben Ayed; P Piantanida"}, {"ref_id": "b48", "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "journal": "Advances in large margin classifiers", "year": "1999", "authors": "J Platt"}, {"ref_id": "b49", "title": "Language Models are Unsupervised Multitask Learners", "journal": "OpenAI Blog", "year": "2019", "authors": "A Radford; J Wu; R Child; D Luan; D Amodei; I Sutskever"}, {"ref_id": "b50", "title": "Differential metrics in probability spaces", "journal": "", "year": "1987", "authors": "C R Rao"}, {"ref_id": "b51", "title": "Alpha-beta divergence for variational inference", "journal": "", "year": "2018", "authors": "J.-B Regli; R Silva"}, {"ref_id": "b52", "title": "An investigation into the validity of some metrics for automatically evaluating natural language generation systems", "journal": "Computational Linguistics", "year": "2009", "authors": "E Reiter; A Belz"}, {"ref_id": "b53", "title": "On measures of entropy and information", "journal": "", "year": "1961", "authors": "A R\u00e9nyi"}, {"ref_id": "b54", "title": "Dis-tilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "journal": "", "year": "2019", "authors": "V Sanh; L Debut; J Chaumond; T Wolf"}, {"ref_id": "b55", "title": "A study of translation edit rate with targeted human annotation", "journal": "", "year": "2006", "authors": "M Snover; B Dorr; R Schwartz; L Micciulla; J Makhoul"}, {"ref_id": "b56", "title": "Quality estimation for machine translation", "journal": "Synthesis Lectures on Human Language Technologies", "year": "2018", "authors": "L Specia; C Scarton; G H Paetzold"}, {"ref_id": "b57", "title": "Depth-based pseudo-metrics between probability distributions. arXiv", "journal": "", "year": "2021", "authors": "G Staerman; P Mozharovskyi; S Cl\u00e9men\u00e7on; F Buc"}, {"ref_id": "b58", "title": "EED: Extended edit distance measure for machine translation", "journal": "", "year": "2019", "authors": "P Stanchev; W Wang; H Ney"}, {"ref_id": "b59", "title": "Tests for comparing elements of a correlation matrix", "journal": "", "year": "1980", "authors": "J H Steiger; R Vedantam; C Lawrence Zitnick; D Parikh"}, {"ref_id": "b60", "title": "Character: Translation edit rate on character level", "journal": "", "year": "2016", "authors": "W Wang; J.-T Peter; H Rosendahl; H Ney"}, {"ref_id": "b61", "title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "journal": "EMNLP", "year": "2015", "authors": "T.-H Wen; M Gasic; N Mrksic; P.-H Su; D Vandyke; S Young"}, {"ref_id": "b62", "title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2019", "authors": "T Zhang; V Kishore; F Wu; K Q Weinberger; Y Artzi"}, {"ref_id": "b63", "title": "Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance", "journal": "EMNLP", "year": "2019", "authors": "W Zhao; M Peyrard; F Liu; Y Gao; C M Meyer; S Eger"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Various trained metrics have been proposed such as BEER, BEND, RUSE, CIDER (Vedantam, Lawrence Zitnick, and Parikh 2015). These methods rely on train/dev/test sets composed of human evaluations. InfoLM does not require any training step and relies on a frozen PMLM.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "However, pairwise comparisons of all the pairs of individual masked contexts are prohibitively expensive (O(L \u00d7 M ) comparisons) when considering long texts. Motivated by efficiency, we instead propose to work with two \"global views\" of the sentences that are well-formed probability distributions and are obtained through the aggregation of individual PMLM predictions. Aggregated probabilities for x x x and y y y are denoted p \u2126|T (\u2022|x x x; \u03b8; T ) and p \u2126|T (\u2022|y y y; \u03b8; T ) respectively. Definition 3.2 (Similarity for texts). Given I, two texts x x x, y y y are said to be similar (denoted x x x I \u2248 y y y) if I p \u2126|T (\u2022|x x x; \u03b8; T ), p \u2126|T (\u2022|y y y; \u03b8; T ) \u2248 0 where p \u2126|T (\u2022|x x x; \u03b8; T ) and p \u2126|T (\u2022|y y y; \u03b8; T ) denotes the aggregated individual masked context predictions.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "3. 22InfoLMOverview InfoLM uses the notion of similarity given in 3.2. Given a reference text x ix ix i together with a candidate text y InfoLM recursively masks each token position of bothx i x i x iand y s i y s i y s i to obtain individual masked contexts. By relying on a PMLM, InfoLM predicts one distribution for each individual masked contexts. The resulting distributions are then averaged (we refer to this operation \"bag of distributions\") to obtain p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) and p \u2126|T (\u2022|x i x i x i ; \u03b8; T ). The final step involves comparing two well formed discrete probability distributions p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) and p \u2126|T (\u2022|x i x i x i ; \u03b8; T ) through a measure of information I. InfoLM writes as: InfoLM(xi xi xi, y s i y s i y s i ) I p \u2126|T (\u2022|xi xi xi; \u03b8; T ), p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) . (3) Remark. It is worth to emphasize that p \u2126|T (\u2022|x i x i x i ; \u03b8; T ) and p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) are two well formed discrete probability distributions. They represent the probability of observing each token of the vocabulary given the candidate and the reference sentence, respectively. Aggregation Procedure Rare tokens can be more indicative of text similarity than common tokens (Banerjee and Lavie 2005). Thus, for the aggregation of the individual masked contexts, we propose to compute a weighted \"bag of distributions\" where the weights are normalized measures of the importance of each token. In practice, p \u2126|T (\u2022|x x x; \u03b8; T ) and p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) write as:", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "DFigure 1 :1Figure1: Results of the correlation between metrics and human judgments on the CNN dataset. First raw reports correlation as measured by the Person (r) and second raw focus on Kendall (\u03c4 ) coefficient. In this experiment, parameter are optimized for each criterion.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Impact of Calibration for summarization.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Expression of the divergences (upper group) and distance between two positives measures p = (p 1 , \u2022 \u2022 \u2022 , p N ) and q = (q 1 , \u2022 \u2022 \u2022 , q N ) as well as the definition domain (see(Regli and Silva 2018)). We omit the index in the summations.", "figure_data": "are unlikely to be representative of the progress of recentNLG systems we instead rely on a different dataset com-ing from the WebNLG2020 challenge (Gardent et al. 2017).Given the following example of triple: (John Blaha birth-Date 1942 08 26) (John Blaha birthPlace San Antonio)(John E Blaha job Pilot) the goal is to generate John Blaha,born in San Antonio on 1942-08-26, worked as a pilot.Annotations. The WebNLG task is evaluated by human an-notators along four different axes: (1) Data Coverage: Areall the descriptions presented in the data included in thetext? (2) Relevance: Does the text contains only predicatesfound in the data? (3) Correctness: Are predicates found inthe data correctly mentioned and adequately introduced? (4)Text structure: Is the produced text well-structured, gram-matically correct and written in acceptable English?, (5) Flu-ency: Does the text progress naturally? Is it easy to under-stand? Is it a coherent whole?"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Correlation at the system level with human judgement along five different axis: correctness, data coverage, fluency, relevance and text structure for the WebNLG task. Best results by group are underlined, overall best results are bolted.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "NLG evaluation. Given a dataset D = {x i x i x i , {y s i y s i y s i , h(x i x i x i , y s i y s i y s i )} S s=1 } N i=1", "formula_coordinates": [2.0, 54.0, 327.57, 238.5, 28.24]}, {"formula_id": "formula_1", "formula_text": "x i x i x i = (x 1 , \u2022 \u2022 \u2022 , x M ) is composed of M tokens (e.g., words or subwords) and y s i y s i y s i = (y s 1 , \u2022 \u2022 \u2022 , y s L", "formula_coordinates": [2.0, 53.56, 382.36, 238.95, 28.24]}, {"formula_id": "formula_2", "formula_text": "x i x i x i , y s i y s i y s i ) \u2208 R +", "formula_coordinates": [2.0, 210.01, 415.46, 56.64, 18.22]}, {"formula_id": "formula_3", "formula_text": "C t,f 1 N N i=1 K(F t i , H t i ),(1)", "formula_coordinates": [2.0, 119.53, 589.92, 172.97, 30.32]}, {"formula_id": "formula_4", "formula_text": "F i = f (x i x i x i , y 1 i y 1 i y 1 i ), \u2022 \u2022 \u2022 , f (x i x i x i , y S i y S i y S i ) and H i = h(x i x i x i , y 1 i y 1 i y 1 i ), \u2022 \u2022 \u2022 , h(x i x i x i , y S i y S i y S i )", "formula_coordinates": [2.0, 58.15, 628.23, 234.35, 31.36]}, {"formula_id": "formula_5", "formula_text": "F sy = 1 N N i=1 f (x i x i x i , y 1 i y 1 i y 1 i ), . . . , 1 N N i=1 f (x i x i x i , y S i y S i y S i ) H sy = 1 N N i=1 h(x i x i x i , y 1 i y 1 i y 1 i ), . . . , 1 N N i=1 h(x i x i x i , y S i y S i y S i ) ,", "formula_coordinates": [2.0, 340.37, 113.29, 201.95, 65.27]}, {"formula_id": "formula_6", "formula_text": "[x x x] j I \u223c [y y y] k ) if the two predicted discrete distributions given by the PMLM, namely p \u2126|T (\u2022|[x x x] j ; \u03b8; T ) and p \u2126|T (\u2022|[y y y] k ; \u03b8; T ), are similar. Formally, [x x x] j I \u223c [y y y] k if I p \u2126|T (\u2022|[x x x] j ; \u03b8; T ), p \u2126|T (\u2022|[y y y] k ; \u03b8; T ) \u2248 0.", "formula_coordinates": [3.0, 54.0, 544.75, 238.5, 59.89]}, {"formula_id": "formula_7", "formula_text": "p \u2126|T (\u2022|x x x; \u03b8; T ) M k=1 \u03b3 k \u00d7 p \u03b8 (\u2022|[x x x] k ; \u03b8; T ), p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) N k=1\u03b3 k \u00d7 p \u03b8 (\u2022|[y s i y s i y s i ] k ; \u03b8; T ),", "formula_coordinates": [3.0, 346.69, 480.29, 184.13, 65.73]}, {"formula_id": "formula_8", "formula_text": "x i x i ; \u03b8; T ) = [p 0 , \u2022 \u2022 \u2022 , p |\u2126| ] and p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) = [q 0 , \u2022 \u2022 \u2022 , q |\u2126| ]. First, let us con- sider two texts x i x i x i , y s i y s i y s i such that x i x i x i I \u2248 y s i y s i y s i . InfoLM with L \u221e is closed to 0 if \u2200i \u2208 [1, |\u2126|] p i \u2248 q i .", "formula_coordinates": [4.0, 319.5, 190.46, 238.5, 57.36]}, {"formula_id": "formula_9", "formula_text": "y s i y s i . Second, let us consider x i x i x i , y s i y s i y s i such that I p \u2126|T (\u2022|x i x i x i ; \u03b8; T ), p \u2126|T (\u2022|y s i y s i y s i ; \u03b8; T ) \u2248 1 (dissimilar texts", "formula_coordinates": [4.0, 319.5, 273.44, 238.5, 30.28]}, {"formula_id": "formula_10", "formula_text": "Name Notation Domain Expression \u03b1-divergence (Csisz\u00e1r 1967) D \u03b1 \u03b1 \u2208 {0, 1} 1 \u03b1(\u03b1\u22121) (1 \u2212 q 1\u2212\u03b1 i p \u03b1 i ) \u03b3 divergence (Fujisawa and Eguchi 2008) D \u03b2 \u03b3 \u03b2 \u2208 {0, \u22121} 1 \u03b2(\u03b2+1) log p \u03b2+1 i + 1 \u03b2+1 log q \u03b2+1 i \u2212 1 \u03b2 log p i q \u03b2 i AB Divergence", "formula_coordinates": [5.0, 100.77, 57.46, 422.95, 57.86]}, {"formula_id": "formula_11", "formula_text": ") D \u03b1,\u03b2 sAB (\u03b1, \u03b2) \u2208 (R * ) 2 \u03b2 + \u03b1 = 0 1 \u03b2(\u03b2+\u03b1) log p \u03b2+\u03b1 i + 1 \u03b2+\u03b1 log q \u03b2+\u03b1 i \u2212 1 \u03b2 log p \u03b1 i q \u03b2 i L 1 distance L 1 |p i \u2212 q i | L 2 distance L 2 (p i \u2212 q i ) 2 L \u221e distance L \u221e max i |p i \u2212 q i | Fisher-Rao distance R 2 \u03c0 arccos \u221a p i \u00d7 q i", "formula_coordinates": [5.0, 115.21, 106.35, 411.64, 65.78]}, {"formula_id": "formula_12", "formula_text": "D AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS", "formula_coordinates": [6.0, 74.04, 143.26, 52.58, 9.22]}, {"formula_id": "formula_13", "formula_text": "D AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS", "formula_coordinates": [6.0, 74.04, 266.16, 52.58, 9.22]}, {"formula_id": "formula_14", "formula_text": "D AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS Ber tS Mo verS BLE U R-1 R-2 R-W E 0.5 0.6 0.7 0.8 (f) Ext -Text D AB D \u03b1 D \u03b2 L 1 L 2 L \u221e R JS", "formula_coordinates": [6.0, 191.62, 210.59, 187.53, 87.71]}], "doi": ""}