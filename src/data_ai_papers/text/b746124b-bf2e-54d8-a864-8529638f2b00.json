{"title": "When Does Translation Require Context? A Data-driven, Multilingual Exploration", "authors": "Patrick Fernandes; Kayo Yin; Emmy Liu; Andr\u00e9 F T Martins; Graham Neubig", "pub_date": "", "abstract": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MUDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. 1   ", "sections": [{"heading": "Introduction", "text": "In order to properly translate discourse phenomena including anaphoric pronouns, lexical cohesion, and discourse markers, a machine translation (MT) model must use information from previous utterances (Guillou et al., 2018;L\u00e4ubli et al., 2018;Toral et al., 2018).\nHowever, while generating proper translations of these phenomena is important for comprehension, they represent a small portion of words in natural language. Therefore, common metrics such as BLEU (Papineni et al., 2002) cannot be used to judge the quality of discourse translation. Table 1: Some representative works on contextual machine translation that perform evaluation on discourse phenomena, contrasted to our work. For a more complete review see Maruf et al. (2021).\nRecent work on neural machine translation (NMT) models that attempt to incorporate extrasentential context (Tiedemann and Scherrer, 2017;Miculicich et al., 2018;Maruf and Haffari, 2018, inter alia) often perform targeted evaluation of certain discourse phenomena, mostly focusing on ellipsis, formality (Voita et al., 2019b,a), and pronoun translation (M\u00fcller et al., 2018;Bawden et al., 2018;Lopes et al., 2020). However, only a limited set of discourse phenomena for a few language pairs have been studied (see summary in Table 1). The difficulty of broadening these studies stems from the reliance of previous work on introspection and domain knowledge to identify the relevant discourse phenomena, frequently involving expert speakers, which then requires engineering complex language-specific methods to create test suites or manually designing data for evaluation.\nIn this paper, we identify sentences that contain discourse phenomena through a data-driven, semiautomatic methodology. We apply this method to create a multilingual benchmark testing discourse phenomena in the domain of MT. First, we develop P-CXMI ( \u00a72) as a metric to identify when context is helpful in MT, or more broadly text generation in general. Then, we perform a systematic analysis of words with high P-CXMI to find categories of trans-lations where context is useful ( \u00a73). We identify novel discourse phenomena that to our knowledge have not been addressed previously (e.g. consistency of verb forms), without requiring a-priori language-specific knowledge. Finally, we design a series of methods to automatically tag words belonging to the identified classes of ambiguities ( \u00a74) and we evaluate existing translation models for different categories of ambiguous translations ( \u00a75).\nWe examine a parallel corpus spanning 14 language pairs, measuring translation ambiguity and model performance. We find that the contextaware methods, while improving on standard evaluation metrics, only perform significantly better than context-agnostic baselines for certain discourse phenomena in our benchmark. Our benchmark provides a more fine-grained evaluation of translation models and reveals weaknesses of context-aware models, such as verb form cohesion. We also find that DeepL, a commercial document-level translation system, does better in our benchmark than its sentence-level ablation and Google Translate. We hope that the released benchmark and code, as well as our findings, will spur targeted evaluation of discourse phenomena in MT to cover more languages and more phenomena in the future.", "publication_ref": ["b12", "b17", "b32", "b27", "b21", "b31", "b22", "b20", "b24", "b1", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Measuring Context Usage", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-Mutual Information", "text": "Past work on contrastive evaluation has examined correct and incorrect translations of specific discourse phenomena (Bawden et al., 2018;M\u00fcller et al., 2018), but this provides only a limited measure of context usage on phenomena defined by the creators of the dataset. We are therefore interested in devising a metric that is able to capture all context usage by a model, beyond a predefined set.\nConditional Cross-Mutual Information (CXMI) (Bugliarello et al., 2020; measures the influence of context on model predictions at the corpus level. CXMI is defined as:\nCXMI(C \u2192 Y |X) = H q M T A (Y |X) \u2212 H q M T C (Y |X, C),\nwhere X and Y are a source and target sentence, respectively, C is the context, H q M T A is the entropy of a context-agnostic MT model, and H q M T C refers to a context-aware MT model. This quantity can be estimated over a held-out set with N sentence pairs and their respective context as:\nCXMI(C \u2192 Y |X) \u2248 \u2212 1 N N i=1 log q M T A (y (i) |x (i) ) q M T C (y (i) |x (i) , C (i) )\nImportantly, the authors find that training a single model q M T as both the context-agnostic and context-aware model ensures that non-zero CXMI values are due to context and not other factors (see  and \u00a73.1 for details).\nAlthough this approach is promising, it is defined only at a corpus level: as the previous equation shows, CXMI is estimated by over a full set of sentences. Since we are interested in measuring how important context is for single sentences or words within a sentence, we extend this definition to capture lower-level context dependency in the next section.", "publication_ref": ["b1", "b24", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Context Usage Per Sentence and Word", "text": "Pointwise Mutual Information (P-MI) (Church and Hanks, 1990) measures the association between two random variables for specific outcomes. Mutual information can be seen as the expected value of P-MI over all possible outcomes of the variables.\nTaking inspiration from this, we define the Pointwise Cross-Mutual Information (P-CXMI) for a source, target, context triplet (x, y, C) as:\nP-CXMI(y, x, C) = \u2212 log q M T A (y|x) q M T C (y|x, C)\nIntuitively, P-CXMI measures how much more (or less) likely a target sentence y is when it is given context C, compared to not being given that context. Note that this is estimated according to the models q M T A and q M T C since, just like CXMI, this measure depends on their learned distributions.\nWe can also apply P-CXMI at the word level to measure how much more likely a particular word in a sentence is when it is given the context, by leveraging the auto-regressive property of the neural decoder. Given the triplet (x, y, C) and the word index i, we can measure the P-CXMI for that particular word as:\nP-CXMI(i, y, x, C) = \u2212 log q M T A (y i |y t<i , x) q M T C (y i |y t<i , x, C)\nNote that nothing constrains the form of C or even x and P-CXMI can, in principle, be applied to any conditional language modelling problem.\nAvelile's mother had HIV virus. Avelile had the virus, she was born with the virus. Our tools today don't look like shovels and picks. They look like the stuff we walk around with. Pronouns As ferramentas de hoje n\u00e3o se parecem com p\u00e1s e picaretas. Elas se parecem com as coisas que usamos.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Lexical Cohesion", "text": "Louis XIV had a lot of people working for him. They made his silly outfits, like this.\nVerb Form Luis XIV ten\u00eda un mont\u00f3n de gente trabajando para \u00e9l. Ellos hac\u00edan sus trajes tontos, como \u00e9ste.\nThey're the ones who know what society is going to be like in another generation. I don't.\nEllipsis Ancak onlar ba\u015fka bir nesilde toplumun nas\u0131l olacag\u0131n\u0131 biliyorlar. Ben bilmiyorum.\nTable 2: Examples of high P-CXMI tokens and corresponding linguistic phenomena. Contextual sentences are italicized. The high P-CXMI target token is highlighted in pink, source and contextual target tokens related to the high P-CXMI token are highlighted in blue and green respectively.\nWe use this metric to find words that are strongly context-dependent, which is to say that their likelihood increases greatly with context relative to other words. These words are the ones that likely correspond to discourse phenomena.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Which Translation Phenomena Benefit from Context?", "text": "To identify salient translation phenomena that require context, we perform a thematic analysis (Braun and Clarke, 2006), examining words with high P-CXMI across different language pairs and manually identifying patterns and categorizing them into phenomena where context is useful for translation.\nTo do so, we systematically examined (1) the mean P-CXMI per part-of-speech (POS) tag, (2) the words with the highest mean P-CXMI across the corpus, and (3) the individual words with the highest P-CXMI in a particular sentence.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Data & Model", "text": "To compare linguistic phenomena that arise during document-level translation across language pairs, we use a dataset consisting of TED talks' transcripts and translations (Qi et al., 2018). We use this dataset due to its abundance of discourse phenomena, as well as its availability across many parallel languages. We study translation between English and Arabic, German, Spanish, French, Hebrew, Italian, Japanese, Korean, Dutch, Portuguese, Romanian, Russian, Turkish and Mandarin Chinese. These 14 target languages are chosen for their high availability of TED talks and linguistic tools, as well as for the diversity of language types in our comparative study (Table 4 in Appendix B). For each language pair, our dataset contains 113,711 parallel training sentences from 1,368 talks, 2,678 development sentences from 41 talks, and 3,385 testing sentences from 43 talks.\nTo obtain the P-CXMI for words in the data, we train a small Transformer (Vaswani et al., 2017) model for every target language and incorporate the target context by concatenating it with the current target sentence (Tiedemann and Scherrer, 2017). We train the model with dynamic context size , by sampling 0-3 target context sentences and estimating P-CXMI by using this model for q M T A and q M T C (details in Appendix G).", "publication_ref": ["b29", "b33", "b31"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Analysis Procedure", "text": "We start our analysis by studying POS tags with high mean P-CXMI. In Appendix C, we report the mean P-CXMI for selected POS tags on test data. Some types of ambiguity, such as dual form pronouns ( \u00a73.3), can be linked to a single POS tag and be identified at this step, whereas others require finer inspection.\nNext, we inspect the vocabulary items with high mean P-CXMI. At this step, we can detect phenomena that are reflected by certain lexical items that consistently benefit from context for translation.\nFinally, we examine individual tokens that obtain the highest P-CXMI. In doing so, we identify patterns that do not depend on lexical features, but rather on syntactic constructions for example. In Table 2, we provide selected examples of tokens that have high P-CXMI and the discourse phenomenon we have identified from them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Identified Phenomena", "text": "Through our thematic analysis of items with high P-CXMI, we identified various types of translation ambiguity. Unlike previous work, our method requires no prior knowledge of languages and easily scales to new languages ( \u00a74.4).\nAlthough this procedure may find phenomena that are intuitive to the annotators, the data-driven approach makes confirmation bias less severe than works relying on introspection. Hence, our procedure can allow us to discover relevant phenomena that have not been previously addressed, such as verb forms. Examples of each phenomenon are given in Table 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lexical Cohesion", "text": "Entities may have multiple possible translations in the target language, but the same entity should be referred to by the same word in a translated document. This is called lexical cohesion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Formality", "text": "We identify two phenomena which fall under the general category of formality. First, several languages we examined have a T-V distinction (Appendix B, \"Pronouns Politeness\") in which the second-person pronouns a speaker uses to refer to someone depend on the relationship between the speaker and the addressee.\nSecond, languages such as Japanese and Korean use honorifics to indicate formality, which are special titles or words expressing courtesy or respect for position.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Pronoun Choice", "text": "Unlike in English, many languages use gendered pronouns for pronouns other than the third-person singular, or assign gender based on formal rules rather than semantic ones. In order to assign the correct pronoun, it is therefore necessary to use the previous context to distinguish the grammatical gender of the antecedent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Verb Form", "text": "While English verbs may have five forms 2 , other languages may have a more fine-grained verb morphology. For example, English has only a single form for the past tense, while the Spanish past tense consists of six verb forms. Verbs must be translated using the verb form that reflects the tone, mood and cohesion of the document.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ellipsis", "text": "Ellipsis refers to the omission of superfluous words that are able to be inferred from the context. For instance, in the last row of Table 2, the English text does not repeat the verb know in the second sentence as it can be understood from the previous sentence. However, in Turkish, there is no natural way to translate the verb-phrase ellipsis, so context is important for translating the verb correctly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Cross-phenomenon MT Evaluation", "text": "Next, we we develop a series of methods to automatically tag tokens belonging to these classes of ambiguous translations and propose the Multilingual Discourse-Aware (MuDA) benchmark for context-aware MT models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MT Evaluation Framework", "text": "Given a pair of parallel source and target documents (X, Y ), our MuDA tagger assigns one or more tags from a set of discourse phenomena\n{t 1 i , \u2022 \u2022 \u2022 , t n i } to each target token y i \u2208 Y .\nUsing the compare-mt toolkit (Neubig et al., 2019), we compute the mean word f-measure of system outputs compared to the reference for each tag. This allows us to identify which discourse phenomena models can translate more or less accurately. ", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Automatic Tagging", "text": "We now describe our taggers for each identified discourse phenomenon. Note that these do not require C-XMI to be calculated, and are based on reliable methods for identifying each phenomenon mentioned in subsection 3.3. For formality, pronoun choice and verb form, we created language-specific word lists that were verified by native speakers. Not all phenomena are present in each language. Phenomena that are absent are indicated in Appendix D, as a zero count for that language. Lexical Cohesion To tag words that require lexical cohesion, we first extract word alignments from a parallel corpus\nD = {(X 1 , Y 1 ), \u2022 \u2022 \u2022 , (X |D| , Y |D| )},\nwhere (X m , Y m ) denote the source and target reference document pair. We use the AWESOME aligner (Dou and ) to obtain:\nA m = {\u27e8x i , y j \u27e9 | x i \u2194 y j , x i \u2208 X m , y j \u2208 Y m },\nwhere each x i and y j are the lemmatized content source and target words and \u2194 denotes a bidirectional word alignment. For each target word y j that is aligned to source word x i , if the alignment pair \u27e8x i , y j \u27e9 occurred at least 3 times already in the current document, excluding the current sentence, we tag y j for lexical cohesion 3 . Formality For languages with T-V distinction, we tag the target pronouns containing formality distinction if there has previously been a word pertaining to the same formality level in the same document.\nSome languages such as Spanish often drop the subject pronoun, and T-V distinction is instead reflected in the verb form. For these languages, we use spaCy (Honnibal and Montani, 2017) and Stanza (Qi et al., 2020) to find POS tags and detect verbs with a second-person subject in the source, and conjugated in the second (T) or third (V) person in the target.\nFor languages with more complex honorifics systems, such as Japanese, we construct a word list of common honorifics-related words to tag (details in Appendix F.3). Pronoun Choice To find pronouns in English that have multiple translations, we manually construct a list P \u2113 = {\u27e8p s , p t \u27e9} for each language (Appendix F.2), where each p s is an English pronoun and p t the list of possible translations of p s in the language \u2113. Then, for each aligned token pair \u27e8x i , y j \u27e9, if x i , y j are both pronouns with \u27e8x i , p t |y j \u2208 p t \u27e9 \u2208 P \u2113 , and the antecedent of x i is not in current sentence, we tag y j as an ambiguous pronoun. To obtain antencedents, we use AllenNLP (Gardner et al., 2017)'s coreference resolution module. This procedure is similar to M\u00fcller et al. (2018). Verb Form For each target language, we define a list V \u2113 = {v 1 , \u2022 \u2022 \u2022 , v k } of verb forms (Appendix F.3) where v i \u2208 V \u2113 if there exists a verb form in English u j and an alternate verb form v k \u0338 = v i in the target language such that an English verb with form u j may be translated to a target verb with form v i or v k depending on the context. Then, for each target token y j , if y j is a verb of form v j \u2208 V \u2113 , and another verb with form v j has appeared previously in the same document, we tag y j as ambiguous. Ellipsis To detect translation ambiguity due to VP and NP ellipsis, we look for instances where the ellipsis occurs on the source side, but not on the target side, which means that the ellipsis must be resolved during translation. Since existing ellipsis models are limited to specific types of ellipsis, we first train an English (source-side) ellipsis detection model. To do so, we extract an ellipsis dataset from the English data in the Penn Treebank (Marcus et al., 1993) and train a BERT text classification model (Devlin et al., 2019), which achieves 0.77 precision and 0.73 recall (see Appendix F.4 for training details). Then, for each sentence pair where the source sentence is predicted to contain an ellipsis, we tag the word y j in the target sentence Y m if: (1) y j is a verb, noun, proper noun or pronoun; (2) y j has occurred in the previous target sentences of the same document; (3) y j is not aligned to any source words, that is, \u0338 \u2203\nx i \u2208 X m s.t. \u27e8x i , y j \u27e9 \u2208 A m .", "publication_ref": ["b14", "b28", "b11", "b24", "b19", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation of Automatic Tags", "text": "We apply the MuDA tagger to the reference translations of our TED talk data. We thus obtain an evaluation set of 3,385 parallel sentences for each of the 14 language pairs. In Appendix C we report the mean P-CXMI for each language and MuDA tag. Overall, we find higher P-CXMI on tokens with a tag compared to those without, which provides empirical evidence that models indeed rely on context to predict words with MuDA tags.\nAppendix D shows that the frequency of tags varies significantly across languages. Overall, only 4.5% of the English sentences have been marked for ellipsis, giving an upper bound for the number of ellipsis tags in other languages. We find that languages from a different family than English have a relatively high number of ellipsis tags. We also find that Korean and especially Japanese have more formality tags than languages with T-V distinction, which reflects that register is more often important when translating to languages with honorifics. Manual Evaluation To evaluate our tagger, we asked native speakers with computational linguistics backgrounds to manually verify MuDA tags for 8 languages on 50 randomly selected utterances as well as all words tagged with ellipsis in our corpus. This allows us to measure how many automatic  tags violate the given definition of the linguistic tag. Table 3 reports the tags' precision 4 . For all languages, we obtain high precision for all tags except ellipsis, confirming that the methodology can scale to languages where no native speakers were involved in developing the tags. For ellipsis, false positives often come from one-to-many or non-literal translations, where the aligner does not align all target words to the corresponding source word. We believe that the ellipsis tagger is still useful in selecting difficult examples that require context for translation; despite the low precision, we find a significantly higher P-CXMI on ellipsis words for many languages (Appendix C). 5", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Extension to New Languages", "text": "While MuDA currently supports 14 language pairs, our methodology can be easily extended to new languages. The lexical and ellipsis tags can be directly applied to other languages provided a word aligner between English and the new target language. The formality tag can be extended by adding a list of pronouns or verb forms related to formality in the new language. Similarly, the pronouns and verb forms tag can also be extended by providing a list of ambiguous pronouns and verb forms.\nExhaustively listing all relevant phenomena in document-level MT is extremely complex and beyond the scope of our paper. To identify new discourse phenomena on other languages, our thematic analysis can be reused as follows: (1) Train a model with dynamic context size on translation between the new language pair; (2) Use the model to compute P-CXMI for words in a parallel documentlevel corpus of the language pair; (3) Manually analyze the POS tags, vocabulary items and individual tokens with high P-CXMI; (4) Link patterns of tokens with high P-CXMI to particular discourse phenomena by consulting linguistic resources. 4 Workers were paid 20$/hour. 5 Also note that wrongly assigned tags should also not penalize a system greatly as it should give a low score only if the translation does not match the falsely tagged word.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Exploring Context-aware MT", "text": "Our MuDA tagger can be applied to documents in the supported languages to create benchmarking datasets for discourse phenomena during translation. We use our benchmark of the TED talk dataset enhanced with MuDA tags to perform an exploration of context usage across languages with 4 models, including commercial systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Trained Models", "text": "We train a sentence-level and document-level concatenation-based small transformer (base) for every target language. While conceptually simple, concatenation approaches have been shown to outperform more complex models when properly trained. For the context-aware model, the major difference from \u00a73.1 is that we use a static context size of 3, since we are not using these models to measure P-CXMI. (Lopes et al., 2020).\nTo evaluate stronger models, we additionally train a large transformer model (large) that was pretrained on a large, sentence-level corpora, for German, French, Japanese and Chinese. Further details can be found in Appendix G.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Commercial Models", "text": "To assess if commercially available machine translation engines are able to leverage context and therefore do well in MuDA, we consider two engines: 6 (1) the Google Cloud Translation v2 API.\nIn early experiments, we assessed that this model only does sentence-level translation, but included it due to its widespread usage; (2) the DeepL v2 API. This model advertises its usage of context as part of translations and our experiments confirm this. Early experimentation with other providers (Amazon and Azure) indicated that these are not contextaware so we refrained from evaluating them.\nTo obtain provider translations, we feed the documents into an API request. To re-segment the translation into sentences, we include special marker tokens in the source that are preserved during translation and split the translation on those tokens. We also evaluate a sentence-level version of DeepL where we feed each sentence separately to compare with its document-level counterpart.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results and Discussion", "text": "Figure 2 shows results for base models, trained either without (no-context) or with context, and for the latter with either predicted (context) or reference context (context-gold) during de-coding. Results are reported with respect to standard MT metrics BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020), as well as the MuDA benchmark. The corpus-level metrics BLEU and COMET are calculated over the entire corpus, rather than just the sentences tagged by MuDA.\nFirst, we find that BLEU scores are highest for context-gold models for most language pairs, but context-agnostic models have higher COMET scores. Moreover, in terms of mean word f-measure overall, we do not find significant differences between the three systems. It is therefore difficult to see which system performs the best on documentlevel ambiguities using only corpus-level metrics.\nFor words tagged by MuDA as requiring context for translation, context-aware models often achieve significantly higher word f-measure than contextagnostic models on certain tags such as ellipsis and formality, but not on other tags such as lexical and verb form. This demonstrates how MuDA allows us to clarify which inter-sentential ambiguities context-aware models are able to resolve.\nFor the pretrained large models (Figure 3), context-aware models perform better than the context-agnostic on corpus-level metrics, especially COMET. On words tagged with MuDA, context-aware models generally obtain the high- est f-measure as well, particularly when given reference context, especially on phenomena such as lexical and pronouns, but improvements are less pronounced than on corpus-level evaluation. Among commercial engines (Figure 4), DeepL outperforms Google on most metrics and language pairs. The sentence-level ablation of DeepL performs worse than its document-level system for most MuDA tags.\nCurrent context-aware MT systems translate some inter-sentential discourse phenomena well, but are unable to consistently obtain significant improvements over context-agnostic counterparts on challenging MuDA data. Tables with all results can be found in Appendix H.", "publication_ref": ["b27", "b30"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Related Work", "text": "Several works have worked on measuring the performance of MT models on contextual discourse phenomena. The first example of this was done by Hardmeier et al. (2010), which evaluated automatically the precision and recall of pronoun translation in statistical MT systems. Jwalapuram et al. (2019) proposed evaluating models on pronoun translation based on a pairwise comparison between translations that were generated with and without context, and later Jwalapuram et al. (2020) extended this work to include more languages and phenomena in their automatic evaluation/test set creation. These works rely on prior domain knowledge and intuition to identify context-aware phenomena, whereas we take a systematic, data-driven approach.\nMost works have focused on evaluating performance in discourse phenomena through the use of contrastive datasets. M\u00fcller et al. (2018) automatically create a dataset for anaphoric pronoun resolution to evaluate MT models in EN \u2192 DE. Bawden et al. (2018) manually creates a dataset for both pronoun resolution and lexical choice in EN \u2192 FR. Voita et al. ( , 2019b creates a dataset for anaphora resolution, deixis, ellipsis and lexical cohesion in EN \u2192 RU. However,  suggest that translating and disambiguating between two contrastive choices are inherently different, motivating our approach in measuring direct translation performance.", "publication_ref": ["b13", "b15", "b16", "b24", "b1", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "We investigate types of ambiguous translations where MT models benefit from context using our proposed P-CXMI metric. We perform a datadriven thematic analysis across 14 languages to identify context-sensitive discourse phenomena, some of which (such as verb forms) have not been previously addressed in work on MT. In comparison to previous work, our approach is systematic, extensible, and does not require prior knowledge of the language. Additionally, the P-CXMI metric can be used to identify other context-dependent words in generation. We construct the MuDA benchmark that tags words in parallel corpora and evaluates models on 5 context-dependent phenomena. Our evaluation reveals that context-aware and commercial translation systems achieve small improvements over context-agnostic models on our benchmark, and we encourage further development of models that improve on context-aware translation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "While MuDA relies on set of hand-crafted rules for tagging specific phenomena, these rules might involve the use of other error-prone systems (such as coreference resolution and alignment models) and these errors might be susceptible to problems (such as lack of out-of-domain generalization) that could limit the applicability of our tagger. However, this could be fixed by extending MuDA to use newer and better versions of these systems.\nThe use of F-1 per tag with surface-form matching between reference/translation can also lead to penalizing translations that use context correctly but choose other equivalent words. Nevertheless, this should also be mitigable by extending the scoring method to, for example, match synonyms.\nFinally, the benchmarking of context-aware models might not apply to newer, state-of-the-art translation models, especially if these leverage large language models that were trained on long-context data.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A MuDA Toolkit Usage", "text": "To tag an existing dataset and extract the tags for later use, run the following command:    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C P-CXMI Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Tag Numbers", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Tagging other Document-level Datasets", "text": "We report the number of tags found for two other document-level datasets commonly used in the literature:\n(1) IWSLT-17 (Cettolo et al., 2012) test sets for EN \u2192 DE and EN \u2192 FR and (2) A randomly subsampled portion of the news-commentary dataset for EN \u2192 {AR, DE, ES, FR, NL, PT, RU, ZH} (Barrault et al., 2019). These results can be found respectively in Figure 5 and Figure 6. Table 5: P-CXMI for all POS tags and our ambiguity tags. In the top two rows, CXMI is the average of P-CXMI for each sentence across the corpus, and P-CXMI is the average of P-CXMI over all tokens in the corpus. Per-tag values are the average of P-CXMI for each token with the tag. The 3 highest P-CXMI scores are highlighted in varying intensities of green.\nF Tagger Details", "publication_ref": ["b5", "b0"], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "F.1 Formality Words", "text": "Table 7 gives the list of words related to formality for each target language.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_14"]}, {"heading": "F.2 Ambiguous Pronouns", "text": "Table 8 provides English pronouns and the list of possible target pronouns.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_15"]}, {"heading": "F.3 Ambiguous Verbs", "text": "Table 9 lists verb forms that may require disambiguation during translation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_16"]}, {"heading": "F.4 Ellipsis Classifier", "text": "We train a BERT text classification model (Devlin et al., 2019) on data from the Penn Treebank, where we labeled each sentence containing the tag '*?*' as containing ellipsis (Bies et al., 1995). We obtain 248,596 sentences total, with 2,863 tagged as ellipsis. Then, our model using HuggingFace Transformers (Wolf   et al., 2020). To address the imbalance in labels, we up-weight the loss for samples tagged as ellipsis by a factor of 100.", "publication_ref": ["b7", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "G Training details", "text": "The transformer-small model has hidden size of 512, feedforward size of 1024, 6 layersa and 8 attention heads. The transformer-large model has hidden size of 1024, feedforward size of 4096, 6 layers, 16 attention heads.\nAs in Vaswani et al. (2017), we train using the Adam optimizer with \u03b2 1 = 0.9 and \u03b2 2 = 0.98 and use an inverse square root learning rate scheduler, with an initial value of 10 \u22124 for large model and 5 \u00d7 10 \u22124 for the base and multi models, with a linear warm-up in the first 4000 steps.\nFor the pretrained models we used Paracrawl (Espl\u00e0 et al., 2019) for German and French, JParacrawl (Morishita et al., 2020) for Japanese and the Backtranslated News from WMT2021 for Chinese.\nDue to the sheer number of experiments, we use a single seed per experiment. We base our experiments on the framework Fairseq (Ott et al., 2019).       B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nko \u110c \u1166\u1100 \u1161, \u110c \u1165\u1112 \u1174, \u1102 \u1161 \u1103 \u1162 \u11a8\u110b \u1166, \u1109 \u1165 \u11bc\u1112 \u1161 \u11b7, \u1107 \u116e \u11ab, \u1109 \u1162 \u11bc\u1109 \u1175 \u11ab, \u1109 \u1175 \u11a8\u1109 \u1161, \u110b \u1167 \u11ab\u1109 \u1166, \u1107 \u1167 \u11bc\u1112 \u116a \u11ab, \u110b \u1163 \u11a8\u110c \u116e, \u110c \u1161\u110c \u1166\u1107 \u116e \u11ab, \u1107 \u116c \u11b8\u1103 \u1161, \u110c \u1165 nl jij,\nDatasets used are commonly used by the community, and the (permissive) license for our tagger is in the official code repository B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\nDatasets used are commonly used by the community B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 4\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Section 4\nC Did you run computational experiments?\nSection 5\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 5.1 and Appendix F", "publication_ref": ["b33", "b9", "b23", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We would like to thank Uri Alon, Ipek Baris, George Bejinariu, Hiba Belkadi, Chlo\u00e9 Billiotte, Giovanni Campagna, Remi Castera, Volkan Cirik, Taisiya Glushkova, Junxian He, Mert Inan, Alina Karakanta, Benno Krojer, Emma Landry, Chanyoung Park, Artidoro Pagnoni, Maria Ryskina, Odette Scharenborg, Melanie Sclar, Jenny Seok, Emma Schippers, Bogdan Vasilescu for advice on various languages and help with manual annotations.\nWe would also like to thank all the members of DeepSPIN and NeuLab who provided feedback on ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Findings of the 2019 conference on machine translation (WMT19)", "journal": "", "year": "2019", "authors": "Lo\u00efc Barrault; Ond\u0159ej Bojar; Marta R Costa-Juss\u00e0; Christian Federmann; Mark Fishel; Yvette Graham; Barry Haddow; Matthias Huck; Philipp Koehn; Shervin Malmasi; Christof Monz; Mathias M\u00fcller"}, {"ref_id": "b1", "title": "Evaluating discourse phenomena in neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rachel Bawden; Rico Sennrich; Alexandra Birch; Barry Haddow"}, {"ref_id": "b2", "title": "Bracketing guidelines for treebank ii style penn treebank project", "journal": "", "year": "1995", "authors": "Ann Bies; Mark Ferguson; Karen Katz; Robert Mac-Intyre; Victoria Tredinnick; Grace Kim; Mary Ann Marcinkiewicz; Britta Schasberger"}, {"ref_id": "b3", "title": "Using thematic analysis in psychology", "journal": "Qualitative research in psychology", "year": "2006", "authors": "Virginia Braun; Victoria Clarke"}, {"ref_id": "b4", "title": "It's easier to translate out of English than into it: Measuring neural translation difficulty by crossmutual information", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Emanuele Bugliarello; Sabrina J Mielke; Antonios Anastasopoulos; Ryan Cotterell; Naoaki Okazaki"}, {"ref_id": "b5", "title": "WIT3: Web inventory of transcribed and translated talks", "journal": "European Association for Machine Translation", "year": "2012", "authors": "Mauro Cettolo; Christian Girardi; Marcello Federico"}, {"ref_id": "b6", "title": "Word association norms, mutual information, and lexicography", "journal": "Computational Linguistics", "year": "1990", "authors": "Kenneth Ward Church; Patrick Hanks"}, {"ref_id": "b7", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b8", "title": "Word alignment by fine-tuning embeddings on parallel corpora", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Yi Zi; Graham Dou;  Neubig"}, {"ref_id": "b9", "title": "ParaCrawl: Web-scale parallel corpora for the languages of the EU", "journal": "European Association for Machine Translation", "year": "2019", "authors": "Miquel Espl\u00e0; Mikel Forcada; Gema Ram\u00edrez-S\u00e1nchez; Hieu Hoang"}, {"ref_id": "b10", "title": "Measuring and increasing context usage in context-aware machine translation", "journal": "", "year": "2021", "authors": "Patrick Fernandes; Kayo Yin; Graham Neubig; Andr\u00e9 F T Martins"}, {"ref_id": "b11", "title": "Allennlp: A deep semantic natural language processing platform", "journal": "", "year": "2017", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke S Zettlemoyer"}, {"ref_id": "b12", "title": "A pronoun test suite evaluation of the English-German MT systems at WMT", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Liane Guillou; Christian Hardmeier; Ekaterina Lapshinova-Koltunski; Sharid Lo\u00e1iciga"}, {"ref_id": "b13", "title": "Modelling pronominal anaphora in statistical machine translation", "journal": "", "year": "2010", "authors": "Christian Hardmeier; Marcello Fondazione; Bruno Kessler"}, {"ref_id": "b14", "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing", "journal": "", "year": "2017", "authors": "Matthew Honnibal; Ines Montani"}, {"ref_id": "b15", "title": "Evaluating pronominal anaphora in machine translation: An evaluation measure and a test suite", "journal": "", "year": "2019", "authors": "Prathyusha Jwalapuram; Shafiq Joty; Irina Temnikova; Preslav Nakov"}, {"ref_id": "b16", "title": "Can your contextaware MT system pass the dip benchmark tests? : Evaluation benchmarks for discourse phenomena in machine translation", "journal": "", "year": "2004", "authors": "Prathyusha Jwalapuram; Barbara Rychalska; R Shafiq; Dominika Joty;  Basaj"}, {"ref_id": "b17", "title": "Has machine translation achieved human parity? a case for document-level evaluation", "journal": "", "year": "2018", "authors": "Samuel L\u00e4ubli; Rico Sennrich; Martin Volk"}, {"ref_id": "b18", "title": "Document-level neural MT: A systematic comparison", "journal": "", "year": "2020", "authors": "Ant\u00f3nio Lopes; M Amin Farajian; Rachel Bawden; Michael Zhang; Andr\u00e9 F T Martins"}, {"ref_id": "b19", "title": "Building a large annotated corpus of English: The Penn Treebank", "journal": "Computational Linguistics", "year": "1993", "authors": "Mitchell P Marcus; Beatrice Santorini; Mary Ann Marcinkiewicz"}, {"ref_id": "b20", "title": "Document context neural machine translation with memory networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Sameen Maruf; Gholamreza Haffari"}, {"ref_id": "b21", "title": "A survey on document-level neural machine translation: Methods and evaluation", "journal": "ACM Comput. Surv", "year": "2021", "authors": "Sameen Maruf; Fahimeh Saleh; Gholamreza Haffari"}, {"ref_id": "b22", "title": "Document-level neural machine translation with hierarchical attention networks", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Lesly Miculicich; Dhananjay Ram; Nikolaos Pappas; James Henderson"}, {"ref_id": "b23", "title": "JParaCrawl: A large scale web-based English-Japanese parallel corpus", "journal": "European Language Resources Association", "year": "2020", "authors": "Makoto Morishita; Jun Suzuki; Masaaki Nagata"}, {"ref_id": "b24", "title": "A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Mathias M\u00fcller; Annette Rios; Elena Voita; Rico Sennrich"}, {"ref_id": "b25", "title": "compare-mt: A tool for holistic comparison of language generation systems", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Graham Neubig; Zi-Yi Dou; Junjie Hu; Paul Michel; Danish Pruthi; Xinyi Wang"}, {"ref_id": "b26", "title": "fairseq: A fast, extensible toolkit for sequence modeling", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"ref_id": "b27", "title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"ref_id": "b28", "title": "Stanza: A python natural language processing toolkit for many human languages", "journal": "", "year": "2020", "authors": "Peng Qi; Yuhao Zhang; Yuhui Zhang; Jason Bolton; Christopher D Manning"}, {"ref_id": "b29", "title": "When and why are pre-trained word embeddings useful for neural machine translation?", "journal": "", "year": "2018", "authors": "Ye Qi; Devendra Sachan; Matthieu Felix; Sarguna Padmanabhan; Graham Neubig"}, {"ref_id": "b30", "title": "COMET: A neural framework for MT evaluation", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Ricardo Rei; Craig Stewart; Ana C Farinha; Alon Lavie"}, {"ref_id": "b31", "title": "Neural machine translation with extended context", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "J\u00f6rg Tiedemann; Yves Scherrer"}, {"ref_id": "b32", "title": "Attaining the unattainable? reassessing claims of human parity in neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Antonio Toral; Sheila Castilho; Ke Hu; Andy Way"}, {"ref_id": "b33", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b34", "title": "Context-aware monolingual repair for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Elena Voita; Rico Sennrich; Ivan Titov"}, {"ref_id": "b35", "title": "When a good translation is wrong in context: Contextaware machine translation improves on deixis, ellipsis, and lexical cohesion", "journal": "", "year": "2019", "authors": "Elena Voita; Rico Sennrich; Ivan Titov"}, {"ref_id": "b36", "title": "Context-aware neural machine translation learns anaphora resolution", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Elena Voita; Pavel Serdyukov; Rico Sennrich; Ivan Titov"}, {"ref_id": "b37", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"ref_id": "b38", "title": "Do context-aware translation models pay the right attention", "journal": "", "year": "2021", "authors": "Kayo Yin; Patrick Fernandes; Danish Pruthi; Aditi Chaudhary; F T Andr\u00e9; Graham Martins;  Neubig"}, {"ref_id": "b39", "title": "Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b40", "title": "C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 5.1 and Appendix F", "journal": "", "year": "", "authors": ""}, {"ref_id": "b41", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b42", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b43", "title": "crowdworkers) or research with human participants? Section", "journal": "", "year": "", "authors": ""}, {"ref_id": "b44", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b45", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b46", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b47", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b48", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Section", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Number of MuDA tags on TED test data. Exact numbers of each tag are given in Appendix D. Number of tags for other document-level datasets can be found in Appendix E.", "figure_data": ""}, {"figure_label": "23", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :Figure 3 :23Figure2: Impact of context on BLEU, COMET, and Word f-measure per tag for base context-aware models. BLEU, COMET and word f-measures statistically significantly higher than no-context (p < 0.05) are marked with *. Languages for which the phenomenon doesn't exist are marked with \u2205. BLEU scores are normalized between[0,1]    ", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Scores for commercial models. DeepL (doc) BLEU, COMET and word f-measures statistically significantly higher than DeepL (sent) are marked with *. Languages for which neither DeepL or Google translations are available are marked with \u2205. BLEU scores are normalized between [0,1]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "To evaluate models on a particular dataset (reporting per-tag metrics dicussed in this paper), run the following command:1 python muda/main.py \\ 2 --src /path/to/src \\ 3 --tgt /path/to/tgt \\ 4 --docids /path/to/docids \\ 5 --hyps /path/to/hyps.m1 /path/to/hyps.m2 \\ 6 --tgt-lang lang B Language Properties", "figure_data": ""}, {"figure_label": "56", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :Figure 6 :56Figure 5: Number of tags for EN \u2192 DE and EN \u2192 FR in the IWSLT17 dataset. Lexical cohesion and verb form are common phenomena in this dataset.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Roger. I got'em. Two-Six, this is Two-Six , we're mobile.", "figure_data": "\u963f\u7ef4\u5229\u5c14\u7684\u6bcd\u4eb2\u662f\u643a\u6709\u827e\u6ecb\u75c5\u75c5\u6bd2\u3002 \u963f\u7ef4\u5229\u5c14\u4e5f\u6709\u827e\u6ecb\u75c5\u75c5\u6bd2\u3002\u5979\u4e00\u751f\u4e0b\u6765\u5c31\u6709\u3002Your daughter? Your niece?FormalityVotre fille ? Votre ni\u00e8ce ?(T-V)Formality\u4e86\u89e3 \u6355\u6349\u3057\u305f\u3002 2-6 \u3053\u3061\u3089\u79fb\u52d5\u4e2d\u3060\u3002(Honorifics)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Precision of MuDA tags on 50 utterances.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Properties of the languages in our study.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "summarizes the properties of the languages analyzed in this work.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "presents the average P-CXMI value per POS tag and per MuDA tag.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "lists the counts of each tag per language.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Total number of MuDA tags on TED test data. '0' indicates that the phenomenon does not apply to that language.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "\u307e\u3059, \u3044\u3089\u3063\u3057\u3083\u308c, \u3044\u3089\u3063\u3057\u3083\u3044, \u3054\u89a7, \u4f3a\u3044, \u4f3a\u3063, \u5b58\u77e5, \u3067\u3059, \u307e\u3057", "figure_data": "dedu sieest\u00fa, tu, tus, ti, contigo, tuyo, te, tuya usted, vosotros, vuestro, vuestra, vuestras, osfrtu, ton,ta, tes, toi, te, tien, tiens, tienne, tiennes vous, votre, vosittu, tuo, tua, tuoi lei, suo, sua, suoija\u3054\u3056\u3044,\u3060, \u3060\u3063, \u3058\u3083, \u3060\u308d\u3046, \u3060, \u3060\u3051\u3069, \u3060\u3063H Results Tables"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Words related to formality for each target language.", "figure_data": "youaritthey, themdeiter, sie, esit\u00e9l, ellathey, themellos, ellasesthis that\u00e9sta, \u00e9ste, esto esa, esetheseestos, estasthoseaquellos, aquellas, \u00e9sos, \u00e9sasitil, elle, luithey, themils, elleswenous, onfrthiscelle, cecithatcelle, celuithese, thosecelles, ceuxitesso, essathisquesta, questoitthat thesequella, quello queste, questithosequelle, quellijaI\u79c1, \u50d5, \u4ffaitele, ela, o, athemeles, elas, os, aspttheyeles, elasthis, thateste, esta, esse, essathese, thoseestes, estas, esses, essasroit they, themel, ea ei, ele"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Ambiguous pronouns w.r.t. English for each target language.", "figure_data": "es Imperfect, Pluperfect, FuturefrImperfect, Past, Pluperfecthe Imperfect, Future, Pluperfectit Imperfect, Pluperfect, FuturenlPastptPluperfectroImperfect, Past, FutureruPasttrPluperfect"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Ambiguous verb forms w.r.t. English for each target language.", "figure_data": "ardeesfrheitjakonlptrorutrzhno-context17.2528.0235.7237.7432.7032.307.106.8032.2239.0325.3617.0012.3215.96BLEUcontext16.9228.2436.0037.2332.9232.114.483.7732.6739.1025.3717.1411.9715.01context-gold18.6128.6036.2737.9633.4132.375.966.9232.7339.5528.4917.7012.4916.05no-context0.0002 0.1841 0.3809 0.3087 0.0948 0.2608 -0.5366 -0.0275 0.3105 0.4562 0.3826 0.0033 0.2113 -0.1419COMETcontext-0.0066 0.1846 0.3875 0.2811 0.0887 0.2496 -0.7728 -0.3339 0.3238 0.4444 0.3747 -0.0190 0.1831 -0.1917context-gold 0.0025 0.1886 0.3879 0.2821 0.0922 0.2467 -0.6827 -0.1000 0.3218 0.4506 0.3805 -0.0173 0.1871 -0.1274no-context0.3740.3870.2100.4000.4390.2590.1230.1690.4000.3420.3330.2550.1650.145ellipsiscontext0.3250.3230.3330.4060.3890.4000.0210.0330.4710.4500.2700.2920.2400.135context-gold0.3880.2960.3000.4350.3710.3810.0250.1500.4440.4500.3060.2260.1870.154no-context-0.6070.3700.792-0.4290.4430.3990.6820.5990.4340.4640.0970.691formalitycontext-0.6390.3510.791-0.4620.4140.3970.6940.6000.4050.4690.0830.695context-gold-0.6610.4430.803-0.4640.4310.4250.6970.6220.4400.4920.1820.741no-context0.6390.7620.8190.8260.7230.7660.6150.5740.8210.8530.6610.6240.6710.645lexicalcontext0.6300.7360.8330.8300.7220.7720.5720.5240.8250.8510.6890.6240.6470.644context-gold0.6750.7370.8320.8320.7270.7730.6140.5930.8280.8570.7130.6250.6470.676no-context0.6600.6130.5760.774-0.5480.473--0.4520.356---pronounscontext0.6910.6140.5380.771-0.5490.377--0.4510.414---context-gold0.7000.6240.5500.788-0.5300.428--0.4850.432---no-context--0.2630.4350.2270.308--0.477-0.2920.2150.128-verb tensecontext--0.2870.4420.2290.282--0.479-0.2920.2150.094-context-gold--0.2720.4350.2290.285--0.487-0.3280.2380.120-"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "BLEU, COMET, and Word f-measure per tag for base context-aware models. BLEU, COMET and word f-measures statistically significantly higher than no-context (p < 0.05) are underlined.", "figure_data": "defrjazhno-context36.0945.6415.5522.15BLEUcontext35.8645.4012.6822.68context-gold 36.6946.6016.6022.98no-context0.5256 0.6332 0.0602 0.1160COMETcontext0.5337 0.6425 0.0753 0.2705context-gold 0.5427 0.6529 0.1808 0.2809no-context0.4290.4620.1260.254ellipsiscontext0.5180.3930.0680.230context-gold 0.4440.4440.1440.209no-context0.6420.8240.5100.747formalitycontext0.6400.8100.5130.739context-gold 0.6920.8200.5370.739no-context0.7730.8640.7040.661lexicalcontext0.7760.8680.6990.671context-gold 0.7960.8750.7400.696no-context0.6330.7900.493-pronounscontext0.6350.7950.541-context-gold 0.6650.8010.536-no-context-0.526--verb tensecontext-0.532--context-gold-0.534--"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "Word f-measure per tag for large models. BLEU, COMET, word f-measures statistically significantly higher than no-context (p < 0.05) are underlined.A1. Did you describe the limitations of your work? Last section in page 9 (unnumber) A2. Did you discuss any potential risks of your work? Work doesn't have immediate ethical risk A3. Do the abstract and introduction summarize the paper's main claims? Section 1 and Abstract A4. Have you used AI writing assistants when working on this paper?", "figure_data": "ardeesfrheitjakonlptrorutrzh"}], "formulas": [{"formula_id": "formula_0", "formula_text": "CXMI(C \u2192 Y |X) = H q M T A (Y |X) \u2212 H q M T C (Y |X, C),", "formula_coordinates": [2.0, 72.5, 662.24, 215.26, 37.09]}, {"formula_id": "formula_1", "formula_text": "CXMI(C \u2192 Y |X) \u2248 \u2212 1 N N i=1 log q M T A (y (i) |x (i) ) q M T C (y (i) |x (i) , C (i) )", "formula_coordinates": [2.0, 306.42, 95.59, 218.77, 56.72]}, {"formula_id": "formula_2", "formula_text": "P-CXMI(y, x, C) = \u2212 log q M T A (y|x) q M T C (y|x, C)", "formula_coordinates": [2.0, 322.69, 478.04, 184.27, 27.57]}, {"formula_id": "formula_3", "formula_text": "P-CXMI(i, y, x, C) = \u2212 log q M T A (y i |y t<i , x) q M T C (y i |y t<i , x, C)", "formula_coordinates": [2.0, 306.42, 698.82, 219.09, 33.79]}, {"formula_id": "formula_4", "formula_text": "{t 1 i , \u2022 \u2022 \u2022 , t n i } to each target token y i \u2208 Y .", "formula_coordinates": [4.0, 306.14, 306.37, 186.51, 20.55]}, {"formula_id": "formula_5", "formula_text": "D = {(X 1 , Y 1 ), \u2022 \u2022 \u2022 , (X |D| , Y |D| )},", "formula_coordinates": [5.0, 70.86, 86.9, 180.02, 18.93]}, {"formula_id": "formula_6", "formula_text": "A m = {\u27e8x i , y j \u27e9 | x i \u2194 y j , x i \u2208 X m , y j \u2208 Y m },", "formula_coordinates": [5.0, 75.09, 153.34, 209.81, 18.93]}, {"formula_id": "formula_7", "formula_text": "x i \u2208 X m s.t. \u27e8x i , y j \u27e9 \u2208 A m .", "formula_coordinates": [5.0, 380.49, 372.34, 123.73, 20.55]}, {"formula_id": "formula_8", "formula_text": "ko \u110c \u1166\u1100 \u1161, \u110c \u1165\u1112 \u1174, \u1102 \u1161 \u1103 \u1162 \u11a8\u110b \u1166, \u1109 \u1165 \u11bc\u1112 \u1161 \u11b7, \u1107 \u116e \u11ab, \u1109 \u1162 \u11bc\u1109 \u1175 \u11ab, \u1109 \u1175 \u11a8\u1109 \u1161, \u110b \u1167 \u11ab\u1109 \u1166, \u1107 \u1167 \u11bc\u1112 \u116a \u11ab, \u110b \u1163 \u11a8\u110c \u116e, \u110c \u1161\u110c \u1166\u1107 \u116e \u11ab, \u1107 \u116c \u11b8\u1103 \u1161, \u110c \u1165 nl jij,", "formula_coordinates": [16.0, 80.32, 386.5, 389.98, 55.34]}], "doi": "10.18653/v1/W19-5301"}