{"title": "A Unified Algebraic Approach to 2-D and 3-D Motion Segmentation", "authors": "Ren\u00e9 Vidal; Yi Ma", "pub_date": "", "abstract": "We present an analytic solution to the problem of estimating multiple 2-D and 3-D motion models from two-view correspondences or optical flow. The key to our approach is to view the estimation of multiple motion models as the estimation of a single multibody motion model. This is possible thanks to two important algebraic facts. First, we show that all the image measurements, regardless of their associated motion model, can be fit with a real or complex polynomial. Second, we show that the parameters of the motion model associated with an image measurement can be obtained from the derivatives of the polynomial at the measurement. This leads to a novel motion segmentation algorithm that applies to most of the two-view motion models adopted in computer vision. Our experiments show that the proposed algorithm outperforms existing algebraic methods in terms of efficiency and robustness, and provides a good initialization for iterative techniques, such as EM, which is strongly dependent on correct initialization.", "sections": [{"heading": "Introduction", "text": "A classic problem in visual motion analysis is to estimate a motion model for a set of 2-D feature points as they move in a video sequence. Ideally, one would like to fit a single model that describes the motion of all the features. In practice, however, different regions of the image obey different motion models due to depth discontinuities, perspective effects, multiple moving objects, etc. Therefore, one is faced with the problem of fitting multiple motion models to the image, without knowing which pixels are moving according to the same model. More specifically:", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem 1 (Multiple-motion estimation and segmentation). Given a set of image measurements {(x j", "text": "1 , x j 2 )} N j=1 taken from two views of a motion sequence related by a collection of n (n known) 2-D or 3-D motion models {M i } n i=1 , estimate the motion models without knowing which image measurements correspond to which motion model. Related literature. There is a rich literature addressing the 2-D motion segmentation problem using the so-called layered representation [1] or different variations of the Expectation Maximization (EM) algorithm [2,3,4]. These approaches alternate between the segmentation of the image measurements (E-step) and the estimation of the motion parameters (M-step) and suffer from the disadvantage that the convergence to the optimal solution strongly depends on correct initialization [5,6]. Existing initialization techniques estimate the motion parameters from local patches and cluster these motion parameters using K-means [7], normalized cuts [5], or a Bayesian version of RANSAC [6]. The only existing algebraic solution to 2-D motion segmentation is based on bi-homogeneous polynomial factorization and can be found in [9]. The 3-D motion segmentation problem has received relatively less attention. Existing approaches include combinations of EM with normalized cuts [8] and factorization methods for orthographic and affine cameras [10,11]. Algebraic approaches based on polynomial and tensor factorization have been proposed in the case of multiple translating objects [12] and in the case of two [13] and multiple [14] rigid-body motions.\nOur contribution. In this paper, we address the initialization of iterative approaches to motion estimation and segmentation by proposing a non-iterative algebraic solution to Problem 1 that applies to most 2-D and 3-D motion models in computer vision, as detailed in Table 1. The key to our approach is to view the estimation of multiple motion models as the estimation of a single, though more complex, multibody motion model that is then factored into the original models. This is achieved by (1) eliminating the feature segmentation problem in an algebraic fashion, (2) fitting a single multibody motion model to all the image measurements, and (3) segmenting the multibody motion model into its individual components. More specifically, our approach proceeds as follows:\n1. Eliminate Feature Segmentation: Find an algebraic equation that is satisfied by all the image measurements, regardless of the motion model associated with each measurement. For the motion models considered in this paper, the i th motion model will be typically defined by an algebraic equation of the form f (x 1 , x 2 , M i ) = 0. Therefore an algebraic equation that is satisfied by all the data is\ng(x 1 , x 2 , M) = f (x 1 , x 2 , M 1 )f (x 1 , x 2 , M 2 ) \u2022 \u2022 \u2022 f (x 1 , x 2 , M n ) = 0. (1)\nSuch an equation represents a single multibody motion model whose parameters M encode those of the original motion models {M i } n i=1 .", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b4", "b5", "b8", "b7", "b9", "b10", "b11", "b12", "b13", "b0"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Multibody Motion Estimation:", "text": "Estimate the parameters M of the multibody motion model from the given image measurements. For the motion models considered in this paper, the parameters M will correspond to the coefficients of a real or complex polynomial p n of degree n. We will show that if n is known such parameters can be estimated linearly after embedding the image data into a higher-dimensional space. 3. Motion Segmentation: Recover the parameters of the original motion models from the parameters of the multibody motion model M, i.e.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "M \u2192 {M", "text": "i } n i=1 . (2\n)\nWe will show that the individual motion parameters M i can be computed from the derivatives of p n evaluated at a collection of n image measurements.\nThis new approach offers two important technical advantages over previously known algebraic solutions to the segmentation of 3-D translational [12] and rigid-body motions (fundamental matrices) [14] based on homogeneous polynomial factorization: \nD translational x2 = x1 + Ti {Ti \u2208 R 2 } n i=1 Hyperplanes in C 2 2-D similarity x2 = \u03bbiRix1 + Ti {(Ri, Ti) \u2208 SE(2), \u03bbi \u2208 R + } n i=1 Hyperplanes in C 3 2-D affine x2 = Ai x1 1 {Ai \u2208 R 2\u00d73 } n i=1 Hyperplanes in C 4 3-D translational 0 = x T 2 [Ti]\u00d7x1 {Ti \u2208 R 3 } n i=1 Hyperplanes in R 3 3-D rigid-body 0 = x T 2 Fix1 {Fi \u2208 R 3\u00d73 : rank(Fi) = 2} n i=1 Bilinear forms in R 3\u00d73 3-D homography x2 \u223c Hix1 {Hi \u2208 R 3\u00d73 } n i=1 Bilinear forms in C 2\u00d73 1.\nIt is based on polynomial differentiation rather than polynomial factorization, which greatly improves the efficiency, accuracy and robustness of the algorithm. 2. It applies to either feature correspondences or optical flows and includes most of the two-view motion models in computer vision: 2-D translational, similarity, and affine, or 3-D translational, rigid body motions (fundamental matrices), or motions of planar scenes (homographies), as shown in Table 1. The unification is achieved by embedding some of the motion models into the complex domain, which resolves cases such as 2-D affine motions and 3-D homographies that could not be solved in the real domain.\nWith respect to extant probabilistic methods, our approach has the advantage that it provides a global, non-iterative solution that does not need initialization. Therefore, our method can be used to initialize any iterative or optimization based technique, such as EM, or else in a layered (multiscale) or hierarchical fashion at the user's discretion.", "publication_ref": ["b11", "b13"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Noisy image data.", "text": "Although the derivation of the algorithm will assume noise free data, the algorithm is designed to work with moderate noise, as we will soon point out.\nNotation. Let z be a vector in R K or C K and let z T be its transpose. A homogeneous polynomial of degree n in z is a polynomial p n (z) such that p n (\u03bbz) = \u03bb n p n (z) for all \u03bb in R or C. The space of all homogeneous polynomials of degree n in K variables,\nR n (K), is a vector space of dimension M n (K) = n + K \u2212 1 K \u2212 1 = n + K \u2212 1 n\n. A particular basis for R n (K) is obtained by considering all the monomials of degree n in K variables, that is z\nI = z n1 1 z n2 2 \u2022 \u2022 \u2022 z n K K with 0 \u2264 n j \u2264 n for j = 1, . . .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": ", K, and", "text": "n 1 + n 2 + \u2022 \u2022 \u2022 + n K = n.\nTherefore, each polynomial p n (z) \u2208 R n (K) can be written as a linear combination of a vector of coefficients c \u2208 R Mn(K) or C Mn(K) as\np n (z) = c T \u03bd n (z) = c n1,n2,\u2022\u2022\u2022 ,n K z n1 1 z n2 2 \u2022 \u2022 \u2022 z n K K ,(3)\nwhere\n\u03bd n : R K (C K ) \u2192 R Mn(K) (C Mn(K)\n) is the Veronese map of degree n [12] defined as \u03bd n : [z 1 , . . . , z K ] T \u2192 [. . . , z I , . . . ] T with I chosen in the degree-lexicographic order. The Veronese map is also known as the polynomial embedding in the machine learning community.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "2-D Motion Segmentation by Clustering Hyperplanes in C K", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Segmentation of 2-D Translational Motions: Clustering Hyperplanes in C 2", "text": "The case of feature points. Under the 2-D translational motion model the two images are related by one out of n possible 2-D translations {T i \u2208 R 2 } n i=1 . That is, for each feature pair x 1 \u2208 R 2 and x 2 \u2208 R 2 there exists a 2-D translation T i \u2208 R 2 such that\nx 2 = x 1 + T i . (4\n)\nTherefore, if we interpret the displacement of the features (x 2 \u2212 x 1 ) and the 2-D translations T i as complex numbers (x 2 \u2212 x 1 ) \u2208 C and T i \u2208 C, then we can re-write equation ( 4) as\nb T i z . = T i 1 1 \u2212(x 2 \u2212 x 1 ) = 0 \u2208 C 2 . (5\n)\nThe above equation corresponds to a hyperplane in C 2 whose normal vector b i encodes the 2-D translational motion T i . Therefore, the segmentation of n 2-D translational\nmotions {T i \u2208 R 2 } n i=1 from a set of correspondences {x j 1 \u2208 R 2 } N j=1 and {x j 2 \u2208 R 2 } N\nj=1 is equivalent to clustering data points {z j \u2208 C 2 } N j=1 lying on n complex hyperplanes with normal vectors {b i \u2208 C 2 } n i=1 . As we will see in short, other 2-D and 3-D motion segmentation problems are also equivalent to clustering data lying on complex hyperplanes in C 3 and C 4 . Therefore, rather than solving the hyperplane clustering problem for the case K = 2, we now present a solution for hyperplanes in C K with arbitrary K by adapting the Generalized PCA algorithm of [15] to the complex domain.\nEliminating feature segmentation. We first notice that each point z \u2208 C K , regardless of which motion model {b i \u2208 C K } n i=1 is associated with it, must satisfy the following homogeneous polynomial of degree n in K complex variables\np n (z) = n i=1 (b T i z) = I c I z I = c n1,...,n K z n1 1 z n2 2 \u2022 \u2022 \u2022 z n K K = c T \u03bd n (z) = 0, (6)\nwhere the coefficient vector c \u2208 C Mn(K) represents the multibody motion parameters.\nEstimating multibody motion. Since the polynomial p n must be satisfied by all the data points Z = {z j \u2208 C K } N j=1 , we obtain the following linear system on c\nL n c = 0 \u2208 C N , (7\n)\nwhere K) . One can show that there is a unique solution for c (up to a scale factor) if N \u2265 M n (K) \u2212 1 and at least K \u2212 1 points belong to each hyperplane. Furthermore, since the last entry of each b i is equal to one, then so is the last entry of c. Therefore, one can solve for c uniquely. In the presence of noise, one can solve for c in a least-squares sense as the singular vector of L n associated with its smallest singular value, and then normalize so that c Mn(K) = 1.\nL n = [\u03bd n (z 1 ), \u03bd n (z 2 ), . . . , \u03bd n (z N )] T \u2208 C N \u00d7Mn(\nSegmenting the multibody motion. Given c, we now present an algorithm for computing the motion parameters b i from the derivatives of p n . To this end, we consider the derivative of p n (z),\nDp n (z) = \u2202p n (z) \u2202z = n i=1 =i (b T z)b i ,(8)\nand notice that if we evaluate Dp n (z) at a point z = y i that corresponds to the i th motion model, i.e. if y i is such that b T i y i = 0, then we have Dp n (y i ) \u223c b i . Therefore, given c we can obtain the motion parameters as\nb i = Dp n (z) e T K Dp n (z) z=y i , (9\n)\nwhere e K = [0, . . . , 0, 1] T \u2208 C K and y i \u2208 C K is a nonzero vector such that b T i y i = 0. The rest of the problem is to find one vector y i \u2208 C K in each one of the hyperplanes\nH i = {z \u2208 C K : b T i z = 0} for i = 1, . . . , n.\nTo this end, notice that we can always choose a point y n lying on one of the hyperplanes as any of the points in the data set Z. However, in the presence of noise and outliers, an arbitrary point in Z may be far from the hyperplanes. The question is then how to compute the distance from each data point to its closest hyperplane, without knowing the normals to the hyperplanes. The following lemma allows us to compute a first order approximation to such a distance:\nLemma 1. Letz \u2208 H i be the projection of a point z \u2208 C K onto its closest hyperplane H i . Also let \u03a0 = (I \u2212 e K e T K )\n. Then the Euclidean distance from z to H i is given by\nz \u2212z = |p n (z)| \u03a0Dp n (z) + O z \u2212z 2 . (10\n)\nTherefore, we can choose a point in the data set close to one of the subspaces as:\ny n = arg min z\u2208Z |p n (z)| \u03a0Dp n (z) , (11\n)\nand then compute the normal vector at y n as b n = Dp n (y n )/(e T K Dp n (y n )). In order to find a point y n\u22121 in one of the remaining hyperplanes, we could just remove the points on H n from Z and compute y n\u22121 similarly to (11), but minimizing over Z \\ H n , and so on. However, the above process is not very robust in the presence of noise. Therefore, we propose an alternative solution that penalizes choosing a point from H n in (11) by dividing the objective function by the distance from z to H n , namely |b T n z|/ \u03a0b n . That is, we can choose a point on or close to \u222a n\u22121 i=1 H i as\ny n\u22121 = arg min z\u2208Z |pn(z)| \u03a0Dpn(z) + \u03b4 |b T n z| \u03a0bn + \u03b4 , (12\n)\nwhere \u03b4 > 0 is a small positive number chosen to avoid cases in which both the numerator and the denominator are zero (e.g. with perfect data). By repeating this process for the remaining hyperplanes, we obtain the following hyperplane clustering algorithm:\nAlgorithm 1 (Clustering hyperplanes in C K ) Given data points Z = {z j \u2208 C K } N j=1 solve for c \u2208 C Mn(K) from the linear system [\u03bd n (z 1 ), \u03bd n (z 2 ), . . . , \u03bd n (z N )] T c = 0; set p n (z) = c T \u03bd n (z); for i = n : 1, y i = arg min z\u2208Z |pn(z)| \u03a0Dpn(z) + \u03b4 |b T i+1 z|\u2022\u2022\u2022|b T n z| \u03a0bi+1 \u2022\u2022\u2022 \u03a0bn + \u03b4 ; b i = Dp n (y i ) e T K Dp n (y i ) ;(13)\nend.\nNotice that one could also choose the points y i in a purely algebraic fashion, e.g., by intersecting a random line with the hyperplanes, or else by dividing the polynomial p n (z) by b T n z. However, we have chosen to present Algorithm 1 instead, because it has a better performance with noisy data and is not very sensitive to the choice of \u03b4.\nThe case of translational optical flow. Imagine now that rather than a collection of feature points we are given the optical flow {u j \u2208 R 2 } N j=1 between two consecutive views of a video sequence. If we assume that the optical flow is piecewise constant, i.e. the optical flow of every pixel in the image takes only n possible values {T i \u2208 R 2 } n i=1 , then at each pixel j \u2208 {1, . . . , N} there exists a motion T i such that\nu j = T i . (14\n)\nThe problem is now to estimate the n motion models {T i } n i=1 from the optical flow\n{u j } N j=1 . If N \u2265 M n (2) \u2212 1 \u223c O(n)\n, this problem can be solved using the same technique as in the case of feature points (Algorithm 1 with K = 3) after replacing x 2 \u2212 x 1 = u.", "publication_ref": ["b14", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Segmentation of 2-D Similarity Motions: Clustering Hyperplanes in C 3", "text": "The case of feature points. In this case, we assume that for each feature point (x 1 , x 2 ) there exists a 2-D rigid-body motion (R i , T i ) \u2208 SE(2) and a scale \u03bb i \u2208 R + such that\nx 2 = \u03bb i R i x 1 + T i = \u03bb i cos(\u03b8 i ) \u2212 sin(\u03b8 i ) sin(\u03b8 i ) cos(\u03b8 i ) x 1 + T i . (15\n)\nTherefore, if we interpret the rotation matrix as a unit number R i = exp(\u03b8 i \u221a \u22121) \u2208 C, and the translation vector and the image features as points in the complex plane T i , x 1 , x 2 \u2208 C, then we can write the 2-D similarity motion model as the following hyperplane in C 3 :\nb T i z . = \u03bb i R i T i 1 \uf8ee \uf8f0 x 1 1 \u2212x 2 \uf8f9 \uf8fb = 0.(16)\nTherefore, the segmentation of 2-D similarity motions is equivalent to clustering hyperplanes in C 3 . As such, we can apply Algorithm 1 with K = 3 to a collection of\nN \u2265 M n (3) \u2212 1 \u223c O(n 2 ) image measurements {z j \u2208 C 3 } N j=1\n, with at least two measurements per motion model, to obtain the motion parameters {b i \u2208 C 3 } n i=1 . The original real motion parameters are then given as\n\u03bb i = |b i1 |, \u03b8 i = \u2220b i1 , and T i = [Re(b i2 ), Im(b i2 )] T , for i = 1, . . . , n. (17)\nThe case of optical flow. Let {u j \u2208 R 2 } N j=1 be N measurements of the optical flow at the N pixels {x j \u2208 R 2 } N j=1 . We assume that the optical flow field can be modeled as a collection of n 2-D similarity motion models as u = \u03bb i R i x + T i . Therefore, the segmentation of 2-D similarity motions from measurements of optical flow can be solved as in the case of feature points, after replacing x 2 = u and x 1 = x.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Segmentation of 2-D Affine Motions: Clustering Hyperplanes in C 4", "text": "The case of feature points. In this case, we assume that the images are related by a collection of n 2-D affine motion models\n{A i \u2208 R 2\u00d73 } n i=1 .\nThat is, for each feature pair (x 1 , x 2 ) there exist a 2-D affine motion A i such that\nx 2 = A i x 1 1 = a 11 a 12 a 13 a 21 a 22 a 23 i x 1 1 . (18\n)\nTherefore, if we interpret x 2 as a complex number x 2 \u2208 C, but we still think of x 1 as a vector in R 2 , then we have\nx 2 = a T i x 1 1 = a 11 + a 21 \u221a \u22121 a 12 + a 22 \u221a \u22121 a 13 + a 23 \u221a \u22121 i x 1 1 . (19\n)\nThe above equation represents the following hyperplane in C 4\nb T i z = a T i 1 \uf8ee \uf8f0 x 1 1 \u2212x 2 \uf8f9 \uf8fb = 0,(20)\nwhere the normal vector b i \u2208 C 4 encodes the affine motion parameters and the data point z \u2208 C 4 encodes the image measurements x 1 \u2208 R 2 and x 2 \u2208 C. Therefore, the segmentation of 2-D affine motion models is equivalent to clustering hyperplanes in C 4 .\nAs such, we can apply Algorithm 1 with K = 4 to a collection of\nN \u2265 M n (4) \u2212 1 \u223c O(n 3 ) image measurements {z j \u2208 C 4 } N j=1\n, with at least three measurements per motion model, to obtain the motion parameters {b i \u2208 C 3 } n i=1 . The original affine motion models are then obtained as\nA i = Re(b i1 ) Re(b i2 ) Re(b i3 ) Im(b i1 ) Im(b i2 ) Im(b i3 ) \u2208 R 2\u00d73 , for i = 1, . . . , n. (21\n)\nThe case of affine optical flow. In this case, the optical flow u is modeled as being generated by a collection of n affine motion models\n{A i \u2208 R 2\u00d73 } n i=1 of the form u = A i x 1\n. Therefore, the segmentation of 2-D affine motions can be solved as in the case of feature points, after replacing x 2 = u and x 1 = x.\n3 3-D Motion Segmentation", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Segmentation of 3-D Translational Motions: Clustering Hyperplanes in R 3", "text": "The case of feature points. In this case, we assume that the scene can be modeled as a mixture of purely translational motion models, {T i \u2208 R 3 } n i=1 , where T i represents the translation (calibrated case) or the epipole (uncalibrated case) of object i relative to the camera between the two frames. A solution to this problem based on polynomial factorization was proposed in [12]. Here we present a much simpler solution based on polynomial differentiation.\nGiven the images x 1 \u2208 P 2 and x 2 \u2208 P 2 of a point in object i in the first and second frame, they must satisfy the well-known epipolar constraint for linear motions\n\u2212x T 2 [T i ] \u00d7 x 1 = T T i (x 2 \u00d7 x 1 ) = T T i = 0,(22)\nwhere = (x 2 \u00d7 x 1 ) \u2208 R 3 is known as the epipolar line associated with the image pair (x 1 , x 2 ). Therefore, the segmentation of 3-D translational motions is equivalent to clustering data (epipolar lines) lying on a collection of hyperplanes in R 3 whose normal vectors are the n epipoles {T i } n i=1 . As such, we can apply Algorithm 1 with\nK = 3 to N \u2265 M n (3) \u2212 1 \u223c O(n 2 ) epipolar lines { j = x j 1 \u00d7 x j 2 } N j=1\n, with at least two epipolar lines per motion, to estimate the epipoles {T i } n i=1 from the derivatives of the polynomial\np n ( ) = (T T 1 ) \u2022 \u2022 \u2022 (T T n ).\nThe only difference is that in this case the last entry of each epipole is not constrained to be equal to one. Therefore, when choosing the points y i in equation ( 13) we should take \u03a0 = I not to eliminate the last coordinate. We therefore compute the epipoles up to an unknown scale factor as\nT i = Dp n (y i )/ Dp n (y i ) , i= 1, . . . , n, (23\n)\nwhere the unknown scale is lost under perspective projection.\nThe case of optical flow. In the case of optical flow generated by purely translating objects we have u T [T i ] \u00d7 x = 0, where u is interpreted as a three vector [u, v, 0] T \u2208 R 3 . Thus, one can estimate the translations {T i \u2208 R 3 } n i=1 as before by replacing x 2 = u and x 1 = x.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Segmentation of 3-D Rigid-Body Motions: Clustering Quadratic Forms in R 3\u00d73", "text": "Assume that the motion of the objects relative to the camera between the two views can be modeled as a mixture of 3-D rigid-body motions {(R i , T i ) \u2208 SE(3)} n i=1 which are represented with a nonzero rank-2 fundamental matrix F i . A solution to this problem based on the factorization of bi-homogeneous polynomials was proposed in [14]. Here we present a much simpler solution based on taking derivatives of the so-called multibody epipolar constraint (see below), thus avoiding polynomial factorization.\nGiven an image pair (x 1 , x 2 ), there exists a motion i such that the following epipolar constraint is satisfied\nx T 2 F i x 1 = 0. (24\n)\nTherefore, the following multibody epipolar constraint [14] must be satisfied by the number of independent motions n, the fundamental matrices {F i } n i=1 and the image pair (x 1 , x 2 ), regardless of the object to which the image pair belongs\np n (x 1 , x 2 ) . = n i=1 x T 2 F i x 1 = 0. (25\n)\nIt was also shown in [14] that the multibody epipolar constraint can be written in bilinear form as \u03bd n (x 2 ) T F\u03bd n (x 1 ) = 0, where F \u2208 R Mn(3)\u00d7Mn (3) is the so-called multibody fundamental matrix, which can be linearly estimated from\nN \u2265 M n (3) 2 \u2212 1 \u223c O(n 4 )\nimage pairs in general position with at least 8 pairs corresponding to each motion. We now present a new solution to the problem of estimating the fundamental matrices {F i } n i=1 from the multibody fundamental matrix F based on taking derivatives of the multibody epipolar constraint. Recall that, given a point x 1 \u2208 P 2 in the first image frame, the epipolar lines associated with it are defined as i .\n= F i x 1 \u2208 R 3 , i = 1, . . . , n. Therefore, if the image pair (x 1 , x 2 ) corresponds to motion i, i.e. if x T 2 F i x 1 = 0, then \u2202 \u2202x 2 \u03bd n (x 2 ) T F\u03bd n (x 1 ) = n i=1 =i (x T 2 F x 1 )(F i x 1 ) = =i (x T 2 F x 1 )(F i x 1 ) \u223c i . (26)\nIn other words, the partial derivative of the multibody epipolar constraint with respect to x 2 evaluated at (x 1 , x 2 ) is proportional to the epipolar line associated with (x 1 , x 2 ) in the second view. 1 Therefore, given a set of image pairs {(x j 1 , x j 2 )} N j=1 and the multibody fundamental matrix F \u2208 R Mn(3)\u00d7Mn (3) , we can estimate a collection of epipolar lines { j } N j=1 . Remember from Section 3.1 that in the case of purely translating objects the epipolar lines were readily obtained as x 1 \u00d7 x 2 . Here the calculation is more involved because of the rotational component of the rigid-body motions. Nevertheless, given a set of epipolar lines we can apply Algorithm 1 with K = 3 and \u03a0 = I to estimate the n epipoles {T i } n i=1 up to a scale factor, as in equation ( 23). Therefore, if the n epipoles are different, 2 then we can immediately compute the n fundamental matrices {F i } n i=1 by assigning the image pair (x j 1 , x j 2 ) to group i if i = arg min =1,...n (T T i j ) 2 and then applying the eight-point algorithm to the image pairs in group i = 1, . . . , n.", "publication_ref": ["b13", "b13", "b13", "b2", "b0", "b2", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Segmentation of 3-D Homographies: Clustering Quadratic Forms in C 2\u00d73", "text": "The motion segmentation scheme described in the previous section assumes that the displacement of each object between the two views relative to the camera is nonzero, i.e. T i = 0, otherwise the individual fundamental matrices are zero. Furthermore, it also requires that the 3-D points be in general configuration, otherwise one cannot uniquely recover each fundamental matrix from its epipolar constraint. The latter case occurs, for example, in the case of planar structures, i.e. when the 3-D points lie on a plane [16].\nBoth in the case of purely rotating objects (relative to the camera) or in the case of a planar 3-D structure, the motion model between the two views x 1 \u2208 P 2 and x 2 \u2208 P 2 is described by a homography matrix H \u2208 R 3\u00d73 such that [16] \nx 2 \u223c Hx 1 = \uf8ee \uf8f0 h 11 h 12 h 13 h 21 h 22 h 23 h 31 h 32 h 33 \uf8f9 \uf8fb x 1 . (27\n)\nConsider now the case in which we are given a set of image pairs {(x j 1 , x j 2 )} N j=1 that can be modeled with n independent homographies {H i } n i=1 (see Remark 2). Note that the n homographies do not necessarily correspond to n different rigid-body motions. This is because it could be the case that one rigidly moving object consists of two or more planes, hence its rigid-body motion will lead to two or more homographies. Therefore, the n homographies can represent anything from 1 up to n rigid-body motions. In either case, it is evident from the form of equation ( 27) that we cannot take the product of all the equations, as we did with the epipolar constraints, because we have two linearly independent equations per image pair. Nevertheless, we show now that one can still solve the problem by working in the complex domain, as we describe below.\nWe interpret the second image x 2 \u2208 P 2 as a point in CP by considering the first two coordinates in x 2 as a complex number and appending a one to it. However, we still think of x 1 as a point in P 2 . With this interpretation, we can rewrite (27) as\nx 2 \u223c Hx 1 . = h 11 + h 21 \u221a \u22121 h 12 + h 22 \u221a \u22121 h 13 + h 23 \u221a \u22121 h 31 h 32 h 33 x 1 , (28\n)\nwhere H \u2208 C 2\u00d73 now represents a complex homography 3 . Let w 2 be the vector in CP perpendicular to x 2 , i.e. if x 2 = (z, 1) then w 2 = (1, \u2212z). Then we can rewrite (28) as the following complex bilinear constraint\nw T 2 Hx 1 = 0,(29)\nwhich we call the complex homography constraint. We can therefore interpret the motion segmentation problem as one in which we are given image data {x j 1 \u2208 P 2 } N j=1 and {w j 2 \u2208 CP} N j=1 generated by a collection of n complex homographies\n{H i \u2208 C 2\u00d73 } n i=1 . Then each image pair (x 1 , w 2 ) has to satisfy the multibody homography constraint n i=1 (w T 2 H i x 1 ) = \u03bd n (w 2 ) T H\u03bd n (x 1 ) = 0,(30)\nregardless of which one of the n complex homographies is associated with the image pair. We call the matrix H \u2208 C Mn(2)\u00d7Mn (3) the multibody homography. Now, since the multibody homography constraint (30) is linear in the multibody homography H, we can linearly solve for H from (30) given\nN \u2265 M n (2)M n (3) \u2212 (M n (3)+1)/2 \u223c O(n 3 )\nimage pairs in general position 4 with at least 4 pairs per moving object. Given the multibody homography H \u2208 C Mn(2)\u00d7Mn (3) , the rest of the problem is to recover the individual homographies {H i } n i=1 . In the case of fundamental matrices discussed in Section 3.2, the key for solving the problem was the fact that fundamental matrices are of rank 2, hence one can cluster epipolar lines based on the epipoles. In principle, we cannot do the same with real homographies H i \u2208 R 3\u00d73 , because in general they are full rank. However, if we work with complex homographies H i \u2208 C 2\u00d73 they automatically have a right null space which we call the complex epipole e i \u2208 C 3 . Then, similarly to (26), we can associate a complex epipolar line\nj \u223c \u2202\u03bd n (w 2 ) T H\u03bd n (x 1 ) \u2202x 1 (x1,w2)=(x j 1 ,w j 2 ) \u2208 CP 2 (31)\nwith each image pair (x j 1 , w j 2 ). Given this set of\nN \u2265 M n (3) \u2212 1 complex epipolar lines { j } N\nj=1 , with at least 2 lines per moving object, we can apply Algorithm 1 with K = 3 and \u03a0 = I to estimate the n complex epipoles {e i \u2208 C 3 } n i=1 up to a scale factor, as in equation ( 23). Therefore, if the n complex epipoles are different, we can cluster the original image measurements by assigning image pair (x j 1 , x j 2 ) to group i if i = arg min =1,...,n |e T j | 2 . Once the image pairs have been clustered, the estimation of each homography, either real or complex, becomes a simple linear problem.", "publication_ref": ["b15", "b15", "b2", "b2", "b3", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Remark 1 (Direct extraction of homographies from H).", "text": "There is yet another way to obtain individual H i from H without segmenting the image pairs first. Once the complex epipoles e i are known, one can compute the following linear combination of the rows of H i (up to scale) from the derivatives of the multibody homography constraint at e i\nw T H i \u223c \u2202\u03bd n (w) T H\u03bd n (x) \u2202x x=ei \u2208 CP 2 , \u2200w \u2208 C 2 . (32\n)\nIn particular, if we take w = [1, 0] T and w = [0, 1] T we obtain the first and second row of H i (up to scale), respectively. By choosing additional w's one obtains more linear combinations from which the rows of H i can be linearly and uniquely determined.\nRemark 2 (Independent homographies). The above solution assumes that the complex epipoles are different (up to a scale factor). We take this assumption as our definition of independent homographies, even though it is more restrictive than saying than the real homographies H i \u2208 R 3\u00d73 are different (up to a scale factor). However, one can show that, under mild conditions, e.g., the third rows of each H i are different, the null spaces of the complex homographies are indeed different for different real homographies. 5 Remark 3 (One rigid-body motion versus multiple ones). A homography is generally of the form H = R + T \u03c0 T where \u03c0 is the plane normal. If the homographies come from different planes (different \u03c0) undergoing the same rigid-body motion, the proposed scheme would work just fine since different normal vectors \u03c0 will cause the complex epipoles to be different. However, if multiple planes with the same normal vector \u03c0 = [0, 0, 1] T undergo pure translational motions of the form T i = [T xi , T yi , T zi ] T , then all the complex epipoles are equal to e i = [ \u221a \u22121, \u22121, 0] T . To avoid this problem, one can complexify the first and third rows of H instead of the first two. The new complex epipoles are e i =[T xi +T zi \u221a \u22121, T yi , \u22121] T , which are different for different translations.", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments on Real and Synthetic Images", "text": "2-D translational. We tested our polynomial differentiation algorithm (PDA) by segmenting 12 frames of a sequence consisting of an aerial view of two robots moving on the ground. The robots are purposely moving slowly, so that it is harder to distinguish the flow from the noise. At each frame, we applied Algorithm 1 with K = 2 and \u03b4 = 0.02 to the optical flow 6 of all N = 240 \u00d7 352 pixels in the image and segmented the image measurements into n = 3 translational motion models. The leftmost column of Figure 1 displays the x and y coordinates of the optical flow for frames 4 and 10, showing that it is not so simple to distinguish the three clusters from the raw data. The remaining columns of Figure 1 show the segmentation of the image pixels. The motion of the two robots and that of the background are correctly segmented. We also applied Algorithm 1 to the optical flow of the flower garden sequence. Figure 2 shows the optical flow of one frame and the segmentation of the pixels into three groups: the tree, the grass, and the background. Notice that the boundaries of the tree can be assigned to any group, and in this case they are grouped with the grass.  3-D translational motions. Figure 3(a) shows the first frame of a 320 \u00d7 240 video sequence containing a truck and a car undergoing two 3-D translational motions. We applied Algorithm 1 with K = 3, \u03a0 = I and \u03b4 = 0.02 to the (real) epipolar lines obtained from a total of N = 92 features, 44 in the truck and 48 in the car. The algorithm obtained a perfect segmentation of the features, as shown in Figure 3(b), and estimated the epipoles with an error of 5.9 \u2022 for the truck and 1.7 \u2022 for the car. We also tested the performance of PDA on synthetic data corrupted with zero-mean Gaussian noise with s.t.d. between 0 and 1 pixels for an image size of 500 \u00d7 500 pixels. For comparison purposes, we also implemented the polynomial factorization algorithm (PFA) of [12] and a variation of the Expectation Maximization algorithm (EM) for clustering hyperplanes in R 3 . Figures 3(c) and (d) show the performance of all the algorithms as a function of the level of noise for n = 2 moving objects. The performance measures are the mean error between the estimated and the true epipoles (in degrees), and the mean percentage of correctly segmented features using 1000 trials for each level of noise. Notice that PDA gives an error of less than 1.3 \u2022 and a classification performance of over 96%. Thus our algorithm PDA gives approximately 1/3 the error of PFA, and improves the classification performance by about 2%. Notice also that EM with the normal vectors initialized at random (EM) yields a nonzero error in the noise free case, because it frequently converges to a local minimum. In fact, our algorithm PDA outperforms EM. However, if we use PDA to initialize EM (PDA+EM), the performance of both EM and PDA improves, showing that our algorithm can be effectively used to initialize iterative approaches to motion segmentation. Furthermore, the number of iterations of PDA+EM is approximately 50% with respect to EM randomly initialized, hence there is also a gain in computing time. Figures 3(e) and (f) show the performance of PDA as a function of the number of moving objects for different levels of noise. As expected, the performance deteriorates with the number of moving objects, though the translation error is still below 8 \u2022 and the percentage of correct classification is over 78%.\n3-D homographies. Figure 4(a) shows the first frame of a 2048 \u00d7 1536 video sequence with two moving objects: a cube and a checkerboard. Notice that although there are only two rigid motions, the scene contains three different homographies, each one associated with each one of the visible planar structures. Furthermore, notice that the top side of the cube and the checkerboard have approximately the same normals. We manually tracked a total of N = 147 features: 98 in the cube (49 in each of the two visible sides) and 49 in the checkerboard. We applied our algorithm in Section 3.3 with \u03a0 = I and \u03b4 = 0.02 to segment the image data and obtained a 97% of correct classification, as shown in Figure 4(b). We then added zero-mean Gaussian noise with standard deviation between 0 and 1 pixels to the features, after rectifying the features in the second view in order to simulate the noise free case. Figure 4(c) shows the mean percentage of correct classification for 1000 trials per level of noise. The percentage of correct classification of our algorithm is between 80% and 100%, which gives a very good initial estimate for any of the existing iterative/optimization/EM based motion segmentation schemes.  ", "publication_ref": ["b11"], "figure_ref": ["fig_4", "fig_4", "fig_4"], "table_ref": []}, {"heading": "Conclusions", "text": "We have presented a unified algebraic approach to 2-D and 3-D motion segmentation from feature correspondences or optical flow. Contrary to extant methods, our approach does not iterate between feature segmentation and motion estimation. Instead, it computes a single multibody motion model that is satisfied by all the image measurements and then extracts the original motion models from the derivatives of the multibody one.\nVarious experiments showed that our algorithm not only outperforms existing algebraic methods with much limited applicability, but also provides a good initialization for iterative techniques, such as EM, which are strongly dependent on correct initialization.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "The authors thank Jacopo Piazzi and Frederik Schaffalitzky for fruitful discussions. Research funded with startup funds from the departments of BME at Johns Hopkins and ECE at UIUC.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Robust estimation of a multi-layered motion representation", "journal": "IEEE Workshop on Visual Motion", "year": "1991", "authors": "T Darrel; A Pentland"}, {"ref_id": "b1", "title": "Mixture models for optical flow computation", "journal": "CVPR", "year": "1993", "authors": "A Jepson; M Black"}, {"ref_id": "b2", "title": "Layered representation of motion video using robust maximumlikelihood estimation of mixture models and MDL encoding", "journal": "ICCV", "year": "1995", "authors": "S Ayer; H Sawhney"}, {"ref_id": "b3", "title": "Smoothness in layers: Motion segmentation using nonparametric mixture estimation", "journal": "CVPR", "year": "1997", "authors": "Y Weiss"}, {"ref_id": "b4", "title": "Motion segmentation and tracking using normalized cuts", "journal": "ICCV", "year": "1998", "authors": "J Shi; J Malik"}, {"ref_id": "b5", "title": "An integrated Bayesian approach to layer extraction from image sequences", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2001", "authors": "P Torr; R Szeliski; P Anandan"}, {"ref_id": "b6", "title": "Layered representation for motion analysis", "journal": "CVPR", "year": "1993", "authors": "J Wang; E Adelson"}, {"ref_id": "b7", "title": "Scene segmentation from 3D motion", "journal": "CVPR", "year": "1998", "authors": "X Feng; P Perona"}, {"ref_id": "b8", "title": "Segmentation of dynamic scenes from image intensities", "journal": "IEEE Workshop on Vision and Motion Computing", "year": "2002", "authors": "R Vidal; S Sastry"}, {"ref_id": "b9", "title": "Multi-body factorization methods for motion analysis", "journal": "ICCV", "year": "1995", "authors": "J Costeira; T Kanade"}, {"ref_id": "b10", "title": "Motion segmentation by subspace separation and model selection", "journal": "ICCV", "year": "2001", "authors": "K Kanatani"}, {"ref_id": "b11", "title": "Generalized principal component analysis (GPCA)", "journal": "", "year": "2003", "authors": "R Vidal; Y Ma; S Sastry"}, {"ref_id": "b12", "title": "Two-body segmentation from two perspective views", "journal": "CVPR", "year": "2001", "authors": "L Wolf; A Shashua"}, {"ref_id": "b13", "title": "Two-view multibody structure from motion", "journal": "International Journal of Computer Vision", "year": "2004", "authors": "R Vidal; Y Ma; S Soatto; S Sastry"}, {"ref_id": "b14", "title": "Piazzi: A new GPCA algorithm for clustering subspaces by fitting, differentiating and dividing polynomials", "journal": "CVPR", "year": "2004", "authors": "R Vidal; Y Ma; J "}, {"ref_id": "b15", "title": "Multiple View Geometry in Computer Vision", "journal": "", "year": "2000", "authors": "R Hartley; A Zisserman"}], "figures": [{"figure_label": "12", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 1 .Fig. 2 .12Fig. 1. Segmenting the optical flow of the two-robot sequence by clustering lines in C 2", "figure_data": ""}, {"figure_label": "43", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "4 Fig. 3 .43Fig. 3. Segmenting 3-D translational motions by clustering planes in R 3 . Left: segmenting a real sequence with 2 moving objects. Center: comparing our algorithm with PFA and EM as a function of noise in the image features. Right: performance of PFA as a function of the number of motions", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 4 .4Fig. 4. Segmenting 3-D homographies by clustering complex bilinear forms in C 2\u00d73", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "2-D and 3-D motion models considered in this paper", "figure_data": "Motion modelsModel equationsModel parametersEquivalent to clustering2-"}], "formulas": [{"formula_id": "formula_0", "formula_text": "g(x 1 , x 2 , M) = f (x 1 , x 2 , M 1 )f (x 1 , x 2 , M 2 ) \u2022 \u2022 \u2022 f (x 1 , x 2 , M n ) = 0. (1)", "formula_coordinates": [2.0, 74.77, 376.55, 314.82, 10.71]}, {"formula_id": "formula_1", "formula_text": "i } n i=1 . (2", "formula_coordinates": [2.0, 235.12, 509.94, 150.59, 13.03]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [2.0, 385.7, 511.96, 3.89, 8.97]}, {"formula_id": "formula_3", "formula_text": "D translational x2 = x1 + Ti {Ti \u2208 R 2 } n i=1 Hyperplanes in C 2 2-D similarity x2 = \u03bbiRix1 + Ti {(Ri, Ti) \u2208 SE(2), \u03bbi \u2208 R + } n i=1 Hyperplanes in C 3 2-D affine x2 = Ai x1 1 {Ai \u2208 R 2\u00d73 } n i=1 Hyperplanes in C 4 3-D translational 0 = x T 2 [Ti]\u00d7x1 {Ti \u2208 R 3 } n i=1 Hyperplanes in R 3 3-D rigid-body 0 = x T 2 Fix1 {Fi \u2208 R 3\u00d73 : rank(Fi) = 2} n i=1 Bilinear forms in R 3\u00d73 3-D homography x2 \u223c Hix1 {Hi \u2208 R 3\u00d73 } n i=1 Bilinear forms in C 2\u00d73 1.", "formula_coordinates": [3.0, 45.13, 76.68, 342.63, 112.31]}, {"formula_id": "formula_4", "formula_text": "R n (K), is a vector space of dimension M n (K) = n + K \u2212 1 K \u2212 1 = n + K \u2212 1 n", "formula_coordinates": [3.0, 43.76, 442.54, 326.44, 19.93]}, {"formula_id": "formula_5", "formula_text": "I = z n1 1 z n2 2 \u2022 \u2022 \u2022 z n K K with 0 \u2264 n j \u2264 n for j = 1, . . .", "formula_coordinates": [3.0, 140.95, 475.79, 211.95, 13.81]}, {"formula_id": "formula_6", "formula_text": "n 1 + n 2 + \u2022 \u2022 \u2022 + n K = n.", "formula_coordinates": [3.0, 43.77, 489.41, 104.9, 10.71]}, {"formula_id": "formula_7", "formula_text": "p n (z) = c T \u03bd n (z) = c n1,n2,\u2022\u2022\u2022 ,n K z n1 1 z n2 2 \u2022 \u2022 \u2022 z n K K ,(3)", "formula_coordinates": [3.0, 108.3, 522.12, 281.29, 13.81]}, {"formula_id": "formula_8", "formula_text": "\u03bd n : R K (C K ) \u2192 R Mn(K) (C Mn(K)", "formula_coordinates": [3.0, 70.32, 548.73, 137.21, 11.56]}, {"formula_id": "formula_9", "formula_text": "x 2 = x 1 + T i . (4", "formula_coordinates": [4.0, 186.96, 129.2, 198.75, 10.71]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [4.0, 385.71, 129.87, 3.88, 8.97]}, {"formula_id": "formula_11", "formula_text": "b T i z . = T i 1 1 \u2212(x 2 \u2212 x 1 ) = 0 \u2208 C 2 . (5", "formula_coordinates": [4.0, 135.33, 190.53, 250.39, 22.66]}, {"formula_id": "formula_12", "formula_text": ")", "formula_coordinates": [4.0, 385.72, 197.28, 3.87, 8.97]}, {"formula_id": "formula_13", "formula_text": "motions {T i \u2208 R 2 } n i=1 from a set of correspondences {x j 1 \u2208 R 2 } N j=1 and {x j 2 \u2208 R 2 } N", "formula_coordinates": [4.0, 43.77, 244.04, 345.83, 25.31]}, {"formula_id": "formula_14", "formula_text": "p n (z) = n i=1 (b T i z) = I c I z I = c n1,...,n K z n1 1 z n2 2 \u2022 \u2022 \u2022 z n K K = c T \u03bd n (z) = 0, (6)", "formula_coordinates": [4.0, 50.41, 400.51, 339.18, 30.81]}, {"formula_id": "formula_15", "formula_text": "L n c = 0 \u2208 C N , (7", "formula_coordinates": [4.0, 179.06, 502.19, 206.66, 12.06]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [4.0, 385.72, 504.2, 3.87, 8.97]}, {"formula_id": "formula_17", "formula_text": "L n = [\u03bd n (z 1 ), \u03bd n (z 2 ), . . . , \u03bd n (z N )] T \u2208 C N \u00d7Mn(", "formula_coordinates": [4.0, 70.21, 524.3, 200.33, 12.08]}, {"formula_id": "formula_18", "formula_text": "Dp n (z) = \u2202p n (z) \u2202z = n i=1 =i (b T z)b i ,(8)", "formula_coordinates": [5.0, 137.44, 88.33, 252.15, 30.89]}, {"formula_id": "formula_19", "formula_text": "b i = Dp n (z) e T K Dp n (z) z=y i , (9", "formula_coordinates": [5.0, 167.91, 173.65, 217.81, 27.84]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [5.0, 385.72, 181.06, 3.87, 8.97]}, {"formula_id": "formula_21", "formula_text": "H i = {z \u2208 C K : b T i z = 0} for i = 1, . . . , n.", "formula_coordinates": [5.0, 43.77, 235.43, 188.98, 13.37]}, {"formula_id": "formula_22", "formula_text": "Lemma 1. Letz \u2208 H i be the projection of a point z \u2208 C K onto its closest hyperplane H i . Also let \u03a0 = (I \u2212 e K e T K )", "formula_coordinates": [5.0, 43.76, 312.48, 345.83, 24.77]}, {"formula_id": "formula_23", "formula_text": "z \u2212z = |p n (z)| \u03a0Dp n (z) + O z \u2212z 2 . (10", "formula_coordinates": [5.0, 135.72, 343.34, 249.71, 24.28]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [5.0, 385.43, 350.75, 4.16, 8.97]}, {"formula_id": "formula_25", "formula_text": "y n = arg min z\u2208Z |p n (z)| \u03a0Dp n (z) , (11", "formula_coordinates": [5.0, 154.55, 394.69, 230.89, 24.28]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [5.0, 385.44, 402.1, 4.15, 8.97]}, {"formula_id": "formula_27", "formula_text": "y n\u22121 = arg min z\u2208Z |pn(z)| \u03a0Dpn(z) + \u03b4 |b T n z| \u03a0bn + \u03b4 , (12", "formula_coordinates": [5.0, 148.84, 520.18, 236.59, 34.07]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [5.0, 385.43, 532.5, 4.16, 8.97]}, {"formula_id": "formula_29", "formula_text": "Algorithm 1 (Clustering hyperplanes in C K ) Given data points Z = {z j \u2208 C K } N j=1 solve for c \u2208 C Mn(K) from the linear system [\u03bd n (z 1 ), \u03bd n (z 2 ), . . . , \u03bd n (z N )] T c = 0; set p n (z) = c T \u03bd n (z); for i = n : 1, y i = arg min z\u2208Z |pn(z)| \u03a0Dpn(z) + \u03b4 |b T i+1 z|\u2022\u2022\u2022|b T n z| \u03a0bi+1 \u2022\u2022\u2022 \u03a0bn + \u03b4 ; b i = Dp n (y i ) e T K Dp n (y i ) ;(13)", "formula_coordinates": [6.0, 43.22, 46.48, 346.38, 93.17]}, {"formula_id": "formula_30", "formula_text": "u j = T i . (14", "formula_coordinates": [6.0, 198.59, 301.61, 186.84, 10.71]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [6.0, 385.44, 302.28, 4.15, 8.97]}, {"formula_id": "formula_32", "formula_text": "{u j } N j=1 . If N \u2265 M n (2) \u2212 1 \u223c O(n)", "formula_coordinates": [6.0, 43.77, 333.57, 161.03, 12.66]}, {"formula_id": "formula_33", "formula_text": "x 2 = \u03bb i R i x 1 + T i = \u03bb i cos(\u03b8 i ) \u2212 sin(\u03b8 i ) sin(\u03b8 i ) cos(\u03b8 i ) x 1 + T i . (15", "formula_coordinates": [6.0, 108.03, 439.15, 277.41, 22.67]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [6.0, 385.44, 445.9, 4.15, 8.97]}, {"formula_id": "formula_35", "formula_text": "b T i z . = \u03bb i R i T i 1 \uf8ee \uf8f0 x 1 1 \u2212x 2 \uf8f9 \uf8fb = 0.(16)", "formula_coordinates": [6.0, 149.94, 521.53, 239.65, 42.59]}, {"formula_id": "formula_36", "formula_text": "N \u2265 M n (3) \u2212 1 \u223c O(n 2 ) image measurements {z j \u2208 C 3 } N j=1", "formula_coordinates": [7.0, 43.76, 45.95, 270.27, 13.18]}, {"formula_id": "formula_37", "formula_text": "\u03bb i = |b i1 |, \u03b8 i = \u2220b i1 , and T i = [Re(b i2 ), Im(b i2 )] T , for i = 1, . . . , n. (17)", "formula_coordinates": [7.0, 57.3, 89.86, 332.29, 12.06]}, {"formula_id": "formula_38", "formula_text": "{A i \u2208 R 2\u00d73 } n i=1 .", "formula_coordinates": [7.0, 206.49, 224.4, 70.69, 13.18]}, {"formula_id": "formula_39", "formula_text": "x 2 = A i x 1 1 = a 11 a 12 a 13 a 21 a 22 a 23 i x 1 1 . (18", "formula_coordinates": [7.0, 136.79, 256.5, 248.65, 25.26]}, {"formula_id": "formula_40", "formula_text": ")", "formula_coordinates": [7.0, 385.44, 263.25, 4.15, 8.97]}, {"formula_id": "formula_41", "formula_text": "x 2 = a T i x 1 1 = a 11 + a 21 \u221a \u22121 a 12 + a 22 \u221a \u22121 a 13 + a 23 \u221a \u22121 i x 1 1 . (19", "formula_coordinates": [7.0, 51.42, 315.25, 334.02, 23.64]}, {"formula_id": "formula_42", "formula_text": ")", "formula_coordinates": [7.0, 385.44, 323.71, 4.15, 8.97]}, {"formula_id": "formula_43", "formula_text": "b T i z = a T i 1 \uf8ee \uf8f0 x 1 1 \u2212x 2 \uf8f9 \uf8fb = 0,(20)", "formula_coordinates": [7.0, 158.79, 359.02, 230.8, 42.59]}, {"formula_id": "formula_44", "formula_text": "N \u2265 M n (4) \u2212 1 \u223c O(n 3 ) image measurements {z j \u2208 C 4 } N j=1", "formula_coordinates": [7.0, 43.77, 446.38, 345.83, 23.76]}, {"formula_id": "formula_45", "formula_text": "A i = Re(b i1 ) Re(b i2 ) Re(b i3 ) Im(b i1 ) Im(b i2 ) Im(b i3 ) \u2208 R 2\u00d73 , for i = 1, . . . , n. (21", "formula_coordinates": [7.0, 90.57, 499.63, 294.87, 22.66]}, {"formula_id": "formula_46", "formula_text": ")", "formula_coordinates": [7.0, 385.44, 506.37, 4.15, 8.97]}, {"formula_id": "formula_47", "formula_text": "{A i \u2208 R 2\u00d73 } n i=1 of the form u = A i x 1", "formula_coordinates": [7.0, 43.77, 548.98, 345.83, 35.25]}, {"formula_id": "formula_48", "formula_text": "\u2212x T 2 [T i ] \u00d7 x 1 = T T i (x 2 \u00d7 x 1 ) = T T i = 0,(22)", "formula_coordinates": [8.0, 129.53, 187.91, 261.72, 13.03]}, {"formula_id": "formula_49", "formula_text": "K = 3 to N \u2265 M n (3) \u2212 1 \u223c O(n 2 ) epipolar lines { j = x j 1 \u00d7 x j 2 } N j=1", "formula_coordinates": [8.0, 43.77, 244.15, 345.83, 24.86]}, {"formula_id": "formula_50", "formula_text": "p n ( ) = (T T 1 ) \u2022 \u2022 \u2022 (T T n ).", "formula_coordinates": [8.0, 43.77, 280.2, 105.7, 12.54]}, {"formula_id": "formula_51", "formula_text": "T i = Dp n (y i )/ Dp n (y i ) , i= 1, . . . , n, (23", "formula_coordinates": [8.0, 123.25, 338.01, 262.19, 11.65]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [8.0, 385.44, 338.68, 4.15, 8.97]}, {"formula_id": "formula_53", "formula_text": "x T 2 F i x 1 = 0. (24", "formula_coordinates": [8.0, 189.36, 580.51, 196.08, 13.03]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [8.0, 385.44, 582.53, 4.15, 8.97]}, {"formula_id": "formula_55", "formula_text": "p n (x 1 , x 2 ) . = n i=1 x T 2 F i x 1 = 0. (25", "formula_coordinates": [9.0, 148.53, 91.96, 236.91, 30.66]}, {"formula_id": "formula_56", "formula_text": ")", "formula_coordinates": [9.0, 385.44, 102.32, 4.15, 8.97]}, {"formula_id": "formula_57", "formula_text": "N \u2265 M n (3) 2 \u2212 1 \u223c O(n 4 )", "formula_coordinates": [9.0, 277.99, 155.0, 111.61, 12.08]}, {"formula_id": "formula_58", "formula_text": "= F i x 1 \u2208 R 3 , i = 1, . . . , n. Therefore, if the image pair (x 1 , x 2 ) corresponds to motion i, i.e. if x T 2 F i x 1 = 0, then \u2202 \u2202x 2 \u03bd n (x 2 ) T F\u03bd n (x 1 ) = n i=1 =i (x T 2 F x 1 )(F i x 1 ) = =i (x T 2 F x 1 )(F i x 1 ) \u223c i . (26)", "formula_coordinates": [9.0, 43.59, 215.04, 346.01, 64.94]}, {"formula_id": "formula_59", "formula_text": "x 2 \u223c Hx 1 = \uf8ee \uf8f0 h 11 h 12 h 13 h 21 h 22 h 23 h 31 h 32 h 33 \uf8f9 \uf8fb x 1 . (27", "formula_coordinates": [10.0, 149.38, 120.91, 236.06, 42.59]}, {"formula_id": "formula_60", "formula_text": ")", "formula_coordinates": [10.0, 385.44, 141.6, 4.15, 8.97]}, {"formula_id": "formula_61", "formula_text": "x 2 \u223c Hx 1 . = h 11 + h 21 \u221a \u22121 h 12 + h 22 \u221a \u22121 h 13 + h 23 \u221a \u22121 h 31 h 32 h 33 x 1 , (28", "formula_coordinates": [10.0, 63.64, 334.69, 321.8, 30.49]}, {"formula_id": "formula_62", "formula_text": ")", "formula_coordinates": [10.0, 385.44, 349.14, 4.15, 8.97]}, {"formula_id": "formula_63", "formula_text": "w T 2 Hx 1 = 0,(29)", "formula_coordinates": [10.0, 187.85, 422.11, 201.74, 13.03]}, {"formula_id": "formula_64", "formula_text": "{H i \u2208 C 2\u00d73 } n i=1 . Then each image pair (x 1 , w 2 ) has to satisfy the multibody homography constraint n i=1 (w T 2 H i x 1 ) = \u03bd n (w 2 ) T H\u03bd n (x 1 ) = 0,(30)", "formula_coordinates": [10.0, 43.59, 470.5, 346.01, 64.39]}, {"formula_id": "formula_65", "formula_text": "N \u2265 M n (2)M n (3) \u2212 (M n (3)+1)/2 \u223c O(n 3 )", "formula_coordinates": [11.0, 202.04, 45.95, 187.56, 12.08]}, {"formula_id": "formula_66", "formula_text": "j \u223c \u2202\u03bd n (w 2 ) T H\u03bd n (x 1 ) \u2202x 1 (x1,w2)=(x j 1 ,w j 2 ) \u2208 CP 2 (31)", "formula_coordinates": [11.0, 119.84, 175.61, 269.75, 28.43]}, {"formula_id": "formula_67", "formula_text": "N \u2265 M n (3) \u2212 1 complex epipolar lines { j } N", "formula_coordinates": [11.0, 43.77, 215.71, 345.83, 21.92]}, {"formula_id": "formula_68", "formula_text": "w T H i \u223c \u2202\u03bd n (w) T H\u03bd n (x) \u2202x x=ei \u2208 CP 2 , \u2200w \u2208 C 2 . (32", "formula_coordinates": [11.0, 102.48, 364.07, 282.95, 26.77]}, {"formula_id": "formula_69", "formula_text": ")", "formula_coordinates": [11.0, 385.43, 372.32, 4.16, 8.97]}], "doi": ""}