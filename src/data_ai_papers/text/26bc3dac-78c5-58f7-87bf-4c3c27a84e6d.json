{"title": "Don't Generate, Discriminate: A Proposal for Grounding Language Models to Real-World Environments", "authors": "Yu Gu; Xiang Deng; Yu Su", "pub_date": "", "abstract": "A key missing capacity of current language models (LMs) is grounding to real-world environments. Most existing work for grounded language understanding uses LMs to directly generate plans that can be executed in the environment to achieve the desired effects. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LMs. We propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability. Pangu consists of a symbolic agent and a neural LM working in a concerted fashion: The agent explores the environment to incrementally construct valid plans, and the LM evaluates the plausibility of the candidate plans to guide the search process. A case study on the challenging problem of knowledge base question answering (KBQA), which features a massive environment, demonstrates the remarkable effectiveness and flexibility of Pangu: A BERT-base LM is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains. Pangu also enables, for the first time, effective few-shot in-context learning for KBQA with large LMs such as Codex. 1 Databases Knowledge Bases Tables Web Pages Apps Physical World", "sections": [{"heading": "Introduction", "text": "Language models (LMs) such as BERT (Devlin et al., 2019), GPT-3 (Brown et al., 2020), and Codex (Chen et al., 2021a) have demonstrated an extraordinary capacity in understanding and generating both natural language (Minaee et al., 2021;Liang et al., 2022) and generic programs (e.g., Python) Jain et al., 2022;Austin et al., 2021). The recent release of ChatGPT and similar large LMs is elevating this paradigm to a new level. It seems to point us towards a future where natural language serves as a universal device, powered by LMs, for automated problem solving and interacting with the (computing) world. However, a key missing piece in realizing this future is the connection between LMs and realworld environments, including both digital environments (e.g., databases, knowledge bases, Excel spreadsheets, software, websites, among others) and physical environments (e.g., instruction following robots (Shridhar et al., 2020;Ahn et al., 2022)). Such environments are where many real problems lie. For example, a biologist may need to find all the species of a certain butterfly genus and their geographic distribution from a biology knowledge base, a local grocery store owner may want to visualize the historical sales of different item categories in Excel to decide what and how much to restock before the holiday season, and a physician may need to find patients with specific conditions in a large database of electronic medical records to inform the current diagnosis. How can LMs enable solving all these problems, which involve seeking information or taking actions in a specific environment, with natural language?\nEach environment is a unique context for interpreting natural language requests from users. Grounding, i.e., linking of (natural language) con-cepts to contexts (Chandu et al., 2021), therefore becomes the fundamental problem. More precisely, we need to produce a plan that can be executed in an environment to achieve the desired effects of the corresponding language request. When a plan is described in a formal language (e.g., SQL for relational databases (Yu et al., 2018) or APIs for web services (Su et al., 2017;Andreas et al., 2020)), it is also called a program. The unique challenge of such grounded language understanding problems stems from 1) the vast heterogeneity of environments and their planning languages (e.g., SQL, GraphQL/REST APIs, \u03bb-calculus, and robot planning languages), and 2) the vast, oftentimes infinite, number of possible instantiations (or states) of each environment. Some environments can also be dynamic, e.g., a database that is constantly updated or a physical environment with moving objects.\nMost existing methods for grounded language understanding follow the popular sequence-tosequence framework (Sutskever et al., 2014;Cho et al., 2014) and generate the plans/programs in an autoregressive fashion Ye et al., 2022;Song et al., 2022a). A core thesis of this paper is that directly generating plans may not be the optimal way of using LMs for grounded language understanding. It requires LMs to have intimate knowledge about each specific planning language and environment, neither of which may be part of an LM's pre-training, to ensure the grammaticality and faithfulness of the generated plans. 2 The infinite and dynamic environment states also reduce the potential effectiveness of pre-training for improving faithfulness, even if one manages to do so. Furthermore, autoregressive generation with a neural LM lacks fine-grained control over planning; it is cumbersome, though not impossible, to factor preferences, business logic, and other values and constraints into the plan generation process. A focus of recent work is to alleviate (some of) these limitations by augmenting autoregressive generation with environment-specific pretraining (Yu et al., 2021;Deng et al., 2021) or constrained decoding (Scholak et al., 2021;Shin et al., 2021;. However, the fundamental challenges still largely remain.\nMathematically, an LM is simply a joint distribu-tion p(x 1 , x 2 , ..., x n ) that factors as a product of conditional distributions n i=1 p(x i |x 1 , ..., x i\u22121 ). Existing work leverages the conditional distribution formulation to generate the plan. It thereby casts the burden of ensuring grammaticality, faithfulness, and controllability all on the LM. The main proposal of this paper is to disentangle LMs from these responsibilities and let LMs be what they originally are-a model that assigns a probability to a sequence of tokens. In other words, we advocate for using the joint distribution formulation of LMs to evaluate the plausibility of (utterance, candidate plan) pairs instead of directly generating the plan.\nTo this end, we propose Pangu, a generic framework for grounded language understanding that capitalizes on the discriminative ability of LMs instead of their generative ability (Figure 1). 3 Pangu consists of a symbolic agent and a neural LM working in a concerted fashion. The symbolic agent explores the environment to propose candidate plans, which are guaranteed by design to be both grammatical and faithful. For most real-world environments, due to the size of the search space or partial observability, it is necessary for the agent to search in the environment and incrementally extend or refine the plans. The LM plays a key role in this search process-it evaluates the candidate (partial) plans at each search step and guides the agent towards promising search directions; it also determines when the search ends. Finally, it is also easier to control the search process of a symbolic agent than the generation process of a neural LM.\nAs a case study, we instantiate the proposed framework for complex question answering over knowledge bases (KBQA). KBQA provides an ideal testbed for grounded language understanding because of its massive environment-direct generation with LMs often fails dramatically (Gu et al., 2021). We show that simply using BERT-base with Pangu is sufficient for setting a new record on standard KBQA datasets, and larger LMs further bring substantial gains. Pangu also enables, for the first time, few-shot KBQA by prompting large language models (e.g., Codex): Using only 10 labeled examples, it outperforms all prior methods on GRAPHQ (Su et al., 2016). It provides unprecedented uniformity for using LMs-one can easily plug encoder-only LMs, encoder-decoder LMs, or decoder-only LMs into Pangu, through either fine-tuning or in-context learning. These results highlight the remarkable effectiveness and flexibility of Pangu and validate the proposal of using LMs for discrimination instead of generation.", "publication_ref": ["b21", "b43", "b40", "b2", "b54", "b0", "b12", "b73", "b59", "b1", "b62", "b17", "b41", "b57", "b24", "b20", "b51", "b53", "b24", "b60"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Generation for Grounded Language Understanding", "text": "The Seq2Seq framework (Sutskever et al., 2014;Bahdanau et al., 2015) has been the de facto choice for grounded language understanding, where the LM directly generates a plan given an input utterance. However, the lack of grounding during pretraining makes generating valid plans from LMs challenging. Recent studies endeavor to alleviate this issue via input augmentation or constrained decoding. For input augmentation, the environment (or some relevant portion of it) is fed to the LM's encoder together with the utterance (Hwang et al., 2019;Wang et al., 2020;. Such methods rely on the LM to understand the interplay between the language requests and the environment and correctly factor that into plan generation. They therefore require substantial training data to learn and also provide no guarantee for grammaticality or faithfulness. In contrast, constrained decoding methods regulate the decoder's behavior to guarantee grammaticality (Scholak et al., 2021;Shu et al., 2022) or even faithfulness (Liang et al., 2017;. However, such uses still cast the burden of generating valid plans on the LM itself; controlling the generation process of an LM can be difficult and specific to each planning language and/or environment. In our proposal, the LM is only used to discriminate valid plans proposed by an agent through a controllable search process. More detailed comparison is presented in \u00a75.3.", "publication_ref": ["b62", "b3", "b28", "b63", "b51", "b55", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Few-Shot Grounded Language Understanding with LLMs", "text": "Large language models (LLMs) (Brown et al., 2020;Chen et al., 2021a) have demonstrated strong few-shot learning capabilities in various tasks, from writing programs to query structured and unstructured data (Austin et al., 2021;Rajkumar et al., 2022;, interacting with online websites (Gur et al., 2022;Nakano et al., 2021), to generating procedural plans and guiding embodied agents in virtual environments (Singh et al., 2022;Ahn et al., 2022;Shah et al., 2022;Song et al., 2022b). Most existing work still capitalizes on the generative ability of LLMs. A common strategy to encourage an LLM to produce valid plans is to directly describe the environment in the LLM's context (i.e., input augmentation), which is difficult for complex environments like KBs. A concurrent work of ours (Li et al., 2023b) asks the LLM to directly generate a proxy plan from the input question without the environment description, which is then used to retrieve a valid plan from a set of candidate plans. However, this design is tailored specifically to the KB query language and is limited to generating plans with at most two hops due to the combinatorial explosion in their candidate enumeration. In contrast, Pangu shields the LLM from the complexity of the environment and lets the LLM focus on evaluating the plausibility of candidate plans proposed by an agent. One interesting related work is Ahn et al. (2022), where an LLM is used to score atomic action (skill) proposals, which are guaranteed to conform to affordance constraints, from an embodied agent. Pangu shares a similar spirit of using LMs for discrimination, but we support more complex plans through a search process in the environment guided by an LM.", "publication_ref": ["b8", "b2", "b48", "b56", "b0", "b52", "b58", "b37", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Bottom-Up Semantic Parsing", "text": "Our instantiation of Pangu on KBQA is closely connected to bottom-up semantic parsing, particularly SmBoP (Rubin and Berant, 2021), a text-to-SQL model that iteratively constructs a complex plan from a set of subplans. Pangu similarly constructs a complex plan incrementally from smaller subplans, but it makes the following main departures. First, SmBoP requires all ingredients (i.e., column headers, table names, and DB values) at the beginning of parsing. This assumption does not generally hold for more complex or partially observable environments, where ingredients need to be discovered through search. In our method, only topic entities are needed as the initial plan, which can be readily obtained using an entity linker . Second, our scoring function is based on a straightforward application of LMs, while SmBoP uses a more intricate architecture with extra parameters. Also related is an array of earlier KBQA methods that adopt an enumerate-and-rank approach (Yih et al., 2015;Gu et al., 2021;Ye et al., 2022). Because they try to enumerate all candidate plans up front, the maximum plan complexity is bound to be small. Our adaptive search process allows for flexible construction of more complex plans.", "publication_ref": ["b50", "b69", "b24", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Knowledge Base", "text": "What is the latest released computer emulator developed in Java?  Figure 2: (a) An illustration of how an agent collaborates with an LM to incrementally produce a complex target plan over a KB using beam search (beam size = 1 in this example). At each step, the agent extends the current plans based on the environment to produce new candidate plans. An LM then scores the candidate plans and returns the top-ranked ones. The search process terminates when there is no candidate plan that scores higher than the current best plan (e.g., 4a-c are all worse than 3c). (b) Using different LMs (left: BERT, right: Codex) to evaluate the plausibility of plan 2a. It resembles using LMs for semantic matching between the utterance and the plan.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "An overview of the Pangu framework is presented in Algorithm 1. An overarching assumption of Pangu is that a complex plan can be incrementally constructed by an agent through its exploration in an environment. Such an agent can be a robot doing household tasks in a physical environment (Shridhar et al., 2020), or a virtual agent that orchestrates API calls of different web services (Andreas et al., 2020) or traverses a database/KB (Yu et al., 2018;. Starting from a set of initial plans P 0 (may be empty), at each step, the agent interacts with the environment E to extend the current plans into a new set of candidate plans (line 4). The candidate plans are guaranteed to be valid (i.e., both grammatical and faithful). An LM then scores the candidate plans, and the top K (the beam size) plans are retained for further exploration in the next step (line 5). The same procedure loops until a termination check is passed (line 6); the best plan is then returned.\nPangu mainly shines in that a symbolic agent explores the environment to propose valid plans and shields the LM from having to handle the large search space for valid plan generation. Instead, the LM only focuses on evaluating the plausibility of the proposed plans. An LM can be easily finetuned to excel at this assignment, or, in the case of LLMs such as Codex, they come with such ability out of the box, which enables few-shot in-context learning. Pangu is a generic framework and can potentially accommodate many grounded language understanding tasks by instantiating the various functions in Algorithm 1 accordingly. Next, we discuss our instantiation on KBQA. More discussion on Pangu's applicability to other tasks, with preliminary results, can be found in Appendix A.", "publication_ref": ["b54", "b1", "b73"], "figure_ref": [], "table_ref": []}, {"heading": "KBQA: Preliminaries", "text": "Without loss of generality, we use KBs as our target environment and the KBQA task as a concrete example for ease of discussion. It is an ideal testbed because of the massive environment provided by modern KBs (e.g., FREEBASE (Bollacker et al., 2008) contains 45 million entities and 3 billion facts for over 100 domains), which makes grounding particularly challenging. Given a KB K \u2282 E \u00d7 R \u00d7 (E \u222a L \u222a C), where C is a set of classes, E a set of entities, L a set of literals and R a set of binary relations, the task of KBQA is to find a set of answer entities to an input utterance in the KB. KBQA is typically modeled as semantic parsing , where the utterance is mapped to an executable program/plan in a certain formal language (e.g., SPARQL, \u03bb-calculus, or Sexpression) whose denotation is the answer. We use S-expressions (Gu et al., 2021) for its compactness. An example is shown in Figure 2.", "publication_ref": ["b7", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Candidate Plan Enumeration", "text": "To handle the large search space, the agent casts the task as a step-wise decision-making problem.\nA plan for KBQA can be decomposed into a nested sequence of subplans  (Figure 2). The length of a plan is defined as the number of atomic subplans it contains.\nFor KBQA, P 0 can be a set of entity proposals (e.g., {Java}) obtained using off-the-shelf entity linkers . At step t, the agent considers P t\u22121 , the length t \u2212 1 plans, and decides how to further extend them into C t , the valid plans of length t, based on the environment. This often involves executing the current plans in the environment. Consider the example in Figure 2 at t = 1, the agent finds all the relations connected to Java and enumerates all the length-1 valid plans. The LM scores the candidate plans and prunes all but the top-ranked plan because beam size is 1. At t = 2, the agent executes plan 1c to get its denotation (i.e., a set of entities) in the KB, based on which the agent further discovers the relations and classes (e.g., ComputerEmulator, ComputerSoftware, and ReadBy) connected to those entities to form valid length-2 plans. All the plans produced in this process are guaranteed to be valid. See Appendix B for a more detailed discussion of this process.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LM-Based Scoring", "text": "After the agent enumerates a set of candidate plans, an LM assists with its decision making by evaluating the plausibility of each candidate plan. The interface for evaluating a plan using LMs resembles using LMs for semantic matching: Given a pair of (u : utterance, c \u2208 C t : candidate plan), an LM acts as a scoring function: s(u, c) \u2192 R, which indicates to what extent the candidate plan matches the intent of the utterance. The plausibility of a candidate oftentimes can be indicated by simple linguistic cues, e.g., ComputerEmulator in 2a might be a strong indicator (Figure 2(a)).\nWe follow the common practice of using LMs for semantic matching. For encoder-only LMs like BERT, we directly get a score from the representation of the [CLS] token (Figure 2(b)). For encoderdecoder LMs like T5 (Raffel et al., 2020), we follow Zhuang et al. (2022) to feed both the utterance and the candidate plan to the encoder and let the decoder decode only for one step. The decoding probability over an token that is unused during pretraining is then repurposed as a proxy for matching score. 4 For decoder-only LMs like Codex, we model the score as the probability of generating the candidate plan conditioned on the utterance, i.e., P (c|u). Intuitively, a good scoring function s should respect the following partial order:\ns(u, c 1 ) > s(u, c 2 ), \u2200c 1 \u2208 G t and \u2200c 2 \u2208 G t\u22121 , s(u, c 1 ) > s(u, c 2 ), \u2200c 1 \u2208 G t and \u2200c 2 \u2208 C t \\G t , s(u, c \u2032 ) > s(u, c i ), \u2200c i \u0338 = c \u2032\nwhere G t is the set of gold (sub-)plans at step t (i.e., length-t subplans of the target plan), C t \\G t is the set of length-t candidate plans except the gold (sub-)plans, and c \u2032 is the target plan.\nIn other words, a gold subplan should be scored higher than (1) any negative (i.e., not gold) plans at the same step (e.g., 2a should be scored higher than 2c), because they contain information irrelevant to u, and (2) any gold sub-plans of length < t (e.g., 2a should be scored higher than 1c) because they are less complete. In addition, c \u2032 should be scored higher than any other plan.", "publication_ref": ["b47", "b74"], "figure_ref": [], "table_ref": []}, {"heading": "Termination Check", "text": "Assuming the LM can assign reasonable scores to candidate plans following the above partial order, we can naturally define the condition for termination in Algorithm 1: It terminates if the highest score of candidate plans at step t is lower than the highest score of candidate plans at step t\u22121, which, ideally, should indicate no reachable candidate plan of length \u2265 t is better than the plans at step t \u2212 1, and thus the search process terminates.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning", "text": "We discuss the learning procedure for both finetuning LMs (e.g., BERT and T5) and in-context learning with LLMs (e.g., Codex). For both settings, we use pairs of utterances and gold plans for supervision.\nFine-tuning. Given a gold plan of length T , we first derive its gold sub-plans G t of each step t \u2264 T (e.g., 1c for step 1 and 2a for step 2 in Figure 2). Fine-tuning proceeds with beam search similar to the test-time behavior, but with bottom-up teacher forcing (Williams and Zipser, 1989;Rubin and Berant, 2021), i.e., the gold plans of the current step should always be inserted into the beam. At each step of beam search, we get the probability of each candidate plan c \u2208 C t with softmax over the scores: p(c) = softmax{s(u, c)} c\u2208Ct\u222aG t\u22121 . G t\u22121 is also included here to encourage LMs to explicitly learn the partial order by minimizing the loss:\n\u2212 1 Z T +1 t=1 c\u2208Ctp (c)log p(c)\nwhere Z is the total number of summed items, and p(c) equals 1 if c \u2208 G t and 0 elsewise. Note that, for the T + 1 step, we let G T +1 = G T . This additional step aims to enforce the third condition in the partial order. Our objective is essentially a listwise learning-to-rank objective based on the cross entropy (Cao et al., 2007).\nIn  (Berant et al., 2013) with program annotations.\nThe gold programs for all three datasets are provided in S-expressions , which can be determinstically converted into SPARQL queries to get final execution results.", "publication_ref": ["b66", "b50", "b10", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Baselines", "text": "We mainly compare Pangu with state-of-theart baselines that use LMs as a generative model, including ArcaneQA , TIARA (Shu et al., 2022), DecAF , and RnG-KBQA (Ye et al., 2022). Constrained decoding (i.e., ArcaneQA and TIARA) and input augmentation (i.e., TIARA, DecAF) are used to enhance plan generation. Also, the last three models use a combination of LMs for multiple purposes (i.e., retrieval/ranking/decoding). In addition, we also compare with UnifiedSKG . UnifiedSKG assumes a set of schema items are provided as input, where the gold schema items are always included and the number of negative schema items is restricted to 20 for GRAILQA. It is thus a less fair comparison for other methods, but we include it anyway because it is a representative way of autoregressive plan generation using an LLM. Compared with the baselines, Pangu requires no extra parameter, no modification to the LM, and no need to combine multiple LMs. Pangu provides unprecedented uniformity of using LMs of different nature. More details on baselines can be found in Appendix C.2.", "publication_ref": ["b55", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "For the fine-tuning experiments, we experiment with BERT-base, T5-base, T5-large, and T5-3B, and use the full training set of each dataset for finetuning. For the in-context learning experiments, we experiment with Codex. 5 We randomly sample 10/100/1,000 training examples from each dataset and use that as the pool for dynamic retrieval. During inference, for each test example, we retrieve   , which is a slightly smaller subset. All baselines after 2020 are trained using gold programs in S-expressions.\n10 in-context examples from the pool using BM25based utterance similarity. We use entity linking results from off-the-shelf entity linkers. More details on implementations can be found in Appendix C.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "Fine-tuning results.  Fine-grained performance decomposition by question complexity can be found in Appendix D, which show that Pangu works well across questions of different complexity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sample Efficiency Analysis", "text": "Intuitively, by using LMs for discrimination instead of generation, the task becomes easier for LMs and thus improves their sample efficiency. Our sample efficiency experiments in Figure 3 confirm this hypothesis. We downsample GRAILQA's training data and randomly sample 1, 10, 100, and 1,000   ArcaneQA. ArcaneQA is the only open-source baseline that uses constrained decoding to enforce the validity of predicted plans. There are two main reasons for Pangu's superiority. First, though constrained decoding can also help ensure plan validity, the autoregressive decoder operates with token-level local normalization and thus lacks a global view. As a result, local failures may break its predictions. For example, a wrong local prediction (e.g., function name) by ArcaneQA leads to catastrophic errors (Table 2). By evaluating candidate plans instead of candidate tokens, Pangu has a more global view and is less likely to make such local errors. Second, Pangu is less susceptible to overfitting and thus achieves better performance in non-i.i.d. settings. Pangu does not learn to generate a plan; instead, it learns to evaluate the plausibility of utterance-plan pairs. Such knowledge is more transferable. An interesting observation is shown in Figure 4, where Pangu's output probability distributions are consistent across programs seen and unseen in training. For ArcaneQA, however, there is a drastic shift from seen to unseen. This is also consistent with prior findings that autoregressive models tend to overfit seen structures during training by Bogin et al. (2022). It makes non-i.i.d. generalization more difficult. We also conduct an error analysis in Appendix E, which sheds some light on future improvements.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Conclusions", "text": "In this paper, we proposed to capitalize on the discriminative ability of language models (LMs) instead of their generative ability for grounded language understanding. Building on this proposal, we proposed a generic framework, Pangu, which consists of a symbolic agent and a neural LM working in a concerted fashion and creates a better separation between the realm of the neural and the symbolic. This work opens the door for developing versatile and sample-efficient grounded language understanding systems that fully capitalize on the language understanding ability of LMs while avoiding their limitations. It also sheds light on developing better neuro-symbolic systems in general.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Despite the strong performance of Pangu, we identify several limitations that call for further improvement. The first major limitation lies in efficiency. Because Pangu requires an LM to iteratively score candidate plans, it is resource-consuming in terms of both time and computing. Compared with Ar-caneQA, which efficiently handles complex questions in KBQA, Pangu is about twice as slow for both training and inference and consumes about twice as much GPU memory when using the same LM. Concretely, to predict a plan of L tokens, generation-based methods involve using an LM to do L forward passes. For Pangu, the number of forward passes is proportional to the number of candidate plans, which can range widely. In the future, algorithms with complexity better than O(N ), N being the number of candidate plans, are desired to find the top-K candidates. That being said, we would like to note that both ArcaneQA and Pangu are more efficient than most existing methods due to their efficient dynamic search design. For example, Pangu is 8 times faster than RnG-KBQA, according to the numbers reported in . Nonetheless, we list efficiency as a limitation because there is clear potential for further improvement.\nSecond, though Pangu has shown some promising results with Codex, the true potential of enabling few-shot grounded language understanding with Pangu has yet to be fully realized. We only experiment with a straightforward scoring function and have not experimented with different prompt designs systematically. In the future, we plan to try different prompt designs, retrievers, and scoring functions, including using latest techniques like chain-of-thought prompting (Wei et al., 2022).\nThird, though orthogonal to the general framework of our proposal, in our current instantiation, we assume gold plans for training. However, gold plans can be expensive to collect for some environments. Exploring fine-tuning LMs with weak supervision can be an interesting direction. In addition to proposing candidate plans to the LM, the agent may also respond to the LM with rewards based on its decisions (Liang et al., 2017).\nFinally, one important merit of Pangu, controllability, is under-explored in this paper, because it is not very necessary for KBQA. While for tasks like text-to-SQL parsing, controllability could be a highly desirable property. Intruders may manipulate text-to-SQL models to launch database attacks via SQL injection . With Pangu, we can easily get rid of malicious SQL operations in candidate enumeration. However, for generationbased methods, such controls are hard to achieve during generation because the decoding process can be shortsighted-it is difficult to tell whether the current predicted token would lead to a malicious operation several steps later. We leave exploration on Pangu's controllability to future work.", "publication_ref": ["b65", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "LLMs are no longer just a laboratory curiosity; they are being used in real-world systems to interact with real-world environments (both digital and physical). To ensure successful deployment of LLMs in these scenarios, it is essential to improve their controllability, as failure to do so could lead to catastrophic results. In digital environments, such as databases, unexpected behavior could lead to safety issues with a company's data and property. In physical environments, it could even put human life at risk. Pangu is proposed to provide better controllability for LLMs when being depolyed to interact with different environments. Specifically, safety considerations can be explicitly incorporated into the agent's candidate proposal (the symbolic part of Pangu) for enhanced security (i.e., harmful actions are directly excluded from the candidates pool).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendices", "text": "In this supplementary material, we provide further details as follows:\n\u2022 Appendix A: Broader Applicability of Pangu ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Broader Applicability of Pangu", "text": "Algorithm 1 describes a generic framework for grounded language understanding, but the concrete implementation for the functions in Algorithm 1 may vary for different tasks. We have shown a representative instantiation for KBQA. In this section, we briefly discuss the possible instantiation for two other tasks of different nature. In addition, we also present some preliminary results we have obtained to demonstrate the feasibility of Pangu.\nA.1 Text-to-SQL Parsing Possible Instantiation. Similar to KBQA, Textto-SQL parsing also aims to map a natural language utterance into a program that can be executed over a relational database (instead of a KB). When the database schema is reasonably small (generally true for existing datasets like Spider (Yu et al., 2018)), we can define P 0 as the set of all schema items (i.e., column headers and table names) plus the set of cell values mentioned in the utterance, which should be straightforward to identify (e.g., with string matching). In this way, the agent can construct candidate programs similarly to Rubin and Berant (2021). 6 The termination check for text-to-SQL parsing can also be implemented similarly.\nPreliminary Results. We test Pangu on Spider, a popular benchmark on Text-to-SQL parsing, following the aforementioned instantiation. Due to the smaller environment of relational databases (compared with KBs), schema linking performance on 6 One necessary step is to convert a SQL query into an algebra tree (Codd, 1970), similar to what is done by Rubin and Berant (2021). In this way, the agent can more easily enumerate the candidate programs in a bottom-up manner. When the database schema is too large, we may define P0 only as the set of mentioned cell values, then the process of candidate enumeration will resemble KBQA (i.e., cell values can be treated as entities in the KB.)\nSpider is already at 99% (Li et al., 2023a). We therefore start the search assuming ingredients have been identified (i.e., the initial plan). With the same language model being used (i.e., CodeBERT (Feng et al., 2020)), we achieved 70.6% Exact-Match on Spider dev, comparable to SmBop's 71.7% (Rubin and Berant, 2021), a strong bottom-up parser baseline, when using the same LM. Note that these are only preliminary results to demonstrate the feasibility of applying Pangu to other tasks. There is still a large room to improve by, e.g., optimizing the search process or using stronger LMs.", "publication_ref": ["b73", "b50", "b18", "b50", "b35", "b23", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Interacting with Real-World Environments", "text": "Possible Instantiation. Pangu can also be used for guiding bots that interact with real-world environments, both online websites (Gur et al., 2022;Nakano et al., 2021) and physical environments through embodied agents (Shridhar et al., 2020). Given a complex task to be accomplished in the environment, an agent may decompose it into a sequence of subplans (e.g., making a cup of coffee entails first finding a cup then picking up the cup, etc.; Song et al. (2022b)), and combine it with all executable actions in the environment to enumerate the candidate plans and select the best action with an LM. One difference in these cases is that realworld environments often contain information from multiple modalities, thus requiring multi-modal language models (Li et al., 2019;Lu et al., 2019) that are capable of jointly handling textual, visual, and other modalities. More concretely, let us consider the task of embodied instruction following, on the popular AL-FRED dataset (Shridhar et al., 2020). We use LMs as high-level planners for the embodied agent. For example, for a command like \"make me a cup of coffee\", a high-level plan like [Navigate cup, PickUp cup, Navigate coffee_maker, . . . ] is first generated. The agent is equipped with an object detector and a low-level planner to execute the high-level plan from the LM (e.g., navigating to a cup is a classic object localization problem handled by the lowlevel planner). At each search step, the agent generates a candidate (high-level action, object) pair for each object observed in the environment as possible extensions of the current plan. The LM then scores the candidate expansions similar to KBQA; the best one is executed by the low-level planner.\nPreliminary Results. We use the object detector \nComposition Rule Signature Comments JOIN R \u00d7 (E \u222a E \u2032 ) \u2192 E \u2032 a single hop along an edge AND (T \u222a E \u2032 ) \u00d7 E \u2032 \u2192 E \u2032 intersection of two sets ARGMAX/ARGMIN (T \u222a E \u2032 ) \u00d7 R \u2192 E \u2032 superlative aggregations LT/LE/GT/GE R \u00d7 E \u2192 E \u2032 < / \u2264 / > / \u2265 COUNT E \u2032 \u2192 N set cardinality", "publication_ref": ["b54", "b58", "b36", "b42", "b54"], "figure_ref": [], "table_ref": []}, {"heading": "B Candidate Enumeration", "text": "Our candidate enumeration for KBQA strictly follows the definition of functions in Table B.1. Specifically, given a set of current plans P t , to construct the candidate set C t+1 , for each plan p i in P t , the agent executes it and gets types and relations that are reachable from the denotation of the plan.\nFor each type t, the agent enumerates (AND t pi) as a candidate. For each relation r, the agent enumerates (JOIN r pi) as a candidate. If the denotation of p i is a numerical value, then four similar candidates with comparatives are also included (LT/LE/GT/GE r pi). In addition, candidate plans with superlatives can be enumerated as (ARGMAX/ARGMIN pi r). Also, (COUNT pi) can always be included to C t+1 . After checking each p i independently, the agent then checks each pair of plans p i and p j from P t , if the execution of p i and p j has an overlap, then (AND pi pj) is also included as a candidate plan. The candidate enumeration process is totally transparent to the LM and can be easily controlled based on different needs.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "C Experimental Setup", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Datasets Statistics", "text": "All three datasets provide gold program annotations. For consistency, we use the converted Sexpressions representation provided by   ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 More Details on Baselines", "text": "Different LMs and decoding strategies are used in the baseline models.\nArcaneQA ) is an encoderdecoder model built on top of a BERT encoder. It leverages constrained decoding and incrementally synthesizes a sequence of subprograms, where the constraints come from both the grammar and the execution of existing subprograms, to enforce grammaticality and faithfulness.\nTIARA (Shu et al., 2022) first uses BERT to retrieve a set of schema items, which are further used as the input, together with the question, to T5 for plan generation. They also apply constrained decoding but only for grammaticality.\nDecAF  similarly retrieves a relevant subgraph from the KB using DPR (Karpukhin et al., 2020), and then input the retrieved items to FiD (Izacard and Grave, 2021), a T5 model finetuned for question answering. RnG-KBQA (Ye et al., 2022) first uses BERT to rank a set of enumerated candidate programs (up to a limited complexity), and then uses T5 to edit the top programs into more complex programs.\nUnifiedSKG ) also retrieves a subgraph from the KB as input to T5. The setting of UnifiedSKG is different from other baselines. It assumes the gold schema items are always included in the retrieved subgraph and restricts the number of negative schema items in the subgraph (i.e., at most 20 schema items for GRAILQA). It is thus a less fair comparison for other methods, but we include it anyway because it is a representative way of autoregressive plan generation using a large LM. A summary of the baselines can be found in Table C.2.", "publication_ref": ["b55", "b32", "b29", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Implementation Details", "text": "For GRAILQA we use the entity linking results from TIARA. For WEBQSP, we get that from ELQ , which is also used by our baseline models. For GRAPHQ, get that from Ar-caneQA. The entity proposals for the input utterance form the initial plans (P 0 ) for our search process. We use beam size 5 for all of our fine-tuning experiments. We run our experiments with T5-3B using a single NVIDIA A100 80GB card, while for all other fine-tuning experiments, we run them using 4\u00d7 NVIDIA A6000 48GB cards.\nFor our experiments with Codex, we use a beam size of 2 and a max number of candidates of 1,000\nIn addition, for in-context learning with Codex (100-shot), we also randomly sample 200 wrong predictions from GRAILQA's dev set. In addition to 22% errors caused by missing entities, the most common errors (25.5%) are due to wrong schema items. Distinguishing gold schema items from confusing ones is challenging for in-context learning. Also, missing constraints (16.5%) and missing relations (10%) are another two major error types, because we use a small batch size (i.e., 2) for Codex and the model tends to prefer short programs. These two error types are also related to wrong termination check. Finally, there are 12% wrong functions. The error types of Pangu w/ Codex are very different from Pangu w/ T5-3B. This is because for a complex task like KBQA, the performance of in-context learning with Pangu still largely lags behind fine-tuning. Particularly, fine-tuning methods directly learn the partial order among programs during training, while Codex needs to implicitly infer a partial order by itself, which is not directly shown in the demonstrations. As a result, Pangu w/ Codex makes more trivial mistakes that fine-tuning methods can easily avoid. More advanced in-context learning techniques to close this gap remains to be explored. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Examples of Prompts", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "The authors would like to thank Percy Liang, Jiawei Han, Jonathan Berant, Huan Sun, and other colleagues from the OSU NLP group for their valuable feedback. The authors would also like to thank Shijie Chen and Chan Hee Song for proof-ofconcept implementation of Pangu on other tasks, Yiheng Shu for sharing their entity linking results, and Tianbao Xie for clarifications on Uni-fiedSKG. This research was supported in part by ARL W911NF2220144, NSF OAC 2112606, and  Ohio Supercomputer Center (Center, 1987).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LMs", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Grounding Strategy Guarantees", "text": "ArcaneQA  BERT-base Constrained Decoding Grammatical+Faithful RnG-KBQA (Ye et al., 2022) BERT-base + T5-base Input Augmentation N/A TIARA (Shu et al., 2022) BERT-base + T5-base Input Augmentation + Constrained Decoding Grammatical DecAF  DPR + FiD-3B Input Augmentation N/A UnifiedSKG  T5-base(/large/3B) Input Augmentation N/A for speed concerns, which to some extent sacrifices the performance. As the first endeavor towards enabling few-shot KBQA with LLMs, we did not tune the hyper-parameters very hard. The only thing we tuned is the scoring function. We tune the scoring function using 10-shot training data from GRAILQA with cross-validation. If we directly use P (c|u) as our scoring function s(u, c) in Section 3.3, Codex tends to favor programs with repeated relations. As a result, we add a penalizing factor to P (c|u), and define s(u, c) as P (c|u) \u00d7 \u03b7 n , where \u03b7 \u2208 [0, 1] is a hyper-parameter, and n is the maximal occurrences of a relation in a program. We set \u03b7 = 0.7 based on cross-validation using the 10 training examples. Finally, a small percentage of questions (around 5%) in GRAPHQ and GRAILQA do not have a topic entity (e.g., \"who is the heaviest film director?\" from GRAILQA, whose target program is (ARGMAX film.director people.person.weight_kg)). For these questions, we use the answer types (e.g., film.director) predicted in  as our initial state P 0 .", "publication_ref": ["b41", "b55"], "figure_ref": [], "table_ref": []}, {"heading": "D Decomposition by Question Complexity", "text": "We present a fine-grained analysis of Pangu with T5-3B and Codex (100-shot) on questions of different complexity, measured by the number of relations in the gold program, in Table D.4. For GRAILQA, we report the performance on its dev set because the test set is hidden. Pangu performs competitively across all complexity. Note that there are only two questions in GRAILQA's dev set with 4 relations, so the results on that may not be indicative. On GRAPHQ, Pangu significantly outperforms ArcaneQA. The F1 of Pangu with T5-3B is almost three times higher than ArcaneQA on questions with 2 and 3 relations. Interestingly, Pangu with Codex also outperforms ArcaneQA considerably on questions with 2 and 3 relations. These findings suggest the superiority of Pangu in generalizing to more complex programs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Error Analysis", "text": "We analyze 200 incorrect predictions (i.e., EM=0) randomly sampled from GRAILQA's dev set for our best model (i.e., T5-3B). The major errors are due to unidentified topic entities during entity linking (62%). 8 Also, Pangu tends to include unrelated entities provided by the entity linker into the final programs (6.5% of the errors), this is because Pangu is fine-tuned with gold entities only, and thus does not learn to handle unrelated entities. In addition, wrong termination check corresponds to 12.5% of the errors, indicating a venue for better enforcing the partial order to Pangu. Apart from these errors, 10.5% of the mistakes are due to ambiguous annotations or annotation errors in GRAILQA. The remaining error types include wrong comparators, answer types, and relations (particularly relations involve a subtle direction like cvg.computer_game_engine.predecessor_engine). # what is the role of opera designer gig who designed the telephone / the medium?  B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Section 4.1; Section C.1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\nThe used datasets are all widely used for this task and there is no report of identifying or offensive content as far as we know B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Section 4.1; Section C.1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. Section C.1 C Did you run computational experiments? Section 5.1; Section 5.2; Section D C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 4.3; Section C.3", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Do as i can, not as i say: Grounding language in robotic affordances", "journal": "CoRR", "year": "2022", "authors": "Michael Ahn; Anthony Brohan; Noah Brown; Yevgen Chebotar; Omar Cortes; Byron David; Chelsea Finn; Keerthana Gopalakrishnan; Karol Hausman; Alex Herzog"}, {"ref_id": "b1", "title": "Task-oriented dialogue as dataflow synthesis", "journal": "Andy Mc-Govern, Aleksandr Nisnevich", "year": "2020", "authors": "Jacob Andreas; John Bufe; David Burkett; Charles Chen; Josh Clausman; Jean Crawford; Kate Crim; Jordan Deloach; Leah Dorner; Jason Eisner; Hao Fang; Alan Guo; David Hall; Kristin Hayes; Kellie Hill; Diana Ho; Wendy Iwaszuk; Smriti Jha; Dan Klein; Jayant Krishnamurthy; Theo Lanman; Percy Liang; Christopher H Lin; Ilya Lintsbakh"}, {"ref_id": "b2", "title": "", "journal": "", "year": "2021", "authors": "Jacob Austin; Augustus Odena; Maxwell I Nye; Maarten Bosma; Henryk Michalewski; David Dohan; Ellen Jiang; Carrie J Cai; Michael Terry; Quoc V Le; Charles Sutton"}, {"ref_id": "b3", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015-05-07", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b4", "title": "Semantic parsing on Freebase from question-answer pairs", "journal": "", "year": "2013", "authors": "Jonathan Berant; Andrew Chou; Roy Frostig; Percy Liang"}, {"ref_id": "b5", "title": "Animesh Garg, and Yoav Artzi. 2022. A persistent spatial semantic representation for high-level natural language instruction execution", "journal": "PMLR", "year": "", "authors": "Valts Blukis; Chris Paxton; Dieter Fox"}, {"ref_id": "b6", "title": "Unobserved local structures make compositional generalization hard", "journal": "", "year": "2022", "authors": "Ben Bogin; Shivanshu Gupta; Jonathan Berant"}, {"ref_id": "b7", "title": "Freebase: a collaboratively created graph database for structuring human knowledge", "journal": "ACM", "year": "2008-06-10", "authors": "Kurt D Bollacker; Colin Evans; Praveen K Paritosh; Tim Sturge; Jamie Taylor"}, {"ref_id": "b8", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "2020-12-06", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b9", "title": "Program transfer for answering complex questions over knowledge bases", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Shulin Cao; Jiaxin Shi; Zijun Yao; Xin Lv; Jifan Yu; Lei Hou; Juanzi Li; Zhiyuan Liu; Jinghui Xiao"}, {"ref_id": "b10", "title": "Learning to rank: from pairwise approach to listwise approach", "journal": "ACM", "year": "2007-06-20", "authors": "Zhe Cao; Tao Qin; Tie-Yan Liu; Ming-Feng Tsai; Hang Li"}, {"ref_id": "b11", "title": "", "journal": "Ohio supercomputer center", "year": "1987", "authors": " Ohio Supercomputer;  Center"}, {"ref_id": "b12", "title": "Grounding 'grounding' in NLP", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Yonatan Khyathi Raghavi Chandu; Alan W Bisk;  Black"}, {"ref_id": "b13", "title": "Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis", "journal": "", "year": "", "authors": "Mark Chen; Jerry Tworek; Heewoo Jun; Qiming Yuan; Henrique Ponde De Oliveira Pinto; Jared Kaplan; Harrison Edwards; Yuri Burda; Nicholas Joseph; Greg Brockman; Alex Ray; Raul Puri; Gretchen Krueger; Michael Petrov; Heidy Khlaaf; Girish Sastry; Pamela Mishkin; Brooke Chan; Scott Gray; Nick Ryder; Mikhail Pavlov; Alethea Power; Lukasz Kaiser; Mohammad Bavarian; Clemens Winter"}, {"ref_id": "b14", "title": "Ilya Sutskever, and Wojciech Zaremba. 2021a. Evaluating large language models trained on code", "journal": "", "year": "", "authors": "Jan Carr; Joshua Leike; Vedant Achiam; Evan Misra; Alec Morikawa; Matthew Radford; Miles Knight; Mira Brundage; Katie Murati; Peter Mayer; Bob Welinder; Dario Mcgrew; Sam Amodei;  Mccandlish"}, {"ref_id": "b15", "title": "ReTraCk: A flexible and efficient framework for knowledge base question answering", "journal": "", "year": "2021", "authors": "Shuang Chen; Qian Liu; Zhiwei Yu; Chin-Yew Lin; Jian-Guang Lou; Feng Jiang"}, {"ref_id": "b16", "title": "Binding language models in symbolic languages", "journal": "CoRR", "year": "2022", "authors": "Zhoujun Cheng; Tianbao Xie; Peng Shi; Chengzu Li; Rahul Nadkarni; Yushi Hu; Caiming Xiong; Dragomir Radev; Mari Ostendorf; Luke Zettlemoyer; Noah A Smith; Tao Yu"}, {"ref_id": "b17", "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merri\u00ebnboer; Caglar Gulcehre; Dzmitry Bahdanau; Fethi Bougares; Holger Schwenk; Yoshua Bengio"}, {"ref_id": "b18", "title": "A relational model of data for large shared data banks", "journal": "Commun. ACM", "year": "1970", "authors": "E F Codd"}, {"ref_id": "b19", "title": "2021. Casebased reasoning for natural language queries over knowledge bases", "journal": "Association for Computational Linguistics", "year": "", "authors": "Rajarshi Das; Manzil Zaheer; Dung Thai; Ameya Godbole; Ethan Perez; Jay Yoon Lee; Lizhen Tan; Lazaros Polymenakos; Andrew Mccallum"}, {"ref_id": "b20", "title": "Structure-grounded pretraining for text-to-SQL", "journal": "", "year": "2021", "authors": "Xiang Deng; Ahmed Hassan Awadallah; Christopher Meek; Oleksandr Polozov; Huan Sun; Matthew Richardson"}, {"ref_id": "b21", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b22", "title": "Learning to paraphrase for question answering", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Li Dong; Jonathan Mallinson; Siva Reddy; Mirella Lapata"}, {"ref_id": "b23", "title": "Code-BERT: A pre-trained model for programming and natural languages", "journal": "", "year": "2020", "authors": "Zhangyin Feng; Daya Guo; Duyu Tang; Nan Duan; Xiaocheng Feng; Ming Gong; Linjun Shou; Bing Qin; Ting Liu; Daxin Jiang; Ming Zhou"}, {"ref_id": "b24", "title": "2021. Beyond I.I.D.: three levels of generalization for question answering on knowledge bases", "journal": "ACM / IW3C2", "year": "2021", "authors": "Yu Gu; Sue Kase; Michelle Vanni; Brian M Sadler; Percy Liang; Xifeng Yan; Yu Su"}, {"ref_id": "b25", "title": "Knowledge base question answering: A semantic parsing perspective", "journal": "", "year": "2022", "authors": "Yu Gu; Vardaan Pahuja; Gong Cheng; Yu Su"}, {"ref_id": "b26", "title": "ArcaneQA: Dynamic program induction and contextualized encoding for knowledge base question answering", "journal": "", "year": "2022", "authors": "Yu Gu; Yu Su"}, {"ref_id": "b27", "title": "Sharan Narang, Noah Fiedel, and Aleksandra Faust. 2022. Understanding HTML with large language models", "journal": "CoRR", "year": "", "authors": "Izzeddin Gur; Ofir Nachum; Yingjie Miao; Mustafa Safdari; Austin Huang; Aakanksha Chowdhery"}, {"ref_id": "b28", "title": "A comprehensive exploration on wikisql with table-aware word contextualization. CoRR, abs", "journal": "", "year": "1069", "authors": "Wonseok Hwang; Jinyeung Yim; Seunghyun Park; Minjoon Seo"}, {"ref_id": "b29", "title": "Leveraging passage retrieval with generative models for open domain question answering", "journal": "", "year": "2021", "authors": "Gautier Izacard; Edouard Grave"}, {"ref_id": "b30", "title": "", "journal": "", "year": "", "authors": "Naman Jain; Skanda Vaidyanath; Arun Shankar Iyer; Nagarajan Natarajan; Suresh Parthasarathy; K Sriram"}, {"ref_id": "b31", "title": "Jigsaw: Large language models meet program synthesis", "journal": "ACM", "year": "2022-05-25", "authors": "Rahul Rajamani;  Sharma"}, {"ref_id": "b32", "title": "Dense passage retrieval for opendomain question answering", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih"}, {"ref_id": "b33", "title": "Query graph generation for answering multi-hop complex questions from knowledge bases", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Yunshi Lan; Jing Jiang"}, {"ref_id": "b34", "title": "Efficient one-pass end-to-end entity linking for questions", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Belinda Z Li; Sewon Min; Srinivasan Iyer; Yashar Mehdad; Wen-Tau Yih"}, {"ref_id": "b35", "title": "Decoupling the skeleton parsing and schema linking for text-to-sql", "journal": "CoRR", "year": "2023", "authors": "Haoyang Li; Jing Zhang; Cuiping Li; Hong Chen"}, {"ref_id": "b36", "title": "VisualBERT: A simple and performant baseline for vision and language", "journal": "", "year": "1908", "authors": "Liunian Harold Li; Mark Yatskar; Da Yin; Cho-Jui Hsieh; Kai-Wei Chang"}, {"ref_id": "b37", "title": "Few-shot in-context learning for knowledge base question answering", "journal": "CoRR", "year": "2023", "authors": "Tianle Li; Xueguang Ma; Alex Zhuang; Yu Gu; Yu Su; Wenhu Chen"}, {"ref_id": "b38", "title": "Competition-level code generation with alphacode", "journal": "Koray Kavukcuoglu, and Oriol Vinyals", "year": "", "authors": "Yujia Li; David Choi; Junyoung Chung; Nate Kushman; Julian Schrittwieser; R\u00e9mi Leblond; Tom Eccles; James Keeling; Felix Gimeno; Agustin Dal Lago; Thomas Hubert; Peter Choy; Cyprien De Masson D'autume; Igor Babuschkin; Xinyun Chen; Po-Sen Huang; Johannes Welbl; Sven Gowal; Alexey Cherepanov; James Molloy; Daniel J Mankowitz; Esme Sutherland Robson; Pushmeet Kohli"}, {"ref_id": "b39", "title": "Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Chen Liang; Jonathan Berant; Quoc Le; Kenneth D Forbus; Ni Lao"}, {"ref_id": "b40", "title": "", "journal": "", "year": "2022", "authors": "Percy Liang; Rishi Bommasani; Tony Lee; Dimitris Tsipras; Dilara Soylu; Michihiro Yasunaga; Yian Zhang; Deepak Narayanan; Yuhuai Wu; Ananya Kumar; Benjamin Newman; Binhang Yuan; Bobby Yan; Ce Zhang; Christian Cosgrove; Christopher D Manning; Christopher R\u00e9; Diana Acosta-Navas; Drew A Hudson; Eric Zelikman; Esin Durmus; Faisal Ladhak; Frieda Rong; Hongyu Ren; Huaxiu Yao; Jue Wang; Keshav Santhanam; Laurel J Orr; Lucia Zheng; Mert Y\u00fcksekg\u00f6n\u00fcl; Mirac Suzgun; Nathan Kim; Neel Guha; Niladri S Chatterji; Omar Khattab; Peter Henderson; Qian Huang; Ryan Chi; Sang Michael Xie; Shibani Santurkar; Surya Ganguli; Tatsunori Hashimoto; Thomas Icard; Tianyi Zhang; Vishrav Chaudhary; William Wang; Xuechen Li; Yifan Mai; Yuhui Zhang; Yuta Koreeda"}, {"ref_id": "b41", "title": "Uni-Parser: Unified semantic parser for question answering on knowledge base and database", "journal": "CoRR", "year": "2022", "authors": "Ye Liu; Semih Yavuz; Rui Meng; Dragomir Radev; Caiming Xiong; Yingbo Zhou"}, {"ref_id": "b42", "title": "ViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks", "journal": "", "year": "2019-12-08", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"ref_id": "b43", "title": "Deep learning-based text classification: A comprehensive review", "journal": "ACM Comput. Surv", "year": "2021", "authors": "Shervin Minaee; Nal Kalchbrenner; Erik Cambria; Narjes Nikzad; Meysam Chenaghlu; Jianfeng Gao"}, {"ref_id": "b44", "title": "Benjamin Chess, and John Schulman. 2021. We-bGPT: Browser-assisted question-answering with human feedback", "journal": "CoRR", "year": "", "authors": "Reiichiro Nakano; Jacob Hilton; Suchir Balaji; Jeff Wu; Long Ouyang; Christina Kim; Christopher Hesse; Shantanu Jain; Vineet Kosaraju; William Saunders; Xu Jiang; Karl Cobbe; Tyna Eloundou; Gretchen Krueger"}, {"ref_id": "b45", "title": "Episodic transformer for vision-and-language navigation", "journal": "IEEE", "year": "2021-10-10", "authors": "Alexander Pashevich; Cordelia Schmid; Chen Sun"}, {"ref_id": "b46", "title": "On the security vulnerabilities of text-to-sql models", "journal": "CoRR", "year": "2022", "authors": "Xutan Peng; Yipeng Zhang; Jingfeng Yang; Mark Stevenson"}, {"ref_id": "b47", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b48", "title": "Evaluating the text-to-sql capabilities of large language models", "journal": "CoRR", "year": "2022", "authors": "Nitarshan Rajkumar; Raymond Li; Dzmitry Bahdanau"}, {"ref_id": "b49", "title": "Mark Steedman, and Mirella Lapata", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Siva Reddy; Oscar T\u00e4ckstr\u00f6m; Slav Petrov"}, {"ref_id": "b50", "title": "SmBoP: Semiautoregressive bottom-up semantic parsing", "journal": "", "year": "2021", "authors": "Ohad Rubin; Jonathan Berant"}, {"ref_id": "b51", "title": "PICARD: Parsing incrementally for constrained auto-regressive decoding from language models", "journal": "", "year": "2021", "authors": "Torsten Scholak; Nathan Schucher; Dzmitry Bahdanau"}, {"ref_id": "b52", "title": "LM-Nav: Robotic navigation with large pre-trained models of language, vision, and action", "journal": "", "year": "2022", "authors": "Dhruv Shah; B\u0142a\u017cej Osi\u0144ski; Sergey Levine"}, {"ref_id": "b53", "title": "Constrained language models yield few-shot semantic parsers", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Richard Shin; Christopher Lin; Sam Thomson; Charles Chen; Subhro Roy; Adam Emmanouil Antonios Platanios; Dan Pauls; Jason Klein; Benjamin Eisner;  Van Durme"}, {"ref_id": "b54", "title": "ALFRED: A benchmark for interpreting grounded instructions for everyday tasks", "journal": "IEEE", "year": "2020-06-13", "authors": "Mohit Shridhar; Jesse Thomason; Daniel Gordon; Yonatan Bisk; Winson Han; Roozbeh Mottaghi; Luke Zettlemoyer; Dieter Fox"}, {"ref_id": "b55", "title": "TIARA: Multi-grained retrieval for robust question answering over large knowledge base", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Yiheng Shu; Zhiwei Yu; Yuhan Li; B\u00f6rje Karlsson; Tingting Ma; Yuzhong Qu; Chin-Yew Lin"}, {"ref_id": "b56", "title": "Progprompt: Generating situated robot task plans using large language models", "journal": "CoRR", "year": "2022", "authors": "Ishika Singh; Valts Blukis; Arsalan Mousavian; Ankit Goyal; Danfei Xu; Jonathan Tremblay; Dieter Fox; Jesse Thomason; Animesh Garg"}, {"ref_id": "b57", "title": "One step at a time: Long-horizon vision-and-language navigation with milestones", "journal": "", "year": "2022", "authors": "Chan Hee Song; Jihyung Kil; Tai-Yu Pan; M Brian; Wei-Lun Sadler; Yu Chao;  Su"}, {"ref_id": "b58", "title": "LLM-Planner: Few-shot grounded planning for embodied agents with large language models", "journal": "CoRR", "year": "2022", "authors": "Chan Hee Song; Jiaman Wu; Clayton Washington; M Brian; Wei-Lun Sadler; Yu Chao;  Su"}, {"ref_id": "b59", "title": "Building natural language interfaces to web apis", "journal": "ACM", "year": "2017-11-06", "authors": "Yu Su; Ahmed Hassan Awadallah; Madian Khabsa; Patrick Pantel; Michael Gamon; Mark J Encarnaci\u00f3n"}, {"ref_id": "b60", "title": "On generating characteristic-rich question sets for QA evaluation", "journal": "", "year": "2016", "authors": "Yu Su; Huan Sun; Brian Sadler; Mudhakar Srivatsa; Izzeddin G\u00fcr; Zenghui Yan; Xifeng Yan"}, {"ref_id": "b61", "title": "SPARQA: skeleton-based semantic parsing for complex questions over knowledge bases", "journal": "AAAI Press", "year": "2020-02-07", "authors": "Yawei Sun; Lingling Zhang; Gong Cheng; Yuzhong Qu"}, {"ref_id": "b62", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014-12-08", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"ref_id": "b63", "title": "RAT-SQL: Relation-aware schema encoding and linking for textto-SQL parsers", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Bailin Wang; Richard Shin; Xiaodong Liu; Oleksandr Polozov; Matthew Richardson"}, {"ref_id": "b64", "title": "CodeT5: Identifier-aware unified pretrained encoder-decoder models for code understanding and generation", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Yue Wang; Weishi Wang; Shafiq Joty; Steven C H Hoi"}, {"ref_id": "b65", "title": "Chain of thought prompting elicits reasoning in large language models", "journal": "", "year": "2022", "authors": "Jason Wei; Xuezhi Wang; Dale Schuurmans; Maarten Bosma; Fei Xia; Ed H Chi; V Quoc; Denny Le;  Zhou"}, {"ref_id": "b66", "title": "A learning algorithm for continually running fully recurrent neural networks", "journal": "Neural Comput", "year": "1989", "authors": "Ronald J Williams; David Zipser"}, {"ref_id": "b67", "title": "Unified-SKG: Unifying and multi-tasking structured knowledge grounding with text-to", "journal": "", "year": "2022", "authors": "Tianbao Xie; Chen Henry Wu; Peng Shi; Ruiqi Zhong; Torsten Scholak; Michihiro Yasunaga; Chien-Sheng Wu; Ming Zhong; Pengcheng Yin; Sida I Wang; Victor Zhong; Bailin Wang; Chengzu Li; Connor Boyle; Ansong Ni; Ziyu Yao; Dragomir R Radev; Caiming Xiong; Lingpeng Kong; Rui Zhang; Noah A Smith; Luke Zettlemoyer; Tao Yu"}, {"ref_id": "b68", "title": "RNG-KBQA: Generation augmented iterative ranking for knowledge base question answering", "journal": "Long Papers", "year": "2022", "authors": "Xi Ye; Semih Yavuz; Kazuma Hashimoto; Yingbo Zhou; Caiming Xiong"}, {"ref_id": "b69", "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Ming-Wei Wen-Tau Yih; Xiaodong Chang; Jianfeng He;  Gao"}, {"ref_id": "b70", "title": "The value of semantic parse labeling for knowledge base question answering", "journal": "Short Papers", "year": "2016", "authors": "Matthew Wen-Tau Yih; Chris Richardson; Ming-Wei Meek; Jina Chang;  Suh"}, {"ref_id": "b71", "title": "De-cAF: Joint decoding of answers and logical forms for question answering over knowledge bases", "journal": "CoRR", "year": "2022", "authors": "Donghan Yu; Sheng Zhang; Patrick Ng; Henghui Zhu; Alexander Hanbo Li; Jun Wang; Yiqun Hu; William Wang; Zhiguo Wang; Bing Xiang"}, {"ref_id": "b72", "title": "GraPPa: Grammar-augmented pre-training for table semantic parsing", "journal": "", "year": "2021-05-03", "authors": "Tao Yu; Chien-Sheng Wu; Xi Victoria Lin; Bailin Wang; Yi Chern Tan; Xinyi Yang; Dragomir R Radev; Richard Socher; Caiming Xiong"}, {"ref_id": "b73", "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Tao Yu; Rui Zhang; Kai Yang; Michihiro Yasunaga; Dongxu Wang; Zifan Li; James Ma; Irene Li; Qingning Yao; Shanelle Roman; Zilin Zhang; Dragomir Radev"}, {"ref_id": "b74", "title": "RankT5: Fine-tuning T5 for text ranking with ranking losses", "journal": "", "year": "2022", "authors": "Honglei Zhuang; Zhen Qin; Rolf Jagerman; Kai Hui; Ji Ma; Jing Lu; Jianmo Ni; Xuanhui Wang; Michael Bendersky"}, {"ref_id": "b75", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b76", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc", "journal": "", "year": "", "authors": ""}, {"ref_id": "b77", "title": "If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b78", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b79", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b80", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b81", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b82", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b83", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: A schematic illustration of the proposed framework, Pangu, where a symbolic agent interacts with the target environment to propose candidate plans, and a neural LM evaluates the plausibility of each plan. The agent searches the environment to incrementally construct the plans, and the LM guides the search process.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "utterance q, initial plans P0, environment E 2 t \u2190 1; 3 while True do /", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "5. 3 Figure 4 :34Figure4: Distribution of the probabilities assigned to predicted programs that are seen and unseen during training. We use kernal density smoothing for better visualization, so the x-axis goes over 1.0.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "We show two examples of prompts with 10 incontext samples retrieved from the 100 training data pool in Figure F.1 and Figure F.2 for two different questions from GRAILQA's dev set. Our prompt design is very straightforward. More advanced prompting techniques for Pangu remains to be explored.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "Input utterance:t = 1t = 2Environment:t = 3t = 4Target plan:(a)(b) Utterance: What is the latest released computer emulator developed in java?"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "We experiment with three KBQA datasets of different scale and nature (statistics in TableC.3). GRAILQA(Gu et al., 2021) is a large-scale dataset that evaluates three levels of generalization, namely, i.i.d., compositional (novel compositions of seen constructs), and zero-shot (totally novel domains). Due to the small size of its training set and the non-i.i.d. setting, GRAPHQ is particularly challenging. In our experiments, we use the processed version by, which maps the original dataset from FREEBASE 2013-07 to FREEBASE 2015-08-09. WEBQSP (Yih et al., 2016) is a moderate-scale dataset with questions from Google query logs. It mainly tests i.i.d. generalization on simple questions. It is a clean subset of WEBQ", "figure_data": "4 Experimental Setup4.1 Datasets"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Pangu (AND tv.tv_song (JOIN music.composition.composer m.015_30)) ( ) ArcaneQA (AND music.recording (JOIN music.recording.song (JOIN music.composition.composer m.015_30))) ( ) ArcaneQA \u25b3 (JOIN music.composition.composer m.015_30) (JOIN music.recording.song #0) (AND music.recording #1) Question II \"which software falls into both continuous integration and build automation genres?\" Pangu (AND computer.software (AND (JOIN computer.software.software_genre m.05vvqy) (JOIN computer.software.software_genre m.0h2vrf))) ( ) ArcaneQA (AND computer.software (JOIN computer.software.software_genre m.05vvqy)) ( ) ArcaneQA \u25b3 (JOIN computer.software.software_genre m.05vvqy) (AND computer.software #0)", "figure_data": "Question I\"neil leslie diamond composed what tv song?\"ing a better protocol for using LMs for groundedlanguage understanding. Pangu's strong generaliz-ability with limited training data is also confirmedby its performance on the zero-shot generaliza-tion of GRAILQA. Our method also shows an un-precedented uniformity in accommodating differ-ent LMs (encoder-only, encoder-decoder, decoder-The main results are shown in Table 1. Using a BERT-base LM, Pangu al-ready achieves a new state of the art on GRAILQAonly, through both fine-tuning and in-context learn-ing) and a reliable return from model size-using increasingly larger LMs yields monotonically im-proved results across the board, with T5-3B set-be-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Two representative examples that Pangu succeeds while ArcaneQA fails, both w/ BERT-base. \u25b3 denotes the original order of the decoder's output. The first incorrect token predicted by ArcaneQA is marked in red. easier for LMs to learn than the generative task, Pangu converges very fast (at most two epochs) and gets fewer training steps for overfitting the i.i.d. setting, in exchange for better non-i.i.d. generalization. The strong performance on WEBQSP, an i.i.d. dataset, further supports this observation, because now Pangu can more sufficiently fit the i.i.d. training data. In-context learning results. For the first time, we show the feasibility of effective few-shot KBQA with LLMs. On GRAILQA, Pangu with Codex achieves an overall F1 of 56.3% with only 10 training examples. Though there is still a gap to the full-data fine-tuning results, it is still impressive, especially considering the massive meaning space of the KB. On GRAPHQ, Pangu with Codex even outperforms ArcaneQA using 10 training examples. This further confirms that Pangu is particularly strong in generalizing to new environments with limited training data. On WEBQSP, Pangu trails behind fine-tuning methods when only using 10 training examples; however, increasing the size of the pool for retrieval can significantly boost the performance, which is expected given WE-BQSP's i.i.d. nature. While for non-i.i.d. datasets like GRAILQA and GRAPHQ, the gain from more training examples is marginal.", "figure_data": ""}, {"figure_label": "B", "figure_type": "table", "figure_id": "tab_8", "figure_caption": ".1: Functions in KBQA. We follow the definitions in). R: relation, T : type, E: entity, E \u2032 : a set of entities, N : integer.", "figure_data": "and low-level planner from HLSM (Blukis et al.,2022), and use GPT-3.5 text-davinci-003 as theLM with in-context learning using only 100 labeled examples. 7 We achieved 10% overall success rateand 25% goal completion on ALFRED's unseendev, already outperforming recent baselines (Pa-shevich et al., 2021) trained with full data (21K+examples)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "in our experiments. Concrete statistics of different datasets are shown in Table C.3.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "s(u, c 1 ) > s(u, c 2 ), \u2200c 1 \u2208 G t and \u2200c 2 \u2208 G t\u22121 , s(u, c 1 ) > s(u, c 2 ), \u2200c 1 \u2208 G t and \u2200c 2 \u2208 C t \\G t , s(u, c \u2032 ) > s(u, c i ), \u2200c i \u0338 = c \u2032", "formula_coordinates": [5.0, 306.14, 286.87, 221.61, 53.63]}, {"formula_id": "formula_1", "formula_text": "\u2212 1 Z T +1 t=1 c\u2208Ctp (c)log p(c)", "formula_coordinates": [6.0, 122.87, 260.16, 114.24, 34.87]}, {"formula_id": "formula_2", "formula_text": "Composition Rule Signature Comments JOIN R \u00d7 (E \u222a E \u2032 ) \u2192 E \u2032 a single hop along an edge AND (T \u222a E \u2032 ) \u00d7 E \u2032 \u2192 E \u2032 intersection of two sets ARGMAX/ARGMIN (T \u222a E \u2032 ) \u00d7 R \u2192 E \u2032 superlative aggregations LT/LE/GT/GE R \u00d7 E \u2192 E \u2032 < / \u2264 / > / \u2265 COUNT E \u2032 \u2192 N set cardinality", "formula_coordinates": [16.0, 75.39, 72.69, 209.22, 55.69]}], "doi": "10.1162/tacl_a_00333"}