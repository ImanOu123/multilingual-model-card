{"title": "WINODICT: Probing language models for in-context word acquisition", "authors": "Julian Martin Eisenschlos; Jeremy R Cole; Fangyu Liu; William W Cohen; Google Research", "pub_date": "", "abstract": "We introduce a new in-context learning paradigm to measure Large Language Models' (LLMs) ability to learn novel words during inference. In particular, we rewrite Winogradstyle co-reference resolution problems by replacing the key concept word with a synthetic but plausible word that the model must understand to complete the task. Solving this task requires the model to make use of the dictionary definition of the new word given in the prompt. This benchmark addresses word acquisition, one important aspect of the diachronic degradation known to afflict LLMs. As LLMs are frozen in time at the moment they are trained, they are normally unable to reflect the way language changes over time. We show that the accuracy of LLMs compared to the original Winograd tasks decreases radically in our benchmark, thus identifying a limitation of current models and providing a benchmark to measure future improvements in LLMs ability to do in-context learning.", "sections": [{"heading": "Introduction", "text": "Large Language Models (LLMs) such as GPT-3 (Brown et al., 2020) and PALM (Chowdhery et al., 2022) can only learn from information that is in their training corpus. However, this is naturally limiting because the training corpus itself is bounded in time to the point of its collection. As a result, recent work has studied how to adapt such models to new data without an expensive retraining phase. Methods range from using semiparametric methods with access to external memory (e.g., Guu et al. 2020;Lewis et al. 2020), to continual learning (e.g., Dhingra et al. 2022;Lazaridou et al. 2021), to parameter efficient fine-tuning (e.g., Ben Zaken et al. 2022;Pfeiffer et al. 2021).\nMuch of this work concerns factual knowledge or task distribution shifts. However, language also changes subtly: for instance, the popularity or meaning of individual words can change over time. In fact, such shifts also cause a consistent decrease in model performance for downstream tasks (Huang and Paul, 2018;Jaidka et al., 2018;Lukes and S\u00f8gaard, 2018;Florio et al., 2020).\nAcquiring new words through either examples or definitions is therefore an important test of LLMs' ability to overcome diachronic degradation. With in-context learning having emerged as the primary way to interact with LLMs (Brown et al., 2020), we propose to study LLMs capability of acquiring new vocabulary via prompting.\nWe propose WINODICT, a novel benchmark for word acquisition for LLMs. Word acquisition is challenging to study in a realistic setting as it is hard to know which terms a model has already been exposed to. To overcome this, we rely on a heuristic method to introduce newly invented words and define them in terms of existing concepts. Following previous work (Chakrabarty et al., 2022), we incorporate the required knowledge into the prompt. We then ask models to perform tasks that require successfully interpreting the invented words.\nWe consider the English co-reference resolution datasets Winograd Schema Challenge (Levesque et al., 2012) and WinoGrande (Sakaguchi et al., 2020). The examples are built in pairs with minimal changes, which allow the identification of the key concept that must be understood to solve the example. An example of WINODICT can be seen in Figure 1. Our contributions are the following: (a) We propose WINODICT, a method and dataset to test models for word acquisition skills. (b) We benchmark the performance of several stateof-the-art models across scale and number of shots. (c) We analyze the effect of prompt, POS tags, word likelihood and similarity for ease of acquisition.\nThese results help us understand the challenges for incorporating new concepts into LLMs. The code to build the dataset has been open-sourced. 1 ", "publication_ref": ["b4", "b8", "b16", "b5", "b14", "b0", "b22", "b10", "b11", "b19", "b6", "b3", "b15", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "WINOGRAD", "text": "The city councilmen refused the demonstrators a permit because they feared violence.\nThe city councilmen refused the demonstrators a permit because they advocated violence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "WINODICT", "text": "The verb to plest means to be scared of, or want to avoid an object.\nThe verb to sparn means to to publicly recommend or support. The city councilmen refused the demonstrators a permit because they plested violence.\nThe city councilmen refused the demonstrators a permit because they sparned violence. Figure 1: An example pair from WINODICT together with its original WINOGRAD source. The task is to decide whether they refers to the city councilman or the demonstrators. Here, the correct answer is shown in blue and the incorrect answer in red. Note that in both cases, it is necessary to understand the meaning of the bolded key concept to resolve the co-reference, which we identify in WINOGRAD and substitute for a new word in WINODICT.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methods", "text": "WINODICT, like WINOGRAD and WINOGRANDE, is a co-reference resolution task in a binary choice setup. A model is given two alternative noun phrases, and has to decide which one is more likely to correspond to a highlighted pronoun or blank.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Construction", "text": "To build WINODICT, we rely on the fact that the examples from WINOGRAD and WINOGRANDE are constructed from contrasting pairs (Gardner et al., 2020;Kaushik et al., 2020). Each instance differs in a minimal way from its counterpart with the true label reversed. This allows the identification of the key concept that needs to be parsed in order to resolve the task. In Figure 1 for instance, the verbs fear and advocate correspond to the key concepts.\nWINOGRAD and WINOGRANDE are similar; however, WINOGRANDE is larger, uses blanks instead of pronouns, and the dataset has been filtered for co-occurrence bias between the key concept and the correct noun-phrase. This results in some examples that do not have a corresponding paired example with a different key concept.\nTo create our examples, we first recover the pairing between the examples, dropping those with no pairing. Secondly, we identify the key concept tokens that change from one example to the other, dropping examples where the key concept consists of multiple tokens. Finally we run the sentence through the spaCy 2 syntactic analyzer and fetch WordNet 3 definitions of the key concepts' lemmas. In the next section we show how the key concept 2 https://spacy.io 3 https://wordnet.princeton.edu (Miller, 1995) 1.", "publication_ref": ["b7", "b12", "b20"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "New Word Creation", "text": "Our goal is to create plausible synthetic words. We create plausible words using a simple probabilistic model of every one-, two-, and three-letter sequence that is trained on the vocabulary of English words 4 . These three-letter sequences are then sampled and combined to form new synthetic words. We filter any words that have a three letter sequence that does not occur in any other English word. We then sample the words based on their log probability, placing them into five buckets and keeping around 500 for each bucket. The morphology for each word is created by aggregating over a sample of proposed synthetic word morphologies. The last 2-4 letters of each word (depending on the morphological edit) form a suffix dictionary that is used as a simple substitution dictionary for the remaining words: failures are dropped. This produces a combination of regular and irregular conjugations over the new words.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Answer Scoring", "text": "Each instance in WINODICT consists of a new word with its definition d, a statement containing a blank where x and y correspond to the text before and after the blank respectively, and two noun phrases o 1 and o 2 . The task consists of identifying which of the noun-phrases better fits the blank.\nPALM, GPT-3 and its predecessors (Radford et al., 2019) use the method proposed by Trinh and Le (2018) to evaluate WINOGRAD and WINO-GRANDE, which we explain below. A prediction score is obtained through comparing the log likelihood of the same continuation y of two possible prefix texts (x : o 1 and x : o 2 ) where the co-reference pronoun or blank marker has been replaced. It is correct if it scores the suffix higher for the prefix with the correct interpretation of the co-reference problem.\nln P\u0398 (y|x : o1) \u2212 ln P\u0398 (y|x : o2) = n i=0 ln P\u0398 (yi|y<i : x : o1) \u2212 ln P\u0398 (yi|y<i : x : o2)\nwhere : denotes concatenation and variables map to:\nx = \"The city councilmen refused the demonstrators a permit because\" o1 = \"the city councilmen\" o2 = \"the demonstrators\" {yi} n i=1 = y = \"feared violence.\"\nIn our setup we add the definition of the new concept as a suffix to the shared term y, thus replacing it with y : d. This achieves higher accuracy than the alternatives. Note that this means that the model is scoring the definition rather than conditioning on it. See Section 4 and Table 4 for a discussion of other variants of the setup, including adding the definition as a prefix.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this work we test GPT-3 (Brown et al., 2020), and PALM (Chowdhery et al., 2022) models of various sizes, ranging from 3B to 540B parameters. Appendix A has more details on the model sizes.\nAs in the original in-context learning evaluations, we try 0, 1, and 5-shot experiments, using random examples to build the prompt. We compare to both a zero-shot human evaluation as well as the original source datasets with only our filtered examples.\nThe main experimental results are shown in Table 2. We observe a consistent gap of 18 or more points between WINODICT examples and their original counterparts. Similar to trends observed in other datasets (Chowdhery et al., 2022), scaling the number of shots and model size consistently improves accuracy. The three smaller versions of GPT-3 and PALM-8B all perform close to random.\nWe verify that omitting any information of the new word yields random results for even the best PALM-540B model. We discuss this and other prompting strategies in more detail in Appendix B.", "publication_ref": ["b4", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Human Evaluation", "text": "The human accuracy on WINODICT is estimated using the responses of 10 volunteers. No native English proficiency was required for participation. Participants were told that the aim of the research is to study how to use words based on their definition. They were presented with 15 sentences that included a pronoun / blank and asked to select one of two noun phrases it most likely refers to.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Foreign Inspired Words", "text": "To explore a more realistic scenario, we conduct an experiment using 20 hand-written WINODICT-like examples whose definitions are inspired by foreign words that do not have a clear single-word definition in English. For instance, \"estrenar\" refers to wearing a piece of clothing for the first time, which does not have a clear English word equivalent. We can then create an example that requires knowing this definition, such as \"I really [ love | hate] my new dress. I can't wait to <word> it.\nIn conducting this experiment, we substitute synthetic words instead of using the original foreign words, and the definitions of the words themselves may not correspond to native speakers' precise understanding: in other words, these are meant to be true new words and data leakage should be minimal. We run the same experiment on these examples. Results are in Table 3 and full details in Appendix C. Overall, the numbers are comparable to the WINO-DICT results, suggesting that models are unlikely to be solving the task using a reverse dictionary.   the best-performing PALM-540B model using a 5-shot setup. See Table 4 for the full results. Concretely, we vary the prompts along a few axes. First, we test whether the definition should be part of the prefix, where the model would condition on it, or the suffix, where the model would score it. Note that in all setups, putting the definition in the suffix works consistently better.\nAdditionally, we test whether the task is made easier by using synonyms instead of definitions. This task indeed appears to be easier, potentially because the model needs to learn only a simple substitution between the new word and the provided synonym, whose definition it knows. We focus on definitions in this work as exact synonyms would rarely be available for novel words.\nAs a baseline, we also examine the \"Empty\" setup, where the model is provided no information about the new word. We observe that PALM approximates random guessing without being given the definition, showing that the task remains roughly unbiased.\nWe additionally test the model's performance on the original task where we also provide the definition of the key concept. Note that the \"Empty\" case here corresponds precisely to the original task. Interestingly, the definition seems to serve as a slight distraction, especially as a prefix, though accuracy is still well above the model's performance on the synthetic words.\nFinally, in the \"Meaning shift\" scenario, we map new definitions to already known words. This task appears to be even more difficult than the standard WINODICT setup, implying that the model is distracted by the surface forms of the words.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "New Word Analysis", "text": "Several factors can affect the capabilities for word acquisition of LLMs. We investigate several attributes, split into quartiles, using PALM-540B with 5-shots, which is the best model from Table 2.\nWe consider the following attributes: (1) the part-of-speech of the synthetic word; (2) the average model negative log likelihood (NLL) of the two model predictions, which measures the likelihood of the suffix for both prefixes; (3) the number of SentencePiece (Kudo and Richardson, 2018) tokens in the synthetic word, to investigate the effect of model tokenization; (4) the number of Sentence-Piece tokens in the definition of the synthetic word, to investigate if longer definitions are more challenging; (5) the Levenshtein edit distance between the synthetic and original word, to investigate if similar words are easier; and (6) the likelihood of the new word as computed by our probabilistic model of three-letter sequences, to see if less probable words are more difficult to acquire.\nOf the six attributes, the two most correlated with accuracy are (4) the definition length and (2) the average NLL. We observe no clear pattern in the other four attributes. In Figure 2 we show their effect in each quartile. The effect of definition length indicated that the 25% longest definitions are the hardest to acquire by a significant margin (12% relative drop for WINOGRAD, 5% for WINOGRANDE). The relative accuracy drop for the largest quartile of the NLL average is 13% for WINOGRAD and 4% for WINOGRANDE. The drop in NLL suggests that when models assigns low probabilities to answers, they make more mistakes: the low probability may indicate the model has a poor understanding of the prefix so scores the suffix randomly.  ", "publication_ref": ["b13"], "figure_ref": ["fig_1"], "table_ref": ["tab_3"]}, {"heading": "Related Work", "text": "Word acquisition for LLMs. Inspired by developmental linguistics (Carey and Bartlett, 1978), Radford et al. (2019) succeeded to prompt GPT-3 to generate plausible example sentences based on definitions of synthetic words. Unlike WINODICT, the evaluation was purely qualitative.\nCommon sense. Li et al. ( 2021) study how prompt structures and scoring methods affect the performance of LLMs on common sense tasks including WINOGRANDE, where they observe the least variation. The format from WINOGRAD has been subsequently used to probe models for other phenomena such as explanations  and gender bias (Zhao et al., 2018).\nBenchmarks for lexical knowledge. Schick and Sch\u00fctze (2020) introduce a benchmark for probing a model's knowledge of the properties of rare words. Hill et al. (2016) train models to match word and definition representations, which they apply to a reverse dictionary task.", "publication_ref": ["b2", "b23", "b28", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this work, we study the question of in-context word acquisition by large language models. While non-trivial to measure, the ability to incorporate knowledge about new words in-context may be useful to decrease the effect of diachronic degradation. We design a mechanism to transform Winogradstyle tasks into challenging probes for reasoning on the meaning assigned to synthetic words, allowing for a more objective measurement of word acquisition. We study the results of models of multiple sizes and families and conclude that while the problem becomes easier with scale, there is still a substantial gap with human performance and the original WINOGRAD and WINOGRANDE tasks, demonstrating the difficulty of the proposed task. Finally, we show that acquiring novel definitions is of similar difficulty, indicating the task is realistic.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "The task described in this work is synthetic and thus an imperfect measure of the phenomena under study. The words in WINODICT are synthetic words with definitions copied from existing concepts; the model could thus solve WINODICT with a reduction to a reverse dictionary task. To partially address this, we conducted pilot experiments described in Section 3.2. Additionally, the choice of prompts for LLMs has been shown to have a large influence on the resulting accuracy (Min et al., 2022;Lu et al., 2022). While we tried multiple templates, it is possible that substantially better prompts exist for this task.", "publication_ref": ["b21", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "A Model Sizes", "text": "While OpenAI does not officially disclose the size of their four models Davinci, Curie, Babbage and Ada, we use the numbers approximated in a blogpost as estimates. 5 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Prompts", "text": "We built prompts for definitions and synonyms to make them sound natural given the structure of most WordNet definitions for each part-of-speech tag. Table 6 shows the different prompt templates in each case.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "C Foreign Inspired Words", "text": "In Table 7 we list the word, approximate definition, and WINODICT-like example. Note that these examples are handwritten and did not go through a debiasing process like WINOGRANDE. In order to reduce the risk of data leakage, in the actual examples we replace the surface form of the word with one of the synthetic surface forms using the same process as in section 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Yasemin Altun, Iulia-Maria Com\u015fa and Srini Narayanan, as well as our anonymous reviewers, for their valuable feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "All annotations for the human evaluation were done by English speaking volunteers using a multiple choice survey tool. No personal identifiable information was collected.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Example Definition", "text": "John frequently goes backpacking and Jake never does because [ Jake | John] disdains the feeling of waldeinsamkeit.\nthe feeling of solitude and connectedness to nature when being alone in the woods  ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models", "journal": "Short Papers", "year": "2022", "authors": "Elad Ben Zaken; Yoav Goldberg; Shauli Ravfogel"}, {"ref_id": "b1", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b2", "title": "Acquiring a single new word", "journal": "", "year": "1978", "authors": "Susan Carey; E Bartlett"}, {"ref_id": "b3", "title": "It's not rocket science: Interpreting figurative language in narratives", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Tuhin Chakrabarty; Yejin Choi; Vered Shwartz"}, {"ref_id": "b4", "title": "Palm: Scaling language modeling with pathways", "journal": "", "year": "2022", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton;  Gehrmann"}, {"ref_id": "b5", "title": "Time-aware language models as temporal knowledge bases", "journal": "Transactions of the Association for Computational Linguistics", "year": "2022", "authors": "Bhuwan Dhingra; Jeremy Cole; Julian Eisenschlos; Daniel Gillick; Jacob Eisenstein; William Cohen"}, {"ref_id": "b6", "title": "Time of your hate: The challenge of time in hate speech detection on social media", "journal": "Applied Sciences", "year": "2020", "authors": "Komal Florio; Valerio Basile; Marco Polignano; Pierpaolo Basile; Viviana Patti"}, {"ref_id": "b7", "title": "Evaluating models' local decision boundaries via contrast sets", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Matt Gardner; Yoav Artzi; Victoria Basmov; Jonathan Berant; Ben Bogin; Sihao Chen; Pradeep Dasigi; Dheeru Dua; Yanai Elazar; Ananth Gottumukkala; Nitish Gupta; Hannaneh Hajishirzi; Gabriel Ilharco; Daniel Khashabi; Kevin Lin; Jiangming Liu; Nelson F Liu; Phoebe Mulcaire; Qiang Ning; Sameer Singh; Noah A Smith; Sanjay Subramanian; Reut Tsarfaty; Eric Wallace; Ally Zhang; Ben Zhou"}, {"ref_id": "b8", "title": "Retrieval augmented language model pre-training", "journal": "PMLR", "year": "2020", "authors": "Kelvin Guu; Kenton Lee; Zora Tung; Panupong Pasupat; Mingwei Chang"}, {"ref_id": "b9", "title": "Learning to understand phrases by embedding the dictionary", "journal": "Transactions of the Association for Computational Linguistics", "year": "2016", "authors": "Felix Hill; Kyunghyun Cho; Anna Korhonen; Yoshua Bengio"}, {"ref_id": "b10", "title": "Examining temporality in document classification", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Xiaolei Huang; Michael J Paul"}, {"ref_id": "b11", "title": "Diachronic degradation of language models: Insights from social media", "journal": "Short Papers", "year": "2018", "authors": "Kokil Jaidka; Niyati Chhaya; Lyle Ungar"}, {"ref_id": "b12", "title": "Learning the difference that makes a difference with counterfactually-augmented data", "journal": "", "year": "2020", "authors": "Divyansh Kaushik; Eduard Hovy; Zachary Lipton"}, {"ref_id": "b13", "title": "SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Taku Kudo; John Richardson"}, {"ref_id": "b14", "title": "Mind the gap: Assessing temporal generalization in neural language models", "journal": "Advances in Neural Information Processing Systems", "year": "2021", "authors": "Angeliki Lazaridou; Adhi Kuncoro; Elena Gribovskaya; Devang Agrawal; Adam Liska; Tayfun Terzi; Mai Gimenez; Cyprien De Masson D'autume; Tomas Kocisky; Sebastian Ruder"}, {"ref_id": "b15", "title": "The winograd schema challenge", "journal": "AAAI Press", "year": "2012", "authors": "Hector J Levesque; Ernest Davis; Leora Morgenstern"}, {"ref_id": "b16", "title": "Retrieval-augmented generation for knowledge-intensive nlp tasks", "journal": "", "year": "2020", "authors": "Patrick Lewis; Ethan Perez; Aleksandra Piktus; Fabio Petroni; Vladimir Karpukhin; Naman Goyal; Heinrich K\u00fcttler; Mike Lewis; Wen-Tau Yih; Tim Rockt\u00e4schel"}, {"ref_id": "b17", "title": "Cyprien de Masson d'Autume, Phil Blunsom, and Aida Nematzadeh. 2021. A systematic investigation of commonsense understanding in large language models", "journal": "", "year": "", "authors": "Lorraine Xiang; Adhi Li;  Kuncoro"}, {"ref_id": "b18", "title": "Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b19", "title": "Sentiment analysis under temporal shift", "journal": "", "year": "2018", "authors": "Jan Lukes; Anders S\u00f8gaard"}, {"ref_id": "b20", "title": "Wordnet: A lexical database for english", "journal": "Commun. ACM", "year": "1995", "authors": "George A Miller"}, {"ref_id": "b21", "title": "Rethinking the role of demonstrations: What makes in-context learning work?", "journal": "", "year": "2022", "authors": "Sewon Min; Xinxi Lyu; Ari Holtzman; Mikel Artetxe; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b22", "title": "AdapterFusion: Non-destructive task composition for transfer learning", "journal": "", "year": "2021", "authors": "Jonas Pfeiffer; Aishwarya Kamath; Andreas R\u00fcckl\u00e9; Kyunghyun Cho; Iryna Gurevych"}, {"ref_id": "b23", "title": "Language models are unsupervised multitask learners", "journal": "CoRR", "year": "2019", "authors": "Alec Radford; Jeff Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b24", "title": "Winogrande: An adversarial winograd schema challenge at scale", "journal": "", "year": "2020", "authors": "Keisuke Sakaguchi; Le Ronan; Chandra Bras; Yejin Bhagavatula;  Choi"}, {"ref_id": "b25", "title": "Rare words: A major problem for contextualized embeddings and how to fix it by attentive mimicking", "journal": "", "year": "2020", "authors": "Timo Schick; Hinrich Sch\u00fctze"}, {"ref_id": "b26", "title": "A simple method for commonsense reasoning", "journal": "CoRR", "year": "2018", "authors": "H Trieu; Quoc V Trinh;  Le"}, {"ref_id": "b27", "title": "WinoWhy: A deep diagnosis of essential commonsense knowledge for answering Winograd schema challenge", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Hongming Zhang; Xinran Zhao; Yangqiu Song"}, {"ref_id": "b28", "title": "Gender bias in coreference resolution: Evaluation and debiasing methods", "journal": "", "year": "2018", "authors": "Jieyu Zhao; Tianlu Wang; Mark Yatskar; Vicente Ordonez; Kai-Wei Chang"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Effect on WINODICT PALM-540B 5-shot accuracy on each quartile splitting by definition length and by average NLL score. Longer definitions and higher NLL correlate with lower accuracy.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "\u00b11.6 57.1 \u00b12.1 59.1 \u00b11.6 83.3 83.3 87.2 51.8 \u00b11.6 54.2 \u00b10.4 52.4 \u00b11.1 69.3 65.5 67.4 PALM 62B 62.2 \u00b10.6 65.9 \u00b13.6 70.3 \u00b11.3 91.1 90.0 92.2 56.7 \u00b11.1 58.2 \u00b11.0 59.7 \u00b11.1 76.6 77.8 78.2 PALM 540B 65.9 \u00b12.5 75.4 \u00b11.3 78.6 \u00b10.6 92.8 92.2 95.6 60.3 \u00b11.4 63.9 \u00b12.3 68.5 \u00b11.9 \u00b12.2 50.9 \u00b11.7 50.2 \u00b14.3 60.0 57.8 61.7 52.2 \u00b11.2 52.0 \u00b13.6 49.4 \u00b11.7", "figure_data": "WINODICT (Ours)OriginalWINOGRANDE WINODICT (Ours)OriginalShots015015015015PALM 8B59.2 80.1 81.3 85.8GPT-3 Ada GPT-3 Babbage GPT-3 Curie GPT-3 Davinci51.9 48.1 53.8 53.2 51.8 \u00b10.8 52.8 \u00b12.0 54.4 \u00b12.3 75.6 71.7 65.6 50.8 \u00b11.7 52.3 \u00b11.0 52.2 \u00b10.8 52.8 55.1 56.6 54.2 \u00b11.6 54.6 \u00b12.4 59.9 \u00b11.5 85.0 81.7 82.8 50.2 \u00b11.5 50.6 \u00b11.6 52.2 \u00b11.0 62.0 61.1 60.8 60.3 \u00b11.3 63.6 \u00b12.3 72.9 \u00b10.5 88.3 85.0 91.1 55.0 \u00b11.1 55.7 \u00b11.4 61.3 \u00b11.4 71.8 69.6 72.5Human91.796.5  *83.394.0  *"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Binary classification accuracy on WINODICT vs. the original datasets using average and standard deviation across 5 sets of new words. Original results may differ from the ones reported byChowdhery et al. (2022) since only a subset of the examples are used. A consistent gap of 18+ points appears when comparing against the original sets. The original human evaluation numbers denoted with * are taken fromSakaguchi et al. (2020).", "figure_data": "Shots015PALM 540B GPT-3 Davinci68.7 73.0 76.0 61.0 56.0 68.0"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Binary classification accuracy on the foreigninspired new words averaged over five runs. Overall, accuracy is comparable to the original dataset. The lines marked with * correspond to the experiments in Table 2.", "figure_data": "Word Type PromptWINOGRAD WINOGRANDESynthetic Def Prefix Def Suffix  *  Syn Prefix Syn Suffix Empty72.2 78.6 74.1 88.4 52.062.7 68.5 60.5 78.2 51.9OriginalDef Prefix Def Suffix Syn Prefix Syn Suffix Empty  *85.5 93.8 87.2 91.6 95.674.0 84.4 74.3 83.2 85.8Meaning shiftDef Prefix Def Suffix Syn Prefix Syn Suffix Empty66.1 75.6 69.4 83.3 51.160.8 60.4 60.1 74.7 49.7Table 4: Analysis of different prompts. We show the results on the synthetic words, original words, and ex-isting words but assigned to a new meaning (\"Meaning shift\"). Prefix/Suffix correspond to the location of the definition, Syn/Def corresponds to using the definition or synonyms of the synthetic word. Empty means nei-ther (should be random for synthetic words). Provid-ing synonyms yields the best results. All results are on PALM-540B 5-shot."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "contains the number of parameters for the models used in our experiments.", "figure_data": "Model# ParametersGPT-3-Ada350MGPT-3-Babbage1.3BGPT-3-Curie6.7BGPT-3-Davinci175BPALM-8B8BPALM-62B62BPALM-540B540B"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Number of parameters of the reported models.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Templates used to integrate the definition into the prompt for each part-of-speech tag.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "ln P\u0398 (y|x : o1) \u2212 ln P\u0398 (y|x : o2) = n i=0 ln P\u0398 (yi|y<i : x : o1) \u2212 ln P\u0398 (yi|y<i : x : o2)", "formula_coordinates": [3.0, 77.48, 387.32, 200.94, 40.6]}], "doi": "10.1162/tacl_a_00478"}