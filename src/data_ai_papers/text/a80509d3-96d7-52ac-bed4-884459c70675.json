{"title": "Lost! Leveraging the Crowd for Probabilistic Visual Self-Localization", "authors": "Marcus A Brubaker; Tti Chicago; Andreas Geiger; Raquel Urtasun", "pub_date": "", "abstract": "In this paper we propose an affordable solution to selflocalization, which utilizes visual odometry and road maps as the only inputs. To this end, we present a probabilistic model as well as an efficient approximate inference algorithm, which is able to utilize distributed computation to meet the real-time requirements of autonomous systems. Because of the probabilistic nature of the model we are able to cope with uncertainty due to noisy visual odometry and inherent ambiguities in the map (e.g., in a Manhattan world). By exploiting freely available, community developed maps and visual odometry measurements, we are able to localize a vehicle up to 3m after only a few seconds of driving on maps which contain more than 2,150km of drivable roads.", "sections": [{"heading": "Introduction", "text": "Self-localization is key for building autonomous systems that are able to help humans in everyday tasks. Despite decades of research, it is still an exciting open problem. In this paper we are interested in building affordable and robust solutions to self-localization for the autonomous driving scenario. Currently, the leading technology in this setting is GPS. While being a fantastic aid for human driving, it has some important limitations in the context of autonomous systems. Notably, the GPS signal is not always available, and its localization can become imprecise (e.g., in the presence of skyscrapers, tunnels or jammed signals). While this might still be viable for human driving, consequences can be catastrophic for self-driving cars.\nTo provide alternatives to GPS localization, place recognition approaches have been developed. They assume that image or depth features from anywhere around the globe can be stored in a database, and cast the localization problem as a retrieval task. Both 3D point clouds [5,7,10,20] and visual features [2,3,11,15,16,24] have been leveraged to solve this problem. In combination with GPS, impressive results have been demonstrated (e.g., the Google self-driving car). However, it remains unclear if main-Figure 1. Visual Self-Localization: We demonstrate localizing a vehicle with an average accuracy of 3.1m within a map of \u223c 2, 150km of road using only visual odometry measurements and freely available maps. In this case, localization took less than 21 seconds. Grid lines are every 2km.\ntaining an up-to-date world representations will be feasible given the computation, memory and communication requirements. Furthermore, these solutions are far from affordable as every corner of the world needs to be visited and updated constantly. Finally, privacy and security issues need to be considered as the recording and storage of such data is illegal in some countries.\nIn contrast to the above mentioned approaches, here we tackle the problem of self-localization in places that we have never seen before. We take our inspiration from humans, which excel in this task while having access to only a rough cartographic description of the environment. We propose to leverage the crowd, and exploit the development of OpenStreetMap (OSM), a free community-driven map, for the task of vision-based localization. The OSM maps are detailed and freely available, making this an inexpensive solution. Moreover, they are more frequently updated than their commercial counterparts. Towards this goal, we derive a probabilistic map localization approach that uses visual odometry estimates and OSM data as the only inputs. We demonstrate the effectiveness of our approach on a variety of challenging scenarios making use of the recently released KITTI visual odometry benchmark [8]. As our experiments show, we are able to localize ourselves after only a few seconds of driving with an accuracy of 3 meters on a 18km 2 map containing 2, 150km of drivable roads. ", "publication_ref": ["b4", "b6", "b9", "b19", "b1", "b2", "b10", "b14", "b15", "b23", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Early approaches for map localization [5,7,10,20] make use of Monte Carlo methods and the Markov assumption to maintain a sample-based posterior representation of the agent's pose. However, they only operate locally without providing any global (geographic) positioning information and thus can not be applied to the problem we consider here. Furthermore, they are typically restricted to smallscale environments and low-noise laser-scan observations. At a larger scale, place recognition methods localize [2,11,16,24] or categorize [22,23,27] an image, given a database of geo-referenced images or video streams [3,15]. While processing single landmarks is clearly feasible, creating an up-to-date \"world database\" seems impractical due to computational and memory requirements. In contrast, the maps used by our localization approach require only a few gigabytes for storing the whole planet earth 1 .\nRelative motion estimates can be obtained using visual odometry [19], which refers to generating motion estimates from visual input alone. While current implementations [1,9,14] demonstrate impressive performance [8], their incremental characteristics inevitably leads to large drift at long distances. Methods for Simultaneous Localization And Mapping (SLAM) [18,25,6] are able to reduce this drift by modelling the map using landmarks and jointly optimizing over poses and landmarks. Limitations in terms of speed and map size have been partially overcome, for example by efficient optimization strategies using incremental sparse matrix factorization [13] or the use of relative representations [17]. Furthermore, recent progress in loop-closure detection [4,21,26] has led to improved maps by constraining the problem at places which have been visited multiple times. However, SLAM methods can only localize themselves in maps that have been previously created with a similar sensor setup, hence strongly limiting their application at larger scales. In contrast, the proposed approach enables geographic localization and relies only on freely available map information (i.e., OpenStreetMap). To our knowledge, ours is the first approach in this domain. 1 http://wiki.openstreetmap.org/wiki/planet.osm", "publication_ref": ["b4", "b6", "b9", "b19", "b1", "b10", "b15", "b23", "b21", "b22", "b26", "b2", "b14", "b0", "b18", "b0", "b8", "b13", "b7", "b17", "b24", "b5", "b12", "b16", "b3", "b20", "b25", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Visual Localization", "text": "We propose to use one or two roof-mounted cameras to self-localize a driving vehicle. The only other information we have is a map of the environment in which the vehicle is driving. This map contains streets as line segments as well as intersection points. We exploit visual odometry in order to obtain the trajectory of the vehicle. As this trajectory is too noisy for direct shape matching, here we propose a probabilistic approach to self-localization that employs visual odometry measurements in order to determine the instantaneous position and orientation of the vehicle in a given map. Towards this goal, we first define a graph-based representation of the map as well as a probabilistic model of how a vehicle can traverse the graph. For inference, we derive a filtering algorithm, which exploits the structure of the graph using Mixtures of Gaussians. In order to keep running times reasonable, we further propose techniques for limiting the complexity of the mixture models which includes an algorithm for simplifying the Gaussian Mixture models. We start our discussion by presenting the employed map information, followed by our probabilistic model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The OpenStreetMap Project", "text": "Following the spirit of Wikipedia, Steve Coast launched the OpenStreetMap (OSM) project in 2004 with the goal of creating a free editable map of the world. So far, more than 800,000 2 users around the globe have contributed by supplying tracks from portable GPS devices, labeling objects using aerial imagery or providing local information. Fig. 2 illustrates the tremendous growth of OSM over the last years. Compared to commercial products like Google Maps, the provided data is more up-to-date, often includes more details (e.g., street types, traffic lights, postboxes, trees, shops, power lines) and -most importantly -can be freely downloaded and used under the Open Database License. We extracted all crossings and drivable roads (represented as piece-wise linear segments) connecting them. For each street we additionally extract its type (i.e., highway or rural) and the direction of traffic. By splitting each bi-directional street into two one-way streets and 'smoothing' intersections using circular arcs, we obtain a lane-based map representation, on which we define the vehicle state.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Lane-based Map Representation", "text": "We assume that the map data is represented by a directed graph where nodes represent street segments and edges define the connectivity of the roads. Roads which dead-end or run off the edge of the map are connected to a \"sink\" node. As mentioned above, we convert all street segments to oneway streets. An example of a map and corresponding graph representation is shown in Fig. 3 (left). Each street segment is either a linear or a circular arc segment. The parameters of the street segment geometry are described in Fig. 3 (right). We define the position and orientation of a vehicle in the map in terms of the street segment u that the vehicle is on, the distance from the origin of that street segment d and the offset of the local street heading \u03b8. The global heading of the vehicle is then \u03b8 + \u03b2 + \u03b1d and its position is\n\u2212d p 0 + d p 1 for a linear segment and c+rd( \u2212d \u03c8 0 + d \u03c8 ) for a circular arc segment, with d(\u03b8) = (cos \u03b8, sin \u03b8) T .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "State-Space Model", "text": "We define the state of the model at time t to be x t = (u t , s t ) where s t = (d t ,d t\u22121 , \u03b8 t ,\u03b8 t\u22121 ) T andd t\u22121 ,\u03b8 t\u22121 are the distance and angle at the previous time defined relative to the current street u t . Visual odometry observations at time t, y t , measure the linear and angular displacement from time t \u2212 1 to time t. We thus model\np(y t |x t ) = N (y t |M ut s t , \u03a3 y ut )(1)\nwhere \nM u = [m d , m \u03b8 ] T , m d = (1, \u22121, 0, 0) T and m \u03b8 = (\u03b1 u , \u2212\u03b1 u , 1, \u22121) T .\np(s t |u t , x t\u22121 ) = N (s t |A ut,ut\u22121 s t\u22121 + b ut,ut\u22121 , \u03a3 x ut )(2\n) with \u03a3 x ut the covariance matrix for a given u t which is learned from data as discussed in Section 4. We use a second-order, constant velocity model for the change in d and a first order autoregressive model, i.e., AR(1), for the angular offset \u03b8. That is, d t = d t\u22121 + (d t\u22121 \u2212d t\u22122 ) plus noise, and \u03b8 t = \u03b3 ut\u22121 \u03b8 t\u22121 plus noise where \u03b3 ut\u22121 \u2208 [0, 1] is the parameter of the AR(1) model which controls the correlation between \u03b8 t and \u03b8 t\u22121 . In practice, we found these models to be both simple and effective. Because the components of s t are relative to the current street, u t , when u t = u t\u22121 the state transition model must be adjusted so that s t becomes relative to u t . Both d t andd t\u22121 must have ut\u22121 subtracted, and\u03b8 t\u22121 needs to be updated so that\u03b8 t\u22121 relative to u t has the same global heading as \u03b8 t\u22121 relative to u t\u22121 . The above model can then be expressed as\nA ut,ut\u22121 = \uf8ee \uf8ef \uf8ef \uf8f0 2 \u22121 0 0 1 0 0 0 0 0 \u03b3 ut 0 0 \u03b1 ut\u22121 \u2212 \u03b1 ut 1 0 \uf8f9 \uf8fa \uf8fa \uf8fb (3) b ut,ut\u22121 = \u2212( ut\u22121 , ut\u22121 , 0, \u03b8 ut,ut\u22121 ) T u t = u t\u22121 (0, 0, 0, 0) T u t = u t\u22121 (4)\nwhere\n\u03b8 ut,ut\u22121 = \u03b2 ut \u2212 (\u03b2 ut\u22121 + \u03b1 ut ut\u22121 )\nis the angle between the end of u t and the beginning of u t\u22121 .\nThe street transition probability p(u t |x t\u22121 ) defines the probability of transitioning onto the street u t given the previous state x t\u22121 . We use the Gaussian transition dynamics to define the probability of changing street segments, i.e.,\np(u t |x t\u22121 ) = \u03be ut,ut\u22121 u t\u22121 + u u t\u22121 N (x|a T d s t\u22121 , a T d \u03a3 x ut\u22121 a d )dx (5) where a d = (2, \u22121, 0, 0), \u03be ut,ut\u22121 = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 u t = u t\u22121 1 |N(uj )| u t \u2208 N(u t\u22121 ) 0 otherwise (6)\nand N(u) is the set of streets to which u connects.\nAs short segments cannot be jumped over in a single time step, we introduce \"leapfrog\" edges which allow the vehicle to move from u t\u22121 to any u t to which there exists a path in the graph. To handle this properly, we update the entries of b ut,ut\u22121 to consider transitioning over a longer path and \u03be ut,ut\u22121 is the product \u03be along the path. As the speed of the vehicle is assumed to be limited, we need to add edges only up to a certain distance. Assuming a top speed of around 110km/h and observations every second, we add leapfrog edges for paths of up to 30m.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inference", "text": "Given the above model we wish to compute the filtering distribution, p(x t |y 1:t ). We can write the posterior using the product rule as p(x t |y 1:t ) = p(s t |u t , y 1:t )p(u t |y 1:t ), where p(u t |y 1:t ) is a discrete distribution over streets and p(s t |u t , y 1:t ) is a continuous distribution over the position and orientation on a given street. We choose to represent p(s t |u t , y 1:t ) using a Mixture of Gaussians, i.e., p(s t |u t , y 1:t ) =\nNu t i=1 \u03c0 (i) ut N (s t |\u00b5 (i) ut , \u03a3 (i) ut )(7)\nAlgorithm 1 Filter for all streets ut reachable from ut\u22121 do\n5: for k = 1, . . . , |M t\u22121 u t\u22121 | do 6:\nif p(ut|ut\u22121, st\u22121) is approx. constant then 7:\nAnalytically approx. c pred N (\u00b5 pred , \u03a3 pred ) Sample to compute c pred N (\u00b5 pred , \u03a3 pred )\n8:\n10:\nIncorporate yt to compute c upd N (\u00b5 upd , \u03a3 upd )\n11:\nAdd N (\u00b5 upd , \u03a3 upd ) to M t u i with weight c upd 12: for all streets u do Normalize the weights of mixture M t u 15: Normalize P t u so that u P t u = 1. 16: Return:\nPosterior at t, {P t u , M t u }\nwhere N ut is the number of components for the mixture associated with u t and M t ut = {\u03c0\n(i) ut , \u00b5 (i) ut , \u03a3 (i) ut } Nu t\ni=1 are the parameters of the mixture for u t . This is a general and powerful representation but still allows for efficient and accurate inference. Assuming independent observations given the states and that the state transitions are first order Markov, we write the filtering distribution recursively as \u00d7 p(u t |u t\u22121 , s t\u22121 )p(s t\u22121 |u t\u22121 , y 1:t\u22121 )ds t\u22121 (9) where P ut\u22121 = p(u t\u22121 |y 1:t\u22121 ) and Z t = p(y t |y 1:t\u22121 ). Substituting in the mixture model form of p(s t\u22121 |u t\u22121 , y 1:t\u22121 ), and the model transition dynamics the integrand in the above equation becomes\nN i=1 \u03c0 (i) p(u t |u t\u22121 ,s t\u22121 )N (s t |As t\u22121 + b, \u03a3 x ) \u00d7 N (s t\u22121 |\u00b5 (i) , \u03a3 (i) )ds t\u22121 .(10)\nIn general, the integral in Eq. (10) is not analytically tractable. However, if p(u t |u t\u22121 , s t\u22121 ) were constant the integral could be solved easily.\nIn our model p(u t |u t\u22121 , s t\u22121 ) is the Gaussian CDF and has a sigmoidal shape. Because of this, it is approximately constant everywhere except near the transition point of the sigmoid. We determine whether p(u t |u t\u22121 , s t\u22121 ) can be considered constant and, if so, use an analytical approximation. Otherwise, we use a Monte Carlo approximation, drawing samples from N (s t\u22121 |\u00b5 (i) , \u03a3 (i) ). Finally the observation y t is  incorporated by multiplying two Gaussian PDFs. This algorithm can also be parallelized by assigning subsets of streets to different threads, a fact which we exploit to achieve realtime performance. Appendix A gives more details and the filtering process is summarized in Algorithm 1.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Managing Posterior Complexity", "text": "The previous section provides a basic algorithm to compute the filtering distributions recursively. Unfortunately, it is impractical as the complexity of the posterior (i.e., the number of mixture components) grows exponentially with time. To alleviate this, we propose three approximations which limit the resulting complexity of the posterior. We have found these approximations to work well in practice and to significantly reduce computational costs.\nFirst, for each pair of connected streets, the modes that transition from u t\u22121 to u t are all likely similar. As such, all of the transitioned modes are replaced with a single component using moment matching. Second, eventually most streets will have negligible probability. Thus, we truncate the distribution for streets whose probability p(u t |y 1:t ) is below a threshold and discard their modes. We use a conservative threshold of 10 \u221250 . Finally, the number of components in the posterior grows with t. Many of those components will have small weight and be redundant. To prevent this from happening, we run a mixture model simplification procedure when the number of modes on a street segment exceeds a threshold. This procedure removes components and updates others while keeping the KL divergence below a threshold . Details of this approximation can be found in Appendix B, and the effects of varying the maximum allowed KL divergence, , are investigated in the experiments. \n1.2 \u2022 1.3 \u2022 1.3 \u2022 1.3 \u2022 G 1.0 \u2022 1.0 \u2022 0.8 \u2022 1.4 \u2022 * 1.2 \u2022 * 1.5 \u2022 1.0 \u2022 0.9 \u2022 1.0 \u2022 1.0 \u2022 Table 1.\nSequence Errors: Average position and heading errors for 11 training sequences. \"M\" and \"S\" indicate monocular and stereo odometry, \"G\" GPS-based odometry and \"O\" is the oracle error, i.e., the error from projecting the GPS positions onto the map. Chance performance is 397m. All averages are computed over localized frames (see text) and \"*\" indicates sequences which did not localize.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "To evaluate our approach in realistic situations, we performed experiments on the recently released KITTI benchmark for visual odometry [8]. We utilize the 11 training sequences for quantitative evaluation (where ground truth GPS data is available), and perform qualitative evaluation on both training and test sequences (see Supplemental Material). This results in 39.2km of driving in total. The visual odometry input to our system is computed using LIBVISO2 [9], a freely available library for monocular and stereo visual odometry. To speed up inference, we subsample the data to a rate of one frame per second. Slower rates were found to suffer from excessive accumulated odometry error. For illustration purposes, here we extracted mid-size regions of OpenStreetMap data which included the true trajectory and the surrounding region. On average, they cover an area of 2km 2 and contain 47km of drivable roads. It is important to note that our method also localizes successfully on much larger maps, see Fig. 1 for example, which covers 18km 2 and contains 2,150km of drivable roads. We set the simplification threshold to = 10 \u22122 which is applied when the number of mixture components for a segment is greater than one per 10m segment length.\nQuantitative Evaluation: Quantitative results can be found in Table 1, with corresponding qualitative results shown in Fig. 8. Here, \"M\" and \"S\" indicate results using monocular and stereo visual odometry respectively. In addition, we computed odometry measurements from the GPS trajectories (entry \"G\" in the table) and ran our algorithm using the learned parameters from the stereo data. Note that this does not have access to absolute positions, but only relative position and orientation with respect to the previous frame. We also projected the GPS data onto the map data and measured the error produced by this projection. These errors, reported as \"O\" for oracle, are a lower bound on the best possible error to be achieved using the given map data. Note for some cases this error can be significant, as the map data does not account for lane widths, number of lanes or intersection sizes. Finally, we compute chance performance to be 397m by computing the average distance of the GPS data to the mean road position of each map.\nWe used the projected GPS data to learn the small number of model parameters. In particular, the street state evolution noise covariance \u03a3 x u , the angular AR(1) parameter \u03b3 u and the observation noise \u03a3 y u were estimated using maximum likelihood. We learn different parameters for highways and city/rural roads as the visual odometry performs significantly worse at higher speeds.\nThe accuracy of position and heading estimates is not well defined until the posterior has converged to a single mode. Thus, we only compute accuracy once a sequence has been localized. All results are divided into two temporally contiguous parts: unlocalized and localized. We define a sequence to be localized when for at least five seconds there is a single mode in the posterior and the distance to the ground truth position from that mode is less than 20 meters. Once the criteria for localization is met, all subsequent frames are considered localized. Errors in global position and heading of the MAP state for localized frames were computed using the GPS data as ground truth. Sequences which did not localize are indicated with a \"*\" in Table 1.\nOverall, we are able to estimate the position and heading to 3.1m and 1.3 \u2022 using stereo visual odometry. Note that this comes very close to the average oracle error of 1.44m, the lower bound on the achievable error induced by inaccuracies in the OpenStreetMap data! These results also outperform typical consumer grade navigation systems which offer accuracies of around 10 meters at best. Furthermore, errors are comparable to those achieved using the GPS-based odometry, suggesting the applicability and utility of low-cost vision-based sensors for localization. Using monocular odometry as input performs worse, but is still accurate to 18.4m and 3.6 \u2022 , once it is localized. However, due to its stronger drift, it fails to localize in some cases as in sequence 01. This sequence contains highway driving only, where high speeds and sparse visual features make monocular visual odometry very challenging, leading to an accumulated error in the monocular odometry of more than 500m. In contrast, while the stereo visual odometry has somewhat higher than typical errors on this sequence, our method is still able to localize successfully as shown in Fig. 8. Ambiguous Sequences: Sequences 04 and 06, shown in Fig. 7, are fundamentally ambiguous and cannot be localized with monocular, stereo or even GPS-based odometry. Sequence 04 is a short sequence on a straight road segment and, in the absence of any turns, cannot be localized beyond somewhere on the long road segment. Sequence 06 is longer and has turns, but traverses a symmetric path which results in a fundamental bimodality. In both cases our approach correctly indicates the set of probable locations.\nSimplification Threshold: We study the impact of varying the mixture model simplification threshold. Fig. 4 depicts computation time per frame and localized position error averaged over sequences as a function of the threshold, ranging from 10 \u22125 to 0.1 nats. We excluded sequences 04 and 06 as they are inherently ambiguous. As expected, computation time decreases and error increases with more simplification (i.e., larger threshold). However, there is a point of diminishing returns for computation time around 10 \u22122 nats, and little difference in error for smaller values. Thus we use a threshold of 10 \u22122 for all other experiments.\nMap Size: To investigate the impact of region size on localization performance, we assign uniform probability to portions of the map in a square region centered at the ground truth initial position and give zero initial probability to map locations outside the region. We varied the size of the square from 100m up to the typical map size of 2km, constituting an average of 300m to 47km of drivable road. We evaluated the time to localization for all non-ambiguous sequences (i.e., all but 04, 06) and plotted the average as a function of the region size in Fig. 5. As expected, small initial regions allow for faster localization. Somewhat surprisingly, after the region becomes sufficiently large, the impact on localization becomes negligible. This is due to the inherent uniqueness of most sufficiently long paths, even in very large regions with many streets as the one shown in Fig. 1. While localization in a large and truly perfect Manhattan world with equiangular intersections and equilength streets would be nearly impossible based purely on odometry, such a world is not often realized as even Manhattan itself has non-perpendicular roads such as Broadway! Noise: To study the impact of noise on the localization accuracy, we synthesized odometry measurements by adding Gaussian noise to the GPS-based odometry. For each sequence five different samples of noisy odometry were created with signal-to-noise ratios (SNR) ranging from 0.1 to 1000. Fig. 6 depicts error in position and heading after localization. As expected, error increases as the SNR decreases, however the performance scales well, showing little change in error until the SNR drops below 1.\nScalability: Running on 16 cores with a basic Python implementation, we are able to achieve real time results as shown in Fig. 4 (right). To test the ability of our method to scale to large maps we ran the sequences using stereo odometry and a map covering the entire urban district of Karlsruhe, Germany. This map was approximately 18km 2 and had over 2,150km of drivable road. Despite this, the errors were the same as with the smaller maps and, while computation was slower, it still only took around 10 seconds per frame on average. We expect this could be greatly improved with suitable optimizations. Results on sequence 02 are shown in Fig. 1 and more are available in the supplemental material.", "publication_ref": ["b7", "b8"], "figure_ref": ["fig_8", "fig_8", "fig_7", "fig_6"], "table_ref": []}, {"heading": "Conclusions", "text": "In this paper we have proposed an affordable approach to self-localization which employs (one or two) cameras mounted on the vehicle as well as crowd sourcing in the form of free online maps. We have demonstrated the effectiveness of our approach in a variety of diverse scenarios including highway, suburbs as well as crowded urban scenes. Furthermore, we have validated our approach on the KITTI visual odometry benchmark and shown that we are able to localize our vehicle with a precision of 3 m after only 20 seconds of driving. This is a new and exciting problem for computer vision and we believe there is much more to do. In particular, OpenStreetMaps contains many other salient pieces of information to aid in localization such as speed limits, street names, numbers of lanes, and more; we plan to exploit this information in the future. Finally, code and videos are available at http://www.cs.toronto.edu/\u02dcmbrubake. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Inference Details", "text": "To measure whether f (s t\u22121 ) = p(u t |x t\u22121 ) is constant for a mixture component N (\u00b5 (i) , \u03a3 (i) ) we consider the function g(\u00b5, \u03a3) = f (s t\u22121 )N (s t\u22121 |\u00b5, \u03a3)ds t\u22121 which, in the case of the Gaussian CDF form of f (s t\u22121 ) can be shown to be a Gaussian CDF (proof in Supplementary Material). Dropping the index, i, if d d\u00b5 g(\u00b5, \u03a3) < \u03b7 for \u03b7 = 10 \u22128 we consider f (s t\u22121 ) to be approximately constantly and the integral in Equation ( 10) can then be computed analytically as f (\u00b5)N (s t |A\u00b5 + b, \u03a3 x + A\u03a3A T ) which corresponds to the prediction step of a Kalman filter. If d d\u00b5 g(\u00b5, \u03a3) \u2265 \u03b7 then the mode overlaps the inflection point of f (s t\u22121 ) and the analytic model will not be a good approximation. Instead, we use a Monte Carlo approximation, drawing a set of M = 400 samples s (j) t\u22121 \u223c N (\u00b5, \u03a3) for j = 1, . . . , M and approximate the integral with a single component cN (s t |\u03bc,\u03a3) where\nt\u22121 ), and\u03bc,\u03a3 are found by moment matching to the Monte Carlo mixture approximation\nOnce the integral in Equation ( 10) is approximated we must incorporate the observation y t . Because the observations are Gauss-Linear and the integral approximations are Gaussians this consists of multiplying two Gaussian distributions as in the update step of the Kalman filter.\nPerforming the above for each component and each pair of nodes produces a set of mixture model components for each u, the weights of which are proportional to P t u . After normalizing the mixtures for each street, normalizing across streets allows for the computation of P t u , the probability of being on a given street. The procedure for recursively updating the posterior is summarized in Algorithm 1 and more details can be found in the Supplemental Material. To compute the upper bound of D(f g) we minimizeD(\u03c6, \u03c8, f, g) with respect to the variational parameters \u03c6 and \u03c8. Similarly, to update the components of g we minimizeD(\u03c6, \u03c8, f, g) with respect to the variational parameters \u03c6 and \u03c8 as well as the parameters \u03c9 b , \u00b5 b and \u03a3 b . While this objective function is non-convex, for each set of parameters individually the exact minima can be found, providing an efficient coordinate-descent algorithm. The update equations for \u03c6, \u03c8, \u03c9 b , \u00b5 b and \u03a3 b , along with the details of their derivation and a summary of the algorithm are found in the Supplementary Material.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Mixture Model Simplification", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Visual odometry priors for robust EKF-SLAM", "journal": "", "year": "2010", "authors": "P Alcantarilla; L Bergasa; F Dellaert"}, {"ref_id": "b1", "title": "Leveraging 3D City Models for Rotation Invariant Place-of-Interest Recognition", "journal": "IJCV", "year": "2012", "authors": "G Baatz; K K\u00f6ser; D Chen; R Grzeszczuk; M Pollefeys"}, {"ref_id": "b2", "title": "Real-time topometric localization", "journal": "", "year": "2012-05", "authors": "H Badino; D Huber; T Kanade"}, {"ref_id": "b3", "title": "FAB-MAP: Probabilistic Localization and Mapping in the Space of Appearance", "journal": "IJRR", "year": "2008", "authors": "M Cummins; P Newman"}, {"ref_id": "b4", "title": "Using the condensation algorithm for robust, vision-based mobile robot localization", "journal": "CVPR", "year": "1999", "authors": "F Dellaert; W Burgard; D Fox; S Thrun"}, {"ref_id": "b5", "title": "Simultaneous Localisation and Mapping (SLAM): Part I The Essential Algorithms", "journal": "", "year": "2006", "authors": "H Durrant-Whyte; T Bailey"}, {"ref_id": "b6", "title": "Monte carlo localization: Efficient position estimation for mobile robots", "journal": "", "year": "1999", "authors": "D Fox; W Burgard; F Dellaert; S Thrun"}, {"ref_id": "b7", "title": "Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite", "journal": "", "year": "2012", "authors": "A Geiger; P Lenz; R Urtasun"}, {"ref_id": "b8", "title": "Stereoscan: Dense 3d reconstruction in real-time", "journal": "", "year": "2011", "authors": "A Geiger; J Ziegler; C Stiller"}, {"ref_id": "b9", "title": "An experimental comparison of localization methods", "journal": "", "year": "1998", "authors": "J.-S Gutmann; W Burgard; D Fox; K Konolige"}, {"ref_id": "b10", "title": "Efros. im2gps: estimating geographic information from a single image", "journal": "", "year": "2008", "authors": "J Hays; A A "}, {"ref_id": "b11", "title": "Approximating the Kullback-Leibler Divergence Between Gaussian Mixture Models", "journal": "", "year": "2007", "authors": "J Hershey; P Olsen"}, {"ref_id": "b12", "title": "iSAM2: Incremental smoothing and mapping using the Bayes tree. IJRR", "journal": "", "year": "2012", "authors": "M Kaess; H Johannsson; R Roberts; V Ila; J J Leonard; F Dellaert"}, {"ref_id": "b13", "title": "Flow separation for fast and robust stereo odometry", "journal": "", "year": "2009", "authors": "M Kaess; K Ni; F Dellaert"}, {"ref_id": "b14", "title": "Map-based precision vehicle localization in urban environments", "journal": "", "year": "2007", "authors": "J Levinson; M Montemerlo; S Thrun"}, {"ref_id": "b15", "title": "Worldwide pose estimation using 3d point clouds", "journal": "", "year": "2012", "authors": "Y Li; N Snavely; D Huttenlocher; P Fua"}, {"ref_id": "b16", "title": "Real: A system for large-scale mapping in constant-time using stereo", "journal": "IJCV", "year": "2010", "authors": "C Mei; G Sibley; M Cummins; P Newman; I Reid"}, {"ref_id": "b17", "title": "Fast-SLAM 2.0: An improved particle filtering algorithm for simultaneous localization and mapping that provably converges", "journal": "", "year": "2003", "authors": "M Montemerlo; S Thrun; D Koller; B Wegbreit"}, {"ref_id": "b18", "title": "Visual odometry", "journal": "", "year": "2004", "authors": "D Nister; O Naroditsky; J R Bergen"}, {"ref_id": "b19", "title": "Map-based priors for localization", "journal": "", "year": "2004", "authors": "S M Oh; S Tariq; B N Walker; F Dellaert"}, {"ref_id": "b20", "title": "FAB-MAP 3D: Topological mapping with spatial and visual appearance", "journal": "", "year": "2010", "authors": "R Paul; P Newman"}, {"ref_id": "b21", "title": "A discriminative approach to robust visual place recognition", "journal": "", "year": "2006", "authors": "A Pronobis; B Caputo; P Jensfelt; H Christensen"}, {"ref_id": "b22", "title": "Place classification of indoor environments with mobile robots using boosting", "journal": "", "year": "2005", "authors": "A Rottmann; O Mozos; C Stachniss; W Burgard"}, {"ref_id": "b23", "title": "Fast image-based localization using direct 2d-to-3d matching", "journal": "", "year": "2011", "authors": "T Sattler; B Leibe; L Kobbelt"}, {"ref_id": "b24", "title": "Mobile robot localization and mapping with uncertainty using scale-invariant visual landmarks", "journal": "IJRR", "year": "2002", "authors": "S Se; D Lowe; J Little"}, {"ref_id": "b25", "title": "A comparison of loop closing techniques in monocular SLAM", "journal": "RAS", "year": "2009", "authors": "B Williams; M Cummins; J Neira; P Newman; I Reid; J Tard\u00f3s"}, {"ref_id": "b26", "title": "Where am I: Place instance and category recognition using spatial PACT", "journal": "", "year": "2008", "authors": "J Wu; J M Rehg"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 .2Figure 2. Evolution of OpenStreetMap coverage from 2006-2012: As of 2012, over 3 billion GPS track points have been added and 1.6 billion nodes / 150 million line segments have been created by the community. Here we use OSM maps and visual odometry estimates as the only inputs for localizing within the map.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "1 Figure 3 .13Figure 3. Map Graph: (left) A simple map and its corresponding graph representation. Street Segment: (right) Each street segment has a start and end position p0 and p1, a length , an initial heading of the street segment \u03b2 and a curvature parameter \u03b1 = \u03c8 1 \u2212\u03c8 0 . For arc segments c is the circle center, r is the radius and \u03c80 and \u03c81 are the start and end angles of the arc. For linear segments, \u03b1 = 0.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "13 :13Set P t u to the sum of the weights of mixture M t u 14:", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .Figure 5 .45Figure 4. Simplification Threshold: Impact of the simplification threshold on localization accuracy (left) and computation time (right). We use = 10 \u22122 for all other experiments.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 .6Figure 6. Localization Accuracy with Noise: Position and heading error with different noise levels. Averaged over five independent samples of noise.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Ambiguous Sequences: Both 04 and 06 cannot be localized due to fundamental ambiguities. Sequence 04 consists of a short, straight driving sequence and 06 traverses a symmetric part of the map, resulting in two equally likely modes.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 .8Figure 8. Selected Frames: Inference results for some of the sequences, full results can be found in the supplemental material. The left most column shows the full map region for each sequence, followed by zoomed in sections of the map showing the posterior distribution over time. The black line is the GPS trajectory and the concentric circles indicate the current GPS position. Grid lines are every 500m.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The curvature of the street, \u03b1 u , is necessary because the global heading of the vehicle depends on both d and \u03b8. We factorize the state transition distribution p(x t |x t\u22121 ) = p(u t |x t\u22121 )p(s t |u t , x t\u22121 ) in terms of the street transition probability p(u t |x t\u22121 ), and the state transition model p(s t |u t , x t\u22121 ). The state transition model is assumed to be Gauss-Linear, taking the form", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Input: Posterior at t\u22121, {P t\u22121 u , M t\u22121u }, and observation, yt 2: Initialize mixtures, M t u \u2190 \u2205, for all u 3: for all streets ut\u22121 do", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u2212d p 0 + d p 1 for a linear segment and c+rd( \u2212d \u03c8 0 + d \u03c8 ) for a circular arc segment, with d(\u03b8) = (cos \u03b8, sin \u03b8) T .", "formula_coordinates": [3.0, 50.11, 330.16, 236.25, 23.13]}, {"formula_id": "formula_1", "formula_text": "p(y t |x t ) = N (y t |M ut s t , \u03a3 y ut )(1)", "formula_coordinates": [3.0, 105.64, 464.77, 180.72, 12.71]}, {"formula_id": "formula_2", "formula_text": "M u = [m d , m \u03b8 ] T , m d = (1, \u22121, 0, 0) T and m \u03b8 = (\u03b1 u , \u2212\u03b1 u , 1, \u22121) T .", "formula_coordinates": [3.0, 50.11, 488.31, 236.25, 23.18]}, {"formula_id": "formula_3", "formula_text": "p(s t |u t , x t\u22121 ) = N (s t |A ut,ut\u22121 s t\u22121 + b ut,ut\u22121 , \u03a3 x ut )(2", "formula_coordinates": [3.0, 58.56, 594.51, 223.93, 23.0]}, {"formula_id": "formula_4", "formula_text": "A ut,ut\u22121 = \uf8ee \uf8ef \uf8ef \uf8f0 2 \u22121 0 0 1 0 0 0 0 0 \u03b3 ut 0 0 \u03b1 ut\u22121 \u2212 \u03b1 ut 1 0 \uf8f9 \uf8fa \uf8fa \uf8fb (3) b ut,ut\u22121 = \u2212( ut\u22121 , ut\u22121 , 0, \u03b8 ut,ut\u22121 ) T u t = u t\u22121 (0, 0, 0, 0) T u t = u t\u22121 (4)", "formula_coordinates": [3.0, 311.63, 156.05, 233.48, 86.93]}, {"formula_id": "formula_5", "formula_text": "\u03b8 ut,ut\u22121 = \u03b2 ut \u2212 (\u03b2 ut\u22121 + \u03b1 ut ut\u22121 )", "formula_coordinates": [3.0, 336.66, 252.37, 157.66, 10.32]}, {"formula_id": "formula_6", "formula_text": "p(u t |x t\u22121 ) = \u03be ut,ut\u22121 u t\u22121 + u u t\u22121 N (x|a T d s t\u22121 , a T d \u03a3 x ut\u22121 a d )dx (5) where a d = (2, \u22121, 0, 0), \u03be ut,ut\u22121 = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 u t = u t\u22121 1 |N(uj )| u t \u2208 N(u t\u22121 ) 0 otherwise (6)", "formula_coordinates": [3.0, 308.86, 328.71, 236.25, 100.38]}, {"formula_id": "formula_7", "formula_text": "Nu t i=1 \u03c0 (i) ut N (s t |\u00b5 (i) ut , \u03a3 (i) ut )(7)", "formula_coordinates": [3.0, 411.88, 684.55, 133.23, 31.78]}, {"formula_id": "formula_8", "formula_text": "5: for k = 1, . . . , |M t\u22121 u t\u22121 | do 6:", "formula_coordinates": [4.0, 54.67, 131.67, 137.44, 20.57]}, {"formula_id": "formula_9", "formula_text": "Posterior at t, {P t u , M t u }", "formula_coordinates": [4.0, 98.38, 259.24, 91.17, 10.63]}, {"formula_id": "formula_10", "formula_text": "(i) ut , \u00b5 (i) ut , \u03a3 (i) ut } Nu t", "formula_coordinates": [4.0, 184.6, 304.63, 69.36, 13.06]}, {"formula_id": "formula_11", "formula_text": "N i=1 \u03c0 (i) p(u t |u t\u22121 ,s t\u22121 )N (s t |As t\u22121 + b, \u03a3 x ) \u00d7 N (s t\u22121 |\u00b5 (i) , \u03a3 (i) )ds t\u22121 .(10)", "formula_coordinates": [4.0, 57.49, 541.65, 228.87, 46.14]}, {"formula_id": "formula_12", "formula_text": "1.2 \u2022 1.3 \u2022 1.3 \u2022 1.3 \u2022 G 1.0 \u2022 1.0 \u2022 0.8 \u2022 1.4 \u2022 * 1.2 \u2022 * 1.5 \u2022 1.0 \u2022 0.9 \u2022 1.0 \u2022 1.0 \u2022 Table 1.", "formula_coordinates": [5.0, 50.11, 146.91, 476.8, 38.21]}], "doi": ""}