{"title": "UNIVERSAL FEW-SHOT LEARNING OF DENSE PREDIC-TION TASKS WITH VISUAL TOKEN MATCHING", "authors": "Donggyun Kim; Jinwoo Kim; Seongwoong Cho; Chong Luo; Seunghoon Hong", "pub_date": "2023-03-27", "abstract": "Dense prediction tasks are a fundamental class of problems in computer vision. As supervised methods suffer from high pixel-wise labeling cost, a few-shot learning solution that can learn any dense task from a few labeled images is desired. Yet, current few-shot learning methods target a restricted set of tasks such as semantic segmentation, presumably due to challenges in designing a general and unified model that is able to flexibly and efficiently adapt to arbitrary tasks of unseen semantics. We propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. It employs non-parametric matching on patchlevel embedded tokens of images and labels that encapsulates all tasks. Also, VTM flexibly adapts to any task with a tiny amount of task-specific parameters that modulate the matching algorithm. We implement VTM as a powerful hierarchical encoder-decoder architecture involving ViT backbones where token matching is performed at multiple feature hierarchies. We experiment VTM on a challenging variant of Taskonomy dataset and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive with fully supervised baselines using only 10 labeled examples of novel tasks (0.004% of full supervision) and sometimes outperforms using 0.1% of full supervision. Codes are available at https://github.com/GitGyun/visual_token_matching.", "sections": [{"heading": "INTRODUCTION", "text": "Dense prediction tasks constitute a fundamental class of computer vision problems, where the goal is to learn a mapping from an input image to a pixel-wise annotated label. Examples include semantic segmentation, depth estimation, edge detection, and keypoint detection, to name a few (Zamir et al., 2018;Cai & Pu, 2019). While supervised methods achieved remarkable progress, they require a substantial amount of manually annotated pixel-wise labels, leading to a massive and often prohibitive per-task labeling cost (Kang et al., 2019;Liu et al., 2020;Ouali et al., 2020). Prior work involving transfer and multi-task learning have made efforts to generally relieve the burden, but they often assume that relations between tasks are known in advance, and still require a fairly large amount of labeled images of the task of interest (e.g., thousands) (Zamir et al., 2018;Standley et al., 2020;O Pinheiro et al., 2020;Wang et al., 2021). This motivates us to seek a few-shot learning solution that can universally learn arbitrary dense prediction tasks from a few (e.g., ten) labeled images.\nHowever, existing few-shot learning methods for computer vision are specifically targeted to solve a restricted set of tasks, such as classification, object detection, and semantic segmentation (Vinyals et al., 2016;Kang et al., 2019;Min et al., 2021). As a result, they often exploit prior knowledge and assumptions specific to these tasks in designing model architecture and training procedure, therefore not suited for generalizing to arbitrary dense prediction tasks (Snell et al., 2017;Fan et al., 2022;Iqbal et al., 2022;Hong et al., 2022). To our knowledge, no prior work in few-shot learning provided approaches to solve arbitrary dense prediction tasks in a universal manner.\nWe argue that designing a universal few-shot learner for arbitrary dense prediction tasks must meet the following desiderata. First, the learner must have a unified architecture that can handle arbitrary tasks by design, and share most of the parameters across tasks so that it can acquire generalizable knowledge for few-shot learning of arbitrary unseen tasks. Second, the learner should flexibly adapt its prediction mechanism to solve diverse tasks of unseen semantics, while being efficient enough to prevent over-fitting. Designing such a learner is highly challenging, as it should be general and unified while being able to flexibly adapt to any unseen task without over-fitting few examples.\nPublished as a conference paper at ICLR 2023 In this work, we propose Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks. We draw inspiration from the cognitive process of analogy making (Mitchell, 2021); given a few examples of a new task, humans can quickly understand how to relate input and output based on a similarity between examples (i.e., assign similar outputs to similar inputs), while flexibly changing the notion of similarity to the given context. In VTM, we implement analogymaking for dense prediction as patch-level non-parametric matching, where the model learns the similarity in image patches that captures the similarity in label patches. Given a few labeled examples of a novel task, it first adapts its similarity that describes the given examples well, then predicts the labels of an unseen image by combining the label patches of the examples based on image patch similarity. Despite the simplicity, the model has a unified architecture for arbitrary dense prediction tasks since the matching algorithm encapsulates all tasks and label structures (e.g., continuous or discrete) by nature. Also, we introduce only a small amount of task-specific parameters, which makes our model robust to over-fitting as well as flexible. Our contributions are as follows.\n(1) For the first time to our knowledge, we propose and tackle the problem of universal few-shot learning of arbitrary dense prediction tasks. We formulate the problem as episodic meta-learning and identify two key desiderata of the learner -unified architecture and adaptation mechanism.\n(2) We propose Visual Token Matching (VTM), a novel universal fewshot learner for dense prediction tasks. It employs non-parametric matching on tokenized image and label embeddings, which flexibly adapts to unseen tasks using a tiny amount of task-specific parameters. (3) We implement VTM as a powerful hierarchical encoder-decoder architecture, where token matching is performed at multiple feature hierarchies using attention mechanism. We employ ViT image and label encoders (Dosovitskiy et al., 2020) and a convolutional decoder (Ranftl et al., 2021), which seamlessly works with our algorithm. (4) We demonstrate VTM on a challenging variant of Taskonomy dataset (Zamir et al., 2018) and observe that it robustly few-shot learns various unseen dense prediction tasks. Surprisingly, it is competitive to or outperforms fully supervised baselines given extremely few examples (0.1%), sometimes using only 10 labeled images (< 0.004%).", "publication_ref": ["b47", "b5", "b20", "b28", "b33", "b47", "b38", "b32", "b40", "b20", "b36", "b11", "b17", "b15", "b31", "b8", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "PROBLEM SETUP", "text": "We propose and tackle the problem of universal few-shot learning of arbitrary dense prediction tasks. In our setup, we consider any arbitrary task T that can be expressed as follows:\nT : R H\u00d7W \u00d73 \u2192 R H\u00d7W \u00d7C T , C T \u2208 N.\n(1)\nThis subsumes a wide range of vision tasks including semantic segmentation, depth estimation, surface normal prediction, edge prediction, to name a few, varying in structure of output space, e.g., dimensionality (C T ) and topology (discrete or continuous), as well as the required knowledge.\nOur goal is to build a universal few-shot learner F that, for any such task T , can produce prediction\u015d Y q for an unseen image (query) X q given a few labeled examples (support set) S T :\nY q = F(X q ; S T ), S T = {(X i , Y i )} i\u2264N .(2)\nTo build such a universal few-shot learner F, we adopt the conventional episodic training protocol where the training is composed of multiple episodes, each simulating a few-shot learning problem.\nTo this end, we utilize a meta-training dataset D train that contains labeled examples of diverse dense prediction tasks. Each training episode simulates a few-shot learning scenario of a specific task T train in the dataset -the objective is to produce correct labels for query images given a support set. By experiencing multiple episodes of few-shot learning, the model is expected to learn general knowledge for fast and flexible adaptation to novel tasks. At test time, the model is asked to perform few-shot learning on arbitrary unseen tasks T test not included in the training dataset (D train ).\nAn immediate challenge in handling arbitrary tasks in Eq. 1 is that each task in both meta-training and testing has different output structures (i.e., output dimension C T varies per task), making it difficult to design a single, unified parameterization of a model for all tasks. As a simple yet general solution, we cast a task T : R H\u00d7W \u00d73 \u2192 R H\u00d7W \u00d7C T into C T single-channel sub-tasks T 1 , \u2022 \u2022 \u2022 , T C T of learning each channel, and model each sub-task T c : R H\u00d7W \u00d73 \u2192 R H\u00d7W \u00d71 independently using the shared model F in Eq. 2. Although multi-channel information is beneficial in general, we observe that its impact is negligible in practice, while the channel-wise decomposition introduces other useful benefits such as augmenting the number of tasks in meta-training, flexibility to generalize to arbitrary dimension of unseen tasks, and more efficient parameter-sharing within and across tasks. Without loss of generality, the rest of the paper considers that every task is of single-channel label.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CHALLENGES AND DESIDERATA", "text": "The above problem setup is universal, potentially benefiting various downstream vision applications. Yet, to our knowledge, no prior work in few-shot learning attempted to solve it. We attribute this to two unique desiderata for a universal few-shot learner that pose a challenge to current methods. Task-Agnostic Architecture As any arbitrary unseen task T test can be encountered in test-time, the few-shot learner must have a unified architecture that can handle all dense prediction tasks by design. This means we cannot exploit any kind of prior knowledge or inductive bias specific to certain tasks. For example, we cannot adopt common strategies in few-shot segmentation such as class prototype or binary masking, as they rely on the discrete nature of categorical labels while the label space of T test can be arbitrary (continuous or discrete). Ideally, the unified architecture would allow the learner to acquire generalizable knowledge for few-shot learning any unseen tasks, as it enables sharing most of the model parameters across all tasks in meta-training and testing.\nAdaptation Mechanism On top of the unified architecture, the learner should have a flexible and efficient adaptation mechanism to address highly diverse semantics of the unseen tasks T test . This is because tasks of different semantics can require distinct sets of features -depth estimation requires 3D scene understanding, while edge estimation prefers low-level image gradient. As a result, even with a unified architecture, a learner that depends on a fixed algorithm and features would either underfit to various training tasks in D train , or fail for an unseen task T test that has a completely novel semantics. Thus, our few-shot learner should be flexibly adapt its features to a given task T based on the support set S T , e.g., through task-specific parameters. At the same time, the adaptation mechanism should be parameter-efficient (e.g., using a tiny amount of task-specific parameters) to prevent over-fitting to training tasks T train or test-time support set S Ttest .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "VISUAL TOKEN MATCHING", "text": "We introduce Visual Token Matching (VTM), a universal few-shot learner for arbitrary dense prediction tasks that is designed to flexibly adapt to novel tasks of diverse semantics. We first discuss the underlying motivation of VTM in Section 3.1 and discuss its architecture in Section 3.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MOTIVATION", "text": "We seek for a general function form of Eq. 2 that produces structured labels of arbitrary tasks with a unified framework. To this end, we opt into a non-parametric approach that operates on patches, where the query label is obtained by weighted combination of support labels. Let X = {x j } j\u2264M denote an image (or label) on patch grid of size M = h \u00d7 w, where x j is j-th patch. Given a query image X q = {x q j } j\u2264M and a support set\n{(X i , Y i )} i\u2264N = {(x i k , y i k )} k\u2264M,i\u2264N\nfor a task T , we project all patches to embedding spaces and predict the query label Y q = {y q j } j\u2264M patch-wise by,\ng(y q j ) = i\u2264N k\u2264M \u03c3 f T (x q j ), f T (x i k ) g(y i k ),(3)\nwhere f T (x) = f (x; \u03b8, \u03b8 T ) \u2208 R d and g(y) = g(y; \u03c6) \u2208 R d correspond to the image and label encoder, respectively, and \u03c3 :\nR d \u00d7 R d \u2192 [0, 1]\ndenotes a similarity function defined on the image patch embeddings. Then, each predicted label embedding can be mapped to a label patch\u0177 q j = h(g(y q j )) by introducing a label decoder h \u2248 g \u22121 . Note that Eq. 3 generalizes the Matching Network (MN) (Vinyals et al., 2016), and serves as a general framework for arbitrary dense prediction tasks 1 . First, while MN interpolates raw categorical labels y for classification (i.e., g is an identity function), we perform matching on the general embedding space of the label encoder g(y); it encapsulates arbitrary tasks (e.g., discrete or continuous) into the common embedding space, thus enabling the matching to work in a consistent way agnostic to tasks. Second, while MN exploits a fixed similarity of images \u03c3(f (x q ), f (x i )), we modulate the similarity \u03c3(f T (x q ), f T (x i )) adaptively to any given task by switching the task-specific parameters \u03b8 T . Having adaptable components in the image encoder f T is essential in our problem since it can adapt the representations to reflect features unique in each task. Finally, our method employs an additional decoder h to project the predicted label embeddings to the output space.\nOnce trained, the prediction mechanism in Eq. 3 can easily adapt to unseen tasks at test-time. Since the label encoder g is shared across tasks, we can use it to embed the label patches of unseen tasks with frozen parameters \u03c6. Adaptation to a novel task is performed by the image encoder f T , by optimizing the task-specific parameters \u03b8 T which take a small portion of the model. This allows our model to robustly adapt to unseen tasks of various semantics with a small support set. In experiments, our model becomes competitive to supervised method using less than 0.1% of labels.", "publication_ref": ["b40"], "figure_ref": [], "table_ref": []}, {"heading": "ARCHITECTURE", "text": "Figure 1 illustrates our model. Our model has a hierarchical encoder-decoder architecture that implements patch-level non-parametric matching of Eq. 3 in multiple hierarchies with four components: image encoder f T , label encoder g, label decoder h, and the matching module. Given the query image and the support set, the image encoder first extracts patch-level embeddings (tokens) of each query and support image independently. The label encoder similarly extracts tokens of each support label. Given the tokens at each hierarchy, the matching module performs non-parametric matching of Eq. 3 to infer the tokens of query label, from which the label decoder forms the raw query label.\nImage Encoder We employ a Vision Transformer (ViT) (Dosovitskiy et al., 2020) for our image encoder. The ViT is applied to query and each support image independently while sharing weights, which produces tokenized representation of image patches at multiple hierarchies. Similar to Ranftl et al. (2021), we extract tokens at four intermediate ViT blocks to form hierarchical features. To aid learning general representation for a wide range of tasks, we initialize the parameters from pretrained BEiT (Bao et al., 2021), which is self-supervised thus less biased towards specific tasks.\nAs discussed in Section 3.1, we design the image encoder to have two sets of parameters \u03b8 and \u03b8 T , where \u03b8 is shared across all tasks and \u03b8 T is specific to each task T . Among many candidates to design an adaptation mechanism through \u03b8 T , we find that bias tuning (Cai et al., 2020;Zaken et al., 2022) provides the best efficiency and performance empirically. To this end, we employ separate sets of biases for each task in both meta-training and meta-testing, while sharing all the other parameters.", "publication_ref": ["b8", "b4"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Label Encoder", "text": "The label encoder employs the same ViT architecture as the image encoder and extracts token embeddings of the support labels. Similar to the image encoder, the label encoder is applied to each support label independently, with a major difference that it sees one channel at a time since we treat each channel as an independent task as discussed in Section 2. Then the label tokens are extracted from multiple hierarchies that matches the image encoder. Contrary to the image encoder, all parameters of the label encoder are trained from scratch and shared across tasks.\nMatching Module We implement the matching module at each hierarchy as a multihead attention layer (Vaswani et al., 2017). At each hierarchy of the image and label encoder, we first obtain the tokens of the query image X q as {q j } j\u2264M and support set {(X i , Y i )} i\u2264N as {(k i k , v i k )} k\u2264M,i\u2264N from the intermediate layers of image and label encoders, respectively. We then stack the tokens to row-matrices, q \u2208 R M \u00d7d and k, v \u2208 R N M \u00d7d . Then, the query label tokens at the hierarchy are inferred as output of a multihead attention layer, as follows:\nMHA(q, k, v) = Concat(o 1 , ..., o H )w O ,(4)\nwhere\no h = Softmax qw Q h (kw K h ) \u221a d H vw V h ,(5)\nwhere H is number of heads, d H is head size, and w Q h , w K h , w V h \u2208 R d\u00d7d H , w O \u2208 R Hd H \u00d7d . Remarkably, each attention head in Eq. 5 implements the intuition of the non-parametric matching in Eq. 3. This is because each query label token is inferred as a weighted combination of support label tokens v, based on the similarity between the query and support image tokens q, k. Here, the similarity function \u03c3 of Eq. 3 is implemented as scaled dot-product attention. Since each head involves different trainable projection matrices w Q h , w K h , w V h , the multihead attention layer in Eq. 4 is able to learn multiple branches (heads) of matching algorithm with distinct similarity functions.", "publication_ref": ["b39"], "figure_ref": [], "table_ref": []}, {"heading": "Label Decoder", "text": "The label decoder h receives the query label tokens inferred at multiple hierarchies, and combines them to predict the query label of original resolution. We adopt the multi-scale decoder architecture of Dense Prediction Transformer (Ranftl et al., 2021) as it seamlessly works with ViT encoders and multi-level tokens. At each hierarchy of the decoder, the inferred query label tokens are first spatially concatenated to a feature map of constant size (M \u2192 h \u00d7 w). Then, (transposed) convolution layers of different strides are applied to each feature map, producing a feature pyramid of increasing resolution. The multi-scale features are progressively upsampled and fused by convolutional blocks, followed by a convolutional head for final prediction.\nSimilar to the label encoder, all parameters of the label decoder are trained from scratch and shared across tasks. This lets the decoder to meta-learn a generalizable strategy of decoding a structured label from the predicted query label tokens. Following the channel split in Section 2, the output of the decoder is single-channel, which allows it to be applied to tasks of arbitrary number of channels.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TRAINING AND INFERENCE", "text": "We train our model on a labeled dataset D train of training tasks T train following the standard episodic meta-learning protocol. At each episode of task T , we sample two labeled sets S T , Q T from D train .\nThen we train the model to predict labels in Q T using S T as support set. We repeat the episodes with various dense prediction tasks in D train so that the model can learn a general knowledge about few-shot learning. Let F(X q ; S T ) denote the prediction of the model on X q using the support set S T . Then the model (f T , g, h, \u03c3) is trained by the following learning objective in end-to-end:\nmin f T ,g,h,\u03c3 E S T ,Q T \u223cDtrain \uf8ee \uf8f0 1 |Q T | (X q ,Y q )\u2208Q T L (Y q , F(X q ; S T ))) \uf8f9 \uf8fb ,(6)\nwhere L is the loss function. We use cross-entropy loss for semantic segmentation task and L1 loss for the others in our experiments. Note that the objective (Eq. 6) does not explicitly enforce the matching equation (Eq. 3) in token space, allowing some knowledge for prediction to be handled by the label decoder h, since we found that introducing explicit reconstruction loss on tokens deteriorates the performance in our initial experiments. During training, as we have a fixed number of training tasks T train , we keep and train separate sets of bias parameters of the image encoder f T for each training task (which are assumed to be channel-splitted).\nAfter training on D train , the model is few-shot evaluated on novel tasks T test given a support set S Ttest . We first perform adaptation of the model by fine-tuning bias parameters of the image encoder f T using the support set S Ttest . For this, we simulate episodic meta-learning by randomly partitioning the support set into a sub-support setS and a sub-query setQ, such that S Ttest =S\u222aQ.\nmin \u03b8 T ES ,Q\u223cS T test \uf8ee \uf8f0 1 |Q| (X q ,Y q )\u2208Q L Y q , F(X q ;S) \uf8f9 \uf8fb ,(7)\nwhere \u03b8 T denotes bias parameters of the image encoder f T . The portion of parameters to be finetuned is negligible so the model can avoid over-fitting on the small support set S Ttest . After fine-tuned, the model is evaluated by predicting the label of unseen query image using the support set S Ttest .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "To the best of our knowledge, the problem of universal few-shot learning of dense prediction tasks remains unexplored. Existing few-shot learning approaches for dense prediction are targeted to specific tasks that require learning unseen classes of objects, such as semantic segmentation (Shaban et al., 2017;Iqbal et al., 2022), instance segmentation (Michaelis et al., 2018;Fan et al., 2020b), and object detection (Fan et al., 2020a;Wang et al., 2020), rather than general tasks.\nAs categorical labels are discrete in nature, most of the methods involve per-class average pooling of support image features, which cannot be generalized to regression tasks as there would be infinitely many \"classes\" of continuous labels. Others utilize masked correlation between support and query features (Min et al., 2021;Hong et al., 2022), learn a Gaussian Process on features (Johnander et al., 2021), or train a classifier weight prediction model (Kang et al., 2019). In principle, these architectures can be extended to more general dense prediction tasks with slight modification (Section 5), yet their generalizability to unseen dense prediction tasks, rather than classes, has not been explored.\nAs our method involves task-specific tuning of a small portion of parameters, it is related to transfer learning that aims to efficiently fine-tune a pre-trained model to downstream tasks. In natural language processing (NLP), language models pre-trained on large-scale corpus (Kenton & Toutanova, 2019;Brown et al., 2020) show outstanding performance on downstream tasks with fine-tuning a minimal amount of parameters (Houlsby et al., 2019;Zaken et al., 2022;Lester et al., 2021). Following the emergence of pre-trained Vision Transformers (Dosovitskiy et al., 2020), similar adaptation approaches have been proven successful in the vision domain (Li et al., 2021;Jia et al., 2022;. While these approaches reduce the amount of parameters required for state-of-the-art performance on downstream tasks, they still require a large amount of labeled images for fine-tuning (e.g., thousands). In this context, our method can be seen as a few-shot extension of the adaptation methods, by incorporating a general few-shot learning framework and a powerful architecture.", "publication_ref": ["b35", "b17", "b29", "b12", "b10", "b42", "b15", "b19", "b20", "b3", "b16", "b8", "b25", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL SETUP", "text": "Dataset We construct a variant of the Taskonomy dataset (Zamir et al., 2018) to simulate fewshot learning of unseen dense prediction tasks. Taskonomy contains indoor images with various annotations, where we choose ten dense prediction tasks of diverse semantics and output dimensions: semantic segmentation (SS), surface normal (SN), Euclidean distance (ED), Z-buffer depth (ZD), texture edge (TE), occlusion edge (OE), 2D keypoints (K2), 3D keypoints (K3), reshading (RS), and principal curvature (PC), 2 . We partition the ten tasks to construct a 5-fold split, in each of which two tasks are used for few-shot evaluation (T test ) and the remaining eight are used for training (T train ). To perform evaluation on tasks of novel semantics, we carefully construct the partition such that tasks for training and test are sufficiently different from each other e.g., by grouping edge tasks (TE, OE) together as test tasks. The split is shown in Table 1. We process some single-channel tasks (ED, TE, OE) to multiple channels to increase task diversity, and standardize all labels to [0, 1]. Additional details are in Appendix A.\nBaselines We compare our method (VTM) with two classes of learning approaches.\n\u2022 Fully supervised baselines have an access to the full supervision of test tasks T test during training, and thus serve as upper bounds of few-shot performance. We consider two state-of-theart baselines in supervised learning and multi-task learning of general dense prediction tasks -DPT (Ranftl et al., 2021) and InvPT (Ye & Xu, 2022), respectively, where DPT is trained on each single task independently and InvPT is trained jointly on all tasks.\n\u2022 Few-shot learning baselines do not have an access to the test tasks T test during training, and are given only a few labeled images at the test-time. As there are no prior few-shot method developed for universal dense prediction tasks, we adapt state-of-the-art few-shot segmentation methods to our setup. We choose three methods, DGPNet (Johnander et al., 2021), HSNet (Min et al., 2021), and VAT (Hong et al., 2022), whose architectures are either inherently task-agnostic (DGPNet) or can be simply extended (HSNet, VAT) to handle general label spaces for dense prediction tasks. We describe the modification on HSNet and VAT in Appendix B.  with self-supervision. For the other baselines (DPGNet, HSNet, VAT), as they rely on convolutional encoder and it is nontrivial to transfer them to ViT, we use ResNet-101 (He et al., 2016) backbone pre-trained on ImageNet-1k with image classification, which is their best-performing configuration.\n\u2191 mErr \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193\nDuring episodic training, we perform task augmentation based on color jittering and MixUp (Zhang et al., 2018) to increase the effective number of training tasks. For all few-shot learning models, we further apply label channel split as described in Section 2. Further details are in Appendix B.\nEvaluation Protocol We use the train/val/test split of the Taskonomy-tiny partition provided by Zamir et al. (2018). We train all models on the train split, where DPT and InvPT are trained with full supervision of test tasks T test and the few-shot models are trained by episodic learning of training tasks T train only. The final evaluation on test tasks T test is done on the test split. During the evaluation, all few-shot models are given a support set randomly sampled from the train split, which is also used for task-specific adaptation of VTM as described in Section 3.3. For evaluation on semantic segmentation (SS), we follow the standard binary segmentation protocol in few-shot semantic segmentation (Shaban et al., 2017) and report the mean intersection over union (mIoU) for all classes.\nFor tasks with continuous labels, we use the mean angle error (mErr) for surface normal prediction (SN) (Eigen & Fergus, 2015) and root mean square error (RMSE) for the others.", "publication_ref": ["b47", "b45", "b19", "b15", "b13", "b48", "b47", "b35", "b9"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "RESULTS", "text": "In Table 1, we report the 10-shot performance of our model and the baselines on ten dense prediction tasks. Our model outperforms all few-shot baselines by a large margin, and is competitive with supervised baselines on many tasks. In Figure 2, we show a qualitative comparison where the fewshot baselines catastrophically underfit to novel tasks while our model successfully learns all tasks. We provide further qualitative comparisons of ours and the baselines in Appendix C.3.\nThe large performance gap between the few-shot learning baselines and our model can be explained by two factors.\n(1) The core architectural component of HSNet and VAT (feature masking) implicitly relies on the discrete nature of labels, and thus fails to learn tasks with continuous labels whose values are densely distributed.\n(2) Since the baselines are designed to solve tasks without any adaptation of their parameters, the core prediction mechanism (e.g., hypercorrelation of HSNet and VAT, kernel of DGPNet) is fixed and cannot adapt to different semantics of tasks. Unlike the baselines, our model is general for all dense prediction tasks, and has a flexible task adaptation mechanism of the similarity in matching. Our model can also be robustly fine-tuned on few-shot support, thanks to the parameter efficiency of adaptation (0.28% of all parameters; see Table 7 for comparison with supervised baselines). To demonstrate how VTM performs task adaptation, we visualize the attention of the matching module (Eq. 4). Figure 4 shows that, when adapted for different tasks, the model flexibly changes the similarity to attend to support patches appropriate for the given task. Surprisingly, with < 0.004% of the full supervision (10 labeled images), our model even performs better than fully-supervised InvPT on some tasks (e.g., SS and SN). This can be attributed to the robust matching architecture equipped with a flexible adaptation mechanism that enables efficient knowledge transfer across tasks. In ablation study, we show that our model can be further improved by increasing the support size, reaching or sometimes surpassing supervised baselines in many tasks.  Figure 3: Performance of VTM on various shots. In general, VTM consistently improves performance as more supervision is given, and even surpasses fully supervised baselines on many tasks.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_0", "tab_8"]}, {"heading": "ABLATION STUDY", "text": "Component-Wise Analysis To analyze the effectiveness of our design, we conduct an ablation study with two variants. (1) Ours w/o Adaptation does not adapt the similarity for each task in the non-parametric matching. This variant shares all parameters across all training tasks T train as well as test tasks T test , and is evaluated without fine-tuning.\n(2) Ours w/o Matching predicts the query label tokens directly from the query image tokens, replacing the matching module with a parametric linear mapping at each hierarchy. This variant retains the task-specific parameters of ours, thus identical in terms of task adaptation; it utilizes the separate sets of task-specific biases in the image encoder to learn the training tasks T train , and fine-tune it on the support set of T test in the test time.\nThe results are in Table 2. Both variants show lower performance than ours in general, which demonstrates that incorporating non-parametric matching and task-specific adaptation are both beneficial for universal few-shot dense prediction problems. It seems that the task-specific adaptation is a crucial component for our problem, as Ours w/o Adaptation suffers from the high discrepancy between training and test tasks. For some low-level tasks whose labels are synthetically generated by applying a computational algorithm on an RGB-image (e.g., TE, K2), Ours w/o Matching achieves slightly better performance than Ours. Yet, for tasks requiring a high-level knowledge of object se-  Figure 4: Attention maps of selected level and head for each task given a query patch (red box in the 1st column). Our model flexibly adapts its similarity to attend to appropriate regions for each task (e.g., chairs for semantic segmentation, all planes orthogonal to camera view for surface normal).\n\u2191 mErr \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 Ours w/o\nmantics (e.g., SS) or 3D space (e.g., ED, OE, K3, RS), the introduction of non-parametric matching fairly improves the few-shot learning performance. Qualitative comparison is in Appendix C.4.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Impact of Support Size", "text": "As our method already performs well with ten labeled examples, a natural question arises: can it reach the performance of fully-supervised approaches if more examples are given? In Figure 3, we plot the performance of VTM by increasing the size of support set from 10 (< 0.004%) to 275 (0.1%). Our model reaches or surpasses the performance of the supervised methods on many tasks with additional data (yet much smaller than full), which implies potential benefits in specialized domains (e.g., medical) where the number of available labels ranges from dozens to hundreds.\nSensitivity to the Choice of Support Set As our model few-shot learns a new task from a very small subset of the full dataset, we analyze the sensitivity of the performance to the choice of support set. Table 3 in Appendix reports the 10-shot evaluation results with 4 different support sets disjointly sampled from the whole train split. We see that the standard deviation is marginal. This shows the robustness of our model to the support set, which would be important in practical scenarios.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Impact of Training Data", "text": "The amount and quality of training data is an important factor that can affect the performance of our universal few-shot learner. In Appendix C.1.4, we analyze the effect of the number of training tasks and show that few-shot performance consistently improves when more tasks are added to meta-training data. Also, in a more practical scenario, we may have an incomplete dataset where images are not associated with whole training task labels. We investigate this setting in Appendix C.1.5 and show that our model trained with an incomplete dataset still performs well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we proposed and addressed the problem of building a universal few-shot learner for arbitrary dense prediction tasks. Our proposed Visual Token Matching (VTM) addresses the challenge of the problem by extending a task-agnostic architecture of Matching Networks to patch-level embeddings of images and labels, and introducing a flexible adaptation mechanism in the image encoder. We evaluated our VTM in a challenging variant of the Taskonomy dataset consisting of ten dense prediction tasks, and showed that VTM can achieve competitive performance to fully supervised baselines on novel dense prediction tasks with an extremely small amount of labeled examples (<0.004% of full supervision), and is able to even closer the gap or outperform the supervised methods with fairly small amount of additional data (<0.1% of full supervision).  1 in Section B.2. Since our work proposes the new experiment settings for the few-shot learning of arbitrary dense prediction tasks, we also provide details of the dataset construction process in the main text (Section 5) and the appendix (Section A), which includes details of the data splits, specification of tasks, data preprocessing, and evaluation protocols. We plan to release the source codes and the dataset to ensure the reproducibility of this paper.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Ethics Statement", "text": "We have read the ICLR Code of Ethics and ensures that this work follows it.\nAll data and pre-trained models used in our experiments are publically available and has no ethical concerns.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX A DATASET DETAILS", "text": "This section describes the details about the dataset we used in experiments (Section 5).\nWe use \"tiny\" version of Taskonomy dataset provided by (Zamir et al., 2018), which consists of images and labels collected from 35 different buildings. We use the train and val split for training and early-stopping, respectively, and use the \"muleshoe\" building included in the test split for evaluation.\nTo demonstrate our universal few-shot learner, we use ten dense prediction tasks in Taskonomy dataset (Zamir et al., 2018), which are semantic segmentation (SS), surface normal (SN), Euclidean distance (ED), Z-buffer depth (ZD), texture edge (TE), occlusion edge (OE), 2D keypoints (K2), 3D keypoints (K3), reshading (RS), and principal curvature (PC). All labels are normalized into [0, 1] with task-specific pre-processing. For details on the pre-processing, we refer readers to Zamir et al. (2018). Based on the annotations provided by Taskonomy, we preprocess some tasks to increase the diversity of tasks. Specifically, we modify three single-channel tasks that can be easily augmented: Euclidean distance, texture edge, and occlusion edge.\n1. Texture edge (TE) labels are generated by applying Sobel edge detector (Kanopoulos et al., 1988) to RGB images, which consists of a Gaussian filter and image gradient computation.\nThe Gaussian filter has two hyper-parameters, namely kernel size and the standard deviation, where adjusting those hyper-parameters yield different thickness of detected edges. We use three different sets of hyper-parameters -(3, 1), (11, 2), (19, 3) -to produce 3-channel labels. We give an example of each channel of TE task in Figure 5. 2. Euclidean distance (ED) labels consists of pixel-wise depth map, where the depth is computed by the Euclidean distance from each image pixel to the camera's optical center. As this task is very similar to the Z-buffer depth prediction (ZD) whose label pixels are the distance from each image pixel to the camera plane, we augment the ED task by segmenting the depth range and re-normalizing within each segment. Specifically, we compute the 5-quantiles of the pixelwise depth labels in the whole dataset, then use each quantile as different channels after renoramlization into [0, 1]. Thus the objective of each channel of the augmented ED task is to predict Euclidean distance within a specific range, where the ranges are disjoint for different channels. We give an example of each channel of ED task in Figure 6. To visualize 5-channel labels, we average the first and the second channels as \"R\"-channel, the third and the fourth channels as \"G\"-channel, and use the fifth channel as \"B\"-channel.\n3. Occlusion edge (OE) labels are similar to texture edge, but they are constructed to depend on only the 3D geometry rather than color or lighting (Zamir et al., 2018). We observe that the channel augmentation by quantiles (that we apply to Euclidean distance task) can fairly diversify the labels. Therefore, we augment the OE labels into 5-channel labels, where we visualize them similar to the ED labels. We give an example of each channel of OE task in Figure 7.\nAlso, for semantic segmentation, we exclude three classes (\"bottle\", \"toilet\", \"book\"), as little images of the classes are included in the Taskonomy dataset. The 12 classes we used in experiments are: \"chair\", \"couch\", \"plant\", \"bed\", \"dining table\", \"tv\", \"mircrowave\", \"oven\", \"sink\", \"fridge\", \"clock\", and \"base\".\nFigure 5: Channel augmentation on texture edge prediction (TE) task. We apply three different sets of hyper-parameters (kernel size, standard deviation) in Sobel edge detector to generate a 3-channel edge task. Second to Fourth columns show the augmented channel with different kernel size and standard deviation, where the last column shows the 3-channel label visualized as RGB.\nRGB Image 5-Channel Visualization 1st 5-quantile 2nd 5-quantile 3rd 5-quantile 4th 5-quantile 5th 5-quantile Figure 6: Channel augmentation on Euclidean distance prediction (ED) task. We compute 5quantiles of the pixel-wise label distribution, and use each p-th 5-quantile as each channel after re-normalizing into [0, 1]. Second to Fifth columns show the augmented channel with different quantile, where the last column shows the 5-channel label visualized as RGB.", "publication_ref": ["b47", "b47", "b47", "b21", "b47"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "RGB Image", "text": "5-Channel Visualization 1st 5-quantile 2nd 5-quantile 3rd 5-quantile 4th 5-quantile 5th 5-quantile ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B IMPLEMENTATION DETAILS", "text": "This section describes the implementation details in our experiments (Section 5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 ARCHITECTURE DETAILS OF VTM", "text": "Encoders and Decoders We employ BEiT-B architecture (Bao et al., 2021) pretrained on Imagenet-22k dataset (Deng et al., 2009) with 224 \u00d7 224 resolution as our image encoder. For our label encoder and decoder, we follow the DPT-B architecture (Ranftl et al., 2021). Specifically, we use a randomly initialized ViT-B (Dosovitskiy et al., 2020) as label encoder g and extract features from 3, 6, 9, 12-th layers of the encoder to form multi-level label features (label tokens). Similarly, we extract multi-level image features (image tokens) from 3, 6, 9, 12-th layers of the image encoder (BEiT). As the DPT-B architecture decodes four-level features using RefineNet-based decoder (Lin et al., 2017), we pass the predicted query label features from matching module at each layer to the decoder. As the label values of tasks in Taskonomy are normalized to [0, 1], we use a sigmoid activation function at the head of the decoder to produce values in [0, 1]. To predict semantic segmentation task whose label values are discrete (either 0 or 1), we discretize the predicted label with threshold 0.1. and after the output projection matrix w O , where we share the layer normalization parameters for w Q h and w K h . Second, we add a residual connection with GELU non-linearity (Hendrycks & Gimpel, 2016) after gathering the outputs from multiple heads as follows:\nMHA(q, k, v) = o + GELU(ow O ),(10)\nwhere o = Concat(o 1 , o 2 , \u2022 \u2022 \u2022 , o H ).(11)\nFinally, we apply Dropout (Srivastava et al., 2014) with rate 0.1 in the attention scores.", "publication_ref": ["b7", "b8", "b26", "b14", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 ARCHITECTURE DETAILS OF BASELINES", "text": "Encoders and Decoders For the supervised learning baselines based on transformer encoder (DPT and InvPT), we use the same encoder backbone with ours (BEiT pretrained on ImageNet-22k). We use the decoder of DPT-B configuration in Ranftl et al. (2021) for DPT as ours, and use the original multi-task decoder implementation provided by Ye & Xu (2022) for InvPT. For fewshot learning baselines (HSNet, VAT, DGPNet), we use ResNet-101 (He et al., 2016) pretrained on ImageNet-1k (Deng et al., 2009) as their encoder backbones, which is their best configuration. For the other architectural details, we follow the original implementation of each method provided by Min et al. Modification on Few-shot Baselines As HSNet and VAT are designed for semantic segmentation, we slightly modify their architectures to train them on general dense prediction tasks. Specifically, both models involve a binary masking operation to filter out support image features using their labels (which are assumed to be binary), before computing 4D correlation tensor between support and query feature pixels. For continuous labels of general dense prediction tasks, the binary masking becomes pixel-wise multiplication with labels. However, as the correlation is computed by cosine similarity between feature pixels that is norm-invariant, all non-zero feature pixels with the same direction are treated in the same manner. This make them unable to discriminate different nonzero label values, e.g., correlation between query and support feature pixels would be the same regardless of the assigned support label values. Therefore, we move the masking operation to after computing the cosine-similarity, so that the models can recognize different non-zero label values through different norms of the masked features by (non-binary) labels.\nWe use the DGPNet without modification as it is based on a regression method (Gaussian Processes) which is inherently applicable to general dense prediction tasks with continuous labels.", "publication_ref": ["b45", "b13", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 TRAINING DETAILS", "text": "Training We train all models with 300,000 iterations using the Adam optimizer (Kingma & Ba, 2015), and use poly learning rate schedule (Liu et al., 2015) with base learning rates 10 \u22125 for pretrained parameters and 10 \u22124 for parameters trained from scratch. The models are early-stopped based on the validation metric. At each episodic training of iteration, we sample a batch of episodes with size 8. In each episode, we construct a 5-channel task from the training tasks T train by first splitting all channels of training tasks and randomly sample 5 channels among them. Then support and query sets are sampled for the selected channels, where we use support and query size of 4 for Ours and DGP, while using 1 for HSNet and VAT as they only supports 1-shot training. To train DPT, we construct a batch of each target task T test , whose channels are given at once, with batch size 64. To train InvPT, we construct a batch of all ten tasks, whose channels are all given at once, while using batch size 16 due to its large memory consumption.\nData Augmentation We apply random crop (from 256 \u00d7 256 resolution to 224 \u00d7 224) and random horizontal flip to images, where the random horizontal flip is applied except for surface normal labels as their values are sensitive to the horizontal direction (flipping images and labels together changes the semantics of the task). As we apply random crop during training, the resolution of test images (256 \u00d7 256) differs from the training images. To evaluate the models with consistent resolution, we perform five-crop (cropping the four corners and center of an image) to test query images so that the model also predicts five-cropped labels, then aggregate them by averaging the overlapping regions to produce final prediction for evaluation of resolution (256 \u00d7 256). For few-shot models, we apply center crop to support images at test-time.\nTask Augmentation For episodic training of few-shot models, we further apply two kinds of task augmentation. First, for each channel of C-channel labels sampled at each episode (C = 5 in our experiments), we apply random jittering and gaussian blur on each channel independently. Then we apply MixUp (Zhang et al., 2018) on the augmented channels and auxiliary channels which are additionally sampled from the training tasks T train , to create a linearly interpolated label of two channels. We apply the task augmentation consistently in each episode to preserve the task identity.", "publication_ref": ["b23", "b27", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "C ADDITIONAL RESULTS", "text": "This section provides additional results on our experiments (Section 5).\nC.1 ADDITIONAL RESULTS ON ABLATION STUDY C.1.1 SENSITIVITY TO THE CHOICE OF SUPPORT SET As discussed in Section 5, we evaluate the 10-shot performance of our VTM with four different support sets that are disjointly sampled from the training data D train . We report the results in Table 3, which shows that our model is robust to the choice of support set. We use the first support set (#1) in Table 3 for comparison with other baselines or ablated variants in Section 5, due to the huge computational cost for evaluating few-shot baselines HSNet and VAT.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4", "tab_4"]}, {"heading": "C.1.2 ABLATION STUDY ON TRAINING PROCEDURE", "text": "To understand the source of the generalization performance of our method more clearly, we conduct an ablation study on training procedure. We compare four models based on DPT architecture with different training procedures as follows.\n\u2022 M1: Randomly initialized DPT, 10-shot trained.\n\u2022 M2: DPT with BEiT pre-trained encoder, 10-shot fine-tuned.\n\u2022 M3 (Ours w/o Matching): DPT with BEiT pre-trained encoder, multi-task trained with taskspecific bias tuning, and then 10-shot fine-tuned. \u2022 M4 (Ours): DPT with BEiT pre-trained encoder, meta-trained with task-specific bias tuning, and then 10-shot fine-tuned.\nWe summarize the quantitative result in Table 4 and qualitative comparison in Figure 8. First, as expected, we observe that DPT with naive 10-shot training (M1) fails to generalize to the test examples in most of the tasks, except for two 2D texture-related tasks (TE, K2). We conjecture that TE and K2 are \"easy\" cases in terms of few-shot learning, as they are defined as low-level computational algorithms on RGB images, while other high-level tasks require knowledge about semantics (SS) or 3D space (SN, ED, ZD, OE, K3, RS, PC). Second, we note that BEiT pretraining (M2) largely improves the few-shot generalization performance, allowing the model to produce coarse predictions of the dense labels. However, it still cannot capture object-level fine-grained details in many tasks. Third, we observe that multi-task training and few-shot adaptation, combined with an efficient parametersharing strategy of bias tuning (M3, M4), further improves the performance with a clear gap with M2 where the predictions are also qualitatively finer than M2's. Finally, as discussed in Section 5.2.1, M4 still further improves over M3 with a clear gap. This shows that in a few-shot learning setting,  our matching framework and episodic training are more effective than simple multi-task pretraining employed in M3. In summary, we may conclude that the fast generalization of Ours is benefitted from episodic training of various tasks followed by parameter-efficient few-shot adaptation as well as powerful pre-training of the encoder (BEiT).", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_5"]}, {"heading": "C.1.3 FINE-TUNING WITH FULL SUPERVISION", "text": "To further explore how our method scales well when a large labeled dataset is given, we also finetuned our VTM with full supervision of test tasks. For the fine-tuning, we used the same training dataset as the fully-supervised DPT and employed the episodic fine-tuning objective (Section 3.3). For evaluation, since providing the entire training data as the support set for the matching module is infeasible, we provide a random subset of the training data as the support set to the model. We summarize the result in Figure 9, which extends Figure 3 in Section 5. In most tasks, our model consistently improves when more supervision is given. With full supervision at test tasks, our model performs slightly worse than the DPT baseline in seven tasks and performs better or similarly in the other three tasks. We conjecture that the performance degradation comes from two aspects: (1) the absence of direct input-output connection, i.e., the matching module serves as a bottleneck, and (2) negative transfer from meta-training tasks to test tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1.4 EFFECT OF NUMBER OF TRAINING TASKS", "text": "The amount of meta-training tasks is an important factor that can affect the performance of the universal few-shot learner. To verify this, we fixed two test tasks (SS, SN) and trained our VTM on five different subsets of the original eight training tasks (three different subsets with two tasks and two different subsets with five tasks). We summarize the results in the Table 5. As expected, the performance consistently improves as we increase the number of training tasks. We also note that the Figure 9: Performance of VTM on various shots. In general, VTM consistently improves performance as more supervision is given, and even surpasses fully supervised baselines on many tasks. few-shot performance becomes sensitive to the choice of training tasks when their number is small (two), presumably as the model becomes reliant on training tasks more correlated to test tasks, while the variance decreases substantially when more training tasks are added. In addition, the experiment with incomplete training data (Appendix C.1.5) shows the potential ability of our methods in more realistic settings where the training dataset is formed by a combination of different task-specific datasets. From these results, we expect that our model can further enhance its universality on fewshot learning by utilizing a combined training dataset of much more diverse tasks, which we leave as future work.  6. We can see that the performance degradation is marginal when we give incomplete data, which implies that our method can be promising in handling realistic scenarios where the training data is a collection of heterogeneous datasets with different label annotations.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_7"]}, {"heading": "C.2 FURTHER ANALYSIS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2.1 PARAMETER-EFFICIENCY ANALYSIS", "text": "We report the number of task-specific and shared parameters of our VTM and two supervised baselines, DPT and InvPT, to compare how our task adaptation is parameter-efficient. As DPT is a single-task learning model, no parameters are shared across tasks and the whole network should be trained independently for every new task. InvPT, which is a multi-task learning model, shares a large portion of its parameters across tasks (e.g., encoder backbone), still consumes many parameters for each task in the decoder.\nDue to the extensive amount of parameter-sharing, our method is also promising in continual learning setting. As all task-specific knowledge is included in the bias parameters of the image encoder, the knowledge acquired from past tasks can be recalled without forgetting by keeping the corresponding bias parameters and switching to them whenever a past model is needed. We especially note that the size of bias parameters is fairly small (288 KB, which amounts to keeping about 3 labeled images of 256x256 resolution for each task). This allows our model to retain past knowledge very efficiently by keeping the tuned bias parameters plus a few-shot support set, whose external memory requirement is far less compared to memory-based approaches in continual learning that keep hundreds of images (Bang et al., 2021;Wang et al., 2022). While the continual learning setting is not our main focus, applying our method to a continual learning setting would be an interesting future direction. ", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "C.2.2 COMPUTATION COST ANALYSIS", "text": "To analyze how our method is computationally efficient compared to supervised DPT, we measured the MACs (multiply-accumulate operations) of our model and DPT using an open-source python library thop 3 . We report the results in Table 8. Having encoded the support set (e.g., 10-shot), we can see that the computational cost of our model's inference on a single query image is about 30% larger than the cost of DPT's, due to the Matching part. ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "C.2.3 ROLE OF ATTENTION HEADS", "text": "To analyze the role of attention heads, in Figure 10, we visualized the attention maps for each head over support images for a given query patch, feature level (3rd level in this example), and task (RS in this example). The figure shows that each head attends to different regions of the support images. Moreover, we can find some patterns in heads; for example, the first head tends to attend to flat areas of the scene, such as the floor or ceiling (low-frequency features), while the third head tends to attend to objects, such as couch or plant (high-frequency features). To further verify the benefit of multihead attention in the matching module, we also trained our VTM with single head in the matching modules. The result is summarized in the table below and Table 9. We can see the performance drop in both SS and SN tasks, which supports that exploiting multiple heads benefits our matching framework.   ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_11"]}, {"heading": "C.3 ADDITIONAL QUALITATIVE COMPARISON WITH BASELINES", "text": "We provide additional results on the qualitative evaluation of our model and the baselines. Figure 11-14 show visualizations on different query image and support set, where we vary the class of semantic segmentation task included in each support. The result shows consistent trends of that we discussed in Section 5. Ours is competitive to the fully supervised baselines (DPT and InvPT), while the other few-shot baselines (HSNet, VAT, DGPNet) fail to learn different dense prediction tasks.\nIn Figure 12, even the GT label for semantic segmentation (\"couch\" class) is noisy as it is a pseudolabel generated by a pre-trained segmentation model (Zamir et al., 2018), our model successfully segments two couches present in the figure. This can be attributed to the task-agnostic architecture of VTM based on non-parametric matching.  ", "publication_ref": ["b47"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "C.4 ADDITIONAL QUALITATIVE COMPARISON WITH OUR VARIANTS", "text": "We also provide additional results on the qualitative evaluation of our model and our ablated variants, Ours w/o Matching and Ours w/o Adaptation. Figure 15-18 show visualizations on different query image and support set. The results show a consistent trend with the quantitative results in Table 1. Interestingly, our method without adaptation already exhibits some degree of adaptation to the unseen tasks even without fine-tuning and task-specific components, showing that the nonparametric architecture of our model and the parameter sharing derived from is appropriate to learn generalizable knowledge to understand the novel tasks. On the other hand, adding a task-specific component and adaptation mechanism to the model allows more dramatic improvement in understanding novel tasks from few-shot examples, showing the importance of the adaptation mechanism in our task. Finally, we observe that equipping the matching mechanism with the adaptation module provides much sharper and fast adaptation to the unseen tasks, which verifies our claims.  ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": ["tab_0"]}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2016", "authors": "Jimmy Lei Ba; Jamie Ryan Kiros; Geoffrey E Hinton"}, {"ref_id": "b1", "title": "Rainbow memory: Continual learning with a memory of diverse samples", "journal": "", "year": "", "authors": "Jihwan Bang; Heesu Kim; Youngjoon Yoo; Jung-Woo Ha; Jonghyun Choi"}, {"ref_id": "b2", "title": "Beit: Bert pre-training of image transformers", "journal": "", "year": "", "authors": "Hangbo Bao; Li Dong; Songhao Piao; Furu Wei"}, {"ref_id": "b3", "title": "Language models are few-shot learners", "journal": "NeurIPS", "year": "2020", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell"}, {"ref_id": "b4", "title": "Tinytl: Reduce memory, not parameters for efficient on-device learning", "journal": "NeurIPS", "year": "2020", "authors": "Han Cai; Chuang Gan; Ligeng Zhu; Song Han"}, {"ref_id": "b5", "title": "Flattenet: a simple and versatile framework for dense pixelwise prediction", "journal": "IEEE Access", "year": "2019", "authors": "Xin Cai; Yi-Fei Pu"}, {"ref_id": "b6", "title": "Vision transformer adapter for dense predictions", "journal": "", "year": "2022", "authors": "Zhe Chen; Yuchen Duan; Wenhai Wang; Junjun He; Tong Lu; Jifeng Dai; Yu Qiao"}, {"ref_id": "b7", "title": "Imagenet: A large-scale hierarchical image database", "journal": "", "year": "2009", "authors": "Jia Deng; Wei Dong; Richard Socher; Li-Jia Li; Kai Li; Li Fei-Fei"}, {"ref_id": "b8", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2020", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly"}, {"ref_id": "b9", "title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "journal": "", "year": "2015", "authors": "David Eigen; Rob Fergus"}, {"ref_id": "b10", "title": "Few-shot object detection with attention-rpn and multi-relation detector", "journal": "", "year": "2020", "authors": "Qi Fan; Wei Zhuo; Chi-Keung Tang; Yu-Wing Tai"}, {"ref_id": "b11", "title": "Self-support few-shot semantic segmentation", "journal": "", "year": "2022", "authors": "Qi Fan; Wenjie Pei; Yu-Wing Tai; Chi-Keung Tang"}, {"ref_id": "b12", "title": "Fgn: Fully guided network for few-shot instance segmentation", "journal": "", "year": "2020", "authors": "Zhibo Fan; Jin-Gang Yu; Zhihao Liang; Jiarong Ou; Changxin Gao; Gui-Song Xia; Yuanqing Li"}, {"ref_id": "b13", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b14", "title": "Gaussian error linear units (gelus)", "journal": "", "year": "2016", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b15", "title": "Cost aggregation with 4d convolutional swin transformer for few-shot segmentation", "journal": "", "year": "2022", "authors": "Sunghwan Hong; Seokju Cho; Jisu Nam; Stephen Lin; Seungryong Kim"}, {"ref_id": "b16", "title": "Parameter-efficient transfer learning for nlp", "journal": "", "year": "2019", "authors": "Neil Houlsby; Andrei Giurgiu; Stanislaw Jastrzebski; Bruna Morrone; Quentin De Laroussilhe; Andrea Gesmundo; Mona Attariyan; Sylvain Gelly"}, {"ref_id": "b17", "title": "Msanet: Multi-similarity and attention guidance for boosting few-shot segmentation", "journal": "", "year": "2022", "authors": "Ehtesham Iqbal; Sirojbek Safarov; Seongdeok Bang"}, {"ref_id": "b18", "title": "Visual prompt tuning", "journal": "", "year": "2022", "authors": "Menglin Jia; Luming Tang; Bor-Chun Chen; Claire Cardie; Serge Belongie; Bharath Hariharan; Ser-Nam Lim"}, {"ref_id": "b19", "title": "Dense gaussian processes for few-shot segmentation", "journal": "", "year": "2021", "authors": "Joakim Johnander; Johan Edstedt; Michael Felsberg; Fahad Shahbaz Khan; Martin Danelljan"}, {"ref_id": "b20", "title": "Few-shot object detection via feature reweighting", "journal": "", "year": "2019", "authors": "Bingyi Kang; Zhuang Liu; Xin Wang; Fisher Yu; Jiashi Feng; Trevor Darrell"}, {"ref_id": "b21", "title": "Design of an image edge detection filter using the sobel operator", "journal": "JSSC", "year": "1988", "authors": "Nick Kanopoulos; Nagesh Vasanthavada; Robert L Baker"}, {"ref_id": "b22", "title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": ""}, {"ref_id": "b23", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b24", "title": "The power of scale for parameter-efficient prompt tuning", "journal": "", "year": "", "authors": "Brian Lester; Rami Al-Rfou; Noah Constant"}, {"ref_id": "b25", "title": "Benchmarking detection transfer learning with vision transformers", "journal": "", "year": "2021", "authors": "Yanghao Li; Saining Xie; Xinlei Chen; Piotr Dollar; Kaiming He; Ross Girshick"}, {"ref_id": "b26", "title": "Refinenet: Multi-path refinement networks for high-resolution semantic segmentation", "journal": "", "year": "2017", "authors": "Guosheng Lin; Anton Milan; Chunhua Shen; Ian Reid"}, {"ref_id": "b27", "title": "Parsenet: Looking wider to see better", "journal": "", "year": "2015", "authors": "Wei Liu; Andrew Rabinovich; Alexander C Berg"}, {"ref_id": "b28", "title": "Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation", "journal": "", "year": "2020", "authors": "Yongfei Liu; Xiangyi Zhang"}, {"ref_id": "b29", "title": "One-shot instance segmentation", "journal": "", "year": "2018", "authors": "Claudio Michaelis; Ivan Ustyuzhaninov; Matthias Bethge; Alexander S Ecker"}, {"ref_id": "b30", "title": "Hypercorrelation squeeze for few-shot segmentation", "journal": "", "year": "", "authors": "Juhong Min; Dahyun Kang; Minsu Cho"}, {"ref_id": "b31", "title": "Abstraction and analogy-making in artificial intelligence", "journal": "Annals of the New York Academy of Sciences", "year": "2021", "authors": "Melanie Mitchell"}, {"ref_id": "b32", "title": "Unsupervised learning of dense visual representations", "journal": "NeurIPS", "year": "2020", "authors": "O O Pedro; Amjad Pinheiro; Ryan Almahairi; Florian Benmalek; Aaron C Golemo;  Courville"}, {"ref_id": "b33", "title": "Semi-supervised semantic segmentation with cross-consistency training", "journal": "", "year": "2020", "authors": "Yassine Ouali; C\u00e9line Hudelot; Myriam Tami"}, {"ref_id": "b34", "title": "Vision transformers for dense prediction", "journal": "", "year": "", "authors": "Ren\u00e9 Ranftl; Alexey Bochkovskiy; Vladlen Koltun"}, {"ref_id": "b35", "title": "Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation", "journal": "", "year": "2017", "authors": "Amirreza Shaban; Shray Bansal; Zhen Liu"}, {"ref_id": "b36", "title": "Prototypical networks for few-shot learning", "journal": "NeurIPS", "year": "2017", "authors": "Jake Snell; Kevin Swersky; Richard Zemel"}, {"ref_id": "b37", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "JMLR", "year": "2014", "authors": "Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b38", "title": "Which tasks should be learned together in multi-task learning", "journal": "", "year": "2020", "authors": "Trevor Standley; Amir Zamir; Dawn Chen; Leonidas Guibas; Jitendra Malik; Silvio Savarese"}, {"ref_id": "b39", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b40", "title": "Matching networks for one shot learning", "journal": "NeurIPS", "year": "2016", "authors": "Oriol Vinyals; Charles Blundell; Timothy Lillicrap; Daan Wierstra"}, {"ref_id": "b41", "title": "Panet: Few-shot image semantic segmentation with prototype alignment", "journal": "", "year": "2019", "authors": "Kaixin Wang; Jun Hao Liew; Yingtian Zou; Daquan Zhou; Jiashi Feng"}, {"ref_id": "b42", "title": "Frustratingly simple few-shot object detection", "journal": "", "year": "2020", "authors": "Xin Wang; Thomas Huang; Joseph Gonzalez; Trevor Darrell; Fisher Yu"}, {"ref_id": "b43", "title": "Dense contrastive learning for self-supervised visual pre-training", "journal": "", "year": "", "authors": "Xinlong Wang; Rufeng Zhang; Chunhua Shen; Tao Kong; Lei Li"}, {"ref_id": "b44", "title": "Continual learning with lifelong vision transformer", "journal": "", "year": "", "authors": "Zhen Wang; Liu Liu; Yiqun Duan; Yajing Kong; Dacheng Tao"}, {"ref_id": "b45", "title": "Inverted pyramid multi-task transformer for dense scene understanding", "journal": "", "year": "2022", "authors": "Hanrong Ye; Dan Xu"}, {"ref_id": "b46", "title": "Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models", "journal": "Short Papers", "year": "", "authors": "Elad Ben Zaken; Yoav Goldberg; Shauli Ravfogel"}, {"ref_id": "b47", "title": "Taskonomy: Disentangling task transfer learning", "journal": "", "year": "2018", "authors": "R Amir; Alexander Zamir; William B Sax; Leonidas J Shen; Jitendra Guibas; Silvio Malik;  Savarese"}, {"ref_id": "b48", "title": "mixup: Beyond empirical risk minimization", "journal": "", "year": "2018", "authors": "Hongyi Zhang; Moustapha Cisse; David Yann N Dauphin;  Lopez-Paz"}, {"ref_id": "b49", "title": "Kernel size = 11, Std. = 2 Kernel size = 19", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Overall architecture of VTM. Our model is a hierarchical encoder-decoder with four main components: the image encoder f T , label encoder g, label decoder h, and the matching module. See the text for more detailed descriptions.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "6.4414 0.0534 0.0268 0.0188 0.0689 0.0358 0.0357 0.0860 0.0347 InvPT 0.3900 12.9249 0.0589 0.0298 0.0517 0.0788 0.0456 0.0384 0.0949 0.0370 10-Shot (< 0.004%) HSNet 0.1069 24.9120 0.2375 0.0748 0.1746 0.1643 0.1056 0.0651 0.2627 0.0610 VAT 0.0353 25.8134 0.2718 0.0779 0.1719 0.1655 0.1450 0.0678 0.2709 0.0796 DGPNet 0.0261 29.1668 0.4579 0.2846 0.1881 0.2130 0.1104 0.1308 0.3680 0.3574 Ours 0.4097 11.4391 0.0741 0.0316 0.0791 0.0912 0.0639 0.0519 0.1089 0.0420Implementation For models based on ViT architecture (Ours, DPT, InvPT), we use BEiT-B(Bao  et al., 2021) backbone as image encoder, which is pre-trained on ImageNet-22k(Deng et al., 2009) ", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Qualitative comparison of few-shot learning methods in 10-shot evaluation for ten dense prediction tasks in Taskonomy. While other approaches fail, our model successfully few-shot learns all novel tasks with diverse semantics and different label representations.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Matching 0.2681 13.0704 0.1111 0.0404 0.0778 0.1061 0.0613 0.0537 0.1559 0.0445 Ours w/o Adaptation 0.0002 23.4212 0.1515 0.0641 0.1513 0.1152 0.1110 0.0625 0", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Channel augmentation on occlusion edge prediction (OE) task. We compute 5-quantiles of the pixel-wise label distribution, and use each p-th 5-quantile as each channel after re-normalizing into [0, 1]. Second to Fifth columns show the augmented channel with different quantile, where the last column shows the 5-channel label visualized as RGB.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "(2021) (HSNet), Hong et al. (2022) (VAT), and Johnander et al. (2021) (DGPNet).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Qualitative comparison of Ours and its ablated variants in training procedure. All models use 10 labeled examples for each target task, where M3 and M4 observe additional labeled examples of training tasks (different from the target task) in each fold.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 10 :10Figure 10: Visualization of multi-head attention maps of VTM. Here we visualize the matching module at 3rd level for reshading (RS) task.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 14 :14Figure 14: Additional results of qualitative comparison between Ours and the baselines.", "figure_data": ""}, {"figure_label": "18", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 18 :18Figure 18: Additional results of qualitative comparison between Ours and its ablated variants.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Quantitative comparison on Taskonomy dataset. Few-shot baselines are 10-shot evaluated on each fold after being trained on the tasks from the other folds, where fully-supervised baselines are trained and evaluated on tasks from each fold (DPT) or all folds (InvPT).", "figure_data": "TasksSupervision ModelSSFold 1 SNEDFold 2ZDTEFold 3OEK2Fold 4K3RSFold 5PCmIoU"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Ablation study on matching and task-specific adaptation.", "figure_data": "TasksModelFold 1Fold 2Fold 3Fold 4Fold 5SSSNEDZDTEOEK2K3RSPCmIoU"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Acknowledgements This work was supported in part by the Institute of Information & communications Technology Planning & Evaluation (IITP) (No. 2022-0-00926, 2022-0-00959, and 2019-0-00075) and the National Research Foundation of Korea (NRF) (No. 2021R1C1C1012540 and 2021R1A4A3032834) funded by the Korea government (MSIT), and Microsoft Research Asia Collaborative Research Project. Reproducibility Statement To help readers reproduce our experiments, we provided detailed descriptions of our architectures in Section B.1, and implementation details of the baseline methods in Table", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation study on the choice of support set. We disjointly sample four different support sets and report the 10-shot performance on each set, with the mean and standard deviation.", "figure_data": "TasksSupport setFold 1Fold 2Fold 3Fold 4Fold 5SSSNEDZDTEOEK2K3RSPC"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "10-shot learning performance of ablated variants of DPT and Ours. RMSE\u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193", "figure_data": "TasksModelFold 1Fold 2Fold 3Fold 4Fold 5SSSNEDZDTEOEK2K3RSPCmIoU \u2191 mErr \u2193 M1 0.0644 21.0976 0.1959 0.0711 0.0995 0.1842 0.0670 0.0600 0.2335 0.0431M2 0.0582 15.8135 0.1615 0.0530 0.1136 0.1480 0.0948 0.0606 0.1858 0.0431M3 0.2681 13.0704 0.1111 0.0404 0.0778 0.1061 0.0613 0.0537 0.1559 0.0445M4 0.4097 11.4391 0.0741 0.0316 0.0791 0.0912 0.0639 0.0519 0.1089 0.0420ImageSSSNEDZDTEOEK2K3RSPCGround-TruthM1(Naive DPT)M2(DPT + BEiT)M3(Ours w/o Matching)M4(Ours)"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "10-shot learning performance of Ours with various number of training tasks. EPISODIC TRAINING WITH INCOMPLETE DATASETIt would make our method more practical if the model could learn from an incomplete dataset where images are not associated with whole training task labels. To see how our framework extends to such incomplete settings, we conducted an additional experiment. We simulate the extreme case of incomplete data by partitioning the training images, such that each image is associated with only a single task out of 8 training tasks. Specifically, we partitioned the buildings in Taskonomy into eight groups -each corresponds to a different training task. As this reduces the effective size of training data by the number of training tasks (1/8 in our case), we also train a baseline where we use complete data but use only 1/8 of the training images (for each building, we discard 7/8 of the images). The results are summarized in Table", "figure_data": "TasksNumber of Training TasksFold 1SSSNmIoU \u2191mErr \u219320.2878 \u00b1 0.0565 17.3947 \u00b1 4.874250.3919 \u00b1 0.0132 12.6769 \u00b1 0.123580.409711.4391C.1.5"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "10-shot learning performance of Ours trained with incomplete and complete multi-task dataset.", "figure_data": "TasksTraining DataFold 1SSSNmIoU \u2191mErr \u2193Incomplete (one task per building)0.355913.6207Complete (1/8 training data)0.398012.1633Complete (whole training data)0.409711.4391"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Number of task-specific and shared parameters for a single-channel task (in million).", "figure_data": "ModelTask-Specific SharedDPT (supervised learning)110.550InvPT (multi-task learning)24.57106.75Ours (few-shot learning)0.0703202.95"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "MACs of Ours and DPT on a single-query inference for a single-channel task.", "figure_data": "ModelMACs (G)DPT30.15Ours after encoding support (10-shot)38.79"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "10-shot learning performance of Ours with different number of attention heads in Matching module.", "figure_data": "TasksNumber of Attention HeadsFold 1SSSNmIoU \u2191mErr \u219310.370212.593640.409711.4391"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Figure 11: Additional results of qualitative comparison between Ours and the baselines.Figure 12: Additional results of qualitative comparison between Ours and the baselines.Figure 13: Additional results of qualitative comparison between Ours and the baselines.", "figure_data": "ImageSSSNEDZDTEOEK2K3RSPCGroundTruthDPT(sup.)InvPT(sup.)ImageSSSNEDZDTEOEK2K3RSPCGround OursTruthDPT HSNetImageSSSNEDZDTEOEK2K3RSPC(sup.) GroundInvPT VAT Truth(sup.) DPTDGPNet (sup.)OursInvPT(sup.)HSNetOursVATHSNetImageSSSNEDZDTEOEK2K3RSPCDGPNet GroundTruth VATDPT(sup.) DGPNetInvPT(sup.)OursHSNetVATDGPNet"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Additional results of qualitative comparison between Ours and its ablated variants. Additional results of qualitative comparison between Ours and its ablated variants. Additional results of qualitative comparison between Ours and its ablated variants.", "figure_data": "SS Figure 15: SS Ground Truth Image Ours Ours w/o Matching Ours w/o Adaptation Ground Truth Image Ours Ours w/o Matching Ours w/o Adaptation Ground Truth Image Ours Ours w/o Matching Ours w/o Adaptation Figure 17: SS Ground Truth Image Ours Ours w/o Figure 16: SS MatchingSN SN SN SNED ED ED EDZD ZD ZD ZDTE TE TE TEOE OE OE OEK2 K2 K2 K2K3 K3 K3 K3RS RS RS RSPC PC PC PCOurs w/oAdaptation"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Y q = F(X q ; S T ), S T = {(X i , Y i )} i\u2264N .(2)", "formula_coordinates": [2.0, 216.97, 516.97, 287.03, 11.72]}, {"formula_id": "formula_1", "formula_text": "{(X i , Y i )} i\u2264N = {(x i k , y i k )} k\u2264M,i\u2264N", "formula_coordinates": [3.0, 276.7, 463.51, 157.42, 12.55]}, {"formula_id": "formula_2", "formula_text": "g(y q j ) = i\u2264N k\u2264M \u03c3 f T (x q j ), f T (x i k ) g(y i k ),(3)", "formula_coordinates": [3.0, 213.5, 496.57, 290.5, 22.89]}, {"formula_id": "formula_3", "formula_text": "R d \u00d7 R d \u2192 [0, 1]", "formula_coordinates": [3.0, 228.58, 538.15, 72.12, 10.87]}, {"formula_id": "formula_4", "formula_text": "MHA(q, k, v) = Concat(o 1 , ..., o H )w O ,(4)", "formula_coordinates": [5.0, 203.1, 112.33, 300.9, 11.72]}, {"formula_id": "formula_5", "formula_text": "o h = Softmax qw Q h (kw K h ) \u221a d H vw V h ,(5)", "formula_coordinates": [5.0, 250.36, 129.48, 253.64, 26.73]}, {"formula_id": "formula_6", "formula_text": "min f T ,g,h,\u03c3 E S T ,Q T \u223cDtrain \uf8ee \uf8f0 1 |Q T | (X q ,Y q )\u2208Q T L (Y q , F(X q ; S T ))) \uf8f9 \uf8fb ,(6)", "formula_coordinates": [5.0, 173.45, 484.2, 330.55, 34.76]}, {"formula_id": "formula_7", "formula_text": "min \u03b8 T ES ,Q\u223cS T test \uf8ee \uf8f0 1 |Q| (X q ,Y q )\u2208Q L Y q , F(X q ;S) \uf8f9 \uf8fb ,(7)", "formula_coordinates": [5.0, 195.5, 661.53, 308.5, 35.37]}, {"formula_id": "formula_8", "formula_text": "\u2191 mErr \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193", "formula_coordinates": [7.0, 209.19, 155.05, 291.02, 7.97]}, {"formula_id": "formula_9", "formula_text": "\u2191 mErr \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 RMSE \u2193 Ours w/o", "formula_coordinates": [9.0, 117.01, 130.41, 380.41, 20.15]}, {"formula_id": "formula_10", "formula_text": "MHA(q, k, v) = o + GELU(ow O ),(10)", "formula_coordinates": [15.0, 219.96, 474.88, 284.04, 11.03]}, {"formula_id": "formula_11", "formula_text": "where o = Concat(o 1 , o 2 , \u2022 \u2022 \u2022 , o H ).(11)", "formula_coordinates": [15.0, 245.55, 490.87, 258.45, 9.68]}], "doi": ""}