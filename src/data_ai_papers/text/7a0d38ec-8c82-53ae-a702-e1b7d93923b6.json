{"title": "Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages", "authors": "Ayyoob Imani; Peiqin Lin; Amir Hossein Kargaran; Silvia Severini; Masoud Jalili Sabet; Nora Kassner; Chunlan Ma; Helmut Schmid; Andr\u00e9 F T Martins; Fran\u00e7ois Yvon; Hinrich Sch\u00fctze; Pierre Platen; Pierre Fran\u00e7ois Cornette; R\u00e9mi Lavall\u00e9e; Samyam Lacroix; Sanchit Rajbhandari; Shaden Gandhi; St\u00e9phane Smith; Suraj Requena; Tim Patil; Ahmed Dettmers; Amanpreet Baruwa; Anasta- Sia Singh; Anne-Laure Cheveleva; Arjun Ligozat; Aur\u00e9lie Subramo- Nian; Charles N\u00e9v\u00e9ol; Dan Lovering; Deepak Garrette; Ehud Tunuguntla; Ekaterina Reiter; Ekaterina Takta- Sheva; Eli Voloshina; Genta Bogdanov; Hailey In- Dra Winata; Jan-Christoph Schoelkopf; Jekaterina Kalo; Jessica Zosa Novikova; Jordan Forde; Jungo Clive; Ken Kasai; Liam Kawamura; Marine Hazan; Miruna Carpuat; Najoung Clinciu; New- Ton Kim; Oleg Cheng; Omer Serikov; Oskar Antverg; Rui Van Der Wal; Ruochen Zhang; Sebas- Tian Zhang; Shachar Gehrmann; Shani Mirkin; Tatiana Pais; Thomas Shavrina; Tian Scialom; Tomasz Yun; Verena Lim- Isiewicz; Vitaly Rieser; Vladislav Protasov; Yada Mikhailov; Yonatan Pruksachatkun; Zachary Belinkov; Zden\u011bk Bamberger; Alice Kasner; Amanda Rueda; Amir Pestana; Ammar Feizpour; Amy Khan; Ana Faranak; Anthony Santos; Antigona Hevia; Arash Unl- Dreaj; Arezoo Aghagol; Aycha Abdollahi; Azadeh Tam- Mour; Bahareh Hajihosseini; Ben- Jamin Behroozi; Bharat Ajibade; Carlos Saxena; Ferran- Dis Mu\u00f1oz; Danish Contractor; David Lansky; Davis David; Douwe Kiela; Duong A Nguyen; Edward Tan; Emi Baylor; Ezinwanne Ozoani; Fatima Mirza; Frankline Ononiwu; Habib Rezanejad; Hessie Jones; Indrani Bhattacharya; Irene Solaiman; Irina Sedenko; Isar Nejadgholi; Jesse Passmore; Josh Seltzer; Julio Bo- Nis Sanz; Livia Dutra; Mairon Samagaio; Maraim Elbadri; Margot Mieskes; Marissa Gerchick; Martha Akinlolu; Michael Mckenna; Mike Qiu; Muhammed Ghauri; Mykola Burynok; Nafis Abrar; Nazneen Ra- Jani; Nour Elkott; Nour Fahmy; Olanrewaju Samuel; Ran An; Rasmus Kromann; Ryan Hao; Samira Al- Izadeh; Sarmad Shubber; Silas Wang; Sourav Roy; Sylvain Viguier; Thanh Le; Tobi Oyebade; Trieu Le; Yoyo Yang; Zach Nguyen; Ramesh Kashyap; Alfredo Palasciano; Alison Callahan; Anima Shukla; Antonio Miranda-Escalada; Ayush Singh; Benjamin Beilharz; Bo Wang; Caio Brito; Chenxi Zhou; Chirag Jain; Chuxin Xu; Cl\u00e9mentine Fourrier; Daniel Le\u00f3n Peri\u00f1\u00e1n; Daniel Molano; Dian Yu; Enrique Manjava- Cas; Fabio Barth; Florian Fuhrimann; Gabriel Altay; Giyaseddin Bayrak; Gully Burns; Helena U Vrabec; Imane Bello; Ishani Dash; Jihyun Kang; John Giorgi; Jonas Golde; Jose David Posada; Rangasai Karthik; Lokesh Sivaraman; Lu Bulchandani; Luisa Liu; Madeleine Shin- Zato; Maiko Hahn De Bykhovetz; Marc Takeuchi; Maria A P\u00e0mies; Marianna Castillo; Mario Nezhurina; Matthias S\u00e4nger; Michael Samwald; Michael Cullan; Michiel Weinberg; Mina De Wolf; Minna Mihalj- Cic; Moritz Liu; Myungsun Freidank; Natasha Kang; Nathan Seelam; Nicholas Dahlberg; Nikolaus Michio Broad; Pascale Muellner; Patrick Fung; Ramya Haller; Renata Chandrasekhar; Robert Eisenberg; Rodrigo Martin; Rosaline Canalli; Ruisi Su; Samuel Su; Samuele Cahyaw\u0133aya;  Garda; S Shlok; Shubhanshu Deshmukh; Sid Mishra; Si- Mon Kiblawi; Sinee Ott; Srishti Sang-Aroonsiri; Stefan Kumar; Sushil Schweter; Tanmay Bharati; Th\u00e9o Laud; Tomoya Gigant; Wojciech Kainuma; Yanis Kusa;  Labrak; Shailesh Yash; Yash Bajaj; Yifan Venkatraman; Yingxin Xu; Yu Xu; Zhe Xu; Zhongli Tan; Zifan Xie; Mathilde Ye", "pub_date": "", "abstract": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \"help\" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.", "sections": [{"heading": "Introduction", "text": "The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., deepening their understanding of high-resource languages by scaling up parameters and training data. While this approach has revolutionized NLP, the achievements are largely limited to high-resource languages. Examples of \"vertical\" LLMs are GPT3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and Bloom (BigScience et al., 2022). In this paper, we create Glot500-m, a model that instead focuses on scaling multilingual LLMs horizontally, i.e., scaling to a large number of languages the great * Equal contribution. majority of which is low-resource. As LLMs are essential for progress in NLP, lack of LLMs supporting low-resource languages is a serious impediment to bringing NLP to all of the world's languages and cultures. Our goal is to address this need with the creation of Glot500-m. 1 Existing multilingual LLMs support only about 100 (Conneau et al., 2020) out of the 7000 languages of the world. These supported languages are the ones for which large amounts of training data are available through projects such as Oscar (Su\u00e1rez et al., 2019) and the Wikipedia dumps. 2 Following Siddhant et al. (2022), we refer to the 100 languages covered by XLM-R (Conneau et al., 2020) as head languages and to the remaining languages as tail languages. This terminology is motivated by the skewed distribution of available data per language: for the best-resourced languages there are huge corpora available, but for the long tail of languages, only small corpora exist. This is a key problem we address: the availability of data for tail languages is limited compared to head languages. As a result, tail languages have often been ignored by language technologies (Joshi et al., 2020).\nAlthough there exists some work on machine translation for a large number of tail languages (Costa-juss\u00e0 et al., 2022;Bapna et al., 2022), existing LLMs for tail languages are limited to a relatively small number of languages (Wang et al., 2019;Alabi et al., 2022;Wang et al., 2022). In this paper, we address this gap. Our work has three parts. (i) Corpus collection. We collect Glot2000-c, a corpus covering thousands of tail languages. (ii) Model training. Using Glot500-c, a subset of Glot2000-c, we train Glot500-m, an LLM covering 511 languages. (iii) Validation. We conduct an extensive evaluation of the quality of Glot500-m's representations of tail languages on a diverse suite of tasks.\nIn more detail, corpus collection considers three major sources: websites that are known to publish content in specific languages, corpora with classified multilingual content and datasets published in specific tail languages. The resulting dataset Glot2000-c comprises 700GB in 2266 languages collected from \u2248150 sources. After cleaning and deduplication, we create the subset Glot500-c, consisting of 511 languages and 534 language-scripts (where we define a language-script as a combination of ISO 639-3 3 and script) to train Glot500-m. Our criterion for including a language-script in Glot500-c is that it includes more than 30,000 sentences.\nModel training. To train Glot500-m, we employ vocabulary extension and continued pretraining. XLM-R's vocabulary is extended with new tokens trained on Glot500-c. We then perform continued pretraining of XLM-R with the MLM objective (Devlin et al., 2019).\nValidation. We comprehensively evaluate Glot500-m on a diverse suite of natural language understanding, sequence labeling and multilingual tasks for hundreds of languages. The results demonstrate that Glot500-m performs better than XLM-R-B (XLM-R-base) for tail languages by a large margin while performing comparably (or better) for head languages.\nPrevious work on multilinguality has been hindered by the lack of LLMs supporting a large number of languages. This limitation has led to studies being conducted in settings dissimilar from realworld scenarios. For example, Dufter and Sch\u00fctze (2020) use synthetic language data. And the curse of multilinguality has been primarily studied for a set of high-resource languages (Conneau et al., 2020). By creating Glot500-m, we can investigate these issues in a more realistic setting. We make code, data and trained models available to foster research by the community on how to include hundreds of languages that are currently ill-served by NLP technology.\nContributions. (i) We train the multilingual model Glot500-m on a 600GB corpus, covering more than 500 diverse languages, and make it publicly available at https://github.com/cisnlp/ Glot500. (ii) We collect and clean Glot500-c, a corpus that covers these diverse languages and al-3 https://iso639-3.sil.org/code_tables/639 lows us to train Glot500-m, and will make as much of it publicly available as possible. (iii) We evaluate Glot500-m on pseudoperplexity and on five diverse tasks across these languages. We observe large improvements for low-resource languages compared to an XLM-R baseline. (iv) Our extensive analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \"help\" from related languages and the total capacity of the model. (v) Our work addresses an important goal of NLP research: we should not limit NLP to a relatively small number of high-resource languages and instead strive to support as many languages as possible to bring the benefits of NLP to all languages and cultures.", "publication_ref": ["b38", "b32", "b30", "b30", "b35", "b7", "b37", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Training multilingual LLMs using the masked language modeling (MLM) objective is effective to achieve cross-lingual representations (Devlin et al., 2019;Conneau et al., 2020). These models can be further improved by incorporating techniques such as discriminative pre-training (Chi et al., 2022) and the use of parallel data (Yang et al., 2020;Chi et al., 2021). However, this primarily benefits a limited set of languages with large corpora.\nRecent research has attempted to extend existing LLMs to languages with limited resources. Wang et al. (2019) propose vocabulary extension; Ebrahimi and Kann (2021) investigate adaptation methods, including MLM and Translation Language Model (TLM) objectives and adapters; Alabi et al. (2022) adapt XLM-R to 17 African languages; Wang et al. (2022) expand language models to low-resource languages using bilingual lexicons.\nAlternatively, parameter-efficient fine-tuning adapts pre-trained models to new languages by training a small set of weights effectively (Zhao et al., 2020;Pfeiffer et al., 2021;Ansell et al., 2022). Pfeiffer et al. (2022) address the \"curse of multilinguality\" by sharing a part of the model among all languages and having separate modules for each language. We show that the common perception that multilinguality increases as we add more languages, until, from some point, it starts decreasing, is naive. The amount of available data per language and the similarity between languages also play important roles ( \u00a76.8).\nAnother approach trains LLMs from scratch for a limited number of tail languages; e.g., AfriBERTa (Ogueji et al., 2021a) and IndicNLPSuite (Kakwani et al., 2020) are LLMs for 11 African languages and 11 Indic languages. In concurrent work, Adebara et al. (2022) train a multilingual model for 517 African languages on a 42 GB corpus, but without making the model available and with an evaluation on a smaller number of languages than ours.\nClosely related to our work on corpus creation, Bapna et al. (2022) and Costa-juss\u00e0 et al. (2022) also create NLP resources for a large number of tail languages. They train a language identifier model and extract textual data for tail languages from largescale web crawls. This approach is effective, but it requires significant computational resources and native speakers for all tail languages. This is hard to do outside of large corporations. Bapna et al. (2022) have not made their data available. Costajuss\u00e0 et al. (2022) have only released a portion of their data in around 200 languages.\nA key benefit of \"horizontally\" scaled multilingual LLMs is transfer from high-to low-resource languages. Our evaluation suggests that Glot500-m excels at this, but this is not the main focus of our paper. There is a large body of work on crosslingual transfer: (Artetxe and Schwenk, 2019;Imani-Googhari et al., 2022;Lauscher et al., 2020;Conneau et al., 2020;Turc et al., 2021;Fan et al., 2021;Severini et al., 2022;Choenni and Shutova, 2022;Wang et al., 2023), inter alia.", "publication_ref": ["b38", "b41", "b35", "b7", "b37", "b22", "b21", "b17", "b30", "b30", "b38", "b34", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Glot2000-c 3.1 Data Collection", "text": "One of the major challenges in developing NLP technologies for tail languages is the scarcity of high-quality training data. In this work, we propose a lightweight methodology that is easily replicable for academic labs. We identify tail language data previously published by researchers, publishers and translators and then crawl or download them. By crawling a few websites and compiling data from around 150 different datasets, we amass more than 700GB of text in 2266 languages. We will refer to these sources of data as data sources. Our data covers many domains, including religious texts, news articles and scientific papers. Some of the data sources are high-quality, verified by native speakers, translators and linguists. Others are less reliable such as web crawls and Wikipedia dumps. It is therefore necessary to clean the data. For a list of data sources, see \u00a7C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Language-Scripts", "text": "Some languages are written in multiple scripts; e.g., Tajik is written in both Cyrillic and Arabic scripts. Some data sources indicate the script, but others either do not or provide mixed text in multiple scripts. We detect the script for each sentence and treat each language-script as a separate entity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ngram LMs and Language Divergence", "text": "We train a 3-gram character-level language model for each language-script , using KenLM (Heafield, 2011). We refer to the perplexity calculated for the corpus of language using language model as PP ( , ). Similar to Gamallo et al. (2017), we define a perplexity-based divergence measure of languages and as:\nD , = max PP ( , ), PP ( , )\nWe use D to filter out noisy data in \u00a73.4 and study the effect of similar languages in LLM training in \u00a76.7 and \u00a76.8. For more details, see \u00a7A.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data Cleaning", "text": "To remove noise, we use chunk-level and corpuslevel filters.\nWhile some sources are sentence-split, others provide multiple sentences (e.g., a paragraph) as one chunk. Chunk-level filters process each chunk of text from a data source as a unit, without sentencesplitting. Some chunk-level filters are based on the notion of word: we use white space tokenization when possible and otherwise resort to sentencePiece (Kudo and Richardson, 2018) trained by Costa-juss\u00e0 et al. (2022).\nAs chunk-level filters, we employ the sentencelevel filters SF1-SF5 from BigScience ROOTS (Lauren\u00e7on et al., 2022).\nSF1 Character repetition. If the ratio of repeated characters is too high, it is likely that the sentence has not enough textual content.\nSF2 Word repetition. A high ratio of repeated words indicates non-useful repetitive content.\nSF3 Special characters. Sentences with a high ratio of special characters are likely to be crawling artifacts or computer code.\nSF4 Insufficient number of words. Since training language models requires enough context, very small chunks of text are not useful. SF5 Deduplication. If two sentences are identical after eliminating punctuation and white space, one is removed.  In the rest of the paper, we refer to a chunk as a sentence'. A sentence' can consist of a short segment, a complete sentence or a chunk (i.e., several sentences).\nCorpus-level filters detect if the corpus of a language-script is noisy; e.g., the corpus is in another language or consists of non-meaningful content such as tabular data. We employ filters CF1 and CF2.\nCF1 In case of mismatch between language and script, the corpus is removed; e.g., Chinese written in Arabic is unlikely to be Chinese.\nCF2 Perplexity mismatch. For each languagescript L1, we find its closest language-script L2: the language-script with the lowest perplexity divergence ( \u00a73.3). If L1 and L2 are not in the same typological family, we check L1/L2 manually and take appropriate action such as removing the corpus (e.g., if it is actually English) or correcting the ISO code assigned to the corpus.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training Data: Glot500-c", "text": "Among the 2000+ language-scripts that we collected data for, after cleaning, most have too little data for pretraining LLMs. It is difficult to quantify the minimum amount needed for pretraining. Therefore, we pick a relatively high \"safe\" threshold, 30,000 sentences', for inclusion of language-scripts in model training. This allows us to train the model effectively and cover many low-resource languages. Table 1 gives Glot500-c statistics. See \u00a7B for a list of language-scripts. We train Glot500-m on Glot500-c; note that while Glot500-c focuses on tail languages, it contains some data in head languages which we include in Glot500-m training to prevent catastrophic forgetting.\nWe divide the corpus for each language into train/dev/test, reserving 1000 sentences' each for dev and test and using the rest for train. We pick 1000 parallel verses if we have a Bible translation 4 Glot500-m\nXLM-R-B XLM-R-L Glot500-m Model Size 278M 560M 395M Vocab Size 250K 250K 401K Transformer Size 86M 303M 86M", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Vocabulary Extension", "text": "To extend XLM-R's vocabulary, we use Sentence-Piece (Kudo and Richardson, 2018) with a unigram language model (Kudo, 2018) to train a tokenizer with a vocabulary size of 250K on Glot500-c. We sample data from different language-scripts according to a multinomial distribution, with =.3. The amount we sample for head languages is the same as tail languages with the lowest amount; this favors tail languages -head languages are already well learned by XLM-R. We merge the obtained tokens with XLM-R's vocabulary. About 100K new tokens were in fact old tokens, i.e., already part of XLM-R's vocabulary. We take the probabilities of the (genuinely) new tokens directly from Sen-tencePiece. After adding the 151K new tokens to XLM-R's vocabulary (which has size 250K), the vocabulary size of Glot500-m is 401K. We could also calculate probabilities of existing and new tokens over a mixture of original XLM-R training corpus and Glot500-c (Chung et al., 2020). For head languages, the percentage of changed tokens using the new tokenizer compared to the original tokenizer ranges from 0.2% to 50%. However, we found no relationship between percentage of changed tokens and change in performance on downstream tasks. Thus, there was little effect of tokenization in our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Continued Pretraining", "text": "We create Glot500-m by continued pretraining of XLM-R-B with the MLM objective. The optimizer used is Adam with betas (0.9, 0.999). Initial learning rate: 5e-5. Each training step contains a batch of 384 training samples randomly picked from all language-scripts. The sampling strategy across language-scripts is the same as for vocabu-  lary extension ( \u00a74.1). We save checkpoints every 10K steps and select the checkpoint with the best average performance on downstream tasks by early stopping. Table 2 lists the sizes of XLM-R-B, XLM-R-L and Glot500-m. Except for a larger vocabulary ( \u00a74.1), Glot500-m has the same size as XLM-R-B.\nWe train Glot500-m on a server with eight NVIDIA RTX A6000 GPUs for two weeks. Similar to XLM-R, we concatenate sentences' of a language-script and feed them as a stream to the tokenizer. The resulting output is then divided into chunks of 512 tokens and fed to the model.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Experimental Setup", "text": "For most tail languages, there are no manually labeled evaluation data. We therefore adopt a mixed evaluation strategy: based partly on human labels, partly on evaluation methods that are applicable to many languages without requiring gold data. Table 3 lists all our evaluation tasks. Salazar et al. (2020), we calculate pseudoperplexity (PPPL) over the heldout test set. PPPL is based on masking tokens one-by-one (not left to right). Salazar et al. (2020) give evidence that PPPL is a better measure of linguistic acceptability compared to standard leftto-right perplexity.", "publication_ref": ["b26", "b26"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Perplexity Following", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Roundtrip Alignment", "text": "For assessing the quality of multilingual representations for a broad range of tail languages without human gold data, we adopt roundtrip evaluation (Dufter et al., 2018). We first word-align sentences' in a parallel corpus based on the multilingual representations of an LLM. We then start from a word in a sentence' in language-script L1, follow the alignment links to its translations in language-script L2, then the alignment links from L2 to L3 and so on, until in the end we follow alignment links back to L1. If this \"roundtrip\" gets us back to , then it indicates that the LLM has similar representations for the meaning of in language-scripts L1, L2, L3, etc. In other words, the cross-lingual quality of representations is high. Vice versa, failure to get back to is a sign of poor multilingual representations.\nWe use SimAlign (Jalili Sabet et al., 2020) and align on the sub-word level on the Bible part of test, based on the representations of the LLM computed by transformer layer 8 as suggested in the original paper. We use intersection symmetrization: each word in a sentence' is aligned to at most one word in the other sentence'.\nAs evaluation measure we compute the percentage of roundtrips that were successes, i.e., the roundtrip starts at in L1 and returns back to . For each language-script in test, we randomly select three language-scripts as intermediate points L2, L3, L4. Since the intermediate points influence the results, we run the experiment five times with different intermediate points and report the average. All models are evaluated with the same five sets of three intermediate language-scripts.\nSequence Labeling We consider two sequence labeling tasks: Named Entity Recognition (NER) and Part-Of-Speech (POS) tagging. We use the WikiANN dataset (Pan et al., 2017) for NER and version v2.11 of Universal Dependencies (UD) (de Marneffe et al., 2021) for POS. Since training data does not exist for some languages, we finetune on English (with early stopping based on dev) and evaluate zero-shot transfer on all languages covered by WikiANN/UD. We set the learning rate to 2e-5 with Adam. Following (Hu et al., 2020), we use up to 1000 English-aligned sentences' from Tatoeba (Artetxe and Schwenk, 2019) to evaluate SentRetr (sentence retrieval). We also use 500 English-aligned sentences' from the Bible part of test. We find nearest neighbors using cosine similarity based on the average word embeddings in layer = 8 -following Jalili Sabet et al. (2020)and compute top10 accuracy. For fair comparison and because the architectures are the same, we do not optimize the hyperparameter for Glot500-m and XLM-R-B.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Sentence Retrieval", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Text Classification", "text": "We evaluate on Taxi1500 (Ma et al., 2023). It provides gold data for text classification with six classes in a large number of language-scripts of which Glot500-m supports 354. We finetune on English (with early stopping on dev) and evaluate zero-shot on test of the target language-script. Learning rate: 2e-5, batch size: 1086 16 (following Ma et al. (2023)).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we discuss aggregate results. For detailed results, see \u00a7D and \u00a7E.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 4 gives results. Glot500-m outperforms XLM-R-B on all tasks for both head and tail language-scripts, except for POS on head. That Glot500-m outperforms XLM-R-B is expected for tail language-scripts (i.e., those not covered by XLM-R). For these language-scripts the improvement margin is large. Outperformance may seem counterintuitive for head language-scripts (those covered by XLM-R) since Glot500-m has the same number of (non-embedding) parameters as XLM-R-B. Since the number of covered languages has greatly increased, leaving less capacity per language, we might expect underperformance. There are a few possible explanations. First, XLM-R may be undertrained, and the inclusion of more head language training data may improve their representations. Second, having more languages may improve multilinguality by allowing languages to synergize and enhance each other's representations and cross-lingual transfer. Third, there are languages similar to head languages among the tail languages, which in turn aids head languages.\nThe gap between Glot500-m and the baselines for tail language-scripts in sequence labeling is smaller. These tasks do not require as deep an understanding of language and thus transfer from head to tail language-scripts is easier through shared tokens.\nGlot500-m also outperforms XLM-R-L for tail language-scripts (all tasks) and head languagescripts (3 tasks). This suggests that scaling up size is not the only way for improvements. We can also improve the quality of multilingual LLM representations by increasing the number of languages.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Language Coverage", "text": "Table 5 compares Glot500-m vs. XLM-R-B on pseudoperplexity. For fair comparison we use word-level normalization. For 69 head languagescripts, Glot500-m underperforms XLM-R-B. This is expected as Glot500-m's training data is small for these language-scripts. Glot500-m outperforms XLM-R-B for 420 tail language-scripts.\nThere are eight tail language-scripts for which Glot500-m performs worse than XLM-R-B. Five are tail languages with a similar head language where the two share a macro-language: ekk/Standard Estonian (est/Estonian), aln/Gheg Albanian (sqi/Albanian), nob/Norwegian Bokmal (nor/Norwegian), hbs/Serbo-Croatian (srp/Serbian), lvs/Standard Latvian (lav/Latvian). Since XLM-R-B's pretraining corpus is large for the five head languages, its performance is good for the close tail languages.\nThe other three languages all have a unique script: sat/Santali (Ol Chiki script), div/Dhivehi (Thaana script), iku/Inuktitut (Inuktitut syllabics). For these languages, XLM-R-B's tokenizer returns many UNK tokens since it is not trained on these scripts, resulting in an unreasonably optimistic estimate of pseudoperplexity by our implementation. Glot500-m's token-level normalized pseudoperplexity ranges from 1.95 for lhu/Lahu to 94.4 for tok/Toki Pona. The average is 13.5, the median 10.6. We analyze the five language-scripts with the highest pseudoperplexity: tok_Latn, luo_Latn, acm_Arab, ach_Latn, and teo_Latn.\ntok/Toki Pona is a constructed language. According to Wikipedia: \"Essentially identical concepts can be described by different words as the choice relies on the speaker's perception and experience.\" This property can result in higher variability and higher perplexity.\nacm/Mesopotamian Arabic contains a large number of tweets in raw form. This may result in difficult-to-predict tokens in test.\nluo/Luo, ach/Acoli and teo/Teso are related Nilotic languages spoken in Kenya, Tanzania, Uganda and South Sudan. Their high perplex-  Table 4: Evaluation of XLM-R base and large (XLM-R-B and XLM-R-L) and Glot500-m on pseudoperplexity and six multilingual tasks across 5 seeds. Each number is an average over head, tail and all language-scripts. See \u00a7D, \u00a7E for results per task and language-script. Glot500-m outperforms XLM-R-B in all tasks for head (except for POS) and tail language-scripts and XLM-R-L for tail language-scripts. Best result per row/column group in bold.\nhead tail Glot500-m is better 37 420 XLM-R-B is better 69 8 ity could be related to the fact that they are tonal languages, but the tones are not orthographically indicated. Another possible explanation is that the training data is dominated by one subcorpus (Jehova's Witnesses) whereas the test data are dominated by PBC. There are orthographic differences between the two, e.g., \"dong\" (JW) vs. \"do\u014b\" (PBC) for Acoli. These three languages are also spoken over a large area in countries with different standard languages, which could increase variability. Our analysis is not conclusive. We note however that the gap between the three languages and the next most difficult languages in terms of pseudoperplexity is not large. So maybe Luo, Acoli and Teso are simply (for reasons still to be determined) languages that have higher perplexity than others.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Training Progression", "text": "To analyze the training process, we evaluate Glot500-m on sequence labeling and SentRetr at 10,000-step intervals. Figure 1 shows that performance improves rapidly at the onset of training, but then the rate of improvement slows down. This trend is particularly pronounced for tail languages in SentRetr. In comparison, sequence labeling is relatively straightforward, with the baseline (XLM-R-B, epoch 0) achieving high performance by correctly transferring prevalent classes such as verb and noun through shared vocabulary, resulting in a smaller improvement of Glot500-m vs. XLM-R-B.\nFor SentRetr, we observe larger improvements for the Bible than for Tatoeba. This is likely due to the higher proportion of religious data in Glot500-c, compared to XLM-R's training data (i.e., CC100).\nThe average performance on downstream tasks peaks at 480K steps. We have taken a snapshot of Glot500-m at this stage and released it.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Analysis across Language-Scripts", "text": "To analyze the effect of language-scripts, we select five tail language-scripts each with the largest and smallest gain when comparing Glot500-m vs. XLM-R-B for SentRetr and sequence labeling.\nTable 6 shows that Glot500-m improves languages with scripts not covered by XLM-R (e.g., div/Dhivehi, Thaana script, see \u00a76.2) by a large margin since XLM-R simply regards the uncovered scripts as unknown tokens and cannot compute meaningful representations for the input. The large amount of data we collected in Glot500-c also contributes to the improvement for tail languages, e.g., for tat_Cyrl (Tatar) in SentRetr Tatoeba and mlt_Latn (Maltese) in POS. See \u00a76.7 for a detailed analysis of the effect of corpus size.\nOn the other hand, Glot500-m achieves just comparable or even worse results for some languagescripts. We see at least three explanations. (i) As discussed in \u00a76.2, some tail languages (e.g., nob/Norwegian Bokmal) are close to a head language (e.g., nor/Norwegian), so Glot500-m has no advantage over XLM-R-B. (ii) A language is at the low end of our corpus size range (i.e., 30,000 sentences'). Example: xav_Latn, Xav\u00e1nte. (iii) Some languages are completely distinct from all other languages in Glot500-c, thus without support from any similar language. An example is mau_Latn, Huautla Mazatec. Glot500-m has a much harder  Table 7: Sentence Retrieval Bible performance of Glot500-m and XLM-R-B for six languages with two scripts: Uighur (uig), Hindi (hin), Uzbek (uzb), Kara-Kalpak (kaa), Northern Kurdish (kmr), Turkmen (tuk). Glot500-m clearly outperforms XLM-R-B with large differences for tail language-scripts.\ntime learning good representations in these cases.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "Languages with Multiple Scripts", "text": "Table 7 compares SentRetr performance XLM-R-B vs. Glot500-m for six languages with two scripts. Unsurprisingly, XLM-R performs much better for a language-script it was pretrained on (\"head\") than on one that it was not (\"tail\"). We can improve the performance of a language, even surpassing the language-script covered by XLM-R, if we collect enough data for its script not covered by XLM-R.\nFor languages with two scripts not covered by XLM-R, the performance is better for the script for which we collect a larger corpus. For example, kaa_Cyrl (Kara-Kalpak) has about three times as much data as kaa_Latn. This explains why kaa_Cyrl outperforms kaa_Latn by 30%. Dufter and Sch\u00fctze (2020) found that, after training a multilingual model with two scripts for English (natural English and \"fake English\"), the model performed well at zero-shot transfer if the capacity of the model was of the right size (i.e., not too small, not too large). Our experiments with real data show the complexity of the issue: even if there is a \"right\" size for an LLM that supports both full acquisition of languages and multilingual transfer, this size is difficult to determine and it may be different for different language pairs in a large horizontally scaled model like Glot500-m.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis across Language Families", "text": "Table 8 compares SentRetr performance Glot500-m vs. XLM-R-B for seven language families that have ten or more language-scripts in Glot500-c. We assign languages to families based on Glottolog. 4 Generally, XLM-R has better performance the more language-scripts from a language family are represented in its training data; e.g., performance is better for indo1319 and worse for maya1287. The results suggest that Glot500-m's improvement over  Table 9: Performance on Sentence Retrieval Bible of continued pretraining on just one language-script (Glot+1) vs. on Glot500-c (Glot500-m). Glot500-m underperforms on the top three and outperforms on the bottom three. Our explanation is that the second group is supported by closely related languages in Glot500-c; e.g., for Southern Quechua (quh), Glot500-m also covers closely related Cuzco Quechua (quz). For the first group this is not the case; e.g., the Wa language (wbm) has no close relative in Glot500-c.\nfamily | | | | XLM-R-B\nXLM-R is the larger, the better our training corpus Glot500-c's coverage is of a family.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "Effect of Amount of Training Data", "text": "We examine correlation between pretraining corpus size and Glot500-m zero-shot performance. We focus on SentRetr Bible ( \u00a75) since it supports the most head and tail languages. We find that Pearson's = .34, i.e., corpus size and performance are moderately, but clearly correlated. We suspect that the correlation is not larger because, in addition to corpus size of language itself, corpus size of languages closely related to is also an important factor (see \u00a76.4 for a similar finding for Norwegian). We therefore also compute Pearson's between (i) performance of language on SentRetr Bible and (ii) joint corpus size of and its nearest neighbors (according to perplexity divergence, \u00a73.3). In this case, Pearson's = .44 (for both = 3 and = 4), indicating that the corpus size of nearest neighbor languages does play a role.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Support through Related Languages", "text": "Building on \u00a76.7, there is another way we can investigate the positive effect of closely related languages on performance: We can compare performance (again on SentRetr Bible) of continued pretraining on just one language (we refer to this model as Glot+1) vs. on all 511 languages represented in Glot500-c (i.e., Glot500-m). Table 9 presents results for six language-scripts selected from various language families and suggests that some languages do not receive support from related languages (top three). In that case, Glot+1 can fully concentrate on learning the isolated language and does better than Glot500-c. Other languages (bottom three) do receive support from related languages. For example, Southern Quechua (quh) seems to receive support in Glot500-m from closely related Cuzco Quechua (quz), resulting in Glot500-m outperforming Glot+1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We collect and data-clean Glot500-c, a large corpus of hundreds of usually neglected tail (i.e., long-tail) languages and create Glot500-m, an LLM that is trained on Glot500-c and covers these languages. We evaluate Glot500-m on six tasks that allow us to evaluate almost all languages. We observe large improvements for both head and tail languages compared to XLM-R. Our analysis shows that no single factor fully explains the quality of the representation of a language in a multilingual model. Rather, a combination of factors is important, including corpus size, script, \"help\" from related languages and the total capacity of the model. This work is the first to create a language model on a dataset of several hundreds of gigabytes and to make it publicly available for such a large and diverse number of low-resource languages. In future research, we would like to train larger models to further investigate the effect of model size, distill highly multilingual models for resource-efficient deployment, explore alternatives to continued pretraining and use models for more tail language downstream tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "(1) We did not perform any comprehensive hyperparameter search, which would have further consolidated our results. This decision was made due to the high cost of training multiple models. (2) Compared to current very large models, Glot500-m is comparatively small. (3) Although we have tried to minimize the amount of noise in our data, some noise is still present.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "There are two issues worth mentioning in regards to this project. First, it was not feasible for us to thoroughly examine the content of the data for all languages, thus we cannot confirm the absence of discrimination based on factors such as race or sexuality. The data was solely utilized as a textual corpus, and the content should not be interpreted as an endorsement by our team. If the model is subsequently utilized for generation, it is possible that the training data may be reflected in the generated output. However, addressing potential biases within the data is an area for future research. Second, it is important to note that while the data sources utilized in this study do not explicitly prohibit the reuse of data for research purposes, some sources do have copyright statements indicating that such use is permissible while others do not. Additionally, certain sources prohibit the redistribution of data. As such, data from these sources is omitted from the published version of Glot2000-c. \nPP ( , ) = =1 1 P \u210e | \u210e \u22121 1 (1)\nwhere\nP \u210e | \u210e \u22121 1\nis computed as by dividing the observed frequency ( ) of \u210e \u22121 1 \u210e by the observed frequency of \u210e \u22121 1 in training data:\nP \u210e | \u210e \u22121 1 = \u210e \u22121 1 \u210e \u210e \u22121 1 (2)\nGiven the definition of perplexity, we can determine how well a trained language model on language 1 predicts the test text of language 2 and vice-versa. The divergence between two languages is computed with the maximum of the perplexity values in both directions. Two reasons lead to the use of max: first, a symmetrical divergence is required, and second, languages differ in their complexity, so one direction of computing perplexity may result in a much lower perplexity than another. Thus, comparing perplexity results becomes difficult. As an example, the Kuanua language (ksd_Latn) has short words and a simple structure, which results in 3\u2212gram models getting lower perplexity on its text compared to other languages. The lower the perplexity the smaller the divergence between languages. The divergence (D) between language and with trained language models of and test texts of , where is the corresponding language, computed as follows:\nD , = max PP ( , ), PP ( , )(3)\nRuns and Data. The data used to train and test the character level n-gram models is the same data used for the training and testing of the Glot500-m.\nThe training of the models was limited to 100, 000 sentences' per language-script. We use KenLM library (Heafield, 2011) to build n-gram models. This library uses an interpolated modified Kneser-Ney smoothing for estimating the unseen n-grams.\nOur evaluation has been performed over 7 n-gram models (3 \u2264 \u2264 9). Baseline and Evaluation. Language family trees were used as a baseline for evaluating the divergence measures of the proposed approach. We obtained language family tree data from Ethnologue online version (Eberhard et al., 2022). For each language, the family tree follows the general order from largest typological language family group to smallest. There is only one family tree for each language in the baseline data. Nodes in the family tree represent typological language family groups.\nEach node only has one parent, so if a node is common in the family tree of two languages, its parent is also common. We evaluate our perplexity method on the following binary classification task: Do the majority of a language 's nearest neighbors belong to the same typological language family group as ? Assuming languages and , with the following family trees:\n: 1 \u2192 2 \u2192 3 \u2192 4 \u2192 5 \u2192 6 : 1 \u2192 2 \u2192 7 \u2192 8\nThese 2 languages belong to the same typological family group with family tree levels of \u2208 {1, 2}, but not with family tree levels of = 3 and higher.\nResult. When it comes to language families, the majority of studies only refer to the largest typological language family group (level = 1). Here, we also assess our methodology for other levels.\nThe results of classification accuracy for 3\u2212gram model, \u2208 {1, 3, 7, 13, 21} and \u2208 {1, 2, 3, max} are shown in Table 10. In cases where the maximum level of a tree is less than the parameter, the maximum level for that language is used. Languages without a family or no other family member in our data are excluded. We only report the 3\u2212gram model results as it gets the best results in most configurations among other n-gram models. With increasing , the accuracy decreases, since more languages fall outside the same typological family.\nAs increases, the accuracy decreases, because languages with faraway neighbors are being included but the number of languages in the language typological group family will remain the same. There are times when languages have a lot of loan words from other languages because of geological proximity or historical reasons (e.g, colonization), which makes them similar to the languages they borrowed words from in our method. However they are different when it comes to their typological families and our method fails in these cases. Aymara (Macrolanguage: aym_Latn) and Quechua (Macrolanguage: que_Latn), for example, had a great deal of contact and influence on each other, but they do not belong to the same typological group. As well, some of the typological families are not that large, which makes our results worse when increases. This is the case, for instance, of the Tarascan typological family which only has two members.\nmodel accuracy (%) ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "B Languages", "text": "The list of languages used to train Glot500-m with the amount of available data for each language is available in Tables 11, 12 and 13.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "On Macrolanguages", "text": "The presence of language codes that are supersets of other language codes within datasets is not uncommon (Kreutzer et al., 2022). This issue becomes more prevalent in extensive collections. Within the ISO 639-3 standard, these languages are referred to as macrolanguages.\nWhen confronted with macrolanguages, if it is not feasible to ascertain the specific individual language contained within a dataset, the macrolanguage code is retained. Consequently, it is possible that in Glot2000-c and Glot500-c both the corpora for the macrolanguage and its individual languages have been included.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C List of data sources", "text": "The datasets and repositories used in this project involve: AI4Bharat, 5 AIFORTHAI-LotusCorpus, 6 Add (El-Haj et al., 2018), AfriBERTa (Ogueji et al., 2021b), AfroMAFT (Adelani et al., 2022;Xue et al., 2021), Anuvaad, 7 AraBench (Sajjad et al., 2020) ", "publication_ref": ["b18", "b7", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "D Results for Each Task and Language", "text": "We report the detailed results for all tasks and languages in ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Perplexity Results for all Languages", "text": "Perplexity number for all languages is presented in Table 23, Table 24, and Table 25.             1107                       B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? section 'Ethics Statement' B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Since our work deals with millions of sentences in hundreds of languages, it was impossible for us to check the content. We leave it as a future work C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? section 5. For continued pretraining, it is a single run due to computational resource limitation. For downstream task evaluation, it is multilple runs across 5 seeds.\nLanguage-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? section 3.3 D Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\nNo response.\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_2"]}, {"heading": "Acknowledgements", "text": "We would like to thank Renhao Pei, Yihong Liu, Verena Blaschke, and the anonymous reviewers. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Parallel corpora for bi-lingual English-Ethiopian languages statistical machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Michael Solomon Teferra Abate; Martha Yifiru Melese; Million Tachbelie; Solomon Meshesha; Wondwossen Atinafu; Yaregal Mulugeta; Hafte Assabie; Binyam Abera; Tewodros Ephrem; Wondimagegnhue Abebe; Amanuel Tsegaye;  Lemma"}, {"ref_id": "b1", "title": "QADI: Arabic dialect identification in the wild", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Ahmed Abdelali; Hamdy Mubarak; Younes Samih; Sabit Hassan; Kareem Darwish"}, {"ref_id": "b2", "title": "Shami: A corpus of Levantine Arabic dialects", "journal": "", "year": "2018", "authors": "Motaz Kathrein Abu Kwaik;  Saad"}, {"ref_id": "b3", "title": "Muhammad Abdul-Mageed, and Alcides Alcoba Inciarte. 2022. SERENGETI: Massively multilingual language models for Africa", "journal": "", "year": "", "authors": "Ife Adebara; Abdelrahim Elmadany"}, {"ref_id": "b4", "title": "Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022. A few thousand translations go a long way! leveraging pre-trained models for African news translation", "journal": "Association for Computational Linguistics", "year": "", "authors": "David Adelani; Jesujoba Alabi; Angela Fan; Julia Kreutzer; Xiaoyu Shen; Machel Reid; Dana Ruiter; Dietrich Klakow; Peter Nabende; Ernie Chang; Tajuddeen Gwadabe; Freshia Sackey; F P Bonaventure; Chris Dossou; Colin Emezue; Michael Leong; Shamsuddeen Beukman; Guyo Muhammad; Oreen Jarso; Andre Niyongabo Yousuf; Gilles Rubungo; Eric Peter Hacheme; Muhammad Umair Wairagala; Benjamin Nasir; Tunde Ajibade; Yvonne Ajayi; Jade Gitau; Mohamed Abbott; Millicent Ahmed; Anuoluwapo Ochieng; Perez Aremu; Jonathan Ogayo;  Mukiibi; Godson Fatoumata Ouoba Kabore; Derguene Kalipe;  Mbaye"}, {"ref_id": "b5", "title": "Ayodele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. 2021. The effect of domain and diacritics in Yoruba-English neural machine translation", "journal": "", "year": "", "authors": "David Adelani; Dana Ruiter; Jesujoba Alabi; Damilola Adebonojo; Adesina Ayeni; Mofe Adeyemi"}, {"ref_id": "b6", "title": "Developing new linguistic resources and tools for the Galician language", "journal": "", "year": "2018", "authors": "Rodrigo Agerri; Xavier G\u00f3mez Guinovart; German Rigau; Miguel Anxo Solla Portela"}, {"ref_id": "b7", "title": "Adapting pretrained language models to African languages via multilingual adaptive fine-tuning", "journal": "", "year": "2022", "authors": "O Jesujoba; David Ifeoluwa Alabi; Marius Adelani; Dietrich Mosbach;  Klakow"}, {"ref_id": "b8", "title": "Ehsaneddin Asgari, and Hinrich Sch\u00fctze. 2023. Taxi1500: A multilingual dataset for text classification in 1500 languages", "journal": "", "year": "", "authors": "Chunlan Ma; Ayyoob Imanigooghari; Haotian Ye"}, {"ref_id": "b9", "title": "W2C -web to corpus -corpora", "journal": "", "year": "2011", "authors": "Martin Majli\u0161"}, {"ref_id": "b10", "title": "LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Orhan Firat, and Sriram Chellappan. 2021. A large-scale study of machine translation in Turkic languages", "journal": "Association for Computational Linguistics", "year": "", "authors": "Jamshidbek Mirzakhalov; Anoop Babu; Duygu Ataman; Sherzod Kariev; Francis Tyers; Otabek Abduraufov; Mammad Hajili; Sardana Ivanova; Abror Khaytbaev; Antonio Laverghetta; Esra Bekhzodbek Moydinboyev; Shaxnoza Onal; Ahsan Pulatova;  Wahab"}, {"ref_id": "b12", "title": "TeDDi sample: Text data diversity sample for language comparison and multilingual NLP", "journal": "", "year": "2022", "authors": "Steven Moran; Christian Bentz; Ximena Gutierrez-Vasques; Olga Pelloni; Tanja Samardzic"}, {"ref_id": "b13", "title": "JParaCrawl: A large scale web-based English-Japanese parallel corpus", "journal": "European Language Resources Association", "year": "2020", "authors": "Makoto Morishita; Jun Suzuki; Masaaki Nagata"}, {"ref_id": "b14", "title": "Overview of the 9th workshop on Asian translation", "journal": "", "year": "2022", "authors": "Toshiaki Nakazawa; Hideya Mino; Isao Goto; Raj Dabre; Shohei Higashiyama; Shantipriya Parida; Anoop Kunchukuttan; Makoto Morishita; Ond\u0159ej Bojar; Chenhui Chu; Akiko Eriguchi; Kaori Abe; Yusuke Oda; Sadao Kurohashi"}, {"ref_id": "b15", "title": "Overview of the 8th workshop on Asian translation", "journal": "", "year": "2021", "authors": "Toshiaki Nakazawa; Hideki Nakayama; Chenchen Ding; Raj Dabre; Shohei Higashiyama; Hideya Mino; Isao Goto; Win Pa Pa; Anoop Kunchukuttan; Shantipriya Parida; Ond\u0159ej Bojar; Chenhui Chu; Akiko Eriguchi; Kaori Abe; Yusuke Oda; Sadao Kurohashi"}, {"ref_id": "b16", "title": "The Kyoto free translation task", "journal": "", "year": "2011", "authors": "Graham Neubig"}, {"ref_id": "b17", "title": "Small data? no problem! exploring the viability of pretrained multilingual language models for lowresourced languages", "journal": "", "year": "2021", "authors": "Kelechi Ogueji; Yuxin Zhu; Jimmy Lin"}, {"ref_id": "b18", "title": "Small data? no problem! exploring the viability of pretrained multilingual language models for lowresourced languages", "journal": "", "year": "2021", "authors": "Kelechi Ogueji; Yuxin Zhu; Jimmy Lin"}, {"ref_id": "b19", "title": "Multilingual open text release 1: Public domain news in 44 languages", "journal": "European Language Resources Association", "year": "2022", "authors": "Chester Palen-Michel; June Kim; Constantine Lignos"}, {"ref_id": "b20", "title": "Cross-lingual name tagging and linking for 282 languages", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Xiaoman Pan; Boliang Zhang; Jonathan May; Joel Nothman; Kevin Knight; Heng Ji"}, {"ref_id": "b21", "title": "Lifting the curse of multilinguality by pre-training modular transformers", "journal": "", "year": "2022", "authors": "Jonas Pfeiffer; Naman Goyal; Xi Lin; Xian Li; James Cross; Sebastian Riedel; Mikel Artetxe"}, {"ref_id": "b22", "title": "UNKs everywhere: Adapting multilingual language models to new scripts", "journal": "", "year": "2021", "authors": "Jonas Pfeiffer; Ivan Vuli\u0107; Iryna Gurevych; Sebastian Ruder"}, {"ref_id": "b23", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b24", "title": "Tilde MODEL -multilingual open data for EU languages", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Roberts Rozis; Raivis Skadin"}, {"ref_id": "b25", "title": "AraBench: Benchmarking dialectal Arabic-English machine translation", "journal": "", "year": "2020", "authors": "Hassan Sajjad; Ahmed Abdelali; Nadir Durrani; Fahim Dalvi"}, {"ref_id": "b26", "title": "Masked language model scoring", "journal": "", "year": "2020", "authors": "Julian Salazar; Davis Liang; Toan Q Nguyen; Katrin Kirchhoff"}, {"ref_id": "b27", "title": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b28", "title": "Wiki-Matrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia", "journal": "Online. Association for Computational Linguistics", "year": "2021", "authors": "Holger Schwenk; Vishrav Chaudhary; Shuo Sun; Hongyu Gong; Francisco Guzm\u00e1n"}, {"ref_id": "b29", "title": "Towards a broad coverage named entity resource: A data-efficient approach for many diverse languages", "journal": "", "year": "2022", "authors": "Silvia Severini; Ayyoob Imani; Philipp Dufter; Hinrich Sch\u00fctze"}, {"ref_id": "b30", "title": "Towards the next 1000 languages in multilingual machine translation: Exploring the synergy between supervised and self-supervised learning", "journal": "", "year": "2022", "authors": "Aditya Siddhant; Ankur Bapna; Orhan Firat; Yuan Cao; Mia Xu Chen; Isaac Caswell; Xavier Garcia"}, {"ref_id": "b31", "title": "Named entity recognition for south and south East Asian languages: Taking stock", "journal": "", "year": "2008", "authors": "Anil Kumar Singh"}, {"ref_id": "b32", "title": "Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures", "journal": "", "year": "2019", "authors": "Pedro Javier Ortiz Su\u00e1rez; Beno\u00eet Sagot; Laurent Romary"}, {"ref_id": "b33", "title": "Parallel data, tools and interfaces in opus", "journal": "", "year": "2012", "authors": "J\u00f6rg Tiedemann"}, {"ref_id": "b34", "title": "Revisiting the primacy of english in zero-shot cross-lingual transfer", "journal": "", "year": "2021", "authors": "Iulia Turc; Kenton Lee; Jacob Eisenstein; Ming-Wei Chang; Kristina Toutanova"}, {"ref_id": "b35", "title": "Improving pre-trained multilingual model with vocabulary expansion", "journal": "", "year": "2019", "authors": "Hai Wang; Dian Yu; Kai Sun; Jianshu Chen; Dong Yu"}, {"ref_id": "b36", "title": "Jannik Str\u00f6tgen, and Hinrich Sch\u00fctze. 2023. NLNDE at semeval-2023 task 12: Adaptive pretraining and source language selection for low-resource multilingual sentiment analysis", "journal": "CoRR", "year": "", "authors": "Mingyang Wang; Heike Adel; Lukas Lange"}, {"ref_id": "b37", "title": "Expanding pretrained models to thousands more languages via lexicon-based adaptation", "journal": "Long Papers", "year": "2022", "authors": "Xinyi Wang; Sebastian Ruder; Graham Neubig"}, {"ref_id": "b38", "title": "Ccnet: Extracting high quality monolingual datasets from web crawl data", "journal": "European Language Resources Association", "year": "2020-05-11", "authors": "Guillaume Wenzek; Marie-Anne Lachaux; Alexis Conneau; Vishrav Chaudhary; Francisco Guzm\u00e1n; Armand Joulin; Edouard Grave"}, {"ref_id": "b39", "title": "CCNet: Extracting high quality monolingual datasets from web crawl data", "journal": "", "year": "2020", "authors": "Guillaume Wenzek; Marie-Anne Lachaux; Alexis Conneau; Vishrav Chaudhary; Francisco Guzm\u00e1n; Armand Joulin; Edouard Grave"}, {"ref_id": "b40", "title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer", "journal": "", "year": "", "authors": "Linting Xue; Noah Constant; Adam Roberts; Mihir Kale; Rami Al-Rfou; Aditya Siddhant"}, {"ref_id": "b41", "title": "Alternating language modeling for cross-lingual pre-training", "journal": "", "year": "2020", "authors": "Jian Yang; Shuming Ma; Dongdong Zhang; Shuangzhi Wu; Zhoujun Li; Ming Zhou"}, {"ref_id": "b42", "title": "Introducing QuBERT: A large monolingual corpus and BERT model for Southern Quechua", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Rodolfo Zevallos; John Ortega; William Chen; Richard Castro; N\u00faria Bel; Cesar Toshio; Renzo Venturas; Hilario Aradiel; Nelsi Melgarejo"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Progression of training for sentence retrieval and sequence labeling. x-axis: epochs/10K. The improvement is fast in the beginning for tail languages, then gets slower and and reaches a plateau. This pattern is partially observed for head languages.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "-B XLM-R-L Glot500-m XLM-R-B XLM-R-L Glot500-m XLM-R-B XLM-R-L Glot500-m", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m afr_Latn 71.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "1104Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m ace_Latn", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Top10 accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Sentence Retrieval Bible (Part I). 1105 Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m dtp_Latn 5.4", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "16: Top10 accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Sentence Retrieval Bible (Part II). 1106 Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m ace_Latn 33.4", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "F1 of XLM-R-B, XLM-R-L, and Glot500-m on NER.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "F1 of XLM-R-B, XLM-R-L, and Glot500-m on POS. 1108 Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "F1 of XLM-R-B, XLM-R-L, and Glot500-m on Text Classification (Part I). 1109 Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "F1 of XLM-R-B, XLM-R-L, and Glot500-m on Text Classification (Part II). 1110 Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "21: Accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Round Trip Alignment (Part I). 1111 Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "1112Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m srd_Latn", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "1113Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m swc_Latn", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "1114Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m bts_Latn", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Statistics for Glot2000-c, Glot500-c and ex-isting multilingual datasets: number of languages, scripts, sentences' and median number of sentences' per language-script."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Model sizes. Glot500-m and XLM-R-B have the same transformer size, but Glot500-m has a larger vocabulary, resulting in an overall larger model.", "figure_data": "and add 500 each to test and dev. These parallel verses convey identical meanings and facilitate crosslingual evaluation. We pretrain the model using only the training data."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Evaluation tasks and measures. |head|/|tail|: number of head/tail language-scripts", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": Pseudoperplexity Glot500-m vs XLM-R-B. Glot500-m's worse performance on head can be at-tributed to smaller training corpora and the relative diffi-culty of learning five times more languages with the same number of (non-embedding) parameters. Glot500-m per-forms better on almost all tail language-scripts.  \u00a76.2 discusses the eight exceptions."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "", "figure_data": ": Results for five tail language-scripts each with the largest (high end) and smallest (low end) gain Glot500-m vs. XLM-R-B for four tasks. Glot500-m's gain over XLM-R-B is large at the high end and small or slightly negative at the low end. L = Latin, C = Cyrillic, H = Hani, A = Armenian, T = Thaanalang-scriptXLM-R-B Glot500-m gainuig_Arab uig_Latn hin_Deva head head tail hin_Latn tail uzb_Latn head uzb_Cyrl tail kaa_Cyrl tail kaa_Latn tail kmr_Cyrl tail kmr_Latn tail tuk_Cyrl tail tuk_Latn tail45.8 9.8 67.0 13.6 54.8 6.2 17.6 9.2 4.0 35.8 13.6 9.656.2 10.4 62.8 53.0 76.6 9.6 43.2 29.6 67.6 12.8 78.8 72.6 73.8 56.2 43.4 34.2 42.4 38.4 63.0 27.2 65.0 51.4 66.2 56.6"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": ": Average Sentence Retrieval Bible performance of Glot500-m and XLM-R-B for seven language families. The difference in coverage of a family by Glot500-m vs. XLM-R-B is partially predictive of the performance difference. | |/| |: number of language-scripts from family covered by Glot500-m/XLM-R.lang-scriptGlot+1 Glot500-mrug_Latn, Roviana yan_Latn, Mayangna/Sumo wbm_Latn, Wa/Va51.0 46.4 49.649.0 31.8 46.4ctd_Latn, Tedim Chin quh_Latn, Southern Quechua 33.4 47.4 tat_Cyrl, Tatar 58.859.4 56.2 67.2"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Israa Alsarsour, Esraa Mohamed, Reem Suwaileh, and Tamer Elsayed. 2018. DART: A large dataset of dialectal Arabic tweets. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440-8451, Online. Association for Computational Linguistics. Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672. El-Haj, Paul Rayson, and Mariam Aboelezz. 2018. Arabic dialect identification in the context of bivalency and code-switching. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187-197, Edinburgh, Scotland. Association for Computational Linguistics. Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Perplexity measures how well a model predicts a sample test data. Assuming a test data contains sequences of characters = \u210e 1 , \u210e 2 , \u2022 \u2022 \u2022 , \u210e , perplexity (PP) of given an n-gram character level language model is computed as follows:", "figure_data": "Antonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Franscisco Guzm\u00e1n, Junjie Hu, Mac-duff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp \u00d6ktem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020. TICO-19: the translation initiative for COvid-19. In Proceed-ings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online. Association for Computational Linguistics. Marie-Catherine de Marneffe, Christopher D. Manning, Joakim Nivre, and Daniel Zeman. 2021. Universal dependencies. Computational Linguistics, 47(2):255-308. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under-Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-ham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4411-4421. PMLR. Ayyoob ImaniGooghari, Silvia Severini, Masoud Jalili Sabet, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2022. Graph-based multilingual label propagation for low-resource part-of-speech tagging. In Proceed-ings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1577-1589, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Masoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2020. SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1627-1643, Online. Association for Computational Linguistics.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901. Jos\u00e9 Camacho-Collados, Claudio Delli Bovi, Alessandro Raganato, and Roberto Navigli. 2016. A large-scale multilingual disambiguation of glosses. In Proceed-ings of the Tenth International Conference on Lan-guage Resources and Evaluation (LREC'16), pages 1701-1708, Portoro\u017e, Slovenia. European Language Resources Association (ELRA). Mahmoud Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021. Be-yond english-centric multilingual machine translation. Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Israel Abebe Azime, Ayodele Awokoya, Duygu Ata-man, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Compu-tational Linguistics, 10:50-72. J. Mach. Learn. Res., 22:107:1-107:48. Pablo Gamallo, Jose Ramom Pichel, and I\u00f1aki Alegria. 2017. A perplexity-based method for similar lan-guages discrimination. In Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial), pages 109-114, Valencia, Spain. Association for Computational Linguistics. Dirk Goldhahn, Thomas Eckart, and Uwe Quasthoff. 2012. Building large monolingual dictionaries at the leipzig corpora collection: From 100 to 200 languages. In Proceedings of the Eighth International Conference on Language Resources and Evaluation, LREC 2012, Istanbul, Turkey, May 23-25, 2012, pages 759-765. European Language Resources Association (ELRA). Taku Kudo. 2018. Subword regularization: Improv-ing neural network translation models with multiple subword candidates. In Proceedings of the 56th An-nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66-75, Melbourne, Australia. Association for Computational Linguistics. Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tok-enizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics.collection and curation of monolingual and bilingual data: focus on under-resourced languages. In Proceedings of the 23rd Annual Conference of the European Associa-tion for Machine Translation, EAMT 2022, Ghent, Belgium, June 1-3, 2022, pages 301-302. European Association for Machine Translation. 2018. Embedding learning through multilingual concept induction. In Proceedings of the 56th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: Long Papers), pages 1520-1530, Melbourne, Australia. Association for Computational Linguistics. Jonathan Dunn. 2020. Mapping languages: the corpus of global language use. Lang. Resour. Evaluation, 54(4):999-1018. Eberhard, David M., Gary F. Simons, and Charles D. Fen-nig (eds.). 2022. Ethnologue: Languages of the world. twenty-fifth edition. Abteen Ebrahimi and Katharina Kann. 2021. How to adapt your pretrained multilingual model to 1600 languages. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4555-4567, Online. Association for Computational Linguistics. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282-6293, Online. Association for Computational Linguistics. Divyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. 2020. IndicNLPSuite: Monolin-gual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4948-4961, Online. Association for Computational Linguistics. Fajri Koto and Ikhwan Koto. 2020. Towards computa-tional linguistics in Minangkabau language: Studies on sentiment analysis and machine translation. In Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation, pages 138-148, Hanoi, Vietnam. Association for Computational Linguistics. Julia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allah-sera Tapo, Nishant Subramani, Artem Sokolov, Clay-tone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-dre Niyongabo Rubungo, Toan Q. Nguyen, Math-ias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen HassanHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa. 2020. Improving multilingual models with language-clustered vocabularies. In Proceed-ings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4536-4546, Online. Association for Computational Linguistics. Santiago G\u00f3ngora, Nicol\u00e1s Giossa, and Luis Chiruzzo. 2021. Experiments on a Guarani corpus of news and social media. In Proceedings of the First Work-shop on Natural Language Processing for Indigenous Languages of the Americas, pages 153-158, Online. Association for Computational Linguistics. Santiago G\u00f3ngora, Nicol\u00e1s Giossa, and Luis Chiruzzo. 2022. Can we use word embeddings for enhancing Guarani-Spanish machine translation? In Proceed-ings of the Fifth Workshop on the Use of Computa-tional Methods in the Study of Endangered Languages, pages 127-132, Dublin, Ireland. Association for Com-putational Linguistics. Thamme Gowda, Zhao Zhang, Chris Mattmann, and Anoop Kunchukuttan, Pratik Mehta, and Pushpak Bhat-tacharyya. 2018. The IIT Bombay English-Hindi parallel corpus. In Proceedings of the Eleventh In-ternational Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA). Hugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Ed-uardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. 2022. The BigScience ROOTS Corpus: A 1.6 TB Compos-ite Multilingual Dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Jonathan May. 2021. Many-to-English machine trans-lation tools, data, and pretrained models. In Proceed-ings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 306-316, Online. As-sociation for Computational Linguistics. Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-lam, Kazi Samin Mubasshir, Yuan-Fang Li, Yong-Bin Kang, M. Sohel Rahman, and Rifat Shahriyar. 2021. Xl-sum: Large-scale multilingual abstrac-tive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL/\u0132CNLP 2021, Online Event, August 1-6, 2021, volume ACL/\u0132CNLP 2021 of Findings of ACL, pages 4693-4703. Association for Computational Linguis-tics. Anne Lauscher, Vinit Ravishankar, Ivan Vuli\u0107, and Goran Glava\u0161. 2020. From zero to hero: On the limita-tions of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-ing (EMNLP), pages 4483-4499, Online. Association for Computational Linguistics. Colin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Mengjie Zhao, Tao Lin, Fei Mi, Martin Jaggi, and Hinrich Sch\u00fctze. 2020. Masking as an efficient alter-native to finetuning for pretrained language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2226-2241, Online. Association for Computa-tional Linguistics. Filighera, Abraham Owodunni, and Daniel White-nack. 2022. Bloom library: Multimodal datasets in Computational Linguistics. ber 7-11, 2022, pages 8608-8621. Association for 2022, Abu Dhabi, United Arab Emirates, Decem-Methods in Natural Language Processing, EMNLP Proceedings of the 2022 Conference on Empirical 300+ languages for a variety of downstream tasks. In A N-"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "", "figure_data": "1100"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "", "figure_data": "1101"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "List of languages used to train Glot500-m (Part III).", "figure_data": "guages (Abate et al., 2018), Phontron (Neubig, 2011), QADI (Abdelali et al., 2021), Quechua-IIC (Zevallos et al., 2022), SLI_GalWeb.1.0 (Agerri et al., 2018), Shami (Abu Kwaik et al., 2018), Stanford NLP, 23 StatMT, 24 TICO (Anastasopou-los et al., 2020), TIL (Mirzakhalov et al., 2021), Tatoeba, 25 TeDDi (Moran et al., 2022), Tilde (Rozis and Skadin , \u0161, 2017), W2C (Majli\u0161, 2011), WAT (Nakazawa et al., 2022), WikiMatrix (Schwenk et al., 2021), Wikipedia, 26 Workshop on NER for South and South East Asian Languages (Singh, 2008), XLSum (Hasan et al., 2021)."}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "", "figure_data": "(Sentence Retrieval Tatoeba), 15, 16 (Sentence Retrieval Bible), 17 (NER), and 18 (POS), 19, 20 (Text Classification), 21, 22 (Round Trip Alignment)."}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_19", "figure_caption": "", "figure_data": ""}, {"figure_label": "15", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_23", "figure_caption": "", "figure_data": ""}, {"figure_label": "17", "figure_type": "table", "figure_id": "tab_25", "figure_caption": "", "figure_data": ""}, {"figure_label": "18", "figure_type": "table", "figure_id": "tab_27", "figure_caption": "", "figure_data": ""}, {"figure_label": "19", "figure_type": "table", "figure_id": "tab_29", "figure_caption": "", "figure_data": ""}, {"figure_label": "20", "figure_type": "table", "figure_id": "tab_31", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_33", "figure_caption": "", "figure_data": ""}, {"figure_label": "22", "figure_type": "table", "figure_id": "tab_35", "figure_caption": "Accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Round Trip Alignment (Part II).", "figure_data": ""}, {"figure_label": "23", "figure_type": "table", "figure_id": "tab_37", "figure_caption": "Perplexity of all languages covered by Glot500-m (Part I).", "figure_data": ""}, {"figure_label": "24", "figure_type": "table", "figure_id": "tab_39", "figure_caption": "Perplexity of all languages covered by Glot500-m (Part II).", "figure_data": ""}, {"figure_label": "25", "figure_type": "table", "figure_id": "tab_41", "figure_caption": "Perplexity of all languages covered by Glot500-m (Part III). A1. Did you describe the limitations of your work? section 'Limitation' A2. Did you discuss any potential risks of your work? section 'Ethics Statement' A3. Do the abstract and introduction summarize the paper's main claims? B2. Did you discuss the license or terms for use and / or distribution of any artifacts?", "figure_data": "1115"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_42", "figure_caption": "B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? section 3.1, appendix a, appendix c B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? section 4.2 C2. Did you discuss the experimental setup, including hyperparameter search and best-found", "figure_data": "hyperparameter values?section 5section 5C Did you run computational experiments?section 4.2"}], "formulas": [{"formula_id": "formula_0", "formula_text": "XLM-R-B XLM-R-L Glot500-m Model Size 278M 560M 395M Vocab Size 250K 250K 401K Transformer Size 86M 303M 86M", "formula_coordinates": [4.0, 318.81, 68.36, 192.96, 47.57]}, {"formula_id": "formula_1", "formula_text": "family | | | | XLM-R-B", "formula_coordinates": [9.0, 83.66, 68.36, 126.28, 17.92]}, {"formula_id": "formula_2", "formula_text": "PP ( , ) = =1 1 P \u210e | \u210e \u22121 1 (1)", "formula_coordinates": [17.0, 102.18, 127.76, 187.68, 37.89]}, {"formula_id": "formula_3", "formula_text": "P \u210e | \u210e \u22121 1", "formula_coordinates": [17.0, 100.57, 172.06, 60.71, 20.96]}, {"formula_id": "formula_4", "formula_text": "P \u210e | \u210e \u22121 1 = \u210e \u22121 1 \u210e \u210e \u22121 1 (2)", "formula_coordinates": [17.0, 110.06, 224.16, 179.81, 34.49]}, {"formula_id": "formula_5", "formula_text": "D , = max PP ( , ), PP ( , )(3)", "formula_coordinates": [17.0, 77.25, 545.61, 212.62, 21.19]}, {"formula_id": "formula_6", "formula_text": ": 1 \u2192 2 \u2192 3 \u2192 4 \u2192 5 \u2192 6 : 1 \u2192 2 \u2192 7 \u2192 8", "formula_coordinates": [17.0, 347.04, 257.57, 153.19, 38.35]}, {"formula_id": "formula_7", "formula_text": "Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L Glot500-m Language-Script XLM-R-B XLM-R-L", "formula_coordinates": [27.0, 75.9, 281.86, 408.8, 8.45]}], "doi": "10.18653/v1/2022.naacl-main.223"}