{"title": "Bidirectional Search That Is Guaranteed to Meet in the Middle: Extended Version", "authors": "Robert C Holte; Ariel Felner; Guni Sharon; Nathan R Sturtevant", "pub_date": "", "abstract": "We present MM, the first bidirectional heuristic search algorithm whose forward and backward searches are guaranteed to \"meet in the middle\", i.e. never expand a node beyond the solution midpoint. We also present a novel framework for comparing MM, A*, and brute-force search, and identify conditions favoring each algorithm. Finally, we present experimental results that support our theoretical analysis.", "sections": [{"heading": "Introduction", "text": "Bidirectional search algorithms interleave two separate searches, a normal search forward from the start state, and a search backward (i.e. using reverse operators) from the goal. Barker and Korf (2015)'s comparison of unidirectional heuristic search (Uni-HS, e.g. A*), bidirectional heuristic search (Bi-HS), and bidirectional brute-force search (Bi-BS) has two main conclusions (for caveats, see their Section 3):\nBK1: Uni-HS will expand fewer nodes than Bi-HS if more than half of the nodes expanded by Uni-HS have g \u2264 C * /2, where C * is the optimal solution cost.\nBK2: If fewer than half of the nodes expanded by Uni-HS using heuristic h have g \u2264 C * /2, then adding h to Bi-BS will not decrease the number of nodes it expands.\nA central assumption made by Barker and Korf is that the forward and backward searches comprising the bidirectional search never expand a node whose g-value (in the given direction) exceeds C * /2. We say that a bidirectional search \"meets in the middle\" if it has this property. This assumption raises a difficulty in applying their theory, because no known Bi-HS algorithm is guaranteed to meet in the middle under all circumstances (see Section 2). For example, in Barker and Korf's Towers of Hanoi experiment BS* (Kwa 1989) often expanded nodes at depth 13 in each direction even though the solution lengths C * were at most 16.\nTo remedy this we present a new front-to-end Bi-HS algorithm, MM, that is guaranteed to meet in the middle. MM 0 is the brute-force (h(s) = 0 \u2200s) version of MM. We also present a new framework for comparing MM 0 , unidirectional brute-force search (Uni-BS), MM, and A* that allows a precise characterization of the regions of the state space that will be expanded by one method but not another. We use this to identify conditions under which one method will expand fewer nodes than another, and conditions guaranteeing BK1's correctness. We also show that, unlike unidirectional search, adding a non-zero heuristic ( = 0 for every non-goal node) to Bi-BS can cause it to expand more nodes. For example, MM expands 4 times more nodes than MM 0 in one of our Pancake Puzzle experiments. Overall, our experiments on the Pancake Puzzle and Rubik's Cube show that the algorithm expanding the fewest nodes could be any one of Uni-HS, MM 0 or MM, depending on the heuristic used.\nAlthough we introduce a new algorithm (MM), we do not present an experimental comparison of MM to existing Bi-HS algorithms, and we do not claim that MM 0 and MM are the best bidirectional search algorithms in terms of minimizing run time or the number of nodes expanded. These issues are outside the scope of this paper. Like the Barker and Korf paper, this paper is theoretical. MM's significance is that it is the only Bi-HS algorithm to which our analysis, and Barker and Korf's, applies. These theories give strong justification for bidirectional search algorithms that meet in the middle. As the first of its breed, MM represents a new direction for developing highly competitive Bi-HS algorithms. A thorough empirical evaluation of MM is an important study that we will undertake in the future.\nThis technical report is an extended version of a AAAI'2016 paper by the same authors entitled \"Bidirectional Search That Is Guaranteed to Meet in the Middle\". The main additional material is the detailed proofs of all the claims in the AAAI paper.", "publication_ref": ["b3", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion of Previous Work", "text": "MM is the very first bidirectional heuristic search algorithm that is guaranteed to meet in the middle. Papers on traditional front-to-front 1 bidirectional heuristic search typi- 1 In bidirectional heuristic search, there are two different ways to define the heuristic function (Kaindl and Kainz 1997). A \"frontto-end\" heuristic-hF (n) for the forward search and hB(n) for the backward search-directly estimates the distance from node n to the target of the search (the target for the forward search is the goal, the target for the backward search is the start state). By contrast a \"front-to-front\" heuristic estimates the distance from n to the search target indirectly. For forward search it is defined as hF (n) = min m\u2208OpenB {h(n, m) + gB(m)} where OpenB is the backward search's open list, h(n, m) is a function estimating the distance between any two nodes, and gB(m) is the g-value of node m in the backward search. A front-to-front heuristic for the cally claim their searches meet in the middle, but none of them has a theorem to this effect (Arefin and Saha 2010;de Champeaux and Sint 1977;de Champeaux 1983;Davis, Pollack, and Sudkamp 1984;Eckerle 1994;Politowski and Pohl 1984). Perimeter-style front-to-front bidirectional searches with a fixed perimeter size (Dillenburg and Nelson 1994;Kaindl and Kainz 1997;Linares L\u00f3pez and Junghanns 2002;Manzini 1995) will only meet in the middle if the middle is known a priori and the perimeter size is chosen accordingly. Wilt and Ruml (2013) describe a perimeter search in which the perimeter size can increase as search progresses, but their aim is to minimize the total number of nodes expanded, not to have the searches meet in the middle.\nThe only theorem we have found asserting that a bidirectional search meets in the middle is Theorem 5 in an unpublished working paper by Mahanti et al. (2011), but no proof is given and the subsequent publication (Sadhukhan 2012) based on the ideas in this working paper does not include any such theorem.\nThe only previous paper on front-to-end bidirectional heuristic search with the explicit goal of having the searches meet in the middle describes an algorithm called 2PBS* (Pulido, Mandow, and P\u00e9rez de la Cruz 2012). Building on the idea of Kaindl et al. (1999), 2PBS* proceeds in two phases. In the first, a normal front-to-end bidirectional search is executed, but if a state is generated in both directions it is removed from both Open lists and put into a special \"frontier\" list. This phase ends when one of the Open lists is empty. The second phase then conducts a normal unidirectional heuristic search from the states in the frontier list in order to find an optimal path and prove its optimality. There is no theoretical or experimental evidence given for the two searches in the first phase meeting in the middle and it would fail to do so on the example in Figure 1.\nStandard front-to-end bidirectional heuristic searches use a variety of rules to decide which direction to expand next. The simplest (Ikeda et al. 1994;Sadhukhan 2012) is to strictly alternate between the directions so that the number of nodes expanded in the two directions is the same (within one). More commonly, the search direction is chosen based on Pohl's \"cardinality\" criterion, which expands a node in the direction whose Open list is smaller (Auer and Kaindl 2004;Kaindl and Khorsand 1994;Kwa 1989; backward search is defined analogously.  (Pohl 1969). All edges cost 1, h(n) = 0 for all nodes.  Pohl 1969). Barker and Korf (2012) use a variation of this idea, expanding an entire f -level in one direction and then choosing the search direction that expanded fewer nodes the most recent time it was used. Pulido et al. (2012) use the cardinality criterion until the two searches first meet, at which point the search direction is whichever has the larger minimum f -value in its Open list. All of these methods will fail to meet in the middle (node n) on the graph in Figure 1. Auer and Kaindl (2004)'s BiMax-BS * F method alternates search direction after completing an entire f -level in one direction. This will meet in the middle when h(n) = 0 for all nodes, but it will not do so when nodes with large g-values can have relatively small f -values.", "publication_ref": ["b13", "b0", "b5", "b6", "b4", "b8", "b26", "b7", "b13", "b19", "b21", "b31", "b20", "b30", "b27", "b15", "b12", "b30", "b1", "b14", "b18", "b25", "b25", "b2", "b27", "b1"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "Definitions and terminology", "text": "A problem instance is a pair (start, goal) of states in a statespace in which all edge weights are non-negative. The aim of search is to find a least-cost path from start to goal. d (u, v) is the distance (cost of a least-cost path) from state u to state v. C * = d(start, goal) is the cost of an optimal solution.\nWe use the usual notation-f, g, Open, etc.-and use gmin and f min for the minimum gand f -value on Open. We have separate copies of these variables for the two search directions, with a subscript (F or B) indicating the direction:\nForward search: f F , g F , h F , Open F , Closed F , etc. Backward search: f B , g B , h B , Open B , Closed B , etc.\nWe assume that each search direction uses an admissible front-to-end heuristic.\nWe say state s is \"near to goal\" if d(s, goal) \u2264 C * /2, and \"far from goal\" otherwise. For start, we make a 3-way distinction: s is \"near to start\" if d(start, s) \u2264 C * /2, \"far from start\" if C * /2 < d(start, s) \u2264 C * , and \"remote\" if d(start, s) > C * . These categories divide the state-space into 6 disjoint regions shown in Figure 2. We denote these regions by two letter acronyms. The first letter indicates the distance from start (N=near, F=far, R=remote) and the second letter indicates the distance from goal (N=near, F=far). For example, FN is the set of states that are far from start and near to goal. NN includes only those states at the exact midpoint of optimal solutions. None of the search algorithms in this paper expands a state in RF.\nIn Sections 6-9 we compare MM 0 , MM, Uni-BS, and A* based mainly on the nodes they expand in each region. A region's name denotes both the set of states and the number of states in the region. We will use the names in equations and inequalities. An inequality involving two algorithms, e.g. A* < MM, indicates that one algorithm (A* in this example) expands fewer nodes than the other.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Nodes vs. States", "text": "States and nodes are different kinds of entities. A state is an immutable element of a state-space, with a fixed distance to start and goal. A node, by contrast, is a mutable entity, created and updated by a search algorithm, representing a path (or set of paths) in the state-space. At a minimum, node n stores the path's cost (g(n)) and the last state on the path, which we call the state associated with n.\nRarely, if ever, is there ambiguity about which term should be used in a given context. Nodes are expanded, not states, because the process of expansion requires a g-value and states do not have g-values, only nodes do. Similarly, the regions we define (NN, FF, etc.) are regions of a statespace-sets of states-because they are defined in terms of distances to start and goal and only states have such distances (nodes have g-values).\nHowever, there are a few situations where the correct wording would be awkward. For example, it is technically incorrect to write \"how many nodes are expanded in region FF?\" The correct way to say this is \"how many nodes are expanded whose associated state is in FF\". We prefer the simpler expression even though it is not technically correct. Using \"node\" in a context that requires \"state\" is harmless because there is a unique state associated with each node. On the other hand, property P4 (below) has the technically incorrect wording \"no state is expanded twice\". What is meant is that among all the nodes expanded, no two have the same associated state. Likewise, if we say a state s is open (or closed), we mean there is an open (or closed) node whose associated state is s.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MM: A Novel Bi-HS Algorithm", "text": "MM runs an A*-like search in both directions, except that MM orders nodes on the Open list in a novel way. The priority of node n on Open F , pr F (n), is defined to be:\npr F (n) = max(f F (n), 2g F (n)). pr B (n) is defined analogously. We use prmin F and prmin B for the minimum priority on Open F and Open B , respectively, and C = min(prmin F , prmin B ). On each iteration MM expands a node with priority C. U is the cost of the cheapest solution found so far. Initially infinite, U is updated whenever a better solution is found. MM stops when U \u2264 max(C, f min F , f min B , gmin F + gmin B + ) where is the cost of the cheapest edge in the state-space.\nEach of the last three terms inside the max is a lower bound on the cost of any solution that might be found by continuing to search. Therefore, if U is smaller than or equal to any of them, its optimality is guaranteed and MM can safely stop. It is safe to stop when U \u2264 C because C \u2264 C * until all optimal solutions have been found (Theorem 10 below). Therefore, U \u2264 C implies U = C * .\nMM has the following properties: Section 5 gives complete proofs that MM has these properties. Here we provide a sketch of the proof of P1, MM's\ndistinctive property. 2g(n) is larger than f (n) when g(n) > h(n). If n is remote or far from start (goal) this makes pr F (n) > C * (pr B (n) > C * ).\nIf m is near to start (goal) on any optimal path, pr F (m) \u2264 C * (pr B (m) \u2264 C * ). Thus, an optimal solution will be found before MM expands any node that is remote or far from start and far from goal. The 2g(n) term also guarantees that a node that is remote or far from start (goal) but near to goal (start) will be expanded by MM's backward (forward) search (if it is expanded at all). The 2g(n) term in pr(n) therefore guarantees property P1.\nAlgorithm 1 gives pseudocode for MM. Lines 2-20 are the usual best-first search expansion cycle. Duplicate detection is in line 11. U is updated in line 18 and checked in line 4. Note that to determine if a better solution path has been found, MM only checks (line 17) if a newly generated node is in the Open list of the opposite search direction. That is all that is required by our proofs; it is not necessary to also check if a newly generated node is in the Closed list of the opposite search direction. As presented, only the cost of the optimal path is returned (line 5). It is straightforward to add code to get an actual solution path.\nWhen prmin F = prmin B any rule could be used to break the tie (e.g. Pohl (1969)'s cardinality criterion). However, to exploit the gmin F + gmin B + stopping condition, it is advantageous to continue to expand nodes in the same direction (in line 6 ties are broken in favor of forward search) until there is no longer a tie or gmin in that direction has increased, and to break ties among nodes with the same priority in the chosen search direction in favor of small g.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "MM 0", "text": "MM 0 is the brute-force version of MM, i.e. MM when h(n) = 0 \u2200n. Thus for MM 0 : pr F (n) = 2g F (n) and pr B (n) = 2g B (n). MM 0 is identical to Nicholson's (1966) algorithm except that Nicholson's stops when U \u2264 gmin F + gmin B , whereas MM 0 stops when U \u2264 gmin F + gmin B + .", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Proofs of MM's Properties", "text": "In this section we prove that MM has properties P1-P4. We assume that all edge weights are non-negative (zero-cost edges are allowed), that start = goal, and that the heuristic used by MM in each search direction is admissible. For our analysis in this section we will use the pseudocode in Algorithm 2, which differs from Algorithm 1 in two details.\n(1) stopping condition (line 4): For the moment, we will use a weak stopping condition: MM will terminate search as soon as U \u2264 C. This simplifies the proofs of MM's key properties. In Section 5.2 we will replace this stopping condition with the stronger stopping condition used in Algorithm 1 and show that MM maintains all its key properties when the stronger stopping condition is used.\n(2) tie-breaking (line 8): To emphasize that the proofs do not depend on how ties among nodes with minimum priority are broken, we have omitted the tie-breaking rule used in Algorithm 1.\nThe following definition and Lemmas 1-3 are very closely based on Lemma 1 and its proof (for A*) in (Hart, Nilsson, and Raphael 1968). Definition 1. Node n is \"permanently closed\" in the forward search direction if n \u2208 Closed F and g F (n) = d(start, n). Likewise, n is permanently closed in the backward search direction if n \u2208 Closed B and g B (n) = d(n, goal).\nThe name \"permanently closed\" is based on the following lemma.\nLemma 1. If node n is permanently closed in a particular search direction at the start of some iteration, it will be permanently closed in that direction at the start of all subsequent iterations. Proof. This proof is for the forward search, the proof for the backward search is analogous. There is no code in Algorithm 2 to directly change g F (n) while n \u2208 Closed F , so n can only stop being permanently closed by being removed from Closed F . This is possible (line 14) but only if a strictly cheaper path to n is found (line 11). This is not possible since g F (n) = d(start, n) for a node permanently closed in the forward direction. Therefore, once n is permanently closed in the forward direction it will remain so. Lemma 2. Let P = s 0 , s 1 , . . . s n be an optimal path from start (s 0 ) to any state s n . If s n is not permanently closed in the forward direction and either n = 0 or n > 0 and s n\u22121 is permanently closed in the forward direction, then s n \u2208 Open F and g F (s n ) = d(start, s n ). Analogously, let P = s 0 , s 1 , . . . s n be an optimal path from any state s 0 to goal = s n . If s 0 is not permanently closed in the backward direction and either n = 0 or n > 0 and s 1 is permanently closed in the backward direction, then s 0 \u2208 Open B and g B (s 0 ) = d(s 0 , goal). Proof. This proof is for the forward search, the proof for the backward search is analogous. If n = 0, s 0 = start has not been closed in the forward direction and the lemma is true because line 1 puts start \u2208 Open F with g F (start) = d(start, start) = 0. Suppose n > 0. When s n\u22121 was expanded to become permanently closed in the forward direction s n was generated via an optimal path (in lines 11 and 15,\ng F (n) + cost(n, c) = d(start, s n\u22121 ) + cost(s n\u22121 , s n ) = d(start, s n )\n). s n cannot have been permanently closed in the forward direction at that time because if it was, it still would be (Lemma 1). If s n \u2208 Closed F \u222a Open F at that time with a suboptimal g-value, then it would have been removed from Closed F \u222a Open F (line 14) and added to Open F (line 16) with g F = d(start, s n ). If s n / \u2208 Closed F \u222a Open F at that time, it would likewise have been added to Open F with g F = d(start, s n ) (line 16). Finally, if s n \u2208 Open F at that time with g F (s n ) = d(start, s n ) it would have remained so. Therefore, no matter what s n 's status was at the time s n\u22121 was expanded to become permanently closed in the forward direction, at the end of that iteration s n \u2208 Open F and g F (s n ) = d(start, s n ). In subsequent iterations g F (s n ) cannot have changed, since that only happens if a strictly cheaper path to s n is found (lines 11 and 12), which is impossible. It also cannot have been closed, since if that had happened it would now be permanently closed.\nLemma 3. Let P = s 0 , s 1 , . . . s n be an optimal path from start (s 0 ) to any state s n . If s n is not permanently closed in the forward direction then there exists an i (0 \u2264 i \u2264 n) such that s i \u2208 Open F and g F (s i ) = d(start, s i ). Define n F (for path P ) to be the smallest such i. Analogously, let P = s 0 , s 1 , . . . s n be an optimal path from any state s 0 to goal = s n . If s 0 is not permanently closed in the backward direction then there exists an i (0 \u2264 i \u2264 n) such that s i \u2208 Open B and g B (s i ) = d(s i , goal). Define n B (for path P ) to be the largest such i.\nProof. This proof is for the forward search, the proof for the backward search is analogous. If s 0 = start / \u2208 Closed F then i = 0 has the required properties (start \u2208 Open F and g F (start) = d(start, start) = 0, because of line 1). Suppose start \u2208 Closed F . Let j (0 \u2264 j < n) be the largest index such that s j is permanently closed. Such a j must exist because start (j = 0) is permanently closed. By Lemma 2 s j+1 \u2208 Open F and g(s j+1 ) = d(start, s j+1 ). Therefore i = j + 1 has the required properties.\nMuch of the following proof is closely based on Pearl's proof that A* always terminates on finite graphs (Section 3.1.2 (Pearl 1984)).\nLemma 4. For any finite state-space S with non-negative edge weights MM halts for any start and goal states in S. If there is no path from start to goal, MM returns \u221e.\nProof. If the condition in line 4 is satisfied on some iteration, MM will halt immediately. Suppose the condition in line 4 is never satisfied. Lines 11 and 12 ensure that MM never expands a node via the same path twice and, because there are no negative-cost cycles 2 (non-negative edge weights guarantee this), they also ensure that MM never expands a node via a path containing a cycle. In a finite space there are a finite number of acyclic paths to each state. Therefore each state can only be expanded a finite number of times in each search direction before it becomes permanently closed in that direction, and once it becomes permanently closed in a direction it remains so (Lemma 1). Since each iteration expands a node in one of the search directions, after a finite number of iterations MM will have permanently closed all the nodes reachable in one of the search directions, the Open list for that search direction will be empty, the condition in line 2 for continuing to iterate will not be satisfied, and MM will halt (line 21).\nIf there is no path from start to goal the condition in line 17 will never be satisfied, so U will always have its initial value of \u221e. If C becomes infinite-for example because all n \u2208 Open F have h F (n) = \u221e indicating that goal cannot be reached from them and all n \u2208 Open B have h B (n) = \u221e indicating that they cannot be reached from start-then the condition in line 4 will be satisfied and MM will return U = \u221e. If C never becomes infinite, we have shown in the previous paragraph that, after a finite number of iterations, the condition in line 2 for continuing to iterate will not be satisfied, and MM will return \u221e (line 21). Definition 2. If s = start and s \u2208 Open F \u222aClosed F at the start of iteration t with g F (s) = g then parent F ( s, t, g ) is defined to be the triple s , t , g such that on iteration t , s was added to Open F with g F (s) = g as a consequence of s being expanded in the forward direction with g F (s ) = g \u2212 cost(s , s). parent F ( start, t, g ) is undefined. Similarly, if s = goal and s \u2208 Open B \u222a Closed B at the start of iteration t with g B (s) = g then parent B ( s, t, g ) is defined to be the triple s , t , g such that on iteration t , s was added to Open B with g B (s) = g as a consequence of s being expanded in the backward direction with g B (s ) = g \u2212 cost(s, s ). parent B ( goal, t, g ) is undefined. Lemma 5. Suppose s = start and s \u2208 Open F \u222a Closed F at the start of iteration t with g F (s) = g. Then: (a) parent F ( s, t, g ) exists, (b) parent F ( s, t, g ) is unique, and (c) If parent F ( s, t, g ) = s , t , g then t < t. Likewise, suppose s = goal and s \u2208 Open B \u222a Closed B at the start of iteration t with g B (s) = g. Then: (a) parent B ( s, t, g ) exists, (b) parent B ( s, t, g ) is unique, and (c) If parent B ( s, t, g ) = s , t , g then t < t. Proof. This proof is for the forward direction, the proof for the backward direction is analogous.\n(a) If s = start, the only way it can be added to Open F is by having been generated by some other node being expanded, and the only way it can be added to Closed F is to have first been added to Open F .\n(b) If a state is added to Open F multiple times, it must be with a different g-value each time. Therefore s and g together uniquely identify the state (s ) that caused s to be added to Open F with g F (s) = g.\n(c) A state cannot be on Open F or Closed F with g F (s) = g until after it has been added to Open F with g F (s) = g. Lemma 6. Suppose s = start and s \u2208 Open F \u222a Closed F at the start of iteration t with g F (s) = d(start, s). If parent F ( s, t, g ) = s , t , g , then s is permanently closed in the forward direction. Likewise, Suppose s = goal and s \u2208 Open B \u222a Closed B at the start of iteration t with g B (s) = d(s, goal). If parent B ( s, t, g ) = s , t , g , then s is permanently closed in the backward direction.\nProof. This proof is for the forward direction, the proof for the backward direction is analogous. d(start, s) = g F (s) = g + cost(s , s) \u2265 d(start, s ) + cost(s , s) \u2265 d(start, s). Therefore all these terms are equal. In particular g + cost(s , s) = d(start, s ) + cost(s , s), i.e. g = d(start, s ). Hence, s became permanently closed on iteration t and will remain so for all future iterations (Lemma 1). Definition 3. If s = start and s \u2208 Open F \u222aClosed F at the start of iteration t with g F (s) = g then the forward generating path for s, t, g , GenP ath F ( s, t, g ), is defined recursively: GenP ath F ( start, t, g ) = \u2205 if s = start, GenP ath F ( s, t, g ) = GenP ath F (parent F ( s, t, g )) :: parent F ( s, t, g ), where X :: Y adds element Y to the end of a sequence X. Likewise, if s = goal and s \u2208 Open B \u222aClosed B at the start of iteration t with g B (s) = g then the backward generating path for s, t, g , GenP ath B ( s, t, g ), is defined analogously.\nThe forward (backward) generating path for s, t, g is well-defined because the recursion must terminate (t strictly decreases as each recursive call is made, and t cannot be negative) and it cannot terminate at any state other than start (goal for the backward direction) because parent F ( s, t, g ) (parent F ( s, t, g ) for the backward direction) exists for all the s, t, g generated in this sequence of recursive calls unless s = start. Proof. This proof is for the forward direction, the proof for the backward direction is analogous. By Lemma 6, if parent F ( s, t, g ) = s , t , g then s is permanently closed in the forward direction. The same lemma can therefore be applied to s , t , g to show that s is permanently closed, where s , t , g = parent F (s , t , g ). This process can be repeated backwards through the entire chain, showing that all states in GenP ath F ( s, t, g ) are permanently closed in the forward direction.\nDefinition 4. If P = s 0 , s 1 , . . . s n is an optimal path from start (s 0 ) to goal (s n ), let i be the largest index such that s k \u2208 Closed F \u2200k \u2208 [0, i \u2212 1]\n, and let j be the smallest index such that s k \u2208 Closed B \u2200k \u2208 [j + 1, n]. We say that P \"has not been found\" if i < j and that P \"has been found\" otherwise (i \u2265 j).\nLemma 8. Let P = s 0 , s 1 , . . . s n be an optimal path from start(s 0 ) to goal(s n ) that has not been found. Then n F and n B , as defined in Lemma 3, both exist for P . Moreover, n F = s i and n B = s j , where s i and s j are as defined in Definition 4.\nProof. Let i and j be as in Definition 4. For the forward search, s 0 , s 1 , . . . s i is an optimal path from start to s i and s i \u2208 Closed F and therefore is not permanently closed in the forward direction. Therefore, s 0 , s 1 , . . . s i satisfies the conditions of Lemma 3 for the forward direction and n F = s i exists for path s 0 , s 1 , . . . s i . Because s 0 , s 1 , . . . s i\u22121 are all in Closed F , it must be that i = i. Since i is the smallest index between 0 and i such that s i \u2208 Open F and g F (s i ) = d(start, s i ), it is also the smallest index between 0 and n with these properties, so s i is also n F for path P . For the backward search, the reasoning is analogous. s j , s j+1 , . . . s n is an optimal path from s j to goal and s j \u2208 Closed B and therefore is not permanently closed in the backward direction. Therefore, s j , s j+1 , . . . s n satisfies the conditions of Lemma 3 for the backward direction and n B = s j exists for path s j , s j+1 , . . . s n . Because s j+1 , s j+2 , . . . s n are all in Closed B , it must be that j = j. Since j is the largest index between j and n such that s j \u2208 Open B and g B (s j ) = d(s j , goal), it is also the largest index between 0 and n with these properties, so s j is also n B for path P . Lemma 9. If P = s 0 , s 1 , . . . s n is an optimal path from start(s 0 ) to goal(s n ) that has not been found, let n F = s i and n B = s j be as defined in Lemma 3. Then g F (n\nF ) + g B (n B ) \u2264 C * .\nProof. Lemma 8 guarantees that n F and n B exist for P . Because edge weights are non-negative and i < j, d(start, s i ) + d(s j , goal) \u2264 C * , the cost of the whole path P . The lemma follows because g F (n F ) = d(start, s i ) and g B (n B ) = d(s j , goal).\nTheorem 10. If, at the beginning of an MM iteration, there exists an optimal path P from start to goal that has not been found, then C \u2264 C * .\nProof. Let n F and n B on path P be as defined in Lemma 3. By Lemma 9, g F (n F ) + g B (n B ) \u2264 C * , and therefore at least one of g F (n F ) and g B (n B ) must be less than or equal to C * /2. Suppose, without loss of generality, that g F (n F ) \u2264 C * /2. Then pr F (n F ) \u2264 C * because f F (n F ) \u2264 C * (because the heuristic h F is admissible and g F (n) is optimal) and g F (n F ) \u2264 C * /2. Since C is the minimum priority of all the nodes in both Open lists and n F \u2208 Open F , C cannot be larger than pr F (n F ) and therefore C \u2264 C * . Lemma 11. If there exists a path from start to goal, MM will not terminate until at least one optimal path from start to goal has been found.\nProof. Lemma 8 guarantees that Open F and Open B are both non-empty as long as there is any optimal path from start to goal that has not been found, so the termination condition in Line 2 cannot be satisfied until all optimal paths from start to goal have been found. The only other termination condition is U \u2264 C (line 4). Assume (for the purpose of contradiction) that this termination condition is satisfied before any optimal path from start to goal has been found. Theorem 10 shows that C \u2264 C * until all optimal paths from start to goal have been found, so for U \u2264 C to hold if no optimal paths from start to goal have been found, U must be equal to C * . We will now show that U = C * implies an optimal path from start to goal has been found, contradicting our assumption, thereby proving the lemma. U is set in line 18. On the iteration in which U was set to C * , there must have been a child node generated, c that satisfied the conditions of line 17, i.e. c \u2208 Open F \u2229 Open B and g F (c)+g B (c) = C * . The latter implies g F (c) = d(start, c) and g B (c) = d(c, goal), i.e. c is on an optimal path from from start to goal with optimal g-values in both directions. This means Lemma 7 applies to c in both directions, i.e. that all the nodes on the forward and backward generating paths for c are permanently closed. The concatenation of these two paths, with c in between, is an optimal path from start to goal that was found on the iteration when U was set to C * .\nLemma 12. If there exists a path from start to goal, let P = s 0 , s 1 , . . . s n be the first optimal path from start(s 0 ) to goal(s n ) that is found during MM's execution, and let n F = s i and n B = s j be as defined in Lemma 3 at the beginning of the iteration on which P is found. Then during that iteration U will be set to C * in line 18.\nProof. Lemma 11 guarantees that P exists, and Lemma 8 guarantees that n F and n B exist for P at the beginning of the iteration on which it becomes found. One of them must be expanded on this iteration because P 's status will not change from \"not found\" to \"found\" if n F remains on Open F and n B remains on Open B . We will complete the proof assuming that n F is expanded. The proof in the case that n B is expanded is analogous. We will prove the following before proving the lemma: (a) When n F is expanded, n B will be generated as one of its children; (b) When the test in Line 11 is applied to n B (n B \u2208 Open F \u222aClosed F and g F (n B ) \u2264 g F (n F )+cost(n F , nB)) it will fail.\nProof of (a): Suppose n B is not generated as a child of n F when it is expanded in the forward direction. Then there must exist one or more nodes between them, i.e. P = start . . . n F t 1 . . . t k n B . . . goal(k \u2265 1). In order for P to be \"found\" at the end of this iteration, it must be the case that t i \u2208 Closed F \u22001 \u2264 i \u2264 k. Since the path start . . . n F t 1 is an optimal path from start to t 1 , t 1 \u2208 Closed F after being generated by n F means that it had previously been generated via a different optimal path, which implies an optimal path had previously been found from start to all the t i and, indeed, to n B . Combining this previously found optimal path from start to n B with the optimal path found by the backwards search from n B to goal creates an optimal path from start to goal that had been found prior to P . This contradicts the premise that P is the first optimal path found from start to goal.\nProof of (b): The path start . . . n F n B is optimal, i.e. g F (n F ) + cost(n F , n B ) = d(start, n B ). The test in line 11 can therefore only succeed if an optimal path from start to n B had previously been found, which contradicts the premise that P is the first optimal path found from start to goal.\nProof of the lemma: Because of (b), the test in line 17 succeeds because n B is a child of n F (by (a)) and n B \u2208 Open B by definition. Because of (b), g F (n B ) + g B (n B ) = d(start, n B ) + d(n B , goal) = C * , so U will be set to C * in line 18. Lemma 13. If there exists a path from start to goal and MM begins an iteration with C > C * it will terminate immediately (i.e. without expanding a node on this iteration) and return U = C * . Proof. By Theorem 10, C > C * implies that all optimal solutions have been found, which implies (Lemma 12) U = C * so the termination criterion in line 4 is satisfied (and it is tested before a node is expanded).\nThe following establishes MM's properties P1 and P2. Corollary 14. MM's forward search never expands a node n with f F (n) > C * or g F (n) > C * /2, and MM's backward search never expands a node n with f B (n\n) > C * or g B (n) > C * /2.\nProof. If there does not exist a path from start to goal, C * = \u221e and nothing can be strictly larger than C * . If there exists a path from start to goal, the proof for the forward search is as follows. The proof for the backward search is analogous. By Lemma 13, MM's forward search never expands a node when C > C * , so if n was expanded in the forward search pr F (n) \u2264 C * . Since pr F (n) = max(f F (n), 2 \u2022 g F (n)) this means both f F (n) and 2 \u2022 g F (n) are less than or equal to C * . Lemma 15. If there exists a path P from start to goal, Open F and Open B are never empty.\nProof. This is the proof for the forward direction. The proof for the backward direction is analogous. By Lemma 3, for Open F to be empty all states reachable from start must be permanently closed in the forward direction. This is impossible because goal is reachable from start but, as we will now show, it will never be closed in the forward direction. There are two cases to consider: (1) C * > 0 and (2) C * = 0. Both cases use the fact that for goal to be closed in the forward direction it would first have to be open in the forward direction. Suppose C * > 0. If goal \u2208 Open F , g F (goal) \u2265 dist(start, goal) = C * > C * /2 so, by Corollary 14, it would not be expanded (closed). Alternatively, suppose C * = 0. In this case, pr F (s) = 0 for all states s \u2208 P and therefore C = 0 on all iterations, so all the states x expanded in the forward direction will have g F (x) = 0. In particular, for goal to be expanded (closed) in the forward direction, it must first have been added to Open F with g F (goal) = 0. At the beginning of the iteration that added goal to Open F with g F (goal) = 0 as a result of expanding node s on path P , path P cannot yet have been \"found\", for it if had previously been found, U would be 0 and MM would not have executed that iteration because the stopping condition in line 4 would have been satisfied (U = C = 0). Since P has not yet been found, goal must still be on Open B with g B (goal) = 0 (line 1) so the test in line 17 will be satisfied and U will be set to 0 in line 18. MM will then terminate (line 4) at the beginning of the next iteration (because U = C = 0) without having expanded goal in the forward direction.\nThe following establishes MM's property P3.\nTheorem 16. If there exists a path from start to goal MM returns U = C * .\nProof. Lemma 15 has shown that MM will never terminate, if there is a path from start to goal, by Open F or Open B becoming empty, and MM cannot terminate if C < C * , because U cannot be smaller than C * . Therefore, MM is certain to reach an iteration where C \u2265 C * . If MM reaches an iteration where C > C * , Lemma 13 guarantees MM will return U = C * . The only reason it might not reach an iteration with C > C * is that it might terminate on an iteration with C = C * . If termination occurs on such an iteration then we have U \u2264 C = C * and therefore U = C * is returned.", "publication_ref": ["b8", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "MM With Consistent Heuristics", "text": "In this section we consider additional properties of MM if its heuristics are consistent.\nThe following trivial lemma will be used in the proofs of Lemmas 18 and 22.\nLemma 17. If a 1 > a 2 and b 1 > b 2 then max(a 1 , b 1 ) > max(a 2 , b 2 ). Proof. Suppose max(a 1 , b 1 ) = a 1 . Then a 1 \u2265 b 1 > b 2 .\nIn addition, a 1 > a 2 is a premise of the lemma. Together these imply a 1 > max(a 2 , b 2 ). Combining this with the symmetric argument when max(a 1 , b 1 ) = b 1 we have proven the lemma.\nLemma 18. If MM's heuristics are consistent and c is a child of n in the forward search then pr F (c) \u2265 pr F (n). Likewise if MM's heuristics are consistent and c is a child of n in the backward search then pr B (c) \u2265 pr B (n).\nProof. This proof is for the forward search, the proof for the backward search is analogous. f F (c) \u2265 f F (n) because the heuristic is consistent, and g F (c) \u2265 g F (n) because edge costs are non-negative. Therefore, by Lemma 17, pr\nF (c) = max(f F (c), 2 \u2022 g F (c)) \u2265 pr F (n) = max(f F (n), 2 \u2022 g F (n)).\nThe proof of Lemma 2 requires the ability to re-open closed nodes, and virtually all the results of the previous section depend on that lemma. With consistent heuristics we wish to remove the re-opening of closed nodes from the algorithm, so we now must re-prove the equivalent of Lemma 2 without re-opening closed nodes.\nLemma 19. If MM's heuristics are consistent then C never decreases from one iteration (n \u2212 1 \u2265 1) of MM to the next (n).\nProof. Let n \u2265 2 be any iteration that MM executed beyond line 5 in solving a given problem, let s n be the node chosen for expansion on iteration n, and X the search direction (forward or backward) used for expanding s n , i.e. on iteration n s n was moved from Open X to Closed X and its children added to Open X with no changes being made to the open and closed lists in the other direction. Finally, let C n = pr X (s n ) be MM's C value as set in line 3 on iteration n.\nIf s n = start and s n = goal then s n was added to Open X with priority pr X (s n ) on one or more previous iterations. Let p < n (p for \"parent\") be the iteration that mostly recently (prior to n) added s n to Open X with priority pr X (s n ). If p = n \u2212 1 then C n = pr X (s n ) \u2265 pr X (p) = pr X (s n\u22121 ) = C n\u22121 follows directly from Lemma 18. If p < n\u22121, then s n has been on Open X with its current pr Xvalue ever since iteration p + 1, so it has been available for expansion, but not selected, on all iterations from p + 1 up to and including n \u2212 1. In particular, it was on Open X with its current pr X -value in the most recent iteration n \u2212 1, where MM chose to expand a different node s n\u22121 in a possibly different direction Y , instead of expanding s n in direction X. Since MM chooses a node with the smallest priority on either open list\nC n = pr X (s n ) \u2265 C n\u22121 = pr Y (s n\u22121 ).\nNow consider start and goal. Before the first iteration begins Open F is initialized to contain start and Open B is initialized to contain goal. Because g F (start) = g B (goal) = 0, once these are expanded they will never be added to the open list in that direction again, since 0 is the shortest possible path to them. One of these was expanded on MM's first iteration (n = 1). Suppose it was start (analogous reasoning applies if goal was expanded on the first iteration). If goal was never expanded then our proof is complete since it plays no role in determining a C value for any of MM's iterations. If goal was first expanded in the backwards direction on the very next iteration (n = 2) then, because MM chooses the node with the smallest priority on either open list we must have C 2 = pr B (goal) \u2265 pr F (start) = C 1 . If goal was first expanded in the backwards direction on a subsequent iteration, n > 2, then similar reasoning to p < n \u2212 1 case (above) applies, as follows. goal has been on Open B with its current pr B value ever since the first iteration (n = 1), so it has been available for expansion, but not selected, on all iterations up to and including n \u2212 1. In particular, it was on Open B with its initial pr B value in the most recent iteration n \u2212 1, where MM chose to expand a different node s n\u22121 in a possibly different direction Y , instead of expanding goal in the backwards direction. Since MM chooses a node with the smallest priority on either open list\nC n = pr B (goal) \u2265 C n\u22121 = pr Y (s n\u22121 ).\nLemma 20. Suppose MM's heuristics are consistent. Let P = s 0 , s 1 , . . . s n be an optimal path from start (s 0 ) to any state s n . If s n is not permanently closed in the forward direction and either n = 0 or n > 0 and s n\u22121 is permanently closed in the forward direction, then s n / \u2208 Closed F with g F (s n ) > d(start, s n ) on any iteration. Analogously, let P = s 0 , s 1 , . . . s n be an optimal path from any state s 0 to goal = s n . If s 0 is not permanently closed in the backward direction and either n = 0 or n > 0 and s 1 is permanently closed in the backward direction, then s 0 / \u2208 Closed B and g B (s 0 ) > d(s 0 , goal) on any iteration.\nProof. This proof is for the forward search, the proof for the backward search is analogous. If n = 0, s 0 = start has not been closed in the forward direction and the lemma is true because Closed F is initially empty and remains so until start is closed in the forward direction.\nSuppose n > 0 and let t 1 be the iteration on which s n\u22121 was expanded to become permanently closed in the forward direction. The value of C = C t1 during that iteration was pr t1 F (s n\u22121 ). Because s n is a child of s n\u22121 , when it was generated on that iteration its priority pr t1 F (s n ) \u2265 pr t1 (s n\u22121 ) (by Lemma 18). If s n had been on Closed F with a suboptimal g F -value prior to iteration t 1 it must have been expanded on an earlier iteration t 0 < t 1 . The value of C = C t0 = pr t0 F (s n ) on that iteration, because s n had only been reached via a suboptimal path, would be strictly greater than pr t1 F (s n ). Hence C t0 > C t1 even though t 0 < t 1 , contradicting Lemma 19. So s n cannot have been on Closed F with a suboptimal g F -value prior to iteration t 1 . On iteration t 1 it was added to Open F (if it was not already there) with an optimal g F -value, so it will never subsequently be added to Open F with a suboptimal g F -value, hence it will never subsequently be on Closed F with a suboptimal g Fvalue.\nThe following is the equivalent of Lemma 2 when MM has consistent heuristics but does not have the ability to re-open closed nodes. Corollary 21. Suppose MM's heuristics are consistent and MM does not re-open closed nodes (\"\u222a Closed F \" is removed from lines 13 and 14). Let P = s 0 , s 1 , . . . s n be an optimal path from start (s 0 ) to any state s n . If s n is not permanently closed in the forward direction and either n = 0 or n > 0 and s n\u22121 is permanently closed in the forward direction, then s n \u2208 Open F and g F (s n ) = d(start, s n ). Analogously, let P = s 0 , s 1 , . . . s n be an optimal path from any state s 0 to goal = s n . If s 0 is not permanently closed in the backward direction and either n = 0 or n > 0 and s 1 is permanently closed in the backward direction, then s 0 \u2208 Open B and g B (s 0 ) = d(s 0 , goal). Proof. The proof of Lemma 2 applies directly since, by Lemma 20, there is no need to test if a newly generated node is on Closed with a suboptimal value.\nSince the proofs of the other lemmas and theorems in the previous section are all based on Lemma 2, because of Corollary 21 they continue to hold when MM has consistent heuristics but does not have the ability to re-open closed nodes. Lemma 22. If MM's heuristics are consistent and MM does not re-open closed nodes (\"\u222a Closed F \" is removed from lines 13 and 14), then when MM expands a node its g-value is optimal. Proof. This is the proof for nodes expanded in MM's forward search. The proof for its backward search is analogous. Suppose node n has just been added to Open F (line 16) with a suboptimal cost c, i.e. n \u2208 Open F with g F (n) = c > d(start, n). Let P be an optimal path from start to n. Since n \u2208 Closed F , P satisfies the conditions of Lemma 3 and there exists a node m = n F \u2208 Open F on P with g F (m) = d(start, m). To prove the lemma, all that we need to show is that m will be expanded before n, i.e. that pr F (m) < pr F (n). \n, i.e. h F (m) \u2264 d(m, n) + h F (n). This implies d(start, m) + h F (m) \u2264 d(start, m) + d(m, n) + h F (n) = d(start, n) + h F (n) < c + h F (n).\nThe following establishes MM's property P4.\nTheorem 23. Suppose MM's heuristics are consistent and MM does not re-open closed nodes (\"\u222a Closed F \" is removed from lines 13 and 14). Then if there exists a path from start to goal and , then MM never expands a state twice.\nProof. MM will not expand a state twice in the same search direction because Lemma 22 guarantees that the first time a state becomes closed it becomes permanently closed. The only remaining possibility for a state to be expanded twice is that it is expanded once in the forward direction and once in the backward direction. If there is such a state, n, by Corollary 14, g F (n) \u2264 C * /2 and g B (n) \u2264 C * /2. Because C * is finite and optimal, this implies g F (n) = C * /2 and g B (n) = C * /2, i.e. n is a state on an optimal solution path P and pr F (n) = pr B (n) = C * . Without loss of generality, suppose n is first expanded in the backward direction. C = pr B (n) = C * at the time of this expansion and it cannot decrease as search continues (Lemma 19). By the time n is about to be expanded in the forward direction path P has been found since at that point n has been added to both Open lists. Therefore U = C * , the cost of path P . Thus we have that U \u2264 C before n is expanded for the second time and therefore MM will terminate before expanding n for the second time. Therefore no state is expanded in both directions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Using A Stronger Stopping Condition", "text": "We will now show that MM maintains its four key properties (P1-P4) if it stops as soon as any of the following conditions is true:\n1. U \u2264 C (the stopping condition used above)\n2. U \u2264 f min F 3. U \u2264 f min B 4. U \u2264 gmin F + gmin B + i.e. U \u2264 max(C, f min F , f min B , gmin F + gmin B + ).\nP3 continues to hold because f min F , f min B , and gmin F + gmin B + are all lower bounds on the cost of any solution that might be found by continuing to search. The other properties continue to hold when MM uses the stronger stopping condition because with a stronger stopping condition MM will execute a subset of the iterations it executed with the stopping condition used to prove MM's properties. Since those properties were true of every iteration done with MM's original stopping condition, they are true of every iteration done with the stronger stopping condition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MM 0 compared to Uni-BS", "text": "We will now use the region-based framework introduced in Section 3 to analyze under what conditions one type of algorithm will expand more nodes than another. The analysis will be made on a region-by-region basis, since, as we will \ng F (n) < C * /2 or g B (n) < C * /2.\nsee, in all cases except Uni-HS vs. Uni-BS no algorithmtype is superior to any other in all regions. We will summarize these analyses with three general rules (GR1, GR2, and GR3). These are general expectations, not iron-clad guarantees. There are many factors in play in a given situation, some favoring one algorithm-type, some favoring another. It is the net sum of these factors that ultimately determines which algorithm-type outperforms another. Our general rules state what we expect will usually be the dominant forces.\nWe begin by analyzing the brute-force algorithms since this lays the foundation for the subsequent comparisons.\nUni-BS only expands nodes that are near to or far from start. We write this as the equation:\n(Eq. 1) Uni-BS = NF + NN + F N + F F F indicates that Uni-BS might not expand all the nodes that are far from start. For example, Uni-BS will usually not expand all nodes that are exactly distance C * from start. By contrast, Uni-BS must expand all nodes near to start.\nMM 0 only expands nodes that are near to start or to goal: (Eq. 2) MM 0 = N F + N N + FN + RN . N indicates that MM 0 might not expand all the nodes that are near to start or goal. For example, in unit-cost spaces when C * is even, MM 0 will not expand any node in NN because an optimal path will be known by the time all the nodes distance C * /2 \u2212 1 from start and goal have been expanded. The in the gmin F + gmin B + termination condition means that MM 0 can terminate before some nodes with g F (n) < C * /2 or g B (n) < C * /2 have been expanded. This is illustrated in Figure 3; the numbers in the nodes are discussed in Sections 7 and 9, they may be ignored for now. S i (G i ) is the layer of nodes at depth i in the tree rooted at start (goal). After MM 0 expands start and goal, A and S 1 will be in Open F , and C and G 1 will be in Open B , all with g = 1. Since ties are broken in favor of the forward direction, MM 0 will next expand A and S 1 , generating B and S 2 with g F = 2. It will then switch directions and expand C and G 1 in some order. As soon as C is expanded a solution costing U = 4 is found. Since gmin F + gmin B + 1 = 2 + 1 + 1 \u2265 U , MM 0 can stop. This may happen before some nodes in G 1 are expanded even though they are distance 1 from goal and C * /2 = 2.\nUni-BS expands more nodes than MM 0 iff (Eq. 1 > Eq. 2) (Eq. 3) NF + NN + F N + F F > N F + N N + FN + RN To identify the core differences between the algorithms, i.e. regions explored by one algorithm but not the other, we ig-Figure 4: State space in which NN is large. nore the difference between N and N and between F and F , which simplifies Eq. 3 to:\n(Eq. 4) FF > RN We have identified two conditions that guarantee FF > RN:\n(1) When C * = D, the diameter of the space, there are no remote states, by definition, so RN is empty.\n(2) When the number of states far from start is larger than the number of states near to goal, i.e. if FF + FN > FN + NN + RN, or equivalently, FF > RN + NN. We say a problem (start, goal) is bi-friendly if it has this property.\nA special case of bi-friendly problems occurs when the number of states distance d from start is the same as the number of states distance d from goal, for all d \u2264 C * . This occurs often in standard heuristic search testbeds, e.g. the Pancake Puzzle, Rubik's Cube, and the Sliding Tile Puzzle when the blank is in the same location type (e.g. a corner) in both start and goal. In such cases, a problem is bi-friendly if the number of states near to start is less than the number of states far from start, i.e. more than half the states at depths d \u2264 C * occur after the solution midpoint. This is similar to the condition in BK1 with h(s) = 0\u2200s. In many testbeds this occurs because the number of states distance d from any state continues to grow as d increases until d is well past D/2. For example, Rubik's Cube has D = 20 and the number of states at distance d only begins to decrease when d = 19 (Table 5.1, (Rokicki et al. 2013)).\nNon-core differences (NF, NN, FN) can sometimes cause large performance differences. The example in Figure 4 exploits the fact that Uni-BS always expands all nodes in NN but MM 0 sometimes does not. All edges cost 1. start and goal each have one neighbor (s and g respectively) that are roots of depth d binary trees that share leaves (the middle layer, which is NN). C * = 2d + 2 and all paths from start to goal are optimal. FF and RN are empty. The values on the figure's left may be ignored for now, they are used in Section 7. MM 0 expands all the nodes except those in the middle layer, for a total of 2 \u2022 2 d nodes expanded. Uni-BS will expand all the nodes except goal, for a total of 3 \u2022 2 d -1 nodes, 1.5 times as many as MM 0 . This ratio can be made arbitrarily large by increasing the branching factor of the trees.\nThe general rule based on the core differences is: GR1: FF and RN usually determine whether MM 0 will expand fewer nodes than Uni-BS (FF > RN) or more.\n7 MM 0 compared to A* A heuristic, h, splits each region into two parts, the states in the region that are pruned by h, and the states that are not pruned. For example, FNU is the unpruned part of FN. The set of states expanded by A* is therefore (modified Eq. 1):\n(Eq. 5) A* = NFU + NNU + FNU + FFU We first compare the first three terms to the corresponding terms in Eq. 2 for MM 0 and then compare FFU and RN . Region NF: We expect A* to expand many nodes in NF. These nodes have g F (n) \u2264 C * /2 so A* would prune them only if h F (n) > C * /2. One might expect MM 0 's N F to be larger than A*'s NFU because A* prunes NF with a heuristic. This underestimates the power of the gmin F +gmin B + termination condition, which can cause N F to be much smaller than NFU. In Figure 3, a number with a right-pointing arrow over it inside node n is h F (n). Not shown are h F (C) = 1 and h F (s) = 1 \u2200s \u2208 S 3 . Region NF contains start, A, S 1 and S 2 . The heuristic does no pruning in this region so these are all expanded by A*. MM 0 will not expand any node n with g F (n) = C * /2 (e.g. S 2 ) so N F is half the size of NFU. As a second example, on Rubik's Cube instances with C * = 20, MM 0 only expands nodes with g F (n) \u2264 9 because of this termination condition. Korf (1997)'s heuristic has a maximum value of 11, so A* with this heuristic will not prune any nodes in N F. In general, we do not expect A* to have a large advantage over MM 0 in NF unless its heuristic is very accurate. 3 Region NN: As discussed above, MM 0 might expand no nodes in NN (i.e. N N is empty). Nodes in NN have g F (n) = g B (n) = C * /2, so A*'s f (s) cannot exceed C * . Therefore, even with an extremely accurate heuristic, A* may do little pruning in NN. For example, the heuristic values shown on the left side of Figure 4 are consistent and \"almost perfect\" (Helmert and R\u00f6ger 2008) yet they produce no pruning at all. A* behaves exactly the same as Uni-BS and expands 1.5 times as many nodes as MM 0 . Region FN: We expect A* to expand far fewer nodes than MM 0 in FN. These nodes have g F (n) > C * /2 and, being relatively close to goal, we expect the heuristic values for these nodes to be very accurate. FFU vs RN : RN certainly can be much smaller than FFU. In Figure 3, RN (G 1 + G 2 ) is about the same size as FF (S 3 ), which is the same as FFU in this example. However, because MM 0 will not expand any nodes with g B (n) = C * /2 in this example, RN is half the size of RN (RN contains G 1 but not G 2 ), so MM 0 expands many fewer nodes in RN than A* does in FF. On the other hand, FFU will certainly be the same size as or smaller than RN with a sufficiently accurate heuristic. In the extreme case, when RN is empty, this requires a heuristic that prunes every node in FF. This is not impossible, since no optimal path passes through FF, but it does require an extremely accurate heuristic. Moreover, FF without any pruning can be much smaller than RN . Deleting S 3 from Figure 3 makes FF empty, while RN can be made arbitrarily large.\nThe general rule based on the core differences is: GR2: When FF > RN, A* will expand more nodes in FF than MM 0 expands in RN unless A*'s heuristic is very accurate.\n8 MM compared to A* Modifying Eq. 2, the equation for MM is:\n(Eq. 6) MM = N FU + N N U + FN B + RN B. B has the same meaning as U, but is based on h B , the heuristic of MM's backwards search. For example, FNB is the part of FN that is not pruned by h B . In general, FNB will be different than FNU, the part that is not pruned by h F , the heuristic used by A*. By definition, N FU \u2264 NFU and N N U \u2264 NN, so MM has an advantage over A* in NF and NN. Region FN: FNU is almost certainly smaller than FN B because in forward search, nodes in FN have g F (n) > C * /2 and h F is estimating a small distance (at most C * /2). By contrast, for the backwards search, nodes in FN have g B (n) \u2264 C * /2 and h B would need to accurately estimate a distance larger than C * /2 to prune them. So, A* has an advantage over MM's backward search in FN. FFU vs RNB: Not much pruning will usually occur during MM's backward search in RN because RN's g B -values are small and the distances being estimated by h B are large. However, if RN is much smaller than FF but h F is accurate enough to make FFU smaller than MM 0 's RN (see the discussion of FFU vs RN in Section 7), then we expect that h B will be accurate enough to do some pruning in RN. Thus, we expect FFU > RNB whenever RN is much smaller than FF.\nThe general rule based on this section's analysis is the same as GR2 with MM 0 replaced by MM.", "publication_ref": ["b29", "b16", "b9"], "figure_ref": ["fig_6", "fig_6", "fig_6", "fig_6"], "table_ref": []}, {"heading": "The Correctness of BK1", "text": "In our notation BK1 (page 1) is written as: FNU + FFU < NNU + NFU =\u21d2 Uni-HS < MM. Here FNU + FFU is the number of nodes expanded by Uni-HS beyond the solution midpoint, NNU + NFU is the number expanded at or before the solution midpoint.\nCombining Eqs. 5 and 6 gives the exact expression: (Eq. 7) Uni-HS < MM \u21d0\u21d2 NFU + NNU + FNU + FFU < N FU + N N U + FN B + RN B. Differences between Eq. 7 and BK1 represent situations in which BK1 will be incorrect. For example, BK1 ignores region RN, but it can be the decisive factor determining whether MM expands more nodes than Uni-HS.\nWe now show that if all of the following conditions hold, BK1's prediction is correct. Dropping the negligible terms from the equations above, BK1 becomes 0 < NFU =\u21d2 Uni-HS < MM i.e. BK1 predicts that Uni-HS < MM always holds under these conditions. The exact analysis simplifies to Uni-HS < MM \u21d0\u21d2 NFU < N FU + FN B + RN B. C2 says the difference between N FU and FN B + RN B is negligible, so this can be rewritten as Uni-HS < MM \u21d0\u21d2 NFU < N FU + N FU This inequality is C3, so BK1 is always correct under conditions C1-C3. C1-C3 hold in our experiment on the Pancake Puzzle with the GAP heuristic (see the GAP rows for A* and MM near the bottom of Table 1) and we conjecture they held in Barker and Korf's experiments. 9 MM 0 compared to MM: an anomaly If h 1 and h 2 are admissible heuristics and h 1 (s) > h 2 (s) for all non-goal nodes, then every node expanded by A* using h 1 will also be expanded by A* using h 2 (RESULT 6, p. 81 (Nilsson 1982)). In particular, A* with a non-zero heuristic cannot expand more distinct nodes than Uni-BS. 4 This is not necessarily true for MM or most Bi-HS algorithms. In Figure 3 the value in a node is its h-value in the direction indicated by the arrow. All nodes in layer S 3 (G 3 ) have h F (s) = 1 (h B (s) = 1). MM expands all the nodes in S 1 and G 1 because they have pr(s) = 3 while pr F (A) = pr B (C) = 4. MM might then expand any number of nodes in S 2 or G 2 since they too have pr(s) = 4. 5 By contrast, we saw (Section 6) that MM 0 could stop before expanding all the nodes in S 1 and G 1 and would never expand a node in S 2 or G 2 . Thus we see that MM 0 can expand strictly fewer nodes than MM with a consistent, non-zero heuristic.\nThis example mimics behavior we saw with the GAP-2 and GAP-3 heuristics in the Pancake puzzle experiments below. We believe it occurs commonly with heuristics that are very accurate near the goal but inaccurate elsewhere. This example reveals a fundamental dilemma for Bi-HS caused by a tension between its two main stopping conditions: S1: U \u2264 max(f min F , f min B ) S2: U \u2264 gmin F + gmin B + . To satisfy S1 as quickly as possible, a node with minimum f -value should be expanded, but to satisfy S2 as quickly as possible a node with minimum g-value should be expanded. These two node-selection rules will disagree if none of the nodes with the smallest f -value also have the smallest gvalue. Breaking ties among nodes with the same f -value in favor of large g-values also causes the two selection rules to make different choices. When the two selection rules disagree, a choice has to be made as to which stopping condition to favor. MM and all previous Bi-HS methods are hard-  coded to favor S1. Bi-BS methods must favor S2, since they have no heuristic. In situations like Figure 3, where S2 can be satisfied more quickly than S1, Bi-BS will outperform Bi-HS. Identifying conditions under which S2 is more quickly satisfied than S1 is an important direction for future research. For now, we offer the following conjecture: GR3: With an inaccurate heuristic, Bi-HS will expand more nodes than Bi-BS.", "publication_ref": ["b23"], "figure_ref": ["fig_6", "fig_6"], "table_ref": ["tab_1"]}, {"heading": "Experiments", "text": "The purpose of the experiments in this section is to verify the correctness our general rules (GR1-GR3). Since some rules refer to the sizes of certain regions, they could only be tested in domains small enough to be fully enumerated. Likewise, since some rules refer to a heuristic's relative accuracy, we used at least two heuristics of different accuracy in each domain. All heuristic used in these experiments were consistent, not just admissible. The two domains used in our study are the 10-Pancake Puzzle and Rubik's Cube. In both domains all problems are bi-friendly. Because GR1-GR3 make predictions about the number of nodes expanded, that is the only quantity we measure in our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "10-Pancake Puzzle", "text": "We ran MM 0 , MM, Uni-BS, and A* on 30 random instances for each possible value of C * (1 \u2264 C * \u2264 11). We used the GAP heuristic (Helmert 2010) and derived less accurate heuristics from it, referred to as GAP-X, by not counting the gaps involving any of the X smallest pancakes. For example, GAP-2 does not count the gaps involving pancakes 0 or 1. The trends reported below were similar for all values of C * . In addition, similar trends were obtained using a pattern database (PDB) based on 6-X pancakes. Table 1 shows the number of nodes expanded in each region for each algorithm using each heuristic for C * = 10. 6 Row \"Reg. Size\" shows the number of states in each region. Column \"Total\" is the total of all the columns to its right. The total for Region Size does not include region RF (it is not in the table because none of the algorithms expand nodes in RF).\nWe see that RN is small and FF is very large. Although we regard GAP-3 as a inaccurate heuristic it does prune almost all the nodes in FF. NF is identical in size to FN+RN because of the symmetry in this space. The asymmetry of MM 0 's expansions in NF and FN+RN is because, for C * = 10, MM 0 must expand all the nodes with g(s) = 4 in one direction but not the other. MM's expansions in these regions are much more balanced. A*'s total is largely determined by NF with the more accurate heuristics, but is determined by FF with the less accurate heuristics. The bold results show that depending on h the algorithm expanding the fewest nodes is A* (GAP), MM (GAP-1), or MM 0 (GAP-2, GAP-3).\nTo examine the effect of the 2g term in MM's definition of a node's priority, we ran an altered version of MM, called MM-2g, omitting the 2g term in the definition of pr(n), so node n's priority is the usual f (n). We also added code to prevent MM-2g from expanding the same node in both directions. Although not identical to any existing Bi-HS algorithm, we believe MM-2g's results are representative of bidirectional search algorithms that do not meet in the middle. For all of the heuristics, MM-2g expands many nodes in FF and many more nodes than MM in NF, NN, and FN. GR1, GR2, and GR3 are all confirmed by this experiment. GR1: For every instance for every value of C * , FF > RN and MM 0 expanded fewer nodes than Uni-BS.\nGR2: A* expands more and more nodes in FF as the heuristic becomes less accurate, while MM and MM 0 always expand less than half the nodes in RN. This trend holds for individual instances, not just for averages. On all 30 instances A* with the GAP heuristic does not expand more nodes in FF than MM 0 expands in RN, and the opposite is true for all instances when A* uses the other heuristics. MM is similar -with all heuristics MM expands fewer nodes in RN than A* does in FF on all instances.\nGR3: With the best heuristic, GAP, MM expands many fewer nodes than MM 0 . As the heuristic becomes less accurate, the difference between MM and MM 0 steadily diminishes and eventually (GAP-2) turns into a steadily growing advantage for MM 0 . This trend holds for individual instances too.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Rubik's Cube", "text": "We use two heuristics in this study: h 888 is the more accurate, using two 8-edge PDBs and the 8-corner PDB. h 1997 is the heuristic used to first optimally solve random instances of Rubik's Cube (Korf 1997). It is based on two 6-edge PDBs and the 8-corner PDB.\nThe Uni-HS algorithm is IDA* with the standard operator pruning rules for Rubik's Cube (Korf 1997  plementations of MM and MM 0 both use external memory. A full description of these implementations is outside of the scope of this paper, but both algorithms are based on delayed duplicate detection (Korf 2004). Our MM 0 implementation expands a full g-layer in one direction, and then removes duplicates and checks for a solution. As a result, it always expands the maximum number of states in a layer before completing. Our MM implementation has priority-based open-and closed-lists stored across two 500GB SSD disks. States with the same priority are found in the same file; before a set of states is expanded, duplicate detection against the closed list is performed. Then, solution detection is performed in parallel to expanding states in the file. Because we only check for solutions when expanding a file, there can be a significant delay between when the full solution is generated and detected. Improving this is an issue for future work. Because operator pruning is, in general, unsafe to use in conjunction with duplicate detection (Holte and Burch 2014), MM and MM 0 did no operator pruning.\nTable 2 shows the results on each of the ten standard test instances (Korf 1997). MM 0 expands fewer nodes than IDA* with h 1997 on all instances except for instance #2 where there was a very small gap. Due to tie-breaking within the last iteration of IDA*, the differences on instances #1 and #2 are not meaningful for either algorithm. This is consistent with GR2 because h 1997 is not especially accurate.\nWith h 1997 MM always expands fewer nodes than IDA*. In fact, MM with h 1997 expands fewer nodes than IDA* with the superior h 888 on instances #9 and #10. MM expands fewer nodes than MM 0 on the easier instances (d = 17) but more on the harder ones (d = 18). There are two possible explanations for this. The first is the anomaly phenomenon described in Section 9. A heuristic that is sufficiently accurate for MM to expand fewer nodes than MM 0 on easier instances might not be sufficiently accurate on harder instances. The second is related to the delayed duplicate (and solution) detection. If we performed solution detection earlier MM would have certainly improved. But earlier solution detection in MM 0 could also improve its performance. Future work will study termination conditions in external memory search. For instance, an in-memory version of MM expanded only 75M nodes on problem #1, while tie-breaking in the order of file expansion for external-memory MM can significantly worsen its performance. The IDA* code expands more nodes per second than MM, but for instances #3-#10 MM found solutions in less time than IDA*.\nh 888 is accurate enough (as is GAP on the Pancake puzzle) for IDA* to outperform the MM variants for the easier instances #1 (d = 16) but the MM variants expand fewer nodes on the harder instances because h 888 is not sufficiently accurate on them.", "publication_ref": ["b16", "b16", "b17", "b11", "b16"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Conclusions and future work", "text": "In this paper we introduced MM, the first Bi-HS algorithm guaranteed to meet in the middle. We also introduced a framework that divides the state-space into disjoint regions and allows a careful analysis of the behavior of the different algorithms in each of the regions. We studied the various types of algorithms and provided some general rules that were confirmed by our experiments.\nThis paper initiated this direction. Future work will continue as follows: (1) A deeper analysis on current and new MM variants may further deepen our knowledge in this issue. (2) A thorough experimental comparison should be done on more domains and with more bidirectional search algorithms.\n(3) Heuristics that are specifically designed for MM, i.e., that only return values larger than C * /2 are needed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Thanks to Joseph Barker for answering questions and providing extra data related to (Barker and Korf 2015) and to Sandra Zilles and Andr\u00e9 Grahl Pereira for suggesting improvements in the theoretical analysis of MM. Financial support for this research was in part provided by Canada's Natural Science and Engineering Research Council (NSERC) and by Israel Science Foundation (ISF) grant #417/13. Computational facilities for some of our experiments were provided by Compute Canada. This material is based upon work supported by the National Science Foundation under Grant No. 1551406.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A new approach of iterative deepening bi-directional heuristic front-to-front algorithm (IDBHFFA)", "journal": "International Journal of Electrical and Computer Sciences (IJECS-IJENS)", "year": "2010", "authors": "K S Arefin; A K Saha"}, {"ref_id": "b1", "title": "A case study of revisiting best-first vs. depth-first search", "journal": "", "year": "2004", "authors": "A Auer; H Kaindl"}, {"ref_id": "b2", "title": "Solving peg solitaire with bidirectional BFIDA*", "journal": "", "year": "2012", "authors": "J K Barker; R E Korf"}, {"ref_id": "b3", "title": "Limitations of frontto-end bidirectional heuristic search", "journal": "", "year": "2015", "authors": "J K Barker; R E Korf"}, {"ref_id": "b4", "title": "Towards a better understanding of bidirectional search", "journal": "", "year": "1984", "authors": "H W Davis; R B Pollack; T Sudkamp"}, {"ref_id": "b5", "title": "An improved bidirectional heuristic search algorithm", "journal": "J. ACM", "year": "1977", "authors": "D De Champeaux; L Sint"}, {"ref_id": "b6", "title": "Bidirectional heuristic search again", "journal": "J. ACM", "year": "1983", "authors": "D De Champeaux"}, {"ref_id": "b7", "title": "Perimeter search", "journal": "Artificial Intelligence", "year": "1994", "authors": "J F Dillenburg; P C Nelson"}, {"ref_id": "b8", "title": "A formal basis for the heuristic determination of minimum cost paths", "journal": "", "year": "1968", "authors": "J Eckerle; P E Hart; N J Nilsson; B Raphael"}, {"ref_id": "b9", "title": "How good is almost perfect?", "journal": "", "year": "2008", "authors": "M Helmert; G R\u00f6ger"}, {"ref_id": "b10", "title": "Landmark heuristics for the pancake problem", "journal": "", "year": "2010", "authors": "M Helmert"}, {"ref_id": "b11", "title": "Automatic move pruning for single-agent search", "journal": "AI Communications", "year": "2014", "authors": "R C Holte; N Burch"}, {"ref_id": "b12", "title": "A fast algorithm for finding better routes by AI search techniques", "journal": "", "year": "1994", "authors": "T Ikeda; M.-Y Hsu; H Imai; S Nishimura; H Shimoura; T Hashimoto; K Tenmoku; K Mitoh"}, {"ref_id": "b13", "title": "Bidirectional heuristic search reconsidered", "journal": "J. Artificial Intelligence Resesearch (JAIR)", "year": "1997", "authors": "H Kaindl; G Kainz"}, {"ref_id": "b14", "title": "Memory-bounded bidirectional search", "journal": "", "year": "1994", "authors": "H Kaindl; A Khorsand"}, {"ref_id": "b15", "title": "Switching from bidirectional to unidirectional search", "journal": "", "year": "1999", "authors": "H Kaindl; G Kainz; R Steiner; A Auer; K Radda"}, {"ref_id": "b16", "title": "Finding optimal solutions to Rubik's Cube using pattern databases", "journal": "", "year": "1997", "authors": "R E Korf"}, {"ref_id": "b17", "title": "Best-first frontier search with delayed duplicate detection", "journal": "", "year": "2004", "authors": "R E Korf"}, {"ref_id": "b18", "title": "BS*: An admissible bidirectional staged heuristic search algorithm", "journal": "Artificial Intelligence", "year": "1989", "authors": "J B H Kwa"}, {"ref_id": "b19", "title": "Perimeter search performance", "journal": "", "year": "2002", "authors": "C Linares L\u00f3pez; A Junghanns"}, {"ref_id": "b20", "title": "A tale of two searches: Bidirectional search algorithm that meets in the middle", "journal": "", "year": "2011", "authors": "A Mahanti; S K Sadhukan; S ; Ghosh;  Iim Calcutta"}, {"ref_id": "b21", "title": "BIDA: an improved perimeter search algorithm", "journal": "Artificial Intelligence", "year": "1995", "authors": "G Manzini"}, {"ref_id": "b22", "title": "Finding the shortest route between two points in a network", "journal": "The Computer Journal", "year": "1966", "authors": "T A J Nicholson"}, {"ref_id": "b23", "title": "Principles of Artificial Intelligence", "journal": "Springer", "year": "1982", "authors": "N J Nilsson"}, {"ref_id": "b24", "title": "Heuristics -Intelligent Search Strategies for Computer Problem Solving", "journal": "Addison-Wesley", "year": "1984", "authors": "J Pearl"}, {"ref_id": "b25", "title": "Bi-directional and heuristic search in path problems", "journal": "Technical Report", "year": "1969", "authors": "I Pohl"}, {"ref_id": "b26", "title": "D-node retargeting in bidirectional heuristic search", "journal": "", "year": "1984", "authors": "G Politowski; I Pohl"}, {"ref_id": "b27", "title": "A two-phase bidirectional heuristic search algorithm", "journal": "", "year": "2012", "authors": "F J Pulido; L Mandow; J Cruz"}, {"ref_id": "b28", "title": "Starting AI Researchers Symposium (STAIRS)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b29", "title": "The diameter of the Rubik's Cube group is twenty", "journal": "SIAM J. Discrete Math", "year": "2013", "authors": "T Rokicki; H Kociemba; M Davidson; J Dethridge"}, {"ref_id": "b30", "title": "A new approach to bidirectional heuristic search using error functions", "journal": "", "year": "2012", "authors": "S K Sadhukhan"}, {"ref_id": "b31", "title": "Robust bidirectional search via heuristic improvement", "journal": "", "year": "2013", "authors": "C M Wilt; W Ruml"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Figure 3.2 in(Pohl 1969). All edges cost 1, h(n) = 0 for all nodes.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Diagrammatic depiction of the different regions.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Algorithm 1 :1Pseudocode for MM 1 gF (start) := gB(goal) := 0; OpenF := {start}; OpenB := {goal}; U := \u221e 2 while (OpenF = \u2205) and (OpenB = \u2205) do 3 C := min(prminF , prminB) 4 if U \u2264 max(C, f minF , f minB, gminF + gminB + ) in the forward direction 8 choose n \u2208 OpenF for which prF (n) = prminF and gF (n) is minimum 9 move n from OpenF to ClosedF 10 for each child c of n do 11 if c \u2208 OpenF \u222a ClosedF and gF (c) \u2264 gF (n) + cost(n, c) then 12 continue 13 if c \u2208 OpenF \u222a ClosedF then 14 remove c from OpenF \u222a ClosedF 15 gF (c) := gF (n) + cost(n, c) 16 add c to OpenF 17 if c \u2208 OpenB then 18 U := min(U , gF (c) + gB(c)) 19 else 20 // Expand in the backward direction, analogously 21 return \u221e (P1) MM's forward and backward searches meet in the middle, i.e. neither search expands a node whose distance from the search's origin (g F (n) for forward search, g B (n) for backward search) is larger than C * /2 (Corollary 14 below). (P2) MM never expands a node whose f -value exceeds C * (Corollary 14 below). (P3) MM returns C * (Lemma 4 below if there is no path from start to goal, Theorem 16 if there is). (P4) If there exists a path from start to goal and MM's heuristics are consistent MM never expands a state twice (Theorem 23 below).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Algorithm 2 :C2Pseudocode for MM 1 gF (start) := gB(goal) := 0; OpenF := {start}; OpenB := {goal}; U := \u221e 2 while (OpenF = \u2205) and (OpenB = \u2205) do 3 in the forward direction 8 choose n \u2208 OpenF for which prF (n) = prminF 9 move n from OpenF to ClosedF 10 for each child c of n do 11 if c \u2208 OpenF \u222a ClosedF and gF (c) \u2264 gF (n) + cost(n, c) then 12 continue 13 if c \u2208 OpenF \u222a ClosedF then 14 remove c from OpenF \u222a ClosedF 15 gF (c) := gF (n) + cost(n, c) 16 add c to OpenF 17 if c \u2208 OpenB then 18 U := min(U , gF (c) + gB(c)) 19 else 20 // Expand in the backward direction, analogously 21 return \u221e", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Lemma 7 .7Suppose s = start and s \u2208 Open F \u222a Closed F at the start of iteration t with g F (s) = d(start, s). Then all the states in GenP ath F ( s, t, g ) are permanently closed in the forward direction. Likewise, suppose s = goal and s \u2208 Open B \u222a Closed B at the start of iteration t with g B (s) = d(start, s). Then all the states in GenP ath B ( s, t, g ) are permanently closed in the backward direction.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "By definition, pr F (m) = max(d(start, m)+h F (m), 2\u2022d(start, m)) and pr F (n) = max(c + h F (n), 2 \u2022 c). By Lemma 17, to show that pr F (m) < pr F (n) we only have to show that d(start, m)+h F (m) < c+h F (n) and that 2\u2022d(start, m) < 2 \u2022 c. The latter follows because edge weights are nonnegative, so d(start, m) \u2264 d(start, n) < c. The former follows because the heuristic h F is consistent", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 3 :3Figure 3: MM 0 need not expand all nodes with g F (n) < C * /2 or g B (n) < C * /2.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "(C1) NN, FNU, and FFU are all negligible in size compared to NFU (C2) FN B + RN B \u2248 N FU (C3) N FU > NFU/2.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "10 pancake results: average nodes expansions by region for instances with C", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "166M 260M 96.0M 18.7M 2 17 1.55B 1.00B 1.51B 1.01B 114M 3 17 1.55B 1.14B 8.13B 1.02B 676M 4 17 1.55B 0.96B 6.56B 387M 467M 5 18 2.88B 4.71B 29.7B 3.58B 2.49B 6 18 2.88B 4.84B 15.4B 3.51B 1.10B 7 18 2.88B 5.89B 41.6B 4.01B 3.16B 8 18 2.88B 4.84B 45.9B 3.67B 3.77B 9 18 2.88B 3.01B 58.4B 2.87B 5.13B 10 18 2.88B 4.25B 70.3B 3.29B 4.82B", "figure_data": "h1997h888# d MM0MM IDA*MM IDA*1 16 218M). Our im-"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Rubik's Cube results. M=million, B=billion.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Forward search: f F , g F , h F , Open F , Closed F , etc. Backward search: f B , g B , h B , Open B , Closed B , etc.", "formula_coordinates": [2.0, 319.5, 484.85, 222.61, 20.68]}, {"formula_id": "formula_1", "formula_text": "distinctive property. 2g(n) is larger than f (n) when g(n) > h(n). If n is remote or far from start (goal) this makes pr F (n) > C * (pr B (n) > C * ).", "formula_coordinates": [3.0, 319.5, 552.7, 238.5, 31.57]}, {"formula_id": "formula_2", "formula_text": "g F (n) + cost(n, c) = d(start, s n\u22121 ) + cost(s n\u22121 , s n ) = d(start, s n )", "formula_coordinates": [4.0, 319.5, 651.36, 238.5, 20.61]}, {"formula_id": "formula_3", "formula_text": "Definition 4. If P = s 0 , s 1 , . . . s n is an optimal path from start (s 0 ) to goal (s n ), let i be the largest index such that s k \u2208 Closed F \u2200k \u2208 [0, i \u2212 1]", "formula_coordinates": [6.0, 54.0, 580.03, 238.5, 31.64]}, {"formula_id": "formula_4", "formula_text": "F ) + g B (n B ) \u2264 C * .", "formula_coordinates": [6.0, 319.5, 313.27, 238.5, 20.61]}, {"formula_id": "formula_5", "formula_text": ") > C * or g B (n) > C * /2.", "formula_coordinates": [7.0, 319.5, 221.18, 238.5, 22.19]}, {"formula_id": "formula_6", "formula_text": "Lemma 17. If a 1 > a 2 and b 1 > b 2 then max(a 1 , b 1 ) > max(a 2 , b 2 ). Proof. Suppose max(a 1 , b 1 ) = a 1 . Then a 1 \u2265 b 1 > b 2 .", "formula_coordinates": [8.0, 54.0, 266.93, 238.5, 36.09]}, {"formula_id": "formula_7", "formula_text": "F (c) = max(f F (c), 2 \u2022 g F (c)) \u2265 pr F (n) = max(f F (n), 2 \u2022 g F (n)).", "formula_coordinates": [8.0, 54.0, 433.77, 238.5, 20.61]}, {"formula_id": "formula_8", "formula_text": "C n = pr X (s n ) \u2265 C n\u22121 = pr Y (s n\u22121 ).", "formula_coordinates": [8.0, 356.03, 177.71, 156.99, 9.65]}, {"formula_id": "formula_9", "formula_text": "C n = pr B (goal) \u2265 C n\u22121 = pr Y (s n\u22121 ).", "formula_coordinates": [8.0, 319.5, 474.5, 164.56, 9.65]}, {"formula_id": "formula_10", "formula_text": ", i.e. h F (m) \u2264 d(m, n) + h F (n). This implies d(start, m) + h F (m) \u2264 d(start, m) + d(m, n) + h F (n) = d(start, n) + h F (n) < c + h F (n).", "formula_coordinates": [9.0, 319.5, 57.16, 238.5, 42.53]}, {"formula_id": "formula_11", "formula_text": "2. U \u2264 f min F 3. U \u2264 f min B 4. U \u2264 gmin F + gmin B + i.e. U \u2264 max(C, f min F , f min B , gmin F + gmin B + ).", "formula_coordinates": [9.0, 317.01, 465.28, 238.38, 57.09]}, {"formula_id": "formula_12", "formula_text": "g F (n) < C * /2 or g B (n) < C * /2.", "formula_coordinates": [10.0, 54.0, 153.02, 238.5, 20.61]}], "doi": ""}