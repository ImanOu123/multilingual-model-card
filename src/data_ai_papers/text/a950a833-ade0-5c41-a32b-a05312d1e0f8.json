{"title": "Information-Theoretic Metric Learning", "authors": "Jason V Davis; Brian Kulis; Prateek Jain; Inderjit S Dhillon", "pub_date": "", "abstract": "In this paper, we present an information-theoretic approach to learning a Mahalanobis distance function. We formulate the problem as that of minimizing the differential relative entropy between two multivariate Gaussians under constraints on the distance function. We express this problem as a particular Bregman optimization problem-that of minimizing the LogDet divergence subject to linear constraints. Our resulting algorithm has several advantages over existing methods. First, our method can handle a wide variety of constraints and can optionally incorporate a prior on the distance function. Second, it is fast and scalable. Unlike most existing methods, no eigenvalue computations or semi-definite programming are required. We also present an online version and derive regret bounds for the resulting algorithm. Finally, we evaluate our method on a recent error reporting system for software called Clarify, in the context of metric learning for nearest neighbor classification, as well as on standard data sets.", "sections": [{"heading": "Introduction", "text": "Selecting an appropriate distance measure (or metric) is fundamental to many learning algorithms such as k-means, nearest neighbor searches, and others. However, choosing such a measure is highly problem-specific and ultimately dictates the success-or failure-of the learning algorithm. To this end, there have been several recent approaches that attempt to learn distance functions, e.g., (Weinberger et al., 2005;Xing et al., 2002;Globerson & Roweis, 2005;Shalev-Shwartz et al., 2004). These methods work by exploiting distance information that is intrinsically available in many learning settings. For example, in the problem of semi-supervised clustering, points are constrained to be ei- ther similar (i.e., the distance between them should be relatively small) or dissimilar (the distance should be larger). In information retrieval settings, constraints between pairs of distances can be gathered from click-through feedback. In fully supervised settings, constraints can be inferred so that points in the same class have smaller distances to each other than to points in different classes.\nWhile existing algorithms for metric learning have been shown to perform well across various learning tasks, each fails to satisfy some basic requirement. First, a metric learning algorithm should be sufficiently flexible to support the variety of constraints realized across different learning paradigms. Second, the algorithm must be able to learn a distance function that generalizes well to unseen test data. Finally, the algorithm should be fast and scalable.\nIn this paper, we propose a novel approach to learning a class of distance functions-namely, Mahalanobis distances-that have been shown to possess good generalization performance. The Mahalanobis distance generalizes the standard Euclidean distance by admitting arbitrary linear scalings and rotations of the feature space. We model the problem in an information-theoretic setting by leveraging the relationship between the multivariate Gaussian distribution and the set of Mahalanobis distances. We translate the problem of learning an optimal distance metric to that of learning the optimal Gaussian with respect to an entropic objective. In fact, a special case of our formulation can be viewed as a maximum entropy objective: maximize the differential entropy of a multivariate Gaussian subject to constraints on the associated Mahalanobis distance.\nOur formulation is quite general: we can accommodate a range of constraints, including similarity or dissimilarity constraints, and relations between pairs of distances. We can also incorporate prior information regarding the distance function itself. For some problems, standard Euclidean distance may work well. In others, the Mahalanobis distance using the inverse of the sample covariance may yield reasonable results. In such cases, our formulation finds a distance function that is 'closest' to an initial distance function while also satisfying the given constraints.\nWe show an interesting connection of our metric learning problem to a recently proposed low-rank kernel learning problem (Kulis et al., 2006). In the latter problem a lowrank kernel K is learned that satisfies a set of given distance constraints by minimizing the LogDet divergence to a given initial kernel K 0 . This allows our metric learning algorithm to be kernelized, resulting in an optimization over a larger class of non-linear distance functions. Algorithmically, the connection also implies that the problem can be solved efficiently: it was shown that the kernel learning problem can be optimized using an iterative optimization procedure with cost O(cd 2 ) per iteration, where c is the number of distance constraints, and d is the dimensionality of the data. In particular, this method does not require costly eigenvalue computations or semi-definite programming. We also present an online version of the algorithm and derive associated regret bounds.\nTo demonstrate our algorithm's ability to learn a distance function that generalizes well to unseen points, we compare it to existing state-of-the-art metric learning algorithms. We apply the algorithms to Clarify, a recently developed system that classifies software errors using machine learning (Ha et al., 2007). In this domain, we show that our algorithm effectively learns a metric for the problem of nearest neighbor software support. Furthermore, on standard UCI datasets, we show that our algorithm consistently equals or outperforms the best existing methods when used to learn a distance function for k-NN classification.", "publication_ref": ["b18", "b19", "b4", "b15", "b11", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Most of the existing work in metric learning relies on learning a Mahalanobis distance, which has been found to be a sufficiently powerful class of metrics that work on many real-world problems. Earlier work by (Xing et al., 2002) uses a semidefinite programming formulation under similarity and dissimilarity constraints. More recently, (Weinberger et al., 2005) formulate the metric learning problem in a large margin setting, with a focus on k-NN classification. They also formulate the problem as a semidefinite programming problem and consequently solve it using a combination of sub-gradient descent and alternating projections. (Globerson & Roweis, 2005) proceed to learn a metric in the fully supervised setting. Their formulation seeks to 'collapse classes' by constraining within class distances to be zero while maximizing the between class distances. While each of these algorithms was shown to yield excellent classification performance, their constraints do not generalize outside of their particular problem domains; in contrast, our approach allows arbitrary linear constraints on the Mahalanobis matrix. Furthermore, these algorithms all require eigenvalue decompositions, an operation that is cubic in the dimensionality of the data.\nOther notable work wherein the authors present methods for learning Mahalanobis metrics includes (Shalev-Shwartz et al., 2004) (online metric learning), Relevant Components Analysis (RCA) (Shental et al., 2002) (similar to discriminant analysis), locally-adaptive discriminative methods (Hastie & Tibshirani, 1996), and learning from relative comparisons (Schutz & Joachims, 2003).\nNon-Mahalanobis based metric learning methods have also been proposed, though these methods usually suffer from suboptimal performance, non-convexity, or computational complexity. Some example methods include neighborhood component analysis (NCA) (Goldberger et al., 2004) that learns a distance metric specifically for nearest-neighbor based classification; convolutional neural net based methods of (Chopra et al., 2005); and a general Riemannian metric learning method (Lebanon, 2006).", "publication_ref": ["b19", "b18", "b4", "b15", "b16", "b7", "b14", "b5", "b2", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Given a set of n points {x 1 , ..., x n } in R d , we seek a positive definite matrix A which parameterizes the (squared) Mahalanobis distance.\nd A (x i , x j ) = (x i \u2212 x j ) T A(x i \u2212 x j ). (3.1)\nWe assume that prior knowledge is known regarding interpoint distances. Consider relationships constraining the similarity or dissimilarity between pairs of points. Two points are similar if the Mahalanobis distance between them is smaller than a given upper bound, i.e., d A (x i , x j ) \u2264 u for a relatively small value of u. Similarly, two points are dissimilar if d A (x i , x j ) \u2265 \u2113 for sufficiently large \u2113. Such constraints are typically inputs for many semi-supervised learning problems, and can also be readily inferred in a classification setting where class labels are known for each instance: distances between points in the same class can be constrained as similar, and points in different classes can be constrained as dissimilar.\nGiven a set of interpoint distance constraints as described above, our problem is to learn a positive-definite matrix A that parameterizes the corresponding Mahalanobis distance (3.1). Typically, this learned distance function is used to improve the accuracy of a k-nearest neighbor classifier, or to incorporate semi-supervision into a distance-based clustering algorithm. In many settings, prior information about the Mahalanobis distance function itself is known. In settings where data is Gaussian, parameterizing the distance function by the inverse of the sample covariance may be appropriate. In other domains, squared Euclidean distance (i.e. the Mahalanobis distance corresponding to the identity matrix) may work well empirically. Thus, we regularize the Mahalanobis matrix A to be as close as possible to a given Mahalanobis distance function, parameterized by A 0 .\nWe now quantify the measure of \"closeness\" between A and A 0 via a natural information-theoretic approach. There exists a simple bijection (up to a scaling function) between the set of Mahalanobis distances and the set of equalmean multivariate Gaussian distributions (without loss of generality, we can assume the Gaussians have mean \u00b5).\nGiven a Mahalanobis distance parameterized by A, we express its corresponding multivariate Gaussian as p(x; A) =\n1 Z exp (\u2212 1 2 d A (x, \u00b5)),\nwhere Z is a normalizing constant and A \u22121 is the covariance of the distribution. Using this bijection, we measure the distance between two Mahalanobis distance functions parameterized by A 0 and A by the (differential) relative entropy between their corresponding multivariate Gaussians:\nKL(p(x; A o ) p(x; A)) = p(x; A 0 ) log p(x; A 0 ) p(x; A) dx.\n(3.\n2) The distance (3.2) provides a well-founded measure of \"closeness\" between two Mahalanobis distance functions and forms the basis of our problem given below.\nGiven pairs of similar points S and pairs of dissimilar points D, our distance metric learning problem is\nmin A KL(p(x; A 0 ) p(x; A)) subject to d A (x i , x j ) \u2264 u (i, j) \u2208 S, d A (x i , x j ) \u2265 \u2113 (i, j) \u2208 D. (3.3)\nIn the above formulation, we consider simple distance constraints for similar and dissimilar points; however, it is straightforward to incorporate other constraints. For example, (Schutz & Joachims, 2003) consider a formulation where the distance metric is learned subject to relative nearness constraints (as in, the distance between i and j is closer than the distance between i and k). Our approach can be easily adapted to handle this setting. In fact, it is possible to incorporate arbitrary linear constraints into our framework in a straightforward manner. For simplicity, we present the algorithm under the simple distance constraints given above.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm", "text": "In this section, we first show that our information-theoretic objective (3.3) can be expressed as a particular type of Bregman divergence, which allows us to adapt Bregman's method (Censor & Zenios, 1997) to solve the metric learning problem. We then show a surprising equivalence to a recently-proposed low-rank kernel learning problem (Kulis et al., 2006), allowing kernelization of the algorithm.", "publication_ref": ["b1", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Metric Learning as LogDet Optimization", "text": "The LogDet divergence is a Bregman matrix divergence generated by the convex function \u03c6(X) = \u2212 log det X defined over the cone of positive-definite matrices, and it equals (for n \u00d7 n matrices A, A 0 )\nD \u2113d (A, A 0 ) = tr(AA \u22121 0 ) \u2212 log det(AA \u22121 0 ) \u2212 n. (4.1)\nIt has been shown that the differential relative entropy between two multivariate Gaussians can be expressed as the convex combination of a Mahalanobis distance between mean vectors and the LogDet divergence between the covariance matrices (Davis & Dhillon, 2006). Assuming the means of the Gaussians to be the same, we have,\nKL(p(x; A 0 ) p(x; A)) = 1 2 D \u2113d (A \u22121 0 , A \u22121 ) = 1 2 D \u2113d (A, A 0 ), (4.2)\nwhere the second line follows from definition (4.1).\nThe LogDet divergence is also known as Stein's loss, having originated in the work of (James & Stein, 1961). It can be shown that Stein's loss is the unique scale invariant loss-function for which the uniform minimum variance unbiased estimator is also a minimum risk equivariant estimator (Lehmann & Casella, 2003). In the context of metric learning, the scale invariance implies that the divergence (4.1) remains invariant under any scaling of the feature space. The result can be further generalized to invariance under any invertible linear transformation S, since\nD \u2113d (S T AS, S T BS) = D \u2113d (A, B). (4.3)\nWe can exploit the equivalence in (4.2) to express the distance metric learning problem (3.3) as the following LogDet optimization problem:\nmin A 0 D \u2113d (A, A 0 ) s.t. tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2264 u (i, j) \u2208 S, tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2265 \u2113 (i, j) \u2208 D, (4.4)\nNote that the distance constraints on d A (x i , x j ) translate into the above linear constraints on A.\nIn some cases, there may not exist a feasible solution to (4.4). To prevent such a scenario from occurring, we incorporate slack variables into the formulation to guarantee the existence of a feasible A. Let c(i, j) denote the index of the (i, j)-th constraint, and let \u03be be a vector of slack variables, initialized to \u03be 0 (whose components equal u for similarity constraints and \u2113 for dissimilarity constraints). Then we can pose the following optimization problem:\nmin A 0,\u03be D \u2113d (A, A 0 ) + \u03b3 \u2022 D \u2113d (diag(\u03be), diag(\u03be 0 )) s. t. tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2264 \u03be c(i,j) (i, j) \u2208 S, tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2265 \u03be c(i,j) (i, j) \u2208 D, (4.5)\nAlgorithm 1 Information-theoretic metric learning Input: X: input d \u00d7 n matrix, S: set of similar pairs D: set of dissimilar pairs, u, \u2113: distance thresholds A0: input Mahalanobis matrix, \u03b3: slack parameter, c: constraint index function Output: A: output Mahalanobis matrix\n1. A \u2190 A0, \u03bbij \u2190 0 \u2200 i, j 2. \u03be c(i,j) \u2190 u for (i, j) \u2208 S; otherwise \u03be c(i,j) \u2190 \u2113 3. repeat 3.1. Pick a constraint (i, j) \u2208 S or (i, j) \u2208 D 3.2. p \u2190 (xi \u2212 xj) T A(xi \u2212 xj) 3.3. \u03b4 \u2190 1 if (i, j) \u2208 S, \u22121 otherwise 3.4. \u03b1 \u2190 min \u03bbij, \u03b4 2 1 p \u2212 \u03b3 \u03be c(i,j) 3.5. \u03b2 \u2190 \u03b4\u03b1/(1 \u2212 \u03b4\u03b1p) 3.6. \u03be c(i,j) \u2190 \u03b3\u03be c(i,j) /(\u03b3 + \u03b4\u03b1\u03be c(i,j) ) 3.7. \u03bbij \u2190 \u03bbij \u2212 \u03b1 3.8. A \u2190 A + \u03b2A(xi \u2212 xj)(xi \u2212 xj) T A 4. until convergence return A\nThe parameter \u03b3 controls the tradeoff between satisfying the constraints and minimizing D \u2113d (A, A 0 ).\nTo solve the optimization problem (4.5), we extend the methods from (Kulis et al., 2006). The optimization method which forms the basis for the algorithm repeatedly computes Bregman projections-that is, projections of the current solution onto a single constraint. This projection is performed via the update\nA t+1 = A t + \u03b2A t (x i \u2212 x j )(x i \u2212 x j ) T A t , (4.6)\nwhere x i and x j are the constrained data points, and \u03b2 is the projection parameter (Lagrange multiplier corresponding to the constraint) computed by the algorithm. Each constraint projection costs O(d 2 ), and so a single iteration of looping through all constraints costs O(cd 2 ). We stress that no eigen-decomposition is required in the algorithm. The resulting algorithm is given as Algorithm 1. The inputs to the algorithm are the starting Mahalanobis matrix A 0 , the constraint data, and the slack parameter \u03b3. If necessary, the projections can be computed efficiently over a factorization W of the Mahalanobis matrix, such that A = W T W .", "publication_ref": ["b3", "b9", "b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Connection to Low-Rank Kernel Learning", "text": "The kernel learning problem posed in (Kulis et al., 2006) seeks to optimize a kernel K subject to constraints on the pairwise distances between points within the kernel. This is quite similar to our proposed metric learning problem, where points between distances are constrained directly. In this section, we present an equivalence between the two models, thus allowing for kernelization of the metric learning problem. The kernel learning problem posed by (Kulis et al., 2006) seeks to optimize a kernel K subject to a set of linear constraints while minimizing the LogDet divergence to a specified kernel K 0 . Given X = [x 1 x 2 ... x n ], and the input n \u00d7 n kernel matrix K 0 = X T A 0 X, the optimization problem is:\nmin K D \u2113d (K, K 0 ) subject to K ii + K jj \u2212 2K ij \u2264 u (i, j) \u2208 S, K ii + K jj \u2212 2K ij \u2265 \u2113 (i, j) \u2208 D, K 0. (4.7)\nIn addition to being convex in the first argument, the LogDet divergence between two matrices is finite if and only if their range spaces are the same (Kulis et al., 2006). Thus, the learned matrix K can be written as a kernel X T AX, for some (d \u00d7 d) positive definite matrix A. The results below can be easily generalized to incorporate slack variables in (4.7).\nFirst we establish that the feasible solutions to (3.3) coincide with the feasible solutions to (4.7).\nLemma 1. Given that K = X T AX, A is feasible for (3.3) if and only if K is feasible for (4.7).\nProof. The expression K ii + K jj \u2212 2K ij can be written as (e i \u2212 e j ) T K(e i \u2212 e j ), or equivalently\n(x i \u2212 x j ) T A(x i \u2212 x j ) = d A (x i , x j ).\nThus, if we have a kernel matrix K satisfying constraints of the form\nK ii + K jj \u2212 2K ij \u2264 u or K ii + K jj \u2212 2K ij \u2265 \u2113, we equivalently have a matrix A satisfying d A (x i , x j ) \u2264 u or d A (x i , x j ) \u2265 \u2113.\nWe can now show an explicit relationship between the optimal solution to (3.3) and (4.7).\nTheorem 1. Let A * be the optimal solution to (3.3) and K * be the optimal solution to (4.7). Then K * = X T A * X.\nProof. We give a constructive proof for the theorem. The Bregman projection update for (4.4) is expressed as\nA t+1 = A t + \u03b2A t (x i \u2212 x j )(x i \u2212 x j ) T A t . (4.8)\nSimilary, the Bregman update for (4.7) is expressed as\nK t+1 = K t + \u03b2K t (e i \u2212 e j )(e i \u2212 e j ) T K t . (4.9)\nIt is straightforward to prove that the value of \u03b2 is the same for (4.9) and (4.8). We can inductively prove that at each iteration t, updates K t and A t satisfy K t = X T A t X. At the first step, K 0 = X T A 0 X, so the base case trivially holds. Now, assume that K t = X T A t X; by the Bregman projection update, K t+1 = X T A t X + \u03b2X T A t X(e i \u2212 e j )(e i \u2212 e j ) T X T A t X =\nX T A t X + \u03b2X T A t (x i \u2212 x j )(x i \u2212 x j ) T A t X = X T (A t + \u03b2A t (x i \u2212 x j )(x i \u2212 x j ) T A T t )X.\nComparing with (4.8), we see that K t+1 = X T A t+1 X.\nSince the method of Bregman projections converges to the optimal A * and K * of (3.3) and (4.7), respectively (Censor & Zenios, 1997), we have K * = X T A * X. Hence, the metric learning (3.3) and the kernel learning (4.7) problems are equivalent.\nWe have proven that the information-theoretic metric learning problem is related to a low-rank kernel learning problem. We can easily modify Algorithm 1 to optimize for K-this is necessary in order to kernelize the algorithm.\nAs input, the algorithm provides K 0 instead of A 0 , the value of p is computed as K ii + K jj \u2212 2K ij , the projection is performed using (4.9), and the output is K.", "publication_ref": ["b11", "b11", "b11", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Kernelizing the Algorithm", "text": "We now consider kernelizing our metric learning algorithm. In this section, we assume that A 0 = I; that is, the maximum entropy formulation that regularizes to the baseline Euclidean distance. Kernelizing for other choices of A 0 is possible, but not presented. If A 0 = I, the corresponding K 0 from the low-rank kernel learning problem is K 0 = X T X, the Gram matrix of the inputs. If instead of an explicit representation X of our data points, we have as input a kernel function \u03ba(x, y) = \u03c6(x) T \u03c6(y), along with the associated kernel matrix K 0 over the training points, a natural question to ask is whether we can evaluate the learned metric on new points in the kernel space. This requires the computation of\nd A \u03c6(x , \u03c6(y)) = \u03c6(x) \u2212 \u03c6(y) T A \u03c6(x) \u2212 \u03c6(y) = \u03c6(x) T A\u03c6(x) \u2212 2\u03c6(x) T A\u03c6(y) + \u03c6(y) T A\u03c6(y).\nWe now define a new kernel function\u03ba(x, y) = \u03c6(x) T A\u03c6(y). The ability to generalize to unseen data points reduces to the ability to compute\u03ba(x, y). Note that A represents an operator in a Hilbert space, and its size is equal to the dimensionality of \u03c6(x), which can potentially be infinite (if the original kernel function is the Gaussian kernel, for example).\nEven though we cannot explicitly compute A, it is still possible to compute\u03ba(x, y). As A 0 = I, we can recursively \"unroll\" the learned A matrix so that it is of the form\nA = I + i,j \u03c3 ij \u03c6(x i )\u03c6(x j ) T .\nThis follows by expanding Equation (4.6) down to I. The new kernel function is therefore computed as\n\u03ba(x, y) = \u03ba(x, y) + i,j \u03c3 ij \u03ba(x, x i )\u03ba(x j , y),\nand is a function of the original kernel function \u03ba and the \u03c3 ij coefficients. The \u03c3 ij coefficients may be updated while minimizing D \u2113d (K, K 0 ) without affecting the asymptotic running time of the algorithm; in other words, by optimizing the low-rank kernel learning problem (4.7) for K, we can obtain the necessary coefficients \u03c3 ij for evaluation of \u03ba(x, y). This leads to a method for finding the nearest neighbor of a new data point in the kernel space under the learned metric which can be performed in O(n 2 ) total time. Details are omitted due to lack of space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Online Metric Learning", "text": "In this section, we describe an online algorithm for metric learning and prove bounds on the total loss when using LogDet divergence as the regularizer.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "As in batch metric learning, the algorithm receives pairs of points; however, unlike in the batch scenario, the online algorithm receives the \"target\" distance for these points (as opposed to just upper and lower bounds on the target distance). Before receiving the target distance, the algorithm uses its current Mahalanobis matrix to predict the distance between the given pair of points at each step. This formulation is standard in many online regression settings (Kivinen & Warmuth, 1997).\nMore formally, assume the algorithm receives an instance (x t , y t , d t ) at time step t, and predictsd t = d At (x t , y t ) using the current model A t . The loss associated with this prediction is measured by l t (A t ) = (d t \u2212d t ) 2 , where d t is the \"true\" (or target) distance between x t and y t . After incurring the loss, the algorithm updates A t to the new model A t+1 and its total loss is given by t l t (A t ).\nIf there is no correlation between input points and their target distances, then a learning algorithm could incur unbounded loss. Hence, a reasonable goal is to compare the performance of an online learning algorithm with the best possible offline algorithm. Given a T -trial sequence S = {(x 1 , y 1 , d 1 ), (x 2 , y 2 , d 2 ), . . . , (x T , y T , d T )}, let the optimal offline solution be given by\nA * = argmin A 0 T t=1 l t (A).\nTypically, the goal of an online algorithm is to bound its total loss in comparison to the loss obtained by A * .\nA common approach for online learning is to solve the following regularized optimization problem at each time step (Kivinen & Warmuth, 1997):\nmin A 0 f (A) = Regularization Term D(A, A t ) +\u03b7 t Loss Term l t (A) , (5.1)\nwhere \u03b7 t is the learning rate at the t-th step, and D(A, A t ) is measures the divergence between the new Mahalanobis", "publication_ref": ["b10", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 Online Metric Learning (OML) Algorithm", "text": "Initialize: \u03b70 \u2190 1 8 , A0 \u2190 1 n I Prediction: Given (xt, yt), predictdt = dA t (xt, yt) . Update: Upon receiving \"true\" dt, update model as\nAt+1 \u2190 At \u2212 2\u03b7t(dt \u2212 dt)At(xt \u2212 yt)(xt \u2212 yt) T At 1 + 2\u03b7t(dt \u2212 dt)(xt \u2212 yt) T At(xt \u2212 yt) , where \u03b7t = \u03b70 ifdt \u2212 dt \u2265 0; otherwise, \u03b7t = min \u03b70, 1 2(dt\u2212dt) 1 (xt\u2212yt) T (I+(A \u22121 t \u2212I) \u22121 )(xt\u2212yt)\n.\nmatrix A and the current matrix A t . In (5.1), the regularization term favors Mahalanobis matrices that are \"close\" to the current model A t , representing a tendency towards conservativeness. On the other hand, the loss term is minimized when A is updated to exactly satisfy the target distance specified at the current time step. Hence, the loss term has a tendency to satisfy target distances for recent examples. The tradeoff between regularization and loss is handled by the learning rate \u03b7 t , which is a critical parameter for many online learning problems.\nAs in batch metric learning, we select D(A, A t ) = D \u2113d (A, A t ) as the regularizer for (5.1):\nA t+1 = argmin A D \u2113d (A, A t ) + \u03b7 t (d t \u2212d t ) 2 .\nTo our knowledge, no relative loss bounds have been proven for the above problem. In fact, we know of no existing loss bounds for any LogDet-based online algorithms (Tsuda et al., 2005). We present below a novel algorithm with guaranteed bound on the regret. Our algorithm uses gradient descent, but it adapts the learning rate according to the input data to maintain positive-definiteness of A.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm and Analysis", "text": "The online metric learning (OML) algorithm is shown as Algorithm 2. Note that (A \u22121 t \u2212 I) \u22121 can be obtained from (A \u22121 t\u22121 \u2212 I) \u22121 by using the Sherman-Morrison-Woodbury formula. Thus, the overall complexity of each iteration of the OML algorithm is O(n 2 ), which is optimal.\nWe now show that the total loss incurred by OML is bounded with respect to the minimum loss incurred by any batch learning algorithm. Let L OML = t l t (A t ) be the loss obtained by the online algorithm, and let L A * = t l t (A * ) be the loss obtained by the offline algorithm. Without loss of generality we can assume that the input data is scaled, so that x \u2212 y 2 \u2264 R 2 for fixed R and for all x and y. One of the key steps in guaranteeing an upper bound on the regret incurred by an online algorithm is to bound the loss incurred at each step in terms of the loss incurred by the optimal offline solution. Below, we state the result-the full proof can be found in (Jain et al., 2007).\nLemma 2 (Loss at one step).\na t (d t \u2212 d t ) 2 \u2212 b t (d * t \u2212 d t ) 2 \u2264 D \u2113d (A * , A t ) \u2212 D \u2113d (A * , A t+1 ),\nwhere A * is the optimal offline solution,\nd * t = d A * (x t , y t ), a t , b t are constants s.t. 0 \u2264 a t \u2264 \u03b7 t and b t = \u03b7t 1\u22122\u03b7tR 2 .\nA repeated use of Lemma 2 allows us to obtain a loss bound for the overall OML algorithm.\nTheorem 2 (Online Metric Learning Loss Bound).\nL OML \u2264 1 4\u03b7 min R 2 L A * + 1 \u03b7 min D \u2113d (A * , I),\nwhere L OML and L A * are the losses incurred by OML and the optimal batch algorithm, respectively; \u03b7 min = min t \u03b7 t .\nProof. Adding the loss bound given by Lemma 2 over all the trials from 1 \u2264 t \u2264 T , we obtain\nT t=1 \u03b7 t (d t \u2212 d t ) 2 \u2264 T t=1 \u03b7 t 1 \u2212 2\u03b7 t R 2 (d * t \u2212 d t ) 2 + D \u2113d (A * , I) \u2212 D \u2113d (A * , A T ).\nThus we can conclude that,\nL OML \u2264 1 4\u03b7 min R 2 L A * + 1 \u03b7 min D \u2113d (A * , I).\nNote that this bound depends on \u03b7 min , which in turn depends on the input data. If the data is scaled properly then generally \u03b7 t will not change much at any time step, an observation that has been verified empirically.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We compare our Information Theoretic Metric Learning algorithm (ITML) to existing methods across two applications: semi-supervised clustering and k-nearest neighbor (k-NN) classification.\nWe evaluate metric learning for k-NN classification via two-fold cross validation with k = 4. All results presented represent the average over 5 runs. Binomial confidence intervals are given at the 95% level.\nTo establish the lower and upper bounds of the right hand side of our constraints (\u2113 and u in problem (3.3)), we use (respectively) the 5 th and 95 th percentiles of the observed distribution of distances between pairs of points within the dataset. To determine the constrained point pairs, we randomly choose 20c 2 pairs, where c is the number of classes in the dataset. Pairs of points in the same class are constrained to be similar, and pairs with differing class labels are constrained to be dissimilar. Overall, we found the algorithm to be robust to these parameters. However, we did   b) that ITML-MaxEnt is the only algorithm to be optimal (within the 95% confidence intervals) across all datasets. ITML is also robust at learning metrics over higher dimensions. In (c), we see that the error rate for the Latex dataset stays relatively constant for ITML.\nfind that the variance between runs increased if the number of constraints used was too small (i.e., fewer than 10c 2 ). The slack variable parameter, \u03b3, is tuned using cross validation over the values {.01, .1, 1, 10}. Finally, the online algorithm is run for approximately 10 5 iterations.\nIn Figure 1(a), we compare ITML-MaxEnt (regularized to the identity matrix) and the online ITML algorithm against existing metric learning methods for k-NN classification.\nWe use the squared Euclidean distance, d(x, y) = (x \u2212 y) T (x \u2212 y) as a baseline method. We also use a Mahalanobis distance parameterized by the inverse of the sample covariance matrix. This method is equivalent to first performing a standard PCA whitening transform over the feature space and then computing distances using the squared Euclidean distance. We compare our method to two recently proposed algorithms: Maximally Collapsing Metric Learning (Globerson & Roweis, 2005 (Globerson & Roweis, 2005), we found the method of (Xing et al., 2002) to be very slow and inaccurate. Overall, ITML is the only algorithm to obtain the optimal error rate (within the specified 95% confidence intervals) across all datasets. For several datasets, the online version is competitive with the best metric learning algorithms. We also observed that the learning rate \u03b7 remained fairly constant, yielding relatively small regret bounds (Theorem 2).\nIn addition to our evaluations on standard UCI datasets, we also evaluate apply our algorithm to the recently proposed problem of nearest neighbor software support for the Clarify system (Ha et al., 2007). The basis of the Clarify system lies in the fact that modern software design promotes modularity and abstraction. When a program terminates abnormally, it is often unclear which component should be responsible for (or is capable of) providing an error report. The system works by monitoring a set of predefined program features (the datasets presented use function counts) during program runtime which are then used by a classifier in the event of abnormal program termination. Nearest neighbor searches are particularly relevant to this problem. Ideally, the neighbors returned should not only have the correct class label, but should also represent those with similar program configurations or program inputs. Such a matching can be a powerful tool to help users diagnose the root cause of their problem. The four datasets shown here are Latex (the document compiler, 9 classes), Mpg321 (an mp3 player, 4 classes), Foxpro (a database manager, 4 classes), and Iptables (a Linux kernel application, 5 classes).\nThe dimensionality of the Clarify dataset can be quite large. However, it was shown (Ha et al., 2007) that high classification accuracy can be obtained by using a relatively small subset of available features. Thus, for each dataset, we use a standard information gain feature selection test to obtain a reduced feature set of size 20. From this, we learn metrics for k-NN classification using the above described procedure. We also evaluate the method ITML-Inverse Covariance, which regularizes to the inverse covariance matrix. Results are given in Figure 1(b). The ITML-MaxEnt algorithm yields significant gains for the Latex benchmark. Note that for datasets where Euclidean distance performs better than using the inverse covariance metric, the ITML-MaxEnt algorithm that normalizes to the standard Euclidean distance yields higher accuracy than that regularized to the inverse covariance matrix (ITML-Inverse Covariance). In general, for the Mpg321, Foxpro, and Iptables datasets, learned metrics yield only marginal gains over the baseline Euclidean distance measure.\nFigure 1(c) shows the error rate for the Latex datasets with a varying number of features (the feature sets are again cho- In Table 1, we see that ITML generally learns metrics significantly faster than other metric learning algorithms. The implementations for MCML and LMNN were obtained from their respective authors. The timing tests were run on a dual processor 3.2 GHz Intel Xeon processor running Ubuntu Linux. Time given is in seconds and represents the average over 5 runs.\nFinally, we present some semi-supervised clustering results. Note that both MCML and LMNN are not amenable to optimization subject to pairwise distance constraints. Instead, we compare our method to the semi-supervised clustering algorithm HMRF-KMeans (Basu et al., 2004). We use a standard 2-fold cross validation approach for evaluating semi-supervised clustering results. Distances are constrained to be either similar or dissimilar, based on class values, and are drawn only from the training set. The entire dataset is then clustered into c clusters using k-means (where c is the number of classes) and error is computed using only the test set. Table 2 provides results for the baseline k-means error, as well as semi-supervised clustering results with 50 constraints.", "publication_ref": ["b4", "b4", "b19", "b6", "b6", "b0"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": ["tab_2"]}, {"heading": "", "text": "Acknowledgments: This research was supported by NSF grant CCF-0431257, NSF Career Award ACI-0093404, and NSF-ITR award IIS-0325116.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A Probabilistic Framework for Semi-Supervised Clustering", "journal": "", "year": "2004", "authors": "S Basu; M Bilenko; R J Mooney"}, {"ref_id": "b1", "title": "Parallel Optimization: Theory, Algorithms, and Applications", "journal": "Oxford University Press", "year": "1997", "authors": "Y Censor; S A Zenios"}, {"ref_id": "b2", "title": "Learning a Similarity Metric Discriminatively, with Application to Face Verification", "journal": "", "year": "2005", "authors": "S Chopra; R Hadsell; Y Lecun"}, {"ref_id": "b3", "title": "Differential Entropic Clustering of Multivariate Gaussians. Adv. in Neural Inf", "journal": "", "year": "2006", "authors": "J V Davis; I S Dhillon"}, {"ref_id": "b4", "title": "Metric Learning by Collapsing Classes. Adv. in Neural Inf. Proc. Sys. (NIPS)", "journal": "", "year": "2005", "authors": "A Globerson; S Roweis"}, {"ref_id": "b5", "title": "Neighbourhood Component Analysis. Adv. in Neural Inf", "journal": "", "year": "2004", "authors": "J Goldberger; S Roweis; G Hinton; R Salakhutdinov"}, {"ref_id": "b6", "title": "Improved Error Reporting for Software that Uses Black Box Components. Programming Language Design and Implementation", "journal": "", "year": "2007", "authors": "J Ha; C Rossbach; J Davis; I Roy; D Chen; H Ramadan; E Witchel"}, {"ref_id": "b7", "title": "Discriminant adaptive nearest neighbor classification. Pattern Analysis and Machine Intelligence", "journal": "", "year": "1996", "authors": "T Hastie; R Tibshirani"}, {"ref_id": "b8", "title": "Online linear regression using burg entropy", "journal": "", "year": "2007", "authors": "P Jain; B Kulis; I S Dhillon"}, {"ref_id": "b9", "title": "Estimation with quadratic loss", "journal": "Univ. of California Press", "year": "1961", "authors": "W James; C Stein"}, {"ref_id": "b10", "title": "Exponentiated gradient versus gradient descent for linear predictors", "journal": "Inf. Comput", "year": "1997", "authors": "J Kivinen; M K Warmuth"}, {"ref_id": "b11", "title": "Learning Low-rank Kernel Matrices. Int. Conf. on Machine Learning (ICML)", "journal": "", "year": "2006", "authors": "B Kulis; M Sustik; I S Dhillon"}, {"ref_id": "b12", "title": "Metric Learning for Text Documents. Pattern Analysis and Machine Intelligence", "journal": "", "year": "2006", "authors": "G Lebanon"}, {"ref_id": "b13", "title": "Theory of Point Estimation", "journal": "Springer", "year": "2003", "authors": "E L Lehmann; G Casella"}, {"ref_id": "b14", "title": "Learning a Distance Metric from Relative Comparisons. Adv. in Neural Inf", "journal": "", "year": "2003", "authors": "M Schutz; T Joachims"}, {"ref_id": "b15", "title": "Online and Batch Learning of Pseudo-Metrics", "journal": "", "year": "2004", "authors": "S Shalev-Shwartz; Y Singer; A Y Ng"}, {"ref_id": "b16", "title": "Adjustment learning and relevant component analysis", "journal": "", "year": "2002", "authors": "N Shental; T Hertz; D Weinshall; M Pavel"}, {"ref_id": "b17", "title": "Matrix exponentiated gradient updates of online learning and bregman projection", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "K Tsuda; G Raetsch; M K Warmuth"}, {"ref_id": "b18", "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification. Adv", "journal": "", "year": "2005", "authors": "K Q Weinberger; J Blitzer; L K Saul"}, {"ref_id": "b19", "title": "Distance metric learning with application to clustering with sideinformation", "journal": "", "year": "2002", "authors": "E P Xing; A Y Ng; M I Jordan; S Russell"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Appearing in Proceedings of the 24 th International Conference on Machine Learning, Corvallis, OR, 2007. Copyright 2007 by the author(s)/owner(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. Classification error rates for k-nearest neighbor classification via different learned metrics. We see in figures (a) and (b) that ITML-MaxEnt is the only algorithm to be optimal (within the 95% confidence intervals) across all datasets. ITML is also robust at learning metrics over higher dimensions. In (c), we see that the error rate for the Latex dataset stays relatively constant for ITML.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Training time (in seconds) for the results presented in Figure1(b). information gain criteria). We see here that ITML is surprisingly robust. Euclidean distance, MCML, and LMNN all achieve their best error rates for five dimensions. ITML, however, attains its lowest error rate of .15 at d = 20 dimensions.", "figure_data": "DatasetITML-MaxEnt MCML LMNNLatex0.051719.80.538Mpg3210.08080.4600.253Foxpro0.07930.1520.189Iptables0.1490.08384.19Table 2. Unsupervised k-means clustering error, along with semi-supervised clustering error with 50 constraints.DatasetUnsupervised ITML HMRF-KMeansIonosphere0.3140.1130.256Digits-3890.2260.1750.286sen using the"}], "formulas": [{"formula_id": "formula_0", "formula_text": "d A (x i , x j ) = (x i \u2212 x j ) T A(x i \u2212 x j ). (3.1)", "formula_coordinates": [2.0, 346.68, 322.27, 194.96, 11.99]}, {"formula_id": "formula_1", "formula_text": "1 Z exp (\u2212 1 2 d A (x, \u00b5)),", "formula_coordinates": [3.0, 56.64, 139.61, 87.45, 14.29]}, {"formula_id": "formula_2", "formula_text": "KL(p(x; A o ) p(x; A)) = p(x; A 0 ) log p(x; A 0 ) p(x; A) dx.", "formula_coordinates": [3.0, 54.48, 222.33, 223.93, 23.18]}, {"formula_id": "formula_3", "formula_text": "min A KL(p(x; A 0 ) p(x; A)) subject to d A (x i , x j ) \u2264 u (i, j) \u2208 S, d A (x i , x j ) \u2265 \u2113 (i, j) \u2208 D. (3.3)", "formula_coordinates": [3.0, 74.64, 334.05, 215.0, 45.08]}, {"formula_id": "formula_4", "formula_text": "D \u2113d (A, A 0 ) = tr(AA \u22121 0 ) \u2212 log det(AA \u22121 0 ) \u2212 n. (4.1)", "formula_coordinates": [3.0, 316.68, 87.86, 224.96, 13.47]}, {"formula_id": "formula_5", "formula_text": "KL(p(x; A 0 ) p(x; A)) = 1 2 D \u2113d (A \u22121 0 , A \u22121 ) = 1 2 D \u2113d (A, A 0 ), (4.2)", "formula_coordinates": [3.0, 328.44, 191.85, 213.2, 47.52]}, {"formula_id": "formula_6", "formula_text": "D \u2113d (S T AS, S T BS) = D \u2113d (A, B). (4.3)", "formula_coordinates": [3.0, 353.76, 388.99, 187.88, 11.98]}, {"formula_id": "formula_7", "formula_text": "min A 0 D \u2113d (A, A 0 ) s.t. tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2264 u (i, j) \u2208 S, tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2265 \u2113 (i, j) \u2208 D, (4.4)", "formula_coordinates": [3.0, 313.32, 453.81, 228.32, 60.63]}, {"formula_id": "formula_8", "formula_text": "min A 0,\u03be D \u2113d (A, A 0 ) + \u03b3 \u2022 D \u2113d (diag(\u03be), diag(\u03be 0 )) s. t. tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2264 \u03be c(i,j) (i, j) \u2208 S, tr(A(x i \u2212 x j )(x i \u2212 x j ) T ) \u2265 \u03be c(i,j) (i, j) \u2208 D, (4.5)", "formula_coordinates": [3.0, 307.44, 657.81, 235.33, 60.75]}, {"formula_id": "formula_9", "formula_text": "1. A \u2190 A0, \u03bbij \u2190 0 \u2200 i, j 2. \u03be c(i,j) \u2190 u for (i, j) \u2208 S; otherwise \u03be c(i,j) \u2190 \u2113 3. repeat 3.1. Pick a constraint (i, j) \u2208 S or (i, j) \u2208 D 3.2. p \u2190 (xi \u2212 xj) T A(xi \u2212 xj) 3.3. \u03b4 \u2190 1 if (i, j) \u2208 S, \u22121 otherwise 3.4. \u03b1 \u2190 min \u03bbij, \u03b4 2 1 p \u2212 \u03b3 \u03be c(i,j) 3.5. \u03b2 \u2190 \u03b4\u03b1/(1 \u2212 \u03b4\u03b1p) 3.6. \u03be c(i,j) \u2190 \u03b3\u03be c(i,j) /(\u03b3 + \u03b4\u03b1\u03be c(i,j) ) 3.7. \u03bbij \u2190 \u03bbij \u2212 \u03b1 3.8. A \u2190 A + \u03b2A(xi \u2212 xj)(xi \u2212 xj) T A 4. until convergence return A", "formula_coordinates": [4.0, 55.44, 137.52, 185.84, 158.73]}, {"formula_id": "formula_10", "formula_text": "A t+1 = A t + \u03b2A t (x i \u2212 x j )(x i \u2212 x j ) T A t , (4.6)", "formula_coordinates": [4.0, 75.0, 432.31, 214.64, 11.98]}, {"formula_id": "formula_11", "formula_text": "min K D \u2113d (K, K 0 ) subject to K ii + K jj \u2212 2K ij \u2264 u (i, j) \u2208 S, K ii + K jj \u2212 2K ij \u2265 \u2113 (i, j) \u2208 D, K 0. (4.7)", "formula_coordinates": [4.0, 320.52, 127.17, 221.12, 64.35]}, {"formula_id": "formula_12", "formula_text": "(x i \u2212 x j ) T A(x i \u2212 x j ) = d A (x i , x j ).", "formula_coordinates": [4.0, 307.44, 367.99, 233.95, 23.5]}, {"formula_id": "formula_13", "formula_text": "K ii + K jj \u2212 2K ij \u2264 u or K ii + K jj \u2212 2K ij \u2265 \u2113, we equivalently have a matrix A satisfying d A (x i , x j ) \u2264 u or d A (x i , x j ) \u2265 \u2113.", "formula_coordinates": [4.0, 307.44, 393.09, 234.03, 34.28]}, {"formula_id": "formula_14", "formula_text": "A t+1 = A t + \u03b2A t (x i \u2212 x j )(x i \u2212 x j ) T A t . (4.8)", "formula_coordinates": [4.0, 327.0, 527.47, 214.64, 11.98]}, {"formula_id": "formula_15", "formula_text": "K t+1 = K t + \u03b2K t (e i \u2212 e j )(e i \u2212 e j ) T K t . (4.9)", "formula_coordinates": [4.0, 327.12, 571.87, 214.52, 11.98]}, {"formula_id": "formula_16", "formula_text": "X T A t X + \u03b2X T A t (x i \u2212 x j )(x i \u2212 x j ) T A t X = X T (A t + \u03b2A t (x i \u2212 x j )(x i \u2212 x j ) T A T t )X.", "formula_coordinates": [4.0, 317.4, 691.03, 204.33, 27.94]}, {"formula_id": "formula_17", "formula_text": "d A \u03c6(x , \u03c6(y)) = \u03c6(x) \u2212 \u03c6(y) T A \u03c6(x) \u2212 \u03c6(y) = \u03c6(x) T A\u03c6(x) \u2212 2\u03c6(x) T A\u03c6(y) + \u03c6(y) T A\u03c6(y).", "formula_coordinates": [5.0, 65.4, 418.87, 213.97, 29.9]}, {"formula_id": "formula_18", "formula_text": "A = I + i,j \u03c3 ij \u03c6(x i )\u03c6(x j ) T .", "formula_coordinates": [5.0, 110.52, 594.67, 123.85, 22.3]}, {"formula_id": "formula_19", "formula_text": "\u03ba(x, y) = \u03ba(x, y) + i,j \u03c3 ij \u03ba(x, x i )\u03ba(x j , y),", "formula_coordinates": [5.0, 78.6, 663.69, 187.69, 20.72]}, {"formula_id": "formula_20", "formula_text": "A * = argmin A 0 T t=1 l t (A).", "formula_coordinates": [5.0, 370.44, 544.27, 107.89, 30.59]}, {"formula_id": "formula_21", "formula_text": "min A 0 f (A) = Regularization Term D(A, A t ) +\u03b7 t Loss Term l t (A) , (5.1)", "formula_coordinates": [5.0, 329.04, 655.96, 212.6, 30.14]}, {"formula_id": "formula_22", "formula_text": "At+1 \u2190 At \u2212 2\u03b7t(dt \u2212 dt)At(xt \u2212 yt)(xt \u2212 yt) T At 1 + 2\u03b7t(dt \u2212 dt)(xt \u2212 yt) T At(xt \u2212 yt) , where \u03b7t = \u03b70 ifdt \u2212 dt \u2265 0; otherwise, \u03b7t = min \u03b70, 1 2(dt\u2212dt) 1 (xt\u2212yt) T (I+(A \u22121 t \u2212I) \u22121 )(xt\u2212yt)", "formula_coordinates": [6.0, 64.44, 126.77, 218.91, 68.65]}, {"formula_id": "formula_23", "formula_text": "A t+1 = argmin A D \u2113d (A, A t ) + \u03b7 t (d t \u2212d t ) 2 .", "formula_coordinates": [6.0, 83.52, 376.13, 177.85, 19.2]}, {"formula_id": "formula_24", "formula_text": "a t (d t \u2212 d t ) 2 \u2212 b t (d * t \u2212 d t ) 2 \u2264 D \u2113d (A * , A t ) \u2212 D \u2113d (A * , A t+1 ),", "formula_coordinates": [6.0, 317.4, 90.89, 214.09, 27.24]}, {"formula_id": "formula_25", "formula_text": "d * t = d A * (x t , y t ), a t , b t are constants s.t. 0 \u2264 a t \u2264 \u03b7 t and b t = \u03b7t 1\u22122\u03b7tR 2 .", "formula_coordinates": [6.0, 307.44, 127.1, 233.97, 25.47]}, {"formula_id": "formula_26", "formula_text": "L OML \u2264 1 4\u03b7 min R 2 L A * + 1 \u03b7 min D \u2113d (A * , I),", "formula_coordinates": [6.0, 336.96, 206.37, 174.97, 23.96]}, {"formula_id": "formula_27", "formula_text": "T t=1 \u03b7 t (d t \u2212 d t ) 2 \u2264 T t=1 \u03b7 t 1 \u2212 2\u03b7 t R 2 (d * t \u2212 d t ) 2 + D \u2113d (A * , I) \u2212 D \u2113d (A * , A T ).", "formula_coordinates": [6.0, 319.68, 308.59, 211.81, 45.34]}, {"formula_id": "formula_28", "formula_text": "L OML \u2264 1 4\u03b7 min R 2 L A * + 1 \u03b7 min D \u2113d (A * , I).", "formula_coordinates": [6.0, 336.6, 380.97, 175.69, 23.96]}], "doi": ""}