{"title": "Robust Online Appearance Models for Visual Tracking", "authors": "Allan D Jepson; David J Fleet; Thomas F El-Maraghi;  \u00a3\u00fd", "pub_date": "", "abstract": "We propose a framework for learning robust, adaptive, appearance models to be used for motion-based tracking of natural objects. The approach involves a mixture of stable image structure, learned over long time courses, along with 2-frame motion information and an outlier process. An online EM-algorithm is used to adapt the appearance model parameters over time. An implementation of this approach is developed for an appearance model based on the filter responses from a steerable pyramid. This model is used in a motion-based tracking algorithm to provide robustness in the face of image outliers, such as those caused by occlusions. It is also provides the ability to adapt to natural changes in appearance, such as those due to facial expressions or variations in 3D pose. We show experimental results on a variety of natural image sequences of people moving within cluttered environments.", "sections": [{"heading": "Introduction", "text": "One of the main factors that limits the performance of visual tracking algorithms is the lack of suitable appearance models. This is true of template-matching methods that do not adapt to appearance changes, and it is true of motionbased tracking where the appearance model can change rapidly, allowing models to drift away from targets.\nThis paper proposes a robust, adaptive appearance model for motion-based tracking of complex natural objects. The model adapts to slowly changing appearance, and it maintains a natural measure of the stability of the observed image structure during tracking. By identifying stable properties of appearance we can weight them more heavily for motion estimation, while unstable properties can be proportionately downweighted.\nThe generative model for appearance is formulated as a mixture of three components, namely, a stable component that is learned with a relatively long time-course, a 2-frame transient component, and an outlier process. The stable model identifies the most reliable structure for motion estimation, while the two-frame constraints provide additional information when the appearance model is being initialized or provides relatively few constraints. The parameters of the model are learned efficiently with an on-line version of the EM algorithm.\nThe appearance model and the tracker can be used with different types of image properties. Here we consider a class of models that express image appearance in terms of the complex-valued coefficients of a steerable pyramid. This wavelet-based model allows for the stability at different scales or in different spatial neighborhoods to be assessed independently.\nTogether these components yield a robust motion estimator that naturally combines both stable appearance constraints and two-frame motion constraints. The approach is robust with respect to occlusions, significant image deformations, and natural appearance changes like those occurring with facial expressions and clothing. The appearance model framework supports tracking and accurate image alignment for a variety of possible applications, such as localized feature tracking, and tracking models for which relative alignment and position is important, such as limbs of a human body.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Previous Work", "text": "Although not always described as such, every motion estimation and tracking method embodies some representation of image appearance. The common appearance models include templates [8,13,14,15], view-based subspace models [2,9], the most recent frame in 2-frame flow estimation [16,17], temporally filtered, motion-compensated images [10,18,20], and global statistics [1,3].\nTracking with fixed templates can be reliable over short durations, but it copes poorly with appearance changes over longer durations that occur in most applications. Reliability can be improved with the use of subspace models of appearance [2,9], but these are object specific and often require training prior to tracking. Frey [8] proposed a tracker with image templates that model the mean and the variance of each pixel during tracking. The method we propose below bears some similarity to this; however, we use wavelets, online learning, and a robust mixture model instead of a Gaussian density at each pixel.\nThe use of global statistics, such as color histograms have been popular for tracking [1,3], but they will not accurately register the model to the image in many cases. These methods also fail to accurately track regions that share similar statistics with nearby regions. Motion-based trackers integrate motion estimates through time. With 2-frame motion estimation the appearance model is, implicitly, just the most recently observed image. This has the advantage of adapting rapidly to appearance changes, but it suffers because models often drift away from the target. This is especially problematic when the motions of the target and background are similar. Motion estimation can be improved by accumulating an appearance model through time. Indeed, optimal motion estimation can be formulated as the estimation of both motion and appearance simultaneously [20]. For example, one could filter the stabilised images with linear IIR filters [10,18]. But linear filtering does not provide robustness to outliers or measures of stability.\nThis paper describes a robust appearance model that adapts to changes in image appearance. The three key contributions include: 1) an appearance model that identifies stable structure and naturally combines both stable structure and transient image information; 2) an on-line version of EM for learning model parameters; and 3) a tracking algorithm which simultaneously estimates both motion and appearance. Like all adaptive appearance models there is a natural trade-off that depends on the time-course of adaptation. Faster time courses allow rapid adaptation to appearance change, while slower time courses provide greater persistence of the model, which allow one to cope with occlusions and other outliers. Here we find a balance between different time courses with a natural mixing of both 2-frame motion information and stable appearance that is learned over many frames.", "publication_ref": ["b7", "b12", "b13", "b14", "b1", "b8", "b15", "b16", "b9", "b17", "b19", "b0", "b2", "b1", "b8", "b7", "b0", "b2", "b19", "b9", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "\u00cf \u00cb \u00c4Appearance Model Framework", "text": "We first introduce the model with a single real-valued data observation, say \u00d8 at each frame \u00d8. As a motiva-tional example, consider tracking a region, such as the face in Fig. 1 (see also [21]), using a simple parametric motion model. As the subject's head moves, the local appearance of the stabilized image can be expected to vary smoothly due to changes in 3D viewpoint and to changes in the subject's facial expression. We also expect the occasional burst of outliers caused by occlusion and sudden appearance changes, such as when the glasses are removed.\nThese phenomena motivate the components of our appearance model. The first component is the stable model, \u00cb, which is intended to capture the behaviour of temporally stable image observations when and where they occur. In particular, given that the stable component generated the observation \u00d8 , we model the probability density for \u00d8 by the Gaussian density \u00d4 \u00d7\u00b4 \u00d8 \u00d7 \u00d8 \u00be \u00d7 \u00d8 \u00b5. Here \u00d7 \u00d8 and \u00be \u00d7 \u00d8 are piecewise, slowly varying functions specifying the mean and variance of the Gaussian model.\nThe second component of the model accounts for data outliers, which are expected to arise due to failures in tracking, or occlusion. We refer to the corresponding random process as the 'lost' component, and denote it by \u00c4. The probability density for \u00c4, denoted by \u00d4 \u00d0\u00b4 \u00d8 \u00b5, is taken to be a uniform distribution over the observation domain.\nThe synthetic signal depicted in Figure 2(top) provides an idealized example of these generative processes. The smooth (dashed blue) curve represents the piecewise slowly varying appearance signal. The observed data (red) has been corrupted by long-tailed noise formed from a mixture of the Gaussian density \u00d4 \u00d7\u00b4 \u00d8 \u00d7 \u00d8 \u00be \u00d7 \u00d8 \u00b5, and the broad distribution \u00d4 \u00d0\u00b4 \u00d8 \u00b5 for the lost component. In accordance with our discussion of Figure 1, we have also included an appearance discontinuity at frame 600, and a burst of outliers representing an occluder between frames 300 and 315.\nThe third component of our model is motivated by the the desire to integrate the appearance model with an imagebased tracking algorithm. That is, for a selected image region we wish to learn a model for the dominant stable image structure within the region and to simultaneously track it. This is difficult because we do not expect to have an initial stable appearance model, nor a good idea for how the object moves. The third component, called the wandering model \u00cf, determines what should be tracked in such a situation. In effect, this wandering component permits the tracker described in Section 6 to gracefully degrade to a 2frame motion tracker when the appearance model does not account for enough past data observations. The wandering component needs to allow both for more rapid temporal variations and shorter temporal histories than are required for the reliable estimation of the stable model parameters. As such, we choose the probability density for \u00d8 , given that it is generated by \u00cf, to be the Gaussian density \u00d4 \u00db\u00b4 \u00d8 \u00d8 \u00bd \u00b5. Here the mean is simply the observation from the previous frame, \u00d8 \u00bd , and the variance is fixed at \u00be \u00db . \n\u00d4\u00b4 \u00d8 q \u00d8 m \u00d8 \u00d8 \u00bd \u00b5 \u00d1 \u00db \u00d4 \u00db\u00b4 \u00d8 \u00d8 \u00bd \u00b5 \u2022 \u00d1 \u00d7 \u00d4 \u00d7\u00b4 \u00d8 q \u00d8 \u00b5 \u2022 \u00d1 \u00d0 \u00d4 \u00d0\u00b4 \u00d8 \u00b5 (1)\nwhere m \u00d1 \u00db \u00d1 \u00d7 \u00d1 \u00d0 \u00b5 are the mixing probabilities, and\nq \u00d8 \u00b4 \u00d7 \u00d8 \u00be \u00d7 \u00d8 \u00b5\ncontains the mean and variance parameters of the stable component of the model.", "publication_ref": ["b20"], "figure_ref": ["fig_0", "fig_1", "fig_0"], "table_ref": []}, {"heading": "Parameter Estimation with On-line EM", "text": "Our goal is to estimate the parameters of the generative model in (1), namely, the mean and variance of the prediction of the data, \u00d8 , by the stable process, q\n\u00b4 \u00d7 \u00be \u00d7 \u00b5,\nand the mixing probabilities m \u00b4\u00d1 \u00db \u00d1 \u00d7 \u00d1 \u00d0 \u00b5. Moreover, since we plan to apply the estimation scheme to filter responses, we seek a simple computational algorithm which requires a small amount of memory for each observation.\nAnticipating a recursive formulation, and allowing for temporal adaptation of the model parameters, we consider data observations under an exponential envelope located at \n\u00c4\u00b4d \u00d8 m \u00d8 q \u00d8 \u00b5 \u00bd \u00d8 \u00cb \u00d8\u00b4 \u00b5 \u00d0\u00d3 \u00d4\u00b4 m \u00d8 q \u00d8 \u00bd \u00b5 (2)\nwhere m \u00d8 and q \u00d8 denote parameters relevant to the data under the temporal support envelope \u00cb \u00d8\u00b4 \u00b5. Although these parameters change slowly through time, we first consider an EM-algorithm [4] for estimating m \u00d8 and q \u00d8 that assumes they are constant under the temporal window. The form of these EM-updates provides the basis for our on-line method. Given a current guess for the state variables m \u00d8 and q \u00d8 (constant over the temporal window), the E-step provides the ownership probabilities for each observation :\n\u00d3 \u00d8\u00b4 \u00b5 \u00d1 \u00d8 \u00d4 \u00b4 q \u00d8 \u00d8 \u00bd \u00b5 \u00d4\u00b4 m \u00d8 q \u00d8 \u00bd \u00b5(3)\nfor \u00be \u00db \u00d7 \u00d0 (see [4]). Conditioned on these ownerships, the M-step then computes new maximum likelihood estimates for the parameters m \u00d8 and q \u00d8 . First, the updated mixture probabilities, m \u00d8 , are given by\n\u00d1 \u00d8 \u00c3 \u00bd \u00d8 \u00cb \u00d8\u00b4 \u00b5 \u00d3 \u00d8\u00b4 \u00b5(4)\nfor \u00be \u00db \u00d7 \u00d0 (we have reused the notation \u00d1 \u00d8 to denote the updated values). Here \u00c3 is a normalization constant to ensure that the mixing probabilities sum to one. Similarly, the M-step for the mean and variance are\n\u00d7 \u00d8 \u00c5 \u00bd \u00d8 \u00d1 \u00d7 \u00d8 \u00be \u00d7 \u00d8 \u00c5 \u00be \u00d8 \u00d1 \u00d7 \u00d8 \u00be \u00d7 \u00d8(5)\nwhere \u00c5 \u00d8 are the ownership weighted first-and secondorder moments,\n\u00c5 \u00d8 \u00bd \u00d8 \u00cb \u00d8\u00b4 \u00b5 \u00d3 \u00d7 \u00d8\u00b4 \u00b5(6)\nfor \u00bd \u00be. The standard EM-algorithm then consists of iterating the steps outlined in equations ( 3) - (6).\nThis EM-algorithm requires that the data from previous times be retained to compute \u00d3 \u00d7 \u00d8\u00b4 \u00b5, which is impractical for an on-line approach. Instead we adopt an approximation to (3) - (6). To this end, we first exploit a recursive expression for the exponential support \u00cb \u00d8\u00b4 \u00b5 to obtain,\n\u00c5 \u00d8 \u00cb \u00d8\u00b4\u00d8 \u00b5 \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00d8 \u00bd \u00cb \u00d8\u00b4 \u00b5 \u00d3 \u00d7 \u00d8\u00b4 \u00b5 \u00ab \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00bd \u00d8 \u00bd \u00cb \u00d8 \u00bd\u00b4 \u00b5 \u00d3 \u00d7 \u00d8\u00b4 \u00b5 (7)\nIn order to avoid having to retain past data, we approximate the current ownership of past data by the ownerships at the times the data were first observed. That is, we replace \u00d3 \u00d7 \u00d8\u00b4 \u00b5 by \u00d3 \u00d7 \u00b4 \u00b5, to obtain the approximate moments\n\u00c5 \u00d8 \u00ab \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00bd \u00d8 \u00bd \u00cb \u00d8 \u00bd\u00b4 \u00b5 \u00d3 \u00d7 \u00b4 \u00b5 \u00ab \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00c5 \u00d8 \u00bd(8)\nWe also approximate the mixing probabilities the same way:\n\u00d1 \u00d8 \u00ab \u00d3 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00d1 \u00d8 \u00bd (9)\nfor \u00be \u00d7 \u00db \u00d0 . One further deviation from these equations is used to avoid singular situations; i.e., we impose a non-zero lower bound on the mixing probabilities and \u00d7 \u00d8 .\nIn this approximation to the batch EM in ( 3) -( 6), as mentioned above, we do not update the data ownerships of the past observations. Therefore when the model parameters change rapidly this on-line approximation is poor. Fortunately, this typically occurs when the data are not stable, which usually results in a low mixing probability and a broad variance for \u00cb in any case. Conversely, when the mean and variance drift slowly, the on-line approximation is typically very good (see Fig. 2).\nGiven sudden changes in appearance, or unstable data, the \u00cb process often loses track of the mean, and is given a small mixing probability (see Fig. 2). Thus it is necessary to occasionally restart the appearance model. ", "publication_ref": ["b3", "b3", "b5", "b5"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Wavelet-based Appearance Model", "text": "There are many properties of image appearance that one could learn for tracking and object search. Examples include local color statistics, multiscale filter responses, and localized edge fragments. In this work, we applied the on-line EM procedure to responses of a steerable pyramid (based on the \u00be and \u00c0 \u00be filters of [7]). Steerable pyramids provide a description of the image at different scales and orientations which is useful for coarse-to-fine differential motion estimation, and for isolating stability at different scales. Here we use \u00be and \u00c0 \u00be filters at two scales, tuned to wavelengths of 8 and 16 pixels (subsampled by factors of 2 and 4), with 4 orientations at each scale.\nFrom the filter outputs, we chose to maintain a representation of the phase structure as our appearance model. This gives us a natural degree of amplitude and illumination independence, and it provides the fidelity for accurate image alignment afforded by phase-based methods [5,6]. Phase responses associated with small filter amplitudes, or those deemed unstable according to the technique described in [5], were treated as outliers.\nIn what follows, given an image pyramid and a tar- 2) the standard deviation of the \u00cf process on phase differences, which we take to be mean-zero Gaussian with \u00db \u00bc \u00bf ; and 3) the minimum standard deviation of the stable process, \u00d7 \u00bc \u00bc \u00bd . These latter parameters are specific to the use of phase.\nget", "publication_ref": ["b6", "b4", "b5", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Motion-Based Tracking", "text": "We demonstrate the behaviour of the adaptive, phasebased appearance model in the context of tracking nonrigid objects. For this demonstration we manually specify an elliptical region AE \u00bc at time \u00bc. The tracking algorithm then estimates the image motion and the appearance model as it tracks the dominant image structure in AE \u00d8 over time.\nThe motion is represented in terms of frame-to-frame parameterized image warps. In particular, given the warp parameters a \u00d8 , a pixel x at frame \u00d8 \u00bd corresponds to the image location x \u00d8 w\u00b4x a \u00d8 \u00b5 at time \u00d8, where w\u00b4x a \u00d8 \u00b5 is the warp function. We use similarity transforms here, so a \u00d8 \u00b4u \u00d8 \u00d8 \u00d8 \u00b5 is a 4-vector describing translation, rotation, and scale changes, respectively. Given the parameter vector a \u00d8 , AE \u00d8 is just the elliptical region provided by warping AE \u00d8 \u00bd by w\u00b4x a \u00d8 \u00b5. Other parameterized image warps and other forms of image regions could also be used.\nTo find an optimal warp we (locally) maximize the sum of the data log likelihood and a log prior that provides a preference for slow and smooth motions (cf. [19]). In terms of the motion and appearance models outlined above, the data log-likelihood can be expressed as\n\u00c4\u00b4 \u00b4w\u00b4x a \u00d8 \u00b5 \u00d8 \u00b5 \u00d8 \u00bd \u00b5 (10) x\u00beAE\u00d8 \u00bd \u00d0\u00d3 \u00d4\u00b4 \u00b4w\u00b4x a \u00d8 \u00b5 \u00d8 \u00b5 m x \u00d8 \u00bd q x \u00d8 \u00bd \u00b4x \u00d8 \u00bd\u00b5\u00b5\nIntuitively, this can be understood as follows: data at the current frame \u00d8 is warped back to the coordinates of frame \u00d8 \u00bd according to the parameters a \u00d8 . The log likelihood of this warped data \u00b4w\u00b4x a \u00d8 \u00b5 \u00d8 \u00b5 is then computed with respect to the appearance model \u00d8 \u00bd . The prior is introduced mainly to cope with occlusions, and to exploit the persistence of the stable component \u00cb.\nWe take the prior density over the motion parameters a \u00d8 \u00b4u \u00d8 \u00d8 \u00d8 \u00b5, conditioned on the motion at time \u00d8 \u00bd, a \u00d8 \u00bd , to be a product of two 4D Gaussians:\n\u00d4\u00b4a \u00d8 a \u00d8 \u00bd \u00b5 \u00b4a \u00d8 C \u00bd \u00b5 \u00b4a \u00d8 a \u00d8 \u00bd C \u00be \u00b5(11)\nThe first Gaussian prefers slow motions, with mean\n\u00b4\u00bc \u00bc \u00bc \u00bd\u00b5 and covariance C \u00bd diag\u00b4 \u00be \u00be \u00bc \u00bc \u00be \u00bc \u00bc\u00bd \u00be \u00b5.\nHere, translations are measured in pixels, rotational velocities in radians, and the scale parameter is a multiplicative factor so specifies the identity warp. The second Gaussian prefers slow changes in motion, with\nC \u00be diag\u00b4\u00bd \u00bd \u00bc \u00bc\u00be \u00be \u00bc \u00bc\u00bd \u00be \u00b5.\nIn order to estimate a \u00d8 we can almost directly apply the EM-algorithm described in [11]. We omit the details due to space limitations, and instead just sketch the E-step and M-step. The E-step determines the ownership probabilities for the backwards warped data \u00b4w\u00b4x a \u00d8 \u00b5 \u00d8 \u00b5 , as in (3) above. The M-step uses these ownerships to form a linear system for the update AEa \u00d8 :\n\u00d7 \u2022\u00af \u00db \u00b5AEa \u00d8 b \u00d7 \u2022\u00afb \u00db (12)\nHere, is a \u00a2 matrix and b a 4-vector, for \u00db \u00d7.\nThese quantities are formed from the motion constraints weighted by the ownership probabilities for the \u00cf and \u00cb processes, respectively (see [11]). Also,\u00afis a weighting factor for the wandering constraints. A proper M-step for maximizing the likelihood in (10) would use the weight\u00af \u00bd. We have found it useful to downweight the constraints owned by the wandering model by a factor of\u00af \u00bd \u00d2 \u00d7 , where \u00d2 \u00d7 is the half-life of the exponential temporal window used in the appearance model. We use coarse-to-fine matching and deterministic annealing (see [11], [12]) in fitting the warp parameters.\nOnce the warp parameters a \u00d8 have been determined, we convect the appearance model \u00d8 \u00bd forward to the current time \u00d8 using the warp specified by a \u00d8 . To perform this warp  we use a piecewise constant interpolant for the \u00cf \u00cb \u00c4state variables m\u00b4x \u00d8 \u00bd\u00b5 and \u00d7\u00b4x \u00d8 \u00bd\u00b5. This interpolation was expected to be too crude to use for the interpolation of the mean \u00b4x \u00d8 \u00bd\u00b5 for the stable process, so instead the mean is interpolated using a piecewise linear model. The spatial phase gradient for this interpolation is determined from the gradient of the filter responses at the nearest pixel to the desired location x on the image pyramid sampling grid [6].", "publication_ref": ["b18", "b10", "b2", "b10", "b10", "b11", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "The behaviour of the tracking algorithm is illustrated in Fig. 3 where we plot the elliptical target region AE \u00d8 , the mixing probability \u00d1 \u00d7\u00b4x \u00d8 \u00b5, the mean \u00d7\u00b4x \u00d8 \u00b5, and the data ownership \u00d3 \u00d7 \u00d8\u00b4x \u00d8 \u00b5 for the stable component, each overlaid on the original images. In these and the following images we only show responses where \u00d1 \u00d7\u00b4\u00dc \u00d8\u00b5 is greater than a fixed threshold. Thus, blank areas indicate that the appearance model has not found stable structure. As is expected, the significant responses (shown in black) for the \u00cb component occur around higher contrast image regions. For Fig. 3 the processing was started roughly 70 frames prior to the one shown on the top row [21]. The significant responses for \u00d1 \u00d7 \u00d8 and \u00d3 \u00d7 \u00d8 demonstrate that the appearance model successfully identified stable structure, typically inside the object boundary. On the second and third rows of Fig. 3, where the person is occluded by the sign, note that \u00d1 \u00d7\u00b4x \u00d8 \u00b5 decays smoothly in the occluded region due to the absence of data support, while the mean \u00d7\u00b4x \u00d8 \u00b5 remains roughly fixed until \u00d1 \u00d7 falls below the plotting threshold. This clearly demonstrates the persistence of the appearance model. The third row depicts the model after The ability to adapt to changing appearance is demonstrated in Fig. 4 [21]. Here, despite the person turning to walk in the opposite direction (at frame 300), the \u00cb component maintains a reasonable model for the stable image structure.\nOne of our goals was to track and identify stable properties in images of nonrigid objects, such as in the example shown in Fig. 5. From the images of \u00d1 \u00d7 in Fig. 5(bottom left), notice that the mouth region was initially identified as stable, but after the person smiles the stability is weakened significantly. Once the new expression has been held for about 20 frames the structure is again identified as stable.\nOther parts of the face, such as the eyebrows show similar behaviour. Conversely, the values of \u00d1 \u00d7 near the hairline and on nose continue to increase through these events, indicating that they are consistently stable and, overall, the head is being accurately tracked.\nThe behaviour during a brief occlusion event is shown in Fig. 6, where the person's hand reaches up to brush their hair back. The model persists, with \u00d1 \u00d7 and \u00d7 remaining essentially constant despite the occlusion. By contrast, notice that the data ownerships \u00d3 \u00d7 \u00d8 clearly reflect the presence of the occluder. Also note that the data ownerships are not perfect; there are some false matches to the appearance model in the area of the occluder. Presumably these are a result of 'accidental' alignments of the phase responses from the occluder with those of the appearance model. Given that the minimum standard deviation for \u00d7 is \u00bc \u00bd , we should expect the false target rate to be reasonably high. In fact, these false targets appear to drag the model into misalignment during the occlusion (see the caption in Fig. 6 for Figure 7. Tracking with partial occlusion along with variable lighting, appearance and size. The camera was stationary, and the sequences are each roughly 250 frames. We show the highlighted target region for selected frames superimposed on the last frame. a pointer to this), but that the appearance model is subsequently able to lock back on. Such a misalignment would clearly persist in any 2-frame tracking algorithm.\nFinally, Fig. 7 shows the stability of the joint estimation of motion and appearance, despite significant changes in size and lighting conditions. Even more challenging for the current method are the (at times) small target regions, and the small separation of the object motion from the background motion (about a pixel per frame). Also, roughly half the target region is occluded by the bushes at times. The two runs depicted in Fig. 7 are close to the limit of our approach in terms of these latter sources of difficulty.\nIn our tests we have identified two failure modes [21]. The first is caused by acceptance of non-target constraints as being consistent with the appearance model (see discussions of Figs. 3, 6 and 7). These erroneous constraints perturb the alignment of the model and, if this effect is sufficiently large, a tracking failure can occur. Second, when the tracked object consistently moves with its background, then the appearance model also learns the background structure. Tracking can fail if the object then moves independently.\nPossible topics for future work include the incorporation of colour and brightness data into the appearance model, and the use of the stable appearance model for image matching to recover from tracking failures caused by total occlusion.", "publication_ref": ["b20", "b20", "b20"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_3", "fig_4", "fig_4", "fig_5", "fig_5", "fig_2"], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Elliptical head tracking using intensity gradients and color histograms", "journal": "", "year": "1998", "authors": "S Birchfield"}, {"ref_id": "b1", "title": "EigenTracking: Robust matching and tracking of articulated objects using a viewbased representation", "journal": "IJCV", "year": "1998", "authors": "M J Black; A D Jepson"}, {"ref_id": "b2", "title": "Real-time tracking of non-rigid objects using mean shift", "journal": "Hilton Head", "year": "2000", "authors": "D Comaniciu; V Ramesh; P Meer"}, {"ref_id": "b3", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "J. Royal Stat. Soc. B", "year": "1977", "authors": "A P Dempster; N M Laird; D B Rubin"}, {"ref_id": "b4", "title": "Stability of phase information", "journal": "IEEE Trans. PAMI", "year": "1993", "authors": "D J Fleet; A D Jepson"}, {"ref_id": "b5", "title": "Phase-based disparity measurement", "journal": "CVIU", "year": "1991", "authors": "D J Fleet; A D Jepson; M Jenkin"}, {"ref_id": "b6", "title": "The design and use of steerable filters", "journal": "IEEE Trans. PAMI", "year": "1991", "authors": "W Freeman; E H Adelson"}, {"ref_id": "b7", "title": "Filling in scenes by propagating probabilities through layers into appearance models", "journal": "Hilton Head", "year": "2000", "authors": "B Frey"}, {"ref_id": "b8", "title": "Efficient region tracking with parametric models of geometry and illumination", "journal": "IEEE Trans. PAMI", "year": "1998", "authors": "G D Hager; P N Belhumeur"}, {"ref_id": "b9", "title": "Computing occluding and transparent motions", "journal": "IJCV", "year": "1994", "authors": "M Irani; B Rousso; S Peleg"}, {"ref_id": "b10", "title": "Mixture models for optical flow computation", "journal": "", "year": "1993", "authors": "A Jepson; M J Black"}, {"ref_id": "b11", "title": "Skin and bones: Multi-layer, locally affine, optical flow and regularization with transparency", "journal": "", "year": "1996", "authors": "S X Ju; M J Black; A D Jepson"}, {"ref_id": "b12", "title": "Singularity analysis for articulated object tracking", "journal": "", "year": "1998", "authors": "D Morris; J Rehg"}, {"ref_id": "b13", "title": "Maximum-likelihood template tracking", "journal": "Proc IEEE CVPR", "year": "2000", "authors": "C Olson"}, {"ref_id": "b14", "title": "Efficient guaranteed search for gray-level patterns", "journal": "", "year": "1997", "authors": "W Rucklidge"}, {"ref_id": "b15", "title": "Good features to track", "journal": "", "year": "1994", "authors": "J Shi; C Tomasi"}, {"ref_id": "b16", "title": "Stochastic tracking of 3d human figures using 2d image motion", "journal": "Springer-Verlag", "year": "2000", "authors": "H Sidenbladh; M J Black; D J Fleet"}, {"ref_id": "b17", "title": "Dynamic layer representation with applications to tracking", "journal": "Hilton Head", "year": "2000", "authors": "H Tao; H S Sawhney; R Kumar"}, {"ref_id": "b18", "title": "Slow and smooth", "journal": "MIT AI Memo", "year": "1998", "authors": "Y Weiss; E Adelson"}, {"ref_id": "b19", "title": "Probabilistic Models of the Brain: Perception and Neural Function", "journal": "MIT Press", "year": "2001", "authors": "Y Weiss; D J Fleet"}, {"ref_id": "b20", "title": "for mpeg videos of tracking results", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. Cropped images from a 1200 frame sequence taken with handheld video camera. The ellipse shows the region in which the motion and appearance are estimated.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Estimation using on-line EM. (top)The original data (thin red) with true state (dashed blue) and the estimated mean of the stable process (thick black). The noise is a mixture of Gaussian and uniform densities, with mixing probabilities (0.9, 0.1), except for 15 frames at 300 which are pure outliers. (bottom) Mixing probabilities for \u00cb (black), \u00cf (dashed red), and the \u00c4 (light green).The three components \u00cf, \u00cb, and \u00c4, are combined in a probabilistic mixture model for \u00d8 ,", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 .3Figure 3. Each row shows, from left to right, the tracking region, the stable component's mixing probability \u00d1\u00d7\u00b4x \u00d8 \u00b5, mean \u00d7\u00b4x \u00d8 \u00b5, and ownership probability \u00d3\u00d7\u00b4x \u00d8 \u00b5. The rows correspond to frames 244, 259, 274, and 289, top to bottom. Note the model persistence and the drop in data ownership within the occluded region.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 .4Figure 4. The adaptation of the model during tracking. (top) The target region in selected frames 200, 300, 480. (bottom) The stable component's mixing probability (left) and mean (right) for the selected frames.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 .5Figure 5. Adaptation to changes of expression. (top) The target region in selected frames 420, 455, 490. (bottom) The stable component's mixing probability (left) and mean (right) for the selected frames (time increases left to right in each set). Note how the regions around the mouth and eyebrows adapt, while others remain stable.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 .6Figure 6. Robust tracking despite occlusion. Tracking results for frames 200, 205, 210 and 215 are shown, top to bottom. The elliptical tracking region, and the stable model's mixing probability, mean and ownership are arranged left to right. Note that the model is misaligned during the occlusion (see the second and third images on the second row) but that it promptly realigns. Also, note the stability and model persistence (left three columns), along with the reduced data ownership on the hand (right column). roughly 20 frames of occlusion (recall the half-life of the model is \u00d2 \u00d7 \u00be \u00bc ), by which time the weaker components in \u00cb have disappeared. However, the model continues to track through this occlusion event and maintains the stable model on the visible portion of the subject. When the person emerges from behind the occluder, the appearance model rebuilds the dissipated stable model.The ability to adapt to changing appearance is demonstrated in Fig.4[21]. Here, despite the person turning to", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "the current time, \u00cb \u00d8\u00b4 \u00b5 \u00ab \u00b4\u00d8 \u00b5 , for \u00d8. Here, \u00d2 \u00d7 \u00d0\u00d3 \u00be, where \u00d2 \u00d7 is the half-life of the envelope in frames, and \u00ab \u00bd \u00bd so the envelope weights \u00cb \u00d8\u00b4 \u00b5 sum to 1. With this envelope we can express the log-likelihood of the observation history, d \u00d8 \u00d8", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The smaller value for \u00d1 \u00d7 \u00d8 reflects an initial uncertainty for the \u00cb model. The new values for the moments \u00c5 \u00d8 for \u00bd \u00be are taken to be \u00d8 \u00d1 \u00d7 \u00d8 and \u00be \u00d7", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "region AE \u00d8 , let \u00b4x \u00d8 \u00b5 x\u00beAE\u00d8 denote the set of phase observations from all filters at time \u00d8 in the region. Let", "figure_data": "\u00d8\u00b4m\u00b4x \u00d8 \u00b5 q\u00b4x \u00d8 \u00b5 x\u00beAE\u00d8 denote the entire appearancemodel of the phase at each orientation, scale, and spatial lo-cation in AE \u00d8 . The half-life of the exponential temporal sup-port, \u00cb \u00d8\u00b4 \u00b5, was set to \u00d2 \u00d7 \u00be \u00bc frames. The other parame-ters of the on-line EM estimator are: 1) the outlier probabil-ity, which is uniform on\u00b5;"}], "formulas": [{"formula_id": "formula_0", "formula_text": "\u00d4\u00b4 \u00d8 q \u00d8 m \u00d8 \u00d8 \u00bd \u00b5 \u00d1 \u00db \u00d4 \u00db\u00b4 \u00d8 \u00d8 \u00bd \u00b5 \u2022 \u00d1 \u00d7 \u00d4 \u00d7\u00b4 \u00d8 q \u00d8 \u00b5 \u2022 \u00d1 \u00d0 \u00d4 \u00d0\u00b4 \u00d8 \u00b5 (1)", "formula_coordinates": [3.0, 75.24, 418.41, 212.74, 28.56]}, {"formula_id": "formula_1", "formula_text": "q \u00d8 \u00b4 \u00d7 \u00d8 \u00be \u00d7 \u00d8 \u00b5", "formula_coordinates": [3.0, 50.16, 465.39, 65.4, 12.66]}, {"formula_id": "formula_2", "formula_text": "\u00b4 \u00d7 \u00be \u00d7 \u00b5,", "formula_coordinates": [3.0, 250.54, 536.07, 35.75, 12.66]}, {"formula_id": "formula_3", "formula_text": "\u00c4\u00b4d \u00d8 m \u00d8 q \u00d8 \u00b5 \u00bd \u00d8 \u00cb \u00d8\u00b4 \u00b5 \u00d0\u00d3 \u00d4\u00b4 m \u00d8 q \u00d8 \u00bd \u00b5 (2)", "formula_coordinates": [3.0, 53.04, 702.87, 233.38, 35.58]}, {"formula_id": "formula_4", "formula_text": "\u00d3 \u00d8\u00b4 \u00b5 \u00d1 \u00d8 \u00d4 \u00b4 q \u00d8 \u00d8 \u00bd \u00b5 \u00d4\u00b4 m \u00d8 q \u00d8 \u00bd \u00b5(3)", "formula_coordinates": [3.0, 358.56, 205.77, 186.58, 27.48]}, {"formula_id": "formula_5", "formula_text": "\u00d1 \u00d8 \u00c3 \u00bd \u00d8 \u00cb \u00d8\u00b4 \u00b5 \u00d3 \u00d8\u00b4 \u00b5(4)", "formula_coordinates": [3.0, 368.64, 283.95, 176.5, 35.7]}, {"formula_id": "formula_6", "formula_text": "\u00d7 \u00d8 \u00c5 \u00bd \u00d8 \u00d1 \u00d7 \u00d8 \u00be \u00d7 \u00d8 \u00c5 \u00be \u00d8 \u00d1 \u00d7 \u00d8 \u00be \u00d7 \u00d8(5)", "formula_coordinates": [3.0, 356.88, 378.33, 188.26, 26.4]}, {"formula_id": "formula_7", "formula_text": "\u00c5 \u00d8 \u00bd \u00d8 \u00cb \u00d8\u00b4 \u00b5 \u00d3 \u00d7 \u00d8\u00b4 \u00b5(6)", "formula_coordinates": [3.0, 361.8, 432.15, 183.34, 35.7]}, {"formula_id": "formula_8", "formula_text": "\u00c5 \u00d8 \u00cb \u00d8\u00b4\u00d8 \u00b5 \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00d8 \u00bd \u00cb \u00d8\u00b4 \u00b5 \u00d3 \u00d7 \u00d8\u00b4 \u00b5 \u00ab \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00bd \u00d8 \u00bd \u00cb \u00d8 \u00bd\u00b4 \u00b5 \u00d3 \u00d7 \u00d8\u00b4 \u00b5 (7)", "formula_coordinates": [3.0, 310.2, 554.19, 234.94, 71.22]}, {"formula_id": "formula_9", "formula_text": "\u00c5 \u00d8 \u00ab \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00bd \u00d8 \u00bd \u00cb \u00d8 \u00bd\u00b4 \u00b5 \u00d3 \u00d7 \u00b4 \u00b5 \u00ab \u00d8 \u00d3 \u00d7 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00c5 \u00d8 \u00bd(8)", "formula_coordinates": [3.0, 309.48, 683.55, 235.66, 53.34]}, {"formula_id": "formula_10", "formula_text": "\u00d1 \u00d8 \u00ab \u00d3 \u00d8\u00b4 \u00d8 \u00b5 \u2022 \u00bd \u00ab\u00b5 \u00d1 \u00d8 \u00bd (9)", "formula_coordinates": [4.0, 80.76, 112.17, 205.66, 12.84]}, {"formula_id": "formula_11", "formula_text": "get", "formula_coordinates": [4.0, 308.88, 202.27, 12.17, 8.97]}, {"formula_id": "formula_12", "formula_text": "\u00c4\u00b4 \u00b4w\u00b4x a \u00d8 \u00b5 \u00d8 \u00b5 \u00d8 \u00bd \u00b5 (10) x\u00beAE\u00d8 \u00bd \u00d0\u00d3 \u00d4\u00b4 \u00b4w\u00b4x a \u00d8 \u00b5 \u00d8 \u00b5 m x \u00d8 \u00bd q x \u00d8 \u00bd \u00b4x \u00d8 \u00bd\u00b5\u00b5", "formula_coordinates": [4.0, 311.64, 624.81, 233.44, 42.06]}, {"formula_id": "formula_13", "formula_text": "\u00d4\u00b4a \u00d8 a \u00d8 \u00bd \u00b5 \u00b4a \u00d8 C \u00bd \u00b5 \u00b4a \u00d8 a \u00d8 \u00bd C \u00be \u00b5(11)", "formula_coordinates": [5.0, 62.4, 572.49, 223.96, 12.84]}, {"formula_id": "formula_14", "formula_text": "\u00b4\u00bc \u00bc \u00bc \u00bd\u00b5 and covariance C \u00bd diag\u00b4 \u00be \u00be \u00bc \u00bc \u00be \u00bc \u00bc\u00bd \u00be \u00b5.", "formula_coordinates": [5.0, 50.16, 607.47, 236.13, 11.28]}, {"formula_id": "formula_15", "formula_text": "C \u00be diag\u00b4\u00bd \u00bd \u00bc \u00bc\u00be \u00be \u00bc \u00bc\u00bd \u00be \u00b5.", "formula_coordinates": [5.0, 50.16, 655.63, 223.68, 20.61]}, {"formula_id": "formula_16", "formula_text": "\u00d7 \u2022\u00af \u00db \u00b5AEa \u00d8 b \u00d7 \u2022\u00afb \u00db (12)", "formula_coordinates": [5.0, 376.92, 540.19, 168.16, 10.11]}], "doi": ""}