{"title": "CAME: Confidence-guided Adaptive Memory Efficient Optimization", "authors": "Yang Luo; Xiaozhe Ren; Zangwei Zheng; Zhuo Jiang; Xin Jiang; Yang You", "pub_date": "", "abstract": "Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available 1 . * Work was done when Yang Luo was an intern at Huawei Noah's Ark Lab.", "sections": [{"heading": "Introduction", "text": "Robust training of large language models (LLMs) often relies on adaptive gradient-based optimization methods Kingma and Ba, 2015;Zhuang et al., 2020). Through the use of cumulative second-order statistics, these methods adapt the per-parameter learning rate and demonstrate superior convergence speed during the training process of LLMs. However, the remarkable performance of adaptive methods incurs an extra cost of memory usage indeed. For example, Adam requires to preserve the first moment estimate and second Figure 1: Visualization of Non-negative Matrix Factorization (NMF). Generally, NMF reduces the memory requirements from O(nm) to O(n + m). In this paper, we focus on the special case of rank-1 factors. raw moment estimate of each gradient in order to tune the learning rate for each parameter, which inevitably triples the memory usage concerning the optimizer states. Besides, with the growing size of the model, LLMs are becoming increasingly expensive in terms of memory, and the limitation of memory is gradually emerging as a main bottleneck for training LLMs.\nMany existing memory-efficient optimizers attempt to store second-order statistics with sublinear memory requirement while retaining the exceptional convergence property of adaptivity (Shazeer and Stern, 2018;Anil et al., 2019). Adafactor optimizer achieves remarkable memory cost reduction by applying the non-negative matrix factorization algorithm (Lee and Seung, 2000) to factorize the accumulator matrix for squared gradients into two rank-1 factors as shown in Figure 1, where the memory requirement for the original matrix V decreases from O(nm) to O(n + m). Whereas, it is observed that Adafactor suffers a performance degradation in the training of large language models universally compared with conventional adaptive gradient-based optimization methods. The reason for this phenomenon is Adafactor inevitably introduces some errors that cause instability in training deep networks due to the operation of nonnegative matrix factorization.\nIn addition, in the case of large-batch training that aims to accelerate the training of deep neural networks, the memory consumption of each machine (GPU/TPU) is much higher than general batch size training, which further imposes a grave constraint on the performance of the trained model. In comparison to standard training tasks, largebatch training presents more challenges for optimizers. Empirically, when the mini-batch size increases after a certain point (e.g. 1024), the test accuracy of the converged solution decreases significantly compared with the baseline (He et al., 2021). To our knowledge, there is currently no work related to memory-efficient optimizers for large-batch training.\nMotivated by these challenges, we firstly study a confidence-guided strategy catered to alleviate the instability of Adafactor by calculating the confidence of the generated update at each training step. On the basis of the adaptation strategy, we propose a novel CAME optimizer that saves nearly the same memory footprint as existing memoryefficient optimizers while attaining faster convergence and superior generalization performance. To further assess the scalability of our proposed algorithm, we consider an additional challenging experiment -performing large-batch training on BERT using CAME optimizer.\nContributions of our paper can be summarized in the following:\n\u2022 Inspired by training instability of Adafactor, we explore a confidence-guided strategy centered on the existing error in the raw updates of Adafactor for parameters of large language models.\n\u2022 In light of the dedicated strategy, we propose a novel optimization algorithm, CAME, for achieving faster convergence and less performance degradation catered at memoryefficient optimization. We further investigate the effect of the proposed memory-efficient optimization algorithm in large-batch training settings.\n\u2022 ", "publication_ref": ["b11", "b32", "b27", "b0", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Memory Efficient Adaptive Optimization Memory efficient optimizers maintain the benefits of standard per-parameter adaptivity while significantly reducing memory footprint. Adafactor (Shazeer and Stern, 2018) proposes to reconstruct a low-rank approximation of the exponentially smoothed accumulator at each training step that is optimal with respect to the generalized Kullback-Leibler divergence. SM3 (Anil et al., 2019) divides the elements in the second-order gradient matrix into sets by the observed similarity of the elements, and each item in the generated approximation matrix is the minimum of the maximum value of each set in which it is located. The methods mentioned above behave poorly in the training of large language models and converge slowly, which raises a significant challenge for memory-efficient optimization methods. Large Batch Training A large-batch training scheme is preferred in distributed machine learning because of its ability to increase parallelism by enhancing large-scale cluster utilization. It has seen growing interest in recent years in large-batch training (Liu et al., 2022;Li et al., 2021;Huo et al., 2021). In particular, a layer-wise adaptive learning rate algorithm LARS (You et al., 2017a) is proposed to scale the batch size to 32k for ResNet-50. Based on LARS, LAMB optimizer (You et al., 2019) can finish the BERT training in 76 minutes through TPU v3 Pod. Despite the success of these approaches for BERT models, the much larger batch size highly boosts the GPU usage which is prohibitively expensive and inaccessible to most researchers.\nMoreover, training with a large batch size incurs additional challenges (Hoffer et al., 2017;Keskar et al., 2016). Large-batch training is prone to converge to sharp local minima, since the number of interactions will decrease when the batch size is increased if the number of epochs is fixed, which causes a wide gap in generalization of the model (Keskar et al., 2016). Traditional methods seek to narrow the generalization gap by carefully tuning hyperparameters, such as learning rate, momentum, and label smoothing, to narrow the generalization gap (Goyal et al., 2017a;Shallue et al., 2018;You et al., 2017b). Yet there have been few attempts to reduce memory usage in large-batch training, and the underlying challenge remains unclear.", "publication_ref": ["b27", "b0", "b22", "b9", "b28", "b29", "b8", "b10", "b10", "b4", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "In this section, we firstly provide a brief description of the Adafactor optimizer and discuss the errors contained in the update of Adafactor (erroneous update). We further study a confidence-guided strategy and introduce the proposed CAME in detail in light of the strategy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "An overview of Adafactor", "text": "The L(\u03b8) \u2208 R represents the loss function that we plan to minimize, where \u03b8 \u2208 R n\u00d7m is the parameter of the model. g t is the gradient at step t, \u03b7 is the learning rate, r t and c t are the exponential moving average of two low-rank factors for the second moments of the gradient. \u03f5 1 is a small regularization constants and u t is the current approximate update.\nIn the training of large language models, Adafactor is required to apply momentum to ensure the convergence (Chowdhery et al., 2022), and the corresponding pseudocode is illustrated in Algorithm 1. The problem setting is as follows. Assume that we aim to minimize the expected value of an objective function f (\u03b8). At each training step, we receive the loss derived from a mini-batch of data, and calculate the gradient g t of the function based on the previous parameters. Subsequently, we update the exponential running averages of two factors for second moments of the gradient r t and c t , compute approximations for the second moments of the gradient v t , and adjust the generated update (u t ) when RM S(u t ) surpasses a specific threshold value d as in:\nu t = u t max(1, RM S(u t )/d) (1)\nwhere RM S(u t ) refers to the root-mean-square calculation of the components of u t . Finally, the first moment of the adjusted update m t is utilized to update the parameter, resulting in a new iteration \u03b8 t . The optimization continues until the parameters converge and returns the final iteration \u03b8 T as our approximate solution.\nAdafactor derives an effective solution for nonnegative matrix factorization in the special case of rank-1 factors, which obtains the minimal Kullback-Leibler divergence (Lee and Seung) between the matrix V and the approximated matrix W H. The formulation of the solution is as follows, in which 1 m = (1, ..., 1) \u2208 R m represents a column vector of m ones:\nW = V 1 m , H = 1 T n V 1 T n V 1 m .(2)\nIt should be noted that Adafactor stores only the moving averages of these factors rather than the entire matrix V , yielding considerable memory savings and requiring memory usage proportional to O(n + m) instead of O(nm).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Erroneous Update", "text": "The non-negative matrix factorization operation in Adafactor will inevitably incur erroneous update in the training of deep neural networks. As shown in Figure 2, Adafactor always converge slower than Adam due to the existing error in calculated updates, which further limits the application scenarios of memory-efficient optimizers.\nAs shown in Figure 3, two scenarios demonstrate how two types of erroneous updates are supposed to be handled in the ideal case. In Figure 3(a), the difference between the momentum of updates m t and the current update u t is large, illustrating that the historical experience for the update of original Adafactor contains high level of errors that will inevitably influence the stability of the training process. If we utilize the raw m t to take an optimization step, the direction of optimization will deviate increasingly from the desired direction, which is reflected by the slow convergence and performance Algorithm 1: Adafactor Optimizer Input: Initial parameters \u03b8 0 , learning rate \u03b7, momentum of update m 0 , r 0 , c 0 , step t, regularization constant \u03f5 1 , exponential moving average parameters\n\u03b2 1 , \u03b2 2 , clipping threshold d while \u03b8 t not converge do Compute g t = \u2207f (\u03b8 t\u22121 ) r t = \u03b2 2 r t\u22121 + (1 \u2212 \u03b2 2 )(g 2 t + \u03f5 1 1 n 1 T m )1 m c t = \u03b2 2 c t\u22121 + (1 \u2212 \u03b2 2 )1 T n (g 2 t + \u03f5 1 1 n 1 T m ) v t = r t c t /1 T n r t u t = g t / \u221a v t u t = u t /max(1, RM S(u t )/d) m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )\u00fb t \u03b8 t = \u03b8 t\u22121 \u2212 \u03b7m t end\ndegradation of existing memory-efficient optimizers. By contrast, when the difference between m t and u t is small as shown in Figure 3(b), the momentum m t is stable with limited errors and high confidence therefore a large optimization step is required with the updating direction close to m t . Inspired by the erroneous update that is universal in existing memory-efficient optimizers, we firstly consider an efficient approach to decrease the side effect caused by insecure updating. Given m t and u t , we take the residual between them as the instability in the preserved momentum and set generated instability as the denominator of original m t to more adaptively take an update step. Following is the formulation of the adjusted update u \u2032 , where \u03f5 is the regularization constant:\nu \u2032 t = m t (m t \u2212 u t ) 2 + \u03f5 (3)\nExtending further on the plain method, we propose a confidence-guided strategy that enables selfadjusted updates by taking the confidence of the raw update of Adafactor into consideration. The intuition behind the proposed strategy is to calculate the residual between the exponential moving average (EMA) of the update and the current update, which represents the deviation of the approximated update. The larger the deviation of the EMA value from the current generated update, the wider the error EMA of update contains, resulting in a lower level of confidence in the EMA of update. Obviously, we expect the optimizer to take a small update when it incorporates huge error (a large residual from the present update), while updating parameters more when the optimization process is stable (involved error of EMA is limited).\nSpecifically, the EMA of update m t is directly used to take an update step in Adafactor, while in our proposed strategy, m t is divided by \u221a U t , where U t is the calculated instability matrix. Therefore, 1 \u221a Ut is the confidence in the observation: viewing m t as the prediction of the update, if m t deviates greatly from u t (U t is large), which indicates a weak confidence in m t , the optimizer performs a small optimization step; if u t closely matches m t , we have solid confidence in m t , and correspondingly take a large optimization step.", "publication_ref": [], "figure_ref": ["fig_0", "fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "CAME Algorithm", "text": "Based on the proposed confidence-guided strategy, we develop a brand-new variant of memory-Algorithm 2: CAME Optimizer Input: Initial parameters \u03b8 0 , learning rate \u03b7, momentum of update m 0 = 0, r 0 = 0, c 0 = 0, step t = 0, regularization constants \u03f5 1 , \u03f5 2 , exponential moving average parameters\n\u03b2 1 , \u03b2 2 , \u03b2 3 , clipping threshold d while \u03b8 t not converge do Compute g t = \u2207f (\u03b8 t\u22121 ) r t = \u03b2 2 r t\u22121 + (1 \u2212 \u03b2 2 )(g 2 t + \u03f5 1 1 n 1 T m )1 m c t = \u03b2 2 c t\u22121 + (1 \u2212 \u03b2 2 )1 T n (g 2 t + \u03f5 1 1 n 1 T m ) v t = r t c t /1 T n r t u t = g t / \u221a v t u t = u t /max(1, RM S(u t )/d) m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )\u00fb t U t = (\u00fb t \u2212 m t ) 2 R t = \u03b2 3 R t\u22121 + (1 \u2212 \u03b2 3 )(U t + \u03f5 2 1 n 1 T m )1 m C t = \u03b2 3 C t\u22121 + (1 \u2212 \u03b2 3 )1 T n (U t + \u03f5 2 1 n 1 T m ) S t = R t C t /1 T n R t \u03b8 t = \u03b8 t\u22121 \u2212 \u03b7 \u221a St m t end\nefficient optimization methods with faster convergence. Our proposed CAME optimization method successfully obtains the same rate of convergence as prevailing first-order optimization algorithms (e.g., Adam) and with almost equal memory cost to available memory-efficient optimizers (e.g., Adafactor). The pseudocode of CAME algorithm is specified in Algorithm 2.\nBy calculating U t at each training step, we employ non-negative matrix factorization on the instability matrix U t following (Shazeer and Stern, 2018) where the generalized Kullback-Leibler divergence between V and W H is minimal. With U t factorized into R t and C t , it is sufficient to store only the moving averages of these factors rather than the full matrix U t , thus saving considerable memory footprint.\nWe simply validate intuitions and the corresponding example is shown in Figure 4, in which the proposed CAME reaches the optimal point much faster than Adafactor. Learning rate is 10 \u22123 for all optimizers. In the example, we set the parameters of CAME to be the same as the default in Adafactor, \u03b2 1 = 0.9, \u03b2 2 = 0.999 and set extra \u03b2 3 = 0.9999 for CAME.", "publication_ref": ["b27"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Experiments", "text": "In this section, we present extensive comparisons with existing optimizers on training tasks of three important large language models: BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2018a) and T5 ", "publication_ref": ["b3", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "Dataset We perform experiments on the BookCorpus (Radford et al., 2018a) and English Wikipedia with 800M and 2.5B words respectively. Furthermore, we focus on the GLUE benchmark (Peters et al., 2018), SQuAD v1.1 dataset (Rajpurkar et al., 2016) and SQuAD v2.0 dataset (Rajpurkar et al., 2018) to demonstrate the performance of pre-trained BERT models with CAME optimizer.\nModel We evaluate the efficiency of our proposed CAME on three trending large language models: BERT, GPT-2 and T5. We further test the performance of CAME for large-batch training with BERT-Large.\nCompared methods The main baselines comprise two widely-used optimizers: classic optimizer Adam and memory-efficient optimizer Adafactor. With regard to large-batch training, LAMB optimizer is additionally considered when setting baselines.\nImplementation Detail We implement our optimization algorithm in Pytorch (Paszke et al., 2019). The parameters \u03b2 1 and \u03b2 2 in Algorithm 2 are set as 0.9 and 0.999 respectively, and we search for optimal \u03b2 3 among {0.9, 0.99, 0.999, 0.9999, 0.99999}. We use 8 Tesla V-100 GPUs and set \u03f5 1 , \u03f5 2 as 10 \u221230 , 10 \u221216 in all experiments with gradient accumulation and model parallelism. Besids, we set \u03b7 as 2 \u00d7 10 \u22124 , 6 \u00d7 10 \u22124 , 3 \u00d7 10 \u22124 for BERT-Large (32K), GPT-2, T5 training and apply learning rate warmup scheduling (Goyal et al., 2017b) to avoid divergence due to the large learning rate, by starting with a smaller learning rate \u03b7 and gradually increasing to the large learning rate \u03b7. To make sure we are comparing with solid baselines, we use grid search to tune the hyperparameters for Adafactor, Adam and LAMB. We further improve the performance of large-batch training by applying Mixup (Zhang et al., 2017) to scale the batch size up to 32,768.", "publication_ref": ["b20", "b24", "b23", "b18", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "BERT Training", "text": "We firstly present empirical results in the training task of BERT model to evaluate the performance of our proposed CAME optimizer, focusing on its larger variant, BERT-Large, which has 340M parameters in all. Following the default setting, we pre-train the BERT-Large model (L = 24, H = 1024) with a sequence length of 128 on 8 Tesla memory footprint with slight training performance degradation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Memory Usage Comparison", "text": "We set batch size to 1 to measure the memory usage of each optimizer more efficiently. As shown in Table 1, the two optimizers (Adam and LAMB) frequently employed for training large language models consume the highest amount of memory usage. Meanwhile, our proposed CAME optimizer exhibits a reduced memory footprint over the existing SM3 memory-efficient optimizer. As a consequence of our confidence-guided strategy in CAME, there is no doubt that CAME will introduce an increased memory footprint in comparison with Adafactor. However, the extra memory footprint incurred of CAME is almost negligible (1%) with a substantial performance improvement.\nFor further demonstration of the memory saving effect of CAME, we expand BERT model to BERT-4B with 4 billion weights using the scaling method of GPT-3 (Brown et al., 2020). We set the mini-batch size to 64 and the accumulation steps to 16 in this experiment. In Figure 7, we train BERT-4B with three different optimizers using PyTorch framework. As a result, CAME can save 47% memory footprint about optimizer states compared with Baseline (Adam) when the weights number of a model get to 4 billion.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Downstream Tasks", "text": "We select a representative set of downstream tasks to further demonstrate the performance of BERT models pre-trained by our proposed CAME. In this part we adopt BERT-Base model for the fine-tuning task and follow the originally published BERT- Base results in (Devlin et al., 2019) and (Liu et al., 2019) as the main baseline. The learning rate is tuned on the dev set for each setting and each task is fine-tuned for three epochs.\nWe compare the end-task performance of BERT-Base with the baseline on typical downstream tasks and the empirical results are presented in Table 2. The experimental results demonstrate the efficiency of our proposed CAME optimizer by showing that BERT-Base model trained with CAME on two batch sizes both achieve comparable performance to the baseline with less memory cost. In particular, we observe that BERT-Base model trained with large batch (32k) presents no performance degradation and even attains higher evaluation metrics scores on some downstream tasks. Specifically, the BERT-Base model trained on CAME improves on average by 0.5 across five metrics compared to the baseline, proving the feasibility of CAME for the large-batch training task.", "publication_ref": ["b3", "b16"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "GPT-2 Training", "text": "In addition to BERT pre-training task, we perform CAME-based training task on another typical large language model, GPT-2. Using the original structure of GPT-2 (Radford et al., 2018b), we specifically adopt GPT-medium (L = 24, H = 1024) with 345M parameters in our experiment. This implementation is based on the code provided by Megatron 3 . Identically, we take English Wikipedia as the training dataset for this section. Unlike the pre-training of BERT in Section 4.2, we only concentrate on standard training batch size (128) for GPT-2 pre-training.\nThe empirical results of validation loss are shown in Figure 8. We are able to find that CAME achieves similar convergence and final accuracy compared to Adam, which reveals an impressive improvement over the performance of Adafactor Steps(K) with comparable training steps. Moreover, as indicated in Figure 9, the validation perplexity of CAME presents the same convergence performance as Adam but faster convergence speed than Adafactor, which clearly supports the validity of CAME that has fast convergence as in traditional adaptive methods and low memory usage as in existing memory-efficient methods. For instance, the converged validation perplexity of CAME and Adafactor is 50.1 and 56.9 respectively, which yields a considerable improvement of 12.0%.", "publication_ref": ["b21"], "figure_ref": ["fig_5", "fig_6"], "table_ref": []}, {"heading": "T5 Training", "text": "Finally, we report empirical results from a different large language model training task: Text-to-Text Transfer Transformer, T5. Concretely, we follow the architecture of T5 (Raffel et al., 2022)   The comparison of CAME with Adafactor and Adam is conducted in the same manner as Section 4.4, and corresponding results of validation loss and validation perplexity are illustrated in Figure 10 and Figure 11 seperately. Note that CAME consistently obtains comparable convergence performance for validation loss and validation perplexity on par with Adam, while reducing similar memory usage as Adafactor.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper we propose a novel memory-efficient optimizer called CAME, which supports adaptive confidence-based updating guided by the residual between predicted update and generated update. CAME achieves a considerable improvement compared to existing memory-efficient optimizers in the training of large language models, with an ignorable extra memory footprint. Moreover, CAME shows comparable convergence to Adam and LAMB with huge memory reduction. In particular, CAME has proven effective for large-batch training, which serves as an advantageous extension to memory-efficient optimizers. We hope our work will provide insight into memory reduction of optimizers in future exploration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Despite the success of our CAME optimizer in training large language models with memory efficiency, there are still some limitations that need to be addressed in the future.\nOur proposed memory-efficient optimizer introduces additional computation costs for the nonnegative matrix factorization of the instability matrix in comparison with Adafactor. We observe, however, that the training time of CAME increases only slightly in our experiments. Beyond that, CAME exhibits minor performance degradation in large-batch training of the BERT-Large model versus LAMB, which allows for further improvement in the future. Meanwhile, it is possible to conduct further experiments on other models in other fields, such as Computer Vision and Reinforcement Learning, thereby exploring the effectiveness of CAME training under more application scenarios. As a final point, it would be much more helpful to provide an in-depth theoretical analysis of CAME to improve comprehensiveness of the paper. B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? section 4. B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? Not applicable. Left blank.\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? Not applicable. Left blank. B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Left blank.", "text": "C Did you run computational experiments? section 4.\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? section 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Yang You's research group is being sponsored by NUS startup grant (Presidential Young Professorship), Singapore MOE Tier-1 grant, ByteDance grant, ARCTIC grant, SMI grant and Alibaba grant. We also thank Huawei Noah's Ark Lab for providing the necessary computing resources and support for datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Memory-Efficient Adaptive Optimization", "journal": "Curran Associates Inc", "year": "2019", "authors": "Rohan Anil; Vineet Gupta; Tomer Koren; Yoram Singer"}, {"ref_id": "b1", "title": "Language models are few-shot learners", "journal": "Curran Associates Inc", "year": "2020", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan"}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton; Parker Gehrmann; Kensen Schuh; Sasha Shi; Joshua Tsvyashchenko;  Maynez"}, {"ref_id": "b3", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b4", "title": "Accurate, large minibatch SGD: training imagenet in 1 hour", "journal": "", "year": "2017", "authors": "Priya Goyal; Piotr Doll\u00e1r; Ross B Girshick; Pieter Noordhuis; Lukasz Wesolowski; Aapo Kyrola; Andrew Tulloch"}, {"ref_id": "b5", "title": "Yangqing Jia, and Kaiming He. 2017b. Accurate, large minibatch sgd: Training imagenet in 1 hour", "journal": "", "year": "", "authors": "Priya Goyal; Piotr Doll\u00e1r; Ross Girshick; Pieter Noordhuis; Lukasz Wesolowski; Aapo Kyrola; Andrew Tulloch"}, {"ref_id": "b6", "title": "Neural networks: a comprehensive foundation", "journal": "Prentice Hall PTR", "year": "1994", "authors": "Simon Haykin"}, {"ref_id": "b7", "title": "", "journal": "", "year": "", "authors": "Xiaoxin He; Fuzhao Xue; Xiaozhe Ren"}, {"ref_id": "b8", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks", "journal": "", "year": "2017", "authors": "Elad Hoffer; Itay Hubara; Daniel Soudry"}, {"ref_id": "b9", "title": "Large batch optimization for deep learning using new complete layer-wise adaptive rate scaling", "journal": "", "year": "2021", "authors": "Zhouyuan Huo; Bin Gu; Heng Huang"}, {"ref_id": "b10", "title": "On large-batch training for deep learning: Generalization gap and sharp minima", "journal": "", "year": "2016", "authors": "Dheevatsa Nitish Shirish Keskar;  Mudigere"}, {"ref_id": "b11", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015-05-07", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b12", "title": "Algorithms for non-negative matrix factorization", "journal": "MIT Press", "year": "2000", "authors": "Daniel Lee; H Sebastian Seung"}, {"ref_id": "b13", "title": "Learning the parts of objects by nonnegative matrix factorization", "journal": "Nature", "year": "1999", "authors": "D Daniel; H Sebastian Lee;  Seung"}, {"ref_id": "b14", "title": "Samyam Rajbhandari, and Yuxiong He. 2021. 1-bit lamb: Communication efficient large-scale largebatch training with lamb's convergence speed", "journal": "", "year": "", "authors": "Conglong Li; Ammar Ahmad Awan; Hanlin Tang"}, {"ref_id": "b15", "title": "Robust training of neural networks using scale invariant architectures", "journal": "PMLR", "year": "2022", "authors": "Zhiyuan Li; Srinadh Bhojanapalli; Manzil Zaheer; Sashank Reddi; Sanjiv Kumar"}, {"ref_id": "b16", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b17", "title": "", "journal": "", "year": "", "authors": "Yong Liu; Siqi Mai; Xiangning Chen; Cho-Jui Hsieh"}, {"ref_id": "b18", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "Curran Associates, Inc", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga; Alban Desmaison; Andreas Kopf; Edward Yang; Zachary Devito; Martin Raison; Alykhan Tejani; Sasank Chilamkurthy; Benoit Steiner; Lu Fang; Junjie Bai; Soumith Chintala"}, {"ref_id": "b19", "title": "Deep contextualized word representations", "journal": "", "year": "2017", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"ref_id": "b20", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2018", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b21", "title": "Language models are unsupervised multitask learners", "journal": "", "year": "2018", "authors": "Alec Radford; Jeffrey Wu; Rewon Child; David Luan; Dario Amodei; Ilya Sutskever"}, {"ref_id": "b22", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2022", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b23", "title": "Know what you don't know: Unanswerable questions for squad", "journal": "", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b24", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b25", "title": "", "journal": "", "year": "", "authors": "Christopher J Shallue; Jaehoon Lee; Joseph Antognini; Jascha Sohl-Dickstein; Roy Frostig; George E "}, {"ref_id": "b26", "title": "Measuring the effects of data parallelism on neural network training", "journal": "", "year": "2018", "authors": " Dahl"}, {"ref_id": "b27", "title": "Adafactor: Adaptive learning rates with sublinear memory cost", "journal": "ArXiv", "year": "2018", "authors": "Noam M Shazeer; Mitchell Stern"}, {"ref_id": "b28", "title": "Large batch training of convolutional networks", "journal": "", "year": "2017", "authors": "Yang You; Igor Gitman; Boris Ginsburg"}, {"ref_id": "b29", "title": "Large batch optimization for deep learning: Training bert in 76 minutes", "journal": "", "year": "2019", "authors": "Yang You; Jing Li; Sashank Reddi; Jonathan Hseu; Sanjiv Kumar; Srinadh Bhojanapalli; Xiaodan Song; James Demmel; Kurt Keutzer; Cho-Jui Hsieh"}, {"ref_id": "b30", "title": "", "journal": "", "year": "2017", "authors": "Yang You; Zhao Zhang; Cho-Jui Hsieh; James Demmel; Kurt Keutzer"}, {"ref_id": "b31", "title": "mixup: Beyond empirical risk minimization", "journal": "", "year": "2017", "authors": "Hongyi Zhang; Moustapha Cisse; Yann N Dauphin; David Lopez-Paz"}, {"ref_id": "b32", "title": "Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. Conference on Neural Information Processing Systems", "journal": "", "year": "2020", "authors": "Juntang Zhuang; Tommy Tang; Yifan Ding; Sekhar Tatikonda; Nicha Dvornek; Xenophon Papademetris; James Duncan"}, {"ref_id": "b33", "title": "Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? section 4", "journal": "", "year": "", "authors": ""}, {"ref_id": "b34", "title": "error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run", "journal": "", "year": "", "authors": ""}, {"ref_id": "b35", "title": "for preprocessing, for normalization, or for evaluation", "journal": "", "year": "", "authors": " Nltk;  Spacy;  Rouge"}, {"ref_id": "b36", "title": "crowdworkers) or research with human participants? Left blank", "journal": "", "year": "", "authors": ""}, {"ref_id": "b37", "title": "Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators", "journal": "", "year": "", "authors": " D1"}, {"ref_id": "b38", "title": "crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic", "journal": "", "year": "", "authors": ""}, {"ref_id": "b39", "title": "Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?", "journal": "", "year": "", "authors": " D3"}, {"ref_id": "b40", "title": "Was the data collection protocol approved (or determined exempt) by an ethics review board? No response", "journal": "", "year": "", "authors": " D4"}, {"ref_id": "b41", "title": "Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response", "journal": "", "year": "", "authors": " D5"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Loss landscape visualization for erroneous update of Adafactor in 1-layer multilayer perceptron (MLP) (Haykin, 1994) with same training steps. Adafactor deviates from the training curve of Adam.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Visualization of two scenarios where Adafactor updates have different stability.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Loss trajectories of Adafactor and CAME. CAME reaches the target local minimum (marked as green cross in 2D plots) much faster than Adafactor.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Masked LM test accuracy of BERT-Large model trained on Wikipedia dataset with 8k batch size.", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: Masked LM test accuracy of BERT-Large model trained on Wikipedia dataset with 32k batch size. CAME achieves comparable accuracy with Adafactor using around only half of required training steps (10k).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Validation loss of GPT-2 language model. CAME demonstrates similar optimization performance to Adam.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 9 :9Figure 9: Validation perplexity of GPT-2 language model. CAME demonstrates comparable convergence speed with Adam.", "figure_data": ""}, {"figure_label": "1011", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 10 :Figure 11 :1011Figure 10: Validation loss of T5 language model. CAME exhibits similar convergence rates to Adam.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "you describe the limitations of your work? section 6. A2. Did you discuss any potential risks of your work? section 6. A3. Do the abstract and introduction summarize the paper's main claims? section 1. A4. Have you used AI writing assistants when working on this paper? Left blank. B Did you use or create scientific artifacts? section 4. B1. Did you cite the creators of artifacts you used? section 4.B2. Did you discuss the license or terms for use and / or distribution of any artifacts? section 4.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "CAME achieves fast convergence speed as Adam without degrading of performance. Notably, in the large-batch training of the BERT model, CAME obtains comparable validation accuracy with LAMB using around 15% less memory usage.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quantitative memory usage per GPU (GB) comparison in the pre-training of BERT-Large model.", "figure_data": "Optimizer Memory Cost (GB)Adam8.24LAMB8.23Adafactor7.00SM37.44CAME7.07"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Results of fine-tuning performance on MNLI-m, SST-2, MRPC and two SQuAD datasets. The F1 and EM for SQuAD v1.1 dataset are firstly averaged, and the average of all results across five datasets is further calculated.", "figure_data": "ModelMNLI-m SST-2 MRPC SQuAD v1.1 SQuAD v2.0 (Acc) (Acc) (Acc) (F1/EM) (F1)Average -Baseline84.392.888.988.5/80.876.385.4CAME (batch size = 8k)84.892.889.988.8/81.877.986.1 (+0.7)CAME (batch size = 32k)84.592.989.888.5/81.277.485.9 (+0.5)020406080100120"}], "formulas": [{"formula_id": "formula_0", "formula_text": "u t = u t max(1, RM S(u t )/d) (1)", "formula_coordinates": [3.0, 119.4, 619.02, 170.47, 26.68]}, {"formula_id": "formula_1", "formula_text": "W = V 1 m , H = 1 T n V 1 T n V 1 m .(2)", "formula_coordinates": [3.0, 351.0, 387.77, 174.15, 29.49]}, {"formula_id": "formula_2", "formula_text": "\u03b2 1 , \u03b2 2 , clipping threshold d while \u03b8 t not converge do Compute g t = \u2207f (\u03b8 t\u22121 ) r t = \u03b2 2 r t\u22121 + (1 \u2212 \u03b2 2 )(g 2 t + \u03f5 1 1 n 1 T m )1 m c t = \u03b2 2 c t\u22121 + (1 \u2212 \u03b2 2 )1 T n (g 2 t + \u03f5 1 1 n 1 T m ) v t = r t c t /1 T n r t u t = g t / \u221a v t u t = u t /max(1, RM S(u t )/d) m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )\u00fb t \u03b8 t = \u03b8 t\u22121 \u2212 \u03b7m t end", "formula_coordinates": [4.0, 81.37, 102.33, 383.02, 147.39]}, {"formula_id": "formula_3", "formula_text": "u \u2032 t = m t (m t \u2212 u t ) 2 + \u03f5 (3)", "formula_coordinates": [4.0, 362.83, 307.57, 162.32, 35.09]}, {"formula_id": "formula_4", "formula_text": "\u03b2 1 , \u03b2 2 , \u03b2 3 , clipping threshold d while \u03b8 t not converge do Compute g t = \u2207f (\u03b8 t\u22121 ) r t = \u03b2 2 r t\u22121 + (1 \u2212 \u03b2 2 )(g 2 t + \u03f5 1 1 n 1 T m )1 m c t = \u03b2 2 c t\u22121 + (1 \u2212 \u03b2 2 )1 T n (g 2 t + \u03f5 1 1 n 1 T m ) v t = r t c t /1 T n r t u t = g t / \u221a v t u t = u t /max(1, RM S(u t )/d) m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )\u00fb t U t = (\u00fb t \u2212 m t ) 2 R t = \u03b2 3 R t\u22121 + (1 \u2212 \u03b2 3 )(U t + \u03f5 2 1 n 1 T m )1 m C t = \u03b2 3 C t\u22121 + (1 \u2212 \u03b2 3 )1 T n (U t + \u03f5 2 1 n 1 T m ) S t = R t C t /1 T n R t \u03b8 t = \u03b8 t\u22121 \u2212 \u03b7 \u221a St m t end", "formula_coordinates": [5.0, 81.37, 102.33, 409.74, 218.93]}], "doi": "10.18653/v1/N19-1423"}