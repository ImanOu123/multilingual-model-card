{"title": "On-Demand Sampling: Learning Optimally from Multiple Distributions *", "authors": "Nika Haghtalab; Michael I Jordan; Eric Zhao", "pub_date": "2024-04-02", "abstract": "Social and real-world considerations such as robustness, fairness, social welfare and multi-agent tradeoffs have given rise to multi-distribution learning paradigms, such as collaborative [9], group distributionally robust [50], and fair federated learning [39]. In each of these settings, a learner seeks to uniformly minimize its expected loss over n predefined data distributions, while using as few samples as possible. In this paper, we establish the optimal sample complexity of these learning paradigms and give algorithms that meet this sample complexity. Importantly, our sample complexity bounds exceed that of learning a single distribution by only an additive factor of n log(n) \u03b5 2 . This improves upon the best known sample complexity bounds for fair federated learning (by Mohri et al. [39]) and collaborative learning (by Nguyen and Zakynthinou [42]) by multiplicative factors of n and log(n) \u03b5 3 , respectively. We also provide the first sample complexity bounds for the group DRO objective of Sagawa et al. [50]. To guarantee these optimal sample complexity bounds, our algorithms learn to sample from data distributions on demand. Our algorithm design and analysis are enabled by our extensions of online learning techniques for solving stochastic zero-sum games. In particular, we contribute stochastic variants of no-regret dynamics that can trade off between players' differing sampling costs.", "sections": [{"heading": "Introduction", "text": "Pervasive needs for robustness, fairness, and multi-agent collaboration in learning have given rise to multidistribution learning paradigms (e.g., [9,50,39,18]). In these settings, we seek to learn a model that performs well on any distribution in a predefined set of interest. For fairness considerations, these distributions may represent heterogeneous populations of different protected or socioeconomic attributes; in robustness applications, they may capture a learner's uncertainty regarding the true underlying task; and in multi-agent collaborative or federated applications, they may represent agent-specific learning tasks. In these applications, the performance and optimality of a model is measured by its worst test-time performance on a distribution in the set. We are concerned with this fundamental problem of designing sample-efficient multi-distribution learning algorithms.\nThe sample complexity of multi-distribution learning differs from that of learning a single distribution in several ways. On one hand, varying numbers of samples are required when learning tasks of varying difficulty. On the other hand, similarity or overlap among learning tasks may obviate the need to sample from some distributions. This makes the use of a fixed per-distribution sample budget highly inefficient and suggests that optimal multi-distribution learning algorithms should sample on demand. That is, algorithms should take additional samples whenever they need them and from whichever data distribution they want them. On-demand sampling is especially appropriate when some population data is scarce (as in fairness mechanisms in which samples are amended [46]); when the designer can actively perturb datasets towards rare or atypical instances (such as in robustness applications [29,59]); or when sample sets represent agents' contributions to an interactive multi-agent system [39,10].", "publication_ref": ["b8", "b49", "b38", "b17", "b45", "b28", "b58", "b38", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Sample Complexity", "text": "Thm Best Previous Result Collab. Learning UB \u03b5 \u22122 (log(|H|) + n log(n/\u03b4)) [5.1] \u03b5 \u22125 log( 1 \u03b5 ) log(n/\u03b4)(log(|H|) + n) [42] Collab. Learning LB \u03b5 \u22122 (log(|H|) + n log(n/\u03b4)) [5.3] \u03b5 \u22121 (log(|H|) + n log(n/\u03b4)) [9] GDRO/AFL UB \u03b5 \u22122 (log(|H|) + n log(n/\u03b4)) [5.1] \u03b5 \u22122 (n log(|H|) + n log(n/\u03b4)) [39] GDRO/AFL UB \u03b5 \u22122 (DH + n log(n/\u03b4)) [6.1] N/A\n(Training error convg.) \u03b5 \u22122 (DH + n log(n/\u03b4)) [6.2] \u03b5 \u22122 n(log(n) + DH) (expected convergence only) [50] Table 1: This table lists upper (UB ) and lower bounds (LB ) on the sample complexity of learning a model class H on n distributions. For the collaborative learning and agnostic federated learning (AFL) settings, the sample complexity upper bounds refer to the problem of learning a (potentially randomized) model whose expected loss on each distribution is at most OPT + \u03b5, where OPT is the best possible such guarantee. For the GDRO setting, sample complexity refers to learning a deterministic model with expected losses of at most OPT + \u03b5, from a convex compact model space H with a Bregman radius of DH. Sample complexity bounds for collaborative and agnostic federated learning in existing works extend to VC dimension and Rademacher complexity. Our results also extend to VC dimension under some assumptions.\nBlum et al. [9] demonstrated the benefit of on-demand sampling in the collaborative learning setting, when all data distributions are realizable with respect to the same target classifier. This line of work established that learning n distributions with on-demand sampling requires a factor of O(log(n)) times the sample complexity of learning a single realizable distribution [9,13,42], whereas relying on batched uniform convergence takes \u2126(n) times more samples than learning a single distribution [9]. However, beyond the realizable setting, the best known multi-distribution learning results fall short of this promise: existing on-demand sample complexity bounds for agnostic collaborative learning have highly suboptimal dependence on \u03b5, requiring O(log(n)/\u03b5 3 ) times the sample complexity of agnostically learning a single distribution [42]. On the other hand, agnostic fair federated learning bounds [39] have been studied only for algorithms that sample in one large batch and thus require \u2126(n) times the sample complexity of a single learning task. Moreover, the test-time performance of some key multi-distribution learning methods, such as group distributionally robust optimization [50], have not been studied from a provable or mathematical perspective before.\nIn this paper, we give a general framework for obtaining optimal and on-demand sample complexity for three multi-distribution learning settings. Table 1 summarizes our results. All three of these settings consider a set D of n data distributions and a model class H, evaluating the performance of a model h by its worst-case expected loss, max D\u2208D R D (h). As a benchmark, they consider the worst-case expected loss of the best model, i.e., OPT = min h * \u2208H max D\u2208D R D (h * ). Notably, all of our sample complexity upper bounds demonstrate only an additive increase of \u03b5 \u22122 n log(n/\u03b4) over the sample complexity of a single learning task, compared to the multiplicative factor increase required by existing works.\n-Collaborative learning of Blum et al. [9]: For agnostic collaborative learning, our Theorem 5.1 gives a randomized and a deterministic model that achieves performance guarantees of OPT + \u03b5 and 2OPT + \u03b5, respectively. Our algorithms have an optimal sample complexity of O( 1 \u03b5 2 (log(|H|) + n log(n/\u03b4))). This improves upon the work of Nguyen and Zakynthinou [42] in two ways. First, it provides risk bounds of OPT + \u03b5 for randomized classifiers, where only 2OPT + \u03b5 was established previously. Second, it improves the upper bound of Nguyen and Zakynthinou [42] by a multiplicative factor of log(n)/\u03b5 3 . In Theorem 5.3, we give a matching lower bound on this sample complexity, thereby establishing the optimality of our algorithms.\n-Group distributionally robust learning (group DRO) of Sagawa et al. [50]: For group DRO, we consider a convex and compact model space H. Our Theorem 6.1 studies a model that achieves an OPT + \u03b5 guarantee on the worst-case test-time performance of the model with an on-demand sample complexity of O 1 \u03b5 2 (D H + n log(n/\u03b4)) . Our results also imply a high-probability bound for the convergence of group DRO training error that improves upon the (expected) convergence guarantees of Sagawa et al. [50] by a factor of n.\n-Agnostic federated learning of [39]: For agnostic federated learning, we consider a finite class of hypotheses. Our Theorems 5.1 and 6.1 show that on-demand sampling can accelerate the generalization of agnostic federated learning by a factor of n compared to batch results established by Mohri et al. [39]. Our results also imply matching high-probability bounds with respect to Mohri et al. [39] on the convergence of the training error in the batched setting.\nTo achieve these results, we frame multi-distribution learning as a stochastic zero-sum game: a maximizing player chooses a weight vector over data distributions D and a minimizing player chooses a weight vector over hypotheses H. These two players require different numbers of datapoints in order to estimate their respective payoff vectors. We therefore solve the game using no-regret dynamics, utilizing stochastic mirror descent to optimally trade off the players' asymmetric needs for datapoints. In Section 3, we give an overview of this approach and its technical challenges and contributions. Our results also extend directly to settings with not only multiple data distributions but also multiple loss functions.", "publication_ref": ["b41", "b8", "b38", "b49", "b8", "b8", "b12", "b41", "b8", "b41", "b38", "b49", "b8", "b41", "b41", "b2", "b49", "b49", "b38", "b38", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "There are many lines of work that study multi-distribution learning but which have evolved independently in separate communities.\nCollaborative and agnostic federated learning. Blum, Haghtalab, Procaccia, and Qiao [9] posed the first fully general description of multi-distribution learning, motivated by the application of collaborative PAC learning. The field of collaborative learning is concerned with the learning of a shared machine learning model by multiple stakeholders that each desire a model with low error on their own data distribution. The line of work studies on-demand sample complexity bounds for the setting where stakeholders collect data so as to minimize the error of the worst-off stakeholder [9,42,13,11]. This setting, stated in its full generality, yields the multi-distribution learning problem as presented in this paper. Blum et al. [9] established a log(n) factor blowup for the realizable case. For the general agnostic setting the best existing sample complexity requires a factor log(n)/\u03b5 3 blowup [42]. In comparison, our work establishes a tight additive increase in the sample complexity (which is comparable to log(n) multiplicative factor blowup with no dependence on \u03b5). A related line of work concerns the strategic considerations of collaborative learning and seeks incentive-aware mechanisms for collecting data in the collaborative learning setting [10].\nThe field of federated learning focuses on a related motivating application where the goal is to learn a model from data dispersed across multiple devices but where querying data from each device is expensive [38]. The agnostic federated learning framework of Mohri, Sivek, and Suresh [39] poses (a variant of) the multidistribution learning objective as a target for federated learning algorithms, and studies it in the offline setting with a data-dependent analysis. Their results involve a blowup by a factor n for the sample complexity.\nGroup distributionally robust optimization (Group DRO). Multi-distribution learning also arises in distributionally robust optimization [8] under the name of Group DRO, a class of DRO problems where the distributional uncertainty set is finite [24]. The group DRO literature is motivated by applications where the distributions correspond to deployment domains or protected demographics that a machine learning model should avoid spuriously linking to labels [24,50,51]. Although Group DRO-like collaborative learning-is mathematically an instance of multi-distribution learning, prior work on Group DRO focuses on the convergence of training error in offline settings, with a particular focus on deep learning applications. As we discuss later, theoretical aspects of on-demand multi-distribution learning can translate into actionable insights for Group DRO applications.\nMulti-group fairness. Multi-distribution learning is also related to the fields of multi-group learning [49,53] and multi-group fairness [19,27]. These works study offline learning settings with a single distribution D and implicitly consider distribution D i to be the conditional distribution on a subset of the support representing group i. In these settings, the learner does not have explicit access to oracles that sample from distributions D 1 , . . . , D n and instead uses rejection sampling to collect data from D 1 , . . . , D n . As a result, they experience a sub-optimal sample complexity blowup by a factor n. This blowup may not be obvious upon first glance, as these works provide theoretical guarantees for each group in terms of the number of datapoints from that group. Multi-group learning [49,53] considers a similar problem to multi-distribution learning; by assuming that there exists a hypothesis that is simultaneously \u03b5-optimal on every distribution (an assumption not made in our setting) they compare their learned hypothesis against the best hypothesis for each individual distribution.\nMulti-source domain adaptation. Multi-source domain adaptation, or multi-task learning, is another related line of work that is concerned with using data from multiple different training distributions to learn some target distribution, under the assumption that the training and target distributions share some task relatedness [7,36]. Multi-distribution learning can be framed similarly as using a finite set of training distributions to simultaneously learn the convex hull of the training distributions. Interestingly, the requirement in the multi-distribution setting of learning the entire convex hull obviates the need for the task-relatedness assumptions of multi-source learning.\nStochastic game equilibria. Our approach relates to a line of research on using online algorithms to find min-max equilibria by playing no-regret algorithms against one another [48,21,45,14,15]. Online mirror descent (OMD) is a well-studied family of methods that can find approximate minima of convex functions, and also find approximate min-max equilibria of convex-concave games, with high probability, using noisy first-order information [47,40,23,6]. We bring these online learning tools to bear on the problem of finding saddle points in robust optimization formulations. The primary technical difference between multi-distribution learning and traditional saddle-point optimization problems is that we have sample access to data distributions instead of noisy local gradients.\nOther paradigms. Several other machine learning paradigms also consider learning from multiple distributions. Notably, distributed learning (e.g., [44,12,5,16,52]) and federated learning (e.g., [32,31,38]) consider learning from data that is spread across multiple sources or devices. Classically, both of these settings have focused on minimizing the training or testing error averaged over these devices. The literature in these fields has primarily focused on methods for minimizing the average loss using communication-efficient, private, and robust-to-dropout training methods. However, optimizing average performance produces models that can significantly underperform on some data sources, especially when the data is heterogeneously spread across the sources. In comparison, multi-distribution learning paradigms such as collaborative learning [9], agnostic federated learning [39], and Group DRO [50] learn models that perform well across any one of the data sources. Subsequent work. Haghtalab et al. [22] formalized multicalibration as a type of multi-distribution learning, building on the framework presented in this manuscript. Their work improves upon state-of-art multicalibration algorithms by implementing multi-distribution learning game dynamics using online learning algorithms that leverage the structure of calibration losses. Zhang et al. [61] extended the discussion on the sample complexity of Group DRO to settings with data budgets. They also noted an erroneous bandit-tofull-information reduction in an earlier version of this manuscript, which we corrected in a previous version (arXiv V2) with a minor change that employs Exp3 [41] or ELP [1] in place of our earlier reduction. Awasthi et al. [4] presented steps towards answering the sample complexity of multi-distribution learning with VC classes. This open problem was recently settled up to log factors by Zhang et al. [62], Peng [43].", "publication_ref": ["b8", "b8", "b41", "b12", "b10", "b8", "b41", "b9", "b37", "b38", "b7", "b23", "b23", "b49", "b50", "b48", "b52", "b18", "b26", "b48", "b52", "b6", "b35", "b47", "b20", "b44", "b13", "b14", "b46", "b39", "b22", "b5", "b43", "b11", "b4", "b15", "b51", "b31", "b30", "b37", "b8", "b38", "b49", "b21", "b60", "b40", "b0", "b3", "b61", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "Throughout this manuscript, we use the shorthands x (1:T ) := x (1) , . . . , x (T ) and f (\u2022, b) := a \u2192 f (a, b). We write \u2206(A) to denote the set of probability distributions supported on a set A and \u2206 d to denote the probability simplex in R d\u22121 . We use \u2225\u2022\u2225 * to denote the dual of the norm \u2225\u2022\u2225 and e i \u2208 R n to denote the ith standard basis vector. Given a data distribution D supported on the space of datapoints Z, hypothesis class H, and a loss function \u2113 : H \u00d7 Z \u2192 [0, 1], we denote the expected loss (risk) of a hypothesis h \u2208 H by R D,\u2113 (h) := E z\u223cD [\u2113(h, z)], writing R D (h) if \u2113 is clear from context.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Distribution Learning", "text": "The goal of multi-distribution learning is finding a hypothesis that uniformly minimizes expected loss across multiple data distributions and loss functions. Importantly, we make no assumptions on the relationships between the data distributions; for example, we do not assume the existence of a hypothesis that is simultaneously optimal for every distribution. Formally, given a set of data distributions D = {D i } n i=1 , losses L = {\u2113 j } m j=1 , and a hypothesis class H, we say a hypothesis h is \u03b5-optimal for the multi-distribution learning problem (D, L, H) if\nmax D\u2208D max \u2113\u2208L R D,\u2113 (h) \u2264 OPT + \u03b5, where OPT := min h\u2208H max D\u2208D max \u2113\u2208L R D,\u2113 (h).(1)\nThroughout this manuscript, we will often assume we are working with smooth and convex loss functions. Formally, we say a multi-distribution learning problem (D, L, H) has smooth convex losses if two conditions are met. First, H is parameterized by a convex compact Euclidean parameter space \u0398 such that H = {h \u03b8 } \u03b8\u2208\u0398 . Second, for the same parameter space \u0398, for every loss function \u2113 \u2208 L and datapoint z \u2208 Z, the mapping f : \u0398 \u2192 [0, 1] defined as f (\u03b8) = \u2113(h \u03b8 , z) is convex and 1-smooth; i.e., \u2225\u2207 \u03b8 f (\u03b8)\u2225 \u2264 1 for all \u03b8 \u2208 \u0398. We remark that the assumption of smooth convex losses is a weak assumption. In fact, we will observe that our results on smooth convex losses easily extend to bounded non-smooth non-convex losses when the hypothesis class H is finite or combinatorially bounded, such as when H has finite VC dimension or Littlestone dimension [33].\nSample complexity. We are interested in the design of multi-distribution learning algorithms that have sample access to the distributions D 1 , . . . , D n and only take a small number of samples from these distributions overall. We formalize this access by defining a set of example oracles, EX(D 1 ), . . . , EX(D n ), where each EX(D i ) returns i.i.d. samples from D i . We can then define the sample complexity of a multi-distribution learning algorithm by the cumulative number of calls it makes to these example oracles in order to find a solution.\nWe note that a multi-distribution learning algorithm may make these example oracle calls in an adaptive fashion; i.e., choosing which example oracle to call based on the datapoints it received from previous oracle calls. As first noted by Blum et al. [9], this ability to query for samples on-demand is critical for achieving efficient multi-distribution learning sample complexities. We also note that multi-distribution algorithms can use a set of example oracles to sample from any mixture distribution q \u2208 \u2206D; e.g., by first sampling a supporting distribution D i from the mixture distribution and then calling its example oracle EX(D i ).", "publication_ref": ["b32", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Instances of Multi-Distribution Learning", "text": "Multi-distribution learning unifies the problem formulations of collaborative learning [9], agnostic federated learning [39], and group distributionally robust optimization (group DRO) [50]. These problems have each spawned a line of highly influential works but were previously not recognized to be equivalent. We emphasize our view that multi-distribution learning is a particularly useful level of generality at which to study these problems, as it allows for their unified treatment both conceptually and algorithmically.\nCollaborative learning. In the collaborative PAC learning model of Blum et al. [9], and its agnostic extensions by Nguyen and Zakynthinou [42], the goal is to learn a hypothesis that guarantees small risk for every distribution in a collection of distributions. These data distributions are usually interpreted as the heterogeneous problem domains faced by multiple participants that are collaborating on data collection; the goal of collaborative learning is to learn a machine learning model that all participants are satisfied with.\nCollaborative learning is usually studied in a supervised learning setting where datapoints consist of a feature-label pair, i.e., Z = X \u00d7 Y, and where hypothesis classes H \u2282 Y X are either finite or combinatorially bounded. Importantly, loss functions are assumed to be bounded in [0, 1], but may be non-smooth and non-convex. Formally, given a set of data distributions, D := {D 1 , . . . , D n }, supported on X \u00d7 Y, a loss function \u2113 : Y X \u00d7 Z \u2192 [0, 1], and a hypothesis class H \u2282 Y X , a collaborative learning instance, (H, D), is formulated as the problem of finding a solution h \u2208 Y X such that (\nWe say a solution h is proper if it is in class, i.e., h \u2208 H, and randomized if h is a probability distribution supported on the class, i.e., h \u2208 \u2206(H). In the latter case, we define the expected loss for a randomized hypothesis as R D (h\n) := E f \u223ch [R D (f )].\nMulti-distribution learning with smooth convex losses and collaborative learning seem to differ significantly in terms of their formally definition. However, we can reduce any collaborative learning problem to multidistribution learning with smooth convex loss functions-as long as we allow for improper or randomized solutions to our collaborative learning problem. Allowing for improper or randomized solutions is not unreasonable and is in fact necessary to achieve non-trivial sample complexities in collaborative learning [9].\nThe first step to reducing collaborative learning to multi-distribution learning is to relax the optimization problem on the hypothesis class H onto the class of randomized hypotheses \u2206(H). \n\u2206(H) \u2192 [0, 1] as \u2113(h, z) = E f \u223ch [\u2113(f, z)].\nThe induced losses in the multi-distribution learning problem, (D, \u2113 , \u2206(H)), are smooth and convex, and any \u03b5-optimal solution h \u2208 \u2206(H) is also an \u03b5-optimal randomized solution to the collaborative learning problem (H, D); i.e., it satisfies Equation 2.\nThis fact implies that multi-distribution learning can solve for deterministic but improper solutions to collaborative learning problems. This is because we can always extract a deterministic solution from a non-deterministic solution h \u2208 \u2206(H) by taking a majority vote, where we denote the majority vote hypothesis as h Maj . The expected loss guarantee of this deterministic hypothesis is approximately bounded by that of the randomized h. We state this formally below for the setting where H is a set of binary classifiers; that is, where the label space is binary, Y = {0, 1}, and the loss function \u2113 can be written as \u2113(h, (x, y)) = g(h(x), y) for some choice of g : Group distributionally robust optimization. In the closely related setting of group distributionally robust optimization (group DRO) of Sagawa et al. [50], the goal is similarly to learn some hypothesis that guarantees small risk for every data distribution in a collection of distributions. In group DRO, the various data distributions are usually interpreted to either represent heterogeneous user populations and protected groups (for algorithmic fairness applications) or potential domains in which a model may be deployed (for robustness applications).\nY 2 \u2192 [0, 1].\nIn contrast to the collaborative learning problem, group DRO problems are typically studied in a convex optimization setting where the hypothesis class is parameterized by some convex set and the loss function is smooth and convex. That is, the usual definition of the group DRO problem setting coincides with the definition of multi-distribution learning with a single smooth convex loss, i.e., |L| = 1. Unlike in collaborative learning where we are interested in potentially improper or randomized solutions, the goal of group DRO is to learn a proper model h \u03b8 \u2208 H where\nmax D\u2208D R D (h \u03b8 ) \u2264 OPT + \u03b5, where OPT := min \u03b8 * \u2208\u0398 max D\u2208D R D (h \u03b8 * ).(3)\nIt is the convexity of the group DRO problem setting that allows for the efficient learning of proper solutions and avoid relaxation to randomized solutions.\nAgnostic federated learning. The agnostic federated learning framework of Mohri et al. [39] also coincides with multi-distribution learning with a single loss function. Like group distributionally robust optimization, agnostic federated learning is usually studied in a convex optimization setting with convex parameter spaces and smooth convex losses. As the general agnostic federated learning setting does not differ from group distributionally robust optimization in its formulation, we provide an identical treatment of both settings in Section 6.", "publication_ref": ["b8", "b38", "b49", "b8", "b41", "b8", "b49", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Technical Background", "text": "We will use tools and definitions from the literature on zero-sum games and no-regret learning throughout the paper. This section provides a brief overview of these concepts.\nZero-sum games. A two-player zero-sum game is described by the tuple (A \u2212 , A + , \u03d5) where A \u2212 , A + are convex compact action sets and \u03d5 :\nA \u2212 \u00d7 A + \u2192 [0, 1]\nis the game payoff. The player who chooses from A \u2212 is called the minimizing player and tries to minimize the game payoff \u03d5, while the player who chooses from A + is called the maximizing player. A pair of actions (p, q) is called an \u03b5-min-max equilibrium if neither player can unilaterally improve their objective by more than \u03b5; that is, \u03d5(p, q) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 \u03b5 and max q * \u2208A+ \u03d5(p, q * ) \u2212 \u03d5(p, q) \u2264 \u03b5. If \u03d5 is convex-concave-i.e., \u03d5(\u2022, q) is convex for every q \u2208 A + and \u03d5(p, \u2022) is concave for every p \u2208 A \u2212 -then an \u03b5-min-max equilibrium always exists for every \u03b5 \u2265 0. In the next section, we will describe methods that find \u03b5-min-max equilibria by playing online learning algorithms against each other, a technique known as no-regret game dynamics [21].\nNo-regret learning. A no-regret (or online) learning algorithm Q A maps from a sequence of costs c (1:t\u22121) to an action a (t) \u2208 A, where a (t) = Q A (c (1:t\u22121) ). Notationally, we use the subscript A when writing an online learning algorithm Q A to denote the action set that the algorithm Q A is defined to act on. Regret is defined for a sequence of actions a (1) , . . . , a (T ) \u2208 A and costs c (1) , . . . , c (T ) : A \u2192 [0, 1] as follows:\nReg(a (1:T ) , c (1:T ) ) := T t=1 c (t) (a (t) ) \u2212 min a * \u2208A T t=1 c (t) (a * ).\nWe say that a no-regret learning algorithm Q A has a regret guarantee of \u03b3 T (Q A ) if, for any sequence of linear cost functions c (1:T ) of bounded norm, i.e., max t\u2208[T ] c (t) \u2264 1, the algorithm Q A chooses an action sequence a (1:T ) with the regret bound Reg(a (1:T ) , c\n(1:T ) ) \u2264 \u03b3 T (Q A )T .\nExamples of no-regret algorithms on probability simplexes. A well-studied online learning setting is that in which the action set is a probability simplex,A = \u2206 n , and all costs are linear functions of bounded norm. In this setting, we can interpret online learning algorithms as choosing mixed strategies a (t) \u2208 \u2206 n over a set of meta-actions, {1, . . . , n}, and the adversary as assigning a cost c (t) (e 1 ), . . . , c (t) (e n ) to each meta-action, so that the algorithm incurs the cost E i\u223ca (t) c (t) (e i ) . An example of a no-regret algorithm in this setting is Exponential Gradient Descent (Hedge), defined as\nHedge A (c (1:t\u22121) ) := a (t) / a (t) 1\nwhere a \u2208 R n and a i := exp \u2212\u03b7\nt\u22121 \u03c4 =1 c (\u03c4 ) (e i ) ,(4)\nwhere \u03b7 \u2208 (0, 1) is a learning rate. The following lemma states a classical result for exponential gradient descent (Hedge), showing a regret guarantee of O (log(n)).", "publication_ref": ["b20", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 2.1 ([55]", "text": "). Let c (1:T ) be any linear cost sequence where max t\u2208[T ] c (t) \u221e \u2264 1 and A = \u2206 n . When \u03b7 = log(n/T ), the actions a (1:T ) chosen by Hedge satisfy Reg(a\n(1:T ) , c (1:T ) ) \u2264 2 log(n)/T .\nThere also exist partial feedback no-regret algorithms-also known as semi-bandit algorithms-that only need to observe the cost functions at each timestep for a few meta-actions (i.e., along a few basis vectors). We can formalize these partial feedback (semi-bandit) algorithms as returning not only an action a (t) \u2208 \u2206 n at each timestep t but also returning the meta-actions I (t) \u2286 [n] whose costs it will observe. We can therefore, somewhat unconventionally, write these algorithms as a mapping {c (1) \n(e i )} i\u2208I (1) , . . . , {c (t\u22121) (e i )} i\u2208I (t\u22121) \u2192 a (t) , I (t) .\nThe well-known partial feedback algorithm Exp3 chooses a (t) = Hedge( c (1:t\u22121) ) and I (t) = {i (t) } at each timestep, where i (t) i.i.d.\n\u223c a (t) and c (t) (a) = a i (t) c (t) (e i (t) )/(a (t) i (t) + \u03bb) and where \u03bb \u2265 0 is a stepsize [41]. An alternatie partial feedback algorithm is ELP which, when given a partition P of the meta-actions [n] into k subsets, guarantees I (t) \u2208 P at each timestep. That is, it fixes a grouping of the meta-actions a priori and at each timestep only observes the costs of meta-actions belonging to a particular group.", "publication_ref": ["b0", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 2.2 ([35]", "text": "). Let c (1:T ) be arbitrary linear costs where max t\u2208[T ] c (t) \u221e \u2264 1 and A = \u2206 n . For any \u03b4 \u2208 (0, 1) and partition P of [n], the actions a (1:T ) chosen by ELP satisfy Reg(a (1:T ) , c (1:T ) ) \u2264 2 |P | log(n/\u03b4)/T with probability 1 \u2212 \u03b4. Moreover, only cost components from one element of P are observed per timestep: I (t) \u2208 P .\nWe emphasize that the results in this manuscript are stated to accommodate general choices of online learning algorithms, with different guarantees and tradeoffs arising depending on which specific online learning algorithms one employs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview of Our Approach", "text": "In this section, we provide an overview of our general approach for studying the sample complexity of multi-distribution learning. Our approach consists of two steps: (1) reducing multi-distribution learning to the problem of finding the equilibrium of a convex-concave zero-sum game, and (2) implementing game dynamics to efficiently find an equilibrium using only stochastic feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Distribution Learning as a Zero-Sum Game", "text": "The multi-distribution learning problem corresponds to a zero-sum game with a minimizing player having action set H, a maximizing player having action set D \u00d7 L, and a payoff function \u03d5(h, (D, \u2113)) = R D,\u2113 (h). Intuitively, the minimizing player can be interpreted as a learner who proposes candidate solutions while the maximizing player can be interpreted as an auditor who tries to pick a data distribution and loss function for which the learner's hypothesis performs poorly. It is not hard to see that any \u03b5-min-max equilibrium (h, D) of this game corresponds to a 2\u03b5-optimal solution. Proof. If (p, q) is an \u03b5-min-max equilibria, the following holds by definition\nR q (p) \u2264 min h * \u2208H R q (h * ) + \u03b5 and R q (p) \u2265 max D * \u2208D,\u2113 * \u2208L R D * ,\u2113 * (p) \u2212 \u03b5.\nRearranging gives max\nD * \u2208D,\u2113 * \u2208L R D * ,\u2113 * (p) \u2264 min h * \u2208H R q (h * ) + 2\u03b5 \u2264 OPT + 2\u03b5.\nA multi-distribution learning problem (D, L, H) with convex losses can similarly be written as a convexconcave zero-sum game where a minimizing player chooses from the actions \u0398, a maximizing player chooses from the actions D \u00d7 L, and the payoff function is defined as \u03d5(p, q) = R q (h p ). As we previously noted, as the payoff function is convex-concave, a min-max equilibrium of this game must exist. Many tools have been developed for efficiently finding the min-max equilibria of convex-concave zero-sum games. The connection between multi-distribution learning and zero-sum games allows us to draw on these tools to derive efficient learning algorithms.\nUnknown payoff functions. The main challenge we will encounter is that of efficiently estimating the payoff function of the multi-distribution learning game, given that evaluating the function \u03d5(p, q) = R q (h p ) requires computing expectations for an unknown data distribution. Typically, to compute the min-max equilibrium of a convex-concave game, one needs a first-order approximation for the payoff function \u03d5 at various strategy profiles-that is, we require the gradients \u2207 p \u03d5(p, q) and \u2207 q \u03d5(p, q) for various choices of actions p \u2208 A \u2212 and q \u2208 A + . We will achieve this by designing noisy first-order oracles that, when queried with a strategy profile (p, q), return unbiased estimates of the gradient \u2207 p \u03d5(p, q) or \u2207 q \u03d5(p, q). To control the variance of these oracles, we will also ask that their estimates be bounded in norm, as we will formalize in the sequel. Behind the scenes, we will implement these first-order approximations by querying example oracles.\nA complication to implementing these noisy first-order oracles efficiently is that payoff estimation is more costly for the maximizing player than the minimizing player. Indeed, consider a strategy profile (p, q) in the multi-distribution learning game. Obtaining an unbiased bounded estimate of the minimizing player (learner)'s payoff gradient requires only drawing a single datapoint from the mixture distribution specified by the other player (the auditor), since the learner only needs a counterfactual estimate of how well each hypothesis would have performed on the mixture. However, obtaining an unbiased bounded estimate of the maximizing player (the auditor)'s payoff gradient requires drawing n datapoints, since the auditor needs a counterfactual estimate of how well the minimizing player's hypothesis would have performed on each potential data distribution D 1 , . . . , D n . This intuitive arugment is formalized as follows.\nFact 3.2. Consider a multi-distribution learning problem (D, L, H) with 1-smooth losses and a strategy profile h \u03b8 \u2208 H and q \u2208 \u2206(D \u00d7 L). The gradient vector \u2207 \u03b8 \u2113(h \u03b8 , z), where z\ni.i.d. \u223c D and (D, \u2113) i.i.d. \u223c q, is an unbiased bounded estimate of the first-order information \u2207 \u03b8 R q (h \u03b8 ); i.e., E z\u223cD,(D,\u2113)\u223cq [\u2207 \u03b8 \u2113(h \u03b8 , z)] = \u2207 \u03b8 R q (h \u03b8 ) and \u2225\u2207 \u03b8 \u2113(h \u03b8 , z)\u2225 \u2264 1. Similarly, the vector [1 \u2212 \u2113 j (h \u03b8 , z i )] i\u2208[n],j\u2208[m] where z i i.i.d.\n\u223c D i is an unbiased bounded estimate of the first-order information vector 1 \u2212 \u2207 q R q (h \u03b8 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Equilibrium Computation in Stochastic Convex-Concave Games", "text": "Algorithm 1 Finding Equilibria in Convex-Concave Games with Asymmetric Costs.\nInput: Action sets A \u2212 , A + , steps T , first-order oracles g -, g + , and online learning algorithms\nQ A\u2212 , Q A+ ; for t = 1, 2, . . . , T do Let p (t) = Q A\u2212 p \u2192 g -(p (\u03c4 ) , q (\u03c4 ) ), p \u03c4 \u2208[t\u22121] ; Let q (t) = Q A+ q \u2192 g + (p (\u03c4 ) , q (\u03c4 ) ), q \u03c4 \u2208[t\u22121] ; end for Return p = 1 T T t=1 p (t) and q = 1 T T t=1 q (t) ;\nWe now describe an online learning framework for finding equilibria in stochastic games using game dynamics. We will later see this framework, which is described by Algorithm 1, easily accommodates the asymmetric costs of estimating each player's payoff gradients. Lemma 3.1 outlines a guarantee of returning an approximate min-max equilibrium with high probability. We note that the guarantee of Lemma 3.1 is stated in terms of the regret bounds \u03b3 T (Q A\u2212 ) and \u03b3 T (Q A+ ) of the online learning algorithms that we plug into Algorithm 1. This means that choosing different online learning algorithms to be Q A\u2212 and Q A+ will yield different guarantees.\nLemma 3.1. Consider a convex-concave zero-sum game (A \u2212 , A + , \u03d5) with an L-smooth payoff \u03d5. Assume that 1. gis a noisy first-order oracle that returns unbiased, bounded, and independent estimates of \u2207 p \u03d5(p, q).\nThat is, for all p \u2208 A \u2212 and q \u2208 A + , we have \u2225g -(p, q)\u2225 \u2264 L with probability one and E [g -(p, q)] = \u2207 p \u03d5(p, q).\n2. g + is a noisy first-order oracle that returns unbiased, bounded, and independent estimates of \u2212\u2207 q \u03d5(p, q).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The action sets A", "text": "\u2212 and A + have a diameters of at most R in the dual norm \u2225\u2022\u2225 * , i.e. max p,p \u2032 \u2208A\u2212 \u2225p \u2212 p \u2032 \u2225 * \u2264 R and max q,q \u2032 \u2208A+ \u2225q \u2212 q \u2032 \u2225 * \u2264 R.\nThen Algorithm 1 returns an \u03b5-min-max equilibrium with probability 1 \u2212 \u03b4 if\nT \u2265 4L 2 \u03b5 2 32R 2 log(2/\u03b4) + 25\u03b3 T (Q A\u2212 ) + 25\u03b3 T (Q A+ ) .(5)\nInformally, we can interpret the regret bound \u03b3 T (Q A\u2212 ) as the difficulty of making rational choices for the minimizing player and \u03b3 T (Q A+ ) as the difficulty of making rational choices for the maximizing player. This lemma then says that the number of iterations required to find an equilibrium depends on the additive combination of the complexity seen by each player. Before we proceed to a proof of this lemma, we recall some standard results on game dynamics and online learning. First, we recall that no-regret dynamics efficiently learns equilibria in convex-concave games [21]. This fact implies that, in order to learn the equilibria of our multi-distribution learning game, it suffices to design suitable no-regret algorithms. Fact 3.3. Let (A \u2212 , A + , \u03d5) be a convex-concave zero-sum game. For any actions p (1:T ) \u2208 A \u2212 and q (1:T ) \u2208 A + with regret Reg p (1:T ) , \u03d5(\u2022, q (t) ) t\u2208[T ] \u2264 T \u03b5 and Reg q (1:T ) , \u2212\u03d5(p (t) , \u2022) t\u2208[T ] \u2264 T \u03b5, the average actions 1 T T t=1 p (t) and 1 T T t=1 q (t) form a 2\u03b5-min-max equilibrium.\nProof. By convexity, p := 1 T T t=1 p (t) \u2208 A \u2212 and q := 1 T T t=1 q (t) \u2208 A + . Since \u03d5 is concave in its second argument, we can apply Jensen's inequality to the regret bound of the minimizing player to get\nReg ({p}, {\u03d5(\u2022, q)}) = \u03d5(p, q) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 1 T T t=1 \u03d5(p, q (t) ) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 \u03b5.\nSince \u03d5 is convex in its first argument, we can again apply Jensen's inequality, this time to the regret bound of the maximizing player, to get\nReg ({q}, {\u2212\u03d5(p, \u2022)}) = max q * \u2208A+ \u03d5(p, q * ) \u2212 \u03d5(p, q) \u2264 max q * \u2208A+ \u03d5(p, q * ) \u2212 1 T T t=1 \u03d5(p (t) , q) \u2264 \u03b5.\nSumming these inequalities yields that max\nq * \u2208A+ \u03d5(p, q * ) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 2\u03b5.\nNext, we recall that, in a no-regret learning problem with linear costs c (1:T ) , a player can run any online learning algorithm directly on independent, unbiased, bounded estimates c (1:T ) of its costs c (1:T ) and expect only a constant factor increase in its worst-case regret bound. This fact, which is classical in both optimization theory [40,28] and online learning theory [21], follows by a standard martingale argument. That no-regret learning algorithms generalize well on stochastic costs will mean that we can efficiently implement no-regret dynamics on stochastic games using noisy payoff observations that need only be unbiased and bounded. Importantly, this means we do not need to obtain \u03b5-accurate estimates of each players' payoff at each iteration, which would make no-regret dynamics prohibitively expensive in terms of sample complexity.\nIn the sequel, given a linear cost function c : A \u2192 R, we will abuse notation and use c to also denote the vector such that c(a) = \u27e8c, a\u27e9 for all a \u2208 A. We will also use \u2225c\u2225 to denote the norm of the vector c. . Assume an action diameter of R, i.e. max a,a \u2032 \u2208A \u2225a \u2212 a \u2032 \u2225 * \u2264 R. The actions a (t) = Q A ( c (1:t\u22121) ) chosen by applying an online learning algorithm Q A to the estimated costs c (1:T ) satisfies the following generalization bound with probability 1 \u2212 \u03b4:\nReg(a (1:T ) , c (1:T ) ) \u2212 Reg(a (1:T ) , c (1:T ) ) \u2264 4L \u221a T R 2 log(1/\u03b4) + \u03b3 T (Q A ) .(6)\nProof. We first upper bound the generalization error by the regret of actions a (1:T ) on the cost differences\nc (t) \u2212 c (t) . Since max a * \u2208A T t=1 c (t) , a * \u2212 max a * \u2208A T t=1 c (t) , a * \u2264 max a * \u2208A T t=1 c (t) \u2212 c (t)\n, a * , we can bound generalization error \u2206 by\n\u2206 := Reg(a (1:T ) , c (1:T ) ) \u2212 Reg(a (1:T ) , c (1:T ) ) \u2264 Reg(a (1:T ) , c (1:T ) \u2212 c (1:T ) ).\nThe remainder of the proof is dedicated to bounding this regret term, which we can write explicitly as\nReg(a (1:T ) , c (1:T ) \u2212 c (1:T ) ) = max a * \u2208A T t=1 a (t) \u2212 a * , c (t) \u2212 c (t) .\nWe will ultimately control this regret term with a martingale argument, appealing to the fact that at each timestep the noisy costs we observe are unbiased even conditioned on previous cost observations. However, we first need to control the variational term a * , which we will do with a standard approach of introducing a shadow term\n\u03b5 (t) = Q A ({c (\u03c4 ) \u2212 c (\u03c4 ) } \u03c4 \u2208[t\u22121]\n). That is, \u03b5 (1:T ) is the result of (hypothetically) running the online learning algorithm Q A on the cost sequences {c (\u03c4 ) \u2212 c (\u03c4 ) }. Adding and subtracting the shadow terms \u03b5 (1:T ) from the inner product,\nmax a * \u2208A T t=1 a (t) \u2212 a * , c (t) \u2212 c (t) = max a * \u2208A T t=1 a (t) \u2212 \u03b5 (t) + \u03b5 (t) \u2212 a * , c (t) \u2212 c (t) = T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) + max a * \u2208A T t=1 \u03b5 (t) \u2212 a * , c (t) \u2212 c (t) .\nSince we constructed \u03b5 (1:T ) with our online learning algorithm Q A , we obtain a regret guarantee for the action sequence \u03b5 (1:T ) on the cost sequence c (t) \u2212 c (t) , which yields\nmax a * \u2208A T t=1 \u03b5 (t) \u2212 a * , c (t) \u2212 c (t) \u2208 4L \u03b3 T (Q A )T . (7\n)\nThe term 4L appears in this bound since\nc (t) (a) \u2212 c (t) (a) \u2208 [\u22122L, 2L] must be normalized to [0, 1]. It remains to bound T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t)\n. This expression is a martingale because, for each summand a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) , the left-hand side a (t) \u2212 \u03b5 (t) is conditionally (on previous summands) independent of c (t) \u2212 c (t) . Formally, we define the filtration {F (t) } T t=0 as the sigma algebra generated by { c (t) } T t=1 . By construction, we know that (a (t) \u2212 \u03b5 (t) ) is F (t\u22121) -measurable and thus a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) is F (t) -measurable. Since c (t) is unbiased, we also have that E a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) F (t\u22121) = 0. Finally, we can observe that the difference sequence of our martingale can be bounded with Holder's inequality as\na (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) \u2264 a (t) \u2212 \u03b5 (t) * c (t) \u2212 c (t) \u2264 4RL.\nBy the Azuma-Hoeffding inequality, we can thus bound, for any \u03b5 > 0,\nPr T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) \u2265 \u03b5 \u2264 exp \u2212 \u03b5 2 32T R 2 L 2 .\nWe can rewrite this as saying, with probability 1 \u2212 \u03b4, that\nT t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) \u2264 4RL 2T log(1/\u03b4).\nIn combination with Equation 7, this inequality yields the desired bound on \u2206.\nWe intend to apply online learning algorithms to our convex-concave games, where the payoff function is not necessarily linear. To overcome the fact that the concentration result in Fact 3.4 is specific to linear costs, we now turn to showing that one can linearize any online learning problem with convex costs. That is, we can reduce online learning on differentiable convex costs to online learning on linear costs, allowing us to apply Fact 3.4. Specifically, we will use the concept of variational error, which is usually defined as VErr(a (1:T ) , c (1:T ) ) := Reg(a (1:T ) , c (1:T ) ) where c (t) (a) = a, \u2207c (t) (a (t) ) . We now formalize the fact that variational error yields an upper bound on regret: VErr(a (1:T ) , c (1:T ) ) \u2265 Reg(a (1:T ) , c (1:T ) ). Fact 3.5. Let c (1:T ) : A \u2192 R be convex functions on a convex compact domain A and let \u2202c (t) (a (t) ) be a partial subgradient of c (t) at a (t) . For any sequence a (1:T ) \u2208 A, \nReg(a (1:T ) , c (1:T ) ) := T t=1 c (t) (a (t) ) \u2212 min a * \u2208A T t=1 c (t) (a * ) \u2264 VErr(a (1:T ) , c (1:T ) ) := max a * \u2208A T t=1 \u2202c (t) (a (t) ), a (t) \u2212 a * . Proof. By the convexity of \u03d5, T t=1 \u2202c (t) (a (t) ), a (t) \u2212 a * \u2265 T t=1 c (t) (a (t) )\u2212c (t) (a * )\nReg p (1:T ) , c - (1:T ) \u2212 Reg p (1:T ) , c - (1:T ) \u2264 4L \u221a T R 2 log(1/\u03b4) + \u03b3 T (Q A\u2212 ) , Reg q (1:T ) , c + (1:T ) \u2212 Reg q (1:T ) , c + (1:T ) \u2264 4L \u221a T R 2 log(1/\u03b4) + \u03b3 T (Q A+ ) .\nNext, we observe that the costs cand c + are constructed so that regret on these costs coincides with variational error on \u03d5; i.e., Reg p (1:T ) , c -(1:T ) = VErr(p (1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) and Reg q (1:T ) , c + (1:T ) = VErr(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ). Our empirical regret and generalization error bounds therefore imply\nVErr(p (1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) \u2264 L \u221a T 4R 2 log(1/\u03b4) + 5 \u03b3 T (Q A\u2212 ) VErr(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ) \u2264 L \u221a T 4R 2 log(1/\u03b4) + 5 \u03b3 T (Q A+ ) .\nFor the stated choice of T , VErr(p\n(1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) \u2264 T \u03b5 and VErr(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ) \u2264 T \u03b5 with probability at least 1 \u2212 2\u03b4. By Fact 3.5, Reg(p (1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) \u2264 T \u03b5 and Reg(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ) \u2264 T \u03b5.\nFinally, by Fact 3.3, we have that (p, q) is an 2\u03b5-min-max equilibrium with probability at least 1 \u2212 2\u03b4.", "publication_ref": ["b20", "b39", "b27", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Distribution Learning", "text": "In this section, we present Algorithm 2, a general recipe for multi-distribution learning. Algorithm 2 is a general framework into which one can plug any choice of online learning algorithm to obtain a variety of multi-distribution learning guarantees. The algorithm, which implements a form of stochastic game dynamics, uses the tools we outlined in Section 3 to reduce multi-distribution learning to the problem of solving a convex-concave game, and we then employ online learning algorithms to solve the game. In Theorem 4.1, we present one example of the multi-distribution learning guarantees that Algorithm 3 can provide for any online-learnable hypothesis class H.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 General Recipe for Multi-Distribution Learning.", "text": "Input: Hypothesis class H with parameter space \u0398, example oracles EX(D 1 ), . . . , EX(D n ), iterations T , online learning algorithm Q \u0398 and a partial feedback online learning algorithm Q \u2206n\u00d7m ; Initialize: \u03b8 (1) = Q \u0398 (\u2205) and w (1) , I (1) = Q \u2206n\u00d7m (\u2205); for t = 2, . . . , T do Sample (i, j) \u223c w (t) and a datapoint z (t\u22121) i.i.d. \u223c EX(D i ); Update the learner's action\n\u03b8 (t) = Q \u0398 ({\u03b8 \u2192 \u2207 \u03b8 \u2113 j (h \u03b8 , z (\u03c4 ) ), \u03b8 } \u03c4 \u2208[t\u22121] ); For all (i, j) \u2208 I (t\u22121) , sample a datapoint z i (t\u22121) i.i.d. \u223c EX(D i ) for every unique i; Update the auditor's action w (t) , I (t) = Q \u2206n\u00d7m ({w \u2192 1 \u2212 n i=1 m j=1 w ij \u2113 j (h \u03b8 (\u03c4 ) , z i (\u03c4 ) )} \u03c4 \u2208[t\u22121]\n); end for Return: h \u03b8 where \u03b8 = 1 T T t=1 \u03b8 (t) ; Theorem 4.1. Consider a multi-distribution learning problem (D, L, H) with convex and 1-smooth losses and a parameter space \u0398 of diameter R: max \u03b8,\u03b8 \u2032 \u2208\u0398 \u2225\u03b8 \u2212 \u03b8 \u2032 \u2225 * \u2264 R. Let Q \u2206n\u00d7m be a high-probability [41] variant of the ELP algorithm [35] implemented on the partition P = [{(i, j)} j\u2208m ] i\u2208n . For any choice of online learning algorithm Q \u0398 , with probability 1 \u2212 \u03b4, Algorithm 2 returns an \u03b5-optimal solution h \u03b8 \u2208 H where\n\u03b5 \u2208 O T \u22121 (\u03b3 T (Q \u0398 ) + n log(mn/\u03b4) + R log(1/\u03b4)) .\nThe sample complexity of the algorithm is 2T .\nProof. Algorithm 2 implements Algorithm 1 on the convex-concave game (\u0398, \u2206(D \u00d7 L), \u03d5), where the payoff function \u03d5 is 1-smooth and defined as \u03d5(\u03b8, (D, \u2113)) = R D,\u2113 (h \u03b8 ).\nWe now turn to verifying that the conditions of Lemma 3.1 are satisfied. Since we assume that all losses are 1-smooth in some norm \u2225\u2022\u2225, the learner's payoff gradient \u2207 p \u03d5(p, q) is always bounded by 1 in the same norm, while we assume the the learner's action set diameter is at most R as measured by the dual norm \u2225\u2022\u2225 * . By linearity of expectation, we also have that the auditor's payoff gradient \u2207 q \u03d5(p, q) is always bounded by 1 in the infinity norm, while the auditor's action set diameter-a probability simplex-is at most 1 as measured by the 1-norm. By Fact 3.2, the gradient estimators used in Algorithm 1, i.e. \u03b8 \u2192 \u2207 \u03b8 \u2113 j (h \u03b8 , z (\u03c4 ) ), \u03b8 and w \u2192 1 \u2212 n i=1 m j=1 w ij \u2113 j (h \u03b8 (\u03c4 ) , z i (\u03c4 ) ), are unbiased, i.i.d., and 1-bounded estimates of the payoff gradients \u2207 \u03b8 \u03d5(\u03b8, w (\u03c4 ) ) and \u2207 w \u03d5(\u03b8 (\u03c4 ) , w) respectively. Thus, all conditions of Lemma 3.1 are satisfied. We therefore know that (h \u03b8 , 1\nT T t=1 w (t) ) is an \u03b5-equilibrium with probability 1 \u2212 \u03b4 if T \u2265 128 \u03b5 2 R 2 log(2/\u03b4) + \u03b3 T (Q \u0398 ) + \u03b3 T (Q \u2206nm ) .\nRecalling that the regret bound of the ELP algorithm is \u03b3 T (Q\n\u2032 \u2206nm ) \u2208 O(n log(nm/\u03b4)) (Lemma 2.2), it suffices if T \u2265 C \u03b5 2 R 2 log(2/\u03b4) + \u03b3 T (Q \u0398 ) + n log(mn/\u03b4))\nfor some universal constant C. By Fact 3.1, it thus follows that h := 1 T T t=1 h (t) is a 2\u03b5-optimal solution with probability 1 \u2212 2\u03b4. We now resolve the sample complexity of our instantiation of Algorithm 2. At every timestep, the learner draws one datapoint z (t) . The number of datapoints that the auditor draws in any given iteration is the number of unique values of i in the set (i, j) \u2208 I (t\u22121) . Concretely, recall that I (t\u22121) denotes which cost components that the partial feedback algorithm Q \u2206n\u00d7m chooses to observe from step t \u2212 1, where an entry (i, j) \u2208 I (t\u22121) indicates that the auditor wishes to estimate the (in hindsight) outcome of auditing the learner on the data distribution D i and loss function \u2113 j . We implement the ELP algorithm on the partition P = [{(1, 1), . . . , (1, m)} , . . . , {(n, 1), . . . , (n, m)}], where all elements of a given group I \u2208 P correspond to the same data distribution D i but different choices of loss functions \u2113 j , meaning that Unique({i | (i, j) \u2208 I}) = 1. Since ELP guarantees that I (t\u22121) \u2208 P , we can conclude the auditor only observes one datapoint per iteration. The total sample complexity of the algorithm is therefore 2T .\nOne interpretation of Theorem 4.1 is that the worst-case sample complexity of multi-distribution learning is not significantly larger than the worst-case sample complexity of learning a single data distribution with an online-to-batch reduction. More specifically, handling multiple data distributions and loss functions only adds an additive factor to one's sample complexity. It may seem surprising that our sample complexity bound for multi-distribution learning-a stochastic setting-is characterized by complexity of online decision-makingan adversarial setting. However, multi-distribution learning is inherently an online decision-making problem, as it requires one to strategize adaptively regarding the choice of data distribution to collect additional samples from. This is in contrast to the usual single-distribution learning setting, where there is no explicit decision-making involved.", "publication_ref": ["b0", "b0", "b40", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Collaborative Learning", "text": "In this section, we present our main result on collaborative learning: a tight bound on the sample complexity of collaborative learning in agnostic settings. In particular, we show that the collaborative learning of a finite hypothesis class H on n data distributions requires \u0398( log(|H|)+n log(n/\u03b4) \u03b5 2\n) samples. This means that, when characterizing hypothesis class complexity by log(|H|), the worst-case sample complexity of learning n distributions is not significantly larger than the worst-case sample complexity of learning one data distribution. Specifically, it requires at most a constant factor or an additive O(n log(n/\u03b4)/\u03b5 2 ) factor additional samples.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sample Complexity Upper Bound", "text": "Theorem 5.1 states our sample complexity upper bound for agnostic collaborative learning. It is a direct implication of the sample complexity of multi-distribution learning because, as we noted previously (Fact 2.1), one can easily reduce agnostic collaborative learning to multi-distribution learning. Theorem 5.1 improves over the best-known sample complexity for agnostic collaborative learning by Nguyen and Zakynthinou [42] in two ways, giving an OPT + \u03b5 bound for randomized classifiers instead of their 2OPT + \u03b5 bound, and improving their sample complexity of O Proof. The reduction of collaborative learning to multi-distribution learning (Fact 2.1) implies that any \u03b5-optimal solution to the multi-distribution learning problem (D, {\u2113} , \u2206(H)) is an \u03b5-optimal solution to the collaborative learning problem (H, D). Fact 2.1 also establishes that (D, {\u2113} , \u2206(H)) has convex and 1-smooth losses, where smoothness is measured in the infinity norm. Since \u2206(H) is a probability simplex, we also have that its diameter is at most 2 in the 1-norm. The guarantees of Theorem 4.1 thus hold in our setting.\nSince we choose to instantiate the online learning algorithm Q \u2206(H) used in Algorithm 2 with Hedge, we recall that Hedge provides the regret guarantee (Lemma 2.1) of \u03b3 T (Q \u2206(H) ) \u2208 O(log(|H|)). Thus, we can write the statement of Theorem 4.1 as guaranteeing that Algorithm takes at most 2T datapoints in total and that, with probability 1 \u2212 \u03b4, the output h of Algorithm 2 is an \u03b5-optimal solution to (D, {\u2113} , \u2206(H)), where \u03b5 \u2208 O T \u22121 (log(|H|) + n log(n/\u03b4)) .\nFor constants \u03b5 and \u03b4, our sample complexity of O (log(|H|) + n log(n)) appears to violate the lower bound of \u2126 (log(|H|) log(n) + n log(log(|H|))) due to Chen, Zhang, and Zhou [13]. This discrepancy is due to a small error in the proof of that lower bound, which we have verified in private communications with the authors.\nRecall that we can convert any randomized solution to a deterministic one by taking a majority vote (Fact 2.2). Our sample complexity bound on finding randomized solutions therefore also implies a sample complexity bound on finding deterministic improper solutions to collaborative learning problems. \nh Maj \u2208 Y X such that max D\u2208D R D (h Maj ) \u2264 2OPT + \u03b5.\nIn the next subsection, we will show that this sample complexity upper bound is tight up to double-log factors and exactly tight in the regime where n \u2208 O(log(|H|).", "publication_ref": ["b41", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Sample Complexity Lower Bound", "text": "We now provide matching lower bounds on the sample complexity of agnostic collaborative learning. We note that these lower bounds hold for any collaborative learning algorithm that returns \u03b5-optimal solutions, regardless of whether those algorithms perform sampling on-demand and regardless of whether the algorithms return randomized or deterministic and proper or improper solutions. We also note that the data distributions we construct to establish these lower bounds are not exotic. For example, to prove these lower bounds, we construct a set of data distributions where all data distributions share the exact same feature distribution and all but one distribution share the exact same label distribution.\nTheorem 5.3 states our lower bound. In this theorem, we refer to an algorithm as an (\u03b5, \u03b4)-collaborative learning algorithm if, for every collaborative learning problem (H, D), the algorithm returns an \u03b5-optimal solution h, i.e., satisfying Equation 2, with probability at least 1 \u2212 \u03b4. We say that a learning algorithm Q is an (\u03b5, \u03b4)-optimal collaborative learning algorithm for a specific set of collaborative learning problems\nV := {(H i , D i )} i if, given any problem (H i , D i ) \u2208 V, with probability at least 1 \u2212 \u03b4 the output of Q is an \u03b5-optimal solution.\nTheorem 5.3. Take any n, d \u2208 Z + , \u03b5, \u03b4 \u2208 (0, 1/8), and (\u03b5, \u03b4)-collaborative learning algorithm Q. There exists a collaborative learning problem (H, D) with |D| = n and |H| = 2 d , on which Q takes at least\n\u2126 1 \u03b5 2 (d + n log(min{n, d}/\u03b4)) samples. When n \u2264 d, this lower bound becomes \u2126 1 \u03b5 2 (d + n log(n/\u03b4)\n) . Before we proceed to a proof of this theorem, we first define a notion of expected sample complexity. Take any collaborative learning problem V = (H, D); we use N Q (V ) to denote the expected sample complexity of a collaborative learning algorithm Q on the problem V , where the expectation is taken both over the randomness of data samples and the algorithm's randomness. Similarly, given a probability distribution P over a set of collaborative learning problems V := {(H i , D i )} i , we define expected sample complexity as\nN Q (P) = E V \u223cP [N Q (V )].\nWe now prove two lemmas, Lemma 5.4 and Lemma 5.5, that directly imply Theorem 5.3. Lemma 5.4 is a standard lower bound on the sample complexity of agnostic PAC learning, and provides the unsurprising \u2126( d \u03b5 2 ) lower bound summand. Lemma 5.5 is more involved and provides the \u2126( n log(min{n,d}/\u03b4) \u03b5 2\n) summand in our lower bound. Lemma 5.4. Take any n, d \u2208 Z + , \u03b5, \u03b4 \u2208 (0, 1/8), and collaborative learning algorithm Q. There exists a set of collaborative learning problems V on which, if Q is (\u03b5, \u03b4)-optimal, Q takes at least \u2126 log|H| Proof. This claim follows directly from the standard lower bound on sample complexity of agnostic probablyapproximately-correct (PAC) learning [54], since we can reduce any single-distribution learning problem to multi-distribution learning problem by defining multiple copies of a data distribution. We defer interested readers to Ehrenfeucht et al. [20]. Proof. We prove this lower bound constructively by defining multiple sets of collaborative learning instances: {V w\u03b7,w } w,\u03b7\u2208N . At a high-level, the proof of this lower bound will follow from proving that multi-distribution learning allows one to solve multiple single-distribution learning problems simultaneously with constant probability using a boosting-like algorithm.\nWe now detail our fairly technical construction of these collaborative learning instances. For every set of instances V u,w , we require all instances (H, D) \u2208 V u,w to share a feature space X = {1, . . . , w}, label space Y = {\u00b11}, hypothesis class H = Y X , and 0/1 loss \u2113. For every x \u2208 [w] and y \u2208 {\u00b11}, we define distributions D x and D \u2032\nx as having the probability mass functions Pr Dx (x, y) = 1 2 \u2212 2y\u03b5 and\nPr D \u2032 x (x, y) = 1 2 + 4y\u03b5. Let D \u2212 = x\u2208[w]\n{D x } \u03b7 be an ordered list of distributions, and for every x \u2208 [w] and i \u2208 [\u03b7], define D x,i to be a set of distributions identical to D \u2212 except with the ith copy of distribution D x replaced with distribution D \u2032\nx . Let P \u03b7w,w be a distribution over collaborative learning instances that, with probability 1 2 , returns (H, D \u2212 ) and for every i \u2208 [\u03b7], x * \u2208 [w], with probability 1 2w\u03b7 returns (H, D x * ,i ). Observe that P \u03b7w,w is a distribution over collaborative learning problems where |H| = 2 w and |D| = u. The following claims characterize sample complexity lower bounds on P u,w .\nClaim 5.1. Consider any \u03b5 \u2208 (0, 1/2), \u03b4 \u2208 (0, 1), and collaborative learning algorithm Q that is (\u03b5, \u03b4)-optimal for V \u03b7,1 . The expected sample complexity of Q is at least \u03b7 256\u03b5 2 log(1/2\u03b4). Claim 5.2. Consider any \u03b5 \u2208 (0, 1/2) and \u03b4 \u2208 (0, 1). Suppose there exists a collaborative learning algorithm Q that is (\u03b5, \u03b4)-optimal for V \u03b7w,w and has an expected sample complexity of N under P \u03b7w,w . Then there exists an (\u03b5, 8\u03b4 7w )-learning algorithm Q \u2032 for V \u03b7,1 under P \u03b7,1 with an expected sample complexity on P \u03b7,1 of . Given an (\u03b5, \u03b4)-algorithm Q for V \u03b7,1 with an expected sample complexity of N (under P \u03b7,1 ), we can construct a coin algorithm Q \u2032 with an expected sample complexity of N (under Pr) and that, under any hypothesis, with probability at least 1 \u2212 \u03b4, can identify whether H 0 is false.\nTo see this, have Q \u2032 run Q by simulating draws from the ith distribution by flipping the ith coin. If all coins are biased towards tails with probability 1/2 + 2\u03b5, any \u03b5-error hypothesis h must satisfy Pr(h(1) = +) > 1/2. Conversely, if one coin is biased towards heads, any \u03b5-error hypothesis h must satisfy Pr(h(1) = +) < 1/2.\nSuppose Q \u2032 , conditioned on H 0 , correctly predicts H 0 with probability at least 1 \u2212 \u03b4. Then, suppose Q \u2032 , under H 0 , takes no more than T i flips from the ith coin. Let p i,j1:j2 be a probability distribution over {0, 1} corresponding to the outcomes of the j 1 st to j 2 nd coin toss by Q \u2032 under H i . Let p * j be a uniform distribution over {0, 1} j . Since p i,j:j and p * j are Bernoulli distributions with a parameter within 4\u03b5 of 1/2, for \u03b5 < 1/2, KL(p i,j:j , p * 1 ) < 128\u03b5 2 [60]. Moreover, KL(p i,1:j , p * j ) < 128j\u03b5 2 by tensorization and TV(p i,1:j , p * j ) \u2264 8\u03b5 \u221a j by Pinsker's inequality. Let E be the set of outcomes of T i flips under which Q \u2032 predicts H 0 . By correctness under H 0 , we have that Pr H0 (E) \u2265 1 \u2212 \u03b4. Thus, total variation distance implies 1 \u2212 \u03b4 \u2212 8\u03b5 \u221a j < Pr Hi (E). Since Pr Hi (E) < \u03b4, we have that\n1 128\u03b5 2 (1 \u2212 2\u03b4) 2 < T i . Thus, if Q \u2032 is \u03b4 accurate under all hypotheses, under H 0 , Q \u2032 must take at least \u03b7 128\u03b5 2 (1 \u2212 2\u03b4) 2 < \u03b7 128\u03b5 2 log(1/2\u03b4\n) samples from each distribution. Thus, the expected sample complexity of Q \u2032 -and similarly that of Q under P \u03b7,1 -must be at least \u03b7 256\u03b5 2 log(1/2\u03b4). Proof of Claim 5.2. This claim is similar to the lower bounds of Blum et al. [9] and Karp and Kleinberg [30]. We construct Q \u2032 as follows. Define the shorthand\nI j := [(j \u2212 1)\u03b7 + 1, j\u03b7]. Consider any problem V \u2032 = (H, D) \u2208 V \u03b7,1 .\n1. Q \u2032 draws an imaginary problem (H, D \u2032 ) \u2208 V \u03b7w,w and chooses an index i \u2208 [w] uniformly at random.\n2. Q \u2032 simulates algorithm Q on (H, D \u2032 ): when Q tries to sample a datapoint from distribution D \u2032 j where j / \u2208 I i , return a sample from D \u2032 j ; when j \u2208 I i , return a sampled datapoint from D j\u2212(i\u22121)\u03b7 .\n3. When Q terminates and returns a classifier h, Q \u2032 checks whether, for every j \u0338 = i: max r\u2208Ij R Dr (h) < 1 2 . If this condition is satisfied, Q \u2032 returns h(1) = h(i). If not, we repeat from Step 1. We denote the number of total iterations by T .\nConsider the probability p i that, in the third step, for every j \u0338 = i we have max r\u2208Ij R Dr (h) < 1 2 but max r\u2208Ii R Dr (h) \u2265 1 2 . Let E t denote the event that Q \u2032 returns an at least \u03b5-error hypothesis after t iterations of our procedure. Noting that E t can only occur if Q failed all t \u2212 1 iterations before and at the tth iteration, Step 3 fails to catch the bad hypothesis for D i . By assumption, \u03b4 \u2265 w i=1 p i . By symmetry of our construction V and recalling \u03b4 < 1/8:\n\u221e t=1 Pr (E t ) \u2264 \u221e t=1 \u03b4 t\u22121 1 w w i=1 p i \u2264 \u221e t=1 \u03b4 t /w \u2264 8\u03b4\n7w . Thus, Q \u2032 is an (\u03b5, 8\u03b4 7w )-algorithm for P \u03b7,1 . We now bound the sample complexity of Q \u2032 . Let N Q \u2032 (t) denote the number of samples that Q \u2032 takes from V \u2032 on the tth iteration. Note that N Q \u2032 (1), N Q \u2032 (2), . . . are i.i.d. In addition, by the symmetry of V and linearity of expectation,\nE V \u2032 \u2208P\u03b7,1 [N Q \u2032 (t)] = m/w. Thus, E V \u2032 T t=1 N Q \u2032 (t) = E V \u2032 [T ] E V \u2032 [N Q \u2032 (1)] = E V \u2032 [T ] m/w.\nWe can upper bound T by observing that our procedure only repeats if Q fails:\nE V \u2032 [T ] = \u221e t=1 Pr(T \u2265 t) \u2264 \u221e t=0 \u03b4 t \u2264 1 1\u2212\u03b4 \u2264 8 7\n. Thus, Q \u2032 has an expected sample complexity of at most 8m 7w .", "publication_ref": ["b53", "b19", "b59", "b8", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Group DRO and Agnostic Federated Learning", "text": "In this section, we present our main result on the sample complexity of the group distributionally robust optimization framework of Sagawa et al. [50] and the agnostic federated learning framework of Mohri et al. [39]. We show that the worst-case sample complexity of group DRO, and equivalently agnostic federated learning, is greater than that of online convex optimization by only a constant factor and an additive O(n log(n/\u03b4)/\u03b5 2 ) samples. This sample complexity upper bound is tight for a difficult class of problems-a class that coincides with collaborative learning. Since the settings of group DRO and agnostic federated learning are generally equivalent, we state the results explicitly for group DRO, with the understanding that the same results apply to agnostic federated learning.\nSetup. Group distributionally robust optimization is typically studied in a convex optimization setting where the hypothesis class is parameterized by a convex compact parameter class and the loss function is smooth and convex in the parameterization. As noted previously, this means that the group DRO setting coincides with general setting of multi-distribution learning with a single smooth convex loss. Moreover, group DRO is usually formulated in a setting where the parameter space admits mirror descent approaches. We first present the definitions which are necessary for describing mirror descent guarantees. A distancegenerating function on a parameter space \u0398 is a continuous and strongly convex, modulus 1, function \u03c9 : \u0398 \u2192 R, where there exists a non-empty subset of the parameter space \u0398 o \u2282 \u0398 where the subdifferential \u2202\u03c9 is non-empty and \u2202\u03c9 admits a continuous selection on \u0398 o . The center of \u0398 with respect to \u03c9 is denoted as \u03b8 c := arg min \u03b8\u2208\u0398 o \u03c9(\u03b8). The Prox function (Bregman divergence) V : \u0398 o \u00d7 Z \u2192 R + associated with a distance-generating function \u03c9 : Z \u2192 R is defined as V (w, u) := \u03c9(u) \u2212 \u03c9(w) \u2212 \u27e8\u03c9 \u2032 (w), u \u2212 w\u27e9. Bregman radius, which is a measure for how difficult it is to learn a parameter class, is then defined as follows. Definition 6.1. Given a convex set \u0398 with a distance-generating function \u03c9, the Bregman radius is defined as D \u0398 := max u\u2208\u0398 V (\u03b8 c , u) where \u03b8 c is the center of \u0398.\nA bounded Bregman radius allows one to apply online mirror descent [6] as an online learning algorithm, with a regret guarantee of \u03b3 T (Q \u0398 ) \u2264 D \u0398 . In group DRO, D \u0398 is typically assumed to be small.\nSample complexity upper bound. Theorem 6.1 states our sample complexity bound for group distributionally robust optimization. This sample complexity bound is a direct implication of our multi-distribution learning sample complexity bound. This theorem establishes the first generalization bound for the problem of group distributionally robust optimization [50] and improves, by a factor of n, existing sample complexity bounds for agnostic federated learning [39]. This significant improvement in sample complexity over Mohri et al. [39] is attained by sampling data on-demand, whereas Mohri et al. [39] work with a distribution over groups/clients that is fixed a priori. Theorem 6.1. Given a set of data distributions D = {D 1 , . . . , D n }, a hypothesis class H with a Bregman radius of D \u0398 and a diameter of R, and a 1-smooth loss \u2113, consider the group distributionally robust optimization problem (D, {\u2113} , H). Consider the output \u03b8 \u2208 \u0398 arising from applying Theorem 4.1's algorithm, choosing the online learning algorithm Q to be online mirror descent. With probability 1 \u2212 \u03b4, h is an \u03b5-optimal solution (see (3)) and the sample complexity\nis O \u03b5 \u22122 (D \u0398 + n log(n/\u03b4) + R log(1/\u03b4)) .\nProof. This claim follows directly by Theorem 4.1 since group distributionally robust optimization is equivalent to multi-distribution learning on a single smooth convex loss. Recall that, for a convex parameter space with Bregman radius D \u0398 for a distance-generating function \u03c9, running the online mirror descent algorithm with respect to \u03c9 guarantees a regret bound of \u03b3 T (Q \u0398 ) \u2264 D \u0398 [6]. We directly plug this online convex optimization regret bound into Theorem 4.1.\nThis sample complexity bound for finding a group DRO solution with low expected loss also trivially implies a bound on the number of mirror descent iterations that are necessary to find a group DRO or agnostic federated learning solution with low empirical training error. This question was considered by Sagawa et al. [50] who presented an iteration complexity bound that we improve upon by a factor of n. Corollary 6.2 (Theorem 6.1). Consider a group distributionally robust optimization problem (D, {\u2113} , H). For every D \u2208 D, let B D \u223c D be a non-empty batch of i.i.d. datapoints and D \u2032 be the empirical distribution of B D . There is an algorithm that only requires O \u03b5 \u22122 (D \u0398 + n log(n/\u03b4) + R log(1/\u03b4)) iterations of mirror descent steps to output, with probability 1 \u2212 \u03b4, an empirically \u03b5-optimal solution.\nSample complexity lower bound. There exists a class of difficult group distributionally robust optimization problems for which our stated sample complexity upper bounds are tight. This is because we can reduce any collaborative learning problem to multi-distribution learning with a single smooth convex loss, and equivalently, group DRO. Thus, our sample complexity lower bound for collaborative learning directly implies a lower bound for group DRO for a class of difficult cases. We formally state this corollary of Theorem 5.3 below. Corollary 6.3. Take any n, m \u2208 N and \u03b5, \u03b4 \u2208 (0, 1/8). There exists a finite set V of group distributionally robust optimization problems with 1-smooth losses and parameter spaces of unit diameter and finite Bregman radius D \u0398 , where every (\u03b5, \u03b4)-algorithm Q has a sample complexity in \u2126 D\u0398+n log(min{n,D\u0398}/\u03b4) \u03b5 2 .", "publication_ref": ["b49", "b38", "b5", "b49", "b38", "b38", "b38", "b2", "b5", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Extensions to Infinite Classes of Binary Classifiers", "text": "In this section, we study the sample complexity of multi-distribution learning when the hypothesis class is infinite but combinatorially bounded. In particular, we will study multi-distribution learning problems involving binary classification tasks and hypothesis classes of finite VC dimension or finite Littlestone dimension [33]. For succinctness, we state all results in this section for the collaborative learning setting, but note that these results extend readily to the general multi-distribution learning setting.\nLittlestone dimension. The Littlestone dimension of a set of binary classifiers quantifies the set's online learnability [33]. Formally, consider a supervised learning setting with domain X and a set of binary classifiers H. Consider a full binary tree of depth d, such that each node in the tree is labeled by a feature x \u2208 X . We say the tree is shattered by H if for every set of labels\n{y i } d i=1 \u2208 {\u00b11}\nd , the root-to-leaf path x 1 , . . . , x d that is defined by starting at the root and moving to the left child if y i = +1 and right if y i = \u22121, there exists a classifier h \u2208 H such that h(x i ) = y i for all i \u2208 [d]-that is, h agrees with the labels we used to reach nodes in the path. In other words, a tree is shattered if every path in the tree is labeled by some hypothesis h \u2208 H. We say that the Littlestone dimension of the classifiers H is d if d is the maximal depth of a tree that is shattered by H.\nIt is not hard to see that Littlestone dimension upper bounds VC dimension and lower bounds logcardinality log(|H|). For binary classifier multi-distribution learning problems, we can strengthen our collaborative learning sample complexity upper bound of Theorem 5.1 to be stated in terms of the Littlestone dimension of a hypothesis class LD (H) rather than log(|H|). This is because there exists an online learning algorithm that guarantees a regret bound of \u03b3 T (Q \u2206(H) ) \u2208 O(LD(H)) that we can have the learner play instead of an algorithm like Hedge. We remark that a similar sample complexity bound can be achieved using the original Standard Optimal Algorithm (SOA) of Littlestone [33] instead of the implicit algorithm of Alon et al. [2], as SOA guarantees a regret bound of \u03b3 T (Q \u2206(H) ) \u2208 O LD(H)T log(T ) .\nVC dimension. It is also nature to ask for the sample complexity of multi-distribution learning in terms of VC dimension VC(H), which characterizes the sample complexity of learning a single data distribution. For example, Blum et al. [9], Nguyen and Zakynthinou [42], Chen et al. [13] provided upper bounds for binary classification multi-distribution learning that are identical to their upper bounds in Table 1 but replacing log(|H|) with VC(H). We now show a similar result to Theorem 5.1 also holds with dependence on the VC dimension of H only when additional mild assumptions hold. In particular, one can run Algorithm 2 on a hypothesis class H \u2032 that is known to be an \u03b5-net of H with respect to each distribution in D. Such an \u03b5-net of size (n/\u03b5) O(VC(H)) necessarily exists (see, e.g., [3]). For example, we can project H onto the union of datapoints sampled from each distribution D \u2208 D. When such a H \u2032 is known in advance, we may directly run Algorithm 2 with H \u2032 .\nCorollary 7.2. Given a set of data distributions D = {D 1 , . . . , D n }, a hypothesis class of binary classifiers H \u2208 {0, 1} X of VC dimension d, and a [0, 1]-bounded loss \u2113, consider the collaborative learning problem (H, D). Suppose we are further given a set of classifiers of size poly (n/\u03b5) d , \u03b5, d, n that is an \u03b5-net of H for each distribution D \u2208 D. There is an algorithm that, with probability 1 \u2212 \u03b4, returns an \u03b5-optimal solution (see (2)) to (H, D) with a sample complexity of O \u03b5 \u22122 (d log(dn/\u03b5) + n log(n/\u03b4)) .\nIt is not strictly necessary to know an \u03b5-net in advance. Instead, one can compute a net from samples or from other information about distributions in D. There a range of assumptions that allow us to compute such an \u03b5-net from samples, without incurring a significant increase in sample complexity. For example, when \u03b5 is sufficiently small, specifically \u03b5 \u2208 O (1/n) (Assumption 1 ), taking an \u03b5-net only increases the sample complexity bound by constant factors versus knowing an \u03b5-net in advance. Additional examples include:\n\u2022 Assumption 2 : we know the marginal distribution for all D \u2208 D;\n\u2022 Assumption 3 : we have access to n marginal distributions P 1 , . . . , P n such that for all x \u2208 X , D i (A) \u2264 p i (A)poly(1/\u03b5, d(H), n) for all A \u2286 X , where p i and D i are the densities of P i and D i , respectively. These latter two assumptions allow one to construct \u03b5-nets of small size for free. Proof. For a data distribution D, we will use D X to denote the marginal distribution of D. We also use the shorthand d \u221e (P ||Q) := sup x\u2208X Q P (x) Q(x) , where d \u03b1 (P ||Q) := 2 D\u03b1(P ||Q) can be understood as the power of the Renyi divergence D \u03b1 (P ||Q). We first recall a standard fact about covering with projections.\nLemma 7.4 (Corollary 3.7 in Haussler and Welzl [25]). Let F be a function class consisting of functions from X to [0, 1] and let P be a probability measure on X . Given N \u2265 8d \u03b5 log 8d \u03b5 + 4 \u03b5 log 2 \u03b4 independent samples x from P, with probability at least 1 \u2212 \u03b4, the projection of F on x constitutes an \u03b5-net. That is, for any\nf 1 , f 2 \u2208 F where Pr x\u223cP (f 1 (x) \u0338 = f 2 (x)) \u2265 \u03b5, \u2225f 1 (x) \u2212 f 2 (x)\u2225 x > 0.\nThe following corollaries of Theorem 5.1 directly imply Theorem 7.3. Corollary 7.5 (Assumption 1). For \u03b5 \u2208 O (1/n), there is an algorithm that, with probability 1 \u2212 \u03b4, returns an \u03b5-optimal solution h \u2208 \u2206(H) using a number of samples that is O d log(dn/\u03b5)+n log(n/\u03b4) \n\u03b5 \u2208 O (1/n), we only needed to sample an additional O d \u03b5 2 log( d \u03b5 ) + n \u03b5 log( n \u03b5 ) \u2282 O nd \u03b5 log( d \u03b5 ) + n \u03b5 log( n \u03b5\n) datapoints to form the cover. Corollary 7.6 (Assumption 2). We say an algorithm has weak unlabeled access if the algorithm can access, for each D \u2208 D, a marginal distribution\nD \u2032 X such that D \u221e (D \u2032 X ||D X ) \u2208 poly(1/\u03b5, d, n), with probability 1 \u2212 \u03b4.\nThere is an algorithm that, given weak access, with probability 1 \u2212 \u03b4, returns an \u03b5-optimal solution h \u2208 \u2206(H) using a number of samples that is\nO d log(dn/\u03b5)+n log(n/\u03b4) \u03b5 2 .\nProof. Observe that when D \u221e (D \u2032 X ||D X ) < \u03b3, D \u2032 X can be written as a mixture over D X with probability at least 1 \u03b3 and some other distribution D X with probability at most 1 \u2212 1 \u03b3 . Once again invoking uniform convergence, we observe that sampling \u0398 D\n\u221e (D \u2032 X ||D X ) d log(d/\u03b5)+log(1/\u03b4) \u03b5 2 i.i.d. samples from distribution D \u2032 X\n, with probability at least 1 \u2212 \u03b4, yields an \u03b5-covering on D. By the Sauer-Shelah lemma, the resulting covering\nH \u2032 D is of size O (poly(1/\u03b5, d, n)) d .\nRepeating this procedure for each D \u2208 D, with probability at least 1 \u2212 n\u03b4, we have an \u03b5-covering\nH \u2032 of D of size |H \u2032 | \u2208 O n(poly(1/\u03b5, d, n)) d .\nWe can then appeal directly to Theorem 5.1 for a sample complexity bound on learning (H \u2032 , D).", "publication_ref": ["b32", "b32", "b32", "b1", "b8", "b41", "b12", "b2", "b1", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Corollary 7.7 (Assumption 3).", "text": "There is an algorithm that, given access to the marginal distribution D X of every D \u2208 D, with probability 1 \u2212 \u03b4, returns an \u03b5-optimal solution h \u2208 \u2206(H) using a number of samples that is O d log(dn/\u03b5)+n log(n/\u03b4) One question left open by these results is whether, for agnostic collaborative learning, it is possible to achieve sample complexity rates of O \u03b5 \u22122 (log(n)VC(H) + n log(n/\u03b4)) without any additional assumptions or a priori knowledge of an \u03b5-net. It also remains an open question whether the log(n) factor in the log(n)VC(H)/\u03b5 2 term is necessary for VC classes, as Theorem 5.1 proves that, for finite/online-learnable classes with sample complexities expressed in terms of log(|H|) or Littlestone dimension LD(H), no such log(n) factor is necessary. We refer interested readers to Awasthi et al. [4] for a complete discussion of these open problems.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Analysis of On-Demand Sampling for Group DRO", "text": "This section describes experiments where we adapt our on-demand sampling-based multi-distribution learning algorithm for deep learning applications. In particular, we compare our algorithm against the de facto standard multi-distribution learning algorithm for deep learning, Group DRO (GDRO) [50]. As GDRO is designed for use with offline-collected datasets, to provide a meaningful comparison, we modify our algorithm to work on offline datasets (i.e., with no on-demand sample access).", "publication_ref": ["b49"], "figure_ref": [], "table_ref": []}, {"heading": "Worst-Group Accuracy", "text": "Gap in Avg. vs. Worst-Group Acc.  Resampling Multi-Distribution Learning (R-MDL). To be more suitable for deep learning applications, we instantiate Algorithm 2 by choosing a minibatch gradient descent algorithm as the minimizing player's algorithm (Q \u0398 ) and a naive uniform-sampling bandit algorithm as the maximizing player's algorithm (Q \u2206(D) ).\nWe can further adapt our algorithm to offline datasets by simulating on-demand sampling on the empirical distributions of datasets. This modified algorithm, R-MDL, is described in full in Algorithm 3. Note that, in contrast, the original group DRO algorithm of Sagawa et al. [50] is also a minibatch gradient descent algorithm but samples minibatches uniformly from all distributions and weights datapoints via a no-regret algorithm that provides importance weights. Though effective, this method is brittle and requires tricks like unconventionally strong regularization [50]. Our theory of on-demand sampling suggests that R-MDL should mollify this brittleness, as it replaces GDRO's upweighting of low-accuracy distributions with upsampling of low-accuracy distributions. Interestingly, the advantage of resampling over reweighting has been previously observed when training neural networks on a dataset with fixed importance weights [51].\nExperiment Setting In Table 2, we replicate the Group DRO experiments of Sagawa et al. [50] and compare the standard GDRO algorithm with our R-MDL algorithm (Algorithm 3). We fine-tune Resnet-50 models (convolutional neural networks) [26] and BERT models (transformer-based network) [17] on the image classification datasets Waterbirds [50,56] and CelebA [34] and the natural language dataset MultiNLI [57] respectively. We train these models in 3 settings: with standard hyperpameters, under strong weight decay (\u2113-2) regularization, or under early stopping. R-MDL consistently outperforms GDRO and ERM. In every dataset and in almost every setting, R-MDL significantly outperforms GDRO and ERM in worst-group accuracy. In addition, whereas GDRO and ERM have large gaps between worst-group accuracy and average accuracy, R-MDL has almost matching worst-group and average accuracies. This indicates that R-MDL is more effective at prioritizing learning on difficult groups. R-MDL is robust to regularization strength. R-MDL retains high worst-group accuracy even without strong regularization. These results challenge the observation of Sagawa et al. [50] that strong regularization is critical for the performance of Group DRO methods. This suggests that the brittleness of GDRO is due to the reweighting rendering the adversary too weak. In contrast, R-MDL provides a robust multi-distribution learning method with significantly less hyperparameter sensitivity.", "publication_ref": ["b49", "b49", "b50", "b49", "b25", "b16", "b49", "b55", "b33", "b56", "b49"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Conclusions", "text": "While learning from a single data distribution is a fundamental abstraction of data-driven pattern recognition, data-driven decision-making calls for a new perspective that captures learning problems involving multiple stakeholders and data sources. This work proposes multi-distribution learning as a unifying theoretical framework, bringing together a number of widely studied problem formulations such as group distributionally robust optimization and collaborative PAC learning under a single umbrella. This unifying perspective distills the challenges of these various learning problems to a fundamental question about the sample complexity of stochastic games. We answered this fundamental question by providing optimal rates for a broad class of problems including convex and Littlestone hypothesis classes, highlighting the importance of on-demand sampling for decoupling the complexity of learning and obtaining robustness. We believe these findings underscore a broader takeaway that adaptive data collection is fundamental for scalable learning outside the single-distribution paradigm of classical pattern recognition. Figure 1: Training (light, dashed) and validation (dark, solid) accuracies for GDRO and R-MDL during training, plotted on a log scale. Note that R-MDL validation accuracy will be noisier than those of GDRO as we constrain R-MDL to limited samples (with replacement) from the validation set. In addition, in the left-most plot, training accuracy for all groups except the blond male group (red) dips to zero due to lack of data-this is because the blond male group (red) is the most challenging so the adversary eventually stops sampling from other groups. Under standard regularization, the red-group accuracy drops off in GDRO while R-MDL maintains a high red-group accuracy by heavily sampling from the red group, as reflected in the near-perfect red-group training error.\nthe CelebA dataset; the smallest distribution (blond-hair + male) is represented by only 1,387 datapoints. We use the official training-testing-validation dataset split.\nThe Waterbirds dataset is a dataset by Sagawa et al. [50] curated from a larger Caltech-UCSD Birds-200-2011 (CUB) dataset [56]. It concerns the task of predicting whether a bird is of some waterbird (sub)species from an image of said bird. This dataset is challenging because traditional ERM models are prone to spuriously correlating backgrounds with foreground subjects; for instance, a model may often predict that a bird is a waterbird only because the image of the bird was taken at a beach. The dataset has 4 distributions: the Cartesian product of the waterbird vs not waterbird label with whether the background of the picture is over water. There are 4,795 datapoints available in the Waterbirds dataset; the smallest distribution (waterbirds on land) is represented by only 56 examples.\nModels. We use two classes of models in our experiments: Resnet-50 [26] and BERT [17]. We use the torchvision [37] implementation of the convolutional neural network Resnet-50, with a default choice of a stochastic gradient descent optimizer with momentum 0.9 and batch size 128. Batch normalization is used; data augmentation and dropout are not used. We use the HuggingFace [58] implementation of the language model BERT, with a default choice of an Adam optimizer with dropout and batch size 32.\nHyperparameters. In the Standard Regularization experiments, we use a Resnet-50 model with an \u2113-2 regularization parameter of \u03bb = 0.0001 and a fixed learning rate of \u03b1 = 0.001 for both Waterbirds and CelebA datasets. The ERM and Group DRO baselines are trained on CelebA for 50 epochs and Waterbirds for 300 epochs. Our multi-distribution learning method is trained on CelebA for only 20 epochs and Waterbirds for 100 epochs; this is due to the faster training error convergence of our method. For the MultiNLI dataset, we use a BERT model with a linearly decaying learning rate starting at \u03b1 0 = 0.00002 and no \u2113-2 regularization. The ERM and Group DRO baselines are trained on Multi-NLI for 20 epochs. Our multi-distribution learning method is trained on Multi-NLI for only 10 epochs. Our multi-distribution learning method uses adversary learning rates \u03b7 + of 1, 1, 0.2 on Waterbirds, CelebA and MultiNLI respectively.\nIn the Strong Regularization experiments, we follow similar settings to the Standard Regularization experiments. The only change is that an \u2113-2 regularization parameter of \u03bb = 1 is used for Waterbirds and an \u2113-2 regularization parameter of \u03bb = 0.1 is used for CelebA. Our multi-distribution learning method uses adversary learning rates \u03b7 + of 1 and 0.2 on Waterbirds and CelebA respectively.\nIn the Early Stopping experiments, we follow similar settings to the Standard Regularization experiments. The only change is that all CelebA and Waterbird experiments are run for a single epoch. MultiNLI experiments are run for 3 epochs. Our multi-distribution learning method uses adversary learning rates \u03b7 + of 1, 1, 1 on Waterbirds, CelebA and MultiNLI respectively.\nThe only hyperparameters we use that differ from prior literature are the number of training epochs and the adversary learning rates of our method (R-MDL). The choice of epoch was not fine-tuned, and was selected due to our observation of early training error convergence. We selected our adversary learning rate \u03b7by training our method, on each dataset, for both \u03b7 -= 1 and \u03b7 -= 0.2 and selecting the \u03b7yielding the highest validation-split worst-group accuracy.\nCompute. The total amount of compute run for the experiments in this section is approximately 50 GPU hours. A \"n1-standard-8\" machine was leased from the Cloud computing service Google Cloud; the \"n1-standard-8\" machine was equipped with 8 Intel Broadwell chips and 1 NVIDIA Tesla V100 GPU. The cost of these computing resources totaled approximately USD $2 per hour, with a total cost of approximately USD $100. All results described in this section, with the exception of existing results cited from other works, were obtained with experiments on said machine. All experiments were implemented in Python and PyTorch.", "publication_ref": ["b49", "b55", "b25", "b16", "b36", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This work was supported in part by the National Science Foundation under grant CCF-2145898, a C3.AI Digital Transformation Institute grant, and the Mathematical Data Science program of the Office of Naval Research. This work was partially done while Haghtalab and Zhao were visitors at the Simons Institute for the Theory of Computing. The authors thank the authors of Chen et al. [13] and Sagawa et al. [50] for communication regarding their work. The authors also thank Tianyi Lin, Guy Rothblum, Abhishek Shetty, Tatjana Chavdarova, Lydia Zakynthinou, and Mingda Qiao for valuable discussions.", "publication_ref": ["b12", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 3 Resampling-based Multi-Distribution Learning (R-MDL)", "text": "Input: Parameter space \u0398, iterations T , batch size B and adversary batch size B \u2032 , and training and validation datasets X tr,i and X val,i for i \u2208 [n]; Initialize: \u03b8 (0) \u2208 \u0398 and w (0) = [1/n] n ; for t = 1, 2, . . . , T do For i \u2208 [n], randomly sample (with replacement)\n, see Equation 4;\nRandomly sample (with replacement) the datapoints\nRun a gradient descent update(s)\n; end for Return:\nA Experiment Details R-MDL Algorithm. The R-MDL algorithm is defined in full in Algorithm 3. It instantiates (a batched version of) Algorithm 2 choosing Q \u0398 to be online gradient descent and Q \u2206n to be a naive bandit-to-fullinformation reduction algorithm that implements Exp3 but observes cost functions uniformly at random and re-uses cost function observations between rounds. This algorithm is an example of instantiating our general multi-distribution learning framework with more practical choices of learning algorithms. An example implementation, along with experiment replications, is provided in the Github repository er-iczhao28/multidistributionlearning.\nAdditional Observation: R-MDL converges faster than ERM or GDRO. The R-MDL methods in Table 2 used a fraction of the training epochs that their GDRO counterparts used. The ratio of R-MDL to GDRO training epochs is 1:3, 2:5, 1:2 on the Waterbirds, CelebA, and MultiNLI datasets respectively. This fast convergence rate is predicted by our theory, particularly Corollary 6.2. In our Figure 1, we also replicate the Figure 2 of Sagawa et al. [50], appending our additional results on R-MDL. We again see a trend of faster test error convergence (solid lines) and more uniform per-group risks by the R-MDL algorithm.", "publication_ref": ["b49"], "figure_ref": [], "table_ref": []}, {"heading": "Datasets.", "text": "Our experiments were performed on three datasets: Multi-NLI, CelebA, and Waterbirds [50]. We use identical preprocessing settings and dataset splits as Sagawa et al. [50]. Our experiments, unless otherwise specified, replicate the exact hyperparameter settings adopted by Sagawa et al. [50] for their Table 2 experiments. This includes the choice of random seeds, batch sizes, learning rates, learning schedules, and regularization. We defer readers to Sagawa et al. [50] or to our public source code for replication details.\nThe Multi-NLI dataset [57] concerns the following natural language inference task: determine if one statement is entailed by, neutral with, or contradicts a given statement. This dataset is challenging because traditional ERM models are prone to spuriously correlating \"contradiction\" labels with the existence of negation words. The dataset is divided into 6 distributions: the Cartesian product of the label space (entailment, neutral, contradiction) and an indicator of whether the sentence contains a negation word. The label space annotations were annotated by [57] while negation labels were annotated by Sagawa et al. [50]. There are 206,175 datapoints available in the Multi-NLI dataset; the smallest distribution (entailment + negation) is represented by only 1,521 datapoints. We use a randomly shuffled 50-20-30 training-validation-testing split.\nThe CelebA dataset is a dataset of celebrity face images and a label space of potential physical attributes [34]. This dataset is challenging because traditional ERM models are prone to spuriously correlating attribute labels with demographic information such as race and gender. Following Sagawa et al. [50], we divide the dataset into 4 distributions: the Cartesian product of the blond vs dark hair attribute label (\"Blond_Hair\") with the \"gender\" attribute label (\"Male\"). Note that the authors of Liu et al. [34] limited the \"gender\" attribute label to binary options of male and not male. There are 162,770 datapoints available in", "publication_ref": ["b49", "b49", "b49", "b49", "b56", "b56", "b49", "b33", "b49", "b33"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "From bandits to experts: a tale of domination and independence", "journal": "Curran Associates, Inc", "year": "2013", "authors": "N Alon; N Cesa-Bianchi; C Gentile; Y Mansour"}, {"ref_id": "b1", "title": "Adversarial laws of large numbers and optimal regret in online classification", "journal": "ACM", "year": "2021", "authors": "N Alon; O Ben-Eliezer; Y Dagan; S Moran; M Naor; E Yogev"}, {"ref_id": "b2", "title": "Neural Network Learning -Theoretical Foundations", "journal": "Cambridge University Press", "year": "2002", "authors": "M Anthony; P L Bartlett"}, {"ref_id": "b3", "title": "Open problem: The sample complexity of multi-distribution learning for vc classes", "journal": "", "year": "2023", "authors": "P Awasthi; N Haghtalab; E Zhao"}, {"ref_id": "b4", "title": "Distributed learning, communication complexity and privacy", "journal": "", "year": "2012", "authors": "M.-F Balcan; A Blum; S Fine; Y Mansour"}, {"ref_id": "b5", "title": "Mirror descent and nonlinear projected subgradient methods for convex optimization", "journal": "Operations Research Letters", "year": "2003", "authors": "A Beck; M Teboulle"}, {"ref_id": "b6", "title": "Exploiting task relatedness for multiple task learning", "journal": "Springer", "year": "2003", "authors": "S Ben-David; R Schuller"}, {"ref_id": "b7", "title": "Robust Optimization", "journal": "Princeton University Press", "year": "2009", "authors": "A Ben-Tal; L El Ghaoui; A Nemirovski"}, {"ref_id": "b8", "title": "Collaborative PAC learning", "journal": "Curran Associates, Inc", "year": "2017", "authors": "A Blum; N Haghtalab; A D Procaccia; M Qiao ; I. Guyon; U V Luxburg; S Bengio; H Wallach; R Fergus; S Vishwanathan; R Garnett"}, {"ref_id": "b9", "title": "One for one, or all for all: equilibria and optimality of collaboration in federated learning", "journal": "PMLR", "year": "2021", "authors": "A Blum; N Haghtalab; R L Phillips; H Shao"}, {"ref_id": "b10", "title": "Communication-aware collaborative learning", "journal": "AAAI Press", "year": "", "authors": "A Blum; S Heinecke; L Reyzin"}, {"ref_id": "b11", "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning", "journal": "", "year": "2011-01", "authors": "S Boyd; N Parikh; E Chu; B Peleato; J Eckstein"}, {"ref_id": "b12", "title": "Tight bounds for collaborative PAC learning via multiplicative weights", "journal": "Curran Associates, Inc", "year": "2018", "authors": "J Chen; Q Zhang; Y Zhou"}, {"ref_id": "b13", "title": "Near-optimal no-regret algorithms for zero-sum games", "journal": "SIAM", "year": "2011", "authors": "C Daskalakis; A Deckelbaum; A Kim"}, {"ref_id": "b14", "title": "Near-optimal no-regret learning in general games", "journal": "Curran Associates, Inc", "year": "2021", "authors": "C Daskalakis; M Fishelson; N Golowich"}, {"ref_id": "b15", "title": "Efficient protocols for distributed classification and optimization", "journal": "Springer", "year": "2012", "authors": "H Daum\u00e9; J M Phillips; A Saha; S Venkatasubramanian"}, {"ref_id": "b16", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova"}, {"ref_id": "b17", "title": "Learning models with uniform performance via distributionally robust optimization", "journal": "The Annals of Statistics", "year": "2021", "authors": "J C Duchi; H Namkoong"}, {"ref_id": "b18", "title": "Outcome indistinguishability", "journal": "ACM", "year": "2021", "authors": "C Dwork; M P Kim; O Reingold; G N Rothblum; G Yona"}, {"ref_id": "b19", "title": "A general lower bound on the number of examples needed for learning", "journal": "Information and Computation", "year": "1989", "authors": "A Ehrenfeucht; D Haussler; M Kearns; L Valiant"}, {"ref_id": "b20", "title": "A decision-theoretic generalization of on-line learning and an application to boosting", "journal": "Journal of Computer and System Sciences", "year": "1997", "authors": "Y Freund; R E Schapire"}, {"ref_id": "b21", "title": "A unifying perspective on multi-calibration: Game dynamics for multi-objective learning", "journal": "", "year": "2022", "authors": "N Haghtalab; M Jordan; E Zhao"}, {"ref_id": "b22", "title": "A simple adaptive procedure leading to correlated equilibrium", "journal": "Econometrica", "year": "2000", "authors": "S Hart; A Mas-Colell"}, {"ref_id": "b23", "title": "Fairness without demographics in repeated loss minimization", "journal": "PMLR", "year": "2018", "authors": "T B Hashimoto; M Srivastava; H Namkoong; P Liang"}, {"ref_id": "b24", "title": "Epsilon-nets and simplex range queries", "journal": "Association for Computing Machinery", "year": "1986", "authors": "D Haussler; E Welzl"}, {"ref_id": "b25", "title": "Deep residual learning for image recognition", "journal": "IEEE Computer Society", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b26", "title": "Metric entropy duality and the sample complexity of outcome indistinguishability", "journal": "PMLR", "year": "2022", "authors": "L Hu; C Peale; O Reingold"}, {"ref_id": "b27", "title": "Solving variational inequalities with stochastic mirror-prox algorithm", "journal": "Stochastic Systems", "year": "2011", "authors": "A Juditsky; A Nemirovski; C Tauvel"}, {"ref_id": "b28", "title": "Meta-Sim: learning to generate synthetic datasets", "journal": "IEEE", "year": "2019", "authors": "A Kar; A Prakash; M.-Y Liu; E Cameracci; J Yuan; M Rusiniak; D Acuna; A Torralba; S Fidler"}, {"ref_id": "b29", "title": "Noisy binary search and its applications", "journal": "SIAM", "year": "2007", "authors": "R M Karp; R Kleinberg"}, {"ref_id": "b30", "title": "Federated optimization: Distributed machine learning for on-device intelligence", "journal": "", "year": "2016", "authors": "J Kone\u010dn\u00fd; H B Mcmahan; D Ramage; P Richt\u00e1rik"}, {"ref_id": "b31", "title": "Federated learning: strategies for improving communication efficiency", "journal": "", "year": "2016", "authors": "J Kone\u010dn\u00fd; H B Mcmahan; F X Yu; P Richt\u00e1rik; A T Suresh; D Bacon"}, {"ref_id": "b32", "title": "Learning quickly when irrelevant attributes abound: a new linear-threshold algorithm", "journal": "", "year": "1987", "authors": "N Littlestone"}, {"ref_id": "b33", "title": "Deep learning face attributes in the wild", "journal": "IEEE Computer Society", "year": "2015", "authors": "Z Liu; P Luo; X Wang; X Tang"}, {"ref_id": "b34", "title": "From bandits to experts: On the value of side-observations", "journal": "", "year": "2011", "authors": "S Mannor; O Shamir"}, {"ref_id": "b35", "title": "Domain Adaptation with Multiple Sources", "journal": "Curran Associates, Inc", "year": "2008", "authors": "Y Mansour; M Mohri; A Rostamizadeh"}, {"ref_id": "b36", "title": "Torchvision the machine-vision package of torch", "journal": "ACM", "year": "2010", "authors": "S Marcel; Y Rodriguez"}, {"ref_id": "b37", "title": "Communication-efficient learning of deep networks from decentralized data", "journal": "PMLR", "year": "2017", "authors": "B Mcmahan; E Moore; D Ramage; S Hampson; B A Y Arcas"}, {"ref_id": "b38", "title": "Agnostic federated learning", "journal": "PMLR", "year": "2019", "authors": "M Mohri; G Sivek; A T Suresh"}, {"ref_id": "b39", "title": "Problem Complexity and Method Efficiency in Optimization", "journal": "Wiley-Interscience", "year": "1983", "authors": "A S Nemirovskij; D B Yudin"}, {"ref_id": "b40", "title": "Explore no more: Improved high-probability regret bounds for non-stochastic bandits", "journal": "", "year": "2015", "authors": "G Neu"}, {"ref_id": "b41", "title": "Improved algorithms for collaborative PAC learning", "journal": "Curran Associates, Inc", "year": "2018", "authors": "H L Nguyen; L Zakynthinou"}, {"ref_id": "b42", "title": "The sample complexity of multi-distribution learning", "journal": "", "year": "2023", "authors": "B Peng"}, {"ref_id": "b43", "title": "Quantized incremental algorithms for distributed optimization", "journal": "IEEE Journal on Selected Areas in Communications", "year": "2005", "authors": "M G Rabbat; R D Nowak"}, {"ref_id": "b44", "title": "Optimization, learning, and games with predictable sequences", "journal": "", "year": "2013", "authors": "A Rakhlin; K Sridharan"}, {"ref_id": "b45", "title": "Fair attribute classification through latent space de-biasing", "journal": "", "year": "2021", "authors": "V V Ramaswamy; S S Kim; O Russakovsky"}, {"ref_id": "b46", "title": "A stochastic approximation method. The Annals of Mathematical Statistics", "journal": "", "year": "1951", "authors": "H Robbins; S Monro"}, {"ref_id": "b47", "title": "An iterative method of solving a game", "journal": "Annals of Mathematics", "year": "1951", "authors": "J Robinson"}, {"ref_id": "b48", "title": "Multi-group agnostic PAC learnability", "journal": "PMLR", "year": "2021", "authors": "G N Rothblum; G Yona"}, {"ref_id": "b49", "title": "Distributionally robust neural networks", "journal": "", "year": "2020", "authors": "S Sagawa; P W Koh; T B Hashimoto; P Liang"}, {"ref_id": "b50", "title": "An investigation of why overparameterization exacerbates spurious correlations", "journal": "PMLR", "year": "2020", "authors": "S Sagawa; A Raghunathan; P W Koh; P Liang"}, {"ref_id": "b51", "title": "Communication-efficient distributed optimization using an approximate newton-type method", "journal": "PMLR", "year": "2014", "authors": "O Shamir; N Srebro; T Zhang"}, {"ref_id": "b52", "title": "Simple and near-optimal algorithms for hidden stratification and multi-group learning", "journal": "PMLR", "year": "2022", "authors": "C J Tosh; D Hsu"}, {"ref_id": "b53", "title": "A theory of the learnable", "journal": "ACM", "year": "1984", "authors": "L G Valiant"}, {"ref_id": "b54", "title": "Algorithms for Convex Optimization", "journal": "Cambridge University Press", "year": "2021", "authors": "N K Vishnoi"}, {"ref_id": "b55", "title": "The Caltech-UCSD Birds-200-2011 dataset", "journal": "", "year": "2011", "authors": "C Wah; S Branson; P Welinder; P Perona; S Belongie"}, {"ref_id": "b56", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "A Williams; N Nangia; S Bowman"}, {"ref_id": "b57", "title": "Huggingface's transformers: state-of-the-art natural language processing", "journal": "", "year": "2019", "authors": "T Wolf; L Debut; V Sanh; J Chaumond; C Delangue; A Moi; P Cistac; T Rault; R Louf; M Funtowicz"}, {"ref_id": "b58", "title": "DeceptionNet: network-driven domain randomization", "journal": "IEEE", "year": "2019", "authors": "S Zakharov; W Kehl; S Ilic"}, {"ref_id": "b59", "title": "Information-theoretic lower bounds of PAC sample complexity", "journal": "", "year": "2019", "authors": "C Zhang"}, {"ref_id": "b60", "title": "Stochastic approximation approaches to group distributionally robust optimization", "journal": "", "year": "2023", "authors": "L Zhang; P Zhao; T Yang; Z Zhou"}, {"ref_id": "b61", "title": "Optimal multi-distribution learning", "journal": "", "year": "2023", "authors": "Z Zhang; W Zhan; Y Chen; S S Du; J D Lee"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "max D\u2208D R D (h) \u2264 OPT + \u03b5, where OPT := min h\u2208H max D\u2208D R D (h).", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fact 2 . 1 .21Consider a collaborative learning problem (H, D). Define the relaxed loss function \u2113 :", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fact 2 . 2 .22Consider a collaborative learning problem (H, D) on a set of binary classifiers. For any randomized solution h \u2208 \u2206(H), define the deterministic hypothesis h Maj as h Maj (x) = 1[Pr f \u223ch (f (x) = 1) > 1 2 ]. The expected loss of h M aj is bounded by max D\u2208D R D (h Maj ) \u2264 2 max D\u2208D R D (h).", "figure_data": ""}, {"figure_label": "31", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fact 3 . 1 .31Given a multi-distribution learning problem, (D, L, H), define the zero-sum game (A \u2212 , A + , \u03d5) where A \u2212 = H, A + = D \u00d7 L, and \u03d5(p, q) = R q (p). In any \u03b5-min-max equilibrium (p, q), p is a 2\u03b5-optimal solution.", "figure_data": ""}, {"figure_label": "34", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fact 3 . 4 .34Let c (1:T ) be independent, unbiased estimates of a set of linear costs c (1:T ) , where c (t) \u2264 L and c (t) \u2264 L at all steps t \u2208 [T ]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "for any fixed a * \u2208 A. We now turn to proving Lemma 3.1.Proof of Lemma 3.1. Let c -(\u03c4 ) (p) = g -(p (\u03c4 ) , q (\u03c4 )), p and c + (\u03c4 ) (q) = g + (p (\u03c4 ) , q (\u03c4 ) ), q denote the cost functions that the online learning algorithms Q A\u2212 and Q A+ are given in Algorithm 1. We first recall that the bounds on the regret for Q A\u2212 and Q A+ yield the empirical regret bounds Reg p (1:T ) , c -(1:T ) \u2264 L \u03b3 T (Q A\u2212 )T , Reg q (1:T ) , c + (1:T ) \u2264 L \u03b3 T (Q A+ )T , as c -(1:T ) and c + (1:T ) are linear cost functions with a bounded norm of at most L. By Fact 3.4, with probability 1 \u2212 2\u03b4, we can bound the generalization error with the true cost functions c -(\u03c4 ) (p) = \u2207 p (\u03c4 ) \u03d5(p (\u03c4 ) , q (\u03c4 ) ), p and c + (\u03c4 ) (q) = \u2212 \u2207 q (\u03c4 ) \u03d5(p (\u03c4 ) , q (\u03c4 ) ), q by", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "1 \u03b5 55log(n) log(|H|) log 1 \u03b5 + n log n \u03b4 by a multiplicative factor of 1 \u03b5 3 log (n) log 1 \u03b5 . Theorem 5.1. Given a set of data distributions D = {D 1 , . . . , D n }, a hypothesis class H \u2208 Y X , and a [0, 1]-bounded loss \u2113, consider the collaborative learning problem (H, D). Consider the output h \u2208 \u2206(H) of applying Theorem 4.1's algorithm to the multi-distribution learning problem (D, {\u2113} , \u2206(H)) where the online learning algorithm Q \u2206(H) is Hedge. With probability 1 \u2212 \u03b4, h is an \u03b5-optimal solution (see (2)) to (H, D) and the sample complexity is O \u03b5 \u22122 (log(|H|) + n log(n/\u03b4)) .", "figure_data": ""}, {"figure_label": "52", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Corollary 5 . 2 (52Theorem 5.1 and Fact 2.2). Consider a collaborative learning problem (H, D) on a set of binary classifiers. There is an algorithm with a sample complexity of O \u03b5 \u22122 (log(|H|) + n log(n/\u03b4)) that, with probability 1 \u2212 \u03b4, returns a deterministic improper solution", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "every (H, D) \u2208 V, |D| = n and |H| = 2 d .", "figure_data": ""}, {"figure_label": "55", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Lemma 5 . 5 .55Take any n, d \u2208 Z + , \u03b5, \u03b4 \u2208 (0, 1/8), and (\u03b5, \u03b4)-collaborative learning algorithm Q. There exists a set of collaborative learning problems V on which Q takes at least \u2126 1 \u03b5 2 (n log(k/\u03b4)) samples and where, for every (H, D) \u2208 V, |D| = n and |H| = 2 d with k := min {n, d}.", "figure_data": ""}, {"figure_label": "51", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": ".Proof of Claim 5 . 1 .51Since our desired lower bound is weakly monotonic in n, d, we fix the smallest choice of \u03b7, d \u2208 Z + and \u03b5, \u03b4 \u2208 (0, 1/8) such that n = \u03b7 \u2022 d. Combining claims 5.1 and 5.2, we see that any (\u03b5, \u03b4) collaborative learning algorithm Q for V n,d has an expected sample complexity on P n,d of at least N \u2265 7n 2048\u03b5 2 log 7d 16\u03b4 . By the probabilistic method, for at least some collaborative learning problem in the set V n,d , our learning algorithm Q must have a sample complexity of \u2126 7n 2048\u03b5 2 log 7d16\u03b4 Consider \u03b7 two-sided coins. Under a H 0 hypothesis, all coins are biased towards tails with probability 1/2 + 2\u03b5. Under a H i hypothesis, the ith coin is biased towards heads with probability 1/2 + 4\u03b5. Let Pr be a probability distribution on H \u2208 {H i }\u03b7 i=0 with Pr(H 0 ) = 1/2 and Pr(H 1 ) = \u2022 \u2022 \u2022 = Pr(H \u03b7 ) = 1 2\u03b7", "figure_data": ""}, {"figure_label": "71", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Theorem 7 . 1 (71Littlestone Dimension Variant of Theorem 5.1). Given a set of data distributions D = {D 1 , . . . , D n }, a hypothesis class of binary classifiers H \u2208 {0, 1} X , and a [0, 1]-bounded loss \u2113, consider the collaborative learning problem (H, D). Consider the output h \u2208 \u2206(H) of applying Theorem 4.1's algorithm to the multi-distribution learning problem (D, {\u2113} , \u2206(H)) where the online learning algorithm Q \u2206(H) is the agnostic Standard-Optimal-Algorithm of Alon et al. [2]. With probability 1 \u2212 \u03b4, h is an \u03b5-optimal solution (see (2)) to (H, D) and the sample complexity is O \u03b5 \u22122 (LD (H) + n log(n/\u03b4)) . Proof. By Fact 2.1, we can reduce the collaborative learning problem (H, D) to solving the multi-distribution learning problem (D, {\u2113} , \u2206(H)) The agnostic SOA algorithm of Alon et al. [2] guarantees a regret bound of \u03b3 T (Q \u2206(H) ) = LD(H). Our claim therefore follows by Theorem 4.1.", "figure_data": ""}, {"figure_label": "73", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Theorem 7 . 3 .73Given a set of data distributions D = {D 1 , . . . , D n }, a hypothesis class of binary classifiers H \u2208 {0, 1}X of VC dimension d, and a [0, 1]-bounded loss \u2113, consider the collaborative learning problem (H, D). If any of Assumptions 1, 2 or 3 is met, there is an algorithm that, with probability 1 \u2212 \u03b4, returns an \u03b5-optimal solution (see(2)) to (H, D) with a sample complexity of O \u03b5 \u22122 (d log(dn/\u03b5) + n log(n/\u03b4)) .", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "\u03b5 2 . 2 d.22Proof. By Lemma 7.4, sampling O nd \u03b5 log( d \u03b5 ) + n \u03b5 log( n \u03b5 ) datapoints provides a covering of H is that is simultaneously an \u03b5-net for every D \u2208 D with probability at least 1 \u2212 \u03b4. Moreover, by the Sauer-Shelah lemma, this net is of size O log(dn/\u03b5)+n log(n/\u03b4) \u03b5 The claim then follows from Corollary 7.2, noting that since", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "\u03b5 2 . 2 i22Proof. By uniform convergence, taking \u0398 d log(d/\u03b5)+log(1/\u03b4) \u03b5 .i.d. samples from distribution D X for each D \u2208 D, with probability at least 1 \u2212 \u03b4, yields an \u03b5-covering on every D \u2208 D. By the Sauer-Shelah lemma, the resulting coveringH \u2032 D is of size O (n\u03b5 \u22122 (log(d/\u03b5) + 1 d log(1/\u03b4))) d .We then appeal to Corollary 7.2.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Worst-group accuracy (our primary performance metric) and the gap between worst-group accuracy and average accuracy, of empirical risk minimization (ERM), Group DRO (GDRO), and our R-MDL algorithm in three experiment settings-standard hyperparameters (Standard Reg.), inflated weight decay regularization (Strong Reg.), and early stopping (Early Stop)-and on three datasets-Waterbirds, CelebA, and MultiNLI. Figures are percentages evaluated on the test split of each dataset, with standard deviation in parentheses. R-MDL consistently outperforms GDRO and performs reliably with or without strong regularization.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "max D\u2208D max \u2113\u2208L R D,\u2113 (h) \u2264 OPT + \u03b5, where OPT := min h\u2208H max D\u2208D max \u2113\u2208L R D,\u2113 (h).(1)", "formula_coordinates": [5.0, 157.35, 214.41, 383.82, 15.33]}, {"formula_id": "formula_2", "formula_text": ") := E f \u223ch [R D (f )].", "formula_coordinates": [6.0, 157.97, 149.38, 80.25, 10.0]}, {"formula_id": "formula_3", "formula_text": "\u2206(H) \u2192 [0, 1] as \u2113(h, z) = E f \u223ch [\u2113(f, z)].", "formula_coordinates": [6.0, 71.24, 251.05, 468.76, 24.6]}, {"formula_id": "formula_4", "formula_text": "Y 2 \u2192 [0, 1].", "formula_coordinates": [6.0, 166.56, 379.33, 50.14, 10.31]}, {"formula_id": "formula_5", "formula_text": "max D\u2208D R D (h \u03b8 ) \u2264 OPT + \u03b5, where OPT := min \u03b8 * \u2208\u0398 max D\u2208D R D (h \u03b8 * ).(3)", "formula_coordinates": [6.0, 175.26, 594.4, 365.9, 15.33]}, {"formula_id": "formula_6", "formula_text": "A \u2212 \u00d7 A + \u2192 [0, 1]", "formula_coordinates": [7.0, 224.34, 142.87, 76.09, 9.65]}, {"formula_id": "formula_7", "formula_text": "Reg(a (1:T ) , c (1:T ) ) := T t=1 c (t) (a (t) ) \u2212 min a * \u2208A T t=1 c (t) (a * ).", "formula_coordinates": [7.0, 191.77, 306.3, 228.47, 30.2]}, {"formula_id": "formula_8", "formula_text": "(1:T ) ) \u2264 \u03b3 T (Q A )T .", "formula_coordinates": [7.0, 252.19, 369.22, 89.63, 11.23]}, {"formula_id": "formula_9", "formula_text": "Hedge A (c (1:t\u22121) ) := a (t) / a (t) 1", "formula_coordinates": [7.0, 131.57, 483.89, 138.62, 17.2]}, {"formula_id": "formula_10", "formula_text": "t\u22121 \u03c4 =1 c (\u03c4 ) (e i ) ,(4)", "formula_coordinates": [7.0, 420.67, 475.55, 120.5, 30.2]}, {"formula_id": "formula_11", "formula_text": "(1:T ) , c (1:T ) ) \u2264 2 log(n)/T .", "formula_coordinates": [7.0, 356.35, 556.61, 120.14, 10.31]}, {"formula_id": "formula_12", "formula_text": "(e i )} i\u2208I (1) , . . . , {c (t\u22121) (e i )} i\u2208I (t\u22121) \u2192 a (t) , I (t) .", "formula_coordinates": [7.0, 217.19, 640.76, 197.61, 12.53]}, {"formula_id": "formula_13", "formula_text": "R q (p) \u2264 min h * \u2208H R q (h * ) + \u03b5 and R q (p) \u2265 max D * \u2208D,\u2113 * \u2208L R D * ,\u2113 * (p) \u2212 \u03b5.", "formula_coordinates": [8.0, 166.99, 435.38, 278.03, 16.73]}, {"formula_id": "formula_14", "formula_text": "D * \u2208D,\u2113 * \u2208L R D * ,\u2113 * (p) \u2264 min h * \u2208H R q (h * ) + 2\u03b5 \u2264 OPT + 2\u03b5.", "formula_coordinates": [8.0, 171.58, 462.73, 253.42, 11.23]}, {"formula_id": "formula_15", "formula_text": "i.i.d. \u223c D and (D, \u2113) i.i.d. \u223c q, is an unbiased bounded estimate of the first-order information \u2207 \u03b8 R q (h \u03b8 ); i.e., E z\u223cD,(D,\u2113)\u223cq [\u2207 \u03b8 \u2113(h \u03b8 , z)] = \u2207 \u03b8 R q (h \u03b8 ) and \u2225\u2207 \u03b8 \u2113(h \u03b8 , z)\u2225 \u2264 1. Similarly, the vector [1 \u2212 \u2113 j (h \u03b8 , z i )] i\u2208[n],j\u2208[m] where z i i.i.d.", "formula_coordinates": [9.0, 71.23, 163.61, 468.49, 38.8]}, {"formula_id": "formula_16", "formula_text": "Q A\u2212 , Q A+ ; for t = 1, 2, . . . , T do Let p (t) = Q A\u2212 p \u2192 g -(p (\u03c4 ) , q (\u03c4 ) ), p \u03c4 \u2208[t\u22121] ; Let q (t) = Q A+ q \u2192 g + (p (\u03c4 ) , q (\u03c4 ) ), q \u03c4 \u2208[t\u22121] ; end for Return p = 1 T T t=1 p (t) and q = 1 T T t=1 q (t) ;", "formula_coordinates": [9.0, 81.96, 273.39, 459.31, 83.84]}, {"formula_id": "formula_17", "formula_text": "\u2212 and A + have a diameters of at most R in the dual norm \u2225\u2022\u2225 * , i.e. max p,p \u2032 \u2208A\u2212 \u2225p \u2212 p \u2032 \u2225 * \u2264 R and max q,q \u2032 \u2208A+ \u2225q \u2212 q \u2032 \u2225 * \u2264 R.", "formula_coordinates": [9.0, 96.91, 551.9, 458.03, 24.68]}, {"formula_id": "formula_18", "formula_text": "T \u2265 4L 2 \u03b5 2 32R 2 log(2/\u03b4) + 25\u03b3 T (Q A\u2212 ) + 25\u03b3 T (Q A+ ) .(5)", "formula_coordinates": [9.0, 188.55, 599.09, 352.62, 23.89]}, {"formula_id": "formula_19", "formula_text": "Reg ({p}, {\u03d5(\u2022, q)}) = \u03d5(p, q) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 1 T T t=1 \u03d5(p, q (t) ) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 \u03b5.", "formula_coordinates": [10.0, 122.97, 157.79, 366.07, 30.2]}, {"formula_id": "formula_20", "formula_text": "Reg ({q}, {\u2212\u03d5(p, \u2022)}) = max q * \u2208A+ \u03d5(p, q * ) \u2212 \u03d5(p, q) \u2264 max q * \u2208A+ \u03d5(p, q * ) \u2212 1 T T t=1 \u03d5(p (t) , q) \u2264 \u03b5.", "formula_coordinates": [10.0, 119.57, 226.49, 372.85, 30.2]}, {"formula_id": "formula_21", "formula_text": "q * \u2208A+ \u03d5(p, q * ) \u2212 min p * \u2208A\u2212 \u03d5(p * , q) \u2264 2\u03b5.", "formula_coordinates": [10.0, 262.25, 263.95, 175.18, 11.23]}, {"formula_id": "formula_22", "formula_text": "Reg(a (1:T ) , c (1:T ) ) \u2212 Reg(a (1:T ) , c (1:T ) ) \u2264 4L \u221a T R 2 log(1/\u03b4) + \u03b3 T (Q A ) .(6)", "formula_coordinates": [10.0, 136.76, 463.35, 404.41, 18.57]}, {"formula_id": "formula_23", "formula_text": "c (t) \u2212 c (t) . Since max a * \u2208A T t=1 c (t) , a * \u2212 max a * \u2208A T t=1 c (t) , a * \u2264 max a * \u2208A T t=1 c (t) \u2212 c (t)", "formula_coordinates": [10.0, 77.81, 505.1, 424.89, 14.11]}, {"formula_id": "formula_24", "formula_text": "\u2206 := Reg(a (1:T ) , c (1:T ) ) \u2212 Reg(a (1:T ) , c (1:T ) ) \u2264 Reg(a (1:T ) , c (1:T ) \u2212 c (1:T ) ).", "formula_coordinates": [10.0, 148.29, 537.43, 315.42, 10.81]}, {"formula_id": "formula_25", "formula_text": "Reg(a (1:T ) , c (1:T ) \u2212 c (1:T ) ) = max a * \u2208A T t=1 a (t) \u2212 a * , c (t) \u2212 c (t) .", "formula_coordinates": [10.0, 175.76, 577.24, 260.48, 30.2]}, {"formula_id": "formula_26", "formula_text": "\u03b5 (t) = Q A ({c (\u03c4 ) \u2212 c (\u03c4 ) } \u03c4 \u2208[t\u22121]", "formula_coordinates": [10.0, 140.78, 649.96, 127.62, 11.53]}, {"formula_id": "formula_27", "formula_text": "max a * \u2208A T t=1 a (t) \u2212 a * , c (t) \u2212 c (t) = max a * \u2208A T t=1 a (t) \u2212 \u03b5 (t) + \u03b5 (t) \u2212 a * , c (t) \u2212 c (t) = T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) + max a * \u2208A T t=1 \u03b5 (t) \u2212 a * , c (t) \u2212 c (t) .", "formula_coordinates": [10.0, 102.67, 695.13, 322.72, 30.2]}, {"formula_id": "formula_28", "formula_text": "max a * \u2208A T t=1 \u03b5 (t) \u2212 a * , c (t) \u2212 c (t) \u2208 4L \u03b3 T (Q A )T . (7", "formula_coordinates": [11.0, 201.74, 143.71, 335.18, 30.2]}, {"formula_id": "formula_29", "formula_text": ")", "formula_coordinates": [11.0, 536.92, 153.46, 4.24, 9.96]}, {"formula_id": "formula_30", "formula_text": "c (t) (a) \u2212 c (t) (a) \u2208 [\u22122L, 2L] must be normalized to [0, 1]. It remains to bound T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t)", "formula_coordinates": [11.0, 86.94, 181.82, 414.18, 25.98]}, {"formula_id": "formula_31", "formula_text": "a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) \u2264 a (t) \u2212 \u03b5 (t) * c (t) \u2212 c (t) \u2264 4RL.", "formula_coordinates": [11.0, 183.4, 279.66, 254.61, 17.2]}, {"formula_id": "formula_32", "formula_text": "Pr T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) \u2265 \u03b5 \u2264 exp \u2212 \u03b5 2 32T R 2 L 2 .", "formula_coordinates": [11.0, 175.77, 320.89, 260.45, 30.2]}, {"formula_id": "formula_33", "formula_text": "T t=1 a (t) \u2212 \u03b5 (t) , c (t) \u2212 c (t) \u2264 4RL 2T log(1/\u03b4).", "formula_coordinates": [11.0, 336.48, 358.53, 205.46, 14.11]}, {"formula_id": "formula_34", "formula_text": "Reg(a (1:T ) , c (1:T ) ) := T t=1 c (t) (a (t) ) \u2212 min a * \u2208A T t=1 c (t) (a * ) \u2264 VErr(a (1:T ) , c (1:T ) ) := max a * \u2208A T t=1 \u2202c (t) (a (t) ), a (t) \u2212 a * . Proof. By the convexity of \u03d5, T t=1 \u2202c (t) (a (t) ), a (t) \u2212 a * \u2265 T t=1 c (t) (a (t) )\u2212c (t) (a * )", "formula_coordinates": [11.0, 72.0, 512.73, 473.89, 51.75]}, {"formula_id": "formula_35", "formula_text": "Reg p (1:T ) , c - (1:T ) \u2212 Reg p (1:T ) , c - (1:T ) \u2264 4L \u221a T R 2 log(1/\u03b4) + \u03b3 T (Q A\u2212 ) , Reg q (1:T ) , c + (1:T ) \u2212 Reg q (1:T ) , c + (1:T ) \u2264 4L \u221a T R 2 log(1/\u03b4) + \u03b3 T (Q A+ ) .", "formula_coordinates": [11.0, 127.95, 702.78, 362.27, 18.57]}, {"formula_id": "formula_36", "formula_text": "VErr(p (1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) \u2264 L \u221a T 4R 2 log(1/\u03b4) + 5 \u03b3 T (Q A\u2212 ) VErr(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ) \u2264 L \u221a T 4R 2 log(1/\u03b4) + 5 \u03b3 T (Q A+ ) .", "formula_coordinates": [12.0, 148.3, 145.16, 315.4, 41.71]}, {"formula_id": "formula_37", "formula_text": "(1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) \u2264 T \u03b5 and VErr(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ) \u2264 T \u03b5 with probability at least 1 \u2212 2\u03b4. By Fact 3.5, Reg(p (1:T ) , {\u03d5(\u2022, q (t) )} t\u2208[T ] ) \u2264 T \u03b5 and Reg(q (1:T ) , {\u2212\u03d5(p (t) , \u2022)} t\u2208[T ] ) \u2264 T \u03b5.", "formula_coordinates": [12.0, 72.0, 201.62, 468.0, 35.65]}, {"formula_id": "formula_38", "formula_text": "\u03b8 (t) = Q \u0398 ({\u03b8 \u2192 \u2207 \u03b8 \u2113 j (h \u03b8 , z (\u03c4 ) ), \u03b8 } \u03c4 \u2208[t\u22121] ); For all (i, j) \u2208 I (t\u22121) , sample a datapoint z i (t\u22121) i.i.d. \u223c EX(D i ) for every unique i; Update the auditor's action w (t) , I (t) = Q \u2206n\u00d7m ({w \u2192 1 \u2212 n i=1 m j=1 w ij \u2113 j (h \u03b8 (\u03c4 ) , z i (\u03c4 ) )} \u03c4 \u2208[t\u22121]", "formula_coordinates": [12.0, 91.92, 453.84, 412.8, 39.29]}, {"formula_id": "formula_39", "formula_text": "\u03b5 \u2208 O T \u22121 (\u03b3 T (Q \u0398 ) + n log(mn/\u03b4) + R log(1/\u03b4)) .", "formula_coordinates": [12.0, 188.15, 606.14, 235.7, 10.49]}, {"formula_id": "formula_40", "formula_text": "T T t=1 w (t) ) is an \u03b5-equilibrium with probability 1 \u2212 \u03b4 if T \u2265 128 \u03b5 2 R 2 log(2/\u03b4) + \u03b3 T (Q \u0398 ) + \u03b3 T (Q \u2206nm ) .", "formula_coordinates": [13.0, 203.07, 124.92, 257.82, 46.09]}, {"formula_id": "formula_41", "formula_text": "\u2032 \u2206nm ) \u2208 O(n log(nm/\u03b4)) (Lemma 2.2), it suffices if T \u2265 C \u03b5 2 R 2 log(2/\u03b4) + \u03b3 T (Q \u0398 ) + n log(mn/\u03b4))", "formula_coordinates": [13.0, 72.0, 179.15, 468.0, 26.59]}, {"formula_id": "formula_42", "formula_text": "h Maj \u2208 Y X such that max D\u2208D R D (h Maj ) \u2264 2OPT + \u03b5.", "formula_coordinates": [14.0, 71.75, 355.14, 468.25, 22.27]}, {"formula_id": "formula_43", "formula_text": "V := {(H i , D i )} i if, given any problem (H i , D i ) \u2208 V, with probability at least 1 \u2212 \u03b4 the output of Q is an \u03b5-optimal solution.", "formula_coordinates": [14.0, 72.0, 577.43, 467.99, 21.92]}, {"formula_id": "formula_44", "formula_text": "\u2126 1 \u03b5 2 (d + n log(min{n, d}/\u03b4)) samples. When n \u2264 d, this lower bound becomes \u2126 1 \u03b5 2 (d + n log(n/\u03b4)", "formula_coordinates": [14.0, 72.0, 631.81, 441.57, 13.47]}, {"formula_id": "formula_45", "formula_text": "N Q (P) = E V \u223cP [N Q (V )].", "formula_coordinates": [14.0, 72.0, 713.2, 106.96, 9.65]}, {"formula_id": "formula_46", "formula_text": "Pr D \u2032 x (x, y) = 1 2 + 4y\u03b5. Let D \u2212 = x\u2208[w]", "formula_coordinates": [15.0, 72.0, 364.22, 468.26, 24.98]}, {"formula_id": "formula_47", "formula_text": "1 128\u03b5 2 (1 \u2212 2\u03b4) 2 < T i . Thus, if Q \u2032 is \u03b4 accurate under all hypotheses, under H 0 , Q \u2032 must take at least \u03b7 128\u03b5 2 (1 \u2212 2\u03b4) 2 < \u03b7 128\u03b5 2 log(1/2\u03b4", "formula_coordinates": [16.0, 72.0, 160.66, 469.38, 28.49]}, {"formula_id": "formula_48", "formula_text": "I j := [(j \u2212 1)\u03b7 + 1, j\u03b7]. Consider any problem V \u2032 = (H, D) \u2208 V \u03b7,1 .", "formula_coordinates": [16.0, 72.0, 220.7, 467.99, 22.27]}, {"formula_id": "formula_49", "formula_text": "\u221e t=1 Pr (E t ) \u2264 \u221e t=1 \u03b4 t\u22121 1 w w i=1 p i \u2264 \u221e t=1 \u03b4 t /w \u2264 8\u03b4", "formula_coordinates": [16.0, 250.53, 394.73, 230.56, 14.56]}, {"formula_id": "formula_50", "formula_text": "E V \u2032 \u2208P\u03b7,1 [N Q \u2032 (t)] = m/w. Thus, E V \u2032 T t=1 N Q \u2032 (t) = E V \u2032 [T ] E V \u2032 [N Q \u2032 (1)] = E V \u2032 [T ] m/w.", "formula_coordinates": [16.0, 141.94, 446.78, 399.99, 14.11]}, {"formula_id": "formula_51", "formula_text": "E V \u2032 [T ] = \u221e t=1 Pr(T \u2265 t) \u2264 \u221e t=0 \u03b4 t \u2264 1 1\u2212\u03b4 \u2264 8 7", "formula_coordinates": [16.0, 82.52, 462.26, 457.48, 26.52]}, {"formula_id": "formula_52", "formula_text": "is O \u03b5 \u22122 (D \u0398 + n log(n/\u03b4) + R log(1/\u03b4)) .", "formula_coordinates": [17.0, 230.14, 350.53, 185.09, 11.23]}, {"formula_id": "formula_53", "formula_text": "{y i } d i=1 \u2208 {\u00b11}", "formula_coordinates": [18.0, 308.18, 203.5, 66.44, 14.11]}, {"formula_id": "formula_54", "formula_text": "f 1 , f 2 \u2208 F where Pr x\u223cP (f 1 (x) \u0338 = f 2 (x)) \u2265 \u03b5, \u2225f 1 (x) \u2212 f 2 (x)\u2225 x > 0.", "formula_coordinates": [19.0, 72.0, 352.11, 288.75, 11.21]}, {"formula_id": "formula_55", "formula_text": "\u03b5 \u2208 O (1/n), we only needed to sample an additional O d \u03b5 2 log( d \u03b5 ) + n \u03b5 log( n \u03b5 ) \u2282 O nd \u03b5 log( d \u03b5 ) + n \u03b5 log( n \u03b5", "formula_coordinates": [19.0, 72.0, 473.87, 458.36, 13.47]}, {"formula_id": "formula_56", "formula_text": "D \u2032 X such that D \u221e (D \u2032 X ||D X ) \u2208 poly(1/\u03b5, d, n), with probability 1 \u2212 \u03b4.", "formula_coordinates": [19.0, 246.38, 523.99, 295.14, 12.47]}, {"formula_id": "formula_57", "formula_text": "O d log(dn/\u03b5)+n log(n/\u03b4) \u03b5 2 .", "formula_coordinates": [19.0, 221.66, 549.67, 109.03, 14.38]}, {"formula_id": "formula_58", "formula_text": "\u221e (D \u2032 X ||D X ) d log(d/\u03b5)+log(1/\u03b4) \u03b5 2 i.i.d. samples from distribution D \u2032 X", "formula_coordinates": [19.0, 72.0, 603.3, 468.0, 28.65]}, {"formula_id": "formula_59", "formula_text": "H \u2032 D is of size O (poly(1/\u03b5, d, n)) d .", "formula_coordinates": [19.0, 112.26, 631.44, 157.0, 12.47]}, {"formula_id": "formula_60", "formula_text": "H \u2032 of D of size |H \u2032 | \u2208 O n(poly(1/\u03b5, d, n)) d .", "formula_coordinates": [19.0, 244.88, 644.39, 200.78, 10.87]}], "doi": ""}