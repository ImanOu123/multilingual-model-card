{"title": "Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions", "authors": "Mihir Parmar; Swaroop Mishra; Mor Geva; Chitta Baral", "pub_date": "", "abstract": "In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize beyond biases originating in the crowdsourcing instructions. We further analyze the influence of instruction bias in terms of pattern frequency and model size, and derive concrete recommendations for creating future NLU benchmarks. 1   ", "sections": [{"heading": "Introduction", "text": "Benchmarks have been proven pivotal for driving progress in Natural Language Understanding (NLU) in recent years (Rogers et al., 2021;Bach et al., 2022;Wang et al., 2022). Nowadays, NLU benchmarks are mostly created through crowdsourcing, where crowdworkers write examples following annotation instructions crafted by dataset creators (Callison-Burch and Dredze, 2010;Zheng et al., 2018;Suhr et al., 2021). The instructions typically include a short description of the task, along with several examples Sakaguchi et al., 2020).\nDespite the vast success of this method, past studies have shown that data collected through crowdsourcing often exhibit various biases that lead to overestimation of model performance (Schwartz et al., 2017;Gururangan et al., 2018;Poliak et al., 2018;Tsuchiya, 2018;Mishra et al., 2020a;Mishra and Arunkumar, 2021;Hettiachchi et al., 2021). Such biases are often attributed to annotator-related biases, such as writing style and background knowledge (Gururangan et al., 2018;Geva et al., 2019) (see more discussion on related work in \u00a7A).\nIn this work, we propose that biases in crowdsourced NLU benchmarks often originate at an early stage in the data collection process of designing the annotation task. In particular, we hypothesize that task instructions provided by dataset creators, which serve as the guiding principles for annotators to complete the task, often influence crowdworkers to follow specific patterns, which are then propagated to the dataset and subsequently over-represented in the collected data. For instance, \u223c 36% of the instruction examples for the QUOREF dataset  start with \"What is the name\", and this same pattern can be observed in \u223c 59% of the collected instances.\nTo test our hypothesis, we conduct a broad study of this form of bias, termed instruction bias, in 14 recent NLU benchmarks. We find that instruction bias is evident in most of these datasets, showing that \u223c 73% of instruction examples on average share a few clear patterns. Moreover, we find that these patterns are propagated by annotators to the collected data, covering \u223c 61% of the instances on average. This suggests that instruction examples play a critical role in the data collection process and the resulting example distribution.\nIt is difficult to represent a task with a few examples, and bias in instruction examples makes it even more difficult since a task and its associated reasoning have a larger scope than instruction patterns. For example co-reference resolution, temporal commonsense reasoning, and numerical reasoning are much broader tasks than the prevalent patterns in QUOREF (\"what is the name...\"), MC-TACO (\"how long...\") and DROP (\"how many field goals...\") datasets.\nWe investigate the effect of instruction bias on model performance, showing that performance is overestimated by instruction bias and that models often fail to generalize beyond instruction patterns. Moreover, we observe that a higher frequency of instruction patterns in the training set often increases the model performance gap on pattern and nonpattern examples and that large models are generally less sensitive to instruction bias.\nIn conclusion, our work shows that instruction bias widely exists in NLU benchmarks, often leading to an overestimation of model performance. Based on our study, we derive concrete recommendations for monitoring and alleviating this bias in future data collection efforts. From a broader perspective, our findings also have implications on the recent learning-by-instructions paradigm (Efrat and Levy, 2020;, where crowdsourcing instructions are used in model training.", "publication_ref": ["b41", "b2", "b4", "b52", "b46", "b44", "b45", "b15", "b35", "b48", "b30", "b29", "b16", "b15", "b13", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Instruction Bias in NLU Benchmarks", "text": "Instructions are the primary resource for educating crowdworkers on how to perform their task . Bias in the instructions, dubbed instruction bias, could lead crowdworkers to propagate specific patterns to the collected data.\nHere, we study instruction bias in NLU benchmarks 2 , focusing on two research questions: (a) Do crowdsourcing instructions exhibit patterns that annotators can pick up on? and (b) Are such patterns propagated by crowdworkers to the collected data? In our study, we use the instructions of 14 recent NLU benchmarks: 3 (1) CLARIQ (Aliannejadi et al., 2020), (2) COSMOSQA (Huang et al., 2019), (3) DROP (Dua et al., 2019), (4) DUORC (Saha et al., 2018), and (5) HOTPOTQA  (6) HYBRIDQA , (7) MC-TACO , (8) MULTIRC (Khashabi et al., 2018), ( 9) PIQA (Bisk et al., 2020), ( 10) QASC (Khot et al., 2020)   , ( 13) SCIQA (Welbl et al., 2017), ( 14) WINOGRANDE (Sakaguchi et al., 2020). These benchmarks were created through different crowdsourcing protocols to evaluate diverse tasks   involves a wide range of different tasks. Also, we believe that the lower number of examples in crowdsourcing instructions might be limiting the imagination of annotators while creating samples, resulting in instruction bias.", "publication_ref": ["b0", "b18", "b10", "b42", "b20", "b3", "b21", "b50", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "Patterns in Crowdsourcing Instructions", "text": "Our goal is to quantify biases in instruction examples that propagate to collected data instances. In this study, we focus on an intuitive form of bias of recurring word patterns, which crowdworkers can easily pick up on. To find such patterns, we manually analyze the instruction examples of each dataset to find a dominant pattern, using the following procedure: (a) identifying repeating patterns of n \u2265 2 words, (b) merging patterns that are semantically similar or have a substantial word overlap, and (c) selecting the most frequent pattern as the dominant pattern (an example is provided in \u00a7C). Tab. 1 shows the dominant pattern in the instruction examples of each dataset. On average, 72.7% of the instruction examples used to create a dataset exhibit the same dominant pattern, and for 10 out of 14 datasets, the dominant pattern covers more than half of the instruction examples. This sug-gests that crowdsourcing instructions demonstrate a small set of repeating \"shallow\" patterns. Moreover, the short length of the patterns (2-4 words) and the typical low number of instruction examples (Tab. 2) make the patterns easily visible to crowdworkers, who can end up following them.\nNotably, our results are an underestimation of the actual instruction bias, since (a) we only consider the dominant pattern for each dataset (b) our manual analysis over instruction examples has a preference for short patterns (c) we do not consider paraphrased patterns (beyond the shallow paraphrases which are visible in annotation instructions), and (d) datasets may include implicit patterns (e.g. writing style and biases from the annotator's background knowledge) that also contribute to instruction bias. Accounting for such patterns is expected to increase the bias percentage in Tab. 1 further.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Instruction Bias Propagation to Datasets", "text": "We now turn to investigate whether patterns in instruction examples are further propagated by crowdworkers to the collected data. We analyze the train and test sets of each benchmark 4 to find the same patterns, using simple string matching. To account for syntactic modifications in identified patterns based on some examples from dataset, we also consider synonym words where appropriate and match the paraphrased version of each pattern.\nTab. 1 shows the results. Across all datasets, instruction patterns are ubiquitous in the collected data, occurring in 60.5% of the instances on average, with similar presence in training (59%) and test (62%) examples. While the dominant pattern's frequency in the data is typically not higher than in the instructions, for CLARIQ, DUORC, MUL-TIRC, QUOREF and ROPES, the pattern frequency was amplified by the crowdworkers. Interestingly, these datasets used a relatively large number of instruction examples (Tab. 2), suggesting that more examples do not necessarily alleviate the propagation of instruction bias. Example data instances with instruction patterns are provided in \u00a7D.\nA natural question that arises is whether patterns in collected data reflect the true task distribution rather than a bias in the instructions. We argue that this is highly unlikely. First, while the space of possible patterns for a NLU task is arguably large, the dataset patterns are imbalanced proportionately   task (a), and to compare model performance on instances with and without instruction patterns (b).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setting", "text": "Datasets Since model training is computationally expensive, we select a subset of seven datasets from those analyzed in \u00a72: (1) CLARIQ, (2) DROP, (3) MULTIRC, (4) PIQA, (5) QUOREF, (6) ROPES, and (7) SCIQA. These datasets cover a variety of tasks, different types and levels of instruction bias (Tab. 1), and are different in size ( \u00a7B).\nModels For all datasets except DROP, we evaluate T5-base and T5-large (Raffel et al., 2020), and BART-base and BART-large (Lewis et al., 2020). For DROP, we use Numnet+ (Ran et al., 2019), a RoBERTa model  with specialized output heads for numerical reasoning. Numnet+ has 355M parameters, which is closer to T5-base (220M) than to T5-large (770M) in size.\nEvaluation We evaluate model performance using the standard F 1 evaluation score, and report the average score over three random seeds.", "publication_ref": ["b36", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We observe similar results for T5 and BART, and thus, present only the results for T5 in this section. Results for BART are provided in \u00a7F.\nModels often fail to generalize beyond instruction patterns. Tab. 3 shows the performance on S p test and S \u2212p test when training only on examples with instruction patterns. Across all experiments, there are large performance gaps, reaching to 58% in DROP and > 10% in both base and large models for CLARIQ, MULTIRC, PIQA, and QUOREF. This indicates that models trained only on examples with instruction patterns fail to generalize to other task examples, and stresses that instruction bias should be monitored and avoided during data collection. Notably, the gap is lower for large models than for base ones, showing that large models are less sensitive to instruction bias. This might be attributed to their larger capacity to capture knowledge and skills during pre-training.\nModel performance is overestimated by instruction bias. We compare the performance on S p test and S \u2212p test of models trained on the full training set (Tab. 4). The average performance across all datasets is higher on examples that exhibit instruction patterns by \u223c 7% and \u223c 3% for the base and large models, respectively. Specifically, base models perform worse on S \u2212p test than on S p test for all datasets except DROP, in some cases by a dramatic gap of > 15% (e.g. 18.7% in ROPES and 15.7% in QUOREF). In contrast, results for the large models vary across datasets, while the performance gap is generally smaller in magnitude. This shows that model performance is often overestimated by instructions bias, and reiterates that large models are generally less sensitive to instruction patterns.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Discussion", "text": "We identify a prominent source of bias in crowdsourced NLU datasets, called instruction bias, which originates in annotation instructions written by dataset creators. We study this bias in 14 NLU benchmarks, showing that instruction examples used to create NLU benchmarks often exhibit clear patterns that are propagated by annotators to the collected data. In addition, we investigate the effect of instruction bias on model performance, showing that instruction patterns can lead to overestimated performance as well as limit the ability of models to generalize to other task examples.\nBased on our findings, we derive three recommendations for future crowdsourced NLU benchmarks: (1) Crowdsourcing instructions should be diverse; this could be achieved ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "This work covers 14 NLU datasets, for which annotation instructions are publicly available. However, most of these datasets are QA datasets. Our analysis can be extended to other NLU task categories, such as Natural Language Inference (NLI) and Relation Extraction (RE).\nOur study reveals a concrete bias that skews the collected data distribution toward specific patterns. While the effect of instruction examples on collected data is prominent, it is hard to quantify how different the distribution of crowdsourced examples is from the natural distribution of the task. Concretely, to conduct a study that compares the distributions of crowdsourced versus natural complex reasoning questions, datasets of complex natural questions are needed. However, to the best of our knowledge, as of today, no such datasets exist.\nIn our analysis, we focused on shallow patterns based on word matching, however, it is known that there are other types of biases that are implicit in the text. Exploring these kinds of biases can be an interesting future direction. In addition, our analysis of model performance is based on splitting dataset instances based on the dominant pattern. However, it might be possible that there are more patterns, and the non-pattern subset might include other less frequent patterns. Hence, exploring the effect of different less frequent patterns on model learning can be a future work.\nLast, our work studied the effect of instruction bias on widely used generative models (i.e., T5 and BART); it would be valuable to investigate whether our findings hold in encoder-only models, such as BERT (Devlin et al., 2019) and RoBERTa .", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "A Biases in NLU Benchmarks", "text": "Crowdsourcing has been a widely adapted approach to create large scale datasets such as SQUAD 1.1 (Rajpurkar et al., 2016(Rajpurkar et al., , 2018, DROP (Dua et al., 2019), QUOREF  and many more (Najafabadi et al., 2015;Callison-Burch and Dredze, 2010;Lasecki et al., 2014;Zheng et al., 2018;Chang et al., 2017). Many past works investigate different types of bias in crowdsourcing datasets such as cognitive bias (Eickhoff, 2018), annotator bias (Gururangan et al., 2018;Geva et al., 2019), sampling bias (Hu et al., 2020), demographic bias (Rahmani and Yang, 2021) and others (Hettiachchi et al., 2021). Many works on bias in NLU benchmarks focus on biases resulting from the crowdsourcing annotations, and how annotator-specific patterns create biases in data (Geva et al., 2019).\nTo mitigate the bias, prior works have focused on priming crowdsourcing annotators with minimal information to increase their imagination (Geva et al., 2021; to avoid recurring patterns. Arunkumar et al. (2020) develops a real time feedback and metric-in-the loop (Mishra et al., 2020b) workflow to educate crowdworkers in controlling dataset biases.  provides an iterative protocol with expert assessments for crowdsourcing data collection to increase difficulty of instances.  introduces dataset map as a model-based tool to characterize and diagnose datasets. Also, Karimi Mahabadi et al. (2020);Mahabadi et al. (2021) propose learning strategies to train neural models, which are more robust to such biases and transfer better to out-ofdomain datasets.\nIn this work, we show that biases exhibited by annotators start from the crowdsourcing instructions designed by dataset creators.", "publication_ref": ["b39", "b38", "b10", "b33", "b4", "b22", "b52", "b5", "b12", "b15", "b13", "b17", "b37", "b16", "b13", "b14", "b1", "b31", "b19", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "B Dataset Statistics", "text": "Tab. 5 describes the statistics of train and evaluation sets of datasets used in our experiments. Here, we can observe that each selected dataset differs in terms of number of training samples, % of instruction patterns, and tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Pattern Extraction Method", "text": "Here, we describe an example to show how we extract the dominant pattern from the crowdsourcing instructions and subsequently identify the same pattern in the dataset. We try to find recurring word patterns such as \"Are you...\", \"how many points...\", \"Was... still...\", \"since... the...\".\nFor example, MC-TACO (event duration) has 3 examples in crowdsourcing instructions: (1) how long did Jack play basketball?, (2) how long did he do his homework?, and (3) how long did it take for him to get the Visa? In step (a), we analyze examples manually and find dominant pattern. Here, we can see that all examples contain tri-gram pattern, i.e., \"how long did\". In step (b), we try to generate more possible patterns that are semantically similar to the dominant pattern or have a significant word overlap. Here, \"how long did\" can be \"how long was\", \"how long does\", etc. (i.e, How long AUX). In step (c), we look for all these possible patterns in datasets using simple word-matching techniques.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Pattern Examples", "text": "Tab. 8 provides dataset, instruction patterns and corresponding examples of data instances that exhibit the instruction patterns.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E The Effect of Instruction Examples on Pattern Frequency in Collected Data", "text": "To In addition, our collected responses where examples are not given contain 10 and 9 unique patterns for MC-TACO (event duration) and QUOREF respectively, in contrast to only 4 and 5 unique patterns in collected data where examples are given. Our finding shows that there is substantial linguistic diversity associated with the NLP tasks, unlike    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Additional Results", "text": "Tab. 6 and Tab. 7 show the performance of BART on S p test and S \u2212p test when training only on examples with instruction patterns and the full train-ing set, respectively. From Tab. 6, there are large performance gaps reaching 45.9% in MULTIRC and > 20% in both base and large models for QUOREF, and PIQA. Overall, the average performance across all datasets is 27.9% and 22.2% higher on S p test for the base and large models, respectively. This indicates that both base and large models often fail to generalize beyond instruction patterns.\nFrom Tab. 7, we see that the average performance across all datasets is higher on examples that exhibit instruction patterns by \u223c 10.5% for both base and large models. From the results, we can conclude that the model's performance is overestimated by instruction bias.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We thank Daniel Khashabi, Sewon Min, and Avia Efrat for helpful feedback, and the anonymous reviewers for constructive suggestions. We acknowledge the Research Computing (RC) at Arizona State University (ASU) for providing computing resources for experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset Pattern", "text": "Examples\nAre you looking for a specific web site?\nWhat kind of train are you looking for?\nDo you want to watch news videos or read the news?\nWould you like the location of the ritz carlton lake las vegas? The dog didn't like its collar but was okay with its leash because the _ was loose on it.\nHunter took Benjamin's clothes to the laundromat, since _ had the day off that day.\nJames sang his song at the top of his voice so as to be heard over the noise but the _ is louder.  were, has, have, had, do, does, did, will, would, can, could, may, might, shall, should, must}. _ : <blank>. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Convai3: Generating clarifying questions for opendomain dialogue systems (clariq)", "journal": "", "year": "2020", "authors": "Mohammad Aliannejadi; Julia Kiseleva; Aleksandr Chuklin; Jeff Dalton; Mikhail Burtsev"}, {"ref_id": "b1", "title": "Realtime visual feedback for educative benchmark creation: A human-and-metric-in-the-loop workflow", "journal": "", "year": "2020", "authors": "Anjana Arunkumar; Swaroop Mishra; Bhavdeep Sachdeva; Chitta Baral; Chris Bryan"}, {"ref_id": "b2", "title": "Promptsource: An integrated development environment and repository for natural language prompts", "journal": "", "year": "2022", "authors": "H Stephen; Victor Bach; Zheng-Xin Sanh; Albert Yong; Colin Webson;  Raffel; V Nihal; Abheesht Nayak; Taewoon Sharma;  Kim; Thibault Bari;  Fevry"}, {"ref_id": "b3", "title": "Piqa: Reasoning about physical commonsense in natural language", "journal": "", "year": "2020", "authors": "Yonatan Bisk; Rowan Zellers; Jianfeng Gao; Yejin Choi"}, {"ref_id": "b4", "title": "Creating speech and language data with Amazon's Mechanical Turk", "journal": "Association for Computational Linguistics", "year": "2010", "authors": "Chris Callison; - Burch; Mark Dredze"}, {"ref_id": "b5", "title": "Revolt: Collaborative crowdsourcing for labeling machine learning datasets", "journal": "", "year": "2017", "authors": "Joseph Chee Chang; Saleema Amershi; Ece Kamar"}, {"ref_id": "b6", "title": "Hy-bridQA: A dataset of multi-hop question answering over tabular and textual data", "journal": "", "year": "2020", "authors": "Wenhu Chen; Hanwen Zha; Zhiyu Chen; Wenhan Xiong; Hong Wang; William Yang Wang"}, {"ref_id": "b7", "title": "Tydi qa: A benchmark for information-seeking question answering in typologically diverse languages", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "H Jonathan; Eunsol Clark; Michael Choi; Dan Collins; Tom Garrette; Vitaly Kwiatkowski; Jennimaria Nikolaev;  Palomaki"}, {"ref_id": "b8", "title": "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Pradeep Dasigi; Nelson F Liu; Ana Marasovi\u0107; Noah A Smith; Matt Gardner"}, {"ref_id": "b9", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b10", "title": "DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Dheeru Dua; Yizhong Wang; Pradeep Dasigi; Gabriel Stanovsky; Sameer Singh; Matt Gardner"}, {"ref_id": "b11", "title": "The turking test: Can language models understand instructions?", "journal": "", "year": "2020", "authors": "Avia Efrat; Omer Levy"}, {"ref_id": "b12", "title": "Cognitive biases in crowdsourcing", "journal": "", "year": "2018", "authors": "Carsten Eickhoff"}, {"ref_id": "b13", "title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Mor Geva; Yoav Goldberg; Jonathan Berant"}, {"ref_id": "b14", "title": "Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies", "journal": "Transactions of the Association for Computational Linguistics", "year": "2021", "authors": "Mor Geva; Daniel Khashabi; Elad Segal; Tushar Khot; Dan Roth; Jonathan Berant"}, {"ref_id": "b15", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b16", "title": "Investigating and mitigating biases in crowdsourced data", "journal": "", "year": "2021", "authors": "Danula Hettiachchi; Mark Sanderson; Jorge Goncalves; Simo Hosio; Gabriella Kazai; Matthew Lease; Mike Schaekermann; Emine Yilmaz"}, {"ref_id": "b17", "title": "Crowdsourcing detection of sampling biases in image datasets", "journal": "", "year": "2020", "authors": "Xiao Hu; Haobo Wang; Anirudh Vegesana; Somesh Dube; Kaiwen Yu; Gore Kao; Shuo-Han Chen; Yung-Hsiang Lu; George K Thiruvathukal; Ming Yin"}, {"ref_id": "b18", "title": "Cosmos QA: Machine reading comprehension with contextual commonsense reasoning", "journal": "", "year": "2019", "authors": "Lifu Huang; Le Ronan; Chandra Bras; Yejin Bhagavatula;  Choi"}, {"ref_id": "b19", "title": "End-to-end bias mitigation by modelling biases in corpora", "journal": "", "year": "2020", "authors": "Yonatan Rabeeh Karimi Mahabadi; James Belinkov;  Henderson"}, {"ref_id": "b20", "title": "Looking beyond the surface: A challenge set for reading comprehension over multiple sentences", "journal": "", "year": "2018", "authors": "Daniel Khashabi; Snigdha Chaturvedi; Michael Roth; Shyam Upadhyay; Dan Roth"}, {"ref_id": "b21", "title": "QASC: A dataset for question answering via sentence composition", "journal": "", "year": "2020", "authors": "Tushar Khot; Peter Clark; Michal Guerquin; Peter Jansen; Ashish Sabharwal"}, {"ref_id": "b22", "title": "Glance: Rapidly coding behavioral video with the crowd", "journal": "", "year": "2014", "authors": "S Walter; Mitchell Lasecki; Danai Gordon;  Koutra; F Malte;  Jung; P Steven; Jeffrey P Dow;  Bigham"}, {"ref_id": "b23", "title": "Adversarial filters of dataset biases", "journal": "", "year": "2020", "authors": "Swabha Ronan Le Bras; Chandra Swayamdipta; Rowan Bhagavatula;  Zellers; E Matthew; Ashish Peters; Yejin Sabharwal;  Choi"}, {"ref_id": "b24", "title": "BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"ref_id": "b25", "title": "Reasoning over paragraph effects in situations", "journal": "", "year": "2019", "authors": "Kevin Lin; Oyvind Tafjord; Peter Clark; Matt Gardner"}, {"ref_id": "b26", "title": "", "journal": "Association for Computational Linguistics", "year": "", "authors": "Hong Kong; China "}, {"ref_id": "b27", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b28", "title": "Variational information bottleneck for effective low-resource fine-tuning", "journal": "", "year": "2021", "authors": "Yonatan Rabeeh Karimi Mahabadi; James Belinkov;  Henderson"}, {"ref_id": "b29", "title": "How robust are model rankings: A leaderboard customization approach for equitable evaluation", "journal": "", "year": "2021", "authors": "Swaroop Mishra; Anjana Arunkumar"}, {"ref_id": "b30", "title": "Our evaluation metric needs an update to encourage generalization. ArXiv, abs", "journal": "", "year": "2007", "authors": "Swaroop Mishra; Anjana Arunkumar; Chris Bryan; Chitta Baral"}, {"ref_id": "b31", "title": "Dqi: Measuring data quality in nlp", "journal": "", "year": "2020", "authors": "Swaroop Mishra; Anjana Arunkumar; Bhavdeep Sachdeva; Chris Bryan; Chitta Baral"}, {"ref_id": "b32", "title": "Cross-task generalization via natural language crowdsourcing instructions", "journal": "ACL", "year": "2021", "authors": "Swaroop Mishra; Daniel Khashabi; Chitta Baral; Hannaneh Hajishirzi"}, {"ref_id": "b33", "title": "Deep learning applications and challenges in big data analytics", "journal": "Journal of big data", "year": "2015", "authors": "M Maryam; Flavio Najafabadi;  Villanustre; M Taghi; Naeem Khoshgoftaar; Randall Seliya; Edin Wald;  Muharemagic"}, {"ref_id": "b34", "title": "What ingredients make for an effective crowdsourcing protocol for difficult NLU data collection tasks?", "journal": "Long Papers", "year": "2021", "authors": "Nikita Nangia; Saku Sugawara; Harsh Trivedi; Alex Warstadt; Clara Vania; Samuel R Bowman"}, {"ref_id": "b35", "title": "Hypothesis only baselines in natural language inference", "journal": "", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"ref_id": "b36", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b37", "title": "Demographic biases of crowd workers in key opinion leaders finding", "journal": "", "year": "2021", "authors": "A Hossein; Jie Rahmani;  Yang"}, {"ref_id": "b38", "title": "Know what you don't know: Unanswerable questions for SQuAD", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"ref_id": "b39", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b40", "title": "NumNet: Machine reading comprehension with numerical reasoning", "journal": "", "year": "2019", "authors": "Yankai Qiu Ran; Peng Lin; Jie Li; Zhiyuan Zhou;  Liu"}, {"ref_id": "b41", "title": "Qa dataset explosion: A taxonomy of nlp resources for question answering and reading comprehension", "journal": "", "year": "2021", "authors": "Anna Rogers; Matt Gardner; Isabelle Augenstein"}, {"ref_id": "b42", "title": "DuoRC: Towards complex language understanding with paraphrased reading comprehension", "journal": "", "year": "2018", "authors": "Amrita Saha; Rahul Aralikatte; M Mitesh; Karthik Khapra;  Sankaranarayanan"}, {"ref_id": "b43", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b44", "title": "Winogrande: An adversarial winograd schema challenge at scale", "journal": "", "year": "2020", "authors": "Keisuke Sakaguchi; Le Ronan; Chandra Bras; Yejin Bhagavatula;  Choi"}, {"ref_id": "b45", "title": "The effect of different writing tasks on linguistic style: A case study of the roc story cloze task", "journal": "", "year": "2017", "authors": "Roy Schwartz; Maarten Sap; Ioannis Konstas; Leila Zilles; Yejin Choi; Noah A Smith"}, {"ref_id": "b46", "title": "Crowdsourcing beyond annotation: Case studies in benchmark data collection", "journal": "", "year": "2021", "authors": "Alane Suhr; Clara Vania; Nikita Nangia; Maarten Sap; Mark Yatskar; Samuel R Bowman; Yoav Artzi"}, {"ref_id": "b47", "title": "Dataset cartography: Mapping and diagnosing datasets with training dynamics", "journal": "", "year": "2020", "authors": "Swabha Swayamdipta; Roy Schwartz; Nicholas Lourie; Yizhong Wang; Hannaneh Hajishirzi; A Noah; Yejin Smith;  Choi"}, {"ref_id": "b48", "title": "Performance impact caused by hidden bias of training data for recognizing textual entailment", "journal": "", "year": "2018", "authors": "Masatoshi Tsuchiya"}, {"ref_id": "b49", "title": "Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran", "journal": "", "year": "", "authors": "Yizhong Wang; Swaroop Mishra; Pegah Alipoormolabashi; Yeganeh Kordi; Amirreza Mirzaei"}, {"ref_id": "b50", "title": "Crowdsourcing multiple choice science questions", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Johannes Welbl; Nelson F Liu; Matt Gardner"}, {"ref_id": "b51", "title": "Hotpotqa: A dataset for diverse, explainable multi-hop question answering", "journal": "", "year": "2018", "authors": "Zhilin Yang; Peng Qi; Saizheng Zhang; Yoshua Bengio; William Cohen; Ruslan Salakhutdinov; Christopher D Manning"}, {"ref_id": "b52", "title": "Crowdsourcing methods for data collection in geophysics: State of the art, issues, and future directions", "journal": "Reviews of Geophysics", "year": "2018", "authors": "Feifei Zheng; Ruoling Tao; R Holger; Linda Maier; Dragan See; Tuqiao Savic; Qiuwen Zhang; Thaine H Chen; Pan Assump\u00e7\u00e3o; Bardia Yang;  Heidari"}, {"ref_id": "b53", "title": "going on a vacation\" takes longer than \"going for a walk\": A study of temporal commonsense understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Ben Zhou; Daniel Khashabi; Qiang Ning; Dan Roth"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": ", for example, by having a large number of instructive examples, rephrasing examples using neural models, or periodically sampling examples from a diverse set of previously collected examples. The latter could be done, for example, by maintaining a pool of diverse examples during the collection process and then presenting every annotator with a different random sample from this growing pool. (2) Word patterns in collected instances should be analyzed during data collection, as well as possible correspondence to instruction examples. Such analysis will help researchers monitor the collection process and the quality of the resulting data. (3) Correlation between model performance and input patterns should be checked during evaluation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Portion of patterns in instruction examples (Ins.) and in the corresponding train (S train ) and test (S test ) sets of NLU datasets. AUX \u2208 {am, is, are, was, were, has, have, had, do, does, did, will, would, can, could, may, might, shall, should, must}, and _ is an empty string. MC-TACO has 5 different data subsets corresponding to different types of temporal reasoning (see Tab. 2), hence, the sum of percentages for this dataset exceeds 100%.et al., 2019), (12) ROPES"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance on S p test vs. S \u2212p test of models trained on data instances containing instruction patterns (S p train ). We further validate the propagation of bias in instruction examples by comparing the pattern distributions of collected instances when the instructions include and do not include examples. We conduct this experiment for MC-TACO and QUOREF and find that, without any examples provided, the dominant pattern is substantially less frequent, showing that instruction bias is propagated during data collection. Full details are provided in \u00a7E.Propagation of instruction bias to the test set raises concerns regarding its reliability for evaluation, which we address next. 26.9 11.8% \u2193 30.7 26.9 12.4% \u2193 DROP 76 78.9 3.8% \u2191 MULTIRC 40.5 39 3.7% \u2193 42.4 44.5 5% \u2191 PIQA 20.7 19.8 4.4% \u2193 21.9 20.6 5.9% \u2193", "figure_data": "3 Effect on Model LearningLet S train (S test ) be the set of training (test) exam-ples, and denote by S p train (S p test ) and S \u2212p train (S \u2212p test ) its disjoint subsets of examples with and without in-struction patterns, respectively. We conduct two ex-periments where we fine-tune models on (a) S p train and (b) S p"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Performance on S p test vs. S \u2212p test of models trained on S train .", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Statistics of number of train and test examples with and without instruction patterns. S train : set of examples in train set, S p train : set of examples in train set with instruction pattern, S \u2212p train : set of examples in train set without instruction pattern, S test : set of examples in test set, S p test : set of examples in test set with instruction pattern, S \u2212p test : set of examples in test set without instruction pattern. ROPES 43.9 33.1 24.6% \u2193 47.2 39.1 17.2% \u2193 SCIQA 76.8 66.1 13.9% \u2193 77.5 69.6 10.2% \u2193 Average 46.6 33.6 27.9% \u2193 49.2 38.3 22.2% \u2193", "figure_data": "BaseLargeS p test 30 26.2 12.7% \u2193 29.2 25.6 12.3% \u2193 S \u2212p test S p test test S \u2212p MULTIRC 27.9 15.1 45.9% \u2193 31.1 CLARIQ 21 32.5% \u2193 PIQA 21.9 15.3 30.1% \u2193 23 16.1 30% \u2193 QUOREF 78.8 45.8 41.9% \u2193 87 58.6 32.6% \u2193"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Performance of BART models on S p test vs. S \u2212p test of models trained on data instances containing instruction patterns (S p train ). 8% \u2193 29.4 26.3 10.5% \u2193 MULTIRC 28.5 29.8 4.6% \u2191 41.5 33.9 18.3% \u2193 PIQA 22.3 20.5 8.1% \u2193 23.1 21.6 6.5% \u2193 QUOREF 80.5 61.2 24% \u2193 87.8 73.4 16.4% \u2193 ROPES 44 44.1 0.2% \u2191 47.9 46.5 2.9% \u2193 SCIQA 76.5 70.6 7.7% \u2193 52.2 50.8 2.7% \u2193", "figure_data": "BaseLargeS p test 29.8 26 12.Average S \u2212p test CLARIQ 46.9 42 10.5% \u2193S p test 47 42.1 10.4% \u2193 test S \u2212p"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Performance of BART models on S p test vs. S \u2212p test of models trained on S train . the patterns covered in instruction examples that get propagated to corresponding datasets.The task instructions and collected annotations are available at https://github.com/Mihir3009/ instruction-bias/blob/main/SURVEY.md.", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/2020.findings-emnlp.91"}