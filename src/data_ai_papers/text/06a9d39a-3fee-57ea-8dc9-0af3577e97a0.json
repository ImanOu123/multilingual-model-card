{"title": "Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory *", "authors": "Stefanos Leonardos; Georgios Piliouras", "pub_date": "2020-12-15", "abstract": "Exploration-exploitation is a powerful and practical tool in multi-agent learning (MAL), however, its effects are far from understood. To make progress in this direction, we study a smooth analogue of Q-learning. We start by showing that our learning model has strong theoretical justification as an optimal model for studying exploration-exploitation. Specifically, we prove that smooth Q-learning has bounded regret in arbitrary games for a cost model that explicitly captures the balance between game and exploration costs and that it always converges to the set of quantal-response equilibria (QRE), the standard solution concept for games under bounded rationality, in weighted potential games with heterogeneous learning agents. In our main task, we then turn to measure the effect of exploration in collective system performance. We characterize the geometry of the QRE surface in low-dimensional MAL systems and link our findings with catastrophe (bifurcation) theory. In particular, as the exploration hyperparameter evolves over-time, the system undergoes phase transitions where the number and stability of equilibria can change radically given an infinitesimal change to the exploration parameter. Based on this, we provide a formal theoretical treatment of how tuning the exploration parameter can provably lead to equilibrium selection with both positive as well as negative (and potentially unbounded) effects to system performance.", "sections": [{"heading": "Introduction", "text": "The problem of optimally balancing exploration and exploitation in multi-agent systems (MAS) has been a fundamental motivating driver of online learning, optimization theory and evolutionary game theory Claus and Boutilier [1998], Panait and Luke [2005]. From a behavioral perspective, it involves the design of realistic models to capture complex human behavior, such as the standard Experienced Weighted Attraction model Ho and Camerer [1999], Ho et al. [2007]. Learning agents use time varying parameters to explore suboptimal, boundedly rational decisions, while at the same time, they try to coordinate with other interacting agent and maximize their profits Ho and Camerer [1998], Bowling and Veloso [2002], Kaisers et al. [2009]. From an AI perspective, the exploration-exploitation dilemma is related to the optimization of adaptive systems. For example, neural networks are trained to parameterize policies ranging from very exploratory to purely exploitative, whereas meta-controllers decide which policy to prioritize Puigdom\u00e8nech Badia et al. [2020]. Similar techniques have been applied to rank agents in tournaments according to performance for preferential evolvability Lanctot et al. [2017], ,  and to design multi-agent learning (MAL) algorithms that prevent collective learning from getting trapped in local optima Tuyls [2010, 2011].\nDespite these notable advances both on the behavioral modelling and AI fronts, the theoretical foundations of learning in MAS are still largely incomplete even in simple settings Wunder et al. [2010], Bloembergen et al. [2015]. While there is still no theory to formally explain the performance of MAL algorithms, and in particular, the effects of exploration in MAS Klos et al. [2010], existing research suggests that many pathologies of exploration already emerge at stateless matrix games at which naturally emerging collective learning dynamics exhibit a diverse set of outcomes Sato and Crutchfield [2003], Sato et al. [2005], Tuyls and Weiss [2012].\nThe reasons for the lack of a formal theory are manifold. First, even without exploration, MAL in games can result in complex behavior that is hard to analyze , Mertikopoulos et al. [2018], Mazumdar et al. [2018]. Once explicit exploration is enforced, the behavior of online learning becomes even more intractable as Nash Equilibria (NE) are no longer fixed points of agents' behavior. Finally, if parameters are changed enough, then we get bifurcations and possibly chaos Wolpert et al. [2012], Palaiopanos et al. [2017], Sanders et al. [2018].\nOur approach & results. Motivated by the above, we study a smooth variant of stateless Q-learning, with softmax or Boltzmann exploration (one of the most fundamental models of exploration-exploitation in MAS), termed Boltzmann Q-learning or smooth Q-learning (SQL), which has recently received a lot of attention due to its connection with evolutionary game theory Tuyls et al. [2003], Kianercy and Galstyan [2012]. Informally (see Section 2 for the rigorous definition), each agent k updates her choice distribution x = (x i ) according to the rul\u0117\nx i /x i = \u03b2 k (u i \u2212\u016b) \u2212 \u03b1 k ln x i \u2212 j x j ln x j\nwhere u i ,\u016b denote agent k's utility from action i and average utility, respectively, given all other agents' actions and \u03b1 k /\u03b2 k is agent k's exploration rate. 1 Agents tune the exploration parameter to increase/decrease exploration during the learning process. We analyze the performance of SQL dynamics along the following axes.\nRegret and Equilibration. First, we benchmark their performance against the optimal choice distribution in a cost model that internalizes agents' utilities from exploring the space (Lemma 3.1), and show that in this context, the SQL dynamics enjoy a constant total regret bound in arbitrary games that depends logarithmically in the number of actions (Theorem 3.2). Second, we show that they converge to Quantal Response Equilibria (QRE) 2 in weighted potential games with heterogeneous agents of arbitrary size (Theorem 3.3). 3 The underpinning intuition is that agents' deviations from pure exploitation are not a result of their bounded rationality but rather a perfectly rational action in the quest for more information about unexplored choices which creates value on its own. This is explicitly captured by a correspondingly modified Lyapunov function (potential) which combines the original potential with the entropy of each agent's choice distribution (Lemma 3.4).\nWhile previously not formally known, these properties mirror results of strong regret guarantees for online algorithms (see e.g., Cesa-Bianchi and Lugosi [2006], Kwoon and Mertikopoulos [2017], Mertikopoulos et al. [2018]; convergence results for SQL in more restricted settings (Leslie and Collins [2005], Coucheney et al. [2015]). 4 However, whereas in previous works such results corresponded to main theorems, in our case they are only our starting point as they clearly not suffice to explain the disparity between the regularity of the SQL dynamics in theory and their unpredictable performance in practice.\nWe are faced with two major unresolved complications. First, the outcome of the SQL algorithm in MAS is highly sensitive on the exploration parameters Tuyls et al. [2006]. The set of QRE ranges from the NE of the underlying game when there is no exploration to uniform randomization when exploration is constantly high (agents never learn). Second, the collective system evolution is path dependent, i.e., in the case of time-evolving parameters, the equilibrium selection process cannot be understood by only examining the final exploration parameter but rather depends on its whole history of play G\u00f6cke [2002], Romero [2015], Yang et al. [2017].\nCatastrophe theory and equilibrium selection. We explain the fundamentally different outcomes of exploration-exploitation with SQL in games via catastrophe theory. The link between these two distinct fields lies on the properties of the underlying game which, in turn, shape the geometry of the QRE surface. As agents' exploration parameters change, the QRE surface also changes. This prompts dramatic phase transitions in the exploration path that ultimately dictate the outcome of the learning process. Catastrophe theory reveals that such transitions tend to occur as part of well-defined qualitative geometric structures.\nIn particular, the SQL dynamics may induce a specific type of catastrophes, known as saddlenode bifurcations Strogatz [2000]. At such bifurcation points, small changes in the exploration parameters change the stability of QRE and cause QRE to merge and/or disappear. However, as we prove, this is not always sufficient to stabilize desired states; the decisive feature is whether the QRE surface is connected or not (see Theorem 4.2 and Figure 2) which in turn, determines the possible types of bifurcations, i.e., whether there are one or two branches of saddle-node bifurcation curves, that may occur as exploration parameters change (Figure 1). Figure 1: Single saddle-node bifurcation curve (left) vs two branches of saddle-node bifurcation curves meeting which is consistent with the emergence of a co-dimension 2 cusp bifurcation (right) on the QRE manifold of two player, two action games as function of the exploration rates, \u03b4 x , \u03b4 y (see also Figures 3,4). The possible learning paths before, during and after exploration depend on the geometry of the QRE surface (Theorems 4.1,4.2).\nIn terms of performance, this is formalized in Theorem 4.1 which states that even in the simplest of MAS, exploration can lead under different circumstances both to unbounded gain as well as unbounded loss. While existential in nature, Theorem 4.1 does not merely say that anything goes when exploration is performed. When coupled with the characterization of the geometric locus of QRE in Theorem 4.2, it suggests that we can identify cases where exploration can be provably beneficial or damaging. This provides a formal geometric argument why exploration is both extremely powerful but also intrinsically unpredictable.\nThe above findings are visualized in systematic experiments in both low and large dimensional games along two representative exploration-exploitation policies, explore-then-exploit and cyclical learning rates (Section 5). We also visualize the modified potential (and how it changes during exploration) in weighted potential games of arbitrary size by properly adapting the technique of Li et al. [2018] for visualizing high dimensional loss functions in deep learning (Figure 5).\nOmitted materials, all proofs, and more experiments are included in Appendices A-D.\n2 Preliminaries: Game Theory and SQL\nWe consider a finite set N of interacting agents indexed by k = 1, 2, . . . , N . Each agent k \u2208 N can take an action from a finite set A k = {1, 2, . . . , n k }. Accordingly, let A := N k=1 A k denote the set of joint actions or pure action profiles, with generic element a = (a 1 , a 2 , . . . , a N ). To track the evolution of the agents' choices, let 5 We consider the dynamics in the collective state space X := N k=1 X k , i.e., the space of all joint choice distributions x = (x k ) k\u2208N . Using conventional notation, we will write (a k ; a \u2212k ) to denote the pure action profile at which agent k \u2208 N chooses action a k \u2208 A k and all other agents in N choose actions a \u2212k \u2208 A \u2212k := l =k A l . Similarly, for choice distribution profiles, we will write (x k , x \u2212k ) with x \u2212k \u2208 X \u2212k := l =k X l . When time is relevant, we will use the index t for agent k's choice distribution x k (t) := (x ki (t)) i\u2208A k at time t \u2265 0.\nX k = {x k \u2208 R n k : n k i=1 x ki = 1, x ki \u2265 0} denote the set of all possible choice distributions x k := (x ki ) i\u2208A k of agent k \u2208 N .\nWhen selecting an action i \u2208 A k , agent k \u2208 N receives a reward u k (i; a \u2212k ) which depends on the choices a \u2212k \u2208 A \u2212k of all other agents. Accordingly, the expected reward of agent k \u2208 N for a choice distribution profile\nx = (x k , x \u2212k ) \u2208 X is equal to u k (x) = a\u2208A x ki u k (i; a \u2212k ) l =k x la l .\nWe will also write r ki (x) := u k (i; x \u2212k ) or equivalently r ki (x \u2212k ) for the reward of pure action i \u2208 A k at the joint choice distribution profile x = (x k ; x \u2212k ) \u2208 X and r k (x) := (r ki (x)) i\u2208A k for the resulting reward vector of all pure actions of agent k. Using this notation, we have that u k (x) = x k , r k (x) , where \u2022, \u2022 denotes the usual inner product in R n k , i.e., x k , r k (x) = j\u2208A k x kj r kj (x). In particular, \u2202u k (x) /\u2202x ki = r ki (x). To sum up, the above setting can be represented in compact form with the notation \u0393 = N , (A k , u k ) k\u2208N .\nWe assume that the updates in the choice distribution x k of agent k \u2208 N are governed by the dynamics\u1e8b\nki /x ki = \u03b2 k [r ki (x) \u2212 j\u2208A k x kj r kj (x)] \u2212 \u03b1 k [ln x ki \u2212 j\u2208A k x kj ln x kj ] (1a) = \u03b2 k [r ki (x) \u2212 x k , r k (x) ] \u2212 \u03b1 k [ln x ki \u2212 x k , ln x k ](1b)\nwhere \u03b2 k \u2208 [0, +\u221e) and \u03b1 k \u2208 [0, 1) are positive constants that control the rate of choice adaptation and memory loss, respectively of the learning agent k \u2208 N and ln\nx k := (ln x ki ) i\u2208A k for x k \u2208 X k .\nThe first term, r ki (x) \u2212 n j\u2208A k x kj r kj (x), corresponds to the vector field of the replicator dynamics and captures the adaptation of the agents' choices towards the best performing strategy (exploitation). The second term, ln x ki \u2212 j\u2208A k x kj ln x kj , corresponds to the memory of the agent and the exploration of alternative choices. Due to their mathematical connection with Q-learning, we will refer to the dynamics in (1) as smooth Q-learning (SQL) dynamics. 6 The interior fixed points x Q \u2208 X of the dynamics in equations (1) are the Quantal Response Equilibria (QRE) of \u0393. In particular, each x Q k \u2208 X k for k = 1, 2, . . . , N satisfies\nx Q ki = exp (r ki (x Q \u2212k )/\u03b4 k )/ i\u2208A k exp (r kj (x Q \u2212k )/\u03b4 k ),(2)\nfor i \u2208 A k , where \u03b4 k := \u03b1 k /\u03b2 k denotes the exploration rate for each agent k \u2208 N .", "publication_ref": ["b7", "b35", "b13", "b14", "b12", "b5", "b16", "b24", "b53", "b4", "b42", "b43", "b50", "b30", "b27", "b52", "b33", "b48", "b19", "b6", "b23", "b30", "b25", "b8", "b49", "b10", "b38", "b54", "b46", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Bounded Regret in All Games and Convergence in Weighted Potential Games", "text": "Our first observation is that the SQL dynamics in (1) can be considered as replicator dynamics in a modified game with the same sets of agents and possible actions for each agent but with modified utilities.\nLemma 3.1. Given \u0393 = N , (A k , u k ) k\u2208N , consider the modified utilities u H k k\u2208N defined by u H k (x) := \u03b2 k x k , r k (x) \u2212 \u03b1 k x k , ln x k , for x \u2208 X.\nThen, the dynamics described by the differential equation\u1e8b ik /x ik in (1) can be written a\u1e61\nx ki /x ki = r H ki (x) \u2212 x k , r H k (x) (3\n)\nwhere r H ki (x) := \u2202 \u2202x ki u H k (x) = \u03b2 k r ki (x) \u2212 \u03b1 k (ln x ki + 1).\nIn particular, the dynamics in (1) describe the replicator dynamics in the modified setting\n\u0393 H = N , A k , u H k k\u2208N .\nThe superscript H refers to the regularizing term, H (x\nk ) := \u2212 x, ln x k = \u2212 j\u2208A k x kj ln x kj which denotes the Shannon entropy of choice distribution x k \u2208 X k .\nBounded regret. To measure the performance of the SQL dynamics in (1), we will use the standard notion of (accumulated) regret Mertikopoulos et al. [2018]. The regret R k (T ) at time\nT > 0 for agent k is R k (T ) := max x k \u2208X k T 0 u k x k ; x \u2212k (t) \u2212 u k (x k (t) , x \u2212k (t)) dt,(4)\ni.e., R k (T ) is the difference in agent k's rewards between the sequence of play x k (t) generated by the SQL dynamics and the best possible choice up to time T in hindsight. Agent k has bounded regret if for every initial condition x k (t) the generated sequence x k (t) satisfies lim sup R k (T ) \u2264 0 as T \u2192 \u221e. Our main result in this respect is a constant upper bound on the regret of the SQL dynamics.\nTheorem 3.2. Consider the modified setting \u0393 H = N , A k , u H k k\u2208N . Then, every agent k \u2208 N who updates their choice distribution x k \u2208 X k according to the dynamics in equation (3) has bounded regret, i.e., there exists a constant C > 0 such that lim sup T \u2192\u221e R H k (T ) \u2264 C.\nFrom the proof of Theorem 3.2, it follows that the constant C is logarithmic in the number of actions given a uniformly random initial condition as is the standard. This yields an optimal bound which is powerful in general MAL settings. In particular, regret minimization by the SQL dynamics at an optimal O(1/T ) rate implies that their time-average converges fast to coarse correlated equilibria (CCE). These are CCE of the perturbed game, \u0393 H , but if exploration parameter is low, they are approximate CCE of the original game as well. Even -CCE are ( \u03bb(1+ ) 1\u2212\u00b5(1+ ) )-optimal for \u03bb \u2212 \u00b5 smooth games, see e.g., Roughgarden [2015]. However, for games that are not smooth (e.g., games with NE that have widely different performance and hence, a large Price of Anarchy), we need more specialized tools (see Section 4).\nConvergence to QRE in weighted potential games with heterogeneous agents. If \u0393 = N , (A k , u k ) k\u2208N describes a potential game, then more can be said about the limiting behavior of the SQL dynamics. Formally, \u0393 is called a weighted potential game if there exists a function \u03c6 : A \u2192 R and a vector of positive weights w = (w k ) k\u2208N such that for each player\nk \u2208 N , u k (i, a \u2212k ) \u2212 u k (j, a \u2212k ) = w k (\u03c6 (i, a \u2212k ) \u2212 \u03c6 (j, a \u2212k )\n), for all i = j \u2208 A k , and a \u2212k \u2208 A \u2212k .\nIf w k = 1 for all k \u2208 N , then \u0393 is called an exact potential game. Let \u03a6 : X \u2192 R denote the multilinear extension of \u03c6 defined by \u03a6 (x) = a\u2208A \u03c6 (a) k\u2208N x ka k , for x \u2208 X. We will refer to \u03a6 as the potential function of \u0393. Using this notation, we have the following.\nTheorem 3.3. If \u0393 = N , (A k , u k ) k\u2208N admits a potential function, \u03a6 : X \u2192 R, then the sequence of play generated by the SQL dynamics in (1) converges to a compact connected set of QRE of \u0393.\nIntuitively, the first term, \u03b2 k (r ki (x) \u2212 x k , r k (x) ), in equation ( 1) corresponds to agent k's replicator dynamics in the underlying game (with utilities rescaled by \u03b2 k that can also absorb agent k's weight) and thus, it is governed by the potential function. The second term, \u2212\u03b1 k (ln x ki \u2212 x k , ln x k ), is an idiosyncratic term which is independent from the environment, i.e., the other agents' choice distributions. Hence, the potential game structure is preserved -up to a multiplicative constant for each player which represents that players' exploration rate \u03b4 k -and Theorem 3.3 can be established by extending the techniques of Kleinberg et al. [2009], Coucheney et al. [2015] to the case of weighted potential games. This is the statement of Lemma 3.4 (which is also useful for the numerical experiments).\nLemma 3.4. Let \u03a6 : X \u2192 R denote a potential function for \u0393 = N , (A k , u k ) k\u2208N , and consider the modified utilities\nu H k (x) := \u03b2 k x k , r k (x) \u2212 \u03b1 k x k , ln x k , for x \u2208 X. Then, the function \u03a6 H (x) defined by \u03a6 H (x) := \u03a6 (x) + k\u2208N \u03b4 k H (x k ) , for x \u2208 X,(5)\nis a potential function for the modified game\n\u0393 H = N , A k , u H k k\u2208N .\nThe time derivativ\u0117 \u03a6 H (x) of the potential function is positive along any sequence of choice distributions generated by the dynamics of equation (3) except for fixed points at which it is 0.", "publication_ref": ["b30", "b39", "b21", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "From Topology to Performance", "text": "While the above establish some desirable topological properties of the SQL dynamics, the effects of exploration are still unclear in practice both in terms of equilibrium selection and agents' individual performance (utility). As we formalize in Theorem 4.1 and visualize in Section 5, exploration -exploitation may lead to (unbounded) improvement, but also to (unbounded) performance loss even in simple settings.\nTo compare agents' utility for different exploration-exploitation policies, it will be convenient to denote the sequence of utilities of agent k \u2208 N by u exploit k (t) , t \u2265 0 if there exist thresholds \u03b4 k > 0 (that may depend on the initial condition x k (0) of agent k) such that \u03b4 k (t) < \u03b4 k for all k \u2208 N , i.e., if exploration remains low for all agents, and by u explore k (t) , t \u2265 0 otherwise. Then we have the following.\nTheorem 4.1 (Catastrophes in Exploration-Exploitation). For any number M > 0, there exist potential games \u0393 M u = {N , (X k , u k ) k\u2208N } and \u0393 M v = {N , (X k , v k ) k\u2208N }, positive-measure sets of initial conditions I u , I v \u2282 X, and exploration rates \u03b4 k > 0, so that lim t\u2192\u221e u exploit k (t) /u explore k (t) \u2265 M, and lim t\u2192\u221e v exploit k (t) /v explore k (t) \u2264 1/M\nfor all k \u2208 N , whenever lim sup t\u2192\u221e \u03b4 k (t) = 0 for all k \u2208 N , i.e., whenever, after some point, exploration stops for all agents. In particular, for all agents k \u2208 N , the individual -and hence, also the aggregate -performance loss (gain) in terms of utility due to exploration can be unbounded, even if exploration is only performed by a single agent.\nThe proof of Theorem 4.1 is constructive and relies on Theorem 4.2 discussed next. Theorem 4.2 characterizes the geometry of the QRE surface (connected or disconnected) which determines the bifurcation type that takes place during exploration. In turn, this dictates the possible outcomes -and hence, the individual and collective performance -after the exploration process, as formalized by Theorem 4.1.\nClassification of 2\u00d72 coordination games and geometry of the QRE surface. First, we introduce some minimal additional notation and terminology regarding coordination games. 7 Two player N = {1, 2}, two action A k = (a 1 , a 2 ) , k = 1, 2, coordination games are games in which the payoffs satisfy u 11 > u 21 , u 22 > u 12 and v 11 > v 21 , v 22 > v 12 where u ij (v ij ) denotes the payoff of agent 1 (2) when that agent selects action i and the other agent action j. Such games admit three NE, two pure on the diagonal and one fully mixed (x mix , y mix ), with x mix , y mix \u2208 (0, 1) (see Appendix C for details). The equilibrium (a 2 , a 2 ) is called risk-dominant if\n(u 22 \u2212 u 12 ) (v 22 \u2212 v 12 ) > (u 11 \u2212 u 21 ) (v 11 \u2212 v 21 ) .(6)\nIn particular, a NE is risk dominant if it has the largest basin of attraction (is less risky) Harsanyi and Selten [1988]. For symmetric games, inequality ( 6) has an intuitive interpretation: the choice at the risk dominant NE is the one that yields the highest expected payoff under complete ignorance, modelled by assigning (1/2, 1/2) probabilities to the other agent's choices. If u 22 \u2265 u 11 and v 22 \u2265 v 11 with at least one inequality strict, then (a 2 , a 2 ) is called payoff-dominant.\n0 1/2 x mix 1 0 1/2 y mix 1 x + y = x mix + y mix \u2192 Location of QRE Location of QRE x Q y Q 0 1/2 x mix 1 0 1/2 y mix 1 x + y = x mix + y mix \u2192 Location of QRE Location of QRE Location of QRE x Q\nFigure 2: Geometric locus of QRE in 2 \u00d7 2 coordination games for all possible exploration rates in the two cases (i) x mix , y mix \u2265 1/2 (upper panel) and (ii) x mix \u2265 1/2, y mix < 1/2 (bottom panel) of Theorem 4.2. The blue dots are the NE of the underlying game \u0393 (when exploration is zero). In both panels, the risk-dominant equilibrium is (0, 0).\nDepending on whether the interests of both agents are perfectly aligned -in the sense that (u 11 \u2212 u 22 ) (v 11 \u2212 v 22 ) > 0 -or not, the QRE surface can be disconnected or connected. A formal characterization is provided in Theorem 4.2. \n, N = {1, 2}, two-action, A 1 = A 2 = {a 1 , a 2 }, coordination game \u0393 = N , (A k , u k ) k\u2208N with payoff functions (u 1 , u 2 ) as in equations (11). If x mix + y mix > 1, then, for any exploration- exploitation rates \u03b1 x , \u03b2 x , \u03b1 y , \u03b2 y > 0 it holds that (i) If x mix , y mix > 1/2, then any QRE (x Q , y Q ) satisfies either x Q > x mix , y Q > y mix or x Q , y Q < 1/2. (ii) If x mix > 1/2, y mix \u2264 1/2, then any QRE (x Q , y Q ) satisfies one of: x Q < 1/2, y Q < y mix , 1/2 < x Q < x mix , y mix < y Q < 1/2 and x Q > x mix , y Q > y mix . Pareto Coordination a 1 a 2 a 1 1, 1 0, 0 a 2 0, 0 1.5, 1.8\nBattle of the Sexes a 1 a 2 a 1 1.5, 1 0, 0 a 2 0, 0 1, 2\nStag Hunt a 1 a 2 a 1 3, 3 0, 2 a 2 2, 0 1.5, 1.5 In particular, if \u0393 is symmetric, i.e., if u 2 = u T 1 , then there exist no symmetric QRE, (x Q , x Q ), with 1/2 < x Q < x mix .\nThe statement of Theorem 4.2 is visualized in Figure 2. In the first case, disconnected QRE surface, the dynamics select the risk-dominant equilibrium after a saddle-node bifurcation in the exploration phase, regardless of whether it coincides with the payoff dominant equilibrium or not. In the second case, the QRE surface is connected via two branches of saddle-node bifurcations which is consistent with the emergence of a cusp bifurcation point. Hence, after exploration which it may rest to either of the two boundary equilibria. In short, the collective outcome of the SQL dynamics depends on the geometry of the QRE surface which is illustrated next.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments: Phase Transitions in Games", "text": "To visualize the above, we start with 2 \u00d7 2 coordination games and then proceed to potential games with action spaces of arbitrary size. In all cases, we consider two representative explorationexploitation policies: an Explore-Then-Exploit (ETE) policy [Bai and Jin, 2020], which starts with (relatively) high exploration that reduces linearly to zero and a Cyclical Learning Rate with one cycle (CLR-1) policy [Smith and Topin, 2017], which starts with low exploration, increases to high exploration around the middle of the cycle and decays to (ultimately) zero exploration (i.e., pure exploitation). 8\nCoordination Games 2 \u00d7 2. As long as agents' interests are aligned, sufficient exploration even by a single agent leads the learning process (after exploration is reduced back to zero) to the risk dominant equilibrium regardless of whether this equilibrium coincides with the payoff dominant equilibrium or not. Typical realizations of these cases are the Pareto Coordination and Stag Hunt games (Table 1). 9 In Pareto Coordination, (a 2 , a 2 ) is both the risk-and payoff-dominant equilibrum whereas in Stag Hunt, the payoff dominant equilibrium is (a 1 , a 1 ). However, in both games x mix , y mix > 1/2 (due to the aligned interests of the players) which implies that the location of the QRE is described by the upper panel in Figure 2. Accordingly, the QRE surface is disconnected and if any agent sufficiently increases their exploration rate, the SQL dynamics converge to the risk-dominant equilibrium independently of the starting point and the exploration policy of the other agent. This is illustrated and explained in Figure 3 (and in a similar fashion in Figure 8 in Appendix D). Note that in both theses cases, the risk-dominant equilibrium is the global maximizer of the potential function, see Lemma C.1 and Al\u00f3s-Ferrer and Netzer [2010], Schmidt et al. [2003].\nBy contrast, if agents' interests are not perfectly aligned, then the outcome of the exploration process is not unambiguous (even if the game remains a coordination game). A representative game of this class, in which no payoff dominant equilibrium exists, is the Battle of the Sexes in Table 1. The most preferable outcome is now different for the two agents which implies that Figure 3: SQL in Stag Hunt. The upper panel shows the QRE surface and the exploration path of agent 1 (light to dark line). The bottom panels show the CLR-1 exploration rates (left) and the probability of action 1 during the learning process for both agents (right). As agents increase exploration, their choice distributions undergo a saddle-node bifurcation (disconnected surface). This prompts a permanent transition from the vicinity of the payoff dominant action profile, (x, y) = (1, 1), in the upper component of the QRE surface to the (0,0) equilibrium when exploration reduces back to zero (right corner of the lower component).\nthere is no payoff dominant equilibrium. However, the pure joint profile (a 2 , a 2 ) remains the risk-dominant equilibrium. 10 In this class of games, the location of the QRE is described by the bottom panel in Figure 2. The QRE surface is connected and the collective output of the exploration process depends on the exploration policies (timing and intensity) of the two agents. This is illustrated in Figure 4 which denotes two different outcomes of the learning process under the same exploration policy for agent 1 but different exploration policies for agent 2. In Appendix D, we provide an exhaustive treatment of the possible outcomes under the ETE and CLR-1 exploration policies.\nFigure 6 shows the QRE manifolds (surfaces) from a different perspective and highlights the bifurcation curves in the Stag Hunt and Battle of the Sexes games (Pareto Coordination is equivalent to Stag Hunt in this respect). Rotations of the images are included in the Multimedia Appendix.\nPotential games in larger dimensions To visualize the modified potential in equation ( 5) of Lemma 3.4, we adapt the two-dimensional projection technique of Li et al. [2018]. Given a potential game with potential \u03a6 and n, m actions for agents 1 and 2, we first embed their choice distributions into R n+m\u22122 to remove the Simplex restrictions via the transformation\ny i := log x i /x n from R n \u2192 R n\u22121 (with n i=1 x i = 1)\nfor the first agent (and similarly for the second agent) and then choose two arbitrary directions in R n+m\u22122 along which we plot the modified potential \u03a6 H (x) = \u03a6 (x) + k\u2208N \u03b4 k H (x k ) , for x \u2208 X, cf. equation (5). For simplicity, we keep the exploration ratio \u03b4 k := \u03b2 k /\u03b1 k equal to a common \u03b4 for both players. 11 A visualization of randomly generated 2-player potential game is given in Figure 5. As players modify their exploration rates, the SQL dynamics converge to different QRE (local maxima) of these changing surfaces. However, when exploration is large, a single attracting QRE remains (similar to the low dimensional case).\nIn Figure 7, we plot the SQL dynamics (1e \u2212 20 Q-value updates for each of 1e \u2212 03 choice Figure 4: Exploration-Exploitation in Battle of the Sexes. In contrast to Stag Hunt, the QRE manifold has two branches of saddle-node bifurcation curves (consistent with the emergence of a co-dimension 2 cusp point) and the phase transition to the lower part of the QRE surface may not be permanent. These two cases are illustrated via the CLR-1 vs CLR-1 policies (up) and the CLR-1 vs ETE policies (down).\nFigure 5: Snapshots of the modified potential \u03a6 H for different exploration rates in a symmetric 2-player potential game with random payoffs in [0, 1]. Unlike Figures 3 and 4, we now visualize the potential function instead of the QRE surface. Hence, we cannot reason about the bifurcation types. However, we see that without exploration, \u03b4 = 0, the potential (equal to the potential, \u03a6, of the original game) has various local maxima, whereas as exploration increases, a unique remaining attractor (maximum) forms at the vicinity of the uniform distribution, (0, 0) in the transformed coordinates. one close to each pure action pair, the SQL dynamics rest at different local optima before the exploration, converge to the uniform distribution when exploration rates reach their peak and then converge to the same (in this case, global) optimum when exploration is gradually reduced back to zero (horizontal line and vanishing shaded region).\nFigure 7: Exploration-Exploitation with the SQL dynamics in a potential game with n = 10 actions. The first two panels show the (log) choice distributions (with the optimal action in different color). The third panel shows the average potential over a set of 10 \u00d7 10 different trajectories (starting points) and one standard deviation (shaded region that disappears after all trajectories converge to the same choice distribution). The fourth panel shows the selected CLR-1 policies.", "publication_ref": ["b45", "b44", "b26"], "figure_ref": ["fig_2", "fig_1"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "A Derivation of SQL Dynamics", "text": "The mathematical connection between Q-learning and the dynamics in (1) via a smoothening process (continuous time limit) at which the Q-values are interpreted as Boltzmann probabilities for the action selection can be found in Tuyls et al. [2003] and Sato and Crutchfield [2003] among others. To make the paper self-contained, we repeat here the main arguments. Each agent k \u2208 N keeps track of the past performance of their actions i \u2208 A k via the Q-learning update rule\nQ ki (n + 1) = Q ki (n) + \u03b1 k [r ki (n) \u2212 Q ki (n)] , i \u2208 A k (7\n)\nwhere n \u2208 N denotes the time (in discrete steps) and \u03b1 k \u2208 [0, 1] the learning rate or memory decay of agent k, cf. Sato and Crutchfield [2003], Kianercy and Galstyan [2012]. Here Q ki (n) is called the memory of agent k about the performance of action i \u2208 A k up to time step n \u2208 N. Agent k \u2208 N updates their actions (choice distributions) according to a Boltzmann type distribution, with\nx ki (n) = exp (\u03b2 k Q ki (n)) j\u2208A k exp (\u03b2 k Q kj (n)) , for each i \u2208 A k ,(8)\nwhere \u03b2 k \u2208 [0, +\u221e) denotes agent k's learning sensitivity or adaptation, i.e., how much the choice distribution is affected by the past performance. Higher values of \u03b2 k indicate a higher exploitation rate, i.e., proclivity of the agent towards the best performing action, whereas values of \u03b2 k close to 0 lead to higher exploration or randomization among the agent's available choices in A k . Combining the above equations, one obtains the recursive equation of the agent's choice distribution\nx ki (n + 1) = x ki (n) exp (\u03b2 k (Q ki (n + 1) \u2212 Q ki (n))) j\u2208A k x kj (n) exp (\u03b2 k (Q kj (n + 1) \u2212 Q kj (n)))\n, for each i \u2208 A k . In practice, agents perform a large number of actions (updates of Q-values) for each choice-distribution (update of Boltzmann selection probabilities). This motivates to consider a continuous time version of the learning process for each agent k \u2208 N which results in the following update rules for both the memories Q ki\nQ ki = \u03b1 k [r ki (x) \u2212 Q ki ] ,\nwhere as in the main text, r ki (x) denotes k's reward for selecting pure action i \u2208 A k at state x = (x k , x \u2212k ) \u2208 X, and the selection probabilities x ki of each action i \u2208 A k\nx ki = \u03b2 k x ki \uf8eb \uf8edQ ki \u2212 j\u2208A kQ kj \uf8f6 \uf8f8 .\nCombining the last two equations under the assumptions that the relationship between pairs of actions is constant over time and that the choice distributions of the various agents are independently distributed, yields the dynamics in equation ( 1), namel\u1e8f\nx ki x ki = \u03b2 k \uf8eb \uf8ed r ki (x) \u2212 j\u2208A k x kj r kj (x) \uf8f6 \uf8f8 \u2212 \u03b1 k \uf8eb \uf8ed ln x ki \u2212 j\u2208A k x kj ln x kj \uf8f6 \uf8f8\nfor each i \u2208 A k and for each agent k \u2208 N .\nTime scale of updates. In the above dynamics, there are three relevant time scales or rates from the perspective of agent k \u2208 N : (1) the rate of change of the environment, i.e., of x \u2212k \u2208 X \u2212k ,\n(2) the rate of adaptation, i.e., the rate of change of x k \u2208 X k , which is captured by \u03b2 k and\n(3) the rate of memory dissipation or exploration of the action space which is captured by \u03b1 k . Typically, cf. Sato et al. [2005], the rate of change of the environment -captured by the choices x \u2212k \u2208 Y of all other agents -is slower than the changes in the choices x k of agent k -updates of equation ( 8) -which in turn, are slower than the rate of interaction -memory updates or equivalently updates of equation ( 7) -with the environment. In other words, the agent fixes a choice distribution x k \u2208 X k and interacts multiple times with the other agents (environment) before updating their choice distribution according to the above scheme.\nTo determine the interplay between updates of Q-values and choice distributions, let n and n + 1 denote the time points of two successive updates of the choice distribution x k (n) and x k (n + 1) of agent k \u2208 N . For any choice i \u2208 A k , let n i := x ki (n) M denote the (on average or expectation) number of times that agent k selects action i \u2208 A k given choice distribution x k (n), where M 0 is the large number of interactions of the agent with its environment under the fixed choice distribution x k (n). Then, using the index t \u2208 [0, n i ] such that n = 0 < 1 < \u2022 \u2022 \u2022 < t < \u2022 \u2022 \u2022 < n i = n + 1 to denote the timing of the interactions (the actual timing is irrelevant; only the number of interactions matters), equation ( 7) yields the following recursion.\nLemma A.1. Let x \u2212k (n) \u2208 X \u2212k and x k (n) \u2208 X k denote the choice distributions of agent k \u2208 N and all other agents in N other than k at time point n \u2265 0. Let n + 1 denote the time point of the next update of the choice distribution of agent k.\nThen, if n = 0 < 1 < \u2022 \u2022 \u2022 < t < \u2022 \u2022 \u2022 < n i = n + 1\ndenote the time points of M 0 interactions of agent k with its environment between time points n and n + 1, the updates in the Q-values of agent k are governed in expectation by the equation\nQ ki (n + 1) = (1 \u2212 \u03b1 k ) n i Q ki (n) + n i t=0 (1 \u2212 \u03b1 k ) t r i (n i \u2212 t) ,(9)\nwith n i := M \u2022 x ki (n). If x \u2212k (n)\nremains constant between two successive updates of the choice distribution of agent k at time points n and n + 1, then r i (n i \u2212 t) = r i for any t \u2208 [0, n i ] and equation (9) simplifies to\nQ ki (n + 1) = (1 \u2212 \u03b1 k ) n i Q ki (n) + r i \u03b1 k 1 \u2212 (1 \u2212 \u03b1 k ) n i +1 .(10)\nProof. Assuming that agent k interacts M times with their environment for each (fixed) choice distribution x k (n), where M is a large positive integer, the expected number of times that agent k selects choice i \u2208 A k is equal to n i = M \u2022 x ki (n). Hence, equation ( 7) yields\nQ ki (n + 1) = r i (n i ) + (1 \u2212 \u03b1 k ) Q ki (n i ) = r i (n i ) + (1 \u2212 \u03b1 k ) [r i (n i \u2212 1) + (1 \u2212 \u03b1) Q ki (n i \u2212 1)] = r i (n i ) + (1 \u2212 \u03b1 k ) r i (n i \u2212 1) + (1 \u2212 \u03b1) 2 [r i (n i \u2212 2) + Q ki (n i \u2212 2)] = . . . = (1 \u2212 \u03b1 k ) n i Q ki (n) + n i t=0 (1 \u2212 \u03b1 k ) t r i (n i \u2212 t)\nwith t indexing the time points at which agent k interacts with their environment between the two successive updates of its choice distribution x k (n) and x k (n + 1). Finally, assuming that the choice distribution of all other agents remains constant between time points n and n + 1, it holds that r i (n i \u2212 t) = r i for any t \u2208 [0, n i ] and equation simpilfies to\nQ ki (n + 1) = (1 \u2212 \u03b1 k ) n i Q ki (n) r i \u03b1 k 1 \u2212 (1 \u2212 \u03b1 k ) n i +1\nfor \u03b1 k \u2208 [0, 1), by the formula of the partial sums of the geometric series,\nn i t=0 (1 \u2212 \u03b1 k ) t = 1 \u03b1 k 1 \u2212 (1 \u2212 \u03b1 k ) n i +1 .\nRemark. Equations ( 10) and ( 9) can now be used to compute the memory updates of agent k between two successive updates of agent k's choice distribution via equation ( 8). Note that both equations hold in expectation (or on average) since the acting agent chooses their choice according to the choice distribution x k (n) each of the M times that they interact with their environment. However, under the working assumption that M 0, i.e., that M is a very large number, the law of large numbers suggests that the expected value n i = M \u2022 x ki (n) is close to the actual value that agent k uses choice i during the time that agent k draws from the (fixed) choice distribution x k (n). This approach has been used in all experiments resulting in a fast (scalable) implementation of the Q-learning process.\nFinally, equation ( 9) in Lemma A.1 suggests that for the extreme values of \u03b1 \u2208 [0, 1), the updates of the Q-values are\nQ i (n + 1) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 r i (n i ) , for \u03b1 \u2192 1, Q i (n) + n i t=0 r i (t) , for \u03b1 = 0.\nThis recovers the intuition that when \u03b1 = 0, the agent has perfect memory, whereas for \u03b1 \u2192 1, the agent is completely oblivious of past rewards.", "publication_ref": ["b48", "b42", "b42", "b19", "b43"], "figure_ref": [], "table_ref": []}, {"heading": "B Omitted Proofs of Section 3", "text": "Proof of Lemma 3.1. Using the defintion of r H ki (x), we have that\nr H ki (x) \u2212 x k , r H k (x) = \u03b2 k (r ki (x) \u2212 x k , r k (x) ) \u2212 \u03b1 k (ln x ki + 1 \u2212 x k , ln x k \u2212 x k , 1 ) .\nSince x k , 1 = j\u2208A k x kj = 1, the right side reduces to the expression in (1b).\nTo compare the performance of two different choice distributions p, x \u2208 X, we will use the notion of KL-divergence, D (p x), which is a measure of the distance from p to x defined by D (p x) := \u2212 n i=1 p i ln x i p i . To prove Theorem 3.2, we will need the following important property that follows immediately from the definition of the KL-divergence.\nLemma B.1. Let p = (p 1 , p 2 , . . . , p n ) \u2208 X be fixed and let x = (x 1 , x 2 , . . . , x n ) \u2208 X denote an arbitrary probability distributions (mixed strategy or population state) in X. Also let \u2022, \u2022 denote the inner product in R n . Then, p, ln p \u2265 p, ln x , for any x \u2208 X, with equality if and only if x = p.\nProof. By linearity of the inner product in R n , and non-negativity of the KL-divergence (which follows from Gibbs inequality and the fact that ln x \u2264 x \u2212 1 for all x > 0), we have that\np, ln p \u2212 ln x = n i=1 p i (ln p i \u2212 ln x i ) = \u2212 n i=1 p i ln x i p i = D (p x) \u2265 0\nwith equality if and only if x = p.\nProof of Theorem 3.2. Consider an agent k \u2208 N and let p k \u2208 X k denote the agent's optimal strategy in hindsight, i.e., p k = arg max\nx k \u2208X k T 0 u H k (x k ; x \u2212k (t)) dt.\nLet also x k (t) denote the sequence of play generated by the dynamics in (3) for an arbitrary initial condition x k (0) \u2208 X k . Then, by taking the time derivative of the term i\u2208A k p ki ln (x ki (t)), we obtain\nd dt p k , ln x k (t) = i\u2208A k p ki \u2022\u1e8b ki x ki = i\u2208A k p ki r H ki (x) \u2212 x k , r H k (x) = i\u2208A k p ik [\u03b2 k (r ki \u2212 x k , r k (x) ) \u2212 \u03b1 k (ln x ki \u2212 x k , ln x k )] = \u03b2 k p k , r k (x) \u2212 \u03b1 k p k , ln x k \u2212 x k , \u03b2 k r k (x) \u2212 \u03b1 k ln x k \u2265 \u03b2 k p k , r k (x) \u2212 \u03b1 k p k , ln p k \u2212 u H k (x k ; x \u2212k ) = u H k (p k ; x \u2212k (t)) \u2212 u H k (x k (t) ; x \u2212k (t))\nwhere the inequality has been established in Lemma B.1. Integrating both sides of the previous inequality from timepoint 0 to T > 0, and using the definition of R k (T ) in equation ( 4) we get\ni\u2208A k p ki (ln x ki (T ) \u2212 ln x ki (0)) \u2265 T 0 u H k (p k ; x \u2212k (t)) \u2212 u H k (x k (t) ; x \u2212k (t)) dt = R H k (T ) .\nSince x ki (T ) \u2208 [0, 1] for all i = 1, 2, . . . , n, and since i\u2208A k p ki = 1, the left side is upper bounded by \u2212 i\u2208A k p ki ln (x ki (0)) which is a constant with respect to T . Hence,\nlim sup T \u2192\u221e R H k (T ) \u2264 \u2212 i\u2208A k p ki ln x ki (0),\nwhich concludes the proof.\nProof of Lemma 3.4. Before proceeding to the proof of the statement of Lemma 3.4, observe that the multiplicative constants \u03b2 k , k \u2208 N in equation ( 1) are essentially equivalent to a rescaling of the payoffs of agent k in the underlying game. Thus, in a weighted potential game \u0393 with vector of positive weights w = (w k ) k\u2208N satisfying\nu k (i, a \u2212k ) \u2212 u k (j, a \u2212k ) = w k (\u03c6 (i, a \u2212k ) \u2212 \u03c6 (j, a \u2212k )) ,\nfor all i = j \u2208 A k , a \u2212k \u2208 A \u2212k , for all agents k \u2208 N , one may rescale the parameters \u03b2 k to \u03b2 k = \u03b2 k /w k for all k \u2208 N and consider the resulting exact potential game instead. This implies that we can adjust the techniques of Kleinberg et al. [2009], Coucheney et al. [2015] to prove the statement of Lemma 3.4.\nIn particular, to see that \u03a6 H (x) defines a potential for \u0393 H , consider the partial derivatives of \u03a6 H (x) at a point\nx = (x ki ) k\u2208N,i\u2208A k \u2208 X, \u2202 \u2202x ki \u03a6 H (x) = \u2202 \u2202x ki \u03a6 (x) \u2212 \u03b1 k \u03b2 k (ln x ki + 1) = \u2202 \u2202x ki u k (x) \u2212 \u03b1 k \u03b2 k (ln x ki + 1) = r ki (x) \u2212 \u03b1 k (ln x ki + 1) = 1 \u03b2 k r H ki (x)\nwhere \u2202 \u2202x ki u k (x) = r ki (x) by definition and\n\u2202 \u2202x ki \u03a6 (x) = \u2202 \u2202x ki u k (x) since \u03a6 (x)\nis a potential function for the unmodified game with utilities u k (x) , k \u2208 N . Hence, \u0393 H is a weighted potential game with potential function \u03a6 H (x) for x \u2208 X and vector of weights \u03b2 = (\u03b2 k ) k\u2208N . Given the above, taking the time derivative of the potential \u03a6 H (x) yield\u1e61\n\u03a6 H (x) = k\u2208N i\u2208A k \u2202 \u2202x ki \u03a6 H (x) \u1e8b ki = k\u2208N 1 \u03b2 k i\u2208A k r H ki (x)\u1e8b ki = k\u2208N 1 \u03b2 k i\u2208A k r H ki (x) x ki r H ki (x) \u2212 x k , r H k (x) = k\u2208N 1 \u03b2 k \uf8ee \uf8f0 i\u2208A k x ki r H ki (x) 2 \u2212 \uf8eb \uf8ed i\u2208A k x ki r H ki (x) \uf8f6 \uf8f8 2 \uf8f9 \uf8fb \u2265 0,\nwhere the last inequality follows directly from the Cauchy-Schwartz inequality (equivalently by observing that the term in braces is the variance of the quantities r H ki (x) , i \u2208 A k under the distribution x k \u2208 X k ). Accordingly, equality holds if the dynamics are at a fixed point which concludes the proof.\nProof of Theorem 3.3. Given Theorem 3.4, and the fact that the fixed points of (3) are precisely the QRE of \u0393, it remains to show that any sequence of play (x k (t)) k\u2208N , t \u2265 0 converges to a compact, connected set consisting entirey of equilibria, i.e., of points x \u2208 X for which \u03a6 H (x) is constant and equal to 0.\nTo see this, observe that for any sequence of play (x (t)) t\u22650 \u2286 X, the limit set \u2126 is defined as\n\u2126 = s\u2208R + cl{x (t) : t > s}\nwhere cl{S} denotes the closure of set S. Hence, \u2126 is compact and connected as the decreasing intersection of compact, connected sets. Moreover, since \u03a6 H (x (t)) is increasing by Lemma 3.4 and bounded on X by definition, it follows that \u03a6 H (x (t)) converges to a value \u03a6 * = sup \u03a6 H (x (t)) for any sequence of play x (t) t\u22650 \u2286 X. By continuity of \u03a6 H , this implies that \u03a6 * = \u03a6 H (x * ) for all x * \u2208 \u2126. Hence, if x * (t) t\u22650 \u2286 \u2126, it must be that \u03a6 H (x * ) = \u03a6 * for all t \u2265 0. Since \u03a6 H (x (t)) is strictly increasing in t unless x (t) is a sequence equilibria, it follows that \u2126 consists entirely of equilibria of the game \u0393 H which are QRE in \u0393. This concludes the proof.", "publication_ref": ["b21", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "C Omitted Materials: Section 4", "text": "The following provides a detailed notation for the coordination games discussed in Section 5. Some content is repetitive.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Coordination games", "text": "Consider a 2-player N = {1, 2}, 2-action\nA 1 = A 2 = {a 1 , a 2 } game \u0393 = N , (A k , u k ) k\u2208N with payoff matrices u 1 = a 1 a 2 a 1 u 11 u 12 a 2 u 21 u 22 , u 2 = a 1 a 2 a 1 v 11 v 21 a 2 v 12 v 22 ,(11a)\nwhere u ij (v ij ) denotes the payoff of agent 1 (2) when this agent uses action a i and the other agent uses action a j for i, j = 1, 2. \u0393 is a coordination game, if\nu 11 > u 21 , u 22 > u 12 and v 11 > v 21 , v 22 > v 12 (11b)\nhold for the payoffs of agents 1 and 2, respectively. In this case, the game admits three NE that can be described in terms of the probabilities x, y \u2208 [0, 1] of agents 1 and 2 using action a 1 : two pure NE (x, y) = (1, 1) and (x, y) = (0, 0) that correspond to the pure action profiles (a 1 , a 1 ) and (a 2 , a 2 ) and one fully mixed at (x mix , y\nmix ) = \u03bb 2 k 2 , \u03bb 1 k 1\nwhere\n\u03bb 1 := u 22 \u2212 u 12 , k 1 := u 11 \u2212 u 12 \u2212 u 21 + u 22 and \u03bb 2 := v 22 \u2212v 12 , k 2 := v 11 \u2212v 12 \u2212v 21 +v 22 , with \u03bb i , k i > 0 for i = 1, 2. The equilibrium (a 2 , a 2 ) is called risk-dominant if (u 22 \u2212 u 12 ) (v 22 \u2212 v 12 ) > (u 11 \u2212 u 21 ) (v 11 \u2212 v 21 ).\nIn symmetric games, i.e., when u 2 = u T 1 , this condition simplifies to u 22 + u 21 > u 11 + u 12 . If the inequality is reversed, then (a 1 , a 1 ) is risk dominant and if equality holds, then none of the pure equilibria risk-dominates the other. Finally, if u 22 \u2265 u 11 and v 22 \u2265 v 11 with at least one inequality strict then (a 2 , a 2 ) is called payoff or Pareto-dominant. For such games, we have the following properties.\nLemma C.1 (Properties of 2 \u00d7 2 coordination games). Let \u0393 = N , (A k , u k ) k\u2208N denote a two-player, N = {1, 2}, two-action, A 1 = A 2 = {a 1 , a 2 }, coordination game with payoff functions (u 1 , u 2 ) as in equations (11). Then, \u0393 is a weighted potential game with weights (1, w) for some w > 0 and it holds that (i) x mix + y mix > 1 if and only if (0, 0), i.e., the pure action profile (a 2 , a 2 ), is the risk-dominant equilibrium.\nIf, in addition, \u0393 is symmetric, i.e., u 2 = u T 1 , then x mix = y mix and property (i) simplifies to (i*) x mix > 1/2 if and only if (0, 0) is the risk-dominant equilibrium.\nIn this case, the (exact) potential is globally maximized at the pure action profile (a 2 , a 2 ), i.e., at the risk-dominant equilibrium.\nProof of Lemma C.1. Since k 1 , k 2 > 0, there exists w > 0 such that k 1 = wk 2 . Then, it is immediate to check that\nP = u 11 \u2212 u 21 u 11 \u2212 u 21 \u2212 w (v 11 \u2212 v 21 ) 0 k 1 \u2212 w (v 11 \u2212 v 21 )\nis a potential function for \u0393 with weights (w 1 , w 2 ) = (1, w). Hence, \u0393 is a (1, w) weighted potential game with potential function P . For (i), using the coordination game assumption, i.e., that u 11 > u 21 , u 22 > u 12 and v 11 > v 21 , v 22 > v 12 and dividing both sides of inequality ( 6) with the product k 1 k 2 , we have that ( 6) is equivalent to\n\u03bb 1 k 1 \u2022 \u03bb 2 k 2 > k 1 \u2212 \u03bb 1 k 1 \u2022 k 2 \u2212 \u03bb 2 k 2 \u21d0\u21d2 y mix x mix > (1 \u2212 y mix ) (1 \u2212 x mix ) \u21d0\u21d2 x mix + y mix > 1 as claimed.\nFinally, if \u0393 is symmetric, i.e., if u T 2 = u 1 , then k 1 = k 2 and \u03bb 1 = \u03bb 2 which implies that x mix = y mix . In this case, inequality (6) yields the condition x mix > 1/2 which proves (*i). To see that the global maximizer of the potential agrees with the risk dominant equilibrium, observe that P = u 11 \u2212 u 21 0 0 u 22 \u2212 u 12 is a potential function for the game. Hence, the global maximum of P is at (a 2 , a 2 ) whenever u 22 \u2212 u 12 > u 11 \u2212 u 21 , i.e., whenever (\u03bb 1 /k 1 ) > 1 \u2212 (\u03bb 1 /k 1 ) or equivalently whenever x mix > 1/2. Since any potential function must satisfy P + c for some constant c \u2208 R (see Monderer and Shapley [1996]), this concludes the proof.\nAs a special case, it is immediate from Lemma C.1 that a necessary and sufficient condition for \u0393 to be an exact potential game is that k 1 = k 2 .\nUsing Lemma C.1, we can reason about the possible locations of QRE in 2 \u00d7 2 coordination games. This is established in Theorem 4.2 where we focus on the case x mix + y mix \u2265 1 (wlog). To prove Theorem 4.2, we will use that for an arbitrary 2-player, 2-action game \u0393 = N , (A k , u k ) k\u2208N , the coupled dynamic equations in (1) becom\u0117\nx x (1 \u2212 x) = \u03b2 x [y (u 11 \u2212 u 21 ) + (1 \u2212 y) (u 12 \u2212 u 22 )] + \u03b1 x ln 1 x \u2212 1 (12a) y y (1 \u2212 y) = \u03b2 y [x (v 11 \u2212 v 21 ) + (1 \u2212 x) (v 12 \u2212 v 22 )] + \u03b1 y ln 1 y \u2212 1 (12b)\nwhere x, y \u2208 [0, 1] denote the probabilities that agent 1 and 2 respectively assign to pure action a 1 . Using Lemma C.1 and the expressions in equations ( 12), we can now prove Theorem 4.2.\nProof of Theorem 4.2. At any QRE (x Q , y Q ) \u2208 (0, 1) \u00d7 (0, 1), the right sides of the coupled equations in ( 12) are simultaneously equal to 0. Using the introduced notation\n\u03bb 1 := u 22 \u2212 u 12 , k 1 := u 11 \u2212 u 12 \u2212 u 21 + u 22 and \u03bb 2 := v 22 \u2212 v 12 , k 2 := v 11 \u2212 v 12 \u2212 v 21 + v 22 , with \u03bb i , k i > 0 for i = 1\n, 2, we can rewrite the QRE conditions as c 1 (y Q \u2212 y mix ) + ln 1\nx Q \u2212 1 = 0 (13a) c 2 (x Q \u2212 x mix ) + ln 1 y Q \u2212 1 = 0 (13b)\nwhere, c 1 := k 1 \u2022 \u03b2 x /\u03b1 x and c 2 := k 2 \u2022 \u03b2 y /\u03b1 y are positive constants (with respect to x, y) by assumption. As above (x mix , y mix ) denote the probabilities of pure action a 1 at the fully mixed equilibrium for agent 1 and 2, respectively. The cases in the statement of Theorem 4.2 now follow from an exhaustive sign analysis of the terms ln 1 x Q \u2212 1 and ln 1 y Q \u2212 1 which are positive for x Q , y Q > 1/2 and negative otherwise. Specifically, let x mix + y mix > 1 (the case x mix + y mix < 1 is analogous and the case x mix + y mix = 1 corresponds to coordination games at which none of the pure equilibria is risk dominant and is treated in Kianercy and Galstyan [2012]). Then, we have following cases Case 1: x mix , y mix > 1/2. If x Q > 1/2, then ln 1\nx Q \u2212 1 < 0 which implies that y Q > y mix . In turn, since y mix > 1/2, this implies in particular, that y Q > 1/2 and hence, that ln 1 y Q \u2212 1 < 0 which imposes the condition x Q > x mix for equation (13b) to be feasible. Hence, if x Q > 1/2, then a necessary condition for QRE is that x Q > x mix and y Q > y mix . This establishes the upper right region in the upper panel of Figure 2. If x Q \u2264 1/2, then ln 1\nx Q \u2212 1 \u2265 0 and hence, y Q < y mix . However, since x mix > 1/2 by assumption, x Q \u2264 1/2 implies that x Q \u2212 x mix < 0 which imposes the restriction y Q < 1/2 for equation (13b) to hold. This yields the feasible region x Q , y Q \u2264 1/2 depicted in the bottom left shaded region in the upper panel of Figure 2.\nCase 2: x mix > 1/2, y mix \u2264 1/2. Proceeding as in Case 1, assume first x Q > 1/2. Then, y Q must satisfy y Q > y mix for equation (13a) to hold. However, if y Q > 1/2, then ln 1 y Q \u2212 1 < 0 which imposes the restriction x Q < x mix for (13b) to be infeasible. Hence, for x Q > 1/2, y Q must satisfy either y mix < y Q < 1/2 when 1/2 < x Q < x mix or y Q > 1/2 when x Q > x mix . This yields the middle and upper right regions in the bottom panel of Figure 2. Finally, when x Q < 1/2, it holds that ln 1\nx Q \u2212 1 > 0 and hence, y Q < y mix for equation (13a) to hold. In turn, the condition y Q < y mix does not impose further restrictions for equation (13b) which results in the bottom left region in the bottom panel of Figure 2.\nCase 3: x mix \u2264 1/2, y mix > 1/2. This case is symmetric to Case 2.\nThe above cases are illustrated in Figure 2 in the main body of the paper. The case x mix , y mix > 1/2 corresponds to a situation at which the interests are better aligned than in the case x mix > 1/2, y mix < 1/2. In particular, symmetric games are special instances of the situation depicted in the upper panel of Figure 2. In this case, x mix = y mix and there are no QRE in the segment [1/2, x mix ].\nRemark (Intuition of Theorem 4.2). The main intuition of Theorem 4.2 is that the QRE surface may or may not be connected depending on the alignment of the interests of the two agents, i.e., on whether x mix > 1/2, y mix \u2265 1/2 (upper panel) or x mix \u2265 1/2, y mix < 1/2 (bottom panel) of Figure 2. In turn, the connectedness of the QRE surface crucially affects the convergence of the learning process. Intuitively, if one agent increases their exploration rate, then the choice distribution of that agent at a QRE will shift towards the uniform distribution, i.e., (1/2, 1/2) in a 2 \u00d7 2 game. In case that the payoff parameters of the underlying game are such that the game is described by the upper panel, then the two agents will find themselves in the bottom left (shaded) square which lies entirely in the attracting region of the risk-dominant equilibrium, (x, y) = (0, 0), at the bottom left corner. Hence, after reducing the exploration rate to 0, the Q-learning dynamics will rest at this equilibrium. Importantly, this outcome is independent of the starting point and the exploration-exploitation profile of the other agent. By contrast, this is not always the case if the payoff parameters of the underlying game are such that the game is described by the bottom panel of Figure 2. In this case, the QRE surface is connected and the effect on the learning process of the exploration policy -i.e., of temporarily increasing the exploration rate before reducing it again back to its initial level -depends on the exploration-exploitation profile of the other agents, and importantly also on their synchronicity. The critical observation is that the middle (shaded) rectangle in the bottom panel of Figure 2 is transcended by the x + y = x mix + y mix line which is the threshold between the attracting regions of the two corner equilibria, (x, y) = (0, 0) and (x, y) = (1, 1). Hence, when one or both agents increase their exploration rates to reach the middle rectangle, they cannot say in advance in which attracting region the learning process will find itself before they start reducing their exploration levels back to their initial zero levels. In this case, the process may well converge to the equilibrium where it started from (see, e.g., Figure 4 and 8).\nProof of Theorem 4.1. The proof is constructive and utilizes Theorem 4.2. For M > 0, let\n\u0393 M u = {N = {1, 2}, ({a 1 , a 2 }) k\u2208N , (u 1 , u 2 )} with u 1 , u 2 given by u 1 = a 1 a 2 a 1 2M 0 a 2 2M \u2212 1 2\n, u 2 = u T 1 . Then, (i) x mix = y mix = 2/3 for any M > 0 and (ii) (a 2 , a 2 ) is risk-dominant since 2M \u2212 1 + 2 > 2M + 0. Condition (i) implies that the basin of attraction of the (a 1 , a 1 ) equilibrium is equal to I u = [2/3, 1] 2 and hence that it has strictly positive measure (and in fact constant for any M > 0). By Theorem 4.2, (ii) implies that the QRE surface is disconnected -upper panel of Figure 2. Hence, given a starting point in the interior of I u , there exist exploration thresholds \u03b4 k that depend on (x k (0)), for k = 1, 2, so that if the exploration rates remain throughout low, i.e., \u03b4 k (t) < \u03b4 k for all t > 0 and for both k = 1, 2, then the dynamics will never escape the basin of attraction of (a 1 , a 1 ). Hence, since lim t\u2192\u221e \u03b4 k (t) = 0 by assumption -i.e., at the end of the learning process, agents stop to explore the space -it holds that lim t\u2192\u221e u exploit k (t) = 2M for both agents, i.e., their choice distributions will approximate (to arbitrary precision), the (a 1 , a 1 ) NE.\nBy contrast, if \u03b4 k (t) > \u03b4 k for some agent k \u2208 N , then the coupled dynamics in equation (1) will reach a fixed point close to the uniform distribution which by Theorem 4.2 lies in the basin of attraction of the risk-dominant (a 2 , a 2 ) equilibrium. When reducing the exploration rates back to zero, the agents' choice distribution will converge to the (a 2 , a 2 ) NE, which implies that lim t\u2192\u221e u explore k (t) = 2 for both agents. Since M > 0 was arbitrary, this concludes the case of unbounded loss.\nNote that a specific realization of the game \u0393 M u that is described above can be obtained by appropriately tuning the payoffs in the Stag Hunt game as described in Table 1.\nTo obtain the other direction, i.e., unbounded gain, consider (in a similar fashion) for M > 0 the game \u0393 M\nv = {N = {1, 2}, ({a 1 , a 2 }) k\u2208N , (v 1 , v 2 )} with u 1 , u 2 given by u 1 = a 1 a 2 a 1 2M 1.5 a 2 2M \u2212 1 2\n, u 2 = u T 1 . Then, (i) x mix = y mix = 1/3 for any M > 0 and (ii) (a 1 , a 1 ) is risk-dominant since 2M \u2212 1 + 2 < 2M + 1.5. Proceeding as in the previous case, condition (i) implies that the basin of attraction of the (a 2 , a 2 ) equilibrium is equal to I v = [0, 1/3] 2 and hence that it has strictly positive measure (and in fact constant for any M > 0). By Theorem 4.2, (ii) implies that the QRE surface is disconnected -upper panel of Figure 2. The difference is that now, the payoff dominant equilibrium (a 1 , a 1 ) is also the risk dominant equilibrium. Hence, starting by an arbitrary point in the interior of I v and by a similar argument as above, we obtain that lim t\u2192\u221e v explore k (t) = 2M and lim t\u2192\u221e v exploit k (t) = 2 for any agent k \u2208 N which concludes the proof.", "publication_ref": ["b31", "b19"], "figure_ref": ["fig_2"], "table_ref": ["tab_0"]}, {"heading": "D Supplementary Experiments", "text": "To understand the effect of the exploration policy in the collective outcome of the SQL dynamics, we study the three coordination games in Table 1. For each game, we consider all possible combinations of the exploration policies CLR-1 and ETE for the two agents. The experiments for the three coordination games (Pareto Coordination, Battle of the Sexes and Stag Hunt) are presented in Figure 8.\nBeyond potential games, e.g., in the Spoiled Child game of Wunder et al. [2010] (or in zero-sum games with unique interior equilibria), the behavior of the dynamics becomes critically dependent on the discretization scheme and is thus, unpredictable. Accordingly, we skip plots of such experiments.\nArbitrary Dimensions As a warm-up, we study the SQL dynamics in pure coordination games -coordination games with non-zero payoffs only on the diagonal -with action spaces of arbitrary size [Kim, 1996]. Depending on the starting point and (especially) on the intensity and timing of the exploration performed by each agent, the SQL dynamics converge after the exploration phase to (typically) improved, yet possibly only locally optimal equilibria. Specifically, we consider games \u0393 = N , (A k , u k ) k\u2208N with two players N = {1, 2} and n \n. . . u nn \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 , u 2 = u T 1 ,(14)\nwith 0 < u 11 < u 22 < \u2022 \u2022 \u2022 < u nn . Any symmetric profile (a i , a i ) , i = 1, 2, . . . , n constitutes a pure NE of \u0393. The equilibrium (a n , a n ) is payoff-dominant and (by properly generalizing the notion of risk-dominance) also risk dominant. In this case, Theorem 3.3 suggests that the Q-learning dynamics converge to a compact connected set of QRE of \u0393. However, it is unclear whether this will be the payoff dominant equilibrium or not and if not, how this outcome depends on the starting point and exploration policy of both agents. Two different instances are given in Figures 9 and 10.\nFigure 9: SQL dynamics (1e + 20 Q-value updates for each of 1e + 3 choice distribution updates) in a pure coordination game with n = 10 actions. The panels are as in Figure 7 and again show averages over a 10 \u00d7 10 grid of starting points, each of which is close to one pure action profile. Both agents perform CLR-1 exploration with different intensities and all trajectories of the SQL dynamics converge to the global optimum after the exploration phase regardless of the starting point (the standard deviation, depicted as the shaded region in the bottom left panel, vanishes).\nFigure 10: The SQL dynamics for the same experiment as in Figure 9 but with less intense exploration rate in the CRL-1 policies for both agents. In this case, the SQL dynamics converge to a suboptimal outcome after the exploration phase (the mean lies at 8.559 instead of the absolute maximum 10).\nFigure 11 shows a second experiment with \u03a6, in which the SQL dynamics converge to a suboptimal outcome. The conclusive finding from the experiments is that after the exploration Figure 11: The game and panels are as in Figure 7. Agent 2 reduces their exploration rate in comparison to Figure 7 (lower right panel -note also the quadratic increase-decrease in the exploration rate). The SQL dynamics converge to a suboptimal state with potential value 9.561 (flat line in the right part of the bottom left panel) for all trajectories (even for those initially close to the global optimum) as can be inferred by the vanishing shaded region (one standard deviation). phase, the SQL dynamics converge to the same (local) optimum regardless of the starting point (the shaded region which represents one standard deviation around the mean vanishes). Finally, concerning the plots of the SQL dynamics in Figure 7, the potential has been generated by the command rnd('1',twister) of Matlab -and the entry in (10, 10) deterministically set to 10 -and is given by 4 4 8 1 9 1 1 9 8 2  7 7 9 4 7 7 4 2 6 9  1 2 3 9 3 2 7 2 7 5  3 8 7 5 8 3 4 8 4 6  2 1 8 7 1 5 1 4 3 4  1 7 9 3 5 1 5 2 9 3  2 4 1 7 9 6 6 9 4 9  4 6 1 8 3 2 5 4 9 6  4 2 2 1 3 6 9 7 6 1  5 2 8 7 2 7 6 7 6 10 \n\u03a6 = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed\n\uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 .\nVisualizing the Potential To study exploration in games with action spaces of arbitrary dimension (the findings are qualitatively equivalent when generalizing to arbitrary numbers of players), we adapt the method of Li et al. [2018]. Specifically, for a two player game with choice distribution spaces X \u2208 R n and Y \u2208 R m consider the transformation g : R n \u2192 R n\u22121 with y 1i := ln (x 1i /x 1n ), for each i = 1, 2, . . . , n and the same for player 2. Together with the constraint n i=1 x 1i = 1, the inverse g \u22121 : R n\u22121 \u2192 R n of this transformation is given by x 1i = x 1n e y 1i , for i = 1, . . . , n. The benefit of working with the transformed variables is that they are not subject Algorithm 1 3D Visualization of the Potential u, v generate random vectors (not parallel)\n5: u = [u 1 , u 2 ], v = [v 1 , v 2 ] with 6: u 1 , v 1 \u2208 R n\u22121 , u 2 , v 2 \u2208 R m\u22121 7: procedure Transform Variables(u, v, \u03b1, \u03b2) Input: \u03b1, \u03b2 \u2190 scalars in R 8: x \u2190 \u03b1 \u2022 u 1 + \u03b2 \u2022 v 1 9:\nx \u2190 exp (x) 10:\nx \u2190 normalized to sum up to 1", "publication_ref": ["b53", "b20", "b26"], "figure_ref": ["fig_2"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "11:", "text": "Repeat to get y 12: procedure Evaluate Potential(x, y, \u03b4, \u03b1, \u03b2) Input: \u03b4 \u2190 Common exploration rate to the Simplex constraints. Hence, we may choose random choice distributions x 1 , x 2 , transform them to y 1 , y 2 and then scale them with two real scalars \u03b1, \u03b2 of arbitrary sign and magnitude to obtain a point on the hyperplane \u03b1 \u2022 y 1 + \u03b2 \u2022 y 2 . Then, we calculate the corresponding choice distributions (via the inverse transformation and the normalization equation) and evaluate the modified potential \u03a6 H at this point. This yields a tuple \u03b1, \u03b2, \u03a6 H g \u22121 (\u03b1 \u2022 y 1 + \u03b2 \u2022 y 2 ) , for each value of \u03b1, \u03b2 which (if combined) yield the surface plots of Figures 5 and 12. The process is summarized in Algorithm 1. Note that the technique readily generalizes to n > 2 players with the same exploration rate. A specific instant of a potential game with n = 20 which shows the transformation of the potential manifold as \u03b4 increases is included in the Multimedia Appednix.\nFigure 12: From top to bottom: snapshots of the modified potential \u03a6 H surface in 2 player potential games with n = 4, 10, 100 and 1000 actions, respectively, and random payoffs in [0, 1]. The surfaces are plotted using Algorithm 1 (see Li et al. [2018]). From left to right: the exploration rate \u03b4 increases from \u03b4 = 0 to \u03b4 = 0.05 and \u03b4 = 0.5. The surface has arbitary maxima (resting points of the SQL dynamics) when \u03b4 is small (exploitation) but a single maximum at (or close to) (0, 0) which corresponds to the uniform distribution when \u03b4 is large (exploration). Intuitively, this is what agents see as they increasingly incorporate exploration in their utilities.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The logit-response dynamics", "journal": "Games and Economic Behavior", "year": "2010", "authors": "C Al\u00f3s-Ferrer; N Netzer"}, {"ref_id": "b1", "title": "Provable Self-Play Algorithms for Competitive Reinforcement Learning", "journal": "Omnipress", "year": "", "authors": "Yu Bai; Chi Jin"}, {"ref_id": "b2", "title": "Smooth markets: A basic mechanism for organizing gradient-based learners", "journal": "", "year": "2020", "authors": "D Balduzzi; W M Czarnecki; T Anthony; I Gemp; E Hughes; J Leibo; G Piliouras; T Graepel"}, {"ref_id": "b3", "title": "A Game-Theoretic Approach to Recommendation Systems with Strategic Content Providers", "journal": "Curran Associates, Inc", "year": "2018", "authors": "O Ben-Porat; M Tennenholtz"}, {"ref_id": "b4", "title": "Evolutionary Dynamics of Multi-Agent Learning: A Survey", "journal": "J. Artif. Int. Res", "year": "2015-05", "authors": "D Bloembergen; K Tuyls; D Hennes; M Kaisers"}, {"ref_id": "b5", "title": "Multiagent learning using a variable learning rate", "journal": "Artificial Intelligence", "year": "2002", "authors": "Michael Bowling; Manuela Veloso"}, {"ref_id": "b6", "title": "Prediction, learning, and games", "journal": "Cambridge university press", "year": "2006", "authors": "Nicolo Cesa; - Bianchi; G\u00e1bor Lugosi"}, {"ref_id": "b7", "title": "The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems", "journal": "", "year": "1998", "authors": "Caroline Claus; Craig Boutilier"}, {"ref_id": "b8", "title": "Penalty-Regulated Dynamics and Robust Learning Procedures in Games", "journal": "Mathematics of Operations Research", "year": "2015", "authors": "P Coucheney; B Gaujal; P Mertikopoulos"}, {"ref_id": "b9", "title": "On the Properties of the Softmax Function with Application in", "journal": "", "year": "2017-04", "authors": "Bolin Gao; Lacra Pavel"}, {"ref_id": "b10", "title": "Various Concepts of Hysteresis Applied in Economics", "journal": "Journal of Economic Surveys", "year": "2002", "authors": "Matthias G\u00f6cke"}, {"ref_id": "b11", "title": "A General Theory of Equilibrium Selection in Games", "journal": "The MIT Press", "year": "1988", "authors": "J C Harsanyi; R Selten"}, {"ref_id": "b12", "title": "Experience-weighted attraction learning in coordination games: Probability rules, heterogeneity, and time-variation", "journal": "Journal of mathematical psychology", "year": "1998", "authors": "Teck-Hua Ho; Colin Camerer"}, {"ref_id": "b13", "title": "Experience-weighted attraction learning in normal form games", "journal": "Econometrica", "year": "1999", "authors": "Teck-Hua Ho; Colin Camerer"}, {"ref_id": "b14", "title": "Self-tuning experience weighted attraction learning in games", "journal": "Journal of economic theory", "year": "2007", "authors": "Teck-Hua Ho; Colin F Camerer; Juin-Kuan Chong"}, {"ref_id": "b15", "title": "Reinforcement learning: A survey", "journal": "Journal of Artificial Intelligence Research", "year": "1996", "authors": "L P Kaelbling; M L Littman; A W Moore"}, {"ref_id": "b16", "title": "An Evolutionary Model of Multi-Agent Learning with a Varying Exploration Rate", "journal": "", "year": "2009", "authors": "M Kaisers; K Tuyls; S Parsons; F Thuijsman"}, {"ref_id": "b17", "title": "Frequency adjusted multi-agent q-learning", "journal": "", "year": "2010", "authors": "Michael Kaisers; Karl Tuyls"}, {"ref_id": "b18", "title": "FAQ-Learning in Matrix Games: Demonstrating Convergence near Nash Equilibria, and Bifurcation of Attractors in the Battle of Sexes", "journal": "AAAI Press", "year": "2011", "authors": "Michael Kaisers; Karl Tuyls"}, {"ref_id": "b19", "title": "Dynamics of Boltzmann Q learning in two-player two-action games", "journal": "Phys. Rev. E", "year": "2012-04", "authors": "Ardeshir Kianercy; Aram Galstyan"}, {"ref_id": "b20", "title": "Equilibrium Selection in n-Person Coordination Games", "journal": "Games and Economic Behavior", "year": "1996", "authors": "Y Kim"}, {"ref_id": "b21", "title": "Multiplicative Updates Outperform Generic No-Regret Learning in Congestion Games", "journal": "", "year": "2009", "authors": "R Kleinberg; G Piliouras; E Tardos"}, {"ref_id": "b22", "title": "Evolutionary dynamics of regret minimization", "journal": "Springer", "year": "2010", "authors": "T Klos; G J Van Ahee; K Tuyls"}, {"ref_id": "b23", "title": "A continuous-time approach to online optimization", "journal": "Journal of Dynamics & Games", "year": "2017", "authors": "J Kwoon; P Mertikopoulos"}, {"ref_id": "b24", "title": "A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning", "journal": "Curran Associates, Inc", "year": "2017", "authors": "M Lanctot; V Zambaldi; A Gruslys; A Lazaridou; K Tuyls; J Perolat; D Silver; T Graepel"}, {"ref_id": "b25", "title": "Individual Q-Learning in Normal Form Games", "journal": "SIAM Journal on Control and Optimization", "year": "2005", "authors": "D S Leslie; E J Collins"}, {"ref_id": "b26", "title": "Visualizing the Loss Landscape of Neural Nets", "journal": "Curran Associates, Inc", "year": "2018", "authors": "Hao Li; Zheng Xu; Gavin Taylor; Christoph Studer; Tom Goldstein"}, {"ref_id": "b27", "title": "On Gradient-Based Learning in Continuous Games. arXiv e-prints, art", "journal": "", "year": "2018-04", "authors": "Eric Mazumdar; Lillian J Ratliff; S Shankar;  Sastry"}, {"ref_id": "b28", "title": "Quantal Response Equilibria for Normal Form Games", "journal": "Games and Economic Behavior", "year": "1995", "authors": "R D Mckelvey; T R Palfrey"}, {"ref_id": "b29", "title": "Learning in Games via Reinforcement and Regularization", "journal": "Mathematics of Operations Research", "year": "2016", "authors": "P Mertikopoulos; W H Sandholm"}, {"ref_id": "b30", "title": "Cycles in Adversarial Regularized Learning", "journal": "Society for Industrial and Applied Mathematics", "year": "2018", "authors": "P Mertikopoulos; C Papadimitriou; G Piliouras"}, {"ref_id": "b31", "title": "Potential Games", "journal": "Games and Economic Behavior", "year": "1996", "authors": "D Monderer; L S Shapley"}, {"ref_id": "b32", "title": "Julien Perolat, and Remi Munos. a-rank: Multi-agent evaluation by evolution", "journal": "", "year": "2019", "authors": "Shayegan Omidshafiei; Christos Papadimitriou; Georgios Piliouras; Karl Tuyls; Mark Rowland; Jean-Baptiste Lespiau; Wojciech M Czarnecki; Marc Lanctot"}, {"ref_id": "b33", "title": "Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos", "journal": "Curran Associates Inc", "year": "2017", "authors": "G Palaiopanos; I Panageas; G Piliouras"}, {"ref_id": "b34", "title": "Average Case Performance of Replicator Dynamics in Potential Games via Computing Regions of Attraction", "journal": "", "year": "2016", "authors": "I Panageas; G Piliouras"}, {"ref_id": "b35", "title": "Cooperative Multi-Agent Learning: The State of the Art", "journal": "Autonomous Agents and Multi-Agent Systems", "year": "2005-11", "authors": "Liviu Panait; Sean Luke"}, {"ref_id": "b36", "title": "From Poincar\u00e9 Recurrence to Convergence in Imperfect Information Games: Finding Equilibrium via Regularization", "journal": "", "year": "2020", "authors": "J Perolat; R Munos; J.-B Lespiau; S Omidshafiei; M Rowland; P Ortega; N Burch; T Anthony; D Balduzzi; B De Vylder; G Piliouras; M Lanctot; K Tuyls"}, {"ref_id": "b37", "title": "Agent57: Outperforming the Atari Human Benchmark. arXiv e-prints, art", "journal": "", "year": "2020-03", "authors": "Bilal Adri\u00e0 Puigdom\u00e8nech Badia; Steven Piot; Pablo Kapturowski; Alex Sprechmann; Daniel Vitvitskyi; Charles Guo;  Blundell"}, {"ref_id": "b38", "title": "The effect of hysteresis on equilibrium selection in coordination games", "journal": "Journal of Economic Behavior & Organization", "year": "2015", "authors": "J Romero"}, {"ref_id": "b39", "title": "Intrinsic Robustness of the Price of Anarchy", "journal": "J. ACM", "year": "2015", "authors": "Tim Roughgarden"}, {"ref_id": "b40", "title": "Multiagent Evaluation under Incomplete Information", "journal": "Curran Associates, Inc", "year": "2019", "authors": "M Rowland; S Omidshafiei; K Tuyls; J Perolat; M Valko; G Piliouras; R Munos"}, {"ref_id": "b41", "title": "The prevalence of chaotic dynamics in games with many players", "journal": "Scientific Reports", "year": "2018", "authors": "B T James; J D Sanders; T Farmer;  Galla"}, {"ref_id": "b42", "title": "Coupled replicator equations for the dynamics of learning in multiagent systems", "journal": "Phys. Rev. E", "year": "2003-01", "authors": "Y Sato; J P Crutchfield"}, {"ref_id": "b43", "title": "Stability and diversity in collective adaptation", "journal": "Physica D:Nonlinear Phenomena", "year": "2005", "authors": "Y Sato; E Akiyama; J P Crutchfield"}, {"ref_id": "b44", "title": "Playing safe in coordination games:: the roles of risk dominance, payoff dominance, and history of play", "journal": "Games and Economic Behavior", "year": "2003", "authors": "D Schmidt; R Shupp; J M Walker; E Ostrom"}, {"ref_id": "b45", "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates", "journal": "", "year": "2017", "authors": "Leslie N Smith; Nicholay Topin"}, {"ref_id": "b46", "title": "Nonlinear Dynamics and Chaos: With Applications to Physics", "journal": "Westview Press", "year": "2000", "authors": "S H Strogatz"}, {"ref_id": "b47", "title": "On Best-Response Dynamics in Potential Games", "journal": "SIAM Journal on Control and Optimization", "year": "2018", "authors": "B Swenson; R Murray; S Kar"}, {"ref_id": "b48", "title": "A Selection-mutation Model for Q-learning in Multi-agent Systems", "journal": "", "year": "2003", "authors": "K Tuyls; K Verbeeck; T Lenaerts"}, {"ref_id": "b49", "title": "An Evolutionary Dynamical Analysis of Multi-Agent Learning in Iterated Games", "journal": "Autonomous Agents and Multi-Agent Systems", "year": "2006", "authors": "K Tuyls; P J T Hoen; B Vanschoenwinkel"}, {"ref_id": "b50", "title": "Multiagent learning: Basics, challenges, and prospects. AI Magazine", "journal": "", "year": "2012-09", "authors": "Karl Tuyls; Gerhard Weiss"}, {"ref_id": "b51", "title": "Technical Note: Q-Learning. Machine Learning", "journal": "", "year": "1992-05", "authors": "C J C H Watkins; P Dayan"}, {"ref_id": "b52", "title": "Hysteresis effects of changing the parameters of noncooperative games", "journal": "Phys. Rev. E", "year": "2012-03", "authors": "D H Wolpert; M Harr\u00e9; E Olbrich; N Bertschinger; J Jost"}, {"ref_id": "b53", "title": "Classes of Multiagent Q-Learning Dynamics with -Greedy Exploration", "journal": "Omnipress", "year": "2010", "authors": "M Wunder; M Littman; M Babes"}, {"ref_id": "b54", "title": "Bifurcation Mechanism Design -from Optimal Flat Taxes to Improved Cancer Treatments", "journal": "", "year": "2017", "authors": "G Yang; G Piliouras; D Basanta"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Theorem 4.2 (Geometric locus of the QRE equilibria in coordination games). Consider a twoplayer", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 6 :6Figure 6: Stag Hunt (left panel) and Battle of the Sexes (right panel). The depicted surfaces are the same as in Figures 1,3,4 and 8. The current perspective is from the top of the z-axis. The x-y axes have been set between 2 and 5 for a better focus on the bifurcation curve. 360\u00b0r otations of these images are shown in the Multimedia Appendix.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 8 :8Figure8: SQL dynamics (1e + 03 Q-value updates for each of 2e + 04 choice distribution updates) with the ETE and CLR-1 policies in coordination games. In Pareto Coordination (Panel A) and Stag Hunt (Panel B), the SQL dynamics converge to the risk dominant equilibrium regardless of the starting point (saddle-node bifurcations). By contrast, in the Battle of the Sexes (Panel C), the collective outcome of the exploration process is a priori ambiguous and the SQL dynamics may converge to either of the pure action equilibria (consistent with cusp bifurcations). The decisive feature is the geometry of the QRE surface, i.e., whether it is connected or not, as formalized in Theorems 4.1 and 4.2.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "1:procedure Input Game(\u03a6, n, m) Input: \u03a6 \u2190 Potential matrix Input: n \u2190 # actions of player 1 Input: m \u2190 # actions of player 2 2: procedure Generate random directions(n, m) 3:for i \u2190 u, v do 4:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "\u03b1, \u03b2) = x \u03a6y \u2212 \u03b4 x i ln x i \u2212 \u03b4 y j ln y j 15: return Plot tuples \u03b1, \u03b2, \u03a6 H (\u03b1, \u03b2)", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Payoffs of the games in Section 5.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x i /x i = \u03b2 k (u i \u2212\u016b) \u2212 \u03b1 k ln x i \u2212 j x j ln x j", "formula_coordinates": [2.0, 189.72, 389.06, 208.37, 12.22]}, {"formula_id": "formula_1", "formula_text": "X k = {x k \u2208 R n k : n k i=1 x ki = 1, x ki \u2265 0} denote the set of all possible choice distributions x k := (x ki ) i\u2208A k of agent k \u2208 N .", "formula_coordinates": [4.0, 72.0, 225.13, 451.57, 29.95]}, {"formula_id": "formula_2", "formula_text": "x = (x k , x \u2212k ) \u2208 X is equal to u k (x) = a\u2208A x ki u k (i; a \u2212k ) l =k x la l .", "formula_coordinates": [4.0, 191.75, 368.93, 333.64, 12.22]}, {"formula_id": "formula_3", "formula_text": "ki /x ki = \u03b2 k [r ki (x) \u2212 j\u2208A k x kj r kj (x)] \u2212 \u03b1 k [ln x ki \u2212 j\u2208A k x kj ln x kj ] (1a) = \u03b2 k [r ki (x) \u2212 x k , r k (x) ] \u2212 \u03b1 k [ln x ki \u2212 x k , ln x k ](1b)", "formula_coordinates": [4.0, 147.86, 502.05, 375.42, 45.82]}, {"formula_id": "formula_4", "formula_text": "x k := (ln x ki ) i\u2208A k for x k \u2208 X k .", "formula_coordinates": [4.0, 72.0, 570.13, 450.15, 24.32]}, {"formula_id": "formula_5", "formula_text": "x Q ki = exp (r ki (x Q \u2212k )/\u03b4 k )/ i\u2208A k exp (r kj (x Q \u2212k )/\u03b4 k ),(2)", "formula_coordinates": [4.0, 185.22, 683.58, 338.06, 26.58]}, {"formula_id": "formula_6", "formula_text": "Lemma 3.1. Given \u0393 = N , (A k , u k ) k\u2208N , consider the modified utilities u H k k\u2208N defined by u H k (x) := \u03b2 k x k , r k (x) \u2212 \u03b1 k x k , ln x k , for x \u2208 X.", "formula_coordinates": [5.0, 71.15, 199.83, 451.81, 29.49]}, {"formula_id": "formula_7", "formula_text": "x ki /x ki = r H ki (x) \u2212 x k , r H k (x) (3", "formula_coordinates": [5.0, 224.56, 252.56, 294.07, 14.27]}, {"formula_id": "formula_8", "formula_text": ")", "formula_coordinates": [5.0, 518.63, 255.06, 4.65, 9.57]}, {"formula_id": "formula_9", "formula_text": "where r H ki (x) := \u2202 \u2202x ki u H k (x) = \u03b2 k r ki (x) \u2212 \u03b1 k (ln x ki + 1).", "formula_coordinates": [5.0, 71.29, 277.28, 278.27, 16.07]}, {"formula_id": "formula_10", "formula_text": "\u0393 H = N , A k , u H k k\u2208N .", "formula_coordinates": [5.0, 336.99, 296.32, 122.56, 15.26]}, {"formula_id": "formula_11", "formula_text": "k ) := \u2212 x, ln x k = \u2212 j\u2208A k x kj ln x kj which denotes the Shannon entropy of choice distribution x k \u2208 X k .", "formula_coordinates": [5.0, 71.61, 323.51, 450.71, 24.32]}, {"formula_id": "formula_12", "formula_text": "T > 0 for agent k is R k (T ) := max x k \u2208X k T 0 u k x k ; x \u2212k (t) \u2212 u k (x k (t) , x \u2212k (t)) dt,(4)", "formula_coordinates": [5.0, 72.0, 383.38, 451.28, 50.55]}, {"formula_id": "formula_13", "formula_text": "k \u2208 N , u k (i, a \u2212k ) \u2212 u k (j, a \u2212k ) = w k (\u03c6 (i, a \u2212k ) \u2212 \u03c6 (j, a \u2212k )", "formula_coordinates": [5.0, 72.0, 761.04, 279.04, 10.77]}, {"formula_id": "formula_14", "formula_text": "u H k (x) := \u03b2 k x k , r k (x) \u2212 \u03b1 k x k , ln x k , for x \u2208 X. Then, the function \u03a6 H (x) defined by \u03a6 H (x) := \u03a6 (x) + k\u2208N \u03b4 k H (x k ) , for x \u2208 X,(5)", "formula_coordinates": [6.0, 72.0, 311.09, 451.28, 61.53]}, {"formula_id": "formula_15", "formula_text": "\u0393 H = N , A k , u H k k\u2208N .", "formula_coordinates": [6.0, 296.58, 385.31, 125.61, 15.26]}, {"formula_id": "formula_16", "formula_text": "Theorem 4.1 (Catastrophes in Exploration-Exploitation). For any number M > 0, there exist potential games \u0393 M u = {N , (X k , u k ) k\u2208N } and \u0393 M v = {N , (X k , v k ) k\u2208N }, positive-measure sets of initial conditions I u , I v \u2282 X, and exploration rates \u03b4 k > 0, so that lim t\u2192\u221e u exploit k (t) /u explore k (t) \u2265 M, and lim t\u2192\u221e v exploit k (t) /v explore k (t) \u2264 1/M", "formula_coordinates": [6.0, 71.1, 620.67, 452.18, 91.35]}, {"formula_id": "formula_17", "formula_text": "(u 22 \u2212 u 12 ) (v 22 \u2212 v 12 ) > (u 11 \u2212 u 21 ) (v 11 \u2212 v 21 ) .(6)", "formula_coordinates": [7.0, 184.46, 255.7, 338.81, 10.63]}, {"formula_id": "formula_18", "formula_text": "0 1/2 x mix 1 0 1/2 y mix 1 x + y = x mix + y mix \u2192 Location of QRE Location of QRE x Q y Q 0 1/2 x mix 1 0 1/2 y mix 1 x + y = x mix + y mix \u2192 Location of QRE Location of QRE Location of QRE x Q", "formula_coordinates": [7.0, 100.31, 356.64, 392.46, 153.44]}, {"formula_id": "formula_19", "formula_text": ", N = {1, 2}, two-action, A 1 = A 2 = {a 1 , a 2 }, coordination game \u0393 = N , (A k , u k ) k\u2208N with payoff functions (u 1 , u 2 ) as in equations (11). If x mix + y mix > 1, then, for any exploration- exploitation rates \u03b1 x , \u03b2 x , \u03b1 y , \u03b2 y > 0 it holds that (i) If x mix , y mix > 1/2, then any QRE (x Q , y Q ) satisfies either x Q > x mix , y Q > y mix or x Q , y Q < 1/2. (ii) If x mix > 1/2, y mix \u2264 1/2, then any QRE (x Q , y Q ) satisfies one of: x Q < 1/2, y Q < y mix , 1/2 < x Q < x mix , y mix < y Q < 1/2 and x Q > x mix , y Q > y mix . Pareto Coordination a 1 a 2 a 1 1, 1 0, 0 a 2 0, 0 1.5, 1.8", "formula_coordinates": [7.0, 71.1, 653.47, 453.68, 99.4]}, {"formula_id": "formula_20", "formula_text": "y i := log x i /x n from R n \u2192 R n\u22121 (with n i=1 x i = 1)", "formula_coordinates": [9.0, 72.0, 577.59, 265.91, 15.24]}, {"formula_id": "formula_21", "formula_text": "Q ki (n + 1) = Q ki (n) + \u03b1 k [r ki (n) \u2212 Q ki (n)] , i \u2208 A k (7", "formula_coordinates": [15.0, 172.5, 433.95, 346.13, 10.77]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [15.0, 518.63, 433.95, 4.65, 9.57]}, {"formula_id": "formula_23", "formula_text": "x ki (n) = exp (\u03b2 k Q ki (n)) j\u2208A k exp (\u03b2 k Q kj (n)) , for each i \u2208 A k ,(8)", "formula_coordinates": [15.0, 174.91, 533.23, 348.37, 28.11]}, {"formula_id": "formula_24", "formula_text": "x ki (n + 1) = x ki (n) exp (\u03b2 k (Q ki (n + 1) \u2212 Q ki (n))) j\u2208A k x kj (n) exp (\u03b2 k (Q kj (n + 1) \u2212 Q kj (n)))", "formula_coordinates": [15.0, 152.89, 659.2, 283.44, 28.11]}, {"formula_id": "formula_25", "formula_text": "Q ki = \u03b1 k [r ki (x) \u2212 Q ki ] ,", "formula_coordinates": [15.0, 239.93, 762.44, 115.42, 10.77]}, {"formula_id": "formula_26", "formula_text": "x ki = \u03b2 k x ki \uf8eb \uf8edQ ki \u2212 j\u2208A kQ kj \uf8f6 \uf8f8 .", "formula_coordinates": [16.0, 222.68, 111.19, 149.92, 38.19]}, {"formula_id": "formula_27", "formula_text": "x ki x ki = \u03b2 k \uf8eb \uf8ed r ki (x) \u2212 j\u2208A k x kj r kj (x) \uf8f6 \uf8f8 \u2212 \u03b1 k \uf8eb \uf8ed ln x ki \u2212 j\u2208A k x kj ln x kj \uf8f6 \uf8f8", "formula_coordinates": [16.0, 136.62, 211.78, 323.23, 38.19]}, {"formula_id": "formula_28", "formula_text": "Then, if n = 0 < 1 < \u2022 \u2022 \u2022 < t < \u2022 \u2022 \u2022 < n i = n + 1", "formula_coordinates": [16.0, 301.12, 585.44, 223.25, 10.63]}, {"formula_id": "formula_29", "formula_text": "Q ki (n + 1) = (1 \u2212 \u03b1 k ) n i Q ki (n) + n i t=0 (1 \u2212 \u03b1 k ) t r i (n i \u2212 t) ,(9)", "formula_coordinates": [16.0, 158.44, 646.7, 364.84, 33.83]}, {"formula_id": "formula_30", "formula_text": "with n i := M \u2022 x ki (n). If x \u2212k (n)", "formula_coordinates": [16.0, 71.29, 692.75, 155.17, 10.77]}, {"formula_id": "formula_31", "formula_text": "Q ki (n + 1) = (1 \u2212 \u03b1 k ) n i Q ki (n) + r i \u03b1 k 1 \u2212 (1 \u2212 \u03b1 k ) n i +1 .(10)", "formula_coordinates": [16.0, 158.94, 739.7, 364.33, 25.64]}, {"formula_id": "formula_32", "formula_text": "Q ki (n + 1) = r i (n i ) + (1 \u2212 \u03b1 k ) Q ki (n i ) = r i (n i ) + (1 \u2212 \u03b1 k ) [r i (n i \u2212 1) + (1 \u2212 \u03b1) Q ki (n i \u2212 1)] = r i (n i ) + (1 \u2212 \u03b1 k ) r i (n i \u2212 1) + (1 \u2212 \u03b1) 2 [r i (n i \u2212 2) + Q ki (n i \u2212 2)] = . . . = (1 \u2212 \u03b1 k ) n i Q ki (n) + n i t=0 (1 \u2212 \u03b1 k ) t r i (n i \u2212 t)", "formula_coordinates": [17.0, 110.77, 127.12, 373.73, 95.44]}, {"formula_id": "formula_33", "formula_text": "Q ki (n + 1) = (1 \u2212 \u03b1 k ) n i Q ki (n) r i \u03b1 k 1 \u2212 (1 \u2212 \u03b1 k ) n i +1", "formula_coordinates": [17.0, 168.64, 294.68, 250.99, 25.64]}, {"formula_id": "formula_34", "formula_text": "n i t=0 (1 \u2212 \u03b1 k ) t = 1 \u03b1 k 1 \u2212 (1 \u2212 \u03b1 k ) n i +1 .", "formula_coordinates": [17.0, 205.07, 352.67, 186.07, 33.83]}, {"formula_id": "formula_35", "formula_text": "Q i (n + 1) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 r i (n i ) , for \u03b1 \u2192 1, Q i (n) + n i t=0 r i (t) , for \u03b1 = 0.", "formula_coordinates": [17.0, 187.48, 557.39, 219.13, 48.79]}, {"formula_id": "formula_36", "formula_text": "r H ki (x) \u2212 x k , r H k (x) = \u03b2 k (r ki (x) \u2212 x k , r k (x) ) \u2212 \u03b1 k (ln x ki + 1 \u2212 x k , ln x k \u2212 x k , 1 ) .", "formula_coordinates": [17.0, 89.56, 712.09, 416.16, 14.27]}, {"formula_id": "formula_37", "formula_text": "p, ln p \u2212 ln x = n i=1 p i (ln p i \u2212 ln x i ) = \u2212 n i=1 p i ln x i p i = D (p x) \u2265 0", "formula_coordinates": [18.0, 134.84, 268.87, 329.84, 33.71]}, {"formula_id": "formula_38", "formula_text": "x k \u2208X k T 0 u H k (x k ; x \u2212k (t)) dt.", "formula_coordinates": [18.0, 264.28, 344.64, 132.07, 17.7]}, {"formula_id": "formula_39", "formula_text": "d dt p k , ln x k (t) = i\u2208A k p ki \u2022\u1e8b ki x ki = i\u2208A k p ki r H ki (x) \u2212 x k , r H k (x) = i\u2208A k p ik [\u03b2 k (r ki \u2212 x k , r k (x) ) \u2212 \u03b1 k (ln x ki \u2212 x k , ln x k )] = \u03b2 k p k , r k (x) \u2212 \u03b1 k p k , ln x k \u2212 x k , \u03b2 k r k (x) \u2212 \u03b1 k ln x k \u2265 \u03b2 k p k , r k (x) \u2212 \u03b1 k p k , ln p k \u2212 u H k (x k ; x \u2212k ) = u H k (p k ; x \u2212k (t)) \u2212 u H k (x k (t) ; x \u2212k (t))", "formula_coordinates": [18.0, 123.87, 398.9, 348.72, 115.58]}, {"formula_id": "formula_40", "formula_text": "i\u2208A k p ki (ln x ki (T ) \u2212 ln x ki (0)) \u2265 T 0 u H k (p k ; x \u2212k (t)) \u2212 u H k (x k (t) ; x \u2212k (t)) dt = R H k (T ) .", "formula_coordinates": [18.0, 90.24, 559.47, 416.62, 32.97]}, {"formula_id": "formula_41", "formula_text": "lim sup T \u2192\u221e R H k (T ) \u2264 \u2212 i\u2208A k p ki ln x ki (0),", "formula_coordinates": [18.0, 210.04, 640.7, 175.2, 25.78]}, {"formula_id": "formula_42", "formula_text": "u k (i, a \u2212k ) \u2212 u k (j, a \u2212k ) = w k (\u03c6 (i, a \u2212k ) \u2212 \u03c6 (j, a \u2212k )) ,", "formula_coordinates": [18.0, 171.61, 762.44, 252.06, 10.77]}, {"formula_id": "formula_43", "formula_text": "x = (x ki ) k\u2208N,i\u2208A k \u2208 X, \u2202 \u2202x ki \u03a6 H (x) = \u2202 \u2202x ki \u03a6 (x) \u2212 \u03b1 k \u03b2 k (ln x ki + 1) = \u2202 \u2202x ki u k (x) \u2212 \u03b1 k \u03b2 k (ln x ki + 1) = r ki (x) \u2212 \u03b1 k (ln x ki + 1) = 1 \u03b2 k r H ki (x)", "formula_coordinates": [19.0, 171.46, 143.25, 244.81, 106.49]}, {"formula_id": "formula_44", "formula_text": "\u2202 \u2202x ki \u03a6 (x) = \u2202 \u2202x ki u k (x) since \u03a6 (x)", "formula_coordinates": [19.0, 291.14, 260.59, 163.16, 16.07]}, {"formula_id": "formula_45", "formula_text": "\u03a6 H (x) = k\u2208N i\u2208A k \u2202 \u2202x ki \u03a6 H (x) \u1e8b ki = k\u2208N 1 \u03b2 k i\u2208A k r H ki (x)\u1e8b ki = k\u2208N 1 \u03b2 k i\u2208A k r H ki (x) x ki r H ki (x) \u2212 x k , r H k (x) = k\u2208N 1 \u03b2 k \uf8ee \uf8f0 i\u2208A k x ki r H ki (x) 2 \u2212 \uf8eb \uf8ed i\u2208A k x ki r H ki (x) \uf8f6 \uf8f8 2 \uf8f9 \uf8fb \u2265 0,", "formula_coordinates": [19.0, 145.01, 328.66, 305.25, 143.07]}, {"formula_id": "formula_46", "formula_text": "\u2126 = s\u2208R + cl{x (t) : t > s}", "formula_coordinates": [19.0, 238.84, 630.83, 117.6, 22.99]}, {"formula_id": "formula_47", "formula_text": "A 1 = A 2 = {a 1 , a 2 } game \u0393 = N , (A k , u k ) k\u2208N with payoff matrices u 1 = a 1 a 2 a 1 u 11 u 12 a 2 u 21 u 22 , u 2 = a 1 a 2 a 1 v 11 v 21 a 2 v 12 v 22 ,(11a)", "formula_coordinates": [20.0, 72.0, 162.76, 451.28, 65.73]}, {"formula_id": "formula_48", "formula_text": "u 11 > u 21 , u 22 > u 12 and v 11 > v 21 , v 22 > v 12 (11b)", "formula_coordinates": [20.0, 194.83, 274.29, 328.45, 10.63]}, {"formula_id": "formula_49", "formula_text": "mix ) = \u03bb 2 k 2 , \u03bb 1 k 1", "formula_coordinates": [20.0, 312.86, 336.78, 65.66, 15.96]}, {"formula_id": "formula_50", "formula_text": "\u03bb 1 := u 22 \u2212 u 12 , k 1 := u 11 \u2212 u 12 \u2212 u 21 + u 22 and \u03bb 2 := v 22 \u2212v 12 , k 2 := v 11 \u2212v 12 \u2212v 21 +v 22 , with \u03bb i , k i > 0 for i = 1, 2. The equilibrium (a 2 , a 2 ) is called risk-dominant if (u 22 \u2212 u 12 ) (v 22 \u2212 v 12 ) > (u 11 \u2212 u 21 ) (v 11 \u2212 v 21 ).", "formula_coordinates": [20.0, 72.0, 339.26, 453.39, 39.85]}, {"formula_id": "formula_51", "formula_text": "P = u 11 \u2212 u 21 u 11 \u2212 u 21 \u2212 w (v 11 \u2212 v 21 ) 0 k 1 \u2212 w (v 11 \u2212 v 21 )", "formula_coordinates": [20.0, 192.55, 656.58, 202.15, 24.18]}, {"formula_id": "formula_52", "formula_text": "\u03bb 1 k 1 \u2022 \u03bb 2 k 2 > k 1 \u2212 \u03bb 1 k 1 \u2022 k 2 \u2212 \u03bb 2 k 2 \u21d0\u21d2 y mix x mix > (1 \u2212 y mix ) (1 \u2212 x mix ) \u21d0\u21d2 x mix + y mix > 1 as claimed.", "formula_coordinates": [20.0, 88.22, 751.42, 420.04, 25.5]}, {"formula_id": "formula_53", "formula_text": "x x (1 \u2212 x) = \u03b2 x [y (u 11 \u2212 u 21 ) + (1 \u2212 y) (u 12 \u2212 u 22 )] + \u03b1 x ln 1 x \u2212 1 (12a) y y (1 \u2212 y) = \u03b2 y [x (v 11 \u2212 v 21 ) + (1 \u2212 x) (v 12 \u2212 v 22 )] + \u03b1 y ln 1 y \u2212 1 (12b)", "formula_coordinates": [21.0, 138.94, 281.71, 695.58, 54.6]}, {"formula_id": "formula_54", "formula_text": "\u03bb 1 := u 22 \u2212 u 12 , k 1 := u 11 \u2212 u 12 \u2212 u 21 + u 22 and \u03bb 2 := v 22 \u2212 v 12 , k 2 := v 11 \u2212 v 12 \u2212 v 21 + v 22 , with \u03bb i , k i > 0 for i = 1", "formula_coordinates": [21.0, 72.0, 398.61, 451.55, 36.67]}, {"formula_id": "formula_55", "formula_text": "x Q \u2212 1 = 0 (13a) c 2 (x Q \u2212 x mix ) + ln 1 y Q \u2212 1 = 0 (13b)", "formula_coordinates": [21.0, 217.1, 454.94, 306.18, 48.69]}, {"formula_id": "formula_56", "formula_text": "\u0393 M u = {N = {1, 2}, ({a 1 , a 2 }) k\u2208N , (u 1 , u 2 )} with u 1 , u 2 given by u 1 = a 1 a 2 a 1 2M 0 a 2 2M \u2212 1 2", "formula_coordinates": [22.0, 72.0, 643.54, 451.28, 54.85]}, {"formula_id": "formula_57", "formula_text": "v = {N = {1, 2}, ({a 1 , a 2 }) k\u2208N , (v 1 , v 2 )} with u 1 , u 2 given by u 1 = a 1 a 2 a 1 2M 1.5 a 2 2M \u2212 1 2", "formula_coordinates": [23.0, 74.34, 280.78, 448.93, 53.93]}, {"formula_id": "formula_58", "formula_text": ". . . u nn \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 , u 2 = u T 1 ,(14)", "formula_coordinates": [24.0, 296.49, 717.14, 226.79, 57.76]}, {"formula_id": "formula_59", "formula_text": "\u03a6 = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed", "formula_coordinates": [26.0, 194.64, 140.44, 31.97, 118.26]}, {"formula_id": "formula_60", "formula_text": "\uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 .", "formula_coordinates": [26.0, 386.24, 140.44, 14.39, 118.26]}, {"formula_id": "formula_61", "formula_text": "5: u = [u 1 , u 2 ], v = [v 1 , v 2 ] with 6: u 1 , v 1 \u2208 R n\u22121 , u 2 , v 2 \u2208 R m\u22121 7: procedure Transform Variables(u, v, \u03b1, \u03b2) Input: \u03b1, \u03b2 \u2190 scalars in R 8: x \u2190 \u03b1 \u2022 u 1 + \u03b2 \u2022 v 1 9:", "formula_coordinates": [26.0, 72.0, 515.27, 244.59, 84.73]}], "doi": "10.1016/j.geb.2009.08.004"}