{"title": "Computational Rationalization: The Inverse Equilibrium Problem", "authors": "Kevin Waugh; Brian D Ziebart", "pub_date": "2018-01-22", "abstract": "Modeling the purposeful behavior of imperfect agents from a small number of observations is a challenging task. When restricted to the single-agent decision-theoretic setting, inverse optimal control techniques assume that observed behavior is an approximately optimal solution to an unknown decision problem. These techniques learn a utility function that explains the example behavior and can then be used to accurately predict or imitate future behavior in similar observed or unobserved situations. In this work, we consider similar tasks in competitive and cooperative multi-agent domains. Here, unlike single-agent settings, a player cannot myopically maximize its reward; it must speculate on how the other agents may act to influence the game's outcome. Employing the 1", "sections": [{"heading": "", "text": "game-theoretic notion of regret and the principle of maximum entropy, we introduce a technique for predicting and generalizing behavior.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Introduction", "text": "Predicting the actions of others in complex and strategic settings is an important facet of intelligence that guides our interactions-from walking in crowds to negotiating multi-party deals. Recovering such behavior from merely a few observations is an important and challenging machine learning task.\nWhile mature computational frameworks for decision-making have been developed to prescribe the behavior that an agent should perform, such frameworks are often ill-suited for predicting the behavior that an agent will perform. Foremost, the standard assumption of decision-making frameworks that a criteria for preferring actions (e.g., costs, motivations and goals) is known a priori often does not hold. Moreover, real behavior is typically not consistently optimal or completely rational; it may be influenced by factors that are difficult to model or subject to various types of error when executed. Meanwhile, the standard tools of statistical machine learning (e.g., classification and regression) may be equally poorly matched to modeling purposeful behavior; an agent's goals often succinctly, but implicitly, encode a strategy that would require a tremendous number of observations to learn.\nA natural approach to mitigate the complexity of recovering a full strategy for an agent is to consider identifying a compactly expressed utility function that rationalizes observed behavior: that is, identify rewards for which the demonstrated behavior is optimal and then leverage these rewards for future prediction. Unfortunately, the problem is fundamentally ill-posed: in general, many reward functions can make behavior seem rational, and in fact, the trivial, everywhere zero reward function makes all behavior appear rational [Ng and Russell, 2000]. Further, after removing such trivial reward functions, there may be no reward function for which the demonstrated behavior is optimal as agents may be imperfect or the world they operate in may only be approximately represented.\nIn the single-agent decision-theoretic setting, inverse optimal control methods have been used to bridge this gap between the prescriptive frameworks and predictive applications [Abbeel and Ng, 2004, Ratliff et al., 2006, Ziebart et al., 2008a, 2010. Successful applications include learning and prediction tasks in personalized vehicle route planning [Ziebart et al., 2008a], predictive cursor control [Ziebart et al., 2012], robotic crowd navigation [Henry et al., 2010], quadruped foot placement and grasp selection [Ratliff et al., 2009]. A reward function is learned by these techniques that both explains demonstrated behavior and approximates the optimality criteria of decisiontheoretic frameworks.\nAs these methods only capture a single reward function and do not reason about competitive or cooperative motives, inverse optimal control proves inadequate for modeling the strategic interactions of multiple agents. In this article, we consider the game-theoretic concept of regret as a standin for the optimality criteria of the single-agent work. As with the inverse optimal control problem, the result is fundamentally ill-posed. We address this by requiring that for any utility function linear in known features, our learned model must have no more regret than that of the observed behavior. We demonstrate that this requirement can be re-cast as a set of equivalent convex constraints that we denote the inverse correlated equilibrium (ICE) polytope.\nAs we are interested in the effective prediction of behavior, we will use a maximum entropy criteria to select behavior from this polytope. We demonstrate that optimizing this criteria leads to mini-max optimal prediction of behavior subject to approximate rationality. We consider the dual of this problem and note that it generalizes the traditional log-linear maximum entropy family of problems [Della Pietra et al., 2002]. We provide a simple and computationally efficient gradient-based optimization strategy for this family and show that only a small number of observations are required for accurate prediction and transfer of behavior. We conclude by considering a variety of experimental results, ranging from predicting travel routes in a synthetic routing game to a market-entry econometric data-analysis exploring regulatory effects on hotel chains in Texas.\nBefore we formalize imitation learning in matrix games, motivate our assumptions and describe and analyze our approach, we review related work.", "publication_ref": ["b20", "b0", "b25", "b32", "b29", "b32", "b35", "b13", "b26", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Many research communities are interested in computational modeling of human behavior and, in particular, in modeling rational and strategic behavior with incomplete knowledge of utility. Here we contrast the contributions of three communities by overviewing their interests and approaches. We conclude by describing our contribution in the same light.\nThe econometrics community combines microeconomics and statistics to investigate the empirical properties of markets from sales records, census data and other publicly available statistics. McFadden first considered estimating consumer preferences for transportation by assuming them to be rational utility maximizers [McFadden, 1974]. Berry, Levinsohn and Pakes estimate both supply and demand-side preferences in settings where the firms must price their goods strategically [Berry et al., 1995]. Their initial work described a procedure for measuring the desirability of certain automobile criteria, such as fuel economy and features like air conditioning, to determine substitution effects. The Berry, Levinsohn and Pakes approach and its derivatives can be crudely described as model-fitting techniques. First, a parameterized class of utility functions are assumed for both the producers and consumers. Variables that are unobservable to the econometrician, such as internal production costs and certain aspects of the consumer's preferences, are known as shocks and are modeled as independent random variables. The draws of these random variables are known to the market's participants, but only their distributions are known to the econometrician. Second, an equilibrium pricing model is assumed for the producers. The consumers are typically assumed to be utility maximizers having no strategic interactions with in the market. Finally, an estimation technique is optimistically employed to determine a set of parameter values that are consistent with the observed behavior. Ultimately, it is from these parameter values that one derives insight into the unobservable characteristics of the market. Unfortunately, neither efficient sample nor computational complexity bounds are generally available using this family of approaches.\nA variety of questions have been investigated by econometricians using this line of reasoning. Petrin investigated the competitive advantage of being the first producer in a market by considering the introduction of the minivan to the North American automotive industry [Petrin, 2002]. Nevo provided evidence against price-fixing in the breakfast cereal market by measuring the effects of advertising [Nevo, 2001]. Others have examined the mid-scale hotel market to determine the effects of different regulatory practices [Suzuki, 2010] and how overcapacity can be used to deter competition [Conlin and Kadiyali, 2006]. As a general theme, the econometricians are interested in the intentions that guide behavior. That is, the observed behavior is considered to be the truth and the decision-making framework used by the producers and consumers is known a priori.\nThe decision theory community is interested in human behavior on a more individual level. They, too, note that out-of-the-box game theory fails to explain how people act in many scenarios. As opposed to viewing this as a flaw in the theories, they focus on both how to alter the games that model our interactions in addition to devising human-like decisionmaking algorithms. The former can be achieved through modifications to the players' utility functions, which are known a priori, to incorporate notions such as risk aversion and spite [Myers andSadler, 1960, Erev et al., 2008]. latter approaches often tweak learning algorithms by integrating memory limitations or emphasizing recent or surprising observations [Camerer andHo, 1999, Erev andBarron, 2005].\nthe Iterative Weighting and Sampling algorithm (I-SAW) is more likely to choose the action with the highest estimated utility, but recent observations are weighted more highly and, in the absence of a surprising observation, the algorithm favors repeating previous actions [Erev et al., 2010]. Memory limitations, or more generally bounded rationality, have also led to novel equilibrium concepts such as the quantal response equilibrium [McKelvey and Palfrey, 1995]. This concept assumes the players' strategies have faults, but that small errors, in terms of forgone utility, are much more common than large errors. Contrasting with the econometricians, the decision theory community is mainly interested in the algorithmic process of human decision-making. The players' preferences are known and observed behavior serves only to validate or inform an experimental hypothesis.\nFinally, the machine learning community is interested in predicting and imitating the behavior of humans and expert systems. Much work in this area focuses on the single-agent setting and in such cases it is known as inverse optimal control or inverse reinforcement learning [Abbeel andNg, 2004, Ng andRussell, 2000]. Here, the observed behavior is assumed to be an approximately optimal solution to an unknown decision problem. At a high level, known solutions typically summarize the behavior as parameters to a low dimensional utility function. A number of methods have been introduced to learn these weights, including margin-based methods [Ratliff et al., 2006] that can utilize black box optimal control or planning software, as well as maximum entropy-based methods with desirable predictive guarantees [Ziebart et al., 2008a]. These utility weights are then used to mimic the behavior in similar situations through a decision-making algorithm. Unlike the other two communities, it is the predictive performance of the learned model that is most pivotal and noisy observations are expected and managed by those techniques.\nThis article extends our prior publication-a novel maximum entropy approach for predicting behavior in strategic multi-agent domains [Waugh et al., 2011a,b]. We focus on the computationally and statistically efficient recovery of good estimates of behavior (the only observable quantity) by leveraging rationality assumptions. The work presented here further develops those ideas in two key ways. First, we consider distributions over games and parameterized families of deviations using the notion of conditional entropy. Second, this work enables more fine-grained assumptions regarding the players' possible preferences. Finally, this work presents the analysis of data-sets from both the econometric and decision theory communities, comparing and contrasting the methods presented with statistical methods that are blind to the strategic aspects of the domains.\nBefore describing our approach, we will introduce the necessary notation and background.", "publication_ref": ["b16", "b1", "b24", "b19", "b29", "b4", "b18", "b3", "b7", "b9", "b17", "b0", "b0", "b20", "b25", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries Notation", "text": "Let V be a Hilbert space with an inner product \u2022, \u2022 : V \u00d7 V \u2192 R. For any set K \u2286 V , let K * = {x | x, y \u2265 0, \u2200y \u2208 K} be its dual cone. We let v 2 = v, v , and, if V is of finite dimension with orthonormal basis {e 1 , . . . , e K },\nlet v 1 = K k=1 |\u03b1 k | where v = K k=1 \u03b1 k e k .\nTypically, we will take V = R K and use the standard inner product.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Game Theory", "text": "Matrix games are the canonical tool of game theorists for representing strategic interactions ranging from illustrative toy problems, such as the \"Prisoner's Dilemma\" and the \"Battle of the Sexes\" games, to important negotiations, collaborations, and auctions. Unlike the traditional definition [Osborne and Rubinstein, 1994], in this work we model games where only the features of the players' utility are known and not the utilities themselves.\nDefinition 1. A vector-valued normal-form game is a tuple \u0393 = (N , A, u \u0393 i ) where\n\u2022 N is the finite set of the game's N players,\n\u2022 A = \u00d7 i\u2208N A i is the set of the game's outcomes or joint-actions, where\n\u2022 A i is the finite set of actions or strategies for player i, and\n\u2022 u \u0393 i : A \u2192 V is the utility feature function for player i.\nWe let A = max i\u2208N |A i |.\nPlayers aim to maximize their utility, a quantity measuring happiness or individual well-being. We assume that the players' utility is a common linear function of the utility features. This will allow us to treat the players anonymously should we so desire. One can expand the utility feature space if separate utility functions are desired. We write the utility for player i at outcome a under utility function w \u2208 V as\nu \u0393 i (a|w) = u \u0393 i (a), w .\nIn contrast to the standard definition of normal-form games, where the utility functions for game outcomes are known, in this work we assume that the true utility function, formed by w * , which governs observed behavior, is unknown. This allows us to model real-world scenarios where a cardinal utility is not available or is subject to personal taste. Consider, for instance, a scenario where multiple drivers each choose a route among shared roads. Each outcome, which specifies a travel plan for all drivers, has a variety of easily measurable quantities that may impact the utility of a driver, such as travel time, distance, average speed, number of intersections and so on, but how these quantities map to utility depends on the internal preferences of the drivers.\nWe model the players using a joint strategy, \u03c3 \u0393 \u2208 \u2206 A , which is a distribution over the game's outcomes. Coordination between players can exist, thus, this distribution need not factor into independent strategies for each player. Conceptually, a trusted signaling mechanism, such as a traffic light, can be thought to sample an outcome from \u03c3 \u0393 and communicate to each player a i , its portion of the joint-action. Even in situations where players are incapable of communication prior to play, correlated play is attainable through repetition. In particular, there are simple learning dynamics that, when employed by each player independently, converge to a correlated solution [Foster andVohra, 1996, Hart andMas-Colell, 2000].\nIf a player can benefit through a unilateral deviation from the proposed joint strategy, the strategy is unstable. As we are considering coordinated strategies, a player may condition its deviations on the recommended action. That is, a deviation for player i is a function f i : A i \u2192 A i [Blum and Mansour, 2007]. To ease the notation, we overload f i : A \u2192 A to be the function that modifies only player i's action according to f i .\nTwo well-studied classes of deviations are the switch deviation,\nswitch x\u2192y i (a i ) = y if a i = x a i otherwise,\nwhich substitutes one action for another, and the fixed deviation, fixed \u2192y i (a i ) = y, which does not condition its change on the prescribed action. A deviation set, denoted \u03a6, is a set of deviation functions. We call the set of all switch deviations the internal deviation set, \u03a6 int , and the set of all fixed deviations the external deviation set, \u03a6 ext . The set \u03a6 swap is the set of all deterministic deviations. Given that the other players indeed play their recommended actions, there is no strategic advantage to considering randomized deviations.\nThe benefit of applying deviation f i when the players jointly play a is known as instantaneous regret. We write the instantaneous regret features as\nr \u0393 f i (a) = u \u0393 i (f i (a)) \u2212 u \u0393 i (a),\nand the instantaneous regret under utility function w as\nr \u0393 f i (a|w) = u \u0393 i (f i (a)|w) \u2212 u \u0393 i (a|w) = r \u0393 f i (a), w .\nMore generally, we can consider broader classes of deviations than the two we have mentioned. Conceptually, a deviation is a strategy modification and its regret is its benefit to a particular player. As we will ultimately only work with the regret features, we can now suppress the implementation details while bearing in mind that a deviation typically has these prescribed semantics. That is, a deviation f \u2208 \u03a6 has associated instantaneous regret features, r \u0393 f (\u2022), and instantaneous regret, r \u0393 f (\u2022|w). As a player is only privileged to its own portion of the coordinated outcome, it must reason about its expected regret. We write the expected regret features as\nr \u0393 f (\u03c3 \u0393 ) = E a\u223c\u03c3 \u0393 r \u0393 f (a) ,\nand the expected regret under utility function w as\nr \u0393 f (\u03c3 \u0393 |w) = E a\u223c\u03c3 \u0393 r \u0393 f (a|w) = r \u0393 f (\u03c3 \u0393 ), w .\nA joint strategy is in equilibrium or, in a sense, stable if no player can benefit through a unilateral deviation. We can quantify this stability using expected regret with respect to the deviation set \u03a6,\nRegret \u0393 \u03a6 (\u03c3 \u0393 |w) = max f \u2208\u03a6 r \u0393 f (\u03c3 \u0393 |w),\nand call a joint strategy \u03c3 \u0393 an \u03b5-equilibrium if\nRegret \u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 \u03b5.\nThe most general deviation set, \u03a6 swap , corresponds with the \u03b5-correlated equilibrium solution concept [Osborne andRubinstein, 1994, Blum andMansour, 2007]. Thus, regret can be thought of as the natural substitute for utility when assessing the optimality of behavior in multi-agent settings.\nThe set \u03a6 swap is typically intractably large. Fortunately, internal regret closely approximates swap regret and is polynomially-sized in both the number of actions and players.\nLemma 1. If joint strategy \u03c3 \u0393 has \u03b5 internal regret, then it is an A\u03b5correlated equilibrium under utility function w. That is, \u2200w \u2208 V ,\nRegret \u0393 \u03a6 int (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 swap (\u03c3 \u0393 |w) \u2264 A \u2022 Regret \u0393 \u03a6 int (\u03c3 \u0393 |w).\nThe proof is provided in the Appendix.", "publication_ref": ["b23", "b10", "b12", "b2", "b23", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Behavior Estimation in a Matrix Game", "text": "We are now equipped with the tools necessary to introduce our approach for imitation learning in multi-agent settings. We start by assuming a notion of rationality on the part of the game's players. By leveraging this assumption, we will then derive an estimation procedure with much better statistical properties than methods that are unaware of the game's structure.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rationality and the ICE Polytope", "text": "Let {a t } T t=1 be a sequence of T independent observations of behavior in game \u0393 distributed according to \u03c3 \u0393 , the players' true behavior. We call the empirical distribution of the observations,\u03c3 \u0393 , the demonstrated behavior.\nWe aim to learn a distribution\u03c3 \u0393 , called the predicted behavior, an estimation of the true behavior from these demonstrations. Moreover, we would like our learning procedure to extract the motives for the behavior so that we may imitate the players in similarly structured, but unobserved games. Initially, let us consider just the estimation problem. While deriving our method, we will assume we have access to the players' true behavior. Afterwards, we will analyze the error introduced by approximating from the demonstrations.\nImitation appears hard barring further assumptions. In particular, if the agents are unmotivated or their intentions are not coerced by the observed game, there is little hope of recovering principled behavior in a new game. Thus, we require a form of rationality.\nProposition 1. The players in a game are rational with respect to deviation set \u03a6 if they prefer joint-strategy \u03c3 \u0393 over joint strategies\u03c3 \u0393 when\nRegret \u0393 \u03a6 (\u03c3 \u0393 |w * ) < Regret \u0393 \u03a6 (\u03c3 \u0393 |w * ).\nOur rationality assumption states that the players are driven to minimize their regret. It is not necessarily the case that they indeed have low or no regret, but simply that they can evaluate their preferences and that they prefer joint strategies with low regret. Through this assumption, we will be able to reason about the players' behavior solely through the game's features; this is what leads to the improved statistical properties of our approach.\nAs agents' true preferences w * are unknown, we consider an encompassing assumption that requires that estimated behavior satisfy this property for all possible utility weights. A prediction\u03c3 \u0393 is strongly rational with respect to deviation set \u03a6 if\n\u2200w \u2208 V, Regret \u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 (\u03c3 \u0393 |w).\nThis assumption is similar in spirit to the utility matching assumption employed by inverse optimal control techniques in single-agent settings. As in those settings, we have an if and only if guarantee relating rationality and strong rationality [Abbeel andNg, 2004, Ziebart et al., 2008a].\nTheorem 1. If a prediction\u03c3 \u0393 is strongly rational with respect to deviation set \u03a6 and the players are rational with respect to \u03a6, then they do not prefer \u03c3 \u0393 over\u03c3 \u0393 . This is immediate as w * \u2208 V . Phrased another way, a strongly rational prediction is no worse than the true behavior.\nCorollary 1. If a prediction\u03c3 \u0393 is strongly rational with respect to deviation set \u03a6 and the true behavior is an \u03b5-equilibrium with respect to \u03a6 under utility function w * \u2208 V , then\u03c3 \u0393 is also an \u03b5-equilibrium.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Again, the proof is immediate as Regret", "text": "\u0393 \u03a6 (\u03c3 \u0393 |w * ) \u2264 Regret \u0393 \u03a6 (\u03c3 \u0393 |w * ) \u2264 \u03b5.\nConversely, if we are uncertain about the true utility function we must assume strong rationality or we risk predicting less desirable behavior.\nTheorem 2. If a prediction\u03c3 \u0393 is not strongly rational with respect to deviation set \u03a6 and the players are rational, then there exists a w * \u2208 V such that \u03c3 \u0393 is preferred to\u03c3 \u0393 .\nThe proof follows from the negation of the definition of strong rationality. By restricting our attention to strongly rational behavior, at worst agents acting according to their unknown true preferences will be indifferent between our predictive distribution and their true behavior. That is, strong rationality is necessary and sufficient under the assumption players are rational given no knowledge of their true utility function.\nUnfortunately, a direct translation of the strong rationality requirement into constraints on the distribution\u03c3 \u0393 leads to a non-convex optimization problem as it involves products of varying utility vectors with the behavior to be estimated. Fortunately, we can provide an equivalent concise convex description of the constraints on\u03c3 \u0393 that ensures any feasible distribution satisfies strong rationality. We denote this set of equivalent constraints as the Inverse Correlated Equilibria (ICE) polytope.\nDefinition 2 (Standard ICE Polytope). r \u0393 f (\u03c3 \u0393 ) = E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .\nHere, we have introduced \u03a6 f , the set of deviations that f will be compared against. Our rationality assumption corresponds to when \u03a6 f = \u03a6, but there are different choices that have reasonable interpretations as alternative rationality assumptions. For example, if each switch deviation is compared only against switches for the same player-a more restrictive conditionthen the quality of the equilibrium is measured by the sum of all players' regrets, as opposed to only the one with the most regret.\nThe following corollary equates strong rationality and the standard ICE polytope.\nCorollary 2. A prediction\u03c3 \u0393 is strongly rational with respect to deviation set \u03a6 if and only if for all f \u2208 \u03a6 there exists \u03b7 f \u2208 \u2206 \u03a6 such that\u03c3 \u0393 and \u03b7 satisfy the standard ICE polytope.\nWe now show a more general result that implies Corollary 2. We start by generalizing the notion of strong rationality by restricting w * to be in a known set K \u2286 V . We say a prediction\u03c3 \u0393 is K-strongly rational with respect to deviation set \u03a6 if\n\u2200w \u2208 K, Regret \u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 (\u03c3 \u0393 |w).\nIf K is convex with non-empty relative interior and 0 \u2208 K, we derive the K-ICE polytope.\nDefinition 3 (K-ICE Polytope). r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) \u2208 \u2212K * , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .\nNote that the above constraints are linear in\u03c3 \u0393 and \u03b7 f , and K * , the dual cone, is convex. The following theorem shows the equivalence of the K-ICE polytope and K-strong rationality.\nTheorem 3. A prediction\u03c3 \u0393 is K-strongly rational with respect to deviation set \u03a6 if and only if for all f \u2208 \u03a6 there exists \u03b7 f \u2208 \u2206 \u03a6 such that\u03c3 \u0393 and \u03b7 satisfy the K-ICE polytope.\nThe proof is provided in the Appendix. By choosing K = V , then K * = {0} and the polytope reduces to the standard ICE polytope. Thus, Corollary 2 follows directly from Theorem 3. By choosing K to be the positive orthant, K = K * = R K + , the polytope reduces to the following inequalities. Here, we explicitly assume the utility to be a positive linear function of the features.\nDefinition 4 (Positive ICE Polytope). r \u0393 f (\u03c3 \u0393 ) \u2264 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .\nPredictive behavior within the ICE polytope will retain the quality of the demonstrations provided. The following corollaries formalize this guarantee.\nCorollary 3. If the true behavior is an \u03b5-correlated equilibrium under w * in game \u0393, then a prediction\u03c3 \u0393 that satisfies the standard ICE polytope where \u03a6 = \u03a6 swap and \u2200f \u2208 \u03a6, \u03a6 f = \u03a6 is also an \u03b5-correlated equilibrium.\nThis follows immediately from the definition of an approximate correlated equilibrium.\nCorollary 4. If the true behavior is an \u03b5-correlated equilibrium under w * in game \u0393, then a prediction\u03c3 \u0393 that satisfies the standard ICE polytope where \u03a6 = \u03a6 int and \u2200f \u2208 \u03a6, \u03a6 f = \u03a6 is also an A\u03b5-correlated equilibrium.\nThis follows immediately from Lemma 1. In two-player constant-sum games, we can make stronger statements about our predictive behavior. In particular, when these requirements are satisfied we may reason about games without coordination. That is, each player chooses their action independently using their strategy, \u03c3 \u0393 i a distribution over A i . A strategy profile \u03c3 \u0393 consists of a strategy for each player. It defines a joint-strategy with no coordination between the players.\nA game is constant-sum if there is a fixed amount of utility divided among the players. That is, if there is a constant C such that \u2200a \u2208 A,\ni\u2208N u \u0393 i (a|w * ) = C.\nIn settings where the players act independently, we use external regret to measure a profile's stability, which corresponds with the famous Nash equilibrium solution concept [Osborne and Rubinstein, 1994]. By using the ICE polytope with external regret, we can recover a Nash equilibrium if one is demonstrated in a constant-sum game.\nTheorem 4. If the true behavior is an \u03b5-Nash equilibrium in a two-player constant-sum game \u0393, then the marginal strategies formed from a prediction\u03c3 \u0393 that satisfies the standard ICE polytope where \u03a6 = \u03a6 ext and \u2200f \u2208 \u03a6, \u03a6 f = \u03a6 is a 2\u03b5-Nash equilibrium.\nThe proof is provided in the Appendix. In general, there can be infinitely many correlated equilibrium with vastly different properties. One such property that has received much attention is the social welfare of a joint strategy, which refers to the total utility over all players. Our strong rationality assumption states that the players have no preference on which correlated equilibrium is selected, and thus without modification cannot capture such a concept should it be demonstrated. We can easily maintain the social welfare of the demonstrations by additionally preserving the players' utilities along side the constraints prescribed by the ICE polytope. A joint strategy is utility-preserving under all utility functions if\n\u2200w \u2208 V, i \u2208 N , u \u0393 i (\u03c3 \u0393 |w) = u \u0393 i (\u03c3 \u0393 |w).\nAs with the correspondence between strong rationality and the ICE polytope, utility preservation can be represented as a set of linear equality constraints. These utility feature matching constraints are exactly the basis of many methods of inverse optimal control [Abbeel andNg, 2004, Ziebart et al., 2008a].\nTheorem 5. A joint strategy is utility-preserving under all utility functions if and only if\ni \u2208 N , u \u0393 i (\u03c3 \u0393 ) = u \u0393 i (\u03c3 \u0393 ).\nThe proof is due to Abbeel and Ng [2004].\nA notable choice for \u03a6 f is we compare each deviation only to itself. As a consequence this enforces a stronger constraint that the regret under each deviation, and in turn the overall regret, is the same under our prediction and the demonstrations. That is,\u03c3 \u0393 is regret-matching as for all w \u2208 V ,\nRegret \u0393 \u03a6 (\u03c3 \u0393 |w) = Regret \u0393 \u03a6 (\u03c3 \u0393 |w).\nThus, regret-matching preserves the equilibrium qualities of the demonstrations.\nUnlike the correspondence between the ICE polytope and strong rationality, matching the regret features for each deviation is not required for a strategy to match the regrets of the demonstrations. That is, the converse does not hold. 1 Theorem 6. A prediction\u03c3 \u0393 matches the regret of \u03c3 \u0393 for all w \u2208 V does not necessarily match the regret features of \u03c3 \u0393 .\nWe use both utility and regret matching in our final set of experiments. The former for predictive reasons, the latter to allow for the use of smooth minimization techniques.", "publication_ref": ["b23", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "The Principle of Maximum Entropy", "text": "As we are interested in the problem of statistical prediction of strategic behavior, we must find a mechanism to resolve the ambiguity remaining after accounting for the rationality constraints. The principle of maximum entropy, due to Jaynes [1957], provides a well-justified method for choosing such a distribution. This choice leads to not only statistical guarantees on the resulting predictions, but to efficient optimization.\nThe Shannon entropy of a joint-strategy \u03c3 \u0393 is\nH \u0393 (\u03c3 \u0393 ) = E a\u223c\u03c3 \u0393 \u2212 log \u03c3 \u0393 (a) ,\nand the principle of maximum entropy advocates choosing the distribution with maximum entropy subject to known constraints [Jaynes, 1957]. That is,\n\u03c3 MaxEnt = argmax \u03c3 \u0393 \u2208\u2206 A H \u0393 (\u03c3 \u0393 ), subject to: g(\u03c3 \u0393 ) = 0 and h(\u03c3 \u0393 ) \u2264 0.\nThe constraint functions, g and h, are typically chosen to capture the important or most salient characteristics of the distribution. When those functions are affine and convex respectively, finding this distribution is a convex optimization problem. The resulting log-linear family of distributions (e.g., logistic regression, Markov random fields, conditional random fields) are widely used within statistical machine learning.\nIn the context of multi-agent behavior, the principle of maximum entropy has been employed to obtain correlated equilibria with predictive guarantees in normal-form games when the utilities are known a priori [Ortiz et al., 2007]. We will now leverage its power with our rationality assumption to select predictive distributions in games where the utilities are unknown, but the important features that define them are available.\nFor our problem, the constraints are precisely that the distribution is in the ICE polytope, ensuring that whatever we predict has no more regret than the demonstrated behavior.\nDefinition 5. The primal maximum entropy ICE optimization problem is\nmaximiz\u00ea \u03c3 \u0393 ,\u03b7 H \u0393 (\u03c3 \u0393 ) subject to: r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) \u2208 \u2212K * , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .\nThis program is convex, feasible, and bounded. That is, it has a solution and is efficiently solvable using simple techniques in this form.\nImportantly, the maximum entropy prediction enjoys the following guarantee:\nLemma 2. The maximum entropy ICE distribution minimizes over all strongly rational distributions the worst-case log-loss, E a\u223c\u03c3 \u0393 \u2212 log 2\u03c3 \u0393 (a) , when \u03c3 \u0393 is chosen adversarially but subject to strong rationality.\nThe proof of Lemma 2 follows immediately from the result of Gr\u00fcnwald and Dawid [2003].", "publication_ref": ["b14", "b14", "b22", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Dual Optimization", "text": "In this section, we will derive and describe a procedure for optimizing the dual program for solving the MaxEnt ICE optimization problem. We will see that the dual multipliers can be interpreted as utility vectors and that optimization in the dual has computational advantages. We begin by presenting the dual program.\nTheorem 7. The dual maximum entropy ICE optimization problem is the following non-smooth, but convex program:\nminimize \u03b8 f \u2208K * * f \u2208\u03a6 Regret \u0393 \u03a6 f (\u03c3 \u0393 |\u03b8 f ) + log Z \u0393 (\u03b8),where\nZ \u0393 (\u03b8) = a\u2208A exp \uf8eb \uf8ed \u2212 f \u2208\u03a6 r \u0393 f (a|\u03b8 f ) \uf8f6 \uf8f8 .\nWe derive the dual in the Appendix.\nAs the dual's feasible set has non-empty relative interior, strong duality holds by Slater's condition-there is no duality gap. We can also use a dual solution to recover\u03c3 \u0393 . Lemma 3. Strong duality holds for the maximum entropy ICE optimization problem and given optimal dual weights \u03b8 * , the maximum entropy ICE joint-\nstrategy\u03c3 \u0393 is\u03c3 \u0393 (a) \u221d exp \uf8eb \uf8ed \u2212 f \u2208\u03a6 r \u0393 f (a|\u03b8 * f ) \uf8f6 \uf8f8 .(1)\nThe dual formulation of our program has important inherent computational advantages. First, so long as K is simple, the optimization is particularly well-suited for gradient-based optimization, a trait not shared by the primal program. Second, the number of dual variables, |\u03a6| dim V , is typically much fewer than the number of primal variables, |A| + |\u03a6| 2 . Though", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Dual MaxEnt ICE Gradient", "text": "Input: Let\u03c3 \u0393 be the prediction given the current dual weights, \u03b8, as from Equation ( 1).\nfor f \u2208 \u03a6 do f * \u2190 argmax f \u2208\u03a6 f r \u0393 f (\u03c3 \u0393 |\u03b8 f ) g f \u2190 r \u0393 f * (\u03c3 \u0393 ) \u2212 r \u0393 f (\u03c3 \u0393 ) end for return g\nthe work per iteration is still a function of |A| (to compute the partition function), these two advantages together let us scale to larger problems than if we consider optimizing the primal objective. Computing the expectations necessary to descend the dual gradient can leverage recent advances in the structured, compact game representations: in particular, any graphical game with low-treewidth or finite horizon Markov game [Kakade et al., 2003] enables these computations to be performed in time that scales only polynomially in the number of decision makers.\nAlgorithm 1 describes the dual gradient computation. This can be incorporated with any non-smooth gradient method, such as the projected subgradient method [Shor, 1985], to approach the optimum dual weights.", "publication_ref": ["b15", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Behavior Estimation in Parameterized Matrix Games", "text": "To account for stochastic, or varying environments, we now consider distributions over games. For example, rain may affect travel time along some routes and make certain modes of transportation less desirable, or even unavailable. Operationally, nature samples a game prior to play from a distribution known to the players. The players then as a group determine a joint strategy conditioned on the particular game and an outcome is drawn by a coordination device. We let G denote our class of games.\nAs before, we observe a sequence of T independent observations of play, but now in addition to an outcome we also observe nature's choice at each time t. Let {(\u0393 t , a t )} T t=1 be the aforementioned sequence of observations drawn from \u03be and \u03c3, the true behavior. The empirical distribution of the observations,\u03be and\u03c3, together are the demonstrated behavior. Now we aim to learn a predictive behavior distribution,\u03c3 \u0393 , for any \u0393 \u2208 G, even ones we have not yet observed. Clearly, we must leverage the observations across the entire family to achieve good predictive accuracy. We continue to assume that the players' utility is an unknown linear function, w * , of the games' features and that this function is fixed across G. Next, we amend our notion of regret and our rationality assumption.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Behavior Estimation through Conditional ICE", "text": "Ultimately, we wish to simply employ an additional expectation over the game distribution when reasoning about the regret and regret features. To do this, our notion of a deviation needs to account for the fact that it may be executed in games with different structures. Operationally, one way to achieve this is by having a deviation not act when it is applied to such a game, which increases the size of \u03a6 by a factor of |G|. If the actions, and in turn the deviations, have similar semantic meanings across our entire family of games, one can simply share the deviations across all games. This allows for one to achieve transfer over an infinitely large class. Given such a decision, we write the expected regret features under deviation f as r \u03be f (\u03c3) = E \u0393\u223c\u03be r \u0393 f (\u03c3 \u0393 ) , and the expected regret under utility function w as r \u03be f (\u03c3|w) = E \u0393\u223c\u03be r \u0393 f (\u03c3 \u0393 |w) . Again, we quantify the stability of a set of joint strategies using this new notion of expected regret with respect to the deviation set \u03a6,\nRegret \u03be \u03a6 (\u03c3|w) = max f \u2208\u03a6 r \u03be f (\u03c3|w),\nwhich, in turn, entails a notion of an \u03b5-equilibrium for a set of joint strategies, a modified rationality assumption, and a slight modification to the K-ICE polytope,\nDefinition 6 (Conditional K-ICE Polytope). r \u03be f (\u03c3) \u2212 E g\u223c\u03b7 f r \u03be g (\u03c3) \u2208 \u2212K * , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A . \u2200\u0393 \u2208 G\nAll that remains is to adjust our notion of entropy to take into account a distribution over games. In particular, we choose to maximize the expected entropy of our prediction, which is conditioned on the game sampled by chance.\nDefinition 7. The conditional Shannon entropy of a set of strategies \u03c3 when games are distributed according to \u03be is\nH \u03be (\u03c3) = E \u0393\u223c\u03be H \u0393 (\u03c3 \u0393 ) .\nThe modified dual optimization problem has a familiar form. We now use the new notion of regret and take the expected value of the log partition function.\nTheorem 8. The dual conditional maximum entropy ICE optimization problem is\nminimize \u03b8 f \u2208K * * f \u2208\u03a6 Regret \u03be \u03a6 f (\u03c3|\u03b8 f ) + E \u0393\u223c\u03be log Z \u0393 (\u03b8) .\nTo recover the predicted behavior for a particular game, we use the same exponential family form as before.\nAs with any machine learning technique, it is advisable to employ some form of complexity control on the resulting predictor to prevent over-fitting.\nAs we now wish to generalize to unobserved games, we too should take the appropriate precautions. In our experiments, we employ L1 and L2 regularization terms to the dual objective for this purpose. Regularization of the dual weights effectively alters the primal constraints by allowing them to hold approximately, leading to higher entropy solutions [Dud\u00edk et al., 2007].", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Behavior Transfer without common deviations", "text": "A principal justification of inverse optimal control techniques that attempt to identify behavior in terms of utility functions is the ability to consider what behavior might result if the underlying decision problem were changed while the interpretation of features into utilities remain the same Russell, 2000, Ratliff et al., 2006]. This enables prediction of agent behavior in a no-regret or agnostic sense in problems such as a robot encountering novel terrain [Silver et al., 2010] as well as route recommendation for drivers traveling to unseen destinations [Ziebart et al., 2008b].\nEconometricians are interested in similar situations, but for much different reasons. Typically, they aim to validate a model of market behavior from observations of product sales. In these models, the firms assume a fixed pricing policy given known demand. The econometrician uses this fixed policy along with product features and sales data to estimate or bound both the consumers' utility functions as well as unknown production parameters, like markup and production cost [Berry et al., 1995, Nevo, 2001. In this line of work, the observed behavior is considered accurate to start with; it is unclear how suitable these methods are for settings with limited or noisy observations.\nIn our prior work, we introduced an approach to behavior transfer applicable between games with different action sets [Waugh et al., 2011a]. It is based off the assumption of transfer rationality, or for two games \u0393 and \u0393 and some constant \u03ba > 0,\n\u2200w \u2208 V, Regret\u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 \u03ba Regret \u0393 \u03a6 (\u03c3 \u0393 |w).\nRoughly, we assume that under preferences with low regret in the original game, the behavior in the unobserved game should also have low regret. By enforcing this property, if the agents are performing well with respect to their true preferences, then the transferred behavior will also be of high quality.\nAssuming transfer rationality is equivalent to using the conditional ICE estimation program with differing game distributions for the predicted and demonstrated regret features. In such a case, the program is not necessarily feasible and the constraints must be relaxed. For example, a slack variable may be added to the primal, or through regularization in the dual. We note that this requires the estimation program to be run at test time.", "publication_ref": ["b28", "b33", "b1", "b19", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Sample Complexity", "text": "In practice, we do not have full access to the agents' true behavior-if we did, prediction would be straightforward and we would not require our estimation technique. Instead, we may only approximate the desired expectations by averaging over a finite number of observations,\nr\u03be f (\u03c3|w) \u2248 1 T T t=1 r \u0393 t f (a t |w).\nIn real applications there are costs associated with gathering these observations and, thus, there are inherent limitations on the quality of this approximation. Next, we will analyze the sensitivity of our approach to these types of errors. First, although |A| is exponential in the number of players, our technique only accesses\u03c3 through expected regret features of the form r\u03be f (\u03c3). That is, we need only approximate these features accurately, not the distribution \u03c3. For finite-dimensional vector spaces, we can bound how well the regrets match in terms of |\u03a6| and the dimension of the space.\nTheorem 9. With probability at least 1 \u2212 \u03b4, for any w, by observing T \u2265\n1 2 2 log 2|\u03a6| dim V \u03b4 outcomes we have for all deviations r\u03be f (\u03c3|w) \u2264 r \u03be f (\u03c3|w) + \u2206 w 1 .\nwhere \u2206 is the maximum possible regret over all basis directions. The proof is an application of the union bound and Hoeffding's inequality and is provided in the Appendix.\nAlternatively, we can bound how well the regrets match independently of the space's dimension by considering each utility function separately.\nTheorem 10. With probability at least 1 \u2212 \u03b4, for any w, by observing T \u2265 1 2 2 log |\u03a6| \u03b4 outcomes we have for all deviations r\u03be f (\u03c3|w) \u2264 r \u03be f (\u03c3|w) + \u2206(w).\nwhere \u2206(w) is the maximum possible regret under w. Again, the proof is in the Appendix.\nBoth of the above bounds imply that, so long as the true utility function is not too complex, with high probability we need only logarithmic many samples in terms of |\u03a6| and dim V to closely approximate r \u03be f (\u03c3) and avoid a large violation of our rationality condition.\nTheorem 11. If for all f , r\u03be f (\u03c3|w) \u2264 r \u03be f (\u03c3|w) + \u03b3, then Regret\u03be \u03a6 (\u03c3|w) \u2264 Regret \u03be \u03a6 (\u03c3|w) + \u03b3.\nProof. For all deviations, f \u2208 \u03a6, r\u03be f (\u03c3|w) \u2264 r \u03be f (\u03c3|w)+\u03b3 \u2264 Regret \u03be \u03a6 (\u03c3|w)+\u03b3. In particular, this holds for the deviation that maximizes the demonstrated regret.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Synthetic Routing Game", "text": "To evaluate our approach experimentally, we first consider a simple synthetic routing game. Seven drivers in this game choose how to travel home during rush hour after a long day at the office. The different road segments have varying capacities that make some of them more or less susceptible to congestion. Upon arrival home, the drivers record the total time and distance they traveled, the fuel that they used, and the amount of time they spent stopped at intersections or in congestion-their utility features.\nIn this game, each of the drivers chooses from four possible routes, yielding over 16, 000 possible outcomes. We obtained an \u03b5-social welfare maximizing correlated equilibrium for those drivers using a subgradient method where the drivers preferred mainly to minimize their travel time, but were also slightly concerned with fuel cost. The demonstrated behavior\u03c3 \u0393 was sampled from this true behavior distribution \u03c3 \u0393 .\nIn Figure 1 we compare the prediction accuracy of MaxEnt ICE, measured using log loss, E a\u223c\u03c3 \u0393 \u2212 log 2\u03c3 \u0393 (a) , against a number of baselines by varying the number of observations sampled from the \u03b5-equilibrium. The baseline algorithms are: a smoothed multinomial distribution over the jointactions, a logistic regression classifier parameterized with the outcome utilities, and a maximum entropy inverse optimal control approach [Ziebart et al., 2008a] trained individually for each player. In Figure 1, we see that MaxEnt ICE predicts behavior with higher accuracy than all other algorithms when the number of observations is limited. In particular, it achieves close to its best performance with only 16 observations. The maximum likelihood estimator eventually overtakes it, as expected since it will ultimately converge to \u03c3 \u0393 , but only after 10, 000 observations, or close to as many observations as there are outcomes in the game. MaxEnt ICE cannot learn the true behavior exactly in this case without  additional constraints due to the social welfare criteria the true behavior optimizes. That is, our rationality assumption does not hold in this case. We note that the logistic regression classifier and the inverse optimal control techniques perform better than the multinomial under low sample sizes, but they fail to outperform MaxEnt ICE due to their inability to appreciate the strategic nature of the game. Next, we evaluate behavior transfer from this routing game to four similar games, the results of which are displayed in Table 1. The first game, Add Highway, adds a new route to the game. That is, we simulate the city building a new highway. The second game, Add Driver, adds another driver to the game. The third game, Gas Shortage, keeps the structure of the game the same, but changes the reward function to make gas mileage more important to the drivers. The final game, Congestion, simulates adding construction to the major roadway, delaying the drivers. Here, we do not share deviations across the training and test game and we add a slack variable in the primal to ensure feasibility.\nThese transfer experiments even more directly demonstrate the benefits of learning utility weights rather than directly learning the joint-action distribution; direct strategy-learning approaches are incapable of being applied to general transfer setting. Thus, we can only compare against the Logistic Regression. We see from Table 1 that MaxEnt ICE outperforms the Logistic Regression in all of our tests. For reference, in these new games, the uniform strategy has a loss of approximately 6.8 in all games, and the true behavior has a loss of approximately 2.7.\nThese experiment demonstrates that learning underlying utility functions to estimate observed behavior can be much more data-efficient for small sample sizes. Additionally, it shows that the regret-based assumptions of MaxEnt ICE are beneficial in strategic settings, even though our rationality assumption does not hold in this case.", "publication_ref": ["b32"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Market Entry Game", "text": "We next evaluate our approach against a number of baselines on data gathered for the Market Entry Prediction Competition [Erev et al., 2010]. The game has four players and is repeated for fifty trials and is meant to simulate a firm's decision to enter into a market. On each round, all four players simultaneous decide whether or not to open a business. All players who enter the market receive a stochastic payoff centered at 10 \u2212 kE, where k is a fixed parameter unknown to the players and E is the number of players who entered. Players who do not enter the market receive a stochastic payoff with zero mean. After each round, each player is shown their reward, as well as the reward they would have received by choosing the alternative.\nObservations of human play were gathered by the CLER lab at Harvard [Erev et al., 2010]. Each student involved in the experiment played ten games lasting fifty rounds each. The students were incentivized to play well through a monetary reward proportional to their cumulative utility. The parameter k was randomly selected in a fashion so that the Nash equilibrium had an entry rate of 50% in expectation. In total, 30, 000 observations of play were recorded. The intent of the competition was to have teams submit programs that would play in a similar fashion to the human subjects. That is, the data was used at test time to validate performance. In contrast, our experiments use actual observations of play at training time to build a predictive model of the human behavior. As we are interested in stationary behavior, we train and test on only the last twenty five trials of each game.\nWe compared against two baselines. The first baseline, labeled Multinomial in the figures, is a smoothed multinomial distribution trained to minimize the leave-one-out cross validation loss. This baseline does not make use of any features of the games. That is, if the players indeed play according to the Nash equilibrium we would expect this baseline to learn the uniform distribution. The second baseline, labeled Logistic Regression in the figures, simply uses regularized logistic regression to learn a linear classification boundary over the outcomes of the game using the same features presented to our method. Operationally, this is equivalent to using MaxEnt Inverse Optimal Control in a single-agent setting where the utility is summed across all the players. This baseline has similar representational power to our method, but lacks an understanding of the strategic elements of the game.\nIn Figure 2, we see a comparison of our method against the baselines when only the game's true expected utility is used as the only feature. We see that our method outperforms both baselines across all sample sizes. We also observe the multinomial distribution performs slightly better than the uniform distribution, which attains a log loss of 4, though substantially worse than logistic regression and our method, indicating that the human players are not particularly well-modeled by the Nash equilibrium. Our method substantially outperforms logistic regression, indicating that there is indeed a strategic interaction that is not captured in the utility features alone.\nIn Figure 3, we see a comparison of our method against the baselines using a variety of predictive features. In particular, we summarize a round using the observed action frequencies, average reward, and reward variance up to that point in the round. To weigh recent observations more strongly, we also employ exponentially-weighted averages. We observe that the use of these features substantially improves the predictive power of the featurebased methods. Interestingly, we also note that the addition of these summary features also narrows the gap between logistic regression and MaxEnt ICE. Under low sample sizes, the logistic model performs the best, but our method overtakes it as more data is made available for training. It appears that in this scenario, much of the strategic behavior demonstrated by the participants can be captured by these history features. ", "publication_ref": ["b9", "b9"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Mid-scale Hotel Market Entry", "text": "For our final experimental evaluation, we considered the task of predicting the behavior of mid-scale hotel chains, like Holiday Inn and Ramada, in the state of Texas. Given demographic and regulatory features of a county, we wish to predict if each chain is likely to open a new hotel or to close an existing one. The observations of play are derived from quarterly tax records over a fifteen year period from forty counties, amounting to a total of 2, 205 observations. The particular counties selected had records of all of the demographic and regulatory features, had at least four action observations, and none was a chain's flagship county. Figure 4 highlights the selected counties and visualizes their regulatory practices.\nThe demographic and regulatory features were aggregated from various sources and generously provided to us by Prof. Junichi Suzuki (2010). The demographic features for each county include quantities such as size of its population and its area, employment and household income statistics, as well as the presence or absence of an interstate, airport or national park. The regulatory features are indices measuring quantities such as commercial land zoning restrictions, tax rates and building costs. In addition to these noted features, which are fixed across all time periods, there are time-varying features such as the number of hotels and rooms for each chain and the aggregate quarterly income.\nWe model each quarterly decision as a parameterized simultaneous-move game with six players. Each player, a mid-scale hotel chain, has the action set {Close, NoAction, Open}, resulting in 729 total outcomes. For the game's utility, we allocated the county's features to each player in proportion to how many hotels they owned. That is, if a player operated 3 out of 10 hotels, the features associated with utility at that outcome would be the county's feature vector scaled by 0.3. We included bias features associated with each action to account for fixed costs associated with opening or closing a hotel.\nIn the observation data, there are a small number of instances where a chain opens or closes more than one hotel during a quarter. These events are mapped to Open and Close respectively. Though the outcome-space is quite large, the outcome distribution is extremely biased and the actions of the chains are highly correlated. In particular, over 80% of time the time no action is taken, around 17% of the time a single chain acts, and less than 3% of the time more than one chain acts. As a result, one expects the featureless multinomial estimator to have reasonable performance despite a large number of classes.\nFor experimentation, we evaluated four algorithms: a smoothed multi-nomial distribution trained to minimize the leave one out cross-validation loss, MaxEnt inverse optimal control trained once for all players, multi-class logistic regression over the joint action space, and regret-matching ICE with utility matching constraints. As the resulting optimizations for the latter two algorithms are smooth, we employed the L-BFGS quasi-Newton method with L2-regularization for training [Nocedal, 1980]. As a substitute for L1regularization, we selected the 23 best features based on their reduction in training error when using logistic regression. Each county had 63 features available. Of the top 23 features selected, 11 were regulatory indices.\nFor the logistic regression and ICE predictors, we only used utility features on the 13 high probability outcomes (no firms build, and one firm acting). The remaining outcomes had only bias features associated with them to help prevent overfitting. We experimented with a number of types of bias features, for example, 4 bias features (one for no firms build, one for a single firm builds, one for a single firm closes and one for all remaining outcomes), as well as 729 bias features (one for each outcome). We found that, though on their own the different bias features had varied predictive performance, when combined with utility and regret features they were quite similar given the appropriate regularization. In the best performing model, which we present here, we used 729 bias features resulting in 1, 028 parameters to the logistic regression model.\nIn the ICE predictor, we tied together the weights for each deviation across all the players to reduce the number of model parameters. For example, all players shared the same dual parameters for the NoAction \u2192 Open deviation. Effectively, this alters the rationality assumption such that the average regret across all players is the quantity of interest, instead of the maximum regret. Operationally, this is implemented as summing each deviation's gradient in the dual. This treats the players anonymously, thus we implicitly and incorrectly assume that conditioned on the county's parameters each firm is identical. Due to the use parameter tying, the ICE predictor has an additional 156 model parameters.\nThe test losses reported were computed using ten-fold cross validation. To fit the regularization parameters for logistic regression, MaxEnt IOC and MaxEnt ICE, we held out 10% of the training data and performed a parameter sweep. For logistic regression, a separate parameter sweep and regularization was used for the bias and utility features. For MaxEnt ICE, an additional regularization parameter was selected for the regret parameters. A sample of the predictions from MaxEnt ICE are shown in Figure 5.\nIn the left of Figure 6, we present the test errors of the three parameterized methods in terms of their offset from that of the featureless multino-Figure 5: The marginalized probability that a chain will build a hotel in Spring 1996 predicted by MaxEnt ICE. Brighter shades of green denote higher probabilities. mial. This quantity has lower variance than the absolute errors, allowing for more accurate comparisons. We see that the addition of the regret features more than doubles the improvement of logistic regression from 2.6% to 6.3%, where as the inverse optimal control method only sees a 4.3% improvement.\nIn the center of Figure 6, we show the test log-loss when the methods are only required to predict if any firm acts. Here, the models are still trained over their complete outcome spaces and their predictions are marginalized. We see that all three methods are equal within noise. That is, the differences in the predictive performances come solely from each method's ability to predict who acts. We additionally performed this experiment without the use of regulatory features and found that the logistic regression method achieved a relative loss of \u22120.027300. Using a paired comparison between the two methods, we note that this difference of 0.004443 is significant with error 0.001886. This echoes Suzuki's conclusions the regulatory environment in this industry affect firms' decisions to build new hotels [Suzuki, 2010], measured here by improvements in predictive performance.\nIn the right of Figure 6, we demonstrate the test log loss conditioned on at least one firm acting-the portion of the loss that differentiates the methods. The logistic regression method with only utility features performs Figure 6: (Left) Test log loss on the full outcome space relative to the smoothed multinomial, which has log loss 1.58234 \u00b1 0.058088. (Center) Test log loss no build vs. build outcomes only. Loss is relative multinomial, with log loss 0.721466 \u00b1 0.016539. (Right) Test log loss conditioned on build outcomes only. Loss is relative multinomial, with log loss 6.5911 \u00b1 0.116231. the worst with a 1.8% improvement over the multinomial base line, the individual inverse optimal control method improves by 4.1% and MaxEnt ICE performs the best with a 6.3% improvement. That is, the addition of regret features, and hence accounting for the strategic aspects of the game, have a significant effect on the predictive performance in this setting. We note that replacing the regulatory features in the regret portion of the MaxEnt ICE model actually slightly improves performance to \u22120.471763, though not by a significant margin. This implies that the regulatory features have little or no bearing on predicting exactly the firm that will act, which suggests the regulatory practices are unbiased.", "publication_ref": ["b29", "b21", "b29"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Conclusion", "text": "In this article, we develop a novel approach to behavior prediction in strategic multi-agent domains. We demonstrate that by leveraging a rationality assumption and the principle of maximum entropy our method can be efficiently implemented while achieving good statistical performance. Empirically, we displayed the effectiveness of our approach on two market entry data sets. We demonstrated both the robustness of our approach to errors in our assumption as well as the importance of considering strategic interactions.\nOur future work will consider two new directions. First, we will address classes of games where the action sets and players differ. A key benefit of our current approach is that it enables these to differ between training and testing which we only leverage modestly in the transfer experiments for route prediction. This will involve investigating from a statistical point of view novel notions of a deviation and their corresponding equilibrium concepts Second, we will consider different models of interactions, such as stochastic games and extensive-form games. These models, though no more expressive than matrix games, can often represent interactions exponentially more succinctly. From a practical standpoint, this avenue of research will allow for the application of our methods to a broader class of problems, including, for instance, exploring the time series dependencies within the Texas Hotel Chain data.\nProof. Assume that for all w \u2208 K, r \u0393\nf (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 f (\u03c3 \u0393 |w), \u2203\u03b7 f \u2208 \u2206 \u03a6 f such that r \u0393 f (\u03c3 \u0393 |w) \u2264 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) Since 0 \u2208 K, max w\u2208K r \u0393 f (\u03c3 \u0393 |w) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) \u2264 0 = r \u0393 f (\u03c3 \u0393 |0) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |0) = max w\u2208K,t r \u0393\nf (\u03c3 \u0393 |w) \u2212 t, subject to: t \u2265 r \u0393 g (\u03c3 \u0393 |w), \u2200g \u2208 \u03a6 f . By Slater's condition, strong duality holds and the resulting dual is the feasibility problem = min\n\u03b7 f \u2208\u2206 \u03a6 f 0, subject to: r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) \u2208 \u2212K * .\nAssume \u2203\u03b7 f \u2208 \u2206 \u03a6 f and y \u2208 K * such that r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) + y = 0, then for any w \u2208 K r \u0393 f (\u03c3 \u0393 |w) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) + y, w = 0, w = 0. By the definition of the dual cone y, w \u2265 0, therefore\nr \u0393 f (\u03c3 \u0393 |w) \u2264 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) \u2264 max g\u2208\u03a6 f r \u0393 g (\u03c3 \u0393 |w) = Regret \u0393 \u03a6 f (\u03c3 \u0393 |w).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 4", "text": "The proof of Theorem 4 immediately follows from the following lemma.\nLemma 5. If joint strategy \u03c3 \u0393 has \u03b5 external regret, and \u0393 is 2-player and constant-sum with respect to w, then the marginal strategies form a 2\u03b5-Nash equilibrium under utility function w.\nProof. Denote one player x and the other y and their marginal strategies as \u03c3 \u0393\nx and\u03c3 \u0393 y respectively. We are given\n\u2200\u03c3 \u0393 x \u2208 \u2206 Ax , u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) \u2212 u \u0393 x (\u03c3 \u0393 |w) \u2264 \u03b5 and, \u2200\u03c3 \u0393 y \u2208 \u2206 Ay , u \u0393 y (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 u \u0393 y (\u03c3 \u0393 |w) \u2264 \u03b5\nas when either player deviates, the other resorts to playing his marginal strategy. Substituting\u03c3 \u0393 y for \u03c3 \u0393 y and summing, we get \u2200\u03c3\n\u0393 x \u2208 \u2206 Ax u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) + u \u0393 y (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 u \u0393 y (\u03c3 \u0393 |w) + u \u0393 x (\u03c3 \u0393 |w) \u2264 2\u03b5 u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) + u \u0393 y (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 C \u2264 2\u03b5 u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) + C \u2212 u \u0393 x (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 C \u2264 2\u03b5 u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) \u2212 u \u0393 x (\u03c3 \u0393 y ,\u03c3 \u0393\nx |w) \u2264 2\u03b5 A symmetric argument shows the equivalent statement for the opposing player.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 7", "text": "Proof. The Legrange dual function,\nL(\u03b8, \u03b1, \u03b2, u, v, x) = max \u03c3 \u0393 ,\u03b7,y \u2212 a\u2208A\u03c3 \u0393 (a) log\u03c3 \u0393 (a)+ f \u2208\u03a6 \u03b1 f \uf8eb \uf8ed 1 \u2212 g\u2208\u03a6 \u03b7 f (g) \uf8f6 \uf8f8 + \u03b2 1 \u2212 a\u2208A\u03c3 \u0393 (a) + f \u2208\u03a6 r \u0393 f (\u03c3 \u0393 ) \u2212 f,g\u2208\u03a6 \u03b7 f (g)r \u0393 g (\u03c3 \u0393 ) + y f , \u03b8 f + f \u2208\u03a6 v f \u2022 \u03b7 f + u \u2022\u03c3 \u0393 + y f , x f ,\nis an upper bound on the primal objective for all u, v \u2265 0 and x \u2208 K * * . We solve the unconstrained maximization by setting the derivatives with respect to\u03c3 \u0393 , \u03b7, y to zero, \nlog\u03c3 \u0393 (a) = u(a) \u2212 1 \u2212 \u03b2 \u2212 f \u2208\u03a6 r \u0393 f (a), \u03b8 f r \u0393 g (\u03c3 \u0393 ), \u03b8 f \u2212 \u03b1 f + v f (g) = 0 \u2200f, g \u2208 \u03a6 \u03b8 f = \u2212x", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 9", "text": "Proof. Let {e 1 , e 2 , . . . , e K } be an orthonormal basis for V , where K = dim V . We first bound how well the regrets match in each basis direction.\nP max f \u2208\u03a6,k\u2208[K] |r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2265 \u2206 \u2264 P f \u2208\u03a6,k\u2208[K] |r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2265 \u2206 \u2264 f \u2208\u03a6,k\u2208[K] P |r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2265 \u2206 \u2264 2|\u03a6|K exp \u22122T 2 \u2264 \u03b4. T \u2265 1 2 2 log 2|\u03a6| dim V \u03b4 .\nNext, we bound how well the regrets match under w, given all regrets are close.\n|r\u03be f (\u03c3|w) \u2212 r \u03be f (\u03c3|w\n)| = | K k=1 \u03b1 k r\u03be f (\u03c3|e k ) \u2212 \u03b1 k r \u03be f (\u03c3|e k )| \u2264 K k=1 |\u03b1 k ||r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2264 \u2206 K k=1 |\u03b1 k | \u2264 \u2206 w 1 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 10", "text": "Proof. Unlike Theorem 9, we will bound the error of regrets directly.\nP max f \u2208\u03a6 r\u03be f (\u03c3|w) \u2212 r \u03be f (\u03c3|w) \u2265 \u2206(w) \u2264 P f \u2208\u03a6 r\u03be f (\u03c3|w) \u2212 r \u03be f (\u03c3|w) \u2265 \u2206(w)\n\u2264 f \u2208\u03a6 P r\u03be f (\u03c3|w) \u2212 r \u03be f (\u03c3|w) \u2265 \u2206(w) \u2264 2|\u03a6| exp \u22122T 2 \u2264 \u03b4 T \u2265 1 2 2 log |\u03a6| \u03b4 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work is supported by the ONR MURI grant N00014-09-1-1052 and by the National Sciences and Engineering Research Council of Canada (NSERC). The authors gratefully acknowledge Prof. Junichi Suzuki for providing the aggregated mid-scale hotel data and Alex Grubb for the application of density estimation code to the data-sets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Proof of Lemma 1", "text": "Proof. The lower bound holds as a consequence of \u03a6 int \u2286 \u03a6 swap .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Since max", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 3", "text": "The proof of Theorem 3 immediately follows from the following lemma.\nLemma 4. For any utility function", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Apprenticeship learning via inverse reinforcement learning", "journal": "", "year": "2004", "authors": "P Abbeel; A Y Ng"}, {"ref_id": "b1", "title": "Automobile prices in market equilibrium", "journal": "Econometrica", "year": "1995-07", "authors": "S Berry; J Levinsohn; A Pakes"}, {"ref_id": "b2", "title": "Algorithmic Game Theory, chapter Learning, Regret Minimization and Equilibria", "journal": "Cambridge University Press", "year": "2007", "authors": "A Blum; Y Mansour"}, {"ref_id": "b3", "title": "Experience weighted attraction learning in normal form games", "journal": "Econometrica", "year": "1999", "authors": "C Camerer; T H Ho"}, {"ref_id": "b4", "title": "Entry-deterring capacity in the texas lodging industry", "journal": "Journal of Economics and Management Strategy", "year": "2006", "authors": "M Conlin; V Kadiyali"}, {"ref_id": "b5", "title": "Inducing features of random fields", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2002", "authors": "S Della Pietra; V Della Pietra; J Lafferty"}, {"ref_id": "b6", "title": "Maximum entropy density estimation with generalized regularization and an application to species distribution modeling", "journal": "Journal of Machine Learning Research", "year": "2007", "authors": "M Dud\u00edk; S J Phillips; R E Schapire"}, {"ref_id": "b7", "title": "On adaptation, maximization and reinforcement learning among cognitive strategies", "journal": "Psychological Review", "year": "2005", "authors": "I Erev; G Barron"}, {"ref_id": "b8", "title": "Loss aversion, diminishing sensitivity, and the effect of experience on repeated decisions", "journal": "Journal of Behavioral Decision Making", "year": "2008", "authors": "I Erev; E Ert; E Yechiam"}, {"ref_id": "b9", "title": "A choice prediction competition for market entry games: An introduction", "journal": "Games", "year": "2010", "authors": "I Erev; E Ert; A E Roth"}, {"ref_id": "b10", "title": "Calibrated learning and correlated equilibrium", "journal": "Games and Economic Behavior", "year": "1996", "authors": "D Foster; R Vohra"}, {"ref_id": "b11", "title": "Game theory, maximum entropy, minimum discrepancy, and robust bayesian decision theory", "journal": "Annals of Statistics", "year": "2003", "authors": "P D Gr\u00fcnwald; A P Dawid"}, {"ref_id": "b12", "title": "A simple adaptive procedure leading to correlated equilibrium", "journal": "Econometrica", "year": "2000", "authors": "S Hart; A Mas-Colell"}, {"ref_id": "b13", "title": "Learning to navigate through crowded environments", "journal": "", "year": "2010", "authors": "P Henry; C Vollmer; B Ferris; D Fox"}, {"ref_id": "b14", "title": "Information theory and statistical mechanics", "journal": "Physical Review", "year": "1957-05", "authors": "E T Jaynes"}, {"ref_id": "b15", "title": "Correlated equilibria in graphical games", "journal": "", "year": "2003", "authors": "S Kakade; M Kearns; J Langford; L Ortiz"}, {"ref_id": "b16", "title": "The measurement of urban travel demand", "journal": "Journal of Public Economics", "year": "1974", "authors": "D Mcfadden"}, {"ref_id": "b17", "title": "Quantal response equilibria for normal form games", "journal": "Games and Economic Behavior", "year": "1995", "authors": "R Mckelvey; T Palfrey"}, {"ref_id": "b18", "title": "Effects of range of payoffs as a variable in risk taking", "journal": "Journal of Experimental Psychology", "year": "1960", "authors": "J Myers; E Sadler"}, {"ref_id": "b19", "title": "Measuring market power in the ready-to-eat cereal industry", "journal": "Econometrica", "year": "2001-03", "authors": "A Nevo"}, {"ref_id": "b20", "title": "Algorithms for inverse reinforcement learning", "journal": "", "year": "2000", "authors": "A Ng; S Russell"}, {"ref_id": "b21", "title": "Updating quasi-newton matrices with limited storage", "journal": "", "year": "1980", "authors": "J "}, {"ref_id": "b22", "title": "Maximum entropy correlated equilibrium", "journal": "", "year": "2007", "authors": "L E Ortiz; R E Shapire; S M Kakade"}, {"ref_id": "b23", "title": "A course in game theory", "journal": "The MIT press", "year": "1994", "authors": "M Osborne; A Rubinstein"}, {"ref_id": "b24", "title": "Quantifying the benefits of new products: The case of the minivan", "journal": "Journal of Political Economy", "year": "2002", "authors": "A Petrin"}, {"ref_id": "b25", "title": "Maximum margin planning", "journal": "", "year": "2006", "authors": "N Ratliff; J A Bagnell; M Zinkevich"}, {"ref_id": "b26", "title": "Learning to search: Functional gradient techniques for imitation learning", "journal": "Autonomous Robots", "year": "2009", "authors": "N D Ratliff; D Silver; J A Bagnell"}, {"ref_id": "b27", "title": "Minimization methods for non-differentiable functions", "journal": "Springer-Verlag New York, Inc", "year": "1985", "authors": "N Z Shor"}, {"ref_id": "b28", "title": "Learning from demonstration for autonomous navigation in complex unstructured terrain", "journal": "International Journal of Robotics Research", "year": "2010-10", "authors": "D Silver; J A Bagnell; A Stentz"}, {"ref_id": "b29", "title": "Land use regulation as a barrier to entry: Evidence from the texas lodging industry", "journal": "International Economic Review", "year": "2010", "authors": "J Suzuki"}, {"ref_id": "b30", "title": "Computational rationalization: The inverse equilibrium problem", "journal": "", "year": "2011", "authors": "K Waugh; B D Ziebart; J A Bagnell"}, {"ref_id": "b31", "title": "Computational rationalization: The inverse equilibrium problem. arXiv, abs/1103", "journal": "", "year": "2011", "authors": "K Waugh; B D Ziebart; J A Bagnell"}, {"ref_id": "b32", "title": "Maximum entropy inverse reinforcement learning", "journal": "", "year": "2008", "authors": "B D Ziebart; A Maas; J A Bagnell; A Dey"}, {"ref_id": "b33", "title": "Navigate like a cabbie: Probabilistic reasoning from observed context-aware behavior", "journal": "", "year": "2008", "authors": "B D Ziebart; A Maas; A K Dey; J A Bagnell"}, {"ref_id": "b34", "title": "Modeling interaction via the principle of maximum causal entropy", "journal": "", "year": "2010", "authors": "B D Ziebart; J A Bagnell; A K Dey"}, {"ref_id": "b35", "title": "Probabilistic pointing target prediction via inverse optimal control", "journal": "", "year": "2012", "authors": "B D Ziebart; A K Dey; J A Bagnell"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Prediction error (log loss) as a function of number of observations in the synthetic routing game.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Test log loss using only the game's expected utility as a feature in the market entry experiment.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Regulatory index values for select counties in Texas. Blue means little regulation and lower costs to enter the market. Red means higher costs.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "\u03b2 subject to:r \u0393 g (\u03c3 \u0393 ), \u03b8 f \u2264 \u03b1 f . \u2200f, g \u2208 \u03a6Solving for \u03b2 explicitly we get \u03b2 = log Z \u0393 (\u03b8) \u2212 1, and moving the constraint into the objective gives our result:min \u03b8\u2208K * * f \u2208\u03a6 Regret \u0393 \u03a6 (\u03c3 \u0393 |\u03b8 f ) + log Z \u0393 (\u03b8).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Transfer error (log loss) on unobserved games.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "let v 1 = K k=1 |\u03b1 k | where v = K k=1 \u03b1 k e k .", "formula_coordinates": [6.0, 190.24, 373.83, 207.29, 15.24]}, {"formula_id": "formula_1", "formula_text": "Definition 1. A vector-valued normal-form game is a tuple \u0393 = (N , A, u \u0393 i ) where", "formula_coordinates": [6.0, 125.8, 542.06, 376.3, 25.07]}, {"formula_id": "formula_2", "formula_text": "u \u0393 i (a|w) = u \u0393 i (a), w .", "formula_coordinates": [7.0, 253.77, 237.02, 102.71, 14.19]}, {"formula_id": "formula_3", "formula_text": "switch x\u2192y i (a i ) = y if a i = x a i otherwise,", "formula_coordinates": [7.0, 221.69, 644.1, 160.69, 24.18]}, {"formula_id": "formula_4", "formula_text": "r \u0393 f i (a) = u \u0393 i (f i (a)) \u2212 u \u0393 i (a),", "formula_coordinates": [8.0, 241.33, 320.22, 127.58, 15.1]}, {"formula_id": "formula_5", "formula_text": "r \u0393 f i (a|w) = u \u0393 i (f i (a)|w) \u2212 u \u0393 i (a|w) = r \u0393 f i (a), w .", "formula_coordinates": [8.0, 193.19, 365.21, 223.87, 15.1]}, {"formula_id": "formula_6", "formula_text": "r \u0393 f (\u03c3 \u0393 ) = E a\u223c\u03c3 \u0393 r \u0393 f (a) ,", "formula_coordinates": [8.0, 246.9, 532.14, 116.46, 14.27]}, {"formula_id": "formula_7", "formula_text": "r \u0393 f (\u03c3 \u0393 |w) = E a\u223c\u03c3 \u0393 r \u0393 f (a|w) = r \u0393 f (\u03c3 \u0393 ), w .", "formula_coordinates": [8.0, 202.73, 577.13, 204.8, 14.27]}, {"formula_id": "formula_8", "formula_text": "Regret \u0393 \u03a6 (\u03c3 \u0393 |w) = max f \u2208\u03a6 r \u0393 f (\u03c3 \u0393 |w),", "formula_coordinates": [8.0, 228.59, 649.22, 153.08, 18.85]}, {"formula_id": "formula_9", "formula_text": "Regret \u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 \u03b5.", "formula_coordinates": [9.0, 258.62, 154.01, 93.0, 14.19]}, {"formula_id": "formula_10", "formula_text": "Regret \u0393 \u03a6 int (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 swap (\u03c3 \u0393 |w) \u2264 A \u2022 Regret \u0393 \u03a6 int (\u03c3 \u0393 |w).", "formula_coordinates": [9.0, 150.24, 316.74, 309.77, 18.15]}, {"formula_id": "formula_11", "formula_text": "Regret \u0393 \u03a6 (\u03c3 \u0393 |w * ) < Regret \u0393 \u03a6 (\u03c3 \u0393 |w * ).", "formula_coordinates": [10.0, 221.26, 230.36, 167.72, 14.19]}, {"formula_id": "formula_12", "formula_text": "\u2200w \u2208 V, Regret \u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 (\u03c3 \u0393 |w).", "formula_coordinates": [10.0, 203.31, 399.51, 203.64, 14.19]}, {"formula_id": "formula_13", "formula_text": "\u0393 \u03a6 (\u03c3 \u0393 |w * ) \u2264 Regret \u0393 \u03a6 (\u03c3 \u0393 |w * ) \u2264 \u03b5.", "formula_coordinates": [10.0, 328.79, 632.32, 155.66, 14.18]}, {"formula_id": "formula_14", "formula_text": "Definition 2 (Standard ICE Polytope). r \u0393 f (\u03c3 \u0393 ) = E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .", "formula_coordinates": [11.0, 125.8, 367.98, 472.96, 71.21]}, {"formula_id": "formula_15", "formula_text": "\u2200w \u2208 K, Regret \u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 (\u03c3 \u0393 |w).", "formula_coordinates": [12.0, 202.56, 168.85, 205.14, 14.19]}, {"formula_id": "formula_16", "formula_text": "Definition 3 (K-ICE Polytope). r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) \u2208 \u2212K * , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .", "formula_coordinates": [12.0, 125.8, 231.89, 421.11, 71.21]}, {"formula_id": "formula_17", "formula_text": "Definition 4 (Positive ICE Polytope). r \u0393 f (\u03c3 \u0393 ) \u2264 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .", "formula_coordinates": [12.0, 125.8, 506.38, 472.96, 71.21]}, {"formula_id": "formula_18", "formula_text": "i\u2208N u \u0393 i (a|w * ) = C.", "formula_coordinates": [13.0, 262.05, 337.97, 86.16, 24.76]}, {"formula_id": "formula_19", "formula_text": "\u2200w \u2208 V, i \u2208 N , u \u0393 i (\u03c3 \u0393 |w) = u \u0393 i (\u03c3 \u0393 |w).", "formula_coordinates": [13.0, 213.34, 655.15, 183.57, 14.19]}, {"formula_id": "formula_20", "formula_text": "i \u2208 N , u \u0393 i (\u03c3 \u0393 ) = u \u0393 i (\u03c3 \u0393 ).", "formula_coordinates": [14.0, 244.13, 245.56, 121.99, 14.19]}, {"formula_id": "formula_21", "formula_text": "Regret \u0393 \u03a6 (\u03c3 \u0393 |w) = Regret \u0393 \u03a6 (\u03c3 \u0393 |w).", "formula_coordinates": [14.0, 226.0, 348.78, 158.26, 14.19]}, {"formula_id": "formula_22", "formula_text": "H \u0393 (\u03c3 \u0393 ) = E a\u223c\u03c3 \u0393 \u2212 log \u03c3 \u0393 (a) ,", "formula_coordinates": [15.0, 230.73, 192.05, 148.8, 14.09]}, {"formula_id": "formula_23", "formula_text": "\u03c3 MaxEnt = argmax \u03c3 \u0393 \u2208\u2206 A H \u0393 (\u03c3 \u0393 ), subject to: g(\u03c3 \u0393 ) = 0 and h(\u03c3 \u0393 ) \u2264 0.", "formula_coordinates": [15.0, 209.32, 260.38, 191.62, 38.72]}, {"formula_id": "formula_24", "formula_text": "maximiz\u00ea \u03c3 \u0393 ,\u03b7 H \u0393 (\u03c3 \u0393 ) subject to: r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) \u2208 \u2212K * , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A .", "formula_coordinates": [15.0, 161.48, 537.22, 449.45, 73.92]}, {"formula_id": "formula_25", "formula_text": "minimize \u03b8 f \u2208K * * f \u2208\u03a6 Regret \u0393 \u03a6 f (\u03c3 \u0393 |\u03b8 f ) + log Z \u0393 (\u03b8),where", "formula_coordinates": [16.0, 185.21, 355.26, 239.83, 24.85]}, {"formula_id": "formula_26", "formula_text": "Z \u0393 (\u03b8) = a\u2208A exp \uf8eb \uf8ed \u2212 f \u2208\u03a6 r \u0393 f (a|\u03b8 f ) \uf8f6 \uf8f8 .", "formula_coordinates": [16.0, 200.32, 385.18, 171.31, 37.26]}, {"formula_id": "formula_27", "formula_text": "strategy\u03c3 \u0393 is\u03c3 \u0393 (a) \u221d exp \uf8eb \uf8ed \u2212 f \u2208\u03a6 r \u0393 f (a|\u03b8 * f ) \uf8f6 \uf8f8 .(1)", "formula_coordinates": [16.0, 125.8, 535.73, 358.66, 59.22]}, {"formula_id": "formula_28", "formula_text": "for f \u2208 \u03a6 do f * \u2190 argmax f \u2208\u03a6 f r \u0393 f (\u03c3 \u0393 |\u03b8 f ) g f \u2190 r \u0393 f * (\u03c3 \u0393 ) \u2212 r \u0393 f (\u03c3 \u0393 ) end for return g", "formula_coordinates": [17.0, 136.71, 175.67, 148.64, 65.93]}, {"formula_id": "formula_29", "formula_text": "Regret \u03be \u03a6 (\u03c3|w) = max f \u2208\u03a6 r \u03be f (\u03c3|w),", "formula_coordinates": [18.0, 234.7, 461.07, 140.86, 19.65]}, {"formula_id": "formula_30", "formula_text": "Definition 6 (Conditional K-ICE Polytope). r \u03be f (\u03c3) \u2212 E g\u223c\u03b7 f r \u03be g (\u03c3) \u2208 \u2212K * , \u2200f \u2208 \u03a6 \u03b7 f \u2208 \u2206 \u03a6 f , \u2200f \u2208 \u03a6 \u03c3 \u0393 \u2208 \u2206 A . \u2200\u0393 \u2208 G", "formula_coordinates": [18.0, 125.8, 537.29, 421.1, 73.32]}, {"formula_id": "formula_31", "formula_text": "H \u03be (\u03c3) = E \u0393\u223c\u03be H \u0393 (\u03c3 \u0393 ) .", "formula_coordinates": [19.0, 245.02, 167.6, 120.22, 13.27]}, {"formula_id": "formula_32", "formula_text": "minimize \u03b8 f \u2208K * * f \u2208\u03a6 Regret \u03be \u03a6 f (\u03c3|\u03b8 f ) + E \u0393\u223c\u03be log Z \u0393 (\u03b8) .", "formula_coordinates": [19.0, 186.21, 276.45, 237.83, 25.65]}, {"formula_id": "formula_33", "formula_text": "\u2200w \u2208 V, Regret\u0393 \u03a6 (\u03c3 \u0393 |w) \u2264 \u03ba Regret \u0393 \u03a6 (\u03c3 \u0393 |w).", "formula_coordinates": [20.0, 201.08, 250.81, 208.1, 14.19]}, {"formula_id": "formula_34", "formula_text": "r\u03be f (\u03c3|w) \u2248 1 T T t=1 r \u0393 t f (a t |w).", "formula_coordinates": [20.0, 241.54, 533.24, 127.16, 33.58]}, {"formula_id": "formula_35", "formula_text": "1 2 2 log 2|\u03a6| dim V \u03b4 outcomes we have for all deviations r\u03be f (\u03c3|w) \u2264 r \u03be f (\u03c3|w) + \u2206 w 1 .", "formula_coordinates": [21.0, 126.99, 181.86, 357.46, 27.55]}, {"formula_id": "formula_36", "formula_text": "Theorem 11. If for all f , r\u03be f (\u03c3|w) \u2264 r \u03be f (\u03c3|w) + \u03b3, then Regret\u03be \u03a6 (\u03c3|w) \u2264 Regret \u03be \u03a6 (\u03c3|w) + \u03b3.", "formula_coordinates": [21.0, 125.8, 425.68, 358.66, 32.64]}, {"formula_id": "formula_37", "formula_text": "f (\u03c3 \u0393 |w) \u2264 Regret \u0393 \u03a6 f (\u03c3 \u0393 |w), \u2203\u03b7 f \u2208 \u2206 \u03a6 f such that r \u0393 f (\u03c3 \u0393 |w) \u2264 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) Since 0 \u2208 K, max w\u2208K r \u0393 f (\u03c3 \u0393 |w) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) \u2264 0 = r \u0393 f (\u03c3 \u0393 |0) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |0) = max w\u2208K,t r \u0393", "formula_coordinates": [32.0, 125.8, 131.0, 356.97, 130.74]}, {"formula_id": "formula_38", "formula_text": "\u03b7 f \u2208\u2206 \u03a6 f 0, subject to: r \u0393 f (\u03c3 \u0393 ) \u2212 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 ) \u2208 \u2212K * .", "formula_coordinates": [32.0, 201.44, 294.54, 261.52, 21.8]}, {"formula_id": "formula_39", "formula_text": "r \u0393 f (\u03c3 \u0393 |w) \u2264 E g\u223c\u03b7 f r \u0393 g (\u03c3 \u0393 |w) \u2264 max g\u2208\u03a6 f r \u0393 g (\u03c3 \u0393 |w) = Regret \u0393 \u03a6 f (\u03c3 \u0393 |w).", "formula_coordinates": [32.0, 143.85, 384.04, 312.18, 19.78]}, {"formula_id": "formula_40", "formula_text": "\u2200\u03c3 \u0393 x \u2208 \u2206 Ax , u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) \u2212 u \u0393 x (\u03c3 \u0393 |w) \u2264 \u03b5 and, \u2200\u03c3 \u0393 y \u2208 \u2206 Ay , u \u0393 y (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 u \u0393 y (\u03c3 \u0393 |w) \u2264 \u03b5", "formula_coordinates": [32.0, 189.75, 575.92, 230.76, 32.37]}, {"formula_id": "formula_41", "formula_text": "\u0393 x \u2208 \u2206 Ax u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) + u \u0393 y (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 u \u0393 y (\u03c3 \u0393 |w) + u \u0393 x (\u03c3 \u0393 |w) \u2264 2\u03b5 u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) + u \u0393 y (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 C \u2264 2\u03b5 u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) + C \u2212 u \u0393 x (\u03c3 \u0393 y ,\u03c3 \u0393 x |w) \u2212 C \u2264 2\u03b5 u \u0393 x (\u03c3 \u0393 x ,\u03c3 \u0393 y |w) \u2212 u \u0393 x (\u03c3 \u0393 y ,\u03c3 \u0393", "formula_coordinates": [33.0, 162.65, 144.89, 284.95, 89.8]}, {"formula_id": "formula_42", "formula_text": "L(\u03b8, \u03b1, \u03b2, u, v, x) = max \u03c3 \u0393 ,\u03b7,y \u2212 a\u2208A\u03c3 \u0393 (a) log\u03c3 \u0393 (a)+ f \u2208\u03a6 \u03b1 f \uf8eb \uf8ed 1 \u2212 g\u2208\u03a6 \u03b7 f (g) \uf8f6 \uf8f8 + \u03b2 1 \u2212 a\u2208A\u03c3 \u0393 (a) + f \u2208\u03a6 r \u0393 f (\u03c3 \u0393 ) \u2212 f,g\u2208\u03a6 \u03b7 f (g)r \u0393 g (\u03c3 \u0393 ) + y f , \u03b8 f + f \u2208\u03a6 v f \u2022 \u03b7 f + u \u2022\u03c3 \u0393 + y f , x f ,", "formula_coordinates": [33.0, 129.67, 324.1, 350.9, 144.38]}, {"formula_id": "formula_43", "formula_text": "log\u03c3 \u0393 (a) = u(a) \u2212 1 \u2212 \u03b2 \u2212 f \u2208\u03a6 r \u0393 f (a), \u03b8 f r \u0393 g (\u03c3 \u0393 ), \u03b8 f \u2212 \u03b1 f + v f (g) = 0 \u2200f, g \u2208 \u03a6 \u03b8 f = \u2212x", "formula_coordinates": [33.0, 144.11, 522.8, 326.27, 60.92]}, {"formula_id": "formula_44", "formula_text": "P max f \u2208\u03a6,k\u2208[K] |r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2265 \u2206 \u2264 P f \u2208\u03a6,k\u2208[K] |r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2265 \u2206 \u2264 f \u2208\u03a6,k\u2208[K] P |r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2265 \u2206 \u2264 2|\u03a6|K exp \u22122T 2 \u2264 \u03b4. T \u2265 1 2 2 log 2|\u03a6| dim V \u03b4 .", "formula_coordinates": [34.0, 125.8, 294.57, 392.53, 137.18]}, {"formula_id": "formula_45", "formula_text": ")| = | K k=1 \u03b1 k r\u03be f (\u03c3|e k ) \u2212 \u03b1 k r \u03be f (\u03c3|e k )| \u2264 K k=1 |\u03b1 k ||r\u03be f (\u03c3|e k ) \u2212 r \u03be f (\u03c3|e k )| \u2264 \u2206 K k=1 |\u03b1 k | \u2264 \u2206 w 1 .", "formula_coordinates": [34.0, 305.11, 481.74, 164.7, 127.38]}, {"formula_id": "formula_46", "formula_text": "\u2264 f \u2208\u03a6 P r\u03be f (\u03c3|w) \u2212 r \u03be f (\u03c3|w) \u2265 \u2206(w) \u2264 2|\u03a6| exp \u22122T 2 \u2264 \u03b4 T \u2265 1 2 2 log |\u03a6| \u03b4 .", "formula_coordinates": [35.0, 291.88, 224.7, 185.84, 88.13]}], "doi": ""}