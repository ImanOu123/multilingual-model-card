{"title": "Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild", "authors": "Shangzhe Wu; Christian Rupprecht; Andrea Vedaldi", "pub_date": "2020-03-31", "abstract": "3D reconstruction Textured Re-lighting Figure 1: Unsupervised learning of 3D deformable objects from in-the-wild images. Left: Training uses only single views of the object category with no additional supervision at all (i.e. no ground-truth 3D information, multiple views, or any prior model of the object). Right: Once trained, our model reconstructs the 3D pose, shape, albedo and illumination of a deformable object instance from a single image with excellent fidelity. Code and demo at https://github.com/elliottwu/unsup3d.", "sections": [{"heading": "Introduction", "text": "Understanding the 3D structure of images is key in many computer vision applications. Futhermore, while many deep networks appear to understand images as 2D textures [16], 3D modelling can explain away much of the variability of natural images and potentially improve image understanding in general. Motivated by these facts, we consider the problem of learning 3D models for deformable object categories.\nWe study this problem under two challenging conditions. The first condition is that no 2D or 3D ground truth information (such as keypoints, segmentation, depth maps, or prior knowledge of a 3D model) is available. Learning without external supervisions removes the bottleneck of collecting image annotations, which is often a major obstacle to deploying deep learning for new applications. The second condition is that the algorithm must use an unconstrained collection of single-view images -in particular, it should not require multiple views of the same instance. Learning from single-view images is useful because in many applications, and especially for deformable objects, we solely have a source of still images to work with. Consequently, our learning algorithm ingests a number of single-view images of a deformable object category and produces as output a deep network that can estimate the 3D shape of any instance given a single image of it (Fig. 1).\nWe formulate this as an autoencoder that internally decomposes the image into albedo, depth, illumination and viewpoint, without direct supervision for any of these factors.\nHowever, without further assumptions, decomposing images into these four factors is ill-posed. In search of minimal assumptions to achieve this, we note that many object categories are symmetric (e.g. almost all animals and many handcrafted objects). Assuming an object is perfectly symmetric, one can obtain a virtual second view of it by simply mirroring the image. In fact, if correspondences between the pair of mirrored images were available, 3D reconstruction could be achieved by stereo reconstruction [41,12,60,54,14]. Motivated by this, we seek to leverage symmetry as a geometric cue to constrain the decomposition.\nHowever, specific object instances are in practice never fully symmetric, neither in shape nor appearance. Shape is non-symmetric due to variations in pose or other details (e.g. hair style or expressions on a human face), and albedo can also be non-symmetric (e.g. asymmetric texture of cat faces). Even when both shape and albedo are symmetric, the appearance may still not be, due to asymmetric illumination.\nWe address this issue in two ways. First, we explicitly model illumination to exploit the underlying symmetry and show that, by doing so, the model can exploit illumination as an additional cue for recovering the shape. Second, we augment the model to reason about potential lack of symmetry in the objects. To do this, the model predicts, along with the other factors, a dense map containing the probability that a given pixel has a symmetric counterpart in the image.\nWe combine these elements in an end-to-end learning formulation, where all components, including the confidence maps, are learned from raw RGB data only. We also show that symmetry can be enforced by flipping internal representations, which is particularly useful for reasoning about symmetries probabilistically.\nWe demonstrate our method on several datasets, including human faces, cat faces and cars. We provide a thorough ablation study using a synthetic face dataset to obtain the necessary 3D ground truth. On real images, we achieve higher fidelity reconstruction results compared to other methods [49,56] that do not rely on 2D or 3D ground truth information, nor prior knowledge of a 3D model of the instance or class. In addition, we also outperform a recent state-ofthe-art method [40] that uses keypoint supervision for 3D reconstruction on real faces, while our method uses no external supervision at all. Finally, we demonstrate that our trained face model generalizes to non-natural images such as face paintings and cartoon drawings without fine-tuning.", "publication_ref": ["b15", "b40", "b11", "b59", "b53", "b13", "b48", "b55", "b39"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Related Work", "text": "In order to assess our contribution in relation to the vast literature on image-based 3D reconstruction, it is important to consider three aspects of each approach: which information is used, which assumptions are made, and what the output is. Below and in Table 1   Our method uses single-view images of an object category as training data, assumes that the objects belong to a specific class (e.g. human faces) which is weakly symmetric, and outputs a monocular predictor capable of decomposing any image of the category into shape, albedo, illumination, viewpoint and symmetry probability.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Structure from Motion.", "text": "Traditional methods such as Structure from Motion (SfM) [11] can reconstruct the 3D structure of individual rigid scenes given as input multiple views of each scene and 2D keypoint matches between the views. This can be extended in two ways. First, monocular reconstruction methods can perform dense 3D reconstruction from a single image without 2D keypoints [74,62,20]. However, they require multiple views [20] or videos of rigid scenes for training [74]. Second, Non-Rigid SfM (NRSfM) approaches [4,44] can learn to reconstruct deformable objects by allowing 3D points to deform in a limited manner between views, but require supervision in terms of annotated 2D keypoints for both training and testing. Hence, neither family of SfM approaches can learn to reconstruct deformable objects from raw pixels of a single view.\nShape from X. Many other monocular cues have been used as alternatives or supplements to SfM for recovering shape from images, such as shading [25,71], silhouettes [33], texture [65], symmetry [41,12] etc. In particular, our work is inspired from shape from symmetry and shape from shading. Shape from symmetry [41,12,60,54] reconstructs symmetric objects from a single image by using the mirrored image as a virtual second view, provided that symmetric correspondences are available. [54] also shows that it is possible to detect symmetries and correspondences using descriptors. Shape from shading [25,71] assumes a shading model such as Lambertian reflectance, and reconstructs the surface by exploiting the non-uniform illumination.\nCategory-specific reconstruction. Learning-based methods have recently been leveraged to reconstruct objects from a single view, either in the form of a raw image or 2D keypoints (see also Table 1). While this task is ill-posed, it has been shown to be solvable by learning a suitable object prior from the training data [47,66,1,48]. A variety of supervisory signals have been proposed to learn such priors. Besides using 3D ground truth directly, authors have considered using videos [2,74,43,63] and stereo pairs [20,38]. Other approaches have used single views with 2D keypoint annotations [29,40,55,6] or object masks [29,7]. For objects such as human bodies and human faces, some methods [28,18,64,15] have learn to reconstruct from raw images, but starting from the knowledge of a predefined shape model such as SMPL [36] or Basel [47]. These prior models are constructed using specialized hardware and/or other forms of supervision, which are often difficult to obtain for deformable objects in the wild, such as animals, and also limited in details of the shape.\nOnly recently have authors attempted to learn the geometry of object categories from raw, monocular views only. Thewlis et al. [58,59] uses equivariance to learn dense landmarks, which recovers the 2D geometry of the objects. DAE [52] learns to predict a deformation field through heavily constraining an autoencoder with a small bottleneck embedding and lift that to 3D in [49] -in post processing, they further decompose the reconstruction in albedo and shading, obtaining an output similar to ours.\nAdversarial learning has been proposed as a way of hallucinating new views of an object. Some of these methods start from 3D representations [66,1,75,48]. Kato et al. [30] trains a discriminator on raw images but uses viewpoint as addition supervision. HoloGAN [42] only uses raw images but does not obtain an explicit 3D reconstruction. Szabo et al. [56] uses adversarial training to reconstruct 3D meshes of the object, but does not assess their results quantitatively. Henzler et al. [23] also learns from raw images, but only experiments with images that contain the object on a white background, which is akin to supervision with 2D silhouettes. In Section 4.3, we compare to [49,56] and demonstrate superior reconstruction results with much higher fidelity.\nSince our model generates images from an internal 3D representation, one essential component is a differentiable renderer. However, with a traditional rendering pipeline,  gradients across occlusions and boundaries are not defined. Several soft relaxations have thus been proposed [37,31,34]. Here, we use an implementation 1 of [31].", "publication_ref": ["b10", "b73", "b61", "b19", "b19", "b73", "b3", "b43", "b24", "b70", "b32", "b64", "b40", "b11", "b40", "b11", "b59", "b53", "b53", "b24", "b70", "b46", "b65", "b0", "b47", "b1", "b73", "b42", "b62", "b19", "b37", "b28", "b39", "b54", "b5", "b28", "b6", "b27", "b17", "b63", "b14", "b35", "b46", "b57", "b58", "b51", "b48", "b65", "b0", "b74", "b47", "b29", "b41", "b55", "b22", "b48", "b55", "b36", "b30", "b33", "b0", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Method", "text": "Given an unconstrained collection of images of an object category, such as human faces, our goal is to learn a model \u03a6 that receives as input an image of an object instance and produces as output a decomposition of it into 3D shape, albedo, illumination and viewpoint, as illustrated in Fig. 2.\nAs we have only raw images to learn from, the learning objective is reconstructive: namely, the model is trained so that the combination of the four factors gives back the input image. This results in an autoencoding pipeline where the factors have, due to the way they are recomposed, an explicit photo-geometric meaning.\nIn order to learn such a decomposition without supervision for any of the components, we use the fact that many object categories are bilaterally symmetric. However, the appearance of object instances is never perfectly symmetric. Asymmetries arise from shape deformation, asymmetric albedo and asymmetric illumination. We take two measures to account for these asymmetries. First, we explicitly model asymmetric illumination. Second, our model also estimates, for each pixel in the input image, a confidence score that explains the probability of the pixel having a symmetric counterpart in the image (see conf \u03c3, \u03c3 in Fig. 2).\nThe following sections describe how this is done, looking first at the photo-geometric autoencoder (Section 3.1), then at how symmetries are modelled (Section 3.2), followed by details of the image formation (Section 3.3) and the supplementary perceptual loss (Section 3.4).", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Photo-geometric autoencoding", "text": "An image I is a function \u2126 \u2192 R 3 defined on a grid \u2126 = {0, . . . , W \u2212 1} \u00d7 {0, . . . , H \u2212 1}, or, equivalently, a tensor in R 3\u00d7W \u00d7H . We assume that the image is roughly centered on an instance of the object of interest. The goal is to learn a function \u03a6, implemented as a neural network, that maps the image I to four factors (d, a, w, l) comprising a depth map d : \u2126 \u2192 R + , an albedo image a : \u2126 \u2192 R 3 , a global light direction l \u2208 S 2 , and a viewpoint w \u2208 R 6 so that the image can be reconstructed from them.\nThe image I is reconstructed from the four factors in two steps, lighting \u039b and reprojection \u03a0, as follows:\nI = \u03a0 (\u039b(a, d, l), d, w) .\n(\n)1\nThe Discussion. The effect of lighting could be incorporated in the albedo a by interpreting the latter as a texture rather than as the object's albedo. However, there are two good reasons to avoid this. First, the albedo a is often symmetric even if the illumination causes the corresponding appearance to look asymmetric. Separating them allows us to more effectively incorporate the symmetry constraint described below. Second, shading provides an additional cue on the underlying 3D shape [24,3]. In particular, unlike the recent work of [52] where a shading map is predicted independently from shape, our model computes the shading based on the predicted depth, mutually constraining each other.", "publication_ref": ["b23", "b2", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "Probably symmetric objects", "text": "Leveraging symmetry for 3D reconstruction requires identifying symmetric object points in an image. Here we do so implicitly, assuming that depth and albedo, which are reconstructed in a canonical frame, are symmetric about a fixed vertical plane. An important beneficial side effect of this choice is that it helps the model discover a 'canonical view' for the object, which is important for reconstruction [44].\nTo do this, we consider the operator that flips a map a \u2208 R C\u00d7W \u00d7H along the horizontal axis 2 : [flip a] c,u,v = a c,W \u22121\u2212u,v . We then require d \u2248 flip d and a \u2248 flip a . While these constraints could be enforced by adding corresponding loss terms to the learning objective, they would be difficult to balance. Instead, we achieve the same effect indirectly, by obtaining a second reconstruction\u00ce from the flipped depth and albedo:\nI = \u03a0 (\u039b(a , d , l), d , w) , a = flip a, d = flip d. (2)\nThen, we consider two reconstruction losses encouraging I \u2248\u00ce and I \u2248\u00ce . Since the two losses are commensurate, they are easy to balance and train jointly. Most importantly, this approach allows us to easily reason about symmetry probabilistically, as explained next.\nThe source image I and the reconstruction\u00ce are compared via the loss:\nL(\u00ce, I, \u03c3) = \u2212 1 |\u2126| uv\u2208\u2126 ln 1 \u221a 2\u03c3 uv exp \u2212 \u221a 2 1,uv \u03c3 uv ,(3)\nwhere 1,uv = |\u00ce uv \u2212 I uv | is the L 1 distance between the intensity of pixels at location uv, and \u03c3 \u2208 R W \u00d7H + is a confidence map, also estimated by the network \u03a6 from the image I, which expresses the aleatoric uncertainty of the model. The loss can be interpreted as the negative log-likelihood of a factorized Laplacian distribution on the reconstruction residuals. Optimizing likelihood causes the model to selfcalibrate, learning a meaningful confidence map [32].\nModelling uncertainty is generally useful, but in our case is particularly important when we consider the \"symmetric\" reconstruction\u00ce , for which we use the same loss L(\u00ce , I, \u03c3 ). Crucially, we use the network to estimate, also from the same input image I, a second confidence map \u03c3 . This confidence map allows the model to learn which portions of the input image might not be symmetric. For instance, in some cases hair on a human face is not symmetric as shown in Fig. 2, and \u03c3 can assign a higher reconstruction uncertainty to the hair region where the symmetry assumption is not satisfied. Note that this depends on the specific instance under consideration, and is learned by the model itself.\nOverall, the learning objective is given by the combination of the two reconstruction errors:\nE(\u03a6; I) = L(\u00ce, I, \u03c3) + \u03bb f L(\u00ce , I, \u03c3 ),(4)\nwhere \u03bb f = 0.5 is a weighing factor, (d, a, w, l, \u03c3, \u03c3 ) = \u03a6(I) is the output of the neural network, and\u00ce and\u00ce are obtained according to Eqs. (1) and (2).", "publication_ref": ["b43", "b31"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Image formation model", "text": "We now describe the functions \u03a0 and \u039b in Eq. (1) in more detail. The image is formed by a camera looking at a 3D object. If we denote with P = (P x , P y , P z ) \u2208 R 3 a 3D point expressed in the reference frame of the camera, this is mapped to pixel p = (u, v, 1) by the following projection:\np \u221d KP, K = \uf8ee \uf8f0 f 0 c u 0 f c v 0 0 1 \uf8f9 \uf8fb , \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 c u = W \u22121 2 , c v = H\u22121 2 , f = W \u22121 2 tan \u03b8 FOV 2 .\n(\n)5\nThis model assumes a perspective camera with field of view (FOV) \u03b8 FOV . We assume a nominal distance of the object from the camera at about 1m. Given that the images are cropped around a particular object, we assume a relatively narrow FOV of \u03b8 FOV \u2248 10 \u2022 . The depth map d : \u2126 \u2192 R + associates a depth value d uv to each pixel (u, v) \u2208 \u2126 in the canonical view. By inverting the camera model ( 5), we find that this corresponds to the\n3D point P = d uv \u2022 K \u22121 p.\nThe viewpoint w \u2208 R 6 represents an Euclidean transformation (R, T ) \u2208 SE(3), where w 1:3 and w 4:6 are rotation angles and translations along x, y and z axes respectively.\nThe map (R, T ) transforms 3D points from the canonical view to the actual view. Thus a pixel (u, v) in the canonical view is mapped to the pixel (u , v ) in the actual view by the warping function \u03b7 d,w : (u, v) \u2192 (u , v ) given by:\np \u221d K(d uv \u2022 RK \u22121 p + T ),(6)\nwhere p = (u , v , 1). Finally, the reprojection function \u03a0 takes as input the depth d and the viewpoint change w and applies the resulting warp to the canonical image J to obtain the actual image\u00ce =\n\u03a0(J, d, w) as\u00ce u v = J uv , where (u, v) = \u03b7 \u22121 d,w (u , v ). 3\nThe canonical image J = \u039b(a, d, l) is in turn generated as a combination of albedo, normal map and light direction. To do so, given the depth map d, we derive the normal map n : \u2126 \u2192 S 2 by associating to each pixel (u, v) a vector normal to the underlying 3D surface. In order to find this vector, we compute the vectors t u uv and t v uv tangent to the surface along the u and v directions. For example, the first one is:\nt u uv = d u+1,v \u2022 K \u22121 (p + e x ) \u2212 d u\u22121,v \u2022 K \u22121 (p \u2212 e x )\nwhere p is defined above and e x = (1, 0, 0). Then the normal is obtained by taking the vector product n uv \u221d t u uv \u00d7 t v uv . The normal n uv is multiplied by the light direction l to obtain a value for the directional illumination and the latter is added to the ambient light. Finally, the result is multiplied by the albedo to obtain the illuminated texture, as follows:\nJ uv = (k s + k d max{0, l, n uv }) \u2022 a uv .\nHere k s and k d are the scalar coefficients weighting the ambient and diffuse terms, and are predicted by the model with range between 0 and 1 via rescaling a tanh output. The light direction l = (l x , l y , 1) T /(l 2\nx + l 2 y + 1) 0.5 is modeled as a spherical sector by predicting l x and l y with tanh.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Perceptual loss", "text": "The L 1 loss function Eq. (3) is sensitive to small geometric imperfections and tends to result in blurry reconstructions. We add a perceptual loss term to mitigate this problem. The k-th layer of an off-the-shelf image encoder e (VGG16 in our case [53]) predicts a representation e (k) \n(I) \u2208 R C k \u00d7W k \u00d7H k\nwhere \u2126 k = {0, . . . , W k \u22121}\u00d7{0, . . . , H k \u22121} is the corresponding spatial domain. Note that this feature encoder does not have to be trained with supervised tasks. Self-supervised encoders can be equally effective as shown in Table 3.\nSimilar to Eq. (3), assuming a Gaussian distribution, the perceptual loss is given by:\nL (k) p (\u00ce, I, \u03c3 (k) ) = \u2212 1 |\u2126 k | uv\u2208\u2126 k ln 1 2\u03c0(\u03c3 (k) uv ) 2 exp \u2212 ( (k) uv ) 2 2(\u03c3 (k) uv ) 2 ,(7)\nwhere\n(k) uv = |e (k) uv (\u00ce) \u2212 e (k)\nuv (I)| for each pixel index uv in the k-th layer. We also compute the loss for\u00ce using \u03c3 (k) . \u03c3 (k) and \u03c3 (k) are additional confidence maps predicted by our model. In practice, we found it is good enough for our purpose to use the features from only one layer relu3 3 of VGG16. We therefore shorten the notation of perceptual loss to L p . With this, the loss function L in Eq. ( 4) is replaced by L + \u03bb p L p with \u03bb p = 1.", "publication_ref": ["b52"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "Datasets. We test our method on three human face datasets: CelebA [35], 3DFAW [21,27,73,69] and BFM [47]. CelebA is a large scale human face dataset, consisting of over 200k images of real human faces in the wild annotated with bounding boxes. 3DFAW contains 23k images with 66 3D keypoint annotations, which we use to evaluate our 3D predictions in Section 4.3. We roughly crop the images around the head region and use the official train/val/test splits. BFM (Basel Face Model) is a synthetic face model, which we use to assess the quality of the 3D reconstructions (since the in-the-wild datasets lack groundtruth). We follow the protocol of [51] to generate a dataset, sampling shapes, poses, textures, and illumination randomly. We use images from SUN Database [68] as background and save ground truth depth maps for evaluation.\nWe also test our method on cat faces and synthetic cars. We use two cat datasets [72,46]. The first one has 10k cat images with nine keypoint annotations, and the second one is a collection of dog and cat images, containing 1.2k cat images with bounding box annotations. We combine the two datasets and crop the images around the cat heads. For cars, we render 35k images of synthetic cars from ShapeNet [5] with random viewpoints and illumination. We randomly split the images by 8:1:1 into train, validation and test sets.\nMetrics. Since the scale of 3D reconstruction from projective cameras is inherently ambiguous [11], we discount it in the evaluation. Specifically, given the depth map d predicted by our model in the canonical view, we warp it to a depth mapd in the actual view using the predicted viewpoint and compare the latter to the ground-truth depth map d * using the scale-invariant depth error (SIDE) [10] No Baseline SIDE (\u00d710 \u22122 ) \u2193 MAD (deg.  \nE SIDE (d, d * ) = ( 1 W H uv \u2206 2 uv \u2212 ( 1 W H uv \u2206 uv ) 2 ) 1 2\nwhere \u2206 uv = logd uv \u2212 log d * uv . We compare only valid depth pixel and erode the foreground mask by one pixel to discount rendering artefacts at object boundaries. Additionally, we report the mean angle deviation (MAD) between normals computed from ground truth depth and from the predicted depth, measuring how well the surface is captured.\nImplementation details. The function (d, a, w, l, \u03c3) = \u03a6(I) that preditcs depth, albedo, viewpoint, lighting, and confidence maps from the image I is implemented using individual neural networks. The depth and albedo are generated by encoder-decoder networks, while viewpoint and lighting are regressed using simple encoder networks. The encoder-decoders do not use skip connections because input and output images are not spatially aligned (since the output is in the canonical viewpoint). All four confidence maps are predicted using the same network, at different decoding layers for the photometric and perceptual losses since these are computed at different resolutions. The final activation function is tanh for depth, albedo, viewpoint and lighting and softplus for the confidence maps. The depth prediction is centered on the mean before tanh, as the global distance is estimated as part of the viewpoint. We do not use any special initialization for all predictions, except that two border pixels of the depth maps on both the left and the right are clamped at a maximal depth to avoid boundary issues.\nWe train using Adam over batches of 64 input images, resized to 64 \u00d7 64 pixels. The size of the output depth and albedo is also 64 \u00d7 64. We train for approximately 50k iterations. For visualization, depth maps are upsampled to 256. We include more details in Section 6.2.", "publication_ref": ["b34", "b20", "b26", "b72", "b68", "b46", "b50", "b67", "b71", "b45", "b4", "b10", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Comparison with baselines. Table 2 uses the BFM dataset to compare the depth reconstruction quality obtained by our method, a fully-supervised baseline and two baselines. The supervised baseline is a version of our model trained to regress the ground-truth depth maps using an L 1 loss. The trivial baseline predicts a constant uniform depth map, which provides a performance lower-bound. The third baseline is a constant depth map obtained by averaging all ground-truth depth maps in the test set. Our method largely outperforms    the two constant baselines and approaches the results of supervised training. Improving over the third baseline (which has access to GT information) confirms that the model learns an instance specific 3D representation.\nAblation. To understand the influence of the individual parts of the model, we remove them one at a time and evaluate the performance of the ablated model in Table 3. Visual results are reported in Fig. 9.\nIn the table, row (1) shows the performance of the full model (the same as in Table 2). Row (2) does not flip the albedo. Thus, the albedo is not encouraged to be symmetric in the canonical space, which fails to canonicalize the viewpoint of the object and to use cues from symmetry to recover shape. The performance is as low as the trivial baseline in Table 2. Row (3) does not flip the depth, with a similar effect to row (2). Row (4) predicts a shading map instead of computing it from depth and light direction. This also harms performance significantly because shading cannot be used as a cue to recover shape. Row (5) switches off the perceptual loss, which leads to degraded image quality and hence degraded reconstruction results. Row ( 6) replaces the ImageNet pretrained image encoder used in the perceptual loss with one 4 trained through a self-supervised task [19], which shows no difference in performance. Finally, row ( 7) switches off the confidence maps, using a fixed and uniform value for the confidence -this reduces losses ( 3) and ( 7) to the basic L 1 and L 2 losses, respectively. The accuracy does not drop significantly, as faces in BFM are highly symmetric (e.g. do not have hair), but its variance increases. To better understand the effect of the confidence maps, we specifically evaluate on partially asymmetric faces using perturbations.\nAsymmetric perturbation. In order to demonstrate that our uncertainty modelling allows the model to handle asymmetry, we add asymmetric perturbations to BFM. Specifically, we generate random rectangular color patches with 20% to 50% of the image size and blend them onto the images with \u03b1-values ranging from 0.5 to 1, as shown in Fig. 3.\nWe then train our model with and without confidence on these perturbed images, and report the results in Table 4. Without the confidence maps, the model always predicts a symmetric albedo and geometry reconstruction often fails. With our confidence estimates, the model is able to reconstruct the asymmetric faces correctly, with very little loss in accuracy compared to the unperturbed case.\nQualitative results. In Fig. 4 we show reconstruction results of human faces from CelebA and 3DFAW, cat faces from [72,46] and synthetic cars from ShapeNet. The 3D shapes are recovered with high fidelity. The reconstructed 3D face, for instance, contain fine details of the nose, eyes and mouth even in the presence of extreme facial expression.\nTo further test generalization, we applied our model trained on the CelebA dataset to a number of paintings and cartoon drawings of faces collected from [9] and the Internet. As shown in Fig. 5, our method still works well even though it has never seen such images during training.  Symmetry and asymmetry detection. Since our model predicts a canonical view of the objects that is symmetric about the vertical center-line of the image, we can easily visualize the symmetry plane, which is otherwise non-trivial to detect from in-the-wild images. In Fig. 6, we warp the centerline of the canonical image to the predicted input viewpoint.\nOur method can detect symmetry planes accurately despite the presence of asymmetric texture and lighting effects. We also overlay the predicted confidence map \u03c3 onto the image, confirming that the model assigns low confidence to asymmetric regions in a sample-specific way.", "publication_ref": ["b1", "b18", "b71", "b45", "b8"], "figure_ref": ["fig_3", "fig_5"], "table_ref": ["tab_3", "tab_5", "tab_3", "tab_3", "tab_6"]}, {"heading": "Comparison with the state of the art", "text": "As shown in Table 1, most reconstruction methods in the literature require either image annotations, prior 3D models input LAE [49] ours input Szab\u00f3 et al. [56] ours\nFigure 7: Qualitative comparison to SOTA. Our method recovers much higher quality shapes compared to [49,56].\nor both. When these assumptions are dropped, the task becomes considerably harder, and there is little prior work that is directly comparable. Of these, [22] only uses synthetic, texture-less objects from ShapeNet, [56] reconstructs in-thewild faces but does not report any quantitative results, and [49] reports quantitative results only on keypoint regression, but not on the 3D reconstruction quality. We were not able to obtain code or trained models from [49,56] for a direct quantitative comparison and thus compare qualitatively.\nQualitative comparison. In order to establish a side-byside comparison, we cropped the examples reported in the papers [49,56] and compare our results with theirs (Fig. 7).\nOur method produces much higher quality reconstructions than both methods, with fine details of the facial expression, whereas [49] recovers 3D shapes poorly and [56] generates unnatural shapes. Note that [56] uses an unconditional GAN that generates high resolution 3D faces from random noise, and cannot recover 3D shapes from images. The input images for [56] in Fig. 7 were generated by their GAN.\n3D keypoint depth evaluation. Next, we compare to the DepthNet model of [40]. This method predicts depth for selected facial keypoints, but uses 2D keypoint annotations as input -a much easier setup than the one we consider here. Still, we compare the quality of the reconstruction of these sparse point obtained by DepthNet and our method. We also compare to the baselines MOFA [57] and AIGN [61] reported in [40]. For a fair comparison, we use their public code which computes the depth correlation score (between 0 and 66) on the frontal faces. We use the 2D keypoint locations to sample our predicted depth and then evaluate the same metric. The set of test images from 3DFAW and the preprocessing are identical to [40]. Since 3DFAW is a small dataset with limited variation, we also report results with CelebA pre-training.\nIn Table 5 we report the results from their paper and the slightly improved results we obtained from their publicly- available implementation. The paper also evaluates a supervised model using a GAN discriminator trained with ground-truth depth information. While our method does not use any supervision, it still outperforms DepthNet and reaches close-to-supervised performance.", "publication_ref": ["b48", "b55", "b48", "b55", "b21", "b55", "b48", "b48", "b55", "b48", "b55", "b48", "b55", "b55", "b55", "b39", "b56", "b60", "b39", "b39"], "figure_ref": [], "table_ref": ["tab_1", "tab_7"]}, {"heading": "Limitations", "text": "While our method is robust in many challenging scenarios (e.g., extreme facial expression, abstract drawing), we do observe failure cases as shown in Fig. 8. During training, we assume a simple Lambertian shading model, ignoring shadows and specularity, which leads to inaccurate reconstructions under extreme lighting conditions (Fig. 8a) or highly non-Lambertian surfaces. Disentangling noisy dark textures and shading (Fig. 8b) is often difficult. The reconstruction quality is lower for extreme poses (Fig. 8c), partly due to poor supervisory signal from the reconstruction loss of side images. This may be improved by imposing constraints from accurate reconstructions of frontal poses.", "publication_ref": [], "figure_ref": ["fig_6", "fig_6", "fig_6", "fig_6"], "table_ref": []}, {"heading": "Conclusions", "text": "We have presented a method that can learn a 3D model of a deformable object category from an unconstrained collection of single-view images of the object category. The model is able to obtain high-fidelity monocular 3D reconstructions of individual object instances. This is trained based on a reconstruction loss without any supervision, resembling an autoencoder. We have shown that symmetry and illumination are strong cues for shape and help the model to converge to a meaningful reconstruction. Our model outperforms a current state-of-the-art 3D reconstruction method that uses 2D keypoint supervision. As for future work, the model currently represents 3D shape from a canonical viewpoint using a depth map, which is sufficient for objects such as faces that have a roughly convex shape and a natural canonical viewpoint. For more complex objects, it may be possible to extend the model to use either multiple canonical views or a different 3D representation, such as a mesh or a voxel map.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary Material", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Differentiable rendering layer", "text": "As noted in Section 3.3, the reprojection function \u03a0 warps the canonical image J to generate the actual image I. In CNNs, image warping is usually regarded as a simple operation that can be implemented efficiently using a bilinear resampling layer [26]. However, this is true only if we can easily send pixels (u , v ) in the warped image I back to pixels (u, v) in the source image J, a process also known as backward warping. Unfortunately, in our case the function \u03b7 d,w obtained by Eq. ( 6) sends pixels in the opposite way.\nImplementing a forward warping layer is surprisingly delicate. One way of approaching the problem is to regard this task as a special case of rendering a textured mesh. The Neural Mesh Renderer (NMR) of [31] is a differentiable renderer of this type. In our case, the mesh has one vertex per pixel and each group of 2\u00d72 adjacent pixels is tessellated by two triangles. Empirically, we found the quality of the texture gradients of NMR to be poor in this case, likely caused by high frequency content in the texture image J.\nWe solve the problem as follows. First, we use NMR to warp only the depth map d, obtaining a versiond of the depth map as seen from the input viewpoint. This has two advantages: backpropagation through NMR is faster and secondly, the gradients are more stable, probably also due to the comparatively smooth nature of the depth map d compared to the texture image J. Given the depth map d, we then use the inverse of Eq. (6) to find the warp field from the observed viewpoint to the canonical viewpoint, and bilinearly resample the canonical image J to obtain the reconstruction.", "publication_ref": ["b25", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Training details", "text": "We report the training details including all hyperparameter settings in Table 6, and detailed network architectures in Tables 7 to 9. We use standard encoder networks for both viewpoint and lighting predictions, and encoderdecoder networks for depth, albedo and confidence predictions. In order to mitigate checkerboard artifacts [45] in the predicted depth and albedo, we add a convolution layer after each deconvolution layer and replace the last deconvolotion layer with nearest-neighbor upsampling, followed by 3 convolution layers. Abbreviations of the operators are defined as follows:\n\u2022 Conv(c in , c out , k, s, p): convolution with c in input channels, c out output channels, kernel size k, stride s and padding p.\n\u2022 Deconv(c in , c out , k, s, p): deconvolution [70] with c in input channels, c out output channels, kernel size k, stride s and padding p.   The output channel size c out is 6 for viewpoint, corresponding to rotation angles w 1:3 and translations w 4:6 in x, y and z axes, and 4 for lighting, corresponding to k s , k d , l x and l y .\n\u2022 Upsample(s): nearest-neighbor upsampling with a scale factor of s.\n\u2022 GN(n): group normalization [67] with n groups.\n\u2022 LReLU(\u03b1): leaky ReLU [39] with a negative slope of \u03b1.", "publication_ref": ["b44", "b69", "b66", "b38"], "figure_ref": [], "table_ref": ["tab_8", "tab_9"]}, {"heading": "Qualitative Results", "text": "We provide more qualitative results in the following and 3D animations in the supplementary video 5 . Fig. 9 reports the qualitative results of the ablated models in Table 3. Fig. 11 shows reconstruction results on human faces from CelebA and 3DFAW. We also show reconstruction results on face paintings and drawings collected from [9] and the Internet in Figs. 12 and 13. Figs. 14 to 16 show results on real cat faces from [72,46], abstract cats collected from the Internet and synthetic cars rendered using ShapeNet.   Re-lighting. Since our model predicts the intrinsic components of an image, separating the albedo and illumination, we can easily re-light the objects with different lighting conditions. In Fig. 10, we demonstrate results of the intrinsic decomposition and the re-lit faces in the canonical view.\nTesting on videos. To further assess our model, we apply the model trained on CelebA faces to VoxCeleb [8] videos frame by frame and include the results in the supplementary video. Our trained model works surprisingly well, producing consistent, smooth reconstructions across different frames and recovering the details of the facial motions accurately.    ", "publication_ref": ["b4", "b8", "b71", "b45", "b7"], "figure_ref": ["fig_7", "fig_1", "fig_7"], "table_ref": ["tab_5"]}, {"heading": "", "text": "Acknowledgements We would like to thank Soumyadip Sengupta for sharing with us the code to generate synthetic face datasets, and Mihir Sahasrabudhe for sending us the reconstruction results of Lifting AutoEncoders. We are also indebted to the members of Visual Geometry Group for insightful discussions. This work is jointly supported by Facebook Research and ERC Horizon 2020 research and innovation programme IDIU 638009.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Learning representations and generative models for 3D point clouds", "journal": "", "year": "2018", "authors": "Panos Achlioptas; Olga Diamanti; Ioannis Mitliagkas; Leonidas Guibas"}, {"ref_id": "b1", "title": "Learning to see by moving", "journal": "", "year": "2015", "authors": "Pulkit Agrawal; Joao Carreira; Jitendra Malik"}, {"ref_id": "b2", "title": "The bas-relief ambiguity", "journal": "IJCV", "year": "1999", "authors": "N Peter; David J Belhumeur; Alan L Kriegman;  Yuille"}, {"ref_id": "b3", "title": "Recovering non-rigid 3D shape from image streams", "journal": "", "year": "2000", "authors": "Christoph Bregler; Aaron Hertzmann; Henning Biermann"}, {"ref_id": "b4", "title": "Shapenet: An information-rich 3d model repository", "journal": "", "year": "2015", "authors": "Angel X Chang; Thomas Funkhouser; Leonidas Guibas; Pat Hanrahan; Qixing Huang; Zimo Li; Silvio Savarese; Manolis Savva; Shuran Song; Hao Su; Jianxiong Xiao; Li Yi; Fisher Yu"}, {"ref_id": "b5", "title": "Unsupervised 3d pose estimation with geometric selfsupervision", "journal": "", "year": "2019", "authors": "Ching-Hang Chen; Ambrish Tyagi; Amit Agrawal; Dylan Drover; M V Rohith; Stefan Stojanov; James M Rehg"}, {"ref_id": "b6", "title": "Learning to predict 3d objects with an interpolation-based differentiable renderer", "journal": "", "year": "2019", "authors": "Wenzheng Chen; Huan Ling; Jun Gao; Edward Smith; Jaako Lehtinen; Alec Jacobson; Sanja Fidler"}, {"ref_id": "b7", "title": "VoxCeleb2: Deep speaker recognition", "journal": "", "year": "2018", "authors": "Joon Son Chung; Arsha Nagrani; Andrew Zisserman"}, {"ref_id": "b8", "title": "Face painting: querying art with photos", "journal": "", "year": "2015", "authors": "Elliot J Crowley; Omkar M Parkhi; Andrew Zisserman"}, {"ref_id": "b9", "title": "Depth map prediction from a single image using a multi-scale deep network", "journal": "", "year": "2014", "authors": "David Eigen; Christian Puhrsch; Rob Fergus"}, {"ref_id": "b10", "title": "The Geometry of Multiple Images", "journal": "MIT Press", "year": "2001", "authors": "Olivier Faugeras; Quang-Tuan Luong"}, {"ref_id": "b11", "title": "Mirror symmetry \u21d2 2-view stereo geometry. Image and Vision Computing", "journal": "", "year": "2003", "authors": "R J Alexandre; G\u00e9rard G Fran\u00e7ois; Roman Medioni;  Waupotitsch"}, {"ref_id": "b12", "title": "3D shape induction from 2D views of multiple objects", "journal": "", "year": "2017", "authors": "Matheus Gadelha; Subhransu Maji; Rui Wang"}, {"ref_id": "b13", "title": "Exploiting symmetry and/or manhattan properties for 3d object structure estimation from single and multiple images", "journal": "", "year": "2017", "authors": "Yuan Gao; Alan L Yuille"}, {"ref_id": "b14", "title": "GANFIT: Generative adversarial network fitting for high fidelity 3D face reconstruction", "journal": "", "year": "2019", "authors": "Baris Gecer; Stylianos Ploumpis; Irene Kotsia; Stefanos Zafeiriou"}, {"ref_id": "b15", "title": "Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness", "journal": "", "year": "2019", "authors": "Robert Geirhos; Patricia Rubisch; Claudio Michaelis; Matthias Bethge; Felix A Wichmann; Wieland Brendel"}, {"ref_id": "b16", "title": "3D guided fine-grained face manipulation", "journal": "", "year": "2019", "authors": "Zhenglin Geng; Chen Cao; Sergey Tulyakov"}, {"ref_id": "b17", "title": "Morphable face models -an open framework", "journal": "", "year": "2018", "authors": "Thomas Gerig; Andreas Morel-Forster; Clemens Blumer; Bernhard Egger; Marcel L\u00fcthi; Sandro Sch\u00f6nborn; Thomas Vetter"}, {"ref_id": "b18", "title": "Unsupervised representation learning by predicting image rotations", "journal": "", "year": "2018", "authors": "Spyros Gidaris; Praveer Singh; Nikos Komodakis"}, {"ref_id": "b19", "title": "Brostow. Unsupervised monocular depth estimation with left-right consistency", "journal": "", "year": "2017", "authors": "Cl\u00e9ment Godard; Oisin Mac Aodha; Gabriel J "}, {"ref_id": "b20", "title": "Multi-pie. Image and Vision Computing", "journal": "", "year": "2010", "authors": "Ralph Gross; Iain Matthews; Jeffrey Cohn; Takeo Kanade; Simon Baker"}, {"ref_id": "b21", "title": "Learning single-image 3D reconstruction by generative modelling of shape, pose and shading", "journal": "IJCV", "year": "2008", "authors": "Paul Henderson; Vittorio Ferrari"}, {"ref_id": "b22", "title": "Escaping plato's cave using adversarial training: 3d shape from unstructured 2d image collections", "journal": "", "year": "2019", "authors": "Philipp Henzler; Niloy Mitra; Tobias Ritschel"}, {"ref_id": "b23", "title": "Obtaining shape from shading information", "journal": "", "year": "1975", "authors": "Berthold Horn"}, {"ref_id": "b24", "title": "Shape from Shading", "journal": "MIT Press", "year": "1989", "authors": "K P Berthold; Michael J Horn;  Brooks"}, {"ref_id": "b25", "title": "Spatial transformer networks", "journal": "", "year": "2015", "authors": "Max Jaderberg; Karen Simonyan; Andrew Zisserman; Koray Kavukcuoglu"}, {"ref_id": "b26", "title": "Dense 3d face alignment from 2d videos in real-time", "journal": "", "year": "2015", "authors": "A L\u00e1szl\u00f3; Jeffrey F Jeni; Takeo Cohn;  Kanade"}, {"ref_id": "b27", "title": "End-to-end recovery of human shape and pose", "journal": "", "year": "2018", "authors": "Angjoo Kanazawa; Michael J Black; David W Jacobs; Jitendra Malik"}, {"ref_id": "b28", "title": "Learning category-specific mesh reconstruction from image collections", "journal": "", "year": "2018", "authors": "Angjoo Kanazawa; Shubham Tulsiani; Alexei A Efros; Jitendra Malik"}, {"ref_id": "b29", "title": "Learning view priors for single-view 3d reconstruction", "journal": "", "year": "2019", "authors": "Hiroharu Kato; Tatsuya Harada"}, {"ref_id": "b30", "title": "Neural 3d mesh renderer", "journal": "", "year": "2018", "authors": "Hiroharu Kato; Yoshitaka Ushiku; Tatsuya Harada"}, {"ref_id": "b31", "title": "What uncertainties do we need in bayesian deep learning for computer vision", "journal": "", "year": "2017", "authors": "Alex Kendall; Yarin Gal"}, {"ref_id": "b32", "title": "What does the occluding contour tell us about solid shape?", "journal": "Perception", "year": "1984", "authors": "J Jan;  Koenderink"}, {"ref_id": "b33", "title": "Soft rasterizer: A differentiable renderer for image-based 3d reasoning", "journal": "", "year": "2019", "authors": "Shichen Liu; Tianye Li; Weikai Chen; Hao Li"}, {"ref_id": "b34", "title": "Deep learning face attributes in the wild", "journal": "", "year": "2015", "authors": "Ziwei Liu; Ping Luo; Xiaogang Wang; Xiaoou Tang"}, {"ref_id": "b35", "title": "SMPL: A skinned multiperson linear model", "journal": "ACM TOG", "year": "2015", "authors": "Matthew Loper; Naureen Mahmood; Javier Romero; Gerard Pons-Moll; Michael J Black"}, {"ref_id": "b36", "title": "OpenDR: An approximate differentiable renderer", "journal": "", "year": "2014", "authors": "M Matthew; Michael J Loper;  Black"}, {"ref_id": "b37", "title": "Single view stereo matching", "journal": "", "year": "2018", "authors": "Yue Luo; Jimmy Ren; Mude Lin; Jiahao Pang; Wenxiu Sun; Hongsheng Li; Liang Lin"}, {"ref_id": "b38", "title": "Rectifier nonlinearities improve neural network acoustic models", "journal": "", "year": "2013", "authors": "Andrew L Maas; Awni Y Hannun; Andrew Y Ng"}, {"ref_id": "b39", "title": "Unsupervised depth estimation, 3d face rotation and replacement", "journal": "", "year": "2008", "authors": "Joel Ruben ; Antony Moniz; Christopher Beckham; Simon Rajotte; Sina Honari; Christopher Pal"}, {"ref_id": "b40", "title": "Shape from symmetry -detecting and exploiting symmetry in affine images", "journal": "Philosophical Transactions of the Royal Society of London", "year": "1995", "authors": "P Dipti; Andrew Mukherjee; J. Michael Zisserman;  Brady"}, {"ref_id": "b41", "title": "Hologan: Unsupervised learning of 3d representations from natural images", "journal": "", "year": "2019", "authors": "Thu Nguyen-Phuoc; Chuan Li; Lucas Theis; Christian Richardt; Yong-Liang Yang"}, {"ref_id": "b42", "title": "Learning 3d object categories by looking around them", "journal": "", "year": "2017", "authors": "David Novotny; Diane Larlus; Andrea Vedaldi"}, {"ref_id": "b43", "title": "C3DPO: Canonical 3d pose networks for non-rigid structure from motion", "journal": "", "year": "2019", "authors": "David Novotny; Nikhila Ravi; Benjamin Graham; Natalia Neverova; Andrea Vedaldi"}, {"ref_id": "b44", "title": "Deconvolution and checkerboard artifacts", "journal": "Distill", "year": "2016", "authors": "Augustus Odena; Vincent Dumoulin; Chris Olah"}, {"ref_id": "b45", "title": "Cats and dogs", "journal": "", "year": "2012", "authors": "M Omkar; Andrea Parkhi; Andrew Vedaldi; C V Zisserman;  Jawahar"}, {"ref_id": "b46", "title": "A 3D face model for pose and illumination invariant face recognition. In Advanced video and signal based surveillance", "journal": "", "year": "2009", "authors": "Pascal Paysan; Reinhard Knothe; Brian Amberg; Sami Romdhani; Thomas Vetter"}, {"ref_id": "b47", "title": "Generating 3D faces using convolutional mesh autoencoders", "journal": "", "year": "2018", "authors": "Anurag Ranjan; Timo Bolkart; Soubhik Sanyal; Michael J Black"}, {"ref_id": "b48", "title": "Lifting autoencoders: Unsupervised learning of a fully-disentangled 3d morphable model using deep non-rigid structure from motion", "journal": "", "year": "2008", "authors": "Mihir Sahasrabudhe; Zhixin Shu; Edward Bartrum"}, {"ref_id": "b49", "title": "Learning to regress 3D face shape and expression from an image without 3D supervision", "journal": "", "year": "2019", "authors": "Soubhik Sanyal; Timo Bolkart; Haiwen Feng; Michael J Black"}, {"ref_id": "b50", "title": "SfSNet: Learning shape, refectance and illuminance of faces in the wild", "journal": "", "year": "2018", "authors": "Soumyadip Sengupta; Angjoo Kanazawa; Carlos D Castillo; David Jacobs"}, {"ref_id": "b51", "title": "Deforming autoencoders: Unsupervised disentangling of shape and appearance", "journal": "", "year": "2018", "authors": "Zhixin Shu; Mihir Sahasrabudhe; Alp Guler"}, {"ref_id": "b52", "title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2015", "authors": "Karen Simonyan; Andrew Zisserman"}, {"ref_id": "b53", "title": "Detecting and reconstructing 3d mirror symmetric objects", "journal": "", "year": "2012", "authors": "N Sudipta; Krishnan Sinha; Richard Ramnath;  Szeliski"}, {"ref_id": "b54", "title": "Discovery of latent 3d keypoints via end-to-end geometric reasoning", "journal": "", "year": "2018", "authors": "Supasorn Suwajanakorn; Noah Snavely; Jonathan Tompson; Mohammad Norouzi"}, {"ref_id": "b55", "title": "Unsupervised generative 3d shape learning from natural images", "journal": "", "year": "2008", "authors": "Attila Szab\u00f3; Givi Meishvili; Paolo Favaro"}, {"ref_id": "b56", "title": "MoFA: Model-based deep convolutional face autoencoder for unsupervised monocular reconstruction", "journal": "", "year": "2017", "authors": "Ayush Tewari; Michael Zollh\u00f6fer; Hyeongwoo Kim; Pablo Garrido; Florian Bernard; Patrick P\u00e9rez; Christian Theobalt"}, {"ref_id": "b57", "title": "Unsupervised learning of object frames by dense equivariant image labelling", "journal": "", "year": "2017", "authors": "James Thewlis; Hakan Bilen; Andrea Vedaldi"}, {"ref_id": "b58", "title": "Modelling and unsupervised learning of symmetric deformable object categories", "journal": "", "year": "2018", "authors": "James Thewlis; Hakan Bilen; Andrea Vedaldi"}, {"ref_id": "b59", "title": "Shape from symmetry", "journal": "", "year": "2005", "authors": "Sebastian Thrun; Ben Wegbreit"}, {"ref_id": "b60", "title": "Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision", "journal": "", "year": "2017", "authors": "Hsiao-Yu Fish Tung; Adam W Harley; William Seto; Katerina Fragkiadaki"}, {"ref_id": "b61", "title": "Demon: Depth and motion network for learning monocular stereo", "journal": "", "year": "2017", "authors": "Benjamin Ummenhofer; Huizhong Zhou; Jonas Uhrig; Nikolaus Mayer; Eddy Ilg; Alexey Dosovitskiy; Thomas Brox"}, {"ref_id": "b62", "title": "Learning depth from monocular videos using direct methods", "journal": "", "year": "2018", "authors": "Chaoyang Wang; Jose Miguel Buenaposada; Rui Zhu; Simon Lucey"}, {"ref_id": "b63", "title": "Yannis Panagakis, Dimitris Samaras, and Stefanos Zafeiriou. An adversarial neuro-tensorial approach for learning disentangled representations", "journal": "IJCV", "year": "2019", "authors": "Mengjiao Wang; Zhixin Shu; Shiyang Cheng"}, {"ref_id": "b64", "title": "Recovering surface shape and orientation from texture", "journal": "Artificial Intelligence", "year": "1981", "authors": "Andrew P Witkin"}, {"ref_id": "b65", "title": "Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling", "journal": "", "year": "2016", "authors": "Jiajun Wu; Chengkai Zhang; Tianfan Xue; William T Freeman; Joshua B Tenenbaum"}, {"ref_id": "b66", "title": "Group normalization", "journal": "", "year": "2018", "authors": "Yuxin Wu; Kaiming He"}, {"ref_id": "b67", "title": "Sun database: Large-scale scene recognition from abbey to zoo", "journal": "", "year": "2010", "authors": "Jianxiong Xiao; James Hays; Krista A Ehinger; Aude Oliva; Antonio Torralba"}, {"ref_id": "b68", "title": "A high-resolution 3d dynamic facial expression database", "journal": "", "year": "2008", "authors": "Lijun Yin; Xiaochen Chen; Yi Sun; Tony Worm; Michael Reale"}, {"ref_id": "b69", "title": "Adaptive deconvolutional networks for mid and high level feature learning", "journal": "", "year": "2011", "authors": "Matthew D Zeiler; Graham W Taylor; Rob Fergus"}, {"ref_id": "b70", "title": "Shape-from-shading: a survey", "journal": "IEEE PAMI", "year": "1999", "authors": "Ruo Zhang; Ping-Sing Tsai; James Edwin Cryer; Mubarak Shah"}, {"ref_id": "b71", "title": "Cat head detection -how to effectively exploit shape and texture features", "journal": "", "year": "2008", "authors": "Weiwei Zhang; Jian Sun; Xiaoou Tang"}, {"ref_id": "b72", "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database", "journal": "Image and Vision Computing", "year": "2014", "authors": "Xing Zhang; Lijun Yin; Jeffrey F Cohn; Shaun Canavan; Michael Reale; Andy Horowitz; Peng Liu; Jeffrey M Girard"}, {"ref_id": "b73", "title": "Unsupervised learning of depth and ego-motion from video", "journal": "", "year": "2017", "authors": "Tinghui Zhou; Matthew Brown; Noah Snavely; David G Lowe"}, {"ref_id": "b74", "title": "Freeman. Visual object networks: Image generation with disentangled 3D representations", "journal": "", "year": "2018", "authors": "Jun-Yan Zhu; Zhoutong Zhang; Chengkai Zhang; Jiajun Wu; Antonio Torralba; Joshua B Tenenbaum; William T "}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Photo-geometric Autoencoding", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Photo-geometric autoencoding. Our network \u03a6 decomposes an input image I into depth, albedo, viewpoint and lighting, together with a pair of confidence maps. It is trained to reconstruct the input without external supervision.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "lighting function \u039b generates a version of the object based on the depth map d, the light direction l and the albedo a as seen from a canonical viewpoint w = 0. The viewpoint w represents the transformation between the canonical view and the viewpoint of the actual input image I. Then, the reprojection function \u03a0 simulates the effect of a viewpoint change and generates the image\u00ce given the canonical depth d and the shaded canonical image \u039b(a, d, l). Learning uses a reconstruction loss which encourages I \u2248\u00ce (Section 3.2).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Asymmetric perturbation. Top: examples of the perturbed dataset. Bottom: reconstructions with and without confidence maps. Confidence allows the model to correctly reconstruct the 3D shape with the asymmetric texture.", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :Figure 5 :45Figure 4: Reconstruction of faces, cats and cars.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 6 :6Figure 6: Symmetry plane and asymmetry detection. (a): our model can reconstruct the \"intrinsic\" symmetry plane of an in-the-wild object even though the appearance is highly asymmetric. (b): asymmetries (highlighted in red) are detected and visualized using confidence map \u03c3 .", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 8 :8Figure 8: Failure cases. See Section 4.4 for details.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "1 )1Light direction lx, ly (\u22121, 1) Viewpoint rotation w 1:3 (\u221260 \u2022 , 60 \u2022 ) Viewpoint translation w 4:6 (\u22120.1, 0.1) Field of view (FOV) 10", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 11 :11Figure 11: Reconstruction of human faces.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 12 :12Figure 12: Reconstruction of face paintings.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 13 :13Figure 13: Reconstruction of abstract faces.", "figure_data": ""}, {"figure_label": "141516", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 14 :Figure 15 :Figure 16 :141516Figure 14: Reconstruction of cat faces.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "we compare our contribution to prior works based on these factors.", "figure_data": "Paper SupervisionGoalsData[47] 3D scans3DMMFace[66] 3DV, IPrior on 3DV, predict from I ShapeNet, Ikea[1] 3DPPrior on 3DPShapeNet[48] 3DMPrior on 3DMFace[17] 3DMM, 2DKP, I Refine 3DMM fit to IFace[15] 3DMM, 2DKP, I Fit 3DMM to I+2DKPFace[18] 3DMMFit 3DMM to 3D scansFace[28] 3DMM, 2DKPPred. 3DMM from IHumans[51] 3DMM, 2DS+KP Pred. N, A, L from IFace[64] 3DMM, IPred. 3DM, VP, T, E from I Face[50] 3DMM, 2DKP, I Fit 3DMM to IFace[13] 2DSPrior on 3DV, pred. from I Model/ScanNet[30] I, 2DS, VPPrior on 3DVScanNet, PAS3D[29] I, 2DS+KPPred. 3DM, T, VP from IBirds[7] I, 2DSPred. 3DM, T, L, VP from I ShapeNet, Birds[23] I, 2DSPred. 3DV, VP from IShapeNet, others[56] IPrior on 3DM, T, IFace[49] IPred. 3DM, VP, T  \u2020 from I Face[22] IPred. V, L, VP from IShapeNetOurs IPred. D, L, A, VP from IFace, others"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison with baselines. SIDE and MAD errors of our reconstructions on the BFM dataset compared against a fully-supervised and trivial baselines.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ablation study. Refer to Section 4.2 for details.", "figure_data": "SIDE (\u00d710 \u22122 ) \u2193 MAD (deg.) \u2193No perturb, no conf.0.829 \u00b10.21316.39 \u00b12.12No perturb, conf.0.793 \u00b10.14016.51 \u00b11.56Perturb, no conf.2.141 \u00b10.84226.61 \u00b15.39Perturb, conf.0.878 \u00b10.16917.14 \u00b11.90"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Asymmetric perturbation. We add asymmetric perturbations to BFM and show that confidence maps allow the model to reject such noise, while the vanilla model without confidence maps breaks.", "figure_data": "Depth Corr. \u2191"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "3DFAW keypoint depth evaluation. Depth correlation between ground truth and prediction evaluated at 66 facial keypoint locations.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Training details and hyper-parameter settings.", "figure_data": "EncoderOutput sizeConv(3, 32, 4, 2, 1) + ReLU32Conv(32, 64, 4, 2, 1) + ReLU16Conv(64, 128, 4, 2, 1) + ReLU8Conv(128, 256, 4, 2, 1) + ReLU4Conv(256, 256, 4, 1, 0) + ReLU1Conv(256, cout, 1, 1, 0) + Tanh \u2192 output1"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Network architecture for viewpoint and lighting.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "https://www.youtube.com/watch?v=5rPJyrU-WE4 Encoder Output size Conv(3, 64, 4, 2, 1) + GN(16) + LReLU(0.2) 32 Conv(64, 128, 4, 2, 1) + GN(32) + LReLU(0.2) 16 Conv(128, 256, 4, 2, 1) + GN(64) + LReLU(0.2) 8 Conv(256, 512, 4, 2, 1) + LReLU(0.2) 4 Conv(512, 256, 4, 1, 0) + ReLU 1", "figure_data": "DecoderOutput sizeDeconv(256, 512, 4, 1, 0) + ReLU4Conv(512, 512, 3, 1, 1) + ReLU4Deconv(512, 256, 4, 2, 1) + GN(64) + ReLU8Conv(256, 256, 3, 1, 1) + GN(64) + ReLU8Deconv(256, 128, 4, 2, 1) + GN(32) + ReLU16Conv(128, 128, 3, 1, 1) + GN(32) + ReLU16Deconv(128, 64, 4, 2, 1) + GN(16) + ReLU32Conv(64, 64, 3, 1, 1) + GN(16) + ReLU32Upsample(2)64Conv(64, 64, 3, 1, 1) + GN(16) + ReLU64Conv(64, 64, 5, 1, 2) + GN(16) + ReLU64Conv(64, cout, 5, 1, 2) + Tanh \u2192 output64"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Network architecture for depth and albedo. The output channel size c out is 1 for depth and 3 for albedo.", "figure_data": "EncoderOutput sizeConv(3, 64, 4, 2, 1) + GN(16) + LReLU(0.2)32Conv(64, 128, 4, 2, 1) + GN(32) + LReLU(0.2)16Conv(128, 256, 4, 2, 1) + GN(64) + LReLU(0.2)8Conv(256, 512, 4, 2, 1) + LReLU(0.2)4Conv(512, 128, 4, 1, 0) + ReLU1DecoderOutput sizeDeconv(128, 512, 4, 1, 0) + ReLU4Deconv(512, 256, 4, 2, 1) + GN(64) + ReLU8Deconv(256, 128, 4, 2, 1) + GN(32) + ReLU16Conv(128, 2, 3, 1, 1) + SoftPlus \u2192 output16Deconv(128, 64, 4, 2, 1) + GN(16) + ReLU32Deconv(64, 64, 4, 2, 1) + GN(16) + ReLU64Conv(64, 2, 5, 1, 2) + SoftPlus \u2192 output64"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Network architecture for confidence maps. The network outputs two pairs of confidence maps at different spatial resolutions for photometric and perceptual losses.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "I = \u03a0 (\u039b(a, d, l), d, w) .", "formula_coordinates": [4.0, 119.48, 235.54, 97.52, 8.77]}, {"formula_id": "formula_1", "formula_text": ")1", "formula_coordinates": [4.0, 278.62, 235.89, 7.74, 8.64]}, {"formula_id": "formula_2", "formula_text": "I = \u03a0 (\u039b(a , d , l), d , w) , a = flip a, d = flip d. (2)", "formula_coordinates": [4.0, 315.39, 106.17, 229.72, 8.99]}, {"formula_id": "formula_3", "formula_text": "L(\u00ce, I, \u03c3) = \u2212 1 |\u2126| uv\u2208\u2126 ln 1 \u221a 2\u03c3 uv exp \u2212 \u221a 2 1,uv \u03c3 uv ,(3)", "formula_coordinates": [4.0, 319.61, 206.07, 225.5, 35.04]}, {"formula_id": "formula_4", "formula_text": "E(\u03a6; I) = L(\u00ce, I, \u03c3) + \u03bb f L(\u00ce , I, \u03c3 ),(4)", "formula_coordinates": [4.0, 352.61, 522.18, 192.5, 9.84]}, {"formula_id": "formula_5", "formula_text": "p \u221d KP, K = \uf8ee \uf8f0 f 0 c u 0 f c v 0 0 1 \uf8f9 \uf8fb , \uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 c u = W \u22121 2 , c v = H\u22121 2 , f = W \u22121 2 tan \u03b8 FOV 2 .", "formula_coordinates": [4.0, 313.84, 667.76, 203.51, 48.63]}, {"formula_id": "formula_6", "formula_text": ")5", "formula_coordinates": [4.0, 537.37, 688.08, 7.74, 8.64]}, {"formula_id": "formula_7", "formula_text": "3D point P = d uv \u2022 K \u22121 p.", "formula_coordinates": [5.0, 50.11, 169.53, 108.18, 11.23]}, {"formula_id": "formula_8", "formula_text": "p \u221d K(d uv \u2022 RK \u22121 p + T ),(6)", "formula_coordinates": [5.0, 110.98, 275.85, 175.38, 11.72]}, {"formula_id": "formula_9", "formula_text": "\u03a0(J, d, w) as\u00ce u v = J uv , where (u, v) = \u03b7 \u22121 d,w (u , v ). 3", "formula_coordinates": [5.0, 50.11, 346.35, 224.95, 13.38]}, {"formula_id": "formula_10", "formula_text": "t u uv = d u+1,v \u2022 K \u22121 (p + e x ) \u2212 d u\u22121,v \u2022 K \u22121 (p \u2212 e x )", "formula_coordinates": [5.0, 78.72, 442.94, 208.81, 12.19]}, {"formula_id": "formula_11", "formula_text": "J uv = (k s + k d max{0, l, n uv }) \u2022 a uv .", "formula_coordinates": [5.0, 49.81, 528.47, 168.53, 9.68]}, {"formula_id": "formula_12", "formula_text": "(I) \u2208 R C k \u00d7W k \u00d7H k", "formula_coordinates": [5.0, 209.13, 673.6, 76.15, 10.87]}, {"formula_id": "formula_13", "formula_text": "L (k) p (\u00ce, I, \u03c3 (k) ) = \u2212 1 |\u2126 k | uv\u2208\u2126 k ln 1 2\u03c0(\u03c3 (k) uv ) 2 exp \u2212 ( (k) uv ) 2 2(\u03c3 (k) uv ) 2 ,(7)", "formula_coordinates": [5.0, 308.86, 151.74, 239.51, 39.12]}, {"formula_id": "formula_14", "formula_text": "(k) uv = |e (k) uv (\u00ce) \u2212 e (k)", "formula_coordinates": [5.0, 342.27, 191.98, 82.5, 12.46]}, {"formula_id": "formula_15", "formula_text": "E SIDE (d, d * ) = ( 1 W H uv \u2206 2 uv \u2212 ( 1 W H uv \u2206 uv ) 2 ) 1 2", "formula_coordinates": [6.0, 50.11, 202.19, 234.56, 14.47]}], "doi": ""}