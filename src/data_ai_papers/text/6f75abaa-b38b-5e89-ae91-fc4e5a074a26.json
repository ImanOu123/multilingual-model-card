{"title": "Priors over Recurrent Continuous Time Processes", "authors": "Ardavan Saeedi; Alexandre Bouchard-C\u00f4t\u00e9", "pub_date": "", "abstract": "We introduce the Gamma-Exponential Process (GEP), a prior over a large family of continuous time stochastic processes. A hierarchical version of this prior (HGEP; the Hierarchical GEP) yields a useful model for analyzing complex time series. Models based on HGEPs display many attractive properties: conjugacy, exchangeability and closed-form predictive distribution for the waiting times, and exact Gibbs updates for the time scale parameters. After establishing these properties, we show how posterior inference can be carried efficiently using Particle MCMC methods [1]. This yields a MCMC algorithm that can resample entire sequences atomically while avoiding the complications of introducing slice and stick auxiliary variables of the beam sampler [2]. We applied our model to the problem of estimating the disease progression in multiple sclerosis [3], and to RNA evolutionary modeling [4]. In both domains, we found that our model outperformed the standard rate matrix estimation approach.", "sections": [{"heading": "Introduction", "text": "The application of non-parametric Bayesian techniques to time series has been an active field in the recent years, and has led to many successful continuous time models. Examples include Dependent Dirichlet Processes (DDP) [5], Ornstein-Uhlenbeck Dirichlet Processes [6], and stick-breaking autoregressive processes [7]. One property of these models is that they are forgetful, meaning that the effect of an observation at time t on a prediction at time t + s will decrease as s \u2192 \u221e. More formally, DDPs and their cousins can be viewed as priors over transient processes (see Section A of the Supplementary Material).\nIn some situations, emphasizing the short term trends is desirable, for example for the analysis of financial time series. However, in other situations, this behavior does not use the data optimally.\nAs a concrete example of the type of time series we are interested in, consider the problem of modeling the progression of recurrent diseases such as multiple sclerosis. Recurrent diseases are characterized by alternations between relapse and remission periods, and patients can undergo this cycle repeatedly. In multiple sclerosis research, measuring the effect of drugs in the presence of these complex cycles is challenging, and is one of the applications that motivated this work.\nThe data available to infer the disease progression typically takes the form of summary measurements taken at different points in time for each patient. We model these measurements as being conditionally independent given a continuous time non-parametric latent process. The main options available for this type of situation are currently limited to parametric Bayesian models [8], or to non-Bayesian models [9].\nIn this work, we propose a family of models, Gamma-Exponential Processes (GEPs), that fills this gap. GEPs are based on priors over recurrent, infinite rate matrices specifying a jump process in a latent space.\nIt is informative to start by a preview of what the predictive distributions look like in GEP models. Indeed, an advantage of GEPs is that they have simple predictive distributions, a situation remi-niscent of the theory of Dirichlet Processes, in which the simple predictive distributions (given by the Chinese Restaurant Process (CRP)) were probably an important factor behind their widespread adoption in Bayesian non-parametric statistics.\nSuppose that the hidden state at the current time step is \u03b8, and that we are interested in the distribution over the waiting time t before the next jump to a different hidden state (we will come back to the predictive distribution over what this next state is in Section 3, showing that it has the form of a CRP). Let t 1 , t 2 , . . . , t n denote the previous, distinct waiting times at \u03b8. The predictive distribution is then specified by the following density over the positive reals:\nf (t) = (\u03b10 + n)(\u03b20 + T ) (\u03b1 0 +n) (\u03b20 + T + t) \u03b1 0 +n+1 ,\nwhere T is the sum over the t i 's, and \u03b1 0 , \u03b2 0 are parameters. It can be checked that this yields an exchangeable distribution over the sequences of waiting times at \u03b8 (if forms a telescoping productsee the proof of Proposition 5 in the Supplementary Material). By de Finetti's theorem, there is therefore a mixing prior distribution. We identify this prior in Section 3, and use it to build a powerful hierarchical model in Section 4. As we will see, this hierarchical model displays many attractive properties: conjugacy, exchangeability and closed-form predictive distributions for the waiting times, and exact Gibbs updates for the time scale parameters. Moreover it admits efficient inference algorithms, described in Section 5.\nIn addition to the connection to DDPs mentioned above, our models are also related to the infinite Hidden Markov Model (iHMM) [10] and to the more general Sticky-HDP-HMM [11], which are both based on priors over discrete time processes. While continuous-time analogues of these discrete time processes can be constructed by subordination, we discuss in Section C of the Supplementary Material the differences and advantages of GEPs compared to these subordinations. A similar argument holds for factorial extensions of the infinite HMM [12].\nGamma (Moran) Processes [13], a building block for our process, have been used in non-parametric Bayesian statistics, but in different contexts, for example in survival analysis [14], spatial statistics [15], and for modeling count data [16]. 1 Note also that the gamma-exponential process introduced here is unrelated to the exponential-gamma process [18].", "publication_ref": ["b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14", "b15", "b0", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Background and notation", "text": "While our process can be defined on continuous state spaces, the essential ideas can be described over countable state spaces. We therefore focus in this section on reviewing Continuous Time Markov Processes (CTMPs) over a countably infinite state space.\nThese CTMPs can be characterized by an infinite matrix q i,j where the off-diagonal entries are nonnegative and each row sums to zero (i.e. the diagonal entries are negative and with magnitude equal to the sum of the off-diagonal row entries). Samples from these processes take the form of a list of pairs of states and waiting times X = (\u03b8 n , J n ) N n=1 (see Figure 1(a)). We will call each pair of that form a (hidden) event. Typically, only a function Y of the events is available. For example, measurements could be taken at fixed or random time intervals. We will come back to the partially observed sequences setup in Section 5.\nTo simulate a sequence of events given parameters Q = (q i,j ), we use the standard Doob-Gillespie algorithm: conditioning on the current state having index i, \u03b8 N = i, the waiting time before the next jump is exponentially distributed J N +1 |(\u03b8 N = i) \u223c Exp(\u2212q i,i ), and the index j = i of the next state \u03b8 N +1 is selected independently with probability proportional to p(j)\n= q i,j 1[i = j].\nThe goal of this work is to develop priors on such infinite rate matrices that are both flexible and easy to work with. To do that, we first note that the off-diagonal elements of each row i can be viewed as a positive measure \u00b5 i . Note that the normalization of this measure in not equal to one in general. We will denote the normalization constant of measures by ||\u00b5|| and the normalized measures b\u0233 \u00b5 = \u00b5/||\u00b5||.  To get a conjugate family, we will base our priors on Moran Gamma Processes (MGPs) [13], a family of measure-valued probability distributions. MGPs have three parameters: (1) A positive real number \u03b1 0 > 0, called the concentration or shape parameter, (2) A probability distribution P 0 : F \u2126 \u2192 [0, 1] called the base probability distribution, (3) A positive real number \u03b2 0 > 0, called the rate parameter. Alternatively, the first two parameters can be grouped into a single finite base measure parameter H 0 = \u03b1 0 P 0 .\n\u03b8 J \u03b8 \u03b8 t = 0 J =0 . . . J \u03b8 t * Y(t ) Y(t ) . . . t Y(t ) \u03b8 X Y Y(t ) Y(t ) \u03a9 (a) (b)\nRecall that by the Kolmogorov consistency theorem, in order to guarantee the existence of a stochastic process on a probability space (\u2126 , F \u2126 ), it is enough to provide a consistent definition of what the marginals of this stochastic process are. As the name suggest, in the case of a Moran Gamma process, the marginals are gamma distributions: Definition 1 (Moran Gamma Process). Let H 0 , \u03b2 0 be of the types listed above. We say that \u00b5 :\nF \u2126 \u2192 (F \u2126 \u2192 [0, \u221e)\n) is distributed according to the Moran Gamma process distribution, denoted by \u00b5 \u223c MGP(H 0 , \u03b2 0 ), if for all measurable partitions of \u2126, (A 1 , . . . , A K ), we have: 2\n(\u00b5(A1), \u00b5(A2), . . . , \u00b5(AK )) \u223c Gamma(H0(A1), \u03b20) \u00d7 \u2022 \u2022 \u2022 \u00d7 Gamma(H0(A k ), \u03b20).", "publication_ref": ["b12"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Gamma-Exponential Process", "text": "We can now describe the basic version of our model, the Gamma-Exponential Process (GEP). In the next section, we will move to a hierarchical version of this model.\nIn GEPs, the rows of a rate matrix Q are obtained by a transformation of iid samples from an MGP, and the states are then generated from Q with the Doob-Gillespie algorithm described in the previous section. In this section we show that this model is conjugate and has a closed form expression for the predictive distribution.\nLet H 0 be a base measure on a countable support \u2126 with H 0 < \u221e. We will relax the countable base measure support assumption in the next section. The GEP is formally defined as follows:\n\u00b5 \u03b8 iid \u223c MGP(H0, \u03b20) \u2200\u03b8 \u2208 \u2126 \u03b8N+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c\u03bc \u03b8 N JN+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c Exp ( \u00b5 \u03b8 N )\nTo understand the connection with the Doob-Gillespie process, note that a rate matrix can be obtained by arbitrarily ordering \u2126 = \u03b8 (1) , \u03b8 (2) , . . . , and setting: 3 q i,j = \u00b5 \u03b8 (i) ({\u03b8 (j) }) if i = j, and\n\u00b5 \u03b8 (i) (\u03bc \u03b8 (i) ({i}) \u2212 1) otherwise.\nIn order to model the initial distribution without cluttering the notation, we assume there is a special state \u03b8 beg always present at the beginning of the sequence, and only at the beginning. In other words, we always condition on (\u03b8 0 = \u03b8 beg ) and (\u03b8 n = \u03b8 beg , n > 0), and drop these conditioning events from the notation. Similarly, we are going to consider distribution over infinite sequences in the notation that follows, but if the goal is to model finite sequences, an additional special state \u03b8 end = \u03b8 beg can be introduced. We would then condition on (\u03b8 N +1 = \u03b8 end ) and (\u03b8 n = \u03b8 end , n \u2208 {1, . . . , N }), and set the total rate for the row corresponding to \u03b8 end to zero.\nNext, we show that the posterior of each row, \u00b5 \u03b8 |X, is also MGP distributed with updated parameters. We assume that all the states are observed for now, and treat the partially observed case in Section 5.\nThe sufficient statistics for the parameters of \u00b5 \u03b8 |X are the empirical transition measures and waiting times:\nF \u03b8 = N n=1 1[\u03b8n\u22121 = \u03b8] \u03b4 \u03b8n , T \u03b8 = N n=1 1[\u03b8n\u22121 = \u03b8] Jn. Proposition 2. The Gamma-Exponential Process (GEP) is a conjugate family, \u00b5 \u03b8 |X \u223c MGP (\u00b5 \u03b8 , \u03b2 \u03b8 ) , where \u00b5 \u03b8 = F \u03b8 + H 0 and \u03b2 \u03b8 = T \u03b8 + \u03b2 0 .\nNote that the \u00b5 \u03b8 are unnormalized versions of the posterior parameters of a Dirichlet process. This connexion with the Dirichlet process is used in the proof below, and also implies that samples from GEPs have countable support even when \u2126 is uncountable (i.e. the chain will always visit a random countable subset of \u2126). For the proof of proposition 2, we will need the following elementary lemma:\nLemma 3. If V \u223c Beta(a, b) and W \u223c Gamma(a + b, c) are independent, then V W \u223c Gamma(a, c).\nSee for example [19] for a survey of standard beta-gamma algebra results such as the one stated in this lemma. We now prove the proposition:\nProof. Fix an arbitrary state \u03b8 and drop the index for simplicity (this is without loss of generality since the rows are iid): let \u00b5 = \u00b5 \u03b8 , \u00b5 = \u00b5 \u03b8 , and \u03b2 = \u03b2 \u03b8 .\nLet (A 1 , . . . , A K ) be a measurable partition of \u2126. By the Kolmogorov consistency theorem, it is enough to show that for all such partition,\n(\u00b5(A1), \u00b5(A2), . . . , \u00b5(AK ))|X \u223c Gamma(\u00b5 (A1), \u03b2 ) \u00d7 \u2022 \u2022 \u2022 \u00d7 Gamma(\u00b5 (A k ), \u03b2 ).\nAssume for simplicity that K = 2 (the argument can be generalized to K > 2 without difficulties), and let \u0393 1 = \u00b5(A 1 ), \u0393 0 = \u00b5 . By elementary properties of Gamma distributed vectors, if we let\nV = \u0393 1 /\u0393 0 , W = \u0393 0 , then V \u223c Beta(H 0 (A 1 ), H 0 (A 2 )), W \u223c Gamma(\u03b1 0 , \u03b2 0 )\n, and V, W are independent (both conditionally on X and unconditionally). By beta-multinomial conjugacy, we also have\n(V |X) = (V |\u03b8 1 , . . . , \u03b8 N ) \u223c Beta(\u00b5 (A 1 ), \u00b5 (A 2 )\n), and by gamma-exponential conjugacy, we have\nW |X \u223c Gamma( \u00b5 , \u03b2 ).\nUsing the lemma with\na = \u00b5 (A 1 ), b = \u00b5 (A 2 ), c = \u03b2 , we finally get that (\u00b5(A 1 )|X) = (V W |X) \u223c Gamma(\u00b5 (A 1 ), \u03b2\n), which concludes the proof.\nWe now turn to the task of finding an expression for the predictive distribution, (\u03b8 N +1 , J N +1 )|X.\nWe will need the following family of densities (see Section F for more information): Definition 4 (Translated Pareto). Let \u03b1 > 0, \u03b2 > 0. We say that a random variable T is translated-Pareto, denoted T \u223c TP(\u03b1, \u03b2), if it has density:\nf (t) = 1[t > 0]\u03b1\u03b2 \u03b1 (t + \u03b2) \u03b1+1 .(1)\nProposition 5. The predictive distribution of the GEP is given by:\n(\u03b8N+1, JN+1)|X \u223c\u03bc \u03b8 N \u00d7 TP( \u00b5 \u03b8 N , \u03b2 \u03b8 N ).(2)\nProof. By Proposition 2, it is enough to show that if \u00b5 \u223c MGP(H 0 , \u03b2 0 ), \u03b8|\u00b5 \u223c\u03bc, and J|\u00b5 \u223c Exp( \u00b5 ), then (\u03b8, J) \u223c\u03bc \u00d7 TP(\u03b1 0 , \u03b2 0 ), where \u03b1 0 = H 0 .\nNote first that we have (J|\u03b8) d = J by the fact that the minimum and argmin of independent exponential random variables are independent. To get the distribution of J, we need to show that the following integral is proportional to Equation (1):\np(t) \u221d x>0 x \u03b1 0 \u22121 exp(\u2212\u03b20x) \u2022 x exp(\u2212xt) dx = x>0 x \u03b1 0 exp (\u2212(\u03b20 + t)x) dx = \u0393(\u03b10 + 1) (\u03b20 + t) \u03b1 0 +1 Hence J \u223c TP(\u03b1 0 , \u03b2 0 ).\nAs a sanity check, and to connect this result with the discussion in the introduction, it is instructive to directly check that these predictive distributions are indeed exchangeable (see Section B for the proof): Proposition 6. Let J j(\u03b8,1) , J j(\u03b8,2) , . . . , J j(\u03b8,K) be the subsequence of waiting times following state \u03b8. Then the random variables J j(\u03b8,1) , J j(\u03b8,2) , . . . , J j(\u03b8,K) are exchangeable. Moreover, the joint density of a sequence of waiting times (J j(\u03b8,1) = j 1 , J j(\u03b8,2) = j 2 , . . . , J j(\u03b8,K) = j K ) is given by:\np(j1, j2, . . . , jK ) = 1[j k > 0, k \u2208 {1, . . . , K}](\u03b10)K \u03b2 \u03b1 0 0 (\u03b20 + j1 + \u2022 \u2022 \u2022 + jK ) \u03b1 0 +K (3)\nwhere the Pochhammer symbol (x) n is defined as (x\n) n = x(x + 1) \u2022 \u2022 \u2022 (x + n \u2212 1).", "publication_ref": ["b0", "b1", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Hierarchical GEP", "text": "In this section, we present a hierarchical version of the GEP, where the rows of the random rate matrix are exchangeable rather than iid. Informally, the motivation behind this construction is to have the rows share information on what states are frequently visited.\nAs with Hierarchical Dirichlet Processes (HDPs) [20], the hierarchical construction is especially important when \u2126 is uncountable. For such spaces, since each GEP sample has a random countable support, any two independent GEP samples will have disjoint supports with probability one. Therefore, GEP alone cannot be used to construct recurrent processes when \u2126 is uncountable. Fortunately, the hierarchical model introduced in this section addresses this issue: it yields a recurrent prior over continuous time jump processes over both countable and uncountable spaces \u2126 (see Section A).\nThe hierarchical process is constructed by making the base measure parameter of the rows shared and random. Formally, the model has the following form:\n\u00b50 \u223c MGP(H0, \u03b30) \u00b5 \u03b8 |\u00b50 iid \u223c MGP(\u00b50, \u03b20) \u03b8N+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c\u03bc \u03b8 N JN+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c Exp( \u00b5 \u03b8 N ).\nIn order to get a tractable predictive distribution, we introduce a set of auxiliary variables. These auxiliary variables can be compared to the variables used in the Chinese Restaurant Franchise (CRF) metaphor [20] to indicate when new tables are created in a given restaurant. In the HGEP, a restaurant can be understood as a row in the rate matrix, and tables, as groups of transitions to the same destination state. These auxiliary variables will be denoted by A n , where the event A n = 1 means informally that the n-th transition creates a new table. The variable takes value A n = 0 otherwise. See Section D in the Supplementary Material for a review of the CRF construction and a formal definition of the auxiliary variables A n .\nWe augment the sufficient statistics with empirical counts for the number of tables across all restaurants that share a given dish, G = N n=1 A n \u03b4 \u03b8n , and introduce one additional auxiliary variable, the normalization of the top level random measure, \u00b5 0 . This latter auxiliary variable has no equivalent in CRFs. As in the previous section, the normalization of the lower level random measures \u00b5 \u03b8 will be marginalized. Finally, we let:\n\u00b5 = G + H0 \u00b5 (H) \u03b8 = F \u03b8 + \u00b50 \u03bc where\u03bc (H) \u03b8\ncan be recognized as the mean parameter of the predictive distribution of the HDP. We use the superscript (H) to disambiguate from the non-hierarchical case. The main result of this section is (see Section B for the proof): \n(\u03b8N+1, JN+1) (X, {An} N n=1 , \u00b50 ) \u223c\u03bc (H) \u03b8 N \u00d7 TP( \u00b5 (H) \u03b8 N , \u03b2 \u03b8 N ).\nTo resample the auxiliary variable \u00b5 0 , a gamma-distributed Gibbs kernel can be used (see Section E of the Supplementary Material).", "publication_ref": ["b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Inference on partially observed sequences", "text": "In this section, we describe how to approximate expectations under the posterior distribution of GEPs, E[h(X)|Y], for a test function h on the hidden events X given observations Y. An example of function h on these events is to interpolate the progression of the disease in a patient with Multiple Sclerosis (MS) between two medical visits. We start by describing the form of the observations Y.\nNote that in most applications, the sequence of states is not directly nor fully observed. First, instead of observing the random variables \u03b8, inference is often carried from X -valued random variables Y n distributed according to a parametric family P indexed by the states \u03b8 of the chain, P = {L \u03b8 :\nF X \u2192 [0, 1], \u03b8 \u2208 \u2126}.\nSecond, the measurements are generally available only for a finite set of times T . To specify the random variables in question, we will need a notation for the event index at a given time t, I(t) = min N : 1, where I(t * ) = N \u2212 1), and for the individual observations, Y (t)|X \u223c L \u03b8 I(t) . The set of all observed random variable is then defined as\nN +1 n=1 J n > t (see Figure\nY = (Y (t 1 ), Y (t 2 ), . . . , Y (t G ) : t g < t g+1 , {t i } = T ) .\nFor simplicity, we assume in this section that P is a conjugate family with respect to H 0 . Nonconjugate models can be handled by incorporating the auxiliary variables of Algorithm 8 in [21]. We will describe inference on the model of Section 3. Extension to hierarchical models is direct (by keeping track of an additional sufficient statistic G, as well as the auxiliary variables A n , \u00b5 0 ).\nIn general, there may be several exchangeable sequences from which we want to learn a model. For example, we learned a model for MS disease progression by using time series from several patients. 4 We denote the number of time series by K, each of the form\nY (k) = Y (k) (t (k) 1 ), Y (k) (t (k) 2 ), . . . , Y (k) (t (k) G ) : t (k) g < t (k) g+1 , {t (k) i } = T (k) , k \u2208 {1, . . . , K}.\nAt a high-level, our inference algorithm works by resampling the hidden events X (k) for one sequence k given the sufficient statistics of the other sequences, (F\n(\\k) \u03b8 , T (\\k) \u03b8\n). This is done using a Sequential Monte Carlo (SMC) algorithm to construct a proposal over sequences of hidden events. Each particle in our SMC algorithm is a sequence of states and waiting times for the current sequence k. By using a Particle MCMC (PMCMC) method [1], we then compute an acceptance ratio  that makes this proposal a valid MCMC move. As we will see shortly, the acceptance is simply given by a ratio of marginal likelihood estimators, which can be computed directly from the unnormalized particle weights.\nFormally, the proposal is based on M particles propagated from generation g = 0 up to generation G, where G is equal to the number of measurements in the current sequence, G = |Y (k) |. Each particle X m,g , m \u2208 {1, . . . , M } consists of a list of hidden events indexed by n, containing both (hidden) states and waiting times: X m,g = (\u03b8 m,n , J m,n )\nNm,g n=1 . The pseudocode for the SMC algorithm, used for constructing the proposals, is presented in Figure 4 of the Supplementary Material. The next step is to compute an acceptance probability for a proposed sequence of states X (k) * . At each MCMC iteration, we assume that we store the value of the data likelihood estimates for the accepted state sequences. These data likelihood estimates are computed from the unnormalized weights \u03c0 g (described in Figure 4 of the Supplementary Material) as follows: k) be the estimate for the previously accepted sequence of states for observed sequence k, and let L (k) * be the estimate for the current MCMC iteration. The acceptance probability for the new sequence is given by min 1, L (k) * /L (k) . If it is accepted, we set\nL (k) = G g=1 \u03c0 g . Let L (\nL (k) = L (k) * .", "publication_ref": ["b20", "b3", "b0"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "In this section we present the results of our experiments. First, we demonstrate the behavior of state trajectories and sojourn times sampled from the prior to give a qualitative idea of the range of time series that can be captured by our model. Second, we evaluate quantitatively our model by applying it to three held-out tasks: synthetic, Multiple Sclerosis (MS) patients, and RNA evolutionary datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Qualitative behavior of the prior", "text": "We can distinguish at least four types of prior behaviors in the HGEP when considering different values for the parameters \u03b2 0 , H 0 and \u03b3 0 . We sampled a sequence of length T = 800 and present the state-time plots. Figure 2(a) shows a sequence with short sojourn times and high volatility of states, whereas Figure 2(b) depicts longer sojourn times with much less volatility. Figures 2(c) and 2(d) illustrate the effect of hyperparameter H 0 . In Figure 2(c) we can see creation of many new states and a sparse transition matrix. Likewise, in Figure 2(d) the high tendency to create new states is present, but we have longer sojourn times. See Section H of the supplementary material for a more detailed account of the interpretation and quantitative effect of the parameters.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Quantitative evaluation", "text": "In this section, we use a simple likelihood model for discrete observations (described in Section G of the supplementary material) to evaluate our method on three held-out tasks. Note that even when the observations are discrete, non-parametric models are still useful for better explaining the data using latent variables [22].\nWe considered three evaluation datasets obtained by holding out each observed datapoint with a 10% probability (see Table 1). We then reconstructed the observations at these held-out times, and measured the mean error. For HGEP, reconstruction was done by using the Bayes estimator approximated from 1000 posterior samples (one after each scan through all the time series). We repeated all experiments 5 times with different random seeds. We compared against the standard maximum likelihood rate matrix estimator learned by EM described in [23]. We also report in Table 1 the mean error for a simpler maximum likelihood estimate ignoring the sequential information (returning the most common observation deterministically). See Section G of the supplementary material for detailed instructions for replicating the following three results. 5 Refer also to Figure 3, where we show error as a function of the number of scans.", "publication_ref": ["b21", "b22", "b4"], "figure_ref": ["fig_3"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Synthetic:", "text": "We used an Erd\u00f6s-R\u00e9nyi model to generate a random sparse matrix of size 10 \u00d7 10, which we perturbed with uniform noise to get a random rate matrix. Both HGEP and the EM-learned maximum likelihood outperformed the baseline. In contrast to the next two tasks, the EM approach slightly outperformed the HGEP model here. We believe this is because the synthetic data was not sufficiently rich to highlight the advantages of HGEPs. However, we compared our results with iHMM after discretizing time. We observed that iHMM had an error rate of 0.47, underperforming both EM and HGEP.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MS disease progression:", "text": "This dataset, obtained from a phase III clinical trial, tracks the progression of MS in 72 patients over 3 years. The observed state of a patient at a given time is binned into three categories as customary in the MS literature [3]. Both HGEP and EM outperformed the baseline by a large margin, and our HGEP model outperformed EM with a relative error reduction of 22%.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "RNA evolution:", "text": "In this task, we used the dataset from [4] containing aligned 16S ribosomal RNA of species from the three domains of life. As a preprocessing, we constructed a rooted phylogenetic tree from a sample of 30 species, and performed ancestral reconstruction using a standard CTMC model and all the sampled taxa in the tree. We then considered the time series consisting of paths from one modern leaf to the root. The task is to reconstruct held-out nucleotides using only the data in this path. Again, both HGEP and EM outperformed the baseline, and our model outperformed EM with a relative error reduction of 29%.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have introduced a method for non-parametric Bayesian modeling of recurrent, continuous time processes. The model has attractive properties and we show that the posterior computations can be done efficiently using a sampler based on particle MCMC methods. Most importantly, our experiments show that the model is useful for analyzing complex real world time series.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to thank Arnaud Doucet, John Petkau and the anonymous reviewers for helpful comments. This work was supported by a NSERC Discovery Grant and the WestGrid cluster.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Particle Markov chain Monte Carlo methods", "journal": "Journal Of The Royal Statistical Society Series B", "year": "2010", "authors": "C Andrieu; A Doucet; R Holenstein"}, {"ref_id": "b1", "title": "Beam sampling for the infinite hidden Markov model", "journal": "", "year": "2008", "authors": "J Van Gael; Y Saatci; Y W Teh; Z Ghahramani"}, {"ref_id": "b2", "title": "Estimating disease progression using panel data", "journal": "Biostatistics", "year": "2010", "authors": "M Mandel"}, {"ref_id": "b3", "title": "The comparative RNA web (CRW) site: An online database of comparative sequence and structure information for ribosomal, intron, and other RNAs", "journal": "BioMed Central Bioinformatics", "year": "2002", "authors": "J J Cannone; S Subramanian; M N Schnare; J R Collett; L M Souza; Y Du; B Feng; N Lin; L V Madabusi; K M Muller; N Pande; Z Shang; N Yu; R R Gutell"}, {"ref_id": "b4", "title": "Dependent nonparametric processes", "journal": "", "year": "1999", "authors": "S N Maceachern"}, {"ref_id": "b5", "title": "The Ornstein-Uhlenbeck Dirichlet process and other time-varying processes for Bayesian nonparametric inference", "journal": "Journal of Statistical Planning and Inference", "year": "2008", "authors": "J E Griffin"}, {"ref_id": "b6", "title": "Stick-breaking autoregressive processes", "journal": "Journal of Econometrics", "year": "2011", "authors": "J E Griffin; M F J Steel"}, {"ref_id": "b7", "title": "The New Palgrave Dictionary of Economics, chapter Bayesian time series analysis. Palgrave Macmillan", "journal": "", "year": "2008", "authors": "M F J Steel"}, {"ref_id": "b8", "title": "A survey on nonparametric time series analysis", "journal": "", "year": "1999", "authors": "S Heiler"}, {"ref_id": "b9", "title": "The infinite hidden Markov model", "journal": "MIT Press", "year": "2002", "authors": "M J Beal; Z Ghahramani; C E Rasmussen"}, {"ref_id": "b10", "title": "An hdp-hmm for systems with state persistence", "journal": "", "year": "2008", "authors": "E B Fox; E B Sudderth; M I Jordan; A S Willsky"}, {"ref_id": "b11", "title": "The infinite factorial hidden Markov model", "journal": "", "year": "2008", "authors": "J Van Gael; Y W Teh; Z Ghahramani"}, {"ref_id": "b12", "title": "The Theory of Storage", "journal": "", "year": "1959", "authors": "P A P Moran"}, {"ref_id": "b13", "title": "Estimation in the Koziol-Green model using a gamma process prior", "journal": "Austrian Journal of Statistics", "year": "2008", "authors": "M "}, {"ref_id": "b14", "title": "Spatial normalized gamma processes", "journal": "", "year": "2009", "authors": "V Rao; Y W Teh"}, {"ref_id": "b15", "title": "Bayesian nonparametric inference for nonhomogeneous Poisson processes", "journal": "", "year": "1997", "authors": "L Kuo; S K Ghosh"}, {"ref_id": "b16", "title": "Poisson Processes. The", "journal": "Clarendon Press Oxford University Press", "year": "1993", "authors": "J F C Kingman"}, {"ref_id": "b17", "title": "Risk-neutral parameter shifts and derivatives pricing in discrete time", "journal": "The Journal of Finance", "year": "2004", "authors": "M Schroder"}, {"ref_id": "b18", "title": "G distributions and the beta-gamma algebra", "journal": "Electronic Journal of Probability", "year": "2010", "authors": "D Dufresne"}, {"ref_id": "b19", "title": "Hierarchical Dirichlet processes", "journal": "Journal of the American Statistical Association", "year": "2004", "authors": "Y W Teh; M I Jordan; M J Beal; D M Blei"}, {"ref_id": "b20", "title": "Markov chain sampling methods for Dirichlet process mixture models", "journal": "", "year": "2000", "authors": "R Neal"}, {"ref_id": "b21", "title": "The infinite PCFG using hierarchical Dirichlet processes", "journal": "", "year": "2007", "authors": "P Liang; S Petrov; M I Jordan; D Klein"}, {"ref_id": "b22", "title": "Statistical inference in evolutionary models of DNA sequences via the EM algorithm", "journal": "Statistical applications in Genetics and Molecular Biology", "year": "2005", "authors": "A Hobolth; J L Jensen"}, {"ref_id": "b23", "title": "Inferring complex DNA substitution processes on phylogenies using uniformization and data augmentation", "journal": "Syst. Biol", "year": "2006", "authors": "L Mateiu; B Rannala"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "J", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: (a) An illustration of our notation for samples from CTMPs. We assume the state space (\u2126) is countable. The notation for the observations Y (t1), . . . , Y (tG) is described in Section 5. (b) Graphical model for the hierarchical model of Section 4. For simplicity we only show a single J and \u03b8.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "1 Figure 2 :12Figure 2: Qualitative behavior of the prior", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Mean reconstruction error on the held-out data as a function of the number of Gibbs scans. Lower is better. The standard maximum likelihood estimate learned with EM outperformed our model in the simple synthetic dataset, but the trend was reversed in the more complex real world datasets.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary statistics and mean error results for the experiments. All experiments were repeated 5 times.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "f (t) = (\u03b10 + n)(\u03b20 + T ) (\u03b1 0 +n) (\u03b20 + T + t) \u03b1 0 +n+1 ,", "formula_coordinates": [2.0, 242.6, 181.37, 126.8, 21.52]}, {"formula_id": "formula_1", "formula_text": "= q i,j 1[i = j].", "formula_coordinates": [2.0, 404.83, 620.36, 61.98, 9.68]}, {"formula_id": "formula_2", "formula_text": "\u03b8 J \u03b8 \u03b8 t = 0 J =0 . . . J \u03b8 t * Y(t ) Y(t ) . . . t Y(t ) \u03b8 X Y Y(t ) Y(t ) \u03a9 (a) (b)", "formula_coordinates": [3.0, 126.48, 99.06, 308.96, 126.38]}, {"formula_id": "formula_3", "formula_text": "F \u2126 \u2192 (F \u2126 \u2192 [0, \u221e)", "formula_coordinates": [3.0, 108.0, 424.92, 88.24, 9.65]}, {"formula_id": "formula_4", "formula_text": "\u00b5 \u03b8 iid \u223c MGP(H0, \u03b20) \u2200\u03b8 \u2208 \u2126 \u03b8N+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c\u03bc \u03b8 N JN+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c Exp ( \u00b5 \u03b8 N )", "formula_coordinates": [3.0, 221.84, 611.41, 168.31, 41.09]}, {"formula_id": "formula_5", "formula_text": "\u00b5 \u03b8 (i) (\u03bc \u03b8 (i) ({i}) \u2212 1) otherwise.", "formula_coordinates": [4.0, 112.98, 85.02, 137.67, 10.46]}, {"formula_id": "formula_6", "formula_text": "F \u03b8 = N n=1 1[\u03b8n\u22121 = \u03b8] \u03b4 \u03b8n , T \u03b8 = N n=1 1[\u03b8n\u22121 = \u03b8] Jn. Proposition 2. The Gamma-Exponential Process (GEP) is a conjugate family, \u00b5 \u03b8 |X \u223c MGP (\u00b5 \u03b8 , \u03b2 \u03b8 ) , where \u00b5 \u03b8 = F \u03b8 + H 0 and \u03b2 \u03b8 = T \u03b8 + \u03b2 0 .", "formula_coordinates": [4.0, 108.0, 232.6, 396.0, 58.4]}, {"formula_id": "formula_7", "formula_text": "Lemma 3. If V \u223c Beta(a, b) and W \u223c Gamma(a + b, c) are independent, then V W \u223c Gamma(a, c).", "formula_coordinates": [4.0, 108.0, 360.84, 396.0, 19.77]}, {"formula_id": "formula_8", "formula_text": "(\u00b5(A1), \u00b5(A2), . . . , \u00b5(AK ))|X \u223c Gamma(\u00b5 (A1), \u03b2 ) \u00d7 \u2022 \u2022 \u2022 \u00d7 Gamma(\u00b5 (A k ), \u03b2 ).", "formula_coordinates": [4.0, 145.42, 489.09, 321.16, 8.35]}, {"formula_id": "formula_9", "formula_text": "V = \u0393 1 /\u0393 0 , W = \u0393 0 , then V \u223c Beta(H 0 (A 1 ), H 0 (A 2 )), W \u223c Gamma(\u03b1 0 , \u03b2 0 )", "formula_coordinates": [4.0, 108.0, 529.44, 325.23, 9.65]}, {"formula_id": "formula_10", "formula_text": "(V |X) = (V |\u03b8 1 , . . . , \u03b8 N ) \u223c Beta(\u00b5 (A 1 ), \u00b5 (A 2 )", "formula_coordinates": [4.0, 129.82, 551.36, 207.87, 9.65]}, {"formula_id": "formula_11", "formula_text": "W |X \u223c Gamma( \u00b5 , \u03b2 ).", "formula_coordinates": [4.0, 143.06, 562.32, 112.65, 8.74]}, {"formula_id": "formula_12", "formula_text": "a = \u00b5 (A 1 ), b = \u00b5 (A 2 ), c = \u03b2 , we finally get that (\u00b5(A 1 )|X) = (V W |X) \u223c Gamma(\u00b5 (A 1 ), \u03b2", "formula_coordinates": [4.0, 108.0, 579.25, 396.0, 20.61]}, {"formula_id": "formula_13", "formula_text": "f (t) = 1[t > 0]\u03b1\u03b2 \u03b1 (t + \u03b2) \u03b1+1 .(1)", "formula_coordinates": [4.0, 265.65, 670.71, 238.35, 21.52]}, {"formula_id": "formula_14", "formula_text": "(\u03b8N+1, JN+1)|X \u223c\u03bc \u03b8 N \u00d7 TP( \u00b5 \u03b8 N , \u03b2 \u03b8 N ).(2)", "formula_coordinates": [4.0, 220.52, 720.75, 283.48, 10.01]}, {"formula_id": "formula_15", "formula_text": "p(t) \u221d x>0 x \u03b1 0 \u22121 exp(\u2212\u03b20x) \u2022 x exp(\u2212xt) dx = x>0 x \u03b1 0 exp (\u2212(\u03b20 + t)x) dx = \u0393(\u03b10 + 1) (\u03b20 + t) \u03b1 0 +1 Hence J \u223c TP(\u03b1 0 , \u03b2 0 ).", "formula_coordinates": [5.0, 110.49, 152.35, 297.82, 56.82]}, {"formula_id": "formula_16", "formula_text": "p(j1, j2, . . . , jK ) = 1[j k > 0, k \u2208 {1, . . . , K}](\u03b10)K \u03b2 \u03b1 0 0 (\u03b20 + j1 + \u2022 \u2022 \u2022 + jK ) \u03b1 0 +K (3)", "formula_coordinates": [5.0, 198.18, 293.76, 305.82, 21.99]}, {"formula_id": "formula_17", "formula_text": ") n = x(x + 1) \u2022 \u2022 \u2022 (x + n \u2212 1).", "formula_coordinates": [5.0, 316.58, 321.02, 125.95, 9.65]}, {"formula_id": "formula_18", "formula_text": "\u00b50 \u223c MGP(H0, \u03b30) \u00b5 \u03b8 |\u00b50 iid \u223c MGP(\u00b50, \u03b20) \u03b8N+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c\u03bc \u03b8 N JN+1 X, {\u00b5 \u03b8 } \u03b8\u2208\u2126 \u223c Exp( \u00b5 \u03b8 N ).", "formula_coordinates": [5.0, 165.32, 505.64, 287.08, 21.38]}, {"formula_id": "formula_19", "formula_text": "\u00b5 = G + H0 \u00b5 (H) \u03b8 = F \u03b8 + \u00b50 \u03bc where\u03bc (H) \u03b8", "formula_coordinates": [5.0, 108.0, 681.78, 287.16, 30.52]}, {"formula_id": "formula_20", "formula_text": "(\u03b8N+1, JN+1) (X, {An} N n=1 , \u00b50 ) \u223c\u03bc (H) \u03b8 N \u00d7 TP( \u00b5 (H) \u03b8 N , \u03b2 \u03b8 N ).", "formula_coordinates": [6.0, 183.27, 291.7, 245.46, 13.55]}, {"formula_id": "formula_21", "formula_text": "F X \u2192 [0, 1], \u03b8 \u2208 \u2126}.", "formula_coordinates": [6.0, 108.0, 464.8, 92.92, 9.65]}, {"formula_id": "formula_22", "formula_text": "N +1 n=1 J n > t (see Figure", "formula_coordinates": [6.0, 251.01, 487.4, 100.93, 14.11]}, {"formula_id": "formula_23", "formula_text": "Y = (Y (t 1 ), Y (t 2 ), . . . , Y (t G ) : t g < t g+1 , {t i } = T ) .", "formula_coordinates": [6.0, 118.79, 517.86, 223.86, 9.65]}, {"formula_id": "formula_24", "formula_text": "Y (k) = Y (k) (t (k) 1 ), Y (k) (t (k) 2 ), . . . , Y (k) (t (k) G ) : t (k) g < t (k) g+1 , {t (k) i } = T (k) , k \u2208 {1, . . . , K}.", "formula_coordinates": [6.0, 124.91, 623.46, 362.19, 12.34]}, {"formula_id": "formula_25", "formula_text": "(\\k) \u03b8 , T (\\k) \u03b8", "formula_coordinates": [6.0, 369.13, 658.16, 42.94, 14.3]}, {"formula_id": "formula_26", "formula_text": "L (k) = G g=1 \u03c0 g . Let L (", "formula_coordinates": [7.0, 108.0, 329.52, 396.0, 26.11]}, {"formula_id": "formula_27", "formula_text": "L (k) = L (k) * .", "formula_coordinates": [7.0, 375.28, 371.71, 51.59, 12.7]}], "doi": ""}