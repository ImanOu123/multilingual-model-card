{"title": "The Life and Death of Discourse Entities: Identifying Singleton Mentions", "authors": "Marta Recasens; Marie-Catherine De Marneffe; Christopher Potts", "pub_date": "", "abstract": "A discourse typically involves numerous entities, but few are mentioned more than once. Distinguishing discourse entities that die out after just one mention (singletons) from those that lead longer lives (coreferent) would benefit NLP applications such as coreference resolution, protagonist identification, topic modeling, and discourse coherence. We build a logistic regression model for predicting the singleton/coreferent distinction, drawing on linguistic insights about how discourse entity lifespans are affected by syntactic and semantic features. The model is effective in its own right (78% accuracy), and incorporating it into a state-of-the-art coreference resolution system yields a significant improvement.", "sections": [{"heading": "Introduction", "text": "Not all discourse entities are created equal. Some lead long lives and appear in a variety of discourse contexts (coreferent), whereas others never escape their birthplaces, dying out after just one mention (singletons). The ability to make this distinction based on properties of the NPs used to identify these referents (mentions) would benefit not only coreference resolution, but also topic analysis, textual entailment, and discourse coherence.\nThe existing literature provides numerous generalizations relevant to answering the question of whether a given discourse entity will be singleton or coreferent. These involve the internal syntax and morphology of the target NP (Prince, 1981a;Prince, 1981b;Wang et al., 2006), the grammatical function and discourse role of that NP (Chafe, 1976;Hobbs, 1979;Walker et al., 1997;Beaver, 2004), and the interaction of all of those features with semantic operators like negation, modals, and attitude predicates (Karttunen, 1973;Karttunen, 1976;Kamp, 1981;Heim, 1982;Heim, 1992;Roberts, 1990;Groenendijk and Stokhof, 1991;Bittner, 2001).\nThe first step in our analysis is to bring these insights together into a single logistic regression model -the lifespan model -and assess their predictive power on real data. We show that the features generally behave as the existing literature leads us to expect, and that the model itself is highly effective at predicting whether a given mention is singleton or coreferent. We then provide an initial assessment of the engineering value of making the singleton/coreferent distinction by incorporating our lifespan model into the Stanford coreference resolution system (Lee et al., 2011). This addition results in a significant improvement on the CoNLL-2012 Shared Task data, across the MUC, B 3 , CEAF, and CoNLL scoring algorithms.", "publication_ref": ["b31", "b32", "b45", "b6", "b18", "b43", "b3", "b23", "b24", "b22", "b16", "b17", "b36", "b12", "b4", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "All the data used throughout the paper come from the CoNLL-2012Shared Task (Pradhan et al., 2012, which included the 1.6M English words from OntoNotes v5.0 (Hovy et al., 2006) that have been annotated with different layers of annotation (coreference, parse trees, etc.). We used the training, development (dev), and test splits as defined in the shared task (Table 1). Since the OntoNotes coreference annotations do not contain singleton mentions, we automatically marked as singletons all the NPs  not annotated as coreferent. Thus, our singletons include non-referential NPs but not verbal mentions.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Predicting lifespans", "text": "Our lifespan model makes a binary distinction between discourse referents that are not part of a coreference chain (singletons) and items that are part of one (coreferent). The distribution of lifespans in our data (Figure 1) suggests that this is a natural division. The propensity of singletons also highlights the relevance of detecting singletons for a coreference system. We fit a binary logistic regression model in R (R Core Team, 2012) on the training data, coding singletons as \"0\" and coreferent mentions as \"1\". Throughout the following tables of coefficient estimates, positive values favor coreferents and negative ones favor singletons. We turn now to describing and motivating the features of this model. Internal morphosyntax of the mention Table 2 summarizes the features from our model that concern the internal morphology and syntactic structure of the mention. Many are common in coreference systems (Recasens and Hovy, 2009), but our model highlights their influence on lifespans. The picture is expected on the taxonomy of given and new defined by Prince (1981b) and assumed throughout dynamic semantics (Kamp, 1981;Heim, 1982): pronouns depend on anaphoric connections to previous mentions for disambiguation and thus are very likely to be coreferent. This is corroborated by the positive coefficient estimate for 'Type = pronoun' in Table 2. Few quantified phrases easily participate in discourse anaphora (Partee, 1987;Wang et al., 2006), accounting for the association between quantifiers and singletons (negative coefficient estimate for 'Quantifier = quantified' in Table 2). The one surprise is the negative coefficient for indefinites. In theories stretching back to Karttunen (1976), indefinites function primarily to establish new discourse entities, and should be able to participate in coreference chains, but here the association with such chains is negative. However, interactions explain this fact (see Table 4 and our discussion of it).\nThe person, number, and animacy values suggest that singular animates are excellent coreferent NPs, a previous finding of Centering Theory (Grosz et al., 1995;Walker et al., 1998) and of cross-linguistic work on obviative case-marking (Aissen, 1997).\nOur model also includes named-entity features for all of the eighteen OntoNotes entity-types (omitted from  Grammatical role of the mention Synthesizing much work in Centering Theory and information structuring, we conclude that coreferent mentions are likely to appear as core verbal arguments and will favor sentence-initial (topic-tracking) positions (Ward and Birner, 2004  Semantic environment of the mention Table 4 highlights the complex interactions between discourse anaphora and semantic operators. These interactions have been a focus of logical semantics since Karttunen (1976), whose guiding observation is semantic: an indefinite interpreted inside the scope of a negation, modal, or attitude predicate is generally unavailable for anaphoric reference outside of the scope of that operator, as in Kim didn't understand [an exam question] i . # It i was too hard.\nOf course, such discourses cohere if the indefinite is interpreted as taking wide scope ('there is a question Kim didn't understand'). Such readings are often disfavored, but they become more salient when modifiers like certain are included (Schwarzschild, 2002) or when the determiner is sensitive to the polarity or intensionality of its environment (Baker, 1970;Ladusaw, 1980;van der Wouden, 1997;Israel, 1996;Israel, 2001;Giannakidou, 1999). Subsequent research identified many other factors that further extend or restrict the anaphoric potential of an indefinite (Roberts, 1996). We do not have direct access to semantic scope, but we expect syntactic scope to correlate strongly with semantic scope, so we used dependency representations to define features capturing syntactic scope for negation, modal auxiliaries, and a broad range of attitude predicates. These features tend to bias in favor of singletons because they so radically restrict the possibilities for intersentential anaphora.\nInteracting these features with those for the internal syntax of mentions is also informative. Since proper names and pronouns are not scope-taking, they are largely unaffected by the environment features, whereas indefinites emerge as even more restricted, just as Karttunen and others would predict.\nAttitude predicates seem initially anomalous, though. They share the relevant semantic properties with negation and modals, and yet they seem to facilitate coreference. Here, the findings of de Marneffe et al. ( 2012) seem informative. Those authors find that, in texts of the sort we are studying, attitude predicates are used predominantly to mark the source of information that is effectively asserted despite being embedded (Rooryck, 2001;Simons, 2007). That is, though X said p does not semantically entail p, it is often interpreted as a commitment to p, which correspondingly elevates mentions in p to main-clause status (Harris and Potts, 2009).  ", "publication_ref": ["b34", "b32", "b22", "b16", "b29", "b45", "b24", "b13", "b44", "b0", "b46", "b24", "b39", "b2", "b25", "b41", "b20", "b21", "b11", "b37", "b38", "b40", "b14"], "figure_ref": ["fig_0"], "table_ref": ["tab_2", "tab_2", "tab_2", "tab_6", "tab_6"]}, {"heading": "Results", "text": "The model successfully learns to tease singletons and coreferent mentions apart.     Cai and Strube (2010). Scores are on automatically predicted mentions.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Application to coreference resolution", "text": "To assess the usefulness of the lifespan model in an NLP application, we incorporate it into the Stanford coreference resolution system (Lee et al., 2011), which we take as our baseline. This was the highestscoring system in the CoNLL-2011 Shared Task, and was also part of the highest-scoring system in the CoNLL-2012 Shared Task (Fernandes et al., 2012). It is a rule-based system that includes a total of ten rules (or \"sieves\") for entity coreference, such as exact string match and pronominal resolution. The sieves are applied from highest to lowest precision, each rule adding coreference links.\nIncorporating the lifespan model The lifespan model can improve coreference resolution in two different ways: (i) mentions classified as singletons should not be considered as either antecedents or coreferent, and (ii) mentions classified as coreferent should be linked with another mention(s). By successfully predicting singletons (i), we can enhance the system's precision; by successfully predicting coreferent mentions (ii), we can improve the system's recall. Here we focus on (i) and use the lifespan model for detecting singletons. This decision is motivated by two factors. First, given the large number of singletons (Figure 1), we are more likely to see a gain in performance from discarding singletons. Second, the multi-sieve nature of the Stanford coreference system does not make it straightforward to decide which antecedent a mention should be linked to even if we know that it is coreferent.\nWe leave the incorporation of coreferent predictions for future work.\nTo integrate the singleton model into the Stanford coreference system, we let a sieve consider whether a pair of mentions is coreferent only if neither of the two mentions are classified as singletons by our CONFIDENT model. Experiments on the dev set showed that the model often made wrong predictions for NEs. We do not trust the model for NE mentions. Performance on coreference (on the dev set) was higher with the CONFIDENT model than with the STANDARD model.", "publication_ref": ["b26", "b10"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Results and discussion", "text": "To evaluate the coreference system with and without the lifespan model, we used the English dev and test sets from the CoNLL-2012 Shared Task, presented in Section 2. Although the CoNLL shared task evaluated systems on only multi-mention (i.e., non-singleton) entities, by stopping singletons from being linked to multi-mention entities, we expected the lifespan model to increase the system's precision. Our evaluation uses five of the measures given by the CoNLL-2012 scorer: MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF-\u03c6 3 and CEAF-\u03c6 4 (Luo, 2005), and the CoNLL official score (Denis and Baldridge, 2009). We do not include BLANC (Recasens and Hovy, 2011) because it assumes gold mentions and so is not suited for the scenario considered in this paper, which uses automatically predicted mentions.\nTable 6 summarizes the test set performance. All the scores are on automatically predicted mentions. We use gold POS, parse trees, and NEs. The base-line is the Stanford system, and 'w/ Lifespan' is the same system extended with our lifespan model to discard singletons, as explained above.\nAs expected, the lifespan model increases precision but decreases recall. Overall, however, we obtain a significant improvement of 0.5-1 points in the F1 score of MUC, CEAF-\u03c6 3 , CEAF-\u03c6 4 and CoNLL. The drop in B 3 traces to a bug in the CoNLL scorer's implementation of Cai and Strube (2010)'s algorithm for aligning gold and automatically predicted mentions, which affects the computation of B 3 and CEAF-\u03c6 3 . 1 Table 7 presents the results after modifying the CoNLL-2012 scorer to compute B 3 and CEAF-\u03c6 3 according to Cai and Strube (2010). 2 We do see an improvement in the precision and F1 scores of B 3 , and the overall CoNLL score remains significant. The CEAF-\u03c6 3 F1 score is no longer significant, but is still in the expected direction.", "publication_ref": ["b1", "b28", "b9", "b35", "b5", "b5"], "figure_ref": [], "table_ref": ["tab_9", "tab_10"]}, {"heading": "Conclusion", "text": "We built a model to predict the lifespan of discourse referents, teasing apart singletons from coreferent mentions. The model validates existing linguistic insights and performs well in its own right. This alone has ramifications for tracking topics, identifying protagonists, and modeling coreference and discourse coherence. We applied the lifespan model to coreference resolution, showing how to incorporate it effectively into a state-of-the-art rule-based coreference system. We expect similar improvements with machine-learning-based coreference systems, where incorporating all the power of the lifespan model would be easier.\nOur lifespan model has been integrated into the latest version of the Stanford coreference resolution system. 3 ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Emili Sapena for modifying the CoNLL-2012 scorer to follow Cai and Strube (2010).\nThis research was supported in part by ONR grant No. N00014-10-1-0109 and ARO grant No. W911NF-07-1-0216. The first author was supported by a Beatriu de Pin\u00f3s postdoctoral scholarship (2010 BP-A 00149) from Generalitat de Catalunya.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On the syntax of obviation", "journal": "", "year": "1997", "authors": "Judith Aissen"}, {"ref_id": "b1", "title": "Algorithms for scoring coreference chains", "journal": "", "year": "1998", "authors": "Amit Bagga; Breck Baldwin"}, {"ref_id": "b2", "title": "Double negatives", "journal": "Linguistic Inquiry", "year": "1970", "authors": "C L Baker"}, {"ref_id": "b3", "title": "The optimization of discourse anaphora", "journal": "Linguistics and Philosophy", "year": "2004", "authors": "David Beaver"}, {"ref_id": "b4", "title": "Surface composition as bridging", "journal": "Journal of Semantics", "year": "2001", "authors": "Maria Bittner"}, {"ref_id": "b5", "title": "Evaluation metrics for end-to-end coreference resolution systems", "journal": "", "year": "2010", "authors": "Jie Cai; Michael Strube"}, {"ref_id": "b6", "title": "Givenness, Contrastiveness, Definiteness, Subjects, Topics, and Point of View", "journal": "Academic Press", "year": "1976", "authors": "Wallace L Chafe"}, {"ref_id": "b7", "title": "Generating typed dependency parses from phrase structure parses", "journal": "", "year": "2006", "authors": "Marie-Catherine De Marneffe; Bill Maccartney; Christopher D Manning"}, {"ref_id": "b8", "title": "Did it happen? The pragmatic complexity of veridicality assessment. Computational Linguistics", "journal": "", "year": "2012", "authors": "Marie-Catherine De Marneffe; Christopher D Manning; Christopher Potts"}, {"ref_id": "b9", "title": "Global joint models for coreference resolution and named entity classification", "journal": "", "year": "2009", "authors": "Pascal Denis; Jason Baldridge"}, {"ref_id": "b10", "title": "Latent structure perceptron with feature induction for unrestricted coreference resolution", "journal": "", "year": "2012", "authors": "Eraldo Fernandes; Santos C\u00edcero Dos; Ruy Milidi\u00fa"}, {"ref_id": "b11", "title": "Affective dependencies", "journal": "Linguistics and Philosophy", "year": "1999", "authors": "Anastasia Giannakidou"}, {"ref_id": "b12", "title": "Dynamic predicate logic", "journal": "Linguistics and Philosophy", "year": "1991", "authors": "Jeroen Groenendijk; Martin Stokhof"}, {"ref_id": "b13", "title": "Centering: A framework for modeling the local coherence of discourse", "journal": "Computational Linguistics", "year": "1995", "authors": "Barbara J Grosz; Aravind K Joshi; Scott Weinstein"}, {"ref_id": "b14", "title": "", "journal": "", "year": "2009", "authors": "Jesse A Harris; Christopher Potts"}, {"ref_id": "b15", "title": "Perspective-shifting with appositives and expressives", "journal": "Linguistics and Philosophy", "year": "", "authors": ""}, {"ref_id": "b16", "title": "The Semantics of Definite and Indefinite Noun Phrases", "journal": "", "year": "1982", "authors": "Irene Heim"}, {"ref_id": "b17", "title": "Presupposition projection and the semantics of attitude verbs", "journal": "Journal of Semantics", "year": "1992", "authors": "Irene Heim"}, {"ref_id": "b18", "title": "", "journal": "Coherence and coreference. Cognitive Science", "year": "1979", "authors": "Jerry R Hobbs"}, {"ref_id": "b19", "title": "OntoNotes: The 90% solution", "journal": "", "year": "2006", "authors": "Eduard Hovy; Mitchell Marcus; Martha Palmer; Lance Ramshaw; Ralph Weischedel"}, {"ref_id": "b20", "title": "Polarity sensitivity as lexical semantics", "journal": "Linguistics and Philosophy", "year": "1996", "authors": "Michael Israel"}, {"ref_id": "b21", "title": "Minimizers, maximizers, and the rhetoric of scalar reasoning", "journal": "Journal of Semantics", "year": "2001", "authors": "Michael Israel"}, {"ref_id": "b22", "title": "A theory of truth and discourse representation", "journal": "Mathematical Centre", "year": "1981", "authors": "Hans Kamp"}, {"ref_id": "b23", "title": "Presuppositions and compound sentences", "journal": "Linguistic Inquiry", "year": "1973", "authors": "Lauri Karttunen"}, {"ref_id": "b24", "title": "Discourse referents", "journal": "Academic Press", "year": "1976", "authors": "Lauri Karttunen"}, {"ref_id": "b25", "title": "On the notion 'affective' in the analysis of negative polarity items", "journal": "Journal of Linguistic Research", "year": "1980", "authors": "William A Ladusaw"}, {"ref_id": "b26", "title": "", "journal": "", "year": "2011", "authors": "Heeyoung Lee; Yves Peirsman; Angel Chang; Nathanael Chambers; Mihai Surdeanu; Dan Jurafsky"}, {"ref_id": "b27", "title": "Stanford's multi-pass sieve coreference resolution system at the CoNLL-2011 Shared Task", "journal": "", "year": "", "authors": ""}, {"ref_id": "b28", "title": "On coreference resolution performance metrics", "journal": "", "year": "2005", "authors": "Xiaoqiang Luo"}, {"ref_id": "b29", "title": "Noun phrase interpretation and type-shifting principles", "journal": "Foris Publications", "year": "1987", "authors": "H Barbara;  Partee"}, {"ref_id": "b30", "title": "CoNLL-2012 Shared Task: Modeling multilingual unrestricted coreference in OntoNotes", "journal": "", "year": "2012", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Olga Xue; Yuchen Uryupina;  Zhang"}, {"ref_id": "b31", "title": "On the inferencing of indefinite 'this' NPs", "journal": "Cambridge University Press", "year": "1981", "authors": "Ellen Prince"}, {"ref_id": "b32", "title": "Toward a taxonomy of givennew information", "journal": "Academic Press", "year": "1981", "authors": "Ellen F Prince"}, {"ref_id": "b33", "title": "R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing", "journal": "", "year": "2012", "authors": " R Core Team"}, {"ref_id": "b34", "title": "A deeper look into features for coreference resolution", "journal": "Springer", "year": "2009", "authors": "Marta Recasens; Eduard Hovy"}, {"ref_id": "b35", "title": "BLANC: Implementing the Rand index for coreference evaluation", "journal": "Natural Language Engineering", "year": "2011", "authors": "Marta Recasens; Eduard Hovy"}, {"ref_id": "b36", "title": "Modal Subordination, Anaphora, and Distributivity", "journal": "", "year": "1990", "authors": "Craige Roberts"}, {"ref_id": "b37", "title": "The Handbook of Contemporary Semantic Theory", "journal": "Blackwell Publishers", "year": "1996", "authors": "Craige Roberts"}, {"ref_id": "b38", "title": "", "journal": "Evidentiality, Part II. Glot International", "year": "2001", "authors": "Johan Rooryck"}, {"ref_id": "b39", "title": "Singleton indefinites", "journal": "Journal of Semantics", "year": "2002", "authors": "Roger Schwarzschild"}, {"ref_id": "b40", "title": "Observations on embedding verbs, evidentiality, and presupposition", "journal": "Lingua", "year": "2007", "authors": "Mandy Simons"}, {"ref_id": "b41", "title": "Negative Contexts: Collocation, Polarity and Multiple Negation", "journal": "", "year": "1997", "authors": "Ton Van Der Wouden"}, {"ref_id": "b42", "title": "Dennis Connolly, and Lynette Hirschman. 1995. A modeltheoretic coreference scoring scheme", "journal": "", "year": "", "authors": "Marc Vilain; John Burger; John Aberdeen"}, {"ref_id": "b43", "title": "Centering in Discourse", "journal": "Oxford University Press", "year": "1997", "authors": "Marilyn A Walker; Aravind K Joshi; Ellen F Prince"}, {"ref_id": "b44", "title": "Centering in naturally-occurring discourse: An overview", "journal": "Clarendon Press", "year": "1998", "authors": "Marilyn A Walker; Aravind K Joshi; Ellen F Prince"}, {"ref_id": "b45", "title": "Information dependency in quantificational subordination", "journal": "Elsevier Science", "year": "2006", "authors": "Linton Wang; Eric Mccready; Nicholas Asher"}, {"ref_id": "b46", "title": "Information structure and non-canonical syntax", "journal": "", "year": "2004", "authors": "Gregory Ward; Betty Birner"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Distribution of lifespans in the dev set. Singletons account for 56% of the data.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "CoNLL-2012  Shared Task data statistics. We added singletons (NPs not annotated as coreferent).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "for space and clarity reasons). As a rule, they behave like 'Type = proper noun' in associating with coreferents. The exceptions are ORDI-NAL, PERCENT, and QUANTITY, which seem intuitively unlikely to participate in coreference chains.", "figure_data": "EstimateP-valueType = pronoun1.21 < 0.001Type = proper noun1.88 < 0.001Animacy = inanimate\u22121.36 < 0.001Animacy = unknown\u22120.38 < 0.001Person = 11.05 < 0.001Person = 20.13 < 0.001Person = 31.62 < 0.001Number = singular0.61 < 0.001Number = unknown0.17 < 0.001Quantifier = indefinite\u22121.49 < 0.001Quantifier = quantified\u22121.23 < 0.001Number of modifiers\u22120.39 < 0.001"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Internal morphosyntactic features.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Grammatical role features.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Semantic environment features and interactions.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "summarizes its performance on the dev set. TheSTANDARD model uses 0.5 as the decision bound-ary, with 78% accuracy. The CONFIDENT modelpredicts singleton if Pr < .2 and coreferent if Pr > .8,which increases precision (P) at a cost to recall (R).STANDARDCONFIDENTPredictionRPF1RPF1Singleton82.3 79.2 80.750.5 89.6 64.6Coreferent72.2 76.1 74.141.3 86.8 55.9"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Recall, precision, and F1 for the lifespan model. Lifespan 66.08 67.33* 66.70* 66.40 73.14* 69.61 58.83* 47.77* 46.38 47.07* 61.13*", "figure_data": "MUCB 3CEAF-\u03c6 3CEAF-\u03c6 4CoNLLSystemRPF1RPF1R / P / F1RPF1F1Baseline66.64* 64.72 65.6768.05* 71.58 69.77*58.3145.49 47.55* 46.5060.65w/"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Performance on the test set according to the official CoNLL-2012 scorer. Scores are on automatically predicted mentions. Stars indicate a statistically significant difference (paired Mann-Whitney U-test, p < 0.05).", "figure_data": "B 3CEAF-\u03c6 3CoNLLSystemRPF1RPF1F1Baseline58.53* 71.58 64.4063.71* 58.31 60.8958.86w/ Lifespan58.14 73.14* 64.78*63.38 58.83* 61.0259.52*"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "B 3 , CEAF-\u03c6 3 and CoNLL measures on the test set according to a modified CoNLL-2012 scorer that follows", "figure_data": ""}], "formulas": [], "doi": ""}