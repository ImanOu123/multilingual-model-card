{"title": "Is Out-of-Distribution Detection Learnable?", "authors": "Zhen Fang; Yixuan Li; Jie Lu; Jiahua Dong; Bo Han; Feng Liu", "pub_date": "2023-02-23", "abstract": "Supervised learning aims to train a classifier under the assumption that training and test data are from the same distribution. To ease the above assumption, researchers have studied a more realistic setting: out-of-distribution (OOD) detection, where test data may come from classes that are unknown during training (i.e., OOD data). Due to the unavailability and diversity of OOD data, good generalization ability is crucial for effective OOD detection algorithms. To study the generalization of OOD detection, in this paper, we investigate the probably approximately correct (PAC) learning theory of OOD detection, which is proposed by researchers as an open problem. First, we find a necessary condition for the learnability of OOD detection. Then, using this condition, we prove several impossibility theorems for the learnability of OOD detection under some scenarios. Although the impossibility theorems are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we next give several necessary and sufficient conditions to characterize the learnability of OOD detection in some practical scenarios. Lastly, we also offer theoretical supports for several representative OOD detection works based on our OOD theory.", "sections": [{"heading": "Introduction", "text": "The success of supervised learning is established on an implicit assumption that training and test data share a same distribution, i.e., in-distribution (ID) [1,2,3,4]. However, test data distribution in many real-world scenarios may violate the assumption and, instead, contain out-of-distribution (OOD) data whose labels have not been seen during the training process [5,6]. To mitigate the risk of OOD data, researchers have considered a more practical learning scenario: OOD detection which determines whether an input is ID/OOD, while classifying the ID data into respective classes. OOD detection has shown great potential to ensure the reliable deployment of machine learning models in the real world. A rich line of algorithms have been developed to empirically address the OOD detection problem [6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]. However, very few works study theory of OOD detection, which hinders the rigorous path forward for the field. This paper aims to bridge the gap.\nIn this paper, we provide a theoretical framework to understand the learnability of the OOD detection problem. We investigate the probably approximately correct (PAC) learning theory of OOD detection, which is posed as an open problem to date. Unlike the classical PAC learning theory in a supervised setting, our problem setting is fundamentally challenging due to the absence of OOD data in training.\nIn many real-world scenarios, OOD data can be diverse and priori-unknown. Given this, we study whether there exists an algorithm that can be used to detect various OOD data instead of merely some specified OOD data. Such is the significance of studying the learning theory for OOD detection [4]. This motivates our question: is OOD detection PAC learnable? i.e., is there the PAC learning theory to guarantee the generalization ability of OOD detection?\nTo investigate the learning theory, we mainly focus on two basic spaces: domain space and hypothesis space. The domain space is a space consisting of some distributions, and the hypothesis space is a space consisting of some classifiers. Existing agnostic PAC theories in supervised learning [21,22] are distribution-free, i.e., the domain space consists of all domains. Yet, in Theorem 4, we shows that the learning theory of OOD detection is not distribution-free. In fact, we discover that OOD detection is learnable only if the domain space and the hypothesis space satisfy some special conditions, e.g., Conditions 1 and 3. Notably, there are many conditions and theorems in existing learning theories and many OOD detection algorithms in the literature. Thus, it is very difficult to analyze the relation between these theories and algorithms, and explore useful conditions to ensure the learnability of OOD detection, especially when we have to explore them from the scratch. Thus, the main aim of our paper is to study these essential conditions. From these essential conditions, we can know when OOD detection can be successful in practical scenarios. We restate our question and goal in following:\nGiven hypothesis spaces and several representative domain spaces, what are the conditions to ensure the learnability of OOD detection? If possible, we hope that these conditions are necessary and sufficient in some scenarios.\nMain Results. We investigate the learnability of OOD detection starting from the largest space-the total space, and give a necessary condition (Condition 1) for the learnability. However, we find that the overlap between ID and OOD data may result in that the necessary condition does not hold. Therefore, we give an impossibility theorem to demonstrate that OOD detection fails in the total space (Theorem 4). Next, we study OOD detection in the separate space, where there are no overlaps between the ID and OOD data. Unfortunately, there still exists impossibility theorem (Theorem 5), which demonstrates that OOD detection is not learnable in the separate space under some conditions.\nAlthough the impossibility theorems obtained in the separate space are frustrating, we find that some conditions of these impossibility theorems may not hold in some practical scenarios. Based on this observation, we give several necessary and sufficient conditions to characterize the learnability of OOD detection in the separate space (Theorems 6 and 10). Especially, when our model is based on fully-connected neural network (FCNN), OOD detection is learnable in the separate space if and only if the feature space is finite. Furthermore, we investigate the learnability of OOD detection in other more practical domain spaces, e.g., the finite-ID-distribution space (Theorem 8) and the densitybased space (Theorem 9). By studying the finite-ID-distribution space, we discover a compatibility condition (Condition 3) that is a necessary and sufficient condition for this space. Next, we further investigate the compatibility condition in the density-based space, and find that such condition is also the necessary and sufficient condition in some practical scenarios (Theorem 11).\nImplications and Impacts of Theory. Our study is not of purely theoretical interest; it has also practical impacts. First, when we design OOD detection algorithms, we normally only have finite ID datasets, corresponding to the finite-ID-distribution space. In this case, Theorem 8 gives the necessary and sufficient condition to the success of OOD detection. Second, our theory provides theoretical support (Theorems 10 and 11) for several representative OOD detection works [7,8,23]. Third, our theory shows that OOD detection is learnable in image-based scenarios when ID images have clearly different semantic labels and styles (far-OOD) from OOD images. Fourth, we should not expect a universally working algorithm. It is necessary to design different algorithms in different scenarios.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b13", "b14", "b15", "b16", "b17", "b18", "b19", "b3", "b20", "b21", "b6", "b7", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Setups", "text": "We start by introducing the necessary concepts and notations for our theoretical framework. Given a feature space X \u2282 R d and a label space Y := {1, ..., K}, we have an ID joint distribution D XIYI over X \u00d7 Y, where X I \u2208 X and Y I \u2208 Y are random variables. We also have an OOD joint distribution D XOYO , where X O is a random variable from X , but Y O is a random variable whose outputs do not belong to Y. During testing, we will meet a mixture of ID and OOD joint distributions: D XY := (1 \u2212 \u03c0 out )D XIYI + \u03c0 out D XOYO , and can only observe the marginal distribution D X := (1 \u2212 \u03c0 out )D XI + \u03c0 out D XO , where the constant \u03c0 out \u2208 [0, 1) is an unknown class-prior probability.\nProblem 1 (OOD Detection [4]). Given an ID joint distribution D XIYI and a training data S := {(x 1 , y 1 ), ..., (x n , y n )} drawn independent and identically distributed from D XIYI , the aim of OOD detection is to train a classifier f by using the training data S such that, for any test data x drawn from the mixed marginal distribution D X : 1) if x is an observation from D XI , f can classify x into correct ID classes; and 2) if x is an observation from D XO , f can detect x as OOD data.\nAccording to the survey [4], when K > 1, OOD detection is also known as the open-set recognition or open-set learning [24,25]; and when K = 1, OOD detection reduces to one-class novelty detection and semantic anomaly detection [26,27,28].\nOOD Label and Domain Space. Based on Problem 1, we know it is not necessary to classify OOD data into the correct OOD classes. Without loss of generality, let all OOD data be allocated to one big OOD class, i.e., Y O = K + 1 [24,29]. To investigate the PAC learnability of OOD detection, we define a domain space D XY , which is a set consisting of some joint distributions D XY mixed by some ID joint distributions and some OOD joint distributions. In this paper, the joint distribution D XY mixed by ID joint distribution D XIYI and OOD joint distribution D XOYO is called domain.\nHypothesis Spaces and Scoring Function Spaces. A hypothesis space H is a subset of function space, i.e., H \u2282 {h : X \u2192 Y \u222a {K + 1}}. We set H in \u2282 {h : X \u2192 Y} to the ID hypothesis space. We also define H b \u2282 {h : X \u2192 {1, 2}} as the hypothesis space for binary classification, where 1 represents the ID data, and 2 represents the OOD data. The function h is called the hypothesis function. A scoring function space is a subset of function space, i.e., F l \u2282 {f : X \u2192 R l }, where l is the output's dimension of the vector-valued function f . The function f is called the scoring function.\nLoss and Risks. Let Y all = Y \u222a {K + 1}. Given a loss function 2 : Y all \u00d7 Y all \u2192 R \u22650 satisfying that (y 1 , y 2 ) = 0 if and only if y 1 = y 2 , and any h \u2208 H, then the risk with respect to D XY is R D (h) := E (x,y)\u223cD XY (h(x), y).\n(\nThe \u03b1-risk R \u03b1 D (h) := (1 \u2212 \u03b1)R in D (h) + \u03b1R out D (h), \u2200\u03b1 \u2208 [0, 1], where the risks R in D (h), R out D (h) are R in D (h) := E (x,y)\u223cD X I Y I (h(x), y), R out D (h) := E x\u223cD X O (h(x), K + 1).\nLearnability. We aim to select a hypothesis function h \u2208 H with approximately minimal risk, based on finite data. Generally, we expect the approximation to get better, with the increase in sample size.\nAlgorithms achieving this are said to be consistent. Formally, we introduce the following definition: Definition 1 (Learnability of OOD Detection). Given a domain space D XY and a hypothesis space H \u2282 {h : X \u2192 Y all }, we say OOD detection is learnable in D XY for H, if there exists an algorithm A 3 : \u222a +\u221e n=1 (X \u00d7 Y) n \u2192 H and a monotonically decreasing sequence cons (n), such that cons (n) \u2192 0, as n \u2192 +\u221e, and for any domain\nD XY \u2208 D XY , E S\u223cD n X I Y I R D (A(S)) \u2212 inf h\u2208H R D (h) \u2264 cons (n),(2)\nAn algorithm A for which this holds is said to be consistent with respect to D XY .\nDefinition 1 is a natural extension of agnostic PAC learnability of supervised learning [30]. If for any D XY \u2208 D XY , \u03c0 out = 0, then Definition 2 is the agnostic PAC learnability of supervised learning.\nAlthough the expression of Definition 1 is different from the normal definition of agnostic PAC learning in [21], one can easily prove that they are equivalent when is bounded, see Appendix D.3.\nSince OOD data are unavailable, it is impossible to obtain information about the class-prior probability \u03c0 out . Furthermore, in the real world, it is possible that \u03c0 out can be any value in [0, 1). Therefore, the imbalance issue between ID and OOD distributions, and the priori-unknown issue (i.e., \u03c0 out is unknown) are the core challenges. To ease these challenges, researchers use AUROC, AUPR and FPR95 to estimate the performance of OOD detection [18,31,32,33,34,35]. It seems that there is a gap between Definition 1 and existing works. To eliminate this gap, we revise Eq. (2) as follows:\nE S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 cons (n), \u2200\u03b1 \u2208 [0, 1].(3)\nIf an algorithm A satisfies Eq. (3), then the imbalance issue and the prior-unknown issue disappear. That is, A can simultaneously classify the ID data and detect the OOD data well. Based on the above discussion, we define the strong learnability of OOD detection as follows:\nDefinition 2 (Strong Learnability of OOD Detection). Given a domain space D XY and a hypothesis space H \u2282 {h : X \u2192 Y all }, we say OOD detection is strongly learnable in D XY for H, if there exists an algorithm A : \u222a +\u221e n=1 (X \u00d7 Y) n \u2192 H and a monotonically decreasing sequence cons (n), such that cons (n) \u2192 0, as n \u2192 +\u221e, and for any domain\nD XY \u2208 D XY , E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 cons (n), \u2200\u03b1 \u2208 [0, 1].\nIn Theorem 1, we have shown that the strong learnability of OOD detection is equivalent to the learnability of OOD detection, if the domain space D XY is a prior-unknown space (see Definition 3).\nIn this paper, we mainly discuss the learnability in the prior-unknown space. Therefore, when we mention that OOD detection is learnable, we also mean that OOD detection is strongly learnable.\nGoal of Theory. Note that the agnostic PAC learnability of supervised learning is distribution-free, i.e., the domain space D XY consists of all domains. However, due to the absence of OOD data during the training process [8,14,24], it is obvious that the learnability of OOD detection is not distribution-free (i.e., Theorem 4). In fact, we discover that the learnability of OOD detection is deeply correlated with the relationship between the domain space D XY and the hypothesis space H. That is, OOD detection is learnable only when the domain space D XY and the hypothesis space H satisfy some special conditions, e.g., Condition 1 and Condition 3. We present our goal as follows:\nGoal: given a hypothesis space H and several representative domain spaces D XY , what are the conditions to ensure the learnability of OOD detection? Furthermore, if possible, we hope that these conditions are necessary and sufficient in some scenarios.\nTherefore, compared to the agnostic PAC learnability of supervised learning, our theory doesn't focus on the distribution-free case, but focuses on discovering essential conditions to guarantee the learnability of OOD detection in several representative and practical domain spaces D XY . By these essential conditions, we can know when OOD detection can be successful in real applications.", "publication_ref": ["b3", "b3", "b23", "b24", "b25", "b26", "b27", "b23", "b28", "b1", "b29", "b20", "b17", "b30", "b31", "b32", "b7", "b13", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Learning in Priori-unknown Spaces", "text": "We first investigate a special space, called prior-unknown space. In such space, Definition 1 and Definition 2 are equivalent. Furthermore, we also prove that if OOD detection is strongly learnable in a space D XY , then one can discover a larger domain space, which is prior-unknown, to ensure the learnability of OOD detection. These results imply that it is enough to consider our theory in the prior-unknown spaces. The prior-unknown space is introduced as follows: Definition 3. Given a domain space D XY , we say D XY is a priori-unknown space, if for any domain D XY \u2208 D XY and any \u03b1 \u2208 [0, 1), we have D\n\u03b1 XY := (1 \u2212 \u03b1)D XIYI + \u03b1D XOYO \u2208 D XY . Theorem 1. Given domain spaces D XY and D XY = {D \u03b1 XY : \u2200D XY \u2208 D XY , \u2200\u03b1 \u2208 [0, 1)}, then 1) D XY is a priori-unknown space and D XY \u2282 D XY ; 2) if D XY\nis a priori-unknown space, then Definition 1 and Definition 2 are equivalent; 3) OOD detection is strongly learnable in D XY if and only if OOD detection is learnable in D XY .\nThe second result of Theorem 1 bridges the learnability and strong learnability, which implies that if an algorithm A is consistent with respect to a prior-unknown space, then this algorithm A can address the imbalance issue between ID and OOD distributions, and the priori-unknown issue well. Based on Theorem 1, we focus on our theory in the prior-unknown spaces. Furthermore, to demystify the learnability of OOD detection, we introduce five representative priori-unknown spaces:\n\u2022 Single-distribution space D D XY XY . For a domain D XY , D D XY XY := {D \u03b1 XY : \u2200\u03b1 \u2208 [0, 1)}. \u2022 Total space D all\nXY , which consists of all domains. \u2022 Separate space D s XY , which consists of all domains that satisfy the separate condition, that is for any D XY \u2208 D s XY , suppD XO \u2229 suppD XI = \u2205, where supp means the support set. \u2022 Finite-ID-distribution space D F XY , which is a prior-unknown space satisfying that the number of distinct ID joint distributions D XIYI in D F XY is finite, i.e., |{D XIYI :\n\u2200D XY \u2208 D F XY }| < +\u221e. \u2022 Density-based space D \u00b5,b\nXY , which is a prior-unknown space consisting of some domains satisfying that: for any D XY , there exists a density function f with 1/b \u2264 f \u2264 b in supp\u00b5 and 0.5 * D XI + \nh) (solid lines with triangle marks) and the estimated E S\u223cD n in R \u03b1 D (A(S)) (dash lines) with \u03b1 \u2208 [0, 1) in different scenarios, where Din = DX I Y I and the algorithm A is the free-energy OOD detection method [23]. Subfigure (a) shows the ID and OOD distributions. In (a), gap IO represents the distance between the support sets of ID and OOD distributions. In (b), since there is an overlap between ID and OOD data, the solid line is a ployline. In (c), since there is no overlap between ID and OOD data, we can check that inf h\u2208H R \u03b1 D (h) forms a straight line (the solid line). However, since dash lines are always straight lines, two observations can be obtained from (b) and (c): 1) dash lines cannot approximate the solid ployline in (b), which implies the unlearnability of OOD detection; and 2) the solid line in (c) is a straight line and may be approximated by the dash lines in (c). The above observations motivate us to propose Condition 1. 0.5 * D XO = f d\u00b5, where \u00b5 is a measure defined over X . Note that if \u00b5 is discrete, then D X is a discrete distribution; and if \u00b5 is the Lebesgue measure, then D X is a continuous distribution.\nThe above representative spaces widely exist in real applications. For example, 1) if the images from different semantic labels with different styles are clearly different, then those images can form a distribution belonging to a separate space D s XY ; and 2) when designing an algorithm, we only have finite ID datasets, e.g., CIFAR-10, MNIST, SVHN, and ImageNet, to build a model. Then, finite-ID-distribution space D F XY can handle this real scenario. Note that the single-distribution space is a special case of the finite-ID-distribution space. In this paper, we mainly discuss these five spaces.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Impossibility Theorems for OOD Detection", "text": "In this section, we first give a necessary condition for the learnability of OOD detection. Then, we show this necessary condition does not hold in the total space D all XY and the separate space D s XY . Necessary Condition. We find a necessary condition for the learnability of OOD detection, i.e., Condition 1, motivated by the experiments in Figure 1. Details of Figure 1 can be found in Appendix C.2. Condition 1 (Linear Condition). For any D XY \u2208 D XY and any \u03b1 \u2208 [0, 1),\ninf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h).\nTo reveal the importance of Condition 1, Theorem 2 shows that Condition 1 is a necessary and sufficient condition for the learnability of OOD detection if the D XY is the single-distribution space.\nTheorem 2. Given a hypothesis space H and a domain D XY , OOD detection is learnable in the single-distribution space D D XY", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "XY", "text": "for H if and only if linear condition (i.e., Condition 1) holds.\nTheorem 2 implies that Condition 1 is important for the learnability of OOD detection. Due to the simplicity of single-distribution space, Theorem 2 implies that Condition 1 is the necessary condition for the learnability of OOD detection in the prior-unknown space, see Lemma 1 in Appendix F.\nImpossibility Theorems. Here, we first study whether Condition 1 holds in the total space D all XY . If Condition 1 does not hold, then OOD detection is not learnable. Theorem 3 shows that Condition 1 is not always satisfied, especially, when there is an overlap between the ID and OOD distributions: Definition 4 (Overlap Between ID and OOD). We say a domain D XY has overlap between ID and OOD distributions, if there is a \u03c3-finite measure\u03bc such that D X is absolutely continuous with respect to\u03bc, and\u03bc(A overlap ) > 0, where A overlap = {x \u2208 X : f I (x) > 0 and f O (x) > 0}. Here f I and f O are the representers of D XI and D XO in Radon-Nikodym Theorem [36],\nD XI = f I d\u03bc, D XO = f O d\u03bc.\nTheorem 3. Given a hypothesis space H and a prior-unknown space D XY , if there is D XY \u2208 D XY , which has overlap between ID and OOD, and inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0, then Condition 1 does not hold. Therefore, OOD detection is not learnable in D XY for H.\nTheorem 3 clearly shows that under proper conditions, Condition 1 does not hold, if there exists a domain whose ID and OOD distributions have overlap. By Theorem 3, we can obtain that the OOD detection is not learnable in the total space D all XY for any non-trivial hypothesis space H. Theorem 4 (Impossibility Theorem for Total Space). OOD detection is not learnable in the total space D all XY for H, if |\u03c6 \u2022 H| > 1, where \u03c6 maps ID labels to 1 and maps OOD labels to 2. Since the overlaps between ID and OOD distributions may cause that Condition 1 does not hold, we then consider studying the learnability of OOD detection in the separate space D s XY , where there are no overlaps between the ID and OOD distributions. However, Theorem 5 shows that even if we consider the separate space, the OOD detection is still not learnable in some scenarios. Before introducing the impossibility theorem for separate space, i.e., Theorem 5, we need a mild assumption: Assumption 1 (Separate Space for OOD). A hypothesis space H is separate for OOD data, if for each data point x \u2208 X , there exists at least one hypothesis function h x \u2208 H such that h x (x) = K +1.\nAssumption 1 means that every data point x has the possibility to be detected as OOD data. Assumption 1 is mild and can be satisfied by many hypothesis spaces, e.g., the FCNN-based hypothesis space (Proposition 1 in Appendix K), score-based hypothesis space (Proposition 2 in Appendix K) and universal kernel space. Next, we use Vapnik-Chervonenkis (VC) dimension [22] to measure the size of hypothesis space, and study the learnability of OOD detection in D s XY based on the VC dimension. Theorem 5 (Impossibility Theorem for Separate Space). If Assumption 1 holds, VCdim(\u03c6 \u2022 H) < +\u221e and sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| = +\u221e, then OOD detection is not learnable in separate space D s XY for H, where \u03c6 maps ID labels to 1 and maps OOD labels to 2. The finite VC dimension normally implies the learnability of supervised learning. However, in our results, the finite VC dimension cannot guarantee the learnability of OOD detection in the separate space, which reveals the difficulty of the OOD detection. Although the above impossibility theorems are frustrating, there is still room to discuss the conditions in Theorem 5, and to find out the proper conditions for ensuring the learnability of OOD detection in the separate space (see Sections 5 and 6).", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "When OOD Detection Can Be Successful", "text": "Here, we discuss when the OOD detection can be learnable in the separate space D s XY , finite-IDdistribution space D F XY and density-based space D \u00b5,b XY . We first study the separate space D s XY . OOD Detection in the Separate Space. Theorem 5 has indicated that VCdim(\u03c6 \u2022 H) = +\u221e or sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| < +\u221e is necessary to ensure the learnability of OOD detection in D s XY if Assumption 1 holds. However, generally, hypothesis spaces generated by feed-forward neural networks with proper activation functions have finite VC dimension [37,38]. Therefore, we study the learnability of OOD detection in the case that |X | < +\u221e, which implies that sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| < +\u221e. Additionally, Theorem 10 also implies that |X | < +\u221e is the necessary and sufficient condition for the learnability of OOD detection in separate space, when the hypothesis space is generated by FCNN. Hence, |X | < +\u221e may be necessary in the space D s XY . For simplicity, we first discuss the case that K = 1, i.e., the one-class novelty detection. We show the necessary and sufficient condition for the learnability of OOD detection in D s XY , when |X | < +\u221e. Theorem 6. Let K = 1 and |X | < +\u221e. Suppose that Assumption 1 holds and the constant function h in := 1 \u2208 H. Then OOD detection is learnable in D s XY for H if and only if H all \u2212 {h out } \u2282 H, where H all is the hypothesis space consisting of all hypothesis functions, and h out is a constant function that h out := 2, here 1 represents ID data and 2 represents OOD data.\nThe condition h in \u2208 H presented in Theorem 6 is mild. Many practical hypothesis spaces satisfy this condition, e.g., the FCNN-based hypothesis space (Proposition 1 in Appendix K), score-based hypothesis space (Proposition 2 in Appendix K) and universal kernel-based hypothesis space. Theorem 6 implies that if K = 1 and OOD detection is learnable in D s XY for H, then the hypothesis space H should contain almost all hypothesis functions, implying that if the OOD detection can be learnable in the distribution-agnostic case, then a large-capacity model is necessary.\nNext, we extend Theorem 6 to a general case, i.e., K > 1. When K > 1, we will first use a binary classifier h b to classify the ID and OOD data. Then, for the ID data identified by h b , an ID hypothesis function h in will be used to classify them into corresponding ID classes. We state this strategy as follows: given a hypothesis space H in for ID distribution and a binary classification hypothesis space H b introduced in Section 2, we use H in and H b to construct an OOD detection's hypothesis space H, which consists of all hypothesis functions h satisfying the following condition: there exist h in \u2208 H in and h b \u2208 H b such that for any x \u2208 X ,\nh(x) = i, if h in (x) = i and h b (x) = 1; otherwise, h(x) = K + 1.(4)\nWe use H in \u2022 H b to represent a hypothesis space consisting of all h defined in Eq. (4). In addition, we also need an additional condition for the loss function . This condition is shown as follows: Condition 2. (y 2 , y 1 ) \u2264 (K + 1, y 1 ), for any in-distribution labels y 1 and y 2 \u2208 Y. OOD Detection in the Finite-ID-Distribution Space. Since researchers can only collect finite ID datasets as the training data in the process of algorithm design, it is worthy to study the learnability of OOD detection in the finite-ID-distribution space D F XY . We first show two necessary concepts below. Definition 5 (ID Consistency). Given a domain space D XY , we say any two domains D XY \u2208 D XY and D XY \u2208 D XY are ID consistency, if D XIYI = D XIYI . We use the notation \u223c to represent the ID consistency, i.e., D XY \u223c D XY if and only if D XY and D XY are ID consistency.\nIt is easy to check that the ID consistency \u223c is an equivalence relation. Therefore, we define the set [D XY ] := {D XY \u2208 D XY : D XY \u223c D XY } as the equivalence class with respect to space D XY . Condition 3 (Compatibility). For any equivalence class [D XY ] with respect to D XY and any > 0, there exists a hypothesis function h \u2208 H such that for any domain\nD XY \u2208 [D XY ], h \u2208 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + } \u2229 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + }.\nIn Appendix F, Lemma 2 has implied that Condition 3 is a general version of Condition 1. Next, Theorem 8 indicates that Condition 3 is the necessary and sufficient condition in the space D F XY . Theorem 8. Suppose that X is a bounded set. OOD detection is learnable in the finite-IDdistribution space D F XY for H if and only if the compatibility condition (i.e., Condition 3) holds. Furthermore, the learning rate cons (n) can attain O(1/ \u221a n 1\u2212\u03b8 ), for any \u03b8 \u2208 (0, 1).\nTheorem 8 shows that, in the process of algorithm design, OOD detection cannot be successful without the compatibility condition. Theorem 8 also implies that Condition 3 is essential for the learnability of OOD detection. This motivates us to study whether OOD detection can be successful in more general spaces (e.g., the density-based space), when the compatibility condition holds.\nOOD Detection in the Density-based Space. To ensure that Condition 3 holds, we consider a basic assumption in learning theory-Realizability Assumption (see Appendix D.2), i.e., for any D XY \u2208 D XY , there exists h * \u2208 H such that R D (h * ) = 0. We discover that in the density-based space D \u00b5,b XY , Realizability Assumption can conclude the compatibility condition (i.e., Condition 3). Based on this observation, we can prove the following theorem: Theorem 9. Given a density-based space D \u00b5,b XY , if \u00b5(X ) < +\u221e, the Realizability Assumption holds, then when H has finite Natarajan dimension [21], OOD detection is learnable in D \u00b5,b XY for H. Furthermore, the learning rate cons (n) can attain O(1/ \u221a n 1\u2212\u03b8 ), for any \u03b8 \u2208 (0, 1).\nTo further investigate the importance and necessary of Realizability Assumption, Theorem 11 has indicated that in some practical scenarios, Realizability Assumption is the necessary and sufficient condition for the learnability of OOD detection in the density-based space. Therefore, Realizability Assumption may be indispensable for the learnability of OOD detection in some practical scenarios.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "Connecting Theory to Practice", "text": "In Section 5, we have shown the successful scenarios where OOD detection problem can be addressed in theory. In this section, we will discuss how the proposed theory is applied to two representative hypothesis spaces-neural-network-based hypothesis spaces and score-based hypothesis spaces.\nFully-connected Neural Networks. Given a sequence q = (l 1 , l 2 , ..., l g ), where l i and g are positive integers and g > 2, we use g to represent the depth of neural network and use l i to represent the width of the i-th layer. After the activation function \u03c3 is selected 4 , we can obtain the architecture of FCNN according to the sequence q. Let f w,b be the function generated by FCNN with weights w and bias b. An FCNN-based scoring function space is defined as: F \u03c3 q := {f w,b : \u2200 weights w, \u2200 bias b}. In addition, for simplicity, given any two sequences q = (l 1 , ..., l g ) and q = (l 1 , ..., l g ), we use the notation q q to represent the following equations and inequalities:\n1) g \u2264 g , l 1 = l 1 , l g = l g ; 2) l i \u2264 l i , \u2200i = 1, ..., g \u2212 1; and 3) l g\u22121 \u2264 l i , \u2200i = g, ..., g \u2212 1.\nIn Appendix L, Lemma 10 shows q q \u21d2 F \u03c3 q \u2282 F \u03c3 q . We use to compare the sizes of FCNNs. Then, the FCNN-based hypothesis space is defined as H \u03c3 q := {h w,b : \u2200 weights w, \u2200 bias b}. Score-based Hypothesis Space. Many OOD detection algorithms detect OOD data by using a score-based strategy. That is, given a threshold \u03bb, a scoring function space F l \u2282 {f : X \u2192 R l } and a scoring function E : F l \u2192 R, then x is regarded as ID data if and only if E(f (x)) \u2265 \u03bb. We introduce several representative scoring functions E as follows: for any f = [f 1 , ..., f l ] \u2208 F l , \u2022 softmax-based function [7] and temperature-scaled function [8]: \u03bb \u2208 ( 1 l , 1) and T > 0, \nE(f ) = max\n\u2022 energy-based function [23]: \u03bb \u2208 (0, +\u221e) and T > 0,\nE(f ) = T log l c=1 exp (f c /T ).(6)\nUsing E, \u03bb and f \u2208 F \u03c3 q , we have a classifier:\nh \u03bb f ,E (x) = 1, if E(f (x)) \u2265 \u03bb; otherwise, h \u03bb f ,E (x) = 2\n, where 1 represents the ID data and 2 represents the OOD data. Hence, a binary classification hypothesis space H b , which consists of all h \u03bb f ,E , is generated. We define H \u03c3,\u03bb q,E := {h \u03bb f ,E : \u2200f \u2208 F \u03c3 q }. Learnability of OOD Detection in Different Hypothesis Spaces. Next, we present applications of our theory regarding the above two practical and important hypothesis spaces H \u03c3 q and H \u03c3,\u03bb q,E . Theorem 10. Suppose that Condition 2 holds and the hypothesis space\nH is FCNN-based or score- based, i.e., H = H \u03c3 q or H = H in \u2022 H b , where H in is an ID hypothesis space, H b = H \u03c3,\u03bb q,E and H = H in \u2022 H b is introduced below Eq. (4), here E is introduced in Eqs. (5) or (6). Then\nThere is a sequence q = (l 1 , ..., l g ) such that OOD detection is learnable in the separate space D s XY for H if and only if |X | < +\u221e. Furthermore, if |X | < +\u221e, then there exists a sequence q = (l 1 , ..., l g ) such that for any sequence q satisfying that q q , OOD detection is learnable in D s XY for H.\nTheorem 10 states that 1) when the hypothesis space is FCNN-based or score-based, the finite feature space is the necessary and sufficient condition for the learnability of OOD detection in the separate space; and 2) a larger architecture of FCNN has a greater probability to achieve the learnability of OOD detection in the separate space. Note that when we select Eqs. (5) or (6) as the scoring function E, Theorem 10 also shows that the selected scoring functions E can guarantee the learnability of OOD detection, which is a theoretical support for the representative works [8,23,7]. Furthermore, Theorem 11 also offers theoretical supports for these works in the density-based space, when K = 1.\nTheorem 11. Suppose that each domain D XY in D \u00b5,b XY is attainable, i.e., arg min h\u2208H R D (h) = \u2205 (the finite discrete domains satisfy this). Let K = 1 and the hypothesis space H be score-based (H = H \u03c3,\u03bb q,E , where E is in Eqs. (5) or ( 6)) or FCNN-based (H = H \u03c3 q ). If \u00b5(X ) < +\u221e, then the following four conditions are equivalent:\nLearnability in D \u00b5,b XY for H \u21d0\u21d2 Condition 1 \u21d0\u21d2 Realizability Assumption \u21d0\u21d2 Condition 3\nTheorem 11 still holds if the function space F \u03c3 q is generated by Convolutional Neural Network. Overlap and Benefits of Multi-class Case. We investigate when the hypothesis space is FCNNbased or score-based, what will happen if there exists an overlap between the ID and OOD distributions? Theorem 12. Let K = 1 and the hypothesis space H be score-based (H = H \u03c3,\u03bb q,E , where E is in Eqs. (5) or ( 6)) or FCNN-based (H = H \u03c3 q ). Given a prior-unknown space D XY , if there exists a domain D XY \u2208 D XY , which has an overlap between ID and OOD distributions (see Definition 4), then OOD detection is not learnable in the domain space D XY for H.\nWhen K = 1 and the hypothesis space is FCNN-based or score-based, Theorem 12 shows that overlap between ID and OOD distributions is the sufficient condition for the unlearnability of OOD detection. Theorem 12 takes roots in the conditions inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0. However, when K > 1, we can ensure inf h\u2208H R in D (h) > 0 if ID distribution D XIYI has overlap between ID classes. By this observation, we conjecture that when K > 1, OOD detection is learnable in some special cases where overlap exists, even if the hypothesis space is FCNN-based or score-based.", "publication_ref": ["b3", "b6", "b7", "b22", "b4", "b5", "b7", "b22", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "Understanding Far-OOD Detection. Many existing works [7,39] \n\u03c4 > 0, a domain space D XY is \u03c4 -far-OOD, if for any domain D XY \u2208 D XY , dist(suppD XO , suppD XI ) > \u03c4.\nTheorems 7, 8 and 10 imply that under appropriate hypothesis space, \u03c4 -far-OOD detection is learnable. In Theorem 7, the condition |X | < +\u221e is necessary for the separate space. However, one can prove that in the far-OOD case, when H in is agnostic PAC learnable for ID distribution, the results in Theorem 7 still holds, if the condition |X | < +\u221e is replaced by a weaker condition that X is compact. In addition, it is notable that when H in is agnostic PAC learnable for ID distribution and X is compact, the KNN-based OOD detection algorithm [44] is consistent in the \u03c4 -far-OOD case.\nUnderstanding Near-OOD Detection. When the ID and OOD datasets have similar semantics or styles, OOD detection tasks become more challenging. [45,46] consider this issue and name it near-OOD detection. Existing benchmarks include 1) MNIST [40] as ID dataset, and Fashion-MNIST [43] or Not-MNIST [47] as OOD datasets; and 2) CIFAR-10 [42] as ID dataset, and CIFAR-100 [48] as OOD dataset. From the theoretical view, some near-OOD tasks may imply the overlap condition, i.e. Definition 4. Therefore, Theorems 3 and 12 imply that near-OOD detection may be not learnable. Developing a theory to understand the feasibility of near-OOD detection is still an open question.\nUnderstanding One-class Novelty Detection. In one-class novelty detection and semantic anomaly detection (i.e. K = 1), Theorem 6 has revealed that it is necessary to use a large-capacity model to ensure the good generalization in the separate space. Theorem 3 and Theorem 12 suggest that we should try to avoid the overlap between ID and OOD distributions in the one-class case. If the overlap cannot be avoided, we suggest considering the multi-class OOD detection instead of the one-class case. Additionally, in the density-based space, Theorem 11 has shown that it is necessary to select a suitable hypothesis space satisfying the Realizability Assumption to ensure the learnability of OOD detection in the density-based space. Generally, a large-capacity model can be helpful to guarantee that the Realizability Assumption holds.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "We briefly review the related theoretical works below. See Appendix A for detailed related works.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "OOD Detection Theory.", "text": "[49] understands the OOD detection via goodness-of-fit tests and typical set hypothesis, and argues that minimal density estimation errors can lead to OOD detection failures without assuming an overlap between ID and OOD distributions. Beyond [49], [50] paves a new avenue to designing provable OOD detection algorithms. Compared to [50,49], our theory focuses on the PAC learnable theory of OOD detection and identifies several necessary and sufficient conditions for the learnability of OOD detection, opening a door to study OOD detection in theory.\nOpen-set Learning Theory.\n[51] and [29,52] propose the agnostic PAC learning bounds for openset detection and open-set domain adaptation, respectively. Unfortunately, [29,51,52] all require that the test data are indispensable during the training process. To investigate open-set learning (OSL) without accessing the test data during training, [24] proposes and investigates the almost agnostic PAC learnability for OSL. However, the assumptions used in [24] are very strong and unpractical.\nLearning Theory for Classification with Reject Option. Many works [53,54] also investigate the classification with reject option (CwRO) problem, which is similar to OOD detection in some cases. [55,56,57,58,59] study the learning theory and propose the PAC learning bounds for CwRO. However, compared to our work regarding OOD detection, existing CwRO theories mainly focus on how the ID risk R in D (i.e., the risk that ID data is wrongly classified) is influenced by special rejection rules. Our theory not only focuses on the ID risk, but also pays attention to the OOD risk.\nRobust Statistics. In the field of robust statistics [60], researchers aim to propose estimators and testers that can mitigate the negative effects of outliers (similar to OOD data). The proposed estimators are supposed to be independent of the potentially high dimensionality of the data [61, 62, 63]. Existing works [64,65,66] in the field have identified and resolved the statistical limits of outlier robust statistics by constructing estimators and proving impossibility results. In the future, it is a promising and interesting research direction to study the robustness of OOD detection based on robust statistics.\nPQ Learning Theory. Under some conditions, PQ learning theory [67, 68] can be regarded as the PAC theory for OOD detection in the semi-supervised or transductive learning cases, i.e., test data are required during training. Besides, [67,68] aim to give the PAC estimation under Realizability Assumption [21]. Our theory does not only study the PAC estimation in the realization cases, but also studies the other cases, which are more difficult than PAC theory under Realizability Assumption.", "publication_ref": ["b28", "b28", "b23", "b23", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions and Future Works", "text": "Detecting OOD data has shown its significance in improving the reliability of machine learning. However, very few works discuss OOD detection in theory, which hinders real-world applications of OOD detection algorithms. In this paper, we are the first to provide the PAC theory for OOD detection. Our results imply that we cannot expect a universally consistent algorithm to handle all scenarios in OOD detection. Yet, it is still possible to make OOD detection learnable in certain scenarios. For example, when we design OOD detection algorithms, we normally only have finite ID datasets. In this real scenario, Theorem 8 provides a necessary and sufficient condition for the success of OOD detection. Our theory reveals many necessary and sufficient conditions for the learnability of OOD detection, hence opening a door to studying the learnability of OOD detection. In the future, we will focus on studying the robustness of OOD detection based on robust statistics [64, 69].\n[34] Wentao Bao, Qi Yu, and Yu Kong. Evidential deep learning for open set action recognition.\nICCV, 2021.\n[ [39] Jingkang Yang, Kaiyang Zhou, and Ziwei Liu. Full-spectrum out-of-distribution detection. CoRR, 2022.\n[40] Li Deng. The MNIST database of handwritten digit images for machine learning research [best of the web]. IEEE Signal Process. Mag., 2012.\n[41] Gustaf Kylberg. Kylberg texture dataset v. 1.0. 2011.\n[42] Alex Krizhevsky and Geoff Hinton. Convolutional deep belief networks on cifar-10. Technical report, Citeseer, 2009.\n[43] Bolei Zhou,\u00c0gata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell., 2018.\n[44] Yiyou Sun, Yifei Ming, Xiaojin Zhu, and Yixuan Li. Out-of-distribution detection with deep nearest neighbors. In ICML, 2022.\n[45] Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. CoRR, abs/2106.09022, 2021.\n[46] Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of-distribution detection. In NeurIPS, 2021. [73] Zhisheng Xiao, Qing Yan, and Yali Amit. Likelihood regret: An out-of-distribution detection score for variational auto-encoder. In NeurIPS, 2020.\n[74] Alireza Zaeemzadeh, Niccol\u00f3 Bisagno, Zeno Sambugaro, Nicola Conci, Nazanin Rahnavard, and Mubarak Shah. Out-of-distribution detection using union of 1-dimensional subspaces. In CVPR, 2021.\n[75] Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep deterministic neural network. In ICML, 2020.\n[76] Sachin Vernekar, Ashish Gaurav, Vahdat Abdelzad, Taylor Denouden, Rick Salay, and Krzysztof Czarnecki. Out-of-distribution detection in classifiers via generation. In NeurIPS Workshop, 2019.\n[77] Ryuichi Kiryo, Gang Niu, Marthinus Christoffel du Plessis, and Masashi Sugiyama. Positiveunlabeled learning with non-negative risk estimator. In NeurIPS, 2017.\n[78] Takashi Ishida, Gang Niu, and Masashi Sugiyama. Binary classification from positiveconfidence data. In NeurIPS, 2018.\n[79] Shuo Chen, Gang Niu, Chen Gong, Jun Li, Jian Yang, and Masashi Sugiyama. Large-margin contrastive learning with distance polarization regularizer. In ICML, 2021.\n[80] Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, and Xiaowei Xu. What can be transferred: Unsupervised domain adaptation for endoscopic lesions segmentation. In CVPR, 2020.\n[81] Zhen Fang, Jie Lu, Feng Liu, and Guangquan Zhang. Semi-supervised heterogeneous domain adaptation: Theory and algorithms. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n[82] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[    ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Detailed Related Work", "text": "OOD Detection Algorithms. We will briefly review many representative OOD detection algorithms in three categories. 1) Classification-based methods use an ID classifier to detect OOD data [7] 5 . Representative works consider using the maximum softmax score [7], temperature-scaled score [14] and energy-based score [23,71] to identify OOD data. 2) Density-based methods aim to estimate an ID distribution and identify the low-density area as OOD data [10].\n3) The recent development of generative models provides promising ways to make them successful in OOD detection [11,12,14,72,73]. Distance-based methods are based on the assumption that OOD data should be relatively far away from the centroids of ID classes [9], including Mahalanobis distance [9,45], cosine similarity [74], and kernel similarity [75].\nEarly works consider using the maximum softmax score to express the ID-ness [7]. Then, temperature scaling functions are used to amplify the separation between the ID and OOD data [14]. Recently, researchers propose hyperparameter-free energy scores to improve the OOD uncertainty estimation [23,71]. Additionally, researchers also consider using the information contained in gradients to help improve the performance of OOD detection [18].\nExcept for the above algorithms, researchers also study the situation, where auxiliary OOD data can be obtained during the training process [13,70]. These methods are called outlier exposure, and have much better performance than the above methods due to the appearance of OOD data. However, the exposure of OOD data is a strong assumption [4]. Thus, researchers also consider generating OOD data to help the separation of OOD and ID data [76]. In this paper, we do not make an assumption that OOD data are available during training, since this assumption may not hold in real world. ", "publication_ref": ["b6", "b6", "b13", "b22", "b9", "b10", "b11", "b13", "b8", "b8", "b6", "b13", "b22", "b17", "b12", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "B Limitations and Potential Negative Societal Impacts", "text": "Limitations. The main limitation of our work lies in that we do not answer the most general question:\nGiven any hypothesis space H and space D XY , what is the necessary and sufficient condition to ensure the PAC learnability of OOD detection?\nHowever, this question is still difficult to be addressed, due to limited mathematical skills. Yet, based on our observations and the main results in our paper, we believe the following result may hold:\nConjecture: If H is agnostic learnable for supervised learning, then OOD detection is learnable in D XY if and only if compatibility condition (i.e., Condition 3) holds.\nWe leave this question as a future work.\nPotential Negative Societal Impacts. Since our paper is a theoretical paper and the OOD detection problem is significant to ensure the safety of deploying existing machine learning algorithms, there are no potential negative societal impacts in our paper.\nC Discussions and Details about Experiments in Figure 1 In this section, we summarize our main results, then give the details of the experiments in Figure 1.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "C.1 Summary", "text": "We summarize our main results as follows:\n\u2022 A necessary condition (i.e., Condition 1) for the learnability of OOD detection is proposed.\nTheorem 2 shows that Condition 1 is the necessary and sufficient condition for the learnability of OOD detection, when the domain space is the single-distribution space D D XY XY . This implies the Condition 1 is the necessary condition for the learnability of OOD detection.\n\u2022 Theorem 3 has shown that the overlap between ID and OOD data can lead the failures of OOD detection under some mild assumptions. Furthermore, Theorem 12 shows that when K = 1, the overlap is the sufficient condition for the failures of OOD detection, when the hypothesis space is FCNN-based or score-based.\n\u2022 Theorem 4 provides an impossibility theorem for the total space D all XY . OOD detection is not learnable in D all XY for any non-trivial hypothesis space. \u2022 Theorem 5 gives impossibility theorems for the separate space D s XY . To ensure the impossibility theorems hold, mild assumptions are required. Theorem 5 also implies that OOD detection may be learnable in the separate space D s XY , if the feature space is finite, i.e., |X | < +\u221e. Additionally, Theorem 10 implies that the finite feature space may be the necessary condition to ensure the learnability of OOD detection in the separate space.\n\u2022 When |X | < +\u221e and K = 1, Theorem 6 provides the necessary and sufficient condition for the learnability of OOD detection in the separate space D s XY . Theorem 6 implies that if the OOD detection can be learnable in the distribution-agnostic case, then a large-capacity model is necessary. Based on Theorem 6, Theorem 7 studies the learnability in the K > 1 case.\n\u2022 The compatibility condition (i.e., Condition 3) for the learnability of OOD detection is proposed.\nTheorem 8 shows that Condition 3 is the necessary and sufficient condition for the learnability of OOD detection in the finite-ID-distribution space D F XY . This also implies Condition 3 is the necessary condition for any prior-unknown space. Note that we can only collect finite ID datasets to build models. Hence, Theorem 8 can handle the most practical scenarios.\n\u2022 To further understand the importance of the compatibility condition (Condition 3). Theorem 9 considers the density-based space D \u00b5,b XY . We discover that Realizability Assumption implies the compatibility condition in the density-based space. Based on this observation, we prove that OOD detection is learnable in D \u00b5,b XY under Realizability Assumption. \u2022 Theorem 10 gives practical applications of our theory. In this theorem, we discover that the finite feature space is a necessary and sufficient condition for the learnability of OOD detection in the separate space D s XY , when the hypothesis space is FCNN-based or score-based. XY are all equivalent. \u2022 Meaning of Our Theory. In classical statistical learning theory, the generalization theory guarantees that a well-trained classifier can be generalized well on the test set as long as the training and test sets are from the same distribution [21,22]. However, since the OOD data are unseen during the training process, it is very difficult to determine whether the generalization theory holds for OOD detection.\nNormally, OOD data are unseen and can be various. We hope that there exists an algorithm that can be used for the various OOD data instead of some certain OOD data, which is the reason why the generalization theory for OOD detection needs to be developed. In this paper, we investigate the generalization theory regarding OOD detection and point out when the OOD detection can be successful. Our theory is based on the PAC learning theory. The impossibility theorems and the given necessary and sufficient conditions outlined provide important perspectives from which to think about OOD detection. 1 In this subsection, we present details of the experiments in Figure 1, including data generation, configuration and OOD detection procedure.", "publication_ref": ["b20", "b21"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "C.2 Details of Experiments in Figure", "text": "Data Generation. ID and OOD data are drawn from the following uniform (U) distributions (note that we use U(I) to present the uniform distribution in region I).\n\u2022 The marginal distribution of ID distribution for class c: for any c \u2208 {1, ..., 10},\nD XI|YI=c = U(I c ), where I c = [d c , d c + 4] \u00d7 [1, 5],(7)\nhere d i = 5 + gap II * (i \u2212 1) + 4(i \u2212 2)\nand gap II is a positive constant.\n\u2022 The class-prior probability for class c: for any c \u2208 {1, ..., 10},\nD YI (y = c) = 1 \u2212 \u03b1 10 .\n\u2022 The marginal distribution of OOD distribution:\nD XO = U(I out ), where I out = [d 1 \u2212 1, d 10 + 5] \u00d7 [5 + gap IO , 10 + gap IO ].(8)\nFigure 2 shows the OOD and ID distributions, when gap II = 20 and gap IO = \u22122. In Figure 1, we draw n data from ID distribution (n = 15, 000, 20, 000, 25, 000) and 25, 000 data from the OOD distribution.\nConfiguration. The architecture of ID classifier is a four-layer FCNN. The number of neurons in hidden layers is set to 100, and the number of neurons of output layer is set to 10. These neurons use sigmoid activations. We use the Adam optimizer [82] to optimize the network's parameters (with the 2 loss). The learning rate is set to 0.001, and the max number of training iterations is set to 10, 000. Within each iteration, we use full batch to update the network's parameters. gap II is set to 20 in our experiments. In Figure 1b, gap IO = \u22122 (the overlap exists, see Figure 2), and in Figure 1c, gap IO = 100 (no overlap).\nOOD Detection Procedure. We first train an ID classifier with n data drawn from the ID distribution.\nThen, according to [23], we apply the free-energy score to identify the OOD data and calculate the \u03b1-risk (with the 0-1 loss). We repeat the above detection procedure 20 times and report the average \u03b1-risk in Figure 1. Note that, following [23], we choose the threshold used by the free-energy method so that 95% of ID data are correctly identified as the ID classes by the OOD detector.", "publication_ref": ["b22", "b22"], "figure_ref": ["fig_7", "fig_0", "fig_0", "fig_7", "fig_0", "fig_0"], "table_ref": []}, {"heading": "D Notations D.1 Main Notations and Their Descriptions", "text": "In this section, we summarize important notations in Table 1.  \nD \u03b1 XY = (1 \u2212 \u03b1)DX I Y I + \u03b1DX O Y O , \u2200\u03b1 \u2208 [0, 1] \u03c0 out class-prior probability for OOD distribution DXY DXY = (1 \u2212 \u03c0 out )DX I Y I + \u03c0 out DX O Y O ,", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "score-based hypothesis function-a binary classifier", "text": "Given f = [f 1 , ..., f l ] , for any x \u2208 X , arg max k\u2208{1,...,l}\nf k (x) := max{k \u2208 {1, ..., l} : f k (x) \u2265 f i (x), \u2200i = 1, ..., l},\nwhere f k is the k-th coordinate of f and f i is the i-th coordinate of f . The above definition about arg max aims to overcome some special cases. For example, there exist k 1 , k 2 (k 1 < k 2 ) such that f k1 (x) = f k2 (x) and f k1 (x) > f i (x), f k2 (x) > f i (x), \u2200i \u2208 {1, ..., l}\u2212{k 1 , k 2 }. Then, according to the above definition, k 2 = arg max k\u2208{1,...,l} f k (x).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Realizability Assumption", "text": "Assumption 2 (Realizability Assumption). A domain space D XY and hypothesis space H satisfy the Realizability Assumption, if for each domain D XY \u2208 D XY , there exists at least one hypothesis function h * \u2208 H such that R D (h * ) = 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Learnability and PAC learnability", "text": "Here we give a proof to show that Learnability given in Definition 1 and PAC learnability are equivalent.\nFirst, we prove that Learnability concludes the PAC learnability.\nAccording to Definition 1,\nE S\u223cD n X I Y I R D (A(S)) \u2264 inf h\u2208H R D (h) + cons (n), which implies that E S\u223cD n X I Y I [R D (A(S)) \u2212 inf h\u2208H R D (h)] \u2264 cons (n).\nNote that R D (A(S)) \u2212 inf h\u2208H R D (h) \u2265 0. Therefore, by Markov's inequality, we have\nP(R D (A(S)) \u2212 inf h\u2208H R D (h) < ) > 1 \u2212 E S\u223cD n X I Y I [R D (A(S)) \u2212 inf h\u2208H R D (h)]/ \u2265 1 \u2212 cons (n)/ .\nBecause cons (n) is monotonically decreasing, we can find a smallest m such that cons (m) \u2265 \u03b4 and cons (m \u2212 1) < \u03b4, for \u03b4 \u2208 (0, 1). We define that m( , \u03b4) = m. Therefore, for any > 0 and \u03b4 \u2208 (0, 1), there exists a function m( , \u03b4) such that when n > m( , \u03b4), with the probability at least 1 \u2212 \u03b4, we have\nR D (A(S)) \u2212 inf h\u2208H R D (h) < ,\nwhich is the definition of PAC learnability.\nSecond, we prove that the PAC learnability concludes Learnability.\nPAC-learnability: for any > 0 and 0 < \u03b4 < 1, there exists a function m( , \u03b4) > 0 such that when the sample size n > m( , \u03b4), we have that with the probability at least 1 \u2212 \u03b4 > 0,\nR D (A(S)) \u2212 inf h\u2208H R D (h) \u2264 .\nNote that the loss defined in Section 2 has upper bound (because Y \u222a {K + 1} is a finite set). We assume the upper bound of is M . Hence, according to the definition of PAC-learnability, when the sample size n > m( , \u03b4), we have that\nE S [R D (A(S)) \u2212 inf h\u2208H R D (h)] \u2264 (1 \u2212 \u03b4) + 2M \u03b4 < + 2M \u03b4.\nIf we set \u03b4 = , then when the sample size n > m( , ), we have that\nE S [R D (A(S)) \u2212 inf h\u2208H R D (h)] < (2M + 1) , this implies that lim n\u2192+\u221e E S [R D (A(S)) \u2212 inf h\u2208H R D (h)] = 0,\nwhich implies the Learnability in Definition 1. We have completed this proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.4 Explanations for Some Notations in Section 2", "text": "First, we explain the concept that S \u223c D n X I Y I in Eq. (2).\nS = {(x 1 , y 1 ), ..., (x n , y n )} is training data drawn independent and identically distributed from D XIYI .\nD n XIYI denotes the probability over n-tuples induced by applying D XIYI to pick each element of the tuple independently of the other members of the tuple.\nBecause these samples are i.i.d. drawn n times, researchers often use \"S \u223c D n XIYI \" to represent a sample set S (of size n) whose each element is drawn i.i.d. from D XIYI .\nSecond, we explain the concept \"+\" in (1 \u2212 \u03c0 out )D XI + \u03c0 out D XO .\nFor convenience, let P = (1 \u2212 \u03c0 out )D XI and Q = \u03c0 out D XO . It is clear that P and Q are measures. Then P + Q is also a measure, which is defined as follows: for any measurable set A \u2282 X , we have (P + Q)(A) = P (A) + Q(A).\nFor example, when P and Q are discrete measures, then P + Q is also discrete measure: for any x \u2208 X , (P + Q)(x) = P (x) + Q(x).\nWhen P and Q are continuous measures with density functions f and g, then P +Q is also continuous measure with density function f + g: for any measurable A \u2282 X ,\nP (A) = A f (x)dx, Q(A) = A g(x)dx, then (P + Q)(A) = A f (x) + g(x)dx.\nThird, we explain the concept E (x,y)\u223cD XY (h(x), y).\nThe concept E (x,y)\u223cD XY (h(x), y) can be computed as follows:\nE (x,y)\u223cD XY (h(x), y) =\nX \u00d7Y all (h(x), y)dD XY (x, y).\nFor example, when D XY is a finite discrete distribution: let Z = {(x 1 , y 1 ), ..., (x m , y m )} be the support set of D XY , and assume that a i is the probability for (x i , y i ), i.e., a i = D XY (x i , y i ). Then\nE (x,y)\u223cD XY (h(x), y) = X \u00d7Y all (h(x), y)dD XY (x, y) = 1 m m i=1 a i (h(x i ), y i ).\nWhen D X is a continuous distribution with density f , and\nD Y |X (Y = k|X = x) (k-th class- conditional distribution for x) is a k (x), then E (x,y)\u223cD XY (h(x), y) = X \u00d7Y all (h(x), y)dD XY (x, y) = X K+1 k=1 (h(x), k)f (x)a k (x)dx,\nwhere D Y |X (Y = k|X = x) is the k-th class-conditional distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Proof of Theorem 1", "text": "Theorem 1. Given domain spaces D XY and D XY = {D \u03b1 XY : \u2200D XY \u2208 D XY , \u2200\u03b1 \u2208 [0, 1)}, then 1) D XY is a priori-unknown space and D XY \u2282 D XY ; 2) if D XY is a priori-unknown space, then Definition 1 and Definition 2 are equivalent; 3) OOD detection is strongly learnable in D XY if and only if OOD detection is learnable in D XY .\nProof of Theorem 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of the First Result.", "text": "To prove that D XY is a priori-unknown space, we need to show that for any D \u03b1 XY \u2208 D XY , then D \u03b1 XY \u2208 D XY for any \u03b1 \u2208 [0, 1). According to the definition of D XY , for any D \u03b1 XY \u2208 D XY , we can find a domain D XY \u2208 D XY , which can be written as\nD XY = (1 \u2212 \u03c0 out )D XIYI + \u03c0 out D XOYO (here \u03c0 out \u2208 [0, 1)) such that D \u03b1 XY = (1 \u2212 \u03b1 )D XIYI + \u03b1 D XOYO .\nNote that D \u03b1 XY = (1 \u2212 \u03b1)D XIYI + \u03b1D XOYO . Therefore, based on the definition of D XY , for any \u03b1 \u2208 [0, 1), D \u03b1 XY \u2208 D XY , which implies that D XY is a prior-known space. Additionally, for any D XY \u2208 D XY , we can rewrite D XY as D \u03c0out XY , thus D XY = D \u03c0out XY \u2208 D XY , which implies that D XY \u2282 D XY . Proof of the Second Result.\nFirst, we prove that Definition 1 concludes Definition 2, if D XY is a prior-unknown space:\nThe domain space D XY is a priori-unknown space, and OOD detection is learnable in D XY for H. \u21d3 OOD detection is strongly learnable in D XY for H: there exist an algorithm A : \u222a +\u221e n=1 (X \u00d7 Y) n \u2192 H, and a monotonically decreasing sequence (n), such that (n) \u2192 0, as n \u2192 +\u221e\nE S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 (n), \u2200\u03b1 \u2208 [0, 1], \u2200D XY \u2208 D XY .\nIn the priori-unknown space, for any D XY \u2208 D XY , we have that for any \u03b1 \u2208 [0, 1),\nD \u03b1 XY = (1 \u2212 \u03b1)D XIYI + \u03b1D XOYO \u2208 D XY .\nThen, according to the definition of learnability of OOD detection, we have an algorithm A and a monotonically decreasing sequence cons (n) \u2192 0, as n \u2192 +\u221e, such that for any \u03b1 \u2208 [0, 1),\nE S\u223cD n X I Y I R D \u03b1 (A(S)) \u2264 inf h\u2208H R D \u03b1 (h) + cons (n), (by the property of priori-unknown space) where R D \u03b1 (A(S)) = X \u00d7Y all (A(S)(x), y)dD \u03b1 XY (x, y), R D \u03b1 (h) = X \u00d7Y all (h(x), y)dD \u03b1 XY (x, y). Since R D \u03b1 (A(S)) = R \u03b1 D (A(S)) and R D \u03b1 (h) = R \u03b1 D (h), we have that E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + cons (n), \u2200\u03b1 \u2208 [0, 1).(9)\nNext, we consider the case that \u03b1 = 1. Note that lim inf\n\u03b1\u21921 inf h\u2208H R \u03b1 D (h) \u2265 lim inf \u03b1\u21921 \u03b1 inf h\u2208H R out D (h) = inf h\u2208H R out D (h).(10)\nThen, we assume that h \u2208 H satisfies that\nR out D (h ) \u2212 inf h\u2208H R out D (h) \u2264 . It is obvious that R \u03b1 D (h ) \u2265 inf h\u2208H R \u03b1 D (h).\nLet \u03b1 \u2192 1. Then, for any > 0,\nR out D (h ) = lim \u03b1\u21921 R \u03b1 D (h ) = lim sup \u03b1\u21921 R \u03b1 D (h ) \u2265 lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h), which implies that inf h\u2208H R out D (h) = lim \u21920 R out D (h ) \u2265 lim \u21920 lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h) = lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h).(11)\nCombining Eq. (10) with Eq. (11), we have\ninf h\u2208H R out D (h) = lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h) = lim inf \u03b1\u21921 inf h\u2208H R \u03b1 D (h),(12)\nwhich implies that inf\nh\u2208H R out D (h) = lim \u03b1\u21921 inf h\u2208H R \u03b1 D (h).(13)\nNote that\nE S\u223cD n X I Y I R \u03b1 D (A(S)) = (1 \u2212 \u03b1)E S\u223cD n X I Y I R in D (A(S)) + \u03b1E S\u223cD n X I Y I R out D (A(S)). Hence, Lebesgue's Dominated Convergence Theorem [36] implies that lim \u03b1\u21921 E S\u223cD n X I Y I R \u03b1 D (A(S)) = E S\u223cD n X I Y I R out D (A(S)).(14)\nUsing Eq. ( 9), we have that\nlim \u03b1\u21921 E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 lim \u03b1\u21921 inf h\u2208H R \u03b1 D (h) + cons (n).(15)\nCombining Eq. (13), Eq. ( 14) with Eq. ( 15), we obtain that\nE S\u223cD n X I Y I R out D (A(S)) \u2264 inf h\u2208H R out D (h) + cons (n).\nSince R out D (A(S)) = R 1 D (A(S)) and R out D (h) = R 1 D (h), we obtain that\nE S\u223cD n X I Y I R 1 D (A(S)) \u2264 inf h\u2208H R 1 D (h) + cons (n). (16\n)\nCombining Eq. (9) and Eq. ( 16), we have proven that: if the domain space D XY is a priori-unknown space, then OOD detection is learnable in D XY for H. \u21d3 OOD detection is strongly learnable in D XY for H: there exist an algorithm A : \u222a +\u221e n=1 (X \u00d7Y) n \u2192 H, and a monotonically decreasing sequence (n), such that (n) \u2192 0, as n \u2192 +\u221e,\nE S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + (n), \u2200\u03b1 \u2208 [0, 1], \u2200D XY \u2208 D XY .\nSecond, we prove that Definition 2 concludes Definition 1:\nOOD detection is strongly learnable in D XY for H: there exist an algorithm A : \u222a +\u221e n=1 (X \u00d7Y) n \u2192 H, and a monotonically decreasing sequence (n), such that (n) \u2192 0, as n \u2192 +\u221e,\nE S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 (n), \u2200\u03b1 \u2208 [0, 1], \u2200D XY \u2208 D XY . \u21d3 OOD detection is learnable in D XY for H. If we set \u03b1 = \u03c0 out , then E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + (n) implies that E S\u223cD n X I Y I R D (A(S)) \u2264 inf h\u2208H R D (h) + (n),\nwhich means that OOD detection is learnable in D XY for H. We have completed this proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of the Third Result.", "text": "The third result is a simple conclusion of the second result. Hence, we omit it.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Proof of Theorem 2", "text": "Before introducing the proof of Theorem 2, we extend Condition 1 to a general version (Condition 4). Then, Lemma 1 proves that Conditions 1 and 4 are the necessary conditions for the learnability of OOD detection. First, we provide the details of Condition 4.\nLet \u2206 o l = {(\u03bb 1 , ..., \u03bb l ) : l j=1 \u03bb j < 1 and \u03bb j \u2265 0, \u2200j = 1, ..., l}, where l is a positive integer. Next, we introduce an important definition as follows: Definition 6 (OOD Convex Decomposition and Convex Domain). Given any domain D XY \u2208 D XY , we say joint distributions Q 1 , ..., Q l , which are defined over X \u00d7 {K + 1}, are the OOD convex decomposition for D XY , if\nD XY = (1 \u2212 l j=1 \u03bb j )D XIYI + l j=1 \u03bb j Q j , for some (\u03bb 1 , ..., \u03bb l ) \u2208 \u2206 o l .\nWe also say domain D XY \u2208 D XY is an OOD convex domain corresponding to OOD convex decomposition Q 1 , ..., Q l , if for any (\u03b1 1 , ..., \u03b1\nl ) \u2208 \u2206 o l , (1 \u2212 l j=1 \u03b1 j )D XIYI + l j=1 \u03b1 j Q j \u2208 D XY .\nWe extend the linear condition (Condition 1) to a multi-linear scenario. Condition 4 (Multi-linear Condition). For each OOD convex domain D XY \u2208 D XY corresponding to OOD convex decomposition Q 1 , ..., Q l , the following function\nf D,Q (\u03b1 1 , ..., \u03b1 l ) := inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) , \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l satisfies that f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ),\nwhere 0 is the 1 \u00d7 l vector, whose elements are 0, and \u03b1 j is the 1 \u00d7 l vector, whose j-th element is 1 and other elements are 0.\nWhen l = 1 and the domain space D XY is a priori-unknown space, Condition 4 degenerates into Condition 1. Lemma 1 shows that Condition 4 is necessary for the learnability of OOD detection.\nLemma 1. Given a priori-unknown space D XY and a hypothesis space H, if OOD detection is learnable in D XY for H, then Conditions 1 and 4 hold.\nProof of Lemma 1.\nSince Condition 1 is a special case of Condition 4, we only need to prove that Condition 4 holds.\nFor any OOD convex domain D XY \u2208 D XY corresponding to OOD convex decomposition Q 1 , ..., Q l , and any (\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , we set\nQ \u03b1 = 1 l i=1 \u03b1 i l j=1 \u03b1 j Q j .\nThen, we define\nD \u03b1 XY = (1 \u2212 l i=1 \u03b1 i )D XIYI + ( l i=1 \u03b1 i )Q \u03b1 , which belongs to D XY . Let R \u03b1 D (h) = X \u00d7Y all (h(x), y)dD \u03b1 XY (x, y).\nSince OOD detection is learnable in D XY for H, there exist an algorithm A : \u222a +\u221e n=1 (X \u00d7 Y) n \u2192 H, and a monotonically decreasing sequence (n), such that (n) \u2192 0, as n \u2192 +\u221e, and\n0 \u2264 E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 (n).\nNote that\nE S\u223cD n X I Y I R \u03b1 D (A(S)) = (1 \u2212 l j=1 \u03b1 j )E S\u223cD n X I Y I R in D (A(S)) + l j=1 \u03b1 j E S\u223cD n X I Y I R Qj (A(S)),and\ninf h\u2208H R \u03b1 D (h) = f D,Q (\u03b1 1 , ..., \u03b1 l ),where\nR Qj (A(S)) = X \u00d7{K+1}\n(A(S)(x), y)dQ j (x, y).\nTherefore, we have that for any (\u03b1 1 , ..., \u03b1\nl ) \u2208 \u2206 o l , (1 \u2212 l j=1 \u03b1 j )E S\u223cD n X I Y I R in D (A(S)) + l j=1 \u03b1 j E S\u223cD n X I Y I R Qj (A(S)) \u2212 f D,Q (\u03b1 1 , ..., \u03b1 l ) \u2264 (n).(17)\nLet\ng n (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )E S\u223cD n X I Y I R in D (A(S)) + l j=1 \u03b1 j E S\u223cD n X I Y I R Qj (A(S)).\nNote that Eq. (17) implies that\nlim n\u2192+\u221e g n (\u03b1 1 , ..., \u03b1 l ) = f D,Q (\u03b1 1 , ..., \u03b1 l ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , lim n\u2192+\u221e g n (0) = f D,Q (0). (18\n)\nStep 1. Since \u03b1 j / \u2208 \u2206 o l , we need to prove that lim\nn\u2192+\u221e E S\u223cD n X I Y I R Qj (A(S)) = f (\u03b1 j ), i.e., lim n\u2192+\u221e g n (\u03b1 j ) = f (\u03b1 j ),(19)\nwhere \u03b1 j is the 1 \u00d7 l vector, whose j-th element is 1 and other elements are 0.\nLetD XY = 0.5 * D XIYI + 0.5 * Q j . The second result of Theorem 1 implies that\nE S\u223cD n X I Y I R out D (A(S)) \u2264 inf h\u2208H R out D (h) + (n). Since R out D (A(S)) = R Qj (A(S)) and R out D (h) = R Qj (h), E S\u223cD n X I Y I R Qj (A(S)) \u2264 inf h\u2208H R Qj (h) + (n). Note that inf h\u2208H R Qj (h) \u2264 E S\u223cD n X I Y I R Qj (A(S)). We have 0 \u2264 E S\u223cD n X I Y I R Qj (A(S)) \u2212 inf h\u2208H R Qj (h) \u2264 (n).(20)\nEq. (20) implies that lim\nn\u2192+\u221e E S\u223cD n X I Y I R Qj (A(S)) = inf h\u2208H R Qj (h). (21\n)\nWe note that inf h\u2208H R Qj (h) = f D,Q (\u03b1 j ). Therefore,\nlim n\u2192+\u221e E S\u223cD n X I Y I R Qj (A(S)) = f D,Q (\u03b1 j ), i.e., lim n\u2192+\u221e g n (\u03b1 j ) = f (\u03b1 j ).(22)\nStep 2. It is easy to check that for any (\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l ,\nlim n\u2192+\u221e g n (\u03b1 1 , ..., \u03b1 l ) = lim n\u2192+\u221e (1 \u2212 l j=1 \u03b1 j )g n (0) + l j=1 \u03b1 j g n (\u03b1 j ) = (1 \u2212 l j=1 \u03b1 j ) lim n\u2192+\u221e g n (0) + l j=1 \u03b1 j lim n\u2192+\u221e g n (\u03b1 j ).(23)\nAccording to Eq. ( 18) and Eq. ( 22), we have\nlim n\u2192+\u221e g n (\u03b1 1 , ..., \u03b1 l ) = f D,Q (\u03b1 1 , ..., \u03b1 l ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , lim n\u2192+\u221e g n (0) = f D,Q (0), lim n\u2192+\u221e g n (\u03b1 j ) = f (\u03b1 j ),(24)\nCombining Eq. (24) with Eq. ( 23), we complete the proof.\nLemma 2. inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h), \u2200\u03b1 \u2208 [0, 1), if and only if for any > 0, {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205.\nProof of Lemma 2. For the sake of convenience, we set f D (\u03b1\n) = inf h\u2208H R \u03b1 D (h), for any \u03b1 \u2208 [0, 1]. First, we prove that f D (\u03b1) = (1 \u2212 \u03b1)f D (0) + \u03b1f D (1), \u2200\u03b1 \u2208 [0, 1) implies {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205.\nFor any > 0 and 0 \u2264 \u03b1 < 1, we can find h \u03b1 \u2208 H satisfying that\nR \u03b1 D (h \u03b1 ) \u2264 inf h\u2208H R \u03b1 D (h) + . Note that inf h\u2208H R \u03b1 D (h) = inf h\u2208H (1 \u2212 \u03b1)R in D (h) + \u03b1R out D (h) \u2265 (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h).\nTherefore,\n(1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h) \u2264 R \u03b1 D (h \u03b1 ) \u2264 inf h\u2208H R \u03b1 D (h) + . (25\n)\nNote that f D (\u03b1) = (1 \u2212 \u03b1)f D (0) + \u03b1f D (1), \u2200\u03b1 \u2208 [0, 1), i.e., inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h), \u2200\u03b1 \u2208 [0, 1). (26\n)\nUsing Eqs. ( 25) and ( 26), we have that for any 0 \u2264 \u03b1 < 1,\n\u2265 R \u03b1 D (h \u03b1 ) \u2212 inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) R in D (h \u03b1 ) \u2212 inf h\u2208H R in D (h) + \u03b1 R out D (h \u03b1 ) \u2212 inf h\u2208H R out D (h) . (27) Since R out D (h \u03b1 ) \u2212 inf h\u2208H R out D (h) \u2265 0 and R in D (h \u03b1 ) \u2212 inf h\u2208H R in D (h) \u2265 0, Eq. (27) implies that: for any 0 < \u03b1 < 1, R in D (h \u03b1 ) \u2264 inf h\u2208H R in D (h) + /(1 \u2212 \u03b1), R out D (h \u03b1 ) \u2264 inf h\u2208H R out D (h) + /\u03b1. Therefore, h \u03b1 \u2208 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + /(1 \u2212 \u03b1)} \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + /\u03b1}.\nIf we set \u03b1 = 0.5, we obtain that for any > 0,\n{h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205.\nSecond, we prove that for any > 0, if\n{h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205, then f D (\u03b1) = (1 \u2212 \u03b1)f D (0) + \u03b1f D (1), for any \u03b1 \u2208 [0, 1). Let h \u2208 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 }. Then, inf h\u2208H R \u03b1 D (h) \u2264 R \u03b1 D (h ) \u2264 (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h) + 2 \u2264 inf h\u2208H R \u03b1 D (h) + 2 , which implies that |f D (\u03b1) \u2212 (1 \u2212 \u03b1)f D (0) \u2212 \u03b1f D (1)| \u2264 2 . As \u2192 0, |f D (\u03b1) \u2212 (1 \u2212 \u03b1)f D (0) \u2212 \u03b1f D (1)| \u2264 0.\nWe have completed the proof.\nTheorem 2. Given a hypothesis space H and a domain D XY , OOD detection is learnable in the single-distribution space D D XY", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "XY", "text": "for H if and only if linear condition (i.e., Condition 1) holds.\nProof of Theorem 2. Based on Lemma 1, we obtain that Condition 1 is the necessary condition for the learnability of OOD detection in the single-distribution space D D XY XY . Next, it suffices to prove that Condition 1 is the sufficient condition for the learnability of OOD detection in the single-distribution space D D XY XY . We use Lemma 2 to prove the sufficient condition. Let F be the infinite sequence set that consists of all infinite sequences, whose coordinates are hypothesis functions, i.e., F = {h = (h 1 , ..., h n , ...) : \u2200h n \u2208 H, n = 1, ...., +\u221e}.\nFor each h \u2208 F , there is a corresponding algorithm A h 6 : A h (S) = h n , if |S| = n. F generates an algorithm class A = {A h : \u2200h \u2208 F }. We select a consistent algorithm from the algorithm class A .\nWe construct a special infinite sequenceh = (h 1 , ...,h n , ...) \u2208 F . For each positive integer n, we selecth n from {h \u2208 H :\nR in D (h ) \u2264 inf h\u2208H R in D (h) + 2/n} \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2/n} (the existence ofh n is based on Lemma 2). It is easy to check that E S\u223cD n X I Y I R in D (Ah(S)) \u2264 inf h\u2208H R in D (h) + 2/n. E S\u223cD n X I Y I R out D (Ah(S)) \u2264 inf h\u2208H R out D (h) + 2/n. Since (1\u2212\u03b1) inf h\u2208H R in D (h)+\u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h), we obtain that for any \u03b1 \u2208 [0, 1], E S\u223cD n X I Y I R \u03b1 D (Ah(S)) \u2264 inf h\u2208H R \u03b1 D (h) + 2/n.\nWe have completed this proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Proofs of Theorem 3 and Theorem 4", "text": "G.1 Proof of Theorem 3 Theorem 3. Given a hypothesis space H and a prior-unknown space D XY , if there is D XY \u2208 D XY , which has overlap between ID and OOD, and inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0, then Condition 1 does not hold. Therefore, OOD detection is not learnable in D XY for H.\nProof of Theorem 3. We first explain how we get f I and f O in Definition 4. Since D X is absolutely continuous respect to \u00b5 (D X \u00b5), then D XI \u00b5 and D XO \u00b5. By Radon-Nikodym Theorem [36], we know there exist two non-negative functions defined over X : f I and f O such that for any \u00b5-measurable set A \u2282 X ,\nD XI (A) = A f I (x)d\u00b5(x), D XO (A) = A f O (x)d\u00b5(x).\nSecond, we prove that for any \u03b1 \u2208 (0, 1), inf h\u2208H R \u03b1 D (h) > 0. We define A m = {x \u2208 X :\nf I (x) \u2265 1 m and f O (x) \u2265 1 m }. It is clear that \u222a +\u221e m=1 A m = {x \u2208 X : f I (x) > 0 and f O (x) > 0} = A overlap ,and\nA m \u2282 A m+1 .\nTherefore,\nlim m\u2192+\u221e \u00b5(A m ) = \u00b5(A overlap ) > 0,\nwhich implies that there exists m 0 such that \u00b5(A m0 ) > 0.\nFor any \u03b1 \u2208 (0, 1), we define\nc \u03b1 = min y1\u2208Y all (1 \u2212 \u03b1) min y2\u2208Y (y 1 , y 2 ) + \u03b1 (y 1 , K + 1) . It is clear that c \u03b1 > 0 for \u03b1 \u2208 (0, 1). Then, for any h \u2208 H, R \u03b1 D (h) = X \u00d7Y all (h(x), y)dD \u03b1 XY (x, y) = X \u00d7Y (1 \u2212 \u03b1) (h(x), y)dD XIYI (x, y) + X \u00d7{K+1} \u03b1 (h(x), y)dD XOYO (x, y) \u2265 Am 0 \u00d7Y (1 \u2212 \u03b1) (h(x), y)dD XIYI (x, y) + Am 0 \u00d7{K+1} \u03b1 (h(x), y)dD XOYO (x, y) = Am 0 (1 \u2212 \u03b1) Y (h(x), y)dD YI|XI (y|x) dD XI (x) + Am 0 \u03b1 (h(x), K + 1)dD XO (x) \u2265 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 )dD XI (x) + Am 0 \u03b1 (h(x), K + 1)dD XO (x) \u2265 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 )f I (x)d\u00b5(x) + Am 0 \u03b1 (h(x), K + 1)f O (x)d\u00b5(x) \u2265 1 m 0 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 )d\u00b5(x) + 1 m 0 Am 0 \u03b1 (h(x), K + 1)d\u00b5(x) = 1 m 0 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 ) + \u03b1 (h(x), K + 1) d\u00b5(x) \u2265 c \u03b1 m 0 \u00b5(A m0 ) > 0. Therefore, inf h\u2208H R \u03b1 D (h) \u2265 c \u03b1 m 0 \u00b5(A m0 ) > 0. Third, Condition 1 indicates that inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R in D(\nh) = 0 (here we have used conditions inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0), which contradicts with inf h\u2208H R \u03b1 D (h) > 0 (\u03b1 \u2208 (0, 1)). Therefore, Condition 1 does not hold. Using Lemma 1, we obtain that OOD detection in D XY is not learnable for H.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Proof of Theorem 4", "text": "Theorem 4 (Impossibility Theorem for Total Space). OOD detection is not learnable in the total space D all XY for H, if |\u03c6 \u2022 H| > 1, where \u03c6 maps ID labels to 1 and maps OOD labels to 2.\nProof of Theorem 4. We need to prove that OOD detection is not learnable in the total space D all XY for H, if H is non-trivial, i.e., {x \u2208 X :\n\u2203h 1 , h 2 \u2208 H, s.t. h 1 (x) \u2208 Y, h 2 (x) = K + 1} = \u2205.\nThe main idea is to construct a domain D XY satisfying that: 1) the ID and OOD distributions have overlap (Definition 4); and 2) R in D (h 1 ) = 0, R out D (h 2 ) = 0. According to the condition that H is non-trivial, we know that there exist\nh 1 , h 2 \u2208 H such that h 1 (x 1 ) \u2208 Y, h 2 (x 1 ) = K +1, for some x 1 \u2208 X . We set D XY = 0.5 * \u03b4 (x1,h1(x1)) +0.5 * \u03b4 (x1,h2(x1)) ,\nwhere \u03b4 is the Dirac measure. It is easy to check that R in D (h 1 ) = 0, R out D (h 2 ) = 0, which implies that inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0. In addition, the ID distribution \u03b4 (x1,h1(x1)) and OOD distribution \u03b4 (x1,h2(x1)) have overlap x 1 . By using Theorem 3, we have completed this proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Proof of Theorem 5", "text": "Before proving Theorem 5, we need three important lemmas. Lemma 3. Suppose that D XY is a domain with OOD convex decomposition Q 1 , ..., Q l (convex decomposition is given by Definition 6 in Appendix F), and D XY is a finite discrete distribution, then (the definition of f D,Q is given in Condition 4)\nf D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , if and only if arg min h\u2208H R D (h) = l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h),\nwhere 0 is the 1 \u00d7 l vector, whose elements are 0, and \u03b1 j is the 1 \u00d7 l vector, whose j-th element is 1 and other elements are 0, and\nR Qj (h) = X \u00d7{K+1}\n(h(x), y)dQ j (x, y).\nProof of Lemma 3. To better understand this proof, we recall the definition of f D,Q (\u03b1 1 , ..., \u03b1 l ):\nf D,Q (\u03b1 1 , ..., \u03b1 l ) = inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) , \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l\nFirst, we prove that if\nf D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , then, arg min h\u2208H R D (h) = l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h). Let D XY = (1 \u2212 l j=1 \u03bb j )D XIYI + l j=1 \u03bb j Q j , for some (\u03bb 1 , ..., \u03bb l ) \u2208 \u2206 o l .\nSince D XY has finite support set, we have\narg min h\u2208H R D (h) = arg min h\u2208H (1 \u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) = \u2205.\nWe can find that h 0 \u2208 arg min h\u2208H (1\n\u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) . Hence, (1 \u2212 l j=1 \u03bb j )R in D (h 0 ) + l j=1 \u03bb j R Qj (h 0 ) = inf h\u2208H (1 \u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) . (28)\nNote that the condition\nf D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ) implies (1\u2212 l j=1 \u03bb j ) inf h\u2208H R in D (h)+ l j=1 \u03bb j inf h\u2208H R Qj (h) = inf h\u2208H (1\u2212 l j=1 \u03bb j )R in D (h)+ l j=1 \u03bb j R Qj (h) . (29)\nTherefore, Eq. ( 28) and Eq. (29) imply that\n(1 \u2212 l j=1 \u03bb j ) inf h\u2208H R in D (h) + l j=1 \u03bb j inf h\u2208H R Qj (h) = (1 \u2212 l j=1 \u03bb j )R in D (h 0 ) + l j=1 \u03bb j R Qj (h 0 ). (30) Since R in D (h 0 ) \u2265 inf h\u2208H R in D (h) and R Qj (h 0 ) \u2265 inf h\u2208H R in Qj(\nh), for j = 1, ..., l, then using Eq. (30), we have that\nR in D (h 0 ) = inf h\u2208H R in D (h), R Qj (h 0 ) = inf h\u2208H R Qj (h), \u2200j = 1, ..., l, which implies that h 0 \u2208 l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h).\nTherefore,\narg min h\u2208H R D (h) \u2282 l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h).(31)\nAdditionally, using\nf D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l ,\nwe obtain that for any h \u2208\nl j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h), inf h\u2208H R D (h) = inf h\u2208H (1 \u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) =(1 \u2212 l j=1 \u03bb j ) inf h\u2208H R in D (h) + l j=1 \u03bb j inf h\u2208H R Qj (h) =(1 \u2212 l j=1 \u03bb j )R in D (h ) + l j=1 \u03bb j R Qj (h ) = R D (h ), which implies that h \u2208 arg min h\u2208H R D (h).\nTherefore,\nl j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h) \u2282 arg min h\u2208H R D (h).(32)\nCombining Eq. (31) with Eq. (32), we obtain that\nl j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h) = arg min h\u2208H R D (h).\nSecond, we prove that if\narg min h\u2208H R D (h) = l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h), then, f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l .\nWe set\nh 0 \u2208 l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h), then, for any (\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l ,(1 \u2212\nl j=1 \u03b1 j ) inf h\u2208H R in D (h) + l j=1 \u03b1 j inf h\u2208H R Qj (h) \u2264 inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) \u2264 (1 \u2212 l j=1 \u03b1 j )R in D (h 0 ) + l j=1 \u03b1 j R Qj (h 0 ) = (1 \u2212 l j=1 \u03b1 j ) inf h\u2208H R in D (h) + l j=1 \u03b1 j inf h\u2208H R Qj (h).\nTherefore, for any (\u03b1 1 , ..., \u03b1\nl ) \u2208 \u2206 o l ,(1 \u2212\nl j=1 \u03b1 j ) inf h\u2208H R in D (h) + l j=1 \u03b1 j inf h\u2208H R Qj (h) = inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) , which implies that: for any (\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ).\nWe have completed this proof.\nLemma 4. Suppose that Assumption 1 holds. If there is a finite discrete domain D XY \u2208 D s XY such that inf h\u2208H R out D (h) > 0, then OOD detection is not learnable in D s XY for H.\nProof of Lemma 4. Suppose that suppD XO = {x out 1 , ..., x out l }, then it is clear that D XY has OOD convex decomposition \u03b4 x out 1 , ..., \u03b4 x out l , where \u03b4 x is the dirac measure whose support set is {x}.\nSince H is the separate space for OOD (i.e., Assumption 1 holds), then \u2200j = 1, ..., l,\ninf h\u2208H R \u03b4 x out j (h) = 0, where R \u03b4 x out j (h) = X (h(x), K + 1)d\u03b4 x out j (x).\nThis implies that: if\nl j=1 arg min h\u2208H R \u03b4 x out j (h) = \u2205, then for \u2200h \u2208 l j=1 arg min h\u2208H R \u03b4 x out j (h), h (x out i ) = K + 1, \u2200i = 1, ..., l. Therefore, if l j=1 arg min h\u2208H R \u03b4 x out j (h) arg min h\u2208H R in D (h) = \u2205, then for any h * \u2208 l j=1 arg min h\u2208H R \u03b4 x out j (h) arg min h\u2208H R in D (h), we have that h * (x out i ) = K + 1, \u2200i = 1, ..., l.\nProof by Contradiction: assume OOD detection is learnable in D s XY for H, then Lemmas 1 and 3 imply that\nl j=1 arg min h\u2208H R \u03b4 x out j (h) arg min h\u2208H R in D (h) = arg min h\u2208H R D (h) = \u2205.\nTherefore, for any h * \u2208 arg min h\u2208H R D (h), we have that h * (x out i ) = K + 1, \u2200i = 1, ..., l, which implies that for any h * \u2208 arg min h\u2208H R D (h), we have R out D (h * ) = 0, which implies that inf h\u2208H R out D (h) = 0. It is clear that inf h\u2208H R out D (h) = 0 is inconsistent with the condition inf h\u2208H R out D (h) > 0. Therefore, OOD detection is not learnable in D s XY for H.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 5.", "text": "If Assumption 1 holds, VCdim(\u03c6 \u2022 H) = v < +\u221e and sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| > m such that v < m, then OOD detection is not learnable in D s XY for H, where \u03c6 maps ID's labels to 1 and maps OOD's labels to 2.\nProof of Lemma 5. Due to sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| > m, we can obtain a set\nC = {x 1 , ..., x m , x m+1 }, which satisfies that there existsh \u2208 H such thath(x i ) \u2208 Y for any i = 1, ..., m, m + 1. Let H \u03c6 C = {(\u03c6 \u2022 h(x 1 ), ..., \u03c6 \u2022 h(x m ), \u03c6 \u2022 h(x m+1 ) : h \u2208 H}. It is clear that (1, 1, ..., 1) = (\u03c6 \u2022h(x 1 ), ..., \u03c6 \u2022h(x m ), \u03c6 \u2022h(x m+1 )) \u2208 H \u03c6 C , where (1, 1, ..., 1) means all elements are 1. Let H \u03c6 m+1 = {(\u03c6\u2022h(x 1 ), ..., \u03c6\u2022h(x m ), \u03c6\u2022h(x m+1 ) : h is any hypothesis function from X to Y all }.\nClearly, H \u03c6 C \u2282 H \u03c6 m+1 and |H \u03c6 m+1 | = 2 m+1 . Sauer-Shelah-Perles Lemma (Lemma 6.10 in [21]) implies that\n|H \u03c6 C | \u2264 v i=0 m+1 i . Since v i=0 m+1 i < 2 m+1 \u2212 1 (because v < m), we obtain that |H \u03c6 C | \u2264 2 m+1 \u2212 2. Therefore, H \u03c6 C \u222a {(2, 2..., 2)} is a proper subset of H \u03c6 m+1\n, where (2, 2, ..., 2) means that all elements are 2. Note that (1, 1..., 1) (all elements are 1) also belongs to H \u03c6 C . Hence, H \u03c6 C \u222a {(2, 2..., 2)} \u222a {(1, 1..., 1)} is a proper subset of H \u03c6 m+1 , which implies that we can obtain a hypothesis function h satisfying that:\n1)(\u03c6 \u2022 h (x 1 ), ..., \u03c6 \u2022 h (x m ), \u03c6 \u2022 h (x m+1 )) / \u2208 H \u03c6 C ; 2) There exist x j , x p \u2208 C such that \u03c6 \u2022 h (x j ) = 2 and \u03c6 \u2022 h (x p ) = 1. Let C I = C \u2229 {x \u2208 X : \u03c6 \u2022 h (x) = 1} and C O = C \u2229 {x \u2208 X : \u03c6 \u2022 h (x) = 2};\nThen, we construct a special domain D XY :\nD XY = 0.5 * D XI * D YI|XI + 0.5 * D XO * D YO|XO , where D XI = 1 |C I | x\u2208CI \u03b4 x and D YI|XI (y|x) = 1, ifh(x) = y and x \u2208 C I ; and D XO = 1 |C O | x\u2208CO \u03b4 x and D YO|XO (K + 1|x) = 1, if x \u2208 C O . Since D XY is a finite discrete distribution and (\u03c6 \u2022 h (x 1 ), ..., \u03c6 \u2022 h (x m ), \u03c6 \u2022 h (x m+1 )) / \u2208 H \u03c6 C , it is clear that arg min h\u2208H R D (h) = \u2205 and inf h\u2208H R D (h) > 0. Additionally, R in D (h) = 0. Therefore, inf h\u2208H R in D (h) = 0. Proof by Contradiction: suppose that OOD detection is learnable in D s XY for H, then Lemma 1 implies that inf h\u2208H R D (h) = 0.5 * inf h\u2208H R in D (h) + 0.5 * inf h\u2208H R out D (h).\nTherefore, if OOD detection is learnable in D s XY for H, then inf h\u2208H R out D (h) > 0. Until now, we have constructed a domain D XY (defined over X \u00d7 Y all ) with finite support and satisfying that inf h\u2208H R out D (h) > 0. Note that H is the separate space for OOD data (Assumption 1 holds). Using Lemma 4, we know that OOD detection is not learnable in D s XY for H, which is inconsistent with our assumption that OOD detection is learnable in D s XY for H. Therefore, OOD detection is not learnable in D s XY for H. We have completed the proof.\nTheorem 5 (Impossibility Theorem for Separate Space). If Assumption 1 holds, VCdim(\u03c6 \u2022 H) < +\u221e and sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| = +\u221e, then OOD detection is not learnable in separate space D s XY for H, where \u03c6 maps ID labels to 1 and maps OOD labels to 2.\nProof of Theorem 5. Let VCdim(\u03c6 \u2022 H) = v. Since sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| = +\u221e, it is clear that sup h\u2208H |{x \u2208 X : h(x) \u2208 Y}| > v. Using Lemma 5, we complete this proof.\nI Proofs of Theorem 6 and Theorem 7\nI.1 Proof of Theorem 6\nFirstly, we need two lemmas, which are motivated by Lemma 19.2 and Lemma 19.3 in [21].\nLemma 6. Let C 1 ,...,C r be a cover of space X , i.e., r i=1 C i = X . Let S X = {x 1 , ..., x n } be a sequence of n data drawn from D XI , i.i.d. Then\nE S X \u223cD n X I i:Ci\u2229S X =\u2205 D XI (C i ) \u2264 r en .\nProof of Lemma 6.\nE S X \u223cD n X I i:Ci\u2229S X =\u2205 D XI (C i ) = r i=1 D XI (C i ) \u2022 E S X \u223cD n X I 1 Ci\u2229S X =\u2205 ,\nwhere 1 is the characteristic function.\nFor each i,\nE S X \u223cD n X I 1 Ci\u2229S X =\u2205 = X n 1 Ci\u2229S X =\u2205 dD n XI (S X ) = X 1 Ci\u2229{x}=\u2205 dD XI (x) n = 1 \u2212 D XI (C i ) n \u2264 e \u2212nD X I (Ci) .\nTherefore,\nE S X \u223cD n X I i:Ci\u2229S=\u2205 D XI (C i ) \u2264 r i=1 D XI (C i )e \u2212nD X I (Ci) \u2264 r max i\u2208{1,...,r} D XI (C i )e \u2212nD X I (Ci) \u2264 r ne ,\nhere we have used inequality: max i\u2208{1,...,r} a i e \u2212nai \u2264 1/(ne). The proof has been completed.\nLemma 7. Let K = 1. When X \u2282 R d is a bounded set, there exists a monotonically decreasing sequence cons (m) satisfying that cons (m) \u2192 0, as m \u2192 0, such that\nE x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) < cons (n),\nwhere dist is the Euclidean distance, \u03c0 1 (x, S) = arg minx \u2208S X dist(x,x), here S X is the feature part of S, i.e., S X = {x 1 , ..., x n }, if S = {(x 1 , y 1 ), ..., (x n , y n )}.\nProof of Lemma 7. Since X is bounded, without loss of generality, we set X \u2282 [0, 1) d . Fix If x, x belong to some\nC i , then dist(x, x ) \u2264 \u221a d ; otherwise, dist(x, x ) \u2264 \u221a d. Therefore, E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) \u2264E S\u223cD n X I Y I \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) + \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) \u2264E S X \u223cD n X I \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) + \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) .\nNote that C 1 , ..., C r are disjoint.\nTherefore, i:\nCi\u2229S X =\u2205 D XI (C i ) \u2264 D XI ( i:Ci\u2229S X =\u2205 C i ) \u2264 1.\nUsing Lemma 6, we obtain d+1) , we complete this proof.\nE x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) \u2264 \u221a d + r \u221a d ne = \u221a d + \u221a d ne d . If we set = 2n \u22121/(d+1) , then E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) \u2264 2 \u221a d n 1/(d+1) + \u221a d 2 d en 1/(d+1) .", "publication_ref": ["b20"], "figure_ref": [], "table_ref": []}, {"heading": "If we set cons", "text": "(n) = 2 \u221a d n 1/(d+1) + \u221a d 2 d en 1/(\nTheorem 6. Let K = 1 and |X | < +\u221e. Suppose that Assumption 1 holds and the constant function h in := 1 \u2208 H. Then OOD detection is learnable in D s XY for H if and only if H all \u2212 {h out } \u2282 H, where H all is the hypothesis space consisting of all hypothesis functions, and h out is a constant function that h out := 2, here 1 represents ID data and 2 represents OOD data.\nProof of Theorem 6. First, we prove that if the hypothesis space H is a separate space for OOD (i.e., Assumption 1 holds), the constant function h in := 1 \u2208 H, then that OOD detection is learnable in D s XY for H implies H all \u2212 {h out } \u2282 H. Proof by Contradiction: suppose that there exists h \u2208 H all such that h = h out and h / \u2208 H.\nLet X = {x 1 , ..., x m }, C I = {x \u2208 X : h (x) \u2208 Y} and C O = {x \u2208 X : h (x) = K + 1}.\nBecause h = h out , we know that C I = \u2205.\nWe \ninf h\u2208H R D (h) > 0. Additionally, R in D (h in ) = 0 (here h in = 1), hence, inf h\u2208H R in D (h) = 0. Since OOD detection is learnable in D s XY for H, Lemma 1 implies that inf h\u2208H R D (h) = (1 \u2212 \u03c0 out ) inf h\u2208H R in D (h) + \u03c0 out inf h\u2208H R out D (h),\nwhere \u03c0 out = D Y (Y = K + 1) = 1 or 0.5. Since inf h\u2208H R in D (h) = 0 and inf h\u2208H R D (h) > 0, we obtain that inf h\u2208H R out D (h) > 0. Until now, we have constructed a special domain D XY \u2208 D s XY satisfying that inf h\u2208H R out D (h) > 0. Using Lemma 4, we know that OOD detection in D s XY is not learnable for H, which is inconsistent with the condition that OOD detection is learnable in D s XY for H. Therefore, the assumption (there exists h \u2208 H all such that h = h out and h / \u2208 H) doesn't hold, which implies that H all \u2212 {h out } \u2282 H.\nSecond, we prove that if H all \u2212 {h out } \u2282 H, then OOD detection is learnable in D s XY for H. To prove this result, we need to design a special algorithm. Let d 0 = min x,x \u2208X and x =x dist(x, x ), where dist is the Euclidean distance. It is clear that d 0 > 0. Let\nA(S)(x) = 1, if dist(x, \u03c0 1 (x, S)) < 0.5 * d 0 ; 2, if dist(x, \u03c0 1 (x, S)) \u2265 0.5 * d 0 ,\nwhere \u03c0 1 (x, S) = arg minx \u2208S X dist(x,x), here S X is the feature part of S, i.e., S X = {x 1 , ..., x n }, if S = {(x 1 , y 1 ), ..., (x n , y n )}.\nFor any x \u2208 suppD XI , it is easy to check that for almost all S \u223c D n XIYI , dist(x, \u03c0 1 (x, S)) > 0.5 * d 0 , which implies that\nA(S)(x) = 2, hence, E S\u223cD n X I Y I R out D (A(S)) = 0. (33\n)\nUsing Lemma 7, for any x \u2208 suppD XI , we have\nE x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) < cons (n),\nwhere cons (n) \u2192 0, as n \u2192 0 and cons (n) is a monotonically decreasing sequence.\nHence, we have that \nD\nwhere B = max{ (1, 2), (2, 1)}. Using Eq. (33) and Eq. (34), we have proved that\nE S\u223cD n X I Y I R D (A(S)) \u2264 0 + 2B cons (m)/d 0 \u2264 inf h\u2208H R D (h) + 2B cons (m)/d 0 . (35\n)\nIt is easy to check that A(S) \u2208 H all \u2212 {h out }. Therefore, we have constructed a consistent algorithm A for H. We have completed this proof.\nI. Proof of Theorem 7. Since |X | < +\u221e, we know that |H| < +\u221e, which implies that H in is agnostic PAC learnable for supervised learning in classification. Therefore, there exist an algorithm A in : \u222a +\u221e n=1 (X \u00d7 Y) n \u2192 H in and a monotonically decreasing sequence (n), such that (n) \u2192 0, as n \u2192 +\u221e, and for any D XY \u2208 D s XY ,\nE S\u223cD n X I Y I R in D (A in (S)) \u2264 inf h\u2208H in R in D (h) + (n).\nSince |X | < +\u221e and H b almost contains all binary classifiers, then using Theorem 6 and Theorem 1, we obtain that there exist an algorithm A b : \u222a +\u221e n=1 (X \u00d7 {1, 2}) n \u2192 H b and a monotonically decreasing sequence (n), such that (n) \u2192 0, as n \u2192 +\u221e, and for any D XY \u2208 D s XY ,\nE S\u223cD n X I Y I R in \u03c6(D) (A b (\u03c6(S))) \u2264 inf h\u2208H b R in \u03c6(D) (h) + (n), E S\u223cD n X I Y I R out \u03c6(D) (A b (\u03c6(S))) \u2264 inf h\u2208H b R out \u03c6(D) (h) + (n),\nwhere \u03c6 maps ID's labels to 1 and OOD's label to 2,\nR in \u03c6(D) (A b (\u03c6(S))) = X \u00d7Y (A b (\u03c6(S))(x), \u03c6(y))dD XIYI (x, y),(36)\nR in \u03c6(D) (h) = X \u00d7Y (h(x), \u03c6(y))dD XIYI (x, y),(37)\nR out \u03c6(D) (A b (\u03c6(S))) = X \u00d7{K+1} (A b (\u03c6(S))(x), \u03c6(y))dD XOYO (x, y),(38)\nand\nR out \u03c6(D) (h) = X \u00d7{K+1} (h(x), \u03c6(y))dD XOYO (x, y),(39)\nhere \u03c6(S) = {(x 1 , \u03c6(y 1 )), ..., (x n , \u03c6(y n ))}, if S = {(x 1 , y 1 ), ..., (x n , y n )}.\nNote that H b almost contains all classifiers, and D s XY is the separate space. Hence,\nE S\u223cD n X I Y I R in \u03c6(D) (A b (\u03c6(S))) \u2264 (n), E S\u223cD n X I Y I R out \u03c6(D) (A b (\u03c6(S))) \u2264 (n).\nNext, we construct an algorithm A using A in and A out . Since inf h\u2208H R in \u03c6(D) (\u03c6 \u2022 h) = 0, inf h\u2208H R out D (h) = 0, then by Condition 2, it is easy to check that\ninf h\u2208H in R in D (h) = inf h\u2208H R in D (h).\nAdditionally, the risk R in D (A(S)) is from two parts: 1) ID data are detected as OOD data; 2) ID data are detected as ID data, but are classified as incorrect ID classes. Therefore, we have the inequality:\nE S\u223cD n X I Y I R in D (A(S)) \u2264 E S\u223cD n X I Y I R in D (A in (S)) + cE S\u223cD n X I Y I R in \u03c6(D) (A b (\u03c6(S))) \u2264 inf h\u2208H in R in D (h) + (n) + c (n) = inf h\u2208H R in D (h) + (n) + c (n),(40)\nwhere c = max y1,y2\u2208Y (y 1 , y 2 )/ min{ (1, 2), (2, 1)}.\nNote that the risk R out D (A(S)) is from the case that OOD data are detected as ID data. Therefore,\nE S\u223cD n X I Y I R out D (A(S)) \u2264 cE S\u223cD n X |rmI Y I R out \u03c6(D) (A b (\u03c6(S))) \u2264 c (n) \u2264 inf h\u2208H R out D (h) + c (n). (41\n) Note that (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h).\nThen, using Eq. (40) and Eq. (41), we obtain that for any \u03b1 \u2208 [0, 1],\nE S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + (n) + c (n).\nAccording to Theorem 1 (the second result), we complete the proof. Proof. Let F be a set consisting of all infinite sequences, whose coordinates are hypothesis functions, i.e., F = {h = (h 1 , ..., h n , ...) : \u2200h n \u2208 H, n = 1, ...., +\u221e}.\nFor each h \u2208 F , there is a corresponding algorithm A h : A h (S) = h n , if |S| = n. F generates an algorithm class A = {A h : \u2200h \u2208 F }. We select a consistent algorithm from the algorithm class A .\nWe construct a special infinite sequenceh = (h 1 , ...,h n , ...) \u2208 F . For each positive integer n, we selecth n from\n\u2200D XY \u2208[D XY ] {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h)+2/n} {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h)+2/n}.\nThe existence ofh n is based on Condition 3. It is easy to check that for any\nD XY \u2208 [D XY ], E S\u223cD n X I Y I R in D (Ah(S)) \u2264 inf h\u2208H R in D (h) + 2/n. E S\u223cD n X I Y I R out D (Ah(S)) \u2264 inf h\u2208H R out D (h) + 2/n. Since (1\u2212\u03b1) inf h\u2208H R in D (h)+\u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h), we obtain that for any \u03b1 \u2208 [0, 1], E S\u223cD n X I Y I R \u03b1 D (Ah(S)) \u2264 inf h\u2208H R \u03b1 D (h) + 2/n.\nUsing Theorem 1 (the second result), we have completed this proof.\nTheorem 8. Suppose that X is a bounded set. OOD detection is learnable in the finite-ID-distribution space D F XY for H if and only if the compatibility condition (i.e., Condition 3) holds. Furthermore, the learning rate cons (n) can attain O(1/ \u221a n 1\u2212\u03b8 ), for any \u03b8 \u2208 (0, 1).\nProof of Theorem 8.\nFirst, we prove that if OOD detection is learnable in D F XY for H, then Condition 3 holds. Since D F XY is the prior-unknown space, by Theorem 1, there exist an algorithm A : \u222a +\u221e n=1 (X \u00d7Y) n \u2192 H and a monotonically decreasing sequence cons (n), such that cons (n) \u2192 0, as n \u2192 +\u221e, and for any D XY \u2208 D F XY ,\nE S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 cons (n), E S\u223cD n X I Y I R out D (A(S)) \u2212 inf h\u2208H R out D (h) \u2264 cons (n).\nThen, for any > 0, we can find n such that \u2265 cons (n ), therefore, if n = n , we have\nE S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 , E S\u223cD n X I Y I R out D (A(S)) \u2212 inf h\u2208H R out D (h) \u2264 , which implies that there exists S \u223c D n XIYI such that R in D (A(S )) \u2212 inf h\u2208H R in D (h) \u2264 , R out D (A(S )) \u2212 inf h\u2208H R out D (h) \u2264 .\nTherefore, for any equivalence class [D XY ] with respect to D F XY and any > 0, there exists a hypothesis function A(S ) \u2208 H such that for any domain\nD XY \u2208 [D XY ], A(S ) \u2208 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + } \u2229 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + },\nwhich implies that Condition 3 holds.\nSecond, we prove Condition 3 implies the learnability of OOD detection in D F XY for H. For convenience, we assume that all equivalence classes are [D 1 XY ], ..., [D m XY ]. By Lemma 8, for every equivalence class [D i XY ], we can find a corresponding algorithm A D i such that OOD detection is learnable in [D i XY ] for H. Additionally, we also set the learning rate for A D i is i (n). By Lemma 8, we know that i (n) can attain O(1/n).\nLet Z be X \u00d7 Y. Then, we consider a bounded universal kernel K(\u2022, \u2022) defined over Z \u00d7 Z. Consider the maximum mean discrepancy (MMD) [83], which is a metric between distributions: for any distributions P and Q defined over Z, we use MMD K (Q, P ) to represent the distance.\nLet F be a set consisting of all finite sequences, whose coordinates are labeled data, i.e., F = {S = (S 1 , ..., S i , ..., S m ) : \u2200i = 1, ..., m and \u2200 labeled data S i }.\nThen, we define an algorithm space as follows: and \u03b4 (x,y) is the Dirac measure. Next, we prove that we can find an algorithm A from the algorithm space A such that A is the consistent algorithm.\nA = {A S 7 : \u2200 S \u2208 F },\nSince the number of different equivalence classes is finite, we know that there exists a constant c > 0 such that for any different equivalence classes [D i XY ] and [D j XY ] (i = j),\nMMD K (D i XIYI , D j XIYI ) > c.\nAdditionally, according to [83] and the property of D F XY (the number of different equivalence classes is finite), there exists a monotonically decreasing (n) \u2192 0, as n \u2192 +\u221e such that for any D XY \u2208 D,\nE S\u223cD n X I Y I MMD K (D XIYI , P S ) \u2264 (n), where (n) = O( 1 \u221a n 1\u2212\u03b8 ).(42)\nTherefore, for every equivalence class [D i XY ], we can find data points S D i such that\nMMD K (D i XIYI , P S D i ) < c 100 . Let S = {S D 1 , ..., S D i , ..., S D m }.\nThen, we prove that A S is a consistent algorithm. By Eq. (42), it is easy to check that for any i \u2208 {1, ..., m} and any 0 < \u03b4 < 1,\nP S\u223cD i,n X I Y I MMD K (D i XIYI , P S ) \u2264 (n) \u03b4 > 1 \u2212 \u03b4, which implies that P S\u223cD i,n X I Y I MMD K (P S D i , P S ) \u2264 (n) \u03b4 + c 100 > 1 \u2212 \u03b4.\nTherefore, (here we set \u03b4 = 200 (n)/c)\nP S\u223cD i,n X I Y I A S (S) = A D i (S) \u2264 200 (n) c .\nBecause A D i is a consistent algorithm for [D i XY ], we conclude that for all \u03b1 \u2208 [0, 1],\nE S\u223cD i,n X I Y I R \u03b1 D (A S (S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 i (n) + 200B (n) c ,\nwhere i (n) = O(1/n) is the learning rate of A D i and B is the upper bound of the loss .\nLet max (n) = max{ 1 (n), ..., m (n)} + 200B (n) c .\nThen, we obtain that for any D XY \u2208 D F XY and all \u03b1 \u2208 [0, 1],\nE S\u223cD n X I Y I R \u03b1 D (A S (S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 max (n) = O( 1 \u221a n 1\u2212\u03b8 ).\nAccording to Theorem 1 (the second result), A S is the consistent algorithm. This proof is completed.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "J.2 Proof of Theorem 9", "text": "Theorem 9. Given a density-based space D \u00b5,b XY , if \u00b5(X ) < +\u221e, the Realizability Assumption holds, then when H has finite Natarajan dimension [21], OOD detection is learnable in D \u00b5,b XY for H. Furthermore, the learning rate cons (n) can attain O(1/ \u221a n 1\u2212\u03b8 ), for any \u03b8 \u2208 (0, 1).\nProof of Theorem 9. First, we consider the case that the loss is the zero-one loss.\nSince \u00b5(X ) < +\u221e, without loss of generality, we assume that \u00b5(X ) = 1. We also assume that Realizability Assumption, it is obvious that for any samples S = {(x 1 , y 1 ), ..., (x n , y n )} \u223c D n XIYI , i.i.d., we have that there exists h * \u2208 H such that\n1 n n i=1 (h * (x i ), y i ) = 0.\nGiven m data points S m = {x 1 , ..., x m } \u2282 X m . We consider the following learning rule:\nmin h\u2208H 1 m m j=1 (h(x j ), K + 1), subject to 1 n n i=1 (h(x i ), y i ) = 0.\nWe denote the algorithm, which solves the above rule, as A Sm 8 . For different data points S m , we have different algorithm A Sm . Let S be the infinite sequence set that consists of all infinite sequences, whose coordinates are data points, i.e., \nUsing S, we construct an algorithm space as follows:\nA := {A S : \u2200 S \u2208 S}, where A S (S) = A Sn (S), if |S| = n.\nNext, we prove that there exists an algorithm A S \u2208 A , which is a consistent algorithm. Given data points S n \u223c \u00b5 n , i.i.d., using the Natarajan dimension theory and Empirical risk minimization principle [21], it is easy to obtain that there exists a uniform constant C \u03b8 such that (we mainly use the uniform bounds to obtain the following bounds)\nE S\u223cD n X I Y I sup h\u2208H S R in D (h) \u2264 inf h\u2208H R in D (h) + C \u03b8 \u221a n 1\u2212\u03b8 ,\nand because of H S \u2282 H,\nE Sn\u223c\u00b5 n sup S\u2208(X \u00d7Y) n [R \u00b5 (A Sn (S), K + 1) \u2212 inf h\u2208H S R \u00b5 (h, K + 1)] \u2264 C \u03b8 \u221a n 1\u2212\u03b8 ,(44)\nwhere\nH S = {h \u2208 H : n i=1 (h(x i ), y i ) = 0}, here S = {(x 1 , y 1 ), ..., (x n , y n )}, and R \u00b5 (h, K + 1) = E x\u223c\u00b5 (h(x), K + 1) = X (h(x), K + 1)d\u00b5(x).\nWe set D I = {D XIYI : there exists D XOYO such that (1 \u2212 \u03b1)D XIYI + \u03b1D XOYO \u2208 D \u00b5,b XY }. Then by Eq. (44), we have\nE Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I [R \u00b5 (A Sn (S), K + 1) \u2212 inf h\u2208H S R \u00b5 (h, K + 1)] \u2264 C \u03b8 \u221a n 1\u2212\u03b8 . (45\n)\nDue to Realizability Assumption, we obtain that inf h\u2208H R in D (h) = 0. Therefore,\nE S\u223cD n X I Y I sup h\u2208H S R in D (h) \u2264 C \u03b8 \u221a n 1\u2212\u03b8 ,(46)\nwhich implies that (in following inequalities, g is the groundtruth labeling function, i.e., R D (g) = 0)\nC \u03b8 \u221a n \u2265 E S\u223cD n X I Y I sup h\u2208H S R in D (h) =E S\u223cD n X I Y I sup h\u2208H S g<K+1 (h(x), g(x))f I (x)d\u00b5(x) \u2265 2 b E S\u223cD n X I Y I sup h\u2208H S g<K+1 (h(x), g(x))d\u00b5(x).\nThis implies that (here we have used the property of zero-one loss)\nE S\u223cD n X I Y I inf h\u2208H S g<K+1 (h(x), K + 1)d\u00b5(x) \u2265 \u00b5(x \u2208 X : g(x) < K + 1) \u2212 C \u03b8 b 2 \u221a n 1\u2212\u03b8 .\nTherefore,\nE S\u223cD n X I Y I inf h\u2208H S R \u00b5 (h, K + 1) \u2265 \u00b5(x \u2208 X : g(x) < K + 1) \u2212 C \u03b8 b 2 \u221a n 1\u2212\u03b8 .(47)\nAdditionally, R \u00b5 (g, K + 1) = \u00b5(x \u2208 X : g(x) < K + 1) and g \u2208 H S , which implies that\ninf h\u2208H S R \u00b5 (h, K + 1) \u2264 \u00b5(x \u2208 X : g(x) < K + 1).(48)\nCombining inequalities (47) and ( 48), we obtain that\nE S\u223cD n X I Y I inf h\u2208H S R \u00b5 (h, K + 1) \u2212 \u00b5(x \u2208 X : g(x) < K + 1) \u2264 C \u03b8 b 2 \u221a n 1\u2212\u03b8 . (49\n)\nUsing inequalities (45) and (49), we obtain that E Sn\u223c\u00b5 n sup\nD X I Y I \u2208DI E S\u223cD n X I Y I R \u00b5 (A Sn (S), K + 1) \u2212 \u00b5(x \u2208 X : g(x) < K + 1) \u2264 C \u03b8 (b + 1) \u221a n 1\u2212\u03b8 .(50)\nUsing inequality (46), we have\nE Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I R in D (A Sn (S)) \u2264 C \u03b8 \u221a n 1\u2212\u03b8 ,(51)\nwhich implies that (here we use the property of zero-one loss)\nE Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I \u2212 g<K+1 (A Sn (S)(x), K + 1)d\u00b5(x) + \u00b5(x \u2208 X : g(x) < K + 1) \u2264 2bC \u03b8 \u221a n 1\u2212\u03b8 . (52\n)\nCombining inequalities (50) and (52), we have\nE Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I g=K+1 (A Sn (S)(x), K + 1)d\u00b5(x) \u2264 2bC \u03b8 \u221a n 1\u2212\u03b8 + C \u03b8 (b + 1) \u221a n 1\u2212\u03b8 .\nTherefore, there exist data points S n such that sup\nD X I Y I \u2208DI E S\u223cD n X I Y I R out D (A S n ) = sup D X I Y I \u2208DI E S\u223cD n X I Y I g=K+1 (A S n (S)(x), K + 1)f O (x)d\u00b5(x) \u22642b sup D X I Y I \u2208DI E S\u223cD n X I Y I g=K+1 (A S n (S)(x), K + 1)d\u00b5(x) \u2264 4b 2 C \u03b8 \u221a n 1\u2212\u03b8 + 2C \u03b8 (b 2 + b) \u221a n 1\u2212\u03b8 .(53)\nCombining inequalities (46) and (53), we obtain that for any n, there exists data points S n such that\nE S\u223cD n X I Y I R \u03b1 D (A S n ) \u2264 max 4b 2 C \u03b8 \u221a n 1\u2212\u03b8 + 2C \u03b8 (b 2 + b) \u221a n 1\u2212\u03b8 , C \u03b8 \u221a n 1\u2212\u03b8 .\nWe set data point sequences S = (S 1 , S 2 , ..., S n , ...). Then, A S \u2208 A is the universally consistent algorithm, i.e., for any \u03b1 \u2208 [0, 1]\nE S\u223cD n X I Y I R \u03b1 D (A S ) \u2264 max 4b 2 C \u03b8 \u221a n 1\u2212\u03b8 + 2C \u03b8 (b 2 + b) \u221a n 1\u2212\u03b8 , C \u03b8 \u221a n 1\u2212\u03b8 .\nWe have completed this proof when is the zero-one loss.\nSecond, we prove the case that is not the zero-one loss. We use the notation 0\u22121 as the zero-one loss. According the definition of loss introduced in Section 2, we know that there exists a constant M > 0 such that for any y 1 , y 2 \u2208 Y all ,\n1 M 0\u22121 (y 1 , y 2 ) \u2264 (y 1 , y 2 ) \u2264 M 0\u22121 (y 1 , y 2 ). Hence, 1 M R \u03b1, 0\u22121 D (h) \u2264 R \u03b1, D (h) \u2264 M R \u03b1, 0\u22121 D (h),\nwhere R\n\u03b1, 0\u22121 D\nis the \u03b1-risk with zero-one loss, and R \u03b1, D is the \u03b1-risk for loss . Above inequality tells us that Realizability Assumption holds with zero-one loss if and only if Realizability Assumption holds with the loss . Therefore, we use the result proven in first step. We can find a consistent algorithm A such that for any \u03b1 \u2208 [0, 1],\nE S\u223cD n X I Y I R \u03b1, 0\u22121 D (A) \u2264 O( 1 \u221a n 1\u2212\u03b8 ),\nwhich implies that for any \u03b1 \u2208\n[0, 1], 1 M E S\u223cD n X I Y I R \u03b1, D (A) \u2264 O( 1 \u221a n 1\u2212\u03b8 ).\nWe have completed this proof.", "publication_ref": ["b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "K Proof of Proposition 1 and Proof of Proposition 2", "text": "To better understand the contents in Appendices K-M, we introduce the important notations for FCNN-based hypothesis space and score-based hypothesis space detaily.\nFCNN-based Hypothesis Space. Given a sequence q = (l 1 , l 2 , ..., l g ), where l i and g are positive integers and g > 2, we use g to represent the depth of neural network and use l i to represent the width of the i-th layer. After the activation function \u03c3 is selected, we can obtain the architecture of FCNN according to the sequence q. Given any weights w i \u2208 R li\u00d7li\u22121 and bias b i \u2208 R li\u00d71 , the output of the i-layer can be written as follows: for any x \u2208 R l1 , f i (x) = \u03c3(w i f i\u22121 (x) + b i ), \u2200i = 2, ..., g \u2212 1,\nwhere f i\u22121 (x) is the i-th layer output and f 1 (x) = x. Then, the output of FCNN is f w,b (x) = w g f g\u22121 (x) + b g , where w = {w 2 , ..., w g } and b = {b 2 , ..., b g }.\nAn FCNN-based scoring function space is defined as:\nF \u03c3 q := {f w,b : \u2200w i \u2208 R li\u00d7li\u22121 , \u2200b i \u2208 R li\u00d71 , i = 2, ..., g}.\nAdditionally, given two sequences q = (l 1 , ..., l g ) and q = (l 1 , ..., l g ), we use the notation q q to represent the following equations and inequalities:\ng \u2264 g , l 1 = l 1 , l g = l g , l i \u2264 l i , \u2200i = 1, ..., g \u2212 1,\nl g\u22121 \u2264 l i , \u2200i = g, ..., g \u2212 1.\nGiven a sequence q = (l 1 , ...l g ) satisfying that l 1 = d and l g = K + 1, the FCNN-based scoring function space F \u03c3 q can induce an FCNN-based hypothesis space. Before defining the FCNN-based hypothesis space, we define the induced hypothesis function. For any f w,b \u2208 F \u03c3 q , the induced hypothesis function is: We will show that f w,b \u2208 F \u03c3 q . We construct f w ,b as follows: if i = 2, ..., g \u2212 1, then w i = w and b i = b i ; if i = g, ..., g \u2212 1, then w i = I lg\u22121\u00d7lg\u22121 and b i = 0 lg\u22121\u00d71 , where I lg\u22121\u00d7lg\u22121 is the l g\u22121 \u00d7 l g\u22121 identity matrix, and 0 lg\u22121\u00d71 is the l g\u22121 \u00d7 1 zero matrix; and if i = g , then w g = w g , b g = b g . Then it is easy to check that the output of the i-th layer is f i = f g\u22121 , \u2200i = g \u2212 1, g, ..., g \u2212 1.\nTherefore, f w ,b = f w,b , which implies that F \u03c3 q \u2282 F \u03c3 q . Hence, H \u03c3 q \u2282 H \u03c3 q . When g = g , we use Lemma 9 (q and q satisfy the condition in Lemma 9), which implies that F \u03c3 q \u2282 F \u03c3 q , H \u03c3 q \u2282 H \u03c3 q . Therefore, F \u03c3 q \u2282 F \u03c3 q , H \u03c3 q \u2282 H \u03c3 q .\nLemma 11. It is easy to find a sequence q = (l 1 , ..., l g ) (l g = 1) such that q i q, for all i = 1, ..., l. Using Lemma 10, we obtain that F \u03c3 q i \u2282 F \u03c3 q . Therefore, Therefore, for each i, we can find g w i ,b i from F \u03c3 q such that\ninf\nmax x\u2208C |g w i ,b i (x) \u2212 f i (x)| < / \u221a l,\nsup h\u2208H \u03c3 q |{x \u2208 X : h(x) \u2208 Y}| = +\u221e, when |X | = +\u221e. Therefore, Theorem 5 implies that OOD detection is not learnable in D s XY for H \u03c3 q , when |X | = +\u221e. Second, we prove that if |X | < +\u221e, there exists a sequence q = (l 1 , ..., l g ) (l 1 = d and l g = K + 1) such that OOD detection is learnable in D s XY for H \u03c3 q . Since |X | < +\u221e, it is clear that |H all | < +\u221e, where H all consists of all hypothesis functions from X to Y all . According to Lemma 14, there exists a sequence q such that H all \u2282 H \u03c3 q . Additionally, Lemma 13 implies that there exist H in and H b such that H \u03c3 q \u2282 H in \u2022 H b . Since H all consists all hypothesis space, H all = H \u03c3 q = H in \u2022 H b . Therefore, H b contains all binary classifiers from X to {1, 2}. Theorem 7 implies that OOD detection is learnable in D s XY for H \u03c3 q . Third, we prove that if |X | < +\u221e, then there exists a sequence q = (l 1 , ..., l g ) (l 1 = d and l g = K + 1) such that for any sequence q = (l 1 , ..., l g ) satisfying that q q , OOD detection is learnable in D s XY for H \u03c3 q . We can use the sequence q constructed in the second step of the proof. Therefore, H \u03c3 q = H all . Lemma 10 implies that H \u03c3 q \u2282 H \u03c3 q . Therefore, H \u03c3 q = H all = H \u03c3 q . The proving process (second step of the proof) has shown that if |X | < +\u221e, Condition 2 holds and hypothesis space H consists of all hypothesis functions, then OOD detection is learnable in D s XY for H. Therefore, OOD detection is learnable in D s XY for H \u03c3 q . We complete the proof when the hypothesis space H is FCNN-based. \u2022 The Case that H is score-based Fourth, we prove that if |X | = +\u221e, then OOD detection is not learnable in D s XY for H in \u2022 H b , where H b = H \u03c3,\u03bb q,E for any sequence q = (l 1 , ..., l g ) (l 1 = d, l g = l), where E is in Eqs. ( 5) or (6). Sixth, we prove that if |X | < +\u221e, then there exists a sequence q = (l 1 , ..., l g ) (l 1 = d and l g = l) such that for any sequence q = (l 1 , ..., l g ) satisfying that q q , OOD detection is learnable in D s XY for for H in \u2022 H b , where H b = H \u03c3,\u03bb q ,E , where E is in Eq. (5) or Eq. (6). In the fifth step, we have proven that Eq. (5) and Eq. (6) meet the condition in Lemma 16. Therefore, Lemma 16 implies this result. We complete the proof when the hypothesis space H is score-based. XY for H \u21d2 Condition 1. 2) By Proposition 1 and Proposition 2, we know that when K = 1, there exist h 1 , h 2 \u2208 H, where h 1 = 1 and h 2 = 2, here 1 represents ID, and 2 represent OOD. Therefore, we know that when K = 1, inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0, for any D XY \u2208 D \u00b5,b XY . By Condition 1, we obtain that inf h\u2208H R D (h) = 0, for any D XY \u2208 D \u00b5,b XY . Because each domain\nD XY in D \u00b5,b\nXY is attainable, we conclude that Realizability Assumption holds. We have proven that Condition 1\u21d2 Realizability Assumption.\n3) By Theorems 5 and 8 in [86], we know that VCdim(\u03c6 \u2022 H \u03c3 q ) < +\u221e and VCdim(H \u03c3,\u03bb q,E ) < +\u221e. Then, using Theorem 9, we conclude that Realizability Assumption\u21d2 Learnability in D \u00b5,b XY for H. 4) According to the results in 1), 2) and 3), we have proven that Learnability in D \u00b5,b\nXY for H \u21d4Condition 1\u21d4 Realizability Assumption. 5) By Lemma 2, we conclude that Condition 3\u21d2Condition 1.\n6) Here we prove that Learnability in D \u00b5,b XY for H \u21d2Condition 3. Since D \u00b5,b XY is the priorunknown space, by Theorem 1, there exist an algorithm A : \u222a +\u221e n=1 (X \u00d7Y) n \u2192 H and a monotonically decreasing sequence cons (n), such that cons (n) \u2192 0, as n \u2192 +\u221e, and for any D XY \u2208 D \u00b5,b\nXY , Then, for any > 0, we can find n such that \u2265 cons (n ), therefore, if n = n , we have XY for H \u21d4Condition 3\u21d4Condition 1. 8) Combining 4) and 7), we have completed the proof.\nE S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 cons (n), ES\u223cD\nE", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "where f k w,b (x) is the k-th coordinate of f w,b (x). Then, we define the FCNN-based hypothesis space as follows:\nH \u03c3 q := {h w,b : \u2200w i \u2208 R li\u00d7li\u22121 , \u2200b i \u2208 R li\u00d71 , i = 2, ..., g}.\nScore-based Hypothesis Space. Many OOD algorithms detect OOD data using a score-based strategy. That is, given a threshold \u03bb, a scoring function space F l \u2282 {f : X \u2192 R l } and a scoring function E : F l \u2192 R, then x is regarded as ID, if E(f (x)) \u2265 \u03bb; otherwise, x is regarded as OOD.\nUsing E, \u03bb and f \u2208 F \u03c3 q , we can generate a binary classifier h \u03bb f ,E :\nwhere 1 represents ID data, and 2 represents OOD data. Hence, a binary classification hypothesis space H b , which consists of all h \u03bb f ,E , is generated. We define the score-based hypothesis space H \u03c3,\u03bb q,E := {h \u03bb f ,E : \u2200f \u2208 F \u03c3 q }. Next, we introduce two important propositions. Proposition 1. Given a sequence q = (l 1 , ...l g ) satisfying that l 1 = d and l g = K + 1 (note that d is the dimension of input data and K + 1 is the dimension of output), then the constant functions h 1 , h 2 ,...,h K+1 belong to H \u03c3 q , where h i (x) = i, for any x \u2208 X . Therefore, Assumption 1 holds for H \u03c3 q .\nProof of Proposition 1. Note that the output of FCNN can be written as\nwhere\nis the output of the l g\u22121 -th layer. If we set w g = 0, and set b g = y i , where y i is the one-hot vector corresponding to label i. Then f w,b (x) = y i , for any x \u2208 X . Therefore, h i (x) \u2208 H \u03c3 q , for any i = 1, ..., K, K + 1.\nNote that in some works [84], b g is fixed to 0. In fact, it is easy to check that when g > 2 and activation function \u03c3 is not a constant, Proposition 1 still holds, even if b g = 0.\nProposition 2. For any sequence q = (l 1 , ..., l g ) satisfying that l 1 = d and l g = l (note that d is the dimension of input data and l is the dimension of output), if {v \u2208 R l : E(v) \u2265 \u03bb} = \u2205 and {v \u2208 R l : E(v) < \u03bb} = \u2205, then the functions h 1 and h 2 belong to H \u03c3,\u03bb q,E , where h 1 (x) = 1 and h 2 (x) = 2, for any x \u2208 X , where 1 represents the ID labels, and 2 represents the OOD labels. Therefore, Assumption 1 holds.\nwhere w g \u2208 R l\u00d7lg\u22121 , b g \u2208 R l\u00d71 and f g\u22121 (x) is the output of the l g\u22121 -th layer.\nIf we set w g = 0 l\u00d7lg\u22121 and b g = v 1 , then f w,b (x) = v 1 for any x \u2208 X , where 0 l\u00d7lg\u22121 is l \u00d7 l g\u22121 zero matrix. Hence, h 1 can be induced by f w,b . Therefore, h 1 \u2208 H \u03c3,\u03bb q,E . Similarly, if we set w g = 0 l\u00d7lg\u22121 and b g = v 2 , then f w,b (x) = v 2 for any x \u2208 X , where 0 l\u00d7lg\u22121 is l \u00d7 l g\u22121 zero matrix. Hence, h 2 can be induced by f w,b . Therefore, h 2 \u2208 H \u03c3,\u03bb q,E .\nIt is easy to check that when g > 2 and activation function \u03c3 is not a constant, Proposition 2 still holds, even if b g = 0.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "L Proof of Theorem 10", "text": "Before proving Theorem 10, we need several lemmas. Lemma 9. Let \u03c3 be ReLU function: max{x, 0}. Given q = (l 1 , ..., l g ) and q = (l 1 , ..., l g ) such that l g = l g and l 1 = l 1 , and l i \u2264 l i (i = 1, ..., g \u2212 1), then F \u03c3 q \u2282 F \u03c3 q and H \u03c3 q \u2282 H \u03c3 q .\nProof of Lemma 9. Given any weights w i \u2208 R li\u00d7li\u22121 and bias b i \u2208 R li\u00d71 , the i-layer output of FCNN with architecture q can be written as\nwhere f i\u22121 (x) is the i-th layer output and f 1 (x) = x. Then, the output of last layer is\nWe will show that f w,b \u2208 F \u03c3 q . We construct f w ,b as follows: for every\nwhere 0 pq means the p \u00d7 q zero matrix. If l i \u2212 l i = 0 and l i\u22121 \u2212 l i\u22121 > 0, we set\n.\nIf l i\u22121 \u2212 l i\u22121 = 0 and l i \u2212 l i = 0, we set\nIt is easy to check that if l i \u2212 l i > 0\n.\nLemma 10. Let \u03c3 be the ReLU function:\nwhere q = (l 1 , ..., l g ) and q = (l 1 , ..., l g ).\nProof of Lemma 10. Given l = (l 1 , ..., l g ) satisfying that g \u2264 g , l i = l i for i = 1, ..., g \u2212 1, l i = l g\u22121 for i = g, ..., g \u2212 1, and l g = l g , we first prove that F \u03c3 q \u2282 F \u03c3 q and H \u03c3 q \u2282 H \u03c3 q . Given any weights w i \u2208 R li\u00d7li\u22121 and bias b i \u2208 R li\u00d71 , the i-th layer output of FCNN with architecture q can be written as\nwhere f i\u22121 (x) is the i-th layer output and f 1 (x) = x. Then, the output of the last layer is\nwhere w i represents weights and b i represents bias.\nWe construct a larger FCNN with q = (l 1 , l 2 , ..., l g ) satisfying that l 1 = d, l i = l * l i , for i = 2, ..., g. We can regard this larger FCNN as a combinations of l FCNNs with architecture q, that is: there are m disjoint sub-FCNNs with architecture q in the larger FCNN with architecture q . For i-th sub-FCNN, we use weights w i and bias b i . For weights and bias which connect different sub-FCNNs, we set these weights and bias to 0. Finally, we can obtain that g\nWe have completed this proof.\nGiven a sequence q = (l 1 , ..., l g ), we are interested in following function space F \u03c3 q,M :\n, where \u2022 means the composition of two functions, \u2022 means the product of two matrices, and\nmatrix whose all elements are 1, and 0 1\u00d7(lg\u22121) is the 1 \u00d7 (l g \u2212 1) zero matrix. Using F \u03c3 q,M , we can construct a binary classification space H \u03c3 q,M , which consists of all classifiers satisfying the following condition:\nLemma 13. Suppose that \u03c3 is the ReLU function: max{x, 0}. Given a sequence q = (l 1 , ..., l g ) satisfying that l 1 = d and l g = K + 1, then the space H \u03c3 q,M contains \u03c6 \u2022 H \u03c3 q , and H \u03c3 q,M has finite VC dimension (Vapnik-Chervonenkis dimension), where \u03c6 maps ID data to 1 and OOD data to 2. Furthermore, if given q = (l 1 , ..., l g ) satisfying that l g = K and l i = l i , for i = 1, ..., g \u2212 1, then\nProof of Lemma 13. For any h w,b \u2208 H \u03c3 q , then there exists f w,b \u2208 F \u03c3 q such that h w,b is induced by f w,b . We can write f w,b as follows: \nis the 1 \u00d7 (l g \u2212 1) zero matrix, and 1 (lg\u22121)\u00d71 is the (l g \u2212 1) \u00d7 1 matrix, whose all elements are 1.\nThen, we define that for any x \u2208 X , h w,b,B (x) := arg max\nwhere f k w,b,B (x) is the k-th coordinate of f w,b,B (x). Furthermore, we can check that h w,b,B can be written as follows: for any\nwhere \u03c6 maps ID labels to 1 and OOD labels to 2. \nAccording to the VC dimension theory [37] for feed-forward neural networks, H \u03c3 q has finite VC dimension. Hence, H \u03c3 q,M has finite VC-dimension. We have completed the proof. Lemma 14. Let |X | < +\u221e and \u03c3 be the ReLU function: max{x, 0}. Given r hypothesis functions h 1 , h 2 , ..., h r \u2208 {h : X \u2192 {1, ..., l}}, then there exists a sequence q = (l 1 , ..., l g ) with l 1 = d and l g = l, such that h 1 , ..., h r \u2208 H \u03c3 q .\nProof of Lemma 14. For each h i (i = 1, ..., r), we introduce a corresponding f i (defined over X ) satisfying that for any x \u2208 X , f i (x) = y k if and only if h i (x) = k, where y k \u2208 R l is the one-hot vector corresponding to the label k. Clearly, f i is a continuous function in X , because X is a discrete set. Tietze Extension Theorem implies that f i can be extended to a continuous function in R d .\nSince X is a compact set, then Lemma 12 implies that there exist a sequence q i = (l i 1 , ..., l\nwhere \u2022 2 is the 2 norm in R l . Therefore, for any x \u2208 X , it easy to check that arg max k\u2208{1,...,l}\nwhere\nUsing Lemma 10, we obtain that H \u03c3 q i \u2282 H \u03c3 q , for each i = 1, ..., r. Therefore, h 1 , ..., h r \u2208 H \u03c3 q .\nLemma 15. Let the activation function \u03c3 be the ReLU function. Suppose that |X | < +\u221e. If {v \u2208 R l : E(v) \u2265 \u03bb} and {v \u2208 R l : E(v) < \u03bb} both contain nonempty open sets of R l (here, open set is a topological terminology). There exists a sequence q = (l 1 , ..., l g ) (l 1 = d and l g = l) such that H \u03c3,\u03bb q,E consists of all binary classifiers.\nProof of Lemma 15. Since {v \u2208 R l : E(v) \u2265 \u03bb}, {v \u2208 R l : E(v) < \u03bb} both contain nonempty open sets, we can find v 1 \u2208 {v \u2208 R l : E(v) \u2265 \u03bb}, v 2 \u2208 {v \u2208 R l : E(v) < \u03bb} and a constant r > 0 such that B r (v 1 ) \u2282 {v \u2208 R l : E(v) \u2265 \u03bb} and B r (v 2 ) \u2282 {v \u2208 R l : E(v) < \u03bb}, where B r (v 1 ) = {v : v \u2212 v 1 2 < r} and B r (v 2 ) = {v : v \u2212 v 2 2 < r}, here \u2022 2 is the 2 norm.\nFor any binary classifier h over X , we can induce a vector-valued function as follows: for any x \u2208 X ,\nSince X is a finite set, then Tietze Extension Theorem implies that f can be extended to a continuous function in R d . Since X is a compact set, Lemma 12 implies that there exists a sequence q h = (l h 1 , ..., l h g h ) (l h 1 = d and l h g h = l) and f w,b \u2208 F \u03c3 q h such that\nwhere \u2022 2 is the 2 norm in R l . Therefore, for any x \u2208 X , it is easy to check that E(f w,b (x)) \u2265 \u03bb if and only if h(x) = 1, and E(f w,b (x)) < \u03bb if and only if h(x) = 2.\nFor each h, we have found a sequence q h such that h is induced by f w,b \u2208 F \u03c3 q h , E and \u03bb. Since |X | < +\u221e, only finite binary classifiers are defined over X . Using Lemma 14, we can find a sequence q such that H b all = H \u03c3,\u03bb q,E , where H b all consists of all binary classifiers. There is a sequence q = (l 1 , ..., l g ) such that OOD detection is learnable in the separate space D s XY for H if and only if |X | < +\u221e. Furthermore, if |X | < +\u221e, then there exists a sequence q = (l 1 , ..., l g ) such that for any sequence q satisfying that q q , OOD detection is learnable in D s XY for H.\nProof of Theorem 10. Note that we use the ReLU function as the activation function in this theorem.\n\u2022 The Case that H is FCNN-based.\nFirst, we prove that if |X | = +\u221e, then OOD detection is not learnable in D s XY for H \u03c3 q , for any sequence q = (l 1 , ..., l g ) (l 1 = d and l g = K + 1). . Given a prior-unknown space D XY , if there exists a domain D XY \u2208 D XY , which has an overlap between ID and OOD distributions (see Definition 4), then OOD detection is not learnable in the domain space D XY for H.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "By", "text": "Proof of Theorem 12. Using Proposition 1 and Proposition 2, we obtain that inf h\u2208H R in D (h) = 0 and inf h\u2208H R out D (h) = 0. Then, Theorem 3 implies this result.\nNote that if we replace the activation function \u03c3 (ReLU function) in Theorem 12 with any other activation functions, Theorem 12 still hold.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "", "authors": "Alexey Dosovitskiy; Lucas Beyer; Alexander Kolesnikov; Dirk Weissenborn; Xiaohua Zhai; Thomas Unterthiner; Mostafa Dehghani; Matthias Minderer; Georg Heigold; Sylvain Gelly; Jakob Uszkoreit; Neil Houlsby"}, {"ref_id": "b1", "title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"ref_id": "b2", "title": "Generalized ODIN: detecting out-of-distribution image without learning from out-of-distribution data", "journal": "", "year": "2020", "authors": "Yen-Chang Hsu; Yilin Shen; Hongxia Jin; Zsolt Kira"}, {"ref_id": "b3", "title": "Generalized out-of-distribution detection: A survey", "journal": "", "year": "2021", "authors": "Jingkang Yang; Kaiyang Zhou; Yixuan Li; Ziwei Liu"}, {"ref_id": "b4", "title": "Towards open set deep networks", "journal": "", "year": "2016", "authors": "Abhijit Bendale; Terrance E Boult"}, {"ref_id": "b5", "title": "Atom: Robustifying out-ofdistribution detection using outlier mining", "journal": "", "year": "", "authors": "Jiefeng Chen; Yixuan Li; Xi Wu; Yingyu Liang; Somesh Jha"}, {"ref_id": "b6", "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "journal": "In ICLR", "year": "2017", "authors": "Dan Hendrycks; Kevin Gimpel"}, {"ref_id": "b7", "title": "Enhancing the reliability of out-of-distribution image detection in neural networks", "journal": "In ICLR", "year": "2018", "authors": "Shiyu Liang; Yixuan Li; R Srikant"}, {"ref_id": "b8", "title": "A simple unified framework for detecting out-of-distribution samples and adversarial attacks", "journal": "", "year": "2018", "authors": "Kimin Lee; Kibok Lee; Honglak Lee; Jinwoo Shin"}, {"ref_id": "b9", "title": "Deep autoencoding gaussian mixture model for unsupervised anomaly detection", "journal": "", "year": "2018", "authors": "Bo Zong; Qi Song; Wei Martin Renqiang Min; Cristian Cheng; Dae-Ki Lumezanu; Haifeng Cho;  Chen"}, {"ref_id": "b10", "title": "Generative probabilistic novelty detection with adversarial autoencoders", "journal": "", "year": "2018", "authors": "Stanislav Pidhorskyi; Ranya Almohsen; Gianfranco Doretto"}, {"ref_id": "b11", "title": "Do deep generative models know what they don't know? In ICLR", "journal": "", "year": "2019", "authors": "Eric T Nalisnick; Akihiro Matsukawa; Yee Whye Teh; Dilan G\u00f6r\u00fcr; Balaji Lakshminarayanan"}, {"ref_id": "b12", "title": "Deep anomaly detection with outlier exposure", "journal": "", "year": "2019", "authors": "Dan Hendrycks; Mantas Mazeika; Thomas G Dietterich"}, {"ref_id": "b13", "title": "Likelihood ratios for out-of-distribution detection", "journal": "", "year": "2019", "authors": "Jie Ren; Peter J Liu; Emily Fertig; Jasper Snoek; Ryan Poplin; Mark A Depristo; Joshua V Dillon; Balaji Lakshminarayanan"}, {"ref_id": "b14", "title": "Mood: Multi-level out-of-distribution detection", "journal": "", "year": "", "authors": "Ziqian Lin; Yixuan Sreya Dutta Roy;  Li"}, {"ref_id": "b15", "title": "A unified survey on anomaly, novelty, open-set, and out-of-distribution detection: Solutions and future challenges", "journal": "", "year": "2021", "authors": "Mohammadreza Salehi; Hossein Mirzaei; Dan Hendrycks; Yixuan Li; Mohammad Hossein Rohban; Mohammad Sabokrou"}, {"ref_id": "b16", "title": "React: Out-of-distribution detection with rectified activations", "journal": "", "year": "", "authors": "Yiyou Sun; Chuan Guo; Yixuan Li"}, {"ref_id": "b17", "title": "On the Importance of Gradients for Detecting Distributional Shifts in the Wild", "journal": "", "year": "", "authors": "Rui Huang; Andrew Geng; Yixuan Li"}, {"ref_id": "b18", "title": "Exploring the Limits of Out-of-Distribution Detection", "journal": "", "year": "", "authors": "Stanislav Fort; Jie Ren; Balaji Lakshminarayanan"}, {"ref_id": "b19", "title": "On the impact of spurious correlation for out-ofdistribution detection", "journal": "AAAI", "year": "2022", "authors": "Yifei Ming; Hang Yin; Yixuan Li"}, {"ref_id": "b20", "title": "Understanding machine learning: From theory to algorithms", "journal": "Cambridge university press", "year": "2014", "authors": "Shai Shalev; - Shwartz; Shai Ben-David"}, {"ref_id": "b21", "title": "Foundations of machine learning", "journal": "MIT press", "year": "2018", "authors": "Mehryar Mohri; Afshin Rostamizadeh; Ameet Talwalkar"}, {"ref_id": "b22", "title": "Energy-based out-of-distribution detection", "journal": "", "year": "2020", "authors": "Weitang Liu; Xiaoyun Wang; John D Owens; Yixuan Li"}, {"ref_id": "b23", "title": "Learning bounds for open-set learning", "journal": "", "year": "", "authors": "Zhen Fang; Jie Lu; Anjin Liu; Feng Liu; Guangquan Zhang"}, {"ref_id": "b24", "title": "Adversarial reciprocal points learning for open set recognition", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2021", "authors": "Guangyao Chen; Peixi Peng; Xiangqian Wang; Yonghong Tian"}, {"ref_id": "b25", "title": "Deep one-class classification", "journal": "", "year": "2018", "authors": "Lukas Ruff; Nico G\u00f6rnitz; Lucas Deecke; Ahmed Shoaib; Robert A Siddiqui; Alexander Vandermeulen; Emmanuel Binder; Marius M\u00fcller;  Kloft"}, {"ref_id": "b26", "title": "DROCC: deep robust one-class classification", "journal": "", "year": "2020", "authors": "Sachin Goyal; Aditi Raghunathan; Moksh Jain; Prateek Harsha Vardhan Simhadri;  Jain"}, {"ref_id": "b27", "title": "Image anomaly detection with generative adversarial networks", "journal": "", "year": "2018", "authors": "Lucas Deecke; Robert A Vandermeulen; Lukas Ruff; Stephan Mandt; Marius Kloft"}, {"ref_id": "b28", "title": "Open set domain adaptation: Theoretical bound and algorithm", "journal": "", "year": "2020", "authors": "Z Fang; F Lu; Junyu Liu; G Xuan;  Zhang"}, {"ref_id": "b29", "title": "Learnability, stability and uniform convergence", "journal": "J. Mach. Learn. Res", "year": "2010", "authors": "Shai Shalev-Shwartz; Ohad Shamir; Nathan Srebro; Karthik Sridharan"}, {"ref_id": "b30", "title": "Shiliang Pu, and Yonghong Tian. Learning open set network with discriminative reciprocal points. ICCV", "journal": "", "year": "2020", "authors": "Guangyao Chen; Limeng Qiao; Yemin Shi; Peixi Peng; Jia Li; Tiejun Huang"}, {"ref_id": "b31", "title": "Informative outlier matters: Robustifying out-of-distribution detection using outlier mining", "journal": "", "year": "", "authors": "Jiefeng Chen; Yixuan Li; Xi Wu; Yingyu Liang; Somesh Jha"}, {"ref_id": "b32", "title": "Robust out-of-distribution detection for neural networks", "journal": "", "year": "2020", "authors": "Jiefeng Chen; Yixuan Li; Xi Wu; Yingyu Liang; Somesh Jha"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Illustration of inf h\u2208H R \u03b1 D (h) (solid lines with triangle marks) and the estimated E S\u223cD n in R \u03b1 D (A(S)) (dash lines) with \u03b1 \u2208 [0, 1) in different scenarios, where Din = DX I Y I and the algorithm A is the free-energy OOD detection method[23]. Subfigure (a) shows the ID and OOD distributions. In (a), gap IO represents the distance between the support sets of ID and OOD distributions. In (b), since there is an overlap between ID and OOD data, the solid line is a ployline. In (c), since there is no overlap between ID and OOD data, we can check that inf h\u2208H R \u03b1 D (h) forms a straight line (the solid line). However, since dash lines are always straight lines, two observations can be obtained from (b) and (c): 1) dash lines cannot approximate the solid ployline in (b), which implies the unlearnability of OOD detection; and 2) the solid line in (c) is a straight line and may be approximated by the dash lines in (c). The above observations motivate us to propose Condition 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "FCNN-based Hypothesis Space. Let l g = K + 1. The FCNN-based scoring function space F \u03c3 q can induce an FCNN-based hypothesis space. For any f w,b \u2208 F \u03c3 q , the induced hypothesis function is: h w,b := arg max k\u2208{1,...,K+1} f k w,b , where f k w,b is the k-th coordinate of f w,b .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "study the far-OOD detection issue. Existing benchmarks include 1) MNIST [40] as ID dataset, and Texture [41], CIFAR-10 [42] or Place365 [43] as OOD datasets; and 2) CIFAR-10 [42] as ID dataset, and MNIST [40], or Fashion-MNIST [43] as OOD datasets. In far-OOD case, we find that the ID and OOD datasets have different semantic labels and different styles. From the theoretical view, we can define far-OOD detection tasks as follows: for", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Checklist 1 .1For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] (b) Did you describe the limitations of your work? [Yes] See Appendix B (c) Did you discuss any potential negative societal impacts of your work? [Yes] See Appendix B (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] (b) Did you include complete proofs of all theoretical results? [Yes] 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [N/A] (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [N/A] (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [N/A] (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [N/A] 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [N/A] (b) Did you mention the license of the assets? [N/A] (c) Did you include any new assets either in the supplemental material or as a URL? [N/A] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "H Proof of Theorem 5 IL5Proofs of Theorem 6 and Theorem 7 I.1 Proof of Theorem 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . I.2 Proof of Theorem 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J Proofs of Theorems 8 and 9 J.1 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . J.2 Proof of Theorem 9 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . K Proof of Proposition 1 and Proof of Proposition 2 Proof of Theorem 10 M Proofs of Theorem 11 and Theorem 12 M.1 Proof of Theorem 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M.2 Proof of Theorem 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Theorem 11 has shown that when K = 1 and the hypothesis space is FCNN-based or score-based, Realizability Assumption, Condition 3, Condition 1 and the learnability of OOD detection in the density-based space D \u00b5,b", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 2 :2Figure 2: ID and OOD distributions in Figure 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "= 1/T , for some integer T . Let r = T d and C 1 , C 2 , ..., C r be a cover of X : for every (a 1 , ..., a T ) \u2208 [T ] d := [1, ..., T ] d , there exists a C i = {x = (x 1 , ..., x d ) : \u2200j \u2208 {1, ..., d}, x j \u2208 [(a j \u2212 1)/T, a j /T )}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "construct a special domain D XY \u2208 D s XY : if C O = \u2205, then D XY = D XI * D YI|XI ; otherwise, D XY = 0.5 * D XI * D YI|XI + 0.5 * D XO * D YO|XO , where D XI = 1 |C I | x\u2208CI \u03b4 x and D YI|XI (y|x) = 1, if h (x) = y and x \u2208 C I , and D XO = 1 |C O | x\u2208CO \u03b4 x and D YO|XO (K + 1|x) = 1, if x \u2208 C O . Since h / \u2208 H and |X | < +\u221e, then arg min h\u2208H R D (h) = \u2205, and", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "XI \u00d7 D n XIYI ({(x, S) : dist(x, \u03c0 1 (x, S)) \u2265 0.5 * d 0 }) \u2264 2 cons (n)/d 0 , where D XI \u00d7 D n XIYI is the product measure of D XI and D n XIYI [36]. Therefore, D XI \u00d7 D n XIYI ({(x, S) : A(S)(x) = 1}) > 1 \u2212 2 cons (n)/d 0 , which implies that E S\u223cD n X I Y I R in D (A(S)) \u2264 2B cons (n)/d 0 ,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "A(S)(x) = K + 1, if A b (\u03c6(S))(x) = 2; A in (S)(x), if A b (\u03c6(S))(x) = 1.", "figure_data": ""}, {"figure_label": "9188", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "J Proofs of Theorems 8 and 9 J. 1 Proof of Theorem 8 Lemma 8 .9188Given a prior-unknown space D XY and a hypothesis space H, if Condition 3 holds, then for any equivalence class [D XY ] with respect to D XY , OOD detection is learnable in the equivalence class [D XY ] for H. Furthermore, the learning rate can attain O(1/n).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "whereA S (S) = A D i (S), if i = arg min i\u2208{1,...m} MMD K (P Si , P S ),here P S = 1 n (x,y)\u2208S \u03b4 (x,y) , P Si = 1 n (x,y)\u2208Si, \u03b4 (x,y)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "f I is D XI 's density function and f O is D XO 's density function. Let f be the density function for 0.5 * D XI + 0.5 * D XO . It is easy to check that f = 0.5 * f I + 0.5 * f O . Additionally, due to", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "S:= {S := (S 1 , S 2 , ..., S m , ...) : S m are any m data points, m = 1, ..., +\u221e}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "h w,b (x) := arg max k\u2208{1,...,K+1} f k w,b (x), \u2200x \u2208 X ,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "[85]  If the activation function \u03c3 is not a polynomial, then for any continuous function f defined in R d , and any compact set C \u2282 R d , there exists a fully-connected neural network with architecture q (l1 = d, l g = 1) such that inf f w,b \u2208F \u03c3 q max x\u2208C |f w,b (x) \u2212 f (x)| < .Proof of Lemma 11. The proof of Lemma 11 can be found in Theorem 3.1 in [85]. Lemma 12. If the activation function \u03c3 is the ReLU function, then for any continuous vector-valued function f \u2208 C(R d ; R l ), and any compact set C \u2282 R d , there exists a fully-connected neural network with architecture q(l 1 = d, l g = l) such that inf f w,b \u2208F \u03c3 q max x\u2208C f w,b (x) \u2212 f (x) 2 < ,where \u2022 2 is the 2 norm. (Note that we can also prove the same result, if \u03c3 is not a polynomial.) Proof of Lemma 12. Let f = [f 1 , ..., f l ] , where f i is the i-th coordinate of f . Based on Lemma 11, we obtain l sequences q 1 , q 2 ,...,q l such that inf x) \u2212 f l (x)| < / \u221a l.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": ") \u2212 f l (x)| < / \u221a l.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "By Theorems 55and 8 in [86], we know that VCdim(H \u03c3,\u03bb q,E ) < +\u221e. Additionally, Proposition 2 implies that Assumption 1 holds and sup h\u2208H \u03c3 q |{x \u2208 X : h(x) \u2208 Y}| = +\u221e, when |X | = +\u221e. Hence, Theorem 5 implies that OOD detection is not learnable in D s XY for H \u03c3 q , when |X | = +\u221e. Fifth, we prove that if |X | < +\u221e, there exists a sequence q = (l 1 , ..., l g ) (l 1 = d and l g = l) such that OOD detection is learnable in D s XY for for H in \u2022 H b , where H b = H \u03c3,\u03bb q,E for any sequence q = (l 1 , ..., l g ) (l 1 = d, l g = l), where E is in Eq. (5) or Eq. (6). Based on Lemma 16, we only need to show that {v \u2208 R l : E(v) \u2265 \u03bb} and {v \u2208 R l : E(v) < \u03bb} both contain nonempty open sets for different scoring functions E. Since max k\u2208{1,...,l} exp (v k ) l c=1 exp (v c ) , max k\u2208{1,...,l} exp (v k /T ) K c=1 exp (v c /T ) and T log l c=1 exp (v c /T ) are continuous functions, whose ranges contain ( 1 l , 1), ( 1 l , 1), (0, +\u221e) and (0, +\u221e), respectively. Based on the property of continuous function (E \u22121 (A) is an open set, if A is an open set), we obtain that {v \u2208 R l : E(v) \u2265 \u03bb} and {v \u2208 R l : E(v) < \u03bb} both contain nonempty open sets. Using Lemma 16, we complete the fifth step.", "figure_data": ""}, {"figure_label": "3111", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "M 3 Proof of Theorem 11 . 1 )3111Proofs of Theorem 11 and Theorem 12 M.1 Proof of Theorem 11 Theorem 11. Suppose that each domain D XY in D \u00b5,b XY is attainable, i.e., arg min h\u2208H R D (h) = \u2205 (the finite discrete domains satisfy this). Let K = 1 and the hypothesis space H be score-based (H = H \u03c3,\u03bb q,E , where E is in Eqs. (5) or (6)) or FCNN-based (H = H \u03c3 q ). If \u00b5(X ) < +\u221e, then the following four conditions are equivalent: Learnability in D \u00b5,b XY for H \u21d0\u21d2 Condition 1 \u21d0\u21d2 Realizability Assumption \u21d0\u21d2 Condition By Lemma 1, we conclude that Learnability in D \u00b5,b", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "n X I Y I R out D (A(S)) \u2212 inf h\u2208H R out D (h) \u2264 cons (n).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 , E S\u223cD n X I Y I R out D (A(S)) \u2212 inf h\u2208H R out D (h) \u2264 , which implies that there exists S \u223c D n XIYI such that R in D (A(S )) \u2212 inf h\u2208H R in D (h) \u2264 , R out D (A(S )) \u2212 inf h\u2208H R out D (h) \u2264 .Therefore, for any equivalence class [D XY ] with respect to D \u00b5,b XY and any > 0, there exists a hypothesis function A(S ) \u2208 H such that for any domainD XY \u2208 [D XY ], A(S ) \u2208 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + } \u2229 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + },which implies that Condition 3 holds. Therefore, Learnability in D \u00b5,b XY for H \u21d2Condition 3. 7) Note that in 4), 5) and 6), we have proven thatLearnability in D \u00b5,bXY for H \u21d2Condition 3\u21d2Condition 1, and Learnability in D \u00b5,b XY for H \u21d4Condition 1, thus, we conclude that Learnability in D \u00b5,b", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Let |X | < +\u221e and H = H in \u2022 H b . If H all \u2212 {h out } \u2282 H b and Condition 2 holds, then OOD detection is learnable in D s XY for H, where H all and h out are defined in Theorem 6.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "[47] Yaroslav Bulatov. Notmnist dataset. Google (Books/OCR), Tech. Rep.[Online]. Available: http://yaroslavvb. blogspot. it/2011/09/notmnist-dataset. html,2, 2011. [48] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 and cifar-100 datasets. 2009. [49] Lily H. Zhang, Mark Goldstein, and Rajesh Ranganath. Understanding failures in out-ofdistribution detection with deep generative models. In ICML, 2021. [50] Peyman Morteza and Yixuan Li. Provable guarantees for understanding out-of-distribution detection. AAAI, 2022. [51] Si Liu, Risheek Garrepalli, Thomas G. Dietterich, Alan Fern, and Dan Hendrycks. Open category detection with PAC guarantees. In ICML, 2018. [52] Yadan Luo, Zijian Wang, Zi Huang, and Mahsa Baktashmotlagh. Progressive graph learning for open-set domain adaptation. In ICML, 2020. [53] C. K. Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information Theory, 1970. [54] Vojtech Franc, Daniel Pr\u016f\u0161a, and V. Voracek. Optimal strategies for reject option classifiers. CoRR, abs/2101.12523, 2021. [55] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Learning with rejection. In ALT, 2016. [56] Corinna Cortes, Giulia DeSalvo, and Mehryar Mohri. Boosting with abstention. In NeurIPS, 2016. [57] Chenri Ni, Nontawat Charoenphakdee, Junya Honda, and Masashi Sugiyama. On the calibration of multiclass classification with rejection. In NeurIPS, 2019. [58] Nontawat Charoenphakdee, Zhenghang Cui, Yivan Zhang, and Masashi Sugiyama. Classification with rejection based on cost-sensitive classification. In ICML, 2021. [59] Peter L. Bartlett and Marten H. Wegkamp. Classification with a reject option using a hinge loss. Journal of Machine Learning Research, 2008. [60] Peter J Rousseeuw, Frank R Hampel, Elvezio M Ronchetti, and Werner A Stahel. Robust statistics: the approach based on influence functions. John Wiley & Sons, 2011. [61] Elvezio M Ronchetti and Peter J Huber. Robust statistics. John Wiley & Sons, 2009. [62] Ilias Diakonikolas, Daniel M. Kane, and Ankit Pensia. Outlier robust mean estimation with subgaussian rates via stability. In NeurIPS, 2020. [63] Ilias Diakonikolas, Daniel Kane, Sushrut Karmalkar, Eric Price, and Alistair Stewart. Outlierrobust high-dimensional sparse estimation via iterative filtering. In NeurIPS, 2019. [64] Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart, and Yuxin Sun. Outlier-robust learning of ising models under dobrushin's condition. In COLT, 2021. [65] Yu Cheng, Ilias Diakonikolas, Daniel M Kane, Rong Ge, Shivam Gupta, and Mahdi Soltanolkotabi. Outlier-robust sparse estimation via non-convex optimization. In NeurIPS, 2021. [66] Ilias Diakonikolas, Daniel M Kane, Jasper CH Lee, and Ankit Pensia. Outlier-robust sparse mean estimation for heavy-tailed distributions. In NeurIPS, 2022. [67] Shafi Goldwasser, Adam Tauman Kalai, Yael Kalai, and Omar Montasser. Beyond perturbations: Learning guarantees with arbitrary adversarial test examples. In NeurIPS, 2020. [68] Adam Tauman Kalai and Varun Kanade. Efficient learning with arbitrary covariate shift. In ALT, Proceedings of Machine Learning Research, 2021. [69] Ilias Diakonikolas and Daniel M. Kane. Recent advances in algorithmic high-dimensional robust statistics. A shorter version appears as an Invited Book Chapter in Beyond the Worst-Case Analysis of Algorithms, 2020. [70] Akshay Raj Dhamija, Manuel G\u00fcnther, and Terrance E. Boult. Reducing network agnostophobia. In NeurIPS, pages 9175-9186, 2018. [71] Haoran Wang, Weitang Liu, Alex Bocchieri, and Yixuan Li. Can multi-label classification networks know what they don't know? In NeurIPS, 2021.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Journal of Machine Learning Research, 2012.[84] Itay Safran and Ohad Shamir. Depth-width tradeoffs in approximating natural functions with neural networks. In ICML, 2017.[85] Allan Pinkus. Approximation theory of the mlp model in neural networks. Acta numerica, 8: 143-195, 1999.[86] Peter L Bartlett and Wolfgang Maass. Vapnik-chervonenkis dimension of neural nets. The handbook of brain theory and neural networks, 2003.", "figure_data": ""}, {"figure_label": "of", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Contents of Appendix A Detailed Related Work B Limitations and Potential Negative Societal Impacts C Discussions and Details about Experiments in Figure 1 C.1 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Details of Experiments in Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . Main Notations and Their Descriptions . . . . . . . . . . . . . . . . . . . . . . . . D.2 Realizability Assumption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.3 Learnability and PAC learnability . . . . . . . . . . . . . . . . . . . . . . . . . . D.4 Explanations for Some Notations in Section 2 . . . . . . . . . . . . . . . . . . . . Proof of Theorem 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . G.2 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .", "figure_data": "D NotationsD.1 E Proof of Theorem 1F Proof of Theorem 2G Proofs of Theorem 3 and Theorem 4G.1"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Many works[53, 54]  also investigate the classification with reject option (CwRO) problem, which is similar to OOD detection in some cases.[55, 56, 57, 58, 59] study the learning theory and propose the agnostic PAC learning bounds for CwRO. However, compared to our work regarding OOD detection, existing CwRO theories mainly focus on how the ID risk (i.e., the risk that ID data is wrongly classified) is influenced by special rejection rules. Our theory not only focuses on the ID risk, but also pays attention to the OOD risk.Robust Statistics. In the field of robust statistics[60], researchers aim to propose estimators and testers that can mitigate the negative effects of outliers (similar to OOD data). The proposed estimators are supposed to be independent of the potentially high dimensionality of the data [61, 62, 63]. Existing works[64, 65, 66]  in the field have identified and resolved the statistical limits of outlier robust Under some conditions, PQ learning theory [67, 68] can be regarded as the PAC theory for OOD detection in the semi-supervised or transductive learning cases, i.e., test data are required during the training process. Additionally, PQ learning theory in[67, 68]  aims to give the PAC estimation under Realizability Assumption[21]. Our theory focuses on the PAC theory in different cases, which is more difficult and more practical than PAC theory under Realizability Assumption.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Main notations and their descriptions.", "figure_data": "NotationDescription\u2022 Spaces and Labelsd and X \u2282 R dthe feature dimension of data point and feature spaceYID label space {1, ..., K}K + 1K + 1 represents the OOD labelsY allY \u222a {K + 1}\u2022 DistributionsXI, XO, YI, YOID feature, OOD feature, ID label, OOD label random variablesDX I Y I , DX O Y O D \u03b1 XYID joint distribution and OOD joint distribution"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Let |X | < +\u221e and H = H in \u2022 H b . If H all \u2212 {h out } \u2282 H b and Condition 2 holds, then OOD detection is learnable in D s XY for H, where H all and h out are defined in Theorem 6.", "figure_data": "2 Proof of Theorem 7Theorem 7."}], "formulas": [{"formula_id": "formula_1", "formula_text": "D XY \u2208 D XY , E S\u223cD n X I Y I R D (A(S)) \u2212 inf h\u2208H R D (h) \u2264 cons (n),(2)", "formula_coordinates": [3.0, 204.24, 469.28, 299.76, 30.57]}, {"formula_id": "formula_2", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 cons (n), \u2200\u03b1 \u2208 [0, 1].(3)", "formula_coordinates": [3.0, 178.75, 641.06, 325.25, 16.73]}, {"formula_id": "formula_3", "formula_text": "D XY \u2208 D XY , E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 cons (n), \u2200\u03b1 \u2208 [0, 1].", "formula_coordinates": [4.0, 178.75, 108.87, 254.49, 33.25]}, {"formula_id": "formula_4", "formula_text": "\u03b1 XY := (1 \u2212 \u03b1)D XIYI + \u03b1D XOYO \u2208 D XY . Theorem 1. Given domain spaces D XY and D XY = {D \u03b1 XY : \u2200D XY \u2208 D XY , \u2200\u03b1 \u2208 [0, 1)}, then 1) D XY is a priori-unknown space and D XY \u2282 D XY ; 2) if D XY", "formula_coordinates": [4.0, 107.25, 479.05, 392.04, 51.52]}, {"formula_id": "formula_5", "formula_text": "\u2022 Single-distribution space D D XY XY . For a domain D XY , D D XY XY := {D \u03b1 XY : \u2200\u03b1 \u2208 [0, 1)}. \u2022 Total space D all", "formula_coordinates": [4.0, 108.0, 608.24, 356.69, 29.74]}, {"formula_id": "formula_6", "formula_text": "\u2200D XY \u2208 D F XY }| < +\u221e. \u2022 Density-based space D \u00b5,b", "formula_coordinates": [4.0, 108.0, 684.51, 376.9, 27.08]}, {"formula_id": "formula_8", "formula_text": "inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h).", "formula_coordinates": [5.0, 198.75, 479.04, 214.51, 16.73]}, {"formula_id": "formula_9", "formula_text": "D XI = f I d\u03bc, D XO = f O d\u03bc.", "formula_coordinates": [5.0, 232.78, 698.84, 146.44, 9.65]}, {"formula_id": "formula_10", "formula_text": "h(x) = i, if h in (x) = i and h b (x) = 1; otherwise, h(x) = K + 1.(4)", "formula_coordinates": [7.0, 165.17, 184.34, 338.84, 11.03]}, {"formula_id": "formula_11", "formula_text": "D XY \u2208 [D XY ], h \u2208 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + } \u2229 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + }.", "formula_coordinates": [7.0, 128.51, 397.05, 354.97, 33.6]}, {"formula_id": "formula_12", "formula_text": "1) g \u2264 g , l 1 = l 1 , l g = l g ; 2) l i \u2264 l i , \u2200i = 1, ..., g \u2212 1; and 3) l g\u22121 \u2264 l i , \u2200i = g, ..., g \u2212 1.", "formula_coordinates": [8.0, 109.61, 222.57, 392.78, 10.62]}, {"formula_id": "formula_13", "formula_text": "E(f ) = max", "formula_coordinates": [8.0, 152.73, 404.62, 60.97, 8.74]}, {"formula_id": "formula_15", "formula_text": "E(f ) = T log l c=1 exp (f c /T ).(6)", "formula_coordinates": [8.0, 240.97, 449.62, 263.03, 30.2]}, {"formula_id": "formula_16", "formula_text": "h \u03bb f ,E (x) = 1, if E(f (x)) \u2265 \u03bb; otherwise, h \u03bb f ,E (x) = 2", "formula_coordinates": [8.0, 287.54, 492.73, 214.0, 12.55]}, {"formula_id": "formula_17", "formula_text": "H is FCNN-based or score- based, i.e., H = H \u03c3 q or H = H in \u2022 H b , where H in is an ID hypothesis space, H b = H \u03c3,\u03bb q,E and H = H in \u2022 H b is introduced below Eq. (4), here E is introduced in Eqs. (5) or (6). Then", "formula_coordinates": [8.0, 108.0, 565.49, 397.65, 35.38]}, {"formula_id": "formula_18", "formula_text": "Learnability in D \u00b5,b XY for H \u21d0\u21d2 Condition 1 \u21d0\u21d2 Realizability Assumption \u21d0\u21d2 Condition 3", "formula_coordinates": [9.0, 113.6, 173.81, 388.61, 13.83]}, {"formula_id": "formula_19", "formula_text": "\u03c4 > 0, a domain space D XY is \u03c4 -far-OOD, if for any domain D XY \u2208 D XY , dist(suppD XO , suppD XI ) > \u03c4.", "formula_coordinates": [9.0, 190.81, 469.87, 305.76, 27.74]}, {"formula_id": "formula_20", "formula_text": "D XI|YI=c = U(I c ), where I c = [d c , d c + 4] \u00d7 [1, 5],(7)", "formula_coordinates": [19.0, 200.05, 478.45, 303.95, 9.99]}, {"formula_id": "formula_21", "formula_text": "here d i = 5 + gap II * (i \u2212 1) + 4(i \u2212 2)", "formula_coordinates": [19.0, 108.0, 494.6, 163.79, 10.59]}, {"formula_id": "formula_22", "formula_text": "D YI (y = c) = 1 \u2212 \u03b1 10 .", "formula_coordinates": [19.0, 261.51, 519.53, 88.99, 22.31]}, {"formula_id": "formula_23", "formula_text": "D XO = U(I out ), where I out = [d 1 \u2212 1, d 10 + 5] \u00d7 [5 + gap IO , 10 + gap IO ].(8)", "formula_coordinates": [19.0, 150.01, 567.19, 353.99, 10.62]}, {"formula_id": "formula_24", "formula_text": "D \u03b1 XY = (1 \u2212 \u03b1)DX I Y I + \u03b1DX O Y O , \u2200\u03b1 \u2208 [0, 1] \u03c0 out class-prior probability for OOD distribution DXY DXY = (1 \u2212 \u03c0 out )DX I Y I + \u03c0 out DX O Y O ,", "formula_coordinates": [21.0, 113.98, 250.03, 335.85, 32.13]}, {"formula_id": "formula_25", "formula_text": "f k (x) := max{k \u2208 {1, ..., l} : f k (x) \u2265 f i (x), \u2200i = 1, ..., l},", "formula_coordinates": [21.0, 201.88, 649.09, 246.2, 10.81]}, {"formula_id": "formula_26", "formula_text": "E S\u223cD n X I Y I R D (A(S)) \u2264 inf h\u2208H R D (h) + cons (n), which implies that E S\u223cD n X I Y I [R D (A(S)) \u2212 inf h\u2208H R D (h)] \u2264 cons (n).", "formula_coordinates": [22.0, 107.64, 245.74, 298.74, 49.16]}, {"formula_id": "formula_27", "formula_text": "P(R D (A(S)) \u2212 inf h\u2208H R D (h) < ) > 1 \u2212 E S\u223cD n X I Y I [R D (A(S)) \u2212 inf h\u2208H R D (h)]/ \u2265 1 \u2212 cons (n)/ .", "formula_coordinates": [22.0, 108.0, 319.4, 396.0, 14.66]}, {"formula_id": "formula_28", "formula_text": "R D (A(S)) \u2212 inf h\u2208H R D (h) < ,", "formula_coordinates": [22.0, 243.65, 387.3, 124.71, 14.66]}, {"formula_id": "formula_29", "formula_text": "R D (A(S)) \u2212 inf h\u2208H R D (h) \u2264 .", "formula_coordinates": [22.0, 243.65, 491.29, 124.7, 14.66]}, {"formula_id": "formula_30", "formula_text": "E S [R D (A(S)) \u2212 inf h\u2208H R D (h)] \u2264 (1 \u2212 \u03b4) + 2M \u03b4 < + 2M \u03b4.", "formula_coordinates": [22.0, 178.72, 560.58, 254.56, 14.66]}, {"formula_id": "formula_31", "formula_text": "E S [R D (A(S)) \u2212 inf h\u2208H R D (h)] < (2M + 1) , this implies that lim n\u2192+\u221e E S [R D (A(S)) \u2212 inf h\u2208H R D (h)] = 0,", "formula_coordinates": [22.0, 108.0, 602.57, 289.64, 49.16]}, {"formula_id": "formula_32", "formula_text": "P (A) = A f (x)dx, Q(A) = A g(x)dx, then (P + Q)(A) = A f (x) + g(x)dx.", "formula_coordinates": [23.0, 108.0, 335.06, 286.26, 55.75]}, {"formula_id": "formula_33", "formula_text": "E (x,y)\u223cD XY (h(x), y) =", "formula_coordinates": [23.0, 189.6, 458.33, 101.06, 10.57]}, {"formula_id": "formula_34", "formula_text": "E (x,y)\u223cD XY (h(x), y) = X \u00d7Y all (h(x), y)dD XY (x, y) = 1 m m i=1 a i (h(x i ), y i ).", "formula_coordinates": [23.0, 190.98, 524.95, 230.04, 50.48]}, {"formula_id": "formula_35", "formula_text": "D Y |X (Y = k|X = x) (k-th class- conditional distribution for x) is a k (x), then E (x,y)\u223cD XY (h(x), y) = X \u00d7Y all (h(x), y)dD XY (x, y) = X K+1 k=1 (h(x), k)f (x)a k (x)dx,", "formula_coordinates": [23.0, 108.0, 583.69, 397.65, 89.52]}, {"formula_id": "formula_36", "formula_text": "D XY = (1 \u2212 \u03c0 out )D XIYI + \u03c0 out D XOYO (here \u03c0 out \u2208 [0, 1)) such that D \u03b1 XY = (1 \u2212 \u03b1 )D XIYI + \u03b1 D XOYO .", "formula_coordinates": [24.0, 204.2, 225.87, 283.89, 32.11]}, {"formula_id": "formula_37", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 (n), \u2200\u03b1 \u2208 [0, 1], \u2200D XY \u2208 D XY .", "formula_coordinates": [24.0, 151.7, 409.85, 312.44, 16.73]}, {"formula_id": "formula_38", "formula_text": "D \u03b1 XY = (1 \u2212 \u03b1)D XIYI + \u03b1D XOYO \u2208 D XY .", "formula_coordinates": [24.0, 216.44, 463.07, 179.12, 12.69]}, {"formula_id": "formula_39", "formula_text": "E S\u223cD n X I Y I R D \u03b1 (A(S)) \u2264 inf h\u2208H R D \u03b1 (h) + cons (n), (by the property of priori-unknown space) where R D \u03b1 (A(S)) = X \u00d7Y all (A(S)(x), y)dD \u03b1 XY (x, y), R D \u03b1 (h) = X \u00d7Y all (h(x), y)dD \u03b1 XY (x, y). Since R D \u03b1 (A(S)) = R \u03b1 D (A(S)) and R D \u03b1 (h) = R \u03b1 D (h), we have that E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + cons (n), \u2200\u03b1 \u2208 [0, 1).(9)", "formula_coordinates": [24.0, 107.64, 510.95, 396.36, 100.85]}, {"formula_id": "formula_40", "formula_text": "\u03b1\u21921 inf h\u2208H R \u03b1 D (h) \u2265 lim inf \u03b1\u21921 \u03b1 inf h\u2208H R out D (h) = inf h\u2208H R out D (h).(10)", "formula_coordinates": [24.0, 189.08, 634.88, 314.92, 16.73]}, {"formula_id": "formula_41", "formula_text": "R out D (h ) \u2212 inf h\u2208H R out D (h) \u2264 . It is obvious that R \u03b1 D (h ) \u2265 inf h\u2208H R \u03b1 D (h).", "formula_coordinates": [24.0, 108.0, 674.81, 258.42, 50.0]}, {"formula_id": "formula_42", "formula_text": "R out D (h ) = lim \u03b1\u21921 R \u03b1 D (h ) = lim sup \u03b1\u21921 R \u03b1 D (h ) \u2265 lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h), which implies that inf h\u2208H R out D (h) = lim \u21920 R out D (h ) \u2265 lim \u21920 lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h) = lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h).(11)", "formula_coordinates": [25.0, 107.64, 91.17, 396.36, 60.46]}, {"formula_id": "formula_43", "formula_text": "inf h\u2208H R out D (h) = lim sup \u03b1\u21921 inf h\u2208H R \u03b1 D (h) = lim inf \u03b1\u21921 inf h\u2208H R \u03b1 D (h),(12)", "formula_coordinates": [25.0, 188.83, 175.44, 315.17, 18.32]}, {"formula_id": "formula_44", "formula_text": "h\u2208H R out D (h) = lim \u03b1\u21921 inf h\u2208H R \u03b1 D (h).(13)", "formula_coordinates": [25.0, 240.21, 210.7, 263.79, 16.73]}, {"formula_id": "formula_45", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) = (1 \u2212 \u03b1)E S\u223cD n X I Y I R in D (A(S)) + \u03b1E S\u223cD n X I Y I R out D (A(S)). Hence, Lebesgue's Dominated Convergence Theorem [36] implies that lim \u03b1\u21921 E S\u223cD n X I Y I R \u03b1 D (A(S)) = E S\u223cD n X I Y I R out D (A(S)).(14)", "formula_coordinates": [25.0, 108.0, 248.89, 396.0, 54.95]}, {"formula_id": "formula_46", "formula_text": "lim \u03b1\u21921 E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 lim \u03b1\u21921 inf h\u2208H R \u03b1 D (h) + cons (n).(15)", "formula_coordinates": [25.0, 190.47, 333.14, 313.53, 16.73]}, {"formula_id": "formula_47", "formula_text": "E S\u223cD n X I Y I R out D (A(S)) \u2264 inf h\u2208H R out D (h) + cons (n).", "formula_coordinates": [25.0, 203.56, 374.21, 204.89, 16.73]}, {"formula_id": "formula_48", "formula_text": "E S\u223cD n X I Y I R 1 D (A(S)) \u2264 inf h\u2208H R 1 D (h) + cons (n). (16", "formula_coordinates": [25.0, 208.39, 418.04, 291.46, 16.73]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [25.0, 499.85, 420.43, 4.15, 8.64]}, {"formula_id": "formula_50", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + (n), \u2200\u03b1 \u2208 [0, 1], \u2200D XY \u2208 D XY .", "formula_coordinates": [25.0, 153.93, 508.97, 304.14, 16.73]}, {"formula_id": "formula_51", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 (n), \u2200\u03b1 \u2208 [0, 1], \u2200D XY \u2208 D XY . \u21d3 OOD detection is learnable in D XY for H. If we set \u03b1 = \u03c0 out , then E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + (n) implies that E S\u223cD n X I Y I R D (A(S)) \u2264 inf h\u2208H R D (h) + (n),", "formula_coordinates": [25.0, 108.0, 581.67, 360.1, 91.15]}, {"formula_id": "formula_52", "formula_text": "D XY = (1 \u2212 l j=1 \u03bb j )D XIYI + l j=1 \u03bb j Q j , for some (\u03bb 1 , ..., \u03bb l ) \u2208 \u2206 o l .", "formula_coordinates": [26.0, 108.0, 202.94, 280.72, 49.43]}, {"formula_id": "formula_53", "formula_text": "l ) \u2208 \u2206 o l , (1 \u2212 l j=1 \u03b1 j )D XIYI + l j=1 \u03b1 j Q j \u2208 D XY .", "formula_coordinates": [26.0, 223.54, 250.73, 205.01, 49.06]}, {"formula_id": "formula_54", "formula_text": "f D,Q (\u03b1 1 , ..., \u03b1 l ) := inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) , \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l satisfies that f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ),", "formula_coordinates": [26.0, 108.0, 354.31, 372.56, 79.06]}, {"formula_id": "formula_55", "formula_text": "Q \u03b1 = 1 l i=1 \u03b1 i l j=1 \u03b1 j Q j .", "formula_coordinates": [26.0, 251.77, 603.58, 108.47, 30.32]}, {"formula_id": "formula_56", "formula_text": "D \u03b1 XY = (1 \u2212 l i=1 \u03b1 i )D XIYI + ( l i=1 \u03b1 i )Q \u03b1 , which belongs to D XY . Let R \u03b1 D (h) = X \u00d7Y all (h(x), y)dD \u03b1 XY (x, y).", "formula_coordinates": [26.0, 108.0, 657.09, 335.02, 68.85]}, {"formula_id": "formula_57", "formula_text": "0 \u2264 E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 (n).", "formula_coordinates": [27.0, 207.04, 102.51, 197.92, 16.75]}, {"formula_id": "formula_58", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) = (1 \u2212 l j=1 \u03b1 j )E S\u223cD n X I Y I R in D (A(S)) + l j=1 \u03b1 j E S\u223cD n X I Y I R Qj (A(S)),and", "formula_coordinates": [27.0, 108.0, 150.14, 383.11, 48.89]}, {"formula_id": "formula_59", "formula_text": "inf h\u2208H R \u03b1 D (h) = f D,Q (\u03b1 1 , ..., \u03b1 l ),where", "formula_coordinates": [27.0, 107.64, 199.87, 262.75, 31.83]}, {"formula_id": "formula_60", "formula_text": "R Qj (A(S)) = X \u00d7{K+1}", "formula_coordinates": [27.0, 200.72, 238.42, 106.11, 17.23]}, {"formula_id": "formula_61", "formula_text": "l ) \u2208 \u2206 o l , (1 \u2212 l j=1 \u03b1 j )E S\u223cD n X I Y I R in D (A(S)) + l j=1 \u03b1 j E S\u223cD n X I Y I R Qj (A(S)) \u2212 f D,Q (\u03b1 1 , ..., \u03b1 l ) \u2264 (n).(17)", "formula_coordinates": [27.0, 116.38, 261.1, 387.62, 63.01]}, {"formula_id": "formula_62", "formula_text": "g n (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )E S\u223cD n X I Y I R in D (A(S)) + l j=1 \u03b1 j E S\u223cD n X I Y I R Qj (A(S)).", "formula_coordinates": [27.0, 136.64, 350.32, 338.72, 30.32]}, {"formula_id": "formula_63", "formula_text": "lim n\u2192+\u221e g n (\u03b1 1 , ..., \u03b1 l ) = f D,Q (\u03b1 1 , ..., \u03b1 l ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , lim n\u2192+\u221e g n (0) = f D,Q (0). (18", "formula_coordinates": [27.0, 180.51, 406.6, 319.34, 35.63]}, {"formula_id": "formula_64", "formula_text": ")", "formula_coordinates": [27.0, 499.85, 421.28, 4.15, 8.64]}, {"formula_id": "formula_65", "formula_text": "n\u2192+\u221e E S\u223cD n X I Y I R Qj (A(S)) = f (\u03b1 j ), i.e., lim n\u2192+\u221e g n (\u03b1 j ) = f (\u03b1 j ),(19)", "formula_coordinates": [27.0, 167.75, 476.01, 336.25, 14.13]}, {"formula_id": "formula_66", "formula_text": "E S\u223cD n X I Y I R out D (A(S)) \u2264 inf h\u2208H R out D (h) + (n). Since R out D (A(S)) = R Qj (A(S)) and R out D (h) = R Qj (h), E S\u223cD n X I Y I R Qj (A(S)) \u2264 inf h\u2208H R Qj (h) + (n). Note that inf h\u2208H R Qj (h) \u2264 E S\u223cD n X I Y I R Qj (A(S)). We have 0 \u2264 E S\u223cD n X I Y I R Qj (A(S)) \u2212 inf h\u2208H R Qj (h) \u2264 (n).(20)", "formula_coordinates": [27.0, 108.0, 535.66, 396.0, 107.55]}, {"formula_id": "formula_67", "formula_text": "n\u2192+\u221e E S\u223cD n X I Y I R Qj (A(S)) = inf h\u2208H R Qj (h). (21", "formula_coordinates": [27.0, 214.42, 663.88, 285.43, 14.66]}, {"formula_id": "formula_68", "formula_text": ")", "formula_coordinates": [27.0, 499.85, 664.2, 4.15, 8.64]}, {"formula_id": "formula_69", "formula_text": "lim n\u2192+\u221e E S\u223cD n X I Y I R Qj (A(S)) = f D,Q (\u03b1 j ), i.e., lim n\u2192+\u221e g n (\u03b1 j ) = f (\u03b1 j ).(22)", "formula_coordinates": [27.0, 159.11, 710.37, 344.89, 14.13]}, {"formula_id": "formula_70", "formula_text": "lim n\u2192+\u221e g n (\u03b1 1 , ..., \u03b1 l ) = lim n\u2192+\u221e (1 \u2212 l j=1 \u03b1 j )g n (0) + l j=1 \u03b1 j g n (\u03b1 j ) = (1 \u2212 l j=1 \u03b1 j ) lim n\u2192+\u221e g n (0) + l j=1 \u03b1 j lim n\u2192+\u221e g n (\u03b1 j ).(23)", "formula_coordinates": [28.0, 153.92, 92.87, 350.08, 66.71]}, {"formula_id": "formula_71", "formula_text": "lim n\u2192+\u221e g n (\u03b1 1 , ..., \u03b1 l ) = f D,Q (\u03b1 1 , ..., \u03b1 l ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , lim n\u2192+\u221e g n (0) = f D,Q (0), lim n\u2192+\u221e g n (\u03b1 j ) = f (\u03b1 j ),(24)", "formula_coordinates": [28.0, 181.34, 182.32, 322.66, 55.06]}, {"formula_id": "formula_72", "formula_text": "Lemma 2. inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h), \u2200\u03b1 \u2208 [0, 1), if and only if for any > 0, {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205.", "formula_coordinates": [28.0, 108.0, 280.35, 378.73, 61.94]}, {"formula_id": "formula_73", "formula_text": ") = inf h\u2208H R \u03b1 D (h), for any \u03b1 \u2208 [0, 1]. First, we prove that f D (\u03b1) = (1 \u2212 \u03b1)f D (0) + \u03b1f D (1), \u2200\u03b1 \u2208 [0, 1) implies {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205.", "formula_coordinates": [28.0, 108.0, 354.91, 397.71, 50.93]}, {"formula_id": "formula_74", "formula_text": "R \u03b1 D (h \u03b1 ) \u2264 inf h\u2208H R \u03b1 D (h) + . Note that inf h\u2208H R \u03b1 D (h) = inf h\u2208H (1 \u2212 \u03b1)R in D (h) + \u03b1R out D (h) \u2265 (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h).", "formula_coordinates": [28.0, 108.0, 428.0, 382.75, 57.15]}, {"formula_id": "formula_75", "formula_text": "(1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h) \u2264 R \u03b1 D (h \u03b1 ) \u2264 inf h\u2208H R \u03b1 D (h) + . (25", "formula_coordinates": [28.0, 128.68, 507.18, 371.17, 16.73]}, {"formula_id": "formula_76", "formula_text": ")", "formula_coordinates": [28.0, 499.85, 509.57, 4.15, 8.64]}, {"formula_id": "formula_77", "formula_text": "Note that f D (\u03b1) = (1 \u2212 \u03b1)f D (0) + \u03b1f D (1), \u2200\u03b1 \u2208 [0, 1), i.e., inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h), \u2200\u03b1 \u2208 [0, 1). (26", "formula_coordinates": [28.0, 108.0, 531.76, 391.85, 32.97]}, {"formula_id": "formula_78", "formula_text": ")", "formula_coordinates": [28.0, 499.85, 550.39, 4.15, 8.64]}, {"formula_id": "formula_79", "formula_text": "\u2265 R \u03b1 D (h \u03b1 ) \u2212 inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) R in D (h \u03b1 ) \u2212 inf h\u2208H R in D (h) + \u03b1 R out D (h \u03b1 ) \u2212 inf h\u2208H R out D (h) . (27) Since R out D (h \u03b1 ) \u2212 inf h\u2208H R out D (h) \u2265 0 and R in D (h \u03b1 ) \u2212 inf h\u2208H R in D (h) \u2265 0, Eq. (27) implies that: for any 0 < \u03b1 < 1, R in D (h \u03b1 ) \u2264 inf h\u2208H R in D (h) + /(1 \u2212 \u03b1), R out D (h \u03b1 ) \u2264 inf h\u2208H R out D (h) + /\u03b1. Therefore, h \u03b1 \u2208 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + /(1 \u2212 \u03b1)} \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + /\u03b1}.", "formula_coordinates": [28.0, 107.69, 593.29, 397.7, 131.52]}, {"formula_id": "formula_80", "formula_text": "{h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205.", "formula_coordinates": [29.0, 125.27, 91.48, 361.47, 16.73]}, {"formula_id": "formula_81", "formula_text": "{h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 } = \u2205, then f D (\u03b1) = (1 \u2212 \u03b1)f D (0) + \u03b1f D (1), for any \u03b1 \u2208 [0, 1). Let h \u2208 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + 2 } \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2 }. Then, inf h\u2208H R \u03b1 D (h) \u2264 R \u03b1 D (h ) \u2264 (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h) + 2 \u2264 inf h\u2208H R \u03b1 D (h) + 2 , which implies that |f D (\u03b1) \u2212 (1 \u2212 \u03b1)f D (0) \u2212 \u03b1f D (1)| \u2264 2 . As \u2192 0, |f D (\u03b1) \u2212 (1 \u2212 \u03b1)f D (0) \u2212 \u03b1f D (1)| \u2264 0.", "formula_coordinates": [29.0, 107.64, 138.14, 398.1, 129.54]}, {"formula_id": "formula_82", "formula_text": "R in D (h ) \u2264 inf h\u2208H R in D (h) + 2/n} \u2229 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + 2/n} (the existence ofh n is based on Lemma 2). It is easy to check that E S\u223cD n X I Y I R in D (Ah(S)) \u2264 inf h\u2208H R in D (h) + 2/n. E S\u223cD n X I Y I R out D (Ah(S)) \u2264 inf h\u2208H R out D (h) + 2/n. Since (1\u2212\u03b1) inf h\u2208H R in D (h)+\u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h), we obtain that for any \u03b1 \u2208 [0, 1], E S\u223cD n X I Y I R \u03b1 D (Ah(S)) \u2264 inf h\u2208H R \u03b1 D (h) + 2/n.", "formula_coordinates": [29.0, 108.0, 476.37, 397.24, 110.24]}, {"formula_id": "formula_83", "formula_text": "D XI (A) = A f I (x)d\u00b5(x), D XO (A) = A f O (x)d\u00b5(x).", "formula_coordinates": [30.0, 189.15, 131.94, 233.7, 17.24]}, {"formula_id": "formula_84", "formula_text": "f I (x) \u2265 1 m and f O (x) \u2265 1 m }. It is clear that \u222a +\u221e m=1 A m = {x \u2208 X : f I (x) > 0 and f O (x) > 0} = A overlap ,and", "formula_coordinates": [30.0, 108.0, 178.28, 323.8, 49.88]}, {"formula_id": "formula_85", "formula_text": "A m \u2282 A m+1 .", "formula_coordinates": [30.0, 277.89, 230.99, 56.22, 9.65]}, {"formula_id": "formula_86", "formula_text": "lim m\u2192+\u221e \u00b5(A m ) = \u00b5(A overlap ) > 0,", "formula_coordinates": [30.0, 235.72, 264.03, 140.57, 14.13]}, {"formula_id": "formula_87", "formula_text": "c \u03b1 = min y1\u2208Y all (1 \u2212 \u03b1) min y2\u2208Y (y 1 , y 2 ) + \u03b1 (y 1 , K + 1) . It is clear that c \u03b1 > 0 for \u03b1 \u2208 (0, 1). Then, for any h \u2208 H, R \u03b1 D (h) = X \u00d7Y all (h(x), y)dD \u03b1 XY (x, y) = X \u00d7Y (1 \u2212 \u03b1) (h(x), y)dD XIYI (x, y) + X \u00d7{K+1} \u03b1 (h(x), y)dD XOYO (x, y) \u2265 Am 0 \u00d7Y (1 \u2212 \u03b1) (h(x), y)dD XIYI (x, y) + Am 0 \u00d7{K+1} \u03b1 (h(x), y)dD XOYO (x, y) = Am 0 (1 \u2212 \u03b1) Y (h(x), y)dD YI|XI (y|x) dD XI (x) + Am 0 \u03b1 (h(x), K + 1)dD XO (x) \u2265 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 )dD XI (x) + Am 0 \u03b1 (h(x), K + 1)dD XO (x) \u2265 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 )f I (x)d\u00b5(x) + Am 0 \u03b1 (h(x), K + 1)f O (x)d\u00b5(x) \u2265 1 m 0 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 )d\u00b5(x) + 1 m 0 Am 0 \u03b1 (h(x), K + 1)d\u00b5(x) = 1 m 0 Am 0 (1 \u2212 \u03b1) min y2\u2208Y (h(x), y 2 ) + \u03b1 (h(x), K + 1) d\u00b5(x) \u2265 c \u03b1 m 0 \u00b5(A m0 ) > 0. Therefore, inf h\u2208H R \u03b1 D (h) \u2265 c \u03b1 m 0 \u00b5(A m0 ) > 0. Third, Condition 1 indicates that inf h\u2208H R \u03b1 D (h) = (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R in D(", "formula_coordinates": [30.0, 107.67, 327.95, 396.33, 362.28]}, {"formula_id": "formula_88", "formula_text": "\u2203h 1 , h 2 \u2208 H, s.t. h 1 (x) \u2208 Y, h 2 (x) = K + 1} = \u2205.", "formula_coordinates": [31.0, 268.18, 141.91, 207.33, 9.65]}, {"formula_id": "formula_89", "formula_text": "h 1 , h 2 \u2208 H such that h 1 (x 1 ) \u2208 Y, h 2 (x 1 ) = K +1, for some x 1 \u2208 X . We set D XY = 0.5 * \u03b4 (x1,h1(x1)) +0.5 * \u03b4 (x1,h2(x1)) ,", "formula_coordinates": [31.0, 108.0, 185.6, 397.24, 20.86]}, {"formula_id": "formula_90", "formula_text": "f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , if and only if arg min h\u2208H R D (h) = l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h),", "formula_coordinates": [31.0, 108.0, 339.5, 365.18, 80.86]}, {"formula_id": "formula_91", "formula_text": "R Qj (h) = X \u00d7{K+1}", "formula_coordinates": [31.0, 218.07, 460.38, 88.76, 17.23]}, {"formula_id": "formula_92", "formula_text": "f D,Q (\u03b1 1 , ..., \u03b1 l ) = inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) , \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l", "formula_coordinates": [31.0, 132.33, 511.18, 346.85, 30.32]}, {"formula_id": "formula_93", "formula_text": "f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , then, arg min h\u2208H R D (h) = l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h). Let D XY = (1 \u2212 l j=1 \u03bb j )D XIYI + l j=1 \u03bb j Q j , for some (\u03bb 1 , ..., \u03bb l ) \u2208 \u2206 o l .", "formula_coordinates": [31.0, 108.0, 568.7, 365.18, 105.87]}, {"formula_id": "formula_94", "formula_text": "arg min h\u2208H R D (h) = arg min h\u2208H (1 \u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) = \u2205.", "formula_coordinates": [31.0, 160.31, 693.66, 291.39, 30.32]}, {"formula_id": "formula_95", "formula_text": "\u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) . Hence, (1 \u2212 l j=1 \u03bb j )R in D (h 0 ) + l j=1 \u03bb j R Qj (h 0 ) = inf h\u2208H (1 \u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) . (28)", "formula_coordinates": [32.0, 120.96, 73.68, 383.04, 56.51]}, {"formula_id": "formula_96", "formula_text": "f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ) implies (1\u2212 l j=1 \u03bb j ) inf h\u2208H R in D (h)+ l j=1 \u03bb j inf h\u2208H R Qj (h) = inf h\u2208H (1\u2212 l j=1 \u03bb j )R in D (h)+ l j=1 \u03bb j R Qj (h) . (29)", "formula_coordinates": [32.0, 112.98, 140.15, 391.02, 54.38]}, {"formula_id": "formula_97", "formula_text": "(1 \u2212 l j=1 \u03bb j ) inf h\u2208H R in D (h) + l j=1 \u03bb j inf h\u2208H R Qj (h) = (1 \u2212 l j=1 \u03bb j )R in D (h 0 ) + l j=1 \u03bb j R Qj (h 0 ). (30) Since R in D (h 0 ) \u2265 inf h\u2208H R in D (h) and R Qj (h 0 ) \u2265 inf h\u2208H R in Qj(", "formula_coordinates": [32.0, 108.0, 223.29, 396.0, 58.07]}, {"formula_id": "formula_98", "formula_text": "R in D (h 0 ) = inf h\u2208H R in D (h), R Qj (h 0 ) = inf h\u2208H R Qj (h), \u2200j = 1, ..., l, which implies that h 0 \u2208 l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h).", "formula_coordinates": [32.0, 107.64, 300.33, 289.7, 88.11]}, {"formula_id": "formula_99", "formula_text": "arg min h\u2208H R D (h) \u2282 l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h).(31)", "formula_coordinates": [32.0, 188.94, 407.39, 315.06, 30.32]}, {"formula_id": "formula_100", "formula_text": "f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l ,", "formula_coordinates": [32.0, 141.31, 469.41, 329.37, 30.32]}, {"formula_id": "formula_101", "formula_text": "l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h), inf h\u2208H R D (h) = inf h\u2208H (1 \u2212 l j=1 \u03bb j )R in D (h) + l j=1 \u03bb j R Qj (h) =(1 \u2212 l j=1 \u03bb j ) inf h\u2208H R in D (h) + l j=1 \u03bb j inf h\u2208H R Qj (h) =(1 \u2212 l j=1 \u03bb j )R in D (h ) + l j=1 \u03bb j R Qj (h ) = R D (h ), which implies that h \u2208 arg min h\u2208H R D (h).", "formula_coordinates": [32.0, 107.64, 509.68, 327.89, 165.86]}, {"formula_id": "formula_102", "formula_text": "l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h) \u2282 arg min h\u2208H R D (h).(32)", "formula_coordinates": [32.0, 188.94, 693.66, 315.06, 30.32]}, {"formula_id": "formula_103", "formula_text": "l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h) = arg min h\u2208H R D (h).", "formula_coordinates": [33.0, 188.11, 104.07, 235.79, 30.32]}, {"formula_id": "formula_104", "formula_text": "arg min h\u2208H R D (h) = l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h), then, f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ), \u2200(\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l .", "formula_coordinates": [33.0, 108.0, 166.91, 365.18, 86.86]}, {"formula_id": "formula_105", "formula_text": "h 0 \u2208 l j=1 arg min h\u2208H R Qj (h) arg min h\u2208H R in D (h), then, for any (\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l ,(1 \u2212", "formula_coordinates": [33.0, 108.0, 277.07, 289.34, 74.79]}, {"formula_id": "formula_106", "formula_text": "l j=1 \u03b1 j ) inf h\u2208H R in D (h) + l j=1 \u03b1 j inf h\u2208H R Qj (h) \u2264 inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) \u2264 (1 \u2212 l j=1 \u03b1 j )R in D (h 0 ) + l j=1 \u03b1 j R Qj (h 0 ) = (1 \u2212 l j=1 \u03b1 j ) inf h\u2208H R in D (h) + l j=1 \u03b1 j inf h\u2208H R Qj (h).", "formula_coordinates": [33.0, 129.33, 332.71, 375.75, 103.09]}, {"formula_id": "formula_107", "formula_text": "l ) \u2208 \u2206 o l ,(1 \u2212", "formula_coordinates": [33.0, 112.43, 443.87, 142.97, 39.4]}, {"formula_id": "formula_108", "formula_text": "l j=1 \u03b1 j ) inf h\u2208H R in D (h) + l j=1 \u03b1 j inf h\u2208H R Qj (h) = inf h\u2208H (1 \u2212 l j=1 \u03b1 j )R in D (h) + l j=1 \u03b1 j R Qj (h) , which implies that: for any (\u03b1 1 , ..., \u03b1 l ) \u2208 \u2206 o l , f D,Q (\u03b1 1 , ..., \u03b1 l ) = (1 \u2212 l j=1 \u03b1 j )f D,Q (0) + l j=1 \u03b1 j f D,Q (\u03b1 j ).", "formula_coordinates": [33.0, 107.64, 464.11, 391.93, 88.96]}, {"formula_id": "formula_109", "formula_text": "inf h\u2208H R \u03b4 x out j (h) = 0, where R \u03b4 x out j (h) = X (h(x), K + 1)d\u03b4 x out j (x).", "formula_coordinates": [34.0, 107.64, 155.45, 284.35, 54.16]}, {"formula_id": "formula_110", "formula_text": "l j=1 arg min h\u2208H R \u03b4 x out j (h) = \u2205, then for \u2200h \u2208 l j=1 arg min h\u2208H R \u03b4 x out j (h), h (x out i ) = K + 1, \u2200i = 1, ..., l. Therefore, if l j=1 arg min h\u2208H R \u03b4 x out j (h) arg min h\u2208H R in D (h) = \u2205, then for any h * \u2208 l j=1 arg min h\u2208H R \u03b4 x out j (h) arg min h\u2208H R in D (h), we have that h * (x out i ) = K + 1, \u2200i = 1, ..., l.", "formula_coordinates": [34.0, 107.69, 214.88, 397.53, 98.06]}, {"formula_id": "formula_111", "formula_text": "l j=1 arg min h\u2208H R \u03b4 x out j (h) arg min h\u2208H R in D (h) = arg min h\u2208H R D (h) = \u2205.", "formula_coordinates": [34.0, 174.76, 352.49, 262.47, 30.32]}, {"formula_id": "formula_112", "formula_text": "C = {x 1 , ..., x m , x m+1 }, which satisfies that there existsh \u2208 H such thath(x i ) \u2208 Y for any i = 1, ..., m, m + 1. Let H \u03c6 C = {(\u03c6 \u2022 h(x 1 ), ..., \u03c6 \u2022 h(x m ), \u03c6 \u2022 h(x m+1 ) : h \u2208 H}. It is clear that (1, 1, ..., 1) = (\u03c6 \u2022h(x 1 ), ..., \u03c6 \u2022h(x m ), \u03c6 \u2022h(x m+1 )) \u2208 H \u03c6 C , where (1, 1, ..., 1) means all elements are 1. Let H \u03c6 m+1 = {(\u03c6\u2022h(x 1 ), ..., \u03c6\u2022h(x m ), \u03c6\u2022h(x m+1 ) : h is any hypothesis function from X to Y all }.", "formula_coordinates": [34.0, 107.64, 561.62, 398.1, 104.52]}, {"formula_id": "formula_113", "formula_text": "|H \u03c6 C | \u2264 v i=0 m+1 i . Since v i=0 m+1 i < 2 m+1 \u2212 1 (because v < m), we obtain that |H \u03c6 C | \u2264 2 m+1 \u2212 2. Therefore, H \u03c6 C \u222a {(2, 2..., 2)} is a proper subset of H \u03c6 m+1", "formula_coordinates": [34.0, 266.48, 695.01, 79.04, 30.32]}, {"formula_id": "formula_114", "formula_text": "1)(\u03c6 \u2022 h (x 1 ), ..., \u03c6 \u2022 h (x m ), \u03c6 \u2022 h (x m+1 )) / \u2208 H \u03c6 C ; 2) There exist x j , x p \u2208 C such that \u03c6 \u2022 h (x j ) = 2 and \u03c6 \u2022 h (x p ) = 1. Let C I = C \u2229 {x \u2208 X : \u03c6 \u2022 h (x) = 1} and C O = C \u2229 {x \u2208 X : \u03c6 \u2022 h (x) = 2};", "formula_coordinates": [35.0, 108.0, 136.64, 347.34, 53.22]}, {"formula_id": "formula_115", "formula_text": "D XY = 0.5 * D XI * D YI|XI + 0.5 * D XO * D YO|XO , where D XI = 1 |C I | x\u2208CI \u03b4 x and D YI|XI (y|x) = 1, ifh(x) = y and x \u2208 C I ; and D XO = 1 |C O | x\u2208CO \u03b4 x and D YO|XO (K + 1|x) = 1, if x \u2208 C O . Since D XY is a finite discrete distribution and (\u03c6 \u2022 h (x 1 ), ..., \u03c6 \u2022 h (x m ), \u03c6 \u2022 h (x m+1 )) / \u2208 H \u03c6 C , it is clear that arg min h\u2208H R D (h) = \u2205 and inf h\u2208H R D (h) > 0. Additionally, R in D (h) = 0. Therefore, inf h\u2208H R in D (h) = 0. Proof by Contradiction: suppose that OOD detection is learnable in D s XY for H, then Lemma 1 implies that inf h\u2208H R D (h) = 0.5 * inf h\u2208H R in D (h) + 0.5 * inf h\u2208H R out D (h).", "formula_coordinates": [35.0, 107.64, 215.84, 397.1, 193.71]}, {"formula_id": "formula_116", "formula_text": "E S X \u223cD n X I i:Ci\u2229S X =\u2205 D XI (C i ) \u2264 r en .", "formula_coordinates": [35.0, 225.13, 697.07, 161.73, 27.88]}, {"formula_id": "formula_117", "formula_text": "E S X \u223cD n X I i:Ci\u2229S X =\u2205 D XI (C i ) = r i=1 D XI (C i ) \u2022 E S X \u223cD n X I 1 Ci\u2229S X =\u2205 ,", "formula_coordinates": [36.0, 150.2, 90.19, 311.61, 31.55]}, {"formula_id": "formula_118", "formula_text": "E S X \u223cD n X I 1 Ci\u2229S X =\u2205 = X n 1 Ci\u2229S X =\u2205 dD n XI (S X ) = X 1 Ci\u2229{x}=\u2205 dD XI (x) n = 1 \u2212 D XI (C i ) n \u2264 e \u2212nD X I (Ci) .", "formula_coordinates": [36.0, 188.16, 164.9, 235.69, 60.75]}, {"formula_id": "formula_119", "formula_text": "E S X \u223cD n X I i:Ci\u2229S=\u2205 D XI (C i ) \u2264 r i=1 D XI (C i )e \u2212nD X I (Ci) \u2264 r max i\u2208{1,...,r} D XI (C i )e \u2212nD X I (Ci) \u2264 r ne ,", "formula_coordinates": [36.0, 157.77, 247.59, 296.46, 55.36]}, {"formula_id": "formula_120", "formula_text": "E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) < cons (n),", "formula_coordinates": [36.0, 211.56, 377.24, 188.87, 13.29]}, {"formula_id": "formula_121", "formula_text": "C i , then dist(x, x ) \u2264 \u221a d ; otherwise, dist(x, x ) \u2264 \u221a d. Therefore, E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) \u2264E S\u223cD n X I Y I \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) + \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) \u2264E S X \u223cD n X I \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) + \u221a d i:Ci\u2229S X =\u2205 D XI (C i ) .", "formula_coordinates": [36.0, 172.52, 468.02, 305.1, 97.82]}, {"formula_id": "formula_122", "formula_text": "Ci\u2229S X =\u2205 D XI (C i ) \u2264 D XI ( i:Ci\u2229S X =\u2205 C i ) \u2264 1.", "formula_coordinates": [36.0, 170.53, 589.54, 195.31, 11.76]}, {"formula_id": "formula_123", "formula_text": "E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) \u2264 \u221a d + r \u221a d ne = \u221a d + \u221a d ne d . If we set = 2n \u22121/(d+1) , then E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) \u2264 2 \u221a d n 1/(d+1) + \u221a d 2 d en 1/(d+1) .", "formula_coordinates": [36.0, 108.0, 601.62, 333.29, 77.95]}, {"formula_id": "formula_124", "formula_text": "(n) = 2 \u221a d n 1/(d+1) + \u221a d 2 d en 1/(", "formula_coordinates": [36.0, 164.4, 680.48, 100.27, 19.83]}, {"formula_id": "formula_125", "formula_text": "inf h\u2208H R D (h) > 0. Additionally, R in D (h in ) = 0 (here h in = 1), hence, inf h\u2208H R in D (h) = 0. Since OOD detection is learnable in D s XY for H, Lemma 1 implies that inf h\u2208H R D (h) = (1 \u2212 \u03c0 out ) inf h\u2208H R in D (h) + \u03c0 out inf h\u2208H R out D (h),", "formula_coordinates": [37.0, 108.0, 331.73, 397.24, 63.21]}, {"formula_id": "formula_126", "formula_text": "A(S)(x) = 1, if dist(x, \u03c0 1 (x, S)) < 0.5 * d 0 ; 2, if dist(x, \u03c0 1 (x, S)) \u2265 0.5 * d 0 ,", "formula_coordinates": [37.0, 225.06, 518.8, 216.61, 23.55]}, {"formula_id": "formula_127", "formula_text": "A(S)(x) = 2, hence, E S\u223cD n X I Y I R out D (A(S)) = 0. (33", "formula_coordinates": [37.0, 108.0, 623.27, 391.85, 40.26]}, {"formula_id": "formula_128", "formula_text": ")", "formula_coordinates": [37.0, 499.85, 650.56, 4.15, 8.64]}, {"formula_id": "formula_129", "formula_text": "E x\u223cD X I ,S\u223cD n X I Y I dist(x, \u03c0 1 (x, S)) < cons (n),", "formula_coordinates": [37.0, 211.56, 691.86, 188.87, 13.29]}, {"formula_id": "formula_130", "formula_text": "D", "formula_coordinates": [38.0, 160.65, 93.07, 8.25, 8.74]}, {"formula_id": "formula_132", "formula_text": "E S\u223cD n X I Y I R D (A(S)) \u2264 0 + 2B cons (m)/d 0 \u2264 inf h\u2208H R D (h) + 2B cons (m)/d 0 . (35", "formula_coordinates": [38.0, 146.06, 196.05, 353.79, 14.66]}, {"formula_id": "formula_133", "formula_text": ")", "formula_coordinates": [38.0, 499.85, 196.37, 4.15, 8.64]}, {"formula_id": "formula_134", "formula_text": "E S\u223cD n X I Y I R in D (A in (S)) \u2264 inf h\u2208H in R in D (h) + (n).", "formula_coordinates": [38.0, 209.47, 362.5, 193.06, 17.27]}, {"formula_id": "formula_135", "formula_text": "E S\u223cD n X I Y I R in \u03c6(D) (A b (\u03c6(S))) \u2264 inf h\u2208H b R in \u03c6(D) (h) + (n), E S\u223cD n X I Y I R out \u03c6(D) (A b (\u03c6(S))) \u2264 inf h\u2208H b R out \u03c6(D) (h) + (n),", "formula_coordinates": [38.0, 192.35, 435.68, 227.3, 38.92]}, {"formula_id": "formula_136", "formula_text": "R in \u03c6(D) (A b (\u03c6(S))) = X \u00d7Y (A b (\u03c6(S))(x), \u03c6(y))dD XIYI (x, y),(36)", "formula_coordinates": [38.0, 174.27, 504.53, 329.73, 19.31]}, {"formula_id": "formula_137", "formula_text": "R in \u03c6(D) (h) = X \u00d7Y (h(x), \u03c6(y))dD XIYI (x, y),(37)", "formula_coordinates": [38.0, 210.21, 536.04, 293.79, 19.31]}, {"formula_id": "formula_138", "formula_text": "R out \u03c6(D) (A b (\u03c6(S))) = X \u00d7{K+1} (A b (\u03c6(S))(x), \u03c6(y))dD XOYO (x, y),(38)", "formula_coordinates": [38.0, 161.85, 564.59, 342.15, 19.31]}, {"formula_id": "formula_139", "formula_text": "R out \u03c6(D) (h) = X \u00d7{K+1} (h(x), \u03c6(y))dD XOYO (x, y),(39)", "formula_coordinates": [38.0, 197.79, 601.95, 306.21, 19.31]}, {"formula_id": "formula_140", "formula_text": "E S\u223cD n X I Y I R in \u03c6(D) (A b (\u03c6(S))) \u2264 (n), E S\u223cD n X I Y I R out \u03c6(D) (A b (\u03c6(S))) \u2264 (n).", "formula_coordinates": [38.0, 148.36, 672.91, 315.29, 15.36]}, {"formula_id": "formula_141", "formula_text": "inf h\u2208H in R in D (h) = inf h\u2208H R in D (h).", "formula_coordinates": [39.0, 248.05, 130.53, 115.9, 17.27]}, {"formula_id": "formula_142", "formula_text": "E S\u223cD n X I Y I R in D (A(S)) \u2264 E S\u223cD n X I Y I R in D (A in (S)) + cE S\u223cD n X I Y I R in \u03c6(D) (A b (\u03c6(S))) \u2264 inf h\u2208H in R in D (h) + (n) + c (n) = inf h\u2208H R in D (h) + (n) + c (n),(40)", "formula_coordinates": [39.0, 122.68, 187.66, 381.32, 35.58]}, {"formula_id": "formula_143", "formula_text": "E S\u223cD n X I Y I R out D (A(S)) \u2264 cE S\u223cD n X |rmI Y I R out \u03c6(D) (A b (\u03c6(S))) \u2264 c (n) \u2264 inf h\u2208H R out D (h) + c (n). (41", "formula_coordinates": [39.0, 186.65, 273.76, 313.2, 36.24]}, {"formula_id": "formula_144", "formula_text": ") Note that (1 \u2212 \u03b1) inf h\u2208H R in D (h) + \u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h).", "formula_coordinates": [39.0, 108.0, 287.93, 396.0, 41.02]}, {"formula_id": "formula_145", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A(S)) \u2264 inf h\u2208H R \u03b1 D (h) + (n) + c (n).", "formula_coordinates": [39.0, 197.64, 343.74, 216.71, 16.73]}, {"formula_id": "formula_146", "formula_text": "\u2200D XY \u2208[D XY ] {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h)+2/n} {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h)+2/n}.", "formula_coordinates": [39.0, 108.0, 581.46, 401.09, 24.57]}, {"formula_id": "formula_147", "formula_text": "D XY \u2208 [D XY ], E S\u223cD n X I Y I R in D (Ah(S)) \u2264 inf h\u2208H R in D (h) + 2/n. E S\u223cD n X I Y I R out D (Ah(S)) \u2264 inf h\u2208H R out D (h) + 2/n. Since (1\u2212\u03b1) inf h\u2208H R in D (h)+\u03b1 inf h\u2208H R out D (h) \u2264 inf h\u2208H R \u03b1 D (h), we obtain that for any \u03b1 \u2208 [0, 1], E S\u223cD n X I Y I R \u03b1 D (Ah(S)) \u2264 inf h\u2208H R \u03b1 D (h) + 2/n.", "formula_coordinates": [39.0, 108.0, 614.91, 397.24, 92.47]}, {"formula_id": "formula_148", "formula_text": "E S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 cons (n), E S\u223cD n X I Y I R out D (A(S)) \u2212 inf h\u2208H R out D (h) \u2264 cons (n).", "formula_coordinates": [40.0, 199.4, 196.17, 213.19, 37.21]}, {"formula_id": "formula_149", "formula_text": "E S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 , E S\u223cD n X I Y I R out D (A(S)) \u2212 inf h\u2208H R out D (h) \u2264 , which implies that there exists S \u223c D n XIYI such that R in D (A(S )) \u2212 inf h\u2208H R in D (h) \u2264 , R out D (A(S )) \u2212 inf h\u2208H R out D (h) \u2264 .", "formula_coordinates": [40.0, 107.64, 259.13, 290.31, 101.5]}, {"formula_id": "formula_150", "formula_text": "D XY \u2208 [D XY ], A(S ) \u2208 {h \u2208 H : R out D (h ) \u2264 inf h\u2208H R out D (h) + } \u2229 {h \u2208 H : R in D (h ) \u2264 inf h\u2208H R in D (h) + },", "formula_coordinates": [40.0, 120.12, 381.12, 371.75, 34.05]}, {"formula_id": "formula_151", "formula_text": "A = {A S 7 : \u2200 S \u2208 F },", "formula_coordinates": [40.0, 256.2, 603.85, 99.59, 11.0]}, {"formula_id": "formula_152", "formula_text": "MMD K (D i XIYI , D j XIYI ) > c.", "formula_coordinates": [41.0, 247.07, 133.06, 117.87, 13.83]}, {"formula_id": "formula_153", "formula_text": "E S\u223cD n X I Y I MMD K (D XIYI , P S ) \u2264 (n), where (n) = O( 1 \u221a n 1\u2212\u03b8 ).(42)", "formula_coordinates": [41.0, 170.68, 190.33, 333.32, 24.07]}, {"formula_id": "formula_154", "formula_text": "MMD K (D i XIYI , P S D i ) < c 100 . Let S = {S D 1 , ..., S D i , ..., S D m }.", "formula_coordinates": [41.0, 108.0, 238.52, 261.07, 46.06]}, {"formula_id": "formula_155", "formula_text": "P S\u223cD i,n X I Y I MMD K (D i XIYI , P S ) \u2264 (n) \u03b4 > 1 \u2212 \u03b4, which implies that P S\u223cD i,n X I Y I MMD K (P S D i , P S ) \u2264 (n) \u03b4 + c 100 > 1 \u2212 \u03b4.", "formula_coordinates": [41.0, 107.64, 303.88, 312.57, 69.68]}, {"formula_id": "formula_156", "formula_text": "P S\u223cD i,n X I Y I A S (S) = A D i (S) \u2264 200 (n) c .", "formula_coordinates": [41.0, 216.91, 399.64, 178.19, 22.31]}, {"formula_id": "formula_157", "formula_text": "E S\u223cD i,n X I Y I R \u03b1 D (A S (S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 i (n) + 200B (n) c ,", "formula_coordinates": [41.0, 178.59, 449.03, 254.82, 22.31]}, {"formula_id": "formula_158", "formula_text": "Let max (n) = max{ 1 (n), ..., m (n)} + 200B (n) c .", "formula_coordinates": [41.0, 108.0, 496.73, 205.07, 14.38]}, {"formula_id": "formula_159", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A S (S)) \u2212 inf h\u2208H R \u03b1 D (h) \u2264 max (n) = O( 1 \u221a n 1\u2212\u03b8 ).", "formula_coordinates": [41.0, 170.18, 535.44, 271.65, 24.07]}, {"formula_id": "formula_160", "formula_text": "1 n n i=1 (h * (x i ), y i ) = 0.", "formula_coordinates": [42.0, 257.66, 100.17, 97.88, 30.32]}, {"formula_id": "formula_161", "formula_text": "min h\u2208H 1 m m j=1 (h(x j ), K + 1), subject to 1 n n i=1 (h(x i ), y i ) = 0.", "formula_coordinates": [42.0, 176.69, 158.16, 258.62, 30.32]}, {"formula_id": "formula_163", "formula_text": "E S\u223cD n X I Y I sup h\u2208H S R in D (h) \u2264 inf h\u2208H R in D (h) + C \u03b8 \u221a n 1\u2212\u03b8 ,", "formula_coordinates": [42.0, 205.45, 350.62, 201.11, 24.07]}, {"formula_id": "formula_164", "formula_text": "E Sn\u223c\u00b5 n sup S\u2208(X \u00d7Y) n [R \u00b5 (A Sn (S), K + 1) \u2212 inf h\u2208H S R \u00b5 (h, K + 1)] \u2264 C \u03b8 \u221a n 1\u2212\u03b8 ,(44)", "formula_coordinates": [42.0, 153.11, 396.23, 350.89, 24.07]}, {"formula_id": "formula_165", "formula_text": "H S = {h \u2208 H : n i=1 (h(x i ), y i ) = 0}, here S = {(x 1 , y 1 ), ..., (x n , y n )}, and R \u00b5 (h, K + 1) = E x\u223c\u00b5 (h(x), K + 1) = X (h(x), K + 1)d\u00b5(x).", "formula_coordinates": [42.0, 108.0, 435.17, 346.13, 66.06]}, {"formula_id": "formula_166", "formula_text": "E Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I [R \u00b5 (A Sn (S), K + 1) \u2212 inf h\u2208H S R \u00b5 (h, K + 1)] \u2264 C \u03b8 \u221a n 1\u2212\u03b8 . (45", "formula_coordinates": [42.0, 125.19, 538.91, 374.66, 25.66]}, {"formula_id": "formula_167", "formula_text": ")", "formula_coordinates": [42.0, 499.85, 545.97, 4.15, 8.64]}, {"formula_id": "formula_168", "formula_text": "E S\u223cD n X I Y I sup h\u2208H S R in D (h) \u2264 C \u03b8 \u221a n 1\u2212\u03b8 ,(46)", "formula_coordinates": [42.0, 234.93, 594.46, 269.07, 24.07]}, {"formula_id": "formula_169", "formula_text": "C \u03b8 \u221a n \u2265 E S\u223cD n X I Y I sup h\u2208H S R in D (h) =E S\u223cD n X I Y I sup h\u2208H S g<K+1 (h(x), g(x))f I (x)d\u00b5(x) \u2265 2 b E S\u223cD n X I Y I sup h\u2208H S g<K+1 (h(x), g(x))d\u00b5(x).", "formula_coordinates": [42.0, 137.66, 641.74, 337.88, 51.96]}, {"formula_id": "formula_170", "formula_text": "E S\u223cD n X I Y I inf h\u2208H S g<K+1 (h(x), K + 1)d\u00b5(x) \u2265 \u00b5(x \u2208 X : g(x) < K + 1) \u2212 C \u03b8 b 2 \u221a n 1\u2212\u03b8 .", "formula_coordinates": [43.0, 125.39, 91.26, 361.22, 24.07]}, {"formula_id": "formula_171", "formula_text": "E S\u223cD n X I Y I inf h\u2208H S R \u00b5 (h, K + 1) \u2265 \u00b5(x \u2208 X : g(x) < K + 1) \u2212 C \u03b8 b 2 \u221a n 1\u2212\u03b8 .(47)", "formula_coordinates": [43.0, 158.24, 137.6, 345.76, 24.07]}, {"formula_id": "formula_172", "formula_text": "inf h\u2208H S R \u00b5 (h, K + 1) \u2264 \u00b5(x \u2208 X : g(x) < K + 1).(48)", "formula_coordinates": [43.0, 204.78, 190.53, 299.22, 15.28]}, {"formula_id": "formula_173", "formula_text": "E S\u223cD n X I Y I inf h\u2208H S R \u00b5 (h, K + 1) \u2212 \u00b5(x \u2208 X : g(x) < K + 1) \u2264 C \u03b8 b 2 \u221a n 1\u2212\u03b8 . (49", "formula_coordinates": [43.0, 158.24, 234.43, 341.61, 24.07]}, {"formula_id": "formula_174", "formula_text": ")", "formula_coordinates": [43.0, 499.85, 241.49, 4.15, 8.64]}, {"formula_id": "formula_175", "formula_text": "D X I Y I \u2208DI E S\u223cD n X I Y I R \u00b5 (A Sn (S), K + 1) \u2212 \u00b5(x \u2208 X : g(x) < K + 1) \u2264 C \u03b8 (b + 1) \u221a n 1\u2212\u03b8 .(50)", "formula_coordinates": [43.0, 147.39, 286.39, 356.61, 35.63]}, {"formula_id": "formula_176", "formula_text": "E Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I R in D (A Sn (S)) \u2264 C \u03b8 \u221a n 1\u2212\u03b8 ,(51)", "formula_coordinates": [43.0, 197.12, 345.45, 306.88, 25.66]}, {"formula_id": "formula_177", "formula_text": "E Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I \u2212 g<K+1 (A Sn (S)(x), K + 1)d\u00b5(x) + \u00b5(x \u2208 X : g(x) < K + 1) \u2264 2bC \u03b8 \u221a n 1\u2212\u03b8 . (52", "formula_coordinates": [43.0, 161.16, 400.91, 338.69, 47.12]}, {"formula_id": "formula_178", "formula_text": ")", "formula_coordinates": [43.0, 499.85, 416.46, 4.15, 8.64]}, {"formula_id": "formula_179", "formula_text": "E Sn\u223c\u00b5 n sup D X I Y I \u2208DI E S\u223cD n X I Y I g=K+1 (A Sn (S)(x), K + 1)d\u00b5(x) \u2264 2bC \u03b8 \u221a n 1\u2212\u03b8 + C \u03b8 (b + 1) \u221a n 1\u2212\u03b8 .", "formula_coordinates": [43.0, 121.37, 475.92, 369.25, 25.66]}, {"formula_id": "formula_180", "formula_text": "D X I Y I \u2208DI E S\u223cD n X I Y I R out D (A S n ) = sup D X I Y I \u2208DI E S\u223cD n X I Y I g=K+1 (A S n (S)(x), K + 1)f O (x)d\u00b5(x) \u22642b sup D X I Y I \u2208DI E S\u223cD n X I Y I g=K+1 (A S n (S)(x), K + 1)d\u00b5(x) \u2264 4b 2 C \u03b8 \u221a n 1\u2212\u03b8 + 2C \u03b8 (b 2 + b) \u221a n 1\u2212\u03b8 .(53)", "formula_coordinates": [43.0, 116.64, 525.14, 387.36, 81.67]}, {"formula_id": "formula_181", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A S n ) \u2264 max 4b 2 C \u03b8 \u221a n 1\u2212\u03b8 + 2C \u03b8 (b 2 + b) \u221a n 1\u2212\u03b8 , C \u03b8 \u221a n 1\u2212\u03b8 .", "formula_coordinates": [43.0, 175.06, 636.06, 261.89, 25.64]}, {"formula_id": "formula_182", "formula_text": "E S\u223cD n X I Y I R \u03b1 D (A S ) \u2264 max 4b 2 C \u03b8 \u221a n 1\u2212\u03b8 + 2C \u03b8 (b 2 + b) \u221a n 1\u2212\u03b8 , C \u03b8 \u221a n 1\u2212\u03b8 .", "formula_coordinates": [43.0, 176.09, 700.6, 259.81, 25.64]}, {"formula_id": "formula_183", "formula_text": "1 M 0\u22121 (y 1 , y 2 ) \u2264 (y 1 , y 2 ) \u2264 M 0\u22121 (y 1 , y 2 ). Hence, 1 M R \u03b1, 0\u22121 D (h) \u2264 R \u03b1, D (h) \u2264 M R \u03b1, 0\u22121 D (h),", "formula_coordinates": [44.0, 108.0, 140.47, 291.83, 58.98]}, {"formula_id": "formula_184", "formula_text": "\u03b1, 0\u22121 D", "formula_coordinates": [44.0, 142.04, 202.36, 23.13, 14.3]}, {"formula_id": "formula_185", "formula_text": "E S\u223cD n X I Y I R \u03b1, 0\u22121 D (A) \u2264 O( 1 \u221a n 1\u2212\u03b8 ),", "formula_coordinates": [44.0, 229.81, 254.84, 152.38, 24.07]}, {"formula_id": "formula_186", "formula_text": "[0, 1], 1 M E S\u223cD n X I Y I R \u03b1, D (A) \u2264 O( 1 \u221a n 1\u2212\u03b8 ).", "formula_coordinates": [44.0, 230.77, 285.93, 151.66, 40.6]}, {"formula_id": "formula_187", "formula_text": "F \u03c3 q := {f w,b : \u2200w i \u2208 R li\u00d7li\u22121 , \u2200b i \u2208 R li\u00d71 , i = 2, ..., g}.", "formula_coordinates": [44.0, 185.64, 545.12, 240.71, 12.69]}, {"formula_id": "formula_188", "formula_text": "g \u2264 g , l 1 = l 1 , l g = l g , l i \u2264 l i , \u2200i = 1, ..., g \u2212 1,", "formula_coordinates": [44.0, 231.27, 600.27, 146.54, 26.41]}, {"formula_id": "formula_189", "formula_text": "max x\u2208C |g w i ,b i (x) \u2212 f i (x)| < / \u221a l,", "formula_coordinates": [47.0, 238.1, 701.25, 135.81, 23.56]}, {"formula_id": "formula_190", "formula_text": "D XY in D \u00b5,b", "formula_coordinates": [52.0, 108.0, 214.36, 54.32, 12.4]}, {"formula_id": "formula_191", "formula_text": "E S\u223cD n X I Y I R in D (A(S)) \u2212 inf h\u2208H R in D (h) \u2264 cons (n), ES\u223cD", "formula_coordinates": [52.0, 199.4, 381.99, 203.53, 32.2]}, {"formula_id": "formula_192", "formula_text": "E", "formula_coordinates": [52.0, 214.05, 454.67, 6.64, 7.11]}], "doi": ""}