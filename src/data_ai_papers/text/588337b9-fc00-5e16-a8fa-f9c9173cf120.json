{"title": "EMERGENCE OF MAPS IN THE MEMORIES OF BLIND NAVIGATION AGENTS", "authors": "Erik Wijmans; Manolis Savva; Irfan Essa; Stefan Lee; Ari S Morcos; Dhruv Batra", "pub_date": "2023-01-30", "abstract": "Animal navigation research posits that organisms build and maintain internal spatial representations, or maps, of their environment. We ask if machines -specifically, artificial intelligence (AI) navigation agents -also build implicit (or 'mental') maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent's perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train 'blind' agents -with sensing limited to only egomotion and no other sensing of any kind -to perform PointGoal navigation ('go to \u2206x, \u2206y') via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments (\u223c95% success); (2) they utilize memory over long horizons (remembering \u223c1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.", "sections": [{"heading": "INTRODUCTION", "text": "Decades of research into intelligent animal navigation posits that organisms build and maintain internal spatial representations (or maps) 1 of their environment, that enables the organism to determine and follow task-appropriate paths (Tolman, 1948;O'keefe & Nadel, 1978;Epstein et al., 2017). Hamsters, wolves, chimpanzees, and bats leverage prior exploration to determine and follow shortcuts they may never have taken before (Chapuis & Scardigli, 1993;Peters, 1976;Menzel, 1973;Toledo et al., 2020;Harten et al., 2020). Even blind mole rats and animals rendered situationallyblind in dark environments demonstrate shortcut behaviors (Avni et al., 2008;Kimchi et al., 2004;Maaswinkel & Whishaw, 1999). Ants forage for food along meandering paths but take near-optimal return trips (M\u00fcller & Wehner, 1988), though there is some controversy about whether insects like ants and bees are capable of forming maps (Cruse & Wehner, 2011;Cheung et al., 2014).\nAnalogously, mapping and localization techniques have long played a central role in enabling nonbiological navigation agents (or robots) to exhibit intelligent behavior (Thrun et al., 2005; Institute, Published as a conference paper at ICLR 2023ICLR 1972Ayache & Faugeras, 1988;Smith et al., 1990). More recently, the machine learning community has produced a surprising phenomenon -neural-network models for navigation that curiously do not contain any explicit mapping modules but still achieve remarkably high performance Wijmans et al., 2020;Kadian et al., 2020;Chattopadhyay et al., 2021;Khandelwal et al., 2022;Partsey et al., 2022;Reed et al., 2022). For instance, Wijmans et al. (2020) showed that a simple 'pixels-to-actions' architecture (using a CNN and RNN) can navigate to a given point in a novel environment with near-perfect accuracy; Partsey et al. (2022) further generalized this result to more realistic sensors and actuators. Reed et al. (2022) showed a similar general purpose architecture (a transformer) can perform a wide variety of embodied tasks, including navigation. The mechanisms explaining this ability remain unknown. Understanding them is both of scientific and practical importance due to safety considerations involved with deploying such systems.\nIn this work, we investigate the following question -is mapping an emergent phenomenon? Specifically, do artificial intelligence (AI) agents learn to build internal spatial representations (or 'mental' maps) of their environment as a natural consequence of learning to navigate?\nThe specific task we study is PointGoal navigation (Anderson et al., 2018), where an AI agent is introduced into a new (unexplored) environment and tasked with navigating to a relative location -'go 5m north, 2m west relative to start' 2 . This is analogous to the direction and distance of foraging locations communicated by the waggle dance of honey bees (Von Frisch, 1967).\nUnlike animal navigation studies, experiments with AI agents allow us to precisely isolate mapping from alternative mechanisms proposed for animal navigation -the use of visual landmarks (Von Frisch, 1967), orientation by the arrangement of stars (Lockley, 1967), gradients of olfaction or other senses (Ioal\u00e8 et al., 1990). We achieve this isolation by judiciously designing the agent's perceptual system and the learning paradigm such that these alternative mechanisms are rendered implausible. Our agents are effectively 'blind'; they possess a minimal perceptual system capable of sensing only egomotion, i.e. change in the agent's location and orientation as the it moves -no vision, no audio, no olfactory, no haptic, no magnetic, or any other sensing of any kind. This perceptual system is deliberately impoverished to isolate the contribution of memory, and is inspired by blind mole rats, who perform localization via path integration and use the Earth's magnetic field as a compass (Kimchi et al., 2004). Further still, our agents are composed of navigation-agnostic, generic, and ubiquitous architectural components (fully-connected layers and LSTM-based recurrent neural networks), and our experimental setup provides no inductive bias towards mapping -no map-like or spatial structural components in the agent, no mapping supervision, no auxiliary tasks, nothing other than a reward for making progress towards a goal. Surprisingly, even under these deliberately harsh conditions, we find the emergence of map-like spatial representations in the agent's non-spatial unstructured memory, enabling it to not only successfully navigate to the goal but also exhibit intelligent behavior (like taking shortcuts, following walls, detecting collisions) similar to aforementioned animal studies, and predict free-space in the environment. Essentially, we demonstrate an 'existence proof' or an ontogenetic developmental account for the emergence of mapping without any previous predisposition. Our results also explain the aforementioned surprising finding in recent literature -that ostensibly map-free neural-network achieve strong autonomous navigation performance -by demonstrating that these 'map-free' systems in fact learn to construct and maintain map-like representations of their environment.\nConcretely, we ask and answer following questions:\n1) Is it possible to effectively navigate with just egomotion sensing? Yes. We find that our 'blind' agents are highly effective in navigating new environments -reaching the goal with 95.1%\u00b11.3% success rate. And they traverse moderately efficient (though far from optimal) paths, reaching 62.9%\u00b11.6% of optimal path efficiency. We stress that these are novel testing environments, the agent has not memorized paths within a training environment but has learned efficient navigation strategies that generalize to novel environments, such as emergent wall-following behavior. 2) What mechanism explains this strong performance by 'blind' agents? Memory. We find that memoryless agents completely fail at this task, achieving nearly 0% success. More importantly, we find that agents with memory utilize information stored over a long temporal and spatial horizon and that collision-detection neurons emerge within this memory. Navigation performance as a function of the number of past actions/observations encoded in the agent's memory does not saturate till one thousand steps (corresponding to the agent traversing 89.1\u00b10.66 meters), suggesting that the agent 'remembers' a long history of the episode. 3) What information does the memory encode about the environment? Implicit maps. We perform an AI rendition of Menzel (1973)'s experiments, where a chimpanzee is carried by a human and shown the location of food hidden in the environment. When the animal is set free to collect the food, it does not retrace the demonstrator's steps but takes shortcuts to collect the food faster. Analogously, we train a blind agent to navigate from a source location (S) to a target location (T). After it has finished navigating, we transplant its constructed episodic memory into a second 'probe'-agent (which is also blind). We find that this implanted-memory probe-agent performs dramatically better in navigating from S to T (and T to S) than it would without the memory transplant. Similar to the chimpanzee, the probe agent takes shortcuts, typically cutting out backtracks or excursions that the memory-creator had undertaken as it tried to work its way around the obstacles. These experiments provide compelling evidence that blind agents learn to build and use implicit map-like representations of their environment solely through learning to navigate. Intriguingly further still, we find that surprisingly detailed metric occupancy maps of the environment (indicating free-space) can be explicitly decoded from the agent's memory. 4) Are maps task-dependent? Yes. We find that the emergent maps are a function of the navigation goal. Agents 'forget' excursions and detours, i.e. their episodic memory only preserves the features of the environment relevant to navigating to their goal. This, in part, explains why transplanting episodic memory from one agent to another leads it to take shortcuts -because the excursion and detours are simply forgotten.\nOverall, our experiments and analyses demonstrate that 'blind' agents solve PointGoalNav by combining information over long time horizons to build detailed maps of their environment, solely through the learning signals imposed by goal-driven navigation. In biological systems, convergent evolution of analogous structures that cannot be attributed to a common ancestor (e.g. eyes in vertebrates and jellyfish (Kozmik et al., 2008)) is often an indicator that the structure is a natural response to the ecological niche and selection pressures. Analogously, our results suggest that mapping may be a natural solution to the problem of navigation by intelligent embodied agents, whether they be biological or artificial. We now describe our findings for each question in detail.", "publication_ref": ["b60", "b44", "b9", "b46", "b41", "b59", "b18", "b1", "b30", "b40", "b42", "b12", "b11", "b58", "b2", "b52", "b29", "b45", "b48", "b45", "b48", "b0", "b63", "b63", "b37", "b24", "b30", "b41", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "BLIND AGENTS ARE EFFECTIVE NAVIGATORS", "text": "We train navigation agents for PointGoalNav in virtualized 3D replicas of real houses utilizing the AI Habitat simulator Szot et al., 2021) and Gibson (Xia et al., 2018) and Matterport3D (Chang et al., 2017) datasets. The agent is physically embodied as an cylinder with a diameter 0.2m and height 1.5m. In each episode, the agent is randomly initialized in the environment, which establishes an episodic agent-centric coordinate system. The goal location is specified in cartesian coordinates (x g , y g , z g ) in this system. The agent has four actionsmove forward (0.25 meters), turn left (10 \u2022 ), turn right (10 \u2022 ), and stop (to signal reaching the goal), and allowed a maximum of 2,000 steps to reach the specified goal. It is equipped with an egomotion sensor providing it relative position (\u2206x, \u2206y, \u2206z) and relative 'heading' (or yaw angle) \u2206\u03b8 between successive steps, which is integrated to keep track of the agent's location and heading relative to start [x t , y t , z t , \u03b8 t ]. This is sometimes referred to as a 'GPS+Compass' sensor in this literature Wijmans et al., 2020).\nWe use two task-performance dependent metrics: i) Success, defined as whether or not the agent predicted the stop action within 0.2 meters of the target, and ii) Success weighted by inverse Path Length (SPL) (Anderson et al., 2018), defined as success weighted by the efficiency of agent's path compared to the oracle path (the shortest path). Given the high success rates we observe, SPL can be roughly interpreted as efficiency of the path taken compared to the oracle pathe.g. an SPL of 95% means the agent took a path 95% as efficient as the oracle path while an SPL of 50% means the agent took a path 50% as efficient. Note that performance is evaluated in previously unseen environments to evaluate whether agents can generalize, not just memorize.\nThe agent's policy is instantiated as a long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) recurrent neural network -formally, given current observations o t = [x g , y g , z g , x t , y t , z t , \u03b8 t ], (h t , c t ) = LSTM(o t , (h t\u22121 , c t\u22121 )). We refer to this (h t , c t ) as the agent's internal memory representation. Note that only contains information gathered during the current navigation episode. We train our agents for this task using a reinforcement learning (Sutton & Barto, 1992) algorithm called DD-PPO (Wijmans et al., 2020). The reward has a term for making progress towards the goal and An agent is initialized in a novel environment (bluesquare) and task with navigation to a point specified relative to the start location (red square). We study 'blind' agents, equipped with just an egomotion sensor (called GPS+Compass in this literature).\n(B) 'Blind' agent vs. bug. Our learned 'blind' agent compared to 2 variants and an oracle equipped variant of the Bug algorithm (Lumelsky & Stepanov, 1987). The Bug algorithm initially orients itself towards the goal and then proceeds towards the goal. Upon hitting a wall, it follows along the wall until it reaches the other side. The oracle version is told whether wall-following left or right is optimal, providing an upper-bound on Bug algorithm performance. (C) t-SNE of the agent's internal representation for collisions. We find 4 overall clusters corresponding to the previous action taken and whether or not that action led to a collision.   1: PointGoalNav performance agents on PointGoalNav. We find that blind agents are surprisingly effective (success) though not efficient (SPL) navigators. They have similar success as an agent equipped with a Depth camera and higher SPL than a clairvoyant version of the 'Bug' algorithm. Surprisingly, we find that agents trained under this impoverished sensing regime are able to navigate with near-perfect efficacy -reaching the goal with 95.1%\u00b11.3% success rate (Table 1), even in situations where the agent must take hundreds of actions and traverse over 25m. This performance is similar in success rate (95.1 vs 94.0) 3 to a sighted agent (equipped with a depth camera) trained on a larger dataset (HM3D) (Ramakrishnan et al., 2021). The paths taken by the blind agent are moderately efficient but (as one might expect) far less so than a sighted agent (62.9 vs 83.0 SPL).\nAt this point, it might be tempting to believe that this is an easy navigation problem, but we urge the reader to fight hindsight bias. We contend that the SPL of this blind agent is surprisingly high given the impoverished sensor suite. To put this SPL in context, we compare it with 'Bug algorithms' (Lumelsky & Stepanov, 1987), which are motion planning algorithms inspired by insect navigation, involving an agent equipped with only a localization sensor. In these algorithms, the agent first orients itself towards the goal and then travels directly towards it until it encounters a wall, in which case it follows along the wall along one of two directions of travel. The primary challenge for Bug algorithms is determining whether to go left or right upon reaching a wall. To provide an upper bound on performance, we implement a 'clairvoyant' Bug algorithm agent with an oracle that tells it whether left or right is optimal. Even with the additional privileged information, the 'clairvoyant' Bug agent achieves an SPL of 46%, which is considerably less efficient than the 'blind' agent. Fig. 1b shows an example of the path our blind agent takes compared to 3 variants of the Bug algorithm. This shows that blind navigation agents trained with reinforcement learning are highly efficient at navigating in previously unseen environments given their sensor suite.", "publication_ref": ["b67", "b8", "b0", "b20", "b56", "b39", "b47", "b39"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "EMERGENCE OF WALL-FOLLOWING BEHAVIOR AND COLLISION-DETECTION NEURONS", "text": "Fig. 1b shows the blind agent exhibiting wall-following behavior (also see blue paths in Fig. A6 and videos in supplement). This behavior is remarkably consistent; the agent spends the majority of an episode near a wall. This is surprising because it is trained to navigate to the target location as quickly as possible, thus, it would be rewarded for traveling in straighter paths (that avoid walls). We hypothesize that this strategy emerges due to two factors. 1) The agent is blind, it has no way to determine where the obstacles are in the environment besides 'bumping' into them. 2) The environment is unknown to the agent. While this is clearly true for testing environments it is also functionally true for training environments because the coordinate system is episodic, every episode uses a randomly-instantiated coordinate system based on how the agent was spawned; and the since the agent is blind, it cannot perform visual localization.\nWe test both hypotheses. To test (2), we provide an experiment in Apx. C.1 showing that when the agent is trained in a single environment with a consistent global coordinate system, it learns to memorize the shortest paths in this environment and wall-following does not emerge. Consequently, this agent is unable to navigate in new environment, achieving 100% success on train and 0% on test.\nTo test (1), we analyze whether the agent is capable of detecting collisions. Note that the agent is not equipped with a collision sensor. In principle, the agent can infer whether it collided -if tries to move forward and the resulting egomotion is atypical, then it is likely that a collision happened. This leads us to askdoes the agent's memory contain information about collisions? We train a linear classifier that uses the (frozen) internal representation (h t+1 , c t+1 ) to predict if action a t resulted in a collision (details in Apx. A.5). The classifier achieves 98% accuracy on held-out data. As comparison, random guessing on this 2-class problem would achieve 50%. This shows the agent's memory not only predicts its collisions, but also that collision-vs-not are linearly separable in internal-representation space, which strongly suggests that the agent has learned a collision sensor.\nNext, we examine how collisions are structured in the agent's internal representation by identifying the subspace that is used for collisions. Specifically, we re-train the linear classifier with an 1weight penalty to encourage sparsity. We then select the top 10 neurons (from 3072) with the largest weight magnitude; this reduces dimensionality by 99.7% while still achieving 96% collision-vs-not accuracy. We use t-SNE (Van der Maaten & Hinton, 2008) and the techniques in Kobak & Berens (2019) to create a 2-dimension visualization of the resulting 10-dimension space. We find 4 distinct semantically-meaningful clusters (Fig. 1c). One cluster always fires for collisions, one for forward actions that did not result in a collision, and the other two correspond to turning actions. Notice that these exceedingly small number of dimensions and neurons essentially predict all collisions and movement of the agent. We include videos in the supplementary materials.  Next, we examine how memory is utilized by asking if the agent uses memory solely to remember short-term information (e.g. did it collide in the last step?) or whether it also includes long-range information (e.g. did it collide hundreds of steps ago?). To answer this question, we restrict the memory capacity of our agent. Specifically, let k denote the memory budget. At each time t, we take the previous k observations, [o t\u2212k+1 , . . . , o t ], and construct the internal representation", "publication_ref": ["b62", "b32"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "MEMORY IS USED OVER LONG HORIZONS", "text": "(h t , c t ) via the recurrence (h i , c i ) = LSTM(o i , (h i\u22121 , c i\u22121 )) for t \u2212 k < i \u2264 t where (h t\u2212k , c t\u2212k ) = (0, 0).\nIf the agent is only leveraging its memory for short-term storage we would expect performance to saturate at a small value of k. Instead, Fig. 2 shows that the agent leverages its memory for significantly long term storage. When memoryless (k = 1), the agent completely fail at the task, achieving nearly 0% success. Navigation performance as a function of the memory budget (k) does not saturate till one thousand steps. Recall that the agent can move forward 0.25 meters or turn 10 \u2022 at each step. The average distance traveled in 1000 steps is 89.1\u00b10.66 meters, indicating that it remembers information over long temporal and spatial horizons. In Apx. C.6 we train agents to operate at a specific memory budget. We find that a budget of k = 256, the largest we are able to train, is not sufficient to achieve the performance of unbounded.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Agent Network", "text": "Probe Network  First, an agent navigates (blue path, blue LSTM) from start (green sphere) to target (red sphere). After the agent navigates, we task a probe (purple LSTM) with performing the same navigation episode with the additional information encapsulated in the agent's internal representation (or memory), h A T . The probe is able to navigate more efficiently by taking shortcuts (purple path). As denoted by the dashed line between the probe and agent networks, the probe does not influence what the agent stores in its internal representation. Environment in the image from the Replica Dataset . (B) Agent memory transplant increases probe efficiency (SPL). Results of our trained probe agent under three configurations -initialized with an empty representation (AllZeroMemory), a representation of a random agent walked along the trained agent's path (UntrainedAgentMemory), and the final representation of the trained agent (TrainedAgentMemory). 95% confidence interval reported over 5 agent-probe pairs.\nLSTM LSTM o A T -1 LSTM LSTM o A T h A T -2 a A T -2 a A T -1 a A T h A T h P 2 o P 1 a P 1 a P 2 o P 2 S T Stop Gradient (A)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MEMORY ENABLES SHORTCUTS", "text": "To investigate what information is encoded in the memory of our blind agents, we develop an experimental paradigm based on 'probe' agents. A probe is a secondary navigation agent 4 that is structurally identical to the original (sensing, architecture, etc.), but parametrically augmented with the primary agent's constructed episodic memory representation (h T , c T ). The probe has no influence on the agent, i.e. no gradients (or rewards) follow from probe to agent (please see training details in Apx. A.2). We use this paradigm to examine whether the agent's final internal representation contains sufficient information for taking shortcuts in the environment.\nAs illustrated in Fig. 3A, the agent first navigates from source (S) to target (T). After the agent reaches T, a probe is initialized 5 at S, its memory initialized with the agent's final memory representation, i.e. (h 0 , c 0 ) probe = (h T , c T ) agent , and tasked with navigating to T. We refer to this probe task as SecondNav(S\u2192T). All evaluations are conducted in environments not used for training the agent nor the probe. Thus, any environmental information in the agent's memory must have been gathered during its trajectory (and not during any past exposure during learning). Similarly, all initial knowledge the probe has of the environment must come from the agent's memory (h T , c T ) agent .\nOur hypothesis is that the agent's memory contains a spatial representation of the environment, which the probe can leverage. If the hypothesis is true, we would expect the probe to navigate Sec-ondNav(S\u2192T) more efficiently than the agent (e.g. by taking shortcuts and cutting out exploratory excursions taken by the agent). If not, we would expect the probe to perform on-par with the agent since the probe is being trained on essentially the same task as the agent 6 . In our experiments, we find that the probe is significantly more efficient than the agent -SPL of 62.9%\u00b11.6% (agent) vs. 85.0%\u00b11.6% (probe). It is worth stressing how remarkable the performance of the probe is -in a new environment, a blind probe navigating without a map traverses a path that is within 15% of the shortest path on the map. The best known sighted agents (equipped with an RGB camera, Depth sensor, and egomotion sensor) achieve an SPL of 84% on this task (Ramakrishnan et al., 2021). Essentially, the memories of a blind agent are as valuable as having vision! Fig. 3A shows the difference in paths between the agent and probe (and videos showing more examples are available in the supplement). While the agent exhibits wall-following behavior, the probe TrainedAgentMemory has a higher mean than UntrainedAgentMemory with p-value \u2264 10 \u22125 (via Wilcoxon signed-rank test (Wilcoxon, 1992)). (Right) Example ground truth and predicted occupancy maps using TrainedAgentMemory (corresponding to (A) and (B) IoU points). Light grey is non-navigable and dark grey is navigable. The agent path is drawn in light blue and navigates from start (green) to target (red). We can see that when the agent travels close to one wall, the map decoder predicts another wall parallel to it, indicating a corridor. instead takes more direct paths and rarely performs wall following. Recall that the only difference in the agent and probe is the contents of the initial hidden state -reward is identical (and available only during training), training environments are identical (although the episodes are different), and evaluation episodes are identical -meaning that the environmental representation in the agent's episodic memory is what enables the probe to navigate more efficiently.\nWe further compare this result (which we denote as TrainedAgentMemory) with two control groups: 1) AllZeroMemory: An empty (all zeros) episodic memory to test for any systematic biases in the probe tasks. This probe contains identical information at the start of an episode as the agent (i.e. no information). 2) UntrainedAgentMemory: Episodic memory generated by an untrained agent (i.e. with a random setting of neural network parameters) as it is walked along the trajectory of the trained agent. This disentangles the agent's structure from its parameters; and tests whether simply being encoded by an LSTM (even one with random parameters) provides an inductive bias towards building good environmental representations (Wieting & Kiela, 2019).\nWe find no evidence for this inductive bias -UntrainedAgentMemory performs no better than AllZeroMemory (Fig. 3B, row 1 vs. 2). Furthermore, TrainedAgentMemory significantly outperforms both controls by +13 points SPL and +4 points Success (Fig. 3B, row 3 vs. 1 and 2). Taken together, these two results indicate that the ability to construct useful spatial representations of the environment from a trajectory is decidedly a learned behavior.\nNext, we examine if there is any directional preference in the episodic memory constructed by the agent. Our claim is that even though the agent navigates from S to T, if its memory indeed contains map-like spatial representations, it should also support probes for the reverse task Second-Nav(T\u2192S). Indeed, we find that TrainedAgentMemory probe performs the same (within margin of error) on both SecondNav(S\u2192T) and SecondNav(T\u2192S) (Fig. 3B right column) -indicating that the memory is equally useful in both directions. In Apx. C.2 we demonstrate that the probe removes excursions from the agent's path and takes shortcuts through previously unseen parts of the environment. Overall, these results provide compelling evidence that blind agents learn to build and use implicit map-like representations that enable shortcuts and reasoning about previously untraversed locations in the environment, solely through learning to navigate between two points.", "publication_ref": ["b47", "b66", "b64"], "figure_ref": ["fig_3", "fig_3", "fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "LEARNING NAVIGATION IMPROVES METRIC MAP DECODING", "text": "Next, we tackle the question 'Does the agent build episodic representations capable of decoding metric maps (occupancy grids) of the environment?'. Formally, given the final representation (h T , c T ) agent , we train a separate decoding network to predict an allocentric top-down occupancy grid (free-space vs not) of the environment. As with the probes, no gradients are propagated from the decoder to the agent's internal representation. We constrain the network to make predictions for a location only if the agent reached within 2.5 meters of it (refer to Apx. A.3 for details). Note that since the agents are 'blind' predictions about any unvisited location require reasoning about unseen Published as a conference paper at ICLR 2023 Qualitative example of the previously-visited location decoder making systematic errors when decoding an excursion. Blue represents the confidence of the decoder that the agent was previously at a given location; we can see that it is lower in the path interval marked in red (excursion) than the rest. (B) Remembrance of excursions. Performance of decoders when predicting previous agent locations broken down into three categories. 'Nonexcursion' is all predictions where the current location of the agent and the prediction time step are not part of an excursions. 'Excursion' is when the prediction time step is part of an excursion. 'Exit' is when the prediction time step is part of the last 10% of the excursion. X-axis is the distance into the past and Y-axis is the relative error between the true and predicted locations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Non-Excursion Excursion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predicted", "text": "space. As before, we compare the internal representation produced by TrainedAgentMemory to internal representation produced by an agent with random parameters, UntrainedAgentMemory.\nFig. 4 shows the distribution of map-prediction accuracy, measured as interaction-over-union (IoU) with the true occupancy grid. We find that TrainedAgentMemory enables uniformly more accurate predictions than UntrainedAgentMemory-32.5% vs 12.5% average IoU. The qualitative examples show that the predictor is commonly able to make accurate predictions about unvisited locations, e.g. when the agent travels close to one wall, the decoder predicts another parallel to it, indicating a corridor. These results show that the internal representation contains necessary information to decode accurate occupancy maps, even for unseen locations. We note that the environment structural priors are also necessary to prediction unseen locations. Thus agent memory is necessary but not sufficient.\nIn Apx. C.4, we conduct this analysis on 'sighted' navigation agents (equipped with a Depth camera and egomotion sensor). Perhaps counter-intuitively, we do not find conclusive evidence that metric maps can be decoded from the memory of sighted agents (despite their sensing suite being a strict superset of blind agents). Our conjecture is that for higher-level strategies like map-building to emerge, the learning problem must not admit 'trivial' solutions such as the ones deep reinforcement learning is know to latch onto (Baker et al., 2020;Lehman et al., 2020;Kadian et al., 2020). We believe that the minimal perception system used in our work served to create a challenging learning problem, which in turn limited the possible 'trivial' solutions, thus inducing map-building.", "publication_ref": ["b34"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "MAPPING IS TASK-DEPENDENT: AGENT FORGETS EXCURSIONS", "text": "Given that the agent is memory-limited, it stands to reason that it might need to choose what information to preserve and what to 'forget'. To examine this, we attempt to decode the agent's past positions from its memory. Formally, given internal state at time t, (h t , c t ), we train a prediction network f k (\u2022) to predict the agent's location k steps in to the past, i.e.\n\u015d t\u2212k = f k (h t , c t ) + s t , k \u2208 [1, 256].\nGiven ground truth location s t+k , we evaluate the decoder via relative L2 error ||\u015d t+k \u2212s t+k ||/||s t+k \u2212s t || (refer to Apx. A.4 for details). Qualitative analysis of past prediction results shows that the agent forgets excursions 7 , i.e. excursions are harder to decode (see Fig. 5a). To quantify this, we manually labelled excursions in 216 randomly sampled episodes in evaluation environments. Fig. 5b shows that excursions are harder to decode than non-excursions, indicating that the agent does indeed forget excursions. Interestingly, we find that the exit of the excursion is considerably easier to decode, indicating that the end of the excursion performs a similar function to landmarks in animal and human navigation (Chan et al., 2012).\nIn the appendix, we study several additional questions that could not be accommodated in the main paper. In Apx. C.2 we further examine the probe's performance. In Apx. C.3 we examine predicting future agent locations. In Apx. C.5 we use agent's hidden state as a world model.", "publication_ref": ["b7"], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "RELATED WORK", "text": "Characterizing spatial representations. Prior work has shown that LSTMs build gridcell (O'keefe & Nadel, 1978) representations of an environment when trained directly for path integration within that environment (Banino et al., 2018;Cueva & Wei, 2018;Sorscher et al., 2020).\nIn contrast, our work provides no direct supervision for path integration, localization, or mapping. Banino et al. (2018) demonstrated that these maps aid in navigation by training a navigation agent that utilizes this cognitive map. In contrast, we show that LSTMs trained for navigation learn to build spatial representations in novel environments. Whether or not LSTMs trained under this setting also utilize grid-cells is a question for future work. Bruce et al. (2018) demonstrated that LSTMs learn localization when trained for navigation in a single environment. We show that they learn mapping when given location and trained in many environments. Huynh et al. (2020) proposed a spatial memory architecture and demonstrated that a spatial representation emerges when trained on a localization task. We show that spatial representations emerge in non-spatial neural networks trained for navigation. Dwivedi et al. (2022) examined what navigation agents learn about their environments. We provided a detailed account of emergent mapping in larger environments, over longer time horizons, and show the emergence of intelligent behavior and mapping in blind agents, which is not the focus of prior work.\n'Map-free' navigation agents. Learned agents that navigate without an explicit mapping module (called 'map-free' or 'pixels-to-actions') have shown strong performance on a variety of tasks Wijmans et al., 2020;Kadian et al., 2020;Chattopadhyay et al., 2021;Khandelwal et al., 2022;Partsey et al., 2022;Reed et al., 2022). In this work, we do not provide any novel techniques nor make any experimental advancement in the efficacy of such (sighted) agents. However, we make two key findings. First, that blind agents are highly effective navigators for PointGoalNav, exhibiting similar efficacy as sighted agents. Second, we begin to explain how 'map-free' navigation agents perform their task: they build implicit maps in their memory, although the story is a bit nuanced due to the results in Apx. C.4; we suspect this understanding might be extended in future work.", "publication_ref": ["b44", "b4", "b13", "b4", "b6", "b22", "b14", "b29", "b45", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "OUTLOOK: LIMITATIONS, REPRODUCIBILITY", "text": "In this work, we have shown that 'blind' AI navigation agents -agents with similar perception as blind mole rats -are capable of performing goal-driven navigation to a high degree of performance. We then showed that these AI navigation agents learn to build map-like representations (supporting the ability to take shortcuts, follow walls, and predict free-space and collisions) of their environment solely through learning goal-driven navigation. Our agents and training regime have no added inductive bias towards map-building, be it explicit or implicit, implying that cognitive maps may be a natural solution to the inductive biases imposed by navigation by intelligent embodied agents, whether they be biological or artificial. In a similar manner, convergent evolution (Kozmik et al., 2008), where two unrelated intelligent systems independently arrive at similar mechanisms, suggests that the mechanism is a natural response of having to adapt to the environment and the task.\nOur results also provide an explanation of the surprising success of map-free neural network navigation agents by showing that these agents in fact learn to build map-like internal representations with no learning signal other than goal driven navigation. This result establish a link between how 'map-free' systems navigate with analytic mapping-and-planning techniques (Thrun et al., 2005;Institute, 1972;Ayache & Faugeras, 1988;Smith et al., 1990).\nOur results and analyses also point towards future directions in AI navigation research. Specifically, imbuing AI navigation agents with explicit (e.g. architectural design) or implicit (e.g. training regime or auxiliary objectives) priors that bias agents towards learning an internal representation with the features found here may improve their performance. Further, it may better equip them to learn more challenging tasks such as rearrangement of an environment by moving objects (Batra et al., 2020).\nWe see several limitations and areas for future work. First, we examined ground-based navigation agents operating in digitizations of real houses. This limits the agent a 2D manifold and induces strong structural priors on environment layout. As such, it is unclear how our results generalize to a drone flying through a large forest. Second, we examined agents with a minimal perceptual system. In the supplementary text, we attempted to decode occupancy grids (metric maps) from Depth sensor equipped agents and did not find convincing evidence. Our conjecture is that for higher-level strategies like map-building to emerge, the learning problem must not admit 'trivial' solutions. We believe that the minimal perception system used in our work also served to create such a challenging learning problem. Third, our experiments do not study the effects of actuation noise, which is an important consideration in both robot navigation systems and path integration in biological systems. Fourth, we examine an implicit map-building mechanism (an LSTM), a similar set of experiments could be performed for agents with a differentiable read/write map but no direct mapping supervision. Fifth, our agents only explore their environment for a short period of time (an episode) before their memory is reset. Animals and robots at deployment experience their environment for significantly longer periods of time. Finally, we do not provide a complete mechanistic account for how the agent learns to build its map or what else it stores in its memory.", "publication_ref": ["b33", "b58", "b23", "b2", "b52", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "A METHODS AND MATERIALS", "text": "A.1 POINTGOAL NAVIGATION TRAINING Task. In PointGoal Navigation, the agent is tasked with navigating to a point specified relative to its initial location, i.e an input of (\u03b4x, \u03b4y) corresponds to going \u03b4x meters forward and \u03b4y meters to the right. The agent succeeds if it predicts the stop action within 0.2 meters of the specified point. The agent has access to 4 low-level actionsmove forward (0.25 meters), turn left (10 \u2022 ), turn right (10 \u2022 ), and stop. There is no noise in the agent's actuations.\nSensors. The agent has access to solely an idealized GPS+Compass sensor that provides it heading and position relative to the starting orientation and location at each time step. There is no noise in the agent's sensors.\nArchitecture. The agent is parameterized by a 3-layer LSTM (Hochreiter & Schmidhuber, 1997) with a 512-d hidden dimension. At each time-step, the agent receives observations g (the location of the goal relative to start), GPS (its current position relative to start), and compass (its current heading relative to start). We also explicitly give the agent an indicator of if it is close to goal in the form of min(||g \u2212 GP S||, 0.5) as we find the agent does not learn robust stopping logic otherwise. All 4 inputs are projected to 32-d using separated fully-connected layers. These are then concatenated with a learned 32-d embedding of the previous action taken to form a 160-d input that is then given to the LSTM. The output of the LSTM is then processed by a fully-connected layer to produce a softmax distribution of the action space and an estimate of the value function.\nTraining Data. We construct our training data based on the Gibson (Xia et al., 2018) and Matter-port3D dataset (Chang et al., 2017). We training on 411 scenes from Gibson and 72 from Matter-port3D.\nTraining Procedure. We train our agents using Proximal Policy Optimization (PPO) (Schulman et al., 2017) with Generalized Advantage Estimation (GAE) (Schulman et al., 2016). We use Decentralized Distributed PPO (DD-PPO) (Wijmans et al., 2020) to train on 16 GPUs. Each GPU/worker collects 256 steps of experience from 16 agents (each in different scenes) and then performs 2 epochs of PPO with 2 mini-batchs per epoch. We use the Adam optimize (Kingma & Ba, 2015) with a learning rate of 2.5 \u00d7 10 \u22124 . We set the discount factor \u03b3 to 0.99, the PPO clip to 0.2, and the GAE hyper-parameter \u03c4 to 0.95. We train until convergence (around 2 billion steps of experience).\nAt every timestep, t, the agent is in state s t and takes action a t , and transitions to state s t+ . It receives shaped reward in the form:\nr t = 2.5 \u2022 Success if a t is Stop \u2212\u2206 geo dist (s t , s t+1 ) \u2212 \u03bb Otherwise (1)\nwhere \u2206 geo dist (s t , s t+1 ) is the change in geodesic (shortest path) distance to goal between s t and s t+1 and \u03bb=0.001 is a slack penalty encouraging shorter episodes. Evaluation Procedure. We evaluate the agent in the 18 scenes from the Matterport3D test set. We use the episodes from Savva et al. , which consist of 56 episodes per scene (1008 in total). Episode range in distance from 1.2 to 30 meters. The ratio of geodesic distance to euclidean distance between start and goal is restricted to be greater than or equal to 1.1, ensuring that episodes are not simple straight lines. Note that reward is not available during evaluation.\nThe agent is evaluated under two metrics, Success, whether or not the agent called the stop action with 0.2 meters of the goal and Success weighted by normalized inverse Path Length (SPL) (Anderson et al., 2018). SPL is calculated as follows: given the agent's path [s 1 , . . . , s T ] and the initial geodesic distance to goal d i for episode i, we first compute the length of the agent's path\nl i = T t=2 ||s t \u2212 s t\u22121 || 2 (2)\nthen SPL for episode i as\nSPL i = Success i \u2022 d i min{d i , l i } (3)\nWe then report SPL as the average of SPL i across all episodes.", "publication_ref": ["b20", "b67", "b8", "b51", "b50", "b31", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 PROBE TRAINING", "text": "Task. The probe task is to either navigate from start to goal again (SecondNav(S\u2192T)) or navigate from goal to start (SecondNav(T\u2192S)). For SecondNav(S\u2192T), the probe is initialized at the starting location but with the agent's final heading. For SecondNav(T\u2192S), the probe is initialized with the agent's final heading and position. In both cases, the probe and the agent share the same coordinate systemi.e. in SecondNav(T\u2192S), the initial GPS and Compass readings for the probe are identical the the final GPS and Compass readings for the agent. When the agent does not successfully reach the goal, the probe task is necessarily undefined and we do not instantiate a probe. Note that no gradients (or rewards) follow from probe to agent. From the agent's perspective, the probe does not exist. From the probe's perspective, the agent provides a dataset of initial locations (or goals) and initial hidden states.\nEvaluation Procedure. We evaluate the probe in a similar manner the agent except that any episode which the agent is unable to complete (5%) is removed due to the probe task being undefined if the agent is unable to complete the task. The agent reaches the goal 95% of the time, thus only 50 out of 1008 possible probe evaluation episodes are invalidated. The control probe type accounts for this. We ignore the agent's trajectory when computing SPL for the probe.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 OCCUPANCY MAP DECODING", "text": "Task. We train a decoding network to predict the top-down occupancy map of the environment from the final internal state of the agent (h t , c t ). We limit the decoder to only predict within 2.5 meters of any location the agent visited.\nArchitecture. The map-decoder is constructed as follows: First the internal state (h t , c t ) is concatenated into a 512\u00d76-d vector. The vector is then passed to a 2-layer MLP with a hidden dimension of 512-d that produces a 4608-d vector. This 4608-d vector is then reshaped into a [128,6,6] featuremap. The feature map is processed by a series of Coordinate Convolution (CoordConv) (Liu et al., 2018) Coordinate Up-Convolution (CoordUpConv) layers decrease the channel-depth and increase spatial resolution to [16,96,96]. Specifically, after an initial CoordConv with an output channeldepth of 128, we use a series of 4 CoordUpConv-CoordConv layers where each CoordUpConv doubles the spatial dimensions (quadruples spatial resolution) and each CoordConv reduces channel-depth by half. We then use a final 1x1-Convolution to create a [2,96,96] tensor representing the nonnormalized log-probabilities of whether or not an given location is navigable or not.\nEach CoordConv has kernel size 3, padding 1, and stride 1. CoordUpConv has kernel size 3, padding 0, and stride 2. Before all CoordConv and CoordUpConv, we use 2D Dropout (Srivastava et al., 2014;Tompson et al., 2015) with a zero-out probability of 0.05. We use Batch Normalization layers (Ioffe & Szegedy, 2015) and the ReLU activation function (Nair & Hinton, 2010) after all layers except the terminal layer.\nTraining Data. We construct our training data by having a trained agent perform episodes of Point-Goal navigation on the training dataset. Note that while evaluation is done utilizing the final hidden state, we construct our training dataset by taking 30 time steps (evenly spaced) from the trajectory and ensuring the final step is included.\nTraining Procedure. We train on 8 GPUs with a batch size of 128 per GPU (total batch size of 1024). We use the AdamW optimizer (Kingma & Ba, 2015;Loshchilov & Hutter, 2019) with an initial learning rate of 10 \u22123 and linearly scale the learning rate to 1.6 \u00d7 10 \u22122 over the first 5 epochs  and use a weight-decay of 10 \u22125 . We use the validation dataset to perform early-stopping. We use Focal Loss (Lin et al., 2017) (a weighted version of Cross Entropy Loss) with \u03b3 = 2.0, \u03b1 NotNavigable = 0.75, and \u03b1 Navigable = 0.25 to handle the class imbalance.\nEvaluation Data and Procedure. We construct our evaluation data using the validation dataset.\nNote that the scenes in evaluation are novel to both the agent and the decoder. We evaluate the predicted occupancy map from the final hidden state/final time step. We collect a total of 5,000 episodes.", "publication_ref": ["b36", "b54", "b61", "b25", "b43", "b31", "b38", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "A.4 PAST AND FUTURE POSITION PREDICTION", "text": "Task. We train a decoder to predict the change in agent location given the internal state at time t (h t , c t ). Specifically, let s t be the agent's position at time t where the coordinate system is defined by the agent's starting location (i.e. s 0 = 0), and s t+k be its position k steps into the future/past, then the decoder is trained to model f ((h t , c t )) = s t+k \u2212 s t .\nArchitecture. The decoder is a 3-layer MLP that produces a 3 dimensional output with hidden sizes of 256 and 128. We use Batch Normalization (Ioffe & Szegedy, 2015) and the ReLU activation function (Nair & Hinton, 2010) after all layers except the last.\nTraining Data. The training data is collected from executing a trained agent on episodes from the training set. For each episode, we collect all possible pairs of s t , s t+k for a given value of k.\nTraining Procedure. We use the AdamW optimizer (Kingma & Ba, 2015;Loshchilov & Hutter, 2019) with a learning rate of 10 \u22123 , a weight decay of 10 \u22124 , and a batch size of 256. We use a Smooth L1 Loss/Huber Loss (Huber, 1964) between the ground-truth change in position and the predicted change in position. We use the validation set to perform early stopping.\nEvaluation Procedure. We evaluate the trained decoded on held-out scenes. Note that the held-out scenes are novel both to the agent and the decoder.\nVisualization of Predictions. For visualization the predictions of past vitiation, we found it easier to train a second decoder that predicts all locations the agent visited previously on a 2D top down map given the internal state (h t , c t ). This decoder shares the exact same architecture and training procedure as the occupancy grid decoder. The decoder removes the temporal aspect from the prediction, so it is ill-suited for any time-dependent analysis, but produces clearer visualizations.\nExcursion Calibrated Analysis. To perform the excursions forgetting analysis, we use the excursion labeled episodes. We marked the end of the excursion as the last 10% of the steps that are part of the excursion. For a given point in time t, we classify that point into one of {Non-Excursion, Excursion, Exit}. We then examine how well this point is remembered by calculating the error of predicting the point t from t + k, i.e. how well can t be predicted when it is k steps into the past. When t is part of an excursions (both the excursion and the exit) we limit t + k to either be part of the same excursion or not part of an excursion. When t is not part of an excursion, t + k must also not be part of an excursion nor can there be any excursion in the range [t, t + k].\nA.5 COLLISION PREDICTION LINEAR PROBE Task. The task of this probe is to predict of the previous action taken lead to a collision given the current hidden state. Specifically it seeks to learn a function Collided t = f ((h t , c t )) where (h t , c t ) is the internal state at time t and Collided t is whether or not the previous action, a t\u22121 lead to a collision.\nArchitecture. The architecture is logistic classifier that takes the concatentation of the internal state and produces logprob of Collided t .\nTraining Data. We construct our training data by having a trained agent perform episodes of Point-Goal navigation on the training set. We collect a total of 10 million samples and then randomly select 1 million for training. We then normalize each dimension independently by computing mean and standard deviation and then subtract mean and divide by standard deviation. This ensures that all dimensions have the same average magnitude.\nTraining Procedure. We training on 1 GPU with a batch size of 256. We use the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 5 \u00d7 10 \u22124 . We train for 20 epochs.\nEvaluation Data and Procedure. We construct our evaluation data using the same procedure as the training data, but on the validation dataset and collect 200,00 samples (which is then subsampled to 20,000).\nImportant Dimension Selection. To select which dimensions are important for predicting collsions, we re-train our probe with various L1 penalties. We sweep from 0 to 1000 and then select the penalty that results in the lowest number of significant dimensions without substantially reducing accuracy.\nWe determine the number of significant dimensions by first ordering all dimensions by the L1 norm of the corresponding weight and then finding the smallest number of dimensions we can keep while maintaining 99% of the performance of keeping all dimensions for that classifier.\nThe t-SNE manifold is computed using 20,000 samples. This is then randomly subsampled to 1,500 for visualization.\nA.6 DATA AND MATERIALS AVAILABILITY The Gibson (Xia et al., 2018) and Matterport3D (Chang et al., 2017) datasets can be acquired from their respective distributors. Habitat  is open source. Code to reproduce experiments will be made available.", "publication_ref": ["b25", "b43", "b31", "b38", "b21", "b31", "b67", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "B ADDITIONAL DISCUSSIONS B.1 RELATIONSHIP TO COGNITIVE MAPS", "text": "Throughout the text, we use the term 'map' to mean a spatial representation that supports intelligent behaviors like taking shortcuts. Whether or not this term is distinct from the specific concept of a 'cognitive map' is debated.\nCognitive maps, as defined by O'keefe & Nadel (1978), imply a set of properties and are generally attached to a specific mechanism. The existence of a cognitive map requires that the agent be able to reach a desired goal in the environment from any starting location without being given that starting location, i.e. be able to navigate against a map. Further, cognitive maps refer to a specific mechanism -place cells and grid cells being present in the hippocampus. Other works have also studied 'cognitive maps' and not put such restrictions on its definition (Gallistel, 1990;Tolman, 1948), however these broader definitions have been debated (Jacobs, 2003).\nOur work shows that the spatial information contained within the agent's hidden state enables maplike properties -a secondary agent to take shortcuts through previously unexplored free space -and supports the decoding of a metric map. However, these do not fully cover the proprieties of O'keefe & Nadel (1978)'s definition nor do we make a mechanistic claim about how this information is stored in the neural network, though we do find the emergence of collision-detection neurons.", "publication_ref": ["b44", "b16", "b60", "b26", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "C ADDITIONAL EXPERIMENTS", "text": "C.1 BLIND SHORTEST PATH NAVIGATION WITH TRUE STATE In the main text, we posited that blind agents learn wall-following as this an effective strategy for blind navigation in unknown environments. We posit that this is because the agent does not have access to true state (it does not know the current environment nor where it is in global coordinates). In this experiment we show that blind agents learn to take shortest paths, as opposed to wall-following, when trained in a single environment (implicitly informing the agent of the current environment) and uses the global coordinate system. 8 We use an identical agent architecture and training procedure as outline for PointGoal navigation training in the Materials and Methods with two differences: 1) A single training and test environment and 2) usage of the global coordinates within the environment for both goal specific and the agent's GPS+Compass sensor. We perform this experiment on 3 scenes, 1 from the Gibson val dataset and 2 from Matterport3D val dataset. The average SPL during training is 99\u00b10.1 showing that the blind agent learns shortest path navigation not wall-following. Figure A6 shows examples of an agent trained in a single scene with global coordinates and an agent trained in many scenes with episodic coordinates.\nThese two settings, i) where the agent uses an episodic coordinate system and navigates in unknown environments, and ii) where the agent uses global coordinates and navigates in a known environment can be seen as the difference between a partially observable Markov decision process (POMDP) and a Markov decision process. In the POMDP case, the agent must learn a generalizable policy while it can overfit in the MDP case.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 FURTHER ANALYSIS OF THE PROBE'S PERFORMANCE", "text": "In the main text, we showed that the probe is indeed much more efficient than the agent, but how is this gain achieved? Our hypothesis is that the probe improves upon the agent's path by taking shortcuts and eliminating excursions (representing an 'out and back'). We define an excursion as a sub-path that approximately forms a loop. To quantify excursions, we manually annotate excursions in 216 randomly sampled episodes in evaluation environments. Of the labeled episodes, 62% have a least 1 excursion. On average, an episode has 0.95 excursions, and excursions have an average length of 101 steps (corresponding to 8.23 meters). Since excursions represent unnecessary portions of the trajectory, this indicates that the probe should be able improve upon the agent's path by removing these excursions.\nWe quantify this excursion removal via the normalized Chamfer distance between the agent's path and the probe's path. Formally, given the agent's path Agent=[s (agent) 1 , . . . , s (agent) T ] and the probe's path Probe=[s (probe) 1 , . . . , s (probe) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "N", "text": "] where s \u2208 R 3 is a point in the environment:\nPathDiff(Agent, Probe) = 1 N N i=1 min 1\u2264j\u2264T GeoDist(s (agent) i , s (probe) j ),(4)\nwhere GeoDist(\u2022, \u2022) indicates the geodesic distance (shortest traverseable path-length).\nNote that Chamfer distance is not symmetric. PathDiff(Probe, Agent) measures the average distance of a point on the probe path s (probe) j from the closest point on the agent path. A large PathDiff(Probe, Agent) indicates that the probe travels through novel parts of the environments (compared to the agent). Conversely, PathDiff(Agent, Probe) measures the average distance of a point on the agent path s (agent) i from the closest point on the probe path. A large PathDiff(Agent, Probe) \u2212 PathDiff(Probe, Agent) gap indicates that agent path contains excursions while the probe does not; thus, we refer to this gap as Excursion Removal. To visually understand why this is the case, consider the example agent and probe paths in Fig. A7. Point (C) lies on an excursion in the agent path. It contributes a term to PathDiff(Agent, Probe) but not to PathDiff(Probe, Agent) because (D) is closer to (E) than (C).\nOn both SecondNav(S\u2192T) and SecondNav(T\u2192S), we find that as the efficiency of a probe increases, Excursion Removal also increases (Table A2, row 1 vs. 2, 2 vs. 3), confirming that the TrainedAgentMemory probe is more efficient because it removes excursions.\nWe next consider if the TrainedAgentMemory probe also travels through previously unexplored space in addition to removing excursions. To quantify this, we report PathDiff(Probe, Agent) on episodes where agent SPL is less than average (less than 62.9%). 9 If probes take the same path as the agent, we would expect this metric to be zero. If, however, probes travel through previously unexplored space to minimize travel distance, we would expect this metric to be significantly nonzero. Indeed, on SecondNav(S\u2192T), we find the TrainedAgentMemory probe is 0.32 meters away on average from the closest point on the agent's path (99% empirical bootstrap of the mean gives a range of (0.299, 0.341)). See Fig. A7 for a visual example. On SecondNav(T\u2192S), this effect is slightly more pronounced, the TrainedAgentMemory probe is 0.55 meters away on average (99% empirical bootstrap of the mean gives a range of (0.52, 0.588)). Taken holistically, these results show that the probe is both more efficient than the agent and consistently travels through new parts of the environment (that the agent did not travel through). Thus, the spatial representation in the agent's memory is not simply a 'literal' episodic summarization, but also contains anticipatory inferences about previously unexplored spaces being navigable (e.g. traveling along the hypotenuses instead of sides of a room).\nIn the text above we reported free space inference only on episodes where the agent gets an SPL bellow average. In Fig. A12 we provide a plot of Free Space Inference vs. Agent SPL to show the impact of other cutoff points. In Fig. A13 we also provide a similar plot of Excursion Removal vs. Agent SPL. In both cases, as agent SPL increase, the probe is able to infer less free space or remove less excursions.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_3"]}, {"heading": "C.3 FUTURE VISITATION PREDICTION", "text": "In the main text we examined what types of systematic errors are made when decoding past agent locations, here we provide addition analysis and look at predicting future observations as that will reveal if there are any idiosyncrasies in what can be predicted about future vs. what will happen in the future.\nGiven ground truth location s t+k , we evaluate the decoder via i) absolute L2 error ||\u015d t+k \u2212s t+k || and ii) relative L2 error ||\u015d t+k \u2212 s t+k ||/||s t+k \u2212 s t ||. To determine baseline (or chance) performance, we train a second set of decoders where instead of using the correct internal state (h t , c t ) as the input, we randomly select an internal state from a different trajectory. This will evaluate if there are any inherent biases in the task.\nIn Fig. A8, we find that the decoder is able to accurately predict where the agent has been, even for long time horizonse.g. at 100 time steps in the past, relative error is 0.55 and absolute error is 1.0m, compared to relative error of 1.0 and absolute error of 3.2m for the chance baseline prediction. For short time horizons the decoder is also able to accurately predict where the agent will be in the future e.g. at 10 time steps into the future, relative and absolute error are below chance. Interestingly, we see that for longer range future predictions, the decoder is worse than chance in relative error but onpar in absolute error. This apparent contradiction arises due to the decoders making (relatively) large systematic errors when the agent backtracks. In order for the decoder to predict backtracking, the agent would need to already know its future trajectory will be sub-optimal (i.e. lead to backtracking) but still take that trajectory. This is in contradiction with the objective the agent is trained for, to reach the goal as quickly as possible, and thus the agent would not take a given path if it knew it would lead to backtracking.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.4 EXTENSION TO SIGHTED NAVIGATION AGENTS", "text": "In the main text we analyzed how 'blind' agents, those with limited perceptual systems, utilize their memory and found evidence that they build cognitive maps. Here, we extend our analysis to agents with rich perceptual systems, those equipped with a Depth camera and an egomotion sensor. Our primary experimental paradigm relies on showing that a probe is able to take shortcuts when given the agent's memory. This experimental paradigm relies on the probe being able to take a shorter path than the agent. Navigation agents with vision can perform PointNav near-perfectly (Wijmans et al., 2020) and thus there isn't room for improving, rendering this experiment infeasible. As a supplement to this experiment, we also show that a metric map (top-down occupancy grid) can be decoded from the agents memory. This procedure can also be applied to sighted agents.\nWe use the ResNet50 (He et al., 2016) Gibson-2plus (Xia et al., 2018) pre-train model from Wijmans et al. (Wijmans et al., 2020) and train an occupancy grid decoder using the same procedure as in the main text. Note however we utilize only Gibson for training and the Gibson validation scenes as held-out data instead of Matterport3D as this agent was only trained on Gibson. As before, we compare performance from TrainedAgentMemory with UntrainedAgentMemory.\nWe find mixed results. When measuring performance with Intersection-over-Union (IoU), UntrainedAgentMemory outperforms TrainedAgentMemory (40.1% vs. 42.9%). However, when measuring performance with average class balanced accuracy, TrainedAgentMemory outperforms UntrainedAgentMemory (61.8% vs. 53.1%). Fig. A9 and Fig. A10 show the corresponding distribution plots.\nOverall, this experiment does not provide convincing evidence either way to whether visionequipped agents build metric maps in their memory. However, it does show that vision-equipped agents, if they do maintain a map of their environment, create one that is considerably more challenging to decode. Further, we note this does not necessarily imply similarly mixed results as to whether or not vision agents maintain a still spatial but sparser representation, such as a topological graph, as their rich perception can fill in the details in the moment.", "publication_ref": ["b19", "b67"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "C.5 NAVIGATION FROM MEMORY ALONE", "text": "In the main text we showed that agents learn to build map-like representations. A map-like representation of the environment, should, to a degree, support navigation with no external information, i.e. by dead reckoning. Given that the actions are deterministic, the probe should be able to perform either task without external inputs and only the agent's internal representation and the previously taken action. The localization performed by the probe in this setting is similar to path integration, however, it must also be able to handle any collisions that occur when navigating. Fig. A11 shows performance vs. episode length for SecondNav(S\u2192T) and SecondNav(T\u2192S). There are two primary trends. For short navigation episodes (\u22645m), the agent is able to complete the task often. We also find that under this setting, SecondNav(T\u2192S) is an easier task. This is due to the information conveyed to the probe by its initial heading. In SecondNav(T\u2192S), the probe can make progress by simply turning around and going forward, while in SecondNav(S\u2192T), the final heading of the agent is not informative of which way the probe should navigate initially. Overall, these results show that the representation built by the agent is sufficient to navigate short distances with no external information.\nExperiment procedure. This experiment mirrors the probe experiment described in methods and materials with three differences: 1) The input from the GPS+Compass sensor is zero-ed out. 2) The change in distance to goal shaping in the reward is normalized by the distance from initial state to goal. We find that the prediction of the value function suffers considerably otherwise. 3) An additional reward signal as to whether or not the last action taken decreased the angle between the probe's current heading and the direction along the shortest path to goal is added. We find the probe has challenges learning to turn around on the SecondNav(T\u2192S) task otherwise (as it almost always starts facing 180 \u2022 in the wrong direction).\nLet h gt t be the heading along the shortest path to goal from the probe's current position s t , h t be the probe's current heading, then AngularDistance(h gt t , h t ) is the error in the probe's heading. The full Published as a conference paper at ICLR 2023 reward for this probe is then r t (s t , a t , s t+1 ) =\n\uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 2.5 \u2022 Success if a t is Stop \u221210.0 \u2022 \u2206 geo dist (s t , s t+1 )/GeoDist(s 0 , g) \u22120.25 \u2022 \u2206 HeadingError (s t , s t+1 ) \u2212\u03bb Otherwise (5) C.6 MEMORY LENGTH\nThe method presented in the main text to examine memory length is post-hoc analysis performed on the 'blind' PointGoal Navigation agents and thus the agent is operating out-of-distribution. From the agent's view, it is still performing a valid PointGoal navigation episode, just with a different starting location, but the agent may not have taken the same sequence of actions if started from that location. While we would still expect performance to stature with a small k if the memory length is indeed short, it is imprecise with measuring the exact memory length of the agent and does not answer what memory budget is required to perform the task.\nHere we examined training agents with a fixed memory length LSTM. Fig. A14 shows similar trends to those described in the main paper -performance increases as the memory budget increases -however performance is higher when the agent is trained for a given memory budget. Due to the increased compute needed to train the model (e.g. training a model with a memory length of 128 is 128\u00d7 more computationally costly), we where unable to train for a memory budget longer than 256.\nWe also note the non-monotonicity in Fig. A14. We conjecture that this is a consequence of inducing the negative effects of large-batch optimization (Keskar et al., 2017) -training with a memory budget of k effectively increases the batch size by a factor of k. Keeping the batch size constant has its own drawbacks; reducing the number of parallel environments will harm data diversity and result in overfitting while reducing the rollout length increases the bias of the return estimate and makes credit assignment harder. Thus we kept number of environments and rollout length constant.", "publication_ref": ["b28"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "D SUPPLEMENTARY VIDEOS", "text": "Movies S1-3 Videos showing blind agent navigation with the location of the hidden state in the collision t-SNE space. Notice that the hidden state stays within a cluster throughout a series of actions.  and SecondNav(T\u2192S) as a function of agent SPL. We see that as agent SPL decreases, the probe is able to take paths that inference more free space. and SecondNav(T\u2192S) as a function of agent SPL. We see that as agent SPL decreases, excursion removal increases since the probe is able to remove additional excursions. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements: We thank Abhishek Kadian for his help in implementing the first version of the SecondNav(T\u2192S) probe experiment. We thank Jitendra Malik for his feedback on the draft and guidance. EW is supported in part by an ARCS fellowship. The Georgia Tech effort was supported in part by NSF, ONR YIP, and ARO PECASE. The Oregon State effort is supported in part by the DARPA Machine Common Sense program. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.\nReproducibility Statement: Implementation details of our analyses are provided in the appendix. Our work builds on datasets and code that are already open-sourced, and our analysis code will be open-sourced.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2018", "authors": "Peter Anderson; Angel X Chang; Devendra Singh Chaplot; Alexey Dosovitskiy; Saurabh Gupta; Vladlen Koltun; Jana Kosecka; Jitendra Malik; Roozbeh Mottaghi; Manolis Savva; Amir Roshan Zamir"}, {"ref_id": "b1", "title": "Exploration and navigation in the blind mole rat (spalax ehrenbergi): global calibration as a primer of spatial representation", "journal": "Journal of Experimental Biology", "year": "2008", "authors": "Reut Avni; Yael Tzvaigrach; David Eilam"}, {"ref_id": "b2", "title": "Building, registrating, and fusing noisy visual maps", "journal": "The International Journal of Robotics Research", "year": "1988", "authors": "Nicholas Ayache; D Olivier;  Faugeras"}, {"ref_id": "b3", "title": "Emergent tool use from multi-agent autocurricula", "journal": "", "year": "", "authors": "Bowen Baker; Ingmar Kanitscheider; Todor Markov; Yi Wu; Glenn Powell; Bob Mcgrew; Igor Mordatch"}, {"ref_id": "b4", "title": "Demis Hassabis, Raia Hadsell, and Dharshan Kumaran. Vector-based navigation using grid-like representations in artificial agents", "journal": "Nature", "year": "2018", "authors": "Andrea Banino; Caswell Barry; Benigno Uria; Charles Blundell; Timothy Lillicrap; Piotr Mirowski; Alexander Pritzel; Martin J Chadwick; Thomas Degris; Joseph Modayil; Greg Wayne; Hubert Soyer; Fabio Viola; Brian Zhang; Ross Goroshin; Neil Rabinowitz; Razvan Pascanu; Charlie Beattie; Stig Petersen; Amir Sadik; Stephen Gaffney"}, {"ref_id": "b5", "title": "Rearrangement: A challenge for embodied ai", "journal": "", "year": "2020", "authors": "Dhruv Batra; X Angel; Sonia Chang;  Chernova; J Andrew; Jia Davison; Vladlen Deng; Sergey Koltun; Jitendra Levine; Igor Malik; Roozbeh Mordatch; Manolis Mottaghi; Hao Savva;  Su"}, {"ref_id": "b6", "title": "Learning deployable navigation policies at kilometer scale from a single traversal", "journal": "", "year": "2018", "authors": "Jake Bruce; Niko S\u00fcnderhauf; Piotr Mirowski; Raia Hadsell; Michael Milford"}, {"ref_id": "b7", "title": "From objects to landmarks: the function of visual location information in spatial navigation", "journal": "Frontiers in psychology", "year": "2012", "authors": "Edgar Chan; Oliver Baumann; A Mark; Jason B Bellgrove;  Mattingley"}, {"ref_id": "b8", "title": "Matterport3d: Learning from rgb-d data in indoor environments", "journal": "", "year": "2017", "authors": "Angel Chang; Angela Dai; Thomas Funkhouser; Maciej Halber; Matthias Niessner; Manolis Savva; Shuran Song; Andy Zeng; Yinda Zhang"}, {"ref_id": "b9", "title": "Shortcut ability in hamsters (mesocricetus auratus): The role of environmental and kinesthetic information", "journal": "Animal Learning & Behavior", "year": "1993", "authors": "Nicole Chapuis; Patricia Scardigli"}, {"ref_id": "b10", "title": "Robustnav: Towards benchmarking robustness in embodied navigation", "journal": "", "year": "", "authors": "Prithvijit Chattopadhyay; Judy Hoffman; Roozbeh Mottaghi; Ani Kembhavi"}, {"ref_id": "b11", "title": "Still no convincing evidence for cognitive map use by honeybees. Proceedings of the National Academy of Sciences", "journal": "", "year": "2014", "authors": "Allen Cheung; Matthew Collett; Thomas S Collett; Alex Dewar; Fred Dyer; Paul Graham; Michael Mangan; Ajay Narendra; Andrew Philippides; Wolfgang St\u00fcrzl; Barbara Webb; Antoine Wystrach; Jochen Zeil"}, {"ref_id": "b12", "title": "No need for a cognitive map: Decentralized memory for insect navigation", "journal": "PLOS Computational Biology", "year": "2011", "authors": "Holk Cruse; R\u00fcdiger Wehner"}, {"ref_id": "b13", "title": "Emergence of grid-like representations by training recurrent neural networks to perform spatial localization", "journal": "", "year": "2018", "authors": "J Christopher; Xue-Xin Cueva;  Wei"}, {"ref_id": "b14", "title": "What do navigation agents learn about their environment?", "journal": "", "year": "2022", "authors": "Kshitij Dwivedi; Gemma Roig; Aniruddha Kembhavi; Roozbeh Mottaghi"}, {"ref_id": "b15", "title": "The cognitive map in humans: Spatial navigation and beyond", "journal": "Nature Neuroscience", "year": "", "authors": "Russell Epstein; Joshua Patai; Hugo Julian;  Spiers"}, {"ref_id": "b16", "title": "Learning, development, and conceptual change.The organization of learning", "journal": "The MIT Press", "year": "1990", "authors": "Charles R Gallistel"}, {"ref_id": "b17", "title": "Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs", "journal": "", "year": "1706", "authors": "Priya Goyal; Piotr Doll\u00e1r; Ross B Girshick; Pieter Noordhuis; Lukasz Wesolowski; Aapo Kyrola; Andrew Tulloch"}, {"ref_id": "b18", "title": "The ontogeny of a mammalian cognitive map in the real world", "journal": "Science", "year": "2020", "authors": "Lee Harten; Amitay Katz; Aya Goldshtein; Michal Handel; Yossi Yovel"}, {"ref_id": "b19", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b20", "title": "Long short-term memory", "journal": "Neural Computation", "year": "1997", "authors": "Sepp Hochreiter; J\u00fcrgen Schmidhuber"}, {"ref_id": "b21", "title": "Robust estimation of a location parameter", "journal": "JSTOR", "year": "1964", "authors": "J Peter;  Huber"}, {"ref_id": "b22", "title": "Multigrid neural memory", "journal": "PMLR", "year": "2020", "authors": "Tri Huynh; Michael Maire; Matthew R Walter"}, {"ref_id": "b23", "title": "Shakey: An experiment in robot planning and learning", "journal": "", "year": "1972", "authors": " Stanford Research Institute"}, {"ref_id": "b24", "title": "Homing pigeons do extract directional information from olfactory stimuli", "journal": "Behavioral Ecology and Sociobiology", "year": "1990", "authors": "P Ioal\u00e8; F Nozzolini;  Papi"}, {"ref_id": "b25", "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "journal": "", "year": "2015", "authors": "Sergey Ioffe; Christian Szegedy"}, {"ref_id": "b26", "title": "The evolution of the cognitive map", "journal": "Brain, behavior and evolution", "year": "2003", "authors": "F Lucia;  Jacobs"}, {"ref_id": "b27", "title": "Are we making real progress in simulated environments? measuring the sim2real gap in embodied visual navigation", "journal": "", "year": "", "authors": "Abhishek Kadian; Joanne Truong; Aaron Gokaslan; Alexander Clegg; Erik Wijmans; Stefan Lee; Manolis Savva; Sonia Chernova; Dhruv Batra"}, {"ref_id": "b28", "title": "On large-batch training for deep learning: Generalization gap and sharp minima", "journal": "", "year": "2017", "authors": "Dheevatsa Nitish Shirish Keskar; Jorge Mudigere; Mikhail Nocedal; Ping Tak Peter Smelyanskiy;  Tang"}, {"ref_id": "b29", "title": "Simple but effective: Clip embeddings for embodied ai", "journal": "", "year": "2022", "authors": "Apoorv Khandelwal; Luca Weihs; Roozbeh Mottaghi; Aniruddha Kembhavi"}, {"ref_id": "b30", "title": "A subterranean mammal uses the magnetic compass for path integration", "journal": "Proceedings of the National Academy of Sciences", "year": "2004", "authors": "Tali Kimchi; Ariane S Etienne; Joseph Terkel"}, {"ref_id": "b31", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2015", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"ref_id": "b32", "title": "The art of using t-sne for single-cell transcriptomics", "journal": "Nature communications", "year": "2019", "authors": "Dmitry Kobak; Philipp Berens"}, {"ref_id": "b33", "title": "Assembly of the cnidarian camera-type eye from vertebrate-like components", "journal": "Proceedings of the National Academy of Sciences", "year": "2008", "authors": "Zbynek Kozmik; Jana Ruzickova; Kristyna Jonasova; Yoshifumi Matsumoto; Pavel Vopalensky; Iryna Kozmikova; Hynek Strnad; Shoji Kawamura; Joram Piatigorsky; Vaclav Paces"}, {"ref_id": "b34", "title": "The surprising creativity of digital evolution: A collection of anecdotes from the evolutionary computation and artificial life research communities", "journal": "Artificial Life", "year": "2020", "authors": "Joel Lehman; Jeff Clune; Dusan Misevic; Christoph Adami; Lee Altenberg; Julie Beaulieu; J Peter; Samuel Bentley; Guillaume Bernard;  Beslon; M David;  Bryson"}, {"ref_id": "b35", "title": "Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection", "journal": "", "year": "2017", "authors": "Tsung-Yi Lin; Priya Goyal; Ross Girshick"}, {"ref_id": "b36", "title": "An intriguing failing of convolutional neural networks and the coordconv solution", "journal": "", "year": "2018", "authors": "Rosanne Liu; Joel Lehman; Piero Molino; Felipe Petroski Such; Eric Frank; Alex Sergeev; Jason Yosinski"}, {"ref_id": "b37", "title": "Animal navigation", "journal": "Pan Books", "year": "1967", "authors": "Ronald Mathias; Lockley "}, {"ref_id": "b38", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b39", "title": "Path-planning strategies for a point mobile automaton moving amidst unknown obstacles of arbitrary shape", "journal": "Algorithmica", "year": "1987", "authors": "J Vladimir; Alexander A Lumelsky;  Stepanov"}, {"ref_id": "b40", "title": "Homing with locale, taxon, and dead reckoning strategies by foraging rats: sensory hierarchy in spatial navigation", "journal": "Behavioural brain research", "year": "1999", "authors": "Hans Maaswinkel; Q Ian;  Whishaw"}, {"ref_id": "b41", "title": "Chimpanzee spatial memory organization", "journal": "Science", "year": "1973", "authors": "W Emil;  Menzel"}, {"ref_id": "b42", "title": "Path integration in desert ants, cataglyphis fortis", "journal": "Proceedings of the National Academy of Sciences", "year": "1988", "authors": "Martin M\u00fcller; R\u00fcdiger Wehner"}, {"ref_id": "b43", "title": "Rectified linear units improve restricted boltzmann machines", "journal": "", "year": "2010", "authors": "Vinod Nair; Geoffrey E Hinton"}, {"ref_id": "b44", "title": "The hippocampus as a cognitive map", "journal": "Clarendon Press", "year": "1978", "authors": "O' John; Lynn Keefe;  Nadel"}, {"ref_id": "b45", "title": "Oles Dobosevych, Dhruv Batra, and Oleksandr Maksymets. Is mapping necessary for realistic pointgoal navigation?", "journal": "", "year": "2022", "authors": "Ruslan Partsey; Erik Wijmans; Naoki Yokoyama"}, {"ref_id": "b46", "title": "Cognitive maps in wolves and men", "journal": "Environmental design research", "year": "1976", "authors": "R Peters"}, {"ref_id": "b47", "title": "Habitat-matterport 3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai", "journal": "", "year": "2021", "authors": "Aaron Santhosh K Ramakrishnan; Erik Gokaslan; Oleksandr Wijmans; Alex Maksymets; John Clegg; Eric Turner; Wojciech Undersander; Andrew Galuba;  Westbury; X Angel;  Chang"}, {"ref_id": "b48", "title": "Jost Tobias Springenberg, et al. A generalist agent", "journal": "", "year": "2022", "authors": "Scott Reed; Konrad Zolna; Emilio Parisotto; Sergio Gomez Colmenarejo; Alexander Novikov; Gabriel Barth-Maron; Mai Gimenez; Yury Sulsky; Jackie Kay"}, {"ref_id": "b49", "title": "Habitat: A Platform for Embodied AI Research", "journal": "", "year": "2019", "authors": "Manolis Savva; Abhishek Kadian; Oleksandr Maksymets; Yili Zhao; Erik Wijmans; Bhavana Jain; Julian Straub; Jia Liu; Vladlen Koltun; Jitendra Malik; Devi Parikh; Dhruv Batra"}, {"ref_id": "b50", "title": "Highdimensional continuous control using generalized advantage estimation", "journal": "", "year": "2016", "authors": "John Schulman; Philipp Moritz; Sergey Levine; Michael Jordan; Pieter Abbeel"}, {"ref_id": "b51", "title": "Proximal policy optimization algorithms", "journal": "", "year": "2017", "authors": "John Schulman; Filip Wolski; Prafulla Dhariwal; Alec Radford; Oleg Klimov"}, {"ref_id": "b52", "title": "Estimating uncertain spatial relationships in robotics", "journal": "Springer", "year": "1990", "authors": "Randall Smith; Matthew Self; Peter Cheeseman"}, {"ref_id": "b53", "title": "A unified theory for the computational and mechanistic origins of grid cells", "journal": "", "year": "", "authors": "Ben Sorscher; Gabriel C Mel; Samuel A Ocko; Lisa Giocomo; Surya Ganguli"}, {"ref_id": "b54", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "Nitish Srivastava; Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov"}, {"ref_id": "b55", "title": "The replica dataset: A digital replica of indoor spaces", "journal": "", "year": "1906", "authors": "Julian Straub; Thomas Whelan; Lingni Ma; Yufan Chen; Erik Wijmans; Simon Green; Jakob J Engel; Raul Mur-Artal; Carl Ren; Shobhit Verma; Anton Clarkson; Mingfei Yan; Brian Budge; Yajie Yan; Xiaqing Pan; June Yon; Yuyang Zou; Kimberly Leon; Nigel Carter; Jesus Briales; Tyler Gillingham; Elias Mueggler; Luis Pesqueira; Manolis Savva; Dhruv Batra; M Hauke; Renzo Strasdat; Michael De Nardi; Steven Goesele; Richard A Lovegrove;  Newcombe"}, {"ref_id": "b56", "title": "Reinforcement learning: An introduction", "journal": "MIT press", "year": "1992", "authors": "S Richard; Andrew G Sutton;  Barto"}, {"ref_id": "b57", "title": "Manolis Savva, and Dhruv Batra. Habitat 2.0: Training home assistants to rearrange their habitat", "journal": "", "year": "", "authors": "Andrew Szot; Alex Clegg; Eric Undersander; Erik Wijmans; Yili Zhao; John Turner; Noah Maestre; Mustafa Mukadam; Devendra Chaplot; Oleksandr Maksymets; Aaron Gokaslan; Vladimir Vondrus; Sameer Dharur; Franziska Meier; Wojciech Galuba; Angel Chang; Zsolt Kira; Vladlen Koltun; Jitendra Malik"}, {"ref_id": "b58", "title": "Probabilistic robotics (intelligent robotics and autonomous agents)", "journal": "", "year": "2005", "authors": "Sebastian Thrun; Wolfram Burgard; Dieter Fox"}, {"ref_id": "b59", "title": "Cognitive map-based navigation in wild bats revealed by a new high-throughput tracking system", "journal": "Science", "year": "2020", "authors": "Sivan Toledo; David Shohami; Ingo Schiffner; Emmanuel Lourie; Yotam Orchan; Yoav Bartan; Ran Nathan"}, {"ref_id": "b60", "title": "Cognitive maps in rats and men", "journal": "Psychological Review", "year": "1948", "authors": "C Edward;  Tolman"}, {"ref_id": "b61", "title": "Efficient object localization using convolutional networks", "journal": "", "year": "2015", "authors": "Jonathan Tompson; Ross Goroshin; Arjun Jain; Yann Lecun; Christoph Bregler"}, {"ref_id": "b62", "title": "Visualizing data using t-sne", "journal": "Journal of machine learning research", "year": "2008", "authors": "Laurens Van Der Maaten; Geoffrey Hinton"}, {"ref_id": "b63", "title": "The dance language and orientation of bees", "journal": "Harvard University Press", "year": "1967", "authors": "Karl Von; Frisch "}, {"ref_id": "b64", "title": "No training required: Exploring random encoders for sentence classification", "journal": "", "year": "2019", "authors": "John Wieting; Douwe Kiela"}, {"ref_id": "b65", "title": "DD-PPO: Learning near-perfect pointgoal navigators from 2.5 billion frames", "journal": "", "year": "", "authors": "Erik Wijmans; Abhishek Kadian; Ari Morcos; Stefan Lee; Irfan Essa; Devi Parikh; Manolis Savva; Dhruv Batra"}, {"ref_id": "b66", "title": "Individual comparisons by ranking methods", "journal": "Springer", "year": "1992", "authors": "Frank Wilcoxon"}, {"ref_id": "b67", "title": "Gibson env: Real-world perception for embodied agents", "journal": "", "year": "2018", "authors": "Fei Xia; Zhiyang Amir R Zamir; Alexander He; Jitendra Sax; Silvio Malik;  Savarese"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: (A) PointGoal navigation.An agent is initialized in a novel environment (bluesquare) and task with navigation to a point specified relative to the start location (red square). We study 'blind' agents, equipped with just an egomotion sensor (called GPS+Compass in this literature). (B) 'Blind' agent vs. bug. Our learned 'blind' agent compared to 2 variants and an oracle equipped variant of the Bug algorithm(Lumelsky & Stepanov, 1987). The Bug algorithm initially orients itself towards the goal and then proceeds towards the goal. Upon hitting a wall, it follows along the wall until it reaches the other side. The oracle version is told whether wall-following left or right is optimal, providing an upper-bound on Bug algorithm performance. (C) t-SNE of the agent's internal representation for collisions. We find 4 overall clusters corresponding to the previous action taken and whether or not that action led to a collision.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Navigation performance vs. memory length. Agent performance does not saturate until memory can contain information from hundreds of steps. A memory of 10 3 steps is half the maximum episode length.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: (A) Probe experiment.First, an agent navigates (blue path, blue LSTM) from start (green sphere) to target (red sphere). After the agent navigates, we task a probe (purple LSTM) with performing the same navigation episode with the additional information encapsulated in the agent's internal representation (or memory), h A T . The probe is able to navigate more efficiently by taking shortcuts (purple path). As denoted by the dashed line between the probe and agent networks, the probe does not influence what the agent stores in its internal representation. Environment in the image from the Replica Dataset. (B) Agent memory transplant increases probe efficiency (SPL). Results of our trained probe agent under three configurations -initialized with an empty representation (AllZeroMemory), a representation of a random agent walked along the trained agent's path (UntrainedAgentMemory), and the final representation of the trained agent (TrainedAgentMemory). 95% confidence interval reported over 5 agent-probe pairs.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure4: Learning navigation improves map prediction from memory. (Left) Accuracy (Intersection over Union) distributions (via kernel density estimation) and means (dashed lines); TrainedAgentMemory has a higher mean than UntrainedAgentMemory with p-value \u2264 10 \u22125 (via Wilcoxon signed-rank test(Wilcoxon, 1992)). (Right) Example ground truth and predicted occupancy maps using TrainedAgentMemory (corresponding to (A) and (B) IoU points). Light grey is non-navigable and dark grey is navigable. The agent path is drawn in light blue and navigates from start (green) to target (red). We can see that when the agent travels close to one wall, the map decoder predicts another wall parallel to it, indicating a corridor.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure5: (A) Excursion prediction example. Qualitative example of the previously-visited location decoder making systematic errors when decoding an excursion. Blue represents the confidence of the decoder that the agent was previously at a given location; we can see that it is lower in the path interval marked in red (excursion) than the rest. (B) Remembrance of excursions. Performance of decoders when predicting previous agent locations broken down into three categories. 'Nonexcursion' is all predictions where the current location of the agent and the prediction time step are not part of an excursions. 'Excursion' is when the prediction time step is part of an excursion. 'Exit' is when the prediction time step is part of the last 10% of the excursion. X-axis is the distance into the past and Y-axis is the relative error between the true and predicted locations.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Sensors, Architecture, Training Procedure, Training Data. The probe uses the same sensor suite, architecture, training procedure, and training data as the agent, described in Section A.1", "figure_data": ""}, {"figure_label": "A6A7A8A9A10A11A12", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure A6 :PathDiffFigure A7 :Figure A8 :Figure A9 :Figure A10 :Figure A11 :Figure A12 :A6A7A8A9A10A11A12FigureA6: True state trajectory comparison. Example trajectories of an agent with true state (trained for a specific environment and using global coordinates), green line, compared to an agent trained for many environments and using episodic coordinates, blue line. The later is what we examine in this work. Notice that the agent with true state take shortest path trajectories while the agent without true state instead exhibits strong wall-following behavior.", "figure_data": ""}, {"figure_label": "A13", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure A13 :A13FigureA13: Excursion Removal for the TrainedAgentMemory probe on both SecondNav(S\u2192T) and SecondNav(T\u2192S) as a function of agent SPL. We see that as agent SPL decreases, excursion removal increases since the probe is able to remove additional excursions.", "figure_data": ""}, {"figure_label": "A14", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure A14 :A14FigureA14: Performance vs. memory length for agents trained under a given memory length. Note that longer memory lengths are challenging to train for under this methodology as it induces the negative effects of large-batch optimization and is computationally expensive.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "6\u00b10.40 71.1\u00b10.27 91.0\u00b10.40 70.8\u00b10.25 2 UntrainedAgentMemory 92.4\u00b10.28 72.0\u00b10.19 91.2\u00b10.54 72.2\u00b10.35 3 TrainedAgentMemory 96.2\u00b10.23 85.0\u00b10.16 96.0\u00b10.16 84.8\u00b10.22", "figure_data": "SecondNav(S\u2192T) Success SPL 91.(B) Probe Type 1 AllZeroMemorySecondNav(T\u2192S) Success SPL"}, {"figure_label": "A2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Excursion removal result of our trained probe agent under three configurations -initialized with an empty representation (AllZeroMemory), a representation of a random agent walked along the trained agent's path (UntrainedAgentMemory), and the final representation of the trained agent (TrainedAgentMemory). 95% confidence interval reported over 5 agent-probe pairs.", "figure_data": "SecondNav(S\u2192T)SecondNav(T\u2192S)Probe TypeExcursion RemovalExcursion Removal1 AllZeroMemory 2 UntrainedAgentMemory 3 TrainedAgentMemory0.21\u00b10.017 0.23\u00b10.009 0.52\u00b10.0140.21\u00b10.004 0.25\u00b10.009 0.51\u00b10.011NavigableNot NavigableAgent Path Novel Scene, Episodic CoordinatesAgent Path Known Scene, Global Coordinates"}], "formulas": [{"formula_id": "formula_0", "formula_text": "(h t , c t ) via the recurrence (h i , c i ) = LSTM(o i , (h i\u22121 , c i\u22121 )) for t \u2212 k < i \u2264 t where (h t\u2212k , c t\u2212k ) = (0, 0).", "formula_coordinates": [5.0, 108.0, 572.21, 247.43, 29.73]}, {"formula_id": "formula_1", "formula_text": "LSTM LSTM o A T -1 LSTM LSTM o A T h A T -2 a A T -2 a A T -1 a A T h A T h P 2 o P 1 a P 1 a P 2 o P 2 S T Stop Gradient (A)", "formula_coordinates": [6.0, 112.0, 83.17, 374.82, 102.49]}, {"formula_id": "formula_2", "formula_text": "\u015d t\u2212k = f k (h t , c t ) + s t , k \u2208 [1, 256].", "formula_coordinates": [8.0, 327.46, 600.94, 149.59, 17.29]}, {"formula_id": "formula_3", "formula_text": "r t = 2.5 \u2022 Success if a t is Stop \u2212\u2206 geo dist (s t , s t+1 ) \u2212 \u03bb Otherwise (1)", "formula_coordinates": [15.0, 213.68, 194.59, 290.32, 31.92]}, {"formula_id": "formula_4", "formula_text": "l i = T t=2 ||s t \u2212 s t\u22121 || 2 (2)", "formula_coordinates": [15.0, 261.05, 366.98, 242.95, 30.2]}, {"formula_id": "formula_5", "formula_text": "SPL i = Success i \u2022 d i min{d i , l i } (3)", "formula_coordinates": [15.0, 244.0, 413.85, 260.0, 29.94]}, {"formula_id": "formula_6", "formula_text": "PathDiff(Agent, Probe) = 1 N N i=1 min 1\u2264j\u2264T GeoDist(s (agent) i , s (probe) j ),(4)", "formula_coordinates": [18.0, 174.07, 548.14, 329.94, 30.32]}, {"formula_id": "formula_7", "formula_text": "\uf8f1 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f3 2.5 \u2022 Success if a t is Stop \u221210.0 \u2022 \u2206 geo dist (s t , s t+1 )/GeoDist(s 0 , g) \u22120.25 \u2022 \u2206 HeadingError (s t , s t+1 ) \u2212\u03bb Otherwise (5) C.6 MEMORY LENGTH", "formula_coordinates": [21.0, 108.25, 101.95, 395.75, 78.21]}], "doi": "10.1038/s41586-018-0102-6"}