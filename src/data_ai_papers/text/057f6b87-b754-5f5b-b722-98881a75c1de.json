{"title": "Preregistering NLP research", "authors": "Emiel Van Miltenburg; Chris Van Der Lee; Emiel Krahmer", "pub_date": "", "abstract": "Preregistration refers to the practice of specifying what you are going to do, and what you expect to find in your study, before carrying out the study. This practice is increasingly common in medicine and psychology, but is rarely discussed in NLP. This paper discusses preregistration in more detail, explores how NLP researchers could preregister their work, and presents several preregistration questions for different kinds of studies. Finally, we argue in favour of registered reports, which could provide firmer grounds for slow science in NLP research. The goal of this paper is to elicit a discussion in the NLP community, which we hope to synthesise into a general NLP preregistration form in future research.", "sections": [{"heading": "Introduction", "text": "Scientific results are only as reliable as the methods that we use to obtain those results. Recent years have seen growing concerns about the reproducibility of scientific research, leading some to speak of a 'reproducibility crisis' (see Fidler and Wilcox 2018 for an overview of the debate). Although the main focus of the debate has been on psychology (e.g. through Open Science Collaboration 2015) and medicine (Macleod et al., 2014), there are worries about the reproducibility of Natural Language Processing (NLP) research as well (Fokkens et al., 2013;Cohen et al., 2018;Moore and Rayson, 2018;Branco et al., 2020). The reproducibility debate has led to Munaf\u00f2 et al.'s (2017) Manifesto for reproducible science, where the authors discuss the different threats to reproducible science, and different ways to address these threats. We will first highlight some of their proposals, and discuss their adoption rate in NLP. Our main observation is that preregistration is rarely used. We believe this is an undesirable situation, and devote the rest of this paper to argue for preregistration of NLP research. Munaf\u00f2 et al. recommend more methodological training, so that e.g. statistical methods are applied correctly. In NLP, we see different researchers picking up the gauntlet to teach others about statistics (Dror et al., 2018(Dror et al., , 2020, achieving language-independence (Bender, 2011), or best practices in human evaluation (van der Lee et al., 2019Lee et al., , 2021. Moreover, every *ACL conference offers tutorials on a wide range of different topics. While efforts to improve methodology could be more systematic (e.g. by actively encouraging methodology tutorials, and working towards community standards), 1 the infrastructure is in place.\nMunaf\u00f2 et al. also recommend to diversify peer review. Instead of only having journals, that are responsible for both the evaluation and dissemination of research, we can now also solicit peer feedback after publishing our work on a platform like ArXiv or OpenReview. The NLP community is clearly ahead of the curve in terms of the adoption of preprints, and actively discussing ways to improve peer review (ACL Reviewing Committee 2020a,b; Rogers and Augenstein 2020). To improve the quality of the reviews themselves, ACL2020 featured a tutorial on peer reviewing .\nAnother advice from Munaf\u00f2 et al. is to adopt reporting guidelines, so that papers include all relevant details for others to reproduce the results. The NLP community is rapidly adopting such guidelines, in the form of Dodge et al.'s (2019) reproducibility checklist that authors for EMNLP2020 need to fill in. Beyond reproducibility, we are also seeing more and more researchers adopting Data statements (Bender and Friedman, 2018), Model cards (Mitchell et al., 2019), and Datasheets (Gebru et al., 2018) ", "publication_ref": ["b15", "b28", "b16", "b9", "b32", "b33", "b12", "b13", "b2", "b47", "b46", "b57", "b4", "b30", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Data collection", "text": "Have any data been collected for this study already? Hypothesis What's the main question being asked or hypothesis being tested in this study?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dependent variable", "text": "Describe the key dependent variable(s) specifying how they will be measured.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conditions", "text": "How many and which conditions will participants be assigned to? Analyses Specify exactly which analyses you will conduct to examine the main question/hypothesis. Outliers and Exclusions Describe exactly how outliers will be defined and handled, and your precise rule(s) for excluding observations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sample Size", "text": "How many observations will be collected or what will determine sample size?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Other", "text": "Anything else you would like to pre-register?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Research aim", "text": "Specify the overall aim of the research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Use of literature", "text": "Specify the role of theory in your research design.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rationale", "text": "Elaborate if your research is conducted from a certain theoretical perspective.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tradition", "text": "Specify the type of tradition you work in: grounded theory, phenomenology, . . .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data collection plan", "text": "Describe your data collection plan freely. Be as explicit as possible.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Type of data collected", "text": "Select the type(s) of data you will collect.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Type of sampling", "text": "Indicate the type of sampling you will rely on: purposive, theoretical, convenience, snowball. . .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Rationale", "text": "Indicate why you choose this particular type of sampling.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sort of sample", "text": "Pick the ideal composition of your sample: heterogenous, homogenous, . . .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stopping rule", "text": "Indicate what will determine to stop data collection: saturation, planning, resources, other.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data collection script", "text": "Upload your topic guide, observation script, focus group script, etc.  (Gelman and Loken, 2013). This negatively impacts the reliability and generalisability of any study. In other words: preregistration allows us to distinguish between exploratory and confirmatory research. Exploratory research does not require preregistration, because the goal is to get a sense of what is possible. Any pattern you come across during exploratory research, allows us to draw up hypotheses. For a subsequent confirmatory study you could/should preregister to test those hypotheses. By explicitly marking (parts of) your study as exploratory or confirmatory, it is easier to understand the status of your results. Compared to the work on reporting quality, there has been little talk of preregistration in the NLP literature; the terms 'preregister' or 'preregistration' are hardly used in the ACL Anthology. 2 For this reason, we will focus on preregistration and its application in NLP research. The next sections discuss how preregistration works ( \u00a72), propose preregistration questions for NLP research ( \u00a73), discuss the idea of 'registered reports' as an alter-2 Looking for these terms, we found four papers that mention preregistration: Cao et al. (2018) and van der Lee et al. (2019) mention it, andvan Miltenburg et al. (2018) and Futrell and Levy (2019) share their own preregistration. native pathway to publication ( \u00a74) and the overall feasibility of preregistrations in NLP ( \u00a75).", "publication_ref": ["b20", "b7", "b48", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "How does preregistration work?", "text": "Before you begin, you enter the hypotheses, design, and analysis plan of your study on a website like the Open Science Framework, AsPredicted, or ResearchBox. These sites provide a time stamp; evidence that you indeed made all the relevant decisions before carrying out the study. During your study, you follow the preregistered plans as closely as possible. In an ideal world, there would be an exact match between your plans and the actual study you carried out. But there are usually unforeseen circumstances that force you to change your study. This is fine, if the changes are clearly specified (including the reasons for those changes) in your final report (Nosek et al., 2018).\nA typical preregistration form. Table 1 shows questions from the preregistration form from As-Predicted. 3 This form is geared towards hypothesisdriven, experimental research where human participants are assigned to different experimental conditions. Simmons et al. (2017) note that answers should state exactly how the study will be executed, but also that it should be short and easy to read.\nData collection, hypothesis, dependent variable. The form first asks whether data collection has been carried out yet (ideally the answer should be no, but see Appendix \u00a7A.1), and then asks researchers to What are your hypotheses/key assumptions? What is the independent variable? (e.g. model architecture) What is the dependent variable (e.g. output quality) How will you measure the dependent variable? Is there just one condition (corpus/task), or more? What parameter settings will you use? What data will you use, and how is it split in train/val/test? Why this data? What are key properties of the data? How will you analyse the results and test the hypotheses? make their main hypothesis explicit so that it cannot be changed after the fact. Following the hypothesis, researchers should describe their key dependent variables (i.e. the main outcome variables) and how they will be measured. This includes cutoff points that will be used to discretise continuous variables (e.g. to divide participants in different groups).\nConditions, analyses, outliers and exclusions. Next, the form asks about the design of the study, the analyses, and the process of determining outliers (and whether those should be excluded). The answer needs to be detailed enough so that other researchers are able to reproduce the study.\nSample size and other. The form then asks how much data will be collected, so as to prevent optional stopping (where researchers keep collecting data until the results are in line with their preferred hypothesis). 4 Finally, the form allows researchers to specify other aspects of the study they would like to preregister, such as \"secondary analyses, variables collected for exploratory purposes, [or] unusual analyses.\"\nQualitative research. Preregistration is not only suitable for quantitative research; Haven and Grootel (2019) present a proposal to preregister qualitative studies as well. Their suggestions are also presented in Table 1. The authors argue that, although qualitative research differs in its goals from quantitative research (developing theories rather than testing them), it is still valuable to make your assumptions and research plans explicit before carrying out your planned study. Because qualitative research is more flexible than quantitative research, Haven and Grootel view qualitative preregistrations as living documents; continuously updated to track the research progress. This stimulates conscientiousness, and avoids sloppy research. Public preregistrations also allow for immediate feedback.\nWhat do you aim to learn from the error analysis? What do you know from the literature about system errors? What kinds of errors do you expect to find? How will you sample the outputs to analyse? Do you also consider the input in your sampling strategy? How do you plan to analyse the output? How many judges will assess the output? Are they trained? How is the reliability of the judges assessed? Is there a fixed error categorisation scheme or not? Table 3: Questions to ask before an error analysis.", "publication_ref": ["b34", "b42"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "Preregistration in NLP research", "text": "To determine what a preregistration for NLP research should look like, we need to consider the different kinds of research contributions in NLP. For this, we use the paper types proposed for COLING 2018. 5 These are: Computationally-aided linguistic analysis; NLP engineering experiment paper; Reproduction/Resource/Position/Survey Paper. Of these, position papers are less suitable for preregistration, since these are more opinion/experiencedriven, and the process of writing them cannot be formalised. We treat the others below.\nAnalysis, experiments, and reproduction papers typically have one or more hypotheses, even though they may not always be marked as such. 6 This means we can ask many of the same questions for these studies as for experimental research. Table 2 provides a rough overview of important questions to ask before carrying out your research.\nIf your study contains an error analysis, then you could ask the more qualitatively oriented questions in Table 3. They acknowledge that you always enter error analysis with some expectation (i.e. researcher bias) of what kinds of mistakes systems are likely to make, and where those mistakes may be found. The questions also stimulate researchers to go beyond the practice of providing some 'lemons' alongside cherry-picked examples showing good performance.\nThe main benefit of asking these questions beforehand is that they force researchers to carefully consider their methodology, and they make researchers' expectations explicit. This also helps to identify unexpected findings, or changes that were made to the research design during the study.\nResource papers are on the qualitative side of the spectrum, and as such the questions from Haven and Grootel ( 2019), presented at the bottom of Table 1, are generally appropriate for these kinds of papers as well. Particularly 1) the original purpose for collecting the data, 2) sampling decisions (what documents to include), and 3) annotation (what framework/perspective to use) are important. Because the former typically influences the latter two, it is useful to document how the goal of the study influenced decisions regarding sampling and annotation, in case the study at some point pivots towards another goal.\nSurvey papers should follow the PRISMA guidelines for structured reviews . According to these guidelines, researchers should state exactly where they searched for existing literature, what search terms they used, and what criteria they used to select relevant papers. This increases reproducibility, allows readers to find any gaps in the survey, and avoids a biased presentation of the literature (i.e. only citing researchers you know, or work that fits your preferred narrative). A recent NLP example of a structured review is provided by Reiter (2018).", "publication_ref": ["b37"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Registered reports", "text": "Registered reports \"[split] conventional peer review in half\" (Chambers, 2019). First, authors submit a well-motivated research plan for review, before carrying out the study (similar to a preregistration). This plan may go back-and-forth between the authors and the reviewers, but once the plan is accepted, the authors receive the guarantee that, if they carry out the study according to plan, their work will be published. As with preregistration, deviations from the original plan are allowed, but these should be indentified in the final report. The main advantage of registered reports is that they provide a means to avoid publication bias. Because studies aren't judged on the basis of their results, positive results are equally likely to be published as negative results. As long as the study is deemed valuable a priori, it should get published. An additional benefit of registered reports is that reviews may actually correct flaws in the research design, meaning that we reduce the chance of running an expensive study all for nothing. In the case of NLP research, this may save a lot of energy (cf. Strubell et al. 2019). We are not aware of any NLP journals that offer registered reports, but strongly encourage the NLP community to take steps in this direction. 7 5 Feasibility Loken (2013, 2014) touch upon the feasibility of preregistration, noting that: \"[f]or most of our own research projects this strategy hardly seems possible: in our many applied research projects, we have learned so much by looking at the data. Our most important hypotheses could never have been formulated ahead of time.\" This certainly rings true for NLP as well. However, we should be careful about conclusions that are drawn on the basis of pre-existing data. Gelman and Loken (2013) note that in such cases, if it is feasible to collect more data, it is good to follow up positive results with a pre-registered replication to confirm your initial findings. One way to do this is to collect and evaluate your model on a new test set (cf. Recht et al. 2019). This tells us to what extent trained models generalise to unseen data. Another idea could be to preregister the human evaluation (or error analysis) of the model output.\nWe believe that preregistration, and especially registered reports, could ease the pressure to publish as soon as possible. If your analysis plan is accepted for publication, you can take as long as you want to actually carry out the study, without having to worry about being scooped. This provides new opportunities for slow science in NLP (also see Min-Yen Kan's keynote at COLING 2018).", "publication_ref": ["b8", "b43", "b20", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Questions about preregistration", "text": "Below we address some common questions about preregistration. We thank our anonymous reviewers for raising some of these questions. Is preregistration more work? In our experience, preregistration adds little overhead to a research project. Especially if a project requires approval by an Institutional Review Board (IRB), you need to write a description along similar lines anyway. For projects not requiring IRB approval, it is good practice to provide a model card (Mitchell et al., 2019), data sheet (Gebru et al., 2018) or data statement (Bender and Friedman, 2018) with your model or resource. Given the ethical aspects of NLP research, it is advisable to consider all dimensions of your study before you carry it out. Moreover, preregistration is a good way to start writing the paper before carrying out the research, a practice advocated by Eisner (2010) to maximise the impact of your work. Finally, it may be more work to prepare a registered report, but this comes with the benefit of having a pre-approved methodology. Once the project is completed, reviewers will not reject your paper based on methodological choices. Should I worry about being scooped? There is no need to worry. We already discussed registered reports, where research proposals are provisionally accepted before data collection starts. Otherwise, this worry has been addressed through the existence of both public and private preregistrations. A researcher can choose to keep a preregistration private until the research is completed. They can make their preregistration public whenever they like, for example to invite feedback from the community. In addition, preregistrations are also time-stamped, and you can use these time stamps during the review phase to show that you have had these ideas before similar work was published. 8 What about citing preregistrations? In some regards, the discussion about preregistrations is similar to the discussion about preprints (i.e. papers on ArXiv), thus similar questions arise. Both preregistrations and published studies are being cited. For example, medical journals like BMC Public Health also publish study protocols (similar to preregistrations), without any results, that are also cited by others (e.g. work using a similar protocol).\nWhat should we do with concurrent work? It may of course happen that multiple researchers have similar ideas around the same time. We believe that it is still valuable to publish multiple independent studies with similar results. Even if they don't provide any new insights (which is rare), they do provide evidence towards the robustness of the findings. Where and how those findings should be published is a separate discussion. 9 How should we teach preregistration? Preregistration is already being incorporated into Psychology courses (see, for example, Blincoe and Buchert 2020). It is relatively straightforward to implement as part of student research proposals during applied courses in NLP: specify what you plan to do 8 The public/private distinction has been implemented by both the Open Science Foundation and AsPredicted.org. The Open Science Foundation allows for a 4-year embargo, during which the preregistration is kept private. Aspredicted allows for preregistrations to be private indefinitely. 9 However, if there is value in publishing the 'first' paper, there is probably also value in publishing the 'second' one. The same holds for the question of whether both studies should be cited; good scholarship considers all the available evidence.\nexactly, and what you expect to find. It is often useful for students to have an explicit format to think through their research plans, to make sure that they make sense.", "publication_ref": ["b30", "b18", "b4", "b14", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "Although preregistration is offered as a solution to improve our work, it does not solve all of our problems. Van 't Veer and Giner-Sorolla (2016) mention three limitations: 1. Flexibility. It may be difficult or infeasible for authors to foresee all possible outcomes, and as such there may be gaps in the preregistration, which still allow for flexibility in the analysis. 2. Fraud. There is no way to prevent fraudulent researchers from, e.g., creating multiple preregistrations, or falsely 'preregistering' studies that were already run. At some point we just have to trust each other to do the right thing, but increased transparency does make it harder to commit fraud. 3. Applicability. Preregistration may not be possible for all kinds of studies. As discussed above, it has mainly been developed for quantitative studies (particularly experiments), and there are proposals for the preregistration of qualitative research (Haven and Grootel, 2019), although we have yet to see whether this idea will catch on. Finally, Szollosi et al. (2020) argue that, although preregistration might offer greater transparency, it does not by itself improve scientific reasoning and theory development. Since large parts of NLP are pre-theoretical (we have observed effects but do not have any theoretical explanations for why these effects occur), one might reasonably argue that we should focus on theory development first, before we can carry out any meaningful experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have discussed how preregistration could benefit NLP research, and how different kinds of contributions could be preregistered. We have also proposed an initial list of questions to ask before carrying out NLP research (and see Appendix A for example preregistration forms). With this paper, we hope to encourage other NLP researchers to consider preregistering their work, so that they will no longer get lost in the garden of forking paths. Still, there is no silver bullet to cure sloppy science. Although preregistration is certainly helpful, it does not guarantee high-quality research, and we do need to stay critical about preregistered studies, and the way they are carried out.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Preregistration forms", "text": "This appendix provides preregistration forms for different kinds of paper types. These forms are preliminary, and they are mainly meant as a starting point for discussions of preregistration in NLP. We are happy to admit that there may be flaws in this appendix (either in the forms or in our reasoning). Future work should investigate whether these forms are complete (i.e. limit researcher degrees of freedom as much as possible) and appropriate for different kinds of NLP research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Preface: data availability in NLP", "text": "Preregistration is a means to avoid hindsight bias, because you have to specify your expectations upfront, when your perspective is not yet colored by your experience with the data. But for NLP studies it is unclear what 'the data' is. We can distinguish three kinds of data: 1. The training/validation/test sets, 2. The model output, 3. Human judgments.\nIn an ideal situation, preregistration would occur before any kind of data has been obtained. The problem is that this is often not the case; there are many canonical datasets for which the data is publicly available. Of course one could collect an additional test set (as we suggested above), but the community often judges new approaches based on their performance for established datasets. So what should we do? Still preregister! Arguably the training, validation, and test data is usually not central to the work. What matters is how a particular system performs. So even if we don't usually find ourselves in the ideal situation where none of the data is available yet, it is typically fine to preregister your study if the train/eval/test data is available but system outputs and evaluation scores are not. When authors are transparent in their data sharing policy, we can reconstruct the timeline of events before and after the preregistration, to see how much their knowledge about the data may have influenced them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Computationally aided linguistic analysis", "text": "This paper type corresponds to several different setups, ranging from experiments with human subjects, to corpus analyses to see if particular generalisations from the literature hold up. Preregistration has been discussed from a linguistics perspective by Roettger (2020). For experiments with human participants, readers may refer to the standard preregistration forms from AsPredicted (see our ", "publication_ref": ["b38"], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Position paper", "text": "Position papers typically do not need to be preregistered, since they often do not provide any new data, but rely on the author's experience. These kinds of papers also usually signal that they are more opinionated than other kinds of papers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Reproduction paper", "text": "For a reproduction paper, the questions are a mix of the questions above ( \u00a7A.  It is at least a bit unexpected to promote preregistration for resource papers. After all, if all you do is data collection, then there are no hypotheses to test. But since the goal of this appendix is to provide a starting point for discussion, we are taking the stance that no study is free from biases or initial expectations. As such, it is useful to at least document what you aim to collect, for what reasons, and how you are planning to do so.\n1. What is the goal of this study? 2. What kind of data will be collected? 3. How will this data be collected? 4. What is the intended application for the data you plan to collect? 5. What sampling strategy will be used? Why?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "Thanks to the anonymous reviewers for their constructive feedback, and to all the #NLProc Twitter people for discussion.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "ACL Reviewing Committee. 2020a. Acl rolling review proposal. Archived by the Internet Archive on", "journal": "", "year": "2020-06-20", "authors": ""}, {"ref_id": "b1", "title": "ACL Reviewing Committee. 2020b. Short-term reform proposals for acl reviewing. Adopted by the ACL Exec on June 8 as an initial step to improve reviewing. Archived by the Internet Archive on", "journal": "", "year": "2020-10-30", "authors": ""}, {"ref_id": "b2", "title": "On achieving and evaluating language-independence in NLP", "journal": "Linguistic Issues in Language Technology", "year": "2011", "authors": "Emily M Bender"}, {"ref_id": "b3", "title": "Proceedings of the 27th International Conference on Computational Linguistics", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Emily M Bender; Leon Derczynski"}, {"ref_id": "b4", "title": "Data statements for natural language processing: Toward mitigating system bias and enabling better science", "journal": "Transactions of the Association for Computational Linguistics", "year": "2018", "authors": "Emily M Bender; Batya Friedman"}, {"ref_id": "b5", "title": "Research preregistration as a teaching and learning tool in undergraduate psychology courses", "journal": "Psychology Learning & Teaching", "year": "2020", "authors": "Sarai Blincoe; Stephanie Buchert"}, {"ref_id": "b6", "title": "Andr\u00e9 Moreira, and Willem Elbers. 2020. A shared task of a new, collaborative type to foster reproducibility: A first exercise in the area of language science and technology with RE-PROLANG2020", "journal": "", "year": "", "authors": "Ant\u00f3nio Branco; Nicoletta Calzolari; Piek Vossen; Gertjan Van Noord; Jo\u00e3o Dieter Van Uytvanck; Lu\u00eds Silva;  Gomes"}, {"ref_id": "b7", "title": "BabyCloud, a technological platform for parents and researchers", "journal": "", "year": "2018", "authors": "Xu\u00e2n-Nga Cao; Cyrille Dakhlia; Patricia Del Carmen; Mohamed-Amine Jaouani; Malik Ould-Arbi; Emmanuel Dupoux"}, {"ref_id": "b8", "title": "What's next for registered reports?", "journal": "Nature", "year": "2019", "authors": "Chris Chambers"}, {"ref_id": "b9", "title": "Three dimensions of reproducibility in natural language processing", "journal": "", "year": "2018", "authors": "K Cohen; Jingbo Xia; Pierre Zweigenbaum; Tiffany Callahan; Orin Hargraves; Foster Goss; Nancy Ide; Aur\u00e9lie N\u00e9v\u00e9ol; Cyril Grouin; Lawrence E Hunter"}, {"ref_id": "b10", "title": "Reviewing natural language processing research", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Kevin Cohen; Kar\u00ebn Fort; Margot Mieskes; Aur\u00e9lie N\u00e9v\u00e9ol"}, {"ref_id": "b11", "title": "Show your work: Improved reporting of experimental results", "journal": "", "year": "2019", "authors": "Jesse Dodge; Suchin Gururangan; Dallas Card; Roy Schwartz; Noah A Smith"}, {"ref_id": "b12", "title": "The hitchhiker's guide to testing statistical significance in natural language processing", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Rotem Dror; Gili Baumer; Segev Shlomov; Roi Reichart"}, {"ref_id": "b13", "title": "Statistical Significance Testing for Natural Language Processing. Human Language Technologies", "journal": "Morgan & Claypool", "year": "2020", "authors": "Rotem Dror; Lotem Peled-Cohen; Segev Shlomov; Roi Reichart"}, {"ref_id": "b14", "title": "Advice published on Jason Eisner's academic webpage", "journal": "", "year": "2010", "authors": "Jason Eisner"}, {"ref_id": "b15", "title": "Reproducibility of Scientific Results", "journal": "", "year": "2018", "authors": "Fiona Fidler; John Wilcox"}, {"ref_id": "b16", "title": "Offspring from reproduction problems: What replication failure teaches us", "journal": "Long Papers", "year": "2013", "authors": "Antske Fokkens; Marten Marieke Van Erp; Ted Postma; Piek Pedersen; Nuno Vossen;  Freire"}, {"ref_id": "b17", "title": "Do RNNs learn human-like abstract word order preferences?", "journal": "", "year": "2019", "authors": "Richard Futrell; Roger P Levy"}, {"ref_id": "b18", "title": "", "journal": "", "year": "2018", "authors": "Timnit Gebru; Jamie Morgenstern; Briana Vecchione; Jennifer Wortman Vaughan; Hanna Wallach; Hal Daum\u00e9 Iii; Kate Crawford"}, {"ref_id": "b19", "title": "Proceedings of the 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning", "journal": "", "year": "", "authors": "Datasets Datasheets For"}, {"ref_id": "b20", "title": "The garden of forking paths: Why multiple comparisons can be a problem, even when there is no \"fishing expedition\" or \"p-hacking\" and the research hypothesis was posited ahead of time", "journal": "", "year": "2013", "authors": "Andrew Gelman; Eric Loken"}, {"ref_id": "b21", "title": "The statistical crisis in science: data-dependent analysis-a\" garden of forking paths\"-explains why many statistically significant comparisons don't hold up", "journal": "American scientist", "year": "2014", "authors": "Andrew Gelman; Eric Loken"}, {"ref_id": "b22", "title": "Preregistering qualitative research", "journal": "Accountability in Research", "year": "2019", "authors": "L Tamarinde;  Haven;  Leonie Dr;  Van Grootel"}, {"ref_id": "b23", "title": "Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions", "journal": "", "year": "2020", "authors": "David M Howcroft; Anya Belz; Miruna-Adriana Clinciu; Dimitra Gkatzia; A Sadid; Saad Hasan; Simon Mahamood;  Mille; Sashank Emiel Van Miltenburg; Verena Santhanam;  Rieser"}, {"ref_id": "b24", "title": "research fast and slow", "journal": "", "year": "2018", "authors": "Min-Yen Kan"}, {"ref_id": "b25", "title": "COLING 2018", "journal": "", "year": "", "authors": ""}, {"ref_id": "b26", "title": "Neural network models for paraphrase identification, semantic textual similarity, natural language inference, and question answering", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Wuwei Lan; Wei Xu"}, {"ref_id": "b27", "title": "The prisma statement for reporting systematic reviews and meta-analyses of studies that evaluate health care interventions: Explanation and elaboration", "journal": "PLOS Medicine", "year": "2009", "authors": "Alessandro Liberati; Douglas G Altman; Jennifer Tetzlaff; Cynthia Mulrow; Peter C G\u00f8tzsche; P A John; Mike Ioannidis; P J Clarke;  Devereaux"}, {"ref_id": "b28", "title": "Biomedical research: increasing value, reducing waste", "journal": "Lancet", "year": "2014", "authors": "Susan Malcolm R Macleod; Ian Michie; Ulrich Roberts; Iain Dirnagl;  Chalmers; P A John; Rustam Ioannidis; An-Wen Al-Shahi Salman; Paul Chan;  Glasziou"}, {"ref_id": "b29", "title": "Evolving ai from research to real life -some challenges and suggestions", "journal": "", "year": "2018", "authors": "Sandya Mannarswamy; Shourya Roy"}, {"ref_id": "b30", "title": "Model Cards for Model Reporting", "journal": "ACM Press", "year": "2019", "authors": "Margaret Mitchell; Simone Wu; Andrew Zaldivar; Parker Barnes; Lucy Vasserman; Ben Hutchinson; Elena Spitzer; Deborah Inioluwa; Timnit Raji;  Gebru"}, {"ref_id": "b31", "title": "Preferred reporting items for systematic reviews and meta-analyses: The prisma statement", "journal": "PLOS Medicine", "year": "2009", "authors": "David Moher; Alessandro Liberati; Jennifer Tetzlaff; Douglas G Altman;  Group"}, {"ref_id": "b32", "title": "Bringing replication and reproduction together with generalisability in NLP: Three reproduction studies for target dependent sentiment analysis", "journal": "", "year": "2018", "authors": "Andrew Moore; Paul Rayson"}, {"ref_id": "b33", "title": "A manifesto for reproducible science", "journal": "Nature Human Behaviour", "year": "2017", "authors": "Marcus R Munaf\u00f2; Brian A Nosek; Dorothy V M Bishop; Katherine S Button; Christopher D Chambers; Nathalie Percie; Uri Sert; Eric-Jan Simonsohn; Jennifer J Wagenmakers; John P A Ware;  Ioannidis"}, {"ref_id": "b34", "title": "The preregistration revolution", "journal": "Proceedings of the National Academy of Sciences", "year": "2018", "authors": "Brian A Nosek; Charles R Ebersole; Alexander C Dehaven; David T Mellor"}, {"ref_id": "b35", "title": "Estimating the reproducibility of psychological science", "journal": "Science", "year": "2015", "authors": ""}, {"ref_id": "b36", "title": "Do ImageNet classifiers generalize to ImageNet?", "journal": "", "year": "2019", "authors": "Benjamin Recht; Rebecca Roelofs; Ludwig Schmidt; Vaishaal Shankar"}, {"ref_id": "b37", "title": "A structured review of the validity of BLEU", "journal": "Computational Linguistics", "year": "2018", "authors": "Ehud Reiter"}, {"ref_id": "b38", "title": "Preregistration in experimental linguistics: Applications, challenges, and limitations", "journal": "", "year": "2020", "authors": "Timo Roettger"}, {"ref_id": "b39", "title": "What can we do to improve peer review in NLP?", "journal": "", "year": "2020", "authors": "Anna Rogers; Isabelle Augenstein"}, {"ref_id": "b40", "title": "Distinguishing affixoid formations from compounds", "journal": "", "year": "2018", "authors": "Josef Ruppenhofer; Michael Wiegand; Rebecca Wilm; Katja Markert"}, {"ref_id": "b41", "title": "The human evaluation datasheet 1.0: A template for recording details of human evaluation experiments in nlp", "journal": "", "year": "2021", "authors": "Anastasia Shimorina; Anya Belz"}, {"ref_id": "b42", "title": "How to properly preregister a study", "journal": "Data Colada", "year": "2017", "authors": "Joe Simmons; Leif Nelson; Uri Simonsohn"}, {"ref_id": "b43", "title": "Energy and policy considerations for deep learning in NLP", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Emma Strubell; Ananya Ganesh; Andrew Mccallum"}, {"ref_id": "b44", "title": "Iris van Rooij, Trisha Van Zandt, and Chris Donkin. 2020. Is preregistration worthwhile?", "journal": "Trends in cognitive sciences", "year": "", "authors": "Aba Szollosi; David Kellen; Danielle J Navarro; Richard Shiffrin"}, {"ref_id": "b45", "title": "Authorless topic models: Biasing models away from known structure", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Laure Thompson; David Mimno"}, {"ref_id": "b46", "title": "Human evaluation of automatically generated text: Current trends and best practice guidelines", "journal": "Computer Speech & Language", "year": "2021", "authors": "Chris Van Der Lee; Albert Gatt; Emiel Emiel Van Miltenburg;  Krahmer"}, {"ref_id": "b47", "title": "Best practices for the human evaluation of automatically generated text", "journal": "", "year": "2019", "authors": "Chris Van Der Lee; Albert Gatt; Sander Emiel Van Miltenburg; Emiel Wubben;  Krahmer"}, {"ref_id": "b48", "title": "DIDEC: The Dutch image description and eye-tracking corpus", "journal": "", "year": "2018", "authors": "\u00c1kos Emiel Van Miltenburg; Ruud K\u00e1d\u00e1r; Emiel Koolen;  Krahmer"}, {"ref_id": "b49", "title": "Pre-registration in social psychology-a discussion and suggested template", "journal": "Journal of Experimental Social Psychology", "year": "2016", "authors": "Anna Elisabeth Van 't Veer; Roger Giner-Sorolla"}, {"ref_id": "b50", "title": "How much data are you planning to collect? (Is there any target or stopping criterion?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b51", "title": "How will the data be analysed? (a) If automatic: what analysis tool will you use, and how will it be configured? (b) If manual: what is the background of the annotators? How will you ensure reliability and validity of the analysis?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b52", "title": "What properties should the data have?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b53", "title": "How will you ensure that the data will have those properties?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b54", "title": ")) for their surveys. This requires authors to develop a review protocol, which means authors should answer the following questions before initiating their study: 1. What is the goal of this study? 2", "journal": "", "year": "2009", "authors": " Moher"}, {"ref_id": "b55", "title": "What search engines will you use? 7. What search queries will you use? 8. What are the variables of interest? 9. How will you synthesize the results? 10. How will you ensure the reliability and validity of your study", "journal": "", "year": "", "authors": ""}, {"ref_id": "b56", "title": "Thus Shimorina and Belz (2021) developed a datasheet for recording all the necessary details. This datasheet can mostly be filled in before the study is carried out. A selection of their questions is provided below", "journal": "", "year": "2020", "authors": " Howcroft"}, {"ref_id": "b57", "title": "What type of input(s) does the system have? 2. What type of output does the system produce? 3. What task is the system supposed to carry out? 4. What languages are involved? 5. How many systems/outputs per system are being evaluated", "journal": "", "year": "", "authors": ""}, {"ref_id": "b58", "title": "How are the outputs selected?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b59", "title": "What is the statistical power of the sample size?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b60", "title": "What kind of evaluators are being used?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b61", "title": "What is the background if the evaluators? 11. How are responses collected? 12. What quality assurance measures are used? 13. What do evaluators see when carrying out evaluations?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b62", "title": "How free are evaluators regarding when and how quickly they are supposed to evaluate the results", "journal": "", "year": "", "authors": ""}, {"ref_id": "b63", "title": "Can evaluators provide feedback or not? 16. What are the experimental conditions like? 17. What type of quality is assessed in the evaluation? 18. How is this quality assessed? 19. How are the responses processed?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b64", "title": "This does not mean that there cannot be a quantitative component (e.g. counting the number of errors, comparing this number between different systems), but often systems are also just analysed by themselves, and we just want to know what future researchers still ought to improve about the system. 1. What is the goal of the error analysis? 2. What type of input(s) does the system have? 3. What type of output does the system produce?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b65", "title": "When does something count as an error?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b66", "title": "How many outputs will you analyse? 10. How will you sample the outputs to analyse? 11. Do you also consider the input in your sampling strategy? 12. How do you plan to analyse the output? 13. How many judges will assess the output? 14", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "(a) What constitutes a successful reproduction? (b) What constitutes an unsuccessful reproduction? (c) What is the margin of error? 4. Do you expect to be successful? Why (not)? 5. How are you planning to reproduce the original results? (a) Will you use the same soft/hardware? (b) Will you use the same data? (c) Will you use the same codebase? (d) If human participants are used: will you target the same demographic, and use the same experimental settings? (e) Will you contact the authors? (f) How much time do authors have to respond to your queries? (g) How much time/effort are you willing to spend?6. Will you carry out an error analysis? If so, see \u00a7A.8.2. 7. Anything else you'd like to preregister? A.6 Resource paper", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "for ethical reasons. Munaf\u00f2 et al.'s final recommendation, preregistration, means that authors should specify what they are going to do, and what they expect to find, before carrying out their studies(Nosek et al.,   ", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "research."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Questions for analysis, experiments, and reproduction papers (expanded in Appendix A).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": ", OSF, or the questions from Roettger's Figure1.For more corpus-oriented studies (e.g.Ruppenhofer et al. 2018), we should consider a mix of the quantitative and qualitative questions from our Table1. Usually these kinds of studies do require some data collection, so authors should ask: If human judgments: see \u00a7A.8.1.14. Will you carry out an error analysis? If so, see \u00a7A.8.2. 15. Anything else you'd like to preregister?", "figure_data": "1. What is the goal of this study?2. What are the main questions/hypotheses?3. What kind of data will be collected?4. How will this data be collected?5. What sampling strategy will be used? Why?6. How much data are you planning to collect?(Is there any target or stopping criterion?)7. How will the data be analysed?(a) If automatic: what analysis tool will you use, andhow will it be configured?(b) If manual: what is the background of the annota-tors? How will you ensure reliability and validityof the analysis?8. What statistical tests will be used, if any?9. Anything else you'd like to preregister?A.3 NLP Engineering experiment paperNLP engineering experiments are like experimentsin the social sciences, except that the subjects areNLP models and the performance data is modeloutput. So the standard social science questionsdo not need to be modified that much to fit NLPexperiments.1. What is the goal of your study?2. What are your hypotheses/key assumptions?3. What are the (in)dependent variables?4. How will these variables be measured?5. Is there just one condition, or more?6. What software libraries will you use?7. What hardware will you use?8. What parameter settings will you use?9. What data set will you use?10. If the data set does not already exist, see  \u00a7A.6.If it does:(a) How familiar are you with the data?(b) To what extent are your hypotheses informed byyourself or others interacting with this data? Towhat extent does this hinder the generalisabilityof your approach?(c) Are you planning to collect additional data tovalidate your approach?11. Why this data? What are its key properties?12. How is the data split in train/val/test?13. How will you analyse the results and test thehypotheses?(a) If automatic: what metric(s) (including imple-mentation) will you use, and how will they beconfigured?(b)"}], "formulas": [], "doi": "10.1162/tacl_a_00041"}