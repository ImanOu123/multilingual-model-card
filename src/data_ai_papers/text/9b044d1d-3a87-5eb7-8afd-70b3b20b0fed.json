{"title": "A Flexible Empirical Bayes Approach to Multiple Linear Regression, and Connections with Penalized Regression", "authors": "Youngseok Kim; Wei Wang; Peter Carbonetto; Matthew Stephens", "pub_date": "2024-06-12", "abstract": "We introduce a new empirical Bayes approach for large-scale multiple linear regression. Our approach combines two key ideas: (i) the use of flexible \"adaptive shrinkage\" priors, which approximate the nonparametric family of scale mixture of normal distributions by a finite mixture of normal distributions; and (ii) the use of variational approximations to efficiently estimate prior hyperparameters and compute approximate posteriors. Combining these two ideas results in fast and flexible methods, with computational speed comparable to fast penalized regression methods such as the Lasso, and with competitive prediction accuracy across a wide range of scenarios. Further, we provide new results that establish conceptual connections between our empirical Bayes methods and penalized methods. Specifically, we show that the posterior mean from our method solves a penalized regression problem, with the form of the penalty function being learned from the data by directly solving an optimization problem (rather than being tuned by cross-validation). Our methods are implemented in an R package, mr.ash.alpha, available from https://github.com/ stephenslab/mr.ash.alpha.", "sections": [{"heading": "Introduction", "text": "Multiple linear regression is one of the oldest statistical methods for relating an outcome variable to predictor variables, dating back at least to the eighteenth century (Stigler, 1984). In recent decades, data sets have grown rapidly in size, with the number of predictor variables often exceeding the number of observations. Fitting even simple models such as multiple linear regression to large data sets raises interesting research questions. These include questions about regularization and prediction (e.g., how to estimate the parameters to optimize out-of-sample prediction accuracy), and questions about variable selection and inference (e.g., how to choose the coefficients in the regression that are non-zero). In this paper, we focus on the former.\nMany different approaches for regularization and prediction have been proposed. Most fall into one of two types: penalized linear regression (PLR) methods based on different penalized least-squares criteria (e.g., Hoerl and Kennard, 1970;Tibshirani, 1996;Fan and Li, 2011;Miller, 2002;Zou and Hastie, 2005;Zhang, 2010;Hazimeh and Mazumder, 2020;Amir et al., 2021) and Bayesian approaches based on different priors and computational methods (e.g., Mitchell and Beauchamp, 1988;George and McCulloch, 1993;Meuwissen et al., 2001;Park and Casella, 2008;Hans, 2009;Carvalho et al., 2010;Li and Lin, 2010;Griffin and Brown, 2010;Guan and Stephens, 2011;Habier et al., 2011;Carbonetto and Stephens, 2012;Zhou et al., 2013;Drugowitsch, 2013;Wang et al., 2020;Ro\u010dkov\u00e1 and George, 2018;Ray and Szab\u00f3, 2022;Zabad et al., 2023).\nThese different approaches have different strengths and weaknesses. For example, ridge regression (an L 2 -penalty on the coefficients; Hoerl and Kennard, 1970;Tikhonov, 1963) is simple, involving a convex optimization problem and a single tuning parameter, and performs well in \"dense\" settings (many predictors with non-zero effects). However, it does not do well in \"sparse\" settings where a small number of non-zero coefficients dominate. The Lasso (an L 1 -penalty on the coefficients; Tibshirani, 1996) is similarly computationally convenient, and behaves better than ridge regression in sparse settings. However, prediction accuracy of the Lasso is limited by its tendency to \"overshrink\" large effects (e.g., Su et al., 2017). The Elastic Net (Zou and Hastie, 2005) combines some of the advantages of ridge regression and the Lasso, and in its most general form includes both as special cases; however, the Elastic Net also introduces an additional tuning parameter that results in a non-trivial additional computation expense.\nNonconvex penalties-examples include the L 0 -penalty (Miller, 2002;Hazimeh and Mazumder, 2020), the smoothly clipped absolute deviation (SCAD) penalty (Fan and Li, 2011) and the minimax concave penalty (MCP) (Zhang, 2010)-can also give better prediction performance in sparse settings, but this comes with the challenge of solving a nonconvex optimization problem.\nBayesian methods, by using flexible priors, have the potential to achieve excellent prediction accuracy in both sparse and dense settings (e.g., Park and Casella, 2008;Hans, 2009;Griffin and Brown, 2010;Li and Lin, 2010;Guan and Stephens, 2011;Zhou et al., 2013;Zeng et al., 2018), but have some practical drawbacks; notably, model fitting typically involves performing Markov chain Monte Carlo (MCMC) with a potentially high computational burden. Further, convergence of the Markov chain can be difficult to diagnose, particularly for non-expert users. In summary, when choosing among existing methods, one must confront tradeoffs between prediction accuracy, flexibility and computational convenience.\nIn this paper, we develop an approach to multiple linear regression that aims to combine the best features of existing methods: it is fast, comparable in speed to the cross-validated Lasso; it is flexible, capable of adapting to sparse and dense settings; it is self-tuning, with no need for user-specified hyperparameters; and, in our numerical studies, its prediction accuracy was competitive with the best methods against which we compared, in a wide range of regression settings, including both dense and sparse effects.\nThis consistent competitive performance across a wide range of dense/sparse settings is particularly valuable in practice, because in practical applications one does not know whether signals are dense or sparse (or perhaps somewhere in between). Thus, in practice, users will be reluctant to trust a method that does not perform consistently well, even if it performs excellently (even optimally) in certain settings. We believe that the consistently strong performance of our method across settings, combined with its computational speed, make it attractive for practitioners looking to apply large-scale multiple linear regression to real problems.\nOur method takes an empirical Bayes (EB) approach (Robbins, 1964;Efron, 2019;Hartley and Rao, 1967;Carlin and Louis, 2000;Stephens, 2016;Casella, 2001) to multiple regression; that is, it assigns a prior to the coefficients in the regression method, and this prior is learned from the data. The EB approach is, in many ways, a natural approach for attempting to attain the benefits of Bayesian methods while addressing some of their computational challenges. Indeed, EB for the multiple regression problem is far from new; for example, the Bayesian Lasso (Park and Casella, 2008;Joo, 2017) takes an EB approach with a Laplace prior in which the \u03bb parameter in the Laplace prior is estimated by maximizing the (marginal) likelihood. Other EB approaches use a normal prior (Nebebe and Stroud, 1986), a point-normal (\"spike-and-slab\") prior (George and Foster, 2000), and a point-double-exponential prior (Yuan and Lin, 2005). (See also van de Wiel et al. 2019 for a review of other EB approaches.) However, previous EB approaches to multiple regression have either focussed on relatively inflexible priors, or have been met with considerable computational challenges.\nHere, we propose a different EB approach that is both more flexible than these previous EB approaches and more computationally scalable. This new EB approach has two key components. First, to increase flexibility, we borrow the \"adaptive shrinkage\" priors used in Stephens (2016); specifically, we use the \"scale mixture of normals\" priors. This prior family includes most of the popular priors that have been used in Bayesian regression, including normal, Laplace, point-normal, point-Laplace, point-t, normal-inverse-gamma, Dirichlet-Laplace and horseshoe priors (Hoerl and Kennard, 1970;George and McCulloch, 1997;Meuwissen et al., 2001Meuwissen et al., , 2009Habier et al., 2011;Anirban Bhattacharya and Dunson, 2015). Increasing model flexibility typically means greater computational expense, but in this case the use of the adaptive shrinkage priors actually simplifies many computations, essentially because the scale mixture family is a convex family. Second, to make computations tractable, we adapt the variational approximation methods for multiple regression from Carbonetto and Stephens (2012). The main limitation of the variational approximation approach is that, in sparse settings with very highly correlated predictors, it will give only one of the correlated predictors a non-negligible coefficient (Carbonetto and Stephens, 2012). This limitation, which is shared by several other existing methods, including the Lasso and L 0 -penalized regression, does not greatly affect prediction accuracy. However, it does limit the conclusions that can be drawn about the selected variables. Consequently, other methods (e.g., Wang et al., 2020) may be preferred when the main goal is variable selection for scientific interpretation rather than prediction. Since our approach combines EB ideas with variational approximations, we refer to it as a \"variational EB\" (VEB) approach.\nWhile variational methods have previously been used to fit Bayesian linear regression models (Girolami, 2001;Logsdon et al., 2010;Carbonetto and Stephens, 2012;Wang et al., 2020;You et al., 2014;Ren et al., 2011), they have not been used to implement an EB method, and not with the flexible class of priors we consider here. (Independently, Zabad et al. 2023 recently used a VEB approach, but with less flexible priors.) Our work arises from the combination of two earlier ideas: (1) the use of variational approximation techniques for fast posterior computation in large-scale linear regression (Carbonetto and Stephens, 2012); and (2) the use of a flexible class of priors (2), which was originally proposed in Stephens (2016) for performing EB inference in a simpler, but related, problem: the \"normal means problem\" (Efron and Morris, 1973;Johnstone and Silverman, 2004;Sun and Stephens, 2018;Castillo and Van Der Vaart, 2012;Bhadra et al., 2019). The combination of these two ideas results in methods that are simpler, faster, more flexible, and often more accurate than those in Carbonetto and Stephens (2012).\nFinally, another key contribution of our paper is to provide a conceptual bridge between PLR methods and Bayesian methods. Specifically, we show that our VEB approach is actually a PLR method, where the penalty function is learned from the data by directly solving an optimization problem rather than being tuned by cross-validation (CV). This result is not only conceptually interesting, but opens the door to other potential algorithms (e.g., gradient descent) for the VEB problem. Tuning multiple parameters by solving an optimization problem is also more practical than CV; for example, our VEB approach has a similar computational cost to methods such as the Lasso that tune a single parameter by CV, and is substantially faster than methods such as the Elastic Net that tune two or more parameters by CV.", "publication_ref": ["b83", "b47", "b86", "b26", "b67", "b107", "b104", "b46", "b0", "b68", "b34", "b65", "b73", "b43", "b15", "b57", "b39", "b41", "b42", "b12", "b105", "b21", "b94", "b79", "b76", "b102", "b47", "b87", "b86", "b84", "b107", "b67", "b46", "b26", "b104", "b73", "b43", "b39", "b57", "b41", "b105", "b103", "b78", "b24", "b44", "b14", "b82", "b16", "b73", "b52", "b71", "b33", "b100", "b90", "b82", "b47", "b35", "b65", "b66", "b42", "b1", "b12", "b12", "b94", "b38", "b59", "b12", "b94", "b99", "b77", "b102", "b12", "b82", "b25", "b50", "b85", "b17", "b7", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Organization of the Paper", "text": "The remainder of the paper is organized as follows. Section 2 gives preliminary background on the normal means model and introduces some notation. Section 3 describes our VEB methods and optimization algorithms in detail. Section 4 makes connections between our VEB approach and penalized approaches. Section 5 gives results from numerical studies comparing prediction performance of different methods for multiple linear regression, including our VEB approach. Section 6 summarizes the contributions of this work and discusses future directions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations and Conventions", "text": "We write vectors in bold, lowercase letters (e.g., b) and matrices are written in bold, uppercase letters (e.g., X) We use R n to denote the set of real-valued vectors of length n, R n + for the set of real non-negative vectors of length n, R m\u00d7n for the set of real m \u00d7 n matrices, and S n = {x \u2208 R n + : n i=1 x i = 1} denotes the n-dimensional simplex. We use x j to denote the jth column of matrix X. We write sets and families in calligraphic font, e.g., G. We use N (x; \u00b5, \u03a3) to denote the probability density of the multivariate normal distribution at x \u2208 R n with mean \u00b5 \u2208 R n and n \u00d7 n covariance matrix \u03a3. We use I n to denote the n \u00d7 n identity matrix. We use \u2225x\u2225 =", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u221a", "text": "x T x to denote the L 2 -norm of vector x \u2208 R n , and \u2225x\u2225 \u221e = max{|x 1 |, . . . , |x n |} denotes the L-infinity norm of x. We use \u225c to indicate definitions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries: the Empirical Bayes Normal Means Model", "text": "The computations for our approach are closely related to those for a simpler model known as the \"normal means\" (NM) model. This section reviews this model and introduces notation that will be used later.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Normal Means Model", "text": "The normal means model is a model for a sequence y 1 , . . . , y p of observations in which each observation y j is normally distributed with unknown mean b j and known variance \u03c3 2 :\ny j | b j , \u03c3 2 \u223c N (b j , \u03c3 2 ), j = 1, . . . , p.\n(1)\nThis can be viewed as a special case of multiple linear regression in which the covariates are orthogonal and the residual variance is known. (Specifically, it is equivalent to ( 14) below with X = I n and \u03c3 2 known.)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Normal Means Model with Adaptive Shrinkage Priors", "text": "Stephens ( 2016) considers an EB version of the NM model that assumes the b j are i.i.d. from some prior distribution g that is to be estimated from the observed data. Specifically, Stephens (2016) considers priors that are scale mixtures of normals; that is, g \u2208 G(\u03c3 2 1 , . . . , \u03c3 2 K ), where\nG(\u03c3 2 1 , . . . , \u03c3 2 K ) \u225c g = K k=1 \u03c0 k N (0, \u03c3 2 k ) : \u03c0 \u2208 S K ,(2)\nand where 0 \u2264 \u03c3 2 1 < \u2022 \u2022 \u2022 < \u03c3 2 K < \u221e is a pre-specified grid of component variances, and \u03c0 1 , . . . , \u03c0 K are unknown mixture proportions. Typically, \u03c3 2 1 = 0 so that G(\u03c3 2 1 , . . . , \u03c3 2 K ) includes sparse prior distributions. (We define N (0, 0) to be the Dirac \"delta\" mass at zero, commonly denoted as \u03b4 0 .) By making the grid of variances sufficiently wide and dense, the prior family G(\u03c3 2 1 , . . . , \u03c3 2 K ) can approximate, with arbitrary accuracy, the nonparametric family of all the scale mixtures of zero-mean normal distributions. This nonparametric family, which we denote by G SMN , is very flexible, and includes most popular distributions used as priors in Bayesian regression models, including normal (\"ridge regression\") (Hoerl and Kennard, 1970), point-normal (\"spike and slab\") (Chipman et al., 2001;McCulloch, 1993, 1997;Mitchell and Beauchamp, 1988), double-exponential or Laplace (Figueiredo, 2003;Park and Casella, 2008;Hans, 2009;Tibshirani, 1996;Li and Lin, 2010), horseshoe (Carvalho et al., 2010), normal-gamma prior (Griffin and Brown, 2010), normalinverse-gamma prior (Meuwissen et al., 2009;Habier et al., 2011), mixture of two normals (BSLMM) (Zhou et al., 2013), and the mixture of four zero-centered normals with different variances suggested by Moser et al. (2015).\nStephens (2016) refers to the priors (2) as \"adaptive shrinkage\" priors. Here, we use these adaptive shrinkage priors, but assume a prior distribution in which the scaled coefficients,\nb j /\u03c3, are i.i.d. from g, b j | g, \u03c3 2 i.i.d. \u223c g \u03c3 ,(3)\nwhere g \u03c3 (b j ) \u225c g(b j /\u03c3)/\u03c3. We use this prior on the scaled coefficients because in the regression setting it may help to reduce issues with multi-modality; see Park and Casella 2008 for example. The scaled prior (3) also provides computational benefits in the \"fully Bayesian\" regression setting; see, for example, Chipman et al. 2001;George and McCulloch 1997;Liang et al. 2008 for arguments in favour of the scaled prior. All our methods can also be applied, with minor modifications, to work with the unscaled prior b j i.i.d.\n\u223c g.", "publication_ref": ["b82", "b47", "b18", "b68", "b27", "b73", "b43", "b86", "b57", "b15", "b39", "b66", "b42", "b105", "b70", "b73", "b18", "b35", "b58"], "figure_ref": [], "table_ref": []}, {"heading": "Augmented-variable Representation", "text": "It is helpful to think of g \u2208 G(\u03c3 2 1 , . . . , \u03c3 2 K ) as determining a set of mixture proportions \u03c0 = (\u03c0 1 , . . . , \u03c0 K ) and component variances \u03c3 2 1 , . . . , \u03c3 2 K of a normal mixture. When g \u2208 G(\u03c3 2 1 , . . . , \u03c3 2 K ), the prior (3) can also be written as\nb j | g, \u03c3 2 i.i.d. \u223c K k=1 \u03c0 k N (0, \u03c3 2 \u03c3 2 k ). (4\n)\nIt is also convenient for some of the derivations to introduce the standard augmentedvariable representation of this mixture:\np(\u03b3 j = k | g) = \u03c0 k b j | g, \u03c3 2 , \u03b3 j = k \u223c N (0, \u03c3 2 \u03c3 2 k ),(5)\nwhere the latent variable \u03b3 j \u2208 {1, . . . , K} indicates which mixture component gave rise to b j .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Bayes for the Normal Means Model", "text": "Stephens (2016) provides EB methods to fit the normal means model. These methods proceed in two steps: estimate g (Step 1); compute the posterior distribution for b given the estimated g (Step 2).\nStep 1 is simplified by the use of a fixed grid of variances in (4), which means that only the mixture proportions \u03c0 need to be estimated. This is done by maximizing the marginal log-likelihood:\n\u03c0 = argmax \u03c0 \u2208 S K log p(y | g, \u03c3 2 ) = argmax \u03c0 \u2208 S K p j=1 log K k=1 \u03c0 k L jk ,(6)\nwhere\nL jk \u225c p(y j | g, \u03c3 2 , \u03b3 j = k) = N (y j ; 0, \u03c3 2 + \u03c3 2 \u03c3 2 k ),(7)\nand y \u225c (y 1 , . . . , y p ). This is a convex optimization problem, and can be solved efficiently using convex optimization techniques (Koenker and Mizera, 2014;Kim et al., 2020), or simply by iterating the following Expectation Maximization (EM) updates (Dempster et al., 1977):\nE-step \u03d5 jk \u2190 \u03d5 k (y j ; g, \u03c3 2 ) \u225c p(\u03b3 j = k | y j , g, \u03c3 2 ) = \u03c0 k L jk K k \u2032 =1 \u03c0 k \u2032 L jk \u2032 , (8\n) M-step \u03c0 k \u2190 1 p p j=1 \u03d5 jk , k = 1, . . . , K.(9)\nThe posterior mixture assignment probabilities \u03d5 jk are sometimes referred to as the \"responsibilities\".\nStep 2, computing the posterior distribution, is also straightforward, again due to the independence of the observations and the conjugacy of the normal (mixture) prior with the normal likelihood:\np NM post (b j , \u03b3 j = k | y j , g, \u03c3 2 ) = p(b j | y j , g, \u03c3 2 , \u03b3 j = k) p(\u03b3 j = k | y j , g, \u03c3 2 ) = \u03d5 jk N (b j ; \u00b5 jk , s 2 jk ),(10)\nwhere\n\u00b5 jk \u225c \u00b5 k (y j ; g, \u03c3 2 ) = \u03c3 2 k 1+\u03c3 2 k \u00d7 y j ,(11)\ns 2 jk \u225c s 2 k (y j ; g, \u03c3 2 ) = \u03c3 2 k 1+\u03c3 2 k \u00d7 \u03c3 2 .(12)\n(Although the posterior variances do not depend on y j , we write them as s 2 k (y j ; g, \u03c3 2 ) for notational consistency.) Summing the component posterior (10) over k then yields an analytic expression for the posterior of b j ,\np NM post (b j | y j , g, \u03c3 2 ) = K k=1 \u03d5 jk N (b j ; \u00b5 jk , s 2 jk ).(13)\n3 Variational Empirical Bayes Linear Regression", "publication_ref": ["b55", "b54", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical Bayes Linear Regression", "text": "We consider the multiple linear regression model,\ny | X, b, \u03c3 2 \u223c N (Xb, \u03c3 2 I n ), (14\n)\nwhere y \u2208 R n is a vector of responses, X \u2208 R n\u00d7p is a design matrix whose columns contain predictors x 1 , . . . , x p \u2208 R n , b \u2208 R p is a vector of regression coefficients, and \u03c3 2 \u2265 0 is the variance of the residual errors. While an intercept is not explicitly included in (14), it is easily accounted for by centering y and the columns of X prior to model fitting (Chipman et al., 2001); see also Section 3.5. To simplify presentation, we will assume throughout the main text of the paper that the columns of X are rescaled so that \u2225x j \u2225 = 1, for j = 1, . . . , p. However, all our methods and results can be extended to the unscaled case; Appendix E includes these extensions.\nTaking an EB approach, as in the NM model above, we assume the scaled regression coefficients, b j /\u03c3, are i.i.d from some prior g, where g is to be estimated from the observed data. Although our methods apply more generally, we focus on the adaptive shrinkage priors (4) because they are flexible and computationally convenient.\nA standard EB approach to fitting the regression model ( 14) with priors (3) would, similar to above, involve the following two steps:\n1. Estimate g, \u03c3 2 by maximizing the marginal likelihood:\n(\u011d,\u03c3 2 ) = argmax g \u2208 G, \u03c3 2 \u2208 R + p(y | X, g, \u03c3 2 ) = argmax g \u2208 G, \u03c3 2 \u2208 R + log p(y | X, b, \u03c3 2 ) p(b | g, \u03c3 2 ) db.(15)\n2. Infer b based on the posterior distribution,\np post (b) \u225c p(b | X, y,\u011d,\u03c3 2 ) \u221d p(y | X, b,\u03c3 2 ) p(b |\u011d,\u03c3 2 ). (16\n)\nUnfortunately, in contrast to the NM model, both steps are computationally impractical due to intractable integrals or very large sums, or both, except in special cases.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "Variational Approximation", "text": "To circumvent the intractability of the EB approach, we use a mean-field variational approximation (Blei et al., 2017;Jordan et al., 1999;Wainwright and Jordan, 2008;Logsdon et al., 2010;Carbonetto and Stephens, 2012) to derive a \"variational empirical Bayes\" (VEB) approach. The idea of VEB inference is mentioned explicitly in Blei et al. (2003), although earlier work implemented similar ideas (e.g., Saul and Jordan 1996;Ghahramani and Hinton 2000; see also van de Wiel et al. 2019). To describe the VEB approach, it is convenient to rewrite the two steps of EB as solving a single optimization problem (see also the Supplementary Materials from Wang et al. 2020):\n(p post ,\u011d,\u03c3 2 ) = argmax q, g \u2208 G, \u03c3 2 \u2208 R + F (q, g, \u03c3 2 ),(17)\nwhere the optimization over q is over all possible distributions on (b, \u03b3), and\nF (q, g, \u03c3 2 ) \u225c log p(y | X, g, \u03c3 2 ) \u2212 D KL (q(b, \u03b3) \u2225 p(b, \u03b3 | X, y, g, \u03c3 2 )). (18\n)\nHere, D KL (q \u2225 p) denotes the Kullback-Leibler (K-L) divergence from a distribution q to a distribution p (Kullback and Leibler, 1951). To aid in deriving the closed-form updates below, we reuse the augmented-variable representation from the NM model, \u03b3 \u225c (\u03b3 1 , . . . , \u03b3 p ), in which \u03b3 j was defined in (5). The function F is often called the \"evidence lower bound\" (ELBO) because it is a lower-bound for the \"evidence\", log p(y | X, g, \u03c3 2 ). In (17), the optimization over q is generally intractable. The VEB approach addresses this by restricting the family of distributions to be optimized. Specifically, our mean-field VEB approach solves (q,\u011d,\u03c3 2 ) = argmax\nq \u2208 Q, g \u2208 G, \u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) (19\n)\nwhere\nQ = q : q(b, \u03b3) = p j=1 q j (b j , \u03b3 j ) ,(20)\nis the family of fully-factorized distributions. The resultingq factorizes into a product over individual factorsq j (b j , \u03b3 j ), j = 1, . . . , p, and serves as an approximation to the EB posterior,p post . With this constraint, the optimization becomes tractable (see Section 3.3 for details).", "publication_ref": ["b10", "b53", "b91", "b59", "b12", "b9", "b80", "b37", "b90", "b94", "b56"], "figure_ref": [], "table_ref": []}, {"heading": "Contrast with Carbonetto and Stephens (2012)", "text": "While Carbonetto and Stephens (2012) also use a mean-field approximation for linear regression, their approach to estimating g, \u03c3 2 is quite different-and substantially more complexthan the VEB approach we describe here. In brief, they treat F (q(g, \u03c3 2 ), g, \u03c3 2 ) as a direct approximation to the evidence,\np(y | X, g, \u03c3 2 ) \u2248 exp{F (q(g, \u03c3 2 ), g, \u03c3 2 )},\nand combine this with a prior distribution on g, \u03c3 2 to arrive at an approximate posterior distribution for g, \u03c3 2 . This approach is computationally burdensome because it requires finding a separate approximationq for each g, \u03c3 2 . It also requires specifying a prior on (g, \u03c3 2 ), which introduces an additional layer of decision-making for the user. Our VEB approach greatly simplifies this by simultaneously fitting q, g, \u03c3 2 in a single optimization (19). Our approach also uses more flexible prior families than Carbonetto and Stephens (2012). And, as we show in numerical experiments below, our approach generally improves predictive performance.", "publication_ref": ["b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Coordinate Ascent Algorithm", "text": "We solve (19) for the multiple linear regression model ( 14) with adaptive shrinkage priors (4) using a simple coordinate-wise approach, outlined in Algorithm 1. While theoretical analysis suggests that coordinate ascent can suffer poor rates of convergence (Beck and Tetruashvili, 2013;Wright, 2015;Hazimeh and Mazumder, 2020), it has the advantage of being guaranteed to converge to a stationary point of the objective under mild conditions (see Proposition 11 in Appendix G). In practice, coordinate ascent has emerged as a simple, fast and reliable approach for optimizing large-scale multiple linear regression models, with both convex and nonconvex penalties (Friedman et al., 2007;Wu and Lange, 2008;Friedman et al., 2010;Mazumder et al., 2011;Breheny and Huang, 2011;Hazimeh and Mazumder, 2020).\nIn the following, we show that the steps in Algorithm 1 are easy to implement:\n(i) The update for each q j involves computing a posterior distribution under the NM model.\n(ii) The update for g involves running a single M-step update for the NM model, in which the exact posterior probabilities are replaced with approximate posterior probabilities.\n(iii) The update for the residual variance, \u03c3 2 , has a simple, closed-form solution.\nAlgorithm 1 Coordinate ascent for fitting VEB model (outline only).\nRequire: Data X \u2208 R n\u00d7p , y \u2208 R n , and initial estimates q 1 , . . . , q p , g, \u03c3 2 . repeat for j \u2190 1 to p do\nq j \u2190 argmax q j F (q, g, \u03c3 2 ) end for g \u2190 argmax g \u2208 G F (q, g, \u03c3 2 ) \u03c3 2 \u2190 argmax \u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) until termination criterion is met return q 1 , . . . , q p , g, \u03c3 2 .\nThese results are formally stated in the following proposition. (We assume x T j x j = 1 here to simplify the expressions; more general results and algorithms that do not make this assumption are given in Appendix E.) Proposition 1 (Coordinate ascent updates for VEB) Assume x T j x j = 1, for j = 1, . . . , p, letb j = E(b j ),b = E(b) be the expected values of b j , b with respect to q, r = y \u2212 Xb \u2208 R n is the vector of expected residuals with respect to q, X \u2212j denotes the design matrix X excluding the jth column, q \u2212j is shorthand for all factors q j \u2032 , j \u2032 \u0338 = j, andr j \u2208 R n is the vector of expected residuals accounting for linear effects of all variables other than j,r\nj \u225c y \u2212 X \u2212jb\u2212j = y \u2212 j \u2032 \u0338 =j x j \u2032b j \u2032 . (21\n)\nAdditionally, we useb j \u225c x T jr j =b j + x T jr to denote the ordinary least squares (OLS) estimate of the coefficient b j when the residualsr j are regressed against x j . Then we have the following results:\n(i) The coordinate ascent update q * j \u225c argmax q j F (q, g, \u03c3 2 ) is obtained by the posterior distribution for b j , \u03b3 j under the normal means model (1, 3) in which the observation y j is replaced by the OLS estimate of b j ; that is,\nq * j (b j , \u03b3 j = k) = p NM post (b j , \u03b3 j = k;b j , g, \u03c3 2 ).\nIn particular, the posterior distribution at the maximum is\nq * j (b j , \u03b3 j = k) = \u03d5 * jk N (b j ; \u00b5 * jk , (s 2 jk ) * ),(22)\nin which\n\u00b5 * jk = \u00b5 k (b j ; g, \u03c3 2 ) (23) (s 2 jk ) * = s 2 k (b j ; g, \u03c3 2 ) (24\n)\n\u03d5 * jk = \u03d5 k (b j ; g, \u03c3 2 ). (25\n)\nSee (8,10,11,13) for the definitions of \u00b5 k , s 2 k , \u03d5 k and p NM post .\n(ii) The coordinate ascent update\ng * \u225c argmax g \u2208 G(\u03c3 2 1 ,...,\u03c3 2 K ) F (q, g, \u03c3 2 )\nis achieved by setting\ng * = K k=1 \u03c0 * k N (0, \u03c3 2 k ) \u03c0 * k = 1 p p j=1 q j (\u03b3 j = k), k = 1, . . . , K.(26)\n(iii) Assuming \u03c3 2 1 = 0, the coordinate ascent update (\u03c3 2 ) * \u225c argmax\n\u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) is achieved with (\u03c3 2 ) * = \u2225r\u2225 2 + p j=1 Var q (b j ) + p j=1 K k=2 \u03d5 jk E[b j | \u03b3 j = k]/\u03c3 2 k n + p \u2212 p j=1 \u03d5 j1 .(27)\nOr, assuming q 1 , . . . , q p and g have just been updated as in (i) and (ii) above, we have the simpler update formula\n(\u03c3 2 ) * = \u2225r\u2225 2 +b T (b \u2212b) + \u03c3 2 p(1 \u2212 \u03c0 * 1 ) n + p(1 \u2212 \u03c0 * 1 ) . (28\n)\nProof See Appendix E.\nInserting these expressions into Algorithm 1 (and organizing computations to limit redundant operations and memory requirements) yields Algorithm 2. The computational complexity of this algorithm is O((n + K)p) per outer-loop iteration, with memory requirements O(n + p + K) (in addition to storing the data matrix X), making it tractable for large data sets. The algorithm also exploits the fact that the approximate posterior q(b, \u03b3) can be recovered from \u03c0, \u03c3 2 ,b by running a single round of the coordinate ascent updates for q 1 , . . . , q p . Because of this, the algorithm is initialized simply by providing an initial estimate ofb; the full q is not needed. See Section 3.5.3 for more on initialization.", "publication_ref": ["b3", "b96", "b46", "b28", "b97", "b29", "b63", "b11", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Accuracy of VEB and Exactness for Orthogonal Predictors", "text": "Carbonetto and Stephens (2012) note that their variational approximation approach provides the exact posterior distribution when the columns of X are orthogonal. Here we extend this result, showing that in this special case the VEB method recovers the standard EB method.\nProposition 2 When X has orthogonal columns, the VEB approach ( 19) is mathematically equivalent to the exact EB approach (15, 16).\nAlgorithm 2 Coordinate ascent for fitting VEB model (in detail).\nRequire: Data X \u2208 R n\u00d7p , y \u2208 R n ; number of mixture components, K;\nprior variances, \u03c3 2\n1 < \u2022 \u2022 \u2022 < \u03c3 2 K , with \u03c3 2 1 = 0; initial estimatesb, \u03c0, \u03c3 2 . r = y \u2212 Xb (compute mean residuals) t \u2190 0 repeat for j \u2190 1 to p d\u014d r j =r + x jbj (disregard jth effect in residuals) b j \u2190 x T jr j . (compute OLS estimate) for k \u2190 1 to K do \u03d5 jk \u2190 \u03d5 k (b j ; g, \u03c3 2 ) \u00b5 jk \u2190 \u00b5 k (b j ; g, \u03c3 2 ) end for (update q j ; eqs. 23, 25) b j \u2190 K k=1 \u03d5 jk \u00b5 jk . (update posterior mean of b j ) r \u2190r j \u2212 x jbj . (update mean residuals) end for for k \u2190 1 to K do \u03c0 k \u2190 p j=1 \u03d5 jk /p. (update g; eq. 26) end for \u03c3 2 \u2190 \u2225r\u2225 2 +b T (b \u2212b) + \u03c3 2 p(1 \u2212 \u03c0 1 ) n + p(1 \u2212 \u03c0 1 ) (update \u03c3 2\n; eq. 28)\nt \u2190 t + 1 until termination criterion is met returnb, \u03c0, \u03c3 2 Proof See Appendix G.\nIn brief, this result follows from the fact that, when X has orthogonal columns, the (exact) posterior distribution for b factorizes as (20), and therefore the mean-field assumption is not an approximation. By contrast, the \"conditional maximum likelihood\" (CML) approach to approximating EB inference (George and Foster, 2000;Yuan and Lin, 2005) is not exact even in the case of orthogonal columns.\nProposition 2 suggests that our VEB method should be accurate when the columns of X are close to orthogonal. It also suggests that the approximation may be less accurate for very highly correlated columns. However, Carbonetto and Stephens (2012) note that even in this setting the estimated hyperparameters (here, g) can be accurate. They also note that, when two predictors are highly correlated and predictive of y, the fully-factorized variational approximation tends to give just one of them an appreciable estimated coefficient. This is similar to the behavior of many PLR methods including the Lasso (but different from a wellmixed MCMC-based Bayesian method). While this behavior is undesirable when the main aim of the analysis is to select variables for scientific interpretation, it does not necessarily harm prediction accuracy. Thus, the VEB approach may perform well for prediction even in settings where the assumptions of the mean-field variational approximation are clearly violated. Our numerical studies (Section 5) confirm this.", "publication_ref": ["b33", "b100", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Practical Issues and Extensions", "text": "In this subsection, we discuss some practical implementation issues and potential extensions of this method.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Intercept", "text": "In multiple regression applications, it is common to include an intercept term that is not regularized in the same way as other variables. A common approach, and the approach we take here, is to center y and the columns of X (Chipman et al., 2001;Friedman et al., 2010); that is, the observed responses y i are replaced with the centered responses y i \u2212 n i \u2032 =1 y i \u2032 /n, and the data x ij are replaced with column-centered values x ij \u2212 n i \u2032 =1 x i \u2032 j /n.", "publication_ref": ["b18", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Selection of Grid for Prior Variances", "text": "Following Stephens ( 2016), we choose a grid {\u03c3 2 1 , . . . , \u03c3 2 K } that is sufficiently broad and dense so that results do not change much if the grid is made broader and denser; the aim is to choose a G(\u03c3 2 1 , . . . , \u03c3 2 K ) that closely approximates the non-parametric family G SMN . Specifically, we set the lower end of the grid to be \u03c3 2 1 = 0, which is a point mass at zero, and we set the largest prior variance to be \u03c3 2 K \u2248 n so that the prior variance of x j b j is close to \u03c3 2 (recall, we assumed x T j x j = 1, so Var(x j ) \u2248 1/n when x j is centered). We have found that 20 grid points spanning this range to be good enough to achieve reliable prediction performance across many settings (see Section 5). Based on this, our default grid is the sequence \u03c3 2 k = n(2 (k\u22121)/K \u2212 1) 2 , k = 1, . . . , 20. In rare cases, Var(x j b j ) may be larger than \u03c3 2 for some j. In that case, we may need a larger \u03c3 2 K to avoid underestimating, or \"overshrinking\", the effect b j . Therefore, we suggest checking that the final estimate of \u03c0 K is negligible and, if not, the grid can be made wider.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Initialization and Update Order", "text": "Except in special cases, maximizing F is a nonconvex optimization problem, and so although Algorithm 2 is guaranteed to converge, the solution obtained may depend on initialization ofb, \u03c0, \u03c3 2 , as well as the order in which the coordinate ascent updates cycle through the coordinates j \u2208 {1, . . . , p} (e.g., Carbonetto and Stephens, 2012;Ray and Szab\u00f3, 2022). Therefore, we experimented with different initialization and update orderings.\nThe simplest initialization forb is the \"null initialization\"b = (0, . . . , 0) T . We also consider initializingb to the Lasso solutionb lasso in which the Lasso penalty is chosen via cross-validation. Givenb, we initialize the residual variance as \u03c3 2 = \u2225y \u2212 Xb\u2225 2 /n, and we initialize the mixture weights to \u03c0 = (1/K, . . . , 1/K). In numerical experiments (Appendix C.2) we found that the Lasso initialization often improved predictive performance when columns of X were highly correlated. In other cases, the null and the Lasso initializations performed similarly. With the Lasso initialization, we did not find any systematic benefit to different update orderings. Therefore, our default approach is to use the Lasso initialization with updates performed in the natural order, 1, 2, . . . , p.", "publication_ref": ["b12", "b76"], "figure_ref": [], "table_ref": []}, {"heading": "Termination Criterion", "text": "We stop iterating when estimates of the prior distribution g stabilize; specifically, we stop at iteration t if \u2225\u03c0 (t) \u2212 \u03c0 (t\u22121) \u2225 \u221e < K \u00d7 10 \u22128 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computing the ELBO", "text": "Although Algorithm 2 optimizes the ELBO, F , it does not require computation of the ELBO. Still, it can be useful to compute the ELBO, for example to monitor progress of the coordinate ascent updates, or to compare fits obtained from different runs of the algorithm. Appendix E includes analytic expressions for the ELBO that may be useful in such cases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Extension to Other Mixture Prior Families", "text": "We have focussed on the normal mixture prior family G = G(\u03c3 2 1 , . . . , \u03c3 2 K ) because it makes computations simple, fast, and numerically stable. Furthermore, this prior family includes most prior distributions previously used for multiple regression, and so we expect it to suffice for many practical applications. However, Algorithm 2 could be adapted to accommodate other prior families of the form G = {g = K k=1 \u03c0 k g k : \u03c0 \u2208 S K }, with fixed mixture components g 1 , . . . , g K . When choosing G, there are two important practical points to consider: (i) the convolution of g k with a normal likelihood should be numerically tractable, ideally with an analytic expression; (ii) the posterior mean in the normal means model with prior b j \u223c g k should be easy to compute. Examples of fixed mixture components g k satisfying (i) and (ii) include point masses, uniform distributions and Laplace distributions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Inference and Variable Selection", "text": "We have focussed on developing flexible multiple regression methods for accurate prediction, which requires only a point estimate for b (e.g., the posterior mean,b). However, the approximate posterior distributions from our method could also be used for inference, that is, to assess uncertainty in the estimated b. For example, to assess significance of each variable in the regression, it is easy to compute the local false sign rate, lfsr (Stephens, 2016), which quantifies confidence in the sign of the effect (and which we generally prefer to the closely related local false discovery rate; see Stephens 2016). However, caution is warranted in settings with highly correlated variables: in such settings, the approximate posterior distribution from the fully-factored variational approximation will often be inaccurate. While predictive performance is quite robust to this issue, inference is more sensitive (Carbonetto and Stephens, 2012). Thus, other methods may be preferred for inference with highly correlated variables; see Wang et al. (2020) for further discussion.", "publication_ref": ["b82", "b12", "b94"], "figure_ref": [], "table_ref": []}, {"heading": "Connecting VEB and Penalized Linear Regression", "text": "This section shows that the approximate posterior mean computed by our VEB approach also solves a PLR with a nonconvex penalty, where the form of the penalty is flexible and is automatically learned from the data without need for cross-validation. . . , \u03c3 2 K ) (left-hand panel) and \u03c3 2 that were chosen to mimic the shrinkage operators from some commonly used penalties (right-hand panel).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Penalties and Shrinkage Operators", "text": "Penalized linear regression (PLR) methods estimate the regression coefficients by minimizing a penalized squared-loss function:\nminimize b \u2208 R p h \u03c1 (b) \u225c 1 2 \u2225y \u2212 Xb\u2225 2 + p j=1 \u03c1(b j ),(29)\nfor some penalty function \u03c1 : R \u2192 R. As mentioned above, the PLR problem ( 29) is often tackled using coordinate descent algorithms; that is, by iterating over the coordinates of b 1 , . . . , b p sequentially, at each iteration solving (29) for one coordinate b j while keeping the remaining coordinates fixed. Assuming the columns of X are scaled such that x T j x j = 1, the solution for the jth coordinate is obtained by\nb j \u2190 S \u03c1 (b j + x T j (y \u2212 Xb)),(30)\nwhere\nS \u03c1 (t) \u225c argmin \u03b8 \u2208 R 1 2 (t \u2212 \u03b8) 2 + \u03c1(\u03b8)(31)\nis the \"shrinkage operator\" (or \"univariate proximal operator\"; Parikh and Boyd 2014) for penalty \u03c1. Studying the shrinkage operator S \u03c1 is often helpful for understanding the behaviour of its corresponding penalty, \u03c1. For example, the shrinkage operators for the L 0 and L 1 penalties both have a \"thresholding\" property in which coefficient estimates t less than some value are driven to zero. This thresholding property tends to produce sparse solutions to (29); see Figure 1 for an illustration. Table 3 in Appendix A gives some commonly used penalty functions and their corresponding shrinkage operators.\nIn the next section, we show that our VEB approach can be interpreted as solving a PLR problem with a penalty function \u03c1 g\u03c3,\u03c3 that depends on the prior g \u03c3 and the residual variance \u03c3 2 . This penalty function has a corresponding shrinkage operator, denoted by S g\u03c3,\u03c3 , that has a particularly simple form and interpretation: it is the posterior mean under a normal means model. It is formally defined as follows.\nDefinition 3 (Normal Means Posterior Mean Operator) Define the normal means posterior mean operator, S f,\u03c3 : R \u2192 R, as the mapping\nS f,\u03c3 (y) \u225c E NM (b | y, f, \u03c3 2 ), (32\n)\nwhere E NM denotes the posterior expectation under the following normal means model with prior f and variance \u03c3 2 ,\ny | b, \u03c3 2 \u223c N (b, \u03c3 2 ) b \u223c f.(33)\nFrom ( 13), S f,\u03c3 has a simple analytic form when f = g \u03c3 , g \u2208 G(\u03c3 2 1 , . . . , \u03c3 2 K ):\nS g\u03c3,\u03c3 (y) = K k=1 \u03d5 k (y; g, \u03c3 2 ) \u00b5 k (y; g, \u03c3 2 ). (34\n)\nIt is easy to show that S g\u03c3,\u03c3 is an odd function and is monotonic in y. Also, S g\u03c3,\u03c3 is a shrinkage operator, in that |S g\u03c3,\u03c3 (y)| \u2264 |y|; see Lemma 12 in Appendix G. Indeed, given a suitable choice of prior family, the shrinkage operator S g\u03c3,\u03c3 can qualitatively mimic the behavior of many commonly used shrinkage operators (Figure 1). The behavior of S g\u03c3,\u03c3 naturally depends on the prior. For example, the more mass the prior places near zero, the stronger the shrinkage toward zero. Our VEB method estimates the prior from within a flexible family capable of capturing a wide range of scenarios; consequently, the corresponding shrinkage operator is also estimated from a flexible family of shrinkage operators. This process is analogous to estimating the tuning parameters in regular PLR methods which is usually done by cross-validation (CV). However, our VEB approach dispenses with CV and makes it possible to efficiently tune across a much wider range of shrinkage operators.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "VEB as Penalized Linear Regression", "text": "The first step to connecting our VEB approach to a PLR is to write it as an optimization over the regression coefficients, b, rather than an optimization over the (approximate) posterior distributions, q. To do this, we define an objective function:\nh(b, g, \u03c3 2 ) \u225c \u2212 max q \u2208 Q, Eq[b] =b F (q, g, \u03c3 2 ) . (35\n)\nThe constraint E q [b] =b means that the expected value of b with respect to q isb. The negative sign is introduced to align with the convention that PLRs are usually minimization problems, as in (29). Any algorithm for optimizing F over q (and possibly g, \u03c3 2 ) also provides a way to optimize h overb (and possibly g, \u03c3 2 ), as formalized in the following proposition.\nProposition 4 (Computing Posterior Mean as an Optimization Problem) Letq, g,\u03c3 2 be a solution toq ,\u011d,\u03c3 2 = argmax\nq \u2208 Q, g \u2208 G, \u03c3 2 \u2208 T F (q, g, \u03c3 2 ),\nwhere Q is the variational mean-field family of approximate posterior distributions (20), G is any family of prior distributions on b \u2208 R, and T is any subset of R + . (This general formulation allows, as a special case, g, \u03c3 2 to be fixed by taking G and T to be singleton sets.) Letb denote the expected value of b with respect toq. Thenb,\u011d,\u03c3 2 also solves the following optimization problem:\nb,\u011d,\u03c3 2 = argmin b \u2208 R p , g \u2208 G, \u03c3 2 \u2208 T h(b, g, \u03c3 2 ).\nProof See Appendix G.\nThe final step to connecting VEB with PLR is to show that h has the form of a penalized squared-loss function.\nTheorem 5 (VEB as a Penalized Log-likelihood) The objective function h defined in (35) has the form of a PLR,\nh(b, g, \u03c3 2 ) = 1 2\u03c3 2 \u2225y \u2212 Xb\u2225 2 + 1 \u03c3 2 p j=1 \u03c1 g\u03c3,\u03c3 (b j ) + n \u2212 p 2 log(2\u03c0\u03c3 2 ),(36)\nin which the penalty function \u03c1 f,\u03c3 satisfies\n\u03c1 f,\u03c3 (S f,\u03c3 (y)) = \u2212\u03c3 2 \u2113 NM (y; f, \u03c3 2 ) \u2212 1 2 (y \u2212 S f,\u03c3 (y)) 2 , (37\n)\nand \u03c1 \u2032 f,\u03c3 (S f,\u03c3 (y)) = (y \u2212 S f,\u03c3 (y)).(38)\nHere, \u2113 NM (y; f, \u03c3 2 ) \u225c log p(y | f, \u03c3 2 ) denotes the marginal log-likelihood under the NM model ( 33), and S f,\u03c3 denotes the shrinkage operator (32).\nProof See Appendix G.\nFrom this theorem, it follows that the NM posterior mean shrinkage operator S f,\u03c3 (32) can be also written in the form of (31), a shrinkage operator for the penalty \u03c1 f,\u03c3 . Explicit computation of \u03c1 f,\u03c3 (b) for a givenb in (37) would require computing the inverse shrinkage operator, y = S \u22121 f,\u03c3 (b). This inverse exists because the shrinkage operator (34) is strictly increasing; however, we do not have an analytic expression for the inverse, so we do not have an analytic expression for \u03c1 f,\u03c3 (b).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Special Case When g and \u03c3 2 Are Fixed", "text": "The special case when g and \u03c3 2 are fixed is particularly simple and helpful for intuition. In this case, the VEB approach is solving a PLR problem with fixed penalty \u03c1 g\u03c3,\u03c3 and Algorithm 3 Coordinate Ascent Iterative Shrinkage Algorithm for Variational Posterior Mean (with fixed g, \u03c3 2 ) Require: X \u2208 R n\u00d7p , y \u2208 R n , \u03c3 2 > 0, prior g, and initial estimatesb. repeat for j \u2190 1 to p d\u014d b j \u2190 S g\u03c3,\u03c3 (b j + x T j (y \u2212 Xb)) end for until convergence criteria is met returnb shrinkage operator S g\u03c3,\u03c3 . This leads to a simple coordinate ascent algorithm (Algorithm 3). Compare this with the inner loop of Algorithm 2 which maximizes the ELBO, F , over each q j in turn. The key computation in the inner loop is the computation of the posterior mean,b j . (When g and \u03c3 2 are fixed, computing \u03d5 jk and \u00b5 jk is needed only to computeb j .) Further, by Proposition 1, this value is computed as the posterior mean under a simple NM model, which is given by the shrinkage operator S g\u03c3,\u03c3 .\nIn summary, for fixed g, \u03c3 2 , Algorithm 2 can be reframed as a coordinate ascent algorithm for PLR, which is Algorithm 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Special Case of a Normal Prior (Ridge Regression)", "text": "When the prior, g, is a fixed normal distribution with zero mean, the NM posterior mean shrinkage operator S g\u03c3,\u03c3 is the same as the ridge regression (or L 2 ) shrinkage operator (Table 3) and the penalty function \u03c1 g\u03c3,\u03c3 is the L 2 -penalty. Thus, in this special case, Algorithm 3 is solving ridge regression (i.e., PLR with L 2 -penalty), which is a convex optimization problem. Furthermore, in this special case Algorithm 3 converges to the exact posterior mean of b because the posterior is multivariate normal, and therefore the posterior mean is equal to the posterior mode. Thus, in this special case, even though the variational posterior approximation q does not exactly match the true posterior-the true posterior does not factorize as in ( 20)-the variational posterior mean recovers the true posterior mean.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Posterior Mean vs. Posterior Mode", "text": "Traditional PLR approaches are sometimes motivated from a Bayesian perspective as computing a posterior mode estimate for b-i.e., a maximum a posteriori (MAP) estimate-in which the penalty term corresponds to some prior on b (Fu, 1998). For example, the Lasso is the MAP with a Laplace (\"double-exponential\") prior (Figueiredo, 2003;Park and Casella, 2008;Tibshirani, 1996). By contrast, the variational approach seeks the posterior mean, not the posterior mode, and likewise the VEB shrinkage ( 32) is based on an averaging (mean) operation instead of the usual maximization (mode). Our formulation of the (approximate) posterior mean as solving a PLR is new, at least as far as we are aware, and this formulation may be useful in other settings.\nFrom a Bayesian decision-theoretic perspective (e.g., Chapter 4 of Berger 1985), the posterior mean for b has better theoretical support than the posterior mode; not only does it minimize the expected mean-squared error in b, it also minimizes the expected mean-squared error in the predicted, or \"fitted,\" responses\u0177 i = (x i1 , . . . , x ip ) T b. Although the posterior mode can have attractive properties-for example, the posterior mode with a Laplace prior is sparse whereas the posterior mean with a Laplace prior is not-the posterior mode has very little support as an estimator for b, particularly when predicting y is the main goal. To illustrate this, consider a spike-and-slab prior (Mitchell and Beauchamp, 1988) with non-zero mass at zero: the posterior mode is always b = 0, which will generally provide poor prediction performance. Also, consider that ifb mode is the posterior mode of b, then (x i1 , . . . , x ip ) Tbmode is not generally the posterior mode of the fitted value\u0177 i .", "publication_ref": ["b30", "b27", "b73", "b86", "b4", "b68"], "figure_ref": [], "table_ref": []}, {"heading": "Numerical Experiments", "text": "We used simulations to empirically assess the predictive performance of our proposed method and compare it with other methods. We call the proposed method multiple regression with adaptive shrinkage priors, or \"Mr.ASH\". Mr.ASH is implemented in the R package mr.ash.alpha, available at https://github.com/stephenslab/mr.ash.alpha. In the experiments, we used mr.ash.alpha version 0.1-33 (git commit id 0845778). The source code and analysis steps used to generate the results of our numerical experiments are included in a separate repository on GitHub, https://github.com/stephenslab/mr-ash-workflow.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Methods Compared", "text": "We compared Mr.ASH with 12 different methods (Table 1). These include a range of well established and recently developed methods based on both PLR and Bayesian ideas. These methods vary in (1) the choice of penalty or prior (and possibly other modeling assumptions); and (2) the algorithm used to fit the model (e.g., point estimation of b vs. approximate posterior inference of b via MCMC or variational inference). When selecting methods to compare, we tried to choose methods that were publicly available as open source software and that were well maintained and documented. Since Mr.ASH and many other popular large-scale linear regression methods are implemented in R, we sought out methods implemented in R (R Core Team, 2019).\nHere we give a brief overview of the different methods and point out some of their expected strengths and weaknesses:\n\u2022 Ridge regression (Hoerl and Kennard 1970) and the Bayesian Lasso (Park and Casella 2008;Perez and de los Campos 2014) are well adapted to dense signals, so should be competitive in such settings. On the other hand, they may perform poorly for sparse signals.\n\u2022 L0Learn (Hazimeh and Mazumder, 2020) and SuSiE (Wang et al., 2020) are better adapted to sparse signals, so they should perform well in such settings. On the other hand, their assumptions are poorly suited to more dense signals.\n\u2022 The Lasso (Tibshirani, 1996) is one of the most widely used PLR methods. Computing the Lasso estimator is a convex optimization problem. One of the well-studied issues is that Lasso estimates can suffer from bias by overshrinking the strongest signals (e.g., Su et al., 2017;Javanmard and Montanari, 2018).\n\u2022 The Elastic Net (Zou and Hastie 2005) is another widely used convex PLR method. It has two tuning parameters. The Elastic Net penalty is more flexible than the Lasso and ridge regression and, indeed, it includes both as special cases. The Elastic Net may therefore perform well across a wider range of settings, at the cost of increased computation in the parameter tuning.\n\u2022 SCAD, MCP, the Spike-and-Slab Lasso (SSLasso) and the Trimmed Lasso are methods based on nonconvex or adaptive penalties that were designed, in part, to address limitations of the Lasso penalty (Bai et al., 2021;Bertsimas et al., 2017;Breheny and Huang, 2011;Ro\u010dkov\u00e1 and George, 2018;Amir et al., 2021;Yun et al., 2019). They might therefore outperform the Lasso (and the Elastic Net), possibly at the cost of some additional computation. Since these methods were primarily developed with sparse regression in mind, they may not always perform as well for dense signals.\n\u2022 BayesB is a Bayesian regression method with a \"spike-and-slab\" prior, in which the \"slab\" is a t distribution (Meuwissen et al., 2001;Perez and de los Campos, 2014). It has the potential to perform well for both sparse and dense signals. One concern is that the Markov chain may or may not be simulated long enough so as to adequately explore the posterior distribution.\n\u2022 varbvs (Carbonetto and Stephens, 2012;Carbonetto et al., 2017) and Mr.ASH both compute approximate posteriors using the same mean field variational approximation. Compared to varbvs, Mr.ASH features a more flexible prior, and uses a simpler and more efficient empirical Bayes approach to estimate the prior. Mr.ASH also uses an initialization based on the Lasso, which, as we show below, can improve the model fit, particularly when the predictors are strongly correlated, or correlated in complex ways. We expect Mr.ASH to outperform varbvs some settings, particularly in \"dense\" settings when many of the predictors affect the outcome, or when the predictors are strongly correlated.\nAs an additional point of comparison in \"sparse\" data simulations, we also show results for the \"Oracle OLS\" method, which is the ordinary least-squares (OLS) estimate of b conditional on knowledge of which coefficients b j are non-zero. This can be considered a lower bound on the achievable prediction error when the number of non-zero coefficients is small. (When the number of non-zero coefficients is large, the Oracle OLS will perform poorly, so we do not include the Oracle OLS result in settings with many non-zero coefficients.)\nA factor that inevitably complicates comparisons is that most methods have many options and tuning parameters. Even a relatively straightforward method such as the Lasso has multiple tuning parameters that can affect performance, some of which involve tradeoffs in computing effort versus prediction accuracy: number of folds to use in the K-fold crossvalidation step; what criterion to use for selecting the optimal penalty strength parameter; whether to \"relax\" the fit; etc. For each method, we tried to follow the recommendations  given in the software documentation or in published papers, and in some cases we changed the default settings to improve performance. However, with so many methods to compare, it was infeasible to find the best settings for each method. Appendix B includes additional information on how the methods were run in these experiments.", "publication_ref": ["b47", "b73", "b74", "b46", "b94", "b86", "b84", "b49", "b107", "b6", "b11", "b79", "b0", "b101", "b65", "b74", "b12", "b13"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Design of Simulations", "text": "To test the methods in a wide variety of settings, we designed five sets of simulations, which we refer to as \"Experiment 1\" through \"Experiment 5.\" In each set of simulations, we varied one aspect of the simulation while keeping the other aspects fixed.\n\u2022 In Experiment 1, we varied the \"sparsity level\"; that is, the proportion of variables with non-zero coefficients. We denote the sparsity level by s, so that s = 1 is the sparsest model (with only a single non-zero coefficient) and s = p is the densest model (all variables affect y).\n\u2022 In Experiment 2, we varied the \"total signal strength\"; specifically, the proportion of variance in the response y that is explained by X. We refer to this parameter as \"PVE\", short for \"proportion of variance explained.\"\n\u2022 In Experiment 3, we considered different distributions for the non-zero coefficients.\nWe use h to denote the distribution that was used to simulate the non-zero coefficients.\n\u2022 In Experiment 4, we varied the number of predictors, p.\n\u2022 In Experiment 5, we simulated residual errors (noise) in different ways. This was done to assess how departures from the assumption of normally-distributed noise-an assumption made by most methods-might affect performance.\nThese experiments focus on the case p > n. However, we note that Mr.ASH also performs well in the easier case where p \u226a n; see Appendix C.1. In all the experiments, we took the following steps to simulate each data set:\n\u2022 First, we generated the n \u00d7 p design matrix, X. We considered three types of design matrices: (1) independent variables, in which the individual observations x ij were simulated i.i.d. from the standard normal distribution;\n(2) correlated variables, in which each row of X was an independent draw from the multivariate normal distribution with mean zero and a covariance matrix diagonal entries set to 1 and off-diagonal entries set to \u03c1 \u2208 [0, 1]; and (3) real genotype data, in which X was a genotype data matrix from the Genotype-Tissue Expression (GTEx) project (GTEx Consortium, 2017).\n(Specifically, we used the processed genotype data sets generated in Wang et al. 2020.) In these data sets, the variables were genetic variants-specifically, single nucleotide polymorphisms, or \"SNPs\"-and the SNPs exhibited complex correlation patterns. Some SNP pairs had very strong correlations, approaching 1 or \u22121. Each genotype matrix X contained the genotypes of all SNPs within 1 Megabase (Mb) of a gene's transcription start site after filtering out SNPs with minor allele frequencies less than 5% (see Wang et al. 2020 for details). Among the thousands of data sets used in Wang et al. 2020, we randomly selected 20 data sets for our simulations. Unless otherwise stated, we simulated independent variables with n = 500 and p = 1,000. For the genotype data sets, n = 287, and p ranged from 4,012 to 8,760. Each genotype matrix X was centered and scaled so that the mean of each column was zero and its standard deviation was 1. The other data matrices were not centered or scaled.\n\u2022 We chose s, the number of non-zero coefficients. Unless stated otherwise we set s = 20. We selected the indices j \u2208 {1, . . . , p} of the s non-zero coefficients uniformly at random among the p variables.\n\u2022 We simulated the s non-zero coefficients b j i.i.d. from some distribution, h. We used the following distributions: standard normal; uniform on [\u22121, 1]; double-exponential (Laplace) distribution centered at zero with variance 2\u03bb 2 , \u03bb = 1 (Gelman et al., 2013); t-distribution with 1, 2, 4 and 8 degrees of freedom; and a point mass (so that all coefficients were the same). Unless stated otherwise, we simulated the coefficients from the standard normal distribution.\n\u2022 Finally, we simulated the responses y i = p j=1 x ij b j + e i , where e i was drawn from some noise distribution. We used the following noise distributions: normal with mean zero; uniform distribution with mean zero; double-exponential (Laplace) distribution centered at zero; and t-distribution with 1, 2, 4 and 8 degrees of freedom. In all cases, the variance of the noise distribution was adjusted to attain the target PVE; specifically, denoting the variance of the noise distribution by \u03c3 2 , we set it to \u03c3 2 = Var(Xb) \u00d7 1\u2212PVE PVE . Unless stated otherwise, we set PVE = 0.5, and simulated e i \u223c N (0, \u03c3 2 ), with \u03c3 2 adjusted to attain the target PVE of 0.5.\nWe repeated the simulations 20 times for each simulation setting in Experiments 1-5. The test sets used to evaluate the model fits were the same size as the training sets and generated using the same coefficients b.", "publication_ref": ["b94", "b94", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation", "text": "Each method returnsb, an estimate of the regression coefficients. We evaluated this estimate using the (scaled) root mean squared prediction error in the test data:\nRMSE-scaled(y test ,b) \u225c RMSE(y test ,b) RMSE(b = 0) ,(39)\nwhere\nRMSE(y test ,b) \u225c 1 \u221a n \u2225y test \u2212 X testb \u2225. (40\n)\nRMSE(b = 0) \u225c \u03c3/ \u221a\n1 \u2212 PVE denotes the expected RMSE for the \"null predictor\",b = 0. The value of \"RMSE-scaled\" will vary from \u221a 1 \u2212 PVE (for the \"oracle\" predictor) to approximately 1 (for the null predictor); however, values greater than 1 are possible ifb performs worse than the null predictor.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We present the results of Experiments 1-5 in Sections 5.4.1-5.4.5, and we give a high-level summary in Section 5.4.6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment 1-Varying the Sparsity Level", "text": "In the first set of simulations, we varied s, the number of non-zero predictors, which controls the model sparsity when p is fixed. The results of these simulations, summarized in Figures 2 and 3, highlight differences in performance between the methods that were better suited to a particular level of sparsity versus the methods that better adapted to different levels of sparsity. For example, SuSiE, L0Learn with the L 0 penalty and the Trimmed Lasso better adapted to sparse settings and often performed poorly in dense settings; by contrast, ridge regression and the Bayesian Lasso are better adapted to dense settings, and as expected performed worse in sparse settings.\nOther methods performed more consistently across different sparsity levels. In particular, the Lasso and Elastic Net performed somewhat similarly, with the Elastic Net usually performing slightly better, but in many settings there was a noticeable gap in performance between these two methods compared with the best performing method. The nonconvex, penalty-based methods MCP and SCAD performed similarly to one another, and were competitive in many settings, except in denser-signal data sets in some scenarios (e.g., in the low-dimension settings, with p = 200 non-zero coefficients). The Spike-and-Slab Lasso (SSLasso) with the adaptive penalty performed competitively across a range of sparse to dense settings when predictor variables were independent, but it performed less well (and sometimes very poorly) in settings with correlated predictors.\nMr.ASH was competitive at all sparsity levels, consistently achieving the best performance or close to the best in almost all simulation settings. Mr.ASH tended to outperform other methods in data sets with correlated predictors, with a couple of exceptions: the Bayesian Lasso had better prediction accuracy in settings with the densest signals, and SuSiE was better in some of the sparse simulation settings with genotype data. One possible explanation for this is that the mean-field variational approximation used by Mr.ASH is less appropriate in settings with strongly correlated predictors, whereas the Bayesian Lasso  Overall, these results illustrate the versatility of Mr.ASH and the effectiveness of the VEB approach to adapt to different sparsity levels by adapting the prior-and therefore penalty-to the data. Two other results stand out. First, BayesB performed poorly in the simulations with correlated predictors. In principle, correlated predictors should not cause problems for Bayesian methods such as BayesB. We suspect that this reflects failure of the MCMC to converge and that the performance of BayesB (and the Bayesian Lasso) would improve with longer MCMC runs.\nSecond, although varbvs is based on the same variational approximation as Mr.ASH, its prediction accuracy was generally worse than Mr.ASH. This is particularly evident in some settings with dense signals, probably because the default settings in varbvs are designed to favor sparse priors. Also, varbvs performed worse than Mr.ASH in some data sets with correlated predictors, probably because varbvs does not put as much effort into initialization. (See Appendix C.2 for investigations of the impact of initialization on the performance of Mr.ASH.)", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Experiment 2-Varying Total Signal Strength", "text": "We did not expect strong systematic differences in performance as the total signal strength (PVE) varied. Results for sparse simulations (top half of Figure 4) generally matched these expectations; the performance curves of the different methods generally did not cross as the PVE varies. And, among the methods compared, Mr.ASH consistently performed among the best. However, results in dense simulations (bottom half of Figure 4), showed a different pattern: some methods that performed competitively at moderate PVE were no longer competitive at high PVE. This included Mr.ASH, which was unexpected because Mr.ASH should, in principle, adapt well to both sparse and dense signals. Further, while Mr.ASH includes ridge regression as a special case, the ridge regression estimates yielded better predictions than Mr.ASH in these dense data sets, suggesting a failure of the variational EB approach to appropriately adapt the prior. We believe that this failure occurred because the fully factorized (mean-field) variational method has a tendency in some settings to favor sparse priors over dense priors; under sparse priors, the true posterior distribution comes close to factorizing so the gap between the ELBO and the true evidence is small (i.e., the ELBO is a tight lower bound), whereas under dense priors with large p there are stronger dependencies in the posterior, especially when the PVE is large. Thus, the factorization assumption is more strongly violated and the ELBO underestimates the evidence more. Since our EB approach seeks to optimize the ELBO in place of the true evidence, this creates a bias towards estimating sparse priors rather than dense priors. In this sense, the VEB approach could be characterized as leaning towards the \"bet on sparsity\" principle (Hastie et al., 2009, p. 610), which argues that one should \"use a procedure that does well in sparse problems, since no procedure does well in dense problems.\" These simulations therefore suggest a situation (dense signal, high PVE, p = 2n) where this principle fails.\nThe good performance of BayesB in these simulations with dense signals and high PVE suggests an advantage of MCMC over the variational approximation in dense scenarios in which many predictors have a small effect on the regression outcome. Consistent with ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment 3-Varying Signal Distribution", "text": "For most methods, the distribution used to simulate the coefficients, h, had only a small impact on performance (Figure 5). There were a few exceptions to this. For example, in dense settings the ridge regression method struggled with long-tailed effect-size distributions such as the t-distribution with 1 degree of freedom. Consider that, when h is long-tailed, often a small number of coefficients will dominate, so the normal prior in ridge regression is poorly suited for this case. The SSLasso also performed poorly in this setting for reasons that remain unclear to us.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment 4-Varying the Number of Predictors", "text": "This experiment assessed how prediction accuracy and computational effort change with the number of predictors, p (Figure 6). In general, running times for different methods increased similarly with p. For large p, SuSiE, L0Learn and the Lasso were the fastest methods. The Elastic Net was considerably slower than other methods because we tuned both of the Elastic Net parameters by CV whereas other methods tuned by CV involved tuning only one parameter. (The Elastic Net could be run faster by tuning only one parameter but at the risk of losing accuracy in some settings.) Fitting the Mr.ASH prior involved tuning a large number of parameters, but because it tuned these parameters via VEB rather than by CV, Mr.ASH ended up being roughly as fast as methods that tune a single parameter by CV. This is an important benefit of the VEB approach.\nThe long running times for the Trimmed Lasso were due to running the algorithm at several different settings of its \"target sparsity level\" parameter ('k'), and the Trimmed Lasso was slow when k was large (even with the settings that were suggested in the software documentation for larger data sets). When k was small, say, less than 10, the Trimmed Lasso running times were comparable to the other methods.\nAn important detail is that a large fraction of the effort in running Mr.ASH was due to running the Lasso (which was used to initialize Mr.ASH). The Lasso initialization often greatly reduced the number of iterations required for the Mr.ASH coordinate ascent updates to converge, so the total running time of Mr.ASH with Lasso initialization was often not much greater than Mr.ASH with null initialization.\nAlthough MCMC methods have a reputation for being slow, here the MCMC-based methods-the Bayesian Lasso and BayesB-had similar running times to other methods. The running time of these methods also increased approximately linearly with p because the defaults in the software implementations set the number of iterations proportional to p. (The per-iteration cost is independent of p when the model configurations are sparse.) However, as p increased, the relative prediction performance of these methods got worse. For the Bayesian Lasso, this was probably due to the fact that we fixed s (the number of non-zero coefficients) to simulate the data sets, so simulations with larger p were based on sparser models, and the Bayesian Lasso tends to be less competitive in sparser settings. For BayesB, the reduction in prediction accuracy may instead reflect a failure of the Markov chain to adequately explore the posterior distribution, and perhaps better performance could have been achieved by running the MCMC longer (at increased computational cost). Nonetheless, it is interesting that BayesB obtained better prediction accuracy than the Lasso at roughly the same computational effort.\nTo summarize, Mr.ASH consistently achieved the best prediction accuracy in these simulations, with computational effort comparable with the fastest methods such as L0Learn and Lasso. ASH results are included in all plots to provide a common reference point. Running times for all methods except Trimmed Lasso are from running the methods in R 3.5.1 (R Core Team, 2019) on a machine with a quad-core 2.6 GHZ Intel Core i7 processor and 16 GB of memory. R was installed from macOS binaries and we used the BLAS libraries that were distributed with R. The Trimmed Lasso was run using MATLAB 9.13 on machines with 4 Intel Xeon E5-2680v4 (\"Broadwell\") processors and 24 GB of memory. The Mr.ASH running times include running the Lasso to initialize the estimates; the running times for Mr.ASH with a \"null\" initialization (b = 0) are also shown for comparison.   ASH results are included in all plots to provide a common reference point.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Experiment 5-Varying Noise Distribution", "text": "In our final set of experiments, we simulated data sets with different noise distributions. Reassuringly, most methods were largely insensitive to the noise distribution (Figure 7). However, the Lasso and Elastic Net both performed poorly in sparse settings when the noise was very heavy-tailed (t distribution with small degrees of freedom). We do not have an explanation for this result, which we have not seen previously.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Summary of the Results", "text": "The five experiments highlighted some differences in performance and behaviour among the multiple regression models. In Figure 8, we give a higher level summary of the results across all five of the experiments. To produce this summary, for each simulation t we calculated the relative prediction accuracy as the ratio of the RMSE to the best RMSE achieved in", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "Relative prediction error (RRMSE)", "text": "A varying s, independent variables 1.0   in simulation t. Defined in this way, the RRMSE can never be smaller than 1 and the most accurate method in a given simulation has an RRMSE of 1. Figure 8 highlights the consistently good prediction accuracy of Mr.ASH compared with the other methods across a range of settings; in all the panels (A-F), Mr.ASH's performance was the best, or close to the best, among all methods compared, and was rarely much worse than the best method. The benefits of Mr.ASH were more mixed in the simulations with correlated predictors (Panel B). Yet, even in these simulations, Mr.ASH remained competitive, achieving an average prediction accuracy that was among the best. This good accuracy was also obtained efficiently with a computational effort that was not much higher than the fastest methods such as L0Learn and the Lasso (Table 2).\n1.1 1.2 1.3 1.0 1.1 1.2 1.3 1.0 1.1 1.2 1.3 1.0 1.1 1.2 1.", "publication_ref": [], "figure_ref": ["fig_7"], "table_ref": ["tab_5"]}, {"heading": "Discussion", "text": "We have presented a new VEB method for multiple linear regression with a focus on fast and accurate prediction. This VEB method combines flexible shrinkage priors with variational methods for efficient posterior computations. Variational methods and EB methods are sometimes criticized because of their tendency to understate uncertainty compared with \"fully Bayesian\" methods; see Morris (1983), Wang and Titterington (2005) and references therein for discussion. However, for some applications uncertainty is of secondary importance compared with speed and accuracy of point estimates. For example, speed and accuracy is often important when multiple regression is used simply to build an accurate predictor for downstream use (see Gamazon et al. 2015 for one such application). Our VEB approach seems particularly attractive for such uses.\nA natural next step would be to produce similar VEB methods for non-Gaussian (i.e., generalized) linear models (McCullagh and Nelder, 1989). Extension of our methods to logistic regression should be possible via additional approximations that allow for efficient analytic computations (Carbonetto and Stephens, 2012;Carbonetto et al., 2017;Jaakkola and Jordan, 2000;Marlin et al., 2011;Bishop, 2006;Wang and Blei, 2013). Extension to other types of outcome distributions and link functions should also be possible but may require more work.\nOur work also illustrates the benefits of an EB approach in an important and well studied statistical problem. While there is much theoretical (Johnstone and Silverman, 2004) and empirical (Efron, 2008) support for the benefits of EB approaches, EB approaches have not been widely adopted outside of specific research topics such as wavelet shrinkage (Johnstone and Silverman, 2005) and moderated estimation in gene expression studies (Smyth, 2004;Lu and Stephens, 2016;Zhu et al., 2019). Recent work has highlighted the potential for EB methods in other applications, including smoothing non-Gaussian data (Xing et al., 2021), multiple testing (Stephens, 2016;Sun and Stephens, 2018;Urbut et al., 2019;Gerard and Stephens, 2020), matrix factorization (Wang and Stephens, 2021) and additive models (Wang et al., 2020). We hope that these examples, including our work here, will inspire readers to apply EB approaches to new problems.\nElastic Net involves two tuning parameters: the penalty strength parameter \u03bb and the \"mixing\" parameter \u03b1. glmnet does not provide an automated way to chose \u03b1 by CV, so we ran cv.glmnet for 11 settings of \u03b1 ranging from 0 to 1 then we chose (\u03b1, \u03bb) minimizing the 10-fold CV error.\nSCAD and MCP. We called the cv.ncvreg function from the R package ncvreg which performs k-fold CV to select the regularization parameter. We called cv.ncvreg with nfolds = 10 and penalty = \"SCAD\" or penalty = \"MCP\". We kept other settings at their defaults. By default, cv.ncvreg standardized X and added an intercept to the model.\nL0Learn. We called the L0Learn.cvfit function from the R package L0Learn which performs k-fold CV to select the penalty strength parameter \u03bb 0 . We called L0Learn.cvfit with penalty = \"L0\" and nFolds = 10. We chose the setting of \u03bb with the smallest (mean) CV error. We kept other settings at their defaults. By default, L0Learn.cvfit included an intercept in the model. SSLasso. We called the SSLASSO function from the R package SSLASSO which fits coefficients paths for spike-and-slab linear regression models over a grid of values for the \u03bb 0 regularization parameter. We used the \"adaptive\" variant of the Spike-and-Slab Lasso which was recommended over the \"separable\" variant. This function automatically standardizes X and the model includes an intercept. SSLASSO did not provide an automated way to select \u03bb 0 so we performed 5-fold CV and chose the setting of \u03bb 0 minimizing the CV error. We did not perform CV to choose the \"spike\" penalty parameter \u03bb 1 ; Ro\u010dkov\u00e1 and George (2018) showed the performance is less sensitive to the choice of \u03bb 1 .\nTrimmed Lasso. We downloaded the MATLAB code for the Trimmed Lasso from https: //github.com/tal-amir/sparse-approximation-gsm and we compiled the MEX file using gcc 10.2.0. We called function sparse approx gsm v1 22 with the following settings: sparse approx gsm v1 22(X,y,k,'profile','fast') in which the target sparsity level k was one of {1, 5, 20, 100, 500, 2000, 10000}. The sparsity level was chosen by 5-fold CV, taking the setting that minimized the average CV error.\nBayesB and Bayesian Lasso We called the BGLR function from the BGLR package, which simulates the posterior using a Gibbs sampler. We called BGLR with the following settings: standardize = FALSE, nIter = 1500, burnIn = 500 and model = \"BayesB\" or model = \"ML\". We kept other settings at their defaults. By default, an intercept was included in the model. varbvs. We called the varbvs function from the varbvs package which fits an approximate posterior distribution for a Bayesian variable selection model using variational inference methods. All settings were kept at their defaults. The model included an intercept and X was not standardized.\nSuSiE. We called the susie function from the susie package which fits a SuSiE model using the iterative Bayesian stepwise selection (IBSS) algorithm. We called susie with standardize = FALSE and we set the upper bound on the number of \"single effects\" to 20. ", "publication_ref": ["b69", "b92", "b31", "b64", "b12", "b13", "b48", "b62", "b8", "b93", "b50", "b22", "b51", "b81", "b60", "b106", "b98", "b82", "b85", "b89", "b36", "b95", "b94", "b79"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Appendix C. Additional Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Simulations with p < n", "text": "Although the paper focusses on large-scale multiple linear regression with many predictor variables, Mr.ASH can also be applied in settings with p \u226a n where one would expect ordinary least squares (OLS) to work well. To illustrate this, we simulated data sets with PVE = 0.5, n = 200, p \u2264 64 and s = p (all predictors had non-zero coefficients). The results show that Mr.ASH performs similarly to OLS when p is very small and outperforms the OLS estimate as p increases (Figure 9).", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "C.2 Impact of Initialization and Update Order on Prediction Accuracy", "text": "Since Mr.ASH is solving a nonconvex optimization problem, and therefore is only guaranteed to converge to a local optimum (except in special cases), the quality of the solution-and hence the accuracy of the predictions-can be sensitive to initialization. This situation is similar to other methods such as SCAD that solve nonconvex optimization problems, but different from methods such as the Lasso that solve a convex optimization problem and are therefore guaranteed to end up with the same final estimates irrespective of initialization (provided of course that the algorithm is given enough time to converge to the solution). Additionally, the order in which the coordinatewise updates are performed can also affect which local solution the Mr.ASH algorithm converges to (Ray and Szab\u00f3, 2022). For these nonconvex optimization problems, sometimes a \"smart\" initialization or update order can lead to a better local solution. From experience, we have found that initializing Mr.ASH to the cross-validated Lasso estimate of b seems to work well and does not greatly increase computational effort. In this experiment, we investigated the benefits of a smart initialization. Specifically, we compared the following four Mr.ASH variants:\n\u2022 \"Null\" initialization. The posterior mean coefficientsb are initialized to zero and the coordinates j = 1, . . . , p are updated in a random order. By \"random order\", we mean a random permutation of the indices 1, 2, . . . , p. A new random permutation is generated for each iteration of the algorithm (that is, for each iteration of the repeat-until loop in Algorithm 2).\n\u2022 Lasso initialization. The posterior mean coefficients are set to b =b lasso (see Section 3.5.3), and the coordinates j = 1, . . . , p are updated in a random order.\n\u2022 Lasso update order. The coordinates j = 1, . . . , p are updated in the order that they are estimated to have non-zero coefficients as the strength of the Lasso penalty is decreased. We call this the \"Lasso update order,\" and it can be understood as the order in which the coefficients \"enter the Lasso path\" (Su et al., 2017). (Note that determining the Lasso update order typically takes less effort than computingb lasso because the cross-validation step is avoided.)\n\u2022 Lasso initialization and Lasso update order. Both the Lasso initializationb = b lasso and Lasso update order are used.\nInitialization of \u03c3 2 and \u03c0 is described in Section 3.5.3.\nTo assess the benefits of these four initialization and update order strategies, we simulated data sets with varying correlation strengths among the predictors, then we compared the performance of the four Mr.ASH variants. The results of these simulations are summarized in Figure 10. The smart initialization and update ordering provided little benefit when the variables were not correlated or only weakly correlated, but produced considerable gains in prediction accuracy when the variables were strongly correlated. Interestingly, once the coefficients were initialized to the Lasso estimates, there was no additional benefit to updating the coordinates using the Lasso update order.\nIn summary, initializing the coefficients to the cross-validated Lasso estimates is a simple way to improve the performance of Mr.ASH when predictors are strongly correlated.", "publication_ref": ["b76", "b84"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Appendix Appendix D. More General Formulation of the Normal Means model", "text": "In Section 2 we defined the normal means (NM) model for the special case when all observations j have the same variance, \u03c3 2 . This special case was sufficient to develop the VEB methods with the assumption that x T j x j = 1, j = 1, . . . , p. Here, we extend the NM model to allow for observation-specific variances, \u03c3 2 j , which is needed to generalize the VEB method to cases in which the x T j x j = 1 assumption no longer holds.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.1 The Normal Means Model", "text": "Let NM p (f, s 2 ) denote the normal means model with prior f and observation-specific variances s 2 = (s 2 1 , . . . , s 2 p ) \u2208 R p + :\ny j | b j , s 2 j \u223c N (b j , s 2 j ), b j i.i.d. \u223c f, j = 1, . . . , p,(41)\nsuch that y j , b j \u2208 R, j = 1, . . . , p. We assume priors that are mixtures of zero-mean normals, f \u2208 G(u 2 1 , . . . , u 2 K ), u 2 k \u2265 0, k = 1, . . . , K, so that any prior can be written as\nb j \u223c K k=1 \u03c0 k N (0, u 2 k ), such that \u03c0 = (\u03c0 1 , . . . , \u03c0 K ) \u2208 S K .\nAs in (5), it is helpful in the derivations to make use of the latent variable representation:\np(\u03b3 j = k | f ) = \u03c0 k b j | f, \u03b3 j = k \u223c N (0, u 2 k ),(42)\nwith \u03b3 j \u2208 {1, . . . , K}, j = 1, . . . , p. We write the joint prior for b j , \u03b3 j as\np prior (b j , \u03b3 j = k) \u225c p(b j , \u03b3 j = k | f ) = \u03c0 k N (b j ; 0, u 2 k ).(43)\nIn the expressions below we sometimes write the joint prior as p prior (f ) to make its dependence on f explicit.\nNote that the definition of the NM model given in the main text (Section 2), with prior (4), is a special case of these definitions and can be obtained with the substitutions s 2 j \u2190 \u03c3 2 , j = 1, . . . , p and u k \u2190 \u03c3 2 \u03c3 2 k , k = 1, . . . , K. \ny | b, s 2 \u223c N (b, s 2 ) b \u223c f. (44\n)\nFor a mixture of normals prior, f \u2208 G(u 2 1 , . . . , u 2 K ), the posterior distribution can be written as\np NM (b | y, s 2 , f ) = K k=1 \u03d5 1k N (b; \u00b5 1k , s 2 1k ),(45)\nin which the posterior component means \u00b5 1k , variances s 2 1k , responsibilities \u03d5 1k , and component (marginal) likelihoods L k are\n\u00b5 1k \u225c \u00b5 1k (y; f, s 2 ) = u 2 k s 2 + u 2 k \u00d7 y (46) s 2 1k \u225c s 2 1k (y; f, s 2 ) = s 2 u 2 k s 2 + u 2 k (47) \u03d5 1k \u225c \u03d5 1k (y; f, s 2 ) = \u03c0 k L k K k \u2032 =1 \u03c0 k \u2032 L k \u2032 (48) L k \u225c L k (y; f, s 2 ) = p(y | s 2 , f, \u03b3 = k) = p(y | b, s 2 ) p(b | f, \u03b3 = k) db (49)\nThe posterior expressions for the NM model given in the main text (10-13) can be recovered from these more general expressions with the substitutions s 2 j \u2190 \u03c3 2 , j = 1, . . . , p and u k \u2190 \u03c3 2 \u03c3 2 k , k = 1, . . . , K.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Evidence Lower Bound for Normal Means Model with One Observation", "text": "Given some probability density on b \u2208 R, denoted by q, the ELBO for the normal means model NM 1 (f, s 2 ) with observation y is\nF NM 1 (q, f, s 2 ; y) = log p(y | f, s 2 ) \u2212 D KL (q \u2225 p NM ) = E q [log p(y | b, s 2 )] \u2212 D KL (q \u2225 p prior (f )) = \u2212 1 2 log(2\u03c0s 2 ) \u2212 1 2s 2 E q [(y \u2212 b) 2 ] \u2212 D KL (q \u2225 p prior (f )). (50\n)\nWith this expression, we state the following result.\nLemma 6 (Normal means posterior as maximum of ELBO) The posterior distribution ( 45) under the NM model NM 1 (f, s 2 ) with observation y maximizes the ELBO (50); that is,\np NM = argmax q \u2212 1 2s 2 E q [(y \u2212 b) 2 ] \u2212 D KL (q \u2225 p prior (f )).\nFrom this lemma it follows that any q maximizing the ELBO (50) must have the following form:\nq(b) = K k=1 \u03d5 1k N (b; \u00b5 1k , s 2 1k ),(51)\nwith \u03d5 1k \u2265 0, \u00b5 1k \u2208 R, s 2 1k > 0, k = 1, . . . , K. For any q of the form (51), the ELBO (50) has an analytic expression, which we derive in part by making use of the formula for the K-L divergence between two normal distributions (Hastie et al., 2009):\nF NM 1 (q, f, s 2 ; y) = E q [log p(y | b, s 2 )] \u2212 D KL (q \u2225 p prior (f )),(52)\nin which\nE q [log p(y | b, s 2 )] = \u2212 1 2 log(2\u03c0s 2 ) \u2212 (y \u2212b) 2 2s 2 \u2212 1 2s 2 K k=1 [\u03d5 1k (\u00b5 2 1k + s 2 1k ) \u2212b 2 ] and D KL (q \u2225 p prior (f )) = K k=1 \u03d5 1k log \u03d5 1k \u03c0 k \u2212 1 2 K k=2 \u03d5 1k 1 + log s 2 1k u 2 k \u2212 \u00b5 2 1k + s 2 1k u 2 k ,\nand whereb is the posterior mean of b with respect to q,b = K k=1 \u03d5 1k \u00b5 1k . Here we have assumed that the first component in the prior mixture is a point mass at zero, \u03c3 2 1 = 0.", "publication_ref": ["b45"], "figure_ref": [], "table_ref": []}, {"heading": "D.4 ELBO for Normal Means Model with Multiple Observations", "text": "Now we extend the above results for the single-observation NM model to the NM model with multiple observations, NM p (f, s 2 ). Since the b j 's are independent under the posterior, the ELBO is simply the sum of the ELBOs for the single-observation NM models:\nF NM (q, f, s 2 ; y) = p j=1 F NM 1 (q j , f, s 2 j ; y j ). (53\n)\nFrom Lemma 6, the q that maximizes the ELBO is\nq(b) = p j=1 q j (b j ) q j (b j ) = p NM (b j | y j , f, s 2 j )\n, It also follows that any q maximizing the ELBO (53) must have the following form:\nq(b) = p j=1 q j (b j ) q j (b j ) = K k=1 \u03d5 1jk N (b j ; \u00b5 1jk , s 2 1jk ),(54)\nin which \u03d5 1jk \u2265 0, \u00b5 1jk \u2208 R, s 2 1jk > 0, j = 1, . . . , p, k = 1, . . . , K. For any q of the form (54), the analytic expression for the ELBO ( 53) is easily obtained by applying the analytic expression for the single-observation NM model (eq. 52). b j \u225c\nx T jr j\nx T j x j denote the ordinary least squares (OLS) estimate of the coefficient b j when the residualsr j are regressed against x j . See Proposition 1 for more definitions. Then we have the following results:\n(i) The coordinate ascent update q * j \u225c argmax q j F (q, g, \u03c3 2 ) is obtained by\nq * j (b j ) = p NM (b j ;b j , \u03c3 2 /d j , g \u03c3 ),\nin which p NM , defined in ( 45), is the posterior distribution of b under the following\nNM model:b | b, \u03c3 2 \u223c N (b, \u03c3 2 /d j ) b | g, \u03c3 2 \u223c g \u03c3 .(55)\n(ii) The coordinate ascent update\ng * \u225c argmax g \u2208 G(\u03c3 2 1 ,...,\u03c3 2 K ) F (q, g, \u03c3 2 )\nis achieved by setting\ng * = K k=1 \u03c0 * k N (0, \u03c3 2 k ) \u03c0 * k = 1 p n j=1 q j (\u03b3 j = k), k = 1, . . . , K.\n(iii) Using the parameterization of q in (54), and assuming that g is updated as in (ii) above, and \u03c3 2 1 = 0, the coordinate ascent update (\u03c3 2 ) * \u225c argmax\n\u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) is achieved by setting (\u03c3 2 ) * = \u2225r\u2225 2 + p j=1 K k=2 \u03d5 jk (d j + 1/\u03c3 2 k )(\u00b5 2 1jk + s 2 1jk ) \u2212 p j=1 d jb 2 j n + p(1 \u2212 \u03c0 * 1 )\n.\nAdditionally, if q 1 , . . . , q p are updated as in (i) above, we obtain the simpler expression\n(\u03c3 2 ) * = \u2225r\u2225 2 +b T D(b \u2212b) + \u03c3 2 p(1 \u2212 \u03c0 * 1 ) n + p(1 \u2212 \u03c0 1 ) , (56\n)\nwhere D is the p \u00d7 p diagonal matrix with diagonal entries d 1 , . . . , d p .\nNote that Proposition 1 is a special case of this proposition when d j = 1, for j = 1, . . . , p.\nIn the next sections, we prove parts (i), (ii) and (iii) of Proposition 7. These proofs start from the ELBO (18). From Bayes' rule,\np post (b) = p(y | X, b, \u03c3 2 ) p(b | g, \u03c3 2 ) p(y | X, b, \u03c3 2 ) p(b | g, \u03c3 2 ) db ,\nwe can write the ELBO as\nF (q, g, \u03c3 2 ) = E q [log p(y | X, b, \u03c3 2 )] \u2212 p j=1 D KL (q j \u2225 p prior ). (57\n)\nNext, using the property that q factorizes over the individual coordinates j = 1, . . . , p, we have\nF (q, g, \u03c3 2 ) = \u2212 n 2 log(2\u03c0\u03c3 2 ) \u2212 1 2\u03c3 2 E q [\u2225y \u2212 Xb\u2225 2 ] \u2212 p j=1 D KL (q j \u2225 p prior ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1 Update for q j", "text": "The coordinate ascent update for q j involves solving the following optimization problem:\nq * j = argmax q j F (q, g, \u03c3 2 ).\nFrom (57), this is equivalent to solving\nq * j = argmax q j E q [log p(y | X, b, \u03c3 2 )] \u2212 D KL (q j \u2225 p prior )\nBy rearranging terms, it can be shown that this is equivalent to solving\nq * j = argmax q j \u2212 d j 2\u03c3 2 E q j [(b j \u2212 b j ) 2 ] \u2212 D KL (q j \u2225 p prior ).(58)\nThe right-hand side of (58) is the ELBO for the NM model (55); that is, if we ignore constant terms, the ELBO in ( 50) recovers ( 58) by making the substitutions y \u2190b j , s 2 \u2190 \u03c3 2 /d j , f \u2190 g \u03c3 . And, therefore, from Lemma 6-and specifically from (51)-we have\nq * j (b j ) = p NM (b j |b j , \u03c3 2 /d j , g \u03c3 ) = K k=1 \u03d5 1jk N (b j ; \u00b5 1jk , s 2 1jk ), in which \u03d5 1jk = \u03d5 1k (b j , g \u03c3 , \u03c3 2 /d j ) \u00b5 1jk = \u00b5 1k (b j , g \u03c3 , \u03c3 2 /d j ) s 2 1jk = s 2 1k (b j , g \u03c3 , \u03c3 2 /d j ).\nThis proves part (i) of Proposition 7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2 Update for g", "text": "The coordinate ascent update for g involves solving the following optimization problem:\ng * \u2190 argmax g \u2208 G F (q, g, \u03c3 2 ). (59\n)\nRecall, for the mixture prior with fixed mixture components, fitting g reduces to fitting the mixture weights, \u03c0. Since \u03c0 only appears in the ELBO in the K-L divergence term with respect to the prior, solving ( 59) is equivalent to solving\n\u03c0 * = argmin \u03c0 \u2208 S K p j=1 D KL (q j \u2225 p prior ),\nwhich simplifies further:\n\u03c0 * = argmax \u03c0 \u2208 S K p j=1 K k=1 \u03d5 jk log \u03c0 k .\nThis has the following analytic solution:\n\u03c0 * k = 1 p p j=1 \u03d5 jk , k = 1, . . . , K.\nThis proves part (ii) of Proposition 7. This update can be thought of as an approximate M-step update for the mixture weights in which the posterior probabilities (the \"responsibilities\") are computed approximately using q.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.3 Update for \u03c3 2", "text": "The coordinate ascent update for the residual variance \u03c3 2 is the solution to (\u03c3 2 ) * = argmax\n\u03c3 2 \u2208 R + F (q, g, \u03c3 2 ).\nFrom earlier results, the ELBO for any q parameterized as (54) works out to When \u03c3 2 is updated following updates to q, we can simplify this expression by noting the specific form of the posterior means and variances,\nF (q, g, \u03c3 2 ) = E q [log p(y | X, b, \u03c3 2 )] \u2212 p j=1 D KL (q j \u2225 p prior ) (60\ns 2 1jk = \u03c3 2 d j + 1/\u03c3 2 k \u00b5 1jk = d j d j + 1/\u03c3 2 k \u00d7b j , for k = 2, . . . , K, which gives (\u03c3 2 ) * = \u2225r\u2225 2 + p j=1 d jbj (b j \u2212b j ) + \u03c3 2 p(1 \u2212 \u03c0 1 ) n + p(1 \u2212 \u03c0 1 ) .\nThis proves part (iii) of Proposition 7.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.4 More detailed VEB algorithm", "text": "Further details about the implementation of the VEB algorithm are given in Algorithm 4. Algorithm 4 does not require x T j x j = 1, j = 1, . . . , p.\nThe following lemma states that this can be understood as optimizing a penalized loss function and gives an explicit form for the penalty.\nLemma 8 h NM 1 (b, f, s 2 ; y) can be written as a penalized loss function, Expressions ( 62) and ( 63) follow from (61). Expression ( 64) is obtained by substitutingb = S f,s (y) into (62) and rearranging, noting that h(b, f, s 2 ; y) attains its minimum at thisb and therefore recovers the marginal loglikelihood, h NM (S f,s (y), f, s 2 ; y) = \u2212\u2113 NM (y; f, s 2 ).\nh\nExpression ( 65) is a consequence of the fact that h NM (b, f, s 2 ; y) attains its minimum at b = S f,s (y) for any y \u2208 R, that is, S f,s (y) = argmin b \u2208 R h NM (b, f, s 2 ; y).\nTherefore, the derivative of (62) with respect tob atb = S f,s (y) must be zero. Finally, ( 66) is obtained by applying Tweedie's formula (Efron, 2011).", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "F.2 VEB as a Penalized Regression Problem", "text": "Here we consider the ELBO for the multiple linear regression model (18). We begin with the following lemma. is maximized with respect to q when q(b, \u03b3) \u221d p(y | X, b, \u03c3 2 ) p(b, \u03b3 | g, \u03c3 2 ). This follows from the equality condition of Jensen's inequality (Jordan et al., 1999). When the columns of X are orthogonal, the posterior factorizes over the individual coordinates j, p(b, \u03b3 | X, y, g, \u03c3 2 ) \u221d p(y | X, b, \u03c3 2 ) p(b,\n\u03b3 | g, \u03c3 2 ) \u221d p j=1 exp \u2212 (b j \u2212 x T j y) 2 2\u03c3 2\n\u00d7 p(b j , \u03b3 j | g, \u03c3 2 ).\nTherefore, when X has orthogonal columns, the best q, even with the restriction of being fully factorized (20), is able to recover the exact posterior since the exact posterior also factorizes over the coordinates j = 1, . . . , p.", "publication_ref": ["b53"], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Proof of Proposition 4", "text": "First, we note that F (q,\u011d,\u03c3 2 ) = max q \u2208 Q F (q,\u011d,\u03c3 2 ) = \u2212h(b,\u011d,\u03c3 2 ).\nHence, for anyb \u2208 R p , we have \u2212h(b, g, \u03c3 2 ) = max q \u2208 Q, Eq[b] =b F (q, g, \u03c3 2 ) \u2264 max q \u2208 Q F (q, g, \u03c3 2 ) \u2264 max q \u2208 Q, g \u2208 G, \u03c3 2 \u2208 T F (q, g, \u03c3 2 ) = F (q,\u011d,\u03c3 2 ) = \u2212h(b,\u011d,\u03c3 2 ).\nThis proves thatb ,\u011d,\u03c3 2 = argmin b \u2208 R p , g \u2208 G, \u03c3 2 \u2208 T h(b, g, \u03c3 2 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.3 Proof of Theorem 5", "text": "The proof of the first part of Theorem 5 follows immediately from Proposition 10 and Lemma 8 by letting d j = 1 for j = 1, . . . , p.\nThe missing piece of the proof is to show that S \u03c1 f,\u03c3 (y) = S f,\u03c3 (y). We start with the definition of S \u03c1 in (31), To solve for the argmin on the right-hand side, we differentiate with respect to b and set the derivative to zero, \u03c1 \u2032 f,\u03c3 (b) = y \u2212 b.\nFrom (38), this derivative vanishes when b = S f,\u03c3 (y). Therefore, S \u03c1 f,\u03c3 (y) = S f,\u03c3 (y).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G.4 Mathematical Properties of the Posterior Mean Shrinkage Operator", "text": "Lemma 12 Let f be a symmetric unimodal distribution on R with a mode at zero, and assume \u03c3 2 > 0. Then the NM posterior mean operator S f,\u03c3 (y) defined in ( 32) is symmetric, non-negative, and non-decreasing on y \u2208 R, and S f,\u03c3 (y) \u2264 y on y \u2208 (0, \u221e). That is, S f,\u03c3 is a \"shrinkage operator\" that shrinks towards zero.\nProof The marginal likelihood for the NM model ( 33 \nwhere\np(y | \u03c3 2 , t) = p(y | b, \u03c3 2 ) p(b | t) db = 1 2t \u03a6 t \u2212 y \u03c3 + \u03a6 t + y \u03c3 \u2212 1 ,(69)\nand where \u03a6(x) denotes the normal cumulative distribution function. Note that, from (33), p(y | b, \u03c3 2 ) = N (y; b, \u03c3 2 ). Since \u03a6(t + y) + \u03a6(t \u2212 y) is non-increasing in y \u2208 (0, \u221e) for any t \u2265 0, p(y | \u03c3 2 , t) is also non-increasing in y \u2208 (0, \u221e) for any t \u2265 0. This implies that p(y | \u03c3 2 , t) is unimodal with a mode at zero, and therefore p(y | f, \u03c3 2 ) must also be unimodal with a mode at zero since it is a mixture of unimodal distributions that all have modes at zero. From (66), which was obtained by applying Tweedie's formula, we have that is non-decreasing with respect to \u00b5, for all \u00b5 \u2208 R, t > 0. To show this, the derivative of the expected value with respect to \u00b5 is always positive: \u2202 \u2202\u00b5 E[X] = Var(X)/s 2 > 0. Therefore, S f,\u03c3 is a mixture of non-decreasing functions on R + .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments and Disclosure of Funding", "text": "We thank Gao Wang for help with processing the the GTEx data, and for helpful discussions. This work was supported by NIH grant R01HG002585 to Matthew Stephens. We also thank the anonymous reviewers for their constructive suggestions for improvement and the staff at the Research Computing Center for providing the high-performance computing resources used to implement the numerical experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "hard thresholding (best subset, L 0 penalty)\notherwise Table 3: Some commonly used penalty functions and their corresponding shrinkage operators.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Appendix B. Additional Notes on the Methods Compared", "text": "Here we give additional details about how the methods were applied to the simulated data sets.\nRidge regression. We used function cv.glmnet from the glmnet R package to choose the penalty strength by CV. Specifically, we called cv.glmnet with alpha = 0, intercept = TRUE and standardize = FALSE; all other settings were kept at their defaults. The setting of the penalty strength parameter \u03bb minimizing the mean CV error (lambda.min) was used to make predictions.\nLasso. We fit Lasso models in the the same way that we fit ridge regression models using glmnet, except that we set the Elastic Net mixing parameter to alpha = 1.\nElastic Net. Elastic Net models were fit similarly to ridge regression and Lasso models, again using the cv.glmnet interface from the glmnet package. The difference is that the", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Appendix E. Derivation of Algorithm 2, Proof of Proposition 1", "text": "We prove Proposition 1 by proving a slightly more general proposition that does not require that x T j x j = 1, j = 1, . . . , p.\nProposition 7 Let d j = x T j x j , j = 1, . . . , p, and let Appendix Appendix F. VEB as a PLR", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Normal Means as a Penalized Estimation Problem", "text": "In this section, we formulate the normal means problem with one observation, NM 1 (f, s 2 ), as penalized estimation problem. Specifically, we express the posterior mean of b under the NM model as a solution to a penalized least squares problem. The results for the singleobservation NM model are used later to derive results for the multiple linear regression model. Recall, F NM 1 (q, f, s 2 ; y) denotes the ELBO for the single-observation NM model (50). From this, we define\nAs a reminder, F NM 1 (q, f, s 2 ; y) attains its maximum over q the exact posterior, q = p NM ; analogously, h NM 1 (b, f, s 2 ; y) attains its minimum overb atb = S f,s (y), the posterior mean of b (see Definition 3). Further, at their respective optima these two functions recover the Algorithm 4 Coordinate ascent for fitting VEB model (more detailed).\nRequire: Data X \u2208 R n\u00d7p , y \u2208 R n ; number of mixture components, K;\nprior variances, \u03c3 2 1 < \u2022 \u2022 \u2022 < \u03c3 2 K , with \u03c3 2 1 = 0; initial estimatesb, \u03c0, \u03c3 2 . r \u2190 y \u2212 Xb (compute mean residuals) t \u2190 0 for j \u2190 1 to p do d j = x T j x j end for repeat for j \u2190 1 to p d\u014d r j =r + x jbj (disregard jth effect in residuals)\n(update mean residuals) end for for k \u2190 1 to K do \u03c0 k \u2190 p j=1 \u03d5 jk /p. (update g; eq. 26) end for\n(update \u03c3 2 ; eq. 56)\nWith these definitions, we can express the posterior mean for b as the solution to a realvalued optimization problem: S f,s (y) = argminb h NM 1 (b, f, s 2 ; y).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendix Appendix G. Additional Results and Proofs", "text": "Proposition 11 (Convergence of cyclic coordinate ascent for VEB) The sequence of iterates {q (t) , g (t) , (\u03c3 2 ) (t) }, t = 0, 1, 2, . . . , generated by Algorithm 2 converge monotonically to a stationary point of the ELBO, F (18).\nProof By Proposition 2.7.1 of Bertsekas (1999), the sequence of iterates {q (t) , g (t) , (\u03c3 2 ) (t) }, t = 0, 1, 2, . . ., generated by Algorithm 2 converges monotonically to a stationary point of F provided that F is continuously differentiable and each coordinate update,\nis finite and uniquely determined. (See also Luo and Tseng 1992;Tseng 2001 for another treatment of convergence of coordinate ascent under general conditions.) A sufficient condition for F to be continuously differentiable and for the coordinate ascent updates (Proposition 1 or Proposition 7) to have a unique solution is that 0 < \u03c3 2 < \u221e, \u03c0 k > 0 for all k = 1, . . . , K, and 0 \u2264 \u03c3 2 1 < \u2022 \u2022 \u2022 < \u03c3 2 K < \u221e.", "publication_ref": ["b5", "b61", "b88"], "figure_ref": [], "table_ref": []}, {"heading": "G.1 Proof of Proposition 2", "text": "The ELBO F (q, g, \u03c3 2 ) = q(b, \u03b3) log p(y | X, b, \u03c3 2 ) p(b, \u03b3 | g, \u03c3 2 ) q(b, \u03b3) db d\u03b3", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "The Trimmed Lasso: sparse recovery guarantees and practical optimization by the generalized soft-min penalty", "journal": "SIAM Journal on Mathematics of Data Science", "year": "2021", "authors": "R Amir; B Basri;  Nadler"}, {"ref_id": "b1", "title": "Dirichlet-Laplace priors for optimal shrinkage", "journal": "Journal of the American Statistical Association", "year": "2015", "authors": "N S P Bhattacharya; Debdeep Pati; D B Dunson"}, {"ref_id": "b2", "title": "Spike-and-slab meets LASSO: A review of the spikeand-slab LASSO", "journal": "Chapman and Hall/CRC", "year": "", "authors": "R Bai; V Ro\u010dkov\u00e1; E I George"}, {"ref_id": "b3", "title": "On the convergence of block coordinate descent type methods", "journal": "SIAM Journal on Optimization", "year": "2013", "authors": "A Beck; L Tetruashvili"}, {"ref_id": "b4", "title": "Statistical Decision Theory and Bayesian Analysis", "journal": "Springer", "year": "1985", "authors": "J O Berger"}, {"ref_id": "b5", "title": "Nonlinear Programming", "journal": "Athena Scientific", "year": "1999", "authors": "D P Bertsekas"}, {"ref_id": "b6", "title": "The Trimmed Lasso: sparsity and robustness. arXiv", "journal": "", "year": "1708", "authors": "D Bertsimas; M S Copenhaver; R Mazumder"}, {"ref_id": "b7", "title": "Lasso meets horseshoe: a survey", "journal": "Statistical Science", "year": "2019", "authors": "A Bhadra; J Datta; N G Polson; B Willard"}, {"ref_id": "b8", "title": "Pattern Recognition and Machine Learning", "journal": "Springer", "year": "2006", "authors": "C Bishop"}, {"ref_id": "b9", "title": "Latent Dirichlet allocation", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "D M Blei; A Y Ng; M I Jordan"}, {"ref_id": "b10", "title": "Variational inference: a review for statisticians", "journal": "Journal of the American Statistical Association", "year": "2017", "authors": "M Blei; A Kucukelbir; J D Mcauliffe"}, {"ref_id": "b11", "title": "Coordinate descent algorithms for nonconvex penalized regression, with applications to biological feature selection", "journal": "Annals of Applied Statistics", "year": "2011", "authors": "P Breheny; J Huang"}, {"ref_id": "b12", "title": "Scalable variational inference for Bayesian variable selection in regression, and its accuracy in genetic association studies", "journal": "Bayesian Analysis", "year": "2012", "authors": "P Carbonetto; M Stephens"}, {"ref_id": "b13", "title": "varbvs: fast variable selection for large-scale regression. arXiv", "journal": "", "year": "1709", "authors": "P Carbonetto; X Zhou; M Stephens"}, {"ref_id": "b14", "title": "Empirical Bayes: past, present and future", "journal": "Journal of the American Statistical Association", "year": "2000", "authors": "B P Carlin; T A Louis"}, {"ref_id": "b15", "title": "The horseshoe estimator for sparse signals", "journal": "Biometrika", "year": "2010", "authors": "C M Carvalho; N G Polson; J G Scott"}, {"ref_id": "b16", "title": "Empirical Bayes Gibbs sampling", "journal": "Biostatistics", "year": "2001", "authors": "G Casella"}, {"ref_id": "b17", "title": "Needles and straw in a haystack: posterior concentration for possibly sparse sequences", "journal": "Annals of Statistics", "year": "2012", "authors": "I Castillo; A Van Der;  Vaart"}, {"ref_id": "b18", "title": "The practical implementation of Bayesian model selection", "journal": "", "year": "2001", "authors": "H Chipman; E I George; R E Mcculloch"}, {"ref_id": "b19", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1977", "authors": "A P Dempster; N M Laird; D B Rubin"}, {"ref_id": "b20", "title": "Unimodality, convexity, and applications", "journal": "Academic Press", "year": "1988", "authors": "S Dharmadhikari; K Joag-Dev"}, {"ref_id": "b21", "title": "Variational Bayesian inference for linear and logistic regression. arXiv, 1310", "journal": "", "year": "2013", "authors": "J Drugowitsch"}, {"ref_id": "b22", "title": "Microarrays, empirical Bayes and the two-groups model", "journal": "Statistical Science", "year": "2008", "authors": "B Efron"}, {"ref_id": "b23", "title": "Tweedie's formula and selection bias", "journal": "Journal of the American Statistical Association", "year": "2011", "authors": "B Efron"}, {"ref_id": "b24", "title": "Bayes, oracle Bayes and empirical Bayes", "journal": "Statistical Science", "year": "2019", "authors": "B Efron"}, {"ref_id": "b25", "title": "Stein's estimation rule and its competitors-an empirical Bayes approach", "journal": "Journal of the American Statistical Association", "year": "1973", "authors": "B Efron; C Morris"}, {"ref_id": "b26", "title": "Variable selection via nonconcave penalized likelihood and its oracle properties", "journal": "Journal of the American Statistical Association", "year": "2011", "authors": "J Fan; R Li"}, {"ref_id": "b27", "title": "Adaptive sparseness for supervised learning", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2003", "authors": "M A T Figueiredo"}, {"ref_id": "b28", "title": "Pathwise coordinate optimization", "journal": "Annals of Applied Statistics", "year": "2007", "authors": "J Friedman; T Hastie; H H\u00f6fling; R Tibshirani"}, {"ref_id": "b29", "title": "Regularization paths for generalized linear models via coordinate descent", "journal": "Journal of Statistical Software", "year": "2010", "authors": "J Friedman; T Hastie; R Tibshirani"}, {"ref_id": "b30", "title": "Penalized regressions: The Bridge versus the Lasso", "journal": "Journal of Computational and Graphical Statistics", "year": "1998", "authors": "W J Fu"}, {"ref_id": "b31", "title": "A genebased association method for mapping traits using reference transcriptome data", "journal": "Nature Genetics", "year": "2015", "authors": "E R Gamazon; H E Wheeler; K P Shah; S V Mozaffari; K Aquino-Michaels; R J Carroll; A E Eyler; J C Denny; D L Nicolae; N J Cox; H K Im"}, {"ref_id": "b32", "title": "Bayesian Data Analysis", "journal": "CRC Press", "year": "2013", "authors": "A Gelman; J B Carlin; H S Stern; D B Dunson; A Vehtari; D B Rubin"}, {"ref_id": "b33", "title": "Calibration and empirical Bayes variable selection", "journal": "Biometrika", "year": "2000", "authors": "E I George; D P Foster"}, {"ref_id": "b34", "title": "Variable selection via Gibbs sampling", "journal": "Journal of the American Statistical Association", "year": "1993", "authors": "E I George; R E Mcculloch"}, {"ref_id": "b35", "title": "Approaches for Bayesian variable selection", "journal": "Statistica Sinica", "year": "1997", "authors": "E I George; R E Mcculloch"}, {"ref_id": "b36", "title": "Empirical Bayes shrinkage and false discovery rate estimation, allowing for unwanted variation", "journal": "Biostatistics", "year": "2020", "authors": "D Gerard; M Stephens"}, {"ref_id": "b37", "title": "Variational learning for switching state-space models", "journal": "Neural Computation", "year": "2000", "authors": "Z Ghahramani; G E Hinton"}, {"ref_id": "b38", "title": "A variational method for learning sparse and overcomplete representations", "journal": "Neural Computation", "year": "2001", "authors": "M Girolami"}, {"ref_id": "b39", "title": "Inference with normal-gamma prior distributions in regression problems", "journal": "Bayesian Analysis", "year": "2010", "authors": "J E Griffin; P J Brown"}, {"ref_id": "b40", "title": "Genetic effects on gene expression across human tissues", "journal": "Nature", "year": "2017", "authors": ""}, {"ref_id": "b41", "title": "Bayesian variable selection regression for genome-wide association studies and other large-scale problems", "journal": "Annals of Applied Statistics", "year": "2011", "authors": "Y Guan; M Stephens"}, {"ref_id": "b42", "title": "Extension of the Bayesian alphabet for genomic selection", "journal": "BMC Bioinformatics", "year": "2011", "authors": "D Habier; R L Fernando; K Kizilkaya; D J Garrick"}, {"ref_id": "b43", "title": "Bayesian lasso regression", "journal": "Biometrika", "year": "2009", "authors": "C Hans"}, {"ref_id": "b44", "title": "Maximum-likelihood estimation for the mixed analysis of variance model", "journal": "Biometrika", "year": "1967", "authors": "H O Hartley; J N K Rao"}, {"ref_id": "b45", "title": "The Elements of Statistical Learning", "journal": "Springer", "year": "2009", "authors": "T Hastie; R Tibshirani; J Friedman"}, {"ref_id": "b46", "title": "Fast best subset selection: coordinate descent and local combinatorial optimization algorithms", "journal": "Operations Research", "year": "2020", "authors": "H Hazimeh; R Mazumder"}, {"ref_id": "b47", "title": "Ridge regression: biased estimation for nonorthogonal problems", "journal": "Technometrics", "year": "1970", "authors": "A E Hoerl; R W Kennard"}, {"ref_id": "b48", "title": "Bayesian parameter estimation via variational methods", "journal": "Statistics and Computing", "year": "2000", "authors": "T S Jaakkola; M I Jordan"}, {"ref_id": "b49", "title": "Debiasing the lasso: optimal sample size for Gaussian designs", "journal": "Annals of Statistics", "year": "2018", "authors": "A Javanmard; A Montanari"}, {"ref_id": "b50", "title": "Needles and straw in haystacks: empirical Bayes estimates of possibly sparse sequences", "journal": "Annals of Statistics", "year": "2004", "authors": "I M Johnstone; B W Silverman"}, {"ref_id": "b51", "title": "Empirical Bayes selection of wavelet thresholds", "journal": "Annals of Statistics", "year": "2005", "authors": "I M Johnstone; B W Silverman"}, {"ref_id": "b52", "title": "Bayesian lasso: an extension for genome-wide association study", "journal": "", "year": "2017", "authors": "L Joo"}, {"ref_id": "b53", "title": "An introduction to variational methods for graphical models", "journal": "", "year": "1999", "authors": "M I Jordan; Z Ghahramani; T S Jaakkola; L K Saul"}, {"ref_id": "b54", "title": "A fast algorithm for maximum likelihood estimation of mixture proportions using sequential quadratic programming", "journal": "Journal of Computational and Graphical Statistics", "year": "2020", "authors": "Y Kim; P Carbonetto; M Stephens; M Anitescu"}, {"ref_id": "b55", "title": "Convex optimization, shape constraints, compound decisions, and empirical Bayes rules", "journal": "Journal of the American Statistical Association", "year": "2014", "authors": "R Koenker; I Mizera"}, {"ref_id": "b56", "title": "On information and sufficiency", "journal": "Annals of Mathematical Statistics", "year": "1951", "authors": "S Kullback; R A Leibler"}, {"ref_id": "b57", "title": "The Bayesian elastic net", "journal": "Bayesian Analysis", "year": "2010", "authors": "Q Li; N Lin"}, {"ref_id": "b58", "title": "Mixtures of g priors for Bayesian variable selection", "journal": "Journal of the American Statistical Association", "year": "2008", "authors": "F Liang; R Paulo; G Molina; M A Clyde; J O Berger"}, {"ref_id": "b59", "title": "A variational Bayes algorithm for fast and accurate multiple locus genome-wide association analysis", "journal": "BMC Bioinformatics", "year": "2010", "authors": "B A Logsdon; G E Hoffman; J G Mezey"}, {"ref_id": "b60", "title": "Variance adaptive shrinkage (vash): flexible empirical Bayes estimation of variances", "journal": "Bioinformatics", "year": "2016", "authors": "M Lu; M Stephens"}, {"ref_id": "b61", "title": "On the convergence of the coordinate descent method for convex differentiable minimization", "journal": "Journal of Optimization Theory and Applications", "year": "1992", "authors": "Z Q Luo; P Tseng"}, {"ref_id": "b62", "title": "Piecewise bounds for estimating Bernoullilogistic latent Gaussian models", "journal": "", "year": "2011", "authors": "B M Marlin; M E Khan; K P Murphy"}, {"ref_id": "b63", "title": "Sparsenet: coordinate descent with nonconvex penalties", "journal": "Journal of the American Statistical Association", "year": "2011", "authors": "R Mazumder; J H Friedman; T Hastie"}, {"ref_id": "b64", "title": "Generalized Linear Models", "journal": "Monographs on Statistics and Applied Probability", "year": "1989", "authors": "P Mccullagh; J A Nelder"}, {"ref_id": "b65", "title": "Prediction of total genetic value using genome-wide dense marker maps", "journal": "Genetics", "year": "2001", "authors": "T H Meuwissen; B J Hayes; M E Goddard"}, {"ref_id": "b66", "title": "A fast algorithm for BayesB type of prediction of genome-wide estimates of genetic value", "journal": "Genetics Selection Evolution", "year": "2009", "authors": "T H E Meuwissen; T R Solberg; R Shepherd; J A Woolliams"}, {"ref_id": "b67", "title": "Subset Selection in Regression", "journal": "Chapman and Hall/CRC", "year": "2002", "authors": "A J Miller"}, {"ref_id": "b68", "title": "Bayesian variable selection in linear regression", "journal": "Journal of the American Statistical Association", "year": "1988", "authors": "T J Mitchell; J J Beauchamp"}, {"ref_id": "b69", "title": "Parametric empirical Bayes inference: theory and applications", "journal": "Journal of the American Statistical Association", "year": "1983", "authors": "C N Morris"}, {"ref_id": "b70", "title": "Simultaneous discovery, estimation and prediction analysis of complex traits using a Bayesian mixture model", "journal": "PLoS Genetics", "year": "2015", "authors": "G Moser; S H Lee; B J Hayes; M E Goddard; N R Wray; P M Visscher"}, {"ref_id": "b71", "title": "Bayes and empirical Bayes shrinkage estimation of regression coefficients", "journal": "Canadian Journal of Statistics", "year": "1986", "authors": "F Nebebe; T Stroud"}, {"ref_id": "b72", "title": "Proximal algorithms", "journal": "Foundations and Trends in Optimization", "year": "2014", "authors": "N Parikh; S Boyd"}, {"ref_id": "b73", "title": "The Bayesian Lasso", "journal": "Journal of the American Statistical Association", "year": "2008", "authors": "T Park; G Casella"}, {"ref_id": "b74", "title": "Genome-wide regression and prediction with the BGLR statistical package", "journal": "Genetics", "year": "2014", "authors": "P Perez; G De Los Campos"}, {"ref_id": "b75", "title": "R: a language and environment for statistical computing. R Foundation for Statistical Computing", "journal": "", "year": "2019", "authors": " R Core Team"}, {"ref_id": "b76", "title": "Variational Bayes for high-dimensional linear regression with sparse priors", "journal": "Journal of the American Statistical Association", "year": "2022", "authors": "K Ray; B Szab\u00f3"}, {"ref_id": "b77", "title": "Variational Bayesian methods for spatial data analysis", "journal": "Computational Statistics and Data Analysis", "year": "2011", "authors": "Q Ren; S Banerjee; A O Finley; J S Hodges"}, {"ref_id": "b78", "title": "The empirical Bayes approach to statistical decision problems", "journal": "Annals of Mathematical Statistics", "year": "1964", "authors": "H Robbins"}, {"ref_id": "b79", "title": "The Spike-and-Slab LASSO", "journal": "Journal of the American Statistical Association", "year": "2018", "authors": "V Ro\u010dkov\u00e1; E I George"}, {"ref_id": "b80", "title": "Exploiting tractable substructures in intractable networks", "journal": "MIT Press", "year": "1996", "authors": "L K Saul; M I Jordan"}, {"ref_id": "b81", "title": "Linear models and empirical Bayes methods for assessing differential expression in microarray experiments", "journal": "Statistical Applications in Genetics and Molecular Biology", "year": "2004", "authors": "G K Smyth"}, {"ref_id": "b82", "title": "False discovery rates: a new deal", "journal": "Biostatistics", "year": "2016", "authors": "M Stephens"}, {"ref_id": "b83", "title": "Studies in the history of probability and statistics XL: Boscovich, Simpson and a 1760 manuscript note on fitting a linear relation", "journal": "Biometrika", "year": "1984", "authors": "S M Stigler"}, {"ref_id": "b84", "title": "False discoveries occur early on the lasso path", "journal": "Annals of Statistics", "year": "2017", "authors": "W Su; M Bogdan; E Candes"}, {"ref_id": "b85", "title": "Solving the empirical Bayes normal means problem with correlated noise", "journal": "", "year": "2018", "authors": "L Sun; M Stephens"}, {"ref_id": "b86", "title": "Regression shrinkage and selection via the lasso", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1996", "authors": "R Tibshirani"}, {"ref_id": "b87", "title": "Solution of incorrectly formulated problems and the regularization method", "journal": "Soviet Mathematics-Doklady", "year": "1963", "authors": "A N Tikhonov"}, {"ref_id": "b88", "title": "Convergence of a block coordinate descent method for nondifferentiable minimization", "journal": "Journal of Optimization Theory and Applications", "year": "2001", "authors": "P Tseng"}, {"ref_id": "b89", "title": "Flexible statistical methods for estimating and testing effects in genomic studies with multiple conditions", "journal": "Nature Genetics", "year": "2019", "authors": "S M Urbut; G Wang; P Carbonetto; M Stephens"}, {"ref_id": "b90", "title": "Learning from a lot: empirical Bayes for high-dimensional model-based prediction", "journal": "Scandinavian Journal of Statistics", "year": "2019", "authors": "M A Van De Wiel; D E Beest; M M M\u00fcnch"}, {"ref_id": "b91", "title": "Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning", "journal": "", "year": "2008", "authors": "M J Wainwright; M I Jordan"}, {"ref_id": "b92", "title": "Inadequacy of interval estimates corresponding to variational bayesian approximations", "journal": "", "year": "2005", "authors": "B Wang; D M Titterington"}, {"ref_id": "b93", "title": "Variational inference in nonconjugate models", "journal": "Journal of Machine Learning Research", "year": "2013", "authors": "C Wang; D M Blei"}, {"ref_id": "b94", "title": "A simple new approach to variable selection in regression, with application to genetic fine mapping", "journal": "Journal of the Royal Statistical Society, Series B", "year": "2020", "authors": "G Wang; A Sarkar; P Carbonetto; M Stephens"}, {"ref_id": "b95", "title": "Empirical Bayes matrix factorization", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "W Wang; M Stephens"}, {"ref_id": "b96", "title": "Coordinate descent algorithms", "journal": "Mathematical Programming", "year": "2015", "authors": "S J Wright"}, {"ref_id": "b97", "title": "Coordinate descent algorithms for lasso penalized regression", "journal": "Annals of Applied Statistics", "year": "2008", "authors": "T T Wu; K Lange"}, {"ref_id": "b98", "title": "Flexible signal denoising via flexible empirical Bayes shrinkage", "journal": "Journal of Machine Learning Research", "year": "2021", "authors": "Z Xing; P Carbonetto; M Stephens"}, {"ref_id": "b99", "title": "On variational Bayes estimation and variational information criteria for linear regression models", "journal": "Australian and New Zealand Journal of Statistics", "year": "2014", "authors": "C You; J T Ormerod; S M\u00fcller"}, {"ref_id": "b100", "title": "Efficient empirical Bayes variable selection and estimation in linear models", "journal": "Journal of the American Statistical Association", "year": "2005", "authors": "M Yuan; Y Lin"}, {"ref_id": "b101", "title": "Trimming the \u2113 1 regularizer: statistical analysis, optimization, and applications to deep learning", "journal": "", "year": "2019", "authors": "J Yun; P Zheng; E Yang; A Lozano; A Aravkin"}, {"ref_id": "b102", "title": "Fast and accurate Bayesian polygenic risk modeling with variational inference", "journal": "American Journal of Human Genetics", "year": "2023", "authors": "S Zabad; S Gravel; Y Li"}, {"ref_id": "b103", "title": "Signatures of negative selection in the genetic architecture of human complex traits", "journal": "Nature Genetics", "year": "2018", "authors": "J Zeng; R Vlaming; Y Wu; M R Robinson; L R Lloyd-Jones; L Yengo; C X Yap; A Xue; J Sidorenko; A F Mcrae; J E Powell; G W Montgomery; A Metspalu; T Esko; G Gibson; N R Wray; P M Visscher; J Yang"}, {"ref_id": "b104", "title": "Nearly unbiased variable selection under minimax concave penalty", "journal": "Annals of Statistics", "year": "2010", "authors": "C.-H Zhang"}, {"ref_id": "b105", "title": "Polygenic modeling with Bayesian sparse linear mixed models", "journal": "PLoS Genetics", "year": "2013", "authors": "X Zhou; P Carbonetto; M Stephens"}, {"ref_id": "b106", "title": "Heavy-tailed prior distributions for sequence count data: removing the noise and preserving large differences", "journal": "Bioinformatics", "year": "2019", "authors": "A Zhu; J G Ibrahim; M I Love"}, {"ref_id": "b107", "title": "Regularization and variable selection via the elastic net", "journal": "Journal of the Royal Statistical Society, Series B", "year": "2005", "authors": "H Zou; T Hastie"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Examples of posterior mean shrinkage operators for different g \u2208 G(\u03c3 21 , . . . , \u03c3 2 K ) (left-hand panel) and \u03c3 2 that were chosen to mimic the shrinkage operators from some commonly used penalties (right-hand panel).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Results from Experiment 1 in which the sparsity level, s, was varied. Each point shows the prediction error (scaled RMSE) averaged over 20 simulations.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Results from Experiment 1 in which the sparsity level, s, was varied. Each point shows prediction error (scaled RMSE) averaged over 20 simulations. The Mr.ASH results are the same as in Figure 2 and provide a common point of reference.", "figure_data": ""}, {"figure_label": "45", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :Figure 5 :45Figure 4: Results from Experiment 2 in which total signal strength (PVE) was varied. Each point shows prediction error (scaled RMSE) averaged over 20 simulations. Left panels show PLR methods; right panels show Bayes-based methods. The Mr.ASH results are included in all plots to provide a common reference point.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: Results from Experiment 4 in which the number of predictors, p, was varied.Each point shows the scaled RMSE (top row) and the running time (bottom row) averaged over 20 simulations. Left panels show PLR methods; right panels show Bayes-based methods. The Mr.ASH results are included in all plots to provide a common reference point. Running times for all methods except Trimmed Lasso are from running the methods in R 3.5.1 (R Core Team, 2019) on a machine with a quad-core 2.6 GHZ Intel Core i7 processor and 16 GB of memory. R was installed from macOS binaries and we used the BLAS libraries that were distributed with R. The Trimmed Lasso was run using MATLAB 9.13 on machines with 4 Intel Xeon E5-2680v4 (\"Broadwell\") processors and 24 GB of memory. The Mr.ASH running times include running the Lasso to initialize the estimates; the running times for Mr.ASH with a \"null\" initialization (b = 0) are also shown for comparison.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure 7: Results from Experiment 5 in which the noise distribution was varied. Each point shows the prediction error (scaled RMSE) averaged over 20 simulations. Left panels show PLR methods; right panels show Bayes-based methods. The Mr.ASH results are included in all plots to provide a common reference point.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 8: Summary of results from Experiments 1-5. The simulation results are summarized slightly differently in this figure to emphasize common trends. The boxplots show the distribution of RMSEs relative to the best performing method in each simulation (see the text for details). The horizontal line inside each box depicts the median; the dot depicts the mean; and the upper and lower lines depict the interquartile range.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9 :9Figure 9: Comparison of Mr.ASH vs. OLS in data sets with p \u2265 64, n = 200. Each box in the boxplot summarizes the difference in the test set prediction error (scaled RMSE) between the Mr.ASH predictions and the ordinary least squares (OLS) estimates across 20 simulations. The horizontal line inside each box depicts the median; the dot depicts the mean; the upper and lower lines depict the interquartile range.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Comparison of Mr.ASH with different initializations and update orders. Each point shows the prediction error (scaled RMSE) averaged over the 20 simulations at that setting.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "D. 22Posterior Distribution under Normal Means Model with One Observation Let q NM (b, \u03b3 | y, s 2 , f ) denote the posterior distribution of b, \u03b3 under the normal means model NM 1 (f, s 2 ) with a single observation (p = 1):", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "D,KL (q j \u2225 p prior ) = and whereb j = K k=1 \u03d5 1jk \u00b5 1jk . Taking the partial derivative of F with respect to \u03c3 2 then solving for \u03c3 2 yields the following update:", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "b) + s 2 D KL (q \u2225 p prior (f )). (63)For any y \u2208 R, this penalty term satisfies\u03c1 f,s (S f,s (y)) = \u2212s 2 \u2113 NM (y; f, s 2 ) \u2212 1 2 (y \u2212 S f,s (y)) 2 ,(64)and\u03c1 \u2032 f,s (S f,s (y)) = (y \u2212 S f,s (y)) (65) = \u2212s 2 \u2113 \u2032 NM (y; f, s 2 ),(66)in which \u2113 NM (y; f, s 2 ) is the marginal log-likelihood \u2113 NM (y; f, s 2 ) \u225c log p(y | f, s 2 ) for the single-observation normal means model, NM 1 (f, s 2 ).Proof From (50), we haveF NM 1 (q, f, s 2 ; y) = \u2212 1 2s 2 (y \u2212 E q (b)) Var q (b) + D KL (q \u2225 p prior (f )) .", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Lemma 9 29If the distribution q(b) factorizes as q(b) = p j=1 q j (b j ), thenE q [\u2225r\u2225 2 ] = \u2225r\u2225 2 + p j=1 d j Var q j (b j ), whereb \u225c E q [b], r \u225c y \u2212 Xb and r \u225c E q [r] = y \u2212 Xb. Proof E q [\u2225r\u2225 2 ] = E q [\u2225r + X(b \u2212 b)\u2225 2 ] = \u2225r\u2225 2 + E q [\u2225X(b \u2212 b)\u2225 2 ] = \u2225r\u2225 2 + E q [(b \u2212 b) T X T X(b \u2212 b)] = \u2225r\u2225 2 + tr(X T XCov q (b)) = \u2225r\u2225 2 + p j=1 d j Var q j (b j ).In the following proposition, we express the ELBO for the multiple linear regression model as a penalized loss function.Proposition 10 The objective function h (35) can be written as a penalized loss function,h(b, g, \u03c3 2 ) = 1 2\u03c3 2 \u2225y \u2212 Xb\u2225 2 + p j=1 \u03c1 g\u03c3,s j (b j )/s 2 function \u03c1 f,s defined in (63), and defining s 2 j \u225c \u03c3 2 /d j , j = 1, . . . , p. Note that when d j = 1, k = 1, . . . , p, (67) simplifies to (36).Var q j (b j )/s 2 j + p j=1 D KL (q j \u2225 p prior (g \u03c3 )). Eq j (b j ) =b j 1 Var q j (b j ) + s 2 j D KL (q j \u2225 p prior (g \u03c3 ))", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "b) 2 + \u03c1 f,\u03c3 (b).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": ") is\u2113 NM (y; f, \u03c3 2 ) \u225c log p(y | f, \u03c3 2 ) = p(y | b, \u03c3 2 ) p(b) db = N (y; b, \u03c3 2 ) f (b) db.By Khintchine's representation theorem(Dharmadhikari and Joag-Dev, 1988), f can be represented as mixture of uniform distributions, for some (possibly improper) univariate mixing density p(t). Let p(b | t) be the density function of the uniform distribution on [\u2212t, t]. Then we havep(y | f, \u03c3 2 ) = p(y | b, \u03c3 2 ) p(b | f, \u03c3 2 ) db = \u221e 0 p(y | b, \u03c3 2 ) p(b | t) db \u00d7 p(t) dt = \u221e 0 p(y | \u03c3 2 , t) p(t) dt,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "Sf,\u03c3 (y) = y + \u03c3 2 \u2113 \u2032 NM (y; f, \u03c3) \u2264 y, in which the inequality is obtained by noting that \u2113 NM (y; f, \u03c3) is non-increasing in y \u2208 (0, \u221e). And since \u2113 NM (y; f, \u03c3 2 ) is symmetric about zero, the shrinkage operator must be an odd function; i.e., S f,\u03c3 (y) = \u2212S f,\u03c3 (\u2212y).It remains to show that S f,\u03c3 (y) is non-decreasing on y \u2208 R + . Since p(y | b, \u03c3 2 ) = N (y; b, \u03c3 2 ) and p(b | t) is the uniform distribution on interval [\u2212t, t], the posterior density is truncated normal:p(b | y, \u03c3 2 , t) \u221d p(y | b, \u03c3 2 ) p(b | t) = N [\u2212t,t] (b; y, s 2 )where N [\u2212t,t] (x; \u00b5, s 2 ) = N (x; \u00b5, s 2 ) I{|x| < t} denotes the probability density of the normal distribution with mean \u00b5 and variance s 2 truncated to the interval x \u2208 [\u2212t, t]. The expected value of the truncated normal,E[X] = \u00b5 + s \u00d7 N (\u2212t; \u00b5, s 2 ) \u2212 N (t; \u00b5, s 2 ) \u03a6((t \u2212 \u00b5)/s) \u2212 \u03a6(\u2212(t + \u00b5)/s) ,", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "y j | b j , \u03c3 2 \u223c N (b j , \u03c3 2 ), j = 1, . . . , p.", "formula_coordinates": [5.0, 218.01, 227.95, 175.98, 13.13]}, {"formula_id": "formula_1", "formula_text": "G(\u03c3 2 1 , . . . , \u03c3 2 K ) \u225c g = K k=1 \u03c0 k N (0, \u03c3 2 k ) : \u03c0 \u2208 S K ,(2)", "formula_coordinates": [5.0, 189.98, 397.57, 332.02, 33.98]}, {"formula_id": "formula_2", "formula_text": "b j /\u03c3, are i.i.d. from g, b j | g, \u03c3 2 i.i.d. \u223c g \u03c3 ,(3)", "formula_coordinates": [5.0, 90.0, 676.96, 432.0, 29.08]}, {"formula_id": "formula_3", "formula_text": "b j | g, \u03c3 2 i.i.d. \u223c K k=1 \u03c0 k N (0, \u03c3 2 \u03c3 2 k ). (4", "formula_coordinates": [6.0, 231.79, 264.19, 285.56, 33.98]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [6.0, 517.35, 275.82, 4.65, 9.57]}, {"formula_id": "formula_5", "formula_text": "p(\u03b3 j = k | g) = \u03c0 k b j | g, \u03c3 2 , \u03b3 j = k \u223c N (0, \u03c3 2 \u03c3 2 k ),(5)", "formula_coordinates": [6.0, 233.78, 350.57, 288.22, 29.46]}, {"formula_id": "formula_6", "formula_text": "\u03c0 = argmax \u03c0 \u2208 S K log p(y | g, \u03c3 2 ) = argmax \u03c0 \u2208 S K p j=1 log K k=1 \u03c0 k L jk ,(6)", "formula_coordinates": [6.0, 235.22, 534.35, 286.78, 60.89]}, {"formula_id": "formula_7", "formula_text": "L jk \u225c p(y j | g, \u03c3 2 , \u03b3 j = k) = N (y j ; 0, \u03c3 2 + \u03c3 2 \u03c3 2 k ),(7)", "formula_coordinates": [6.0, 243.64, 633.82, 278.37, 31.96]}, {"formula_id": "formula_8", "formula_text": "E-step \u03d5 jk \u2190 \u03d5 k (y j ; g, \u03c3 2 ) \u225c p(\u03b3 j = k | y j , g, \u03c3 2 ) = \u03c0 k L jk K k \u2032 =1 \u03c0 k \u2032 L jk \u2032 , (8", "formula_coordinates": [7.0, 147.27, 130.21, 370.08, 29.08]}, {"formula_id": "formula_9", "formula_text": ") M-step \u03c0 k \u2190 1 p p j=1 \u03d5 jk , k = 1, . . . , K.(9)", "formula_coordinates": [7.0, 144.7, 137.79, 377.3, 57.46]}, {"formula_id": "formula_10", "formula_text": "p NM post (b j , \u03b3 j = k | y j , g, \u03c3 2 ) = p(b j | y j , g, \u03c3 2 , \u03b3 j = k) p(\u03b3 j = k | y j , g, \u03c3 2 ) = \u03d5 jk N (b j ; \u00b5 jk , s 2 jk ),(10)", "formula_coordinates": [7.0, 140.69, 285.0, 381.31, 32.14]}, {"formula_id": "formula_11", "formula_text": "\u00b5 jk \u225c \u00b5 k (y j ; g, \u03c3 2 ) = \u03c3 2 k 1+\u03c3 2 k \u00d7 y j ,(11)", "formula_coordinates": [7.0, 230.22, 349.1, 291.79, 21.57]}, {"formula_id": "formula_12", "formula_text": "s 2 jk \u225c s 2 k (y j ; g, \u03c3 2 ) = \u03c3 2 k 1+\u03c3 2 k \u00d7 \u03c3 2 .(12)", "formula_coordinates": [7.0, 233.13, 373.27, 288.87, 21.57]}, {"formula_id": "formula_13", "formula_text": "p NM post (b j | y j , g, \u03c3 2 ) = K k=1 \u03d5 jk N (b j ; \u00b5 jk , s 2 jk ).(13)", "formula_coordinates": [7.0, 203.91, 458.37, 318.09, 33.98]}, {"formula_id": "formula_14", "formula_text": "y | X, b, \u03c3 2 \u223c N (Xb, \u03c3 2 I n ), (14", "formula_coordinates": [7.0, 239.77, 573.41, 277.38, 13.13]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [7.0, 517.15, 575.9, 4.85, 9.57]}, {"formula_id": "formula_16", "formula_text": "(\u011d,\u03c3 2 ) = argmax g \u2208 G, \u03c3 2 \u2208 R + p(y | X, g, \u03c3 2 ) = argmax g \u2208 G, \u03c3 2 \u2208 R + log p(y | X, b, \u03c3 2 ) p(b | g, \u03c3 2 ) db.(15)", "formula_coordinates": [8.0, 188.74, 204.81, 333.26, 48.77]}, {"formula_id": "formula_17", "formula_text": "p post (b) \u225c p(b | X, y,\u011d,\u03c3 2 ) \u221d p(y | X, b,\u03c3 2 ) p(b |\u011d,\u03c3 2 ). (16", "formula_coordinates": [8.0, 185.54, 290.75, 331.62, 13.13]}, {"formula_id": "formula_18", "formula_text": ")", "formula_coordinates": [8.0, 517.15, 293.25, 4.85, 9.57]}, {"formula_id": "formula_19", "formula_text": "(p post ,\u011d,\u03c3 2 ) = argmax q, g \u2208 G, \u03c3 2 \u2208 R + F (q, g, \u03c3 2 ),(17)", "formula_coordinates": [8.0, 213.65, 497.73, 308.35, 22.24]}, {"formula_id": "formula_20", "formula_text": "F (q, g, \u03c3 2 ) \u225c log p(y | X, g, \u03c3 2 ) \u2212 D KL (q(b, \u03b3) \u2225 p(b, \u03b3 | X, y, g, \u03c3 2 )). (18", "formula_coordinates": [8.0, 143.69, 553.44, 373.46, 13.18]}, {"formula_id": "formula_21", "formula_text": ")", "formula_coordinates": [8.0, 517.15, 555.93, 4.85, 9.57]}, {"formula_id": "formula_22", "formula_text": "q \u2208 Q, g \u2208 G, \u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) (19", "formula_coordinates": [8.0, 270.68, 685.34, 246.47, 22.24]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [8.0, 517.15, 687.83, 4.85, 9.57]}, {"formula_id": "formula_24", "formula_text": "Q = q : q(b, \u03b3) = p j=1 q j (b j , \u03b3 j ) ,(20)", "formula_coordinates": [9.0, 224.68, 102.12, 297.32, 34.29]}, {"formula_id": "formula_25", "formula_text": "p(y | X, g, \u03c3 2 ) \u2248 exp{F (q(g, \u03c3 2 ), g, \u03c3 2 )},", "formula_coordinates": [9.0, 210.82, 294.85, 190.36, 12.06]}, {"formula_id": "formula_26", "formula_text": "q j \u2190 argmax q j F (q, g, \u03c3 2 ) end for g \u2190 argmax g \u2208 G F (q, g, \u03c3 2 ) \u03c3 2 \u2190 argmax \u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) until termination criterion is met return q 1 , . . . , q p , g, \u03c3 2 .", "formula_coordinates": [10.0, 198.11, 189.59, 161.35, 80.33]}, {"formula_id": "formula_27", "formula_text": "j \u225c y \u2212 X \u2212jb\u2212j = y \u2212 j \u2032 \u0338 =j x j \u2032b j \u2032 . (21", "formula_coordinates": [10.0, 222.85, 428.88, 294.3, 12.26]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [10.0, 517.15, 428.91, 4.85, 9.57]}, {"formula_id": "formula_29", "formula_text": "q * j (b j , \u03b3 j = k) = p NM post (b j , \u03b3 j = k;b j , g, \u03c3 2 ).", "formula_coordinates": [10.0, 221.68, 544.38, 195.92, 14.19]}, {"formula_id": "formula_30", "formula_text": "q * j (b j , \u03b3 j = k) = \u03d5 * jk N (b j ; \u00b5 * jk , (s 2 jk ) * ),(22)", "formula_coordinates": [10.0, 230.64, 588.54, 291.36, 14.27]}, {"formula_id": "formula_31", "formula_text": "\u00b5 * jk = \u00b5 k (b j ; g, \u03c3 2 ) (23) (s 2 jk ) * = s 2 k (b j ; g, \u03c3 2 ) (24", "formula_coordinates": [10.0, 269.27, 632.7, 252.73, 32.75]}, {"formula_id": "formula_32", "formula_text": ")", "formula_coordinates": [10.0, 517.15, 653.68, 4.85, 9.57]}, {"formula_id": "formula_33", "formula_text": "\u03d5 * jk = \u03d5 k (b j ; g, \u03c3 2 ). (25", "formula_coordinates": [10.0, 281.1, 669.66, 236.05, 14.27]}, {"formula_id": "formula_34", "formula_text": ")", "formula_coordinates": [10.0, 517.15, 672.15, 4.85, 9.57]}, {"formula_id": "formula_35", "formula_text": "g * \u225c argmax g \u2208 G(\u03c3 2 1 ,...,\u03c3 2 K ) F (q, g, \u03c3 2 )", "formula_coordinates": [11.0, 252.94, 117.68, 133.4, 24.42]}, {"formula_id": "formula_36", "formula_text": "g * = K k=1 \u03c0 * k N (0, \u03c3 2 k ) \u03c0 * k = 1 p p j=1 q j (\u03b3 j = k), k = 1, . . . , K.(26)", "formula_coordinates": [11.0, 224.78, 178.25, 297.23, 32.32]}, {"formula_id": "formula_37", "formula_text": "\u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) is achieved with (\u03c3 2 ) * = \u2225r\u2225 2 + p j=1 Var q (b j ) + p j=1 K k=2 \u03d5 jk E[b j | \u03b3 j = k]/\u03c3 2 k n + p \u2212 p j=1 \u03d5 j1 .(27)", "formula_coordinates": [11.0, 117.27, 254.13, 404.73, 88.97]}, {"formula_id": "formula_38", "formula_text": "(\u03c3 2 ) * = \u2225r\u2225 2 +b T (b \u2212b) + \u03c3 2 p(1 \u2212 \u03c0 * 1 ) n + p(1 \u2212 \u03c0 * 1 ) . (28", "formula_coordinates": [11.0, 221.23, 394.23, 295.92, 28.94]}, {"formula_id": "formula_39", "formula_text": ")", "formula_coordinates": [11.0, 517.15, 403.57, 4.85, 9.57]}, {"formula_id": "formula_40", "formula_text": "1 < \u2022 \u2022 \u2022 < \u03c3 2 K , with \u03c3 2 1 = 0; initial estimatesb, \u03c0, \u03c3 2 . r = y \u2212 Xb (compute mean residuals) t \u2190 0 repeat for j \u2190 1 to p d\u014d r j =r + x jbj (disregard jth effect in residuals) b j \u2190 x T jr j . (compute OLS estimate) for k \u2190 1 to K do \u03d5 jk \u2190 \u03d5 k (b j ; g, \u03c3 2 ) \u00b5 jk \u2190 \u00b5 k (b j ; g, \u03c3 2 ) end for (update q j ; eqs. 23, 25) b j \u2190 K k=1 \u03d5 jk \u00b5 jk . (update posterior mean of b j ) r \u2190r j \u2212 x jbj . (update mean residuals) end for for k \u2190 1 to K do \u03c0 k \u2190 p j=1 \u03d5 jk /p. (update g; eq. 26) end for \u03c3 2 \u2190 \u2225r\u2225 2 +b T (b \u2212b) + \u03c3 2 p(1 \u2212 \u03c0 1 ) n + p(1 \u2212 \u03c0 1 ) (update \u03c3 2", "formula_coordinates": [12.0, 100.91, 124.05, 806.57, 256.5]}, {"formula_id": "formula_41", "formula_text": "t \u2190 t + 1 until termination criterion is met returnb, \u03c0, \u03c3 2 Proof See Appendix G.", "formula_coordinates": [12.0, 90.0, 380.67, 172.26, 83.62]}, {"formula_id": "formula_42", "formula_text": "minimize b \u2208 R p h \u03c1 (b) \u225c 1 2 \u2225y \u2212 Xb\u2225 2 + p j=1 \u03c1(b j ),(29)", "formula_coordinates": [15.0, 203.81, 424.53, 318.19, 34.29]}, {"formula_id": "formula_43", "formula_text": "b j \u2190 S \u03c1 (b j + x T j (y \u2212 Xb)),(30)", "formula_coordinates": [15.0, 241.12, 546.62, 280.88, 14.19]}, {"formula_id": "formula_44", "formula_text": "S \u03c1 (t) \u225c argmin \u03b8 \u2208 R 1 2 (t \u2212 \u03b8) 2 + \u03c1(\u03b8)(31)", "formula_coordinates": [15.0, 230.84, 580.56, 291.16, 26.3]}, {"formula_id": "formula_45", "formula_text": "S f,\u03c3 (y) \u225c E NM (b | y, f, \u03c3 2 ), (32", "formula_coordinates": [16.0, 242.12, 203.32, 275.04, 13.27]}, {"formula_id": "formula_46", "formula_text": ")", "formula_coordinates": [16.0, 517.15, 205.82, 4.85, 9.57]}, {"formula_id": "formula_47", "formula_text": "y | b, \u03c3 2 \u223c N (b, \u03c3 2 ) b \u223c f.(33)", "formula_coordinates": [16.0, 261.21, 252.32, 260.8, 28.6]}, {"formula_id": "formula_48", "formula_text": "S g\u03c3,\u03c3 (y) = K k=1 \u03d5 k (y; g, \u03c3 2 ) \u00b5 k (y; g, \u03c3 2 ). (34", "formula_coordinates": [16.0, 216.03, 314.98, 301.12, 33.98]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [16.0, 517.15, 326.61, 4.85, 9.57]}, {"formula_id": "formula_50", "formula_text": "h(b, g, \u03c3 2 ) \u225c \u2212 max q \u2208 Q, Eq[b] =b F (q, g, \u03c3 2 ) . (35", "formula_coordinates": [16.0, 203.99, 607.76, 313.16, 20.43]}, {"formula_id": "formula_51", "formula_text": ")", "formula_coordinates": [16.0, 517.15, 610.26, 4.85, 9.57]}, {"formula_id": "formula_52", "formula_text": "q \u2208 Q, g \u2208 G, \u03c3 2 \u2208 T F (q, g, \u03c3 2 ),", "formula_coordinates": [17.0, 266.56, 119.15, 125.33, 21.55]}, {"formula_id": "formula_53", "formula_text": "b,\u011d,\u03c3 2 = argmin b \u2208 R p , g \u2208 G, \u03c3 2 \u2208 T h(b, g, \u03c3 2 ).", "formula_coordinates": [17.0, 216.65, 226.61, 178.7, 22.55]}, {"formula_id": "formula_54", "formula_text": "h(b, g, \u03c3 2 ) = 1 2\u03c3 2 \u2225y \u2212 Xb\u2225 2 + 1 \u03c3 2 p j=1 \u03c1 g\u03c3,\u03c3 (b j ) + n \u2212 p 2 log(2\u03c0\u03c3 2 ),(36)", "formula_coordinates": [17.0, 149.52, 366.27, 372.48, 34.29]}, {"formula_id": "formula_55", "formula_text": "\u03c1 f,\u03c3 (S f,\u03c3 (y)) = \u2212\u03c3 2 \u2113 NM (y; f, \u03c3 2 ) \u2212 1 2 (y \u2212 S f,\u03c3 (y)) 2 , (37", "formula_coordinates": [17.0, 184.35, 436.11, 332.8, 15.26]}, {"formula_id": "formula_56", "formula_text": ")", "formula_coordinates": [17.0, 517.15, 438.61, 4.85, 9.57]}, {"formula_id": "formula_57", "formula_text": "and \u03c1 \u2032 f,\u03c3 (S f,\u03c3 (y)) = (y \u2212 S f,\u03c3 (y)).(38)", "formula_coordinates": [17.0, 90.0, 463.3, 432.0, 25.5]}, {"formula_id": "formula_58", "formula_text": "RMSE-scaled(y test ,b) \u225c RMSE(y test ,b) RMSE(b = 0) ,(39)", "formula_coordinates": [23.0, 207.3, 209.04, 314.7, 26.2]}, {"formula_id": "formula_59", "formula_text": "RMSE(y test ,b) \u225c 1 \u221a n \u2225y test \u2212 X testb \u2225. (40", "formula_coordinates": [23.0, 215.76, 261.14, 301.39, 15.68]}, {"formula_id": "formula_60", "formula_text": ")", "formula_coordinates": [23.0, 517.15, 263.43, 4.85, 9.57]}, {"formula_id": "formula_61", "formula_text": "RMSE(b = 0) \u225c \u03c3/ \u221a", "formula_coordinates": [23.0, 90.0, 279.97, 106.55, 18.96]}, {"formula_id": "formula_62", "formula_text": "1.1 1.2 1.3 1.0 1.1 1.2 1.3 1.0 1.1 1.2 1.3 1.0 1.1 1.2 1.", "formula_coordinates": [32.0, 138.05, 105.0, 199.79, 407.83]}, {"formula_id": "formula_63", "formula_text": "y j | b j , s 2 j \u223c N (b j , s 2 j ), b j i.i.d. \u223c f, j = 1, . . . , p,(41)", "formula_coordinates": [39.0, 233.32, 420.74, 288.68, 34.17]}, {"formula_id": "formula_64", "formula_text": "b j \u223c K k=1 \u03c0 k N (0, u 2 k ), such that \u03c0 = (\u03c0 1 , . . . , \u03c0 K ) \u2208 S K .", "formula_coordinates": [39.0, 90.0, 500.35, 263.83, 55.67]}, {"formula_id": "formula_65", "formula_text": "p(\u03b3 j = k | f ) = \u03c0 k b j | f, \u03b3 j = k \u223c N (0, u 2 k ),(42)", "formula_coordinates": [39.0, 247.07, 580.61, 274.93, 29.46]}, {"formula_id": "formula_66", "formula_text": "p prior (b j , \u03b3 j = k) \u225c p(b j , \u03b3 j = k | f ) = \u03c0 k N (b j ; 0, u 2 k ).(43)", "formula_coordinates": [39.0, 223.25, 642.11, 298.76, 29.46]}, {"formula_id": "formula_67", "formula_text": "y | b, s 2 \u223c N (b, s 2 ) b \u223c f. (44", "formula_coordinates": [40.0, 262.72, 201.35, 254.44, 28.6]}, {"formula_id": "formula_68", "formula_text": ")", "formula_coordinates": [40.0, 517.15, 212.05, 4.85, 9.57]}, {"formula_id": "formula_69", "formula_text": "p NM (b | y, s 2 , f ) = K k=1 \u03d5 1k N (b; \u00b5 1k , s 2 1k ),(45)", "formula_coordinates": [40.0, 210.91, 265.83, 311.09, 33.98]}, {"formula_id": "formula_70", "formula_text": "\u00b5 1k \u225c \u00b5 1k (y; f, s 2 ) = u 2 k s 2 + u 2 k \u00d7 y (46) s 2 1k \u225c s 2 1k (y; f, s 2 ) = s 2 u 2 k s 2 + u 2 k (47) \u03d5 1k \u225c \u03d5 1k (y; f, s 2 ) = \u03c0 k L k K k \u2032 =1 \u03c0 k \u2032 L k \u2032 (48) L k \u225c L k (y; f, s 2 ) = p(y | s 2 , f, \u03b3 = k) = p(y | b, s 2 ) p(b | f, \u03b3 = k) db (49)", "formula_coordinates": [40.0, 139.58, 339.72, 382.42, 108.69]}, {"formula_id": "formula_71", "formula_text": "F NM 1 (q, f, s 2 ; y) = log p(y | f, s 2 ) \u2212 D KL (q \u2225 p NM ) = E q [log p(y | b, s 2 )] \u2212 D KL (q \u2225 p prior (f )) = \u2212 1 2 log(2\u03c0s 2 ) \u2212 1 2s 2 E q [(y \u2212 b) 2 ] \u2212 D KL (q \u2225 p prior (f )). (50", "formula_coordinates": [40.0, 139.67, 567.7, 377.48, 49.77]}, {"formula_id": "formula_72", "formula_text": ")", "formula_coordinates": [40.0, 517.15, 604.43, 4.85, 9.57]}, {"formula_id": "formula_73", "formula_text": "p NM = argmax q \u2212 1 2s 2 E q [(y \u2212 b) 2 ] \u2212 D KL (q \u2225 p prior (f )).", "formula_coordinates": [40.0, 180.57, 687.09, 250.87, 20.16]}, {"formula_id": "formula_74", "formula_text": "q(b) = K k=1 \u03d5 1k N (b; \u00b5 1k , s 2 1k ),(51)", "formula_coordinates": [41.0, 239.68, 117.46, 282.32, 33.98]}, {"formula_id": "formula_75", "formula_text": "F NM 1 (q, f, s 2 ; y) = E q [log p(y | b, s 2 )] \u2212 D KL (q \u2225 p prior (f )),(52)", "formula_coordinates": [41.0, 170.83, 214.56, 351.17, 14.19]}, {"formula_id": "formula_76", "formula_text": "E q [log p(y | b, s 2 )] = \u2212 1 2 log(2\u03c0s 2 ) \u2212 (y \u2212b) 2 2s 2 \u2212 1 2s 2 K k=1 [\u03d5 1k (\u00b5 2 1k + s 2 1k ) \u2212b 2 ] and D KL (q \u2225 p prior (f )) = K k=1 \u03d5 1k log \u03d5 1k \u03c0 k \u2212 1 2 K k=2 \u03d5 1k 1 + log s 2 1k u 2 k \u2212 \u00b5 2 1k + s 2 1k u 2 k ,", "formula_coordinates": [41.0, 90.0, 251.13, 409.1, 89.15]}, {"formula_id": "formula_77", "formula_text": "F NM (q, f, s 2 ; y) = p j=1 F NM 1 (q j , f, s 2 j ; y j ). (53", "formula_coordinates": [41.0, 211.6, 452.39, 305.55, 34.29]}, {"formula_id": "formula_78", "formula_text": ")", "formula_coordinates": [41.0, 517.15, 464.6, 4.85, 9.57]}, {"formula_id": "formula_79", "formula_text": "q(b) = p j=1 q j (b j ) q j (b j ) = p NM (b j | y j , f, s 2 j )", "formula_coordinates": [41.0, 242.86, 509.53, 122.65, 53.57]}, {"formula_id": "formula_80", "formula_text": "q(b) = p j=1 q j (b j ) q j (b j ) = K k=1 \u03d5 1jk N (b j ; \u00b5 1jk , s 2 1jk ),(54)", "formula_coordinates": [41.0, 227.47, 586.19, 294.53, 74.45]}, {"formula_id": "formula_81", "formula_text": "q * j (b j ) = p NM (b j ;b j , \u03c3 2 /d j , g \u03c3 ),", "formula_coordinates": [42.0, 246.85, 290.71, 144.06, 14.19]}, {"formula_id": "formula_82", "formula_text": "NM model:b | b, \u03c3 2 \u223c N (b, \u03c3 2 /d j ) b | g, \u03c3 2 \u223c g \u03c3 .(55)", "formula_coordinates": [42.0, 115.76, 330.93, 406.24, 39.48]}, {"formula_id": "formula_83", "formula_text": "g * \u225c argmax g \u2208 G(\u03c3 2 1 ,...,\u03c3 2 K ) F (q, g, \u03c3 2 )", "formula_coordinates": [42.0, 252.18, 403.61, 133.4, 24.42]}, {"formula_id": "formula_84", "formula_text": "g * = K k=1 \u03c0 * k N (0, \u03c3 2 k ) \u03c0 * k = 1 p n j=1 q j (\u03b3 j = k), k = 1, . . . , K.", "formula_coordinates": [42.0, 224.02, 458.7, 189.72, 32.32]}, {"formula_id": "formula_85", "formula_text": "\u03c3 2 \u2208 R + F (q, g, \u03c3 2 ) is achieved by setting (\u03c3 2 ) * = \u2225r\u2225 2 + p j=1 K k=2 \u03d5 jk (d j + 1/\u03c3 2 k )(\u00b5 2 1jk + s 2 1jk ) \u2212 p j=1 d jb 2 j n + p(1 \u2212 \u03c0 * 1 )", "formula_coordinates": [42.0, 115.76, 540.65, 362.13, 85.5]}, {"formula_id": "formula_86", "formula_text": "(\u03c3 2 ) * = \u2225r\u2225 2 +b T D(b \u2212b) + \u03c3 2 p(1 \u2212 \u03c0 * 1 ) n + p(1 \u2212 \u03c0 1 ) , (56", "formula_coordinates": [42.0, 215.66, 657.77, 301.49, 27.45]}, {"formula_id": "formula_87", "formula_text": ")", "formula_coordinates": [42.0, 517.15, 667.1, 4.85, 9.57]}, {"formula_id": "formula_88", "formula_text": "p post (b) = p(y | X, b, \u03c3 2 ) p(b | g, \u03c3 2 ) p(y | X, b, \u03c3 2 ) p(b | g, \u03c3 2 ) db ,", "formula_coordinates": [43.0, 205.1, 153.57, 201.79, 26.49]}, {"formula_id": "formula_89", "formula_text": "F (q, g, \u03c3 2 ) = E q [log p(y | X, b, \u03c3 2 )] \u2212 p j=1 D KL (q j \u2225 p prior ). (57", "formula_coordinates": [43.0, 169.95, 210.17, 347.2, 34.29]}, {"formula_id": "formula_90", "formula_text": ")", "formula_coordinates": [43.0, 517.15, 222.38, 4.85, 9.57]}, {"formula_id": "formula_91", "formula_text": "F (q, g, \u03c3 2 ) = \u2212 n 2 log(2\u03c0\u03c3 2 ) \u2212 1 2\u03c3 2 E q [\u2225y \u2212 Xb\u2225 2 ] \u2212 p j=1 D KL (q j \u2225 p prior ).", "formula_coordinates": [43.0, 136.59, 277.87, 338.82, 34.29]}, {"formula_id": "formula_92", "formula_text": "q * j = argmax q j F (q, g, \u03c3 2 ).", "formula_coordinates": [43.0, 249.1, 369.27, 113.8, 20.99]}, {"formula_id": "formula_93", "formula_text": "q * j = argmax q j E q [log p(y | X, b, \u03c3 2 )] \u2212 D KL (q j \u2225 p prior )", "formula_coordinates": [43.0, 181.07, 424.37, 249.87, 20.99]}, {"formula_id": "formula_94", "formula_text": "q * j = argmax q j \u2212 d j 2\u03c3 2 E q j [(b j \u2212 b j ) 2 ] \u2212 D KL (q j \u2225 p prior ).(58)", "formula_coordinates": [43.0, 183.2, 478.51, 338.8, 25.93]}, {"formula_id": "formula_95", "formula_text": "q * j (b j ) = p NM (b j |b j , \u03c3 2 /d j , g \u03c3 ) = K k=1 \u03d5 1jk N (b j ; \u00b5 1jk , s 2 1jk ), in which \u03d5 1jk = \u03d5 1k (b j , g \u03c3 , \u03c3 2 /d j ) \u00b5 1jk = \u00b5 1k (b j , g \u03c3 , \u03c3 2 /d j ) s 2 1jk = s 2 1k (b j , g \u03c3 , \u03c3 2 /d j ).", "formula_coordinates": [43.0, 90.0, 566.87, 353.76, 116.64]}, {"formula_id": "formula_96", "formula_text": "g * \u2190 argmax g \u2208 G F (q, g, \u03c3 2 ). (59", "formula_coordinates": [44.0, 247.72, 136.65, 269.43, 20.88]}, {"formula_id": "formula_97", "formula_text": ")", "formula_coordinates": [44.0, 517.15, 139.15, 4.85, 9.57]}, {"formula_id": "formula_98", "formula_text": "\u03c0 * = argmin \u03c0 \u2208 S K p j=1 D KL (q j \u2225 p prior ),", "formula_coordinates": [44.0, 227.79, 220.46, 156.43, 34.29]}, {"formula_id": "formula_99", "formula_text": "\u03c0 * = argmax \u03c0 \u2208 S K p j=1 K k=1 \u03d5 jk log \u03c0 k .", "formula_coordinates": [44.0, 232.41, 280.45, 147.18, 34.56]}, {"formula_id": "formula_100", "formula_text": "\u03c0 * k = 1 p p j=1 \u03d5 jk , k = 1, . . . , K.", "formula_coordinates": [44.0, 232.42, 345.61, 147.16, 34.29]}, {"formula_id": "formula_101", "formula_text": "\u03c3 2 \u2208 R + F (q, g, \u03c3 2 ).", "formula_coordinates": [44.0, 283.88, 505.76, 86.31, 22.24]}, {"formula_id": "formula_102", "formula_text": "F (q, g, \u03c3 2 ) = E q [log p(y | X, b, \u03c3 2 )] \u2212 p j=1 D KL (q j \u2225 p prior ) (60", "formula_coordinates": [44.0, 171.47, 564.11, 345.69, 34.29]}, {"formula_id": "formula_103", "formula_text": "s 2 1jk = \u03c3 2 d j + 1/\u03c3 2 k \u00b5 1jk = d j d j + 1/\u03c3 2 k \u00d7b j , for k = 2, . . . , K, which gives (\u03c3 2 ) * = \u2225r\u2225 2 + p j=1 d jbj (b j \u2212b j ) + \u03c3 2 p(1 \u2212 \u03c0 1 ) n + p(1 \u2212 \u03c0 1 ) .", "formula_coordinates": [45.0, 90.0, 224.35, 333.35, 132.86]}, {"formula_id": "formula_104", "formula_text": "h", "formula_coordinates": [47.0, 207.03, 170.36, 6.29, 9.57]}, {"formula_id": "formula_105", "formula_text": "\u03b3 | g, \u03c3 2 ) \u221d p j=1 exp \u2212 (b j \u2212 x T j y) 2 2\u03c3 2", "formula_coordinates": [50.0, 242.7, 145.06, 144.52, 51.37]}, {"formula_id": "formula_107", "formula_text": "p(y | \u03c3 2 , t) = p(y | b, \u03c3 2 ) p(b | t) db = 1 2t \u03a6 t \u2212 y \u03c3 + \u03a6 t + y \u03c3 \u2212 1 ,(69)", "formula_coordinates": [51.0, 192.05, 473.48, 329.95, 43.42]}], "doi": ""}