{"title": "Ethically Compliant Sequential Decision Making", "authors": "Justin Svegliato; Samer B Nashed; Shlomo Zilberstein", "pub_date": "", "abstract": "Enabling autonomous systems to comply with an ethical theory is critical given their accelerating deployment in domains that impact society. While many ethical theories have been studied extensively in moral philosophy, they are still challenging to implement by developers who build autonomous systems. This paper proposes a novel approach for building ethically compliant autonomous systems that optimize completing a task while following an ethical framework. First, we introduce a definition of an ethically compliant autonomous system and its properties. Next, we offer a range of ethical frameworks for divine command theory, prima facie duties, and virtue ethics. Finally, we demonstrate the accuracy and usability of our approach in a set of autonomous driving simulations and a user study of planning and robotics experts.", "sections": [{"heading": "Introduction", "text": "Enabling autonomous systems to comply with an ethical theory is critical given their accelerating deployment in domains that impact society (Charisi et al. 2017). For example, a self-driving car that drives a route ought to slow near a school zone, crosswalk, or park to avoid endangering pedestrians (Svegliato et al. 2019;Basich et al. 2020). Similarly, an elder care robot that helps caregivers perform medical diagnostics ought to tailor its support based on the physical and mental state of the patient to reduce the risk of injury and the loss of dignity (Shim, Arkin, and Pettinatti 2017). While many ethical theories have been studied extensively in moral philosophy, they are still challenging to implement by developers who build autonomous systems. Hence, there is a growing need to simplify and standardize the process of implementing an ethical theory within autonomous systems.\nA simple approach to enabling an autonomous system to comply with an ethical theory is to modify its objective function directly. Modifying this objective function, however, poses two problems. First, adjusting the objective function can lead to unpredictable effects on the behavior of the autonomous system due to the complexity of its decisionmaking model. In fact, small changes to the objective function can generate large changes to the behavior of the autonomous system (Bostrom 2016). Second, using the objective function to represent both a task and an ethical theory can result in incommensurable conversions as it blends them Copyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nwithin the decision-making model implicitly (Taylor et al. 2016). These problems cause the behavior of an autonomous system to fail to reflect the intentions of developers or the values of stakeholders (Hadfield-Menell and Hadfield 2019).\nIdeally, any developer who builds an autonomous system with the ability to comply with an ethical theory could instead use an approach that exhibits several desirable properties. First, it should be general-purpose by supporting any task or ethical theory as long as they can be represented appropriately. Next, it should be modular by encapsulating the task and ethical theory as separate modules that avoid an objective function that blends them implicitly. Finally, it should be interpretable by describing the ethical theory in terms of the behavior and environment of the autonomous system.\nWe propose a novel approach with these properties for building ethically compliant autonomous systems that optimize completing a task subject to following an ethical framework. As expected, the task defines the goal that the system must achieve using a decision-making model. More importantly, the ethical framework approximates a wellknown ethical theory that the system must comply with using a moral principle and an ethical context. The moral principle evaluates whether or not the system violates the ethical framework and the ethical context includes the contextual information needed to evaluate the system. Formally, this is expressed as an optimization problem with a set of constraints for the task and a constraint for the ethical framework. While our approach supports different decision-making models and ethical frameworks, we consider a Markov decision process as the decision-making model and divine command theory, prima facie duties, and virtue ethics as the ethical frameworks in this paper.\nWe evaluate our approach in two ways. In a set of autonomous driving simulations, we observe that our approach produces optimal behavior that meets a set of moral requirements. In a user study, we find that planning and robotics experts who use our approach to produce optimal behavior that meets a set of moral requirements make fewer development errors and need less development time than a simple approach that modifies the decision-making model directly.\nOur contributions are: (1) a definition of an ethically compliant autonomous system and its properties, (2) a range of ethical frameworks for divine command theory, prima facie duties, and virtue ethics, and (3) a set of autonomous driving simulations and a user study of planning and robotics experts that show the accuracy and usability of our approach.", "publication_ref": ["b23", "b53", "b8", "b50", "b17", "b54", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Autonomous systems perform an array of tasks in diverse social contexts. Their potential harms can be mitigated via many strategies: (1) abandonment of technologies that are likely to be abused from a historical context (Browne 2015), such as facial recognition (Brey 2004;Introna and Wood 2004) and online surveillance (Zimmer 2008;Burgers and Robinson 2017), (2) legal intervention that enforces oversight to discourage or prevent malevolent or negligent use (Raymond and Shackelford 2013;Scherer 2015;Goodman and Flaxman 2017;Desai and Kroll 2017), including metaregulation (Pasquale 2017), and (3) technical advances that improve the accuracy and interpretability of algorithms. While these strategies are important, our approach focuses on a different strategy that reduces the likelihood for error during the design and development of autonomous systems.\nSimilarly, there are various principles (Friedman, Kahn, and Borning 2008;Boden et al. 2017), guidelines (Fallman 2003;Robertson et al. 2019), and standards (Read et al. 2015;Adamson, Havens, and Chatila 2019) that have recently been proposed to lower the chance for error during the design and development of autonomous systems. However, though critical to promoting the intentions of developers or the values of stakeholders, they do not address implementing an ethical theory within autonomous systems. In fact, autonomous systems that try to satisfy a set of moral requirements only through careful construction, called implicit ethical agents, may not produce ethical behavior (Moor 2006). Hence, many autonomous systems must be explicit ethical agents capable of some notion of moral reasoning (Bench-Capon and Modgil 2017;Dignum et al. 2018).\nEfforts to build autonomous systems that are explicit ethical agents take two approaches (Allen, Smit, and Wallach 2005). Bottom-up approaches produce ethical behavior by gradually evolving or learning in an environment that rewards and penalizes behavior (Anderson, Anderson, and Berenz 2017;Shaw et al. 2018). Although this is compelling given the natural development of ethical ideas in society, they can lack stability or interpretability. Hence, top-down approaches produce ethical behavior by directly following prescriptive rules provided by a human or an ethical theory. These methods often use different logics, such as deontic logic (van der Torre 2003;Bringsjord, Arkoudas, and Bello 2006), temporal logic (Wooldridge and Van Der Hoek 2005;Atkinson and Bench-Capon 2006;Dennis et al. 2016), answer set programming (Berreby, Bourgne, and Ganascia 2015), or planning formalisms (Dennis et al. 2016). Some methods even use metareasoning over many logics (Bringsjord et al. 2011). While we offer a top-down approach in this paper, we do not employ logics since they are challenging to use given the growing complexity of autonomous systems (Abel, MacGlashan, and Littman 2016).\nA common top-down approach that addresses the complexity of autonomous systems uses an ethical governor to determine online whether an action is required, permitted, or prohibited (Arkin 2008). Applications include eldercare (Shim, Arkin, and Pettinatti 2017) and physical safety (Winfield, Blum, and Liu 2014;Vanderelst and Winfield 2018). However, an ethical governor is myopic because it only considers the immediate reaction that must be made by the system at each time step. In contrast, our approach is nonmyopic since it reasons about the sequence of actions that must be performed by the system over every time step.\nWe know of only one other nonmyopic top-down approach to explicit ethical agents (Kasenberg and Scheutz 2018). However, the approach cannot represent different ethical theories, such as utilitarianism or Kantianism, because it is specific to norms. Moreover, the approach cannot guarantee ethical behavior since both task completion and ethical compliance are defined by real-valued weights. Our approach instead produces desirable behavior that complies with different ethical theories and avoids unpredictable trade-offs between task completion and ethical compliance.", "publication_ref": ["b21", "b18", "b33", "b59", "b22", "b43", "b47", "b29", "b25", "b41", "b28", "b16", "b27", "b45", "b44", "b1", "b38", "b11", "b26", "b2", "b3", "b49", "b55", "b19", "b58", "b7", "b24", "b13", "b24", "b20", "b0", "b5", "b50", "b57", "b56", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "A Markov decision process (MDP) is a decision-making model for reasoning in fully observable, stochastic environments (Bellman 1952). An MDP can be described as a tuple S, A, T, R, d , where S is a finite set of states, A is a finite set of actions, T : S \u00d7 A \u00d7 S \u2192 [0, 1] represents the probability of reaching a state s \u2208 S after performing an action a \u2208 A in a state s \u2208 S, R : S \u00d7 A \u00d7 S \u2192 R represents the expected immediate reward of reaching a state s \u2208 S after performing an action a \u2208 A in a state s \u2208 S, and d : S \u2192 [0, 1] represents the probability of starting in a state s \u2208 S. A solution to an MDP is a policy \u03c0 : S \u2192 A indicating that an action \u03c0(s) \u2208 A should be performed in a state s \u2208 S. A policy \u03c0 induces a value function V \u03c0 : S \u2192 R representing the expected discounted cumulative reward V \u03c0 (s) \u2208 R for each state s \u2208 S given a discount factor 0 \u2264 \u03b3 < 1. An optimal policy \u03c0 * maximizes the expected discounted cumulative reward for every state s \u2208 S by satisfying the Bellman optimality equation V * (s) = max a\u2208A s \u2208S T (s, a, s )[R(s, a, s )+\u03b3V * (s )].\nA common approach for finding an optimal policy expresses the optimization problem as a linear program in either the primal form or the dual form (Manne 1960). In this paper, we propose ethical frameworks that naturally map to the dual form. The dual form maximizes a set of occupancy measures \u00b5 s a for the discounted number of times an action a \u2208 A is performed in a state s \u2208 S subject to a set of constraints that maintain consistent and nonnegative occupancy. ", "publication_ref": ["b10", "b37"], "figure_ref": [], "table_ref": []}, {"heading": "Ethically Compliant Autonomous Systems", "text": "We propose a novel approach for building ethically compliant autonomous systems that decouples ethical compliance from task completion. The system optimizes completing a task by using a decision-making model subject to following an ethical framework by adhering to a moral principle within an ethical context. We describe these attributes of an ethically compliant autonomous system below.\nFirst, the system has a decision-making model that describes the information needed to complete the task. For example, a self-driving vehicle could have a decision-making model that includes a map of a city (Basich et al. 2021). A developer must select a representation for the decisionmaking model that reflects the properties of the task. For many tasks, an MDP, a decision process that assumes full observability, can be used easily. However, for more complex tasks with partial observability, start and goal states, or multiple agents, it is possible to use a decision process like a partially observable MDP, a stochastic shortest path problem, or a decentralized MDP instead. In short, the decisionmaking model is an amoral, descriptive model for completing the task but not following the ethical framework.\nNext, the system has an ethical context that describes the information required to follow the ethical framework. For instance, an autonomous vehicle could have an ethical context that includes any details related to inconsiderate or hazardous driving that permit speeding on a highway in some scenarios but never around a school zone or near a crosswalk (Vanderelst and Winfield 2018). Similar to the decision-making model, a developer must select a representation for the ethical context that informs the fundamental principles of the ethical framework. Although the ethical context can be represented as a tuple of different values, sets, and functions, the particular specification of the tuple depends on the ethical framework. In brief, the ethical context is a moral, prescriptive model for following the ethical framework but not completing the task.\nFinally, the system has a moral principle that evaluates the morality of a policy for the decision-making model within the ethical context. This considers the information that describes how to both complete the task and follow the ethical framework. As an illustration, a moral principle could require a policy to maximize the well-being of the moral community in utilitarianism (Bentham 1789;Mill 1895) or universalize to the moral community without contradiction in Kantianism (Kant and Schneewind 2002). Given a decisionmaking model and an ethical context, a developer must express the moral principle as a general function that maps a policy to its moral status in the following way. Definition 1. A moral principle, \u03c1 : \u03a0 \u2192 B, represents whether a policy \u03c0 \u2208 \u03a0 of a decision-making model D is moral or immoral within an ethical context E. Now, putting these attributes together, we offer a description of an ethically compliant autonomous system below. Definition 2. An ethically compliant autonomous system, D, E, \u03c1 , optimizes completing a task by using a decisionmaking model D while following an ethical framework by adhering to a moral principle \u03c1 within an ethical context E.\nAn ethically compliant autonomous system has the objective of finding an optimal policy that completes its task and follows its ethical framework. This can naturally be expressed as an optimization problem that solves for a policy within the space of policies that maximizes the value of the policy subject to the constraint that the policy satisfies the moral principle. We now turn to a description of the objective of an ethically compliant autonomous system below. Definition 3. The objective of an ethically compliant autonomous system is to find an optimal moral policy, \u03c0 * \u03c1 \u2208 \u03a0, by solving for a policy \u03c0 \u2208 \u03a0 within the space of policies \u03a0 that maximizes a value function V \u03c0 subject to a moral principle \u03c1 in the following optimization problem.\nmaximize \u03c0\u2208\u03a0 V \u03c0 subject to \u03c1(\u03c0)\nThe objective of a standard autonomous system has typically been to find an optimal amoral policy, \u03c0 * \u2208 \u03a0, that only completes its task without following any ethical framework.\nFigure 1 illustrates the objective of both an ethically compliant autonomous system and a standard autonomous system. For a moral principle \u03c1, the space of policies \u03a0 has been partitioned into a moral region \u03a0 \u03c1 and an immoral region \u03a0 \u00ac\u03c1 . The moral region \u03a0 \u03c1 contains the optimal moral policy \u03c0 * \u03c1 \u2208 \u03a0 \u03c1 of the ethically compliant autonomous system while the immoral region \u03a0 \u00ac\u03c1 contains the optimal amoral policy \u03c0 * \u2208 \u03a0 \u00ac\u03c1 of the standard autonomous system. In general, the optimal amoral policy \u03c0 * \u2208 \u03a0 can be contained by either the moral region \u03a0 \u03c1 or the immoral region \u03a0 \u00ac\u03c1 .\nAn ethically compliant autonomous system may follow an ethical framework that negatively impacts completing its task. In this situation, a developer can evaluate the cost of this impact by calculating the maximum difference across all states between the value function of the optimal moral policy and the value function of the optimal amoral policy. We describe this idea more formally below. Definition 4. Given the optimal moral policy \u03c0 * \u03c1 \u2208 \u03a0 and the optimal amoral policy \u03c0 * \u2208 \u03a0, the price of morality, \u03c8, can be represented by the expression\n\u03c8 = V \u03c0 * \u03c1 \u2212 V \u03c0 * \u221e .\nIn fact, an ethically compliant autonomous system may even follow an ethical framework that is mutually exclusive with completing its task. In this situation, a developer should reconsider the moral implications of the system and could augment the decision-making model or adjust the ethical context if deemed safe. Intuitively, the system can be called either feasible or infeasible depending on whether or not there is a solution to the optimization problem. We express this notion more formally below. Definition 5. An ethically compliant autonomous system is realizable if there exists a policy \u03c0 \u2208 \u03a0 such that its moral principle \u03c1(\u03c0) is satisfied. Otherwise, it is unrealizable.", "publication_ref": ["b9", "b56", "b12", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Moral Constraint Type Conjunctions", "text": "Operations Computations Naturally, to find the optimal moral policy by solving the optimization problem of an ethically compliant autonomous system, we use mathematical programming. This process involves four steps. First, the moral principle is mapped to a moral constraint in terms of the occupancy measures of a policy. We show that this mapping can be performed below. Theorem 1. A moral principle, \u03c1 : \u03a0 \u2192 B, can be expressed as a moral constraint c \u03c1 (\u00b5) in terms of the matrix of occupancy measures \u00b5 for a given policy \u03c0 \u2208 \u03a0. Proof (Sketch) 1. We start with a moral principle \u03c1(\u03c0) using a deterministic or stochastic policy \u03c0(s) or \u03c0(a|s). First, recall that the discounted number of times that an action a \u2208 A is performed in a state s \u2208 S is an occupancy measure \u00b5 s a . Next, observe that the discounted number of times that a state s \u2208 S is visited is the expression a\u2208A \u00b5 s a . Finally, a policy \u03c0(s) or \u03c0(a|s) is thus arg max a\u2208A \u00b5 s a / a\u2208A \u00b5 s a or \u00b5 s a / a\u2208A \u00b5 s a . Therefore, by substitution, we end with a moral constraint c \u03c1 (\u00b5) that maps to a moral principle \u03c1(\u03c0).\nc\u03c1 F (\u00b5) = \u2227 s\u2208S,a\u2208A,f \u2208F T (s, a, f )\u00b5 s a = 0 Linear |S||A||F | 2 2|S||A||F | c\u03c1 \u2206 (\u00b5) = s\u2208S,a\u2208A \u00b5 s a s \u2208S T (s, a, s ) \u03b4\u2208\u2206 s \u03c6(\u03b4, s ) \u2264 \u03c4 Linear 1 3|S||A||S||\u2206| + 1 3|S||A||S||\u2206| + 1 c\u03c1 M (\u00b5) = \u2227s\u2208S,a\u2208A \u00b5 s a \u2264 [\u03b1(s, a)] Linear |S||A| 1 + 3L|M| |S||A| 1 + 3L|M|\nSecond, the moral principle is considered either linear or nonlinear depending on the form of its moral constraint. If the moral constraint is linear in the occupancy measures of a policy, the moral principle is linear. Otherwise, it is nonlinear. Although we use linear moral principles for the ethical theories considered in this paper, it is possible to use nonlinear moral principles for ethical theories like utilitarianism and Kantianism. We formalize this property below. Definition 6. A moral principle, \u03c1 : \u03a0 \u2192 B, is linear if it can be expressed as a moral constraint c \u03c1 (\u00b5) that is linear with respect to the matrix of occupancy measures \u00b5 for a given policy \u03c0 \u2208 \u03a0. Otherwise, it is nonlinear.\nThird, the optimization problem is described as mathematical program. As expected, to represent task completion, following the linear program of an MDP in the dual form, the program maximizes a set of occupancy measures \u00b5 s a for the discounted number of times an action a \u2208 A is performed in a state s \u2208 S subject to a set of constraints that maintain consistent and nonnegative occupancy. More importantly, to represent ethical compliance, the program uses a moral constraint c \u03c1 (\u00b5) derived from the moral principle \u03c1(\u03c0) given a matrix of occupancy measures \u00b5 for a policy \u03c0.\nFourth, the mathematical program is solved to find the optimal moral policy. Given a linear moral principle, it can be solved with linear programming techniques, such as the simplex method or the criss-cross algorithm (Bertsimas and Tsitsiklis 1997). However, given a nonlinear moral principle, it can be solved with nonlinear programming techniques (Bertsekas 1997). Note that this four-step process can also be used with the primal form of the linear program.", "publication_ref": ["b15", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Frameworks", "text": "In this section, we offer a range of ethical frameworks that can be used to build an ethically compliant autonomous system. Each ethical framework approximates a well-known ethical theory in moral philosophy (Shafer-Landau 2009). During the design of an ethical framework, a developer must select a representation for the ethical context and the moral principle. This involves choosing the contextual details of the ethical context and the logical structure of the moral principle that best describe the moral implications of the system.\nTable 1 offers the moral constraints that have been derived from the moral principle of each ethical framework. For each moral constraint, there are several columns that describe its computational tractability. The Type column lists whether the moral constraint is linear or nonlinear with respect to the occupancy measures of a policy. The Conjunctions column states the number of logical conjunctions that compose the moral constraint. The Operations column indicates an upper bound on the number of arithmetic, comparison, and logical operations that must be performed for each logical conjunction. The Computations column contains an upper bound on the number of computations that must be executed for the moral constraint to evaluate the moral status of a policy.\nWe present a set of simplified ethical frameworks examples below. Their purpose is to encode an ethical theory in a tractable way that may not capture all nuances of an ethical theory. We encourage work on more complex ethical frameworks that reflect the nuances of different ethical theories.", "publication_ref": ["b48"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Divine Command Theory", "text": "Divine command theory (DCT), a monistic, absolutist ethical theory, holds that the morality of an action is based on whether a divine entity commands or forbids that action (Idziak 1979;Quinn 2013). Similar to earlier work on dead ends (Kolobov, Mausam, and Weld 2012), we consider an ethical framework that requires a policy that selects actions that have a nil probability of transitioning to any forbidden state (Mouaddib, Jeanpierre, and Zilberstein 2015). Definition 7. A DCT ethical context, E F , is represented by a tuple, E F = F , where F is a set of forbidden states. Definition 8. A DCT moral principle, \u03c1 F , is expressed as the following equation:\n\u03c1 F (\u03c0) = s\u2208S,f \u2208F T (s, \u03c0(s), f ) = 0 .\nNote that the DCT moral constraint c \u03c1 F in Table 1 ", "publication_ref": ["b32", "b42", "b36", "b40"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Prima Facie Duties", "text": "Prima facie duties (PFD), a pluralistic, nonabsolutist ethical theory, holds that the morality of an action is based on whether that action fulfills fundamental moral duties that can contradict each other (Ross 1930;Morreau 1996). Related to recent work on norm conflict resolution (Kasenberg and Scheutz 2018), we consider an ethical framework that requires a policy that selects actions that do not neglect duties of different penalties within some tolerance. Definition 9. A PFD ethical context, E \u2206 , is represented by a tuple, E \u2206 = \u2206, \u03c6, \u03c4 , where \u2022 \u2206 is a set of duties, \u2022 \u03c6 : \u2206 \u00d7 S \u2192 R + is a penalty function that represents the expected immediate penalty for neglecting a duty \u03b4 \u2208 \u2206 in a state s \u2208 S, and \u2022 \u03c4 \u2208 R + is a tolerance. Definition 10. A PFD moral principle, \u03c1 \u2206 , is expressed as the following equation:\n\u03c1 \u2206 (\u03c0) = s\u2208S d(s)J \u03c0 (s) \u2264 \u03c4.\nThe expected cumulative penalty, J \u03c0 : S \u2192 R, is below:\nJ \u03c0 (s) = s \u2208S T (s, \u03c0(s), s ) \u03b4\u2208\u2206 s \u03c6(\u03b4, s ) + J \u03c0 (s ) ,\nwhere \u2206 s is the set of duties neglected in a state s \u2208 S. Note that the PFD moral constraint c \u03c1\u2206 in Table 1 requires 3|S||A||S||\u2206| + 1 computations in the worst case as 3|S||A||S||\u2206|+1 operations are performed in 1 conjunction.", "publication_ref": ["b46", "b39", "b35"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Virtue Ethics", "text": "Virtue ethics (VE), a monistic, absolutist ethical theory, holds that the morality of an action is based on whether a virtuous person who acts in character performs that action in a similar situation (Anscombe 1958;Hursthouse 1999). Drawing on its natural connection to learning by demonstration from a human operator with domain expertise (Atkeson and Schaal 1997), we consider an ethical framework that requires a policy that selects actions that align with any moral trajectory performed by a moral exemplar.  ", "publication_ref": ["b4", "b31", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Autonomous Driving", "text": "We turn to an application of ethically compliant autonomous systems to autonomous driving. An ethically compliant selfdriving vehicle must complete a navigation task by driving from an origin to a destination within a city. However, to follow a given ethical framework, the ethically compliant selfdriving vehicle must adjust its route and speed depending on the type and pedestrian traffic of each road to avoid harming people and damaging property. Note that our approach can be used in many other applications, such as a security robot that patrols a college campus or a robot assistant that navigates a grocery store to help customers or prevent theft. We describe how to separate task completion and ethical compliance in an ethically compliant self-driving vehicle below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task Completion", "text": "The vehicle must complete a navigation task by driving from a start location \u03bb 0 \u2208 \u039b to a goal location \u03bb g \u2208 \u039b along a set of roads \u2126 in a city with a set of locations \u039b. At each location \u03bb \u2208 \u039b, the vehicle must turn onto a road \u03c9 \u2208 \u2126. Each road \u03c9 \u2208 \u2126 is a type \u03c5 \u2208 \u03a5 that indicates either a city street, county road, or highway with a low, medium, or high speed limit. Once the vehicle turns onto a road \u03c9 \u2208 \u2126, the vehicle observes the pedestrian traffic \u03b8 \u2208 \u0398 as either light or heavy with a probability Pr(\u0398 = \u03b8). After the vehicle observes the pedestrian traffic \u03b8 \u2208 \u0398, the vehicle accelerates to a speed \u03c3 \u2208 \u03a3 that reflects either a low, normal, or high speed under, at, or above the speed limit. To drive along the road \u03c9 \u2208 \u2126 from the current location \u03bb \u2208 \u039b to the next location \u03bb \u2208 \u039b, the vehicle cruises at the speed \u03c3 \u2208 \u03a3. This is repeated until the vehicle arrives at the goal location \u03bb g \u2208 \u039b.\nMore formally, we represent the decision-making model of the navigation task by an MDP D = S, A, T, R, d . The set of states S = S \u039b \u222a S \u2126 has a set of location states S \u039b for being at a location \u03bb \u2208 \u039b and a set of road states S \u2126 for being on a road \u03c9 \u2208 \u2126 of a type \u03c5 \u2208 \u03a5 with a pedestrian traffic \u03b8 \u2208 \u0398 at a speed \u03c3 \u2208 \u03a3. The set of actions A = A \u2126 \u222a A \u03a3 \u222a {\u2297, } has a set of turn actions A \u2126 for turning onto a road \u03c9 \u2208 \u2126, a set of accelerate actions A \u03a3 for accelerating to a speed \u03c3 \u2208 \u03a3, a stay action \u2297, and a cruise action . The transition function T : S \u00d7 A \u00d7 S \u2192 [0, 1] reflects the dynamics of a turn action a \u2208 A \u2126 and a stay action \u2297 in a location state \u03bb \u2208 S \u039b or an accelerate action a \u2208 A \u03a3 and a cruise action in a road state s \u2208 S \u2126 (with a selfloop for any invalid action a \u2208 A). The reward function R : S \u00d7A\u00d7S \u2192 R reflects the duration of a turn action a \u2208 A \u2126 from a location state S \u039b to a road state s \u2208 S \u2126 , a stay action \u2297 at a location state \u03bb \u2208 S \u039b , an accelerate action a \u2208 A \u03a3 at a road state s \u2208 S \u2126 , and a cruise action from a road state s \u2208 S \u2126 to a location state S \u039b (with an infinite duration for any invalid action a \u2208 A and a nil duration for a stay action \u2297 at a state s \u2208 S that represents the goal location \u03bb g \u2208 \u039b). The start state function d : S \u2192 [0, 1] has unit probability at a state s \u2208 S that represents the start location \u03bb 0 \u2208 \u039b and nil probability at every other state s \u2208 S.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Compliance", "text": "The vehicle must follow one of the ethical frameworks. First, the vehicle can follow DCT with forbidden states comprised Next, the vehicle can follow PFD with duties comprised of smooth operation \u03b4 1 and careful operation \u03b4 2 . Smooth operation \u03b4 1 is neglected in any road state at low speed with light pedestrian traffic while careful operation \u03b4 2 is neglected in any road state at high speed or at normal speed with heavy pedestrian traffic. When smooth operation \u03b4 1 and careful operation \u03b4 2 are neglected, they incur a low and high penalty that changes with any pedestrian traffic. Neglecting duties is permitted until a limit . With the PFD moral principle \u03c1 \u2206 , we represent the PFD ethical context by a tuple, E \u2206 = \u2206, \u03c6, \u03c4 , where \u2206 = {\u03b4 1 , \u03b4 2 } is the set of duties, \u03c6 : \u2206 \u00d7 S \u2192 R + is the penalty function that represents the expected immediate penalty for neglecting smooth operation \u03b4 1 \u2208 \u2206 and careful operation \u03b4 2 \u2208 \u2206 in a state s \u2208 S with a pedestrian traffic \u03b8 \u2208 \u0398, and \u03c4 = is the tolerance.\nFinally, the vehicle can follow VE with moral trajectories comprised of cautious trajectories C and proactive trajectories P. Cautious trajectories C exemplify driving on any road state at normal speed with light pedestrian traffic or at low speed with heavy pedestrian traffic and proactive trajectories P exemplify avoiding any highway road states and a set of populated location states. With the VE moral principle \u03c1 M , we represent the VE ethical context by a tuple, E M = M , where M = C \u222a P is the set of moral trajectories.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We now demonstrate that the application of ethically compliant autonomous systems to autonomous driving is effective in a set of simulations and a user study.\nIn the set of simulations, a standard self-driving vehicle that cannot follow any ethical framework and an ethically compliant self-driving vehicle that can follow different ethical frameworks must complete a set of navigation tasks.\nEach navigation task can use a different start location \u03bb 0 \u2208 \u039b and goal location \u03bb g \u2208 \u039b based on the city in Figure 2. The speed limits of city streets, county roads, and highways are 25, 45, and 75 MPH. The probability Pr(\u0398 = \u03b8) of observing light or heavy pedestrian traffic \u03b8 \u2208 \u0398 is 0.8 and 0.2. A low, normal, and high speed is 10 MPH under, at, and 10 MPH above the speed limit. Turning onto a road \u03c9 \u2208 \u2126 from a location \u03bb \u2208 \u039b requires 5 seconds. Accelerating 10 MPH requires 2 seconds. Cruising requires a time equal to the distance of the road \u03c9 \u2208 \u2126 divided by the speed \u03c3 \u2208 \u03a3. Staying at a location \u03bb \u2208 \u039b other than the goal location \u03bb g \u2208 \u039b requires 120 seconds.\nEach ethical framework can use different settings. For DCT, the forbidden states F can be just hazardous states H or both hazardous states H and inconsiderate states I. For PFD, the tolerance \u03c4 = can be the limit = 3, = 6, or = 9. For VE, the moral trajectories can be just cautious trajectories C or both cautious trajectories C and proactive trajectories P that avoid any highway road states and a set of populated location states that contains the School and College locations with many students on campus.\nTable 2 shows that the price of morality incurred by the agent is appropriate for each ethical framework. The standard self-driving vehicle does not incur a price of morality. However, the ethically compliant self-driving vehicle incurs a price of morality that increases with more forbidden states for DCT, decreases with more tolerance for PFD, and increases with more moral trajectories for VE.\nFigure 5 shows that the behavior performed by the agent is appropriate for each ethical framework. The standard selfdriving vehicle drives the shortest route at high speed. However, the ethically compliant self-driving vehicle differs for each ethical framework. For DCT, the vehicle drives the shortest route at low or normal speed based on pedestrian traffic. For PFD, the vehicle drives the shortest route at low Table 2: The price of morality relative to the value of the optimal amoral policy for each vehicle on all navigation tasks.\nFigure 4: The results of the user study. For each task and location in the city, a point denotes the resulting policy. For each policy, the horizontal axis is its time savings relative to the policy from the opposing task while the vertical axis is its number of violations, averaged over 10 simulations. The moral and immoral regions are highlighted in green and red.\nor normal speed based on pedestrian traffic aside from driving on the first road at normal or high speed with some probability for light pedestrian traffic and at normal speed for heavy pedestrian traffic due to the tolerance. For VE, the vehicle drives at low or normal speed based on pedestrian traffic but drives a different route to avoid any highway road states and the set of populated location states.\nIn the user study, 7 planning and robotics experts with experience in MDPs but not ethics had to complete two tasks that implemented an ethically compliant self-driving vehicle in a random order. In both tasks, developers were given the decision-making model for navigating the city from any start location to the OFFICE location. They then had to enforce the following moral requirements: the self-driving vehicle should drive at high speed with light pedestrian traffic or at normal speed with heavy pedestrian traffic at most once in expectation but should never drive at high speed with heavy pedestrian traffic. In one task, developers were asked to enforce these requirements by modifying the reward function of the decision-making model, specifically an MDP. In the other task, developers were asked to enforce these requirements by defining the ethical context of an ethical framework, specifically PFD. The user study therefore evaluates the accuracy and usability of modifying a decision-making model versus defining an ethical framework.\nFigure 4 shows that defining the ethical context of the ethical framework led to better policies than modifying the reward function of the decision-making model. In our approach, all policies optimize the navigation task and satisfy the requirements with exactly one violation. However, in the other approach, most policies fail to optimize the navigation task or satisfy the requirements: aggressive policies in the upper right corner with more than one violation are faster but immoral while conservative policies with less than one violation in the lower left corner are slower but moral. It is also encouraging that our method (24 minutes) had a lower mean development time than the other method (45 minutes).\nOur open source library, Morality.js, available on the website https://www.moralityjs.com with the customizable grid world environment dashboard in Figure 3, was used in all experiments (Svegliato, Nashed, and Zilberstein 2020a,b).", "publication_ref": [], "figure_ref": ["fig_3", "fig_5", "fig_4"], "table_ref": []}, {"heading": "Conclusion", "text": "We propose a novel approach for building ethically compliant autonomous systems that optimize completing a task while following an ethical framework. It simplifies and standardizes the process of implementing an ethical theory within autonomous systems as it is general-purpose, modular, and interpretable. We then offer a range of ethical frameworks for divine command theory, prima facie duties, and virtue ethics. Finally, we demonstrate the accuracy and usability of our approach in a set of autonomous driving simulations and a user study of planning and robotics experts. Future work will develop nuanced ethical frameworks for the ethical theories in this paper and explore new ethical frameworks for ethical theories like utilitarianism and Kantianism.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethics Statement", "text": "Although we discuss important ethical considerations surrounding ethically compliant autonomous systems throughout the paper, we highlight three ethical implications below.\nFirst, we stress that simply defining some ethically compliant autonomous system does not guarantee that it exhibits perfect ethical compliance with respect to its developers or stakeholders in the real world. In fact, similar to any autonomous system, the quality of an ethically compliant autonomous system is limited by the accuracy of its decision-making model, ethical framework, and ethical context. If these attributes have not been specified in a way that reflects the intentions of its developers or the values of its stakeholders, the system may still result in undesirable consequences. Moreover, any conflict between stakeholders can perhaps be resolved using multiple ethical frameworks together by forming a moral principle that is a conjunction over the moral principle for each ethical framework. It is therefore critical that developers seek continual participation and feedback from a range of stakeholders who interact with the system in as many diverse situations as possible.\nNext, while our approach gives autonomous systems the ability to satisfy an arbitrary set of moral requirements, we emphasize that developers must still remain transparent about the moral requirements of their autonomous systems. This could be in the form of ethical documentation that specifies the moral requirements of the autonomous system and its limitations. For example, if a self-driving vehicle has an ethical framework that considers the level of pedestrian traffic but not the presence of wildlife along a route, there should be ethical documentation in the user manual that is provided to the owners of the vehicle. Hence, by providing ethical documentation prior to the deployment of the autonomous system, deployers can ensure that any conditions necessary for ethical compliance are satisfied throughout operation.\nFinally, even though our approach gives autonomous system the ability to comply with a given ethical theory, we highlight that developers must still think carefully about the design and development of their autonomous systems. This involves selecting the moral requirements of the autonomous system, which can include determining the best ethical theory, the best ethical framework for that ethical theory, and the best settings for that ethical framework. In other words, our approach does not substitute for the deliberate process of determining the best way to build an ethically compliant autonomous system. In fact, developers should avoid intentionally selecting ethically compliant autonomous systems that are easy to implement in practice. However, as the vast discourse surrounding the best ethical theory to use in autonomous systems continues to evolve over time, our approach can be used in a way that reflects this discussion.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their valuable comments and Alan Labouseur for his helpful feedback on the Morality.js library. This work was supported in part by NSF Graduate Research Fellowship DGE-1451512, NSF grant IIS-1724101, and NSF grant IIS-1813490.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Reinforcement learning as a framework for ethical decisions", "journal": "", "year": "2016", "authors": "D Abel; J Macglashan; M L Littman"}, {"ref_id": "b1", "title": "Designing a value-driven future for ethical autonomous and intelligent systems", "journal": "Proceedings of the IEEE", "year": "2019", "authors": "G Adamson; J C Havens; R Chatila"}, {"ref_id": "b2", "title": "Artificial morality: Topdown, bottom-up, and hybrid approaches", "journal": "Ethics and Information Technology", "year": "2005", "authors": "C Allen; I Smit; W Wallach"}, {"ref_id": "b3", "title": "A value driven agent: An instantiation of a case-supported principle-based behavior paradigm", "journal": "", "year": "2017", "authors": "M Anderson; S L Anderson; V Berenz"}, {"ref_id": "b4", "title": "Modern moral philosophy", "journal": "Philosophy", "year": "1958", "authors": "G E M Anscombe"}, {"ref_id": "b5", "title": "Governing lethal behavior: Embedding ethics in a hybrid deliberative/reactive robot architecture", "journal": "ACM", "year": "2008", "authors": "R C Arkin"}, {"ref_id": "b6", "title": "Robot learning from demonstration", "journal": "", "year": "1997", "authors": "C G Atkeson; S Schaal"}, {"ref_id": "b7", "title": "Addressing moral problems through practical reasoning", "journal": "Springer", "year": "2006", "authors": "K Atkinson; T Bench-Capon"}, {"ref_id": "b8", "title": "Learning to optimize autonomy in competence-aware systems", "journal": "", "year": "2020", "authors": "C Basich; J Svegliato; K H Wray; S Witwicki; J Biswas; S Zilberstein"}, {"ref_id": "b9", "title": "Improving competence for reliable autonomy", "journal": "", "year": "2021", "authors": "C Basich; J Svegliato; S Zilberstein; K H Wray; S J Witwicki"}, {"ref_id": "b10", "title": "On the theory of dynamic programming", "journal": "National Academy of Sciences of the United States of America", "year": "1952", "authors": "R Bellman"}, {"ref_id": "b11", "title": "Norms and value based reasoning: Justifying compliance and violation", "journal": "Artificial Intelligence and Law", "year": "2017", "authors": "T Bench-Capon; S Modgil"}, {"ref_id": "b12", "title": "An introduction to the principles of morals", "journal": "", "year": "1789", "authors": "J Bentham"}, {"ref_id": "b13", "title": "Modelling moral reasoning and ethical responsibility with logic programming", "journal": "Springer", "year": "2015", "authors": "F Berreby; G Bourgne; J.-G Ganascia"}, {"ref_id": "b14", "title": "Nonlinear programming", "journal": "Journal of the Operational Research Society", "year": "1997", "authors": "D P Bertsekas"}, {"ref_id": "b15", "title": "Introduction to linear optimization", "journal": "", "year": "1997", "authors": "D Bertsimas; J N Tsitsiklis"}, {"ref_id": "b16", "title": "Principles of robotics: regulating robots in the real world", "journal": "Connection Science", "year": "2017", "authors": "M Boden; J Bryson; D Caldwell; K Dautenhahn; L Edwards; S Kember; P Newman; V Parry; G Pegman; T Rodden; T Sorrell; M Wallis; B Whitby; A Winfield"}, {"ref_id": "b17", "title": "Superintelligence: Paths, Dangers, Strategies", "journal": "Wiley Online Library", "year": "2016", "authors": "N Bostrom"}, {"ref_id": "b18", "title": "Ethical aspects of facial recognition systems in public places", "journal": "Journal of Information, Communication, and Ethics In Society", "year": "2004", "authors": "P Brey"}, {"ref_id": "b19", "title": "Toward a general logicist methodology for engineering ethically correct robots", "journal": "IEEE Intelligent Systems", "year": "2006", "authors": "S Bringsjord; K Arkoudas; P Bello"}, {"ref_id": "b20", "title": "Piagetian roboethics via category theory: Moving beyond mere formal operations to engineer robots whose decisions are guaranteed to be ethically correct", "journal": "Cambridge University Press", "year": "2011", "authors": "S Bringsjord; J Taylor; B Van Heuveln; K Arkoudas; M Clark; R Wojtowicz"}, {"ref_id": "b21", "title": "Dark matters: On the surveillance of blackness", "journal": "Duke University Press", "year": "2015", "authors": "S Browne"}, {"ref_id": "b22", "title": "Networked authoritarianism is on the rise", "journal": "Sicherheit und Frieden", "year": "2017", "authors": "T Burgers; D R Robinson"}, {"ref_id": "b23", "title": "Towards moral autonomous systems. In arXiv preprint", "journal": "", "year": "2017", "authors": "V Charisi; L Dennis; M Fisher; R Lieck; A Matthias; M Slavkovik; J Sombetzki; A F Winfield; R Yampolskiy"}, {"ref_id": "b24", "title": "Formal verification of ethical choices in autonomous systems", "journal": "Robotics and Autonomous Systems", "year": "2016", "authors": "L Dennis; M Fisher; M Slavkovik; M Webster"}, {"ref_id": "b25", "title": "Trust but verify: A guide to algorithms and the law", "journal": "Harvard Journal of Law and Technology", "year": "2017", "authors": "D R Desai; J A Kroll"}, {"ref_id": "b26", "title": "Ethics by Design: necessity or curse", "journal": "", "year": "2018", "authors": "V Dignum; M Baldoni; C Baroglio; M Caon; R Chatila; L Dennis; G G\u00e9nova; G Haim; M S Klie\u00df; M Lopez-Sanchez"}, {"ref_id": "b27", "title": "Design-oriented human-computer interaction", "journal": "", "year": "2003", "authors": "D Fallman"}, {"ref_id": "b28", "title": "The handbook of information and computer ethics", "journal": "", "year": "2008", "authors": "B Friedman; P H Kahn; A Borning"}, {"ref_id": "b29", "title": "European Union regulations on algorithmic decision-making and a \"right to explanation", "journal": "AI magazine", "year": "2017", "authors": "B Goodman; S Flaxman"}, {"ref_id": "b30", "title": "Incomplete contracting and AI alignment", "journal": "", "year": "2019", "authors": "D Hadfield-Menell; G K Hadfield"}, {"ref_id": "b31", "title": "On virtue ethics", "journal": "Oxford University Press", "year": "1999", "authors": "R Hursthouse"}, {"ref_id": "b32", "title": "Divine command morality", "journal": "Edwin Mellen Press", "year": "1979", "authors": "J M Idziak"}, {"ref_id": "b33", "title": "Picturing algorithmic surveillance: The politics of facial recognition systems", "journal": "Surveillance & Society", "year": "2004", "authors": "L Introna; D Wood"}, {"ref_id": "b34", "title": "Groundwork for the metaphysics of morals", "journal": "Yale University Press", "year": "2002", "authors": "I Kant; J B Schneewind"}, {"ref_id": "b35", "title": "Norm conflict resolution in stochastic domains", "journal": "", "year": "2018", "authors": "D Kasenberg; M Scheutz"}, {"ref_id": "b36", "title": "A theory of goaloriented MDPs with dead ends", "journal": "", "year": "2012", "authors": "A Kolobov; D S Weld"}, {"ref_id": "b37", "title": "Linear programming and sequential decisions", "journal": "Management Science", "year": "1960", "authors": "A S Manne"}, {"ref_id": "b38", "title": "The nature, importance, and difficulty of machine ethics", "journal": "IEEE Intelligent Systems", "year": "2006", "authors": "J H Moor"}, {"ref_id": "b39", "title": "Prima Facie and seeming duties", "journal": "Studia Logica", "year": "1996", "authors": "M Morreau"}, {"ref_id": "b40", "title": "Handling advice in MDPs for semi-autonomous systems", "journal": "", "year": "2015", "authors": "A.-I Mouaddib; L Jeanpierre; S Zilberstein"}, {"ref_id": "b41", "title": "Toward a fourth law of robotics: Preserving attribution, responsibility, and explainability in an algorithmic society", "journal": "Ohio State Law Journal", "year": "2017", "authors": "F Pasquale"}, {"ref_id": "b42", "title": "Divine command theory. The Blackwell guide to ethical theory", "journal": "", "year": "2013", "authors": "P L Quinn"}, {"ref_id": "b43", "title": "Technology, ethics, and access to justice: Should an algorithm be deciding your case", "journal": "Michigan Journal of International Law", "year": "2013", "authors": "A H Raymond; S J Shackelford"}, {"ref_id": "b44", "title": "Designing sociotechnical systems with cognitive work analysis: Putting theory back into practice", "journal": "Ergonomics", "year": "2015", "authors": "G J Read; P M Salmon; M G Lenn\u00e9; N A Stanton"}, {"ref_id": "b45", "title": "Engineering-based design methodology for embedding ethics in autonomous robots", "journal": "Proceedings of the IEEE", "year": "2019", "authors": "L J Robertson; R Abbas; G Alici; A Munoz; K Michael"}, {"ref_id": "b46", "title": "The right and the good", "journal": "Oxford University Press", "year": "1930", "authors": "W D Ross"}, {"ref_id": "b47", "title": "Regulating artificial intelligence systems: Risks, challenges, competencies, and strategies", "journal": "Harvard Journal of Law and Technology", "year": "2015", "authors": "M U Scherer"}, {"ref_id": "b48", "title": "The fundamentals of ethics", "journal": "Oxford University Press", "year": "2009", "authors": "R Shafer-Landau"}, {"ref_id": "b49", "title": "Towards provably moral AI agents in bottom-up learning frameworks", "journal": "", "year": "2018", "authors": "N P Shaw; A St\u00f6ckel; R W Orr; T F Lidbetter; R Cohen"}, {"ref_id": "b50", "title": "An Intervening Ethical Governor for a robot mediator in patient-caregiver relationship", "journal": "", "year": "2017", "authors": "J Shim; R Arkin; M Pettinatti"}, {"ref_id": "b51", "title": "An integrated approach to moral autonomous systems", "journal": "", "year": "2020", "authors": "J Svegliato; S Nashed; S Zilberstein"}, {"ref_id": "b52", "title": "Ethically compliant planning in moral autonomous systems", "journal": "", "year": "2020", "authors": "J Svegliato; S B Nashed; S Zilberstein"}, {"ref_id": "b53", "title": "Belief space metareasoning for exception recovery", "journal": "", "year": "2019", "authors": "J Svegliato; K H Wray; S J Witwicki; J Biswas; S Zilberstein"}, {"ref_id": "b54", "title": "Alignment for advanced machine learning systems", "journal": "", "year": "2016", "authors": "J Taylor; E Yudkowsky; P Lavictoire; A Critch"}, {"ref_id": "b55", "title": "Contextual deontic logic: Normative agents, violations and independence", "journal": "Annals of mathematics and artificial intelligence", "year": "2003", "authors": "L Van Der Torre"}, {"ref_id": "b56", "title": "An architecture for ethical robots inspired by the simulation theory of cognition", "journal": "Cognitive Systems Research", "year": "2018", "authors": "D Vanderelst; A Winfield"}, {"ref_id": "b57", "title": "Towards an ethical robot: Internal models, consequences and ethical action selection", "journal": "Springer", "year": "2014", "authors": "A F Winfield; C Blum; W Liu"}, {"ref_id": "b58", "title": "On obligations and normative ability: Towards a logical analysis of the social contract", "journal": "Journal of Applied Logic", "year": "2005", "authors": "M Wooldridge; W Van Der Hoek"}, {"ref_id": "b59", "title": "The gaze of the perfect search engine: Google as an infrastructure of dataveillance", "journal": "Springer", "year": "2008", "authors": "M Zimmer"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure1: A simple view of the goal of an ethically compliant autonomous system (green) and the goal of a standard autonomous system (red) in terms of the space of policies.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Definition 11. A VE ethical context, E M , is represented by a tuple, E M = M , where M is a set of moral trajectories. Definition 12. A VE moral principle, \u03c1 M , is expressed as the following equation: \u03c1 M (\u03c0) = s\u2208S \u03b1(s, \u03c0(s)). The alignment function, \u03b1 : S \u00d7 A \u2192 B, is below: \u03b1(s, a) = \u2203 m\u2208M,0\u2264i\u2264 s = m(s i ) \u2227 a = m(a i ) , where m(s i ) and m(a i ) are the ith state and the ith action of a moral trajectory m = s 0 , a 0 , s 1 , a 1 , . . . , s \u22121 , a \u22121 , s of length \u2264 L bounded by a maximum length L. Note that the VE moral constraint c \u03c1 M in", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: A city with different places that are connected by city streets, county roads, and highways.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: An agent completes a task and follows an ethical framework with a blue amoral path and a green moral path in the Morality.js customizable grid world environment.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: The optimal policies for select vehicles with (a) no ethical framework, (b) DCT with H \u222a I, (c) PFD with = 9, and (d) VE with C \u222a P on a navigation task. A blue node denotes a location and a gray node denotes pedestrian traffic. With a thickness that represents probability, a gray line denotes turning onto a road and an orange, green, or purple line denotes cruising at high, normal, or low speed.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "The moral constraints that have been derived from the moral principle of each ethical framework.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "needs 2|S||A||F | computations in the worst case because 2 operations are performed in |S||A||F | conjunctions.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "|S||A|(1 + 3L|M|) computations in the worst case since 1+ 3L|M| operations are performed in |S||A| conjunctions.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "maximize \u03c0\u2208\u03a0 V \u03c0 subject to \u03c1(\u03c0)", "formula_coordinates": [3.0, 405.07, 273.74, 67.36, 31.22]}, {"formula_id": "formula_1", "formula_text": "\u03c8 = V \u03c0 * \u03c1 \u2212 V \u03c0 * \u221e .", "formula_coordinates": [3.0, 468.15, 553.93, 88.4, 13.29]}, {"formula_id": "formula_2", "formula_text": "c\u03c1 F (\u00b5) = \u2227 s\u2208S,a\u2208A,f \u2208F T (s, a, f )\u00b5 s a = 0 Linear |S||A||F | 2 2|S||A||F | c\u03c1 \u2206 (\u00b5) = s\u2208S,a\u2208A \u00b5 s a s \u2208S T (s, a, s ) \u03b4\u2208\u2206 s \u03c6(\u03b4, s ) \u2264 \u03c4 Linear 1 3|S||A||S||\u2206| + 1 3|S||A||S||\u2206| + 1 c\u03c1 M (\u00b5) = \u2227s\u2208S,a\u2208A \u00b5 s a \u2264 [\u03b1(s, a)] Linear |S||A| 1 + 3L|M| |S||A| 1 + 3L|M|", "formula_coordinates": [4.0, 57.99, 72.28, 492.61, 44.01]}, {"formula_id": "formula_3", "formula_text": "\u03c1 F (\u03c0) = s\u2208S,f \u2208F T (s, \u03c0(s), f ) = 0 .", "formula_coordinates": [4.0, 371.3, 644.52, 134.9, 20.14]}, {"formula_id": "formula_4", "formula_text": "\u03c1 \u2206 (\u03c0) = s\u2208S d(s)J \u03c0 (s) \u2264 \u03c4.", "formula_coordinates": [5.0, 114.15, 275.77, 118.2, 22.14]}, {"formula_id": "formula_5", "formula_text": "J \u03c0 (s) = s \u2208S T (s, \u03c0(s), s ) \u03b4\u2208\u2206 s \u03c6(\u03b4, s ) + J \u03c0 (s ) ,", "formula_coordinates": [5.0, 70.28, 320.69, 205.93, 23.66]}], "doi": ""}