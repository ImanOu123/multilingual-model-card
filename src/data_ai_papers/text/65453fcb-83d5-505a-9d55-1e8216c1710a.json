{"title": "Mind the instructions: a holistic evaluation of consistency and interactions in prompt-based learning", "authors": "Lucas Weber; Elia Bruni; Dieuwke Hupkes", "pub_date": "", "abstract": "Finding the best way of adapting pre-trained language models to a task is a big challenge in current NLP. Just like the previous generation of task-tuned models (TT), models that are adapted to tasks via in-context-learning (ICL) are robust in some setups but not in others. Here, we present a detailed analysis of which design choices cause instabilities and inconsistencies in LLM predictions. First, we show how spurious correlations between input distributions and labels -a known issue in TT models -form only a minor problem for prompted models. Then, we engage in a systematic, holistic evaluation of different factors that have been found to influence predictions in a prompting setup. We test all possible combinations of a range of factors on both vanilla and instructiontuned (IT) LLMs of different scale and statistically analyse the results to show which factors are the most influential, interactive or stable. Our results show which factors can be used without precautions and which should be avoided or handled with care in most settings.", "sections": [{"heading": "Introduction", "text": "Transfer learning from large-scale pre-trained language models is nowadays the standard approach to a wide range of NLP tasks. One of its great challenges is to optimally interface information that pre-trained language models accumulate in their parameters and adapt it to the task of interest (Zhou et al., 2023;Ouyang et al., 2022). The standard approach for task adaptation has recently shifted from updating model parameters for a specific task (from here on task tuning or TT) to using promptingbased methods based on in-context learning (from here on ICL). ICL can be subdivided into few-shot (Brown et al., 2020) or zero-shot inference (primarily using instruction-tuned models Wei et al., 2022). Both approaches offer certain benefits over TT: it eliminates costly, task-specific finetuning and provides greater flexibility, as a single model can be applied to many tasks. However, ICL also currently yields overall weaker performance compared to task-tuning and is less stable and reliable on many benchmarks (see, e.g. Bang et al., 2023;Ohmer et al., 2023;Zhao et al., 2021).\nWhile for TT, much research has been conducted to understand weaknesses in the paradigm (for an overview, see , the sources of instabilities in ICL remain nebulous. Since ICL is more constrained (less data and no parameter updates), out-of-distribution generalisation has been suggested to be less of a problem (Awadalla et al., 2022;Si et al., 2023). On the other hand, new frontiers emerge. For example, the format, order, or semantics of provided in-context examples can greatly influence learning outcomes, as does the proportion of labels in the context and the exact labels used . Little is known, however, about how these factors interact (work from Wei et al., 2023;Yoo et al., 2022, suggests that they cannot be isolated); it is unclear which aspects are consistently beneficial, which vary across setups, and which are sensible to combine or decouple. The volatility of the paradigm warrants more research into the reliability of different design choices.\nIn this paper, we conduct a detailed exploration of vanilla and instruction-tuned LLMs across various shifts and setups to understand their robustness. We start with one of the prominent themes in robustness studies for TT models: robustness to spurious correlations between input and label distributions (Kavumba et al., 2019;McCoy et al., 2019;Niven and Kao, 2019) and find that in ICL, spurious correlations do not have a significant impact on learning outcomes.\nWe go on to investigate ICL's sensitivity to other features of adaptation context, as well as the consistency of predictions across different design choices. To do so, we conduct a large-scale grid search across various combinations of factors and statistically analyse the results to shed light on the inter-dependencies of different design choices. We find that the exact in-context setup (the number of in-context examples, the distribution of in-context labels, or the type of instructions given in the context) has a surprisingly small but reliable impact on prediction outcomes. On the other hand, the type of instructions used to query the target has, by far, the most significant impact on model behaviour. It is also the most volatile across settings, making it the most pivotal factor.", "publication_ref": ["b34", "b5", "b43", "b3", "b33", "b49", "b1", "b38", "b47", "b20", "b26", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Background and related work", "text": "In the following, we first briefly define TT and ICL and then cover known problems with model robustness.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Task tuning and spurious correlations", "text": "TT aligns a pre-trained model with a specific task by iteratively updating model parameters to minimise prediction loss on adaptation data. In our definition here, TT does not include finetuning on more abstract objectives like instruction tuning (IT; Wei et al., 2022). TT models often fit spurious correlations between inputs and associated labels that are idiosyncratic artefacts to the specific dataset (Niven and Kao, 2019;Kavumba et al., 2019;McCoy et al., 2019;Geva et al., 2019;Poliak et al., 2018;Gururangan et al., 2018;Kavumba et al., 2022) and do not align with the causal structure of the process that generated the data in 'the real world' (Sch\u00f6lkopf et al., 2012). Such adaptations (sometimes also referred to as 'shortcut solutions'; Geirhos et al., 2020) usually fail as soon as the data distribution shifts between the adaptation and test phase. Pre-training improves robustness compared to task training from scratch (Hendrycks et al., 2019(Hendrycks et al., , 2020. However, the necessary posthoc task adaptation still overfits spurious correlations (Niven and Kao, 2019). An effective way to mitigate issues in task adaptation is to expose the model to counterexamples of spurious correlations (Kaushik et al., 2020).", "publication_ref": ["b43", "b32", "b20", "b26", "b10", "b35", "b11", "b21", "b37", "b9", "b12", "b14", "b32", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "In-context learning", "text": "ICL describes the adaptation of a model to a task by inferring the task from the input given to the model. ICL can be subdivided into (1) few-shot learning, where in-context examples (consisting of input-output pairs) are given in the left-handed context of a tested input, and (2) zero-shot learning, referring to the case in which there are no examples. In this paper, we investigate few-shot scenarios.\nIn contrast to TT, ICL is a considerably cheaper adaptation method as it does not require any parameter updates. Aky\u00fcrek et al. (2022) and Garg et al. (2022) show that adaptation of transformer models via ICL exhibits the same degree of expressivity as simple linear algorithms, small neural networks or decision trees. While ICL emerges spontaneously with increasing size of untuned LLMs (Brown et al., 2020), the ICL performance of such 'vanilla' LLMs lags behind the tuned state-of-theart on almost all common NLP benchmarks .\nPrevious research has also shown that ICL is highly unstable. For example, the order of incontext examples , the recency of certain labels in the context (Zhao et al., 2021) or the format of the prompt  as well as the distribution of training examples and the label space ) strongly influence model performance. Curiously, whether the labels provided in the examples are *correct* is less important . However, these findings are not uncontested: Yoo et al. ( 2022) paint a more differentiated picture, demonstrating that in-context input-label mapping does matter, but that it depends on other factors such as model size or instruction verbosity. Along a similar vein, Wei et al. (2023) show that in-context learners can acquire new semantically non-sensical mappings from in-context examples if presented in a specific setup.\nFrom this listing, we see that ICL entails many design choices, that task-unrelated design choices change prediction outcomes and that the effects of design choices do not exist in isolation. The field is only beginning to understand the complex interplays of different prompting setups.", "publication_ref": ["b0", "b8", "b5", "b49"], "figure_ref": [], "table_ref": []}, {"heading": "Experiment I: Robustness to spurious correlations", "text": "We clarify open questions about robustness of incontext learners by elucidating their sensitivity to factors to which they should be invariant (from here on invariance factors). First, we focus on one of the most prominent forms of non-robustness in TT models: susceptibility to spurious correlations between inputs and labels (see Section 2.1).\nIn the first set of experiments, we test how differ-  ent models behave when spurious correlations are contained in their adaptation data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "We here describe the datasets and models used to test sensitivity to spurious correlations.\nTask Base dataset Adversarial dataset NLI MNLI (Williams et al., 2018) HANS (McCoy et al., 2019) ANLI (Nie et al., 2020) PI QQP (Wang et al., 2017) PAWS (Zhang et al., 2019) QA SQuAD (Rajpurkar et al., 2016) SQuAD adv. (Jia and Liang, 2017) adv. QA (Bartolo et al., 2020) SQuAD shifts (Miller et al., 2020)  Datasets We use different common NLU datasets (from here on base datasets) which are known to contain spurious correlations between input and label distributions (Gururangan et al., 2018;Geva et al., 2019;Poliak et al., 2018), as well as adversarial datasets of the same tasks. Adversarial datasets are designed to not contain the spurious correlations of the base datasets; then, they can be used to test whether models use short-cut solutions (for an overview see Table 2). Our base datasets span three different types of NLU tasks: natural language inference (NLI), paraphrase identification (PI) and extractive question answering (QA). An overview can be found in Table 2 and additional details about dataset properties and their construction in Appendix C.\nModels Our first experiment compares TT models with models that perform tasks through ICL.\nFor the latter, we consider two types of models: 'vanilla' LLMs, and LLMs that are tuned to follow instructions (IT see e.g. Wei et al., 2022;Zhong et al., 2021).\nFor TT, we use models based on RoBERTa BASE and RoBERTa LARGE (Liu et al., 2019). If available, we reutilise finetuned versions of RoBERTa that have been open-sourced through the huggingface hub (Wolf et al., 2019); if not available, we finetune the respective models ourselves (with training details in Appendix B).  Our vanilla LLMs consist of the series of LLaMA models (7B, 13B, 33B, 65B; Touvron et al., 2023). The IT counterparts are the freely available Alpaca models, which are based on the same LLaMA models but are additionally finetuned via low-rank adaptation (LoRA; Hu et al., 2022) on the Alpaca self-instruct dataset (Taori et al., 2023;. We run all models using mixed-precision decomposition as described by Dettmers et al. (2022). For an overview of all used models, see Table 3. probability distribution over possible labels y \u2208 C using argmax y\u2208C P (y|x 1 , y 1 ...x k , y k , x) where C is the set of possible labels. Every data point x is wrapped by an instruction template that explains the task the model should solve in natural language.\nThe label space C is determined by the type of instruction template and can differ across templates. We mitigate the influences of potential confounds like the template format, the order of (x i , y i ), imbalanced distribution of y i or the semantics of x i by a pseudo-random sampling x i for every new model inference. Our sampling of x i ensures that the in-context labels y i are balanced over all possible labels (similar to Wei et al., 2023;Brown et al., 2020, inter alia). Moreover, we use multiple instruction templates sourced from FLAN (Wei et al., 2022) to avoid systematic bias.", "publication_ref": ["b45", "b26", "b31", "b42", "b48", "b36", "b18", "b4", "b27", "b11", "b10", "b35", "b43", "b50", "b24", "b46", "b40", "b15", "b39", "b43"], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_4"]}, {"heading": "Results", "text": "We first evaluate the capacity of different models to robustly generalise from adaptation data to test data.\nIn the taxonomy of generalisation capabilities, this constitutes a covariate shift between the adaptation data (finetuning data in TT and in-context data in ICL) and the test data (compare GenBench; . The corresponding GenBench evaluation card can be found in Table 1.\nBase data in-context First, we adapt the TT and ICL models on the base data and then compare their performance between the base data and the respective adversarial counterparts. If an approach is robust to spurious correlations in the adaptation data (which are the fine-tuning data or in-context examples, respectively), it should perform approximately equally on the base dataset and the adversarial dataset. We relate both scores in the first row of Figure 1.\nResults from in-context learners land generally closer to the diagonal, hence indicating -despite overall weaker performance -that they are more robust to the spurious correlations in their adaptation data. To quantify this visual result, we fit a linear regression model on the data presented in the scatterplot in Figure 1a (hence, predict the adversarialfrom the base accuracies) with the intercept fixed at \u03b2 0 = 0. The coefficient \u03b2 1 can then be interpreted as a degree of robustness to the different adaptation data, with \u03b2 1 = 1 indicating complete robustness and \u03b2 1 = 0 complete reliance on non-generalisable patterns in the base data. The \u03b2 1 values for different adaptation types can be found in the top row of Figure 1b. The \u03b2 1 values across all tasks are significantly closer to the parity value of 1 for ICL models than for TT models, with IT models having the edge over vanilla models.\nOur results demonstrate that ICL models are much less sensitive to spurious correlations in their adaptation data than TT models. However, the fact that ICL models do not reach the parity value of 1 means that gains on adversarial data are smaller compared to gains on the base data. This suggests that ICL may still be mildly sensitive to spurious correlations, or, alternatively, that the adversarial datasets used are simply inherently more difficult, resulting in lower performances compared to the base data 1 . We will further explore this question in the next experiment.  (Kaushik et al., 2020). This should not make a difference for models that are robust to spurious correlations, but we expect a performance drop between these two conditions for models that learned solutions that exploited those correlations. As we are now evaluating the adversarial data points in both scenarios, we eliminate the potential impact of the dataset difficulty on the scores. In the second row of Figure 1, we plot performances with base adaptation examples in the context against the performance with adversarial adaptation data, noting that ICL models are mostly unaffected by adaptation data type while TT models land far underneath the diagonal again. A regression analysis shows almost all \u03b2-values of ICL models moving closer to parity, showing us how the dataset difficulty impacted the results. However, even without the effect of dataset difficulty on the \u03b2-values, they are still not quite equal to 1, suggesting that the type of adaptation data has a small influence on ICL learners.", "publication_ref": ["b19"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": ["tab_1"]}, {"heading": "Adversarial data in-context", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiment II: Consistency evaluation in ICL", "text": "In the previous section, we saw that the robustness of in-context learners is likely influenced more by other factors than by spurious correlations in the in-context data. Although previous studies have reported the susceptibilities of LLMs to various factors, the impact of different design decisions and their interactions in the context of ICL robust- 1 An illustrative example of the base data being easier: adversarial QA contains only a single answer alternative while squad contains three. ness has not been systematically evaluated. Here, we test the effects of an extensive range of these factors on prediction outcomes in consistency and accuracy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental details", "text": "For all of the following experiments, we use promptsource templates (P3; Bach et al., 2022) and the ANLI dataset (Nie et al., 2020). We continue to use the models and the evaluation procedure from Section 3 (excluding the TT models). The following briefly describes the factors we consider in our analysis.", "publication_ref": ["b31"], "figure_ref": [], "table_ref": []}, {"heading": "Factors", "text": "We distinguish two types of factors. Firstly, we consider factors that constitute interventions to improve consistency and performance, which we call variance factors 2 or \u03bb var for short. We expect a model to change their response when we change the value of those factors: Secondly, we consider factors from which we want a model to not change their response (or 'be robust to') when we change their value. We will call these invariance factors or \u03bb inv :\nCross-templates Whether in-context instructions are drawn randomly from all available instruction templates or use the same instructions as for the target.  Combining the above factors results in 1536 setups. We evaluate each of these constellations using the same subset of 600 data points 3 that we draw uniformly from either of the ANLI validation sets. In-context examples are drawn at random from the respective training sets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Analysis methods", "text": "Our analysis entails two steps:\n1. Main effects: how much does a single factor impact consistency and the accuracy across many setups?\n2. Interactions: when we disentangle the main effects, do we find systematic interactions across pairs or triplets of factors?\nMain effects To evaluate the main effect of each factor \u03bb, we employ linear regression to predict the accuracy of a model based on \u03bb, considering all possible combinations of the remaining factors. The regression model is formulated as Acc = \u03b2 1 \u03bb+ \u03b2 0 . The coefficient \u03b2 1 represents the main effect of a specific \u03bb, approximating the average change in accuracy across all possible setups given \u03bb. We also fit the intercept \u03b2 0 , but won't interpret it.\nInteractions We analyse interactions by fitting a factorial ANOVA considering the effect of all possible 2-and 3-way interactions 4 of factors on the accuracy of predictions. We then count the number of significant interactions every factor maintains with other factors. A larger number of interactions suggests that a factor is volatile, i.e. it changes the predictions depending on the overall setup. Further, as the factors have been chosen to be orthogonal and should not influence each other. On the other hand, if factors are not interacting, we can interpret their main effects directly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consistency metrics", "text": "We measure the consistency of model predictions using Cohen's \u03ba (Cohen, 1960), a measure of interrater agreement adjusted for agreement by chance. The metric \u03ba equals 1 if two (or more) sets of predictions perfectly align while agreement by chance results in \u03ba equalling 0. In our case, we calculate \u03ba to compare the predictions of a model before and after we change the value of a factor \u03bb (e.g. if all labels in-context are the same or if they are not; see One label) across all possible setups. We make the metric less dependent on the accuracy of a model by calculating \u03ba only on the subset of predictions that have been correctly predicted in either of the two cases. Figure 3: The \u03b2-values of the main effects of each individual factor across many different runs. The values can be directly interpreted as 'expected accuracy gain/loss' when a factor is present compared to when it is absent.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Probing instructions", "text": "To find a set of high-and low-performing instructions for the instruction quality factor, we run a preliminary analysis where we probe model behaviour in response to all 15 available P3 ANLI instructions. We assess the performance of different instructions based on accuracy and consistency.\nWe first get a general picture of each model's average consistency \u03ba avg across all templates. We find that \u03ba avg increases with the number of parameters and is overall higher when a model has been instruction tuned (Figure 2a).\nWe then consider the consistency of each individual instruction and find a congruent pattern of consistency across all models (Figure 2b) that corresponds generally to the accuracy scores of the same instructions (compare Figure 2c). Interestingly, we also find two groups of high-accuracy instructions making very different predictions (see the consistency scores of 9, 10 and 15 vs. rest). Based on these observations, we choose the two highest-and lowest-performing instructions to constitute the instruction quality factor and templates 14 and 15 as realisations of the instructions factor.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Results", "text": "We evaluate the models on all possible combinations of \u03bb var and \u03bb inv . Appendix G shows the distribution of accuracy scores across all runs for different models. The wide spread of scores is striking: large models score from below chance to up to 67% accuracy, depending on the overall setup. This extreme variability underlines the importance of better understanding the impact of different design decisions and prediction consistency in ICL. The subsequent section comprehensively summarises the results of our statistical analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Main effects", "text": "The main effects separated by model size are shown in Figure 3, illustrating each factor's impact in isolation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Variance factors", "text": "The variance factors we chose are generally thought to improve accuracy and, hence, should have positive main effects. We find two out of five variance factors significantly improve performance on average, from which instruction quality stands out as the most influential factor across all model sizes. Similarly, we find that instruction tuning is consistently beneficial while balancing the in-context labels and the number of in-context examples (n-shots) have on average positive but small and nonsignificant effects. Surprisingly, calibration harms rather than helps performance for all but our smallest model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Invariance factors", "text": "Different from variance factors, invariance factors are chosen such that they should not influence a robust model's predictions. Accordingly, the main effects should be optimally close to 0. We find that models are generally robust to having varied instructions in-context (cross-instruction), or even having a slightly positive effect. This is intriguing, as this factor entails considerable changes to the in-context setup, and we previously saw how the type of target instructions (in instruction quality) plays a major role. Further, we identify vulnerabilities of large models to the factors cross-task and one label. The ambivalent effect of the instructions factor suggests high volatility across similarly performing instructions (i.e. different instructions perform differently for different models and setups).\nThese main effects give us a general idea of the tendencies of factors. To better understand all main effects, we will investigate interactions in Section 4.2.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consistency of invariance factors", "text": "Additionally to a factor's impact on accuracy, we also compute the prediction consistency \u03ba of the factors (as defined in Section 4.1.3). To do so, we calculate the agreement of predictions when a factor is present with when it is absent. This way, the value of \u03ba shows us the degree of robustness of a model to an invariance factor by quantifying the degree of prediction change caused by that factor. Figure 4 shows how robustness increases with size and instruction tuning. The very low \u03ba scores for the detrimental cross-task factor come as no surprise, while low scores in the instructions factor corroborate the previous suspicion that instructions are highly volatile: if we change the type of used instructions, the predictions across a lot of setups change.  ", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Interactions", "text": "The main effects give us a good idea of the general direction of the impact of a single factor. However, the main effects do not tell the whole story: consider the case in which factor A improves performance if it is paired with factor B, but performance deteriorates when paired with C. A's overall main effect might be close to zero even though it influences certain settings. To better understand the impact of each factor, we will have to investigate its interactions. We determine interactions following the procedure described in Section 4.1.2. Figure 5 shows the number of interactions that each factor maintains. A general observation is that large models tend to have simpler 2-way interactions, while smaller models tend to have more complex 3-way interactions.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Highly interactive factors", "text": "The most important factor of instruction quality maintains many interactions. Hence, many other factors change predictions depending on the used instruction template.\nWe find a similar effect for the instructions factor 5 . This demonstrates the intricacy of the formulation of instructions: the instruction quality has the largest positive impact on prediction outcomes, but at the same time, the instructions are highly interactive and volatile, with their the effects of many other factors depending on it. Otherwise, we observe that calibration is the most volatile, with eight significant interactions with other factors. The previously observed main effect has to be seen in this perspective: calibration is not generally detrimental, but its effects depend very much on the setup in which it is used. For example, we find on closer inspection that calibration leads to the highest overall accuracies for the 7B parameter models when presented with specific instructions and paraphrase identification incontext examples (cross-task).", "publication_ref": ["b53"], "figure_ref": [], "table_ref": []}, {"heading": "Low interactive factors", "text": "On the other end of the spectrum, we find that factors like the number of in-context examples (n-shots), the balancing of in-context labels or using just one label have little to no interactions at all. Conveniently, there are no ambiguities for these factors and we can therefore interpret their main effects directly, as they are most likely to be stable across setups. For example, suppose it is possible to increase the number of examples in the context. In that case, we can reliably expect small gains in accuracy without the danger of otherwise interfering with the learning process. Similarly, balancing labels leads to reliable small improvements and having just a single label in the context reliably reduces accuracy for large models.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "We will first summarise the findings of this paper and then discuss their implications.\nFindings We saw in Section 3 how spurious correlations do not influence predictions in ICL in a relevant manner as they did previously in TT. This, however, does not resolve the problem of robustness: depending on the setup, ICL accuracy in our experiments differs up to 40%, as other factors in the setup become pivotally important. We here conducted a comprehensive analysis of the influence of different setups on the consistency of predictions in ICL models. Considering different setups, wellchosen instructions promise the largest performance gains across many setups. At the same time, they are among the most volatile factors of all and highly sensitive to the setting in which they are used. On the other hand, factors that relate to the exact organisation of the in-context examples, such as the label distribution or in-context instructions (cross-instructions), have surprisingly small impacts. Other factors like n-shots -among others -are not interactive, which makes them much easier to handle: their expected gain or loss should, in most cases, correspond to our observed main effects. Across all of our experiments, we also find the general tendency that larger numbers of model parameters and instruction tuning are beneficial for model consistency across many settings.\nImplications and future research What do these findings imply? As we have seen, inconsistency is a severe concern in ICL, and we here contribute to narrowing down its sources. Unlike previously in TT, concentrating on spurious correlations is not vital for ICL robustness and investigating design choices concerned with in-context examples (i.e. the exact few-shot setting) promises to be less impactful or mostly dependent on other setup factors. Instead, our findings suggest that the exact phrasing of instruction templates is pivotally important. To get hold of inconsistent predictions in ICL, finding the exact properties of instructions that so strongly influence model predictions is a sensible next step (potentially with a similar methodology as it is presented here). Insights into the impact of instruction properties can help us to find the source of inconsistencies and avoid them in production, while they can also contribute to the theoretical understanding of in-context learning which is currently still under investigation. While our analysis focused on the few-shot setting, it also significantly impacts the increasingly popular zero-shot learning, as instructions are central in that setting. For model deployment, our findings demand caution as minor changes to certain parts of prompts (e.g. the instructions) can change the performance of the general setup. This is especially true for employing smaller, untuned models. A consistent finding across all our experiments is that instruction tuning improves consistency and robustness to irrelevant factors across all setups. Therefore, we advocate for the use of tuned models to improve robustness. Finally, recent research has suggested that dynamics in ICL are, to a certain degree, chaotic . It might be advised to use more diverse evaluation setups and a rigorous statistical analysis of the results to guarantee the generality of results and avoid Type-I errors in publications (Ioannidis, 2005).", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We here analysed robustness and variability in the recent learning paradigm of ICL, showing that they are generally different from in task-tuning. By using a methodology that covers a wide range of potential prompt design decisions, we show which factors actually matter in prompt design and how these factors influence each other.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations and Acknowledgements", "text": "For a discussion of the limitations of our work and the acknowledgements, we refer to Appendix I and Appendix J, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Experiment 1: List of TT models", "text": "We compare the sensitivity to spurious correlations of ICL models with TT models. The following table contains all TT models we used during these experiments, providing the respective handle for the huggingface hub or indicating with 'own' that we fine-tuned the respective model ourselves. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Experiment 1: Finetuning details of own models", "text": "We finetuned all RoBERTa models using the same set of hyperparameters, based on the literature and experience.\nHyperparameters We train using the ADAM Optimizer with \u03b3 = 1e-05, inverse square root decay and \u03b2 1/2 = (0.9, 0.999), no weight decay, 250 warmup steps and a batch size of 8. We stop training if the model does not show improvement on the validation set for 1 epoch of training.\nData For adversarially tuned models, we mixed the training set of the base data with 70% of the adversarial data (30% retained for evaluation). We ensured a mixing ratio of 20%/80% adversarial/base data.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Experiment 1: Datasets details", "text": "We here provide additional information about the datasets we use in Experiment 1: The aim is to identify semantically equivalent questions, addressing challenges such as paraphrasing and varying levels of detail.\nSQuAD (Stanford Question Answering Dataset; Rajpurkar et al. 2016) A reading comprehension dataset consisting of questions about passages from Wikipedia. The questions are human-annotated, and the answer to each question is a segment (or span) of the passage.\nThe goal of models is to identify and extract the correct span from the passage that answers the question. SQuAD Adversarial (Jia and Liang, 2017) A derivative of the Stanford Question Answering Dataset (SQuAD) where adversarial sentences are introduced into the context paragraphs, aiming to mislead models into selecting incorrect answers while the correct answers remain unchanged.\nAdversarial QA (Bartolo et al., 2020) A reading comprehension dataset, where each question is tied to a Wikipedia passage. Distinctively, answer annotations are freeform human responses rather than extracts from the passage, testing the extractive capability boundaries of SQuAD-inspired models.\nSQuAD Shifts (Miller et al., 2020) Formed by perturbing the original SQuAD distribution in terms of linguistic and stylistic attributes. This dataset gauges model robustness against unseen data distributions, such as domain shifts or synthetic noise.", "publication_ref": ["b36", "b18", "b4", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "D Experiment 1: Impact of spurious correlations in ICL", "text": "We conducted an additional analysis of the results in Section 3.2. The goal of this additional analysis is to understand the impact of the type of adaptation data (adversarial vs. base) on the prediction outcomes in comparison with other factors that we varied in our experiments (such as the type of instruction template, whether the model was instruction tuned or the size of the model). Type data is a binary factor indicating whether the model was adapted on base or adversarial data; Size is a quarternary factor indicating model size; Type instructions is a binary factor indicating the type of template that was used; Instruction tuned is a binary factor indicating whether the tested model was instruction tuned or not.\nTable 4 shows the summary statistics of an ANOVA that we apply to these factors and their impact on the model accuracy. We can see from Table 4 that adaptation data is the only factor that does not significantly impact prediction outcomes.  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_12", "tab_12"]}, {"heading": "E.2 P3 details", "text": "In the following, we provide more details on the instruction templates (Bach et al., 2022), as used in Experiments II.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Experiment 2: Factors details", "text": "In the following, we provide a more detailed description of the factors used in Section 4 and also provide our motivation to include these factors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1 Invariance factors", "text": "Size We consider models of different sizes. Model size has been shown to be an important moderating factor in probably all previous studies on in-context learning.\nInstruction tuning We have seen previously that instruction tuning improves the consistency of a model across templates (see Section 4.1.4). We introduce it as a factor to show which other invariance factors it may affect.\nCalibration Previous research has shown how small models are especially biased towards single labels when prompted. We find similar tendencies for our model: We exploratively calculate the entropy of a model's predictions across all data points in a dataset. This allows us to estimate whether a model is biased toward predicting a single label (low entropy). Optimally, a model's prediction should be close to the entropy of the target distribution H(Y ). We find that smaller models have a larger bias towards predicting a single label (lower prediction entropy), while larger and IT models get closer to H(Y ) (see Figure 6). Instruction quality Ultimately, we have seen how some instructions produce consistent and relatively well-performing responses across different models while others do not (see Section 4.1.4. We add this last factor to see which other types of factors help the in-context learner cope with varying instruction quality. We chose the two best and two worst-performing templates 6 from our previous analysis.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.2 Invariance factors", "text": "The following briefly describes each of the tested \u03bb inv .\nBalanced labels Zhao et al. (2021) additionally showed how a majority label among the in-context example can influence the distribution of model outputs. Therefore, we compare contexts with balanced incontext label distribution with randomly sampled labels and an extreme case with only a single in-context label. Cross-task In cross-task, we exchange the task of the in-context examples such that the only consistency between in-context and target examples is the general format (x followed by y) and the truthfulness of the x to y mapping. To see whether conditioning on a fixed label space matters, we add tasks with a discriminative (QQP) and a generative (SQuAD) objective as different factors. Compared to a zero-shot baseline, we can see that large models can benefit from conditioning on other tasks (Figure 8). For our principal analysis, we only include QQP as an in-context task, as SQuAD is incompatible with many other factors (such as balanced labels, one label aso...)\nInstructions Besides the quality of the instructions, we are also interested in how consistent model behaviour is across instructions that are of similar quality. To get an insight into this, we bin the high-quality instructions respectively into a new factor.", "publication_ref": ["b49"], "figure_ref": ["fig_7"], "table_ref": []}, {"heading": "G Experiment 2: Accuracy distribution", "text": "We here show the distribution of accuracy scores for all setups in experiment 2, separated by model size (hue) and whether the model is instruction tuned or not (i.e. vanilla). H Experiment 2: Interactions details H.1 ANOVA using instructions factor\nWe fit an ANOVA using the factor instructions instead of instruction quality. In that case, we find a similar pattern of interactions, showing that the size of the main effect can not merely explain the number of interactions. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H.2 Interaction mappings and effect sizes", "text": "The following shows the exact mapping of the interacting factors as well as the size of the corresponding effect size, measured by \u03b2 \u03bb 1 \u00d7\u03bb 2 values from a post hoc regression analysis.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I Limitations", "text": "For the first set of experiments in Section 3, the comparison between TT models and ICL is not 'fair'. Model sizes are not comparable, the amount of adaptation data differs significantly (thousand for tasktuning compared to 5 for ICL) and some of the adversarial datasets were created with some of the TT models 'in-the-loop' (e.g. ANLI). However, our motivation here is not to be fair, but to show practically relevant effects in either type of task adaptation. For a fair comparison, see Mosbach et al. (2023).\nFor the second set of experiments in Section 4, we only consider a subset of factors that we deemed the most relevant or interesting. Adding more factors would enrich the analysis. However, the number of model inferences to compute grows exponentially with the number of considered factors, which sets soft limits for the number of analysed factors. For potential follow-ups, we suggest a more fine-grained investigation of different instruction designs for the target example, as this potentially yields exciting insights on what exactly leads to the large performance gains and high volatility. Our study is coarse in this aspect.\nOur analysis would have been more expressive if we chose an 'easier' task than the relatively 'hard' ANLI dataset to run our evaluation: our smaller models perform relatively poorly across many factors on challenging datasets like ANLI and provide less variance for a meaningful analysis.", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "J Acknowledgements", "text": "We would like to thank the anonymous reviewers for their time and productive feedback. Beyond that, we thank the whole COLT group at UPF for their helpful feedback and other support during the projectespecially Emily Cheng and Xixian Liao. Further, LW thanks the Department of Translation and Language Sciences at the University Pompeu Fabra for funding.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "What learning algorithm is in-context learning?", "journal": "", "year": "2022", "authors": "Ekin Aky\u00fcrek; Dale Schuurmans; Jacob Andreas; Tengyu Ma; Denny Zhou"}, {"ref_id": "b1", "title": "Exploring the landscape of distributional robustness for question answering models", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Anas Awadalla; Mitchell Wortsman; Gabriel Ilharco; Sewon Min; Ian Magnusson; Hannaneh Hajishirzi; Ludwig Schmidt"}, {"ref_id": "b2", "title": "Tian-jian Jiang, and Alexander Rush. 2022. Prompt-Source: An integrated development environment and repository for natural language prompts", "journal": "Association for Computational Linguistics", "year": "", "authors": "Stephen Bach; Victor Sanh; Zheng Xin Yong; Albert Webson; Colin Raffel; V Nihal; Abheesht Nayak; Taewoon Sharma;  Kim; Thibault Bari; Zaid Fevry; Manan Alyafeai; Andrea Dey; Zhiqing Santilli; Srulik Sun; Canwen Ben-David; Gunjan Xu; Han Chhablani; Jason Wang; Maged Fries; Shanya Alshaibani;  Sharma"}, {"ref_id": "b3", "title": "A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity", "journal": "", "year": "2023", "authors": "Yejin Bang; Samuel Cahyawijaya; Nayeon Lee; Wenliang Dai; Dan Su; Bryan Wilie; Holy Lovenia; Ziwei Ji; Tiezheng Yu; Willy Chung"}, {"ref_id": "b4", "title": "Beat the AI: Investigating adversarial human annotation for reading comprehension", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Max Bartolo; Alastair Roberts; Johannes Welbl"}, {"ref_id": "b5", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "", "year": "2020-12-06", "authors": "Tom B Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel M Ziegler; Jeffrey Wu; Clemens Winter; Christopher Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b6", "title": "A coefficient of agreement for nominal scales. Educational and psychological measurement", "journal": "", "year": "1960", "authors": "Jacob Cohen"}, {"ref_id": "b7", "title": "8-bit matrix multiplication for transformers at scale. ArXiv preprint", "journal": "2022. Llm", "year": "", "authors": "Tim Dettmers; Mike Lewis; Younes Belkada; Luke Zettlemoyer"}, {"ref_id": "b8", "title": "What can transformers learn in-context? a case study of simple function classes", "journal": "", "year": "2022", "authors": "Shivam Garg; Dimitris Tsipras; S Percy; Gregory Liang;  Valiant"}, {"ref_id": "b9", "title": "Shortcut learning in deep neural networks", "journal": "Nature Machine Intelligence", "year": "2020", "authors": "Robert Geirhos; J\u00f6rn-Henrik Jacobsen; Claudio Michaelis; Richard Zemel; Wieland Brendel; Matthias Bethge; Felix A Wichmann"}, {"ref_id": "b10", "title": "Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Mor Geva; Yoav Goldberg; Jonathan Berant"}, {"ref_id": "b11", "title": "Annotation artifacts in natural language inference data", "journal": "", "year": "2018", "authors": "Swabha Suchin Gururangan; Omer Swayamdipta; Roy Levy; Samuel Schwartz; Noah A Bowman;  Smith"}, {"ref_id": "b12", "title": "Using pre-training can improve model robustness and uncertainty", "journal": "", "year": "2019", "authors": "Dan Hendrycks; Kimin Lee; Mantas Mazeika"}, {"ref_id": "b13", "title": "", "journal": "Proceedings of Machine Learning Research", "year": "2019-06", "authors": ""}, {"ref_id": "b14", "title": "Pretrained transformers improve out-of-distribution robustness", "journal": "", "year": "2020", "authors": "Dan Hendrycks; Xiaoyuan Liu; Eric Wallace; Adam Dziedzic; Rishabh Krishnan; Dawn Song"}, {"ref_id": "b15", "title": "Lora: Low-rank adaptation of large language models", "journal": "", "year": "2022-04-25", "authors": "Edward J Hu; Yelong Shen; Phillip Wallis; Zeyuan Allen-Zhu; Yuanzhi Li; Shean Wang; Lu Wang; Weizhu Chen"}, {"ref_id": "b16", "title": "A taxonomy and review of generalization research in nlp", "journal": "Nature Machine Intelligence", "year": "2023", "authors": "D Hupkes; M Giulianelli; V Dankers"}, {"ref_id": "b17", "title": "Why most published research findings are false", "journal": "PLoS medicine", "year": "2005", "authors": "P A John;  Ioannidis"}, {"ref_id": "b18", "title": "Adversarial examples for evaluating reading comprehension systems", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Robin Jia; Percy Liang"}, {"ref_id": "b19", "title": "Learning the difference that makes A difference with counterfactually-augmented data", "journal": "", "year": "2020-04-26", "authors": "Divyansh Kaushik; Eduard H Hovy; Zachary Chase Lipton"}, {"ref_id": "b20", "title": "When choosing plausible alternatives, clever hans can be clever", "journal": "", "year": "2019", "authors": "Pride Kavumba; Naoya Inoue; Benjamin Heinzerling; Keshav Singh; Paul Reisert; Kentaro Inui"}, {"ref_id": "b21", "title": "Are prompt-based models clueless?", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Pride Kavumba; Ryo Takahashi; Yusuke Oda"}, {"ref_id": "b22", "title": "Prompt waywardness: The curious case of discretized interpretation of continuous prompts", "journal": "", "year": "2022", "authors": "Daniel Khashabi; Xinxi Lyu; Sewon Min; Lianhui Qin; Kyle Richardson; Sean Welleck; Hannaneh Hajishirzi; Tushar Khot; Ashish Sabharwal; Sameer Singh"}, {"ref_id": "b23", "title": "Holistic evaluation of language models", "journal": "", "year": "2022", "authors": "Percy Liang; Rishi Bommasani; Tony Lee; Dimitris Tsipras; Dilara Soylu; Michihiro Yasunaga; Yian Zhang; Deepak Narayanan; Yuhuai Wu"}, {"ref_id": "b24", "title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"ref_id": "b25", "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity", "journal": "Long Papers", "year": "2022", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b26", "title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"ref_id": "b27", "title": "The effect of natural distribution shift on question answering models", "journal": "PMLR", "year": "2020-07", "authors": "John Miller; Karl Krauth; Benjamin Recht; Ludwig Schmidt"}, {"ref_id": "b28", "title": "Rethinking the role of demonstrations: What makes in-context learning work?", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Sewon Min; Xinxi Lyu; Ari Holtzman; Mikel Artetxe; Mike Lewis; Hannaneh Hajishirzi; Luke Zettlemoyer"}, {"ref_id": "b29", "title": "Reframing instructional prompts to GPTk's language", "journal": "", "year": "2022", "authors": "Swaroop Mishra; Daniel Khashabi; Chitta Baral; Yejin Choi; Hannaneh Hajishirzi"}, {"ref_id": "b30", "title": "Few-shot fine-tuning vs. in-context learning: A fair comparison and evaluation", "journal": "", "year": "2023", "authors": "Marius Mosbach; Tiago Pimentel; Shauli Ravfogel; Dietrich Klakow; Yanai Elazar"}, {"ref_id": "b31", "title": "Adversarial NLI: A new benchmark for natural language understanding", "journal": "", "year": "2020", "authors": "Yixin Nie; Adina Williams; Emily Dinan; Mohit Bansal; Jason Weston; Douwe Kiela"}, {"ref_id": "b32", "title": "Probing neural network comprehension of natural language arguments", "journal": "", "year": "2019", "authors": "Timothy Niven; Hung-Yu Kao"}, {"ref_id": "b33", "title": "Evaluating task understanding through multilingual consistency: A chatgpt case study", "journal": "", "year": "2023", "authors": "Xenia Ohmer; Elia Bruni; Dieuwke Hupkes"}, {"ref_id": "b34", "title": "Training language models to follow instructions with human feedback", "journal": "Advances in Neural Information Processing Systems", "year": "2022", "authors": "Long Ouyang; Jeffrey Wu; Xu Jiang; Diogo Almeida; Carroll Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama; Alex Ray"}, {"ref_id": "b35", "title": "Hypothesis only baselines in natural language inference", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme"}, {"ref_id": "b36", "title": "SQuAD: 100,000+ questions for machine comprehension of text", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"ref_id": "b37", "title": "On causal and anticausal learning", "journal": "", "year": "2012-06-26", "authors": "Bernhard Sch\u00f6lkopf; Dominik Janzing; Jonas Peters; Eleni Sgouritsa; Kun Zhang; Joris M Mooij"}, {"ref_id": "b38", "title": "", "journal": "", "year": "2023", "authors": "Chenglei Si; Zhe Gan; Zhengyuan Yang; Shuohang Wang; Jianfeng Wang; Jordan Boyd-Graber; Lijuan Wang"}, {"ref_id": "b39", "title": "Stanford alpaca: An instruction-following llama model", "journal": "", "year": "2023", "authors": "Rohan Taori; Ishaan Gulrajani; Tianyi Zhang; Yann Dubois; Xuechen Li; Carlos Guestrin; Percy Liang; Tatsunori B Hashimoto"}, {"ref_id": "b40", "title": "Llama: Open and efficient foundation language models", "journal": "", "year": "2023", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Naman Baptiste Rozi\u00e8re; Eric Goyal; Faisal Hambro;  Azhar"}, {"ref_id": "b41", "title": "Self-instruct: Aligning language model with self generated instructions", "journal": "", "year": "2022", "authors": "Yizhong Wang; Yeganeh Kordi; Swaroop Mishra; Alisa Liu; A Noah; Daniel Smith; Hannaneh Khashabi;  Hajishirzi"}, {"ref_id": "b42", "title": "Bilateral multi-perspective matching for natural language sentences", "journal": "", "year": "2017-08-19", "authors": "Zhiguo Wang; Wael Hamza; Radu Florian"}, {"ref_id": "b43", "title": "Finetuned language models are zero-shot learners", "journal": "Virtual Event", "year": "2022-04-25", "authors": "Jason Wei; Maarten Bosma; Y Vincent; Kelvin Zhao; Adams Wei Guu; Brian Yu; Nan Lester; Andrew M Du;  Dai; V Quoc;  Le"}, {"ref_id": "b44", "title": "", "journal": "", "year": "", "authors": "Jerry Wei; Jason Wei; Yi Tay; Dustin Tran; Albert Webson; Yifeng Lu; Xinyun Chen; Hanxiao Liu; Da Huang; Denny Zhou"}, {"ref_id": "b45", "title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"ref_id": "b46", "title": "Huggingface's transformers: State-ofthe-art natural language processing", "journal": "", "year": "2019", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz"}, {"ref_id": "b47", "title": "Ground-truth labels matter: A deeper look into input-label demonstrations", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Junyeob Kang Min Yoo; Hyuhng Joon Kim; Hyunsoo Kim; Hwiyeol Cho; Sang-Woo Jo; Sang-Goo Lee; Taeuk Lee;  Kim"}, {"ref_id": "b48", "title": "PAWS: Paraphrase adversaries from word scrambling", "journal": "Long and Short Papers", "year": "2019", "authors": "Yuan Zhang; Jason Baldridge; Luheng He"}, {"ref_id": "b49", "title": "Calibrate before use: Improving few-shot performance of language models", "journal": "PMLR", "year": "2021-07-24", "authors": "Zihao Zhao; Eric Wallace; Shi Feng; Dan Klein; Sameer Singh"}, {"ref_id": "b50", "title": "Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections", "journal": "", "year": "2021", "authors": "Ruiqi Zhong; Kristy Lee; Zheng Zhang; Dan Klein"}, {"ref_id": "b51", "title": "", "journal": "", "year": "", "authors": "Chunting Zhou; Pengfei Liu; Puxin Xu; Srini Iyer; Jiao Sun; Yuning Mao; Xuezhe Ma; Avia Efrat; Ping Yu; Lili Yu; Susan Zhang; Gargi Ghosh; Mike Lewis; Luke Zettlemoyer"}, {"ref_id": "b52", "title": "1 P3 details -Names Names of all available P3-instructions, following the ordering of Figure 2: 1. 'MNLI Crowdsource' 2. 'Guaranteed Possible Impossible' 3. 'Always Sometimes Never' 4", "journal": "", "year": "", "authors": "E "}, {"ref_id": "b53", "title": "Must Be True' 10. 'Based on the Previous Passage' 11. 'Should Assume' 12. 'Can We Infer' 13", "journal": "", "year": "", "authors": ""}, {"ref_id": "b54", "title": ".2 P3 details -Examples We here show examples of P3 prompt templates as they are used in Experiment 2: The prompt templates wrap the respective ANLI data point and provide natural language instructions about the task to the model. High-performing templates 'Claim true false inconclusive", "journal": "", "year": "", "authors": "E "}, {"ref_id": "b55", "title": "better known by his stage name Lil Jon, is an American rapper, record producer, and DJ. He was the frontman of the group Lil Jon & The East Side Boyz, which he formed in 1997, and they released several albums until 2004. Does it follow that Jonathan Smith spent much of his time in China", "journal": "", "year": "1971", "authors": "Jonathan Smith"}, {"ref_id": "b56", "title": "He was the frontman of the group Lil Jon & The East Side Boyz, which he formed in 1997, and they released several albums until", "journal": "", "year": "1971", "authors": "Jonathan Smith"}, {"ref_id": "b57", "title": "better known by his stage name Lil Jon, is an American rapper, record producer, and DJ. He was the frontman of the group Lil Jon & The East Side Boyz, which he formed in 1997, and they released several albums until", "journal": "", "year": "1971", "authors": ""}, {"ref_id": "b58", "title": "Jonathan Smith spent much of his time in China", "journal": "", "year": "", "authors": " Therefore"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "EvaluationFigure 1 :1Figure 1: Figure (a) shows the f1-scores of different models -normalised for random accuracy -on different datasets when adapted via base or adversarial data. On each y-axis, we plot accuracy under distributional shift (base + adv) while on each x-axis there is no shift (base + base or adv + adv). Each column shows a different type of task. Marker size represents model size and colour represents the type of task adaptation. Dots close to the diagonal indicate invariance to the adaptation data and therefore robust generalisation, while dots in the bottom right indicate sensitivity to spurious correlations. Figure (b) shows the \u03b2-parameter of the linear regression (fixed intercept) on the data of Figure (a). We fit a linear regression for each task and adaptation type separately. Values close to 0 indicate very strong sensitivity to adaptation data, while values close to 1 indicate no sensitivity.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Figure (a) shows the consistency of a model when used with all 15 different P3 instructions, in an otherwise fixed setup. A value of 1 indicates perfect agreement (all templates produce the same prediction); Figure (b) shows how consistent individual instructions are with all other instructions. A value of 0 indicates a complete change of predictions while a value of 1 indicates perfect agreement; Figure (c) shows the respective accuracies of the instructions in Figure (b).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: The consistency values when a specific factor is present or not across all other setups. A value of 0 indicates a complete change of predictions while a value of 1 indicates perfect agreement (i.e. a low value indicates that a model is not robust to a change in a specific factor).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: The number of interactions per factor with other factors. A large number of interactions means that the outcome of a change in these factors depends on a lot of other variables.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "C. 11Base datasets MNLI (Multi Natural Language inference; Williams et al. 2018) A large-scale natural language inference dataset. It contains sentence pairs annotated with three categories: entailment, contradiction, and neutral. The dataset is sourced from a variety of genres, like fiction, government documents, and telephone conversations, thus encouraging models to learn domain-agnostic representations. QQP (Quora Question Pairs; Wang et al. 2017) A collection of question pairs from the Quora platform, labelled as either duplicates or non-duplicates.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure 7", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 9", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: Interactions when excluding Instruction quality and keeping Instructions instead. We find similar patterns.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Our analyses, categorised according to the GenBench taxonomy. The token \u25b3 represents Experiment I and \u25a1 represents Experiment II.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Tasks and corresponding datasets.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "As a follow-up experiment, we consider what happens when the adaptation data contains adversarial examples. As those examples do not contain the same spurious correlations, models cannot overfit them", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Size We consider models with 7B, 13B, 30B and 65B learnable parameters. Balanced labels Whether examples with labels are balanced across all possible classes in the context or use randomly sampled examples.", "figure_data": "Instruction tuning Whethermodelsareinstruction-tuned or not ('vanilla' mod-els).Calibration Whether model outputs are calibratedusing 'content-free prompts' following Zhaoet al. (2021).n-shots Whether there are many (k = 5) or few (k= 2) in-context examples in the prompt.Instruction quality Whether instructions belongto one of two groups of semantically equiva-lent but differently performing instruction tem-plates (high-vs. low-performing; more detailsin Section 4.1.4)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Cross-task Whether another classification task (QQP) is used as in-context examples or the same task as the target task (ANLI) is used. Instructions Different semantically equivalent target instructions that perform similarly (more details in Section 4.1.4). One label Whether in-context examples have only a single randomly selected label or diverse labels.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "C.2 Adversarial datasets HANS (Heuristic Analysis for NLI Systems;McCoy et al. 2019) Constructed to evaluate models on non-entailment cases that appear entailed due to spurious biases. Built upon common NLI datasets like SNLI and MultiNLI, it dissects three heuristic strategies that a model might utilise: lexical overlap, subsequence, and syntactic structure.", "figure_data": "ANLI (Adversarial Natural Language Inference; Nie et al. 2020)Generated by first training models on existing datasets (e.g., SNLI and MultiNLI) and then havinghuman annotators produce examples that the models predict incorrectly. Generation of additionalexamples was done in multiple rounds with respectively improved models, accordingly each roundincreases the adversarial difficulty.PAWS (Paraphrase Adversaries from Word Scrambling; Zhang et al. 2019)Comprises sentence pairs with high lexical overlap but differing semantics, challenging models thatheavily weigh word overlap. An adversarial expansion to datasets like the Quora Question Pairsdataset (QQP)."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "", "figure_data": ": Results of ANOVA"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Figure 11: The exact mappings of all 2-way interactions in our experiments.", "figure_data": "Calibration Cross task Instruction tuned Cross instruction One label Balanced labels n_shots Instruction quality factor2 CalibrationInstruction qualityCross task size = 7b Instruction tuned Cross instruction One label factor1n_shotsBalanced labels0.04 0.02 0.00 0.02 0.04CalibrationInstruction qualityCross task size = 13b Instruction tuned Cross instruction One label factor1n_shotsBalanced labels0.04 0.02 0.00 0.02 0.04CalibrationInstruction qualityCross task size = 30b Instruction tuned Cross instruction One label factor1n_shotsBalanced labels0.04 0.02 0.00 0.02 0.04CalibrationInstruction qualityCross task size = 65b Instruction tuned factor1 Cross instruction One labeln_shotsBalanced labels0.04 0.02 0.00 0.02 0.04"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "The exact mappings of all 3-way interactions in our experiments.", "figure_data": "Model\u03bb 1\u03bb 2\u03bb 3\u03b2 \u03bb 1 \u00d7\u03bb 2 \u00d7\u03bb 37BInstruction quality CalibrationCross task0.03710613BInstruction tuned Calibration Instruction quality 0.00210213BInstruction quality Cross taskCalibration-0.013176"}], "formulas": [], "doi": "10.18653/v1/2022.acl-demo.9"}