{"title": "Greykite: Deploying Flexible Forecasting at Scale at LinkedIn", "authors": "Reza Hosseini; Albert Chen; Kaixu Yang; Sayan Patra; Yi Su; Saad Eddin Al Orjany; Sishi Tang; Parvez Ahammad; Saad Eddin", "pub_date": "2022-07-15", "abstract": "Forecasts help businesses allocate resources and achieve objectives. At LinkedIn, product owners use forecasts to set business targets, track outlook, and monitor health. Engineers use forecasts to efficiently provision hardware. Developing a forecasting solution to meet these needs requires accurate and interpretable forecasts on diverse time series with sub-hourly to quarterly frequencies. We present Greykite, an open-source Python library for forecasting that has been deployed on over twenty use cases at LinkedIn. Its flagship algorithm, Silverkite, provides interpretable, fast, and highly flexible univariate forecasts that capture effects such as time-varying growth and seasonality, autocorrelation, holidays, and regressors. The library enables self-serve accuracy and trust by facilitating data exploration, model configuration, execution, and interpretation. Our benchmark results show excellent out-ofthe-box speed and accuracy on datasets from a variety of domains. Over the past two years, Greykite forecasts have been trusted by Finance, Engineering, and Product teams for resource planning and allocation, target setting and progress tracking, anomaly detection and root cause analysis. We expect Greykite to be useful to forecast practitioners with similar applications who need accurate, interpretable forecasts that capture complex dynamics common to time series related to human activity.\u2022 Software and its engineering \u2192 Software libraries and repositories; \u2022 Applied computing \u2192 Forecasting; \u2022 Mathematics of computing \u2192 Time series analysis; \u2022 Computing methodologies \u2192 Machine learning.", "sections": [{"heading": "INTRODUCTION", "text": "Time series forecasts aim to provide accurate future expectations for metrics and other quantities that are measurable over time. At LinkedIn, forecasts are used for performance management and resource management. Performance management is the process of setting business metric targets and tracking progress to ensure we achieve them. This is the heartbeat of how LinkedIn manages its business [30]. Resource management involves optimizing allocation of budget, hardware, and other resources, often based on forecasts of business and infrastructure metrics.\nBefore this work, the processes for resource and performance management at LinkedIn were highly manual, relying on rulebased spreadsheets, simple linear regressions, or expert opinions. Domain knowledge and expert judgment, while often accurate, are subjective, hard to validate over time, and very hard to scale. Furthermore, they have trouble adapting to ecosystem shocks (such as COVID- 19) that change underlying dynamics and invalidate existing approaches to estimate metric growth. We have developed a forecasting library and framework that bring accuracy, scale, and consistency to the process. Furthermore, algorithmic forecasts can be automated and therefore integrated into systems that derive additional insights for decision-making.", "publication_ref": ["b29", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "MOTIVATION AND RELATED WORK", "text": "LinkedIn has diverse forecast requirements, with data frequencies from sub-hourly to quarterly (fiscal quarter) and forecast horizons from 15 minutes to over a year. Many of our time series exhibit strong growth and seasonality patterns that change over time due to product launches, ecosystem dynamics, or long-term drifts. Seasonality here refers to periodic/cyclic patterns over multiple horizons, e.g. daily seasonality refers to a cyclic pattern over the course of a day. For example, a new recommendation system may change the delivery schedule of mobile app notifications, affecting the seasonality pattern of user engagement; external shocks like COVID-19 can have an effect on hiring, job-seeking activity, and advertiser demand; long-term changes may shift engagement from desktop to mobile and affect traffic to our services. New features are constantly impacting the metrics, as LinkedIn runs hundreds of concurrent A/B tests each day.\nA suitable forecast algorithm must account for time series characteristics such as: (a) strong seasonality, with periods from daily to yearly, (b) growth and seasonality changes, (c) high variability around holidays, month/quarter boundaries, (d) local anomalies from external events and engineering bugs, (e) floating holidays (those with irregular intervals), (f) dependency on external factors such as macroeconomic indicators. It must also provide reliable prediction intervals to quantify uncertainty for decision making and anomaly detection. We expect such requirements to be common across many industries, and certainly within the technology sector, where time series patterns depend on human activity (of users) and product launches, and where forecasts are needed for both long-term planning and short-term monitoring.\nOur goal is to deliver a self-serve forecasting solution to data scientists and engineers across the company, many of whom have no specialized forecasting expertise. Developers need a way to develop accurate forecasts with little effort. Stakeholders need to trust the forecasts, which requires answering questions such as: Does the forecast include impact from the latest product launch? How quickly does the forecast adapt to new patterns? What is the expected impact of the upcoming holiday? Why has the forecast changed? Can the forecast account for macroeconomic effects? What would happen if economic recovery is fast/slow? To answer these questions, the model must be interpretable and easily tuned according to expert input when available.\nThe area of forecasting time series has a long history, with many models and techniques developed in the past decades. Some important examples include: classical time series models such as ARIMA (e.g. [17]) and GARCH [29]; exponential smoothing methods [32]; state-space models [7,18,31]; generalized linear model extensions to time series [10,14]; deep learning models such as RNN [22]. There are several popular open-source forecasting packages. Notable examples include fable [21], statsmodels [23], Prophet [26], and GluonTS [1]. The list of packages has grown quickly in recent years due to high demand for reliable forecasting in many domains. Each package has a different focus within the forecasting landscape.\nfable and statsmodels implement statistical models such as ARIMA and ETS, which can be trained and scored quickly. The ARIMA model captures temporal dependence and non-stationarity, and has extensions to include seasonality and regressors. However, it is not flexible enough to capture effects such as seasonality that changes in magnitude and shape over time, or volatility that increases around certain periods such as holidays or weekends. Nor is it readily interpretable for business stakeholders. ETS, while more interpretable than ARIMA, has similar problems capturing interaction effects and does not support regressors.\nProphet is a popular forecasting library that includes the Prophet model, designed to capture trend, seasonality, trend changepoints, and holidays. The package allows visualization of these effects for interpretability. However, model training and inference are slower due to its Bayesian formulation. And while the interface is user-friendly, with only a few tuning parameters, the model is not flexible enough to achieve high accuracy on complex time series. For example, it does not perform as well for short-term forecasts due to lack of native support for autoregression or other mechanisms to capture short-term dynamics. Prophet supports custom regressors, but these must be provided by the user for both training and inference; this is inconvenient for standard time features and cumbersome for complex interaction terms.\nGluonTS is a time series modeling library that includes the DeepAR algorithm [22]. DeepAR is a deep learning model that trains a single global model on multiple time series to forecast. While this can be powerful for automated forecasting, the crosstime series dependencies between input and output and lack of intuitive parameters make it hard to interpret the model or apply expert knowledge to improve the forecast.\nTo support LinkedIn's forecasting needs, we developed Greykite, a Python library for self-serve forecasting. 1 Its flagship algorithm, Silverkite, provides univariate forecasts that capture diverse time series characteristics with speed and interpretabilty. Our contributions include: (1) flexible design to capture complex time series dynamics for any frequency and forecast horizon, (2) forecast components that can be explicitly tuned, (3) interpretable output to explain forecast drivers, (4) fast training and inference, (5) decoupled volatility model that allows time-varying prediction intervals, (6) flexible objective function to predict peak as well as mean, ( 7) self-serve forecasting library that facilitates data exploration, model configuration, tuning, and diagnostics.", "publication_ref": ["b16", "b28", "b31", "b6", "b17", "b30", "b9", "b13", "b21", "b20", "b22", "b25", "b0", "b21", "b0", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "METHODOLOGY", "text": "In this section, we explain the design of Silverkite, the core forecasting algorithm in the Greykite library. The Silverkite algorithm architecture is shown in Figure 1. The user provides the input time series and any known anomalies, events, regressors, or changepoint dates. The model returns forecasts, prediction intervals, and diagnostics. The computation steps of the algorithm are decomposed into two phases:\n\u2022 Phase (1): the conditional mean model.\n\u2022 Phase (2): the volatility/uncertainty model.\nIn (1), a model predicts the metric of interest, and in (2), a volatility model is fit to the residuals. This design helps us with flexibility and speed, because integrated models are often more susceptible to poor tractability (convergence issues for parameter estimates) or divergence in the predicted values. Phase (1) can be broken down into these steps:\n(1.a) Extract raw features from timestamps, events data, and history of the series (e.g. hour, day of week Step (1.b) transforms the features into a space which can be used in additive models for better interpretability. For Step (1.d), we recommend regularization-based models such as ridge regression for mean prediction and (regularized) quantile regression for quantile prediction (e.g. for peak demand). In Phase (2), a conditional variance model is fit to the residuals, which allows the volatility to be a function of specified categories, e.g. day of week.", "publication_ref": ["b0"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "The Model Formulation", "text": "In this subsection, we give a brief overview of the mathematical formulation of the Silverkite model. We will demonstrate how this works on a real example in Section 4. Suppose { ( )}, = 0, 1, . . . is a real-valued time series where denotes time. Following [10] and [13], we denote the available information up to time by F ( ). For example F ( ) can include \nwhere , , , , , are functions of covariates in F ( ). They are linear combinations of these covariates or their interactions.\n( ) is the general growth term that may include trend changepoints 1 , . . . , , as\n( ) = 0 ( ) + \u2211\ufe01 =1 1 { > } ( ( ) \u2212 ( )),\nwhere ( ) is any growth function, e.g. ( ) = or ( ) = \u221a and 's are parameters to be estimated. Note that ( ) is a continuous and piecewise smooth function of .\n( ) = \u2208 P ( ) includes all Fourier series bases for different seasonality components (yearly seasonality, weekly seasonality, etc.), where P is the set of all seasonal periods. A single seasonality component ( ) can be written as\n( ) = \u2211\ufe01 =1 [ sin(2 ( )) + cos(2 ( ))],\nwhere , are Fourier series coefficients to be estimated by the model and is the series order. ( ) \u2208 [0, 1] is the corresponding time within a seasonal period. For example, daily seasonality has ( ) equal to the time of day (in hours divided by 24) at time . Moreover, for a list of time points 1 , . . . , where the seasonality is expected to change in either shape or magnitude or both, Silverkite models these changes as\n( ) = ( ; { , }) + \u2211\ufe01 =1 1 { > } ( ; { , }),\nwhere ( ) is a single seasonality component and is the seasonality term with coefficients { , }. Similar to trend changepoints, this formulation allows the Fourier series' coefficients to change and adapt to the most recent seasonal patterns. ( ) can also be modeled with categorical variables such as hour of day.\n( ) includes indicators on holidays/events and their neighboring days. For example, 1 { \u2208Christmas day} models Christmas effect. Holidays can have extended impact over several days in their proximity. Silverkite allows the user to customize the number of days before and after the event where the impact is non-negligible, and models them with separate indicators/effects. Indicators on month/quarter/year boundaries also belong to this category.\n( ) includes any time series information known up to time to model the remaining time dependence. For example, it can be lagged observations ( \u2212 1), . . . , ( \u2212 ) for some order , or the aggregation of lagged observations such as ( ( ); 1, 2, 3) = \u03a3 3 =1 ( \u2212 )/3. Aggregation allows for parsimonious models that capture long-range time dependencies.\n( ) includes other time series that have the same frequency as ( ) that can meaningfully be used to predict ( ). These time series are regressors, denoted ( ) = 1 ( ), . . . , ( ) , = 0, 1, . . . in the case of regressors. If available forecasts\u02c6( ) of ( ) are available, let ( ) =\u02c6( ). Otherwise, lagged regressors or aggregations of lagged regressors can be used, such as ( ) = ( \u2212 \u210e) where \u210e is the forecast horizon. If the minimum lag order is at least the forecast horizon, the inputs needed for forecasting have already been observed. Otherwise, in autoregression, having lag orders smaller than forecast horizon means that forecasted values must be used. To handle this, we incrementally simulate the future values needed for calculating the later forecasted values.\n( ) includes any interaction of the above terms to model complex patterns. For example, we can model different daily seasonality patterns during weekend and weekdays by including 1 { \u2208weekend} \u00d7 daily seasonality Fourier series terms, where \u00d7 denotes interaction between two components.\nTo mitigate the risk of the model becoming degenerate or overfitting, regularization can be used in the machine learning fitting algorithm for the conditional model. In fact, regularization also helps in minimizing the risk of divergence of the simulated future series for the model [12,14,28].\nAutomatic changepoint detection. The trend and seasonality changepoints help Silverkite stay flexible and adapt to the most recent growth and seasonal patterns. Silverkite offers fast automatic changepoint detection algorithms to reduce manual modeling efforts. Automatic changepoint detection is described below.\nFor trend changepoints, we first aggregate the time series into a coarser time series to eliminate short-term fluctuations (which are captured by other features, such as holidays). For example, daily data can be aggregated into weekly data. Next, a large number of potential trend changepoints are placed evenly across time, except for a time window at the end of the time series to prevent extrapolation from limited data. We model the aggregated time series as a function of trend, while controlling for yearly seasonality:\n( ) \u223c ( ) + ( ),\nwhere ( ) is the aggregated time series, ( ) is the growth with all potential trend changepoints and ( ) is yearly seasonality. The adaptive lasso penalty is used to identify significant trend changepoints [33] (lasso would over-shrink significant changepoints' coefficients to reach the desired sparsity level [27,33]). Finally, identified changepoints that are too close to each other are merged to improved model stability, and the detected changepoints are used to construct the piecewise growth basis function. See [15] for implementation details.\nFor seasonality changepoints, we use a similar formulation:\n( ) \u223c \u2211\ufe01 \u2208 P ( ),\nwhere ( ) is the de-trended time series and ( ) is defined in Section 3.1 , with 1 , . . . , being potential seasonality changepoints. Automatic selection is also done with adaptive lasso. The formulation allows seasonality estimates to change in both pattern and magnitude. This approach can capture similar effects as multiplicative seasonality, but is far more flexible in terms of the pattern changes, and avoids the problems of multiplicative seasonality magnitude growing too large with longer forecast horizons.\nThe volatility model. The volatility model is fit separately from the conditional mean model, with the following benefits compared to an integrated model that estimates mean and volatility jointly:\n(a) Stable parameter estimates and forecasts (e.g. [12,28]). (b) Speed gain by avoiding computationally heavy Maximum Likelihood Estimation (e.g. [14]) or Monte Carlo methods (e.g. [31]). These factors are important in a production environment that requires speed, accuracy, reliability, and code maintainability. It is typical to see larger volatility around certain periods such as holidays and month/quarter boundaries. Silverkite's default volatility model uses conditional prediction intervals to adapt to such effects, allowing volatility to depend on categorical features. The model is described below.\nLet ( ) be the target series and\u02c6( ) be the forecasted series. Define the residual series as ( ) = ( ) \u2212\u02c6( ). Assume the volatility depends on given categorical features 1 , . . . , which are also known into the future, for example, day of week. Consider the empirical distribution ( | 1 , . . . , ) and fit a parametric or nonparametric distribution to the combination as long as the sample size for that combination, denoted by ( 1 , . . . , ) is sufficiently large e.g. ( 1 , . . . , ) > , = 20. Note that one can find an appropriate using data (for example, during cross-validation by checking the distribution of the residuals). Then from this distribution, we estimate the quantiles ( 1 , . . . , ) to form the prediction interval with level 1 \u2212 :\n(\u02c6( ) + ( 1 , . . . , )( /2),\u02c6( ) + ( 1 , . . . , )(1 \u2212 /2)).\nOne choice for a parametric distribution is the Gaussian distribution N (0, 2 ( 1 , . . . , )). While the residuals of a naive model can be far from normal, it is possible that after conditioning on the appropriate features, the residuals of a sufficiently complex mean model are approximately normal as observed in [14]. This assumption can be checked by inspecting the qq-plots of the conditional errors.\nSilverkite offers an option to use empirical quantiles to construct the prediction intervals when this assumption is violated. Silverkite's flexibility allows other volatility models to be added. For example, a regression-based volatility model could be used to condition on many features, including continuous ones. ", "publication_ref": ["b9", "b12", "b11", "b13", "b27", "b32", "b26", "b32", "b14", "b11", "b27", "b13", "b30", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "How Silverkite Meets the Requirements", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MODELING FRAMEWORK", "text": "We implemented these algorithms in Greykite, a Python library for data scientists and engineers across the company. Figure 2 shows how the library facilitates each step of the modeling process. The first step is data exploration. Greykite provides interactive, exploratory plots to assess seasonality, trend, and holiday effects. These plots help users to identify effects that are hard to see when plotting the entire time series. We illustrate this with the D.C. bikesharing time series [2] in Figure 3. The top plot shows the daily seasonality pattern across the week. The shape changes between weekends and weekdays, revealing an interaction between daily seasonality and is_weekend. The bottom plot shows the (mean centered) yearly seasonality and an overlay for each year. By comparing how the lines change over time, we can see that seasonality magnitude increased until 2017, then decreased. The automatic changepoint detection module can be used on its own to explore trend and seasonality. Figure 4 shows a changepoint detection result, revealing both trend and seasonality changepoints in the time series.\nThe second step is configuration. The user provides time series data and a forecast config to the Forecaster class, which produces the forecast, diagnostics, and the trained model object. The config allows the user to select an algorithm, forecast horizon, coverage, model tuning parameters, and evaluation parameters. The config is optional to make quick start easy, but also very customizable.\nGreykite exposes different algorithms, including Silverkite, Prophet, and SARIMA, as scikit-learn estimators that can be configured from the forecast config. Silverkite provides intuitive tuning parameters such as: autoregressive lags, changepoint regularization strength, and the list of holiday countries. Many Silverkite tuning parameters, including changepoints, seasonality, autoregression, and interaction terms have an intelligent \"auto\" setting. Others have reasonable defaults, such as ridge regression for the machine learning model.\nFor high-level tuning, we introduce the concept of model templates. Model templates define forecast parameters for various data characteristics and forecast requirements (e.g. hourly short-term forecast, daily long-term forecast). Model templates drastically reduce the search space to find a satisfactory forecast. They allow decent forecasts out-of-the-box even without data exploration. When model template is \"AUTO\", Greykite automatically selects the best model template for the input data.\nFine-tuning is important to get the best possible accuracy for key business metrics with high visibility and strict accuracy requirements. Therefore, our library provides full flexibility to customize the settings of a model template. For example, the user can add custom changepoints to enable faster adaptation to known changes and label known anomalies in the training data. The user can easily experiment with derived features by specifying model formula terms such as 'is_weekend:y_lag_1' (weekend/AR1 interaction). Because Silverkite generates many features internally, the user can leverage these to fine-tune the model without writing any code.\nThe third step is running the model. Internally, the Forecaster class runs an end-to-end forecasting pipeline with pre-processing, hyperparameter grid search, evaluation, backtest, and forecast. Grid search enables forecasting many metrics in a more automated way by selecting the optimal model from multiple candidates. We offer a default parameter grid for efficient search of the space. Automated machine learning techniques may also be used. Silverkite's fast training and inference facilitates such hyperparameter tuning. Greykite offers a benchmarking class to compare algorithms.\nThe last step is to diagnose the model and interpret results, both to improve the model and to establish trust with stakeholders. Again, we illustrate this on the D.C. bike-sharing dataset. Figure 5 plots forecast components such as trend, seasonality, autoregression, and holidays, representing the forecast as a sum of the contribution from each group. This view helps stakeholders understand the algorithm's assumptions (how it makes predictions) and the patterns present in the dataset. In Figure 5, the fitted trend first increases then slightly decreases after a few detected changepoints. The yearly seasonality reflects a higher number of rides during warmer months and a lower number of rides during colder months, with increasing magnitude over time. In the presence of multicollinearity, one should treat this plot as descriptive of the model rather than showing the true effect of each component. Effect interpretation is improved through (1) groups of covariates that capture mostly orthogonal effects, (2) regularization, (3) fewer covariates when data are limited, and (4) enough training data to distinguish effects. While component plot shows the effect of groups of covariates, model summary shows the effect of individual covariates, as shown in Figure 6. It includes model overview, coefficients, p-values, and confidence intervals. Greykite supports model summary for OLS and regularized models from scikit-learn and statsmodels, calculating the intervals using bootstrap for ridge and multi-sample splitting for lasso [5,8]. This diagnostic can be used to inspect coefficients and assess whether any terms might be removed.\nComponent plot and model summary provide explanations of why forecasts change after the model is trained on new data. When strategic business decisions depend on the forecasted value, it is particularly important to assess whether the drivers are reasonable before deciding to take action.\nThus, Greykite's modeling framework addresses three key requirements of self-serve forecasting adoption: (1) accuracy, (2) ease of use, (3) trust. It achieves accuracy by making the Silverkite's flexible algorithms easy to configure and tune, ease of use by aiding each step of the modeling process, and trust through interpretability.", "publication_ref": ["b1", "b4", "b7"], "figure_ref": ["fig_2", "fig_3", "fig_4", "fig_5", "fig_5", "fig_6"], "table_ref": []}, {"heading": "BENCHMARK", "text": "In this section, we compare Silverkite against Prophet and SARIMA (seasonal ARIMA) on a variety of datasets. Prophet is a popular univariate forecasting algorithm designed for interpretability and self-serve in the business context [26]. SARIMA is a well-known forecasting model that has stood the test of time and is widely available in many languages. While the model is not readily interpretable, it captures seasonality, temporal-dependence, and non-stationarity and has been a strong baseline in recent forecasting competitions [19]. For the benchmark, we use the python packages greykite v0.4.0, prophet v1.0.1, and pmdarima v1.8.0 [24].", "publication_ref": ["b25", "b18", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Benchmark Setup", "text": "Due to temporal dependency in time series data, the standard -fold cross-validation is not appropriate. Rolling origin cross-validation is a common technique in the time series literature [25]. Unlike \"fixed origin, \" which fits the data on the training set and forecasts on the following test period only once, the rolling origin method evaluates a model with a sequence of forecasting origins that keeps moving forward. This makes model evaluation more robust by averaging across time periods. We use -fold rolling origin evaluation, described in Figure 10 in Appendix A.1.\nTo evaluate multiple time series at different scales, scaleindependent metrics are frequently used, such as MAPE, sMAPE [9], and Mean Absolute Scaled Error (MASE) [16]. The first two metrics have the disadvantage of being infinite or undefined when there are zero values in the data. In addition, they put a heavier penalty on errors for low actual values, hence they are biased. Thus, we compare MASE with seasonal period according to the data frequency. For calculation details, see Appendix A.2.", "publication_ref": ["b24", "b8", "b15"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dataset and Setup.", "text": "Comparing time series forecasting models has gained interest in recent years, such as in the M Competitions [19,20]. The curation of time series datasets has also grown rapidly, including UCI Machine Learning Repository (with 100+ datasets) [6], and the recently released Monash time series archive (30 datasets from different domains) [11]. Prophet and Greykite packages also have built-in datasets.\nMany dataset collections contain either multivariate time series for global modeling or hundreds of univariate time series. They are intended for fixed origin evaluation, and the result is averaged across many datasets. In our benchmark framework, we intend to evaluate the algorithms over a comprehensive period of time using a large number of splits. Thus, we choose nine datasets from the above sources suitable for rolling origin evaluation. The chosen datasets span a broad range of categories: energy, web, economy, finance, transportation, and nature. Their frequencies range from hourly to monthly and we benchmark them across multiple forecast horizons. The datasets are summarized in  [11] 203 yr. 24 months 24 (M) House supply [3] 59 yr. 24 months Table 1: Datasets and their benchmark configuration. We use forecast horizons 1 and 24 for hourly data; 1, 7, and 30 for daily data; 1 and 12 for monthly data.", "publication_ref": ["b18", "b19", "b5", "b10", "b10", "b2"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Model Comparison.", "text": "We benchmark the performance of Silverkite, Prophet, and SARIMA. For Silverkite, we use the \"AUTO\" model template. For SARIMA, we use the out-of-the-box settings of pmdarima. It uses statistical tests and AIC to identify the optimal , , , , , parameters. 2 For Prophet, we use the out-of-the-box settings with holidays added for hourly and daily frequencies to match the configuration of Silverkite. Because Prophet also natively supports holidays, we use the same default holiday countries for a fair comparison: US, GB, IN, FR, CN. Holidays are not needed for monthly data due to the level of aggregation, so they are not included for monthly frequency.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "MASE", "text": "Horizon For each frequency and forecast horizon combination, Table 2 shows the MASE (lower is better) averaged across benchmark datasets. Silverkite significantly outperforms the other two algorithms in all but one setting. For that setting (monthly frequency with horizon 1), Silverkite is optimal on sunspot data and a close second on house supply data. The full results for each dataset are in Table 3 in Appendix A.4. Of the 24 dataset/horizon combinations, Silverkite is optimal on 20. Because Silverkite has the advantage of being the most flexible of the three models, it is possible to fine-tune Silverkite to achieve better performance on the rest.\nFor a given frequency, Silverkite and SARIMA show noticable improvement on shorter horizons, but Prophet does not. This suggests that Silverkite and SARIMA make better use of recent information through autoregression, whereas Prophet focuses on overall trends. Silverkite's interpretable approach, with groups of covariates that capture underlying time series dynamics, could be one reason it outperforms SARIMA.\nIt is noteworthy that Prophet and SARIMA usually have MASE > 1 (i.e. forecast error is higher than the naive method's in-sample error). Silverkite, on the other hand, usually has MASE < 1. Thus, Silverkite offers good out-of-the-box performance on a wide range of datasets, frequencies, and horizons. Moreover, Silverkite is often faster than the other algorithms. The runtime comparison for training and inference can be found in Tables 4 and 5 in Appendix A.5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "DEPLOYMENT", "text": "We deployed model-driven forecasting solutions at LinkedIn for more than twenty use cases to streamline product performance management and optimize resource allocation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Performance Management", "text": "The performance management process at Linkedin involves setting targets for core business metrics, making investments, tracking progress, detecting anomalies, and finding root causes. Given its centrality and ubiquity across the business, it is critical to move away from manual processes and adapt to growing data volume and complexity. To achieve this, we facilitate adoption of Greykite.\nFirst, we partnered with Financial Planning & Analysis (FP&A) to build automated forecasting-based performance management for LinkedIn Marketing Solutions (LMS). Success for this customer was key to extending our outcomes and learnings to more use cases for other business verticals.\nLMS is a fast-growing business, with a complex ads ecosystem for advertisers and potential customers. Ads marketplace metrics exhibit complex growth and seasonality patterns and need to be modeled at different frequencies and across many dimensions. Due to this complexity, this use case became an essential learning experience for us to deploy new solutions and scale them to other business verticals. It helped us define requirements, for example:\n\u2022 Autoregression to improve next-day forecast accuracy.\n\u2022 Indicators to capture sharp changes around month/quarter start and end. \u2022 Trend and seasonality changepoints to capture the effect of large feature launches. \u2022 User-provided anomalies to ignore temporary issues.\nTo enable efficient detection of revenue issues, our forecasts needed to meet a high level of accuracy (e.g. < 2% MAPE for daily nextday forecast) and have reliable prediction intervals. An internal benchmark showed that Silverkite met the requirements and has 75% lower error on revenue metrics compared to Prophet.\nWe deployed models for over thirty LMS metrics such as ads revenue and ads impressions and their key dimensions. The forecasts are at daily, weekly, and monthly frequencies with horizons from 1 day to 6 months. These are integrated into production dashboards and are sent in daily emails to show business outlook and flag concerns about business health. The emails compare forecasted metrics against their targets and alert anomalies in observed data compared to the forecasted baseline interval. To aid investigation when revenue falls outside expectations, the dashboard includes forecasts for supply-side and demand-side marketplace metrics and their dimensions. This helps isolate the problem to a particular part of the supply or demand funnel or segment of the business. The automated performance management solution has been in production for 18 months and is the primary source for FP&A to quickly assess business health and begin investigations if needed.\nTo scale our solution to other business verticals, we partnered with LinkedIn Premium and Talent Solutions to provide short-term dimensional forecasts for key metrics such as sign-ups, bookings, sessions, and weekly active users. We observed significant improvements in forecasting accuracy (MAPE) and faster anomaly detection with higher precision and recall. Figure 7 shows how Greykite detected anomalies for LinkedIn Premium and helped them assess impact severity; these anomalies were missed by the existing weekover-week threshold detection. Greykite achieves remarkable performance on long-term forecasts as well. In partnership with FP&A, we developed monthly forecasts for the next half-year of revenue that significantly outperformed manual forecasts, providing better guidance for strategic decisions. Figure 8 shows how the forecast adapted to revenue growth momentum earlier than the manual forecast through autoregression and automatic changepoint detection. ", "publication_ref": [], "figure_ref": ["fig_7", "fig_8"], "table_ref": []}, {"heading": "Resource Management", "text": "On the infrastructure side, forecasts help LinkedIn maintain site and service availability in a cost-effective manner as site traffic continues to increase. Better projections about future traffic, combined with accurate site capacity measurements, enable confident decision-making and sustainable growth through right-sized applications whose provisioned instances match required workloads.\nPrior to deploying Greykite forecasts, capacity planning was highly manual, reactive, and had a tendency to overprovision resources. We provide hourly peak-minute queries per second (QPS) forecasts for hundreds of services, which are used to automate right-sizing. With the Automatic Rightsizing system conducting hundreds of rightsize actions a day, we eliminate most of the toil required of application owners to manually adjust compute resources to support organic business growth, save millions of dollars by removing excess capacity, and optimize fleet resource utilization by re-purposing such excess capacity to applications lacking capacity. The forecasts are shown in production dashboards alongside allocated serving capacity, as shown in Figure 9. The system has been in production for over two years. Since this collaboration, we have seen significant reduction of capacity-related incidents as applications are automatically uplifted to match expected workload.\nFigure 9: The production dashboard for application headroom shows projected peak load QPS (red) and excess capacity (shaded blue) to inform decision-making on resource allocation. This plot reveals excess capacity that could be removed while safely supporting peak load.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lessons Learned", "text": "We learned some lessons on algorithms, framework, and deployment through solving company-wide forecasting problems at scale.\nDiverse requirements can be met with a highly flexible family of models. The flexible design that captures trend, seasonality, changepoints, holidays, autoregression, interactions, and regressors enables accurate forecasts of business and infrastructure metrics across frequencies and horizons.\nIt is possible to achieve high accuracy without sacrificing interpretability. Interpretation is essential to building trust with stakeholders who want to understand how forecasts are made, modify assumptions, and understand why forecasted values change after training on more data. Silverkite transforms features into a space that can be used in a regularized linear model, allowing additive component plots and model summary.\nEnabling self-serve allowed scaling forecast solutions across the business. This required high out-of-the-box accuracy with little effort, which we achieved with an end-to-end forecasting library and intuitive tuning interfaces, and a fast algorithm for interactive tuning and hyperparameter search.\nPartnership with customers is essential to deploying new research ideas. During alpha and beta, we worked with a few champion customers to clarify requirements and deliver wins. Then, we generalized the solution and scaled it through a self-serve model. Champion customers should be high impact, representative of other customers, aligned with company priorities, and willing to form a close partnership. As we proved success, the champion use cases became advocates and examples to drive adoption forward.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DISCUSSION", "text": "Prior to this work, LinkedIn's forecasts were mostly manual, ad-hoc, and intuition-driven. Now, customers from all lines of business and all functions (engineering, product, finance) are adopting algorithmic forecasts. Customers understand the benefits of accuracy, scale, and consistency. Our forecasts save time and drive clarity on the business/infrastructure that empowers LinkedIn to plan and adapt dynamically to new information. This culture shift was achieved through a fast, flexible, and interpretable family of models and a modeling framework that enables self-serve forecasting with ease and accuracy.\nThe Silverkite algorithm performs well on both internal datasets and out-of-the-box on external datasets with frequencies from hourly to monthly from a variety of domains. Its flexible design allows fine-tuning of covariates, objective function, and volatility model. Thus, we expect the open-source Greykite library to be useful to forecast practitioners, especially those whose time series include time-varying growth and seasonality, holiday effects, anomalies, and/or dependency on external factors, characteristics that are common in time series related to human activity.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A APPENDIX A.1 Rolling Origin Evaluation", "text": "Greykite includes the BenchmarkForecastConfig class to perform rolling origin benchmarking. Rolling origin evaluation works as follows. We replicate the time series data to train-test splits. Each split contains a test period (red) immediately after its training period (deep and light blue). The forecasting origins (test start dates) are picked in a rolling manner, working backward from the end of the time series.\nEach candidate model is fit on the training period of each of the rolling splits, and the trained model is used to predict on the corresponding test period. The average test error across all splits mimics the backtest error if the model were put in production, and this provides a reliable estimate of the future forecast error. A few parameters control how the rolling origin splits are generated. First, since one of the goals of rolling origin evaluation is to make sure the estimated forecast error is robust and representative, needs to be sufficiently large. We recommend that the test periods cover at least one year of data for daily and less granular frequencies (weekly, monthly, etc.). For sub-daily data with a short forecast horizon, we recommend the test periods cover at least one month.\nshould be chosen together with the period between the splits, which controls the step size between every two successive splits. In general, we recommend step size of 1 to obtain the most comprehensive evaluation. In practice, for small granularity data (such as hourly or minutely), it can make sense to increase the period between splits and reduce to decrease computational cost. As a result, the test periods in these rolling splits may or may not overlap.\nAnother two parameters are the lengths of the training and test periods for each split. In order to best estimate the model's performance in real applications, the length of each test period should match the desired forecast horizon. Ideally, the minimum training period is at least two years for daily and less granular frequencies in order to accurately estimate the yearly seasonality effect. For sub-daily data, a shorter training period could be used for short-term forecasts. The model that minimizes the average error across all the rolling splits is deemed as the best model.\nFinally, the split training periods could either use expanding windows (fixed train start date) or moving windows (fixed length), as shown in Figure 10. We recommend choosing the one that best mimics the setting for deployment. Moving windows is preferred to increase speed when not all history is needed for an accurate forecast. Expanding windows is preferred if speed is not an issue and using more history improves accuracy.\nSuch an extensive rolling evaluation not only provides a robust estimate of the model performance, but also enables users to run comprehensive diagnostics, such as error breakdowns on different seasonal cycles (e.g. day of week) and holiday effects. Then, these systematic errors could be addressed by incorporating these signals as features into the model. We developed an internal product Greykite-on-Spark for benchmarking and hyperparameter tuning on Spark. With parallel execution on Spark, all experiments could be finished within a day.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "A.2 Evaluation Metrics", "text": "MASE [16] is calculated by dividing the out-of-sample MAE by the in-sample MAE from a naive forecast. For daily time series with weekly seasonality, the naive forecast uses the value at \u2212 7 to predict the value at time , where 7 is called the \"seasonal period\". For non-seasonal time series, the seasonal period is 1. Similar to [11], we choose the seasonal period based on the frequency of the data. In our experiments, we use 24 for hourly data, 7 for daily data, and 12 for monthly data.", "publication_ref": ["b15", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Benchmark Datasets", "text": "Hourly. 4 datasets, horizons 1 and 24. The hourly datasets are solar power, wind power, electricity, and traffic. These datasets are from Monash [11].\nThe solar (wind) power dataset contains the solar (wind) power production of an Australian wind farm from August 2019 to July 2020, with original frequency 4-second. We aggregate it to an hourly series and remove any incomplete hours. Since the time series is shorter than one year, the seasonal effects may not be well estimated. Also, the solar power dataset is an intermittent time series where a lot of zeros are present, hence using MASE is better than MAPE or sMAPE. We use a moving 300-day window for training, and test on every 4 hours in the last 30 days (30*6 splits).\nThe electricity dataset contains the hourly consumption (in kW) of 321 clients from 2012 to 2014 published by Monash [11], originally from [6]. We aggregate them by taking the average across the 321 clients. The averaged series has a history of 3 years. We use a moving two-year window for training, and test on every 4 hours in the last year (365*6 splits).\nThe SF Bay Area traffic dataset records the road occupancy rates (between 0 and 1) measured by different sensors on San Francisco Bay area freeways from 2015 to 2016. We take the average of these occupancy rates. We use a moving 600-day window for training, and test on every 4 hours in the last 60 days (60*6 splits).\nThese datasets present both intraday and interday seasonalities. Daily. 4 datasets, horizons 1, 7, and 30. The daily datasets are page views, bike rental counts, traffic, and Bitcoin transactions. We test on the last year (365 splits, period between splits 1), with an expanding window for training.\nThe Peyton Manning dataset contains the logarithm of daily page views for the Wikipedia page for football player Peyton Manning. This dataset is from Prophet [26] and it has 8 years of history. Missing values are imputed by us with linear interpolation when fitting the models.\nThe bike-sharing dataset contains aggregated daily counts of the number of rented bikes in Washington, D.C. from 2010 to 2019 (incomplete days are removed). The raw dataset is from Capital Bikeshare [2] and it is also available in Greykite.\nThe SF Bay Area traffic dataset is the same as for hourly benchmarking. We aggregate it to a daily time series from 2015 to 2016.\nThe Bitcoin dataset has the number of Bitcoin transactions from 2009 to 2021. The dataset was curated (with missing values filled) by Monash [11].\nThe Peyton Manning and bike-sharing datasets present weekly and yearly seasonalities and possible changepoints. The SF Bay Area traffic dataset also shows relatively strong weekly and yearly seasonalities but has a shorter history, which makes it harder to estimate long-term effects. The Bitcoin dataset is much more volatile than the others.\nMonthly. 2 datasets, horizons 1 and 12. The monthly datasets are sunspot activity and house supply. We test on the last 2 years of data (24 splits, 1 period between splits), with an expanding window for training.\nThe first dataset is daily sunspot activity from 1818 to 2020 published by Monash [11], originally from [4]. We aggregate it to a monthly time series more than 200 years long. The second dataset is monthly house supply from 1963 to 2021 obtained from FRED [3]. Both have mild yearly seasonality and some multi-year patterns.", "publication_ref": ["b10", "b10", "b5", "b25", "b1", "b10", "b10", "b3", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "A.4 Full Benchmark Results", "text": "The complete benchmark results (MASE) are shown in Table 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.5 Runtime Comparison", "text": "We compare the training and inference runtime on a MacBook Pro with 2.4 GHz 8-Core Intel Core i9 processor, and 32 GB 2667 MHz DDR4 memory. Results are recorded in Tables 4 and 5. Hourly models are evaluated with 2 years of training data using the electricity dataset. Daily models are evaluated with 8 years of training data using the Peyton Manning dataset. Monthly models are evaluated with 202 years of training data using the sunspot dataset. All models use forecast horizon 1. All models use the same configurations as before to produce a forecast and 95% prediction interval. All measurements are the best of 7 runs.\nSilverkite's training speed is significantly faster than Prophet and SARIMA. The only exception is the monthly Prophet model, which is extremely fast because its features for monthly data are quite limited. However, Prophet's MASE for sunspot monthly data with horizon 1 is 17x higher than Silverkite's (1.497 vs 0.088).\nSilverkite's inference speed is significantly faster than Prophet's for all frequencies because Prophet requires MCMC to sample from a distribution. The inference time of SARIMA is near zero because its predictions are easy to calculate from the analytical solution, whereas Silverkite prepares the future features and performs a larger matrix multiplication.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is done by the Data Science Applied Research team at LinkedIn. Special thanks to our TPM Shruti Sharma and our collaborators in Data Science, Engineering, SRE, FP&A, BizOps, and Product for adopting the library. In particular, thanks to Ashok Sridhar, Mingyuan Zhong, Peter Huang, Hamin Oh, Neha Gupta, Neelima Rathi, Deepti Rai, Christian Rhally, Camilo Rivera, Priscilla Tam, Meenakshi Adaikalavan, Zheng Shao, Mike Snow, Stephen Bisordi, Dong Wang, Ankit Upadhyaya, and Rachit Kumar for allowing us to share their use cases. We also thank our leadership team Ya Xu and Sofus Macsk\u00e1ssy for their support.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "GluonTS: Probabilistic and Neural Time Series Modeling in Python", "journal": "Journal of Machine Learning Research", "year": "2020", "authors": "Alexander Alexandrov; Konstantinos Benidis; Michael Bohlke-Schneider; Valentin Flunkert; Jan Gasthaus; Tim Januschowski; Danielle C Maddix; Syama Rangapuram; David Salinas; Jasper Schulz; Lorenzo Stella; Ali Caner T\u00fcrkmen; Yuyang Wang"}, {"ref_id": "b1", "title": "System Data", "journal": "", "year": "2009", "authors": "Capital Bikeshare"}, {"ref_id": "b2", "title": "Department of Housing, and Urban Development. 2022. Monthly Supply of Houses in the United States", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "Revisiting the sunspot number", "journal": "Space Science Reviews", "year": "2014", "authors": "Fr\u00e9d\u00e9ric Clette; Leif Svalgaard; M Jos\u00e9; Edward W Vaquero;  Cliver"}, {"ref_id": "b4", "title": "High-dimensional inference: confidence intervals, p-values and R-software hdi", "journal": "Statistical science", "year": "2015", "authors": "Ruben Dezeure; Peter B\u00fchlmann; Lukas Meier; Nicolai Meinshausen"}, {"ref_id": "b5", "title": "UCI Machine Learning Repository", "journal": "", "year": "2017", "authors": "Dheeru Dua; Casey Graff"}, {"ref_id": "b6", "title": "Time Series Analysis by State Space Methods", "journal": "Oxford University Press", "year": "2012", "authors": "James Durbin;  Siem"}, {"ref_id": "b7", "title": "An introduction to the bootstrap", "journal": "CRC press", "year": "1994", "authors": "Bradley Efron; J Robert;  Tibshirani"}, {"ref_id": "b8", "title": "A Pragmatic View of Accuracy Measurement in Forecasting", "journal": "Omega", "year": "1986", "authors": "E Benito;  Flores"}, {"ref_id": "b9", "title": "Regression models for time series analysis", "journal": "Wiley-Interscience", "year": "2002", "authors": "Konstantinos Fokianos"}, {"ref_id": "b10", "title": "Monash time series forecasting archive", "journal": "", "year": "2021", "authors": "Rakshitha Godahewa; Christoph Bergmeir; Geoffrey I Webb; Rob J Hyndman; Pablo Montero-Manso"}, {"ref_id": "b11", "title": "Model selection for count timeseries with applications in forecasting number of trips in bike-sharing systems and its volatility", "journal": "", "year": "2020", "authors": "Alireza Hosseini; Reza Hosseini"}, {"ref_id": "b12", "title": "A characterization of categorical Markov chains", "journal": "Journal of Statistical Theory and Practice", "year": "2011", "authors": "Reza Hosseini; Nhu Le; Jim Zidek"}, {"ref_id": "b13", "title": "Non-linear timevarying stochastic models for agroclimate risk assessment", "journal": "Environmental and Ecological Statistics", "year": "2015", "authors": "Reza Hosseini; Akimichi Takemura; Alireza Hosseini"}, {"ref_id": "b14", "title": "2021. A flexible forecasting model for production systems", "journal": "", "year": "2021", "authors": "Reza Hosseini; Kaixu Yang; Albert Chen; Sayan Patra"}, {"ref_id": "b15", "title": "Another look at forecast-accuracy metrics for intermittent demand", "journal": "Foresight: The International Journal of Applied Forecasting", "year": "2006", "authors": "J Rob;  Hyndman"}, {"ref_id": "b16", "title": "Forecasting: principles and practice", "journal": "OTexts", "year": "2018", "authors": "J Rob; George Hyndman;  Athanasopoulos"}, {"ref_id": "b17", "title": "A New Approach to Linear Filtering and Prediction Problems", "journal": "Journal of Basic Engineering", "year": "1960-03", "authors": "R E Kalman"}, {"ref_id": "b18", "title": "The M4 Competition: Results, findings, conclusion and way forward", "journal": "International Journal of Forecasting", "year": "2018", "authors": "Spyros Makridakis"}, {"ref_id": "b19", "title": "Evangelos Spiliotis, and Vassilios Assimakopoulos. 2022. M5 accuracy competition: Results, findings, and conclusions", "journal": "International journal of forecasting", "year": "2022", "authors": "Spyros Makridakis"}, {"ref_id": "b20", "title": "2021. fable: Forecasting Models for Tidy Time Series", "journal": "", "year": "", "authors": "O' Mitchell; Rob Hara-Wild; Earo Hyndman;  Wang"}, {"ref_id": "b21", "title": "DeepAR: Probabilistic forecasting with autoregressive recurrent networks", "journal": "International Journal of Forecasting", "year": "2020-01", "authors": "David Salinas; Valentin Flunkert"}, {"ref_id": "b22", "title": "Statsmodels: Econometric and statistical modeling with python", "journal": "", "year": "2010", "authors": "Skipper Seabold; Josef Perktold"}, {"ref_id": "b23", "title": "", "journal": "ARIMA estimators for Python", "year": "2009", "authors": "Taylor G Smith"}, {"ref_id": "b24", "title": "Out-of-sample tests of forecasting accuracy: an analysis and review", "journal": "International journal of forecasting", "year": "2000", "authors": "J Leonard;  Tashman"}, {"ref_id": "b25", "title": "Forecasting at scale", "journal": "The American Statistician", "year": "2018", "authors": "J Sean; Benjamin Taylor;  Letham"}, {"ref_id": "b26", "title": "Regression shrinkage and selection via the Lasso", "journal": "Journal of the Royal Statistical Society: Series B (Methodological)", "year": "1996", "authors": "Robert Tibshirani"}, {"ref_id": "b27", "title": "Non-linear time series: a dynamical system approach", "journal": "Oxford university press", "year": "1990", "authors": "Howell Tong"}, {"ref_id": "b28", "title": "Analysis of Financial Time Series", "journal": "Wiley", "year": "2010", "authors": "S Ruey;  Tsay"}, {"ref_id": "b29", "title": "Next Play Ventures. 2022. Forecasting. Retrieved", "journal": "", "year": "2009-02", "authors": ""}, {"ref_id": "b30", "title": "Bayesian forecasting and dynamic models", "journal": "Springer Science & Business Media", "year": "2006", "authors": "Mike West; Jeff Harrison"}, {"ref_id": "b31", "title": "Forecasting sales by exponentially weighted moving averages", "journal": "Management Science", "year": "1960-04", "authors": " Peter R Winters"}, {"ref_id": "b32", "title": "The adaptive Lasso and its oracle properties", "journal": "Journal of the American statistical association", "year": "2006", "authors": "Hui Zou"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Architecture diagram for Greykite's main forecasting algorithm: Silverkite. covariates such as ( \u2212 1), ( \u2212 2), ( ), ( \u2212 1) where ( \u2212 ) denotes lags of ; ( ) is the value of a regressor known at time and ( \u2212 1) is the value of the same regressor at time \u2212 1. The latter is often referred to as lagged regressor. The conditional mean model. The conditional mean model is E[ ( )|F ( )] \u223c ( ) + ( ) + ( ) + ( ) + ( ) + ( ),(1)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(c) Flexible to pair any conditional mean model with any volatility model for better accuracy. (d) Modular engineering framework for simple development and testing.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Library aids each step of the forecast workflow.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: Greykite provides exploratory plots to identify seasonality effects and interactions.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Automatic trend and seasonality changepoint detection in bike-sharing data.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: The component plot shows how groups of covariates contribute to the forecasted value. It can be used for interpretability and debugging.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 6 :6Figure 6: Model summary shows the effect of individual covariates for interpretability, model tuning, and debugging.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Greykite detected unexpected anomalies and helped LinkedIn Premium assess impact severity.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 8 :8Figure 8: Greykite outperforms manual forecast by picking up revenue growth momentum at the start of the half-year.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 10 :10Figure 10: -fold rolling origin evaluation. The example shows four splits with forecast horizon 4 and period between splits 2. The light blue points could either be included or not included in the training set, depending on user's choice of whether to use expanding windows (light blue + deep blue) or moving windows (deep blue only).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Silverkite handles the time series characteristics mentioned in Section 2. Strong seasonality is captured by the Fourier series basis function ( ). A higher order or categorical covariates (e.g. hour of day) can be used to capture complex seasonality patterns. Growth and seasonality changes over time are handled by automatic detection of trend and seasonality changepoints. Additionally, autoregression allows quick adaptation to new patterns, which is especially useful in short-term forecasts. High variability around holidays and month/quarter boundaries is addressed by explicitly including their effects in the mean model, and by allowing the volatility model to condition on such features. To capture changes in seasonality on holidays, Silverkite allows for interactions between holiday indicators and seasonality. Floating holidays are easily handled by looking up their dates in Greykite's holiday database. Local anomalies are handled by removing known anomalies from the training set. Impact of external factors is incorporated through regressors, whose forecasted values can come from Silverkite or another model. This allows for comparing forecast scenarios. Thus, Silverkite's design captures these time series characteristics in an intuitive manner amenable to modeling and interpretation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "with their detailed descriptions in Appendix A.3. Any missing values are imputed by linear interpolation for model training. The imputed values are not used when calculating model accuracy.", "figure_data": "(Frequency) DatasetLength Test LengthSplits(H) Solar power [11]1 yr.30 days30*6(H) Wind power [11]1 yr.30 days30*6(H) Electricity [6]3 yr.365 days365*6(H) SF Bay Area traffic [11]2 yr.60 days60*6(D) Peyton Manning [26]9 yr.365 days365(D) Bike sharing [2]8 yr.365 days365(D) SF Bay Area traffic [11]2 yr.365 days365(D) Bitcoin transactions [11] 11 yr.365 days365(M) Sunspot"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Model comparison. Average MASE across datasets.The best model for each frequency and horizon is in bold.", "figure_data": "Silverkite Prophet SARIMAHourly10.7412.0471.178240.9452.0343.429Daily10.9401.5271.35871.0971.5391.515301.2511.5751.705Monthly 10.3331.2890.303120.5791.2920.724"}, {"figure_label": "345", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Detailed model comparison. Showing MASE for each dataset and horizon. The best model for each row is in bold. Training time comparison (in seconds). Inference time comparison (in seconds).", "figure_data": "MASEHorizon Silverkite Prophet SARIMA(H) Solar power10.7321.7601.397240.8541.7213.794(H) Wind power10.2200.5690.211240.7420.7060.626(H) Electricity10.8203.1681.323241.0303.4036.738(H) SF Bay Area11.1912.6891.779traffic241.1532.3062.558(D) Peyton10.6460.8260.709Manning70.8100.8270.884300.7920.8421.116(D) Bike sharing10.8431.0020.92770.9641.0101.031301.0061.0291.166(D) SF Bay Area10.7120.8401.921traffic70.7980.8472.244300.8140.8742.354(D) Bitcoin11.5573.4381.876transactions71.8163.4701.902302.3923.5542.184(M) Sunspot10.0881.4970.114120.2191.5720.397(M) House supply 10.5771.0800.492120.9391.0121.050Runtime Train Length Silverkite Prophet SARIMAHourly1752030.57142.39173.86Daily29634.7516.937.98Monthly 24283.410.395.46Runtime Horizon Silverkite Prophet SARIMAHourly10.881.390.00Daily10.512.080.00Monthly 10.121.050.00"}], "formulas": [{"formula_id": "formula_1", "formula_text": "( ) = 0 ( ) + \u2211\ufe01 =1 1 { > } ( ( ) \u2212 ( )),", "formula_coordinates": [3.0, 100.61, 462.69, 153.26, 21.5]}, {"formula_id": "formula_2", "formula_text": "( ) = \u2211\ufe01 =1 [ sin(2 ( )) + cos(2 ( ))],", "formula_coordinates": [3.0, 89.97, 577.46, 177.13, 21.5]}, {"formula_id": "formula_3", "formula_text": "( ) = ( ; { , }) + \u2211\ufe01 =1 1 { > } ( ; { , }),", "formula_coordinates": [3.0, 83.95, 689.53, 192.51, 22.16]}, {"formula_id": "formula_4", "formula_text": "( ) \u223c ( ) + ( ),", "formula_coordinates": [4.0, 144.49, 308.21, 67.96, 7.7]}, {"formula_id": "formula_5", "formula_text": "( ) \u223c \u2211\ufe01 \u2208 P ( ),", "formula_coordinates": [4.0, 147.7, 438.85, 61.75, 21.76]}, {"formula_id": "formula_6", "formula_text": "(\u02c6( ) + ( 1 , . . . , )( /2),\u02c6( ) + ( 1 , . . . , )(1 \u2212 /2)).", "formula_coordinates": [4.0, 329.09, 275.0, 218.28, 9.38]}], "doi": "10.1145/3534678.3539165"}