{"title": "Trait-Based Hypothesis Selection For Machine Translation", "authors": "Jacob Devlin; Spyros Matsoukas", "pub_date": "", "abstract": "In the area of machine translation (MT) system combination, previous work on generating input hypotheses has focused on varying a core aspect of the MT system, such as the decoding algorithm or alignment algorithm. In this paper, we propose a new method for generating diverse hypotheses from a single MT system using traits. These traits are simple properties of the MT output such as \"average output length\" and \"average rule length.\" Our method is designed to select hypotheses which vary in trait value but do not significantly degrade in BLEU score. These hypotheses can be combined using standard system combination techniques to produce a 1.2-1.5 BLEU gain on the Arabic-English NIST MT06/MT08 translation task.", "sections": [{"heading": "Introduction", "text": "In Machine Translation (MT), the output from multiple decoding systems can be used to create a new output which is better than any single input system, using a procedure known as system combination.\nNormally, the input systems are generated by varying some important aspect of the MT system, such as the alignment algorithm (Xu and Rosti, 2010) or tokenization algorithm (de Gispert et al., 2009). Unfortunately, creating novel algorithms to perform some important aspect of MT decoding is obviously quite challenging. Thus, it is difficult to increase the number of input systems in a meaningful way.\nIn this paper, we show it is possible to create diverse input hypotheses for combination without making any algorithmic changes. Instead, we use traits, which are very simple attributes of the MT output, such as \"output length\" and \"average rule length.\" Our basic procedure is to intelligently select hypotheses from our decoding forest which vary in trait value, but have minimal BLEU degradation compared to our baseline. We then combine these to produce a substantial gain. Note that all of the hypotheses are generated from a single decode of a single input system. Additionally, our method is completely compatible with multi-system combination, since our procedure can be applied to each input system, and then these systems can be combined as normal.\nMethods for automatically creating diverse hypotheses from a single system have been explored in speech recognition (Siohan et al., 2005), but we know of no analogous work applied to machine translation. Our procedure does share some surface similarities with techniques such as variational decoding (VD) (Li et al., 2009), but the goal in those techniques is to find output which is consistent with the entire forest, rather than to select hypotheses with particular attributes. In fact, VD can be applied in conjunction by running VD on the rescored forest for each trait condition. 1", "publication_ref": ["b9", "b2", "b8", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "Description of MT System", "text": "Our machine translation system is a string-todependency hierarchical decoder based on (Shen et al., 2008) and (Chiang, 2007). Bottom-up chart parsing is performed to produce a shared forest of derivations. The decoder uses a log-linear translation model, so the score of derivation d is defined as:\nS d ( w) = m i=1 w i r\u2208R(d) F ri (1)\nwhere R(d) is the set of translation rules that make up derivation d, m is the number of features, F ri is the score of the i th feature in rule r, and w i is the weight of feature i. This weight vector is optimized discriminatively to maximize BLEU score on a tuning set, using the Expected-BLEU optimization procedure .\nOur decoder uses all of the standard statistical MT features, such as the language model, rule probabilities, and lexical probabilities. Additionally, we use 50,000 sparse, binary-valued features such as \"Is the bi-gram 'united states' present in the output?\", based on (Chiang et al., 2009). We use a 3-gram LM for decoding and a 5-gram LM for rescoring.", "publication_ref": ["b7", "b1", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Trait Features", "text": "An MT trait represents a high-level property of the MT output.\nThe traits used in this paper are:\n\u2022 Null Source Words -The percentage of source content words which align to null, i.e., are not translated. \u2022 Source Reorder -The percentage of source terminals/non-terminals which cross alignment links inside their decoding rule. \u2022 Ngram Frequency -The percentage of target 3grams which are seen more than 10 times in the monolingual training. \u2022 Rule Frequency -The percentage of rules which are seen more than 3 times in the parallel training.\n\u2022 Rule Length -The average number of target words per rule. \u2022 Output Length -The ratio of the number of target words in the MT output divided by the number of source words in the input. \u2022 High Lex Prob -The percentage of source words which have a lexical translation probability greater than 0.1.\nEach trait can be represented as the ratio of two linear decoding features. For example, for the Output Length trait, the \"numerator\" feature is the number of target words in the hypothesis, while the \"denominator\" feature is the number of source words in the input sentence. We can sum these feature scores over a test set, and the resulting quotient is the Output Length for that set.\nIntuitively, each trait is associated with a particular tradeoff, such as fluency/adequacy or precision/recall. For example, when MT performance is maximized, shorter output tends to have higher precision but lower recall than longer output. For the Ngram Frequency trait, a greater percentage of highfrequency n-grams tends to result in more fluent but less adequate output. Similar intuitive justifications should be evident for the remaining traits.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Hypothesis Generation", "text": "The main goal of this work is to generate additional hypotheses which vary in trait values, while minimizing degradation to the BLEU score. So, imagine that we have some baseline MT output. Then, we want to generate a second set of hypotheses which have maximal BLEU score, subject to the constraint that the output must be 5% shorter. 2 The question then becomes how to figure out which 5% of words should be removed. Rather than attempting to do this with a new algorithm, we simply let our existing MT models do it for us, using our standard optimization procedure. This is the essential purpose of the trait features -using the Output Length feature, the optimizer has a \"knob\" with which it can control the trait value independently of everything else. 3 Thus, the new hypotheses that we select are \"optimal\" in terms of our existing MT model probabilities, but have trait values which vary from the baseline in a precise way.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimization Function", "text": "Our normal optimization procedure uses nbest-based Expected-BLEU tuning , which is a differentiable approximation of Maximum-BLEU tuning. To \"target\" a particular trait value, we add a second term equal to the squared error between the current trait value and the target trait value. Our modified optimization function which we seek to maximize is then:\nObj( w) = ExpBLEU ( w) \u2212 \u03b1 N ( w) D( w) \u2212 \u03c4 \u03b3 2\nwhere w is the MT feature weight vector, \u03b1 is the weight of the trait term, \u03b3 is the baseline value of the trait, and \u03c4 is the \"target\" trait multiplier, N ( w) is the expected-value of the numerator feature, and D( w) is the expected value of the denominator feature.\nTo give an example, imagine that for our baseline tune set the Output Length ratio is 1.2, and we want to create a hypothesis set with 5% fewer words. In that case, we would set \u03b3 = 1.2 and \u03c4 = 0.95, so the target trait value is 1.14. We fix the free parameter \u03b1 to 10, which forces the optimized trait value to be very close to the target. 4 The trait-value functions N ( w) and D( w) are computed as standard expected value functions, e.g.:\nN ( w) = i j p ij ( w)N ij\nwhere p ij ( w) is the posterior probability of the j th hypothesis of sentence i, and N ij is the value of the numerator feature for hypothesis ij. 5", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Meta-Optimization", "text": "It is somewhat problematic to use a fixed multiplier \u03c4 on all of the traits, since on some traits it may cause a larger degradation than others. So, we take the reverse approach -for some targeted BLEU loss traits.\n\u03b2, we find the maximum (or minimum) value of \u03c4 which causes a loss no greater than \u03b2, as computed on a held-out portion of the tune set. 6 Here, we find the maximum and minimum trait value for \u03b2 = 0.5 and \u03b2 = 2.0, resulting in 4 sets of weights per trait. We can find the optimal \u03c4 for each \u03b2 by performing a binary search on \u03c4 , where we run our optimization procedure and then compute the BLEU loss at each iteration.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Forest-Based Optimization", "text": "Since we have 7 traits, and we generate 4 sets of weights per trait, we have 28 \"systems\" to combine. Obviously, running 28 full decodes on each new test sentence is highly undesirable.\nWe resolve this issue by using our baseline derivation forest for both optimization and hypothesis generation. We perform a single round of decoding to generate a forest, and then perform iterative n-best optimization by rescoring the forest rather than redecoding from scratch. 7 We constrain the 50,000 sparse feature weights to be fixed at their baseline values, to prevent over-fitting.\nOnce the weight sets are generated, the hypotheses for each trait condition can be generated by rescoring the forest inside of the decoder. Therefore, all 28 trait hypotheses can be generated for almost no cost over a single decode.\nIt should be noted we have found it beneficial to relax our MT pruning parameters in order to create a larger forest. This results in decoding which is roughly 2x-3x as slow as the baseline, and requires storing the larger forest in memory. However, we have found that the procedure still works well even with the standard pruning parameters. Additionally, we are investigating methods for diversifying the forest with less of a slowdown to decoding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Combination", "text": "Once the different trait hypotheses have been generated, system combination can be performed using any method.\nHere, we use a confusion network decoder based on . The basic procedure is to select one hypothesis as the \"skeleton\" and then incrementally align the remaining hypotheses to create a confusion network. The confusion network is decoded using an arc-level confidence score for each input system and a language model, the weights for which are estimated discriminatively to maximize BLEU.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We present MT results in Table 1. Our experimental setup is compatible with the NIST MT08 constrained track. We trained our translation model on 35 million words of parallel data and our language model on 3.8 billion words of monolingual data. We use a portion of MT02-05 for tuning the MT baseline and the trait systems, and another portion of MT02-05 for tuning system combination.\nWe present results on Arabic-English MT06newswire and MT08-eval. The systems were tuned and evaluated using IBM-BLEU. Our baseline system is 1.5 BLEU better than the best result from the NIST M08 evaluation.\nFor the Trait Feats condition, we simply added the numerator and denominator features for all 7 traits to the baseline system and re-optimized. 8 Somewhat surprisingly, this produces an 0.5-0.7 BLEU gain on its own. In this condition, although we do not target any particular trait values, the optimizer will naturally fine-tune the trait values to whatever is optimal for BLEU score. For example, the MT08 baseline value of Source Reorder is 0.307, while for the Trait Feats it is 0.330, so the system determined it is \"optimal\" to have 7.5% (0.330/0.307) more re-ordering than the baseline.\nFor the Trait Comb condition, we generated 28 trait hypothesis sets using the decoding forest from the Trait Feats condition. We combined these with the Trait Feats output using consensus network decoding. This produces an additional 0.8 BLEU gain, resulting in a 1.2-1.5 BLEU gain over the baseline.\nWe also present another condition, n-best Comb, where we perform confusion network combination on the 28-best hypotheses from Trait Feats. This represents the simplest and most trivial method of hypothesis selection. We observe no gain in BLEU on this condition. Other simple methods of hy-8 Including the 50k sparse features.\npotheses selection, such as optimizing systems to be \"different\" from one another (i.e., have high intersystem TER), also produced no gain over the single system. We include these results simply to demonstrate that it is not trivial to select hypotheses from a single system which produce a significant improvement in from system combination.   (Koehn, 2004). * * = Significant improvement at 99.9% confidence. BLEU = IBM-BLEU score. Len = Hypothesisto-reference length ratio.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Conclusions and Future Work", "text": "We demonstrated a method of intelligently selecting hypotheses from a decoding forest which can be combined with the baseline hypotheses to produce a significant gain in BLEU score. In the future, we plan to explore more trait types and alternate methods of system combination.\nOne possible application of this work is in fielded translation systems. Because our method produces high-quality complementary hypotheses at a low computational cost, the system could present these to the user as alternate translations. Going further, a user could prefer a particular output type, such as the fluency-tuned condition, and set that to be their default translation.\nThe major open question is how our trait-based combination interacts with multi-system combination. Imagine there are three different types of decoders which can be combined to produce some gain in the baseline condition. If you independently improve all three using trait-based combination, will the relative gain from multi-system combination be reduced? Or can you jointly combine all of the trait hypotheses and get an even greater relative gain? We plan to thoroughly explore this in the future.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "11,001 new features for statistical machine translation", "journal": "", "year": "2009", "authors": "D Chiang; K Knight; W Wang"}, {"ref_id": "b1", "title": "Hierarchical phrase-based translation", "journal": "Computational Linguistics", "year": "2007", "authors": "D Chiang"}, {"ref_id": "b2", "title": "Minimum Bayes risk combination of translation hypotheses from alternative morphological decompositions", "journal": "", "year": "2009", "authors": "A De Gispert; S Virpioja; M Kurimo; W Byrne"}, {"ref_id": "b3", "title": "Pharaoh: A beam search decoder for phrase-based statistical machine translation models", "journal": "", "year": "2004", "authors": "P Koehn"}, {"ref_id": "b4", "title": "Variational decoding for statistical machine translation", "journal": "", "year": "2009", "authors": "Z Li; J Eisner; S Khudanpur"}, {"ref_id": "b5", "title": "Consensus training for consensus decoding in machine translation", "journal": "", "year": "2009", "authors": "A Pauls; J Denero; D Klein"}, {"ref_id": "b6", "title": "BBN system description for WMT10 system combination task", "journal": "", "year": "2010", "authors": "A Rosti; B Zhang; S Matsoukas; R Schwartz"}, {"ref_id": "b7", "title": "A new stringto-dependency machine translation algorithm with a target dependency language model", "journal": "", "year": "2008", "authors": "L Shen; J Xu; R Weischedel"}, {"ref_id": "b8", "title": "Constructing ensembles of ASR systems using randomized decision trees", "journal": "", "year": "2005", "authors": "O Siohan; B Ramabhadran; B Kingsbury"}, {"ref_id": "b9", "title": "Combining unsupervised and supervised alignments for MT: An empirical study", "journal": "", "year": "2010", "authors": "J Xu; A Rosti"}], "figures": [{"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Results on Arabic-English MT.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "S d ( w) = m i=1 w i r\u2208R(d) F ri (1)", "formula_coordinates": [2.0, 114.84, 205.08, 180.98, 32.56]}, {"formula_id": "formula_1", "formula_text": "Obj( w) = ExpBLEU ( w) \u2212 \u03b1 N ( w) D( w) \u2212 \u03c4 \u03b3 2", "formula_coordinates": [3.0, 74.32, 241.82, 215.71, 27.45]}, {"formula_id": "formula_2", "formula_text": "N ( w) = i j p ij ( w)N ij", "formula_coordinates": [3.0, 113.02, 494.41, 117.92, 21.17]}], "doi": ""}