{"title": "CowClip: Reducing CTR Prediction Model Training Time from 12 hours to 10 minutes on 1 GPU", "authors": "Zangwei Zheng; Pengtai Xu; Xuan Zou; Da Tang; Zhen Li; Chenguang Xi; Peng Wu; Leqi Zou; Yijie Zhu; Ming Chen; Xiangzhuo Ding; Fuzhao Xue; Ziheng Qin; Youlong Cheng; Yang You", "pub_date": "", "abstract": "The click-through rate (CTR) prediction task is to predict whether a user will click on the recommended item. As mindboggling amounts of data are produced online daily, accelerating CTR prediction model training is critical to ensuring an up-to-date model and reducing the training cost. One approach to increase the training speed is to apply large batch training. However, as shown in computer vision and natural language processing tasks, training with a large batch easily suffers from the loss of accuracy. Our experiments show that previous scaling rules fail in the training of CTR prediction neural networks. To tackle this problem, we first theoretically show that different frequencies of ids make it challenging to scale hyperparameters when scaling the batch size. To stabilize the training process in a large batch size setting, we develop the adaptive Column-wise Clipping (CowClip). It enables an easy and effective scaling rule for the embeddings, which keeps the learning rate unchanged and scales the L2 loss. We conduct extensive experiments with four CTR prediction networks on two real-world datasets and successfully scaled 128 times the original batch size without accuracy loss. In particular, for CTR prediction model DeepFM training on the Criteo dataset, our optimization framework enlarges the batch size from 1K to 128K with over 0.1% AUC improvement and reduces training time from 12 hours to 10 minutes on a single V100 GPU. Our code locates at github.com/bytedance/LargeBatchCTR.", "sections": [{"heading": "Introduction", "text": "With the development of the Internet and the e-economy, numerous clicking happens in online shopping (Ma et al. 2020;Zhou et al. 2019), video apps (Gomez-Uribe and Hunt 2016; Xie et al. 2020) and web advertisements (Covington, Adams, and Sargin 2016;Zhao et al. 2019). Click-through Rate (CTR) prediction is to predict whether a user will click on the recommended item. It is a fundamental task in advertising and recommendation systems. An accurate CTR prediction can directly improve user experience (Kaasinen et al. * Work done during an internship at Bytedance. \u2020 Yang You is the corresponding author.  2009) and enhance ads profit (Wang 2020).\nIn a typical industrial dataset, the number of click samples has grown up to hundreds of billion (Zhao et al. 2019;Xie et al. 2020) and keeps increasing on a daily basis. The clickthrough rate (CTR) prediction task is to predict whether a user will click on the recommended item. It is a fundamental task in advertising and recommendation systems. Since CTR prediction is a time-sensitive task (Zhao et al. 2019) (e.g., latest topics, hottest videos, and new users' hobbies), it is necessary to shorten the time needed for re-training on a massive dataset to maintain an up-to-date CTR prediction model. In addition, given a constant computing budget, decreasing the training time also reduces the training cost, giving rise to a high return-to-investment ratio.\nRecent years have witnessed rapid growth in GPU processing ability (Baji 2018). With the growth of GPU memory and FLOPS, a larger batch size can take better advantage of the parallel processing capability of GPUs. As shown in Figure 1 (a), the time of one forward and backward pass is almost the same when scaling 8 times batch size, indicating GPU with a small batch size is extremely underused. Since the number of training epochs remains the same, large batch training reduces the number of steps and thus significantly shortens the total training time (Figure 1 (b)). In addition, a large batch benefits more in a multi-GPUs setting, where gradients of the large embedding layer need to be exchanged between different GPUs and machines, resulting in arXiv:2204.06240v3 [cs.LG] 30 Nov 2022   high communication costs. To avoid distraction from system optimization in reducing communication costs (Mudigere et al. 2021;Zhao et al. 2019;Xie et al. 2020), we focus on designing an accuracy-preserving algorithm for scaling batch size on a single GPU, which can be easily extended for multi-node training. The challenge of applying large batch training is an accuracy loss when naively increasing the batch size (He et al. 2021), especially considering that CTR prediction is a very sensitive task and cannot bear the accuracy loss. Hyperparameter scaling rules (Krizhevsky 2014;Goyal et al. 2017) and carefully designed optimization methods (You, Gitman, and Ginsburg 2017;You et al. 2020) in CV and NLP tasks are not directly suitable for CTR prediction. This is because, in CTR prediction, the inputs are more sparse and frequency-unbalanced, and the embedding layers dominate the parameters of the whole network (e.g., 99.9%, see Table 1). In this paper, we identified the failure reason behind previous scaling rules on CTR prediction and proposed an effective algorithm and scaling rule for large batch training.\nIn conclusion, our contributions are as follows: \u2022 To the best of our knowledge, we are the first to investigate the stability of the training CTR prediction model in very large batch sizes. We attribute the hardship in scaling the batch size to the difference in id frequencies. \u2022 With rigorous mathematical analysis, we prove that the learning rate for infrequent features should not be scaled when scaling up the batch size. With CowClip, we can adopt an easy and effective scaling strategy for scaling up the batch size. \u2022 We propose an effective optimization method of adaptive Column-wise Clipping (CowClip) to stabilize the training process of the CTR prediction task. We successfully scale up 128 times batch size for four models on two public datasets. In particular, we train the DeepFM model with 72 times speedup and 0.1% AUC improvement on the Criteo dataset. ", "publication_ref": ["b29", "b55", "b46", "b11", "b54", "b43", "b54", "b46", "b54", "b6", "b34", "b54", "b46", "b20", "b23", "b17", "b47"], "figure_ref": ["fig_1", "fig_1"], "table_ref": ["tab_0"]}, {"heading": "Related Work", "text": "Embeddings of CTR Prediction model The input of the CTR prediction model is high-dimensional, sparse, and frequent-unbalanced. As we will discuss the frequency in the Section 3, we focus on the fact that the input feature space for CTR prediction is high-dimensional and sparse, which is an essential difference between the CTR prediction model and other deep learning models.\nA typical industrial CTR prediction model (Zhao et al. 2019;Xie et al. 2020;Zhou et al. 2019) has a highdimensional input space with 10 8 to 10 12 dimensions after one-hot encoding of the categorical features. At the same time, a single clicking log may contain only hundreds of non-zero entries. As a result, when we create the embedding for each feature, the whole embedding layer can be extremely large, and the parameters of the CTR prediction model are dominated (e.g., 99.9%) by the embedding part instead of the deep network part (Miao et al. 2021;Ginart et al. 2021). Table 1 shows the case under our experimental setting.\nAs the number of parameters in the embedding layer overwhelms the one of the dense networks, the difficulty of large batch optimization lies in the embedding layers. This paper focuses on addressing the training instability caused by the properties of embedding layers in the CTR prediction model. No matter how the dense part, e.g., MLP, LSTM (Chen and Li 2021), Transformer (Chen et al. 2019), changes, the training instability caused by the embedding part still exists.\nCTR prediction network A thread of work started from (Cheng et al. 2016;Wang et al. 2017) occupies a majority of the above networks. They focused on designing a two-stream network, as shown in Figure 6. Following W&D model (Cheng et al. 2016), there are many designs on the wide/cross-stream. The details of DeepFM (Guo et al. 2018), W&D, DCN (Wang et al. 2017), andDCN-v2 (Wang et al. 2021) used in our experiments are presented in the Appendix .\nLarge batch training methods. To preserve the performance of deep models at a large batch size, we need a good scaling rule and a stable optimization strategy. The scaling rule tells us how to scale the hyperparameters when scaling up the batch size. The two most important hyperparameters  (Goyal et al. 2017) and square root scaling (Krizhevsky 2014;Hoffer, Hubara, and Soudry 2017) are the two most common scaling rules in the deep learning community. Besides, optimization strategies such as warmup (Gotmare et al. 2019) and gradient clipping (Zhang et al. 2020) can help stabilize the large batch training process. LARS (You, Gitman, and Ginsburg 2017) and LAMB (You et al. 2020) are two optimizers designed for large batch training, which adopt different adaptive learning rates for each layer. Although they achieve good results in CV and NLP tasks, they are ineffective in the CTR prediction task because it is unnecessary to use a layer-wise optimizer with a shallow network (e.g., three or four layers). This paper re-designs the scaling rule and optimization strategy for the embedding layer, which can successfully scale up the batch size for CTR prediction.\nAdditional related work can be found in Appendix , including sensitiveness of CTR prediction and works utilizing different frequencies.", "publication_ref": ["b54", "b46", "b55", "b33", "b14", "b8", "b9", "b10", "b41", "b10", "b18", "b41", "b17", "b23", "b21", "b16", "b50", "b47"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Method", "text": "In CTR prediction, we have the training dataset D = {x i , y i } N i=1 , where y \u2208 {0, 1} denotes whether the user clicked or not. The x contains information about the user, the product, and the interaction, which can be categorical or continuous. The categorical field is one-hot encoded to be a vector x fj i of d fj length, where d fj is the number of possible values (ids) in this field. To represent the frequency of each id, we denote the k-th id in field j as id fj k . The frequency and occurrence probability of the id is:\ncount(id fj k ) = N i=1 \u03b4(x fj i [k] = 1), P(id fj k \u2208 x) = count(id fj k ) N ,\nwhere \u03b4(\u2022) equals 1 if the boolean condition holds and 0 otherwise. Given the predicting network f , the prediction is made from f (x). The network and embeddings weights are denoted as w, and the training loss is L. This paper focuses on the Wide/Cross-and-Deep kind of CTR prediction model, as briefly described in Figure 2, one of the state-of-the-art networks in CTR prediction Zhang, Huang, and Zhang 2019).\nIn training the network, we use a batch size of b = |B|, where B is a specific batch. The learning rate and L2regularization weight are denoted as \u03b7 and \u03bb. The total number of steps in an epoch is N b .", "publication_ref": ["b51"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Failure cause of traditional scaling rules", "text": "When training a neural network, at step t, an optimizer Opt(\u2022) takes in the weights and gradients, and output the updated weights. With the L2-regularization, the update process can be formulated as:\ng t = x\u2208Bt \u2207L(w, x) + \u03bb 2 \u2022 w 2 2 w t+1 = \u03b7 \u2022 Opt(w t , g t ).\nWhen changing the batch size, the hyper-parameter learning rate \u03b7 and L2-regularization weight \u03bb should be adjusted for maintaining the same performance as the original batch size. Square root scaling (Krizhevsky 2014;Hoffer, Hubara, and Soudry 2017) and linear scaling (Goyal et al. 2017) are two widely used scaling rules in deep learning. The motivation for sqrt scaling is to keep the covariance matrix of the parameters update the same, while for linear scaling, the motivation is to keep the update in a large batch equal to updates from s small batches when scaling s times the batch size (details in Appendix ). The two scaling rules have been shown effective in CV and NLP tasks, and they are shown as follows: Scaling Rule 1 (Sqrt Scaling) When scaling batch size from b to s \u2022 b, do as follows:\n\u03b7 \u2192 \u221a s \u2022 \u03b7, \u03bb \u2192 \u221a s \u2022 \u03bb Scaling Rule 2 (Linear Scaling) When scaling batch size from b to s \u2022 b, do as follows: \u03b7 \u2192 s \u2022 \u03b7, \u03bb \u2192 \u03bb\nOur first attempt at large batch training of the CTR prediction model is to apply the above classic scaling rules: no scaling, linear scaling (Goyal et al. 2017), and square root scaling (Krizhevsky 2014). However, as seen in experiments on the Criteo dataset with DeepFM model in Table 2 left, the above rules fail in a CTR prediction model. We claim that the reason for the failure lies in the different frequencies of ids.\nThe product id of a popular item or ids in fields with a few options (e.g., male and female in gender field) are frequent, while the id of an inactive user seldom appears. In Figure 4, we visualize the distribution of different ids' frequencies in three fields. The exponential distribution reveals different frequencies among different ids. For the dense weights (e.g., kernel weights), their gradients appear for each sample while embedding does not have gradients if the corresponding ids do not show up. In CTR prediction, the embedding layers dominate the parameters of the whole network, and different occurrences of gradients make a great difference from other deep neural networks.\nFirst, we empirically verify our claim by the following experiment. We keep the top three frequent ids in each field and label the rest as a fourth id. In this way, all four ids are very frequent and variations in frequencies are ablated in this modified version of Criteo. As shown in Table 2 right, both scaling rules successfully apply to the modified dataset, which means the traditional scaling rule does not work in CTR prediction due to the presence of infrequent ids.\nNext, we provide the theoretical analysis for the failure of sqrt and linear scaling. Different frequencies lead to varying occurrences of ids in batches. Only when the id appears in the batch can the corresponding embedding be updated. They only occur in a small fraction of batches for ids with a low frequency. Suppose we draw the training samples with replacement from the dataset, the probability of an id id fj k in the batch B is:\nP(id fj k \u2208 B) = 1 \u2212 (1 \u2212 P(id fj k \u2208 x)) b .\nFor frequent ids, and also dense weights whose frequency rate is 1, we have (1 \u2212 P(id fj k \u2208 x)) b \u2248 0; while for the infrequent ids, when p 1 B , we can use binomial approximation and obtain:\nP(id fj k \u2208 B) \u2248 1 id fj k is frequent b \u2022 P(id fj k \u2208 x) id fj k is infrequent . (1)\nNow, reconsider the linear scaling motivation for an id's embedding w. Denote the weight update as \u2206w = w t \u2212 w t+1 . Consider the expected update in a large batch B =\ns i=1 B i with b = |B | = s \u2022 b, we have E[\u2206w] = E[\u03b7 \u2022 \u03b4(id fj k \u2208 B ) \u2022 1 b x\u2208B \u2207L(w, x)] = \u03b7 \u2022 P(id fj k \u2208 B ) \u2022 E[\u2207L(w, x)].\nWith the assumption that E[\u2207L(w i , x)] \u2248 E[\u2207L(w, x)], the expected update in small batches B i is:\nE[\u2206w] = E[\u03b7 \u2022 s i=1 \u03b4(id fj k \u2208 B i ) \u2022 1 b x\u2208Bi \u2207L(w i , x)] \u2248 \u03b7 \u2022 s \u2022 P(id fj k \u2208 B) \u2022 E[\u2207L(w, x)].\nFor dense weight or embeddings of frequent id, the term P(id fj k \u2208 B) equals 1, making no difference to the original linear scaling rule. However, with an infrequent id, it shows that the new scaling strategy should be using the same learning rate when scaling the batch size due to the following fact for infrequent ids:\nP(id fj k \u2208 B ) \u2248 s \u2022 P(id fj k \u2208 B).\nA similar discussion based on sqrt scaling motivation (see Appendix ) shows that under a very strong assumption can we obtain the same conclusion. However, without the assumption, we cannot even choose hyperparameters maintaining the same covariance matrix after scaling the batch size.\nWhen using a relatively small batch size we find most ids satisfied p < 1 B . Thus, we propose to use no scaling on the whole embedding layers, which suits infrequent ids. In addition, a smaller learning rate for layers at the bottom leads to a smooth learning process. Experiments show this scaling rule leads to a better result.\nAfter the discussion of learning rate scaling, now let's turn to the L2-regularization weight \u03bb. In CTR prediction, an unsuitable \u03bb can easily lead to overfitting. For the scaling of \u03bb, we first consider the embedding vector w of id fj k , the expected gradient of which in a batch is:\nE[g] = 1 b E[\u03b4(id fj k \u2208 B)\nx\u2208B \u2207L(w, x)]\n= P(id fj k \u2208 B) \u2022 E[\u2207L(w, x)].(2)\nThe term P(id fj k \u2208 B) still has no effect with dense weight and embeddings of frequent ids as the probability equals to 1. However, for the infrequent ids, there is a scaling multiplier before the expectation of the gradient as some ids may not appear in a certain batch. When using an adaptive optimizer such as Adam, this scaling multiplier results in a different behaviour, which is equivalent to adjusting the L2regularization weight \u03bb as follows (see Appendix for the proof): \u03bb\nP(id fj k \u2208 B) = \u03bb b \u2022 P(id fj k \u2208 x)\n.\nThus, to maintain the same L2-regularization strength, we scale up the \u03bb by n. Combined with the learning rate scaling rule, we have the following one. However, we find directly applying the above rule leads to overfitting due to s times less application of L2regularization when scaling up batch size by s times. If no additional regularization technique is introduced, L2regularization should be strengthened further with a large batch size. In the case of an SGD optimizer, we have (details in Appendix ):\n\u03b7 \u03bb \u2248 s\u03b7\u03bb.\nHence, we need to further scale up the \u03bb by s times when the learning rate is unchanged. Although the behavior of adaptive optimizers such as Adam is different from SGD, we find a larger \u03bb prevents overfitting. Thus, we have the following scaling rule which can scale up the batch size to 4K without additional optimization strategy. Scaling Rule 4 (n 2 -\u03bb Scaling) When scaling batch size from b to s \u2022 b, use sqrt scaling for the dense weights, and do as follows for embeddings:\n\u03b7 e \u2192 \u03b7 e , \u03bb e \u2192 s 2 \u2022 \u03bb e", "publication_ref": ["b23", "b21", "b17", "b17", "b23"], "figure_ref": ["fig_5"], "table_ref": ["tab_1", "tab_1"]}, {"heading": "CowClip algorithm", "text": "Although the Scaling Rule 4 helps us scale to 4 times the original batch size, it fails on a larger batch size. The challenge in choosing the proper learning rate \u03b7 and L2-weight \u03bb mentioned above impairs the performance for larger batch sizes. To enable large batch training, the gradient norm clipping (Zhang et al. 2020) can smooth the process of training and alleviate the sensitiveness of hyperparameters. Given a clip threshold clip t, gradient norm clipping does follows:\ng \u2192 min{1, clip t g } \u2022 g\nGradient norm clipping smoothes the training process by reducing the norm of a large gradient greater than a threshold. However, it is hard to choose an appropriate threshold for clipping the norm. Besides, as one column of the embedding matrix represents the embedding vector for an id, Figure 5 shows that the magnitude of gradients for different columns varies. We denote the column for an id as w[id\nfj k ].\nClipping on the whole embeddings whose gradient norm is dominated by gradients of columns with large gradients impairs the ones with normal but smaller gradients. end for 16: end for In addition, according to Equation (2), since we want to clip on 1 \u2022 \u2207L(w, x), the different frequencies of ids lead to the scaler of P(id fj k \u2208 B) on the expected gradients. To tackle the above problems, inspired by LAMB optimizer (You et al. 2020), which normalizes the norm of gradients of each kernel to be proportional to the norm of the kernel weight, we relate the clip threshold with the norm of id embedding vectors. The difference between our clipping method and the gradient norm clipping is three-fold: First, every id embedding vector has a unique clipping threshold for more flexible clipping. Second, the clipping threshold is multiplied by the occurrence number of the id to make sure the bound is based on 1\u2022\u2207L(w, x). Last but not the least, the clipping threshold is calculated by the norm of the id vector in consideration of different magnitudes:\nclip(id fj k ) = cnt(id fj k ) \u2022 max{r \u2022 w e t [id fj k ] , \u03b6}\nwhere cnt(id fj k ) is the number of occurence of the id in a batch.\nAs the weights grow larger in the training process, the benefit of a threshold proportional to the norm of the weight is that the clipping value adaptively grows with the network. As some infrequent id embedding vectors become too small due to the continual application of L2-regularization with no id occurrence in steps, we restrict the clipping norm by a lower-bound \u03b6 to avoid a too strong clipping.\nThe network training with CowClip is summarized in the Algorithm 1. In practice, tensor multiplication instead of forloop is adopted for less computational overhead. Since Cow-Clip stabilizes the training process, it is possible to use the CowClip scaling 3 rule to 128\u00d7 batch size, leaving \u03b7 e unchanged and linear scaling the \u03bb. We give a proof sketch on the convergence of CowClip method in Appendix . Our large batch training framework contains the CowClip gradient clipping and scaling strategy.   (Mattson et al. 2020) in CTR prediction: AUC (Area Under ROC) and Logloss (Logistic loss). Our implementation is based on Tensorflow (Abadi et al. 2015) and DeepCTR (Shen 2017) framework. The experiments are conducted on one Tesla V100 GPU. We use Adam (Kingma and Ba 2015) optimizer and an L2-regularization on embedding layers. The base learning rate and L2-regularization weight on batch size 1024 are 10 \u22124 and 10 \u22125 . Scaling rules are performed based on the 1024 batch size. For Cow-Clip, we use r = 1 and tune \u03b6 \u2208 {10 \u22125 , 10 \u22124 } due to a different initialization weight norm. We also use learning rate warmup (Gotmare et al. 2019) and larger initialization weights. More discussion on hyperparameter choice and techniques can be found in the Appendix . We run our experiments with three random seeds, and the standard deviation among all experiments for AUC is less than 0.012%. ", "publication_ref": ["b50", "b0", "b38", "b22", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Large batch training results", "text": "First, as shown in Table 3, previous scaling strategy fails to maintain the performance at batch size 8K, and fails to converge at batch size 128K. In contrast, CowClip methods can achieve a better accuracy at 8K batch size and almost no performance loss at batch size 128K, which shows the CowClip method successfully stablizes the training processes.\nThen, we compare different scaling strategies on the DeepFM model. The results on Criteo dataset are presented in Table 4, and the results on Criteo-seq and Avazu are attached in Appendix. As we can see, traditional scaling rules fail to meet the AUC requirement with a large gap when the batch size grows up to 4K. This is consistent with results in (Guo et al. 2018), where results with 4K batch size are worse than those with 1K. With n 2 -\u03bb Scaling rule 4, it can  scale batch size to 4K but fails with 8K. When we successfully scale the batch size, there is a performance gain in the AUC, which is also observed in (Zhu et al. 2021). For our CowClip algorithm, it outperforms the original optimization method by 0.1% AUC on the Criteo dataset at a small batch size. When scaling to a large batch size in Table 5 and Table 12, instead of AUC loss, our algorithm achieves a further performance gain of about 0.1% in the Criteo. For Criteoseq and Avazu, CowClip can scale up to 128\u00d7 and 64\u00d7 batch size without performance loss respectively. Equipped with CowClip, it can scale all four models to a large batch size with performance improvement, as shown in Table 5 and Table 12. This shows that CowClip is a model-agnostic optmization technique.\nTraining with large batches can significantly reduce the training time. As shown in Table 6 for the Criteo dataset and Table 13 in Appendix for the Avazu dataset, the speedup achieved by scaling up the batch size is almost linear when the batch size is under 16K. We can still accomplish a sublinear speedup when continuing to scale up the batch size and achieve a 76.8\u00d7 speed up with 128K batch size on the Criteo dataset. The compared four methods take advantage of different system optimization, such as reducing the communication and computational cost, so they achieve a much faster training speed with a 1K batch size. However, these methods have a low AUC performance and can only scale up to a 4K batch size due to performance loss in a larger batch size. Besides, they scale the batch size by using more GPUs, with 2 and 4 GPUs for 2K and 4K batch sizes, resulting in 2\u00d7 and 4\u00d7 cost in the GPU hours. In contrast, our method takes the advantage of large batch training, which can achieve a much shorter training time within only one GPU resource and obtain a higher AUC score. ", "publication_ref": ["b18", "b56"], "figure_ref": [], "table_ref": ["tab_3", "tab_4", "tab_5", "tab_5", "tab_0", "tab_6", "tab_0"]}, {"heading": "Appendix More Related Work", "text": "Details on Wide/Cross stream of CTR prediction models Deep learning has been widely used by the community to boost CTR prediction performance. CTR prediction networks (Zhang, Du, and Wang 2016;Cheng et al. 2016;Qu et al. 2016;Guo et al. 2018;Wang et al. 2017;Lian et al. 2018;Song et al. 2019;Li et al. 2019;Deng et al. 2021;Wang et al. , 2020Wang, She, and Zhang 2021;Lyu et al. 2021;Zhang, Huang, and Zhang 2019; have outperformed traditional methods such as Logistic Regression (LR) (McMahan et al. 2013) and Factorization Machine (FM) (Rendle 2010). The progress of AUC of CTR prediction models on the Criteo dataset is shown in Figure 3.\nHere we illustrate the architecture of models used in our experiment. After one-hot encoding of every categorical field, the input is embedded into a dense vector in the network. The wide or cross-stream serves to model the feature interactions explicitly. For instance, the LR model is a first-order predictor, FM models the second-order interactions, and n cross layers (Wang et al. 2017) models n-th-order interactions. The first two methods are called \"wide\" for only one layer is used. The other stream is a feed-forward neural network to compensate for the ability to learn higher-order interactions.\nThe DeepFM (Guo et al. 2018) adopts a factorization machine for the wide part, which is also the cross-stream for it explicitly models the second-order relationship between different features. After embedding each column into a d-dimensional vector v, the factorization machine models\u0177 as follows:\ny = w 0 + n i=1 w i x i + n i=1 n j=i+1 v i , v j x i x j .\nFor the W&D model, the wide part is logistic regression. Denote the output of the network as\u0177, the predicted probability of clicking is obtained by sigmoid(\u0177). After one-hot encoding of input x, the vector is a n-dimensional x. The logistic regression considers each feature independently and lets the DNN captures high-order relations in the data.\ny = w 0 + n i=1 w i x i .\nFor DCN (Wang et al. 2017), it introduces the cross-layer to automatically learns a high-order interaction. With L cross layers, it can model interactions from 2 to L + 1 order. Denote the x 0 as input vector and x as the output of the -th layer. The -th cross-layer does as follows:\nx +1 = x 0 x w + b + x . The DCN-v2  proposed a new cross-layer to model high-order interaction. Specifically, with W \u2208 R d\u00d7d , the -th layer is:\nx +1 = x 0 (W x + b ) + x .\nSensitiveness of CTR prediction. Preserving the CTR prediction model's performance with a large batch size is a great challenge in that the CTR model is very sensitive. In accelerating the training of ResNet-50, an accuracy loss within 1% is tolerable (You, Gitman, and Ginsburg 2017;You et al. 2020;Kumar et al. 2019). However, considering the tremendous amounts of clicking happening every day, a 0.1% loss in AUC will cost a company too much to bear. As shown in Figure 3, a continuous effort to develop new CTR prediction models only improved the AUC by less than 2% on the Criteo (Labs 2014) dataset in the past six years., and an improvement in a month of 0.02% is considered significant in Criteo dataset in this paper. In our experiments, a tiny shift in learning rate results in a significant 0.02% drop in model performance at batch size 1K (Figure 6), indicating the task is sensitive to hyperparameters.\nMethods utilizing embedding frequency in CTR prediction Many works have shed light on the importance of different frequencies in embeddings to accelerate or improve the accuracy of CTR prediction models. (Miao et al. 2021;Adnan et al. 2021) finds that most access to the embedding layer happens in a small fraction of embeddings. They both propose to cache the most frequent embeddings to reduce the communication bottleneck. (Ginart et al. 2021) proposed a mixed dimension embedding where the embedding vector's dimension scales with its query frequency to reduce the memory cost. To improve the performance of the CTR prediction models, (Zhou et al. 2019) shows a regularization by filtering out ids with a low occurrence frequency. (Li et al. 2016) proposes to apply stronger regularization on more frequent ids for better generalization performance. In this paper, we show that the different frequencies of ids result in the failure of previous scaling rules when scaling up the batch size.", "publication_ref": ["b53", "b10", "b36", "b18", "b41", "b27", "b39", "b26", "b12", "b45", "b44", "b28", "b51", "b32", "b37", "b41", "b18", "b41", "b47", "b24", "b33", "b1", "b14", "b55", "b25"], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Traditional Scaling Rules Derivation", "text": "The deduction of scaling rules is mostly based on the SGD optimizer. However, many experimental results (He et al. 2021;You et al. 2020;Hoffer, Hubara, and Soudry 2017) show they are also effective when applied to adaptive optimizers such as Adam. We consider the case batch size is scaled from B to B , where b = |B | = s|B| = sb. For a given big batch B t , the corresponding s small batches are {B t,i } s i=1 .", "publication_ref": ["b20", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "AUC", "text": "\u00b10.02%", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "LR (x10 )", "text": "-4\nFigure 6: AUC performance of DeepFM on Criteo dataset with different learning rates. The one-year improvement is about 0.3%. The accuracy drops a lot when a slight disturbance is made.\nLinear Scaling. For Linear Scaling, when scaling up s times the batch size, the motivation is to keep the update by a big batch equivalent to the update made within s small batches. Denote the update made by SGD update as \u2206w = w t \u2212 w t+1 , for the small batches. We have the expected update:\nE[\u2206w] = \u03b7 s i=1 E[ 1 b \u2022 x\u2208Bt,i \u2207L(w t,i , x)] = \u03b7 s i=1 E[\u2207L(w t,i , x)],\nwhile for the big batch:\nE[\u2206w] = \u03b7 E[ 1 b \u2022 x\u2208B t \u2207L(w t , x)] = \u03b7 E[\u2207L(w t , x)].(3)\nUnder the assumption that E[\u2207L(w t,i , x)] \u2248 E[\u2207L(w t , x)] for i = 1 to s, to make the two update equal, we need to scale the learning rate \u03b7 \u2192 s\u03b7.\nSqrt Scaling. For Sqrt Scaling, its motivation is to keep the covariance matrix of the parameters updates \u2206w the same. The derivation comes from , (Hoffer, Hubara, and Soudry 2017) and we only consider the case that samples are randomly drawn from a dataset with replacement. According to (Hoffer, Hubara, and Soudry 2017), with minibatch gradient denoted a\u015d g = 1 b\nx\u2208B g x , we have\ncov(\u011d,\u011d) = ( 1 b \u2212 1 N ) 1 N N i=1\u011d i\u011d i .\nSince 1 N is small, the update of SGD has covariance:\ncov(\u2206w, \u2206w) = cov(\u03b7\u011d, \u03b7\u011d) \u2248 \u03b7 2 b \u2022 N N i=1\u011d i\u011d i .(4)\nWhen scaling b \u2192 sb, to keep the covariance matrix to be the same, we need to scale \u03b7 \u2192 \u221a s\u03b7.\nScaling the L2-regularization weight. We follow the discussion in (Krizhevsky 2014) to scale the L2-regularization weight with batch size. Consider the case only L2-regularization is applied, under a large batch we have:\nw t+1 = w t (1 \u2212 \u03b7 \u03bb w),\nwhile for small batches:\nw t+1 = w t (1 \u2212 \u03b7\u03bbw) s = w t (1 \u2212 s\u03b7\u03bbw).\nTo make the L2-regularization strength at the same level, when scaling the batch size, we have the equation:\n\u03b7 \u03bb \u2248 s\u03b7\u03bb.\nTherefore, considering the learning rate scaling in linear scaling and sqrt scaling, it is easy to calculate the scaling strategy for the \u03bb.", "publication_ref": ["b21", "b21", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Sqrt Scaling Motivation with Different Frequencies", "text": "In Section , we have discussed the scaling rules for columns with different frequencies. Here, we consider the sqrt scaling rule. The derivation is inspired by (Hoffer, Hubara, and Soudry 2017). Similar to Equation 2, we have\ng = 1 b \u03b4(id fj k \u2208 B) x\u2208B \u2207L(w, x) = \u03b4(id fj k \u2208 B)\u011d.\nWith Equation 4, since mini-batches are uncorrelated, the covariance is\ncov(\u2206g, \u2206g) = E[gg ] \u2212 E[g] E[g ] = 1 b 2 b i=1 b j=1 E[\u03b4(id fj k \u2208 B)\u03b4(id fj k \u2208 B )]\u011d i\u011dj \u2212 P(id fj k \u2208 B) 2 E[\u011d] E[\u011d ] = P(id fj k \u2208 B) E[\u011d\u011d ] \u2212 P(id fj k \u2208 B) 2 E[\u011d] E[\u011d ].\nAs we can see, the frequency differently scales the two parts in covariance. It is impossible to correct to the original behavior by multiplying a scaler to the learning rate. Only under a strong assumption that E[\u011d\u011d ] E[\u011d] E[\u011d ] can we make the following approximation:\ncov(\u2206g, \u2206g) \u2248 P(id fj k \u2208 B)cov(\u2206\u011d, \u2206\u011d). The assumption may not hold in practice, which adds to the problem's difficulty. Under the hypothesis, we have the covariance matrix for the \u2206w is:\ncov(\u2206w, \u2206w) = cov(\u03b7\u011d, \u03b7\u011d) \u2248 P(id fj k \u2208 B) \u2022 \u03b7 2 b \u2022 N N i=1\u011d i\u011d i .\nFor dense weight and frequent ids, with P(id fj k \u2208 B) \u2248 1, it is the same to the sqrt scaling. However, for infrequent ids, the covariance matrix becomes:\ncov(\u2206w, \u2206w) = cov(\u03b7\u011d, \u03b7\u011d) \u2248 \u03b7 2 P(id fj k \u2208 x) \u2022 N N i=1\u011d i\u011d i .\nHence, there is no need to scale the learning rate to keep the same covariance matrix.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Effect of Loss Scaling", "text": "In training the network, the gradient of weights w on one sample x concerning the training loss L is \u2207 w L(w, x). When training with batch size b, we take the average of gradients on the samples and thus have the expected gradients as follows:\nE[G] = E[ 1 b b i=1 \u2207 w L(w, x i )] = E[\u2207 w L(w, x)].\nAs discussed in Equation 2, in training with weights of different frequencies, there is a case a constant multiplier is applied to the expected gradients. We consider the case gradients are scaled by c and show different behaviors with SGD and Adam with L2-regularization following discussion in (Soroush 2019):\nE[G] = c \u2022 E[\u2207 w L(w, x)].\nFirst, for SGD optimizer with learning rate \u03b7 and L2-regularization weight \u03bb, the expected update is:\nE[\u2206w] = E[w t \u2212 w t+1 ] = \u03b7 \u2022 (E[\u2207 wt L(w t , x)] \u2212 \u03bb 2 w t ).\nWhen the gradients are scaled by c, we have\nE[\u2206w] = \u03b7 \u2022 (c \u2022 E[\u2207 wt L(w t , x)] \u2212 \u03bb 2 w t ) = c\u03b7 \u2022 (\u2022 E[\u2207 wt L(w t , x)] \u2212 \u03bb 2c w t ).\nSo the effect is using a new learning rate of c\u03b7 and new L2-regularization weight of \u03bb c . For Adam optimizer with (\u03b2 1 , \u03b2 2 ) without L2-regularization, the expected update is: For large batch training with CowClip, there are two additional techniques. Warmup on learning rate has been widely used in training CV and NLP networks (Gotmare et al. 2019). It helps the network start smoothly for a more stable training process. We find warmup on the learning rate of the embeddings has little improvement, so we only apply one-epoch learning rate warmup to the dense weights of the CTR prediction model. Weight initialization is also important for a good starting state. We use Kaiming initialization (He et al. 2015) for all dense weights. The original initialization for embeddings is w \u223c N (0, \u03c3), \u03c3 = 10 \u22124 . With a dimension of d, the initial weight norm is \u221a d \u2022 \u03c3. To allow for a greater gradient norm bound in CowClip, we use a larger initial weight by setting the \u03c3 to 10 \u22122 for training with CowClip. To avoid a too strong clipping value, we lower bound the clipping value with 10 \u22124 , which is the original initial weight norm, and in Criteo 10 \u22125 , which yields a better result.\nE[\u2206w] = \u03b7 E[ m t \u221a v t + ].(5)", "publication_ref": ["b16", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Additional Experimental Results", "text": "We run our experiments with three random seeds (1234,1235,1236), and the standard deviation among all experiments for AUC is less than 0.012%. The performance comparisons between previous scaling methods and CowClip for Criteo-seq and Avazu dataset are shown in Table 10 and Table 11 respectively. The performance of CowClip methods at different batch size with four different network architectures on Avazu is presented in Table 12. As we can see, Cowclip maintains the performance at a large batch size on both dataset and achieves a fast speedup (for Criteo-seq, the speedup is the same as Criteo). The training time comparison and speedup for the Avazu dataset are shown in Table 13.The training and testing AUC and loss curve at different epochs with different batch sizes are shown in Figure 7 and Figure 8 respectively.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_0", "tab_0"]}, {"heading": "More Ablation Study", "text": "More ablation study is shown in Table 14. The first two rows verify the effectiveness of our scaling rule. Warmup on the dense weights is critical when the batch size is very large, while large initialization weights prevent the network from underfitting when the batch size is not that large.\nWe decide the hyperparameters for variants of gradient clipping in Table 7 as follows, which may be helpful if a simple version of gradient clipping is adopted for a complex system. First, we run the experiment and log out the gradients of interested    units (i.e., global, field, column). For gradient clipping, the upper bound is 25, and after searching in {25, 20, 10, 1}, we find the performance is not sensitive to the clipping value. For field-wise and column-wise, we search in {10 \u22123 , 10 \u22122 , 10 \u22121 , 1, 10} and {10 \u22125 , 10 \u22124 , 10 \u22123 } respectively. For the gradient clipping with constant value clip t for the global embedding or a field, note that when scaling the batch size, we also need to scale this value. Take the field-wise gradient clipping for example. Consider scaling the batch size from b to s \u2022 b. The scaling rule for the embedding layers is as follows. First, consider the case all ids are frequent, then doubling the batch size doubles the occurrence of these ids in the batch. Thus, the gradients are also doubled. This indicates a linear scaling on the gradient clipping value. However, if all the ids are infrequent, considering the process of merging s small batches into a big batch. For a specific id, the probability that it occurs in two of s batches is small. Thus, with the assumption that no colliding ids occur in the s small batches, we have the gradient g for the large batch B (g for the small batch B):\ng = s i=1 g s = \u221a sg.\nThis indicates we should use a square root scaling for the gradient clipping value. In practice, although both frequent and infrequent ids exist, we find the sparse one dominates the gradients. We find that the scaling in the norm of gradients is approximately \u221a s when scaling the batch size and suggest square root scaling on gradient clipping value on the embedding layer is a better choice.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0", "tab_7"]}, {"heading": "Proof Sketch for Convergence of CowClip", "text": "To see the convergence property of our algorithm, we go through the following steps to get the CowClip optimizer, and the convergence of CowClip is ensured by each component.\nFirst, the CowClip algorithm only applies to the embedding layers (the first layer) of the whole model. The convergence of the dense part is guaranteed by the Adam optimizer (Kingma and Ba 2015). Next, if we change the gradient clipping operation in CowClip to gradient normalization, our algorithm can be viewed as a variant of the LAMB optimizer (You et al. 2020), which also ensures convergence. The differences between this version of CowClip and LAMB are as follows. CowClip focuses on a smaller granularity, which has been studied in the AGC optimizer (Brock et al. 2021) CowClip also scales the threshold by the occurrence time, but this is to correct the reduce-mean from each field's perspective. Finally, we relax CowClip from normalization to gradient clipping. The convergence of gradient clipping has been fully understood according to (Zhang et al. 2020;Mai and Johansson 2021). A clipping method is a weak form of the normalization method (leaving the gradients below the threshold unchanged). Thus the relaxation does not change the convergence of the CowClip algorithm.", "publication_ref": ["b22", "b7", "b50", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion and Future Work", "text": "With CowClip, we can scale the batch size of the CTR prediction model to 128 times larger than the original size on one GPU. Despite the great power of our method, there are more works to be done for large batch CTR prediction training, which we leave as future works.\nFirst, as mentioned in the introduction, many works have been devoted to designing a sound system for a multi-node CTR prediction model. Due to the computational resource, we verified our method on a single GPU setting. Although it seems straightforward to integrate our approach into a multi-GPU training setting, system optimization is still needed for fast distributed training. It is also interesting to know how much can our method accelerate the training in a communication and memory-efficient multi-node CTR prediction system.\nSecond, when scaling to a very large batch size (e.g., 256K or even larger), the AUC still drops even with CowClip. One possible reason for this is that as the batch size grows, the assumption most ids are infrequent may not be held. One possible way to deal with the problem is to design an id-wise scaling strategy, which may not be computational-efficient. Another possibility lies in the loss of generalization ability of models trained at a large batch, as found and discussed in CV and NLP areas. Since our experiments are conducted on only one GPU, a larger batch size is needed when scaling to a multi-GPU setting.\nIn addition, our experiments use the setting of adam optimizer and L2-regularization on all weights. In this setting, every weight and embeddings, along with their optimization states in the optimizer, are updated in each step. In some modern CTR prediction systems, a 'lazy' optimizer (e.g., adagrad (Duchi, Hazan, and Singer 2010), lazy-adam) updates the state of embeddings only when the corresponding id appears in the batch, and an L2-regularization only imposed on these ids is used for fast computation. In addition, sparse representation of embedding matrix also makes a difference to the optimization process (e.g., sparse optimizer update of tensor in Tensorflow). The clipping strategy should be modified to suit these methods. Combining our method with these variants of optimization strategies is also an interesting problem for practical deployment.\nOur experiments follow previous work and train the network with 10 epochs. In reality, when the training dataset is huge, it is unaffordable or too slow to do multi-epoch training. In this case, one-epoch training is the choice to train an update-to-date CTR prediction model. As shown in the training curve in Figure 8, although we achieve better results when finishing the training, the AUC at the first epoch drops compared to the small-batch setting. We also find that to maintain the first epoch AUC value, a much smaller L2-regularization should be adopted. The reason may be that in the first training epoch, overfitting is not likely to occur and thus requires a weaker regularization. We believe an investigation into one-epoch large batch training will be valuable work.\nApart from CTR prediction, there are other tasks with a large embedding table, such as NLP tasks. For instance, in the Chinese embedding table, an unbalanced-frequency exists among different characters. Even if the frequencies of different ids are not as varied as those in CTR prediction, a simplified version of CowClip (e.g., remove the occurrence count) may help to stabilize the training of models in these tasks.\nWith the growth of hardware and modern CTR prediction systems, we believe a trend is to adopt a larger and larger batch size for fast CTR prediction model training. To use a large batch size, apart from a robust system, a suitable algorithm is also needed to maintain the performance. As there are many works on large batch training in CV and NLP, few works discuss the problem of large batch training in the CTR prediction model. We think it is worthwhile to investigate this problem.", "publication_ref": ["b13"], "figure_ref": [], "table_ref": []}, {"heading": "Broader Impact", "text": "Accelerating the training speed of the CTR prediction model by large batch training is directly beneficial to the ad-tech and ecommerce practicians. Time and cost are reduced for re-training a model, contributing to faster product development iterations. In addition, the personalized recommendation could be more accurate and up-to-date, which potentially improves the user experience.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgement", "text": "We thank Google TFRC for supporting us to get access to the Cloud TPUs. This work is supported NUS startup grant, the Singapore MOE Tier-1 grant, and the ByteDance grant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "(1, 10 \u22125 ) 1 \u00d7 10 \u22124 1.6 \u00d7 10 \u22123 4 \u00d7 10 \u22124\n(1, 10 \u22124 ) 32K ( 32768)\n(1, 10 \u22124 )\nIf we omit the term and bias correction for simplicity, when the gradients are scaled by c, the momentum term m t has:\nwhich is c times the original E[m t ]. The similar deduction finds v t \u2192 c 2 v t . Thus,\nso the behaviour of Adam without regularization is not changed. However, with L2-regularization, we have:\nand the same deduction applies to v 2 . This shows that when the gradients are scaled by c, it is equivalent to using a new L2-regularization weight of \u03bb c .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Additional Implementation Details", "text": "In this paper, \"K\" means \u00d71024, so 1K means 1024 batch size. Following the common network setting (Guo et al. 2018), the dimension of categorical field embedding is 10, the depth of hidden layers for MLP is 3, and the number of neurons is 400 per layer. All models are trained with 10 epochs, and the final model is evaluated at the test set. The detailed learning rate, L2-regularization weights, and other hyperparameters are listed in Table 8 and Table 9, square roots are round to four decimal places in practice. The hyperparameter r is not sensitive, so we set it directly to 1, and the choice of \u03b6 is related to the initialization weight in the next paragraph. LR denotes the learning rate \u03b7, and L2 denotes the L2-regularization weight \u03bb (no L2-regularization is imposed on dense weights). All activation functions are ReLU (Agarap 2018), and dropout (Srivastava et al. 2014) is not used as we do not see its improvement. For DCN and DCNv2, the number of cross-layer is 3, and we only adopt the cross-layer form from the DCNv2 paper. The CowClip method is performed on id vector embeddings (columns), but is not applied to LR method for DeepFM and W&D, whose biases can be viewed as a 1-dimension embedding. For the continuous field, they do not involve in the wide or cross stream and are directly sent into the DNN stream.\nOne technique to train with CowClip is that as the learning process of embedding becomes more smooth and stable, we can fix the learning rate for the embeddings and scale up the learning rate for the dense layer until the training process diverges for better performance. As the batch size grows beyond a threshold, the proposed scaling rule may face accuracy loss (8K for empirical scaling rules and 128K for the CowClip scaling rule in the Avazu dataset). In that case, we do a little hyperparameter fine-tuning by scaling some of the hyperparameters to twice or half of their supposed value. Basically, we increase the L2 \u03bb when the network is overfitting and increase \u03b6 when the network is underfitting. These values are underlined in the table.", "publication_ref": ["b18", "b4", "b40"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "journal": "", "year": "2015", "authors": "M Abadi; A Agarwal; P Barham; E Brevdo; Z Chen; C Citro; G S Corrado; A Davis; J Dean; M Devin; S Ghemawat; I Goodfellow; A Harp; G Irving; M Isard; Y Jia; R Jozefowicz; L Kaiser; M Kudlur; J Levenberg; D Man\u00e9; R Monga; S Moore; D Murray; C Olah; M Schuster; J Shlens; B Steiner; I Sutskever; K Talwar; P Tucker; V Vanhoucke; V Vasudevan; F Vi\u00e9gas; O Vinyals; P Warden; M Wattenberg; M Wicke; Y Yu; X Zheng"}, {"ref_id": "b1", "title": "Accelerating input dispatching for deep learning recommendation models training", "journal": "", "year": "2021", "authors": "M Adnan"}, {"ref_id": "b2", "title": "", "journal": "", "year": "", "authors": "M Adnan; Y E Maboud; D Mahajan; P J Nair"}, {"ref_id": "b3", "title": "Accelerating Recommendation System Training by Leveraging Popular Choices", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "Deep learning using rectified linear units (relu). arXiv", "journal": "", "year": "2018", "authors": "A F Agarap"}, {"ref_id": "b5", "title": "Avazu Click-Through Rate Prediction", "journal": "", "year": "2015", "authors": " Avazu"}, {"ref_id": "b6", "title": "Evolution of the GPU Device widely used in AI and Massive Parallel Processing", "journal": "IEEE", "year": "2018", "authors": "T Baji"}, {"ref_id": "b7", "title": "High-performance large-scale image recognition without normalization", "journal": "PMLR", "year": "2021", "authors": "A Brock; S De; S L Smith; K Simonyan"}, {"ref_id": "b8", "title": "Improved CTR Prediction Algorithm based on LSTM and Attention", "journal": "", "year": "2021", "authors": "Q Chen; D Li"}, {"ref_id": "b9", "title": "Behavior sequence transformer for e-commerce recommendation in alibaba", "journal": "", "year": "2019", "authors": "Q Chen; H Zhao; W Li; P Huang; W Ou"}, {"ref_id": "b10", "title": "Wide & Deep Learning for Recommender Systems", "journal": "", "year": "2016", "authors": "H.-T Cheng; L Koc; J Harmsen; T Shaked; T Chandra; H B Aradhye; G Anderson; G S Corrado; W Chai; M Ispir; R Anil; Z Haque; L Hong; V Jain; X Liu; H Shah"}, {"ref_id": "b11", "title": "Deep Neural Networks for YouTube Recommendations", "journal": "", "year": "2016", "authors": "P Covington; J K Adams; E Sargin"}, {"ref_id": "b12", "title": "DeepLight: Deep Lightweight Feature Interactions for Accelerating CTR Predictions in Ad Serving", "journal": "", "year": "2021", "authors": "W Deng; J Pan; T Zhou; A Flores; G Lin"}, {"ref_id": "b13", "title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "journal": "Journal of Machine Learning Research", "year": "2010", "authors": "J C Duchi; E Hazan; Y Singer"}, {"ref_id": "b14", "title": "Mixed Dimension Embeddings with Application to Memory-Efficient Recommendation Systems", "journal": "", "year": "2021", "authors": "A A Ginart; M Naumov; D Mudigere; J Yang; J Y Zou"}, {"ref_id": "b15", "title": "The Netflix Recommender System: Algorithms, Business Value, and Innovation", "journal": "ACM Transactions on Management Information Systems (TMIS)", "year": "2016", "authors": "C Gomez-Uribe; N Hunt"}, {"ref_id": "b16", "title": "A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation. arXiv", "journal": "", "year": "2019", "authors": "A D Gotmare; N S Keskar; C Xiong; R Socher"}, {"ref_id": "b17", "title": "Accurate, Large Minibatch SGD: Training", "journal": "", "year": "2017", "authors": "P Goyal; P Doll\u00e1r; R B Girshick; P Noordhuis; L Wesolowski; A Kyrola; A Tulloch; Y Jia; K He"}, {"ref_id": "b18", "title": "DeepFM: An End-to-End Wide & Deep Learning Framework for CTR Prediction", "journal": "", "year": "2018", "authors": "H Guo; R Tang; Y Ye; Z Li; X He; Z Dong"}, {"ref_id": "b19", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification", "journal": "", "year": "2015", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b20", "title": "Large-Scale Deep Learning Optimizations: A Comprehensive Survey. arXiv", "journal": "", "year": "2021", "authors": "X He; F Xue; X Ren; Y You"}, {"ref_id": "b21", "title": "Train longer, generalize better: closing the generalization gap in large batch training of neural networks. arXiv, abs/1705.08741", "journal": "International Journal of Mobile Human Computer Interaction (IJMHCI)", "year": "2009", "authors": "E Hoffer; I Hubara; D Soudry; E Kaasinen; V Roto; K Roloff; K V\u00e4\u00e4n\u00e4nen-Vainio-Mattila; T Vainio; W Maehr; D Joshi; S Shrestha"}, {"ref_id": "b22", "title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba"}, {"ref_id": "b23", "title": "One weird trick for parallelizing convolutional neural networks", "journal": "", "year": "2014", "authors": "A Krizhevsky"}, {"ref_id": "b24", "title": "Scale MLPerf-0.6 models on Google TPU-v3 Pods", "journal": "", "year": "2019", "authors": "S Kumar; V Bitorff; D Chen; C.-H Chou; B A Hechtman; H Lee; N Kumar; P Mattson; S Wang; T Wang; Y Xu; Z Zhou"}, {"ref_id": "b25", "title": "DiFacto: Distributed Factorization Machines", "journal": "", "year": "2016", "authors": "M Li; Z Liu; A Smola; Y.-X Wang"}, {"ref_id": "b26", "title": "Fi-GNN: Modeling Feature Interactions via Graph Neural Networks for CTR Prediction", "journal": "", "year": "2019", "authors": "Z Li; Z Cui; S Wu; X Zhang; L Wang"}, {"ref_id": "b27", "title": "xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems", "journal": "", "year": "2018", "authors": "J Lian; X Zhou; F Zhang; Z Chen; X Xie; G Sun"}, {"ref_id": "b28", "title": "Memorize, Factorize, or be Na\u00efve: Learning Optimal Feature Interaction Methods for CTR Prediction. arXiv", "journal": "", "year": "2021", "authors": "F Lyu; X Tang; H Guo; R Tang; X He; R Zhang; X Liu"}, {"ref_id": "b29", "title": "Temporal-Contextual Recommendation in Real-Time", "journal": "", "year": "2020", "authors": "Y Ma; B Narayanaswamy; H Lin; H Ding"}, {"ref_id": "b30", "title": "Stability and convergence of stochastic gradient clipping: Beyond lipschitz continuity and smoothness", "journal": "PMLR", "year": "2021", "authors": "V V Mai; M Johansson"}, {"ref_id": "b31", "title": "", "journal": "", "year": "", "authors": "P Mattson; C Cheng; C A Coleman; G F Diamos; P Micikevicius; D Patterson; H Tang; G.-Y Wei; P Bailis; V Bittorf; D M Brooks; D Chen; D Dutta; U Gupta; K M Hazelwood; A Hock; X Huang; B Jia; D Kang; D Kanter; N Kumar; J Liao; G Ma; D Narayanan; T Oguntebi; G Pekhimenko; L Pentecost; V J Reddi; T Robie; T S John; C.-J Wu; L Xu; C Young; Zaharia "}, {"ref_id": "b32", "title": "Ad click prediction: a view from the trenches", "journal": "", "year": "2013", "authors": "H B Mcmahan; G Holt; D Sculley; M Young; D Ebner; J Grady; L Nie; T Phillips; E Davydov; D Golovin; S Chikkerur; D Liu; M Wattenberg; A M Hrafnkelsson; T Boulos; J Kubica"}, {"ref_id": "b33", "title": "HET: Scaling out Huge Embedding Model Training via Cache-enabled Distributed Framework", "journal": "", "year": "2021", "authors": "X Miao; H Zhang; Y Shi; X Nie; Z Yang; Y Tao; B Cui"}, {"ref_id": "b34", "title": "", "journal": "", "year": "2021", "authors": "D Mudigere; Y Hao; J Huang; Z Jia; A Tulloch; S Sridharan; X Liu; M Ozdal; J Nie; J Park; L Luo; J A Yang; L Gao; D Ivchenko; A Basant; Y Hu; J Yang; E K Ardestani; X Wang; R Komuravelli; C.-H Chu; S Yilmaz; H Li; J Qian; Z Feng; Y.-A Ma; J Yang; E Wen; H Li; L Yang; C Sun; W Zhao; D Melts; K Dhulipala; K G Kishore; T Graf; A Eisenman; K K Matam; A Gangidi; G J Chen; M Krishnan; A Nayak; K Nair; B Muthiah; M Khorashadi; P Bhattacharya; P Lapukhov; M Naumov; A S Mathews; L Qiao; M Smelyanskiy; B Jia; V Rao"}, {"ref_id": "b35", "title": "Deep Learning Recommendation Model for Personalization and", "journal": "", "year": "2019", "authors": "M Naumov; D Mudigere; H.-J M Shi; J Huang; N Sundaraman; J Park; X Wang; U Gupta; C.-J Wu; A G Azzolini; D Dzhulgakov; A Mallevich; I Cherniavskii; Y Lu; R Krishnamoorthi; A Yu; V Kondratenko; S Pereira; X Chen; W Chen; V Rao; B Jia; L Xiong; M Smelyanskiy"}, {"ref_id": "b36", "title": "Product-Based Neural Networks for User Response Prediction", "journal": "", "year": "2016", "authors": "Y Qu; H Cai; K Ren; W Zhang; Y Yu; Y Wen; J Wang"}, {"ref_id": "b37", "title": "Factorization Machines", "journal": "", "year": "2010", "authors": "S Rendle"}, {"ref_id": "b38", "title": "DeepCTR: Easy-to-use,Modular and Extendible package of deep-learning based CTR models", "journal": "", "year": "2017", "authors": "W Shen"}, {"ref_id": "b39", "title": "AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks", "journal": "", "year": "2019", "authors": "W Song; C Shi; Z Xiao; Z Duan; Y Xu; M Zhang; J Tang"}, {"ref_id": "b40", "title": "Dropout: a simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "N Srivastava; G E Hinton; A Krizhevsky; I Sutskever; R Salakhutdinov"}, {"ref_id": "b41", "title": "Deep & Cross Network for Ad Click Predictions", "journal": "", "year": "2017", "authors": "R Wang; B Fu; G Fu; M Wang"}, {"ref_id": "b42", "title": "DCN V2: Improved Deep & Cross Network and Practical Lessons for Web-scale Learning to Rank Systems", "journal": "", "year": "2021", "authors": "R Wang; R Shivanna; D Z Cheng; S Jain; D Lin; L Hong; E H Chi"}, {"ref_id": "b43", "title": "A Survey of Online Advertising Click-Through Rate Prediction Models", "journal": "", "year": "2020", "authors": "X Wang"}, {"ref_id": "b44", "title": "MaskNet: Introducing Feature-Wise Multiplication to CTR Ranking Models by Instance-Guided Mask. arXiv", "journal": "", "year": "2021", "authors": "Z Wang; Q She; J Zhang"}, {"ref_id": "b45", "title": "Correct Normalization Matters: Understanding the Effect of Normalization On Deep Neural Network Models For Click-Through Rate Prediction", "journal": "", "year": "2006", "authors": "Z Wang; Q She; P Zhang; J Zhang"}, {"ref_id": "b46", "title": "Kraken: memoryefficient continual learning for large-scale real-time recommendations", "journal": "", "year": "2020", "authors": "M Xie; K Ren; Y Lu; G Yang; Q Xu; B Wu; J Lin; H Ao; W Xu; J Shu"}, {"ref_id": "b47", "title": "Large Batch Training of Convolutional Networks. arXiv", "journal": "", "year": "2017", "authors": "Y You; I Gitman; B Ginsburg"}, {"ref_id": "b48", "title": "", "journal": "", "year": "", "authors": "Y You; J Li; S J Reddi; J Hseu; S Kumar; S Bhojanapalli; X Song; J Demmel; K Keutzer; C.-J Hsieh"}, {"ref_id": "b49", "title": "Large Batch Optimization for Deep Learning: Training BERT in 76 minutes. arXiv", "journal": "", "year": "", "authors": ""}, {"ref_id": "b50", "title": "Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity", "journal": "", "year": "1905", "authors": "J Zhang; T He; S Sra; A Jadbabaie"}, {"ref_id": "b51", "title": "FAT-DeepFFM: Field Attentive Deep Field-aware Factorization Machine. In ICDM", "journal": "", "year": "2019", "authors": "J Zhang; T Huang; Z Zhang"}, {"ref_id": "b52", "title": "Deep learning based recommender system: A survey and new perspectives", "journal": "ACM Computing Surveys (CSUR)", "year": "2019", "authors": "S Zhang; L Yao; A Sun; Y Tay"}, {"ref_id": "b53", "title": "Deep Learning over Multi-field Categorical Data -A Case Study on User Response Prediction", "journal": "ECIR", "year": "2016", "authors": "W Zhang; T Du; J Wang"}, {"ref_id": "b54", "title": "AIBox: CTR Prediction Model Training on a Single Node", "journal": "", "year": "2019", "authors": "W Zhao; J Zhang; D Xie; Y Qian; R Jia; P Li"}, {"ref_id": "b55", "title": "Deep Interest Evolution Network for Click-Through Rate Prediction. arXiv", "journal": "", "year": "2019", "authors": "G Zhou; N Mou; Y Fan; Q Pi; W Bian; C Zhou; X Zhu; K Gai"}, {"ref_id": "b56", "title": "Open Benchmarking for Click-Through Rate Prediction", "journal": "", "year": "2021", "authors": "J Zhu; J Liu; S Yang; Q Zhang; X He"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Relative time of training DeepFM model on Criteo dataset with one V100 GPU.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: A simple illustration of a Wide/Cross-and-Deep style of CTR prediction model. The green data denotes a dense one, while brown input in a categorical field stands for a selected id.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "0.431M 0.431M 0.433M 0.655M 372M 104M", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure 3: Progress on AUC of CTR prediction models on Criteo dataset in the past six years.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Distribution of different ids in three fields of the Criteo dataset. The y-axis is in logarithm scale. The total number of samples is 4.13 \u00d7 10 7 .", "figure_data": ""}, {"figure_label": "35", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Scaling Rule 3 (Figure 5 :35Figure 5: L2-norm distribution of different columns gradients at 1000th step of DeepFM on Criteo dataset. Only columns with existing ids in the batch are shown. The x-axis is the L2 norm value and y-axis is the count of columns.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Baselines. Four CTR prediction models are considered in our experiments: Wide-and-Deep Network (W&D)(Cheng et al. 2016), DeepFM(Guo et al. 2018), Deep-and-Cross Network (DCN)(Wang et al. 2017), DCN v2. The architectures of these networks are detailed in Appendix ). For the scaling strategy, No Scaling means we use the same hyper-parameters as the ones in batch size 1K. Sqrt Scaling and LR Scaling are described in Section 3. Sqrt Scaling * is a variant version of Sqrt Scaling used in(Guo et al. 2018), which does not scale up the L2regularization. For batch size from 1K to 8K, we also do a grid search on learning rate and the weight decay, but it turns out no simple combination yields better results than the above scaling methods. DLRM(Naumov et al. 2019) uses model parallelism on the embedding table to accelerate the training. XDL(Adnan et al. 2021) is a highly optimized implementation of the above model. FAE(Adnan et al. 2021) takes the frequency of embeddings into consideration as well and uses a hot-embedding aware data layout in the memory. Hotline (Adnan 2021) better organizes the frequent and infrequent embeddings in the GPU and main memory. CowClip denotes training with the CowClip method and the CowClip scaling rule 3.", "figure_data": ""}, {"figure_label": "78", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 :Figure 8 :78Figure 7: Training AUC (left) and Loss (right) at different epochs with different batch sizes during the training.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Number of parameters for different layers.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "AUC (%) changes at different batch sizes on Criteo with DeepFM and a modified version. Previous scaling rules fail on Criteo but work for a revised version.", "figure_data": "No Scale Sqrt Scale Linear ScaleCriteo1k80.7680.7680.762k-0.15-0.01-0.014k-1.35-0.06-0.118k-3.21-0.21-0.20Criteo (Top 3 frequent ids)1k74.9774.9774.972k-0.10-0.01+0.044k-0.20-0.02-0.028k-0.28-0.01-0.01when scaling the batch size are learning rate and regular-ization weight. Based on different assumptions, linear scal-ing"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Adaptive Column-wise Clipping(CowClip) Input: CowClip coefficient r and lower-bound \u03b6, number of steps T , batch size b, learning rate for dense and embedding \u03b7, \u03b7 e , optimizer Opt(\u2022) 1: for t \u2190 1 to T do", "figure_data": "2:Draw b samples B from D3:g t , g e t \u2190 1 bx\u2208B \u2207L(x, w t , w e t )4:w t+1 \u2190 \u03b7 \u2022 Opt(w t , g t )// Update dense weights5: 6: 7:for each field and each column in the field do n g \u2190 g e t [id fj k ] cnt \u2190 |{x \u2208 B|id fj k \u2208 x}|8: 9:// Calculate the number of occurrence cnt clip t \u2190 cnt \u2022 max{r \u2022 w e t [id fj k ] , \u03b6}10:// Clip norm threshold11:g c \u2190 min{1, clip t ng } \u2022 g e t [idfj k ]12:// Gradient clipping13:w e t [idfj k ] \u2190 \u03b7 e \u2022 Opt(w e t [id fj k ], g c )14:// Update the id embedding15:"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance comparison between Cowclip and previous scaling methods with different batch size. Prev. best CowClip Prev. best CowClip Prev. best Cowclip", "figure_data": "1K8K128KCriteo80.7680.8680.5580.97-80.90Criteo-seq80.4880.5080.0380.50-80.49Avazu78.8478.8376.6979.06-78.80"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance of different scaling methods on Criteo dataset from 1K to 8K on DeepFM.", "figure_data": "1K (1024)2K (2048)4K (4096)8K (8192)AUC (%) LogLoss AUC (%) LogLoss AUC (%) LogLoss AUC (%) LogLossNo Scaling80.760.443880.660.445680.480.451880.310.4530Sqrt Scaling80.760.443880.710.443080.590.445080.280.4582Sqrt Scaling  *80.760.443880.750.444480.690.444980.550.4547LR Scaling80.760.443880.770.443480.650.443480.460.4542n 2 -\u03bb Scaling (Ours)80.760.443880.860.443280.900.442680.730.4441CowClip (Ours)80.860.443080.930.442780.970.442280.970.4425ExperimentExperimental setting"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance of CowClip methods on Criteo dataset from 1K to 128K on four models.", "figure_data": "Baseline1K2K4K8K16K32K64K128KDeepFM (Guo et al. 2018)AUC (%) Logloss80.76 0.443880.86 0.4430 0.4427 0.4422 0.4425 0.4424 0.4423 0.4429 0.4430 80.93 80.97 80.97 80.94 80.95 80.96 80.90W&D (Cheng et al. 2016)AUC (%) Logloss80.75 0.443980.86 0.4430 0.4424 0.4422 0.4425 0.4422 0.4428 0.4429 0.4434 80.94 80.96 80.96 80.95 80.94 80.96 80.89DCN (Wang et al. 2017)AUC (%) Logloss80.76 0.443880.86 0.4429 0.4424 0.4422 0.4428 0.4419 0.4426 0.4426 0.4428 80.93 80.96 80.97 80.98 80.95 80.99 80.91DCN v2 (Wang et al. 2021)AUC (%) Logloss80.78 0.443780.87 0.4429 0.4425 0.4422 0.4423 0.4420 0.4424 0.4427 0.4427 80.94 80.97 80.98 80.97 80.95 80.97 80.89"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The training time of different methods on Criteo dataset. Last four are trained with CowClip.", "figure_data": "Time (minutes)AUC (%) Logloss 1K2K4K8K16K32K64K128KXDL80.20.452196179  \u2020160  \u2021-----FAE80.20.452122116  \u2020104  \u2021-----DLRM79.80.456196133  \u202076  \u2021-----Hotline79.80.4565345  \u202039  \u2021-----DeepFM80.870.4428 7683902041024827159W&D80.860.4430 76839020410248271510DCN80.860.4429 76839020410248281711DCN v280.870.4429 82240821010860403430Speedup (DeepFM)1\u00d7 1.96\u00d7 3.76\u00d7 7.52\u00d7 16.00\u00d7 28.44\u00d7 51.2\u00d7 76.8\u00d7"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Ablation study of CowClip on Criteo with DeepFM.", "figure_data": "b = 8Kb = 128KAUC (%) LogLoss AUC (%) LogLossGradient Clipping (GC)80.630.445277.240.4953Field-wise GC80.630.445380.620.4454Column-wise GC80.650.409580.750.4432Adaptive Field-wise GC80.620.445377.900.4824Adaptive Column-wise GC80.970.442580.900.4430Ablation studyGC showthat gradient clipping on fine-grained granularity yields bet-ter results. The next two lines add the adaptive design tothe clipping on the above two granularities, which adap-tively decide the clipping values for each column (line 8in Alg. 1). The reason that Field-wise adaptive GC fails toachieve a good result is because magnitudes of column gra-dients are different even in a field. Thus, Gradient clippingapplied to a smaller unit yields better performance. CowClip(Adaptive Column-wise GC) outperforms all other methodsin both settings. Hyperparameters for these clipping variantsand more ablation study into the effectiveness of each com-ponent of CowClip can be found in the Appendix , whichshows each component contributes to the final results.ConclusionTo accelerate the training of CTR prediction models on oneGPU, we have explored large batch training and found thatdifferent frequencies hinder the scaling of learning rate andL2-regularization weight when scaling the batch size. Sinceprevious scaling rules used in CV and NLP fail, we proposea novel optimization strategy CowClip with a simple scalingrule to stabilize the training process for large batch train-ing in CTR prediction system. Experiments show that our"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "AUC (%) of different scaling methods on Criteo-seq dataset from 1K to 8K on DeepFM. Ours) 80.48 80.50 80.50 80.49 80.49", "figure_data": "1K2K4K8K128KNo Scaling80.48 80.25 79.79 79.04-Sqrt Scaling80.48 80.26 79.82 79.91-LR Scaling80.48 80.29 80.29 80.03-CowClip ("}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Performance of different scaling methods on Avazu dataset from 1K to 8K on DeepFM.", "figure_data": "1K (1024)2K (2048)4K (4096)8K (8192)AUC (%) LogLoss AUC (%) LogLoss AUC (%) LogLoss AUC (%) LogLossNo Scaling78.840.374878.790.377577.690.395275.850.4411Sqrt Scaling78.840.374878.880.376177.780.392676.230.4299Sqrt Scaling  *78.840.374878.880.375977.980.397676.230.4140LR Scaling78.840.374878.780.376377.720.388376.690.4043n 2 -\u03bb Scaling (Ours)78.840.374878.840.375478.260.381577.240.3912CowClip (Ours)78.830.374878.820.375278.900.375279.060.3740"}, {"figure_label": "1213", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Performance of CowClip methods on Avazu dataset from 1K to 128K on four models. The training time of different methods on Avazu dataset. . Last four are trained with CowClip. 00\u00d7 12.3\u00d7 21\u00d7 31.3\u00d7 43.7\u00d7 \u2020 Trained with 2 GPUs \u2021 Trained with 4 GPUs.", "figure_data": "Baseline1K2K4K8K16K32K64K128KDeepFM (Guo et al. 2018)AUC (%) Logloss78.84 0.374878.83 0.3751 0.3752 0.3752 0.3740 0.3759 0.3780 0.3781 0.3758 78.82 78.90 79.06 79.01 78.82 78.82 78.80W&D (Cheng et al. 2016)AUC (%) Logloss78.80 0.375278.80 0.3754 0.3752 0.3752 0.3744 0.3754 0.3782 0.3784 0.3758 78.81 78.90 79.06 79.03 78.82 78.81 78.79DCN (Wang et al. 2017)AUC (%) Logloss78.82 0.374978.80 0.3754 0.3752 0.3751 0.3744 0.3760 0.3787 0.3780 0.3758 78.81 78.91 79.05 78.97 78.74 78.78 78.79DCN v2 (Wang et al. 2021)AUC (%) Logloss78.84 0.374878.83 0.3750 0.3754 0.3751 0.3742 0.3760 0.3778 0.3779 0.3760 78.82 78.89 79.07 78.97 78.80 78.81 78.75Time (minutes)AUC (%) Logloss 1K2K4K8K16K32K64K128KXDL (Adnan et al. 2021)75.80.3901088474-----FAE (Adnan et al. 2021)77.80.391726261-----DLRM (Naumov et al. 2019)76.60.38716314154-----Hotline (Adnan 2021)76.80.386702824-----DeepFM78.840.3748 210108543017106.74.8W&D78.800.3750 210108543017106.75.0DCN78.820.3749 210108543018117.25.7DCN v278.840.3748 2341266637251918.519.5Speedup (DeepFM)1\u00d7 1.94\u00d7 3.89\u00d7 7."}, {"figure_label": "14", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "More Ablation study of CowClip on Criteo with DeepFM.", "figure_data": "b = 8Kb = 128KAUC (%) LogLoss AUC (%) LogLossCowClip w./ Linear Scale on DensedivergedivergedivergedivergeCowClip w./ Empirical Scale80.850.443079.830.4539CowClip w.o. \u03b680.960.442680.880.4438CowClip w.o. warmup80.970.442280.520.4463CowClip w.o. large init weight80.920.443280.900.4431CowClip80.970.442580.900.4430"}], "formulas": [{"formula_id": "formula_0", "formula_text": "count(id fj k ) = N i=1 \u03b4(x fj i [k] = 1), P(id fj k \u2208 x) = count(id fj k ) N ,", "formula_coordinates": [3.0, 103.75, 616.06, 138.99, 59.76]}, {"formula_id": "formula_1", "formula_text": "g t = x\u2208Bt \u2207L(w, x) + \u03bb 2 \u2022 w 2 2 w t+1 = \u03b7 \u2022 Opt(w t , g t ).", "formula_coordinates": [3.0, 366.43, 436.13, 144.15, 41.95]}, {"formula_id": "formula_2", "formula_text": "\u03b7 \u2192 \u221a s \u2022 \u03b7, \u03bb \u2192 \u221a s \u2022 \u03bb Scaling Rule 2 (Linear Scaling) When scaling batch size from b to s \u2022 b, do as follows: \u03b7 \u2192 s \u2022 \u03b7, \u03bb \u2192 \u03bb", "formula_coordinates": [3.0, 319.5, 646.77, 238.5, 57.16]}, {"formula_id": "formula_3", "formula_text": "P(id fj k \u2208 B) = 1 \u2212 (1 \u2212 P(id fj k \u2208 x)) b .", "formula_coordinates": [4.0, 93.88, 466.15, 158.74, 14.29]}, {"formula_id": "formula_4", "formula_text": "P(id fj k \u2208 B) \u2248 1 id fj k is frequent b \u2022 P(id fj k \u2208 x) id fj k is infrequent . (1)", "formula_coordinates": [4.0, 61.52, 546.47, 230.98, 28.39]}, {"formula_id": "formula_5", "formula_text": "s i=1 B i with b = |B | = s \u2022 b, we have E[\u2206w] = E[\u03b7 \u2022 \u03b4(id fj k \u2208 B ) \u2022 1 b x\u2208B \u2207L(w, x)] = \u03b7 \u2022 P(id fj k \u2208 B ) \u2022 E[\u2207L(w, x)].", "formula_coordinates": [4.0, 54.0, 605.52, 238.0, 79.59]}, {"formula_id": "formula_6", "formula_text": "E[\u2206w] = E[\u03b7 \u2022 s i=1 \u03b4(id fj k \u2208 B i ) \u2022 1 b x\u2208Bi \u2207L(w i , x)] \u2248 \u03b7 \u2022 s \u2022 P(id fj k \u2208 B) \u2022 E[\u2207L(w, x)].", "formula_coordinates": [4.0, 332.68, 74.54, 212.14, 49.61]}, {"formula_id": "formula_7", "formula_text": "P(id fj k \u2208 B ) \u2248 s \u2022 P(id fj k \u2208 B).", "formula_coordinates": [4.0, 374.34, 207.63, 128.82, 14.29]}, {"formula_id": "formula_8", "formula_text": "E[g] = 1 b E[\u03b4(id fj k \u2208 B)", "formula_coordinates": [4.0, 359.02, 426.57, 96.78, 22.31]}, {"formula_id": "formula_9", "formula_text": "= P(id fj k \u2208 B) \u2022 E[\u2207L(w, x)].(2)", "formula_coordinates": [4.0, 379.76, 457.74, 178.24, 14.29]}, {"formula_id": "formula_10", "formula_text": "P(id fj k \u2208 B) = \u03bb b \u2022 P(id fj k \u2208 x)", "formula_coordinates": [4.0, 375.48, 583.06, 123.76, 26.96]}, {"formula_id": "formula_11", "formula_text": "\u03b7 \u03bb \u2248 s\u03b7\u03bb.", "formula_coordinates": [5.0, 148.98, 313.16, 48.54, 8.74]}, {"formula_id": "formula_12", "formula_text": "\u03b7 e \u2192 \u03b7 e , \u03bb e \u2192 s 2 \u2022 \u03bb e", "formula_coordinates": [5.0, 123.08, 436.97, 99.85, 11.72]}, {"formula_id": "formula_13", "formula_text": "g \u2192 min{1, clip t g } \u2022 g", "formula_coordinates": [5.0, 120.19, 567.67, 105.76, 21.06]}, {"formula_id": "formula_14", "formula_text": "fj k ].", "formula_coordinates": [5.0, 71.92, 670.15, 11.8, 14.29]}, {"formula_id": "formula_15", "formula_text": "clip(id fj k ) = cnt(id fj k ) \u2022 max{r \u2022 w e t [id fj k ] , \u03b6}", "formula_coordinates": [5.0, 338.1, 479.0, 201.29, 14.29]}, {"formula_id": "formula_16", "formula_text": "y = w 0 + n i=1 w i x i + n i=1 n j=i+1 v i , v j x i x j .", "formula_coordinates": [11.0, 213.99, 249.66, 184.03, 30.32]}, {"formula_id": "formula_17", "formula_text": "y = w 0 + n i=1 w i x i .", "formula_coordinates": [11.0, 265.27, 323.14, 81.45, 30.32]}, {"formula_id": "formula_18", "formula_text": "x +1 = x 0 (W x + b ) + x .", "formula_coordinates": [11.0, 237.8, 428.74, 136.39, 9.68]}, {"formula_id": "formula_19", "formula_text": "E[\u2206w] = \u03b7 s i=1 E[ 1 b \u2022 x\u2208Bt,i \u2207L(w t,i , x)] = \u03b7 s i=1 E[\u2207L(w t,i , x)],", "formula_coordinates": [12.0, 172.59, 285.95, 266.82, 30.47]}, {"formula_id": "formula_20", "formula_text": "E[\u2206w] = \u03b7 E[ 1 b \u2022 x\u2208B t \u2207L(w t , x)] = \u03b7 E[\u2207L(w t , x)].(3)", "formula_coordinates": [12.0, 193.1, 338.29, 364.9, 28.84]}, {"formula_id": "formula_21", "formula_text": "cov(\u011d,\u011d) = ( 1 b \u2212 1 N ) 1 N N i=1\u011d i\u011d i .", "formula_coordinates": [12.0, 232.5, 450.78, 147.0, 30.32]}, {"formula_id": "formula_22", "formula_text": "cov(\u2206w, \u2206w) = cov(\u03b7\u011d, \u03b7\u011d) \u2248 \u03b7 2 b \u2022 N N i=1\u011d i\u011d i .(4)", "formula_coordinates": [12.0, 203.45, 509.27, 354.55, 30.32]}, {"formula_id": "formula_23", "formula_text": "w t+1 = w t (1 \u2212 \u03b7 \u03bb w),", "formula_coordinates": [12.0, 257.78, 598.01, 96.43, 9.65]}, {"formula_id": "formula_24", "formula_text": "w t+1 = w t (1 \u2212 \u03b7\u03bbw) s = w t (1 \u2212 s\u03b7\u03bbw).", "formula_coordinates": [12.0, 222.44, 627.55, 167.12, 11.72]}, {"formula_id": "formula_25", "formula_text": "\u03b7 \u03bb \u2248 s\u03b7\u03bb.", "formula_coordinates": [12.0, 281.73, 665.22, 48.54, 8.74]}, {"formula_id": "formula_26", "formula_text": "g = 1 b \u03b4(id fj k \u2208 B) x\u2208B \u2207L(w, x) = \u03b4(id fj k \u2208 B)\u011d.", "formula_coordinates": [13.0, 203.25, 99.24, 205.51, 26.8]}, {"formula_id": "formula_27", "formula_text": "cov(\u2206g, \u2206g) = E[gg ] \u2212 E[g] E[g ] = 1 b 2 b i=1 b j=1 E[\u03b4(id fj k \u2208 B)\u03b4(id fj k \u2208 B )]\u011d i\u011dj \u2212 P(id fj k \u2208 B) 2 E[\u011d] E[\u011d ] = P(id fj k \u2208 B) E[\u011d\u011d ] \u2212 P(id fj k \u2208 B) 2 E[\u011d] E[\u011d ].", "formula_coordinates": [13.0, 125.4, 153.76, 361.2, 65.06]}, {"formula_id": "formula_28", "formula_text": "cov(\u2206w, \u2206w) = cov(\u03b7\u011d, \u03b7\u011d) \u2248 P(id fj k \u2208 B) \u2022 \u03b7 2 b \u2022 N N i=1\u011d i\u011d i .", "formula_coordinates": [13.0, 175.33, 297.02, 261.34, 30.32]}, {"formula_id": "formula_29", "formula_text": "cov(\u2206w, \u2206w) = cov(\u03b7\u011d, \u03b7\u011d) \u2248 \u03b7 2 P(id fj k \u2208 x) \u2022 N N i=1\u011d i\u011d i .", "formula_coordinates": [13.0, 181.81, 362.82, 248.38, 30.63]}, {"formula_id": "formula_30", "formula_text": "E[G] = E[ 1 b b i=1 \u2207 w L(w, x i )] = E[\u2207 w L(w, x)].", "formula_coordinates": [13.0, 208.15, 467.87, 195.7, 30.32]}, {"formula_id": "formula_31", "formula_text": "E[G] = c \u2022 E[\u2207 w L(w, x)].", "formula_coordinates": [13.0, 252.73, 546.15, 106.55, 9.65]}, {"formula_id": "formula_32", "formula_text": "E[\u2206w] = E[w t \u2212 w t+1 ] = \u03b7 \u2022 (E[\u2207 wt L(w t , x)] \u2212 \u03bb 2 w t ).", "formula_coordinates": [13.0, 191.23, 581.63, 229.54, 22.31]}, {"formula_id": "formula_33", "formula_text": "E[\u2206w] = \u03b7 \u2022 (c \u2022 E[\u2207 wt L(w t , x)] \u2212 \u03bb 2 w t ) = c\u03b7 \u2022 (\u2022 E[\u2207 wt L(w t , x)] \u2212 \u03bb 2c w t ).", "formula_coordinates": [13.0, 148.93, 627.52, 314.15, 22.31]}, {"formula_id": "formula_34", "formula_text": "E[\u2206w] = \u03b7 E[ m t \u221a v t + ].(5)", "formula_coordinates": [13.0, 256.84, 684.17, 301.16, 23.28]}, {"formula_id": "formula_35", "formula_text": "g = s i=1 g s = \u221a sg.", "formula_coordinates": [17.0, 259.32, 227.84, 93.36, 30.32]}], "doi": ""}