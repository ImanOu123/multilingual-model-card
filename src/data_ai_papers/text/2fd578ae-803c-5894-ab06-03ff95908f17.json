{"title": "Deep Neural Decision Forests", "authors": "Peter Kontschieder; Madalina Fiterau; Antonio Criminisi; Samuel Rota Bul\u00f2; Microsoft Research; Carnegie Mellon University; Fondazione Bruno Kessler", "pub_date": "", "abstract": "We present Deep Neural Decision Forests -a novel approach that unifies classification trees with the representation learning functionality known from deep convolutional networks, by training them in an end-to-end manner. To combine these two worlds, we introduce a stochastic and differentiable decision tree model, which steers the representation learning usually conducted in the initial layers of a (deep) convolutional network. Our model differs from conventional deep networks because a decision forest provides the final predictions and it differs from conventional decision forests since we propose a principled, joint and global optimization of split and leaf node parameters. We show experimental results on benchmark machine learning datasets like MNIST and ImageNet and find onpar or superior results when compared to state-of-the-art deep models. Most remarkably, we obtain Top5-Errors of only 7.84%/6.38% on ImageNet validation data when integrating our forests in a single-crop, single/seven model GoogLeNet architecture, respectively. Thus, even without any form of training data set augmentation we are improving on the 6.67% error obtained by the best GoogLeNet architecture (7 models, 144 crops).", "sections": [{"heading": "Introduction", "text": "Random forests [1,4,7] have a rich and successful history in machine learning in general and the computer vision community in particular. Their performance has been empirically demonstrated to outperform most state-of-the-art learners when it comes to handling high dimensional data problems [6], they are inherently able to deal with multiclass problems, are easily distributable on parallel hardware architectures while being considered to be close to an ideal learner [11]. These facts and many (computationally) appealing properties make them attractive for various research areas and commercial products. In such a way, random forests could be used as out-of-the-box classifiers for many computer vision tasks such as image classification [3] or semantic segmentation [5,32], where the input space (and corresponding data representation) they operate on is typically predefined and left unchanged.\nOne of the consolidated findings of modern, (very) deep learning approaches [19,23,36] is that their joint and unified way of learning feature representations together with their classifiers greatly outperforms conventional feature descriptor & classifier pipelines, whenever enough training data and computation capabilities are available. In fact, the recent work in [12] demonstrated that deep networks could even outperform humans on the task of image classification. Similarly, the success of deep networks extends to speech recognition [38] and automated generation of natural language descriptions of images [9].\nAddressing random forests to learn both, proper representations of the input data and the final classifiers in a joint manner is an open research field that has received little attention in the literature so far. Notable but limited exceptions are [18,24] where random forests were trained in an entangled setting, stacking intermediate classifier outputs with the original input data. The approach in [28] introduced a way to integrate multi-layer perceptrons as split functions, however, representations were learned only locally at split node level and independently among split nodes. While these attempts can be considered early forms of representation learning in random forests, their prediction accuracies remained below the state-of-the-art.\nIn this work we present Deep Neural Decision Forestsa novel approach to unify appealing properties from representation learning as known from deep architectures with the divide-and-conquer principle of decision trees. We introduce a stochastic, differentiable, and therefore backpropagation compatible version of decision trees, guiding the representation learning in lower layers of deep convolutional networks. Thus, the task for representation learning is to reduce the uncertainty on the routing decisions of a sample taken at the split nodes, such that a globally defined loss function is minimized.\nAdditionally, the optimal predictions for all leaves of our trees given the split decisions can be obtained by minimizing a convex objective and we provide an optimization algorithm for it that does not resort to tedious step-size selection. Therefore, at test time we can take the optimal decision for a sample ending up in the leaves, with respect to all the training data and the current state of the network.\nOur realization of back-propagation trees is modular and we discuss how to integrate them in existing deep learning frameworks such as Caffe [16], MatConvNet [37], Minerva 1 , etc. supported by standard neural network layer implementations. Of course, we also maintain the ability to use back-propagation trees as (shallow) stand-alone classifiers. We demonstrate the efficacy of our approach on a range of datasets, including MNIST and ImageNet, showing superior or on-par performance with the state-of-the-art.\nRelated Works. The main contribution of our work relates to enriching decision trees with the capability of representation learning, which requires a tree training approach departing from the prevailing greedy, local optimization procedures typically employed in the literature [7]. To this end, we will present the parameter learning task in the context of empirical risk minimization. Related approaches of tree training via global loss function minimization were e.g. introduced in [30] where during training a globally tracked weight distribution guides the optimization, akin to concepts used in boosting. The work in [15] introduced regression tree fields for the task of image restoration, where leaf parameters were learned to parametrize Gaussian conditional random fields, providing different types of interaction. In [35], fuzzy decision trees were presented, including a training mechanism similar to back-propagation in neural networks. Despite sharing some properties in the way parent-child relationships are modeled, our work differs as follows: i) We provide a globally optimal strategy to estimate predictions taken in the leaves (whereas [35] simply uses histograms for probability mass estimation). ii) The aspect of representation learning is absent in [35] and iii) We do not need to specify additional hyper-parameters which they used for their routing functions (which would potentially account for millions of additional hyper-parameters needed in the ImageNet experiments). The work in [24] investigated the use of sigmoidal functions for the task of differentiable information gain maximization. In [25], an approach for global tree refinement was presented, proposing joint optimization of leaf node parameters for trained trees together with pruning strategies to counteract overfitting. The work in [26] describes how (greedily) trained, cascaded random forests can be represented by deep networks (and refined by additional training), building upon the work 1 https://github.com/dmlc/minerva in [31] (which describes the mapping of decision trees into multi-layer neural networks).\nIn [2], a Bayesian approach using priors over all parameters is introduced, where also sigmoidal functions are used to model splits, based on linear functions on the input (c.f . the non-Bayesian work from Jordan [17]). Other hierarchical mixture of expert approaches can also be considered as tree-structured models, however, lacking both, representation learning and ensemble aspects.", "publication_ref": ["b0", "b3", "b6", "b5", "b10", "b2", "b4", "b31", "b18", "b22", "b35", "b11", "b37", "b8", "b17", "b23", "b27", "b15", "b36", "b0", "b6", "b29", "b14", "b34", "b34", "b34", "b23", "b24", "b25", "b0", "b30", "b1", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Decision Trees with Stochastic Routing", "text": "Consider a classification problem with input and (finite) output spaces given by X and Y, respectively. A decision tree is a tree-structured classifier consisting of decision (or split) nodes and prediction (or leaf) nodes. Decision nodes indexed by N are internal nodes of the tree, while prediction nodes indexed by L are the terminal nodes of the tree. Each prediction node \u2113 \u2208 L holds a probability distribution \u03c0 \u2113 over Y. Each decision node n \u2208 N is instead assigned a decision function d n (\u2022; \u0398) : X \u2192 [0, 1] parametrized by \u0398, which is responsible for routing samples along the tree. When a sample x \u2208 X reaches a decision node n it will be sent to the left or right subtree based on the output of d n (x; \u0398). In standard decision forests, d n is binary and the routing is deterministic. In this paper we will consider rather a probabilistic routing, i.e. the routing direction is the output of a Bernoulli random variable with mean d n (x; \u0398). Once a sample ends in a leaf node \u2113, the related tree prediction is given by the class-label distribution \u03c0 \u2113 . In the case of stochastic routings, the leaf predictions will be averaged by the probability of reaching the leaf. Accordingly, the final prediction for sample x from tree T with decision nodes parametrized by \u0398 is given by\nP T [y|x, \u0398, \u03c0] = \u2113\u2208L \u03c0 \u2113y \u00b5 \u2113 (x|\u0398) ,(1)\nwhere \u03c0 = (\u03c0 \u2113 ) \u2113\u2208L and \u03c0 \u2113y denotes the probability of a sample reaching leaf \u2113 to take on class y, while \u00b5 \u2113 (x|\u0398) is regarded as the routing function providing the probability that sample x will reach leaf \u2113. Clearly, \u2113 \u00b5 \u2113 (x|\u0398) = 1 for all x \u2208 X . In order to provide an explicit form for the routing function we introduce the following binary relations that depend on the tree's structure: \u2113 \u0582 n, which is true if \u2113 belongs to the left subtree of node n, and n \u0581 \u2113, which is true if \u2113 belongs to the right subtree of node n. We can now exploit these relations to express \u00b5 \u2113 as follows: product in ( 2) runs over all nodes, only decision nodes along the path from the root node to the leaf \u2113 contribute to \u00b5 \u2113 , because for all other nodes 1 \u2113\u0582n and 1 n\u0581\u2113 will be both 0 (assuming 0 0 = 1, see Fig. 1 for an illustration).\n\u00b5 \u2113 (x|\u0398) = n\u2208N d n (x; \u0398) 1 \u2113\u0582nd n (x; \u0398) 1 n\u0581\u2113 , (2)", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Decision nodes", "text": "In the rest of the paper we consider decision functions delivering a stochastic routing with decision functions defined as follows:\nd n (x; \u0398) = \u03c3(f n (x; \u0398)) ,(3)\nwhere \u03c3(x) = (1 + e \u2212x ) \u22121 is the sigmoid function, and f n (\u2022; \u0398) : X \u2192 R is a real-valued function depending on the sample and the parametrization \u0398. Further details about the functions f n can be found in Section 4.1, but intuitively depending on how we choose these functions we can model trees having shallow decisions (e.g. such as in oblique forests [13]) as well as deep ones.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "Forests of decision trees.", "text": "A forest is an ensemble of decision trees F = {T 1 , . . . , T k }, which delivers a prediction for a sample x by averaging the output of each tree, i.e.\nP F [y|x] = 1 k k h=1 P T h [y|x] ,(4)\nomitting the tree parameters for notational convenience.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Trees by Back-Propagation", "text": "Learning a decision tree modeled as in Section 2 requires estimating both, the decision node parametrizations \u0398 and the leaf predictions \u03c0. For their estimation we adhere to the minimum empirical risk principle with respect to a given data set T \u2282 X \u00d7 Y under log-loss, i.e. we search for the minimizers of the following risk term:\nR(\u0398, \u03c0; T ) = 1 |T | (x,y)\u2208T L(\u0398, \u03c0; x, y) ,(5)\nwhere L(\u0398, \u03c0; x, y) is the log-loss term for the training sample (x, y) \u2208 T , which is given by\nL(\u0398, \u03c0; x, y) = \u2212 log(P T [y|x, \u0398, \u03c0]) ,(6)\nand P T is defined as in (1). We consider a two-step optimization strategy, described in the rest of this section, where we alternate updates of \u0398 with updates of \u03c0 in a way to minimize (5).", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Decision Nodes", "text": "All decision functions depend on a common parameter \u0398, which in turn parametrizes each function f n in (3). So far, we made no assumptions about the type of functions in f n , therefore nothing prevents the optimization of the risk with respect to \u0398 for a given \u03c0 from eventually becoming a difficult and large-scale optimization problem. As an example, \u0398 could absorb all the parameters of a deep neural network having f n as one of its output units. For this reason, we will employ a Stochastic Gradient Descent (SGD) approach to minimize the risk with respect to \u0398, as commonly done in the context of deep neural networks:\n\u0398 (t+1) = \u0398 (t) \u2212 \u03b7 \u2202R \u2202\u0398 (\u0398 (t) , \u03c0; B) = \u0398 (t) \u2212 \u03b7 |B| (x,y)\u2208B \u2202L \u2202\u0398 (\u0398 (t) , \u03c0; x, y)(7)\nHere, 0 < \u03b7 is the learning rate and B \u2286 T is a random subset (a.k.a. mini-batch) of samples from the training set.\nAlthough not shown explicitly, we additionally consider a momentum term to smooth out the variations of the gradients. The gradient of the loss L with respect to \u0398 can be decomposed by the chain rule as follows\n\u2202L \u2202\u0398 (\u0398, \u03c0; x, y) = n\u2208N \u2202L(\u0398, \u03c0; x, y) \u2202f n (x; \u0398) \u2202f n (x; \u0398) \u2202\u0398 .(8)\nHere, the gradient term that depends on the decision tree is given by\n\u2202L(\u0398, \u03c0; x, y) \u2202f n (x; \u0398) = d n (x; \u0398)A nr \u2212d n (x; \u0398)A n l , (9)\nwhere n l and n r indicate the left and right child of node n, respectively, and we define A m for a generic node m \u2208 N as\nA m = \u2113\u2208Lm \u03c0 \u2113y \u00b5 \u2113 (x|\u0398) P T [y|x, \u0398, \u03c0] .\nWith L m \u2286 L we denote the set of leaves held by the subtree rooted in node m. Detailed derivations of ( 9) can be found in Section 2 of the supplementary document. Moreover, in Section 4 we describe how A m can be efficiently computed for all nodes m with a single pass over the tree.\nAs a final remark, we considered also an alternative optimization procedure to SGD, namely Resilient Back-Propagation (RPROP) [27], which automatically adapts a specific learning rate for each parameter based on the sign change of its risk partial derivative over the last iteration.", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Learning Prediction Nodes", "text": "Given the update rules for the decision function parameters \u0398 from the previous subsection, we now consider the problem of minimizing ( 5) with respect to \u03c0 when \u0398 is fixed, i.e. min \u03c0 R(\u0398, \u03c0; T ) .\nThis is a convex optimization problem and a global solution can be easily recovered. A similar problem has been encountered in the context of decision trees in [28], but only at the level of a single node. In our case, however, the whole tree is taken into account, and we are jointly estimating all the leaf predictions.\nIn order to compute a global minimizer of ( 10) we propose the following iterative scheme:\n\u03c0 (t+1) \u2113y = 1 Z (t) \u2113 (x,y \u2032 )\u2208T 1 y=y \u2032 \u03c0 (t) \u2113y \u00b5 \u2113 (x|\u0398) P T [y|x, \u0398, \u03c0 (t) ] ,(11)\nfor all \u2113 \u2208 L and y \u2208 Y, where\nZ (t)\n\u2113 is a normalizing factor ensuring that y \u03c0 (t+1) \u2113y = 1. The starting point \u03c0 (0) can be arbitrary as long as every element is positive. A typical choice is to start from the uniform distribution in all leaves, i.e. \u03c0 (0) \u2113y = |Y| \u22121 . It is interesting to note that the update rule in (11) is step-size free and it guarantees a strict decrease of the risk at each update until a fixed-point is reached (see proof in supplementary material).\nAs opposed to the update strategy for \u0398, which is based on mini-batches, we adopt an offline learning approach to obtain a more reliable estimate of \u03c0, because suboptimal predictions in the leaves have a strong impact on the final prediction. Moreover, we interleave the update of \u03c0 with a whole epoch of stochastic updates of \u0398 as described in the previous subsection.", "publication_ref": ["b27"], "figure_ref": [], "table_ref": []}, {"heading": "Learning a Forest", "text": "So far we have dealt with a single decision tree setting. Now, we consider an ensemble of trees F, where all trees can possibly share same parameters in \u0398, but each tree can have a different structure with a different set of decision functions (still defined as in (3)), and independent leaf predictions \u03c0.\nSince each tree in forest F has its own set of leaf parameters \u03c0, we can update the prediction nodes of each tree independently as described in Subsection 3.2, given the current estimate of \u0398.\nAs for \u0398, instead, we randomly select a tree in F for each mini-batch and then we proceed as detailed in Subsection 3.1 for the SGD update. This strategy somewhat resembles the basic idea of Dropout [34], where each SGD update is potentially applied to a different network topology, which is sampled according to a specific distribution. In addition, updating individual trees instead of the entire forest reduces the computational load during training.\nDuring test time, as shown in ( 4), the prediction delivered by each tree is averaged to produce the final outcome.", "publication_ref": ["b33"], "figure_ref": [], "table_ref": []}, {"heading": "Summary of the Learning Procedure", "text": "The learning procedure is summarized in Algorithm 1. We start with a random initialization of the decision nodes parameters \u0398 and iterate the learning procedure for a predetermined number of epochs, given a training set T . At each epoch, we initially obtain an estimation of the prediction node parameters \u03c0 given the actual value of \u0398 by running the iterative scheme in (11), starting from the uniform distribution in each leaf, i.e. \u03c0 (0) \u2113y = |Y| \u22121 . Then we split the training set into a random sequence of mini-batches and we perform for each mini-batch a SGD update of \u0398 as in (7). After each epoch we might eventually change the learning rate according to pre-determined schedules.\nMore details about the computation of some tree-specific terms are given in the next section. for all B: mini-batch from T do 6:\nUpdate \u0398 by SGD step in (7)\n7:\nend for 8: end for 4. Implementation Notes", "publication_ref": ["b10", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Decision Nodes", "text": "We have defined decision functions d n in terms of realvalued functions f n (\u2022; \u0398), which are not necessarily independent, but coupled through the shared parametrization \u0398. Our intention is to endow the trees with feature learning capabilities by embedding functions f n within a deep convolutional neural network with parameters \u0398. In the specific, we can regard each function f n as a linear output unit of a deep network that will be turned into a probabilistic routing decision by the action of d n , which applies a sigmoid activation to obtain a response in the [0, 1] range. Fig. 2 provides a schematic illustration of this idea, showing how  3). Each output of fn is brought in correspondence with a split node in a tree, eventually producing the routing (split) decisions dn(x) = \u03c3(fn(x)). The order of the assignments of output units to decision nodes can be arbitrary (the one we show allows a simple visualization). The circles at bottom correspond to leaf nodes, holding probability distributions \u03c0 \u2113 as a result from solving the convex optimization problem defined in Equ. (10). decision nodes can be implemented by using typically available fully-connected (or inner-product) and sigmoid layers in DNN frameworks like Caffe or MatConvNet. Easy to see, the number of split nodes is determined by the number of output nodes of the preceding fully-connected layer.\nd 1 d 2 d 4 \u03c0 1 \u03c0 2 d 5 \u03c0 3 \u03c0 4 d 3 d 6 \u03c0 5 \u03c0 6 d 7 \u03c0 7 \u03c0 8 f 7 f 3 f 6 f 1 f 5 f 2 f 4 d 8\nUnder the proposed construction, the output units of the deep network are therefore not directly delivering the final predictions, e.g. through a Softmax layer, but each unit is responsible for driving the decision of a node in the forest. Indeed, during the forward pass through the deep network, a data sample x produces soft activations of the routing decisions of the tree that induce via the routing function a mixture of leaf predictions as per (1), which will form the final output. Finally, please note that by assuming linear and independent (via separate parametrizations) functions f n (x; \u03b8 n ) = \u03b8 \u22a4 n x, we recover a model similar to oblique forests [13].", "publication_ref": ["b9", "b12"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Routing Function", "text": "The computation of the routing function \u00b5 \u2113 can be carried out by traversing the tree once. Let \u22a4 \u2208 N be the root node and for each node n \u2208 N let n l and n r denote its left and right child, respectively. We start from the root by setting \u00b5 \u22a4 = 1 and for each node n \u2208 N that we visit in breadth-first order we set \u00b5 n l = d n (x; \u0398)\u00b5 n and \u00b5 nr =d n (x; \u0398)\u00b5 n . At the end, we can read from the leaves the desired values of the routing function.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Decision Nodes", "text": "The forward pass of the back-propagation algorithm precomputes the values of the routing function \u00b5 \u2113 (x; \u0398) and the value of the tree prediction P T [y|x, \u0398, \u03c0] for each sam-ple (x, y) in the mini-batch B. The backward pass requires the computation of the gradient term in ( 9) for each sample (x, y) in the mini-batch. This can be carried out by a single, bottom-up tree traversal. We start by setting\nA \u2113 = \u03c0 \u2113y \u00b5 \u2113 (x; \u0398) P T [y|x, \u0398, \u03c0]\nfor each \u2113 \u2208 L. Then we visit the tree in reversed breadthfirst order (bottom-up). Once in a node n \u2208 N , we can compute the partial derivative in (9) since we can read A n l and A nr from the children, and we set A n = A n l + A nr , which will be required by the parent node.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Learning Prediction Nodes", "text": "Before starting the iterations in (11), we precomputed \u00b5 \u2113 (x; \u0398) for each \u2113 \u2208 L and for each sample x in the training set, as detailed in Subsection 4.2. The iterative scheme requires few iterations to converge to a solution with an acceptable accuracy (20 iterations were enough for all our experiments).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "Our experiments illustrate both, the performance of shallow neural decision forests (sNDFs) as standalone classifiers, as well as their effect when used as classifiers in deep, convolutional neural networks (dNDF). To this end, we evaluate our proposed classifiers on diverse datasets, covering a broad range of classification tasks (ranging from simple binary classification of synthetically generated data up to large-scale image recognition on the 1000-class Ima-geNet dataset).\nG50c [33] Letter [10] USPS [14] MNIST [20] Char74k [8] # ", "publication_ref": ["b32", "b9", "b13", "b19", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Comparison of sNDFs to Forest Classifiers", "text": "We first compared sNDFs against state-of-the-art in terms of stand-alone, off-the-shelf forest ensembles. In order to have a fair comparison, our classifier is built without hidden layers, i.e. we consider feature mappings having the simple form f n (x; \u03b8 n ) = \u03b8 \u22a4 n x. We used the 5 datasets in [30] to compare the performance of sNDFs to that of Alternating Decision Forests (ADF). The details of this experiment are summarized in Tab. 1. For ADF, we provide results reported in their paper and we use their reported maximal tree depth and forest size as an upper bound on the size of our models. Essentially, for each of the datasets, all our trees are less deep and there are fewer of them than in the corresponding ADF models. We used ensembles of different sizes depending on the size of the dataset and the complexity of the learning tasks. In all cases, we use RPROP and the recommended hyper-parameters of the original publication [27] for split node parameter optimization. We report the average error with standard deviations resulting from 10 repetitions of the experiment. Overall, we outperform ADF, though significant results, with p-values less than 0.05, were obtained for the Letter, USPS and Char74k datasets.", "publication_ref": ["b29", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Improving Performance with dNDF", "text": "In the following experiments we integrated our novel forest classifiers in end-to-end image classification pipelines, using multiple convolutional layers for representation learning as typically done in deep learning systems.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MNIST", "text": "We used the MatConvNet library [37] and their reference implementation of LeNet-5 [21], for building an end-to-end digit classification system on MNIST training data, replacing the conventionally used Softmax layer by our forest.\nThe baseline yields an error of 0.9% on test data, which we obtained by re-running the provided example architecture with given settings for optimization, hyper-parameters, etc. By using our proposed dNDF on top of LeNet-5, each decision function being driven by an output unit fully connected to the last hidden layer of the CNN, we can reduce the classification error to 0.7%. The ensemble size was fixed to 10 trees, each with a depth of 5. Please note the positive effect on MNIST performance compared to Section 5.1 when spending additional layers on representation learning.", "publication_ref": ["b36", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "ImageNet", "text": "ImageNet [29] is a benchmark for large-scale image recognition tasks and its images are assigned to a single out of 1000 possible ground truth labels. The dataset contains \u22481.2M training images, 50.000 validation images and 100.000 test images with average dimensionality of 482x415 pixels. Training and validation data are publicly available and we followed the commonly agreed protocol by reporting Top5-Errors on validation data. The GoogLeNet architecture [36], which has a reported Top5-Error of 10.07% when used in a single-model, single-crop setting (see first row in Tab. 3 in [36]) served as basis for our experiments. It uses 3 Softmax layers at different stages of the network to encourage the construction of informative features, due to its very deep architecture. Each of these Softmax layers gets their input from a Fully Connected (FC) layer, built on top of an Average Pool layer, which in turn is built on top of a corresponding Concat layer. Let DC0, DC1 and DC2 be the Concat layers preceding each of the Softmax layers in GoogLeNet. Let AvgPool0, AvgPool1 and AvgPool2 be the Average Pool layers preceding these Softmax layers. To avoid problems with propagation of gradients given the depth of the network and in order to provide the final classification layers with the features obtained in the early stages of the pipeline, we have also supplied DC0 GoogLeNet [36] GoogLeNet\u22c6 dNDF.NET # Models Top5-Errors 10.07% 9.15% 7.89% 8.09% 7.62% 6.67% 10.02% 7.84% 7.08% 6.38% as input to AvgPool1 and AvgPool2 and DC1 as input to AvgPool2. We have implemented this modified network using the Distributed (Deep) Machine Learning Common (DMLC) library [22] 2 and dub it GoogLeNet\u22c6. Its singlemodel, single crop Top5-Error is 10.02% (when trained with SGD, 0.9 momentum, fixed learning rate schedule, decreasing the learning rate by 4% every 8 epochs and minibatches composed of 50 images).\nIn order to obtain a Deep Neural Decision Forest architecture coined dNDF.NET, we have replaced each Softmax layer from GoogLeNet\u22c6 with a forest consisting of 10 trees (each fixed to depth 15), resulting in a total number of 30 trees. We refer to the individual forests as dNDF 0 (closest to raw input), dNDF 1 (replacing middle loss layer in GoogLeNet\u22c6) and dNDF 2 (as terminal layer). We provide a visualization for our dNDF.NET architecture in the supplementary document. Following the implementation guideline in Subsection 4.1, we randomly selected 500 output dimensions of the respectively preceding layers in GoogLeNet\u22c6 for each decision function f n . In such a way, a single FC layer with #trees \u00d7 #split nodes/tree output units provides all the split node inputs per dNDF x . The resulting architecture was implemented in DMLC as well, and we trained the network for 1000 epochs using (mini-) batches composed of 100.000 images (which was feasible due to distribution of the computational load to a cluster of 52 CPUs and 12 hosts, where each host is equipped with a NVIDIA Tesla K40 GPU).\nFor posterior learning, we only update the leaf node predictions of the tree that also receives split node parameter updates, i.e. the randomly selected one as described in Subsection 3.3. To improve computational efficiency, we consider only the samples of the current mini-batch for posterior learning, while all the training data could be used in principle. However, since we use mini-batches composed of 100.000 samples, we can approximate the training set sufficiently well while simultaneously introducing a positive, regularizing effect.\nTab. 2 provides a summary of Top5-Errors on validation data for our proposed dNDF.NET against GoogLeNet and GoogLeNet\u22c6. We ascribe the improvements on the single crop, single model setting (Top5-Error of only 7.84%) to our proposed approach, as the only architectural difference to GoogLeNet\u22c6 (Top5-Error of 10.02%) is deploying 2 https://github.com/dmlc/cxxnet.git our dNDFs. By using an ensemble of 7 dNDF.NETs (still single crop inputs), we can improve further and obtain a Top5-Error of 6.38%, which is better than the best result of 6.67%, obtained with 7 GoogLeNets using 144 crops per image [36]. Next, we discuss some operational characteristics of our single-model, single-crop setting.\nEvaluation of tree nodes Intuitively, a tree-structured classifier aims to produce pure leaf node distributions. This means that the training error is reduced by (repeatedly) partitioning the input space in a way such that it correlates with target classes in Y.\nAnalyzing the outputs of decision functions f n is informative about the routing uncertainties for a given sample x, as it traverses the tree(s). In Fig. 3, we show histograms of all available split node outputs of our three forests (i.e. dNDF 0 , dNDF 1 , dNDF 2 ) for all samples of the validation set after running for 100, 500 and 1000 epochs over the training data. The leftmost histogram (after 100 training epochs) shows the highest uncertainty about the routing direction, i.e. the split decisions are not yet very crisp such that a sample will be routed to many leaf nodes. As training progresses (middle and right plots after 500 and 1000 epochs), we can see how the distributions get very peaked at 0 and 1 (i.e. samples are routed either to the left or right child with low uncertainty), respectively. As a result, the samples will only be routed to a small subset of available leaf nodes with reasonably high probability. In other words, most available leaves will never be reached from a samplecentric view and therefore only a small number of overall paths needs to be evaluated at test time. As part of future work and in order to decrease computational load, we plan to route samples along the trees by sampling from these split distributions, rather than sending them to every leaf node.\nTo assess the quality of the resulting leaf posterior distributions obtained from the global optimization procedure, we illustrate how the mean leaf entropy develops as a function of training epochs (see Fig. 4). To this end, we randomly selected 1024 leaves from all available ones per tree and computed their mean entropy after each epoch. The highest entropy would result from a uniform distribution and is \u22489.96 bits for a 1000-class problem. Instead, we want to obtain highly peaked distributions for the leaf predictors, leading to low entropy. Indeed, the average entropy decreases as training progresses, confirming the efficacy of our proposed leaf node parameter learning approach.  Evaluation of model performance In Fig. 5 we show the development of Top5-Errors for each dNDF x in dNDF.NET as well as their ensemble performances as a function of training epochs. The left plot shows the development over all training epochs (1000 in total) while the right plot is a zoomed view from epochs 500 to 1000 and Top5-Errors 0-12%. As expected, dNDF 0 (which is closest to the input layer) performs worse than dNDF 2 , which constitutes the final layer of dNDF.NET, however, only by 1.34%. Consequently, the computational load between dNDF 0 and dNDF 2 could be traded for a degradation of only 1.34% in accuracy during inference. Conversely, taking the mean over all three dNDFs yields the lowest Top5-Error of 7.84% after 1000 epochs over training data.", "publication_ref": ["b28", "b35", "b35", "b35", "b21", "b35"], "figure_ref": ["fig_3", "fig_4", "fig_5"], "table_ref": []}, {"heading": "Conclusions", "text": "In this paper we have shown how to model and train stochastic, differentiable decision trees, usable as alternative classifiers for end-to-end learning in (deep) convolutional networks. Prevailing approaches for decision tree training typically operate in a greedy and local manner, making representation learning impossible. To overcome this problem, we introduced stochastic routing for decision trees, enabling split node parameter learning via backpropagation. Moreover, we showed how to populate leaf nodes with their optimal predictors, given the current state of the tree/underlying network. We have successfully validated our new decision forest model as stand-alone classifier on standard machine learning datasets and surpass stateof-the-art performance on ImageNet when integrating them in the GoogLeNet architecture, without any form of dataset augmentation.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Shape quantization and recognition with randomized trees", "journal": "", "year": "1997", "authors": "Y Amit; D Geman"}, {"ref_id": "b1", "title": "Bayesian hierarchical mixtures of experts", "journal": "", "year": "2003", "authors": "C M Bishop; M Svens\u00e9n"}, {"ref_id": "b2", "title": "Image classification using random forests and ferns", "journal": "", "year": "2007", "authors": "A Bosch; A Zisserman; X Mu\u00f1oz"}, {"ref_id": "b3", "title": "Random forests", "journal": "", "year": "2001", "authors": "L Breiman"}, {"ref_id": "b4", "title": "Segmentation and recognition using structure from motion point clouds", "journal": "Springer", "year": "2008", "authors": "G J Brostow; J Shotton; J Fauqueur; R Cipolla"}, {"ref_id": "b5", "title": "An empirical evaluation of supervised learning in high dimensions", "journal": "", "year": "2008", "authors": "R Caruana; N Karampatziakis; A Yessenalina"}, {"ref_id": "b6", "title": "Decision Forests in Computer Vision and Medical Image Analysis", "journal": "Springer", "year": "2002", "authors": "A Criminisi; J Shotton"}, {"ref_id": "b7", "title": "Character recognition in natural images", "journal": "", "year": "2009-02", "authors": "T E De Campos; B R Babu; M Varma"}, {"ref_id": "b8", "title": "Deep visual-semantic alignments for generating image descriptions", "journal": "", "year": "", "authors": "A K L Fei-Fei"}, {"ref_id": "b9", "title": "Letter recognition using hollandstyle adaptive classifiers", "journal": "", "year": "1991-03", "authors": "P W Frey; D J Slate"}, {"ref_id": "b10", "title": "The Elements of Statistical Learning", "journal": "Springer", "year": "2009", "authors": "T Hastie; R Tibshirani; J H Friedman"}, {"ref_id": "b11", "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "journal": "", "year": "2015", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b12", "title": "Induction of oblique decision trees", "journal": "Journal of Artificial Intelligence Research", "year": "1993", "authors": "D Heath; S Kasif; S Salzberg"}, {"ref_id": "b13", "title": "A database for handwritten text recognition research", "journal": "", "year": "1994", "authors": "J J Hull"}, {"ref_id": "b14", "title": "Loss-specific training of non-parametric image restoration models: A new state of the art", "journal": "", "year": "2012", "authors": "J Jancsary; S Nowozin; C Rother"}, {"ref_id": "b15", "title": "Caffe: Convolutional architecture for fast feature embedding", "journal": "", "year": "2014", "authors": "Y Jia; E Shelhamer; J Donahue; S Karayev; J Long; R Girshick; S Guadarrama; T Darrell"}, {"ref_id": "b16", "title": "Hierarchical mixtures of experts and the em algorithm", "journal": "", "year": "1994", "authors": "M I Jordan"}, {"ref_id": "b17", "title": "GeoF: Geodesic forests for learning coupled predictors", "journal": "", "year": "2013", "authors": "P Kontschieder; P Kohli; J Shotton; A Criminisi"}, {"ref_id": "b18", "title": "Imagenet classification with deep convolutional neural networks. In (NIPS)", "journal": "", "year": "2012", "authors": "A Krizhevsky; I Sutskever; G Hinton"}, {"ref_id": "b19", "title": "Gradientbased learning applied to document recognition", "journal": "", "year": "1998", "authors": "Y Lecun; L Bottou; Y Bengio; P Haffner"}, {"ref_id": "b20", "title": "Efficient backprop", "journal": "Springer", "year": "1998", "authors": "Y Lecun; L Bottou; G Orr; K Muller"}, {"ref_id": "b21", "title": "Scaling distributed machine learning with the parameter server", "journal": "", "year": "2014", "authors": "M Li; D G Andersen; J W Park; A J Smola; A Ahmed; V Josifovski; J Long; E J Shekita; B.-Y. Su"}, {"ref_id": "b22", "title": "", "journal": "", "year": "2013", "authors": "M Lin; Q Chen; S Yan"}, {"ref_id": "b23", "title": "Entangled forests and differentiable information gain maximization. In Decision Forests in Computer Vision and Medical Image Analysis", "journal": "Springer", "year": "2013", "authors": "A Montillo; J Tu; J Shotton; J Winn; J E Iglesias; D N Metaxas; A Criminisi"}, {"ref_id": "b24", "title": "Global refinement of random forest", "journal": "", "year": "2015", "authors": "S Ren; X Cao; Y Wei; J Sun"}, {"ref_id": "b25", "title": "Relating cascaded random forests to deep convolutional neural networks for semantic segmentation", "journal": "CoRR", "year": "2015", "authors": "D L Richmond; D Kainmueller; M Y Yang; E W Myers; C Rother"}, {"ref_id": "b26", "title": "A direct adaptive method for faster backpropagation learning: The RPROP algorithm", "journal": "", "year": "1993", "authors": "M Riedmiller; H Braun"}, {"ref_id": "b27", "title": "Neural decision forests for semantic image labelling", "journal": "", "year": "2004", "authors": "S Bul\u00f2; P Kontschieder"}, {"ref_id": "b28", "title": "ImageNet Large Scale Visual Recognition Challenge", "journal": "International Journal of Computer Vision", "year": "2014", "authors": "O Russakovsky; J Deng; H Su; J Krause; S Satheesh; S Ma; Z Huang; A Karpathy; A Khosla; M Bernstein; A C Berg; L Fei-Fei"}, {"ref_id": "b29", "title": "Alternating decision forests", "journal": "", "year": "2013", "authors": "S Schulter; P Wohlhart; C Leistner; A Saffari; P M Roth; H Bischof"}, {"ref_id": "b30", "title": "Entropy nets: from decision trees to neural networks", "journal": "Proceedings of the IEEE", "year": "", "authors": "I Sethi"}, {"ref_id": "b31", "title": "Efficient human pose estimation from single depth images", "journal": "", "year": "2013", "authors": "J Shotton; R Girshick; A Fitzgibbon; T Sharp; M Cook; M Finocchio; R Moore; P Kohli; A Criminisi; A Kipman; A Blake"}, {"ref_id": "b32", "title": "Beyond the point cloud: From transductive to semi-supervised learning", "journal": "ACM", "year": "2005", "authors": "V Sindhwani; P Niyogi; M Belkin"}, {"ref_id": "b33", "title": "Dropout: A simple way to prevent neural networks from overfitting", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "N Srivastava; G Hinton; A Krizhevsky; I Sutskever; R Salakhutdinov"}, {"ref_id": "b34", "title": "Globally optimal fuzzy decision trees for classification and regression", "journal": "", "year": "1999", "authors": "A Su\u00e1rez; J F Lutsko"}, {"ref_id": "b35", "title": "Going deeper with convolutions. CoRR, abs/1409", "journal": "", "year": "2007", "authors": "C Szegedy; W Liu; Y Jia; P Sermanet; S Reed; D Anguelov; D Erhan; V Vanhoucke; A Rabinovich"}, {"ref_id": "b36", "title": "Matconvnet -convolutional neural networks for matlab. CoRR, abs/1412", "journal": "", "year": "2014", "authors": "A Vedaldi; K Lenc"}, {"ref_id": "b37", "title": "Automatic Speech Recognition: A Deep Learning Approach", "journal": "Springer", "year": "2014", "authors": "D Yu; L Deng"}], "figures": [{"figure_label": "41", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "4 Figure 1 .41Figure 1. Each node n \u2208 N of the tree performs routing decisions via function dn(\u2022) (we omit the parametrization \u0398). The black path shows an exemplary routing of a sample x along a tree to reach leaf \u21134, which has probability \u00b5 \u2113 4 = d1(x)d2(x)d5(x).", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Algorithm 1 3 :13Learning trees by back-propagation Require: T : training set, nEpochs 1: random initialization of \u0398 2: for all i \u2208 {1, . . . , nEpochs} do Compute \u03c0 by iterating (11) 4:break T into a set of random mini-batches 5:", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 .2Figure2. Illustration how to implement a deep neural decision forest (dNDF). Top: Deep CNN with variable number of layers, subsumed via parameters \u0398. FC block: Fully Connected layer used to provide functions fn(\u2022; \u0398) (here: inner products), described in Equ.(3). Each output of fn is brought in correspondence with a split node in a tree, eventually producing the routing (split) decisions dn(x) = \u03c3(fn(x)). The order of the assignments of output units to decision nodes can be arbitrary (the one we show allows a simple visualization). The circles at bottom correspond to leaf nodes, holding probability distributions \u03c0 \u2113 as a result from solving the convex optimization problem defined in Equ.(10).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 .3Figure3. Histograms over all split node responses of all three forests in dNDF.NET on ImageNet validation data after accomplishing 100 (left), 500 (middle) and 1000 (right) epochs over training data. As training progresses, the split node outputs approach 0 or 1 which corresponds to eliminating routing uncertainty of samples when being propagated through the trees.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 .4Figure 4. Average leaf entropy development as a function of training epochs in dNDF.NET.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 .5Figure 5. Top5-Error plots for individual dNDFx used in dNDF.NET as well as their joint ensemble errors. Left: Plot over all 1000 training epochs. Right: Zoomed version of left plot, showing Top5-Errors from 0-12% between training epochs 500-1000.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Comparison of alternating decision forests (ADF) to shallow neural decision forests (sNDFs, no hidden layers) on selected standard machine learning datasets. Top: Details about datasets. Middle: Average error [%] (with corresponding standard deviation) obtained from 10 repetitions of the experiment. Bottom: Details about the parametrization of our model.", "figure_data": "Train Samples501600072916000066707# Test Samples50040002007100007400# Classes226101062# Input dimensions501625678464Alternating Decision Forest (ADF) [30] 18.71\u00b11.273.52\u00b10.175.59\u00b10.162.71\u00b10.1016.67\u00b10.21Shallow Neural Decision Forest (sNDF)17.4\u00b11.522.92\u00b10.175.01\u00b10.242.8\u00b10.1216.04\u00b10.20Tree input features10 (random)8 (random)10x10 patches 15x15 patches 10 (random)Depth510101012Number of trees507010080200Batch size2550025010001000"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Top5-Errors obtained on ImageNet validation data, comparing our dNDF.NET to GoogLeNet(\u22c6).", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "P T [y|x, \u0398, \u03c0] = \u2113\u2208L \u03c0 \u2113y \u00b5 \u2113 (x|\u0398) ,(1)", "formula_coordinates": [2.0, 357.75, 486.41, 187.36, 20.88]}, {"formula_id": "formula_1", "formula_text": "\u00b5 \u2113 (x|\u0398) = n\u2208N d n (x; \u0398) 1 \u2113\u0582nd n (x; \u0398) 1 n\u0581\u2113 , (2)", "formula_coordinates": [2.0, 334.27, 658.46, 210.84, 22.1]}, {"formula_id": "formula_2", "formula_text": "d n (x; \u0398) = \u03c3(f n (x; \u0398)) ,(3)", "formula_coordinates": [3.0, 115.54, 371.67, 170.83, 10.39]}, {"formula_id": "formula_3", "formula_text": "P F [y|x] = 1 k k h=1 P T h [y|x] ,(4)", "formula_coordinates": [3.0, 112.09, 530.97, 174.27, 30.7]}, {"formula_id": "formula_4", "formula_text": "R(\u0398, \u03c0; T ) = 1 |T | (x,y)\u2208T L(\u0398, \u03c0; x, y) ,(5)", "formula_coordinates": [3.0, 84.08, 686.66, 202.28, 28.32]}, {"formula_id": "formula_5", "formula_text": "L(\u0398, \u03c0; x, y) = \u2212 log(P T [y|x, \u0398, \u03c0]) ,(6)", "formula_coordinates": [3.0, 347.73, 106.77, 197.39, 10.64]}, {"formula_id": "formula_6", "formula_text": "\u0398 (t+1) = \u0398 (t) \u2212 \u03b7 \u2202R \u2202\u0398 (\u0398 (t) , \u03c0; B) = \u0398 (t) \u2212 \u03b7 |B| (x,y)\u2208B \u2202L \u2202\u0398 (\u0398 (t) , \u03c0; x, y)(7)", "formula_coordinates": [3.0, 326.56, 339.0, 218.55, 52.79]}, {"formula_id": "formula_7", "formula_text": "\u2202L \u2202\u0398 (\u0398, \u03c0; x, y) = n\u2208N \u2202L(\u0398, \u03c0; x, y) \u2202f n (x; \u0398) \u2202f n (x; \u0398) \u2202\u0398 .(8)", "formula_coordinates": [3.0, 317.99, 480.76, 227.12, 27.54]}, {"formula_id": "formula_8", "formula_text": "\u2202L(\u0398, \u03c0; x, y) \u2202f n (x; \u0398) = d n (x; \u0398)A nr \u2212d n (x; \u0398)A n l , (9)", "formula_coordinates": [3.0, 322.6, 548.52, 222.51, 23.96]}, {"formula_id": "formula_9", "formula_text": "A m = \u2113\u2208Lm \u03c0 \u2113y \u00b5 \u2113 (x|\u0398) P T [y|x, \u0398, \u03c0] .", "formula_coordinates": [3.0, 368.42, 622.1, 117.12, 25.09]}, {"formula_id": "formula_11", "formula_text": "\u03c0 (t+1) \u2113y = 1 Z (t) \u2113 (x,y \u2032 )\u2208T 1 y=y \u2032 \u03c0 (t) \u2113y \u00b5 \u2113 (x|\u0398) P T [y|x, \u0398, \u03c0 (t) ] ,(11)", "formula_coordinates": [4.0, 71.22, 336.71, 215.14, 32.76]}, {"formula_id": "formula_12", "formula_text": "Z (t)", "formula_coordinates": [4.0, 182.6, 382.0, 16.75, 12.9]}, {"formula_id": "formula_13", "formula_text": "d 1 d 2 d 4 \u03c0 1 \u03c0 2 d 5 \u03c0 3 \u03c0 4 d 3 d 6 \u03c0 5 \u03c0 6 d 7 \u03c0 7 \u03c0 8 f 7 f 3 f 6 f 1 f 5 f 2 f 4 d 8", "formula_coordinates": [5.0, 135.7, 110.07, 276.47, 103.97]}, {"formula_id": "formula_14", "formula_text": "A \u2113 = \u03c0 \u2113y \u00b5 \u2113 (x; \u0398) P T [y|x, \u0398, \u03c0]", "formula_coordinates": [5.0, 385.37, 386.53, 82.02, 23.96]}], "doi": ""}