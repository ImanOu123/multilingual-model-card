{"title": "Computationally Efficient Optimization of Plackett-Luce Ranking Models for Relevance and Fairness", "authors": "Harrie Oosterhuis", "pub_date": "2021-07-07", "abstract": "Recent work has proposed stochastic Plackett-Luce (PL) ranking models as a robust choice for optimizing relevance and fairness metrics. Unlike their deterministic counterparts that require heuristic optimization algorithms, PL models are fully differentiable. Theoretically, they can be used to optimize ranking metrics via stochastic gradient descent. However, in practice, the computation of the gradient is infeasible because it requires one to iterate over all possible permutations of items. Consequently, actual applications rely on approximating the gradient via sampling techniques. In this paper, we introduce a novel algorithm: PL-Rank, that estimates the gradient of a PL ranking model w.r.t. both relevance and fairness metrics. Unlike existing approaches that are based on policy gradients, PL-Rank makes use of the specific structure of PL models and ranking metrics. Our experimental analysis shows that PL-Rank has a greater sample-efficiency and is computationally less costly than existing policy gradients, resulting in faster convergence at higher performance. PL-Rank further enables the industry to apply PL models for more relevant and fairer real-world ranking systems.\u2022 Information systems \u2192 Learning to rank.", "sections": [{"heading": "", "text": "user to find the items they are looking for, even when these items are part of a very large collection [8,10,26].\nTraditionally, ranking systems consist of a scoring function that assigns an individual score to each item, and subsequently, produce rankings by sorting items according to their assigned scores [6,17,18,31]. The crucial difference with LTR and regression or classification is that only the relative differences between scores matter. In other words, in LTR it is not important what the exact score of an item is but how much greater or smaller it is than the scores of the other items. The main difficulty in LTR is that the ranking procedure is deterministic and non-differentiable, since there is no gradient w.r.t. the sorting function. The methods in the LTR field can be divided in applying one of two solutions: optimizing a heuristic function that bounds or approximates the ranking performance [4-6, 17, 18, 31]; or optimizing a probabilistic ranking system [7,21,29,34].\nIn recent years, the popularity of the Plackett-Luce (PL) ranking model has increased [6,11,19,25,28]. It models ranking as a succession of decision problems where each individual decision is made by a PL model (also known as the Soft-Max in deep learning). Previous research from the industry indicates that the probabilistic nature of the PL model leads to more robust performance [3]. In online LTR, it appears the PL model is very good at exploration because it explicitly quantifies its uncertainty [21,23]. Recent work has also posed that the PL model is well suited to address fairness aspects of ranking [11,28], because unlike deterministic models, it can give multiple items an equal probability of being the top-item.\nHowever, calculating the gradient of a PL ranking model requires an iteration over every possible ranking that the model could produce, i.e., every possible permutation. In practice this computational infeasibility is circumvented by estimating the gradient based on rankings sampled from the model [11,22,23,28]. The main downside of this approach is that it can be computationally very costly. This is a particular problem in online settings where optimization is performed periodically as more data is gathered [20,22,23,28].\nIn this paper, we introduce PL-Rank a novel method that can efficiently optimize both relevance and exposure-based fairness ranking metrics or linear combinations of them. We contribute to the theory of the LTR field, by deriving novel estimators that can unbiasedly estimate the gradient of a PL ranking model w.r.t. a ranking metric, on which PL-Rank is build. To the best of our knowledge, PL-Rank is the first LTR method that utilizes specific properties of ranking metrics and the PL-ranking model. Our experimental results show that compared to existing LTR methods, PL-Rank has increased sample-efficiency: it requires less sampled rankings to reach the same performance, and increased computational time-efficiency: PL-Rank requires less time to compute the estimation of the gradient and less computational time to converge at optimal performance. The introduction of PL-Rank makes the optimization of PL ranking models more practical by greatly reducing its computational costs, additionally, these gains also help in the further promotion of fairness aspects of ranking models [11].", "publication_ref": ["b7", "b9", "b25", "b5", "b16", "b17", "b30", "b6", "b20", "b28", "b33", "b5", "b10", "b18", "b24", "b27", "b2", "b20", "b22", "b10", "b27", "b10", "b21", "b22", "b27", "b19", "b21", "b22", "b27", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "One of the earliest LTR approaches is the pairwise approach where the loss function is based on the order of pairs of items [5,17,18]. While pairwise losses are easy to compute and scale well with the number of items to rank, pairwise loss functions do not consider the entire ranking [6,18]. As a result, minimizing a pairwise loss often does not translate to the optimal ranking behavior. Subsequently, the idea of a listwise method that considers the complete ranking was introduced with the ListNet and ListMLE methods [7,34]. These methods optimize PL ranking models to maximize the probability of the optimal ranking. They have three main limitations: (i) They assume there is a single optimal ranking per query, while often there are multiple optimal rankings. (ii) They are not based on actual ranking metrics and thus may not actually maximize the desired metrics over the entire dataset. (iii) They bring substantial computational costs, i.e. the cost of ListNet is so high Cao et al. only optimize the top-1 ranking [7]. Some of these issues are avoided by the later LambdaRank and LambdaMART methods [6]. LambdaRank is an extension of the pairwise RankNet method, where the loss function weights each pair by the absolute difference in Discounted Cumulative Gain (DCG) that would result from swapping the pair. This approach optimizes a deterministic ranking model and also works for other ranking metrics than DCG [6,31]. The Lambda methods are listwise because their gradient is based on the ranking metric values of the current ranking and therefore consider the complete ranking. While initially, there was only empirical evidence for the great performance of Lambda methods in optimizing ranking metrics [6,12]. Recently, Wang et al. [31] proved that the Lamba methods optimize a lower bound on ranking metrics and introduced a novel bound in the form of the LambdaLoss framework [31]. Thus although the Lamba methods are based around ranking metrics and are computationally feasible, they do not optimize metrics directly and are therefore heuristic methods. Another heuristic approach is to replace the rank function in a metric by a differentiable probabilistic approximation, notable examples of this approach are SoftRank [29] and ApproxNDCG [4]. While all of these methods can be useful in practice, none optimize rankings metrics directly in a computationally feasible manner. Interestingly, multiple lines of previous work have found PLranking models to be very effective for various ranking tasks: for result randomization in interleaving [16], multileaving [27] and counterfactual evaluation [22]; for exploration in online LTR [21,23]; for fair distributions of attention exposure [11,28]; and for topic diversity in ranking [32,35]. In particular, Bruch et al. [3] argue that the stochastic nature of PL models results in more robust ranking performance. Furthermore, Bruch et al. show that, with small alterations, many existing LTR methods can adequately optimize PL methods. An interesting property of the PL ranking model is that it has a gradient w.r.t. ranking metrics but that it is generally infeasible to compute, existing work has thus approximated this gradient [11,22,23,28] (see Section 4). The computational costs are particularly relevant because the PL model is often used in online settings where it optimization is performed repeatedly and frequently [22,23,28]. For instance, Oosterhuis and de Rijke [23] show that frequently optimizing the logging-policy model during the gathering of data greatly reduces the data-requirements for online/counterfactual LTR. Similarly, Morik et al. [20] show that to prevent very unfair distributions of exposure, rankings should be updated continuously as more interaction data is gathered. To the best of our knowledge, no previous work has developed a novel LTR method specifically for PL ranking models that optimizes ranking metrics directly nor with a focus on computational efficiency.", "publication_ref": ["b4", "b16", "b17", "b5", "b17", "b6", "b33", "b6", "b5", "b5", "b30", "b5", "b11", "b30", "b30", "b28", "b3", "b15", "b26", "b21", "b20", "b22", "b10", "b27", "b31", "b34", "b2", "b10", "b21", "b22", "b27", "b21", "b22", "b27", "b22", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "RELEVANCE RANKING METRICS", "text": "Generally, LTR methods assume each item has some relevance w.r.t. a query [18], in the context of fairness this is often considered the merit of an item [11]. This is often modelled as the probability that a user finds the item relevant for their issued query . To keep our notation brief, we use to denote this probability: ( = 1| , ) = . Whenever we talk about the relevance of an item, it will be clear from the context what the corresponding query is, hence we keep out of our notation for the sake of brevity. Rankings are ordered lists of items, we use to denote a ranking and for the th item in ranking : = [ 1 , 2 ,..., ], thus = means that is the item at rank in ranking . We will assume that all rankings are of length , for instance, because only items can be displayed. A ranking model can be seen as a distribution over rankings, where ( | ) indicates the probability that ranking is sampled for query by model . For brevity, we will use ( ) = ( | ) as the corresponding query will always be clear from the context.\nThe relevance performance (also called the reward) of a ranking model is represented by the metric value R. Relevance ranking metrics use weights per rank where the relevance of an item at rank is weighted by . For a single query, the metric is computed as an expectation over the ranking behavior of :\nR ( ) = \u2211\ufe01 \u2208 ( | ) \u2211\ufe01 =1 ( = 1| , ) = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 = E \u2211\ufe01 =1 .(1)\nBy choosing accordingly, R ( ) can represent the most common relevance ranking metrics. For example, top-DCG is computed with the following weights:\nDCG@K = 1[ \u2264 ]\nlog 2 ( +1) , precision at with:\nPREC@K = 1 1[ \u2264 ]\n, or Average Relevance Position (ARP) can be represented by: ARP = \u2212 . The overall relevance of a ranking system is simply the expected performance over the distribution of user-issued queries:\nR = E [R ( )] = \u2211\ufe01 \u2208Q ( )R ( ).(2)\nThe value of R is also often called the performance or the reward. Accordingly, LTR for relevance optimizes to maximize R, given the item relevances and according to the chosen metric represented by .\nAs noted in Section 2, the PL model [19,25] has often been deployed to model a probabilistic distribution over rankings [3, 11, 16, 21-23, 28, 32, 35]. In the PL model, an item is chosen from a pool of available items based on the individual scores each item has. For our ranking problem, a learned prediction model predicts the log score of an item w.r.t. to query as ( , ) \u2208 R. For brevity, we again keep the query out of our notation: ( , ) = ( ).\nThe probability that item is chosen to be the th item in ranking from the set of items D is the score of : ( ) , divided by the sum of scores for the items that have not been placed yet:\n( | 1: \u22121 ,D) = ( ) 1[ \u2209 1: \u22121 ] \u2032 \u2208 \\ 1: \u22121 ( \u2032 ) ,(3)\nwhere 1: \u22121 indicates the ranking up to rank \u22121, i.e.,\n1: \u22121 = [ 1 , 2 ,..., \u22121 ].(4)\nAs such the placement probabilities at depend only on the scores of the items not placed before rank . Because the learned predicts the log score, the actual score is always greater than zero: ( ) > 0, consequently, ( | 1: \u22121 ) is always a valid probability distribution over the unplaced items. We note that this probability is extremely similar to the Soft-Max function commonly used in deep learning.\nTo prevent items from appearing in a ranking twice, the probability of ( | 1: \u22121 ,D) = 0 if has already been placed:\n\u2208 1: \u22121 .\nFinally, the probability of a ranking is simply the product of the placement probabilities of each individual item:\n( ) = =1 ( | 1: \u22121 ,D).\n(5)", "publication_ref": ["b17", "b10", "b18", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Computationally Efficient Sampling", "text": "An advantage of the PL ranking model is that rankings can be sampled quite efficiently. At first glance, sampling a ranking may seem computationally costly, as it involves repeatedly sampling from the item distribution and renormalizing it. However, using the Gumbel Softmax trick [14] one can sample an entire ranking without having to calculate any of the actual probabilities [3]. Our goal is to acquire a sampled ranking ( ) from the distribution: ( ) \u223c . Instead of calculating the actual placement probabilities, for each item a sample from the Gumbel distribution is taken:\n( ) \u223c Gumbel(0,0). This can be done by first sampling uniformly from the [0,1] range: ( ) \u223c Uniform(0,1), and then applying:\n( ) = \u2212log(\u2212log( ( ) )\n). Subsequently, per item we take the sum of their Gumbel sample and their log score:\n( ) = ( ) + ( ) .(6)\nFinally, we sort the items according to their\u02c6( ) values, resulting in the sampled ranking:\n( ) = [( ) 1 , ( ) 2 ,\n...,\ns.t. \u2200( ( ) ,( ) ]\n) .\nThis sampling procedure follows the PL distribution of [3,14]. In practice, this means we can sample rankings as quickly as we can sort top-rankings, which translates to a computational complexity of O (|D|log(|D|)).", "publication_ref": ["b13", "b2", "b2", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Basic Policy Gradient Estimation", "text": "As noted by Singh and Joachims [28] and Bruch et al. [3], PL ranking models can be optimized via policy-gradients. They utilize the famous log-trick from the REINFORCE algorithm [33]. We apply the log-trick to Eq. 5 to obtain:\n( ) = ( ) log( ( )) .(8)\nBy combining this result with Eq. 1, we find that the derivate can be expressed as an expectation over the ranking distribution :\nR ( ) = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 = \u2211\ufe01 \u2208 ( ) log( ( )) \u2211\ufe01 =1 = E log( ( )) gradient w.r.t. complete ranking \u2211\ufe01 =1 full reward .(9)\nWe see that this policy gradient is composed of two parts: a gradient w.r.t. the log probability of a complete ranking multiplied by the reward for that ranking. In practice, it is infeasible to compute this gradient exactly since it requires a summation over every possible ranking . Luckily, because the gradient can be expressed as an expectation over ranking w.r.t. to the distribution according to , the gradient can be estimated using a simple sampling strategy. If we sample rankings from for query , with ( ) denoting the th sample, then the gradient can be estimated using:\nR ( ) \u2248 1 \u2211\ufe01 =1 log( ( ( ) )) \u2211\ufe01 =1 ( ) .(10)\nA straightforward implementation first samples rankings using Gumbel sampling (Section 4.1), and then computes the reward for each ranking, to finally use a machine learning framework to compute the gradient w.r.t. the log probabilities: log( ( ( ) )) . This approach works well with currently popular deep-learning frameworks such as PyTorch [24] or Tensorflow [1].\nThis concludes our description of the basic policy gradient approach to optimizing PL ranking models. While this approach works adequately, our results show that this approach is computationally expensive and can have convergency issues when < 1000. Finally, we note that this approach is not specific to PL-ranking models as it essentially just applies the very general REINFORCE algorithm [33]. In contrast, the remainder of this paper will introduce methods that make use of specific PL properties, and as a result, show better performance in our experimental results. between the basic policy gradient estimation and PL-Rank. Unlike existing methods, PL-Rank utilizes specific properties about PL ranking models and ranking metrics.", "publication_ref": ["b27", "b2", "b32", "b23", "b0", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Ranking Metric Based Approximation", "text": "The basic estimator in Eq. 10 only deals with the reward of the entire ranking. This can lead to very unintuitive behavior, for instance, when a ranking is sampled that receives a very high reward but only due the first placed item, the gradient w.r.t. entire ranking will be multiplied with this reward. Thus despite the fact that only the first item contributed positively to the reward, the probability of placement for all items will be increased.\nBy rewriting Eq. 1 we can see that relevance rewards only need to interact with the probability of the ranking up to the corresponding rank:\nR ( ) = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 = \u2211\ufe01 =1 \u2211\ufe01 \u2208 ( ) = \u2211\ufe01 =1 \u2211\ufe01 1: \u2208 ( 1: ) ,(11)\nwhere 1: \u2208 is a summation over all possible (sub)rankings of length according to . In other words, the relevance at any rank only interacts with the probability of the ranking up to :\n(\n). Inuitively this makes sense because the placement of any item after will not affect the previously obtained reward. We can use this fact when estimating the gradient w.r.t. the complete reward.\nBefore we derive the gradient w.r.t. the complete reward, we first consider that the derivate of the log probability of a ranking can be decomposed as a sum over log probabilities of the individual item placements. Using Eq. 5:\n( 1: ) = ( 1: ) log( ( 1: )) = ( 1: ) \u2211\ufe01 =1 log( ( | 1: \u22121 )) .(12)\nWe can now use to get the derivative w.r.t. to R ( ) using Eq. 11 & 12: , note that we use following:\nR ( ) = \u2211\ufe01 =1 \u2211\ufe01 1: \u2208 ( 1: ) = \u2211\ufe01 =1 \u2211\ufe01 1: \u2208 ( 1: ) \u2211\ufe01 =1 log( ( | 1: \u22121 )) = \u2211\ufe01 =1 E 1: \u2211\ufe01 =1 log( ( | 1: \u22121 )) (13) = E \u2211\ufe01 =1 \u2211\ufe01 =1 log( ( | 1: \u22121 )) = E \u2211\ufe01 =1 log( ( | 1: \u22121 ))\n=1 E 1: [ ( 1: )] = E =1 ( 1:\n) , to move the expectation from partial rankings to complete rankings. Eq. 13 shows us that the derivative consists of two parts: the gradient w.r.t. individual item placements and the reward received following each placement. Again, this gradient can be estimated using rankings sampled from :\nR ( ) \u2248 1 \u2211\ufe01 =1 \u2211\ufe01 =1 log( ( ( ) | ( ) 1: \u22121 )) \u2211\ufe01 = ( ) . (14\n)\nWe will call this estimator the placement policy gradient estimator, in contrast with the basic policy gradient estimator (Eq. 10), this estimator weights the gradients of placement probabilities with the observed following rewards. By doing so, it makes use of the structure of ranking metrics and thus is more tailored towards these metrics than the basic estimator.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Computationally Efficient Estimation", "text": "So far our placement policy gradient estimator has made use of the fact that the probability of a ranking is a product of individual placement probabilities, however, it has made no further use of the fact that is a PL ranking model. We will now show that using the knowledge that is a PL model can lead to an estimator that can be computed with greater computational efficiency. We start by taking the derivative of an item placement probability:\n( | 1: \u22121 ) =(15)\n( | 1: \u22121 ) ( ) \u2212 \u2211\ufe01 \u2032 \u2208D ( \u2032 | 1: \u22121 ) ( \u2032 ) .\nWe note that the probability of placing an item that has already been placed is zero: \u2208 1: \u22121 \u2192 ( | 1: \u22121 ) = 0. Combining Eq. 13 & 15 results in the following gradient:\nR ( ) = E \u2211\ufe01 =1 log( ( | 1: \u22121 )) \u2211\ufe01 = = E \u2211\ufe01 =1 ( ) \u2211\ufe01 = (16) \u2212 \u2211\ufe01 =1 \u2211\ufe01 \u2032 \u2208D ( \u2032 | 1: \u22121 ) ( \u2032 ) \u2211\ufe01 = .\nFor the sake of simplicity, we will further derive the resulting two parts of Eq. 16 separately, starting with the first part:\nE \u2211\ufe01 =1 ( ) \u2211\ufe01 = = E \u2211\ufe01 \u2208D ( ) \u2211\ufe01 =1 1[ = ] \u2211\ufe01 = = E \u2211\ufe01 \u2208D ( ) \u2211\ufe01 =1 1[ \u2208 1: ] = \u2211\ufe01 \u2208D ( ) E \u2211\ufe01 =rank( , ) .(17)\nWe see that this first part results in summing over the derivatives of each item score according to model ( ) weighted by the reward expected to follow a placement of .\nThen for the second part of Eq. 16:\nE \u2211\ufe01 =1 \u2211\ufe01 \u2208 D ( | 1: \u22121 ) ( ) \u2211\ufe01 = = E \u2211\ufe01 \u2208 D ( ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2211\ufe01 = = \u2211\ufe01 \u2208 D ( ) E rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2211\ufe01 = ,(18)\nwhere we used the fact that: > rank( , ) \u2192 ( | 1: \u22121 ) = 0. We see that the second part sums over each rank where it multiplies the expected probability that an item was added with the expected following reward. This product represents the risk imposed by an item : if is not placed at then ( | 1: \u22121 ) indicates how likely would have been placed instead of and in which case the following reward = may not have occurred. For cases where is the item at rank : = , the risk stops the log score ( ) from increasing too far as the placement probability ( | 1: \u22121 ) may already be very great. By combining Eq. 16, 17 & 18 we obtain the full derivative: \nWe see that the derivative multiplies the gradient of the item log score ( ) with the expected reward following its placement minus the expected risk imposed by before it is placed. Finally, this gradient can also be estimated using sampled rankings:\nR ( ) \u2248 1 \u2211\ufe01 \u2208 D ( ) \u2211\ufe01 =1 \u2211\ufe01 =rank( , ( ) ) ( ) \u2212 rank( , ( ) ) \u2211\ufe01 =1 ( | ( ) 1: \u22121 ) \u2211\ufe01 = ( ) .(20)\nWe call this estimator PL-Rank-1, to the best of our knowledge this is the first gradient estimation method that is specifically designed for optimizing PL-ranking models w.r.t. ranking metrics. While both the placement policy gradient estimator (Eq. 14) and PL-Rank-1 (Eq. 20) estimate the same gradient, their formulas look radically different.\nA big advantage of PL-Rank-1 is that it can be computed with a timecomplexity of O ( \u2022 \u2022 ). Our experimental results indicate that while both estimators have comparable sample-efficiency, PL-Rank-1 requires considerably less time to compute than using a machine learning framework to automatically compute the placement policy gradient.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Improving Sample-Efficiency", "text": "In Eq. 19 we see that an item receives a positive weight from the expected following reward. Therefore, even when an item has a low probability of being placed it can compensate with a high relevance ( ) to get a positive weight. However, when an estimate of the gradient is based on a low number of samples ( ), they may not include a ranking where such an item is placed at all and thus these items will nevertheless receive a negative weight in the estimate. We propose one last estimator to mitigate this potential issue. First, we can rewrite the expected reward following placement so that the reward obtained from and that from items placed afterwards are separated:\nE \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , )+1 + rank( , ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , )+1 + \u2211\ufe01 =1 ( | 1: \u22121 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , )+1 + rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,(21)\nwhere again we make use of the fact that: > rank( , ) \u2192 ( | \nWe see that the gradient w.r.t. an item's log score ( ) is weighted by the reward after placement (not including the reward from ) plus the expected direct reward (the reward from ) minus the expected risk imposed by before its placement. From Eq. 22 we can derive the following novel estimator:\nR ( ) \u2248 1 \u2211\ufe01 \u2208D ( ) \u2211\ufe01 =1 \u2211\ufe01 =rank( , ( ) )+1 ( ) + rank( , ( ) ) \u2211\ufe01 =1 ( | ( ) 1: \u22121 ) \u2212 \u2211\ufe01 = ( ) .(23)\nWe will call this estimator: PL-Rank-2. Unlike PL-Rank-1 (Eq. 20), PL-Rank-2 can provide a positive weight to items that were not in the top-K of any of the sampled rankings. While this is expected to increase the sample-efficiency, it does not come at the cost of computational complexity as both PL-Rank-1 and PL-Rank-2 have a complexity of O ( \u2022 \u2022 ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The PL-Rank Algorithm", "text": "Finally, we will show how PL-Rank-2 can be implemented efficiently.\nOur goal is to compute a weight per item so that the gradient is estimated by:\nR ( ) \u2248 1 \u2211\ufe01 \u2208 D ( ) ,(24)\nwhere following Eq. 23 these weights are:\n= 1 \u2211\ufe01 =1 \u2211\ufe01 =rank( , )+1 + rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2212 \u2211\ufe01 = . (25\n)\nAlgorithm 1 displays the PL-Rank-2 algorithm in pseudo-code. As input it requires the item collection D, the relevances , the (precomputed) log scores per item ( ) according to the current model , the metric weights per rank , and finally, the number of rankings to sample (Line 1). First, rankings are sampled using Gumbel sampling (Line 2) and zero weights are initialized for every (Line 2). Subsequently, the initial denominator for the PL model is computed and stored (Line 4). Then the algorithm starts iterating over each of the sampled rankings, where first, the rewards following each rank are precomputed (Line 9). Second, it loops over every rank where it adds the following reward to the of at rank in the sampled ranking (Line 11), thus computing the first part of Eq. 23. For every item, the placement probability ( | ( ) ) is computed (Line 14) and multiplied by the difference between the item's direct reward and the following reward (Line 15), this is added to to compute the second part of Eq. 23. Finally, the denominator is updated to account for the item placed at rank (Line 16). Algorithm 1 reveals that PL-Rank-2 can be computed in O ( \u2022 \u2022 ), we note that with small alterations to Line 11 and 15 PL-Rank-1 can be computed with this algorithm as well.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "METHOD: PL-RANK FOR FAIRNESS", "text": "So far we have introduced the PL-Rank algorithms for estimating the gradient of a PL ranking model w.r.t. a relevance metric. However, the applicability of these algorithms are much wider than just relevance metrics, in particular, they can be applied to any exposure-based metrics [2,11,20,28]. Exposure represents the expected number of people that will examine an item. In general, user behavior has position-bias which means that they are less likely to examine an item if it displayed at a lower rank [9,30]. Let the rank weight indicate the probability that a user examines an item at rank , then the exposure an item receives under is:\nE ( , ) = E \u2211\ufe01 =1 1[ = ] = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 1[ = ],(26)\nwhere again for brevity we denote E = E ( , ). Thus E could be interpreted as the probability that a user examines when is deployed. Most fairness metrics for rankings consider how exposure is distributed over items and specifically how fair this distribution is. Regardless of the exact metric, PL-Rank can be applied to a fairness metric F if the chain-rule can be applied as follows:\nF ( ) = \u2211\ufe01 \u2208D F ( ) E E .(27)\nTo derive this gradient, we first note that the E (Eq. 26) and R ( ) (Eq. 1) are equivalent if\n\u2200 \u2032 \u2032 = 1[ \u2032 = ]\n, therefore if we replace in PL-Rank-2 (Eq. 22) accordingly, it will provide us the gradient E . If we combine this fact with Eq. 27 we obtain the following PL-Rank-2 based gradient:\nF ( ) = \u2211\ufe01 \u2208D ( ) E \u2211\ufe01 =rank( , )+1 F ( ) E (28\n)\n+ rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) F ( ) E \u2212 \u2211\ufe01 = F ( ) E .\nIn other words, we can apply PL-Rank by simply replacing the item relevances with the gradients:\nF ( )\nE before computation. Similarly, any linear combination of R and F can be optimized by replacing the relevances with the corresponding linear combination between and F ( ) E . For instance, we can follow Singh and Joachims [28] and choose a disparity-based metric. This metric measures the disparity between two items via a function ( , \u2032 ) and takes the average disparity over all item pairs:\nF ( ) = 1 |D|(|D|\u22121) \u2211\ufe01 \u2208D \u2211\ufe01 \u2032 \u2208D ( , \u2032 ). (29\n)\nSingh and Joachims [28] divide the exposure of an item by its relevance: E to model the proportion between the exposure and the merit of an item. However, in our experimental datasets many items have zero relevances, thus making such a division impossible. Instead, we introduce a novel alternative disparity measure:\n( , \u2032 ) = (E \u2032 \u2212E \u2032 ) 2 . (30\n)\nThis measure looks at the reward item would receive if it had the exposure of \u2032 : E \u2032 , in other words, the reward would receive if it was treated as \u2032 is. This measure can handle items without merit and has the gradient:\nF ( ) E = 4 |D|(|D|\u22121) \u2211\ufe01 \u2032 \u2208 D (E \u2032 \u2212E \u2032 ) \u2032 . (31\n)\nThus in order to apply PL-Rank-2 to this fairness metric, one only needs to compute (or estimate) Eq. 31 and then run Algorithm 1 where the relevances are replaced with the gradients:\nF ( ) E .\nTo conclude, we have shown that PL-Rank-2 can efficiently estimate the gradient of exposure-based fairness metrics, in addition to relevance metrics and any linear combination of any set of these metrics.", "publication_ref": ["b1", "b10", "b19", "b27", "b8", "b29", "b27", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTAL SETUP", "text": "The experiments performed for this paper aim to answer three research questions:\nRQ1 Does PL-Rank require fewer sampled rankings for optimal convergence than policy gradients or LambdaLoss? RQ2 Is less computational time needed to reach high performance with PL-Rank than with policy gradients or LambdaLoss? RQ3 Is PL-Rank also effective at optimizing an exposure-based fairness metric? In other words, we address the sample-efficiency and the computational costs of PL-Rank, in addition to its applicability to rankingfairness metrics.\nTo evaluate these aspects we compare with three baselines: (i) the policy gradient as described in Section 4.2, this is the most basic form of gradient estimation [3,28,33]; (ii) the placement policy gradient as introduced in Section 5.1, this gradient estimation considers individual item placements; and (iii) LambdaLoss [31], a state-of-the-art heuristic for optimizing deterministic ranking models. Following Bruch et al. [3] we apply an average of the gradients over sampled rankings. One can easily extend the existing proof that Lambda-Loss optimizes a lower bound on the performance of a deterministic model [31] to prove our approach also optimizes a lower bound on the expected performance of a stochastic PL ranking model. We note that Bruch et al. [3] introduced additional heuristic methods for PL-Ranking model optimization, due to their high similarity with LambdaLoss we omitted these methods from our baselines. To the best of our knowledge, our choice of baselines cover every category of existing methods for the metric-based optimization of PL-Ranking models.\nWe base our experiments on the three largest publicly-available LTR industry datasets: Yahoo! Webscope [8], MSLR-WEB30k [26], and Istella [10]. Each dataset contains queries, preselected documents per query, and relevance labels indicating the expert-judged relevance of a preselected document w.r.t. a query. Query-document combinations are represented by feature vectors, each dataset varies in the number of features, queries and average number of preselected documents: Yahoo contains 29,921 queries and on average 24 preselected documents per query encoded in 700 features; MSLR has 30,000 queries, on average 125 documents per query and 136 features; and lastly, Istella has 33,118 queries, on average 315 documents per query and 220 features. For our relevance experiments, we optimize top-5 Discounted Cumulative Gain (DCG@5) and choose accordingly (Eq. 3). 1 The relevance of a document is set to a transformation of its label: = 2 relevance_label( ) \u22121. For the fairness experiments, we optimize the disparity metric introduced in Section 6, exposure values E are estimated using 1000 sampled rankings. To compare the computational costs of each method, we ran repeated experiments under identical circumstances on a single Intel Xeon Silver 4214 CPU and measured the time taken to complete each epoch. All our reported results are averaged over 20 independent runs.\nBased on preliminary parameter tuning, we chose to optimize neural networks with two hidden layers of 32 sigmoid activated nodes, we used standard stochastic gradient descent with a 0.01 learning rate for all methods. For calculating gradients we utilize Tensorflow [1] with two exceptions: the sampling of rankings and R ( ) with the PL-Rank algorithm (Algorithm 1) are computed using Numpy [15].", "publication_ref": ["b2", "b27", "b32", "b30", "b2", "b30", "b2", "b7", "b25", "b9", "b0", "b0", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "RESULTS", "text": "Our discussion of the results is divided per research question, our results are displayed in Figure 1, 2 and 3 and Table 1 and 2.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": ["tab_1"]}, {"heading": "Sample-Efficiency", "text": "We will first consider RQ1: whether PL-Rank needs fewer sampled rankings for optimal convergence. Figure 1 shows the performance of PL ranking models trained using different gradient estimation methods with varying numbers of sampled rankings used for estimation. We see that increasing the number of samples beyond = 10 does not have any noticeable effect on the performance of LambdaLoss. In all cases, LambdaLoss converges at suboptimal performance after only a few epochs. In contrast, the basic policy gradient is very affected by and on all three datasets it requires = 1000 to get close to optimal performance, it has extreme convergence issues when = 10. However, in all cases the placement policy gradient  outperforms the basic policy gradient and can converge near optimal performance with = 100. The performance of PL-Rank-1 and the placement policy gradient are indistinguishable. This strongly suggests that PL-Rank-1 and the placement policy gradient perform the same estimation, although Section 8.2 will reveal that PL-Rank-1 does so in a more computationally efficient way. Lastly, we see that PL-Rank-2 outperforms PL-Rank-1 and the policy gradient methods when = 10 on the Yahoo and Istella datasets. Noticeable but limited improvements are also present with = 100 on these datasets. Thus while we can conclude that PL-Rank-2 has the best sampleefficiency of all the methods, the improvements over PL-Rank-1 and the placement policy gradient are most substantial when < 100. Overall, we see that the basic policy gradient and LambdaLoss are poor choices for optimization, despite the fact that these are the methods we find in previous work [3,28]. The choice between PL-Rank-1 and the placement policy gradient does not seem to matter when only the number of epochs is considered. PL-Rank-2 appears the safest choice because, in all tested cases, it either outperforms or has comparable performance to the other methods.\nTo conclude, we answer RQ1 in the affirmative: PL-Rank-2 is the most sample-efficient method, although when > 100 PL-Rank-1 and the placement policy gradient have comparable performance.", "publication_ref": ["b2", "b27"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Computational Costs and Time-Efficiency", "text": "In order to answer RQ2: whether PL-Rank requires less computational time to reach optimal performance, we first consider Table 1 which displays the average time taken to perform a single epoch per method for various values.\nWe see that in all cases PL-Rank-1 takes the least amount of time to compute, with PL-Rank-2 being the second fastest method. There is little difference between the policy gradient methods but they are always much slower than the PL-Rank methods. Depending on the dataset and , the difference between PL-Rank and the policy gradients varies from around a minute to almost five minutes. In Figure 1 we see that convergence requires at least 40 training epochs, thus differences in minutes per epoch can easily add up to reaching convergence over an hour earlier.\nLambdaLoss is especially affected by the parameter, a likely explanation is that it is the only method that has to sample the complete ranking, whereas the other methods only need to sample the top-ranking (top-5 in this case).\nBy considering both the results from Table 1 and Figure 1, we can make three observations: (i) when = 10, decent but not optimal performance is reached; (ii) = 100 is enough to converge near optimal performance; and (iii) performing an epoch with = 10 is considerably faster than with = 100. Based on these observations, it seems reasonable to increase at every training step so that decent performance is reached very quickly but convergence is still optimal.  Figure 2 shows the performance of PL ranking models trained using this dynamic strategy over training time in minutes. Again we see that LambdaLoss converges fast but at suboptimal performance. Similarly, there is clearly a large difference visible between the basic policy gradient and the placement policy gradient. However, on all datasets, we see an improvement of PL-Rank-1 over the placement policy gradient which is very large on Yahoo and MSLR but smaller on Istella. This improvement can be attributed to the reduced computational costs of PL-Rank-1, as a result, it is capable of completing more epochs in the same amount of time and can therefore reach a higher performance in less computational time. Finally, compared to PL-Rank-1, PL-Rank-2 has an even higher performance on the Yahoo and Istella datasets but not on MSLR. It appears that its increased sample-efficiency helps PL-Rank-2 initially, when is low, except on the MSLR dataset where Figure 1 also shows us that the difference in sample-efficiency is very limited. To better verify that PL-Rank-2 is the best choice, we performed a two-sided student t-test on the performance differences, the results are displayed in Table 2. We see that Pl-Rank-2 is significantly better compared to the other methods in all tested cases, with the single exception of PL-Rank-1 on the MSLR dataset.\nTo conclude, we answer RQ2 in the affirmative: the PL-Rank methods achieves significantly higher performance with less computational time required than LambdaLoss or the policy gradients. In particular, PL-Rank-2 is the most time-efficient method across all datasets.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": ["tab_1", "tab_1", "tab_3"]}, {"heading": "Optimizing a Ranking Fairness Metric", "text": "Finally, we address RQ3: whether PL-Rank is effective at optimizing ranking fairness. Figure 3 displays the mean disparity error for models optimized with the different gradient estimation methods, with the same increasing strategy applied as in Section 8.2. While all methods decrease the disparity, the PL-Rank methods and the placement policy gradient are considerably more efficient than LambdaLoss and the basic policy gradient. Unlike with the optimization for relevance, there appears only a very small improvement of the PL-Rank methods over the placement policy gradient. Therefore, we answer RQ3 in the affirmative: PL-Rank can effectively optimize ranking fairness, where we note that the placement policy gradient has comparable time-efficiency.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we tackled the optimization of PL-ranking models for both relevance and fairness ranking metrics. While previous work has found PL-ranking models effective for various ranking tasks, their optimization can involve large computational costs. To alleviate these costs, we introduced three new estimators for efficiently estimating the gradient of a ranking metric w.r.t. a PL ranking model: the placement policy gradient and two PL-Rank methods. The latter two can be computed using the PL-Rank algorithm. To the best of our knowledge, PL-Rank is the first algorithm designed specifically for efficiently optimizing PL ranking models w.r.t. ranking metrics. Our experimental results indicate that our novel methods considerably reduce the computational time required to reach optimal performance compared to existing methods. In particular, the PL-Rank-2 method has the best sample-effiency and was found to reach significantly higher performance when ran for the same amount of time as other methods. Compared to the popular basic policy gradient, PL-Rank-2 converges several hours earlier, thus immensely alleviating the computational costs of optimization.\nWith the introduction of PL-Rank, we hope that the usage of stochastic ranking models is made more attractive in real-world scenarios. Finally, we think PL-Rank is also an important theoretical contribution to the LTR field, as it proves that PL ranking models can be optimized with computational efficiency, without relying on heuristic methods.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Code and data", "text": "To facilitate reproducibility, this work only made use of publicly available data and our experimental implementation is publicly available at https://github.com/HarrieO/2021-SIGIR-plackett-luce.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Tensorflow: A system for large-scale machine learning", "journal": "", "year": "2016", "authors": "Mart\u00edn Abadi; Paul Barham; Jianmin Chen; Zhifeng Chen; Andy Davis; Jeffrey Dean; Matthieu Devin; Sanjay Ghemawat; Geoffrey Irving; Michael Isard"}, {"ref_id": "b1", "title": "Equity of attention: Amortizing individual fairness in rankings", "journal": "", "year": "2018", "authors": "J Asia; Krishna P Biega; Gerhard Gummadi;  Weikum"}, {"ref_id": "b2", "title": "A Stochastic Treatment of Learning to Rank Scoring Functions", "journal": "", "year": "2020", "authors": "Sebastian Bruch; Shuguang Han; Michael Bendersky; Marc Najork"}, {"ref_id": "b3", "title": "Revisiting approximate metric optimization in the age of deep neural networks", "journal": "", "year": "2019", "authors": "Sebastian Bruch; Masrour Zoghi; Michael Bendersky; Marc Najork"}, {"ref_id": "b4", "title": "Learning to rank using gradient descent", "journal": "", "year": "2005", "authors": "Chris Burges; Tal Shaked; Erin Renshaw; Ari Lazier; Matt Deeds; Nicole Hamilton; Greg Hullender"}, {"ref_id": "b5", "title": "From RankNet to LambdaRank to LambdaMART: An Overview", "journal": "", "year": "2010", "authors": "J C Christopher;  Burges"}, {"ref_id": "b6", "title": "Learning to rank: from pairwise approach to listwise approach", "journal": "", "year": "2007", "authors": "Zhe Cao; Tao Qin; Tie-Yan Liu; Ming-Feng Tsai; Hang Li"}, {"ref_id": "b7", "title": "Yahoo! Learning to Rank Challenge Overview", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "Olivier Chapelle; Yi Chang"}, {"ref_id": "b8", "title": "An experimental comparison of click position-bias models", "journal": "", "year": "2008", "authors": "Nick Craswell; Onno Zoeter; Michael Taylor; Bill Ramsey"}, {"ref_id": "b9", "title": "Fast Ranking with Additive Ensembles of Oblivious and Non-Oblivious Regression Trees", "journal": "ACM Transactions on Information Systems (TOIS)", "year": "2016", "authors": "Domenico Dato; Claudio Lucchese; Maria Franco; Salvatore Nardini; Raffaele Orlando; Nicola Perego; Rossano Tonellotto;  Venturini"}, {"ref_id": "b10", "title": "Evaluating Stochastic Rankings with Expected Exposure", "journal": "Association for Computing Machinery", "year": "2020", "authors": "Fernando Diaz; Bhaskar Mitra; Michael D Ekstrand; Asia J Biega; Ben Carterette"}, {"ref_id": "b11", "title": "On the local optimality of LambdaRank", "journal": "", "year": "2009", "authors": "Pinar Donmez; M Krysta;  Svore; J C Christopher;  Burges"}, {"ref_id": "b12", "title": "Towards Meaningful Statements in IR Evaluation. Mapping Evaluation Measures to Interval Scales", "journal": "", "year": "2021", "authors": "Marco Ferrante; Nicola Ferro; Norbert Fuhr"}, {"ref_id": "b13", "title": "Statistical theory of extreme values and some practical applications: a series of lectures", "journal": "", "year": "1954", "authors": "Emil Julius Gumbel"}, {"ref_id": "b14", "title": "Array programming with NumPy", "journal": "Nature", "year": "2020", "authors": "Charles R Harris; K Jarrod Millman; J St'efan; Ralf Van Der Walt; Pauli Gommers; David Virtanen; Eric Cournapeau; Julian Wieser; Sebastian Taylor; Nathaniel J Berg; Robert Smith; Matti Kern; Stephan Picus; Marten H Hoyer; Matthew Van Kerkwijk; Allan Brett; Jaime Haldane; Mark Fern; Pearu Wiebe;  Peterson; G Pierre; Kevin 'erard-Marchant; Tyler Sheppard; Warren Reddy; Hameer Weckesser; Christoph Abbasi; Travis E Gohlke;  Oliphant"}, {"ref_id": "b15", "title": "A Probabilistic Method for Inferring Preferences from Clicks", "journal": "", "year": "2011", "authors": "Katja Hofmann; Shimon Whiteson; Maarten De Rijke"}, {"ref_id": "b16", "title": "Optimizing Search Engines Using Clickthrough Data", "journal": "ACM", "year": "2002", "authors": "Thorsten Joachims"}, {"ref_id": "b17", "title": "Learning to Rank for Information Retrieval", "journal": "Foundations and Trends in Information Retrieval", "year": "2009", "authors": "Tie-Yan Liu"}, {"ref_id": "b18", "title": "Individual choice behavior: A theoretical analysis", "journal": "Courier Corporation", "year": "2012", "authors": "Luce R Duncan"}, {"ref_id": "b19", "title": "Controlling Fairness and Bias in Dynamic Learning-to-Rank", "journal": "ACM", "year": "2020", "authors": "Marco Morik; Ashudeep Singh; Jessica Hong; Thorsten Joachims"}, {"ref_id": "b20", "title": "Differentiable Unbiased Online Learning to Rank", "journal": "ACM", "year": "2018", "authors": "Harrie Oosterhuis;  Maarten De Rijke"}, {"ref_id": "b21", "title": "Taking the Counterfactual Online: Efficient and Unbiased Online Evaluation for Ranking", "journal": "ACM", "year": "2020", "authors": "Harrie Oosterhuis;  Maarten De Rijke"}, {"ref_id": "b22", "title": "Unifying Online and Counterfactual Learning to Rank", "journal": "ACM", "year": "2021", "authors": "Harrie Oosterhuis;  Maarten De Rijke"}, {"ref_id": "b23", "title": "Pytorch: An imperative style, high-performance deep learning library", "journal": "", "year": "2019", "authors": "Adam Paszke; Sam Gross; Francisco Massa; Adam Lerer; James Bradbury; Gregory Chanan; Trevor Killeen; Zeming Lin; Natalia Gimelshein; Luca Antiga"}, {"ref_id": "b24", "title": "The analysis of permutations", "journal": "Journal of the Royal Statistical Society: Series C (Applied Statistics)", "year": "1975", "authors": "L Robin;  Plackett"}, {"ref_id": "b25", "title": "", "journal": "", "year": "2013", "authors": "Tao Qin; Tie-Yan Liu"}, {"ref_id": "b26", "title": "Probabilistic multileave for online retrieval evaluation", "journal": "", "year": "2015", "authors": "Anne Schuth; Robert-Jan Bruintjes; Fritjof Bu\u00fcttner; Joost Van Doorn; Carla Groenland; Harrie Oosterhuis; Cong-Nguyen Tran; Bas Veeling; Jos Van Der; Roger Velde;  Wechsler"}, {"ref_id": "b27", "title": "Policy learning for fairness in ranking", "journal": "", "year": "2019", "authors": "Ashudeep Singh; Thorsten Joachims"}, {"ref_id": "b28", "title": "Softrank: optimizing non-smooth rank metrics", "journal": "", "year": "2008", "authors": "Michael Taylor; John Guiver; Stephen Robertson; Tom Minka"}, {"ref_id": "b29", "title": "Position Bias Estimation for Unbiased Learning to Rank in Personal Search", "journal": "ACM", "year": "2018", "authors": "Xuanhui Wang; Nadav Golbandi; Michael Bendersky; Donald Metzler; Marc Najork"}, {"ref_id": "b30", "title": "The LambdaLoss Framework for Ranking Metric Optimization", "journal": "ACM", "year": "2018", "authors": "Xuanhui Wang; Cheng Li; Nadav Golbandi; Michael Bendersky; Marc Najork"}, {"ref_id": "b31", "title": "Reinforcement learning to rank with Markov decision process", "journal": "", "year": "2017", "authors": "Zeng Wei; Jun Xu; Yanyan Lan; Jiafeng Guo; Xueqi Cheng"}, {"ref_id": "b32", "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "journal": "Machine learning", "year": "1992", "authors": "J Ronald;  Williams"}, {"ref_id": "b33", "title": "Listwise approach to learning to rank: theory and algorithm", "journal": "", "year": "2008", "authors": "Fen Xia; Tie-Yan Liu; Jue Wang; Wensheng Zhang; Hang Li"}, {"ref_id": "b34", "title": "Adapting Markov decision process for search result diversification", "journal": "", "year": "2017", "authors": "Long Xia; Jun Xu; Yanyan Lan; Jiafeng Guo; Wei Zeng; Xueqi Cheng"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "1: \u2212 1 )1= 0. Combining this result with Eq. 19 we get: minus the risk of placement .", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Algorithm 1 PL-Rank-2 Gradient Estimation 1 :11Input: items: D; Relevances: ; Metric weights: ; Scores: ; Number of samples: . 2: { (1) , (2) ,..., ( ) } \u2190 Gumbel_Sample( , ) 3: \u2190 0 // initialize zero weight per item 4: \u2190 \u2208D exp( ( )) // initialize PL denominator 5: for \u2208 [1,2,..., ] do", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: Performance in DCG@5 of PL ranking models trained using different gradient estimation methods with varying number of sampled rankings per estimation. Results are the mean of 20 independent runs.", "figure_data": ""}, {"figure_label": "22", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "2 Figure 2 :22Figure 2: Performance in DCG@5 of PL ranking models trained using different gradient estimation methods following a dynamically updated number of sampled rankings per estimation. Results are the mean of 20 independent runs. Yahoo! Webscope", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: The mean disparity error of models trained using different gradient estimation methods following a dynamically updated number of sampled rankings per estimation. Results are the mean of 20 independent runs. We found that = 10 + 90 \u2022 epoch 40 outperformed the static choices = 10 and = 100 in terms of learning speed while maintaining optimal convergence.Figure2shows the performance of PL ranking models trained using this dynamic strategy over training time in minutes. Again we see that LambdaLoss converges fast but at suboptimal performance. Similarly, there is clearly a large difference visible between the basic policy gradient and the placement policy gradient. However, on all datasets, we see an improvement of PL-Rank-1 over the placement policy gradient which is very large on Yahoo and MSLR but smaller on Istella. This improvement can be attributed to the reduced computational costs of PL-Rank-1, as a result, it is capable of completing more epochs in the same amount of time and can therefore reach a higher performance in less computational time. Finally, compared to PL-Rank-1, PL-Rank-2 has an even higher performance on the Yahoo and Istella datasets but not on MSLR. It appears that its increased sample-efficiency helps PL-Rank-2 initially, when is low, except on the MSLR dataset where Figure1also shows us that the difference in sample-efficiency is very limited. To better verify that PL-Rank-2 is the best choice, we performed a two-sided student t-test on the performance differences, the results are displayed in Table2. We see that Pl-Rank-2 is significantly better compared to the other methods in all tested cases, with the single exception of PL-Rank-1 on the MSLR dataset.To conclude, we answer RQ2 in the affirmative: the PL-Rank methods achieves significantly higher performance with less computational time required than LambdaLoss or the policy gradients. In particular, PL-Rank-2 is the most time-efficient method across all datasets.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Average time in minutes taken to perform one training epoch for different numbers of sampled rankings , the standard deviation is displayed in brackets.", "figure_data": "= 1= 10= 100= 1000LambdaLoss2.48 ( 0.05) 2.53 ( 0.04) 3.06 ( 0.08)10.25 ( 0.53)YahooPolicy Gradient 3.79 ( 0.09) 3.80 ( 0.06) 4.28 ( 0.15) Placement P.G. 3.83 ( 0.08) 3.86 ( 0.05) 4.42 ( 0.10)8.27 ( 0.50) 8.26 ( 0.44)PL-Rank-12.45 ( 0.06) 2.49 ( 0.06) 2.82 ( 0.09) 5.70 ( 0.14)PL-Rank-22.49 ( 0.06) 2.52 ( 0.06) 2.87 ( 0.08)6.22 ( 0.15)LambdaLoss2.73 ( 0.11) 3.96 ( 0.59) 36.36 ( 31.46) 1669.59 ( 450.69)MSLRPolicy Gradient 3.30 ( 0.10) 3.45 ( 0.10) 5.25 ( 0.35) Placement P.G. 3.32 ( 0.17) 3.42 ( 0.13) 5.27 ( 0.41)24.20 ( 2.77) 23.97 ( 2.68)PL-Rank-12.16 ( 0.14) 2.28 ( 0.13) 3.34 ( 0.13) 18.48 ( 2.10)PL-Rank-22.19 ( 0.15) 2.35 ( 0.17) 3.45 ( 0.06)21.10 ( 2.78)LambdaLoss3.53 ( 0.12) 4.50 ( 0.10) 27.81 ( 19.20) 142.74 ( 16.25)IstellaPolicy Gradient 4.10 ( 0.17) 4.51 ( 0.16) 8.74 ( 0.26) Placement P.G. 4.08 ( 0.17) 4.51 ( 0.18) 8.72 ( 0.23)44.29 ( 2.17) 44.74 ( 2.55)PL-Rank-13.01 ( 0.12) 3.27 ( 0.09) 6.90 ( 0.19) 39.80 ( 2.65)PL-Rank-23.04 ( 0.13) 3.31 ( 0.10) 7.02 ( 0.14)40.64 ( 3.96)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "DCG@5 reached using different gradient estimation methods following a dynamically updated and being optimized for the same amount of time. Results are the mean of 20 independent runs, the standard deviation is displayed in brackets, \u25bd indicates the result is significantly worse ( < 0.01) than that of PL-Rank-2 on the same dataset.(0.04) 8.21 \u25bd ( 0.07) 18.75 \u25bd ( 0.10) Placement Policy Gradient 11.31 \u25bd ( 0.02) 8.33 \u25bd ( 0.04) 19.23 \u25bd ( 0.06) PL-Rank-1 11.38 \u25bd ( 0.03) 8.39 \u2212 ( 0.04) 19.31 \u25bd ( 0.05) PL-Rank-2 11.42 \u2212 ( 0.02) 8.39 \u2212 ( 0.03) 19.38 \u2212 ( 0.05)", "figure_data": "YahooMSLRIstellaMinutes Optimized100120200LambdaLoss11.11 \u25bd ( 0.05) 7.80 \u25bd ( 0.09) 18.75 \u25bd ( 0.10)Policy Gradient11.03 \u25bd"}], "formulas": [{"formula_id": "formula_0", "formula_text": "R ( ) = \u2211\ufe01 \u2208 ( | ) \u2211\ufe01 =1 ( = 1| , ) = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 = E \u2211\ufe01 =1 .(1)", "formula_coordinates": [2.0, 361.42, 478.4, 196.78, 55.07]}, {"formula_id": "formula_1", "formula_text": "DCG@K = 1[ \u2264 ]", "formula_coordinates": [2.0, 420.52, 563.52, 62.75, 11.22]}, {"formula_id": "formula_2", "formula_text": "PREC@K = 1 1[ \u2264 ]", "formula_coordinates": [2.0, 322.54, 579.78, 78.5, 11.29]}, {"formula_id": "formula_3", "formula_text": "R = E [R ( )] = \u2211\ufe01 \u2208Q ( )R ( ).(2)", "formula_coordinates": [2.0, 383.75, 635.17, 174.45, 21.99]}, {"formula_id": "formula_4", "formula_text": "( | 1: \u22121 ,D) = ( ) 1[ \u2209 1: \u22121 ] \u2032 \u2208 \\ 1: \u22121 ( \u2032 ) ,(3)", "formula_coordinates": [3.0, 113.63, 215.06, 180.41, 26.24]}, {"formula_id": "formula_5", "formula_text": "1: \u22121 = [ 1 , 2 ,..., \u22121 ].(4)", "formula_coordinates": [3.0, 136.13, 263.3, 157.92, 10.0]}, {"formula_id": "formula_6", "formula_text": "\u2208 1: \u22121 .", "formula_coordinates": [3.0, 237.02, 356.94, 29.88, 10.0]}, {"formula_id": "formula_7", "formula_text": "( ) = =1 ( | 1: \u22121 ,D).", "formula_coordinates": [3.0, 132.3, 404.02, 89.01, 18.82]}, {"formula_id": "formula_8", "formula_text": "( ) = \u2212log(\u2212log( ( ) )", "formula_coordinates": [3.0, 58.76, 575.39, 74.66, 11.32]}, {"formula_id": "formula_9", "formula_text": "( ) = ( ) + ( ) .(6)", "formula_coordinates": [3.0, 149.26, 604.83, 144.78, 11.82]}, {"formula_id": "formula_10", "formula_text": "( ) = [( ) 1 , ( ) 2 ,", "formula_coordinates": [3.0, 104.5, 652.87, 51.11, 14.75]}, {"formula_id": "formula_11", "formula_text": "s.t. \u2200( ( ) ,( ) ]", "formula_coordinates": [3.0, 113.24, 652.87, 67.72, 29.08]}, {"formula_id": "formula_14", "formula_text": "( ) = ( ) log( ( )) .(8)", "formula_coordinates": [3.0, 399.98, 186.73, 158.23, 8.43]}, {"formula_id": "formula_15", "formula_text": "R ( ) = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 = \u2211\ufe01 \u2208 ( ) log( ( )) \u2211\ufe01 =1 = E log( ( )) gradient w.r.t. complete ranking \u2211\ufe01 =1 full reward .(9)", "formula_coordinates": [3.0, 353.27, 238.28, 204.93, 106.03]}, {"formula_id": "formula_16", "formula_text": "R ( ) \u2248 1 \u2211\ufe01 =1 log( ( ( ) )) \u2211\ufe01 =1 ( ) .(10)", "formula_coordinates": [3.0, 363.2, 456.92, 195.0, 24.4]}, {"formula_id": "formula_17", "formula_text": "R ( ) = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 = \u2211\ufe01 =1 \u2211\ufe01 \u2208 ( ) = \u2211\ufe01 =1 \u2211\ufe01 1: \u2208 ( 1: ) ,(11)", "formula_coordinates": [4.0, 92.32, 267.14, 201.72, 55.07]}, {"formula_id": "formula_19", "formula_text": "( 1: ) = ( 1: ) log( ( 1: )) = ( 1: ) \u2211\ufe01 =1 log( ( | 1: \u22121 )) .(12)", "formula_coordinates": [4.0, 104.91, 454.03, 189.14, 47.44]}, {"formula_id": "formula_20", "formula_text": "R ( ) = \u2211\ufe01 =1 \u2211\ufe01 1: \u2208 ( 1: ) = \u2211\ufe01 =1 \u2211\ufe01 1: \u2208 ( 1: ) \u2211\ufe01 =1 log( ( | 1: \u22121 )) = \u2211\ufe01 =1 E 1: \u2211\ufe01 =1 log( ( | 1: \u22121 )) (13) = E \u2211\ufe01 =1 \u2211\ufe01 =1 log( ( | 1: \u22121 )) = E \u2211\ufe01 =1 log( ( | 1: \u22121 ))", "formula_coordinates": [4.0, 77.55, 534.34, 216.5, 154.05]}, {"formula_id": "formula_21", "formula_text": "=1 E 1: [ ( 1: )] = E =1 ( 1:", "formula_coordinates": [4.0, 425.75, 87.74, 119.46, 11.35]}, {"formula_id": "formula_22", "formula_text": "R ( ) \u2248 1 \u2211\ufe01 =1 \u2211\ufe01 =1 log( ( ( ) | ( ) 1: \u22121 )) \u2211\ufe01 = ( ) . (14", "formula_coordinates": [4.0, 342.1, 163.86, 212.69, 24.4]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [4.0, 554.78, 169.93, 3.42, 7.94]}, {"formula_id": "formula_24", "formula_text": "( | 1: \u22121 ) =(15)", "formula_coordinates": [4.0, 345.56, 370.7, 212.64, 10.0]}, {"formula_id": "formula_25", "formula_text": "( | 1: \u22121 ) ( ) \u2212 \u2211\ufe01 \u2032 \u2208D ( \u2032 | 1: \u22121 ) ( \u2032 ) .", "formula_coordinates": [4.0, 345.56, 393.16, 204.42, 21.97]}, {"formula_id": "formula_26", "formula_text": "R ( ) = E \u2211\ufe01 =1 log( ( | 1: \u22121 )) \u2211\ufe01 = = E \u2211\ufe01 =1 ( ) \u2211\ufe01 = (16) \u2212 \u2211\ufe01 =1 \u2211\ufe01 \u2032 \u2208D ( \u2032 | 1: \u22121 ) ( \u2032 ) \u2211\ufe01 = .", "formula_coordinates": [4.0, 337.79, 464.57, 220.41, 87.16]}, {"formula_id": "formula_27", "formula_text": "E \u2211\ufe01 =1 ( ) \u2211\ufe01 = = E \u2211\ufe01 \u2208D ( ) \u2211\ufe01 =1 1[ = ] \u2211\ufe01 = = E \u2211\ufe01 \u2208D ( ) \u2211\ufe01 =1 1[ \u2208 1: ] = \u2211\ufe01 \u2208D ( ) E \u2211\ufe01 =rank( , ) .(17)", "formula_coordinates": [4.0, 335.49, 590.02, 222.71, 120.01]}, {"formula_id": "formula_28", "formula_text": "E \u2211\ufe01 =1 \u2211\ufe01 \u2208 D ( | 1: \u22121 ) ( ) \u2211\ufe01 = = E \u2211\ufe01 \u2208 D ( ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2211\ufe01 = = \u2211\ufe01 \u2208 D ( ) E rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2211\ufe01 = ,(18)", "formula_coordinates": [5.0, 65.86, 143.47, 228.19, 88.76]}, {"formula_id": "formula_30", "formula_text": "R ( ) \u2248 1 \u2211\ufe01 \u2208 D ( ) \u2211\ufe01 =1 \u2211\ufe01 =rank( , ( ) ) ( ) \u2212 rank( , ( ) ) \u2211\ufe01 =1 ( | ( ) 1: \u22121 ) \u2211\ufe01 = ( ) .(20)", "formula_coordinates": [5.0, 81.61, 521.94, 212.43, 62.06]}, {"formula_id": "formula_31", "formula_text": "E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , )+1 + rank( , ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , )+1 + \u2211\ufe01 =1 ( | 1: \u22121 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 \u2211\ufe01 =rank( , )+1 + rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,(21)", "formula_coordinates": [5.0, 328.5, 231.22, 229.7, 140.93]}, {"formula_id": "formula_33", "formula_text": "R ( ) \u2248 1 \u2211\ufe01 \u2208D ( ) \u2211\ufe01 =1 \u2211\ufe01 =rank( , ( ) )+1 ( ) + rank( , ( ) ) \u2211\ufe01 =1 ( | ( ) 1: \u22121 ) \u2212 \u2211\ufe01 = ( ) .(23)", "formula_coordinates": [5.0, 336.39, 577.23, 221.81, 62.06]}, {"formula_id": "formula_34", "formula_text": "R ( ) \u2248 1 \u2211\ufe01 \u2208 D ( ) ,(24)", "formula_coordinates": [6.0, 129.28, 139.9, 164.76, 24.22]}, {"formula_id": "formula_35", "formula_text": "= 1 \u2211\ufe01 =1 \u2211\ufe01 =rank( , )+1 + rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) \u2212 \u2211\ufe01 = . (25", "formula_coordinates": [6.0, 80.52, 193.08, 210.1, 59.85]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [6.0, 290.62, 216.27, 3.42, 7.94]}, {"formula_id": "formula_37", "formula_text": "E ( , ) = E \u2211\ufe01 =1 1[ = ] = \u2211\ufe01 \u2208 ( ) \u2211\ufe01 =1 1[ = ],(26)", "formula_coordinates": [6.0, 67.41, 627.62, 226.63, 22.15]}, {"formula_id": "formula_38", "formula_text": "F ( ) = \u2211\ufe01 \u2208D F ( ) E E .(27)", "formula_coordinates": [6.0, 400.89, 344.18, 157.31, 24.79]}, {"formula_id": "formula_39", "formula_text": "\u2200 \u2032 \u2032 = 1[ \u2032 = ]", "formula_coordinates": [6.0, 404.88, 382.86, 65.54, 10.64]}, {"formula_id": "formula_40", "formula_text": "F ( ) = \u2211\ufe01 \u2208D ( ) E \u2211\ufe01 =rank( , )+1 F ( ) E (28", "formula_coordinates": [6.0, 340.66, 438.26, 214.13, 24.99]}, {"formula_id": "formula_41", "formula_text": ")", "formula_coordinates": [6.0, 554.78, 444.9, 3.42, 7.94]}, {"formula_id": "formula_42", "formula_text": "+ rank( , ) \u2211\ufe01 =1 ( | 1: \u22121 ) F ( ) E \u2212 \u2211\ufe01 = F ( ) E .", "formula_coordinates": [6.0, 348.63, 469.19, 198.36, 29.49]}, {"formula_id": "formula_43", "formula_text": "F ( )", "formula_coordinates": [6.0, 432.56, 512.59, 14.91, 6.25]}, {"formula_id": "formula_44", "formula_text": "F ( ) = 1 |D|(|D|\u22121) \u2211\ufe01 \u2208D \u2211\ufe01 \u2032 \u2208D ( , \u2032 ). (29", "formula_coordinates": [6.0, 369.13, 610.37, 185.65, 24.22]}, {"formula_id": "formula_45", "formula_text": ")", "formula_coordinates": [6.0, 554.78, 616.44, 3.42, 7.94]}, {"formula_id": "formula_46", "formula_text": "( , \u2032 ) = (E \u2032 \u2212E \u2032 ) 2 . (30", "formula_coordinates": [6.0, 395.34, 698.5, 159.44, 10.93]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [6.0, 554.78, 701.49, 3.42, 7.94]}, {"formula_id": "formula_48", "formula_text": "F ( ) E = 4 |D|(|D|\u22121) \u2211\ufe01 \u2032 \u2208 D (E \u2032 \u2212E \u2032 ) \u2032 . (31", "formula_coordinates": [7.0, 94.32, 134.38, 196.3, 24.79]}, {"formula_id": "formula_49", "formula_text": ")", "formula_coordinates": [7.0, 290.62, 141.02, 3.42, 7.94]}, {"formula_id": "formula_50", "formula_text": "F ( ) E .", "formula_coordinates": [7.0, 253.31, 185.14, 18.66, 14.58]}], "doi": "10.1145/3404835.3462830"}