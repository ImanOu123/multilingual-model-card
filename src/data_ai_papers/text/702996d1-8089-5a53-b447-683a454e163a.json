{"title": "The Mechanics of n-Player Differentiable Games", "authors": "David Balduzzi; S\u00e9bastien Racani\u00e8re; James Martens; Jakob Foerster; Karl Tuyls; Thore Graepel", "pub_date": "", "abstract": "The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood -and is becoming increasingly important as adversarial and multiobjective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs -whilst at the same time being applicable to -and having guarantees in -much more general games.", "sections": [{"heading": "Introduction", "text": "Recent progress in machine learning is heavily dependent on using gradient descent, applied to optimize the parameters of models with respect to a (single) objectives. A basic result is that gradient descent converges to a local minimum of the objective under a broad range of conditions (Lee et al., 2017). However, there is a rapidly growing set of powerful models that do not optimize a single objective, including: generative adversarial networks (Goodfellow et al., 2014), proximal gradient TD learning (Liu et al., 2016), multi-level optimization (Pfau & Vinyals, 2016), synthetic gradients (Jaderberg et al., 2017), hierarchical reinforcement learning (Wayne & Abbott, 2014;Vezhnevets et al., 2017), curiosity (Pathak et al., 2017), and imaginative agents (Racani\u00e8re et al., 2017). In effect, the models are trained via games played by cooperating and competing modules.\nNo-regret algorithms such as gradient descent are guaranteed to converge to coarse correlated equilibria in games (Stoltz & Lugosi, 2007). However, the dynamics do not converge to Nash equilibria -and do not even stabilizein general (Mertikopoulos et al., 2018). Concretely, cyclic behaviors emerge even in simple cases, see example 1. This paper presents an analysis of the second-order structure of game dynamics that allows to identify two classes of games, potential and Hamiltonian, that are easy to solve separately. We then derive symplectic gradient adjustment (SGA), a method for finding stable fixed points in games. SGA's performance is evaluated in basic experiments.\nBackground and problem description. Procedures that converge to Nash equilibria have been found for restricted game classes: potential games, 2-player zero-sum games and a few others (Hu & Wellman, 2003;Hart & Mas-Colell, 2013). Finding equilibria can be reformulated as a nonlinear complementarity problem, but these are 'hopelessly impractical to solve' in general (Shoham & Leyton-Brown, 2008) because the problem is PPAD hard (Daskalakis et al., 2009). Players are primarily neural nets in our setting. We therefore restrict to gradient-based methods (game-theorists have considered a much broader range of techniques). Losses are not necessarily convex in any of their parameters, so Nash equilibria do not necessarily exist. Leaving existence aside, finding Nash equilibria is analogous to, but much harder than, finding global minima in neural nets -which is not realistic with gradient-based methods.\nThere are (at least) three problems with gradient descent in games. Firstly, the potential existence of cycles (recurrent dynamics) implies there are no convergence guarantees, see example 1 and Mertikopoulos et al. (2018). Secondly, even when gradient descent converges, the rate may be too slow in practice because 'rotational forces' necessitate extremely small learning rates (see figure 3). Finally, since there is no single objective, there is no way to measure arXiv:1802.05642v2 [cs.LG] 6 Jun 2018 progress. Application-specific proxies have been proposed, for example the inception score for GANs (Salimans et al., 2016), but these are little help during training -the inception score is no substitute for looking at samples.\nOutline and summary of main contributions. We start with the well-known case of a zero-sum bimatrix game: example 1. It turns out that the dynamics (that is, the dynamics under simultaneous gradient descent) can be reformulated via Hamilton's equations. The cyclic behavior arises because the dynamics live on the level sets of the Hamiltonian. More directly useful, gradient descent on the Hamiltonian converges to a Nash equilibrium.\nLemma 1 shows that the Hessian of any game decomposes into symmetric and antisymmetric components. There are thus two 'pure' cases: when only the symmetric component is present, or only the antisymmetric. The first case, known as potential games (Monderer & Shapley, 1996), have been intensively studied because they are exactly the games where gradient descent does converge.\nThe second case, Hamiltonian 1 games, were not studied previously, probably because they coincide with zero-sum games in the bimatrix case (or constant-sum, depending on the constraints). Zero-sum and Hamiltonian games differ when the losses are not bilinear or there are more than two players. Hamiltonian games are important because (i) they are easy to solve and (ii) general games combine potential-like and Hamiltonian-like dynamics. The concept of a zero-sum game is too loose to be useful when there are many players: any n-player game can be reformulated as a zero-sum (n + 1)-player game where n+1 = \u2212 n i=1 i . Zero-sum games are as complicated as general-sum games. Theorem 3 shows that Hamiltonian games obey a conservation law -which also provides the key to solving them, by gradient descent on the conserved quantity.\nThe general case, neither potential nor Hamiltonian, is more difficult and the focus of the remainder of the paper. Section 3 proposes symplectic gradient adjustment (SGA), a gradient-based method for finding stable fixed points in general games. Appendix C contains TensorFlow code to compute the adjustment. The algorithm computes two Hessianvector products, at a cost of two iterations of backprop. SGA satisfies a few natural desiderata: D1 it is compatible with the original dynamics and D2, D3 it is guaranteed to find stable equilibria in potential and Hamiltonian games.\nFor general games, correctly picking the sign of the adjustment (whether to add or subtract) is critical since it determines the behavior near stable and unstable equilibria. Section 2.3 defines stable equilibria and relates them to local Nash equilibria. Lemma 10 then shows how to set the sign so as to converge to stable fixed points. Correctly aligning 1 Lu (1992) defined an unrelated notion of Hamiltonian game.\nSGA allows higher learning rates and faster, more robust convergence, see theorem 7 and experiments in section 4. Section 4 investigates a basic GAN setup from Metz et al. (2017), that tests for mode-collapse and mode hopping. Whereas simultaneous gradient descent completely fails; the symplectic adjustment leads to rapid convergence -slightly improved by correctly choosing the sign of the adjustment. Finally, section 3.5 applies the same criterion to align consensus optimization (Mescheder et al., 2017), preventing it from converging to unstable equilibria and (slightly) improving performance, figure 9 in the appendix.\nCaveat. The behavior of SGA near fixed points that are neither negative nor positive semi-definite is not analysed. On the one hand, it was only recently shown that gradient descent behaves well near saddles when optimizing a single objective (Lee et al., 2016;2017). On the other hand, Newton's method is attracted to saddles, see analysis and recently proposed remedy in Dauphin et al. (2014). Studying indefinite fixed points is deferred to future work.", "publication_ref": ["b16", "b11", "b18", "b29", "b15", "b37", "b36", "b28", "b30", "b35", "b20", "b14", "b12", "b33", "b7", "b20", "b32", "b24", "b19", "b23", "b22", "b17", "b2", "b9"], "figure_ref": ["fig_4", "fig_10"], "table_ref": []}, {"heading": "Related work.", "text": "Convergence to Nash equilibria in twoplayer games was studied in Singh et al. (2000). WoLF (Win or Learn Fast) converges to Nash equilibria in two-player two-action games (Bowling & Veloso, 2002). Extensions include weighted policy learning (Abdallah & Lesser, 2008) and GIGA-WoLF (Bowling, 2004). There has been almost no work on convergence to fixed points in general games. Optimistic mirror descent approximately converges in twoplayer bilinear zero-sum games (Daskalakis et al., 2018), a special case of Hamiltonian games. In more general settings it converges to coarse correlated equilibria.\nThere has been interesting recent work on convergence in GANs. Heusel et al. (2017) propose a two-time scale methods to find Nash. However, it likely scales badly with the number of players. Nagarajan & Kolter (2017) prove convergence for some algorithms, but under very strong assumptions Mescheder (2018). Consensus optimization (Mescheder et al., 2017) is discussed in section 3. Learning with opponent-learning awareness (LOLA) infinitesimally modifies the objectives of players to take into account their opponents' goals (Foerster et al., 2018). However, there are no guarantees that LOLA converges and in general it may modify the fixed-points of the game.\nNotation. Dot products are written as v w or v, w . The angle between vectors is \u03b8(v, w). Positive definiteness is denoted S 0. Omitted proofs are in appendix B.", "publication_ref": ["b34", "b4", "b5", "b8", "b13", "b25", "b21", "b22", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "The infinitesimal structure of games", "text": "In contrast to the classical formulation of games, we do not constrain the parameter sets (e.g. to the probability simplex) or require losses to be convex in the corresponding players' parameters. Players could be interacting neural nets such as GANs (Goodfellow et al., 2014).\nDefinition 1. A game is a set of players [n] = {1, . . . , n} and twice continuously differentiable losses\n{ i : R d \u2192 R} n i=1 . Parameters are w = (w 1 , . . . , w n ) \u2208 R d with w i \u2208 R di where n i=1 d i = d. . Player i controls w i .\nIt is sometimes convenient to write w = (w i , w \u2212i ) where w \u2212i concatenates the parameters of all the players other than the i th , which is placed out of order by abuse of notation.\nThe simultaneous gradient is the gradient of the losses with respect to the parameters of the respective players:\n\u03be(w) = (\u2207 w1 1 , . . . , \u2207 wn n ) \u2208 R d .\nBy the dynamics of the game, we mean following the negative of the vector field \u03be with infinitesimal steps. There is no reason to expect \u03be to be the gradient of a single function in general, and therefore no reason to expect the dynamics to converge to a fixed point.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Warmup: Hamiltonian mechanics in games", "text": "The next example illustrates the essential problem with gradients in games and the key insight motivating our approach.\nExample 1 (Conservation of energy in a zero-sum unconstrained bimatrix game). Zero-sum games, where n i=1 i \u2261 0, are well-studied. The zero-sum game 1 (x, y) = x Ay and 2 (x, y) = \u2212x Ay has Nash equilibrium at (x, y) = (0, 0). The simultaneous gradient \u03be(x, y) = (Ay, \u2212A x) rotates around the Nash, see figure 1.\nThe matrix A admits singular value decomposition (SVD)\nA = U DV. Changing to coordinates u = D 1 2 Ux and v = D 1 2 Vy gives 1/2 (u, v) = \u00b1u v. Introduce the Hamiltonian H = 1 2 u 2 2 + v 2 2 = 1 2 (x U DUx + y V DVy) .\nRemarkably, the dynamics can be reformulated via Hamilton's equations in the coordinates given by the SVD of A:\n\u03be(u, v) = \u2202H \u2202v , \u2212 \u2202H \u2202u .\nVector field \u03be cycles around the equilibrium because \u03be conserves the Hamiltonian's level sets (i.e. \u03be, \u2207H = 0). However, gradient descent on the Hamiltonian converges to the Nash equilibrium. The remainder of the paper explores the implications and limitations of this insight.\nPapadimitriou & Piliouras ( 2016) recently analyzed the dynamics of Matching Pennies (essentially, the above example) and showed with some effort that the cyclic behavior covers the entire parameter space. The Hamiltonian reformulation directly explains the cyclic behavior via a conservation law.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The generalized Helmholtz decomposition", "text": "The Hessian of a game is the\n(d \u00d7 d)-matrix of second- derivatives H(w) := \u2207 w \u2022 \u03be(w) = \u2202\u03be\u03b1(w) \u2202w \u03b2 d \u03b1,\u03b2=1\n. Concretely, the Hessian can be written\nH(w) = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed \u2207 2 w1 1 \u2207 2 w1,w2 1 \u2022 \u2022 \u2022 \u2207 2 w1,wn 1 \u2207 2 w2,w1 2 \u2207 2 w2 2 \u2022 \u2022 \u2022 \u2207 2 w2,wn 2 . . . . . . \u2207 2 wn,w1 n \u2207 2 wn,w2 n \u2022 \u2022 \u2022 \u2207 2 wn n \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 ,\nwhere \u2207 2 wi,wj k is the (d i \u00d7 d j )-block of 2 nd -order derivatives. The Hessian of a game is not necessarily symmetric. Note: \u03b1, \u03b2 run over dimensions; i, j run over players.\nLemma 1 (generalized Helmholtz decomposition). The Hessian of any vector field decomposes uniquely into two components H(w) = S(w) + A(w) where S \u2261 S is symmetric and A + A \u2261 0 is antisymmetric. The decomposition is preserved by orthogonal change-ofcoordinates P MP = P SP + P AP since the terms remain symmetric and antisymmetric.\nThe connection to the classical Helmholtz decomposition in calculus is sketched in appendix E. Two obvious classes of games arise from the decomposition: Definition 2. A game is a potential game if A(w) \u2261 0. It is a Hamiltonian game if S(w) \u2261 0.\nPotential games are well-studied and easy to solve. Hamiltonian games are a new class of games that are also easy to solve. The general case is more delicate, see section 3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stable fixed points", "text": "Gradient-based methods can reliably find local -but not global -optima of nonconvex objective functions (Lee et al., 2016;2017). Similarly, gradient-based methods cannot be expected to find global Nash equilibria in games. Definition 3. A fixed point w * , with \u03be(w * ) = 0, is stable if S(w) 0 and unstable if S(w) \u227a 0 for w in a neighborhood of w * .\nFixed points that are neither positive nor negative definite are beyond the scope of the paper.\nLemma 2 (Stable fixed points are local Nash equilibria). A point w * is a local Nash equilibrium if, for all i, there is a neighborhood U i of w * i such that i (w i , w\n* \u2212i ) \u2265 i (w * i , w * \u2212i ) for w i \u2208 U i .\nIf fixed point w * is stable then it is a local Nash equilibrium.\nProof. If S is positive semidefinite then so are its\n(d i \u00d7 d i )- submatrices S i := \u2207 2\nwi i for all i. The result follows.\nAppendix A contains more details on local Nash equilibria.", "publication_ref": ["b17", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Potential games", "text": "Potential games were introduced in Monderer & Shapley (1996). A game is a potential game if there is a single potential function \u03c6 : R d \u2192 R and positive numbers {\u03b1 i > 0} n i=1 such that\n\u03c6(w i , w \u2212i )\u2212\u03c6(w i , w \u2212i ) = \u03b1 i i (w i , w \u2212i )\u2212 i (w i , w \u2212i )\nfor all i and all w i , w i , w \u2212i . Monderer & Shapley (1996) show a game is a potential game iff \u03b1 i \u2207 wi i = \u2207 wi \u03c6 for all i, which is equivalent to\n\u03b1 i \u2207 2 wiwj i = \u03b1 j \u2207 2 wiwj j = \u03b1 j \u2207 2 wj wi j \u2200i, j.\nOur definition of potential game is the special case where \u03b1 i = 1 for all i, which Monderer & Shapley (1996) call an exact potential game. We use the shorthand 'potential game' to refer to exact potential games in what follows.\nPotential games have been extensively studied since they are one of the few classes of games for which Nash equilibria can be computed (Rosenthal, 1973). For our purposes, they are games where simultaneous gradient descent on the losses is gradient descent on a single function. It follows that descent on \u03be converges to a fixed point that is a local minimum of \u03c6 -or saddle, but see Lee et al. (2017).", "publication_ref": ["b24", "b24", "b24", "b31", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Hamiltonian games", "text": "A concrete example may help understand antisymmetric matrices. Suppose n competitors play one-on-one and that the probability of player i beating player j is p ij . Then, assuming there are no draws, the probabilities satisfy p ij + p ji = 1 and p ii = 1 2 . The matrix\nA = log pij 1\u2212pij n i,j=1\nof logits is then antisymmetric. Intuitively, antisymmetry reflects a hyperadversarial setting where all pairwise interactions between players are zero-sum. In general, Hamiltonian games are related to -but distinct from -zero-sum games.\nExample 2 (an unconstrained 2 bimatrix game is zerosum iff it is Hamiltonian). Consider bimatrix game with 1 (x, y) = x Py and 2 (x, y) = x Qy. Then \u03be = (Py, Q x) and the Hessian components have block structure\n2A = 0 P \u2212 Q (Q \u2212 P) 0 2S = 0 P + Q (P + Q) 0\nThe game is Hamiltonian iff S = 0\niff P + Q = 0 iff 1 + 2 = 0.\nThere are Hamiltonian games that are not zero-sum and vice versa.\nExample 3 (Hamiltonian game that is not zero-sum). Fix constants a and b and suppose players 1 and 2 minimize losses\n1 (x, y) = x(y \u2212 b) and 2 (x, y) = \u2212(x \u2212 a)y\nwith respect to x and y respectively.\nExample 4 (zero-sum game that is not Hamiltonian). Players 1 and 2 minimize\n1 (x, y) = x 2 + y 2 2 (x, y) = \u2212(x 2 + y 2 ).\nThe game actually has potential function \u03c6(x, y) = x 2 \u2212 y 2 .\nHamiltonian games are quite different from potential games.\nThere is a Hamiltonian function H that specifies a conserved quantity. Whereas the dynamics equal \u2207\u03c6 in potential games; they are orthogonal to \u2207H in Hamiltonian games. The orthogonality implies the conservation law that underlies the cyclic behavior in example 1.\nTheorem 3. Let H(w) := 1 2 \u03be(w) 2 2 .\nIf the game is Hamiltonian then (i) \u2207H = A \u03be and (ii) \u03be preserves the level sets of H since \u03be, \u2207H = 0. If the Hessian is invertible and lim w \u2192\u221e H(w) = \u221e then (iii) gradient descent on H converges to a local Nash equilibrium.\nIn fact, H is a Hamiltonian 3 function for the game dynamics. We use the notation H(w) = 1 2 \u03be(w) 2 throughout the paper. However, H is only a Hamiltonian function for \u03be if the game is Hamiltonian. There is a precise mapping from Hamiltonian games to symplectic geometry, see Appendix E. Symplectic geometry is the modern formulation of classical mechanics. Recall that periodic behaviors (e.g. orbits) often arise in classical mechanics. The orbits lie on the level sets of the Hamiltonian, which expresses the total energy of the system.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithms", "text": "Fixed points of potential and Hamiltonian games can be found by descent on \u03be and \u2207H respectively. This section tackles finding stable fixed points in general games.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Finding fixed points", "text": "If the Hessian H(w) is invertible, then \u2207H = H \u03be = 0 iff \u03be = 0. Thus, gradient descent on H converges to fixed points of \u03be. However, there is no guarantee that descent on H will find a stable fixed point. Mescheder et al. (2017) propose consensus optimization, a gradient adjustment of the form\n\u03be + \u03bb \u2022 H \u03be = \u03be + \u03bb \u2022 \u2207H.\nUnfortunately, consensus optimization can converge to unstable fixed points even in simple cases where the 'game' is to minimize a single function: Example 5 (consensus optimization converges to global maximum). Consider a potential game with losses\n1 (x, y) = 2 (x, y) = \u2212 \u03ba 2 (x 2 + y 2 ) with \u03ba 0. Then \u03be = \u2212\u03ba \u2022 x y and H = \u2212 \u03ba 0 0 \u03ba Note that \u03be 2 = \u03ba 2 (x 2 + y 2 ) and \u03be + \u03bb \u2022 H \u03be = \u03ba(\u03bb\u03ba \u2212 1) \u2022 x y .\nDescent on \u03be + \u03bb \u2022 H \u03be converges to the global maximum (x, y) = (0, 0) unless \u03bb < 1 \u03ba . Although consensus optimization works well in some special cases like two-player zero-sum; it cannot be considered a candidate algorithm for finding stable fixed points in general games, since it fails in the basic case of potential games.", "publication_ref": ["b22"], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 1 Symplectic Gradient Adjustment", "text": "Input:\nlosses L = { i } n i=1 , weights W = {w i } n i=1 \u03be \u2190 gradient( i , w i ) for ( i , w i ) \u2208 (L, W) A \u03be \u2190 get sym adj(L, W) // appendix C if align then \u2207H \u2190 gradient( 1 2 \u03be 2 , w) for w \u2208 W) \u03bb \u2190 sign 1 d \u03be, \u2207H A \u03be, \u2207H + // = 1 10 else \u03bb \u2190 1 end if Output: \u03be + \u03bb \u2022 A \u03be\n// plug into any optimizer", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Finding stable fixed points", "text": "There are two classes of games where we know how to find stable fixed points: potential games where \u03be converges to a local minimum and Hamiltonian games where \u2207H, which is orthogonal to \u03be, finds stable fixed points. In the general case, the following desiderata provide a set of reasonable properties for an adjustment \u03be \u03bb of the game dynamics:\nDesiderata. To find stable fixed points, an adjustment \u03be \u03bb to the game dynamics should satisfy D1. compatible with game dynamics:\n\u03be \u03bb , \u03be = \u03b1 1 \u2022 \u03be 2 ;\nD2. compatible with potential dynamics: if the game is a potential game then \u03be \u03bb , \u2207\u03c6 = \u03b1 2 \u2022 \u2207\u03c6 2 ; D3. compatible with Hamiltonian dynamics: If the game is Hamiltonian then \u03be \u03bb , \u2207H = \u03b1 3 \u2022 \u2207H 2 ; D4. attracted to stable equilibria: in neighborhoods where S 0, require \u03b8(\u03be \u03bb , \u2207H) \u2264 \u03b8(\u03be, \u2207H); D5. repelled by unstable equilibria: in neighborhoods where S \u227a 0, require \u03b8(\u03be \u03bb , \u2207H) \u2265 \u03b8(\u03be, \u2207H); for some \u03b1 1 , \u03b1 2 , \u03b1 3 > 0.\nDesideratum D1 does not guarantee that players act in their own self-interest -this requires a stronger positivity condition on dot-products with subvectors of \u03be, see Balduzzi (2017). Desiderata D4 and D5 are explained in section 3.4. The unadjusted dynamics \u03be satisfies all the desiderata except D3. Consensus optimization only satisfies D3 and D4.\nIdeally, desideratum D5 would be strengthened to 'repelled by saddle points where \u03be(w) = 0 but S(w) 0'.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Symplectic gradient adjustment", "text": "Proposition 4. The symplectic gradient adjustment (SGA)\n\u03be \u03bb := \u03be + \u03bb \u2022 A \u03be.\nsatisfies D1-D3 for \u03bb > 0, with \u03b1 1 = 1 = \u03b1 2 and \u03b1 3 = \u03bb.\nNote that desiderata D1 and D2 are true even when \u03bb < 0. This will prove useful, since example 6 and theorem 5 show it is necessary pick negative \u03bb near S \u227a 0. Section 3.4 shows how to also satisfy desiderata D4 and D5.\nProof. First claim: \u03bb \u2022 \u03be A \u03be = 0 by skew-symmetry of A. Second claim: A \u2261 0 in a potential game, so \u03be \u03bb = \u03be = \u2207\u03c6. Third claim:\n\u03be \u03bb , \u2207H = \u03be \u03bb , H \u03be = \u03be \u03bb , A \u03be = \u03bb \u2022 \u03be AA \u03be = \u03bb \u2022 \u2207H 2 since H = A\nby assumption and since \u03be A \u03be = 0 by antisymmetry.\nIn the example below, almost any choice of positive \u03bb results in convergence to an unstable equilibrium. The problem arises from the combination of a weak repellor with a strong rotational force. The next section shows how to pick \u03bb to avoid unstable equilibria. Example 6 (failure case for \u03bb > 0). Suppose > 0 is small and\nf (x, y) = \u2212 2 x 2 \u2212 xy and g(x, y) = \u2212 2 y 2 + xy\nwith an unstable equilibrium at (0, 0). The dynamics are\n\u03be = \u2022 \u2212x \u2212y + \u2212y x with A = 0 \u22121 1 0 and A \u03be = x y + \u2212y x Finally observe that \u03be + \u03bb \u2022 A \u03be = (\u03bb \u2212 ) \u2022 x y + (1 + \u03bb) \u2022 \u2212y x\nwhich converges to the unstable equilibrium if \u03bb > .\nLemma 9 in the appendix shows that, if S and A commute and S 0, then \u03be \u03bb , \u2207H \u2265 0 for \u03bb > 0. The proof of theorem 5 introduces the additive condition number to upper-bound the worst-case noncommutativity of S, which allows to quantify the relationship between \u03be \u03bb and \u2207H. Theorem 5. Let S be a symmetric matrix with eigenvalues\n\u03c3 max \u2265 \u2022 \u2022 \u2022 \u2265 \u03c3 min . The additive condition number 4 of S is \u03ba = \u03c3 max \u2212 \u03c3 min . If S 0 is positive semidefinite with additive condition number \u03ba then \u03bb \u2208 (0, 4 \u03ba ) implies \u03be \u03bb , \u2207H \u2265 0. If S is negative semidefinite, then \u03bb \u2208 (0, 4 \u03ba ) implies \u03be \u2212\u03bb , \u2207H \u2264 0.\nThe inequalities are strict if H is invertible.\nIf \u03ba = 0, then S = \u03c3 \u2022 I commutes with all matrices. The larger the additive condition number \u03ba, the larger the potential failure of S to commute with other matrices. 4 The condition number of a positive definite matrix is \u03c3max \u03c3 min . ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "How to pick sign(\u03bb)", "text": "This section explains desiderata D4-D5 and shows how to pick sign(\u03bb) to speed up convergence towards stable and away from unstable fixed points.\nFirst, observe that \u03be, \u2207H = \u03be (S + A) \u03be = \u03be S\u03be. It follows that for \u03be = 0:\nif S 0 then \u03be, \u2207H \u2265 0; if S \u227a 0 then \u03be, \u2207H < 0.\n(1)\nA criterion to probe the positive/negative definiteness of S is thus to check the sign of \u03be, \u2207H . The dot product can take any value if S is not positive nor negative definite. The behavior near saddles is beyond the scope of the paper.\nDesiderata D4 can be interpreted as follows. If \u03be points at a stable equilibrium then we require that \u03be \u03bb points more towards the equilibrium (i.e. has smaller angle). Conversely if \u03be points away then the adjustment should point further away. More formally, Definition 4. Let u and v be two vectors. The infinitesimal alignment of \u03be \u03bb := u + \u03bb \u2022 v with a third vector w is\nalign(\u03be \u03bb , w) := d d\u03bb cos 2 \u03b8 \u03bb |\u03bb=0 for \u03b8 \u03bb := \u03b8(\u03be \u03bb , w).\nIf u and w point the same way, u w > 0, then align > 0 when v bends u further toward w, see figure 2A. Otherwise align > 0 when v bends u away from w, figure 2B. Proposition 6. Desiderata D4-D5 are satisfied for \u03bb such that \u03bb \u2022 \u03be, \u2207H \u2022 A \u03be, \u2207H \u2265 0.\nComputing the sign of \u03be, \u2207H provides a check for stable and unstable fixed points. Computing the sign of A \u03be, \u2207H checks whether the adjustment term points towards or away from the (nearby) fixed point. Putting the two checks together yields a prescription for the sign of \u03bb.  Alignment and convergence rates. Finally, we show that increasing alignment helps speed convergence. Gradient descent is also known as the method of steepest descent. In general games, however, \u03be does not follow the steepest path to fixed points due to the 'rotational force', which forces lower learning rates and slows down convergence:\nTheorem 7. Suppose f is convex and Lipschitz smooth with\n\u2207f (x) \u2212 \u2207f (y) \u2264 L \u2022 x \u2212 y . Let w t+1 = w t \u2212 \u03b7 \u2022 v where v = \u2207f (w t ) .\nThen the optimal step size is \u03b7 * = cos \u03b8 L where \u03b8 := \u03b8(\u2207f (w t ), v), with\nf (w t+1 ) \u2264 f (w t ) \u2212 cos 2 \u03b8 2L \u2022 \u2207f (w t ) 2 .\nIncreasing the cosine with the steepest direction improves convergence. The alignment computation in algorithm 1 chooses \u03bb to be positive or negative such that \u03be \u03bb is bent towards stable (increasing the cosine) and away from unstable fixed points. Adding a small > 0 to the computation introduces a weak bias towards stable fixed points.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Aligned consensus optimization", "text": "The stability criterion in (1) also provides a simple way to prevent consensus optimization from converging to unstable equilibria. Aligned consensus optimization is\n\u03be + |\u03bb| \u2022 sign \u03be, \u2207H \u2022 H \u03be,(2)\nwhere in practice we set \u03bb = 1. Aligned consensus satisfies desiderata D3-D5. However, it behaves strangely in potential games. Multiplying by the Hessian is the 'inverse' of Newton's method: it increases the gap between small and large eigenvalues, increasing the (usual, multiplicative) condition number and slows down convergence. Nevertheless, consensus optimization works well in GANs (Mescheder et al., 2017), and aligned consensus may improve performance, see figure 9 in appendix.\nFinally, note that dropping the first term \u03be from (2) yields a simpler update that also satisfies D3-D5. However, the resulting algorithm performs poorly in experiments (not shown), perhaps because it is attracted to saddles.", "publication_ref": ["b22"], "figure_ref": ["fig_10"], "table_ref": []}, {"heading": "Experiments", "text": "We compare SGA with simultaneous gradient descent, optimistic mirror descent (Daskalakis et al., 2018) and consensus optimization (Mescheder et al., 2017) in basic settings.", "publication_ref": ["b8", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Learning rates and alignment", "text": "We investigate the effect of SGA when a weak attractor is coupled to a strong rotational force:\n1 (x, y) = 1 2 x 2 + 10xy and 2 (x, y) = 1 2 y 2 \u2212 10xy\nGradient descent is extremely sensitive to the choice of learning rate \u03b7, top row of figure 3. As \u03b7 increases through {0.01, 0.032, 0.1} gradient descent goes from converging extremely slowly, to diverging slowly, to diverging rapidly. SGA yields faster, more robust convergence. SGA converges faster with learning rates \u03b7 = 0.01 and \u03b7 = 0.032, and only starts overshooting the fixed point for \u03b7 = 0.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Basic adversarial games", "text": "Figure 4 compares SGA with optimistic mirror descent on a zero-sum bimatrix game with 1/2 (w 1 , w 2 ) = \u00b1w 1 w 2 . The example is modified from Daskalakis et al. (2018) who also consider a linear offset that makes no difference. A run converges, panel A, if the average absolute value of losses on the last 10 iterations is < 0.01.\nAlthough OMD's peak performance is better than SGA, we find that SGA converges -and does so faster -for a wider range of learning rates. OMD diverges for learning rates not in the range [0.3, 1.2]. Simultaneous gradient descent oscillates without converging (not shown). Individual runs are shown in the appendix. The appendix also compares the performance of the algorithms on four-player games.", "publication_ref": ["b8"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Generative adversarial networks", "text": "We apply SGA to a basic setup adapted from Metz et al. (2017). Data is sampled from a highly multimodal distribution designed to probe the tendency to collapse onto a subset of modes during training. The distribution is a mixture of 16 Gaussians arranged in a 4 \u00d7 4 grid, see ground truth in figure 8. The generator and discriminator networks both have 6 ReLU layers of 384 neurons. The generator has two output neurons; the discriminator has one.\nFigure 5 shows results after {2000, 4000, 6000, 8000} iterations. The networks are trained under RMSProp. Learning rates were chosen by visual inspection of grid search results at iteration 8000, see appendix. Simultaneous gradient descent and SGA are shown in the figure. Results for consensus optimization are in the appendix.\nSimultaneous gradient descent exhibits mode collapse fol- lowed by mode hopping in later iterations (not shown). Mode hopping is analogous to the cycles in example 1. Unaligned SGA converges to the correct distribution; alignment speeds up convergence slightly. Consensus optimization performs similarly, see figure 9 in the appendix. However, it can converge to local maxima, recall example 5.", "publication_ref": ["b23"], "figure_ref": ["fig_9", "fig_10"], "table_ref": []}, {"heading": "Discussion", "text": "Modern deep learning treats differentiable modules like plug-and-play lego blocks. For this to work, at the very least, we need to know that gradient descent will find local minima. Unfortunately, gradient descent does not necessarily find local minima when optimizing multiple interacting objectives. With the recent proliferation of algorithms that optimize more than one loss, it is becoming increasingly urgent to understand and control the dynamics of interacting losses. Although there is interesting recent work on twoplayer adversarial games such as GANs, there is essentially no work on finding stable fixed points in general.\nThe generalized Helmholtz decomposition provides a powerful new perspective on game dynamics. A key feature is that the analysis is indifferent to the number of players. Instead, it is the interplay between the simultaneous gradient \u03be on the losses and the symmetric and antisymmetric matrices of second-order terms that guides algorithm design and governs the dynamics under gradient adjustments.\nSymplectic gradient adjustment is a straightforward application of the Helmholtz decomposition. It is unlikely that SGA is the only or best approach to finding stable fixed points. A deeper understanding of the interaction between the potential and Hamiltonian components will lead to more effective algorithms. Of particular interest are pure firstorder methods that do not use Hessian-vector products.\nFinally, it is worth raising a philosophical point. The goal in this paper is to find stable fixed points (e.g. because in GANs they yield pleasing samples). We are not concerned with the losses of the players per se. The gradient adjustments may lead to a player acting against its own self-interest by increasing its loss. We consider this acceptable insofar as it encourages convergence to a stable fixed point.\nLemma 8 (local Nash equilibria are stable fixed points in two-player zero-sum games). If a game is two-player zerosum, then local Nash equilibria are stable fixed points.\nProof. Consider a two-player zero-sum game with losses f (x, y) and g(x, y) = \u2212f (x, y).\nThen \u03be = (\u2207 x f, \u2212\u2207 y f ) and H = \u2207 2 x,x f \u2207 2 x,y f \u2212\u2207 2 y,x f \u2212\u2207 2 y,y f = \u2207 2 x,x f 0 0 \u2212\u2207 2 y,y f S + 0 \u2207 2 x,y f \u2212\u2207 2 y,x f 0 A\nThus, S is positive semidefinite iff the blocks \u2207 2 x,x f and \u2207 2 y,y g = \u2212\u2207 2 y,y f are both positive semidefinite. The result follows.\nThe following example, due to Alistair Letcher, shows there are local Nash equilibria that are not stable fixed points:\nExample 7 (a local Nash equilibrium that is not stable). Let 1 (x, y) =\nx 2 2 + 2xy and 2 (x, y) = y 2 2 + 2xy\nThen\n\u03be = x + 2y y + 2x and H = 1 2 2 1 .\nThere is a fixed point at (x, y) = (0, 0) which is a local Nash equilibrium since \u2202 2 \u2202x 2 1 (0, 0) = 1 = \u2202 2 \u2202y 2 2 (0, 0) > 0. However, S = H has eigenvalues \u03bb 1 = 3 and \u03bb 2 = \u22121, and so is not positive semidefinite.\nWhich is the 'right' solution concept? We prefer stable fixed points. In the case of GANs (two-player zero-sum) there is no difference between the solution concepts by lemma 8. However, the solution concepts differ for potential games. Example 7 is a potential game since S = H. It is easy to check that the potential function\n\u03c6(x, y) = x 2 2 + 2xy + y 2 2\nsatisfies \u03be = \u2207\u03c6 and H = \u2207 2 \u03c6. The Nash equilibrium is a saddle-point for the potential function. An algorithm that is guaranteed to converge to the Nash equilibrium in example 7 is thus guaranteed to converge to the saddle point of the potential function \u03c6.\nThis rules out local Nash equilibrium as a solution concept for our purposes -since desideratum D2 is that the gradient adjustment behaves well for potential games and converging to saddles of the potential function does not count as good behavior.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B. Proofs", "text": "Proof of theorem 3.\nProof. Direct computation shows \u2207H = H \u03be for any game. The first statement follows since H = A in Hamiltonian games. For the second statement, the directional derivative is\nD \u03be H = \u03be, \u2207H = \u03be A \u03be where \u03be A \u03be = (\u03be A \u03be) = \u03be A\u03be = \u2212(\u03be A \u03be) since A = \u2212A by skew-symmetry. It follows that \u03be A \u03be = 0.\nFor the third, gradient descent on H will converge to a point where \u2207H = H \u03be(w) = 0. If the Hessian is invertible then clearly \u03be(w) = 0. The fixed-point is a local Nash equilibrium by Lemma 2 since 0 \u2261 S 0 in a Hamiltonian game.\nProof of theorem 5. The following lemma, whilst not used in the proof of theorem 5, nevertheless provides a useful intuition about the role of commutativity.\nRecall that two matrices A and S commute iff [A, S] := AS \u2212 SA = 0. That is, iff AS = SA. Lemma 9. If S 0 is symmetric positive semidefinite and S commutes with A then \u03be \u03bb , \u2207H \u2265 0 for all \u03bb \u2265 0.\nProof. First observe that \u03be AS\u03be = \u03be S A \u03be = \u2212\u03be SA\u03be, where the first equality holds since the expression is a scalar, and the second holds since S = S and A = \u2212A . It follows that \u03be AS\u03be = 0 if SA = AS.\nRewrite the inequality as\n\u03be \u03bb \u2022 A \u03be, \u2207H = \u03be + \u03bb \u2022 A \u03be, S\u03be + A \u03be = \u03be S\u03be + \u03bb\u03be AA \u03be \u2265 0\nsince \u03be AS\u03be = 0 and by positivity of S, \u03bb and AA .\nThe proof of the theorem follows.\nProof. We prove the case S 0; the case S 0 is similar.\nRewrite the inequality as\n\u03be + \u03bb \u2022 A \u03be, \u2207H = (\u03be + \u03bb \u2022 A \u03be) \u2022 (S + A )\u03be = \u03be S\u03be + \u03bb\u03be AS\u03be + \u03bb\u03be AA \u03be Let \u03b2 = A \u03be andS = S \u2212 \u03c3 min \u2022 I,\nwhere I is the identity matrix. Then\n\u03be S\u03be + \u03bb\u03be AS\u03be + \u03bb \u2022 \u03b2 2 \u2265 \u03be S \u03be + \u03bb\u03be AS\u03be + \u03bb \u2022 \u03b2 2\nsince \u03be S\u03be \u2265 \u03be S \u03be by construction and \u03be AS\u03be = \u03be AS\u03be \u2212 \u03c3 min \u03be A\u03be = \u03be AS\u03be because \u03be A\u03be = 0 by the skew-symmetry of A. It therefore suffices to show that the inequality holds when \u03c3 min = 0 and \u03ba = \u03c3 max .\nSince S is positive semidefinite, there exists an uppertriangular square-root matrix T such that T T = S and so \u03be S\u03be = T\u03be 2 . Further,\n|\u03be AS\u03be| \u2264 A \u03be \u2022 T T\u03be \u2264 \u221a \u03c3 max \u2022 A \u03be \u2022 T\u03be .\nsince T 2 = \u221a \u03c3 max . Putting the observations together obtains\nT\u03be 2 + \u03bb( A\u03be 2 \u2212 A\u03be, S\u03be ) \u2265 T\u03be 2 + \u03bb( A\u03be 2 \u2212 A\u03be S\u03be \u2265 T\u03be 2 + \u03bb A\u03be ( A\u03be \u2212 S\u03be ) \u2265 T\u03be 2 + \u03bb A\u03be ( A\u03be \u2212 \u221a \u03c3 max T\u03be ) Set \u03b1 = \u221a \u03bb and \u03b7 = \u221a \u03c3 max . We can continue the above computation = T\u03be 2 + \u03b1 2 A\u03be ( A\u03be \u2212 \u03b7 T\u03be ) = T\u03be 2 + \u03b1 2 A\u03be 2 \u2212 \u03b1 2 A\u03be \u03b7 T\u03be =( T\u03be \u2212 \u03b1 A\u03be ) 2 + 2\u03b1 A\u03be T\u03be \u2212 \u03b1 2 \u03b7 A\u03be T\u03be =( T\u03be \u2212 \u03b1 A\u03be ) 2 + A\u03be T\u03be (2\u03b1 \u2212 \u03b1 2 \u03b7)\nFinally, 2\u03b1 \u2212 \u03b1 2 \u03b7 > 0 for any \u03b1 in the range (0, 2 \u03b7 ), which is to say, for any 0 < \u03bb < 4 \u03c3max . The kernel of S and the kernel of T coincide. If \u03be is in the kernel of A, resp. T, it cannot be in the kernel of T, resp. A and the term ( T\u03be \u2212 \u03b1 A\u03be ) 2 is positive. Otherwise, the term A\u03be T\u03be is positive.\nProof of proposition 6.\nLemma 10. When \u03be \u03bb is the symplectic gradient adjustment, sign align(\u03be \u03bb , \u2207H) = sign \u03be, \u2207H \u2022 A \u03be, \u2207H .\nProof. Observe that\ncos 2 \u03b8 \u03bb = \u03be \u03bb , \u2207H \u03be \u03bb \u2022 \u2207H 2 = \u03be, \u2207H + 2\u03bb \u03be, \u2207H A \u03be, \u2207H + O(\u03bb 2 ) \u03be 2 + O(\u03bb 2 ) \u2022 \u2207H 2\nwhere the denominator has no linear term in \u03bb because \u03be \u22a5 A \u03be. It follows that the sign of the infinitesimal alignment is\nsign d d\u03bb cos 2 \u03b8 \u03bb = sign \u03be, \u2207H A \u03be, \u2207H as required.\nThe proposition follows easily.\nProof. If we are in a neighborhood of a stable fixed point then \u03be, \u2207H \u2265 0. It follows by lemma 10 that sign align(\u03be \u03bb ), \u2207H) = sign A \u03be, \u2207H and so choosing sign(\u03bb) = sign A \u03be, \u2207H leads to the angle between \u03be \u03bb and \u2207H being smaller than the angle between \u03be and \u2207H, satisfying desideratum D4.\nThe proof for the unstable case is similar.\nProof of theorem 7. The following lemma provides some intuition for theorem 7. The idea is that, the smaller the cosine between the 'correct direction' w and the 'update direction' \u03be, the smaller the learning rate needs to be for the update to stay in a unit ball, see figure 6.\nLemma 11 (alignment lemma). If w and \u03be are unit vectors with\n0 < w \u03be then w \u2212 \u03b7 \u2022 \u03be \u2264 1 for 0 \u2264 \u03b7 \u2264 2w \u03be. Proof. Check w \u2212 \u03b7 \u2022 \u03be 2 = 1 + \u03b7 2 \u2212 2\u03b7 \u2022 w \u03be \u2264 1 iff \u03b7 2 \u2264 2\u03b7 \u2022 w \u03be. The result follows.\nThe next lemma is a standard technical result (e.g. Nesterov, 2004). . Alignment and learning rates. The larger cos \u03b8, the larger the learning rate \u03b7 that can be applied to unit vector \u03be without w + \u03b7 \u2022 \u03be leaving the unit circle.\nLemma 12. Let f :\nR d \u2192 R be a convex Lipschitz smooth function satisfying \u2207f (y) \u2212 \u2207f (x) \u2264 L \u2022 y \u2212 x for all x, y \u2208 R d . Then |f (y) \u2212 f (x) \u2212 \u2207f (x), y \u2212 x | \u2264 L 2 \u2022 y \u2212 x 2 for all x, y \u2208 R d .\nFinally the proof of the theorem is adapted from a standard result in Nesterov (2004), where there is no cosine.\nProof. By the above lemma,\nf (y) \u2264 f (x) + \u2207f (x), y \u2212 x + L 2 y \u2212 x 2 = f (x) \u2212 \u03b7 \u2022 \u2207f, \u03be + \u03b7 2 L 2 \u2022 \u03be 2 = f (x) \u2212 \u03b7 \u2022 \u2207f, \u03be + \u03b7 2 L 2 \u2022 \u2207f 2 = f (x) \u2212 \u03b7(\u03b1 \u2212 \u03b7 2 L) \u2022 \u2207f 2\nwhere \u03b1 := cos \u03b8. Solve\nmin \u03b7 \u2206(\u03b7) = min \u03b7 \u2212\u03b7(\u03b1 \u2212 \u03b7 2 L)\nto obtain \u03b7 * = \u03b1 L and \u2206(\u03b7 * ) = \u2212 \u03b1 2 2 L as required.", "publication_ref": ["b26", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "C. TensorFlow code to compute A \u03be:", "text": "The code requires a list of n losses, Ls, and a list of variables for the n players, xs. The function fwd gradients is in the module tf.contrib.kfac.utils.  xi\n= [tf.gradients( , x)[0] for ( , x) in zip(Ls, xs)] H xi = jac vec(xi, xs, xi) Ht xi = jac tran vec(xi, xs, xi) At xi = [ ht\u2212h 2 for (h, ht) in zip(H xi, Ht xi)] return At xi D.\nFurther experiments OMD and SGA on a four-player game. \nA = \uf8eb \uf8ec \uf8ec \uf8ed 0 1 1 1 \u22121 0 1 1 \u22121 \u22121 0 1 \u22121 \u22121 \u22121 0 \uf8f6 \uf8f7 \uf8f7 \uf8f8 and the symmetric component is S = \u2022 \uf8eb \uf8ec \uf8ec \uf8ed 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 \uf8f6 \uf8f7 \uf8f7 \uf8f8 .\nOMD converges considerably slower than SGA across the full range of learning rates. It also diverges for learning rates > 0.22. SGA converges more quickly and robustly. Ground truth for GAN experiments. Figure 8 shows the probability distribution that is sampled to train the generator and discriminator in the GAN example.\nMore details on GAN experiments. Figure 9 shows the performance of consensus optimization without and with alignment. Introducing alignment slightly improves speed of convergence (second column) and final result (fourth column), although intermediate results in third column are ambiguous.\nGrid search was over learning rates {1e-5, 2e-5,5e-5, 8e-5, 1e-4, 2e-4, 5e-4} and then a more refined linear search over [8e-5, 2e-4].", "publication_ref": [], "figure_ref": ["fig_9", "fig_10"], "table_ref": []}, {"heading": "E. Helmholtz, Hamilton, Hodge, and Harmonic games", "text": "This section explains the mathematical connections with the Helmholtz decomposition, symplectic geometry and the Hodge decomposition. The discussion is not necessary to understand the main text. It is also not self-contained. The details can be found in textbooks covering differential and symplectic geometry.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1. The Helmholtz decomposition", "text": "The classical Helmholtz decomposition states that any vector field \u03be in 3-dimensions is the sum of curl-free (gradient) and divergence-free (infinitesimal rotation) components:\n\u03be = \u2207\u03c6 gradient component + curl(B) rotational component curl(\u2022) := \u2207\u00d7(\u2022)\nWe explain the link between curl and the antisymmetric component of the game Hessian. Recall that gradients of functions are actually differential 1-forms, not vector fields.\nDifferential 1-forms and vector fields on a manifold are canonically isomorphic once a Riemannian metric has been chosen. In our case, we are implicitly using the Euclidean metric. The antisymmetric matrix A is the differential 2form obtained by applying the exterior derivative d to the 1-form \u03be.\nIn 3-dimensions, the Hodge star operator is an isormorphism from differential 2-forms to vector fields, and the curl can be reformulated as curl(\u2022) = * d(\u2022). In claiming A is analogous to curl, we are simply dropping the Hodge-star operator.\nFinally, recall that the Lie algebra of infinitesimal rotations in d-dimensions is given by antisymmetric matrices. When d = 3, the Lie algebra can be represented as vectors (three numbers specify a 3 \u00d7 3 antisymmetric matrix) with the \u00d7-product as Lie bracket. In general, the antisymmetric matrix A captures the infinitesimal tendency of \u03be to rotate at each point in the parameter space.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.2. Hamiltonian mechanics", "text": "A symplectic form \u03c9 is a closed nondegenerate differential 2-form. Given a manifold with a symplectic form, a vector field \u03be is Hamiltonian vector field if there exists a function\nH : M \u2192 R satisfying \u03c9(\u03be, \u2022) = dH(\u2022) = \u2207H, \u2022 . (3\n)\nThe function is then referred to as the Hamiltonian function of the vector field. In our case, the antisymmetric matrix A is a closed 2-form because A = d\u03be and d \u2022 d = 0. It may however be degenerate. It is therefore a presymplectic form (Bottacin, 2005).\nSetting \u03c9 = A, equation ( 3) can be rewritten in our notation as \u03c9(\u03be, \u2022)\nA \u03be = dH(\u2022) \u2207H ,\njustifying the terminology 'Hamiltonian'.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "E.3. The Hodge decomposition", "text": "The exterior derivative d k : \u2126 k (M ) \u2192 \u2126 k+1 (M ) is a linear operator that takes differential k-forms on a manifold M , \u2126 k (M ), to differential k + 1-forms, \u2126 k+1 (M ). In the case k = 0, the exterior derivative is the gradient, which takes 0-forms (that is, functions) to 1-forms. Given a Riemannian metric, the adjoint of the exterior derivative \u03b4 goes in the opposite direction. Hodge's theorem states that k-forms on a compact manifold decompose into a direct sum over three types: Setting k = 1, we recover a decomposition that closely resembles the generalized Helmholtz decomposition:\n\u2126 k (M ) = d\u2126 k\u22121 (M ) \u2295 Harmonic k (M ) \u2295 \u03b4\u2126 k+1 (M ).\n\u2126 1 (M ) 1-forms = d\u2126 0 (M ) gradients of functions \u2295Harm k (M )\u2295 \u03b4\u2126 2 (M ) antisymmetric component\nThe harmonic component is isomorphic to the de Rham cohomology of the manifold -which is zero when k = 1 and M = R n .\nUnfortunately, the Hodge decomposition does not straightforwardly apply to the case when M = R n , since R n is not compact. It is thus unclear how to relate the generalized Helmholtz decomposition to the Hodge decomposition.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.4. Harmonic and potential games", "text": "Candogan et al. (2011) derive a Hodge decomposition for games that is closely related in spirit to our generalized Helmholtz decomposition -although the details are quite different. Candogan et al. (2011) work with classical games (probability distributions on finite strategy sets). Their losses are multilinear, which is easier than our setting, but they have constrained solution sets, which is harder in many ways. Their approach is based on combinatorial Hodge theory rather than differential and symplectic geometry. Finding a best-of-both-worlds approach that encompasses both settings is an open problem.", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "F. Type consistency", "text": "The next two sections carefully work through the units in classical mechanics and two-player games respectively. The third section briefly describes a use-case for type consis-tency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1. Units in classical mechanics", "text": "Consider the well-known Hamiltonian\nH(p, q) = 1 2 \u03ba \u2022 q 2 + 1 \u00b5 \u2022 p 2\nwhere q is position, p = \u00b5 \u2022q is momentum, \u00b5 is mass, \u03ba is surface tension and H measures energy. The units (denoted by \u03c4 ) are\n\u03c4 (q) = m \u03c4 (p) = kg\u2022m s \u03c4 (\u03ba) = kg s 2 \u03c4 (\u00b5) = kg\nwhere m is meters, kg is kilograms and s is seconds. Energy is measured in joules, and indeed it is easy to check that \u03c4 (H) = kg\u2022m 2 s 2 . Note that the units for differentation by x are \u03c4 ( \u2202 \u2202x ) = 1 \u03c4 (x) . For example, differentiating by time has units 1 s . Hamilton's equations state thatq\n= \u2202H \u2202p = 1 \u00b5 \u2022 p and\u1e57 = \u2212 \u2202H \u2202q = \u2212\u03ba \u2022 q where \u03c4 (q) = m s \u03c4 (\u1e57) = kg\u2022m s 2 \u03c4 \u2202 \u2202q = 1 m \u03c4 \u2202 \u2202p = s kg\u2022m\nThe resulting flow describing the dynamics of the system is\n\u03be =q \u2022 \u2202 \u2202q +\u1e57 \u2022 \u2202 \u2202p = 1 \u00b5 p \u2022 \u2202 \u2202q \u2212 \u03baq \u2022 \u2202 \u2202p\nwith units \u03c4 (\u03be) = 1 s . Hamilton's equations can be reformulated more abstractly via symplectic geometry. Introduce the symplectic form \u03c9 = dq \u2227 dp with units \u03c4 (\u03c9) = kg \u2022 m 2 s .\nObserve that contracting the flow with the Hamiltonian obtains\n\u03b9 \u03be \u03c9 = \u03c9(\u03be, \u2022) = dH = \u2202H \u2202q \u2022 dq + \u2202H \u2202p \u2022 dp with units \u03c4 (dH) = \u03c4 (H) = kg\u2022m 2 s 2 .\nLosses in classical mechanics. Although there is no notion of \"loss\" in classical mechanics, it is useful (for the next section) to keep pushing the formal analogy. Define the \"losses\"\n1 (q, p) = 1 \u00b5 \u2022 qp and 2 (q, p) = \u2212\u03ba \u2022 qp (4) with units \u03c4 ( 1 ) = m 2 s and \u03c4 ( 2 ) = kg 2 \u2022m 2 s 3\n. The Hamiltonian dynamics can then be recovered game-theoretically by differentiating 1 and 2 with respect to q and p respectively. It is easy to check that\n\u03be = \u2202H \u2202p \u2202 \u2202q \u2212 \u2202H \u2202q \u2202 \u2202p = \u2202 1 \u2202q \u2202 \u2202q + \u2202 2 \u2202p \u2202 \u2202p .\nThe duality between vector fields and differential forms.\nFinally recall that the symplectic form in games was not \"pulled out of thin air\" as \u03c9 = dq \u2227 dp, but rather derived as \u03c9 = d\u03be , where \u03be is the differential form corresponding to the vector field \u03be under the musical isomorphism : T M \u2192 T * M . \n\u03be = \u2202 1 \u2202q \u2022 dq + \u2202 2 \u2202p \u2022 dp results in a type violation because \u03c4 \u2202 1 \u2202q \u2022 dq = \u03c4 ( 1 ) = m 2 s whereas \u03c4 \u2202 2 \u2202p \u2022 dp = \u03c4 ( 2 ) = kg 2 \u2022 m 2 s 3\nand we cannot add objects with different types. The correction terms in the direction : T M \u2192 T * M invert the coupling terms \u03ba and 1 \u00b5 that were originally introduced into the Hamiltonian for physical reasons. Applying the corrected musical isomorphism to \u03be yields\n\u03be = \u00b5 2 \u2022 \u2202f \u2202q \u2022 dq + 1 2\u03ba \u2022 \u2202g \u2202p \u2022 dp = 1 2 (p \u2022 dq \u2212 q \u2022 dp) .\nThe two terms of \u03be then have coherent types\n\u03c4 \u2202 1 \u2202q \u2022 \u00b5 \u2022 dq = m s \u2022 kg \u2022 m = kg\u2022m 2 s \u03c4 \u2202 2 \u2202p \u2022 1 \u03ba \u2022 dp = kg\u2022m s 2 \u2022 s 2 kg \u2022 kg\u2022m s = kg\u2022m 2 s\nas required. The associated two form is\n\u03c9 := d\u03be = \u2212 \u00b5 \u2022 \u2202 2 f \u2202q\u2202p \u2212 1 \u03ba \u2022 \u2202 2 g \u2202q\u2202p dq\u2227dp = \u2212dq\u2227dp\nwhich recovers the symplectic form (up to sign) with units \u03c4 (\u03c9) = kg\u2022m 2 s as required. Finally, observe that\n\u03be, \u03be = 1 2 p \u00b5 \u2022 \u2202 \u2202q \u2212 \u03baq \u2022 \u2202 \u2202p , p \u2022 dq \u2212 q \u2022 dp = 1 2 \u03ba \u2022 q 2 + 1 \u00b5 \u2022 p 2 = H(p, q)\nrecovering the Hamiltonian.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.2. Units in two-player games", "text": "Without loss of generality let w = (x; y) where we refer to x as position and y as momentum so that \u03c4 (x) = m and \u03c4 (y) = kg\u2022m s . The aim of this section is to check typeconsistency under these, rather arbitrarily assigned, units. Since we are considering a game, we do not require that x and y have the same dimension -even though this would necessarily be the case for a physical system. The goal is to verify that units can be consistently assigned to games. .\nType-consistency via SVD. It is necessary, as in section F.1, to correct the naive musical isomorphism by taking into account the coupling constants for the mixed positionmomentum terms. In the classical setup the coupling constants were the scalars 1 \u00b5 and \u03ba, whereas in a game they are the off-diagonal blocks A 12 and C 12 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Apply singular value decomposition to factorize", "text": "A 12 = U A D A V A and C 12 = U C D C V C\nwhere the entries of the diagonal matrices have types \u03c4 (D A ) = 1 kg and \u03c4 (D C ) = kg s 2 , and the types of the orthogonal matrices U and V are pure scalars. The diagonal matrices D A and D C have the same types as 1 \u00b5 and \u03ba in the classical system since they play the same coupling role.\nExtending the procedure adopted in the section F.1, fix the type-inconsistency by defining the musical isomorphisms as\n\u2202 \u2202x = U A D \u22121 A U A \u2022 dx and \u2202 \u2202y = V C D \u22121 C V C \u2022 dy.\nAlternatively, the isomorphisms can be computed by noting that U A D \u22121 A U A = ( \u221a A 12 A 21 ) \u22121 and V C D \u22121\nC V C = ( \u221a C 21 C 12 ) \u22121 .\nThe dual isomorphism : T * M \u2192 T M is then\n(dx) = U A D A U A \u2022 \u2202 \u2202x and (dy) = V C D C V C \u2022 \u2202 \u2202y If \u03be = A 12 y + b 1 C 21 x + d 2 then it follows that \u03be = U A D \u22121 A U A b 1 + U A U A y V C D \u22121 C V C d 2 + V C V C x with associated closed two form \u03c9 \u03c4 = d\u03be = \u2212 (U A V A \u2212 U C V C ) dx \u2227 dy.\nwhere the notation \u03c9 \u03c4 emphasizes that the two-form is typeconsistent.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.3. What does type-consistency buy?", "text": "Example 8. Consider the loss functions f (x, y) = xy and g(x, y) = 2xy, with \u03be = (y, 2x). There is no function \u03c6 : R 2 \u2192 R such that \u2207\u03c6 = \u03be. However, there is a family of functions \u03c6 \u03b1 (x, y) = \u03b1 \u2022 xy which satisfies \u03be, \u2207\u03c6 \u03b1 = \u03b1 \u2022 (x 2 + 2y 2 ) \u2265 0 for all \u03b1 > 0.\nAlthough \u03be is not a potential field, there is a family of functions on which \u03be performs gradient descent -albeit with coordinate-wise learning rates that may not be optimal. The vector field \u03be arguably does not require adjustment. This kind of situation often arises when the learning rates of different parameters are set adaptively during training of neural nets, by rescaling them by positive numbers.\nThe vanilla and type-consistent 1-forms corresponding to \u03be are, respectively, \u03be = y \u2022 dx + 2x \u2022 dy and \u03be \u03c4 = y \u2022 dx + x \u2022 dy with \u03c9 = d\u03be non = dx \u2227 dy and \u03c9 \u03c4 = d\u03be \u03c4 = 0.\nIt follows that the type-consistent symplectic gradient adjustment is zero. Type-consistency 'detects' that no gradient adjustment is needed in example 8. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgements. We thank Guillame Desjardins, Csaba Szepesvari and especially Alistair Letcher for useful feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPENDIX", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Stable fixed points vs local Nash equilibria", "text": "The main text introduces two solutions concepts for general games: stable fixed points and local Nash equilibria. Lemma 2 shows that if w * is a stable fixed point then it is also a local Nash equilibrium. If the game is two-player zero-sum, then the converse also holds:", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A multiagent reinforcement learning algorithm with non-linear dynamics", "journal": "J. Artif", "year": "", "authors": "S Abdallah; V R Lesser"}, {"ref_id": "b1", "title": "", "journal": "Differentiable Game Mechanics Intell. Res", "year": "2008", "authors": ""}, {"ref_id": "b2", "title": "Strongly-Typed Agents are Guaranteed to Interact Safely", "journal": "", "year": "2017", "authors": "D Balduzzi"}, {"ref_id": "b3", "title": "A Marsden-Weinstein Reduction Theorem for Presymplectic Manifolds", "journal": "", "year": "2005", "authors": "F Bottacin"}, {"ref_id": "b4", "title": "Multiagent learning using a variable learning rate", "journal": "Artificial Intelligence", "year": "2002", "authors": "M Bowling; M Veloso"}, {"ref_id": "b5", "title": "Convergence and no-regret in multiagent learning", "journal": "", "year": "2004", "authors": "M H Bowling"}, {"ref_id": "b6", "title": "Flows and decompositions of games: Harmonic and potential games", "journal": "Mathematics of Operations Research", "year": "2011", "authors": "O Candogan; I Menache; A Ozdaglar; P A Parrilo"}, {"ref_id": "b7", "title": "The Complexity of Computing a Nash Equilibrium", "journal": "SIAM J. Computing", "year": "2009", "authors": "C Daskalakis; P W Goldberg; C Papadimitriou"}, {"ref_id": "b8", "title": "Training GANs with Optimism", "journal": "", "year": "2018", "authors": "C Daskalakis; A Ilyas; V Syrgkanis; H Zeng"}, {"ref_id": "b9", "title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "journal": "", "year": "2014", "authors": "Y Dauphin; R Pascanu; C Gulcehre; K Cho; S Ganguli; Y Bengio"}, {"ref_id": "b10", "title": "Learning with Opponent-Learning Awareness", "journal": "", "year": "2018", "authors": "J N Foerster; R Y Chen; M Al-Shedivat; S Whiteson; P Abbeel; I Mordatch"}, {"ref_id": "b11", "title": "", "journal": "Generative Adversarial Nets. In NIPS", "year": "2014", "authors": "I J Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b12", "title": "Simple Adaptive Strategies: From Regret-Matching to Uncoupled Dynamics", "journal": "World Scientific", "year": "2013", "authors": "S Hart; A Mas-Colell"}, {"ref_id": "b13", "title": "GANs Trained by a Two Time-Scale Update Rule Converge to a Nash Equilibrium", "journal": "", "year": "2017", "authors": "M Heusel; H Ramsauer; T Unterthiner; B Nessler; G Klambauer; S Hochreiter"}, {"ref_id": "b14", "title": "Learning for General-Sum Stochastic Games", "journal": "Journal of Machine Learning Research", "year": "2003", "authors": "J Hu; M P Wellman;  Nash Q-"}, {"ref_id": "b15", "title": "Decoupled Neural Interfaces using Synthetic Gradients", "journal": "", "year": "2017", "authors": "M Jaderberg; W M Czarnecki; S Osindero; O Vinyals; A Graves; K Kavukcuoglu"}, {"ref_id": "b16", "title": "", "journal": "", "year": "2017", "authors": "J Lee; I Panageas; G Piliouras; M Simchowitz; M Jordan; B Recht"}, {"ref_id": "b17", "title": "Gradient Descent Converges to Minimizers", "journal": "", "year": "2016", "authors": "J D Lee; M Simchowitz; M I Jordan; B Recht"}, {"ref_id": "b18", "title": "Proximal Gradient Temporal Difference Learning Algorithms", "journal": "", "year": "2016", "authors": "B Liu; J Liu; M Ghavamzadeh; S Mahadevan; M Petrik"}, {"ref_id": "b19", "title": "Hamiltonian games", "journal": "Journal of Combinatorial Theory, Series B", "year": "1992", "authors": "X Lu"}, {"ref_id": "b20", "title": "Cycles in adversarial regularized learning", "journal": "", "year": "2018", "authors": "P Mertikopoulos; C Papadimitriou; G Piliouras"}, {"ref_id": "b21", "title": "On the convergence properties of GAN training", "journal": "", "year": "2018", "authors": "L Mescheder"}, {"ref_id": "b22", "title": "The Numerics of GANs", "journal": "", "year": "2017", "authors": "L Mescheder; S Nowozin; A Geiger"}, {"ref_id": "b23", "title": "Unrolled generative adversarial networks", "journal": "", "year": "2017", "authors": "L Metz; B Poole; D Pfau; J Sohl-Dickstein"}, {"ref_id": "b24", "title": "", "journal": "Potential Games. Games and Economic Behavior", "year": "1996", "authors": "D Monderer; L S Shapley"}, {"ref_id": "b25", "title": "Gradient descent GAN optimization is locally stable", "journal": "", "year": "2017", "authors": "V Nagarajan; J Z Kolter"}, {"ref_id": "b26", "title": "Introductory Lectures on Convex Optimization: A Basic Course", "journal": "Kluwer", "year": "2004", "authors": "Y Nesterov"}, {"ref_id": "b27", "title": "From Nash Equilibria to Chain Recurrent Sets: Solution Concepts and Topology", "journal": "", "year": "2016", "authors": "C Papadimitriou; G Piliouras"}, {"ref_id": "b28", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "journal": "", "year": "2017", "authors": "D Pathak; P Agrawal; A A Efros; Darrell ; T "}, {"ref_id": "b29", "title": "Connecting Generative Adversarial Networks and Actor-Critic Methods", "journal": "", "year": "2016", "authors": "D Pfau; O Vinyals"}, {"ref_id": "b30", "title": "Imagination-Augmented Agents for Deep Reinforcement Learning", "journal": "", "year": "2017", "authors": "S Racani\u00e8re; T Weber; D P Reichert; L Buesing; A Guez; D J Rezende; A P Badia; O Vinyals; N Heess; Y Li; R Pascanu; P Battaglia; D Hassabis; D Silver; D Wierstra"}, {"ref_id": "b31", "title": "A Class of Games Possessing Pure-Strategy Nash Equilibria", "journal": "Int J Game Theory", "year": "1973", "authors": "R W Rosenthal"}, {"ref_id": "b32", "title": "Improved Techniques for Training GANs", "journal": "", "year": "2016", "authors": "T Salimans; I Goodfellow; W Zaremba; V Cheung; A Radford; Chen ; X "}, {"ref_id": "b33", "title": "Algorithmic, Game-Theoretic, and Logical Foundations", "journal": "Cambridge University Press", "year": "2008", "authors": "Y Shoham; K Multiagent Leyton-Brown;  Systems"}, {"ref_id": "b34", "title": "Nash Convergence of Gradient Dynamics in General-Sum Games", "journal": "", "year": "2000", "authors": "S Singh; M Kearns; Y Mansour"}, {"ref_id": "b35", "title": "Learning correlated equilibria in games with compact sets of strategies", "journal": "Games and Economic Behavior", "year": "2007", "authors": "G Stoltz; G Lugosi"}, {"ref_id": "b36", "title": "FeUdal Networks for Hierarchical Reinforcement Learning", "journal": "", "year": "2017", "authors": "A Vezhnevets; S Osindero; T Schaul; N Heess; M Jaderberg; D Silver; K Kavukcuoglu"}, {"ref_id": "b37", "title": "Hierarchical Control Using Networks Trained with Higher-Level Forward Models", "journal": "Neural Computation", "year": "2014", "authors": "G Wayne; L F Abbott"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. (A) \u03be cycles around the origin. (B) gradient descent on H converges to Nash.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Proof.Any matrix decomposes uniquely as M = S + A where S = 1 2 (M + M ) and A = 1 2 (M \u2212 M ) are symmetric and antisymmetric. Applying the decomposition to the game Hessian yields the result.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 .2Figure 2. Infinitesimal alignment is positive (cyan) when small positive \u03bb either: (A) pulls u toward w, if w and u have angle < 90 \u2022 ; or (B) pushes u away from w if their angle is > 90 \u2022 .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 3 .3Figure 3. SGA allows faster and more robust convergence to stable fixed points. Note the scale of top-right panel differs from rest.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 4 .4Figure 4. Comparison of SGA with optimistic mirror descent. Sweep over learning rates in range [0.01, 1.75]; \u03bb = 1 throughout. (Left): iterations to convergence (up to 250). (Right): average absolute value of losses over iterations 240-250, cutoff at 5.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure6. Alignment and learning rates. The larger cos \u03b8, the larger the learning rate \u03b7 that can be applied to unit vector \u03be without w + \u03b7 \u2022 \u03be leaving the unit circle.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 7 .7Figure 7. Time to convergence of OMD and SGA on two 4-player games. Times are cutoff after 5000 iterations. Left panel: Weakly positive definite S with = 1 100 . Right panel: Symmetric component is identically zero.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 7 shows time to convergence (same criterion as section 4.2 for optimistic mirror descent and SGA. The games are constructed with four players, each of which controls one parameter. The losses are 1 (w, x, y, z) = 2 w 2 + wx + wy + wz 2 (w, x, y, z) = \u2212wx + 2 x 2 + xy + xz 3 (w, x, y, z) = \u2212wy \u2212 xy + 2 y 2 + yz 4 (w, x, y, z) = \u2212wz \u2212 xz \u2212 yz + 2 z 2 , where = 1 100 in the left panel and = 0 in the right panel. The antisymmetric component of the game Hessian is", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 8 .8Figure 8. Ground truth for GAN experiments. A mixture of 16 Gaussians.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 9 .9Figure 9. Top: Consensus optimization on GAN example. Bottom: Consensus optimization with alignment.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "The presymplectic form \u03c9 = d\u03be makes use of the musical isomorphism :T M \u2192 T * M . As in section F.1,if we naively define ( \u2202 \u2202x ) = dx and ( \u2202 \u2202y ) = dy then inconsistent because \u03c4 ( \u2202 1 \u2202x \u2022 dx) = m 2 s and \u03c4 ( \u2202 2 \u2202y \u2022 dy) = kg 2 \u2022m 2 s 3", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Top: Simultaneous gradient descent suffers from mode collapse and in later iterations (not shown) mode hopping. Middle: vanilla SGA converges smoothly to the ground truth (figure 8 in appendix). Bottom: SGA with alignment converges slightly faster.", "figure_data": "Iteration:2000400060008000GRADIENT DESCENTlearning rate 1e-4SGA without ALIGNMENTlearning rate 9e-5SGA with ALIGNMENTlearning rate 9e-5Figure 5."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "def jac vec(ys, xs, vs) : return fwd gradients(ys, xs, grad xs = vs, stop gradients = xs) def jac tran vec(ys, xs, vs) : dydxs = tf.gradients(ys, xs, grad ys = vs, stop gradients = xs) return [tf.zeros like(x) if dydx is None", "figure_data": "STEPS TO CONVERGEWEAK ATTRACTOROMD SGASTEPS TO CONVERGEOMD SGA ATTRACTOR NOLEARNING RATELEARNING RATE"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "To correct the type inconsistency, define the musical isomorphism as", "figure_data": "\u2202 \u2202q=\u00b5 2\u2022 dq and\u2202 \u2202p=1 2\u03ba\u2022 dpwith inverse(dq) =2 \u00b5\u2022\u2202 \u2202qand (dp) = 2\u03ba \u2022\u2202 \u2202p."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Figure10. Individual runs on zero-sum bimatrix game in section 4.2.", "figure_data": "Differentiable Game MechanicsSYMPLECTIC GRADIENT ADJUSTMENTlearning rate 1.0OPTIMISTIC MIRROR DESCENTlearning rate 1.0SYMPLECTIC GRADIENT ADJUSTMENTlearning rate 0.4OPTIMISTIC MIRROR DESCENTlearning rate 0.4SYMPLECTIC GRADIENT ADJUSTMENTlearning rate 1.16OPTIMISTIC MIRROR DESCENTlearning rate 1.16"}], "formulas": [{"formula_id": "formula_0", "formula_text": "{ i : R d \u2192 R} n i=1 . Parameters are w = (w 1 , . . . , w n ) \u2208 R d with w i \u2208 R di where n i=1 d i = d. . Player i controls w i .", "formula_coordinates": [3.0, 54.91, 266.23, 234.52, 42.66]}, {"formula_id": "formula_1", "formula_text": "\u03be(w) = (\u2207 w1 1 , . . . , \u2207 wn n ) \u2208 R d .", "formula_coordinates": [3.0, 97.81, 385.61, 149.13, 18.92]}, {"formula_id": "formula_2", "formula_text": "A = U DV. Changing to coordinates u = D 1 2 Ux and v = D 1 2 Vy gives 1/2 (u, v) = \u00b1u v. Introduce the Hamiltonian H = 1 2 u 2 2 + v 2 2 = 1 2 (x U DUx + y V DVy) .", "formula_coordinates": [3.0, 54.89, 646.51, 237.69, 66.31]}, {"formula_id": "formula_3", "formula_text": "\u03be(u, v) = \u2202H \u2202v , \u2212 \u2202H \u2202u .", "formula_coordinates": [3.0, 370.39, 100.93, 108.2, 23.79]}, {"formula_id": "formula_4", "formula_text": "(d \u00d7 d)-matrix of second- derivatives H(w) := \u2207 w \u2022 \u03be(w) = \u2202\u03be\u03b1(w) \u2202w \u03b2 d \u03b1,\u03b2=1", "formula_coordinates": [3.0, 307.44, 293.1, 235.76, 36.11]}, {"formula_id": "formula_5", "formula_text": "H(w) = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed \u2207 2 w1 1 \u2207 2 w1,w2 1 \u2022 \u2022 \u2022 \u2207 2 w1,wn 1 \u2207 2 w2,w1 2 \u2207 2 w2 2 \u2022 \u2022 \u2022 \u2207 2 w2,wn 2 . . . . . . \u2207 2 wn,w1 n \u2207 2 wn,w2 n \u2022 \u2022 \u2022 \u2207 2 wn n \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 ,", "formula_coordinates": [3.0, 309.43, 341.09, 230.13, 67.52]}, {"formula_id": "formula_6", "formula_text": "* \u2212i ) \u2265 i (w * i , w * \u2212i ) for w i \u2208 U i .", "formula_coordinates": [4.0, 59.48, 236.2, 229.96, 30.6]}, {"formula_id": "formula_7", "formula_text": "(d i \u00d7 d i )- submatrices S i := \u2207 2", "formula_coordinates": [4.0, 55.32, 286.87, 235.76, 29.26]}, {"formula_id": "formula_8", "formula_text": "\u03c6(w i , w \u2212i )\u2212\u03c6(w i , w \u2212i ) = \u03b1 i i (w i , w \u2212i )\u2212 i (w i , w \u2212i )", "formula_coordinates": [4.0, 55.32, 429.34, 241.55, 15.61]}, {"formula_id": "formula_9", "formula_text": "\u03b1 i \u2207 2 wiwj i = \u03b1 j \u2207 2 wiwj j = \u03b1 j \u2207 2 wj wi j \u2200i, j.", "formula_coordinates": [4.0, 63.3, 500.59, 218.14, 18.92]}, {"formula_id": "formula_10", "formula_text": "A = log pij 1\u2212pij n i,j=1", "formula_coordinates": [4.0, 412.26, 90.45, 91.94, 22.95]}, {"formula_id": "formula_11", "formula_text": "2A = 0 P \u2212 Q (Q \u2212 P) 0 2S = 0 P + Q (P + Q) 0", "formula_coordinates": [4.0, 307.44, 232.32, 238.55, 29.25]}, {"formula_id": "formula_12", "formula_text": "iff P + Q = 0 iff 1 + 2 = 0.", "formula_coordinates": [4.0, 311.59, 260.83, 229.96, 24.41]}, {"formula_id": "formula_13", "formula_text": "1 (x, y) = x(y \u2212 b) and 2 (x, y) = \u2212(x \u2212 a)y", "formula_coordinates": [4.0, 332.46, 368.66, 187.87, 18.78]}, {"formula_id": "formula_14", "formula_text": "1 (x, y) = x 2 + y 2 2 (x, y) = \u2212(x 2 + y 2 ).", "formula_coordinates": [4.0, 332.44, 440.51, 188.26, 18.92]}, {"formula_id": "formula_15", "formula_text": "Theorem 3. Let H(w) := 1 2 \u03be(w) 2 2 .", "formula_coordinates": [4.0, 307.11, 560.08, 152.1, 19.14]}, {"formula_id": "formula_16", "formula_text": "\u03be + \u03bb \u2022 H \u03be = \u03be + \u03bb \u2022 \u2207H.", "formula_coordinates": [5.0, 115.7, 461.1, 113.35, 17.29]}, {"formula_id": "formula_17", "formula_text": "1 (x, y) = 2 (x, y) = \u2212 \u03ba 2 (x 2 + y 2 ) with \u03ba 0. Then \u03be = \u2212\u03ba \u2022 x y and H = \u2212 \u03ba 0 0 \u03ba Note that \u03be 2 = \u03ba 2 (x 2 + y 2 ) and \u03be + \u03bb \u2022 H \u03be = \u03ba(\u03bb\u03ba \u2212 1) \u2022 x y .", "formula_coordinates": [5.0, 54.99, 540.08, 218.94, 92.83]}, {"formula_id": "formula_18", "formula_text": "losses L = { i } n i=1 , weights W = {w i } n i=1 \u03be \u2190 gradient( i , w i ) for ( i , w i ) \u2208 (L, W) A \u03be \u2190 get sym adj(L, W) // appendix C if align then \u2207H \u2190 gradient( 1 2 \u03be 2 , w) for w \u2208 W) \u03bb \u2190 sign 1 d \u03be, \u2207H A \u03be, \u2207H + // = 1 10 else \u03bb \u2190 1 end if Output: \u03be + \u03bb \u2022 A \u03be", "formula_coordinates": [5.0, 317.41, 81.37, 219.27, 132.63]}, {"formula_id": "formula_19", "formula_text": "\u03be \u03bb , \u03be = \u03b1 1 \u2022 \u03be 2 ;", "formula_coordinates": [5.0, 464.55, 362.56, 78.65, 18.78]}, {"formula_id": "formula_20", "formula_text": "\u03be \u03bb := \u03be + \u03bb \u2022 A \u03be.", "formula_coordinates": [5.0, 384.98, 686.26, 79.03, 17.29]}, {"formula_id": "formula_21", "formula_text": "\u03be \u03bb , \u2207H = \u03be \u03bb , H \u03be = \u03be \u03bb , A \u03be = \u03bb \u2022 \u03be AA \u03be = \u03bb \u2022 \u2207H 2 since H = A", "formula_coordinates": [6.0, 55.32, 153.44, 234.11, 29.25]}, {"formula_id": "formula_22", "formula_text": "f (x, y) = \u2212 2 x 2 \u2212 xy and g(x, y) = \u2212 2 y 2 + xy", "formula_coordinates": [6.0, 70.61, 291.83, 203.18, 18.92]}, {"formula_id": "formula_23", "formula_text": "\u03be = \u2022 \u2212x \u2212y + \u2212y x with A = 0 \u22121 1 0 and A \u03be = x y + \u2212y x Finally observe that \u03be + \u03bb \u2022 A \u03be = (\u03bb \u2212 ) \u2022 x y + (1 + \u03bb) \u2022 \u2212y x", "formula_coordinates": [6.0, 54.71, 331.88, 211.01, 105.69]}, {"formula_id": "formula_24", "formula_text": "\u03c3 max \u2265 \u2022 \u2022 \u2022 \u2265 \u03c3 min . The additive condition number 4 of S is \u03ba = \u03c3 max \u2212 \u03c3 min . If S 0 is positive semidefinite with additive condition number \u03ba then \u03bb \u2208 (0, 4 \u03ba ) implies \u03be \u03bb , \u2207H \u2265 0. If S is negative semidefinite, then \u03bb \u2208 (0, 4 \u03ba ) implies \u03be \u2212\u03bb , \u2207H \u2264 0.", "formula_coordinates": [6.0, 55.15, 535.17, 234.28, 102.66]}, {"formula_id": "formula_25", "formula_text": "if S 0 then \u03be, \u2207H \u2265 0; if S \u227a 0 then \u03be, \u2207H < 0.", "formula_coordinates": [6.0, 367.54, 368.76, 120.75, 33.13]}, {"formula_id": "formula_26", "formula_text": "align(\u03be \u03bb , w) := d d\u03bb cos 2 \u03b8 \u03bb |\u03bb=0 for \u03b8 \u03bb := \u03b8(\u03be \u03bb , w).", "formula_coordinates": [6.0, 310.21, 549.49, 228.57, 24.84]}, {"formula_id": "formula_27", "formula_text": "\u2207f (x) \u2212 \u2207f (y) \u2264 L \u2022 x \u2212 y . Let w t+1 = w t \u2212 \u03b7 \u2022 v where v = \u2207f (w t ) .", "formula_coordinates": [7.0, 55.32, 302.22, 234.25, 30.74]}, {"formula_id": "formula_28", "formula_text": "f (w t+1 ) \u2264 f (w t ) \u2212 cos 2 \u03b8 2L \u2022 \u2207f (w t ) 2 .", "formula_coordinates": [7.0, 86.59, 347.52, 171.58, 25.16]}, {"formula_id": "formula_29", "formula_text": "\u03be + |\u03bb| \u2022 sign \u03be, \u2207H \u2022 H \u03be,(2)", "formula_coordinates": [7.0, 107.09, 533.62, 182.34, 18.78]}, {"formula_id": "formula_30", "formula_text": "1 (x, y) = 1 2 x 2 + 10xy and 2 (x, y) = 1 2 y 2 \u2212 10xy", "formula_coordinates": [7.0, 316.58, 188.05, 219.63, 23.79]}, {"formula_id": "formula_31", "formula_text": "Then \u03be = (\u2207 x f, \u2212\u2207 y f ) and H = \u2207 2 x,x f \u2207 2 x,y f \u2212\u2207 2 y,x f \u2212\u2207 2 y,y f = \u2207 2 x,x f 0 0 \u2212\u2207 2 y,y f S + 0 \u2207 2 x,y f \u2212\u2207 2 y,x f 0 A", "formula_coordinates": [10.0, 55.01, 537.79, 211.57, 91.84]}, {"formula_id": "formula_32", "formula_text": "\u03be = x + 2y y + 2x and H = 1 2 2 1 .", "formula_coordinates": [10.0, 344.85, 141.17, 159.28, 21.93]}, {"formula_id": "formula_33", "formula_text": "\u03c6(x, y) = x 2 2 + 2xy + y 2 2", "formula_coordinates": [10.0, 370.84, 314.11, 105.62, 24.92]}, {"formula_id": "formula_34", "formula_text": "D \u03be H = \u03be, \u2207H = \u03be A \u03be where \u03be A \u03be = (\u03be A \u03be) = \u03be A\u03be = \u2212(\u03be A \u03be) since A = \u2212A by skew-symmetry. It follows that \u03be A \u03be = 0.", "formula_coordinates": [10.0, 306.27, 567.72, 235.62, 35.93]}, {"formula_id": "formula_35", "formula_text": "\u03be \u03bb \u2022 A \u03be, \u2207H = \u03be + \u03bb \u2022 A \u03be, S\u03be + A \u03be = \u03be S\u03be + \u03bb\u03be AA \u03be \u2265 0", "formula_coordinates": [11.0, 84.31, 220.05, 175.83, 32.24]}, {"formula_id": "formula_36", "formula_text": "\u03be + \u03bb \u2022 A \u03be, \u2207H = (\u03be + \u03bb \u2022 A \u03be) \u2022 (S + A )\u03be = \u03be S\u03be + \u03bb\u03be AS\u03be + \u03bb\u03be AA \u03be Let \u03b2 = A \u03be andS = S \u2212 \u03c3 min \u2022 I,", "formula_coordinates": [11.0, 55.32, 330.48, 226.19, 51.38]}, {"formula_id": "formula_37", "formula_text": "\u03be S\u03be + \u03bb\u03be AS\u03be + \u03bb \u2022 \u03b2 2 \u2265 \u03be S \u03be + \u03bb\u03be AS\u03be + \u03bb \u2022 \u03b2 2", "formula_coordinates": [11.0, 60.31, 391.53, 223.64, 19.59]}, {"formula_id": "formula_38", "formula_text": "|\u03be AS\u03be| \u2264 A \u03be \u2022 T T\u03be \u2264 \u221a \u03c3 max \u2022 A \u03be \u2022 T\u03be .", "formula_coordinates": [11.0, 60.31, 499.3, 224.14, 24.17]}, {"formula_id": "formula_39", "formula_text": "T\u03be 2 + \u03bb( A\u03be 2 \u2212 A\u03be, S\u03be ) \u2265 T\u03be 2 + \u03bb( A\u03be 2 \u2212 A\u03be S\u03be \u2265 T\u03be 2 + \u03bb A\u03be ( A\u03be \u2212 S\u03be ) \u2265 T\u03be 2 + \u03bb A\u03be ( A\u03be \u2212 \u221a \u03c3 max T\u03be ) Set \u03b1 = \u221a \u03bb and \u03b7 = \u221a \u03c3 max . We can continue the above computation = T\u03be 2 + \u03b1 2 A\u03be ( A\u03be \u2212 \u03b7 T\u03be ) = T\u03be 2 + \u03b1 2 A\u03be 2 \u2212 \u03b1 2 A\u03be \u03b7 T\u03be =( T\u03be \u2212 \u03b1 A\u03be ) 2 + 2\u03b1 A\u03be T\u03be \u2212 \u03b1 2 \u03b7 A\u03be T\u03be =( T\u03be \u2212 \u03b1 A\u03be ) 2 + A\u03be T\u03be (2\u03b1 \u2212 \u03b1 2 \u03b7)", "formula_coordinates": [11.0, 55.32, 548.89, 234.11, 164.33]}, {"formula_id": "formula_40", "formula_text": "cos 2 \u03b8 \u03bb = \u03be \u03bb , \u2207H \u03be \u03bb \u2022 \u2207H 2 = \u03be, \u2207H + 2\u03bb \u03be, \u2207H A \u03be, \u2207H + O(\u03bb 2 ) \u03be 2 + O(\u03bb 2 ) \u2022 \u2207H 2", "formula_coordinates": [11.0, 313.03, 251.41, 221.74, 62.48]}, {"formula_id": "formula_41", "formula_text": "sign d d\u03bb cos 2 \u03b8 \u03bb = sign \u03be, \u2207H A \u03be, \u2207H as required.", "formula_coordinates": [11.0, 307.44, 361.11, 209.51, 43.32]}, {"formula_id": "formula_42", "formula_text": "0 < w \u03be then w \u2212 \u03b7 \u2022 \u03be \u2264 1 for 0 \u2264 \u03b7 \u2264 2w \u03be. Proof. Check w \u2212 \u03b7 \u2022 \u03be 2 = 1 + \u03b7 2 \u2212 2\u03b7 \u2022 w \u03be \u2264 1 iff \u03b7 2 \u2264 2\u03b7 \u2022 w \u03be. The result follows.", "formula_coordinates": [11.0, 307.44, 632.43, 234.11, 55.56]}, {"formula_id": "formula_43", "formula_text": "R d \u2192 R be a convex Lipschitz smooth function satisfying \u2207f (y) \u2212 \u2207f (x) \u2264 L \u2022 y \u2212 x for all x, y \u2208 R d . Then |f (y) \u2212 f (x) \u2212 \u2207f (x), y \u2212 x | \u2264 L 2 \u2022 y \u2212 x 2 for all x, y \u2208 R d .", "formula_coordinates": [12.0, 55.32, 201.94, 234.11, 96.19]}, {"formula_id": "formula_44", "formula_text": "f (y) \u2264 f (x) + \u2207f (x), y \u2212 x + L 2 y \u2212 x 2 = f (x) \u2212 \u03b7 \u2022 \u2207f, \u03be + \u03b7 2 L 2 \u2022 \u03be 2 = f (x) \u2212 \u03b7 \u2022 \u2207f, \u03be + \u03b7 2 L 2 \u2022 \u2207f 2 = f (x) \u2212 \u03b7(\u03b1 \u2212 \u03b7 2 L) \u2022 \u2207f 2", "formula_coordinates": [12.0, 77.35, 361.64, 189.56, 94.4]}, {"formula_id": "formula_45", "formula_text": "min \u03b7 \u2206(\u03b7) = min \u03b7 \u2212\u03b7(\u03b1 \u2212 \u03b7 2 L)", "formula_coordinates": [12.0, 103.19, 479.68, 131.74, 23.79]}, {"formula_id": "formula_46", "formula_text": "= [tf.gradients( , x)[0] for ( , x) in zip(Ls, xs)] H xi = jac vec(xi, xs, xi) Ht xi = jac tran vec(xi, xs, xi) At xi = [ ht\u2212h 2 for (h, ht) in zip(H xi, Ht xi)] return At xi D.", "formula_coordinates": [12.0, 307.44, 270.49, 228.69, 99.94]}, {"formula_id": "formula_47", "formula_text": "A = \uf8eb \uf8ec \uf8ec \uf8ed 0 1 1 1 \u22121 0 1 1 \u22121 \u22121 0 1 \u22121 \u22121 \u22121 0 \uf8f6 \uf8f7 \uf8f7 \uf8f8 and the symmetric component is S = \u2022 \uf8eb \uf8ec \uf8ec \uf8ed 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 \uf8f6 \uf8f7 \uf8f7 \uf8f8 .", "formula_coordinates": [12.0, 307.44, 552.37, 173.3, 124.29]}, {"formula_id": "formula_48", "formula_text": "\u03be = \u2207\u03c6 gradient component + curl(B) rotational component curl(\u2022) := \u2207\u00d7(\u2022)", "formula_coordinates": [13.0, 55.32, 651.36, 236.75, 26.43]}, {"formula_id": "formula_49", "formula_text": "H : M \u2192 R satisfying \u03c9(\u03be, \u2022) = dH(\u2022) = \u2207H, \u2022 . (3", "formula_coordinates": [13.0, 307.44, 362.65, 230.24, 43.52]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [13.0, 537.68, 387.39, 3.87, 12.01]}, {"formula_id": "formula_51", "formula_text": "A \u03be = dH(\u2022) \u2207H ,", "formula_coordinates": [13.0, 395.52, 507.8, 64.77, 29.38]}, {"formula_id": "formula_52", "formula_text": "\u2126 k (M ) = d\u2126 k\u22121 (M ) \u2295 Harmonic k (M ) \u2295 \u03b4\u2126 k+1 (M ).", "formula_coordinates": [13.0, 312.42, 705.64, 224.14, 19.14]}, {"formula_id": "formula_53", "formula_text": "\u2126 1 (M ) 1-forms = d\u2126 0 (M ) gradients of functions \u2295Harm k (M )\u2295 \u03b4\u2126 2 (M ) antisymmetric component", "formula_coordinates": [14.0, 55.32, 362.02, 240.26, 26.64]}, {"formula_id": "formula_54", "formula_text": "H(p, q) = 1 2 \u03ba \u2022 q 2 + 1 \u00b5 \u2022 p 2", "formula_coordinates": [14.0, 360.99, 387.2, 119.17, 23.79]}, {"formula_id": "formula_55", "formula_text": "\u03c4 (q) = m \u03c4 (p) = kg\u2022m s \u03c4 (\u03ba) = kg s 2 \u03c4 (\u00b5) = kg", "formula_coordinates": [14.0, 366.97, 454.99, 113.86, 39.7]}, {"formula_id": "formula_56", "formula_text": "= \u2202H \u2202p = 1 \u00b5 \u2022 p and\u1e57 = \u2212 \u2202H \u2202q = \u2212\u03ba \u2022 q where \u03c4 (q) = m s \u03c4 (\u1e57) = kg\u2022m s 2 \u03c4 \u2202 \u2202q = 1 m \u03c4 \u2202 \u2202p = s kg\u2022m", "formula_coordinates": [14.0, 307.08, 570.35, 234.11, 63.79]}, {"formula_id": "formula_57", "formula_text": "\u03be =q \u2022 \u2202 \u2202q +\u1e57 \u2022 \u2202 \u2202p = 1 \u00b5 p \u2022 \u2202 \u2202q \u2212 \u03baq \u2022 \u2202 \u2202p", "formula_coordinates": [14.0, 338.75, 661.22, 170.29, 23.79]}, {"formula_id": "formula_58", "formula_text": "\u03b9 \u03be \u03c9 = \u03c9(\u03be, \u2022) = dH = \u2202H \u2202q \u2022 dq + \u2202H \u2202p \u2022 dp with units \u03c4 (dH) = \u03c4 (H) = kg\u2022m 2 s 2 .", "formula_coordinates": [15.0, 54.96, 152.6, 206.57, 50.03]}, {"formula_id": "formula_59", "formula_text": "1 (q, p) = 1 \u00b5 \u2022 qp and 2 (q, p) = \u2212\u03ba \u2022 qp (4) with units \u03c4 ( 1 ) = m 2 s and \u03c4 ( 2 ) = kg 2 \u2022m 2 s 3", "formula_coordinates": [15.0, 54.96, 268.49, 234.47, 50.03]}, {"formula_id": "formula_60", "formula_text": "\u03be = \u2202H \u2202p \u2202 \u2202q \u2212 \u2202H \u2202q \u2202 \u2202p = \u2202 1 \u2202q \u2202 \u2202q + \u2202 2 \u2202p \u2202 \u2202p .", "formula_coordinates": [15.0, 82.93, 362.01, 178.9, 23.79]}, {"formula_id": "formula_61", "formula_text": "\u03be = \u2202 1 \u2202q \u2022 dq + \u2202 2 \u2202p \u2022 dp results in a type violation because \u03c4 \u2202 1 \u2202q \u2022 dq = \u03c4 ( 1 ) = m 2 s whereas \u03c4 \u2202 2 \u2202p \u2022 dp = \u03c4 ( 2 ) = kg 2 \u2022 m 2 s 3", "formula_coordinates": [15.0, 54.96, 565.02, 186.93, 132.3]}, {"formula_id": "formula_62", "formula_text": "\u03be = \u00b5 2 \u2022 \u2202f \u2202q \u2022 dq + 1 2\u03ba \u2022 \u2202g \u2202p \u2022 dp = 1 2 (p \u2022 dq \u2212 q \u2022 dp) .", "formula_coordinates": [15.0, 312.64, 236.76, 223.72, 23.79]}, {"formula_id": "formula_63", "formula_text": "\u03c4 \u2202 1 \u2202q \u2022 \u00b5 \u2022 dq = m s \u2022 kg \u2022 m = kg\u2022m 2 s \u03c4 \u2202 2 \u2202p \u2022 1 \u03ba \u2022 dp = kg\u2022m s 2 \u2022 s 2 kg \u2022 kg\u2022m s = kg\u2022m 2 s", "formula_coordinates": [15.0, 333.52, 289.76, 180.25, 38.7]}, {"formula_id": "formula_64", "formula_text": "\u03c9 := d\u03be = \u2212 \u00b5 \u2022 \u2202 2 f \u2202q\u2202p \u2212 1 \u03ba \u2022 \u2202 2 g \u2202q\u2202p dq\u2227dp = \u2212dq\u2227dp", "formula_coordinates": [15.0, 307.44, 351.49, 239.2, 25.16]}, {"formula_id": "formula_65", "formula_text": "\u03be, \u03be = 1 2 p \u00b5 \u2022 \u2202 \u2202q \u2212 \u03baq \u2022 \u2202 \u2202p , p \u2022 dq \u2212 q \u2022 dp = 1 2 \u03ba \u2022 q 2 + 1 \u00b5 \u2022 p 2 = H(p, q)", "formula_coordinates": [15.0, 329.99, 417.44, 185.42, 51.7]}, {"formula_id": "formula_66", "formula_text": "A 12 = U A D A V A and C 12 = U C D C V C", "formula_coordinates": [16.0, 77.36, 574.88, 189.54, 13.9]}, {"formula_id": "formula_67", "formula_text": "\u2202 \u2202x = U A D \u22121 A U A \u2022 dx and \u2202 \u2202y = V C D \u22121 C V C \u2022 dy.", "formula_coordinates": [16.0, 122.89, 67.66, 360.76, 652.63]}, {"formula_id": "formula_68", "formula_text": "C V C = ( \u221a C 21 C 12 ) \u22121 .", "formula_coordinates": [16.0, 306.27, 122.15, 235.28, 22.68]}, {"formula_id": "formula_69", "formula_text": "(dx) = U A D A U A \u2022 \u2202 \u2202x and (dy) = V C D C V C \u2022 \u2202 \u2202y If \u03be = A 12 y + b 1 C 21 x + d 2 then it follows that \u03be = U A D \u22121 A U A b 1 + U A U A y V C D \u22121 C V C d 2 + V C V C x with associated closed two form \u03c9 \u03c4 = d\u03be = \u2212 (U A V A \u2212 U C V C ) dx \u2227 dy.", "formula_coordinates": [16.0, 307.08, 170.78, 233.11, 157.18]}], "doi": ""}