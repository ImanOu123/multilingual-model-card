{"title": "Collaborative Topic Modeling for Recommending Scientific Articles", "authors": "Chong Wang; David M Blei", "pub_date": "", "abstract": "Researchers have access to large online archives of scientific articles. As a consequence, finding relevant papers has become more difficult. Newly formed online communities of researchers sharing citations provides a new way to solve this problem. In this paper, we develop an algorithm to recommend scientific articles to users of an online community. Our approach combines the merits of traditional collaborative filtering and probabilistic topic modeling. It provides an interpretable latent structure for users and items, and can form recommendations about both existing and newly published articles. We study a large subset of data from CiteULike, a bibliography sharing service, and show that our algorithm provides a more effective recommender system than traditional collaborative filtering.", "sections": [{"heading": "INTRODUCTION", "text": "Modern researchers have access to large archives of scientific articles. These archives are growing as new articles are placed online and old articles are scanned and indexed. While this growth has allowed researchers to quickly access more scientific information, it has also made it more difficult for them to find articles relevant to their interests. Modern researchers need new tools for managing what is available to them.\nHistorically, one way that researchers find articles is by following citations in other articles that they are interested in. This is an effective practice-and one that we should continue-but it limits researchers to specific citation communities, and it is biased towards heavily cited papers. A statistician may miss a relevant paper in economics or biology because the two literatures rarely cite each other; and she may miss a relevant paper in statistics because it was also missed by the authors of the papers that she has read. One of the opportunities of online archives is to inform researchers about literature that they might not be aware of.\nA complementary method of finding articles is keyword search. This is a powerful approach, but it is also limited. Forming queries for finding new scientific articles can be difficult as a researcher may not know what to look for; search is mainly based on content, while good articles are also those that many others found valuable; and search is only good for directed exploration, while many researchers would also like a \"feed\" of new and interesting articles.\nRecently, websites like CiteULike 1 and Mendeley 2 allow researchers to create their own reference libraries for the articles they are interested in and share them with other researchers. This has opened the door to using recommendation methods [13] as a third way to help researchers find interesting articles. In this paper, we develop an algorithm for recommending scientific articles to users of online archives. Each user has a library of articles that he or she is interested in, and our goal is to match each user to articles of interest that are not in his or her library.\nWe have several criteria for an algorithm to recommend scientific articles. First, recommending older articles is important. Users of scientific archives are interested in older articles for learning about new fields and understanding the foundations of their fields. When recommending old articles, the opinions of other users plays a role. A foundational article will be in many users' libraries; a less important article will be in few.\nSecond, recommending new articles is also important. For example, when a conference publishes its proceedings, users would like see the recommendations from these new articles to keep up with the state-of-the-art in their discipline. Since the articles are new, there is little information about which or how many other users placed the articles in their libraries, and thus traditional collaborative filtering methods has difficulties making recommendations. With new articles, a recommendation system must use their content.\nFinally, exploratory variables can be valuable in online scientific archives and communities. For example, we can summarize and describe each user's preference profile based on the content of the articles that he or she likes. This lets us connect similar users to enhance the community, and indicate why we are connecting them. Further, we can describe articles in terms of what kinds of users like them. For example, we might detect that a machine learning article is of strong interest to computer vision researchers. If enough researchers use such services, these variables might also give an alternative measure of the impact of an article within a field.\nWith these criteria in mind, we develop a machine learning algorithm for recommending scientific articles to users in an online scientific community. Our algorithm uses two types of data-the other users' libraries and the content of the articles-to form its recommendations. For each user, our algorithm can finds both older papers that are important to other similar users and newly written papers whose content reflects the user's specific interests. Finally, our algorithm gives interpretable representations of users and articles.\nOur approach combines ideas from collaborative filtering based on latent factor models [17,18,13,1,22] and content analysis based on probabilistic topic modeling [7,8,20,2]. Like latent factor models, our algorithm uses information from other users' libraries. For a particular user, it can recommend articles from other users who liked similar articles. Latent factor models work well for recommending known articles, but cannot generalize to previously unseen articles.\nTo generalize to unseen articles, our algorithm uses topic modeling. Topic modeling provides a representation of the articles in terms of latent themes discovered from the collection. When used in our recommender system, this component can recommend articles that have similar content to other articles that a user likes. The topic representation of articles allows the algorithm to make meaningful recommendations about articles before anyone has rated them.\nWe combine these approaches in a probabilistic model, where making a recommendation for a particular user is akin to computing a conditional expectation of hidden variables. We will show how the algorithm for computing these expectations naturally balances the influence of the content of the articles and the libraries of the other users. An article that has not been seen by many will be recommended based more on its content; an article that has been widely seen will be recommended based more on the other users.\nWe studied our algorithm with data from CiteULike: 5, 551 users, 16, 980 articles, and 204, 986 bibliography entries. We will demonstrate that combining content-based and collaborative-based methods works well for recommending scientific articles. Our method provides better performance than matrix factorization methods alone, indicating that content can improve recommendation systems. Further, while traditional collaborative filtering cannot suggest articles before anyone has rated them, our method can use the content of new articles to make predictions about who will like them.", "publication_ref": ["b14", "b18", "b19", "b14", "b24", "b8", "b9", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND", "text": "We first give some background. We describe two types of recommendation problems we address; we describe the classical matrix factorization solution to recommendation; and we review latent Dirichlet allocation (LDA) for topic modeling of text corpora.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Recommendation Tasks", "text": "The two elements in a recommender system are users and items. In our problem, items are scientific articles and users are researchers. We will assume I users and J items. The rating variable rij \u2208 {0, 1} denotes whether user i includes article j in her library [12]. If it is in the library, this means that user i is interested in article j. (This differs from some other systems where users explicitly rate items on a scale.) Note that rij = 0 can be interpreted into two ways. One way is that user i is not interested in article j; the other is that user i does not know about article j.\nFor each user, our task is to recommend articles that are not in her library but are potentially interesting. There are two types of recommendation: in-matrix prediction and out-of-matrix prediction. Figure 1 illustrates the idea.\nIn-matrix prediction.\nFigure 1 (a) illustrates in-matrix prediction. This refers to the problem of making recommendations about those articles that have been rated by at least one user in the system. This is the task that traditional collaborative filtering can address.\nOut-of-matrix prediction.\nFigure 1 (b) illustrates out-of-matrix prediction, where articles 4 and 5 have never been rated. (This is sometimes called \"cold start recommendation.\") Traditional collaborative filtering algorithms cannot make predictions about these articles because those algorithms only use information about other users' ratings. This task is important for online scientific archives, however, because users want to see new articles in their fields. A recommender system that cannot handle out-of-matrix prediction cannot recommend newly published papers to its users.", "publication_ref": ["b13"], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "Recommendation by Matrix Factorization", "text": "The traditional approach to recommendation is collaborative filtering (CF), where items are recommended to a user based on other users with similar patterns of selected items. (Note that collaborative filtering does not use the content of the items.) Most successful recommendation methods are latent factor models [17,18,13,1,22], which provide better recommendation results than the neighborhood methods [11,13]. In this paper, we focus on latent factor models.\nAmong latent factor methods, matrix factorization performs well [13]. In matrix factorization, we represent users and items in a shared latent low-dimensional space of dimension K-user i is represented by a latent vector ui \u2208 R K and item j by a latent vector vj \u2208 R K . We form the prediction of whether user i will like item j with the inner product between their latent representations,\nrij = u T i vj.(1)\nBiases for different users and items can also be incorporated [13].\nTo use matrix factorization, we must compute the latent representations of the users and items given an observed matrix of ratings. The common approach is to minimize the regularized squared error loss with respect to U = (ui) I i=1 and V = (vj) J j=1 ,\nminU,V i,j (rij \u2212 u T i vj) 2 + \u03bbu||ui|| 2 + \u03bbv||vj|| 2 , (2)\nwhere \u03bbu and \u03bbv are regularization parameters. This matrix factorization for collaborative filtering can be generalized as a probabilistic model [18]. In probabilistic matrix factorization (PMF), we assume the following generative process, 1. For each user i, draw user latent vector ui \u223c N (0, \u03bb \u22121 u IK ). 2. For each item j, draw item latent vector vj \u223c N (0, \u03bb \u22121 v IK ).\n3. For each user-item pair (i, j), draw the response\nrij \u223c N (u T i vj, c \u22121 ij ),(3)\nwhere cij is the precision parameter for rij.\n(Note that IK is a K-dimensional identity matrix.) This is the interpretation of matrix factorization that we will build on. When cij = 1, for \u2200i, j, the maximum a posteriori estimation (MAP) of PMF corresponds to the solution in Eq. 2. Here, the precision parameter cij serves as a confidence parameter for rating rij. If cij is large, we trust rij more. As we mentioned above, rij = 0 can be interpreted into two ways-the user i is either not interested in item j or is unaware of it. This is thus a \"one-class collaborative filtering problem,\" similar to the TV program and news article recommendation problems studied in [12] and [16]. In that work, the authors introduce different confidence parameters cij for different ratings rij. We will use the same strategy to set cij a higher value when rij = 1 than when rij = 0,\ncij = a, if rij = 1, b, if rij = 0,(4)\nwhere a and b are tuning parameters satisfying a > b > 0.\nWe fit a CF model by finding a locally optimal solution of the user variables U and item variables V , usually with an iterative algorithm [12]. We then use Eq. 1 to predict the ratings of the articles outside of each user's library.\nThere are two main disadvantages to matrix factorization for recommendation. First, the learnt latent space is not easy to interpret; second, as mentioned, matrix factorization only uses information from other users-it cannot generalize to completely unrated items.", "publication_ref": ["b18", "b19", "b14", "b24", "b12", "b14", "b14", "b14", "b19", "b13", "b17", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic Topic Models", "text": "Topic modeling algorithms [5] are used to discover a set of \"topics\" from a large collection of documents, where a topic is a distribution over terms that is biased around those associated under a single theme. Topic models provide an interpretable low-dimensional representation of the documents [8]. They have been used for tasks like corpus exploration, document classification, and information retrieval. Here we will exploit the discovered topic structure for recommendation.\nThe simplest topic model is latent Dirichlet allocation (LDA) [7]. Assume there are K topics \u03b2 = \u03b21:K , each of which is a distribution over a fixed vocabulary. The generative process of LDA is as follows. For each article wj in the corpus, This process reveals how the words of each document are assumed to come from a mixture of topics: the topic proportions are documentspecific, but the set of topics is shared by the corpus.\nGiven a collection, the posterior distribution (or maximum likelihood estimate) of the topics reveals the K topics that likely generated its documents. Unlike a clustering model, where each document is assigned to one cluster, LDA allows documents to exhibit multiple topics. For example, LDA can capture that one article might be about biology and statistics, while another might be about biology and physics. Since LDA is unsupervised, the themes of \"physics\" \"biology\" and \"statistics\" can be discovered from the corpus; the mixed-membership assumptions lead to sharper estimates of word co-occurrence patterns. Given a corpus of documents, we can use variational EM to learn the topics and decompose the documents according to them [7]. Further, given a new document, we can use variational inference to situate its content in terms of the topics. Our goal is to use topic modeling to give a content-based representation of items in a recommender system.", "publication_ref": ["b6", "b9", "b8", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "COLLABORATIVE TOPIC REGRESSION", "text": "In this section, we describe the collaborative topic regression (CTR) model. CTR combines traditional traditional collaborative filtering with topic modeling.\nA first approach to combining collaborative filtering and topic modeling is to fit a model that uses the latent topic space to explain both the observed ratings and the observed words. For example, we can use the topic proportion \u03b8j in place of the latent item latent vector vj in Eq. 3,\nrij \u223c N (u T i \u03b8j, c \u22121 ij ).(5)\n(We note that [19] proposed a similar approach to Eq. 5, but based on correlated topic models [4]. It showed modest improvement over matrix factorization on several movie recommendation datasets.) This model suffers from the limitation that it cannot distinguish topics for explaining recommendations from topics important for explaining content. Consider two articles A and B that are both about machine learning applied to social networks. They are similar and, therefore, have similar topic proportions \u03b8A and \u03b8B. Now further suppose that these articles are interesting to different kinds of users: Article A might give an interesting machine learning algorithm that is applied to social network applications; article B uses standard machine learning techniques, but gives an important piece of data analysis on social network data.\nUsers that work in machine learning will prefer article A and rarely consider article B; users that work in social networks will prefer the opposite. However, using the topic proportions as in Eq. 5 will be likely to make similar recommendations for both articles to both types of users. Collaborative topic regression can detect this difference-that one type of user likes the first article and another type likes the second.\nAs above, collaborative topic regression (CTR) represents users with topic interests and assumes that documents are generated by a topic model. CTR additionally includes a latent variable j that offsets the topic proportions \u03b8j when modeling the user ratings. As more users rate articles, we have a better idea of what this offset is. This offset variable can explain, for example, that article A is more interesting to machine learning researchers than it is to social network analysis researchers. How much of the prediction relies on content and how much it relies on other users depends on how many users have rated the article.\nFigure 2 shows the graphical model. Again, assume there are K topics \u03b2 = \u03b21:K . The generative process of CTR is as follows, 1. For each user i, draw user latent vector ui \u223c N (0, \u03bb \u22121 u IK ).\n2. For each item j, (a) Draw topic proportions \u03b8j \u223c Dirichlet(\u03b1).\n(b) Draw item latent offset j \u223c N (0, \u03bb \u22121 v IK ) and set the item latent vector as vj = j + \u03b8j. (c) For each word wjn, i. Draw topic assignment zjn \u223c Mult(\u03b8). ii. Draw word wjn \u223c Mult(\u03b2z jn ).\n3. For each user-item pair (i, j), draw the rating\nrij \u223c N (u T i vj, c \u22121 ij ).(6)\nThe key property in CTR lies in how the item latent vector vj is generated. Note that vj = j + \u03b8j, where j \u223c N (0,\n\u03bb \u22121 v I k ), is equivalent to vj \u223c N (\u03b8j, \u03bb \u22121 v IK ),\nwhere we assume the item latent vector vj is close to topic proportions \u03b8j, but could diverge from it if it has to. Note that the expectation of rij is a linear function of \u03b8j,\nE[rij|ui, \u03b8j, j ] = u T i (\u03b8j + j )\n. This is why we call the model collaborative topic regression.", "publication_ref": ["b21", "b5"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Learning the parameters.", "text": "Given topic parameter \u03b2, computing the full posterior of ui, vj and \u03b8j is intractable. We develop an EMstyle algorithm to learn the maximum a posteriori (MAP) estimates.\nMaximization of the posterior is equivalent to maximizing the complete log likelihood of U , V , \u03b81:J , and R given \u03bbu, \u03bbv and \u03b2,\nL = \u2212 \u03bbu 2 i u T i ui \u2212 \u03bbv 2 j (vj \u2212 \u03b8j) T (vj \u2212 \u03b8j)(7)\n+ j n log k \u03b8 jk \u03b2 k,w jn \u2212 i,j c ij 2 (rij \u2212 u T i vj) 2 .\nWe have omitted a constant and set \u03b1 = 1. We optimize this function by coordinate ascent, iteratively optimizing the collaborative filtering variables {ui, vj} and the topic proportions \u03b8j.\nFor ui and vj, maximization follows in a similar fashion as for basic matrix factorization [12]. Given the current estimate of \u03b8j, taking the gradient of L with respect to ui and vj and setting it to zero leads to (recall the matrix definition U = (ui) I i=1 and V = (vj) J j=1 )\nui \u2190 (V CiV T + \u03bbuIK ) \u22121 V CiRi (8) vj \u2190 (U CjU T + \u03bbvIK ) \u22121 (U CjRj + \u03bbv\u03b8j). (9\n)\nwhere Ci is a diagonal matrix with cij, j = 1 \u2022 \u2022 \u2022 , J as its diagonal elements and Ri = (rij) J j=1 for user i. For item j, Cj and Rj are similarly defined. Eq. 9 shows how topic proportions \u03b8j affects item latent vector vj, where \u03bbv balances this effect. Finally, we note that the complexity is linear in the number of articles in the users' libraries. This follows from the special structure of cij defined in Eq. 4. (See [12] for details.)\nGiven U and V , we now describe how to learn the topic proportions \u03b8j. 3 We first define q(zjn = k) = \u03c6 jnk . Then we separate the items that contain \u03b8j and apply Jensen's inequality,\nL(\u03b8j) \u2265 \u2212 \u03bbv 2 (vj \u2212 \u03b8j) T (vj \u2212 \u03b8j) + n k \u03c6 jnk log \u03b8 jk \u03b2 k,w jn \u2212 log \u03c6 jnk = L(\u03b8j, \u03c6j). (10\n)\nLet \u03c6j = (\u03c6 jnk ) N \u00d7K n=1,k=1 . The optimal \u03c6 jnk satisfies\n\u03c6 jnk \u221d \u03b8 jk \u03b2 k,w jn . (11\n)\nThe L(\u03b8j, \u03c6j) gives the tight lower bound of L(\u03b8j). We cannot optimize \u03b8j analytically, so we use projection gradient [3]. We use 3 On our data, we found that simply fixing \u03b8j as the estimate from vanilla LDA gives comparable performance and saves computation.\ncoordinate ascent to optimize the remaining parameters, U , V , \u03b81:J and \u03c61:J . After we estimate U , V and \u03c6, we can optimize \u03b2,\n\u03b2 kw \u221d j n \u03c6 jnk 1[wjn = w]. (12\n)\nNote this is the same M-step update for topics as in LDA [7].", "publication_ref": ["b13", "b13", "b4", "b4", "b4", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Prediction.", "text": "After all the (locally) optimal parameters U * , V * , \u03b8 * 1:J and \u03b2 * are learned, the CTR model can be used for both inmatrix and out-of-matrix prediction. Let D be the observed data, in general each prediction is estimated as\nE[rij|D] \u2248 E[ui | D] T (E[\u03b8j | D] + E[ j | D]) .(13)\nFor in-matrix prediction, we use the point estimate of ui, \u03b8j and j to approximate their expectations,\nr * ij \u2248 (u * i ) T (\u03b8 * j + * j ) = (u * i ) T v * j ,(14)\nwhere recall that vj = \u03b8j + j .\nIn out-of-matrix prediction the article is new, and no other ratings are available. Thus, E[ j ] = 0 and we predict with\nr * ij \u2248 (u * i ) T \u03b8 * j .(15)\nTo obtain the topic proportions \u03b8 * j for a new article, we optimize Eq. 10. The first term is dropped because vj = \u03b8j.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related work.", "text": "Several other work uses content for recommendation [15,14,1,2]. Among these, the closest work to ours is fLDA by [2]. FLDA generalizes the supervised topic model (sLDA) [6], using the empirical topic proportionszj = (1/N ) N n=1 zjn as well as several other covariates to form predictions. In our settings, where we do not have additional covariates, their approach is roughly akin to setting vj = \u03b8j. We show in Section 4 that a similar setting does not perform as well as the CTR model because it largely ignores the other users ratings.\nOther recent work considers the related problem of using topic modeling to predict legislative votes [21,10]. Neither of these methods introduces offset terms to account for votes (i.e., ratings). Legislative votes might be an interesting application for the CTR model.", "publication_ref": ["b16", "b15", "b7", "b23", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "EMPIRICAL STUDY", "text": "We demonstrate our model by analyzing a real-world community of researchers and their citation files. 4 Dataset.\nOur data are users and their libraries of articles obtained from CiteULike. 5 At CiteUlike, registered users create personal reference libraries; each article usually has a title and abstract. (The other information about the articles, such as the authors, publications and keywords, is not used in this paper.)\nWe merged duplicated articles, removed empty articles, and removed users with fewer than 10 articles to obtain a data set of 5, 551 users and 16, 980 articles with 204, 986 observed user-item pairs. (This matrix has a sparsity of 99.8%; it is highly sparse.) On average, each user has 37 articles in the library, ranging from 10 to 403. 93% of the users have fewer than 100 articles.\nFor each article, we concatenate its title and abstract. We remove stop words and use tf-idf to choose the top 8, 000 distinct words as the vocabulary [5]. This yielded a corpus of 1.6M words. These articles were added to CiteULike between 2004 and 2010. On average, each article appears in 12 users' libraries, ranging from 1 to 321. 97% of the articles appear in fewer than 40 libraries.", "publication_ref": ["b5", "b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Evaluation.", "text": "In our experiments, we will analyze a set of articles and user libraries. We will evaluate recommendation algorithms on sets of held-out articles and ratings. We will (hypothetically) present each user with M articles sorted by their predicted rating and evaluate based on which of these articles were actually in each user's library.\nTwo possible metrics are precision and recall. However, as we discussed earlier, zero ratings are uncertain. They may indicate that a user does not like an article or does not know about it. This makes it difficult to accurately compute precision. Rather, since ratings of rij = 1 are known to be true positives, we focus on recall. Recall only considers the positively rated articles within the top M -a high recall with lower M will be a better system. For each user, the definition of recall@M is recall@M = number of articles the user likes in top M total number of article the user likes .\nThe recall for the entire system can be summarized using the average recall from all users.\nThe recall above we defined is user-oriented. We also consider article-oriented recall for testing the system's predictive performance on a particular article. For article j, we consider the population of users that like the article and the proportion of those for whom that article appears in their top M recommended articles. This evaluates the predictive power of the system on a chosen set of articles.\nAs we discussed in section 2, we consider two recommendation tasks users, in-matrix prediction and out-of-matrix prediction.\nIn-matrix prediction.\nIn-matrix prediction considers the case where each user has a set of articles that she has not seen, but that at least one other user has seen. We ask the question, how good is each system at rating that set of articles for each user?\nAs discussed in section 2.1, this task is similar to traditional collaborative filtering. We split the data into a training set and test set, ensuring that all articles in the test set have appeared in the training set. Content information is not required to perform recommendations-though we will see that it helps-and thus matrix factorization can be used.\nWe use 5-fold cross-validation. For every article that appears at least 5 times in the users' libraries, we evenly split their user-item pairs (both 1's and 0's) into 5 folds. We iteratively consider each fold to be a test set and the others to be the training set. For those articles that appear fewer than 5 times, we always put them into the training set. This guarantees that all articles in the test set must appear in the training set. (9% of the articles are always in the training set, since they appear fewer than 5 times.)\nFor each fold, we fit a model to the training set and test on the within-fold articles for each user. (Note: each user has a different set of within-fold articles.) We form predictive ratings for the test set, and generate a list of the top M recommended articles.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Out-of-matrix prediction.", "text": "Out-of-matrix prediction considers the case where a new set of articles is published and no one has seen them. Again we ask, how good is each system at rating that set of articles for each user?\nWe again use 5-fold cross validation. First, we evenly group all articles into 5 folds. For each fold, we fit the model to the submatrix formed by the out-of-fold articles and then test the recommendations for each user on the within-fold articles. Note that in this case, each user has the same set of within-fold articles and we are guaranteed that none of these articles is in the training set for any user. Again, we form predictive ratings for the test set, and generate a list of the top M recommended articles. These two experimental set-ups-in-matrix and out-of-matrix predictions-are designed to be comparable-the top M articles are computed from the same size of candidate populations.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental settings.", "text": "For matrix factorization for collaborative filtering (CF), we used grid search to find that K = 200, \u03bbu = \u03bbv = 0.01, a = 1, b = 0.01 gives good performance on held out recommendations. We use CF to denote this method.\nFor collaborative topic regression (CTR), we set the parameters similarly as for CF, K = 200, \u03bbu = 0.01, a = 1 and b = 0.01. In addition, the precision parameter \u03bbv balances how the article's latent vector vj diverges from the topic proportions \u03b8j. We vary \u03bbv \u2208 {10, 100, 1000, 10000}, where a larger \u03bbv increases the penalty of vj diverging from \u03b8j.\nWe also compare to the model that only uses LDA-like features, as we discussed in the beginning of section 3. This is equivalent to fixing the per-item latent vector vj = \u03b8j in the CTR model. This is a nearly content-only model-while the per-user vectors are fit to the ratings data, the document vectors \u03b8j are only based on the words of the document. We use LDA to denote this method. (Note that we use the resulting topics and proportions of LDA to initialize the CTR model.)\nThe baseline is the random model, where a user see M random recommended articles. We note that the expected recall for the random method from a pool of Mtot articles is irrelevant to library size. It is always M/Mtot.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparisons.", "text": "Figure 3 shows the overall performance for in-matrix and out-of-matrix prediction, when we vary the number of returned articles M = 20, 40, \u2022 \u2022 \u2022 , 200. For CTR, we pick \u03bbv = 100; Figure 4 shows the performs when we change \u03bbv for the CTR model compared with CF and LDA when we fix M = 100.\nFigure 3 and 4 shows that matrix factorization works well for in-matrix prediction, but adding content with CTR improves performance. The improvement is greater when the number of returned documents M is larger. The reason is as follows. Popular articles are more likely to be recommended by both methods. However, when M becomes large, few user ratings are available to ensure that CF gives good recommendations; the contribution of the content becomes more important.\nCompared to both CF and CTR, LDA suffers for in-matrix pre- The expected recall of random recommendation is about 3%. CF can not do out-of-matrix prediction.\ndiction. It does not account enough for the users' information in forming its predicted ratings. The gap between CF and LDA is interesting-other users provide a better assessment of preferences than content alone. Out-of-matrix prediction is a harder problem, as shown by the relatively lower recall. In this task, CTR performs slightly better than LDA. Matrix factorization cannot perform out-of-matrix prediction. (Note also that LDA performs almost the same on both in-matrix and out-of-matrix predictions. This is expected because, in both settings, it makes its recommendations almost entirely based on content.) Overall, CTR is the best model.\nIn Figure 4 we study the effect of the precision parameter \u03bbv. When \u03bbv is small in CTR, the per-item latent vector vj can diverge significantly from the topic proportions \u03b8j. Here, CTR behaves more like matrix factorization where no content is considered. When \u03bbv increases, CTR is penalized for vj diverging from the topic proportions; this brings the content into the recommendations. When \u03bbv is too large, vj is nearly the same as \u03b8j and, consequently, CTR behaves more like LDA.\nWe next study the relationship, across models, between recommendation performance and properties of the users and articles. For this study we set the number of recommended articles M = 100 and the precision \u03bbv = 100. Figure 5 shows how the performance varies as a function of the number of articles in a user's library; Figure 6 shows how the performance varies as a function of the number of users that like an article.\nAs we see from Figure 5, for both in-matrix and out-of-matrix prediction, users with more articles tend to have less variance in their predictions. Users with few articles tend to have a diversity in the predictions, whose recall values vary around the extreme values of 0 and 1. In addition, we see that recall for users with more articles have a decreasing trend. This is reasonable because when a user has more articles then there will be more infrequent ones. As we see next, these articles are harder to predict.\nFrom Figure 6, on in-matrix prediction for CF, CTR and LDA articles with high frequencies tend to have high recalls for in-matrix prediction and their predictions have less variance. This is because these articles have more collaborative information than infrequent ones, and, furthermore, CF and CTR make use of this information. var theta theta correction topic 1: estimate, estimates, likelihood, maximum, estimated, missing, distances topic 10: parameters, Bayesian, inference, optimal, procedure, prior, assumptions Here, \"theta\" denotes \u03b8j and \"theta correction\" denotes the offset j . The 10 topics are obtained by joining the top 5 topics ranked by \u03b8 jk and another top 5 topics ranked by\n| jk |, k = 1, \u2022 \u2022 \u2022 , K.\nUnder CTR, an article of wide interest is likely to exhibit more topics than its text exhibits. For example, this article brings in several other topics, including one on \"Bayesian statistics\" (topic 10). Note that the EM article is mainly about parameter estimation (topic 1), though is frequently referenced by Bayesian statisticians (and scholars in other fields as well).\nFor LDA, this trend is much smaller. In out-of-matrix predictions, since predictions are made on new articles, these frequencies do not have an effect on training the model. We now turn to an exploratory analysis of our results on the CTR model. (In the following, the precision \u03bbv = 100.) Examining User Profiles.\nOne advantage of the CTR model is that it can explain the user latent space using the topics learned from the data. For one user, we can find the top matched topics by ranking the entries of her latent vector ui. Table 1 shows two example users and their top 3 matched topics along with their top 10 preferred articles as predicted by the CTR model.\nThe learned topics serve as a summary of what users might be interested in. For user I, we see that he or she might be a researcher working on machine learning and its applications to texts and images. Although the predicted top 10 articles don't contain a vision article, we see such articles when more articles are retrieved. For user II, he or she might be a researcher who is interested in user interfaces and collaborative filtering.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Examining the Latent Space of Articles.", "text": "We can also examine the latent space of articles beyond their topic proportions. Here we inspect the articles with the largest overall offsets j . Table 2 shows the top 10 articles with the largest offsets measured by the distance between vj and \u03b8j, T j j = (vj \u2212 \u03b8j) T (vj \u2212 \u03b8j). The last two columns show the average of predicted ratings over those users who actually have that article (avg-like) and those users who do not have that article (avg-dislike).\nThese articles are popular in this data. Among the top 50 articles by this measure, 94% of them have at least 50 appearances. Articles with large offsets enjoy readership from different areas, and their item latent vectors have to diverge from the topic proportions to account for this. For example, Figure 7 illustrates the article that is the main citation for the expectation-maximization algorithm, \"Maximum likelihood from incomplete data via the EM algorithm\" [9]. Its top topic (found by k = arg max k \u03b8 jk ), is shown as topic 1. It is about \"parameter estimation,\" which is the main focus of this article. We can also examine the topics that are offset the most, k = arg max k | jk | = arg max k |v jk \u2212 \u03b8 jk |. The maximum offset is for topic 10, a topic about \"Bayesian statistics.\" Topic 10 has a low value in \u03b8j-the EM paper is not a Bayesian paper-but readers of Bayesian statistics typically have this paper in their library.\nExamining the offset can yield the opposite kind of article. For example, consider the article \"Phase-of-firing coding of natural visual stimuli in primary visual cortex\" in Figure 8. Its most probable topic is topic 1 (about \"Computational Neuroscience\"). Taking into account the offset, the most probable topic does not change and nor are new topics brought in. This indicates that the offset j only adjusts vj so that the objective function is well minimized. This article is not as interesting to users outside of Neuroscience.", "publication_ref": ["b10"], "figure_ref": ["fig_6", "fig_7"], "table_ref": []}, {"heading": "CONCLUSIONS AND FUTURE WORK", "text": "We proposed an algorithm for recommending scientific articles to users based on both content and other users' ratings. Our study showed that this approach works well relative to traditional matrix factorization methods and makes good predictions on completely unrated articles.\nFurther, our algorithm provides interpretable user profiles. Such profiles could be useful in real-world recommender systems. For example, if a particular user recognizes her profile as representing different topics, she can choose to \"hide\" some topics when seeking recommendations about a subject.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements.", "text": "The authors thank anonymous reviewers for their insightful comments. Chong Wang is supported by Google PhD fellowship. David M. Blei is supported by ONR 175-6343, NSF CAREER 0745520, AFOSR 09NL202, the Alfred P. Sloan foundation, and a grant from Google.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Regression-based latent factor models", "journal": "ACM", "year": "2009", "authors": "D Agarwal; B.-C Chen"}, {"ref_id": "b1", "title": "ACM. , measure, measures, images, motion, matching, transformation, entropy, overlap, computed, match 2. learning, machine, training, vector, learn, machines, kernel, learned, classifiers, classifier", "journal": "", "year": "2010", "authors": "D Agarwal; B.-C Chen"}, {"ref_id": "b2", "title": "interface, interfaces, needs, explicit, implicit, usability, preferences, interests, personalized 2. based, world, real, characteristics, actual, exploring, exploration, quite, navigation, possibilities, dealing 3. evaluation, collaborative, products, filtering, product, reviews, items, recommendations, recommender top 10 articles 1. Combining collaborative filtering with personal agents for better recommendations \u00d7", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "We show their position in latent space via their highest weighted topics. We list the top 10 preferred articles as predicted by CTR. The last column shows whether", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "Nonlinear Programming", "journal": "Athena Scientific", "year": "1999", "authors": "D Bertsekas"}, {"ref_id": "b5", "title": "A correlated topic model of Science", "journal": "Annals of Applied Statistics", "year": "2007", "authors": "D Blei; J Lafferty"}, {"ref_id": "b6", "title": "Topic models", "journal": "Taylor and Francis", "year": "2009", "authors": "D Blei; J Lafferty"}, {"ref_id": "b7", "title": "Supervised topic models", "journal": "", "year": "2007", "authors": "D Blei; J Mcauliffe"}, {"ref_id": "b8", "title": "Latent Dirichlet allocation", "journal": "Journal of Machine Learning Research", "year": "2003-01", "authors": "D Blei; A Ng; M Jordan"}, {"ref_id": "b9", "title": "Reading tea leaves: How humans interpret topic models", "journal": "", "year": "2009", "authors": "J Chang; J Boyd-Graber; S Gerrish; C Wang; D Blei"}, {"ref_id": "b10", "title": "Maximum likelihood from incomplete data via the EM algorithm", "journal": "Journal of the Royal Statistical Society, Series B", "year": "1977", "authors": "A Dempster; N Laird; D Rubin"}, {"ref_id": "b11", "title": "Predicting legislative roll calls from text", "journal": "", "year": "2011", "authors": "S M Gerrish; D M Blei"}, {"ref_id": "b12", "title": "An algorithmic framework for performing collaborative filtering", "journal": "ACM", "year": "1999", "authors": "J L Herlocker; J A Konstan; A Borchers; J Riedl"}, {"ref_id": "b13", "title": "Collaborative filtering for implicit feedback datasets", "journal": "IEEE Computer Society", "year": "2008", "authors": "Y Hu; Y Koren; C Volinsky"}, {"ref_id": "b14", "title": "Matrix factorization techniques for recommender systems", "journal": "IEEE Computer", "year": "2009", "authors": "Y Koren; R Bell; C Volinsky"}, {"ref_id": "b15", "title": "Content-boosted collaborative filtering for improved recommendations", "journal": "", "year": "2002", "authors": "P Melville; M R ; R Nagaraja"}, {"ref_id": "b16", "title": "Content-based book recommending using learning for text categorization", "journal": "ACM", "year": "2000", "authors": "R J Mooney; L Roy"}, {"ref_id": "b17", "title": "One-class collaborative filtering", "journal": "IEEE Computer Society", "year": "2008", "authors": "R Pan; Y Zhou; B Cao; N N Liu; R Lukose; M Scholz; Q Yang"}, {"ref_id": "b18", "title": "Bayesian probabilistic matrix factorization using Markov chain Monte Carlo", "journal": "ACM", "year": "2008", "authors": "R Salakhutdinov; A Mnih"}, {"ref_id": "b19", "title": "Probabilistic matrix title # dataset # Google avg-like avg-dislike", "journal": "", "year": "", "authors": "R Salakhutdinov; A Mnih"}, {"ref_id": "b20", "title": "Most of these articles are popular. The last two columns give the average values of \"predicted\" ratings over those users who have the article (avg-like) in the library and those who do not (avg-dislike). factorization", "journal": "", "year": "2008", "authors": ""}, {"ref_id": "b21", "title": "Generalized probabilistic matrix factorizations for collaborative filtering", "journal": "IEEE Computer Society", "year": "2010", "authors": "H Shan; A Banerjee"}, {"ref_id": "b22", "title": "Hierarchical Dirichlet processes", "journal": "Journal of the American Statistical Association", "year": "2007", "authors": "Y Teh; M Jordan; M Beal; D Blei"}, {"ref_id": "b23", "title": "Joint analysis of time-evolving binary matrices and associated documents", "journal": "", "year": "2010", "authors": "E Wang; D Liu; J Silva; D Dunson; L Carin"}, {"ref_id": "b24", "title": "Large-scale collaborative prediction using a nonparametric random effects model", "journal": "ACM", "year": "2009", "authors": "K Yu; J Lafferty; S Zhu; Y Gong"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Illustration of the two tasks for scientific article recommendation systems, where \u221a indicates \"like\", \u00d7 \"dislike\" and ? \"unknown\".", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "1 .1Draw topic proportions \u03b8j \u223c Dirichlet(\u03b1). 2. For each word n, (a) Draw topic assignment zjn \u223c Mult(\u03b8j). (b) Draw word wjn \u223c Mult(\u03b2z jn ).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: The graphical model for the CTR model.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure3: Recall comparison on in-matrix and out-of-matrix prediction tasks by varying the number of recommended articles. For CTR, we set \u03bbv = 100. Error bars are too small to show. The maximum expected recall for random recommendation is about 6%. CF can not do out-of-matrix prediction. CTR performs best.", "figure_data": ""}, {"figure_label": "564", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 5 :Figure 6 :Figure 4 :564Figure 5: These scatter plots show how the number of articles a user has affects his or her recall. Red lines indicate the average.In these plots, the number of recommended articles is 100. CF can not do out-of-matrix prediction. This shows that CTR performs the best over user-oriented recall.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 7 :7Figure7: Maximum likelihood from incomplete data via the EM algorithm. Here, \"theta\" denotes \u03b8j and \"theta correction\" denotes the offset j . The 10 topics are obtained by joining the top 5 topics ranked by \u03b8 jk and another top 5 topics ranked by | jk |, k = 1, \u2022 \u2022 \u2022 , K. Under CTR, an article of wide interest is likely to exhibit more topics than its text exhibits. For example, this article brings in several other topics, including one on \"Bayesian statistics\" (topic 10). Note that the EM article is mainly about parameter estimation (topic 1), though is frequently referenced by Bayesian statisticians (and scholars in other fields as well).", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 8 :8Figure8: Phase-of-firing coding of natural visual stimuli in primary visual cortex. This figure was created in the same way as Figure7. It shows that a less popular article might also have a high offset value j . In this case, it changes the actual magnitudes in \u03b8j, but does not bring in other topics.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "rij = u T i vj.(1)", "formula_coordinates": [2.0, 413.82, 549.85, 142.1, 11.13]}, {"formula_id": "formula_1", "formula_text": "minU,V i,j (rij \u2212 u T i vj) 2 + \u03bbu||ui|| 2 + \u03bbv||vj|| 2 , (2)", "formula_coordinates": [2.0, 340.04, 629.91, 215.88, 11.79]}, {"formula_id": "formula_2", "formula_text": "rij \u223c N (u T i vj, c \u22121 ij ),(3)", "formula_coordinates": [3.0, 144.99, 73.71, 147.91, 11.55]}, {"formula_id": "formula_3", "formula_text": "cij = a, if rij = 1, b, if rij = 0,(4)", "formula_coordinates": [3.0, 126.24, 254.53, 166.66, 18.52]}, {"formula_id": "formula_4", "formula_text": "rij \u223c N (u T i \u03b8j, c \u22121 ij ).(5)", "formula_coordinates": [3.0, 396.88, 350.4, 159.04, 11.55]}, {"formula_id": "formula_5", "formula_text": "rij \u223c N (u T i vj, c \u22121 ij ).(6)", "formula_coordinates": [4.0, 144.99, 155.36, 147.91, 11.55]}, {"formula_id": "formula_6", "formula_text": "\u03bb \u22121 v I k ), is equivalent to vj \u223c N (\u03b8j, \u03bb \u22121 v IK ),", "formula_coordinates": [4.0, 53.8, 184.01, 239.1, 21.09]}, {"formula_id": "formula_7", "formula_text": "E[rij|ui, \u03b8j, j ] = u T i (\u03b8j + j )", "formula_coordinates": [4.0, 114.77, 232.47, 114.09, 11.13]}, {"formula_id": "formula_8", "formula_text": "L = \u2212 \u03bbu 2 i u T i ui \u2212 \u03bbv 2 j (vj \u2212 \u03b8j) T (vj \u2212 \u03b8j)(7)", "formula_coordinates": [4.0, 61.6, 325.46, 231.3, 12.09]}, {"formula_id": "formula_9", "formula_text": "+ j n log k \u03b8 jk \u03b2 k,w jn \u2212 i,j c ij 2 (rij \u2212 u T i vj) 2 .", "formula_coordinates": [4.0, 70.0, 340.01, 215.1, 13.32]}, {"formula_id": "formula_10", "formula_text": "ui \u2190 (V CiV T + \u03bbuIK ) \u22121 V CiRi (8) vj \u2190 (U CjU T + \u03bbvIK ) \u22121 (U CjRj + \u03bbv\u03b8j). (9", "formula_coordinates": [4.0, 88.13, 448.71, 204.78, 25.84]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [4.0, 289.42, 466.77, 3.48, 7.77]}, {"formula_id": "formula_12", "formula_text": "L(\u03b8j) \u2265 \u2212 \u03bbv 2 (vj \u2212 \u03b8j) T (vj \u2212 \u03b8j) + n k \u03c6 jnk log \u03b8 jk \u03b2 k,w jn \u2212 log \u03c6 jnk = L(\u03b8j, \u03c6j). (10", "formula_coordinates": [4.0, 79.74, 591.85, 209.43, 39.17]}, {"formula_id": "formula_13", "formula_text": ")", "formula_coordinates": [4.0, 289.17, 623.25, 3.73, 7.77]}, {"formula_id": "formula_14", "formula_text": "\u03c6 jnk \u221d \u03b8 jk \u03b2 k,w jn . (11", "formula_coordinates": [4.0, 138.01, 659.22, 151.16, 9.41]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [4.0, 289.17, 659.51, 3.73, 7.77]}, {"formula_id": "formula_16", "formula_text": "\u03b2 kw \u221d j n \u03c6 jnk 1[wjn = w]. (12", "formula_coordinates": [4.0, 374.53, 95.71, 177.65, 10.02]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [4.0, 552.18, 95.99, 3.73, 7.77]}, {"formula_id": "formula_18", "formula_text": "E[rij|D] \u2248 E[ui | D] T (E[\u03b8j | D] + E[ j | D]) .(13)", "formula_coordinates": [4.0, 350.12, 176.78, 205.8, 10.63]}, {"formula_id": "formula_19", "formula_text": "r * ij \u2248 (u * i ) T (\u03b8 * j + * j ) = (u * i ) T v * j ,(14)", "formula_coordinates": [4.0, 372.57, 221.54, 183.35, 11.13]}, {"formula_id": "formula_20", "formula_text": "r * ij \u2248 (u * i ) T \u03b8 * j .(15)", "formula_coordinates": [4.0, 407.82, 277.93, 148.09, 11.13]}, {"formula_id": "formula_21", "formula_text": "| jk |, k = 1, \u2022 \u2022 \u2022 , K.", "formula_coordinates": [7.0, 53.8, 249.44, 80.77, 8.35]}], "doi": ""}