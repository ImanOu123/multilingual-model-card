{"title": "Riemannian Score-Based Generative Modelling", "authors": "Valentin De Bortoli; \u00c9mile Mathieu; Michael Hutchinson; James Thornton; Yee Whye Teh; Arnaud Doucet", "pub_date": "2022-11-22", "abstract": "Score-based generative models (SGMs) are a powerful class of generative models that exhibit remarkable empirical performance. Score-based generative modelling (SGM) consists of a \"noising\" stage, whereby a diffusion is used to gradually add Gaussian noise to data, and a generative model, which entails a \"denoising\" process defined by approximating the time-reversal of the diffusion. Existing SGMs assume that data is supported on a Euclidean space, i.e. a manifold with flat geometry. In many domains such as robotics, geoscience or protein modelling, data is often naturally described by distributions living on Riemannian manifolds and current SGM techniques are not appropriate. We introduce here Riemannian Score-based Generative Models (RSGMs), a class of generative models extending SGMs to Riemannian manifolds. We demonstrate our approach on a variety of manifolds, and in particular with earth and climate science spherical data. * equal contribution.", "sections": [{"heading": "Introduction", "text": "Score-based Generative Models (SGMs) also called diffusion models (Song and Ermon, 2019;Song et al., 2021;Ho et al., 2020;Dhariwal and Nichol, 2021) formulate generative modelling as a denoising process. Noise is incrementally added to data using a diffusion process until it becomes approximately Gaussian. The generative model is then obtained by simulating an approximation of the corresponding time-reversal process, which progressively denoises a Gaussian sample to obtain a data sample. This process is also a diffusion whose drift depends on the logarithmic gradients of the noised data densities, i.e. the Stein scores, estimated using a neural network via score matching (Hyv\u00e4rinen, 2005;Vincent, 2011).\nSGMs have been primarily applied to data living on Euclidean spaces, i.e. manifolds with flat geometry. However, in a large number of scientific domains the distributions of interest are supported on Riemannian manifolds. These include, to name a few, protein modelling (Shapovalov and Dunbrack Jr, 2011), cell development (Klimovskaia et al., 2020), image recognition (Lui, 2012), geological sciences (Karpatne et al., 2018;Peel et al., 2001), graph-structured and hierarchical data (Roy et al., 2007;Steyvers and Tenenbaum, 2005), robotics (Feiten et al., 2013;Senanayake and Ramos, 2018) and high-energy physics (Brehmer and Cranmer, 2020).\nWe introduce in this work Riemannian Score-based Generative Models (RSGMs), an extension of SGMs to Riemannian manifolds which incorporate the geometry of the data by defining the forward diffusion process directly on the Riemannian manifold, inducing a manifold-valued reverse process. This requires constructing a noising process on the manifold that converges to an easy-to-sample reference distribution. We establish that, as in the Euclidean case, the corresponding time-reversal process is also a diffusion whose drift includes the Stein score which is intractable but can similarly be estimated via score matching. Methodological extensions are required as in most cases the transition kernel of the noising process cannot be sampled exactly. For example on compact manifolds it is typically only available as an infinite sum through the Sturm-Liouville decomposition (Chavel, 1984). To this end, we develop non-standard techniques for score estimation and rely on the use of Geodesic Random Walks for sampling (J\u00f8rgensen, 1975). We provide theoretical convergence bounds for RSGMs on compact manifolds and demonstrate our approach on a range of manifolds and tasks, including modelling a number of natural disaster occurrence datasets collected by Mathieu and Nickel (2020). We show that RGSMs achieve better performance than recent baselines (Mathieu and Nickel, 2020;Rozen et al., 2021) and scale better to high-dimensional manifolds.", "publication_ref": ["b19", "b44", "b24", "b49", "b58", "b54", "b32", "b12", "b15", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Euclidean Score-based Generative Modelling", "text": "We recall here briefly the key concepts behind SGMs on the Euclidean space R d and refer the readers to Song et al. (2021) for a more detailed introduction. We consider a forward noising process (X t ) t\u22650 defined by the following Stochastic Differential Equation (SDE)\ndX t = \u2212X t dt + \u221a 2dB t , X 0 \u223c p 0 ,(1)\nwhere (B t ) t\u22650 is a d-dimensional Brownian motion and p 0 is the data distribution. The available data gives us an empirical approximation of p 0 . The process (X t ) t\u22650 is simply an Ornstein-Ulhenbeck (OU) process which converges with geometric rate to N(0, Id). Under mild conditions on p 0 , the timereversed process (Y t ) t\u22650 = (X T \u2212t ) t\u2208[0,T ] also satisfies an SDE (Cattiaux et al., 2021;Haussmann and Pardoux, 1986) given by\ndY t = {Y t + 2\u2207 log p T \u2212t (Y t )}dt + \u221a 2dB t , Y 0 \u223c p T ,(2)\nwhere p t denotes the density of X t . By construction, the law of Y T \u2212t is equal to the law of X t for t \u2208 [0, T ] and in particular Y T \u223c p 0 . Hence, if one could sample from (Y t ) t\u2208[0,T ] then its final distribution would be the data distribution p 0 . Unfortunately we cannot sample exactly from (2) as p T and the scores (\u2207 log p t (x)) t\u2208[0,T ] are intractable. Hence SGMs rely on a few approximations. First, p T is replaced by the reference distribution N(0, Id) as we know that p T converges geometrically towards it. Second, the following denoising score matching identity is exploited to estimate the scores \u2207 xt log p t (x t ) = R d \u2207 xt log p t|0 (x t |x 0 ) p 0|t (x 0 |x t )dx 0 , where p t|0 (x t |x 0 ) is the transition density of the OU process (1) which is available in closed-form. It follows directly that \u2207 log p t is the minimizer of t (s) = E[ s(X t ) \u2212 \u2207 xt log p t|0 (X t |X 0 ) 2 ] over functions s where the expectation is over the joint distribution of X 0 , X t . This result can be leveraged by considering a neural network s \u03b8 : [0, T ] \u00d7 R d \u2192 R d trained by minimizing the loss function (\u03b8) = T 0 \u03bb t t (s \u03b8 (t, \u2022))dt for some weighting function \u03bb t > 0. Finally, an Euler-Maruyama discretization of (2) is performed using a discretization step \u03b3 such that T = \u03b3N for N \u2208 N\nY n+1 = Y n + \u03b3{Y n + 2s \u03b8 (T \u2212 n\u03b3, Y n )} + 2\u03b3Z n+1 , Y 0 \u223c N(0, Id), Z n i.i.d. \u223c N(0, Id).\nThe above showcases the basics of SGMs but we highlight that many improvements have been proposed; see e.g. Song and Ermon (2020), Jolicoeur-Martineau et al. (2021), and Dhariwal and Nichol (2021). In particular, selecting an adaptive stepsize (\u03b3 n ) n\u2208N (Bao et al., 2022;Watson et al., 2021) and using a predictor-corrector scheme (Song et al., 2021) instead of a simple Euler-Maruyama discretization drastically improves performance.", "publication_ref": ["b19", "b14", "b42", "b51", "b24", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Riemannian Score-based Generative Modelling", "text": "We now move to the Riemannian manifold setting, and more specifically assume that M is a complete, orientable connected and boundaryless Riemannian manifold, endowed with a Riemannian metric g 4 . Four components are required to extend SGMs to this setting: i) a forward noising process on M which converges to an easy-to-sample reference distribution, ii) a time-reversal formula on M which defines a backward generative process, iii) a method for approximating samples of SDEs on manifolds, iv) a method to efficiently approximate the drift of the time-reversal process. Notation are gathered in App. B.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Noising processes on manifolds", "text": "The first necessary component is a suitable generic noising process on manifolds that will converge to a convenient stationary distribution. A simple choice is to use Langevin dynamics described by\ndX t = \u2212 1 2 \u2207 Xt U (X t )dt + dB M t ,(3)\nwhich admits the invariant density (w.r.t. the volume form) given by dp ref /dVol M (x) \u221d e \u2212U (x) (Durmus, 2016, Section 2.4), where \u2207 is the Riemannian gradient 5 .\nTwo simple choices for U (x) present themselves. Firstly, setting U (x) = d M (x, \u00b5) 2 /(2\u03b3 2 ), where d M is the geodesic distance and \u00b5 \u2208 M is an arbitrary mean location, induces the drift \u2207 Xt U (X t ) = \u2212 exp \u22121 Xt (\u00b5)/\u03b3 2 6 . This is the potential of the 'Riemannian normal' (Pennec, 2006) distribution. An alternative is to target the 'exponential wrapped' Gaussian. This is the pushforward of a Gaussian distribution in the tangent space at the mean location along the exponential map. The potential is given by U (x) = d M (x, \u00b5) 2 /(2\u03b3 2 ) + log |D exp \u22121 \u00b5 (x)| 7 . In contrast to the Riemannian normal, sampling and evaluating the density of this distribution is easy (e.g. Mathieu et al., 2019).\nOne recovers the standard Ornstein-Uhlenbeck noising process (Song et al., 2021) for both of these target distributions when M = R d and \u00b5 = 0 since then the drift b(t, X t ) = 1 2 exp \u22121 Xt (0) = \u2212 1 2 X t . On compact manifolds, the invariant measure Vol M has finite volume, thus a natural choice is to target the uniform distribution which is given by Vol M /|M|. In this case, \u2207 Xt U (X t ) = 0 and the noising process is simply a Brownian motion on M.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Time-reversal on Riemannian manifolds", "text": "In order to use these noising processes we prove the time-reversal formula for manifolds, a generalisation of the results in the Euclidean case, e.g. see Cattiaux et al. (2021, Theorem 4.9). Consider an SDE of the form dX t = b(X t )dt + dB M t where B M t is a Brownian motion on M. We refer to App. C.3 for an introduction to Brownian motions on manifolds. This result shows that if (X t ) t\u2208[0,T ] is a diffusion process then (X T \u2212t ) t\u2208[0,T ] is also a diffusion process w.r.t. the backward filtration whose coefficients can be computed, and are shown in Eq. (4). The proof relies on an extension of Cattiaux et al. (2021, Theorem 4.9) to the Riemannian manifold case and is postponed to App. H. Theorem 3.1 (Time-reversed diffusion): Let T \u2265 0 and (B M t ) t\u22650 be a Brownian motion on M such that B M 0 has distribution the volume form p ref a . Let (X t ) t\u2208[0,T ] be associated with the SDE dX t = b(X t )dt + dB M t . Let (Y t ) t\u2208[0,T ] = (X T \u2212t ) t\u2208[0,T ] and assume that KL(P|Q) < +\u221e, where Q is the distribution of (B M t ) t\u2208[0,T ] and P the distribution of (X t ) t\u2208[0,T ] . In addition, assume that P t = L(X t ), the distribution of X t , admits a smooth positive density p t w.r.t. p ref for any t \u2208 [0, T ]. Then, (Y t ) t\u2208[0,T ] is associated with the SDE\ndY t = {\u2212b(Y t ) + \u2207 log p T \u2212t (Y t )}dt + dB M t .(4)\na Note that in the case of a non-compact manifold pref is only a measure and not a probability measure.\nThis result can easily be extended to the case where (B M t ) t\u22650 is replaced by (g(t)B M t ) t\u22650 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Approximate sampling of diffusions", "text": "Obtaining samples from SDEs on a manifold is non-trivial in general. If M is isometrically embedded into R p (with p \u2265 d) one can define (B M t ) t\u22650 as a R p -valued process, see App. C.3. However, this approach is extrinsic, as it requires the knowledge of the projection operator to place points back on the manifold at each step which can accumulate errors.\nHere we consider an intrisic approach based on Geodesic Random Walks (GRWs), see J\u00f8rgensen (1975) for a review of their properties. GRWs can approximate any well-behaved diffusion on M. (c) Gaussian Random Walk [Left] and the Brownian motion density [Right] agree well for small time steps.\nFigure 1: Geodesic Random Walks can be used to approximate Brownian motion and more generally SDEs on manifolds. (a) At each step, tangential noise is sampled (red), which is added the drift term (not pictured). This tangent vector is then pushed through the exponential map to produce a geodesics step on the manifold (blue). (b) Iterating this procedure yield approximate sample paths from the process.\nAlgorithm 1 GRW (Geodesic Random Walk) Require: T, N, X \u03b3 0 , b, \u03c3, P 1: \u03b3 = T /N\nStep-size 2: for k \u2208 {0, . . . , N \u2212 1} do 3:\nZ k+1 \u223c N(0, Id) Sample a Gaussian in the tangent space of X \u03b3 k 4:\nW k+1 = \u03b3b(k\u03b3, X \u03b3 k ) + \u221a \u03b3\u03c3(k\u03b3, X \u03b3 k )Z k+1\nCompute the Euler-Maruyama step on tangent space 5:\nX\n\u03b3 k+1 = exp X \u03b3 k [W k+1 ]\nMove along the geodesic defined by W k+1 and\nX \u03b3 k on M 6: return {X \u03b3 k } N k=0\nHence, we introduce GRWs in a general framework and consider a discrete-time process (X \u03b3 n ) n\u2208N which approximates the diffusion (X t ) t\u22650 defined by\ndX t = b(t, X t )dt + \u03c3(t, X t )dB M t .(5)\nThis generalisation is key to sampling the backward diffusion process defined in Theorem 3.1.\nDefinition 3.2 (Geodesic Random Walk): Let X \u03b3 0 be a M-valued random variable. For any \u03b3 > 0, we define (X \u03b3 n ) n\u2208N such that for any n \u2208 N, X \u03b3 n+1 = exp X \u03b3 n [\u03b3{b(X \u03b3 n )+ \u221a \u03b3V n+1 }], where (V n ) n\u2208N is a sequence of TM-valued random variables such that for any n\n\u2208 N, E[V n+1 |F n ] = 0 and E[V n+1 V n+1 |F n ] = \u03c3\u03c3 (X \u03b3 n )\n, where F n is the filtration generated by {X \u03b3 k } n k=0 . We say that the M-valued process (X \u03b3 n ) n\u2208N is a Geodesic Random Walk.\nAlgorithm 1 approximately simulates the diffusion (X t ) t\u2208[0,T ] defined in Eq. (5) using GRWs; see Kuwada (2012) and Cheng et al. (2022) for quantitative error bounds in the time-homogeneous case and App. I.2 for a novel extentsion for the time-inhomogeneous case. Fig. 1 provides a graphical illustration of this procedure.", "publication_ref": ["b53", "b63", "b18"], "figure_ref": ["fig_11", "fig_11"], "table_ref": []}, {"heading": "Score approximation on Riemannian manifolds", "text": "Score matching and loss functions. The reverse process from Eq. (4) involves the Stein score \u2207 log p t which is unfortunately intractable. To derive an approximation, we first remark that for any s, t \u2208 (0, T ] with t > s and x t \u2208 M, p t (x t ) = M p t|s (x t |x s )dP s (x s ), where P s = L(X s ), the distribution of X s . Thus, we have that for any s, t Algorithm 2 RSGM (Riemannian Score-Based Generative Model)\nRequire: \u03b5, T, N, {X m 0 } M m=1 , loss, s, \u03b80, Niter, pref, bfwd, P 1: /// TRAINING /// 2: for n \u2208 {0, . . . , Niter \u2212 1} do 3:\nX0 \u223c (1/M ) M m=1 \u03b4 X m 0 Random mini-batch from dataset 4: t \u223c U ([\u03b5, T ])\nUniform sampling between \u03b5 and T 5:\nXt = GRW(t, N, X0, b, Id, P) Approximate forward diffusion with Algorithm 1 6:\n(\u03b8n) = t(T , N, X0, Xt, loss, s \u03b8n ) Compute score matching loss from Table 2  7: \u03b8n+1 = optimizer_update(\u03b8n, (\u03b8n)) ADAM optimizer step 8: \u03b8 = \u03b8N epoch 9: /// SAMPLING /// 10: Y0 \u223c pref Sample from uniform distribution 11:\nb \u03b8 (t, x) = \u2212b(T \u2212 t, x) + s \u03b8 (T \u2212 t, x) for any t \u2208 [0, T ], x \u2208 M Reverse process drift 12: {Y k } N k=0 = GRW(T, N, Y0, b \u03b8 , Id, P) Approximate reverse diffusion with Algorithm 1 13: return \u03b8 , {Y k } N k=0\nThe proof is postponed to App. J. For any t \u2208 (0, T ] the minimizers of the loss im t on X (M) (where X (M) is the set of vector fields on M) are the same as the ones for t|s . The loss im t is referred to as the implicit score matching (ISM) loss (Hyv\u00e4rinen, 2005). These losses are direct analogous to the versions typically used in Euclidean space.\nIn the case where we have access to {\u2207 log p t|s : T \u2264 t > s \u2265 0}, the forward noising process transition kernels, or an approximation of this family, then we can use the DSM loss to learn {s t \u2208 X (M) : t \u2208 [0, t]}. If this is not the case then we turn to im t . Note that im t requires the computation of a divergence term which requires d Jacobian-vector calls. In high dimension, a stochastic estimator is necessary (Hutchinson, 1989). Following Song and Ermon (2020) and Nichol and Dhariwal (2021) the loss can be weighted with a term \u03bb t > 0.\nParametric family of vector fields. We approximate (\u2207 log p t ) t\u2208[0,T ] by a family of functions {s \u03b8 } \u03b8\u2208\u0398 where \u0398 is a set of parameters and s \u03b8 : [0, T ] \u2192 X (M). In a Euclidean space, vector fields are simply functions s \u03b8 : R d \u2192 R d . In manifolds, although for any x \u2208 M, T x M \u223c = R d , there does not necessarily exist a set of d smooth vector fields Lee, 2006) 8 . Fortunately, one can rely on a larger set of smooth vector fields {E i (x)} n i=1 with n > d that does span the tangent bundle. Then it suffices to construct a neural network s \u03b8 : [0, T ] \u00d7 M \u2192 R n to parametrise the score network as s \u03b8 (t, x) = n i=1 s i \u03b8 (t, x)E i (x). See App. E for a discussion on the different choices of generating sets {E i (x)} n i=1 . Combining this parameterization with the score matching losses, the time-reversal formula of Theorem 3.1 and the sampling of forward and backward processes described in Sec. 3.3, we define our RGSM algorithm in Algorithm 2. This algorithm can also benefit from a predictor-corrector scheme as in Song et al. (2021), see App. G.\n{E i } d i=1 such that span {E i (x)} d i=1 = T x M (Chapter 8, page 179,", "publication_ref": ["b49", "b48", "b66", "b19"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "RSGMs on compact manifolds", "text": "Assuming compactness of the manifold M, we can leverage a number of special properties to implement a specific case of our algorithm. In particular we benefit from the fact that on compact manifolds we have a proper uniform distribution over the manifold, and have access to a variety of approximations of the heat kernel. As highlighted in Sec. 3.1, in the compact setting we use Brownian motion as the noising SDE, which targets the uniform distribution as the stationary distribution. Table 1 highlights the main differences between RSGMs on compact manifolds, generic manifolds and Euclidean score-based models.\nHeat kernel on compact Riemannian manifolds. For any x 0 \u2208 M and t \u2265 s \u2265 0, the heat kernel p t|s (\u2022|x s ) is defined as the density of B M t w.r.t. the uniform measure on the manifold. Contrary to the Gaussian transition density of the OU process (or the Brownian motion) in the Euclidean setting, it is typically only available as an infinite series. In order to circumvent this issue we consider two techniques: i) a truncation approach, ii) a Taylor expansion around t = 0 called a Varadhan asymptotics. First, we recall that in the case of compact manifolds the heat kernel is given by the Sturm-Liouville decomposition (Chavel, 1984) given for any t > 0 and x 0 , x t \u2208 M by\np t|0 (x t |x 0 ) = j\u2208N e \u2212\u03bbj t \u03c6 j (x 0 )\u03c6 j (x t ),(6)\nwhere the convergence occurs in L 2 (p ref \u2297p ref ), (\u03bb j ) j\u2208N and (\u03c6 j ) j\u2208N are the eigenvalues, respectively the eigenvectors, of \u2212\u2206 M , the Laplace-Beltrami operator in the manifold, in L 2 (p ref ) (Saloff-Coste, 1994, Section 2). When the eigenvalues and eigenvectors are known, we rely on an approximation of the logarithmic gradient of p t|0 by truncating the sum in Eq. (6) with J \u2208 N terms to obtain for any t > 0 and\nx 0 , x t \u2208 M \u2207 xt log p t|0 (x t |x 0 ) \u2248 S J,t (x 0 , x t ) \u2207 xt log J j=0 e \u2212\u03bbj t \u03c6 j (x 0 )\u03c6 j (x t ).(7)\nUnder regularity conditions on M it can be shown that for any x, y \u2208 M and t \u2265 0, lim J\u2192+\u221e S J,t (x 0 , x t ) = \u2207 xt log p t|0 (x t |x 0 ) (Jones et al., 2008, Lemma 1). In the case of the d-dimensional torus or sphere the eigenvalues and eigenvectors are computable (Saloff-Coste, 1994, Section 2) and we can apply this method to approximate p t|0 for any t > 0, see App. F\nWhen the eigenvalues and eigenvectors are unknown or not tractable, we can still derive an approximation of the heat kernel for small times t. Using Varadhan's asymptotics-see Bismut (1984, Theorem 3.8) or Chen et al. (2021, Theorem 2.1)-for any x, y \u2208 M with y / \u2208 Cut(x) (where Cut(x) is the cut-locus of x in M (Lee, 2018, Chapter 10)) we have that\nlim t\u21920 t\u2207 xt log p t|0 (x t |x 0 ) = exp \u22121 xt (x 0 ).(8)\nUsing the previously defined score-matching losses and the approximations to the heat kernel above, we highlight three methods to compute \u2207 log p t in Table 2. \np t|0 exp \u22121 X t t|0 (DSM) None 1 2 E s(Xt) \u2212 \u2207 log p t|0 (Xt|X0) 2 O(1) Truncation (7) 1 2 E s(Xt) \u2212 SJ,t(X0, Xt) 2 asymptotic expansion O(1) Varhadan (8) 1 2 E s(Xt) \u2212 exp \u22121 X t (X0)/t 2 O(1) t|s (DSM) Varhadan (8) 1 2 E s(Xt) \u2212 exp \u22121 X t (Xs)/(t \u2212 s) 2 O(1) im t (ISM) Deterministic E 1 2 s(Xt) 2 + div(s)(Xt) O(d) Stochastic E 1 2 s(Xt) 2 + \u03b5 \u2202s(Xt)\u03b5 O(1)\nConvergence results in the compact setting We now provide a theoretical analysis of RSGM under the assumption that M is compact. The following result ensures that RSGM generates samples whose distribution is close to the data distribution p 0 . Let us denote {Y k } n\u2208{0,...,N } the sequence generated by Algorithm 2. This result relies on the following assumption, which is satisfied for a large class of manifolds M such as the d-dimensional sphere and torus, compact matrix groups and products of these manifolds.\nAssumption 1: There exist C, \u03b1 > 0 such that for any t \u2208 (0, 1] and x \u2208 M, p t|0 (x|x) \u2264 Ct \u2212\u03b1/2 , where p t|0 (\u2022|x 0 ) is the density of the heat kernel, i.e. the density of B M t with initial condition x 0 a .\na The diagonal upper-bound is implied by Sobolev inequalities which control of the growth of some functions by the growth of their gradient. A1 is satisfied in our experiments, see Saloff-Coste (1994) and Gross (1992).\nTheorem 4.1: Assume A1, that p 0 is smooth and positive and that there exists M \u2265 0 such that for any\nt \u2208 [0, T ] and x \u2208 M, s \u03b8 (t, x) \u2212 \u2207 log p t (x) \u2264 M, with s \u03b8 \u2208 C([0, T ] , X (M)). Then if T > 1/2, there exists C \u2265 0 independent on T such that W 1 (L(Y N ), p 0 ) = C(e \u2212\u03bb1T + T /2M + e T \u03b3 1/2 ),\nwhere W 1 is the Wasserstein distance of order one on the probability measures on M.\nThe proof is postponed to App. I. In particular, for any \u03b5 > 0, choosing T > 0 large enough, M small enough (which can be achieved using the universal property of neural networks) and \u03b3 small enough, we get that W 1 (L(Y N ), p 0 ) \u2264 \u03b5. This result might seem weaker than the result obtained for Moser flows in Rozen et al. (2021, Theorem 3), but we emphasize that our bound takes into account the time-discretization contrary to Rozen et al. ( 2021) which considers the continuous-time flow. If we consider the time-reversed continuous-time SDE then we recover a bound in total variation distance, see App. I. Note that the upper bound M encompasses both the bias introduced by the use of a neural network and the bias introduced by the use of an approximation of the score.", "publication_ref": ["b15", "b40"], "figure_ref": [], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Related work", "text": "In this section we discuss previous work on parametrizing family of distributions for manifold-valued data. Here, the manifold structure is considered to be prescribed, in contrast with methods that jointly learn the manifold structure and density (e.g. Brehmer and Cranmer, 2020;Caterini et al., 2021).\nPush-forward of Euclidean normalizing flows. More recently, approaches leveraging the flexibility of normalizing flows (Papamakarios et al., 2019) have been proposed. Following the wrapping method described above, these methods parametrize a normalizing flow in R n before being pushed along an invertible map \u03c8 : R n \u2192 M. However, to globally represent the manifold, the map \u03c8 needs to be a homeomorphism, which can only happen if M is topologically equivalent to R n , hence limiting the scope of that approach. One natural choice for this map is the exponential map exp x : T x M \u223c = R d . This approach has been taken, for instance, by Falorsi et al. (2019) and Bose et al. (2020), respectively parametrizing distributions on Lie groups and hyperbolic space.\nNeural ODE on manifolds. To avoid artifacts or numerical instabilities due to the manifold embedding, another line of work uses tools from Riemannian geometry to define flows directly on the manifold of interest (Falorsi and Forr\u00e9, 2020;Mathieu and Nickel, 2020;Falorsi, 2021). Since these methods do not require a specific embedding mapping, they are referred as Riemannian. They extend continuous normalizing flows (CNFs) (Grathwohl et al., 2019) to the manifold setting, by implicity parametrizing flows as solutions of Ordinary Differential Equations (ODEs). As such, the parametric flow is a continuous function of time. This approach has recently been extended by Rozen et al. (2021) introducing Moser flows, whose main appeal being that it circumvents the need to solve an ODE in the training process. We refer to App. K for an in-depth discussion on the links between our work and Moser flows.\nOptimal transport on manifolds. Another line of work has developed flows on manifolds using tools from optimal transport. (Sei, 2013) introduced a flow that is given by f \u03b8 : x \u2192 exp x (\u2207\u03c8 c \u03b8 ) with \u03c8 c \u03b8 a c-convex function and c = d 2 M the squared geodesic distance. This approach is motivated by the fact that the optimal transport map takes such an expression (Ambrosio, 2003). These methods operate directly on the manifold, similarly to CNFs, yet in contrast they are discrete in time. The benefits of this approach depend on the specific choice of parametric family of c-convex functions (Rezende and Racani\u00e8re, 2021; Cohen et al., 2021), trading-off expressivity with scalability.  ", "publication_ref": ["b12", "b13", "b29", "b10", "b30", "b28", "b37", "b3", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In this section we benchmark the empirical performance of RSGMs along with other manifold-valued methods introduced in Sec. 5. We also compare to a 'Stereographic' score-based model, introduced in App. N. First, we assess their modelling capacity on earth and climate science spherical data. Then, we test the methods scalability with respect to manifold dimensions with a synthetic experiment on the torus T d . Eventually, we evaluate the models' regularity and time complexity with a synthetic SO 3 (R) target. Experimental details are provided in App. O. The code used to run the experiments can be found at github.com/oxcsml/riemannian-score-sde.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Earth and climate science datasets on the sphere", "text": "We start by evaluating RSGMs on a collection of simple datasets, each containing an empirical distribution of occurrences of earth and climate science events on the surface of the earth. These events are: volcanic eruptions ((NGDC/WDS), 2022b), earthquakes ((NGDC/WDS), 2022a), floods (Brakenridge, 2017) and wild fires (EOSDIS, 2020). We compare to previous baseline methods: Riemannian Continuous Normalizing Flows (Mathieu andNickel, 2020), Moser Flows (Rozen et al., 2021) and a mixture of Kent distributions (Peel et al., 2001). Additionally, we consider a standard SGM on the 2D plane followed by the inverse stereographic projection which induces a density on the sphere (Gemici et al., 2016). We evaluate the log-likelihood of each model, extending to the manifold setting the likelihood computation techniques of SGMs, see App. D. We observe from Table 4, that all benchmarked methods have comparable performance when evaluated on these simple tasks with RSGM performing marginally better on most datasets. However, we empirically notice that Moser flows are slow to train and additionally that both Moser flows and stereographic SGMs are computationally expensive to evaluate.", "publication_ref": ["b27", "b36"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Synthetic data on tori", "text": "We now move to another manifold, that is the torus   memory required to compute this estimator grows either in O(Kd) for exact divergences or O(K) for approximated divergences (see Table 3).\nT d = S 1 \u00d7 \u2022 \u2022 \u2022 \u00d7 S 1 ,\nIn Fig. 3, we observe that RSGMs are able to fit well the target distribution even in high dimension, with a linear or constant computational cost-depending on the divergence estimator. In contrast, Moser flows scale poorly with the dimension, to the extent that we are unable to train them for d \u2265 10. This is due to the combination of the complexity which grows linearly with both the dimension d and the number of MC samples K, which itself ought to grow exponentially with d-as discussed in the previous paragraph. This is illustrated by the gap between the 'Moser' and 'ODE' likelihoods which increases with the manifold dimension (see left Fig. 3).", "publication_ref": [], "figure_ref": ["fig_2", "fig_2"], "table_ref": ["tab_3"]}, {"heading": "Synthetic data on the Special Orthogonal group", "text": "In order to demonstrate the broad range of applicability of our model we now turn to the task of density estimation on the special orthogonal group SO d (R) = {Q \u2208 M d (R) : QQ = Id, det(Q) = 1}. We consider the synthetic dataset consisting of samples in SO 3 (R) from a mixture of wrapped normal distributions with M components.\nWe compare RSGMs against Moser flows and a wrapped-exponential baseline inspired by Falorsi et al. (2019)-where we parametrize a standard Euclidean SGM on so(3) that is then pushed-forward on SO 3 (R). RSGMs are trained using the t|0 (DSM) loss with the Varadhan approximation (see Table 2). From Table 5 we observe that, RSGMs perform consistently, whether the target distribution has few or many mixture components M , as opposed to Exp-wrapped SGMs and Moser flows which only perform well in some range of M . Similarly to Sec. 6.2, we find Moser flows to be much slower to train due to the large number of Monte Carlo samples needed in the reguralizer (K = 10 4 ). We Test log-likelihood (b) RSGMs are much more robust to hyperparameters than Exp-wrapped SGMs. The diffusion coefficient is given by \u03c3(t, Xt) = \u03b2(t), \u03b2(t) = \u03b20 + (\u03b2 f \u2212 \u03b20)t.  also note from Table 5 that the number of score network evaluations (NFE) is significantly lower for RSGMs, and is particularly detrimental for Moser flows ( 10 3 ). ", "publication_ref": ["b29"], "figure_ref": [], "table_ref": ["tab_2", "tab_6", "tab_6"]}, {"heading": "Synthetic data on hyperbolic space", "text": "Finally we demonstrate RSGM on a non-compact manifold: the two dimensional hyperbolic space H 2 , which is defined as the simply connected space of constant negative curvature. We use Langevin dynamics as the noising process (Eq. (3)) and target a wrapped Gaussian as the invariant distribution. We again consider a synthetic dataset of samples from a mixture of exp-wrapped normal distribution. From Fig. 5, we can qualitatively see that both score-based models are able to fit the target distribution.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "Discussion and limitations", "text": "In this paper we introduced Riemannian Score-Based Generative Models (RSGMs), a class of deep generative models that represent target densities supported on manifolds, as the time-reversal of Langevin dynamics. The main benefits of our method stems from its scalability to high dimensions, its applicability to a broad class of manifolds due to the diversity of available loss functions, its robustness and crucially its capacity to model complex datasets. We also provided theoretical guarantees on the convergence of RSGMs. In future work, we would like explore more generic classes of manifolds, such a ones with a boundary, along with alternative noising processes. Another promising extension concerns stochastic control on manifolds and more precisely, deriving efficient algorithms to solve Schr\u00f6dinger bridges (Thornton et al., 2022) ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Supplementary to:", "text": "Riemannian Score-Based Generative Modelling A Organization of the supplementary\nIn this supplementary we first introduce notation in App. B. We gather the proof of Theorem 3.1 as well as additional derivations on score-based generative models and Riemannian manifolds. In App. C, we recall basics on stochastic Riemannian geometry following Hsu (2002). In App. D, we introduce an extension to the Riemannian setting of the likelihood computation techniques in diffusion models. Details about parametric vector fields are given in App. E. In App. F, we recall some basic facts about eigenvalues and eigenfunctions of the Laplace-Beltrami operator on the d-dimensional sphere and torus. We present an extension of Algorithm 2 using predictor-corrector schemes in App. G. In App. H, we prove the extension of the time-reversal formula to manifold in Theorem 3.1. We prove the convergence of RSGM, i.e. Theorem 4.1, in App. I. The proof of lemma 3.3 drawing links between the denoising score matching loss and the implicit score matching loss is presented App. J. We provide a thorough comparison between our approach and the one of ", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "B Notation", "text": "We refer to App. C for more details about the basic concepts of Riemannian geometry and stochastic processes. In this section, we merely introduce the notation used in our work. We postpone an introduction to stochastic processes on manifolds to App. C.2.\nIn this work we always consider a smooth, connected and complete manifold M. We focus on the case of Riemannian manifolds, namely manifolds equipped with a metric g. Metrics g are smooth scalar product on the manifold allowing us to define the notion of distance on a manifold. We refer to App. C for a precise definition and a discussion on metrics. Given a smooth map f \u2208 C \u221e (M, R), the gradient \u2207f is defined for any\nf : M \u2192 R, x \u2208 M, v \u2208 T x M, \u2207f, v g = df (v). The distane d M (x, y\n) is defined as the infimum of the length of all the curves on M joining x and y. Geodesics are path defined on M by a second order equation (and a starting point and speed). This second order equation corresponds to the first order minimization of an energy functional whose minimizers also minimize the length. In App. C, we introduce the notion of geodesics using parallel transport. The exponential mapping exp\nx : UM \u2192 M with U \u2282 T x M is such that exp x (v) = \u03b3(1) with \u03b3(1)\nthe geodesics with initial condition (x, v) at time t = 1. Finally the volume form is a differentiable form of same degree as the dimension of M. Since M is an orientable Riemannian manifold there is a natural volume form defined using the metric g, namely \u03c9\n(x) = |g(x)| 1/2 dx 1 \u2227 . . . dx d .\nIn this paper, we abuse notation and denote by the volume form this natural volume form.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Preliminaries on stochastic Riemannian geometry", "text": "In this section, we recall some basic facts on Riemannian geometry and stochastic Riemannian geometry. We follow Hsu (2002), Lee (2018), and Lee (2006) and refer to Lee (2010) and Lee (2013) for a general introduction to topological and smooth manifolds. Throughout this section M is a d-dimensional smooth manifold, TM its tangent bundle and T M it cotangent bundle. We denote C \u221e (M) the set of real-valued smooth functions on M and X (M) the set of vector fields on M.", "publication_ref": ["b47", "b65", "b66", "b64", "b67"], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Tensor field, metric, connection and transport", "text": "Tensor field and Riemannian metric For a vector space V let T k, (V ) = V \u2297k \u2297 (V ) \u2297 with k, \u2208 N. For any k, \u2208 N we define the space of (k, )-tensors as\nT k, M = p\u2208M T k, (T p M). Note that \u0393(M, T 0,0 M) = C \u221e (M), X (M) = \u0393(M, T 1,0 M) and that the space of 1-form on M is given by \u0393(M, T 0,1 M), where \u0393(M, V (M)\n) is a section of a vector bundle V (M) (see Lee, 2013, Chapter 10). For any k \u2208 N, we denote T |k| M = k j=0 T j,k\u2212j M. M is said to be a Riemannian manifold if there exists g \u2208 \u0393(M, T 0,2 M) such that for any x \u2208 M, g(x) is positive definite. g is called the Riemannian metric of M. Every smooth manifold can be equipped with a Riemannian metric (see Lee, 2018, Proposition 2.4). In local coordinates we define G = {g i,j } 1\u2264i,j\u2264d = {g(X i , X j )} 1\u2264i,j\u2264d , where {X i } d i=1 is a basis of the tangent space. In what follows we consider that M is equipped with a metric g and for any X, Y \u2208 X (M) we denote X, Y M = g(X, Y ).\nConnection A connection \u2207 is a mapping which allows one to differentiate vector fields w.r.t other vector fields. \u2207 is a linear map \u2207 : X (M) \u00d7 X (M) \u2192 X (M). In addition, we assume that i) for any\nf \u2208 C \u221e (M), X, Y \u2208 X (M), \u2207 f X (Y ) = f \u2207 X Y , ii) for any f \u2208 C \u221e (M), X, Y \u2208 X (M), \u2207 X (f Y ) = f \u2207 X Y + X(f )Y .\nGiven a system of local coordinates, the Christoffel symbols {\u0393 k i,j } 1\u2264i,j,k\u2264d are given for any i, j \u2208 {1, . . . , d} by\n\u2207 Xi X j = d k=1 \u0393 k i,j X k .\nWe also define the Levi-Civita connection \u2207 by considering the additional two conditions: i) \u2207 is torsion-free, i.e. for any X, Y \u2208 X (M) we have\n\u2207 X Y \u2212 \u2207 Y X = [X, Y ], where [X, Y ] is the Lie bracket between X and Y , ii) \u2207 is compatible with the metric g, i.e. for any X, Y, Z \u2208 X (M), X( Y, Z M ) = \u2207 X Y, Z M + Y, \u2207 X Z M .\nWe recall that the Levi-Civita connection is uniquely defined since for any X, Y, Z \u2208 X (M) we have\n2 \u2207 X Y, Z M = X( Y, Z M ) + Y ( Z, X M ) \u2212 Z( X, Y M ) + [X, Y ], Z M \u2212 [Z, X], Y M \u2212 [Y, Z], X M .\nIn this case, the Christoffel symbols are given for any i, j, k \u2208 {1, . . . , d} by\n\u0393 k i,j = 1 2 d m=1 g km (\u2202 j g m,i + \u2202 i g m,j \u2212 \u2202 m g i,j ),\nwhere {g i,j } 1\u2264i,j\u2264d = G \u22121 . Note that if M is Euclidean then for any i, j, k \u2208 {1, . . . , d}, \u0393 k i,j = 0. We also extend the connection so that for any X \u2208 X (M) and f \u2208 C \u221e (M ) we have \u2207 X f = X(f ).\nIn particular, we have that \u2207 X f \u2208 C \u221e (M). In addition, we extend the connection such that for any \u03b1 \u2208 \u0393(M, T 0,1 M), X, Y \u2208 X (M) we have \u2207 X \u03b1(Y ) = \u03b1(\u2207 X Y ) \u2212 X(\u03b1(Y )). In particular, we have that \u2207 X \u03b1 \u2208 \u0393(M, T 1,0 M). Note that for any X \u2208 X (M) and \u03b1, \u03b2 \u2208 T |1| M we have \u2207 X (\u03b1 \u2297 \u03b2) = \u2207 X \u03b1 \u2297 \u03b2 + \u03b1 \u2297 \u2207 X \u03b2. Similarly, we can define recursively \u2207 X \u03b1 for any \u03b1 \u2208 \u0393(M, T k, M) with k, \u2208 N. Such an extension is called a covariant derivative.\nParallel transport, geodesics and exponential mapping Given a connection, we can define the notion of parallel transport, which transports vector fields along a curve. Let \u03b3 : [0, 1] \u2192 M be a smooth curve. We define the covariant derivative along the curve \u03b3 by D\u03b3 : X (\u03b3) \u2192 X (\u03b3) similarly to the connection, where X (\u03b3) = \u0393(\u03b3([0, 1]), TM). In particular if\u03b3 and X \u2208 X (\u03b3) can be extended to X (M) then we define D\u03b3(X) = \u2207\u03b3X \u2208 X (M). In what follows, we denote D = \u2207 for simplicity. We say that X \u2208 X (\u03b3) is parallel to \u03b3 if for any t \u2208 [0, 1], \u2207\u03b3X(t) = 0. In local coordinates, let X \u2208 X (\u03b3) be given for any t \u2208\n[0, 1] by X = d i=1 a i (t)E i (t) (assuming that \u03b3([0, 1]\n) is entirely contained in a local chart), then we have that for any t \u2208 [0, 1] and k \u2208 {1, . . . , d}\na k (t) + d i,j=1 \u0393 k i,j (x(t))\u1e8b i (t)a j (t) = 0.(1)\nA curve \u03b3 on M is said to be a geodesics if\u03b3 is parallel to \u03b3. Using Eq. (1) we get tha\u1e97\nx k (t) + d i,j=1 \u0393 k i,j (x(t))\u1e8b i (t)\u1e8b j (t) = 0.\nFor more details on geodesics and parallel transport, we refer to Lee (2018, Chapter 4). In addition, we have that parallel transport provides a linear isomorphism between tangent spaces. Indeed, let v \u2208 T x M and \u03b3 : [0, 1] \u2192 M with \u03b3(0) = x a smooth curve. Then, there exists a unique vector field X v \u2208 X (\u03b3) such that X v (x) = v and X v is parallel to \u03b3. For any t \u2208 [0, 1], we denote\n\u0393 t 0 : T x M \u2192 T \u03b3(t) M the linear isomorphism such that \u0393 t 0 (v) = X v (\u03b3(t)). For any x \u2208 M and v \u2208 T x M we denote \u03b3 x,v : [0, \u03b5 x,v ] the geodesics (defined on the maximal interval [0, \u03b5 x,v ]) on M such that \u03b3(0) = x and\u03b3(0) = v. We denote U x = {v \u2208 T x M : \u03b5 x,v \u2265 1}.\nNote that 0 \u2208 U x . For any x \u2208 M, we define the exponential mapping exp x : U x \u2192 M such that for any v \u2208 U x , exp x (v) = \u03b3 x,v (1). If for any x \u2208 M, U x = T x M, the manifold is called geodesically complete. As any connected compact manifold is geodesically complete, there exists a geodesic between any two points x, y \u2208 M (see Lee, 2018, Lemma 6.18). For any x, y \u2208 M, we denote Geo x,y the sets of geodesics \u03b3 such that \u03b3(0) = x and \u03b3(y) = 1. For any x, y \u2208 M we denote \u0393 y\nx (\u03b3) : T x M \u2192 T y M the linear isomorphism such that for any v \u2208 T x M, \u0393 y x (v) = X v (\u03b3(1)), where \u03b3 \u2208 Geo x,y . Note that for any x \u2208 M there exists V x \u2282 M such that x \u2208 V x and for any y \u2208 V x we have that |Geo x,y | = 1. In this case, we denote \u0393 y\nx = \u0393 y x (\u03b3) with \u03b3 \u2208 Geo x,y .\nOrthogonal projection We will make repeated use of orthonormal projections on manifolds.\nRecall that since M is a closed Riemannian manifold we can use the Nash embedding theorem (Gunther, 1991). In the rest of this paragraph, we assume that M is a Riemannian submanifold of R p for some p \u2208 N such that its metric is induced by the Euclidean metric. In order to define the projection we introduce \nM \u2282 F \u2282 E(M). Let p \u2208 C \u221e (R p , R p\n) such that for any x \u2208 F, p(x) =p(x) (given by Whitney extension theorem for instance). Finally, we define P : R p \u2192 R p such that for any x \u2208 R p , P (x) = dp(x). Note that for any x \u2208 M, P (x) is the orthogonal projection T x M and that P \u2208 C \u221e (R p , R p ).", "publication_ref": ["b41"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Stochastic Differential Equations on manifolds", "text": "Stratanovitch integral For reasons that will become clear in the next paragraph, it is easier to define Stochastic Differential Equations (SDEs) on manifolds w.r.t the Stratanovitch integral (Kloeden and Platen, 2011, Part II, Chapter 3). We consider a filtered probability space (\u2126, (F t ) t\u22650 , P). Let (X t ) t\u22650 and (Y t ) t\u22650 be two real continuous semimartingales. We define the quadratic covariation ([X, Y] t ) t\u22650 such that for any t \u2265 0\n[X, Y] t = X t Y t \u2212 X 0 Y 0 \u2212 t 0 X s dY s \u2212 t 0 Y s dX s .\nWe refer to Revuz and Yor (1999, Chapter IV) for more details on semimartingales and quadratic variations. We denote [X] = [X, X]. In particular, we have that ([X, Y] t ) t\u22650 is an adapted continuous process with finite-variation and therefore [[X, Y]] = 0. Let (X t ) t\u22650 and (Y t ) t\u22650 be two real continuous semimartingales, then we define the Stratanovitch integral as follows for any t \u2265 0\nt 0 X s \u2022 dY s = t 0 X s dY s + 1 2 [X, Y] t . In particular, denoting (Z 1\nt ) t\u22650 and (Z 2 t ) t\u22650 the processes such that for any t \u2265 0,\nZ 1 t = t 0 X s \u2022dY s and Z 2 t = t 0 X s dY s , we have that [Z 1 ] = [Z 2 ].\nWe refer to Kurtz et al. (1995) for more details on Stratanovitch integrals. Note that if for any t \u2265 0,\nX t = t 0 f (X s ) \u2022 dY s with C 1 (R, R), then [X, Y] t = t 0 f (X s )f (X s )dY s . Assuming that f \u2208 C 3 (R, R)\nwe have that (Revuz and Yor, 1999, Chapter IV, Exercise 3.15)\nf (X t ) = f (X 0 ) + t 0 f (X s ) \u2022 dX s .\nThe proof relies on the fact that for any t \u2265 0, d[X, f (X)] t = f (X t )d[X] t . This result should be compared with It\u00f4's lemma. In particular, Stratanovitch calculus satisfies the ordinary chain rule making it a useful tool in differential geometry which makes a heavy use of diffeomorphism. Finally, we have the following correspondence between Stratanovitch and It\u00f4 SDEs. Assume that\n(X t ) t\u2208[0,T ] is a strong solution to dX t = b(t, X t )dt + \u03c3(t, X t ) \u2022 dB t , with b \u2208 C \u221e (R d , R d ) and \u03c3 \u2208 C \u221e (R d , R d\u00d7d ).\nThen, we have that\ndX t = {b(t, X t ) +b(X t )}dt + \u03c3(t, X t )dB t ,b = (\u22121/2)[div(\u03c3\u03c3 ) \u2212 \u03c3div(\u03c3 )]. (2)\nwhere for any A \u2208 C \u221e (R d , R d\u00d7d ) we have that div(A) \u2208 C \u221e (R d , R d ) and for any i \u2208 {1, . . . , d} and\nx \u2208 R d , div(A) i (x) = d j=1 \u2202 j A i,j (x). In particular, note that if for x 0 \u2208 R d , \u03c3(x 0 ) is an orthogonal projection, then \u03c3(x 0 )b(x 0 ) = 0.", "publication_ref": ["b59", "b62"], "figure_ref": [], "table_ref": []}, {"heading": "SDEs on manifolds", "text": "We define semimartingales and SDEs on manifold through the lens of their actions on functions. A continuous M-valued stochastic process (X t ) t\u22650 is called a M-valued semimartingale if for any f \u2208 C \u221e (M) we have that (f (X t )) t\u22650 is a real valued semimartingale. Let \u2208 N, V 1: = {V i } i=1 \u2208 X (M) and Z 1: = {Z i } i=1 a collection of real-valued semimartingales.\nA M-valued semimartingale (X t ) t\u22650 is said to be the solution of SDE(V 1: , Z 1: , X 0 ) up to a stopping \u03c4 with X 0 a M-valued random variable if for all f \u2208 C \u221e (M) and t \u2208 [0, \u03c4 ] we have\nf (X t ) = f (X 0 ) + i=1 t 0 V i (f )(X s ) \u2022 dZ i s .\nSince the previous SDE is defined w.r.t the Stratanovitch integral we have that if (X t ) t\u22650 is a solution of SDE(V 1: , Z 1: , X 0 ) and \u03a6 : M \u2192 N is a diffeomorphism then (\u03a6(X t )) t\u22650 is a solution of SDE(\u03a6 V 1: , Z 1: , \u03a6(X 0 )), where \u03a6 is the pushforward operation (see Hsu, 2002, Proposition 1.2.4). Because the vector fields {V i } i=1 are smooth we have that for any \u2208 N, V 1: = {V i } i=1 \u2208 X (M) and Z 1: = {Z i } i=1 a collection of real-valued semimartingales, there exists a unique solution to SDE(V 1: , Z 1: , X 0 ) (see Hsu, 2002, Theorem 1.2.9).", "publication_ref": ["b47"], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Brownian motion on manifolds", "text": "In this section, we introduce the notion of Brownian motion on manifolds. We derive some of its basic convergence properties and provide alternative definitions (stochastic development, isometric embedding, random walk limit). These alternative definitions are the basis for our alternative methodologies to sample from the time-reversal. To simplify our discussion, we assume that M is a connected compact orientable Riemannian manifold equipped with the Levi-Civita connection \u2207. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Gradient, divergence and Laplace operators", "text": "Let f \u2208 C \u221e (M). We define \u2207f \u2208 X (M) such that for any X \u2208 X (M) we have X, \u2207f M = X(f ). Let {X i } d i=1 \u2208 X (M) d such that for any x \u2208 M, {X i (x)} d\ni=1 is an orthonormal basis of T x M. Then, we define div :\nX (M) \u2192 C \u221e (M) (linear) such that for any X \u2208 X (M), div(X) = d i=1 \u2207 Xi X, X i M .\nThe following Stokes formula (also called divergence theorem, see Lee (2018, p.51)) holds for any f \u2208 C \u221e (M) and\nX \u2208 X (M), M div(X)(x)f (x)dp ref (x) = \u2212 M X(f )(x)dp ref (x). Let X = d i=1 a i X i in local coordinates.\nUsing the Stokes formula and the definition of the gradient we get that in local coordinates \u2207f\n= d i,j=1 g i,j \u2202 i f X j , div(X) = det(G) \u22121/2 d i=1 \u2202 i (det(G) 1/2 a i ). The Laplace-Beltrami operator is given by \u2206 M : C \u221e (M ) \u2192 C \u221e (M ) and for any f \u2208 C \u221e (M ) by \u2206 M (f ) = div(grad(f )). In local coordinates we obtain \u2206 M (f ) = det(G) \u22121/2 d i=1 \u2202 i (det(G) 1/2 d j=1 g i,j \u2202 j f ).\nUsing the Nash isometric embedding theorem (Gunther, 1991) we will see that \u2206 M can always be written as a sum of squared operators. However, this result requires an extrinsic point of view as it relies on the existence of projection operators. In contrast, if we consider the orthonormal bundle OM, see (Hsu, 2002, Chapter 2), we can define the Laplace-Bochner operator\n\u2206 OM : C \u221e (OM) \u2192 C \u221e (OM) as \u2206 OM = d i=1 H 2\ni , where we recall that for any i \u2208 {1, . . . , d}, H i is the horizontal lift of e i . In this case, \u2206 OM is a sum of squared operators and we have that for any Hsu, 2002, Proposition 3.1.2). Being able to express the various Laplace operators as a sum of squared operators is key to express the associated diffusion process as the solution of an SDE.\nf \u2208 C \u221e (M), \u2206 OM (f \u2022 \u03c0) = \u2206 M (f ) (see", "publication_ref": ["b41", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Alternatives definitions of Brownian motion", "text": "We are now ready to define a Brownian motion on the manifold M. Using the Laplace-Beltrami operator, we can introduce the Brownian motion through the lens of diffusion processes.\nDefinition C.1 (Brownian motion): Let (B M t ) t\u22650 be a M-valued semimartingale. (B M t ) t\u22650 is a Brownian motion on M if for any f \u2208 C \u221e (M), (M f t )\nt\u22650 is a local martingale where for any t \u2265 0\nM f t = f (B M t ) \u2212 f (B M 0 ) \u2212 1 2 t 0 \u2206 M f (B M s )ds.\nNote that this definition is in accordance with the definition of the Brownian motion as a diffusion process in the Euclidean space R d , since in this case \u2206 M = \u2206. A key property of frame bundles and orthonormal bundles is that any semimartingale on M can be associated to a process on FM (or OM) and a process on R d . The proof of the following result can be found in Hsu (2002, Propositions 3.2.1 and 3.2.2).\nProposition C.2 (Intrinsic view of Brownian motion): Let (B M t ) t\u22650 be a M-valued semimartingales. Then (B M t ) t\u22650 is a Brownian motion on M if and only on the following conditions hold: a) The horizontal lift (U t ) t\u22650 is a \u2206 OM /2 diffusion process, i.e. for any f \u2208 C \u221e (OM),\nwe have that (M f t ) t\u22650 is a local martingale where for any t \u2265 0\nM f t = f (U t ) \u2212 f (U 0 ) \u2212 1 2 t 0 \u2206 OM f (U s )ds. b) The stochastic antidevelopment of (B M t ) t\u22650 is a R d -valued Brownian motion (B t ) t\u22650 .\nIn particular the previous proposition provides us with an intrisic way to sample the Brownian motion on M with initial condition\nB M 0 . First sample (U t ) t\u22650 solution of SDE(H 1:d , B 1:d , U 0 ) with H 1:d = {H i } d\ni=1 and \u03c0(U 0 ) = B M 0 and B 1:d the Euclidean d-dimensional Brownian motion. Then, we recover the M-valued Brownian motion (B M t ) t\u22650 upon letting (B M t ) t\u22650 = (\u03c0(U t )) t\u22650 . We now consider an extrinsic approach to the sampling of Brownian motions on M. Using the Nash embedding theorem (Gunther, 1991), there exists p \u2208 N such that without loss of generality we can assume that M \u2282 R p . For any x \u2208 M, we denote P(x) : R p \u2192 T x M the projection operator. In addition for any x \u2208 M, we denote {P i (x)} p i=1 = {P(x)e i } p i=1 , where {e i } p i=1 is the canonical basis of R p . For any i \u2208 {1, . . . , p}, we smoothly extend P i to R p . In this case, we have the following proposition (Hsu, 2002, Theorem 3.1.4):\nProposition C.3 (Extrinsic view of Brownian motion): For any f \u2208 C \u221e (M) we have that \u2206 M (f ) = p i=1 P i (P i (f )). Hence, we have that (B M t ) t\u22650 solution of SDE({P i } p i=1 , B 1:p , B M 0 ) with B M\n0 a M-valued random variable and B 1:p a R p -valued Brownian motion.\nThe second part of this proposition, stems from the fact that any solution of SDE({V i } i=1 , B 1: , X 0 ), where X 0 is a M-valued random variable and B 1: a R -valued Brownian motion is a diffusion process with generator A such that for any\nf \u2208 C \u221e (M), A(f ) = i=1 V i (V i (f )).\nThe extrinsic approach is particularly convenient since the SDE appearing in lemma C.3 can be seen as an SDE on the Euclidean space R p .\nWe finish this paragraph, by investigating the behaviour of the Brownian motion in local coordinates.\nFor simplicity, we assume here that we have access to a system of global coordinates. In the case where the coordinates are strictly local then we refer to Ikeda and Watanabe (1989, Chapter 5, Theorem 1) for a construction of a global solution by patching local solutions. We denote {X k , X i,j } 1\u2264i,j,k\u2264d such that for any u \u2208 FM, {X k (u), X i,j (u)} 1\u2264i,j,k\u2264d is a basis of T u FM, Using properties of the horizontal lift, see (Hsu, 2002, Chapter 2), we get that (U t ) t\u22650 = ({X k t , E i,j t } 1\u2264i,j,k\u2264d ) obtained in lemma C.2 is given in the global coordinates for any i, j, k \u2208 {1, . . . , d} by\ndX k t = d j=1 E k,j t \u2022 dB k t , dE i,j t = \u2212 d n=1 { d ,m=1 E ,n t E m,j t \u0393 i ,m (X t )} \u2022 dB n t .\nBy definition of the Stratanovitch integral we have that for any k \u2208 {1, . . . , d}\ndX k t = d j=1 {E k,j t dB k t + 1 2 d[E k,j t , B j t ] t }. Let (M t ) t\u22650 = ({M k t } d k=1 ) t\u22650 such that for any t \u2265 0 and k \u2208 {1, . . . , d} M k t = d j=1 t 0 E k,j t dB k t .\nWe obtain that dM t = G(X t ) \u22121/2 dB t for some d-dimensional Brownian motion (B t ) t\u22650 , using L\u00e9vy's characterization of Brownian motion. In addition, we have that for any k, j \u2208 {1, . . . , d}\n[E k,j , B j ] t = \u2212 d ,m=1 t 0 E ,j t E m,j t \u0393 k\n,m (X t )dt Hence, using this result and the fact that d j=1 E ,j t E m,j t = g ,m (X t ), we get that for any k \u2208 {1, . . . , d}\ndX k t = \u2212 1 2 d ,m=1 g ,m (X t )\u0393 k ,m (X t )dt + (G(X t ) \u22121/2 dB t ) k .\nNote that this result could also have been obtained using the expression of the Laplace-Beltrami in local coordinates.\nBrownian motion and random walks In the previous paragraph we consider three SDEs to obtain a Brownian motion on M (stochastic development, isometric embedding and local coordinates).\nIn this section, we summarize results from J\u00f8rgensen (1975) establishing the limiting behaviour of Geodesic Random Walks (GRWs) when the stepsize of the random walk goes to 0. This will be of particular interest when considering the time-reversal process. We start by defining the geodesic random walk on M, following J\u00f8rgensen (1975, Section 2).\nLet\n{\u03bd x } x\u2208M such that for any x \u2208 M, \u03bd x : B(T x M) \u2192 [0, 1] with \u03bd x (T x M) = 1, i.e. for any x \u2208 M, \u03bd x is a probability measure on T x M. Assume that for any x \u2208 M, M v 3 d\u03bd x (v) < +\u221e.\nIn addition assume that there exists \u00b5 (1) \u2208 X (M) and \u00b5 (2) \u2208 X 2 (M), where X 2 (M) is the section\n\u0393(M, x\u2208M L(T x M)), such that for any x \u2208 M, M vd\u03bd x (v) = \u00b5 (1) (x) and M v \u2297 vd\u03bd x (v) = \u00b5 (2) (x).\nIn addition, we assume that for any\nx \u2208 M, \u03a3(x) = \u00b5 (2) (x) \u2212 \u00b5 (1) (x) \u2297 \u00b5 (1) (x)\nis strictly positive definite and that there exists L \u2265 such that for any x, y \u2208 M, \u03bd x \u2212 \u03bd y TV \u2264 Ld M (x, y).\nWhere we have that for any \u03bd 1 \u2208 P(T x M) and \u03bd 2 \u2208 P(T y M),\n\u03bd x \u2212 \u03bd y TV = sup{\u03bd 1 [f ] \u2212 \u0393 y x (\u03b3) # \u03bd 2 [f ] : \u03b3 \u2208 Geo x,y , f \u2208 C(T x M)}.\nNote that if d M (x, y) \u2264 \u03b5 then for some \u03b5 > 0 we have that |Geo x,y | = 1.\nDefinition C.4 (Geodesic random walk): Let X 0 be a M-valued random variable. For any \u03b3 > 0, we define (X \u03b3 t ) t\u22650 such that X \u03b3 0 = X 0 and for any n \u2208 N and\nt \u2208 [0, \u03b3], X n\u03b3+t = exp Xn\u03b3 [t\u03b3{\u00b5 n + (1/ \u221a \u03b3)(V n \u2212 \u00b5 n )}],\nwhere (V n ) n\u2208N is a sequence of random variables in such that for any n \u2208 N, V n has distribution \u03bd Xn\u03b3 conditionally to X n\u03b3 .\nFor any \u03b3 > 0, the process (X \u03b3 n ) n\u2208N = (X \u03b3 n\u03b3 ) n\u2208N is called a geodesic random walk. In particular, for any \u03b3 > 0 we denote (R \u03b3 n ) n\u2208N the sequence of Markov kernels such that for any n \u2208 N, x \u2208 M and A \u2208 B(M) we have that \u03b4 x R(A) = P(X \u03b3 n \u2208 A), with X \u03b3 0 = x. The following theorem establishes that the limiting dynamics of a geodesic random walk is associated with a diffusion process on M whose coefficients only depends on the properties of \u03bd (see J\u00f8rgensen, 1975, Theorem 2.1).\nTheorem C.5 (Convergence of geodesic random walks): For any t \u2265 0, f \u2208 C(M) and x \u2208 M we have that lim \u03b3\u21920 R\nt/\u03b3 \u03b3 [f ] \u2212 P t [f ] \u221e = 0, where (P t ) t\u22650 is the semi-group associated with the infinitesimal generator A : C \u221e (M) \u2192 C \u221e (M) given for any f \u2208 C \u221e (M) by A(f ) = \u00b5 (1) , \u2207f M + 1 2 \u03a3, \u2207 2 f M .\nIn particular if \u00b5 (1) = 0 and \u00b5 (2) = Id then the random walk converges towards a Brownian motion on M in the sense of the convergence of semi-groups. For any x \u2208 M in local coordinates we have that \u03a6 # \u03bd x has zero mean and covariance matrix G(x), where \u03a6 is a local chart around x and G(x) = (g i,j (x)) 1\u2264i,j\u2264d the coordinates of the metric in that chart.", "publication_ref": ["b41", "b47", "b53", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Convergence of Brownian motion", "text": "We finish this section with a few considerations regarding the convergence of the Brownian motion on M. Since we have assumed that M is compact we have that there exist (\u03a6 k ) k\u2208N an orthonormal basis of \u2212\u2206 M in L 2 (p ref ), (\u03bb k ) k\u2208N such that for any i, j \u2208 N, i \u2264 j, \u03bb i \u2264 \u03bb j and \u03bb 0 = 0, \u03a6 0 = 1 and for any k Proposition C.6 (Convergence of Brownian motion): For any t > 0, P t admits a density p t|0 w.r.t p ref and p ref P t = p ref , i.e. p ref is an invariant measure for (P t ) t\u22650 . In addition, if there exists C, \u03b1 \u2265 0 such that for any t \u2208 (0, 1], p t|0 (x|x) \u2264 Ct \u2212\u03b1/2 then for any p 0 \u2208 P(M) and for any t \u2265 1/2 we have\n\u2208 N, \u2206 M \u03a6 k = \u2212\u03bb k \u03a6 k . For any t \u2265 0 and x, y \u2208 M, p t|0 (y|x) = k\u2208N e \u2212\u03bb k t \u03a6 k (x)\u03a6 k (y)\np 0 P t \u2212 p ref TV \u2264 C 1/2 e \u03bb1/2 e \u2212\u03bb1t ,\nwhere \u03bb 1 is the first non-negative eigenvalue of \u2212\u2206 M in L 2 (p ref ) and we recall that (P t ) t\u22650 is the semi-group of the Brownian motion.\nA review on lower bounds on the first positive eigenvalue of the Laplace-Beltrami operator can be found in (He, 2013). These lower bounds usually depend on the Ricci curvature of the manifold or its diameter. We conclude this section by noting that in the non-compact case (Li, 1986) establishes similar estimates in the case of a manifold with non-negative Ricci curvature and maximal volume growth.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "D Likelihood computation D.1 ODE likelihood computation", "text": "Similarly to Song et al. (2021), once the score is learned we can use it in conjunction with an Ordinary Differential Equation (ODE) solver to compute the likelihood of the model. Let (\u03a6 t ) t\u2208[0,T ] be a family of vector fields. We define (X t ) t\u2208[0,T ] such that X 0 has distribution p 0 (the data distribution) and satisfying dX t = \u03a6 t (X t )dt. Assuming that p 0 admits a density w.r.t. p ref then for any t \u2208 [0, T ], the distribution of X t admits a density w.r.t. p ref and we denote p t this density. We recall that d log p t (X t ) = \u2212div(\u03a6 t )(X t )dt, see Mathieu and Nickel (2020, Proposition 2) for instance.\nRecall that we consider a Brownian motion on the manifold as a forward process (B M t ) t\u2208[0,T ] with {p t } t\u2208[0,T ] the associated family of densities. Thus we have that for any t \u2208 [0, T ] and x \u2208 M\n\u2202 t p t (x) = 1 2 \u2206 M p t (x) = div 1 2 p t \u2207 log p t (x).\nHence, we can define (X t ) t\u2208[0,T ] satisfying dX t = \u2212 1 2 \u2207 log p t (X t )dt such that X 0 has distribution p 0 . Defining (X t ) t\u2208[0,T ] = (X T \u2212t ) t\u2208[0,T ] , it follows thatX 0 has distribution L(X T ) and satisfies\ndX t = 1 2 \u2207 log p T \u2212t (X t )dt. (3\n)\nFinally, we introduce (Y t ) t\u2208[0,T ] satisfying (3) but such that Y 0 \u223c p ref .\nNote that if T \u2265 0 is large then the two processes (Y t ) t\u2208[0,T ] and (X t ) t\u2208[0,T ] are close since L(X T ) is close to p ref .\nTherefore, using the score network and a manifold ODE solver (as in Mathieu and Nickel, 2020), we are able to approximately solve the following ODE\nd log q t (X \u03b8 t ) = \u2212 1 2 div(s \u03b8 (T \u2212 t, \u2022))(X \u03b8 t )dt,\nwith q t the density of Y \u03b8 t w.r.t. p ref and log q 0 (Y 0 ) = 0 with\ndY \u03b8 t = 1 2 div(s \u03b8 (T \u2212 t, Y \u03b8 t ))dt and Y \u03b8 0 \u223c p ref .\nThe likelihood approximation of the model is then given by E[log q T (X\n\u03b8 T )] = M log q T (x)dp data (x), where (X \u03b8 t ) t\u2208[0,T ] = (X \u03b8 T \u2212t ) t\u2208[0,T ] with dX \u03b8 t = \u2212 1 2 div(s \u03b8 (t, X \u03b8 t )\n)dt and X 0 \u223c p data . In App. D.2, we highlight that this is not the likelihood of the SDE model.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Difference between ODE and SDE likelihood computations", "text": "In this section, we show that the likelihood computation from Song et al. (2021) does not coincide with the likelihood computation obtained with the SDE model. We present our findings in the Riemannian setting but our results can be adapted to the Euclidean setting with arbitrary forward dynamics. Recall that we consider a Brownian motion on the manifold as a forward process (B M t ) t\u2208[0,T ] with (p t ) t\u2208[0,T ] the associated family of densities. We have that for any t \u2208 [0, T ] and x \u2208 M\n\u2202 t p t (x) = 1 2 \u2206 M p t|0 (x) = div( 1 2 p t \u2207 log p t )(x).(4)\nODE model. In the case of the ODE model, we define (X t ) t\u2208[0,T ] such that X 0 \u223c p 0 and satisfies dX t = \u2212 1 2 \u2207 log p t (X t )dt. The family of densities (q t ) t\u2208[0,T ] associated with (X t ) t\u2208[0,T ] also satisfies (4). Now consider (X t ) t\u2208[0,T ] = (X T \u2212t ) t\u2208[0,T ] , this satisfiesX 0 \u223c p T with dX t = 1 2 \u2207 log p T \u2212t (X t )dt.\n(5)\nFinally, we consider (Y ODE t\n) t\u2208[0,T ] which also satisfies Eq. ( 5) and such that \nY ODE 0 \u223c p ref . Denoting (q ODE t ) t\u2208[0,T ] the densities of (Y ODE t ) t\u2208[0,T ] w.r.t. p ref we have for any t \u2208 [0, T ] and x \u2208 M \u2202 t q ODE t (x) = \u2212div( 1 2 q ODE t \u2207 log p T \u2212t )(x).(6\n\u2202 t q SDE t (x) = \u2212div(\u2207 log p T \u2212t q SDE t (x)) + 1 2 \u2206 M q SDE t (x) = \u2212div(q SDE t {\u2207 log p T \u2212t \u2212 1 2 \u2207 log q SDE t })(x).(7)\nHence, Eq. (6) and Eq. ( 7 have the same distribution as X T . Note that it is possible to evaluate the likelihood of the SDE model using that\n\u2202 t log q SDE t (Y SDE t ) = \u2207 log p T \u2212t (Y SDE t ) \u2212 1 2 \u2207 log q SDE t (Y SDE t ) dt.\nWe can use the score approximation s \u03b8 (t, x) to approximate \u2207 log p t (x) for any t \u2208 [0, T ] and x \u2208 M. In order to approximate \u2207 log q SDE t , one can consider another neural network t \u03b8 (t, x) approximating \u2207 log q SDE t (x) for any t \u2208 [0, T ] and x \u2208 M. This approximation can be obtained using the implicit score loss presented in Sec. 3.4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E Parametric family of vector fields", "text": "We approximate (\u2207 log p t ) t\u2208[0,T ] by a family of functions {s \u03b8 } \u03b8\u2208\u0398 where \u0398 is a set of parameters and for any \u03b8 \u2208 \u0398, s \u03b8 : [0, T ] \u2192 X (M). In this work, we consider several parameterisations of vector fields:\n\u2022 Projected vector field. We define s \u03b8 (t, x) = proj TxM (s \u03b8 (t, x)) = P (x)s \u03b8 (t, x) for any t \u2208 [0, T ] and x \u2208 M, withs \u03b8 : R p \u00d7 [0, T ] \u2192 R p an ambient vector field and P (x) the orthogonal projection over T x M at x \u2208 M . According to Rozen et al. (2021, Lemma 2), then div(s \u03b8 )(x, t) = div E (s \u03b8 )(x, t) for any x \u2208 M, where div E denotes the standard Euclidean divergence.\n\u2022 Divergence-free vector fields: For any Lie group G, any basis of the Lie algebra g = T e G yields a global frame. Indeed, let v \u2208 g and define the flow \u03a6 : R \u00d7 M \u2192 M given for any t \u2208 R and x \u2208 M by \u03a6 v t (x) = x exp e (tv). Then defining\n{E i } d i=1 = {\u2202 t \u03a6 vi 0 } d i=1 , where {v i } d i=1 is a basis of g, we get that {E i } d\ni=1 is a left-invariant global frame. As a result, we have that for any i \u2208 {1, . . . , d}, div(E i ) = 0 (for the classical left invariant metric). This result simplifies the computation of div(s \u03b8 ) where Falorsi and Forr\u00e9, 2020). Note that this approach can be extended to any homogeneous space (G, H).\ns \u03b8 (t, x) = d i=1 s i \u03b8 (t, x)E i (x) for any t \u2208 [0, T ] and x \u2208 M since we have that div(s \u03b8 )(t, x) = d i=1 E i (s i \u03b8 )(t, x) + d i=1 s i \u03b8 (t, x)div(E i )(x) = d i=1 ds i \u03b8 (E i )(t, x) (see\n\u2022 Coordinates vector fields. We define s \u03b8 (t,\nx) = d i=1 s i \u03b8 (t, x)E i (x) for any t \u2208 [0, T ] and x \u2208 M, with {E i } d i=1 = {\u2202 i \u03d5(\u03d5 \u22121 (x))} d i=1\nthe vector fields induced by a choice of local coordinates, where \u03d5 is a local parameterization \u03d5 : U \u2192 M and z \u2208 U \u2282 R d . Then the divergence can be computed in these local coordinates div(s \u03b8 )(t, \u03d5(z))\n= | det G| \u22121/2 d i=1 \u2202 i {| det G| 1/2 s i \u03b8 (t, \u03d5(\u2022))}(z).\nIn the case of the sphere, one recovers the standard divergence in spherical coordinates using this formula. Note that {E i } d i=1 does not span the tangent bundle except if the manifold is parallelizable. The sphere is a well-known example of non-parallelizable manifold, as per the hairy ball theorem. ", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "F Eigensystems of the Laplace-Beltrami operator and heat kernels", "text": "In this section, we recall the eigenfunctions and eigenvalues of the Laplace-Beltrami operator in two specific cases: the d-dimensional torus and the d-dimensional sphere. We also highlight that the heat kernel on compact manifold can be written as an infinite series using the Sturm-Liouville decomposition.\nThe case of the torus Let {b i } d i=1 be a basis of R d . We consider the associated lattice on R\nd , i.e. \u0393 = { d i=1 \u03b1 i b i : {\u03b1 i } d i=1 \u2208 Z d }. Finally, the associated d-dimensional torus is defined as T \u0393 = R d /\u0393. Denote B = (b 1 , . . . , b d ) \u2208 R d\u00d7d . Let {b i } d i=1 \u2208 (R d ) d such that (B \u22121 ) = (b 1 , . . . ,b d ). We define \u0393 = { d i=1 \u03b1 ibi : {\u03b1 i } d i=1 \u2208 Z d },\nthe dual lattice. Note that for any x \u2208 \u0393 and y \u2208 \u0393 we have that x, y \u2208 Z and that if {b i } d i=1 is an orthonormal basis then \u0393 = \u0393 . The torus R d /\u0393 is a (flat) compact Riemannian manifold. The set of eigenvalues of the Laplace-Beltrami operator is given by {\u22124\u03c0 2 y 2 : y \u2208 \u0393 }. The eigenfunctions of the Laplace-Beltrami operator are given by {x \u2192 sin(2\u03c0 x, y ) : y \u2208 \u0393 } and {x \u2192 cos(2\u03c0 x, y ) : y \u2208 \u0393 }.\nThe case of the sphere Next, we investigate the case of the d-dimensional sphere (see Saloff-Coste, 1994). The set of eigenvalues of the Laplace-Beltrami operator is given by\n{\u2212k(k + d \u2212 1) : k \u2208 N}. Note that \u03bb k = k(k + d \u2212 1) has multiplicity d k = (k + d \u2212 2)!/{(d \u2212 1)!k}(2k + d \u2212 1).\nThe eigenfunctions of the Laplace-Beltrami operator are known as the spherical harmonics and can be defined in terms of Legendre polynomials. When investigating the heat kernel on the ddimensional sphere, we are interested in the product (x, y) \u2192 \u03c6\u2208\u03a6n \u03c6(x)\u03c6(y), where \u03a6 n is the set of eigenfunctions associated with the eigenvalue \u03bb n for n \u2208 N. This function can be described using the Gegenbauer polynomials (see Atkinson and Han, 2012, Theorem 2.9). More precisely, we have that for any n \u2208 N and x, y\n\u2208 S d G n (x, y) = \u03c6\u2208\u03a6n \u03c6(x)\u03c6(y) = n!\u0393((d \u2212 1)/2) n/2 k=0 (\u22121) k (1 \u2212 x, y 2 ) x, y n\u22122k /(4 k k!(n \u2212 2k)!\u0393(k + (d \u2212 1)/2)),\nwhere here \u0393 : R + \u2192 R is given for any v > 0 by \u0393(v) = +\u221e 0 t v\u22121 e \u2212t dt. In the special case where d = 1, then the heat kernel coincide with the wrapped Gaussian density and can be easily evaluated.\nHeat kernel on compact Riemannian manifolds. We recall that in the case of compact manifolds the heat kernel is given by the Sturm-Liouville decomposition (Chavel, 1984) given for any t > 0 and x, y \u2208 M by p t|0 (y|x) = j\u2208N e \u2212\u03bbj t \u03c6 j (x)\u03c6 j (y),\nwhere the convergence occurs in L 2 (p ref \u2297p ref ), (\u03bb j ) j\u2208N and (\u03c6 j ) j\u2208N are the eigenvalues, respectively the eigenvectors, of \u2212\u2206 M in L 2 (p ref ) (see Saloff-Coste, 1994, Section 2). When the eigenvalues and eigenvectors are known, we approximate the logarithmic gradient of p t|0 by truncating the sum in (8) with J \u2208 N terms. Another possibility to approximate \u2207 log p t|0 is to rely on the so-called Varadhan approximation, see Sec. 3.4, which is valid for small t > 0 . Fig. 1 illustrates these different approximations of the heat kernel and Table 1 compares the different loss functions.  7)\n1 2 E s(Xt) \u2212 SJ,t(X0, Xt) 2 (J \u2192 \u221e) 0 Varhadan (8) 1 2 E s(Xt) \u2212 log X t (X0)/t 2 (t \u2192 0) 0 t|s (DSM) Varhadan (8) 1 2 E s(Xt) \u2212 log X t (Xs)/(t \u2212 s) 2 (t \u2192 s) 0 im t (ISM) Deterministic E 1 2 s(Xt) 2 + div(s)(Xt) 0 Stochastic E 1 2 s(Xt) 2 + \u03b5 \u2202s(Xt)\u03b5 2 \u2202s F G Predictor-corrector schemes\nIn this section, we present a predictor-corrector scheme, adapting the techniques of Allgower and Georg (2012) and Song et al. (2021) to the manifold setting. Changes between Algorithm 1, Algorithm 2 and Algorithm 3, Algorithm 4 are highlighted in red. Let t \u2208 [0, T ], \u03b3 > 0 and k = t/\u03b3 . We remark that Algorithm 3 corresponds to the recursion associated with (Y j t ) j\u2208N such that for any j \u2208 N\nY j+1 t = exp Y j t [ \u03b3 2 \u2207 log p T \u2212j\u03b3 (Y j t ) + \u221a \u03b3Z j+1 ],\nwhere {Z j } j\u2208N is a family of i.i.d Gaussian random variables with zero mean and identity covariances matrix in R p and for any j \u2208 N, Z j = P(Y j t )Z j . Note that here k \u2208 {0, N \u2212 1} is fixed. Letting \u03b3 \u2192 0, we obtain that under mild assumptions, see (Kuwada, 2012, Theorem 3\n.1), (Y j t ) j\u2208N converges to (Y s t ) s\u22650 such that dY s t = 1 2 \u2207 log p T \u2212t (Y s t )ds + dB M s .\nWe have that p T \u2212t is the invariant measure of (Y s t ) s\u22650 . Hence, the role of the corrector step is to project the distribution back onto p T \u2212t for all times t \u2208 [0, T ], see Fig. 2. Standard Gaussian noise in ambient space R p 5:\np data = p 0 L(Y 0 ) = p T L(Y 0 \u03b3 ) L(Y 1 \u03b3 ) \u2022 \u2022 \u2022 p \u03b3 L(Y 0 2\u03b3 ) L(Y 1 2\u03b3 ) p 2\u03b3\nZ k+1 = P(Y k )Z k+1\nProjection in the tangent space TxM 6:\nY k+1 = Y k + \u03b3 \u2212b(T \u2212 k\u03b3, Y k ) + \u03c3(T \u2212 k\u03b3) 2 \u2207 log p T \u2212k\u03b3 (Y k ) + \u221a \u03b3\u03c3(T \u2212 k\u03b3)Z k+1 E-M step 7:\n/// CORRECTOR STEP 8:\nY 0 k+1 = Y k+1 9:\nfor s \u2208 {0, . . . , S \u2212 1} do 10:Z s k+1 \u223c N(0, Id) Standard Gaussian noise in ambient space R p 11:\nZ s k+1 = P(Y s k+1 )Z s k+1\nProjection in the tangent space TxM 12:\nY s+1 k+1 = Y s k+1 + \u03b3s 1 2 \u2207 log p T \u2212k\u03b3 (Y s k+1 ) + \u221a \u03b3sZ s k+1 Langevin step 13: Y k+1 = Y S k+1 14: return {Y k } N k=0\nAlgorithm 4 RSGM-c (Riemannian Score-Based Generative Model with corrector)\nRequire: \u03b5, T, N, {X m 0 } M m=1 , loss, s, \u03b80, Niter, pref, P 1: /// TRAINING /// 2: for n \u2208 {0, . . . , Niter \u2212 1} do 3: X0 \u223c (1/M ) M m=1 \u03b4 X m 0 Random mini-batch from dataset 4: t \u223c U ([\u03b5, T ])\nUniform sampling between \u03b5 and T 5:\nXt = GRW(t, N, X0, 0, Id, P) Approximate forward diffusion with Algorithm 1 6:\n(\u03b8n) = t(T , N, X0, Xt, loss, s \u03b8n ) Compute score matching loss from Table 2  7 Theorem 5.7) to obtain our results. Note that the time-reversal on manifold could also be obtained by readily extending arguments from Haussmann and Pardoux (1986), however the entropic conditions found by Cattiaux et al. (2021) are more natural when it comes to the study of the Schr\u00f6dinger Bridge problem. For the interested reader we provide an informal derivation of the time-reversal formula obtained by Haussmann and Pardoux (1986) in App. H.1. The proof of Theorem 3.1 is given in App. H.2. Finally, we emphasize that Garc\u00eda-Zelada and Huguet (2021) have developed a Girsanov theory for stochastic processes defined on compact manifolds with boundary in order to study the Brenier-Schr\u00f6dinger problem.", "publication_ref": ["b15", "b2", "b19", "b42", "b14", "b42"], "figure_ref": ["fig_11", "fig_1"], "table_ref": ["tab_1", "tab_2"]}, {"heading": "H.1 Informal derivation", "text": "In this section, we provide a non-rigorous derivation of Theorem 3.1 following the approach of Haussmann and Pardoux (1986). Let (X t ) t\u2208[0,T ] be a continuous process such that for any\nf \u2208 C 2 (M) we have that (M X,f t ) t\u2208[0,T ] is a X-martingale where for any t \u2208 [0, T ] M X,f t = f (X t ) \u2212 t 0 { b(X s ), \u2207f (X s ) + 1 2 \u2206 M f (X s )}ds. (9) Let (Y t ) t\u2208[0,T ] = (X T \u2212t ) t\u2208[0,T ] . Our goal is to show that for any f \u2208 C 2 (M), (M Y,f t ) t\u2208[0,T ] is a Y-martingale where for any t \u2208 [0, T ] M Y,f t = f (Y t ) \u2212 t 0 { \u2212b(Y s ) + \u2207 log p T \u2212s (Y s ), \u2207f (Y s ) + 1 2 \u2206 M f (Y s )}ds.\nNote that here we implicitly assume that for any t \u2208 [0, T ], X t admits a smooth positive density w.r.t. p ref denoted p t . In other words, we want to show that for any g \u2208 C 2 (M) and s, t \u2208 [0, T ] with t \u2265 s we have\nE[g(Y s )(f (Y t ) \u2212 f (Y s ))] (10) = E[g(Y s ) t s { \u2212b(Y u ) + \u2207 log p T \u2212u (Y u ), \u2207f (Y u ) + 1 2 \u2206 M f (Y u )}du].", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "We introduce the infinitesimal generator", "text": "A : C 2 (M) \u2192 C(M) given for any f \u2208 C 2 (M) and x \u2208 M by A(f )(x) = b(x), \u2207f (x) + 1 2 \u2206 M f (x)\n. Similarly, we introduce the infinitesimal generator\u00c3 :\n[0, T ] \u00d7 C 2 (M) \u2192 C(M) given for any f \u2208 C 2 (M), t \u2208 [0, T ] and x \u2208 M b\u1ef9 A(t, f )(x) = \u2212b(x) + \u2207 log p T \u2212t (x), \u2207f (x) + 1 2 \u2206 M f (x)\n. With these notations, (11) can be written as follows: we want to show that for any g \u2208 C 2 (M) and s, t \u2208 [0, T ] with t \u2265 s we have\nE[g(Y s )(f (Y t ) \u2212 f (Y s ))] = E[g(Y s ) t s\u00c3 (u, Y u )du].\n(11) The rest of this section follows the first part of the proof of Haussmann and Pardoux (1986, Theorem 2.1). Let t, s \u2208 [0, T ] with t \u2265 s. We have\nE[g(Y s )(f (Y t ) \u2212 f (Y s ))] = E[g(X T \u2212s )(f (X T \u2212t ) \u2212 f (X T \u2212t ))] = E[E[g(X T \u2212s )|X T \u2212t ]f (X T \u2212t )] \u2212 E[g(X T \u2212s )f (X T \u2212s )] = E[v(T \u2212 t, X T \u2212t )f (X T \u2212t )] \u2212 E[v(T \u2212 s, X T \u2212s )f (X T \u2212s )],(12)\nwith v : [0, T \u2212 s] \u00d7 M \u2192 R given for any u \u2208 [0, T \u2212 s] and x \u2208 M by v(u, x) = E[g(X T \u2212s )|X u = x].\nWe have that v satisfies the backward Kolmogorov equation, i.e. we have for any\nu \u2208 [0, T \u2212 s] and x \u2208 M \u2202 u v(u, x) = \u2212Av(u, x).(13\n) Note that it is not trivial to show that v is regular enough to satisfy the backward Kolmogorov equation. In this informal derivation, we assume that v is regular enough and will provide a different rigorous proof of the time-reversal formula in App. H.2. However, note that it is possible to show that v indeed satisfies the backward Kolmogorov equation by adapting arguments from Haussmann and Pardoux (1986) to the manifold framework. \n\u2202 u h(u, x) + Ah(u, x) = f (x)\u2202 u v(u, x) + f (x)Av(u, x) + v(u, x)Af (x) + \u2207f (x), \u2207v(u, x) = v(u, x)Af (x) + \u2207f (x), \u2207v(u, x) . (14\n)\nIn addition, using the divergence theorem (see Lee, 2018, p.51), we have for any\nu \u2208 [0, T \u2212 s] E[ \u2207f (X u ), \u2207v(u, X u ) ] = M \u2207f (x u ), \u2207v(u, x u )p u (x u ) dp ref (x u ) = \u2212 M v(u, x u )div(p u \u2207f )(x u )dp ref (x u ) = \u2212 M v(u, x u )\u2206 M f (x u )p u (x u )dp ref (x u ) \u2212 M v(u, x u ) \u2207f (x u ), \u2207 log p u (x u ) p u (x u )dp ref (x u ) = \u2212E[v(u, X u )\u2206 M f (X u )] \u2212 E[v(u, X u ) \u2207f (X u ), \u2207 log p u (X u ) ]\n. Therefore, using this result and ( 14) we get that for any\nu \u2208 [0, T \u2212 s] E[\u2202 u h(u, X u ) + Ah(u, X u )] = E[v(u, X u ){ b(X u ) \u2212 \u2207 log p u (X u ), \u2207f (X u ) \u2212 1 2 \u2206 M f (X u )}] = \u2212E[v(u, X u )\u00c3(T \u2212 u, f )(X u )].\nCombining this result and (9) and that for any\nu \u2208 [0, T \u2212 s] and x \u2208 M, v(u, x) = E[g(X T \u2212s )|X u = x] we get E[v(T \u2212 t, X T \u2212t )f (X T \u2212t )] \u2212 E[v(T \u2212 s, X T \u2212s )f (X T \u2212s )] = E[h(T \u2212 t, X T \u2212t ) \u2212 h(T \u2212 s, X T \u2212s )] = T \u2212s T \u2212t E[v(u, X u )\u00c3(T \u2212 u, X u )]du = E[g(X T \u2212s ) T \u2212s T \u2212t\u00c3 (T \u2212 u, X u )du].\nUsing this result, ( 12) and the change of variable u \u2192 T \u2212 u we obtain\nE[g(Y s )(f (Y t ) \u2212 f (Y s ))] = E[g(X T \u2212s ) T \u2212s T \u2212t\u00c3 (T \u2212 u, X u )du] = E[g(Y s ) t s\u00c3 (u, Y u )du].\nHence, (10) holds and we have proved Theorem 3.1. Again, we emphasize that in order to make the proof completely rigourous one needs to derive regularity properties of v.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "H.2 Proof of Theorem 3.1", "text": "In this section, we follow another approach to prove the time-reversal formula. We are going to use the integration by part formula of Cattiaux et al. (2021, Theorem 3.17) in a similar spirit as Cattiaux et al. (2021, Theorem 4.9) in the Euclidean setting. In order to adapt arguments from Cattiaux et al. (2021) to our Riemannian setting, we use the Nash embedding theorem in order to embed our processes in a Euclidean space and leverage tools from Girsanov theory. The rest of the section is organized as follows. First in App. H.2.1, we recall basic properties of infinitesimal generators and recall the integration by part formula of Cattiaux et al. (2021, Theorem 3.17). Then in App. H.2.2, we extend some Girsanov theory to compact Riemannian manifolds using the Nash embedding theorem. We conclude the proof in App. H.2.3.", "publication_ref": ["b14"], "figure_ref": [], "table_ref": []}, {"heading": "H.2.1 Diffusion processes and integration by part formula", "text": "In this section, we state a simplified version of Cattiaux et al. (2021, Theorem 3.17) for Markov continuous path (probability) measure on Polish spaces. Let (X, X ) be a Polish space. We say that P is a path measure if P \u2208 P(C([0, T ] , X)). Let (X t ) t\u2208[0,T ] with distribution P. We denote (F t ) t\u2208[0,T ] the filtration such that for any t \u2208\n[0, T ], F t = \u03c3(X s , s \u2208 [0, t]). Let (M t ) t\u2208[0,T ] be a Polish-valued stochastic process. We say that (M t ) t\u2208[0,T ] is a P-local martingale if it is a local martingale w.r.t. the filtration (F t ) t\u2208[0,T ] . A function u : [0, T ] \u00d7 X \u2192 R is\nsaid to be in the domain of the extended generator of P if there exists a process (\u0100 P u(t, X [0,t] )) t\u2208[0,T ] such that: \nM t = u(t, X t ) \u2212 u(0, X 0 ) \u2212 t 0\u0100 P u(s, X [0,s] )ds.\nThe domain of the extended generator is denoted dom(\u0100 P ). We say that (u, v) with u, v : [0, T ] \u00d7 X \u2192 R is in the domain of the carr\u00e9 du champ if u, v, uv \u2208 dom(\u0100 P ). In this case, we define the carr\u00e9 du champ\u1fe9 P as\u1fe9\nP (u, v) =\u0100 P (uv) \u2212\u0100 P (u)v \u2212\u0100 P (v)u. Note that if X = M is a Riemannian manifold, C 2 (M) \u2282 dom(\u0100 P ) and for any u \u2208 C 2 (M) A P (u) = \u2207u, X + 1 2 \u2206 M u with X \u2208 \u0393(TM) then we have that C 2 (M) \u00d7 C 2 (M) \u2282 dom(\u1fe9 P ) and for any u, v \u2208 C 2 (M),\u1fe9 P (u, v) = \u2207u, \u2207v . Assume that there exists U P \u2282 dom(\u0100 P )\u2229C b (X)\nsuch that U P is an algebra. We denote U P,2 such that U P,2 = {u \u2208 U P :\u0100 P u \u2208 L 2 (P),\u1fe9 P (u, u) \u2208 L 1 (P)}.\nFinally we denote R(P) the time-reverse path measure, i.e. for any A \u2208 B(C([0, T ] , X)) we have R(P)(A) = P(R(A)), where R(A) = {t \u2192 \u03c9 T \u2212t : \u03c9 \u2208 A}. In what follows, we assume P is Markov. It is well-known, see (L\u00e9onard et al., 2014, Theorem 1.2) for instance, that in this case R(P) is also Markov. In addition, since P is Markov, for any u \u2208 dom(\u0100 P ) and t \u2208 [0, T ] there exists A P such that\u0100 P u(t, X [0,t] ) = A P u(t, X t ) with A P u : [0, T ] \u00d7 X \u2192 R. Similarly, we define\n\u03a5 P (u, v) : [0, T ] \u00d7 X \u2192 R from\u1fe9 P (u, v).\nWe are now ready to state the integration by part formula, (Cattiaux et al., 2021, Theorem 3.17).\nTheorem H.1: Let u, v \u2208 U P,2 . The following hold:\n(a) If u \u2208 dom(A R(P) ) and A R(P) u \u2208 L 1 (P) then for almost any t \u2208 [0, T ] E[{A P u(t, X t ) + A R(P) u(T \u2212 t, X t )}v(X t ) + \u03a5 P (u, u)(t, X t )] = 0.\n(b) If the following hold:\ni) \u03a5 P (u, v) \u2208 C([0, T ] \u00d7 X, R).\nii) U 2,P determines the weak convergence of Borel measures.\niii) \u00b5 defines a finite measure on [0, T ] \u00d7 X where for any \u03c9 \u2208\u016a 2,P we have\n\u00b5[\u03c9] = E[ T 0 \u03a5 P (u, \u03c9 t )(t, X t )dt], where\u016a 2,P = {\u03c9 \u2208 C([0, T ] \u00d7 X, R) : \u03c9(t, \u2022) \u2208 U 2,P for any t \u2208 [0, T ]}.\nThen u \u2208 dom(A R(P) ) and A R(P) u \u2208 L 1 (P).\nNote that this theorem is a simplified version of Cattiaux et al. (2021, Theorem 3.17) where we restrict ourselves to the case of Markov path measures. In what follows, we wish to apply Theorem H.1 to diffusion processes on manifolds. To do so, we will verify that under a finite entropy assumption, the conditions u \u2208 dom(A R(P) ) and A R(P) u \u2208 L 1 (P) are fullfilled for a class of regular functions u. These integrability results are obtained using Girsanov theory.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H.2.2 Girsanov theory on compact Riemannian manifolds", "text": "In this section, we will consider two types of martingale problems: one on Euclidean spaces and one on the compact Riemannian manifold M. Let P \u2208 P(C([0, T ] , R p )). We say that P satisfies the (Euclidean) martingale problem with infinitesimal generator A :\n[0, T ] \u00d7 C 2 (R p ) \u00d7 R p \u2192 R if for any u \u2208 C 2 c (R p ), (M t ) t\u2208[0,T ] is a P-martingale where for any t \u2208 [0, T ] we have M t = M 0 + t 0 A(t, u)(X s )\nds, where (X t ) t\u2208[0,T ] has distribution P and T 0 |A(t, u)(X s )dt| < +\u221e, P-a.s. Let P \u2208 P(C([0, T ] , M)). We say that P satisfies the (Riemannian) martingale problem with infinitesimal generator\u00c3 :\n[0, T ] \u00d7 C 2 (M) \u00d7 M \u2192 R if for any u \u2208 C 2 (M), (M t ) t\u2208[0,T ] is a P-martingale\nwhere for any t \u2208 [0, T ] we have\nM t = M 0 + t 0\u00c3 (t, u)(X s )ds,\nwhere (X t ) t\u2208[0,T ] has distribution P and T 0 |\u00c3(t, u)(X s )dt| < +\u221e, P-a.s. We now prove the following theorem. Proposition H.2: Let Q be the path measure of a Brownian motion on M. Let P be a Markov path measure on C([0, T ] , M) such that KL (P|Q) < +\u221e. Then there exists \u03b2 such that for any t \u2208 [0, T ] and x \u2208 M, \u03b2(t, x) \u2208 T x M and we have that P satisfies the martingale problem with infinitesimal generator A where for any t \u2208 [0, T ], u \u2208 C 2 (M) and x \u2208 M we have\nA(t, u)(x) = \u03b2(t, x), \u2207u(x) + 1 2 \u2206 M u(x).\nIn addition, we have that\nKL (P|Q) = KL (P 0 |Q 0 ) + 1 2 T 0 E[ \u03b2(t, X t ) 2 ]dt,\nwhere (X t ) t\u2208[0,T ] has distribution P.\nProof: First, we extend (B M t ) t\u2208[0,T ] to R p using the Nash embedding theorem (see Gunther, 1991). (B M t ) t\u2208[0,T ] can be seen as a process on R p (for some p \u2208 N) which satisfies in a weak sense\ndB M t = p i=1 P i (B M t ) \u2022 dB i t = P (B M t )\n\u2022 dB t , where (B t ) t\u2208[0,T ] is a p-dimensional Brownian motion and P \u2208 C \u221e (R p , R p\u00d7p ) is such that for any x \u2208 M, P(x) is the projection onto T x M and for any i \u2208 {1, . . . , p}, P i \u2208 C \u221e (R p , R p ) with P i = Pe i where {e j } p j=1 is the canonical basis of R p . We refer to App. C.1 for more details on the projection operator and its extension to R p . Using the link between Stratanovitch and It\u00f4 integral, there existsb \u2208 C \u221e (R p , R p ) such that (B M t ) t\u2208[0,T ] can be seen as a process on R p which satisfies in a weak sense dB M t =b(B M t )dt + P(B M t )dB t , whereb is given in (2) and satisfies Pb(x) = 0 for any x \u2208 M, see the remark following (2). For any u, v \u2208 C 2 c (M), we consider\u016b,v extensions to C 2 c (R p ) and we have for any s, t \u2208 [0, T ]\nE[v(B M s ) t s 1 2 \u2206 M u(B M u )du] = E[v(B M s ) t s { \u2207\u016b(B M w ),b(B M w ) + 1 2 P(B M w ), \u2207 2\u016b (B M w ) }dw].\nIn particular, we get that for any\nx \u2208 M, \u2206 M u(x) = 2 \u2207\u016b(x),b(x) + P(x), \u2207 2\u016b (x) . Note that (B M t ) t\u2208[0,T ]\n(seen as a process on R p ) satisfies the condition (U) in L\u00e9onard (2012b), i.e. uniqueness of the trajectories given an initial condition. Therefore applying (L\u00e9onard, 2012b, Theorem 2.1), (Cattiaux et al., 2021, Claim 4.5), there exists\u03b2 :\n[0, T ] \u00d7 R p \u2192 R p such that KL (P|Q) = KL (P 0 |Q 0 ) + 1 2 T 0 E[ P(X t )\u03b2(t, X t ) 2 ]dt.(15)\nIn addition, P (seen as a process on R p ) satisfies a martingale problem with infinitesimal generator\nA : [0, T ] \u00d7 C 2 c (R p ) \u00d7 R p \u2192 R such that for any t \u2208 [0, T ],\u016b \u2208 C 2 c (R p ) and x \u2208 R p A(t,\u016b)(x) = b (x) + P(x)\u03b2(t, x), \u2207\u016b(x) + 1 2 P(x), \u2207 2\u016b (x) . Let \u03b2 : [0, T ] \u00d7 M such that for any t \u2208 [0, T ] and x \u2208 M we have \u03b2(t, x) = P(x)\u03b2(t, x).\nIn particular, we have that for any\nx \u2208 M, \u03b2(t, x) \u2208 T x M. Let u \u2208 C 2 c (M)\nand consider an extension\u016b to C 2 c (R p ). For any t \u2208 [0, T ] and x \u2208 M we hav\u0113\nA(t,\u016b)(x) = b (x) + P(x)\u03b2(t, x), \u2207\u016b(x) + 1 2 P(x), \u2207 2\u016b (x) = \u03b2(t, x), \u2207\u016b(x) + 1 2 \u2206 M u(x) = \u03b2(t, x), \u2207u(x) + 1 2 \u2206 M u(x).\nIn particular, we have that P (seen as a process on M) satisfies a martingale problem with infinitesimal generator A :\n[0, T ] \u00d7 C 2 c (M) \u00d7 M \u2192 R such that for any t \u2208 [0, T ], u \u2208 C 2 (R p ) and x \u2208 M A(t,\u016b)(x) = \u03b2(t, x), \u2207u(x) + 1 2 \u2206 M u(x)\n. In addition, rewriting (15) we have\nKL (P|Q) = KL (P 0 |Q 0 ) + 1 2 T 0 E[ \u03b2(t, X t ) 2 ]dt, which concludes the proof.\nWe also derive the following useful lemma, which will be used in the proof of convergence of RSGM.\nCorollary H.3: Assume A1. Let P 1 , P 2 be a Markov path measure on C([0, T ] , M) with P 1 0 = P 2 0 . In addition, assume that there exist b 1 , b 2 \u2208 C \u221e ([0, T ] , X (M)) such that (X 1 t ) t\u2208[0,T ] and (X 2 t\n) t\u2208[0,T ] are associated to P 1 and P 2 respectively and satisfy weakly dX i t = b 1 (t, X i t )dt + dB t for i \u2208 {1, 2}. Then, we have that\nKL(P 1 |P 2 ) = 1 2 T 0 E[ b 1 (t, X 1 t ) \u2212 b 2 (t, X 1 t ) 2 ]dt.\nProof: Upon, using the Nash embedding theorem (see Gunther, 1991), we can assume that M is a sub-manifold of R p with p \u2208 N such that the Riemannian metric on M is induced by the Euclidean metric on R p . Since M is compact, there exists R > 0 such that M \u2282B(0, R).\nLet \u03d5 \u2208 C \u221e (R p , [0, 1]) such that for any x \u2208B(0, R), \u03d5(x) = 1 and for any x \u2208 R p with\nx \u2265 R + 1, \u03d5(x) = 0. Considerb 1 ,b 2 \u2208 C 2 c ([0, T ] \u00d7 R p , R p ) such that for any t \u2208 [0, T ] and x \u2208 M,b i (x) = b i (x) with i \u2208 {1, 2}. Consider (X i t ) t\u2208[0,T ] such that for any i \u2208 {1, 2} dX i t = \u03d5(X i t ){P(X i t )b i (t,X i t ) +b(X t )}dt + \u03d5(X i t )P(X i t )dB t ,\nwhereb \u2208 C \u221e (R p , R p ) is defined in the proof of Lemma H.2. LetX i 0 \u223c P 1 0 for any i \u2208 {1, 2} then for any i \u2208 {1, 2}, (X i t ) t\u2208[0,T ] (seen as a process on M) is such that L((X i t ) t\u2208[0,T ] ) = P i . Indeed, denote {\u0100 i t } t\u2208[0,T ] the generator of (X i t ) t\u2208[0,T ] for any i \u2208 {1, 2}. Let f \u2208 C \u221e (M, R) andf \u2208 C \u221e (R p , R) an extension to R p . We have that for any i \u2208 {1, 2}, x \u2208 M and t \u2208 [0, T ]\nA i t (f )(x) = b i (t, x) +b(x), \u2207f (x) + 1 2 P(x), \u2207 2f (x) = b i (t, x), \u2207f (x) + 1 2 \u2206 M f (x).\nHence, for any i \u2208 {1, 2}, (X i t ) t\u2208[0,T ] (seen as a process on M) and (X i t ) t\u2208[0,T ] have the same infinitesimal generators. Hence, L((X i t ) t\u2208[0,T ] ) = P i for any i \u2208 {1, 2}. For any i \u2208 {1, 2}, denoteP i = L((X i t ) t\u2208[0,T ] ) (seen as a process on R p ). Note that since for any x \u2208 R p with x \u2265 R + 1, \u03d5(x) = 0 we have that (Liptser and Shiryaev, 2001, Equation (7.137)) is satisfied. In addition, since for any x \u2208 R p with x \u2265 R + 1, \u03d5(x) + \u2207\u03d5(x) = 0, we have that (Liptser and Shiryaev, 2001, Equation (4.110), Equation (4.111)) are satisfied. In addition, letting for any\nt \u2208 [0, T ] and x \u2208 R p , \u03b1(t, x) =b 1 (t, x) \u2212b 2 (t, x) = P(x)(b 1 (t, x) \u2212b 2 (t, x)), we have that for any t \u2208 [0, T ], P(x)\u03b1(t, x) = P(x)(b 1 (t, x) \u2212b 2 (t, x)\n). Therefore, we can apply (Liptser and Shiryaev, 2001, Section 7.6.4) and using that P(x)b(x) = 0 for any x \u2208 M (see the proof of Lemma H.2), we have that\n(dP 1 /dP 2 )((X 1 t ) t\u2208[0,T ] ) = exp [ T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )dX 1 t \u2212 1 2 T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )(b 1 (t,X 1 t ) +b 2 (t,X 1 t )) dt] = exp [ T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t ){b 1 (t,X 1 t ) +b(X 1 t )} dt + T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )dB t \u2212 1 2 T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )(b 1 (t,X 1 t ) +b 2 (t,X 1 t )) dt] = exp[ 1 2 T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ) 2 dt + T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )dB t ]\n. Therefore, we have that\nKL(P 1 |P 2 ) = 1 2 T 0 E[ b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ) 2 ]dt. Hence, we get KL(P 1 |P 2 ) = 1 2 T 0 E[ b 1 (t, X 1 t ) \u2212 b 2 (t, X 1 t ) 2\n]dt. which concludes the proof.\nOnce Lemma H.2 is established, we can obtain the following straightforward extension of Cattiaux et al. (2021, Proposition 4.6).\nProposition H.4: Assume A1. Let Q be a Brownian motion with Q 0 = p ref and P a path measure on\nC([0, T ] , M) such that KL (P|Q) < +\u221e. Then, there exist \u03b2 P , \u03b2 R(P) : [0, T ] \u00d7 M \u2192 such that for any t \u2208 [0, T ] and x \u2208 M, \u03b2 P (t, x), \u03b2 R(P) (t, x) \u2208 T x M.\nIn addition, we have that P and R(P) satisfy martingale problems with infinitesimal generator A P , respectively A R(P) where for any t \u2208 [0, T ], u \u2208 C 2 (M) and x \u2208 M we have\nA P (t, u)(x) = \u03b2 P (t, x), \u2207u(x) + 1 2 \u2206 M u(x), A R(P) (t, u)(x) = \u03b2 R(P) (t, x), \u2207u(x) + 1 2 \u2206 M u(x). Finally, we have that T 0 E[ \u03b2 P (t, X t ) 2 ]dt + T 0 E[ \u03b2 R(P) (t, X T \u2212t ) 2 ]dt < +\u221e,\nwhere (X t ) t\u2208[0,T ] has distribution P.\nProof: The proof is straightforward upon combining lemma H.2 and the fact that KL (P|Q) = KL (R(P)|R(Q)) = KL (R(P)|Q) < +\u221e, using that Q is stationary.\nWe conclude this section, with the following application of Theorem H.1.\nProposition H.5: For any u, v \u2208 C \u221e c (M), we have that for almost any t \u2208 [0, T ]\nE[v(X t )( \u03b2 P (t, X t ) + \u03b2 R(P) (T \u2212 t, X t ), \u2207u(X t ) + \u2206 M u(X t )) + \u2207u(X t ), \u2207v(X t ) ] = 0.(16)\nProof: Remark that C 2 c (M) \u2282 dom(\u03a5 P ) and C 2 c (M) \u2282 dom(\u03a5 R(P) ). In addition, we have that for any u, v \u2208 C 2 c (M), \u03a5 P (u, v) = \u03a5 R(P) (u, v) = u, v . Note that by Lemma H.4 and Theorem H.1 we have that for any u, v \u2208 C \u221e c (M), ( 16) holds.", "publication_ref": ["b41", "b41"], "figure_ref": [], "table_ref": []}, {"heading": "H.2.3 Concluding the proof", "text": "Using lemma H.5 we can now conclude the proof of Theorem 3.1. First, remark that we can identify\n\u03b2 P = b. Let u, v \u2208 C \u221e (M), we have that E[v(X t ) b(X t ) + \u03b2 R(P) (T \u2212 t, X t ), \u2207u(X t ) + \u2206 M u(X t )v(X t ) + \u2207u(X t ), \u2207v(X t ) ] = 0.\nUsing that for any t \u2208 [0, T ], P t admits a smooth positive density w.r.t. p ref denoted p t and the divergence theorem, see (Lee, 2018, p.51), we have that for any t \u2208 [0, T ],\nM { \u03b2 R(P) (T \u2212 t, x), \u2207u(x) + b(x), \u2207u(x) }v(x)p t (x)dp ref (x) = \u2212 M \u2207u(x)p t (x), \u2207v(x) dp ref (x) \u2212 M \u2206 M u(x)v(x)p t (x)dp ref (x) = M \u2207 log p t (x), \u2207u(x)v(x)p t (x)dp ref (x).\nTherefore, we get that for any t \u2208 [0, T ] and\nx \u2208 M, \u03b2 R(P) (T \u2212 t, x), \u2207u(x) = \u2212b(x) + \u2207 log p t (x), \u2207u(x)\n, which concludes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I Convergence of RSGM", "text": "In this section, we study the convergence of RSGM and prove Theorem 4.1. We state our main results in App. I.1 and give discretization bounds following the recent work of Cheng et al. (2022) in sec:discr-bounds-grw.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "I.1 Main results", "text": "In this section, we prove Theorem 4.1. We start by recalling the sequence considered in RSGM. Let (Y k ) k\u2208{0,...,N } be given by Y 0 \u223c p ref and for any k \u2208 {0, . . . , N \u2212 1}\nY k+1 = exp Y k [\u03b3s \u03b8 (T \u2212 n\u03b3, Y k ) + \u221a 2Z k+1 ],\nwhere {Z k }) n\u2208N is a sequence of independent square integrable random variables with zero mean and identity covariance matrix. For ease of reading, we restate Theorem 4.1.\nTheorem I.1: Assume A1, that p 0 is smooth and positive and that there exists M \u2265 0 such that for any\nt \u2208 [0, T ] and x \u2208 M, s \u03b8 (t, x) \u2212 \u2207 log p t (x) \u2264 M, with s \u03b8 \u2208 C([0, T ] , X (M)). Then if T > 1/2, there exists C \u2265 0 independent on T such that W 1 (L(Y N ), p 0 ) = C(e \u2212\u03bb1T + T /2M + e T \u03b3 1/2 ),\nwhere W 1 is the Wasserstein distance of order one on the probability measures on M.\nProof: For any k \u2208 {1, . . . , N }, denote R k such that for any\nx \u2208 R d , A \u2208 B(R d ) and k \u2208 {0, . . . , N \u2212 1} we have E[R k+1 (Y k , A)] = E[1 A (Y k+1 )]. Define for any k 0 , k 1 \u2208 {1, . . . , N } with k 1 \u2265 k 0 Q k0,k1 = k1 =k0 R k1+k0\u2212 .\nFinally, for ease of notation, we also define for any k \u2208 {1, . . . , N }, Q k = Q k+1,N . Note that for any k \u2208 {1, . . . , N }, Y k has distribution \u03c0 \u221e Q k , where \u03c0 \u221e \u2208 P(M) with density w.r.t. the Hausdorff measure p ref .\nLet P \u2208 P(C) be the probability measure associated with (B t ) t\u2208[0,T ] with B 0 \u223c \u03c0 0 , where \u03c0 0 \u2208 P(M) admits a density w.r.t. the Hausdorff measure given by p 0 . We denote (\u0176 t ) t\u2208[0,T ] the process defined by the diffusion d\u0176 t = s \u03b8 (T \u2212 t,\u0176 t )dt + dB t and\u0176 0 \u223c \u03c0 \u221e . We also denot\u00ea P R \u2208 P(C) the probability measure associated with (\u0176 t ) t\u2208[0,T ] . First note that using that P 0 = \u03c0 0 we have for any A \u2208 B(M) \u03c0 0 P T |0 (P R ) T |0 (A) = P T (P R ) T |0 (A) = (P R ) 0 (P R ) T |0 (A) = (P R ) T (A) = \u03c0 0 (A).\nHence we have that \u03c0 0 = \u03c0 0 P T |0 (P R ) T |0 .\n(17) Let \u03d5 \u2208 C(M) with is 1-Lipschitz, i.e. for any x, y \u2208 M, |\u03d5(x) \u2212 \u03d5(y)| \u2264 d(x, y). Since M is compact, we have that \u03d5 is bounded. Using this result, ( 17), the data processing theorem (Kullback, 1997, Theorem 4.1) and Pinsker's inequality (Bakry et al., 2014, Equation 5.2.2) we have\n|E[\u03d5(Y N )] \u2212 M \u03d5(x)p 0 (x)d\u00b5(x)| \u2264 |E[\u03d5(B 0 )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]||E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 \u2212 \u03c0 \u221e (P R ) T |0 TV + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 P T |0 (P R ) T |0 \u2212 \u03c0 \u221e (P R ) T |0 TV + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV + \u221a 2 \u03d5 \u221e KL 1/2 (\u03c0 \u221e P R |0 |\u03c0 \u221eP R |0 ) + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]|.\nWe now control each one of these terms. The first term can be easily controlled using the geometric ergodicity of the Brownian motion on compact manifolds. The second term can be controlled using the Girsanov theory on isometrically embedded manifolds. For the last term, we rely on the convergence of the GRW to its associated diffusion as presented in App. I.2. We now control each one of these terms. (a) Using Lemma C.6, we have that \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV \u2264 C 1/2 e \u03bb1/2 e \u2212\u03bb1T where \u03bb 1 is the first positive eigenvalue of \u2212\u2206 M in L 2 (\u03c0 \u221e ). Therefore, we get that \n\u03d5 \u221e \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV \u2264 C 1/2 e \u03bb1/2 \u03d5 \u221e e \u2212\u03bb1T . (b)\nR |0 ) = 1 2 T 0 E[ s \u03b8 (T \u2212 t, Y t ) \u2212 \u2207 log p T \u2212t (Y t ) 2 ] \u2264 M 2 T.\n(c) Let us define {\u0232 k } N k=0 such that for any k \u2208 {0, . . . , N },\u0232 k 0 =\u0176 0 = Y 0 and for any t \u2208 [0, k\u03b3] we have that\u0232 0 t =\u0176 t . For any t \u2208 [k\u03b3, T ], we have that\u0232 k t = Y t,k , where Y k\u03b3,k =\u0176 k\u03b3 and for any j \u2208 {k, . . . , N \u2212 1} and t \u2208 [0, \u03b3]\nY j\u03b3+t,k = exp Y j\u03b3,k [ts \u03b8 (T \u2212 j\u03b3, Y j\u03b3,k ) + \u221a tE k j Z j ],\nwhere {Z j } N \u22121 j=0 are independent Gaussian random variables with identity covariance matrix and zero mean and E k j is a frame of T Y j\u03b3,k M such that for any j \n\u2208 {k + 1, . . . , N \u2212 1}, E k+1 j = \u0393 Y j\u03b3,k+1 Y j\u03b3,k E k j and {E 0 j } N \u22121 j=0 is such that for any j \u2208 {0, . . . , N \u2212 1}, E 0 j is a frame of T Yj\u03b3 M. One [0, k\u03b3],\n(\u0176 T ) \u2212 \u03d5(Y N )| = |\u03d5(\u0232 0 T ) \u2212 \u03d5(\u0232 N T )| \u2264 N \u22121 k=0 |\u03d5(\u0232 k T ) \u2212 \u03d5(\u0232 k+1 T )| \u2264 \u2207\u03d5 \u221e N \u22121 k=0 d(\u0232 k T ,\u0232 k+1 T\n).\nIn addition, using Lemma I.6 and Lemma I.7, we have that there exists C \u2265 0 such that for any k \u2208 {0, . . . , N \u2212 1}\nE[d(\u0232 k,T ,\u0232 k+1,T )] \u2264 C exp[(N \u2212 k)\u03b3]\u03b3 3/2 .\nTherefore, we get that there exists C \u2265 0 such that\n|E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 C exp[T ]\u03b3 1/2 ,\nTherefore, we get that there exists C \u2265 0 such that for any \u03d5 \u2208 C(M) which is 1-Lipschitz, we have We now state a result regarding the continuous-time process (i.e. we now longer consider discretization errors). We recall that we denote (\u0176 t ) t\u2208[0,T ] the process defined by the diffusion d\u0176 t = s \u03b8 (T \u2212 t,\u0176 t )dt + dB t and\u0176 0 \u223c \u03c0 \u221e . Proof (T): e proof is identical to the one of Theorem I.1, except that we do not have to deal with the discretization error. We use that for any \u00b5, \u03bd \u2208 P(M)\nE[\u03d5(Y N )] \u2212 M \u03d5(x)p 0 (x)dp ref (x) \u2264 C(e \u03bb1/2 \u03d5 \u221e e \u2212\u03bb1T + T /2 \u03d5 \u221e M + e T \u03b3 1/2 ). (18\n\u00b5 \u2212 \u03bd TV = sup{\u00b5[f ] \u2212 \u03bd[f ] : f \u2208 C(M), f \u221e \u2264 1}.\nThe result of Theorem I.2 should be compared with the one of (Rozen et al., 2021, Theorem 3). With our result we control a L 1 bound between the density of\u0176 T and the one of p 0 . In (Rozen et al., 2021, Theorem 3) a L \u221e bound between the densities is recovered. It can be shown thatp T = L(\u0176 T ). Let \u03ba be the modulus of continuity ofp T \u2212 p 0 , i.e. for any \u03b5 \u2265 0 Hence, there exists C \u2265 0 such that for any T > 1/2 p T \u2212 p 0 \u221e \u2264 C(e \u2212\u03bb1T + T /2M).\n\u03ba(\u03b5) = sup{|p T (x) \u2212 p 0 (x) \u2212p T (y) + p 0 (y)| : x, y \u2208 M, d(x, y) \u2264 \u03b5}. Let x 0 \u2208 M such that |p T (x 0 ) \u2212 p 0 (x 0 )| = M = sup{|p T (x) \u2212 p 0 (x)| : x \u2208 M}.\nTherefore, we recover the same guarantees as Theorem I.2 (note that M is not explicitly controlled using network properties in our work, but we could use universal approximation properties as in Rozen et al. (2021) in order to obtain a similar result).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I.2 Discretization bounds for GRW", "text": "In this section, we establish discretization bounds for GRW. Our results are a straightforward extension of Cheng et al. (2022) to the case where the drift term in the GRW is time-inhomogeneous.\nSince M is compact, we have that for any x 1 , x 2 \u2208 M , there exists a minimizing geodesic such that \u03b3 \u2208 C \u221e ([0, 1] , M) and \u03b3(0) = x 1 and \u03b3(1) = x 2 . When this choice is not unique we fix a minimizing geodesic. We denote\n\u0393 x2 x1 : T x1 M \u2192 T x2 M the associated parallel transport. Let b \u2208 C \u221e ([0, T ] , X (M)).\nWe start by introducing a family of GRWs defined on progressively finer grids. Let \u03b3 > 0, X 0 \u2208 M, E 0 \u2208 F X0 M (the vector space of frames at X 0 ) and consider the families\n{E k : k \u2208 {0, . . . , 2 }, \u2208 N}, {X k : k \u2208 {0, . . . , 2 }, \u2208 N} such that X 0 0 = X 0 , X 0 1 = exp X 0 0 [\u03b3b(0, X 0 0 ) + \u221a \u03b3(B 1 \u2212 B 0 )E 0 0 ] and E 0 1 = \u0393 X 0 1 X 0 0 E 0 0 (\nnote that E 2 is not used in the proof but defined for completeness). In addition, we have that for any \u2208 N with \u2265 1, X 0 = X 0 , E 0 = E 0 and for any k \u2208 {0, . . . , 2 \u22121 \u2212 1}\nX 2k+1 = exp X 2k [\u03b3 b(2k\u03b3 , X 2k ) + E 2k (B (2k+1)\u03b3 \u2212 B 2k\u03b3 )], E 2k+1 = \u0393 X 2k+1 X 2k E 2k , X 2k+2 = exp X 2k+1 [\u03b3 b((2k + 1)\u03b3 , X 2k+1 ) + E 2k+1 (B (2k+2)\u03b3 \u2212 B (2k+1)\u03b3 )], E 2k+2 = \u0393 X 2k+2 X \u22121 k+1 E \u22121 k+1 ,(19)\nwhere \u03b3 = \u03b3/2 . For any \u2208 N, we also define (X t ) t\u2208[0,\u03b3] such that for any \u2208 N, k \u2208 {0, . . . , 2 \u2212 1}, we have for any t \u2208 [k\u03b3 ,\n(k + 1)\u03b3 ) , X t = exp X k [(t \u2212 k\u03b3 )b(k\u03b3 , X k ) + E k (B t \u2212 B k\u03b3 )].\nNote that for any \u2208 N and k \u2208 {0, . . . , 2 \u2212 1}, X k\u03b3 = X k .\nWe are going to use the following useful lemma, see (Cheng et al., 2022, Lemma 62).\nLemma I.3: Assume A1. Then, there exists C \u2265 0 such that for any x, y \u2208 M, \u03b3 : [0, 1] \u2192 M minimizing geodesic with \u03b3(0) = x, \u03b3(1) = y and u \u2208 T x M, v \u2208 T y M we have\nd(exp y [v], exp x [u]) 2 \u2264 (1 + C\u03ba 2 exp[4\u03ba])d(x, y) 2 + C exp[4\u03ba] \u0393 x y v \u2212 u 2 + 2 \u03b3 (0), \u0393 x y v \u2212 u , with \u03ba = u + v .\nWe are now ready to state the main result of this section.\nProposition I.4: Assume A1. Then, there exists C \u2265 0 such that for any \u2208 N\nE[sup t\u2208[0,\u03b3] d(X t , X +1 t ) 2 ] \u2264 C\u03b3 3 2 \u22122 . Proof: Let \u2208 N, k \u2208 {0, . . . , 2 \u2212 1} and t \u2208 [k\u03b3 , (k + 1)\u03b3 ]. We define U t k = d(X t , X +1 t ) 2 , U k = sup{U t k : t \u2208 [k\u03b3 , (k + 1)\u03b3 ]} and U \u22121 = 0.\nWe also introduce for any j \u2208 {0, . . . , 2 \u2212 1} and for t \u2208 [k\u03b3 ,\n(2k + 1)\u03b3 +1 ),X +1 t = X +1 t and for t \u2208 [(2k + 1)\u03b3 +1 , (k + 1)\u03b3 ) X +1 t = exp X +1 2j [\u03b3 +1 b(2j\u03b3 +1 , X +1 2j ) + (t \u2212 (2k + 1)\u03b3 +1 )b((2j + 1)\u03b3 +1 , X +1 2j ) + (B t \u2212 B j\u03b3 )E +1 2j ].\nUsing this result and that for any a, b \u2265 0, (a + b) 2 \u2264 (1 + 2 \u2212 )a 2 + (1 + 2 )b 2 , we have that for any t \u2208 [k\u03b3 , (k + 1)\u03b3 ]\nU t k+1 \u2264 (1 + 2 \u2212 )d(X t ,X +1 t ) 2 + (1 + 2 )d(X +1 t , X +1 t ) 2 . (20\n)\nNote that for t \u2208 [k\u03b3 , (2k + 1)\u03b3 +1 ], the second term in ( 20) is zero. We now bound each one of these terms:\n(a) First, we assume that t \u2208 [(k + 1)\u03b3 , (2k + 1)\u03b3 +1 ]. Recall that\nX +1 t = exp X +1 2k [\u03b3 +1 b(k\u03b3 , X +1 2k ) (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B k\u03b3 )E +1 2k ], X t = exp X k [(t \u2212 k\u03b3 )b(k\u03b3 , X k ) + (B t \u2212 B k\u03b3 )E k ].\nHence, using Lemma I.3, we have that\nd(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 (21) + C exp[4\u03ba k ] \u0393 X k X +1 2k v k \u2212 u k 2 + 2 w (0), \u0393 X k X +1 2k v k \u2212 u k ,\nwith w : [0, 1] \u2192 M a minimizing geodesic between X k and X +1 2k\n\u03ba k = u k + v k , u 1 k = (t \u2212 k\u03b3 )b(k\u03b3 , X k ), v 1 k = \u03b3 +1 b(2k\u03b3 +1 , X +1 2k ) + (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ), u 2 k = (B t \u2212 B k\u03b3 )E k , v 2 k = (B t \u2212 B k\u03b3 )E +1 2k , u k = u 1 k + u 2 k , v k = v 1 k + v 2 k .\nIn particular, since E k = \u0393\nX k X +1 2k E +1 2k using (19), we have that u 2 k = \u0393 X k X +1 2k v 2 k .\nTherefore, combining this result and that t \u2212 (2k + 1)\u03b3 +1 + \u03b3 +1 = t \u2212 k\u03b3 , we get that\n\u0393 X k X +1 2k v 1 k \u2212 u 1 k \u2264 \u03b3 +1 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) + \u03b3 +1 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ) \u2264 \u03b3 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) + L 2 \u03b3 2 \u2264 L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 .\nTherefore, we get that u k \u2212v k \u2264 L 1 \u03b3 d(X k , X +1 2k )+L 2 \u03b3 2 . In addition, we have that w (0) \u2264 d(X k , X +1 2k ) since w is a minimizing geodesic. Combining these results and ( 21) we get that\nd(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 + C exp[4\u03ba k ](L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 ) 2 + 2(L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 )d(X k , X +1 2k ) \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 )d(X k , X +1 2k ) 2 + 2(L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 )d(X k , X +1 2k ) + 2L 2 2 \u03b3 4 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 + 2L 1 \u03b3 + 4L 2 \u03b3 )d(X k , X +1 2k ) 2 + 8L 2 \u03b3 3 , Hence, there exists C 1 \u2265 0 (not dependent on k or ) such that (1 + 2 \u2212 )d(X +1 t , X t ) 2 \u2264 (1 + C 1 {\u03ba 2 k exp[4\u03ba k ] + \u03b3 2 exp[4\u03ba k ] + 2 \u2212 })d(X k , X +1 2k ) 2 + C 1 \u03b3 3 . Next, we assume that t \u2208 [k\u03b3 , (2k + 1)\u03b3 +1 ]. Recall that X +1 t = exp X +1 2k [(t \u2212 k\u03b3 )b(k\u03b3 , X +1 2k ) + (B t \u2212 B k\u03b3 )E +1 2k ], X t = exp X k [(t \u2212 k\u03b3 )b(k\u03b3 , X k ) + (B t \u2212 B k\u03b3 )E k ].\nHence, using Lemma I.3, we have that\nd(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 (22) + C exp[4\u03ba k ] \u0393 X k X +1 2k v k \u2212 u k 2 + 2 w (0), \u0393 X k X +1 2k v k \u2212 u k ,\nwith w : [0, 1] \u2192 M a minimizing geodesic between X k and X +1 2k\n\u03ba k = u k + v k , u 1 k = (t \u2212 k\u03b3 )b(k\u03b3 , X k ), v 1 k = (t \u2212 k\u03b3 )b(k\u03b3 , X +1 2k ), u 2 k = (B t \u2212 B k\u03b3 )E k , v 2 k = (B t \u2212 B k\u03b3 )E +1 2k , u k = u 1 k + u 2 k , v k = v 1 k + v 2 k .\nIn particular, since E k = \u0393 19) and t \u2212 (2k + 1)\u03b3 +1 + \u03b3 +1 = t \u2212 k\u03b3 , we have\nX k X +1 2k E +1 2k using (\nthat u 2 k = \u0393 X k X +1 2k v 2 k .\nTherefore, we get that\n\u0393 X k X +1 2k v 1 k \u2212 u 1 k \u2264 \u03b3 +1 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) \u2264 \u03b3 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) + L 2 \u03b3 2 \u2264 L 1 \u03b3 d(X k , X +1 2k ).\nTherefore, we get that\nu k \u2212 v k \u2264 L 1 \u03b3 d(X k , X +1 2k ).\nIn addition, we have that w (0\n) \u2264 d(X k , X +1\n2k ) since w is a minimizing geodesic. Combining these results and ( 22) we get that\nd(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 + C exp[4\u03ba k ]L 2 1 \u03b3 2 d(X k , X +1 2k ) 2 + 2L 1 \u03b3 d(X k , X +1 2k )d(X k , X +1 2k ) \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 )d(X k , X +1 2k ) 2 + 2L 1 \u03b3 d(X k , X +1 2k ) 2 + 2L 2 2 \u03b3 4 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 + 2L 1 \u03b3 )d(X k , X +1 2k ) 2 .\nHence, there exists C 1 \u2265 0 (not dependent on k or ) such that for any t \u2208 [k\u03b3 , (k + 1)\u03b3 ] 20) is zero. Therefore in what follows, we assume t \u2208 [(2k + 1)\u03b3 +1 , (k + 1)\u03b3 ]. We introduc\u00ea\n(1 + 2 \u2212 )d(X +1 t , X t ) 2 \u2264 (1 + C 1 {\u03ba 2 k exp[4\u03ba k ] + \u03b3 2 exp[4\u03ba k ] + 2 \u2212 })d(X k , X +1 2k ) 2 + C 1 \u03b3 3 . (23\n) (b) We recall that if t \u2208 [k\u03b3 , (2k + 1)\u03b3 +1 ] the second term in (\nX +1 t = exp X +1 2k+1 [(t \u2212 (2k + 1)\u03b3 +1 )\u0393 X +1 2k+1 X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ) (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 ].(24)\nIn what follows, we provide an upper-bound for d(\nX +1 t , X +1 t )\n. First, we have that\nd(X +1 t , X +1 t ) \u2264 d(X +1 t ,X +1 t ) + d(X +1 t , X +1 t ).\nWe recall that 19), ( 24) and ( 25) we have that\nX +1 t = exp X +1 2k [\u03b3 +1 b(2k\u03b3 +1 , X +1 2k ) + (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B k\u03b3 )E +1 2k ]. (25\n) Denote a k , b k such that a k = b(2k\u03b3 +1 , X +1 2k ) + (B (2k+1)\u03b3 +1 \u2212 B k\u03b3 )E +1 2k , b k = (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k . Using (\nX +1 2k+1 = exp X +1 2k [a k ],X +1 t = exp X +1 2k+1 [\u0393 X +1 2k+1 X +1 2k b k ],X +1 t = exp X +1 2k [a k + b k ].\nUsing this result and (Sun et al., 2019, Lemma 3), there exists\nC 2 \u2265 0 (not dependent on k or ) such that d(X +1 t ,X +1 t ) \u2264 C 2 ( a k + b k ) 3 .\nUsing this result and that for any t \u2208 [0, \u03b3] and x \u2208 M, b(t, x) \u2264 K we get that there exists\nC 3 \u2265 0 (not dependent on k or ) such that d(X +1 t ,X +1 t ) 2 \u2264 C 3 (\u03b3 6 +1 + B t \u2212 B (2k+1)\u03b3 +1 6 + B (2k+1)\u03b3 \u2212 B (k+1)\u03b3 6 ).(26)\nFinally, we recall that\nX +1 t = exp X +1 2k+1 [(t \u2212 (2k + 1)\u03b3 +1 )\u0393 X +1 2k+1 X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 ], X +1 t = exp X +1 2k+1 [(t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k+1 ) + (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 ].\nLet us define\n\u03c4 k = c k + d k , c k = c 1 k + c 2 k , d k = d 1 k + d 2 k , c 1 k = (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k+1 ), d 1 k = (t \u2212 (2k + 1)\u03b3 +1 )\u0393 X +1 2k+1 X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ), c 2 k = d 2 k = (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 .(27)\nUsing Lemma I.3, we get that\nd(X +1 t ,X +1 t ) 2 \u2264 C exp[4\u03c4 k ] c k \u2212 d k 2 \u2264 CL 2 2 \u03b3 2 +1 exp[4\u03c4 k ]d(X +1 2k+1 , X +1 2k ) 2 .(28)\nIn addition, using Lemma I.3, we get that\nd(X +1 2k+1 , X +1 2k ) 2 \u2264 exp[4 e k ] e k , with e k = \u03b3 +1 b(k\u03b3 , X +1 2k ) + (B (2k+1)\u03b3 +1 \u2212 B k\u03b3 )E +1 2k .\nCombining this result and (28), we get that\nd(X +1 t ,X +1 t ) 2 \u2264 C 3 \u03b3 2 +1 (\u03b3 2 +1 + B (2k+1)\u03b3 +1 \u2212 B k\u03b3 2 ) exp[4\u03c4 k + e k ].(29)\nCombining ( 26) and ( 29), there exists C 5 such that\nd(X +1 t , X +1 t ) 2 \u2264 C 5 \u03b3 2 +1 (\u03b3 2 +1 + B (2k+1)\u03b3 +1 \u2212 B k\u03b3 2 ) exp[4\u03c4 k + e k ] + C 5 (\u03b3 6 +1 + B t \u2212 B (2k+1)\u03b3 +1 6 + B (2k+1)\u03b3 \u2212 B (k+1)\u03b3 6 ). (30\n)\nIn what follows, we denote 27). Therefore, using (20), ( 23) and (30), we get that for any k \u2208 {0, . . . , 2 \u2212 1}\n\u03b1 k = C 1 {(\u03ba + k ) 2 exp[4\u03ba k ] + \u03b3 2 exp[4\u03ba + k ] + 2 \u2212 }. \u03b2 k = C 1 \u03b3 3 + C 5 (1 + 2 )\u03b3 2 +1 (\u03b3 2 +1 + B (2k+1)\u03b3 +1 \u2212 B k\u03b3 2 ) exp[4\u03c4 + k + e k ] +C 5 (1 + 2 ) (\u03b3 6 +1 + sup t\u2208[k\u03b3 ,(k+1)\u03b3 ] { B t \u2212 B (2k+1)\u03b3 +1 6 } + B (2k+1)\u03b3 \u2212 B (k+1)\u03b3 6 ), with \u03c4 + k = sup{ c k + d k : t \u2208 [k\u03b3 , (k + 1)\u03b3 ]}, see(\nU k+1 \u2264 (1 + \u03b1 k )U k + \u03b2 k . Let {R k } 2 k=\u22121 such that R \u22121 = 0 and for any k \u2208 {0, . . . , 2 \u2212 1} R k+1 = (1 + \u03b1 k )R k + \u03b2 k .\nThen, for any k \u2208 {0, . . . , 2 \u2212 1}, we have that R\n2 \u22121 \u2265 R k \u2265 U k . Therefore E[R 2 ] \u2265 E[sup{U k : k \u2208 {0, . . . , 2 }}] \u2265 E[sup{d(X t , X +1 t ) 2 : t \u2208 [0, \u03b3]}].(31)\nIn addition, using that for any k \u2208 {0, . \nE[R k+1 ] = (1 +\u1fb1 k )E[R k ] +\u03b2 k .\nTherefore, using the discrete Gr\u00f6nwall lemma we get that for any k \u2208 {0, . . . , 2 \u2212 1}\nE[R 2 ] \u2264\u03b2 2 \u22121 + exp[ 2 \u22121 n=0\u1fb1 n ] 2 \u22121 j=0\u03b2 j\u1fb1j .\nIn addition, there exists C 8 \u2265 0 such that for any k \u2208 {0, . . . , 2 },\u1fb1 k \u2264 C 8 2 \u2212 and\u03b2 k \u2264 C 8 \u03b3 3 2 \u22122 . Hence, there exists C 9 \u2265 0 such that\nE[R 2 ] \u2264 C 9 \u03b3 3 2 \u22122 ,\nwhich concludes the proof upon using (31).\nProposition I.5: Assume A1. Then, there exists (X t ) t\u2208[0,\u03b3] such that lim \u2192+\u221e sup{d(X t , X t ) :\nt \u2208 [0, \u03b3]} = 0 and (X t ) t\u2208[0,\u03b3] is a weak solution to dX t = b(t, X t )dt + dB M t .\nProof: The proof is a straightforward application of Lemma I.4 and (Cheng et al., 2022, A.1 (Step 2 and Step 3), A.2).\nProposition I.6: Assume A1. Then, there exists\nC \u2265 0 such that E d(X 0 1 , X \u03b3 ) 2 \u2264 C\u03b3 3/2 . Proof: Using Lemma I.4, there exists C \u2265 0 such that for any \u2208 N E[sup t\u2208[0,\u03b3] d(X t , X +1 t )] \u2264 C\u03b3 3/2 2 \u2212 .\nTherefore, combining this result and Lemma I.5 we get that for any \u2208 N E[sup t\u2208[0,\u03b3] d(X t , X t )] \u2264 2C\u03b3 3/2 , which concludes the proof.\nFinally, we consider the two following processes (X 1 k , X 2 k ) k\u2208N such that for any k \u2208 N and i \u2208 {1, 2}\nX i k+1 = exp X i k [\u03b3b(k\u03b3, X i k ) + \u221a \u03b3E i k Z k ]\n, where {Z k } k\u2208N is a family of independent Gaussian random variables with zero mean and identity covariance matrix, and for any k\n\u2208 N, E 1 k is a frame for T X 1 k M and E 2 k = \u0393 X 2 k X 1 k E 1 k .\nProposition I.7: Assume A1. Then, there exists C \u2265 0 such that for any k\n\u2208 N E d(X 1 k , X 2 k ) \u2264 exp[Ck\u03b3]E d(X 1 0 , X 2 0 ) . Proof: Let k \u2208 N. Using Lemma I.3, there exists D \u2265 0 such that d(X 1 k+1 , X 2 k+1 ) 2 \u2264 (1 + D\u03ba 2 k exp[4\u03ba k ])d(X 1 k , X 2 k ) 2 + D exp[4\u03ba k ] \u0393 X 1 k X 2 k v k \u2212 u k We have that \u0393 X 1 k X 2 k v 2 k = v k and \u0393 X 1 k X 2 k v 1 k \u2212 u 1 k \u2264 L 1 \u03b3d(X 1 k , X 2 k ).\nIn addition, w (0) \u2264 d(X 1 k , X 2 k ). Therefore, we get that\nd(X 1 k+1 , X 2 k+1 ) 2 \u2264 (1 + D\u03ba 2 k exp[4\u03ba k ] + D\u03b3 2 exp[4\u03ba k ] + 2\u03b3)d(X 1 k , X 2 k ) 2 .\nHence, using that for any t \u2265 0, \u221a 1 + t \u2264 1 + t/2, we have\nd(X 1 k+1 , X 2 k+1 ) \u2264 (1 + D\u03ba 2 k exp[4\u03ba k ] + D\u03b3 2 exp[4\u03ba k ] + 2\u03b3)d(X 1 k , X 2 k )\n. Therefore, we get that there exists C \u2265 0 such that\nE[d(X 1 k+1 , X 2 k+1 )] \u2264 (1 + C\u03b3)E[d(X 1 k , X 2 k )\n], which concludes the proof.", "publication_ref": ["b18"], "figure_ref": [], "table_ref": []}, {"heading": "J Proof of proposition 3.3", "text": "The regularity conditions on p t|s (x t |x s )s t (x t ) are\n\u2022 p t|s (x t |x s )s t (x t ) is a vector field in C 1 \u2200x s . \u2022 |p t|s (x t |x s )s t (x t )| \u2208 L 1 \u2200x s . \u2022 div(p t|s (x t |x s )s t (x t )) \u2208 L 1 \u2200x s .\nThese conditions are not difficult to show. We can manually control s t by our choice of score network, and p t|s (x t |x s ) is controlled by choice of noising process. Under these conditions we can prove the statement.\nProof: Let t \u2208 (0, T ] and s t \u2208 C \u221e (M). Using a divergence theorem for non-compact manifolds (see Gaffney, 1954, p.2), we have which concludes the proof.  Assume that that M |p T (x) \u2212 1|dp ref (x) \u2264 \u03b5 and that there exists x 0 \u2208 M such that |p T (x) \u2212 1| > \u03ba\u03b5 with \u03ba > 0 and let T \u2265 T 0 with T 0 = (\u03ba\u03b5/(2C 1 )) 1/\u03b2 . Then, using ( 32) and ( 33), we have for any r \u2208 (0, r 0 )\nt|s (s t )= M\u00d7M \u2207 log p t|s (x t |x s ) 2 dP s,t (x s , x t ) + M s t (x t ) 2 dP t (x t ) \u22122 M\u00d7M \u2207 log p t|s (x t |x s ), s t (x t ) M dP s,t (x s , x t ) Looking at the last term M\u00d7M \u2207 log p t|s (x t |x s ), s t (x t ) M dP s,t (x s , x t ) = M\u00d7M \u2207 log p t|s (x t |x s ), s t (x t ) M p t|s (x t |x s )p s (x s )d(p ref \u2297 p ref )(x s , x t ) = M M \u2207p t|s (x t |x s ), s t (x t ) M dp ref (x t ) p s (x s )dp ref (x s ) = \u2212 M M div(s t )(x\n\u03b5 \u2265 B (0,r) |p T (x) \u2212 1| \u2265 C 2 r d (\u03ba\u03b5 \u2212 C 1 (1 + T \u03b2 )r). Since \u03ba\u03b5/(2C 1 (1 + T \u03b2 )) \u2208 (0, r 0 ) we have \u03b5 \u2265 C 2 (\u03ba\u03b5) d+1 /(4C 1 (1 + T \u03b2 )).\nTherefore, we get that \u03b5 \u2265 C 2 (\u03ba\u03b5) d+1 /(4C 1 (1 + T \u03b2 )).\nTherefore, we get that \u03ba \u2264 (4C d+1) . Therefore, we have that for any\n1 (1 + T \u03b2 )/C 2 ) 1/(d+1) \u03b5 \u22121/(\nx \u2208 M |p T (x) \u2212 1| \u2264 (8C 1 (1 + T \u03b2 )/C 2 ) 1/(d+1) \u03b5 1\u22121/(d+1) . (34\n)\nLet T 0 \u2265 0 such that for any T \u2265 T 0 we have\n(8C 1 (1 + T \u03b2 )/C 2 ) 1/(d+1) C 1\u22121/(d+1) 0 e \u2212(1\u22121/(d+1))\u03bb1T \u2264 2 1\u2212\u03b2 C 1 .\nCombining this result and (34), we get that for any x \u2208 M and T \u2265 0\n|p T (x) \u2212 1| \u2264 (8C 1 (1 + T \u03b2 )/C 2 ) 1/(d+1) C 1\u22121/(d+1) 0 e \u2212(1\u22121/(d+1))\u03bb1T ,\nwhich concludes the proof.\nThe following proposition quantifies this approximation.\nProposition L.2: Assume A1 and that p 0 \u2208 C \u221e (M, (0, +\u221e)). Let x 0 \u2208 M and assume that for any\nt \u2208 [0, T ], |\u015d \u03b8 (t, x 0 ) \u2212 \u2202 t log p t (x 0 )| \u2264 M with M \u2265 0.\nThen, there exists C, T 0 \u2265 0 such that for any T \u2265 0\n| log p 0 (x 0 ) \u2212 T 0\u015d \u03b8 (t, x 0 )dt| \u2264 C exp[\u2212\u03bb 1 T /2] + MT., where \u03bb 1 is the first non-negative eigenvalue of \u2212\u2206 M in L 2 (p ref ). Proof: First using, Lemma L.1, there exists C 0 , T (a) 0 \u2265 0 such that for any T \u2265 T (a) 0 |p T (x 0 ) \u2212 1| \u2264 C 0 exp[\u2212\u03bb 1 T /2]. Let T (b) 0 = |log(C 0 )| /\u03bb 1 . Using that for any s \u2208 [1/2, +\u221e) we have that | log(1 + s)| \u2264 2 log(2)|s| we get that for any T \u2265 max(T (a) 0 , T (b) 0 ) | log p T (x 0 )| \u2264 2 log(2)C 0 exp[\u2212\u03bb 1 T /2],\nwhich concludes the proof.\nIn practice, we do not have access to \u2202 t log p t . However, following (Choi et al., 2021 \nL(s) = (1/2)E[ T 0 \u03bb(t)s(t, X t )dt] + E[ T 0 \u03bb(t)\u2202 t s(t, X t )dt] +E[ T 0 \u2202 t \u03bb(t)\u2202 t s(t, X t )dt] + E[\u03bb(0)s(0, X 0 )] \u2212 E[\u03bb(T )s(T, X T )], where \u03bb \u2208 C \u221e ([0, T ] , R) is a weighting function. Proof: For any t \u2208 [0, T ] and x t \u2208 M we hav\u00ea s(x t ) = M \u2202 t log p t|0 (x t |x 0 )p 0|t (x 0 |x t )dx 0 . Hence, since M is compact and\u015d \u2208 C \u221e ([0, T ] \u00d7 M, R), we have that\u015d = arg min{L 0 (s) : s \u2208 C \u221e ([0, T ] \u00d7 M, R)} where for any s \u2208 C \u221e ([0, T ] \u00d7 M, R) we have L 0 (s) = T 0 \u03bb(t) M\u00d7M (s(t, x t ) \u2212 \u2202 t log p t|0 (x t |x 0 )) 2 dp 0,t (x 0 , x t )dt = T 0 \u03bb(t) M s(t, x t ) 2 dp t (x t )dt \u22122 T 0 \u03bb(t) M\u00d7M s(t, x t )\u2202 t log p t|0 (x 0 , x t )dp 0,t (x 0 , x t )dt + T 0 \u03bb(t) M dp t (x t )dt(35)\nIn addition, we have that\nT 0 \u03bb(t) M\u00d7M s(t, x t )\u2202 t log p t|0 (x t |x 0 )dp 0,t (x 0 , x t )dt = T 0 M\u00d7M \u03bb(t)s(t, x t )\u2202 t p t|0 (x t )dp 0 (x 0 )dp ref (x t )dt.\nLet's consider the probability flow \u03c6 associated with the reverse diffusion (4)-given by dY t = {\u2212b(Y t ) + \u2207 log p T \u2212t (Y t )}dt + dB M t -i.e. the solution of the following ODE (see App. D)\ndY t = {\u2212b(Y t ) + 1 2 \u2207 log p T \u2212t (Y t )}dt.\nIn practice, the Stein score \u2207 log p t is approximated with the score network s \u03b8 (t, \u2022). It is sufficient to parametrize the score network so that it is equivariant w.r.t. its second argument -assuming that \u03c1(g) and the drift b commute (e.g. which is true for a linear drift)-since we then have\n\u2212b + 1 2 s \u03b8 (T \u2212 t, \u2022) (\u03c1(g)Y t ) = \u03c1(g) \u2212b + 1 2 s \u03b8 (T \u2212 t, \u2022) (Y t ).", "publication_ref": ["b33", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "N Stereographic baseline details", "text": "In the experiments on the sphere we compare to a Stereographic Score-Based baseline model. This model is an alternative to the RSGM the we propose in order to construct score-based models on manifolds without having to construct the intrinsic approach presented in the paper as Riemannian Score-Based models. They can be applied to more cases than just the sphere.\nIn general these models work as follows:\n1. Project the datapoints from the manifold to Euclidean space through a invertible 9 function f : M \u2192 R d .\n2. Train a Euclidean score-based generative model on the datapoints projected to Euclidean space, giving a density p \u03b8 on R d (where \u03b8 are the parameters of the density).\n3. Define the density on the manifold as the pushforward of the density in Euclidean space under the inverse of the bijection, P \u03b8,M = f \u22121 * p \u03b8 .\nOne could also apply these models to the torus. By using the bijection f : \u03b8 \u2192 tan(\u03b8) we can project each coordinate onto the real line.\nIn general we found that these models perform less well than their intrinsic counterparts. In order to map density near the seams of the bijection, it requires the model to send data points off to infinity in the Euclidean space. This is numerically challenging and leaves artefacts in the pushforward density on the manifold. In addition, these methods depend on the bijection used to project the data into a Euclidean space and therefore are not intrinsic.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "O Experimental details", "text": "In what follows we describe the experimental settings used to generate results introduced in Sec. 6. The models and experiments have been implemented in Jax (Bradbury et al., 2018), using a modified version of the Riemannian geometry library Geomstats (Miolane et al., 2020).\nAnonymized code can be found at here 10 . Due to difficulties referencing anonymized repositories, the modified version of geomstats is included as a zip file in the supplementary material. Additionally modified versions of the submitit and hydra-submitit-launcher packages are not supplied for the same reasons, but the default versions of these will suffice for most users. Full code and all repos will be publicly available after publication. Following Song et al. (2021), the score-based generative models (SGMs) diffusion coefficient is parametrized as g(t) = \u03b2(t) with \u03b2 : t \u2192 \u03b2 min + (\u03b2 max \u2212 \u03b2 min ) \u2022 t.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Models", "text": "Architecture The architecture of the score network s \u03b8 is given by a multilayer perceptron with 5 hidden layers for the Earth and SO(3) experiments, and 3 for the high-dimension experiments with 512 units each. We use sinusoidal activation functions. We decompose the output of the score network on the set of divergence free vector fields as per Sec. 3.4.\nLoss Where not specified, SGMs are trained with the sliced score matching (SSM) loss im t , relying on the Hutchinson estimator for computing the divergence with Rademacher noise described in Sec. 3.4. We found that training with the denoising score matching (DSM) loss t|0 gave similar results. Regarding the weighting function, for DSM loss t|0 we use \u03bb t = Var[X t |X 0 ] (where we rely on the closed-form standard deviation available in the Euclidean setting as a proxy for the compact manifold setting), while for the ISM/SSM losses im t we use \u03bb t = g(t) 2 = \u03b2(t).\nOptimization All models are trained by the stochastic optimizer Adam (Kingma and Ba, 2014) with parameters \u03b2 1 = 0.9, \u03b2 2 = 0.999, batch-size of 512 data-points. The learning rate is annealed with a linear ramp from 0 to 1000 and from then with a cosine schedule.", "publication_ref": ["b57"], "figure_ref": [], "table_ref": []}, {"heading": "Likelihood evaluation and sample drawing", "text": "We rely on the Dormand-Prince solver (Dormand and Prince, 1980), an adaptive Runge-Kutta 4(5) solver, with absolute and relative tolerance of 1e \u2212 5 to compute approximate numerical solutions of any ODEs. For the rollouts of the SGM SDEs we use a Euler Maruyama predictor and no corrector. Unless stated we use 100 step rollouts.\nHardware Models are trained on a cluster with a mixture of GeForce RTX 1080, 1080 Ti and 2080 Ti GPU cards.", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "O.1 Sphere", "text": "Data We randomly split the datasets intro training, validation and test datasets with (0.8, 0.1, 0.1) proportions. In each case the earth is approximated as a perfect sphere.\nModels The mixture of Kent distributions (Peel et al., 2001) were optimised using the EM algorithm and the number of components were selected from a grid search over the range 5,10,15,20,25,30,40,50,75, 100, based on validation set likelihood and 250 EM iterations. The number of components selected were: Volcano 25, Earthquake 50, Flood 100 and Fire 100.\nFor the stereographic SGM-which is a standard SGM with an Ornstein-Uhlenbeck process followed with the inverse stereographic projection-we found \u03b2 min = 0.001 and \u03b2 max = 2 to work best.\nOptimization The score-based models are trained for 600k iterations for all datasets but 'Flood' where 300k performed best.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Additional experimental results", "text": "Approximate forward sampling Standard Euclidean SGMs rely on a Ornstein-Ulhenbeck (OU) forward process (1) which can easily be simulated since X t |X 0 is Gaussian. In contrast, for most manifolds one has to rely on an approximate sampling scheme-see Sec. 3.3. First, we directly assess the quality of the approximate samplesX t |X 0 obtained via geodesic random walk (GRW), against 'exact' samples X t |X 0 which are obtained by using a high number of discretization steps (N = 1000). We report on Fig. 4a the discrepancy between these distributions for different values of discretization steps N , as measured by maximum mean discrepancy (MMD) (Gretton et al., 2012). We see that from N = 5 the approximate samples are very closely distributed to the true samples. Then, in order to assess the impact of this approximation on the RSGMs' performance, we report on Fig. 4b the log-likelihood when varying the number of discretization steps N . We similarly observe that apart from very small values of N , the models' performance is very robust to the approximation quality of the forward sampling samples.\nDSM loss t|0 On Fig. 5, we show how the test log-likelihood varies with respect to the two hyparameters of the DSM loss, by training RSGMs over a grid of values for \u03c4 and J on the Flood dataset. We can see that the Varadhan approximation by itself (\u03c4 = 1) yields descent performance, although a wise combination of Varadhan approximation with a truncation of the heat kernel can give even better results. The performance is relatively robust to the choice of such hyperparameters as long as \u03c4 and J are high enough.   Negative log-likelihood ", "publication_ref": ["b38"], "figure_ref": ["fig_3", "fig_4", "fig_5"], "table_ref": []}, {"heading": "O.2 Torus", "text": "Data The synthetic data trained on consists of a wrapped Gaussian distribution on T n with uniformly chosen random mean and standard deviation of 0.2. Such a distribution is defined by taking the density of a Normal distribution in the tangent space of the manifold at the mean and passing it through the exponential map at the mean.\nArchitecture To parametrize the vector field on T n we use a single filed per dimension pointing in a consistent direction around the i th component in the product, with unit norm.\nModels All models were trained with the same 3 layer, 512 units per layer MLP across different dimension sizes.\nOptimization The models are optimized for 50k iterations. The RSGM models are trained with both the implicit score-matching loss and the sliced score-matching loss.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "O.3 Special Orthogonal group", "text": "Applications of orthogonal constraints span various fields, such as protein docking with ligands binding pose prediction (Ganea et al., 2022), robotics and Computer vision with rigid body transformation estimation (Barfoot et al., 2011;Prokudin et al., 2018), and medical imaging for data alignment (Hou et al., 2018).\nData We consider the synthetic dataset consisting of samples in SO 3 (R d ) 11 from the mixture distribution with density p(Q) = 1 K K k=1 N W (Q|Q k , \u03c3 2 k ) with K \u2208 N, where for any k \u2208 {1, . . . , K}, we have that Q = Q k exp Id [\u03c3 k\u1e91 ] with z \u223c N(0, Id R 3 ) satisfies Q \u223c N W (Q k , \u03c3 k ) and (\u2022) \u2227 : R 3 \u2192 so(3). For any k \u2208 {1, . . . , K}, we set Q k \u223c \u00b5 where \u00b5 is the uniform distribution on SO 3 (R) and \u03c3 2 k \u223c IG(\u03b1 = 100, \u03b2 = 1), where IG is the inverse Gaussian distribution. We choose K = 32 mixture components. We showcase a conditional sampling extension of our model-see App. M for more details-by targeting individual mixture components p(Q|k). Our model is trained using the t|0 (DSM) loss along with the Varadhan asymptotic approximation, see (8).\nArchitecture To parametrize the vector field, we rely on the basis of the Lie group, so(n) = {A \u2208 M d (R) : A = \u2212A} given by E ij = U ij \u2212 U ji for i, j \u2208 {1, . . . , d} with i < j and U ij = (\u03b4 ij (k, )) 1\u2264k, \u2264d , which induces a basis on the tangent spaces T Q SO d for any Q \u2208 SO d (R) given by {QE ij } 1\u2264i<j\u2264d . This is the divergence-free vector field approach described in Sec. 3.4.\nModels We compare our proposed approach against Moser flows (Rozen et al., 2021) and a wrappedexponential baseline (Falorsi et al., 2019) defined as the pushforward along the transformation\nR 3 F \u22121 \u03b8 \u2212 \u2212\u2212 \u2192 R 3 g \u2212 \u2192 R 3 \u2227 \u2212 \u2192 so(3)\nexp \u2212 \u2212 \u2192 SO 3 (R) with F \u22121 \u03b8 denoting the approximate time-reversed diffusion, g denoting the radial operator defined by g : x \u2192 2\u03c0 tanh( x )x/ x , (\u2022) \u2227 : R 3 \u2192 so(n) the isomorphism given by the basis on so(3) and exp the matrix exponential. The radial g operator's constant 2\u03c0 is chosen as the injectivity radius of the group so that the transformation tanh \u2022 \u2227 \u2022 exp is injective (the set of elements with no preimage is then only the cut locus which is known to have measure zero). Henceforth, this wrapped-exponential transformation cannot be bijective, it is either injective or surjective depending on the choice of radius in the radial operator g.\nOptimization Models are trained for 100k iterations. The Riemannian SGM is trained with the Varhadan approximation of the denoising score-matching loss (DSM) Sec. 3.4, and the wrappedexponential model relies on the exact DSM loss. After a first hyperparameter exploration, a grid search is performed over learning_rate \u2208 [2e \u2212 5, 4e \u2212 5], for SGMs over \u03b2 f \u2208 [0.5, 1, 2, 4, 6, 8, 10] and for Moser flows over K \u2208 [1000, 10000] and \u03bb min \u2208 [1, 10, 100].", "publication_ref": ["b34", "b7", "b45", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We are grateful to the anonymous reviewers for their insightful comments and the for fruitful discussion more generally. We thank the hydra (Yadan, 2019), jax (Bradbury et al., 2018) ", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "K Comparison with Moser flows", "text": "In this section, we compare ourselves with Rozen et al. (2021) in greater details. Rozen et al. (2021) also aims at interpolating between a reference distribution p ref and a target distribution p 0 . We assume that we have access to the density p ref and that we know how to sample form p ref (which is often the case if p ref is the uniform distribution on M).\nWe then consider the following interpolationp t = (1 \u2212 t)p 0 + tp 1 , withp 0 = p ref andp 1 = p 0 . Let (X t ) t\u2208[0,1] be given by X 0 \u223cp 0 and dX t = v t (X t )dt where for any t \u2208 [0, 1], v t = u/((1 \u2212 t)p 0 +p 1 ), with div(u) =p 0 \u2212p 1 . Using the Fokker-Planck equation, we have that for any t \u2208 [0, 1], X t \u223cp t . In Rozen et al. (2021), u is replaced by a parametric version u \u03b8 and the authors optimize the loss\nwith \u03bb, \u03b5 > 0 and for any f : M \u2192 R, f +,\u03b5 = max(f, \u03b5) and f \u2212,\u03b5 = \u03b5 \u2212 min(f, \u03b5). Given u \u03b8 , we then consider (X \u03b8 t ) t\u2208[0,1] such that dX \u03b8 t = v \u03b8 t (X \u03b8 t )dt, where for any t \u2208 [0, 1], v \u03b8 t = u \u03b8 /(p 0 + tdiv(u \u03b8 )). Note that u \u03b8 also enables density estimation using thatp 1 =p 0 \u2212 div(u \u03b8 ). Density estimation is not directly accessible using RSGM, however in App. L we propose a way to perform such an estimation using Fisher score in a manner akin to Choi et al. (2021). ", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "L Density estimation with Fisher score", "text": "In this section, we show how we can adapt ideas from Choi et al. (2021) for density estimation on M using the Fisher score. The main idea of using Fisher score is to leverage the following decomposition for any x \u2208 M log p 0 (x) = log p T (x) \u2212 T 0 \u2202 t log p t (x)dt. Assume that an approximation\u015d \u03b8 of \u2202 t log p t (the Fisher score) is available then we have that for any\nBefore turning to our main result, we state the following lemma.\nLemma L.1: Assume A1. Then, there exists C, T 0 \u2265 0 such that for any x \u2208 M and\nProof: First, using Lemma C.6, there exists C 0 \u2265 0 such that for any T \u2265 1/2 we have\nUsing (Grigor'yan, 1999, Corollary 5.5), (Hsu, 1999, Theorem 1.2)and the fact that M is compact, there exists\nIn addition, using (Croke, 1980, Proposition 14) we have that there exists C 2 , r 0 > 0 such that for any x 0 \u2208 M and r \u2208 (0, r 0 )\nBy integration by parts we get\nCombining this result and ( 35) we get that\nHence, using Lemma L.3, we could estimate jointly the spatial (or Stein) score used in RSGM and the Fisher score considered in this section, see Choi et al. (2021).", "publication_ref": ["b19", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "M Extensions", "text": "M.1 Schr\u00f6dinger bridge.\nFor Euclidean SGM, the generative model is given by an approximation of the time-reversal of the noising dynamics (X t ) t\u2208[0,T ] while the backward dynamics (Y t ) t\u2208[0,T ] is initialized with the invariant distribution of the noising dynamics (the uniform distribution p ref in case of RSGM). However, in order for the method to yield good results we need L(Y 0 ) \u2248 L(X T ) (see De Bortoli et al., 2021, Theorem 1). Usually, this requires the number of steps in the backward process to be large in order to keep T large and \u03b3 small (where \u03b3 > 0 is the stepsize in the GRW). Another limitation of SGM is that existing methods target an easy-to-sample reference distribution. Hence, classical SGM cannot interpolate between two distributions defined by datasets. To circumvent this problem, one can consider a process whose initial and terminal distribution are pinned down using Schr\u00f6dinger bridges (Schr\u00f6dinger, 1932;L\u00e9onard, 2012a;Chen et al., 2016;De Bortoli et al., 2021;Vargas et al., 2021).", "publication_ref": ["b17", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "M.2 Conditional RSGM.", "text": "Another extension of interest is conditional sampling. By amortizing SGM with respect to an observation y it is possible to approximately sample from a given posterior distribution. In the Euclidean setting this idea has been successfully applied for several image processing problems such as deblurring, denoising or inpainting (see for instance Kawar et al., 2021a;Kawar et al., 2021b;Lee et al., 2021;Sinha et al., 2021;Batzolis et al., 2021;Chung et al., 2021). Similarly, RSGM can be amortized to handle such situations in the case where the underlying posterior distribution is supported on a manifold. Practically, this requires for the score network takes an additional input, i.e s \u03b8 (t, x; y).", "publication_ref": ["b68", "b8", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "M.3 Invariant distributions", "text": "In what follows, we propose an extension for modelling probability distributions which known invariance. That is, we assume that p 0 (\u03c1(g)x) = p 0 (x) for all g \u2208 G, with G a group and \u03c1 : G \u2192 GL n (R) a representation. Following K\u00f6hler et al. (2020), we have that if p ref is invariant w.r.t. G and \u03c6 : M \u2192 M is equivariant w.r.t. to G, then the pushforward probability density p = p ref \u2022 \u03c6 \u22121 is invariant w.r.t. G.", "publication_ref": ["b60"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "NGDC/WDS). NCEI/WDS Global Significant Earthquake Database", "journal": "", "year": "2022", "authors": ""}, {"ref_id": "b1", "title": "NCEI/WDS Global Significant", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Numerical Continuation Methods: An Introduction", "journal": "Springer Science & Business Media", "year": "2012", "authors": "E L Allgower; K Georg"}, {"ref_id": "b3", "title": "Optimal Transport Maps in Monge-Kantorovich Problem", "journal": "", "year": "2003", "authors": "L Ambrosio"}, {"ref_id": "b4", "title": "Spherical Harmonics and Approximations on the Unit Sphere: An Introduction", "journal": "Springer Science & Business Media", "year": "2012", "authors": "K Atkinson; W Han"}, {"ref_id": "b5", "title": "", "journal": "Analysis and Geometry of Markov Diffusion Operators", "year": "2014", "authors": "D Bakry; I Gentil; M Ledoux"}, {"ref_id": "b6", "title": "Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models", "journal": "", "year": "2022", "authors": "F Bao; C Li; J Zhu; B Zhang"}, {"ref_id": "b7", "title": "Pose Estimation Using Linearized Rotations and Quaternion Algebra", "journal": "Acta Astronautica", "year": "2011", "authors": "T Barfoot; J R Forbes; P T Furgale"}, {"ref_id": "b8", "title": "Conditional Image Generation with Score-Based Diffusion Models", "journal": "", "year": "2021", "authors": "G Batzolis; J Stanczuk; C.-B Sch\u00f6nlieb; C Etmann"}, {"ref_id": "b9", "title": "Large deviations and the Malliavin calculus", "journal": "Birkhauser Prog. Math", "year": "1984", "authors": "J.-M Bismut"}, {"ref_id": "b10", "title": "Latent variable modelling with hyperbolic normalizing flows", "journal": "", "year": "2020", "authors": "J Bose; A Smofsky; R Liao; P Panangaden; W Hamilton"}, {"ref_id": "b11", "title": "JAX: composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "J Bradbury; R Frostig; P Hawkins; M J Johnson; C Leary; D Maclaurin; G Necula; A Paszke; J Vanderplas; S Wanderman-Milne; Q Zhang"}, {"ref_id": "b12", "title": "Flows for simultaneous manifold learning and density estimation", "journal": "", "year": "2020", "authors": "J Brehmer; K Cranmer"}, {"ref_id": "b13", "title": "Rectangular flows for manifold learning", "journal": "", "year": "2021", "authors": "A L Caterini; G Loaiza-Ganem; G Pleiss; J P Cunningham"}, {"ref_id": "b14", "title": "Time reversal of diffusion processes under a finite entropy condition", "journal": "", "year": "2021", "authors": "P Cattiaux; G Conforti; I Gentil; C L\u00e9onard"}, {"ref_id": "b15", "title": "Eigenvalues in Riemannian Geometry", "journal": "Academic press", "year": "1984", "authors": "I Chavel"}, {"ref_id": "b16", "title": "Logarithmic heat kernels: estimates without curvature restrictions", "journal": "", "year": "2021", "authors": "X Chen; X M Li; B Wu"}, {"ref_id": "b17", "title": "Entropic and displacement interpolation: a computational approach using the Hilbert metric", "journal": "SIAM Journal on Applied Mathematics", "year": "2016", "authors": "Y Chen; T Georgiou; M Pavon"}, {"ref_id": "b18", "title": "Theory and Algorithms for Diffusion Processes on Riemannian Manifolds", "journal": "", "year": "2022", "authors": "X Cheng; J Zhang; S Sra"}, {"ref_id": "b19", "title": "Density Ratio Estimation via Infinitesimal Classification", "journal": "", "year": "2021", "authors": "K Choi; C Meng; Y Song; S Ermon"}, {"ref_id": "b20", "title": "Come-Closer-Diffuse-Faster: Accelerating Conditional Diffusion Models for Inverse Problems through Stochastic Contraction", "journal": "", "year": "2021", "authors": "H Chung; B Sim; J C Ye"}, {"ref_id": "b21", "title": "", "journal": "", "year": "2021", "authors": "S Cohen; B Amos; Y Lipman"}, {"ref_id": "b22", "title": "Some isoperimetric inequalities and eigenvalue estimates", "journal": "", "year": "1980", "authors": "C B Croke"}, {"ref_id": "b23", "title": "Diffusion Schr\u00f6dinger Bridge with Applications to Score-Based Generative Modeling", "journal": "", "year": "2021", "authors": "V De Bortoli; J Thornton; J Heng; A Doucet"}, {"ref_id": "b24", "title": "Diffusion models beat GAN on Image Synthesis", "journal": "", "year": "2021", "authors": "P Dhariwal; A Nichol"}, {"ref_id": "b25", "title": "A Family of Embedded Runge-Kutta Formulae", "journal": "Journal of Computational and Applied Mathematics", "year": "1980", "authors": "R J Dormand; J P Prince"}, {"ref_id": "b26", "title": "High Dimensional Markov Chain Monte Carlo Methods: Theory, Methods and Application", "journal": "", "year": "2016", "authors": "A Durmus"}, {"ref_id": "b27", "title": "Atmosphere Near real-time Capability for EOS (LANCE) system operated by NASA's Earth Science Data and Information System (ESDIS)", "journal": "", "year": "2020", "authors": " Eosdis;  Land"}, {"ref_id": "b28", "title": "Continuous Normalizing Flows on Manifolds", "journal": "", "year": "2021-03", "authors": "L Falorsi"}, {"ref_id": "b29", "title": "Reparameterizing distributions on lie groups", "journal": "", "year": "2019", "authors": "L Falorsi; P De Haan; T R Davidson; P Forr\u00e9"}, {"ref_id": "b30", "title": "Neural ordinary differential equations on manifolds", "journal": "", "year": "2020", "authors": "L Falorsi; P Forr\u00e9"}, {"ref_id": "b31", "title": "Geometric Measure Theory", "journal": "Springer", "year": "2014", "authors": "H Federer"}, {"ref_id": "b32", "title": "Rigid motion estimation using mixtures of projected Gaussians", "journal": "IEEE", "year": "2013", "authors": "W Feiten; M Lang; S Hirche"}, {"ref_id": "b33", "title": "A Special Stokes's Theorem for Complete Riemannian Manifolds", "journal": "Annals of Mathematics", "year": "1954", "authors": "M P Gaffney"}, {"ref_id": "b34", "title": "Independent SE(3)-Equivariant Models for End-to-End Rigid Protein Docking", "journal": "", "year": "2022", "authors": "O.-E Ganea; X Huang; C Bunne; Y Bian; R Barzilay; T S Jaakkola; A Krause"}, {"ref_id": "b35", "title": "Brenier-Schr\u00f3dinger problem on compact manifolds with boundary. Stochastic Analysis and Applications:1-29", "journal": "", "year": "2021", "authors": "D Garc\u00eda-Zelada; B Huguet"}, {"ref_id": "b36", "title": "Normalizing flows on Riemannian manifolds", "journal": "", "year": "2016", "authors": "M C Gemici; D Rezende; S Mohamed"}, {"ref_id": "b37", "title": "Scalable Reversible Generative Models with Free-Form Continuous Dynamics", "journal": "", "year": "2019", "authors": "W Grathwohl; R T Q Chen; J Bettencourt; D Duvenaud"}, {"ref_id": "b38", "title": "A Kernel Two-Sample Test", "journal": "Journal of Machine Learning Research", "year": "2012", "authors": "A Gretton; K M Borgwardt; M J Rasch; B Sch\u00f6lkopf; A Smola"}, {"ref_id": "b39", "title": "Estimates of heat kernels on Riemannian manifolds", "journal": "", "year": "1999", "authors": "A Grigor'yan"}, {"ref_id": "b40", "title": "Logarithmic Sobolev inequalities on Lie groups", "journal": "Illinois journal of mathematics", "year": "1992", "authors": "L Gross"}, {"ref_id": "b41", "title": "Isometric embeddings of Riemannian manifolds", "journal": "Math. Soc. Japan", "year": "1990", "authors": "M Gunther"}, {"ref_id": "b42", "title": "Time reversal of diffusions", "journal": "The Annals of Probability", "year": "1986", "authors": "U G Haussmann; E Pardoux"}, {"ref_id": "b43", "title": "A lower bound for the first eigenvalue in the Laplacian operator on compact Riemannian manifolds", "journal": "Journal of Geometry and Physics", "year": "2013", "authors": "Y He"}, {"ref_id": "b44", "title": "Denoising diffusion probabilistic models", "journal": "", "year": "2020", "authors": "J Ho; A Jain; P Abbeel"}, {"ref_id": "b45", "title": "Computing CNN Loss and Gradients for Pose Estimation with Riemannian Geometry", "journal": "Springer International Publishing", "year": "2018", "authors": "B Hou; N Miolane; B Khanal; M C H Lee; A Alansary; S Mcdonagh; J V Hajnal; D Rueckert; B Glocker; B Kainz"}, {"ref_id": "b46", "title": "Estimates of derivatives of the heat kernel on a compact Riemannian manifold", "journal": "", "year": "1999", "authors": "E Hsu"}, {"ref_id": "b47", "title": "Stochastic Analysis on Manifolds", "journal": "American Mathematical Society", "year": "2002", "authors": "E P Hsu"}, {"ref_id": "b48", "title": "A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines", "journal": "Communications in Statistics-Simulation and Computation", "year": "1989", "authors": "M F Hutchinson"}, {"ref_id": "b49", "title": "Estimation of non-normalized statistical models by score matching", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "A Hyv\u00e4rinen"}, {"ref_id": "b50", "title": "Stochastic Differential Equations and Diffusion Processes", "journal": "North-Holland Publishing Co", "year": "1989", "authors": "N Ikeda; S Watanabe ; Kodansha; Ltd "}, {"ref_id": "b51", "title": "Adversarial score matching and improved sampling for image generation", "journal": "", "year": "2021", "authors": "A Jolicoeur-Martineau; R Pich\u00e9-Taillefer; R Tachet Des Combes; I Mitliagkas"}, {"ref_id": "b52", "title": "Manifold parametrizations by eigenfunctions of the Laplacian and Heat Kernels", "journal": "Proceedings of the National Academy of Sciences of the United States of America", "year": "2008", "authors": "P W Jones; M Maggioni; R Schul"}, {"ref_id": "b53", "title": "Zeitschrift f\u00fcr Wahrscheinlichkeitstheorie und verwandte Gebiete", "journal": "", "year": "1975", "authors": "E J\u00f8rgensen"}, {"ref_id": "b54", "title": "Machine learning for the geosciences: Challenges and opportunities", "journal": "IEEE Transactions on Knowledge and Data Engineering", "year": "2018", "authors": "A Karpatne; I Ebert-Uphoff; S Ravela; H A Babaie; V Kumar"}, {"ref_id": "b55", "title": "SNIPS: Solving Noisy Inverse Problems Stochastically", "journal": "", "year": "2021", "authors": "B Kawar; G Vaksman; M Elad"}, {"ref_id": "b56", "title": "Stochastic Image Denoising by Sampling from the Posterior Distribution", "journal": "", "year": "2021", "authors": "B Kawar; G Vaksman; M Elad"}, {"ref_id": "b57", "title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "D P Kingma; J Ba"}, {"ref_id": "b58", "title": "Poincar\u00e9 maps for analyzing complex hierarchies in single-cell data", "journal": "Nature communications", "year": "2020", "authors": "A Klimovskaia; D Lopez-Paz; L Bottou; M Nickel"}, {"ref_id": "b59", "title": "Numerical Solution of Stochastic Differential Equations. Stochastic Modelling and Applied Probability", "journal": "Springer", "year": "2011", "authors": "P Kloeden; E Platen"}, {"ref_id": "b60", "title": "Equivariant Flows: Exact Likelihood Generative Learning for Symmetric Densities", "journal": "", "year": "2020-06", "authors": "J K\u00f6hler; L Klein; F No\u00e9"}, {"ref_id": "b61", "title": "Information Theory and Statistics", "journal": "Dover Publications, Inc", "year": "1997", "authors": "S Kullback"}, {"ref_id": "b62", "title": "Stratonovich stochastic differential equations driven by general semimartingales", "journal": "", "year": "1995", "authors": "T G Kurtz; \u00c9 Pardoux; P Protter"}, {"ref_id": "b63", "title": "Convergence of time-inhomogeneous geodesic random walks and its application to coupling methods. The Annals of Probability", "journal": "", "year": "2012", "authors": "K Kuwada"}, {"ref_id": "b64", "title": "Introduction to Topological Manifolds", "journal": "Springer Science & Business Media", "year": "2010", "authors": "J Lee"}, {"ref_id": "b65", "title": "Introduction to Riemannian manifolds", "journal": "Springer", "year": "2018", "authors": "J M Lee"}, {"ref_id": "b66", "title": "Riemannian Manifolds: An Introduction to Curvature", "journal": "Springer Science & Business Media", "year": "2006", "authors": "J M Lee"}, {"ref_id": "b67", "title": "Smooth Manifolds. In Introduction to Smooth Manifolds", "journal": "Springer", "year": "2013", "authors": "J M Lee"}, {"ref_id": "b68", "title": "PriorGrad: Improving Conditional Denoising Diffusion Models with Data-Driven Adaptive Prior", "journal": "", "year": "2021", "authors": "S Lee; H Kim; C Shin; X Tan; C Liu; Q Meng; T Qin; W Chen; S Yoon; T.-Y Liu"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Trained score-based generative models on earth sciences data. The learned density is colored green-blue. Blue and red dots represent training and testing datapoints, respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Comparison of Moser flows and RSGMs training speed and performance on the synthetic highdimension torus task. Moser flows trained with \u03bbmin = 1. We report two likelihoods, the 'Moser' closed form density-not guaranteed to be normalized-and the 'ODE' likelihood given by solving an augmented ODE (as in CNFs) with the vector field induced by the Moser flow density-which is guaranteed to have unit volume.", "figure_data": ""}, {"figure_label": "a", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "( a )aHistograms of SO3(R) samples from a target mixture distribution with M = 4 components, represented via their Euler angles.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Trained score-based generative models on synthetic SO3(R) data.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Samples from different probability distributions on H 2 coloured w.r.t their density.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "in the same spirit as De Bortoli et al. (2021) on Euclidean state spaces. G. Leobacher and A. Steinicke. Existence, uniqueness and regularity of the projection onto differentiable manifolds. Annals of Global Analysis and Geometry, 60(3):559-587, 2021. Cited on page 3. C. L\u00e9onard. From the Schr\u00f6dinger problem to the Monge-Kantorovich problem. Journal of Functional Analysis, 262(4):1879-1920, 2012. Cited on page 29. C. L\u00e9onard. Girsanov theory under a finite entropy condition. In S\u00e9minaire de Probabilit\u00e9s XLIV, pages 429-465. Springer, 2012. Cited on page 15. C. L\u00e9onard, S. Roelly, J.-C. Zambrini, et al. Reciprocal processes: a measure-theoretical point of view. Probability Surveys, 11:237-269, 2014. Cited on page 13. P. Li. Large time behavior of the heat equation on complete manifolds with non-negative Ricci curvature. Annals of Mathematics, 124(1):1-21, 1986. Cited on page 7. R. S. Liptser and A. N. Shiryaev. Statistics of Random Processes. I, volume 5 of Applications of Mathematics (New York). Springer-Verlag, Berlin, expanded edition, 2001, pages xvi+427. Cited on page 16. Y. M. Lui. Advances in matrix manifolds for computer vision. Image and Vision Computing, 30(6-7):380-388, 2012. Cited on page 1. E. Mathieu, C. L. Lan, C. J. Maddison, R. Tomioka, and Y. W. Teh. Continuous Hierarchical Representations with Poincar\u00e9 Variational Auto-Encoders. arXiv preprint arXiv:1901.06033, 2019. Cited on page 3. E. Mathieu and M. Nickel. Riemannian Continuous Normalizing Flows. In Advances in Neural Information Processing Systems 33. Curran Associates, Inc., 2020. Cited on pages 2, 7, 8. N. Miolane, N. Guigui, A. L. Brigant, J. Mathe, B. Hou, Y. Thanwerdas, S. Heyder, O. Peltre, N. Koep, H. Zaatiti, H. Hajri, Y. Cabanes, T. Gerald, P. Chauchat, C. Shewmake, D. Brooks, B. Kainz, C. Donnat, S. Holmes, and X. Pennec. Geomstats: A Python Package for Riemannian Geometry in Machine Learning. Journal of Machine Learning Research, 21(223):1-9, 2020. Cited on pages 11, 30. A. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. arXiv preprint arXiv:2102.09672, 2021. Cited on page 5. G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. arXiv preprint arXiv:1912.02762, 2019. Cited on page 7. D. Peel, W. J. Whiten, and G. J. McLachlan. Fitting mixtures of Kent distributions to aid in joint set identification. Journal of the American Statistical Association, 96(453):56-63, 2001. Cited on pages 1, 8, 31. X. Pennec. Intrinsic Statistics on Riemannian Manifolds: Basic Tools for Geometric Measurements. Journal of Mathematical Imaging and Vision, 25(1):127-154, 2006. Cited on page 3. S. Prokudin, P. Gehler, and S. Nowozin. Deep Directional Statistics: Pose Estimation with Uncertainty Quantification. In European Conference on Computer Vision (ECCV), Oct. 2018. Cited on page 32. D. Revuz and M. Yor. Continuous Martingales and Brownian Motion, volume 293 of Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences]. Springer-Verlag, Berlin, third edition, 1999, pages xiv+602. Cited on page 3. D. J. Rezende and S. Racani\u00e8re. Implicit Riemannian concave potential maps. arXiv preprint arXiv:2110.01288, 2021. Cited on page 7. D. M. Roy, C. Kemp, V. Mansinghka, and J. B Tenenbaum. Learning annotated hierarchies from relational data, 2007. Cited on page 1. N. Rozen, A. Grover, M. Nickel, and Y. Lipman. Moser Flow: Divergence-based Generative Modeling on Manifolds. Advances in Neural Information Processing Systems, 2021. Cited on pages 2, 7, 8, 1, 19, 26, 33. O. Yadan. Hydra -A framework for elegantly configuring complex applications. Github, 2019. Cited on page 11. Checklist 1. For all authors... (a) Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? [Yes] Our main contribution is the extension of diffusion models on Riemannian manifolds. (b) Did you describe the limitations of your work? [Yes] See Sec. 7. (c) Did you discuss any potential negative societal impacts of your work? [No] The work presented in this paper focuses on the learning of score-based models on manifold. We do not foresee any immediate societal impact of such a study. (d) Have you read the ethics review guidelines and ensured that your paper conforms to them? [Yes] We have read the ethics review guidelines and our paper conforms to them. 2. If you are including theoretical results... (a) Did you state the full set of assumptions of all theoretical results? [Yes] Yes, see A1. (b) Did you include complete proofs of all theoretical results? [Yes] Yes, proofs are postponed to the supplementary material. 3. If you ran experiments... (a) Did you include the code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL)? [Yes] Experimental details are given in App. O. (b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] Experimental details are given in App. O. (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Error bars are reported for each experiment. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] Experimental details are given in App. O. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] See Sec. 6.1. (b) Did you mention the license of the assets? [Yes] See App. O. (c) Did you include any new assets either in the supplemental material or as a URL? [No] Not applicable. (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [No] Not applicable. (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [No] Not applicable. 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [No] Not applicable. (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [No] Not applicable. (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [No] Not applicable.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Rozen et al. (2021)  in App. K. We show how our method can be adapted to perform density estimation in App. L. Extensions to conditional SGM and Schr\u00f6dinger Bridges are discussed in App. M. In Sec. 3.1, we briefly discuss the non compact setting. Details on the stereographic SGM are given in App. N. Experimental details are given in App. O.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "unpp(M) = {x \u2208 R d : there exists a unique \u03be x such that x \u2212 \u03be x = d(x, M)}. Let E(M) = int(unpp(M)). By Leobacher and Steinicke (2021, Theorem 1), we have M \u2282 E(M). We definep : E(M) \u2192 M such that for any x \u2208 E(M),p(x) = \u03be x . Using Leobacher and Steinicke (2021, Theorem 2), we havep \u2208 C \u221e (R p , M) and for any x \u2208 M,P (x) = dp(x) is the orthogonal projection on T x M. Since R p is normal and M and E(M) c are closed, there exists F open such that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "We denote p m ref the Haussdorff measure of the manifold (which coincides with the measure associated with the Riemannian volume form (see Federer, 2014, Theorem 2.10.10) and p ref = p m ref /p ref (M) the associated probability measure.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "where for any f \u2208 C \u221e we have E[f (B M,x t )] = M p t|0 (x, y)f (y)dp ref (y), where (B M,x t ) t\u22650 is the Brownian motion on M with B M,x 0 = x and p ref is the probability measure associated with the Haussdorff measure on M. We also have the following result (see Urakawa, 2006, Proposition 2.6).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 1 :1Figure 1: Slice of heat kernel p t|0 (xt|x0) on S 2 for different approximations.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Figure 2 :2Figure 2: Illustration of the effect of the corrector step on RSGM. The black line corresponds to the dynamics of the noising process (pt) t\u2208[0,T ] . The blue dashed lines correspond to the predictor step (going backward in time) and the red dashed lines correspond to the corrector step (projecting back onto the initial dynamics). Note that L(Y s \u03b3 ) \u2248 pT \u2212\u03b3 and L(Y s 2\u03b3 ) \u2248 pT \u22122\u03b3 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": ": \u03b8n+1 = optimizer_update(\u03b8n, (\u03b8n)) ADAM optimizer step 8: \u03b8 = \u03b8N epoch 9: /// SAMPLING /// 10: Y0 \u223c pref Sample from uniform distribution 11: b \u03b8 (t, x) = s \u03b8 (T \u2212 t, x) for any t \u2208 [0, T ], x \u2208 M Reverse process drift 12: {Y k } N k=0 = GRW-c(T, N, Y0, b \u03b8 , Id, P) Approximate reverse diffusion with Algorithm 3 13: return \u03b8 , {Y k } N k=0", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Let h : [0, T \u2212 s] \u00d7 M \u2192 R given for any u \u2208 [0, T \u2212 s] and x \u2208 M by h(u, x) = v(u, x)f (x). Using (13), we have for any u \u2208 [0, T \u2212 s] and x \u2208 M", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "(a) (\u0100 P u(t, X [0,t] )) t\u2208[0,T ] is adapted w.r.t. (F t ) t\u2208[0,T ] . (b) T 0 |\u0100 P u(t, X [0,t] )|dt < +\u221e, P-a.s. (c) The process (M t ) t\u2208[0,T ] is a P-local martingale, where for any t \u2208 [0, T ]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": ")Let x 0 \u2208 M. Let Lip(M) the set of Lipschitz functions on M with Lipschitz constant equal to 1. Let Lip(M) 0 the set of Lipschitz functions on M with Lipschitz constant equal to 1 and such that for any \u03d5 \u2208 Lip(M) 0 , \u03d5(x 0 ) = 0. Note that in this case, we have that \u03d5 \u221e \u2264 diam(M). Using (18), we haveW 1 (L(Y N ), p 0 ) = sup{E[\u03d5(Y N )] \u2212 M \u03d5(x)p 0 (x)dp ref (x) : \u03d5 \u2208 Lip(M)} = sup{E[\u03d5(Y N )] \u2212 M \u03d5(x)p 0 (x)dp ref (x) : \u03d5 \u2208 Lip(M) 0 } \u2264 C(e \u03bb1/2 diam(M)e \u2212\u03bb1T + T /2diam(M)M + e T \u03b3 1/2 ),which concludes the proof.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Theorem I. 2 :2Assume A1, that p 0 is smooth and positive and that there exists M \u2265 0 such that for anyt \u2208 [0, T ] and x \u2208 M, s \u03b8 (t, x) \u2212 \u2207 log p t (x) \u2264 M, with s \u03b8 \u2208 C([0, T ] , X (M)). Then if T > 1/2, there exists C \u2265 0 independent on T such that L(\u0176 T ) \u2212 p 0 TV = C(e \u2212\u03bb1T + T /2M).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Forany x \u2208B(x 0 , \u03ba(M/2)), we have |p T (x) \u2212 p 0 (x)| \u2265 M/2. Hence, denoting Vol \u03ba = B (x0,\u03ba(M/2)) dp ref (x) > 0, we have (2/Vol \u03ba ) M |p T (x) \u2212 p 0 (x)|dp ref (x) \u2265 p T \u2212 p 0 \u221e .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Interpolated density between the reference pref = N(0, 1) and target p0 = N(8, 1) distributions. Interpolated histograms between the reference pref = N(0, 1) and target p0 = N(8, 1) distributions. Expected norm of the Stein score along trajectories interpolating between reference and target p0 = N(a, 1) distributions for different target mean.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "Figure 3 :3Figure 3: The reference distribution is pref = N(0, 1).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": ", Proposition 2), we have the following property. Proposition L.3: Let\u015d such that for any t \u2208 [0, T ] and x \u2208 M,\u015d(t, x) = \u2202 t log p t (x). Then, we have that\u015d = arg min{L(s) : s \u2208 C \u221e ([0, T ] \u00d7 M, R)}, where for any s \u2208 C \u221e ([0, T ] \u00d7 M, R) we have", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_22", "figure_caption": "Figure 4 :4Figure 4: Ablation study on the impact of the forward sampling approximation quality on S 2 .", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_23", "figure_caption": "Figure 5 :5Figure5: Ablation study on the denoising score matching (DSM) loss t|0 when combining the heat kernel truncation and the Varadhan approximation: \u2207x t log p t|0 (xt|x0) \u2248 1(t \u2264 \u03c4 ) exp \u22121x t (x0) + 1(t > \u03c4 )SJ,t(x0, xt).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "\u2208 [0, T ] with t > s and x t \u2208 M \u2207 xt log p t (x t ) = M \u2207 xt log p t|s (x t |x s )P s|t (x t , dx s ).Hence, for any s, t \u2208 [0, T ] with t > s we have that \u2207 log p t = arg min{ t|s (s t ) : s t \u2208 L 2 (P t )}, where t|s (s t ) = M 2 \u2207 x log p t|s (x t |x s ) \u2212 s t (x t ) 2 dP s,t (x s , x t ), which is referred as the Denoising Score Matching (DSM) loss. It can also be written in an implicit fashion.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Differences between SGM on Euclidean spaces and RSGM on Riemannian manifolds.", "figure_data": "Ingredient \\ SpaceEuclidean'Generic' ManifoldCompact ManifoldForward process dXt =\u2212 1 2 Xtdt + dB M t\u2212 1 2 \u2207 X t U (Xt)dt + dB M tdB M tEasy-to-sample distributionGaussianWrapped GaussianUniformTime reversalCattiaux et al. (2021)Theorem 3.1Sampling forward processDirectGeodesic Random Walk (Algorithm 1)Sampling backward processEuler-MaruyamaGeodesic Random Walk (Algorithm 1)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": "LossApproximationLoss functionRequirementsComplexity"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Summary of computational complexity (w.r.t. neural network forward and backward passes) for different methods. d is the manifold dimension, k the number of Monte Carlo batches in Moser flow's regularizer, N is the number of steps in the (adaptive) ODE solver, whereas N * is the number of steps in the SDE Euler-Maruyama solver-which can usually be lower than N . Moser flow and RSGM training complexity varies if the Hutchinson stochastic estimator is used. See Table 2 for score matching losses complexity.", "figure_data": "MethodTrainingLikelihood evaluationSamplingRCNFSolving ODE O(dN )Solving augmented ODE O(dN ) Solving ODE O(N )Moser flow Computing div O(dk) or O(k) Solving augmented ODE O(dN ) Solving ODE O(N )RSGMScore matching O(d) or O(1) Solving augmented ODE O(dN ) Solving SDE O(N"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Negative log-likelihood scores for each method on the earth and climate science datasets. Bold indicatesbest results (up to statistical significance). Means and confidence intervals are computed over 5 different runs.Novel methods are shown with blue shading.MethodVolcanoEarthquakeFloodFireMixture of Kent\u22120.80\u00b10.470.33\u00b10.050.73\u00b10.07\u22121.18\u00b10.06Riemannian CNF\u22126.05 \u00b10.610.14\u00b10.231.11\u00b10.19 \u22120.80 \u00b10.54Moser Flow\u22124.21\u00b10.17 \u22120.16 \u00b10.06 0.57 \u00b10.10 \u22121.28 \u00b10.05Stereographic Score-Based\u22123.80\u00b10.27 \u22120.19 \u00b10.05 0.59 \u00b10.07 \u22121.28 \u00b10.12Riemannian Score-Based\u22124.92\u00b10.25 \u22120.19 \u00b10.07 0.45 \u00b10.17 \u22121.33 \u00b10.06Dataset size8276120487512809"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Test log-likelihood and associated number of function evaluations (NFE) in 10 3 on the synthetic mixture distribution with M components on SO3(R). Bold indicates best results (up to statistical significance). Means and standard deviations are computed over 5 different runs. Novel methods are shown with blue shading.", "figure_data": "MethodM = 16M = 32M = 64log-likelihoodNFE log-likelihoodNFE log-likelihoodNFEMoser Flow0.85\u00b10.032.3\u00b10.50.17\u00b10.032.3\u00b10.9 \u22120.49 \u00b10.027.3\u00b11.4Exp-wrapped SGM0.87 \u00b10.040.5\u00b10.10.16\u00b10.030.5\u00b10.0\u22120.58\u00b10.040.5\u00b10.0RSGM0.89 \u00b10.03 0.1 \u00b10.00.20 \u00b10.03 0.1 \u00b10.0 \u22120.49 \u00b10.02 0.1 \u00b10.0"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Riemannian score matching losses.", "figure_data": "LossApproximationLoss functionUnbiased Consistent Variancet|0 (DSM)Truncation ("}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Recall that we have that P R |0 is associated with the process dY t = \u2207 log p T \u2212t (Y t )dt + dB M", "figure_data": "tand thatP R |0 is associated with the process d\u0176 t = s \u03b8 (T \u2212 t,\u0176 t )dt + dB M t . Using Corollary H.3 we have thatKL(\u03c0 \u221e P R |0 |\u03c0 \u221eP"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": ". . , 2 \u2212 1}, E[\u03b1 k |F k ] =\u1fb1 k and E[\u03b2 k |F k ] =\u03b2 k are constant, where F k = \u03c3({B t : t \u2208 [0, k\u03b3 ]}). Therefore, we get that for any k \u2208 {0, . . . , 2 \u2212 1}", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "t )p t|s (x t |x s )dp ref (x t ) p s (x s )dp ref (x s ) by the divergence theorem = \u2212 M\u00d7M div(s t )(x t )dP s,t (x s , x t ) = \u2212 M\u00d7M \u2207 log p t|s (x t |x s ) 2 dP s,t (x s , x t ) +", "figure_data": "div(s t )(x t )dP t (x t )M\u00d7MThereforet|s (s t ) ="}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Test log-likelihood of trained RSGMs on the Flood dataset while varying the number of discretization steps N when simulating forward sampling Xt|X0.", "figure_data": "10 2 10 1 t |X MMD(X 0 , X t |X 0 )N=1 N=2 N=5 N=50 N=100 N=1000Test log-likelihood1.50 1.25 1.00 0.75 0.500.00.20.4t0.60.81.025 10 20 Discretization steps N 50 100 200(a) Maximum mean discrepancy (MMD) distance be-(b)tween 'exact' (i.e. approximated with N = 1000 steps)Xt|X0 and approximateXt|X0 at for every t \u2208 [0, 1]."}], "formulas": [{"formula_id": "formula_0", "formula_text": "dX t = \u2212X t dt + \u221a 2dB t , X 0 \u223c p 0 ,(1)", "formula_coordinates": [2.0, 228.13, 223.76, 275.87, 26.01]}, {"formula_id": "formula_1", "formula_text": "dY t = {Y t + 2\u2207 log p T \u2212t (Y t )}dt + \u221a 2dB t , Y 0 \u223c p T ,(2)", "formula_coordinates": [2.0, 185.07, 304.67, 318.93, 26.01]}, {"formula_id": "formula_2", "formula_text": "Y n+1 = Y n + \u03b3{Y n + 2s \u03b8 (T \u2212 n\u03b3, Y n )} + 2\u03b3Z n+1 , Y 0 \u223c N(0, Id), Z n i.i.d. \u223c N(0, Id).", "formula_coordinates": [2.0, 119.21, 502.62, 373.59, 19.75]}, {"formula_id": "formula_3", "formula_text": "dX t = \u2212 1 2 \u2207 Xt U (X t )dt + dB M t ,(3)", "formula_coordinates": [3.0, 235.06, 124.4, 268.94, 19.13]}, {"formula_id": "formula_4", "formula_text": "dY t = {\u2212b(Y t ) + \u2207 log p T \u2212t (Y t )}dt + dB M t .(4)", "formula_coordinates": [3.0, 207.19, 518.57, 292.56, 19.13]}, {"formula_id": "formula_5", "formula_text": "W k+1 = \u03b3b(k\u03b3, X \u03b3 k ) + \u221a \u03b3\u03c3(k\u03b3, X \u03b3 k )Z k+1", "formula_coordinates": [4.0, 138.39, 299.61, 162.8, 15.45]}, {"formula_id": "formula_6", "formula_text": "\u03b3 k+1 = exp X \u03b3 k [W k+1 ]", "formula_coordinates": [4.0, 146.0, 312.95, 79.33, 14.1]}, {"formula_id": "formula_7", "formula_text": "X \u03b3 k on M 6: return {X \u03b3 k } N k=0", "formula_coordinates": [4.0, 112.98, 312.95, 391.02, 26.84]}, {"formula_id": "formula_8", "formula_text": "dX t = b(t, X t )dt + \u03c3(t, X t )dB M t .(5)", "formula_coordinates": [4.0, 233.59, 378.05, 270.41, 13.77]}, {"formula_id": "formula_9", "formula_text": "\u2208 N, E[V n+1 |F n ] = 0 and E[V n+1 V n+1 |F n ] = \u03c3\u03c3 (X \u03b3 n )", "formula_coordinates": [4.0, 112.25, 440.16, 387.75, 29.23]}, {"formula_id": "formula_10", "formula_text": "X0 \u223c (1/M ) M m=1 \u03b4 X m 0 Random mini-batch from dataset 4: t \u223c U ([\u03b5, T ])", "formula_coordinates": [5.0, 112.98, 121.14, 391.02, 21.43]}, {"formula_id": "formula_11", "formula_text": "b \u03b8 (t, x) = \u2212b(T \u2212 t, x) + s \u03b8 (T \u2212 t, x) for any t \u2208 [0, T ], x \u2208 M Reverse process drift 12: {Y k } N k=0 = GRW(T, N, Y0, b \u03b8 , Id, P) Approximate reverse diffusion with Algorithm 1 13: return \u03b8 , {Y k } N k=0", "formula_coordinates": [5.0, 108.5, 206.46, 396.18, 31.43]}, {"formula_id": "formula_12", "formula_text": "{E i } d i=1 such that span {E i (x)} d i=1 = T x M (Chapter 8, page 179,", "formula_coordinates": [5.0, 107.64, 411.15, 396.36, 31.24]}, {"formula_id": "formula_13", "formula_text": "p t|0 (x t |x 0 ) = j\u2208N e \u2212\u03bbj t \u03c6 j (x 0 )\u03c6 j (x t ),(6)", "formula_coordinates": [6.0, 223.35, 223.7, 280.65, 18.63]}, {"formula_id": "formula_14", "formula_text": "x 0 , x t \u2208 M \u2207 xt log p t|0 (x t |x 0 ) \u2248 S J,t (x 0 , x t ) \u2207 xt log J j=0 e \u2212\u03bbj t \u03c6 j (x 0 )\u03c6 j (x t ).(7)", "formula_coordinates": [6.0, 149.23, 289.78, 354.77, 36.59]}, {"formula_id": "formula_15", "formula_text": "lim t\u21920 t\u2207 xt log p t|0 (x t |x 0 ) = exp \u22121 xt (x 0 ).(8)", "formula_coordinates": [6.0, 220.03, 431.3, 283.97, 18.63]}, {"formula_id": "formula_16", "formula_text": "p t|0 exp \u22121 X t t|0 (DSM) None 1 2 E s(Xt) \u2212 \u2207 log p t|0 (Xt|X0) 2 O(1) Truncation (7) 1 2 E s(Xt) \u2212 SJ,t(X0, Xt) 2 asymptotic expansion O(1) Varhadan (8) 1 2 E s(Xt) \u2212 exp \u22121 X t (X0)/t 2 O(1) t|s (DSM) Varhadan (8) 1 2 E s(Xt) \u2212 exp \u22121 X t (Xs)/(t \u2212 s) 2 O(1) im t (ISM) Deterministic E 1 2 s(Xt) 2 + div(s)(Xt) O(d) Stochastic E 1 2 s(Xt) 2 + \u03b5 \u2202s(Xt)\u03b5 O(1)", "formula_coordinates": [6.0, 118.87, 537.4, 363.16, 120.36]}, {"formula_id": "formula_17", "formula_text": "t \u2208 [0, T ] and x \u2208 M, s \u03b8 (t, x) \u2212 \u2207 log p t (x) \u2264 M, with s \u03b8 \u2208 C([0, T ] , X (M)). Then if T > 1/2, there exists C \u2265 0 independent on T such that W 1 (L(Y N ), p 0 ) = C(e \u2212\u03bb1T + T /2M + e T \u03b3 1/2 ),", "formula_coordinates": [7.0, 112.25, 197.59, 387.5, 41.64]}, {"formula_id": "formula_18", "formula_text": "T d = S 1 \u00d7 \u2022 \u2022 \u2022 \u00d7 S 1 ,", "formula_coordinates": [8.0, 307.66, 645.7, 83.0, 18.41]}, {"formula_id": "formula_19", "formula_text": "f : M \u2192 R, x \u2208 M, v \u2208 T x M, \u2207f, v g = df (v). The distane d M (x, y", "formula_coordinates": [18.0, 108.0, 459.5, 396.0, 26.76]}, {"formula_id": "formula_20", "formula_text": "x : UM \u2192 M with U \u2282 T x M is such that exp x (v) = \u03b3(1) with \u03b3(1)", "formula_coordinates": [18.0, 224.89, 514.05, 280.27, 17.29]}, {"formula_id": "formula_21", "formula_text": "(x) = |g(x)| 1/2 dx 1 \u2227 . . . dx d .", "formula_coordinates": [18.0, 349.45, 546.74, 125.47, 19.8]}, {"formula_id": "formula_22", "formula_text": "T k, M = p\u2208M T k, (T p M). Note that \u0393(M, T 0,0 M) = C \u221e (M), X (M) = \u0393(M, T 1,0 M) and that the space of 1-form on M is given by \u0393(M, T 0,1 M), where \u0393(M, V (M)", "formula_coordinates": [18.0, 383.59, 711.15, 122.16, 18.41]}, {"formula_id": "formula_23", "formula_text": "f \u2208 C \u221e (M), X, Y \u2208 X (M), \u2207 f X (Y ) = f \u2207 X Y , ii) for any f \u2208 C \u221e (M), X, Y \u2208 X (M), \u2207 X (f Y ) = f \u2207 X Y + X(f )Y .", "formula_coordinates": [19.0, 108.0, 208.82, 397.25, 29.54]}, {"formula_id": "formula_24", "formula_text": "\u2207 Xi X j = d k=1 \u0393 k i,j X k .", "formula_coordinates": [19.0, 376.48, 231.89, 107.35, 19.8]}, {"formula_id": "formula_25", "formula_text": "\u2207 X Y \u2212 \u2207 Y X = [X, Y ], where [X, Y ] is the Lie bracket between X and Y , ii) \u2207 is compatible with the metric g, i.e. for any X, Y, Z \u2208 X (M), X( Y, Z M ) = \u2207 X Y, Z M + Y, \u2207 X Z M .", "formula_coordinates": [19.0, 108.0, 256.23, 397.25, 39.1]}, {"formula_id": "formula_26", "formula_text": "2 \u2207 X Y, Z M = X( Y, Z M ) + Y ( Z, X M ) \u2212 Z( X, Y M ) + [X, Y ], Z M \u2212 [Z, X], Y M \u2212 [Y, Z], X M .", "formula_coordinates": [19.0, 154.62, 306.97, 302.77, 31.18]}, {"formula_id": "formula_27", "formula_text": "\u0393 k i,j = 1 2 d m=1 g km (\u2202 j g m,i + \u2202 i g m,j \u2212 \u2202 m g i,j ),", "formula_coordinates": [19.0, 206.17, 356.81, 199.67, 19.8]}, {"formula_id": "formula_28", "formula_text": "[0, 1] by X = d i=1 a i (t)E i (t) (assuming that \u03b3([0, 1]", "formula_coordinates": [19.0, 108.0, 538.25, 396.0, 23.64]}, {"formula_id": "formula_29", "formula_text": "a k (t) + d i,j=1 \u0393 k i,j (x(t))\u1e8b i (t)a j (t) = 0.(1)", "formula_coordinates": [19.0, 222.03, 571.17, 281.97, 14.97]}, {"formula_id": "formula_30", "formula_text": "x k (t) + d i,j=1 \u0393 k i,j (x(t))\u1e8b i (t)\u1e8b j (t) = 0.", "formula_coordinates": [19.0, 221.61, 607.61, 168.79, 14.97]}, {"formula_id": "formula_31", "formula_text": "\u0393 t 0 : T x M \u2192 T \u03b3(t) M the linear isomorphism such that \u0393 t 0 (v) = X v (\u03b3(t)). For any x \u2208 M and v \u2208 T x M we denote \u03b3 x,v : [0, \u03b5 x,v ] the geodesics (defined on the maximal interval [0, \u03b5 x,v ]) on M such that \u03b3(0) = x and\u03b3(0) = v. We denote U x = {v \u2208 T x M : \u03b5 x,v \u2265 1}.", "formula_coordinates": [19.0, 108.0, 671.85, 397.74, 46.81]}, {"formula_id": "formula_32", "formula_text": "M \u2282 F \u2282 E(M). Let p \u2208 C \u221e (R p , R p", "formula_coordinates": [20.0, 126.56, 281.36, 168.38, 18.63]}, {"formula_id": "formula_33", "formula_text": "[X, Y] t = X t Y t \u2212 X 0 Y 0 \u2212 t 0 X s dY s \u2212 t 0 Y s dX s .", "formula_coordinates": [20.0, 195.68, 417.75, 220.63, 20.36]}, {"formula_id": "formula_34", "formula_text": "t 0 X s \u2022 dY s = t 0 X s dY s + 1 2 [X, Y] t . In particular, denoting (Z 1", "formula_coordinates": [20.0, 108.0, 488.68, 278.72, 29.98]}, {"formula_id": "formula_35", "formula_text": "Z 1 t = t 0 X s \u2022dY s and Z 2 t = t 0 X s dY s , we have that [Z 1 ] = [Z 2 ].", "formula_coordinates": [20.0, 108.0, 505.38, 395.5, 30.47]}, {"formula_id": "formula_36", "formula_text": "X t = t 0 f (X s ) \u2022 dY s with C 1 (R, R), then [X, Y] t = t 0 f (X s )f (X s )dY s . Assuming that f \u2208 C 3 (R, R)", "formula_coordinates": [20.0, 108.0, 534.16, 396.0, 34.75]}, {"formula_id": "formula_37", "formula_text": "f (X t ) = f (X 0 ) + t 0 f (X s ) \u2022 dX s .", "formula_coordinates": [20.0, 230.99, 576.65, 150.01, 20.36]}, {"formula_id": "formula_38", "formula_text": "(X t ) t\u2208[0,T ] is a strong solution to dX t = b(t, X t )dt + \u03c3(t, X t ) \u2022 dB t , with b \u2208 C \u221e (R d , R d ) and \u03c3 \u2208 C \u221e (R d , R d\u00d7d ).", "formula_coordinates": [20.0, 106.83, 638.22, 397.17, 31.63]}, {"formula_id": "formula_39", "formula_text": "dX t = {b(t, X t ) +b(X t )}dt + \u03c3(t, X t )dB t ,b = (\u22121/2)[div(\u03c3\u03c3 ) \u2212 \u03c3div(\u03c3 )]. (2)", "formula_coordinates": [20.0, 122.49, 669.19, 381.52, 17.29]}, {"formula_id": "formula_40", "formula_text": "x \u2208 R d , div(A) i (x) = d j=1 \u2202 j A i,j (x). In particular, note that if for x 0 \u2208 R d , \u03c3(x 0 ) is an orthogonal projection, then \u03c3(x 0 )b(x 0 ) = 0.", "formula_coordinates": [20.0, 108.0, 696.13, 396.0, 27.1]}, {"formula_id": "formula_41", "formula_text": "f (X t ) = f (X 0 ) + i=1 t 0 V i (f )(X s ) \u2022 dZ i s .", "formula_coordinates": [21.0, 211.99, 146.13, 188.02, 20.36]}, {"formula_id": "formula_42", "formula_text": "Let f \u2208 C \u221e (M). We define \u2207f \u2208 X (M) such that for any X \u2208 X (M) we have X, \u2207f M = X(f ). Let {X i } d i=1 \u2208 X (M) d such that for any x \u2208 M, {X i (x)} d", "formula_coordinates": [21.0, 108.0, 368.05, 396.35, 42.63]}, {"formula_id": "formula_43", "formula_text": "X (M) \u2192 C \u221e (M) (linear) such that for any X \u2208 X (M), div(X) = d i=1 \u2207 Xi X, X i M .", "formula_coordinates": [21.0, 107.67, 392.05, 397.49, 32.08]}, {"formula_id": "formula_44", "formula_text": "X \u2208 X (M), M div(X)(x)f (x)dp ref (x) = \u2212 M X(f )(x)dp ref (x). Let X = d i=1 a i X i in local coordinates.", "formula_coordinates": [21.0, 108.0, 428.57, 396.0, 23.3]}, {"formula_id": "formula_45", "formula_text": "= d i,j=1 g i,j \u2202 i f X j , div(X) = det(G) \u22121/2 d i=1 \u2202 i (det(G) 1/2 a i ). The Laplace-Beltrami operator is given by \u2206 M : C \u221e (M ) \u2192 C \u221e (M ) and for any f \u2208 C \u221e (M ) by \u2206 M (f ) = div(grad(f )). In local coordinates we obtain \u2206 M (f ) = det(G) \u22121/2 d i=1 \u2202 i (det(G) 1/2 d j=1 g i,j \u2202 j f ).", "formula_coordinates": [21.0, 107.69, 465.9, 396.66, 56.64]}, {"formula_id": "formula_46", "formula_text": "\u2206 OM : C \u221e (OM) \u2192 C \u221e (OM) as \u2206 OM = d i=1 H 2", "formula_coordinates": [21.0, 233.06, 554.54, 226.38, 19.8]}, {"formula_id": "formula_47", "formula_text": "f \u2208 C \u221e (M), \u2206 OM (f \u2022 \u03c0) = \u2206 M (f ) (see", "formula_coordinates": [21.0, 283.07, 577.53, 175.76, 18.63]}, {"formula_id": "formula_48", "formula_text": "Definition C.1 (Brownian motion): Let (B M t ) t\u22650 be a M-valued semimartingale. (B M t ) t\u22650 is a Brownian motion on M if for any f \u2208 C \u221e (M), (M f t )", "formula_coordinates": [21.0, 112.25, 666.61, 387.5, 32.05]}, {"formula_id": "formula_49", "formula_text": "M f t = f (B M t ) \u2212 f (B M 0 ) \u2212 1 2 t 0 \u2206 M f (B M s )ds.", "formula_coordinates": [21.0, 206.11, 701.41, 199.78, 20.36]}, {"formula_id": "formula_50", "formula_text": "M f t = f (U t ) \u2212 f (U 0 ) \u2212 1 2 t 0 \u2206 OM f (U s )ds. b) The stochastic antidevelopment of (B M t ) t\u22650 is a R d -valued Brownian motion (B t ) t\u22650 .", "formula_coordinates": [22.0, 134.84, 198.28, 361.23, 36.96]}, {"formula_id": "formula_51", "formula_text": "B M 0 . First sample (U t ) t\u22650 solution of SDE(H 1:d , B 1:d , U 0 ) with H 1:d = {H i } d", "formula_coordinates": [22.0, 108.0, 254.64, 396.0, 30.88]}, {"formula_id": "formula_52", "formula_text": "Proposition C.3 (Extrinsic view of Brownian motion): For any f \u2208 C \u221e (M) we have that \u2206 M (f ) = p i=1 P i (P i (f )). Hence, we have that (B M t ) t\u22650 solution of SDE({P i } p i=1 , B 1:p , B M 0 ) with B M", "formula_coordinates": [22.0, 111.43, 371.63, 389.49, 35.36]}, {"formula_id": "formula_53", "formula_text": "f \u2208 C \u221e (M), A(f ) = i=1 V i (V i (f )).", "formula_coordinates": [22.0, 284.4, 442.45, 162.74, 18.63]}, {"formula_id": "formula_54", "formula_text": "dX k t = d j=1 E k,j t \u2022 dB k t , dE i,j t = \u2212 d n=1 { d ,m=1 E ,n t E m,j t \u0393 i ,m (X t )} \u2022 dB n t .", "formula_coordinates": [22.0, 127.57, 564.39, 356.86, 19.8]}, {"formula_id": "formula_55", "formula_text": "dX k t = d j=1 {E k,j t dB k t + 1 2 d[E k,j t , B j t ] t }. Let (M t ) t\u22650 = ({M k t } d k=1 ) t\u22650 such that for any t \u2265 0 and k \u2208 {1, . . . , d} M k t = d j=1 t 0 E k,j t dB k t .", "formula_coordinates": [22.0, 108.0, 598.06, 396.0, 46.76]}, {"formula_id": "formula_56", "formula_text": "[E k,j , B j ] t = \u2212 d ,m=1 t 0 E ,j t E m,j t \u0393 k", "formula_coordinates": [22.0, 205.26, 665.8, 163.4, 20.36]}, {"formula_id": "formula_57", "formula_text": "dX k t = \u2212 1 2 d ,m=1 g ,m (X t )\u0393 k ,m (X t )dt + (G(X t ) \u22121/2 dB t ) k .", "formula_coordinates": [22.0, 175.09, 709.76, 261.82, 19.8]}, {"formula_id": "formula_58", "formula_text": "{\u03bd x } x\u2208M such that for any x \u2208 M, \u03bd x : B(T x M) \u2192 [0, 1] with \u03bd x (T x M) = 1, i.e. for any x \u2208 M, \u03bd x is a probability measure on T x M. Assume that for any x \u2208 M, M v 3 d\u03bd x (v) < +\u221e.", "formula_coordinates": [23.0, 108.0, 186.84, 397.74, 28.8]}, {"formula_id": "formula_59", "formula_text": "\u0393(M, x\u2208M L(T x M)), such that for any x \u2208 M, M vd\u03bd x (v) = \u00b5 (1) (x) and M v \u2297 vd\u03bd x (v) = \u00b5 (2) (x).", "formula_coordinates": [23.0, 108.0, 222.34, 396.0, 24.72]}, {"formula_id": "formula_60", "formula_text": "x \u2208 M, \u03a3(x) = \u00b5 (2) (x) \u2212 \u00b5 (1) (x) \u2297 \u00b5 (1) (x)", "formula_coordinates": [23.0, 283.35, 235.73, 182.51, 18.41]}, {"formula_id": "formula_61", "formula_text": "\u03bd x \u2212 \u03bd y TV = sup{\u03bd 1 [f ] \u2212 \u0393 y x (\u03b3) # \u03bd 2 [f ] : \u03b3 \u2208 Geo x,y , f \u2208 C(T x M)}.", "formula_coordinates": [23.0, 158.82, 281.72, 299.35, 18.91]}, {"formula_id": "formula_62", "formula_text": "t \u2208 [0, \u03b3], X n\u03b3+t = exp Xn\u03b3 [t\u03b3{\u00b5 n + (1/ \u221a \u03b3)(V n \u2212 \u00b5 n )}],", "formula_coordinates": [23.0, 112.25, 340.87, 387.5, 28.19]}, {"formula_id": "formula_63", "formula_text": "t/\u03b3 \u03b3 [f ] \u2212 P t [f ] \u221e = 0, where (P t ) t\u22650 is the semi-group associated with the infinitesimal generator A : C \u221e (M) \u2192 C \u221e (M) given for any f \u2208 C \u221e (M) by A(f ) = \u00b5 (1) , \u2207f M + 1 2 \u03a3, \u2207 2 f M .", "formula_coordinates": [23.0, 112.25, 475.8, 387.5, 43.21]}, {"formula_id": "formula_64", "formula_text": "\u2208 N, \u2206 M \u03a6 k = \u2212\u03bb k \u03a6 k . For any t \u2265 0 and x, y \u2208 M, p t|0 (y|x) = k\u2208N e \u2212\u03bb k t \u03a6 k (x)\u03a6 k (y)", "formula_coordinates": [23.0, 108.0, 622.44, 396.0, 29.59]}, {"formula_id": "formula_65", "formula_text": "p 0 P t \u2212 p ref TV \u2264 C 1/2 e \u03bb1/2 e \u2212\u03bb1t ,", "formula_coordinates": [24.0, 236.93, 123.03, 143.13, 18.63]}, {"formula_id": "formula_66", "formula_text": "\u2202 t p t (x) = 1 2 \u2206 M p t (x) = div 1 2 p t \u2207 log p t (x).", "formula_coordinates": [24.0, 208.96, 396.08, 194.07, 18.72]}, {"formula_id": "formula_67", "formula_text": "dX t = 1 2 \u2207 log p T \u2212t (X t )dt. (3", "formula_coordinates": [24.0, 248.91, 454.68, 251.22, 18.72]}, {"formula_id": "formula_68", "formula_text": ")", "formula_coordinates": [24.0, 500.13, 457.34, 3.87, 8.64]}, {"formula_id": "formula_69", "formula_text": "Finally, we introduce (Y t ) t\u2208[0,T ] satisfying (3) but such that Y 0 \u223c p ref .", "formula_coordinates": [24.0, 108.0, 475.86, 287.41, 17.29]}, {"formula_id": "formula_70", "formula_text": "d log q t (X \u03b8 t ) = \u2212 1 2 div(s \u03b8 (T \u2212 t, \u2022))(X \u03b8 t )dt,", "formula_coordinates": [24.0, 216.4, 537.0, 179.21, 18.91]}, {"formula_id": "formula_71", "formula_text": "dY \u03b8 t = 1 2 div(s \u03b8 (T \u2212 t, Y \u03b8 t ))dt and Y \u03b8 0 \u223c p ref .", "formula_coordinates": [24.0, 108.0, 558.92, 396.0, 32.61]}, {"formula_id": "formula_72", "formula_text": "\u03b8 T )] = M log q T (x)dp data (x), where (X \u03b8 t ) t\u2208[0,T ] = (X \u03b8 T \u2212t ) t\u2208[0,T ] with dX \u03b8 t = \u2212 1 2 div(s \u03b8 (t, X \u03b8 t )", "formula_coordinates": [24.0, 112.71, 573.13, 391.29, 32.23]}, {"formula_id": "formula_73", "formula_text": "\u2202 t p t (x) = 1 2 \u2206 M p t|0 (x) = div( 1 2 p t \u2207 log p t )(x).(4)", "formula_coordinates": [24.0, 208.15, 710.85, 295.85, 18.72]}, {"formula_id": "formula_74", "formula_text": "Y ODE 0 \u223c p ref . Denoting (q ODE t ) t\u2208[0,T ] the densities of (Y ODE t ) t\u2208[0,T ] w.r.t. p ref we have for any t \u2208 [0, T ] and x \u2208 M \u2202 t q ODE t (x) = \u2212div( 1 2 q ODE t \u2207 log p T \u2212t )(x).(6", "formula_coordinates": [25.0, 106.83, 144.88, 397.17, 53.74]}, {"formula_id": "formula_75", "formula_text": "\u2202 t q SDE t (x) = \u2212div(\u2207 log p T \u2212t q SDE t (x)) + 1 2 \u2206 M q SDE t (x) = \u2212div(q SDE t {\u2207 log p T \u2212t \u2212 1 2 \u2207 log q SDE t })(x).(7)", "formula_coordinates": [25.0, 184.95, 242.33, 319.05, 35.21]}, {"formula_id": "formula_76", "formula_text": "\u2202 t log q SDE t (Y SDE t ) = \u2207 log p T \u2212t (Y SDE t ) \u2212 1 2 \u2207 log q SDE t (Y SDE t ) dt.", "formula_coordinates": [25.0, 160.6, 323.28, 290.8, 18.91]}, {"formula_id": "formula_77", "formula_text": "{E i } d i=1 = {\u2202 t \u03a6 vi 0 } d i=1 , where {v i } d i=1 is a basis of g, we get that {E i } d", "formula_coordinates": [25.0, 108.0, 548.76, 396.0, 31.33]}, {"formula_id": "formula_78", "formula_text": "s \u03b8 (t, x) = d i=1 s i \u03b8 (t, x)E i (x) for any t \u2208 [0, T ] and x \u2208 M since we have that div(s \u03b8 )(t, x) = d i=1 E i (s i \u03b8 )(t, x) + d i=1 s i \u03b8 (t, x)div(E i )(x) = d i=1 ds i \u03b8 (E i )(t, x) (see", "formula_coordinates": [25.0, 107.64, 584.54, 397.52, 37.14]}, {"formula_id": "formula_79", "formula_text": "x) = d i=1 s i \u03b8 (t, x)E i (x) for any t \u2208 [0, T ] and x \u2208 M, with {E i } d i=1 = {\u2202 i \u03d5(\u03d5 \u22121 (x))} d i=1", "formula_coordinates": [25.0, 108.0, 639.54, 396.0, 32.25]}, {"formula_id": "formula_80", "formula_text": "= | det G| \u22121/2 d i=1 \u2202 i {| det G| 1/2 s i \u03b8 (t, \u03d5(\u2022))}(z).", "formula_coordinates": [25.0, 300.0, 677.03, 205.74, 19.8]}, {"formula_id": "formula_81", "formula_text": "d , i.e. \u0393 = { d i=1 \u03b1 i b i : {\u03b1 i } d i=1 \u2208 Z d }. Finally, the associated d-dimensional torus is defined as T \u0393 = R d /\u0393. Denote B = (b 1 , . . . , b d ) \u2208 R d\u00d7d . Let {b i } d i=1 \u2208 (R d ) d such that (B \u22121 ) = (b 1 , . . . ,b d ). We define \u0393 = { d i=1 \u03b1 ibi : {\u03b1 i } d i=1 \u2208 Z d },", "formula_coordinates": [26.0, 107.53, 305.35, 398.21, 57.74]}, {"formula_id": "formula_82", "formula_text": "{\u2212k(k + d \u2212 1) : k \u2208 N}. Note that \u03bb k = k(k + d \u2212 1) has multiplicity d k = (k + d \u2212 2)!/{(d \u2212 1)!k}(2k + d \u2212 1).", "formula_coordinates": [26.0, 108.0, 425.17, 397.74, 28.19]}, {"formula_id": "formula_83", "formula_text": "\u2208 S d G n (x, y) = \u03c6\u2208\u03a6n \u03c6(x)\u03c6(y) = n!\u0393((d \u2212 1)/2) n/2 k=0 (\u22121) k (1 \u2212 x, y 2 ) x, y n\u22122k /(4 k k!(n \u2212 2k)!\u0393(k + (d \u2212 1)/2)),", "formula_coordinates": [26.0, 113.75, 501.76, 384.5, 55.19]}, {"formula_id": "formula_85", "formula_text": "1 2 E s(Xt) \u2212 SJ,t(X0, Xt) 2 (J \u2192 \u221e) 0 Varhadan (8) 1 2 E s(Xt) \u2212 log X t (X0)/t 2 (t \u2192 0) 0 t|s (DSM) Varhadan (8) 1 2 E s(Xt) \u2212 log X t (Xs)/(t \u2212 s) 2 (t \u2192 s) 0 im t (ISM) Deterministic E 1 2 s(Xt) 2 + div(s)(Xt) 0 Stochastic E 1 2 s(Xt) 2 + \u03b5 \u2202s(Xt)\u03b5 2 \u2202s F G Predictor-corrector schemes", "formula_coordinates": [27.0, 108.0, 112.75, 379.96, 104.57]}, {"formula_id": "formula_86", "formula_text": "Y j+1 t = exp Y j t [ \u03b3 2 \u2207 log p T \u2212j\u03b3 (Y j t ) + \u221a \u03b3Z j+1 ],", "formula_coordinates": [27.0, 207.54, 281.04, 196.93, 23.98]}, {"formula_id": "formula_87", "formula_text": ".1), (Y j t ) j\u2208N converges to (Y s t ) s\u22650 such that dY s t = 1 2 \u2207 log p T \u2212t (Y s t )ds + dB M s .", "formula_coordinates": [27.0, 108.0, 328.63, 396.0, 43.16]}, {"formula_id": "formula_88", "formula_text": "p data = p 0 L(Y 0 ) = p T L(Y 0 \u03b3 ) L(Y 1 \u03b3 ) \u2022 \u2022 \u2022 p \u03b3 L(Y 0 2\u03b3 ) L(Y 1 2\u03b3 ) p 2\u03b3", "formula_coordinates": [27.0, 177.57, 508.95, 253.88, 97.29]}, {"formula_id": "formula_89", "formula_text": "Z k+1 = P(Y k )Z k+1", "formula_coordinates": [28.0, 138.39, 139.97, 78.41, 8.37]}, {"formula_id": "formula_90", "formula_text": "Y k+1 = Y k + \u03b3 \u2212b(T \u2212 k\u03b3, Y k ) + \u03c3(T \u2212 k\u03b3) 2 \u2207 log p T \u2212k\u03b3 (Y k ) + \u221a \u03b3\u03c3(T \u2212 k\u03b3)Z k+1 E-M step 7:", "formula_coordinates": [28.0, 112.98, 144.37, 369.49, 33.57]}, {"formula_id": "formula_91", "formula_text": "Y 0 k+1 = Y k+1 9:", "formula_coordinates": [28.0, 112.98, 178.08, 80.84, 19.79]}, {"formula_id": "formula_92", "formula_text": "Z s k+1 = P(Y s k+1 )Z s k+1", "formula_coordinates": [28.0, 151.84, 209.1, 87.65, 10.91]}, {"formula_id": "formula_93", "formula_text": "Y s+1 k+1 = Y s k+1 + \u03b3s 1 2 \u2207 log p T \u2212k\u03b3 (Y s k+1 ) + \u221a \u03b3sZ s k+1 Langevin step 13: Y k+1 = Y S k+1 14: return {Y k } N k=0", "formula_coordinates": [28.0, 108.5, 216.73, 395.5, 40.75]}, {"formula_id": "formula_94", "formula_text": "Require: \u03b5, T, N, {X m 0 } M m=1 , loss, s, \u03b80, Niter, pref, P 1: /// TRAINING /// 2: for n \u2208 {0, . . . , Niter \u2212 1} do 3: X0 \u223c (1/M ) M m=1 \u03b4 X m 0 Random mini-batch from dataset 4: t \u223c U ([\u03b5, T ])", "formula_coordinates": [28.0, 108.0, 287.85, 396.0, 52.49]}, {"formula_id": "formula_95", "formula_text": "f \u2208 C 2 (M) we have that (M X,f t ) t\u2208[0,T ] is a X-martingale where for any t \u2208 [0, T ] M X,f t = f (X t ) \u2212 t 0 { b(X s ), \u2207f (X s ) + 1 2 \u2206 M f (X s )}ds. (9) Let (Y t ) t\u2208[0,T ] = (X T \u2212t ) t\u2208[0,T ] . Our goal is to show that for any f \u2208 C 2 (M), (M Y,f t ) t\u2208[0,T ] is a Y-martingale where for any t \u2208 [0, T ] M Y,f t = f (Y t ) \u2212 t 0 { \u2212b(Y s ) + \u2207 log p T \u2212s (Y s ), \u2207f (Y s ) + 1 2 \u2206 M f (Y s )}ds.", "formula_coordinates": [28.0, 107.57, 586.57, 396.44, 111.4]}, {"formula_id": "formula_96", "formula_text": "E[g(Y s )(f (Y t ) \u2212 f (Y s ))] (10) = E[g(Y s ) t s { \u2212b(Y u ) + \u2207 log p T \u2212u (Y u ), \u2207f (Y u ) + 1 2 \u2206 M f (Y u )}du].", "formula_coordinates": [29.0, 132.38, 87.59, 371.62, 34.4]}, {"formula_id": "formula_97", "formula_text": "A : C 2 (M) \u2192 C(M) given for any f \u2208 C 2 (M) and x \u2208 M by A(f )(x) = b(x), \u2207f (x) + 1 2 \u2206 M f (x)", "formula_coordinates": [29.0, 108.0, 118.67, 396.0, 40.23]}, {"formula_id": "formula_98", "formula_text": "[0, T ] \u00d7 C 2 (M) \u2192 C(M) given for any f \u2208 C 2 (M), t \u2208 [0, T ] and x \u2208 M b\u1ef9 A(t, f )(x) = \u2212b(x) + \u2207 log p T \u2212t (x), \u2207f (x) + 1 2 \u2206 M f (x)", "formula_coordinates": [29.0, 108.0, 155.35, 396.35, 44.42]}, {"formula_id": "formula_99", "formula_text": "E[g(Y s )(f (Y t ) \u2212 f (Y s ))] = E[g(Y s ) t s\u00c3 (u, Y u )du].", "formula_coordinates": [29.0, 191.61, 221.09, 228.79, 20.36]}, {"formula_id": "formula_100", "formula_text": "E[g(Y s )(f (Y t ) \u2212 f (Y s ))] = E[g(X T \u2212s )(f (X T \u2212t ) \u2212 f (X T \u2212t ))] = E[E[g(X T \u2212s )|X T \u2212t ]f (X T \u2212t )] \u2212 E[g(X T \u2212s )f (X T \u2212s )] = E[v(T \u2212 t, X T \u2212t )f (X T \u2212t )] \u2212 E[v(T \u2212 s, X T \u2212s )f (X T \u2212s )],(12)", "formula_coordinates": [29.0, 120.85, 261.77, 383.15, 48.58]}, {"formula_id": "formula_101", "formula_text": "with v : [0, T \u2212 s] \u00d7 M \u2192 R given for any u \u2208 [0, T \u2212 s] and x \u2208 M by v(u, x) = E[g(X T \u2212s )|X u = x].", "formula_coordinates": [29.0, 107.64, 314.66, 396.36, 26.76]}, {"formula_id": "formula_102", "formula_text": "u \u2208 [0, T \u2212 s] and x \u2208 M \u2202 u v(u, x) = \u2212Av(u, x).(13", "formula_coordinates": [29.0, 124.73, 336.48, 375.12, 28.19]}, {"formula_id": "formula_103", "formula_text": "\u2202 u h(u, x) + Ah(u, x) = f (x)\u2202 u v(u, x) + f (x)Av(u, x) + v(u, x)Af (x) + \u2207f (x), \u2207v(u, x) = v(u, x)Af (x) + \u2207f (x), \u2207v(u, x) . (14", "formula_coordinates": [29.0, 112.57, 443.84, 387.29, 31.18]}, {"formula_id": "formula_104", "formula_text": ")", "formula_coordinates": [29.0, 499.85, 458.98, 4.15, 8.64]}, {"formula_id": "formula_105", "formula_text": "u \u2208 [0, T \u2212 s] E[ \u2207f (X u ), \u2207v(u, X u ) ] = M \u2207f (x u ), \u2207v(u, x u )p u (x u ) dp ref (x u ) = \u2212 M v(u, x u )div(p u \u2207f )(x u )dp ref (x u ) = \u2212 M v(u, x u )\u2206 M f (x u )p u (x u )dp ref (x u ) \u2212 M v(u, x u ) \u2207f (x u ), \u2207 log p u (x u ) p u (x u )dp ref (x u ) = \u2212E[v(u, X u )\u2206 M f (X u )] \u2212 E[v(u, X u ) \u2207f (X u ), \u2207 log p u (X u ) ]", "formula_coordinates": [29.0, 143.92, 471.09, 343.87, 92.46]}, {"formula_id": "formula_106", "formula_text": "u \u2208 [0, T \u2212 s] E[\u2202 u h(u, X u ) + Ah(u, X u )] = E[v(u, X u ){ b(X u ) \u2212 \u2207 log p u (X u ), \u2207f (X u ) \u2212 1 2 \u2206 M f (X u )}] = \u2212E[v(u, X u )\u00c3(T \u2212 u, f )(X u )].", "formula_coordinates": [29.0, 166.46, 559.62, 279.08, 62.9]}, {"formula_id": "formula_107", "formula_text": "u \u2208 [0, T \u2212 s] and x \u2208 M, v(u, x) = E[g(X T \u2212s )|X u = x] we get E[v(T \u2212 t, X T \u2212t )f (X T \u2212t )] \u2212 E[v(T \u2212 s, X T \u2212s )f (X T \u2212s )] = E[h(T \u2212 t, X T \u2212t ) \u2212 h(T \u2212 s, X T \u2212s )] = T \u2212s T \u2212t E[v(u, X u )\u00c3(T \u2212 u, X u )]du = E[g(X T \u2212s ) T \u2212s T \u2212t\u00c3 (T \u2212 u, X u )du].", "formula_coordinates": [29.0, 108.0, 618.58, 396.0, 92.32]}, {"formula_id": "formula_108", "formula_text": "E[g(Y s )(f (Y t ) \u2212 f (Y s ))] = E[g(X T \u2212s ) T \u2212s T \u2212t\u00c3 (T \u2212 u, X u )du] = E[g(Y s ) t s\u00c3 (u, Y u )du].", "formula_coordinates": [30.0, 111.71, 95.0, 388.58, 21.19]}, {"formula_id": "formula_109", "formula_text": "[0, T ], F t = \u03c3(X s , s \u2208 [0, t]). Let (M t ) t\u2208[0,T ] be a Polish-valued stochastic process. We say that (M t ) t\u2208[0,T ] is a P-local martingale if it is a local martingale w.r.t. the filtration (F t ) t\u2208[0,T ] . A function u : [0, T ] \u00d7 X \u2192 R is", "formula_coordinates": [30.0, 108.0, 350.51, 397.74, 41.31]}, {"formula_id": "formula_110", "formula_text": "M t = u(t, X t ) \u2212 u(0, X 0 ) \u2212 t 0\u0100 P u(s, X [0,s] )ds.", "formula_coordinates": [30.0, 203.26, 475.15, 205.48, 20.36]}, {"formula_id": "formula_111", "formula_text": "P (u, v) =\u0100 P (uv) \u2212\u0100 P (u)v \u2212\u0100 P (v)u. Note that if X = M is a Riemannian manifold, C 2 (M) \u2282 dom(\u0100 P ) and for any u \u2208 C 2 (M) A P (u) = \u2207u, X + 1 2 \u2206 M u with X \u2208 \u0393(TM) then we have that C 2 (M) \u00d7 C 2 (M) \u2282 dom(\u1fe9 P ) and for any u, v \u2208 C 2 (M),\u1fe9 P (u, v) = \u2207u, \u2207v . Assume that there exists U P \u2282 dom(\u0100 P )\u2229C b (X)", "formula_coordinates": [30.0, 108.0, 540.32, 746.49, 61.09]}, {"formula_id": "formula_112", "formula_text": "\u03a5 P (u, v) : [0, T ] \u00d7 X \u2192 R from\u1fe9 P (u, v).", "formula_coordinates": [30.0, 107.22, 695.89, 172.44, 17.29]}, {"formula_id": "formula_113", "formula_text": "i) \u03a5 P (u, v) \u2208 C([0, T ] \u00d7 X, R).", "formula_coordinates": [31.0, 121.1, 145.09, 128.85, 17.29]}, {"formula_id": "formula_114", "formula_text": "\u00b5[\u03c9] = E[ T 0 \u03a5 P (u, \u03c9 t )(t, X t )dt], where\u016a 2,P = {\u03c9 \u2208 C([0, T ] \u00d7 X, R) : \u03c9(t, \u2022) \u2208 U 2,P for any t \u2208 [0, T ]}.", "formula_coordinates": [31.0, 132.17, 190.08, 297.05, 40.05]}, {"formula_id": "formula_115", "formula_text": "[0, T ] \u00d7 C 2 (R p ) \u00d7 R p \u2192 R if for any u \u2208 C 2 c (R p ), (M t ) t\u2208[0,T ] is a P-martingale where for any t \u2208 [0, T ] we have M t = M 0 + t 0 A(t, u)(X s )", "formula_coordinates": [31.0, 108.0, 356.91, 396.17, 48.07]}, {"formula_id": "formula_116", "formula_text": "[0, T ] \u00d7 C 2 (M) \u00d7 M \u2192 R if for any u \u2208 C 2 (M), (M t ) t\u2208[0,T ] is a P-martingale", "formula_coordinates": [31.0, 182.74, 430.47, 321.26, 18.41]}, {"formula_id": "formula_117", "formula_text": "M t = M 0 + t 0\u00c3 (t, u)(X s )ds,", "formula_coordinates": [31.0, 240.96, 458.24, 130.07, 16.08]}, {"formula_id": "formula_118", "formula_text": "A(t, u)(x) = \u03b2(t, x), \u2207u(x) + 1 2 \u2206 M u(x).", "formula_coordinates": [31.0, 214.99, 561.87, 182.03, 18.72]}, {"formula_id": "formula_119", "formula_text": "KL (P|Q) = KL (P 0 |Q 0 ) + 1 2 T 0 E[ \u03b2(t, X t ) 2 ]dt,", "formula_coordinates": [31.0, 201.21, 597.7, 209.59, 20.36]}, {"formula_id": "formula_120", "formula_text": "dB M t = p i=1 P i (B M t ) \u2022 dB i t = P (B M t )", "formula_coordinates": [31.0, 205.47, 675.28, 171.69, 19.8]}, {"formula_id": "formula_121", "formula_text": "E[v(B M s ) t s 1 2 \u2206 M u(B M u )du] = E[v(B M s ) t s { \u2207\u016b(B M w ),b(B M w ) + 1 2 P(B M w ), \u2207 2\u016b (B M w ) }dw].", "formula_coordinates": [32.0, 154.36, 164.08, 303.28, 37.74]}, {"formula_id": "formula_122", "formula_text": "x \u2208 M, \u2206 M u(x) = 2 \u2207\u016b(x),b(x) + P(x), \u2207 2\u016b (x) . Note that (B M t ) t\u2208[0,T ]", "formula_coordinates": [32.0, 112.25, 203.21, 387.5, 23.96]}, {"formula_id": "formula_123", "formula_text": "[0, T ] \u00d7 R p \u2192 R p such that KL (P|Q) = KL (P 0 |Q 0 ) + 1 2 T 0 E[ P(X t )\u03b2(t, X t ) 2 ]dt.(15)", "formula_coordinates": [32.0, 187.86, 235.94, 311.89, 39.2]}, {"formula_id": "formula_124", "formula_text": "A : [0, T ] \u00d7 C 2 c (R p ) \u00d7 R p \u2192 R such that for any t \u2208 [0, T ],\u016b \u2208 C 2 c (R p ) and x \u2208 R p A(t,\u016b)(x) = b (x) + P(x)\u03b2(t, x), \u2207\u016b(x) + 1 2 P(x), \u2207 2\u016b (x) . Let \u03b2 : [0, T ] \u00d7 M such that for any t \u2208 [0, T ] and x \u2208 M we have \u03b2(t, x) = P(x)\u03b2(t, x).", "formula_coordinates": [32.0, 112.25, 285.52, 605.22, 57.17]}, {"formula_id": "formula_125", "formula_text": "x \u2208 M, \u03b2(t, x) \u2208 T x M. Let u \u2208 C 2 c (M)", "formula_coordinates": [32.0, 254.88, 335.19, 177.32, 18.41]}, {"formula_id": "formula_126", "formula_text": "A(t,\u016b)(x) = b (x) + P(x)\u03b2(t, x), \u2207\u016b(x) + 1 2 P(x), \u2207 2\u016b (x) = \u03b2(t, x), \u2207\u016b(x) + 1 2 \u2206 M u(x) = \u03b2(t, x), \u2207u(x) + 1 2 \u2206 M u(x).", "formula_coordinates": [32.0, 177.81, 363.91, 252.51, 51.34]}, {"formula_id": "formula_127", "formula_text": "[0, T ] \u00d7 C 2 c (M) \u00d7 M \u2192 R such that for any t \u2208 [0, T ], u \u2208 C 2 (R p ) and x \u2208 M A(t,\u016b)(x) = \u03b2(t, x), \u2207u(x) + 1 2 \u2206 M u(x)", "formula_coordinates": [32.0, 112.25, 425.63, 388.66, 40.23]}, {"formula_id": "formula_128", "formula_text": "KL (P|Q) = KL (P 0 |Q 0 ) + 1 2 T 0 E[ \u03b2(t, X t ) 2 ]dt, which concludes the proof.", "formula_coordinates": [32.0, 111.89, 480.85, 298.9, 30.83]}, {"formula_id": "formula_129", "formula_text": "Corollary H.3: Assume A1. Let P 1 , P 2 be a Markov path measure on C([0, T ] , M) with P 1 0 = P 2 0 . In addition, assume that there exist b 1 , b 2 \u2208 C \u221e ([0, T ] , X (M)) such that (X 1 t ) t\u2208[0,T ] and (X 2 t", "formula_coordinates": [32.0, 111.09, 544.3, 388.66, 36.73]}, {"formula_id": "formula_130", "formula_text": "KL(P 1 |P 2 ) = 1 2 T 0 E[ b 1 (t, X 1 t ) \u2212 b 2 (t, X 1 t ) 2 ]dt.", "formula_coordinates": [32.0, 202.26, 598.83, 207.48, 20.36]}, {"formula_id": "formula_131", "formula_text": "x \u2265 R + 1, \u03d5(x) = 0. Considerb 1 ,b 2 \u2208 C 2 c ([0, T ] \u00d7 R p , R p ) such that for any t \u2208 [0, T ] and x \u2208 M,b i (x) = b i (x) with i \u2208 {1, 2}. Consider (X i t ) t\u2208[0,T ] such that for any i \u2208 {1, 2} dX i t = \u03d5(X i t ){P(X i t )b i (t,X i t ) +b(X t )}dt + \u03d5(X i t )P(X i t )dB t ,", "formula_coordinates": [32.0, 112.25, 671.3, 387.5, 44.2]}, {"formula_id": "formula_132", "formula_text": "A i t (f )(x) = b i (t, x) +b(x), \u2207f (x) + 1 2 P(x), \u2207 2f (x) = b i (t, x), \u2207f (x) + 1 2 \u2206 M f (x).", "formula_coordinates": [33.0, 189.09, 130.13, 229.94, 35.81]}, {"formula_id": "formula_133", "formula_text": "t \u2208 [0, T ] and x \u2208 R p , \u03b1(t, x) =b 1 (t, x) \u2212b 2 (t, x) = P(x)(b 1 (t, x) \u2212b 2 (t, x)), we have that for any t \u2208 [0, T ], P(x)\u03b1(t, x) = P(x)(b 1 (t, x) \u2212b 2 (t, x)", "formula_coordinates": [33.0, 112.25, 237.44, 387.67, 29.32]}, {"formula_id": "formula_134", "formula_text": "(dP 1 /dP 2 )((X 1 t ) t\u2208[0,T ] ) = exp [ T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )dX 1 t \u2212 1 2 T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )(b 1 (t,X 1 t ) +b 2 (t,X 1 t )) dt] = exp [ T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t ){b 1 (t,X 1 t ) +b(X 1 t )} dt + T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )dB t \u2212 1 2 T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )(b 1 (t,X 1 t ) +b 2 (t,X 1 t )) dt] = exp[ 1 2 T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ) 2 dt + T 0 b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ), P(X 1 t )dB t ]", "formula_coordinates": [33.0, 116.05, 288.3, 377.13, 109.63]}, {"formula_id": "formula_135", "formula_text": "KL(P 1 |P 2 ) = 1 2 T 0 E[ b1 (t,X 1 t ) \u2212b 2 (t,X 1 t ) 2 ]dt. Hence, we get KL(P 1 |P 2 ) = 1 2 T 0 E[ b 1 (t, X 1 t ) \u2212 b 2 (t, X 1 t ) 2", "formula_coordinates": [33.0, 112.25, 415.2, 297.49, 51.72]}, {"formula_id": "formula_136", "formula_text": "C([0, T ] , M) such that KL (P|Q) < +\u221e. Then, there exist \u03b2 P , \u03b2 R(P) : [0, T ] \u00d7 M \u2192 such that for any t \u2208 [0, T ] and x \u2208 M, \u03b2 P (t, x), \u03b2 R(P) (t, x) \u2208 T x M.", "formula_coordinates": [33.0, 112.25, 528.73, 387.49, 29.3]}, {"formula_id": "formula_137", "formula_text": "A P (t, u)(x) = \u03b2 P (t, x), \u2207u(x) + 1 2 \u2206 M u(x), A R(P) (t, u)(x) = \u03b2 R(P) (t, x), \u2207u(x) + 1 2 \u2206 M u(x). Finally, we have that T 0 E[ \u03b2 P (t, X t ) 2 ]dt + T 0 E[ \u03b2 R(P) (t, X T \u2212t ) 2 ]dt < +\u221e,", "formula_coordinates": [33.0, 111.64, 581.85, 318.67, 71.51]}, {"formula_id": "formula_138", "formula_text": "E[v(X t )( \u03b2 P (t, X t ) + \u03b2 R(P) (T \u2212 t, X t ), \u2207u(X t ) + \u2206 M u(X t )) + \u2207u(X t ), \u2207v(X t ) ] = 0.(16)", "formula_coordinates": [34.0, 115.85, 113.98, 383.9, 20.78]}, {"formula_id": "formula_139", "formula_text": "\u03b2 P = b. Let u, v \u2208 C \u221e (M), we have that E[v(X t ) b(X t ) + \u03b2 R(P) (T \u2212 t, X t ), \u2207u(X t ) + \u2206 M u(X t )v(X t ) + \u2207u(X t ), \u2207v(X t ) ] = 0.", "formula_coordinates": [34.0, 108.0, 229.83, 389.76, 36.04]}, {"formula_id": "formula_140", "formula_text": "M { \u03b2 R(P) (T \u2212 t, x), \u2207u(x) + b(x), \u2207u(x) }v(x)p t (x)dp ref (x) = \u2212 M \u2207u(x)p t (x), \u2207v(x) dp ref (x) \u2212 M \u2206 M u(x)v(x)p t (x)dp ref (x) = M \u2207 log p t (x), \u2207u(x)v(x)p t (x)dp ref (x).", "formula_coordinates": [34.0, 123.41, 295.39, 369.88, 49.0]}, {"formula_id": "formula_141", "formula_text": "x \u2208 M, \u03b2 R(P) (T \u2212 t, x), \u2207u(x) = \u2212b(x) + \u2207 log p t (x), \u2207u(x)", "formula_coordinates": [34.0, 108.0, 345.0, 397.93, 29.3]}, {"formula_id": "formula_142", "formula_text": "Y k+1 = exp Y k [\u03b3s \u03b8 (T \u2212 n\u03b3, Y k ) + \u221a 2Z k+1 ],", "formula_coordinates": [34.0, 212.4, 497.25, 187.2, 26.01]}, {"formula_id": "formula_143", "formula_text": "t \u2208 [0, T ] and x \u2208 M, s \u03b8 (t, x) \u2212 \u2207 log p t (x) \u2264 M, with s \u03b8 \u2208 C([0, T ] , X (M)). Then if T > 1/2, there exists C \u2265 0 independent on T such that W 1 (L(Y N ), p 0 ) = C(e \u2212\u03bb1T + T /2M + e T \u03b3 1/2 ),", "formula_coordinates": [34.0, 112.25, 567.49, 387.5, 41.64]}, {"formula_id": "formula_144", "formula_text": "x \u2208 R d , A \u2208 B(R d ) and k \u2208 {0, . . . , N \u2212 1} we have E[R k+1 (Y k , A)] = E[1 A (Y k+1 )]. Define for any k 0 , k 1 \u2208 {1, . . . , N } with k 1 \u2265 k 0 Q k0,k1 = k1 =k0 R k1+k0\u2212 .", "formula_coordinates": [34.0, 112.25, 641.91, 387.5, 57.55]}, {"formula_id": "formula_145", "formula_text": "|E[\u03d5(Y N )] \u2212 M \u03d5(x)p 0 (x)d\u00b5(x)| \u2264 |E[\u03d5(B 0 )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]||E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 \u2212 \u03c0 \u221e (P R ) T |0 TV + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 P T |0 (P R ) T |0 \u2212 \u03c0 \u221e (P R ) T |0 TV + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y T )]| + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 \u03d5 \u221e \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV + \u221a 2 \u03d5 \u221e KL 1/2 (\u03c0 \u221e P R |0 |\u03c0 \u221eP R |0 ) + |E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]|.", "formula_coordinates": [35.0, 119.78, 225.34, 372.45, 118.22]}, {"formula_id": "formula_146", "formula_text": "\u03d5 \u221e \u03c0 0 P T |0 \u2212 \u03c0 \u221e TV \u2264 C 1/2 e \u03bb1/2 \u03d5 \u221e e \u2212\u03bb1T . (b)", "formula_coordinates": [35.0, 112.25, 429.61, 299.08, 31.32]}, {"formula_id": "formula_147", "formula_text": "R |0 ) = 1 2 T 0 E[ s \u03b8 (T \u2212 t, Y t ) \u2212 \u2207 log p T \u2212t (Y t ) 2 ] \u2264 M 2 T.", "formula_coordinates": [35.0, 215.56, 491.93, 248.75, 20.63]}, {"formula_id": "formula_148", "formula_text": "Y j\u03b3+t,k = exp Y j\u03b3,k [ts \u03b8 (T \u2212 j\u03b3, Y j\u03b3,k ) + \u221a tE k j Z j ],", "formula_coordinates": [35.0, 198.8, 553.23, 214.41, 25.87]}, {"formula_id": "formula_149", "formula_text": "\u2208 {k + 1, . . . , N \u2212 1}, E k+1 j = \u0393 Y j\u03b3,k+1 Y j\u03b3,k E k j and {E 0 j } N \u22121 j=0 is such that for any j \u2208 {0, . . . , N \u2212 1}, E 0 j is a frame of T Yj\u03b3 M. One [0, k\u03b3],", "formula_coordinates": [35.0, 112.25, 594.78, 388.74, 47.87]}, {"formula_id": "formula_150", "formula_text": "(\u0176 T ) \u2212 \u03d5(Y N )| = |\u03d5(\u0232 0 T ) \u2212 \u03d5(\u0232 N T )| \u2264 N \u22121 k=0 |\u03d5(\u0232 k T ) \u2212 \u03d5(\u0232 k+1 T )| \u2264 \u2207\u03d5 \u221e N \u22121 k=0 d(\u0232 k T ,\u0232 k+1 T", "formula_coordinates": [35.0, 140.02, 686.1, 319.14, 35.94]}, {"formula_id": "formula_151", "formula_text": "E[d(\u0232 k,T ,\u0232 k+1,T )] \u2264 C exp[(N \u2212 k)\u03b3]\u03b3 3/2 .", "formula_coordinates": [36.0, 212.1, 104.43, 187.8, 18.91]}, {"formula_id": "formula_152", "formula_text": "|E[\u03d5(\u0176 T )] \u2212 E[\u03d5(Y N )]| \u2264 C exp[T ]\u03b3 1/2 ,", "formula_coordinates": [36.0, 220.69, 141.9, 170.61, 18.91]}, {"formula_id": "formula_153", "formula_text": "E[\u03d5(Y N )] \u2212 M \u03d5(x)p 0 (x)dp ref (x) \u2264 C(e \u03bb1/2 \u03d5 \u221e e \u2212\u03bb1T + T /2 \u03d5 \u221e M + e T \u03b3 1/2 ). (18", "formula_coordinates": [36.0, 119.06, 190.85, 376.54, 19.24]}, {"formula_id": "formula_154", "formula_text": "\u00b5 \u2212 \u03bd TV = sup{\u00b5[f ] \u2212 \u03bd[f ] : f \u2208 C(M), f \u221e \u2264 1}.", "formula_coordinates": [36.0, 191.78, 478.8, 233.42, 17.29]}, {"formula_id": "formula_155", "formula_text": "\u03ba(\u03b5) = sup{|p T (x) \u2212 p 0 (x) \u2212p T (y) + p 0 (y)| : x, y \u2208 M, d(x, y) \u2264 \u03b5}. Let x 0 \u2208 M such that |p T (x 0 ) \u2212 p 0 (x 0 )| = M = sup{|p T (x) \u2212 p 0 (x)| : x \u2208 M}.", "formula_coordinates": [36.0, 108.0, 553.44, 348.5, 51.35]}, {"formula_id": "formula_156", "formula_text": "\u0393 x2 x1 : T x1 M \u2192 T x2 M the associated parallel transport. Let b \u2208 C \u221e ([0, T ] , X (M)).", "formula_coordinates": [37.0, 108.0, 144.08, 396.0, 30.34]}, {"formula_id": "formula_157", "formula_text": "{E k : k \u2208 {0, . . . , 2 }, \u2208 N}, {X k : k \u2208 {0, . . . , 2 }, \u2208 N} such that X 0 0 = X 0 , X 0 1 = exp X 0 0 [\u03b3b(0, X 0 0 ) + \u221a \u03b3(B 1 \u2212 B 0 )E 0 0 ] and E 0 1 = \u0393 X 0 1 X 0 0 E 0 0 (", "formula_coordinates": [37.0, 108.0, 195.25, 397.25, 33.87]}, {"formula_id": "formula_158", "formula_text": "X 2k+1 = exp X 2k [\u03b3 b(2k\u03b3 , X 2k ) + E 2k (B (2k+1)\u03b3 \u2212 B 2k\u03b3 )], E 2k+1 = \u0393 X 2k+1 X 2k E 2k , X 2k+2 = exp X 2k+1 [\u03b3 b((2k + 1)\u03b3 , X 2k+1 ) + E 2k+1 (B (2k+2)\u03b3 \u2212 B (2k+1)\u03b3 )], E 2k+2 = \u0393 X 2k+2 X \u22121 k+1 E \u22121 k+1 ,(19)", "formula_coordinates": [37.0, 143.63, 262.74, 360.37, 79.26]}, {"formula_id": "formula_159", "formula_text": "(k + 1)\u03b3 ) , X t = exp X k [(t \u2212 k\u03b3 )b(k\u03b3 , X k ) + E k (B t \u2212 B k\u03b3 )].", "formula_coordinates": [37.0, 230.99, 367.74, 274.75, 17.29]}, {"formula_id": "formula_160", "formula_text": "d(exp y [v], exp x [u]) 2 \u2264 (1 + C\u03ba 2 exp[4\u03ba])d(x, y) 2 + C exp[4\u03ba] \u0393 x y v \u2212 u 2 + 2 \u03b3 (0), \u0393 x y v \u2212 u , with \u03ba = u + v .", "formula_coordinates": [37.0, 112.25, 450.02, 387.5, 31.1]}, {"formula_id": "formula_161", "formula_text": "E[sup t\u2208[0,\u03b3] d(X t , X +1 t ) 2 ] \u2264 C\u03b3 3 2 \u22122 . Proof: Let \u2208 N, k \u2208 {0, . . . , 2 \u2212 1} and t \u2208 [k\u03b3 , (k + 1)\u03b3 ]. We define U t k = d(X t , X +1 t ) 2 , U k = sup{U t k : t \u2208 [k\u03b3 , (k + 1)\u03b3 ]} and U \u22121 = 0.", "formula_coordinates": [37.0, 112.25, 531.0, 388.74, 61.09]}, {"formula_id": "formula_162", "formula_text": "(2k + 1)\u03b3 +1 ),X +1 t = X +1 t and for t \u2208 [(2k + 1)\u03b3 +1 , (k + 1)\u03b3 ) X +1 t = exp X +1 2j [\u03b3 +1 b(2j\u03b3 +1 , X +1 2j ) + (t \u2212 (2k + 1)\u03b3 +1 )b((2j + 1)\u03b3 +1 , X +1 2j ) + (B t \u2212 B j\u03b3 )E +1 2j ].", "formula_coordinates": [37.0, 152.55, 585.96, 617.53, 58.21]}, {"formula_id": "formula_163", "formula_text": "U t k+1 \u2264 (1 + 2 \u2212 )d(X t ,X +1 t ) 2 + (1 + 2 )d(X +1 t , X +1 t ) 2 . (20", "formula_coordinates": [37.0, 182.21, 676.15, 313.39, 19.06]}, {"formula_id": "formula_164", "formula_text": ")", "formula_coordinates": [37.0, 495.6, 679.16, 4.15, 8.64]}, {"formula_id": "formula_165", "formula_text": "X +1 t = exp X +1 2k [\u03b3 +1 b(k\u03b3 , X +1 2k ) (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B k\u03b3 )E +1 2k ], X t = exp X k [(t \u2212 k\u03b3 )b(k\u03b3 , X k ) + (B t \u2212 B k\u03b3 )E k ].", "formula_coordinates": [38.0, 157.87, 93.19, 296.26, 54.03]}, {"formula_id": "formula_166", "formula_text": "d(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 (21) + C exp[4\u03ba k ] \u0393 X k X +1 2k v k \u2212 u k 2 + 2 w (0), \u0393 X k X +1 2k v k \u2212 u k ,", "formula_coordinates": [38.0, 145.53, 165.76, 354.21, 38.2]}, {"formula_id": "formula_167", "formula_text": "\u03ba k = u k + v k , u 1 k = (t \u2212 k\u03b3 )b(k\u03b3 , X k ), v 1 k = \u03b3 +1 b(2k\u03b3 +1 , X +1 2k ) + (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ), u 2 k = (B t \u2212 B k\u03b3 )E k , v 2 k = (B t \u2212 B k\u03b3 )E +1 2k , u k = u 1 k + u 2 k , v k = v 1 k + v 2 k .", "formula_coordinates": [38.0, 157.07, 227.94, 297.87, 76.48]}, {"formula_id": "formula_168", "formula_text": "X k X +1 2k E +1 2k using (19), we have that u 2 k = \u0393 X k X +1 2k v 2 k .", "formula_coordinates": [38.0, 231.7, 312.09, 219.38, 19.17]}, {"formula_id": "formula_169", "formula_text": "\u0393 X k X +1 2k v 1 k \u2212 u 1 k \u2264 \u03b3 +1 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) + \u03b3 +1 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ) \u2264 \u03b3 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) + L 2 \u03b3 2 \u2264 L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 .", "formula_coordinates": [38.0, 158.32, 349.57, 295.37, 86.86]}, {"formula_id": "formula_170", "formula_text": "d(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 + C exp[4\u03ba k ](L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 ) 2 + 2(L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 )d(X k , X +1 2k ) \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 )d(X k , X +1 2k ) 2 + 2(L 1 \u03b3 d(X k , X +1 2k ) + L 2 \u03b3 2 )d(X k , X +1 2k ) + 2L 2 2 \u03b3 4 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 + 2L 1 \u03b3 + 4L 2 \u03b3 )d(X k , X +1 2k ) 2 + 8L 2 \u03b3 3 , Hence, there exists C 1 \u2265 0 (not dependent on k or ) such that (1 + 2 \u2212 )d(X +1 t , X t ) 2 \u2264 (1 + C 1 {\u03ba 2 k exp[4\u03ba k ] + \u03b3 2 exp[4\u03ba k ] + 2 \u2212 })d(X k , X +1 2k ) 2 + C 1 \u03b3 3 . Next, we assume that t \u2208 [k\u03b3 , (2k + 1)\u03b3 +1 ]. Recall that X +1 t = exp X +1 2k [(t \u2212 k\u03b3 )b(k\u03b3 , X +1 2k ) + (B t \u2212 B k\u03b3 )E +1 2k ], X t = exp X k [(t \u2212 k\u03b3 )b(k\u03b3 , X k ) + (B t \u2212 B k\u03b3 )E k ].", "formula_coordinates": [38.0, 112.25, 469.69, 387.5, 191.62]}, {"formula_id": "formula_171", "formula_text": "d(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 (22) + C exp[4\u03ba k ] \u0393 X k X +1 2k v k \u2212 u k 2 + 2 w (0), \u0393 X k X +1 2k v k \u2212 u k ,", "formula_coordinates": [38.0, 145.53, 679.84, 354.21, 38.2]}, {"formula_id": "formula_172", "formula_text": "\u03ba k = u k + v k , u 1 k = (t \u2212 k\u03b3 )b(k\u03b3 , X k ), v 1 k = (t \u2212 k\u03b3 )b(k\u03b3 , X +1 2k ), u 2 k = (B t \u2212 B k\u03b3 )E k , v 2 k = (B t \u2212 B k\u03b3 )E +1 2k , u k = u 1 k + u 2 k , v k = v 1 k + v 2 k .", "formula_coordinates": [39.0, 188.44, 94.68, 235.13, 76.48]}, {"formula_id": "formula_173", "formula_text": "X k X +1 2k E +1 2k using (", "formula_coordinates": [39.0, 218.81, 178.35, 71.46, 19.17]}, {"formula_id": "formula_174", "formula_text": "that u 2 k = \u0393 X k X +1 2k v 2 k .", "formula_coordinates": [39.0, 112.25, 198.51, 79.33, 19.17]}, {"formula_id": "formula_175", "formula_text": "\u0393 X k X +1 2k v 1 k \u2212 u 1 k \u2264 \u03b3 +1 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) \u2264 \u03b3 b(k\u03b3 , X k ) \u2212 \u0393 X k X +1 2k b(k\u03b3 , X +1 2k ) + L 2 \u03b3 2 \u2264 L 1 \u03b3 d(X k , X +1 2k ).", "formula_coordinates": [39.0, 176.73, 224.55, 263.03, 63.72]}, {"formula_id": "formula_176", "formula_text": "u k \u2212 v k \u2264 L 1 \u03b3 d(X k , X +1 2k ).", "formula_coordinates": [39.0, 212.49, 288.47, 129.75, 19.06]}, {"formula_id": "formula_177", "formula_text": ") \u2264 d(X k , X +1", "formula_coordinates": [39.0, 112.25, 290.24, 387.5, 25.39]}, {"formula_id": "formula_178", "formula_text": "d(X +1 t , X t ) 2 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ])d(X k , X +1 2k ) 2 + C exp[4\u03ba k ]L 2 1 \u03b3 2 d(X k , X +1 2k ) 2 + 2L 1 \u03b3 d(X k , X +1 2k )d(X k , X +1 2k ) \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 )d(X k , X +1 2k ) 2 + 2L 1 \u03b3 d(X k , X +1 2k ) 2 + 2L 2 2 \u03b3 4 \u2264 (1 + C\u03ba 2 k exp[4\u03ba k ] + 2C exp[4\u03ba k ]L 2 1 \u03b3 2 + 2L 1 \u03b3 )d(X k , X +1 2k ) 2 .", "formula_coordinates": [39.0, 148.99, 320.57, 314.02, 100.87]}, {"formula_id": "formula_179", "formula_text": "(1 + 2 \u2212 )d(X +1 t , X t ) 2 \u2264 (1 + C 1 {\u03ba 2 k exp[4\u03ba k ] + \u03b3 2 exp[4\u03ba k ] + 2 \u2212 })d(X k , X +1 2k ) 2 + C 1 \u03b3 3 . (23", "formula_coordinates": [39.0, 112.25, 437.59, 387.5, 22.63]}, {"formula_id": "formula_180", "formula_text": ") (b) We recall that if t \u2208 [k\u03b3 , (2k + 1)\u03b3 +1 ] the second term in (", "formula_coordinates": [39.0, 112.25, 451.58, 387.5, 30.73]}, {"formula_id": "formula_181", "formula_text": "X +1 t = exp X +1 2k+1 [(t \u2212 (2k + 1)\u03b3 +1 )\u0393 X +1 2k+1 X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ) (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 ].(24)", "formula_coordinates": [39.0, 166.74, 492.75, 333.01, 43.56]}, {"formula_id": "formula_182", "formula_text": "X +1 t , X +1 t )", "formula_coordinates": [39.0, 317.42, 537.08, 50.02, 13.69]}, {"formula_id": "formula_183", "formula_text": "d(X +1 t , X +1 t ) \u2264 d(X +1 t ,X +1 t ) + d(X +1 t , X +1 t ).", "formula_coordinates": [39.0, 198.1, 555.92, 215.81, 19.06]}, {"formula_id": "formula_184", "formula_text": "X +1 t = exp X +1 2k [\u03b3 +1 b(2k\u03b3 +1 , X +1 2k ) + (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B k\u03b3 )E +1 2k ]. (25", "formula_coordinates": [39.0, 151.78, 589.52, 343.82, 37.82]}, {"formula_id": "formula_185", "formula_text": ") Denote a k , b k such that a k = b(2k\u03b3 +1 , X +1 2k ) + (B (2k+1)\u03b3 +1 \u2212 B k\u03b3 )E +1 2k , b k = (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k . Using (", "formula_coordinates": [39.0, 112.25, 611.29, 387.5, 76.94]}, {"formula_id": "formula_186", "formula_text": "X +1 2k+1 = exp X +1 2k [a k ],X +1 t = exp X +1 2k+1 [\u0393 X +1 2k+1 X +1 2k b k ],X +1 t = exp X +1 2k [a k + b k ].", "formula_coordinates": [39.0, 121.63, 694.84, 368.75, 22.15]}, {"formula_id": "formula_187", "formula_text": "C 2 \u2265 0 (not dependent on k or ) such that d(X +1 t ,X +1 t ) \u2264 C 2 ( a k + b k ) 3 .", "formula_coordinates": [40.0, 112.25, 75.45, 388.16, 39.1]}, {"formula_id": "formula_188", "formula_text": "C 3 \u2265 0 (not dependent on k or ) such that d(X +1 t ,X +1 t ) 2 \u2264 C 3 (\u03b3 6 +1 + B t \u2212 B (2k+1)\u03b3 +1 6 + B (2k+1)\u03b3 \u2212 B (k+1)\u03b3 6 ).(26)", "formula_coordinates": [40.0, 112.25, 122.43, 387.5, 35.8]}, {"formula_id": "formula_189", "formula_text": "X +1 t = exp X +1 2k+1 [(t \u2212 (2k + 1)\u03b3 +1 )\u0393 X +1 2k+1 X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ) + (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 ], X +1 t = exp X +1 2k+1 [(t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k+1 ) + (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 ].", "formula_coordinates": [40.0, 116.81, 174.24, 378.38, 60.49]}, {"formula_id": "formula_190", "formula_text": "\u03c4 k = c k + d k , c k = c 1 k + c 2 k , d k = d 1 k + d 2 k , c 1 k = (t \u2212 (2k + 1)\u03b3 +1 )b((2k + 1)\u03b3 +1 , X +1 2k+1 ), d 1 k = (t \u2212 (2k + 1)\u03b3 +1 )\u0393 X +1 2k+1 X +1 2k b((2k + 1)\u03b3 +1 , X +1 2k ), c 2 k = d 2 k = (B t \u2212 B (2k+1)\u03b3 +1 )E +1 2k+1 .(27)", "formula_coordinates": [40.0, 192.24, 254.65, 307.5, 90.81]}, {"formula_id": "formula_191", "formula_text": "d(X +1 t ,X +1 t ) 2 \u2264 C exp[4\u03c4 k ] c k \u2212 d k 2 \u2264 CL 2 2 \u03b3 2 +1 exp[4\u03c4 k ]d(X +1 2k+1 , X +1 2k ) 2 .(28)", "formula_coordinates": [40.0, 131.71, 361.73, 368.04, 19.06]}, {"formula_id": "formula_192", "formula_text": "d(X +1 2k+1 , X +1 2k ) 2 \u2264 exp[4 e k ] e k , with e k = \u03b3 +1 b(k\u03b3 , X +1 2k ) + (B (2k+1)\u03b3 +1 \u2212 B k\u03b3 )E +1 2k .", "formula_coordinates": [40.0, 111.89, 396.69, 270.5, 38.89]}, {"formula_id": "formula_193", "formula_text": "d(X +1 t ,X +1 t ) 2 \u2264 C 3 \u03b3 2 +1 (\u03b3 2 +1 + B (2k+1)\u03b3 +1 \u2212 B k\u03b3 2 ) exp[4\u03c4 k + e k ].(29)", "formula_coordinates": [40.0, 147.06, 445.95, 352.68, 19.06]}, {"formula_id": "formula_194", "formula_text": "d(X +1 t , X +1 t ) 2 \u2264 C 5 \u03b3 2 +1 (\u03b3 2 +1 + B (2k+1)\u03b3 +1 \u2212 B k\u03b3 2 ) exp[4\u03c4 k + e k ] + C 5 (\u03b3 6 +1 + B t \u2212 B (2k+1)\u03b3 +1 6 + B (2k+1)\u03b3 \u2212 B (k+1)\u03b3 6 ). (30", "formula_coordinates": [40.0, 119.19, 480.91, 376.41, 35.34]}, {"formula_id": "formula_195", "formula_text": ")", "formula_coordinates": [40.0, 495.6, 500.2, 4.15, 8.64]}, {"formula_id": "formula_196", "formula_text": "\u03b1 k = C 1 {(\u03ba + k ) 2 exp[4\u03ba k ] + \u03b3 2 exp[4\u03ba + k ] + 2 \u2212 }. \u03b2 k = C 1 \u03b3 3 + C 5 (1 + 2 )\u03b3 2 +1 (\u03b3 2 +1 + B (2k+1)\u03b3 +1 \u2212 B k\u03b3 2 ) exp[4\u03c4 + k + e k ] +C 5 (1 + 2 ) (\u03b3 6 +1 + sup t\u2208[k\u03b3 ,(k+1)\u03b3 ] { B t \u2212 B (2k+1)\u03b3 +1 6 } + B (2k+1)\u03b3 \u2212 B (k+1)\u03b3 6 ), with \u03c4 + k = sup{ c k + d k : t \u2208 [k\u03b3 , (k + 1)\u03b3 ]}, see(", "formula_coordinates": [40.0, 111.89, 532.61, 359.05, 86.76]}, {"formula_id": "formula_197", "formula_text": "U k+1 \u2264 (1 + \u03b1 k )U k + \u03b2 k . Let {R k } 2 k=\u22121 such that R \u22121 = 0 and for any k \u2208 {0, . . . , 2 \u2212 1} R k+1 = (1 + \u03b1 k )R k + \u03b2 k .", "formula_coordinates": [40.0, 112.25, 631.36, 267.55, 48.44]}, {"formula_id": "formula_198", "formula_text": "2 \u22121 \u2265 R k \u2265 U k . Therefore E[R 2 ] \u2265 E[sup{U k : k \u2208 {0, . . . , 2 }}] \u2265 E[sup{d(X t , X +1 t ) 2 : t \u2208 [0, \u03b3]}].(31)", "formula_coordinates": [40.0, 135.02, 686.69, 364.73, 36.13]}, {"formula_id": "formula_199", "formula_text": "E[R k+1 ] = (1 +\u1fb1 k )E[R k ] +\u03b2 k .", "formula_coordinates": [41.0, 238.84, 107.07, 134.32, 10.71]}, {"formula_id": "formula_200", "formula_text": "E[R 2 ] \u2264\u03b2 2 \u22121 + exp[ 2 \u22121 n=0\u1fb1 n ] 2 \u22121 j=0\u03b2 j\u1fb1j .", "formula_coordinates": [41.0, 208.53, 144.93, 194.94, 20.03]}, {"formula_id": "formula_201", "formula_text": "E[R 2 ] \u2264 C 9 \u03b3 3 2 \u22122 ,", "formula_coordinates": [41.0, 263.74, 196.43, 84.52, 19.13]}, {"formula_id": "formula_202", "formula_text": "C \u2265 0 such that E d(X 0 1 , X \u03b3 ) 2 \u2264 C\u03b3 3/2 . Proof: Using Lemma I.4, there exists C \u2265 0 such that for any \u2208 N E[sup t\u2208[0,\u03b3] d(X t , X +1 t )] \u2264 C\u03b3 3/2 2 \u2212 .", "formula_coordinates": [41.0, 112.25, 320.31, 366.08, 64.52]}, {"formula_id": "formula_203", "formula_text": "X i k+1 = exp X i k [\u03b3b(k\u03b3, X i k ) + \u221a \u03b3E i k Z k ]", "formula_coordinates": [41.0, 221.94, 462.1, 165.35, 20.23]}, {"formula_id": "formula_204", "formula_text": "\u2208 N, E 1 k is a frame for T X 1 k M and E 2 k = \u0393 X 2 k X 1 k E 1 k .", "formula_coordinates": [41.0, 240.21, 493.79, 200.6, 21.73]}, {"formula_id": "formula_205", "formula_text": "\u2208 N E d(X 1 k , X 2 k ) \u2264 exp[Ck\u03b3]E d(X 1 0 , X 2 0 ) . Proof: Let k \u2208 N. Using Lemma I.3, there exists D \u2265 0 such that d(X 1 k+1 , X 2 k+1 ) 2 \u2264 (1 + D\u03ba 2 k exp[4\u03ba k ])d(X 1 k , X 2 k ) 2 + D exp[4\u03ba k ] \u0393 X 1 k X 2 k v k \u2212 u k We have that \u0393 X 1 k X 2 k v 2 k = v k and \u0393 X 1 k X 2 k v 1 k \u2212 u 1 k \u2264 L 1 \u03b3d(X 1 k , X 2 k ).", "formula_coordinates": [41.0, 112.25, 523.56, 314.39, 97.52]}, {"formula_id": "formula_206", "formula_text": "d(X 1 k+1 , X 2 k+1 ) 2 \u2264 (1 + D\u03ba 2 k exp[4\u03ba k ] + D\u03b3 2 exp[4\u03ba k ] + 2\u03b3)d(X 1 k , X 2 k ) 2 .", "formula_coordinates": [42.0, 152.44, 146.01, 307.11, 18.91]}, {"formula_id": "formula_207", "formula_text": "d(X 1 k+1 , X 2 k+1 ) \u2264 (1 + D\u03ba 2 k exp[4\u03ba k ] + D\u03b3 2 exp[4\u03ba k ] + 2\u03b3)d(X 1 k , X 2 k )", "formula_coordinates": [42.0, 156.91, 184.97, 294.85, 18.91]}, {"formula_id": "formula_208", "formula_text": "E[d(X 1 k+1 , X 2 k+1 )] \u2264 (1 + C\u03b3)E[d(X 1 k , X 2 k )", "formula_coordinates": [42.0, 212.46, 220.73, 180.8, 18.91]}, {"formula_id": "formula_209", "formula_text": "\u2022 p t|s (x t |x s )s t (x t ) is a vector field in C 1 \u2200x s . \u2022 |p t|s (x t |x s )s t (x t )| \u2208 L 1 \u2200x s . \u2022 div(p t|s (x t |x s )s t (x t )) \u2208 L 1 \u2200x s .", "formula_coordinates": [42.0, 135.4, 310.13, 186.19, 47.28]}, {"formula_id": "formula_210", "formula_text": "t|s (s t )= M\u00d7M \u2207 log p t|s (x t |x s ) 2 dP s,t (x s , x t ) + M s t (x t ) 2 dP t (x t ) \u22122 M\u00d7M \u2207 log p t|s (x t |x s ), s t (x t ) M dP s,t (x s , x t ) Looking at the last term M\u00d7M \u2207 log p t|s (x t |x s ), s t (x t ) M dP s,t (x s , x t ) = M\u00d7M \u2207 log p t|s (x t |x s ), s t (x t ) M p t|s (x t |x s )p s (x s )d(p ref \u2297 p ref )(x s , x t ) = M M \u2207p t|s (x t |x s ), s t (x t ) M dp ref (x t ) p s (x s )dp ref (x s ) = \u2212 M M div(s t )(x", "formula_coordinates": [42.0, 112.25, 430.96, 348.54, 180.23]}, {"formula_id": "formula_211", "formula_text": "\u03b5 \u2265 B (0,r) |p T (x) \u2212 1| \u2265 C 2 r d (\u03ba\u03b5 \u2212 C 1 (1 + T \u03b2 )r). Since \u03ba\u03b5/(2C 1 (1 + T \u03b2 )) \u2208 (0, r 0 ) we have \u03b5 \u2265 C 2 (\u03ba\u03b5) d+1 /(4C 1 (1 + T \u03b2 )).", "formula_coordinates": [44.0, 112.25, 602.9, 301.49, 55.68]}, {"formula_id": "formula_212", "formula_text": "1 (1 + T \u03b2 )/C 2 ) 1/(d+1) \u03b5 \u22121/(", "formula_coordinates": [44.0, 237.71, 682.37, 113.4, 12.31]}, {"formula_id": "formula_213", "formula_text": "x \u2208 M |p T (x) \u2212 1| \u2264 (8C 1 (1 + T \u03b2 )/C 2 ) 1/(d+1) \u03b5 1\u22121/(d+1) . (34", "formula_coordinates": [44.0, 112.25, 694.63, 383.35, 28.19]}, {"formula_id": "formula_214", "formula_text": ")", "formula_coordinates": [44.0, 495.6, 706.77, 4.15, 8.64]}, {"formula_id": "formula_215", "formula_text": "(8C 1 (1 + T \u03b2 )/C 2 ) 1/(d+1) C 1\u22121/(d+1) 0 e \u2212(1\u22121/(d+1))\u03bb1T \u2264 2 1\u2212\u03b2 C 1 .", "formula_coordinates": [45.0, 169.45, 93.19, 273.11, 19.97]}, {"formula_id": "formula_216", "formula_text": "|p T (x) \u2212 1| \u2264 (8C 1 (1 + T \u03b2 )/C 2 ) 1/(d+1) C 1\u22121/(d+1) 0 e \u2212(1\u22121/(d+1))\u03bb1T ,", "formula_coordinates": [45.0, 161.8, 131.6, 288.39, 19.97]}, {"formula_id": "formula_217", "formula_text": "t \u2208 [0, T ], |\u015d \u03b8 (t, x 0 ) \u2212 \u2202 t log p t (x 0 )| \u2264 M with M \u2265 0.", "formula_coordinates": [45.0, 129.43, 206.06, 214.3, 17.29]}, {"formula_id": "formula_218", "formula_text": "| log p 0 (x 0 ) \u2212 T 0\u015d \u03b8 (t, x 0 )dt| \u2264 C exp[\u2212\u03bb 1 T /2] + MT., where \u03bb 1 is the first non-negative eigenvalue of \u2212\u2206 M in L 2 (p ref ). Proof: First using, Lemma L.1, there exists C 0 , T (a) 0 \u2265 0 such that for any T \u2265 T (a) 0 |p T (x 0 ) \u2212 1| \u2264 C 0 exp[\u2212\u03bb 1 T /2]. Let T (b) 0 = |log(C 0 )| /\u03bb 1 . Using that for any s \u2208 [1/2, +\u221e) we have that | log(1 + s)| \u2264 2 log(2)|s| we get that for any T \u2265 max(T (a) 0 , T (b) 0 ) | log p T (x 0 )| \u2264 2 log(2)C 0 exp[\u2212\u03bb 1 T /2],", "formula_coordinates": [45.0, 112.0, 227.26, 387.74, 137.05]}, {"formula_id": "formula_219", "formula_text": "L(s) = (1/2)E[ T 0 \u03bb(t)s(t, X t )dt] + E[ T 0 \u03bb(t)\u2202 t s(t, X t )dt] +E[ T 0 \u2202 t \u03bb(t)\u2202 t s(t, X t )dt] + E[\u03bb(0)s(0, X 0 )] \u2212 E[\u03bb(T )s(T, X T )], where \u03bb \u2208 C \u221e ([0, T ] , R) is a weighting function. Proof: For any t \u2208 [0, T ] and x t \u2208 M we hav\u00ea s(x t ) = M \u2202 t log p t|0 (x t |x 0 )p 0|t (x 0 |x t )dx 0 . Hence, since M is compact and\u015d \u2208 C \u221e ([0, T ] \u00d7 M, R), we have that\u015d = arg min{L 0 (s) : s \u2208 C \u221e ([0, T ] \u00d7 M, R)} where for any s \u2208 C \u221e ([0, T ] \u00d7 M, R) we have L 0 (s) = T 0 \u03bb(t) M\u00d7M (s(t, x t ) \u2212 \u2202 t log p t|0 (x t |x 0 )) 2 dp 0,t (x 0 , x t )dt = T 0 \u03bb(t) M s(t, x t ) 2 dp t (x t )dt \u22122 T 0 \u03bb(t) M\u00d7M s(t, x t )\u2202 t log p t|0 (x 0 , x t )dp 0,t (x 0 , x t )dt + T 0 \u03bb(t) M dp t (x t )dt(35)", "formula_coordinates": [45.0, 112.25, 455.6, 387.5, 208.04]}, {"formula_id": "formula_220", "formula_text": "T 0 \u03bb(t) M\u00d7M s(t, x t )\u2202 t log p t|0 (x t |x 0 )dp 0,t (x 0 , x t )dt = T 0 M\u00d7M \u03bb(t)s(t, x t )\u2202 t p t|0 (x t )dp 0 (x 0 )dp ref (x t )dt.", "formula_coordinates": [45.0, 177.03, 680.29, 262.65, 39.65]}, {"formula_id": "formula_221", "formula_text": "dY t = {\u2212b(Y t ) + 1 2 \u2207 log p T \u2212t (Y t )}dt.", "formula_coordinates": [47.0, 220.62, 102.02, 170.76, 18.72]}, {"formula_id": "formula_222", "formula_text": "\u2212b + 1 2 s \u03b8 (T \u2212 t, \u2022) (\u03c1(g)Y t ) = \u03c1(g) \u2212b + 1 2 s \u03b8 (T \u2212 t, \u2022) (Y t ).", "formula_coordinates": [47.0, 170.97, 166.96, 274.22, 18.72]}, {"formula_id": "formula_223", "formula_text": "R 3 F \u22121 \u03b8 \u2212 \u2212\u2212 \u2192 R 3 g \u2212 \u2192 R 3 \u2227 \u2212 \u2192 so(3)", "formula_coordinates": [50.0, 108.0, 232.04, 126.1, 23.91]}], "doi": ""}