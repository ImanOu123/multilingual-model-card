{"title": "A Nearly-Linear Time Framework for Graph-Structured Sparsity", "authors": "Chinmay Hegde; Piotr Indyk; Ludwig Schmidt", "pub_date": "", "abstract": "We introduce a framework for sparsity structures defined via graphs. Our approach is flexible and generalizes several previously studied sparsity models. Moreover, we provide efficient projection algorithms for our sparsity model that run in nearly-linear time. In the context of sparse recovery, we show that our framework achieves an information-theoretically optimal sample complexity for a wide range of parameters. We complement our theoretical analysis with experiments demonstrating that our algorithms also improve on prior work in practice.", "sections": [{"heading": "Introduction", "text": "Over the past decade, sparsity has emerged as an important tool in several fields including signal processing, statistics, and machine learning. In compressive sensing, sparsity reduces the sample complexity of measuring a signal, and statistics utilizes sparsity for high-dimensional inference tasks. In many settings, sparsity is a useful ingredient because it enables us to model structure in high-dimensional data while still remaining a mathematically tractable concept. For instance, natural images are often sparse when represented in a wavelet basis, and objects in a classification task usually belong to only a small number of classes.\nDue to the success of sparsity, a natural question is how we can refine the notion of sparsity in order to capture more complex structures. There are many examples where such an approach is applicable: (i) large wavelet coefficients of natural images tend to form connected trees, (ii) active genes can be arranged in functional groups, and (iii) approximate point sources in astronomical data often form clusters. In such cases, exploiting this additional structure can lead to improved compression ratio for images, bet- ter multi-label classification, or smaller sample complexity in compressive sensing and statistics. Hence an important question is the following: how can we model such sparsity structures, and how can we make effective use of this additional information in a computationally efficient manner?\nThere has been a wide range of work addressing these questions, e.g., (Yuan & Lin, 2006;Jacob et al., 2009;He & Carin, 2009;Kim & Xing, 2010;Bi & Kwok, 2011;Huang et al., 2011;Duarte & Eldar, 2011;Bach et al., 2012b;Rao et al., 2012;Negahban et al., 2012;Simon et al., 2013;El Halabi & Cevher, 2015). Usually, the proposed solutions offer a trade-off between the following conflicting goals:\nGenerality What range of sparsity structures does the approach apply to?\nStatistical efficiency What statistical performance improvements does the use of structure enable?\nComputational efficiency How fast are the resulting algorithms?\nIn this paper, we introduce a framework for sparsity models defined through graphs, and we show that it achieves a compelling trade-off between the goals outlined above. At a high level, our approach applies to data with an underlying graph structure in which the large coefficients form a small number of connected components (optionally with additional constraints on the edges). Our approach offers three main features: (i) Generality: the framework encompasses several previously studied sparsity models, e.g., tree sparsity and cluster sparsity. (ii) Statistical efficiency: our sparsity model leads to reduced sample complexity in sparse recovery and achieves the information-theoretic optimum for a wide range of parameters. (iii) Computational efficiency: we give a nearly-linear time algorithm for our sparsity model, significantly improving on prior work both in theory and in practice. Due to the growing size of data sets encountered in science and engineering, algorithms with (nearly-)linear running time are becoming increasingly important.\nWe achieve these goals by connecting our sparsity model to the prize collecting Steiner tree (PCST) problem, which has been studied in combinatorial optimization and approximation algorithms. To establish this connection, we introduce a generalized version of the PCST problem and give a nearly-linear time algorithm for our variant. We believe that our sparsity model and the underlying algorithms are useful beyond sparse recovery, and we have already obtained results in this direction. To keep the presentation in this paper coherent, we focus on our results for sparse recovery and briefly mention further applications in Sec. 7.\nBefore we present our theoretical results in Sections 3 to 5, we give an overview in Section 2. Section 6 complements our theoretical results with an empirical evaluation on both synthetic and real data (a background-subtracted image, an angiogram, and an image of text). We defer proofs and additional details to the supplementary material.\nBasic notation Let [d] be the set {1, 2, . . . , d}. We say that a vector \u03b2 \u2208 R d is s-sparse if at most s of its coefficients are nonzero. The support of \u03b2 contains the indices corresponding to nonzero entries in \u03b2, i.e., supp(\u03b2\n) = {i \u2208 [d] | \u03b2 i = 0}. Given a subset S \u2286 [d],\nwe write \u03b2 S for the restriction of \u03b2 to indices in S: we have (\u03b2\nS ) i = \u03b2 i for i \u2208 S and (\u03b2 S ) i = 0 otherwise. The 2 -norm of \u03b2 is \u03b2 = i\u2208[d] \u03b2 2 i .", "publication_ref": ["b33", "b20", "b16", "b24", "b4", "b19", "b11", "b1", "b28", "b31", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Sparsity models", "text": "In some cases, we have more information about a vector than only \"standard\" s-sparsity. A natural way of encoding such additional structure is via sparsity models (Baraniuk et al., 2010): let M be a family of supports, i.e., M = {S 1 , S 2 , . . . , S L } where\nS i \u2286 [d].\nThen the corresponding sparsity model M is the set of vectors supported on one of the S i :\nM = {\u03b2 \u2208 R d | supp(\u03b2) \u2286 S for some S \u2208 M} . (1)", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Our contributions", "text": "We state our main contributions in the context of sparse recovery (see Section 7 for further applications). Our goal is to estimate an unknown s-sparse vector \u03b2 \u2208 R d from observations of the form\ny = X\u03b2 + e ,(2)\nwhere X \u2208 R n\u00d7d is the design matrix, y \u2208 R n are the observations, and e \u2208 R n is an observation noise vector. By imposing various assumptions on X and e, sparse recovery encompasses problems such as sparse linear regression and compressive sensing.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Weighted graph model (WGM)", "text": "The core of our framework for structured sparsity is a novel, general sparsity model which we call the weighted graph model. In the WGM, we use an underlying graph G = (V, E) defined on the coefficients of the unknown vector\n\u03b2, i.e., V = [d].\nMoreover, the graph is weighted and we denote the edge weights with w : E \u2192 N. We identify supports S \u2286 [d] with subgraphs in G, in particular forests (unions of individual trees). Intuitively, the WGM captures sparsity structures with a small number of connected components in G. In order to control the sparsity patterns, the WGM offers three parameters:\n\u2022 s, the total sparsity of S.\n\u2022 g, the maximum number of connected components formed by the forest F corresponding to S.\n\u2022 B, the bound on the total weight w(F ) of edges in the forest F corresponding to S.\nMore formally, let \u03b3(H) be the number of connected components in a graph H. Then we can define the WGM:\nDefinition 1. The (G, s, g, B)-WGM is the set of supports M = {S \u2286 [d] | |S| = s\nand there is a F \u2286 G with V F = S, \u03b3(F ) = g, and w(F ) \u2264 B} .\n(3) Fig. 1 shows how two sparsity structures can be encoded with the WGM. Since our sparsity model applies to arbitrary graphs G, it can describe a wide range of structures.\nIn particular, the model generalizes several previously studied sparsity models, including 1D-clusters, (wavelet) tree hierarchies, the Earth Mover Distance (EMD) model, and the unweighted graph model (see Table 1).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Recovery of vectors in the WGM", "text": "We analyze the statistical efficiency of our framework in the context of sparse recovery. In particular, we prove that the sample complexity of recovering vectors in the WGM is provably smaller than the sample complexity for \"standard\" s-sparse vectors. To formally state this result, we first introduce a key property of graphs.\nDefinition 2. Let G = (V, E) be a weighted graph with edge weights w : E \u2192 N. Then the weight-degree \u03c1(v) of a node v is the largest number of adjacent nodes connected by edges with the same weight, i.e.,\n\u03c1(v) = max b\u2208N |{(v , v) \u2208 E | w(v , v) = b}| .(4)\nWe define the weight-degree of G to be the maximum weight-degree of any v \u2208 V .\nNote that for graphs with uniform edge weights, the weight-degree of G is the same as the maximum node degree. Intuitively, the (weight) degree of a graph is an important property for quantifying the sample complexity of the WGM because the degree determines how restrictive the bound on the number of components g is. In the extreme case of a complete graph, any support can be formed with only a single connected component (see Figure 1). Using Definitions 1 and 2, we now state our sparse recovery result (see Theorem 12 in Section 5 for a more general version):\nTheorem 3. Let \u03b2 \u2208 R d be in the (G, s, g, B)-WGM. Then n = O s log \u03c1(G) + log B s + g log d g(5)\ni.i.d. Gaussian observations suffice to estimate \u03b2. More precisely, let e \u2208 R n be an arbitrary noise vector and let y \u2208 R n be defined as in Eq. 2 where X is an i.i.d. Gaussian matrix. Then we can efficiently find an estimate \u03b2 such that\n\u03b2 \u2212 \u03b2 \u2264 C e , (6\n)\nwhere C is a constant independent of all variables above.\nNote that in the noiseless case (e = 0), we are guaranteed to recover \u03b2 exactly. Moreover, our estimate \u03b2 is in a slightly enlarged WGM for any amount of noise, see Section 5.\nOur bound (5) can be instantiated to recover previous sample complexity results, e.g., the n = O(s log d s ) bound for \"standard\" sparse recovery, which is tight (Do Ba et al., 2010). 1 For the image grid graph example in Figure 1, Equation ( 5\n) becomes n = O(s + g log d g\n), which matches the information-theoretic optimum n = O(s) as long as the number of clusters is not too large, i.e., g = O(s/ log d). 2\n1 To be precise, encoding s-sparsity with a complete graph as in Figure 1 gives a bound of n = O(s log d). To match the log d s term, we can encode s-sparsity as g = s clusters of size one in a fully disconnected graph with no edges.\n2 Optimality directly follows from a simple dimensionality argument: even if the s-sparse support of the vector \u03b2 is known, recovering the unknown coefficients requires solving a linear system with s unknowns uniquely. For this, we need at least s linear equations, i.e., s observations.", "publication_ref": ["b10"], "figure_ref": ["fig_1", "fig_1", "fig_1"], "table_ref": []}, {"heading": "Efficient projection into the WGM", "text": "The algorithmic core of our sparsity framework is a computationally efficient procedure for projecting arbitrary vectors into the WGM. More precisely, the modelprojection problem is the following: given a vector b \u2208 R d and a WGM M, find the best approximation to b in M, i.e.,\nP M (b) = arg min b \u2208M b \u2212 b .(7)\nIf such a model-projection algorithm is available, one can instantiate the framework of (Baraniuk et al., 2010) in order to get an algorithm for sparse recovery with the respective sparsity model. 3 However, solving Problem ( 7) exactly is NP-hard for the WGM due to a reduction from the classical Steiner tree problem (Karp, 1972). To circumvent this hardness result, we use the approximation-tolerant framework of (Hegde et al., 2014a). Instead of solving ( 7) exactly, the framework requires two algorithms with the following complementary approximation guarantees.\nTail approximation:\nFind an S \u2208 M such that b \u2212 b S \u2264 c T \u2022 min S \u2208M b \u2212 b S .(8)\nHead approximation:\nFind an S \u2208 M such that b S \u2265 c H \u2022 max S \u2208M b S .(9)\nHere, c T > 1 and c H < 1 are arbitrary, fixed constants. Note that a head approximation guarantee does not imply a tail guarantee (and vice versa). In fact, stable recovery is not possible with only one type of approximate projection guarantee (Hegde et al., 2014a). We provide two algorithms for solving (8) and ( 9) (one per guarantee) which both run in nearly-linear time.\nOur model-projection algorithms are based on a connection to the prize-collecting Steiner tree problem (PCST), which is a generalization of the classical Steiner tree problem. Instead of finding the cheapest way to connect all terminal nodes in a given weighted graph, we can instead omit some terminals from the solution and pay a specific price for each omitted node. The goal is to find a subtree with the optimal trade-off between the cost paid for edges used to connect a subset of the nodes and the price of the remaining, unconnected nodes (see Section 3 for a formal definition).\nWe make the following three main algorithmic contributions. Due to the wide applicability of the PCST problem, we believe that these algorithms can be of independent interest (see Section 7).\n\u2022 We introduce a variant of the PCST problem in which the goal is to find a set of g trees instead of a single tree. We call this variant the prize-collecting Steiner forest (PCSF) problem and adapt the algorithm of (Goemans & Williamson, 1995) for this variant.\n\u2022 We reduce the projection problems ( 8) and ( 9) to a small set of adaptively constructed PCSF instances.\n\u2022 We give a nearly-linear time algorithm for the PCSF problem and hence also the model projection problem.", "publication_ref": ["b2", "b23", "b17", "b17", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Improvements for existing sparsity models", "text": "Our results are directly applicable to several previously studied sparsity models that can be encoded with the WGM. Table 1 summarizes these results. In spite of its generality, our approach at least matches the sample complexity of prior work in all cases and actually offers an improvement for the EMD model. Moreover, our running time is always within a polylogarithmic factor of the best algorithm, even in the case of models with specialized solvers such as tree sparsity. For the EMD and cluster models, our algorithm is significantly faster than prior work and improves the time complexity by a polynomial factor. To complement these theoretical results, our experiments in Section 6 show that our algorithm is more than one order of magnitude faster than previous algorithms with provable guarantees and offers a better sample complexity in many cases.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Comparison to related work", "text": "In addition to the \"point-solutions\" for individual sparsity models outlined above, there has been a wide range of work on general frameworks for utilizing structure in sparse recovery. The approach most similar to ours is (Baraniuk et al., 2010), which gives a framework underlying many of the algorithms in Table 1. However, the framework has one important drawback: it does not come with a full recovery algorithm. Instead, the authors only give a recovery scheme that assumes the existence of a model-projection algorithm satisfying (7). Such an algorithm must be constructed from scratch for each model, and the techniques that have been used for various models so far are quite different. Our contribution can be seen as complementing the framework of (Baraniuk et al., 2010) with a nearly-linear time projection algorithm that is applicable to a wide range of sparsity structures. This answers a question raised by the authors of (Huang et al., 2011), who also give a framework for structured sparsity with a universal and complete recovery algorithm. Their framework is applicable to a wide range of sparsity models, but the corresponding algorithm is significantly slower than ours, both in theory (\"Graph clusters\" in Table 1) and in practice (see Section 6). Moreover, our recovery algorithm shows more robust performance across different shapes of graph clusters.\nBoth of the approaches mentioned above use iterative greedy algorithms for sparse recovery. There is also a large body of work on combining M-estimators with convex regularizers that induce structured sparsity, e.g., see the surveys (Bach et al., 2012a) and (Wainwright, 2014). The work closest to ours is (Jacob et al., 2009), which uses an overlapping group Lasso to enforce graph-structured sparsity (graph Lasso). In contrast to their approach, our algorithm gives more fine-grained control over the number of clusters in the graph. Moreover, our algorithm has better computational complexity, and to the best of our knowledge there are no formal results relating the graph structure to the sample complexity of the graph Lasso. Emprirically, our algorithm recovers an unknown vector with graph structure faster and from fewer observations than the graph Lasso (see Section A in the supplementary material).", "publication_ref": ["b2", "b2", "b19", "b0", "b32", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "The prize-collecting Steiner forest problem", "text": "We now establish our connection between prize-collecting Steiner tree (PCST) problems and the weighted graph model. First, we formally define the PCST problem: Let G = (V, E) be an undirected, weighted graph with edge costs c : E \u2192 R + 0 and node prizes \u03c0 : V \u2192 R + 0 . For a subset of edges E \u2286 E, we write c(E ) = e\u2208E c(e) and adopt the same convention for node subsets. Moreover, for a node subset V \u2286 V , let V be the complement V = V \\ V . Then the goal of the PCST problem is to find a subtree T = (V , E ) such that c(E ) + \u03c0(V ) is minimized. We sometimes write c(T ) and \u03c0(T ) if the node and edge sets are clear from context. Similar to the classical Steiner tree problem, PCST is NP-hard. Most algorithms with provable approximation guarantees build on the seminal work of (Goemans & Williamson, 1995) (GW), who gave an efficient primaldual algorithm with the following guarantee:\nc(T ) + 2\u03c0(T ) \u2264 2 min T is a tree c(T ) + \u03c0(T ) . (10\n)\nNote that the PCST problem already captures three important aspects of the WGM: (i) there is an underlying graph G, (ii) edges are weighted, and (iii) nodes have prizes. If we set the prizes to correspond to vector coefficients, i.e.,\n\u03c0(i) = b 2 i , the term \u03c0(T ) in the PCST objective function becomes \u03c0(T ) = b \u2212 b T\n2 , which matches the objective in the model-projection problems (8) and (9). However, there are two important differences. First, the objective in the PCST problem is to find a single tree T , while the WGM can contain supports defined by multiple connected components (if g > 1). Moreover, the PCST problem optimizes the trade-off c(T ) + \u03c0(T ), but we are interested in minimizing b \u2212 b T subject to hard constraints on the support cardinality |T | and the support cost c(T ) (the parameters s and B, respectively). In this section, we ad-Table 1. Results of our sparsity framework applied to several sparsity models. In order to simplify the running time bounds, we assume that all coefficients are polynomially bounded in d, and that s \u2264 d 1/2\u2212\u00b5 for some \u00b5 > 0. For the graph cluster model, we consider the case of graphs with constant degree. The exponent \u03c4 depends on the degree of the graph and is always greater than 1. The parameters w and h are specific to the EMD model, see (Hegde et al., 2014a) for details. We always have w \u2022 h = d and s \u2265 w. Our sparsity framework improves on the sample complexity and running time of both the EMD and graph cluster models (bold entries). dress the first of these two issues; Section 4 then completes the connection between PCST and the WGM. We begin by defining the following variant of the PCST problem.", "publication_ref": ["b15", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Model", "text": "Definition 4 (The prize-collecting Steiner forest problem).\nLet g \u2208 N be the target number of connected components.\nThen the goal of the prize-collecting Steiner forest (PCSF) problem is to find a subgraph F = (V , E ) with \u03b3(F ) = g that minimizes c(E ) + \u03c0(V ).\nAs defined in Section 2.1, \u03b3(F ) is the number of connected components in the (sub-)graph F . To simplify notation in the rest of the paper, we say that a forest F is a g-forest if \u03b3(F ) = g. There is always an optimal solution for the PCSF problem which consists of g trees because removing edges cannot increase the objective value. This allows us to employ the PCSF problem for finding supports in the WGM that consist of several connected components.\nIn order to give a computationally efficient algorithm for the PCSF variant, we utilize prior work for PCST: (i) To show correctness of our algorithm, we prove that the GW scheme for PCST can be adapted to our PCSF variant. (ii) To achieve a good time complexity, we show how to simulate the GW scheme in nearly-linear time.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "The Goemans-Williamson (GW) scheme for PCSF", "text": "A useful view of the GW scheme is the \"moat-growing\" interpretation of (J\u00fcnger & Pulleyblank, 1995), which describes the algorithm as an iterative clustering method that constructs \"moats\" around every cluster. These moats are essentially the dual variables in the linear program of the GW scheme. Initially, every node forms its own active cluster with a moat of size 0. The moats around each active cluster then grow at a uniform rate until one of the following two events occurs:\nCluster deactivation When the sum of moats in a cluster reaches the sum of node prizes in that cluster, the cluster is deactivated.\nCluster merge When the sum of moats that are intersected by an edge e reaches the cost of e, the clusters at the two endpoints of e are merged and e is added to the current solution.\nThe moat-growing stage of the algorithm terminates when only a single active cluster remains. After that, the resulting set of edges is pruned in order to achieve a provable approximation ratio. We generalize the proof of (Feofiloff et al., 2010) and show that it is possible to extract more than one tree from the moat-growing phase as long as the trees come from different clusters. Our modification of GW terminates the moat-growing phase when exactly g active clusters remain, and we then apply the GW pruning algorithm to each resulting tree separately. This gives the following result.\nTheorem 5. There is an algorithm for the PCSF problem that returns a g-forest F such that c(F ) + 2\u03c0(F ) \u2264 min\nF \u2286G, \u03b3(F )\u2264g 2c(F ) + 2\u03c0(F ) . (11\n)\nFor g = 1, the theorem recovers the guarantee in (10). We defer the proof to Sec. D.1 of the supplementary material.", "publication_ref": ["b22", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "A fast algorithm for Goemans-Williamson", "text": "While the modified GW scheme produces good approximate solutions, it is not yet sufficient for a nearly-linear time algorithm: we still need an efficient way of simulating the moat-growing phase. There are two main difficulties: (i) The remaining \"slack\" amounts on edges can shrink at different rates depending on how many of the edge endpoints are in active clusters. (ii) A single cluster event (merge or deactivation) can change this rate for up to \u0398(|V |) edges. In order to maintain edge events efficiently, we use the dynamic edge splitting approach introduced by (Cole et al., 2001). This technique essentially ensures that every edge always has at most one active endpoint, and hence its slack either shrinks at rate 1 or not at all. However, edge splitting introduces additional edge events that do not directly lead to a cluster merge. While it is relatively straightforward to show that every such intermediate edge event halves the remaining amount of slack on an edge, we still need an overall bound on the number of intermediate edge events necessary to achieve a given precision. For this, we prove the following new result about the GW moat growing scheme.\nTheorem 6. Let all edge costs c(e) and node prizes \u03c0(v) be even integers. Then all finished moats produced by the GW scheme have integer sizes.\nIn a nutshell, this theorem shows that one additional bit of precision is enough to track all events in the moat-growing phase accurately. We prove the theorem via induction over the events in the GW scheme, see Section D.2.2 for details.\nOn the theoretical side, this result allows us to bound the overall running time of our algorithm for PCSF. Combined with suitable data structures, we can show the following:\nTheorem 7. Let \u03b1 be the number of bits used to specify a single edge cost or node prize. Then there is an algorithm achieving the PCSF guarantee of Theorem 5 in time\nO(\u03b1 \u2022 |E| log|V |).\nOn the practical side, we complement Theorem 6 with a new adaptive edge splitting scheme that leads to a small number of intermediate edge events. Our experiments show that our scheme results in less than 3 events per edge on average (see Section D.3 in the supplementary material).", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Sparse approximation with the WGM", "text": "In order to utilize the WGM in sparse recovery, we employ the framework of (Hegde et al., 2014a). As outlined in Section 2.3, the framework requires us to construct two approximation algorithms satisfying the head-and tailapproximation guarantees (8) and ( 9). We now give two such model-projection algorithms, building on our tools for PCSF developed in the previous section.", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Tail-approximation algorithm", "text": "We can connect the PCSF objective to the WGM quantities by setting \u03c0(i) = b 2 i and c(e) = w(e) + 1, which gives:\nc(F ) = w(F ) + (|F | \u2212 g) and \u03c0(F ) = b \u2212 b F 2 .\nAfter multiplying the edge costs with a trade-off parameter \u03bb, the PCSF objective \u03bb\u2022c(F )+\u03c0(F ) essentially becomes a Lagrangian relaxation of the model-constrained optimization problem (8). We build our tail-approximation algorithm on top of this connection, starting with an algorithm for the \"tail\"-variant of the PCSF problem. By performing a binary search over the parameter \u03bb (see Algorithm 1), we get a bicriterion guarantee for the final forest.\nAlgorithm 1 PCSF-TAIL 1: Input: G, c, \u03c0, g, cost-budget C, parameters \u03bd and \u03b4.\n2: We write c \u03bb (e) = \u03bb \u2022 c(e) .\n3:\n\u03c0 min \u2190 min \u03c0(i)>0 \u03c0(i), \u03bb 0 \u2190 \u03c0min 2C 4: F \u2190 PCSF-GW(G, c \u03bb0 , \u03c0, g) 5: if c(F ) \u2264 2C and \u03c0(F ) = 0 then return F 6: \u03bb r \u2190 0, \u03bb l \u2190 3\u03c0(G), \u03b5 \u2190 \u03c0min\u03b4 C 7: while \u03bb l \u2212 \u03bb r > \u03b5 do 8: \u03bb m \u2190 (\u03bb l + \u03bb r )/2 9: F \u2190 PCSF-GW(G, c \u03bbm , \u03c0, g) 10:\nif c(F ) \u2265 C and c(F ) \u2264 \u03bdC then return F\n11: if c(F ) > \u03b3C then \u03bb r \u2190 \u03bb m else \u03bb l \u2190 \u03bb m 12: end while 13: return F \u2190 PCSF-GW(G, c \u03bb l , \u03c0, g) Theorem 8. Let \u03bd > 2 and \u03b4 > 0. Then PCSF-TAIL re- turns a g-forest F \u2286 G such that c(F ) \u2264 \u03bd \u2022 C and \u03c0(F ) \u2264 1 + 2 \u03bd \u2212 2 + \u03b4 min \u03b3(F )=g,c(F )\u2264C \u03c0(F ) . (12\n)\nTheorem 8 does not give c(F ) \u2264 C exactly, but the cost of the resulting forest is still guaranteed to be within a constant factor of C. The framework of (Hegde et al., 2014a) also applies to projections into such slightly larger models. As we will see in Section 5, this increase by a constant factor does not affect the sample complexity.\nFor the trade-off between support size and support weight, we also make use of approximation. By scalarizing the two constraints carefully, i.e., setting c(e) = w(e) + B s , we get the following result. The proofs of Theorems 8 and 9 can be found in the supplementary material, Section C.1.\nTheorem 9. Let M be a (G, s, g, B)-WGM, let b \u2208 R d , and let \u03bd > 2. Then there is an algorithm that returns a support\nS \u2286 [d] in the (G, 2\u03bd \u2022 s + g, g, 2\u03bd \u2022 B)-WGM satisfying (8) with c T = 1 + 3/(\u03bd \u2212 2) . Moreover, the algorithm runs in time O(|E| log 3 d).", "publication_ref": ["b17"], "figure_ref": [], "table_ref": []}, {"heading": "Head-approximation algorithm", "text": "For our head-approximation algorithm, we also use the PCSF objective as a Lagrangian relaxation of the modelconstraint problem (9). This time, we multiply the node prizes instead of the edge costs with a parameter \u03bb. We perform a binary search similar to Alg. 1, but the final step of the algorithm requires an extra subroutine. At the end of the binary search, we are guaranteed to have a forest with good \"density\" \u03c0(F ) c(F ) , but the good forest could correspond to either the lower bound \u03bb l or the upper bound \u03bb r . In the latter case, we have no bound on the cost of the corresponding forest F r . However, it is always possible to extract a high-density sub-forest with bounded cost from F r :\nAlgorithm 2 GRAPH-COSAMP 1: Input: y, X, G, s, g, B, number of iterations t.\n2: \u03b2 0 \u2190 0 3: for i \u2190 1, . . . , t do\n4: b \u2190 X T (y \u2212 X \u03b2 i\u22121 ) 5: S \u2190 supp( \u03b2 i\u22121 ) \u222a HEADAPPROX'(b, G, s, g, B) 6: z S \u2190 X \u2020 S y, z S C \u2190 0 7:\nS \u2190 TAILAPPROX(z, G, s, g, B)\n8:\n\u03b2 i \u2190 z S 9: end for 10: return \u03b2 \u2190 \u03b2 i Lemma 10. Let T be a tree and C \u2264 c(T ). Then there is a subtree T \u2286 T such that c(T ) \u2264 C and \u03c0(T ) \u2265 C 6 \u2022 \u03c0(T ) c(T ) . Moreover, we can find such a subtree T in linear time.\nThe algorithm first converts the tree into a list of nodes corresponding to a tour through T . Then we can extract a good sub-tree by either returning a single, high-prize node or sliding a variable-size window across the list of nodes. See Section C.2 in the supplementary material for details. Combining these components, we get a headapproximation algorithm with the following properties. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Application in sparse recovery", "text": "We now instantiate the framework of (Hegde et al., 2014a) to give a sparse recovery algorithm using the WGM. The resulting algorithm (see Alg. 2) is a variant of CoSaMP (Needell & Tropp, 2009) and uses the head-and tailapproximation algorithms instead of the hard thresholding operators. 4 In order to state the corresponding recovery guarantee in full generality, we briefly review the definition of the (model-) restricted isometry property (RIP) (Cand\u00e8s & Tao, 2005;Baraniuk et al., 2010). We say that a matrix X satisfies the (M, \u03b4)-model-RIP if for all \u03b2 \u2208 M:\n(1 \u2212 \u03b4) \u2022 \u03b2 2 \u2264 X\u03b2 2 \u2264 (1 + \u03b4) \u2022 \u03b2 2 . (13\n)\nTheorem 12. Let \u03b2 \u2208 R d be in the (G, s, g, B)-WGM M and let X \u2208 R n\u00d7d be a matrix satisfying the model-RIP for a (G, c 1 s, g, c 2 B)-WGM and a fixed constant \u03b4, where c 1 and c 2 are also fixed constants. Moreover, let e \u2208 R n be an arbitrary noise vector and let y \u2208 R n be defined as in (2).\nThen GRAPH-COSAMP returns a \u03b2 in the (G, 5s, g, 5B)-WGM such that \u03b2 \u2212 \u03b2 \u2264 c 3 e , where c 3 is a fixed constant. Moreover, GRAPH-COSAMP runs in time\nO (T X + |E| log 3 d) log \u03b2 e ,\nwhere T X is the time complexity of a matrix-vector multiplication with X.\nIn order to establish sample complexity bounds for concrete matrix ensembles (e.g., random Gaussian matrices as in Theorem 3), we use a result of (Baraniuk et al., 2010) that relates the sample complexity of sub-Gaussian matrix ensembles to the size of the model, i.e., the quantity |M|. Next, we turn our attention to the running time of GRAPH-COSAMP. Since our model-projection algorithms run in nearly-linear time, the matrix-vector products involving X can become the bottleneck in the overall time complexity: 5 for a dense Gaussian matrix, we have T X = \u2126(sd), which would dominate the overall running time. If we can control the design matrix (as is often the case in compressive sensing), we can use the construction of (Hegde et al., 2014b) to get a sample-optimal matrix with nearly-linear T X in the regime of s \u2264 d 1/2\u2212\u00b5 , \u00b5 > 0. Such a matrix then gives an algorithm with nearly-linear running time. Note that the bound on s is only a theoretical restriction in this construction: as our experiments show, a partial Fourier matrix empirically performs well for significantly larger values of s.", "publication_ref": ["b17", "b26", "b5", "b2", "b2", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We focus on the performance of our algorithm Graph-CoSaMP for the task of recovering 2D data with clustered sparsity. Multiple methods have been proposed for this problem, and our theoretical analysis shows that our algorithm should improve upon the state of the art (see Table 1). We compare our results to StructOMP (Huang et al., 2011) and the heuristic Lattice Matching Pursuit (LaMP) (Cevher et al., 2009a). The implementations were supplied by the  In the regime where the algorithms recover with high probability, the estimates \u03b2 are essentially identical to the original images. Our algorithm Graph-CoSaMP achieves consistently good recovery performance and offers the best sample complexity for images (b) and (c). Moreover, our algorithm is about 20 times faster than StructOMP, the other method with provable guarantees for the image cluster model.\nauthors and we used the default parameter settings. Moreover, we ran two common recovery algorithms for \"standard\" s-sparsity: Basis Pursuit (Cand\u00e8s et al., 2006) and CoSaMP (Needell & Tropp, 2009). We follow a standard evaluation procedure for sparse recovery / compressive sensing: we record n observations y = X\u03b2 of the (vectorized) image \u03b2 \u2208 R d using a subsampled Fourier matrix X. We assume that all algorithms possess prior knowledge of the sparsity s and the number of connected-components g in the true support of the image \u03b2.\nWe declare a trial successful if the squared 2 -norm of the recovery error is at most 5% of the squared 2 -norm of the original vector \u03b2. The probability of successful recovery is then estimated by averaging over 50 trials. We perform several experiments with varying oversampling ratios n/s and three different images. See Section A in the supplementary material for a description of the dataset, experiments with noise, and a comparison with the graph Lasso.\nFigure 2 demonstrates that Graph-CoSaMP yields consistently competitive phase transitions and exhibits the best sample complexity for images with \"long\" connected clusters, such as the angiogram image (b) and the text image (c). While StructOMP performs well on \"blob\"-like images such as the background-subtracted image (a), its performance is poor in our other test cases. For example, it can successfully recover the text image only for oversampling ratios n/s > 15. Note that the performance of Graph-CoSaMP is very consistent: in all three examples, the phase transition occurs between oversampling ratios 3 and 4. Other methods show significantly more variability.\nWe also investigate the computational efficiency of Graph-CoSaMP. We consider resized versions of the angiogram image and record n = 6s observations for each image size d. Figure 2(d) displays the recovery times (averaged over 50 trials) as a function of d. We observe that the runtime of Graph-CoSaMP scales nearly linearly with d, comparable to the conventional sparse recovery methods. Moreover, Graph-CoSaMP is about 20\u00d7 faster than StructOMP.", "publication_ref": ["b19", "b7", "b6", "b26"], "figure_ref": ["fig_6", "fig_6"], "table_ref": []}, {"heading": "Further applications", "text": "We expect our algorithms to be useful beyond sparse recovery and now briefly describe two promising applications.\nSeismic feature extraction In (Schmidt et al., 2015), the authors use Steiner tree methods for a seismic feature extraction task. Our new algorithms for PCSF give a principled way of choosing tuning parameters for their proposed optimization problem.. Moreover, our fast algorithms for PCSF can speed-up their method.\nEvent detection in social networks (Rozenshtein et al., 2014) introduce a method for event detection in social networks based on the PCST problem. Their method performs well but produces spurious results in the presence of multiple disconnected events because their PCST algorithm produces only a single tree instead of a forest. Our new algorithm for PCSF gives exact control over the number of trees in the solution and hence directly addresses this issue. Furthermore, the authors quote a running time of O(|V | 2 log|V |) for their GW scheme, so our nearly-linear time algorithm allows their method to scale to larger data.", "publication_ref": ["b30", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Jayadev Acharya, Stefanie Jegelka, Youssef Mroueh, Devavrat Shah, and the anonymous reviewers for many helpful comments on earlier versions of this paper. This work was supported by grants from the MITEI-Shell program, the MADALGO center, and the Simons Investigator Award.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Optimization with sparsityinducing penalties. Foundations and Trends in Machine Learning", "journal": "", "year": "2012", "authors": "Francis Bach; Rodolphe Jenatton; Julien Mairal; Guillaume Obozinski"}, {"ref_id": "b1", "title": "Structured sparsity through convex optimization", "journal": "Statistical Science", "year": "2012", "authors": "Francis Bach; Rodolphe Jenatton; Julien Mairal; Guillaume Obozinski"}, {"ref_id": "b2", "title": "Model-based compressive sensing", "journal": "IEEE Transactions on Information Theory", "year": "2010", "authors": "Richard G Baraniuk;  Cevher;  Volkan; Marco F Duarte; Chinmay Hegde"}, {"ref_id": "b3", "title": "Prize-collecting steiner problems on planar graphs", "journal": "", "year": "2011", "authors": "Mohammadhossein Bateni;  Chekuri;  Chandra; Alina Ene; Mohammad T Hajiaghayi; Nitish Korula; Daniel Marx"}, {"ref_id": "b4", "title": "Multi-label classification on tree-and DAG-structured hierarchies", "journal": "", "year": "2011", "authors": "Wei Bi; James T Kwok"}, {"ref_id": "b5", "title": "Decoding by linear programming", "journal": "IEEE Transactions on Information Theory", "year": "2005", "authors": "Emmanuel J Cand\u00e8s; Terence Tao"}, {"ref_id": "b6", "title": "Robust uncertainty principles: exact signal reconstruction from highly incomplete frequency information", "journal": "IEEE Transactions on Information Theory", "year": "2006", "authors": "Emmanuel J Cand\u00e8s; Justin Romberg; Terence Tao"}, {"ref_id": "b7", "title": "Sparse signal recovery using markov random fields", "journal": "", "year": "2009", "authors": " Cevher;  Volkan; Marco F Duarte; Chinmay Hegde; Richard Baraniuk"}, {"ref_id": "b8", "title": "Recovery of clustered sparse signals from compressive measurements", "journal": "", "year": "2009", "authors": " Cevher;  Volkan;  Indyk;  Piotr; Chinmay Hegde; Richard G Baraniuk"}, {"ref_id": "b9", "title": "A faster implementation of the Goemans-Williamson clustering algorithm", "journal": "", "year": "2001", "authors": "Richard Cole;  Hariharan;  Ramesh; Moshe Lewenstein; Ely Porat"}, {"ref_id": "b10", "title": "Lower bounds for sparse recovery", "journal": "", "year": "2010", "authors": "Do Ba;  Khanh;  Indyk;  Piotr; Eric Price; David P Woodruff"}, {"ref_id": "b11", "title": "Structured compressed sensing: From theory to applications", "journal": "IEEE Transactions on Signal Processing", "year": "2011", "authors": "Marco F Duarte; Yonina C Eldar"}, {"ref_id": "b12", "title": "An efficient polynomial-time approximation scheme for steiner forest in planar graphs", "journal": "", "year": "2012", "authors": "David Eisenstat; Philip Klein; Claire Mathieu"}, {"ref_id": "b13", "title": "A totally unimodular view of structured sparsity", "journal": "", "year": "2015", "authors": "El Halabi;  Marwa; Volkan Cevher"}, {"ref_id": "b14", "title": "A note on Johnson, Minkoff and Phillips' algorithm for the prize-collecting Steiner tree problem", "journal": "Computing Research", "year": "2010", "authors": "Paulo Feofiloff; Cristina G Fernandes; Carlos E Ferreira; Jos\u00e9 De Pina;  Coelho"}, {"ref_id": "b15", "title": "A general approximation technique for constrained forest problems", "journal": "SIAM Journal on Computing", "year": "1995", "authors": "Michel X Goemans; David P Williamson"}, {"ref_id": "b16", "title": "Exploiting structure in wavelet-based bayesian compressive sensing", "journal": "IEEE Transactions on Signal Processing", "year": "2009", "authors": "Lihan He; Lawrence Carin"}, {"ref_id": "b17", "title": "Approximation algorithms for model-based compressive sensing", "journal": "", "year": "2014", "authors": "Chinmay Hegde; Piotr Indyk; Ludwig Schmidt"}, {"ref_id": "b18", "title": "Nearly linear-time model-based compressive sensing", "journal": "", "year": "2014", "authors": "Chinmay Hegde; Piotr Indyk; Ludwig Schmidt"}, {"ref_id": "b19", "title": "Learning with structured sparsity", "journal": "The Journal of Machine Learning Research", "year": "2011", "authors": "Junzhou Huang; Tong Zhang; Dimitris Metaxas"}, {"ref_id": "b20", "title": "Group Lasso with overlap and graph Lasso", "journal": "", "year": "2009", "authors": "Laurent Jacob; Guillaume Obozinski; Jean-Philippe Vert"}, {"ref_id": "b21", "title": "The prize collecting Steiner tree problem: Theory and practice", "journal": "", "year": "2000", "authors": "David S Johnson; Maria Minkoff; Steven Phillips"}, {"ref_id": "b22", "title": "New primal and dual matching heuristics", "journal": "Algorithmica", "year": "1995", "authors": "Michael J\u00fcnger; William R Pulleyblank"}, {"ref_id": "b23", "title": "Reducibility among combinatorial problems", "journal": "", "year": "1972", "authors": "Richard M Karp"}, {"ref_id": "b24", "title": "Tree-guided group Lasso for multi-task regression with structured sparsity", "journal": "", "year": "2010", "authors": "Seyoung Kim; Eric P Xing"}, {"ref_id": "b25", "title": "A primal-dual algorithm for group sparse regularization with overlapping groups", "journal": "", "year": "2010", "authors": "Sofia Mosci;  Villa;  Silvia; Alessandro Verri; Lorenzo Rosasco"}, {"ref_id": "b26", "title": "CoSaMP: Iterative signal recovery from incomplete and inaccurate samples", "journal": "Applied and Computational Harmonic Analysis", "year": "2009", "authors": "Deanna Needell; Joel A Tropp"}, {"ref_id": "b27", "title": "A unified framework for highdimensional analysis of m-estimators with decomposable regularizers", "journal": "Statistical Science", "year": "", "authors": "Sahand N Negahban;  Ravikumar;  Pradeep; Martin J Wainwright;  Yu;  Bin"}, {"ref_id": "b28", "title": "Universal measurement bounds for structured sparse signal recovery", "journal": "", "year": "2012", "authors": "Nikhil S Rao; Ben Recht; Robert D Nowak"}, {"ref_id": "b29", "title": "Event detection in activity networks", "journal": "", "year": "2014", "authors": "Polina Rozenshtein;  Anagnostopoulos;  Aris; Aristides Gionis; Nikolaj Tatti"}, {"ref_id": "b30", "title": "Seismic feature extraction using Steiner tree methods", "journal": "", "year": "2015", "authors": "Ludwig Schmidt;  Hegde;  Chinmay;  Indyk;  Piotr;  Lu; Chi Ligang; Xingang Hohl; Detlef "}, {"ref_id": "b31", "title": "A sparse-group Lasso", "journal": "Journal of Computational and Graphical Statistics", "year": "2013", "authors": "Noah Simon; Jerome Friedman; Trevor Hastie; Robert Tibshirani"}, {"ref_id": "b32", "title": "Structured regularizers for highdimensional problems: Statistical and computational issues", "journal": "Annual Review of Statistics and Its Application", "year": "2014", "authors": "Martin J Wainwright"}, {"ref_id": "b33", "title": "Model selection and estimation in regression with grouped variables", "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": "2006", "authors": "Ming Yuan; Yi Lin"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Proceedings of the 32 nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s).", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 .1Figure 1. Two examples of the weighted graph model. (a) In a complete graph, any s-sparse support can be mapped to a single tree (g = 1). (b) Using a grid graph, we can model a small number of clusters in an image by setting g accordingly. For simplicity, we use unit edge weights and set B = s \u2212 g in both examples.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "2 B log d) O(wh 2 log 4 d) Graph clusters (Huang et al., 2011) O(s + g log d) O(s + g log d g ) O(d \u03c4 ) O(d log 4 d)", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Theorem 11 .11Let M be a (G, s, g, B)-WGM and let b \u2208 R d . Then there is an algorithm that returns a support S \u2286 [d] in the (G, 2s + g, g, 2B)-WGM satisfying (9) with c H = 1/14 . The algorithm runs in time O(|E| log 3 d).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "More precisely, n = O(s + log|M|) rows / observations suffice for such matrices to satisfy the model-RIP for M and a fixed constant \u03b4. For the WGM, we use a counting argument to bound |M| (see Section B in the supplementary material). Together with Theorem 12, the following theorem establishes Theorem 3 from Section 2.2. Theorem 13. Let M be the set of supports in the (G, s, g, B)-WGM. Then log|M| = O s log \u03c1(G)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 2 .2Figure2. Sparse recovery experiments. The images in the top row are the original images \u03b2. In the regime where the algorithms recover with high probability, the estimates \u03b2 are essentially identical to the original images. Our algorithm Graph-CoSaMP achieves consistently good recovery performance and offers the best sample complexity for images (b) and (c). Moreover, our algorithm is about 20 times faster than StructOMP, the other method with provable guarantees for the image cluster model.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": ") = {i \u2208 [d] | \u03b2 i = 0}. Given a subset S \u2286 [d],", "formula_coordinates": [2.0, 55.44, 327.12, 234.0, 21.61]}, {"formula_id": "formula_1", "formula_text": "S ) i = \u03b2 i for i \u2208 S and (\u03b2 S ) i = 0 otherwise. The 2 -norm of \u03b2 is \u03b2 = i\u2208[d] \u03b2 2 i .", "formula_coordinates": [2.0, 55.44, 351.03, 234.0, 38.0]}, {"formula_id": "formula_2", "formula_text": "S i \u2286 [d].", "formula_coordinates": [2.0, 225.15, 452.31, 38.57, 9.65]}, {"formula_id": "formula_3", "formula_text": "M = {\u03b2 \u2208 R d | supp(\u03b2) \u2286 S for some S \u2208 M} . (1)", "formula_coordinates": [2.0, 64.82, 491.17, 224.62, 11.37]}, {"formula_id": "formula_4", "formula_text": "y = X\u03b2 + e ,(2)", "formula_coordinates": [2.0, 142.74, 589.09, 146.7, 8.96]}, {"formula_id": "formula_5", "formula_text": "\u03b2, i.e., V = [d].", "formula_coordinates": [2.0, 335.49, 296.19, 67.88, 8.96]}, {"formula_id": "formula_6", "formula_text": "Definition 1. The (G, s, g, B)-WGM is the set of supports M = {S \u2286 [d] | |S| = s", "formula_coordinates": [2.0, 307.44, 515.83, 230.18, 31.61]}, {"formula_id": "formula_7", "formula_text": "\u03c1(v) = max b\u2208N |{(v , v) \u2208 E | w(v , v) = b}| .(4)", "formula_coordinates": [3.0, 83.0, 176.62, 206.44, 14.66]}, {"formula_id": "formula_8", "formula_text": "Theorem 3. Let \u03b2 \u2208 R d be in the (G, s, g, B)-WGM. Then n = O s log \u03c1(G) + log B s + g log d g(5)", "formula_coordinates": [3.0, 55.44, 355.06, 234.0, 41.28]}, {"formula_id": "formula_9", "formula_text": "\u03b2 \u2212 \u03b2 \u2264 C e , (6", "formula_coordinates": [3.0, 136.8, 458.35, 148.77, 8.96]}, {"formula_id": "formula_10", "formula_text": ")", "formula_coordinates": [3.0, 285.57, 458.67, 3.87, 8.64]}, {"formula_id": "formula_11", "formula_text": ") becomes n = O(s + g log d g", "formula_coordinates": [3.0, 101.65, 581.25, 118.24, 13.47]}, {"formula_id": "formula_12", "formula_text": "P M (b) = arg min b \u2208M b \u2212 b .(7)", "formula_coordinates": [3.0, 366.35, 160.6, 175.09, 16.6]}, {"formula_id": "formula_13", "formula_text": "Find an S \u2208 M such that b \u2212 b S \u2264 c T \u2022 min S \u2208M b \u2212 b S .(8)", "formula_coordinates": [3.0, 373.31, 317.2, 168.13, 36.12]}, {"formula_id": "formula_14", "formula_text": "Find an S \u2208 M such that b S \u2265 c H \u2022 max S \u2208M b S .(9)", "formula_coordinates": [3.0, 389.13, 367.99, 152.32, 36.12]}, {"formula_id": "formula_15", "formula_text": "c(T ) + 2\u03c0(T ) \u2264 2 min T is a tree c(T ) + \u03c0(T ) . (10", "formula_coordinates": [4.0, 326.14, 512.61, 211.15, 14.74]}, {"formula_id": "formula_16", "formula_text": ")", "formula_coordinates": [4.0, 537.29, 512.93, 4.15, 8.64]}, {"formula_id": "formula_17", "formula_text": "\u03c0(i) = b 2 i , the term \u03c0(T ) in the PCST objective function becomes \u03c0(T ) = b \u2212 b T", "formula_coordinates": [4.0, 307.44, 585.99, 234.0, 24.32]}, {"formula_id": "formula_18", "formula_text": "F \u2286G, \u03b3(F )\u2264g 2c(F ) + 2\u03c0(F ) . (11", "formula_coordinates": [5.0, 387.78, 475.97, 149.51, 15.05]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [5.0, 537.29, 476.29, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "O(\u03b1 \u2022 |E| log|V |).", "formula_coordinates": [6.0, 55.44, 343.2, 73.27, 8.74]}, {"formula_id": "formula_21", "formula_text": "c(F ) = w(F ) + (|F | \u2212 g) and \u03c0(F ) = b \u2212 b F 2 .", "formula_coordinates": [6.0, 60.89, 604.06, 223.1, 12.62]}, {"formula_id": "formula_22", "formula_text": "\u03c0 min \u2190 min \u03c0(i)>0 \u03c0(i), \u03bb 0 \u2190 \u03c0min 2C 4: F \u2190 PCSF-GW(G, c \u03bb0 , \u03c0, g) 5: if c(F ) \u2264 2C and \u03c0(F ) = 0 then return F 6: \u03bb r \u2190 0, \u03bb l \u2190 3\u03c0(G), \u03b5 \u2190 \u03c0min\u03b4 C 7: while \u03bb l \u2212 \u03bb r > \u03b5 do 8: \u03bb m \u2190 (\u03bb l + \u03bb r )/2 9: F \u2190 PCSF-GW(G, c \u03bbm , \u03c0, g) 10:", "formula_coordinates": [6.0, 307.94, 105.1, 193.53, 94.47]}, {"formula_id": "formula_23", "formula_text": "11: if c(F ) > \u03b3C then \u03bb r \u2190 \u03bb m else \u03bb l \u2190 \u03bb m 12: end while 13: return F \u2190 PCSF-GW(G, c \u03bb l , \u03c0, g) Theorem 8. Let \u03bd > 2 and \u03b4 > 0. Then PCSF-TAIL re- turns a g-forest F \u2286 G such that c(F ) \u2264 \u03bd \u2022 C and \u03c0(F ) \u2264 1 + 2 \u03bd \u2212 2 + \u03b4 min \u03b3(F )=g,c(F )\u2264C \u03c0(F ) . (12", "formula_coordinates": [6.0, 307.44, 202.72, 234.0, 116.79]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [6.0, 537.29, 304.25, 4.15, 8.64]}, {"formula_id": "formula_25", "formula_text": "S \u2286 [d] in the (G, 2\u03bd \u2022 s + g, g, 2\u03bd \u2022 B)-WGM satisfying (8) with c T = 1 + 3/(\u03bd \u2212 2) . Moreover, the algorithm runs in time O(|E| log 3 d).", "formula_coordinates": [6.0, 307.44, 505.63, 234.0, 33.68]}, {"formula_id": "formula_26", "formula_text": "4: b \u2190 X T (y \u2212 X \u03b2 i\u22121 ) 5: S \u2190 supp( \u03b2 i\u22121 ) \u222a HEADAPPROX'(b, G, s, g, B) 6: z S \u2190 X \u2020 S y, z S C \u2190 0 7:", "formula_coordinates": [7.0, 60.42, 117.47, 226.74, 48.7]}, {"formula_id": "formula_27", "formula_text": "(1 \u2212 \u03b4) \u2022 \u03b2 2 \u2264 X\u03b2 2 \u2264 (1 + \u03b4) \u2022 \u03b2 2 . (13", "formula_coordinates": [7.0, 75.78, 593.0, 209.51, 11.92]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [7.0, 285.29, 596.29, 4.15, 8.64]}, {"formula_id": "formula_29", "formula_text": "O (T X + |E| log 3 d) log \u03b2 e ,", "formula_coordinates": [7.0, 356.74, 115.04, 135.41, 22.31]}], "doi": ""}