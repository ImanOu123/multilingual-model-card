{"title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs", "authors": "Simone Balloccu; Patr\u00edcia Schmidtov\u00e1; Mateusz Lango; Ond\u0159ej Du\u0161ek", "pub_date": "", "abstract": "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of indirect data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using Ope-nAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to \u223c4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.", "sections": [{"heading": "Introduction", "text": "The recent emergence of large language models (LLMs), that show remarkable performance on a wide range of tasks, has led not only to a dramatic increase in their use in research but also to a growing number of companies joining the race for the biggest and most powerful models. In pursuing a competitive advantage, many popular LLMs today are locked behind API access and their details are unknown (OpenAI, 2023;Thoppilan et al., 2022;Touvron et al., 2023). This includes model weights (OpenAI, 2023), training data (Piktus et al., 2023), or infrastructural details to assess model carbon footprint (Lacoste et al., 2019).\nIn particular, the lack of information on training data raises important questions about the credibility of LLMs performance evaluation. The data from which these models learn, typically collected automatically by scraping documents from the web, may contain training, validation, and -most critically -test sets coming from NLP benchmarks. Because of this, researchers and stakeholders may later inadvertently evaluate LLMs on the same data they were trained on. This phenomenon, known as data contamination, may not be an issue in the general use of commercial LLMs, where adherence to research principles is not mandatory, but it becomes a serious problem when these models are widely used and evaluated in research.\nUnfortunately, many proprietary models are locked behind inference-only APIs, making it hard to inspect data contamination. Because of this, existing work on the matter mostly focuses on detecting extreme forms of overfitting and memorization, such as the model's ability to generate benchmarks verbatim. These approaches are not only limited but also neglect that recent proprietary LLMs get iteratively improved from user interactions. If such interactions involve benchmark data (for example when researchers evaluate LLMs against baselines), the model may, in fact, become contaminated even if it was contamination-free during its initial training. We refer to this phenomenon as indirect data leaking.\nIn this paper, we address the issue of indirect data contamination in closed-source 1 LLMs by conducting a systematic literature review. We review 255 papers and carefully detail data leakage emerging from them. We focus primarily on the models accessible through OpenAI's ChatGPT, 2 (GPT-3.5 and GPT-4 3 ) as these are the most frequently used commercial LLMs in NLP research. By considering OpenAI's data usage policy, we assess how much data was reported to be sent to the models in a way that it could be used for further training, hence giving the models an unfair advantage during evaluation. We also report a series of emergent evaluation malpractices, including lack of comparison with other approaches, differences in the evaluation scale (e.g., evaluating open models on entire benchmarks while comparing to proprietary LLMs evaluated on samples only), lack of code and data access, or data leakage even in situations where it could be avoided. To our knowledge, this work is the most comprehensive and extensive quantification of the data leakage issue in LLMs to date.\nIn short, our contributions are as follows:\n(1) We systematically analyse 255 papers evaluating OpenAI's GPT-3.5 and GPT-4 on a variety of tasks in NLP and other domains (Section 4).\n(2) For each paper, we estimate the amount of data leaked in such a way that it could be used for further model training. Overall, we conclude that \u223c42% of the reviewed papers leaked data to GPT-3.5 and GPT-4, for a total of \u223c4.7M benchmark samples across 263 benchmarks (Section 5.1).\n(3) We further analyse the evaluation protocols of the selected papers, and we reveal some critical malpractices limiting both the experiments' reproducibility and fairness (Sections 5.2 and 5.3).\n(4) Based on our findings, we propose a list of suggested practices for the evaluation of closed-source LLMs (Section 6).\nWe believe that our work can contribute to ongoing efforts on quantifying LLM data contamination by pointing out which datasets are worthy of further investigation. We release our survey results as a collaborative repository, in the form of a webpage at https://leak-llm.github.io/. It features a list of datasets, detailing the extend of data leakage for each of them. We invite other researchers to contribute any additional known leaks to the list.", "publication_ref": ["b21", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Prior Work on LLM Data Contamination", "text": "Work on LLMs data contamination traces back to OpenAI's GPT-3 (Brown et al., 2020;Magar and Schwartz, 2022), one of the first models with APIonly access and limited training data disclosure.\nDespite results hinting at the presence of significant data contamination (Raffel et al., 2020;Magar and Schwartz, 2022), the model has been used extensively in research and the issue was rarely taken into account when interpreting its performance. With the release of ChatGPT and following closed-source models to general public, 4 the data contamination topic became an even more pressing issue.\nWhen a model is closed-source, it becomes implicitly complex to assess data contamination from known benchmarks. Therefore, only few practical approaches have been proposed to investigate the issue.\nOne notable example is the LM Contamination Index, 5 featuring a regularly updated estimate of contamination for a list of both open and proprietary models. This approach works by zero-shot prompting the model to generate instances from specific datasets, providing details on the required split and format (Sainz et al., 2023). The premise is that no model should be able to replicate specific benchmark formats without having seen them first.\nMore applied approaches have been proposed recently , where LLMs are prompted to complete a given sentence coming from a known benchmark. The completion is then compared with the original reference through text overlap metrics and a statistical test is used to assess if the model is contaminated.\nAlthough these preliminary works are promising, they cannot be fully trusted and have some limitations. Most importantly, they are based on an assessment of the model's ability to generate an example from the benchmark. The recall of such methods can be affected by two issues:\n(1) Some closed-source models have incorporated special filters into their decoding algorithms that prevent them from generating texts that significantly overlap with their training sets (GitHub, 2022;. This creates an additional noise for the detection methods and results in the lack of confidence that even the datasets tested negative for data leakage are not present in LLM training data.\n(2) Such approaches can only detect the most extreme form of overfitting which results in (almost) complete memorization of data samples by the model. However, even a regular adjustment of the model by training on the leaked data, which does not necessarily lead to its memorization, poses a problem for fair comparisons.", "publication_ref": ["b16", "b22", "b16", "b23", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "The Issue of Indirect Data Leaking", "text": "The related work presented in Section 2 approaches the issue of data contamination mainly by backtracking models' training data. It is commonly assumed that using benchmarks available only to authorised parties, or datasets being constructed after the ChatGPT release, is a guarantee that they have not been leaked. This ignores the fact that models using reinforcement learning from human feedback (RLHF, Ouyang et al., 2022), such as those used by ChatGPT, are subject to repeated updates  with training data also coming from user interactions. This process leads to a previously overlooked phenomenon, where new data are leaked to the model just through using it. We refer to this problem as indirect data leaking and consider it a new development of the issue for two main reasons:\n(1) Unlike plain text scraped from the internet, data from users might be harder to inspect for contamination as it might involve model prompts, textual alterations, or truncation of benchmark samples.\n(2) Users supply the data along with instructions on how to perform the task. In LLMs, this can be considered a novel form of gold-standard data for continued training, even in the absence of target labels. Model updates on such data are likely much more effective than plain in-domain text.\nThe issue (1) is particularly complex to trace, even with a conscious and targeted effort by the LLM vendor. When evaluating a closed-source LLM, users often feed the model with test-set samples (with or without labels) surrounded by additional text, such as instructions in the form of prompts. In some cases, especially when evaluating the LLM robustness, the test-set samples are perturbed and hence no longer an exact match of their original version. Therefore, it is unlikely that LLM vendors could effectively exclude leaked benchmarks from further model fine-tuning, especially at scale. For (2), it would be necessary to understand how the LLM vendor uses the data to improve the model. A very likely scenario is continued pretraining, where the data leaked by users is treated as an in-domain corpus (and thus given more influence than pretraining data). This procedure is known to improve models' performances in the leaked domains (Gururangan et al., 2020). Notably,  find that fine-tuning a model on in-domain text enriched by textual instructions leads to an increase in the model performance even if gold labels are not shown to the model. This setup perfectly matches the kind of data shown to chat LLMs when evaluated by researchers. This means that closed-source LLMs such as GPT-3.5 and GPT-4 can make use of these gold standard examples from widely used NLP benchmarks to gain an unfair advantage over other models.\nWe also point out that recent work  showed that after model updates, Chat-GPT performance improved on benchmarks to which it was previously exposed (Zhang et al., 2022). With these motivations, we conduct a systematic review to quantify how much of such data the models powering ChatGPT could have obtained.", "publication_ref": ["b20", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Methodology", "text": "Following the standard systematic review protocol from the medical domain (Khan et al., 2003), we analyse the existing work on LLMs evaluation to inspect the issue of indirect data contamination and other evaluation malpractices. We focus on OpenAI's GPT-3.5 and GPT-4 models, as they are the most prominently used in recent NLP research. We organize our work into five macro-steps, corresponding to the following subsections.", "publication_ref": ["b11"], "figure_ref": [], "table_ref": []}, {"heading": "Framing questions", "text": "In reviewing the existing work evaluating the performace of GPT-3.5 and GPT-4, we pose the following research questions:\n(1) Which datasets have been demonstrably leaked to GPT-3.5 and GPT-4 during the last year?\n(2) Do all papers evaluating these models include a fair comparison with existing baselines?", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Identifying relevant work", "text": "We employ commonly used online databases 6 and major NLP conferences proceedings (including ACL, NAACL, EMNLP, NeurIPS), considering both peer-reviewed work and pre-prints, as the interaction with LLMs happened regardless of publication status. We filter our queries on work containing the terms \"ChatGPT\", \"GPT-4\", \"GPT-3.5\" \"OpenAI\" \"evaluation\", \"large language models\", \"AI\" either in title, abstract, body, or all of them. We also do not limit our search to computer science works only, as recent LLMs have been investigated by researchers from many other domains, e.g. healthcare , psychology  and education . Since the ChatGPT models are our primary focus, we limit our search to works between late November 2022 (when the first model was publicly released) and early October 2023. Among all the papers, we first do a preliminary screening, assessing if they effectively run GPT-3.5 or GPT-4 in any form. 7", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Assessing quality and relevance", "text": "To assess which work effectively leaked data to ChatGPT, we refer to OpenAI's data usage policy, 8  Therefore, only the work interacting with the models through the web interface 9 is considered to 6 We query Google Scholar, Semantic Scholar, DBLP, arXiV, ACL Anthology. 7 We encountered a small number of papers also comparing to other closed-source LLMs, such as Anthropic's Claude.\n8 https://help.openai.com/en/articles/ 5722486-how-your-data-is-used-to-improve-model-performance 9 https://chat.openai.com/ leak data. We note that while it is possible to opt out of providing the data for model improvement purposes, 21 we found no evidence suggesting any of the surveyed papers did so.\nA small number of works used both the web interface and API access. 10 We carefully review such works to calculate which portion of the data was used in the former setup. We drew our conclusions from the paper draft history on arXiv; in some cases, this information was also transparently disclosed by the authors. In the case of work with multiple drafts dating before the model release in November 2022, we consider the earliest draft that includes GPT-3.5 or GPT-4 for the calculation.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Summarizing the evidence", "text": "We inspect each surveyed paper, looking for information on the used datasets, split, and number of samples. If no mention of sampling or similar information is made, we assume that the whole dataset has been used. Similarly, if no information on the used split is provided, we assume that the authors treated the dataset as a whole. It could be argued that feeding entire datasets to ChatGPT is unrealistic because of the usage restrictions imposed by OpenAI on the web interface, and the amount of work necessary for manually inputting the data inside the chat. However, we note that quickly after ChatGPT release, many unofficial wrappers have been developed 11 for circumventing said issues, most of which are still in active use. We also point out that many of the papers we surveyed mentioned the use of such tools explicitly.\nWe also track secondary information relevant to the evaluation -for each work, we inspect: (1) if it has been peer-reviewed; 12 (2) if the used prompts are available; (3) if a repository to reproduce the experiment is provided; (4) if the authors used a whole dataset or a sample; (5) if GPT-3.5 or GPT-4 were compared to other open models/approaches and if the evaluation scale was the same; (6) if the version of the model used is reported.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interpreting the findings", "text": "We report the results of our review both quantitatively and qualitatively. Specifically, we report the number of works surveyed leaking data to GPT-3.5 or GPT-4 in such a way that it can be used by OpenAI to further improve the model (according to their data policy). In this paper we do not distinguish between works leaking data to GPT-3.5, GPT-4, or both. This is because indirect data leaking is caused by browser access, where both models are available through the ChatGPT Plus subscription. We also note that OpenAI confirmed that creating GPT-4 involved the use of ChatGPT to some extent. 13 For this reason, we estimate the data leakage to be effectively shared across the two models and for simplicity, we refer to both models as \"ChatGPT\" from now on.\nWe also document a series of evaluation practices emerging for the work reviewed that is problematic with respect to objectiveness and reproducibility. Finally, drawing upon our results, we present a series of best practices for researchers evaluating OpenAI's and other closedsource LLMs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Following our methodology, in the first step we identified 255 research papers, 212 of which were found relevant 14 during the initial screening (see Sec. 4.2). Among the relevant papers, 70 (\u223c 32%) were peer-reviewed, while the remainder (142) consisted of pre-prints. 15 We subsequently analysed the retrieved papers to examine the problem of data contamination and the adopted evaluation practices.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Indirect data contamination", "text": "From our analysis, 90 papers (\u223c 42%) accessed ChatGPT through the web interface, hence providing data that OpenAI could have used to further improve its models.\nWe first inspected the time distribution of the reviewed works (Figure 1) to gain insight into when most data leaks happened. Unsurprisingly, the majority of the papers leaking data dates before the official release of ChatGPT API, and it can be seen that web interface access rapidly decreased following March 2023. However, we must note that (1) a considerable amount of work kept using the web interface to access ChatGPT until September 2023 and (2) our analysis cannot inspect the preliminary stages of prompt engineering, which are rarely reported and might still be done through the web interface because of its trial-and-error nature.\nThe presence of leaked data after the API release may indicate that a part of the research community is either unaware of OpenAI's data policy, or does not consider it a problem when conducting experiments. Many works, especially small case studies, also reported using the web interface for cost reasons, as it allows free access to the models.\nAs a second step, we quantified leak severity per dataset and split. For work specifying the amount of data used (either in the paper or through a repository), we consider the given value. For the rest, we calculate it by inspecting the actual dataset. 16 In seven papers, no number of samples used was specified, so we contacted the authors for clarification. In the two cases where the authors did not respond, we assumed the entire split of a dataset was used. We calculated both the number of instances and the percentage of the considered split (or the whole dataset when applicable).\nSince a small number of datasets ( 18) was used in multiple papers in different amounts, we had to consider whether these should be interpreted as Figure 2: Data leakage distribution. We report the number of times (y) we observed a specific percentage of leaking (x) for the considered split. As some work vaguely describes the used split as \"test or dev set\", we merge these two values in a unique chart.\nindividual separate leaks (that should be summed up) or not. We were not able to verify this from the provided data, so we adopted an \"optimistic\" approach and assumed that the largest leak for a given dataset is always a superset of all smaller ones. 17 Our calculations show that the 90 papers leaked data from 263 unique datasets, for a total of over 4.7M samples (see Tables 4 to 6 in the Appendix). 18 We find most samples (\u223c 93.8%) coming from datasets treated as whole (with no split), followed by test and development (\u223c 5.6%), 19 and training (\u223c 0.6%) sets. In line with what we discussed in Section 3, we can conclude that ChatGPT was exposed to millions of benchmark samples, enriched with instructions that could be considered de-facto novel gold-standard data in some cases.\nWe also report that several works included the examples' labels when few-shot prompting Chat-GPT or using it as a reference-based evaluation metric. We consider this the worst possible case of data leaking, as it gives the model information 17 We also tried a pessimistic approach, where we assumed all the leaks were independent, but due to the small number of works covering the same data, the results are virtually identical. 18 The survey total is 4,714,753 leaked samples. 19 As some work vaguely describes the used split as \"test or dev set\", we merge these two values.  about the desired output as well.\nTo classify leak severity, we examine the frequency distribution of leak sizes (Figure 2). It appears that most works either leak full splits or very small samples, with only a few works leaking intermediate amounts. With this information, we classify a portion of leaked data as low (< 5%), moderate-low (5 \u2212 50%), moderate-high (50 \u2212 95%), or high (> 95%).\nConsequently, we categorize all leaked datasets into these 4 thresholds. Overall, we find a low leak for 66 (\u223c 25%) datasets, moderate-low for 47 (\u223c 18%), moderate-high for 10 (\u223c 4%) and high for 142 (\u223c 53%). This result is particularly worrying as the majority of datasets were almost completely leaked.\nFinally, we inspect which NLP tasks are covered by the leaked data (Table 1). We find that the tasks suffering the most from high leaks are natural language inference, question answering, and natural language generation. These and other tasks include many highly popular NLP benchmarks, as well as high-quality custom datasets created adhoc for individual evaluations (see Tables 4 to 6 in the Appendix). To name a few, almost the entire test sets from Semeval2016 Task 6 (Mohammad et al., 2016), SAMSum (Gliwa et al., 2019), and MultiWOZ 2.4 (Ye et al., 2022) are leaked. The custom datasets were frequently phrased as an exam in a field different from NLP, e.g., medicine, physics, psychology, or law. Other custom datasets explored, for example, the LLMs' sense of humour, philosophical and political leaning, or bias. We note that not all the leaked custom datasets have been publicly released. This makes the leak even more severe, as it potentially makes OpenAI the only organisation (besides the authors) with access to such data.", "publication_ref": ["b17", "b7", "b29"], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "Reproducibility", "text": "We assess the evaluations' reproducibility by checking whether the prompts used to query ChatGPT were provided, whether a repository containing data or code was available, and whether the datasets used were custom-made. Finally, we also check for sampling of the original data or other practices that make it impossible to exactly reconstruct the data used.\nFrom our results (Figure 3), 192 (\u223c 91%) works report the prompts used to convert data into a query and possibly to instruct the model on how to perform a given task. The number of works providing a code repository is significantly smaller, at 113 (\u223c 53%). This figure excludes papers that provided a link to a non-existent or empty repository. Overall, 72 (\u223c 51%) of the pre-prints and 34 (\u223c 48%) peer-reviewed papers provided both prompts and a repository. We report further details on this data in Appendix B.\nAnother barrier to reproducibility is that most closed-source LLMs are being regularly updated. Therefore, it is crucial to report the used model version, as different versions may lead to significantly different outputs . In the surveyed works, this was generally done by reporting the running period of the experiments when using the web interface, or by reporting which version of the model has been accessed via the API. Unfortunately, as regular model updates are a relatively new concept, this practice is not yet common. Only 29 (40%) of the peer-reviewed papers and 33 (23%) of the pre-prints provide this information.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Evaluation fairness", "text": "We find the evaluation of ChatGPT's performance to be often unfair. First, comparison to any open-source LLM or non-LLM-based method may be missing. Our results (Figure 4) show that this is similarly prevalent regardless of the publication status, appearing in 71 (\u223c 50%) of pre-prints and 30 (\u223c 43%) of published papers. Second, when a comparison with open models and baselines is made, 54 pre-prints (\u223c 38%) and 34 peer-reviewed (\u223c 49%) papers compare the results computed on different samples. ChatGPT is typically evaluated on a random sample of the benchmark while other models are compared on its entirety. In many works, Chat-GPT's performance is measured on only a handful (10-50) of examples, which substantially lowers the expressive power of the comparison. For instance, considering a simplistic case with binary assessment of model output (correct/incorrect) on 10 examples, the difference should be more than 30% to be statistically significant, 20 which is rarely seen. Statistical analysis of results is almost never performed. We report further details on evaluation fairness in Appendix B.\nAnother concerning practice is how the size of the evaluation data is reported, especially when sampling is used. We find that papers often show the size of the whole evaluation dataset upfront (e.g. in a table or in the dataset description section), but they report the actual sample sizes used for evaluation only later and in a less obvious way (in footnotes, limitations sections, or appendices). This practice makes the experimental results harder to interpret.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Suggested Practices in Closed-source LLM Evaluation", "text": "Our survey revealed both a significant amount of data leakage in ChatGPT and many worrying trends in its evaluation. In light of this, we list a series of suggested practices that we believe could help mitigate the issues. We believe that researchers looking to objectively evaluate LLMs today should:\nAccess the model in a way that does not leak data The first step when planning proprietary LLMs evaluation should be reading their most upto-date data policies, and access models accordingly (e.g. API instead of web interface for Ope-nAI's LLMs). We also acknowledge that in some cases this might not be viable due to budget limits, or an overly steep learning curve for the use of APIs by researchers outside of computer science. 21 Interpret performance with caution The lack of system specifications and training details can make proprietary LLMs look like incredibly powerful tools with impressive zero-shot performance. This can often be explained by data contamination . In our review, we documented that over 4 million samples across more than 200 NLP datasets have been leaked to these models. The performance of closed-source LLMs should always be interpreted while keeping these results in mind.\nWhen possible, avoid using closed-source models We strongly encourage using the available open-source LLMs. While there has been discussion in the research community about proprietary models being consistently better than open-source ones, we note that (1) this is often driven by hype, while there is evidence of the opposite (Koco\u0144 et al., 2023), ( 2) research done solely on closed LLMs limits scientific progress, bringing benefits mainly to the LLM vendors and (3) LLM vendors can arbitrarily make changes to the models, e.g., making previous versions unavailable, changing their behaviour in a way that may not be visible to the user  or changing the data treatment policy.\nAdopt a fair and objective comparison Evaluating closed-source LLMs is tied to comparing them with pre-existing approaches. Evaluating proprietary models on a limited number of samples while evaluating open ones on dramatically larger sets is scientifically dubious at best. When sampling is required (for example because of budgetary restrictions), it should be applied to all the considered approaches. We also discourage taking state-of-theart values directly from previous work and suggest to re-run all approaches on the considered data only.\nMake the evaluation reproducible In light of the known NLP evaluation reproducibility crisis (Belz et al., 2023;Thomson et al., 2024) we strongly encourage researchers to report as many details about their setup. Besides all the relevant details about the setup for reproducibility, such as random seeds, open model parameters, etc., we note that when the evaluation involves closed models, additional details should be disclosed. Prompts, as well as the process leading to them, should be detailed since LLMs are very sensitive to even minor changes in prompts (Lu et al., 2022). The model version and experiment running period should be mentioned as well so that further researchers can use the same model checkpoint if possible. Data, especially if sampled, should be released (ideally in a repository) to avoid potential differences in sampling.\nReport indirect data leaking Indirect data leaking is a serious issue, and when it happens it should be reported. Clear information on which benchmarks have been leaked benefits research, helps other researchers orient their experiments, and ultimately leads to a more objective evaluation of proprietary LLMs. We invite all researchers to contribute to our collaborative project at https: //leak-llm.github.io/.", "publication_ref": ["b1", "b26", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "In this work, we present our findings based on the analysis of 255 papers evaluating the performance of GPT-3.5 and GPT-4. We investigate the problem of indirect data contamination and report that 4.7M samples coming from 263 distinct datasets have been exposed to the models in such a way that this data could be used for training by OpenAI. We also report concerning research practices with respect to reproducibility and fairness. Finally, informed by our analysis, we detailed some suggested practices for the evaluation of closed-source LLMs.\nFuture Work In our future work, we aim to run experiments via the OpenAI API to see the impact of leaked test data on the performance of GPT-3.5 and GPT-4 on the leaked datasets and the tasks in general. Furthermore, we consider investigating indirect data leakage in other closed-source models, namely from Anthropic or Cohere, which appeared in a small number of papers reviewed in this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Limitations", "text": "We are aware the list of contaminated datasets we compiled in our work is not fully conclusive for one of several reasons:\n(1) We review the information that has been publicly revealed via articles. We postulate more experiments could have revealed test set data to closed-source models but were never published.\n(2) In this paper, we focus on the works that use ChatGPT or GPT-4. However, prior to March 1st, 2023, OpenAI's policy stated that they may also use data from the API to improve their models. This would imply that data sent to GPT-3 via the API could have been used for training.\n(3) The number of papers investigating the performance of ChatGPT is vast, and despite our best efforts, we could have missed some works.\n(4) Information on whether individual works are pre-prints or published is given at the time of writing (early October 2023). This is subject to change, especially given the freshness of many of the works reviewed.\n(5) Many datasets released prior to 2021 could have been fully leaked by being a part of the models' pre-training data.\nAs mentioned in Section 4, in some cases the papers were not clear about some aspects of the experiments. We contacted the authors of such papers for clarification, however, two of them did not respond. Therefore, our best-judgment assumptions may be wrong for these papers.\nXuanting Chen, Junjie Ye, Can Zu, Nuo Xu, Rui Zheng, Minlong Peng, Jie Zhou, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023d. How robust is GPT-3.5 to predecessors? a comprehensive study on language understanding tasks.  and code/data repository (Repo), the usage of custom datasets (Custom), the application of random sampling or any other practice that does not allow the exact reconstruction of the data used (Sampl.).  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was supported by the European Research Council (Grant agreement No. 101039303 NG-NLG) and by Charles University project SVV 260 698. Patr\u00edcia Schmidtov\u00e1 was also supported by the Women in Quant Finance Network grant awarded by G-Research. We would also like to thank Zden\u011bk Kasner and Dominik Mach\u00e1\u010dek for their valuable feedback on the manuscript.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Full list of the reviewed work", "text": "In this section, we list all the work that we reviewed and classified as relevant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Detail on evaluation malpractices", "text": "As the Sankey diagrams showed in Section 5.2 and Section 5.3 offer limited insights on our findings regarding evaluation reproducibility and fairness, we do provide additional details in this section. We provide concrete numbers for our assessment of reproducibility (Sec. 5.2) and evaluation (mal)practices (Sec. 5.3) in Tables 2 and 3, respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Detailed List of ChatGPT Data Leak", "text": "We show which datasets have been leaked to Chat-GPT in Tables 4 and 5.    ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Can we trust the evaluation on Chat-GPT?", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Rachith Aiyappa; Jisun An; Haewoon Kwak; Yongyeol Ahn"}, {"ref_id": "b1", "title": "Non-repeatable experiments and nonreproducible results: The reproducibility crisis in human evaluation in NLP", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Anya Belz; Craig Thomson; Ehud Reiter; Simon Mille"}, {"ref_id": "b2", "title": "Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners", "journal": "Curran Associates, Inc", "year": "", "authors": "Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared D Kaplan; Prafulla Dhariwal; Arvind Neelakantan; Pranav Shyam; Girish Sastry; Amanda Askell; Sandhini Agarwal; Ariel Herbert-Voss; Gretchen Krueger; Tom Henighan; Rewon Child; Aditya Ramesh; Daniel Ziegler; Jeffrey Wu; Clemens Winter; Chris Hesse; Mark Chen; Eric Sigler; Mateusz Litwin"}, {"ref_id": "b3", "title": "Does ChatGPT resemble humans in language use? arXiv preprint", "journal": "", "year": "2023", "authors": "G Zhenguang; David A Cai; Xufeng Haslett; Shuqi Duan; Martin J Wang;  Pickering"}, {"ref_id": "b4", "title": "How is ChatGPT's behavior changing over time?", "journal": "", "year": "2023", "authors": "Lingjiao Chen; Matei Zaharia; James Zou"}, {"ref_id": "b5", "title": "", "journal": "", "year": "", "authors": "Aakanksha Chowdhery; Sharan Narang; Jacob Devlin; Maarten Bosma; Gaurav Mishra; Adam Roberts; Paul Barham;  Hyung Won; Charles Chung; Sebastian Sutton; Parker Gehrmann; Kensen Schuh; Sasha Shi; Joshua Tsvyashchenko; Abhishek Maynez; Parker Rao; Yi Barnes; Noam Tay; Vinodkumar Shazeer; Emily Prabhakaran; Nan Reif; Ben Du; Reiner Hutchinson; James Pope; Jacob Bradbury; Michael Austin; Guy Isard; Pengcheng Gur-Ari; Toju Yin; Anselm Duke; Sanjay Levskaya; Sunipa Ghemawat; Henryk Dev; Xavier Michalewski; Vedant Garcia; Kevin Misra; Liam Robinson; Denny Fedus; Daphne Zhou; David Ippolito; Hyeontaek Luan;  Lim"}, {"ref_id": "b6", "title": "About github copilot", "journal": "", "year": "2022", "authors": " Github"}, {"ref_id": "b7", "title": "SAMSum corpus: A humanannotated dialogue dataset for abstractive summarization", "journal": "", "year": "2019", "authors": "Bogdan Gliwa; Iwona Mochol; Maciej Biesek; Aleksander Wawer"}, {"ref_id": "b8", "title": "Time travel in LLMs: Tracing data contamination in large language models", "journal": "", "year": "2023", "authors": "Shahriar Golchin; Mihai Surdeanu"}, {"ref_id": "b9", "title": "2020. Don't stop pretraining: Adapt language models to domains and tasks", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Ana Suchin Gururangan; Swabha Marasovi\u0107; Kyle Swayamdipta; Iz Lo; Doug Beltagy; Noah A Downey;  Smith"}, {"ref_id": "b10", "title": "Preventing generation of verbatim memorization in language models gives a false sense of privacy", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Daphne Ippolito; Florian Tramer; Milad Nasr; Chiyuan Zhang; Matthew Jagielski; Katherine Lee; Christopher Choquette Choo; Nicholas Carlini"}, {"ref_id": "b11", "title": "Five steps to conducting a systematic review", "journal": "Journal of the Royal Society of Medicine", "year": "2003", "authors": "Regina Khalid S Khan; Jos Kunz; Gerd Kleijnen;  Antes"}, {"ref_id": "b12", "title": "Stanis\u0142aw Wo\u017aniak, and Przemys\u0142aw Kazienko. 2023. ChatGPT: Jack of all trades", "journal": "", "year": "", "authors": "Jan Koco\u0144; Igor Cichecki; Oliwier Kaszyca; Mateusz Kochanek; Dominika Szyd\u0142o; Joanna Baran; Julita Bielaniewicz; Marcin Gruza; Arkadiusz Janz; Kamil Kanclerz; Anna Koco\u0144; Bart\u0142omiej Koptyra; Wiktoria Mieleszczenko-Kowszewicz; Piotr Mi\u0142kowski; Marcin Oleksy; Maciej Piasecki; \u0141ukasz Radli\u0144ski; Konrad Wojtasik"}, {"ref_id": "b13", "title": "Performance of ChatGPT on USMLE: Potential for AIassisted medical education using large language models", "journal": "Plos Digit Health", "year": "2023", "authors": " Th Kung;  Cheatham; C Medenilla;  Sillos; C De Leon; M Elepa\u00f1o;  Madriaga;  Aggabao;  Diaz-Candido;  Maningo"}, {"ref_id": "b14", "title": "Quantifying the carbon emissions of machine learning", "journal": "", "year": "2019", "authors": "Alexandre Lacoste; Alexandra Luccioni; Victor Schmidt; Thomas Dandres"}, {"ref_id": "b15", "title": "Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity", "journal": "Long Papers", "year": "2022", "authors": "Yao Lu; Max Bartolo; Alastair Moore; Sebastian Riedel; Pontus Stenetorp"}, {"ref_id": "b16", "title": "Data contamination: From memorization to exploitation", "journal": "Association for Computational Linguistics", "year": "2022", "authors": "Inbal Magar; Roy Schwartz"}, {"ref_id": "b17", "title": "Parinaz Sobhani, Xiaodan Zhu, and Colin Cherry", "journal": "", "year": "2016", "authors": "Saif Mohammad; Svetlana Kiritchenko"}, {"ref_id": "b18", "title": "SemEval-2016 task 6: Detecting stance in tweets", "journal": "", "year": "", "authors": ""}, {"ref_id": "b19", "title": "OpenAI. 2023. GPT-4 technical report", "journal": "", "year": "", "authors": ""}, {"ref_id": "b20", "title": "Training language models to follow instructions with human feedback", "journal": "Curran Associates, Inc", "year": "2022-01", "authors": "Long Ouyang; Jeffrey Wu; Xu Jiang; Diogo Almeida; Carroll Wainwright; Pamela Mishkin; Chong Zhang; Sandhini Agarwal; Katarina Slama; Alex Ray; John Schulman; Jacob Hilton; Fraser Kelton; Luke Miller; Maddie Simens; Amanda Askell; Peter Welinder; F Paul;  Christiano"}, {"ref_id": "b21", "title": "The ROOTS search tool: Data transparency for LLMs", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Aleksandra Piktus; Christopher Akiki; Paulo Villegas; Hugo Lauren\u00e7on; G\u00e9rard Dupont; Sasha Luccioni; Yacine Jernite; Anna Rogers"}, {"ref_id": "b22", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "J. Mach. Learn. Res", "year": "2020", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b23", "title": "Did Chat-GPT cheat on your test?", "journal": "", "year": "2023", "authors": "Oscar Sainz; Jon Ander Campos; Iker Garc\u00eda-Ferrero; Julen Etxaniz; Eneko Agirre"}, {"ref_id": "b24", "title": "Don't stop pretraining? make prompt-based fine-tuning powerful learner", "journal": "", "year": "2023", "authors": "Zhengxaing Shi; Aldo Lipani"}, {"ref_id": "b25", "title": "Analyzing chatgpt's aptitude in an introductory computer engineering course", "journal": "", "year": "2023", "authors": "Jakub Szefer; Sanjay Deshpande"}, {"ref_id": "b26", "title": "Common Flaws in Running Human Evaluation Experiments in NLP", "journal": "", "year": "2024", "authors": "Craig Thomson; Ehud Reiter; Anya Belz"}, {"ref_id": "b27", "title": "", "journal": "Johnny Soraker", "year": "", "authors": "Romal Thoppilan; Daniel De Freitas; Jamie Hall; Noam Shazeer; Apoorv Kulshreshtha;  Heng-Tze; Alicia Cheng; Taylor Jin; Leslie Bos; Yu Baker; Yaguang Du; Hongrae Li;  Lee; Amin Huaixiu Steven Zheng; Marcelo Ghafouri; Yanping Menegali; Maxim Huang; Dmitry Krikun; James Lepikhin; Dehao Qin; Yuanzhong Chen; Zhifeng Xu; Adam Chen; Maarten Roberts; Vincent Bosma; Yanqi Zhao; Chung-Ching Zhou; Igor Chang; Will Krivokon; Marc Rusch; Pranesh Pickett; Laichee Srinivasan; Kathleen Man;  Meier-Hellstern"}, {"ref_id": "b28", "title": "Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models", "journal": "", "year": "", "authors": "Hugo Touvron; Thibaut Lavril; Gautier Izacard; Xavier Martinet; Marie-Anne Lachaux; Timoth\u00e9e Lacroix; Naman Baptiste Rozi\u00e8re; Eric Goyal;  Hambro"}, {"ref_id": "b29", "title": "MultiWOZ 2.4: A multi-domain task-oriented dialogue dataset with essential annotation corrections to improve state tracking evaluation", "journal": "", "year": "2022", "authors": "Fanghua Ye; Jarana Manotumruksa; Emine Yilmaz"}, {"ref_id": "b30", "title": "Daijun Ding, and Liwen Jing. 2022. How would stance detection techniques evolve after the launch of ChatGPT? arXiv preprint", "journal": "", "year": "", "authors": "Bowen Zhang"}, {"ref_id": "b31", "title": "Maxamed Axmed, Kalika Bali, and Sunayana Sitaram", "journal": "", "year": "2023", "authors": "Kabir Ahuja; Harshita Diddee; Rishav Hada; Millicent Ochieng; Krithika Ramesh; Prachi Jain; Akshay Nambi; Tanuja Ganu; Sameer Segal"}, {"ref_id": "b32", "title": "Can we trust the evaluation on ChatGPT?", "journal": "", "year": "2023", "authors": "Rachith Aiyappa; Jisun An; Haewoon Kwak; Yong-Yeol Ahn"}, {"ref_id": "b33", "title": "RL4F: Generating natural language feedback with reinforcement learning for repairing model outputs", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Ekin Afra Feyza Akyurek; Ashwin Akyurek; Peter Kalyan;  Clark; Niket Derry Tanti Wijaya;  Tandon"}, {"ref_id": "b34", "title": "Will affective computing emerge from foundation models and general AI? a first evaluation on ChatGPT", "journal": "", "year": "2023", "authors": "M Mostafa; Erik Amin; Bj\u00f6rn W Cambria;  Schuller"}, {"ref_id": "b35", "title": "Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models", "journal": "", "year": "", "authors": "Chenxin An; Shansan Gong; Ming Zhong; Xingjian Zhao; Mukai Li"}, {"ref_id": "b36", "title": "Evaluating the performance of ChatGPT in ophthalmology: An analysis of its successes and shortcomings", "journal": "Ophthalmology Science", "year": "2023", "authors": "Samir Fares Antaki; Daniel Touma; Jonathan Milad; Renaud El-Khoury;  Duval"}, {"ref_id": "b37", "title": "Chrysoula Zerva, and Wilker Aziz. 2023. Uncertainty in natural language generation: From theory to applications", "journal": "", "year": "", "authors": "Joris Baan; Nico Daheim; Evgenia Ilia; Dennis Ulmer; Haau-Sing Li; Raquel Fern\u00e1ndez"}, {"ref_id": "b38", "title": "Longbench: A bilingual, multitask benchmark for long context understanding", "journal": "", "year": "2023", "authors": "Yushi Bai; Xin Lv; Jiajie Zhang; Hongchang Lyu; Jiankai Tang; Zhidian Huang; Zhengxiao Du; Xiao Liu; Aohan Zeng; Lei Hou; Yuxiao Dong; Jie Tang; Juanzi Li"}, {"ref_id": "b39", "title": "Benchmarking foundation models with language", "journal": "", "year": "2023", "authors": "Yushi Bai; Jiahao Ying; Yixin Cao; Xin Lv; Yuze He; Xiaozhi Wang; Jifan Yu; Kaisheng Zeng; Yijia Xiao; Haozhe Lyu; Jiayin Zhang; Juanzi Li; Lei Hou"}, {"ref_id": "b40", "title": "A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning", "journal": "", "year": "2023", "authors": "Yejin Bang; Samuel Cahyawijaya; Nayeon Lee; Wenliang Dai; Dan Su; Bryan Wilie; Holy Lovenia; Ziwei Ji; Tiezheng Yu; Willy Chung; Quyet V Do; Yan Xu; Pascale Fung"}, {"ref_id": "b41", "title": "ByGPT5: End-to-end style-conditioned poetry generation with token-free language models", "journal": "", "year": "2023", "authors": "Jonas Belouadi; Steffen Eger"}, {"ref_id": "b42", "title": "ChatGPT is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models", "journal": "", "year": "2023", "authors": "Ning Bian; Xianpei Han; Le Sun; Hongyu Lin; Yaojie Lu; Ben He"}, {"ref_id": "b43", "title": "Chat-GPT participates in a computer science exam", "journal": "", "year": "2023", "authors": "Sebastian Bordt; Ulrike Von Luxburg"}, {"ref_id": "b44", "title": "A categorical archive of ChatGPT failures", "journal": "", "year": "2023", "authors": "Ali Borji"}, {"ref_id": "b45", "title": "Detoxifying online discourse: A guided response generation approach for reducing toxicity in user-generated text", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Ritwik Bose; Ian Perera; Bonnie Dorr"}, {"ref_id": "b46", "title": "Sparks of artificial general intelligence: Early experiments with GPT-4", "journal": "", "year": "2023", "authors": "S\u00e9bastien Bubeck; Varun Chandrasekaran; Ronen Eldan; Johannes Gehrke; Eric Horvitz; Ece Kamar; Peter Lee; Yin Tat Lee; Yuanzhi Li; Scott Lundberg; Harsha Nori; Hamid Palangi; Marco Tulio Ribeiro; Yi Zhang"}, {"ref_id": "b47", "title": "Utilizing ChatGPT generated data to retrieve depression symptoms from social media", "journal": "", "year": "2023", "authors": "Ana-Maria Bucur"}, {"ref_id": "b48", "title": "and Ilias Chalkidis. 2023. Pokemonchat: Auditing ChatGPT for pok\u00e9mon universe knowledge", "journal": "", "year": "", "authors": "Laura Cabello; Jiaang Li"}, {"ref_id": "b49", "title": "", "journal": "", "year": "2023", "authors": "Alex Cabrera; Graham Neubig"}, {"ref_id": "b50", "title": "Does ChatGPT resemble humans in language use? arXiv preprint", "journal": "", "year": "2023", "authors": "G Zhenguang; David A Cai; Xufeng Haslett; Shuqi Duan; Martin J Wang;  Pickering"}, {"ref_id": "b51", "title": "Assessing cross-cultural alignment between ChatGPT and human societies: An empirical study", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Yong Cao; Li Zhou; Seolhwa Lee; Laura Cabello; Min Chen; Daniel Hershcovich"}, {"ref_id": "b52", "title": "Quantifying memorization across neural language models", "journal": "", "year": "2023", "authors": "Nicholas Carlini; Daphne Ippolito; Matthew Jagielski; Katherine Lee; Florian Tramer; Chiyuan Zhang"}, {"ref_id": "b53", "title": "Zero-shot approach to overcome perturbation sensitivity of prompts", "journal": "Long Papers", "year": "2023", "authors": "Mohna Chakraborty; Adithya Kulkarni; Qi Li"}, {"ref_id": "b54", "title": "Good data, large data, or no data? comparing three approaches in developing research aspect classifiers for biomedical papers", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Shreya Chandrasekhar; Chieh-Yang Huang; Ting-Hao Huang"}, {"ref_id": "b55", "title": "", "journal": "", "year": "", "authors": "Yupeng Chang; Xu Wang; Jindong Wang; Yuan Wu; Linyi Yang; Kaijie Zhu; Hao Chen; Xiaoyuan Yi; Cunxiang Wang; Yidong Wang; Wei Ye; Yue Zhang; Yi Chang; Philip S Yu; Qiang Yang; Xing Xie"}, {"ref_id": "b56", "title": "Skills-in-context prompting: Unlocking compositionality in large language models", "journal": "", "year": "2023", "authors": "Jiaao Chen; Xiaoman Pan; Dian Yu; Kaiqiang Song; Xiaoyang Wang; Dong Yu; Jianshu Chen"}, {"ref_id": "b57", "title": "How is ChatGPT's behavior changing over time?", "journal": "", "year": "2023", "authors": "Lingjiao Chen; Matei Zaharia; James Zou"}, {"ref_id": "b58", "title": "Large language models meet Harry Potter: A bilingual dataset for aligning dialogue agents with characters", "journal": "", "year": "2023", "authors": "Nuo Chen; Yan Wang; Haiyun Jiang; Deng Cai; Yuhan Li; Ziyang Chen; Longyue Wang; Jia Li"}, {"ref_id": "b59", "title": "Dinggang Shen, Tianming Liu, and Xiang Li. 2023a. AugGPT: Leveraging ChatGPT for text data augmentation", "journal": "", "year": "", "authors": "Haixing Dai; Zhengliang Liu; Wenxiong Liao; Xiaoke Huang; Yihan Cao; Zihao Wu; Lin Zhao; Shaochen Xu; Wei Liu; Ninghao Liu; Sheng Li; Dajiang Zhu; Hongmin Cai; Lichao Sun; Quanzheng Li"}, {"ref_id": "b60", "title": "Uncovering ChatGPT's capabilities in recommender systems", "journal": "", "year": "2023", "authors": "Sunhao Dai; Ninglu Shao; Haiyuan Zhao; Weijie Yu; Zihua Si; Chen Xu; Zhongxiang Sun; Xiao Zhang"}, {"ref_id": "b61", "title": "Evaluating ChatGPT's performance for multilingual and emoji-based hate speech detection", "journal": "", "year": "2023", "authors": "Mithun Das; Animesh Saurabh Kumar Pandey;  Mukherjee"}, {"ref_id": "b62", "title": "Benchmarks for automated commonsense reasoning: A survey", "journal": "", "year": "2023", "authors": "Ernest Davis"}, {"ref_id": "b63", "title": "Analyzing ChatGPT's aptitude in an introductory computer engineering course", "journal": "", "year": "2023", "authors": "Sanjay Deshpande; Jakub Szefer"}, {"ref_id": "b64", "title": "Mind meets machine: Unravelling GPT-4's cognitive psychology", "journal": "", "year": "2023", "authors": "Sifatkaur Dhingra; Manmeet Singh; S B Vaisakh; Neetiraj Malviya; Sukhpal Singh Gill"}, {"ref_id": "b65", "title": "Is GPT-3 a good data annotator?", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Bosheng Ding; Chengwei Qin; Linlin Liu; Yew Ken Chia; Boyang Li; Shafiq Joty; Lidong Bing"}, {"ref_id": "b66", "title": "You've got a friend in ... a language model? a comparison of explanations of multiple-choice items of reading comprehension between ChatGPT and humans", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "George Duenas; Sergio Jimenez; Geral Mateus Ferro"}, {"ref_id": "b67", "title": "GPT-3.5, GPT-4, or bard? evaluating LLMs reasoning ability in zero-shot setting and performance boosting through prompts", "journal": "", "year": "2023", "authors": "Jessica L\u00f3pez Espejel; Mahaman El Hassane Ettifouri;  Sanoussi Yahaya Alassan; Walid El Mehdi Chouham;  Dahhane"}, {"ref_id": "b68", "title": "Uncovering the potential of ChatGPT for discourse analysis in dialogue: An empirical study", "journal": "", "year": "2023", "authors": "Yaxin Fan; Feng Jiang"}, {"ref_id": "b69", "title": "Is ChatGPT a highly fluent grammatical error correction system?", "journal": "", "year": "2023", "authors": "Tao Fang; Shu Yang; Kaixin Lan; Derek F Wong; Jinpeng Hu; Lidia S Chao; Yue Zhang"}, {"ref_id": "b70", "title": "Leveraging large language models for automated dialogue analysis", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Sarah E Finch; Ellie S Paek; Jinho D Choi"}, {"ref_id": "b71", "title": "What makes a good counter-stereotype? Evaluating strategies for automated responses to stereotypical text", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Kathleen Fraser; Svetlana Kiritchenko; Isar Nejadgholi; Anna Kerkhof"}, {"ref_id": "b72", "title": "Philipp Christian Petersen, and Julius Berner. 2023. Mathematical capabilities of ChatGPT", "journal": "", "year": "", "authors": "Simon Frieder; Luca Pinchetti; Alexis Chevalier; Ryan-Rhys Griffiths; Tommaso Salvatori; Thomas Lukasiewicz"}, {"ref_id": "b73", "title": "Is ChatGPT a good causal reasoner?", "journal": "", "year": "2023", "authors": "Jinglong Gao; Xiao Ding; Bing Qin; Ting Liu"}, {"ref_id": "b74", "title": "Exploring the feasibility of ChatGPT for event extraction", "journal": "", "year": "2023", "authors": "Jun Gao; Huan Zhao; Changlong Yu; Ruifeng Xu"}, {"ref_id": "b75", "title": "Shiping Yang, and Xiaojun Wan. 2023c. Human-like summarization evaluation with ChatGPT", "journal": "", "year": "", "authors": "Mingqi Gao; Jie Ruan; Renliang Sun; Xunjian Yin"}, {"ref_id": "b76", "title": "Enabling large language models to generate text with citations", "journal": "", "year": "2023", "authors": "Tianyu Gao; Howard Yen; Jiatong Yu; Danqi Chen"}, {"ref_id": "b77", "title": "How to design translation prompts for ChatGPT: An empirical study", "journal": "", "year": "", "authors": "Yuan Gao; Ruili Wang; Feng Hou"}, {"ref_id": "b78", "title": "ChatGPT has aced the test of understanding in college economics: Now what?", "journal": "The American Economist", "year": "2023", "authors": "Wayne Geerling; G Dirk Mateer; Jadrian Wooten; Nikhil Damodaran"}, {"ref_id": "b79", "title": "SUT at SemEval-2023 task 1: Prompt generation for visual word sense disambiguation", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Omid Ghahroodi; Sahel Seyed Arshan Dalili; Ehsaneddin Mesforoush;  Asgari"}, {"ref_id": "b80", "title": "ChatGPT for suicide risk assessment on social media: Quantitative evaluation of model performance", "journal": "", "year": "2023", "authors": "Isar Hamideh Ghanadian; Hussein Al Nejadgholi;  Osman"}, {"ref_id": "b81", "title": "ChatGPT outperforms crowd-workers for textannotation tasks", "journal": "", "year": "2023", "authors": "Fabrizio Gilardi; Meysam Alizadeh; Ma\u00ebl Kubli"}, {"ref_id": "b82", "title": "How does ChatGPT perform on the United States Medical Licensing Examination? The implications of large language models for medical education and knowledge assessment", "journal": "JMIR Med Educ", "year": "2023", "authors": "Aidan Gilson; W Conrad; Thomas Safranek; Vimig Huang; Ling Socrates; Richard Andrew Chi; David Taylor;  Chartash"}, {"ref_id": "b83", "title": "Evaluation papers for ChatGPT", "journal": "", "year": "2023", "authors": " Github"}, {"ref_id": "b84", "title": "Time travel in LLMs: Tracing data contamination in large language models", "journal": "", "year": "2023", "authors": "Shahriar Golchin; Mihai Surdeanu"}, {"ref_id": "b85", "title": "Language-agnostic transformers and assessing ChatGPT-based query rewriting for multilingual document-grounded QA", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Srinivas Gowriraj; Mitali Soham Dinesh Tiwari; Srijan Potnis; Teruko Bansal; Eric Mitamura;  Nyberg"}, {"ref_id": "b86", "title": "Linguistically informed ChatGPT prompts to enhance japanese-chinese machine translation: A case study on attributive clauses", "journal": "", "year": "2023", "authors": "Wenshi Gu"}, {"ref_id": "b87", "title": "When truth matters -addressing pragmatic categories in natural language inference (NLI) by large language models (LLMs)", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Reto Gubelmann; Aikaterini-Lida Kalouli; Christina Niklaus; Siegfried Handschuh"}, {"ref_id": "b88", "title": "", "journal": "", "year": "", "authors": "Biyang Guo; Xin Zhang; Ziyuan Wang; Minqi Jiang; Jinran Nie; Yuxuan Ding; Jianwei Yue"}, {"ref_id": "b89", "title": "2020. Don't stop pretraining: Adapt language models to domains and tasks", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": "Ana Suchin Gururangan; Swabha Marasovi\u0107; Kyle Swayamdipta; Iz Lo; Doug Beltagy; Noah A Downey;  Smith"}, {"ref_id": "b90", "title": "Generating faithful text from a knowledge graph with noisy reference text", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Tahsina Hashem; Weiqing Wang; Mohammed Eunus Derry Tanti Wijaya; Yuan-Fang Ali;  Li"}, {"ref_id": "b91", "title": "Multilingual language models are not multicultural: A case study in emotion", "journal": "", "year": "2023", "authors": "Shreya Havaldar; Bhumika Singhal; Sunny Rai; Langchen Liu; Sharath Chandra Guntuku; Lyle Ungar"}, {"ref_id": "b92", "title": "ICL-D3IE: In-context learning with diverse demonstrations updating for document information extraction", "journal": "", "year": "2023", "authors": "Jiabang He; Lei Wang; Yi Hu; Ning Liu; Hui Liu; Xing Xu; Heng Tao Shen"}, {"ref_id": "b93", "title": "", "journal": "", "year": "2023", "authors": "Qianyu He; Jie Zeng; Wenhao Huang; Lina Chen; Jin Xiao; Qianxi He; Xunzhe Zhou; Lida Chen; Xintao Wang; Yuncheng Huang; Haoning Ye; Zihan Li; Shisong Chen; Yikai Zhang; Zhouhong Gu; Jiaqing Liang; Yanghua Xiao"}, {"ref_id": "b94", "title": "MGTBench: Benchmarking machine-generated text detection", "journal": "", "year": "2023", "authors": "Xinlei He; Xinyue Shen; Zeyuan Chen; Michael Backes; Yang Zhang"}, {"ref_id": "b95", "title": "ChatGPT for zero-shot dialogue state tracking: A solution or an opportunity?", "journal": "Short Papers", "year": "2023", "authors": "Michael Heck; Nurul Lubis; Benjamin Ruppik; Renato Vukovic; Shutong Feng; Christian Geishauser"}, {"ref_id": "b96", "title": "How good are GPT models at machine translation?", "journal": "", "year": "", "authors": ""}, {"ref_id": "b97", "title": "Diagnostic accuracy of differential-diagnosis lists generated by generative pretrained transformer 3 chatbot for clinical vignettes with common chief complaints: A pilot study", "journal": "International Journal of Environmental Research and Public Health", "year": "2023", "authors": "Takanobu Hirosawa; Yukinori Harada; Masashi Yokose; Tetsu Sakamoto; Ren Kawamura; Taro Shimizu"}, {"ref_id": "b98", "title": "Does ChatGPT have theory of mind?", "journal": "", "year": "2023", "authors": "Bart Holterman;  Kees Van Deemter"}, {"ref_id": "b99", "title": "Generative models as a complex systems science: How can we make sense of large language model behavior?", "journal": "", "year": "2023", "authors": "Ari Holtzman; Peter West; Luke Zettlemoyer"}, {"ref_id": "b100", "title": "Faithful question answering with Monte-Carlo planning", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Ruixin Hong; Hongming Zhang; Hong Zhao; Dong Yu; Changshui Zhang"}, {"ref_id": "b101", "title": "Chain-of-symbol prompting elicits planning in large langauge models", "journal": "", "year": "2023", "authors": "Hanxu Hu; Hongyuan Lu; Huajian Zhang; Yun-Ze Song; Wai Lam; Yue Zhang"}, {"ref_id": "b102", "title": "An empirical study of pre-trained language models in simple knowledge graph question answering", "journal": "", "year": "2023", "authors": "Nan Hu; Yike Wu; Guilin Qi; Dehai Min; Jiaoyan Chen; Jeff Z Pan; Zafar Ali"}, {"ref_id": "b103", "title": "", "journal": "", "year": "", "authors": "Yan Hu; Iqra Ameer; Xu Zuo; Xueqing Peng; Yujia Zhou; Zehan Li; Yiming Li; Jianfu Li; Xiaoqian Jiang"}, {"ref_id": "b104", "title": "Is ChatGPT better than human annotators? potential and limitations of ChatGPT in explaining implicit hate speech", "journal": "", "year": "2023", "authors": "Fan Huang; Haewoon Kwak; Jisun An"}, {"ref_id": "b105", "title": "Not all languages are created equal in LLMs: Improving multilingual capability by crosslingual-thought prompting", "journal": "", "year": "2023", "authors": "Haoyang Huang; Tianyi Tang; Dongdong Zhang; Wayne Xin Zhao; Ting Song; Yan Xia; Furu Wei"}, {"ref_id": "b106", "title": "Examining bias in opinion summarisation through the perspective of opinion diversity", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Nannan Huang; Lin Tian; Haytham Fayek; Xiuzhen Zhang"}, {"ref_id": "b107", "title": "Look before you leap: An exploratory study of uncertainty measurement for large language models", "journal": "", "year": "2023", "authors": "Yuheng Huang; Jiayang Song; Zhijie Wang; Shengming Zhao; Huaming Chen; Felix Juefei-Xu; Lei Ma"}, {"ref_id": "b108", "title": "Maosong Sun, and Junxian He. 2023e. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models", "journal": "", "year": "", "authors": "Yuzhen Huang; Yuzhuo Bai; Zhihao Zhu; Junlei Zhang; Jinghan Zhang; Tangjun Su; Junteng Liu; Chuancheng Lv; Yikai Zhang; Jiayi Lei; Yao Fu"}, {"ref_id": "b109", "title": "Preventing generation of verbatim memorization in language models gives a false sense of privacy", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Daphne Ippolito; Florian Tramer; Milad Nasr; Chiyuan Zhang; Matthew Jagielski; Katherine Lee; Christopher Choquette Choo; Nicholas Carlini"}, {"ref_id": "b110", "title": "Evaluation of ChatGPT on biomedical tasks: A zero-shot comparison with finetuned generative transformers", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Israt Jahan;  Md Tahmid Rahman; Chun Laskar; Jimmy Peng;  Huang"}, {"ref_id": "b111", "title": "Consistency analysis of ChatGPT", "journal": "", "year": "2023", "authors": "Myeongjun Jang; Thomas Lukasiewicz"}, {"ref_id": "b112", "title": "ChatGPT is fun, but it is not funny! humor is still challenging large language models", "journal": "", "year": "2023", "authors": "Sophie Jentzsch; Kristian Kersting"}, {"ref_id": "b113", "title": "Struct-GPT: A general framework for large language model to reason over structured data", "journal": "", "year": "2023", "authors": "Jinhao Jiang; Kun Zhou; Zican Dong; Keming Ye; Wayne Xin Zhao; Ji-Rong Wen"}, {"ref_id": "b114", "title": "Is ChatGPT a good translator?", "journal": "", "year": "2023", "authors": "Wenxiang Jiao; Wenxuan Wang; Jen-Tse Huang; Xing Wang; Zhaopeng Tu"}, {"ref_id": "b115", "title": "Gpt is becoming a turing machine: Here are some ways to program it", "journal": "", "year": "2023", "authors": "Ana Jojic; Zhen Wang; Nebojsa Jojic"}, {"ref_id": "b116", "title": "Large language models effectively leverage document-level context for literary translation", "journal": "", "year": "2023", "authors": "Marzena Karpinska; Mohit Iyyer"}, {"ref_id": "b117", "title": "Zeroshot information extraction for clinical meta-analysis using large language models", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "David Kartchner; Selvi Ramalingam; Irfan Al-Hussaini; Olivia Kronick; Cassie Mitchell"}, {"ref_id": "b118", "title": "Evaluating GPT-4 and ChatGPT on japanese medical licensing examinations", "journal": "", "year": "2023", "authors": "Jungo Kasai; Yuhei Kasai; Keisuke Sakaguchi; Yutaro Yamada; Dragomir Radev"}, {"ref_id": "b119", "title": "Can ChatGPT understand causal language in science claims?", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Yuheun Kim; Lu Guo; Bei Yu; Yingya Li"}, {"ref_id": "b120", "title": "Large language models are state-of-the-art evaluators of translation quality", "journal": "", "year": "2023", "authors": "Tom Kocmi; Christian Federmann"}, {"ref_id": "b121", "title": "Stanis\u0142aw Wo\u017aniak, and Przemys\u0142aw Kazienko. 2023. ChatGPT: Jack of all trades", "journal": "", "year": "", "authors": "Jan Koco\u0144; Igor Cichecki; Oliwier Kaszyca; Mateusz Kochanek; Dominika Szyd\u0142o; Joanna Baran; Julita Bielaniewicz; Marcin Gruza; Arkadiusz Janz; Kamil Kanclerz; Anna Koco\u0144; Bart\u0142omiej Koptyra; Wiktoria Mieleszczenko-Kowszewicz; Piotr Mi\u0142kowski; Marcin Oleksy; Maciej Piasecki; \u0141ukasz Radli\u0144ski; Konrad Wojtasik"}, {"ref_id": "b122", "title": "Bad: Bias detection for large language models in the context of candidate screening", "journal": "", "year": "2023", "authors": "Nam Ho Koh; Joseph Plata; Joyce Chai"}, {"ref_id": "b123", "title": "Humans in humans out: On GPT converging toward common sense in both success and failure", "journal": "", "year": "2023", "authors": "Philipp Koralus; Vincent Wang-Ma\u015bcianica"}, {"ref_id": "b124", "title": "Could an artificial-intelligence agent pass an introductory physics course?", "journal": "", "year": "2023", "authors": "Gerd Kortemeyer"}, {"ref_id": "b125", "title": "Theory of mind might have spontaneously emerged in large language models", "journal": "", "year": "2023", "authors": "Michal Kosinski"}, {"ref_id": "b126", "title": "Language generation models can cause harm: So what can we do about it? an actionable survey", "journal": "", "year": "2023", "authors": "Sachin Kumar; Vidhisha Balachandran; Lucille Njoo; Antonios Anastasopoulos; Yulia Tsvetkov"}, {"ref_id": "b127", "title": "Performance of ChatGPT on usmle: Potential for AI-assisted medical education using large language models", "journal": "PLOS Digital Health", "year": "2023", "authors": "Tiffany H Kung; Morgan Cheatham; Arielle Medenilla; Czarina Sillos; Lorie De Leon; Camille Elepa\u00f1o; Maria Madriaga; Rimel Aggabao; Giezel Diaz-Candido; James Maningo; Victor Tseng"}, {"ref_id": "b128", "title": "Large language models on the chessboard: A study on ChatGPT's formal language comprehension and complex reasoning skills", "journal": "", "year": "2023", "authors": "Mu-Tien Kuo; Chih-Chung Hsueh; Richard Tzong-Han Tsai"}, {"ref_id": "b129", "title": "Causal reasoning and large language models: Opening a new frontier for causality", "journal": "", "year": "2023", "authors": "Emre K\u0131c\u0131man; Robert Ness; Amit Sharma; Chenhao Tan"}, {"ref_id": "b130", "title": "ChatGPT beyond english: Towards a comprehensive evaluation of large language models in multilingual learning", "journal": "", "year": "2023", "authors": "Nghia Trung Viet Dac Lai; Amir Pouran Ben Ngo; Hieu Veyseh; Franck Man; Trung Dernoncourt; Thien Huu Bui;  Nguyen"}, {"ref_id": "b131", "title": "Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Xiangji Huang. 2023. A systematic study and comprehensive evaluation of ChatGPT on benchmark datasets", "journal": "", "year": "", "authors": " Md Tahmid Rahman Laskar; Mizanur Bari;  Rahman"}, {"ref_id": "b132", "title": "ChatGPT: A meta-analysis after 2.5 months", "journal": "", "year": "2023", "authors": "Christoph Leiter; Ran Zhang; Yanran Chen; Jonas Belouadi; Daniil Larionov; Vivian Fresen; Steffen Eger"}, {"ref_id": "b133", "title": "Bhasa: A holistic southeast asian linguistic and cultural evaluation suite for large language models", "journal": "", "year": "2023", "authors": "Jian Wei Qi Leong; Yosephine Gang Ngui; Hamsawardhini Susanto; Kengatharaiyer Rengarajan; William Chandra Sarveswaran;  Tjhi"}, {"ref_id": "b134", "title": "Evaluating ChatGPT's information extraction capabilities: An assessment of performance, explainability, calibration", "journal": "", "year": "2023", "authors": "Bo Li; Gexiang Fang; Yang Yang; Quansen Wang; Wei Ye; Wen Zhao; Shikun Zhang"}, {"ref_id": "b135", "title": "Pingyu Wu, and Haozhen Sun. 2023b. ChatHaruhi: Reviving anime character in reality via large language model", "journal": "", "year": "", "authors": "Cheng Li; Ziang Leng; Chenxi Yan; Junyi Shen; Hao Wang; M I Weishi; Yaying Fei; Xiaoyang Feng; Song Yan; Haosheng Wang; Linkang Zhan; Yaokai Jia"}, {"ref_id": "b136", "title": "Can LLM already serve as a database interface?", "journal": "", "year": "2023", "authors": "Jinyang Li; Binyuan Hui; Ge Qu; Binhua Li; Jiaxi Yang; Bowen Li; Bailin Wang; Rongyu Bowen Qin; Ruiying Cao; Nan Geng; Xuanhe Huo; Chenhao Zhou; Guoliang Ma;  Li; C C Kevin; Fei Chang; Reynold Huang; Yongbin Cheng;  Li"}, {"ref_id": "b137", "title": "HaluEval: A largescale hallucination evaluation benchmark for large language models", "journal": "", "year": "2023", "authors": "Junyi Li; Xiaoxue Cheng; Wayne Xin Zhao; Jian-Yun Nie; Ji-Rong Wen"}, {"ref_id": "b138", "title": "hot\" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media", "journal": "", "year": "2023", "authors": "Lingyao Li; Lizhou Fan; Shubham Atreja; Libby Hemphill"}, {"ref_id": "b139", "title": "Soujanya Poria, and Lidong Bing. 2023f. Chain-of-knowledge: Grounding large language models via dynamic knowledge adapting over heterogeneous sources", "journal": "", "year": "", "authors": "Xingxuan Li; Ruochen Zhao; Yew Ken Chia; Bosheng Ding; Shafiq Joty"}, {"ref_id": "b140", "title": "Pengcheng He, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2023g. Guiding large language models via directional stimulus prompting", "journal": "", "year": "", "authors": "Zekun Li; Baolin Peng"}, {"ref_id": "b141", "title": "Holistic evaluation of language models", "journal": "Transactions on Machine Learning Research. Featured Certification", "year": "2023", "authors": "Percy Liang; Rishi Bommasani; Tony Lee; Dimitris Tsipras; Dilara Soylu; Michihiro Yasunaga; Yian Zhang; Deepak Narayanan; Yuhuai Wu; Ananya Kumar; Benjamin Newman; Binhang Yuan; Bobby Yan; Ce Zhang; Christian Alexander Cosgrove; Christopher D Manning; Christopher Re; Diana Acosta-Navas; Drew Arad Hudson; Eric Zelikman; Esin Durmus; Faisal Ladhak; Frieda Rong; Hongyu Ren; Huaxiu Yao; Wang Jue; Keshav Santhanam; Laurel Orr; Lucia Zheng; Mert Yuksekgonul; Mirac Suzgun; Nathan Kim; Neel Guha; Niladri S Chatterji; Omar Khattab; Peter Henderson; Qian Huang"}, {"ref_id": "b142", "title": "Breaking the bank with ChatGPT: Few-shot text classification for finance", "journal": "", "year": "2023", "authors": "Yancheng Liang; Jiajie Zhang; Hui Li; Xiaochen Liu; Yi Hu; Yong Wu; Jiaoyao Zhang; Yongyan Liu; Yi Wu"}, {"ref_id": "b143", "title": "A comprehensive evaluation of ChatGPT's zero-shot text-to-sql capability", "journal": "", "year": "2023", "authors": "Aiwei Liu; Xuming Hu; Lijie Wen; Philip S Yu"}, {"ref_id": "b144", "title": "We're afraid language models aren't modeling ambiguity", "journal": "", "year": "2023", "authors": "Alisa Liu; Zhaofeng Wu; Julian Michael; Alane Suhr; Peter West; Alexander Koller; Swabha Swayamdipta; Noah A Smith; Yejin Choi"}, {"ref_id": "b145", "title": "Evaluating large language models on graphs: Performance insights and comparative analysis", "journal": "", "year": "2023", "authors": "Chang Liu; Bo Wu"}, {"ref_id": "b146", "title": "Evaluating the logical reasoning ability of ChatGPT and GPT-4", "journal": "", "year": "2023", "authors": "Hanmeng Liu; Ruoxi Ning; Zhiyang Teng; Jian Liu; Qiji Zhou; Yue Zhang"}, {"ref_id": "b147", "title": "Logicot: Logical chain-of-thought instruction-tuning data collection with GPT-4", "journal": "", "year": "2023", "authors": "Hanmeng Liu; Zhiyang Teng; Leyang Cui; Chaoli Zhang; Qiji Zhou; Yue Zhang"}, {"ref_id": "b148", "title": "Is your code generated by ChatGPT really correct? rigorous evaluation of large language models for code generation", "journal": "", "year": "2023", "authors": "Jiawei Liu; Chunqiu Steven Xia; Yuyao Wang; Lingming Zhang"}, {"ref_id": "b149", "title": "Not the end of story: An evaluation of ChatGPT-driven vulnerability description mappings", "journal": "", "year": "2023", "authors": "Xin Liu; Yuan Tan; Zhenghang Xiao; Jianwei Zhuge; Rui Zhou"}, {"ref_id": "b150", "title": "Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023g. G-Eval: NLG evaluation using GPT-4 with better human alignment", "journal": "", "year": "", "authors": "Yang Liu; Dan Iter; Yichong Xu"}, {"ref_id": "b151", "title": "Dingang Shen, Tianming Liu, and Bao Ge. 2023h. Summary of ChatGPT-related research and perspective towards the future of large language models", "journal": "", "year": "", "authors": "Yiheng Liu; Tianle Han; Siyuan Ma; Jiayue Zhang; Yuanyuan Yang; Jiaming Tian; Hao He; Antong Li; Mengshen He; Zhengliang Liu; Zihao Wu; Lin Zhao; Dajiang Zhu; Xiang Li; Ning Qiang"}, {"ref_id": "b152", "title": "MolXPT: Wrapping molecules with text for generative pre-training", "journal": "Short Papers", "year": "2023", "authors": "Zequn Liu; Wei Zhang; Yingce Xia; Lijun Wu; Shufang Xie; Tao Qin; Ming Zhang; Tie-Yan Liu"}, {"ref_id": "b153", "title": "Dajiang Zhu, and Xiang Li. 2023j. Deid-GPT: Zero-shot medical", "journal": "", "year": "", "authors": "Zhengliang Liu; Xiaowei Yu; Lu Zhang; Zihao Wu; Chao Cao; Haixing Dai; Lin Zhao; Wei Liu; Dinggang Shen; Quanzheng Li; Tianming Liu"}, {"ref_id": "b154", "title": "Predicting the quality of revisions in argumentative writing", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Zhexiong Liu; Diane Litman; Elaine Wang; Lindsay Matsumura; Richard Correnti"}, {"ref_id": "b155", "title": "Exploring effectiveness of GPT-3 in grammatical error correction: A study on performance and controllability in prompt-based methods", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Mengsay Loem; Masahiro Kaneko; Sho Takase; Naoaki Okazaki"}, {"ref_id": "b156", "title": "Hybrid long document summarization using c2f-far and ChatGPT: A practical study", "journal": "", "year": "2023", "authors": "Guang Lu; Sylvia B Larcher; Tu Tran"}, {"ref_id": "b157", "title": "Toward human-like evaluation for natural language generation with error analysis", "journal": "Long Papers", "year": "2023", "authors": "Qingyu Lu; Liang Ding; Liping Xie; Kanjian Zhang; Derek F Wong; Dacheng Tao"}, {"ref_id": "b158", "title": "Error analysis prompting enables human-like translation evaluation in large language models: A case study on ChatGPT", "journal": "", "year": "2023", "authors": "Qingyu Lu; Baopu Qiu; Liang Ding; Kanjian Zhang; Tom Kocmi; Dacheng Tao"}, {"ref_id": "b159", "title": "HIT-SCIR at WASSA 2023: Empathy and emotion analysis at the utterance-level and the essay-level", "journal": "", "year": "2023", "authors": "Xin Lu; Zhuojun Li; Yanpeng Tong; Yanyan Zhao; Bing Qin"}, {"ref_id": "b160", "title": "ChatGPT as a factual inconsistency evaluator for text summarization", "journal": "", "year": "2023", "authors": "Zheheng Luo; Qianqian Xie; Sophia Ananiadou"}, {"ref_id": "b161", "title": "LLM-Rec: Personalized recommendation via prompting large language models", "journal": "", "year": "2023", "authors": "Hanjia Lyu; Song Jiang; Hanqing Zeng; Qifan Wang; Si Zhang; Ren Chen; Chris Leung; Jiajie Tang; Yinglong Xia; Jiebo Luo"}, {"ref_id": "b162", "title": "Translating radiology reports into plain language using ChatGPT and GPT-4 with prompt learning: Promising results, limitations", "journal": "", "year": "2023", "authors": "Qing Lyu; Josh Tan; Michael E Zapadka; Janardhana Ponnatapura; Chuang Niu; Kyle J Myers; Ge Wang; Christopher T Whitlow"}, {"ref_id": "b163", "title": "Chat2vis: Generating data visualisations via natural language using ChatGPT, codex and GPT-3 large language models", "journal": "", "year": "2023", "authors": "Paula Maddigan; Teo Susnjak"}, {"ref_id": "b164", "title": "SelfCheckGPT: Zero-resource black-box hallucination detection for generative large language models", "journal": "", "year": "2023", "authors": "Potsawee Manakul; Adian Liusie; Mark J F Gales"}, {"ref_id": "b165", "title": "Gpteval: A survey on assessments of ChatGPT and GPT-4", "journal": "", "year": "2023", "authors": "Rui Mao; Guanyi Chen; Xulang Zhang; Frank Guerin; Erik Cambria"}, {"ref_id": "b166", "title": "ChatGPT as a medical doctor? a diagnostic accuracy study on common and rare diseases", "journal": "", "year": "2023", "authors": "Lars Mehnen; Stefanie Gruarin; Mina Vasileva; Bernhard Knapp"}, {"ref_id": "b167", "title": "Uzh_clyp at semeval-2023 task 9: Head-first fine-tuning and ChatGPT data generation for cross-lingual learning in tweet intimacy prediction", "journal": "", "year": "2023", "authors": "Andrianos Michail; Stefanos Konstantinou; Simon Clematide"}, {"ref_id": "b168", "title": "Boosting theory-of-mind performance in large language models via prompting", "journal": "", "year": "2023", "authors": "Rahimi Shima; Christopher J Moghaddam;  Honey"}, {"ref_id": "b169", "title": "Debiasing should be good and bad: Measuring the consistency of debiasing techniques in language models", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Robert Morabito; Jad Kabbara; Ali Emami"}, {"ref_id": "b170", "title": "MUX-PLMs: Pretraining language models with data multiplexing", "journal": "", "year": "2023", "authors": "Vishvak Murahari; Ameet Deshpande; Carlos Jimenez; Izhak Shafran; Mingqiu Wang; Yuan Cao; Karthik Narasimhan"}, {"ref_id": "b171", "title": "Frontier review of multimodal AI", "journal": "Chinese Information Processing Society of China", "year": "2023", "authors": "Duan Nan"}, {"ref_id": "b172", "title": "Generative pretrained transformers for emotion detection in a code-switching setting", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Andrew Nedilko"}, {"ref_id": "b173", "title": "Capabilities of GPT-4 on medical challenge problems", "journal": "", "year": "2023", "authors": "Harsha Nori; Nicholas King; Scott Mayer Mckinney; Dean Carignan; Eric Horvitz"}, {"ref_id": "b174", "title": "Evaluation of question generation needs more references", "journal": "", "year": "2023", "authors": "Shinhyeok Oh; Hyojun Go; Hyeongdon Moon; Yunsung Lee; Myeongho Jeong; Hyun Seung Lee; Seungtaek Choi"}, {"ref_id": "b175", "title": "ChatGPT versus traditional question answering for knowledge graphs: Current status and future directions towards knowledge graph chatbots", "journal": "", "year": "2023", "authors": "Reham Omar; Omij Mangukiya; Panos Kalnis; Essam Mansour"}, {"ref_id": "b176", "title": "Empowering conversational agents using semantic in-context learning", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Amin Omidvar; Aijun An"}, {"ref_id": "b177", "title": "OpenAI. 2023. GPT-4 technical report", "journal": "", "year": "", "authors": ""}, {"ref_id": "b178", "title": "Juan Carlos Armenteros, and Adri\u00e1n Alonso. 2023. Linguistic ambiguity analysis in ChatGPT", "journal": "", "year": "", "authors": "Miguel Ortega-Mart\u00edn; \u00d3scar Garc\u00eda-Sierra; Alfonso Ardoiz; Jorge \u00c1lvarez"}, {"ref_id": "b179", "title": "Chat-GPT vs. crowdsourcing vs. experts: Annotating opendomain conversations with speech functions", "journal": "", "year": "2023", "authors": "Lidiia Ostyakova; Veronika Smilga; Kseniia Petukhova; Maria Molchanova; Daniel Kornev"}, {"ref_id": "b180", "title": "On the underspecification of situations in open-domain conversational datasets", "journal": "", "year": "2023", "authors": "Naoki Otani; Jun Araki; Hyeongsik Kim; Eduard Hovy"}, {"ref_id": "b181", "title": "Wanxiang Che, and Libo Qin. 2023. A preliminary evaluation of ChatGPT for zero-shot dialogue understanding", "journal": "", "year": "", "authors": "Wenbo Pan; Qiguang Chen; Xiao Xu"}, {"ref_id": "b182", "title": "Using Chat-GPT for entity matching", "journal": "", "year": "2023", "authors": "Ralph Peeters; Christian Bizer"}, {"ref_id": "b183", "title": "To ChatGPT, or not to ChatGPT: That is the question!", "journal": "", "year": "2023", "authors": "Alessandro Pegoraro; Kavita Kumari; Hossein Fereidooni; Ahmad-Reza Sadeghi"}, {"ref_id": "b184", "title": "Check your facts and try again: Improving large language models with external knowledge and automated feedback", "journal": "", "year": "2023", "authors": "Baolin Peng; Michel Galley; Pengcheng He; Hao Cheng; Yujia Xie; Yu Hu; Qiuyuan Huang; Lars Liden; Zhou Yu; Weizhu Chen; Jianfeng Gao"}, {"ref_id": "b185", "title": "Pengcheng He, Michel Galley, and Jianfeng Gao. 2023b. Instruction tuning with GPT-4", "journal": "", "year": "", "authors": "Baolin Peng; Chunyuan Li"}, {"ref_id": "b186", "title": "Yuanxin Ouyang, and Dacheng Tao. 2023c. Towards making the most of ChatGPT for machine translation", "journal": "", "year": "", "authors": "Keqin Peng; Liang Ding; Qihuang Zhong; Li Shen; Xuebo Liu; Min Zhang"}, {"ref_id": "b187", "title": "Credible without credit: Domain experts assess generative language models", "journal": "Short Papers", "year": "2023", "authors": "Denis Peskoff; Brandon Stewart"}, {"ref_id": "b188", "title": "Large language models sensitivity to the order of options in multiple-choice questions", "journal": "", "year": "2023", "authors": "Pouya Pezeshkpour; Estevam Hruschka"}, {"ref_id": "b189", "title": "GPoeT: a language model trained for rhyme generation on synthetic data", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Andrei Popescu-Belis; \u00c0lex R Atrio; Bastien Bernath; Etienne Boisson; Teo Ferrari; Xavier Theimerlienhard; Giorgos Vernikos"}, {"ref_id": "b190", "title": "ChatGPT vs human-authored text: Insights into controllable text summarization and sentence style transfer", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Dongqi Pu; Vera Demberg"}, {"ref_id": "b191", "title": "Is ChatGPT a general-purpose natural language processing task solver?", "journal": "", "year": "2023", "authors": "Chengwei Qin; Aston Zhang; Zhuosheng Zhang; Jiaao Chen; Michihiro Yasunaga; Diyi Yang"}, {"ref_id": "b192", "title": "Beyond black box AI generated plagiarism detection: From sentence to document level", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Ali Quidwai; Chunhui Li; Parijat Dube"}, {"ref_id": "b193", "title": "Reliable natural language understanding with large language models and answer set programming", "journal": "", "year": "2023", "authors": "Abhiramon Rajasekharan; Yankai Zeng; Parth Padalkar; Gopal Gupta"}, {"ref_id": "b194", "title": "ChatGPTcrawler: Find out if ChatGPT really knows what it's talking about", "journal": "", "year": "2023", "authors": "Aman Rangapur; Haoran Wang"}, {"ref_id": "b195", "title": "Assessing the utility of ChatGPT throughout the entire clinical workflow", "journal": "", "year": "2023", "authors": "Arya Rao; Michael Pang; John Kim; Meghana Kamineni; Winston Lie; Anoop K Prasad; Adam Landman; J Keith; Marc D Dreyer;  Succi"}, {"ref_id": "b196", "title": "Can ChatGPT Assess Human Personalities? A General Evaluation Framework", "journal": "", "year": "2023", "authors": "Haocong Rao; Cyril Leung; Chunyan Miao"}, {"ref_id": "b197", "title": "Unsupervised summarization re-ranking", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Shafiq Mathieu Ravaut; Nancy Joty;  Chen"}, {"ref_id": "b198", "title": "A system for answering simple questions in multiple languages", "journal": "", "year": "2023", "authors": "Anton Razzhigaev; Mikhail Salnikov; Valentin Malykh; Pavel Braslavski; Alexander Panchenko"}, {"ref_id": "b199", "title": "Investigating the factual knowledge boundary of large language models with retrieval augmentation", "journal": "", "year": "2023", "authors": "Ruiyang Ren; Yuhao Wang; Yingqi Qu; Wayne Xin Zhao; Jing Liu; Hao Tian; Hua Wu; Ji-Rong Wen; Haifeng Wang"}, {"ref_id": "b200", "title": "Exploring new frontiers in agricultural nlp: Investigating the potential of large language models for food applications", "journal": "", "year": "2023", "authors": "Zhengliang Saed Rezayi; Zihao Liu; Chandra Wu; Bao Dhakal; Haixing Ge; Gengchen Dai; Ninghao Mai; Chen Liu; Tianming Zhen; Sheng Liu;  Li"}, {"ref_id": "b201", "title": "ChatGPT MT: Competitive for high-(but not low-) resource languages", "journal": "", "year": "2023", "authors": "Nathaniel R Robinson; Perez Ogayo; David R Mortensen; Graham Neubig"}, {"ref_id": "b202", "title": "Ina Dormuth, and Markus Pauly. 2023. The selfperception and political biases of ChatGPT", "journal": "", "year": "", "authors": "J\u00e9r\u00f4me Rutinowski; Sven Franke"}, {"ref_id": "b203", "title": "", "journal": "", "year": "", "authors": "Oscar Sainz; Jon Ander Campos; Iker Garc\u00eda-Ferrero; Julen Etxaniz"}, {"ref_id": "b204", "title": "Neural theory-of-mind? on the limits of social intelligence in large lms", "journal": "", "year": "2023", "authors": "Maarten Sap; Ronan Lebras; Daniel Fried; Yejin Choi"}, {"ref_id": "b205", "title": "Nutcracking sledgehammers: Prioritizing target language data over bigger language models for cross-lingual metaphor detection", "journal": "", "year": "2023", "authors": "Jakob Schuster; Katja Markert"}, {"ref_id": "b206", "title": "An independent evaluation of ChatGPT on mathematical word problems (mwp)", "journal": "", "year": "2023", "authors": "Paulo Shakarian; Abhinav Koyyalamudi; Noel Ngu; Lakshmivihari Mareedu"}, {"ref_id": "b207", "title": "How well do large language models perform on faux pas tests?", "journal": "", "year": "2023", "authors": "Natalie Shapira; Guy Zwirn; Yoav Goldberg"}, {"ref_id": "b208", "title": "ChatGPT we trust? measuring and characterizing the reliability of ChatGPT", "journal": "", "year": "2023", "authors": "Xinyue Shen; Zeyuan Chen; Michael Backes; Yang Zhang"}, {"ref_id": "b209", "title": "Chatgraph: Interpretable text classification by converting ChatGPT knowledge to graphs", "journal": "", "year": "2023", "authors": "Yucheng Shi; Hehuan Ma; Wenliang Zhong; Qiaoyu Tan; Gengchen Mai; Xiang Li; Tianming Liu; Junzhou Huang"}, {"ref_id": "b210", "title": "Don't stop pretraining? make prompt-based fine-tuning powerful learner", "journal": "", "year": "2023", "authors": "Zhengxaing Shi; Aldo Lipani"}, {"ref_id": "b211", "title": "An analysis of the automatic bug fixing performance of ChatGPT", "journal": "", "year": "2023", "authors": "Dominik Sobania; Martin Briesch; Carol Hanna; Justyna Petke"}, {"ref_id": "b212", "title": "Huafeng Liu, and Liping Jing. 2023. Is ChatGPT a good keyphrase generator? a preliminary study", "journal": "", "year": "", "authors": "Mingyang Song; Haiyun Jiang; Shuming Shi; Songfang Yao; Shilong Lu; Yi Feng"}, {"ref_id": "b213", "title": "Comparing abstractive summaries generated by ChatGPT to real summaries through blinded reviewers and text classification algorithms", "journal": "", "year": "2023", "authors": "Mayank Soni; Vincent Wade"}, {"ref_id": "b214", "title": "ChatGPT is not a good indigenous translator", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "David Stap; Ali Araabi"}, {"ref_id": "b215", "title": "Yeyun Gong, Lionel M. Ni, Heung-Yeung Shum, and Jian Guo. 2023a. Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph", "journal": "", "year": "", "authors": "Jiashuo Sun; Chengjin Xu; Lumingyuan Tang; Saizhuo Wang; Chen Lin"}, {"ref_id": "b216", "title": "Is ChatGPT good at search?", "journal": "", "year": "2023", "authors": "Weiwei Sun; Lingyong Yan; Xinyu Ma; Pengjie Ren; Dawei Yin; Zhaochun Ren"}, {"ref_id": "b217", "title": "Assessing the ability of ChatGPT to screen articles for systematic reviews", "journal": "", "year": "2023", "authors": "Eugene Syriani; Istvan David; Gauransh Kumar"}, {"ref_id": "b218", "title": "Towards benchmarking and improving the temporal reasoning capability of large language models", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Qingyu Tan; Lidong Hwee Tou Ng;  Bing"}, {"ref_id": "b219", "title": "Can ChatGPT replace traditional kbqa models? an in-depth analysis of the question answering performance of the GPT LLM family", "journal": "", "year": "2023", "authors": "Yiming Tan; Dehai Min; Yu Li; Wenbo Li; Nan Hu; Yongrui Chen; Guilin Qi"}, {"ref_id": "b220", "title": "DAMO-NLP at SemEval-2023 task 2: A unified retrievalaugmented system for multilingual named entity recognition", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Zeqi Tan; Shen Huang; Zixia Jia; Jiong Cai; Yinghui Li; Weiming Lu; Yueting Zhuang; Kewei Tu; Pengjun Xie; Fei Huang; Yong Jiang"}, {"ref_id": "b221", "title": "Understanding factual errors in summarization: Errors, summarizers, datasets, error detectors", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Liyan Tang; Tanya Goyal; Alex Fabbri; Philippe Laban; Jiacheng Xu; Semih Yavuz; Wojciech Kryscinski; Justin Rousseau; Greg Durrett"}, {"ref_id": "b222", "title": "Does synthetic data generation of LLMs help clinical text mining?", "journal": "", "year": "2023", "authors": "Ruixiang Tang; Xiaotian Han; Xiaoqian Jiang; Xia Hu"}, {"ref_id": "b223", "title": "Causaldiscovery performance of ChatGPT in the context of neuropathic pain diagnosis", "journal": "", "year": "2023", "authors": "Ruibo Tu; Chao Ma; Cheng Zhang"}, {"ref_id": "b224", "title": "Chatlog: Recording and analyzing ChatGPT across time", "journal": "", "year": "2023", "authors": "Shangqing Tu; Chunyang Li; Jifan Yu; Xiaozhi Wang; Lei Hou; Juanzi Li"}, {"ref_id": "b225", "title": "Improving Dutch vaccine hesitancy monitoring via multilabel data augmentation with GPT-3.5", "journal": "", "year": "2023", "authors": "Jens Van Nooten; Walter Daelemans"}, {"ref_id": "b226", "title": "PrecogIIITH@WASSA2023: Emotion detection for Urdu-English code-mixed text", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Prashant Bhaskara Hanuma Vedula; Manish Kodali; Ponnurangam Shrivastava;  Kumaraguru"}, {"ref_id": "b227", "title": "ChatGPT for robotics: Design principles and model abilities", "journal": "", "year": "2023", "authors": "Sai Vemprala; Rogerio Bonatti; Arthur Bucker; Ashish Kapoor"}, {"ref_id": "b228", "title": "Can ChatGPT defend its belief in truth? evaluating LLM reasoning via debate", "journal": "", "year": "2023", "authors": "Boshi Wang; Xiang Yue; Huan Sun"}, {"ref_id": "b229", "title": "", "journal": "", "year": "2023", "authors": "Boxin Wang; Weixin Chen; Hengzhi Pei; Chulin Xie; Mintong Kang; Chenhui Zhang; Chejian Xu; Zidi Xiong; Ritik Dutta; Rylan Schaeffer; Sang T Truong; Simran Arora; Mantas Mazeika; Dan Hendrycks; Zinan Lin; Yu Cheng; Sanmi Koyejo; Dawn Song; Bo Li"}, {"ref_id": "b230", "title": "", "journal": "", "year": "", "authors": "Jiaan Wang; Yunlong Liang; Fandong Meng; Zengkui Sun; Haoxiang Shi; Zhixu Li; Jinan Xu; Jianfeng Qu"}, {"ref_id": "b231", "title": "Zeroshot cross-lingual summarization via large language models", "journal": "", "year": "2023", "authors": "Jiaan Wang; Yunlong Liang; Fandong Meng; Beiqi Zou; Zhixu Li; Jianfeng Qu; Jie Zhou"}, {"ref_id": "b232", "title": "Yue Zhang, and Xing Xie. 2023e. On the robustness of ChatGPT: An adversarial and out-of-distribution perspective", "journal": "", "year": "", "authors": "Jindong Wang; Xixu Hu; Wenxin Hou; Hao Chen; Runkai Zheng; Yidong Wang; Linyi Yang; Haojun Huang; Wei Ye; Xiubo Geng; Binxin Jiao"}, {"ref_id": "b233", "title": "UMASS_BioNLP at MEDIQA-chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Junda Wang; Zonghai Yao; Avijit Mitra; Samuel Osebe; Zhichao Yang; Hong Yu"}, {"ref_id": "b234", "title": "Is ChatGPT a good teacher coach? measuring zero-shot performance for scoring and providing actionable insights on classroom instruction", "journal": "", "year": "2023", "authors": "Rose Wang; Dorottya Demszky"}, {"ref_id": "b235", "title": "ChatCAD: Interactive computer-aided diagnosis on medical image using large language models", "journal": "", "year": "2023", "authors": "Sheng Wang; Zihao Zhao; Xi Ouyang; Qian Wang; Dinggang Shen"}, {"ref_id": "b236", "title": "Scibench: Evaluating college-level scientific problem-solving abilities of large language models", "journal": "", "year": "2023", "authors": "Xiaoxuan Wang; Ziniu Hu; Pan Lu; Yanqiao Zhu; Jieyu Zhang; Satyen Subramaniam; Arjun R Loomba; Shichang Zhang; Yizhou Sun; Wei Wang"}, {"ref_id": "b237", "title": "Mint: Evaluating LLMs in multi-turn interaction with tools and language feedback", "journal": "", "year": "2023", "authors": "Xingyao Wang; Zihan Wang; Jiateng Liu; Yangyi Chen; Lifan Yuan; Hao Peng; Heng Ji"}, {"ref_id": "b238", "title": "Metacognitive prompting improves understanding in large language models", "journal": "", "year": "2023", "authors": "Yuqing Wang; Yun Zhao"}, {"ref_id": "b239", "title": "Is ChatGPT a good sentiment analyzer?", "journal": "", "year": "2023", "authors": "Zengzhi Wang; Qiming Xie; Zixiang Ding; Yi Feng; Rui Xia"}, {"ref_id": "b240", "title": "Zero-shot information extraction via chatting with ChatGPT", "journal": "", "year": "2023", "authors": "Xiang Wei; Xingyu Cui; Ning Cheng; Xiaobin Wang; Xin Zhang; Shen Huang; Pengjun Xie; Jinan Xu; Yufeng Chen; Meishan Zhang; Yong Jiang; Wenjuan Han"}, {"ref_id": "b241", "title": "ChatGPT prompt patterns for improving code quality, refactoring, requirements elicitation, and software design", "journal": "", "year": "2023", "authors": "Jules White; Sam Hays; Quchen Fu; Jesse Spencer-Smith; Douglas C Schmidt"}, {"ref_id": "b242", "title": "ChatGPT or grammarly? evaluating ChatGPT on grammatical error correction benchmark", "journal": "", "year": "2023", "authors": "Haoran Wu; Wenxuan Wang; Yuxuan Wan; Wenxiang Jiao; Michael Lyu"}, {"ref_id": "b243", "title": "Multi-level knowledge distillation for out-of-distribution detection in text", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Qianhui Wu; Huiqiang Jiang; Haonan Yin"}, {"ref_id": "b244", "title": "Do PLMs know and understand ontological knowledge?", "journal": "Long Papers", "year": "2023", "authors": "Weiqi Wu; Chengyue Jiang; Yong Jiang; Pengjun Xie; Kewei Tu"}, {"ref_id": "b245", "title": "Evaluating reading comprehension exercises generated by LLMs: A showcase of ChatGPT in education applications", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Changrong Xiao; Sean Xin Xu; Kunpeng Zhang; Yufang Wang; Lei Xia"}, {"ref_id": "b246", "title": "Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts", "journal": "", "year": "2023", "authors": "Jian Xie; Kai Zhang; Jiangjie Chen; Renze Lou; Yu Su"}, {"ref_id": "b247", "title": "The wall street neophyte: A zero-shot analysis of ChatGPT over multimodal stock movement prediction challenges", "journal": "", "year": "2023", "authors": "Qianqian Xie; Weiguang Han; Yanzhao Lai; Min Peng; Jimin Huang"}, {"ref_id": "b248", "title": "Are large language models really good logical reasoners? a comprehensive evaluation and beyond", "journal": "", "year": "2023-06", "authors": "Fangzhi Xu; Qika Lin; Jiawei Han; Tianzhe Zhao"}, {"ref_id": "b249", "title": "SuperCLUE: A comprehensive chinese large language model benchmark", "journal": "", "year": "2023", "authors": "Liang Xu; Anqi Li; Lei Zhu; Hang Xue; Changtai Zhu; Kangkang Zhao; Haonan He; Xuanwei Zhang; Qiyue Kang; Zhenzhong Lan"}, {"ref_id": "b250", "title": "IDOL: Indicator-oriented logic pre-training for logical reasoning", "journal": "", "year": "2023", "authors": "Zihang Xu; Ziqing Yang; Yiming Cui; Shijin Wang"}, {"ref_id": "b251", "title": "Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023a. Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond", "journal": "", "year": "", "authors": "Jingfeng Yang; Hongye Jin; Ruixiang Tang; Xiaotian Han"}, {"ref_id": "b252", "title": "Neural machine translation data generation and augmentation using ChatGPT", "journal": "", "year": "2023", "authors": "Wayne Yang; Garrett Nicolai"}, {"ref_id": "b253", "title": "Exploring the limits of ChatGPT for query or aspect-based text summarization", "journal": "", "year": "2023", "authors": "Xianjun Yang; Yan Li; Xinlu Zhang; Haifeng Chen; Wei Cheng"}, {"ref_id": "b254", "title": "Mmreact: Prompting ChatGPT for multimodal reasoning and action", "journal": "", "year": "2023", "authors": "Zhengyuan Yang; Linjie Li; Jianfeng Wang; Kevin Lin; Ehsan Azarnasab; Faisal Ahmed; Zicheng Liu; Ce Liu; Michael Zeng; Lijuan Wang"}, {"ref_id": "b255", "title": "Tree of thoughts: Deliberate problem solving with large language models", "journal": "", "year": "2023", "authors": "Shunyu Yao; Dian Yu; Jeffrey Zhao; Izhak Shafran; Thomas L Griffiths; Yuan Cao; Karthik Narasimhan"}, {"ref_id": "b256", "title": "", "journal": "", "year": "", "authors": "Junjie Ye; Xuanting Chen; Nuo Xu; Can Zu; Zekai Shao; Shichun Liu; Yuhan Cui; Zeyang Zhou; Chao Gong; Yang Shen; Jie Zhou; Siming Chen; Tao Gui; Qi Zhang"}, {"ref_id": "b257", "title": "Zero-shot temporal relation extraction with ChatGPT", "journal": "", "year": "2023", "authors": "Chenhan Yuan; Qianqian Xie; Sophia Ananiadou"}, {"ref_id": "b258", "title": "How well do large language models perform in arithmetic tasks?", "journal": "", "year": "2023", "authors": "Zheng Yuan; Hongyi Yuan; Chuanqi Tan; Wei Wang; Songfang Huang"}, {"ref_id": "b259", "title": "Daijun Ding, and Liwen Jing. 2022. How would stance detection techniques evolve after the launch of ChatGPT? arXiv preprint", "journal": "", "year": "", "authors": "Bowen Zhang"}, {"ref_id": "b260", "title": "Investigating chain-of-thought with ChatGPT for stance detection on social media", "journal": "", "year": "2023", "authors": "Bowen Zhang; Xianghua Fu; Daijun Ding; Hu Huang; Yangyang Li; Liwen Jing"}, {"ref_id": "b261", "title": "Extractive summarization via ChatGPT for faithful summary generation", "journal": "", "year": "2023", "authors": "Haopeng Zhang; Xiao Liu; Jiawei Zhang"}, {"ref_id": "b262", "title": "Silvio Savarese, and Caiming Xiong. 2023c. Dialogstudio: Towards richest and most diverse unified dataset collection for conversational AI", "journal": "", "year": "", "authors": "Jianguo Zhang; Kun Qian; Zhiwei Liu; Shelby Heinecke; Rui Meng; Ye Liu; Zhou Yu; Huan Wang"}, {"ref_id": "b263", "title": "BayLing: Bridging cross-lingual alignment and instruction following through interactive translation for large language models", "journal": "", "year": "2023", "authors": "Shaolei Zhang; Qingkai Fang; Zhuocheng Zhang; Zhengrui Ma; Yan Zhou; Langlin Huang; Mengyu Bu; Shangtong Gui; Yunji Chen; Xilin Chen; Yang Feng"}, {"ref_id": "b264", "title": "Sinno Jialin Pan, and Lidong Bing. 2023e. Sentiment analysis in the era of large language models: A reality check", "journal": "", "year": "", "authors": "Wenxuan Zhang; Yue Deng; Bing Liu"}, {"ref_id": "b265", "title": "2023f. Modeling label semantics improves activity recognition", "journal": "", "year": "", "authors": "Xiyuan Zhang; Ranak Roy Chowdhury; Dezhi Hong; Rajesh K Gupta; Jingbo Shang"}, {"ref_id": "b266", "title": "Yanpeng Tong, and Bing Qin. 2023a. Is ChatGPT equipped with emotional dialogue capabilities?", "journal": "", "year": "", "authors": "Weixiang Zhao; Yanyan Zhao; Xin Lu; Shilong Wang"}, {"ref_id": "b267", "title": "HW-TSC at SemEval-2023 task 7: Exploring the natural language inference capabilities of ChatGPT and pretrained language model for clinical trial", "journal": "", "year": "2023", "authors": "Xiaofeng Zhao; Min Zhang; Miaomiao Ma; Chang Su; Yilun Liu; Minghan Wang; Xiaosong Qiao; Jiaxin Guo; Yinglu Li; Wenbing Ma"}, {"ref_id": "b268", "title": "Pre-trained language models can be fully zero-shot learners", "journal": "Long Papers", "year": "2023", "authors": "Xuandong Zhao; Siqi Ouyang; Zhiguo Yu; Ming Wu; Lei Li"}, {"ref_id": "b269", "title": "Large language models are not robust multiple choice selectors", "journal": "", "year": "2023", "authors": "Chujie Zheng; Hao Zhou; Fandong Meng; Jie Zhou; Minlie Huang"}, {"ref_id": "b270", "title": "Why does ChatGPT fall short in providing truthful answers?", "journal": "", "year": "2023", "authors": "Jie Shen Zheng; Kevin Chen-Chuan Huang;  Chang"}, {"ref_id": "b271", "title": "What makes good counterspeech? a comparison of generation approaches and evaluation metrics", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Yi Zheng; Bj\u00f6rn Ross; Walid Magdy"}, {"ref_id": "b272", "title": "Generative job recommendations with large language model", "journal": "", "year": "2023", "authors": "Zhi Zheng; Zhaopeng Qiu; Xiao Hu; Likang Wu; Hengshu Zhu; Hui Xiong"}, {"ref_id": "b273", "title": "Can ChatGPT understand too? a comparative study on ChatGPT and fine-tuned bert", "journal": "", "year": "2023", "authors": "Qihuang Zhong; Liang Ding; Juhua Liu; Bo Du; Dacheng Tao"}, {"ref_id": "b274", "title": "Dinggang Shen, Tianming Liu, and Tuo Zhang. 2023b. Chatabl: Abductive learning via natural language interaction with ChatGPT", "journal": "", "year": "", "authors": "Tianyang Zhong; Yaonai Wei; Li Yang; Zihao Wu; Zhengliang Liu; Xiaozheng Wei; Wenjun Li; Junjie Yao; Chong Ma; Xiang Li; Dajiang Zhu; Xi Jiang; Junwei Han"}, {"ref_id": "b275", "title": "Ryan Cotterell, and Mrinmaya Sachan. 2023. RecurrentGPT: Interactive generation of (arbitrarily) long text", "journal": "", "year": "", "authors": "Wangchunshu Zhou; Yuchen Eleanor Jiang; Peng Cui; Tiannan Wang; Zhenxin Xiao; Yifan Hou"}, {"ref_id": "b276", "title": "Can ChatGPT reproduce human-generated labels? a study of social computing tasks", "journal": "", "year": "2023", "authors": "Yiming Zhu; Peixian Zhang; Ehsan-Ul Haq; Pan Hui; Gareth Tyson"}, {"ref_id": "b277", "title": "Chunyang Chen, and Zhenchang Xing. 2023. Red teaming ChatGPT via jailbreaking: Bias, robustness, reliability and toxicity", "journal": "", "year": "", "authors": "Yujin Terry Yue Zhuo;  Huang"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Distribution of the dates when papers evaluating ChatGPT were first uploaded to arXiv or published. The dotted line represents the ChatGPT API release (March 1st, 2023, dotted line in the chart) as a cutoff point. The single paper shown using the API in February is by a research group that reported having early API access.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: Evaluation reproducibility. Through the above Sankey diagram, we report facilitators and barriers to reproducing the carried-out experiments. This includes providing the used prompts, a repository with usable code and the use of sampling.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure4: Evaluation fairness. Through the above Sankey diagram, we report whether the proprietary LLMs were compared against other models, and if the comparison was equal. In this context, \"Unfair\" comparison refers to evaluating different models on different amounts of data.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ", which explicitly mentions the use of users' data for model training: \"[...] when you use our services for individuals such as ChatGPT or DALL-E, we may use your content to train our models [...]\" It also clarifies that the user data are not used for model training if sent via API and business services: \"[...] we don't use content from our business offerings [...] and our API Platform to train our models [...]\"", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": The number of datasets with low (Lo), moderate-low (M-Lo), moderate-high (M-Hi) and high leak severity (Hi) is reported for each task, omitting cus-tom datasets. A more detailed table, including specific dataset names, is provided in the Appendix C."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Yanran Chen and Steffen Eger. 2022. Transformers go for the lols: Generating (humourous) titles from scientific abstracts end-to-end.Cheng-Han Chiang and Hung-yi Lee. 2023. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607-15631, Toronto, Canada. Association for Computational Linguistics. Jonathan H. Choi, Kristin E. Hickman, Amy Monahan, and Daniel B. Schwarcz. 2023. ChatGPT goes to law school. Journal of Legal Education. Mohita Chowdhury, Ernest Lim, Aisling Higham, Rory McKinnon, Nikoletta Ventoura, Yajie He, and Nick De Pennington. 2023. Can large language models safely address patient questions following cataract surgery? In Proceedings of the 5th Clinical Natural Language Processing Workshop, pages 131-137, Toronto, Canada. Association for Computational Linguistics. Haoran Chu and Sixiao Liu. 2023. Can AI tell good stories? narrative transportation and persuasion with ChatGPT. PsyArXiv. Ted M. Clark. 2023. Investigating the use of an artificial intelligence chatbot with general chemistry exam questions. Journal of Chemical Education, 100(5):1905-1916. Merten Nikolay Dahlkemper, Simon Zacharias Lahme, and Pascal Klein. 2023. How do physics students evaluate artificial intelligence responses on comprehension questions? a study on the perceived scientific accuracy and linguistic quality.", "figure_data": "Prompts Repo Sampl. Customn. (%)3 (2.11%)Prompts Repo Sampl. Custom n. (%)1 (0.70%)1 (1.43%)8 (5.63%)1 (1.43%)3 (2.11%)1 (1.43%)2 (1.41%)14 (20.00%)20 (14.08%)7 (10.00%)3 (2.11%)9 (12.86%)27 (19.01%)3 (4.29%)3 (2.11%)8 (11.43%)37 (26.06%)4 (5.71%)4 (2.82%)16 (22.86%)27 (19.01%)6 (6.57%)4 (2.82%)(a) Pre-prints(b) Peer-reviewed works"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Statistics related to the reproducibility of the work reviewed: the availability of used prompts (Prompts)", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Fairness statistics for reviewed work. Statistics related to the practices of performance comparisons between ChatGPT/GPT-4 and other open models: whether such comparisons are performed at all (Comp.) and whether they are of the same scale (Scale).", "figure_data": ""}], "formulas": [], "doi": "10.18653/v1/2023.trustnlp-1.5"}