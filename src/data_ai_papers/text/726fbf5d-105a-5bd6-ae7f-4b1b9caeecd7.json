{"title": "Improved guarantees and a multiple-descent curve for Column Subset Selection and the Nystr\u00f6m method", "authors": "Micha L Derezi\u0144ski; Rajiv Khanna; Michael W Mahoney", "pub_date": "2020-12-18", "abstract": "The Column Subset Selection Problem (CSSP) and the Nystr\u00f6m method are among the leading tools for constructing small low-rank approximations of large datasets in machine learning and scientific computing. A fundamental question in this area is: how well can a data subset of size k compete with the best rank k approximation? We develop techniques which exploit spectral properties of the data matrix to obtain improved approximation guarantees which go beyond the standard worst-case analysis. Our approach leads to significantly better bounds for datasets with known rates of singular value decay, e.g., polynomial or exponential decay. Our analysis also reveals an intriguing phenomenon: the approximation factor as a function of k may exhibit multiple peaks and valleys, which we call a multiple-descent curve. A lower bound we establish shows that this behavior is not an artifact of our analysis, but rather it is an inherent property of the CSSP and Nystr\u00f6m tasks. Finally, using the example of a radial basis function (RBF) kernel, we show that both our improved bounds and the multiple-descent curve can be observed on real datasets simply by varying the RBF parameter.", "sections": [{"heading": "Introduction", "text": "We consider the task of selecting a small but representative sample of column vectors from a large matrix. Known as the Column Subset Selection Problem (CSSP), this is a well-studied combinatorial optimization task with many applications in machine learning (e.g., feature selection, see Guyon & Elisseeff, 2003;Boutsidis et al., 2008), scientific computing (e.g., Chan & Hansen, 1992;Drineas et al., 2008) and signal processing (e.g., Balzano et al., 2010). In a commonly studied variant of this task, we aim to minimize the squared error of projecting all columns of the matrix onto the subspace spanned by the chosen column subset.\nDefinition 1 (CSSP). Given an m\u02c6n matrix A, pick a set S \u010e t1, ..., nu of k column indices, to minimize\nEr A pSq :\" }A\u00b4P S A} 2 F ,\nwhere }\u00a8} F is the Frobenius norm, P S is the projection onto spanta i : i P Su and a i denotes the ith column of A.\nAnother variant of the CSSP emerges in the kernel setting under the name Nystr\u00f6m method (Williams & Seeger, 2001;Drineas & Mahoney, 2005;Gittens & Mahoney, 2016). We also discuss this variant, showing Approximation factor Deshpande et al. (2006) This paper (upper) This paper (lower) Experiments\nFigure 1: Empirical study of the expected approximation factor ErEr A pSqs{OPT k for a k-DPP with different subset sizes |S| \" k, compared to our theory. We use a data matrix A whose spectrum exhibits two sharp drops, demonstrating multiple-descent. The lower bounds are based on Theorem 3, whereas, as our upper bound, we plot the minimum over all \u03a6 s pkq from Theorem 1. Note that multiple-descent vanishes under smooth spectral decay, resulting in improved guarantees (see Theorem 2 and Figure 2).\nhow our analysis applies in this context. Both the CSSP and the Nystr\u00f6m method are ways of constructing accurate low-rank approximations by using submatrices of the target matrix. Therefore, it is natural to ask how close we can get to the best possible rank k approximation error:\nOPT k :\" min B: rankpBq\"k }A\u00b4B} 2 F \u010f min S: |S|\"k Er A pSq.\nOur goal is to find a subset S of size k for which the ratio between Er A pSq and OPT k is small. Furthermore, a brute force search requires iterating over all`n k\u02d8s ubsets, which is prohibitively expensive, so we would like to find our subset more efficiently.\nExtensive literature has been dedicated to developing algorithms for the CSSP (an in depth discussion of the related work can be found in Appendix A). In terms of worst-case analysis, Deshpande et al. (2006) gave a randomized method which returns a set S of size k such that:\nErEr A pSqs OPT k \u010f k`1.(1)\nWhile the original algorithm was slow, efficient implementations have been provided since then (e.g., see Deshpande & Rademacher, 2010;Derezi\u0144ski, 2019). The method belongs to the family of cardinality constrained Determinantal Point Processes (DPPs, see , and will be denoted as S \" k-DPPpA J Aq. The approximation factor k`1 is optimal in the worst-case, since for any 0 \u0103 k \u0103 n \u010f m and 0 \u0103 \u03b4 \u0103 1, an m\u02c6n matrix A can be constructed for which Er A pSq OPT k \u011b p1\u00b4\u03b4qpk`1q for all subsets S of size k. Yet it is known that, in practice, CSSP algorithms perform better than worst-case, so the question we consider is: how can we go beyond the usual worst-case analysis to accurately reflect what is possible in the CSSP?\nContributions. We provide improved guarantees for the CSSP approximation factor, which go beyond the worst-case analysis and which lead to surprising conclusions.\n1. New upper bounds: We develop a family of upper bounds on the CSSP approximation factor (Theorem 1), which we call the Master Theorem as they can be used to derive a number of new guarantees.\nIn particular, we show that when the data matrix A exhibits a known spectral decay, then (1) can often be drastically improved (Theorem 2).\n2. New lower bound: Even though the worst-case upper bound in (1) can often be loose, there are cases when it cannot be improved. We give a new lower bound construction (Theorem 3) showing that there are matrices A for which multiple different subset sizes exhibit worst-case behavior.\n3. Multiple-descent curve: Our upper and lower bounds reveal that for some matrices the CSSP approximation factor can exhibit peaks and valleys as a function of the subset size k (see Figure 1). We show that this phenomenon is an inherent property of the CSSP (Corollary 1).", "publication_ref": ["b37", "b12", "b16", "b30", "b5", "b58", "b29", "b33", "b28", "b28", "b27", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Main results", "text": "Our upper bounds rely on the notion of effective dimensionality called stable rank (Alaoui & Mahoney, 2015).\nHere, we use an extended version of this concept, as defined by Bartlett et al. (2019).\nDefinition 2 (Stable rank). Let \u03bb 1 \u011b \u03bb 2 \u011b ... denote the eigenvalues of the matrix A J A. For 0 \u010f s \u0103 rankpAq, we define the stable rank of order s as sr s pAq \" \u03bb\u00b41 s`1 \u0159 i\u0105s \u03bb i . In the following result, we define a family of functions \u03a6 s pkq which bound the approximation factor Er A pSq{OPT k in the range of k between s and s`sr s pAq. We call this the Master Theorem because we use it to derive a number of more specific upper bounds.\nTheorem 1 (Master Theorem). Given 0 \u010f s \u0103 rankpAq, let t s \" s`sr s pAq, and suppose that s`7 4 ln 2 1 \u010f k \u010f t s\u00b41 , where\n0 \u0103 \u010f 1 2 . If S \" k-DPPpA J Aq, then ErEr A pSqs OPT k \u010f p1`2 q 2 \u03a6 s pkq,\nwhere \u03a6 s pkq \"`1`s k\u00b4s\u02d8b 1`2 pk\u00b4sq ts\u00b4k .\nNote that we separated out the dependence on from the function \u03a6 s pkq, because the term p1`2 q 2 is an artifact of a concentration of measure analysis that is unlikely to be of practical significance. In fact, we believe that the dependence on can be eliminated from the statement entirely (see Conjecture 1).\nWe next examine the consequences of the Master Theorem, starting with a sharp transition that occurs as k approaches the stable rank of A.\nRemark 1 (Sharp transition). For any k it is true that:\n1. For all A, if k \u010f sr 0 pAq\u00b41, then there is a subset S of size k such that Er A pSq OPT k \" Op ? k q.", "publication_ref": ["b0", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "2.", "text": "There is A such that sr 0 pAq\u00b41 \u0103 k \u0103 sr 0 pAq and for every size k subset S, Er A pSq OPT k \u011b 0.9 k. Part 1 of the remark follows from the Master Theorem by setting s \" 0, whereas part 2 follows from the lower bound of Guruswami & Sinop (2012). Observe how the worst-case approximation factor jumps from Op ? k q to \u2126pkq, as k approaches sr 0 pAq. An example of this sharp transition is shown in Figure 1, where the stable rank of A is around 20.\nWhile certain matrices directly exhibit the sharp transition from Remark 1, many do not. In particular, for matrices with a known rate of spectral decay, the Master Theorem can be used to provide improved guarantees on the CSSP approximation factor over all subset sizes.\nTo illustrate this, we give novel bounds for the two most commonly studied decay rates: polynomial and exponential.\nTheorem 2 (Examples without sharp transition). Let \u03bb 1 \u011b \u03bb 2 \u011b ... be the eigenvalues of A J A. There is an absolute constant c such that for any 0 \u0103 c 1 \u010f c 2 , with \u03b3 \" c 2 {c 1 , if:\n1. (polynomial spectral decay) c 1 i\u00b4p \u010f \u03bb i \u010f c 2 i\u00b4p @ i , with p \u0105 1, then S \" k-DPPpA J Aq satisfies ErEr A pSqs OPT k \u010f c\u03b3p. 2. (exponential spectral decay) c 1 p1\u00b4\u03b4q i \u010f \u03bb i \u010f c 2 p1\u00b4\u03b4q i @ i , \u03b4 P p0, 1q, then S \" k-DPPpA J Aq satisfies ErEr A pSqs OPT k \u010f c\u03b3p1`\u03b4kq.\nNote that for polynomial decay, unlike in (1), the approximation factor is constant, i.e., it does not depend on k. For exponential decay, our bound provides an improvement over (1) when \u03b4 \" op1q. To illustrate how these types of bounds can be obtained from the Master Theorem, consider the function \u03a6 s pkq for some s \u0105 0. The first term in the function, 1`s k\u00b4s , decreases with k, whereas the second term (the square root) increases, albeit at a slower rate. This creates a U-shaped curve which, if sufficiently wide, has a valley where the approximation factor can get arbitrarily close to 1. This will occur when sr s pAq is large, i.e., when the spectrum of A J A has a relatively flat region after the sth eigenvalue (Figure 1 for k between 20 and 40). Note that a peak value of some function \u03a6 s1 may coincide with a valley of some \u03a6 s2 , so only taking a minimum over all functions reveals the true approximation landscape predicted by the Master Theorem. To prove Theorem 2, we show that the stable ranks sr s pAq are sufficiently large so that any k lies in the valley of some function \u03a6 s pkq (see Section 2).\nThe peaks and valleys of the CSSP approximation factor suggested by Theorem 1 are in fact an inherent property of the problem, rather than an artifact of our analysis or the result of using a particular algorithm. We prove this by constructing a family of matrices A for which the best possible approximation factor is large, i.e., close to the worst-case upper bound of Deshpande et al. (2006), not just for one size k, but for a sequence of increasing sizes.\nTheorem 3 (Lower bound). For any \u03b4 P p0, 1q and 0 \" k 0 \u0103 k 1 \u0103 ... \u0103 k t \u0103 n \u010f m, there is a matrix A P R m\u02c6n such that for any subset S of size k i , where i P t1, ..., tu,\nEr A pSq OPT ki \u011b p1\u00b4\u03b4qpk i\u00b4ki\u00b41 q.\nCombining the Master Theorem with the lower bound of Theorem 3 we can easily provide an example matrix for which the optimal solution to the CSSP problem exhibits multiple peaks and valleys. We refer to this phenomenon as the multiple-descent curve.\nCorollary 1 (Multiple-descent curve). For t P N and \u03b4 P p0, 1q, there is a sequence 0 \u0103 k l\n1 \u0103 k u 1 \u0103 k l 2 \u0103 k u 2 \u0103 .\n.. \u0103 k l t \u0103 k u t and A P R m\u02c6n such that for any i P t1, ..., tu:\nmin S:|S|\"k l i Er A pSq OPT k l i \u010f 1`\u03b4 and min S:|S|\"k u i Er A pSq OPT k u i \u011b p1\u00b4\u03b4qpk u i`1 q.\nConnection to double descent. A number of phase transitions have been recently observed in the machine learning literature which are commonly dubbed double descent. The term was introduced by Belkin et al. (2019a) in the context of generalization error of statistical learning, however Poggio et al. (2019) observed that the behavior of the generalization error, at least for linear models, can be explained by a more fundamental double descent phenomenon observed in the condition number of random matrices. Also, Liao et al. (2020) showed that double descent is merely a manifestation of the spectral phase transitions in high-dimensional random kernel matrices. Further,  observed that the spectral phase transitions of certain kernels can lead to multiple double descent peaks. The multiple-descent phenomenon in the CSSP that emerges from our analysis is related to those works in that the spectral properties of the data matrix (and, in particular, the condition number) determine the peaks (i.e., phase transitions) discussed in Corollary 1. Those parallels manifest themselves most clearly when comparing our analysis to the work of Bartlett et al. (2019), which uses the same notion of stable rank as we do, and Derezi\u0144ski et al. (2019b), where determinantal sampling plays a central role in the analysis of double descent. However, we stress that there are also important differences in our setting: (1) the CSSP is a deterministic optimization task, and (2) we study the approximation factor, rather than the generalization error (further discussion in Appendix A).", "publication_ref": ["b36", "b28", "b9", "b52", "b46", "b6", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "The Nystr\u00f6m method", "text": "We briefly discuss how our results translate to guarantees for the Nystr\u00f6m mehod, a variant of the CSSP in the kernel setting which has gained considerable interest in the machine learning literature (Drineas & Mahoney, 2005;Gittens & Mahoney, 2016). In this context, rather than being given the column vectors explicitly, we consider the n\u02c6n matrix K whose entry pi, jq is the dot product between the ith and jth vector in the kernel space, xa i , a j y K . A Nystr\u00f6m approximation of K based on subset S is defined as p KpSq \" CB : C J , where B is the |S|\u02c6|S| submatrix of K indexed by S, whereas C is the n\u02c6|S| submatrix with columns indexed by S. The Nystr\u00f6m method has many applications in machine learning, including for kernel machines (Williams & Seeger, 2001), Gaussian Process regression (Burt et al., 2019) and Independent Component Analysis (Bach & Jordan, 2003).\nRemark 2. If K \" A J A and }\u00a8}\u02dais the trace norm, then \u203a \u203a K\u00b4p KpSq \u203a \u203a\u02da\" Er A pSq for all S \u010e t1, ..., nu. Moreover, the trace norm error of the best rank k approximation of K, is equal to the squared Frobenius norm error of the best rank k approximation of A, i.e., min\nx K: rankpKq\"k }K\u00b4p K}\u02da\" OPT k .\nThis connection was used by Belabbas & Wolfe (2009) to adapt the k`1 approximation factor bound of Deshpande et al. (2006) to the Nystr\u00f6m method. Similarly, all of our results for the CSSP, including the multiple-descent curve that we have observed, can be translated into analogous statements for the trace norm approximation error in the Nystr\u00f6m method. Of particular interest are the improved bounds for kernel matrices with known eigenvalue decay rates. Such matrices arise naturally in machine learning when using standard kernel functions such as the Gaussian Radial Basis Function (RBF) kernel (a.k.a. the squared exponential kernel) and the Mat\u00e9rn kernel (Burt et al., 2019). RBF kernel: If xa i , a j y K \" expp\u00b4}a i\u00b4aj } 2 {\u03c3 2 q and the data comes from N p0, \u03b7 2 q, then, for large n, Santa et al., 1997), so Theorem 2 yields an approximation factor of Op1`a`c a`b`c kq, better than k`1 when \u03c3 2 ! \u03b7 2 . Note that the parameter \u03c3 defines the size of a neighborhood around which the data points are deemed similar by the RBF kernel. Therefore, smaller \u03c3 means that each data point has fewer similar neighbors.\n\u03bb i -\u03bb 1 p b a`b`c q i , where a \" 1 4\u03b7 2 , b \" 1 \u03c3 2 and c \" ? a 2`2 ab (\nMat\u00e9rn kernel: If K is the Mat\u00e9rn kernel with parameters \u03bd and and the data is distributed according to a uniform measure in one dimension, then \u03bb i -\u03bb 1 i\u00b42 \u03bd\u00b41 (Rasmussen & Williams, 2006), so Theorem 2 yields a Nystr\u00f6m approximation factor of Op1`\u03bdq for any subset size k.\nIn Section 4, we also empirically demonstrate our improved guarantees and the multiple-descent curve for the Nystr\u00f6m method with the RBF kernel.", "publication_ref": ["b29", "b33", "b58", "b14", "b4", "b7", "b28", "b14", "b55", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "Upper bounds", "text": "In this section, we derive the upper bound given in Theorem 1 by using a novel expectation formula for the squared projection error of a DPP. We then show how this result can be used to obtain improved guarantees for matrices with known eigenvalue decays, i.e., Theorem 2. Our analysis heavily relies on the theory of DPPs , so for completeness, in Appendix B we provide a brief summary of DPPs and the relevant results.\nLet S \" DPPp 1 \u03b1 A J Aq denote a distribution over all subsets S \u010e rns so that PrpSq9 detp 1 \u03b1 A J S A S q, where \u03b1 \u0105 0. Then, k-DPPpA J Aq is simply a restriction of DPPp 1 \u03b1 A J Aq to the subsets of size k (regardless of the choice of \u03b1). However, the expected subset size for DPPp 1 \u03b1 A J Aq does depend on \u03b1. Our analysis relies on a careful selection of this parameter. In Lemma 6 (Appendix B), we show the following expectation formula for the CSSP approximation error:\nErEr A pSqs \" Er|S|s\u00a8\u03b1, for S \" DPPp 1 \u03b1 A J Aq.\nIf we set \u03b1 \" OPT k \" \u0159 n i\"k`1 \u03bb i , where \u03bb i are the eigenvalues of A J A in decreasing order, then:\nEr|S|s \" n \u00ff i\"1 \u03bb i \u03b1`\u03bb i \u010f k \u00ff i\"1 \u03bb i \u03b1`\u03bb i`1 \u010f k`1.\nThis recovers the upper bound of Deshpande et al. (2006), i.e., ErEr A pSqs \u010f pk`1qOPT k , except that the subset size is randomized with expectation bounded by k`1, instead of a fixed subset size equal k. However, a more refined choice of the parameter \u03b1 allows us to significantly improve on the above error bound in certain regimes, as shown below.\nLemma 1. For any A, 0 \u010f \u0103 1 and s \u0103 k \u0103 t s , where t s \" s`sr s pAq, say S \" DPPp 1 \u03b1 A J Aq for \u03b1 \" \u03b3spkqOPT k p1\u00b4 qpk\u00b4sq and \u03b3 s pkq :\" b 1`2 pk\u00b4sq ts\u00b4k . Then, defining \u03a6 s pkq :\"`1`s k\u00b4s\u02d8\u03b3 s pkq,\nE \" Er A pSq \u2030 OPT k \u010f \u03a6 s pkq 1\u00b4\nand Er|S|s \u010f k\u00b4 k\u00b4s \u03b3 s pkq .\nNote that, setting \" 0, the above lemma implies that we can achieve approximation factor \u03a6 s pkq with a DPP whose expected size is bounded by k. We introduce so that we can convert the bound from DPP to the fixed size k-DPP via a concentration argument. Intuitively, our strategy is to show that the randomized subset size of a DPP is sufficiently concentrated around its expectation that with high probability it will be bounded by k, and for this we need the expectation to be strictly below k. A careful application of the Chernoff bound for a Poisson binomial random variable yields the following concentration bound.\nLemma 2. Let S be as in Lemma 1 with \u010f 1 2 . If s`7 4 ln 2 1 \u010f k \u010f t s\u00b41 , then Prp|S| \u0105 kq \u010f . Finally, any expected bound for random size DPPs can be converted to an expected bound for a fixed size k-DPP via the following result.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma 3. For any", "text": "A P R m\u02c6n , k P rns and \u03b1 \u0105 0, if S \" DPPp 1 \u03b1 A J Aq and S 1 \" k-DPPpA J Aq, then E \" Er A pS 1 q \u2030 \u010f E \" Er A pSq | |S| \u010f k \u2030 .\nThe above inequality may seem intuitively obvious since adding more columns to a set S to complete it to size k always reduces the error. However, a priori, it could happen that going from subsets of size k\u00b41 to subsets of size k results in a redistribution of probabilities to the subsets with larger error. To show that this will not happen, our proof relies on classic but non-trivial combinatorial bounds called Newton's inequalities. Putting together Lemmas 1, 2 and 3, we obtain our Master Theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Theorem 1 Let S \" DPPp 1", "text": "\u03b1 A J Aq be sampled as in Lemma 1, and let S 1 \" k-DPPpA J Aq. We have:\nE \" Er A pS 1 q \u2030 paq \u010f E \" Er A pSq | |S| \u010f k \u2030 \u010f E \" Er A pSq \u2030 Prp|S| \u010f kq pbq \u010f \u03a6 s pkq p1\u00b4 q 2\u00a8O PT k ,\nwhere paq follows from Lemma 3 and pbq follows from Lemmas 1 and 2. Since 0 \u0103 \u010f 1 2 , we have 1 p1\u00b4 q 2 \u010f p1`2 q 2 , which completes the proof. We now demonstrate how Theorem 1 can be used as the Master Theorem to derive new bounds on the CSSP approximation factor under additional assumptions on the singular value decay of matrix A. Rather than a single upper bound, Theorem 1 provides a family of upper bounds \u03a6 s , each with a range of applicable values k. Since each \u03a6 s pkq forms a U-shaped curve, its smallest point falls near the middle of that range. In Figure 2 we visualize these bounds as a sliding window that sweeps across the axis representing possible subset sizes. The width of the window varies: when it starts at s then its width is the stable rank sr s pAq. The wider the window, the lower is the valley of the corresponding U-curve. Thus, when bounding the approximation factor for a given k, we should choose the widest window such that k falls near the bottom of its U-curve. Showing a guarantee that holds for all k requires lower-bounding the stable ranks sr s pAq for each s. This is straightforward for both polynomial and exponential decay. Specifically, using the notation from Theorem 2, in Appendix E we prove that: sr s pAq \"", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "#", "text": "\u2126ps{pq, for polynomial rate \u03bb i -1{i p , \u2126p1{\u03b4q, for exponential rate \u03bb i -p1\u00b4\u03b4q i . Figure 2: Illustration of the upper bound functions \u03a6 s pkq for different values of s, with a 200\u02c6200 matrix A such that the ith eigenvalue of A J A is set to: (top) 1{i; (bottom) 1 for i \u0103 30 and 0.01 for i \u011b 30. For each function, we marked the window of applicable k's with a horizontal line. For polynomial spectral decay (top), the stable rank sr s pAq (i.e., the width of the window starting at s) increases, while for the sharp spectrum drop (bottom) the stable rank shrinks as the window approaches the drop, causing a peak in the upper bound.\nAs an example, Figure 2 (left) shows that the stable rank sr s pAq, i.e., the width of the window starting at s, grows linearly with s for eigenvalues decaying polynomially with p \" 1. As a result, the bottom of each U-shaped curve remains at roughly the same level, making the CSSP approximation factor independent of k, as in Theorem 2. In contrast, Figure 2 (right) provides the same plot for a different matrix A with a sharp drop in the spectrum. The U-shaped curves cannot slide smoothly across that drop because of the shrinking stable ranks, which results in a peak similar to the ones observed in Figure 1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lower bound", "text": "As discussed in the previous section, our upper bounds for the CSSP approximation factor exhibit a peak (a high point, with the bound decreasing on either side) around a subset size k when there is a sharp drop in the spectrum of A around the kth singular value. It is natural to ask whether this peak is an artifact of our analysis, or a property of the k-DPP distribution, or whether even optimal CSSP subsets exhibit this phenomenon. In this section, we extend a lower bound construction of Deshpande et al. (2006) and use it to show that for certain matrices the approximation factor of the optimal CSSP subset, i.e., min |S|\"k Er A pSq{OPT k , can exhibit not just one but any number of peaks as a function of k, showing that the multiple-descent curve in Figure 1 describes a real phenomenon in the CSSP.\nThe lower bound construction of Deshpande et al. (2006) relies on arranging the column vectors of a pk`1q\u02c6pk`1q matrix A into a centered symmetric k-dimensional simplex. This way, the k`1 columns are spanning a k dimensional subspace which contains the k leading singular vectors of A. They then proceed to shift the columns slightly in the direction orthogonal to that subspace so that the pk`1qst singular value of A becomes non-zero. This results in an instance of the CSSP with a sharp drop in the spectrum. Due to the symmetry in this construction, all subsets of size k have an identical squared projection error. It is easy to show that this error satisfies Er A pSq \u011b p1\u00b4\u03b4qpk`1qOPT k , where \u03b4 is a parameter which depends on the condition number of matrix A and it can be driven arbitrarily close to 0. Another variant of this construction was also provided by Guruswami & Sinop (2012). The key limitation of both of these constructions is that they only provide a lower bound for a single subset size k in a given matrix, whereas our goal is to show that the CSSP can exhibit the multiple-descent curve, which requires lower bounds for multiple different values of k holding with respect to the same matrix A.\nOur strategy for constructing the lower bound matrix is to concatenate together multiple sets of columns, each of which represents a simplex spanning some subspace of R m . The key challenge that we face in this approach is that, unlike in the construction of Deshpande et al. (2006), different subsets of the same size will have different projection errors. Lemma 4. Fix \u03b4 P p0, 1q and consider unit vectors a i,j P R m in general position, where i P rts, j P rl i s, such that \u0159 j a i,j \" 0 for each i, and for any i, j, i 1 , j 1 , if i \u2030 i 1 then a i,j is orthogonal to a i 1 ,j 1 . Also, let unit vectors tv i u iPrts be orthogonal to each other and to all a i,j . There are positive scalars \u03b1 i , \u03b2 i for i P rts such that matrix A with columns \u03b1 i a i,j`\u03b2i v i over all i and j satisfies:\nmin |S|\"ki Er A pSq OPT ki \u011b p1\u00b4\u03b4ql i , for k i \" l 1`. ..`l i\u00b41 .\nProof of Theorem 3 We let l 1 \" k 1`1 and then for i \u0105 1 we set l i \" k i\u00b4ki\u00b41 . We then construct the vectors a i,j that satisfy Lemma 4 by letting each set ta i,j u j be the corners of a centered pl i\u00b41 q-dimensional regular simplex. We ensure that each simplex is orthogonal to every other simplex by placing them in orthogonal subspaces.", "publication_ref": ["b28", "b28", "b36", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical evaluation", "text": "In this section, we provide an empirical evaluation designed to demonstrate how our improved guarantees for the CSSP and Nystr\u00f6m method, as well as the multiple-descent phenomenon, can be easily observed on real datasets. We use a standard experimental setup for data subset selection using the Nystr\u00f6m method (Gittens & Mahoney, 2016), where an n\u02c6n kernel matrix K for a dataset of size n is defined so that the entry pi, jq is computed using the Gaussian Radial Basis Function (RBF) kernel: xa i , a j y K \" expp\u00b4}a i\u00b4aj } 2 {\u03c3 2 q, where \u03c3 is a free parameter. We are particularly interested in the effect of varying \u03c3. Nystr\u00f6m subset selection is performed using S \" k-DPPpKq (Definition 3), and we plot the expected approximation factor Er}K\u00b4p KpSq}\u02das{OPT k (averaged over 1000 runs), where p KpSq is the Nystr\u00f6m approximation of K based on the subset S (see Section 1.2), }\u00a8}\u02dais the trace norm, and OPT k is the trace norm error of the best rank k approximation. Additional experiments, using greedy selection instead of a k-DPP, are in Appendix H. As discussed in Section 1.2, this task is equivalent to the CSSP task defined on the matrix A such that K \" A J A.\nThe aim of our empirical evaluation is to verify the following two claims motivated by our theory (and to illustrate that doing so is as easy as varying the RBF parameter \u03c3):\n1. When the spectral decay is sufficiently slow/smooth, the approximation factor for CSSP/Nystr\u00f6m is much better than suggested by previous worst-case bounds.\n2. A drop in spectrum around the kth eigenvalue results in a peak in the approximation factor near subset size k. Several drops result in the multiple-descent curve.\nIn Figure 3 (top), we plot the approximation factor against the subset size k (in the range of 1 to 40) for an artificial toy dataset and for two benchmark regression datasets from the Libsvm repository (bodyfat and eunite2001, see Chang & Lin, 2011). The toy dataset is constructed by scaling the eigenvalues of a random 50\u02c650 Gaussian matrix so that the spectrum is flat with a single drop at the 21-st eigenvalue. For each dataset, in Figure 3 (bottom), we also show the top 40 eigenvalues of the kernel K in decreasing order. For the toy dataset, to maintain full control over the spectrum we use the linear kernel xa i , a j y K \" a J i a j , and we show results for three different values of the condition number \u03ba of kernel K. For the benchmark datasets, we show results on the RBF kernel with three different values of the parameter \u03c3. Examining the toy dataset (Figure 3, left), it is apparent that a larger drop in spectrum leads to a sharper peak in the approximation factor as a function of the subset size k, whereas a flat spectrum results in the approximation factor being close to 1. A similar trend is observed for dataset bodyfat (Figure 3, center), where large parameter \u03c3 results in a peak that is aligned with a spectrum drop, while decreasing \u03c3 makes the spectrum flatter and the factor closer to 1. Finally, dataset eunite2001 (Figure 3, right) exhibits a full multiple-descent curve with up to three peaks for large values of \u03c3, and the peaks are once again aligned with the spectrum drops. Decreasing \u03c3 gradually eliminates the peaks, resulting in a uniformly small approximation factor. Thus, both of our theoretical claims can easily be verified on this dataset simply by adjusting the RBF parameter.\nWhile the right choice of the parameter \u03c3 ultimately depends on the downstream machine learning task, it has been observed that varying \u03c3 has a pronounced effect on the spectral properties of the kernel matrix, (see, e.g., Gittens & Mahoney, 2016;Lawlor et al., 2016;Wang et al., 2019). The main takeaway from our results here is that, depending on the structure of the problem, we may end up in the regime where the Nystr\u00f6m approximation factor exhibits a multiple-descent curve (e.g., due to a hierarchical nature of the data) or in the regime where it is relatively flat.", "publication_ref": ["b33", "b17", "b33", "b56"], "figure_ref": ["fig_2", "fig_2", "fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Conclusions and open problems", "text": "We derived new guarantees for the Column Subset Selection Problem (CSSP) and the Nystr\u00f6m method, going beyond worst-case analysis by exploiting the structural properties of a dataset, e.g., when the spectrum exhibits a known rate of decay. Our upper and lower bounds for the CSSP/Nystr\u00f6m approximation factor reveal an intruiguing phenomenon we call the multiple-descent curve: the approximation factor can exhibit a highly non-monotonic behavior as a function of k, with multiple peaks and valleys. These observations suggest a possible connection to the double descent curve exhibited by the generalization error of many machine learning models (see Appendix A for a more in depth discussion of similarities and differences between the two phenomena).\nOur analysis technique relies on converting an error bound from random-size DPPs to fixed-size k-DPPs, which results in an additional constant factor of p1`2 q 2 in Theorem 1. We put forward a conjecture which would eliminate this factor from Theorem 1 and is of independent interest to the study of elementary symmetric polynomials, a classical topic in combinatorics (Hardy et al., 1952).\nConjecture 1. The following function is convex with respect to k P rns for any \u03bb 1 , ..., \u03bb n \u0105 0: Deshpande et al. (2006) showed that if S \" k-DPPpA J Aq and \u03bb i are the eigenvalues of A J A, then ErEr A pSqs \" f pkq. If f pkq is convex then Jensen's inequality implies:\nf pkq \" pk`1q \u0159 S:|S|\"k`1 \u015b iPS \u03bb i \u0159 S:|S|\"k \u015b iPS \u03bb i .\nErEr A pSqs \u010f ErEr A pS 1 qs for S 1 \" DPPp 1 \u03b1 k A J Aq,\nwhere \u03b1 k is chosen so that Er|S 1 |s \" k. This would allow us to use the bound from Lemma 1 directly on a k-DPP without relying on the concentration argument of Lemma 2, thereby improving the bounds in Theorems 1 and 2.", "publication_ref": ["b38", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "A Additional related works", "text": "The Column Subset Selection Problem is one of the most classical tasks in matrix approximation (Boutsidis et al., 2008). The original version of the problem compares the projection error of a subset of size k to the best rank k approximation error. The techniques used for finding good subsets have included many randomized methods (Deshpande et al., 2006;Boutsidis et al., 2008;Belhadji et al., 2018;Boutsidis & Woodruff, 2014), as well as deterministic methods (Gu & Eisenstat, 1996). Variants of these algorithms have also been extended to more general losses (Chierichetti et al., 2017;Khanna et al., 2017;Elenberg et al., 2018). Later on, most works have relaxed the problem formulation by allowing the number of selected columns |S| to exceed the rank k. These approaches include deterministic sparsification based algorithms (Boutsidis et al., 2011), greedy selection (e.g., Altschuler et al., 2016) and randomized methods (e.g., Drineas et al., 2008;Guruswami & Sinop, 2012;Paul et al., 2015). Note that we study the original version of the CSSP (i.e., without the relaxation), where the number of columns |S| must be equal to the rank k.\nThe Nystr\u00f6m method has been given significant attention independently of the CSSP. The guarantees most comparable to our setting are due to Belabbas & Wolfe (2009), who show the approximation factor k`1 for the trace norm error. Many recent works allow the subset size |S| to exceed the target rank k, which enables the use of i.i.d. sampling techniques such as leverage scores (Gittens & Mahoney, 2016) and ridge leverage scores (Alaoui & Mahoney, 2015;Musco & Musco, 2017). In addition to the trace norm error, these works consider other types of guarantees, e.g., based on spectral and Frobenius norms, which are not as readily comparable to the CSSP error bounds.\nThe double descent curve was introduced by Belkin et al. (2019a) to explain the remarkable success of machine learning models which generalize well despite having more parameters than training data. This research has been primarily motivated by the success of deep neural networks, but double descent has also been observed in linear regression (Belkin et al., 2019b;Bartlett et al., 2019;Derezi\u0144ski et al., 2019b) and other learning models. Double descent is typically presented by plotting the absolute generalization error as a function of the number of parameters used in the learning model, although Poggio et al. (2019) and Liao et al. (2020) showed that the behavior of generalization error is merely an artifact of the phase transitions in the spectral properties of random matrices. Importantly, although the descent curves we obtain are reminiscent of the above works, our setting is different in that it is a deterministic combinatorial optimization problem for relative error. In particular, Corollary 1 shows that our multiple-descent curve can occur as a purely deterministic property of the optimal CSSP solution. Despite the differences, there are certain similarities between the two settings, namely (a) the notion of stable rank we use matches the one used by Bartlett et al. (2019), (b) the peaks in both the settings are closely aligned -these peaks coincide with the size k crossing the corresponding sharp drops in the respective spectra, (c) the analysis of bias of the minimum norm solution for double descent for linear regression under DPP sampling obtained by Derezi\u0144ski et al. (2019b) leads to expressions very similar to ours for the CSSP error for DPP sampling.\nDeterminantal point processes have been shown to provide near-optimal guarantees not only for the CSSP but also other tasks in numerical linear algebra, such as least squares regression (e.g., Avron & Boutsidis, 2013;Derezi\u0144ski & Warmuth, 2018;Derezi\u0144ski et al., 2019a). They are also used in recommender systems, stochastic optimization and other tasks in machine learning (for a review, see Kulesza & Taskar, 2012). Efficient algorithms for sampling from these distributions have been proposed both in the CSSP setting (i.e., given matrix A; see, e.g., Deshpande & Rademacher, 2010;Derezi\u0144ski, 2019) and in the Nystr\u00f6m setting (i.e., given kernel K; see, e.g., Anari et al., 2016;Derezi\u0144ski et al., 2019). The term \"cardinality constrained DPP\" (also known as a \"k-DPP\" or \"volume sampling\") was introduced by Kulesza & Taskar (2011) to differentiate from standard DPPs which have random cardinality. Our proofs rely in part on converting DPP bounds to k-DPP bounds via a refinement of the concentration of measure argument used by .\nBeyond worst-case analysis of algorithms is crucial to understanding the often-noticed gap between practical performance and theoretical guarantees of these algorithms. However, there have been limited number of works in machine learning that undertake finer-grained studies for beyond worst-case analyses. We refer to Roughgarden (2019) for a recent survey of such studies for a few problems in machine learning. Mahoney (2012) takes an alternative view and studies implicit statistical properties of worst case algorithms.", "publication_ref": ["b12", "b28", "b12", "b8", "b11", "b35", "b18", "b40", "b31", "b13", "b1", "b30", "b36", "b51", "b7", "b33", "b0", "b49", "b9", "b10", "b6", "b25", "b52", "b46", "b6", "b25", "b3", "b22", "b24", "b43", "b27", "b20", "b2", "b23", "b42", "b54", "b48"], "figure_ref": [], "table_ref": []}, {"heading": "B Determinantal point processes", "text": "Since our main results rely on randomized subset selection via determinantal point processes (DPPs), we provide a brief overview of the relevant aspects of this class of distributions. First introduced by Macchi (1975), a determinantal point process is a probability distribution over subsets S \u010e rns, where we use rns to denote the set t1, ..., nu. The relative probability of a subset being drawn is governed by a positive semidefinite (p.s.d.) matrix K P R n\u02c6n , as stated in the definition below, where we use K S,S to denote the |S|\u02c6|S| submatrix of K with rows and columns indexed by S. Definition 3. For an n\u02c6n p.s.d. matrix K, define S \" DPPpKq as a distribution over all subsets S \u010e rns so that PrpSq \" detpK S,S q detpI`Kq .\nA restriction to subsets of size k is denoted as k-DPPpKq.\nDPPs can be used to introduce diversity in the selected set or to model the preference for selecting dissimilar items, where the similarity is stated by the kernel matrix K. DPPs are commonly used in many machine learning applications where these properties are desired, e.g., recommender systems (Warlop et al., 2019), model interpretation (Kim et al., 2016), text and video summarization (Gong et al., 2014), and others (Kulesza & Taskar, 2012). They have also played an important role in randomized numerical linear algebra .\nGiven a p.s.d. matrix K P R n\u02c6n with eigenvalues \u03bb 1 , ... \u03bb n , the size of the set S \" DPPpKq is distributed as a Poisson binomial random variable, namely, the number of successes in n Bernoulli random trials where the probability of success in the ith trial is given by \u03bbi \u03bbi`1 . This leads to a simple expression for the expected subset size:\nEr|S|s \" \u00ff i \u03bb i \u03bb i`1 \" trpKpI`Kq\u00b41q. (2\n)\nNote that if S \" DPPp 1 \u03b1 Kq, where \u03b1 \u0105 0, then PrpSq is proportional to \u03b1\u00b4| S| detpK S,S q, so rescaling the kernel by a scalar only affects the distribution of the subset sizes, giving us a way to set the expected size to a desired value (larger \u03b1 means smaller expected size). Nevertheless, it is still often preferrable to restrict the size of S to a fixed k, obtaining a k-DPPpKq (Kulesza & Taskar, 2011).\nBoth DPPs and k-DPPs can be sampled efficiently, with some of the first algorithms provided by Hough et al. (2006), Deshpande & Rademacher (2010), Kulesza & Taskar (2011) and others. These approaches rely on an eigendecomposition of the kernel K, at the cost of Opn 3 q. When K \" A J A, as in the CSSP, and the dimensions satisfy m ! n, then this can be improved to Opnm 2 q. More recently, algorithms that avoid computing the eigendecomposition have been proposed (Derezi\u0144ski, 2019;Derezi\u0144ski et al., 2019;Calandriello et al., 2020;Anari et al., 2016), resulting in running times of r Opnq when given matrix K and r Opnmq for matrix A, assuming small desired subset size. See Gautier et al. (2019) for an efficient Python implementation of DPP sampling.\nThe key property of DPPs that enables our analysis is a formula for the expected value of the random matrix that is the orthogonal projection onto the subspace spanned by vectors selected by DPPpA J Aq. In the special case when A is a square full rank matrix, the following result can be derived as a corollary of Theorem 1 by Mutny et al. (2020), and a variant for DPPs over continuous domains can be found as Lemma 8 of Derezi\u0144ski et al. (2019b). For completeness, we also provide a proof in Appendix C.\nLemma 5. For any A and S \u010e rns, let P S be the projection onto the spanta i : i P Su. If S \" DPPpA J Aq, then\nErP S s \" ApI`A J Aq\u00b41A J .\nLemma 5 implies a simple closed form expression for the expected error in the CSSP. Here, we use a rescaling parameter \u03b1 \u0105 0 for controlling the distribution of the subset sizes. Note that it is crucial that we are using a DPP with random subset size, because the corresponding expression for the expected error of the fixed size k-DPP is combinatorial, and therefore much harder to work with. Lemma 6. For any \u03b1 \u0105 0, if S \" DPPp 1 \u03b1 A J Aq, then\nE \" Er A pSq \u2030 \" tr`AA J pI`1 \u03b1 AA J q\u00b41\u02d8\" Er|S|s\u00a8\u03b1.\nProof. Using Lemma 5, the expected loss is given by:\nE \" Er A pSq \u2030 \" E \" }pI\u00b4P S qA} 2 F \u2030 \" trpAA J ErI\u00b4P S sq \" tr`AA J pI\u00b41 \u03b1 ApI`1 \u03b1 A J Aq\u00b41A J qp\u02daq \" tr`AA J pI`1 \u03b1 AA J q\u00b41\u02d8,\nwhere p\u02daq follows from the matrix identity pI`AA J q\u00b41 \" I\u00b4ApI`A J Aq\u00b41A J .", "publication_ref": ["b47", "b57", "b41", "b34", "b43", "b42", "b39", "b27", "b42", "b20", "b23", "b15", "b2", "b32", "b50", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "C Proof of Lemma 5", "text": "We will use the following standard determinantal summation identity (see Theorem 2.1 in Kulesza & Taskar, 2012) which corresponds to computing the normalization constant detpI`Kq for a DPP.\nLemma 7. For any n\u02c6n matrix K, we have\ndetpI`Kq \" \u00ff S\u010erns detpK S,S q.\nWe now proceed with the proof of Lemma 5 (restated below for convenience).\nLemma' 5. For any A and S \u010e rns, let P S denote the projection onto the spanta i : i P Su. If S \" DPPpA J Aq, then\nErP S s \" ApI`A J Aq\u00b41A J .\nProof. Fix m as the column dimension of A and let A S denote the submatrix of A consisting of the columns indexed by S. We have P S \" A S pK S,S q : A S , where : denotes the Moore-Penrose inverse and K \" A J A. Let v P R m be an arbitrary vector. When K S,S is invertible, then a standard determinantal identity states that:\ndetpK S,S qv J P S v \" detpK S,S qv J A S K\u00b41 S,S A J S v \" detpK S,S`A J S vv J A S q\u00b4detpK S,S q.\nWhen K S,S is not invertible then detpK S,S q \" detpK S,S`A J S vv J A S q \" 0, because the rank of K S,S\u00c0 J S vv J A S \" A J S pI`vv J qA S cannot be higher than the rank of K S,S \" A J S A S . Thus, detpI`Kqv J ErP S sv \"\n\u00ff S\u010erns: detpK S,S q\u01050 detpK S,S qv J A S K\u00b41 S,S A J S v \" \u00ff S\u010erns detpK S,S`A J S vv J A S q\u00b4detpK S,S q \" \u00ff S\u010erns det`rK`A J vv J As S,S\u02d8\u00b4\u00ff S\u010erns detpK S,S q p\u02daq \" detpI`K`A J vv J Aq\u00b4detpI`Kq \" detpI`Kqv J ApI`Kq\u00b41A J v,\nwhere p\u02daq involves two applications of Lemma 7. Since the above calculation holds for arbitrary vector v, the claim follows.", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "D Proofs omitted from Section 2", "text": "Lemma' 1. For any A, 0 \u010f \u0103 1 and s \u0103 k \u0103 t s , where t s \" s`sr s pAq, suppose that S \" DPPp 1 \u03b1 A J Aq for \u03b1 \" \u03b3spkqOPT k p1\u00b4 qpk\u00b4sq and \u03b3 s pkq \" b 1`2 pk\u00b4sq ts\u00b4k . Then:\nE \" Er A pSq \u2030 OPT k \u010f \u03a6 s pkq 1\u00b4\nand Er|S|s \u010f k\u00b4 k\u00b4s \u03b3 s pkq , where \u03a6 s pkq \"`1`s k\u00b4s\u02d8\u03b3 s pkq.\nProof. Let \u03bb 1 \u011b \u03bb 2 \u011b ... be the eigenvalues of A J A. Note that scaling the matrix A by any constant c and scaling \u03b1 by c 2 preserves the distribution of S as well as the approximation ratio, so without loss of generality, assume that \u03bb s`1 \" 1. Furthermore, using the shorthands l \" k\u00b4s and r \" sr s pAq, we have t s\u00b4k \" r\u00b4l and so \u03b3 s pkq \" b r`l r\u00b4l . We now lower bound the optimum as follows:\nOPT k \" \u00ff j\u0105k \u03bb j \" sr s pAq\u00b4k \u00ff j\"s`1 \u03bb j \u011b r\u00b4l.\nWe will next define an alternate sequence of eigenvalues which is in some sense \"worst-case\", by shifting the spectral mass away from the tail. Let \u03bb 1 s`1 \" ... \" \u03bb 1 k \" 1, and for i \u0105 k set \u03bb 1 i \" \u03b2\u03bb i , where \u03b2 \" r\u00b4l OPT k \u010f 1. Additionally, define:\n\u03b1 1 \" \u03b2\u03b1 \" \u03b3 s pkqpr\u00b4lq p1\u00b4 ql \" ? r 2\u00b4l2 p1\u00b4 ql , \u03b1 2 \" p1\u00b4 q ? r`l`?r\u00b4l 2 ? r`l \u03b1 1 \" p ? r`l`?r\u00b4lq ? r\u00b4l r`l\u00b4pr\u00b4lq \" ? r\u00b4l ? r`l\u00b4?r\u00b4l .(3)\nand note that \u03b1 2 \u010f \u03b1 1 \u010f \u03b1. Moreover, for s`1 \u010f i \u010f k, we let \u03b1 1 i \" \u03b1 2 , while for i \u0105 k we set \u03b1 1 i \" \u03b1 1 . We proceed to bound the expected subset size Er|S|s by converting all the eigenvalues from \u03bb i to \u03bb 1 i and \u03b1 to \u03b1 1 i , which will allow us to easily bound the entire expression:\nEr|S|s \" \u00ff i \u03bb i \u03bb i`\u03b1 \u010f s`k \u00ff i\"s`1 \u03bb i \u03bb i`\u03b1 1 i`\u00ff i\u0105k \u03b2\u03bb i \u03b2\u03bb i`\u03b2 \u03b1 \u010f s`k \u00ff i\"s`1 \u03bb 1 i \u03bb 1 i`\u03b1 2`\u00ff i\u0105k \u03bb 1 i \u03bb 1 i`\u03b1 1 .(4)\nWe bound each of the two sums separately starting with the first one:\nk \u00ff i\"s`1 \u03bb 1 i \u03bb 1 i`\u03b1 2 \" l 1`\u03b1 2 \" l\u00b4l 1`1 \u03b1 2 \" l\u00b4l 1`? r`l\u00b4?r\u00b4l ? r\u00b4l \" l\u00b4l ? r\u00b4l ? r`l .(5)\nTo bound the second sum, we use the fact that \u0159 i\u0105k \u03bb 1 i \" \u03b2 OPT k \" r\u00b4l, and obtain:\n\u00ff i\u0105k \u03bb 1 i \u03bb 1 i`\u03b1 1 \u010f 1 \u03b1 1 \u00ff i\u0105k \u03bb 1 i \" r\u00b4l \u03b1 1 \" p1\u00b4 q l ? r\u00b4l ? r`l . (6\n)\nCombining the two sums, we conclude that Er|S|s \u010f s`l\u00b4 l b r\u00b4l r`l \" k\u00b4 l \u03b3spkq . Finally, Lemma 6 yields:\nE \" Er A pSq \u2030 OPT k \" Er|S|s\u00a8\u03b1 OPT k \u010f k k\u00b4s \u03b3 s pkq 1\u00b4 \" \u03a6 s pkq 1\u00b4 ,\nwhich concludes the proof.\nLemma' 2. Let S be sampled as in Lemma 1 with \u010f 1 2 . If s`7 4 ln 2 1 \u010f k \u010f t s\u00b41 , then Prp|S| \u0105 kq \u010f .\nProof. Let p i \"\n\u03bb 1 i \u03bb 1 i`\u03b1 1 i\nbe the Bernoulli probabilities for b i \" Bernoullipp i q and X \" \u0159 i\u0105s b i , where \u03bb 1 i and \u03b1 1 i are as defined in the proof of Lemma 1. Note that |S| is distributed as a Poisson binomial random variable such that the success probability associated with the ith eigenvalue is upper-bounded by p i for each i \u0105 s. It follows that Prp|S| \u0105 kq \u010f PrpX \u0105 lq, where l \" k\u00b4s. Moreover, letting r \" sr s pAq, in the proof of Lemma 1 we showed that:\nk\u00b4ErXs \u011b l ? r\u00b4l ?\nr`l , and furthermore, using the derivations in ( 5) and ( 6) together with the formula Varrb i s \" p i p1\u00b4p i q, we obtain that:\nVarrXs \u010f k \u00ff i\"s`1 p1\u00b4p i q`\u00ff i\u0105k p i \u010f l ? r\u00b4l ? r`l`p 1\u00b4 q l ? r\u00b4l ? r`l \" p2\u00b4 q l ? r\u00b4l ? r`l .\nUsing Theorem 2.6 from Chung & Lu (2006) with \u03bb \" l ? r\u00b4l ? r`l , we have:\nPrp|S| \u0105 kq \u010f PrpX \u0105 lq \u010f PrpX \u0105 ErXs`\u03bbq \u010f exp\u00b4\u00b4\u03bb 2 2pVarrXs`\u03bb{3q\u010f exp\u00b4\u00b4\u03bb 2 2p 2\u00b4 \u03bb`\u03bb{3q\u00af\u010f expp\u00b4 \u03bb{4q \" exp\u00b4\u00b4 2 l ? r\u00b4l 4 ? r`l\u00af.\nNote that since 7 \u010f l \u010f r\u00b41, we have l\n? r\u00b4l ? r`l \u011b l ? 2l`1 \u011b 7 16 ?\nl, so by simple algebra it follows that for l \u011b 7 4 ln 2 1 , we have l ? r\u00b4l ? r`l \u011b 4\n2 ln 1 and therefore Prp|S| \u0105 kq \u010f .\nLemma' 3. For any A P R m\u02c6n , k P rns and \u03b1 \u0105 0, if S \" DPPp 1 \u03b1 A J Aq and S 1 \" k-DPPpA J Aq, then\nE \" Er A pS 1 q \u2030 \u010f E \" Er A pSq | |S| \u010f k \u2030 .\nProof. Let \u03bb 1 \u011b \u03bb 2 \u011b ... denote the eigenvalues of A J A and let e k be the kth elementary symmetric polynomial of A:\ne k \" \u00ff T :|T |\"k detpA J T A T q \" \u00ff T :|T |\"k \u017a iPT \u03bb i .\nAlso let\u0113 k \" e k {`n k\u02d8d enote the kth elementary symmetric mean. Newton's inequalities imply that:\n1 \u011b\u0113 k\u00b41\u0113k`1 e 2 k \" e k\u00b41 e k`1 e 2 k`n kn k\u00b41\u02d8`n kn k`1\u02d8\" e k\u00b41 e k`1 e 2 k n`1\u00b4k k k`1 n\u00b4k .\nThe results of Deshpande et al. (2006) and Guruswami & Sinop (2012) establish that ErEr A pSq | |S| \" ks \" pk`1q e k`1 e k , so it follows that:\nErEr A pSq | |S| \" ks ErEr A pSq | |S| \" k\u00b41s \" k`1 k e k`1 e k\u00b41 e 2 k \u010f n\u00b4k n`1\u00b4k \u010f 1. (7\n)\nFinally, note that ErEr A pSq | |S| \u010f ks is a weighted average of components ErEr A pSq | |S| \" ss for s P rks, and ( 7) implies that the smallest of those components is associated with s \" k. Since the weighted average is lower bounded by the smallest component, this completes the proof.", "publication_ref": ["b28", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "E Proof of Theorem 2", "text": "Before showing Theorem 2, we give an additional lemma which covers the corner case of the theorem when k is close to n.\nLemma 8. Given A P R m\u02c6n and s \u0103 k \u0103 n, let \u03bb 1 \u011b ... \u011b \u03bb n \u0105 0 be the eigenvalues of A J A. If S \" k-DPPpA J Aq and we let b \" mintk\u00b4s, n\u00b4ku, then for any 0 \u0103 \u010f 1 2 we have\nErEr A pSqs OPT k \u010f`1\u00b4e\u00b4 2 b 10\u02d8\u00b41 p1\u00b4 q\u00b41\u03a8 s pkq,\nwhere \u03a8 s pkq \" \u03bbs`1 \u03bbn`1`s k\u00b4s\u02d8.\nProof. Let \u03b1 \" \u03bbs`1 p1\u00b4 q\u03bbn OPT k k\u00b4s . Note that OPT k \" \u0159 i\u0105k \u03bb i \u011b pn\u00b4kq\u03bb n . Define b i \" Bernoullip \u03bbi \u03bbi`\u03b1 q and let X \" \u0159 i\u0105s b i . We have: From this, it follows that:\nErXs \" \u00ff i\u0105s \u03bb i \u03bb i`\u03b1 \u010f pn\u00b4sq\u03bb s`1 \u03bb s`1`\u03bb s`1 \u03bbn pn\u00b4kq\u03bbn p1\u00b4 qpk\u00b4sq \" 1 1 n\u00b4s`1 p1\u00b4 qpk\u00b4sq n\u00b4k n\u00b4s \" 1 1 n\u00b4s`1 p1\u00b4 qpk\u00b4sq p1\u00b4k\u00b4s n\u00b4s q \" 1 1 p1\u00b4 qpk\u00b4sq\u00b4 1\u00b4 1 n\u00b4s \" 1\u00b4 1 k\u00b4s\u00b4 n\u00b4s . Let S 1 \" DPPp 1 \u03b1 A J Aq.\nErEr A pS 1 qs OPT k \" Er|S|s\u00a8\u03b1 OPT k \u010f p1\u00b4 q\u00b41 k k\u00b4s \u03bb s`1 \u03bb n \" p1\u00b4 q\u00b41\u00b41`s k\u00b4s\u00af\u03bb s`1 \u03bb n .\nWe now give an upper bound on Prp|S 1 | \u0105 kq by considering two cases. Case 1: k\u00b4s \u010f n\u00b4k. Then, using \u03bb \" pk\u00b4sq{2, we have pk\u00b4sq\u00b4ErXs \u011b \u03bb, so using Theorem 2.4 from Chung & Lu (2006), we get:\nPrp|S 1 | \u0105 kq \u010f PrpX \u0105 k\u00b4sq \u010f PrpX \u0105 ErXs`\u03bbq \u010f e\u00b4\u03bb 2 2pk\u00b4sq \" e\u00b4 2 pk\u00b4sq{8 .\nCase 2: k\u00b4s \u0105 n\u00b4k. Then, using Theorem 2.4 from Chung & Lu (2006) with \u03bb \" k\u00b4Er|S 1 |s \" pn\u00b4kq 2`\u2206 , where \u2206 \u0105 0, we get:\nPrp|S 1 | \u0105 kq \" Prpn\u00b4|S 1 | \u0103 n\u00b4kq \u010f exp\u00b4\u00b4\u03bb 2 2 Ern\u00b4|S 1 |s\" exp\u00b4\u00b4\u03bb 2 2 pn\u00b4kq`\u2206 n\u00b4k` 2 pn\u00b4kq`\u2206\u010f exp\u00b4\u00b4\u03bb 2 2 pn\u00b4kq n\u00b4k` 2 pn\u00b4kq\" exp\u00b4\u00b4 2 pn\u00b4kq 8p1` {2qp\u02daq \u010f exp\u00b4\u00b4 2 pn\u00b4kq 10\u00af,\nwhere in p\u02daq we used the fact that P p0, 1 2 q. Now, the result follows easily by invoking Lemma 3:\nE \" Er A pSq \u2030 \u010f E \" Er A pS 1 q | |S 1 | \u010f k \u2030 \u010f E \" Er A pS 1 q \u2030 Prp|S 1 | \u010f kq \u010f`1\u00b4e\u00b4 2 b 10\u02d8\u00b41 p1\u00b4 q\u00b41 \u03bb s`1 \u03bb n\u00b41`s k\u00b4s\u00af\u00a8O PT k ,\nwhich completes the proof.\nNote that since b \u011b 1, setting \" 1 2 in Lemma 8 yields the following simpler (but usually much weaker) bound:\nErEr A pSqs OPT k \u010f 2`1\u00b4e\u00b41 40\u02d8\u00b41 \u03a8 s pkq \u010f 82 \u03a8 s pkq.\nTheorem' 2. Let \u03bb 1 \u011b \u03bb 2 \u011b ... be the eigenvalues of A J A. There is an absolute constant c such that for any 0 \u0103 c 1 \u010f c 2 , with \u03b3 \" c 2 {c 1 , if:\n1. (polynomial spectral decay) c 1 i\u00b4p \u010f \u03bb i \u010f c 2 i\u00b4p @ i , with p \u0105 1, then S \" k-DPPpA J Aq satisfies ErEr A pSqs OPT k \u010f c\u03b3p.\n2. (exponential spectral decay) c 1 p1\u00b4\u03b4q i \u010f \u03bb i \u010f c 2 p1\u00b4\u03b4q i @ i , with \u03b4 P p0, 1q, then S \" k-DPPpA J Aq satisfies\nErEr A pSqs OPT k \u010f c\u03b3p1`\u03b4kq.\nProof.\n(1) Polynomial decay. We provide the proof by splitting it into two cases. Case 1(a):`k`1 n\u02d8p\u00b41 \u010f 1\nWe can use upper and lower integrals to bound the sum\n\u0159 i\u011bs 1 i p as: \u017c x\u011bps`1q 1 i p dx \u010f \u00ff i\u011bs 1 i p \u010f \u017c x\u011bs 1 i p dx \u00f9\u00f1 n \u00ff i\"s`1 1 i p \u011b ps`2q 1\u00b4p p\u00b41\u00b4p n`1q 1\u00b4p p\u00b41 .\nWe lower bound the stable rank for s \u010f k using the upper/lower bounds on the eigenvalues and the condition for Case 1(a): sr s pAq \"\n\u0159 n i\"s`1 \u03bb i \u03bb s`1 \u011b c 1 c 2\u02c6p s`2q 1\u00b4p pp\u00b41qps`1q\u00b4p\u00b4p n`1q 1\u00b4p pp\u00b41qps`1q\u00b4p\" 1 \u03b3\u02c6s`2 p\u00b41\u00b41\u00b41 s`2\u00afp\u00b4s`1 p\u00b41\u00b4s`1 n`1\u00afp\u00b41\u011b 1 \u03b3\u02c6s`2 p\u00b41\u00b41\u00b4s`1 p\u00b41\u00a81 2\u02d9\" 1 2\u03b3 s`1 p\u00b41\u00b41 \u03b3 .\nFurther using u \" k\u00b4s, we can call upon Theorem 1 to get,\n\u03a6 s pkq \u010f k u c 1`2 u sr s\u00b4u \u010f k u`k 1 2\u03b3 s`1 p\u00b41\u00b4\u03b3\u00b41\u00b4u \" k u`p 2p\u00b42qk \u03b3\u00b41ps`1\u00b42p`2q\u00b4p2p\u00b42qu \u010f k u`p 2p\u00b42`\u03b3\u00b41qk \u03b3\u00b41pk`3\u00b42pq\u00b4p2p\u00b42`\u03b3\u00b41qu\nOptimizing over u, we see that the minimum is reached for u \"\u00fb \"\nk`3\u00b42p 2\u03b3p2p\u00b42`\u03b3\u00b41q which achieves the value 4p\u03b3p2p\u00b42q`1qk k`3\u00b42p\nwhich is upper bounded by 12\u03b3pk pk\u00b42pq . We assume k \u011b\u00fb \u0105 60p \u0105 60. If not, Deshpande et al. (2006) ensure an upper bound of pk`1q \u010f 60p`1 \u0103 61p. With p \u0103 k{60, we get:\n12\u03b3pk k\u00b42p \u010f 12\u03b3pk k\u00b4k{30 \" 12\u03b3p 1\u00b41{30 \u010f 360 29 \u03b3p.\nSince we assumed that\u00fb \u0105 60, then k\u00b4s \u0105 7 4 ln 2 1 for \" 0.5 which means p1`2 q 2 \u010f 4, which makes the approximation ratio upper bounded by 1440 29 \u03b3p. The overall bound thus becomes 61\u03b3p.\nCase 1(b):`k`1 n\u02d8p\u00b41 \u0105 1 2\nFrom Lemma 8, we know that the approximation ratio is upper bounded by constant factor times \u03a8 s pkq \" \u03bbs`1 \u03bbn k k\u00b4s . Consider,\n\u03a8 s pkq \" \u03bb s`1 \u03bb n k k\u00b4s \u010f \u03b3 n p ps`1q p k k\u00b4s \" \u03b3\u02c6n k`1\u02d9p\u00b41 k`1 n pk`1q p ps`1q p k k\u00b4s \u010f 2\u03b3\u02c6k`1 s`1\u02d9p k k\u00b4s ,\nwhich holds true for all s \u010f k, and is optimized for s \"\u015d \" pk\u00b41 p`1 . We get that the approximation ratio is bounded as:\n\u03a8 s pkq \u010f \u03b3 kpp`1q k`1\u02c6p`1 p\u02d9p \u010f e\u03b3pp`1q \u010f 2e\u03b3p.\nCombining in the factor based on in Lemma 8, we get an upper bound of 164e\u03b3p that is larger than the bound obtained in the case 1(a) above and hence covers all the subcases.\n(2) Exponential decay.\nWe first lower bound the stable rank of A of order s:\nsr s pAq \" \u00ff j\u0105s \u03bb j {\u03bb s`1 \u011b c 1 p1\u00b4p1\u00b4\u03b4q n\u00b4s q{\u03b4 c 2 \" 1\u00b4p1\u00b4\u03b4q n\u00b4s \u03b3\u03b4 .\nWe present the proof by considering two subcases separately : when k \u010f n\u00b4l n 2 \u03b4 and k \u0105 n\u00b4l n 2 \u03b4 . Case 2(a): k \u010f n\u00b4l n 2 \u03b4 . From the assumption, letting s \u010f k we have\ns \u010f n\u00b4l n 2 \u03b4 \u00f9\u00f1 s \u010f n\u00b4l n 2 ln 1 1\u00b4\u03b4 \u00f9\u00f1 pn\u00b4sq ln 1 1\u00b4\u03b4 \u011b ln 2 \u00f0\u00f1 1\u00b4p1\u00b4\u03b4q n\u00b4s \u011b 1 2 \u00f9\u00f1 sr s pKq \u011b 1 2\u03b3\u03b4 ,\nwhere the second inequality follows because x 1`x \u010f lnp1`xq with x \" \u03b4{p1\u00b4\u03b4q. We will use u \" k\u00b4s. From Theorem 1, using sr s \u011b 1 2\u03b3\u03b4 we have the following upper bound:\n\u03a6 s pkq \u010f k u\u02c61`2 \u03b3\u03b4u 1\u00b42\u03b3\u03b4u\u02d9\" k u\u00a81 1\u00b42\u03b3\u03b4u .\nRHS is minimized for\u00fb \" 1 4\u03b3\u03b4 . We let \" 0.5 and assume that\u00fb \u011b 60 which is bigger than 7 4 ln 2 1 . If not, then \u03b4 \u011b 4 60\u03b3 \u0105 1 \u03b3 and the worst-case bound of Deshpande et al. (2006) ensures that the approximation factor is no more than k`1 \u010f \u03b3p1`1 \u03b3 kq \u010f \u03b3p1`\u03b4kq. By a similar argument we can assume that k \u011b 60.\nIf k \u010f\u00fb, in this case we can set s \" 0, i.e., u \" k, obtaining \u03a6 s pkq \u010f 1 1\u00b42\u03b3\u03b4k \u010f 2. And so the approximation ratio is bounded by p1`2 q 2\u00a82 \u010f 8. On the other hand, if k \u0105\u00fb, we can set u \"\u00fb, which implies \u03a6 s pkq \u010f 8\u03b3\u03b4k, and so the approximation ratio is bounded by 32\u03b3\u03b4k. The overall bound is thus 61\u03b3p1`\u03b4kq covering all possible subcases.\nCase 2(b): k \u0105 n\u00b4l n 2 \u03b4 . We make use of Lemma 8 for the case when k is close to n. The approximation guarantee uses:\n\u03a8 s pkq \" \u03bb s`1 \u03bb n k k\u00b4s ,\nwhere s \u0103 k. For our bound, we choose s \" tk\u00b4l n 2 \u03b4 u. This implies that n\u00b4s \u0103 2 ln 2 \u03b4`1 \" \u03b4`ln 4 \u03b4 . It follows that\n\u03bb s`1 \u03bb n \u010f \u03b3 p1\u00b4\u03b4q n\u00b4s \u010f \u03b3 p1\u00b4\u03b4q p\u03b4`ln 4q{\u03b4 \" \u03b3 \" p1\u00b4\u03b4q\u00b41 \u03b4 \u0131 \u03b4`ln 4 \u010f \u03b3e \u03b4`ln 4 1\u00b4\u03b4 .\nIf \u03b4 \u011b 1 20 , then the worst-case result of Deshpande et al. (2006) suffices to show that the approximation ratio is bounded by k`1 \u010f 20p1`\u03b4kq, so assume that \u03b4 \u0103 1 20 . Then we have e \u03b4`ln 4 1\u00b4\u03b4 \u0103 5. Combining this with the fact that k k\u00b4s \u010f \u03b4k ln 2 , we obtain: \u03a6 s pkq \u010f 5\u03b3\u03b4k ln 2 .\nCombining with factor based on in Lemma 8, we get 82\u00a85 \u03b3\u03b4k ln 2 . Thus, the bound of 82\u00a85 ln 2 \u03b3p1`\u03b4kq holds in all cases, completing the proof.", "publication_ref": ["b28", "b28", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "H Empirical evaluation with greedy subset selection", "text": "In this section, we provide a more detailed empirical evaluation to complement what we presented in Section 4. Our aim here is to demonstrate that our improved analysis of the CSSP/Nystr\u00f6m approximation factor can be useful in understanding the performance of not only the k-DPP method, but also of greedy subset selection. Note that our theory does not strictly apply to the greedy algorithm. Nevertheless, we show that, similar to the k-DPP method, greedy selection also exhibits the improved guarantees and the multiple-descent curve predicted by our analysis.\nThe most standard version of the greedy algorithm (see, e.g., Altschuler et al., 2016) starts with an empty set and then iteratively adds columns that minimize the approximation error at every step, until we reach a set of size k. The pseudo-code is given below.\nGreedy subset selection algorithm for CSSP/Nystr\u00f6m Input: k P rns and an m\u02c6n matrix A (CSSP), or an n\u02c6n p.s.d. matrix K \" A J A (Nystr\u00f6m) S \u00d0 H for i \" 1 to k do Pick i P rnszS that minimizes Er A pS Y tiuq, or equivalently, }K\u00b4p KpS Y tiuq}S \u00d0 S Y tiu end for return S\nIn our empirical evaluation we use the same experimental setup as in Section 4, by running greedy on a toy dataset with the linear kernel xa i , a j y K \" a J i a j that has one sharp spectrum drop (controlled by the condition number \u03ba), and two Libsvm datasets with the RBF kernel xa i , a j y K \" expp\u00b4}a i\u00b4aj } 2 {\u03c3 2 q for three values of the RBF parameter \u03c3. The main question motivating these experiments is: does the approximation factor of the greedy algorithm exhibit the multiple-descent curve that is predicted in our analysis, and are the peaks in this curve aligned with the sharp drops in the spectrum of the data?\nThe plots in Figure 4 confirm that the Nystr\u00f6m approximation factor of greedy subset selection exhibits similar peaks and valleys as those indicated by our theoretical and empirical analysis of the k-DPP method. This is most clearly observed for the toy dataset (Figure 4 left), where the peak grows with the condition number \u03ba, and for the bodyfat dataset (Figure 4 center), where the size of the peak is proportional to the RBF parameter \u03c3. Moreover, we observe that when the spectral decay is slow/smooth, which corresponds to smaller values of \u03c3, then the approximation factor of the greedy algorithm stays relatively close to 1. For the eunite2001 dataset (Figure 4 right), the behavior of the approximation factor is very non-linear, with several peaks occurring for large values of \u03c3. Interestingly, while the peaks do align with some of the drops in the spectrum, not all of the spectrum drops result in a peak for the greedy algorithm. This goes in line with our analysis, in the sense that a sharp drop in the spectrum following the kth eigenvalue is a necessary but not sufficient condition for the approximation factor of the optimal subset S of size k to exhibit a peak.\nOur empirical evaluation leads to an overall conclusion that the multiple-descent curve of the CSSP/Nystr\u00f6m approximation factor is a phenomenon exhibited by both randomized methods, such as the k-DPP, and deterministic algorithms, such as greedy subset selection. While the exact behavior of this curve is algorithmdependent, significant insight can be gained about it by studying the spectral properties of the data. Our results suggest that performing a theoretical analysis of the multiple-descent phenomenon for greedy methods is a promising direction for future work. Figure 4: Top plots show the Nystr\u00f6m approximation factor }K\u00b4p KpSq}\u02da{OPT k , where S is constructed using greedy subset selection, against the subset size k, for a toy dataset (\u03ba is the condition number) and two Libsvm datasets (\u03c3 is the RBF parameter). Bottom plots show the spectral decay for the top 40 eigenvalues of each kernel K, demonstrating how the peaks in the Nystr\u00f6m approximation factor align with the drops in the spectrum.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We would like to acknowledge DARPA, IARPA (contract W911NF20C0035), NSF, and ONR via its BRC on RandNLA for providing partial support of this work. Our conclusions do not necessarily reflect the position or the policy of our sponsors, and no official endorsement should be inferred.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F Proof of Lemma 4", "text": "Lemma' 4. Fix \u03b4 P p0, 1q and consider unit vectors a i,j P R m in general position, where i P rts, j P rl i s, such that \u0159 j a i,j \" 0 for each i, and for any i, j, i 1 , j 1 , if i \u2030 i 1 then a i,j is orthogonal to a i 1 ,j 1 . Also, let unit vectors tv i u iPrts be orthogonal to each other and to all a i,j . There are positive scalars \u03b1 i , \u03b2 i for i P rts such that matrix A with columns \u03b1 i a i,j`\u03b2i v i over all i and j satisfies:\nProof. Say p A i is the matrix obtained by stacking all the a i,j and let \u03bb i,1 \u011b \u03bb i,2 \u011b ... \u011b \u03bb i,li\u00b41 denote the non-zero eigenvalues of p A J i p A i . We write\u00e3 i,j \" \u03b1 i a i,j`\u03b2i v i and note that for each i, 1 li is an eigenvector of p A J i p A i with eigenvalue 0. Further, A J A is a block-diagonal matrix with blocks\nTherefore, the eigenvalues of\n, \u03b2 2 t l t , and so we can always choose the parameters so that \u03b1 i \" \u03b2 i \" \u03b1 i`1 for each i, ensuring that these eigenvalues are in decreasing order. Let us fix an arbitrary c P rts. From the above, it follows that for k c \"`\u0159 i\u010fc l i\u02d8\u00b41 we have:\nwhere we use \u03c6 c \" \u0159 i\u0105c trpB i q as a shorthand. Since the centroid of t\u00e3 c,1 , . . . ,\u00e3 c,lc u is \u03b2v c , we can writ\u1ebd a c,lc \" l c \u03b2v c\u00b4\u0159 j\u0103lc\u00e3 c,j . For selecting the set S \u0102 rns of size k c , since \u03b1 i \" \u03b1 i`1 , we can assume without loss of generality that S does not select any vectors\u00e3 i,j such that i \u0105 c and does not drop any such that i \u0103 c, and so for some j 1 P rl c s we let S j 1 be the index set such that P S j 1 is the projection onto the span of \u0164 i\u0103c \u0164 j t\u00e3 i,j u\u00afY t\u00e3 c,1 , . . . ,\u00e3 c,lc uzt\u00e3 c,j 1 u. We now lower bound the squared projection error of that set:\northogonal to the subspace spanned by S j 1 , so we can choose \u03b2 c small enough so that }v\u00b4P S j 1 v} 2 \u011b 1\u00b4\u03b4 2 for each j 1 P rl c s. Furthermore, we have\nwhich implies that Er A pS j 1 q \u011b p1\u00b4\u03b4ql c OPT kc . Note that all the conditions we required on \u03b1 i and \u03b2 i can be satisfied by a sufficiently quickly decreasing sequence \u03b1 1 \" \u03b2 1 \" \u03b1 2 \" \u03b2 2 \" ... \" \u03b1 t \" \u03b2 t \u0105 0, which completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "G Proof of Corollary 1", "text": "Corollary' 1. For t P N and \u03b4 P p0, 1q, there is a sequence k l 1 \u0103 k u 1 \u0103 k l 2 \u0103 k u 2 \u0103 ... \u0103 k l t \u0103 k u t and A P R m\u02c6n such that for any i P rts:\nProof. We will use Theorem 3 to construct the matrix A using the sequence we build below to make sure the upper and lower bounds are satisfied. Theorem 3 uses Lemma 4 to construct the matrix A which has a \"step\" eigenvalue profile i.e. there are multiple groups of eigenvalues and in each group the eigenvalue is constant (each group corresponds to a regular simplex, see Section 3). Below we consider a single such group that starts at s \" k u i and ends at w \" k u i`1 , and we let k \" k l i`1 , for any i P t0, . . . , t\u00b41u, with k u 0 \" 0. Theorem 1 implies that there is a set S with an upper bound on the approximation factor Er A pSq{OPT k of p1`2 q 2`1`s k\u00b4s\u02d8`1`k\u00b4s ts\u00b4k\u02d8. Consider the following three conditions to ensure that each of the three terms in the above approximation factor is less than p1`\u03b4 1 q where \u03b4 1 \" \u03b4{7:\n, where is chosen so as to satisfy the above condition.\n2. k \u011b s \u03b41`s`\u03c4 ensures that p1`s k\u00b4s q \u010f p1`\u03b4 1 q and that k\u00b4s \u011b \u03c4 . 3. w \u011b kp1`1 \u03b41 q`1.\nTo see the usefulness of condition 3, note that each group of vectors in column set of A constructed from Theorem 3 form a shifted regular simplex. A regular simplex has the smallest eigenvalue 0 and the rest of the eigenvalues are all pw\u00b4sq\u03b1 2 {pw\u00b4s\u00b41q, where \u03b1 is the length of each of the pw\u00b4sq vectors in the simplex. Thus, we can lower bound the stable rank of the shifted simplex as sr s pAq \u011b pw\u00b4sq\u03b1 2 pw\u00b4sq\u03b1 2 pw\u00b4s\u00b41q \" pw\u00b4s\u00b41q. From condition 3:\nThus if all the above three conditions are satisfied, the approximation ratio can be upper bounded by p1`\u03b4 1 q 3 \u010f p1`\u03b4q, since \u03b4 1 \" \u03b4{7.\nSimilarly for the lower bound, we will need condition 4 below.\n4. w \u011b 2s \u03b4`2 \u03b4 . Now, we apply Theorem 3 using k i \" w and k i\u00b41 \" s to get the following lower bound with \u03b4 2 \" \u03b4{2:\nwhere the last inequality follows from condition 4. Also, observe that we can replace conditions 3 and 4 with a single stronger condition: w \u011b kp1`7 \u03b4 q`1`2 \u03b4 .\nWe now iteratively construct the sequence that satisfies all of the above conditions:\n1. k u 0 \" 0 2. For 1 \u010f i \u010f t (a) k l i \"\ni \" rk l i p1`7{\u03b4q`2 \u03b4`1 s. We can now use Theorem 3 with subsequence tk u i u which also constructs the matrix A through Lemma 4, to ensure that the lower bound of p1`\u03b4qpk u i`1 q is satisfied for A for all i. We can also use Theorem 1 for the same matrix A and k \" k l i for any i to ensure that the upper bound of p1`\u03b4q is also satisfied for any i.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Fast randomized kernel ridge regression with statistical guarantees", "journal": "", "year": "2015-12", "authors": "A E Alaoui; M W Mahoney"}, {"ref_id": "b1", "title": "Greedy column subset selection: New bounds and distributed algorithms", "journal": "PMLR", "year": "2016-06", "authors": "J Altschuler; A Bhaskara; G Fu; V Mirrokni; A Rostamizadeh; M Zadimoghaddam"}, {"ref_id": "b2", "title": "Monte carlo markov chain algorithms for sampling strongly rayleigh distributions and determinantal point processes", "journal": "PMLR", "year": "2016-06", "authors": "N Anari; S O Gharan; A Rezaei"}, {"ref_id": "b3", "title": "Faster subset selection for matrices and applications", "journal": "SIAM Journal on Matrix Analysis and Applications", "year": "2013", "authors": "H Avron; C Boutsidis"}, {"ref_id": "b4", "title": "Kernel independent component analysis", "journal": "J. Mach. Learn. Res", "year": "2003-03", "authors": "F R Bach; M I Jordan"}, {"ref_id": "b5", "title": "High-dimensional matched subspace detection when data are missing", "journal": "IEEE", "year": "2010", "authors": "L Balzano; B Recht; R Nowak"}, {"ref_id": "b6", "title": "Benign overfitting in linear regression", "journal": "", "year": "2019", "authors": "P L Bartlett; P M Long; G Lugosi; A Tsigler"}, {"ref_id": "b7", "title": "Spectral methods in machine learning and new strategies for very large datasets", "journal": "Proceedings of the National Academy of Sciences", "year": "2009", "authors": "M.-A Belabbas; P J Wolfe"}, {"ref_id": "b8", "title": "A determinantal point process for column subset selection", "journal": "", "year": "2018-12", "authors": "A Belhadji; R Bardenet; P Chainais"}, {"ref_id": "b9", "title": "Reconciling modern machine-learning practice and the classical bias-variance trade-off", "journal": "", "year": "2019", "authors": "M Belkin; D Hsu; S Ma; S Mandal"}, {"ref_id": "b10", "title": "Two models of double descent for weak features", "journal": "", "year": "2019", "authors": "M Belkin; D Hsu; J Xu"}, {"ref_id": "b11", "title": "Optimal CUR matrix decompositions", "journal": "Association for Computing Machinery", "year": "2014", "authors": "C Boutsidis; D P Woodruff"}, {"ref_id": "b12", "title": "An improved approximation algorithm for the column subset selection problem", "journal": "", "year": "2008", "authors": "C Boutsidis; M Mahoney; P Drineas"}, {"ref_id": "b13", "title": "Near optimal column-based matrix reconstruction", "journal": "", "year": "2011-10", "authors": "C Boutsidis; P Drineas; M Magdon-Ismail"}, {"ref_id": "b14", "title": "Rates of convergence for sparse variational Gaussian process regression", "journal": "PMLR", "year": "2019-06", "authors": "D Burt; C E Rasmussen;  Van Der; M Wilk"}, {"ref_id": "b15", "title": "Sampling from a k-dpp without looking at all items", "journal": "", "year": "2020", "authors": "D Calandriello; M Derezi\u0144ski; M Valko"}, {"ref_id": "b16", "title": "Some applications of the rank revealing QR factorization", "journal": "SIAM Journal on Scientific and Statistical Computing", "year": "1992", "authors": "T F Chan; P C Hansen"}, {"ref_id": "b17", "title": "A library for support vector machines", "journal": "ACM Transactions on Intelligent Systems and Technology", "year": "2011", "authors": "C.-C Chang; C.-J Lin;  Libsvm"}, {"ref_id": "b18", "title": "Algorithms for p low-rank approximation", "journal": "Proceedings of Machine Learning Research", "year": "2017-08", "authors": "F Chierichetti; S Gollapudi; R Kumar; S Lattanzi; R Panigrahy; D P Woodruff"}, {"ref_id": "b19", "title": "Complex Graphs and Networks (Cbms Regional Conference Series in Mathematics)", "journal": "", "year": "", "authors": "F Chung; L Lu"}, {"ref_id": "b20", "title": "Fast determinantal point processes via distortion-free intermediate sampling", "journal": "", "year": "2019-06", "authors": "M Derezi\u0144ski"}, {"ref_id": "b21", "title": "Determinantal point processes in randomized numerical linear algebra", "journal": "", "year": "2020", "authors": "M Derezi\u0144ski; M W Mahoney"}, {"ref_id": "b22", "title": "Reverse iterative volume sampling for linear regression", "journal": "Journal of Machine Learning Research", "year": "2018", "authors": "M Derezi\u0144ski; M K Warmuth"}, {"ref_id": "b23", "title": "Exact sampling of determinantal point processes with sublinear time preprocessing", "journal": "Curran Associates, Inc", "year": "2019", "authors": "M Derezi\u0144ski; D Calandriello; M Valko"}, {"ref_id": "b24", "title": "Minimax experimental design: Bridging the gap between statistical and worst-case approaches to least squares regression", "journal": "", "year": "2019-06", "authors": "M Derezi\u0144ski; K L Clarkson; M W Mahoney; M K Warmuth"}, {"ref_id": "b25", "title": "Exact expressions for double descent and implicit regularization via surrogate random design", "journal": "", "year": "2019", "authors": "M Derezi\u0144ski; F Liang; M W Mahoney"}, {"ref_id": "b26", "title": "Bayesian experimental design using regularized determinantal point processes", "journal": "", "year": "2020", "authors": "M Derezi\u0144ski; F Liang; M Mahoney"}, {"ref_id": "b27", "title": "Efficient volume sampling for row/column subset selection", "journal": "", "year": "2010-10", "authors": "A Deshpande; L Rademacher"}, {"ref_id": "b28", "title": "Matrix approximation and projective clustering via volume sampling", "journal": "", "year": "2006-01", "authors": "A Deshpande; L Rademacher; S Vempala; Wang ; G "}, {"ref_id": "b29", "title": "On the Nystr\u00f6m method for approximating a Gram matrix for improved kernel-based learning", "journal": "Journal of Machine Learning Research", "year": "2005", "authors": "P Drineas; M W Mahoney"}, {"ref_id": "b30", "title": "Relative-error CUR matrix decompositions", "journal": "SIAM Journal on Matrix Analysis and Applications", "year": "2008", "authors": "P Drineas; M W Mahoney; S Muthukrishnan"}, {"ref_id": "b31", "title": "Restricted Strong Convexity Implies Weak Submodularity", "journal": "Annals of Statistics", "year": "2018", "authors": "E R Elenberg; R Khanna; A G Dimakis; S Negahban"}, {"ref_id": "b32", "title": "DPPy: DPP Sampling with Python", "journal": "", "year": "2019", "authors": "G Gautier; G Polito; R Bardenet; M Valko"}, {"ref_id": "b33", "title": "Revisiting the Nystr\u00f6m method for improved large-scale machine learning", "journal": "J. Mach. Learn. Res", "year": "2016-01", "authors": "A Gittens; M W Mahoney"}, {"ref_id": "b34", "title": "Diverse sequential subset selection for supervised video summarization", "journal": "Curran Associates, Inc", "year": "2014", "authors": "B Gong; W.-L Chao; K Grauman; F ; Z Sha; M Welling; C Cortes; N D Lawrence; Weinberger "}, {"ref_id": "b35", "title": "Efficient algorithms for computing a strong rank-revealing qr factorization", "journal": "SIAM Journal on Scientific Computing", "year": "1996", "authors": "M Gu; S C Eisenstat"}, {"ref_id": "b36", "title": "Optimal column-based low-rank matrix reconstruction", "journal": "", "year": "2012-01", "authors": "V Guruswami; A K Sinop"}, {"ref_id": "b37", "title": "An introduction to variable and feature selection", "journal": "J. Mach. Learn. Res", "year": "2003-03", "authors": "I Guyon; A Elisseeff"}, {"ref_id": "b38", "title": "", "journal": "Cambridge University Press", "year": "1952", "authors": "G Hardy; J Littlewood; G P\u00f3lya;  Inequalities"}, {"ref_id": "b39", "title": "Determinantal processes and independence", "journal": "", "year": "2006", "authors": "J B Hough; M Krishnapur; Y Peres; B Vir\u00e1g"}, {"ref_id": "b40", "title": "Scalable greedy support selection via weak submodularity", "journal": "", "year": "2017", "authors": "R Khanna; E R Elenberg; A G Dimakis; S Neghaban; J Ghosh"}, {"ref_id": "b41", "title": "Examples are not enough, learn to criticize! criticism for interpretability", "journal": "", "year": "2016", "authors": "B Kim; R Khanna; O Koyejo"}, {"ref_id": "b42", "title": "k-DPPs: Fixed-Size Determinantal Point Processes", "journal": "", "year": "2011-06", "authors": "A Kulesza; B Taskar"}, {"ref_id": "b43", "title": "Determinantal Point Processes for Machine Learning", "journal": "Now Publishers Inc", "year": "2012", "authors": "A Kulesza; B Taskar"}, {"ref_id": "b44", "title": "Mapping the similarities of spectra: Global and locally-biased approaches to SDSS galaxy data", "journal": "Astrophysical Journal", "year": "", "authors": "D Lawlor; T Budav\u00e1ri; M W Mahoney"}, {"ref_id": "b45", "title": "On the multiple descent of minimum-norm interpolants and restricted lower isometry of kernels", "journal": "", "year": "", "authors": "T Liang; A Rakhlin; X Zhai"}, {"ref_id": "b46", "title": "A random matrix analysis of random fourier features: beyond the gaussian kernel", "journal": "", "year": "2020", "authors": "Z Liao; R Couillet; M W Mahoney"}, {"ref_id": "b47", "title": "The coincidence approach to stochastic point processes", "journal": "Advances in Applied Probability", "year": "1975", "authors": "O Macchi"}, {"ref_id": "b48", "title": "Approximate computation and implicit regularization for very large-scale data analysis", "journal": "", "year": "2012", "authors": "M W Mahoney"}, {"ref_id": "b49", "title": "Recursive sampling for the nystrom method", "journal": "Curran Associates, Inc", "year": "2017", "authors": "C Musco; C ; Musco; U V Luxburg; S Bengio; H Wallach; R Fergus; S Vishwanathan; Garnett "}, {"ref_id": "b50", "title": "Convergence analysis of block coordinate algorithms with determinantal sampling", "journal": "", "year": "2020", "authors": "M Mutny; M Derezi\u0144ski; A Krause"}, {"ref_id": "b51", "title": "Column selection via adaptive sampling", "journal": "MIT Press", "year": "2015", "authors": "S Paul; M Magdon-Ismail; P Drineas"}, {"ref_id": "b52", "title": "Double descent in the condition number", "journal": "", "year": "2019", "authors": "T Poggio; G Kur; A Banburski"}, {"ref_id": "b53", "title": "Gaussian Processes for Machine Learning", "journal": "MIT Press", "year": "2006", "authors": "C E Rasmussen; C K I Williams"}, {"ref_id": "b54", "title": "Beyond worst-case analysis", "journal": "Communications of the ACM", "year": "2019", "authors": "T Roughgarden"}, {"ref_id": "b55", "title": "Gaussian regression and optimal finite dimensional linear models", "journal": "Springer-Verlag", "year": "1997", "authors": "H Z Santa; H Zhu; C K I Williams; R Rohwer; M Morciniec"}, {"ref_id": "b56", "title": "Block basis factorization for scalable kernel evaluation", "journal": "SIAM Journal on Matrix Analysis and Applications", "year": "2019", "authors": "R Wang; Y Li; M W Mahoney; E Darve"}, {"ref_id": "b57", "title": "Tensorized determinantal point processes for recommendation", "journal": "ACM", "year": "2019", "authors": "R Warlop; J Mary; M Gartrell"}, {"ref_id": "b58", "title": "Using the Nystr\u00f6m method to speed up kernel machines", "journal": "MIT Press", "year": "2001", "authors": "C K I Williams; M Seeger"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure3: Top three plots show the Nystr\u00f6m approximation factor Er}K\u00b4p KpSq}\u02das{OPT k , where S \" k-DPPpKq (experiments using greedy selection instead of a k-DPP are in Appendix H), for a toy dataset (\u03ba is the condition number) and two Libsvm datasets (\u03c3 is the RBF parameter). Error bars show three times the standard error of the mean over 1000 trials. Bottom three plots show the spectral decay for the top 40 eigenvalues of each kernel K. Note that the peaks in the approximation factor align with the drops in the spectrum.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Er A pSq :\" }A\u00b4P S A} 2 F ,", "formula_coordinates": [1.0, 251.4, 612.04, 109.2, 12.69]}, {"formula_id": "formula_1", "formula_text": "OPT k :\" min B: rankpBq\"k }A\u00b4B} 2 F \u010f min S: |S|\"k Er A pSq.", "formula_coordinates": [2.0, 200.22, 376.84, 211.55, 17.57]}, {"formula_id": "formula_2", "formula_text": "ErEr A pSqs OPT k \u010f k`1.(1)", "formula_coordinates": [2.0, 263.72, 486.0, 276.28, 22.22]}, {"formula_id": "formula_3", "formula_text": "0 \u0103 \u010f 1 2 . If S \" k-DPPpA J Aq, then ErEr A pSqs OPT k \u010f p1`2 q 2 \u03a6 s pkq,", "formula_coordinates": [3.0, 144.6, 295.14, 171.81, 43.03]}, {"formula_id": "formula_4", "formula_text": "1. (polynomial spectral decay) c 1 i\u00b4p \u010f \u03bb i \u010f c 2 i\u00b4p @ i , with p \u0105 1, then S \" k-DPPpA J Aq satisfies ErEr A pSqs OPT k \u010f c\u03b3p. 2. (exponential spectral decay) c 1 p1\u00b4\u03b4q i \u010f \u03bb i \u010f c 2 p1\u00b4\u03b4q i @ i , \u03b4 P p0, 1q, then S \" k-DPPpA J Aq satisfies ErEr A pSqs OPT k \u010f c\u03b3p1`\u03b4kq.", "formula_coordinates": [3.0, 71.5, 608.77, 468.5, 87.68]}, {"formula_id": "formula_5", "formula_text": "Er A pSq OPT ki \u011b p1\u00b4\u03b4qpk i\u00b4ki\u00b41 q.", "formula_coordinates": [4.0, 243.44, 307.15, 126.33, 23.22]}, {"formula_id": "formula_6", "formula_text": "1 \u0103 k u 1 \u0103 k l 2 \u0103 k u 2 \u0103 .", "formula_coordinates": [4.0, 72.0, 383.22, 468.0, 24.16]}, {"formula_id": "formula_7", "formula_text": "min S:|S|\"k l i Er A pSq OPT k l i \u010f 1`\u03b4 and min S:|S|\"k u i Er A pSq OPT k u i \u011b p1\u00b4\u03b4qpk u i`1 q.", "formula_coordinates": [4.0, 148.29, 417.6, 315.43, 26.05]}, {"formula_id": "formula_8", "formula_text": "x K: rankpKq\"k }K\u00b4p K}\u02da\" OPT k .", "formula_coordinates": [5.0, 235.98, 215.06, 140.04, 16.86]}, {"formula_id": "formula_9", "formula_text": "\u03bb i -\u03bb 1 p b a`b`c q i , where a \" 1 4\u03b7 2 , b \" 1 \u03c3 2 and c \" ? a 2`2 ab (", "formula_coordinates": [5.0, 72.0, 333.17, 273.06, 18.94]}, {"formula_id": "formula_10", "formula_text": "ErEr A pSqs \" Er|S|s\u00a8\u03b1, for S \" DPPp 1 \u03b1 A J Aq.", "formula_coordinates": [5.0, 197.68, 614.85, 216.63, 13.48]}, {"formula_id": "formula_11", "formula_text": "Er|S|s \" n \u00ff i\"1 \u03bb i \u03b1`\u03bb i \u010f k \u00ff i\"1 \u03bb i \u03b1`\u03bb i`1 \u010f k`1.", "formula_coordinates": [5.0, 207.26, 661.14, 197.48, 29.33]}, {"formula_id": "formula_12", "formula_text": "E \" Er A pSq \u2030 OPT k \u010f \u03a6 s pkq 1\u00b4", "formula_coordinates": [6.0, 199.04, 168.76, 87.56, 25.52]}, {"formula_id": "formula_13", "formula_text": "A P R m\u02c6n , k P rns and \u03b1 \u0105 0, if S \" DPPp 1 \u03b1 A J Aq and S 1 \" k-DPPpA J Aq, then E \" Er A pS 1 q \u2030 \u010f E \" Er A pSq | |S| \u010f k \u2030 .", "formula_coordinates": [6.0, 164.21, 330.24, 355.44, 32.0]}, {"formula_id": "formula_14", "formula_text": "E \" Er A pS 1 q \u2030 paq \u010f E \" Er A pSq | |S| \u010f k \u2030 \u010f E \" Er A pSq \u2030 Prp|S| \u010f kq pbq \u010f \u03a6 s pkq p1\u00b4 q 2\u00a8O PT k ,", "formula_coordinates": [6.0, 152.44, 461.09, 307.13, 25.16]}, {"formula_id": "formula_15", "formula_text": "min |S|\"ki Er A pSq OPT ki \u011b p1\u00b4\u03b4ql i , for k i \" l 1`. ..`l i\u00b41 .", "formula_coordinates": [8.0, 192.2, 235.33, 227.59, 23.23]}, {"formula_id": "formula_16", "formula_text": "f pkq \" pk`1q \u0159 S:|S|\"k`1 \u015b iPS \u03bb i \u0159 S:|S|\"k \u015b iPS \u03bb i .", "formula_coordinates": [10.0, 230.75, 136.08, 150.49, 27.56]}, {"formula_id": "formula_17", "formula_text": "ErEr A pSqs \u010f ErEr A pS 1 qs for S 1 \" DPPp 1 \u03b1 k A J Aq,", "formula_coordinates": [10.0, 195.9, 206.04, 220.19, 14.14]}, {"formula_id": "formula_18", "formula_text": "Er|S|s \" \u00ff i \u03bb i \u03bb i`1 \" trpKpI`Kq\u00b41q. (2", "formula_coordinates": [15.0, 223.61, 369.01, 312.15, 26.15]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [15.0, 535.76, 375.75, 4.24, 8.74]}, {"formula_id": "formula_20", "formula_text": "ErP S s \" ApI`A J Aq\u00b41A J .", "formula_coordinates": [15.0, 244.33, 650.01, 123.33, 10.32]}, {"formula_id": "formula_21", "formula_text": "E \" Er A pSq \u2030 \" tr`AA J pI`1 \u03b1 AA J q\u00b41\u02d8\" Er|S|s\u00a8\u03b1.", "formula_coordinates": [16.0, 195.75, 127.33, 220.5, 13.26]}, {"formula_id": "formula_22", "formula_text": "E \" Er A pSq \u2030 \" E \" }pI\u00b4P S qA} 2 F \u2030 \" trpAA J ErI\u00b4P S sq \" tr`AA J pI\u00b41 \u03b1 ApI`1 \u03b1 A J Aq\u00b41A J qp\u02daq \" tr`AA J pI`1 \u03b1 AA J q\u00b41\u02d8,", "formula_coordinates": [16.0, 190.07, 169.59, 361.35, 50.02]}, {"formula_id": "formula_23", "formula_text": "detpI`Kq \" \u00ff S\u010erns detpK S,S q.", "formula_coordinates": [16.0, 240.91, 333.82, 130.19, 23.05]}, {"formula_id": "formula_24", "formula_text": "ErP S s \" ApI`A J Aq\u00b41A J .", "formula_coordinates": [16.0, 244.33, 421.37, 123.33, 10.32]}, {"formula_id": "formula_25", "formula_text": "detpK S,S qv J P S v \" detpK S,S qv J A S K\u00b41 S,S A J S v \" detpK S,S`A J S vv J A S q\u00b4detpK S,S q.", "formula_coordinates": [16.0, 118.87, 487.95, 374.26, 11.76]}, {"formula_id": "formula_26", "formula_text": "\u00ff S\u010erns: detpK S,S q\u01050 detpK S,S qv J A S K\u00b41 S,S A J S v \" \u00ff S\u010erns detpK S,S`A J S vv J A S q\u00b4detpK S,S q \" \u00ff S\u010erns det`rK`A J vv J As S,S\u02d8\u00b4\u00ff S\u010erns detpK S,S q p\u02daq \" detpI`K`A J vv J Aq\u00b4detpI`Kq \" detpI`Kqv J ApI`Kq\u00b41A J v,", "formula_coordinates": [16.0, 244.61, 542.15, 217.48, 117.16]}, {"formula_id": "formula_27", "formula_text": "E \" Er A pSq \u2030 OPT k \u010f \u03a6 s pkq 1\u00b4", "formula_coordinates": [17.0, 199.04, 137.37, 87.56, 25.52]}, {"formula_id": "formula_28", "formula_text": "OPT k \" \u00ff j\u0105k \u03bb j \" sr s pAq\u00b4k \u00ff j\"s`1 \u03bb j \u011b r\u00b4l.", "formula_coordinates": [17.0, 207.76, 266.4, 196.48, 22.6]}, {"formula_id": "formula_29", "formula_text": "\u03b1 1 \" \u03b2\u03b1 \" \u03b3 s pkqpr\u00b4lq p1\u00b4 ql \" ? r 2\u00b4l2 p1\u00b4 ql , \u03b1 2 \" p1\u00b4 q ? r`l`?r\u00b4l 2 ? r`l \u03b1 1 \" p ? r`l`?r\u00b4lq ? r\u00b4l r`l\u00b4pr\u00b4lq \" ? r\u00b4l ? r`l\u00b4?r\u00b4l .(3)", "formula_coordinates": [17.0, 135.0, 339.41, 405.0, 59.24]}, {"formula_id": "formula_30", "formula_text": "Er|S|s \" \u00ff i \u03bb i \u03bb i`\u03b1 \u010f s`k \u00ff i\"s`1 \u03bb i \u03bb i`\u03b1 1 i`\u00ff i\u0105k \u03b2\u03bb i \u03b2\u03bb i`\u03b2 \u03b1 \u010f s`k \u00ff i\"s`1 \u03bb 1 i \u03bb 1 i`\u03b1 2`\u00ff i\u0105k \u03bb 1 i \u03bb 1 i`\u03b1 1 .(4)", "formula_coordinates": [17.0, 110.32, 456.83, 429.68, 27.45]}, {"formula_id": "formula_31", "formula_text": "k \u00ff i\"s`1 \u03bb 1 i \u03bb 1 i`\u03b1 2 \" l 1`\u03b1 2 \" l\u00b4l 1`1 \u03b1 2 \" l\u00b4l 1`? r`l\u00b4?r\u00b4l ? r\u00b4l \" l\u00b4l ? r\u00b4l ? r`l .(5)", "formula_coordinates": [17.0, 144.52, 512.29, 395.48, 35.58]}, {"formula_id": "formula_32", "formula_text": "\u00ff i\u0105k \u03bb 1 i \u03bb 1 i`\u03b1 1 \u010f 1 \u03b1 1 \u00ff i\u0105k \u03bb 1 i \" r\u00b4l \u03b1 1 \" p1\u00b4 q l ? r\u00b4l ? r`l . (6", "formula_coordinates": [17.0, 199.44, 574.83, 336.32, 33.51]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [17.0, 535.76, 588.7, 4.24, 8.74]}, {"formula_id": "formula_34", "formula_text": "E \" Er A pSq \u2030 OPT k \" Er|S|s\u00a8\u03b1 OPT k \u010f k k\u00b4s \u03b3 s pkq 1\u00b4 \" \u03a6 s pkq 1\u00b4 ,", "formula_coordinates": [17.0, 200.97, 648.1, 211.26, 25.52]}, {"formula_id": "formula_35", "formula_text": "\u03bb 1 i \u03bb 1 i`\u03b1 1 i", "formula_coordinates": [18.0, 143.41, 93.81, 21.83, 18.12]}, {"formula_id": "formula_36", "formula_text": "k\u00b4ErXs \u011b l ? r\u00b4l ?", "formula_coordinates": [18.0, 258.03, 161.06, 91.78, 23.16]}, {"formula_id": "formula_37", "formula_text": "VarrXs \u010f k \u00ff i\"s`1 p1\u00b4p i q`\u00ff i\u0105k p i \u010f l ? r\u00b4l ? r`l`p 1\u00b4 q l ? r\u00b4l ? r`l \" p2\u00b4 q l ? r\u00b4l ? r`l .", "formula_coordinates": [18.0, 137.23, 229.36, 337.54, 33.51]}, {"formula_id": "formula_38", "formula_text": "Prp|S| \u0105 kq \u010f PrpX \u0105 lq \u010f PrpX \u0105 ErXs`\u03bbq \u010f exp\u00b4\u00b4\u03bb 2 2pVarrXs`\u03bb{3q\u010f exp\u00b4\u00b4\u03bb 2 2p 2\u00b4 \u03bb`\u03bb{3q\u00af\u010f expp\u00b4 \u03bb{4q \" exp\u00b4\u00b4 2 l ? r\u00b4l 4 ? r`l\u00af.", "formula_coordinates": [18.0, 140.9, 300.44, 441.37, 53.46]}, {"formula_id": "formula_39", "formula_text": "? r\u00b4l ? r`l \u011b l ? 2l`1 \u011b 7 16 ?", "formula_coordinates": [18.0, 258.62, 362.08, 95.52, 19.98]}, {"formula_id": "formula_40", "formula_text": "E \" Er A pS 1 q \u2030 \u010f E \" Er A pSq | |S| \u010f k \u2030 .", "formula_coordinates": [18.0, 229.79, 429.36, 152.43, 11.32]}, {"formula_id": "formula_41", "formula_text": "e k \" \u00ff T :|T |\"k detpA J T A T q \" \u00ff T :|T |\"k \u017a iPT \u03bb i .", "formula_coordinates": [18.0, 218.55, 483.85, 174.9, 23.44]}, {"formula_id": "formula_42", "formula_text": "1 \u011b\u0113 k\u00b41\u0113k`1 e 2 k \" e k\u00b41 e k`1 e 2 k`n kn k\u00b41\u02d8`n kn k`1\u02d8\" e k\u00b41 e k`1 e 2 k n`1\u00b4k k k`1 n\u00b4k .", "formula_coordinates": [18.0, 156.61, 544.46, 298.77, 25.7]}, {"formula_id": "formula_43", "formula_text": "ErEr A pSq | |S| \" ks ErEr A pSq | |S| \" k\u00b41s \" k`1 k e k`1 e k\u00b41 e 2 k \u010f n\u00b4k n`1\u00b4k \u010f 1. (7", "formula_coordinates": [18.0, 175.27, 616.08, 360.49, 24.74]}, {"formula_id": "formula_44", "formula_text": ")", "formula_coordinates": [18.0, 535.76, 622.82, 4.24, 8.74]}, {"formula_id": "formula_45", "formula_text": "ErEr A pSqs OPT k \u010f`1\u00b4e\u00b4 2 b 10\u02d8\u00b41 p1\u00b4 q\u00b41\u03a8 s pkq,", "formula_coordinates": [19.0, 212.45, 161.96, 188.29, 22.22]}, {"formula_id": "formula_46", "formula_text": "ErXs \" \u00ff i\u0105s \u03bb i \u03bb i`\u03b1 \u010f pn\u00b4sq\u03bb s`1 \u03bb s`1`\u03bb s`1 \u03bbn pn\u00b4kq\u03bbn p1\u00b4 qpk\u00b4sq \" 1 1 n\u00b4s`1 p1\u00b4 qpk\u00b4sq n\u00b4k n\u00b4s \" 1 1 n\u00b4s`1 p1\u00b4 qpk\u00b4sq p1\u00b4k\u00b4s n\u00b4s q \" 1 1 p1\u00b4 qpk\u00b4sq\u00b4 1\u00b4 1 n\u00b4s \" 1\u00b4 1 k\u00b4s\u00b4 n\u00b4s . Let S 1 \" DPPp 1 \u03b1 A J Aq.", "formula_coordinates": [19.0, 72.0, 251.84, 308.9, 199.75]}, {"formula_id": "formula_47", "formula_text": "ErEr A pS 1 qs OPT k \" Er|S|s\u00a8\u03b1 OPT k \u010f p1\u00b4 q\u00b41 k k\u00b4s \u03bb s`1 \u03bb n \" p1\u00b4 q\u00b41\u00b41`s k\u00b4s\u00af\u03bb s`1 \u03bb n .", "formula_coordinates": [19.0, 138.22, 667.18, 336.76, 24.29]}, {"formula_id": "formula_48", "formula_text": "Prp|S 1 | \u0105 kq \u010f PrpX \u0105 k\u00b4sq \u010f PrpX \u0105 ErXs`\u03bbq \u010f e\u00b4\u03bb 2 2pk\u00b4sq \" e\u00b4 2 pk\u00b4sq{8 .", "formula_coordinates": [20.0, 139.61, 119.15, 332.79, 15.34]}, {"formula_id": "formula_49", "formula_text": "Prp|S 1 | \u0105 kq \" Prpn\u00b4|S 1 | \u0103 n\u00b4kq \u010f exp\u00b4\u00b4\u03bb 2 2 Ern\u00b4|S 1 |s\" exp\u00b4\u00b4\u03bb 2 2 pn\u00b4kq`\u2206 n\u00b4k` 2 pn\u00b4kq`\u2206\u010f exp\u00b4\u00b4\u03bb 2 2 pn\u00b4kq n\u00b4k` 2 pn\u00b4kq\" exp\u00b4\u00b4 2 pn\u00b4kq 8p1` {2qp\u02daq \u010f exp\u00b4\u00b4 2 pn\u00b4kq 10\u00af,", "formula_coordinates": [20.0, 199.66, 182.17, 244.68, 154.04]}, {"formula_id": "formula_50", "formula_text": "E \" Er A pSq \u2030 \u010f E \" Er A pS 1 q | |S 1 | \u010f k \u2030 \u010f E \" Er A pS 1 q \u2030 Prp|S 1 | \u010f kq \u010f`1\u00b4e\u00b4 2 b 10\u02d8\u00b41 p1\u00b4 q\u00b41 \u03bb s`1 \u03bb n\u00b41`s k\u00b4s\u00af\u00a8O PT k ,", "formula_coordinates": [20.0, 168.38, 368.05, 275.24, 52.49]}, {"formula_id": "formula_51", "formula_text": "ErEr A pSqs OPT k \u010f 2`1\u00b4e\u00b41 40\u02d8\u00b41 \u03a8 s pkq \u010f 82 \u03a8 s pkq.", "formula_coordinates": [20.0, 206.31, 481.57, 200.58, 22.22]}, {"formula_id": "formula_52", "formula_text": "1. (polynomial spectral decay) c 1 i\u00b4p \u010f \u03bb i \u010f c 2 i\u00b4p @ i , with p \u0105 1, then S \" k-DPPpA J Aq satisfies ErEr A pSqs OPT k \u010f c\u03b3p.", "formula_coordinates": [20.0, 71.5, 542.98, 436.05, 44.32]}, {"formula_id": "formula_53", "formula_text": "ErEr A pSqs OPT k \u010f c\u03b3p1`\u03b4kq.", "formula_coordinates": [20.0, 252.43, 631.65, 108.34, 22.22]}, {"formula_id": "formula_54", "formula_text": "\u0159 i\u011bs 1 i p as: \u017c x\u011bps`1q 1 i p dx \u010f \u00ff i\u011bs 1 i p \u010f \u017c x\u011bs 1 i p dx \u00f9\u00f1 n \u00ff i\"s`1 1 i p \u011b ps`2q 1\u00b4p p\u00b41\u00b4p n`1q 1\u00b4p p\u00b41 .", "formula_coordinates": [21.0, 136.92, 73.28, 338.16, 51.15]}, {"formula_id": "formula_55", "formula_text": "\u0159 n i\"s`1 \u03bb i \u03bb s`1 \u011b c 1 c 2\u02c6p s`2q 1\u00b4p pp\u00b41qps`1q\u00b4p\u00b4p n`1q 1\u00b4p pp\u00b41qps`1q\u00b4p\" 1 \u03b3\u02c6s`2 p\u00b41\u00b41\u00b41 s`2\u00afp\u00b4s`1 p\u00b41\u00b4s`1 n`1\u00afp\u00b41\u011b 1 \u03b3\u02c6s`2 p\u00b41\u00b41\u00b4s`1 p\u00b41\u00a81 2\u02d9\" 1 2\u03b3 s`1 p\u00b41\u00b41 \u03b3 .", "formula_coordinates": [21.0, 219.35, 165.28, 296.77, 109.91]}, {"formula_id": "formula_56", "formula_text": "\u03a6 s pkq \u010f k u c 1`2 u sr s\u00b4u \u010f k u`k 1 2\u03b3 s`1 p\u00b41\u00b4\u03b3\u00b41\u00b4u \" k u`p 2p\u00b42qk \u03b3\u00b41ps`1\u00b42p`2q\u00b4p2p\u00b42qu \u010f k u`p 2p\u00b42`\u03b3\u00b41qk \u03b3\u00b41pk`3\u00b42pq\u00b4p2p\u00b42`\u03b3\u00b41qu", "formula_coordinates": [21.0, 110.52, 305.33, 389.77, 54.97]}, {"formula_id": "formula_57", "formula_text": "k`3\u00b42p 2\u03b3p2p\u00b42`\u03b3\u00b41q which achieves the value 4p\u03b3p2p\u00b42q`1qk k`3\u00b42p", "formula_coordinates": [21.0, 73.2, 371.08, 466.8, 30.32]}, {"formula_id": "formula_58", "formula_text": "12\u03b3pk k\u00b42p \u010f 12\u03b3pk k\u00b4k{30 \" 12\u03b3p 1\u00b41{30 \u010f 360 29 \u03b3p.", "formula_coordinates": [21.0, 217.56, 435.28, 178.08, 22.31]}, {"formula_id": "formula_59", "formula_text": "Case 1(b):`k`1 n\u02d8p\u00b41 \u0105 1 2", "formula_coordinates": [21.0, 86.94, 503.38, 114.78, 13.47]}, {"formula_id": "formula_60", "formula_text": "\u03a8 s pkq \" \u03bb s`1 \u03bb n k k\u00b4s \u010f \u03b3 n p ps`1q p k k\u00b4s \" \u03b3\u02c6n k`1\u02d9p\u00b41 k`1 n pk`1q p ps`1q p k k\u00b4s \u010f 2\u03b3\u02c6k`1 s`1\u02d9p k k\u00b4s ,", "formula_coordinates": [21.0, 91.44, 559.43, 429.13, 24.8]}, {"formula_id": "formula_61", "formula_text": "\u03a8 s pkq \u010f \u03b3 kpp`1q k`1\u02c6p`1 p\u02d9p \u010f e\u03b3pp`1q \u010f 2e\u03b3p.", "formula_coordinates": [21.0, 197.26, 629.79, 217.49, 22.31]}, {"formula_id": "formula_62", "formula_text": "sr s pAq \" \u00ff j\u0105s \u03bb j {\u03bb s`1 \u011b c 1 p1\u00b4p1\u00b4\u03b4q n\u00b4s q{\u03b4 c 2 \" 1\u00b4p1\u00b4\u03b4q n\u00b4s \u03b3\u03b4 .", "formula_coordinates": [22.0, 168.27, 91.47, 275.47, 27.73]}, {"formula_id": "formula_63", "formula_text": "s \u010f n\u00b4l n 2 \u03b4 \u00f9\u00f1 s \u010f n\u00b4l n 2 ln 1 1\u00b4\u03b4 \u00f9\u00f1 pn\u00b4sq ln 1 1\u00b4\u03b4 \u011b ln 2 \u00f0\u00f1 1\u00b4p1\u00b4\u03b4q n\u00b4s \u011b 1 2 \u00f9\u00f1 sr s pKq \u011b 1 2\u03b3\u03b4 ,", "formula_coordinates": [22.0, 249.48, 165.75, 115.8, 124.23]}, {"formula_id": "formula_64", "formula_text": "\u03a6 s pkq \u010f k u\u02c61`2 \u03b3\u03b4u 1\u00b42\u03b3\u03b4u\u02d9\" k u\u00a81 1\u00b42\u03b3\u03b4u .", "formula_coordinates": [22.0, 209.81, 336.8, 192.39, 22.31]}, {"formula_id": "formula_65", "formula_text": "\u03a8 s pkq \" \u03bb s`1 \u03bb n k k\u00b4s ,", "formula_coordinates": [22.0, 261.71, 507.38, 88.59, 23.23]}, {"formula_id": "formula_66", "formula_text": "\u03bb s`1 \u03bb n \u010f \u03b3 p1\u00b4\u03b4q n\u00b4s \u010f \u03b3 p1\u00b4\u03b4q p\u03b4`ln 4q{\u03b4 \" \u03b3 \" p1\u00b4\u03b4q\u00b41 \u03b4 \u0131 \u03b4`ln 4 \u010f \u03b3e \u03b4`ln 4 1\u00b4\u03b4 .", "formula_coordinates": [22.0, 153.55, 566.48, 306.09, 23.44]}], "doi": "10.1073/pnas.0810600105"}