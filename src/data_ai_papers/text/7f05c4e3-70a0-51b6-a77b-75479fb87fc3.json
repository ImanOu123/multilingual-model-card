{"title": "On the Foundation of Distributionally Robust Reinforcement Learning", "authors": "Shengbo Wang; Nian Si; Jose Blanchet; Zhengyuan Zhou", "pub_date": "2024-01", "abstract": "Motivated by the need for a robust policy in the face of environment shifts between training and deployment, we contribute to the theoretical foundation of distributionally robust reinforcement learning (DRRL). This is accomplished through a comprehensive modeling framework centered around distributionally robust Markov decision processes (DRMDPs). This framework obliges the decision maker to choose an optimal policy under the worst-case distributional shift orchestrated by an adversary. By unifying and extending existing formulations, we rigorously construct DRMDPs that embrace various modeling attributes for both the decision maker and the adversary. These attributes include adaptability granularity, exploring history-dependent, Markov, and Markov time-homogeneous decision maker and adversary dynamics. Additionally, we delve into the flexibility of shifts induced by the adversary, examining SA and S-rectangularity. Within this DRMDP framework, we investigate conditions for the existence or absence of the dynamic programming principle (DPP). From an algorithmic standpoint, the existence of DPP holds significant implications, as the vast majority of existing data and computationally efficient RL algorithms are reliant on the DPP. To study its existence, we comprehensively examine combinations of controller and adversary attributes, providing streamlined proofs grounded in a unified methodology. We also offer counterexamples for settings in which a DPP with full generality is absent.", "sections": [{"heading": "Introduction", "text": "Our current era has growingly witnessed the benefits of data-driven sequential decision making, which has unleashed -and has much further potential to unleash -productivity forces in a diverse range of applications of societal impact. Such applications range from the traditional engineering domains such as automation [Landgraf et al., 2021], inventory management [Gong andSimchi-Levi, 2023, Mao et al., 2020], and control of service systems [Hu et al., 2021, Zhalechian et al., 2023 to the newly emerged areas in the digital economy such as personalized product recommendation in online platforms [Afsar et al., 2022], feature-based dynamic pricing [Bertsimas and Perakis, 2006], and real-time bidding in high-frequency auctions [Grislain et al., 2019, Zhao et al., 2018.\nIn modern applications, the reliability of the learning process is crucial to ensure that a policy learned from historical data or simulations performs effectively when deployed in real-world scenarios. Classical reinforcement learning (RL), where the learning process is typically modeled as an unknown Markov decision process (MDP), has demonstrated remarkable empirical success in simulated environments, with application domains spanning robotics [Kormushev et al., 2013], autonomous driving [Shalev-Shwartz et al., 2016, Pan et al., 2017, Kiran et al., 2021, and games [Mnih et al., 2015, Silver et al., 2017. However, extending the success observed in simulated environments to applications in real environments poses challenges and remains an active area of research. A major obstacle is the frequently misplaced assumption that the training and deployment environments of RL policies are congruent. The reality, however, is rife with discrepancies caused by model mis-specifications, environment shifts, and various other confounding factors. Another challenge emerges from the intricacies of computing optimal policies in models where the complete Markov state of the system is either not fully observable to the modeler and, consequently, to the controller, or intentionally omitted due to computational considerations. To address these challenges, it is natural to explore distributionally robust (DR) formulations that take into account the impact on performance arising from the differences between the training and real environments.\nThis paper studies foundational aspects of distributionally robust reinforcement learning (DRRL), an approach designed to bridge the chasm between idealistic models and the complex reality. At the heart of our formulation lies a strategic game. One agent, representing the controller, desires to maximize the expected cumulative reward. In contrast, the other agent, severing as the adversary that embodies potential environmental shifts, selects the stochastic environment with the aim of diminishing this reward. This adversarial interaction model proves insightful in capturing the complexities associated with designing a reliable controller for real environments, especially when relying on a simplified training environment. Hence, DRRL provides a more resilient and realistic training paradigm, addressing the challenges that arise in deploying policies to real-world application.\nA key contribution of our research lies in presenting a unified approach to both controller and adversary models, incorporating salient attributes. This broadens the spectrum of behaviors encapsulated by our formulations of DRMDP, and hence enhances their applicability. Moreover, our investigation delves into various facets of the interactions between the controller and the adversary. One aspect is the information availability to the controller and the adversary at the time of action, encompassing considerations of historydependence, Markov, and time-homogeneity. For instance, within a simulated MDP environment, the use of time-homogeneous adversary implies that the controllers trained in the simulator assumes the real environment is also an MDP with the same states and actions, although likely with different and unknown transition dynamics. In this setting, the corresponding DRRL problem aims to learn from the simulator a best controller policy for the worst shift in a predetermined set of possible environments.\nIn certain applications, there is a legitimate concern that the true complexity of the real-world environment far exceeds the simplicity of the Markov model, particularly with respect to the states considered. For example, a simulated MDP environment may fail to capture some hidden states of the real environment, making it not an MDP for the controller when using the same state information as in the simulator. This learning settings necessitates the consideration of a richer model of how the distributional shift occurs. As such, even if the modeler doesn't posit the presence of an actual adversary actively harming the controller, the inclusion of a history-dependent controller and adversary model is beneficial: this model accommodates the non-Markov nature of the partially observed system, which comprises latent states inaccessible to the controller but influential in driving the system's evolution. For further elaborations, see the proceeding Section 1.2. This addresses a distinct source of the simulator-to-real-environment gap, one that cannot be effectively tackled by the time-homogeneous adversary mentioned earlier.\nAdditionally, we examine attributes of the adversary necessary for an accurate DRRL model, aiming to strike a balance between capturing the essential features of the real environment and maintaining computa-tional feasibility. Concretely, an effective adversary in this context must navigate the challenge of addressing environment shifts while preserving the structural integrity of real environments and avoiding overly conservative policies. Striking the right balance is crucial: an adversary that is too weak may fail to encompass the full spectrum of environments an RL agent might encounter, thereby undermining the model's reliability in the deployment environment. On the other hand, an adversary that is too powerful might compel the RL agent to consider an array of improbable scenarios, leading to the violation of the structural integrity of the modeled system and the adoption of overly cautious and impractical policies. Our framework provides the flexibility to tailor an adversary to the problem by integrating various specific attributes, such as rectangular adversarial decisions and the imposition of convexity in the adversarial action set.\nIn this paper, our central objective is to comprehensively investigate the conditions under which formulations of DRRL, accounting for the aforementioned variety of attributes for both the controller and the adversary, result in a dynamic programming principle (DPP). This is expressed in the form of a Bellmantype equation [Bellman, 1954]. This equation, once established, will serve as a foundation for the subsequent development and analysis of efficient RL algorithms-a domain we plan to explore in subsequent research endeavors.\nThis paper achieves this goal by establishing a unifying framework for the verification of the existence or absence of a DPP. A more detailed exposition of our results is presented in the upcoming discussion. Moreover, our framework provides guidelines for both theorists and practitioners in the selection and design of DRRL environments, encompassing a diverse spectrum of controller and adversary attributes, as presented in Section 1.2. The flexibility within our framework facilitates the development of practical DRRL algorithms, both model-based and model-free, while simultaneously yielding policies that effectively hedge against the unpredictability inherent in real-world applications.", "publication_ref": ["b8", "b14", "b2", "b12", "b65", "b18", "b40", "b34", "b17", "b29", "b48", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Results and Methodology", "text": "We first present an overview of the fundamental concepts in this manuscript. The study addresses the problem of distributionally robust Markov decision processes (DRMDP) with infinite-horizon \u03b3-discounted reward, formulated as follows:\nsup \u03c0\u2208\u03a0 inf \u03ba\u2208K E \u03c0,\u03ba \u00b5 \u221e k=0 \u03b3 k r(X t , A t ) , (1.1)\nwhere r is the reward function, X t and A t represent the state and action at time t respectively, and X 0 \u223c \u00b5 as the initial distribution. The controller selects a policy \u03c0 from the class \u03a0, maximizing the worst-case reward against an adversary who, with knowledge of the controller's policy, selects a policy from a class K Here, the policy classes (\u03a0, K) considered in this paper will be formally defined in Section 2. Despite extensive exploration in the literature [Nilim and El Ghaoui, 2005, Gonz\u00e1lez-Trejo et al., 2002, Iyengar, 2005, Xu and Mannor, 2010, Le Tallec, 2007, Wiesemann et al., 2013, Shapiro, 2021, Goyal and Grand-Cl\u00e9ment, 2023, Li and Shapiro, 2023, there remains ambiguity in the rigorous formulation of Problem 1.1. This paper contributes to this literature by defining and examining the dynamic behavior arising from various combinations of controllers and adversaries. The investigation takes into account variations in information availability and rectangularity, shedding light on nuanced aspects that enrich our understanding of the DRMDP problem.\nIn terms of information perspectives, we systematically examine history-dependent, Markov, and Markov time-homogeneous controllers and adversaries. A history-dependent entity in either role has the flexibility to choose different actions or distributions based on its historical information. In contrast, a Markov controller or adversary lacks memory of prior states and actions but retains the capacity to take time-and-state-dependent actions or distributions. Further narrowing the scope, a Markov time-homogeneous controller or adversary relinquishes information about time, compelling it to consistently employ a static state-dependent action or distribution, starting from time 0 and persisting thereafter.\nThe notion of rectangularity, as initially introduced in the DRMDP literature, originally pertained to the adversary's freedom to vary actions over time [Iyengar, 2005]. However, owing to the adoption of the information perspectives we discussed earlier and the formulation characteristics allowing for the strategic use of rectangularity to limit the adversary's power, the concept has evolved into a mechanism for imposing structural constraints on the adversary. This conceptual change is articulated in Le Tallec [2007] and Wiesemann et al. [2013]. In the DRMDP literature, the prevalent assumptions are S and SA-rectangularity. An SArectangular adversary enjoys the liberty to choose different action distributions for each current state-action pair (s, a) \u2208 S \u00d7 A. On the contrary, S-rectangularity restricts this freedom by permitting the adversary to choose its action distribution with prescribed dependence among each action a \u2208 A within the same state s. Example 1 provides a demonstration of the difference between an SA and an S-rectangular adversary. Additionally, it is conceivable to further limit the adversary's power by mandating that their choices maintain predefined dependences among all state and action pairs. This type of structural constraint is termed general-rectangularity.\nThe preceding explanations offer heuristic insights into the pivotal concepts recurrently discussed in this paper. For precise and formal definitions, please refer to Section 2.\nAs mentioned earlier, the focus of this paper is to identify scenarios in which the optimal policy satisfies the distributionally robust variant DPP, a.k.a. distributionally robust Bellman equation:\nu(s) = sup d\u2208Q inf ps\u2208Ps E a\u223cd [r(s, a)] + \u03b3E s \u2032 \u223cd\u2297ps [u(s \u2032 )], s \u2208 S, (1.2)\nwhere \u2297 denotes the product of measures and Q represents the set of probability distributions if the controller is allowed to be randomized, whereas Q denotes the action set (understood as Dirac measures) when the controller is limited to being deterministic. P s is the adversary's action distribution set (also known as an ambiguity set in the DRMDP literature) at state s.\nIn view of this objective, it is natural to assume an SA or S-rectangular adversary, as achieving a DPP with complete generality might not be feasible when dealing with a general-rectangular adversary [Le Tallec, 2007, Wiesemann et al., 2013.\nIn the subsequent discussion, we provide a concise review of the results of the existing literature, drawing comparisons to the implications outlined in the theoretical framework of this paper. This comparison is presented through tables, specifically Table 1, 2, 3, and 4. These tables are structured based on the rectangularity and convexity characteristics of the adversary, as well as whether the controller's actions are deterministic or randomized. Here's a breakdown:\n\u2022 Table 1 concerns results with an SA-rectangular adversary. The controller's actions may either be randomized or restricted to deterministic choices.\n\u2022 Table 2 considers scenarios with an S-rectangular adversary operating in a convex action set, while the controller opts for fully randomized actions.\n\u2022 Table 3 assumes an S-rectangular adversary that takes action in a possibly non-convex set, while the controller can choose randomized actions.\n\u2022 Table 4 also assumes an S-rectangular taking actions in a convex or possibly non-convex set. However, the controller is constrained to choose deterministic actions.    Within these tables, a green check mark signifies the validation of the DPP, as defined in Definition 4. Conversely, a red cross indicates a counterexample discussed in Section 5. We address all the 36 cases in the tables and beyond, either confirming or disproving them. While some findings rediscover results in the previous literature, as evidenced by citations in table entries, we present streamlined proofs of the existence of DPPs grounded in unified principles.\nAs summarized in Section 3.4, the unifying principles stem from two perspectives: the convexity of the action sets of the controller and the adversary, and the interchangeability of the sup-inf within the DR Bellman equation (3.1).\nFrom the convexity viewpoint, the six cases located in the lower triangles of all tables consistently hold without requiring convexity assumptions, as proved in Theorem 1. If the controller opts for randomized actions from a convex set of distributions, then, additionally, the scenario where a history-dependent controller interacts with an S-rectangular Markov adversary with a possibly non-convex action set results in a DPP. It's important to note that even if the adversary's action set is convex, a non-convex controller could lead to the same non-existence of DPPs as in the non-convex adversary case. This is also indicated in Table 4.\nFrom the interchangeability perspective, if the interchanged Bellman equation (3.2) yields the same solution as (3.1), then the DPPs hold for all 9 cases of information structures of the two parties. This includes the case where the adversary is SA-rectangular, validating Table 1.\nThe convergence of these two perspectives occurs when the conditions of Sion's min-max principle [Sion, 1958] are met-specifically, when both parties have convex action distribution sets, and one of them is compact. Consequently, the interchanged equation (3.2) has the same solution as (3.1), ensuring that all 9 cases adhere to their respective DPPs. This results in the validity of Table 2.\nWe further establish the anticipated implication of the DPP: the optimality of a Markov time-homogeneous controller. Moreover, we demonstrate that such an optimal policy can be derived directly from the solution to the DR Bellman equation. Consequently, we establish that the satisfaction of DPP validates the DRRL approaches that aim to learn an optimal robust policy by approximately solving the DR Bellman equation, either for the value function (3.1) in the S-rectangular case, or for the q-function (3.4) in the SA-rectangular case.\nFinally, we establish counterexamples for all cases that cannot be covered by the previously mentioned principles, thus completing our unified framework. A notable insight behind the construction of these counterexamples is that the non-existence of a DPP is closely related to the controller's ability to learn certain characteristics of the worst-case adversary over a sequence of strategically deployed actions. Therefore, an adept controller actively engages in learning, and the existence of a DPP is tied to the controller's inability to learn within the given information structure. Consequently, in constructing counterexamples, it proves fruitful to conceptualize the controller as an agent in a multi-armed bandit or RL problem, as opposed to an MDP problem.", "publication_ref": ["b32", "b9", "b16", "b59", "b21", "b58", "b41", "b10", "b23", "b16", "b58", "b21", "b58", "b51"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_1", "tab_2", "tab_3", "tab_3", "tab_0", "tab_1"]}, {"heading": "Criteria on the Selection of Different Formulations", "text": "In this subsection, we provide guidance on selecting the most suitable formulation from the 36 cases presented in Tables 1 to 4 for the DRRL problems. Our criteria for selection are based on four main factors: rectangularity, information structure, convexity of adversaries, and the policy classes of controllers, whether deterministic or randomized. As we shall elaborate in the proceeding discussions, a variety of attributes related to the adversary in the underlying DRMDP not only lead to a formulation that facilitates robust decision-making in a dynamic environment but also function as modeling tools to mitigate various forms of model miss-specification.\nRectangularity of the adversary:\n\u2022 SA-rectangular adversary: SA-rectangular adversary models emerge naturally when a RL agent is expect to encounter distinct states or environments after executing different actions. For instance, consider an autonomous driving agent determining whether to turn left, right, or proceed straight at an intersection. Once the agent makes a decision on the action to take, it will observe entirely different states. Therefore, it is reasonable to assume that the stochasticity in subsequent state transitions resulting from one action is unrelated of the probability structure of state transitions resulting from any other action. In such instances, SA-rectangularity could serve as an appropriate attribute of the adversary in the underlying DRRL problem. Ding et al. [2023] successfully adopted the DRRL formulation with a special SA-rectangular ambiguity set and proposed an algorithm that outperforms all baselines in the autonomous driving tasks.\n\u2022 S-rectangular adversary: Compared to SA-rectangular adversaries, S-rectangular adversaries exhibit a more constrained influence, limiting the set of transition probabilities by enforcing specific correlations among different actions. As we will demonstrate in the inventory control Example 1, in this modeling environment, it is natural to assume that at each time t, the adversary is restricted so that it cannot use the information of the realization today's ordered quantity (action). This feature leads to the use of S-rectangular adversarial decision rules. Here, S-rectangularity allows the modeler to limit the power of the adversary so that it cannot use the realization of the controller's action. Owing to this refinement, the resulting robust policies tend to be less conservative. In addition, adding reasonable constraints could facilitate the calibration of the uncertainty set; see, for example, Lotidis et al. [2023].\nIt is important to note that SA-rectangular adversaries are more powerful the S-rectangular counterparts, which may result in the derivation of a more conservative policy. Nevertheless, from a computational perspective, for SA-rectangular adversaries, the existence of a DPP is guaranteed (Table 1), and a q-function [Watkins and Dayan, 1992] with its Bellman equation is definable (equation (3.6)). These characteristics facilitate the design of model-based [Zhou et al., 2021, Panaganti and Kalathil, 2021, Shi and Chi, 2022, Xu et al., 2023 and model-free [Liu et al., 2022, Wang et al., 2023a,b, Yang et al., 2023 DRRL algorithms, enhancing the efficiency of the learning process.\nInformation structure of the adversary:\n\u2022 Markov time-homogeneous adversary: Markov time-homogeneous formulation is widely analyzed in the (DR)MDP literature. An adversary with this attribute is compelled to predefine one transition kernel and adhere to its transition probability across all time. Hence, such an adversary is sometimes referred to as static in the literature. Although this formulation is preferred and commonly used by practitioners and DRRL theorists due to its structural simplicity and ease of interpretation [Wiesemann et al., 2013, Zhou et al., 2021, Grand-Clement et al., 2023, we demonstrate that the corresponding DRMDP problem could pose significant computational challenges due to the absence of a DPP and, consequently, the sub-optimality of time-homogeneous policies. Specifically, we illustrate that DPPs do not exist in general if the ambiguity set is not convex (Table 3) or if the controller is deterministic (Table 4). Therefore, caution is advised for researchers and practitioners when attempting to derive optimal robust control using this formulation. In scenarios where a DPP does not exist, the development of new exact or approximation methods for computation should be attempted on a case-by-case basis.\nIt's worth noting that previous DRRL papers commonly adopt the modeling perspective of a timehomogeneous adversary [Zhou et al., 2021, Panaganti and Kalathil, 2021, Liu et al., 2022, Shi and Chi, 2022, Xu et al., 2023. However, these papers all refer to the optimality of deterministic Markov time homogeneous policies and Bellman equations established in Iyengar [2005], where the principal modeling assumption involves a Markov adversary and a history-dependent controller (as summarized in Table 1). In this paper, we establish the existence or non-existence of the DPP and Bellman equation for both the value and the Q-function in the case of a Markov time-homogeneous adversary, providing the theoretical foundation for these works.\n\u2022 Markov adversary: Markov adversaries is the natural modeling choice when the environment is believed to exhibit Markov transitions under the prescribed state and action spaces. However, this attribute permits non-stationary transition probabilities. Therefore, a Markov adversary could be an appropriate modeling assumption when the probability structure in the underlying Markov dynamics can vary across time. For instance, in the aforementioned inventory Example 1, the demand distribution could change from day to day, resulting in possible time-nonhomogeneity. From a computational perspective, DPPs also always exist with randomized controllers, see the second columns of Tables 1 and 3.\n\u2022 History-dependent adversary: History-dependence enables the adversary's selection of different distributions based on various histories realized over time. Even in situations where the modeler strongly believes that the environment adheres to Markov dynamics, the presence of partial observation or incomplete modeling of the system state usually results in the optimal controller being history-dependent. This, in turn, gives rise to non-Markov state transition dynamics. Therefore, formulating the DRRL problem with a history-dependent adversary can not only fortify the optimal controller against potential non-Markov shifts in the transition structures of the environment, but also can be wielded as a tool to hedge against errors stemming from the use of a Markov model in cases where the underlying Markov states are misspecified or partially observable.\nMoreover, this attribute of the adversary is also relevant in environments where the underlying dynamics or the data generating process is not an MDP, but the practitioner chooses to employ a DRRL model as an approximation due to computational or data efficiency considerations. This situation arises, for instance, when the RL agent attempts to achieve learning and optimize its policy through strategic interaction with the environment. For example, consider an RL agent learning bidding strategies in online advertising auctions [Cai et al., 2017]. In recent years, the industry has witnessed a paradigm shift, transitioning from the established second-price auctions to the first-price auction model 1 . In second-price auctions, it is widely recognized that truthful bidding is a weakly dominant strategy; however, this is not the case in first-price auctions. Suppose that the RL agent learns from an exchange with secondprice auctions and is poised to deploy a policy in another exchange governed by first-price auctions.\nIn this scenario, the environment comprises numerous competing strategic users, potentially reacting distinctively to the RL agent's varied actions. As the competing strategic users are learning to respond to the strategy of the agent in the first-price auctions, they may take into account the historical behavior of the RL agent. Therefore, this may render the underlying environment non-Markov. In this setting, a non-Markov control formulation that specifically models this learning process might be excessively intricate to formulate, learn, or optimize, while a DRRL formulation that exhibits a DPP could serve as a computationally feasible approximation.\nFurthermore, from a computational standpoint, this formulation is also advantageous, as evidenced by the guaranteed existence of DPPs, highlighted in the first columns of Tables 1 to 4.\nWe remark that, by symmetry, the controller can also adopt these attributes pertaining their information availability. The rationale behind selecting these information structures for the controller mirrors that for the adversary. Consequently, we opt not to delve into a separate discussion on this matter. Convexity of the adversarial ambiguity sets:\n\u2022 Convex adversary: DRMDP and DRRL models with convex adversaries are prevalent in literature, with instances spanning various ambiguity sets such as optimal-transport-based [Ta\u015fkesen et al., 2023, Yang, 2021, convex f -divergence-based , TV-distance-based , as well as the special ambiguity set employed in Xu and Mannor [2010], Wiesemann et al. [2013], Ding et al. [2023]. Our findings, as presented in Tables 1 and 2, further underscore the computational benefits of convex adversaries. Nevertheless, a word of caution is necessary when navigating scenarios involving a deterministic controller paired with an S-rectangular convex adversary, as the DPP may not be applicable in this case.\n\u2022 Non-convex adversary. A non-convex adversary could be a modeling choice when the underlying system dynamics is believed to be deterministic [Post and Ye, 2013]. Also, the utilization of a nonconvex adversary can also stems from the adoption of unions of convex adversarial ambiguity sets. This choice may be the consequence of a data-driven calibration aiming to adjust the size and geometry of ambiguity sets, as proposed by Hong et al. [2020]. From a computational standpoint, the presence of non-convex adversaries not only introduces complexity in solving the distributionally robust variant of the Bellman equations for both the value (3.1) and the q-function (3.4), but our research findings also suggest that the DPP may not be applicable at all. This observation holds true for both deterministic and randomized controller policies, as summarized in Table 3 and Table 4.\nPossibility of randomization (convexity) of the controller:\n\u2022 The randomized controller policy class: While it is well-known that deterministic policies could achieve optimality in the traditional (non-DR) MDP setting, it is in general not the case under the presence of a S-rectangular adversary. This behavior is first observed and analyzed in Wiesemann et al. [2013]. Hence, in the pursuit of optimal robust control or DRRL, the freedom to choose randomized controller policies becomes essential. Furthermore, from the perspective of the existence of a DPP, randomized controllers offer an additional guarantee, when a history-dependent controller policies is paired with Markov adversaries. This is evident from a comparison of Table 3 and 4.\n\u2022 The deterministic controller policy class: While optimality may not always be achieved with deterministic policies, implementing randomized policies can prove challenging in various real-world scenarios. Take, for instance, the context of medical treatment planning, where a patient's current health condition dictates the selection from a range of dynamic treatment regimes [Chakraborty and Murphy, 2014]. In such a scenario, a randomized policy would necessitate assigning probability distributions across various treatments for a specific health state, a practice that is neither legally permissible nor ethically acceptable.\nHowever, when restricting our focus to deterministic policies, it is important to note that the absence of a DPP implies that Markov time-homogeneous policies are not assured to be optimal in numerous scenarios, even in the presence of a convex adversary. This is highlighted in Table 4. To pursue higher rewards, the controller might explore the implementation of non-stationary or even history-dependent policies.\nIt's worth noting that as pointed out in the previous section, while we currently introduce only randomized and deterministic controller policies, the subsequent sections will build on the convexity of the controller policy classes. Specifically, the randomized controller policy class represents a convex controller, whereas the deterministic controller policy class is an instance of a non-convex controller. The findings presented in Table 3 and 4 extend to general convex and non-convex controller policy classes, respectively. From both a modeling and learning standpoint, the presence of a DPP for a set of controller policies with general geometries opens avenues for potential extensions of DRRL algorithms. These extensions can be tailored to scenarios where the controller is constrained to employ only a subset of randomized decisions.", "publication_ref": ["b6", "b26", "b57", "b66", "b35", "b46", "b60", "b25", "b55", "b58", "b66", "b11", "b66", "b35", "b25", "b46", "b60", "b4", "b54", "b61", "b59", "b58", "b6", "b38", "b58", "b5"], "figure_ref": [], "table_ref": ["tab_0", "tab_0", "tab_2", "tab_3", "tab_0", "tab_0", "tab_0", "tab_0", "tab_2", "tab_2", "tab_3", "tab_2"]}, {"heading": "Literature Review", "text": "This paper contributes to the existing literature by offering a unified perspective on the existence of a DPP in the context of DRMDP, encompassing various information structures and adversarial rectangularity. This unification leads to the identification of new results regarding the existence and non-existence of DPP under various policy structures of the controller and the adversary. To appreciate this contribution, we begin with a concise overview of the relevant literature in this domain. The exploration of DRMDP models is motivated by the pursuit of optimal decision-making in stochastic environments, where the underlying probability structure is shaped by a decision maker (referred to as the controller) and an opposing counterpart (referred to as the adversary), engaged in a zero-sum interaction. This naturally aligns with the literature on two-player zero-sum stochastic games [Shapley, 1953, Solan andVieille, 2015], a connection we will explore subsequently in Section 6.2. However, unlike traditional stochastic games, in DRMDP, the adversary's choices are confined to a predetermined subset of transition distributions, while the controller typically still has the liberty to choose an arbitrary randomized action.\nThe presence of a DPP is a highly desirable property of the (DR)MDP. With the DPP in place, there exists an optimal robust policy for the controller that is stationary Markov deterministic. The DPP not only offers valuable theoretical insights into the underlying dynamics but also serves as a crucial computational tool for achieving policy learning, with or without full knowledge of the probability structure of the underlying model. Therefore, the focus of DRMDP studies is on establishing conditions that facilitate the presence of a DPP while maintaining the necessary structural integrity tailored to the modeled environments. To achieve this, previous works considered adversarial environments exhibiting a property known as rectangularity. The concept of a rectangular adversary was first introduced by Iyengar [2005] and Nilim and El Ghaoui [2005]. In these foundational works, what is now recognized as an SA-rectangular adversary was considered, and the existence of a DPP was demonstrated, as summarized in Table 1. This assumption also underlies the pioneering work in min-max control by Gonz\u00e1lez-Trejo et al. [2002], where the same DPP is established within the context of a history-dependent adversary.\nHowever, as illustrated in Example 1 below, the SA-rectangular adversary may wield excessive power in many practical modeling environments. This leads to the introduction of S-rectangularity by Le Tallec [2007], Xu and Mannor [2010], and Wiesemann et al. [2013]. Notably, these works consider specific convex Srectangular adversaries and establish the existence of a DPP, as outlined in Table 2. Unlike the SA-rectangular setting, Wiesemann et al. [2013] shows that, in general, the optimal stationary Markov robust policy needs to be randomized.\nBuilding on this, Le Tallec [2007] and Wiesemann et al. [2013] also introduce the concept of a general rectangular adversary and establish the non-existence of a canonical DPP. To further constrain the adversary's power and preserve the structural integrity of the modeled environment, the recent work by Goyal and Grand-Cl\u00e9ment [2023] introduces a new notion known as R-rectangularity.\nIn addition to rectangularity, another crucial factor influencing the existence or non-existence of the DPP is the asymmetry in information availability between the controller and the adversary. Iyengar [2005] noted, without offering a proof, that the asymmetry in historical information structure between the controller and the adversary dynamics could lead to the non-existence of a stationary optimal policy for maximizing infinitehorizon discounted rewards. This insight is also noted by Wiesemann et al. [2013, Table 1, S-rectangular, nonconvex] without giving a counterexample. The work by Gonz\u00e1lez-Trejo et al. [2002] approaches the minmax control problem with symmetric history-dependent controllers and adversaries. On the other hand, Iyengar [2005], Nilim and El Ghaoui [2005], Xu and Mannor [2010], Wiesemann et al. [2013] take into account non-symmetric information structures, as indicated in the tables.\nMuch like the formulations of DRMDPs using the language of classical MDPs, there exists a closely related line of research to DRMDP that studies the distributionally robust variant of multistage stochastic programs (MSPs), see for example , Shapiro [2021], Pichler and Shapiro [2021], Shapiro et al. [2021, Chapter 7] and references therein. While this literature primarily focuses on finite-horizon environments, it shares strong connections with our DRMDP framework. In this paper, we provide a discussion on the reformulation of our DRMDP problems with history-dependent controllers and adversaries as distributionally robust MSPs.\nThere exists a substantial body of literature dedicated to investigating the finite-time sample complexity of achieving optimality in tabular DRRL environments. This line of research crucially relies on DPPs and specifically focuses on divergence-based ambiguity sets of SA-rectangular adversaries. In line with the classical tabular RL setting, the approaches in these algorithms can be categorized based on whether the algorithm explicitly estimates the entire transition kernel. If it does, it falls under the scope of a model-based approach. Notable papers adopting a model-based approach include Zhou et al. [2021], Panaganti and Kalathil [2021], , Shi and Chi [2022], Xu et al. [2023], , . Conversely, the literature exploring model-free algorithms includes works such as Liu et al. [2022], Wang et al. [2023a,b], Yang et al. [2023]. In the specific domain of health care, Saghafian [2023] proposes RL methods to learn an optimal dynamic treatment regime in an ambiguous environment.", "publication_ref": ["b52", "b16", "b32", "b9", "b59", "b58", "b58", "b58", "b10", "b9", "b32", "b59", "b58", "b41", "b37", "b66", "b35", "b46", "b60", "b25", "b63", "b39"], "figure_ref": [], "table_ref": ["tab_0", "tab_1"]}, {"heading": "Remark on Paper Organization", "text": "The structure of the paper unfolds as follows: Section 2 formulates DRMDP problems, encompassing various types of controllers and adversaries. Our key findings for scenarios where the DPP holds with full generality are expounded in Section 3. Section 4 delves into the implications of the DPP, specifically addressing the optimality of Markov time-homogeneous policies. In Section 5, we provide counterexamples for cases where the DPP does not hold. Sections 6.1 and 6.2 explore the connections between our DRMDP formulations and distributionally robust multistage stochastic programs and stochastic games, respectively. Finally, in Section 7, we draw conclusions and outline potential avenues for future research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "DRMDPs: Construction and Definitions", "text": "While the theory we present in this paper naturally extends to infinite (countable, continuum, or general measurable) state and action spaces, to facilitate a better understanding of the readers from multiple disciplines, we focus our discussion exclusively on the setting of finite state and action spaces. In particular, let S and A denote the finite state and action spaces, respectively, and equip them with \u03c3-fields S = 2 S and A = 2 A . It should be noted that the formulation in this section is valid for abstract state and action measurable spaces (S, S) and (A, A).\nIn this context of finite states and actions, we consider the canonical space of an MDP. Specifically, let (\u2126 = (|S| \u00d7 |A|) Z \u22650 , F ) be the underlying measurable space where F is the cylinder \u03c3-field. Define the stochastic process {(X t , A t ), t \u2265 0} by the point evaluation X t (\u03c9) = s t , A t (\u03c9) = a t for all t \u2265 0 and any \u03c9 = (s 0 , a 0 , s 1 , a 1 , . . . ) \u2208 \u2126. In classical MDP formulations, the policy of the controller induces a probability measure on (\u2126, F ). In comparison, within the realm of robust MDPs, the interplay between the controller and the adversary dictates a measure on the sample path space. Consequently, to rigorously formulate a robust MDP, we commence by introducing the policy classes of both the controller and the adversary.\nSimilar to regular MDP formulations, a controller typically has the flexibility to choose its action at the current time and space based on all historical information (i.e. the state-action sequence realized until the current time instance) available. In a robust MDP, both the controller and the adversary, in general, are empowered to employ history-dependent policies as well. To establish a formulation of robust MDP with this characteristic, we introduce the definitions of the controller's and adversary's history and their respective notions of history-dependence in the context of a robust MDP setting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Controller's Policy", "text": "Similar to classical MDPs, a history-dependent controller in the context of a robust MDP takes (randomized) actions based on the state-action sequence until the current state. Formally, the controller's history {H t : t \u2265 0} is the t indexed family of truncated sample paths H t := {h t = (s 0 , a 0 , . . . , a t\u22121 , s t ) : \u03c9 = (s 0 , a 0 , . . . , a t\u22121 , s t , . . . ) \u2208 \u2126} .\nWe also define the random element H t : \u2126 \u2192 H t by point evaluation H t (\u03c9) = h t . Given a prescribed subset of probability measures Q \u2282 P(A), a controller policy \u03c0 is a sequence of decision rules \u03c0 = (d 0 , d 1 , d 2 , . . . ). Each decision rule d t , indexed by t, is a measure valued function d t : H t \u2192 Q. We call the set Q the action distributions of the controller.\nThe set of Q-constrained history-dependent controller policies is\n\u03a0 C H := {\u03c0 = (d 0 , d 1 , . . . ) : d t \u2208 H t \u2192 Q, \u2200t \u2265 0} . (2.1)\nNote that the superscript \"C\" stands for constrained. We also suppress the dependence on Q to simplify the notation.\nA decision rule d t is Markov if for all h t , h \u2032 t \u2208 H t s.t. s t = s \u2032 t , then d t (h t ) = d t (h \u2032 t )\n. Therefore, a Markov decision rule d t is indifferent to the state-action sequence (s 0 , a 0 , . . . , s t\u22121 , a t\u22121 ). Consequently, we can express d t (s t ) = d t (h t ) when d t is Markov, employing an abuse of notation for the sake of simplicity. Then, define the set of Q-constrained Markov controller policies is defined as \n\u03a0 C M := {\u03c0 = (d 0 , d 1 , . . . ) : d t \u2208 H t \u2192 Q is Markov, \u2200t \u2265 0} . A contoller's policy \u03c0 = (d 0 , d 1 , . . . )\n\u2208 H t1 , h \u2032 t2 \u2208 H t2 s.t. s t1 = s \u2032 t2 , then d t1 (h t1 ) = d t2 (h \u2032 t2 ).\nIn the case of a Markov time-homogeneous policy, the decision rule remains consistent regardless of the time and history once a state s is specified-hence the term \"static\". Consequently, such a policy can be succinctly represented as \u03c0 = (d, d, d, . . .), where d : S \u2192 P(A). Then, the set of Q-constrained Markov time-homogeneous (or static) controller policies is defined as\n\u03a0 C S := {\u03c0 = (d, d, . . . ) : d \u2208 S \u2192 Q} .\nOf particular interests in RL applications are the following two special type of constraints for controller's decision rule. First, when Q = P(A) the set of probability measures on (A, A), then we have the fully randomized (or unconstrained) history-dependent/Markov/Markov time-homogeneous policy classes:\n\u03a0 H /\u03a0 M /\u03a0 S . Second, if Q = Q D := \u03b4 {a} : a \u2208 A , then we denote the deterministic history-dependent/Markov/Markov time-homogeneous policy classes as \u03a0 D H /\u03a0 D M /\u03a0 D S respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Adversary's Policy", "text": "We define the adversary's history {G t : t \u2265 0} as the t indexed family of truncated sample paths G t := {g t = (s 0 , a 0 , . . . , s t , a t ) : \u03c9 = (s 0 , a 0 , . . . , s t , a t . . . ) \u2208 \u2126} .\nNote that g t represents the concatenation of the history h t with the controller's action at time t, i.e., g t = (h t , a t ), where h t \u2208 H t . Also, define the random element G t : \u2126 \u2192 G t by point evaluation G t (\u03c9) = g t . By\nincorporating this additional a t into the definition of the adversarial history, we provide the modeling flexibility for the adversary to be informed about the realization of the action based on the controller's decision at the current time point. Similar to the controller, an adversarial policy \u03ba := (\u03ba 0 , \u03ba 1 , \u03ba 2 , . . . ) is also a sequence of measure-valued functions of the history \u03ba t : G t \u2192 P(S). For any t \u2265 0, we call \u03ba t the adversarial decision rule at t. Under this formulation, the largest set of adversarial policies K H , a.k.a. the set of unconstrained history-dependent adversarial policies is K H := {\u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 G t \u2192 P(S), \u2200t \u2265 0} .\nAnalogue to the controller-side definitions, we call an adversarial decision rule \u03ba t Markov if \u03ba t if for all g t , g \u2032 t \u2208 G t s.t. (s t , a t ) = (s \u2032 t , a \u2032 t ), then \u03ba t (g t ) = \u03ba t (g \u2032 t ). So, a Markov adversarial decision rule \u03ba t is indifferent to g t\u22121 = (s 0 , . . . , a t\u22121 ) given (s t , a t ). Thus, by an abuse of notation, we can write \u03ba t (s t , a t ) = \u03ba t (g t ).\n(2.2)\nMoreover, an adversary's policy \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) is called Markov time-homogeneous (or static) if for all t 1 , t 2 \u2265 0 and\ng t1 \u2208 G t1 , g \u2032 t2 \u2208 G t2 s.t. (s t1 , a t1 ) = (s \u2032 t2 , a \u2032 t2 ), then \u03ba t1 (g t1 ) = \u03ba t2 (g \u2032 t2 )\n. So, for a Markov time-homogeneous policy, once a state action pair (s, a) is given, the distribution of the next state is the same regardless of the time and history-hence the term \"static\". Again, such policies can be represented as (\u03ba, \u03ba, \u03ba, . . . ) where \u03ba : S \u00d7 A \u2192 P(S).\nIn environments where a robust MDP serves as a viable model, it becomes necessary to limit the adversary's power to a relatively constrained set of policies. This restriction is crucial because an adversary with an unconstrained history-dependent policy class tends to be overly potent, rendering the controller's actions ineffective. Therefore, allowing unrestricted adversarial policy choices in the robust MDP model can result in excessively conservative controller's policies and overly pessimistic estimates of value. On the other hand, in order to achieve reinforcement learning and optimal control, it is desirable for the optimal value to satisfy a dynamic programming principle (DPP), a.k.a. Bellman equation. We will provide a rigorous definition of this principle in the forthcoming sections. These two factors, i.e., restricting the adversary's power and adhering to the Bellman equation, constitute the primary motivation behind the concept of \"rectangularity\" as introduced in Iyengar [2005]. This notion has subsequently been extended and generalized in works such as Wiesemann et al. [2013] and Goyal and Grand-Cl\u00e9ment [2023].\nWe first introduce the notion of rectangularity considered by Gonz\u00e1lez-Trejo et al. [2002], Iyengar [2005], and Nilim and El Ghaoui [2005]. In contemporary terminology, this concept is referred to as SA-rectangularity.", "publication_ref": ["b16", "b58", "b10", "b9", "b16", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "SA-Rectangular Set of Adversary's Policies", "text": "For each (s, a) \u2208 S\u00d7A, let P s,a \u2282 P(S) be a prescribed subset of probability measure, which can be equivalently seen as a subset of row vectors in R |S| . We first consider the SA-rectangular marginal set of history-dependent adversarial decision rules \nK SA H (t) := {\u03ba t : \u03ba t (g t\u22121 , s, a) \u2208 P s,a , \u2200(s, a) \u2208 S \u00d7 A, g t\u22121 \u2208 G t\u22121 } . (2.3) K SA M (t)\n:= \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K SA H (t), \u2200t \u2265 0 = t\u22650 K SA H (t).\nSimilar to the controller policy classes, we define the SA-rectangular set of Markov/Markov time-homogeneous adversarial policies as\nK SA M := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K SA H (t) is Markov, \u2200t \u2265 0 K SA S := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K SA H (t)\nis Markov time-homogeneous, \u2200t \u2265 0 .\n(2.4)\nRecall the alternative notation for Markov and Markov time-homogeneous adversarial decision rule in (2.2), we recognize that the above definition can be equivalently characterized by (2.5)\nK SA M = {(\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t = {\u03ba t (s,\nWe note that to preserve the mathematical rigor in later constructions (e.g. definition (2.15)), (2.5) should be understood in terms of (2.4). A benefit of the representation in (2.5) is that it has a more natural interpretation and hence widely used in the literature; e.g. [Gonz\u00e1lez-Trejo et al., 2002, Iyengar, 2005, Nilim and El Ghaoui, 2005, and formulation (2.21) in Li and Shapiro [2023]. Take the Markov setting as an example, (2.5) can be interpreted as the adversary choosing transition kernel\n\u03ba t = {\u03ba t (s, a) : (s, a) \u2208 S \u00d7 A} \u2208 (s,a)\u2208S\u00d7A\nP s,a =: P SA for each instance of time. The product set P SA is referred to as a SA-rectangular ambiguity set of transition kernels. In this paper, we will take on the notation in (2.4) as it expresses history-dependence in an elegant way. We emphasize that this digression is only meant for a comparison with the terminologies and formulations in the literature.", "publication_ref": ["b9", "b16", "b32", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "S-Rectangular Set of Adversary's Policies", "text": "We also consider the notion of S-rectangularity. To our knowledge, this concept is introduced to the robust MDP literature independently in Le Tallec [2007] and Wiesemann et al. [2013] as a way to further limit the power of the adversary while preserving the dynamic programming principle. As exemplified in Example 1, an S-rectangular adversary could be a more natural assumption in many relevant robust MDP models in the space of operations research and management sciences in comparison to SA-rectangularity. This is often due to that an SA-rectangular adversary has too much freedom, and thus has the potential to violate some natural constraint of the modeled environment. Instead of freely choosing \u03ba t (g t\u22121 , s, a) \u2208 P s,a , in the S-rectangular setting, for a fixed s \u2208 S, choosing adversarial decision rule \u03ba t (g t\u22121 , s, a) = p s,a for one a \u2208 A could affect the possible choice of \u03ba t (g t\u22121 , s, a \u2032 ) for any other a \u2032 \u2208 A. Concretely, fix a prescribed set of measure-valued functions P s \u2282 {A \u2192 P(S)} for every s \u2208 S. An element p s \u2208 P s can be equivalently seen as a R |A|\u00d7|S| matrix, where each row is a probability vector on S. Define the S-rectangular marginal set of history-dependent adversarial decision rules as\nK S H (t) := {\u03ba t : \u03ba t (g t\u22121 , s, \u2022) \u2208 P s , \u2200s \u2208 S, g t\u22121 \u2208 G t\u22121 } (2.6)\nand the S-rectangular set of history-dependent/Markov/Markov time-homogeneous adversarial policies Again, this should be understood as a re-interpretation of (2.7). In this representation, the adversary chooses transition kernels \u03ba t \u2208 s\u2208S P s =: P S instead of history dependent-decision rules, where P S is known as a S-rectangular ambiguity set.\nK S H := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K S H (t), \u2200t \u2265 0 K S M := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K S H (t) is Markov, \u2200t \u2265 0 K S S := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K S H (t)\nWith these definitions we are now ready to introduce a modeling environment where the S-rectangular set of adversarial decision rules can lead to a more natural implementation. Here, we also exemplify a S-rectangular ambiguity set that is not SA-rectangular.\nExample 1 (Simple Inventory Model). Consider an inventory control problem where the next day's inventory level is\nI t+1 = (I t + a t + D t ) +\nwhere {D t : t \u2265 0} i.i.d. is the demand process and a t is the ordered inventory. In this modeling environment, it is natural to assume that at each time t, the adversary can only change the law of D t dependent on I t but not dependent on the controller's action a t . This leads to an S-rectangular set of adversarial decision rules. However, if we instead assume SA-rectangularity, then the adversary must be able to freely choose different laws for D t for different controller's action a; say if a t is large, then the adversary chooses the demand to be small, and if a t is small, chooses the demand to be large. Through this example, one can see that the S-rectangularity allows the modeler to limit the power of the adversary so that it cannot use the realization of the controller's action. Literature explores risk-averse distributionally robust control of inventory models, employing a wide array of adversaries and establishing properties of the optimal controller; for instance Shapiro and Xin [2020] and Shapiro [2022].\nFrom these definitions, it is evident that an SA-rectangular set of adversarial policies is S-rectangular with\nP s := {p s = {p s,a : a \u2208 A} : p s,a \u2208 P s,a } = a\u2208A P s,a\n(2.8)\nthe Cartesian product set. We say that P s is SA-rectangular if it admits a product representation as in (2.8) for some {P s,a : a \u2208 A}.", "publication_ref": ["b58", "b43", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "General Rectangular Set of Adversary's Policies", "text": "For completeness, we also introduce the general (a.k.a. non-rectangular in the literature, see Wiesemann et al. [2013]) sets of adversarial decision rules. In this case, choosing adversarial decision rule \u03ba t (g t\u22121 , s, a) = p s,a\nfor one pair of (s, a) \u2208 S \u00d7 A could affect the possible choice of \u03ba t (g t\u22121 , s \u2032 , a \u2032 ) for any other (s \u2032 , a \u2032 ) \u2208 S \u00d7 A. Concretely, fix prescribed subset P \u2282 {S \u00d7 A \u2192 P(S)} where each p \u2208 P is a transition kernel of the form {p s,a (s \u2032 ) : s, s \u2032 \u2208 S, a \u2208 A}, which can be equivalently seen as a R |S||A|\u00d7|S| matrix. For any t \u2265 0, Define the general rectangular set of adversarial decision rules\nK N H (t) := {\u03ba t : \u03ba t (g t\u22121 , \u2022, \u2022) \u2208 P, \u2200g t\u22121 \u2208 G t\u22121 } . (2.9)\nThe general history-dependent/Markov/Markov time-homogeneous adversarial policy classes K N H /K N M /K N S can be defined analogously. We omit the presentation.", "publication_ref": ["b58"], "figure_ref": [], "table_ref": []}, {"heading": "Adversary's Policy: Summary of Notations", "text": "For future references, we summarize the definitions and notations concerning adversary's policy classes and discuss some elementary properties. At the heart of these definitions lie the indexed sets P s,a , P s , and P, all of which we will refer to as adversarial action sets. This name comes from the fact that a SA/S/general rectangular adversary can be viewed as choosing an action within P s,a /P s /P, see definitions (2.3)/(2.6)/(2.9). However, it is crucial to emphasize that in the context of finite state and action spaces, the elements within each of these sets possess varying dimensions when viewed as vectors and matrices. To clarify these concepts and their respective representations, we summarize them in Table 5.  5 that the concept of rectangularity is closely tied to the extent of historical information available to the adversary before making a decision. To illustrate, consider the following scenarios: An SA-rectangular adversary at time t possesses visibility into g t = (g t\u22121 , s t , a t ) and hence can be seen as making a choice from the action set p st,at \u2208 P st,at . In contrast, an S-rectangular adversary sees only (g t\u22121 , s t ) and must determine the distribution of the next state for each possible action a t . Consequently, it selects an action p st \u2208 P st . It is possible to further reduce the amount of historical information accessible to the adversary at the time of decision making, leading to the definition of even more general adversarial policy classes. However, such settings typically devoid the DPP and are therefore beyond the scope of this paper's concerns. We remark that this perspective of information availability is more explicitly represented by the multistage stochastic program formulation in Section 6.1.\nThese distinctions emphasize how rectangularity relates to the depth of information available to the adversary at the time of decision-making. It is a reasonable inference that the extent of information available to the adversary directly correlates with its power. In this context, the SA-rectangular adversary, which has access to the most information, would be the most powerful one. This holds true when the action sets are compatible in the sense of being derived from the marginalization of a common general rectangular action set P, which we will explain later, c.f. (2.12).\nNext, we present Table 6 which provides a concise summary of the various adversarial policy classes and the adversarial action sets.\nAn element within any of the policy classes listed in Table 6 can be represented as \u03ba = (\u03ba 0 , \u03ba 1 , . . .), where where SA can be replaced with S or N. Note that this can be memorized as \"top contains bottom\" in reference to Table 6. One recognizes that a large adversarial policy class implies that the adversary enjoys greater flexibility in selecting its policies, thereby becoming more powerful. Hence, by (2.10), a history-dependent adversary possesses greater power compared to a Markov adversary, and, in turn, a Markov adversary is more powerful than a Markov time-homogeneous adversary. Given a general rectangular adversarial action set P, we can construct S or SA-rectangular action sets {P s : s \u2208 S} or {P s,a : s \u2208 S, a \u2208 A} by marginalization: P s := {p s : p \u2208 P} and P s,a := {p s,a : p \u2208 P} .\n(2.11)\nLet the SA and S-rectangular adversarial action sets {P s,a : s \u2208 S, a \u2208 A} and {P s : s \u2208 S} be formed by via marginalizing P as in (2.11), and we construct the adversary's policy classes K SA H /K S H /K N H using the action sets {P s,a : s \u2208 S, a \u2208 A} / {P s : s \u2208 S} /P, respectively. Then, as noted before, we have the inclusion relation\nK SA H \u2283 K S H \u2283 K N H (2.12)\nwhere H can be replaced with M or S. This can be remembered as \"left contains right\" by referencing Table 6.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_6", "tab_6", "tab_7", "tab_7", "tab_7", "tab_7"]}, {"heading": "The Max-Min Control Problem", "text": "With the careful establishment of the controller's and adversary's policy classes in this section, we lay the foundation for the subsequent definition of the max-min control value of a robust MDP. The formulation of the value function allows us to formalize robust policy learning and decision-making using the robust MDP framework.\nWe start with introducing some notations. For function f on the measurable space (E, E) and measure \u03bd \u2208 P(E), we define the integral \u03bd[f ] := With these notations in place, we are now equipped to articulate the collective effect of a pair (\u03c0, \u03ba), comprising the controller's and adversary's policies, from the point of view of the probabilistic characteristics of the state-action sequence. This effect is manifested by uniquely defining the distribution of the process {(X t , A t ) : t \u2265 0}.\nDefinition 1 (Induced Probability Measure). For any \u03c0 \u2208 \u03a0 H , \u03ba \u2208 K H , and initial distribution \u00b5 \u2208 P(S), define P \u03c0,\u03ba \u00b5 to be the unique probability measure on (\u2126, F ) such that\nP \u03c0,\u03ba \u00b5 (G t (\u03c9) = g t ) = \u00b5(s 0 )d 0 (s 0 )[a 0 ]\u03ba 0 (s 0 , a 0 )[s 1 ]d 1 (s 0 , a 0 , s 1 )[a 1 ]\u03ba 1 (s 0 , a 0 , s 1 , a 1 )[s 2 ] . . . d t (h t )[a t ] (2.15)\nfor all t \u2265 0 and g t = (s 0 , a 0 , . . . , s t , a t ) \u2208 G t .\nWe remark that (2.15) defines a measure on the cylinder sets on \u2126, hence uniquely extends to a measure on (\u2126, F ). Also, note that (2.15) determines the finite dimensional distribution of the stochastic process {(X t , A t ) : t \u2265 0}. Now, we are ready to define the max-min control value of a robust MDP problem. Let r : S \u00d7 A \u2192 [0, 1] be a bounded non-negative reward function, and \u03b3 \u2208 (0, 1) be the discount factor.\nDefinition 2 (Max-Min Control Value). Let E \u03c0,\u03ba \u00b5 denote the expectation w.r.t. measure P \u03c0,\u03ba \u00b5 . We define the value of the triple (\u00b5, \u03c0, \u03ba) as\nv(\u00b5, \u03c0, \u03ba) := E \u03c0,\u03ba \u00b5 \u221e k=0 \u03b3 k r(X k , A k ) .\nFor any subsets \u03a0 \u2282 \u03a0 H and K \u2282 K H , we define the max-min control optimal value associated with the triple (\u00b5, \u03a0, K) by v * (\u00b5, \u03a0, K) := sup\n\u03c0\u2208\u03a0 inf \u03ba\u2208K v(\u00b5, \u03c0, \u03ba). (2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "16)", "text": "To familiarize the reader with this definition in conjunction with the policy classes of the controller and the adversary, we now introduce the following set of elementary properties of the max-min control optimal value. Lemma 1. Recall the definitions of the controller's and adversary's policy classes. Then,\nv * (\u00b5, \u03a0 C H , K) \u2265 v * (\u00b5, \u03a0 C M , K) \u2265 v * (\u00b5, \u03a0 C S , K) for K = K S H , K S M , K S S and v * (\u00b5, \u03a0, K S H ) \u2264 v * (\u00b5, \u03a0, K S M ) \u2264 v * (\u00b5, \u03a0, K S S ) for \u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S as defined in (2.1).\nThis lemma follows from the inclusion relationships of the policy classes, see Appendix B.1 for a proof. Note that since a SA-rectangular set of adversarial policies is S-rectangular as explained in (2.8), Lemma 1 is also true if we replace K S H , K S M , K S S with K SA H , K SA M , K SA S . In this section, we have formally constructed the controller's and adversary's policy classes and the probability representation of the max-min optimal value. Our journey now leads us to address two fundamental questions: When does the optimal value satisfy a dynamic programming principle (DPP)? Is there an optimal policy that is time-and history-independent? The former will be addressed in Section 3, while the latter will be explored in Section 4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dynamic Programming Principles", "text": "As previously highlighted, the motivation behind defining rectangular sets of adversarial policies is to ensure that the max-min control problem adheres to a DPP. This is manifested in the form of the value function being the solution to the DR Bellman equation, a concept that we shall define next. We have noted that it is in general impossible to formulate a DPP for the general rectangular adversary. So, in the following sections, we will restrict our attention to the S-rectangular case.\nDefinition 3 (DR Bellman Equation). Given the constraint set of action distributions Q \u2282 P(A) and Srectangular adversarial action sets P s \u2282 {A \u2192 P(S)}, s \u2208 S, define the distributionally robust Bellman equation as\nu(s) = sup d\u2208Q inf ps\u2208Ps d[r(s, \u2022)] + \u03b3d \u2297 p s [u], s \u2208 S. (3.1)\nMoreover, we interchange the infimum and supremum to introduce the following inf-sup equation. This equation plays a crucial role in deducing sufficient conditions for the optimal value v * to satisfy a DPP:\nu(s) = inf ps\u2208Ps sup d\u2208Q d[r(s, \u2022)] + \u03b3d \u2297 p s [u], s \u2208 S. (3.2)\nIn the context of finite state and action spaces DRMDPs, a bounded solution of the DR Bellman equation (3.1) always exists and is unique: Proposition 1. There exists a unique solution u * to (3.1). Moreover,\nu * \u221e \u2264 1/(1 \u2212 \u03b3).\nThis proposition follows from the well-known contraction property of the distributionally robust Bellman operator. We include a proof in Appendix B.2 so as to make the work self-contained.\nHaving confirmed the existence of a solution to the Bellman equation, we define the satisfaction of the DPP as the max-min optimal value being identified with that solution in the following sense.\nDefinition 4 (Dynamic Programming Principle). Given the action distribution sets Q, {P s : s \u2208 S}, we say that the pair of controller and adversary's policy class (\u03a0, K) satisfies the dynamic programming principle (DPP) if the solution u * to the DR Bellman equation (3.1) satisfies \u00b5[u * ] = v * (\u00b5, \u03a0, K) for all \u00b5 \u2208 P(S).\nAs highlighted in the introduction, the significance of the satisfaction of a DPP in the context of DRRL lies in the fundamental role played by the Bellman equation, which serves as the primary computational tool underlying nearly all RL algorithms. In particular, there is no known computationally tractable algorithms that can achieve policy learning within the DRRL framework where the solution of the Bellman equation doesn't correspond to the max-min optimal value.\nWith the DPP defined, we proceed to systematically explore the satisfaction of a DPP under various assumptions regarding the policy classes of the controller and the adversary. A concise summary of the results can be found in the tables provided earlier and Section 3.4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Maxmin Optimal Values and Bellman Equation: The General Case", "text": "We first consider the most general setting formulated in this paper where the action sets of the controller Q \u2282 P(A) and the S-rectangular adversary P s \u2282 {A \u2192 P(S)} are arbitrary prescribed subsets. At this level of generality, the following theorem holds: Theorem 1. Let u * : S \u2192 R + be the unique solution of (3.1). Then,\n\u00b5[u * ] = v * (\u00b5, \u03a0 C H , K S H ) = v * (\u00b5, \u03a0 C M , K S H ) = v * (\u00b5, \u03a0 C M , K S M ) = v * (\u00b5, \u03a0 C S , K S H ) = v * (\u00b5, \u03a0 C S , K S M ) = v * (\u00b5, \u03a0 C S , K S S )\nfor all \u00b5 \u2208 P(S).\nThe proof of Theorem 1 is deferred to Appendix A.1. Since a SA-rectangular set of adversarial policies is S-rectangular as constructed in (2.8), the same result as in Theorem 1 is also true for the SA-rectangular adversaries. Therefore, Theorem 1 resolves the existence of the DPP for all 6 \u00d7 4 cases corresponding to the lower triangular portion of Tables 1 -4.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Deterministic Controller Policies", "text": "An important special case is when Q = Q D = \u03b4 {a} : a \u2208 A . A deterministic rule is naturally uniquely identified with a function d t : H t \u2192 A. We will always assume such identification and use d t (h t ) as a measure or an action interchangeably.\nAs clarified in Wiesemann et al. [2013], in the S-rectangular setting with Q = Q D for \u03a0 C and Q = P(A) for \u03a0, v * (\u00b5, \u03a0 C , K) < v * (\u00b5, \u03a0, K) could happen for K = K S H , K S M , K S S . Thus, there could be no deterministic policy that is optimal for the controller that is allowed to take randomized actions. However, if one is to constrain the controller to take policies that chooses deterministic action, it is still instrumental to define the q-function and its Bellman equation as in the classical MDP settings. The following corollary links q * and u * to the solution of (3.4), and hence establishes an alternative equivalent formulation of the DPP in the case of deterministic controller.\nCorollary 1.1. q * is the unique solution to (3.4) and u * (\u2022) = max a\u2208A q * (\u2022, a) satisfying Theorem 1.\nProof of Corollary 1.1. The uniqueness follows from the same contraction argument as in the proof of Proposition 1. Moreover,\nu * (s) = sup d\u2208Q D d[r(s, \u2022)] + \u03b3 inf ps\u2208Ps d \u2297 p s [u * ] = max a\u2208A r(s, a) + \u03b3 inf ps\u2208Ps p s,a [u * ] = max a\u2208A q * (s, a).\nPlug this into (3.3), one sees that q * is indeed the solution, hence equivalently solves (3.4). Lastly, Theorem 1 still holds in the context of Q = Q D . This corresponds to all presences of six check marks in Table 4. Subsequently, we will examine and establish the optimality attributed to the greedy policies derived from the optimal q * -function in Remark 5. Moreover, we will provide counterexamples in Section 5.3, substantiating the red crosses within Table 4.", "publication_ref": ["b58"], "figure_ref": [], "table_ref": ["tab_3", "tab_3"]}, {"heading": "The Inf-Sup Equation and the Minimax Theorem", "text": "In this section, we first go back to the scenario where Q \u2282 P(A) denotes an arbitrary subset containing action distributions. Our focus shifts towards assessing the validity of the DPP given by the maxmin Bellman equation (3.1). This investigation encompasses an additional assumption wherein the solution u * of (3.1) concurrently satisfies (3.2). Apart from establishing the DPP within Table 2, our pursuit involves addressing the subsequent query: Does it always hold at this level of generality that the optimal maxmin value in (2.16) conforms to a minimax theorem? In other words, does interchanging the sup and inf operations in (2.16) always preserve the value? The answer is yes, and is proved in Theorem 2. From this theorem, the validity of Table 2 is then established.\nTheorem 2. Let u * be the solution of the DR Bellman equation (3.1). Assume u * further satisfies the inf-sup equation (3.2). Then, the following properties holds for all \u00b5 \u2208 P(S) and any mixture of\nK = K S H , K S M , K S S and \u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S : 1. The optimal values satisfy v * (\u00b5, \u03a0, K) = \u00b5[u * ].", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "The interchange of sup-inf preserves the optimal values:", "text": "v * (\u00b5, \u03a0, K) = sup \u03c0\u2208\u03a0 inf \u03ba\u2208K v * (\u00b5, \u03c0, \u03ba) = inf \u03ba\u2208K sup \u03c0\u2208\u03a0 v * (\u00b5, \u03c0, \u03ba).\nTheorem 2 implies that if the supremum and the infimum in the Bellman equation (3.1) interchange, then so does the sup and inf in the optimal value function. Theorem 2 is proved in Appendix A.2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sion's Minimax Principle", "text": "In light of the exchange of the order of inf-sup in (3.1) and (3.2), a natural sufficient condition that can guarantee the assumption of Theorem 2 can be deduced from Sion's minimax principles [Sion, 1958].\nWe consider the s-indexed family of functionals {w * s : s \u2208 S} defined as follows: For d \u2208 Q and p s \u2208 P s\nw * s (d, p s ) := d[r(s, \u2022)] + d \u2297 p s [u * ]. (3.5)\nClearly, w * s is continuous, and w * s (\u2022, p s ) is linear in the sense of vectors in R A . Moreover, p s \u2208 P s \u2282 {A \u2192 P(S)} can be seen as a vector-valued function A \u2192 R |S| which is a vector space, and w * s (d, \u2022) is affine. Therefore, we can apply Sion's minimax theorem if Q and P s satisfy appropriate convexity and compactness assumptions. Definition 6. We say that a set of action distributions\nQ \u2282 P(A) is convex if for all d, d \u2032 \u2208 Q, {td + (1 \u2212 t)d \u2032 : t \u2208 [0, 1]} \u2282 Q.\nMoreover, the set of adversarial decision rules P s is convex if for all p s , p \u2032 s \u2208 P s ,\n{tp s + (1 \u2212 t)p \u2032 s : t \u2208 [0, 1]} \u2282 P s .\nCorollary 2.1 (Convexity and Compactness). Assume that Q and P s : s \u2208 S are convex in the sense in Definition 6. Moreover, assume that either Q or all P s : s \u2208 S are compact. Then, the solution u * of (3.1) satisfies (3.2). Hence the conclusions of Theorem 2 holds.\nProof of Corollary 2.1. A direct application of Sion's minimax theorem (Corollary 3.3 in Sion [1958]) We note that Corollary 2.1 validates all nine check marks in Table 2.\nRemark 1. The convexity and compactness assumption in Corollary 2.1 is a sufficient condition for u * satisfying both (3.1) and (3.2). Many S-rectangular DRMDP models of engineering systems naturally have a convex adversary under mild assumptions, c.f. the inventory model in Section 4 of Shapiro [2022]. Yet, the convexity is not a necessary condition for the interchange in (3.1) and (3.2) to hold. As we will see in the next section, in the important special setting of SA-rectangular adversarial policy classes, the DPP always holds, even if both the controller's and the adversary's action set are non-convex.", "publication_ref": ["b51", "b42"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "The SA-Rectangular Setting", "text": "Next, we focus on the important special case where the adversary is SA-rectangular with sets of adversarial decision rules {P s,a \u2208 P(S) : s \u2208 S, a \u2208 A}.\nNote that the SA-rectangular setting, in general, doesn't satisfy Corollary 2.1 as the set P s = a\u2208A P s,a can be non-convex if P s,a is not convex. Moreover, we allow the controller to be non-convex as well; e.g. Q = Q D . However, the sup-inf in the Bellman equation does interchanges. A generalization of this is stated below. Theorem 3 implies that if P s = a\u2208A P s,a for all s \u2208 S are SA-rectangular and the controller is allowed to take on deterministic policies, then a DPP always holds with deterministic decision rules. We defer the proof of this theorem to Appendix A.3.\nRemark 2. In either cases when Q = P(A) or Q = Q D , the assumptions of Theorem 3 holds. This proves the validity of all nine cases in Table 1. We note that in the SA-rectangular setting, the enlargement of Q to non-deterministic controller actions doesn't change the optimal value of the max-min control problem. This is not true in general for S-rectangular adversaries, as pointed out in Section 3.1.1.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Convex Set of Controller's Action", "text": "We have proved the validity of the check marks for Table 4. One notices the difference between Table 3 and  4, where the (1,2)th entry, i.e. history-dependent controllers with Markov adversaries, becomes a check. In this section, we prove its validity by establishing a general principle for controllers with a convex set of action distributions Q defined in Definition 6. We highlight the effect of convexity of Q by introducing the following proposition.\nProposition 2. Let the set of action distributions Q be convex, and u * be the solution to (3.1). Then,\nv * (\u00b5, \u03a0 C H , K S M ) \u2264 \u00b5[u * ]\nfor all \u00b5 \u2208 P(S).\nThe main message of Proposition 2 is that when the adversary works against a convex history-dependent controller, it is sufficient for it to pick only Markov adversarial decision rules to make the value smaller than the solution of the Bellman equation. The proof of this key proposition is deferred to Appendix B.3.\nAn immediate consequence of Proposition 2 and Theorem 1 is the following Theorem:\nTheorem 4. Assume the assumptions of Proposition 2. Then, we have that\n\u00b5[u * ] = v * (\u00b5, \u03a0 C H , K S M ) in addition to the equalities in Theorem 1. Proof of Theorem 4. Given Theorem 1, it remains to show that v * (\u00b5, \u03a0 C H , K S M ) = \u00b5[u * ].\nBy Theorem 1, Lemma 1, and Proposition 2, we have\n\u00b5[u * ] = v * (\u00b5, \u03a0 C S , K S M ) \u2264 v * (\u00b5, \u03a0 C H , K S M ) \u2264 \u00b5[u * ].\nTherefore, the inequalities must be equalities, completing the proof.\nRemark 3. As the probability simplex is convex, the case Q = P(A) satisfies Theorem 4. Therefore, in conjunction with Theorem 1, we have established all seven check marks in Table 3. The counterexamples for the remaining two cases are then presented in Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3", "tab_2", "tab_2"]}, {"heading": "Satisfaction of the DPP: A Summary", "text": "We can summarize the results by providing structural insights into the conditions under which a DPP (see Definition 4) holds. The discussion in this section can be approached from two perspectives: one in terms of the convexity of the action sets of the controller and the adversary (in the sense of Definition 6), and the other regarding the interchangeability of the sup-inf operations in the Bellman equation (3.1).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Convexity perspective:", "text": "\u2022 A DPP always holds for the six cases outlined in Theorem 1, where neither the controller's nor the adversary's action set is assumed to be convex.\nAn important scenario where the controller action distribution set becomes non-convex is when Q = Q D is deterministic. In this context, Corollary 1.1 characterizes the solution of the Bellman equation in terms of the q-function.\n\u2022 Moreover, if the controller action distribution set Q is convex, then Theorem 4 implies the additional satisfaction of the DPP \u00b5[\nu * ] = v * (\u00b5, \u03a0 C H , K S M )\nwhere the controller is history-dependent and the adversary is Markov.\n\u2022 When both the controller's and the adversary's action sets are convex and at least one is compact, the Sion's minimax principle holds for the DR Bellman equation. In this case, the DPP invariably holds.\n\u2022 However, if the controller's action set is non-convex, even with a convex adversary, the DPP generally holds only for the six cases specified in Theorem 1.\nCounterexamples for the other three cases can be found by considering Q = Q D , as detailed in Section 5.3.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Sup-inf interchangeability perspective:", "text": "\u2022 From the viewpoint of the interchange of sup-inf operations in the DR Bellman equation, if this interchange is valid, the DPP always holds. Moreover, the validity of this interchange at the level of the Bellman equation implies the interchange of the order of the two players in the dynamic decision-making environment defined by the value (2.16).\n\u2022 An important special case in which the interchange of sup-inf in the DR Bellman equation is consistently valid is when the adversary adopts an SA-rectangular strategy, and the controller is allowed to use deterministic policies, as shown in Theorem 3.\nThe two perspectives intersect when Sion's minimax principle is satisfied by the DR Bellman equation. In such cases, both players have convex action sets, and the interchange of sup-inf operations is valid. Consequently, the DPP holds for all nine cases, see Corollary 2.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Optimality of Markov Time-Homogeneous Policies", "text": "We consider the general S-rectangular setting where Q \u2282 P(A) and P s \u2282 {A \u2192 P(S)} are arbitrary subsets. Let u * be the unique solution to (3.1).\nDefinition 7 (\u03b7-optimal Decision Rule). A decision rule \u2206 = {\u2206(s) \u2208 Q : s \u2208 S} is \u03b7-optimal for the Bellman\nequation (3.1) if for all s \u2208 S u * (s) \u2264 \u2206(s)[r(s, \u2022)] + \u03b3 inf ps\u2208Ps \u2206(s) \u2297 p s [u * ] + \u03b7.\nTheorem 5. Let u * be the solution to (3.1) and \u03b7 \u2265 0. Assume for some\n\u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S and K = K S H , K S M , K S S \u00b5[u * ] = v * (\u00b5, \u03a0, K).\nThen, any \u03b7-optimal decision rule \u2206 for (3.1) induces a \u03b7/(1 \u2212 \u03b3)-optimal policy \u03c0 := (\u2206, \u2206, . . . ) \u2208 \u03a0 C S for the maxmin control problem v * (\u00b5, \u03a0, K) in the sense that for all \u00b5 \u2208 P(S)\n0 \u2264 v * (\u00b5, \u03a0, K) \u2212 inf \u03ba\u2208K v(\u00b5, \u03c0, \u03ba) \u2264 \u03b7 1 \u2212 \u03b3 .\nThe proof of this theorem is provided in the Appendix C.\nRemark 4. Here, we intentionally allow \u03b7 = 0; i.e., if the assumptions in Theorem 5 hold and the solution of the Bellman equation induces a decision rule \u2206 = {\u2206(s) : s \u2208 S} that achieve the supremum in (3.1), then the Markov time-homogeneous policy \u03c0 := (\u2206, \u2206, . . . ) is optimal for the maxmin control problem.\nRemark 5. For the cases either Q = Q D or the adversary policy class is SA-rectangular, Corollary 1.1 and Theorem 3 implies that any greedy deterministic decision rule\nd(s) \u2208 arg max a\u2208A q * (s, a)\nis 0-optimal for the Bellman equation. Therefore, if the DPP holds (a green check in Table 4 or 1), by Theorem 5, the corresponding greedy policy \u03c0 := (d, d, . . . ) \u2208 \u03a0 C S is optimal for the maxmin control problem.\nThe preceding remarks describe scenarios in which Markov time-homogeneous policies prove to be optimal. Additionally, Theorem 5 implies that Markov time-homogeneous policies, achieving arbitrarily small errors in terms of satisfying the DR Bellman equation (3.1), will yield near optimal performance with arbitrarily small optimality gap in the context of the maxmin control. Consequently, a DRRL procedure that identifies a policy approximating the solution of (3.1) is assured to achieve reasonable performance in addressing the maxmin control problem.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Absence of the DPP: Counterexamples", "text": "In this section, we provide counterexamples in the form of specific DRMDP instances for which the DPP does not hold. In particular, this section is organized as follows: In Section 5.1, we consider the setting where a history-dependent randomized controller is paired with a Markov time-homogeneous non-convex adversary, as indicated by the cross mark at (1,3)th entry in Table 3. In Section 5.2, we consider an instance with a Markov randomized controller and a Markov time-homogeneous non-convex adversary, represented by the cross mark at (2,3)th entry in Table 3. Finally, in Section 5.3, we establish counterexamples for all the cases with deterministic controllers in Table 4 for which a DPP is not proven.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_2", "tab_3"]}, {"heading": "History-dependent Randomized Controller Versus", "text": "Markov Time-Homogeneous Non-convex Adversary\nWe display the DRMDP instance used as the counterexample in Figure 1. This is used to verify the cross mark at (1,3)th entry in Table 3. We assume that reward function r only depends on the states so that\nr(I) = 0, r(G) = 1, r(B) = \u22121.\nWe consider a history-dependent policy \u03c0; At time 0, we randomly pick one action at state I. If we observe state G, we will choose the same action for the following time steps. If we observe state B, we will choose the alternative action for the following time steps.\nFor any Markov time-homogeneous adversary's policy \u03ba, we have\nv(I, \u03c0, \u03ba) = 0 + 0 \u00d7 \u03b3 + 0 \u00d7 \u03b3 2 + \u221e i=3,i odd \u03b3 i = \u03b3 3 1 \u2212 \u03b3 2 > 0.\n(5.1)\nHere, for notation simplicity, we abuse the notation and write v(I, \u03c0, \u03ba) := v(\u03b4 I , \u03c0, \u03ba) as the value of state I under the policy pair (\u03c0, \u03ba), where \u03b4 I is the point mass measure at I. On the other hand, the DR Bellman equation corresponding to this instance is Notice that u * (I) < v(I, \u03c0, \u03ba) in (5.1). By construction, we have v * (I, \u03a0 H , K S S ) \u2265 v(I, \u03c0, \u03ba) > u * (I). Therefore, the DPP is not satisfied.\nIn this case, due to the Markov time-homogeneity of the adversary, the task of the controller becomes a bandit learning problem [Lattimore and Szepesv\u00e1ri, 2020]. Therefore, it is possible for the controller to implement a history-dependent policy that optimally \"learns\" the environment.", "publication_ref": ["b20"], "figure_ref": ["fig_6"], "table_ref": ["tab_2"]}, {"heading": "Markov Randomized Controller Versus", "text": "Markov Time-Homogeneous Non-convex Adversary In Figure 2, we illustrate the transition structure of the counterexample we constructed for verifying the cross at (2,3)the entry of Table 3. In this example, we consider a Markov chain with three states I,B,C. The chain starts at state I, where there are two actions: A = {a 1 , a 2 }. In states B,C, there is only one action.\nI B C 1/2 1 1/2 (a) p (1) I B C 1/2 1/2 1/2 1/2 (b) p (2)\nThe adversary's action distribution set P I := p\n(1) = (1, 0, 0) \u22a4 and p\nI , p(2)\n(1)\nI,a2 = 1 2 , 1 2 , 0 \u22a4 , p(2)\nI,a1 = 0, 1 2 , 1 2 \u22a4 and p (2\n)\nI,a2 = 1 2 , 0, 1 2 \u22a4 ,\nwhere we use a column vector to denote the probabilities of states I,B,C. Further, once the chain arrives state B or state C, it will stay in that state forever, i.e., p B (B) = p C (C) = 1.\nWe assume again that rewards only depend on the states r(I) = r(C) = 0, r(B) = 3/5 and the discount factor \u03b3 = 0.8.\nWe now proceed to solve the DR Bellman equation. It is easy to see that u * (C) = 0, u * (B) = 3. We claim that u * (I) = 1. By Proposition 1, the solution u * is unique, so if u * (C) = 0, u * (B) = 3, u * (I) = 1 verifies (3.1), it must be the solution. We proceed to check u * (I) = \u03b3 sup\nd\u2208P(A) inf pI \u2208PI d(a 1 )p \u22a4 I,a1 u * + d(a 2 )p \u22a4 I,a2 u * = \u03b3 sup d(a1)\u2208[0,1] inf d(a 1 ) + 1 + 3 2 (1 \u2212 d(a 1 )), 0 + 3 2 d(a 1 ) + 0 + 1 2 (1 \u2212 d(a 1 )) = 0.8 sup d(a1)\u2208[0,1] inf 2 \u2212 d(a 1 ), 1 2 + d(a 1 ) = 1,\nwhere the optimal controller's action distribution is d(a 1 ) = 3/4. We then consider a Markov but not time-homogeneous policy \u03c0 = (\u03c0 0 , \u03c0 1 , . . . ) where \u03c0 0 (I) = a 1 and \u03c0 1 (I) = a 2 and \u03c0 i (I) = (3/4, 1/4) for i \u2265 2.\nAt time 0, if the adversary chooses p\n(2)\nI , we have v(I, \u03c0, p\nI ) = 0.8(u * (B) + u * (C))/2 = 1.2 > 1.\nOn the other hand, if the adversary chooses p \nI ) = \u03b3v 1 (A, \u03c0, p\n(1)\nI ) = 1.44 > 1,\nwhere v 1 (I, \u03c0, p\nI ) stands for the value of state A at time 1 under the Markov policy \u03c0 and the adversary with the choice p (1) I . Therefore, we conclude under the Markov non-time-homogeneous policy \u03c0, the controller can have a higher value than the solution of the DR Bellman equation, hence the absence of the DPP.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": ["tab_2"]}, {"heading": "Deterministic Controllers", "text": "In this subsection, we focus on deterministic controllers. Specifically, we will use the DRMDP instance in Figure 3 to construct counterexamples to address the various cases with a red cross in Table 4. These cases include a history-dependent deterministic controller against a Markov convex adversary, a history-dependent deterministic controller paired with Markov time-homogeneous convex adversary, and a Markov deterministic controller with Markov time-homogeneous convex adversary.\nWe consider a DRMDP environment with six states I0,I1,I2,I,B,G. The initial state is always I0. At state I, there are two actions: A = {a 1 , a 2 }. For all other states, there is only one action. As shown in the Figure 3, at state I0, the transition probabilities are\np I0 (I1) = p I0 (I2) = 1 2 .\nThe adversary has action distribution set P I at state I. If we assume a convex adversary, we will use P I := \u03b1p In what follows, we only verify the case when the adversary is convex; i.e. P I := \u03b1p (1)\nI + (1 \u2212 \u03b1)p (2) I : \u03b1 \u2208 [0, 1] .\nThe absence of the DPP is still valid when we assume a non-convex adversary with P I := p The solution u * is\nu * (I) = \u2212 \u03b3 1 \u2212 \u03b3 2 , u * (I1) = u * (I2) = \u2212 \u03b3 2 1 \u2212 \u03b3 2 , u * (I0) = \u2212 \u03b3 3 1 \u2212 \u03b3 2 , u * (G) = 1 \u2212 \u03b3 2 1 \u2212 \u03b3 2 , u * (B) = \u2212 1 1 \u2212 \u03b3 2 .\nWe first consider the case of a Markov controller with Markov time-homogeneous convex adversary. In particular, the controller uses the following Markov policy \u03c0 with \u03c0 2 = a 1 , \u03c0 4 = a 2 , \u03c0 6 = a 1 , \u03c0 8 = a 2 , . . . . We claim that this policy can achieve a higher reward than the solution of the DR Bellman equation. To see that, we use p \u03b1 I to denote the adversary's action \u03b1p Solving the recursion, one obtains the solution and verifies the value v(I0, \u03c0, p \u03b1 I ) > u * (I0) uniformly in \u03b1 as follows:\nv(I0, \u03c0, p \u03b1 I ) = (1 \u2212 2\u03b1) \u03b3 3 1 + \u03b3 2 \u2265 \u2212 \u03b3 3 1 + \u03b3 2 > u * (I0), v 2 (I, \u03c0, p \u03b1 I ) = (1 \u2212 2\u03b1) \u03b3 1 + \u03b3 2 \u2265 \u2212 \u03b3 1 + \u03b3 2 > u * (I), v 3 (B, \u03c0, p \u03b1 I ) = \u22121 + (2\u03b1 \u2212 1) \u03b3 2 1 + \u03b3 2 , v 3 (G, \u03c0, p \u03b1 I ) = 1 + (2\u03b1 \u2212 1) \u03b3 2 1 + \u03b3 2 v 4 (I, \u03c0, p \u03b1 I ) = (2\u03b1 \u2212 1) \u03b3 1 + \u03b3 2 .\nBy Lemma 1, there must be a history-dependent deterministic controller that achieve no worse performance than \u03c0. So, the above constructions and inequalities also imply that a history-dependent deterministic controller with a Markov time-homogeneous convex adversary can achieve a higher value. Therefore, the DPP doesn't hold for the two cases discussed above.\nFinally, we analyze the case with a history-dependent deterministic controller and a Markov convex adversary. We consider the history-dependent policy such that \u03c0 2i = a 1 , i = 1, 2, . . . , if the controller sees state I1 and \u03c0 2i = a 2 , i = 1, 2, . . . , if the controller sees state I2. Then, for any Markov adversary \u03ba, we always have v(I0, \u03c0, \u03ba) = 0 > u * (I0).\nAgain, this suggests the absence of a DPP in this setting as well.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": ["tab_3"]}, {"heading": "Connections with other Formulations", "text": "In this section, we elucidate the links between the formulations of DRMDP and the multistage stochastic programs [Shapiro et al., 2021, Chapter 3] within the realm of optimization literature. Additionally, we explore their connections with stochastic games, as introduced by Shapley [1953] in the economics literature.", "publication_ref": ["b45"], "figure_ref": [], "table_ref": []}, {"heading": "Multistage Stochastic Programs", "text": "Early works on DRMDP, such as Gonz\u00e1lez-Trejo et al. [2002] and Nilim and El Ghaoui [2005], interpreted the max-min problem as dynamic interactions between a controller and an SA-rectangular adversary. Indeed, the presence of a Dynamic Programming Principle (DPP) in these scenarios implies that the interactions under optimal robust control are equivalent to another dynamic model wherein the controller and adversary sequentially make decisions. This interpretation results in what is now considered as a robustified variant of the multistage stochastic program (MSP) . In contrast, Nilim and El Ghaoui [2005]'s version of sequential max-min-max-min decisions (their Equation ( 5)) is positioned before time 0.\nAs a widely used paradigm for modeling dynamic decision-making, MSP finds numerous applications in addressing real-world control problems. There exists a substantial body of literature on MSP, and multiple recent works dedicating effort to consider a distributionally robust variant of it; see, for example, Shapiro et al. [2021, Chapter 7] and , Shapiro [2021], Pichler and Shapiro [2021]. Consequently, in this section, we delve into generalizations of the MSP formulation of DRMDP. We aim to achieve a formulation equivalent to scenarios where a controller with an arbitrary action set contends with a history-dependent S-rectangular adversary with an arbitrary action set. Additionally, we establish connections between our formulations and these generalized MSPs by leveraging the corresponding DPPs.\nWe now study the finite horizon problem to avoid the issue of needing to make sense of countably many nested stochastic programs. We note that an infinite horizon version could be defined through a limiting argument, due to the presence of the discount. However, introducing this will distract the main message in this section. We focus on the case of a history-dependent controller and adversarial policy classes and explain how the max-min control problem defined by (2.16) can be seen as a variant of the MSP.\nBefore introducing the multistage stochastic programs, we define some notations. let T + 1 denote the time horizon. We consider a general controller action distribution set Q \u2282 P(A) and adversarial action set P \u2282 {S \u00d7 A \u2192 P(S)}.\nThe SA-rectangular marginalization of P (see (2.11)) is defined as P s,a := {p s,a : p \u2208 P} for all s \u2208 S. We consider the product action set: Also, the S-rectangular marginalization of P is defined as P s := {p s : p \u2208 P} for all s \u2208 S. We also consider the product action set: P S := s\u2208S P s .\nTo simplify notation, we recall the bracket integral notation in (2.13). We further identify that for function\nf : h T +1 \u2192 R, \u00b5[f (h T +1 )] = s0\u2208S \u00b5[s 0 ]f (s 0 , . . . , a T , s T +1 ), d t (h t )[f (h T +1 )] = at\u2208A d t (h t )[a t ]f (s 0 , . . . , s t , a t , . . . , s T +1 ), \u03ba t (g t )[f (h T +1 )] = st\u2208S \u03ba t (g t )[s t+1 ]f (s 0 , . . . , a t , s t+1 , . . . , s T +1 ).\nThe style of this section is a brief exposition rather than a detailed discussion. One can find a more refined discussion of the MSP formulations and equivalent marginalization of general rectangular action sets in Li and Shapiro [2023].\nwhere \u03a0 C H and K S H has action sets Q and P s : s \u2208 S respectively. The first equality can be seen by a similar reason. Let us consider the second infimum taken over \u03ba 1 (g 0 , \u2022, \u2022) \u2208 P as an example. The inf is within the sum over s 0 , a 0 , s 1 . In particular, the adversary can choose different \u03ba 1 for each s 1 observed. Therefore, one can see that the use of P for the MSP problem v S H (\u00b5, Q, P) effectively gives the adversary the freedom to choose from the larger set P S . This observation is consistent with the behavior of formulation (2.1) in Li and Shapiro [2023]. In their work, (2.1) formulates the transition dynamics as a two-player zero-sum game. In this setting, they also establish an equivalence relationship similar to (6.3).", "publication_ref": ["b9", "b32", "b32", "b41", "b37", "b23", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "General Rectangular Case: Some Comments", "text": "In the same spirit, one can also define a generalization of the MSP with terminal reward u in which the adversary cannot observe the realization of the current state, leading to the adversary's need to choose its action from the general action set. However, caution is required to maintain the sup-inf structure. This is because the controller is one who can use the realization of the current state as available information, while the adversary is not, leading to the temptation to consider a formulation that puts the inf over \u03ba t outside the sum of \u03ba t\u22121 . However, doing so directly will result in the reverse of the ordering of the inf and sup. To address this, we can redefine the controller as one who selects, for each and every state, an action distribution; i.e., d t (g t\u22121 , \u2022) \u2208 {S \u2192 Q}. We note that the MSP for both the SA and S-rectangular cases in the preceding sections can also adopt this equivalent formulation of the controller. Then, the sup can be moved outside of the sum with respect to the preceding \u03ba. We didn't adopt this reformulation of the controller in the SA and S-rectangular cases so as to keep it consistent with the DRMDP formulations. This leads to the following definition of a generalized MSP: for some terminal values u. Observe that this is not a MSP in the classical sense: the segment highlighted in blue shows that the inf over \u03ba 1 is between inf over \u03ba 0 and the expectation w.r.t. \u03ba 0 . Nevertheless, one faces difficulties when trying to conclude that the max-min control value in (2.16) with the general adversarial policy class K N H and controller's policy class \u03a0 C H is equivalent to this generalized MSP. This is attributed to the absence of a DPP-as mentioned earlier, the max-min control value in this context doesn't satisfy a Bellman equation in full generality. One can envision a similar situation for the generalized MSP value. Notably, the inf over \u03ba t is always positioned outside of the expectation over \u03ba t\u22121 , as exemplified in blue in the above definition (6.4). This suggests that determining the optimal \u03ba t requires considering the choice of \u03ba t\u22121 , taking into account both the history and the preceding choices. In turn, \u03ba t\u22121 depends on the selection of \u03ba t\u22122 , and so on, resulting in a fully intertwined relationship. This intricate interplay differs from the SA-rectangular (6.1) and S-rectangular (6.2) formulations and it falls beyond the scope of this paper.\nIn summary, it is possible to formulate a robust dynamic decision-making environment, namely the generalized MSP v N H (\u00b5, Q, P, u), in which an adversary chooses from a general rectangular ambiguity set P possibly yields different value as one that chooses from P S ; i.e.\nv N H (\u00b5, Q, P S , u) \u2264 v N H (\u00b5, Q, P, u), which contrasts with the equality (6.3). Notably, instances can be formulated where the aforementioned inequality changes to strict inequality (<), even when Q = P(A). As an illustrative example, consider the inventory control setting in Example 1 with an adversary that lacks visibility of the current inventory level I t (general rectangular) compared to one that has full information about I t available for decision-making (S-rectangular).\nIn addition, the connection between this generalized MSP and DRMDP with history-dependent controllers and general-rectangular adversaries remains an open research question; i.e., whether there exists a terminal reward u s.t.\nv N H (\u00b5, Q, P, u)\n? = v * (\u00b5, \u03a0 C H , K N H )\nholds all \u00b5 \u2208 P(S). It remains an intriguing research avenue that extends beyond the scope of the present paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Stochastic Games", "text": "Stochastic games (a.k.a. Markov games) model interactions among players where the environment evolves based on the actions of the players [Solan and Vieille, 2015]. In a stochastic game, a group of players participate.\nDuring each stage of the game, the game is in a specific state, with each player selecting an action, possibly history-dependent, from a set of possible actions. The actions chosen by the players, along with the current state, dictate the payoff each player receives for that stage and also establish a probability distribution that determines the new state the play will transition to. In our formulation of DRMDP with an S-rectangular adversary, the controller and the adversarial can be viewed as two players with zero-sum rewards, which fits in the framework of stochastic games. This connection is also noted in Gonz\u00e1lez-Trejo et al. [2002], Nilim and El Ghaoui [2005], Le Tallec [2007], Shapiro [2021], Li and Shapiro [2023]. The line of research concerning stochastic games was pioneered by Shapley [1953], who proved the existence of a stationary equilibrium in a zero-sum game. Following the groundbreaking results of Shapley [1953], a substantial body of literature has emerged, with a primary focus on identifying scenarios in which stationary Markov equilibria exist [Fink, 1964, Takahashi, 1964, Parthasarathy and Sinha, 1989, Nowak, 2003, Simon, 2003, Levy, 2013, Maskin and Tirole, 2001 For a review of the history of stochastic games, we refer readers to Solan and Vieille [2015]. Furthermore, stochastic games become a standard framework of multi-agent reinforcement learning [Littman, 1994]. We note that the solutions to the DRMDP (2.16) may not be an equilibrium if sup-inf cannot interchange, as there may not exist a consistent belief. One major difference between stochastic games and DRMDPs lies in the symmetry or asymmetry of the two players. In stochastic games, both players are symmetric in that they can both choose policies that are history-dependent. However, in the DRMDP literature, there is often an asymmetry between the controller and the adversary. Here, the controller's strategies are history-dependent, while the adversary's strategies are typically modeled as Markov time-homogeneous. This crucial difference is also noted in Grand-Clement et al. [2023].\nDRMDPs with different types of controllers and adversaries are also related to stochastic games with asymmetric information [Nayyar et al., 2013, Nayyar andGupta, 2017]. Their formulations are based on partially observable Markov decision processes and their approach is to reduce the game with asymmetric information to another game with symmetric information.", "publication_ref": ["b52", "b9", "b32", "b41", "b23", "b7", "b53", "b36", "b33", "b50", "b22", "b28", "b52", "b24", "b11", "b31", "b30"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion and Future Works", "text": "In this paper, we clarify the definitions of distributionally robust Markov decision processes (DRMDP) and identify the scenarios in which the dynamic programming principle (DPP) is either held with full generality or violated. Following this trajectory, several intriguing future directions emerge:\n\u2022 In cases where a DPP in the form of the DR Bellman equation does not hold with full generality, are there alternative optimality equations that can characterize the optimal robust control? If such equations exist, can they be effectively employed to facilitate DRRL?\n\u2022 While our current findings center on SA-and S-rectangularity, can analogous results be developed for general-rectangular adversaries?\nWe believe that partial answers to either research question will yield valuable theoretical insights and ultimately facilitate the development of more effective DRRL algorithms by improving learning efficiency and enriching the expressiveness of the DRRL model.\nSo, the inequalities are indeed equalities. This implies item 1 of Theorem 2.\nNext we prove item 2 of Theorem 2. We claim that it is sufficient to prove the following proposition: Proposition 6. Assume the assumptions of Proposition 5. Then, for any \u01eb > 0, there exists \u03ba \u2208 K S S such that sup\n\u03c0\u2208\u03a0 C H v(\u00b5, \u03c0, \u03ba) \u2264 \u00b5[u * ] + \u01eb.\nWe prove the sufficiency of Proposition 6 and defer its proof to the appendix Section B.6 as well. Therefore, we conclude that the inequalities in (A.1) must be equalities. This proves item 2 of Theorem 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Proof of Theorem 3", "text": "Proof. Notice that since P s = a\u2208A P s,a , the Bellman equation for the q-function where (i) again follows from the LP fact that the optima are achieved at Q D . Since \u03b4 is arbitrary, all inequalities above must be equalities. In particular, u * satisfies (3.2).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Proofs of Key Propositions in Sections 3 and A", "text": "A technique that we will be using in the subsequent proof is to truncate the infinite horizon reward to a finite time T that is large enough. Specifically, consider for any \u03c0 \u2208 \u03a0 H and \u03ba \u2208 K S H , v(\u00b5, \u03c0, \u03ba Since d k\u22121 (h k\u22121 ) \u2208 Q where Q is convex,\n) = E \u03c0,\u03ba \u00b5 T \u22121 k=0 \u03b3 k r(X k , A k ) + \u03b3 T u * (X T ) + \u221e k=T \u03b3 k r(X k , A k ) \u2212 \u03b3 T u * (X T ) \u2264 E \u03c0,\u03ba \u00b5 T \u22121 k=0 \u03b3 k r(X k , A k ) + \u03b3 T u * (X T ) + \u01eb 2 (B.1) if \u03b3 T \u2264 \u01eb(1 \u2212 \u03b3)/4\n\u2206 k\u22121 (s)[\u2022] = h k\u22121 \u2208H k\u22121 E \u03c0,\u03ba (k\u22121) \u00b5 [d k\u22121 (H k\u22121 )[\u2022]1 {H k\u22121 = h k\u22121 }|X k\u22121 = s] = g k\u22122 \u2208G k\u22122 d k\u22121 (g k\u22122 , s)[\u2022]E \u03c0,\u03ba (k\u22121) \u00b5 [1 {G k\u22122 = g k\u22122 }|X k\u22121 = s] .\nwhich is a convex combination of d k\u22121 (g k\u22122 , s) \u2208 Q. Therefore, \u2206 k\u22121 (s) \u2208 Q Fix any \u03b4 > 0. Since for all s \u2208 S u * (s) = sup As \u03c0 and \u01eb > 0 are arbitrary, this implies the statement of Proposition 2.\nFinally, it remains to prove the claim (B.6). We begin with analyzing the following quantity\nE \u03c0,\u03ba (k) \u00b5 [r(X k\u22121 , A k\u22121 ) + \u03b3u * (X k )] = E \u03c0,\u03ba (k) \u00b5 [d k\u22121 (H k\u22121 )[r(X k\u22121 , \u2022)] + \u03b3d k\u22121 (H k\u22121 ) \u2297 \u03c8 k\u22121 (X k\u22121 , \u2022)[u * ]] (B.7)\nNotice that where (i) follows from the observation that the law of H k\u22121 is not dependent on the adversarial decision rules after time k \u2212 1. Apply this to (B.7), we have that\nE \u03c0,\u03ba (k) \u00b5 [d k\u22121 (H k\u22121 ) \u2297 \u03c8 k\u22121 (X k\u22121 , \u2022)[u * ]|X k\u22121 = s] = E \u03c0,\u03ba (k) \u00b5 s \u2032 \u2208S a\u2208A d k\u22121 (H k\u22121 )[a]\u03c8\nE \u03c0,\u03ba (k) \u00b5 [r(X k\u22121 , A k\u22121 ) + \u03b3u * (X k )|X k\u22121 ] = \u2206 k\u22121 (X k\u22121 )[r(X k\u22121 , \u2022)] + \u03b3\u2206 k\u22121 (X k\u22121 ) \u2297 \u03c8 k\u22121 (X k\u22121 , \u2022)[u * ] (i)\n\u2264 u * (X k\u22121 ) + \u03b4. \u03b3 j r(X j , A j )\n\uf8f9 \uf8fb + \u03b3 k\u22121 E \u03c0,\u03ba (k) \u00b5 E \u03c0,\u03ba (k) \u00b5 [r(X k\u22121 , A k\u22121 ) + \u03b3u * (X k )|X k\u22121 ] \u2264 E \u03c0,\u03ba (k) \u00b5 \uf8ee \uf8f0 k\u22122 j=0\n\u03b3 j r(X j , A j ) \n\uf8f9 \uf8fb + \u03b3 k\u22121 E \u03c0,\u03ba (k) \u00b5 u * (X k\u22121 ) + \u03b3 k\u22121 \u03b4 (i) = E \u03c0,\u03ba (k\u22121) \u00b5 \uf8ee \uf8f0 k\u22122 j=0 \u03b3 j r(X j , A j ) + \u03b3 k\u22121 u * (X k\u22121 ) \uf8f9 \uf8fb + \u03b3 k\u22121 \u03b4 = v \u03c0 \u00b5 (k \u2212 1, \u03ba (k\u22121) ) + \u03b3", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "The material in this paper is based upon work supported by the Air Force Office of Scientific Research under award number FA9550-20-1-0397. Additional support is gratefully acknowledged from NSF 1915967, 2118199,  2229012, 2312204.   Zhengyuan Zhou graciously acknowledges the generous support provided by NSF 2312205 and NYU Center for Global Economy and Business Grant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "SA-Rectangular Formulation", "text": "Let u SA be the solution associated with the DR Bellman equation (3.1) with P s = a\u2208A P s,a . We use r T +1 (s, a) = u SA (s) as the terminal reward. We first consider the following SA-rectangular MSP formulation: We claim that v SA H (\u00b5, Q, P, u SA ) = v SA H (\u00b5, Q, P SA , u SA ).\nTo see this, let's consider the first inf taken over \u03ba 0 (\u2022, \u2022) \u2208 P. Since it is inside the second square bracket, where the sum over s 0 and a 0 is taken outside, the inf is taken for each history g 0 = (s 0 , a 0 ) separately. In particular, the adversary can choose different \u03ba 0 different (s 0 , a 0 ). Thus, the inf over \u03ba 0 (\u2022, \u2022) \u2208 P is equivalent to choosing from P SA . For t > 0, the same holds true analogously. This observation implies the claim.\nUpon writing the dynamic programming equations for this program, one realizes that\nwhere \u03a0 C H and K SA H has action sets Q and P s,a : (s, a) \u2208 S \u00d7 A respectively. We remark that in their endeavor to formulate a MSP with adversarial dynamics,  successfully derived a formulation that is a special class of instances of the MSP formulation presented in this section. This consequently results in a DPP that is equivalent to the SA-rectangular case detailed in this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "S-Rectangular Formulation", "text": "Based on the same core idea, we can also define a S-rectangular MSP from the general adversarial action set P. Concretely, let u S be the solution to (3.1) with P s = P s , we consider\nwith terminal reward r T +1 (s, a) = u S (s), where the final terms are handled in a similar fashion as \u039e SA T \u22121 , except with the infimum taken outside d. Similar to the SA-rectangular version, we have that", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Appendices", "text": "A Proofs of Theorems in Section 3\nIn this section, we outline some key properties of of the maxmin values for S-rectangular robust MDPs. We present a proof of Theorem 1 and 2 given these results.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Proof of Theorem 1", "text": "We break down the proof of Theorem 1 using the following two key propositions.\nProposition 3. Let u * be the solution to (3.1). Then,\n) for all \u00b5 \u2208 P(S).\nProposition 4. Let u * be the solution to (3.1). Then, \u00b5[u * ] = v * (\u00b5, \u03a0 C S , K S H ) for all \u00b5 \u2208 P(S).\nWith these results, we present the proof of Theorem 1 given Lemma 1 and Proposition 3 and 4.\nProof of Theorem 1. First, by Lemma 1, v * (\u00b5, \u03a0 C S , K S H ) is the smallest amongst all the 9 values in Lemma 1. This and Proposition 2 and 4 implies that\nTherefore, all the inequalities above are equality. Moreover, by Lemma 1 and Proposition 3,\nSo, equalites hold for above inequalities as well. Finally, by the above equalities and Proposition 3,\nThis completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Theorem 2", "text": "Now we go back to the general case where Q \u2282 P(A) is an arbitrary subset. We highlight a consequence of the extra assumption (3.2) on u * . This strengthens the statement of Proposition 2 to the case of K S S .\nProposition 5. Assume that the solution\nThe proof to this proposition is deferred to Appendix B.6. Assume the validity of this proposition, we present our proof of item 1 in Theorem 2.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Item 1 in Theorem", "text": "for all \u00b5 \u2208 P(S) and any mixture of K = K S H , K S M , K S S and \u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S . This, Theorem 1, and Proposition 5 implies that\nThis implies the second inequality.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Proof of Proposition 1", "text": "Proof. We consider the distributionally robust Bellman operator B : R S \u2192 R S defined as\nThen, B is a \u03b3-contraction on (R S , \u2022 \u221e ). Indeed for u, v \u2208 R S ,\nand (iii) is because d \u2297 p s is a probability measure. Therefore, by the Banach fixed point theorem, there exists unique u * \u2208 R S s.t. B(u * ) = u * . Moreover,\nRearrange and use the upper bound r \u221e = 1 proves that u * \u221e \u2264 1/(1 \u2212 \u03b3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Proof of Proposition 2", "text": "Proof. As explained before, we choose T large so that for all \u03c0 \u2208 \u03a0 C H and\nwhere Q is assumed to be convex. We construct the sequence of Markovian adversaries \u03ba (\u22121) , \u03ba (0) , . . . , \u03ba (T ) as follow. Fix any homogeneous adversary \u03ba \u2032 and let \u03ba (\u22121) = \u03ba \u2032 = (\u03ba \u2032 , \u03ba \u2032 , . . . ). Then recursively construct\nfor some Markovian \u03c8 k+1 (g k+1 ) = \u03c8 k+1 (s k+1 , a k+1 ) that we will specify later. This leads to\nfor all k = 0, . . . , T . Now we specify the recursive procedure to construct \u03ba (k) . Note that \u03ba (\u22121) is fully specified. Fix any 0 \u2264 k \u2264 T , suppose that we have specified \u03ba (k\u22121) (or equivalently, \u03c8 0 , . . . , \u03c8 k\u22121 ). Let us define the probability measures {\u2206 k (s) \u2208 P(A) : s \u2208 S} by\nFor any \u03b4 > 0, we can always choose \u03ba k (h k , \u2022) \u2208 P s so that\n\u2022) Markov time-homogeneous) so that (B.9) holds for all k \u2265 0 and h k = (g k\u22121 , s) \u2208 H k . Define \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) \u2208 K S H (or K S M , K S S resp.) with {\u03ba k : k \u2265 0} specified above.\nNext, we consider\nWith this bound, we choose T large so that for all \u03c0, \u03ba,\nSince \u03b4 > 0 is arbitrary, we let \u03b4 = \u01eb(1 \u2212 \u03b3)/2 and conclude that\nRecall that \u01eb > 0 and \u03c0 \u2208 \u03a0 Let \u03c0 = (\u2206, \u2206, . . . ) \u2208 \u03a0 C S . We consider for any \u03ba \u2208 K S H ,\nNow, if we define the random adversarial\nwhere (i) follows from (B.10) and (ii) can be seen by first condition on G k\u22122 and noting that \u2206 is history independent given X k\u22121 . Fix any \u01eb > 0, let \u03c0 be defined as above. Recall the definition of v \u03c0 \u00b5 (T, \u03ba) in (B.2). We still choose T large so that for all \u03ba, |v \u03c0 \u00b5 (T, \u03ba) \u2212 v(\u00b5, \u03c0, \u03ba)| \u2264 \u01eb/2. Then, for any \u03ba \u2208 K S H ,\nBy definition of inf and that \u03c0 \u2208 \u03a0 C S , there exists \u03ba \u2032 s.t.\nThen, .11) where (i) follows from Proposition 3, and (ii) is due to the choice of T . Since \u03b4 can be chosen as (1 \u2212 \u03b3)\u01eb/4, this implies \u00b5[u * ] = v * (\u00b5, \u03a0 C S , K S H ) as \u01eb > 0 is arbitrary.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.6 Proof of Propositions 5 and 6", "text": "Proof of Proposition 5. Since u * satisfies (3.2), for any \u03b4 > 0, there exists adversarial decision rule \u03c8 = {\u03c8(s, \u2022) \u2208 P s : s \u2208 S} s.t.\nSince \u03b4 > 0 is arbitrary, we let \u03b4 = \u01eb(1 \u2212 \u03b3)/2 and conclude that inf\nSince \u03c0 \u2208 \u03a0 C H and \u01eb > 0 are arbitrary, this implies the proposition.\nNext, we present the proof of Proposition 6 using the results in the previous proof.\nProof of Proposition 6. For any \u03b4 > 0, let \u03ba = (\u03c8, \u03c8, . . . ) \u2208 K S S be constructed as above. Then the same proof as above implies that for any \u03c0 \u2208 \u03a0 C H ,\nTherefore, by the choice of T and the definition of sup, there exists \u03c0 \u2032 \u2208 \u03a0 C H s.t.\nsup\nSince \u03b4 > 0 is arbitrary, choose \u03b4 = (1 \u2212 \u03b3)\u01eb/4 will complete the proof. Recall the proof of Proposition 4, where (B.10) is satisfied with \u03b4 replaced by \u03b7. Therefore, for any \u01eb > 0, adapting the same argument, one can conclude that as in B.11\nSince \u01eb > 0 is arbitrary, this implies that 0 \u2264 v * (\u00b5, \u03a0, K) \u2212 inf \u03ba\u2208K v(\u00b5, \u03c0, \u03ba) \u2264 \u03b7 1 \u2212 \u03b3 as claimed.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Reinforcement learning based recommender systems: A survey", "journal": "ACM Computing Surveys", "year": "", "authors": "Trafford M Mehdi Afsar; Behrouz Crump;  Far"}, {"ref_id": "b1", "title": "The theory of dynamic programming", "journal": "Bulletin of the American Mathematical Society", "year": "1954", "authors": "Richard Bellman"}, {"ref_id": "b2", "title": "Mathematical and computational models for congestion charging", "journal": "", "year": "2006", "authors": "Dimitris Bertsimas; Georgia Perakis"}, {"ref_id": "b3", "title": "Double pessimism is provably efficient for distributionally robust offline reinforcement learning: Generic algorithm and robust partial coverage", "journal": "", "year": "2023", "authors": "Jose Blanchet; Miao Lu; Tong Zhang; Han Zhong"}, {"ref_id": "b4", "title": "Real-time bidding by reinforcement learning in display advertising", "journal": "", "year": "2017", "authors": "Han Cai; Weinan Kan Ren; Kleanthis Zhang; Jun Malialis; Yong Wang; Defeng Yu;  Guo"}, {"ref_id": "b5", "title": "Dynamic treatment regimes. Annual review of statistics and its application", "journal": "", "year": "2014", "authors": "Bibhas Chakraborty; Susan A Murphy"}, {"ref_id": "b6", "title": "Seeing is not believing: Robust reinforcement learning against spurious correlation", "journal": "", "year": "2023", "authors": "Wenhao Ding; Laixi Shi; Yuejie Chi; Ding Zhao"}, {"ref_id": "b7", "title": "Journal of science of the hiroshima university, series ai (mathematics)", "journal": "", "year": "1964", "authors": "M Arlington;  Fink"}, {"ref_id": "b8", "title": "Bandits atop reinforcement learning: Tackling online inventory models with cyclic demands", "journal": "Management Science", "year": "", "authors": "Xiao-Yue Gong; David Simchi-Levi"}, {"ref_id": "b9", "title": "Minimax control of discrete-time stochastic systems", "journal": "SIAM Journal on Control and Optimization", "year": "2002", "authors": "J I Gonz\u00e1lez-Trejo; On\u00e9simo Hern\u00e1ndez-Lerma; Luis F Hoyos-Reyes "}, {"ref_id": "b10", "title": "Robust markov decision processes: Beyond rectangularity", "journal": "Mathematics of Operations Research", "year": "2023", "authors": "Vineet Goyal; Julien Grand-Cl\u00e9ment"}, {"ref_id": "b11", "title": "Beyond discounted returns: Robust markov decision processes with average and blackwell optimality", "journal": "", "year": "2023", "authors": "Julien Grand-Clement; Marek Petrik; Nicolas Vieille"}, {"ref_id": "b12", "title": "Recurrent neural networks for stochastic control in real-time bidding", "journal": "", "year": "2019", "authors": "Nicolas Grislain; Nicolas Perrin; Antoine Thabault"}, {"ref_id": "b13", "title": "Learning-based robust optimization: Procedures and statistical guarantees", "journal": "", "year": "2020", "authors": "L ; Jeff Hong; Zhiyuan Huang; Henry Lam"}, {"ref_id": "b14", "title": "Prediction-driven surge planning with application in the emergency department", "journal": "", "year": "2021", "authors": "Yue Hu; Carri W Chan; Jing Dong"}, {"ref_id": "b15", "title": "A study of distributionally robust multistage stochastic optimization", "journal": "", "year": "2017", "authors": "Jianqiu Huang; Kezhuo Zhou; Yongpei Guan"}, {"ref_id": "b16", "title": "Mathematics of Operations Research", "journal": "", "year": "2005", "authors": "N Garud;  Iyengar"}, {"ref_id": "b17", "title": "Deep reinforcement learning for autonomous driving: A survey", "journal": "IEEE Transactions on Intelligent Transportation Systems", "year": "2021", "authors": "Ibrahim B Ravi Kiran; Victor Sobh; Patrick Talpaert; Ahmad A Al Mannion; Senthil Sallab; Patrick Yogamani;  P\u00e9rez"}, {"ref_id": "b18", "title": "Reinforcement learning in robotics: Applications and real-world challenges", "journal": "Robotics", "year": "2013", "authors": "Petar Kormushev; Darwin G Sylvain Calinon;  Caldwell"}, {"ref_id": "b19", "title": "A reinforcement learning approach to view planning for automated inspection tasks", "journal": "Sensors", "year": "", "authors": "Christian Landgraf; Bernd Meese; Michael Pabst; Georg Martius; Marco F Huber"}, {"ref_id": "b20", "title": "Bandit algorithms", "journal": "Cambridge University Press", "year": "2020", "authors": "Tor Lattimore; Csaba Szepesv\u00e1ri"}, {"ref_id": "b21", "title": "Robust, risk-sensitive, and data-driven control of Markov decision processes", "journal": "", "year": "2007", "authors": "Yann Le Tallec"}, {"ref_id": "b22", "title": "Discounted stochastic games with no stationary nash equilibrium: two examples", "journal": "Econometrica", "year": "2013", "authors": "Yehuda Levy"}, {"ref_id": "b23", "title": "Rectangularity and duality of distributionally robust markov decision processes", "journal": "", "year": "2023", "authors": "Yan Li; Alexander Shapiro"}, {"ref_id": "b24", "title": "Markov games as a framework for multi-agent reinforcement learning", "journal": "Elsevier", "year": "1994", "authors": " Michael L Littman"}, {"ref_id": "b25", "title": "Distributionally robust Q-learning", "journal": "PMLR", "year": "2022-07", "authors": "Zijian Liu; Qinxun Bai; Jose Blanchet; Perry Dong; Wei Xu; Zhengqing Zhou; Zhengyuan Zhou"}, {"ref_id": "b26", "title": "Wasserstein distributionally robust linearquadratic estimation under martingale constraints", "journal": "PMLR", "year": "2023-04", "authors": "Kyriakos Lotidis; Nicholas Bambos; Jose Blanchet; Jiajin Li"}, {"ref_id": "b27", "title": "Model-free nonstationary rl: Near-optimal regret and applications in multi-agent rl and inventory control", "journal": "", "year": "2020", "authors": "Weichao Mao; Kaiqing Zhang; Ruihao Zhu; David Simchi-Levi; Tamer Ba\u015far"}, {"ref_id": "b28", "title": "Markov perfect equilibrium: I. observable actions", "journal": "Journal of Economic Theory", "year": "2001", "authors": "Eric Maskin; Jean Tirole"}, {"ref_id": "b29", "title": "Human-level control through deep reinforcement learning", "journal": "nature", "year": "2015", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Andrei A Rusu; Joel Veness; G Marc; Alex Bellemare; Martin Graves; Andreas K Riedmiller; Georg Fidjeland;  Ostrovski"}, {"ref_id": "b30", "title": "Information structures and values in zero-sum stochastic games", "journal": "IEEE", "year": "2017", "authors": "Ashutosh Nayyar; Abhishek Gupta"}, {"ref_id": "b31", "title": "Common information based markov perfect equilibria for stochastic games with asymmetric information: Finite games", "journal": "IEEE Transactions on Automatic Control", "year": "2013", "authors": "Ashutosh Nayyar; Abhishek Gupta; Cedric Langbort; Tamer Ba\u015far"}, {"ref_id": "b32", "title": "Robust control of markov decision processes with uncertain transition matrices", "journal": "Operations Research", "year": "2005", "authors": "Arnab Nilim; Laurent El Ghaoui"}, {"ref_id": "b33", "title": "On a new class of nonzero-sum discounted stochastic games having stationary nash equilibrium points", "journal": "International Journal of Game Theory", "year": "2003", "authors": "S Andrzej;  Nowak"}, {"ref_id": "b34", "title": "Virtual to real reinforcement learning for autonomous driving", "journal": "", "year": "2017", "authors": "Xinlei Pan; Yurong You; Ziyan Wang; Cewu Lu"}, {"ref_id": "b35", "title": "Sample complexity of robust reinforcement learning with a generative model", "journal": "", "year": "2021", "authors": "Kishan Panaganti; Dileep Kalathil"}, {"ref_id": "b36", "title": "Existence of stationary equilibrium strategies in non-zero sum discounted stochastic games with uncountable state space and state-independent transitions", "journal": "International Journal of Game Theory", "year": "1989", "authors": "T Parthasarathy;  Sinha"}, {"ref_id": "b37", "title": "Mathematical foundations of distributionally robust multistage optimization", "journal": "SIAM Journal on Optimization", "year": "2021", "authors": "Alois Pichler; Alexander Shapiro"}, {"ref_id": "b38", "title": "The simplex method is strongly polynomial for deterministic markov decision processes", "journal": "", "year": "2013", "authors": "Ian Post; Yinyu Ye"}, {"ref_id": "b39", "title": "Ambiguous dynamic treatment regimes: A reinforcement learning approach", "journal": "Management Science", "year": "2023", "authors": "Soroush Saghafian"}, {"ref_id": "b40", "title": "Safe, multi-agent, reinforcement learning for autonomous driving", "journal": "", "year": "2016", "authors": "Shai Shalev-Shwartz; Shaked Shammah; Amnon Shashua"}, {"ref_id": "b41", "title": "Distributionally robust optimal control and MDP modeling", "journal": "Operations Research Letters", "year": "2021", "authors": "Alexander Shapiro"}, {"ref_id": "b42", "title": "Distributionally robust modeling of optimal control", "journal": "Operations Research Letters", "year": "2022", "authors": "Alexander Shapiro"}, {"ref_id": "b43", "title": "Technical note-time inconsistency of optimal policies of distributionally robust inventory models", "journal": "Operations Research", "year": "2020", "authors": "Alexander Shapiro; Linwei Xin"}, {"ref_id": "b44", "title": "Lectures on stochastic programming: modeling and theory", "journal": "SIAM", "year": "2021", "authors": "Alexander Shapiro; Darinka Dentcheva; Andrzej Ruszczynski"}, {"ref_id": "b45", "title": "Stochastic games", "journal": "", "year": "1953", "authors": "S Lloyd;  Shapley"}, {"ref_id": "b46", "title": "Distributionally robust model-based offline reinforcement learning with near-optimal sample complexity", "journal": "", "year": "2022", "authors": "Laixi Shi; Yuejie Chi"}, {"ref_id": "b47", "title": "The curious price of distributional robustness in reinforcement learning with a generative model", "journal": "", "year": "2023", "authors": "Laixi Shi; Gen Li; Yuting Wei; Yuxin Chen; Matthieu Geist; Yuejie Chi"}, {"ref_id": "b48", "title": "Mastering the game of go without human knowledge", "journal": "nature", "year": "2017", "authors": "David Silver; Julian Schrittwieser; Karen Simonyan; Ioannis Antonoglou; Aja Huang; Arthur Guez; Thomas Hubert; Lucas Baker; Matthew Lai; Adrian Bolton"}, {"ref_id": "b49", "title": "A general reinforcement learning algorithm that masters chess, shogi, and go through self-play", "journal": "Science", "year": "2018", "authors": "David Silver; Thomas Hubert; Julian Schrittwieser; Ioannis Antonoglou; Matthew Lai; Arthur Guez; Marc Lanctot; Laurent Sifre; Dharshan Kumaran; Thore Graepel"}, {"ref_id": "b50", "title": "Games of incomplete information, ergodic theory, and the measurability of equilibria", "journal": "Israel Journal of Mathematics", "year": "2003", "authors": "Robert Samuel; Simon "}, {"ref_id": "b51", "title": "On general minimax theorems", "journal": "Pacific Journal of Mathematics", "year": "1958", "authors": "Maurice Sion"}, {"ref_id": "b52", "title": "Stochastic games", "journal": "Proceedings of the National Academy of Sciences", "year": "2015", "authors": "Eilon Solan; Nicolas Vieille"}, {"ref_id": "b53", "title": "Equilibrium points of stochastic non-cooperative n-person games", "journal": "", "year": "1964", "authors": "Masayuki Takahashi"}, {"ref_id": "b54", "title": "", "journal": "Distributionally robust linear quadratic control", "year": "2023", "authors": "Dan A Bahar Ta\u015fkesen;  Iancu; Daniel Ko\u00e7yigit;  Kuhn"}, {"ref_id": "b55", "title": "A finite sample complexity bound for distributionally robust Q-learning", "journal": "", "year": "2023", "authors": "Shengbo Wang; Nian Si; Jose Blanchet; Zhengyuan Zhou"}, {"ref_id": "b56", "title": "Sample complexity of variance-reduced distributionally robust Q-learning", "journal": "", "year": "2023", "authors": "Shengbo Wang; Nian Si; Jose Blanchet; Zhengyuan Zhou"}, {"ref_id": "b57", "title": "Q-learning", "journal": "Machine learning", "year": "1992", "authors": "Jch Christopher; Peter Watkins;  Dayan"}, {"ref_id": "b58", "title": "Robust markov decision processes", "journal": "", "year": "2013", "authors": "Wolfram Wiesemann; Daniel Kuhn; Ber\u00e7 Rustem"}, {"ref_id": "b59", "title": "Distributionally robust markov decision processes", "journal": "", "year": "2005", "authors": "Huan Xu; Shie Mannor"}, {"ref_id": "b60", "title": "Improved sample complexity bounds for distributionally robust reinforcement learning", "journal": "", "year": "2023", "authors": "Zaiyan Xu; Kishan Panaganti; Dileep Kalathil"}, {"ref_id": "b61", "title": "Wasserstein distributionally robust stochastic control: A data-driven approach", "journal": "IEEE Transactions on Automatic Control", "year": "2021", "authors": "Insoon Yang"}, {"ref_id": "b62", "title": "Towards theoretical understandings of robust markov decision processes: Sample complexity and asymptotics", "journal": "", "year": "2021", "authors": "Wenhao Yang; Liangyu Zhang; Zhihua Zhang"}, {"ref_id": "b63", "title": "Avoiding model estimation in robust markov decision processes with a generative model", "journal": "", "year": "2023", "authors": "Wenhao Yang; Han Wang; Tadashi Kozuno; Scott M Jordan; Zhihua Zhang"}, {"ref_id": "b64", "title": "Data-driven hospital admission control: A learning approach", "journal": "Operations Research", "year": "", "authors": "Mohammad Zhalechian; Esmaeil Keyvanshokooh; Cong Shi; Mark P Van Oyen"}, {"ref_id": "b65", "title": "Deep reinforcement learning for sponsored search real-time bidding", "journal": "", "year": "2018", "authors": "Jun Zhao; Guang Qiu; Ziyu Guan; Wei Zhao; Xiaofei He"}, {"ref_id": "b66", "title": "Finite-sample regret bound for distributionally robust offline tabular reinforcement learning", "journal": "PMLR", "year": "2021-04", "authors": "Zhengqing Zhou; Zhengyuan Zhou; Qinxun Bai; Linhai Qiu; Jose Blanchet; Peter Glynn"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "a) : (s, a) \u2208 S \u00d7 A} , \u03ba t (s, a) \u2208 P s,a , \u2200(s, a) \u2208 S \u00d7 A, t \u2265 0} , K SA S = {(\u03ba, \u03ba, . . . ) : \u03ba = {\u03ba(s, a) : (s, a) \u2208 S \u00d7 A} , \u03ba(s, a) \u2208 P s,a , \u2200(s, a) \u2208 S \u00d7 A} .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "is Markov time-homogeneous, \u2200t \u2265 0 . (2.7) Similar to the SA-rectangular setting, we can use the notation identification in (2.2) to represent K S M = {(\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t = {\u03ba t (s, a) : (s, a) \u2208 S \u00d7 A} , \u03ba t (s, \u2022) \u2208 P s , \u2200s \u2208 S, t \u2265 0} , K S S = {(\u03ba, \u03ba, . . . ) : \u03ba = {\u03ba(s, a) : (s, a) \u2208 S \u00d7 A} , \u03ba(s, \u2022) \u2208 P s , \u2200s \u2208 S} .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "to the expectation E Z\u223c\u03bd [f (Z)]. Through an abuse of notation, we write \u03bd[e] = \u03bd({e}) when e \u2208 E, equivalent to the probability of event e under measure \u03bd. Moreover, for d \u2208 P(A) and p s = {p s,a : a \u2208 A}, we define the measure d \u2297 p s \u2208 P(S \u00d7 A) by d \u2297 p s [f ] = a\u2208A d[a] s \u2032 \u2208S p s,a [s \u2032 ]f (s, a) (2.14) for all f : S \u00d7 A \u2192 R. Of course, when u : S \u2192 R, we have d \u2297 p s [u] = d \u2297 p s [u1 a\u2208A ] = a\u2208A d[a]p s,a [u].", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Definition 5 .5Define the DR Bellman equation for the q-function as q(s, a) = r(s, a) + \u03b3 inf ps\u2208Ps to note that for each s, the infimum chooses a p s = {p s,a : a \u2208 A} for each a \u2208 A; i.e. such choice could be different across a. Effectively, for different controller action's action a \u2208 A, q(s, a) = r(s, a) particular, {p s,a : p s \u2208 P s } is the marginalization (see (2.11)) of P s . With the DR Bellman equation for the q-function defined, we explore the connection of its solution with that of the Bellman equation of the value function (3.1). Let u * denote the unique solution of (3.1). Then, we define the optimal q-function as follows: for each (s, a) \u2208 S \u00d7 A q * (s, a) = r(s, a) + \u03b3 inf ps\u2208Ps p s,a [u * ].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "p s )for all s \u2208 S. Therefore, u * satisfies (3.2).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Theorem 3 .3Suppose {P s : s \u2208 S} are SA-rectangular (c.f. (2.8)) and Q D \u2282 Q. Let u * be the solution of (3.1). Then q * (s, a) = r(s, a) + \u03b3 inf ps,a\u2208Ps,a p s,a [u * ] (3.6) solves the Bellman equation of the q-function (3.4) and u * (\u2022) = max a\u2208A q * (\u2022, a). Moreover, u * solves (3.2) and hence conclusions of Theorem 2 holds.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 1 :1Figure 1: The adversarial actions in the adversary's action distribution set, where the red line and the blue line represent actions a 1 and a 2 , respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": ") = \u22121 + \u03b3u(I).(5.4) Solving the equations, we have that the solution u * to (5.2 -5.4) is u * (I) = 0, u * (G) = 1, u * (B) = \u22121.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 2 :2Figure 2: The adversarial actions in the action distribution set, where the red line and the blue line represent actions a 1 and a 2 , respectively.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "IFigure 3 :3Figure 3: The extreme point of adversarial actions in the action distribution set, where the red line and the blue line represent actions a 1 and a 2 , respectively.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "we assume that rewards only depend on the states:r(I0) = 0, r(I1) = 0, r(I2) = 0, r(I) = 0, r(G) = 1, r(B) = \u22121.We compute the solution u * (\u2022) of the DR Bellman equation: ), u(I1) = \u03b3u(I), u(I2) = \u03b3u(I), u(G) = 1 + \u03b3u(I), u(B) = \u22121 + \u03b3u(I).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "I, \u03c0, p \u03b1 I ) = 0 + \u03b3\u03b1v 3 (B, \u03c0, p \u03b1 I ) + \u03b3(1 \u2212 \u03b1)v 3 (G, \u03c0, p \u03b1 I ), v 3 (B, \u03c0, p \u03b1 I ) = \u22121 + \u03b3v 4 (I, \u03c0, p \u03b1 I ), v 3 (G, \u03c0, p \u03b1 I ) = 1 + \u03b3v 4 (I, \u03c0, p \u03b1 I ), v 4 (I, \u03c0, p \u03b1 I ) = 0 + \u03b3\u03b1v 5 (G, \u03c0, p \u03b1 I ) + \u03b3(1 \u2212 \u03b1)v 5 (B, \u03c0, p \u03b1 I ), v 5 (B, \u03c0, p \u03b1 I ) = \u22121 + \u03b3v 6 (I, \u03c0, p \u03b1 I ), v 5 (G, \u03c0, p \u03b1 I ) = 1 + \u03b3v 6 (I, \u03c0, p \u03b1 I ), v 6 (I, \u03c0, p \u03b1 I ) = v 2 (I, \u03c0, p \u03b1 I ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "a = p = {p s,a : (s, a) \u2208 S \u00d7 A} : p s,a \u2208 P s,a .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "(h 0 ) r(s 0 , a 0 ) + \u03b3 sup d1(g0,\u2022)\u2208{S\u2192Q} inf \u03ba1(g0,\u2022,\u2022)\u2208P \u03ba 0 (g 0 ) [d 1 (h 1 ) [. . . [u]]] ,(6.4)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Item 2 in Theorem 2. Note that by assumption, elementary properties of sup-inf, and set inclusions, it is clear that \u00b5[u * ] = sup , \u03c0, \u03ba) \u2264 \u00b5[u * ].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "(3.4) can be written as q(s, a) = r(s, a) + \u03b3 inf s, \u2022)] + \u03b3d \u2297 p s [u * ] ]r(s, a) + \u03b3d[a]p s,a [u * ] ps,a |A| \u2208Ps,a |A| a\u2208A d[a]r(s, a) + \u03b3d[a]p s,a [u * ] follows from SA-rectangularity and the enumeration A = a 1 , a 2 , . . . , a |A| , and (ii) follows from a standard result in linear programming that optima are achieved at extreme points Q D of P(A). On the other hand, since Q D \u2282 Q, u * (s) \u2265 max a\u2208A inf ps\u2208Ps r(s, a) + \u03b3\u03b4 {a} \u2297 p s [u * ] = max a\u2208A r(s, a) + \u03b3 inf ps\u2208Ps p s,a [u * ] again follows from SA-rectangularity. Therefore, u * (s) = max a\u2208A q * (s, a) and hence q * solves (A.2). Finally, we show that u * solves (3.2). By the definition of q * , for any \u03b4 > 0, there exists \u03c8(s, a) \u2208 P s,a s.t. r(s, a) + \u03b3\u03c8(s, a)[u * ] \u2212 \u03b4 \u2264 q * (s, a) for all (s, a) \u2208 S \u00d7 A. Then, we consider u * (s) \u2264 inf ps\u2208Ps sup d\u2208Q d[r(s, \u2022)] + \u03b3d \u2297 p s [u * ] \u2264 sup d\u2208P(A) d[r(s, \u2022)] + \u03b3d \u2297 \u03c8(s, \u2022)[u * ] , a) + \u03b3\u03c8(s, a)[u * ] \u2264 max a\u2208A q * (s, a) + \u03b4 = u * (s) + \u03b4", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": ".\u03b3In light of (B.1), we choose T so that for all \u03c0 \u2208 \u03a0 H and \u03ba\u2208 K S H , k r(X k , A k ) + \u03b3 T u * (X T ) (B.2) satisfies |v \u03c0 \u00b5 (T, \u03ba) \u2212 v(\u00b5, \u03c0, \u03ba)| \u2264 \u01eb/2.B.1 Proof of Lemma 1Proof. The first inequality follows simply from\u03a0 C H \u2283 \u03a0 C M \u2283 \u03a0 C S .For the second inequality, recall the inclusion relation (2.10). Thus, for any\u03c0 \u2208 \u03a0 = \u03a0 C S , \u03a0 C M , \u03a0 C H inf \u03ba\u2208K S H v(\u00b5, \u03c0, \u03ba) \u2264 inf\u03ba\u2208K S M v(\u00b5, \u03c0, \u03ba) \u2264 inf \u03ba\u2208K S S v(\u00b5, \u03c0, \u03ba).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "s, \u2022)] + \u03b3d \u2297 p s [u * ] \u2265 inf ps\u2208Ps \u2206 k\u22121 (s)[r(s, \u2022)] + \u03b3\u2206 k\u22121 (s) \u2297 p s [u * ] (B.4) there exists {q s \u2208 P s : s \u2208 S} s.t. u * (s) + \u03b4 \u2265 \u2206 k\u22121 (s)[r(s, \u2022)] + \u03b3\u2206 k\u22121 (s) \u2297 q s [u * ].Then, for each s and q s = {q s,a : a \u2208 A} as above, we define\u03c8 k\u22121 (s, a) = {q s,a : s, a \u2208 S \u00d7 A}. Then \u2206 k\u22121 (s)[r(s, \u2022)] + \u03b3\u2206 k\u22121 (s) \u2297 \u03c8 k\u22121 (s, \u2022)[u * ] \u2264 u * (s) + \u03b4. (B.5)Doing this for k = 1, . . . , T will complete the construction. Now, we claim and defer the proof of the inequalityv \u03c0 \u00b5 (k, \u03ba (k) ) \u2264 v \u03c0 \u00b5 (k \u2212 1, \u03ba (k\u22121) ) + \u03b3 k\u22121 \u03b4 (B.6)for all 1 \u2264 k \u2264 T . With (B.6), we see thatv \u03c0 \u00b5 (T, \u03ba (T ) ) \u2264 v \u03c0 \u00b5 (T \u2212 1, \u03ba (T \u22121) ) + \u03b3 T \u22121 \u03b4 \u2264 v \u03c0 \u00b5 (0, \u03ba (0) ) +We choose \u03b4 = (1 \u2212 \u03b3)\u01eb/2. Since \u03ba(T )  ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "k\u22121 (s, a)(s \u2032 )u * (s \u2032 ) X k\u22121 = s = s \u2032 \u2208S a\u2208A \u03c8 k\u22121 (s, a)[s \u2032 ]u * (s \u2032 )E \u03c0,\u03ba (k) \u00b5 [d k\u22121 (H k\u22121 )[a]|X k\u22121 = s] a\u2208A \u03c8 k\u22121 (s, a)[s \u2032 ]u * (s \u2032 )E \u03c0,\u03ba (k\u22121) \u00b5 [d k\u22121 (H k\u22121 )[a]|X k\u22121 = s] = s \u2032 \u2208S a\u2208A \u03c8 k\u22121 (s, a)[s \u2032 ]u * (s \u2032 )\u2206 k\u22121 (s)[a] = \u2206 k\u22121 (s) \u2297 \u03c8 k\u22121 (s, \u2022)[u * ]", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "follows from (B.5). With this result, we can boundv \u03c0 \u00b5 (k, \u03ba (k) ) = E\u03c0,\u03ba (k)    ", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_21", "figure_caption": "k\u22121 \u03b4 as claimed, where (i) follows from the quantity inside the expectation is a bounded function of H k\u22121 . B.4 Proof of Proposition 3 Proof. It is equivalent to show that for any \u03c0 \u2208 \u03a0 \u03c0, \u03ba) \u2264 \u00b5[u * ]; for U = H, M, S, respectively. Fix any \u01eb > 0. Given a policy \u03c0 = (d 0 , d 1 , . . . ) \u2208 \u03a0 C H (or \u03a0 C M , \u03a0 C S ), we prove this by constructing an adversarial policy \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) \u2208 K S H (or K S M , K S S resp.), so that v(\u00b5, \u03c0, \u03ba) \u2264 \u00b5[u * ] + \u01eb.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Randomized or deterministic controller versus SA-rectangular adversary policy classes.", "figure_data": "Adversary"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Randomized controller versus convex S-rectangular adversary policy classes.", "figure_data": "Convex AdversaryHistory-dependentMarkovTime-homogeneousRandomizedControllerHistory-dependent Markov Time-homogeneous\u2714 \u2714 \u2714\u2714 Xu and Mannor [2010] \u2714 Li and Shapiro [2023] \u2714\u2714 Wiesemann et al. [2013] Xu and Mannor [2010] \u2714 \u2714 Le Tallec [2007]"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Randomized controller versus possibly non-convex S-rectangular adversary policy classes.", "figure_data": "Non-Convex AdversaryHistory-dependentMarkovTime-homogeneousRandomizedControllerHistory-dependent Markov Time-homogeneous\u2714 \u2714 \u2714\u2714 \u2714 Li and Shapiro [2023] \u2714\u2717 Wiesemann et al. [2013] \u2717 \u2714Le Tallec [2007]"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Deterministic controller versus convex or non-convex S-rectangular adversary policy classes.", "figure_data": "Convex/Non-Convex AdversaryHistory-dependent Markov Time-homogeneousDeterministicControllerHistory-dependent Markov Time-homogeneous\u2714 \u2714 \u2714\u2717 \u2714 \u2714\u2717 \u2717 \u2714"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "is called Markov time-homogeneous (a.k.a. static as in Iyengar[2005]) if for any t 1 , t 2 \u2265 0 and h t1", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "and K SA S (t) can be analogously defined when \u03ba t being Markov or Markov time-homogeneous. To avoid introducing unused notations, we omit the details. The SA-rectangular set of history-dependent adversarial policies is K SA H", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Adversarial actions at time t and history g t\u22121 for different notions of rectangularity.", "figure_data": "RectangularitySASGeneralAdversarial Action SetsP s,a \u2282 P(S)P s \u2282 {A \u2192 P(S)}P \u2282 {S \u00d7 A \u2192 P(S)}Adversarial Action\u03ba"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Adversary's policy classes. SA: {P s,a : (s, a) \u2208 S \u00d7 A} S: {P s : s \u2208 S} General: P \u2265 0 are adversarial decision rules. Consequently, it is natural to explore the inclusion relationships among these adversary policy classes.By the definitions in the previous sections, it is clear that every adversarial policy sets defined in Table6is a subset of K H . Moreover, ordering in terms of the memory of the adversary, we have that", "figure_data": "History-dependentK SA HK S HK N HMarkovK SA MK S MK N MMarkov Time-homogeneousK SA SK S SK N S\u03ba t , t K SA H \u2283 K SA M \u2283 K SA S(2.10)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "sup \u03c0\u2208\u03a0 inf \u03ba\u2208K E \u03c0,\u03ba \u00b5 \u221e k=0 \u03b3 k r(X t , A t ) , (1.1)", "formula_coordinates": [3.0, 239.52, 464.09, 307.71, 31.2]}, {"formula_id": "formula_1", "formula_text": "u(s) = sup d\u2208Q inf ps\u2208Ps E a\u223cd [r(s, a)] + \u03b3E s \u2032 \u223cd\u2297ps [u(s \u2032 )], s \u2208 S, (1.2)", "formula_coordinates": [4.0, 183.72, 357.17, 363.51, 19.2]}, {"formula_id": "formula_2", "formula_text": "\u03a0 C H := {\u03c0 = (d 0 , d 1 , . . . ) : d t \u2208 H t \u2192 Q, \u2200t \u2265 0} . (2.1)", "formula_coordinates": [12.0, 204.6, 258.41, 342.63, 13.57]}, {"formula_id": "formula_3", "formula_text": "A decision rule d t is Markov if for all h t , h \u2032 t \u2208 H t s.t. s t = s \u2032 t , then d t (h t ) = d t (h \u2032 t )", "formula_coordinates": [12.0, 87.0, 313.85, 362.1, 12.84]}, {"formula_id": "formula_4", "formula_text": "\u03a0 C M := {\u03c0 = (d 0 , d 1 , . . . ) : d t \u2208 H t \u2192 Q is Markov, \u2200t \u2265 0} . A contoller's policy \u03c0 = (d 0 , d 1 , . . . )", "formula_coordinates": [12.0, 87.0, 382.73, 351.36, 38.77]}, {"formula_id": "formula_5", "formula_text": "\u2208 H t1 , h \u2032 t2 \u2208 H t2 s.t. s t1 = s \u2032 t2 , then d t1 (h t1 ) = d t2 (h \u2032 t2 ).", "formula_coordinates": [12.0, 204.36, 423.89, 263.16, 12.72]}, {"formula_id": "formula_6", "formula_text": "\u03a0 C S := {\u03c0 = (d, d, . . . ) : d \u2208 S \u2192 Q} .", "formula_coordinates": [12.0, 229.8, 495.05, 159.6, 13.57]}, {"formula_id": "formula_7", "formula_text": "\u03a0 H /\u03a0 M /\u03a0 S . Second, if Q = Q D := \u03b4 {a} : a \u2208 A , then we denote the deterministic history-dependent/Markov/Markov time-homogeneous policy classes as \u03a0 D H /\u03a0 D M /\u03a0 D S respectively.", "formula_coordinates": [12.0, 72.0, 547.17, 475.89, 40.65]}, {"formula_id": "formula_8", "formula_text": "g t1 \u2208 G t1 , g \u2032 t2 \u2208 G t2 s.t. (s t1 , a t1 ) = (s \u2032 t2 , a \u2032 t2 ), then \u03ba t1 (g t1 ) = \u03ba t2 (g \u2032 t2 )", "formula_coordinates": [13.0, 138.84, 290.69, 321.18, 12.84]}, {"formula_id": "formula_9", "formula_text": "K SA H (t) := {\u03ba t : \u03ba t (g t\u22121 , s, a) \u2208 P s,a , \u2200(s, a) \u2208 S \u00d7 A, g t\u22121 \u2208 G t\u22121 } . (2.3) K SA M (t)", "formula_coordinates": [13.0, 72.0, 623.81, 475.22, 40.09]}, {"formula_id": "formula_10", "formula_text": ":= \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K SA H (t), \u2200t \u2265 0 = t\u22650 K SA H (t).", "formula_coordinates": [13.0, 197.04, 693.17, 246.36, 22.68]}, {"formula_id": "formula_11", "formula_text": "K SA M := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K SA H (t) is Markov, \u2200t \u2265 0 K SA S := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K SA H (t)", "formula_coordinates": [14.0, 138.96, 111.89, 247.38, 30.85]}, {"formula_id": "formula_12", "formula_text": "K SA M = {(\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t = {\u03ba t (s,", "formula_coordinates": [14.0, 98.4, 193.25, 143.4, 13.57]}, {"formula_id": "formula_13", "formula_text": "\u03ba t = {\u03ba t (s, a) : (s, a) \u2208 S \u00d7 A} \u2208 (s,a)\u2208S\u00d7A", "formula_coordinates": [14.0, 190.32, 320.85, 184.55, 21.58]}, {"formula_id": "formula_14", "formula_text": "K S H (t) := {\u03ba t : \u03ba t (g t\u22121 , s, \u2022) \u2208 P s , \u2200s \u2208 S, g t\u22121 \u2208 G t\u22121 } (2.6)", "formula_coordinates": [14.0, 192.12, 631.61, 355.11, 13.57]}, {"formula_id": "formula_15", "formula_text": "K S H := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K S H (t), \u2200t \u2265 0 K S M := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K S H (t) is Markov, \u2200t \u2265 0 K S S := \u03ba = (\u03ba 0 , \u03ba 1 , . . . ) : \u03ba t \u2208 K S H (t)", "formula_coordinates": [15.0, 142.68, 98.09, 239.94, 48.25]}, {"formula_id": "formula_16", "formula_text": "I t+1 = (I t + a t + D t ) +", "formula_coordinates": [15.0, 260.4, 381.21, 97.92, 10.65]}, {"formula_id": "formula_17", "formula_text": "P s := {p s = {p s,a : a \u2208 A} : p s,a \u2208 P s,a } = a\u2208A P s,a", "formula_coordinates": [15.0, 198.72, 593.85, 221.28, 20.96]}, {"formula_id": "formula_18", "formula_text": "K N H (t) := {\u03ba t : \u03ba t (g t\u22121 , \u2022, \u2022) \u2208 P, \u2200g t\u22121 \u2208 G t\u22121 } . (2.9)", "formula_coordinates": [16.0, 206.28, 142.01, 340.94, 13.45]}, {"formula_id": "formula_19", "formula_text": "K SA H \u2283 K S H \u2283 K N H (2.12)", "formula_coordinates": [17.0, 272.88, 462.65, 274.47, 13.57]}, {"formula_id": "formula_20", "formula_text": "P \u03c0,\u03ba \u00b5 (G t (\u03c9) = g t ) = \u00b5(s 0 )d 0 (s 0 )[a 0 ]\u03ba 0 (s 0 , a 0 )[s 1 ]d 1 (s 0 , a 0 , s 1 )[a 1 ]\u03ba 1 (s 0 , a 0 , s 1 , a 1 )[s 2 ] . . . d t (h t )[a t ] (2.15)", "formula_coordinates": [18.0, 88.2, 298.63, 459.14, 13.06]}, {"formula_id": "formula_21", "formula_text": "v(\u00b5, \u03c0, \u03ba) := E \u03c0,\u03ba \u00b5 \u221e k=0 \u03b3 k r(X k , A k ) .", "formula_coordinates": [18.0, 228.12, 465.53, 162.96, 31.2]}, {"formula_id": "formula_22", "formula_text": "\u03c0\u2208\u03a0 inf \u03ba\u2208K v(\u00b5, \u03c0, \u03ba). (2.", "formula_coordinates": [18.0, 302.4, 537.33, 232.15, 17.36]}, {"formula_id": "formula_23", "formula_text": "v * (\u00b5, \u03a0 C H , K) \u2265 v * (\u00b5, \u03a0 C M , K) \u2265 v * (\u00b5, \u03a0 C S , K) for K = K S H , K S M , K S S and v * (\u00b5, \u03a0, K S H ) \u2264 v * (\u00b5, \u03a0, K S M ) \u2264 v * (\u00b5, \u03a0, K S S ) for \u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S as defined in (2.1).", "formula_coordinates": [18.0, 72.0, 638.57, 332.42, 75.97]}, {"formula_id": "formula_24", "formula_text": "u(s) = sup d\u2208Q inf ps\u2208Ps d[r(s, \u2022)] + \u03b3d \u2297 p s [u], s \u2208 S. (3.1)", "formula_coordinates": [19.0, 207.36, 358.41, 339.87, 17.36]}, {"formula_id": "formula_25", "formula_text": "u(s) = inf ps\u2208Ps sup d\u2208Q d[r(s, \u2022)] + \u03b3d \u2297 p s [u], s \u2208 S. (3.2)", "formula_coordinates": [19.0, 207.36, 424.29, 339.87, 17.36]}, {"formula_id": "formula_26", "formula_text": "u * \u221e \u2264 1/(1 \u2212 \u03b3).", "formula_coordinates": [19.0, 388.68, 489.53, 80.57, 12.01]}, {"formula_id": "formula_27", "formula_text": "\u00b5[u * ] = v * (\u00b5, \u03a0 C H , K S H ) = v * (\u00b5, \u03a0 C M , K S H ) = v * (\u00b5, \u03a0 C M , K S M ) = v * (\u00b5, \u03a0 C S , K S H ) = v * (\u00b5, \u03a0 C S , K S M ) = v * (\u00b5, \u03a0 C S , K S S )", "formula_coordinates": [20.0, 129.6, 230.09, 360.03, 48.25]}, {"formula_id": "formula_28", "formula_text": "u * (s) = sup d\u2208Q D d[r(s, \u2022)] + \u03b3 inf ps\u2208Ps d \u2297 p s [u * ] = max a\u2208A r(s, a) + \u03b3 inf ps\u2208Ps p s,a [u * ] = max a\u2208A q * (s, a).", "formula_coordinates": [21.0, 219.24, 225.53, 180.72, 64.8]}, {"formula_id": "formula_29", "formula_text": "K = K S H , K S M , K S S and \u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S : 1. The optimal values satisfy v * (\u00b5, \u03a0, K) = \u00b5[u * ].", "formula_coordinates": [21.0, 72.0, 554.21, 475.13, 61.24]}, {"formula_id": "formula_30", "formula_text": "v * (\u00b5, \u03a0, K) = sup \u03c0\u2208\u03a0 inf \u03ba\u2208K v * (\u00b5, \u03c0, \u03ba) = inf \u03ba\u2208K sup \u03c0\u2208\u03a0 v * (\u00b5, \u03c0, \u03ba).", "formula_coordinates": [21.0, 203.88, 655.13, 236.28, 19.2]}, {"formula_id": "formula_31", "formula_text": "w * s (d, p s ) := d[r(s, \u2022)] + d \u2297 p s [u * ]. (3.5)", "formula_coordinates": [22.0, 236.04, 148.37, 311.19, 13.32]}, {"formula_id": "formula_32", "formula_text": "Q \u2282 P(A) is convex if for all d, d \u2032 \u2208 Q, {td + (1 \u2212 t)d \u2032 : t \u2208 [0, 1]} \u2282 Q.", "formula_coordinates": [22.0, 241.8, 226.25, 246.72, 37.6]}, {"formula_id": "formula_33", "formula_text": "{tp s + (1 \u2212 t)p \u2032 s : t \u2208 [0, 1]} \u2282 P s .", "formula_coordinates": [22.0, 237.6, 304.61, 144.0, 13.32]}, {"formula_id": "formula_34", "formula_text": "v * (\u00b5, \u03a0 C H , K S M ) \u2264 \u00b5[u * ]", "formula_coordinates": [23.0, 261.24, 414.53, 96.72, 13.57]}, {"formula_id": "formula_35", "formula_text": "\u00b5[u * ] = v * (\u00b5, \u03a0 C H , K S M ) in addition to the equalities in Theorem 1. Proof of Theorem 4. Given Theorem 1, it remains to show that v * (\u00b5, \u03a0 C H , K S M ) = \u00b5[u * ].", "formula_coordinates": [23.0, 72.0, 529.01, 474.92, 49.93]}, {"formula_id": "formula_36", "formula_text": "\u00b5[u * ] = v * (\u00b5, \u03a0 C S , K S M ) \u2264 v * (\u00b5, \u03a0 C H , K S M ) \u2264 \u00b5[u * ].", "formula_coordinates": [23.0, 204.96, 605.81, 209.28, 13.57]}, {"formula_id": "formula_37", "formula_text": "u * ] = v * (\u00b5, \u03a0 C H , K S M )", "formula_coordinates": [24.0, 235.96, 266.57, 88.79, 13.33]}, {"formula_id": "formula_38", "formula_text": "equation (3.1) if for all s \u2208 S u * (s) \u2264 \u2206(s)[r(s, \u2022)] + \u03b3 inf ps\u2208Ps \u2206(s) \u2297 p s [u * ] + \u03b7.", "formula_coordinates": [24.0, 72.0, 675.21, 342.0, 41.72]}, {"formula_id": "formula_39", "formula_text": "\u03a0 = \u03a0 C H , \u03a0 C M , \u03a0 C S and K = K S H , K S M , K S S \u00b5[u * ] = v * (\u00b5, \u03a0, K).", "formula_coordinates": [25.0, 72.0, 73.13, 475.14, 40.0]}, {"formula_id": "formula_40", "formula_text": "0 \u2264 v * (\u00b5, \u03a0, K) \u2212 inf \u03ba\u2208K v(\u00b5, \u03c0, \u03ba) \u2264 \u03b7 1 \u2212 \u03b3 .", "formula_coordinates": [25.0, 221.28, 158.61, 176.64, 23.52]}, {"formula_id": "formula_41", "formula_text": "d(s) \u2208 arg max a\u2208A q * (s, a)", "formula_coordinates": [25.0, 259.68, 306.05, 99.87, 19.2]}, {"formula_id": "formula_42", "formula_text": "r(I) = 0, r(G) = 1, r(B) = \u22121.", "formula_coordinates": [26.0, 233.28, 440.73, 152.76, 9.96]}, {"formula_id": "formula_43", "formula_text": "v(I, \u03c0, \u03ba) = 0 + 0 \u00d7 \u03b3 + 0 \u00d7 \u03b3 2 + \u221e i=3,i odd \u03b3 i = \u03b3 3 1 \u2212 \u03b3 2 > 0.", "formula_coordinates": [26.0, 183.48, 531.17, 252.24, 31.45]}, {"formula_id": "formula_44", "formula_text": "I B C 1/2 1 1/2 (a) p (1) I B C 1/2 1/2 1/2 1/2 (b) p (2)", "formula_coordinates": [27.0, 165.74, 286.75, 291.13, 108.0]}, {"formula_id": "formula_45", "formula_text": "I , p(2)", "formula_coordinates": [27.0, 297.48, 487.01, 30.35, 14.76]}, {"formula_id": "formula_46", "formula_text": "I,a2 = 1 2 , 1 2 , 0 \u22a4 , p(2)", "formula_coordinates": [27.0, 198.0, 516.41, 209.88, 49.96]}, {"formula_id": "formula_47", "formula_text": "I,a1 = 0, 1 2 , 1 2 \u22a4 and p (2", "formula_coordinates": [27.0, 203.04, 546.65, 142.15, 26.56]}, {"formula_id": "formula_48", "formula_text": ")", "formula_coordinates": [27.0, 345.19, 553.49, 3.4, 6.97]}, {"formula_id": "formula_49", "formula_text": "I,a2 = 1 2 , 0, 1 2 \u22a4 ,", "formula_coordinates": [27.0, 338.4, 546.65, 82.8, 26.56]}, {"formula_id": "formula_50", "formula_text": "d\u2208P(A) inf pI \u2208PI d(a 1 )p \u22a4 I,a1 u * + d(a 2 )p \u22a4 I,a2 u * = \u03b3 sup d(a1)\u2208[0,1] inf d(a 1 ) + 1 + 3 2 (1 \u2212 d(a 1 )), 0 + 3 2 d(a 1 ) + 0 + 1 2 (1 \u2212 d(a 1 )) = 0.8 sup d(a1)\u2208[0,1] inf 2 \u2212 d(a 1 ), 1 2 + d(a 1 ) = 1,", "formula_coordinates": [28.0, 148.68, 127.61, 345.87, 93.76]}, {"formula_id": "formula_53", "formula_text": "I ) = 1.44 > 1,", "formula_coordinates": [28.0, 332.52, 404.85, 66.72, 11.84]}, {"formula_id": "formula_55", "formula_text": "p I0 (I1) = p I0 (I2) = 1 2 .", "formula_coordinates": [28.0, 259.8, 632.97, 99.6, 23.64]}, {"formula_id": "formula_56", "formula_text": "I + (1 \u2212 \u03b1)p (2) I : \u03b1 \u2208 [0, 1] .", "formula_coordinates": [29.0, 429.72, 328.85, 126.12, 14.88]}, {"formula_id": "formula_57", "formula_text": "u * (I) = \u2212 \u03b3 1 \u2212 \u03b3 2 , u * (I1) = u * (I2) = \u2212 \u03b3 2 1 \u2212 \u03b3 2 , u * (I0) = \u2212 \u03b3 3 1 \u2212 \u03b3 2 , u * (G) = 1 \u2212 \u03b3 2 1 \u2212 \u03b3 2 , u * (B) = \u2212 1 1 \u2212 \u03b3 2 .", "formula_coordinates": [29.0, 172.92, 533.93, 273.36, 52.48]}, {"formula_id": "formula_58", "formula_text": "v(I0, \u03c0, p \u03b1 I ) = (1 \u2212 2\u03b1) \u03b3 3 1 + \u03b3 2 \u2265 \u2212 \u03b3 3 1 + \u03b3 2 > u * (I0), v 2 (I, \u03c0, p \u03b1 I ) = (1 \u2212 2\u03b1) \u03b3 1 + \u03b3 2 \u2265 \u2212 \u03b3 1 + \u03b3 2 > u * (I), v 3 (B, \u03c0, p \u03b1 I ) = \u22121 + (2\u03b1 \u2212 1) \u03b3 2 1 + \u03b3 2 , v 3 (G, \u03c0, p \u03b1 I ) = 1 + (2\u03b1 \u2212 1) \u03b3 2 1 + \u03b3 2 v 4 (I, \u03c0, p \u03b1 I ) = (2\u03b1 \u2212 1) \u03b3 1 + \u03b3 2 .", "formula_coordinates": [30.0, 147.6, 230.93, 322.33, 100.12]}, {"formula_id": "formula_59", "formula_text": "f : h T +1 \u2192 R, \u00b5[f (h T +1 )] = s0\u2208S \u00b5[s 0 ]f (s 0 , . . . , a T , s T +1 ), d t (h t )[f (h T +1 )] = at\u2208A d t (h t )[a t ]f (s 0 , . . . , s t , a t , . . . , s T +1 ), \u03ba t (g t )[f (h T +1 )] = st\u2208S \u03ba t (g t )[s t+1 ]f (s 0 , . . . , a t , s t+1 , . . . , s T +1 ).", "formula_coordinates": [31.0, 72.0, 438.93, 373.08, 103.88]}, {"formula_id": "formula_60", "formula_text": "? = v * (\u00b5, \u03a0 C H , K N H )", "formula_coordinates": [34.0, 305.64, 157.13, 70.83, 15.13]}, {"formula_id": "formula_61", "formula_text": "\u03c0\u2208\u03a0 C H v(\u00b5, \u03c0, \u03ba) \u2264 \u00b5[u * ] + \u01eb.", "formula_coordinates": [41.0, 251.16, 145.97, 116.88, 22.32]}, {"formula_id": "formula_62", "formula_text": ") = E \u03c0,\u03ba \u00b5 T \u22121 k=0 \u03b3 k r(X k , A k ) + \u03b3 T u * (X T ) + \u221e k=T \u03b3 k r(X k , A k ) \u2212 \u03b3 T u * (X T ) \u2264 E \u03c0,\u03ba \u00b5 T \u22121 k=0 \u03b3 k r(X k , A k ) + \u03b3 T u * (X T ) + \u01eb 2 (B.1) if \u03b3 T \u2264 \u01eb(1 \u2212 \u03b3)/4", "formula_coordinates": [42.0, 72.0, 458.09, 475.23, 89.68]}, {"formula_id": "formula_63", "formula_text": "\u2206 k\u22121 (s)[\u2022] = h k\u22121 \u2208H k\u22121 E \u03c0,\u03ba (k\u22121) \u00b5 [d k\u22121 (H k\u22121 )[\u2022]1 {H k\u22121 = h k\u22121 }|X k\u22121 = s] = g k\u22122 \u2208G k\u22122 d k\u22121 (g k\u22122 , s)[\u2022]E \u03c0,\u03ba (k\u22121) \u00b5 [1 {G k\u22122 = g k\u22122 }|X k\u22121 = s] .", "formula_coordinates": [44.0, 141.96, 97.65, 335.28, 54.28]}, {"formula_id": "formula_64", "formula_text": "E \u03c0,\u03ba (k) \u00b5 [r(X k\u22121 , A k\u22121 ) + \u03b3u * (X k )] = E \u03c0,\u03ba (k) \u00b5 [d k\u22121 (H k\u22121 )[r(X k\u22121 , \u2022)] + \u03b3d k\u22121 (H k\u22121 ) \u2297 \u03c8 k\u22121 (X k\u22121 , \u2022)[u * ]] (B.7)", "formula_coordinates": [44.0, 158.52, 681.21, 388.71, 33.8]}, {"formula_id": "formula_65", "formula_text": "E \u03c0,\u03ba (k) \u00b5 [d k\u22121 (H k\u22121 ) \u2297 \u03c8 k\u22121 (X k\u22121 , \u2022)[u * ]|X k\u22121 = s] = E \u03c0,\u03ba (k) \u00b5 s \u2032 \u2208S a\u2208A d k\u22121 (H k\u22121 )[a]\u03c8", "formula_coordinates": [45.0, 169.68, 97.65, 227.88, 50.01]}, {"formula_id": "formula_66", "formula_text": "E \u03c0,\u03ba (k) \u00b5 [r(X k\u22121 , A k\u22121 ) + \u03b3u * (X k )|X k\u22121 ] = \u2206 k\u22121 (X k\u22121 )[r(X k\u22121 , \u2022)] + \u03b3\u2206 k\u22121 (X k\u22121 ) \u2297 \u03c8 k\u22121 (X k\u22121 , \u2022)[u * ] (i)", "formula_coordinates": [45.0, 174.0, 304.05, 273.96, 44.25]}, {"formula_id": "formula_67", "formula_text": "\uf8f9 \uf8fb + \u03b3 k\u22121 E \u03c0,\u03ba (k) \u00b5 E \u03c0,\u03ba (k) \u00b5 [r(X k\u22121 , A k\u22121 ) + \u03b3u * (X k )|X k\u22121 ] \u2264 E \u03c0,\u03ba (k) \u00b5 \uf8ee \uf8f0 k\u22122 j=0", "formula_coordinates": [45.0, 152.88, 395.84, 361.32, 73.53]}, {"formula_id": "formula_68", "formula_text": "\uf8f9 \uf8fb + \u03b3 k\u22121 E \u03c0,\u03ba (k) \u00b5 u * (X k\u22121 ) + \u03b3 k\u22121 \u03b4 (i) = E \u03c0,\u03ba (k\u22121) \u00b5 \uf8ee \uf8f0 k\u22122 j=0 \u03b3 j r(X j , A j ) + \u03b3 k\u22121 u * (X k\u22121 ) \uf8f9 \uf8fb + \u03b3 k\u22121 \u03b4 = v \u03c0 \u00b5 (k \u2212 1, \u03ba (k\u22121) ) + \u03b3", "formula_coordinates": [45.0, 152.88, 435.68, 264.7, 93.81]}], "doi": "10.1287/moor.2022.1259"}