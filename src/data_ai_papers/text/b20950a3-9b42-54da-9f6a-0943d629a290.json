{"title": "Nonparametric Density Estimation and Convergence of GANs under Besov IPM Losses", "authors": "Ananya Uppal; Shashank Singh; Barnab\u00e1s P\u00f3czos", "pub_date": "2020-01-13", "abstract": "We study the problem of estimating a nonparametric probability density under a large family of losses called Besov IPMs, which include, for example, L p distances, total variation distance, and generalizations of both Wasserstein and Kolmogorov-Smirnov distances. For a wide variety of settings, we provide both lower and upper bounds, identifying precisely how the choice of loss function and assumptions on the data interact to determine the minimax optimal convergence rate. We also show that linear distribution estimates, such as the empirical distribution or kernel density estimator, often fail to converge at the optimal rate. Our bounds generalize, unify, or improve several recent and classical results. Moreover, IPMs can be used to formalize a statistical model of generative adversarial networks (GANs). Thus, we show how our results imply bounds on the statistical error of a GAN, showing, for example, that GANs can strictly outperform the best linear estimator. * Now at Google. 2 While the name IPM seems most widely used [39,50,7,60], many other names have been used for these quantities, including adversarial loss [48,13], MMD [17], and F-distance or neural net distance [5].", "sections": [{"heading": "Introduction", "text": "This paper studies the problem of estimating a nonparametric probability density, using an integral probability metric as a loss. That is, given a sample space X \u2286 R D , suppose we observe n IID samples X 1 , ..., X n IID \u223c p from a probability density p over X that is unknown but assumed to lie in a regularity class P. We seek an estimator p : X n \u2192 P of p, with the goal of minimizing a loss d F (p, p(X 1 , ..., X n )) := sup\nf \u2208F E X\u223cp [f (X)] \u2212 E X\u223c p(X1,...,Xn) [f (X)] ,( * )\nwhere F, called the discriminator class, is some class of bounded, measurable functions on X .\nMetrics of the form ( * ) are called integral probability metrics (IPMs), or F-IPMs 2 , and can capture a wide variety of metrics on probability distributions by choosing F appropriately [39]. This paper studies the case where both F and P belong to the family of Besov spaces, a large family of nonparametric smoothness spaces that include, as examples, L p , Lipschitz/H\u00f6lder, and Hilbert-Sobolev spaces. The resulting IPMs include, as examples, L p , total variation, Kolmogorov-Smirnov, and Wasserstein distances. We have two main motivations for studying this problem:\n1. This problem unifies nonparametric density estimation with the central problem of empirical process theory, namely bounding quantities of the form d F (P, P ) when P is the empirical distribution\nP n = 1 n n i=1\n\u03b4 Xi of the data [43]. Whereas empirical process theory typically avoids restricting P and fixes the estimator P = P n , focusing on the discriminator class F, nonparametric density estimation typically fixes the loss to be an L p distance, and seeks a good estimator P for a given distribution class P. In contrast, we study how constraints on F and P jointly determine convergence rates of a number of estimates P of P . In particular, since Besov spaces comprise perhaps the largest commonly-studied family of nonparametric function spaces, this perspective allows us to unify, generalize, and extend several classical and recent results in distribution estimation (see Section 3). 2. This problem is a theoretical framework for analyzing generative adversarial networks (GANs). Specifically, given a GAN whose discriminator and generator networks encode functions in F and P, respectively, recent work [32,28,29,48] showed that a GAN can be seen as a distribution estimate 3\nP = argmin Q\u2208P sup f \u2208F E X\u223cQ [f (X)] \u2212 E X\u223c Pn [f (X)] = argmin Q\u2208P d F Q, P n ,(1)\ni.e., an estimate which directly minimizes empirical IPM risk with respect to a (regularized) empirical distribution P n . While, in the original GAN model [21], P n was the empirical distribution P n = 1 n n i=1 \u03b4 Xi of the data, Liang [28] showed that, under smoothness assumptions on the population distribution, performance is improved by replacing P n with a regularized version P n , equivalent to the instance noise trick that has become standard in GAN training [49,35]. We show, in particular, that, when P n is a wavelet-thresholding estimate, a GAN based on sufficiently large fully-connected neural networks with ReLU activations learns Besov probability distributions at the optimal rate. ", "publication_ref": ["b38", "b42", "b31", "b27", "b28", "b47", "b20", "b27", "b48", "b34"], "figure_ref": [], "table_ref": []}, {"heading": "Set up and Notation", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multiresolution Approximation and Besov Spaces", "text": "We now provide some notation that is necessary to define the family of Besov spaces studied in this paper. Since the statements and formal justifications behind these definitions are a bit complex, some technical details are relegated to the Appendix, and several well-known examples from the rich class of resulting spaces are given in Section 3. The diversity of Besov spaces arises from the fact that, unlike the H\u00f6lder or Sobolev spaces that they generalize, Besov spaces model functions simultaneously across multiple spatial scales. In particular, they rely on the following notion: Definition 1. A multiresolution approximation (MRA) of L 2 (R D ) is an increasing sequence {V j } j\u2208Z of closed linear subspaces of L 2 (R D ) with the following properties:\n1.\n\u221e j=\u2212\u221e V j = {0}, and the closure of\n\u221e j=\u2212\u221e V j = L 2 (R D ). 2. For f \u2208 L 2 (R D ), k \u2208 Z D , j \u2208 Z, f (x) \u2208 V 0 \u21d4 f (x \u2212 k) \u2208 V 0 & f (x) \u2208 V j \u21d4 f (2x) \u2208 V j+1 .\n3. For some \"father wavelet\" \u03c6 \u2208 V 0 , {\u03c6(x\u2212k) : k \u2208 Z D } is an orthonormal basis of V 0 \u2282 L 2 (R D ).\nFor intuition, consider the best-known MRA of L 2 (R), namely the Haar wavelet basis. Let \u03c6(x) = 1 {[0,1)} be the Haar father wavelet, let V 0 = Span{\u03c6(x \u2212 k) : k \u2208 Z} be the span of translations of \u03c6 by an integer, and let V j defined recursively for all j \u2208 Z by V j = {f (2x) : f (x) \u2208 V j\u22121 } be the set of horizontal scalings of functions in V j\u22121 by 1 /2. Then, {V j } j\u2208Z is an MRA of L 2 (R).\nThe importance of an MRA is that it generates an orthonormal basis of L 2 (R D ), via the following: Lemma 2 ([36], Section 3.9). Let {V j } j\u2208Z be an MRA of L 2 (R D ) with father wavelet \u03c6. Then, for E = {0, 1} D \\ (0, . . . , 0), there exist \"mother wavelets\"\n{\u03c8 } \u2208E such that {2 Dj/2 \u03c8 (2 j x \u2212 k) : \u2208 E, k \u2208 Z D } \u222a {2 Dj/2 \u03c6(2 j x \u2212 k) : k \u2208 Z D } is an orthonormal basis of V j \u2286 L 2 (R D ). Let \u039b j = {2 \u2212j k + 2 \u2212j\u22121 : k \u2208 Z D , \u2208 E} \u2286 R D .\nThen k, are uniquely determined for any \u03bb \u2208 \u039b j . Thus, for all \u03bb \u2208 \u039b := j\u2208Z \u039b j , we can let \u03c8 \u03bb (x) = 2 Dj/2 \u03c8 (2 j x \u2212 k). Equipped with the orthonormal basis {\u03c8 \u03bb : \u03bb \u2208 \u039b} of L 2 (R D ), we are almost ready to define Besov spaces.\nFor technical reasons (see, e.g., [36,Section 3.9]), we need MRAs of smoother functions than Haar wavelets, which are called r-regular. Due to space constraints, r-regularity is defined precisely in Appendix A; we note here that standard r-regular MRAs exist, such as the Daubechies wavelet [11]. We assume for the rest of the paper that the wavelets defined above are supported on [\u2212A, A]. Definition 3 (Besov Space). Let 0 \u2264 \u03c3 < r, and let p, q \u2208 [1, \u221e]. Given an r-regular MRA of L 2 (R D ) with father and mother wavelets \u03c6, \u03c8 respectively, the Besov space B \u03c3 p,q (R D ) is defined as the set of functions f : R D \u2192 R such that, the wavelet coefficients\n\u03b1 k := R D f (x)\u03c6(x \u2212 k)dx for k \u2208 Z D and \u03b2 \u03bb := R D f (x)\u03c8 \u03bb (x)dx for \u03bb \u2208 \u039b, satisfy f B \u03c3 p,q := {\u03b1 k } k\u2208Z D l p + 2 j(\u03c3+D(1/2\u22121/p)) {\u03b2 \u03bb } \u03bb\u2208\u039bj l p j\u2208N l q < \u221e\nThe quantity f B \u03c3 p,q is called the Besov norm of f , and, for any L > 0, we write B \u03c3 p,q (L) to denote the closed Besov ball B \u03c3 p,q (L) = {f \u2208 B \u03c3 p,q : f B \u03c3 p,q \u2264 L}. When the constant L is unimportant (e.g., for rates of convergence), B \u03c3 p,q denotes a ball B \u03c3 p,q (L) of finite but arbitrary radius L.", "publication_ref": ["b35", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Formal Problem Statement", "text": "Having defined Besov spaces, we now formally state the statistical problem we study in this paper. Fix an r-regular MRA. We observe n IID samples X 1 , ..., X n IID \u223c p from an unknown probability density p lying in a Besov ball B \u03c3g pg,qg (L g ) with \u03c3 g < r. We want to estimate p, measuring error with\nan IPM d B \u03c3 d p d ,q d (L d ) . Specifically, for general \u03c3 d , \u03c3 g , p d , p g , q d , q g , we seek to bound minimax risk M B \u03c3 d p d ,q d , B \u03c3g pg,qg := inf p sup p\u2208B \u03c3g pg ,qg E X1:n d B \u03c3 d p d ,q d (p, p(X 1 , . . . , X n ))(2)\nof estimating densities in F g = B \u03c3g pg,qg , where the infimum is taken over all estimators p(X 1 , . . . , X n ). In the rest of this paper, we suppress dependence of p(X 1 , ..., X n ) on X 1 , ..., X n , writing simply p.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The current paper unifies, extends, or improves upon a number of recent and classical results in the nonparametric density estimation literature. Two areas of prior work are most relevant:\nNonparametric estimation over inhomogeneous smoothness spaces First is the classical study of estimation over inhomogeneous smoothness spaces under L p losses. Nemirovski [41] first noticed that, over classes of regression functions with inhomogeneous (i.e., spatially-varying) smoothness, many widely-used regression estimators, called \"linear\" estimators (defined precisely in Section 4.2), are provably unable to converge at the minimax optimal rate, in L 2 loss. Donoho et al. [14] identified a similar phenomenon for estimating probability densities in a Besov space B \u03c3g pg,qg on R under L p d losses with p d > p g , corresponding to the case \u03c3 d = 0, D = 1 in our work. [14] also showed that the wavelet-thresholding estimator we consider in Section 4.1 does converge at the minimax optimal rate. We generalize these phenomena to many new loss functions; in many cases, linear estimators continue to be sub-optimal, whereas the wavelet-thresholding estimator continues to be optimal. We also show that sub-optimality of linear estimators is more pronounced in higher dimensions.", "publication_ref": ["b40", "b13", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Distribution estimation under IPMs", "text": "The second, more recent body of results [28,48,29] concerns nonparametric distribution estimation under IPM losses. Prior work focused on the case where F and P are both Sobolev ellipsoids, corresponding to the case p d = q d = p g = q g = 2 in our work. Notably, over these smaller spaces (of homogeneous smoothness), the linear estimators mentioned above are minimax rate-optimal. Perhaps the most important finding of these works is that the curse of dimensionality pervading classical nonparametric statistics is significantly diminished under weaker loss functions than L p losses (namely, many IPMs). For example, Singh et al. [48] showed that, when \u03c3 d > D/2, one can estimate P at the parametric rate n \u22121/2 in the loss\nd B \u03c3 d 2,2\n, without any regularity assumptions whatsoever on the probability distribution P . We generalize this to other losses\nd B \u03c3 d p d ,q d .\nThese papers were motivated in part by a desire to understand theoretical properties of GANs, and, in particular, Liang [28] and Singh et al. [48] helped establish (1) as a valid statistical model of GANs. In particular, we note that Singh et al. [48] showed that the implicit generative modeling problem (\"sampling\") in terms of which GANs are usually framed, is equivalent, in terms of minimax convergence rates, to nonparametric density estmation, justifying our focus on the latter problem in this paper. We show, in Section 4.3, that, given a sufficiently good optimization algorithm, GANs based on appropriately constructed deep neural networks can learn Besov densities at the minimax optimal rate. In this context, our results are among the first to suggest theoretically that GANs can outperform classical density estimators (namely, linear estimators mentioned above).\nLiu et al. [32] provided general sufficient conditions for weak consistency of GANs in a generalization of the model (1). Since many IPMs, such as Wasserstein distances, metrize weak convergence of probability measures under mild additional assumptions Villani [54], this implies consistency under these IPMs. However, Liu et al. [32] did not study rates of convergence.\nWe end this section with a brief survey of known results for estimating distributions under specific Besov IPM losses, noting that our results (Equations ( 3) and (4) below) generalize all these rates:\n1. L p Distances: If F d = L p = B 0\np ,p , then, for distributions P, Q with densities p, q \u2208 L p , d F d (P, Q) = p\u2212q L p . These are the most well-studied losses in nonparametric statistics, especially for p \u2208 {1, 2, \u221e} [42,55,53]. [14] studied the minimax rate of convergence of density estimation over Besov spaces under L p losses, obtaining minimax rates n\n\u2212 \u03c3g 2\u03c3g +D + n \u2212 \u03c3g +D(1\u22121/pg \u22121/p d ) 2\u03c3g +D(1\u22122/pg )\nover general estimators, and n \u2212 \u03c3g\n2\u03c3g +D + n \u2212 \u03c3g \u2212D/pg +D/p d 2\u03c3g +D\u22122D/pg +2D/p\nd when restricted to linear estimators. 2. Wasserstein Distance:\nIf F d = C 1 (1) B 1\n\u221e,\u221e is the space of 1-Lipschitz functions, then d F d is the 1-Wasserstein or Earth mover's distance (via the Kantorovich dual formulation [24,54]). A long line of work has established convergence rates of the empirical distribution to the true distribution in spaces as general as unbounded metric spaces [56,26,47]). In the Euclidean setting, this is well understood [15,2,19], although, to the best of our knowledge, minimax lower bounds have been proven only recently [47]; this setting intersects with our work in the case \u03c3 d = 1, \u03c3 g = 0, p d = \u221e, matching our minimax rate of n \u22121/D + n \u22121/2 . More general p-Wasserstein distances W p (p \u2265 1) cannot be expressed exactly as IPMs, but, our results complement recent results of Weed and Berthet [57], who showed that, for densities p and q that are bounded above and below (i.e.,\n0 < m \u2264 p, q \u2264 M < \u221e), the bounds M \u22121/p d B 1 p ,\u221e (p, q) \u2264 W p (p, q) \u2264 m \u22121/p d B 1 p ,1 (p, q)\nhold; for such densities, our rates match theirs (n \u2212 1+\u03c3g 2\u03c3g +D + n \u22121/2 ) up to polylogarithmic factors. Weed and Berthet [57] showed that, without the lower-boundedness assumption (m > 0), minimax rates under W p are strictly slower (by a polynomial factor in n). In machine learning applications, Arora et al. [5] recently used this rate to argue that, for data from a continuous distribution, Wasserstein GANs [4] cannot generalize at a rate faster than n \u22121/D (at least without additional regularization, as we use in Theorem 9). A variant in which F d \u2282 C 1 \u2229 L \u221e is both uniformly bounded and 1-Lipschitz gives rise to the Dudley metric [16], which has also been suggested for use in GANs [1]. Finally, we note that the more general distances induced by \nF d = B \u03c3 d \u221e,\nF d = BV B 1 1,\n\u2022 is the set of functions of bounded variation, then, in the 1-dimensional case, d F d is the well-known Kolmogorov-Smirnov metric [10], and so the famous Dvoretzky-Kiefer-Wolfowitz inequality [34] gives a parametric convergence rate of n \u22121/2 . 4. Sobolev Distances: 2 is the corresponding negative Sobolev pseudometric [59]. Recent work [28,48,29] established a minimax rate of n\nIf F d = W \u03c3 d ,2 = B \u03c3 2,2 is a Hilbert-Sobolev space, for \u03c3 \u2208 R, then d F d = \u2022 \u2212 \u2022 W \u2212\u03c3 d ,\n\u2212 \u03c3g +\u03c3 d 2\u03c3g +1 + n \u22121/2 when F g = W \u03c3g,2\nis also a Hilbert-Sobolev space.", "publication_ref": ["b27", "b47", "b28", "b47", "b27", "b47", "b47", "b31", "b0", "b53", "b31", "b41", "b54", "b52", "b13", "b23", "b53", "b55", "b25", "b46", "b14", "b1", "b18", "b46", "b56", "b56", "b4", "b3", "b15", "b0", "b9", "b33", "b1", "b58", "b27", "b47", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Main Results", "text": "The three main technical contributions of this paper are as follows:\n1. We prove lower and upper bounds (Theorems 4 and 5, respectively) on minimax convergence rates of distribution estimation under IPM losses when the distribution class P = B \u03c3g pg,qg and the discriminator class F = B \u03c3 d p d ,q d are Besov spaces; these rates match up to polylogarithmic factors in the sample size n. Our upper bounds use the wavelet-thresholding estimator proposed in Donoho et al. [14], which we show converges at the optimal rate for a much wider range of losses than previously known. Specifically, if M (F, P) denotes minimax risk (2), we show that for\np d \u2265 p g , \u03c3 g \u2265 D/p g , M B \u03c3 d p d ,q d , B \u03c3g pg,qg max n \u22121/2 , n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D , n \u2212 \u03c3g +\u03c3 d +D(1\u22121/pg \u22121/p d ) 2\u03c3g +D(1\u22122/pg ) .(3)\n2. We show (Theorem 7) that, for p d \u2265 p g and \u03c3 g \u2265 D/p g , no estimator in a large class of distribution estimators, called \"linear estimators\", can converge at a rate faster than\nM lin B \u03c3 d p d ,q d , B \u03c3g pg,qg n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D/p d 2\u03c3g +D(1\u22122/pg )+2D/p d .(4)\n\"Linear estimators\" include the empirical distribution, kernel density estimates with uniform bandwidth, and the orthogonal series estimators recently used in Liang [28] and Singh et al. [48]). The lower bound (4) implies that, in many settings (discussed in Section 5), linear estimators converge at sub-optimal rates. This effect is especially pronounced when the data dimension D is large and the distribution P has relatively sparse support (e.g., if P is supported near a low-dimensional manifold).\n3. We show that the minimax convergence rate can be achieved by a GAN with generator and discriminator networks of bounded size, after some regularization. As one of the first theoretical results separating performance of GANs from that of classic nonparametric tools such as kernel methods, this may help explain GANs' successes with high-dimensional data such as images.", "publication_ref": ["b13", "b27", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Minimax Rates over Besov Spaces", "text": "We now present our main lower and upper bounds for estimating densities that live in a Besov space under a Besov IPM loss. Then, we have the following lower bound on the convergence rate:\nTheorem 4. (Lower Bound) Let r > \u03c3 g \u2265 D/p g , then, M B \u03c3 d p d ,q d , B \u03c3g pg,qg max \uf8eb \uf8ed n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D , log n n \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg \uf8f6 \uf8f8 (5)\nBefore giving a corresponding upper bound, we describe the estimator on which it depends.\nWavelet-Thresholding: Our upper bound uses the wavelet-thresholding estimator proposed by [14]:\np n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb + j1 j=j0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb .(6)\np n estimates p via its truncated wavelet expansion, with\n\u03b1 k = 1 n n i=1 \u03c6 k (X i ), \u03b2 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ), and \u03b2 \u03bb = \u03b2 \u03bb 1 { \u03b2 \u03bb > \u221a j/\nn} are empirical estimates of respective coefficient of the wavelet expansion of p. As [14] first showed, attaining optimality over Besov spaces requires truncating high-resolution terms (of order j \u2208 [j 0 , j 1 ]) when their empirical estimates are too small; this \"nonlinear\" part of the estimator distinguishes it from the \"linear\" estimators we study in the next section. The hyperparameters j 0 and j 1 are set to j\n0 = 1 2\u03c3g+D log 2 n, j 1 = 1 2\u03c3g+D\u22122D/pg log 2 n. Theorem 5. (Upper Bound) Let r > \u03c3 g \u2265 D/p g and p d > p g . Then, for a constant C depending only on p d , \u03c3 g , p g , q g , D, L g , L d and \u03c8 p d , M B \u03c3 d p d ,q d , B \u03c3g pg,qg \u2264 C log n n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D + n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D/p d 2\u03c3g +D\u22122D/pg + n \u22121/2 (7)\nWe will comment only briefly on Theorems 4 and 5 here, leaving extended discussion for Section 5. First, note that the lower bound (5) and upper bound (7) are essentially tight; they differ only by a polylogarithmic factor in n. Second, both bounds contain two main terms of interest. The simpler term, n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D , matches the rate observed in the Sobolev case by Singh et al. [48]. The other term is unique to more general Besov spaces. Depending on the values of D, \u03c3 d , \u03c3 g , p d , and p g , one of these two terms dominates, leading to two main regimes of convergence rates, which we call the \"Sparse\" regime and the \"Dense\" regime. Section 5 discusses these and other interesting phenomena in detail.", "publication_ref": ["b13", "b13", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "Minimax Rates of Linear Estimators over Besov Spaces", "text": "We now show that, for many Besov densities and IPM losses, many widely-used nonparametric density estimators cannot converge at the optimal rate (5). These estimators are as follows: Definition 6 (Linear Estimator). Let (\u2126, F, P ) be a probability space. An estimate P of P is said to be linear if there exist functions T i (X i , \u2022) : F \u2192 R such that for all measurable A \u2208 F,\nP (A) = n i=1 T i (X i , A).(8)\nClassic examples of linear estimators include the empirical distribution (T i (X i , A) = 1 n 1 {Xi\u2208A} , the kernel density estimate (T i (X i , A) = 1 n A K(X i , \u2022) for some bandwidth h > 0 and smoothing kernel K : X \u00d7 X \u2192 R) and the orthogonal series estimate (T i (X i , A) = 1 n J j=1 g j (X i ) A g j for some cutoff J and orthonormal basis {g j } \u221e j=1 (e.g., Fourier, wavelet, or polynomial) of L 2 (\u2126)). Theorem 7 (Minimax rate for Linear Estimators).\nSuppose r > \u03c3 g \u2265 D/p g , M lin B \u03c3 d p d ,q d , B \u03c3g pg,qg := inf Plin sup p\u2208Fg E X1:n d F d \u00b5 p , P n \u2212 1 2 + n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D/p d 2\u03c3g +D\u22122D/pg +2D/p d + n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D\nwhere the inf is over all linear estimates of p \u2208 F g , and \u00b5 p is the distribution with density p.\nOne can check that the above error decays no faster than n\n\u2212 \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg\n. Comparing with the rate in Theorem 5, this implies that, in certain cases, convergence the rate for linear estimators is strictly slower than that for general estimators; i.e., linear estimators fail to achieve the minimax optimal rate over certain Besov space. We defer detailed discussion of this phenomenon to Section 5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Upper Bounds on a Generative Adversarial Network", "text": "Pioneered by Goodfellow et al. [21] as a mechanism for applying deep neural networks to the problem of unsupervised image generation, Generative adversarial networks (GANs) have since been widely applied not only to computer vision [61,25], but also to such diverse problems and data as machine translation using natural language data [58], discovering drugs [23] and designing materials [46] using molecular structure data, inferring expression levels using gene expression data [12], and sharing patient data under privacy constraints using electronic health records [9]. Besides the Jensen-Shannon divergence used by [21], many GAN formulations have been proposed based on minimizing other losses, including the Wasserstein metric [4,22], total variation distance [31], \u03c7 2 divergence [33], MMD [27], Dudley metric [1], and Sobolev metric [38]. The diversity of data types and losses with which GANs have been used motivates studying GANs in a very general (nonparametric) setting. In particular, Besov spaces likely comprise the largest widely-studied family of nonparametric smoothness class; indeed, most of the losses listed above are Besov IPMs.\nGANs are typically described as a two-player minimax game between a generator network N g and a discriminator network N d ; we denote by F d the class of functions that can be implemented by N d and by F g the class of distributions that can be implemented by N g . A recent line of work has argued that a natural statistical model for a GAN as a distribution estimator is\nP := argmin Q\u2208Fg sup f \u2208F d E X\u223cQ [f (X)] \u2212 E X\u223c Pn [f (X)] ,(9)\nwhere P n is an (appropriately regularized) empirical distribution, and that, when F d and F g respectively approximate classes F and P well, one can bound the risk, under F-IPM loss, of estimating distributions in P by ( 9) [32,28,48,29]. We emphasize, that, as Singh et al. [48] showed, the minimax risk in this framework is identical to that under the \"sampling\" (or \"implicit generative modeling\" [37]) framework in terms of which GANs are usually cast. 4 In this section, we show such a result for Besov spaces; namely, we show the existence of a particular GAN (specifically, a sequence of GANs, necessarily growing with the sample size n), that estimates distributions in a Besov space at the minimax optimal rate (7) under Besov IPM losses. This construction uses a standard neural network architecture (a fully-connected neural network with rectified linear unit (ReLU) activations), and a simple data regularizer P n , namely the waveletthresholding estimator described in Section 4.1. Our results extend those of Liang [28] and Singh et al. [48], for Wasserstein loss over Sobolev spaces, to general Besov IPM losses over Besov spaces.\nWe begin with a formal definition of the network architectures that we consider: Definition 8. A fully-connected ReLU network f (A1,...,A H ),(b1,...,b H ) : R W \u2192 R has the form\nA H \u03b7 (A H\u22121 \u03b7 (\u2022 \u2022 \u2022 \u03b7(A 1 x + b 1 ) \u2022 \u2022 \u2022 ) + b H\u22121 ) + b H ,\nwhere, for each \u2208 [H \u2212 1], A \u2208 R W \u00d7W , and A H \u2208 R 1\u00d7W and the ReLU operation \u03b7(x) = max{x, 0} is applied element-wise to vectors in R W .\nThe size of f (A1,...,A H ),(b1,...,b H ) (x) can be measured in terms of the following four (hyper)parameters: the depth H, the width W , the sparsity S := \u2208[H] A 0,0 + b 0 (i.e., the total number of non-zero weights), and the maximum weight\nB := max{ A \u221e,\u221e , b \u221e : \u2208 [H]}.\nFor given size parameters H, W, S, B we write \u03a6(H, W, S, B) to denote the set of functions satisfying the corresponding size constraints.\nOur results rely on a recent construction (Lemma 17 in the Appendix), by [51], of a fully-connected ReLU network that approximates Besov functions. [51] used this approximation to bound the risk of a neural network for nonparametric regression over Besov spaces, under L r loss. Here, we use this approximation result Lemma 17 to bound the risk of a GAN for nonparametric distribution estimation over Besov spaces, under the much larger class of Besov IPM losses. Our precise result is as follows: Theorem 9 (Convergence Rate of a Well-Optimized GAN). Fix a Besov density class B \u03c3g pg,qg with \u03c3 g > D/p g and discriminator class\nB \u03c3 d p d ,q d with \u03c3 d > D/p d .\nThen, for any desired approximation error > 0, one can construct a GAN p of the form (9) (with p n ) with discriminator network\nN d \u2208 \u03a6(H d , W d , S d , B d ) and generator network N g \u2208 \u03a6(H g , W g , S g , B g ), s.t. for all p \u2208 B \u03c3g pg,qg E d B \u03c3 d p d ,q d ( p, p) + E d B \u03c3 d p d ,q d ( p n , p)\nwhere This theorem implies that the rate of convergence of the GAN estimate p of the form 9 is the same as the convergence rate of the estimator p n with which the GAN estimate is generated (Here we assume that all distributions have densities). Therefore, given our upper bound from theorem 5 we have the following direct consequence. \nH\nd F d ( p, p) \u2264 n \u2212\u03b7(D,\u03c3 d ,p d ,\u03c3g,pg) log n where \u03b7(D, \u03c3 d , p d , \u03c3 g , p g ) = min 1 2 , \u03c3g+\u03c3 d 2\u03c3g+D , \u03c3g+\u03c3 d +D\u2212D/pg\u2212D/p d 2\u03c3g+D(1\u22122/pg)\nis the exponent from (7).\nIn other words there is a GAN estimate that is minimax rate optimal for a smooth class of densities over an IPM generated by a smooth class of discriminator functions.", "publication_ref": ["b20", "b60", "b24", "b57", "b22", "b45", "b11", "b8", "b20", "b3", "b21", "b30", "b32", "b26", "b0", "b37", "b31", "b27", "b47", "b28", "b47", "b36", "b3", "b27", "b47", "b50", "b50", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion of Results", "text": "In this section, we discuss some general phenomena that can be gleaned from our technical results.\nFirst, we note that, perhaps surprisingly, q d and q g do not appear in our bounds. Tao [52] suggests that q d and q g may have only logarithmic effects (contrasted with the polynomial effects of \u03c3 d , p d , \u03c3 g , and p g ). Thus, a more fine-grained analysis to close the polylogarithmic gap between our lower and upper bounds for general estimators (Theorems 4 and 5) might require incorporating q d and q g .\nOn the other hand, the parameters \u03c3 d , p d , \u03c3 g , and p g each play a significant role in determining minimax convergence rates, in both the linear and general cases. We first discuss each of these parameters independently, and then discuss some interactions between them. Roles of the smoothness orders \u03c3 d and \u03c3 g As a visual aid for understanding our results, Figure 1 show phase diagrams of minimax convergence rates, as functions of discriminator smoothness \u03c3 d and distribution smoothness \u03c3 g , in the illustrative case\nD = 4, p d = 1.2, p g = 2. When 1/p g + 1/p d > 1, a minimum total smoothness \u03c3 d + \u03c3 g \u2265 D(1/p d + 1/p g \u2212 1)\nis needed for consistent estimation to be possible -this fails in the \"Infeasible\" region of the phase diagrams. Intuitively, this occurs because F d is not contained in the topological dual F g of F g . For linear estimators, even greater smoothness\n\u03c3 d +\u03c3 g \u2265 D(1/p d +1/p g ) is needed.\nAt the other extreme, for highly smooth discriminator functions, both linear and nonlinear estimators converge at the parametric rate O n \u22121/2 , corresponding to the \"Parametric\" region. In between, rates for linear estimators vary smoothly with \u03c3 d and \u03c3 g , while rates for nonlinear estimators exhibit another phase transition on the line \u03c3 g + 3\u03c3 d = D; to the left lies the \"Sparse\" case, in which estimation error is dominated by a small number of large errors at locations where the distribution exhibits high local variation; to the right lies the \"Dense\" case, where error is relatively uniform on the sample space.\nThe left boundary \u03c3 d = 0 corresponds to the classical results of Donoho et al. [14], who consequently identified the \"Infeasible\", \"Sparse\", and \"Dense\" phases, but not the \"Parametric\" phase. When restricting to linear estimators, the \"Infeasible\" region grows and the \"Parametric\" region shrinks.\nRole of the powers p d and p g At one extreme (p d = \u221e) lie L 1 or total variation loss (\u03c3 d = 0), Wasserstein loss (\u03c3 d = 1), and its higher-order generalizations, for which we showed the rate\nM B \u03c3 d \u221e,q d , B \u03c3g pg,qg n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D + n \u22121/2 ,\ngeneralizing the rate first shown by Singh et al. [48] for Hilbert-Sobolev classes to other distribution classes, such as F g = BV. Because discriminator functions in this class exhibit homogeneous smoothness, these losses effectively weight the sample space relatively uniformly in importance, the \"Sparse\" region in Figure (1a) vanishes, and linear estimators can perform optimally.\nAt the other extreme (p d = 1) lie L \u221e loss (\u03c3 d = 0), Kolmogorov-Smirnov loss (\u03c3 d = 1), and its higher-order generalizations, for which we have shown that the rate is always\nM B \u03c3 d 1,q d , B \u03c3g pg,qg n \u2212 \u03c3g +\u03c3 d +D(1\u22121/p d \u22121/pg ) 2\u03c3g +D(1\u22122/pg ) + n \u22121/2 ;\nexcept in the parametric regime (D \u2264 2\u03c3 d ), this rate differs from that of Singh et al. [48]. Because discriminator functions can have inhomogeneous smoothness, and hence weight some portions of the sample space much more heavily than others, the \"Dense\" region in Figure 1a vanishes, and linear estimators are always sub-optimal. We note that Sadhanala et al. [45] recently proposed using these higher-order distances (integer \u03c3 d > 1) in a fast two-sample test that generalizes the well-known Kolmogorov-Smirnov test, improving sensitivity to the tails of distributions; our results may provide a step towards understanding theoretical properties of this test.\nComparison of linear and general rates Letting \u03c3 g := \u03c3 g \u2212 D(1/p g + 1/p d ), one can write the sparse term of the linear minimax rate in the same form as the Dense rate, replacing \u03c3 g with \u03c3 g :\nM lin B \u03c3 d p d ,q d , B \u03c3g pg,qg n \u2212 \u03c3 g +\u03c3 d 2\u03c3 g +D .(10)\nThis is not a coincidence; Morrey's inequality [18, Section 5.6.2] in functional analysis tells us that for general \u03c3 g > D(1\n/p g + 1/p d ), \u03c3 g := \u03c3 g \u2212 D(1/p g + 1/p d ) is largest possible value such that the embedding B \u03c3g pg,pg \u2286 B \u03c3 g p d ,p d holds.\nIn the extreme case p d = \u221e (corresponding to generalizations of total variation loss), one can interpret the rate (10) as saying that linear estimators benefit only from homogeneous (e.g., H\u00f6lder) smoothness, and not from weaker inhomogeneous (e.g., Besov) smoothness. For general p d , linear estimator can still benefit from inhomogeneous smoothness, but to a lesser extent than general minimax optimal estimators.", "publication_ref": ["b51", "b13", "b47", "b47", "b44"], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusions", "text": "We have shown, up to log factors, unified minimax convergence rates for a large class of pairs of F d -IPM losses and distribution classes F g . By doing so, we have generalized several phenomena that had observed in special cases previously. First, under sufficiently weak loss functions, distribution estimation is possible at the parametric rate O(n \u22121/2 ) even over very large nonparametric distribution classes. Second, in many cases, optimal estimation requires estimators that adapt to inhomogeneous smoothness conditions; many commonly used distribution estimators fail to do this, and hence converge at sub-optimal rates, or even fail to converge. Finally, GANs with sufficiently large fully-connected ReLU neural networks using wavelet-thresholding regularization perform statistically minimax rate-optimal distribution estimation over inhomogeneous nonparametric smoothness classes (assuming the GAN optimization problem can be solved accurately). Importantly, since GANs optimize IPM losses much weaker than traditional L p losses, they may be able to learn reasonable approximations of even high-dimensional distributions with tractable sample complexity, perhaps explaining why they excel in the case of image data. Thus, our results suggest that the curse of dimensionality may be less severe than indicated by classical nonparametric lower bounds.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Technical Definitions and Notation", "text": "As noted in the main text, we need a multiresolution approximation (MRA) satisfying an r-regularity condition, defined as follows: Definition 11. Given a non-negative integer r, an MRA is called r-regular if the function \u03c6 can be chosen in such a way that, for every m \u2208 N and multi-index \u03b1 = (\u03b1 1 , . . . , \u03b1\nD ) \u2208 N D satisfying |\u03b1| \u2264 r, for some constant C \u03b1,m , |\u2202 \u03b1 \u03c6(x)| \u2264 C \u03b1,m (1 + |x|) \u2212m . Here, \u2202 \u03b1 = (\u2202/\u2202x 1 ) \u03b11 \u2022 \u2022 \u2022 (\u2202/\u2202x D ) \u03b1 D is the mixed derivative of index \u03b1, |\u03b1| = D j=1\n\u03b1 j and |x| is any of the equivalent norms on a finite dimensional Euclidean space. That is, all derivatives of \u03c6 of order up to r are bounded and decay at a rate faster than any polynomial.\nWhile constructing an r-regular MRA is nontrivial, it suffices for our purpose to note that r-regular MRAs exist; the most famous example is the Daubechies wavelet [11,36].\nWe also note the following result showing that for any function in V j (i.e., at a certain \"level\" in the MRA) its L p norm is equivalent to the l p sequence norm of its coefficients in the wavelet basis; this helps motivate the sequence-based definition of the Besov norm.\nProposition 12 (Meyer [36], Section 6.10, Proposition 7). There exist positive constants C, C s.t. for every 1 \u2264 p \u2264 \u221e, j \u2208 Z and\n{\u03b1 k } \u2208 l p , f (x) = a k 2 Dj/2 \u03c8 (2 j x \u2212 k), \u2208 E, k \u2208 Z D , C f p \u2264 2 Dj(1/2\u22121/p) |a k | p 1/p \u2264 C f p .\nAppendix A.1 of Donoho et al. [14] offers a more extended background of Besov spaces, including how the sequence-based definition corresponds to more conventional smoothness measures (moduli of continuity), as well as some direct connections between Besov spaces and minimax theory for linear estimators.", "publication_ref": ["b10", "b35", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "B Upper Bound -Linear Case", "text": "For any density function p let\n\u03b1 p k = \u03c6 k (x)p(x)dx \u03b2 p \u03bb = \u03c8 \u03bb (x)p(x)dx\nWe first show that Besov IPMs essentially measure the distance in co-efficient space between compactly supported densities. Lemma 13. For any compactly supported probability densities p, q \u2208 L p d where\nF d = B \u03c3 d p d ,q d d F d (p, q) = sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 q k ) + j\u22650 \u03bb\u2208\u039b \u03b2 f \u03bb (\u03b2 p \u03bb \u2212 \u03b2 q \u03bb )\nwhere for f\n\u2208 F d f = k\u2208Z \u03b1 f k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03c8 \u03bb\nProof. We notice that the convergence to f above is in the L \u221e norm. So for probability measures P, Q we have,\nd F d (p, q) = sup f \u2208F d |E X\u223cp [f (X)] \u2212 E X\u223cq [f (X)]| = sup f \u2208F d X f (x)p(x)dx \u2212 f (x)q(x)dx = sup f \u2208F d X \uf8eb \uf8ed k\u2208Z \u03b1 f k \u03c6 k (x) + j\u22650 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03c8 \u03bb (x) \uf8f6 \uf8f8 (p(x) \u2212 q(x)) dx\nIf p, q are compactly supported on [\u2212B, B] then we can assume WLOG that f is compactly supported on [\u2212B, B] so convergence of f n to f in L \u221e norm implies convergence in L 1 norm. Therefore,\nd F d (P, Q) = sup f \u2208F d k\u2208Z X \u03b1 f k \u03c6 k (dP (x) \u2212 dQ(x)) + j\u22650 \u03bb\u2208\u039bj X \u03b2 f \u03bb \u03c8 \u03bb (dP (x) \u2212 dQ(x)) = sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 q k ) + j\u22650 \u03bb\u2208\u039b \u03b2 f \u03bb (\u03b2 p \u03bb \u2212 \u03b2 q \u03bb )\nWe will need the following inequalities to estimate the error of the wavelet estimator under the IPM loss.\nThe first lemma is the standard upper bound on the mth moment of a sum of IID random variables with bounded variance. The second is a standard concentration inequality used to bound large deviations in our error estimate. Lemma 14. (Rosenthal's Inequality ( [44])) Let m \u2208 R and Y 1 , . . . , Y n be IID random variables with\nE[Yi] = 0, E[Y 2 i ] \u2264 \u03c3 2 .\nThen there is a constant c m that depends only on m s.t.\nE 1 n n i=1 Y i m \u2264 c m \u03c3 m n m/2 + E |Y 1 | m n m\u22121 for 2 < m < \u221e, E 1 n n i=1 Y i m \u2264 \u03c3 m n \u2212m/2 for 1 \u2264 m \u2264 2. Lemma 15. (Bernstein's Inequality ([6])) If Y 1 , . . . , Y n are IID random variables such that E[Yi] = 0, E[Y 2 i ] = \u03c3 2 and |Y i | \u2264 Y \u221e < \u221e, then Pr 1 n n i=1 Y i > \u03bb \u2264 2 exp \u2212 n\u03bb 2 2(\u03c3 2 + Y \u221e \u03bb/3)\nwhere Y \u221e = ess sup Y .", "publication_ref": ["b43"], "figure_ref": [], "table_ref": []}, {"heading": "Given discriminator and generator classes as", "text": "F d = {f : f \u03c3 d p d ,q d \u2264 L d } F g = {p : p \u03c3g pg,qg \u2264 L g } \u2229 P P = {p : p \u2265 0, p L 1 = 1, supp(p) \u2286 [\u2212T, T ]}, we decompose f \u2208 F d as f = k\u2208Z \u03b1 k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb .\nWe use the linear wavelet estimator to demonstrate the upper bound. Let X 1 , . . . , X n be IID with density p \u2208 F g and consider the wavelet estimator of p i.e.\np = k\u2208Z \u03b1 p k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 p \u03bb \u03c8 \u03bb p n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb\nwhere\n\u03b1 p k = E X\u223cp [\u03c6 k (X)] \u03b2 p \u03bb = E X\u223cp [\u03c8 \u03bb (X)] \u03b1 k = 1 n n i=1 \u03c6 k (X i ) \u03b2 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i )\nThen applying lemma 13, we bound\nd F d (p, p n ) \u2264 sup f \u2208F d k\u2208Z \u03b1 k (\u03b1 p k \u2212 \u03b1 k ) + sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb + sup f \u2208F d j\u2265j1 \u03bb\u2208\u039bj \u03b2 \u03bb \u03b2 p \u03bb\nwhere the first two terms constitute the stochastic error and the last term is the bias. We bound these separately below. We first prove a few lemmas that will be used repeatedly to upper bound the different terms.\nLemma 16. Let n 1 , n 2 \u2208 N \u222a {\u221e} and \u03b7 be any sequence of numbers. Then\nE X1,...,Xn sup f \u2208F d n2 j=n1 \u03bb\u2208\u039bj \u03b3 \u03bb \u03b7 \u03bb \u2264 L D n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed E X1,...,Xn \u03bb\u2208\u039bj |\u03b7 \u03bb | p d \uf8f6 \uf8f8 1/p d\nNote that if the above is true also if \u03b3 = \u03b1 f and n 1 = n 2 = 0.\nProof. Since f \u2208 F d , applying H\u00f6lder's inequality twice we get,\nE X1,...,Xn sup f \u2208F d n2 j=n1 \u03bb\u2208\u039bj \u03b3 \u03bb \u03b7 \u03bb \u2264 E X1,...,Xn sup f \u2208F d n2 j=n1 \u03b3 p d \u03b7 p d \u2264 E X1,...,Xn sup f \u2208F d \uf8eb \uf8ed n2 j=n1 2 j(\u03c3 d +D/2\u2212D/p d ) \u03b3 p d q d \uf8f6 \uf8f8 1/q d \u00d7 n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b7 p d (l 1 \u2286 l q d ) \u2264 L D n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) E X1,...,Xn \u03b7 p d \u2264 L D n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed E X1,...,Xn \u03bb\u2208\u039bj |\u03b7 \u03bb | p d \uf8f6 \uf8f8 1/p d where p d is the conjugate of p d i.e. 1 p d + 1 p d = 1\nand we applied Jensen's to get the last inequality.\nLemma 17. Let f \u2208 B \u03c3g pg,qg where \u03c3 g > D/p g then f \u221e \u2264 4A \u03c8 \u221e L g (1 \u2212 2 (\u03c3g\u2212D/pg)q g ) \u22121/q g\nThis implies that sufficiently smooth Besov spaces B \u03c3g pg,qq are uniformly bounded.\nProof. We have that k\u2208Z D \u03b1 k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb converges to f in L \u221e . So, using the fact that l p d \u2286 l \u221e and proposition 12,\nf \u221e \u2264 2A \u03c8 \u221e \uf8eb \uf8ed {\u03b1 k } k\u2208Z D \u221e + j\u22650 2 Dj/2 {\u03b2 \u03bb } \u03bb\u2208\u039bj \u221e ) \uf8f6 \uf8f8 .\nWe can upper bound, by H\u00f6lder's inequality,\nj\u22650 2 Dj/2 {\u03b2 \u03bb } \u03bb\u2208\u039bj \u221e \u2264 j\u22650 1 2 j(\u03c3g\u2212D/pg) \u00d7 2 j(\u03c3g+D/2\u2212D/pg) {\u03b2 \u03bb } \u03bb\u2208\u039bj \u221e \u2264 \uf8eb \uf8ed j\u22650 1 2 j(\u03c3g\u2212D/pg)q g \uf8f6 \uf8f8 1/q g \uf8eb \uf8ed j\u22650 2 jqg(\u03c3g+D/2\u2212D/pg) {\u03b2 \u03bb } \u03bb\u2208\u039bj qg \u221e \uf8f6 \uf8f8 1/qg \u2264 1 1 \u2212 2 \u2212(\u03c3g\u2212D/pg)q g 1/q g \uf8eb \uf8ed j\u22650 2 jqg(\u03c3g+D/2\u2212D/pg) {\u03b2 \u03bb } \u03bb\u2208\u039bj qg pg \uf8f6 \uf8f8 1/qg \u2264 1 \u2212 2 \u2212(\u03c3g\u2212D/pg)q g \u22121/q g f \u03c3g pgqg \u2264 1 \u2212 2 \u2212(\u03c3g\u2212D/pg)q g \u22121/q g L g .\nPutting the above together we obtain the required upper bound.\nWe also need a few preliminary results namely, the moments of error of linear estimates of the wavelet coefficients are essentially bounded by 1/ \u221a n and the probability that this error is large is negligibly small. In particular,\nLemma 18. (Moment Bounds) Let X 1 , . . . , X n \u223c p, m \u2265 1 s.t. there is a constant c with Ep |\u03c8 \u03bb (X)| m \u2264 c2 Dj(m/2\u22121) . Let \u03b3 p \u03bb = E[\u03c8\u03bb(X)], \u03b3 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ),\nThen for all j s.t.\n2 Dj \u2208 O(n), E[| \u03b3 jk \u2212 \u03b3 jk | m ] \u2264 cn \u2212m/2 .\nwhere c = c m Ep |\u03c8 \u03bb (X)| 2 m/2 is a constant.\nProof. Since \u03c8 \u03bb is bounded for every \u03bb, let\nY i = \u03c8 \u03bb (X i ) \u2212 E[\u03c8\u03bb(X)]\nthen for all m \u2265 1, applying Jensen's inequality repeatedly we get\nE[|Yi| m ] \u2264 E[(|\u03c8\u03bb(Xi)| + | E[\u03c8\u03bb(Xi)]|) m ] (triangle inequality) \u2264 2 m\u22121 (E[|\u03c8 \u03bb (X i )| m ] + | E[\u03c8\u03bb(Xi)]| m ) (Jensen's) \u2264 2 m E[|\u03c8\u03bb(Xi)| m ]. (Jensen's)\nTherefore, by Rosenthal's inequality we have,\nE[|\u03b3 p \u03bb \u2212 \u03b3 \u03bb | m ] \u2264 c m E p |\u03c8 \u03bb (X)| 2 m/2 + c 2 Dj n (m/2\u22121)+ n \u2212m/2\nwhere c m is a constant that only depends on m. Therefore,\nE[|\u03b3 p \u03bb \u2212 \u03b3 \u03bb | m ] \u2264 c m E p |\u03c8 \u03bb (X)| 2 m/2 n \u2212m/2\nNote that we have from above 2 Dj1 \u2264 n so this bound holds for any j \u2264 j 1 . Lemma 19. (Large Deviations) Let X 1 , . . . , X n \u223c p such that for a constant c, Ep |\u03c8 \u03bb (X)| 2 \u2264 c. Let\n\u03b3 p \u03bb = E[\u03c8\u03bb(X)], \u03b3 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ),\nLet l = j/n and \u03b3 > 0, then, for all j s.t. 2 Dj \u2208 o(n), we have,\nPr(| \u03b3 \u03bb \u2212 \u03b3 \u03bb | > (K/2)l) \u2264 2 \u00d7 2 \u2212\u03b3nl 2\nwhere K large enough such that\nK 2 8(c + \u03c8 \u221e (K/3)) > log 2\u03b3\nProof. Applying Bernstein's inequality we have\nPr(| \u03b3 \u03bb \u2212 \u03b3 \u03bb | > (K/2)l) \u2264 2 exp \u2212 n(K/2) 2 l 2 2(c + 2 Dj/2 \u03c8 \u221e (K/3)l) \u2264 2 exp \u2212 K 2 nl 2 8(L g + \u03c8 \u221e (K/3))\nThis implies for K satisfying the above condition,\nPr(| \u03b3 \u03bb \u2212 \u03b3 \u03bb | > (K/2)l) \u2264 2 \u00d7 2 (\u2212\u03b3nl 2 )\nNow for every j \u2264 j 1 , l satisfies the requirements of the above lemma. So if nl 2 (= j) \u2192 \u221e as n \u2192 \u221e the probability of large deviation goes to zero. Lemma 20. (Variance) Let X 1 , . . . , X n \u223c p where p is compactly supported, such that for a constant c, Ep |\u03c8 \u03bb (X)| m \u2264 c2 Dj(m/2\u22121) . Let F d = B \u03c3 d p d ,q d , then the variance of a linear wavelet estimator p with j 0 terms i.e. \np n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb is bounded by d F d ( p n , E[ p n ]) \u2264 c 1 \u221a n + 2 j0(D/2\u2212\u03c3 d ) \u221a n where c = c p d Ep |\u03c8 \u03bb (X)| 2 1/2 is a constant. Proof. Since F d = B \u03c3 d p d ,\nE X1,...,Xn sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 k ) + E X1,...,Xn sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb Since, for a constant c, Ep |\u03c8 \u03bb (X)| m \u2264 c2 Dj(m/2\u22121)\nwe can apply the moment bound below. For the first term we have, (taking \u03b3 = \u03b1 and n 1 = n 2 = 0 in lemma 16 above)\nE X1,...,Xn sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 k ) \u2264 L D k E X1,...,Xn |\u03b1 p k \u2212 \u03b1 k | p d 1/p d (finitely many terms) \u2264 cL D p \u221e (T + A)n \u2212p d /2 1/p d (moment bound)\n\u2264 cn \u22121/2 where we use the fact only finitely many of the \u03b1s are non-zero because of the compactness of the support of the densities we consider and the compactness of the wavelets. Similarly taking \u03b3 = \u03b2, n 1 = 0, n 2 = j 0 in lemms 16 we have, using the moment bound as above,\nE X1,...,Xn sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb \u2264 c p \u221e L D j0 j=0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 Dj (T + A)n \u2212p d /2 1/p d \u2264 L D j0 j=0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 Dj/p d n \u22121/2 \u2264 cL D p \u221e j0 j=0 2 j(D/2\u2212\u03c3 d ) n \u22121/2 \u2264 c p \u221e 2 j0(D/2\u2212\u03c3 d ) n \u22121/2 \u03c3 d \u2264 D/2 n \u22121/2 \u03c3 d > D/2\nLemma 21. (Bias) Let X 1 , . . . , X n \u223c p where p \u2208 B \u03c3g pg,qg is compactly supported and\n\u03c3 g \u2265 D/p g , F d = B \u03c3 d p d ,q d .\nThen the bias of a linear wavelet estimator p with j 0 terms is bounded by\nd F d (p, E p [ p n ]) \u2264 c2 \u2212j0(\u03c3 d +\u03c3g\u2212(D/pg\u2212D/p d )+)\nwhere c is a constant that depends on p d and \u03c8 m .\nProof. Since p is compactly supported, by lemma 13 we need to upper bound\nsup \u03b2\u2208F d j\u2265j1 \u03bb\u2208\u039b \u03b2 f \u03bb \u03b2 p \u03bb\nUsing lemma 16 and the fact that\n\u03c3 g \u2265 D/p g sup \u03b2\u2208F d j\u2265j0 \u03bb\u2208\u039b \u03b2 f \u03bb \u03b2 p \u03bb \u2264 L D j\u2265j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p p d = L D j\u2265j0 2 j(\u03c3g+D/2\u2212D/pg) 2 j(\u03c3 d +\u03c3g+D\u2212D/p d \u2212D/pg) 2 j(D/p d \u2212D/pg)+ \u03b2 p pg \u2264 L D j\u2265j0 2 j(D/p d \u2212D/pg)+ 2 j(\u03c3 d +\u03c3g+D/p d \u2212D/pg) sup j\u2265j0 2 j(\u03c3g+D/2\u2212D/pg) \u03b2 p pg \u2264 2 \u2212j0(\u03c3 d +\u03c3g\u2212(D/pg\u2212D/p d )+) p \u03c3g pgqg (\u03c3 g \u2265 D/p g ) \u2264 c2 \u2212j0(\u03c3 d +\u03c3g\u2212(D/pg\u2212D/p d )+)\nUsing lemmas 21 and 20 we get the following upper bound on the bias and variance of the linear wavelet estimator.\nc n \u22121/2 + n \u22121/2 2 j0(D/2\u2212\u03c3 d ) + 2 \u2212j0(\u03c3g+\u03c3 d \u2212D/pg+D\u2212D/p d )\nwhich when minimized for j 0 gives,\n2 j0 = n 1/(2\u03c3g+D+2D/p d \u22122D/pg)\nwhich implies an upper bound of\nn \u22121/2 + n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D\u2212D/p d 2\u03c3g +D+2D/p d \u22122D/pg\nas desired.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Proof of the Lower Bound", "text": "In this section we prove our main lower bound i.e. Theorem 4 using Fano's lemma and the Varshamov Gilbert bound as summarized below. Lemma 22. (Fano's Lemma; Simplified Form of Theorem 2.5 of [53])\nFix a family P of distributions over a sample space X and fix a pseudo-metric \u03c1 : P \u00d7 P \u2192 [0, \u221e] over P. Suppose there exists a set T \u2286 P such that there is a p 0 \u2208 T with p p 0 \u2200p \u2208 T and\ns := inf p,p \u2208T \u03c1(p, p ) > 0 , sup p\u2208T D KL (p, p 0 ) \u2264 log |T | 16 ,\nwhere\nD KL : P \u00d7 P \u2192 [0, \u221e] denotes Kullback-Leibler divergence. Then, inf p sup p\u2208P E [\u03c1(p, p)] \u2265 s 16\nwhere the inf is taken over all estimators p. Lemma 23. Proof. (of Theorem 4) We follow the method in Donoho et. al. [14] and separate our proof into \"sparse\" and \"dense\" cases. As is standard procedure, for both cases we pick a finite subset of densities from F g over which estimation is difficult. Since any function in a Besov space can be defined by its wavelet coefficients we pick a set of densities by an appropriate choice of wavelet coefficients.\nHere we also need to pick a subset of functions from F d so as to estimate d F d . Following the method in [47] we pick from F d , functions that are analogous to the ones we pick from F g so that we measure the difference in the densities along the chosen perturbations.\nWe now fill in the details. We first let g 0 be a density function supported on an interval that contains [\u2212A, A] D such that g 0 \u03c3gpgqg \u2264 L G /2 and g 0 = c > 0 on [\u2212A, A] D .\nAt a particular resolution j, we choose 2 Dj wavelets with disjoint supports; pick \u03c8 \u03bb = 2 Dj/2 \u03c8 1 (2 Dj x \u2212 k) indexed by \u03bb = 2 \u2212j k + 2 \u2212(j+1) 1 s.t. k \u2208 K j where K j = {\u2212(2 j \u2212 1)A + 2lA, l = 0, . . . , (2 j \u2212 1)} D and 1 = (1, 0, . . . , 0) (i.e. we pick the first wavelet). Note here that if \u03bb = \u03bb then \u03c8 \u03bb and \u03c8 \u03bb have disjoint support.\nWe now describe our choice of densities based on the set of coefficients \u03b6 \u2286 {\u03c4 \u2208 Z\n|Kj | : |\u03c4 \u03bb | \u2264 1} i.e. \u2126 g := {g 0 + c g \u03bb \u03c4 \u03bb \u03c8 \u03bb : \u03c4 \u2208 \u03b6, \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j }.\nIf we pick c g to be small enough, every p in \u2126 g is a density function and is lower bounded on\n[\u2212A, A] D . Specifically if c g s.t. c g \u2264 c 2 \u03c8 \u221e 2 \u2212Dj/2 then g 0 + c g \u03bb \u03c4 \u03bb \u03c8 \u03bb = 1 (since \u03c8 \u03bb = 0) and, g 0 \u2212 p \u221e = c g 2 Dj/2 \u03c8 \u221e \u2264 c/2\nso that p is lower bounded on the domain of \u03c8 \u03bb by c/2 for every \u03bb. This also implies that p is always positive.\nNow the following lemma states that if you have a small perturbation of a density s.t. the density is lower bounded on the support of the perturbation then the KL divergence between the perturbed and the original density is upper bounded by the L 2 norm of the perturbation.\nLemma 24. Let g = g 0 + h, g 0 be density functions such that h \u2264 g 0 . If S = supp(h) \u2286 supp(g) and c \u2264 g on S, where c is a constant. Then\nD KL (g n , g n 0 ) \u2264 cn g 0 \u2212 g 2 L 2\nProof. Since g \u2264 2g 0 we have,\ng 0 \u2212 g g \u2265 \u2212 1 2 so using the fact that \u2212 log(1 + x) \u2264 x 2 \u2212 x for all x \u2265 \u22121/2 we get D KL (g n , g n 0 ) = nD KL (g, g 0 ) = n S g(x) log g(x) g 0 (x) dx = \u2212n S g(x) log 1 + g 0 (x) \u2212 g(x) g(x) dx \u2264 n S g(x) g 0 (x) \u2212 g(x) g(x) 2 \u2212 g 0 (x) \u2212 g(x) g(x) dx = n S (g 0 (x) \u2212 g(x)) 2 g(x) dx\nwhich, since g \u2265 c on S, is smaller than cn S (g 0 (x) \u2212 g(x)) 2 as desired.\nUsing this fact we conclude that for any p \u03c4 \u2208 \u2126 g ,\nKL(p \u03c4 , g 0 ) \u2264 nc 2 g c \u03bb \u03c4 \u03bb \u03c8 \u03bb 2 L 2 = cnc 2 g \u03c4 2 2\nFollowing the technique in [48] we also pick an analogous set of functions that live in F d so that we can lower bound d F D . In particular let\n\u2126 d := {c d \u03bb \u03c4 \u03bb \u03c8 \u03bb : \u03c4 \u2208 \u03b6, \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j }\nIt now, only remains to choose appropriate sets \u03b6 for the wavelet coefficients in each of the sparse and dense cases. In the remainder let c be a constant not necessarily the same.\nSparse or low-smoothness case:\nFor the sparse/lower smoothness case we choose worst case densities to be perturbations along only a specific scaling of the wavelet at a time. In particular, let\n\u03b6 = {\u03c4 : \u03c4 \u03bb = 1, \u03c4 \u03bb = 0, \u03bb = \u03bb = 2 \u2212j k + 2 \u2212(j+1) 1 , k \u2208 K j }\nWe know from above that for any c g \u2264 c2 \u2212Dj/2 , every p \u2208 \u2126 g is a density such that D KL (p n , g n 0 ) \u2264 cnc 2 g \u03c4 2 . Now, we need\ng 0 + c g \u03c8 \u03bb \u03c3g pgqg \u2264 g 0 \u03c3g pgqg + 2 j(\u03c3g+D/2\u2212D/pg) c g \u2264 L g so that \u2126 g \u2286 F g . Since \u03c3 g \u2265 D/p g the choice of c g = c2 \u2212j(\u03c3g+D/2\u2212D/pg) suffices. Similarly, c d = L d 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) implies \u2126 d \u2286 F d .\nThen we pick j large enough such that the KL divergence between any p \u03c4 and g 0 is small. This enables us to apply Fano's lemma from above and get a lower bound. i.e.\nn \u2264 cj/c 2 g \u21d0\u21d2 n \u2264 2 2j(\u03c3g+D/2\u2212D/pg) j for the KL divergence to be small. Given such a j we have,\nd F d (p \u03bb , p \u03bb ) \u2265 sup f \u2208\u2126 d c g (f (x)(\u03c8 \u03bb \u2212 \u03c8 \u03bb )dx = \u03c8 2 L 2 c g c d (since, \u03c8 \u03bb 2 L 2 = \u03c8 2 L 2 ). So, if 2 j = (n/ log n) 1 2\u03c3g +D\u22122D/pg we have, M (F g , F d ) log n n \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg\nDense or higher smoothness case:\nIn the dense case, we choose our set of densities by perturbing g 0 along every scaling of the wavelet simultaneously i.e. let \u03b6 = {\u03c4 : \u03c4 \u03bb \u2208 {\u22121, +1}} Now, we need\ng 0 + c g \u03bb \u03c4 \u03bb \u03c8 \u03bb \u03c3g pgqg \u2264 g 0 \u03c3g pgqg + 2 j(\u03c3g+D/2) c g \u2264 L g\nso that \u2126 g \u2286 F g . The choice of c g = c2 \u2212j(\u03c3g+D/2) suffices. Similarly,\nc d = L d 2 \u2212j(\u03c3 d +D/2) implies \u2126 d \u2286 F d .\nNow the Varshamov-Gilbert bound from above implies we can pick a subset of \u2126 G with size at least 2 |Kj |/8 such that \u03c9(\u03c4 \u03bb , \u03c4 k ) \u2265 |K j |/8 which gives,\nd F d (p \u03bb , p \u03bb ) = sup f \u2208\u2126 d c g (f (x)(\u03c8 \u03bb \u2212 \u03c8 \u03bb )dx = c g c d \u03c9(\u03c4 \u03bb , \u03c4 \u03bb ) \u2265 c g c d 2 Dj 4\nWe pick j large enough such that the KL divergence between any p \u03c4 and g 0 is small. This enables us to apply Fano's lemma from above and get a lower bound. In particular we need, for any\np \u03c4 \u2208 \u2126 g , D KL (p n \u03c4 , g n 0 ) \u2264 cnc 2 g \u03c4 2 = cnc 2 g |K j | to be at most log |\u03b6| 16 = |Kj |\n16 which is equivalent to n \u2264 2 j(2\u03c3g+D) . Then by Fano's lemma the lower bound in the dense case is\nn \u2212 \u03c3g +\u03c3 d 2\u03c3g +D\nWe combine the above two cases to get the following lower bound on the rate max (n\n\u2212 \u03c3g +\u03c3 d 2\u03c3g +D , n \u2212 \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg )", "publication_ref": ["b52", "b13", "b46", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "D Proof of the Upper Bound", "text": "We use the wavelet thresholding estimate as introduced in [14] to get an upper bound on our minimax rate.\nProof. (of theorem 5) We first upper bound our error by three terms namely, the stochastic error, the bias and the non-linear terms. The stochastic error is bounded above as usual by the above moment bound. The bias is bounded above by virtue of our density belonging to the besov space B \u03c3g pg,qg . The non-linear terms are more delicate. We follow the procedure in [14] and split them into four groups the first two of which are shown to be negligible as the probability of large deviations falls exponentially rapidly from Bernstein's inequality above. We simplify the upper bounds on the other two terms considerably by paying a penalty on the rate by the factor that is logarithmic in the sample size. We now fill in the details of the proof.\nWe first let our discriminator and generator classes be\nF d = {f : f \u03c3 d p d ,q d \u2264 L d } F g = {p : p \u03c3g pg,qg \u2264 L g } \u2229 P P = {p : p \u2265 0, p L 1 = 1, supp(p) \u2286 [\u2212T, T ]}\nGiven X 1 , . . . , X n be IID with density p \u2208 F g and the thresholded wavelet estimator of p i.e.\np = k\u2208Z \u03b1 p k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 p \u03bb \u03c8 \u03bb p n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb + j1 j=j0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb where \u03b1 p k = E X\u223cp [\u03c6 k (X)] \u03b2 p \u03bb = E X\u223cp [\u03c8 \u03bb (X)] \u03b1 k = 1 n n i=1 \u03c6 k (X i ) \u03b2 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ) \u03b2 \u03bb = \u03b2 \u03bb 1 { \u03b2 \u03bb >t}\nwith t = K j/n, where K is a constant to be specified later, and\n2 j0 = n 1 2\u03c3g +D 2 j1 = n 1 2\u03c3g +D\u22122D/pg\nwe can upper bound the error as,\nd F d (p, p n ) \u2264 sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 k ) + sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb + sup f \u2208F d j1 j\u2265j0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb + sup f \u2208F d j\u2265j1 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb\nwhere the first three terms constitute the stochastic error (the non-linear terms or thresholded terms are also called 'detail' terms [14]) and the last term is the bias. In particular:\n1. The first term in our upper bound of the risk is the stochastic error or the variance of a linear wavelet estimator with j 0 terms. Note that since \u03c3 g \u2265 D/p g p \u2208 F g implies by lemma 17 that p \u221e < \u221e. Then by substitution\nE p |\u03c8 \u03bb (X)| p d \u2264 2 \u2212Dj(p d /2\u22121)\nTherefore by lemma 20 we have an upper bound here of\ncn \u22121/2 (2 j0(D/2\u2212\u03c3 d ) + 1) n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D + n \u22121/2 2.\nThe third term is the bias of a linear wavelet estimator with j 1 terms which by lemma 21 for p d \u2265 p g is bounded above by\nc2 \u2212j1(\u03c3 d +\u03c3g\u2212D/pg+D/p d ) n \u2212 \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg\n3. For the second term we have, by lemmas 13 and 16\nE sup f \u2208F d j\u2265j0 \u03bb\u2208\u039b \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed E \u03bb\u2208\u039bj |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1 A \uf8f6 \uf8f8 1/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed \u03bb\u2208\u039bj E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1 A \uf8f6 \uf8f8 1/p d\nwhere we are only summing over finitely many terms. The set A is given by the following cases:\n(For the upper bounds of the first two cases we have chosen \u03b3 (which in turn determines the value of K) to be large enough so that the exponent of 2 j is negative and thus we can upper bound the geometric series by a constant multiple of the first term.) (a) Let A be the set of k s.t. \u03b2 \u03bb > t and \u03b2 p \u03bb < t/2 and r \u2265 1/p d then\nL D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed \u03bb\u2208\u039bj E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1 A \uf8f6 \uf8f8 1/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed \u03bb\u2208\u039bj (E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d r ) 1/r Pr(A) 1/r \uf8f6 \uf8f8 1/p d", "publication_ref": ["b13", "b13", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Using the large deviation and moment bound", "text": "Pr(A) \u2264 Pr | \u03b2 \u03bb \u2212 \u03b2 p \u03bb | \u2265 t/2 \u2264 c2 \u2212\u03b3j we get, \u2264 c j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 Dj n \u2212p d /2 2 \u2212j\u03b3/r 1/p d \u2264 c j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d \u2212D/p d ) n \u22121/2 2 \u2212\u03b3j/p d r \u2264 cn \u22121/2 j1 j=j0 2 \u2212j(\u03c3 d \u2212D/2+\u03b3/p d r ) \u2264 cn \u22121/2 2 \u2212j0(\u03c3 d \u2212D/2+\u03b3/p d r ) n \u2212 \u03c3g +\u03c3 d +\u03b3/p d r 2\u03c3g +D\n, which is negligible compared to the linear term.\n(b) Let B be the set of k s.t. \u03b2 \u03bb < t and \u03b2 p \u03bb > 2t then same as above\nE sup f \u2208F d j1 j=j0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb 1 B \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p \u03bb p d (Pr(B)) 1/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p \u03bb p d 2 \u2212\u03b3j/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +\u03c3 g +\u03b3/p d ) sup j0\u2264j\u2264j1 2 j(\u03c3 g +D/2\u2212D/p d ) \u03b2 \u03bb p d \u2264 L D L G j1 j=j0 2 \u2212j(\u03c3 d +\u03c3 g +\u03b3/p d ) \u2264 L D L G C2 \u2212j0(\u03c3 d +\u03c3 g +\u03b3/p d ) n \u2212 \u03c3 d +\u03c3 g +\u03b3 2\u03c3g +D\nwhich is negligible compared to the bias term.\n(c) Let C be the set of k s.t. \u03b2 \u03bb > t and \u03b2 p \u03bb > t/2 then: Proof. Just as in the proof of the lower bound above we let j \u2265 0 and \u2126 g := {g 0 \u00b1 c g \u03c8 \u03bb : \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j } where 1 = (1, 0, . . . , 0). Here we let g 0 = 2 Dj c on at least [\u2212A, A] D and c g = min c 2 \u03c8 \u221e 2 \u2212Dj/2 , L g 2 2 \u2212j(\u03c3g+D/2\u2212D/pg) such that \u2126 g \u2286 F g . We also let\nE sup f \u2208F d j1 j=j0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb 1 C \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) k\u2208C E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1/p d \u2264 L D j1 j=j0 Cn \u22121/2 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) k\u2208C 2\u03b2 p \u03bb n/j K pg 1/p d \u2264 L D j1 j=j0 Cn \u22121/2 ( n/j) pg/p d 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p pg/p d pg \u2264 L D j1 j=j0 Cn \u22121/2 ( n/j) pg/p d 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 \u2212j(\u03c3g+D/2\u2212D/pg)pg/p d sup j0\u2264j\u2264j1 \u03b2 pg 2 j(\u03c3g+D/2\u2212D/pg) \u2264 CL D L G n 1/2(pg/p d \u22121) j1 j=j0 2 \u2212j((\u03c3g+D/2)pg/p d +\u03c3 d \u2212D/2) j \u2212pg/2p d \u2264 CL D L G n 1/2(pg/p d \u22121) 2 \u2212jm((\u03c3g+D/2)pg/p d +\u03c3 d \u2212D/2) where j m = j 0 (2\u03c3 g + D)p g \u2265 (D \u2212 2\u03c3 d )p d j 1 (2\u03c3 g + D)p g \u2264 (D \u2212 2\u03c3 d )p d In\n\u2126 d := {c d \u03bb \u03c4 \u03bb \u03c8 \u03bb : \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j , \u03c4 \u2264 L d } s.t. c d \u2264 L d 2 \u2212j(\u03c3 d +D/2\u22121/p d )\ni.e. \u2126 d \u2286 F d .\nThen for any linear estimate P with \u03b1 \u03bb = \u03c8 \u03bb (x)d P (x), \nsup\n\uf8eb \uf8ed \u03bb =\u03bb ( E g0+cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0+cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d \uf8f6 \uf8f8 1/p d \u2265 c d \uf8eb \uf8ed 1 2 Dj \u03bb =\u03bb ( E g0+cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0+cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d \uf8f6 \uf8f8 1/p d\nNow the expression inside the brackets is bounded below in [14]  Our statistical guarantees rely on a recent construction, by Suzuki [51], of a fully-connected ReLU network that approximates Besov functions. Specifically, we leverage the following result: Lemma 26 (Proposition 1 of Suzuki [51]). Suppose that p, q, r \u2208 (0, \u221e] and \u03c3 > \u03b4 := D(1/p \u2212 1/r) + and let \u03bd = (\u03c3 \u2212 \u03b4)/(2\u03b4). Then, for sufficiently small \u2208 (0, 1), there exists a constant C > 0, depending only on D, p, q, r, \u03c3, such that, for some ", "publication_ref": ["b13", "b50", "b50"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Deep Lipschitz networks and Dudley GANs", "journal": "", "year": "2018", "authors": "Ehsan Abbasnejad; Javen Shi; Anton Van Den;  Hengel"}, {"ref_id": "b1", "title": "", "journal": "On optimal matchings. Combinatorica", "year": "1984", "authors": "Mikl\u00f3s Ajtai; J\u00e1nos Koml\u00f3s; G\u00e1bor Tusn\u00e1dy"}, {"ref_id": "b2", "title": "Towards principled methods for training generative adversarial networks", "journal": "", "year": "2017", "authors": "Martin Arjovsky; L\u00e9on Bottou"}, {"ref_id": "b3", "title": "Wasserstein generative adversarial networks", "journal": "", "year": "2017", "authors": "Martin Arjovsky; Soumith Chintala; L\u00e9on Bottou"}, {"ref_id": "b4", "title": "Generalization and equilibrium in generative adversarial nets", "journal": "", "year": "2017", "authors": "Sanjeev Arora; Rong Ge; Yingyu Liang; Tengyu Ma; Yi Zhang"}, {"ref_id": "b5", "title": "On a modification of Chebyshev's inequality and on the error in Laplace formula. Collected Works, Izd-vo'Nauka", "journal": "", "year": "1964", "authors": "S N Bernstein"}, {"ref_id": "b6", "title": "Geometrical insights for implicit generative modeling", "journal": "Springer", "year": "2018", "authors": "Leon Bottou; Martin Arjovsky; David Lopez-Paz; Maxime Oquab"}, {"ref_id": "b7", "title": "Normal approximation by Stein's method", "journal": "Springer Science & Business Media", "year": "2010", "authors": "H Y Louis; Larry Chen; Qi-Man Goldstein;  Shao"}, {"ref_id": "b8", "title": "Generating multi-label discrete patient records using generative adversarial networks", "journal": "", "year": "2017", "authors": "Edward Choi; Siddharth Biswal; Bradley Malin; Jon Duke; F Walter; Jimeng Stewart;  Sun"}, {"ref_id": "b9", "title": "Applied nonparametric statistics", "journal": "", "year": "1978", "authors": "W Wayne;  Daniel"}, {"ref_id": "b10", "title": "Ten lectures on wavelets", "journal": "", "year": "1992", "authors": "Ingrid Daubechies"}, {"ref_id": "b11", "title": "Semi-supervised generative adversarial network for gene expression inference", "journal": "ACM", "year": "2018", "authors": "Xiaoqian Kamran Ghasedi Dizaji; Heng Wang;  Huang"}, {"ref_id": "b12", "title": "Towards a deeper understanding of adversarial losses", "journal": "", "year": "2019", "authors": "Wen Hao; Yi-Hsuan Dong;  Yang"}, {"ref_id": "b13", "title": "Density estimation by wavelet thresholding. The Annals of Statistics", "journal": "", "year": "1996", "authors": "Iain M David L Donoho; G\u00e9rard Johnstone; Dominique Kerkyacharian;  Picard"}, {"ref_id": "b14", "title": "The speed of mean Glivenko-Cantelli convergence", "journal": "The Annals of Mathematical Statistics", "year": "1969", "authors": "R M Dudley"}, {"ref_id": "b15", "title": "Speeds of metric probability convergence", "journal": "", "year": "1972", "authors": "R M Dudley"}, {"ref_id": "b16", "title": "Training generative neural networks via maximum mean discrepancy optimization", "journal": "", "year": "2015", "authors": " Gk Dziugaite; Z Roy;  Ghahramani"}, {"ref_id": "b17", "title": "Partial differential equations", "journal": "American Mathematical Society", "year": "2010", "authors": "C Lawrence;  Evans"}, {"ref_id": "b18", "title": "On the rate of convergence in Wasserstein distance of the empirical measure", "journal": "Probability Theory and Related Fields", "year": "2015", "authors": "Nicolas Fournier; Arnaud Guillin"}, {"ref_id": "b19", "title": "Simon Lacoste-Julien, and Ioannis Mitliagkas. Negative momentum for improved game dynamics", "journal": "", "year": "2018", "authors": " Gauthier Gidel; Askari Reyhane; Mohammad Hemmat; Gabriel Pezeshki; Remi Huang;  Lepriol"}, {"ref_id": "b20", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "Ian Goodfellow; Jean Pouget-Abadie; Mehdi Mirza; Bing Xu; David Warde-Farley; Sherjil Ozair; Aaron Courville; Yoshua Bengio"}, {"ref_id": "b21", "title": "Improved training of wasserstein gans", "journal": "", "year": "2017", "authors": "Ishaan Gulrajani; Faruk Ahmed; Martin Arjovsky; Vincent Dumoulin; Aaron C Courville"}, {"ref_id": "b22", "title": "drugan: an advanced generative adversarial autoencoder model for de novo generation of new molecules with desired molecular properties in silico", "journal": "Molecular pharmaceutics", "year": "2017", "authors": "Artur Kadurin; Sergey Nikolenko; Kuzma Khrabrov; Alex Aliper; Alex Zhavoronkov"}, {"ref_id": "b23", "title": "On a space of completely additive functions", "journal": "", "year": "1958", "authors": "Leonid Vasilevich Kantorovich;  Gennady S Rubinstein"}, {"ref_id": "b24", "title": "Photo-realistic single image superresolution using a generative adversarial network", "journal": "", "year": "2017", "authors": "Christian Ledig; Lucas Theis; Ferenc Husz\u00e1r; Jose Caballero; Andrew Cunningham; Alejandro Acosta; Andrew Aitken; Alykhan Tejani; Johannes Totz; Zehan Wang"}, {"ref_id": "b25", "title": "Convergence and concentration of empirical measures under Wasserstein distance in unbounded functional spaces", "journal": "", "year": "2018", "authors": "Jing Lei"}, {"ref_id": "b26", "title": "Mmd gan: Towards deeper understanding of moment matching network", "journal": "", "year": "2017", "authors": "Chun-Liang Li; Wei-Cheng Chang; Yu Cheng; Yiming Yang; Barnab\u00e1s P\u00f3czos"}, {"ref_id": "b27", "title": "How well can generative adversarial networks (GAN) learn densities: A nonparametric view", "journal": "", "year": "2017", "authors": "Tengyuan Liang"}, {"ref_id": "b28", "title": "On how well generative adversarial networks learn densities: Nonparametric and parametric results", "journal": "", "year": "2018", "authors": "Tengyuan Liang"}, {"ref_id": "b29", "title": "Interaction matters: A note on non-asymptotic local convergence of generative adversarial networks", "journal": "", "year": "2018", "authors": "Tengyuan Liang; James Stokes"}, {"ref_id": "b30", "title": "Pacgan: The power of two samples in generative adversarial networks", "journal": "", "year": "2018", "authors": "Zinan Lin; Ashish Khetan; Giulia Fanti; Sewoong Oh"}, {"ref_id": "b31", "title": "Approximation and convergence properties of generative adversarial learning", "journal": "", "year": "2017", "authors": "Shuang Liu; Olivier Bousquet; Kamalika Chaudhuri"}, {"ref_id": "b32", "title": "Least squares generative adversarial networks", "journal": "", "year": "2017", "authors": "Xudong Mao; Qing Li; Haoran Xie; Y K Raymond; Zhen Lau; Stephen Paul Wang;  Smolley"}, {"ref_id": "b33", "title": "The tight constant in the dvoretzky-kiefer-wolfowitz inequality. The annals of Probability", "journal": "", "year": "1990", "authors": "Pascal Massart"}, {"ref_id": "b34", "title": "Which training methods for gans do actually converge?", "journal": "", "year": "2018", "authors": "Lars Mescheder; Andreas Geiger; Sebastian Nowozin"}, {"ref_id": "b35", "title": "Wavelets and operators", "journal": "Cambridge university press", "year": "1992", "authors": "Yves Meyer"}, {"ref_id": "b36", "title": "Learning in implicit generative models", "journal": "", "year": "2016", "authors": "Shakir Mohamed; Balaji Lakshminarayanan"}, {"ref_id": "b37", "title": "", "journal": "", "year": "2017", "authors": "Youssef Mroueh; Chun-Liang Li; Tom Sercu; Anant Raj; Yu Cheng;  Sobolev Gan"}, {"ref_id": "b38", "title": "Integral probability metrics and their generating classes of functions", "journal": "Advances in Applied Probability", "year": "1997", "authors": "Alfred M\u00fcller"}, {"ref_id": "b39", "title": "Gradient descent gan optimization is locally stable", "journal": "", "year": "2017", "authors": "Vaishnavh Nagarajan;  Kolter"}, {"ref_id": "b40", "title": "Nonparametric estimation of smooth regression functions", "journal": "Izv. Akad. Nauk. SSR Teckhn. Kibernet", "year": "1985", "authors": "S Arkadi;  Nemirovski"}, {"ref_id": "b41", "title": "Topics in non-parametric", "journal": "", "year": "2000", "authors": "S Arkadi;  Nemirovski"}, {"ref_id": "b42", "title": "Empirical processes: theory and applications", "journal": "JSTOR", "year": "1990", "authors": "David Pollard"}, {"ref_id": "b43", "title": "On the subspaces of l p (p > 2) spanned by sequences of independent random variables", "journal": "Israel Journal of Mathematics", "year": "1970", "authors": "P Haskell;  Rosenthal"}, {"ref_id": "b44", "title": "A higher-order kolmogorov-smirnov test", "journal": "", "year": "2019", "authors": "Veeranjaneyulu Sadhanala; Aaditya Ramdas; Yu-Xiang Wang; Ryan Tibshirani"}, {"ref_id": "b45", "title": "Optimizing distributions over molecular space. an objective-reinforced generative adversarial network for inversedesign chemistry (organic", "journal": "", "year": "2017", "authors": "Benjamin Sanchez-Lengeling; Carlos Outeiral; L Gabriel; Alan Guimaraes;  Aspuru-Guzik"}, {"ref_id": "b46", "title": "Minimax distribution estimation in Wasserstein distance", "journal": "", "year": "2018", "authors": "Shashank Singh; Barnab\u00e1s P\u00f3czos"}, {"ref_id": "b47", "title": "Manzil Zaheer, and Barnabas Poczos. Nonparametric density estimation under adversarial losses", "journal": "", "year": "2018", "authors": "Shashank Singh; Ananya Uppal; Boyue Li; Chun-Liang Li"}, {"ref_id": "b48", "title": "Amortised map inference for image super-resolution", "journal": "", "year": "2016", "authors": "Jose Casper Kaae S\u00f8nderby; Lucas Caballero; Wenzhe Theis; Ferenc Shi;  Husz\u00e1r"}, {"ref_id": "b49", "title": "Non-parametric estimation of integral probability metrics", "journal": "IEEE", "year": "2010", "authors": "K Bharath; Kenji Sriperumbudur; Arthur Fukumizu; Bernhard Gretton; Gert Rg Sch\u00f6lkopf;  Lanckriet"}, {"ref_id": "b50", "title": "Adaptivity of deep relu network for learning in besov and mixed smooth besov spaces: optimal rate and curse of dimensionality", "journal": "", "year": "2018", "authors": "Taiji Suzuki"}, {"ref_id": "b51", "title": "A type diagram for function spaces", "journal": "", "year": "2011", "authors": "Terence Tao"}, {"ref_id": "b52", "title": "Introduction to nonparametric estimation. Revised and extended from the 2004 French original. Translated by Vladimir Zaiats", "journal": "Springer", "year": "2009", "authors": "B Alexandre;  Tsybakov"}, {"ref_id": "b53", "title": "Optimal transport: old and new", "journal": "Springer Science & Business Media", "year": "2008", "authors": "C\u00e9dric Villani"}, {"ref_id": "b54", "title": "All of nonparametric statistics", "journal": "", "year": "2006", "authors": "Larry Wassermann"}, {"ref_id": "b55", "title": "Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance", "journal": "", "year": "2017", "authors": "Jonathan Weed; Francis Bach"}, {"ref_id": "b56", "title": "Estimation of smooth densities in wasserstein distance", "journal": "", "year": "2019", "authors": "Jonathan Weed; Quentin Berthet"}, {"ref_id": "b57", "title": "Improving neural machine translation with conditional sequence generative adversarial nets", "journal": "", "year": "2017", "authors": "Zhen Yang; Wei Chen; Feng Wang; Bo Xu"}, {"ref_id": "b58", "title": "Functional analysis. reprint of the sixth (1980) edition. classics in mathematics", "journal": "Springer-Verlag", "year": "1995", "authors": "Kosaku Yosida"}, {"ref_id": "b59", "title": "Robust unsupervised domain adaptation for neural networks via moment alignment", "journal": "Information Sciences", "year": "2019", "authors": "Werner Zellinger; A Bernhard; Thomas Moser; Edwin Grubinger; Thomas Lughofer; Susanne Natschl\u00e4ger;  Saminger-Platz"}, {"ref_id": "b60", "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "journal": "", "year": "2017", "authors": "Han Zhang; Tao Xu; Hongsheng Li; Shaoting Zhang; Xiaogang Wang; Xiaolei Huang; Dimitris N Metaxas"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "For non-negative real sequences {a n } n\u2208N , {b n } n\u2208N , a n b n indicates lim sup n\u2192\u221e an bn < \u221e, and a n b n indicates a n b n a n . For p \u2208 [1, \u221e], p := p p\u22121 denotes the H\u00f6lder conjugate of p (with 1 = \u221e, \u221e = 1). L p (R D ) (resp. l p ) denotes the set of functions f (resp. sequences a) with f p := |f (x)| p dx 1/p < \u221e (resp. a l p := n\u2208N |a n | p 1/p < \u221e).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "d , H g grow logarithmically with 1/ , W d , S d , B d , W g , S g , B g grow polynomially with 1/ and C > 0 is a constant that depends only on B \u03c3 d p d ,q d and B \u03c3g pg,qg .", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Corollary 10 .10For a Besov density class B \u03c3g pg,qg with \u03c3 g > D/p g and discriminator class B \u03c3 d p d ,q d with \u03c3 d > D/p d there exists an appropriately constructed GAN estimate p s.t.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: Minimax convergence rates as functions of discriminator smoothness \u03c3 d and distribution function smoothness \u03c3 g , for (a) general and (b) linear estimators, in the case D = 4, p d = 1.2, p g = 2. Color shows exponent of minimax convergence rate (i.e., \u03b1(\u03c3 d , \u03c3 g ) such that M B \u03c3 d 1.2,q d (R D ), B \u03c3g 2,qg (R D ) n \u2212\u03b1(\u03c3 d ,\u03c3g) ), ignoring polylogarithmic factors.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": ")) Let \u2126 = {0, 1} m where m \u2265 8. Then there exists a subset {w 0 , . . . , w M } of \u2126 such that w 0 = (0, . . . , 0) and\u03c9(w j , w k ) \u2265 m 8 \u22000 \u2264 j, k \u2264 Mwhere M \u2265 2 m/8 , where \u03c9(w j , w k ) = m i=1 1 {w j i =w k i } is the Hamming distance.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "2the first case we have an upper bound of n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D and in the second case we have an upper bound of n \u2212 \u03c3g +\u03c3 d +D\u2212D/p d \u2212D/pg 2\u03c3g +D\u22122D/pg (d) Let E be the set of k s.t. \u03b2 \u03bb < t and \u03b2 p \u03bb < 2t then: \u2212j(\u03c3 d +D/2\u2212D/p d ) (\u03c3 d +D/2\u2212D/p d ) (2t) 1\u2212pg/p d \u03b2 pg/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) (2 j/n) 1\u2212pg/p d 2 \u2212j(\u03c3g+D/2\u2212D/pg)pg/p d L g \u2264 c j 1 n 1/2(pg/p d \u22121) j1 j=j0 2 \u2212j((\u03c3g+D/2)pg/p d +\u03c3 d \u2212D/2) j \u2212pg/2p d n \u2212 \u03c3g +\u03c3 d 2\u03c3g +1 + n \u2212 \u03c3g +\u03c3 d +D\u2212D/p d \u2212D/pg 2\u03c3g +D\u22122D/pglog n E Proof of Theorem 7Lower Bound", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "\u03c4\u03c4: \u03c4 p d \u2264L d \u03bb =\u03bb |\u03c4 \u03bb \u03b1 \u03bb | + |\u03c4 \u03bb ||c g \u2212 \u03b1 \u03bb | : \u03c4 p d \u2264L d \u03bb =\u03bb |\u03c4 \u03bb \u03b1 \u03bb | + |\u03c4 \u03bb ||c g \u2212 \u03b1 \u03bb | \u03bb || \u03b1 \u03bb | + E g0\u2212cg\u03c8 \u03bb |\u03c4 \u03bb || \u03b1 \u03bb | + E g0+cg\u03c8 \u03bb |\u03c4 \u03bb ||c g \u2212 \u03b1 \u03bb | + E g0\u2212cg\u03c8 \u03bb |\u03c4 \u03bb ||c g \u2212 \u03b1 \u03bb |", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fappendix A.3 by n \u22121/2 2 jD/p d where 2 j = n 1 2\u03c3g \u22122D/pg +2D/p d +D which implies a lower bound in our case ofc2 \u2212j(\u03c3 d +D/2\u2212D/p d ) n \u22121/2 2 Dj/p d = c2 j(D/2\u2212\u03c3 d ) n \u22121/2which gives us a lower bound ofn \u2212 \u03c3 d +\u03c3g \u2212D/pg +D/p d 2\u03c3g \u22122D/pg +2D/p d +Das desired. Proof of Theorem 9Here, we prove the following theorem, which upper bounds the risk of an appropriately constructed GAN for learning Besov distributions: Theorem 25 (Convergence Rate of a Well-Optimized GAN). Fix a Besov density class B \u03c3g pg,qg with \u03c3 g > D/p g and discriminator class B \u03c3 d p d ,q d with \u03c3 d > D/p d . Then, for any desired approximation error > 0, one can construct a GAN p of the form (9) (with p n ) with discriminator networkN d \u2208 \u03a6(H d , W d , S d , B d ) and generator network N g \u2208 \u03a6(H g , W g , S g , B g ), s.t. for all p \u2208 B \u03c3g pg,qg E d B \u03c3 d p d ,q d ( p, p) + E d B \u03c3 d p d ,q d ( p n ,p) where H d , H g grow logarithmically with 1/ , W d , S d , B d , W g , S g , B g grow polynomially with 1/ and C > 0 is a constant that depends only on B \u03c3 d p d ,q d and B \u03c3g pg,qg .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "H\u2264 C log(1/ ), W \u2264 C \u2212D/\u03c3 , S \u2264 C \u2212D/\u03c3 log(1/ ), B \u2264 C \u2212(D/\u03bd+1)(1\u2228(D/p\u2212\u03c3)+)/\u03c3 , \u03a6(H, W, S, B) \u2286 B \u03c3 p,q(1) and \u03a6(H, W, S, B) approximates B \u03c3 p,q(1) to accuracy in L r ; i.e., supf \u2208B \u03c3 p,q (1) inf f \u2208\u03a6(H,W,S,B) f \u2212 f L r \u2264 C . Proof.Liang [28, Inequality 2.2] showed that we can decompose the error, for densities p, p, d F d ( p, p) \u2264 inf q\u2208\u03a6(Hg,Wg,Sg,Bg)d F d (p, q) + 2 sup f \u2208F d inf g\u2208\u03a6(H d ,W d ,S d ,B d ) f \u2212 g \u221e + d \u03a6(H d ,W d ,S d ,B d ) (p, p n ) + d F d (p, p n ), where the 3 summands above correspond respectively the error of approximatingF g by \u03a6(L g , W g , S g , B g ) (generator approximation error), the error of approximating F d by \u03a6(L d , W d , S d , B d ) (discriminatorapproximation error), and statistical error.To bound the first term, note also that, since we assumed \u03c3 d > D/p d , we have the embeddingB \u03c3 d p d ,q d \u2286 L \u221e , and, in particular, M := sup f \u2208B \u03c3 d p d ,q d f L \u221e < \u221e.Thus, by H\u00f6lder's inequality, the assumption that densities in P are supported only on [\u2212T, T ], and Lemma 26 (with r = \u221e), infq\u2208Fg d F d (p, q) \u2264 inf q\u2208Fg (p, q) sup f \u2208F D f L 1 ([\u2212T,T ]) p \u2212 q L \u221e \u2264 2M T .To bound the second term, simply observe that, by Lemma 26 (with r = \u221e), supf \u2208F d inf g\u2208\u03c6(Lg,Wg,Sg,Bg) f \u2212 g \u221e \u2264 .Since, byLemma  26, \u03a6(L d , W d , S d , B d ) \u2286 B \u03c3 d p d ,q d , the last term is immediately bounded (in expectation) by d F d ( p n , p). Combining the bounds on these three terms gives d F d ( p, p) \u2264 2(M T + 1) + 2d F d ( p n , p).", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "q d and p is compactly supported we can, by lemma 13 upper bound", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "f \u2208F E X\u223cp [f (X)] \u2212 E X\u223c p(X1,...,Xn) [f (X)] ,( * )", "formula_coordinates": [1.0, 268.86, 501.27, 235.14, 16.6]}, {"formula_id": "formula_1", "formula_text": "P n = 1 n n i=1", "formula_coordinates": [1.0, 108.0, 637.11, 58.25, 14.56]}, {"formula_id": "formula_2", "formula_text": "P = argmin Q\u2208P sup f \u2208F E X\u223cQ [f (X)] \u2212 E X\u223c Pn [f (X)] = argmin Q\u2208P d F Q, P n ,(1)", "formula_coordinates": [2.0, 159.79, 166.14, 344.22, 17.56]}, {"formula_id": "formula_3", "formula_text": "\u221e j=\u2212\u221e V j = L 2 (R D ). 2. For f \u2208 L 2 (R D ), k \u2208 Z D , j \u2208 Z, f (x) \u2208 V 0 \u21d4 f (x \u2212 k) \u2208 V 0 & f (x) \u2208 V j \u21d4 f (2x) \u2208 V j+1 .", "formula_coordinates": [2.0, 108.0, 480.09, 394.87, 26.34]}, {"formula_id": "formula_4", "formula_text": "{\u03c8 } \u2208E such that {2 Dj/2 \u03c8 (2 j x \u2212 k) : \u2208 E, k \u2208 Z D } \u222a {2 Dj/2 \u03c6(2 j x \u2212 k) : k \u2208 Z D } is an orthonormal basis of V j \u2286 L 2 (R D ). Let \u039b j = {2 \u2212j k + 2 \u2212j\u22121 : k \u2208 Z D , \u2208 E} \u2286 R D .", "formula_coordinates": [2.0, 108.0, 606.44, 397.39, 44.48]}, {"formula_id": "formula_5", "formula_text": "\u03b1 k := R D f (x)\u03c6(x \u2212 k)dx for k \u2208 Z D and \u03b2 \u03bb := R D f (x)\u03c8 \u03bb (x)dx for \u03bb \u2208 \u039b, satisfy f B \u03c3 p,q := {\u03b1 k } k\u2208Z D l p + 2 j(\u03c3+D(1/2\u22121/p)) {\u03b2 \u03bb } \u03bb\u2208\u039bj l p j\u2208N l q < \u221e", "formula_coordinates": [3.0, 122.36, 143.99, 367.28, 45.62]}, {"formula_id": "formula_6", "formula_text": "an IPM d B \u03c3 d p d ,q d (L d ) . Specifically, for general \u03c3 d , \u03c3 g , p d , p g , q d , q g , we seek to bound minimax risk M B \u03c3 d p d ,q d , B \u03c3g pg,qg := inf p sup p\u2208B \u03c3g pg ,qg E X1:n d B \u03c3 d p d ,q d (p, p(X 1 , . . . , X n ))(2)", "formula_coordinates": [3.0, 108.0, 294.1, 396.0, 44.35]}, {"formula_id": "formula_7", "formula_text": "d B \u03c3 d 2,2", "formula_coordinates": [3.0, 390.21, 647.63, 20.27, 12.79]}, {"formula_id": "formula_8", "formula_text": "d B \u03c3 d p d ,q d .", "formula_coordinates": [3.0, 473.47, 661.54, 32.28, 13.47]}, {"formula_id": "formula_9", "formula_text": "1. L p Distances: If F d = L p = B 0", "formula_coordinates": [4.0, 108.0, 216.53, 158.44, 11.23]}, {"formula_id": "formula_10", "formula_text": "\u2212 \u03c3g 2\u03c3g +D + n \u2212 \u03c3g +D(1\u22121/pg \u22121/p d ) 2\u03c3g +D(1\u22122/pg )", "formula_coordinates": [4.0, 356.2, 252.34, 126.43, 17.5]}, {"formula_id": "formula_11", "formula_text": "2\u03c3g +D + n \u2212 \u03c3g \u2212D/pg +D/p d 2\u03c3g +D\u22122D/pg +2D/p", "formula_coordinates": [4.0, 216.03, 272.43, 119.72, 17.04]}, {"formula_id": "formula_12", "formula_text": "If F d = C 1 (1) B 1", "formula_coordinates": [4.0, 216.68, 292.95, 84.63, 11.23]}, {"formula_id": "formula_13", "formula_text": "0 < m \u2264 p, q \u2264 M < \u221e), the bounds M \u22121/p d B 1 p ,\u221e (p, q) \u2264 W p (p, q) \u2264 m \u22121/p d B 1 p ,1 (p, q)", "formula_coordinates": [4.0, 108.0, 394.33, 397.16, 14.71]}, {"formula_id": "formula_14", "formula_text": "F d = B \u03c3 d \u221e,", "formula_coordinates": [4.0, 108.0, 505.89, 44.81, 12.2]}, {"formula_id": "formula_15", "formula_text": "F d = BV B 1 1,", "formula_coordinates": [4.0, 266.32, 521.7, 65.57, 12.2]}, {"formula_id": "formula_16", "formula_text": "If F d = W \u03c3 d ,2 = B \u03c3 2,2 is a Hilbert-Sobolev space, for \u03c3 \u2208 R, then d F d = \u2022 \u2212 \u2022 W \u2212\u03c3 d ,", "formula_coordinates": [4.0, 115.2, 558.79, 388.8, 24.41]}, {"formula_id": "formula_17", "formula_text": "\u2212 \u03c3g +\u03c3 d 2\u03c3g +1 + n \u22121/2 when F g = W \u03c3g,2", "formula_coordinates": [4.0, 233.04, 582.87, 144.88, 15.9]}, {"formula_id": "formula_18", "formula_text": "p d \u2265 p g , \u03c3 g \u2265 D/p g , M B \u03c3 d p d ,q d , B \u03c3g pg,qg max n \u22121/2 , n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D , n \u2212 \u03c3g +\u03c3 d +D(1\u22121/pg \u22121/p d ) 2\u03c3g +D(1\u22122/pg ) .(3)", "formula_coordinates": [4.0, 414.61, 713.2, 86.81, 10.98]}, {"formula_id": "formula_19", "formula_text": "M lin B \u03c3 d p d ,q d , B \u03c3g pg,qg n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D/p d 2\u03c3g +D(1\u22122/pg )+2D/p d .(4)", "formula_coordinates": [5.0, 207.09, 139.22, 296.91, 19.84]}, {"formula_id": "formula_20", "formula_text": "Theorem 4. (Lower Bound) Let r > \u03c3 g \u2265 D/p g , then, M B \u03c3 d p d ,q d , B \u03c3g pg,qg max \uf8eb \uf8ed n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D , log n n \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg \uf8f6 \uf8f8 (5)", "formula_coordinates": [5.0, 107.67, 328.21, 396.33, 46.61]}, {"formula_id": "formula_21", "formula_text": "p n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb + j1 j=j0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb .(6)", "formula_coordinates": [5.0, 200.63, 443.61, 303.37, 31.02]}, {"formula_id": "formula_22", "formula_text": "\u03b1 k = 1 n n i=1 \u03c6 k (X i ), \u03b2 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ), and \u03b2 \u03bb = \u03b2 \u03bb 1 { \u03b2 \u03bb > \u221a j/", "formula_coordinates": [5.0, 109.2, 486.83, 394.8, 30.15]}, {"formula_id": "formula_23", "formula_text": "0 = 1 2\u03c3g+D log 2 n, j 1 = 1 2\u03c3g+D\u22122D/pg log 2 n. Theorem 5. (Upper Bound) Let r > \u03c3 g \u2265 D/p g and p d > p g . Then, for a constant C depending only on p d , \u03c3 g , p g , q g , D, L g , L d and \u03c8 p d , M B \u03c3 d p d ,q d , B \u03c3g pg,qg \u2264 C log n n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D + n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D/p d 2\u03c3g +D\u22122D/pg + n \u22121/2 (7)", "formula_coordinates": [5.0, 107.67, 549.58, 396.33, 71.32]}, {"formula_id": "formula_24", "formula_text": "P (A) = n i=1 T i (X i , A).(8)", "formula_coordinates": [6.0, 258.27, 139.77, 245.73, 30.32]}, {"formula_id": "formula_25", "formula_text": "Suppose r > \u03c3 g \u2265 D/p g , M lin B \u03c3 d p d ,q d , B \u03c3g pg,qg := inf Plin sup p\u2208Fg E X1:n d F d \u00b5 p , P n \u2212 1 2 + n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D/p d 2\u03c3g +D\u22122D/pg +2D/p d + n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D", "formula_coordinates": [6.0, 108.0, 242.54, 400.33, 41.49]}, {"formula_id": "formula_26", "formula_text": "\u2212 \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg", "formula_coordinates": [6.0, 345.3, 314.69, 84.58, 11.09]}, {"formula_id": "formula_27", "formula_text": "P := argmin Q\u2208Fg sup f \u2208F d E X\u223cQ [f (X)] \u2212 E X\u223c Pn [f (X)] ,(9)", "formula_coordinates": [6.0, 207.58, 572.09, 296.42, 17.56]}, {"formula_id": "formula_28", "formula_text": "A H \u03b7 (A H\u22121 \u03b7 (\u2022 \u2022 \u2022 \u03b7(A 1 x + b 1 ) \u2022 \u2022 \u2022 ) + b H\u22121 ) + b H ,", "formula_coordinates": [7.0, 200.75, 155.11, 210.5, 9.65]}, {"formula_id": "formula_29", "formula_text": "B := max{ A \u221e,\u221e , b \u221e : \u2208 [H]}.", "formula_coordinates": [7.0, 337.27, 229.97, 168.47, 9.65]}, {"formula_id": "formula_30", "formula_text": "B \u03c3 d p d ,q d with \u03c3 d > D/p d .", "formula_coordinates": [7.0, 255.13, 336.2, 98.27, 12.86]}, {"formula_id": "formula_31", "formula_text": "N d \u2208 \u03a6(H d , W d , S d , B d ) and generator network N g \u2208 \u03a6(H g , W g , S g , B g ), s.t. for all p \u2208 B \u03c3g pg,qg E d B \u03c3 d p d ,q d ( p, p) + E d B \u03c3 d p d ,q d ( p n , p)", "formula_coordinates": [7.0, 108.0, 357.34, 391.33, 39.7]}, {"formula_id": "formula_32", "formula_text": "H", "formula_coordinates": [7.0, 134.62, 405.8, 8.28, 8.74]}, {"formula_id": "formula_33", "formula_text": "d F d ( p, p) \u2264 n \u2212\u03b7(D,\u03c3 d ,p d ,\u03c3g,pg) log n where \u03b7(D, \u03c3 d , p d , \u03c3 g , p g ) = min 1 2 , \u03c3g+\u03c3 d 2\u03c3g+D , \u03c3g+\u03c3 d +D\u2212D/pg\u2212D/p d 2\u03c3g+D(1\u22122/pg)", "formula_coordinates": [7.0, 108.0, 519.92, 275.58, 39.89]}, {"formula_id": "formula_34", "formula_text": "D = 4, p d = 1.2, p g = 2. When 1/p g + 1/p d > 1, a minimum total smoothness \u03c3 d + \u03c3 g \u2265 D(1/p d + 1/p g \u2212 1)", "formula_coordinates": [8.0, 108.0, 289.95, 397.24, 20.56]}, {"formula_id": "formula_35", "formula_text": "\u03c3 d +\u03c3 g \u2265 D(1/p d +1/p g ) is needed.", "formula_coordinates": [8.0, 108.0, 334.97, 146.9, 9.65]}, {"formula_id": "formula_36", "formula_text": "M B \u03c3 d \u221e,q d , B \u03c3g pg,qg n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D + n \u22121/2 ,", "formula_coordinates": [8.0, 220.03, 480.92, 171.94, 17.54]}, {"formula_id": "formula_37", "formula_text": "M B \u03c3 d 1,q d , B \u03c3g pg,qg n \u2212 \u03c3g +\u03c3 d +D(1\u22121/p d \u22121/pg ) 2\u03c3g +D(1\u22122/pg ) + n \u22121/2 ;", "formula_coordinates": [8.0, 192.32, 583.74, 227.36, 17.84]}, {"formula_id": "formula_38", "formula_text": "M lin B \u03c3 d p d ,q d , B \u03c3g pg,qg n \u2212 \u03c3 g +\u03c3 d 2\u03c3 g +D .(10)", "formula_coordinates": [9.0, 234.92, 73.38, 269.08, 20.13]}, {"formula_id": "formula_39", "formula_text": "/p g + 1/p d ), \u03c3 g := \u03c3 g \u2212 D(1/p g + 1/p d ) is largest possible value such that the embedding B \u03c3g pg,pg \u2286 B \u03c3 g p d ,p d holds.", "formula_coordinates": [9.0, 108.0, 120.63, 396.0, 26.97]}, {"formula_id": "formula_40", "formula_text": "D ) \u2208 N D satisfying |\u03b1| \u2264 r, for some constant C \u03b1,m , |\u2202 \u03b1 \u03c6(x)| \u2264 C \u03b1,m (1 + |x|) \u2212m . Here, \u2202 \u03b1 = (\u2202/\u2202x 1 ) \u03b11 \u2022 \u2022 \u2022 (\u2202/\u2202x D ) \u03b1 D is the mixed derivative of index \u03b1, |\u03b1| = D j=1", "formula_coordinates": [9.0, 106.83, 455.49, 397.17, 37.25]}, {"formula_id": "formula_41", "formula_text": "{\u03b1 k } \u2208 l p , f (x) = a k 2 Dj/2 \u03c8 (2 j x \u2212 k), \u2208 E, k \u2208 Z D , C f p \u2264 2 Dj(1/2\u22121/p) |a k | p 1/p \u2264 C f p .", "formula_coordinates": [9.0, 201.53, 605.87, 282.77, 38.04]}, {"formula_id": "formula_42", "formula_text": "\u03b1 p k = \u03c6 k (x)p(x)dx \u03b2 p \u03bb = \u03c8 \u03bb (x)p(x)dx", "formula_coordinates": [10.0, 260.66, 118.48, 90.67, 40.03]}, {"formula_id": "formula_43", "formula_text": "F d = B \u03c3 d p d ,q d d F d (p, q) = sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 q k ) + j\u22650 \u03bb\u2208\u039b \u03b2 f \u03bb (\u03b2 p \u03bb \u2212 \u03b2 q \u03bb )", "formula_coordinates": [10.0, 180.53, 199.49, 304.91, 53.64]}, {"formula_id": "formula_44", "formula_text": "\u2208 F d f = k\u2208Z \u03b1 f k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03c8 \u03bb", "formula_coordinates": [10.0, 157.31, 262.69, 213.16, 33.63]}, {"formula_id": "formula_45", "formula_text": "d F d (p, q) = sup f \u2208F d |E X\u223cp [f (X)] \u2212 E X\u223cq [f (X)]| = sup f \u2208F d X f (x)p(x)dx \u2212 f (x)q(x)dx = sup f \u2208F d X \uf8eb \uf8ed k\u2208Z \u03b1 f k \u03c6 k (x) + j\u22650 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03c8 \u03bb (x) \uf8f6 \uf8f8 (p(x) \u2212 q(x)) dx", "formula_coordinates": [10.0, 146.33, 339.61, 316.02, 84.57]}, {"formula_id": "formula_46", "formula_text": "d F d (P, Q) = sup f \u2208F d k\u2208Z X \u03b1 f k \u03c6 k (dP (x) \u2212 dQ(x)) + j\u22650 \u03bb\u2208\u039bj X \u03b2 f \u03bb \u03c8 \u03bb (dP (x) \u2212 dQ(x)) = sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 q k ) + j\u22650 \u03bb\u2208\u039b \u03b2 f \u03bb (\u03b2 p \u03bb \u2212 \u03b2 q \u03bb )", "formula_coordinates": [10.0, 119.83, 472.72, 369.02, 62.74]}, {"formula_id": "formula_47", "formula_text": "E[Yi] = 0, E[Y 2 i ] \u2264 \u03c3 2 .", "formula_coordinates": [10.0, 127.66, 641.23, 95.46, 12.32]}, {"formula_id": "formula_48", "formula_text": "E 1 n n i=1 Y i m \u2264 c m \u03c3 m n m/2 + E |Y 1 | m n m\u22121 for 2 < m < \u221e, E 1 n n i=1 Y i m \u2264 \u03c3 m n \u2212m/2 for 1 \u2264 m \u2264 2. Lemma 15. (Bernstein's Inequality ([6])) If Y 1 , . . . , Y n are IID random variables such that E[Yi] = 0, E[Y 2 i ] = \u03c3 2 and |Y i | \u2264 Y \u221e < \u221e, then Pr 1 n n i=1 Y i > \u03bb \u2264 2 exp \u2212 n\u03bb 2 2(\u03c3 2 + Y \u221e \u03bb/3)", "formula_coordinates": [10.0, 157.28, 657.78, 297.45, 67.55]}, {"formula_id": "formula_49", "formula_text": "F d = {f : f \u03c3 d p d ,q d \u2264 L d } F g = {p : p \u03c3g pg,qg \u2264 L g } \u2229 P P = {p : p \u2265 0, p L 1 = 1, supp(p) \u2286 [\u2212T, T ]}, we decompose f \u2208 F d as f = k\u2208Z \u03b1 k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb .", "formula_coordinates": [11.0, 107.64, 189.62, 299.75, 95.86]}, {"formula_id": "formula_50", "formula_text": "p = k\u2208Z \u03b1 p k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 p \u03bb \u03c8 \u03bb p n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb", "formula_coordinates": [11.0, 239.18, 329.86, 133.14, 60.35]}, {"formula_id": "formula_51", "formula_text": "\u03b1 p k = E X\u223cp [\u03c6 k (X)] \u03b2 p \u03bb = E X\u223cp [\u03c8 \u03bb (X)] \u03b1 k = 1 n n i=1 \u03c6 k (X i ) \u03b2 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i )", "formula_coordinates": [11.0, 210.48, 415.21, 192.71, 63.51]}, {"formula_id": "formula_52", "formula_text": "d F d (p, p n ) \u2264 sup f \u2208F d k\u2208Z \u03b1 k (\u03b1 p k \u2212 \u03b1 k ) + sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb + sup f \u2208F d j\u2265j1 \u03bb\u2208\u039bj \u03b2 \u03bb \u03b2 p \u03bb", "formula_coordinates": [11.0, 140.95, 514.9, 323.66, 60.41]}, {"formula_id": "formula_53", "formula_text": "E X1,...,Xn sup f \u2208F d n2 j=n1 \u03bb\u2208\u039bj \u03b3 \u03bb \u03b7 \u03bb \u2264 L D n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed E X1,...,Xn \u03bb\u2208\u039bj |\u03b7 \u03bb | p d \uf8f6 \uf8f8 1/p d", "formula_coordinates": [11.0, 125.36, 662.07, 360.28, 36.06]}, {"formula_id": "formula_54", "formula_text": "E X1,...,Xn sup f \u2208F d n2 j=n1 \u03bb\u2208\u039bj \u03b3 \u03bb \u03b7 \u03bb \u2264 E X1,...,Xn sup f \u2208F d n2 j=n1 \u03b3 p d \u03b7 p d \u2264 E X1,...,Xn sup f \u2208F d \uf8eb \uf8ed n2 j=n1 2 j(\u03c3 d +D/2\u2212D/p d ) \u03b3 p d q d \uf8f6 \uf8f8 1/q d \u00d7 n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b7 p d (l 1 \u2286 l q d ) \u2264 L D n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) E X1,...,Xn \u03b7 p d \u2264 L D n2 j=n1 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed E X1,...,Xn \u03bb\u2208\u039bj |\u03b7 \u03bb | p d \uf8f6 \uf8f8 1/p d where p d is the conjugate of p d i.e. 1 p d + 1 p d = 1", "formula_coordinates": [12.0, 107.64, 91.67, 378.0, 210.9]}, {"formula_id": "formula_55", "formula_text": "Lemma 17. Let f \u2208 B \u03c3g pg,qg where \u03c3 g > D/p g then f \u221e \u2264 4A \u03c8 \u221e L g (1 \u2212 2 (\u03c3g\u2212D/pg)q g ) \u22121/q g", "formula_coordinates": [12.0, 108.0, 323.63, 292.42, 35.83]}, {"formula_id": "formula_56", "formula_text": "f \u221e \u2264 2A \u03c8 \u221e \uf8eb \uf8ed {\u03b1 k } k\u2208Z D \u221e + j\u22650 2 Dj/2 {\u03b2 \u03bb } \u03bb\u2208\u039bj \u221e ) \uf8f6 \uf8f8 .", "formula_coordinates": [12.0, 171.43, 422.62, 274.12, 33.7]}, {"formula_id": "formula_57", "formula_text": "j\u22650 2 Dj/2 {\u03b2 \u03bb } \u03bb\u2208\u039bj \u221e \u2264 j\u22650 1 2 j(\u03c3g\u2212D/pg) \u00d7 2 j(\u03c3g+D/2\u2212D/pg) {\u03b2 \u03bb } \u03bb\u2208\u039bj \u221e \u2264 \uf8eb \uf8ed j\u22650 1 2 j(\u03c3g\u2212D/pg)q g \uf8f6 \uf8f8 1/q g \uf8eb \uf8ed j\u22650 2 jqg(\u03c3g+D/2\u2212D/pg) {\u03b2 \u03bb } \u03bb\u2208\u039bj qg \u221e \uf8f6 \uf8f8 1/qg \u2264 1 1 \u2212 2 \u2212(\u03c3g\u2212D/pg)q g 1/q g \uf8eb \uf8ed j\u22650 2 jqg(\u03c3g+D/2\u2212D/pg) {\u03b2 \u03bb } \u03bb\u2208\u039bj qg pg \uf8f6 \uf8f8 1/qg \u2264 1 \u2212 2 \u2212(\u03c3g\u2212D/pg)q g \u22121/q g f \u03c3g pgqg \u2264 1 \u2212 2 \u2212(\u03c3g\u2212D/pg)q g \u22121/q g L g .", "formula_coordinates": [12.0, 108.25, 489.11, 415.88, 160.43]}, {"formula_id": "formula_58", "formula_text": "Lemma 18. (Moment Bounds) Let X 1 , . . . , X n \u223c p, m \u2265 1 s.t. there is a constant c with Ep |\u03c8 \u03bb (X)| m \u2264 c2 Dj(m/2\u22121) . Let \u03b3 p \u03bb = E[\u03c8\u03bb(X)], \u03b3 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ),", "formula_coordinates": [13.0, 108.0, 75.09, 396.0, 70.22]}, {"formula_id": "formula_59", "formula_text": "2 Dj \u2208 O(n), E[| \u03b3 jk \u2212 \u03b3 jk | m ] \u2264 cn \u2212m/2 .", "formula_coordinates": [13.0, 178.86, 150.66, 183.72, 29.35]}, {"formula_id": "formula_60", "formula_text": "Y i = \u03c8 \u03bb (X i ) \u2212 E[\u03c8\u03bb(X)]", "formula_coordinates": [13.0, 252.87, 228.32, 106.27, 10.24]}, {"formula_id": "formula_61", "formula_text": "E[|Yi| m ] \u2264 E[(|\u03c8\u03bb(Xi)| + | E[\u03c8\u03bb(Xi)]|) m ] (triangle inequality) \u2264 2 m\u22121 (E[|\u03c8 \u03bb (X i )| m ] + | E[\u03c8\u03bb(Xi)]| m ) (Jensen's) \u2264 2 m E[|\u03c8\u03bb(Xi)| m ]. (Jensen's)", "formula_coordinates": [13.0, 144.77, 256.59, 322.46, 42.97]}, {"formula_id": "formula_62", "formula_text": "E[|\u03b3 p \u03bb \u2212 \u03b3 \u03bb | m ] \u2264 c m E p |\u03c8 \u03bb (X)| 2 m/2 + c 2 Dj n (m/2\u22121)+ n \u2212m/2", "formula_coordinates": [13.0, 157.91, 320.07, 295.68, 26.19]}, {"formula_id": "formula_63", "formula_text": "E[|\u03b3 p \u03bb \u2212 \u03b3 \u03bb | m ] \u2264 c m E p |\u03c8 \u03bb (X)| 2 m/2 n \u2212m/2", "formula_coordinates": [13.0, 210.19, 370.74, 191.12, 25.02]}, {"formula_id": "formula_64", "formula_text": "\u03b3 p \u03bb = E[\u03c8\u03bb(X)], \u03b3 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ),", "formula_coordinates": [13.0, 264.23, 463.48, 83.55, 46.37]}, {"formula_id": "formula_65", "formula_text": "Pr(| \u03b3 \u03bb \u2212 \u03b3 \u03bb | > (K/2)l) \u2264 2 \u00d7 2 \u2212\u03b3nl 2", "formula_coordinates": [13.0, 227.09, 533.08, 156.83, 13.35]}, {"formula_id": "formula_66", "formula_text": "K 2 8(c + \u03c8 \u221e (K/3)) > log 2\u03b3", "formula_coordinates": [13.0, 243.79, 566.68, 125.07, 26.29]}, {"formula_id": "formula_67", "formula_text": "Pr(| \u03b3 \u03bb \u2212 \u03b3 \u03bb | > (K/2)l) \u2264 2 exp \u2212 n(K/2) 2 l 2 2(c + 2 Dj/2 \u03c8 \u221e (K/3)l) \u2264 2 exp \u2212 K 2 nl 2 8(L g + \u03c8 \u221e (K/3))", "formula_coordinates": [13.0, 167.99, 619.19, 267.48, 55.13]}, {"formula_id": "formula_68", "formula_text": "Pr(| \u03b3 \u03bb \u2212 \u03b3 \u03bb | > (K/2)l) \u2264 2 \u00d7 2 (\u2212\u03b3nl 2 )", "formula_coordinates": [13.0, 223.21, 693.68, 165.07, 13.55]}, {"formula_id": "formula_69", "formula_text": "p n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb is bounded by d F d ( p n , E[ p n ]) \u2264 c 1 \u221a n + 2 j0(D/2\u2212\u03c3 d ) \u221a n where c = c p d Ep |\u03c8 \u03bb (X)| 2 1/2 is a constant. Proof. Since F d = B \u03c3 d p d ,", "formula_coordinates": [14.0, 108.0, 136.49, 276.3, 114.54]}, {"formula_id": "formula_70", "formula_text": "E X1,...,Xn sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 k ) + E X1,...,Xn sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb Since, for a constant c, Ep |\u03c8 \u03bb (X)| m \u2264 c2 Dj(m/2\u22121)", "formula_coordinates": [14.0, 108.0, 255.66, 345.61, 54.83]}, {"formula_id": "formula_71", "formula_text": "E X1,...,Xn sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 k ) \u2264 L D k E X1,...,Xn |\u03b1 p k \u2212 \u03b1 k | p d 1/p d (finitely many terms) \u2264 cL D p \u221e (T + A)n \u2212p d /2 1/p d (moment bound)", "formula_coordinates": [14.0, 131.42, 325.46, 349.17, 83.86]}, {"formula_id": "formula_72", "formula_text": "E X1,...,Xn sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb \u2264 c p \u221e L D j0 j=0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 Dj (T + A)n \u2212p d /2 1/p d \u2264 L D j0 j=0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 Dj/p d n \u22121/2 \u2264 cL D p \u221e j0 j=0 2 j(D/2\u2212\u03c3 d ) n \u22121/2 \u2264 c p \u221e 2 j0(D/2\u2212\u03c3 d ) n \u22121/2 \u03c3 d \u2264 D/2 n \u22121/2 \u03c3 d > D/2", "formula_coordinates": [14.0, 143.4, 468.4, 324.2, 171.6]}, {"formula_id": "formula_73", "formula_text": "\u03c3 g \u2265 D/p g , F d = B \u03c3 d p d ,q d .", "formula_coordinates": [14.0, 108.0, 662.96, 397.74, 22.2]}, {"formula_id": "formula_74", "formula_text": "d F d (p, E p [ p n ]) \u2264 c2 \u2212j0(\u03c3 d +\u03c3g\u2212(D/pg\u2212D/p d )+)", "formula_coordinates": [14.0, 213.87, 689.94, 183.77, 17.15]}, {"formula_id": "formula_75", "formula_text": "sup \u03b2\u2208F d j\u2265j1 \u03bb\u2208\u039b \u03b2 f \u03bb \u03b2 p \u03bb", "formula_coordinates": [15.0, 266.02, 90.67, 79.46, 22.89]}, {"formula_id": "formula_76", "formula_text": "\u03c3 g \u2265 D/p g sup \u03b2\u2208F d j\u2265j0 \u03bb\u2208\u039b \u03b2 f \u03bb \u03b2 p \u03bb \u2264 L D j\u2265j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p p d = L D j\u2265j0 2 j(\u03c3g+D/2\u2212D/pg) 2 j(\u03c3 d +\u03c3g+D\u2212D/p d \u2212D/pg) 2 j(D/p d \u2212D/pg)+ \u03b2 p pg \u2264 L D j\u2265j0 2 j(D/p d \u2212D/pg)+ 2 j(\u03c3 d +\u03c3g+D/p d \u2212D/pg) sup j\u2265j0 2 j(\u03c3g+D/2\u2212D/pg) \u03b2 p pg \u2264 2 \u2212j0(\u03c3 d +\u03c3g\u2212(D/pg\u2212D/p d )+) p \u03c3g pgqg (\u03c3 g \u2265 D/p g ) \u2264 c2 \u2212j0(\u03c3 d +\u03c3g\u2212(D/pg\u2212D/p d )+)", "formula_coordinates": [15.0, 127.87, 120.81, 356.26, 171.61]}, {"formula_id": "formula_77", "formula_text": "c n \u22121/2 + n \u22121/2 2 j0(D/2\u2212\u03c3 d ) + 2 \u2212j0(\u03c3g+\u03c3 d \u2212D/pg+D\u2212D/p d )", "formula_coordinates": [15.0, 179.66, 365.53, 246.23, 10.81]}, {"formula_id": "formula_78", "formula_text": "2 j0 = n 1/(2\u03c3g+D+2D/p d \u22122D/pg)", "formula_coordinates": [15.0, 240.61, 400.03, 130.28, 10.81]}, {"formula_id": "formula_79", "formula_text": "n \u22121/2 + n \u2212 \u03c3g +\u03c3 d \u2212D/pg +D\u2212D/p d 2\u03c3g +D+2D/p d \u22122D/pg", "formula_coordinates": [15.0, 246.43, 432.6, 127.47, 16.58]}, {"formula_id": "formula_80", "formula_text": "s := inf p,p \u2208T \u03c1(p, p ) > 0 , sup p\u2208T D KL (p, p 0 ) \u2264 log |T | 16 ,", "formula_coordinates": [15.0, 188.52, 574.0, 234.96, 23.26]}, {"formula_id": "formula_81", "formula_text": "D KL : P \u00d7 P \u2192 [0, \u221e] denotes Kullback-Leibler divergence. Then, inf p sup p\u2208P E [\u03c1(p, p)] \u2265 s 16", "formula_coordinates": [15.0, 134.47, 604.57, 274.24, 35.68]}, {"formula_id": "formula_82", "formula_text": "|Kj | : |\u03c4 \u03bb | \u2264 1} i.e. \u2126 g := {g 0 + c g \u03bb \u03c4 \u03bb \u03c8 \u03bb : \u03c4 \u2208 \u03b6, \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j }.", "formula_coordinates": [16.0, 108.0, 264.29, 396.0, 43.53]}, {"formula_id": "formula_83", "formula_text": "[\u2212A, A] D . Specifically if c g s.t. c g \u2264 c 2 \u03c8 \u221e 2 \u2212Dj/2 then g 0 + c g \u03bb \u03c4 \u03bb \u03c8 \u03bb = 1 (since \u03c8 \u03bb = 0) and, g 0 \u2212 p \u221e = c g 2 Dj/2 \u03c8 \u221e \u2264 c/2", "formula_coordinates": [16.0, 108.0, 326.99, 270.74, 72.02]}, {"formula_id": "formula_84", "formula_text": "D KL (g n , g n 0 ) \u2264 cn g 0 \u2212 g 2 L 2", "formula_coordinates": [16.0, 243.73, 493.15, 123.54, 12.91]}, {"formula_id": "formula_85", "formula_text": "g 0 \u2212 g g \u2265 \u2212 1 2 so using the fact that \u2212 log(1 + x) \u2264 x 2 \u2212 x for all x \u2265 \u22121/2 we get D KL (g n , g n 0 ) = nD KL (g, g 0 ) = n S g(x) log g(x) g 0 (x) dx = \u2212n S g(x) log 1 + g 0 (x) \u2212 g(x) g(x) dx \u2264 n S g(x) g 0 (x) \u2212 g(x) g(x) 2 \u2212 g 0 (x) \u2212 g(x) g(x) dx = n S (g 0 (x) \u2212 g(x)) 2 g(x) dx", "formula_coordinates": [16.0, 108.0, 532.1, 338.8, 174.19]}, {"formula_id": "formula_86", "formula_text": "KL(p \u03c4 , g 0 ) \u2264 nc 2 g c \u03bb \u03c4 \u03bb \u03c8 \u03bb 2 L 2 = cnc 2 g \u03c4 2 2", "formula_coordinates": [17.0, 209.06, 93.51, 193.39, 34.56]}, {"formula_id": "formula_87", "formula_text": "\u2126 d := {c d \u03bb \u03c4 \u03bb \u03c8 \u03bb : \u03c4 \u2208 \u03b6, \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j }", "formula_coordinates": [17.0, 184.82, 171.92, 242.36, 22.21]}, {"formula_id": "formula_88", "formula_text": "\u03b6 = {\u03c4 : \u03c4 \u03bb = 1, \u03c4 \u03bb = 0, \u03bb = \u03bb = 2 \u2212j k + 2 \u2212(j+1) 1 , k \u2208 K j }", "formula_coordinates": [17.0, 176.79, 289.06, 258.43, 11.72]}, {"formula_id": "formula_89", "formula_text": "g 0 + c g \u03c8 \u03bb \u03c3g pgqg \u2264 g 0 \u03c3g pgqg + 2 j(\u03c3g+D/2\u2212D/pg) c g \u2264 L g so that \u2126 g \u2286 F g . Since \u03c3 g \u2265 D/p g the choice of c g = c2 \u2212j(\u03c3g+D/2\u2212D/pg) suffices. Similarly, c d = L d 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) implies \u2126 d \u2286 F d .", "formula_coordinates": [17.0, 108.0, 342.9, 397.24, 48.43]}, {"formula_id": "formula_90", "formula_text": "d F d (p \u03bb , p \u03bb ) \u2265 sup f \u2208\u2126 d c g (f (x)(\u03c8 \u03bb \u2212 \u03c8 \u03bb )dx = \u03c8 2 L 2 c g c d (since, \u03c8 \u03bb 2 L 2 = \u03c8 2 L 2 ). So, if 2 j = (n/ log n) 1 2\u03c3g +D\u22122D/pg we have, M (F g , F d ) log n n \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg", "formula_coordinates": [17.0, 107.67, 491.49, 324.17, 81.61]}, {"formula_id": "formula_91", "formula_text": "g 0 + c g \u03bb \u03c4 \u03bb \u03c8 \u03bb \u03c3g pgqg \u2264 g 0 \u03c3g pgqg + 2 j(\u03c3g+D/2) c g \u2264 L g", "formula_coordinates": [17.0, 193.05, 658.6, 230.69, 34.57]}, {"formula_id": "formula_92", "formula_text": "c d = L d 2 \u2212j(\u03c3 d +D/2) implies \u2126 d \u2286 F d .", "formula_coordinates": [17.0, 108.0, 700.71, 396.44, 22.13]}, {"formula_id": "formula_93", "formula_text": "d F d (p \u03bb , p \u03bb ) = sup f \u2208\u2126 d c g (f (x)(\u03c8 \u03bb \u2212 \u03c8 \u03bb )dx = c g c d \u03c9(\u03c4 \u03bb , \u03c4 \u03bb ) \u2265 c g c d 2 Dj 4", "formula_coordinates": [18.0, 209.42, 113.53, 189.85, 45.47]}, {"formula_id": "formula_94", "formula_text": "p \u03c4 \u2208 \u2126 g , D KL (p n \u03c4 , g n 0 ) \u2264 cnc 2 g \u03c4 2 = cnc 2 g |K j | to be at most log |\u03b6| 16 = |Kj |", "formula_coordinates": [18.0, 108.0, 192.9, 306.3, 14.61]}, {"formula_id": "formula_95", "formula_text": "n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D", "formula_coordinates": [18.0, 287.23, 227.53, 35.67, 14.99]}, {"formula_id": "formula_96", "formula_text": "\u2212 \u03c3g +\u03c3 d 2\u03c3g +D , n \u2212 \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg )", "formula_coordinates": [18.0, 259.98, 271.13, 132.61, 15.09]}, {"formula_id": "formula_97", "formula_text": "F d = {f : f \u03c3 d p d ,q d \u2264 L d } F g = {p : p \u03c3g pg,qg \u2264 L g } \u2229 P P = {p : p \u2265 0, p L 1 = 1, supp(p) \u2286 [\u2212T, T ]}", "formula_coordinates": [18.0, 205.99, 502.07, 200.02, 47.03]}, {"formula_id": "formula_98", "formula_text": "p = k\u2208Z \u03b1 p k \u03c6 k + j\u22650 \u03bb\u2208\u039bj \u03b2 p \u03bb \u03c8 \u03bb p n = k\u2208Z \u03b1 k \u03c6 k + j0 j=0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb + j1 j=j0 \u03bb\u2208\u039bj \u03b2 \u03bb \u03c8 \u03bb where \u03b1 p k = E X\u223cp [\u03c6 k (X)] \u03b2 p \u03bb = E X\u223cp [\u03c8 \u03bb (X)] \u03b1 k = 1 n n i=1 \u03c6 k (X i ) \u03b2 \u03bb = 1 n n i=1 \u03c8 \u03bb (X i ) \u03b2 \u03bb = \u03b2 \u03bb 1 { \u03b2 \u03bb >t}", "formula_coordinates": [18.0, 107.64, 582.05, 329.79, 143.28]}, {"formula_id": "formula_99", "formula_text": "2 j0 = n 1 2\u03c3g +D 2 j1 = n 1 2\u03c3g +D\u22122D/pg", "formula_coordinates": [19.0, 264.37, 91.54, 80.89, 32.12]}, {"formula_id": "formula_100", "formula_text": "d F d (p, p n ) \u2264 sup f \u2208F d k\u2208Z \u03b1 f k (\u03b1 p k \u2212 \u03b1 k ) + sup f \u2208F d j0 j=0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb + sup f \u2208F d j1 j\u2265j0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb + sup f \u2208F d j\u2265j1 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb", "formula_coordinates": [19.0, 131.8, 148.06, 341.96, 68.49]}, {"formula_id": "formula_101", "formula_text": "E p |\u03c8 \u03bb (X)| p d \u2264 2 \u2212Dj(p d /2\u22121)", "formula_coordinates": [19.0, 264.28, 298.3, 118.81, 17.15]}, {"formula_id": "formula_102", "formula_text": "cn \u22121/2 (2 j0(D/2\u2212\u03c3 d ) + 1) n \u2212 \u03c3g +\u03c3 d 2\u03c3g +D + n \u22121/2 2.", "formula_coordinates": [19.0, 131.41, 338.7, 287.21, 37.31]}, {"formula_id": "formula_103", "formula_text": "c2 \u2212j1(\u03c3 d +\u03c3g\u2212D/pg+D/p d ) n \u2212 \u03c3g +\u03c3 d +D\u2212D/pg \u2212D/p d 2\u03c3g +D\u22122D/pg", "formula_coordinates": [19.0, 219.09, 394.66, 207.5, 15.09]}, {"formula_id": "formula_104", "formula_text": "E sup f \u2208F d j\u2265j0 \u03bb\u2208\u039b \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed E \u03bb\u2208\u039bj |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1 A \uf8f6 \uf8f8 1/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed \u03bb\u2208\u039bj E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1 A \uf8f6 \uf8f8 1/p d", "formula_coordinates": [19.0, 143.87, 439.8, 372.77, 79.22]}, {"formula_id": "formula_105", "formula_text": "L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed \u03bb\u2208\u039bj E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1 A \uf8f6 \uf8f8 1/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \uf8eb \uf8ed \u03bb\u2208\u039bj (E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d r ) 1/r Pr(A) 1/r \uf8f6 \uf8f8 1/p d", "formula_coordinates": [19.0, 166.14, 623.78, 334.51, 79.22]}, {"formula_id": "formula_106", "formula_text": "Pr(A) \u2264 Pr | \u03b2 \u03bb \u2212 \u03b2 p \u03bb | \u2265 t/2 \u2264 c2 \u2212\u03b3j we get, \u2264 c j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 Dj n \u2212p d /2 2 \u2212j\u03b3/r 1/p d \u2264 c j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d \u2212D/p d ) n \u22121/2 2 \u2212\u03b3j/p d r \u2264 cn \u22121/2 j1 j=j0 2 \u2212j(\u03c3 d \u2212D/2+\u03b3/p d r ) \u2264 cn \u22121/2 2 \u2212j0(\u03c3 d \u2212D/2+\u03b3/p d r ) n \u2212 \u03c3g +\u03c3 d +\u03b3/p d r 2\u03c3g +D", "formula_coordinates": [20.0, 163.43, 120.82, 279.79, 237.24]}, {"formula_id": "formula_107", "formula_text": "E sup f \u2208F d j1 j=j0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb 1 B \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p \u03bb p d (Pr(B)) 1/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p \u03bb p d 2 \u2212\u03b3j/p d \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +\u03c3 g +\u03b3/p d ) sup j0\u2264j\u2264j1 2 j(\u03c3 g +D/2\u2212D/p d ) \u03b2 \u03bb p d \u2264 L D L G j1 j=j0 2 \u2212j(\u03c3 d +\u03c3 g +\u03b3/p d ) \u2264 L D L G C2 \u2212j0(\u03c3 d +\u03c3 g +\u03b3/p d ) n \u2212 \u03c3 d +\u03c3 g +\u03b3 2\u03c3g +D", "formula_coordinates": [20.0, 163.79, 467.85, 354.07, 181.0]}, {"formula_id": "formula_108", "formula_text": "E sup f \u2208F d j1 j=j0 \u03bb\u2208\u039bj \u03b2 f \u03bb \u03b2 p \u03bb \u2212 \u03b2 \u03bb 1 C \u2264 L D j1 j=j0 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) k\u2208C E |\u03b2 p \u03bb \u2212 \u03b2 \u03bb | p d 1/p d \u2264 L D j1 j=j0 Cn \u22121/2 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) k\u2208C 2\u03b2 p \u03bb n/j K pg 1/p d \u2264 L D j1 j=j0 Cn \u22121/2 ( n/j) pg/p d 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) \u03b2 p pg/p d pg \u2264 L D j1 j=j0 Cn \u22121/2 ( n/j) pg/p d 2 \u2212j(\u03c3 d +D/2\u2212D/p d ) 2 \u2212j(\u03c3g+D/2\u2212D/pg)pg/p d sup j0\u2264j\u2264j1 \u03b2 pg 2 j(\u03c3g+D/2\u2212D/pg) \u2264 CL D L G n 1/2(pg/p d \u22121) j1 j=j0 2 \u2212j((\u03c3g+D/2)pg/p d +\u03c3 d \u2212D/2) j \u2212pg/2p d \u2264 CL D L G n 1/2(pg/p d \u22121) 2 \u2212jm((\u03c3g+D/2)pg/p d +\u03c3 d \u2212D/2) where j m = j 0 (2\u03c3 g + D)p g \u2265 (D \u2212 2\u03c3 d )p d j 1 (2\u03c3 g + D)p g \u2264 (D \u2212 2\u03c3 d )p d In", "formula_coordinates": [21.0, 163.43, 86.16, 354.84, 310.31]}, {"formula_id": "formula_109", "formula_text": "\u2126 d := {c d \u03bb \u03c4 \u03bb \u03c8 \u03bb : \u03bb = 2 \u2212j k + 2 \u2212j\u22121 1 , k \u2208 K j , \u03c4 \u2264 L d } s.t. c d \u2264 L d 2 \u2212j(\u03c3 d +D/2\u22121/p d )", "formula_coordinates": [22.0, 108.0, 243.04, 327.88, 52.68]}, {"formula_id": "formula_110", "formula_text": "\uf8eb \uf8ed \u03bb =\u03bb ( E g0+cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0+cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d \uf8f6 \uf8f8 1/p d \u2265 c d \uf8eb \uf8ed 1 2 Dj \u03bb =\u03bb ( E g0+cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb | \u03b1 \u03bb |) p d + ( E g0+cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d + ( E g0\u2212cg\u03c8 \u03bb |c g \u2212 \u03b1 \u03bb |) p d \uf8f6 \uf8f8 1/p d", "formula_coordinates": [22.0, 131.05, 568.86, 445.74, 79.22]}], "doi": ""}