{"title": "Monarch: Expressive Structured Matrices for Efficient and Accurate Training", "authors": "Tri Dao; Beidi Chen; Nimit Sohoni; Arjun Desai; Michael Poli; Jessica Grogan; Alexander Liu; Aniruddh Rao; Atri Rudra; Christopher R\u00e9", "pub_date": "2022-04-04", "abstract": "Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute/memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency-quality tradeoffs, and (2) in denseto-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2\u00d7 with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called \"reverse sparsification,\" Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2\u00d7 without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7\u00d7 with comparable accuracy.", "sections": [{"heading": "Introduction", "text": "Large neural networks excel in many domains, but their training and fine-tuning demand extensive computation and memory [54]. A natural approach to mitigate this cost is to replace dense weight matrices with structured ones, such as sparse & low-rank matrices and the Fourier transform. However, structured matrices (which can be viewed as a general form of sparsity) have not yet seen wide adoption to date, due to two main challenges. (1) In the end-to-end (E2E) training setting, they have shown unfavorable efficiency-quality tradeoffs. Model efficiency refers how efficient these structured matrices are on modern hardware (e.g., GPUs). Model quality (performance on tasks) is determined by how expressive they are (e.g., can they represent commonly used transforms such as convolution or Fourier/cosine transforms that encode domain-specific knowledge). Existing structured matrices are either not hardware-efficient, or not expressive enough. (2) In the setting of dense-to-sparse (D2S) fine-tuning of pretrained models, a long-standing problem for most classes of structured matrices is the lack of tractable algorithms to approximate dense pretrained weight matrices [79]. Figure 1: Monarch matrices unlock several ways to train sparse and dense models: end-to-end training a sparse (Monarch) model can be 2x faster than dense training thanks to its hardware efficiency; sparse-to-dense \"reverse sparsification\" can speed up training of large models such as GPT-2; and our dense-to-sparse Monarch projection algorithm can transfer knowledge from pretrained dense model to Monarch model and speed up BERT fine-tuning.\nSparse matrices have seen advances in training deep learning models (e.g., pruning [44], lottery tickets [30]), but most work on (entrywise) sparsification focuses on reducing training or inference FLOPs, which do not necessarily map to E2E training time on modern hardware (e.g., GPUs). In fact, most sparse training methods slow down training in wall-clock time [33,48]. Moreover, sparse matrices are not able to represent commonly used transforms such as convolution and the Fourier transform. Another class of structured matrices, such as Fourier, sine/cosine, Chebyshev, are used in specialized domains such as PDE solving [100] and medical imaging [49]. However, they are difficult to use in E2E training since only specific instances of these structured matrices have fast GPU implementations (e.g., FFT). Moreover, their applications requires domain expertise to hand-pick the right transforms. Generalizations of these transforms (e.g., Toeplitz-like [95], orthogonal polynomial transforms [25], low-displacement rank [53], quasi-separable [27]), though learnable, often lack efficient implementation on GPUs [98] for E2E training as well. In addition, they have no known tractable algorithm to approximate a given dense matrix [79], making them difficult to use in D2S fine-tuning.\nE2E training. The technical challenge in addressing the efficiency-quality tradeoff of structured matrices is to find a parameterization that is both efficient on block-oriented hardware (e.g., GPUs) and expressive (e.g., can represent many commonly used transforms). We propose a class of matrices called Monarch, 1 parameterized as products of two block-diagonal matrices (up to permutation), to address this challenge. This parameterization leverages optimized batch-matrix-multiply (BMM) routines on GPUs, yielding up to 2\u00d7 speedup compared to dense matrix multiply (Section 5.1.1). We show that the class of Monarch matrices contains the class of butterfly matrices [80,12], which can represent any low-depth arithmetic circuits in near optimal runtime and parameter size [13]. Monarch matrices inherit this expressiveness and thus can represent many fast transforms (e.g., Fourier, sine/cosine/Chebyshev transforms, convolution) (Proposition 3.2).\nSparse-to-dense (S2D) training, aka \"reverse sparsification\". The hardware-efficiency and expressiveness of Monarch matrices unlock a new way to train dense models: training with Monarch weight matrices for most of the time and then transitioning to dense weight matrices (Fig. 3). This technique can be used in cases where sparse training faces representation or optimization difficulties [28] or a dense model is necessary. One such application is language modeling on large datasets, where a massive number of parameters are required [54] to memorize the textual patterns [35]. Monarch matrices can serve as a fast intermediate representation to speed up the training process of the dense model.\nD2S fine-tuning. While transitioning from sparse to dense matrices is easy, the reverse direction is challenging. The main technical difficulty is the projection problem: finding a matrix in a class of structured matrices that is the closest to a given dense matrix. Only a few specific classes of structured matrices have a tractable projection solution, such as entrywise sparse matrices (magnitude pruning [97]), low-rank matrices (the Eckart-Young theorem [26]), and orthogonal matrices (the orthogonal Procrustes problem [93]). For more expressive classes of structured matrices, projection remains a long-standing problem [79]. For example, De Sa et al. [16] show that all structured matrices (in the form of arithmetic circuits) can be written as products of sparse matrices, which can be represented as products of butterfly matrices [13]. There have been numerous heuristics proposed to project on the set of butterfly matrices or products of sparse matrices, based on iterative first-order optimization [63,12,55] or alternating minimization [67]. However, they lack theoretical guarantees. In contrast, we derive a projection algorithm for our Monarch parameterization and prove that it finds the optimal solution (Theorem 1). We also derive an algorithm to factorize matrices that are products of Monarch matrices (Section 3.4). These new algorithms allows us to easily finetune a pretrained model into a model with Monarch weight matrices (Section 5.3).\nWe validate our approach empirically in these three settings, showing that our Monarch matrix parameterization achieves a favorable efficiency-accuracy tradeoff compared to baselines on a wide range of domains: text, images, PDEs, MRI. \u2022 In the E2E sparse training setting (Section 5.1), our Monarch matrices model trains 2\u00d7 faster than dense models while achieving the same accuracy / perplexity on benchmark tasks (ViT on ImageNet classification, GPT-2 on Wikitext-103 language modeling). On scientific and medical tasks relying on hand-crafted fast transforms (PDE solving, MRI reconstruction), Monarch reduces the error by up to 40% at the same training speed compared to domain-specific Fourier-based methods. \u2022 In the S2D training setting (Section 5.2), our \"reverse sparsification\" process with Monarch matrices speeds up GPT-2 pretraining on the large OpenWebText dataset by 2\u00d7 compared to an optimized implementation from NVIDIA [94], with comparable upstream and downstream (text classification) quality. When applied to BERT pretraining, our method is 23% faster than the implementation from Nvidia that set the MLPerf [72] 1.1 record. \u2022 In the D2S fine-tuning setting (Section 5.3), we show a proof of concept that our Monarch projection algorithm speeds up BERT fine-tuning. We project a pretrained BERT model to a Monarch matrix model and fine-tune on GLUE, with 2\u00d7 fewer parameters, 1.7\u00d7 faster fine-tuning speed, and similar average GLUE accuracy as the dense model. 2 2 Related Work and Background", "publication_ref": ["b53", "b0", "b1", "b78", "b43", "b29", "b32", "b47", "b99", "b48", "b94", "b24", "b52", "b26", "b97", "b78", "b0", "b79", "b11", "b12", "b27", "b53", "b34", "b96", "b25", "b92", "b78", "b15", "b12", "b62", "b11", "b54", "b66", "b93", "b1"], "figure_ref": ["fig_4", "fig_1"], "table_ref": []}, {"heading": "Related Work", "text": "Sparse Training. Sparse training is an active research topic. There has been inspiring work along the line of compressing models such as neural network pruning and lottery tickets [44,45,30]. Pruning methods usually eliminate neurons and connections through iterative retraining [44,45,92] or at runtime [66,23].\nAlthough both Monarch and pruning methods aim to produce sparse models, we differ in our emphasis on overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost of finding the smaller model. Lottery tickets [30,31,32] are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization.\nMonarch can be roughly seen as a class of manually constructed lottery tickets. Structured Matrices. Structured matrices are those with subquadratic (o(n 2 ) for dimension n \u00d7 n) number of parameters and runtime. Examples include sparse and low-rank matrices, and fast transforms (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). They are commonly used to replace the dense weight matrices of deep learning models, thus reducing the number of parameters and training/inference FLOPs. Large classes of structured matrices (e.g., Toeplitz-like [95], low-displacement rank [53], quasiseparable [27]) have been shown to be able to represent many commonly used fast transforms. For example, De Sa et al. [16] show that a simple divide-and-conquer scheme leads to a fast algorithm for a large class of structured matrices. Our work builds on butterfly matrices [80,12], which have been shown to be expressive but remain hardware-inefficient. Pixelated butterfly [6] has attempted to make butterfly matrices more hardware-friendly, but at the cost of reduced expressiveness. Furthermore, it is not known if one can directly decompose a dense pretrained model to a model with butterfly weight matrices without retraining.", "publication_ref": ["b43", "b44", "b29", "b43", "b44", "b91", "b65", "b22", "b29", "b30", "b31", "b94", "b52", "b26", "b15", "b79", "b11", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Butterfly Matrices", "text": "Our work builds on recent work on butterfly matrices. Dao et al. [12] introduced the notion of a butterfly matrix as a certain product of permuted block-diagonal matrices, inspired by the Cooley-Tukey fast Fourier transform algorithm [11]. They encode the divide-and-conquer structure of many fast multiplication algorithms. Dao et al. [13] showed that all structured matrices can be written as products of such butterfly matrices, and this representation has optimal memory and runtime complexity up to polylogarithmic factors. We now review these definitions (following [13]).\nA butterfly factor of size k (where k is even) is a matrix of the form\nD 1 D 2 D 3 D 4\nwhere each D i is a k 2 \u00d7 k 2 diagonal matrix. We call this class of matrices BF (k,k) . A butterfly factor matrix of size n and block size k is a block diagonal matrix of n k butterfly factors of size k:\ndiag B 1 , B 2 , . . . , B n k ,\nwhere B i \u2208 BF (k,k) . We call this class of matrices BF (n,k) . Finally, a butterfly matrix of size n = 2 s is a matrix M that can be expressed as a product of butterfly factor matrices:\nM = B n B n/2 . . . B 2 ,\nwhere each B i \u2208 BF (n,i) . We denote the set of size-n butterfly matrices by B (n) . Equivalently, M can be written in the following form:\nM = B n M 1 0 0 M 2 ,\nwhere\nB n \u2208 BF (n,n) and M 1 , M 2 \u2208 B ( n 2 )\n. Dao et al. [13] further introduce the kaleidoscope matrix hierarchy: the class BB * (n) is the set of matrices of the form n) , and the class (BB * (n) ) w e is the set of all matrices of the form\nM 1 M * 2 for M 1 , M 2 \u2208 B (\nw i=1 M i [1:n, 1:n]\nwhere each M i \u2208 BB * (e\u2022n) . (A * denotes the conjugate transpose of A.) When the size n is clear from context, we will omit the superscript (n) (i.e., just write B, BB * , etc.). As shown by Theorem 1 of Dao et al. [13], the kaleidoscope hierarchy can represent any structured matrix with nearly-optimal parameters and runtime: if M is an n \u00d7 n matrix such that multiplying any vector v by M can be represented as a linear arithmetic circuit with depth d and s total gates, then M \u2208 (BB * (n) )\nO(d) O(s/n) .", "publication_ref": ["b11", "b10", "b12", "b12", "b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Monarch: Definition & Algorithms", "text": "In Section 3.1, we introduce Monarch matrices, and describe how they relate to butterfly matrices. In Section 3.2 we show that the class of Monarch matrices is at least as expressive as the class of butterfly matrices, while admitting a practically efficient representation. In particular, many fast transforms (e.g., Fourier, convolution) can be represented as a Monarch matrix or as the product of two or four Monarch matrices (Proposition 3.2). In Section 3.3, we show how to project onto the set of Monarch matrices. This allows us to tractably approximate a given matrix (e.g., a dense pretrained weight matrix) with a Monarch matrix, unlocking new applications (cf. Section 5). In Section 3.4, we show how to recover the individual factors of the larger class of products of two Monarch matrices.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Monarch Parametrization for Square Matrices", "text": "Inspired by the 4-step FFT algorithm [3], we propose the class of Monarch matrices, each parametrized as the product of two block-diagonal matrices up to permutation: Figure 2: Monarch matrices are parametrized as products of two block-diagonal matrices up to permutation, allowing efficient multiplication algorithm that leverages batch matrix multiply.\nWe call this the Monarch parametrization. We denote the class of all matrices that can be written in this form as M (n) (dropping the superscript when clear from context). Fig. 2 illustrates this parametrization.\nWe now provide more intuition for this parametrization and connect it to butterfly matrices. For ease of exposition, suppose B \u2208 B (n) where n is a power of 4. Then let L be obtained by multiplying together the first log 2 n 2 butterfly factor matrices in the butterfly factorization of B, and R by multiplying together the last log 2 n 2 butterfly factor matrices. (We detail this more rigorously in Theorem 4.) The matrix R is block-diagonal with m = \u221a n dense blocks, each block of size m\u00d7m: R = diag(R 1 , . . . , R m ). The matrix L is composed of m \u00d7 m blocks of size m \u00d7 m, where each block is a diagonal matrix:\nL = \uf8ee \uf8ef \uf8f0 D 11 . . . D 1m . . . . . . . . . D m1 . . . D mm \uf8f9 \uf8fa \uf8fb .\nThe matrix L can also be written as block-diagonal with the same structure as R after permuting the rows and columns. Specifically, let P be the permutation of Definition 3.1. We can interpret P as follows: it reshapes the vector x of size n as a matrix of size m \u00d7 m, transposes the matrix, then converts back into a vector of size n. Note that P = P . Then we can write L = PL P , where L = diag(L 1 , . . . , L m ).\nHence, up to permuting rows and columns, L is also a block-diagonal matrix of m dense blocks, each of size m \u00d7 m.\nThus we can write B = PLP R, where L, R, and P are as in Definition 3.1. So, B \u2208 B (n) implies that B \u2208 M (n) .\nProducts of Monarch Matrices. Another important class of matrices (due to their expressiveness, cf. Proposition 3.2) is the class MM * : matrices that can be written as\nM 1 M * 2 for some M 1 , M 2 \u2208 M.\nFurther, (MM * ) 2 denotes the class of matrices that can be written\nM 1 M 2 for M 1 , M 2 \u2208 MM * .\nExtension to Rectangular Matrices. In practice, we also want a way to parametrize rectangular weight matrices, and to increase the number of parameters of Monarch matrices to fit different applications (analogous to the rank parameter in low-rank matrices and the number of nonzeros in sparse matrices). We make the simple choice to increase the block size of the block-diagonal matrices in the Monarch parametrization, and to allow rectangular blocks. More details are in Appendix C.", "publication_ref": ["b2"], "figure_ref": ["fig_9", "fig_9"], "table_ref": []}, {"heading": "Expressiveness and Efficiency", "text": "We remark on the expressiveness of Monarch matrices and their products (ability to represent many structured transforms), and on their computational and memory efficiency.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Expressiveness", "text": "As described in Section 3.1, any matrix B \u2208 B (n) can be written in the Monarch butterfly representation, by simply condensing the log 2 n total factors into two matrices. Thus, the Monarch butterfly representation is strictly more general than the original butterfly representation (as there also exist matrices in M (n) but not B (n) ). In other words, for a given size n, M \u2283 B; similarly MM * \u2283 BB * . In particular, Dao et al. [13] showed that the following matrix classes are contained in BB * , which implies they are in MM * as well: Proposition 3.2. The matrix class MM * can represent convolution, Hadamard transform, Toeplitz matrices [37], and AFDF matrices [74]. The matrix class (MM * ) 2 can represent the Fourier transform, discrete sine and cosine transforms (DST/DCT), the (HD) 3 [106] class, Fastfood [62], and ACDC matrices [74].", "publication_ref": ["b12", "b36", "b73", "b2", "b61", "b73"], "figure_ref": [], "table_ref": []}, {"heading": "Efficiency", "text": "Parameters. A Monarch matrix M = PLP R is described by 2n \u221a n parameters: L, R both have \u221a n dense blocks of size \u221a n \u00d7 \u221a n, for a total parameter count of n \u221a n each. The permutation P is fixed, and thus doesn't add any parameters. Speed. To multiply by M, we need to multiply by a block diagonal matrix R, permute, multiply by a block diagonal matrix L, and finally permute. All four of these steps can be implemented efficiently. The total number of FLOPs is O(n \u221a n), which is more the O(n log n) for a butterfly matrix. However, since we can leverage efficient block-diagonal multiplication (e.g., batch matrix multiply), Monarch multiplication is easy to implement and is fast in practice (2x faster than dense multiply, cf. Section 5).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Projection on the Set M of Monarch Matrices", "text": "Given our class of structured matrices, a natural question is the projection problem: finding a Monarch matrix that is the closest to a given dense matrix. We show that this problem has an analytical optimal solution, and show how to compute it efficiently. This allows us to project dense models to Monarch models, enabling D2S fine-tuning (Section 5.3).\nWe formalize the problem: for a given matrix A, find\nargmin M\u2208M A \u2212 M 2 F .(1)\nEven though this problem is nonconvex (as M is parametrized as the product of two matrices), in Theorem 1 we show that there exists an analytical solution (full proof in Appendix D). This is analogous to the Eckart-Young theorem that establishes that optimal low-rank approximation is obtained from the SVD [26]. Theorem 1. Given an n\u00d7n matrix A, there is an O(n 5/2 )-time algorithm that optimally solves the projection problem (1), and returns the Monarch factors L and R.\nWe now derive this algorithm (Algorithm 1) by examining the structure of a Monarch matrix M. We first rewrite the steps of Monarch matrix-vector multiplication (i.e., computing Mx). The main idea is to view the input x, which is a vector of size n = m 2 , as a 2D tensor of size m \u00d7 m. Then the two matrices L and R in the Monarch parametrization M = PLP R correspond to batched matrix multiply along one dimension of x, followed by batched matrix multiply along the other dimension of x. Thus we view x as a 2D tensor of size m \u00d7 m, and each of L and R as a 3D tensor of size m \u00d7 m \u00d7 m.\nSteps to multiply x by a Monarch matrix M = PLP R: 1. Multiply R by x: y kj = i R kji x ki , to obtain an output y that is a 2D tensor of size m \u00d7 m. 2. Multiply PLP by y: z j = k L j k y kj , to obtain an output that is a 2D tensor of size m \u00d7 m. 3. Reshape z back into a vector of size n, and return this. We can thus write the output z as z j = k,i L j k R kji x ki . Since M = PLP R, we can write:\nM jki = L j k R kji .(2)\nNote that here we view M as a 4D tensor of size m \u00d7 m \u00d7 m \u00d7 m.\nWhen viewed as a 4D tensor, the structure of the matrix M becomes apparent, and the solution to the projection problem is easy to see. Let's examine Eq. (2): M jki = L j k R kji . We see that this reshaped tensor version of M is simply m \u2022 m batches of rank-1 matrices: we batch over the dimensions k and j, and each batch is simply a rank-1 matrix (p jk )(q jk ) for some length-m vectors p jk , q jk . Therefore, the projection objective (Eq. ( 1)) can be broken up into the sum of m \u2022 m independent terms, each term corresponding to a block of A of size m \u00d7 m. As the structure of a Monarch matrix forces each block to have rank 1 as described above, the solution to the projection problem becomes apparent: given a matrix A, reshape it to a 4D tensor of size m \u00d7 m \u00d7 m \u00d7 m, and take the rank-1 approximation of each batch with the SVD, which (after reshaping) yields the factors L, R of the desired matrix M \u2208 M. (Note that if A \u2208 M itself, this algorithm recovers the factors such that A = PLP R.) Algorithm 1 Projection on the set of Monarch matrices Require:\nMatrix A \u2208 R n\u00d7n , with n = m 2 . Reshape A into a 4D tensor A of size m \u00d7 m \u00d7 m \u00d7 m, where A jki = A ( \u22121)m+j,(k\u22121)m+i for , j, k, i = 1, . . . , m. for 1 \u2264 j, k \u2264 m do Let M jk = A :,j,k,: of size m \u00d7 m.\nCompute the best rank-1 approximation of M jk as u jk v jk with the SVD of A. end for Let R be the m \u00d7 m \u00d7 m tensor where R kji = (v jk ) i . Let L be the m \u00d7 m \u00d7 m tensor where L j k = (u jk ) . Return L, R as block-diagonal matrices L, R (where the b th block of L, R are L b,:,: , R b,:,: respectively)", "publication_ref": ["b25"], "figure_ref": [], "table_ref": []}, {"heading": "Factorization of MM * Matrices", "text": "In the previous section, we saw how to project onto the set M. As Theorem 3.2 shows, the broader class MM * also encompasses many important linear transforms. In this section, we present an algorithm to compute the Monarch factorization of a given matrix M \u2208 MM * , under mild assumptions. This allows us to store and apply M efficiently.\nSpecifically, observe that if M \u2208 MM * , we can write M = (PLP R)(R * PL * P ) = (PL  where m = \u221a n, the A i 's and C j 's denote the m \u00d7 m diagonal blocks of L 1 , L 2 respectively, and each D ij is an m \u00d7 m diagonal matrix. If we write M as a block matrix with m \u00d7 m blocks each of size m \u00d7 m, then we see that the block M ij is equal to A i D ij C j . Notice that M is invertible only if all the A i 's and C j 's are (since if any one of these is singular, then L 1 or L 2 is singular).\n1 P )R(PL 2 P ) for block-diagonal L 1 , L 2 , R\nThus, our goal is to find matrices\u00c2 1 , . . . ,\u00c2 m ,\u0108 1 , . . . ,\u0108 m and diagonal matricesD 11 , . . . ,D mm such that M ij =\u00c2 iDij\u0108j for all i, j; this represents a valid Monarch factorization of M.\nTo provide intuition for how to do this, let's analyze a simple case in which all the D ij 's are the identity matrix. Then we have the set of equations A i C j = M ij . Again assume the A i 's and C j 's are invertible, so each M ij is as well. Suppose we set\u0108 1 = I (identity matrix). Then we can immediately read off\u00c2 i = M i1 for all i. We can then set\u0108 j =\u00c2 \u22121 1 M 1j for all j. Let's now check that this strategy gives a valid factorization, i.e., that M ij =\u00c2 i\u0108j for all i, j. We\nhave\u00c2 i\u0108j = M i1 M \u22121 11 M 1j .\nRecalling that in the \"true\" factorization we have\nM ij = A i C j , this equals (A i C 1 )(A 1 C 1 ) \u22121 (A 1 C j ) = A i C j , as desired.\nIn the general case, we must deal with the diagonal D ij matrices as well. We will no longer be able to freely set\u0108 1 = I. However, once we find a proper choice of\u0108 1 , we can use it to find all the\u00c2 i 's and C j 's. We can find such a\u0108 1 via the idea of simultaneous diagonalization; for space reasons, we defer a full description of our algorithm (Algorithm 2), and its analysis, to Appendix D.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Using Monarch Matrices in Model Training", "text": "We can use our class of Monarch matrices to parameterize weight matrices of deep learning models in several settings.\n\u2022 In the E2E sparse training setting, we replace the dense weight matrices of a baseline model with Monarch matrices with the same dimension, initialize them randomly, and train as usual. Most of our baseline models are Transformers, and we replace the projection matrices in the attention blocks, along with the weights of the feed-forward network (FFN) blocks, with Monarch matrices. The Monarch parameterization is differentiable, and we rely on autodifferentiation to train with first-order methods such as Adam [57]. \u2022 In the S2D training setting, we first replace the dense weight matrices of a baseline model with Monarch matrices, then train the sparse model for about 90% of the usual number of iterations. We then convert the Monarch matrices to dense matrices (by simply multiplying the factors L and R along with permutations), and continue training for the remaining 10% of the iterations. Compared to dense end-to-end training, we train for the same number of iterations, but the first 90% of the iterations are faster due to the hardware efficiency of Monarch matrices. \u2022 In the D2S fine-tuning setting, we start with a dense pretrained model (e.g., BERT), and project the dense weight matrices (e.g., in the attention blocks and FFN blocks) on the set of Monarch matrices using the algorithm in Section 3.3. We then fine-tune the resulting model on downstream tasks (e.g., GLUE), using first-order methods. We typically set the number of blocks in the block-diagonal matrices to be between 2 and 4 based on the parameter budgets (25% -50% of the dense model).", "publication_ref": ["b56"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We validate our approach empirically, showing that our Monarch matrix parametrization achieves a favorable efficiency-accuracy tradeoff compared to baselines on a wide range of domains (  We show that replacing dense matrices with Monarch matrices in ViT, MLP-Mixer, and GPT-2 can speed up training by up to 2\u00d7 without sacrificing model quality in Tables 1 and 2.\nSetup. We use the popular vision benchmark, ImageNet [17]. We choose recent popular Vision Transformer [24], and MLP-Mixer [99] as representative base dense models. For language modeling, we evaluate GPT-2 [86] on WikiText-103 [73].  ", "publication_ref": ["b16", "b23", "b98", "b85", "b72"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "PDE solving and multi-coil MRI reconstruction", "text": "Many scientific or medical imaging tasks rely on specialized transforms such as the Fourier transform. We show that replacing the fixed Fourier transform with the more expressive Monarch matrices yields higher model quality (lower reconstruction error) with comparable model speed.\nSolving PDEs with Monarch Neural Operators. We follow the experimental setting in FNO [65] and apply a Monarch-based neural operator to the task of solving the Navier-Stokes PDE. Compared to baseline U-Nets [90], TF-Nets [103], ResNets [47] and FNOs [65], neural operators based on Monarch improve solution accuracy across spatial resolutions by up to 40% (Table 3).\nNon-periodic boundary conditions. Traditional spectral methods based on Fourier transform work best with periodic boundary conditions and forcing terms. However, PDEs of practical interest often exhibit non-periodic or even unknown boundary conditions. Monarch operators are not constrained to the Fourier transform and can thus still learn the solution operator with excellent accuracy. Accelerated MRI Reconstruction. We characterize the utility of Monarch-based FFT operations for accelerated MRI reconstruction, a task which requires methods with both structured Fourier operators and dealiasing properties to recover high quality images. On the clinically-acquired 3D MRI SKM-TEA dataset [20], Monarch-SENSE (mSENSE) enhances image quality by over 1.5dB pSNR and 2.5% SSIM compared to zero-filled SENSE and up to 4.4dB and 3.8% SSIM compared to U-Net baselines in data-limited settings. Setup details are available in Appendix E.5.\nExpressive FFT. By definition, standard IFFT in zero-filled SENSE cannot dealias the signal, resulting in artifacts in the reconstructed image. mSENSE replaces the inverse FFT (IFFT) operation in standard SENSE with learnable Monarch matrices. Thus, mSENSE preserves the structure of the Fourier transform while learning to reweight frequencies to suppress aliasing artifacts. Across multiple accelerations, mSENSE achieved up to +1.5dB and 2.5% improvement in peak signal-to-noise ratio (pSNR) and structural similarity (SSIM), respectively (Table 4). Data Efficiency. While CNNs have shown promise for MRI reconstruction tasks, training these networks requires extensive amounts of labeled data to avoid overfitting. However, large data corpora are difficult to acquire in practice. mSENSE can be trained efficiently with limited supervised examples. In few shot settings, mSENSE can outperform U-Net by +4.4dB (\u224815%) and 3.8% SSIM (Table 5).  ", "publication_ref": ["b64", "b89", "b102", "b46", "b64", "b19"], "figure_ref": [], "table_ref": ["tab_3", "tab_4", "tab_5"]}, {"heading": "Sparse-to-Dense Training (reverse sparsification)", "text": "GPT-2 pretraining. On the large OpenWebtext dataset [36], we train a GPT-2 model with Monarch weight matrices for 90% of the training iterations, then relax the constraint on the weight matrices and train them as dense matrices for the remaining 10% of the iterations. We call this technique \"reverse sparsification.\" Previous sparse training techniques often don't speed up training, whereas our hardware-efficient Monarch matrices do. Therefore we can use them as an intermediate step to pretrain a large language model (GPT-2) in 2\u00d7 less time. We also evaluate its downstream quality on zero-shot generation from [34] and classification tasks from [108], achieving comparable performance to the dense counterparts (Table 6). In Fig. 5, we show the training time of the dense GPT-2 model, along with the Monarch GPT-2 model. After training the Monarch model for 90% of the time, in the last 10% of the training steps, by transitioning to dense weight matrices, the model is able to reach the same performance of another model that was trained with dense weight matrices from scratch. By training with Monarch matrices for 90% of the time, we reduce the total training time by 2\u00d7.\nBERT pretraining. On the Wikipedia + BookCorpus datasets [110], we train a BERT-large model with Monarch weight matrices for 70% of the time and transition to dense weight matrices for the remaining 30% of the time, which yields the same pretraining loss as conventional dense training. In Table 7, we compare the total training time to several baseline implementations: the widely-used implementation from HuggingFace [104], the more optimized implementation from Megatron [94], and the most optimized implementation we know of from Nvidia that was used to set MLPerf 1.1 training speed record. Our method is 3.5x faster than HuggingFace and 23% faster than Nvidia's MLPerf 1.1 implementation 3 . Experiment details are in Appendix E.4.", "publication_ref": ["b35", "b33", "b107", "b109", "b103", "b93", "b2"], "figure_ref": ["fig_5"], "table_ref": ["tab_6", "tab_7"]}, {"heading": "Dense-to-Sparse Fine-tuning", "text": "We show that our Monarch approximation algorithm allows us to efficiently use pretrained models, such as speeding up BERT finetuning on GLUE.\nBERT finetuning. We take the BERT pretrained weights, approximate them with Monarch matrices, and finetune the resulting model on the 9 GLUE tasks. The results in Table 8 shows that we obtain a Monarch finetuned model with similar quality to the dense BERT model, but with 1.7\u00d7 faster finetuning speed. This   ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "A Extended Related Work", "text": "In this section, we extend the related works referenced in the main paper and discuss them in detail.\nSparse Training. Our work is loosely related to neural network pruning. By iteratively eliminating neurons and connections, pruning has seen great success in compressing complex models. Han et al. [44,45] put forth two naive but effective algorithms to compress models up to 49x and maintain comparable accuracy. Li et al. [64] employ filter pruning to reduce the cost of running convolution models up to 38 %, Lin et al. [66] prunes the network at runtime, hence retaining the flexibility of the full model. Dong et al. [23] prunes the network locally in a layer by layer manner. Sanh et al. [92] prunes with deterministic first-order information, which is more adaptive to pretrained model weights. Lagunas et al. [60] prunes transformers models with block sparsity pattern during fine-tuning, which leads to real hardware speed up while maintaining the accuracy.\nZhu & Gupta [109] finds large pruned sparse network consistently outperform the small dense networks with the same compute and memory footprints. Although both our and all the pruning methods are aiming to produce sparse models, we differ in our emphasis on the overall efficiency, whereas pruning mostly focuses on inference efficiency and disregards the cost in finding the smaller model. There has been more recent work on sparse methods that focuses on speeding up training and not just inference, such as SNFS [21], RigL [21], Top-KAST [50]. These methods often focus on FLOP counts, which may not correlate well with wall-clock time on modern hardware (e.g., GPUs). Block-sparsity is another approach that exploits the block-oriented nature of GPUs [38,7,41]. Sparse models have also been found useful to improve the training process of dense models. For example, sparsity can be used to regularize dense models to improve accuracy [46], or to alternate between sparse and dense training to ease deployment [82]. Our sparse-to-dense reverse sparsification instead focuses on speeding up dense training, where the sparse model is used for efficiency and not regularization.\nIn addition, models proposed in our work can be roughly seen as a class of manually constructed lottery tickets. Lottery tickets Frankle & Carbin [30] are a set of small sub-networks derived from a larger dense network, which outperforms their parent networks in convergence speed and potentially in generalization. A huge number of studies are carried out to analyze these tickets both empirically and theoretically: Morcos et al. [75] proposed to use one generalized lottery tickets for all vision benchmarks and got comparable results with the specialized lottery tickets; Frankle et al. [31] improves the stability of the lottery tickets by iterative pruning; Frankle et al. [32] found that subnetworks reach full accuracy only if they are stable against SGD noise during training; Orseau et al. [78] provides a logarithmic upper bound for the number of parameters it takes for the optimal sub-networks to exist; Pensia et al. [81] suggests a way to construct the lottery ticket by solving the subset sum problem and it's a proof by construction for the strong lottery ticket hypothesis. Furthermore, follow-up works [68,102,96] show that we can find tickets without any training labels.\nStructured matrices and butterfly matrices. Structured matrices are those with asymptotically fast matrix-vector multiplication algorithm (o(n 2 ) time complexity) and few parameters (o(n 2 ) space complexity). Common examples include sparse & low-rank matrices, and fast transforms such as Fourier transform, Chebyshev transform, Legendre transform, and more generally orthogonal polynomial transforms. These transforms have been widely used in data preprocessing (e.g., DFT in speech processing [52]) and kernel approximation [62,106]. Many generalizations of these transforms have been used in machine learning to replace dense weight matrices [95,98,40]. De Sa et al. [16] shows that any structured matrix (in the form of arithmetic circuits) can be written as product of sparse matrices, and Dao et al. [13] shows that products of butterfly matrices can represent these structured matrices almost optimally in terms of runtime and memory. The class of butterfly matrices [80] have also been used in kernel models [76,8] and deep learning models [101,67,1].\nNeural Operators for PDEs. Deep learning has found application in the domain of differential equations and scientific computing [85], with methods developed for prediction and control problems [56,71], as well as acceleration of numerical schemes [83,51]. Specific to the partial differential equations (PDEs) are approaches designed to learn solution operators [87,29,65], and hybridized solvers [59], evaluated primarily on classical fluid dynamics.\nThe promise of these approaches is to offer, at the cost of an initial training procedure, accurate yet faster solutions than an appropriate numerical method tuned for a specific problem, which can then be leveraged for real-time forecasting or within larger feedback loops. Nonetheless, optimal design of neural operators remains an open problem, with most relying on fast Fourier transforms (FFT) or standard dense neural architectures. Instead, neural operators based on Monarch are capable of approximating all fast transforms, thus allowing automated optimization towards a suitable transform on a given PDE problem.\nMRI. Accelerated multi-coil MRI is an essential mechanism for reducing long scan times and making certain scan types feasible. In multi-coil MRI, data is acquired in the spatial Fourier domain (a.k.a k-space) across multiple coils (sensors). To reduce scan time, this data is sampled below the required rate for recovering the underlying signal (i.e. Nyquist rate), which results in signal aliasing (see Appendix E.5). In these settings, direct application of the inverse fast Fourier transform (FFT) cannot suppress aliasing artifacts.\nClassical MRI reconstruction approaches supplement the FFT by leveraging shared information across multiple coils and strong analytical priors to regularize image recovery objectives. SENSE-based methods jointly dealias images across multiple coils and reweight the final image based on the spatial sensitivity profile of each coil [84]. Compressed sensing promotes image sparsity in transformation domains (e.g. Fourier, wavelet) while enforcing data consistency between the Fourier transform of the reconstructed image and the observed measurements [69]. Low-rank methods enforce low rank structure across slowly-varying dimensions or local patches in the data [77,89,42]. Additionally, GRAPPA-based techniques optimize kernels to directly interpolate missing k-space samples to promote smoothness in the Fourier domain [39]. Despite their efficacy, these methods have long reconstruction times, require explicit analytical priors, and require careful hyperparameter fine-tuning.\nCNNs have shown promise as a fast-at-inference, learnable alternative to classical MRI reconstruction methods [58]. In supervised learning, fully convolutional networks (e.g. U-Net [90] or unrolled networks [91,43]) learn a mapping between paired zero-filled and fully-sampled, ground truth images. However, supervised methods require a large fully-sampled (labeled) data corpus and are sensitive to distribution drifts due to patient, hardware, and sequence heterogeneity [15]. To reduce dependence on labeled data, unsupervised methods have used generative adversarial networks [9,70], self-supervised learning [105], dictionary learning [61], and untrained networks [14]. Despite their label efficiency, these techniques still underperform supervised methods and are also sensitive to distribution shift. Recently, a family of semi-supervised reconstruction methods demonstrated label efficiency and robustness to physics-driven perturbations, such as changes in signalto-noise ratio or patient motion [19,18]. However, these methods require large amounts of unlabeled data, which can be difficult to curate in few-shot settings. Thus, despite their success in controlled environments, prospective clinical deployment of these models has been stifled [5].\nIn our work, we propose a model with a single FFT-initialized factorized Monarch matrix. Such a matrix can provide the benefits of both a simple linearized transformation like FFT and a learnable mechanism to remove aliasing artifacts resulting from the undersampled k-space. The smaller learnable parameter set may reduce overfitting in data-limited settings while preserving the transformation structure of Fourier matrices. Thus, our approach can be interpreted as a hybrid between analytically-constrained classical methods and data-dependent CNNs.", "publication_ref": ["b43", "b44", "b63", "b65", "b22", "b91", "b59", "b108", "b20", "b20", "b49", "b37", "b6", "b40", "b45", "b81", "b29", "b74", "b30", "b31", "b77", "b80", "b67", "b101", "b95", "b51", "b61", "b105", "b94", "b97", "b39", "b15", "b12", "b79", "b75", "b7", "b100", "b66", "b0", "b84", "b55", "b70", "b82", "b50", "b86", "b28", "b64", "b58", "b83", "b68", "b76", "b88", "b41", "b38", "b57", "b89", "b90", "b42", "b14", "b8", "b69", "b104", "b60", "b13", "b18", "b17", "b4"], "figure_ref": [], "table_ref": []}, {"heading": "B Notation Review", "text": "Throughout this paper, we use lowercase to denote scalars (e.g., k), lowercase boldface to denote vectors (e.g., v), and uppercase boldface to denote matrices (e.g., A).\nI denotes the identity matrix. We use A to denote the transpose of a matrix and A * to denote the conjugate transpose of a matrix. All results in this paper apply to matrices over the either the reals R or the complex numbers C; when the field under consideration can be either one of these, we denote it by F.\nWe use 1-indexing throughout this paper except where explicitly stated.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C General Monarch Matrix Parametrization", "text": "In Section C.1, we define a parametrization for square Monarch matrices of different \"block sizes\" (i.e., not necessarily \u221a n), and prove some basic properties about them. In Section C.2, we further extend this to define rectangular Monarch matrices, and prove some basic properties about them.\nNote: In this section, we use 0-indexing rather than 1-indexing, for notational convenience.\nC.1 General square matrices\nC.1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parametrization", "text": "In this section, we define a more general Monarch parametrization for square matrices, allowing for different \"block sizes.\" Like Definition 3.1, the parametrization involves the product of a permuted block-diagonal matrix with another block-diagonal matrix; the difference is that we now allow the matrices L and R to have diagonal blocks of different sizes. Thus, the permutations applied to L (to turn it into a block matrix where each block matrix is diagonal) will correspondingly also be different. First, in Definition C.1, we define notation for a class of block-diagonal matrices.\nDefinition C.1 (Class BD (b,n) ). Let b \u2208 (1, n\n) be an integer that divides n. For 0 \u2264 i < n b , let R i \u2208 F b\u00d7b be a b \u00d7 b \"block\" matrix. Then define the matrix R with block size b as follows:\nR = diag R 0 , . . . , R n b \u22121 .(3)\n(Note that the number of possible nonzero values in R is n b \u2022 b 2 = nb.) We denote the class of all matrices R expressible in this form by BD (b,n) . Note that this class is closed under (conjugate) transposition and contains the identity matrix.\nNext, in Definition C.2, we define notation for a class of block matrices whose blocks are diagonal.\nDefinition C.2 (Class DB (b,n) ). Let b \u2208 (1, n) be an integer that divides n. For 0 \u2264 i, j < b, let D i,j \u2208 F b\u00d7b be a b \u00d7 b diagonal matrix.\nThen let L be an n \u00d7 n matrix with the following form:\nL = \uf8ee \uf8ef \uf8f0 D 0,0 . . . D 0, n b \u22121 . . . . . . . . . D n b \u22121,0 . . . D n b \u22121, n b \u22121 \uf8f9 \uf8fa \uf8fb (4) (Note that the number of possible nonzero values in L is n b 2 \u2022 b = n 2 b .)\nWe denote the class of all matrices L expressible in this form by DB (b,n) . Note that this class is closed under (conjugate) transposition and contains the identity matrix. As we show in Appendix C.1.2, L can be written as a block-diagonal matrix with b blocks of size n b \u00d7 n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices. We denote the class of all matrices L expressible in this form by DB (b,n) . Note that this class is closed under (conjugate) transposition. As we show in Appendix C.1.2, L can be written as a block-diagonal matrix with b blocks of size n b \u00d7 n b (i.e., a matrix in BD ( n b , n) ), multiplied on the left and right with appropriate permutation matrices.\nUsing these two definitions, we define the class of Monarch matrices with a given block size.\nDefinition C.3 (Class M (b,n) ). Let b \u2208 (1, n\n) be an integer that divides n. A Monarch matrix of size n \u00d7 n and \"block size b\" is a matrix of the form:\nM = LR (5)\nwhere L \u2208 DB (b,n) and R \u2208 BD (b,n) .\nWe denote the class of all matrices M expressible in this form by M (b,n) . Observe that when b = \u221a n, this is exactly the matrix class M Observe that when b = \u221a n, MM * (b,n) is exactly the matrix class MM * (n) defined in Section 3. Note that a matrix in MM * (b,n) or M * M (b,n) . is represented by 2 n 2 b + 2nb parameters. Finally, we define the following \"Monarch hierarchy\" based on the kaleidoscope hierarchy of [13]:\nDefinition C.5 (Class (MM * (b,n) ) w e ). Let b \u2208 (1, n) be an integer that divides n. We define the matrix class (MM * (b,n) ) w e as the set of all matrices M that can be expressed as\nM = \uf8eb \uf8ed w i=1 M i \uf8f6 \uf8f8 [1 : n, 1 : n](6)\nwhere each M i \u2208 MM * (b,e\u2022n) .\nNote that a matrix in (MM * (b,n) ) w e is represented by 2w e 2 n 2 b + 2wenb parameters.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}, {"heading": "C.1.2 Properties", "text": "Here we show some properties of the matrix classes defined above. We first show some basic equivalent ways to define these classes. We then show (Theorem 3) that the matrices in DB (b,n) are permuted blockdiagonal matrices; specifically, that they can be converted to matrices in BD ( n b ,n) by applying the appropriate permutation. Finally, we state an expressivity result for the general \"Monarch hierarchy\" which follows from Theorem 1 of [13].\nFirst, we define a class of permutations. Let 1 \u2264 b \u2264 n be integers such that b divides n. We will need to express each index 0 \u2264 i < n in \"block form.\" More specifically: Definition C.6. Let i \u2265 0, b \u2265 1 be integers. Then define i 0 = i mod b, and\ni 1 = i b .\nWe use the notation i \u2261 (i 1 , i 0 ) b to denote the representation above. In particular, if i \u2261 (i 1 , i 0 ) b , then we have\ni = i 1 \u2022 b + i 0\nUsing this notation, we define the following class of permutations:\nDefinition C.7. Let b \u2208 [1, n] be an integer that divides n. Let i \u2261 (i 1 , i 0 ) b . Define \u03c3 (b,n) (i) = i 0 \u2022 n b + i 1 .(7)\nThat is,\n\u03c3 (b,n) (i) \u2261 (i 0 , i 1 ) n b\n. Let P (b,n) denote the n \u00d7 n permutation matrix defined by the permutation \u03c3 (b,n) .\nIntuitively, P (b,n) can be interpreted as reshaping a length-n vector into an b \u00d7 n b matrix in row-major order, transposing the result, and then flattening this back into a vector (again in row-major order). Now, we restate the formulation in Definition C.1 equivalently as:\nProposition C.8. A matrix R satisfies Equation (3) (i.e., R \u2208 BD (b,n) ) if and only if the following holds for any 0 \u2264 i, j < n. Let i \u2261 (i 1 , i 0 ) b and j \u2261 (j 1 , j 0 ) b . Then\n1. if i 1 = j 1 , then R[i, j] = 0.\n2. Else (i.e., when i 1 = j 1 ), then R\n[i, j] = R i1 [i 0 , j 0 ].\nWe restate the formulation in Definition C.2 equivalently as:\nProposition C.9. A matrix L satisfies Equation (4) (i.e., L \u2208 DB (b,n) ) if and only if the following holds for any 0 \u2264 i, j < n. Let i \u2261 (i 1 , i 0 ) b and j \u2261 (j 1 , j 0 ) b . Then 1. if i 0 = j 0 , then L[i, j] = 0.\n2. Else, (i.e., when i 0 = j 0 ), then\nL[i, j] = D i1,j1 [i 0 , i 0 ].\nWe will argue the following:\nTheorem 3. Let 1 \u2264 b \u2264 n such that b divides n.\nRecall that P (b,n) is the permutation matrix defined by the permutation \u03c3 (b,n) . Let L be a matrix in DB (b,n) . Then we have\nR = P (b,n) \u2022 L \u2022 P (b,n) ,\nwhere R \u2208 BD ( n b , n) .\nProof. We first note that multiplying an n \u00d7 n matrix on the right (and left resp.) by P (b,n) = P ( n b ,n) (and P (b,n) resp.) permutes the columns (and rows resp.) of the matrix according to \u03c3 (b,n) . 4 This implies that for any 0 \u2264 i, j < n:\nR [\u03c3 (b,n) (i), \u03c3 (b,n) (j)] = L[i, j].(8)\nTo complete the proof, we will argue that R satisfies the two conditions in Proposition C.8. Towards this end, let 0 \u2264 i, j < n be arbitrary indices and further, define\ni = (i 1 , i 0 ) b and j = (j 1 , j 0 ) b . Then note that \u03c3 (b,n) (i) = (i 0 , i 1 ) n b and \u03c3 (b,n) (j) = (j 0 , j 1 ) n b\n. By Proposition C.9, we have that if i 0 = j 0 , then L[i, j] = 0. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (\u03c3 (b,n) (i), \u03c3 (b,n) (j)) in item 1 in Proposition C.8. Then by Eq. ( 8), we have that R [\u03c3 (b,n) (i), \u03c3 (b,n) (j)] = 0, which satisfies item 1 in Proposition C.8. Now consider the case that i 0 = j 0 ; then by item 2 in Proposition C.9, we have that\nL[i, j] = D i1,j1 [i 0 , i 0 ]. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (\u03c3 (b,n) (i), \u03c3 (b,n) (j)) in item 2 in Proposition C.8 if we define R i0 \u2208 F n b \u00d7 n b as follows: R i0 [i 1 , j 1 ] = D i1,j1 [i 0 , i 0 ].", "publication_ref": ["b12", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Note that the above implies that", "text": "R = diag R 0 , . . . , R b\u22121 ,\nwhere R \u2022 is as defined in the above paragraph. This means R \u2208 BD ( n b ,n) , since each block R i0 is a matrix of size n b \u00d7 n b .\nWe now briefly note some alternate ways to express matrices in MM * (b,n) .\nProposition C.10. For any M \u2208 MM * (b,n) , we can write (b,n) . Notice that since R * 1 , R 2 are both block-diagonal with the same structure (i.e., both have blocks of size b \u00d7 b), their product R is also in BD (b,n) . Also, by Theorem 3 we can write\nM = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 \u2208 BD ( n b ,n) and R \u2208 BD (b,n) .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof. By definition (see Definition C.1 and Definition", "text": "C.2), if M \u2208 MM * (b,n) , we can write M = (L 1 R 1 )(L 2 R 2 ) * = L 1 (R * 1 R 2 )L * 2 , where L 1 , L 2 \u2208 DB (b,n) , R 1 , R 2 \u2208 BD\nL 1 = P (b,n) L 1 P (b,n) , L 2 = P (b,n) L 2 P (b,n) , where L 1 , L 2 are both in BD ( n b ,n) (i.\ne., block diagonal with blocks of size n b \u00d7 n b ). Thus, we can write M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 \u2208 BD ( n b ,n) and R \u2208 BD (b,n) .\nleast n 2 b \u2265 n 3/2 free parameters (the entries in the blocks of the block-diagonal matrix P (b,n) LP (b,n) can be arbitrary, and there are b such blocks each of size n b ). Similarly, in the case b > \u221a n, we can set L to the identity, and M has at least nb \u2265 n 3/2 free parameters (the entries of the block-diagonal matrix R can be arbitrary, and there are nb total of these). Thus, at least n 3/2 parameters are required to uniquely describe any matrix in M (b,n) . However, a butterfly matrix in B (n) has only 2n log 2 n parameters. For n > 256, 2n log 2 n < n 3/2 . (Note that this analysis is not tight: a more careful analysis can show the inclusion is strict even for smaller values of n.)\nWe end this section with a theorem on the expressivity of the \"monarch hierarchy\" (products of monarch matrices), which follows from Theorem 1 of [13].\nTheorem 5 (Monarch hierarchy expressivity). Let M be an n\u00d7n matrix such that matrix-vector multiplication of M and an arbitrary vector v (i.e., computation of Mv) can be represented as a linear arithmetic circuit with depth d and s total gates. Let b \u2208 (1, n) be a power of 2 that divides n.\nThen, M \u2208 (MM * (b,n) ) O(d) O(s/n) .\nProof. Theorem 1 of Dao et al. [13] says that if n is a power of 2 and A is an n \u00d7 n matrix such that multiplying any vector v by A can be represented as a linear arithmetic circuit with depth \u2264 d and \u2264 s total gates, then A \u2208 (BB\n* (n) ) O(d) O(s/n) (this is the \"kaleidoscope representation\" of A).\nRecall from Theorem 4 that for any b \u2208 (1, n) that is a power of 2 and divides n, M (b,n) \u2283 B (n) ; thus, this implies MM * (b,e\u2022n) \u2283 BB * (e\u2022n) , and in turn (MM * (b,n) ) w e \u2283 (BB\n* (n) ) w e . As A \u2208 (BB * (n) ) O(d) O(s/n) , we thus have A \u2208 (MM * (b,n) ) O(d) O(s/n) .\nAs per [13], the class of kaleidoscope matrices (BB * Define f (n, s) to be the largest power of 2 that is \u2264 \nmin n 2 , \u221a s . Note that f (n, s) = O( \u221a s), and since s = O(n 2 ), f (n, s) = \u2126( \u221a s), so f (n, s) = \u0398( \u221a s). We thus have A \u2208 (MM * (f (n,s),n) ) O(", "publication_ref": ["b12", "b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 General rectangular matrices", "text": "In this section, we extend the Monarch parametrization to apply to rectangular matrices, and prove some basic properties of the relevant matrix classes. (Note that our subsequent theoretical results (Appendix D) do not depend on this section, as they focus on the square parametrization.) For the rest of the section, we will assume that n 1 , n 2 , n 3 , b 1 , b 2 , b 3 \u2265 1 are integers such that:\n\u2022 b i divides n i for all 1 \u2264 i \u2264 3, and\n\u2022 n1 b1 = n2 b2\n. We begin with the definition of the following class of rectangular block-diagonal matrices: Definition C.14. For 0 \u2264 i < n b1 , let R i \u2208 F b2\u00d7b1 be a b 2 \u00d7 b 1 matrix. Then define the matrix R \u2208 F n2\u00d7n1 as follows:\nR = diag R 0 , . . . , R n 1 b 1 \u22121 .(9)\nWe say that R has block size b 2 \u00d7 b 1 . Recall that we have assumed n1 b1 = n2 b2 , so Eq. ( 9) is well-defined. (Note that the number of possible nonzero values in R is\nn1 b1 \u2022 b 1 \u00d7 b 2 = n 1 b 2 .)\nWe denote the class of all matrices R expressible in this form by BD (b2\u00d7b1,n2\u00d7n1) . Note that this class is only defined when n1 b1 = n2 n2 . We restate the above definition equivalently as: Proposition C.15. R \u2208 F n2\u00d7n1 is in BD (b2\u00d7b1,n2\u00d7n1) (with n1 b1 = n2 n2 ) if and only if the following holds for any 0 \u2264 i < n 2 and 0 \u2264 j < n 1 . Let i \u2261 (i 1 , i 0 ) b2 and j \u2261 (j 1 , j 0 ) b1 (recalling this notation from Definition C.6. Then 1. if i 1 = j 1 , then R[i, j] = 0.\n2. Else (i.e., when i 1 = j 1 ), then R[i, j] = R i1 [i 0 , j 0 ]. Before we define the rectangular L, we first need to define the notion of a 'wrapped diagonal' matrix: Definition C. 16. A wrapped diagonal matrix S \u2208 F b3\u00d7b2 is defined as follows. First assume b 2 \u2264 b 3 . Then for any 0 \u2264 i < b 3 and 0 \u2264 j < b 2 , we have the following. If i mod b 2 = j, then S[i, j] = 0. (If b 2 > b 3 , then instead apply the previous definition to S .)\nWe now define the following class of block matrices with each block a wrapped diagonal matrix.\nDefinition C.17. Let L \u2208 F n3\u00d7n2 have the form:\nL = \uf8ee \uf8ef \uf8ef \uf8f0 S 0,0 . . . S 0, n 2 b 2 \u22121 . . . . . . . . . S n 3 b 3 \u22121,0 . . . S n 3 b 3 \u22121, n 2 b 2 \u22121 \uf8f9 \uf8fa \uf8fa \uf8fb ,(10)\nwhere each S \u2022,\u2022 is a wrapped diagonal matrix in F b3\u00d7b2 .\nWe say that L has block size b 3 \u00d7 b 2 . (Note that the number of possible nonzero values in L is\nn2 b2 \u2022 n3 b3 max(b 2 , b 3 ) = n2\u2022n3 min(b2,b3) .)\nWe denote the class of all matrices L expressible in this form by DB (b3\u00d7b2,n3\u00d7n2) .\nWe restate the above definition equivalently as: n3\u00d7n2) if and only if the following holds for any 0 \u2264 i < n 3 and 0 \u2264 j < n 2 . Let i \u2261 (i 1 , i 0 ) b3 and j \u2261 (j 1 , j 0 ) b2 . Assuming b 2 \u2264 b 3 , we have:\nProposition C.18. L \u2208 F n3\u00d7n2 is in DB (b3\u00d7b2,\n1. if i 0 mod b 2 = j 0 , then L[i, j] = 0.\n2. Else, (i.e., when i 0 mod b 2 = j 0 ), then\nL[i, j] = S i1,j1 [i 0 , j 0 ].\nIf b 2 > b 3 , then in the above, the condition \"i 0 mod b 2 = j 0 \" gets replaced by \"j 0 mod b 2 = i 0 .\"\nUsing the above definitions, we now define the class of rectangular Monarch matrices.\nDefinition C.19 (Rectangular Monarch Matrix). Let M \u2208 F n3\u00d7n1 be a matrix of the form:\nM = LR (11)\nwhere L \u2208 DB (b3\u00d7b2,n3\u00d7n2) and R \u2208 BD (b2\u00d7b1,n2\u00d7n1) .\n(As mentioned before, we assume b i divides n i for i = 1, 2, 3 and that n 1 /b 1 = n 2 /b 2 .) We denote the class of all matrices M expressible in this form by M ((b1,b2,b3),(n1,n2,n3)) . Observe that when b 1 = b 2 = b 3 = b and n 1 = n 2 = n 3 = n, this is exactly the matrix class M (b,n) in Definition C. 19.\nWe are now ready to prove our main result in this section, which essentially follows from the observation that if we permute the rows and columns of L such that the row/column block size in L becomes the number of row/columns blocks in the permuted matrix (and vice-versa) then the permuted matrix has the form of R. Theorem 6. Let 1 \u2264 b, n 2 , n 3 be such that b divides n 2 and n 3 . Suppose L \u2208 F n3\u00d7n2 \u2208 DB (b\u00d7b,n3\u00d7n2) . Then if we define R = P (b,n3) \u2022 L \u2022 P (b,n2) ,\nwe have that R \u2208 BD ( n 3 b 3 \u00d7 n 2 b 2 ,n3\u00d7n2) .\nProof. We recall that multiplying an m \u00d7 n matrix on the right (and left resp.) by P (b,n) = P ( n b ,n) (and P (b,m) resp.) permutes the columns (and rows resp.) of the matrix according to \u03c3 (b,n) (and \u03c3 (b,m) ) respectively. 5 This implies that for any 0 \u2264 i, j < n:\nR [\u03c3 (b,n3) (i), \u03c3 (b,n2) (j)] = L[i, j].(12)\nSince the squared Frobenius norm objective A \u2212 M 2 F (Eq. ( 1)) only depends on the entries of A and M and not their shape, we can rewrite the objective after reshaping:\nA \u2212 M 2 F = jki (A jki \u2212 M jki ) 2 = jki (A jki \u2212 L j k R kji ) 2 = jk i (A jki \u2212 L j k R kji ) 2 .\nWe see that the objective decomposes into m \u00d7 m independent terms (indexed by j and k). For each value of j and k, the objective is exactly the rank-1 approximation objective for the corresponding slice A :,j,k,: . Let u jk v jk be the best rank-1 approximation of A :,j,k,: (which we can compute using the SVD, by the Eckart-Young theorem [26] for Frobenius norm). Let R be the 3D tensor of size m \u00d7 m \u00d7 m where R kji = (v jk ) i , and let L be the 3D tensor of size m \u00d7 m \u00d7 m where L j k = (u jk ) . Then each of the terms in the objective is minimized, and thus the overall objective is minimized.\nWe see that the algorithm requires m \u2022 m SVD's, each of size m \u00d7 m. Each SVD takes O(m 3 ) time [100], so the overall time complexity is O(m 5 ) = O(n 5/2 ).", "publication_ref": ["b15", "b18", "b4", "b25", "b99"], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Monarch Factorizations for Matrices in MM *", "text": "In this section, we describe the algorithm for factorizing matrices in MM * previously outlined in Section 3.4 (Algorithm 2). Again, Algorithm 2 handles the general case where the block sizes of L and R can be different. We then prove Theorem 7, which has Theorem 2 as an immediate corollary.\nOur goal is thus to compute the matrices L 1 , R, L where each block is a diagonal matrix. Thus, we can write: \n\uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed M 11 M 12 . . . M 1b M 21 M 22 . . . M 2b . . . . . . . . . . . . M b1 M b2 . . . M bb \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed A 1 A 2 . . . A b \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed D 11 D 12 . . . D 1b D 21 D 22 . . . D 2b . . . . . . . . . . . . D b1 D b2 . . . D bb \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed C 1 C 2 . . . C b \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 ,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Thus, we have the set of matrix equations", "text": "A i D ij C j = M ij , for 1 \u2264 i, j \u2264 b.\nNotice that the assumption that the R has no nonzero entries in its blocks (Assumption D.1) is equivalent to assuming that none of the diagonal entries of any matrix D ij is equal to zero. Also, the assumption that M is invertible implies that L 1 , L 2 are invertible (since the product of square singular matrices is singular), which in turn implies that each block matrix A i and each block matrix C j is invertible (since a square block-diagonal matrix where one Notice that by setting b = \u221a n, we immediately recover Theorem 2. Note also that by Proposition C.11, Theorem 7 implies that given an M \u2208 M * M ( n b ,n) , we can find its Monarch factorization in time O( n 3 b ) as well (e.g., simply permute it to a matrix in MM * (b,n) and then run Algorithm 2). We now prove Theorem 7.\nProof. We first show that the factorization returned by Algorithm 2 is valid, which reduces to showing that (1) M ij =\u00c2 iDij\u0108j and (2)D ij is diagonal, for all 1 \u2264 i, j \u2264 b as argued above.\nAs argued above, since M satisfies Assumption D.1, then there exists a matrix (C 1 ) that simultaneously diagonalizes all the F(i, j)'s. Thus, we can always compute some matrix that simultaneously diagonalizes these matrices (i.e., line 2 of Algorithm 2 will always return a valid solution); we discuss how to actually do this below. By definition of simultaneous diagonalization, this matrix (which we set\u0108 1 to) is invertible.\nSo\n,\u00c2 i = M i1\u0108 \u22121\n1 is invertible for all i. Thus\u0108 j =\u00c2 \u22121 1 M 1j is invertible for all j as well. (Note that the equation\u0108 j =\u00c2 \u22121 1 M 1j holds by construction of\u0108 j for j \u2265 2, and by construction of\u00c2 1 when j = 1.) A\u015d\nD ij =\u00c2 \u22121 i M ij\u0108 \u22121 j\nby definition, we thus have that M ij =\u00c2 iDij\u0108j for all i, j. It remains to show thatD ij is diagonal.\nD ij =\u00c2 \u22121 i M ij\u0108 \u22121 j = ( M i1\u0108 \u22121 1 ) \u22121 M ij (\u00c2 \u22121 1 M 1j ) \u22121 =\u0108 1 M \u22121 i1 M ij M \u22121 1j\u00c2 1 =\u0108 1 ( M \u22121 i1 M ij M \u22121 1j M 11 )\u0108 \u22121 1 =\u0108 1 F(i, j)\u0108 \u22121 1 But\u0108 1 F(i, j)\u0108 \u22121 1\nis diagonal for all i, j by definition of\u0108 1 as a matrix that simultaneously diagonalizes the F(i, j)'s.\nAs for L 1 , R, L 2 , recall that we can simply set b 3 ) time. (Note that we can compute each of these faster using fast matrix multiplication / inversion; however, it turns out not to matter as the simultaneous diagonalization is the bottleneck.)\nFinally, we analyze the simultaneous diagonalization runtime. Simultaneous diagonalization of a set of matrices {G 1 , . . . , G k } is equivalent to finding a mutual eigenbasis for the matrices, since if D i is a diagonal matrix and QG i Q \u22121 = D i , then the j th column of Q is an eigenvector of G i with eigenvalue equal to the j th entry of D i .\nA simple algorithm for simultaneous diagonalizing a set of matrices, assuming that they are in fact simultaneously diagonalizable (which implies that each matrix is individually diagonalizable), is as follows (e.g. see [10,4]): first, set i = 1 and diagonalize the first matrix G i = G 1 (i.e., find an eigenbasis), and set Q to be the diagonalizing matrix (i.e., the matrix of eigenvectors). So, QG 1 Q \u22121 is diagonal. By the assumption that the matrices are in fact simultaneously diagonalizable, QG j Q \u22121 will be permuted block diagonal for all j = i as well: the size of each block corresponds to the multiplicity of the corresponding eigenvalue of G 1 . (Note that if G 1 's has unique eigenvalues, then the eigenbasis is unique (up to permutation and nonzero scaling), and thus in this case G 1 uniquely determines the simultaneously diagonalizing matrix, up to arbitrary permutation and nonzero scaling of the rows. In other words, the block size will be 1 in this case, meaning that QG j Q \u22121 will be diagonal for all j, and we are done.)\nSo now, we repeat the following for all i up to k. Increment i and compute QG i Q \u22121 . If it is already diagonal, move on. Otherwise, first permute Q \u2190 PQP so that it is block diagonal (observe that this maintains the property that QG j Q \u22121 is diagonal for all j < i, since PDP is diagonal for any permutation P and diagonal matrix D). Then for each block of size > 1, compute a matrix that diagonalizes that block; denoting the number of blocks (including size-1 blocks) by b, let Q 1 , . . . , Q b denote the corresponding diagonalizing transformations, or the scalar 1 when the block is of size 1. Finally set Q \u2190 diag(Q 1 , . . . , Q b ) and Q \u2190 Q \u22121 QQ . By construction, QG i Q \u22121 will now be diagonal; also, QG j Q \u22121 is still diagonal for all j < i, because any linear combination of a set of eigenvectors of a diagonalizable matrix corresponding to a repeated eigenvalue \u03bb is itself an eigenvector of that matrix with eigenvalue \u03bb.\nThus, once we've processed all k of the G i 's, Q is a matrix that simultaneously diagonalizes all of them. At each step i, we compute diagonalizing transformations for square block matrices whose sizes s 1 , . . . , s k sum to n. As eigendecomposition (for a fixed desired precision) takes O(n 3 ) time for an n \u00d7 n matrix, this means the total runtime of step i is O k j=1 s 3 i \u2264 O(n 3 ). Thus the total runtime of the entire simultaneous diagonalization procedure is O(kn 3 ), where k is the number of matrices. (Note that iterative methods for simultaneous diagonalization also exist [4,2] and could be used to speed up this step in practice.)\nApplying this to our problem, we have b 2 matrices to simultaneously diagonalize, each of size for the entire simultaneous diagonalization procedure, and thus the runtime of Algorithm 2 is also O n 3 b , as desired. (Note: As can be seen from the above analysis, we don't actually need M itself to be invertible-we simply need all its blocks M ij to be, so that all the A i 's and C j 's are, which is a weaker assumption that invertibility of M given that we already assumed the D ij 's are invertible due to the nonzero assumption on the blocks of R.)", "publication_ref": ["b9", "b3", "b3", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "E Experiment Details E.1 Model Configurations and Hyperparameters", "text": "We summarize the details required to replicate our experiments below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1.1 Image Classification", "text": "Baseline Model: For dense models, we use standard implementations of ViT [24], MLP-Mixertolstikhin2021mlp from the timm library and from the T2T-ViT codebase [107].\nThe Monarch version of these models simply swap out the dense weight matrices in the attention blocks (projection matrices) and in the FFN block (linear layers) with Monarch matrices. We set the number of blocks in the block-diagonal matrices to 4. We also reduce the amount of regularization (stochastic depth) as our Monarch models are smaller than the dense models.\nWe adopt the hyperparameters (optimizer, learning rate, learning rate scheduler) from Yuan et al. [107]. Details are in Table 9.\nWe measure the wall-clock training time on V100 GPUs. We follow the naming convention in the Vision Transformer paper and MLP-Mixer paper. In particular, ", "publication_ref": ["b23", "b106", "b106"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "E.4 Details for BERT Pretraining", "text": "We follow the training procedure and hyperparameters of the reference implementation from Nvidia Deep Learning examples (https://github.com/NVIDIA/DeepLearningExamples). In particular, we use the LAMB optimizer with learning rate 4e-3. We use as large a minibatch size as possible that still fits in the GPU memory (A100-40GB), and use gradient accumulation to reach an effective batch size of 64k sequences for phase 1 (maximum sequence length 128) and 32k for phase 2 (maximum sequence legnth 512). We train is mixed precision (fp16 and fp32).\nWe use all the optimizations that were in Nvidia's BERT implementation in MLPerf 1.1:\n1. Only compute the prediction scores (last layer) for masked tokens as the outputs of other tokens are not used to compute the masked language modeling loss.\n2. Remove padding tokens and only compute the attention for non-padding tokens.\n3. Use a fused CUDA kernel (FMHA) that combines 4 steps into one kernel: computes QK T , take softmax, apply dropout, multiply by V , where Q, K, V are the query, key, and value respectively.\n4. Fuse matrix multiplication and adding bias into one CUDA kernel in the feed-forward network (FFN) layers. The gradient of the bias is also fused with the matrix multiplication the backward pass.\n5. Fuse matrix multiplication and adding bias into one CUDA kernel in the attention output projection.\n6. Fuse dropout and adding residual in the residual connection at the end on the attention and FFN blocks.\nWe train with DeepSpeed [88] ZeRO optimizer stage 1 to shard the optimizer states, thus reducing GPU memory usage and allowing us to use larger batch sizes. For the Nvidia MLPerf implementation, we report the speed for both Apex's automatic mix-precision (AMP) level O2 (as in the original implementation), and DeepSpeed ZeRO optimizer.", "publication_ref": ["b87"], "figure_ref": [], "table_ref": []}, {"heading": "E.5 Accelerated Multi-coil MRI Reconstruction", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.5.1 Background", "text": "In multi-coil MRI, multiple receiver coils (i.e. sensors) acquire complex-valued measurements in the spatial frequency (a.k.a. k-space) domain. These measurements are modulated by the spatially-varying sensitivity maps, which characterize the sensitivity of each coil to the imaging target. In accelerated MRI, scan times are reduced by decreasing the number of samples acquired in k-space. Because the data is sampled below the Nyquist rate, reconstructing the underlying image is an ill-posed problem.\nThe forward problem for accelerated multi-coil MRI can be written as the matrix equation\ny = \u2126F Sx +\nwhere \u2126 is the binary undersampling mask that indexes acquired samples in k-space, y is the vectorized measured signal in k-space, F is the discrete Fourier transform matrix, S is the receiver coil sensitivity maps, x is the ground-truth signal in image-space, and is additive complex Gaussian noise. The acceleration factor is given by R =  \n|N | i \u2126i |\u2126| .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Laurel Orr, Xun Huang, Trevor Gale, Jian Zhang, Victor Bittorf, Sarah Hooper, Neel Guha, and Michael Zhang for their helpful discussions and feedback on early drafts of the paper.\nWe gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Proof. Suppose M \u2208 MM * (b,n) . By Proposition C.10 we can write M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 \u2208 BD ( n b ,n) and R \u2208 BD (b,n) . Thus P (b,n) MP (b,n) = L 1 (P (b,n) RP (b,n) )L 2 . Letting L 1 = L 1 , L 2 = L * 2 , R 1 = P (b,n) RP (b,n) , and R 2 = I, we have L 1 , L 2 \u2208 BD ( n b ,n) , R 1 , R 2 \u2208 DB ( n b ,n) , and (b,n) and L 1 , L 2 \u2208 DB (b,n) . Thus by Theorem 3 (and the fact that BD (b,n) is closed under conjugate transposition) we can write R * 1 = P ( n b ,n) R 1 P ( n b ,n) = P (b,n) R 1 P (b,n) for some R 1 \u2208 DB ( n b ,n) , and similarly, can write R 2 = P (b,n) R 2 P (b,n) for some R 2 \u2208 DB ( n b ,n) . So P (b,n) MP (b,n) = R 1 (P (b,n) ) L * 1 )(L 2 P (b,n) ))R 2 = R 1 (P (b,n) L * 1 P (b,n) )(P (b,n) L 2 P (b,n) )R 2 = (R 1 L 1 )(L 2 R 2 ), where\nWe now show that the class M (b,n) strictly contains the class B (n) of n \u00d7 n butterfly matrices (as defined in Dao et al. [13]). We first show two elementary \"helper\" results.\nProof. Suppose R \u2208 BD (b,n) . Then by Proposition C.8, R[i, j] = 0 whenever i b = j b . Thus, whenever Proof. Suppose L \u2208 DB (c,n) . Then by Proposition C.9, L[i, j] = 0 whenever (i mod c) = (j mod c). Thus, whenever (i mod b) = (j mod b), L[i, j] = 0, since (i mod b) = (j mod b) implies (i mod c) = (j mod c) by the assumption that b divides c. Applying Proposition C.9 again, this means L \u2208 DB (b,n) as well.\nTheorem 4. Let n \u2265 4 be a power of 2. The class of matrices B (n) is a subset of the class M (b,n) , for all b \u2208 (1, n) that divide n. When n \u2265 512 it is a strict subset.\nProof. Recall from Section 2.2 that if B \u2208 B (n) , it has a butterfly factorization B = B n B n/2 . . . B 2 , where each B i \u2208 BF (n,i) .\nConsider multiplying together the factors B b B b/2 . . . B 2 (where b \u2208 (1, n) divides n). Since B i \u2208 BF (n,i) , by definition it is block diagonal with diagonal blocks of size i \u00d7 i; in other words, B i \u2208 BD (i,n) . Thus, each of the matrices B b , B b/2 , . . . , B 2 is in BD (b,n) (by Proposition C.12), i.e. block-diagonal with block size b \u00d7 b. This means their product B b B b/2 . . . B 2 is also block diagonal with block size b \u00d7 b, i.e., it is in BD (b,n) . Now, note that since B i \u2208 BF (n,i) , by definition it is a block matrix with blocks of size i/2 \u00d7 i/2, where each block is a diagonal matrix (note that some of these blocks are zero, except for the case of B n ). In other words, B i \u2208 DB (i/2,n) . Thus, for all i \u2208 {n, n/2, . . . , 2b}, B i \u2208 DB ((2b)/2,n) = DB (b,n) (by Proposition C.13). So, their product B n B n/2 . . . B 2b is in DB (b,n) as well, as by Theorem 3 we can write B n B n/2 . . . (b,n) , which means that B \u2208 M (b,n) (Definition C. 19).\nTo show that the inclusion is strict, notice that any M \u2208 M (b,n) is the product of L and R, where R \u2208 BD (b,n) and P (b,n) LP (b,n) \u2208 BD ( n b ,n) (by Theorem 3). Notice that the identity matrix is contained in both BD (b,n) and DB (b,n) . Suppose first that b \u2264 \u221a n. Then even if we set R to the identity, M has at Recall that in the notation of Definition C.17 we have b 2 = b 3 = b, so we are in the b 2 \u2264 b 3 case. To complete the proof, we will argue that R satisfies the two conditions in Proposition C.15. 6 Towards this end, let 0 \u2264 i, j < n be arbitrary indices and further, define i = (i 1 , i 0 ) b and j = (j 1 , j 0 ) b . Then note that \u03c3 (b,n3)\n. By Proposition C.18, we have that if i 0 mod b = j 0 , then L[i, j] = 0. Note that since i 0 , j 0 < b by definition, the condition i 0 mod b = j 0 is equivalent to saying i 0 = j 0 . Note that i 0 = j 0 satisfies the pre-condition for base size n3 b \u00d7 n2 b for indices (\u03c3 (b,n3) (i), \u03c3 (b,n2) (j)) in item 1 in Proposition C.15. Then by Eq. ( 12), we have that R [\u03c3 (b,n3) (i), \u03c3 (b,n2) (j)] = 0, which satisfies item 1 in Proposition C. 15. Now consider the case that i 0 = j mod b, which by the observation in the above paragraph is the same as i 0 = j 0 . Then by item 2 in Proposition C.18, we have that L\nNote that the above implies that\nwhere R \u2022 is as defined in the above paragraph. This means R \u2208 BD ", "publication_ref": ["b12", "b18", "b5", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Projection onto M", "text": "In Algorithm 1, we provide pseudocode for the algorithm outlined in Section 3.3. We now prove Theorem 1. Note that the rectangular matrix case generalizes naturally from the square matrix case, by replacing square blocks with rectangular blocks.\nProof of Theorem 1. As shown in Section 3.3, after reshaping the Monarch matrix M as a 4D tensor M jki and writing the two block-diagonal matrices L and R as 3D tensors L j k and R kji , we obtain:\nWe can similarly reshape the given matrix A into a 4D tensor A jki with size m \u00d7 m \u00d7 m \u00d7 m. of the blocks is singular is itself singular). Taken together, this means that each matrix M ij is invertible, since M ij = A i D ij C j and each of the matrices on the RHS of the equation is invertible.\nObserve that given a solution to the set of equations A i D ij C j = M ij , if we rescale and permute the matrices A i , D ij , C j appropriately, the result is still a solution to the equations. Specifically, let P be any permutation matrix and {S i } b i=1 , {S j } b j=1 be any invertible diagonal matrices (i.e., diagonal matrices without any zeros on the diagonal). Define D ij = S i P D ij PS j for all i, j. Notice that P D ij P = P \u22121 D ij P is diagonal because D ij is diagonal. Thus, D ij is diagonal (and invertible) since the product of diagonal matrices is diagonal. Define A i = A i PS \u22121 i and C j = P S \u22121 j C j for all i, j. Thus, we have that\nfor all i, j: in other words, we can scale the A i 's on the right by any invertible diagonal matrix, the C j 's on the left by any invertible diagonal matrix, and apply a matching permutation to the rows of the C j 's and the columns of the A i 's, and apply matching transformations to the D ij 's and the result will still be a valid factorization. This implies that as long as we recover a \"correct\"\u0108 1 up to a permutation and scaling of its rows, we can set theD i1 's andD 1j 's to the identity matrix, and then compute the remaining\u00c2 i 's and\u0108 j 's via the equations\u00c2\nTo understand how we can compute such a matrix\u0108 1 , define\nis diagonal for all i, j, i.e., C 1 simultaneously diagonalizes all the matrices F(i, j). (Note: In this paper, we say that a matrix Q \"simultaneously diagonalizes\" a set of matrices G 1 , . . . , G k if QG i Q \u22121 is a diagonal matrix for all 1 \u2264 i \u2264 k. Note that sometimes the opposite convention [i.e., Q \u22121 G i Q must be diagonal] is used in the literature; we adopt the former for notational convenience.) Indeed, if any matrix simultaneously diagonalizes all these matrices, then it leads to a valid factorization, which we show in the proof of Theorem 7. Therefore, we compute some matrix that simultaneously diagonalizes all these matrices, and set\u0108 1 to that matrix.\nThese ideas form the basis of Algorithm 2, which is presented formally below. Algorithm 2 uses simultaneous diagonalization as a subroutine; we discuss how to solve simultaneous diagonalization problems below.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Algorithm 2 MM * Factorization", "text": "Require: ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "E.1.2 Language Modeling", "text": "For dense models, we use standard implementations of GPT-2 [86] from Huggingface transformers library and from Nvidia's Megatron-LM repo. We follow the training recipe of the Megatron-LM repo.\nThe Monarch version of these models simply swap out the dense weight matrices in the attention blocks (projection matrices) and in the FFN block (linear layers) with Monarch matrices. We set the number of blocks in the block-diagonal matrices to 4. We also reduce the regularization strength (dropout) as our model is smaller.\nWe report the hyperparameters used in Table 10 and Table 11. We use an effective batch size of 512, and use gradient accumulation to fit into available GPU memory.\nWe measure the wall-clock training time on V100 GPUs.  ", "publication_ref": ["b85"], "figure_ref": [], "table_ref": []}, {"heading": "E.2 Details for PDE Solving", "text": "We adopt the experiment setting and data generation of Navier-Stokes Equation from FNO [65]. It considers the 2-d Navier-Stokes equation for a viscous, incompressible fliud in vorticity form on the unit tortus:\n\u2207w(x, t) = 0, x \u2208 (0, 1) 2 , t \u2208 (0, T ] ( 14)\nwhere u \u2208 C([, T 0]);H per ((0, 1) 2 ; R 2 )) for any r > 0 is the velocity field, w = \u2207 \u00d7 u is the vorticity, w 0 \u2208 L 2 per ((0, 1) 2 ; R) is the initial vorticity, v \u2208 R + is the viscosity coefficient, and f \u2208 L 2 per ((0, 1) 2 ; R) is the forcing function. T represents the time interval since it is time-dependent equation. v represents the viscosity. N represents the number of training pairs or data. Table 3 shows the results for viscosities v = 1e \u2212 3, 1e \u2212 4, 1e \u2212 5, T = 50, 30, 20 respectively and use N = 1000.", "publication_ref": ["b64"], "figure_ref": [], "table_ref": []}, {"heading": "E.3 Details for GPT-2 Downstream Tasks", "text": "We train Pixelfly-GPT2-small on a larger scale dataset, OpenWebText, and evaluate the downstream quality on zero-shot generation and classification tasks from [108], achieving comparable and even better performance to the dense model. Specifically, the datasets contains five popular classification tasks: SST2, Trec, CB, Agnews, and Dbpedia. We also adapated the calibrated metric from [108] for evaluation. Results for each individual task are shown in Table 12.", "publication_ref": ["b107", "b107"], "figure_ref": [], "table_ref": []}, {"heading": "E.5.2 Experimental Details", "text": "Dataset. We benchmark our method on the SKM-TEA Raw Data Track, which consists of dual-echo 3D MRI scans [20]. Scans are accelerated using Poisson Disc undersampling masks distributed with the dataset. During training, Poisson Disc masks are generated, cached, and applied to mask the k-space data to simulate accelerated scans.\nMatrix Shape. Like all matrices, Monarch matrices have an explicit shape constraint, which is a limitation of these matrices for MRI reconstruction tasks. Thus, the SKM-TEA dataset was filtered to include scans of shape 512 \u00d7 512 \u00d7 160, which is the most frequently occuring scan shape. A total of 3 scans were dropped from the original 155 scans in the dataset. Our method and all baselines were trained on this filtered dataset. Baselines. We compare our method to two baselines, SENSE and U-Net. Parameter count and hyperparameters are available in Table 13.\n\u2022 SENSE : SENSE performs a linear combination of the images acquired on each coil [84]. Here, the inverse fsat Fourier transform (IFFT) is applied to the acquired k-space for each coil. The resulting images are combined into a single complex image by weighting each coil image by corresponding coil sensitivity maps. In accelerated MRI, the unsampled frequencies are zero-valued; thus, SENSE produces a zero-filled image. Note, SENSE does not require any training.\n\u2022 U-Net: U-Net is a popular fully convolutional neural network baseline for MRI reconstruction [90]. We use the default implementation and hyperparameters used by Desai et al. [20] to benchmark the SKM-TEA dataset. In this approach, the SENSE-reconstructed zero-filled image is mapped to SENSE-reconstructed ground truth images.", "publication_ref": ["b19", "b83", "b89", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Monarch-SENSE (mSENSE):", "text": "We propose a modification to the SENSE method, in which the (IFFT) is parameterized by a factorized Monarch matrix. This matrix is initialized to the IFFT but, unlike SENSE, is learnable. While mSENSE is trainable, it has 137x fewer trainable parameters than U-Net.\nMetrics: We evaluate reconstruction performance using peak signal-to-noise ratio (pSNR) and structural similarity (SSIM) on both echoes (echo1 -E1, echo2 -E2) separately. Both metrics were computed on the 3D volume of each echo.\nExtended Results. We provide sample reconstructions of SENSE, mSENSE, and U-Net in data-limited settings for first (Fig. 6) and second (Fig. 7) echoes. Both SENSE and U-Net reconstructed images have aliasing artifacts. Due to the random Poisson Disc undersampling pattern, these artifacts are incoherent, causing them to manifest as blurring around fine structures and edges. In contrast, mSENSE can recover these structures with higher fidelity. Even in the second echo, which has lower signal-to-noise ratio (SNR) than the first echo, mSENSE does not overblur the image.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Sparse linear networks with a fixed butterfly structure: theory and practice", "journal": "PMLR", "year": "2021", "authors": "N Ailon; O Leibovitch; V Nair"}, {"ref_id": "b1", "title": "Approximate simultaneous diagonalization of matrices via structured low-rank approximation", "journal": "", "year": "2020", "authors": "R Akema; M Yamagishi; I Yamada"}, {"ref_id": "b2", "title": "FFTs in external or hierarchical memory", "journal": "The journal of Supercomputing", "year": "1990", "authors": "D H Bailey"}, {"ref_id": "b3", "title": "Numerical methods for simultaneous diagonalization", "journal": "SIAM Journal on Matrix Analysis and Applications", "year": "1993", "authors": "A Bunse-Gerstner; R Byers; V Mehrmann"}, {"ref_id": "b4", "title": "Prospective deployment of deep learning in MRI: A framework for important considerations, challenges, and recommendations for best practices", "journal": "Journal of Magnetic Resonance Imaging", "year": "2020", "authors": "A S Chaudhari; C M Sandino; E K Cole; D B Larson; G E Gold; S S Vasanawala; M P Lungren; B A Hargreaves; C P Langlotz"}, {"ref_id": "b5", "title": "Pixelated butterfly: Simple and efficient sparse training for neural network models", "journal": "", "year": "", "authors": "B Chen; T Dao; K Liang; J Yang; Z Song; A Rudra; C R\u00e9"}, {"ref_id": "b6", "title": "Generating long sequences with sparse transformers", "journal": "", "year": "2019", "authors": "R Child; S Gray; A Radford; I Sutskever"}, {"ref_id": "b7", "title": "Unifying orthogonal Monte Carlo methods", "journal": "", "year": "2019", "authors": "K Choromanski; M Rowland; W Chen; A Weller"}, {"ref_id": "b8", "title": "Unsupervised MRI reconstruction with generative adversarial networks", "journal": "", "year": "2020", "authors": "E K Cole; J M Pauly; S S Vasanawala; F Ong"}, {"ref_id": "b9", "title": "The minimal polynomial and some applications", "journal": "", "year": "", "authors": "K Conrad"}, {"ref_id": "b10", "title": "An algorithm for the machine calculation of complex fourier series", "journal": "Mathematics of computation", "year": "1965", "authors": "J W Cooley; J W Tukey"}, {"ref_id": "b11", "title": "Learning fast algorithms for linear transforms using butterfly factorizations", "journal": "", "year": "2019", "authors": "T Dao; A Gu; M Eichhorn; A Rudra; C R\u00e9"}, {"ref_id": "b12", "title": "Kaleidoscope: An efficient, learnable representation for all structured linear maps", "journal": "", "year": "", "authors": "T Dao; N Sohoni; A Gu; M Eichhorn; A Blonder; M Leszczynski; A Rudra; C R\u00e9"}, {"ref_id": "b13", "title": "Accelerated MRI with un-trained neural networks", "journal": "IEEE Transactions on Computational Imaging", "year": "2021", "authors": "M Z Darestani; R Heckel"}, {"ref_id": "b14", "title": "Measuring robustness in deep learning based compressive sensing", "journal": "", "year": "2021", "authors": "M Z Darestani; A Chaudhari; R Heckel"}, {"ref_id": "b15", "title": "A two-pronged progress in structured dense matrix vector multiplication", "journal": "SIAM", "year": "2018", "authors": "C De Sa; A Gu; R Puttagunta; C R\u00e9; A Rudra"}, {"ref_id": "b16", "title": "Imagenet: A large-scale hierarchical image database", "journal": "Ieee", "year": "2009", "authors": "J Deng; W Dong; R Socher; L.-J Li; K Li; L Fei-Fei"}, {"ref_id": "b17", "title": "Physics-driven data augmentations for consistency training for robust accelerated MRI reconstruction", "journal": "", "year": "2021", "authors": "A D Desai; B Gunel; B M Ozturkler; H Beg; S Vasanawala; B A Hargreaves; C R\u00e9; J M Pauly; A S Chaudhari;  Vortex"}, {"ref_id": "b18", "title": "Noise2recon: A semi-supervised framework for joint MRI reconstruction and denoising", "journal": "", "year": "2021", "authors": "A D Desai; B M Ozturkler; C M Sandino; S Vasanawala; B A Hargreaves; C M Re; J M Pauly; A S Chaudhari"}, {"ref_id": "b19", "title": "SKM-TEA: A dataset for accelerated MRI reconstruction with dense image labels for quantitative clinical evaluation", "journal": "", "year": "", "authors": "A D Desai; A M Schmidt; E B Rubin; C M Sandino; M S Black; V Mazzoli; K J Stevens; R Boutin; C Re; G E Gold"}, {"ref_id": "b20", "title": "Sparse networks from scratch: Faster training without losing performance", "journal": "", "year": "2019", "authors": "T Dettmers; L Zettlemoyer"}, {"ref_id": "b21", "title": "Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "J Devlin; M.-W Chang; K Lee; K Toutanova;  Bert"}, {"ref_id": "b22", "title": "Learning to prune deep neural networks via layer-wise optimal brain surgeon", "journal": "", "year": "2017", "authors": "X Dong; S Chen; S J Pan"}, {"ref_id": "b23", "title": "An image is worth 16x16 words: Transformers for image recognition at scale", "journal": "", "year": "2020", "authors": "A Dosovitskiy; L Beyer; A Kolesnikov; D Weissenborn; X Zhai; T Unterthiner; M Dehghani; M Minderer; G Heigold; S Gelly"}, {"ref_id": "b24", "title": "Fast discrete polynomial transforms with applications to data analysis for distance transitive graphs", "journal": "SIAM Journal on Computing", "year": "1997", "authors": "J R Driscoll; D M Healy; D N Rockmore"}, {"ref_id": "b25", "title": "The approximation of one matrix by another of lower rank", "journal": "Psychometrika", "year": "1936", "authors": "C Eckart; G Young"}, {"ref_id": "b26", "title": "On a new class of structured matrices. Integral Equations and Operator Theory", "journal": "", "year": "1999", "authors": "Y Eidelman; I Gohberg"}, {"ref_id": "b27", "title": "The difficulty of training sparse neural networks", "journal": "", "year": "2019", "authors": "U Evci; F Pedregosa; A Gomez; E Elsen"}, {"ref_id": "b28", "title": "Solving inverse problems in steady-state navier-stokes equations using deep neural networks", "journal": "", "year": "2020", "authors": "T Fan; K Xu; J Pathak; E Darve"}, {"ref_id": "b29", "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "journal": "", "year": "2018", "authors": "J Frankle; M Carbin"}, {"ref_id": "b30", "title": "Stabilizing the lottery ticket hypothesis", "journal": "", "year": "2019", "authors": "J Frankle; G K Dziugaite; D M Roy; M Carbin"}, {"ref_id": "b31", "title": "Linear mode connectivity and the lottery ticket hypothesis", "journal": "PMLR", "year": "2020", "authors": "J Frankle; G K Dziugaite; D Roy; M Carbin"}, {"ref_id": "b32", "title": "The state of sparsity in deep neural networks", "journal": "", "year": "2019", "authors": "T Gale; E Elsen; S Hooker"}, {"ref_id": "b33", "title": "A framework for few-shot language model evaluation", "journal": "", "year": "2021-09", "authors": "L Gao; J Tow; S Biderman; S Black; A Dipofi; C Foster; L Golding; J Hsu; K Mcdonell; N Muennighoff; J Phang; L Reynolds; E Tang; A Thite; B Wang; K Wang; A Zou"}, {"ref_id": "b34", "title": "Transformer feed-forward layers are key-value memories", "journal": "", "year": "2020", "authors": "M Geva; R Schuster; J Berant; O Levy"}, {"ref_id": "b35", "title": "", "journal": "", "year": "2019", "authors": "A Gokaslan; V Cohen; P Ellie; S Tellex"}, {"ref_id": "b36", "title": "Toeplitz and circulant matrices: A review. Foundations and Trends\u00ae in Communications and Information Theory", "journal": "", "year": "2006", "authors": "R M Gray"}, {"ref_id": "b37", "title": "GPU kernels for block-sparse weights", "journal": "", "year": "2017", "authors": "S Gray; A Radford; D P Kingma"}, {"ref_id": "b38", "title": "Generalized autocalibrating partially parallel acquisitions (grappa)", "journal": "An Official Journal of the International Society for Magnetic Resonance in Medicine", "year": "2002", "authors": "M A Griswold; P M Jakob; R M Heidemann; M Nittka; V Jellus; J Wang; B Kiefer; A Haase"}, {"ref_id": "b39", "title": "Hippo: Recurrent memory with optimal polynomial projections", "journal": "", "year": "", "authors": "A Gu; T Dao; S Ermon; A Rudra; C R\u00e9"}, {"ref_id": "b40", "title": "Accelerating sparse dnn models without hardware-support via tile-wise sparsity", "journal": "IEEE", "year": "2020", "authors": "C Guo; B Y Hsueh; J Leng; Y Qiu; Y Guan; Z Wang; X Jia; X Li; M Guo; Y Zhu"}, {"ref_id": "b41", "title": "Low-rank modeling of local k-space neighborhoods (loraks) for constrained MRI", "journal": "IEEE transactions on medical imaging", "year": "2013", "authors": "J P Haldar"}, {"ref_id": "b42", "title": "Learning a variational network for reconstruction of accelerated MRI data. Magnetic resonance in medicine", "journal": "", "year": "2018", "authors": "K Hammernik; T Klatzer; E Kobler; M P Recht; D K Sodickson; T Pock; F Knoll"}, {"ref_id": "b43", "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "journal": "", "year": "2015", "authors": "S Han; H Mao; W J Dally"}, {"ref_id": "b44", "title": "Learning both weights and connections for efficient neural networks", "journal": "", "year": "2015", "authors": "S Han; J Pool; J Tran; W J Dally"}, {"ref_id": "b45", "title": "Dense-sparse-dense training for deep neural networks", "journal": "", "year": "2016", "authors": "S Han; J Pool; S Narang; H Mao; E Gong; S Tang; E Elsen; P Vajda; M Paluri; J Tran"}, {"ref_id": "b46", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "K He; X Zhang; S Ren; J Sun"}, {"ref_id": "b47", "title": "The hardware lottery", "journal": "", "year": "2020", "authors": "S Hooker"}, {"ref_id": "b48", "title": "Computed tomography: principles, design, artifacts, and recent advances", "journal": "SPIE press", "year": "2003", "authors": "J Hsieh"}, {"ref_id": "b49", "title": "Top-K always sparse training", "journal": "", "year": "2021", "authors": "S M Jayakumar; R Pascanu; J W Rae; S Osindero; E Elsen;  Top-Kast"}, {"ref_id": "b50", "title": "Gotta go fast when generating data with score-based models", "journal": "", "year": "2021", "authors": "A Jolicoeur-Martineau; K Li; R Pich\u00e9-Taillefer; T Kachman; I Mitliagkas"}, {"ref_id": "b51", "title": "Speech and language processing", "journal": "Pearson London", "year": "2014", "authors": "D Jurafsky; J H Martin"}, {"ref_id": "b52", "title": "Displacement ranks of matrices and linear equations", "journal": "Journal of Mathematical Analysis and Applications", "year": "1979", "authors": "T Kailath; S.-Y Kung; M Morf"}, {"ref_id": "b53", "title": "Scaling laws for neural language models", "journal": "", "year": "2020", "authors": "J Kaplan; S Mccandlish; T Henighan; T B Brown; B Chess; R Child; S Gray; A Radford; J Wu; Amodei ; D "}, {"ref_id": "b54", "title": "Sparse factorization of large square matrices", "journal": "", "year": "2021", "authors": "R Khalitov; T Yu; L Cheng; Yang ; Z "}, {"ref_id": "b55", "title": "Neural controlled differential equations for irregular time series", "journal": "", "year": "2020", "authors": "P Kidger; J Morrill; J Foster; T Lyons"}, {"ref_id": "b56", "title": "A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J Ba;  Adam"}, {"ref_id": "b57", "title": "Deep-learning methods for parallel magnetic resonance imaging reconstruction: A survey of the current approaches, trends, and issues. IEEE signal processing magazine", "journal": "", "year": "2020", "authors": "F Knoll; K Hammernik; C Zhang; S Moeller; T Pock; D K Sodickson; M Akcakaya"}, {"ref_id": "b58", "title": "Machine learningaccelerated computational fluid dynamics", "journal": "Proceedings of the National Academy of Sciences", "year": "", "authors": "D Kochkov; J A Smith; A Alieva; Q Wang; M P Brenner; S Hoyer"}, {"ref_id": "b59", "title": "Block pruning for faster transformers", "journal": "", "year": "2021", "authors": "F Lagunas; E Charlaix; V Sanh; A M Rush"}, {"ref_id": "b60", "title": "Blind primed supervised (blips) learning for mr image reconstruction", "journal": "", "year": "2021", "authors": "A Lahiri; G Wang; S Ravishankar; J A Fessler"}, {"ref_id": "b61", "title": "Fastfood-computing hilbert space expansions in loglinear time", "journal": "", "year": "2013", "authors": "Q Le; T Sarl\u00f3s; A Smola"}, {"ref_id": "b62", "title": "Flexible multilayer sparse approximations of matrices and applications", "journal": "IEEE Journal of Selected Topics in Signal Processing", "year": "2016", "authors": "Le Magoarou; L Gribonval; R "}, {"ref_id": "b63", "title": "Pruning filters for efficient convnets", "journal": "", "year": "2016", "authors": "H Li; A Kadav; I Durdanovic; H Samet; H P Graf"}, {"ref_id": "b64", "title": "Fourier neural operator for parametric partial differential equations", "journal": "", "year": "2020", "authors": "Z Li; N B Kovachki; K Azizzadenesheli; K Bhattacharya; A Stuart; A Anandkumar"}, {"ref_id": "b65", "title": "Runtime neural pruning. In Guyon, I", "journal": "Curran Associates, Inc", "year": "2017", "authors": "J Lin; Y Rao; J Lu; J ; Zhou; U V Luxburg; S Bengio; H Wallach; R Fergus; S Vishwanathan; Garnett "}, {"ref_id": "b66", "title": "Deformable butterfly: A highly structured and sparse linear transform", "journal": "", "year": "", "authors": "R Lin; J Ran; K H Chiu; G Chesi; N Wong"}, {"ref_id": "b67", "title": "Finding trainable sparse networks through neural tangent transfer", "journal": "PMLR", "year": "2020", "authors": "T Liu; F Zenke"}, {"ref_id": "b68", "title": "Sparse MRI: The application of compressed sensing for rapid mr imaging", "journal": "Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine", "year": "2007", "authors": "M Lustig; D Donoho; Pauly ; J M "}, {"ref_id": "b69", "title": "Deep generative adversarial neural networks for compressive sensing MRI", "journal": "IEEE transactions on medical imaging", "year": "2018", "authors": "M Mardani; E Gong; J Y Cheng; S S Vasanawala; G Zaharchuk; L Xing; Pauly ; J M "}, {"ref_id": "b70", "title": "Differentiable multiple shooting layers", "journal": "", "year": "2021", "authors": "S Massaroli; M Poli; S Sonoda; T Suzuki; J Park; A Yamashita; H Asama"}, {"ref_id": "b71", "title": "Mlperf training benchmark", "journal": "", "year": "2020", "authors": "P Mattson; C Cheng; G Diamos; C Coleman; P Micikevicius; D Patterson; H Tang; G.-Y Wei; P Bailis; V Bittorf"}, {"ref_id": "b72", "title": "Pointer sentinel mixture models", "journal": "", "year": "2016", "authors": "S Merity; C Xiong; J Bradbury; R Socher"}, {"ref_id": "b73", "title": "ACDC: a structured efficient linear layer", "journal": "", "year": "2016", "authors": "M Moczulski; M Denil; J Appleyard; N Freitas"}, {"ref_id": "b74", "title": "One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers", "journal": "", "year": "2019", "authors": "A S Morcos; H Yu; M Paganini; Y Tian"}, {"ref_id": "b75", "title": "Quadrature-based features for kernel approximation", "journal": "Curran Associates, Inc", "year": "2018", "authors": "M Munkhoeva; Y Kapushev; E Burnaev; I Oseledets; S Bengio; H Wallach; H Larochelle; K Grauman; N Cesa-Bianchi; Garnett "}, {"ref_id": "b76", "title": "Beyond low rank+ sparse: Multiscale low rank matrix decomposition", "journal": "IEEE journal of selected topics in signal processing", "year": "2016", "authors": "F Ong; M Lustig"}, {"ref_id": "b77", "title": "Logarithmic pruning is all you need", "journal": "Advances in Neural Information Processing Systems", "year": "2020", "authors": "L Orseau; M Hutter; O Rivasplata"}, {"ref_id": "b78", "title": "Structured matrices and polynomials: unified superfast algorithms", "journal": "Springer Science & Business Media", "year": "2012", "authors": "V Y Pan"}, {"ref_id": "b79", "title": "Random butterfly transformations with applications in computational linear algebra", "journal": "", "year": "1995", "authors": "D S Parker"}, {"ref_id": "b80", "title": "Optimal lottery tickets via subsetsum: Logarithmic over-parameterization is sufficient", "journal": "", "year": "2020", "authors": "A Pensia; S Rajput; A Nagle; H Vishwakarma; D Papailiopoulos"}, {"ref_id": "b81", "title": "Ac/dc: Alternating compressed/decompressed training of deep neural networks", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "A Peste; E Iofinova; A Vladu; Alistarh ; D "}, {"ref_id": "b82", "title": "Toward fast continuousdepth models", "journal": "", "year": "2020", "authors": "M Poli; S Massaroli; A Yamashita; H Asama; J Park"}, {"ref_id": "b83", "title": "Sense: sensitivity encoding for fast MRI", "journal": "Magnetic Resonance in Medicine: An Official Journal of the International Society for Magnetic Resonance in Medicine", "year": "1999", "authors": "K P Pruessmann; M Weiger; M B Scheidegger; P Boesiger"}, {"ref_id": "b84", "title": "Universal differential equations for scientific machine learning", "journal": "", "year": "2020", "authors": "C Rackauckas; Y Ma; J Martensen; C Warner; K Zubov; R Supekar; D Skinner; A Ramadhan; A Edelman"}, {"ref_id": "b85", "title": "Language models are unsupervised multitask learners", "journal": "OpenAI blog", "year": "2019", "authors": "A Radford; J Wu; R Child; D Luan; D Amodei; I Sutskever"}, {"ref_id": "b86", "title": "Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations", "journal": "Journal of Computational Physics", "year": "2019", "authors": "M Raissi; P Perdikaris; G E Karniadakis"}, {"ref_id": "b87", "title": "Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters", "journal": "", "year": "2020", "authors": "J Rasley; S Rajbhandari; O Ruwase; Y He"}, {"ref_id": "b88", "title": "Low-rank and adaptive sparse signal (lassi) models for highly accelerated dynamic imaging", "journal": "IEEE transactions on medical imaging", "year": "2017", "authors": "S Ravishankar; B E Moore; R R Nadakuditi; J A Fessler"}, {"ref_id": "b89", "title": "U-net: Convolutional networks for biomedical image segmentation", "journal": "Springer", "year": "2015", "authors": "O Ronneberger; P Fischer; T Brox"}, {"ref_id": "b90", "title": "Compressed sensing: From research to clinical practice with deep neural networks: Shortening scan times for magnetic resonance imaging", "journal": "IEEE signal processing magazine", "year": "2020", "authors": "C M Sandino; J Y Cheng; F Chen; M Mardani; J M Pauly; S S Vasanawala"}, {"ref_id": "b91", "title": "Movement pruning: Adaptive sparsity by fine-tuning", "journal": "", "year": "2020", "authors": "V Sanh; T Wolf; A M Rush"}, {"ref_id": "b92", "title": "A generalized solution of the orthogonal procrustes problem", "journal": "Psychometrika", "year": "1966", "authors": "P H Sch\u00f6nemann"}, {"ref_id": "b93", "title": "Megatron-LM: Training multi-billion parameter language models using model parallelism", "journal": "", "year": "2019", "authors": "M Shoeybi; M Patwary; R Puri; P Legresley; J Casper; B Catanzaro"}, {"ref_id": "b94", "title": "Structured transforms for small-footprint deep learning", "journal": "", "year": "2015", "authors": "V Sindhwani; T Sainath; S Kumar"}, {"ref_id": "b95", "title": "Pruning neural networks without any data by iteratively conserving synaptic flow", "journal": "", "year": "2020", "authors": "H Tanaka; D Kunin; D L Yamins; S Ganguli"}, {"ref_id": "b96", "title": "Sparse matrices", "journal": "Academic Press", "year": "1973", "authors": "R P Tewarson"}, {"ref_id": "b97", "title": "Learning compressed transforms with low displacement rank", "journal": "", "year": "2018", "authors": "A Thomas; A Gu; T Dao; A Rudra; C R\u00e9"}, {"ref_id": "b98", "title": "Mixer: An all-mlp architecture for vision", "journal": "", "year": "2021", "authors": "I Tolstikhin; N Houlsby; A Kolesnikov; L Beyer; X Zhai; T Unterthiner; J Yung; D Keysers; J Uszkoreit; M Lucic"}, {"ref_id": "b99", "title": "Spectral methods in MATLAB. SIAM", "journal": "", "year": "2000", "authors": "L N Trefethen"}, {"ref_id": "b100", "title": "Butterfly transform: An efficient fft based neural architecture design", "journal": "IEEE", "year": "2020", "authors": "K A Vahid; A Prabhu; A Farhadi; M Rastegari"}, {"ref_id": "b101", "title": "Picking winning tickets before training by preserving gradient flow", "journal": "", "year": "2020", "authors": "C Wang; G Zhang; R Grosse"}, {"ref_id": "b102", "title": "Towards physics-informed deep learning for turbulent flow prediction", "journal": "", "year": "2020", "authors": "R Wang; K Kashinath; M Mustafa; A Albert; Yu ; R "}, {"ref_id": "b103", "title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020-10", "authors": "T Wolf; L Debut; V Sanh; J Chaumond; C Delangue; A Moi; P Cistac; T Rault; R Louf; M Funtowicz; J Davison; S Shleifer; P Von Platen; C Ma; Y Jernite; J Plu; C Xu; T L Scao; S Gugger; M Drame; Q Lhoest; A M Rush"}, {"ref_id": "b104", "title": "Self-supervised physics-based deep learning MRI reconstruction without fully-sampled data", "journal": "IEEE", "year": "2020", "authors": "B Yaman; S A H Hosseini; S Moeller; J Ellermann; K Ugurbil; M Ak\u00e7akaya"}, {"ref_id": "b105", "title": "Orthogonal random features", "journal": "Curran Associates, Inc", "year": "2016", "authors": "F X Yu; A T Suresh; K M Choromanski; D N Holtmann-Rice; S Kumar"}, {"ref_id": "b106", "title": "Tokens-to-token ViT: Training vision transformers from scratch on imagenet", "journal": "", "year": "2021", "authors": "L Yuan; Y Chen; T Wang; W Yu; Y Shi; F E Tay; J Feng; Yan ; S "}, {"ref_id": "b107", "title": "Calibrate before use: Improving few-shot performance of language models", "journal": "", "year": "2021", "authors": "T Z Zhao; E Wallace; S Feng; D Klein; S Singh"}, {"ref_id": "b108", "title": "To prune, or not to prune: exploring the efficacy of pruning for model compression", "journal": "", "year": "2017", "authors": "M Zhu; S Gupta"}, {"ref_id": "b109", "title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Y Zhu; R Kiros; R Zemel; R Salakhutdinov; R Urtasun; A Torralba; S Fidler"}, {"ref_id": "b110", "title": "refers to the small and base ViT models respectively, and 16 refers to the patch size of 16x16. The MLP-Mixer models follow the same convention", "journal": "", "year": "", "authors": "-S Vit; -B Vit"}], "figures": [{"figure_label": "31", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Definition 3 . 1 .31Let n = m 2 . An n \u00d7 n Monarch matrix has the form: M = PLP R, where L and R are block-diagonal matrices, each with m blocks of size m \u00d7 m, and P is the permutation that maps [x 1 , . . . , x n ] to [x 1 , x 1+m , . . . , x 1+(m\u22121)m , x 2 , x 2+m , . . . , x 2+(m\u22121)m , . . . , x m , x 2m , . . . , x n ].", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: With the \"reverse sparsification\" process, Monarch matrices can speed up GPT-2 training by 2x.", "figure_data": ""}, {"figure_label": "33", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Assumption 3 . 3 .33and the permutation P of Definition 3.1. Then, we can compute L 1 , L 2 , R in such a factorization under Assumption 3.3, as stated in Theorem 2. (Note that the factorization is not unique.) Assume that (1) M \u2208 MM * is invertible and (2) M can be written as (PL 1 P )R(PL 2 P ) where the blocks of R have no zero entries.", "figure_data": ""}, {"figure_label": "24", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Theorem 2 .Figure 4 :24Figure 4: With Algorithm 1 for our Monarch parameterization, we can convert a pretrained model into a model with Monarch weight matrices and speed up downstream fine-tuning.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "5. 11End-to-End Training 5.1.1 Benchmark Tasks: Image Classification, Language Modeling", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 5 :5Figure 5: Time required (in A100 GPU hours) to reach the same perplexity (18.0) for GPT-2-small on OpenWebText. With \"reverse sparsification\", Monarch can speed up GPT-2 training by 2\u00d7.serves as a proof of concept, and we expect further speedup if additional model compression techniques are applied (e.g., quantization, kernel fusion).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "(n)  inDefinition 3.1. (In other words, M (n) is shorthand for M ( \u221a n,n) .) Note that a matrix in M (b,n) is represented by n 2 b + nb parameters. We remark that M (b,n) \u2283 B (n) for all block sizes b \u2208 (1, n) that divide n. Based on Definition C.19, we define the classes MM * (b,n) and M * M (b,n) :: Definition C.4 (Class MM * (b,n) , M * M (b,n) ). Let b \u2208 (1, n) be an integer that divides n and suppose M 1 , M 2 \u2208 M (b,n) . We define MM * (b,n) to be the the class of all matrices M expressible in the form M = M 1 M * 2 . We define M * M (b,n) to be the the class of all matrices M expressible in the form M = M * 1 M 2 .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "(n) ) O(d) O(s/n) has O(ds log s) parameters and runtime, compared to the O(s) parameters and runtime of the circuit. Note that at worst, s is O(n 2 ).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "d) O(s/n) . The class (MM * (f (n,s),n) ) O(d) O(s/n) has O(d s 2 f (n,s) + dsf (n, s)) = O(ds 3/2 ) parameters. Thus, the monarch representation of A is suboptimal by at most an O(d \u221a s) factor compared to the O(d log s) of kaleidoscope.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "22in the factorization of M. In order to compute this factorization, we require the following assumption on M: Assumption D.1. Assume that (1) M \u2208 MM * (b,n) is invertible and (2) M can be written as (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) where L 1 , L 2 \u2208 BD ( n b ,n) , R \u2208 BD (b,n) , and R has no nonzero entries in its diagonal blocks. (Note that by Proposition C.10, we can write any M \u2208 MM * (b,n) as (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ); thus, (2) is merely the assumption that R has no zero entries in its blocks.) This is analogous to Assumption 3.3, except applicable to the more general block size b. We now present Algorithm 2 to find factors L 1 , R, L 2 of matrices satisfying Assumption D.1. First, observe that if we define M = P (b,n) MP (b,n) , we have M = L 1 (P (b,n) RP (b,n) )L 2 . By Theorem 3, the matrix P (b,n) RP (b,n) is in DB ( n b ,n) , i.e., is a block matrix with blocks of size n b \u00d7 n b", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "where A 11, . . . , A b are n b \u00d7 n b matrices that are the diagonal blocks of L 1 ; C 1 , . . . , C b are n b \u00d7 n b matrices that are the diagonal blocks of L 2 ; D 11 , . . . , D 1b , D 21 , . . . , D 2b , . . . , D b1 , . . . , D bb are n b \u00d7 n b diagonal matrices that are the blocks of P (b,n) RP (b,n) ; and M 11 , . . . , M 1b , M 21 , . . . , M 2b , . . . , M b1 , . . . , M bb are n b \u00d7 n b matrices that are the blocks of M = P (b,n) MP (b,n) .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "L 1 =P1diag(\u00c2 1 , . . . ,\u00c2 b ), L 2 = diag(\u0108 1 , . . . ,\u0108 b ), and R = P (b,n) (b,n) , and we have M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ) with L 1 , L 2 \u2208 BD ( n b ,n) and R \u2208 BD (b,n) as argued above. This completes the proof of correctness. Now, we analyze the runtime. There are b 2 matrices F(i, j) to compute, and computing each one takes O( n 3 b 3 ) time. Once we've found\u0108 1 , there are b matrices\u00c2 i to compute, each one taking O( n 3 b 3 ) time, and b \u2212 1 matrices\u0108 j (for j \u2265 2) to compute, each one taking O( n 3 b 3 ) time, and then b 2 matricesD ij to compute, each taking O( n 3", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "n b \u00d7 n b . This leads to a total runtime of O b 2 \u2022 ( n b ) 3 = O n 3 b", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Figure 6 :6Figure 6: Sample reconstructions at 2x acceleration for the first echo in the SKM-TEA dataset using SENSE, Monarch-SENSE (mSENSE), and U-Net. Both mSENSE and U-Net are trained with 1 training scan. SENSE is an untrained method.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 7 :7Figure 7: Sample reconstructions at 2x acceleration for the second echo in the SKM-TEA dataset using SENSE, Monarch SENSE (mSENSE), and U-Net. Both mSENSE and U-Net are trained with 1 training scan. SENSE is an untrained method.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "text, images, PDEs, MRI), in three settings (E2E training, S2D training, and D2S fine-tuning): \u2022 In Section 5.1.1, on image classification and language modeling benchmarks, such as ViT / MLP Mixer on ImageNet and GPT-2 on Wikitext-103, Monarch is 2\u00d7 faster to train than dense models, while achieving the same accuracy / perplexity. In Section 5.1.2, in scientific and medical domains where special transforms (Fourier) are common, Monarch outperforms Fourier transform based methods on PDE solving, with up to 40% lower error, and on MRI reconstruction attains up to 15% higher pSNR and 3.8% higher SSIM. \u2022 In Section 5.1.2, we show that on the large OpenWebText dataset, reverse sparsification (training with Monarch weight matrices for most of the time, then transitioning to dense weight matrices) speeds up the pretraining of GPT-2 models by 2\u00d7 compared to the dense model, with no loss in upstream or downstream quality. Moreover, reverse sparsification speeds up BERT pretraining by 23% even compared to the implementation from Nvidia that set the MLPerf [72] 1.1 record. \u2022 In Section 5.3, as a proof of concept, we demonstrate that our Monarch approximation algorithm can improve fine-tuning efficiency for pretrained models. We show that compressing BERT to a Monarch matrix model performs comparably to a finetuned dense model on GLUE, with 2\u00d7 fewer parameters and 1.7\u00d7 faster finetuning speed.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The performance of Monarch matrices and ViT / MLP-Mixer on ImageNet, including the number of parameters and FLOPs. We measure the Top-1 accuracy and the training time speedup compared to the corresponding dense model.", "figure_data": "ModelImageNet acc. Speedup Params FLOPsMixer-S/1674.0-18.5M3.8GMonarch-Mixer-S/1673.71.7\u00d77.0M1.5GMixer-B/1677.7-59.9M 12.6GMonarch-Mixer-B/1677.81.9\u00d720.9M5.0GViT-S/1679.4-48.8M9.9GMonarch-ViT-S/1679.11.9\u00d719.6M3.9GViT-B/1678.5-86.6M 17.6GMonarch-ViT-B/1678.92.0\u00d733.0M5.9G"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Performance of Monarch matrices and GPT-2-Small/Medium on WikiText-103, including the # of parameters and FLOPs. Monarch achieves similar perplexity (ppl) but 2.0\u00d7 faster.", "figure_data": "ModelPPL Speedup Params FLOPsGPT-2-Small20.6-124M106GMonarch-GPT-2-Small20.71.8\u00d772M51GGPT-2-Medium20.9-355M361GMonarch-GPT-2-Medium20.32.0\u00d7165M166G"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Benchmarks on Navier-Stokes (fixing resolution 64 \u00d7 64 for both training and testing). Decreasing the viscosity coefficient \u03bd makes the dynamics more chaotic.", "figure_data": "Modelv = 10 \u22123v = 10 \u22124v = 10 \u22125U-Net0.0250.2050.198TF-Net0.0230.2250.227ResNet0.0700.2870.275FNO0.0170.1780.155Monarch-NO0.0100.1450.136"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Mean \u00b1 standard error of the mean of conventional and Monarch-SENSE (mSENSE) on dual-echo (E1,E2) MRI reconstruction at multiple acceleration factors (Acc.).", "figure_data": "pSNR (dB) (\u2191)SSIM (\u2191)Acc.ModelE1E2E1E22SENSE mSENSE32.8\u00b10.2 34.3\u00b10.235.4\u00b10.2 36.6\u00b10.20.871\u00b10.003 0.886\u00b10.0020.865\u00b10.003 0.882\u00b10.0033SENSE mSENSE30.9\u00b10.2 32.3\u00b10.233.5\u00b10.2 34.6\u00b10.20.819\u00b10.004 0.843\u00b10.0030.795\u00b10.004 0.820\u00b10.0044SENSE mSENSE30.1\u00b10.2 31.2\u00b10.232.8\u00b10.2 33.5\u00b10.20.789\u00b10.004 0.812\u00b10.0030.753\u00b10.005 0.767\u00b10.005"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Impact of number of training examples (N ) on dual-echo MRI reconstruction at 2x acceleration.", "figure_data": "pSNR (dB) (\u2191)SSIM (\u2191)NModelE1E2E1E2N/ASENSE32.8\u00b10.235.4\u00b10.20.871\u00b10.0030.865\u00b10.0031U-Net mSENSE29.4\u00b10.2 33.8\u00b10.234.4\u00b10.3 36.0\u00b10.20.848\u00b10.004 0.886\u00b10.0030.857\u00b10.004 0.867\u00b10.0032U-Net mSENSE29.9\u00b10.3 34.0\u00b10.235.1\u00b10.3 36.4\u00b10.20.858\u00b10.003 0.883\u00b10.0020.871\u00b10.003 0.877\u00b10.0033U-Net mSENSE31.0\u00b10.3 33.9\u00b10.235.2\u00b10.3 36.5\u00b10.20.866\u00b10.003 0.882\u00b10.0020.867\u00b10.004 0.878\u00b10.0035U-Net mSENSE31.4\u00b10.3 33.9\u00b10.235.6\u00b10.2 36.5\u00b10.20.877\u00b10.002 0.881\u00b10.0020.870\u00b10.003 0.877\u00b10.003"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": "ModelOpenWebText (ppl) Speedup Classification (avg acc)GPT-2m18.0-38.9Monarch-GPT-2m18.02\u00d738.8"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "The total training time of BERT-large trained with Monarch reverse sparsification and with conventional dense training on 8 A100-40GB GPUs (DGX A100). Training consists of two phases, phase 1 with sequence length 128 and phase 2 with sequence length 512. Monarch training is 3.5x faster than HuggingFace and 23% faster than Nvidia's MLPerf 1.1 implementation.", "figure_data": "ImplementationTraining time (h)HuggingFace84.5MegaTron52.5Nvidia MLPerf 1.130.2Nvidia MLPerf 1.1 + DeepSpeed29.3Monarch (ours)23.8"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "The performance of Monarch matrices in finetuning BERT on GLUE.No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), and members of the Stanford DAWN project: Facebook, Google, and VMWare. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government.", "figure_data": "ModelGLUE (avg) Speedup Params FLOPsBERT-base78.6-109M11.2GMonarch-BERT-base78.31.5\u00d755M6.2GBERT-large80.4-335M39.5GMonarch-BERT-large79.61.7\u00d7144M14.6G"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Configuration of the ImageNet experiment", "figure_data": "ModelOptimizer Weight Decay Learning Rate Drop Path Warmup/EpochViT-SmallAdamW0.050.0010.15/300Monarch-ViT-SmallAdamW0.050.00105/300ViT-BaseAdamW0.050.0010.15/300Monarch-ViT-BaseAdamW0.050.00105/300Mixer-SmallAdamW0.10.0010.15/300Monarch-Mixer-SmallAdamW0.10.00105/300Mixer-BaseAdamW0.10.0010.15/300Monarch-Mixer-BaseAdamW0.10.00105/300"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "The performance (accuracy) of GPT-2-medium trained with Monarch reverse sparsification and with conventional dense training on text classification benchmarks.", "figure_data": "ModelOpenWebText (ppl) Speedup Classification (avg acc)GPT-2m68.337.010.752.0 26.6Monarch-GPT-2m7238.612.547.3 23.0"}], "formulas": [{"formula_id": "formula_0", "formula_text": "D 1 D 2 D 3 D 4", "formula_coordinates": [4.0, 410.85, 123.71, 35.98, 21.64]}, {"formula_id": "formula_1", "formula_text": "diag B 1 , B 2 , . . . , B n k ,", "formula_coordinates": [4.0, 255.57, 184.96, 100.86, 11.7]}, {"formula_id": "formula_2", "formula_text": "M = B n B n/2 . . . B 2 ,", "formula_coordinates": [4.0, 261.16, 240.54, 89.69, 9.99]}, {"formula_id": "formula_3", "formula_text": "M = B n M 1 0 0 M 2 ,", "formula_coordinates": [4.0, 258.51, 283.16, 94.99, 21.64]}, {"formula_id": "formula_4", "formula_text": "B n \u2208 BF (n,n) and M 1 , M 2 \u2208 B ( n 2 )", "formula_coordinates": [4.0, 100.45, 311.66, 149.53, 12.54]}, {"formula_id": "formula_5", "formula_text": "M 1 M * 2 for M 1 , M 2 \u2208 B (", "formula_coordinates": [4.0, 128.57, 336.89, 113.99, 12.2]}, {"formula_id": "formula_6", "formula_text": "w i=1 M i [1:n, 1:n]", "formula_coordinates": [4.0, 79.33, 348.97, 74.96, 24.35]}, {"formula_id": "formula_7", "formula_text": "O(d) O(s/n) .", "formula_coordinates": [4.0, 421.23, 410.11, 28.53, 14.68]}, {"formula_id": "formula_8", "formula_text": "L = \uf8ee \uf8ef \uf8f0 D 11 . . . D 1m . . . . . . . . . D m1 . . . D mm \uf8f9 \uf8fa \uf8fb .", "formula_coordinates": [5.0, 248.01, 322.84, 115.98, 41.43]}, {"formula_id": "formula_9", "formula_text": "M 1 M * 2 for some M 1 , M 2 \u2208 M.", "formula_coordinates": [5.0, 366.71, 514.35, 135.38, 12.2]}, {"formula_id": "formula_10", "formula_text": "M 1 M 2 for M 1 , M 2 \u2208 MM * .", "formula_coordinates": [5.0, 331.02, 526.3, 127.83, 11.22]}, {"formula_id": "formula_11", "formula_text": "argmin M\u2208M A \u2212 M 2 F .(1)", "formula_coordinates": [6.0, 263.57, 372.83, 276.43, 19.51]}, {"formula_id": "formula_12", "formula_text": "M jki = L j k R kji .(2)", "formula_coordinates": [6.0, 266.91, 637.11, 273.09, 9.65]}, {"formula_id": "formula_13", "formula_text": "Matrix A \u2208 R n\u00d7n , with n = m 2 . Reshape A into a 4D tensor A of size m \u00d7 m \u00d7 m \u00d7 m, where A jki = A ( \u22121)m+j,(k\u22121)m+i for , j, k, i = 1, . . . , m. for 1 \u2264 j, k \u2264 m do Let M jk = A :,j,k,: of size m \u00d7 m.", "formula_coordinates": [7.0, 81.46, 345.86, 458.54, 60.04]}, {"formula_id": "formula_14", "formula_text": "1 P )R(PL 2 P ) for block-diagonal L 1 , L 2 , R", "formula_coordinates": [7.0, 72.0, 561.92, 472.7, 21.64]}, {"formula_id": "formula_15", "formula_text": "have\u00c2 i\u0108j = M i1 M \u22121 11 M 1j .", "formula_coordinates": [8.0, 236.37, 322.22, 121.89, 13.03]}, {"formula_id": "formula_16", "formula_text": "M ij = A i C j , this equals (A i C 1 )(A 1 C 1 ) \u22121 (A 1 C j ) = A i C j , as desired.", "formula_coordinates": [8.0, 109.28, 336.73, 310.85, 11.23]}, {"formula_id": "formula_17", "formula_text": "C.1.", "formula_coordinates": [22.0, 72.0, 179.95, 20.88, 8.77]}, {"formula_id": "formula_18", "formula_text": "Definition C.1 (Class BD (b,n) ). Let b \u2208 (1, n", "formula_coordinates": [22.0, 72.0, 276.5, 202.85, 10.31]}, {"formula_id": "formula_19", "formula_text": "R = diag R 0 , . . . , R n b \u22121 .(3)", "formula_coordinates": [22.0, 247.62, 311.92, 292.38, 11.7]}, {"formula_id": "formula_20", "formula_text": "Definition C.2 (Class DB (b,n) ). Let b \u2208 (1, n) be an integer that divides n. For 0 \u2264 i, j < b, let D i,j \u2208 F b\u00d7b be a b \u00d7 b diagonal matrix.", "formula_coordinates": [22.0, 72.0, 390.45, 467.5, 22.27]}, {"formula_id": "formula_21", "formula_text": "L = \uf8ee \uf8ef \uf8f0 D 0,0 . . . D 0, n b \u22121 . . . . . . . . . D n b \u22121,0 . . . D n b \u22121, n b \u22121 \uf8f9 \uf8fa \uf8fb (4) (Note that the number of possible nonzero values in L is n b 2 \u2022 b = n 2 b .)", "formula_coordinates": [22.0, 86.94, 424.31, 453.06, 68.72]}, {"formula_id": "formula_22", "formula_text": "Definition C.3 (Class M (b,n) ). Let b \u2208 (1, n", "formula_coordinates": [22.0, 72.0, 598.14, 196.9, 10.31]}, {"formula_id": "formula_23", "formula_text": "M = LR (5)", "formula_coordinates": [22.0, 286.18, 623.59, 253.82, 8.77]}, {"formula_id": "formula_24", "formula_text": "M = \uf8eb \uf8ed w i=1 M i \uf8f6 \uf8f8 [1 : n, 1 : n](6)", "formula_coordinates": [23.0, 245.39, 208.51, 294.61, 35.81]}, {"formula_id": "formula_25", "formula_text": "i 1 = i b .", "formula_coordinates": [23.0, 284.05, 464.12, 43.9, 22.31]}, {"formula_id": "formula_26", "formula_text": "i = i 1 \u2022 b + i 0", "formula_coordinates": [23.0, 277.92, 518.47, 55.67, 9.65]}, {"formula_id": "formula_27", "formula_text": "Definition C.7. Let b \u2208 [1, n] be an integer that divides n. Let i \u2261 (i 1 , i 0 ) b . Define \u03c3 (b,n) (i) = i 0 \u2022 n b + i 1 .(7)", "formula_coordinates": [23.0, 72.0, 557.73, 468.0, 40.23]}, {"formula_id": "formula_28", "formula_text": "\u03c3 (b,n) (i) \u2261 (i 0 , i 1 ) n b", "formula_coordinates": [23.0, 110.3, 607.3, 81.9, 13.16]}, {"formula_id": "formula_29", "formula_text": "1. if i 1 = j 1 , then R[i, j] = 0.", "formula_coordinates": [23.0, 83.78, 714.03, 129.31, 9.68]}, {"formula_id": "formula_30", "formula_text": "[i, j] = R i1 [i 0 , j 0 ].", "formula_coordinates": [24.0, 235.32, 75.99, 80.84, 9.68]}, {"formula_id": "formula_31", "formula_text": "L[i, j] = D i1,j1 [i 0 , i 0 ].", "formula_coordinates": [24.0, 233.99, 167.65, 92.61, 9.68]}, {"formula_id": "formula_32", "formula_text": "Theorem 3. Let 1 \u2264 b \u2264 n such that b divides n.", "formula_coordinates": [24.0, 71.6, 207.5, 215.55, 8.77]}, {"formula_id": "formula_33", "formula_text": "R = P (b,n) \u2022 L \u2022 P (b,n) ,", "formula_coordinates": [24.0, 256.29, 244.34, 99.42, 10.9]}, {"formula_id": "formula_34", "formula_text": "R [\u03c3 (b,n) (i), \u03c3 (b,n) (j)] = L[i, j].(8)", "formula_coordinates": [24.0, 239.81, 326.49, 300.19, 9.99]}, {"formula_id": "formula_35", "formula_text": "i = (i 1 , i 0 ) b and j = (j 1 , j 0 ) b . Then note that \u03c3 (b,n) (i) = (i 0 , i 1 ) n b and \u03c3 (b,n) (j) = (j 0 , j 1 ) n b", "formula_coordinates": [24.0, 71.64, 356.41, 470.29, 25.11]}, {"formula_id": "formula_36", "formula_text": "L[i, j] = D i1,j1 [i 0 , i 0 ]. Note that i 0 = j 0 satisfies the pre-condition for base size n b for indices (\u03c3 (b,n) (i), \u03c3 (b,n) (j)) in item 2 in Proposition C.8 if we define R i0 \u2208 F n b \u00d7 n b as follows: R i0 [i 1 , j 1 ] = D i1,j1 [i 0 , i 0 ].", "formula_coordinates": [24.0, 72.0, 418.06, 469.93, 57.5]}, {"formula_id": "formula_37", "formula_text": "R = diag R 0 , . . . , R b\u22121 ,", "formula_coordinates": [24.0, 247.86, 498.78, 116.29, 10.65]}, {"formula_id": "formula_38", "formula_text": "M = (P (b,n) L 1 P (b,n) )R(P (b,n) L 2 P (b,n) ), where L 1 , L 2 \u2208 BD ( n b ,n) and R \u2208 BD (b,n) .", "formula_coordinates": [24.0, 72.0, 571.21, 468.0, 24.48]}, {"formula_id": "formula_39", "formula_text": "C.2), if M \u2208 MM * (b,n) , we can write M = (L 1 R 1 )(L 2 R 2 ) * = L 1 (R * 1 R 2 )L * 2 , where L 1 , L 2 \u2208 DB (b,n) , R 1 , R 2 \u2208 BD", "formula_coordinates": [24.0, 70.83, 604.39, 469.17, 24.16]}, {"formula_id": "formula_40", "formula_text": "L 1 = P (b,n) L 1 P (b,n) , L 2 = P (b,n) L 2 P (b,n) , where L 1 , L 2 are both in BD ( n b ,n) (i.", "formula_coordinates": [24.0, 72.0, 641.8, 469.39, 26.19]}, {"formula_id": "formula_41", "formula_text": "Then, M \u2208 (MM * (b,n) ) O(d) O(s/n) .", "formula_coordinates": [26.0, 405.57, 237.47, 134.18, 14.68]}, {"formula_id": "formula_42", "formula_text": "* (n) ) O(d) O(s/n) (this is the \"kaleidoscope representation\" of A).", "formula_coordinates": [26.0, 163.92, 284.17, 255.45, 14.68]}, {"formula_id": "formula_43", "formula_text": "* (n) ) w e . As A \u2208 (BB * (n) ) O(d) O(s/n) , we thus have A \u2208 (MM * (b,n) ) O(d) O(s/n) .", "formula_coordinates": [26.0, 86.94, 312.66, 317.48, 27.02]}, {"formula_id": "formula_44", "formula_text": "min n 2 , \u221a s . Note that f (n, s) = O( \u221a s), and since s = O(n 2 ), f (n, s) = \u2126( \u221a s), so f (n, s) = \u0398( \u221a s). We thus have A \u2208 (MM * (f (n,s),n) ) O(", "formula_coordinates": [26.0, 72.0, 371.61, 468.0, 30.8]}, {"formula_id": "formula_45", "formula_text": "\u2022 n1 b1 = n2 b2", "formula_coordinates": [26.0, 86.95, 544.02, 43.96, 13.64]}, {"formula_id": "formula_46", "formula_text": "R = diag R 0 , . . . , R n 1 b 1 \u22121 .(9)", "formula_coordinates": [26.0, 244.29, 609.44, 295.71, 14.15]}, {"formula_id": "formula_47", "formula_text": "n1 b1 \u2022 b 1 \u00d7 b 2 = n 1 b 2 .)", "formula_coordinates": [26.0, 331.58, 642.12, 88.06, 13.64]}, {"formula_id": "formula_48", "formula_text": "L = \uf8ee \uf8ef \uf8ef \uf8f0 S 0,0 . . . S 0, n 2 b 2 \u22121 . . . . . . . . . S n 3 b 3 \u22121,0 . . . S n 3 b 3 \u22121, n 2 b 2 \u22121 \uf8f9 \uf8fa \uf8fa \uf8fb ,(10)", "formula_coordinates": [27.0, 228.77, 207.88, 311.23, 48.89]}, {"formula_id": "formula_49", "formula_text": "n2 b2 \u2022 n3 b3 max(b 2 , b 3 ) = n2\u2022n3 min(b2,b3) .)", "formula_coordinates": [27.0, 79.15, 297.36, 148.67, 13.64]}, {"formula_id": "formula_50", "formula_text": "Proposition C.18. L \u2208 F n3\u00d7n2 is in DB (b3\u00d7b2,", "formula_coordinates": [27.0, 72.0, 344.3, 212.76, 10.87]}, {"formula_id": "formula_51", "formula_text": "1. if i 0 mod b 2 = j 0 , then L[i, j] = 0.", "formula_coordinates": [27.0, 83.78, 376.0, 161.3, 9.68]}, {"formula_id": "formula_52", "formula_text": "L[i, j] = S i1,j1 [i 0 , j 0 ].", "formula_coordinates": [27.0, 267.68, 395.06, 90.85, 9.68]}, {"formula_id": "formula_53", "formula_text": "M = LR (11)", "formula_coordinates": [27.0, 286.18, 469.42, 253.82, 8.77]}, {"formula_id": "formula_54", "formula_text": "we have that R \u2208 BD ( n 3 b 3 \u00d7 n 2 b 2 ,n3\u00d7n2) .", "formula_coordinates": [27.0, 71.35, 624.97, 159.98, 14.05]}, {"formula_id": "formula_55", "formula_text": "R [\u03c3 (b,n3) (i), \u03c3 (b,n2) (j)] = L[i, j].(12)", "formula_coordinates": [27.0, 235.92, 694.22, 304.08, 9.99]}, {"formula_id": "formula_56", "formula_text": "A \u2212 M 2 F = jki (A jki \u2212 M jki ) 2 = jki (A jki \u2212 L j k R kji ) 2 = jk i (A jki \u2212 L j k R kji ) 2 .", "formula_coordinates": [29.0, 220.42, 107.71, 176.14, 80.67]}, {"formula_id": "formula_57", "formula_text": "\uf8eb \uf8ec \uf8ec \uf8ec \uf8ec \uf8ed M 11 M 12 . . . M 1b M 21 M 22 . . . M 2b . . . . . . . . . . . . M b1 M b2 . . . M bb \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f7 \uf8f8 = \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed A 1 A 2 . . . A b \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed D 11 D 12 . . . D 1b D 21 D 22 . . . D 2b . . . . . . . . . . . . D b1 D b2 . . . D bb \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 \uf8eb \uf8ec \uf8ec \uf8ec \uf8ed C 1 C 2 . . . C b \uf8f6 \uf8f7 \uf8f7 \uf8f7 \uf8f8 ,", "formula_coordinates": [29.0, 72.0, 537.7, 482.05, 58.85]}, {"formula_id": "formula_58", "formula_text": "A i D ij C j = M ij , for 1 \u2264 i, j \u2264 b.", "formula_coordinates": [29.0, 272.9, 666.21, 143.8, 9.68]}, {"formula_id": "formula_59", "formula_text": ",\u00c2 i = M i1\u0108 \u22121", "formula_coordinates": [31.0, 99.42, 197.58, 65.56, 11.87]}, {"formula_id": "formula_60", "formula_text": "D ij =\u00c2 \u22121 i M ij\u0108 \u22121 j", "formula_coordinates": [31.0, 72.0, 225.31, 84.81, 13.16]}, {"formula_id": "formula_61", "formula_text": "D ij =\u00c2 \u22121 i M ij\u0108 \u22121 j = ( M i1\u0108 \u22121 1 ) \u22121 M ij (\u00c2 \u22121 1 M 1j ) \u22121 =\u0108 1 M \u22121 i1 M ij M \u22121 1j\u00c2 1 =\u0108 1 ( M \u22121 i1 M ij M \u22121 1j M 11 )\u0108 \u22121 1 =\u0108 1 F(i, j)\u0108 \u22121 1 But\u0108 1 F(i, j)\u0108 \u22121 1", "formula_coordinates": [31.0, 86.94, 263.09, 298.26, 107.85]}, {"formula_id": "formula_62", "formula_text": "y = \u2126F Sx +", "formula_coordinates": [34.0, 274.24, 614.95, 57.27, 8.77]}, {"formula_id": "formula_63", "formula_text": "|N | i \u2126i |\u2126| .", "formula_coordinates": [34.0, 152.36, 670.88, 25.13, 17.2]}], "doi": "10.5281/zenodo.5371628"}