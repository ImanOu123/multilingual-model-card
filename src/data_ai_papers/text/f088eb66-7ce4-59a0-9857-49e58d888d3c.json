{"title": "Meta AI at Arabic Hate Speech 2022: MultiTask Learning with Self-Correction for Hate Speech Classification", "authors": "Badr Alkhamissi; Mona Diab", "pub_date": "", "abstract": "In this paper, we tackle the Arabic Fine-Grained Hate Speech Detection shared task and demonstrate significant improvements over reported baselines for its three subtasks. The tasks are to predict if a tweet contains (1) Offensive language; and whether it is considered (2) Hate Speech or not and if so, then predict the (3) Fine-Grained Hate Speech label from one of six categories. Our final solution is an ensemble of models that employs multitask learning and a self-consistency correction method yielding 82.7% on the hate speech subtask-reflecting a 3.4% relative improvement compared to previous work.", "sections": [{"heading": "Introduction", "text": "Disclaimer: Due to the nature of this work, some examples contain offensiveness and hate speech. This does not reflect authors' values, however our aim is to help in detecting and preventing spread of such harmful content.\nThe advent of online social networks have created a platform for billions of people to express their thoughts freely on the internet. This has enormous benefits for advancing culture. However, it also can be used by malicious actors to distribute misinformation and offensive content. This led to an increasing interest in the NLP community for the automatic detection of Hate Speech (HS) (Waseem and Hovy, 2016;Schmidt and Wiegand, 2017;Zampieri et al., 2019;MacAvaney et al., 2019;League, 2020;Vogels, 2021). Its dangers are becoming more apparent with studies showing its connection to hate crimes around the globe (Paz et al., 2020). Further, the spread of hateful content on the internet has also been linked to degenerate effects on peoples' psychological well-being (G\u00fcla\u00e7ti, 2010;Waldron, 2012). HS is defined as any kind of abusive or offensive language (e.g. insults, threats, etc.) that expresses prejudice against a specific person or a group based on common characteristics such as race, religion or sexual orientation (Davidson et al., 2017;Mollas et al., 2020). Despite the growing body of HS research, few have focused on it in the context of the Arabic language. Arabic is the mother tongue of more than 420M people, and is spoken in the fastest growing markets (Tinsley and Board, 2013). Arabic content is rapidly growing on the internet during the past couple of years (Abdelali et al., 2021). For instance, studies have shown that there are more than 27 million tweets per day in the Arab region (Alshehri et al., 2018). In this work, we focus on the Arabic language by participating in the three subtasks of the Arabic Fine-Grained Hate Speech Detection shared task (Mubarak", "publication_ref": ["b25", "b20", "b28", "b10", "b7", "b23", "b17", "b3", "b24", "b11", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "MARBERT MARBERT", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Offensive", "text": "Hate Speech HS Type", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tweet", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "MARBERT", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Self-Consistency Correction", "text": "Figure 1: System Architecture The input tweet is encoded using a fine-tuned MARBERT model and the output embedding is given to 3 task-specific classifiers. The final prediction is computed using an ensemble of those models. et al., 2022). The three subtasks use the same dataset from (Mubarak et al., 2022) (see Section 3 for more details.) The goal of the first subtask is to detect whether a tweet is offensive or not, while the second subtask focuses on HS detection. The third subtask further classifies a HS post into one of six fine-grained categories: Race/Ethnicity/Nationality, Religion/Belief, Ideology, Disability/Disease, Social Class, and Gender. Table 1 shows an example with its corresponding label for each subtask. An offensive post is not necessarily HS, while a HS post is always offensive. If offensive speech is not targeting an individual or a group based on common characteristics, then it is not HS. The contributions of this paper are as follows: (1) We present a solution that outperforms the baseline models of (Mubarak et al., 2022) ", "publication_ref": ["b13", "b13"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Motivation", "text": "There has been a growing body of research in recent years for the automatic detection of offensive language and HS online (Waseem and Hovy, 2016;Davidson et al., 2017;Schmidt and Wiegand, 2017;Fortuna and Nunes, 2018;Founta et al., 2018). Studies have shown that 41% of internet users have been harassed online with a third of these cases being targeted for something related to their inherent identity such as race or sexual orientation (Vogels, 2021;League, 2020). The massive amount of content shared on social media platforms renders manual filtering out of such malicious content impossible, driving platform providers to resort to automated means for detecting hateful content. On the other hand, machine learning based methods are data hungry and require large amounts of labelled data in order to train reliable HS classification systems. Moreover, such data has been proven hard to collect especially for low-resource languages such as Arabic. For example, (Mubarak et al., 2017) show that only 1-2% of a randomly collected sample of Arabic tweets are abusive, and only a small percentage of these are considered HS. Therefore, generalizable and robust systems for detecting offensive and HS content are direly needed.\nPrevious work has framed this problem as a binary classification task. However, binary judgments of HS are known to be unreliable (Sanguinetti et al., 2018;Assimakopoulos et al., 2020a). Therefore, in order to collect higher quality HS datasets researchers resorted to more complex annotation schema. For example, (Sap et al., 2020;Assimakopoulos et al., 2020b) proposed to decompose a post into several subtasks (such as the HS class and group targeted) in an effort to minimize subjectivity when deciding the HS label.\nHere, we leverage the task decomposed dataset provided by (Mubarak et al., 2022) to train an Arabic transformer in a multitask manner for improving the performance of fine-grained HS detection.", "publication_ref": ["b25", "b20", "b1", "b2", "b23", "b7", "b12", "b18", "b19", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Dataset", "text": "We use the dataset from (Mubarak et al., 2022). It consists of \u223c 13k tweets in both Modern Standard Ara-bic (MSA) and Dialectal forms of Arabic (DA). It is the largest annotated corpus of Arabic tweets that is not biased towards specific topics, genres, or dialects (Mubarak et al., 2022). Each tweet was judged by 3 annotators using crowd-sourcing. Table 2 shows the number and percentages of each annotated category. The data was split into 70% for training, 10% for development, and 20% for test. The dataset has also annotations for vulgar and violent tweets representing 1.5% and 0.7% of the whole corpus, respectively, however we are not using them in this work. Moreover, one or more user mentions are reduced to @USER , URLs are replaced with URL , and empty lines in original tweets are replaced with <LF> . See Table 1 for an example of each annotated class.\nOne limitation of this dataset is that the classes are highly imbalanced. Moreover, the Disability/Disease subclass does not exist in the training set. There are only 3 tweets related to this category and they appear in the validation and test sets only.\nClass -Subclass # of Tweets Percentage (%)    (Mubarak et al., 2022). Our model, AraHS, outperforms the baseline QARiB on every metric. NB: no baseline was reported for the HSC subtask.", "publication_ref": ["b13", "b13", "b13"], "figure_ref": [], "table_ref": ["tab_3", "tab_1"]}, {"heading": "System Description", "text": "In  (Wu et al., 2016).\nWe frame the 3 subtasks as a multi-task classification problem. Specifically, the input text is encoded using MARBERTv2 and is then passed to 3 task-specific classification heads as shown in Figure 1. Each class specific head is made up of a multi-layered feed forward neural network with layer normalization (Ba et al., 2016). Concretely, the [CLS] embedding of the final MARBERTv2 transformer block is forwarded to a dense layer with 768 units, which is then passed through a GELU activation function (Hendrycks and Gimpel, 2016), the output of which is normalized using layer normalization, and this is finally given to a linear layer that maps it to the corresponding number of classes.\nThe final model is an ensemble of several trained models each of which uses a different set of hyperparameters. To obtain the final prediction we perform elementwise multiplication of the corresponding probabilities across the different models then take the argmax.\nSelf-Consistency Correction Since we are training one model for all three subtasks, and the subtasks themselves are interdependent, we leverage that to our advantage. We perform a post-processing step where errors of one classification head are corrected by the others. Concretely, the fine-grained HS prediction is corrected in the following cases: the tweet is predicted to be offensive and contains HS using the first two classification heads respectively, while the fine-grained classifier predicted that it is not HS. In that case, we take the second most probable class prediction as the label since there is an inconsistency. The other scenario in which it is corrected is when the tweet is predicted as not offensive and does not contain HS while the finegrained classifier predicted it as one of the HS classes.", "publication_ref": ["b27", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "To train the AraHS model, we use the AdamW optimzier (Loshchilov and Hutter, 2019) and a learning rate scheduler that is warmed-up linearly for 500 steps to some initial learning rate. This is then decayed linearly to zero over the course of 10 epochs. The model is evaluated on the validation-set 4 times every epoch with equal intervals, and a checkpoint for the corresponding subtask is saved when its F1-macro score improves. The objective function is the sum of the negative loglikelihood of the three classification heads. The tokenizer encodes the input text using a maximum length of 256 tokens. The model is trained 12 times over a grid of {2, 4, 8, 16} batch-sizes and {1e \u2212 5, 5e \u2212 6, 1e \u2212 6} initial learning rates.\nFor the fine-grained HS detection subtask, we further finetune the best single model on only this subtask, using the same experimental setup described above.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 3 shows the performance on the test-set for each subtask. Our method (AraHS) outperforms the baseline models reported in (Mubarak et al., 2022) ", "publication_ref": ["b13"], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Ablation Study", "text": "In order to demonstrate the importance of training the subtasks jointly, we train each subtask on its own. Specifically, Table 4 compares the validation performance of each subtask with its multitask counterpart. Performance improves when using multitask learning (MTL) for the HS subtasks. However, for the offensive subtask we observe similar performance to the singletask trained models. Similar to Table 4: The validation performance on each subtask. The single-task models are trained on the subtask alone, while the multitask model trains all subtasks jointly. The results before and after applying the self-consistency correction as a post-processing step is shown for the HSC subtask. Bold shows the best result for each subtask.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Error Analysis", "text": "The subtasks we are training are not independent of one another, even though we are modelling them that way. For example, as previously mentioned, a tweet that is considered HS is automatically offensive as well, and needless to say that each of the fine-grained HS classes (e.g. race, gender, etc.) are HS. Therefore, in this section we want to measure the degree of selfconsistency within the trained multitask model. Specifically, we take the predictions of the best ensemble of models and calculate the percentage of contradiction between each classification head (see Table 5). Concretely, we compute the number of times in which the OFFD head yielded a negative prediction whereas the HSD or the HSC yielded a positive one. This is a contradiction since each HS post must be offensive as well.\nSimilarly, we calculate the number of times the HSD head predicted negative while the HSC predicted positive and vice versa. As illustrated in Table 5, correcting the HSC head based on the two other subtasks reduces the contradiction considerably (from 2.6% to 0.79%) while achieving a better performance overall.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8", "tab_8"]}, {"heading": "Subtask Contradiction (%)", "text": "OFFD 2.44% HSD 2.60% HSC 2.60% / 0.79% Furthermore, Table 6 shows two examples where the classification heads disagreed with one another. For example, the first tweet was detected as HS but the finegrained classification head classified it as non-HS leading to a disagreement. Using our self-consistency correction method, the model was able to correct itself and yield the correct label, which was the Ideology subclass in this case. Example 1 in the table is a modification of an Arabic adage: \" \", corresponding to \"Birds of a feather flock together. Changing birds to frogs implies ugliness. Such tweets are not straight forward to classify since they require an understanding of cultural knowledge and implicit social nuance that is not explicitly encoded in language models such as MARBERT. One way to mitigate this is to finetune the model on a corpus that contains such information explicitly incorporating such inductive bias. Another method would be training the language model to generate the implication of the tweet as an additional subtask. The other example in the table implies that people of a certain nationality are ignorant. We believe that the provided gold label is incorrect (not HS). We believe that this tweet constitutes HS because it is offensive (a certain group of people is ignorant since they parrot rather than understand information) and it targets a group. Accordingly, the model was able to successfully predict it as HS, and even yield the correct class for it using the self-consistency method. In Table 7 we report the percentage of false positives (FP) and false negatives (FN) of the best checkpoint of each subtask. To compute the percentage of FP and FN for the HSC subtask we convert it into a binary variable with negative implying that the prediction is not-HS and positive otherwise. Interestingly, the selfconsistency correction method increases the percentage of FPs, as it takes the second top prediction as its label when both the HSD and OFFD are positive. We note that HS systems can tolerate more false positives (i.e. over enforcement) than false negatives (i.e. under enforcement), since the latter will lead to more propagation of harm. This highlights the advantage of selfconsistency correction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Datasets The first Arabic HS dataset was collected by (Albadi et al., 2018) and consisted of \u223c 6.6k Arabic HS tweets. In an effort to collect a more dialect specific dataset, (Haddad et al., 2019) compiled 6k tweets of the Tunisian dialect containing both abusive language and HS. (Mulki et al., 2019) similarly collected a Levantine HS dataset. In the multilingual front, (Ousidhoum et al., 2019) created a HS dataset made up of 13k Arabic, English and French tweets with fine-grained labels Tweet OFFD HSD HSC\nFrogs settle with their own kind.", "publication_ref": ["b4", "b14", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "\u2713-\u2713 \u2713-\u2713 \u2717-Ideology", "text": "The people of this country memorize without understanding.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "\u2713-\u2717 \u2713-\u2717 \u2717-\u2717", "text": "Necessary rituals in this community that is based on social hypocrisy. \u2717-\u2713 \u2717-\u2713 \u2717-Nationality Table 6: The first two are examples that led to a disagreement between the classification heads, while the third one show a false negative example. Below each example is a rough English translation that is used to convey the meaning. On the right-side of the table, the prediction of the model is shown first followed by the ground truth label for each subtask. Note that the self-consistency correction method was able to correct the first two examples among others.\nFalse +ve (%) False -ve (%) OFFD 4.96% 6.54% HSD 1.97% 1.81% HSC 2.52%/3.86% 2.44%/1.73%\nTable 7: Percentage of false positives and false negatives for the best checkpoint for each subtask. In the HSC we report the percentage before and after applying the self-consistency correction method.\ncovering different aspects such as target groups, directness, target attributes and hostility types.\nModels Early work tackled this problem by extracting n-gram features using term frequency weighting, which was then passed to a Support Vector Machine (SVM) and Naive Bayes (NB) classifiers (Mulki et al., 2019). Other work used a gated recurrent unit (GRU) coupled with an SVM trained on the AraVec embeddings (Ashi et al., 2018) to classify HS (Albadi et al., 2018). (Hassan et al., 2020) ", "publication_ref": ["b14", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "In this paper, we propose MTL as an approach to Hate Speech Classification. Our proposed model, AraHS, outperforms the baseline models. AraHS is an ensemble of MARBERT (Abdul-Mageed et al., 2021) models trained with different hyperparameters using MTL.\nThe fine-grained HS subtask is then finetuned on its own for a couple of epochs. We demonstrate the importance of training the three subtasks jointly through an ablation study and propose the self-consistency correction method that improves the final result even further. In future work, we would like to explore the limits of combining multilingual models (e.g. mBART (Liu et al., 2020)) with Arabic monolingual models such as MARBERT. Further, we would like to explore treating the problem as a conditional generation task using the AraT5 model (Nagoudi et al., 2021) that has been shown to outperform MARBERT on the Arabic language understanding evaluation benchmark (ARLUE) (Abdul-Mageed et al., 2021).", "publication_ref": ["b8", "b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Ethics and Social Impact", "text": "Modern deep learning models are energy intensive and can cause environmental damage due to the carbon dioxide emissions required for running modern hardware. Studies have shown that training a BERT model on GPU has a comparable carbon footprint to a trans-American flight (Strubell et al., 2019). In this work, even though we do not pre-train the model, we still run multiple experiments across a grid of hyperparameters, that when combined consumes significant energy. Therefore, one of the reasons we chose multitask learning (MTL) is that we can reduce the amount of training substantially by training one model on multiple tasks. MTL does not only offer energy efficiency, but is also more data efficient, it has been shown to converge faster by leveraging auxiliary information and reduces over-fitting through shared representations (Crawshaw, 2020). Further, building models for detecting OFF language and HS can help improve the moderation of hateful content on the internet. This can potentially lead to less hate crimes and better psychological well-being for users receiving such content. However, the authors are aware of potential misuse of HS models, such as propagating the spread of HS rather than suppressing it. Therefore, human moderation is required for preventing such misuse.", "publication_ref": ["b21"], "figure_ref": [], "table_ref": []}, {"heading": "Bibliographical References", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "April. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b1", "title": "A survey on automatic detection of hate speech in text", "journal": "ACM Computing Surveys (CSUR)", "year": "2018", "authors": "P Fortuna; S Nunes"}, {"ref_id": "b2", "title": "Large scale crowdsourcing and characterization of twitter abusive behavior", "journal": "", "year": "2018", "authors": "A M Founta; C Djouvas; D Chatzakou; I Leontiadis; J Blackburn; G Stringhini; A Vakali; M Sirivianos; N Kourtellis"}, {"ref_id": "b3", "title": "The effect of perceived social support on subjective well-being", "journal": "Procedia -Social and Behavioral Sciences", "year": "2010", "authors": "F G\u00fcla\u00e7ti"}, {"ref_id": "b4", "title": "T-HSAB: A Tunisian Hate Speech and Abusive Dataset", "journal": "", "year": "2019", "authors": "H Haddad; H Mulki; A Oueslati"}, {"ref_id": "b5", "title": "ALT submission for OSACT shared task on offensive language detection", "journal": "", "year": "2020", "authors": "S Hassan; Y Samih; H Mubarak; A Abdelali; A Rashed; S A Chowdhury"}, {"ref_id": "b6", "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units", "journal": "ArXiv", "year": "2016", "authors": "D Hendrycks; K Gimpel"}, {"ref_id": "b7", "title": "Online hate and harassment. the american experience 2021. Center for Technology and Society", "journal": "", "year": "2020", "authors": "A.-D League"}, {"ref_id": "b8", "title": "Multilingual denoising pre-training for neural machine translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Y Liu; J Gu; N Goyal; X Li; S Edunov; M Ghazvininejad; M Lewis; L Zettlemoyer"}, {"ref_id": "b9", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "I Loshchilov; F Hutter"}, {"ref_id": "b10", "title": "Hate speech detection: Challenges and solutions", "journal": "PloS one", "year": "2019", "authors": "S Macavaney; H.-R Yao; E Yang; K Russell; N Goharian; O Frieder"}, {"ref_id": "b11", "title": "Ethos: an online hate speech detection dataset. ArXiv, abs", "journal": "", "year": "2006", "authors": "I Mollas; Z Chrysopoulou; S Karlos; G Tsoumakas"}, {"ref_id": "b12", "title": "Abusive language detection on Arabic social media", "journal": "", "year": "2017", "authors": "H Mubarak; K Darwish; W Magdy"}, {"ref_id": "b13", "title": "Emojis as anchors to detect arabic offensive language and hate speech", "journal": "", "year": "2022", "authors": "H Mubarak; S Hassan; S A Chowdhury"}, {"ref_id": "b14", "title": "L-HSAB: A Levantine Twitter dataset for hate speech and abusive language", "journal": "", "year": "2019-08", "authors": "H Mulki; H Haddad; C Bechikh Ali; Alshabani ; H "}, {"ref_id": "b15", "title": "Arat5: Text-to-text transformers for arabic language generation", "journal": "", "year": "2021", "authors": "E M B Nagoudi; A Elmadany; Abdul-Mageed ; M "}, {"ref_id": "b16", "title": "Multilingual and multiaspect hate speech analysis", "journal": "Association for Computational Linguistics", "year": "2019-11", "authors": "N Ousidhoum; Z Lin; H Zhang; Y Song; D.-Y Yeung"}, {"ref_id": "b17", "title": "Hate speech: A systematized review", "journal": "SAGE Open", "year": "2020", "authors": "M A Paz; J Montero-D\u00edaz; A Moreno-Delgado"}, {"ref_id": "b18", "title": "An Italian Twitter corpus of hate speech against immigrants", "journal": "", "year": "2018", "authors": "M Sanguinetti; F Poletto; C Bosco; V Patti; M Stranisci"}, {"ref_id": "b19", "title": "Social bias frames: Reasoning about social and power implications of language", "journal": "", "year": "2020", "authors": "M Sap; S Gabriel; L Qin; D Jurafsky; N A Smith; Y Choi"}, {"ref_id": "b20", "title": "A survey on hate speech detection using natural language processing", "journal": "", "year": "2017-04", "authors": "A Schmidt; M Wiegand"}, {"ref_id": "b21", "title": "Energy and policy considerations for deep learning in nlp. ArXiv, abs", "journal": "", "year": "1906", "authors": "E Strubell; A Ganesh; A Mccallum"}, {"ref_id": "b22", "title": "Languages for the Future", "journal": "British Council", "year": "2013", "authors": "T Tinsley; K Board"}, {"ref_id": "b23", "title": "The state of online harassment", "journal": "Pew Research Center", "year": "2021", "authors": "E A Vogels"}, {"ref_id": "b24", "title": "The Harm in Hate Speech", "journal": "Harvard University Press", "year": "2012", "authors": "J Waldron"}, {"ref_id": "b25", "title": "Hateful symbols or hateful people? predictive features for hate speech detection on Twitter", "journal": "", "year": "2016-06", "authors": "Z Waseem; D Hovy"}, {"ref_id": "b26", "title": "Huggingface's transformers: State-of-the-art natural language processing", "journal": "ArXiv", "year": "2019", "authors": "T Wolf; L Debut; V Sanh; J Chaumond; C Delangue; A Moi; P Cistac; T Rault; R Louf; M Funtowicz; J Brew"}, {"ref_id": "b27", "title": "Google's neural machine translation system", "journal": "", "year": "2016", "authors": "Y Wu; M Schuster; Z Chen; Q Le; M Norouzi; W Macherey; M Krikun; Y Cao; Q Gao; K Macherey; J Klingner; A Shah; M Johnson; X Liu;  Kaiser; S Gouws; Y Kato; T Kudo; H Kazawa; J Dean"}, {"ref_id": "b28", "title": "Predicting the type and target of offensive posts in social media", "journal": "Association for Computational Linguistics", "year": "2019-06", "authors": "M Zampieri; S Malmasi; P Nakov; S Rosenthal; N Farra; R Kumar"}], "figures": [{"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "You won't have a better tomorrow as long you are thinking about yesterday.May your father be damned for this question! I hope this fool will just wither!", "figure_data": "ClassExampleClean1.(Non-Off)Offensive2.(Off/Non-HS)Hate Speech3. This dwarf got two prizes, but he does not know how to express. (Off/HS)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ": Here we show examples and their translation in English adapted from (Mubarak et al., 2022). Example1 is non offensive (Non-Off), Example 2 is Offensive but not Hate speech (Off/Non-HS), Example 3 is offensiveand hate speech (Off/HS)."}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Dataset StatisticsThe number and percentages of tweets as represented in the entire corpus of each annotated category used in this work as described in(Mubarak et al., 2022).1   ", "figure_data": "Subtask Model Accuracy Precision Recall F1 MacroOFFDQARiB AraHS84.0% 86.0%82.5% 84.6%82.1% 84.3%82.3% 84.5%HSDQARiB AraHS93.0% 94.1%83.0% 87.0%77.7% 79.5%80.0% 82.7%HSCAraHS92.6%55.1%50.8%51.9%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Performance on Test for each of the subtasks Offensive Detection (OFFD), Hate Speech Detection (HSD), Hate Speech Classification (HSC) in comparison with the QARiB baseline models reported in", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "SubtaskModelAccuracyPrecisionRecallF1 MacroOFFDSingle-task Multitask88.7% 88.5%87.4% 87.1%86.1% 86.1%86.7% 86.6%HSDSingle-task Multitask95.8% 96.2%87.2% 87.7%85.7% 88.4%86.4% 88.1%HSCSingle-task Multitask95.3% 95.0/94.4% 58.5/54.8% 52.5/58.8% 54.8/56.6% 72.4% 46.8% 51.0%"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Percentage of contradiction between classification heads before and after self-correction for the HSC. Each row correspond to the best checkpoint achieved for that subtask.", "figure_data": ""}], "formulas": [], "doi": ""}