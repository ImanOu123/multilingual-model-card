{"title": "Non-delusional Q-learning and Value Iteration", "authors": "Tyler Lu; Dale Schuurmans; Craig Boutilier", "pub_date": "", "abstract": "We identify a fundamental source of error in Q-learning and other forms of dynamic programming with function approximation. Delusional bias arises when the approximation architecture limits the class of expressible greedy policies. Since standard Q-updates make globally uncoordinated action choices with respect to the expressible policy class, inconsistent or even conflicting Q-value estimates can result, leading to pathological behaviour such as over/under-estimation, instability and even divergence. To solve this problem, we introduce a new notion of policy consistency and define a local backup process that ensures global consistency through the use of information sets-sets that record constraints on policies consistent with backed-up Q-values. We prove that both the model-based and model-free algorithms using this backup remove delusional bias, yielding the first known algorithms that guarantee optimal results under general conditions. These algorithms furthermore only require polynomially many information sets (from a potentially exponential support). Finally, we suggest other practical heuristics for value-iteration and Q-learning that attempt to reduce delusional bias.", "sections": [{"heading": "Introduction", "text": "Q-learning is a foundational algorithm in reinforcement learning (RL) [34,26]. Although Q-learning is guaranteed to converge to an optimal state-action value function (or Q-function) when stateaction pairs are explicitly enumerated [34], it is potentially unstable when combined with function approximation (even simple linear approximation) [1,8,29,26]. Numerous modifications of the basic update, restrictions on approximators, and training regimes have been proposed to ensure convergence or improve approximation error [12,13,27,18,17,21]. Unfortunately, simple modifications are unlikely to ensure near-optimal performance, since it is NP-complete to determine whether even a linear approximator can achieve small worst-case Bellman error [23]. Developing variants of Qlearning with good worst-case behaviour for standard function approximators has remained elusive. Despite these challenges, Q-learning remains a workhorse of applied RL. The recent success of deep Q-learning, and its role in high-profile achievements [19], seems to obviate concerns about the algorithm's performance: the use of deep neural networks (DNNs), together with various augmentations (such as experience replay, hyperparameter tuning, etc.) can reduce instability and poor approximation. However, deep Q-learning is far from robust, and can rarely be applied successfully by inexperienced users. Modifications to mitigate systematic risks in Q-learning include double Q-learning [30], distributional Q-learning [4], and dueling network architectures [32]. A study of these and other variations reveals surprising results regarding the relative benefits of each under ablation [14]. Still, the full range of risks of approximation in Q-learning has yet to be delineated.\nIn this paper, we identify a fundamental problem with Q-learning (and other forms of dynamic programming) with function approximation, distinct from those previously discussed in the literature. Specifically, we show that approximate Q-learning suffers from delusional bias, in which updates are based on mutually inconsistent values. This inconsistency arises because the Q-update for a stateaction pair, (s, a), is based on the maximum value estimate over all actions at the next state, which ignores the fact that the actions so-considered (including the choice of a at s) might not be jointly realizable given the set of admissible policies derived from the approximator. These \"unconstrained\" updates induce errors in the target values, and cause a distinct source of value estimation error: Q-learning readily backs up values based on action choices that the greedy policy class cannot realize.\nOur first contribution is the identification and precise definition of delusional bias, and a demonstration of its detrimental consequences. From this new perspective, we are able to identify anomalies in the behaviour of Q-learning and value iteration (VI) under function approximation, and provide new explanations for previously puzzling phenomena. We emphasize that delusion is an inherent problem affecting the interaction of Q-updates with constrained policy classes-more expressive approximators, larger training sets and increased computation do not resolve the issue.\nOur second contribution is the development of a new policy-consistent backup operator that fully resolves the problem of delusion. Our notion of consistency is in the same spirit as, but extends, other recent notions of temporal consistency [5,22]. This new operator does not simply backup a single future value at each state-action pair, but instead backs up a set of candidate values, each with the associated set of policy commitments that justify it. We develop a model-based value iteration algorithm and a model-free Q-learning algorithm using this backup that carefully integrate valueand policy-based reasoning. These methods complement the value-based nature of value iteration and Q-learning with explicit constraints on the policies consistent with generated values, and use the values to select policies from the admissible policy class. We show that in the tabular case with policy constraints-isolating delusion-error from approximation error-the algorithms converge to an optimal policy in the admissible policy class. We also show that the number of information sets is bounded polynomially when the greedy policy class has finite VC-dimension; hence, the algorithms have polynomial-time iteration complexity in the tabular case.\nFinally, we suggest several heuristic methods for imposing policy consistency in batch Q-learning for larger problems. Since consistent backups can cause information sets to proliferate, we suggest search heuristics that focus attention on promising information sets, as well as methods that impose (or approximate) policy consistency within batches of training data, in an effort to drive the approximator toward better estimates.", "publication_ref": ["b33", "b25", "b33", "b0", "b7", "b28", "b25", "b11", "b12", "b26", "b17", "b16", "b20", "b22", "b18", "b29", "b3", "b31", "b13", "b4", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "A Markov decision process (MDP) is defined by a tuple M = (S, A, p, p 0 , R, \u03b3) specifying a set of states S and actions A; a transition kernel p; an initial state distribution p 0 ; a reward function R; and a discount factor \u03b3 \u2208 [0, 1]. A (stationary, deterministic) policy \u03c0 : S \u2192 A specifies the agent's action at every state s. The state-value function for \u03c0 is given by V \u03c0 (s) = E[ t\u22650 \u03b3 t R(s t , \u03c0(s t ))] while the state-action value (or Q-function) is Q \u03c0 (s, a) = R(s, a)+\u03b3 E p(s |s,a) V \u03c0 (s ), where expectations are taken over random transitions and rewards. Given any Q-function, the policy \"Greedy\" is defined by selecting an action a at state s that maximizes Q(s, a). If Q = Q * , then Greedy is optimal.\nWhen p is unknown, Q-learning can be used to acquire the optimal Q * by observing trajectories generated by some (sufficiently exploratory) behavior policy. In domains where tabular Q-learning is impractical, function approximation is typically used [33,28,26]. With function approximation, Q-values are approximated by some function from a class parameterized by \u0398 (e.g., the weights of a linear function or neural network). We let F = {f \u03b8 : S \u00d7A \u2192 R | \u03b8 \u2208 \u0398} denote the set of expressible value function approximators, and denote the class of admissible greedy policies by\nG(\u0398) = \u03c0 \u03b8 \u03c0 \u03b8 (s) = argmax a\u2208A f \u03b8 (s, a), \u03b8 \u2208 \u0398 .(1)\nIn such cases, online Q-learning at transition s, a, r, s (action a is taken at state s, leading to reward r and next state s ) uses the following update given a previously estimated Q-function Q \u03b8 \u2208 F,  \n\u03b8 \u2190 \u03b8 + \u03b1 r + \u03b3 max a \u2208A Q \u03b8 (s , a ) \u2212 Q \u03b8 (s, a) \u2207 \u03b8 Q \u03b8 (s, a).(2", "publication_ref": ["b32", "b27", "b25"], "figure_ref": [], "table_ref": []}, {"heading": "Delusional bias and its consequences", "text": "The problem of delusion can be given a precise statement (which is articulated mathematically in Section 4): delusional bias occurs whenever a backed-up value estimate is derived from action choices that are not realizable in the underlying policy class. A Q-update backs up values for each state-action pair (s, a) by independently choosing actions at the corresponding next states s via the max operator; this process implicitly assumes that max a \u2208A Q \u03b8 (s , a ) is achievable. However, the update can become inconsistent under function approximation: if no policy in the admissible class can jointly express all past (implicit) action selections, backed-up values do not correspond to Q-values that can be achieved by any expressible policy. (We note that the source of this potential estimation error is quite different than the optimistic bias of maximizing over noisy Q-values addressed by double Q-learning; see Appendix A.5.) Although the consequences of such delusional bias might appear subtle, we demonstrate how delusion can profoundly affect both Q-learning and value iteration. Moreover, these detrimental effects manifest themselves in diverse ways that appear disconnected, but are symptoms of the same underlying cause. To make these points, we provide a series of concrete counter-examples. Although we use linear approximation for clarity, the conclusions apply to any approximator class with finite capacity (e.g., DNNs with fixed architectures), since there will always be a set of d + 1 state-action choices that are jointly infeasible given a function approximation architecture with VC-dimension d < \u221e [31] (see Theorem 1 for the precise statement).", "publication_ref": ["b30"], "figure_ref": [], "table_ref": []}, {"heading": "A concrete demonstration", "text": "We begin with a simple illustration. Consider the undiscounted MDP in Fig. 1, where episodes start at s 1 , and there are two actions: a 1 causes termination, except at s 1 where it can move to s 4 with probability q; a 2 moves deterministically to the next state in the sequence s 1 to s 4 (with termination when a 2 taken at s 4 ). All rewards are 0 except for R(s 1 , a 1 ) and R(s 4 , a 2 ). For concreteness, let q = 0.1, R(s 1 , a 1 ) = 0.3 and R(s 4 , a 2 ) = 2. Now consider a linear approximator f \u03b8 (\u03c6(s, a)) with two state-action features: \u03c6(s 1 , a 1 ) = \u03c6(s 4 , a 1 ) = (0, 1); \u03c6(s 1 , a 2 ) = \u03c6(s 2 , a 2 ) = (0.8, 0); \u03c6(s 3 , a 2 ) = \u03c6(s 4 , a 2 ) = (\u22121, 0); and \u03c6(s 2 , a 1 ) = \u03c6(s 3 , a 1 ) = (0, 0). Observe that no \u03c0 \u2208 G(\u0398) can satisfy both \u03c0(s 2 ) = a 2 and \u03c0(s 3 ) = a 2 , hence the optimal unconstrained policy (take a 2 everywhere, with expected value 2) is not realizable. Q-updating can therefore never converge to the unconstrained optimal policy. Instead, the optimal achievable policy in G(\u0398) takes a 1 at s 1 and a 2 at s 4 (achieving a value of 0.5, realizable with \u03b8 * = (\u22122, 0.5)).\nUnfortunately, Q-updating is unable to find the optimal admissible policy \u03c0 \u03b8 * in this example. How this inability materializes depends on the update regime, so consider online Q-learning (Eq. 2) with data generated using an \u03b5Greedy behavior policy (\u03b5 = 0.5). In this case, it is not hard to show that Qlearning must converge to a fixed point\u03b8 = (\u03b8 1 ,\u03b8 2 ) where \u2212\u03b8 1 \u2264\u03b8 2 , implying that \u03c0\u03b8(s 2 ) = a 2 , i.e., \u03c0\u03b8 = \u03c0 \u03b8 * (we also show this for any \u03b5 \u2208 [0, 1 /2] when R(s 1 , a 1 ) = R(s 4 , a 2 ) = 1; see derivations in Appendix A.1). Instead, Q-learning converges to a fixed point that gives a \"compromised\" admissible policy which takes a 1 at both s 1 and s 4 (with a value of 0.3;\u03b8 \u2248 (\u22120.235, 0.279)).\nThis example shows how delusional bias prevents Q-learning from reaching a reasonable fixed-point.\nConsider the backups at (s 2 , a 2 ) and (s 3 , a 2 ). Suppose\u03b8 assigns a \"high\" value to (s 3 , a 2 ) (i.e., so that Q\u03b8(s 3 , a 2 ) > Q\u03b8(s 3 , a 1 )) as required by \u03c0 \u03b8 * ; intuitively, this requires that\u03b8 1 < 0, and generates a \"high\" bootstrapped value for (s 2 , a 2 ). But any update to\u03b8 that tries to fit this value (i.e., makes Q\u03b8(s 2 , a 2 ) > Q\u03b8(s 2 , a 1 )) forces\u03b8 1 > 0, which is inconsistent with the assumption,\u03b8 1 < 0, needed to generate the high bootstrapped value. In other words, any update that moves (s 2 , a 2 ) higher undercuts the justification for it to be higher. The result is that the Q-updates compete with each other, with Q\u03b8(s 2 , a 2 ) converging to a compromise value that is not realizable by any policy in G(\u0398).\nThis induces an inferior policy with lower expected value than \u03c0 \u03b8 * . We show in Appendix A.1 that avoiding any backup of these inconsistent edges results in Q-learning converging to the optimal expressible policy. Critically, this outcome is not due to approximation error itself, but the inability of Q-learning to find the value of the optimal representable policy.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Consequences of delusion", "text": "There are several additional manifestations of delusional bias that cause detrimental outcomes under Q-updating. Concrete examples are provided to illustrate each, but we relegate details to the appendix.\nDivergence: Delusional bias can cause Q-updating to diverge. We provide a detailed example of divergence in Appendix A.2 using a simple linear approximator. While divergence is typically attributed to the interaction of the approximator with Bellman or Q-backups, the example shows that if we correct for delusional bias, convergent behavior is restored. Lack of convergence due to cyclic behavior (with a lower-bound on learning rates) can also be caused by delusion: see Appendix A.3.\nThe Discounting Paradox: Another phenomenon induced by delusional bias is the discounting paradox: given an MDP with a specific discount factor \u03b3 eval , Q-learning with a different discount \u03b3 train results in a Q-function whose greedy policy has better performance, relative to the target \u03b3 eval , than when trained with \u03b3 eval . In Appendix A.4, we provide an example where the paradox is extreme: a policy trained with \u03b3 = 1 is provably worse than one trained myopically with \u03b3 = 0, even when evaluated using \u03b3 = 1. We also provide an example where the gap can be made arbitrarily large. These results suggest that treating the discount as hyperparameter might yield systematic training benefits; we demonstrate that this is indeed the case on some benchmark (Atari) tasks in Appendix A.10.\nApproximate Dynamic Programming: Delusional bias arises not only in Q-learning, but also in approximate dynamic programming (ADP) (e.g., [6,9]), such as approximate value iteration (VI). With value function approximation, VI performs full state Bellman backups (as opposed to sampled backups as in Q-learning), but, like Q-learning, applies the max operator independently at successor states when computing expected next state values. When these choices fall outside the greedy policy class admitted by the function approximator, delusional bias can arise. Delusion can also occur with other forms of policy constraints (without requiring the value function itself to be approximated).\nBatch Q-learning: In the example above, we saw that delusional bias can cause convergence to Q-functions that induce poor (greedy) policies in standard online Q-learning. The precise behavior depends on the training regime, but poor behavior can emerge in batch methods as well. For instance, batch Q-learning with experience replay and replay buffer shuffling will induce the same tension between the conflicting updates. Specific (nonrandom) batching schemes can cause even greater degrees of delusion; for example, training in a sequence of batches that run through a batch of transitions at s 4 , followed by batches at s 3 , then s 2 , then s 1 will induce a Q-function that deludes itself into estimating the value of (s 1 , a 2 ) to be that of the optimal unconstrained policy.\n4 Non-delusional Q-learning and dynamic programming\nWe now develop a provably correct solution that directly tackles the source of the problem: the potential inconsistency of the set of Q-values used to generate a Bellman or Q-backup. Our approach avoids delusion by using information sets to track the \"dependencies\" contained in all Q-values, i.e., the policy assumptions required to justify any such Q-value. Backups then prune infeasible values whose information sets are not policy-class consistent. Since backed-up values might be designated inconsistent when new dependencies are added, this policy-consistent backup must maintain alternative information sets and their corresponding Q-values, allowing the (implicit) backtracking of prior decisions (i.e., max Q-value choices). Such a policy-consistent backup can be viewed as unifying both value-and policy-based RL methods, a perspective we detail in Sec. 4.3.\nWe develop policy consistent backups in the tabular case while allowing for an arbitrary policy class (or arbitrary policy constraints)-the case of greedy policies with respect to some approximation architecture f \u03b8 is simply a special case. This allows the method to focus on delusion, without making any assumptions about the specific value approximation. Because delusion is a general phenomenon, we first develop a model-based consistent backup, which gives rise to non-delusional policy-class value iteration, and then describe the sample-backup version, policy-class Q-learning.\nOur main theorem establishes the convergence, correctness and optimality of the algorithm (including for all s, a do 6: end for 10: until Q converges: dom(Q(sa)) and Q(sa)(X) does not change for all s, a, X 11: /* Then recover an optimal policy */ 12: X * \u2190 argmax X ConQ[s 0 ](X) 13: q * \u2190 ConQ[s 0 ](X * ) 14: \u03b8 * \u2190 Witness(X * ) 15: return \u03c0 \u03b8 * and q * . the complete removal of delusional bias), and computational tractability (subject to a tractable consistency oracle).\nQ[sa] \u2190 R sa + \u03b3 s p(s | s, a)ConQ[s ] 7: ConQ[sa](Z) \u2190 Q[sa](X) for all X such that Z = X \u2229 [s \u2192 a]", "publication_ref": ["b5", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Policy-class value iteration", "text": "We begin by defining policy-class value iteration (PCVI), a new VI method that operates on collections of information sets to guarantee discovery of the optimal policy in a given class. For concreteness, we specify a policy class using Q-function parameters, which determines the class of realizable greedy policies (just as in classical VI). Proofs and more formal definitions can be found in Appendix A.6. We provide a detailed illustration of the PCVI algorithm in Appendix A.7, walking through the steps of PCVI on the example MDP in Fig. 1.\nAssume an MDP with n states S = {s 1 , . . . , s n } and m actions A = {a 1 , . . . , a m }. Let \u0398 be the parameter class defining Q-functions. Let F and G(\u0398), as above, denote the class of expressible value functions and admissible greedy policies respectively. (We assume ties are broken in some canonical fashion.) Define [s \u2192 a] = {\u03b8 \u2208 \u0398 | \u03c0 \u03b8 (s) = a}. An information set X \u2286 \u0398 is a set of parameters (more generally, policy constraints) that justify assigning a particular Q-value q to some (s, a) pair. Below we use the term \"information set\" to refer both to X and (X, q) as needed. Information sets will be organized into finite partitions of \u0398, i.e., a set of non-empty subsets P = {X 1 , . . . , X k } such that X 1 \u222a \u2022 \u2022 \u2022 \u222a X k = \u0398 and X i \u2229 X j = \u2205, for all i = j. We abstractly refer to the elements of P as cells. A partition P is a refinement of P if for all X \u2208 P there exists an X \u2208 P such that X \u2286 X. Let P(\u0398) be the set of all finite partitions of \u0398. A partition function h : P \u2192 R associates values (e.g., Q-values) with all cells (e.g., information sets). Let H = {h : P \u2192 R | P \u2208 P(\u0398)} denote the set of all such partition functions. Define the intersection sum for h 1 , h 2 \u2208 H to be:\n(h 1 \u2295 h 2 )(X 1 \u2229 X 2 ) = h 1 (X 1 ) + h 2 (X 2 ), \u2200X 1 \u2208 dom(h 1 ), X 2 \u2208 dom(h 2 ), X 1 \u2229 X 2 = \u2205.\nNote that the intersection sum incurs at most a quadratic blowup:\n|dom(h)| \u2264 |dom(h 1 )| \u2022 |dom(h 2 )|.\nThe methods below require an oracle to check whether a policy \u03c0 \u03b8 is consistent with a set of state-toaction constraints: i.e., given {(s, a)} \u2286 S \u00d7 A, whether there exists a \u03b8 \u2208 \u0398 such that \u03c0 \u03b8 (s) = a for all pairs. We assume access to such an oracle, \"Witness\". For linear Q-function parameterizations, Witness can be implemented in polynomial time by checking the consistency of a system of linear inequalities. PCVI, shown in Alg. 1, computes the optimal policy \u03c0 \u03b8 * \u2208 G(\u0398) by using information sets and their associated Q-values organized into partitions (i.e., partition functions over \u0398). We represent Q-functions using a table Q with one entry Q[sa] for each (s, a) pair. Each such Q[sa] is a partition function over dom(Q[sa]) \u2208 P(\u0398). For each X i \u2208 dom(Q[sa]) (i.e., for each information set X i \u2286 \u0398 associated with (s, a)), we assign a unique Q-value Q[sa](X i ). Intuitively, the Q-value Q[sa](X i ) is justified only if we limit attention to policies {\u03c0 \u03b8 : \u03b8 \u2208 X i }. Since dom(Q[sa]) is a partition, we have a Q-value for any realizable policy. \nX i \u2229 [s \u2192 a] = \u2205 for some X i \u2208 dom(Q[sa]), the corresponding Q-value disappears in ConQ[sa]. Finally, ConQ[s] = \u222a a ConQ[sa]\nis the partition function over \u0398 obtained by collecting all the \"restricted\" action value functions. Since \u222a a [s \u2192 a] is a partition of \u0398, so is ConQ[s].\nThe key update in Alg. 1 is Line 6, which jointly updates all Q-values of the relevant sets of policies in G(\u0398). Notice that the maximization typically found in VI is not present-this is because the operation computes and records Q-values for all choices of actions at the successor state s . This is the key to allowing VI to maintain consistency: if a future Bellman backup is inconsistent with some previous max-choice at a reachable state, the corresponding cell will be pruned and an alternative maximum will take its place. Pruning of cells, using the oracle Witness, is implicit in Line 6 (pruning of \u2295) and Line 7 (where non-emptiness is tested). 1 Convergence of PCVI requires that each Q[sa] table-both its partition and associated Q-value-converge to a fixed point. Theorem 1 (PCVI Theorem). PCVI (Alg. 1) has the following guarantees:\n(a) (Convergence and correctness) The function Q converges and, for each s \u2208 S, a \u2208 A, and any \u03b8 \u2208 \u0398: there is a unique X \u2208 dom(Q[sa]) such that \u03b8 \u2208 X and Q \u03c0 \u03b8 (s, a) = Q[sa](X).\n(b) (Optimality and non-delusion) \u03c0 \u03b8 * is an optimal policy within G(\u0398) and q * is its value. (A more complete statement of the Cor. 2 is found in Appendix A.6.) The number of cells in a partition may be significantly less than suggested by the bounds, as it depends on the reachability structure of the MDP. For example, in an MDP with only self-transitions, the partition for each state has a single cell. We note that Witness is tractable for linear approximators, but is NP-hard for DNNs [7]. The poly-time result in Cor. 2 does not contradict the NP-hardness of finding a linear approximator with small worst-case Bellman error [23], since nothing is asserted about the Bellman error and we are treating the approximator's VC-dimension as a constant.\nDemonstrating PCVI: We illustrate PCVI with a simple example that shows how poorly classical approaches can perform with function approximation, even in \"easy\" environments. Consider a simple deterministic grid world with the 4 standard actions and rewards of 0, except 1 at the top-right, 2 at the bottom-left, and 10 at the bottom-right corners; the discount is \u03b3 = 0.95. The agent starts at the top-left. The optimal policy is to move down the left side to the left-bottom corner, then along the bottom to the right bottom corner, then staying. To illustrate the effects of function approximation, we considered linear approximators defined over random feature representations: feature vectors were produced for each state-action pair by drawing independent standard normal values. Fig. 2 shows the estimated maximum value achievable from the start state produced by each method (dark lines), along with the actual expected value achieved by the greedy policies produced by each method (light lines). The left figure shows results for a 4 \u00d7 4 grid with 4 random features, and the right for a 5 \u00d7 5 grid with 5 random features. Results are averaged over 10 runs with different random feature sets (shared by the algorithms). Surprisingly, even when the linear approximator can support near-optimal policies, classical methods can utterly fail to realize this possibility: in 9 of 10 trials (4 \u00d7 4) and 10 of 10 trials (5 \u00d7 5) the classical methods produce greedy policies with an expected value of zero, while PCVI produces policies with value comparable to the global optimum.   ", "publication_ref": ["b0", "b6", "b22"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Policy-class Q-learning", "text": "A tabular version of Q-learning using the same partition-function representation of Q-values as in PCVI yields policy-class Q-learning PCQL, shown in Alg. 2. 2 The key difference with PCVI is simply that we use sample backups in Line 4 instead of full Bellman backups as in PCVI.\nAlgorithm 2 Policy-Class Q-learning (PCQL) Input: Batch B = {(s t , a t , r t , s t )} T t=1 , \u03b3, \u0398, scalars \u03b1 sa t . 1: for (s, a, r, s ) \u2208 B, t is iteration counter do\n2: For all a , if s a \u2208 ConQ then initialize ConQ[s a ] \u2190 ([s \u2192 a ] \u2192 0). 3: Update ConQ[s ] by combining ConQ[s a ](X), for all a , X \u2208 dom(ConQ[s a ]) 4: Q[sa] \u2190 (1 \u2212 \u03b1 sa t )Q[sa] \u2295 \u03b1 sa t (r + \u03b3ConQ[s ]) 5: ConQ[sa](Z) \u2190 Q[sa](X) for all X such that Z = X \u2229 [s \u2192 a]\nis non-empty 6: end for 7: Return ConQ, Q\nThe method converges under the usual assumptions for Q-learning: a straightforward extension of the proof for PCVI, replacing full VI backups with Q-learning-style sample backups, yields the following: Theorem 3. The (a) convergence and correctness properties and (b) optimality and non-delusion properties associated with the PCVI Theorem 1 hold for PCQL, assuming the usual sampling requirements, the Robbins-Monro stochastic convergence conditions on learning rates \u03b1 sa t and access to the Witness oracle.\nDemonstrating PCQL: We illustrate PCQL in the same grid world tasks as before, again using random features. Figure 2 shows that PCQL achieves comparable performance to PCVI, but with lighter time and space requirements, and is still significantly better than classical methods.\nWe also applied PCQL to the initial illustrative example in Fig. 1 with R(s 1 , a 1 ) = 0.3, R(s 4 , a 2 ) = 2 and uniform random exploration as the behaviour policy, adding the use of a value approximator (a linear regressor). We use a heuristic that maintains a global partition of \u0398 with each cell X holding a regressor ConQ(s, a; w X ), for w X \u2208 \u0398 predicting the consistent Q-value at s, a (see details in Sec. 5 and Appendix A.8). The method converges with eight cells corresponding to the realizable policies. The policy (equivalence class) is ConQ(s 1 , \u03c0 X (s 1 ); w X ) where \u03c0 X (s 1 ) is the cell's action at s 1 ; the value is w X \u2022 \u03c6(s, \u03c0 X (s 1 )). The cell X * with the largest such value at s 1 is indeed the optimal realizable policy: it takes a 1 at s 1 and s 2 , and a 2 elsewhere. The regressor w X * \u2248 (\u22122, 0.5) fits the consistent Q-values perfectly, yielding optimal (policy-consistent) Q-values, because ConQ need not make tradeoffs to fit inconsistent values.", "publication_ref": ["b1"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Unification of value-and policy-based RL", "text": "We can in some sense interpret PCQL (and, from the perspective of model-based approximate dynamic programming, PCVI) as unifying value-and policy-based RL. One prevalent view of value-based RL methods with function approximation, such Q-learning, is to find an approximate value function or Q-function (VF/QF) with low Bellman error (BE), i.e., where the (pointwise) difference between the approximate VF/QF and its Bellman backup is small. In approximate dynamic programming one often tries to minimize this directly, while in Q-learning, one usually fits a regression model to minimize the mean-squared temporal difference (a sampled form of Bellman error minimization) over a training data set. One reason for this emphasis on small BE is that the max norm of BE can be used to directly bound the (max norm) loss of the value of greedy policy induced by the approximate VF/QF and the value of the optimal policy. It is this difference in performance that is of primary interest.\nUnfortunately, the bounds on induced policy quality using the BE approximation are quite loose, typically 2||BE|| \u221e /(1 \u2212 \u03b3) (see [6], bounds with p norm are similar [20]). As such, minimizing BE does not generally provide policy guarantees of practical import (see, e.g., [11]). As we see in the cases above (and also in the appendix) that involve delusional bias, a small BE can in fact be rather misleading with respect to the induced policy quality. For example, Q-learning, using least squares to minimize the TD-error as a proxy for BE, often produces policies of poor quality.\nPCQL and PCVI take a different perspective, embracing the fact that the VF/QF approximator strictly limits that class of greedy policies that can be realized. In these algorithms, no Bellman backup or Q-update ever involves values that cannot be realized by an admissible policy. This will often result in VFs/QFs with greater BE than their classical counterparts. But, in the exact tabular case, we derive the true value of the induced (approximator-constrained) policy and guarantee that it is optimal. In the regression case (see Sec. 5), we might view this as attempting to minimize BE within the class of admissible policies, since we only regress toward policy-consistent values.\nThe use of information sets and consistent cells effectively means that PCQL and PCVI are engaging in policy search-indeed, in the algorithms presented here, they can be viewed as enumerating all consistent policies (in the case of Q-learning, distinguishing only those that might differ on sampled data). In contrast to other policy-search methods (e.g., policy gradient), both PCQL and PCVI use (sampled or full) Bellman backups to direct the search through policy space, while simultaneously using policy constraints to limit the Bellman backups that are actually realized. They also use these values to select an optimal policy from the feasible policies generated within each cell.", "publication_ref": ["b5", "b19", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Toward practical non-delusional Q-learning", "text": "The PCVI and PCQL algorithms can be viewed as constructs that demonstrate how delusion arises and how it can be eliminated in Q-learning and approximate dynamic programming by preventing inadmissible policy choices from influencing Q-values. However, the algorithms maintain information sets and partition functions, which is impractical with massive state and action sets. In this section, we suggest several heuristic methods that allow the propagation of some dependency information in practical Q-learning to mitigate the effects of delusional bias.\nMultiple regressors: With multiple information sets (or cells), we no longer have a unique set of labels with which to fit an approximate Q-function regressor (e.g., DNN or linear approximator). Instead, each cell has its own set of labels. Thus, if we maintain a global collection of cells, each with its own Q-regressor, we have a set of approximate Q-functions that give both a compact representation and the ability to generalize across state-action pairs for any set of policy consistent assumptions. This works in both batch and pure online Q-learning (see Appendix A.8 for details.)\nThe main challenge is the proliferation of information sets. One obvious way to address this is to simply limit the total number of cells and regressors: given the current set of regressors, at any update, we first create the (larger number of) new cells needed for the new examples, fit the regressor for each new consistent cell, then prune cells according to some criterion to keep the total number of regressors manageable. This is effectively a search through the space of information sets and can be managed using a variety of methods (branch-and-bound, beam search, etc.). Criteria for generating, sampling and/or pruning cells can involve: (a) the magnitude to the Q-labels (higher expected values are better); (b) the constraints imposed by the cell (less restrictive is better, since it minimizes future inconsistency); the diversity of the cell assignments (since the search frontier is used to manage \"backtracking\").\nIf cell search maintains a restricted frontier, our cells may no longer cover all of policy space (i.e, Q is no longer a partition of \u0398). This runs the risk that some future Q-updates may not be consistent with any cell. If we simply ignore such updates, the approach is hyper-vigilant, guaranteeing policy-class consistency at the expense of losing training data. An alternative relaxed approach is to merge cells to maintain a full partition of policy space (or prune cells and in some other fashion relax the constraints of the remaining cells to recover a partition). This relaxed approach ensures that all training data is used, but risks allowing some delusion to creep into values by not strictly enforcing all Q-value dependencies.\nQ-learning with locally consistent data: An alternative approach is to simply maintain a single regressor, but ensure that any batch of Q-labels is self-consistent before updating the regressor. Specifically, given a batch of training data and the current regressor, we first create a single set of consistent labels for each example (see below), then update the regressor using these labels. With no information sets, the dependencies that justified the previous regressor are not accounted for when constructing the new labels. This may allow delusion to creep in; but the aim is that this heuristic approach may mitigate its effects since each new regressor is at least \"locally\" consistent with respect to its own updates. Ideally, this will keep the sequence of approximations in a region of \u03b8-space where delusional bias is less severe. Apart from the use of a consistent labeling procedure, this approach incurs no extra overhead relative to Q-learning.\nOracles and consistent labeling: The first approach above requires an oracle, Witness, to test consistency of policy choices, which is tractable for linear approximators (linear feasibility test), but requires solving an integer-quadratic program when using DQN (e.g., a ReLU network). The second approach needs some means for generating consistent labels. Given a batch of examples B = {(s t , a t , r t , s t )} T t=1 , and a current regressor Q, labels are generated by selecting an a t for each s t as the max. The selection should satisfy: (a) \u2229 t [s t \u2192 a t ] = \u2205 (i.e., selected max actions are mutually consistent); and (b) [s t \u2192 a t ] \u2229 [s t \u2192 a t ] = \u2205, for all t (i.e., choice at s t is consistent with taking a t at s t ). We can find a consistent labeling maximizing some objective (e.g., sum of resulting labels), subject to these constraints. For a linear approximator, the problem can be formulated as a (linear) mixed integer program (MIP); and is amenable to several heuristics (see Appendix A.9).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have identified delusional bias, a fundamental problem in Q-learning and approximate dynamic programming with function approximation or other policy constraints. Delusion manifests itself in different ways that lead to poor approximation quality or divergence for reasons quite independent of approximation error itself. Delusional bias thus becomes an important entry in the catalog of risks that emerge in the deployment of Q-learning. We have developed and analyzed a new policyclass consistent backup operator, and the corresponding model-based PCVI and model-free PCQL algorithms, that fully remove delusional bias. We also suggested several practical heuristics for large-scale RL problems to mitigate the effect of delusional bias.\nA number of important direction remain. The further development and testing of practical heuristics for policy-class consistent updates, as well as large-scale experiments on well-known benchmarks, is critical. This is also important for identifying the prevalence of delusional bias in practice. Further development of practical consistency oracles for DNNs and consistent label generation is also of interest. We are also engaged in a more systematic study of the discounting paradox and the use of the discount factor as a hyper-parameter.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Residual algorithms: Reinforcement learning with function approximation", "journal": "", "year": "1995", "authors": "Leemon Baird"}, {"ref_id": "b1", "title": "Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks", "journal": "", "year": "2017", "authors": "L Peter; Nick Bartlett; Chris Harvey; Abbas Liaw;  Mehrabian"}, {"ref_id": "b2", "title": "The arcade learning environment: An evaluation platform for general agents", "journal": "Journal of Artificial Intelligence Research", "year": "2013-06", "authors": "M G Bellemare; Y Naddaf; J Veness; M Bowling"}, {"ref_id": "b3", "title": "A distributional perspective on reinforcement learning", "journal": "", "year": "2017", "authors": "G Marc; Will Bellemare; R\u00e9mi Dabney;  Munos"}, {"ref_id": "b4", "title": "Increasing the action gap: New operators for reinforcement learning", "journal": "", "year": "2016", "authors": "G Marc; Georg Bellemare; Arthur Ostrovski; Philip S Guez; R\u00e9mi Thomas;  Munos"}, {"ref_id": "b5", "title": "Neuro-dynamic Programming", "journal": "", "year": "1996", "authors": "Dimitri P Bertsekas; John N Tsitsiklis"}, {"ref_id": "b6", "title": "Training a 3-node neural network is NP-complete", "journal": "", "year": "1988", "authors": "Avrim Blum; Ronald L Rivest"}, {"ref_id": "b7", "title": "Generalization in reinforcement learning: Safely approximating the value function", "journal": "MIT Press", "year": "1995", "authors": "Justin A Boyan; Andrew W Moore"}, {"ref_id": "b8", "title": "The linear programming approach to approximate dynamic programming", "journal": "Operations Research", "year": "2003", "authors": "Daniela Pucci De Farias; Benjamin Van Roy"}, {"ref_id": "b9", "title": "Probability: Theory and Examples", "journal": "Cambridge University Press", "year": "2013", "authors": "Rick Durrett"}, {"ref_id": "b10", "title": "Is the Bellman residual a bad proxy?", "journal": "", "year": "2017", "authors": "Matthieu Geist; Bilal Piot; Olivier Pietquin"}, {"ref_id": "b11", "title": "Stable function approximation in dynamic programming", "journal": "", "year": "1995", "authors": "Geoffrey J Gordon"}, {"ref_id": "b12", "title": "Approximation Solutions to Markov Decision Problems", "journal": "", "year": "1999", "authors": "Geoffrey J Gordon"}, {"ref_id": "b13", "title": "Rainbow: Combining improvements in deep reinforcement learning", "journal": "", "year": "2017", "authors": "Matteo Hessel; Joseph Modayil; Tom Hado Van Hasselt; Georg Schaul; Will Ostrovski; Dan Dabney; Bilal Horgan; Mohammad Piot; David Azar;  Silver"}, {"ref_id": "b14", "title": "The dependence of effective planning horizon on model accuracy", "journal": "", "year": "2015", "authors": "Nan Jiang; Alex Kulesza; Satinder Singh; Richard Lewis"}, {"ref_id": "b15", "title": "On value function representation of long horizon problems", "journal": "", "year": "2018", "authors": "Lucas Lehnert; Romain Laroche; Harm Van Seijen"}, {"ref_id": "b16", "title": "Toward off-policy learning control with function approximation", "journal": "", "year": "2010", "authors": "Hamid Maei; Csaba Szepesv\u00e1ri; Shalabh Bhatnagar; Richard S Sutton"}, {"ref_id": "b17", "title": "Q-learning with linear function approximation", "journal": "", "year": "2007", "authors": "Francisco Melo; M Ribeiro"}, {"ref_id": "b18", "title": "Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning", "journal": "Science", "year": "2015", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Andrei Rusu; Joel Veness; Marc Bellemare; Alex Graves; Martin Riedmiller; Andreas Fidjeland; Georg Ostrovski; Stig Petersen"}, {"ref_id": "b19", "title": "Performance bounds in lp-norm for approximate value iteration", "journal": "SIAM Journal on Control and Optimization", "year": "2007", "authors": "R\u00e9mi Munos"}, {"ref_id": "b20", "title": "Safe and efficient off-policy reinforcement learning", "journal": "", "year": "2016", "authors": "R\u00e9mi Munos; Thomas Stepleton; Anna Harutyunyan; Marc G Bellemare"}, {"ref_id": "b21", "title": "Bridging the gap between value and policy based reinforcement learning", "journal": "", "year": "2017", "authors": "Ofir Nachum; Mohammad Norouzi; Kelvin Xu; Dale Schuurmans"}, {"ref_id": "b22", "title": "Optimization-based Approximate Dynamic Programming", "journal": "", "year": "2010", "authors": "Marek Petrik"}, {"ref_id": "b23", "title": "On the density of families of sets", "journal": "Journal of Combinatorial Theory, Series A", "year": "1972-07", "authors": "Norbert Sauer"}, {"ref_id": "b24", "title": "A combinatorial problem; stability and order for models and theories in infinitary languages", "journal": "Pacific Journal of Mathematics", "year": "1972-04", "authors": "Saharon Shelah"}, {"ref_id": "b25", "title": "Reinforcement Learning: An Introduction", "journal": "MIT Press", "year": "2018", "authors": "Richard S Sutton; Andrew G Barto"}, {"ref_id": "b26", "title": "Interpolation-based Q-learning", "journal": "", "year": "2004", "authors": "Csaba Szepesv\u00e1ri; William Smart"}, {"ref_id": "b27", "title": "Practical issues in temporal difference learning", "journal": "", "year": "1992-05", "authors": "Gerald Tesauro"}, {"ref_id": "b28", "title": "An analysis of temporal-difference learning with function approximation", "journal": "IEEE Transactions on Automatic Control", "year": "1996", "authors": "H John; Benjamin Tsitsiklis;  Van Roy"}, {"ref_id": "b29", "title": "Double Q-learning", "journal": "", "year": "2010", "authors": " Hado Van Hasselt"}, {"ref_id": "b30", "title": "Statistical Learning Theory", "journal": "Wiley-Interscience", "year": "1998-09", "authors": "Vladimir N Vapnik"}, {"ref_id": "b31", "title": "Dueling network architectures for deep reinforcement learning", "journal": "", "year": "2016", "authors": "Ziyu Wang; Tom Schaul; Matteo Hessel; Marc Hado Van Hasselt; Nando Lanctot;  De Freitas"}, {"ref_id": "b32", "title": "Learning from Delayed Rewards", "journal": "", "year": "1989-05", "authors": "J C H Christopher;  Watkins"}, {"ref_id": "b33", "title": "Q-learning", "journal": "", "year": "1992", "authors": "J C H Christopher; Peter Watkins;  Dayan"}], "figures": [{"figure_label": "4111421", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "4 R(s 1 , a 1 ) 1 R(s 4 , a 2 )Figure 1 :4111421Figure 1: A simple MDP that illustrates delusional bias (see text for details).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "(The partitions dom(Q[sa]) for each (s, a) generally differ.) ConQ[sa] is a restriction of Q[sa] obtained by intersecting each cell in its domain, dom(Q[sa]), with [s \u2192 a]. In other words, ConQ[sa] is a partition function defined on some partition of [s \u2192 a] (rather than all of \u0398), and represents Q-values of cells that are consistent with [s \u2192 a]. Thus, if", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "(c) (Runtime bound) Assume \u2295 and non-emptiness checks (lines 6 and 7) have access to Witness.LetG = {g \u03b8 (s, a, a ) := 1[f \u03b8 (s, a) \u2212 f \u03b8 (s, a ) > 0], \u2200s, a = a | \u03b8 \u2208 \u0398}. Each iteration of Alg. 1 runs in time O(nm \u2022 [ m 2 n] 2 VCDim(G) (m \u2212 1)w)where VCDim(\u2022) denotes the VC-dimension of a set of boolean-valued functions, and w is the worst-case running time of Witness (with at most nm state-action constraints). Combined with Part (a), if VCDim(G) is finite then Q converges in time polynomial in n, m and w. Corollary 2. Alg. 1 runs in polynomial time for linear greedy policies. It runs in polynomial time in the presence of a polynomial time Witness for deep Q-network (DQN) greedy policies.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: Planning and learning in a grid world with random feature representations. (Left: 4 \u00d7 4 grid using 4 features; Right: 5 \u00d7 5 grid using 5 features.) Here \"iterations\" means a full sweep over state-action pairs, except for Q-learning and PCQL, where an iteration is an episode of length 3/(1 \u2212 \u03b3) = 60 using \u03b5Greedy exploration with \u03b5 = 0.7. Dark lines: estimated maximum achievable expected value. Light lines: actual expected value achieved by greedy policy.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Algorithm 1 Policy-Class Value Iteration (PCVI) Input: S, A, p(s | s, a), R, \u03b3, \u0398, initial state s 0 1: Q[sa] \u2190 initialize to mapping \u0398 \u2192 0 for all s, a 2: ConQ[sa] \u2190 initialize to mapping [s \u2192 a] \u2192 0 for all s, a 3: Update ConQ[s] for all s (i.e., combine all table entries in ConQ[sa 1 ], . . . , ConQ[sa m ])", "figure_data": "4: repeat5:"}], "formulas": [{"formula_id": "formula_0", "formula_text": "G(\u0398) = \u03c0 \u03b8 \u03c0 \u03b8 (s) = argmax a\u2208A f \u03b8 (s, a), \u03b8 \u2208 \u0398 .(1)", "formula_coordinates": [2.0, 205.04, 608.29, 298.96, 17.83]}, {"formula_id": "formula_1", "formula_text": "\u03b8 \u2190 \u03b8 + \u03b1 r + \u03b3 max a \u2208A Q \u03b8 (s , a ) \u2212 Q \u03b8 (s, a) \u2207 \u03b8 Q \u03b8 (s, a).(2", "formula_coordinates": [2.0, 187.38, 674.51, 312.75, 17.29]}, {"formula_id": "formula_2", "formula_text": "Q[sa] \u2190 R sa + \u03b3 s p(s | s, a)ConQ[s ] 7: ConQ[sa](Z) \u2190 Q[sa](X) for all X such that Z = X \u2229 [s \u2192 a]", "formula_coordinates": [5.0, 112.98, 153.42, 287.15, 28.19]}, {"formula_id": "formula_3", "formula_text": "(h 1 \u2295 h 2 )(X 1 \u2229 X 2 ) = h 1 (X 1 ) + h 2 (X 2 ), \u2200X 1 \u2208 dom(h 1 ), X 2 \u2208 dom(h 2 ), X 1 \u2229 X 2 = \u2205.", "formula_coordinates": [5.0, 116.22, 573.04, 379.56, 17.29]}, {"formula_id": "formula_4", "formula_text": "|dom(h)| \u2264 |dom(h 1 )| \u2022 |dom(h 2 )|.", "formula_coordinates": [5.0, 363.93, 592.23, 141.81, 17.29]}, {"formula_id": "formula_5", "formula_text": "X i \u2229 [s \u2192 a] = \u2205 for some X i \u2208 dom(Q[sa]), the corresponding Q-value disappears in ConQ[sa]. Finally, ConQ[s] = \u222a a ConQ[sa]", "formula_coordinates": [6.0, 108.0, 134.27, 397.74, 28.19]}, {"formula_id": "formula_6", "formula_text": "2: For all a , if s a \u2208 ConQ then initialize ConQ[s a ] \u2190 ([s \u2192 a ] \u2192 0). 3: Update ConQ[s ] by combining ConQ[s a ](X), for all a , X \u2208 dom(ConQ[s a ]) 4: Q[sa] \u2190 (1 \u2212 \u03b1 sa t )Q[sa] \u2295 \u03b1 sa t (r + \u03b3ConQ[s ]) 5: ConQ[sa](Z) \u2190 Q[sa](X) for all X such that Z = X \u2229 [s \u2192 a]", "formula_coordinates": [7.0, 112.98, 423.83, 339.49, 50.01]}], "doi": ""}