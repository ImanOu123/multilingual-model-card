{"title": "Learning to Combine Bottom-Up and Top-Down Segmentation", "authors": "Anat Levin; Yair Weiss", "pub_date": "", "abstract": "Bottom-up segmentation based only on low-level cues is a notoriously difficult problem. This difficulty has lead to recent top-down segmentation algorithms that are based on class-specific image information. Despite the success of top-down algorithms, they often give coarse segmentations that can be significantly refined using low-level cues. This raises the question of how to combine both top-down and bottom-up cues in a principled manner. In this paper we approach this problem using supervised learning. Given a training set of ground truth segmentations we train a fragment-based segmentation algorithm which takes into account both bottom-up and top-down cues simultaneously, in contrast to most existing algorithms which train top-down and bottom-up modules separately. We formulate the problem in the framework of Conditional Random Fields (CRF) and derive a feature induction algorithm for CRF, which allows us to efficiently search over thousands of candidate fragments. Whereas pure top-down algorithms often require hundreds of fragments, our simultaneous learning procedure yields algorithms with a handful of fragments that are combined with low-level cues to efficiently compute high quality segmentations.", "sections": [{"heading": "Introduction", "text": "Figure 1 (replotted from [2]) illustrates the importance of combining top-down and bottom-up segmentation. The leftmost image shows an image of a horse and the middle column show three possible segmentations based only on low-level cues. Even a sophisticated bottom-up segmentation algorithm (e.g. [12,16]) has difficulties correctly segmenting this image.\nThe difficulty in pure low-level segmentation has led to the development of topdown, class-specific segmentation algorithms [3,11,22,19]. These algorithms fit a deformable model of a known object (e.g. a horse) to the image -the shape of the deformed model gives an estimate of the desired segmentation. The right-hand column of figure 1 shows a top-down segmentation of the horse figure obtained by the algorithm of [3]. In this algorithm, image fragments from horses in a training database are correlated with the novel image. By combining together the segmentations of the fragments, the novel image is segmented. As can be seen, the top-down segmentation is better than any of the bottom-up segmentations but still misses important details. In recent years, several authors have therefore suggested combining top-down and bottom-up segmentation [2,21,17,6]. Borenstein et al. [2] choose among a discrete set of possible low-level segmentations by minimizing a cost function that includes a bias towards the top-down segmentation. In the image parsing framework of Tu et al. [17] object-specific detectors serve as a proposal distribution for a data-driven Monte-Carlo sampling over possible segmentations. In the OBJ-CUT algorithm [6] a layered pictorial structure is used to define a bias term for a graph-cuts energy minimization algorithm (the energy favors segmentation boundaries occurring at image discontinuities).\nThese recent approaches indeed improve the quality of the achieved segmentations by combining top-down and bottom-up cues at run-time. However, the training of the bottom-up and top-down modules is performed independently. In the work of Borenstein and colleagues, training the top-down module consists of choosing a set of fragments from a huge set of possible image fragments. This training is performed without taking into account low-level cues. In the image parsing framework [17], the top-down module are object detectors trained using AdaBoost to maximize detection performance. Again, this training is performed without taking into account low-level cues. In the OBJ-CUT algorithm, the training of the algorithm is based on a set of learned layered pictorial structures [6]. These learned models are then used to define a detection cascade (which calculates putative part locations by comparing the image to a small number of templates) and a bounding box for the relative part locations. Again, the choice of which templates to apply to a given images is performed independent of the low-level segmentation cues.\nFigure 2(a) shows a potential disadvantage of training the top-down model while ignoring low-level cues. Suppose we wish to train a segmentation algorithm for octopi. Since octopi have 8 tentacles and each tentacle has multiple degrees of freedom, any top-down algorithm would require a very complex deformable template to achieve reasonable performance. Consider for example the top-down algorithm of Borenstein and Ullman [3] which tries to cover the segmentations in the dataset with a subset of image fragments. It would obviously require a huge number of fragments to achieve reasonable performance. Similarly, the layered pictorial structure algorithm of Kumar et al. [6] would require a large number of parts and a complicated model for modeling the allowed spatial configurations. While Octopi can appear in a large number of poses, their low-level segmentation can be easy since their color is relatively uniform and (depending on the scene) may be distinct from the background. Thus an algorithm that trains the top-down module while taking into account the low-level cues can choose to devote far less resources to the deformable templates. The challenge is to provide a principled framework for simultaneous training of the top-down and bottom-up segmentation algorithms.\nIn this paper we provide such a framework. The algorithm we propose is similar at run-time to the OBJ-CUT and Borenstein et al. algorithms. As illustrated in figure 3, at run-time a novel image is scanned with an object detector which tries all possible subimages until it finds a subimage that is likely to contain the object (for most of the databases in this paper the approximate location was known so no scanning was performed). Within that subimage we search for object parts by performing normalized correlation with a set of fragments (each fragment scans only a portion of the subimage where it is likely to occur thus modeling the spatial interaction between fragment locations). The location of a fragment gives rise to a local bias term for an energy function. In addition to the local bias, the energy function rewards segmentation boundaries occurring at image discontinuities. The final segmentation is obtained by finding the global minimum of the energy function.\nWhile our algorithm is similar at run-time to existing segmentation algorithms, the training method is unique in that it simultaneously takes into account low-level and high-level cues. We show that this problem can be formulated in the context of Conditional Random Fields [8,7] which leads to a convex cost function for simultaneous training of both the low-level and the high-level segmenter. We use the CRFs formulation to derive a novel fragment selection algorithm, which allows us to efficiently learn models with a small number of fragments. Whereas pure top-down algorithms often require hundreds of fragments, our simultaneous learning procedure yields algorithms with a handful of fragments that are combined with low-level cues to efficiently compute high quality segmentations. ", "publication_ref": ["b1", "b11", "b15", "b2", "b10", "b21", "b18", "b2", "b1", "b20", "b16", "b5", "b1", "b16", "b5", "b16", "b5", "b2", "b5", "b7", "b6"], "figure_ref": ["fig_0", "fig_0", "fig_1", "fig_2"], "table_ref": []}, {"heading": "Segmentation using Conditional Random Fields", "text": "Given an image I, we define the energy of a binary segmentation map x as:\nE(x; I) = \u03bd i,j w ij |x(i) \u2212 x(j)| + k \u03bb k |x \u2212 x F k ,I | (1)\nThis energy is a combination of a pairwise low-level term and a local class-dependent term.\nThe low level term is defined via a set of affinity weights w(i, j). w(i, j) are high when the pixels (i, j) are similar and decrease to zero when they are different. Similarity can be defined using various cues including intensity, color, texture and motion as used for bottom up image segmentation [12]. Thus minimizing i,j w ij |x(i) \u2212 x(j)| means that labeling discontinuities are cheaper when they are aligned with the image discontinuities. In this paper we used 8-neighbors connectivity, and we set:\nw ij = 1 1 + \u03c3d 2 ij\nwhere d ij is the RGB difference between pixels i and j and \u03c3 = 5 \u2022 10 4 .\nThe second part of eq 1 encodes the local bias, defined as a sum of local energy terms each weighted by a weight \u03bb k . Following the terminology of Conditional Random Fields, we call each such local energy term a feature. In this work, these local energy terms are derived from image fragments with thresholds. To calculate the energy of a segmentation, we shift the fragment over a small window (10 pixels in each direction) around its location in its original image. We select the location in which the normalized correlation between the fragment and the new image is maximal (see Our goal in this paper is to learn a set of fragments {F k }, thresholds and weights {\u03bb k }, \u03bd that will favor the true segmentation. In the training stage the algorithm is provided a set of images {I t } t=1:T and their binary segmentation masks {x t } t=1:T , as in figure 2(b). The algorithm needs to select features and weights such that minimizing the energy with the learned parameters will provide the desired segmentation.", "publication_ref": ["b11"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Conditional Random Fields", "text": "Using the energy (eq. 1) we define the likelihood of the labels x conditioned on the image I as\nP (x|I) = 1 Z(I)\ne \u2212E(x;I) where:\nZ(I) = x e \u2212E(x;I)\nThat is, x forms a Conditional Random Field (CRF) [8]. The goal of the learning process is to select a set of fragments {F k }, thresholds and weights {\u03bb k }, \u03bd that will maximize the sum of the log-likelihood over training examples: ( \u03bb, \u03bd; F ) = t t ( \u03bb, \u03bd; F )\nt ( \u03bb, \u03bd; F ) = log P (x t |I t ; \u03bb, \u03bd, F ) = \u2212E(x t ; I t , \u03bb, \u03bd, F ) \u2212 log Z(I t ; \u03bb, \u03bd, F ) (2)\nThe idea of the CRF log likelihood is to select parameters that will maximize the likelihood of the ground truth segmentation for training examples. Such parameters should minimize the energy of the true segmentations x t , while maximizing the energy of all other configurations. The CRF formulation has proven useful in many vision applications [7,15,14,4,5]. Below we review several properties of the CRF log likelihood:\n1. For a given features set F = [F 1 , ..., F K ], if there exists a parameter set \u03bb * = [\u03bb * 1 , .., \u03bb * K ], \u03bd * for which the minimum of the energy function is exactly the true segmentation: x t = arg min x E(x; I t , \u03bb * , \u03bd * , F ). Then selecting \u03b1 \u03bb * , \u03b1\u03bd * with \u03b1 \u2192 \u221e will maximize the CRF likelihood, since: P (x t |I t ; \u03b1 \u03bb * , \u03b1\u03bd * , F ) = 1 (see [10]). 2. The CRF log likelihood is convex with respect to the weighting parameters \u03bb k , \u03bd as discussed in [8]. 3. The derivative of the log-likelihood with respect to the coefficient of a given feature is known to be the difference between the expected feature response, and the observed one. This can be expressed in a simple closed form way as:\n\u2202 t ( \u03bb, \u03bd; F ) \u2202\u03bb k = \u2202 log P (x t |I t ; \u03bb, \u03bd, F ) \u2202\u03bb k = i\u2208F k r p i (r)|r \u2212 x F k ,It (i)| \u2212 i\u2208F k |x t (i) \u2212 x F k ,It (i)| = < |x t \u2212 x F k ,It | > P (xt|It; \u03bb,\u03bd, F ) \u2212 < |x t \u2212 x F k ,It | > Obs (3) \u2202 t ( \u03bb, \u03bd; F ) \u2202\u03bd = \u2202 log P (x t |I t ; \u03bb, \u03bd, F ) \u2202\u03bd = ij rs p ij (r, s)w ij |r \u2212 s| \u2212 ij w ij |x t (i) \u2212 x t (j)| = < |x t (i) \u2212 x t (j)| > P (xt|It; \u03bb,\u03bd, F ) \u2212 < |x t (i) \u2212 x t (j)| > Obs (4)\nWhere p i (r), p ij (r, s) are the marginal probabilities P (x i = r|I t ; \u03bb, \u03bd, F ),P (x i = r, x j = s|I t ; \u03bb, \u03bd, F ).\nSuppose we are given a set of features F = [F 1 , ...F K ] and the algorithm task is to select weights \u03bb = [\u03bb 1 , .., \u03bb K ], \u03bd that will maximize the CRF log likelihood. Given that the cost is convex with respect to \u03bb, \u03bd it is possible to randomly initialize the weights vector and run gradient decent, when the gradients are computed using equations 3,4. Note that gradient decent can be used for selecting the optimal weights, without computing the explicit CRF log likelihood (eq 2).\nExact computation of the derivatives is intractable, due to the difficulty in computing the marginal probabilities p i (r), p ij (r, s). However, any approximate method for estimating marginal probabilities can be used. One approach for approximating the marginal probabilities is using Monte Carlo sampling, like in [4,1]. An alternative approach is to approximate the marginal probabilities using the beliefs output of sum product belief propagation or generalized belief propagation. Similarly, an exact computation of the CRF log likelihood (eq 2) is challenging due to the need to compute the log-partition function Z(I) = x e \u2212E(x;I) . Exact computation of Z(I) is in general intractable (except for tree structured graphs). However, approximate inference methods can be used here as well, such as the Bethe free energy or the Kikuchi approximations [20]. Monte-Carlo methods can also be used. In this work we have approximated the marginal probabilities and the partition function using sum product tree-reweighted belief propagation [18], which provides a rigorous bound on the partition function, and has better convergence properties than standard belief propagation. Tree reweighted belief propagation is described in the Appendix.", "publication_ref": ["b7", "b6", "b14", "b13", "b3", "b4", "b9", "b7", "b3", "b0", "b19", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "Features Selection", "text": "The learning algorithm starts with a large pool of candidate local features. In this work we created a 2, 000 features pool, by extracting image fragments from training images. Fragments are extracted at random sizes and random locations. The learning goal is to select from the features pool a small subset of features that will construct the energy function E, in a way that will maximize the conditional log likelihood t log P (x t |I t ). Since the goal is to select a small subset of features out of a big pool, the required learning algorithm for this application is more than a simple gradient decent.\nLet E k denote the energy function at the k'th iteration. The algorithm initializes E 0 with the pairwise term and adds local features in an iterative greedy way, such that in each iteration a single feature is added:\nE k (x; I) = E k\u22121 (x; I) + \u03bb k |x \u2212 x F k ,I |.\nIn each iteration we would like to add the feature F k that will maximize the conditional log likelihood. We denote by L k (F, \u03bb) the possible likelihood if the feature F , weighted by \u03bb, is added at the k'th iteration:\nL k (F, \u03bb) = ( \u03bb k\u22121 , \u03bb, \u03bd; F k\u22121 , F ) = t log P (x t |I t ; E k\u22121 (x t ; I t ) + \u03bb|x \u2212 x F,I | )\nStraightforward computation of the likelihood improvement is not practical since in each iteration, it will require inference for each candidate feature and for every possible weight \u03bb we may assign to this feature. For example, suppose we have 50 training images, we want to scan 2, 000 features, 2 possible \u03bb values, and we want to perform 10 features selection iterations. This results in 2, 000, 000 inference operations. Given that each inference operation itself is not a cheap process, the resulting computation can not be performed in a reasonable time. However, we suggest that by using a firstorder approximation to the log likelihood, one can efficiently learn a small number of effective features. Similar ideas in other contexts have been proposed by [23,9,13].\nObservation: A first order approximation to the conditional log likelihood can be computed efficiently, without a specific inference process per feature.\nProof:\nL k (F, \u03bb) \u2248 k\u22121 ( \u03bb k\u22121 , \u03bd) + \u03bb \u2202L k (F, \u03bb) \u2202\u03bb \u03bb=0 (5\n)\nwhere\n\u2202L k (F, \u03bb) \u2202\u03bb \u03bb=0 =< |x t \u2212 x F,It | > P (xt|It; \u03bb k\u22121 ,\u03bd, F k\u22121 ) \u2212 < |x t \u2212 x F,It | > Obs (6) and k\u22121 ( \u03bb k\u22121 , \u03bd) = t log P (x t |I t ; E k\u22121 )\n. We note that computing the above first order approximation requires a single inference process on the previous iteration energy E k\u22121 , from which the local beliefs (approximated marginal probabilities) {b k\u22121 t,i } are computed. Since the gradient is evaluated at the point \u03bb = 0, it can be computed using the k \u2212 1 iteration beliefs and there is no need for a specific inference process per feature.\nComputing the first order approximation for each of the training images is linear in the filter size. This enables scanning thousands of candidate features within several minutes. As evident from the gradient formula (eq 6) and demonstrated in the experiments section, the algorithm tends to select fragments that: (1) have low error in the training set (since it attempts to minimize < |x t \u2212 x F,It | > Obs ) and (2) are not already accounted for by the existing model (since it attempts to maximize\n< |x t \u2212 x F,It | > P (xt|It; \u03bb k\u22121 ,\u03bd, F k\u22121 ) ).\nOnce the first order approximations have been calculated we can select a small set of the features F k1 ...F kN with the largest approximated likelihood gains. For each of the selected features, and for each of a small discrete set of possible \u03bb values \u03bb \u2208 {\u03bb 1 , ..., \u03bb M }, we run an inference process and evaluate the explicit conditional log likelihood. The optimal feature (and scale) is selected and added to the energy function E. The features selection steps are summarized in Algorithm 1.\nOnce a number of features have been selected, we can also optimize the choice of weights {\u03bb k }, \u03bd using several gradient decent steps. Since the cost is convex with respect to the weights a local optimum is not an issue. \n(F kn , \u03bb m ) = arg max n=1:N, m=1:M L k (F kn , \u03bb m ) Set \u03bb k = \u03bb m , F k = F kn , E k (x; I) = E k\u22121 (x; I) + \u03bb k |x \u2212 xF k ,I |.", "publication_ref": ["b22", "b8", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "In our first experiment we tried to segment a synthetic octopus dataset. Few sample images are shown in Fig 4 . It's clear that our synthetic octopi are highly non rigid objects. Any effort to fully cover all the octopi tentacles with fragments (like [2,11,6]), will require a huge number of different fragments. On the other hand, there is a lot of edges information in the images that can guide the segmentation. The first feature selected by our algorithm is located on the octopi head, which is a rigid part common to all examples. This single feature, combined with pairwise constraints was enough to propagate the true segmentation to the entire image. The MAP segmentation given the selected feature is shown in Fig 4.\nWe then tested our algorithm on two real datasets, of horses [3,2] and cows [11]. We measured the percentage of mislabeled pixels in the segmented images on training and testing images, as more fragments are learned. Those are shown for horses in Fig 5(a), and for cows in Fig 5 (b). Note that after selecting 3 fragments our algorithm performs at over 95% correct on test data for the horse dataset. The algorithm of Borenstein et al. [2] performed at 95% for pixels in which its confidence was over 0.1 and at 66% for the rest of the pixels. Thus our overall performance seems comparable (if not better) even though we used far less fragments. The OBJ-CUT algorithm also performs at around 96% for a subset of this dataset using a LPS model of 10 parts whose likelihood function takes into consideration chamfer distance and texture and is therefore significantly more complex than normalized correlation.\nIn the horses and cows experiments we rely on the fact that we are searching for a shape in the center of the window, and used an additional local feature predicting that the pixels lying on the boundary of the subimage should be labeled as background.\nIn Fig 6 we present several testing images of horses, the ground truth segmentation, the local features responses and the inferred segmentation. While low level information adds a lot of power to the segmentation process, it can also be misleading. For example, the example on the right of Fig 9 demonstrates the weakness of the low level information.\nIn Fig 7 we present segmentation results on cows test images, for an energy function consisting of 4 features. The segmentation in this case is not as good as in the horses' case, especially in the legs. We note that in most of these examples the legs are in a different color than the cow body, hence the low-level information can not easily propagate labeling from the cow body to its legs. The low level cue we use in the work is quite simple-based only on the RGB difference between neighboring pixels. It's possible that using more sophisticated edges detectors [12] will enable a better propagation.\nThe first 3 horse fragments that were selected by the algorithm are shown in Fig 8 . In Fig 9 we illustrate the first 3 training iterations on several training images. Quite a good segmentation can be obtained even when the response of the selected features does not cover the entire image. For example the first fragment was located around the horse's front legs. As can be seen in the first 3 columns of Fig 9, some images can be segmented quite well based on this single local feature. We can also see that the algorithm tends to select new features in image areas that were mislabeled in the previous iterations. For example, in several horses (mainly the 3 middle columns) there is still a problem in the upper part, and the algorithm therefore selects a second feature in the upper part of the horse. Once the second fragment was added there are still several mislabeled head areas (see the 3 right columns), and as a result the 3rd fragment is located on the horse head. ", "publication_ref": ["b1", "b10", "b5", "b2", "b1", "b10", "b1", "b11"], "figure_ref": ["fig_4", "fig_5", "fig_6", "fig_9", "fig_7", "fig_9", "fig_9"], "table_ref": []}, {"heading": "Discussion", "text": "Evidence from human vision suggests that humans utilize significant top-down information when performing segmentation. Recent works in computer vision also suggest Cows data Note that after 4 fragments our algorithm performs at over 95% correct on test data for the horse dataset. These results are comparable if not better than [2,6] while using a simpler model.   that segmentation performance in difficult scenes is best approached by combining topdown and bottom-up cues. In this paper we presented an algorithm that learns how to combine these two disparate sources of information into a single energy function. We showed how to formulate the problem as that of estimation in Conditional Random Fields which will provably find the correct parameter settings if they exist. We used the CRF formulation to derive a novel fragment selection algorithm that allowed us to efficiently search over thousands of image fragments for a small number of fragments that will improve the segmentation performance Our learned algorithm achieves state-ofthe-art performance with a small number of fragments combined with very rudimentary low-level cues.\nBoth the top-down module and the bottom-up module that we used can be significantly improved. Our top-down module translates an image fragment and searches for the best normalized correlation, while other algorithms also allow rescaling and rotation of the parts and use more sophisticated image similarity metrics. Our bottom-up module uses only local intensity as an affinity function between pixels, whereas other algorithms have successfully used texture and contour as well. In fact, one advantage of the CRFs framework is that we can learn the relative weights of different affinity functions. We believe that by improving both the low-level and high-level cues we will obtain even better performance on the challenging task of image segmentation.", "publication_ref": ["b1", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Appendix: Tree-reweighted Belief Propagation and Tree-reweighted Upper Bound", "text": "In this section we summarize the basic formulas from [18] for applying tree-rewighted belief propagation and for computing the tree-rewighted upper bound. For a given graph G, we let \u00b5 e = {\u00b5 e |e \u2208 E(G)} represent a vector of edge appearance probabilities. That is, \u00b5 e is the probability that the edge e appears in a spanning tree of the graph G, chosen under a particular distribution on spanning trees. For 2D-grid graphs with 4-neighbors connectivity a reasonable choice of edges distributions is \u00b5 e = \u00b5 e = 1 2 |e \u2208 E(G) and for 8-neighbors connectivity, \u00b5 e = \u00b5 e = 1 4 |e \u2208 E(G) . The edge appearance probabilities are used for defining a tree-rewighted massages passing scheme. Denote the graph potentials as: xi,xj) , and assume P (x) can be factorized as: \n\u03a8 i (x i ) = e \u2212Ei(xi) , \u03a8 ij (x i , x j ) = e \u2212Eij (\nP (x) \u221d i \u03a8 i (x i ) i,j \u03a8 ij (x i , x j ).\nm n+1 ji (x i ) = \u03ba x j exp(\u2212 1 \u00b5 ij E ij (x i , x j )\u2212E j (x j )) \uf8f1 \uf8f2 \uf8f3 k\u2208\u0393 (j)\\i m n kj (x j ) \u00b5 kj m n ij (x j ) (1\u2212\u00b5ji) \uf8fc \uf8fd \uf8fe\nwhere \u03ba is a normalization factor such that xi m n ji (x i ) = 1.\nThe process converges when m n+1 ji = m n ji for every ij. Once the process has converged, the messages can be used for computing the local and pairwise beliefs:\nb i (x i ) = \u03ba exp(\u2212E i (x i )) k\u2208\u0393 (i) [m ki (x i )] \u00b5 ki (7) b ij (x i , x j ) = \u03ba exp(\u2212 1 \u00b5 ij E ij (x i , x j ) \u2212 E i (x i ) \u2212 E j (x j )) k\u2208\u0393 (i)\\j [m ki (x i )] \u00b5 ki [m ji (x i )] (1\u2212\u00b5ij ) k\u2208\u0393 (j)\\i [m kj (x j )] \u00b5 kj [m ij (x j )] (1\u2212\u00b5ji)(8)\nWe define a pseudo-marginals vector q = {q i , q ij } as a vector satisfying:\nxi q i (x i ) = 1 and xj q ij (x i , x j ) = q i (x i ). In particular, the beliefs vectors in equations 7,8 are a peseudo-marginals vector. We use the peseudo-marginals vectors for computing the tree-rewighted upper bound.\nDenote by \u03b8 the energy vector \u03b8 = {E i , E ij }. We define an \"average energy\" term as:\nq \u2022 \u03b8 = i xi \u2212q i (x i )E i (x i ) + ij xi,xj \u2212q ij (x i , x j )E ij (x i , x j ).\nWe define the single node entropy: H i (q i ) = \u2212 xi q i (x i ) log q i (x i ). Similarly, we define the mutual information between i and j, measured under q ij as:\nI ij (q ij ) = xi,xj q ij (x i , x j ) log qij (xi,xj) \" P x j qij (xi,x j ) \u00ab \" P x i qij (x i ,xj)\n\" . This is used to define a free energy: F ( q; \u00b5 e ; \u03b8) \u2212 i H i (q i ) + ij \u00b5 ij I ij (q ij ) \u2212 q \u2022 \u03b8.\nIn [18] Wainwright et al prove that F ( q; \u00b5 e ; \u03b8) provides an upper bound for the log partition function:\nlog Z = x exp(\u2212 i E i (x i ) \u2212 ij E ij (x i , x j )) \u2264 F( q; \u00b5 e ; \u03b8)\nThey also show that the free energy F ( q; \u00b5 e ; \u03b8) is minimized using the peseudomarginals vector b defined using the tree-rewighted messages passing output. Therefore the tighter upper bound on log Z is provided by b.\nThis result follows the line of approximations to the log partition function using free energy functions. As stated in [20], when standard belief propagation converges, the output beliefs vector is a stationary point of the bethe free energy function, and when generalized belief propagation converges, the output beliefs vector is a stationary point of the Kikuchi free energy function. However, unlike the bethe free energy and Kikuchi approximations, the tree-rewighted free energy is convex with respect to the peseudo-marginals vector, and hence tree-rewighted belief propagation can not end in a local minima.\nA second useful property of using the tree-rewighted upper bound as an approximation for the log partition function, is that computing the likelihood derivatives (equations 3-4) using the beliefs output of tree-rewighted massages passing, will result in exact derivatives for the upper bound approximation.\nIn this paper we used F ( b; \u00b5 e ; \u03b8) as an approximation for the log partition function, where b is the output of tree-rewighted belief propagation. We also used the tree-rewighted beliefs b in the derivatives computation (equations 3-4), as our approximation for the marginal probabilities.", "publication_ref": ["b17", "b17", "b19"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Graph partition by swendsen-wang cut", "journal": "", "year": "2003", "authors": "A Barbu; S C Zhu"}, {"ref_id": "b1", "title": "Combining top-down and bottom-up segmentation", "journal": "", "year": "2004-06", "authors": "E Borenstein; E Sharon; S Ullman"}, {"ref_id": "b2", "title": "Class-specific, top-down segmentation", "journal": "", "year": "2002-05", "authors": "E Borenstein; S Ullman"}, {"ref_id": "b3", "title": "Multiscale conditional random fields for image labeling", "journal": "", "year": "2004", "authors": "X He; R Zemel; M Carreira-Perpi"}, {"ref_id": "b4", "title": "Learning and incorporating top-down cues in image segmentation", "journal": "", "year": "2006", "authors": "Xuming He; Richard S Zemel; Debajyoti Ray"}, {"ref_id": "b5", "title": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "journal": "", "year": "2004", "authors": "M Kumar; P H S Torr; A Zisserman;  Objcut"}, {"ref_id": "b6", "title": "Discriminative random fields: A discriminative framework for contextual interaction in classification", "journal": "", "year": "2003", "authors": "S Kumar; M Hebert"}, {"ref_id": "b7", "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "journal": "Morgan Kaufmann", "year": "2001", "authors": "John Lafferty; Andrew Mccallum; Fernando Pereira"}, {"ref_id": "b8", "title": "Kernel conditional random fields: Representation and clique selection", "journal": "", "year": "2004", "authors": "John Lafferty; Xiaojin Zhu; Yan Liu"}, {"ref_id": "b9", "title": "Loss functions for discriminative training of energy-based models", "journal": "", "year": "2005", "authors": "Yann Lecun;  Fu Jie;  Huang"}, {"ref_id": "b10", "title": "Combined object categorization and segmentation with an implicit shape model", "journal": "", "year": "2004-05", "authors": "B Leibe; A Leonardis; B Schiele"}, {"ref_id": "b11", "title": "Perceptual Organization for artificial vision systems", "journal": "Kluwer Academic", "year": "2000", "authors": "J Malik; S Belongie; T Leung; J Shi"}, {"ref_id": "b12", "title": "Efficiently inducing features of conditional random fields", "journal": "", "year": "2003", "authors": "Andrew Mccallum"}, {"ref_id": "b13", "title": "Conditional random fields for object recognition", "journal": "", "year": "2004", "authors": "A Quattoni; M Collins; T Darrell"}, {"ref_id": "b14", "title": "Cue integration in figure/ground labeling", "journal": "", "year": "2005", "authors": "X Ren; C Fowlkes; J Malik"}, {"ref_id": "b15", "title": "Segmentation and boundary detection using multiscale intensity measurements", "journal": "", "year": "2001", "authors": "E Sharon; A Brandt; R Basri"}, {"ref_id": "b16", "title": "Image parsing: segmentation, detection, and recognition", "journal": "", "year": "2003", "authors": "Z W Tu; X R Chen; A Yuille; S C Zhu"}, {"ref_id": "b17", "title": "Tree-reweighted belief propagation and approximate ml estimation by pseudo-moment matching", "journal": "", "year": "2003", "authors": "M J Wainwright; T Jaakkola; A S Willsky"}, {"ref_id": "b18", "title": "Locus: Learning object classes with unsupervised segmentation", "journal": "", "year": "2005", "authors": "J Winn; N Jojic"}, {"ref_id": "b19", "title": "Constructing free-energy approximations and generalized belief propagation algorithms", "journal": "IEEE Transactions on Information Theory", "year": "2005", "authors": "J S Yedidia; W T Freeman; Y Weiss"}, {"ref_id": "b20", "title": "Object-specific figure-ground segregation", "journal": "", "year": "2003", "authors": "S X Yu; J Shi"}, {"ref_id": "b21", "title": "Deformable templates", "journal": "MIT press", "year": "2002", "authors": "A Yuille; P Hallinan"}, {"ref_id": "b22", "title": "Minimax entropy principle and its application to texture modeling", "journal": "Neural Computation", "year": "1997", "authors": "Zing Nian Song Chun Zhu; David Wu;  Mumford"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 1 .1Fig. 1. The relative merits of the bottom-up and the top-down approaches, replotted from [2]. (a) Input image. (b) The bottom-up hierarchical segmentation at three different scales. (c) The top-down approach provides a meaningful approximation for the figureground segmentation of the image, but may not follow exactly image discontinuities.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 2 .2Fig. 2. (a) Octopi: Combining low-level information can significantly reduce the required complexity of a deformable model. (b) Examples from horses training data. Each training image is provided with its segmentation mask.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 .3Fig. 3. System overview: (a) Detection algorithm applied to an input image (b) Fragments search range, dots indicate location of maximal normalized correlation (c) Fragments local evidence, overlaid with ground truth contour (d) Resulting segmentation contour", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig 3(b)). The feature is added to the energy, if this normalized correlation is large than a threshold. Each fragment is associated with a mask fragment x F extracted from the training set (Fig 8 shows some fragments examples). We denote by x F,I the fragment mask x F placed over the image I, according to the maximal normalized correlation location. For each fragment we add a term to the energy function which penalizes for the number of pixels for which x is different from the fragment mask x F,I , |x\u2212x F,I | = i\u2208F |x(i)\u2212 x F,I (i)|. Where i \u2208 F means the pixel i is covered by the fragment F after the fragment was moved to the maximal normalized correlation location (see Fig 3(c)).", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 4 .4Fig. 4. Results on synthetic octopus data. Top: Input images. Middle: response of the local feature, with the ground truth segmentation contour overlaid in red. Bottom: MAP segmentation contour overlaid on input image.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 5 .5Fig.5. Percents of miss-classified pixels: (a) Horses data (b) Cows data Note that after 4 fragments our algorithm performs at over 95% correct on test data for the horse dataset. These results are comparable if not better than[2,6] while using a simpler model.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 6 .6Fig. 6. Testing results on horses data. Top row: Input images. Second row: Response of the local features and the boundary feature, with the ground truth segmentation contour overlaid in red. Bottom row: MAP segmentation contour overlaid on input image.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 7 .7Fig. 7. Testing results on cows' data with 4 features. Top row: Input images. Second row: Response of the local features and the boundary feature, with the ground truth segmentation contour overlaid in red. Bottom row: MAP segmentation contour overlaid on input image.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Fig. 8 .8Fig. 8. The first 3 horse fragments selected by the learning algorithm", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Fig. 9 .9Fig. 9. Training results on horses data. For each group: Top row -response of the local features and the boundary feature, with the ground truth segmentation contour overlaid in red. Middle row -MAP segmentation. Bottom row -MAP segmentation contour overlaid on input image.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Run tree-reweighted belief propagation using the k \u2212 1 iteration energy E k\u22121 (xt; It). Compute local beliefs {b k\u22121 t,i }. 2. For each feature F compute the approximated likelihood using eq 5.Select the N features F k 1 ...F k N with largest approximated likelihood gains. 3. For each of the features F k 1 ...F k N , and for each scale \u03bb \u2208 {\u03bb 1 , ..., \u03bb M }, run treereweighted belief propagation and compute the likelihood L k (F kn , \u03bb m ) 4. Select the feature and scale with maximal likelihood gain:", "figure_data": "Algorithm 1 : Features Selection Initialization: E0(xt; It) = \u03bd P ij wij|xt(i) \u2212 xt(j)|.for k=1 to maxItr1."}], "formulas": [{"formula_id": "formula_0", "formula_text": "E(x; I) = \u03bd i,j w ij |x(i) \u2212 x(j)| + k \u03bb k |x \u2212 x F k ,I | (1)", "formula_coordinates": [4.0, 198.36, 290.76, 282.22, 21.21]}, {"formula_id": "formula_1", "formula_text": "w ij = 1 1 + \u03c3d 2 ij", "formula_coordinates": [4.0, 275.04, 425.52, 63.06, 25.53]}, {"formula_id": "formula_2", "formula_text": "P (x|I) = 1 Z(I)", "formula_coordinates": [5.0, 189.72, 233.52, 63.99, 23.52]}, {"formula_id": "formula_3", "formula_text": "Z(I) = x e \u2212E(x;I)", "formula_coordinates": [5.0, 344.99, 238.4, 80.28, 20.17]}, {"formula_id": "formula_4", "formula_text": "t ( \u03bb, \u03bd; F ) = log P (x t |I t ; \u03bb, \u03bd, F ) = \u2212E(x t ; I t , \u03bb, \u03bd, F ) \u2212 log Z(I t ; \u03bb, \u03bd, F ) (2)", "formula_coordinates": [5.0, 150.84, 314.64, 329.74, 11.97]}, {"formula_id": "formula_5", "formula_text": "\u2202 t ( \u03bb, \u03bd; F ) \u2202\u03bb k = \u2202 log P (x t |I t ; \u03bb, \u03bd, F ) \u2202\u03bb k = i\u2208F k r p i (r)|r \u2212 x F k ,It (i)| \u2212 i\u2208F k |x t (i) \u2212 x F k ,It (i)| = < |x t \u2212 x F k ,It | > P (xt|It; \u03bb,\u03bd, F ) \u2212 < |x t \u2212 x F k ,It | > Obs (3) \u2202 t ( \u03bb, \u03bd; F ) \u2202\u03bd = \u2202 log P (x t |I t ; \u03bb, \u03bd, F ) \u2202\u03bd = ij rs p ij (r, s)w ij |r \u2212 s| \u2212 ij w ij |x t (i) \u2212 x t (j)| = < |x t (i) \u2212 x t (j)| > P (xt|It; \u03bb,\u03bd, F ) \u2212 < |x t (i) \u2212 x t (j)| > Obs (4)", "formula_coordinates": [5.0, 164.52, 549.6, 316.06, 68.37]}, {"formula_id": "formula_6", "formula_text": "E k (x; I) = E k\u22121 (x; I) + \u03bb k |x \u2212 x F k ,I |.", "formula_coordinates": [6.0, 297.86, 593.64, 171.07, 11.24]}, {"formula_id": "formula_7", "formula_text": "L k (F, \u03bb) = ( \u03bb k\u22121 , \u03bb, \u03bd; F k\u22121 , F ) = t log P (x t |I t ; E k\u22121 (x t ; I t ) + \u03bb|x \u2212 x F,I | )", "formula_coordinates": [7.0, 136.56, 102.24, 342.16, 20.73]}, {"formula_id": "formula_8", "formula_text": "L k (F, \u03bb) \u2248 k\u22121 ( \u03bb k\u22121 , \u03bd) + \u03bb \u2202L k (F, \u03bb) \u2202\u03bb \u03bb=0 (5", "formula_coordinates": [7.0, 211.92, 271.08, 264.78, 25.89]}, {"formula_id": "formula_9", "formula_text": ")", "formula_coordinates": [7.0, 476.7, 278.47, 3.87, 8.97]}, {"formula_id": "formula_10", "formula_text": "\u2202L k (F, \u03bb) \u2202\u03bb \u03bb=0 =< |x t \u2212 x F,It | > P (xt|It; \u03bb k\u22121 ,\u03bd, F k\u22121 ) \u2212 < |x t \u2212 x F,It | > Obs (6) and k\u22121 ( \u03bb k\u22121 , \u03bd) = t log P (x t |I t ; E k\u22121 )", "formula_coordinates": [7.0, 134.76, 317.64, 345.82, 47.97]}, {"formula_id": "formula_11", "formula_text": "< |x t \u2212 x F,It | > P (xt|It; \u03bb k\u22121 ,\u03bd, F k\u22121 ) ).", "formula_coordinates": [7.0, 134.76, 496.8, 151.37, 13.28]}, {"formula_id": "formula_12", "formula_text": "(F kn , \u03bb m ) = arg max n=1:N, m=1:M L k (F kn , \u03bb m ) Set \u03bb k = \u03bb m , F k = F kn , E k (x; I) = E k\u22121 (x; I) + \u03bb k |x \u2212 xF k ,I |.", "formula_coordinates": [8.0, 151.68, 194.99, 254.6, 36.45]}, {"formula_id": "formula_13", "formula_text": "\u03a8 i (x i ) = e \u2212Ei(xi) , \u03a8 ij (x i , x j ) = e \u2212Eij (", "formula_coordinates": [11.0, 134.76, 568.28, 345.81, 24.01]}, {"formula_id": "formula_14", "formula_text": "P (x) \u221d i \u03a8 i (x i ) i,j \u03a8 ij (x i , x j ).", "formula_coordinates": [11.0, 144.12, 581.64, 335.81, 24.21]}, {"formula_id": "formula_15", "formula_text": "m n+1 ji (x i ) = \u03ba x j exp(\u2212 1 \u00b5 ij E ij (x i , x j )\u2212E j (x j )) \uf8f1 \uf8f2 \uf8f3 k\u2208\u0393 (j)\\i m n kj (x j ) \u00b5 kj m n ij (x j ) (1\u2212\u00b5ji) \uf8fc \uf8fd \uf8fe", "formula_coordinates": [12.0, 151.68, 548.64, 329.62, 43.76]}, {"formula_id": "formula_16", "formula_text": "b i (x i ) = \u03ba exp(\u2212E i (x i )) k\u2208\u0393 (i) [m ki (x i )] \u00b5 ki (7) b ij (x i , x j ) = \u03ba exp(\u2212 1 \u00b5 ij E ij (x i , x j ) \u2212 E i (x i ) \u2212 E j (x j )) k\u2208\u0393 (i)\\j [m ki (x i )] \u00b5 ki [m ji (x i )] (1\u2212\u00b5ij ) k\u2208\u0393 (j)\\i [m kj (x j )] \u00b5 kj [m ij (x j )] (1\u2212\u00b5ji)(8)", "formula_coordinates": [13.0, 181.08, 112.56, 299.5, 84.57]}, {"formula_id": "formula_17", "formula_text": "q \u2022 \u03b8 = i xi \u2212q i (x i )E i (x i ) + ij xi,xj \u2212q ij (x i , x j )E ij (x i , x j ).", "formula_coordinates": [13.0, 170.76, 266.4, 293.37, 12.21]}, {"formula_id": "formula_18", "formula_text": "I ij (q ij ) = xi,xj q ij (x i , x j ) log qij (xi,xj) \" P x j qij (xi,x j ) \u00ab \" P x i qij (x i ,xj)", "formula_coordinates": [13.0, 145.32, 291.72, 335.23, 35.96]}, {"formula_id": "formula_19", "formula_text": "log Z = x exp(\u2212 i E i (x i ) \u2212 ij E ij (x i , x j )) \u2264 F( q; \u00b5 e ; \u03b8)", "formula_coordinates": [13.0, 178.68, 380.28, 257.91, 20.98]}], "doi": ""}