{"title": "Variance-based regularization with convex objectives", "authors": "John C Duchi; Hongseok Namkoong", "pub_date": "2017-12-14", "abstract": "We develop an approach to risk minimization and stochastic optimization that provides a convex surrogate for variance, allowing near-optimal and computationally efficient trading between approximation and estimation error. Our approach builds off of techniques for distributionally robust optimization and Owen's empirical likelihood, and we provide a number of finite-sample and asymptotic results characterizing the theoretical performance of the estimator. In particular, we show that our procedure comes with certificates of optimality, achieving (in some scenarios) faster rates of convergence than empirical risk minimization by virtue of automatically balancing bias and variance. We give corroborating empirical evidence showing that in practice, the estimator indeed trades between variance and absolute performance on a training sample, improving out-of-sample (test) performance over standard empirical risk minimization for a number of classification problems.", "sections": [{"heading": "Introduction", "text": "We propose and study a new approach to risk minimization that automatically trades between bias-or approximation error-and variance-or estimation error. Let X be a sample space, P 0 a distribution on X , and \u0398 a parameter space. For a loss function \u2113 : \u0398 \u00d7 X \u2192 R, consider the problem of finding \u03b8 \u2208 \u0398 minimizing the risk R(\u03b8) := E[\u2113(\u03b8, X)] = \u2113(\u03b8, x)dP (x)\n(1)\ngiven a sample {X 1 , . . . , X n } drawn i.i.d. according to the distribution P . Under appropriate conditions on the loss \u2113, parameter space \u0398, and random variables X, a number of researchers [3,4,10,25] have shown results of the form that with high probability,\nR(\u03b8) \u2264 1 n n i=1 \u2113(\u03b8, X i ) + C 1 Var(\u2113(\u03b8, X)) n + C 2 n for all \u03b8 \u2208 \u0398 (2)\nwhere C 1 and C 2 depend on the parameters of problem (1) and the desired confidence guarantee. Such bounds justify empirical risk minimization (ERM), which chooses \u03b8 n to minimize 1 n n i=1 \u2113(\u03b8, X i ) over \u03b8 \u2208 \u0398. Further, these bounds showcase a tradeoff between bias and variance, where we identify the bias (or approximation error) with the empirical risk 1 n n i=1 \u2113(\u03b8, X i ), while the variance arises from the second term in the bound. Short (NIPS) version of the paper is available at https://goo.gl/o6Y3nF. Given bounds of the form above and heuristically considering the classical \"bias-variance\" tradeoff in estimation and statistical learning, it is natural to instead choose \u03b8 to directly minimize a quantity trading between approximation and estimation error, say of the form 1\nn n i=1 \u2113(\u03b8, X i ) + C Var Pn (\u2113(\u03b8, X)) n ,(3)\nwhere Var Pn denotes the empirical variance of its argument. Maurer and Pontil [33] considered precisely this idea, giving a number of guarantees on the convergence and good performance of such a procedure. Unfortunately, even when the loss \u2113 is convex in \u03b8, the formulation ( 3) is in general non-convex, yielding computationally intractable problems, which has limited the applicability of procedures that minimize the variance-corrected empirical risk (3). In this paper, we develop an approach based on Owen's empirical likelihood [38] and ideas from distributionally robust optimization [5,8,7] that-whenever the loss \u2113 is convex-provides a tractable convex formulation that very closely approximates the penalized risk (3), and we give a number of theoretical guarantees and empirical evidence for its performance. Before summarizing our contributions, we first describe our approach. Let \u03c6 : R + \u2192 R be a convex function with \u03c6(1) = 0. Then the \u03c6-divergence between distributions P and Q defined on a space X is\nD \u03c6 (P ||Q) = \u03c6 dP dQ dQ = X \u03c6 p(x) q(x) q(x)d\u00b5(x),\nwhere \u00b5 is any measure for which P, Q \u226a \u00b5, and p = dP d\u00b5 , q = dQ d\u00b5 . Throughout this paper, we use \u03c6(t) = 1 2 (t \u2212 1) 2 , which gives the \u03c7 2 -divergence [45]. Given \u03c6 and a sample X 1 , . . . , X n , we define the local neighborhood of the empirical distribution with radius \u03c1 by P n := distributions P such that D \u03c6 P || P n \u2264 \u03c1 n ,\nwhere P n denotes the empirical distribution of the sample, and our choice of \u03c6(t) = 1 2 (t \u2212 1) 2 means that P n consists of discrete distributions supported on the sample {X i } n i=1 . We then define the robustly regularized risk R n (\u03b8, P n ) := sup\nP \u2208Pn E P [\u2113(\u03b8, X)] = sup P E P [\u2113(\u03b8, X)] : D \u03c6 (P || P n ) \u2264 \u03c1 n . (4\n)\nAs it is the supremum of a family of convex functions, the robust risk \u03b8 \u2192 R n (\u03b8, P n ) is convex in \u03b8 whenever \u2113 is convex, no matter the value of \u03c1 \u2265 0. Given the robust empirical risk (4), our proposed estimation procedure is to choose a parameter \u03b8 rob n by minimizing R n (\u03b8, P n ). Let us now discuss a few of the properties of procedures minimizing the robust empirical risk (4). Our first main technical result, which we show in Section 2, is that for bounded loss functions, the robust risk R n (\u03b8, P n ) is a good approximation to the variance-regularized quantity (3). That is,\nR n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1Var Pn (\u2113(\u03b8, X)) n + \u03b5 n (\u03b8),(5)\nwhere \u03b5 n (\u03b8) \u2264 0 and is O P (1/n) uniformly in \u03b8. We show specifically that whenever \u2113(\u03b8, X) has suitably large variance, with high probability we have \u03b5 n = 0. From variance expansions of the form (5) and empirical Bernstein inequality (2), we see that R n (\u03b8, P n ) is a O(1/n)-approximation to the population risk R(\u03b8), in contrast to the cruder O(1/ \u221a n)-approximation that the empirical risk E Pn [\u2113(\u03b8; X)] provides. Based on this intuition that the robustly regularized risk R n (\u03b8; P n ) is a tighter approximation to the population risk R(\u03b8), we show a number of finite-sample convergence guarantees for the estimator \u03b8 rob n \u2208 argmin \u03b8\u2208\u0398 sup P E P [\u2113(\u03b8, X)] :\nD \u03c6 P || P n \u2264 \u03c1 n(6)\nthat are often tighter than those available for ERM (see Section 3). The above problem is a convex optimization problem when the original loss \u2113(\u2022; X) is convex and \u0398 is a convex set.\nBased on the expansion (5), solutions \u03b8 rob n of problem ( 6) enjoy automatic finite sample optimality certificates: for \u03c1 \u2265 0, with probability at least 1 \u2212 C 1 exp(\u2212\u03c1) we have R( \u03b8 rob n ) = E[\u2113( \u03b8 rob n ; X)] \u2264 R n ( \u03b8 rob n ; P n ) +\nC 2 \u03c1 n = inf \u03b8\u2208\u0398 R n (\u03b8, P n ) + C 2 \u03c1 n\nwhere C 1 , C 2 are constants (which we specify) that depend on the loss \u2113 and domain \u0398. That is, with high probability the robust solution has risk no worse than the optimal finite sample robust objective up to an O(\u03c1/n) error term. To guarantee a desired level of risk performance with probability 1 \u2212 \u03b4, we may specify the robustness penalty \u03c1 = O(log 1 \u03b4 ). Secondly, we show that the procedure (6) allows us to automatically and near-optimally trade between approximation and estimation error (bias and variance), so that\nR( \u03b8 rob n ) = E[\u2113( \u03b8 rob n ; X)] \u2264 inf \u03b8\u2208\u0398 E[\u2113(\u03b8; X)] + 2 2\u03c1 n Var(\u2113(\u03b8; X)) + C\u03c1 n(7)\nwith high probability. When there are parameters \u03b8 with small risk R(\u03b8) and small variance Var(\u2113(\u03b8, X)), this guarantees that the excess risk R( \u03b8 rob n )\u2212inf \u03b8\u2208\u0398 R(\u03b8) is essentially of order O(\u03c1/n), where \u03c1 governs our desired confidence level. Our bounds do not require the Bernstein-type condition Var(\u2113(\u03b8; X)) \u2264 M R(\u03b8) often required for ERM. Since it is often the case that M depends on global information (e.g. size of parameter space \u0398), we have Var(\u2113(\u03b8; X)) \u226a M R(\u03b8), in which case the bound (7) offers a tighter guarantee than that available for the ERM solution \u03b8 erm n . In particular, we give an explicit example in Section 3.3 where our robustly regularized procedure (6) converges at rate O(log n/n) compared to O(1/ \u221a n) of empirical risk minimization.\nBounds that trade between risk and variance are known in a number of cases in the empirical risk minimization literature [32,44,3,10,4,11,25], which is relevant when one wishes to achieve \"fast rates\" of convergence for statistical learning algorithms (that is, faster than the O(1/ \u221a n) guaranteed by a number of uniform convergence results [2,10,11]). In many cases, however, such tradeoffs require either conditions such as the Mammen and Tsybakov's noise condition [32,10] or localization results made possible by curvature conditions that relate the loss/risk and variance [4,3,35]. The robust solutions (6) enjoy a different tradeoff between variance and risk than that in this literature, but essentially without conditions except compactness of \u0398.\nIn proposing any new estimator, it is essential to understand the limits of the proposed procedure and identify situations in which its performance may be worse than existing estimators. There are indeed situations in which minimizing the robust-regularized risk (4) yields some inefficiency (for example, in classical statistical estimation problems with correctly specified model). To understand limits of the inefficiency induced by using the distributionally-robustified estimator (6), in Section 4 we study explicit finite sample properties of the robust estimator for general stochastic optimization problems, and we also provide asymptotic normality results in classical problems. There are a number of situations, based on growth conditions on the population risk R, when convergence rates faster than 1/ \u221a n (or even 1/n) are attainable (see Shapiro et al. [40,Chapter 5]). We show that under these conditions, the robust procedure (6) still enjoys (near-optimal) fast rates of convergence, similar to empirical risk minimization (also known as sample average approximation in the stochastic programming literature). Our study of asymptotics makes precise the asymptotic efficiency loss of the robust procedure over minimizing the standard (asymptotically optimal) empirical expectation: there is a bias term that scales as \u03c1/n in the limiting distribution of \u03b8 rob n , though its variance is optimal. We complement our theoretical results in Section 5, where we conclude by providing three experiments comparing empirical risk minimization strategies to robustly-regularized risk minimization (6). These results validate our theoretical predictions, showing that the robust solutions are a practical alternative to empirical risk minimization. In particular, we observe that the robust solutions outperform their ERM counterparts on \"harder\" instances with higher variance. In classification problems, for example, the robustly regularized estimators exhibit an interesting tradeoff, where they improve performance on rare classes (where ERM usually sacrifices performance to improve the common cases-increasing variance slightly) at minor cost in performance on common classes.", "publication_ref": ["b2", "b3", "b9", "b24", "b0", "b32", "b2", "b37", "b4", "b7", "b6", "b44", "b2", "b4", "b4", "b6", "b5", "b31", "b43", "b2", "b9", "b3", "b10", "b24", "b1", "b9", "b10", "b31", "b9", "b3", "b2", "b34", "b5", "b5", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "The theoretical foundations of empirical risk minimization are solid [48,2,10,11]. When the expectation of the excess loss bounds its variance, it is possible to achieve faster rates than the O(1/ \u221a n)\noffered by standard uniform convergence arguments [49,50,4,25,11] (see Boucheron et al. [10,Section 5] for an overview in the case of classification, and Shapiro et al. [40,Chapter 5.3] for more general stochastic optimization problems). Vapnik and Chervonenkis [49,50] first provided such results in the context of {0, 1}-valued losses for classification (see also [1]), where the expectation of the loss always upper bounds its variance, so that if there exists a perfect classifier the convergence rates of empirical risk minimization procedures are O(1/n). Mammen and Tsybakov [32,44] give low noise conditions for binary classification substantially generalizing these results, which yield a spectrum of fast rates. Under related conditions, Bartlett, Jordan, and McAuliffe [4] show similar fast rates of convergence for convex risk minimization under appropriate curvature conditions on the loss. The robust procedure (6), on the other hand, is guaranteed to provide an at most O(1/n) over-estimate of the population risk and a small increase of its variance regularized population counterpart. It may be the case that the variance-regularized risk inf \u03b8 {R(\u03b8) + Var(\u2113(\u03b8, X))/n} decreases to R(\u03b8 \u22c6 ) more slowly than 1/n. As we note above and detail in Section 4, however, in stochastic optimization problems the variance-regularized approach (6) suffers limited degradation with respect to empirical risk minimization strategies, even under convexity and curvature properties that allow faster rates of convergence than those achievable in classical regimes, as detailed by [40,Chapter 5.3]. Most related to our work is that of Maurer and Pontil [33], who propose directly regularizing empirical risk minimization by variance, providing guarantees similar to ours and giving a natural foundation off of which many of our results build. In their setting, however-as they carefully note-it is unclear how to actually solve the variance-regularized problem, as it is generally nonconvex. Shivaswamy and Jebara [41,42] build on this and develop an elegant approach for boosting binary classifiers based on a variance penalty applied to the exponential loss; as it is a boosting approach, their approach provides a coordinate-wise strategy for decreasing the loss, but it is not guaranteed to converge to a global minimizer and applies to classification-like problems. Our approach, handling general stochastic optimization problems, removes these obstructions.\nThe robust procedure ( 6) is based on distributionally robust optimization ideas that many researchers have developed [6,8,27], where the goal (as in robust optimization more broadly [5]) is to protect against all deviations from a nominal data model. In the optimization literature, there is substantial work on tractability of the problem (6), including that of Ben-Tal et al. [6], who show that the dual of (4) often admits a standard form (such as a second-order cone problem) to which standard polynomial-time interior point methods can be applied. Namkoong and Duchi [36] develop stochastic-gradient-like procedures for solving the problem (6), which efficiently provide low accuracy solutions (which are still sufficient for statistical tasks). Work on the statistical analysis of such procedures is nascent; Bertsimas, Gupta, and Kallus [8] and Lam and Zhou [27] provide confidence intervals for solution quality under various conditions, and Duchi et al. [20] give asymptotics showing that the optimal robust risk R n ( \u03b8 rob n ; P n ) is a calibrated upper confidence bound for inf \u03b8\u2208\u0398 E[\u2113(\u03b8; X)]. They and Gotoh et al. [22] also provide a number of asymptotic results showing relationships between the robust risk R n (\u03b8; P n ) and variance regularization, but they do not leverage these results for guarantees on the solutions \u03b8 rob n .\nNotation We collect our notation here. We let B denote a unit norm ball in R d , B = {\u03b8 \u2208 R d : \u03b8 \u2264 1}, where d and \u2022 are generally clear from context. Given sets A \u2282 R d and B \u2282 R d , we\nlet A + B = {a + b : a \u2208 A, b \u2208 B} denote Minkowski addition. For a convex function f , the subgradient set \u2202f (x) of f at x is \u2202f (x) = {g : f (y) \u2265 f (x) + g \u22a4 (y \u2212 x) for all y}. For a function h : R d \u2192 R, we let h * denote its Fenchel (convex) conjugate, h * (y) = sup x {y \u22a4 x \u2212 h(x)}.\nFor sequences a n , b n , we let a n b n denote that there is a numerical constant C < \u221e such that a n \u2264 Cb n for all n. For a sequence of random vectors X 1 , X 2 , . . ., we let X n d \u2192 X \u221e denote that X n converges in distribution to X \u221e . For a nonegative sequence a 1 , a 2 , . . ., we say X n = O P (a n ) if lim c\u2192\u221e sup n P( X n \u2265 ca n ) = 0, and we say X n = o P (a n ) if lim c\u21920 lim sup n P( X n \u2265 ca n ) = 0.", "publication_ref": ["b47", "b1", "b9", "b10", "b48", "b49", "b3", "b24", "b10", "b9", "b39", "b48", "b49", "b0", "b31", "b43", "b3", "b39", "b32", "b40", "b41", "b5", "b7", "b26", "b4", "b5", "b5", "b35", "b5", "b7", "b26", "b19", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "Variance Expansion", "text": "We begin our study of the robust regularized empirical risk R n (\u03b8, P n ) by showing that it is a good approximation to the empirical risk plus a variance term, that is, studying the variance expansion (5). Although the variance of the loss is in general non-convex (see Figure 1 for a simple example), the robust formulation ( 6) is a convex optimization problem for variance regularization whenever the loss function is convex (the supremum of convex functions is convex [24, Prop. 2.1.2.]).", "publication_ref": ["b4"], "figure_ref": ["fig_14"], "table_ref": []}, {"heading": "Variance expansion for a single variable", "text": "To gain intuition for the variance expansion that follows, we begin with a slightly simpler problem, which is to study the quadratically constrained linear maximization problem maximize\np n i=1 p i z i subject to p \u2208 P n = p \u2208 R n + : 1 2 np \u2212 1 2 2 \u2264 \u03c1, 1, p = 1 ,(8)\n\u22124 \u22123 \u22122 \u22121 0 1 2 3 4 0.8 1.0 1.2 1.4 1.6 1.8 Figure 1. Plot of \u03b8 \u2192 Var(\u2113(\u03b8, X)) for \u2113(\u03b8; X) = |\u03b8 \u2212 X| where X \u223c Uni({\u22122, \u22121, 0, 1, 2}).\nThe function is non-convex, with multiple local minima, inflection points, and does not grow as \u03b8 \u2192 \u00b1\u221e.\nwhere z \u2208 R n is a vector. For simplicity, let\ns 2 n = 1 n z 2 2 \u2212 (z) 2 = 1 n z \u2212 z 2\n2 denote the empirical \"variance\" of the vector z, where z = 1 n 1, z is the mean value of z. Then by introducing the variable\nu = p \u2212 1 n 1, the objective in problem (8) satisfies p, z = z + u, z = z + u, z \u2212 z because u, 1 = 0. Thus problem (8) is equivalent to solving maximize u\u2208R n z + u, z \u2212 z subject to u 2 2 \u2264 2\u03c1 n 2 , 1, u = 0, u \u2265 \u2212 1 n .\nNotably, by the Cauchy-Schwarz inequality, we have u, z \u2212 z \u2264 \u221a 2\u03c1 z \u2212 z 2 /n = 2\u03c1s 2 n /n, and equality is attained if and only if\nu i = \u221a 2\u03c1(z i \u2212 z) n z \u2212 z 2 = \u221a 2\u03c1(z i \u2212 z) n ns 2 n .\nIt is possible to choose such u i while satisfying the constraint u i \u2265 \u22121/n if and only if min\ni\u2208[n] \u221a 2\u03c1(z i \u2212 z) ns 2 n \u2265 \u22121.(9)\nThus, if inequality (9) holds for the vector z-that is, there is enough variance in z-we have\nsup p\u2208Pn p, z = z + 2\u03c1s 2 n n .\nFor losses \u2113(\u03b8, X) with enough variance relative to \u2113(\u03b8\n, X i ) \u2212 E Pn [\u2113(\u03b8, X i )]\n, that is, those satisfying inequality (9), then, we have\nR n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1Var Pn (\u2113(\u03b8, X)) n .\nA slight elaboration of this argument, coupled with the application of a few concentration inequalities, yields the next theorem. The theorem as stated applies only to bounded random variables, but in subsequent sections we relax this assumption by applying the characterization (9) of the exact expansion. As usual, we assume that \u03c6(t) = 1 2 (t \u2212 1) 2 in our definition of the \u03c6-divergence.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 1. Let Z be a random variable taking values in", "text": "[M 0 , M 1 ], and let M = M 1 \u2212 M 0 . Let \u03c3 2 = Var(Z) and s 2 n = E Pn [Z 2 ] \u2212 E Pn [Z] 2\ndenote the population and sample variance of Z, respectively. Fix \u03c1 \u2265 0. Then\n2\u03c1 n s 2 n \u2212 2M \u03c1 n + \u2264 sup P E P [Z] : D \u03c6 (P || P n ) \u2264 \u03c1 n \u2212 E Pn [Z] \u2264 2\u03c1 n s 2 n .(10)\nMoreover, for n \u2265 max 5,\nM 2 \u03c3 2 max {8\u03c3, 44} , with probability at least 1 \u2212 exp \u2212 n\u03c3 2 11M 2 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [Z] = E Pn [Z] + 2\u03c1 n s 2 n .(11)\nSee Section A for the proof of Theorem 1. Inequality ( 10) and the exact expansion (11) show that, at least for bounded loss functions \u2113, the robustly regularized risk ( 4) is a natural (and convex) surrogate for empirical risk plus standard deviation of the loss, and the robust formulation approximates exact variance regularization with a convex penalty. In the sequel, we leverage this result to provide sharp guarantees for a number of stochastic risk minimization problems.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Uniform variance expansions", "text": "We now turn to a more uniform variant Theorem 1, which depends on familiar notions of function complexity based on Rademacher averages. For a sample x 1 , . . . , x n and i.i.d. random signs \u03b5 i \u2208 {\u22121, 1}, independent of the x i , the empirical Rademacher complexity of the class F is\nR n (F) := E sup f \u2208F 1 n n i=1 \u03b5 i f (x i ) .\nThe worst-case Rademacher complexity [43] is\nR sup n (F) := sup x 1 ,...,xn\u2208X E sup f \u2208F 1 n n i=1 \u03b5 i f (x i ) .\nFor example, when F is a class of functions bounded by M with VC-subgraph dimension d, we With this definition, we provide a result showing that the variance expansion (5) holds uniformly for all functions with enough variance.\nhave the inequalities E[R n (F)] \u2264 R sup n (F) M d n .", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 2. Let F be a collection of bounded functions", "text": "f : X \u2192 [M 0 , M 1 ] where M = M 1 \u2212 M 0 , and M \u2264 n. There exists a universal constant C such that if \u03c4 2 > 0 satisfies \u03c4 2 \u2265 4\u03c1M 2 n + C R sup n (F) 2 log 3 n + M 2 n (t + log log n) .\nThen with probability at least 1 \u2212 3e \u2212t sup\nP :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] = E Pn [f (X)] + 2\u03c1 n Var Pn (f (X)) (12) for all f \u2208 F such that Var(f ) \u2265 \u03c4 2 .\nWe prove the theorem in Section B. Theorem 2 shows that the variance expansion of Theorem 1 holds uniformly for all functions f with sufficient variance. An asymptotic analogue of the equality (12) for heavier tailed random variables is also possible [20]. In the remainder of the section, we consider examples and applications to make the theorem somewhat clearer.", "publication_ref": ["b11", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Linear and margin-based losses", "text": "Consider a standard margin-based classification problem [2], where we have data pairs (x, y) \u2208 X \u00d7 {\u22121, 1}, and X \u2282 R d . Let \u0398 \u2282 R d be a norm ball of radius r(\u0398), \u0398 = {\u03b8 \u2208 R d | \u03b8 \u2264 r}, and let \u2022 * be the associated dual norm, assuming also that X \u2282 {x \u2208 R d | x * \u2264 r(X )}. We may then consider the standard loss minimization setting, where for some non-increasing and 1-Lipschitz loss \u2113 : R \u2192 R + , we have the risk\nR(\u03b8) := E [\u2113(Y \u03b8, X )] ,\nso that \u2113(y x, \u03b8 ) is the loss suffered by making prediction \u03b8, x when the label is y. By taking the function class F = {(x, y) \u2192 \u2113(y x, \u03b8 ) \u2212 \u2113(0) | \u03b8 \u2208 \u0398}, in this case, an application of the Ledoux-Talagrand contraction inequality [28] implies for any\ny 1 , x 1 , . . . , y n , x n that E sup \u03b8\u2208\u0398 n i=1 \u03b5 i [\u2113(y i \u03b8, x i ) \u2212 \u2113(0)] \u2264 E sup \u03b8\u2208\u0398 n i=1 \u03b5 i \u03b8, x i \u2264 r(\u0398)E n i=1 \u03b5 i x i * .(13)\nExample 1 (Euclidean norms): In the above context, suppose that norm \u2022 is the standard \u2113 2 Euclidean norm so that \u0398 is contained in an \u2113 2 -ball of radius r(\u0398), and X \u2282 R d in an \u2113 2 ball of radius r(X ). Then Jensen's inequality and independence of \u03b5 i 's give the bound\nE[ n i=1 \u03b5 i x i ] \u2264 E d j=1 n i=1 \u03b5 i x ij 2 \u2264 r(X ) \u221a n.\nThen, inequality (13) and Theorem 1 imply that sup\nP :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [\u2113(Y \u03b8, X )] = E Pn [\u2113(Y \u03b8, X )] + 2\u03c1 n Var Pn (\u2113(Y \u03b8, X )) for all \u03b8 satisfying Var(\u2113(Y \u03b8, X )) \u2265 r(X ) 2 r(\u0398) 2 n 4\u03c1 + C log 3 n + Ct ,\nwith probability at least 1 \u2212 e \u2212t . \u2738 Example 2 (High-dimensional problems): In high dimensional problems, the Euclidean scaling of Example 1 may be problematic, so that using \u2113 1 -constraints is preferred [16]. Thus, taking the norm \u2022 in the preceding to be the \u2113 1 norm, so that \u0398 \u2282 {\u03b8 \u2208 R\nd | \u03b8 1 \u2264 r 1 (\u0398)} and \u2022 * = \u2022 \u221e , then E[ n i=1 \u03b5 i x i \u221e ] \u2264 r(X ) n log(2d)\n, where r \u221e (X ) denotes the \u2113 \u221e -radius of X \u2282 R d . Thus, if we take the loss class F = {\u2113( \u03b8, \u2022 ) \u2212 \u2113(0) | \u03b8 \u2208 \u0398}, we obtain\nR sup n (F) sup x 1 ,...,xn\u2208X r 1 (\u0398) n E n i=1 \u03b5 i x i \u221e \u2264 r 1 (\u0398)r \u221e (X ) log(2d) n .\nThen the exact variance expansion (12) holds with probability at least 1 \u2212 e \u2212t uniformly over \u03b8 satisfying Var(\u2113(Y \u03b8, X ))\n\u2265 r 1 (\u0398) 2 r\u221e(X ) 2 n [4\u03c1 + C log d \u2022 log 3 n + Ct]. \u2738", "publication_ref": ["b1", "b27", "b12", "b15", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Covering number guarantees", "text": "It is also possible to provide guarantees on the exact variance expansion using standard covering numbers, though careful arguments based on Rademacher complexity can be tighter. We begin by recalling the appropriate notions from approximation theory. Let V be a vector space and V \u2282 V be any collection of vectors in V. Let \u2022 be a (semi)norm on V. We say a collection\nv 1 , . . . , v N \u2282 V is an \u01eb-cover of V if for each v \u2208 V, there exists v i such that v \u2212 v i \u2264 \u01eb.\nThe covering number of V with respect to \u2022 is then\nN (V, \u01eb, \u2022 ) := inf {N \u2208 N :\nthere is an \u01eb-cover of V with respect to \u2022 } . Now, let F be a collection of functions f : X \u2192 R, and define the L \u221e (X ) norm on f by\nf \u2212 g L \u221e (X ) := sup x\u2208X |f (x) \u2212 g(x)|.\nWe also relax our covering number requirements to empirical \u2113 \u221e -covering numbers as follows.\nDefine F(x) = {(f (x 1 ), . . . , f (x n )) : f \u2208 F} for x \u2208 X n , and define the empirical \u2113 \u221e -covering numbers\nN \u221e (F, \u01eb, n) = sup x\u2208X n N (F(x), \u01eb, \u2022 \u221e ) ,\nwhich bound the number of \u2113 \u221e -balls of radius \u01eb required to cover F(x). Note that we always have\nN \u221e (F, \u01eb, n) \u2264 N (F, \u01eb, \u2022 L \u221e (X )\n) by definition. The classical Dudley entropy integral [21,47] shows that, if P n denotes the point masses on x 1 , . . . , x n and\n\u2022 L 2 (Pn) the empirical L 2 -norm on functions f : X \u2192 [\u2212M, M ], then E 1 n sup f \u2208F n i=1 \u03b5 i f (x i ) inf \u03b4\u22650 \u03b4 + 1 \u221a n M \u03b4 log N (F, \u01eb, \u2022 L 2 (Pn) )d\u01eb \u2264 inf \u03b4\u22650 \u03b4 + 1 \u221a n M \u03b4 log N \u221e (F, \u01eb, n)d\u01eb .(14)\nOur main (essentially standard [47]) motivating example is that of Lipschitz loss functions for a parametric set \u0398, as follows. Example 3: Let \u0398 \u2282 R d and assume that \u2113 : \u0398 \u00d7 X \u2192 [0, M ] is L-Lipschitz in \u03b8 with respect to the \u2113 2 -norm for all x \u2208 X , meaning that |\u2113(\u03b8,\nx) \u2212 \u2113(\u03b8 \u2032 , x)| \u2264 L \u03b8 \u2212 \u03b8 \u2032 2 . Then taking F = {\u2113(\u03b8, \u2022) : \u03b8 \u2208 \u0398}, any \u01eb-covering {\u03b8 1 , . . . , \u03b8 N } of \u0398 in \u2113 2 -norm guarantees that min i |\u2113(\u03b8, x) \u2212 \u2113(\u03b8 i , x)| \u2264 L\u01eb for all \u03b8, x. That is, N (F, \u01eb, \u2022 L \u221e (X ) ) \u2264 N (\u0398, \u01eb/L, \u2022 2 ) \u2264 1 + diam(\u0398)L \u01eb d ,\nwhere diam(\u0398) = sup \u03b8,\u03b8 \u2032 \u2208\u0398 \u03b8 \u2212 \u03b8 \u2032 2 . Thus \u2113 2 -covering numbers of \u0398 control L \u221e -covering numbers of the family F, and we have by the entropy integral (14) that\nR sup n (F) d n diam(\u0398)L 0 log diam(\u0398)L \u01eb d\u01eb diam(\u0398)L d n .\nThat is, with high probability, for all \u03b8 such that Var(\u2113(\u03b8, X))\n\u2265 4M 2 \u03c1 n + Cd diam(\u0398) 2 L 2 log 3 n n\n, we have the exact variance expansion (12). \u2738", "publication_ref": ["b20", "b46", "b46", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "Optimization by Minimizing the Robust Loss", "text": "Based on the precise variance expansions in the preceding section, it is natural to expect that the robust solution (6) automatically trades between approximation and estimation error. This intuition is accurate, and we show that the robustly regularized objective R n (\u03b8; P n ) overestimates the population risk R(\u03b8) by at most O(1/n). By virtue of optimizing this tighter approximationas opposed to the usual O(1/ \u221a n)-approximation given by the empirical risk E Pn [\u2113(\u03b8; X)]-the robustly regularized solution (6) enjoys a number of favorable finite-sample properties, which are not always comparable to those for empirical risk minimization (ERM). In Section 3.1, we present two versions of our main result that depend on covering numbers and discuss their consequences, and we provide an example where the robustly regularized solution \u03b8 rob n achieves a tighter excess risk bound compared to those that a straightforward application of localized Rademacher complexities [3] show that the ERM solution \u03b8 erm n achieves. As evidenced by the substantial work on Rademacher-and Gaussian-complexity and symmetrization, in some instances covering-number-based arguments do not provide the sharpest scaling [2,3,43]; thus, in Section 3.2 we present a version of our main result that depends on localized Rademacher complexities, which can allow more refined uniform concentration bounds than covering numbers. We also provide a concrete (but admittedly somewhat contrived) example where our robustly regularized procedure (6) achieves R( \u03b8 rob n ) \u2212 inf \u03b8\u2208\u0398 R(\u03b8) log n n , while empirical risk minimization suffers R( \u03b8 erm n ) \u2212 inf \u03b8\u2208\u0398 R(\u03b8)", "publication_ref": ["b5", "b2", "b1", "b2", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "1", "text": "\u221a n , in Section 3.3. The robust \"regularizer\" has invariance properties other regularization procedures do not, and we mention these briefly in Section 3.4.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Covering arguments", "text": "Our first guarantee depends on the covering numbers of the function class F as we describe in Section 2.2.2. While we state our results abstractly, in the loss minimization setting we typically consider the function class F := {\u2113(\u03b8, \u2022) : \u03b8 \u2208 \u0398} parameterized by \u03b8. We have the following theorem, where as usual, we let F be a collection of functions f :\nX \u2192 [M 0 , M 1 ] with M = M 1 \u2212 M 0 .\nTheorem 3. Let n \u2265 8M 2 /t, t \u2265 log 12, \u01eb > 0, and \u03c1 \u2265 9t. Then with probability at least 1 \u2212 2(3N \u221e (F, \u01eb, 2n) + 1)e \u2212t , E[f (X)] \u2264 sup\nP :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 11 3 M \u03c1 n + 2 + 4 2t n \u01eb (15)\nfor all f \u2208 F. Defining the empirical minimizer\nf \u2208 argmin f \u2208F sup P E P [f (X)] : D \u03c6 (P || P n ) \u2264 \u03c1 n\nwe have with the same probability that\nE[ f (X)] \u2264 inf f \u2208F E[f ] + 2 2\u03c1 n Var(f ) + 19M \u03c1 3n + 2 + 4 2t n \u01eb.(16)\nSee Section C for a proof of the theorem. Because uniform L \u221e -covering numbers upper bound empirical L \u221e -covering numbers, it is immediate that covering F in \u2022 L \u221e (X ) provides an identical result.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Covering bounds: corollaries", "text": "We turn to a number of corollaries that expand on Theorem 3 to investigate its consequences. Our first corollary shows that Theorem 3 applies to standard Vapnik-Chervonenkis (VC) classes. As VC dimension is preserved through composition, this result also extends to the procedure (6) in typical empirical risk minimization scenarios.\nCorollary 3.1. In addition to the conditions of Theorem 3, let F have finite VC-dimension VC(F).\nThen for a numerical constant c < \u221e, the bounds (15) and (16) hold with probability at least\n1 \u2212 c VC(F) 16M ne \u01eb VC(F )\u22121 + 2 e \u2212t .\nProof Let f L 1 (Q) := |f (x)|dQ(x) denote the L 1 -norm on F for the probability distribution Q. Then by Theorem 2.6.7 of van der Vaart and Wellner [47], we have\nsup Q N (F, \u01eb, \u2022 L 1 (Q) ) \u2264 cVC(F) 8M e \u01eb VC(F )\u22121 for a numerical constant c. Because x \u221e \u2264 x 1 , taking Q to be uniform on x \u2208 X 2n yields N (F(x), \u01eb, \u2022 \u221e ) \u2264 N (F, \u01eb 2n , \u2022 L 1 (Q)\n). The result is immediate.\nNext, we focus more explicitly on the estimator \u03b8 rob n defined by minimizing the robust regularized risk (6). Let us assume that \u0398 \u2282 R d , and that we have a typical linear modeling situation, where a loss h is applied to an inner product, that is, \u2113(\u03b8, x) = h(\u03b8 \u22a4 x). In this case, by making the substitution that the class F = {\u2113(\u03b8, \u2022) : \u03b8 \u2208 \u0398} in Corollary 3.1, we have VC(F) \u2264 d, and we obtain the following corollary. In the corollary, recall the definition (1) of the population risk R(\u03b8) = E[\u2113(\u03b8, X)], and the uncertainty set P n = {P : D \u03c6 (P || P n ) \u2264 \u03c1 n }, and that R n (\u03b8, P n ) = sup P \u2208Pn E P [\u2113(\u03b8, X)]. By setting \u01eb = M/n in Corollary 3.1, we obtain the following result.", "publication_ref": ["b14", "b15", "b46", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Corollary 3.2. Let the conditions of the previous paragraph hold and let \u03b8 rob", "text": "n \u2208 argmin \u03b8\u2208\u0398 R n (\u03b8, P n ). Assume also that \u2113(\u03b8, x) \u2208 [0, M ] for all \u03b8 \u2208 \u0398, x \u2208 X . Then if n \u2265 \u03c1 \u2265 9 log 12, R( \u03b8 rob n ) \u2264 R n ( \u03b8 rob n , P n ) + 11M \u03c1 3n + 2M n 1 + \u03c1 n \u2264 inf \u03b8\u2208\u0398 R(\u03b8) + 2 2\u03c1 n Var(\u2113(\u03b8; X)) + 11M \u03c1 n with probability at least 1 \u2212 2 exp(c 1 d log n \u2212 c 2 \u03c1)\n, where c i are universal constants with c 2 \u2265 1/9.\nTo give an alternate concrete variant of Corollary 3.2 and Theorem 3, let \u0398 \u2282 R d and recall Example 3. We assume that for each x \u2208 X , inf \u03b8\u2208\u0398 \u2113(\u03b8, x) = 0 and that \u2113 is L-Lipschitz in \u03b8.\nIf D := diam(\u0398) = sup \u03b8,\u03b8 \u2032 \u2208\u0398 \u03b8 \u2212 \u03b8 \u2032 2 < \u221e, then \u2113(\u03b8, x) \u2264 L diam(\u0398)\n, and for \u03b4 > 0, we define\n\u03c1 = log 2 \u03b4 + d log(2nDL). (17\n)\nSetting t = \u03c1 and \u01eb = 1 n in Theorem 3 and assuming that \u03b4 1/n, D n k and L n k for a numerical constant k, choosing \u03b4 = 1\nn we obtain that with probability at least\n1 \u2212 \u03b4 = 1 \u2212 1/n, E[\u2113( \u03b8 rob n ; X)] = R( \u03b8 rob n ) \u2264 inf \u03b8\u2208\u0398 R(\u03b8) + C d Var(\u2113(\u03b8, X)) n log n + C dLD log n n (18\n)\nwhere C is a numerical constant.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Examples and heuristic discussion", "text": "Unpacking Theorem 3, the first result (15) (and its in Corollary 3.2) provides a high-probability guarantee that the true expectation E[ f ] cannot be more than O(1/n) worse than its robustlyregularized empirical counterpart. The second result (16) (and inequality ( 18)) guarantees convergence of the empirical minimizer to a parameter with risk at most O(log n/n) larger than the best possible variance-corrected risk.\nTo illustriate how variance regularization can yield tighter guarantees than empirical risk minimization by optimizing a O(1/n) upper bound on the risk, we now compare the second bound (16) with an analogous result for empirical risk minimization (ERM). We first give a heuristic version, making it more precise in a coming example. For the ERM solution \u03b8 erm n \u2208 argmin \u03b8\u2208\u0398 E Pn [\u2113(\u03b8; X)], one common assumption is an upper bound of the variance by the risk; for example, when the losses take values in [0, M ], one has Var(\u2113(\u03b8, X)) \u2264 M R(\u03b8). In such cases, there is typically some complexity measure Comp n associated with the class of functions being learned, and it is possible to achieve bounds of the form\nR( \u03b8 erm n ) \u2264 R(\u03b8 \u22c6 ) + C Comp n M R(\u03b8 \u22c6 ) n + C Comp n M n (19\n)\nwhere \u03b8 \u22c6 \u2208 argmin \u03b8\u2208\u0398 R(\u03b8), a type of result common for bounded nonnegative losses [10,49,48]. For example, for classes of functions of VC-dimension d, we typically have Comp n d log n d . In this caricature, when Var(\u2113(\u03b8 \u22c6 , X)) \u226a M R(\u03b8 \u22c6 ) and \u03c1 Comp n , the optimality guarantee (16) for variance regularization can be tighter than its ERM counterpart (19). This bound is certainly not always sharp, but yields minimax optimal rates in some cases. Example 4 (Well-specified least-absolute-deviation regression): We consider the least-absolutedeviation (LAD) regression problem, comparing the rates of convergence that localized Rademacher complexities guarantee against those that the robust program provides. Let Z = (X, Y ) \u2208 R d \u00d7 R, where X \u2208 {x \u2208 R d | x 2 \u2264 L}, and let D := diam(\u0398) be the \u2113 2 -diameter of \u0398. The LAD loss is\n\u2113(\u03b8; (x, y)) := |y \u2212 \u03b8, x |. For some \u03b8 \u22c6 \u2208 \u0398, assume that Y = \u03b8 \u22c6 , X + \u01eb where \u01eb \u2208 [\u2212B, B] is independent of X. We have the global bound \u2113(\u03b8; (X, Y )) \u2264 DL + B =: M.\nSuppose for simplicity that \u01eb is uniform on [\u2212B, B]; then \u03b8 \u22c6 = argmin \u03b8\u2208\u0398 R(\u03b8) and R(\u03b8\n\u22c6 ) = E[\u2113(\u03b8 \u22c6 ; Z)] = 1 2 B. In this case, Var (\u2113(\u03b8 \u22c6 ; Z)) = B 2 12 \u2264 1 2 (DL + B)B = M E[\u2113(\u03b8 \u22c6 ; Z)] = M R(\u03b8 \u22c6 ).\nUsing that the loss is 1-Lipschitz, the L \u221e covering numbers for the set of functions\nF := {f \u03b8 (x, y) = | \u03b8, x \u2212 y| | \u03b8 \u2208 \u0398} satisfy log N (F, \u01eb, \u2022 L \u221e (X ) )\nd log DL \u01eb , and so applying the bound (18) for the robustly regularized solution \u03b8 rob n with \u01eb = DL/n, we obtain\nR( \u03b8 rob n ) \u2264 R(\u03b8 \u22c6 ) + C d log n n B 2 + C d(LD + B) log n n\nwith probability at least 1\u2212 1/n. On the other hand, even an \"optimistic\" (but naive) ERM bound, achieved by taking Comp n 1 in the bound ( 19), yields 2 for some \u03b4 > 0, so that \u03b8 \u22c6 = argmin R(\u03b8) = B and R(\u03b8 \u22c6 ) = (1\u2212\u03b4)B. In this case, taking \u03b8 0 = 0 yields Var(\u2113(\u03b8; X)) = 0 and R(\u03b8 0 )\u2212R(\u03b8 \u22c6 ) = \u03b4B. For \u03b4 small (on the order of 1/ \u221a n), with constant probability the empirical risk minimizer is\nR( \u03b8 erm n ) \u2264 R(\u03b8 \u22c6 ) + C log n n (BDL + B 2 ) + C (LD + B)\n\u03b8 erm n = \u2212B, yielding risk R( \u03b8 erm n ) \u2212 R(\u03b8 \u22c6 ) = 2\u03b4B.\nOn the other hand, with high probability \u03b8 rob n \u2265 0 (because Var(\u2113(\u03b8 0 ; X)) = 0 as \u2113(0; X) \u2261 B), and so R( \u03b8 rob n ) \u2212 R(\u03b8 \u22c6 ) \u2264 \u03b4B. This gap is of course small, but it shows that the robust solution is more conservative: it chooses \u03b8 rob n so that large losses (of scale 2B) are less frequent. \u2738 When the population problem is \"easy\", it is often possible to achieve faster rates of convergence than the usual O (1/\n\u221a n) rate. The simplest scenario where this occurs is if the problem is realizable R(\u03b8 \u22c6 ) = 0, in which case \u03b8 erm n has excess risk of the order O(log n/n); see the bound (19). The robustly regularized solution \u03b8 rob n enjoys the same faster rates of convergence under the more general condition that Var(\u2113(\u03b8 \u22c6 ; X)) is small. As a concrete instance of this, let \u2113(\u03b8; X) \u2208 [0, M ] and assume that \u2113(\u03b8; X) satisfies the conditions of the first part of Example 3, and let the problem be realizable R(\u03b8 \u22c6 ) = 0. Since Var(\u2113(\u03b8; X)) \u2264 M R(\u03b8), we have from the bounds ( 18) and ( 19) that\nR( \u03b8 erm n ) \u2264 CdDL log n n and R( \u03b8 rob n ) \u2264 CdDL log n n .\nFor example, Var(\u2113(\u03b8; X)) = 0 allows for the existence of some \u03b8 0 \u2208 \u0398 such that \u2113(\u03b8 0 ; X) < \u2113(\u03b8 \u22c6 ; X) with positive probability.", "publication_ref": ["b15", "b9", "b48", "b47", "b18", "b17", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "Localized Rademacher Complexity", "text": "A somewhat more sophisticated approach to concentration inequalities and generalization bounds is based on localization ideas, motivated by the fact that near the optimum of an empirical risk, the complexity of the function class may be smaller than over the entire (global) class [47,3]. With this in mind, we now present a refined version of Theorem 3 that depends on localized Rademacher averages.\nThe starting point for this approach is a notion of localized Rademacher complexity (we give a slightly less general notion than Bartlett et al. [3], as it is sufficient for our derivations). For a function class F of functions f : X \u2192 R, the localized Rademacher complexity at level r is\nE R n cf | f \u2208 F, c \u2208 [0, 1], E[c 2 f 2 \u2264 r]\n.\nIn addition, we require a few analytic notions, beginning with sub-root functions, where we recall [3] that a function \u03c8 : R + \u2192 R + is sub-root if it is nonnegative, nondecreasing, and r \u2192 \u03c8(r)/ \u221a r is nonincreasing for all r > 0. Any non-constant sub-root function \u03c8 is continuous and has a unique positive fixed point r \u22c6 = \u03c8(r \u22c6 ), where r \u2265 \u03c8(r) for all r \u2265 r \u22c6 . Lastly, we consider upper bounds \u03c8 n : R + \u2192 R + on the localized Rademacher complexity satisfying\n\u03c8 n (r) \u2265 E[R n ({cf : f \u2208 F, c \u2208 [0, 1], E[c 2 f 2 ] \u2264 r})],(20)\nwhere \u03c8 n is sub-root. (The localized Rademacher complexity itself is sub-root.) Roots of \u03c8 n play a fundamental role in providing uniform convergence guarantees, and Bartlett et al. [3] and Koltchinskii [25] provide careful analyses of localized Rademacher complexities, with typical results as follows. For a class of functions f with range bounded by 1, for any root r \u22c6 n of \u03c8 n , with probability at least 1 \u2212 e \u2212t we have\nE[f ] \u2264 E Pn [f ] + 1 \u03b7 E Pn [f ] + C(1 + \u03b7) r \u22c6 n + 1 n + t n\nfor all f \u2208 F and \u03b7 \u2265 0.\nAs an example, when F is a bounded VC-class, we have r \u22c6 n \u224d VC(F ) log(n/VC(F ))\nn [3,Corollary 3.7]. With this motivation, we have the following theorem. Theorem 4. For M \u2265 1, let F be a collection of functions f : X \u2192 [0, M ], let \u03c8 n be a sub-root function bounding the localized complexity (20), and let r \u22c6 n \u2265 \u03c8 n (r \u22c6 n ). Let t > 0 be arbitrary and assume that \u03c1 satisfies\n\u03c1 n \u2265 8 45M n t + log log n t + 18r \u22c6 n .(21)\nThen with probability at least 1 \u2212 e \u2212t ,\nE[f ] \u2264 1 + 2 2\u03c1 n sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f ] + 13 + 4 2\u03c1 n M \u03c1 n for all f \u2208 F.(22)\nAdditionally, if f minimizes sup P :D \u03c6 (P || Pn)\u2264\u03c1/n E P [f ], then with probability at least 1 \u2212 3e \u2212t ,\nE[ f ] \u2264 1 + 2 2\u03c1 n inf f \u2208F E[f ] + 91\u03c1 45n Var(f ) + 14 + 6 2\u03c1 n M (3\u03c1 + t) n .(23)\nWe provide the proof of Theorem 4 in Appendix D. It builds off of and parallels many of the techniques developed by Bartlett, Bousquet, and Mendelson [3], but we require a bit of care to develop the precise variance bounds we provide.\nLet us consider the additional \u03c1 n factors in Theorem 4 (as compared to Theorem 3). In general, these terms are negligible to the extent that the variance of f dominates the first moment of the function f -heuristically, in situations in which we expect penalizing the variance to improve performance. Let us make this more precise in a regime where n is large. Letting f \u2208 F, we see that we have the inequality\n(1 + \u03c1/n) E[f ] + \u03c1 n Var(f ) \u2264 E[f ] + C \u03c1 n Var(f ) (for a constant C > 1+ \u03c1/n) if and only if (C \u22121\u2212 \u03c1/n) 2 Var(f ) \u2265 E[f ] 2 .\nEquivalently, as n gets large, this occurs roughly when\nE[f 2 ] \u2265 C 2 \u22122C+2 C 2 \u22122C+1 E[f ] 2\n, which holds for large enough C whenever Var(f ) > 0.\nIn some scenarios, we can obtain substantially tighter bounds by using localized Rademacher averages instead of the covering number arguments considered in Section 3.1. (Recall also the discussion following Theorem 2.) To illustriate this point, we consider the case where F is a bounded subset of a reproducing kernel Hilbert space generated by some sufficiently nice kernel K; even for the Gaussian kernel K(x, z) = exp(\u2212 1 2 x \u2212 z 2 ), log covering numbers for such function spaces grow at least exponentially in the dimension [51,26]. Example 6 (Reproducing kernels and least-absolute-deviation regression): We now give an example using a non-parametric class of functionals in which covering number arguments do not apply, as the covering numbers of the associated classes are too large. Let H be a reproducing kernel Hilbert space (RKHS) with norm \u2022 H and associated kernel (representer of evaluation) K : X \u00d7 X \u2192 R. Letting P be a distribution on X , Mercer's theorem [e.g. 18] implies that the integral operator\nT K : L 2 (X , P ) \u2192 L 2 (X , P ) defined by T K (f )(x) = K(x, z)dP (z) is compact, and K(x, x \u2032 ) = \u221e j=1 \u03bb j \u03c6 j (x)\u03c6 j (z)\nwhere \u03bb j are the eigenvalues of T in decreasing order and \u03c6 j form an orthonormal decomposition of L 2 (X , P ).\nConsider now the least absolute deviation (LAD) loss function \u2113(h; x, y) = |h(x)\u2212 y|, defined for h \u2208 H, and let B H be the unit \u2022 H -ball of H. Assume additionally that the model is well-specified, and that y = h \u22c6 (x) + \u03be for some random variable \u03be with\nE[\u03be | X] = 0, E[\u03be 2 ] \u2264 \u03c3 2 , and h \u22c6 \u2208 B H . Let the function class {\u2113 \u2022 H} \u2264r := (x, y) \u2192 c\u2113(h(x), y) | c \u2208 [0, 1], c 2 E[\u2113(h(X), Y ) 2 ] \u2264 r .\nBased on inequality (20), we consider the localized complexity\nR n ({\u2113 \u2022 H} \u2264r ) = E 1 n sup h\u2208B H ,c\u2208[0,1] \u03b5 i c\u2113(h(x i ), y i ) | E[\u2113(h(X), Y ) 2 ] \u2264 r/c 2 .\nWe claim that\nR n ({\u2113 \u2022 H} \u2264r ) r/n + \uf8eb \uf8ed 1 n \u221e j=1 min{\u03bb j , r} \uf8f6 \uf8f8 1 2 . (24\n)\nAs this claim is not central to our development-but does show a slightly different localization result based on Gaussian comparison inequalities than available, for example, in Mendelson [34]we provide its proof in Appendix G.1.\nLet us use inequality (24). To apply Theorem 3, we must find a bound on the fixed point of the localized complexity. To give this bound, we require some knowledge on the eigenvalues \u03bb j , for which there exists a body of work. For example [34], the Gaussian kernel\nK(x, x \u2032 ) = exp(\u2212 1 2 x \u2212 x \u2032 2 2\n) generates a class of smooth functions for which the eigenvalues \u03bb j decay exponentially, as \u03bb j e \u2212j 2 . Kernel operators underlying Sobolev spaces with different smoothness orders [9,23] typically have eigenvalues scaling as \u03bb j j \u22122\u03b1 for some \u03b1 > 1 2 . As a concrete example, the first-order Sobolev (min) kernel K(x, x \u2032 ) = 1 + min{x, x \u2032 } generates an RKHS of Lipschitz functions with \u03b1 = 1. In the former case of \u03bb j e \u2212j 2 , r\n\u22c6 n = \u221a log n n \uf8eb \uf8ed 1 n \u221e j=1 min e \u2212j 2 , log n n \uf8f6 \uf8f8 1 2 \u2248 \uf8eb \uf8ed 1 n \u221a log n j=1 \u221a log n n + 1 n \u221e \u221a log n e \u2212t 2 dt \uf8f6 \uf8f8 1 2 \u221a log n n = r \u22c6 n .\nIn the latter case of polynomially decaying eigenvalues \u03bb j j \u22122\u03b1 , we have j \u22122\u03b1 = r when r\n\u2212 1 2\u03b1 = j, so \u221e j=1 min{j \u22122\u03b1 , r} \u2248 r 2\u03b1\u22121 2\u03b1 + \u221e r \u22121/2\u03b1 t \u22122\u03b1 dt \u224d r 2\u03b1\u22121 2\u03b1 . Solving for nr = r 2\u03b1\u22121 2\u03b1 , we find the fixed point (r \u22c6 n ) 2\u03b1\u22121 4\u03b1 = r \u22c6 n \u221a n yields r \u22c6 n = n \u2212 2\u03b1 2\u03b1+1\n. Ignoring constants, the above analysis shows that in the case that the kernel eigenvalues scale as \u03bb j e \u2212j 2 , as soon as \u03c1 \u221a log n we have\nE[\u2113(h(X), Y )] \u2264 (1 + 2 2\u03c1/n) E Pn [\u2113(h(X), Y )] + 2\u03c1 n Var Pn (\u2113(h(X), Y )) + C\u03c1 n for all h \u2208 B H\nwith high probability. In the case of polynomial eigenvalues, if h minimizes the robust empirical loss sup P :\nD \u03c6 (P || Pn)\u2264\u03c1/n E P [\u2113(h(X), Y )] and \u03c1 \u224d n 1\u2212 2\u03b1 2\u03b1+1 , then E \u2113( h(X), Y ) \u2264 1 + Cn \u2212 \u03b1 2\u03b1+1 inf h\u2208B H E[\u2113(h(X), Y )] + Cn \u2212 \u03b1 2\u03b1+1 Var(\u2113(h(X), Y )) + Cn \u2212 2\u03b1 2\u03b1+1 .\nThis rate of convergence holds without any assumptions on the smoothness of the distribution of the noise \u03be. \u2738", "publication_ref": ["b46", "b2", "b2", "b2", "b2", "b24", "b2", "b19", "b2", "b50", "b25", "b23", "b33", "b8", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Beating empirical risk minimization", "text": "We now provide a concrete example where the robustly regularized estimator \u03b8 rob n exhibits a substantial performance gap over empirical risk minimization. In the sequel, we bound the performance degradation to show that the formulation (6) in general loses little over empirical risk minimization. For intuition in this section, consider the (admittedly contrived) setting in which we replace the loss \u2113(\u03b8, X) with \u2113(\u03b8, X) \u2212 \u2113(\u03b8 \u22c6 , X), where \u03b8 \u22c6 \u2208 argmin \u03b8\u2208\u0398 R(\u03b8). Then in this case, by taking \u03b8 = \u03b8 \u22c6 in Corollary 3.2, we have R( \u03b8 rob n ) \u2264 R(\u03b8 \u22c6 ) + O(1/n) with high probability. More broadly, we expect the robustly regularized approach to offer performance benefits in situations in which the empirical risk minimizer is highly sensitive to noise, say, because the losses are piecewise linear, and slight under-or over-estimates of slope may significantly degrade solution quality.\nWith this in mind, we construct a concrete 1-dimensional example-estimating the median of a discrete distribution supported on X = {\u22121, 0, 1}-in which the robustly regularized estimator has convergence rate log n/n, while empirical risk minimization is at best 1/ \u221a n. Define the loss \u2113(\u03b8; x) = |\u03b8 \u2212 x| \u2212 |x|, and for \u03b4 \u2208 (0, 1) let the distribution P be defined by\nP (X = 1) = 1 \u2212 \u03b4 2 , P (X = \u22121) = 1 \u2212 \u03b4 2 , P (X = 0) = \u03b4.(25)\nThen for \u03b8 \u2208 R, the risk of the loss is\nR(\u03b8) = \u03b4|\u03b8| + 1 \u2212 \u03b4 2 |\u03b8 \u2212 1| + 1 \u2212 \u03b4 2 |\u03b8 + 1| \u2212 (1 \u2212 \u03b4).\nBy symmetry, it is clear that \u03b8 \u22c6 := argmin \u03b8 R(\u03b8) = 0, which satisfies R(\u03b8 \u22c6 ) = 0. (Note also that \u2113(\u03b8, x) = \u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u22c6 , x).) Without loss of generality, we assume that \u0398 = [\u22121, 1] in this problem. Now, consider a sample X 1 , . . . , X n drawn i.i.d. from the distribution P , let P n denote its empirical distribution, and define the empirical risk minimizer\n\u03b8 erm n := argmin \u03b8\u2208R E Pn [\u2113(\u03b8, X)] = argmin \u03b8\u2208[\u22121,1] E Pn [|\u03b8 \u2212 X|].\nIf too many of the observations satisfy X i = 1 or too many satisfy X i = \u22121, then \u03b8 erm n will be either 1 or \u22121; for small \u03b4, such events become reasonably probable, as the following lemma makes precise. In the lemma, \u03a6\n(x) = 1 \u221a 2\u03c0 x \u2212\u221e e \u2212 1 2 t 2 dt denotes the standard Gaussian CDF. (See Section G.2 for a proof.) Lemma 3.1. Let the loss \u2113(\u03b8; x) = |\u03b8 \u2212 x| \u2212 |x|, \u03b4 \u2208 [0, 1], and X follow the distribution (25). Then R( \u03b8 erm n ) \u2212 R(\u03b8 \u22c6 ) \u2265 \u03b4 with probability at least 2\u03a6 \u2212 n\u03b4 2 1 \u2212 \u03b4 2 \u2212 (1 \u2212 \u03b4 2 ) n 2 8 \u03c0n .\nOn the other hand, we certainly have \u2113(\u03b8 \u22c6 ; x) = 0 for all x \u2208 X , so that Var(\u2113(\u03b8 \u22c6 ; X)) = 0. Now, consider the bound in Theorem 3. We see that log N ({\u2113(\u03b8,\n\u2022) : \u03b8 \u2208 \u0398}, \u01eb, \u2022 L \u221e (X ) ) \u2264 2 log 1\n\u01eb , and taking \u01eb = 1 n , we have that if \u03b8 rob n \u2208 argmin \u03b8\u2208\u0398 R n (\u03b8, P n ), then\nR( \u03b8 rob n ) \u2264 R(\u03b8 \u22c6 ) + 15\u03c1 n with probability \u2265 1 \u2212 4 exp (2 log n \u2212 \u03c1) .\nIn particular, taking \u03c1 = 3 log n, we see that R( \u03b8 rob n ) \u2264 R(\u03b8 \u22c6 ) + 45 log n n with probability at least 1 \u2212 4 n .\nThe risk for the empirical risk minimizer, as Lemma 3.1 shows, may be substantially higher; taking \u03b4 = 1/ \u221a n we see that with probability at least 2\u03a6(\u2212\nn n\u22121 ) \u2212 2 \u221a 2/ \u221a \u03c0en \u2265 2\u03a6(\u2212 n n\u22121 ) \u2212 n \u2212 1 2 , R( \u03b8 erm n ) \u2265 R(\u03b8 \u22c6 ) + n \u2212 1 2 .\n(For n \u2265 20, the probability of this event is \u2265 .088.) For this (specially constructed) example, there is a gap of nearly n 1 2 in order of convergence.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Invariance properties", "text": "The robust regularization (4) technique enjoys a number of invariance properties. Standard regularization techniques (such as \u2113 1 -and \u2113 2 -regularization), which generally regularize a parameter toward a particular point in the parameter space, do not. While we leave deeper discussion of these issues to future work, we make two observations, which apply when \u0398 = R d is unconstrained. Throughout, we let \u03b8 rob n \u2208 argmin \u03b8 R n (\u03b8, P n ) denote the robustly regularized empirical solution. First, consider a location estimation problem in which we wish to estimate the minimizer of some the expectation of a loss of the form \u2113(\u03b8, X) = h(\u03b8 \u2212 X), where h : R d \u2192 R is convex and symmetric about zero. Then the robust solution is by inspection shift invariant, as \u2113(\u03b8 + c, X + c) = \u2113(\u03b8, X) for any vector c \u2208 R d . Concretely, in the example of the previous section, \u2113 1 -or \u2113 2 -regularization achieve better convergence guarantees than ERM does, but if we shift all data x \u2192 x + c, then non-invariant regularization techniques lose efficiency (while the robust regularization technique does not). Second, we may consider a generalized linear modeling problem, in which data comes in pairs (x, y) \u2208 X \u00d7 Y and \u2113(\u03b8, (x, y)) = h(y, \u03b8 \u22a4 x) for a function h : Y \u00d7 R \u2192 R that is convex in its second argument. Then \u03b8 rob n is invariant to invertible linear transformations, in the sense that for any invertible A \u2208 R d\u00d7d , argmin\n\u03b8 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [\u2113(\u03b8, (X, Y ))] = argmin \u03b8 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [\u2113(A \u22121 \u03b8, (AX, Y ))] = \u03b8 rob n .\nOur results in this section do not precisely apply as we require unbounded \u03b8, however, the next section shows that localization approaches can address this.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Robust regularization cannot be too bad", "text": "The previous two sections provide guarantees on the performance of the robust regularized estimator (6), it does not-cannot-dominate classical approaches based on empirical risk minimization (also known as sample average approximation in the stochastic optimization literature), though it can improve on them in some cases. For example, with a correctly specified linear regression model with gaussian noise, least-squares-empirical risk minimization with the loss \u2113(\u03b8, (x, y)) = 1 2 (\u03b8 \u22a4 x \u2212 y) 2 -is essentially optimal. Our goal in this section is thus to provide more understanding of potential poor behavior of the procedure (6) with respect to ERM, considering two scenarios. The first is in stochastic (convex) optimization problems, where we investigate the finite-sample convergence rates of the robust solution to the population optimal risk. We show that the robust solution \u03b8 rob n enjoys fast rates of convergence in cases in which the risk has substantial curvature-precisely as with empirical risk minimization. The second is to consider the asymptotics of the robust solution \u03b8 rob n , where we show that in classical statistical scenarios the robust solution is nearly efficient, though there is an asymptotic bias of order 1/ \u221a n that scales with the confidence \u03c1.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fast Rates", "text": "In cases in which the risk R has curvature, empirical risk minimization often enjoys faster rates of convergence [10,40]. The robust solution \u03b8 rob n similarly attains faster rates of convergence in such cases, even with approximate minimizers of R n (\u03b8, P n ). For the risk R and \u01eb \u2265 0, let\nS \u01eb \u22c6 := \u03b8 \u2208 \u0398 : R(\u03b8) \u2264 inf \u03b8 \u22c6 \u2208\u0398 R(\u03b8 \u22c6 ) + \u01eb\ndenote the \u01eb-sub-optimal (solution) set, and similarly let\nS \u01eb \u22c6 := \u03b8 \u2208 \u0398 : R n (\u03b8, P n ) \u2264 inf \u03b8 \u2032 \u2208\u0398 R n (\u03b8 \u2032 , P n ) + \u01eb .\nFor a vector \u03b8 \u2208 \u0398, let \u03c0 S\u22c6 (\u03b8) = argmin \u03b8 \u22c6 \u2208S\u22c6 \u03b8 \u22c6 \u2212 \u03b8 2 denote the Euclidean projection of \u03b8 onto the set S \u22c6 ; this projection operator is very useful for showing faster rates of convergence in stochastic optimization (see Shapiro et al. [40], whose techniques we closely follow). In the statement of the result, for A \u2282 \u0398, we let R n (A) denote the Rademacher complexity of the localized process {x \u2192 \u2113(\u03b8; x) \u2212 \u2113(\u03c0 S\u22c6 (\u03b8); x) : \u03b8 \u2208 A}. We then have the following result, whose proof we provide in Section E.\nTheorem 5. Let \u0398 be convex and let \u2113(\u2022; x) be convex and L-Lipshitz in its first argument for all x \u2208 X . For constants \u03bb > 0, \u03b3 > 1, and r > 0, assume the risk R satisfies\nR(\u03b8) \u2212 inf \u03b8\u2208\u0398 R(\u03b8) \u2265 \u03bb dist(\u03b8, S \u22c6 ) \u03b3 for all \u03b8 such that dist(\u03b8, S \u22c6 ) \u2264 r. (26\n)\nLet t > 0. If 0 \u2264 \u01eb \u2264 1 2 \u03bbr \u03b3 satisfies \u01eb \u2265 2 8 \u03b3 L \u03b3 \u03bb 1 \u03b3\u22121 \u03c1 n \u03b3 2(\u03b3\u22121) and \u01eb 2 \u2265 2E[R n (S 2\u01eb \u22c6 )] + L 2\u01eb \u03bb 1 \u03b3 2t n , (27\n)\nthen P( S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 ) \u2265 1 \u2212 e \u2212t ,\nWe provide a brief discussion of this result as well as a corollary that gives more explicit rates of convergence. First, we note that (by an inspection of the proof) the L-Lipschitz assumption need only hold in the neighborhood S 2\u01eb \u22c6 for the result to hold. We also have the following Corollary 4.1. In addition to the conditions of Theorem 5, assume that S \u22c6 = {\u03b8 \u22c6 } is a single point and \u0398 \u2282 R d . Then for any \u01eb \u2264 1 2 \u03bbr \u03b3 , we have\nP( S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 ) \u2265 1 \u2212 e \u2212t for \u01eb L \u03b3 \u03bb 1 \u03b3\u22121 d n log n d + t n + \u03c1 n \u03b3 2(\u03b3\u22121)\n. So long as \u03c1 d log n d , this rate of convergence is as good as that enjoyed by standard empirical risk minimization approaches [40,Ch. 5] under these types of growth conditions. The case that \u03b3 = 2 corresponds (roughly) to strong convexity, and in this case we get the approximate rate of convergence of L 2 \u03bb d log n d n , the familiar rate of convergence under these conditions. Of course, if there is too much variance penalization (i.e. \u03c1 is too large), then the rates of convergence may be slower.\nProof That S \u22c6 is a singleton implies that S 2\u01eb \u22c6 \u2282 {\u03b8 | \u03b8 \u2212 \u03b8 \u22c6 \u2264 (2\u01eb/\u03bb) 1 \u03b3 }.\nMoreover, in this case we also have that\nE Pn [\u2113(\u03b8; X) \u2212 \u2113(\u03b8 \u22c6 ; X)] \u2212 E Pn [\u2113(\u03b8 \u2032 ; X) \u2212 \u2113(\u03b8 \u22c6 ; X)] \u2264 L \u03b8 \u2212 \u03b8 \u2032 , so that an \u01eb/L-cover of {\u03b8 | \u03b8 \u2212 \u03b8 \u22c6 \u2264 (2\u01eb/\u03bb) 1 \u03b3 } is an \u01eb-cover of the function class F = {f (x) = \u2113(\u03b8; x) \u2212 \u2113(\u03b8 \u22c6 ; x) | \u03b8 \u2208 S 2\u01eb \u22c6 } in \u2022 L 2 (Pn)\nnorm. Thus, the standard Dudley entropy integral [21,47] yields\nE[R n (S 2\u01eb \u22c6 )] 1 \u221a n \u221e 0 log N (F, \u03b4, \u2022 L 2 (Pn) )d\u03b4 1 \u221a n L(2\u01eb/\u03bb) 1 \u03b3 0 d log L \u03b4 d\u03b4 \u2264 L d n 2\u01eb \u03bb 1 \u03b3 1 + 1 \u03b3 log \u03bb 2L \u03b3 \u01eb\nwhere we have used that\n\u03b5 0 log L \u03b4 d\u03b4 \u2264 \u03b5 1 + log L \u03b5 .\nSolving for \u01eb in the localization inequality (27) then yields the corollary, showing that the specified choice of \u01eb is sufficient for all the conditions (27) to hold.", "publication_ref": ["b9", "b39", "b39", "b39", "b20", "b46", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Asymptotics", "text": "It is important to understand the precise limiting behavior of the robust estimator in addition to its finite sample properties-this allows us to more precisely characterize when there may be degradation relative to classical risk minimization strategies. With that in mind, in this section we provide asymptotic results for the robust solution (6) to better understand the consequences of penalizing the variance of the loss itself. In particular, we would like to understand efficiency losses relative to (say) maximum likelihood in situations in which maximum likelihood is efficient. Before stating the results, we make a few standard assumptions on the risk R(\u03b8), the loss \u2113, and the moments of \u2113 and its derivatives. Concretely, we assume that\n\u03b8 \u22c6 := argmin \u03b8 R(\u03b8) and \u2207 2 R(\u03b8 \u22c6 ) \u227b 0,\nthat is, the risk functional has strictly positive definite Hessian at \u03b8 \u22c6 , which is thus unique. Additionally, we have the following smoothness assumptions on the loss function, which are satisfied by common loss functions, including the negative log-likelihood for any exponential family or generalized linear model [29]. In the assumption, we let B denote the \u2113 2 -ball of radius 1 in R d . Assumption A. For some \u01eb > 0, there exists a function L :\nX \u2192 R + satisfying |\u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u2032 , x)| \u2264 L(x) \u03b8 \u2212 \u03b8 \u2032 2 for \u03b8, \u03b8 \u2032 \u2208 \u03b8 \u22c6 + \u01ebB and E[L(X) 2 ] \u2264 L(P ) < \u221e.\nAdditionally, there is a function H such that the function \u03b8 \u2192 \u2113(\u03b8, x) has H(x)-Lipschitz continuous Hessian (with respect to the Frobenius norm) on \u03b8 \u22c6 + \u01ebB, where\nE[H(X) 2 ] < \u221e.\nThen, recalling the robust estimator (6) as the minimizer of R n (\u03b8, P n ), we have the following theorem, which we prove in Section F. Theorem 6. Let Assumption A hold, and let the sequence \u03b8 rob n be defined by \u03b8 rob n \u2208 argmin \u03b8 R n (\u03b8,\nP n ). Define b(\u03b8 \u22c6 ) := Cov(\u2207 \u03b8 \u2113(\u03b8 \u22c6 , X), \u2113(\u03b8 \u22c6 , X)) Var(\u2113(\u03b8 \u22c6 , X)) and \u03a3(\u03b8 \u22c6 ) = \u2207 2 R(\u03b8 \u22c6 ) \u22121 Cov(\u2207\u2113(\u03b8 \u22c6 , X)) \u2207 2 R(\u03b8 \u22c6 ) \u22121 .\nThen \u03b8 rob n a.s.\n\u2192 \u03b8 \u22c6 and \u221a n( \u03b8 rob n \u2212 \u03b8 \u22c6 ) d \u2192 N \u2212 2\u03c1 b(\u03b8 \u22c6 ), \u03a3(\u03b8 \u22c6 )\nThe asymptotic variance \u03a3(\u03b8 \u22c6 ) in Theorem 6 is generally unimprovable, as made apparent by Le Cam's local asymptotic normality theory and the H\u00e1jek-Le Cam local minimax theorems [47]. Thus, Theorem 6 shows that the robust regularized estimator (6) has some efficiency loss, but it is only in the bias term. We explore this a bit more in the context of the risk of \u03b8 rob n . Letting W \u223c N(0, \u03a3(\u03b8 \u22c6 )), as an immediate corollary to this theorem, the delta-method implies that\nn R( \u03b8 rob n ) \u2212 R(\u03b8 \u22c6 ) d \u2192 1 2 2\u03c1 b(\u03b8 \u22c6 ) + W 2 \u2207 2 R(\u03b8 \u22c6 ) ,(28)\nwhere we recall that x 2 A = x \u22a4 Ax. This follows from a Taylor expansion, because \u2207R(\u03b8 \u22c6 ) = 0 and so R\n(\u03b8) \u2212 R(\u03b8 \u22c6 ) = 1 2 (\u03b8 \u2212 \u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )(\u03b8 \u2212 \u03b8 \u22c6 ) + o( \u03b8 \u2212 \u03b8 \u22c6 2 ), or n(R( \u03b8 rob n ) \u2212 R(\u03b8 \u22c6 )) = n 1 2 ( \u03b8 rob n \u2212 \u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + o( \u03b8 rob n \u2212 \u03b8 \u22c6 2 ) = 1 2 \u221a n( \u03b8 rob n \u2212 \u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 ) \u221a n( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + o P (1) d \u2192 1 2 ( 2\u03c1 b(\u03b8 \u22c6 ) + W ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )( 2\u03c1 b(\u03b8 \u22c6 ) + W )\nby Theorem 6.\nThe limiting random variable in expression (28) has expectation\n1 2 E[ 2\u03c1b(\u03b8 \u22c6 ) + W 2 \u2207 2 R(\u03b8 \u22c6 ) ] = \u03c1b(\u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )b(\u03b8 \u22c6 ) + 1 2 tr(\u2207 2 R(\u03b8 \u22c6 ) \u22121 Cov(\u2113(\u03b8 \u22c6 , X)),\nwhile the classical empirical risk minimization procedure (standard M -estimation) [29,47] has limiting mean-squared error\n1 2 tr(\u2207 2 R(\u03b8 \u22c6 ) \u22121 Cov(\u2113(\u03b8 \u22c6 , X))). Thus there is an additional \u03c1 b(\u03b8 \u22c6 ) 2 \u2207 2 R(\u03b8 \u22c6 )\npenalty in the asymptotic risk (at a rate of 1/n) for the robustly-regularized estimator. An inspection of the proof of Theorem 6 reveals that b(\u03b8 \u22c6 ) = \u2207 \u03b8 Var(\u2113(\u03b8 \u22c6 , X)); if the variance of the loss is stable near \u03b8 \u22c6 , so that moving to a parameter \u03b8 = \u03b8 \u22c6 + \u2206 for some small \u2206 has little effect on the variance, then the standard loss terms dominate, and robust regularization has asymptotically little effect. On the other hand, highly unstable loss functions for which \u2207 \u03b8 Var(\u2113(\u03b8 \u22c6 , X)) is large yield substantial bias.\nWe conclude our study of the asymptotics with a (to us) somewhat surprising example. Consider the classical linear regression setting in which y = x \u22a4 \u03b8 \u22c6 + \u03b5, where \u03b5 \u223c N(0, \u03c3 2 ). Using the standard squared error loss \u2113(\u03b8, (x, y)) = 1 2 (\u03b8 \u22a4 x \u2212 y) 2 , we obtain that\n\u2207\u2113(\u03b8 \u22c6 , (x, y)) = (x \u22a4 \u03b8 \u22c6 \u2212 y)x = (x \u22a4 \u03b8 \u22c6 \u2212 x \u22a4 \u03b8 \u22c6 \u2212 \u03b5)x = \u2212\u03b5x, while \u2113(\u03b8 \u22c6 , (x, y)) = 1 2 \u03b5 2 . The covariance Cov(\u03b5X, \u03b5 2 ) = E[\u03b5X(\u03b5 2 \u2212 \u03c3 2\n)] = 0 by symmetry of the error distribution, and so-in the special classical case of correctly specified linear regressionthe bias term b(\u03b8 \u22c6 ) = 0 for linear regression in Theorem 6. That is, the robustly regularized estimator ( 6) is asymptotically efficient.", "publication_ref": ["b28", "b46", "b28", "b46", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "We present three experiments in this section. The first is a small simulation example, which serves as a proof of concept allowing careful comparison of standard empirical risk minimization (ERM) strategies to our variance-regularized approach. The latter two are classification problems on real datasets; for both of these we compare performance of robust solution (6) to its ERM counterpart.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Minimizing the robust objective", "text": "As a first step, we give a brief description of our (essentially standard) method for solving the robust risk problem. Our work in this paper focuses mainly on the properties of the robust objective (4) and its minimizers (6), so we only briefly describe the algorithm we use; we leave developing faster and more accurate specialized methods to further work. To solve the robust problem, we use a gradient descent-based procedure, and we focus on the case in which the empirical sampled losses {\u2113(\u03b8, X i )} n i=1 have non-zero variance for all parameters \u03b8 \u2208 \u0398, which is the case for all of our experiments.\nRecall the definition of the subdifferential \u2202f (\u03b8\n) = {g \u2208 R d : f (\u03b8 \u2032 ) \u2265 f (\u03b8) + g, \u03b8 \u2032 \u2212 \u03b8 for all \u03b8 \u2032 }\n, which is simply the gradient for differentiable functions f . A standard result in convex analysis [24,Theorem VI.4.4.2] is that if the vector p * \u2208 R n + achieving the supremum in the definition (4) of the robust risk is unique, then\n\u2202 \u03b8 R n (\u03b8, P n ) = \u2202 \u03b8 sup P \u2208Pn E P [\u2113(\u03b8; X)] = n i=1 p * i \u2202 \u03b8 \u2113(\u03b8; X i ),\nwhere the final summation is the standard Minkowski sum of sets. As this maximizing vector p is indeed unique whenever Var Pn (\u2113(\u03b8; X)) = 0, we see that for all our problems, so long as \u2113 is differentiable, so too is R n (\u03b8, P n ) and\n\u2207 \u03b8 R n (\u03b8, P n ) = n i=1 p * i \u2207 \u03b8 \u2113(\u03b8; X i ) where p * = argmax p\u2208Pn n i=1 p i \u2113(\u03b8; X i ) .(29)\nIn order to perform gradient descent on the risk R n (\u03b8, P n ), then, by equation ( 29) we require only the computation of the worst-case distribution p * . By taking the dual of the maximization (29), this is an efficiently solvable convex problem; for completeness, we provide a procedure for this computation in Section H that requires time O(n log n + log 1 \u01eb log n) to compute an \u01eb-accurate solution to the maximization (29). As all our examples have smooth objectives, we perform gradient descent on the robust risk R n (\u2022, P n ), with stepsizes chosen by a backtracking (Armijo) line search [15, Chapter 9.2].\nCode is available at https://github.com/hsnamkoong/robustopt.", "publication_ref": ["b23", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Simulation experiment", "text": "For our simulation experiment, we use a quadratic loss with linear perturbation. For v, x \u2208 R d , define the loss \u2113(\u03b8; x) = 1 2 \u03b8 \u2212 v 2 2 + x \u22a4 (\u03b8 \u2212 v). We set d = 50 and take X \u223c Uni({\u2212B, B} d ), varying B in the experiment. For concreteness, we let the domain \u0398 = {\u03b8 \u2208 R d : \u03b8 2 \u2264 r} and set v = r 2 \u221a d 1, so that v \u2208 int \u0398; we take r = 10. Notably, standard regularization strategies, such as \u2113 1 or \u2113 2 -regularization, pull \u03b8 toward 0, while the variance of \u2113(\u03b8; X) is minimized by \u03b8 = v (thus naturally advantaging the variance-based regularization we consider, as R(v) = inf \u03b8 R(\u03b8) = 0). Moreover, as X is pure noise, this is an example where we expect variance regularization to be particularly useful. We choose \u03b4 = .05 and set \u03c1 as in Eq. (17) (using that \u2113 is (3r+ \u221a dB)-Lipschitz) to obtain robust coverage with probability at least 1 \u2212 \u03b4. In our experiments, we obtained 100% coverage in the sense of ( 15), as the high probability bound is conservative.  \nn = argmin \u03b8\u2208\u0398 E Pn [\u2113(\u03b8, X)] in terms of the true risk E[\u2113(\u03b8, X)] = 1 2 \u03b8 \u2212 v 2 2 .\nEach experiment consists of 1,200 independent replications for each sample size n and value B. In Tables 1 and 2, we display the risks of \u03b8 erm n and \u03b8 rob n and variances, respectively, computed for the 1,200 independent trials. The gap between the risk of \u03b8 erm n and \u03b8 rob n is siginificant at level p < .01 for all sample sizes and values of B we considered according to a one-sided T-test. Notice also in Table 2 that the variance of the robust solutions is substantially smaller than that of the empirical risk minimizer-often several orders of magnitude smaller for large sample sizes n. This simulation shows that-in a simple setting favorable to it-our procedure outperforms standard alternatives.   \nB = .01 B = .1 B = 1 B = 10 n R( \u03b8 erm n ) R( \u03b8 rob n ) R( \u03b8 erm n ) R( \u03b8 rob n ) R( \u03b8 erm n ) R( \u03b8 rob n ) R( \u03b8 erm n ) R( \u03b8", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1", "tab_2"]}, {"heading": "Protease cleavage experiments", "text": "For our second experiment, we compare our robust regularization procedure to other regularizers using the HIV-1 protease cleavage dataset from the UCI ML-repository [31]. In this binary classification task, one is given a string of amino acids (a protein) and a featurized representation of the string of dimension d = 50960, and the goal is to predict whether the HIV-1 virus will cleave the amino acid sequence in its central position. We have a sample of n = 6590 observations of this process, where the class labels are somewhat skewed: there are 1360 examples with label Y = +1 (HIV-1 cleaves) and 5230 examples with Y = \u22121 (does not cleave).\nWe use the logistic loss \u2113(\u03b8; (x, y)) = log(1 + exp(\u2212y\u03b8 \u22a4 x)). We compare the performance of different constraint sets \u0398 by taking\n\u0398 = \u03b8 \u2208 R d : a 1 \u03b8 1 + a 2 \u03b8 2 \u2264 r ,\nwhich is equivalent to elastic net regularization [52], while varying a 1 , a 2 , and r. We experiment with \u2113 1 -constraints (a 1 = 1, a 2 = 0) with r \u2208 {50, 100, 500, 1000, 5000}, \u2113 2 -constraints (a 1 = 0, a 2 = 1) with r \u2208 {5, 10, 50, 100, 500}, elastic net (a 1 = 1, a 2 = 10) with r \u2208 {100, 200, 1000, 2000, 10000}, our robust regularizer with \u03c1 \u2208 {100, 1000, 10000, 50000, 100000} and our robust regularizer coupled with the \u2113 1 -constraint (a 1 = 1, a 2 = 0) with r = 100. Though we use a convex surrogate (logistic loss), we measure performance of the classifiers using the 0-1 (misclassification) loss 1{sign(\u03b8 T x)y \u2264 0}. For validation, we perform 50 experiments, where in each experiment we randomly select 9/10 of the data to train the model, evaluating its performance on the held out fraction (test).\nWe plot results summarizing these experiments in Figure 3. The horizontal axis in each figure indexes our choice of regularization value (so \"Regularizer = 1\" for the \u2113 1 -constrained problem corresponds to r = 50). The figures show that the robustly regularized risk provides a different type of protection against overfitting than standard regularization or constraint techniques do: while other regularizers underperform in heavily constrained settings, the robustly regularized estimator \u03b8 rob n achieves low classification error for all values of \u03c1 (Figure 3(b)). Notably, even when coupled with a fairly stringent \u2113 1 -constraint (r = 100), robust regularization has perofrmance better than \u2113 1 except for large values r, especially on the rare label Y = +1 (Figure 3 ", "publication_ref": ["b30", "b51"], "figure_ref": ["fig_5", "fig_5", "fig_5"], "table_ref": []}, {"heading": "(d) and (f)).", "text": "We investigate the effects of the robust regularizer with a slightly different perspective in Figure 4, where we use \u0398 = {\u03b8 : \u03b8 1 \u2264 r} with r = 100 for the constraint set for each experiment. The horizontal axis indicates the tolerance \u03c1 we use in construction of the robust estimator \u03b8 rob n , where ERM means \u03c1 = 0. In Fig. 4(a), we plot the logistic risk R( \u03b8) = E[\u2113( \u03b8, (X, Y ))] for the train and test distribution. We also plot the upper confidence bound R n (\u03b8, P n ) in this plot, which certainly over-estimates the test risk-we hope to tighten this overestimate in future work. In Figure 4(b), we plot the misclassification error on train and test for different values of \u03c1, along with 2-standarderror intervals for the 50 runs. Figures 4(c) and (d) show the error rates restricted to examples from the uncommon (c) and common (d) classes. In Table 3 we give explicit error rates and logistic risk values for the different procedures. Due to the small size of the test dataset (n test = 659), the deviation across folds is somewhat large.    In this experiment, we see (roughly) that the ERM solutions achieve good performance on the common class (Y = \u22121) but sacrifice performance on the uncommon class. As we increase \u03c1, performance of the robust solution \u03b8 rob n on the rarer label Y = +1 improves (Fig. 4(c)), while the misclassification rate on the common class degrades a small (insignificant) amount (Fig. 4(d)); see also Table 3. This behavior is roughly what we might expect for the robust estimator: the poor performance of the ERM estimator \u03b8 erm n on the rare class induces (relatively) more variance, which the robust solution reduces by via improved classification performance on the rare (Y = +1) class. This occurs at little expense over the more common label Y = \u22121 so that overall performance improves by a small amount. We remark-but are unable to explain-that this improvement on classification error for the rare labels comes despite increases in logistic risk; while the average logistic loss increases, misclassification errors decrease.  4 gives the number of times a document is labeled as each of the four categories (so each document has about 1.18 associated classes). In this experiment, we expect the robust solution to outperform ERM on the rarer category (Economics), as the robustification ( 6) naturally upweights rarer (harder) instances, which disproportionally affect variance-as in the experiment on HIV-1 cleavage. For each category k \u2208 {1, 2, 3, 4}, we use the logistic loss \u2113(\u03b8 k ; (x, y)) = log(1 + exp(\u2212y\u03b8 \u22a4 k x)). For each binary classifier, we use the \u2113 1 constraint set \u0398 = \u03b8 \u2208 R d : \u03b8 1 \u2264 1000 . To evaluate performance on this multi-label problem, we use precision (ratio of the number of correct positive labels to the number classified as positive) and recall (ratio of the number of correct positive labels to the number of actual positive labels):\nprecision = 1 n n i=1 4 k=1 1{\u03b8 \u22a4 k x i \u2265 0, y i = 1} 4 k=1 1{\u03b8 \u22a4 k x i > 0} , recall = 1 n n i=1 4 k=1 1{\u03b8 \u22a4 k x i \u2265 0, y i = 1} 4 k=1 1 {y i = 1}\n.\nWe partition the data into ten equally-sized sub-samples and perform ten validation experiments, where in each experiment we use one of the ten subsets for fitting the logistic models and the remaining nine partitions as a test set to evaluate performance. In Figure 5, we summarize the results of our experiment averaged over the 10 runs, with 2standard error bars (computed across the folds). To facilitate comparison across the document categories, we give exact values of these averages in Tables 5 and 6. Both \u03b8 rob n and \u03b8 erm n have reasonably high precision across all categories, with increasing \u03c1 giving a mild improvement in  precision (from .93 \u00b1 .005 to .94 \u00b1 .005); see also Figure 5(a). On the other hand, we observe in Figure 5(d) that ERM has low recall (.69 on test) for the Economics category, which contains about 15% of documents. As we increase \u03c1 from 0 (ERM) to 10 5 , we see a smooth and substantial improvement in recall for this rarer category (without significant degradation in precision). This improvement in recall amounts to reducing variance in predictions on the rare class. We also note that while the robust solutions outperform ERM in classification performance for \u03c1 \u2264 10 5 , for very large \u03c1 = 10 6 \u2265 10n, the regularizing effects of robustness degrade the solution \u03b8 rob n . This precision and recall improvement comes in spite of the increase in the average binary logistic risk for each of the 4 classes, which we show in Figure 5a, which plots the average binary logistic loss (on train and test sets) averaged over the 4 categories as well as the upper confidence bound R n (\u03b8, P n ) as we vary \u03c1. The robust regularization effects reducing variance appear to improve the performance of the binary logistic loss as a surrogate for true misclassification error.  ", "publication_ref": [], "figure_ref": ["fig_7", "fig_7", "fig_7", "fig_7", "fig_7", "fig_7", "fig_9", "fig_9", "fig_9", "fig_9"], "table_ref": ["tab_3", "tab_3", "tab_4", "tab_5"]}, {"heading": "Summary", "text": "We have seen through multiple examples that robustification-our convex surrogate for variance regularization-is an effective tool in a number of applications. As we heuristically expect, variancebased regularization (robust regularization) yields predictors with better performance on \"hard\" instances, or subsets of the problem that induce higher variance, such as classes with relatively few training examples in classification problems. The robust regularization \u03c1 gives a principled knob for tuning performance to trade between variance (uniform or across-the-board performance) and-sometimes-absolute performance.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "In this paper, we have developed theoretical results for robust regularization (6) that apply to general stochastic optimization and learning problems problems. The examples we describe in Section 3 illustrate our expectation that the robust solution \u03b8 rob n should have good performance in cases in which Var(\u2113(\u03b8 \u22c6 ; X)) is small (recall also Theorems 3 and 4). Identifying the separation between the performance empirical risk minimization and related estimators and that of the robustly-regularized estimators-as well as variance-regularized estimates-we consider more generally remains a challenge. We hope that this paper inspires work in this direction in machine learning and statistics, and more broadly, torward considering distributionally robust problems. Part of this is likely to come from making rigorous our empirical observations (Section 5) that robust regularization improves performance on \"hard\" instances without sacrificing performance on easier cases.\nOur understanding of so-called \"fast rates\" for stochastic optimization problems, while considering robustness, is also limited. For empirical risk minimization, fast rates of convergence hold under conditions in which the the gap R(\u03b8) \u2212 R(\u03b8 \u22c6 ) controls the variance of the excess loss \u2113(\u03b8, X) \u2212 \u2113(\u03b8 \u22c6 , X) [cf. 32,3,10,4], which usually requires some type of uniform convexity assump-tion. These bounds typically follow from localization guarantees [3,Section 5] \non the function class {x \u2192 \u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u22c6 , x) | \u03b8 \u2208 \u0398} .\nWhile in Section 4.1, we show that the robust estimate \u03b8 rob n enjoys faster rates of convergence under growth conditions analogous to uniform convexity of the risk, as Var(\u2113(\u03b8; X) \u2212 \u2113(\u03b8 \u22c6 ; X)) = Var(\u2113(\u03b8; X)), it is not clear how to directly connect these guarantees to results of the form in Theorems 3 and 4. We leave investigation of these topics to future work.\nThe last point of our discussion is to revisit Theorem 4, which provides a guarantee for robustly regularized estimators based on localized Rademacher complexities. An investigation of our proof shows that our derivation proceeds by considering the complexity of self-normalized classes of functions of the form\nG r = r E[f 2 ] \u2228 r f | f \u2208 F .\nIn contrast, the analogous result of Bartlett et al. [3,Thereom 3.3] for empirical risk minimization considers the complexity of classes of functions of the form\nG r = r E[f 2 ] \u2228 r f | f \u2208 F .\nThe latter class normalizes functions f by E[f 2 ]-a type of self-normalization that arises in the computation of pivotal (asymptotically independent of the underlying distribution) statistics. While this choice prima facie is just a step in our proof, the robust objective R n (\u03b8, P n ) defined in Eq. ( 4) is an empirical likelihood upper confidence bound on the optimal population risk [see also 20]. One of the important characteristics of empirical likelihood confidence bounds is that they are self-normalizing and yield pivotal statistics [38]. Investigating such self-normalization in complexity guarantees seems likely to yield fruitful insights.", "publication_ref": ["b31", "b2", "b9", "b3", "b2", "b2", "b37"], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "A Proof of Theorem 1", "text": "The theorem is immediate if s n = 0 or \u03c3 2 = 0, as in this case sup P :\nD \u03c6 (P || Pn)\u2264\u03c1/n E P [Z] = E Pn [Z] = E[Z].\nIn what follows, we will thus assume that \u03c3 2 , s 2 n > 0. We recall the maximization problem (8), which is maximize\np n i=1 p i z i subject to p \u2208 P n = p \u2208 R n + : 1 2 np \u2212 1 2 2 \u2264 \u03c1, 1, p = 1 ,\nand the solution criterion (9), which guarantees that the maximizing value of problem ( 8)\nis z + 2\u03c1s 2 n /n whenever 2\u03c1 z i \u2212 z ns 2 n \u2265 \u22121.\nLetting z = Z, then under the conditions of the theorem, we have |z i \u2212 z| \u2264 M , and to satisfy inequality ( 9) it is certainly sufficient that 2\u03c1\nM 2 ns 2 n \u2264 1, or n \u2265 2\u03c1M 2 s 2 n , or s 2 n \u2265 2\u03c1M 2 n .(30)\nConversely, suppose that s 2 n < 2\u03c1M 2 n . Then we have 2\u03c1s 2\nn n < 4\u03c1 2 M 2 n 2 , which in turn implies that sup p\u2208Pn p, z \u2265 1 n 1, z + 2\u03c1s 2 n n \u2212 2M \u03c1 n + .\nCombining this inequality with the condition (30) for the exact expansion to hold yields the twosided variance bounds (10).\nWe now turn to showing the high-probability exact expansion (11), which occurs whenever the sample variance is large enough by expression (30). To that end, we show that s 2 n is bounded from below with high probability. Define the event\nE n := s 2 n \u2265 3 64 \u03c3 2 , and let n \u2265 4M 2 \u03c3 2 max {2\u03c3, 11}. Then, on event E n we have n \u2265 44\u03c1M 2 \u03c3 2 \u2265 2\u03c1M 2 s 2 n\n, so that the sufficient condition (30) holds and expression (11) follows. We now argue that the event E n has high probability via the following lemma, which is an application of concentration inequalities for convex functions coupled with careful estimates of the expectation of standard deviations.\nLemma A.1. Let Z i be i.i.d. random variables taking values in [M 0 , M 1 ] with M = M 1 \u2212 M 0 , and let s 2 n = 1 n n i=1 Z 2 i \u2212 1 n n i=1 Z i 2 . Let c n = 1 + 7 4n + 3 n 2 .\nFor all t \u2265 0, we have\nP s n \u2265 Es 2 n + t \u2228 P s n \u2264 Es 2 n \u2212 c n M 2 n \u2212 t \u2264 exp \u2212 nt 2 2M 2 .\nThe proof of the lemma is involved, but the lemma yields a quick proof of the theorem that we now provide. (See Section A.1 for a proof.)\nLet \u03c3 2 = Var(Z) and note that E[s\n2 n ] = (1 \u2212 1 n )\u03c3 2 . Set t = \u221a 3\n4 \u03c3 and note that c n < 3 2 if n \u2265 5. Then, since n \u2265 8M 2 \u03c3 by hypothesis, we have that\n\u03c3 1 \u2212 n \u22121 \u2212 c n M 2 n \u2212 t \u2265 2 \u221a 5 5 \u03c3 \u2212 3M 2 2n \u2212 t > 7 10 \u03c3 \u2212 t > \u221a 3 8 \u03c3. Lemma A.1 implies that s n \u2265 \u03c3 \u221a 1 \u2212 n \u22121 \u2212 cnM 2 n\n\u2212 t with probability at least 1 \u2212 e \u2212nt 2 /2M 2 = 1 \u2212 e \u22123n\u03c3 2 /32M 2 , which gives the theorem.", "publication_ref": ["b8", "b9", "b10", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Proof of Lemma A.1", "text": "We use three technical lemmas in the proof of this lemma. Then for all t \u2265 0,\nP(f (Z 1:n ) \u2265 E[f (Z 1:n )] + t) \u2228 P(f (Z 1:n ) \u2264 E[f (Z 1:n )] \u2212 t) \u2264 exp \u2212 t 2 2L 2 (b \u2212 a) 2 . The function R n \u220b z \u2192 (I \u2212 (1/n)11 \u22a4 )z 2 is 1-Lipschitz with respect to the Euclidean norm, so Lemma A.2 implies P Var Pn (Z) \u2265 E[ Var Pn (Z)] + t \u2228 P Var Pn (Z) \u2264 E[ Var Pn (Z)] \u2212 t \u2264 exp \u2212 nt 2 2M 2 . As E[Var Pn (Z) 1 2 ] \u2264 E[Var Pn (Z)] 1 2 = (1 \u2212 1/n)Var(Z)\n, this yields the first part of the first inequality of the lemma. We must, however, also lower bound E[Var Pn (Z) Lemma A.3. Let Y i be random variables with finite 4th moment and assume that Cov(Y 2 i , Y 2 j ) \u2264 \u03c3 4 for all pairs i, j. Then we have the following inequalities:\nE 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 \u221a n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 n n i=1 E[Y 2 i ] (31a) E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 n n i=1 E[Y 2 i ] .(31b)\nLemma A.4. Let Z 1 , . . . , Z n be i.i.d variables with finite fourth moment.\nLet Y i = Z i \u2212 1 n n j=1 Z j . Then E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 n E[(Z \u2212 E[Z]) 4 ] Var(Z) + 7 + 12/n n Var(Z) If max j Z j \u2212 min j Z j \u2264 C with probability 1, then for the constant c n = 1 + 7 4n + 3 n 2 , we have E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 c n C 2 n .\nWe provide the proof of Lemmas A. 3 \nY i = Z i \u2212 1 n n j=1 Z j , so that s 2 n = 1 n n i=1 Y 2 i ,", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Lemma A.3", "text": "We first prove the claim (31a). To see this, we use that\ninf \u03bb\u22650 a 2 2\u03bb + \u03bb 2 = \u221a a 2 = |a|,\nand taking derivatives yields that for all \u03bb \u2032 \u2265 0,\na 2 2\u03bb + \u03bb 2 \u2265 a 2 2\u03bb \u2032 + \u03bb \u2032 2 \u2212 a 2 2\u03bb \u2032 2 \u2212 1 2 (\u03bb \u2212 \u03bb \u2032 ). By setting \u03bb n = 1 n n i=1 Y 2 i , we thus have for any \u03bb \u2265 0 that E 1 n n i=1 Y 2 i 1 2 = E n i=1 Y 2 i 2n\u03bb n + \u03bb n 2 \u2265 E n i=1 Y 2 i 2n\u03bb + \u03bb 2 + E 1 2 \u2212 n i=1 Y 2 i 2n\u03bb 2 (\u03bb n \u2212 \u03bb) . Now we take \u03bb = 1 n n i=1 E[Y 2 i ]\n, and we apply the Cauchy-Schwarz inequality to obtain\nE 1 n n i=1 Y 2 i 1 2 (32) \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 2\u03bb 2 E \uf8ee \uf8f0 1 n n i=1 (Y 2 i \u2212 E[Y 2 i ]) 2 \uf8f9 \uf8fb 1 2 E \uf8ee \uf8ef \uf8f0 \uf8eb \uf8ed 1 n n i=1 Y 2 i 1 2 \u2212 1 n n i=1 E[Y 2 i ] 1 2 \uf8f6 \uf8f8 2 \uf8f9 \uf8fa \uf8fb 1 2\n.\nWe control each of these quantities in turn. First, our assumption that Cov(\nY 2 i , Y 2 j ) \u2264 \u03c3 4 implies that E \uf8ee \uf8f0 1 n n i=1 (Y 2 i \u2212 E[Y 2 i ]) 2 \uf8f9 \uf8fb = 1 n 2 n i=1 Var(Y 2 i ) + 1 n 2 i =j E (Y 2 i \u2212 E[Y 2 i ])(Y 2 j \u2212 E[Y 2 j ]) \u2264 1 n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 . The triangle inequality implies that E[((n \u22121 n i=1 Y 2 i ) 1 2 \u2212(n \u22121 n i=1 E[Y 2 i ]) 1 2 ) 2 ] 1 2 \u2264 2\u03bb = 2 1 n n i=1 E[Y 2 i ]\n, so that substituting in inequality (32) we have\nE 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 \u03bb \u221a n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 2\n. This is the bound (31a). Now we give the sharper result. We have\nE \uf8ee \uf8ef \uf8f0 \uf8eb \uf8ed 1 n n i=1 Y 2 i 1 2 \u2212 1 n n i=1 E[Y 2 i ] 1 2 \uf8f6 \uf8f8 2 \uf8f9 \uf8fa \uf8fb = 2 n n i=1 E[Y 2 i ] \u2212 2 1 n n i=1 E[Y 2 i ] 1 2 E 1 n n i=1 Y 2 i 1 2 \u2264 2 \u221a n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 2\n, where we have used the bound (31a). Returning to inequality (32), we obtain the second claim (31b) of the lemma.\nA.3 Proof of Lemma A.4\nAs Y i = Z i \u2212 1 n n j=1 Z j ,\nit is no loss of generality to assume that E[Z] = 0, as Y i is shift-invariant. The lemma follows immediately from Lemma A.3 once we prove the claims that\nVar(Y 2 1 ) \u2264 n \u2212 1 n E[Z 4 ] + 6 n Var(Z) 2(33a)\nand\nE[(Y 2 1 \u2212 E[Y 2 1 ])(Y 2 1 \u2212 E[Y 2 1 ])] \u2264 1 + 12/n n 2 Var(Z) 2 . (33b\n)\nNote that E[Y 2 i ] = E[(Z i \u2212 1 n n j=1 Z j ) 2 ] = n\u22121 n Var(Z).\nWe begin with the first inequality (33a). We have\nVar(Y 2 1 ) = E[Y 4 1 ] \u2212 E[Y 2 1 ] 2 = E[(Z 1 \u2212 Z n ) 4 ] \u2212 (n \u2212 1) 2 n 2 Var(Z) 2 ,\nwhere we use the shorthand Z n = 1 n n j=1 Z j . Expanding the first quantity, the fourth moment of Y 1 , we have\nE[(Z 1 \u2212 Z n ) 4 ] = E[Z 4 ] \u2212 4E[Z 3 1 Z n ] + 6E[Z 2 1 Z 2 n ] \u2212 4E[Z 1 Z 3 n ] + E[Z 4 n ]. Using that E[Z 1 Z 3 n ] = E[Z 4 ]/n 3 + 3(n\u22121) n 3 Var(Z) 2 , E[Z 2 1 Z 2 n ] = 1 n 2 E[Z 4 ] + n\u22121 n 2 Var(Z) 2 , and E[Z 4 n ] \u2264 1 n 3 E[Z 4 ] + 3 n 2 Var(Z) 2 , we obtain E[(Z 1 \u2212 Z n ) 4 ] \u2264 1 \u2212 4 n + 6 n 2 \u2212 3 n 3 E[Z 4 ] + 6 n \u2212 1 n 2 \u2212 4 3n \u2212 3 n 3 + 3 n 2 Var(Z) 2 \u2264 n \u2212 1 n E[Z 4 ] + 6 n Var(Z) 2 ,\nwhich is the result (33a).\nFor the second claim (33b), we expand\nY 2 i \u2212 E[Y 2 i ] = Z 2 i + Z 2 n \u2212 2Z i Z n \u2212 n \u2212 1 n Var(Z).\nThe right-hand-side of Eq. (33b) is thus\nE Z 2 1 \u2212 Var(Z) + n \u22121 Var(Z) + Z 2 n \u2212 2Z 1 Z n Z 2 2 \u2212 Var(Z) + n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n = 2E (Z 2 1 \u2212 Var(Z)) n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n + E n \u22121 Var(Z) + Z 2 n \u2212 2Z 1 Z n n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n .(34)\nThe first term above satisfies\nE (Z 2 1 \u2212 Var(Z)) n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n = E (Z 2 1 \u2212 Var(Z))Z 2 n = Var(Z 2 ) n 2 .\nThe latter term ( 34) is equal to\n1 n 2 Var(Z) 2 + 2 n Var(Z)E[Z 2 n ] \u2212 4 n Var(Z)E[Z 1 Z n ] + E[Z 4 n ] \u2212 4E[Z 1 Z 3 n ] + 4E[Z 1 Z 2 Z 2 n ] = \u2212 1 n 2 Var(Z) 2 + E[Z 4 n ] \u2212 4E[Z 1 Z 3 n ] + 4E[Z 1 Z 2 Z 2 n ] = Var(Z 2 n ) + 4 E[Z 1 Z 2 Z 2 n ] \u2212 E[Z 1 Z 3 n ] .\nAs earlier, we have that\nE[Z 1 Z 3 n ] = E[Z 4 ]/n 3 + 3(n\u22121) n 3 Var(Z) 2 , E[Z 1 Z 2 Z 2 n ] = 2 n 2 E[Z 2 1 Z 2 2 ] = 2 n 2 Var(Z) 2 , and E[Z 4 n ] \u2264 1 n 3 E[Z 4 ] + 3 n 2 Var(Z) 2\n, so that we can bound the last term (34) by\n\u2212 3 n 3 E[Z 4 ] + 3 n 2 Var(Z) 2 + 4 3 \u2212 n n 3 Var(Z) 2 \u2264 12 \u2212 n n 3 Var(Z) 2 .", "publication_ref": ["b31", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "This yields the claim (33b).", "text": "The final inequality, when Z i are bounded, follows immediately upon noticing that E[(Z \u2212 E[Z]) 4 ] \u2264 C 2 Var(Z) when Z takes values in an interval of width at most C, and that moreover, in this circumstance, Var(Z) \u2264 C 2 4 .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Proof of Theorem 2", "text": "Our starting point is to recall from inequality (30) in the proof of Theorem 1 that for each f \u2208 F, the empirical variance equality (12) holds if n \u2265 4\u03c1M 2 Var Pn (f ) . As a consequence, Theorem 2 will follow if we can provide a uniform lower bound on the sample variances Var Pn (f ) that holds with high enough probability. We use C to denote a universal constant whose value may change from line to line. Noting that Var Pn (f 2 , we proceed in two parts. First, we give a lower bound for\n) = E Pn (f \u2212 E[f ]) 2 \u2212 (E Pn (f \u2212 E[f ]))\nE Pn (f \u2212 E[f ]) 2 . Lemma B.1. Let F be a collection of bounded functions f : X \u2192 [M 0 , M 1 ] with M := M 1 \u2212 M 0 .\nThen, with probability at least 1 \u2212 e \u2212t , for every f \u2208 F\nVar(f ) \u2264 2E Pn (f \u2212 E[f ]) 2 + C R sup n (F) 2 log 3 (nM ) + M 2 n (t + log log n) .\nProof We follow the arguments of Srebro et al. [43] and Bousquet [13,Thm. 6.1]. For x 1 , . . . , x n \u2208 X , let\nF n,r := f \u2212 E[f ] \u2208 F | E Pn [(f \u2212 E[f ]) 2 ] \u2264 r ,\nwhere P n is the empirical measure on x 1 , . . . , x n . Let \u03c8 sup n be a sub-root upper bound on the worst-case Rademacher complexity\n\u03c8 sup n (r) \u2265 R sup n (F n,r ),\nwhere implicitly in the right hand side we take the supremum over x 1 , . . . , x n definining F n,r as well. The function t \u2192 t 2 has 2-Lipschitz derivatives, so we may apply Srebro et al. [43,Lemma 2.2] to obtain\nR sup n (F 2 n,r ) \u2264 C \u221a rR sup n (F) log 3 2 n (35\n)\nwhere we recall the notation that G 2 = {g 2 | g \u2208 G} for any function class G. Thus we may take\n\u03c8 sup n (r) = C \u221a rR sup n (F) log 3 2\nn, which has fixed point r sup n = C 2 R sup n (F) 2 log 3 n. Since f 2 \u2265 0, Theorem 6.1 of Bousquet [13] yields that for all f \u2208 F,\nE(f \u2212 E[f ]) 2 \u2264 2E Pn (f \u2212 E[f ]) 2 + C R sup n (F) 2 log 3 nM + M 2 n (t + log log n)\nwith probability at least 1 \u2212 e \u2212t .\nNext, we give an upper bound for (E Pn (f \u2212 E[f ])) 2 . We use the following version of Talagrand's inequality due to Bousquet [12,14]. (See also Bartlett et \n{E[f ] \u2212 E Pn [f ]} \u2264 inf \u03b1>0 2(1 + \u03b1)E[R n (F)] + 2rt n + t n (b \u2212 a) 1 3 + 1 \u03b1 .\nThe same statement holds with sup f \u2208F (E Pn [f ]\u2212E[f ]) replacing the left-hand side of the inequalities.\nApplying Lemma B.2 and letting \u03b1 = 1 2 , with probability at least 1 \u2212 2e \u2212t\n|E Pn [f ] \u2212 E[f ]| \u2264 3E[R n (F)] + 2M 2t n\nholds for all f \u2208 F. Combining the above display with Lemma B.1, we obtain the desired result.", "publication_ref": ["b29", "b1", "b42", "b12", "b42", "b12", "b11", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "C Proof of Theorem 3", "text": "Before proving the theorem proper, we state a technical lemma that provides uniform Bernstein-like bounds for the class F using empirical \u2113 \u221e -covering numbers.\nLemma C.1 (Maurer and Pontil [33], Theorem 6). Let n \u2265 8M 2 t and t \u2265 log 12. Then with probability at least 1 \u2212 6N \u221e (F, \u01eb, 2n)e \u2212t , we have\nE[f ] \u2264 E Pn [f ] + 3 2Var Pn (f )t n + 15M t n + 2 1 + 2 2t n \u01eb (36\n)\nfor all f \u2208 F.\nWe return to the proof of Theorem 3. Let E 1 denote that the event that the inequalities (36) hold. Then on E 1 hold, uniformly over f \u2208 F we have\nE[f ] \u2264 E Pn [f ] + 18Var Pn (f (X))t n + 15M t n + 2 1 + 2 2t n \u01eb (i) \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 2\u03c1Var Pn (f (X)) n \u2212 \uf8eb \uf8ed 2\u03c1Var Pn (f (X)) n \u2212 2M \u03c1 n \uf8f6 \uf8f8 + + 5M \u03c1 3n + 2 1 + 2 2t n \u01eb \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 11 3 M \u03c1 n + 2 1 + 2 2t n \u01eb for all f \u2208 F,(37)\nwhere inequality (i) follows from the bounds (10) in Theorem 1 and the fact that \u03c1 \u2265 9t by assumption. This gives the first result (15). For the second result ( 16), we recall that f \u2208 argmin f \u2208F sup P {E P [f (X)] : D \u03c6 (P || P n ) \u2264 \u03c1 n }, and we bound the supremum term in expression (37). First, we note that because f minimizes the supremum term in expression (37), we have\nE[ f ] \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 11M \u03c1 3n + 2 1 + 2 2t n \u01eb for all f \u2208 F. Now fix f \u2208 F.\nAs the function f is fixed, by Bernstein's inequality, we have\nE Pn [f ] \u2264 E[f ] + 2Var(f )t n + 2M t 3n\nwith probability at least 1 \u2212 e \u2212t . Similarly, we have by Lemma A.1 that\nVar Pn (f ) \u2264 1 \u2212 n \u22121 Var(f ) + 2tM 2 n\nwith probability at least 1 \u2212 e \u2212t . That is, for any fixed f \u2208 F, we have with probability at least 1 \u2212 2e \u2212t that sup\nP :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] (i) \u2264 E Pn [f ] + 2\u03c1Var Pn (f ) n \u2264 E[f ] + 2Var(f )t n + 2M 3n t + 2\u03c1Var(f ) n + 2 M 2 \u03c1t n (ii) \u2264 E[f ] + 2 2Var(f )\u03c1 n + 8 3 M \u03c1 n ,\nwhere inequality (i) follows from the uniform upper bound (10) of Theorem 1 and inequality (ii) from our assumption that \u03c1 \u2265 t. Substituting this expression into our earlier bound (37) yields that for any f \u2208 F, with probability at least 1 \u2212 2(3N \u221e (F, \u01eb, 2n) + 1)e \u2212t ,\nwe have\nE[ f (X)] \u2264 E[f (X)] + 2 2\u03c1Var(f (X)) n + 19 3 M \u03c1 n + 2 1 + 2 2t n \u01eb.\nLemma D.3. Let F be a collection of bounded functions f : X \u2192 [0, M ] satisfying the localization inequality (20) for some sub-root function \u03c8 n (\u2022) with root r \u22c6 n . Let \u03b7 > 0. Then, with probability at least 1 \u2212 e \u2212t , for every f \u2208 F\nE[f 2 ] \u2264 E Pn [f 2 ] + 1 \u03b7 E Pn [f 2 ] + 72M 2 (1 + \u03b7)r \u22c6 n + M t n 4 + 7 3 M .\nAlso, with probability at least 1 \u2212 e \u2212t , for every f \u2208 F\nE Pn [f 2 ] \u2264 E[f 2 ] + \u03b7 1 + \u03b7 E[f 2 ] + 72M 2 (1 + \u03b7)r \u22c6 n + M t n 4 + 7 3 M .\nSee Section D.3 for the proof. Now, we make two additional pieces of shorthand notation. Let\nV n = 4((2e + 84M )B n + 36r \u22c6 n ).\nThen, Lemma D.2 implies that\nE[f ] \u2264 E Pn [f ] + V n E[f 2 ] + 6r \u22c6 n + 14M B n\nwith probability at least 1 \u2212 e \u2212t . Applying Lemma D.3 to this bound with the choice \u03b7 = 1 immediately yields that\nE[f ] \u2264 E Pn [f ] + 2V n E Pn [f 2 ] + 144M 2 V n r \u22c6 n + 7V n M max{M, 1}t/n + 6r \u22c6 n + 14M B n \u2264 E Pn [f ] + 2V n E Pn [f 2 ] + 12M V n r \u22c6 n + 7 max{M, 1} M t n + 6r \u22c6 n + 14M B n\nfor all f \u2208 F with probability at least 1 \u2212 2e \u2212t . Subtracting and adding (E Pn [f ]) 2 to the second term, we have\n2V n E Pn [f 2 ] = 2V n Var Pn (f ) + 2V n E Pn [f ] 2 \u2264 2V n Var Pn (f ) + 2V n E Pn [f ],\nwhere we have used that f \u2265 0. We thus obtain\nE[f ] \u2264 1 + 2V n E Pn [f ] + 2V n Var Pn (f ) + 12M V n r \u22c6 n + 7 max{M, 1} M t n + 6r \u22c6 n + 14M B n \u2264 1 + 2V n E Pn [f ] + 2V n Var Pn (f ) + 6M V n + 6M r \u22c6 n + 7 max{M, 1}t M n + 6r \u22c6 n + 14M B n ,\nwhere the second inequality follows because\n\u221a ab \u2264 1 2 a + 1 2 b for a, b \u2265 0. Recalling the bound (21), which implies \u03c1 \u2265 nV n , \u03c1 \u2265 n(r \u22c6 n + 7 max{M,1}tM n\n), and \u03c1/n \u2265 6r \u22c6 n + 14M B n , we obtain that for all f \u2208 F with probability at least 1 \u2212 2e \u2212t . This is the first result (22).\nE[f ] \u2264 1 + 2\u03c1 n E Pn [f ] + 2\u03c1 n Var Pn (f ) + 13M \u03c1 n .\nTo show the second result, we simply apply Bernstein's inequality and the concentration inequalities for the standard deviation in Lemma A.1. For any fixed f \u2208 F, by Bernstein's inequality, we have\nE Pn [f ] \u2264 E[f ] + 2tVar(f ) n + 2M t 3n\nwith probability at least 1 \u2212 e \u2212t . From Lemma A.1, we have\nVar Pn (f ) \u2264 1 \u2212 n \u22121 Var(f ) + 2tM 2 n\nwith probability at least 1 \u2212 e \u2212t .\nWe thus obtain that for any fixed f , sup\nP :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f ] \u2264 E Pn [f ] + 2\u03c1 n Var Pn (f ) \u2264 E[f ] + 2t n Var(f ) + 2\u03c1 n Var(f ) + 2M \u221a \u03c1t n + 2M t 3n\nwith probability at least 1 \u2212 2e \u2212t . Noting that \u03c1 \u2265 45M t by assumption (21), so \u221a \u03c1 + \u221a t \u2264 46\u03c1/45 + 45t \u2264 91\u03c1/45 and that always 2 \u221a \u03c1t \u2264 3\u03c1 + 1 3 t, we have that with probability at least 1 \u2212 2e \u2212t that sup\nP :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f ] \u2264 E[f ] + 91\u03c1 45n Var(f ) + 3M \u03c1 n + M t n .\nNoting that we could take f to minimize the right hand side of the preceding expression and that f minimizes sup P :D \u03c6 (P || Pn)\u2264\u03c1/n E P [f ], we have the result (23).", "publication_ref": ["b14", "b36", "b36", "b19", "b21", "b20", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "D.1 Proof of Lemma D.1", "text": "We first show the claim for g \u2208 F centered = {f \u2212 E[f ] : f \u2208 F}. To see the claim for g \u2208 F centered , let us fix L \u2208 N to be chosen later, and for l = 1, . . . , L \u2212 1 define the classes\nF l := g \u2208 F centered : e \u2212l r < E[g 2 ] \u2264 e \u2212(l\u22121) r , F L := g \u2208 F centered : E[g 2 ] \u2264 e \u2212L r\nso that F centered = \u222a L l=1 F l . Let z > 0 be such that t \u2264 z. Applying Lemma B.2 (with the choice \u03b1 = 1\n2 ) to F l for each l = 1, . . . , L \u2212 1, we have with probability at least 1 \u2212 e \u2212t , for every g \u2208 F l\nE[g] \u2264 E Pn [g] + 2te \u2212(l\u22121) r n + 3E[R n (F l )] + 5M t n \u2264 E Pn [g] + 2et n E[g 2 ] + 3E[R n (F l )] + 5M t n\nwhere in the last line we have used e \u2212l r \u2264 E[g 2 ] for g \u2208 F l . Similarly, applying Lemma B.2 to F L , then with probability at least 1 \u2212 e \u2212t , for every g \u2208 F L\nE[g] \u2264 E Pn [g] + 2te \u2212L r n + 3E[R n (F L )] + 5M t n \u2264 E Pn [g] + 2et n E[g 2 ] + 2te \u2212L r n + 3E[R n (F L )] + 5M t n .\nTaking a union bound, we have with probability at least 1 \u2212 Le \u2212t , for every g \u2208 F centered\nE[g] \u2264 E Pn [g] + 2et n E[g 2 ] + 3E[R n (F centered )] + 5M t n + 2te \u2212L r n .\nNoting that E[R n (F centered )] \u2264 2E[R n (F)] by Jensen's inequality, we take L = log rn M 2 t and map t to t + log L to obtain the lemma. The case when the roles of E[f ] and E Pn [f ] are reversed follows similarly.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 Proof of Lemma D.2", "text": "Let r \u2265 r \u22c6 n be an arbitrary but fixed value to be choosen later. Using this r, define the selfnormalized class of functions\nG r := r E[f 2 ] \u2228 r f : f \u2208 F \u2286 cf : f \u2208 F, E[c 2 f 2 ] \u2264 r, c \u2208 [0, 1] .\nFrom the truncation by r, we have E[g 2 ] \u2264 r for all g \u2208 G r . Lemma D.1 implies that with probability at least 1 \u2212 e \u2212t , uniformly over g \u2208 G r\nE[g] \u2264 E Pn [g] + 2e n E[g 2 ] t + log log n t + 6E[R n (G r )] + 7M n t + log log n t .(38)\nUsing the sub-root property of \u03c8 n and that \u03c8 n (r \u22c6 n ) = r \u22c6 n , we have the inequality\n\u03c8 n (r) = \u221a r\u03c8 n (r)/ \u221a r \u2264 \u221a r\u03c8 n (r \u22c6 n )/ r \u22c6 n = rr \u22c6 n for any r \u2265 r \u22c6 n , so E[R n G r ] \u2264 E[R n cf : f \u2208 F, E[c 2 f 2 ] \u2264 r, c \u2208 [0, 1] ] \u2264 \u03c8 n (r) \u2264 rr \u22c6 n\nUsing this upper bound in Eq. (38) and recalling the notation B n = 1 n t + log log n t , we get\nE[g] \u2264 E Pn [g] + 2eB n E[g 2 ] + 6 r \u22c6 n r + 7M B n .(39)\nNow, we return to choose the value r to optimize the bound (39). let r be the largest solution to 6 \u221a r \u22c6 n r + 7M B n = 6r. The following elementary lemma provides a bound on r. For each g \u2208 G r , there exists f \u2208 F such that g\n= r E[f 2 ]\u2228r f . If E[f 2 ]\n\u2264 r, we have g = f and the bound ( 39) yields\nE[f ] \u2264 E Pn [f ] + 2eB n E[f 2 ] + 6r \u22c6 n + 14M B n .\nIf E[f 2 ] > r, rescaling g in the bound (39) and using the choice 6r = 6\n\u221a r \u22c6 n r + 7M B n yields E[f ] \u2264 E Pn [f ] + 2eB n E[f 2 ] + 6 rE[f 2 ] \u2264 E Pn [f ] + 2eB n E[f 2 ] + 6 (r \u22c6 n + 7M B n /3)E[f 2 ] instead. Combining the cases E[f 2 ] \u2276 r, we conclude that for all f \u2208 F, E[f ] \u2264 E Pn [f ] + 2eB n + 6 r \u22c6 n + 7M B n /3 E[f 2 ] + 6r \u22c6 n + 14M B n\nwith probability at least 1 \u2212 e \u2212t . Similarly, we can reverse the roles of E[f ] and E Pn [f ] to get the second result.", "publication_ref": ["b37", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "D.3 Proof of Lemma D.3", "text": "We frequently use the Rademacher contraction principle [28,Thm. 4.12] in what follows.\nLemma D.5. Let \u03c6 : R \u2192 R be L-Lipschitz. Then, for every class G\nE \u01eb [R n (\u03c6 \u2022 G)] \u2264 LE \u01eb [R n (G)]\nwhere \u03c6 \u2022 G = {\u03c6 \u2022 f : f \u2208 G}.\nAs in Section D.2, define the self-normalized functions in F\nG r := r E[f 2 ] \u2228 r f : f \u2208 F \u2286 cf : f \u2208 F, E[c 2 f 2 ] \u2264 r, c \u2208 [0, 1]\nwhere r \u2265 r \u22c6 n will be choosen later. Let G 2 r = {g 2 : g \u2208 G r }. From the truncation by r, we have that for all \ng 2 \u2208 G 2 r , Var(g 2 ) \u2264 E[g 4 ] \u2264 M 2 E[g 2 ] \u2264 M 2 r. Let c 1 = 3 and c 2 =\ng \u2208 G r E[g 2 ] \u2264 E Pn [g 2 ] + c 1 E[R n (G 2 r )] + M 2rt n + c 2 M 2 t n (a) \u2264 E Pn [g 2 ] + 2c 1 M E[R n (G r )] + M 2rt n + c 2 M 2 t n (b) \u2264 E Pn [g 2 ] + 2c 1 M rr \u22c6 n + M 2rt n + c 2 M 2 t n (40\n)\nwhere in step (a) we used the contraction principle (Lemma D.5) and that x \u2192 x 2 is 2M -Lipschitz on [\u2212M, M ], and in step (b), we used that \u03c8 n (r) \u2264 \u221a rr \u22c6 n as in the proof of Lemma D.2 in Section D.2.\nLet\nA = 2c 1 M \u221a r \u22c6 n + M 2t n and D = c 2 M 2 t n .\nFor any fixed K > 1, choose r to be the largest solution to A \u221a r + D = r K so that the bound ( 40) becomes\nE[g 2 ] \u2264 E Pn [g 2 ] + r D .\nFrom Lemma D.4, we have (40) and using the upper bound on r, we obtain\nK 2 A 2 \u2264 r \u2264 K 2 A 2 + 2KD and in particular, r \u2265 K 2 A 2 \u2265 r \u22c6 n . For each g \u2208 G r , there exists f \u2208 F such that g = r E[f 2 ]\u2228r f . If E[f 2 ] \u2264 r, rescaling the inequality\nE[f 2 ] \u2264 E Pn [f 2 ] + r K \u2264 E Pn [f 2 ] + KA 2 + 2D. If E[f 2 ] > r, rescaling instead yields E[f 2 ] \u2264 E Pn [f 2 ] + E[f 2 ] K .\nCombining the two cases, we obtain\nE[f 2 ] \u2264 K K \u2212 1 E Pn [f 2 ] + KA 2 + 2D. Noting that A \u2264 2 4c 2 1 M 2 r \u22c6 n + 2 M 2 t n\nby convexity, we have the first result once we replace K with \u03b7 = K \u2212 1 > 0. The second result similarly follows by reversing the roles of E[f ] and E Pn [f ] in the above argument.", "publication_ref": ["b27", "b39"], "figure_ref": [], "table_ref": []}, {"heading": "E Proof of Theorem 5", "text": "Recall our shorthand notation that \u03c0(\u03b8) = argmin \u03b8 * \u2208S\u22c6 { \u03b8 \u2212 \u03b8 * 2 } denotes the Euclidean projection of \u03b8 onto S \u22c6 , which is a closed convex set. Define also the localized empirical deviation function\n\u2206 n (\u03b8) := E [\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)] \u2212 E Pn [\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)] .(41)\nWe begin with the following\nClaim E.1. If S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 , then sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) + 2\u03c1 n Var Pn (\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)) \u2265 \u01eb.(42)\nDeferring the proof of the claim, let us prove the theorem. First, the growth condition (26) shows that\nS 2\u01eb \u22c6 \u2282 \u03b8 \u2208 \u0398 : \u03b8 \u2212 \u03c0(\u03b8) 2 \u2264 2\u01eb \u03bb 1 \u03b3 = \u03b8 \u2208 \u0398 : dist(\u03b8, S \u22c6 ) \u2264 2\u01eb \u03bb 1 \u03b3\n.\nTherefore, we have for all \u03b8 \u2208 S 2\u01eb \u22c6 that Var Pn (\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X))\n\u2264 L 2 dist(\u03b8, S \u22c6 ) 2 \u2264 L 2 2\u01eb \u03bb 2 \u03b3\n, and so by the assumption (27\n) that \u01eb \u2265 ( 8L 2 \u03c1 n ) \u03b3 2(\u03b3\u22121) ( 2 \u03bb ) 1 \u03b3\u22121 , we have 2\u03c1 n Var Pn (\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)) \u2264 L 2\u03c1 n 2\u01eb \u03bb 1 \u03b3 \u2264 \u01eb 2 .\nIn particular, if the event (42) holds then sup\n\u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 \u01eb 2 ,\nand recalling the definition (41) of \u2206 n , it then follows that\nP S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 \u2264 P sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 \u01eb 2 . (43\n)\nTo bound the probability (43), we use standard bounded difference and symmetrization arguments [e.g. 11, Theorem 6.5]. Letting f (X 1 , . . . , X n ) := sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8), the function f satisfies bounded differences:\nsup x,x \u2032 \u2208X |f (X 1 , \u2022 \u2022 \u2022 , X j\u22121 , x, X j+1 , \u2022 \u2022 \u2022 , X n ) \u2212 f (X 1 , \u2022 \u2022 \u2022 , X j\u22121 , x \u2032 , X j+1 , \u2022 \u2022 \u2022 , X n )| \u2264 sup x,x \u2032 \u2208X sup \u03b8\u2208S 2\u01eb \u22c6 1 n (\u2113(\u03b8; x) \u2212 \u2113(\u03c0(\u03b8); x)) \u2212 1 n (\u2113(\u03b8; x \u2032 ) \u2212 \u2113(\u03c0(\u03b8); x \u2032 )) \u2264 2L n sup \u03b8\u2208S 2\u01eb \u22c6 dist(\u03b8, S \u22c6 ) \u2264 2L n 2\u01eb \u03bb 1 \u03b3 for j = 1, . . . , n. Using the standard symmetrization inequality E[sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8)] \u2264 2E[R n (S 2\u01eb \u22c6 )\n] and the bounded differences inequality [11, Theorem 6.5], we have\nP sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 2E[R n (S 2\u01eb \u22c6 )] + t \u2264 exp \u2212 nt 2 2L 2 \u03bb 2\u01eb 2 \u03b3 for all t \u2265 0. Letting u = nt 2 2L 2 \u03bb 2\u01eb 2 \u03b3\nabove and recalling the assumption (27) upper bounding E[R n (S 2\u01eb\n\u22c6 )], we have P(sup\n\u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 \u01eb 2 ) \u2264 e \u2212u .\nThe theorem follows from the bound (43).\nProof of Claim E.1 If S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 , then certainly it is the case that there is some\n\u03b8 \u2208 \u0398 \\ S 2\u01eb \u22c6 such that R n (\u03b8, P n ) \u2264 inf \u03b8\u2208\u0398 R n (\u03b8, P n ) + \u01eb \u2264 R n (\u03c0(\u03b8), P n ) + \u01eb.\nUsing the convexity of R n , we have for all t \u2208 [0, 1] that\nR n (t\u03b8 + (1 \u2212 t)\u03c0(\u03b8), P n ) \u2264 tR n (\u03b8, P n ) + (1 \u2212 t)R n (\u03c0(\u03b8), P n ) \u2264 R n (\u03c0(\u03b8), P n ) + t\u01eb.\nFor all t \u2208 [0, 1], we have by definition of orthogonal projection (because the vector \u03b8 \u2212 \u03c0(\u03b8) belongs to the normal cone to S \u22c6 at \u03c0(\u03b8); cf. [24, Prop. III.5.3.3]) that \u03c0(t\u03b8 + (1 \u2212 t)\u03c0(\u03b8)) = \u03c0(\u03b8). Thus, choosing t appropriately, there exists \u03b8 \u2032 \u2208 bd S 2\u01eb \u22c6 with \u03b8 \u2032 = t\u03b8 + (1 \u2212 t)\u03c0(\u03b8), \u03c0(\u03b8 \u2032 ) = \u03c0(\u03b8), and R n (\u03b8 \u2032 , P n ) \u2264 R n (\u03c0(\u03b8 \u2032 ), P n ) + \u01eb.\nAdding and subtracting the risk R(\u03b8) and R(\u03c0(\u03b8)), we have that for some \u03b8 \u2208 bd\nS 2\u01eb \u22c6 that R n (\u03b8, P n ) \u2212 R(\u03b8) + R(\u03c0(\u03b8)) \u2212 R n (\u03c0(\u03b8), P n ) \u2264 R(\u03c0(\u03b8)) \u2212 R(\u03b8) + \u01eb \u2264 \u2212\u01eb,\nwhere we have used that R(\u03b8) = R(\u03c0(\u03b8)) + 2\u01eb by construction. Multiplying by \u22121 on each side of the preceding display and taking suprema, we find that\n\u01eb \u2264 sup \u03b8\u2208S 2\u01eb \u22c6 {R(\u03b8) \u2212 R n (\u03b8, P n ) \u2212 (R(\u03c0(\u03b8)) \u2212 R n (\u03c0(\u03b8), P n ))} \u2264 sup \u03b8\u2208S 2\u01eb \u22c6 sup P :D \u03c6 (P || Pn)\u2264\u03c1/n {R(\u03b8) \u2212 R(\u03c0) + E P [\u2113(\u03c0(\u03b8); X) \u2212 \u2113(\u03b8; X)]} .\nApplying the upper bound in inequality (10) of Theorem 1 gives the claim.", "publication_ref": ["b26", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "F Proof of Theorem 6", "text": "We begin by establishing a few technical lemmas, after which the proof of the theorem follows essentially standard arguments in asymptotics. To prove Theorem 6, we first show that (eventually) we have the exact expansion\nR n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1Var Pn (\u2113(\u03b8, X)) n\nfor all \u03b8 in a neighborhood of \u03b8 \u22c6 . As in the proof of Theorem 1, this exact equality holds once there is suitable variability in the values \u2113(\u03b8, X i ) over i = 1, . . . , n, however, we require a bit more care as the values \u2113(\u03b8, X i ) may be unbounded below and above. Heuristically, however, assuming that we have this exact expansion and that \u03b8 rob n \u2212 \u03b8 \u22c6 = O P (n \u2212 1 2 ), then we can write the expansions\n0 = \u2207 \u03b8 R n ( \u03b8 rob n , P n ) = \u2207 1 n n i=1 \u2113(\u03b8 \u22c6 , X i ) + \u2207 2 1 n n i=1 \u2113(\u03b8 \u22c6 , X i ) ( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + \u2207 2\u03c1Var Pn (\u2113( \u03b8 rob n , X)) n + o P (n \u2212 1 2 ) = 1 n n i=1 \u2207\u2113(\u03b8 \u22c6 , X i ) + \u2207 2 R(\u03b8 \u22c6 )( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + \u2207 2\u03c1Var(\u2113(\u03b8 \u22c6 , X)) n + o P (n \u2212 1 2 ).\nMultiplying by \u221a n and solving for \u03b8 rob n in the preceding expression, computing \u2207 Var(\u2113(\u03b8 \u22c6 , X)) then yields the theorem.\nThe remainder of the proof makes this heuristic rigorous, and the outline is as follows:\n1. We show that there is a uniform expansion of the form (12) in a neighborhood of \u03b8 \u22c6 . (See Section F.1.)\n2. Using the uniform expansion, we can then leverage standard techniques for asymptotic analysis of finite-dimensional estimators (see, e.g. van der Vaart and Wellner [47] or Lehmann and Casella [29]), which proceed by performing a Taylor expansion of the objective in a neighborhood of the optimum and using local asymptotic normality arguments. (See Section F.2.)", "publication_ref": ["b46", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "F.1 The uniform variance expansion", "text": "To lighten notation, we define a few quantities similar to those used in the proof of Theorem 1. Let\nZ(\u03b8) := \u2113(\u03b8, X) \u2212 E[\u2113(\u03b8, X)]\nbe the deviation of \u2113(\u03b8, X) around its mean (the risk), and similarly let Z i (\u03b8) be the version of this quantity for observation X i . In addition, let s 2 n (\u03b8) = Var Pn (Z(\u03b8)) be the empirical variance of Z(\u03b8), which is identical to the empirical variance of \u2113(\u03b8, X). Now, recall the problem\nmaximize P E P [Z(\u03b8)] subject to D \u03c6 (P || P n ) \u2264 \u03c1 n ,\nand for each \u03b8 \u2208 \u0398, let p(\u03b8) = argmax p\u2208Pn n i=1 p i Z i (\u03b8) be the solution (probability) vectors. Following expression ( 9) we see for any \u01eb \u2265 0 that min\ni\u2208[n] \u221a 2\u03c1(Z i (\u03b8) \u2212 Z(\u03b8)) \u221a ns n (\u03b8) \u2265 \u22121 for all \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB\nis sufficient for the exact variance expansion to hold. We now show that this is indeed likely. Let \u01eb > 0 be small enough that Assumption A holds, that is, the random Lipschitz function L(X)\nsatisfies |\u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u2032 , x)| \u2264 L(x) \u03b8 \u2212 \u03b8 \u2032 for \u03b8, \u03b8 \u2032 \u2208 \u03b8 \u22c6 + \u01ebB. Then because \u221a ns n (\u03b8) \u2212 \u221a ns n (\u03b8 \u2032 ) \u2264 sup u: u 2 \u22641 n i=1 u i \u2113(\u03b8, X i ) \u2212 \u2113(\u03b8 \u2032 , X i ) \u2264 sup u: u 2 \u22641 n i=1 u i L(X i ) \u03b8 \u2212 \u03b8 \u2032 \u2264 n i=1 L 2 (X i ) \u03b8 \u2212 \u03b8 \u2032 so \u03b8 \u2192 s n (\u03b8) is 1 n n i=1 L(X i ) 2 -Lipschitz for \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB, we have inf \u03b8\u2208\u03b8 \u22c6 +\u01ebB min i\u2208[n] \u221a 2\u03c1(Z i (\u03b8) \u2212 Z(\u03b8)) \u221a ns n (\u03b8) \u2265 min i\u2208[n] \u221a 2\u03c1(Z i (\u03b8 \u22c6 ) \u2212 Z(\u03b8 \u22c6 ) \u2212 2\u01ebL(X i )) n s n (\u03b8 \u22c6 ) \u2212 \u01eb 1 n n j=1 L(X j ) 2 .\nSummarizing our development thus far, we have the following lemma.\nLemma F.1. Let the conditions of the previous paragraph hold. Then\nmin i\u2208[n] 2\u03c1(Z i (\u03b8 \u22c6 ) \u2212 Z(\u03b8 \u22c6 ) \u2212 2\u01ebL(X i )) \u2265 \u221a n s n (\u03b8 \u22c6 ) \u2212 \u01eb 1 n n i=1 L(X i ) 2 1 2\nimplies that R n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1 n Var Pn (\u2113(\u03b8, X)) for all \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB.\nand E[\u2113(h(X), Y ) 2 ] \u2264 r/c 2 also implies \u03c3 2 \u2264 r/c 2 . Returning to expression (46) and enlarging the sets over which we take suprema, we thus obtain\nG n (\u2113 \u2022 H) \u2264 E sup h\u2208B H ,c 1 ,c 2 \u2208[0,1] n i=1 g i |c 1 (h(x i ) \u2212 h \u22c6 (x i )) \u2212 c 2 \u03be i | | E[(h(X) \u2212 h \u22c6 (X)) 2 ] \u2264 r c 2 1 , \u03c3 2 \u2264 r c 2 2 \u2264 E sup f \u22082B H ,c\u2208[0,1] n i=1 g i |f (x i ) \u2212 c\u03be i | | E[f (X) 2 ] \u2264 r, \u03c3 2 \u2264 r/c 2 ,\nwhere we have used that h \u2212 h \u22c6 \u2208 2B H and that the set B H is convex to obtain the second inequality. We now upper bound the final display using the classical Sudakov-  The last term in the expression has bound \u221a nr by Jensen's inequality and the relaxation that c \u2208 [\u22121, 1]. For the first term, Mendelson [34,Thm. 2.1] shows that for RKHS with kernel eigenvalues \u03bb 1 , \u03bb 2 , . . ., we have\nE[(Y f 1 ,c 1 \u2212 Y f 2 ,c 2 ) 2 ] = n i=1 (|f 1 (x i ) \u2212 c 1 \u03be i | \u2212 |f 2 (x i ) \u2212 c 2 \u03be i |) 2 \u2264 n i=1 (f 1 (x i ) \u2212 f 2 (x i ) + (c 2 \u2212 c 1 )\u03be i ) 2 \u2264 2 n i=1 (f 1 (x i ) \u2212 f 2 (x i )) 2 + 2(c 2 \u2212 c 1 ) 2 n i=1 \u03be 2 i . Moreover, E[(Z f 1 ,c 1 \u2212 Z f 2 ,c 2 ) 2 ] = n i=1 (f 1 (x i ) \u2212 f 2 (x i )) 2 + (c 1 \u2212 c 2 ) 2 n i=1 \u03be 2 i .\nE sup f \u22082B H n i=1 g i f (X i ) | E[f (X) 2 ] \u2264 r \u221a n \uf8eb \uf8ed \u221e j=1 min{\u03bb j , r} \uf8f6 \uf8f8 1 2\n, which yields our desired claim (24).", "publication_ref": ["b45", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "G.2 Proof of Lemma 3.1", "text": "Defining N y := card{i \u2208 [n] : X i = y} for y \u2208 {\u22121, 0, 1}, we immediately obtain\nE Pn [\u2113(\u03b8; X)] = 1 n [N \u22121 |\u03b8 + 1| + N 1 |\u03b8 \u2212 1| + N 0 |\u03b8| \u2212 (n \u2212 N 0 )] , because N 1 + N \u22121 + N 0 = n.\nIn particular, we find that the empirical risk minimizer \u03b8 satisfies\n\u03b8 erm n := argmin \u03b8\u2208R E Pn [\u2113(\u03b8; X)] = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if N 1 > N 0 + N \u22121 \u22121 if N \u22121 > N 0 + N 1 \u2208 [\u22121, 1] otherwise.\nOn the events N 1 > N \u22121 + N 0 or N \u22121 > N 0 + N 1 , which are disjoint, then, we have R( \u03b8 erm n ) = \u03b4 = R(\u03b8 \u22c6 ) + \u03b4.\nLet us give a lower bound on the probability of this event. Noting that marginally N 1 \u223c Bin(n, 1\u2212\u03b4 2 ) and using N 0 + N \u22121 = n \u2212 N 1 , we have N 1 > N 0 + N \u22121 if and only if N 1 > n 2 , and we would like to lower bound\nP N 1 > n 2 = P Bin n, 1 \u2212 \u03b4 2 > n 2 = P Bin n, 1 + \u03b4 2 < n 2 .\nLetting \u03a6(t) = 1 \u221a 2\u03c0 t \u2212\u221e e \u2212u 2 /2 du denote the standard Gaussian CDF, then Zubkov and Serov [53] show that\nP N 1 \u2265 n 2 \u2265 \u03a6 \u2212 2nD kl 1 2 || 1 + \u03b4 2\nwhere D kl (p||q) = p log p q + (1 \u2212 p) log 1\u2212p 1\u2212q denotes the binary KL-divergence. We have by standard bounds on the KL-divergence [45,Lemma 2.7] that D kl ( 1 2 || 1+\u03b4 2 ) \u2264 \u03b4 2 2(1\u2212\u03b4 2 ) , so that\nP N 1 > n 2 or N \u22121 > n 2 \u2265 2\u03a6 \u2212 n\u03b4 2 1 \u2212 \u03b4 2 \u2212 2P N 1 = n 2 .\nFor n odd, the final probability is 0, while for n even, we have\nP N 1 = n 2 = 2 \u2212n n n/2 (1 \u2212 \u03b4 2 ) n/2 \u2264 (1 \u2212 \u03b4 2 ) n/2 2 \u03c0n ,\nwhere the inequality uses that 2n n \u2264 4 n \u221a \u03c0n by Stirling's approximation. Summarizing, we find that\nP N 1 > n 2 or N \u22121 > n 2 \u2265 2\u03a6 \u2212 n\u03b4 2 1 \u2212 \u03b4 2 \u2212 (1 \u2212 \u03b4 2 ) n/2 8 \u03c0n .", "publication_ref": ["b52", "b44"], "figure_ref": [], "table_ref": []}, {"heading": "G.3 Proof of Lemma F.4", "text": "Under the conditions of the theorem, the compactness of \u03b8 \u22c6 + \u01ebB guarantees that sup \u2192 0. Now, we use the fact that \u2207 2 R(\u03b8 \u22c6 ) \u227b 0, and that \u03b8 \u2192 \u2207 2 R(\u03b8) is continuous in a neighborhood of \u03b8 \u22c6 . Fix \u01eb > 0 small enough that the preceding uniform convergence guarantees hold over \u03b8 \u22c6 + 2\u01ebB and \u2207 2 R(\u03b8) \u03bbI for some \u03bb > 0 and all \u03b8 \u2208 \u03b8 \u22c6 + 2\u01ebB. Let \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB, but \u03b8 \u2208 \u03b8 \u22c6 + 2\u01ebB. Then for sufficently large n, we have that R n (\u03b8, P n ) \u2265 E Pn [\u2113(\u03b8, X)]\n(i) \u2265 R(\u03b8) \u2212 \u03bb 4 \u01eb 2 (ii) \u2265 R(\u03b8 \u22c6 ) + \u03bb 2 \u03b8 \u2212 \u03b8 \u22c6 2 2 \u2212 \u03bb 4 \u01eb 2 (iii) \u2265 R(\u03b8 \u22c6 ) + \u03bb 4 \u01eb 2 (iv) \u2265 E Pn [\u2113(\u03b8 \u22c6 , X)] + \u03bb 4 \u01eb 2 \u2212 \u03bb 8 \u01eb 2 = E Pn [\u2113(\u03b8 \u22c6 , X)] + \u03bb 8 \u01eb 2 ,\nwhere inequalities (i) and (iv) follow from the uniform convergence guarantee, inequality (ii) from the strong convexity of R near \u03b8 \u22c6 , and (iii) because \u03b8 \u2212 \u03b8 \u22c6 2 \u2265 \u01eb. Finally, we have that\nE Pn [\u2113(\u03b8 \u22c6 , X)] \u2265 R n (\u03b8 \u22c6 , P n ) \u2212 2\u03c1 n\nVar Pn (\u2113(\u03b8 \u22c6 , X))\na.s.\n\u2192 0 , so that eventually R n (\u03b8, P n ) > R n (\u03b8 \u22c6 , P n ) for all \u03b8 \u2208 \u03b8 \u22c6 +2\u01ebB\\\u01ebB. By convexity, then this inequality holds for all \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB. Thus if \u03b8 rob n \u2208 argmin \u03b8 R n (\u03b8, P n ), then for any \u01eb > 0 we must eventually have \u03b8 rob n \u2212 \u03b8 \u22c6 2 < \u01eb.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H Efficient solutions to computing the robust expectation", "text": "In this appendix, we give a detailed description of the procedure we use to compute the supremum problem (8). In particular, our procedure requires time O(n log n + log 1 \u01eb log n), where \u01eb is the desired solution accuracy. Let us reformulate this as a minimization problem in a variable p \u2208 R n for simplicity. Then we wish to solve minimize p \u22a4 z subject to 1 2n np \u2212 1 2 2 \u2264 \u03c1, p \u2265 0, p \u22a4 1 = 1.\nWe take a partial dual of this minimization problem, then maximize this dual to find the optimizing p. Introducing the dual variable \u03bb \u2265 0 for the constraint that 1 2 p \u2212 1 n 1 2 2 \u2264 \u03c1 n and performing the standard min-max swap [15] (strong duality obtains for this problem because the Slater condition is satisfied by p = 1 n 1) yields the maximization problem maximize \u03bb\u22650 f (\u03bb) := inf\np \u03bb 2 p \u2212 1 n 1 2 2 \u2212 \u03bb\u03c1 n + p \u22a4 z | p \u2265 0, 1 \u22a4 p = 1 .(47)\nIf we can efficiently compute the infimum (47), then it is possible to binary search over \u03bb. Recall the standard fact [24,Chapter VI.4.4] that for a collection {f p } p\u2208P of concave functions, if the infimum f (x) = inf p\u2208P f p (x) is attained at some p 0 then any vector \u2207f p 0 (x) is a supergradient of f (x). Thus, letting p(\u03bb) be the (unique) minimizing value of p for any \u03bb > 0, the objective (47) becomes f (\u03bb) = \u03bb 2 p(\u03bb) \u2212 1 n 1 2 2 \u2212 \u03bb\u03c1 n + p(\u03bb) \u22a4 z, whose derivative with respect to \u03bb (holding p fixed) is f \u2032 (\u03bb) = 1 2 p(\u03bb) \u2212 1 n 1 2 2 \u2212 \u03c1 n . Now we use well-known results on the Euclidean projection of a vector to the probability simplex [19] to provide an efficient computation of the infimum (47). First, we assume with no loss of generality that z 1 \u2264 z 2 \u2264 \u2022 \u2022 \u2022 \u2264 z n and that 1 \u22a4 z = 0, because neither of these changes the original optimization problem (as 1 \u22a4 p = 0 and the objective is symmetric). Then we define the two vectors s, \u03c3 2 \u2208 R n , which we use for book-keeping in the algorithm, by\ns i = j\u2264i z j , \u03c3 2 i = j\u2264i z 2 j ,\nand we let z 2 be the vector whose entries are z 2 i . The infimum problem ( 47) is equivalent to projecting the vector v(\u03bb) \u2208 R n defined by\nv i = 1 n \u2212 1 \u03bb z i\nonto the probability simplex. Notably [19], the projection p(\u03bb) has the form p i (\u03bb) = (v i \u2212 \u03b7) + for some \u03b7 \u2208 R, where \u03b7 is chosen such that n i=1 p i (\u03bb) = 1. Finding such a value \u03b7 is equivalent [19, Figure 1] to finding the unique index i such that i j=1 (v j \u2212 v i ) < 1 and i+1 j=1 (v j \u2212 v i+1 ) \u2265 1, taking i = n if no such index exists (the sum i j=1 (v j \u2212 v i ) is increasing in i and v 1 \u2212 v 1 = 0). Given the index i, algebraic manipulations show that \u03b7 = 1 n \u2212 1 i \u2212 1 i i j=1 z j /\u03bb = 1 n \u2212 1 i \u2212 1 i s i /\u03bb satisfies the equality n i=1 (v i \u2212 \u03b7) + = 1 and that v j \u2212 \u03b7 \u2265 0 for all j \u2264 i while v j \u2212 \u03b7 \u2264 0 for j > i. Of course, given the index i and \u03b7, we may calculate the derivative \u2202 \u2202\u03bb f (\u03bb) efficiently as well:\nf \u2032 (\u03bb) = \u2202 \u2202\u03bb \u03bb 2 p(\u03bb) \u2212 n \u22121 1 2 2 \u2212 \u03bb\u03c1 n + p(\u03bb) \u22a4 z = 1 2 p(\u03bb) \u2212 n \u22121 1 2 2 \u2212 \u03c1 n = 1 2 i j=1 (v j \u2212 \u03b7 \u2212 n \u22121 ) 2 + 1 2 n j=i+1 1 n 2 \u2212 \u03c1 n = 1 2 i j=1 1 \u03bb z j + \u03b7 2 + n \u2212 i 2n 2 \u2212 \u03c1 n = \u03c3 2 i 2\u03bb 2 + i\u03b7 2 2 + s i \u03b7 \u03bb + n \u2212 i 2n 2 \u2212 \u03c1 n .\nFinding the index optimal i can be done by a binary search, which requires O(log n) time, and f \u2032 (\u03bb) is then computable in O(1) time using the vectors s and \u03c3 2 . It is then possible to perform a binary search over \u03bb using f \u2032 (\u03bb), which which requires log 1 \u01eb iterations to find \u03bb within accuracy \u01eb, from which it is easy to compute p(\u03bb) via p i (\u03bb) = (v i \u2212 \u03b7) + = n \u22121 \u2212 \u03bb \u22121 z i \u2212 \u03b7 + .\nWe summarize this discussion with pseudo-code in Figures 6 and 7, which provide a main routine and sub-routine for finding the optimal vector p. These routines show that, once provided the sorted vector z with z 1 \u2264 z 2 \u2264 \u2022 \u2022 \u2022 \u2264 z n (which requires n log n time to compute), we require only O(log 1 \u01eb \u2022 log n) computations.", "publication_ref": ["b7", "b14", "b23", "b0", "b18", "b46", "b18"], "figure_ref": ["fig_14"], "table_ref": []}, {"heading": "Acknowledgments", "text": "We thank Feng Ruan for pointing out a much simpler proof of Theorem 1 than in our original paper. JCD and HN were partially supported by the SAIL-Toyota Center for AI Research and HN was partially supported Samsung Fellowship. JCD was also partially supported by the National Science Foundation award NSF-CAREER-1553086 and the Sloan Fellowship.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "This gives the theorem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D Proof of Theorem 4", "text": "We first show the following version of uniform Bernstein's inequality with Rademacher complexities. The proof uses a peeling technique [3,46], in conjuction with Talagrand's concentration inequality (Lemma B.2).\nLemma D.1. Let r > 0 and F be a collection of bounded functions f : X \u2192 [0, M ] with Var(f (X)) \u2264 r. Then, with probability at least 1 \u2212 e \u2212t , for every f \u2208 F\nThe same statements hold with the roles of E[f ] and E Pn [f ] reversed.\nWe defer the proof to section D.1 at the end of this section. Because Var(f ) \u2264 M 2 for all f \u2208 F, Lemma D.1 also holds if we replace the terms log nr M 2 t with log n t \u2264 1 + log n t . Next, we show an important extension of Lemma D.1 that replaces the Rademacher complexity term E[R n (F)] by a local quantity r \u22c6 n , the fixed point of \u03c8 n (r). To this end, we use another peeling argument and apply Lemma D.1 to the self-normalized class\nThis idea follows the techniques of Bartlett et al. [3,Thm. 3.3], though we use a type of selfnormalizing scale, that is, f / E[f 2 ], whereas they use a variance-normalizing scaling by studying classes of functions of the form f /E[f 2 ]. Our use of this alternative normalization is important in the next lemma, which allows us to obtain bounds that apply to the robustly regularized risk.\nLemma D.2. Let F be a collection of bounded functions f :\nThen with probability at least 1 \u2212 e \u2212t , for every f \u2208 F\nThe same statement holds with the roles of E[f ] and E Pn [f ] reversed.\nSee Section D.2 for the proof. Next, we give an analogous result for f 2 . Now, we use the following standard result to show that the conditions of Lemma F.1 eventually hold with probability one. [37], Lemma 3). Let Y i be independent random variables with", "publication_ref": ["b2", "b45", "b36"], "figure_ref": [], "table_ref": []}, {"heading": "Lemma F.2 (Owen", "text": "Based on Lemma F.2 and the strong law of large numbers, we see immediately that 1 \u221a n max\nApplying the strong law of large numbers to obtain\nwe see immediately that for small enough \u01eb > 0, the condition of Lemma F.1 holds eventually with probability 1. That is, the following uniform expansion holds.\nLemma F.3. There exists \u01eb > 0 such that, with probability 1, there exists an N (which may be random) such that n \u2265 N implies", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.2 Asymptotics and Taylor expansions", "text": "Let E n,exact be the event that the exact variance expansion of Lemma F.3 occurs for \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB. Now that we know that P(E n,exact eventually) = 1, we may perform a few asymptotic expansions of the variance-regularized objective to provide the convergence guarantees specified by the theorem. We use the following lemma.\nLemma F.4. Let the conditions of the theorem hold. If\nThe proof is standard, but for completeness we include it in Section G.3. By combining Lemmas F.3 and F.4, we see that with probability 1, for any \u01eb > 0, we eventually have both\nAssume for the remainder of the argument that both of these conditions hold. Standard results on subdifferentiability of maxima of collections of convex functions [24,Chapter X] give that R n (\u03b8, P n ) is differentiable near \u03b8 \u22c6 , and thus\nBecause \u03b8 rob n a.s.\n\u2192 \u03b8 \u22c6 , by the continuous mapping theorem and local uniform convergence of the empirical expectations E Pn [\u2022] to E[\u2022], the second term of expression ( 45) satisfies\nFor simplicity, we let b(\u03b8 \u22c6 ) denote the final term, which we shall see becomes an asymptotic bias. Thus, performing a Taylor expansion of the terms \u2207\u2113( \u03b8 rob n , X i ) around \u03b8 \u22c6 in equality ( 45), there exist (random) error matrices E n (X i ), where\nMultiplying both sides by \u221a n, using that \u2207 2 R(\u03b8 \u22c6 ) + o P (1) is eventually invertible, and applying the continuous mapping theorem, we have\nThe first term on the right side of the above display converges in distribution to a N(0, \u03a3) distribution, where\nas claimed in the theorem statement.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "G Proofs of Technical Lemmas G.1 Proof of Inequality (24)", "text": "Define the Gaussian complexity\nwhere g i iid \u223c N(0, 1) (here we recall the standard result [2] that Gaussian complexity upper bounds Rademacher complexities up to a constant). Now, the set h \u2212 h \u22c6 such that h \u2208 B H is contained in 2B H , which is convex. Moreover, we have E[\u2113(h(X), Y ) 2 ] = E[(h(X) \u2212 h \u22c6 (X)) 2 ] + \u03c3 2 , and so we have for any c that\nInputs: Sorted vector z \u2208 R n with 1 \u22a4 z = 0, parameter \u03c1 > 0, solution accuracy \u01eb Set \u03bb min = 0 and \u03bb max = \u03bb \u221e = max{n z \u221e , n/2\u03c1 z 2 } Set s i = j\u2264i z j and \u03c3 Inputs: Sorted vector z with 1 \u22a4 z = 0, \u03bb > 0, vector s with s i = j\u2264i z j\n\u03bb (iz i \u2212 s i ) // (this is s left = i j=1 (v j \u2212 v i )) s right = \u03bb ((i + 1)z i+1 \u2212 s i+1 ) // (this is s right = i+1 j=1 (v j \u2212 v i+1 )) If s right \u2265 1 and s left < 1\nSet \u03b7 = 1 n \u2212 1 i \u2212 1 \u03bbi s i and return (\u03b7, i) Else if s left \u2265 1 Set i high = i \u2212 1 Else Set i low = i + 1 Set i = i low and \u03b7 = 1 n \u2212 1 i \u2212 1 \u03bbi s i and return (\u03b7, i)\nFigure 7. Procedure FindShift to find index i and parameter \u03b7 such that, for the definition v i = 1 n \u2212 1 \u03bb z i , we have v j \u2212 \u03b7 \u2265 0 for j \u2264 i, v j \u2212 \u03b7 \u2264 0 for j > i, and n j=1 (v j \u2212 \u03b7) + = 1. Method requires time O(log n).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "A result of Vapnik with applications", "journal": "Discrete Applied Mathematics", "year": "1993", "authors": "M Anthony; J Shawe-Taylor"}, {"ref_id": "b1", "title": "Rademacher and Gaussian complexities: Risk bounds and structural results", "journal": "Journal of Machine Learning Research", "year": "2002", "authors": "P L Bartlett; S Mendelson"}, {"ref_id": "b2", "title": "Local Rademacher complexities", "journal": "Annals of Statistics", "year": "2005", "authors": "P L Bartlett; O Bousquet; S Mendelson"}, {"ref_id": "b3", "title": "Convexity, classification, and risk bounds", "journal": "Journal of the American Statistical Association", "year": "2006", "authors": "P L Bartlett; M I Jordan; J Mcauliffe"}, {"ref_id": "b4", "title": "Robust Optimization", "journal": "Princeton University Press", "year": "2009", "authors": "A Ben-Tal; L E Ghaoui; A Nemirovski"}, {"ref_id": "b5", "title": "Robust solutions of optimization problems affected by uncertain probabilities", "journal": "Management Science", "year": "2013", "authors": "A Ben-Tal; D Hertog; A D Waegenaere; B Melenberg; G Rennen"}, {"ref_id": "b6", "title": "Oracle-based robust optimization via online learning", "journal": "Operations Research", "year": "2015", "authors": "A Ben-Tal; E Hazan; T Koren; S Mannor"}, {"ref_id": "b7", "title": "", "journal": "", "year": "2014", "authors": "D Bertsimas; V Gupta; N Kallus; Saa Robust"}, {"ref_id": "b8", "title": "Piecewise-polynomial approximations of functions of the classes W \u03b1 p", "journal": "Sbornik: Mathematics", "year": "1967", "authors": "M Birman; M Solomjak"}, {"ref_id": "b9", "title": "Theory of classification: a survey of some recent advances", "journal": "ESAIM: Probability and Statistics", "year": "2005", "authors": "S Boucheron; O Bousquet; G Lugosi"}, {"ref_id": "b10", "title": "Concentration Inequalities: a Nonasymptotic Theory of Independence", "journal": "Oxford University Press", "year": "2013", "authors": "S Boucheron; G Lugosi; P Massart"}, {"ref_id": "b11", "title": "A bennett concentration inequality and its application to suprema of empirical processes", "journal": "Comptes Rendus Mathematique", "year": "2002", "authors": "O Bousquet"}, {"ref_id": "b12", "title": "Concentration inequalities and empirical processes theory applied to the analysis of learning algorithms", "journal": "", "year": "2002", "authors": "O Bousquet"}, {"ref_id": "b13", "title": "Concentration inequalities for sub-additive functions using the entropy method", "journal": "Springer", "year": "2003", "authors": "O Bousquet"}, {"ref_id": "b14", "title": "Convex Optimization", "journal": "Cambridge University Press", "year": "2004", "authors": "S Boyd; L Vandenberghe"}, {"ref_id": "b15", "title": "Statistics for High-Dimensional Data: Methods, Theory and Applications", "journal": "Springer", "year": "2011", "authors": "P B\u00fchlmann; S Van De Geer"}, {"ref_id": "b16", "title": "An error bound in the Sudakov-Fernique inequality", "journal": "", "year": "2005", "authors": "S Chatterjee"}, {"ref_id": "b17", "title": "Kernel Methods for Pattern Analysis", "journal": "Cambridge University Press", "year": "2004", "authors": "N Cristianini; J Shawe-Taylor"}, {"ref_id": "b18", "title": "Efficient projections onto the \u2113 1 -ball for learning in high dimensions", "journal": "", "year": "2008", "authors": "J C Duchi; S Shalev-Shwartz; Y Singer; T Chandra"}, {"ref_id": "b19", "title": "Statistics of robust optimization: A generalized empirical likelihood approach", "journal": "", "year": "2016", "authors": "J C Duchi; P W Glynn; H Namkoong"}, {"ref_id": "b20", "title": "Uniform Central Limit Theorems", "journal": "Cambridge University Press", "year": "1999", "authors": "R M Dudley"}, {"ref_id": "b21", "title": "Robust empirical optimization is almost the same as mean-variance optimization. Available at SSRN 2827400", "journal": "", "year": "2015", "authors": "J Gotoh; M J Kim; A Lim"}, {"ref_id": "b22", "title": "Smoothing spline ANOVA models", "journal": "Springer", "year": "2002", "authors": "C Gu"}, {"ref_id": "b23", "title": "Convex Analysis and Minimization Algorithms I & II", "journal": "Springer", "year": "1993", "authors": "J Hiriart-Urruty; C "}, {"ref_id": "b24", "title": "Local Rademacher complexities and oracle inequalities in risk minimization", "journal": "Annals of Statistics", "year": "2006", "authors": "V Koltchinskii"}, {"ref_id": "b25", "title": "Covering numbers of Gaussian reproducing kernel Hilbert spaces", "journal": "Journal of Complexity", "year": "2011", "authors": "T K\u00fchn"}, {"ref_id": "b26", "title": "Quantifying input uncertainty in stochastic optimization", "journal": "IEEE", "year": "2015", "authors": "H Lam; E Zhou"}, {"ref_id": "b27", "title": "Probability in Banach Spaces", "journal": "Springer", "year": "1991", "authors": "M Ledoux; M Talagrand"}, {"ref_id": "b28", "title": "Theory of Point Estimation, Second Edition", "journal": "Springer", "year": "1998", "authors": "E L Lehmann; G Casella"}, {"ref_id": "b29", "title": "RCV1: A new benchmark collection for text categorization research", "journal": "Journal of Machine Learning Research", "year": "2004", "authors": "D Lewis; Y Yang; T Rose; F Li"}, {"ref_id": "b30", "title": "UCI machine learning repository", "journal": "", "year": "2013", "authors": "M Lichman"}, {"ref_id": "b31", "title": "Smooth discrimination analysis", "journal": "Annals of Statistics", "year": "1999", "authors": "E Mammen; A B Tsybakov"}, {"ref_id": "b32", "title": "Empirical Bernstein bounds and sample variance penalization", "journal": "", "year": "2009", "authors": "A Maurer; M Pontil"}, {"ref_id": "b33", "title": "On the performance of kernel classes", "journal": "Journal of Machine Learning Research", "year": "2003-10", "authors": "S Mendelson"}, {"ref_id": "b34", "title": "Learning without concentration", "journal": "", "year": "2014", "authors": "S Mendelson"}, {"ref_id": "b35", "title": "Stochastic gradient methods for distributionally robust optimization with f -divergences", "journal": "", "year": "2016", "authors": "H Namkoong; J C Duchi"}, {"ref_id": "b36", "title": "Empirical likelihood ratio confidence regions. The Annals of Statistics", "journal": "", "year": "1990", "authors": "A Owen"}, {"ref_id": "b37", "title": "Empirical likelihood", "journal": "CRC press", "year": "2001", "authors": "A B Owen"}, {"ref_id": "b38", "title": "Concentration of measure inequalities for Markov chains and \u03c6-mixing processes", "journal": "Annals of Probability", "year": "2000", "authors": "P Samson"}, {"ref_id": "b39", "title": "Lectures on Stochastic Programming: Modeling and Theory", "journal": "SIAM and Mathematical Programming Society", "year": "2009", "authors": "A Shapiro; D Dentcheva; A Ruszczy\u0144ski"}, {"ref_id": "b40", "title": "Empirical Bernstein boosting", "journal": "", "year": "2010", "authors": "P K Shivaswamy; T Jebara"}, {"ref_id": "b41", "title": "Variance penalizing AdaBoost", "journal": "", "year": "2011", "authors": "P K Shivaswamy; T Jebara"}, {"ref_id": "b42", "title": "Smoothness, low noise and fast rates", "journal": "", "year": "2010", "authors": "N Srebro; K Sridharan; A Tewari"}, {"ref_id": "b43", "title": "Optimal aggregation of classifiers in statistical learning", "journal": "Annals of Statistics", "year": "2004", "authors": "A B Tsybakov"}, {"ref_id": "b44", "title": "Introduction to Nonparametric Estimation", "journal": "Springer", "year": "2009", "authors": "A B Tsybakov"}, {"ref_id": "b45", "title": "Empirical Processes in M-Estimation", "journal": "Cambridge University Press", "year": "2000", "authors": "S Van De Geer"}, {"ref_id": "b46", "title": "Weak Convergence and Empirical Processes: With Applications to Statistics", "journal": "Springer", "year": "1996", "authors": "A W Van Der Vaart; J A Wellner"}, {"ref_id": "b47", "title": "Statistical Learning Theory", "journal": "Wiley", "year": "1998", "authors": "V N Vapnik"}, {"ref_id": "b48", "title": "On the uniform convergence of relative frequencies of events to their probabilities", "journal": "", "year": "1971", "authors": "V N Vapnik; A Y Chervonenkis"}, {"ref_id": "b49", "title": "Theory of Pattern Recognition", "journal": "Nauka", "year": "1974", "authors": "V N Vapnik; A Y Chervonenkis"}, {"ref_id": "b50", "title": "Capacity of reproducing kernel spaces in learning theory", "journal": "IEEE Transactions on Information Theory", "year": "2003", "authors": "D.-X Zhou"}, {"ref_id": "b51", "title": "Regularization and variable selection via the elastic net", "journal": "Journal of the Royal Statistical Society, Series B", "year": "2005", "authors": "H Zou; T Hastie"}, {"ref_id": "b52", "title": "A complete proof of universal inequalities for the distribution function of the binomial law", "journal": "Theory of Probability & Its Applications", "year": "2013", "authors": "A Zubkov; A Serov"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "log n n with probability at least 1\u22121/n. We see that leading term for the robustly regularized solution \u03b8 rob n only depends on the noise-level B 2 while the corresponding term for the ERM solution \u03b8 erm n depends on global information like the size of the parameter space D, and a uniform bound over covariates L. For typical VC and other d-dimensional classes, the bound Comp n scales linearly in d (cf. [3, Corollary 3.7], in which case the bound (19) scales as R(\u03b8 \u22c6 )+C d(BDL + B 2 ) log n/n+O(log n/n), which is worse. \u2738 Example 5 (A hard median estimation problem): To give a bit more insight into the behavior of the robust estimator, consider the simple 1-dimensional median problem, where \u2113(\u03b8; x) = |\u03b8\u2212x|, and assume that x \u2208 {\u2212B, B} with P(X = B) = 1+\u03b4", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 .2Figure 2. Simulation experiment. log E[R( \u03b8 erm n )] is the solid lines, in decreasing order from B = 10 (top) to B = .01 (bottom). log E[R( \u03b8 rob n )] is the dashed line, in the same vertical ordering at sample size n = 10 2 .", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 22Figure 2 summarizes the results. The robust solution \u03b8 rob n = argmin \u03b8\u2208\u0398 R n (\u03b8, P n ) always outperforms the empirical risk minimizer \u03b8 erm", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "-07 2.68E-16 8.00E-06 2.74E-18 8.27E-04 5.09E-20 8.38E-02 6.55E-20 10000 7.64E-08 1.32E-16 4.02E-06 1.32E-18 4.13E-04 2.57E-20 4.18E-02 3.34E-20", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Train error on rare class (Y i = +1) (d) Test error on rare class (Y i = +1) Train error on common class (Y i = \u22121) (f) Test error on common class (Y i = \u22121)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 3 .3Figure 3. HIV-1 Protease Cleavage plots (2-standard error confidence bars). Comparison of misclassification error rates among different regularizers.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Error on rare class (Y i = +1) (d) Error on common class (Y i = \u22121)", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 4 .4Figure 4. HIV-1 Protease Cleavage plots (2-standard error confidence bars). Plot (a) shows the logistic risk R(\u03b8) = E[log(1 + e \u2212Y \u03b8 \u22a4 X )] and confidence bounds computed from the robust risk (4). Plots (b)-(d) show misclassification error rates plotted against robustness parameter \u03c1.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Recall on rare category (Economics)", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 5 :5Figure 5: Reuters Corpus (2-standard error deviations)", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Lemma A. 2 (2Samson [39], Corollary 3). Let f : R n \u2192 R be convex and L-Lipschitz continuous with respect to the \u2113 2 -norm over [a, b] n , and let Z 1 , . . . , Z n be independent random variables on [a, b].", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "and apply Lemma A.4 with C = M .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "al. [3, Thm 2.1].) Lemma B.2. Let r > 0 and F be a class of functions that map X into [a, b] such that for every f \u2208 F, Var(f (X)) \u2264 r. Then, with probability at least 1 \u2212 e \u2212t sup f \u2208F", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Theorem 1 implies1E Pn [f ] + 2\u03c1 n Var Pn (f ) \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 2M \u03c1n , so we immediately we arrive atE[f ] \u2264 1 + 2 2\u03c1 n sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "Lemma D. 4 .4Let x be the largest solution to ax + b = x 2 d where a, b, d > 0. Then a 2 d 2 \u2264 x 2 \u2264 a 2 d 2 + 2bd. Proof From the quadratic formula, we have x = 1 2 ad + \u221a a 2 d 2 + 4b from which the lower bound follows. From convexity of z \u2192 z 2 and \u221a z 1 + z 2 \u2264 \u221a z 1 + \u221a z 2 for z 1 , z 2 > 0, we obtain the upper bound. Lemma D.4 immediately yields r \u22c6 n \u2264 r \u2264 r \u22c6 n + 7M B n 3 .", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "7 3 .3Then by Lemma B.2 applied to G 2 r , with probability at least 1 \u2212 e \u2212t , for every", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "Fernique comparison theorem [e.g. 17]. Indeed, define the two Gaussian processes indexed by f \u2208 H and c\u2208 [0, 1] by Y f,c = n i=1 g i |f (x i ) \u2212 c\u03be i | and Z f,c = n i=1 g i f (x i ) + c n i=1 w i \u03be i ,where g i iid \u223c N(0, 1) and w i iid \u223c N(0, 1). Then we have for any f 1 , f 2 \u2208 H and c 1 , c 2 \u2208 [0, 1] that", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Thus, the Sudakov-Fernique inequality guarantees that E[sup f,c Y f,c ] \u2264 \u221a 2E[sup f,c Z f,c], andG n (\u2113 \u2022 H) E sup f \u22082B H n i=1 g i f (x i ) | E[f (X) 2 ] \u2264 r + E sup c\u2208[0,1] c n i=1 w i \u03be i | c 2 \u03c3 2 \u2264 r .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "\u03b8\u2208\u03b8 \u22c6 +\u01ebB |E Pn [\u2113(\u03b8, X)] \u2212 R(\u03b8)| a.s. \u2192 0, as the functions \u03b8 \u2192 \u2113(\u03b8, x) are Lipschitz in a neighborhood of \u03b8 \u22c6 by Assumption A. Similarly, sup \u03b8\u2208\u03b8 \u22c6 +\u01ebB Var Pn (\u2113(\u03b8, X)) \u2212 Var(\u2113(\u03b8, X)) a.s.\u2192 0, using the local Lipschitzness of \u2207 2 \u2113. (See, for example, the Glivenko-Cantelli results in Chapters 2.4-2.5 of van der Vaart and Wellner[47].) Thus, using the two-sided bounds (10) of Theorem 1, we have that sup\u03b8\u2208\u03b8 \u22c6 +\u01ebB |R n (\u03b8, P n ) \u2212 R(\u03b8)| \u2264 sup \u03b8\u2208\u03b8 \u22c6 +\u01ebB E Pn [\u2113(\u03b8, P n )] \u2212 R(\u03b8) + 2\u03c1 n sup \u03b8\u2208\u03b8 \u22c6 +\u01ebBVar Pn (\u2113(\u03b8, X))a.s.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Simulation experiment: Mean risks over 1,200 simulations", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Simulation  ", "figure_data": "experiment: Variances of R( \u03b8) over 1,200 simulationsB = .01B = .1B = 1B = 10nERMRobust ERMRobust ERMRobust ERMRobust1007.06E-13 9.76E-14 6.58E-09 1.03E-09 7.09E-05 1.08E-05 7.37E-01 9.20E-025005.98E-14 7.15E-28 3.04E-10 3.52E-26 2.80E-06 2.26E-24 2.92E-02 3.26E-2110002.63E-14 1.07E-31 7.53E-11 1.99E-35 7.14E-07 3.44E-33 7.03E-03 4.78E-3250007.34E-15 2.94E-33 2.70E-12 3.28E-37 2.95E-08 2.50E-39 2.74E-04 5.24E-3810000 1.60E-15 6.54E-34 6.74E-13 7.59E-38 7.04E-09 3.34E-39 6.52E-05 2.25E-38"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "HIV-1 Cleavage Error Document classification in the Reuters corpusFor our final experiment, we consider a multi-label classification problem with a reasonably large dataset. The Reuters RCV1 Corpus[30] has 804,414 examples with d = 47,236 features, where feature j is an indicator variable for whether word j appears in a given document. The goal is to classify documents as a subset of the 4 categories Corporate, Economics, Goverment, and Markets, and each document in the data is labeled with a subset of those. As each document can belong to multiple categories, we fit binary classifiers on each of the four categories. There are different numbers of documents labeled as each category, with the Economics category having the fewest number of positive examples. Table", "figure_data": "\u03c1trainrisktesterror (%) train test train error (Y = +1) error (Y = \u22121) test train testerm0.1587 0.1706 5.52 6.39 17.3218.792.453.171000.1623 0.1763 4.99 5.92 15.0117.042.383.0210000.1777 0.1944 4.55.92 13.3516.332.193.210000 0.2830.3031 2.39 5.67 7.1814.651.153.325.4"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Reuters Number of Examples", "figure_data": "Corporate Economics Government Markets381,327119,920239,267204,820"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Reuters Corpus Precision (%) .72 92.7 93.55 93.55 89.02 89 94.1 94.12 92.88 92.94 1E3 92.97 92.95 93.31 93.33 87.84 87.81 93.73 93.76 92.56 92.62 1E4 93.45 93.45 93.58 93.61 87.6 87.58 93.77 93.8 92.71 92.75 1E5 94.17 94.16 94.18 94.19 86.55 86.56 94.07 94.09 93.16 93.24 1E6 91.2 91.19 92 92.02 74.81 74.8 91.19 91.25 89.98 90.18", "figure_data": "PrecisionCorporateEconomicsGovernmentMarkets\u03c1traintest traintest traintest traintest traintesterm 92"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Reuters Corpus Recall (%) 90.96 90.20 90.25 67.53 67.56 90.49 90.49 88.77 88.78 1E3 91.72 91.69 90.83 90.86 70.42 70.39 91.26 91.23 89.62 89.58 1E4 92.40 92.39 91.47 91.54 72.38 72.36 91.76 91.76 90.48 90.45 1E5 93.46 93.44 92.65 92.71 76.79 76.78 92.26 92.21 91.46 91.47 1E6 93.10 93.08 92.00 92.04 79.84 79.71 91.89 91.90 92.00 91.97", "figure_data": "RecallCorporateEconomicsGovernmentMarkets\u03c1traintest traintest traintest traintest traintesterm 90.97"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "and A.4 in Sections A.2 and A.3, respectively. Based on these lemmas, Lemma A.1 is immediate once we set", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "R(\u03b8) \u2264 1 n n i=1 \u2113(\u03b8, X i ) + C 1 Var(\u2113(\u03b8, X)) n + C 2 n for all \u03b8 \u2208 \u0398 (2)", "formula_coordinates": [1.0, 159.36, 539.9, 380.55, 34.61]}, {"formula_id": "formula_1", "formula_text": "n n i=1 \u2113(\u03b8, X i ) + C Var Pn (\u2113(\u03b8, X)) n ,(3)", "formula_coordinates": [2.0, 221.28, 126.38, 318.63, 34.73]}, {"formula_id": "formula_2", "formula_text": "D \u03c6 (P ||Q) = \u03c6 dP dQ dQ = X \u03c6 p(x) q(x) q(x)d\u00b5(x),", "formula_coordinates": [2.0, 176.16, 331.78, 259.46, 33.07]}, {"formula_id": "formula_3", "formula_text": "P \u2208Pn E P [\u2113(\u03b8, X)] = sup P E P [\u2113(\u03b8, X)] : D \u03c6 (P || P n ) \u2264 \u03c1 n . (4", "formula_coordinates": [2.0, 205.08, 496.54, 330.22, 32.47]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [2.0, 535.3, 503.86, 4.61, 10.91]}, {"formula_id": "formula_5", "formula_text": "R n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1Var Pn (\u2113(\u03b8, X)) n + \u03b5 n (\u03b8),(5)", "formula_coordinates": [2.0, 172.8, 629.62, 367.11, 27.89]}, {"formula_id": "formula_6", "formula_text": "D \u03c6 P || P n \u2264 \u03c1 n(6)", "formula_coordinates": [3.0, 341.16, 149.26, 198.75, 25.97]}, {"formula_id": "formula_7", "formula_text": "C 2 \u03c1 n = inf \u03b8\u2208\u0398 R n (\u03b8, P n ) + C 2 \u03c1 n", "formula_coordinates": [3.0, 340.44, 245.98, 130.8, 26.09]}, {"formula_id": "formula_8", "formula_text": "R( \u03b8 rob n ) = E[\u2113( \u03b8 rob n ; X)] \u2264 inf \u03b8\u2208\u0398 E[\u2113(\u03b8; X)] + 2 2\u03c1 n Var(\u2113(\u03b8; X)) + C\u03c1 n(7)", "formula_coordinates": [3.0, 135.48, 372.1, 404.43, 25.97]}, {"formula_id": "formula_9", "formula_text": "let A + B = {a + b : a \u2208 A, b \u2208 B} denote Minkowski addition. For a convex function f , the subgradient set \u2202f (x) of f at x is \u2202f (x) = {g : f (y) \u2265 f (x) + g \u22a4 (y \u2212 x) for all y}. For a function h : R d \u2192 R, we let h * denote its Fenchel (convex) conjugate, h * (y) = sup x {y \u22a4 x \u2212 h(x)}.", "formula_coordinates": [5.0, 72.0, 361.06, 468.05, 45.78]}, {"formula_id": "formula_10", "formula_text": "p n i=1 p i z i subject to p \u2208 P n = p \u2208 R n + : 1 2 np \u2212 1 2 2 \u2264 \u03c1, 1, p = 1 ,(8)", "formula_coordinates": [5.0, 133.08, 642.14, 406.83, 34.73]}, {"formula_id": "formula_11", "formula_text": "\u22124 \u22123 \u22122 \u22121 0 1 2 3 4 0.8 1.0 1.2 1.4 1.6 1.8 Figure 1. Plot of \u03b8 \u2192 Var(\u2113(\u03b8, X)) for \u2113(\u03b8; X) = |\u03b8 \u2212 X| where X \u223c Uni({\u22122, \u22121, 0, 1, 2}).", "formula_coordinates": [6.0, 88.32, 78.95, 413.52, 200.02]}, {"formula_id": "formula_12", "formula_text": "s 2 n = 1 n z 2 2 \u2212 (z) 2 = 1 n z \u2212 z 2", "formula_coordinates": [6.0, 285.48, 301.22, 152.43, 21.49]}, {"formula_id": "formula_13", "formula_text": "u = p \u2212 1 n 1, the objective in problem (8) satisfies p, z = z + u, z = z + u, z \u2212 z because u, 1 = 0. Thus problem (8) is equivalent to solving maximize u\u2208R n z + u, z \u2212 z subject to u 2 2 \u2264 2\u03c1 n 2 , 1, u = 0, u \u2265 \u2212 1 n .", "formula_coordinates": [6.0, 76.2, 329.06, 463.84, 61.09]}, {"formula_id": "formula_14", "formula_text": "u i = \u221a 2\u03c1(z i \u2212 z) n z \u2212 z 2 = \u221a 2\u03c1(z i \u2212 z) n ns 2 n .", "formula_coordinates": [6.0, 226.68, 426.58, 158.54, 41.45]}, {"formula_id": "formula_15", "formula_text": "i\u2208[n] \u221a 2\u03c1(z i \u2212 z) ns 2 n \u2265 \u22121.(9)", "formula_coordinates": [6.0, 250.92, 485.38, 288.99, 37.3]}, {"formula_id": "formula_16", "formula_text": "sup p\u2208Pn p, z = z + 2\u03c1s 2 n n .", "formula_coordinates": [6.0, 247.56, 553.7, 116.9, 27.89]}, {"formula_id": "formula_17", "formula_text": ", X i ) \u2212 E Pn [\u2113(\u03b8, X i )]", "formula_coordinates": [6.0, 327.26, 591.94, 95.0, 18.65]}, {"formula_id": "formula_18", "formula_text": "R n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1Var Pn (\u2113(\u03b8, X)) n .", "formula_coordinates": [6.0, 191.88, 631.54, 228.26, 27.89]}, {"formula_id": "formula_19", "formula_text": "[M 0 , M 1 ], and let M = M 1 \u2212 M 0 . Let \u03c3 2 = Var(Z) and s 2 n = E Pn [Z 2 ] \u2212 E Pn [Z] 2", "formula_coordinates": [7.0, 72.0, 109.9, 467.98, 32.21]}, {"formula_id": "formula_20", "formula_text": "2\u03c1 n s 2 n \u2212 2M \u03c1 n + \u2264 sup P E P [Z] : D \u03c6 (P || P n ) \u2264 \u03c1 n \u2212 E Pn [Z] \u2264 2\u03c1 n s 2 n .(10)", "formula_coordinates": [7.0, 149.88, 161.62, 389.91, 31.65]}, {"formula_id": "formula_21", "formula_text": "M 2 \u03c3 2 max {8\u03c3, 44} , with probability at least 1 \u2212 exp \u2212 n\u03c3 2 11M 2 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [Z] = E Pn [Z] + 2\u03c1 n s 2 n .(11)", "formula_coordinates": [7.0, 202.68, 203.96, 337.11, 66.82]}, {"formula_id": "formula_22", "formula_text": "R n (F) := E sup f \u2208F 1 n n i=1 \u03b5 i f (x i ) .", "formula_coordinates": [7.0, 227.52, 443.3, 156.98, 36.63]}, {"formula_id": "formula_23", "formula_text": "R sup n (F) := sup x 1 ,...,xn\u2208X E sup f \u2208F 1 n n i=1 \u03b5 i f (x i ) .", "formula_coordinates": [7.0, 197.4, 509.06, 217.1, 36.75]}, {"formula_id": "formula_24", "formula_text": "have the inequalities E[R n (F)] \u2264 R sup n (F) M d n .", "formula_coordinates": [7.0, 72.0, 566.9, 251.06, 21.73]}, {"formula_id": "formula_25", "formula_text": "f : X \u2192 [M 0 , M 1 ] where M = M 1 \u2212 M 0 , and M \u2264 n. There exists a universal constant C such that if \u03c4 2 > 0 satisfies \u03c4 2 \u2265 4\u03c1M 2 n + C R sup n (F) 2 log 3 n + M 2 n (t + log log n) .", "formula_coordinates": [7.0, 72.0, 634.06, 467.98, 61.61]}, {"formula_id": "formula_26", "formula_text": "P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] = E Pn [f (X)] + 2\u03c1 n Var Pn (f (X)) (12) for all f \u2208 F such that Var(f ) \u2265 \u03c4 2 .", "formula_coordinates": [8.0, 72.0, 97.9, 467.79, 61.37]}, {"formula_id": "formula_27", "formula_text": "R(\u03b8) := E [\u2113(Y \u03b8, X )] ,", "formula_coordinates": [8.0, 250.8, 328.66, 110.3, 10.91]}, {"formula_id": "formula_28", "formula_text": "y 1 , x 1 , . . . , y n , x n that E sup \u03b8\u2208\u0398 n i=1 \u03b5 i [\u2113(y i \u03b8, x i ) \u2212 \u2113(0)] \u2264 E sup \u03b8\u2208\u0398 n i=1 \u03b5 i \u03b8, x i \u2264 r(\u0398)E n i=1 \u03b5 i x i * .(13)", "formula_coordinates": [8.0, 98.64, 378.7, 441.15, 58.75]}, {"formula_id": "formula_29", "formula_text": "E[ n i=1 \u03b5 i x i ] \u2264 E d j=1 n i=1 \u03b5 i x ij 2 \u2264 r(X ) \u221a n.", "formula_coordinates": [8.0, 190.8, 493.7, 230.42, 37.01]}, {"formula_id": "formula_30", "formula_text": "P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [\u2113(Y \u03b8, X )] = E Pn [\u2113(Y \u03b8, X )] + 2\u03c1 n Var Pn (\u2113(Y \u03b8, X )) for all \u03b8 satisfying Var(\u2113(Y \u03b8, X )) \u2265 r(X ) 2 r(\u0398) 2 n 4\u03c1 + C log 3 n + Ct ,", "formula_coordinates": [8.0, 72.0, 565.54, 407.79, 79.49]}, {"formula_id": "formula_31", "formula_text": "d | \u03b8 1 \u2264 r 1 (\u0398)} and \u2022 * = \u2022 \u221e , then E[ n i=1 \u03b5 i x i \u221e ] \u2264 r(X ) n log(2d)", "formula_coordinates": [9.0, 72.0, 72.98, 468.02, 33.97]}, {"formula_id": "formula_32", "formula_text": "R sup n (F) sup x 1 ,...,xn\u2208X r 1 (\u0398) n E n i=1 \u03b5 i x i \u221e \u2264 r 1 (\u0398)r \u221e (X ) log(2d) n .", "formula_coordinates": [9.0, 141.0, 124.22, 330.02, 38.07]}, {"formula_id": "formula_33", "formula_text": "\u2265 r 1 (\u0398) 2 r\u221e(X ) 2 n [4\u03c1 + C log d \u2022 log 3 n + Ct]. \u2738", "formula_coordinates": [9.0, 199.56, 181.88, 208.67, 23.24]}, {"formula_id": "formula_34", "formula_text": "v 1 , . . . , v N \u2282 V is an \u01eb-cover of V if for each v \u2208 V, there exists v i such that v \u2212 v i \u2264 \u01eb.", "formula_coordinates": [9.0, 72.0, 282.46, 467.12, 32.21]}, {"formula_id": "formula_35", "formula_text": "N (V, \u01eb, \u2022 ) := inf {N \u2208 N :", "formula_coordinates": [9.0, 129.84, 334.06, 128.9, 18.65]}, {"formula_id": "formula_36", "formula_text": "f \u2212 g L \u221e (X ) := sup x\u2208X |f (x) \u2212 g(x)|.", "formula_coordinates": [9.0, 228.84, 383.02, 159.5, 19.65]}, {"formula_id": "formula_37", "formula_text": "N \u221e (F, \u01eb, n) = sup x\u2208X n N (F(x), \u01eb, \u2022 \u221e ) ,", "formula_coordinates": [9.0, 215.64, 454.18, 180.74, 19.65]}, {"formula_id": "formula_38", "formula_text": "N \u221e (F, \u01eb, n) \u2264 N (F, \u01eb, \u2022 L \u221e (X )", "formula_coordinates": [9.0, 72.0, 494.38, 148.85, 18.65]}, {"formula_id": "formula_39", "formula_text": "\u2022 L 2 (Pn) the empirical L 2 -norm on functions f : X \u2192 [\u2212M, M ], then E 1 n sup f \u2208F n i=1 \u03b5 i f (x i ) inf \u03b4\u22650 \u03b4 + 1 \u221a n M \u03b4 log N (F, \u01eb, \u2022 L 2 (Pn) )d\u01eb \u2264 inf \u03b4\u22650 \u03b4 + 1 \u221a n M \u03b4 log N \u221e (F, \u01eb, n)d\u01eb .(14)", "formula_coordinates": [9.0, 72.0, 507.98, 467.98, 104.45]}, {"formula_id": "formula_40", "formula_text": "x) \u2212 \u2113(\u03b8 \u2032 , x)| \u2264 L \u03b8 \u2212 \u03b8 \u2032 2 . Then taking F = {\u2113(\u03b8, \u2022) : \u03b8 \u2208 \u0398}, any \u01eb-covering {\u03b8 1 , . . . , \u03b8 N } of \u0398 in \u2113 2 -norm guarantees that min i |\u2113(\u03b8, x) \u2212 \u2113(\u03b8 i , x)| \u2264 L\u01eb for all \u03b8, x. That is, N (F, \u01eb, \u2022 L \u221e (X ) ) \u2264 N (\u0398, \u01eb/L, \u2022 2 ) \u2264 1 + diam(\u0398)L \u01eb d ,", "formula_coordinates": [9.0, 285.96, 661.7, 254.06, 20.41]}, {"formula_id": "formula_41", "formula_text": "R sup n (F) d n diam(\u0398)L 0 log diam(\u0398)L \u01eb d\u01eb diam(\u0398)L d n .", "formula_coordinates": [10.0, 156.36, 188.66, 299.18, 29.57]}, {"formula_id": "formula_42", "formula_text": "\u2265 4M 2 \u03c1 n + Cd diam(\u0398) 2 L 2 log 3 n n", "formula_coordinates": [10.0, 366.36, 227.48, 129.09, 23.48]}, {"formula_id": "formula_43", "formula_text": "X \u2192 [M 0 , M 1 ] with M = M 1 \u2212 M 0 .", "formula_coordinates": [10.0, 365.28, 648.1, 169.7, 18.65]}, {"formula_id": "formula_44", "formula_text": "P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 11 3 M \u03c1 n + 2 + 4 2t n \u01eb (15)", "formula_coordinates": [11.0, 213.12, 112.06, 326.67, 34.87]}, {"formula_id": "formula_45", "formula_text": "f \u2208 argmin f \u2208F sup P E P [f (X)] : D \u03c6 (P || P n ) \u2264 \u03c1 n", "formula_coordinates": [11.0, 190.8, 171.94, 213.79, 32.71]}, {"formula_id": "formula_46", "formula_text": "E[ f (X)] \u2264 inf f \u2208F E[f ] + 2 2\u03c1 n Var(f ) + 19M \u03c1 3n + 2 + 4 2t n \u01eb.(16)", "formula_coordinates": [11.0, 148.2, 231.46, 391.59, 30.43]}, {"formula_id": "formula_47", "formula_text": "1 \u2212 c VC(F) 16M ne \u01eb VC(F )\u22121 + 2 e \u2212t .", "formula_coordinates": [11.0, 204.48, 440.42, 202.94, 29.17]}, {"formula_id": "formula_48", "formula_text": "sup Q N (F, \u01eb, \u2022 L 1 (Q) ) \u2264 cVC(F) 8M e \u01eb VC(F )\u22121 for a numerical constant c. Because x \u221e \u2264 x 1 , taking Q to be uniform on x \u2208 X 2n yields N (F(x), \u01eb, \u2022 \u221e ) \u2264 N (F, \u01eb 2n , \u2022 L 1 (Q)", "formula_coordinates": [11.0, 72.0, 516.62, 467.98, 73.35]}, {"formula_id": "formula_49", "formula_text": "n \u2208 argmin \u03b8\u2208\u0398 R n (\u03b8, P n ). Assume also that \u2113(\u03b8, x) \u2208 [0, M ] for all \u03b8 \u2208 \u0398, x \u2208 X . Then if n \u2265 \u03c1 \u2265 9 log 12, R( \u03b8 rob n ) \u2264 R n ( \u03b8 rob n , P n ) + 11M \u03c1 3n + 2M n 1 + \u03c1 n \u2264 inf \u03b8\u2208\u0398 R(\u03b8) + 2 2\u03c1 n Var(\u2113(\u03b8; X)) + 11M \u03c1 n with probability at least 1 \u2212 2 exp(c 1 d log n \u2212 c 2 \u03c1)", "formula_coordinates": [12.0, 72.0, 74.74, 473.38, 96.05]}, {"formula_id": "formula_50", "formula_text": "If D := diam(\u0398) = sup \u03b8,\u03b8 \u2032 \u2208\u0398 \u03b8 \u2212 \u03b8 \u2032 2 < \u221e, then \u2113(\u03b8, x) \u2264 L diam(\u0398)", "formula_coordinates": [12.0, 72.0, 186.7, 467.97, 32.21]}, {"formula_id": "formula_51", "formula_text": "\u03c1 = log 2 \u03b4 + d log(2nDL). (17", "formula_coordinates": [12.0, 246.48, 221.62, 288.5, 25.79]}, {"formula_id": "formula_52", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 228.94, 4.81, 10.91]}, {"formula_id": "formula_53", "formula_text": "1 \u2212 \u03b4 = 1 \u2212 1/n, E[\u2113( \u03b8 rob n ; X)] = R( \u03b8 rob n ) \u2264 inf \u03b8\u2208\u0398 R(\u03b8) + C d Var(\u2113(\u03b8, X)) n log n + C dLD log n n (18", "formula_coordinates": [12.0, 115.92, 269.62, 419.07, 51.65]}, {"formula_id": "formula_54", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 302.62, 4.81, 10.91]}, {"formula_id": "formula_55", "formula_text": "R( \u03b8 erm n ) \u2264 R(\u03b8 \u22c6 ) + C Comp n M R(\u03b8 \u22c6 ) n + C Comp n M n (19", "formula_coordinates": [12.0, 179.76, 565.94, 355.23, 27.01]}, {"formula_id": "formula_56", "formula_text": ")", "formula_coordinates": [12.0, 534.98, 574.3, 4.81, 10.91]}, {"formula_id": "formula_57", "formula_text": "\u2113(\u03b8; (x, y)) := |y \u2212 \u03b8, x |. For some \u03b8 \u22c6 \u2208 \u0398, assume that Y = \u03b8 \u22c6 , X + \u01eb where \u01eb \u2208 [\u2212B, B] is independent of X. We have the global bound \u2113(\u03b8; (X, Y )) \u2264 DL + B =: M.", "formula_coordinates": [13.0, 72.0, 112.06, 317.93, 99.17]}, {"formula_id": "formula_58", "formula_text": "\u22c6 ) = E[\u2113(\u03b8 \u22c6 ; Z)] = 1 2 B. In this case, Var (\u2113(\u03b8 \u22c6 ; Z)) = B 2 12 \u2264 1 2 (DL + B)B = M E[\u2113(\u03b8 \u22c6 ; Z)] = M R(\u03b8 \u22c6 ).", "formula_coordinates": [13.0, 72.0, 214.58, 467.96, 65.29]}, {"formula_id": "formula_59", "formula_text": "F := {f \u03b8 (x, y) = | \u03b8, x \u2212 y| | \u03b8 \u2208 \u0398} satisfy log N (F, \u01eb, \u2022 L \u221e (X ) )", "formula_coordinates": [13.0, 72.0, 287.38, 467.96, 32.21]}, {"formula_id": "formula_60", "formula_text": "R( \u03b8 rob n ) \u2264 R(\u03b8 \u22c6 ) + C d log n n B 2 + C d(LD + B) log n n", "formula_coordinates": [13.0, 177.0, 342.7, 256.75, 26.09]}, {"formula_id": "formula_61", "formula_text": "R( \u03b8 erm n ) \u2264 R(\u03b8 \u22c6 ) + C log n n (BDL + B 2 ) + C (LD + B)", "formula_coordinates": [13.0, 159.0, 416.14, 268.59, 25.97]}, {"formula_id": "formula_62", "formula_text": "\u03b8 erm n = \u2212B, yielding risk R( \u03b8 erm n ) \u2212 R(\u03b8 \u22c6 ) = 2\u03b4B.", "formula_coordinates": [13.0, 72.0, 593.18, 468.14, 33.97]}, {"formula_id": "formula_63", "formula_text": "R( \u03b8 erm n ) \u2264 CdDL log n n and R( \u03b8 rob n ) \u2264 CdDL log n n .", "formula_coordinates": [14.0, 179.28, 149.26, 253.46, 25.97]}, {"formula_id": "formula_64", "formula_text": "E R n cf | f \u2208 F, c \u2208 [0, 1], E[c 2 f 2 \u2264 r]", "formula_coordinates": [14.0, 198.6, 360.26, 194.06, 20.89]}, {"formula_id": "formula_65", "formula_text": "\u03c8 n (r) \u2265 E[R n ({cf : f \u2208 F, c \u2208 [0, 1], E[c 2 f 2 ] \u2264 r})],(20)", "formula_coordinates": [14.0, 184.08, 459.38, 355.71, 20.89]}, {"formula_id": "formula_66", "formula_text": "E[f ] \u2264 E Pn [f ] + 1 \u03b7 E Pn [f ] + C(1 + \u03b7) r \u22c6 n + 1 n + t n", "formula_coordinates": [14.0, 122.4, 557.62, 244.38, 26.09]}, {"formula_id": "formula_67", "formula_text": "\u03c1 n \u2265 8 45M n t + log log n t + 18r \u22c6 n .(21)", "formula_coordinates": [14.0, 209.28, 669.7, 330.51, 25.97]}, {"formula_id": "formula_68", "formula_text": "E[f ] \u2264 1 + 2 2\u03c1 n sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f ] + 13 + 4 2\u03c1 n M \u03c1 n for all f \u2208 F.(22)", "formula_coordinates": [15.0, 120.48, 100.54, 419.31, 34.87]}, {"formula_id": "formula_69", "formula_text": "E[ f ] \u2264 1 + 2 2\u03c1 n inf f \u2208F E[f ] + 91\u03c1 45n Var(f ) + 14 + 6 2\u03c1 n M (3\u03c1 + t) n .(23)", "formula_coordinates": [15.0, 118.44, 176.86, 421.35, 30.55]}, {"formula_id": "formula_70", "formula_text": "(1 + \u03c1/n) E[f ] + \u03c1 n Var(f ) \u2264 E[f ] + C \u03c1 n Var(f ) (for a constant C > 1+ \u03c1/n) if and only if (C \u22121\u2212 \u03c1/n) 2 Var(f ) \u2265 E[f ] 2 .", "formula_coordinates": [15.0, 72.0, 340.42, 367.47, 58.61]}, {"formula_id": "formula_71", "formula_text": "E[f 2 ] \u2265 C 2 \u22122C+2 C 2 \u22122C+1 E[f ] 2", "formula_coordinates": [15.0, 226.92, 392.24, 105.99, 23.25]}, {"formula_id": "formula_72", "formula_text": "T K : L 2 (X , P ) \u2192 L 2 (X , P ) defined by T K (f )(x) = K(x, z)dP (z) is compact, and K(x, x \u2032 ) = \u221e j=1 \u03bb j \u03c6 j (x)\u03c6 j (z)", "formula_coordinates": [15.0, 72.0, 570.02, 467.78, 28.73]}, {"formula_id": "formula_73", "formula_text": "E[\u03be | X] = 0, E[\u03be 2 ] \u2264 \u03c3 2 , and h \u22c6 \u2208 B H . Let the function class {\u2113 \u2022 H} \u2264r := (x, y) \u2192 c\u2113(h(x), y) | c \u2208 [0, 1], c 2 E[\u2113(h(X), Y ) 2 ] \u2264 r .", "formula_coordinates": [15.0, 72.0, 639.02, 468.02, 58.45]}, {"formula_id": "formula_74", "formula_text": "R n ({\u2113 \u2022 H} \u2264r ) = E 1 n sup h\u2208B H ,c\u2208[0,1] \u03b5 i c\u2113(h(x i ), y i ) | E[\u2113(h(X), Y ) 2 ] \u2264 r/c 2 .", "formula_coordinates": [16.0, 122.04, 100.66, 367.94, 27.45]}, {"formula_id": "formula_75", "formula_text": "R n ({\u2113 \u2022 H} \u2264r ) r/n + \uf8eb \uf8ed 1 n \u221e j=1 min{\u03bb j , r} \uf8f6 \uf8f8 1 2 . (24", "formula_coordinates": [16.0, 189.0, 146.08, 345.98, 45.15]}, {"formula_id": "formula_76", "formula_text": ")", "formula_coordinates": [16.0, 534.98, 167.98, 4.81, 10.91]}, {"formula_id": "formula_77", "formula_text": "K(x, x \u2032 ) = exp(\u2212 1 2 x \u2212 x \u2032 2 2", "formula_coordinates": [16.0, 399.24, 265.46, 135.99, 21.61]}, {"formula_id": "formula_78", "formula_text": "\u22c6 n = \u221a log n n \uf8eb \uf8ed 1 n \u221e j=1 min e \u2212j 2 , log n n \uf8f6 \uf8f8 1 2 \u2248 \uf8eb \uf8ed 1 n \u221a log n j=1 \u221a log n n + 1 n \u221e \u221a log n e \u2212t 2 dt \uf8f6 \uf8f8 1 2 \u221a log n n = r \u22c6 n .", "formula_coordinates": [16.0, 99.0, 331.1, 414.02, 73.61]}, {"formula_id": "formula_79", "formula_text": "\u2212 1 2\u03b1 = j, so \u221e j=1 min{j \u22122\u03b1 , r} \u2248 r 2\u03b1\u22121 2\u03b1 + \u221e r \u22121/2\u03b1 t \u22122\u03b1 dt \u224d r 2\u03b1\u22121 2\u03b1 . Solving for nr = r 2\u03b1\u22121 2\u03b1 , we find the fixed point (r \u22c6 n ) 2\u03b1\u22121 4\u03b1 = r \u22c6 n \u221a n yields r \u22c6 n = n \u2212 2\u03b1 2\u03b1+1", "formula_coordinates": [16.0, 72.0, 417.32, 467.9, 84.6]}, {"formula_id": "formula_80", "formula_text": "E[\u2113(h(X), Y )] \u2264 (1 + 2 2\u03c1/n) E Pn [\u2113(h(X), Y )] + 2\u03c1 n Var Pn (\u2113(h(X), Y )) + C\u03c1 n for all h \u2208 B H", "formula_coordinates": [16.0, 72.0, 542.5, 467.53, 26.09]}, {"formula_id": "formula_81", "formula_text": "D \u03c6 (P || Pn)\u2264\u03c1/n E P [\u2113(h(X), Y )] and \u03c1 \u224d n 1\u2212 2\u03b1 2\u03b1+1 , then E \u2113( h(X), Y ) \u2264 1 + Cn \u2212 \u03b1 2\u03b1+1 inf h\u2208B H E[\u2113(h(X), Y )] + Cn \u2212 \u03b1 2\u03b1+1 Var(\u2113(h(X), Y )) + Cn \u2212 2\u03b1 2\u03b1+1 .", "formula_coordinates": [16.0, 73.32, 596.36, 465.38, 53.96]}, {"formula_id": "formula_82", "formula_text": "P (X = 1) = 1 \u2212 \u03b4 2 , P (X = \u22121) = 1 \u2212 \u03b4 2 , P (X = 0) = \u03b4.(25)", "formula_coordinates": [17.0, 167.16, 281.5, 372.63, 26.09]}, {"formula_id": "formula_83", "formula_text": "R(\u03b8) = \u03b4|\u03b8| + 1 \u2212 \u03b4 2 |\u03b8 \u2212 1| + 1 \u2212 \u03b4 2 |\u03b8 + 1| \u2212 (1 \u2212 \u03b4).", "formula_coordinates": [17.0, 184.68, 337.54, 242.42, 25.97]}, {"formula_id": "formula_84", "formula_text": "\u03b8 erm n := argmin \u03b8\u2208R E Pn [\u2113(\u03b8, X)] = argmin \u03b8\u2208[\u22121,1] E Pn [|\u03b8 \u2212 X|].", "formula_coordinates": [17.0, 184.44, 435.26, 243.02, 22.37]}, {"formula_id": "formula_85", "formula_text": "(x) = 1 \u221a 2\u03c0 x \u2212\u221e e \u2212 1 2 t 2 dt denotes the standard Gaussian CDF. (See Section G.2 for a proof.) Lemma 3.1. Let the loss \u2113(\u03b8; x) = |\u03b8 \u2212 x| \u2212 |x|, \u03b4 \u2208 [0, 1], and X follow the distribution (25). Then R( \u03b8 erm n ) \u2212 R(\u03b8 \u22c6 ) \u2265 \u03b4 with probability at least 2\u03a6 \u2212 n\u03b4 2 1 \u2212 \u03b4 2 \u2212 (1 \u2212 \u03b4 2 ) n 2 8 \u03c0n .", "formula_coordinates": [17.0, 72.0, 496.52, 468.01, 115.16]}, {"formula_id": "formula_86", "formula_text": "\u2022) : \u03b8 \u2208 \u0398}, \u01eb, \u2022 L \u221e (X ) ) \u2264 2 log 1", "formula_coordinates": [17.0, 382.56, 630.74, 153.27, 20.77]}, {"formula_id": "formula_87", "formula_text": "R( \u03b8 rob n ) \u2264 R(\u03b8 \u22c6 ) + 15\u03c1 n with probability \u2265 1 \u2212 4 exp (2 log n \u2212 \u03c1) .", "formula_coordinates": [17.0, 146.52, 672.58, 318.86, 25.98]}, {"formula_id": "formula_88", "formula_text": "n n\u22121 ) \u2212 2 \u221a 2/ \u221a \u03c0en \u2265 2\u03a6(\u2212 n n\u22121 ) \u2212 n \u2212 1 2 , R( \u03b8 erm n ) \u2265 R(\u03b8 \u22c6 ) + n \u2212 1 2 .", "formula_coordinates": [18.0, 248.28, 135.94, 286.34, 58.38]}, {"formula_id": "formula_89", "formula_text": "\u03b8 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [\u2113(\u03b8, (X, Y ))] = argmin \u03b8 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [\u2113(A \u22121 \u03b8, (AX, Y ))] = \u03b8 rob n .", "formula_coordinates": [18.0, 91.44, 477.74, 443.66, 29.79]}, {"formula_id": "formula_90", "formula_text": "S \u01eb \u22c6 := \u03b8 \u2208 \u0398 : R(\u03b8) \u2264 inf \u03b8 \u22c6 \u2208\u0398 R(\u03b8 \u22c6 ) + \u01eb", "formula_coordinates": [19.0, 213.12, 246.38, 177.58, 25.47]}, {"formula_id": "formula_91", "formula_text": "S \u01eb \u22c6 := \u03b8 \u2208 \u0398 : R n (\u03b8, P n ) \u2264 inf \u03b8 \u2032 \u2208\u0398 R n (\u03b8 \u2032 , P n ) + \u01eb .", "formula_coordinates": [19.0, 188.76, 305.18, 234.38, 25.47]}, {"formula_id": "formula_92", "formula_text": "R(\u03b8) \u2212 inf \u03b8\u2208\u0398 R(\u03b8) \u2265 \u03bb dist(\u03b8, S \u22c6 ) \u03b3 for all \u03b8 such that dist(\u03b8, S \u22c6 ) \u2264 r. (26", "formula_coordinates": [19.0, 145.44, 460.94, 389.54, 21.01]}, {"formula_id": "formula_93", "formula_text": ")", "formula_coordinates": [19.0, 534.99, 463.3, 4.81, 10.91]}, {"formula_id": "formula_94", "formula_text": "Let t > 0. If 0 \u2264 \u01eb \u2264 1 2 \u03bbr \u03b3 satisfies \u01eb \u2265 2 8 \u03b3 L \u03b3 \u03bb 1 \u03b3\u22121 \u03c1 n \u03b3 2(\u03b3\u22121) and \u01eb 2 \u2265 2E[R n (S 2\u01eb \u22c6 )] + L 2\u01eb \u03bb 1 \u03b3 2t n , (27", "formula_coordinates": [19.0, 72.0, 490.46, 462.98, 55.09]}, {"formula_id": "formula_95", "formula_text": ")", "formula_coordinates": [19.0, 534.98, 526.9, 4.81, 10.91]}, {"formula_id": "formula_96", "formula_text": "then P( S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 ) \u2265 1 \u2212 e \u2212t ,", "formula_coordinates": [19.0, 72.0, 557.06, 132.7, 20.41]}, {"formula_id": "formula_97", "formula_text": "P( S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 ) \u2265 1 \u2212 e \u2212t for \u01eb L \u03b3 \u03bb 1 \u03b3\u22121 d n log n d + t n + \u03c1 n \u03b3 2(\u03b3\u22121)", "formula_coordinates": [19.0, 208.2, 640.94, 237.03, 54.55]}, {"formula_id": "formula_98", "formula_text": "Proof That S \u22c6 is a singleton implies that S 2\u01eb \u22c6 \u2282 {\u03b8 | \u03b8 \u2212 \u03b8 \u22c6 \u2264 (2\u01eb/\u03bb) 1 \u03b3 }.", "formula_coordinates": [20.0, 72.0, 142.76, 360.26, 23.72]}, {"formula_id": "formula_99", "formula_text": "E Pn [\u2113(\u03b8; X) \u2212 \u2113(\u03b8 \u22c6 ; X)] \u2212 E Pn [\u2113(\u03b8 \u2032 ; X) \u2212 \u2113(\u03b8 \u22c6 ; X)] \u2264 L \u03b8 \u2212 \u03b8 \u2032 , so that an \u01eb/L-cover of {\u03b8 | \u03b8 \u2212 \u03b8 \u22c6 \u2264 (2\u01eb/\u03bb) 1 \u03b3 } is an \u01eb-cover of the function class F = {f (x) = \u2113(\u03b8; x) \u2212 \u2113(\u03b8 \u22c6 ; x) | \u03b8 \u2208 S 2\u01eb \u22c6 } in \u2022 L 2 (Pn)", "formula_coordinates": [20.0, 72.0, 180.14, 467.96, 62.77]}, {"formula_id": "formula_100", "formula_text": "E[R n (S 2\u01eb \u22c6 )] 1 \u221a n \u221e 0 log N (F, \u03b4, \u2022 L 2 (Pn) )d\u03b4 1 \u221a n L(2\u01eb/\u03bb) 1 \u03b3 0 d log L \u03b4 d\u03b4 \u2264 L d n 2\u01eb \u03bb 1 \u03b3 1 + 1 \u03b3 log \u03bb 2L \u03b3 \u01eb", "formula_coordinates": [20.0, 129.48, 254.06, 351.82, 66.17]}, {"formula_id": "formula_101", "formula_text": "\u03b5 0 log L \u03b4 d\u03b4 \u2264 \u03b5 1 + log L \u03b5 .", "formula_coordinates": [20.0, 202.92, 331.7, 133.7, 22.09]}, {"formula_id": "formula_102", "formula_text": "\u03b8 \u22c6 := argmin \u03b8 R(\u03b8) and \u2207 2 R(\u03b8 \u22c6 ) \u227b 0,", "formula_coordinates": [20.0, 215.4, 523.22, 181.1, 21.89]}, {"formula_id": "formula_103", "formula_text": "X \u2192 R + satisfying |\u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u2032 , x)| \u2264 L(x) \u03b8 \u2212 \u03b8 \u2032 2 for \u03b8, \u03b8 \u2032 \u2208 \u03b8 \u22c6 + \u01ebB and E[L(X) 2 ] \u2264 L(P ) < \u221e.", "formula_coordinates": [20.0, 72.0, 612.58, 383.14, 60.89]}, {"formula_id": "formula_104", "formula_text": "E[H(X) 2 ] < \u221e.", "formula_coordinates": [20.0, 72.0, 680.18, 75.22, 20.41]}, {"formula_id": "formula_105", "formula_text": "P n ). Define b(\u03b8 \u22c6 ) := Cov(\u2207 \u03b8 \u2113(\u03b8 \u22c6 , X), \u2113(\u03b8 \u22c6 , X)) Var(\u2113(\u03b8 \u22c6 , X)) and \u03a3(\u03b8 \u22c6 ) = \u2207 2 R(\u03b8 \u22c6 ) \u22121 Cov(\u2207\u2113(\u03b8 \u22c6 , X)) \u2207 2 R(\u03b8 \u22c6 ) \u22121 .", "formula_coordinates": [21.0, 72.0, 109.06, 478.06, 60.95]}, {"formula_id": "formula_106", "formula_text": "\u2192 \u03b8 \u22c6 and \u221a n( \u03b8 rob n \u2212 \u03b8 \u22c6 ) d \u2192 N \u2212 2\u03c1 b(\u03b8 \u22c6 ), \u03a3(\u03b8 \u22c6 )", "formula_coordinates": [21.0, 123.72, 181.22, 272.43, 36.97]}, {"formula_id": "formula_107", "formula_text": "n R( \u03b8 rob n ) \u2212 R(\u03b8 \u22c6 ) d \u2192 1 2 2\u03c1 b(\u03b8 \u22c6 ) + W 2 \u2207 2 R(\u03b8 \u22c6 ) ,(28)", "formula_coordinates": [21.0, 183.96, 296.86, 355.83, 30.79]}, {"formula_id": "formula_108", "formula_text": "(\u03b8) \u2212 R(\u03b8 \u22c6 ) = 1 2 (\u03b8 \u2212 \u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )(\u03b8 \u2212 \u03b8 \u22c6 ) + o( \u03b8 \u2212 \u03b8 \u22c6 2 ), or n(R( \u03b8 rob n ) \u2212 R(\u03b8 \u22c6 )) = n 1 2 ( \u03b8 rob n \u2212 \u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + o( \u03b8 rob n \u2212 \u03b8 \u22c6 2 ) = 1 2 \u221a n( \u03b8 rob n \u2212 \u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 ) \u221a n( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + o P (1) d \u2192 1 2 ( 2\u03c1 b(\u03b8 \u22c6 ) + W ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )( 2\u03c1 b(\u03b8 \u22c6 ) + W )", "formula_coordinates": [21.0, 112.15, 345.26, 373.17, 105.37]}, {"formula_id": "formula_109", "formula_text": "1 2 E[ 2\u03c1b(\u03b8 \u22c6 ) + W 2 \u2207 2 R(\u03b8 \u22c6 ) ] = \u03c1b(\u03b8 \u22c6 ) \u22a4 \u2207 2 R(\u03b8 \u22c6 )b(\u03b8 \u22c6 ) + 1 2 tr(\u2207 2 R(\u03b8 \u22c6 ) \u22121 Cov(\u2113(\u03b8 \u22c6 , X)),", "formula_coordinates": [21.0, 102.84, 489.94, 407.3, 26.59]}, {"formula_id": "formula_110", "formula_text": "1 2 tr(\u2207 2 R(\u03b8 \u22c6 ) \u22121 Cov(\u2113(\u03b8 \u22c6 , X))). Thus there is an additional \u03c1 b(\u03b8 \u22c6 ) 2 \u2207 2 R(\u03b8 \u22c6 )", "formula_coordinates": [21.0, 185.04, 533.54, 354.41, 21.87]}, {"formula_id": "formula_111", "formula_text": "\u2207\u2113(\u03b8 \u22c6 , (x, y)) = (x \u22a4 \u03b8 \u22c6 \u2212 y)x = (x \u22a4 \u03b8 \u22c6 \u2212 x \u22a4 \u03b8 \u22c6 \u2212 \u03b5)x = \u2212\u03b5x, while \u2113(\u03b8 \u22c6 , (x, y)) = 1 2 \u03b5 2 . The covariance Cov(\u03b5X, \u03b5 2 ) = E[\u03b5X(\u03b5 2 \u2212 \u03c3 2", "formula_coordinates": [21.0, 166.2, 679.58, 279.62, 21.01]}, {"formula_id": "formula_112", "formula_text": ") = {g \u2208 R d : f (\u03b8 \u2032 ) \u2265 f (\u03b8) + g, \u03b8 \u2032 \u2212 \u03b8 for all \u03b8 \u2032 }", "formula_coordinates": [22.0, 310.62, 357.74, 225.07, 20.41]}, {"formula_id": "formula_113", "formula_text": "\u2202 \u03b8 R n (\u03b8, P n ) = \u2202 \u03b8 sup P \u2208Pn E P [\u2113(\u03b8; X)] = n i=1 p * i \u2202 \u03b8 \u2113(\u03b8; X i ),", "formula_coordinates": [22.0, 179.28, 420.86, 253.34, 36.63]}, {"formula_id": "formula_114", "formula_text": "\u2207 \u03b8 R n (\u03b8, P n ) = n i=1 p * i \u2207 \u03b8 \u2113(\u03b8; X i ) where p * = argmax p\u2208Pn n i=1 p i \u2113(\u03b8; X i ) .(29)", "formula_coordinates": [22.0, 136.32, 514.58, 403.47, 34.61]}, {"formula_id": "formula_115", "formula_text": "n = argmin \u03b8\u2208\u0398 E Pn [\u2113(\u03b8, X)] in terms of the true risk E[\u2113(\u03b8, X)] = 1 2 \u03b8 \u2212 v 2 2 .", "formula_coordinates": [23.0, 72.0, 568.54, 467.99, 34.85]}, {"formula_id": "formula_116", "formula_text": "B = .01 B = .1 B = 1 B = 10 n R( \u03b8 erm n ) R( \u03b8 rob n ) R( \u03b8 erm n ) R( \u03b8 rob n ) R( \u03b8 erm n ) R( \u03b8 rob n ) R( \u03b8 erm n ) R( \u03b8", "formula_coordinates": [24.0, 78.0, 87.34, 437.72, 27.69]}, {"formula_id": "formula_117", "formula_text": "\u0398 = \u03b8 \u2208 R d : a 1 \u03b8 1 + a 2 \u03b8 2 \u2264 r ,", "formula_coordinates": [24.0, 214.56, 483.98, 182.9, 21.01]}, {"formula_id": "formula_118", "formula_text": "precision = 1 n n i=1 4 k=1 1{\u03b8 \u22a4 k x i \u2265 0, y i = 1} 4 k=1 1{\u03b8 \u22a4 k x i > 0} , recall = 1 n n i=1 4 k=1 1{\u03b8 \u22a4 k x i \u2265 0, y i = 1} 4 k=1 1 {y i = 1}", "formula_coordinates": [28.0, 200.28, 515.06, 211.46, 75.37]}, {"formula_id": "formula_119", "formula_text": "on the function class {x \u2192 \u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u22c6 , x) | \u03b8 \u2208 \u0398} .", "formula_coordinates": [31.0, 72.0, 74.74, 467.93, 45.77]}, {"formula_id": "formula_120", "formula_text": "G r = r E[f 2 ] \u2228 r f | f \u2208 F .", "formula_coordinates": [31.0, 232.8, 226.18, 146.42, 33.53]}, {"formula_id": "formula_121", "formula_text": "G r = r E[f 2 ] \u2228 r f | f \u2208 F .", "formula_coordinates": [31.0, 238.2, 296.38, 135.5, 33.53]}, {"formula_id": "formula_122", "formula_text": "D \u03c6 (P || Pn)\u2264\u03c1/n E P [Z] = E Pn [Z] = E[Z].", "formula_coordinates": [32.0, 72.0, 99.1, 467.96, 25.07]}, {"formula_id": "formula_123", "formula_text": "p n i=1 p i z i subject to p \u2208 P n = p \u2208 R n + : 1 2 np \u2212 1 2 2 \u2264 \u03c1, 1, p = 1 ,", "formula_coordinates": [32.0, 133.08, 147.98, 366.26, 34.73]}, {"formula_id": "formula_124", "formula_text": "is z + 2\u03c1s 2 n /n whenever 2\u03c1 z i \u2212 z ns 2 n \u2265 \u22121.", "formula_coordinates": [32.0, 82.92, 193.78, 457.04, 53.97]}, {"formula_id": "formula_125", "formula_text": "M 2 ns 2 n \u2264 1, or n \u2265 2\u03c1M 2 s 2 n , or s 2 n \u2265 2\u03c1M 2 n .(30)", "formula_coordinates": [32.0, 209.04, 289.46, 330.75, 29.45]}, {"formula_id": "formula_126", "formula_text": "n n < 4\u03c1 2 M 2 n 2 , which in turn implies that sup p\u2208Pn p, z \u2265 1 n 1, z + 2\u03c1s 2 n n \u2212 2M \u03c1 n + .", "formula_coordinates": [32.0, 200.88, 328.64, 314.79, 63.48]}, {"formula_id": "formula_127", "formula_text": "E n := s 2 n \u2265 3 64 \u03c3 2 , and let n \u2265 4M 2 \u03c3 2 max {2\u03c3, 11}. Then, on event E n we have n \u2265 44\u03c1M 2 \u03c3 2 \u2265 2\u03c1M 2 s 2 n", "formula_coordinates": [32.0, 72.0, 479.62, 401.93, 59.93]}, {"formula_id": "formula_128", "formula_text": "Lemma A.1. Let Z i be i.i.d. random variables taking values in [M 0 , M 1 ] with M = M 1 \u2212 M 0 , and let s 2 n = 1 n n i=1 Z 2 i \u2212 1 n n i=1 Z i 2 . Let c n = 1 + 7 4n + 3 n 2 .", "formula_coordinates": [32.0, 72.0, 585.82, 467.93, 33.77]}, {"formula_id": "formula_129", "formula_text": "P s n \u2265 Es 2 n + t \u2228 P s n \u2264 Es 2 n \u2212 c n M 2 n \u2212 t \u2264 exp \u2212 nt 2 2M 2 .", "formula_coordinates": [32.0, 140.76, 624.26, 330.5, 27.73]}, {"formula_id": "formula_130", "formula_text": "2 n ] = (1 \u2212 1 n )\u03c3 2 . Set t = \u221a 3", "formula_coordinates": [33.0, 256.68, 66.74, 127.47, 27.49]}, {"formula_id": "formula_131", "formula_text": "\u03c3 1 \u2212 n \u22121 \u2212 c n M 2 n \u2212 t \u2265 2 \u221a 5 5 \u03c3 \u2212 3M 2 2n \u2212 t > 7 10 \u03c3 \u2212 t > \u221a 3 8 \u03c3. Lemma A.1 implies that s n \u2265 \u03c3 \u221a 1 \u2212 n \u22121 \u2212 cnM 2 n", "formula_coordinates": [33.0, 72.0, 105.58, 386.42, 63.29]}, {"formula_id": "formula_132", "formula_text": "P(f (Z 1:n ) \u2265 E[f (Z 1:n )] + t) \u2228 P(f (Z 1:n ) \u2264 E[f (Z 1:n )] \u2212 t) \u2264 exp \u2212 t 2 2L 2 (b \u2212 a) 2 . The function R n \u220b z \u2192 (I \u2212 (1/n)11 \u22a4 )z 2 is 1-Lipschitz with respect to the Euclidean norm, so Lemma A.2 implies P Var Pn (Z) \u2265 E[ Var Pn (Z)] + t \u2228 P Var Pn (Z) \u2264 E[ Var Pn (Z)] \u2212 t \u2264 exp \u2212 nt 2 2M 2 . As E[Var Pn (Z) 1 2 ] \u2264 E[Var Pn (Z)] 1 2 = (1 \u2212 1/n)Var(Z)", "formula_coordinates": [33.0, 72.0, 282.5, 468.02, 130.57]}, {"formula_id": "formula_133", "formula_text": "E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 \u221a n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 n n i=1 E[Y 2 i ] (31a) E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 n n i=1 E[Y 2 i ] .(31b)", "formula_coordinates": [33.0, 140.28, 466.88, 399.63, 80.04]}, {"formula_id": "formula_134", "formula_text": "Let Y i = Z i \u2212 1 n n j=1 Z j . Then E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 n E[(Z \u2212 E[Z]) 4 ] Var(Z) + 7 + 12/n n Var(Z) If max j Z j \u2212 min j Z j \u2264 C with probability 1, then for the constant c n = 1 + 7 4n + 3 n 2 , we have E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 c n C 2 n .", "formula_coordinates": [33.0, 72.0, 554.06, 467.98, 142.49]}, {"formula_id": "formula_135", "formula_text": "Y i = Z i \u2212 1 n n j=1 Z j , so that s 2 n = 1 n n i=1 Y 2 i ,", "formula_coordinates": [34.0, 72.0, 85.46, 467.91, 31.97]}, {"formula_id": "formula_136", "formula_text": "inf \u03bb\u22650 a 2 2\u03bb + \u03bb 2 = \u221a a 2 = |a|,", "formula_coordinates": [34.0, 239.16, 170.62, 133.7, 28.85]}, {"formula_id": "formula_137", "formula_text": "a 2 2\u03bb + \u03bb 2 \u2265 a 2 2\u03bb \u2032 + \u03bb \u2032 2 \u2212 a 2 2\u03bb \u2032 2 \u2212 1 2 (\u03bb \u2212 \u03bb \u2032 ). By setting \u03bb n = 1 n n i=1 Y 2 i , we thus have for any \u03bb \u2265 0 that E 1 n n i=1 Y 2 i 1 2 = E n i=1 Y 2 i 2n\u03bb n + \u03bb n 2 \u2265 E n i=1 Y 2 i 2n\u03bb + \u03bb 2 + E 1 2 \u2212 n i=1 Y 2 i 2n\u03bb 2 (\u03bb n \u2212 \u03bb) . Now we take \u03bb = 1 n n i=1 E[Y 2 i ]", "formula_coordinates": [34.0, 72.0, 226.46, 408.14, 158.21]}, {"formula_id": "formula_138", "formula_text": "E 1 n n i=1 Y 2 i 1 2 (32) \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 2\u03bb 2 E \uf8ee \uf8f0 1 n n i=1 (Y 2 i \u2212 E[Y 2 i ]) 2 \uf8f9 \uf8fb 1 2 E \uf8ee \uf8ef \uf8f0 \uf8eb \uf8ed 1 n n i=1 Y 2 i 1 2 \u2212 1 n n i=1 E[Y 2 i ] 1 2 \uf8f6 \uf8f8 2 \uf8f9 \uf8fa \uf8fb 1 2", "formula_coordinates": [34.0, 72.0, 393.56, 477.53, 83.16]}, {"formula_id": "formula_139", "formula_text": "Y 2 i , Y 2 j ) \u2264 \u03c3 4 implies that E \uf8ee \uf8f0 1 n n i=1 (Y 2 i \u2212 E[Y 2 i ]) 2 \uf8f9 \uf8fb = 1 n 2 n i=1 Var(Y 2 i ) + 1 n 2 i =j E (Y 2 i \u2212 E[Y 2 i ])(Y 2 j \u2212 E[Y 2 j ]) \u2264 1 n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 . The triangle inequality implies that E[((n \u22121 n i=1 Y 2 i ) 1 2 \u2212(n \u22121 n i=1 E[Y 2 i ]) 1 2 ) 2 ] 1 2 \u2264 2\u03bb = 2 1 n n i=1 E[Y 2 i ]", "formula_coordinates": [34.0, 72.0, 501.74, 495.01, 134.53]}, {"formula_id": "formula_140", "formula_text": "E 1 n n i=1 Y 2 i 1 2 \u2265 1 n n i=1 E[Y 2 i ] 1 2 \u2212 1 \u03bb \u221a n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 2", "formula_coordinates": [34.0, 132.72, 652.28, 340.01, 38.76]}, {"formula_id": "formula_141", "formula_text": "E \uf8ee \uf8ef \uf8f0 \uf8eb \uf8ed 1 n n i=1 Y 2 i 1 2 \u2212 1 n n i=1 E[Y 2 i ] 1 2 \uf8f6 \uf8f8 2 \uf8f9 \uf8fa \uf8fb = 2 n n i=1 E[Y 2 i ] \u2212 2 1 n n i=1 E[Y 2 i ] 1 2 E 1 n n i=1 Y 2 i 1 2 \u2264 2 \u221a n 1 n n i=1 Var(Y 2 i ) + n\u03c3 4 1 2", "formula_coordinates": [35.0, 76.8, 102.88, 451.01, 95.67]}, {"formula_id": "formula_142", "formula_text": "As Y i = Z i \u2212 1 n n j=1 Z j ,", "formula_coordinates": [35.0, 72.0, 270.14, 116.3, 21.49]}, {"formula_id": "formula_143", "formula_text": "Var(Y 2 1 ) \u2264 n \u2212 1 n E[Z 4 ] + 6 n Var(Z) 2(33a)", "formula_coordinates": [35.0, 221.88, 307.54, 317.91, 26.09]}, {"formula_id": "formula_144", "formula_text": "E[(Y 2 1 \u2212 E[Y 2 1 ])(Y 2 1 \u2212 E[Y 2 1 ])] \u2264 1 + 12/n n 2 Var(Z) 2 . (33b", "formula_coordinates": [35.0, 187.2, 351.1, 347.64, 26.09]}, {"formula_id": "formula_145", "formula_text": ")", "formula_coordinates": [35.0, 534.84, 358.54, 5.07, 10.91]}, {"formula_id": "formula_146", "formula_text": "Note that E[Y 2 i ] = E[(Z i \u2212 1 n n j=1 Z j ) 2 ] = n\u22121 n Var(Z).", "formula_coordinates": [35.0, 88.92, 380.06, 257.42, 21.49]}, {"formula_id": "formula_147", "formula_text": "Var(Y 2 1 ) = E[Y 4 1 ] \u2212 E[Y 2 1 ] 2 = E[(Z 1 \u2212 Z n ) 4 ] \u2212 (n \u2212 1) 2 n 2 Var(Z) 2 ,", "formula_coordinates": [35.0, 155.28, 404.78, 301.46, 27.85]}, {"formula_id": "formula_148", "formula_text": "E[(Z 1 \u2212 Z n ) 4 ] = E[Z 4 ] \u2212 4E[Z 3 1 Z n ] + 6E[Z 2 1 Z 2 n ] \u2212 4E[Z 1 Z 3 n ] + E[Z 4 n ]. Using that E[Z 1 Z 3 n ] = E[Z 4 ]/n 3 + 3(n\u22121) n 3 Var(Z) 2 , E[Z 2 1 Z 2 n ] = 1 n 2 E[Z 4 ] + n\u22121 n 2 Var(Z) 2 , and E[Z 4 n ] \u2264 1 n 3 E[Z 4 ] + 3 n 2 Var(Z) 2 , we obtain E[(Z 1 \u2212 Z n ) 4 ] \u2264 1 \u2212 4 n + 6 n 2 \u2212 3 n 3 E[Z 4 ] + 6 n \u2212 1 n 2 \u2212 4 3n \u2212 3 n 3 + 3 n 2 Var(Z) 2 \u2264 n \u2212 1 n E[Z 4 ] + 6 n Var(Z) 2 ,", "formula_coordinates": [35.0, 72.0, 473.54, 467.96, 123.37]}, {"formula_id": "formula_149", "formula_text": "Y 2 i \u2212 E[Y 2 i ] = Z 2 i + Z 2 n \u2212 2Z i Z n \u2212 n \u2212 1 n Var(Z).", "formula_coordinates": [35.0, 191.16, 641.14, 229.58, 25.97]}, {"formula_id": "formula_150", "formula_text": "E Z 2 1 \u2212 Var(Z) + n \u22121 Var(Z) + Z 2 n \u2212 2Z 1 Z n Z 2 2 \u2212 Var(Z) + n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n = 2E (Z 2 1 \u2212 Var(Z)) n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n + E n \u22121 Var(Z) + Z 2 n \u2212 2Z 1 Z n n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n .(34)", "formula_coordinates": [36.0, 85.44, 96.5, 454.35, 70.09]}, {"formula_id": "formula_151", "formula_text": "E (Z 2 1 \u2212 Var(Z)) n \u22121 Var(Z) + Z 2 n \u2212 2Z 2 Z n = E (Z 2 1 \u2212 Var(Z))Z 2 n = Var(Z 2 ) n 2 .", "formula_coordinates": [36.0, 105.48, 191.42, 401.06, 27.73]}, {"formula_id": "formula_152", "formula_text": "1 n 2 Var(Z) 2 + 2 n Var(Z)E[Z 2 n ] \u2212 4 n Var(Z)E[Z 1 Z n ] + E[Z 4 n ] \u2212 4E[Z 1 Z 3 n ] + 4E[Z 1 Z 2 Z 2 n ] = \u2212 1 n 2 Var(Z) 2 + E[Z 4 n ] \u2212 4E[Z 1 Z 3 n ] + 4E[Z 1 Z 2 Z 2 n ] = Var(Z 2 n ) + 4 E[Z 1 Z 2 Z 2 n ] \u2212 E[Z 1 Z 3 n ] .", "formula_coordinates": [36.0, 86.76, 247.3, 439.7, 51.89]}, {"formula_id": "formula_153", "formula_text": "E[Z 1 Z 3 n ] = E[Z 4 ]/n 3 + 3(n\u22121) n 3 Var(Z) 2 , E[Z 1 Z 2 Z 2 n ] = 2 n 2 E[Z 2 1 Z 2 2 ] = 2 n 2 Var(Z) 2 , and E[Z 4 n ] \u2264 1 n 3 E[Z 4 ] + 3 n 2 Var(Z) 2", "formula_coordinates": [36.0, 72.0, 306.14, 475.1, 39.61]}, {"formula_id": "formula_154", "formula_text": "\u2212 3 n 3 E[Z 4 ] + 3 n 2 Var(Z) 2 + 4 3 \u2212 n n 3 Var(Z) 2 \u2264 12 \u2212 n n 3 Var(Z) 2 .", "formula_coordinates": [36.0, 162.0, 348.82, 288.02, 26.09]}, {"formula_id": "formula_155", "formula_text": ") = E Pn (f \u2212 E[f ]) 2 \u2212 (E Pn (f \u2212 E[f ]))", "formula_coordinates": [36.0, 197.04, 540.74, 180.75, 20.41]}, {"formula_id": "formula_156", "formula_text": "E Pn (f \u2212 E[f ]) 2 . Lemma B.1. Let F be a collection of bounded functions f : X \u2192 [M 0 , M 1 ] with M := M 1 \u2212 M 0 .", "formula_coordinates": [36.0, 72.0, 555.86, 467.98, 41.77]}, {"formula_id": "formula_157", "formula_text": "Var(f ) \u2264 2E Pn (f \u2212 E[f ]) 2 + C R sup n (F) 2 log 3 (nM ) + M 2 n (t + log log n) .", "formula_coordinates": [36.0, 129.36, 612.74, 353.3, 27.73]}, {"formula_id": "formula_158", "formula_text": "F n,r := f \u2212 E[f ] \u2208 F | E Pn [(f \u2212 E[f ]) 2 ] \u2264 r ,", "formula_coordinates": [36.0, 193.56, 678.02, 224.9, 21.01]}, {"formula_id": "formula_159", "formula_text": "\u03c8 sup n (r) \u2265 R sup n (F n,r ),", "formula_coordinates": [37.0, 254.64, 109.1, 102.62, 20.89]}, {"formula_id": "formula_160", "formula_text": "R sup n (F 2 n,r ) \u2264 C \u221a rR sup n (F) log 3 2 n (35", "formula_coordinates": [37.0, 226.68, 166.66, 308.3, 27.05]}, {"formula_id": "formula_161", "formula_text": ")", "formula_coordinates": [37.0, 534.98, 175.06, 4.81, 10.91]}, {"formula_id": "formula_162", "formula_text": "\u03c8 sup n (r) = C \u221a rR sup n (F) log 3 2", "formula_coordinates": [37.0, 72.0, 201.7, 136.37, 19.41]}, {"formula_id": "formula_163", "formula_text": "E(f \u2212 E[f ]) 2 \u2264 2E Pn (f \u2212 E[f ]) 2 + C R sup n (F) 2 log 3 nM + M 2 n (t + log log n)", "formula_coordinates": [37.0, 121.08, 243.62, 364.23, 27.85]}, {"formula_id": "formula_164", "formula_text": "{E[f ] \u2212 E Pn [f ]} \u2264 inf \u03b1>0 2(1 + \u03b1)E[R n (F)] + 2rt n + t n (b \u2212 a) 1 3 + 1 \u03b1 .", "formula_coordinates": [37.0, 135.0, 385.66, 359.42, 25.97]}, {"formula_id": "formula_165", "formula_text": "|E Pn [f ] \u2212 E[f ]| \u2264 3E[R n (F)] + 2M 2t n", "formula_coordinates": [37.0, 211.68, 473.38, 187.42, 25.97]}, {"formula_id": "formula_166", "formula_text": "E[f ] \u2264 E Pn [f ] + 3 2Var Pn (f )t n + 15M t n + 2 1 + 2 2t n \u01eb (36", "formula_coordinates": [37.0, 164.52, 641.02, 370.47, 27.89]}, {"formula_id": "formula_167", "formula_text": ")", "formula_coordinates": [37.0, 534.98, 650.26, 4.81, 10.91]}, {"formula_id": "formula_168", "formula_text": "E[f ] \u2264 E Pn [f ] + 18Var Pn (f (X))t n + 15M t n + 2 1 + 2 2t n \u01eb (i) \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 2\u03c1Var Pn (f (X)) n \u2212 \uf8eb \uf8ed 2\u03c1Var Pn (f (X)) n \u2212 2M \u03c1 n \uf8f6 \uf8f8 + + 5M \u03c1 3n + 2 1 + 2 2t n \u01eb \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 11 3 M \u03c1 n + 2 1 + 2 2t n \u01eb for all f \u2208 F,(37)", "formula_coordinates": [38.0, 132.24, 115.9, 407.55, 161.23]}, {"formula_id": "formula_169", "formula_text": "E[ f ] \u2264 sup P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] + 11M \u03c1 3n + 2 1 + 2 2t n \u01eb for all f \u2208 F. Now fix f \u2208 F.", "formula_coordinates": [38.0, 72.0, 368.74, 406.46, 61.49]}, {"formula_id": "formula_170", "formula_text": "E Pn [f ] \u2264 E[f ] + 2Var(f )t n + 2M t 3n", "formula_coordinates": [38.0, 220.92, 436.78, 168.94, 26.09]}, {"formula_id": "formula_171", "formula_text": "Var Pn (f ) \u2264 1 \u2212 n \u22121 Var(f ) + 2tM 2 n", "formula_coordinates": [38.0, 211.2, 497.42, 198.75, 27.01]}, {"formula_id": "formula_172", "formula_text": "P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f (X)] (i) \u2264 E Pn [f ] + 2\u03c1Var Pn (f ) n \u2264 E[f ] + 2Var(f )t n + 2M 3n t + 2\u03c1Var(f ) n + 2 M 2 \u03c1t n (ii) \u2264 E[f ] + 2 2Var(f )\u03c1 n + 8 3 M \u03c1 n ,", "formula_coordinates": [38.0, 118.2, 574.54, 374.38, 97.01]}, {"formula_id": "formula_173", "formula_text": "E[ f (X)] \u2264 E[f (X)] + 2 2\u03c1Var(f (X)) n + 19 3 M \u03c1 n + 2 1 + 2 2t n \u01eb.", "formula_coordinates": [39.0, 143.4, 160.18, 325.1, 25.97]}, {"formula_id": "formula_174", "formula_text": "E[f 2 ] \u2264 E Pn [f 2 ] + 1 \u03b7 E Pn [f 2 ] + 72M 2 (1 + \u03b7)r \u22c6 n + M t n 4 + 7 3 M .", "formula_coordinates": [40.0, 151.68, 124.42, 308.66, 25.97]}, {"formula_id": "formula_175", "formula_text": "E Pn [f 2 ] \u2264 E[f 2 ] + \u03b7 1 + \u03b7 E[f 2 ] + 72M 2 (1 + \u03b7)r \u22c6 n + M t n 4 + 7 3 M .", "formula_coordinates": [40.0, 147.72, 185.5, 316.58, 25.97]}, {"formula_id": "formula_176", "formula_text": "V n = 4((2e + 84M )B n + 36r \u22c6 n ).", "formula_coordinates": [40.0, 232.32, 258.5, 147.26, 15.17]}, {"formula_id": "formula_177", "formula_text": "E[f ] \u2264 E Pn [f ] + V n E[f 2 ] + 6r \u22c6 n + 14M B n", "formula_coordinates": [40.0, 203.4, 307.46, 204.57, 21.01]}, {"formula_id": "formula_178", "formula_text": "E[f ] \u2264 E Pn [f ] + 2V n E Pn [f 2 ] + 144M 2 V n r \u22c6 n + 7V n M max{M, 1}t/n + 6r \u22c6 n + 14M B n \u2264 E Pn [f ] + 2V n E Pn [f 2 ] + 12M V n r \u22c6 n + 7 max{M, 1} M t n + 6r \u22c6 n + 14M B n", "formula_coordinates": [40.0, 104.28, 373.82, 402.81, 52.69]}, {"formula_id": "formula_179", "formula_text": "2V n E Pn [f 2 ] = 2V n Var Pn (f ) + 2V n E Pn [f ] 2 \u2264 2V n Var Pn (f ) + 2V n E Pn [f ],", "formula_coordinates": [40.0, 128.16, 481.1, 366.62, 19.69]}, {"formula_id": "formula_180", "formula_text": "E[f ] \u2264 1 + 2V n E Pn [f ] + 2V n Var Pn (f ) + 12M V n r \u22c6 n + 7 max{M, 1} M t n + 6r \u22c6 n + 14M B n \u2264 1 + 2V n E Pn [f ] + 2V n Var Pn (f ) + 6M V n + 6M r \u22c6 n + 7 max{M, 1}t M n + 6r \u22c6 n + 14M B n ,", "formula_coordinates": [40.0, 72.0, 536.02, 475.34, 58.49]}, {"formula_id": "formula_181", "formula_text": "\u221a ab \u2264 1 2 a + 1 2 b for a, b \u2265 0. Recalling the bound (21), which implies \u03c1 \u2265 nV n , \u03c1 \u2265 n(r \u22c6 n + 7 max{M,1}tM n", "formula_coordinates": [40.0, 72.0, 599.02, 468.02, 44.09]}, {"formula_id": "formula_182", "formula_text": "E[f ] \u2264 1 + 2\u03c1 n E Pn [f ] + 2\u03c1 n Var Pn (f ) + 13M \u03c1 n .", "formula_coordinates": [40.0, 178.32, 651.94, 255.26, 25.97]}, {"formula_id": "formula_183", "formula_text": "E Pn [f ] \u2264 E[f ] + 2tVar(f ) n + 2M t 3n", "formula_coordinates": [41.0, 220.92, 218.26, 168.94, 25.97]}, {"formula_id": "formula_184", "formula_text": "Var Pn (f ) \u2264 1 \u2212 n \u22121 Var(f ) + 2tM 2 n", "formula_coordinates": [41.0, 211.2, 274.94, 198.75, 27.01]}, {"formula_id": "formula_185", "formula_text": "P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f ] \u2264 E Pn [f ] + 2\u03c1 n Var Pn (f ) \u2264 E[f ] + 2t n Var(f ) + 2\u03c1 n Var(f ) + 2M \u221a \u03c1t n + 2M t 3n", "formula_coordinates": [41.0, 72.0, 338.62, 466.78, 42.67]}, {"formula_id": "formula_186", "formula_text": "P :D \u03c6 (P || Pn)\u2264 \u03c1 n E P [f ] \u2264 E[f ] + 91\u03c1 45n Var(f ) + 3M \u03c1 n + M t n .", "formula_coordinates": [41.0, 169.2, 430.42, 273.5, 34.87]}, {"formula_id": "formula_187", "formula_text": "F l := g \u2208 F centered : e \u2212l r < E[g 2 ] \u2264 e \u2212(l\u22121) r , F L := g \u2208 F centered : E[g 2 ] \u2264 e \u2212L r", "formula_coordinates": [41.0, 102.36, 568.58, 400.68, 21.01]}, {"formula_id": "formula_188", "formula_text": "E[g] \u2264 E Pn [g] + 2te \u2212(l\u22121) r n + 3E[R n (F l )] + 5M t n \u2264 E Pn [g] + 2et n E[g 2 ] + 3E[R n (F l )] + 5M t n", "formula_coordinates": [41.0, 184.68, 635.42, 241.39, 56.53]}, {"formula_id": "formula_189", "formula_text": "E[g] \u2264 E Pn [g] + 2te \u2212L r n + 3E[R n (F L )] + 5M t n \u2264 E Pn [g] + 2et n E[g 2 ] + 2te \u2212L r n + 3E[R n (F L )] + 5M t n .", "formula_coordinates": [42.0, 156.36, 112.82, 299.18, 57.61]}, {"formula_id": "formula_190", "formula_text": "E[g] \u2264 E Pn [g] + 2et n E[g 2 ] + 3E[R n (F centered )] + 5M t n + 2te \u2212L r n .", "formula_coordinates": [42.0, 143.88, 205.82, 324.26, 26.89]}, {"formula_id": "formula_191", "formula_text": "G r := r E[f 2 ] \u2228 r f : f \u2208 F \u2286 cf : f \u2208 F, E[c 2 f 2 ] \u2264 r, c \u2208 [0, 1] .", "formula_coordinates": [42.0, 141.72, 354.82, 328.58, 33.53]}, {"formula_id": "formula_192", "formula_text": "E[g] \u2264 E Pn [g] + 2e n E[g 2 ] t + log log n t + 6E[R n (G r )] + 7M n t + log log n t .(38)", "formula_coordinates": [42.0, 97.8, 432.58, 441.99, 26.09]}, {"formula_id": "formula_193", "formula_text": "\u03c8 n (r) = \u221a r\u03c8 n (r)/ \u221a r \u2264 \u221a r\u03c8 n (r \u22c6 n )/ r \u22c6 n = rr \u22c6 n for any r \u2265 r \u22c6 n , so E[R n G r ] \u2264 E[R n cf : f \u2208 F, E[c 2 f 2 ] \u2264 r, c \u2208 [0, 1] ] \u2264 \u03c8 n (r) \u2264 rr \u22c6 n", "formula_coordinates": [42.0, 72.0, 484.9, 398.61, 76.01]}, {"formula_id": "formula_194", "formula_text": "E[g] \u2264 E Pn [g] + 2eB n E[g 2 ] + 6 r \u22c6 n r + 7M B n .(39)", "formula_coordinates": [42.0, 191.64, 592.34, 348.15, 19.57]}, {"formula_id": "formula_195", "formula_text": "= r E[f 2 ]\u2228r f . If E[f 2 ]", "formula_coordinates": [43.0, 318.96, 164.54, 104.18, 16.37]}, {"formula_id": "formula_196", "formula_text": "E[f ] \u2264 E Pn [f ] + 2eB n E[f 2 ] + 6r \u22c6 n + 14M B n .", "formula_coordinates": [43.0, 195.6, 204.26, 220.82, 21.01]}, {"formula_id": "formula_197", "formula_text": "\u221a r \u22c6 n r + 7M B n yields E[f ] \u2264 E Pn [f ] + 2eB n E[f 2 ] + 6 rE[f 2 ] \u2264 E Pn [f ] + 2eB n E[f 2 ] + 6 (r \u22c6 n + 7M B n /3)E[f 2 ] instead. Combining the cases E[f 2 ] \u2276 r, we conclude that for all f \u2208 F, E[f ] \u2264 E Pn [f ] + 2eB n + 6 r \u22c6 n + 7M B n /3 E[f 2 ] + 6r \u22c6 n + 14M B n", "formula_coordinates": [43.0, 72.0, 222.82, 435.94, 118.25]}, {"formula_id": "formula_198", "formula_text": "E \u01eb [R n (\u03c6 \u2022 G)] \u2264 LE \u01eb [R n (G)]", "formula_coordinates": [43.0, 239.04, 454.06, 133.7, 18.65]}, {"formula_id": "formula_199", "formula_text": "G r := r E[f 2 ] \u2228 r f : f \u2208 F \u2286 cf : f \u2208 F, E[c 2 f 2 ] \u2264 r, c \u2208 [0, 1]", "formula_coordinates": [43.0, 144.12, 517.3, 317.18, 33.41]}, {"formula_id": "formula_200", "formula_text": "g 2 \u2208 G 2 r , Var(g 2 ) \u2264 E[g 4 ] \u2264 M 2 E[g 2 ] \u2264 M 2 r. Let c 1 = 3 and c 2 =", "formula_coordinates": [43.0, 104.4, 566.42, 317.12, 20.41]}, {"formula_id": "formula_201", "formula_text": "g \u2208 G r E[g 2 ] \u2264 E Pn [g 2 ] + c 1 E[R n (G 2 r )] + M 2rt n + c 2 M 2 t n (a) \u2264 E Pn [g 2 ] + 2c 1 M E[R n (G r )] + M 2rt n + c 2 M 2 t n (b) \u2264 E Pn [g 2 ] + 2c 1 M rr \u22c6 n + M 2rt n + c 2 M 2 t n (40", "formula_coordinates": [43.0, 175.2, 581.74, 359.79, 110.21]}, {"formula_id": "formula_202", "formula_text": ")", "formula_coordinates": [43.0, 534.98, 673.3, 4.81, 10.91]}, {"formula_id": "formula_203", "formula_text": "A = 2c 1 M \u221a r \u22c6 n + M 2t n and D = c 2 M 2 t n .", "formula_coordinates": [44.0, 108.84, 108.82, 196.1, 21.57]}, {"formula_id": "formula_204", "formula_text": "E[g 2 ] \u2264 E Pn [g 2 ] + r D .", "formula_coordinates": [44.0, 255.48, 151.06, 100.94, 26.09]}, {"formula_id": "formula_205", "formula_text": "K 2 A 2 \u2264 r \u2264 K 2 A 2 + 2KD and in particular, r \u2265 K 2 A 2 \u2265 r \u22c6 n . For each g \u2208 G r , there exists f \u2208 F such that g = r E[f 2 ]\u2228r f . If E[f 2 ] \u2264 r, rescaling the inequality", "formula_coordinates": [44.0, 72.0, 194.78, 468.02, 58.57]}, {"formula_id": "formula_206", "formula_text": "E[f 2 ] \u2264 E Pn [f 2 ] + r K \u2264 E Pn [f 2 ] + KA 2 + 2D. If E[f 2 ] > r, rescaling instead yields E[f 2 ] \u2264 E Pn [f 2 ] + E[f 2 ] K .", "formula_coordinates": [44.0, 72.0, 252.1, 342.98, 81.77]}, {"formula_id": "formula_207", "formula_text": "E[f 2 ] \u2264 K K \u2212 1 E Pn [f 2 ] + KA 2 + 2D. Noting that A \u2264 2 4c 2 1 M 2 r \u22c6 n + 2 M 2 t n", "formula_coordinates": [44.0, 72.0, 359.86, 320.66, 56.93]}, {"formula_id": "formula_208", "formula_text": "\u2206 n (\u03b8) := E [\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)] \u2212 E Pn [\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)] .(41)", "formula_coordinates": [44.0, 158.4, 527.02, 381.39, 18.65]}, {"formula_id": "formula_209", "formula_text": "Claim E.1. If S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 , then sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) + 2\u03c1 n Var Pn (\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)) \u2265 \u01eb.(42)", "formula_coordinates": [44.0, 72.0, 565.1, 467.79, 55.35]}, {"formula_id": "formula_210", "formula_text": "S 2\u01eb \u22c6 \u2282 \u03b8 \u2208 \u0398 : \u03b8 \u2212 \u03c0(\u03b8) 2 \u2264 2\u01eb \u03bb 1 \u03b3 = \u03b8 \u2208 \u0398 : dist(\u03b8, S \u22c6 ) \u2264 2\u01eb \u03bb 1 \u03b3", "formula_coordinates": [44.0, 126.0, 661.52, 344.34, 30.92]}, {"formula_id": "formula_211", "formula_text": "\u2264 L 2 dist(\u03b8, S \u22c6 ) 2 \u2264 L 2 2\u01eb \u03bb 2 \u03b3", "formula_coordinates": [45.0, 299.64, 95.6, 140.1, 30.92]}, {"formula_id": "formula_212", "formula_text": ") that \u01eb \u2265 ( 8L 2 \u03c1 n ) \u03b3 2(\u03b3\u22121) ( 2 \u03bb ) 1 \u03b3\u22121 , we have 2\u03c1 n Var Pn (\u2113(\u03b8; X) \u2212 \u2113(\u03c0(\u03b8); X)) \u2264 L 2\u03c1 n 2\u01eb \u03bb 1 \u03b3 \u2264 \u01eb 2 .", "formula_coordinates": [45.0, 185.76, 135.56, 252.62, 58.52]}, {"formula_id": "formula_213", "formula_text": "\u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 \u01eb 2 ,", "formula_coordinates": [45.0, 266.76, 222.46, 78.5, 29.12]}, {"formula_id": "formula_214", "formula_text": "P S \u01eb \u22c6 \u2282 S 2\u01eb \u22c6 \u2264 P sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 \u01eb 2 . (43", "formula_coordinates": [45.0, 214.2, 286.78, 320.79, 29.12]}, {"formula_id": "formula_215", "formula_text": ")", "formula_coordinates": [45.0, 534.98, 294.1, 4.81, 10.91]}, {"formula_id": "formula_216", "formula_text": "sup x,x \u2032 \u2208X |f (X 1 , \u2022 \u2022 \u2022 , X j\u22121 , x, X j+1 , \u2022 \u2022 \u2022 , X n ) \u2212 f (X 1 , \u2022 \u2022 \u2022 , X j\u22121 , x \u2032 , X j+1 , \u2022 \u2022 \u2022 , X n )| \u2264 sup x,x \u2032 \u2208X sup \u03b8\u2208S 2\u01eb \u22c6 1 n (\u2113(\u03b8; x) \u2212 \u2113(\u03c0(\u03b8); x)) \u2212 1 n (\u2113(\u03b8; x \u2032 ) \u2212 \u2113(\u03c0(\u03b8); x \u2032 )) \u2264 2L n sup \u03b8\u2208S 2\u01eb \u22c6 dist(\u03b8, S \u22c6 ) \u2264 2L n 2\u01eb \u03bb 1 \u03b3 for j = 1, . . . , n. Using the standard symmetrization inequality E[sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8)] \u2264 2E[R n (S 2\u01eb \u22c6 )", "formula_coordinates": [45.0, 72.0, 374.42, 464.29, 122.65]}, {"formula_id": "formula_217", "formula_text": "P sup \u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 2E[R n (S 2\u01eb \u22c6 )] + t \u2264 exp \u2212 nt 2 2L 2 \u03bb 2\u01eb 2 \u03b3 for all t \u2265 0. Letting u = nt 2 2L 2 \u03bb 2\u01eb 2 \u03b3", "formula_coordinates": [45.0, 72.0, 512.72, 365.46, 68.96]}, {"formula_id": "formula_218", "formula_text": "\u03b8\u2208S 2\u01eb \u22c6 \u2206 n (\u03b8) \u2265 \u01eb 2 ) \u2264 e \u2212u .", "formula_coordinates": [45.0, 199.2, 574.46, 113.66, 20.77]}, {"formula_id": "formula_219", "formula_text": "\u03b8 \u2208 \u0398 \\ S 2\u01eb \u22c6 such that R n (\u03b8, P n ) \u2264 inf \u03b8\u2208\u0398 R n (\u03b8, P n ) + \u01eb \u2264 R n (\u03c0(\u03b8), P n ) + \u01eb.", "formula_coordinates": [45.0, 72.0, 606.02, 467.93, 47.53]}, {"formula_id": "formula_220", "formula_text": "R n (t\u03b8 + (1 \u2212 t)\u03c0(\u03b8), P n ) \u2264 tR n (\u03b8, P n ) + (1 \u2212 t)R n (\u03c0(\u03b8), P n ) \u2264 R n (\u03c0(\u03b8), P n ) + t\u01eb.", "formula_coordinates": [45.0, 113.4, 681.94, 385.22, 18.65]}, {"formula_id": "formula_221", "formula_text": "S 2\u01eb \u22c6 that R n (\u03b8, P n ) \u2212 R(\u03b8) + R(\u03c0(\u03b8)) \u2212 R n (\u03c0(\u03b8), P n ) \u2264 R(\u03c0(\u03b8)) \u2212 R(\u03b8) + \u01eb \u2264 \u2212\u01eb,", "formula_coordinates": [46.0, 134.88, 127.22, 383.31, 39.73]}, {"formula_id": "formula_222", "formula_text": "\u01eb \u2264 sup \u03b8\u2208S 2\u01eb \u22c6 {R(\u03b8) \u2212 R n (\u03b8, P n ) \u2212 (R(\u03c0(\u03b8)) \u2212 R n (\u03c0(\u03b8), P n ))} \u2264 sup \u03b8\u2208S 2\u01eb \u22c6 sup P :D \u03c6 (P || Pn)\u2264\u03c1/n {R(\u03b8) \u2212 R(\u03c0) + E P [\u2113(\u03c0(\u03b8); X) \u2212 \u2113(\u03b8; X)]} .", "formula_coordinates": [46.0, 146.76, 200.62, 318.5, 54.19]}, {"formula_id": "formula_223", "formula_text": "R n (\u03b8, P n ) = E Pn [\u2113(\u03b8, X)] + 2\u03c1Var Pn (\u2113(\u03b8, X)) n", "formula_coordinates": [46.0, 193.32, 365.14, 224.07, 28.01]}, {"formula_id": "formula_224", "formula_text": "0 = \u2207 \u03b8 R n ( \u03b8 rob n , P n ) = \u2207 1 n n i=1 \u2113(\u03b8 \u22c6 , X i ) + \u2207 2 1 n n i=1 \u2113(\u03b8 \u22c6 , X i ) ( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + \u2207 2\u03c1Var Pn (\u2113( \u03b8 rob n , X)) n + o P (n \u2212 1 2 ) = 1 n n i=1 \u2207\u2113(\u03b8 \u22c6 , X i ) + \u2207 2 R(\u03b8 \u22c6 )( \u03b8 rob n \u2212 \u03b8 \u22c6 ) + \u2207 2\u03c1Var(\u2113(\u03b8 \u22c6 , X)) n + o P (n \u2212 1 2 ).", "formula_coordinates": [46.0, 83.52, 458.66, 444.99, 94.37]}, {"formula_id": "formula_225", "formula_text": "Z(\u03b8) := \u2113(\u03b8, X) \u2212 E[\u2113(\u03b8, X)]", "formula_coordinates": [47.0, 239.64, 119.86, 132.62, 18.65]}, {"formula_id": "formula_226", "formula_text": "maximize P E P [Z(\u03b8)] subject to D \u03c6 (P || P n ) \u2264 \u03c1 n ,", "formula_coordinates": [47.0, 192.96, 203.86, 226.1, 25.97]}, {"formula_id": "formula_227", "formula_text": "i\u2208[n] \u221a 2\u03c1(Z i (\u03b8) \u2212 Z(\u03b8)) \u221a ns n (\u03b8) \u2265 \u22121 for all \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB", "formula_coordinates": [47.0, 187.68, 269.86, 236.71, 34.77]}, {"formula_id": "formula_228", "formula_text": "satisfies |\u2113(\u03b8, x) \u2212 \u2113(\u03b8 \u2032 , x)| \u2264 L(x) \u03b8 \u2212 \u03b8 \u2032 for \u03b8, \u03b8 \u2032 \u2208 \u03b8 \u22c6 + \u01ebB. Then because \u221a ns n (\u03b8) \u2212 \u221a ns n (\u03b8 \u2032 ) \u2264 sup u: u 2 \u22641 n i=1 u i \u2113(\u03b8, X i ) \u2212 \u2113(\u03b8 \u2032 , X i ) \u2264 sup u: u 2 \u22641 n i=1 u i L(X i ) \u03b8 \u2212 \u03b8 \u2032 \u2264 n i=1 L 2 (X i ) \u03b8 \u2212 \u03b8 \u2032 so \u03b8 \u2192 s n (\u03b8) is 1 n n i=1 L(X i ) 2 -Lipschitz for \u03b8 \u2208 \u03b8 \u22c6 + \u01ebB, we have inf \u03b8\u2208\u03b8 \u22c6 +\u01ebB min i\u2208[n] \u221a 2\u03c1(Z i (\u03b8) \u2212 Z(\u03b8)) \u221a ns n (\u03b8) \u2265 min i\u2208[n] \u221a 2\u03c1(Z i (\u03b8 \u22c6 ) \u2212 Z(\u03b8 \u22c6 ) \u2212 2\u01ebL(X i )) n s n (\u03b8 \u22c6 ) \u2212 \u01eb 1 n n j=1 L(X j ) 2 .", "formula_coordinates": [47.0, 72.0, 340.94, 416.9, 187.93]}, {"formula_id": "formula_229", "formula_text": "min i\u2208[n] 2\u03c1(Z i (\u03b8 \u22c6 ) \u2212 Z(\u03b8 \u22c6 ) \u2212 2\u01ebL(X i )) \u2265 \u221a n s n (\u03b8 \u22c6 ) \u2212 \u01eb 1 n n i=1 L(X i ) 2 1 2", "formula_coordinates": [47.0, 126.36, 587.6, 357.53, 35.52]}, {"formula_id": "formula_230", "formula_text": "G n (\u2113 \u2022 H) \u2264 E sup h\u2208B H ,c 1 ,c 2 \u2208[0,1] n i=1 g i |c 1 (h(x i ) \u2212 h \u22c6 (x i )) \u2212 c 2 \u03be i | | E[(h(X) \u2212 h \u22c6 (X)) 2 ] \u2264 r c 2 1 , \u03c3 2 \u2264 r c 2 2 \u2264 E sup f \u22082B H ,c\u2208[0,1] n i=1 g i |f (x i ) \u2212 c\u03be i | | E[f (X) 2 ] \u2264 r, \u03c3 2 \u2264 r/c 2 ,", "formula_coordinates": [50.0, 72.0, 110.06, 461.07, 74.43]}, {"formula_id": "formula_231", "formula_text": "E[(Y f 1 ,c 1 \u2212 Y f 2 ,c 2 ) 2 ] = n i=1 (|f 1 (x i ) \u2212 c 1 \u03be i | \u2212 |f 2 (x i ) \u2212 c 2 \u03be i |) 2 \u2264 n i=1 (f 1 (x i ) \u2212 f 2 (x i ) + (c 2 \u2212 c 1 )\u03be i ) 2 \u2264 2 n i=1 (f 1 (x i ) \u2212 f 2 (x i )) 2 + 2(c 2 \u2212 c 1 ) 2 n i=1 \u03be 2 i . Moreover, E[(Z f 1 ,c 1 \u2212 Z f 2 ,c 2 ) 2 ] = n i=1 (f 1 (x i ) \u2212 f 2 (x i )) 2 + (c 1 \u2212 c 2 ) 2 n i=1 \u03be 2 i .", "formula_coordinates": [50.0, 72.0, 274.7, 387.74, 138.01]}, {"formula_id": "formula_232", "formula_text": "E sup f \u22082B H n i=1 g i f (X i ) | E[f (X) 2 ] \u2264 r \u221a n \uf8eb \uf8ed \u221e j=1 min{\u03bb j , r} \uf8f6 \uf8f8 1 2", "formula_coordinates": [50.0, 153.0, 523.24, 299.45, 47.17]}, {"formula_id": "formula_233", "formula_text": "E Pn [\u2113(\u03b8; X)] = 1 n [N \u22121 |\u03b8 + 1| + N 1 |\u03b8 \u2212 1| + N 0 |\u03b8| \u2212 (n \u2212 N 0 )] , because N 1 + N \u22121 + N 0 = n.", "formula_coordinates": [50.0, 158.64, 654.22, 294.74, 25.97]}, {"formula_id": "formula_234", "formula_text": "\u03b8 erm n := argmin \u03b8\u2208R E Pn [\u2113(\u03b8; X)] = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if N 1 > N 0 + N \u22121 \u22121 if N \u22121 > N 0 + N 1 \u2208 [\u22121, 1] otherwise.", "formula_coordinates": [51.0, 159.36, 89.56, 291.51, 61.07]}, {"formula_id": "formula_235", "formula_text": "P N 1 > n 2 = P Bin n, 1 \u2212 \u03b4 2 > n 2 = P Bin n, 1 + \u03b4 2 < n 2 .", "formula_coordinates": [51.0, 140.52, 248.14, 330.98, 25.79]}, {"formula_id": "formula_236", "formula_text": "P N 1 \u2265 n 2 \u2265 \u03a6 \u2212 2nD kl 1 2 || 1 + \u03b4 2", "formula_coordinates": [51.0, 203.88, 316.3, 185.92, 26.09]}, {"formula_id": "formula_237", "formula_text": "P N 1 > n 2 or N \u22121 > n 2 \u2265 2\u03a6 \u2212 n\u03b4 2 1 \u2212 \u03b4 2 \u2212 2P N 1 = n 2 .", "formula_coordinates": [51.0, 156.6, 399.14, 298.7, 34.45]}, {"formula_id": "formula_238", "formula_text": "P N 1 = n 2 = 2 \u2212n n n/2 (1 \u2212 \u03b4 2 ) n/2 \u2264 (1 \u2212 \u03b4 2 ) n/2 2 \u03c0n ,", "formula_coordinates": [51.0, 170.52, 463.06, 270.98, 25.97]}, {"formula_id": "formula_239", "formula_text": "P N 1 > n 2 or N \u22121 > n 2 \u2265 2\u03a6 \u2212 n\u03b4 2 1 \u2212 \u03b4 2 \u2212 (1 \u2212 \u03b4 2 ) n/2 8 \u03c0n .", "formula_coordinates": [51.0, 150.6, 529.34, 310.82, 34.45]}, {"formula_id": "formula_240", "formula_text": "(i) \u2265 R(\u03b8) \u2212 \u03bb 4 \u01eb 2 (ii) \u2265 R(\u03b8 \u22c6 ) + \u03bb 2 \u03b8 \u2212 \u03b8 \u22c6 2 2 \u2212 \u03bb 4 \u01eb 2 (iii) \u2265 R(\u03b8 \u22c6 ) + \u03bb 4 \u01eb 2 (iv) \u2265 E Pn [\u2113(\u03b8 \u22c6 , X)] + \u03bb 4 \u01eb 2 \u2212 \u03bb 8 \u01eb 2 = E Pn [\u2113(\u03b8 \u22c6 , X)] + \u03bb 8 \u01eb 2 ,", "formula_coordinates": [52.0, 240.12, 249.62, 252.74, 84.01]}, {"formula_id": "formula_241", "formula_text": "E Pn [\u2113(\u03b8 \u22c6 , X)] \u2265 R n (\u03b8 \u22c6 , P n ) \u2212 2\u03c1 n", "formula_coordinates": [52.0, 184.68, 377.5, 163.92, 26.09]}, {"formula_id": "formula_242", "formula_text": "p \u03bb 2 p \u2212 1 n 1 2 2 \u2212 \u03bb\u03c1 n + p \u22a4 z | p \u2265 0, 1 \u22a4 p = 1 .(47)", "formula_coordinates": [52.0, 236.76, 669.7, 303.03, 25.97]}, {"formula_id": "formula_243", "formula_text": "s i = j\u2264i z j , \u03c3 2 i = j\u2264i z 2 j ,", "formula_coordinates": [53.0, 246.48, 228.26, 119.06, 25.61]}, {"formula_id": "formula_244", "formula_text": "v i = 1 n \u2212 1 \u03bb z i", "formula_coordinates": [53.0, 274.68, 292.3, 62.16, 26.09]}, {"formula_id": "formula_245", "formula_text": "f \u2032 (\u03bb) = \u2202 \u2202\u03bb \u03bb 2 p(\u03bb) \u2212 n \u22121 1 2 2 \u2212 \u03bb\u03c1 n + p(\u03bb) \u22a4 z = 1 2 p(\u03bb) \u2212 n \u22121 1 2 2 \u2212 \u03c1 n = 1 2 i j=1 (v j \u2212 \u03b7 \u2212 n \u22121 ) 2 + 1 2 n j=i+1 1 n 2 \u2212 \u03c1 n = 1 2 i j=1 1 \u03bb z j + \u03b7 2 + n \u2212 i 2n 2 \u2212 \u03c1 n = \u03c3 2 i 2\u03bb 2 + i\u03b7 2 2 + s i \u03b7 \u03bb + n \u2212 i 2n 2 \u2212 \u03c1 n .", "formula_coordinates": [53.0, 134.04, 474.94, 343.94, 104.73]}], "doi": ""}