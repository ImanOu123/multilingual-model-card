{"title": "Out of Sight But Not Out of Mind: An Answer Set Programming Based Online Abduction Framework for Visual Sensemaking in Autonomous Driving", "authors": "Jakob Suchan; Mehul Bhatt; Srikrishna Varadarajan", "pub_date": "2019-05-31", "abstract": "We demonstrate the need and potential of systematically integrated vision and semantics solutions for visual sensemaking (in the backdrop of autonomous driving). A general method for online visual sensemaking using answer set programming is systematically formalised and fully implemented. The method integrates state of the art in (deep learning based) visual computing, and is developed as a modular framework usable within hybrid architectures for perception & control. We evaluate and demo with community established benchmarks KITTIMOD and MOT. As use-case, we focus on the significance of human-centred visual sensemaking -e.g., semantic representation and explainability, question-answering, commonsense interpolationin safety-critical autonomous driving situations. Car (c) is in-front, and indicating to turn-right; during this time, person (p) is on a bicycle (b) and positioned front-right of c and movingforward. Car c turns-right, during which the bicyclist < p, b > is not visible. Subsequently, bicyclist < p, b > reappears. The occlusion scenario indicates several challenges concerning aspects such as: identity maintenance, making default assumptions, computing counterfactuals, projection, and interpolation of missing information (e.g., what could be hypoth-", "sections": [{"heading": "MOTIVATION", "text": "Autonomous driving research has received enormous academic & industrial interest in recent years. This surge has coincided with (and been driven by) advances in deep learning based computer vision research. Although deep learning based vision & control has (arguably) been successful for self-driving vehicles, we posit that there is a clear need and tremendous potential for hybrid visual sensemaking solutions (integrating vision and semantics) towards fulfilling essential legal and ethical responsibilities involving explainability, human-centred AI, and industrial standardisation (e.g, pertaining to representation, realisation of rules and norms).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Autonomous Driving: \"Standardisation & Regulation\"", "text": "As the self-driving vehicle industry develops, it will be necessary -e.g., similar to sectors such as medical computing, computer aided design-to have an articulation and community consensus on aspects such as representation, interoperability, human-centred performance benchmarks, and data archival & retrieval mechanisms. 1 In spite of major in-1 Within autonomous driving, the need for standardisation and ethical regulation has most recently garnered interest internationally, e.g., with the Federal Ministry of Transport and Digital Infrastructure in Germany taking a lead in eliciting 20 key propositions (with legal implications) for the fulfilment of ethical commitments for automated and connected driving systems [BMVI, 2018]. vestments in self-driving vehicle research, issues related to human-centred'ness, human collaboration, and standardisation have been barely addressed, with the current focus in driving research primarily being on two basic considerations: how fast to drive, and which way and how much to steer. This is necessary, but inadequate if autonomous vehicles are to become commonplace and function with humans. Ethically driven standardisation and regulation will require addressing challenges in semantic visual interpretation, natural / multimodal human-machine interaction, high-level data analytics (e.g., for post hoc diagnostics, dispute settlement) etc. This will necessitate -amongst other things-human-centred qualitative benchmarks and multifaceted hybrid AI solutions.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Visual Sensemaking Needs Both \"Vision & Semantics\"", "text": "We demonstrate the significance of semantically-driven methods rooted in knowledge representation and reasoning (KR) in addressing research questions pertaining to explainability and human-centred AI particularly from the viewpoint of sensemaking of dynamic visual imagery. Consider the occlusion scenario in Fig. 1: Addressing such challenges -be it realtime or post-hocin view of human-centred AI concerns pertaining to ethics, explainability and regulation requires a systematic integration of Semantics and Vision, i.e., robust commonsense representation & inference about spacetime dynamics on the one hand, and powerful low-level visual computing capabilities, e.g., pertaining to object detection and tracking on the other. Key Contributions. We develop a general and systematic declarative visual sensemaking method capable of online abduction: realtime, incremental, commonsense semantic question-answering and belief maintenance over dynamic visuospatial imagery. Supported are (1-3): (1). human-centric representations semantically rooted in spatio-linguistic primitives as they occur in natural language [Bhatt et al., 2013;Mani and Pustejovsky, 2012];\n(2). driven by Answer Set Programming (ASP) [Brewka et al., 2011], the ability to abductively compute commonsense interpretations and explanations in a range of (a)typical everyday driving situations, e.g., concerning safety-critical decision-making;\n(3). online performance of the overall framework modularly integrating high-level commonsense reasoning and state of the art lowlevel visual computing for practical application in real world settings. We present the formal framework & its implementation, and demo & empirically evaluate with community established real-world datasets and benchmarks, namely: KIT-TIMOD [Geiger et al., 2012] and MOT [Milan et al., 2016].", "publication_ref": ["b12", "b12", "b14", "b17"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "VISUAL SENSEMAKING: A GENERAL METHOD DRIVEN BY ASP", "text": "Our proposed framework, in essence, jointly solves the problem of assignment of detections to tracks and explaining overall scene dynamics (e.g. appearance, disappearance) in terms of high-level events within an online integrated lowlevel visual computing and high level abductive reasoning framework (Fig. 2). Rooted in answer set programming, the framework is general, modular, and designed for integration as a reasoning engine within (hybrid) architectures designed for real-time decision-making and control where visual perception is needed as one of the several components. In such large scale AI systems the declarative model of the scene dynamics resulting from the presented framework can be used for semantic Q/A, inference etc. to support decision-making. \u2022 \u03a3 dyn : The set of fluents \u03a6 = {\u03c6 1 , ..., \u03c6 n } and events \u0398 = {\u03b8 1 , ..., \u03b8 n } respectively characterise the dynamic properties of the objects in the scene and high-level abducibles (Table 1). For reasoning about dynamics (with <\u03a6, \u0398>), we use a variant of event calculus as per [Ma et al., 2014;Miller et al., 2013]; in particular, for examples of this paper, the functional event calculus fragment (\u03a3 dyn ) of Ma et al. [2014] suffices: main axioms relevant here pertain to occurs-at(\u03b8, t) denoting that an event occurred at time t and holds-at(\u03c6, v, t) denoting that v holds for a fluent \u03c6 at time t. 2", "publication_ref": ["b15", "b15"], "figure_ref": ["fig_1"], "table_ref": ["tab_2"]}, {"heading": "SPACE, MOTION, OBJECTS, EVENTS", "text": "\u2022 \u03a3:\nLet \u03a3 \u2261 def \u03a3 dyn <\u03a6, \u0398> \u222a \u03a3st <O, E, T , MT , R>", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "TRACKING AS ABDUCTION", "text": "Scene dynamics are tracked using a detect and track approach: we tightly integrate low-level visual computing (for detecting scene elements) with high-level ASP-based abduction to solve the assignment of observations to object tracks in an incremental manner. For each time point t we generate a problem specification consisting of the object tracks and visual observations and use (ASP) to abductively solve the corresponding assignment problem incorporating the ontological structure of the domain / data (abstracted with \u03a3). Steps 1-3 (Alg. 1 & Fig. 3) are as follows:\nStep 1. FORMULATING THE PROBLEM SPECIFICATION\nThe ASP problem specification for each time point t is given by the tuple < VOt, Pt, MLt > and the sequence of events (H events ) before time point t.\n\u2022 Visual Observations Scene elements derived directly from the visual input data are represented as spatial entities E, i.e., VO t = {\u03b5 obs1 , ..., \u03b5 obsn } is the set of observations at time t (Fig. 3).   [Chen et al., 2018], and Lane Detectionestimating lane markings, to detect lanes on the road, using SCNN [Pan et al., 2018]. Type and confidence score for each observation is given by type obsi and conf obsi .\n\u2022 Movement Prediction For each track trk i changes in position and size are predicted using kalman filters; this results in an estimate of the spatial entity \u03b5 for the next time-point t of each motion track P t = {\u03b5 trk1 , ..., \u03b5 trkn }.\n\u2022 Matching Likelihood For each pair of tracks and observations \u03b5 trki and \u03b5 obsj , where \u03b5 trki \u2208 P t and \u03b5 obsj \u2208 VO t , we compute the likelihood ML t = {ml trk1,obs1 , ..., ml trki,obsj } that \u03b5 obsj belongs to \u03b5 trki . The intersection over union (IoU) provides a measure for the amount of overlap between the spatial entities \u03b5 obsj and \u03b5 trki .\nStep 2. ABDUCTION BASED ASSOCIATION Following perception as logical abduction most directly in the sense of Shanahan [2005], we define the task of abducing visual explanations as finding an association (H assign t ) of observed scene elements (VO t ) to the motion tracks of objects (MT ) given by the predictions P t , together with a high-level explanation (H events t ), such that [H assign t \u2227 H events t ] is consistent with the background knowledge and the previously abduced event sequence H events , and entails the perceived scene given by < VO t , P t , ML t >:\n\u03a3 \u2227 H events \u2227 [H assign t \u2227 H events t ] |= VO t \u2227 P t \u2227 ML t\nwhere H assign t consists of the assignment of detections to object tracks, and H events t consists of the high-level events \u0398 explaining the assignments.\n\u2022 Associating Objects and Observations Finding the best match between observations (VO t ) and object tracks (P t ) is done by generating all possible assignments and then maximising a matching likelihood ml trki,obsj between pairs of spatial entities for matched observations \u03b5 obsj and predicted track region \u03b5 trki (See Step 3). Towards this we use choice rules [Gebser et al., 2014] (i.e., one of the heads of the rule has to be in the stable model) for \u03b5 obsj and \u03b5 trki , generating all possible assignments in terms of assignment actions: assign, start, end, halt, resume, ignore det, ignore trk.   box2d(trk_41, 631, 471, 40, 47) For each assignment action we define integrity constraints 3 that restrict the set of answers generated by the choice rules, e.g., the following constraints are applied to assigning an observation \u03b5 obsj to a track trk i , applying thresholds on the IoU trki,obsj and the confidence of the observation conf obsj , further we define that the type of the observation has to match the type of the track it is assigned to:   We define the event hides behind/2, stating that an object hides behind another object by defining the conditions that 3 Integrity constraints restrict the set of answers by eliminating stable models where the body is satisfied.\nINTEGRITY", "publication_ref": ["b13", "b18", "b18", "b14"], "figure_ref": ["fig_10", "fig_10"], "table_ref": ["tab_2"]}, {"heading": "EVENTS Description enters fov(Trk)", "text": "Track Trk enters the field of view.     For abducing the occurrence of an event we use choice rules that connect the event with assignment actions, e.g., a track getting halted may be explained by the event that the track hides behind another track.       We then maximize the matching likelihood for all assignments, using the build in maximize statement:   To find the best set of hypotheses with respect to the observations, we minimize the occurrence of certain events and association actions, e.g., the following optimization statements minimize starting and ending tracks; the resulting assignment is then used to update the motion tracks accordingly.  It is important here to note that: (1). by jointly abducing the object dynamics and high-level events we can impose con-  straints on the assignment of detections to tracks, i.e., an assignment is only possible if we can find an explanation supporting the assignment; and (2). the likelihood that an event occurs guides the assignments of observations to tracks. Instead of independently tracking objects and interpreting the interactions, this yields to event sequences that are consistent with the abduced object tracks, and noise in the observations is reduced (See evaluation in Sec. 3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "APPLICATION & EVALUATION", "text": "We demonstrate applicability towards identifying and interpreting safety-critical situations (e.g., Table 2); these encompass those scenarios where interpretation of spacetime dynamics, driving behaviour, environmental characteristics is necessary to anticipate and avoid potential dangers.\nReasoning about Hidden Entities Consider the situation of Fig. 4: a car gets occluded by another car turning left and reappears in front of the autonomous vehicle. Using online abduction for abducing high-level interactions of scene objects we can hypothesize that the car got occluded and anticipate its reappearance based on the perceived scene dynamics.\nThe following shows data and abduced events. We define a rule stating that a hidden object may unhide from behind the object it is hidden by and anticipate the time point t based on the object movement as follows:    ... det(det_1, car, 98). ... box2d(trk_3, 660, 460, 134, 102). ... ... box2d(trk_41, 631, 471, 40, 47). ... We then interpolate the objects position at time point t to predict where the object may reappear.    ... det(det_1, car, 98). ... box2d(trk_3, 660, 460, 134, 102). ... ... box2d(trk_41, 631, 471, 40, 47). ... For the occluded car in our example we get the following prediction for time t and position x, y:    3) show that jointly abducing high-level object interactions together with low-level scene dynamics increases the accuracy of the object tracks, i.e, we consistently observe an improvement of about 5%, from 45.72% to 50.5% for cars and 28.71% to 32.57% for pedestrians on KITTI, and from 41.4% to 46.2% on MOT.\n\u2022 Online Performance and Scalability Performance of online abduction is evaluated with respect to its real-time capabilities. 4 (1). We compare the time & accuracy of online abduction for state of the art (real-time) detection methods: YOLOv3, SSD [Liu et al., 2016], and Faster RCNN [Ren et al., 2015] (Fig. 5).\n(2). We evaluate scalability of the ASP based abduction on a synthetic dataset with controlled number of tracks and % of overlapping tracks per frame. Results (Fig. 5) show that online abduction can perform with above 30 frames per second for scenes with up to 10 highly overlapping object tracks, and more than 50 tracks with 1fps (for the sake of testing, it is worth noting that even for 100 objects per frame it only takes about an average of 4 secs per frame). Importantly, for realistic scenes such as in the KITTI dataset, abduction runs realtime at 33.9fps using YOLOv3, and 46.7 using SSD with a lower accuracy but providing good precision. ", "publication_ref": ["b15"], "figure_ref": [], "table_ref": ["tab_14", "tab_21"]}, {"heading": "Discussion of Empirical Results", "text": "Results show that integrating high-level abduction and object tracking improves the resulting object tracks and reduce the noise in the visual observations. For the case of online visual sense-making, ASP based abduction provides the required performance: even though the complexity of ASP based abduction increases quickly, with large numbers of tracked objects the framework can track up to 20 objects simultaneously with 30f ps and achieve real-time performance on the KITTI benchmark dataset. It is also important to note that the tracking approach in this paper is based on tracking by detection using a naive measure, i.e, the IoU (Sec. 2.2; Step 1), to associate observations and tracks, and it is not using any visual information in the prediction or association step. Naturally, this results in a lower accuracy, in particular when used with noisy detections and when tracking fast moving objects in a benchmark dataset such as KITTI. That said, due to the modularity of the implemented framework, extensions with different methods for predicting motion (e.g., using particle filters or optical flow based prediction) are straightforward: i.e., improving tracking is not the aim of our research.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "ASP is now widely used as an underlying knowledge representation language and robust methodology for nonmonotonic reasoning [Brewka et al., 2011;Gebser et al., 2012]. With ASP as a foundation, and driven by semantics, commonsense and explainability [Davis and Marcus, 2015], this research aims to bridge the gap between high-level formalisms for logical abduction and low-level visual processing by tightly integrating semantic abstractions of space-change with their underlying numerical representations. Within KR, the significance of high-level (abductive) explanations in a range of contexts is well-established: planning & process recognition [Kautz, 1991], vision & abduction [Shanahan, 2005], probabilistic abduction [Blythe et al., 2011], reasoning about spatio-temporal dynamics [Bhatt and Loke, 2008], reasoning about continuous spacetime change [Muller, 1998;Hazarika andCohn, 2002] etc. Dubba et al. [2015] uses abductive reasoning in an inductive-abductive loop within inductive logic programming (ILP). Aditya et al. [2015] formalise general rules for image interpretation with ASP. Similarly motivated to this research is [Suchan et al., 2018], which uses a two-step approach (with one huge problem specification), first tracking and then explaining (and fixing) tracking errors; such an approach is not runtime / realtime capable. In computer vision research there has recently been an interest to synergise with cognitively motivated methods; in particular, e.g., for perceptual grounding & inference [Yu et al., 2015] and combining video analysis with textual information for understanding events & answering queries about video data [Tu et al., 2014].", "publication_ref": ["b12", "b14", "b13", "b15", "b18", "b12", "b12", "b18", "b11", "b18", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "CONCLUSION & OUTLOOK", "text": "We develop a novel abduction-driven online (i.e., realtime, incremental) visual sensemaking framework: general, systematically formalised, modular and fully implemented. Integrating robust state-of-the-art methods in knowledge representation and computer vision, the framework has been evaluated and demonstrated with established benchmarks. We highlight application prospects of semantic vision for autonomous driving, a domain of emerging & long-term significance. Specialised commonsense theories (e.g., about multi-sensory integration & multi-agent belief merging, contextual knowledge) may be incorporated based on requirements. Our ongoing focus is to develop a novel dataset emphasising semantics and (commonsense) explainability; this is driven by mixed-methods research -AI, Psychology, HCI-for the study of driving behaviour in low-speed, complex urban environments with unstructured traffic. Here, emphasis is on natural interactions (e.g., gestures, joint attention) amongst drivers, pedestrians, cyclists etc. Such interdisciplinary studies are needed to better appreciate the complexity and spectrum of varied human-centred challenges in autonomous driving, and demonstrate the significance of integrated vision & semantics solutions in those contexts.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "Partial funding by the German Research Foundation (DFG) via the CRC 1320 EASE -Everyday Activity Science and Engineering\" (www.ease-crc.org), Project P3: Spatial Reasoning in Everyday Activity is acknowledged.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Based on this prediction we can then define a rule that gives a warning if a hidden entity may reappear in front of the ve- ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "trk(trk_3, car). trk_state(trk_3, active)", "journal": "", "year": "", "authors": " . "}, {"ref_id": "b1", "title": "", "journal": "", "year": "", "authors": ") . . . "}, {"ref_id": "b2", "title": "occurs_at(hides_behind(trk_41,trk_3),179))", "journal": "", "year": "", "authors": ""}, {"ref_id": "b3", "title": "anticipate(unhides_from_behind(Trk1, Trk2), T) :-time(T), curr_time < T, holds_at(hidden_by(Trk1, Trk2), curr_time), topology(proper_part, Trk1, Trk2), movement(moves_out_of, Trk1, Trk2, T)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b4", "title": "PosX, PosY) :-time(T), curr_time < T, T1 = T-curr_time, box2d", "journal": "", "year": "", "authors": ""}, {"ref_id": "b5", "title": "495) warning(hidden_entity_in_front(Trk1, T)) :-time(T), T-curr_time < anticipation_threshold, anticipate(unhides_from_behind(Trk1, _), T), position", "journal": "", "year": "", "authors": ""}, {"ref_id": "b6", "title": "trk(trk_41, car).trk_state(trk_41, active)", "journal": "", "year": "", "authors": ") . . . ; . . "}, {"ref_id": "b7", "title": "occurs_at(hides_behind(trk_41,trk_3),179))", "journal": "", "year": "", "authors": ""}, {"ref_id": "b8", "title": "anticipate(unhides_from_behind(Trk1, Trk2), T) :-time(T), curr_time < T, holds_at(hidden_by(Trk1, Trk2), curr_time), topology(proper_part, Trk1, Trk2), movement(moves_out_of, Trk1, Trk2, T)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b9", "title": "PosX, PosY) :-time(T), curr_time < T, T1 = T-curr_time, box2d", "journal": "", "year": "", "authors": ""}, {"ref_id": "b10", "title": "495) warning(hidden_entity_in_front(Trk1, T)) :-time(T), T-curr_time < anticipation_threshold, anticipate(unhides_from_behind(Trk1, _), T), position", "journal": "", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: The clear mot metrics", "journal": "Morgan Kaufmann", "year": "1983", "authors": "[ References;  Aditya"}, {"ref_id": "b12", "title": "Representing space in cognition: Interrelations of behavior, language, and formal models", "journal": "Oxford University Press", "year": "2008", "authors": "Loke ; Mehul Bhatt; Seng W Loke;  Bhatt"}, {"ref_id": "b13", "title": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "journal": "J. Artif. Intell. Res. (JAIR)", "year": "2015", "authors": ""}, {"ref_id": "b14", "title": "Abducing qualitative spatio-temporal histories from partial observations", "journal": "Morgan & Claypool Publishers", "year": "2002", "authors": " Gebser"}, {"ref_id": "b15", "title": "Jiefei Ma, Rob Miller, Leora Morgenstern, and Theodore Patkos. An epistemic event calculus for asp-based reasoning about knowledge of the past, present and future", "journal": "EasyChair", "year": "1991", "authors": "; Kautz; A Henry;  Kautz;  Liu"}, {"ref_id": "b16", "title": "Inderjeet Mani and James Pustejovsky. Interpreting Motion -Grounded Representations for Spatial Language, volume 5 of Explorations in language and space", "journal": "Oxford University Press", "year": "2012", "authors": "Pustejovsky "}, {"ref_id": "b17", "title": "Rob Miller, Leora Morgenstern, and Theodore Patkos. Reasoning about knowledge and action in an epistemic event calculus", "journal": "", "year": "2013", "authors": " Milan"}, {"ref_id": "b18", "title": "Shaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks", "journal": "AAAI Press", "year": "1998", "authors": "; Philippe Muller; ; Muller"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Out of sight but not out of mind; the case of hidden entities: an occluded cyclist.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: A General Online Abduction Framework / Conceptual Overview", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "MATCHING TRACKS AND DETECTIONS I MATCHING TRACKS AND DETECTIONS 1{ assign(Trk, Det): det(Det, _, _); end(Trk); ignore_trk(Trk); halt(Trk); resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _). 1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _). I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh. I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": ". ... ... occurs_at(hides_behind(trk_41,trk_3),179)) ... anticipate(unhides_from_behind(Trk1, Trk2), T) :time(T), curr_time < T, holds_at(hidden_by(Trk1, Trk2), curr_time), topology(proper_part, Trk1, Trk2),", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "CONSTRAINTS ON MATCHING I MATCHING TRACKS AND DETECTIONS 1{ assign(Trk, Det): det(Det, _, _); end(Trk); ignore_trk(Trk); halt(Trk); resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _). 1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _). I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh. I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "IVISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "have to hold for the event to possibly occur, and the effects the occurrence of the event has on the properties of the objects, i.e., the value of the visibility fluent changes to f ully occluded.OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS I MATCHING TRACKS AND DETECTIONS 1{ assign(Trk, Det): det(Det, _, _); end(Trk); ignore_trk(Trk); halt(Trk); resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _). 1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _). I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh.I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": ".(Trk1, T)) :time(T), T-curr_time < anticipation_threshold, anticipate(unhides_from_behind(Trk1, _), T), position(in_front, interpolated_pos(Trk1, T)).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "IVISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": ".curr_time < T, T1 = T-curr_time, box2d(Trk1, X, Y,_,_), trk_mov(Trk1, MovX, MovY), PosX = X+MovX * T1, PosY = Y+MovX * T1. anticipate(unhides_from_behind(trk_41, trk_2), 202) point2d(interpolated_position(trk_41, 202), 738, 495) warning(hidden_entity_in_front(Trk1, T)) :time(T), T-curr_time < anticipation_threshold, anticipate(unhides_from_behind(Trk1, _), T), position(in_front, interpolated_pos(Trk1, T)).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Step 3 .3FINDING THE OPTIMAL HYPOTHESIS To ensure an optimal assignment, we use ASP based optimization to maximize the matching likelihood between matched pairs of tracks and detections. Towards this, we first define the matching likelihood based on the Intersection over Union (IoU) between the observations and the predicted boxes for each track as described in [Bewley et al., 2016]: ASSIGNMENT LIKELIHOOD I MATCHING TRACKS AND DETECTIONS 1{ assign(Trk, Det): det(Det, _, _); end(Trk); ignore_trk(Trk); halt(Trk); resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _). 1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _). I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh. I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "... occurs_at(hides_behind(trk_41,trk_3),179)) ... anticipate(unhides_from_behind(Trk1, Trk2), T) :time(T), curr_time < T, holds_at(hidden_by(Trk1, Trk2), curr_time), topology(proper_part, Trk1, Trk2), movement(moves_out_of, Trk1, Trk2, T). point2d(interpolated_position(Trk, T), PosX, PosY) :time(T), curr_time < T, T1 = T-curr_time, box2d(Trk1, X, Y,_,_), trk_mov(Trk1, MovX, MovY), PosX = X+MovX * T1, PosY = Y+MovX * T1. anticipate(unhides_from_behind(trk_41, trk_2), 202) point2d(interpolated_position(trk_41, 202), 738, 495) warning(hidden_entity_in_front(Trk1, T)) :time(T), T-curr_time < anticipation_threshold,", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "IMATCHING TRACKS AND DETECTIONS 1{ assign(Trk, Det): det(Det, _, _); end(Trk); ignore_trk(Trk); halt(Trk); resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _). 1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _). I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh. I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "... occurs_at(hides_behind(trk_41,trk_3),179)) ... anticipate(unhides_from_behind(Trk1, Trk2), T) :time(T), curr_time < T, holds_at(hidden_by(Trk1, Trk2), curr_time), topology(proper_part, Trk1, Trk2), movement(moves_out_of, Trk1, Trk2, T). point2d(interpolated_position(Trk, T), PosX, PosY) :time(T), curr_time < T, T1 = T-curr_time, box2d(Trk1, X, Y,_,_), trk_mov(Trk1, MovX, MovY), PosX = X+MovX * T1, PosY = Y+MovX * T1. anticipate(unhides_from_behind(trk_41, trk_2), 202) point2d(interpolated_position(trk_41, 202), 738, 495)", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "OPTIMIZE EVENT AND ASSOCIATION COSTS I MATCHING TRACKS AND DETECTIONS 1{ assign(Trk, Det): det(Det, _, _); end(Trk); ignore_trk(Trk); halt(Trk); resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _). 1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _). I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh. I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "1{ assign(Trk, Det): trk(Trk, _); start(Det); ignore_det(Det); resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _).I INTEGRITY CONSTRAINTS ON MATCHING :-assign(Trk, Det), not assignment_constraints(Trk, Det). assignment_constraints(Trk, Det) :trk(Trk, Trk_Type), trk_state(Trk, active), det(Det, Det_Type, Conf), Conf > conf_thresh_assign, match_type(Trk_Type, Det_Type), iou(Trk, Det, IOU), IOU > iou_thresh.I VISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}. I OPTIMIZE EVENT AND ASSOCIATION COSTS #minimize {5@2,Trk: end(Trk)}. #minimize {5@2,Det: start(Det)}. trk(trk_3, car). trk_state(trk_3, active). ... ... trk(trk_41, car).trk_state(trk_41, active). ... ... det(det_1, car, 98). ... box2d(trk_3, 660, 460, 134, 102). ... ... box2d(trk_41, 631, 471, 40, 47). ... ... occurs_at(hides_behind(trk_41,trk_3),179)) ... anticipate(unhides_from_behind(Trk1, Trk2), T) :time(T), curr_time < T, holds_at(hidden_by(Trk1, Trk2), curr_time), topology(proper_part, Trk1, Trk2), movement(moves_out_of, Trk1, Trk2, T). point2d(interpolated_position(Trk, T), PosX, PosY) :time(T), curr_time < T, T1 = T-curr_time, box2d(Trk1, X, Y,_,_), trk_mov(Trk1, MovX, MovY), PosX = X+MovX * T1, PosY = Y+MovX * T1. anticipate(unhides_from_behind(trk_41, trk_2), 202) point2d(interpolated_position(trk_41, 202), 738, 495) warning(hidden_entity_in_front(Trk1, T)) :time(T), T-curr_time < anticipation_threshold, anticipate(unhides_from_behind(Trk1, _), T), position(in_front, interpolated_pos(Trk1, T)).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "IVISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": ".", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "IVISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "., T-curr_time < anticipation_threshold, anticipate(unhides_from_behind(Trk1, _), T), position(in_front, interpolated_pos(Trk1, T)).", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "IVISIBILITY -FLUENT AND POSSIBLE VALUES fluent(visibility(Trk)) :-trk(Trk, _). possVal(visibility(Trk), fully_visible) :-trk(Trk, _). possVal(visibility(Trk), partially_visible) :-trk(Trk,_). possVal(visibility(Trk), not_visible) :-trk(Trk, _). I OCCLUSION -EVENT, EFFECTS AND (SPATIAL) CONSTRAINTS event(hides_behind(Trk1,Trk2)) :-trk(Trk1,_),trk(Trk2,_). causesValue(hides_behind(Trk1, Trk2), visibility(Trk1), not_visible, T) :trk(Trk1,_), trk(Trk2,_), time(T). :-occurs_at(hides_behind(Trk1, Trk2), curr_time), trk(Trk1,_), trk(Trk2,_), not position(overlapping_top, Trk1, Trk2). I GENERATING HYPOTHESES ON EVENTS 1{ occurs_at(hides_behind(Trk, Trk2), curr_time): trk(Trk2,_); ... }1 :-halt(Trk). I ASSIGNMENT LIKELIHOOD assignement_prob(Trk, Det, IOU) :det(Det, _, _), trk(Trk, _), iou(Trk, Det, IOU). I MAXIMIZING ASSIGNMENT LIKELIHOOD #maximize {(Prob)@1,Trk,Det : assign(Trk, Det), assignment_prob(Trk, Det, Prob)}.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": ", ..., t n }. MT oi = (\u03b5 ts , ..., \u03b5 te ) represents the motion track of a single object o i , where t s and t e denote the start and end time of the track and \u03b5 ts to \u03b5 te denotes the spatial entity (E) -e.g., the axis-aligned bounding box-corresponding to the object o i at time points t s to t e . The spatial configuration of the scene and changes within it are characterised based on the qualitative spatiotemporal relationships (R) between the domain objects. For the running and demo examples of this paper, positional relations on axis-aligned rectangles based on the rectangle algebra (RA)[Balbiani et al., 1999] suffice; RA uses the relations of Interval Algebra (IA)[Allen, 1983] R IA \u2261 {before, Visual imagery (V), and background knowledge \u03a3 \u2261 def \u03a3 dyn \u222a \u03a3st", "figure_data": "Algorithm 1: Online Abduction(V, \u03a3)Result: Visual Explanations (EX P)(also: Refer Fig 3)3 4 5 6 7 8VOt \u2190 observe(Vt) Pt \u2190 \u2205, MLt \u2190 \u2205 for trk \u2208 MT t\u22121 do p trk \u2190 kalman predict(trk) Pt \u2190 Pt \u222a p trk for obs \u2208 VOt do10MLt \u2190 MLt \u222a ml trk,obs11Abduce(< H assign t \u03a3\u2227H events \u2227[H assign , H events t t \u2227H events >), such that: t ] |= VOt \u2227Pt \u2227MLt (Step 2)"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Level Events For the length of this paper, we restrict to high-level visuo-spatial abducibles pertaining to object persistence and visibility (Table1):(1). Occlusion: Objects can disappear or reappear as result of occlusion with other objects; (2). Entering / Leaving the Scene: Objects can enter or leave the scene at the borders of the field of view;(3). Noise and Missing Observation: (Missing-)observations can be the result of faulty detections.Lets take the case of occlusion: functional fluent visibility could be denoted f ully visible, partially occluded or f ully occluded:", "figure_data": "I MATCHING TRACKS AND DETECTIONS1{ assign(Trk, Det): det(Det, _, _);end(Trk); ignore_trk(Trk); halt(Trk);resume(Trk, Det): det(Det, _, _) }1 :-trk(Trk, _).1{ assign(Trk, Det): trk(Trk, _);start(Det); ignore_det(Det);resume(Trk, Det): trk(Trk, _). }1 :-det(Det, _, _).I INTEGRITY CONSTRAINTS ON MATCHING:-assign(Trk, Det), not assignment_constraints(Trk, Det).assignment_constraints(Trk, Det) :-trk(Trk, Trk_Type), trk_state(Trk, active),det(Det, Det_Type, Conf), Conf > conf_thresh_assign,match_type(Trk_Type, Det_Type),iou(Trk, Det, IOU), IOU > iou_thresh.VISIBILITY -FLUENT AND POSSIBLE VALUESI GENERATING HYPOTHESES ON EVENTS1{ occurs_at(hides_behind(Trk, Trk2), curr_time):trk(Trk2,_); ... }1 :-halt(Trk)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "leaves fov(Trk)   Track Trk leaves the field of view.hides behind(Trk1, Trk2) Track Trk1 hides behind track Trk2.unhides from behind(Trk1, Trk2) Track Trk1 unhides from behind track Trk2.missing detections(Trk) Missing detections for track Trk. Track Trk is in the field of view.", "figure_data": "FLUENTSValuesDescriptionin fov(Trk) hidden by(Trk1, Trk2) visibility(Trk){true;false} {true;false} {fully visible; fully occluded} partially occluded;Track Trk1 is hidden by Trk2. Visibility state of track Trk."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": (Dis)Appearance Abducibles; Events and Fluents Explaining"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": ", car). trk_state(trk_3, active). ... ... trk(trk_41, car).trk_state(trk_41, active). ... ... det(det_1, car, 98). ... box2d(trk_3, 660, 460, 134, 102). ...", "figure_data": "I OPTIMIZE EVENT AND ASSOCIATION COSTS#minimize {5@2,Trk: end(Trk)}.#minimize {5@2,Det: start(Det)}.trk(trk_3"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Safety-Critical Situations    ", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "Our evaluation uses the KITTI object tracking dataset[Geiger et al., 2012], which is a community established benchmark dataset for autonomous cars: it consists of 21 training and 29 test scenes, and provides accurate track annotations for 8 object classes (e.g., car, pedestrian, van, cyclist). We also evaluate tracking results using the more general cross-domain Multi-Object Tracking (MOT) dataset[Milan et al., 2016] established as part of the MOT Challenge; it consists of 7 training and 7 test scenes which are highly unconstrained videos filmed with both static and moving cameras. We evaluate on the available groundtruth for training scenes of both KITTI using YOLOv3 detections, and MOT17 using the provided faster RCNN detections.", "figure_data": "Empirical Evaluation For online sensemaking, evaluation focusses on accuracy of abduced motion tracks, real-time per-formance, and the tradeoff between performance and accu-racy. \u2022 Evaluating Object Tracking For evaluating accuracy (MOTA) and precision (MOTP) of abduced object tracks we follow the ClearMOT [Bernardin and Stiefelhagen, 2008] evaluation schema. Results (TableI OPTIMIZE EVENT AND ASSOCIATION COSTS#minimize {5@2,Trk: end(Trk)}.#minimize {5@2,Det: start(Det)}."}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "Evaluation of Tracking Performance; accuracy (MOTA), precision (MOTP), mostly tracked (MT) and mostly lost (ML) tracks, false positives (FP), false negatives (FN), identity switches (ID Sw.), and fragmentation (Frag.). Online Performance and Scalability; performance for pretrained detectors on the 'cars' class of KITTI dataset, and processing time relative to the no. of tracks on synthetic dataset.", "figure_data": "DETECTORRecallMOTAMOTPf ps detf ps abdYOLOv3 SSD FRCNN0.690 0.599 0.62450.5 % 30.63 % 37.96 %74.76 % 77.4 % 72.9 %45 8 533.9 46.7 32.0ms/frame1K 1 10 10051020501 fps 100 30 fps 15 fpsNo. tracks 5 10 20 50 100ms/f rame 23.33 31.36 62.08 511.83 3996.38f ps 42.86 31.89 16.11 1.95 0.25num. tracksFigure 5:2"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Let \u03a3 \u2261 def \u03a3 dyn <\u03a6, \u0398> \u222a \u03a3st <O, E, T , MT , R>", "formula_coordinates": [3.0, 83.84, 398.35, 213.16, 10.8]}, {"formula_id": "formula_1", "formula_text": "\u03a3 \u2227 H events \u2227 [H assign t \u2227 H events t ] |= VO t \u2227 P t \u2227 ML t", "formula_coordinates": [4.0, 69.47, 91.19, 227.03, 14.22]}, {"formula_id": "formula_2", "formula_text": "INTEGRITY", "formula_coordinates": [4.0, 62.33, 407.92, 31.88, 7.4]}], "doi": ""}