{"title": "ArcaneQA: Dynamic Program Induction and Contextualized Encoding for Knowledge Base Question Answering", "authors": "Yu Gu; Yu Su", "pub_date": "", "abstract": "Question answering on knowledge bases (KBQA) poses a unique challenge for semantic parsing research due to two intertwined challenges: large search space and ambiguities in schema linking. Conventional rankingbased KBQA models, which rely on a candidate enumeration step to reduce the search space, struggle with flexibility in predicting complicated queries and have impractical running time. In this paper, we present ArcaneQA, a novel generation-based model that addresses both the large search space and the schema linking challenges in a unified framework with two mutually boosting ingredients: dynamic program induction for tackling the large search space and dynamic contextualized encoding for schema linking. Experimental results on multiple popular KBQA datasets demonstrate the highly competitive performance of ArcaneQA in both effectiveness and efficiency. 1   ", "sections": [{"heading": "Introduction", "text": "Modern knowledge bases (KBs) contain a wealth of structured knowledge. For example, FREEBASE (Bollacker et al., 2008) contains over 45 million entities and 3 billion facts across more than 100 domains, while GOOGLE KNOWLEDGE GRAPH has amassed over 500 billion facts about 5 billion entities (Sullivan, 2020). Question answering on knowledge bases (KBQA) has emerged as a user-friendly solution to access the massive structured knowledge in KBs.\nKBQA is commonly modeled as a semantic parsing problem (Zelle and Mooney, 1996;Zettlemoyer and Collins, 2005) with the goal of mapping a natural language question into a logical form that can be executed against the KB (Berant et al., 2013;Cai and Yates, 2013;Yih et al., 2015). Compared with other semantic parsing settings such as text-to-SQL parsing (Zhong et al., 2017;Yu et al., 2018), ", "publication_ref": ["b6", "b30", "b41", "b42", "b2", "b7", "b38", "b45", "b40"], "figure_ref": [], "table_ref": []}, {"heading": "(b) Contextualized Encoding", "text": "Step 2\nStep 1\nStep 1\nStep 3\n(ARGMAX #3 vintage)\nFigure 1: KBQA is commonly modeled as semantic parsing with the goal of mapping a question into an executable program. (a) A high-level illustration of our program induction procedure. The target program is induced by incrementally synthesizing a sequence of subprograms (#1-3). The execution of each subprogram can significantly reduce the search space of subsequent subprograms. (b) Alignments between question words and schema items at different steps achieved by a BERT encoder. A pre-trained language model like BERT can jointly encode the question and schema items to get the contextualized representation at each step, which further guides the search process.\nwhere the underlying data is moderate-sized, the massive scale and the broad-coverage schema of KBs makes KBQA a uniquely challenging setting for semantic parsing research. The unique difficulty stems from two intertwined challenges: large search space and ambiguities in schema linking. On the one hand, transductive semantic parsing models that are highly effective in other semantic parsing settings (Dong and Lapata, 2016;Wang et al., 2020) struggle with the large vocabulary size and often generate logical forms (i.e., formal queries) 2 that are not faithful to the underlying KB (Gu et al., 2021;Xie et al., 2022). Therefore, a candidate enumeration and ranking approach is commonly adopted for KBQA (Berant and Liang, 2014;Yih et al., 2015;Abujabal et al., 2017;Lan et al., 2019a;Sun et al., 2020;Gu et al., 2021;Ye et al., 2021). However, these methods have to make various compromises on the complexity of admissible logical forms to deal with the large search space. Not only does this limit the type of answerable questions, but it also leads to impractical runtime performance due to the time-consuming candidate enumeration (Gu et al., 2021). On the other hand, schema linking, 3 i.e., mapping natural language to the corresponding schema items in the KB (e.g., in Figure 1, wine sub region is a linked schema items), is also a core challenge of KBQA.\nCompared with text-to-SQL parsing (Hwang et al., 2019;Zhang et al., 2019;Wang et al., 2020), the broad schema of KBs and the resulting ambiguity between schema items makes accurate schema linking more important and challenging for KBQA. Recent studies show that contextualized joint encoding of natural language questions and schema items with BERT (Devlin et al., 2019) can significantly boost the schema linking accuracy (Gu et al., 2021;Chen et al., 2021). However, existing methods still struggle with the large search space and need to encode a large number of schema items, which is detrimental to both accuracy and efficiency.\nWe present ArcaneQA (Dynamic Program Induction and Contextualized Encoding for Question Answering), a generation-based KBQA model that addresses both the large search space and the schema linking challenges in a unified framework. Compared with the predominant ranking-based KBQA models, our generationbased model can prune the search space on the fly and thus is more flexible to generate diverse queries without compromising the expressivity or complexity of answerable questions. Inspired by prior work (Dong and Lapata, 2016;Liang et al., 2017;Semantic Machines et al., 2020;Chen et al., 2021), we model KBQA using the encoder-decoder framework. However, instead of top-down decoding with grammar-level constraints as in prior work, which does not guarantee the faithfulness of the generated queries to the underlying KB, ArcaneQA performs dynamic program induction (Liang et al., 2017;Semantic Machines et al., 2020), where we incrementally synthesize a program by dynamically predicting a sequence of subprograms to answer a question; i.e., bottom-up parsing (Cheng et al., 2019;Rubin and Berant, 2021). Each subprogram is grounded to the KB and its grounding (i.e., denotation or execution results) can further guide an efficient search for faithful programs (see Figure 1(a)). In addition, we unify the meaning representation (MR) for programs in KBQA using S-expressions and support more diverse operations over the KB (e.g., numerical operations such as COUNT/ARGMIN/ARGMAX and diverse graph traversal operations).\nAt the same time, we employ pre-trained language models (PLMs) like BERT to jointly encode the question and schema items and get the contextualized representation of both, which implicitly links words to the corresponding schema items via self-attention. One unique feature to note is that the encoding is also dynamic: at each prediction step, only the set of admissible schema items determined by the dynamic program induction process needs to be encoded, which allows extracting the most relevant information from the question for each prediction step while avoiding the need to encode a large number of schema items. Figure 1(b) illustrates the contextualization of different steps via the attention heatmaps of BERT. In this example, the attention of each question word over candidate schema items serves as a strong indicator of the gold items for both steps (i.e., wine sub region for step 1 and percentage alcohol for step 3). The two key ingredients of our model are mutually boosting: dynamic program induction significantly reduces the number of schema items that need to be encoded, while dynamic contextualized encoding intelligently guides the search process.\nOur main contribution is as follows: a) We propose a novel generation-based KBQA model that is flexible to generate diverse complex queries while also being more efficient than ranking-based models. b) We propose a novel strategy to effectively employ PLMs to provide contextualized encoding for KBQA. c) We unify the meaning representation (MR) of different KBQA datasets and support more diverse operations. d) With our unified MR, we evaluate our model on three popular KBQA datasets and show highly competitive results.", "publication_ref": ["b13", "b34", "b16", "b36", "b3", "b38", "b0", "b20", "b31", "b16", "b37", "b16", "b17", "b34", "b12", "b16", "b8", "b13", "b22", "b28", "b8", "b22", "b28", "b10", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Ranking-Based KBQA. To handle the large search space in KBQA, existing studies typically rely on hand-crafted templates with a pre-specified maximum number of relations to enumerate candidate logical forms (Yih et al., 2015;Abujabal et al., 2017;Lan et al., 2019a;Bhutani et al., 2019Bhutani et al., , 2020, which suffers from limited expressivity and scalability. For example, Yih et al. (2015) limit the candidate programs to be a core relational chain, whose length is at most two, plus constraints. Ye et al. (2021) additionally adopts a post-generation module to revise the enumerated logical forms into more complicated ones, however, their method still heavily depends on the candidate enumeration step. In addition, the time-consuming candidate enumeration results in impractical online inference time for ranking-based models. In contrast, ArcaneQA obviates the need for candidate enumeration by pruning the search space on the fly and thus can generate more diverse and complicated programs within practical running time.\nGeneration-Based KBQA. To relax the restriction on candidate enumeration, some recent efforts are made to reduce the search space using beam search (Lan et al., 2019b;Chen et al., 2019;Lan and Jiang, 2020), however, Lan et al. (2019b) and Chen et al. (2019) can only generate programs of path structure, while Lan and Jiang (2020) follow the query graph structure proposed by Yih et al. (2015). A few recent studies (Liang et al., 2017;Chen et al., 2021) formulate semantic parsing over the KB as sequence transduction using encoderdecoder models to enable more flexible generation. Chen et al. (2021) apply schema-level constraints to eliminate ill-formed programs from the search space, however, they do not guarantee the faithfulness of predicted programs. Similar to Liang et al. (2017), our dynamic program induction uses KB contents-level constraints to ensure the faithfulness of generated programs, but we extend it to handle more complex and diverse questions and also use it jointly with dynamic contextualized encoding.\nUsing PLMs in Semantic Parsing. PLMs have been widely applied in many semantic parsing tasks, typically being used to jointly encode the input question and schema items (Hwang et al., 2019;Zhang et al., 2019;Wang et al., 2020;Scholak et al., 2021). However, PLMs have been under-exploited in KBQA. One major difficulty of using PLMs in KBQA lies in the high volume of schema items in a KB; simply concatenating all schema items with the input question for joint encoding, as commonly done in text-to-SQL parsing, will vastly exceed PLMs' maximum input length. Existing KBQA models either use PLMs to provide features for downstream classifiers (Lan and Jiang, 2020;Sun et al., 2020) or adopts a pipeline design to identify a smaller set of schema items beforehand and only use PLMs to encode these identified items (Gu et al., 2021;Chen et al., 2021), which can lead to error propagation. By comparison, ArcaneQA can fully exploit PLMs to provide contextualized representation for the question and schema items dynamically, where only the most relevant schema items are encoded at each step. More recently, Ye et al. ( 2021) use T5 (Raffel et al., 2019) to output a new program given a program as input, while T5's decoder generates free-formed text and does not always produce faithful programs. By contrast, Ar-caneQA only uses PLMs for encoding and uses its customized decoder with a faithfulness guarantee. In this paper, we follow Gu et al. (2021) to use S-expressions as our meaning representation due to their expressivity and simplicity. To support more diverse operations over the KB, we extend their definitions with two additional functions CONS and TC, which are used to support constraints with implicit entities and temporal constraints respectively (see details in Appendix A). For implicit entities, consider the question \"What was Elie Wiesel's father's name?\", whose target query involves two entities: Elie Wiesel and Male. The entity Male is an implicit constraint rather than a named entity, 4 and it is used as an argument of CONS in the target logical form: (CONS (JOIN people.person.children Elie Wiesel) people.person.gender Male). TC works in a similar way, with the difference being that the constraint should be a temporal expression (e.g., 2015-08-10) rather than an implicit entity.", "publication_ref": ["b38", "b0", "b20", "b4", "b5", "b37", "b21", "b9", "b18", "b21", "b9", "b18", "b38", "b22", "b8", "b8", "b22", "b17", "b34", "b27", "b18", "b31", "b16", "b8", "b24", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Approach", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "The core idea of our generation-based model is to gradually expand a subprogram (i.e., a partial query) into the finalized target program, instead of enumerating all possible finalized programs from the KB directly, which suffers from combinatorial explosion. There are two common strategies to instantiate the idea of gradual subprogram expansion, depending on the type of meaning representation being used. For a graph-like meaning representation, we can directly perform graph search over the KB to expand a subprogram (Chen et al., 2019;Lan and Jiang, 2020). Also, we can linearize a program into a sequence of tokens and perform decoding in the token space (Liang et al., 2017;Scholak et al., 2021). Because S-expressions can be easily converted into sequences of tokens, we choose to follow the second strategy and take advantage of the encoder-decoder framework, which has been a de facto choice for many semantic parsing tasks. Concretely, ArcaneQA learns to synthesize the target program by dynamically generating a sequence of subprograms token by token until predicting \u27e8EOS\u27e9, where each subsequent subprogram is an expansion from one or more preceding subprograms (denoted as parameters in the subsequent subprogram). Formally, the goal is to map an input question q = x 1 , ..., x |q| to a sequence of subprograms  (Sutskever et al., 2014;Bahdanau et al., 2015), in which the conditional proba-4 WEBQSP is the only dataset we consider that has this feature. Though there might be a more systematic way to differentiate implicit entities from named entities, we choose an expedient way to collect implicit entities from the training data according to whether an entity is explicitly mentioned in the question. bility p(o|q) is decomposed as:\no = o 1 1 , ..., o 1 |o 1 | , ..., o k 1 , ..., o k |o k | = y 1 , ..., y |o| ,\np(o|q) = |o| t=1 p(y t |y <t , q),(1)\nwhere each token y t is either a token from the vocabulary V or an intermediate subprogram from the set S storing all previously generated subprograms.\nV comprises all schema items in K, syntactic symbols in S-expressions (i.e., parentheses and function names), and the special token \u27e8EOS\u27e9. S initially contains the identified entities in the question (e.g., #1 in Figure 1). Every time a subprogram is predicted, it is executed and added to S (e.g., #2 in Figure 1). ArcaneQA builds on two key ideas: dynamic program induction and dynamic contextualized encoding (see Figure 2). At each decoding step, Ar-caneQA only makes a prediction from a small set of admissible tokens instead of the entire vocabulary. This is achieved by the dynamic program induction framework (subsection 4.2), which effectively prunes the search space by orders of magnitude and guarantees that the predicted programs are faithful to the KB. In addition, we dynamically apply BERT to provide contextualized joint encoding for both the question and admissible tokens at each decoding step (subsection 4.3). In this way, we allow the contextualized encoding to only focus on the most relevant information without introducing noise from irrelevant tokens.", "publication_ref": ["b9", "b18", "b22", "b27", "b32", "b1"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "Dynamic Program Induction", "text": "Dynamic program induction capitalizes on the idea that a complicated program can be gradually expanded from a list of subprograms. To ensure the expanded program is faithful to the KB, we query the KB with a subprogram to expand and a function defined in Table 4 to get a set of admissible actions (tokens). For example, in Figure 2, given #1 and the function JOIN, the admissible actions are defined by predicting a relation connecting to the execution result of #1 (i.e., Tulum Valley), and there are only four relations to choose from (e.g., appellation and wine sub region). Table 1 shows a comprehensive description of expansion rules for different functions. With these rules, ArcaneQA can greatly reduce the search space for semantic parsing over the KB dynamically. The reduced candidate space further allows us to perform dynamic contextualized encoding (subsection 4.3). Within our encoder-decoder framework, this idea is implemented using constrained decoding (Liang et al., 2017;Scholak et al., 2021), i.e., at each decoding step, a small set of admissible tokens from the vocabulary is determined based on the decoding history following predefined rules. The expansion rules in Table 1 have already comprised part of our rules for constrained decoding. In addition, several straightforward grammar rules are applied to ensure that the generated programs are well-formed. For instance, after predicting \"(\", the admissible tokens for the next step can only be a function name. After predicting a function name, the decoder can only choose a preceding subprogram to expand. After predicting \")\", the admissible tokens for next step can only be either \"(\", indicating the start of a new subprogram, or \"\u27e8EOS\u27e9\", denoting termination. The decoding process can be viewed as a sequential decision-making process, which decomposes the task of finding a program from the enormous search space into making decisions from a sequence of smaller search spaces.", "publication_ref": ["b22", "b27"], "figure_ref": ["fig_3"], "table_ref": ["tab_8"]}, {"heading": "Dynamic Contextualized Encoding", "text": "In semantic parsing, PLMs have typically been used to jointly encode the input question and all schema items via concatenation (Hwang et al., 2019;Zhang et al., 2019;Wang et al., 2020). However, direct concatenation is not feasible for KBQA due to a large number of schema items. Instead of obtaining a static representation for the question and items from V before decoding (Gu et al., 2021;Chen et al., 2021), we propose to do dynamic contextualized encoding at each decoding step; for each step, we use BERT to jointly encode the question and only the admissible tokens from V. ArcaneQA's dynamic program induction vastly reduces the number of candidate tokens at each step and allows us to concatenate the question and the admissible tokens into a compact sequence: 5\n[CLS], x 1 , ..., x |q| , [SEP], s t 1 , ..., s t m , [SEP]\nwhere {s t i } \u2282 V are admissible tokens at\nCurrent function Admissible actions JOIN {r|h \u2208 #, (h, r, t) \u2208 K r } AND {v|v \u2208 S, v \u2229 # \u0338 = \u2205} \u222a {c|e \u2208 #, (e, c) \u2208 K c } ARGMAX/ARGMIN {r|h \u2208 #, t \u2208 L, (h, r, t) \u2208 K r } LT(LE/GT/GE) {r|t < (\u2264 / > / \u2265)#, (h, r, t) \u2208 K r } COUNT {)} CONS {(r, t)|h \u2208 #, (h, r, t) \u2208 K r } TC {(r, t)|h \u2208 #, (h, r, t) \u2208 K r , t \u2208 L is a temporal expression}\nTable 1: A set of rules to expand a preceding subprogram given a function. The execution of the subprogram is denoted as #. These expansion rules reduce the search space significantly with a faithfulness guarantee. COUNT takes no other argument, so the only admissible token is \")\".\nstep t and |{s t i }| = m. After feeding the concatenated sequence to BERT, we obtain the question representation Q t = (x 1 , ..., x q ) by further feeding the outputs from BERT to an LSTM encoder. For each admissible token, we represent it by averaging BERT outputs corresponding to its wordpieces. In this way, we also obtain the embedding matrix W t \u2208 R m\u00d7d , where each row corresponds to the embedding of an admissible token. The contextualized representation Q t and W t are both dynamically computed at each time step. Words and corresponding schema items are implicitly linked to each other via BERT's self-attention.", "publication_ref": ["b17", "b34", "b16", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Decoding", "text": "We use an LSTM decoder. At decoding step t, given the hidden state h t\u22121 and input c t\u22121 , we obtain the updated hidden state h t by:\nh t = LSTM \u03b8 (h t\u22121 , c t\u22121 ) (2)\nwhere our LSTM decoder is parameterized by \u03b8. With h t and W t -the embedding matrix of admissible tokens (determined by dynamic program induction)-we obtain the probability of generating a token from the admissible tokens:\np(y t = s ti |q, y <t ) = [Softmax(W t h t )] i (3)\nThe input c t for the next step is obtained via the concatenation of the contextualized embedding of the current output token and the weighted representation of the question based on attention:\na t = softmax(Q t h t )(4)\nq t = (a t ) T Q t (5) c t = [[W t ] j ; q t ](6)\nwhere ; denotes concatenation, and j denotes the index of the predicted y t in W t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Training and Inference", "text": "We train ArcaneQA with question-program pairs using cross entropy loss. The model learns to maximize the probability of predicting the gold token out of a small set of admissible tokens at each step, which is different from training a conventional Seq2Seq model using a static vocabulary. During inference, ArcaneQA assumes an entity linker to identify a set of entities from the question at the beginning of program induction. However, the entity linker may identify false entities. To deal with it, ArcaneQA initiate its decoding process with different hypotheses from the set of entities. Basically, it tries out all possible combinations of the identified entities (i.e., the power set of the identified entities), considering that our entity linker normally can only identify no more than two entities from a question.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Setup", "text": "Datasets. We evaluate ArcaneQA on three KBQA datasets covering diverse KB queries. GRAILQA (Gu et al., 2021) is a large-scale KBQA dataset that contains complex questions with various functions, including comparatives, superlatives, and counting. It evaluates the generalizability of KBQA at three levels: i.i.d., compositional and zero-shot. GRAPHQ (Su et al., 2016) (Berant et al., 2013) with annotated logical forms.\nAll questions in it are collected from Google query logs, featuring more realistic and complicated information needs such as questions with temporal constraints.\nThe total number of questions in GRAILQA, GRAPHQ, and WEBQ is 64,331, 5,166, and 4,737 respectively.\nEvaluation Metrics. For GRAILQA, we use their official evaluation script with two metrics, EM, i.e., program exact match accuracy, and F1, which is computed based on the predicted and the gold answer set. For GRAPHQ and WEBQSP, we follow the standard practice and report F1.\nModels for Comparison. We compare ArcaneQA with the previous best-performing models on three different datasets. For GRAILQA and WEBQSP, the state-of-the-art model is RnG-KBQA (Ye et al., 2021). Though RnG-KBQA uses T5 to decode the target program as unconstrained sequence transduction, it still heavily depends on candidate enumeration as a prerequisite. Therefore, it is not a generation-based model like ours. ReTraCk (Chen et al., 2021) is the state-of-the-art generation-based model on GRAILQA which poses grammar-level constraints to the decoder to generate well-formed but unnecessarily faithful programs. For GRAPHQ, the ranking-based model SPARQA (Sun et al., 2020) has achieved the best results so far. It uses BERT as a feature extractor for downstream classifiers. In addition to the state-of-the-art models, we also compare ArcaneQA with BERT+Transduction and BERT+Ranking (Gu et al., 2021), which are two baseline models on GRAILQA that enhance a vanilla Seq2Seq model with BERT to perform generation and ranking respectively.\nImplementation. Our models are implemented using PyTorch and AllenNLP (Gardner et al., 2018). For BERT, we use the bert-base-uncased version provided by HuggingFace. For more details about implementation and hyper-parameters, we refer the reader to Appendix B.", "publication_ref": ["b16", "b29", "b2", "b37", "b8", "b31", "b16", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overall Evaluation", "text": "We show the overall results in Table 2 (for dev set results see Appendix C). ArcaneQA achieves the state-of-the-art performance on both GRAPHQ and WEBQSP. For GRAPHQ, there are 188 questions in GRAPHQ's test set that cannot be converted into FREEBASE 2015-08-09 version, so we treat the F1 of all those questions as 0 following Gu et al. (2021), while the numbers in the parentheses are the actual F1 on the test set if we exclude those questions. ArcaneQA significantly outperforms the prior art by over 10%. The improvement over SPARQA shows the advantage of using PLMs for contextualized joint encoding instead of just providing features for ranking. On both WEBQSP and GRAILQA, ArcaneQA also achieves the best performance or performs on par with the prior art in terms of F1. It outperforms ReTraCk by 4.3% and 1.9% (using the same entity linking results) on WE-BQSP and GRAILQA respectively, suggesting that ArcaneQA can more effectively reduce the search space with dynamic program induction compared with ReTraCk's grammar-based decoding. Also, our model performs on par with the previous stateof-the-art RnG-KBQA (i.e., same numbers on WE-BQSP, while 0.7% lower on GRAILQA). However, ArcaneQA under-performs RnG-KBQA in EM on GRAILQA. The overall EM of ArcaneQA is lower than RnG-KBQA by 5%, and the gap on zero-shot generalization is even larger (i.e., around 10%), despite the comparable numbers in F1. This can be explained by that ArcaneQA learns to predict a program in a more flexible way and may potentially find some novel structures. This may further be supported by the observation that ArcaneQA performs better than RnG-KBQA on compositional generalization, which requires KBQA models to generalize to unseen query structures during training. Overall, the results demonstrate ArcaneQA's flexibility in handling KBQA scenarios of different natures.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "In-Depth Analyses", "text": "To gain more insights into ArcaneQA's strong performance, we conduct in-depth analyses on the two key designs of ArcaneQA. Dynamic Program Induction. One vanilla implementation of ArcaneQA without dynamic program induction is BERT+Transduction, i.e., its search space and vocabulary during decoding is independent of previous predictions. As shown in Table 2a, when using the same entity linking results, ArcaneQA outperforms BERT+Transduction by 30.4% in overall F1 and is twice as good on zero-shot generalization. One major weakness of ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Model F1", "text": "UDEPLAMBDA  17.7 PARA4QA (Dong et al., 2017) 20.4 SPARQA (Sun et al., 2020) 21.5 BERT+Ranking (Gu et al., 2021) 25 NSM (Liang et al., 2017) 69.0 KBQA-GST (Lan et al., 2019a) 67.9 TextRay (Bhutani et al., 2019) 60.3 QGG (Lan and Jiang, 2020) 74.0 ReTraCk (Chen et al., 2021) 71.0 CBR (Das et al., 2021) 72.8 RnG-KBQA (Ye et al., 2021) 75.6 (74.  BERT+Transduction is that it predicts many programs that are not faithful to the KB, executing which will lead to empty answers. Note that posthoc filtering by execution  can only help to a limited degree due to the KB's broad schema, while this type of mistake is rooted out in ArcaneQA by design.\nDifferent from our search space pruning achieved with dynamic program induction, rankingbased models such as BERT+Ranking prunes unfaithful programs from their search space by ranking a set of faithful programs enumerated from the KB. These models typically make compromises on the complexity and diversity of programs during candidate enumeration. We break down the performance of ArcaneQA on GRAILQA's validation set in terms of question complexity and function types and show the fine-grained results in Table 3. The comparison with BERT-Ranking demonstrates the scalability and flexibility of our dynamic program induction. We also compare with RnG-KBQA, which adopts exactly the same candidate enumeration module as BERT+Ranking, but it is enhanced with a T5-based revision module to edit the enumerated programs into more diverse ones. We observe that RnG-KBQA performs uniformly well across different programs except for programs with superlative functions (i.e., ARGMAX/ARGMIN), i.e., the F1 of it is lower than ArcaneQA by over 50%. This is because in their candidate generation step, there is no superlative function enumerated. Despite the effectiveness of their T5-based revision, their performance still heavily depends on the diversity of candidate enumeration, which restricts the flexibility of their method.\nDynamic Contextualized Encoding. To show the key role of dynamic contextualized encoding, we use GloVe (Pennington et al., 2014) to provide non-contextualized embeddings for both questions and tokens in V. We fix GloVe embeddings during training to make the model less biased to the training distribution (Gu et al., 2021) for GRAILQA and GRAPHQ, which address non-i.i.d. generalization, while for WEBQSP, we also update the word embeddings during training. Results in Table 2a show the importance of dynamic contextualized encoding, i.e., without contextualized encoding, the overall F1 decreases by 14.6%, 11.1%, and 6.5% on three datasets respectively. We also notice that dynamic contextualized encoding is more criti-  ", "publication_ref": ["b14", "b31", "b16", "b22", "b20", "b4", "b18", "b8", "b37", "b23", "b16"], "figure_ref": [], "table_ref": ["tab_5", "tab_3"]}, {"heading": "Efficiency Analysis", "text": "We compare the running time of ArcaneQA and ranking-based models in the online mode (i.e., no offline caching) to mimic the real application scenario. To make the comparison fair, we configure all models to interact with the KB via the same Virtuoso SPARQL endpoint. We run each model on 1,000 randomly sampled questions and report the average running time per question on a GTX 2080 Ti card. As shown below, our model is faster than BERT+Ranking and RnG-KBQA by an order of magnitude, because ArcaneQA dynamically prunes the search space and does not run the timeconsuming queries for enumerating two-hop candidates. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Meaning Representation", "text": "We provide a detailed description of our defined functions for S-expressions in Table 4. We provide annotations in S-expressions for several KBQA datasets, including WEBQSP, GRAPHQ, and COM-PLEXWEBQ (which we did not use for experiments). All data files annotated by us can be found in our Github Repo.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_8"]}, {"heading": "B Implementation Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Entity Linking Results", "text": "For GRAILQA, we use the entity linking results provided by Ye et al. (2021); for GRAPHQ, we use the entity linking results provided by Gu et al. (2021); for WEBQSP, we follow the entity linking results provided by (Lan and Jiang, 2020). In addition, we find that answer types can be a strong clue for GRAILQA, so we also predict a set of FREEBASE classes for GRAILQA as a special type of entity using a BERT-based classifier. All entity linking results can be found in our Github Repo.", "publication_ref": ["b37", "b16", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Entity Anonymization", "text": "After identifying a set of entities, we do entity anonymization for WEBQ, i.e., we replace the entity mention with the type of the corresponding entity. For example, mention \"Barack Obama\" will be replaced by \"US president\". However, the entity linker might identify some false positive mentions, and anonymizing these mentions would lead to some critical information loss. To address this problem, we identify a set of common false positive mentions that contain important information about the question in training data. Such words include \"government\", \"zip\", etc. For mentions include these words, we do not do anonymization. Doing entity anonymization is a common practice on WEBQ, which can normally bring some gain of around 1 to 2 percent in F1, while for GRAILQA and GRAPHQ, we did not observe any improvement, so we keep the original entity mentions for these two datasets.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Hyper Parameters", "text": "For ArcaneQA, we are only able to train our model with batch size 1 due to the memory consumption, so we choose a workaround to set the number of gradient accumulations to 16. We use Adam optimizer with an initial learning rate of 0.001 to update our own parameters in BERT-based models. For BERT's parameters, we fine-tune them with a learning rate of 2e-5. For ArcaneQA w/o BERT, we train it with batch size 32 and an initial learning rate of 0.001 using Adam optimizer. For both models, the hidden sizes of both encoder and decoder are set to 768, and the dropout rate is set to 0.5. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.5 Other Details", "text": "We summarize some other details in our implementation that are critical to reproduction. We identify the literals in GRAILQA and GRAPHQ using hand-crafted regular expressions. There are two types of literals, i.e., date time and numerical value. Our regular expressions can identify around 98% of all literals.\nDuring dynamic program induction of Ar-caneQA, we follow the rules in Table 1 to run SPARQL queries to retrieve the admissible schema items. However, in some rare cases, the execution of a subprogram may contain a tremendous number of entities For example, the execution of (JOIN USA people.person.nationality) contains over 500,000 entities, and running SPARQL queries for all entities in them is infeasible. As a result, we only run SPARQL queries for 100 entities sampled from the execution results. One better choice could be to use some more efficient indexing to query the KB instead of using SPARQL.\nWe construct the vocabulary V for different datasets in different ways. For GRAILQA, following Gu et al. (2021), we construct the vocabulary using schema items from FREEBASECOMMONS. For GRAPHQ, we construct the vocabulary using schema items from the entire FREEBASE. For WEBQ, because it evaluates i.i.d. generalization, so we construct the vocabulary from its training data.   We show the results of ArcaneQA, BERT+Ranking, and RnG-KBQA on the validation set of GRAILQA in Table 5. We observe that ArcaneQA achieves a better F1 than RnG-KBQA. Overall, the trend is consistent with the test set. We also observe that the EM of ArcaneQA on zero-shot generalization is significantly higher than the test set, which is interesting and remains for further investigation.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "Acknowledgement", "text": "The authors would like to thank the colleagues from the OSU NLP group and the anonymous reviewers for their thoughtful comments. This research was supported by NSF OAC 2112606.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Automated template generation for question answering over knowledge graphs", "journal": "", "year": "2017", "authors": "Abdalghani Abujabal; Mohamed Yahya; Mirek Riedewald; Gerhard Weikum"}, {"ref_id": "b1", "title": "Neural machine translation by jointly learning to align and translate", "journal": "", "year": "2015-05-07", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"ref_id": "b2", "title": "Semantic parsing on Freebase from question-answer pairs", "journal": "", "year": "2013", "authors": "Jonathan Berant; Andrew Chou; Roy Frostig; Percy Liang"}, {"ref_id": "b3", "title": "Semantic parsing via paraphrasing", "journal": "Long Papers", "year": "2014", "authors": "Jonathan Berant; Percy Liang"}, {"ref_id": "b4", "title": "Learning to answer complex questions over knowledge bases with query composition", "journal": "", "year": "2019", "authors": "Nikita Bhutani; Xinyi Zheng"}, {"ref_id": "b5", "title": "Answering complex questions by combining information from curated and extracted knowledge bases", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Nikita Bhutani; Xinyi Zheng; Kun Qian; Yunyao Li; H Jagadish"}, {"ref_id": "b6", "title": "Freebase: a collaboratively created graph database for structuring human knowledge", "journal": "", "year": "2008", "authors": "Kurt Bollacker; Colin Evans; Praveen Paritosh; Tim Sturge; Jamie Taylor"}, {"ref_id": "b7", "title": "Semantic parsing Freebase: Towards open-domain semantic parsing", "journal": "", "year": "2013", "authors": "Qingqing Cai; Alexander Yates"}, {"ref_id": "b8", "title": "Retrack: A flexible and efficient framework for knowledge base question answering", "journal": "", "year": "2021", "authors": "Shuang Chen; Qian Liu; Zhiwei Yu; Chin-Yew Lin; Jian-Guang Lou; Feng Jiang"}, {"ref_id": "b9", "title": "Uhop: An unrestricted-hop relation extraction framework for knowledge-based question answering", "journal": "", "year": "2019", "authors": "Chih-Hung Zi-Yuan Chen; Yi-Pei Chang; Jijnasa Chen; Lun-Wei Nayak;  Ku"}, {"ref_id": "b10", "title": "Learning an executable neural semantic parser", "journal": "Computational Linguistics", "year": "2019", "authors": "Jianpeng Cheng; Siva Reddy; Vijay Saraswat; Mirella Lapata"}, {"ref_id": "b11", "title": "Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021. Case-based reasoning for natural language queries over knowledge bases", "journal": "", "year": "", "authors": "Rajarshi Das; Manzil Zaheer; Dung Thai; Ameya Godbole; Ethan Perez; Jay-Yoon Lee"}, {"ref_id": "b12", "title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"ref_id": "b13", "title": "Language to logical form with neural attention", "journal": "Long Papers", "year": "2016", "authors": "Li Dong; Mirella Lapata"}, {"ref_id": "b14", "title": "Learning to paraphrase for question answering", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Li Dong; Jonathan Mallinson; Siva Reddy; Mirella Lapata"}, {"ref_id": "b15", "title": "AllenNLP: A deep semantic natural language processing platform", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Matt Gardner; Joel Grus; Mark Neumann; Oyvind Tafjord; Pradeep Dasigi; Nelson F Liu; Matthew Peters; Michael Schmitz; Luke Zettlemoyer"}, {"ref_id": "b16", "title": "Beyond iid: three levels of generalization for question answering on knowledge bases", "journal": "", "year": "2021", "authors": "Yu Gu; Sue Kase; Michelle Vanni; Brian Sadler; Percy Liang; Xifeng Yan; Yu Su"}, {"ref_id": "b17", "title": "A comprehensive exploration on WikiSQL with table-aware word contextualization", "journal": "", "year": "2019", "authors": "Wonseok Hwang; Jinyeung Yim; Seunghyun Park; Minjoon Seo"}, {"ref_id": "b18", "title": "Query graph generation for answering multi-hop complex questions from knowledge bases", "journal": "", "year": "2020", "authors": "Yunshi Lan; Jing Jiang"}, {"ref_id": "b19", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "Online. Association for Computational Linguistics", "year": "", "authors": ""}, {"ref_id": "b20", "title": "Knowledge base question answering with topic units", "journal": "", "year": "2019", "authors": "Yunshi Lan; Shuohang Wang; Jing Jiang"}, {"ref_id": "b21", "title": "Multi-hop knowledge base question answering with an iterative sequence matching model", "journal": "IEEE", "year": "2019", "authors": "Yunshi Lan; Shuohang Wang; Jing Jiang"}, {"ref_id": "b22", "title": "Neural symbolic machines: Learning semantic parsers on Freebase with weak supervision", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Chen Liang; Jonathan Berant; Quoc Le; Kenneth D Forbus; Ni Lao"}, {"ref_id": "b23", "title": "GloVe: Global vectors for word representation", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"ref_id": "b24", "title": "Exploring the limits of transfer learning with a unified text-to-text transformer", "journal": "", "year": "2019", "authors": "Colin Raffel; Noam Shazeer; Adam Roberts; Katherine Lee; Sharan Narang; Michael Matena; Yanqi Zhou; Wei Li; Peter J Liu"}, {"ref_id": "b25", "title": "Mark Steedman, and Mirella Lapata", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Siva Reddy; Oscar T\u00e4ckstr\u00f6m; Slav Petrov"}, {"ref_id": "b26", "title": "SmBoP: Semiautoregressive bottom-up semantic parsing", "journal": "", "year": "2021", "authors": "Ohad Rubin; Jonathan Berant"}, {"ref_id": "b27", "title": "Picard: Parsing incrementally for constrained auto-regressive decoding from language models", "journal": "", "year": "2021", "authors": "Torsten Scholak; Nathan Schucher; Dzmitry Bahdanau"}, {"ref_id": "b28", "title": "Task-oriented dialogue as dataflow synthesis", "journal": "Transactions of the Association for Computational Linguistics", "year": "2020", "authors": "Semantic Machines; Jacob Andreas; John Bufe; David Burkett; Charles Chen; Josh Clausman; Jean Crawford; Kate Crim; Jordan Deloach; Leah Dorner; Jason Eisner; Hao Fang; Alan Guo; David Hall; Kristin Hayes; Kellie Hill; Diana Ho; Wendy Iwaszuk; Smriti Jha; Dan Klein; Jayant Krishnamurthy; Theo Lanman; Percy Liang; Christopher H Lin"}, {"ref_id": "b29", "title": "On generating characteristic-rich question sets for QA evaluation", "journal": "", "year": "2016", "authors": "Yu Su; Huan Sun; Brian Sadler; Mudhakar Srivatsa; Izzeddin G\u00fcr; Zenghui Yan; Xifeng Yan"}, {"ref_id": "b30", "title": "A reintroduction to our Knowledge Graph and knowledge panels", "journal": "", "year": "2020", "authors": "Danny Sullivan"}, {"ref_id": "b31", "title": "Sparqa: Skeleton-based semantic parsing for complex questions over knowledge bases", "journal": "AAAI Press", "year": "2020", "authors": "Yawei Sun; Lingling Zhang; Gong Cheng; Yuzhong Qu"}, {"ref_id": "b32", "title": "Sequence to sequence learning with neural networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; Quoc V Le"}, {"ref_id": "b33", "title": "Advances in Neural Information Processing Systems", "journal": "Curran Associates, Inc", "year": "", "authors": "K Q Lawrence;  Weinberger"}, {"ref_id": "b34", "title": "RAT-SQL: Relation-aware schema encoding and linking for textto-SQL parsers", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Bailin Wang; Richard Shin; Xiaodong Liu; Oleksandr Polozov; Matthew Richardson"}, {"ref_id": "b35", "title": "Robust text-to-sql generation with execution-guided decoding", "journal": "", "year": "2018", "authors": "Chenglong Wang; Kedar Tatwawadi; Marc Brockschmidt; Po-Sen Huang; Yi Mao; Oleksandr Polozov; Rishabh Singh"}, {"ref_id": "b36", "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models", "journal": "", "year": "2022", "authors": "Tianbao Xie; Chen Henry Wu; Peng Shi; Ruiqi Zhong; Torsten Scholak; Michihiro Yasunaga; Chien-Sheng Wu; Ming Zhong; Pengcheng Yin; I Sida;  Wang"}, {"ref_id": "b37", "title": "Rng-kbqa: Generation augmented iterative ranking for knowledge base question answering", "journal": "", "year": "2021", "authors": "Xi Ye; Semih Yavuz; Kazuma Hashimoto; Yingbo Zhou; Caiming Xiong"}, {"ref_id": "b38", "title": "Semantic parsing via staged query graph generation: Question answering with knowledge base", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Ming-Wei Wen-Tau Yih; Xiaodong Chang; Jianfeng He;  Gao"}, {"ref_id": "b39", "title": "The value of semantic parse labeling for knowledge base question answering", "journal": "Short Papers", "year": "2016", "authors": "Matthew Wen-Tau Yih; Chris Richardson; Ming-Wei Meek; Jina Chang;  Suh"}, {"ref_id": "b40", "title": "Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Tao Yu; Rui Zhang; Kai Yang; Michihiro Yasunaga; Dongxu Wang; Zifan Li; James Ma; Irene Li; Qingning Yao; Shanelle Roman; Zilin Zhang; Dragomir Radev"}, {"ref_id": "b41", "title": "Learning to parse database queries using inductive logic programming", "journal": "", "year": "1996", "authors": "M John; Raymond J Zelle;  Mooney"}, {"ref_id": "b42", "title": "Learning to map sentences to logical form: structured classification with probabilistic categorial grammars", "journal": "", "year": "2005", "authors": "S Luke; Michael Zettlemoyer;  Collins"}, {"ref_id": "b43", "title": "", "journal": "", "year": "", "authors": "Rui Zhang; Tao Yu; Yang He; Sungrok Er; Eric Shim; Xi Xue; Tianze Victoria Lin; Caiming Shi; Richard Xiong; Dragomir Socher;  Radev"}, {"ref_id": "b44", "title": "Editing-based sql query generation for crossdomain context-dependent questions", "journal": "", "year": "", "authors": ""}, {"ref_id": "b45", "title": "Seq2sql: Generating structured queries from natural language using reinforcement learning", "journal": "", "year": "2017", "authors": "Victor Zhong; Caiming Xiong; Richard Socher"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "1Data and code: dki-lab/ArcaneQA Question: Which wine in Tulum valley has the most alcohol?", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "A knowledge base K consists of a set of relational triplets K r \u2282 E \u00d7 R \u00d7 (E \u222a L) and a set of class assertions K c \u2282 E \u00d7 C, where C is a set of classes, E is a set of entities, L is a set of literals and R is a set of binary relations. Elements in C and R are also called the schema items of K. Meaning Representation for KBQA. Prior work adopt different meaning representations to represent logical forms for KBQA. For example, Yih et al. (2015) use graph query, which represents a program as a core relation chain with (optionally) some entity constraints. Cai and Yates (2013) use \u03bb-Calculus as their meaning representation.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "where k is the number of total subprograms and |o| = k i=1 |o i |. We base ArcaneQA on Seq2Seq with attention", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "(Figure 2 :2Figure 2: (a) Overview of ArcaneQA. ArcaneQA synthesizes the target program by iteratively predicting a sequence of subprograms. (b) At each step, it makes a prediction from a small set of admissible tokens A dynamically determined based on the execution of previous subprograms (for faithfulness to the KB) as well as the grammar (for well-formedness). (c) ArcaneQA also leverages BERT to provide dynamic contextualized encoding of the question and the admissible tokens at each step, which enables implicit schema linking and guides the dynamic program induction process.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "entities u \u2282 (E \u222a L) and a relation r \u2208 R all entities connecting to any e \u2208 u via r AND two set of entities u1 \u2282 E and u2 \u2282 E the intersection of two entities sets.ARGMAX/ARGMINa set of entities u \u2282 E and a numerical relation r \u2208 R a set of entities from u with the maximum/minimum value for r LT(LE/GT/GE) a numerical value u \u2282 L and a numerical relation r \u2208 R all entities with a value < (\u2264 / > / \u2265)u for relation rCOUNT a set of entities u \u2282 E the number of entities in u CONS a set of entities a set of entities u \u2282 E, a relation r \u2208 R, and a constraint c \u2208 (E \u222a L) all e \u2208 u satisfying (e, r, c) \u2208 Kr TC a set of entities a set of entities u \u2282 E, a relation r \u2208 R, and a temporal constraint c \u2208 L all e \u2208 u satisfying (e, r, c) \u2208 Kr", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Overall results on three datasets. ArcaneQA follows entity linking results from previous methods (i.e., RnG-KBQA's results on GRAILQA, QGG's results on WEBQSP, and Gu et al. (2021)'s results on GRAPHQ) for fair comparison. Model names with * indicate using the baseline entity linking results on GRAILQA.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Fine-grained results (EM/F1) on GRAILQA's dev set. None denotes programs with only AND and JOIN.", "figure_data": "cal for non-i.i.d. generalization, i.e., on GRAILQAthe F1 on i.i.d. generalization only decreases by6.8%, while it decreases by 15.9% and 17.5%on compositional and zero-shot generalization.Without contextualized encoding, identifying thecorrect schema items from the KB in non-i.i.d.setting is particularly challenging. Schema linkingpowered by dynamic contextualized encoding isthe key to non-i.i.d. generalization, which is along-term goal of KBQA."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Detailed descriptions of functions defined in our S-expressions. We extend the definitions inGu et al. (2021) by introducing two new functions CONS and TC. Also, we remove the function R and instead represent the inverse of a relation by adding a suffix \" inv\" to it. Note that, for arguments in AND function, a class c \u2208 C can also indicate a set of entities which fall into c.", "figure_data": "OverallI.I.D.CompositionalZero-shotModelEMF1EMF1EMF1EMF1BERT+Ranking (Gu et al., 2021)51.058.458.666.140.948.151.859.2RnG-KBQA (Ye et al., 2021)71.476.886.789.061.768.968.874.7ArcaneQA69.576.986.189.265.573.964.072.8"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The results on the validation set of GRAILQA. The overall trend is basically consistent with the test set.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "(ARGMAX #3 vintage)", "formula_coordinates": [1.0, 351.93, 333.31, 57.87, 4.53]}, {"formula_id": "formula_1", "formula_text": "o = o 1 1 , ..., o 1 |o 1 | , ..., o k 1 , ..., o k |o k | = y 1 , ..., y |o| ,", "formula_coordinates": [4.0, 72.0, 605.85, 218.27, 27.94]}, {"formula_id": "formula_2", "formula_text": "p(o|q) = |o| t=1 p(y t |y <t , q),(1)", "formula_coordinates": [4.0, 360.21, 92.59, 166.06, 34.6]}, {"formula_id": "formula_3", "formula_text": "[CLS], x 1 , ..., x |q| , [SEP], s t 1 , ..., s t m , [SEP]", "formula_coordinates": [5.0, 322.0, 704.2, 188.81, 13.87]}, {"formula_id": "formula_4", "formula_text": "Current function Admissible actions JOIN {r|h \u2208 #, (h, r, t) \u2208 K r } AND {v|v \u2208 S, v \u2229 # \u0338 = \u2205} \u222a {c|e \u2208 #, (e, c) \u2208 K c } ARGMAX/ARGMIN {r|h \u2208 #, t \u2208 L, (h, r, t) \u2208 K r } LT(LE/GT/GE) {r|t < (\u2264 / > / \u2265)#, (h, r, t) \u2208 K r } COUNT {)} CONS {(r, t)|h \u2208 #, (h, r, t) \u2208 K r } TC {(r, t)|h \u2208 #, (h, r, t) \u2208 K r , t \u2208 L is a temporal expression}", "formula_coordinates": [6.0, 135.15, 67.91, 327.24, 90.36]}, {"formula_id": "formula_5", "formula_text": "h t = LSTM \u03b8 (h t\u22121 , c t\u22121 ) (2)", "formula_coordinates": [6.0, 122.64, 497.29, 168.36, 10.81]}, {"formula_id": "formula_6", "formula_text": "p(y t = s ti |q, y <t ) = [Softmax(W t h t )] i (3)", "formula_coordinates": [6.0, 87.53, 597.59, 203.47, 10.67]}, {"formula_id": "formula_7", "formula_text": "a t = softmax(Q t h t )(4)", "formula_coordinates": [6.0, 134.36, 684.34, 156.64, 10.67]}, {"formula_id": "formula_8", "formula_text": "q t = (a t ) T Q t (5) c t = [[W t ] j ; q t ](6)", "formula_coordinates": [6.0, 144.16, 699.88, 146.84, 29.67]}], "doi": "10.1145/3038912.3052583"}