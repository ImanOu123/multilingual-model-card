{"title": "Matrix Completion has No Spurious Local Minimum", "authors": "Rong Ge; Jason D Lee; Tengyu Ma", "pub_date": "2018-07-24", "abstract": "Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for positive semidefinite matrix completion has no spurious local minima -all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semidefinite matrix completion with arbitrary initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations.", "sections": [{"heading": "Introduction", "text": "Matrix completion is the problem of recovering a low rank matrix from partially observed entries. It has been widely used in collaborative filtering and recommender systems [Kor09,RS05], dimension reduction [CLMW11] and multiclass learning [AFSU07]. There has been extensive work on designing efficient algorithms for matrix completion with guarantees. One earlier line of results (see [Rec11,CT10,CR09] and the references therein) rely on convex relaxations. These algorithms achieve strong statistical guarantees, but are quite computationally expensive in practice.\nMore recently, there has been growing interest in analyzing non-convex algorithms for matrix completion [KMO10a, KMO10b, JNS13, Har14, HW14, SL15, ZWL15, CW15, SRO15, CW15]. Let M \u2208 R d\u00d7d be the target matrix with rank r d that we aim to recover, and let \u2126 = {(i, j) : M i,j is observed} be the set of observed entries. These methods are instantiations of optimization algorithms applied to the objective 1 , f (X) = 1 2\n(i,j)\u2208\u2126 M i,j \u2212 (XX ) i,j 2 , (1.1)\nThese algorithms are much faster than the convex relaxation algorithms, which is crucial for their empirical success in large-scale collaborative filtering applications [Kor09].\nMost of the theoretical analysis of the nonconvex procedures require careful initialization schemes: the initial point should already be close to optimum 2 . In fact, Sun and Luo [SL15] showed that after this initialization the problem is effectively strongly-convex, hence many different optimization procedures can be analyzed by standard techniques from convex optimization.\nHowever, in practice people typically use a random initialization, which still leads to robust and fast convergence. Why can these practical algorithms find the optimal solution in spite of the non-convexity? In this work we investigate this question and show that the matrix completion objective has no spurious local minima. More precisely, we show that any local minimum X of objective function f (\u2022) is also a global minimum with f (X) = 0, and recovers the correct low rank matrix M .\nOur characterization of the structure in the objective function implies that (stochastic) gradient descent from arbitrary starting point converge to a global minimum. This is because gradient descent converges to a local minimum [GHJY15,LSJR16], and every local minimum is also a global minimum.", "publication_ref": ["b20", "b30", "b5", "b0", "b29", "b7", "b6", "b20", "b32", "b10", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Main results", "text": "Assume the target matrix M is symmetric and each entry of M is observed with probability p independently 3 . We assume M = ZZ for some matrix Z \u2208 R d\u00d7r .\nThere are two known issues with matrix completion. First, the choice of Z is not unique since M = (ZR)(ZR) for any orthonormal matrix Z. Our goal is to find one of these equivalent solutions.\nAnother issue is that matrix completion is impossible when M is \"aligned\" with standard basis. For example, when M is the identity matrix in its first r \u00d7 r block, we will very likely be observing only 0 entries. To address this issue, we make the following standard assumption: Assumption 1. For any row Z i of Z, we have\nZ i \u00b5/ \u221a d \u2022 Z F .\nMoreover, Z has a bounded condition number \u03c3 max (Z)/\u03c3 min (Z) = \u03ba.\nThroughout this paper we think of \u00b5 and \u03ba as small constants, and the sample complexity depends polynomially on these two parameters. Also note that this assumption is independent of the choice of Z: all Z such that ZZ T = M have the same row norms and Frobenius norm.\nThis assumption is similar to the \"incoherence\" assumption [CR09]. Our assumption is the same as the one used in analyzing non-convex algorithms [KMO10a,KMO10b,SL15].\nWe enforce X to also satisfy this assumption by a regularizer f (X) = 1 2\n(i,j)\u2208\u2126 M i,j \u2212 (XX ) i,j 2 + R(X), (1.2) where R(X) is a function that penalizes X when one of its rows is too large. See Section 4 and Section 5 for the precise definition. Our main result shows that in this setting, the regularized objective function has no spurious local minimum:\nTheorem 1.1.\n[Informal] All local minimum of the regularized objective (1.2) satisfy XX T = ZZ T = M when p poly(\u03ba, r, \u00b5, log d)/d.\nCombined with the results in [GHJY15,LSJR16] (see Theorem 2.3) 4 , we have, Theorem 1.2 (Informal). With high probability, stochastic gradient descent on the regularized objective (1.2) will converge to a solution X such that XX T = ZZ T = M in polynomial time from any starting point. Gradient descent will converge to such a point with probability 1 from a random starting point.\nOur results are also robust to noise. Even if each entry is corrupted with Gaussian noise of standard deviation \u00b5 2 Z 2 F /d (comparable to the magnitude of the entry itself!), we can still guarantee that all the local minima satisfy XX T \u2212 ZZ T F \u03b5 when p is large enough. See the discussion in Appendix B for results on noisy matrix completion. Our main technique is to show that every point that satisfies the first and second order necessary conditions for optimality must be a desired solution. To achieve this we use new ideas to analyze the effect of the regularizer and show how it is useful in modifying the first and second order conditions to exclude any spurious local minimum.", "publication_ref": ["b6", "b18", "b19", "b32", "b10", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Matrix Completion. The earlier theoretical works on matrix completion analyzed the nuclear norm minimization [SS05,Rec11,CT10,CR09,NW12]. This line of work has the cleanest and strongest theoretical guarantees; [CT10,Rec11] showed that if |\u2126| dr\u00b5 2 log 2 d the nuclear norm convex relaxation recovers the exact underlying low rank matrix. The solution can be computed via the solving a convex program in polynomial time. However the primary disadvantage of nuclear norm methods is their computational and memory requirements -the fastest known provable algorithms require O(d 2 ) memory and thus at least O(d 2 ) running time, which could be both prohibitive for moderate to large values of d. Many algorithms have been proposed to improve the runtime (either theoretically or empirically) (see, for examples, [SRJ04, MHT10, HMZ14], and the reference therein). Burer and Monteiro [BM03] proposed factorizing the optimization variable M = XX T , and optimizing over X \u2208 R d\u00d7r instead of M \u2208 R d\u00d7d . This approach only requires O(dr) memory, and a single gradient iteration takes time O(|\u2126|), so has much lower memory requirement and computational complexity than the nuclear norm relaxation. On the other hand, the factorization causes the optimization problem to be non-convex in X, which leads to theoretical difficulties in analyzing algorithms. Keshavan et al. [KMO10a,KMO10b] showed that well-initialized gradient descent recovers M . The works [HW14, Har14, JNS13, CW15] showed that well-initialized alternating least squares, block coordinate descent, and gradient descent converges M . Jain and Netrapalli [JN15] showed a fast algorithm by iteratively doing gradient descent in the relaxed space and projecting to the set of low-rank matrices. The work [SRO15] analyzes stochastic gradient descent with fresh samples at each iteration from random initialization and shows that it approximately converge to the optimal solution. [SL15, ZWL15, ZL16, TBSR15] provided a more unified analysis by showing that with careful initialization many algorithms, including gradient descent and alternating least squares, succeed. [SL15,ZL16] accomplished this by showing an analog of strong convexity in the neighborhood of the solution M .\nNon-convex Optimization. Recently, a line of work analyzes non-convex optimization by separating the problem into two aspects: the geometric aspect which shows the function has no spurious local minimum and the algorithmic aspect which designs efficient algorithms can converge to local minimum that satisfy first and (relaxed versions) of second order necessary conditions.\nOur result is the first that explains the geometry of the matrix completion objective. Similar geometric results are only known for a few problems: SVD/PCA phase retrieval/synchronization, orthogonal tensor decomposition, dictionary learning [BH89, SJ13, GHJY15, SQW15, BBV16]. The matrix completion objective requires different tools due to the sampling of the observed entries, as well as carefully managing the regularizer to restrict the geometry. Parallel to our work Bhojanapalli et al. [BNS16] showed similar results for matrix sensing, which is closely related to matrix completion. Loh and Wainwright [LW15] showed that for many statistical settings that involve missing/noisy data and non-convex regularizers, any stationary point of the non-convex objective is close to global optima; furthermore, there is a unique stationary point that is the global minimum under stronger assumptions [LW14].\nOn the algorithmic side, it is known that second order algorithms like cubic regularization [NP06] and trust-region [SQW15] algorithms converge to local minima that approximately satisfy first and second order conditions. Gradient descent is also known to converge to local minima [LSJR16] from a random starting point. Stochastic gradient descent can converge to a local minimum in polynomial time from any starting point [Pem90,GHJY15]. All of these results can be applied to our setting, implying various heuristics used in practice are guaranteed to solve matrix completion.", "publication_ref": ["b36", "b29", "b7", "b6", "b27", "b7", "b29", "b3", "b18", "b19", "b16", "b35", "b32", "b38", "b4", "b24", "b23", "b26", "b22", "b28", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Notations", "text": "For\n\u2126 \u2282 [d] \u00d7 [d]\n, let P \u2126 be the linear operator that maps a matrix A to P \u2126 (A), where P \u2126 (A) has the same values as A on \u2126, and 0 outside of \u2126.\nWe will use the following matrix norms: \u2022 F the frobenius norm, \u2022 spectral norm, |A| \u221e elementwise infinity norm, and |A| p\u2192q = max x p =1 A q . We use the shorthand A \u2126 = P \u2126 A F . The trace inner product of two matrices is A, B = tr(A B), and \u03c3 min (X), \u03c3 max (X) are the smallest and largest singular values of X. We also use X i to denote the i-th row of a matrix X.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Necessary conditions for Optimality", "text": "Given an objective function f (x) : R n \u2192 R, we use \u2207f (x) to denote the gradient of the function, and \u2207 2 f (x) to denote the Hessian of the function (\u2207 2 f (x) is an n \u00d7 n matrix where [\u2207 2 f (x)] i,j = \u2202 2 \u2202xi\u2202xj f (x)). It is well known that local minima of the function f (x) must satisfy some necessary conditions: Definition 2.1. A point x satisfies the first order necessary condition for optimality (later abbreviated as first order optimality condition) if \u2207f (x) = 0. A point x satisfies the second order necessary condition for optimality (later abbreviated as second order optimality condition)if \u2207 2 f (x) 0.\nThese conditions are necessary for a local minimum because otherwise it is easy to find a direction where the function value decreases. We will also consider a relaxed second order necessary condition, where we only require the smallest eigenvalue of the Hessian \u2207 2 f (x) to be not very negative: Definition 2.2. For \u03c4 0, a point x satisfies the \u03c4 -relaxed second order optimality condition, if \u2207 2 f (x) \u2212\u03c4 \u2022 I.\nThis relaxation to the second order condition makes the conditions more robust, and allows for efficient algorithms.\nTheorem 2.3. [NP06, SQW15, GHJY15] Let f be twice-differentiable function form R d to R. Suppose there exist \u03b5 0 , \u03c4 0 > 0 and a universal constant c > 0 such that if a point x satisfies \u2207f (x) \u03b5 \u03b5 0 and \u2207 2 f (x) \u2212\u03c4 0 \u2022 I, then x is \u03b5 c -close to a global minimum of f . Then, many optimization algorithms including cubic regularization, trust-region, and stochastic gradient descent, can find a global minimum of f up to \u03b4 error in 2 norm in domain in time poly(1/\u03b4, 1/\u03c4 0 , d).\n3 Proof Strategy: \"simple\" proofs are more generalizable\nIn this section, we demonstrate the key ideas behind our analysis using the rank r = 1 case. In particular, we first give a \"simple\" proof for the fully observed case. Then we show this simple proof can be easily generalized to the random observation case. We believe that this proof strategy is applicable to other statistical problems involving partial/noisy observations. The proof sketches in this section are only meant to be illustrative and may not be fully rigorous in various places. We refer the readers to Section 4 and Section 5 for the complete proofs.\nIn the rank r = 1 case, we assume M = zz , where z = 1, and z \u221e \u00b5 \u221a d . Let \u03b5 1 be the target accuracy that we aim to achieve in this section and let p = poly(\u00b5, log d)/(d\u03b5).\nFor simplicity, we focus on the following domain B of incoherent vectors where the regularizer R(x) vanishes,\nB = x : x \u221e < 2\u00b5 \u221a d . (3.1)\nInside this domain B, we can restrict our attention to the objective function without the regularizer, defined as,\ng(x) = 1 2 \u2022 P \u2126 (M \u2212 xx ) 2 F . (3.2)\nThe global minima ofg(\u2022) are z and \u2212z with function value 0. Our goal of this section is to (informally) prove that all the local minima ofg(\u2022) are O( \u221a \u03b5)-close to \u00b1z. In later section we will formally prove that the only local minima are \u00b1z. It turns out to be insightful to consider the full observation case when\n\u2126 = [d] \u00d7 [d]. The corresponding objective is g(x) = 1 2 \u2022 M \u2212 xx 2 F . (3.3)\nObserve thatg(x) is a sampled version of the g(x), and therefore we expect that they share the same geometric properties. In particular, if g(x) does not have spurious local minima then neither doesg(x). Lemma 3.2 (Full observation case, informally stated). Under the setting of this section, in the domain B, the function g(\u2022) has only two local minima {\u00b1z} . Before introducing the \"simple\" proof, let us first look at a delicate proof that does not generalize well.\nDifficult to Generalize Proof of Lemma 3.2. We compute the gradient and Hessian of g(x),\n\u2207g(x) = M x \u2212 x 2 x, and \u2207 2 g(x) = 2xx \u2212 M + x 2 \u2022 I .\n(3.4) Therefore, a critical point x satisfies \u2207g(x) = M x \u2212 x 2 x = 0, and thus it must be an eigenvector of M and x 2 is the corresponding eigenvalue. Next, we prove that the hessian is only positive definite at the top eigenvector . Let x be an eigenvector with eigenvalue \u03bb = x 2 , and \u03bb is strictly less than the top eigenvalue \u03bb * . Let z be the top eigenvector. We have that z, \u2207 2 g(x)z = \u2212 z, M z + x 2 = \u2212\u03bb * + \u03bb < 0, which shows that x is not a local minimum. Thus only z can be a local minimizer, and it is easily verified that \u2207 2 g(z) is indeed positive definite.\nThe difficulty of generalizing the proof above to the partial observation case is that it uses the properties of eigenvectors heavily. Suppose we want to imitate the proof above for the partial observation case, the first difficulty is how to solve the equationg(x) = P \u2126 (M \u2212 xx )x = 0. Moreover, even if we could have a reasonable approximation for the critical points (the solution of \u2207g(x) = 0), it would be difficult to examine the Hessian of these critical points without having the orthogonality of the eigenvectors.\n\"Simple\" and Generalizable proof. The lessons from the subsection above suggest us find an alternative proof for the full observation case which is generalizable. The alternative proof will be simple in the sense that it doesn't use the notion of eigenvectors and eigenvalues. Concretely, the key observation behind most of the analysis in this paper is the following, Proofs that consist of inequalities that are linear in 1 \u2126 are often easily generalizable to partial observation case.\nHere statements that are linear in 1 \u2126 mean the statements of the form ij 1 (i,j)\u2208\u2126 T ij a. We will call these kinds of proofs \"simple\" proofs in this section. Roughly speaking, the observation follows from the law of large numbers -Suppose T ij , (i, j) \u2208 [d] \u00d7 [d] is a sequence of bounded real numbers, then the sampled sum (i,j)\u2208\u2126 T ij = i,j 1 (i,j)\u2208\u2126 T ij is an accurate estimate of the sum p i,j T ij , when the sampling probability p is relatively large. Then, the mathematical implications of p T ij a are expected to be similar to the implications of (i,j)\u2208\u2126 T ij a, up to some small error introduced by the approximation. To make this concrete, we give below informal proofs for Lemma 3.2 and Lemma 3.1 that only consists of statements that are linear in 1 \u2126 . Readers will see that due to the linearity, the proof for the partial observation case (shown on the right column) is a direct generalization of the proof for the full observation case (shown on the left column) via concentration inequalities (which will be discussed more at the end of the section).\nA \"simple\" proof for Lemma 3.2.\nClaim 1f. Suppose x \u2208 B satisfies \u2207g(x) = 0, then x, z 2 = x 4 . Proof. We have, \u2207g(x) = (zz \u2212 xx )x = 0 \u21d2 x, \u2207g(x) = x, (zz \u2212 xx )x = 0 (3.5) \u21d2 x, z 2 = x 4\nIntuitively, this proof says that the norm of a critical point x is controlled by its correlation with z.\nHere at the lasa sampling version of the f aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nGeneralization to Lemma 3.1. Claim 1p. Suppose x \u2208 B satisfies \u2207g(x) = 0, then x, z 2 = x 4 \u2212 \u03b5.\nProof. Imitating the proof on the left, we have\n\u2207g(x) = P \u2126 (zz \u2212 xx )x = 0 \u21d2 x, \u2207g(x) = x, P \u2126 (zz \u2212 xx )x = 0 (3.6) \u21d2 x, z 2 x 4 \u2212 \u03b5\nThe last step uses the fact that equation (3.5) and (3.6) are approximately equal up to scaling factor p for any x \u2208 B, since (3.6) is a sampled version of (3.5).\nClaim 2f. If x \u2208 B has positive Hessian \u2207 2 g(x)\n0, then x 2 1/3.\nProof. By the assumption on x, we have that z, \u2207 2 g(x)z 0. Calculating the quadratic form of the Hessian (see Proposition 4.1 for details),\nz, \u2207 2 g(x)z = zx + xz 2 \u2212 2z (zz \u2212 xx )z 0aaaaaa (3.7) \u21d2 x 2 + 2 z, x 2 1 \u21d2 x 2 1/3 (since z, x 2 x 2 ) aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Claim 2p. If x \u2208 B has positive Hessian \u2207 2g (x) 0, then x 2 1/3 \u2212 \u03b5.\nProof. Imitating the proof on the left, calculating the quadratic form over the Hessian at z (see Proposition 4.1) , we have aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nz, \u2207 2g (x)z = P \u2126 (zx + xz ) 2 \u2212 2z P \u2126 (zz \u2212 xx )z 0 (3.8) \u21d2 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 (same step as the left) \u21d2 x 2 1/3 \u2212 \u03b5\nHere we use the fact that z,\n\u2207 2g (x)z \u2248 p z, \u2207 2 g(x)z for any x \u2208 B.\nWith these two claims, we are ready to prove Lemma 3.2 and 3.1 by using another step that is linear in 1 \u2126 .\nProof of Lemma 3.2. By Claim 1f and 2f, we have x satisfies x, z 2 x 4 1/9. Moreover, we have that \u2207g\n(x) = 0 implies z, \u2207g(x) = z, (zz \u2212 xx )x = 0 (3.9) \u21d2 x, z (1 \u2212 x 2 ) = 0 \u21d2 x 2 = 1 (by x, z 2 1/9)\nThen by Claim 1f again we obtain x, z 2 = 1, and therefore x = \u00b1z. aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\nProof of Lemma 3.1. By Claim 1p and 2p, we have x satisfies x, z 2 x 4 1/9 \u2212 O(\u03b5). Moreover, we have that \u2207g\n(x) = 0 implies z, \u2207g(x) = z, P \u2126 (zz \u2212 xx )x = 0 (3.10) \u21d2 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 (same step as the left) \u21d2 x 2 = 1 \u00b1 O(\u03b5) (same step as the left)\nSince (3.10) is the sampled version of equation (3.9), we expect they lead to the same conclusion up to some approximation. Then by Claim 1p again we obtain x, z 2 = 1 \u00b1 O(\u03b5), and therefore x is O( \u221a \u03b5)-close to either of \u00b1z.\nSubtleties regarding uniform convergence. In the proof sketches above, our key idea is to use concentration inequalities to link the full observation objective g(x) with the partial observation counterpart. However, we require a uniform convergence result. For example, we need a statement like \"w.h.p over the choice of \u2126, equation (3.5) and (3.6) are similar to each other up to scaling\". This type of statement is often only true for x inside the incoherent ball B. The fix to this is the regularizer. For non-incoherent x, we will use a different argument that uses the property of the regularizer. This is besides the main proof strategy of this section and will be discussed in subsequent sections.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Warm-up: Rank-1 Case", "text": "In this section, using the general proof strategy described in previous section, we provide a formal proof for the rank-1 case. In subsection 4.1, we formally work out the proof sketches of Section 3. In subsection 4.2, we prove that due to the effect of the regularizer, outside incoherent ball B, the objective function doesn't have any local minimum.\nIn the rank-1 case, the objective function simplifies to,\nf (x) = 1 2 P \u2126 (M \u2212 xx ) 2 F + \u03bbR(x) . (4.1)\nHere we use the the regularization R(x)\nR(x) = d i=1 h(x i ), and h(t) = (|t| \u2212 \u03b1) 4 I t \u03b1 .\nThe parameters \u03bb and \u03b1 will be chosen later as in Theorem 4.2. We will choose \u03b1 > 10\u00b5/ \u221a d so that R(x) = 0 for incoherent x, and thus it only penalizes coherent x. Moreover, we note R(x) has Lipschitz second order derivative. 5 We first state the optimality conditions, whose proof is deferred to Appendix A.\nProposition 4.1. The first order optimality condition of objective (4.1) is,\n2P \u2126 (M \u2212 xx )x = \u03bb\u2207R(x) , (4.2)\nand the second order optimality condition requires:\n\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v . (4.3)\nMoreover, The \u03c4 -relaxed second order optimality condition requires\n\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v \u2212 \u03c4 v 2 . (4.4)\nWe give the precise version of Theorem 1.1 for the rank-1 case.\nTheorem 4.2. For p\nc\u00b5 6 log 1.5 d d\nwhere c is a large enough absolute constant, set \u03b1 = 10\u00b5 1/d and \u03bb \u00b5 2 p/\u03b1 2 .Then, with high probability over the randomness of \u2126, the only points in R d that satisfy both first and second order optimality conditions (or \u03c4 -relaxed optimality conditions with \u03c4 < 0.1p) are z and \u2212z.\nIn the rest of this section, we will first prove that when x is constrained to be incoherent (and hence the regularizer is 0 and concentration is straightforward) and satisfies the optimality conditions, then x has to be z or \u2212z. Then we go on to explain how the regularizer helps us to change the geometry of those points that are far away from z so that we can rule out them from being local minimum. For simplicity, we will focus on the part that shows a local minimum x must be close enough to z. where \u03b5 = \u00b5 3 (pd) \u22121/2 . This turns out to be the main challenge. Once we proved x is close, we can apply the result of Sun and Luo [SL15] (see Lemma C.1), and obtain Theorem 4.2.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Handling incoherent x", "text": "To demonstrate the key idea, in this section we restrict our attention to the subset of R d which contains incoherent x with 2 norm bounded by 1, that is, we consider,\nB = x : x \u221e 2\u00b5 \u221a d , x 1 . (4.5)\nNote that the desired solution z is in B, and the regularization R(x) vanishes inside B.\nThe following lemmas assume x satisfies the first and second order optimality conditions, and deduce a sequence of properties that x must satisfy.\nLemma 4.4. Under the setting of Theorem 4.2 , with high probability over the choice of \u2126, for any x \u2208 B that satisfies second-order optimality condition (4.3) we have,\nx 2 1/4.\nThe same is true if x \u2208 B only satisfies \u03c4 -relaxed second order optimality condition for \u03c4 0.1p.\nProof. We plug in v = z in the second-order optimality condition (4.3), and obtain that\nP \u2126 (zx + xz ) 2 F 2z P \u2126 (M \u2212 xx )z .\n(4.6)\nIntuitively, when restricted to \u2126, the squared Frobenius on the LHS and the quadratic form on the RHS should both be approximately a p fraction of the unrestricted case. In fact, both LHS and RHS can be written as the sum of terms of the form P \u2126 (uv T ), P \u2126 (st T ) , because P \u2126 (zx + xz ) 2 F = 2 P \u2126 (zx T ), P \u2126 (zx T ) + 2 P \u2126 (zx T ), P \u2126 (xz T ) 2z P \u2126 (M \u2212 xx )z = 2 P \u2126 (zz T ), P \u2126 (zz T ) \u2212 2 P \u2126 (xx T ), P \u2126 (zz T ) .\nTherefore we can use concentration inequalities (Theorem D.1), and simplify the equation\nLHS of (4.6) = p zx + xz 2 F \u00b1 O( pd x 2 \u221e z 2 \u221e x 2 z 2 ) = 2p x 2 z 2 + 2p x, z 2 \u00b1 O(p\u03b5) , (Since x, z \u2208 B)\nwhere \u03b5 = O(\u00b5 2 log d pd ). Similarly, by Theorem D.1 again, we have RHS of (4.6) = 2 P \u2126 (zz ), P \u2126 (zz ) \u2212 P \u2126 (xx ), P \u2126 (zz )\n(Since M = zz ) = 2p z 4 \u2212 2p x, z 2 \u00b1 O(p\u03b5)\n(by Theorem D.1 and x, z \u2208 B)\n(Note that even we use the \u03c4 -relaxed second order optimality condition, the RHS only becomes 1.99p z 4 \u2212 2p x, z 2 \u00b1 O(p\u03b5) which does not effect the later proofs.)\nTherefore plugging in estimates above back into equation (4.6), we have that\n2p x 2 z 2 + 2p x, z 2 \u00b1 O(p\u03b5) 2 z 4 \u2212 2 x, z 2 \u00b1 O(p\u03b5) , which implies that 6p x 2 z 2 2p x 2 z 2 + 4p x, z 2 2p z 4 \u2212 O(p\u03b5)\n. Using z 2 = 1, and \u03b5 being sufficiently small, we complete the proof.\nNext we use first order optimality condition to pin down another property of x -it has to be close to z after scaling. Note that this doesn't mean directly that x has to be close to z since x = 0 also satisfies first order optimality condition (and therefore the conclusion (4.7) below).\nLemma 4.5. With high probability over the randomness of \u2126, for any x \u2208 B that satisfies first-order optimality condition (4.2), we have that x also satisfies z,\nx z \u2212 x 2 x O(\u03b5) . (4.7)\nwhere \u03b5 =\u00d5(\u00b5 3 (pd) \u22121/2 ).\nProof. Note that since x \u2208 B, we have R(x) = 0. Therefore first-order optimality condition says that\nP \u2126 (M \u2212 xx )x = P \u2126 (zz )x \u2212 P \u2126 (xx )x = 0 . (4.8)\nAgain, intuitively we hope P \u2126 (zz T ) \u2248 pzz T and P \u2126 (xx T )x \u2248 p x 2 x. These are made precise by the concentration inequalities Lemma D.4 and Theorem D.2 respectively.\nBy Theorem D.2, we have that with high probability over the choice of \u2126, for every x \u2208 B,\nP \u2126 (xx )x \u2212 pxx x F p\u03b5 x 3 p\u03b5 (4.9)\nwhere \u03b5 =\u00d5(\u00b5 3 (pd) \u22121/2 ). Similarly, by Lemma D.4, we have that for with high probability over the choice of \u2126,\nP \u2126 (zz ) \u2212 pzz \u03b5p .\nfor \u03b5 =\u00d5(\u00b5 2 (pd) \u22121/2 ). Therefore for every x, P \u2126 (zz )x \u2212 pzz x \u03b5p x \u03b5p .\n(4.10)\nPlugging in estimates (4.10) and (4.9) into equation (4.8), we complete the proof.\nFinally we combine the two optimality conditions and show equation (4.7) implies xx T must be close to zz T .\nLemma 4.6. Suppose vector x satisfies that x 2 1/4, and that z, x z \u2212 x 2 x \u03b4 . Then for \u03b4 \u2208 (0, 0.1),\nxx \u2212 zz 2 F O(\u03b4) .\nProof. We write z = ux+v where u \u2208 R and v is a vector orthogonal to x. Now we know z,\nx z = u 2 x 2 x+u x 2 v, therefore \u03b4 z, x z \u2212 x 2 x = x 2 u 2 v 2 + (1 \u2212 u 2 ) 2 .\nIn particular, we know |1 \u2212 u 2 | 4\u03b4 and u v 4\u03b4. This means |u| \u2208 1 \u00b1 3\u03b4 and v 8\u03b4. Now we expand\nxx T \u2212 zz T : xx T \u2212 zz T = (1 \u2212 u 2 )xx T + uxv T + uvx T + vv T It is clear that all the terms have norm bounded by O(\u03b4), therefore xx \u2212 zz 2 F O(\u03b4).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Extension to general x", "text": "We have shown when x is incoherent and satisfies first and second order optimality conditions, then it must be close to z or \u2212z. Now we need to consider more general cases when x may have some very large coordinates. Here the main intuition is that the first order optimality condition with a proper regularizer is enough to guarantee that x cannot have a entry that is too much bigger than \u00b5/ \u221a d.\nLemma 4.7. With high probability over the choice of \u2126, for any x that satisfies first-order order optimality condition (4.2), we have x \u221e 4 max \u03b1, \u00b5 p/\u03bb . (4.11)\nHere we recall that \u03b1 was chosen to be 10\u00b5/ \u221a d and \u03bb is chosen to be large so that the \u03b1 dominates the second term \u00b5 p/\u03bb in the setting of Theorem 4.2.\nProof of Lemma 4.7. Suppose i = max j |x j |. Without loss of generality, suppose x i 0. Suppose i -th row of \u2126 consists of entries with index [i] \u00d7 S i . If |x i | 2\u03b1, we are done. Therefore in the rest of the proof we assume |x i | > 2\u03b1. Note that when p c(log d)/d for sufficiently large constant c, with high probability over the choice of \u2126, we have |S i | 2pd. In the rest of argument we are working with such an \u2126 with |S i | 2pd.\nWe will compare the i -th coordinate of LHS and RHS of first-order optimality condition (4.2). For preparation, we have\n|(P \u2126 (M )x) i | = P \u2126 (zz )x i = j\u2208S i z i z j x j |x i | j\u2208S i |z i z j | |x i | \u2022 \u00b5 2 /d \u2022 |S i | 2|x i |p\u00b5 2\n(4.12) where the last step we used the fact that |S i | 2pd. Moreover, we have that\n(P \u2126 (xx )x) i = j\u2208S i x i x 2 j 0 ,\nand that\n(\u03bb\u2207R(x)) i = 4\u03bb(|x i | \u2212 \u03b1) 3 sign(x i ) \u03bb 2 |x i | 3 (Since x i 2\u03b1)\nNow plugging in the bounds above into the i -th coordinate of equation (4.2), we obtain\n4|x i |p\u00b5 2 2(P \u2126 (M \u2212 xx )x) i (\u03bb\u2207R(x)) i \u03bb 2 |x i | 3 , which implies that |x i | 4 p\u00b5 2 /\u03bb.\nSetting \u03bb \u00b5 2 p/\u03b1 2 and \u03b1 = 10\u00b5 1/d, Lemma 4.7 ensures that any x that satisfies first-order optimality condition is the following ball,\nB = x \u2208 R d : x \u221e 4\u03b1 .\nThen we would like to continue to use arguments similar to Lemma 4.4 and 4.5. However, things have become more complicated as now we need to consider the contribution of the regularizer. The guarantees and proofs are very similar to Lemma 4.4. The main intuition is that we can restrict our attentions to coordinates whose regularizer is equal to 0. See Section A for details.\nWe will now deal with first order optimality condition. We first write out the basic extension of Lemma 4.5, which follows from the same proof except we now include the regularizer term.\nLemma 4.9 (Basic extension of Lemma 4.5). With high probability over the randomness of \u2126, for any x \u2208 B that satisfies first-order optimality condition (4.2), we have that x also satisfies\nz, x z \u2212 x 2 x \u2212 \u03b3 \u2022 \u2207R(x) O(\u03b5) . (4.13)\nwhere \u03b5 =\u00d5(\u00b5 6 (pd) \u22121/2 ) and \u03b3 = \u03bb/(2p) 0.\nNext we will show that we can remove the regularizer term, the main observation here is nonzero entries \u2207R(x) all have the same sign as the corresponding entries in x. See Section A for details.\nLemma 4.10. Suppose x \u2208 B satisfies that x 2 1/8, under the same assumption as Lemma 4.9. we have,\nx, z z \u2212 x 2 x O(\u03b5)\nFinally we combine Lemma 4.7, Lemma 4.8, Lemma 4.10 and Lemma 4.6 to prove Lemma 4.3. The argument are also summarized in Figure 1, where we partition R d into regions where our lemmas apply.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Rank-r case", "text": "In this section we show how to extend the results in Section 4 to recover matrices of rank r. Here we still use the same proof strategy of Section 3. Though for simplicity we only write down the proof for the partial observation case, while the analysis for the full observation case (which was our starting point) can be obtained by substituting\n[d] \u00d7 [d] for \u2126 everywhere.\nRecall that in this case we assume the original matrix M = ZZ T , where Z \u2208 R d\u00d7r . We also assume Assumption 1. The objective function is very similar to the rank 1 case\nf (X) = 1 2 P \u2126 (M \u2212 XX ) 2 F + \u03bbR(X) , (5.1)\nwhere R(X) = d i=1 r( X i ) . Recall that r(t) = (|t| \u2212 \u03b1) 4 I t \u03b1 . Here \u03b1 and \u03bb are again parameters that we will determined later.\nWithout loss of generality, we assume that Z 2 F = r in this section. This implies that \u03c3 max (Z) 1 \u03c3 min (Z). Now we shall state the first and second order optimality conditions: Proposition 5.1. If X is a local optimum of objective function (5.1), its first order optimality condition is, 2P \u2126 (M )X = 2P \u2126 (XX )X + \u03bb\u2207R(X) ,\n(5.2)\nand the second order optimality condition is equivalent to\n\u2200V \u2208 R d\u00d7r , P \u2126 (V X + XV ) 2 F + \u03bb V , \u2207 2 R(X)V 2 P \u2126 (M \u2212 XX ), V V . (5.3)\nNote that the regularizer now is more complicated than the one dimensional case, but luckily we still have the following nice property.\nProposition 5.2. We have that \u2207R(X) = \u0393X where \u0393 \u2208 R d\u00d7d is a diagonal matrix with\n\u0393 ii = 4( Xi \u2212\u03b1) 4 Xi I Xi \u03b1 . As a direct consequence, (\u2207R(X)) i , X i 0 for every i \u2208 [d].\nNow we are ready to state the precise version of Theorem 1.1:\nTheorem 5.3. Suppose p C max{\u00b5 6 \u03ba 16 r 4 , \u00b5 4 \u03ba 4 r 6 }d \u22121 log 2 d where C is a large enough constant. Let \u03b1 = 32\u00b5\u03bar/ \u221a d, \u03bb \u00b5 2 rp/\u03b1 2 . Then with high probability over the randomness of \u2126, any local minimum X of f (\u2022) satisfies that f (X) = 0, and in particular, ZZ = XX .\nMoreover, If X satisfies that \u2207f (X) F \u03b4 p\u03c3 3 (Z)/C and \u2207 2 f (X) \u22121/C \u2022 \u00b5 2 \u03bar 2 p 1/2 d \u22121/2 I, then X is an approximate global minimum in the sense that\nXX \u2212 M 2 F O(\u03b4/p).\nThe proof of this Theorem follows from a similar path as Theorem 4.2. We first notice that because of the regularizer, any matrix X that satisfies first order optimality condition must be somewhat incoherent (this is analogues to Lemma 4.7):\nLemma 5.4. Suppose |S i | 2pd. Then for any X satisfies 1st order optimality (5.2), we have\nX 2\u2192\u221e = max i X i\n4 max \u03b1, \u00b5 rp/\u03bb (5.4)\nProof. Assume i = argmax i X i . Suppose the ith row of \u2126 consists of entries with index [i] \u00d7 S i . If X i 2\u03b1, then we are done. Therefore in the rest of the proof we assume X i 2\u03b1. We will compare the i-th row of LHS and RHS of (5.2). For preparation, we have (P \u2126 (M )x) i = P \u2126 (ZZ )X i = P \u2126 (ZZ ) i X\n(5.5)\nThen we have that\nP \u2126 (ZZ ) i 1 = j\u2208S i | Z i , Z j | j\u2208S i Z i Z j j\u2208S i \u00b5 2 r/d|S 1 | (by incoherence of Z) 2\u00b5 2 rp . (by |S i | 2pd)\nTherefore we can bound the 2 norm of LHS of 1st order optimality condition (5.2) by P \u2126 (ZZ )\nX i P \u2126 (ZZ ) i 1 X 1\u21922 2\u00b5 2 rp X 2\u2192\u221e (by X 2\u2192\u221e = X 1\u21922 ) = 2\u00b5 2 rp X i (5.6)\nNext we lowerbound the norm of the RHS of equation (5.2). We have that\n(P \u2126 (XX )X) i = j\u2208S i X i , X j X j = X i j\u2208X i X j X j ,\nwhich implies that\n(P \u2126 (XX )X) i , X i = X i \uf8eb \uf8ed j\u2208X i X j X j \uf8f6 \uf8f8 X i 0 .\n(5.7)\nUsing Proposition 5.2 we obtain that\n(P \u2126 (XX )X) i , (\u2207R(X)) i = \u0393 ii X i \uf8eb \uf8ed j\u2208X i X j X j \uf8f6 \uf8f8 X i 0 .\n(5.8)\nIt follows that\n(P \u2126 (XX )X) i + (\u03bb\u2207R(X)) i (\u03bb\u2207R(X)) i (by equation (5.8)) = 4\u03bb( X i \u2212 \u03b1) 3 X i \u2022 X i (by Proposition 5.2) \u03bb 2 X i 3 (by the assumptino X i 2\u03b1)\nTherefore plugging in equation above and equation (5.6) into 1st order optimality condition (5.2). We obtain that X i 8\u00b5 2 rp/\u03bb which completes the proof.\nNext, we prove a property implied by first order optimality condition, which is similar to Lemma 4.9.\nLemma 5.5. In the setting of Theorem 5.3, with high probability over the choice of \u2126, for any X that satisfies 1st order optimality condition (5.2), we have X 2 F 2r\u03c3 max (Z) 2 .\n(5.9)\nMoreover, we have \u03c3 max (X) 2\u03c3 max (Z)r 1/6 .\n(5.10) and ZZ T X \u2212 XX T X \u2212 \u03b3\u2207R(X) F O(\u03b4) (5.11)\nwhere \u03b4 = O(\u00b5 3 \u03ba 3 r 2 log 0.75 (d)\u03c3 max (Z) \u22123 (dp) \u22121/2 ) and \u03b3 = \u03bb/(2p) 0.\nProof. If X F r\u03c3 max (Z) 2 we are done. When X F r\u03c3 max (Z) 2 , by Lemma 5.4, we have that max X i 4\u03b1 = O(\u00b5\u03bar/ \u221a d), and therefore max X i \u03bd X F with \u03bd = O(\u00b5\u03ba \u221a r/\u03c3 max (Z)). Then by Theorem D.2, we have that P \u2126 (ZZ )X \u2212 pZZ X F p\u03b4 , and P \u2126 (XX )X \u2212 pXX X F p\u03b4 ,\nwhere \u03b4 = O(\u00b5 3 \u03ba 3 r 2 log 0.75 (d)\u03c3 max (Z) \u22123 (dp) \u22121/2 ). These two imply equation (5.11). Moreover, we have p ZZ X F = P \u2126 (ZZ )X F \u00b1 p\u03b4 = P \u2126 (XX )X + \u03bbR(X) F \u00b1 p\u03b4 (by equation (5.2))\nP \u2126 (XX )X F \u00b1 p\u03b4 (by equation (5.8))\np XX X F \u00b1 2p\u03b4 (5.12) Suppose X has singular value \u03c3 1 . . . \u03c3 r . Then we have ZZ X\n2 F ZZ 2 X 2 F \u03c3 max (Z) 4 X 2 F = \u03c3 max (Z) 4 (\u03c3 2 1 + \u2022 \u2022 \u2022 + \u03c3 2 r ).\nOn the other hand, XX X\n2\nF = \u03c3 6 1 + \u2022 \u2022 \u2022 + \u03c3 6 r .\nTherefore, equation (5.12) implies that\n(1 + O(\u03b4))\u03c3 max (Z) 4 r i=1 \u03c3 2 i r i=1 \u03c3 6 i\nThen we have (by Proposition E.2) we complete the proof.\nNow we look at the second order optimality condition, this condition implies the smallest singular value of X is large (similar to Lemma 4.8). Note that this lemma is also true even if x only satisfies relaxed second order optimality condition with \u03c4 = 0.01p\u03c3 min (Z).\nLemma 5.6. In the setting of Theorem 5.3. With high probability over the choice of \u2126, suppose X satisfies equation (5.9), (5.4) the 2nd order optimality condition (5.3). Then,\n\u03c3 min (X) 1 4 \u03c3 min (Z) (5.13)\nProof. Let J = {i : X i \u03b1}. Let v \u2208 R r such that Xv = \u03c3 min (X). . Let Z J be the matrix that has the same i-th row as Z for every i \u2208 J and 0 elsewhere.\nWe claim that \u03c3 min (Z J )\n1 2 \u03c3 min (Z). Let L = [d] \u2212 J.\nSince for any i \u2208 L it holds that X i \u03b1, we have |L|\u03b1 2\nX 2 F 2r\u03c3 max (Z) 2 (by equation (5.9)), and it follows that |L| 2r\u03c3 max (Z) 2 /\u03b1 2 . Therefore,\n\u03c3 min (Z J ) \u03c3 min (Z) \u2212 \u03c3 max (Z L ) \u03c3 min (Z) \u2212 Z L F \u03c3 min (Z) \u2212 |L|r\u00b5 2 /d \u03c3 min (Z) \u2212 2r 2 \u03c3 max (Z) 2 \u00b5 2 /(\u03b1 2 d) 1 2 \u03c3 min (Z) . (by \u03b1 r\u03ba\u00b5 \u221a d )\nTherefore, Z J has column rank exactly r. By variational characterization of singular values, we have that for there exists unit vector z J \u2208 col-span(Z J ) such that X z J \u03c3 min (X). Since z J \u2208 col-span(Z J ) is a unit vector, we have that z J can be written as z J = Z J \u03b2 where \u03b2 1 \u03c3min(Z J )\nO(1/\u03c3 min (Z)). Therefore this in turn implies that\nz J \u221e Z J 2\u2192\u221e \u03b2 O(\u00b5 r/d/\u03c3 min (Z)\n) O(\u00b5\u03ba r/d). We will plug in V = z J v T in the 2nd order optimality condition (5.3). Note that since z J \u2208 col-span(Z J ), it is supported on subset J, and therefore \u2207 2 R(X)V = 0. Therefore the term about regularization in (5.3) will vanish. For simplicity, let y = X z J , w = Xv We obtain that taking V = z J v in equation (5.3) will result in\nP \u2126 (wz J + z J w ) 2 F 2 P \u2126 (ZZ \u2212 XX ), z J z J Note that we have that w \u221e X 2\u2192\u221e v \u00b5 r/d. Recalling that z J \u221e O(\u00b5\u03ba r/d), by Theorem D.1, we have that p wz J + z J w 2 F 2p ZZ \u2212 XX , z J z J \u2212 \u03b4p where \u03b4 = O(\u00b5 2 \u03bar 2 (pd) \u22121/2 ).\nThen simple algebraic manipulation gives that\nw, z J 2 + w 2 z J 2 + X z J 2 Z z J 2 \u2212 \u03b4/2 (5.14)\nNote that w, z J = v, X z J = y, v . Recall that z J = 1 and z \u2208 col-span(Z J ), and therefore Z z J = Z J z J \u03c3 2 min (Z J ). Moreover, recall that y = X z J \u03c3 min (X). Using these with equation (5.14) we obtain that\nw, z J 2 + w 2 z J 2 + X z J 2 y, v 2 + w 2 + y 2 2 y 2 + \u03c3 2 min (X)\n(by Cauchy-Schwarz and w = \u03c3 min (X).)\n3\u03c3 2 min (X) (by y \u03c3 min (X).)\nTherefore together with equation (5.14) and Z z J \u03c3 2 min (Z J ) we obtain that \u03c3 min (X) (1/2 \u2212 \u2126(\u03b4))\u03c3 min (Z J ) (5.15) Therefore combining equation (5.15) and the lower bound on \u03c3 min (Z J ) we complete the proof.\nSimilar as before, we show it is possible to remove the regularizer term here, again the intuition is that the regularizer is always in the same direction as X.\nLemma 5.7. Suppose X satisfies equation (5.4) and (5.13) and (5.10), then for any \u03b3 0,\nZZ T X \u2212 XX T X 2 F ZZ T X \u2212 XX T X \u2212 \u03b3\u2207R(X) 2 F (5.16) Proof. Let L = {i : X i \u03b1}.\nFor i \u2208 L, we have that (\u2207R(X)) i = 0. Therefore it suffices to prove that for every\ni \u2208 L, Z i Z X \u2212 X i X X 2 Z i Z X \u2212 X i X X \u2212 (\u03b3\u2207R(X)) i 2\nIt suffices to prove that (\u2207R(X)) i , X i X X \u2212 Z i Z X 0 (5.17) By proposition 5.2, we have \u2207R(X\n)) i = \u0393 ii X i for \u0393 ii 0. Then (\u2207R(X)) i , X i X X = \u0393 ii X i , X i X X = \u0393 ii X i X XX i \u0393 ii X i 2 \u03c3 min (X T X) 1 16 \u0393 ii X i 2 \u03c3 min (Z) 2\n(by equation 5.13)\nOn the other hand, we have\n(\u2207R(X)) i , Z i Z X = \u0393 ii X i , Z i Z X \u0393 ii X i Z i \u03c3 max (Z T X) \u0393 ii X i Z i \u03c3 max (Z)\u03c3 max (X) 2\u0393 ii X i Z i \u03c3 max (Z) 2 r 1/6\n(by equation (5.10)) 1 16\n\u0393 ii X i 2 \u03c3 min (Z) 2 r \u22121/3 (because X i \u03b1 32\u03ba\u00b5r/ \u221a d 32 \u221a r Z i )\nTherefore combining two equations above we obtain equation (5.17) which completes the proof.\nFinally we show the form in Equation (5.16) implies ZZ T is close to XX T (this is similar to Lemma 4.6).\nLemma 5.8. Suppose X and Z satisfies that \u03c3 min (X) 1/4 \u2022 \u03c3 min (Z) and that\nZZ T X \u2212 XX T X 2 F \u03b4 2\nwhere \u03b4 \u03c3 3 min (Z)/C for a large enough constant C, then\nXX \u2212 ZZ 2 F O(\u03b4\u03ba 2 /\u03c3 min (Z)).\nProof. The proof is similar to the one-dimensional case, we will separate Z into the directions that are in column span of X and its orthogonal subspace. We will then show the projection of Z in the column span is close to X, and the projection on the orthogonal subspace must be small. Let Z = U + V where U = Proj span(X) Z is the projection of Z to the column span of X, and V is the projection to the orthogonal subspace. Then since V T X = 0 we know\nZZ T X = (U + V )(U + V ) T X = U U T X + V U T X.\nHere columns of the first term U U T X are in the column span of X, and the columns second term V U T X are in the orthogonal subspace. Therefore,\nZZ T X \u2212 XX T X 2 F = U U T X \u2212 XX T X 2 F + V U T X 2 F \u03b4 2 .\nIn particular, both terms should be bounded by \u03b4 2 . Therefore U U T \u2212 XX T 2 F \u03b4 2 /\u03c3 2 min (X) 16\u03b4 2 /\u03c3 2 min (Z). Also, we know \u03c3 min (U\nU T X) \u03c3 min (XX T X) \u2212 \u03b4 \u03c3 min (Z) 3 /128 if \u03b4 \u03c3 min (Z) 3 /128. Therefore \u03c3 min (U T X) is at least \u03c3 min (Z) 3 / Z 128. Now V 2 F \u03b4 2 /\u03c3 min (U T X) 2 O(\u03b4 2 Z 2 /\u03c3 min (Z) 6 ). Finally, we can bound U V T F by U V F Z V F (last inequality is because U is a projection of Z), which at least \u2126( V 2 F ) when \u03b4 \u03c3 min (Z) 3 /128, therefore ZZ T \u2212 XX T F U U T \u2212 XX T F + 2 U V T F + V V T F O(\u03b4 Z 2 /\u03c3 min (Z) 3 ).\nLast thing we need to prove the main theorem is a result from Sun and Luo [SL15], which shows whenever XX T is close to ZZ T , the function is essentially strongly convex, and the only points that have 0 gradient are points where XX T = ZZ T , this is explained in Lemma C.1. Now we are ready to prove Theorem 5.3:\nProof of Theorem 5.3. Suppose X satisfies 1st and 2nd order optimality condition. Then by Lemma 5.5 and Lemma 5.4, we have that X satisfies equation (5.4), (5.9), (5.10) and (5.11). Then by Lemma 5.6, we obtain that \u03c3 min (X) 1/6 \u2022 \u03c3 min (Z). Now by Lemma 5.7 and equation (5.11), we have that ZZ T X \u2212 XX T X F \u03b4 for \u03b4 c\u03c3 min (Z) 3 /\u03ba 2 for sufficiently small constant c. Then by Lemma 5.8 we obtain that ZZ \u2212 XX F c\u03c3 min (Z) 2 for sufficiently small constant c. By Lemma C.1, in this region the only points that satisfy the first order optimality condition must satisfy XX T = ZZ T .\nHandling Noise. To handle noise, notice that we can only hope to get an approximate solution in presence of noise, and to get that our Lemmas only depend on concentration bounds which still apply in the noisy setting. See Section B for details.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusions", "text": "Although the matrix completion objective is non-convex, we showed the objective function has very nice properties that ensures the local minima are also global. This property gives guarantees for many basic optimization algorithms. An important open problem is the robustness of this property under different model assumptions: Can we extend the result to handle asymmetric matrix completion? Is it possible to add weights to different entries (similar to the settings studied in [LLR16])? Can we replace the objective function with a different distance measure rather than Frobenius norm (which is related to works on 1-bit matrix sensing [DPvdBW14])? We hope this framework of analyzing the geometry of objective function can be applied to other problems.", "publication_ref": ["b21", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "A Omitted Proofs in Section 4", "text": "We first prove the equivalent form of the first and second order optimality conditions: Lemma A.1 (Proposition 4.1 restated). The first order optimality condition of objective (4.1) is,\n2P \u2126 (M \u2212 xx )x = \u03bb\u2207R(x) ,\nand the second order optimality condition requires:\n\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v .\nMoreover, The \u03c4 -relaxed second order optimality condition requires\n\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v \u2212 \u03c4 v 2 .\nProof. We take the Taylor's expansion around point x. Let \u03b4 be an infinitesimal vector, we have\nf (x + \u03b4) = 1 2 P \u2126 (M \u2212 (x + \u03b4)(x + \u03b4) ) 2 F + \u03bbR(x + \u03b4) + o( \u03b4 2 ) = 1 2 P \u2126 (M \u2212 xx \u2212 (x\u03b4 + \u03b4x ) \u2212 \u03b4\u03b4 ) 2 F + \u03bb R(x) + \u2207R(x), \u03b4 + 1 2 \u03b4 T \u2207 2 R(x)\u03b4 + o( \u03b4 2 ) = 1 2 M \u2212 xx 2 \u2126 + \u03bbR(x) \u2212 P \u2126 (M \u2212 xx ), x\u03b4 + \u03b4x + \u2207R(x), \u03b4 + o( \u03b4 2 ) \u2212 P \u2126 (M \u2212 xx ), \u03b4\u03b4 + 1 2 P \u2126 (x\u03b4 + \u03b4x ) 2 F + 1 2 \u03bb\u03b4 \u2207 2 R(x)\u03b4 + o( \u03b4 2 ). By symmetry P \u2126 (M \u2212 xx ), x\u03b4 = P \u2126 (M \u2212 xx ), \u03b4x = P \u2126 (M \u2212 xx )x, \u03b4 , so the first order optimal- ity condition is \u2200\u03b4, \u22122P \u2126 (M \u2212 xx )x + \u03bb\u2207R(x), \u03b4 = 0, which is equivalent to that 2P \u2126 (M \u2212 xx )x = \u03bb\u2207R(x).\nThe second order optimality condition says \u2212 P \u2126 (M \u2212 xx ), \u03b4\u03b4 + 1 2 x\u03b4 + \u03b4x 2 F + 1 2 \u03bb\u03b4 \u2207 2 R(x)\u03b4 0 for every \u03b4, which is exactly equivalent to Equation (4.3).\nNext we show the full proof for the second order optimality condition: Lemma A.2 (Lemma 4.8 restated). In the setting of Theorem 4.2, with high probability over the choice of \u2126, suppose x \u2208 B satisfies second-order optimality condition (4.3) or \u03c4 -relaxed condition for \u03c4 0.1p, we have x 2 1/8.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof. If x", "text": "1, then we are done. Therefore in the rest of the proof we assume x 1. The proof is very similar to Lemma 4.4. We plug in v = z J instead into equation (4.3), where J = {i : |x i | \u03b1}. Note that z J \u2207 2 R(z J )z J vanishes. We plug in v = z J in the equation (4.3) and obtain that x satisfies that\nP \u2126 (z J x + xz J ) 2 F 2z J P \u2126 (M \u2212 xx )z J . (A.1)\nNote that we assume x \u221e 2\u03b1, and in the beginning of the proof we assume wlog x 1. Moreover, we have\nz J \u00b5 \u221a d an, z J 1.\nSimilarly to the derivation in the proof of Lemma 4.4, we apply Theorem D.1 (twice) and obtain that with high probability over the choice of \u2126, for every x, for \u03b5 =\u00d5(\u00b5 2 (pd) \u22121/2 ),\nLHS of (A.1) = p z J x + xz J 2 F \u00b1 O(p\u03b5) = 2p x 2 z J 2 + 2p x, z J 2 \u00b1 O(p\u03b5) .\nRHS of (A.1) = 2 P \u2126 (zz ), P \u2126 (z J z J ) \u2212 P \u2126 (xx ), P \u2126 (z J z J )\n(Since M = zz ) = 2 z J 4 \u2212 2 x, z J 2 \u00b1 O(p\u03b5) .\n(by Theorem D.1) (Again notice that using \u03c4 -relaxed second order optimality condition does not effect the RHS by too much, so it does not change later steps.) Therefore plugging the estimates above back into equation (A.1), we have that\np x 2 z J 2 + 2p x, z J 2 p z J 4 \u00b1 O(p\u03b5) ,\nUsing Cauchy-Schwarz, we have x 2 z J 2\nx, z J 2 , and therefore we obtain that z\nJ 2 x 2 1 3 z J 4 \u2212 O(\u03b5). Finally, we claim that z J 2 1/2, which completes the proof since x 2 1 3 z J 2 \u2212 O(\u03b5) 1/8. Claim A.3. Suppose \u03b1 4\u00b5 \u221a d and x satisfies x \u221e 4\u03b1 and x 2. Let J = {i : |x i | \u03b1}. Then we have that z J 1/2.\nThe claim can be simply proved as follows: Since x 2 2 we have that |J c | 2/\u03b1 2 and therefore z J c 2 2\u00b5 2 /(d\u03b1 2 ). This further implies that z J\n2 = z 2 \u2212 z L 2 (1 \u2212 2\u00b5 2 /(d\u03b1 2 )) 1 2 because \u03b1 2\u00b5 \u221a d .\nLemma A.4 (Lemma 4.10 restated). Suppose x \u2208 B satisfies that x 2 1/8, under the same assumption as Lemma 4.9. we have,\nx, z z \u2212 x 2 x O(\u03b5)\nProof. Let L = {i : x i \u03b1}. For i \u2208 L, we have that (\u2207R(x)) i = 0. Therefore it suffices to prove that for every i \u2208 L,\n(z i z x \u2212 x i x ) 2 (z i z x \u2212 x i x \u2212 (\u03b3\u2207R(x)) i ) 2\nIt suffices to prov that\n(\u2207R(x)) i (x i x 2 \u2212 z i z, x ) 0 (A.2)\nSince we have \u2207R(x) i = \u03b3 i x i for some \u03b3 i 0, we have\n(\u2207R(x)) i \u2022 x i x 2 = \u03b3 i x i , x i x 2 \u03b3 i x 2 i x 2 1 \u221a 8 \u03b3 i x 2 i x (since x 2 1/8)\nOn the other hand, we have\n(\u2207R(x)) i \u2022 z i z, x = \u03b3 i x i z i z, x 1 4 \u03b3 i x 2 i x z (by |x i | \u03b1 4|z i |)\nTherefore combining two equations above we obtain equation (A.2) which completes the proof.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Handling Noise", "text": "Suppose instead of observing the matrix ZZ T , we actually observe a noisy version M = ZZ T + N , where N is a Gaussian matrix with independent N (0, \u03c3 2 ) entries. In this case we should not hope to exactly recover ZZ T (as two close Z's may generate the same observation). In this Section we show even with fairly large noise our arguments can still hold.\nTheorem B.1. Let\u03bc = max{\u00b5, 4\u03c3d \u221a log d r }. Suppose p C\u03bc 6 \u03ba 12 r 4 d \u22121 \u03b5 \u22122 log 1.5 d where C is a large enough constant. Let \u03b1 = 2\u03bc\u03bar/ \u221a d, \u03bb \u03bc 2 rp/\u03b1 2 .\nThen with high probability over the randomness of \u2126, any local minimum\nX of f (\u2022) satisfies XX T \u2212 ZZ T F \u03b5.\nIn fact, a noise level \u03c3 \u221a log d \u00b5 2 r/d (when the noise is almost as large as the maximum possible entry) does not change the conclusions of Lemmas in this Section.\nProof. There are only three places in the proof where the noise will make a difference. These are: 1. The infinity norm bound of M , used in Lemma 5.4. 2. The LHS of first order optimality condition (Equation (5.2)). 3. The RHS of second order optimality condition (Equation (5.3)).\nWhat we require in these three steps are: 1. |M | \u221e should be smaller than \u00b5 2 r/d. 2. P \u2126 (N ), W should be smaller than | P \u2126 (N ), P \u2126 (W ) | O(\u03c3|Z| \u221e dr log d + pd 2 r\u03c3 2 |W | \u221e W F log d). 3. P \u2126 (N ) \u03b5p ZZ T F . When we define the\u03bc = max{\u00b5, 4\u03c3d \u221a log d r }, all of these are satisfied (by Lemma D.5 and D.6). Now we can follow the proof and see \u03b4 c\u03b5\u03c3 min (Z)/\u03ba 2 for small enough constant c, and By Lemma 5.8 we know XX T \u2212 ZZ T F \u03b5.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Finding the Exact Factorization", "text": "In Section 5, we showed that any point that satisfies the first and second order necessary condition must satisfy XX T \u2212 ZZ T F c for a small enough constant c. In this section we will show that in fact XX T must be exactly equal to ZZ T . The proof technique here is mostly based on the work of Sun and Luo [SL15]. However we have to modify their proof because we use slightly different regularizers, and we work in the symmetric case. The main Lemma in [SL15] can be rephrased as follows in our setting: Lemma C.1 (Analog to Lemma 3.1 in [SL15]). Suppose p C\u00b5 4 r 6 \u03ba 4 d \u22121 log d for large enough absolute constant C, and \u03b5 = \u03c3 min (Z) 2 /100. with high probability over the randomness of \u2126, we have that for any point X in the set\nB \u03b5 = X \u2208 R d\u00d7r : XX T \u2212 ZZ T F \u03b5, X 2\u2192\u221e 16\u00b5\u03bar \u221a d , (C.1)\nthere exists a matrix U such that U U T = ZZ T and \u2207f (X),\nX \u2212 U p 4 M \u2212 XX T 2 F .\nAs a consequence, any point X in the set B that satisfies first order optimality condition must be a global optimum (or, equivalently, satisfy XX T = ZZ T ).\nRecall f (X) = 1 2 P \u2126 (M \u2212 XX T ) 2 F + \u03bbR(X). The proof of Lemma C.1 consists of three steps: 1. The regularizer has nonnegative correlation with (X \u2212 U ): for any U such that U U T = ZZ T , we have \u2207R(X), X \u2212 U 0. (Claim C.3). 2. There exists a matrix U such that U U T = ZZ T , and U is close to X. (Claim C.4) 3. Argue that \u2207f (x), X \u2212 U\np 4 P \u2126 (M \u2212 XX T ) 2\nF when U is close to X. (See proof of Lemma C.1). Before going into details, the first useful observation is that all matrices U with U U T = ZZ T have the same row norm.\nClaim C.2. Suppose U, Z \u2208 R d\u00d7r satisfy U U = ZZ . Then, for any i \u2208 [d] we have U i = Z i . Consequently, U F = Z F .\nProof. Suppose U U = ZZ , then we have U = ZR where R is an orthonormal matrix. In particular, the i-th row of U is equal to\nU i = Z i R.\nSince 2 norm (and Frobenius norm) is preserved after multiplying with an orthonormal matrix, we know U i = Z i . The Frobenius norm bound follows immediately.\nNote that this simple observation is only true in the symmetric case. This Claims serves as the same role of the bounds on row norms of U, V in the asymmetric case (Propositions 4.1 and 4.2 of [SL15]).\nNext we are ready to argue that the regularizer is always positively correlated with X \u2212 U .\nClaim C.3. For any U such that U U T = ZZ T , we have, \u2207R(X), X \u2212 U 0.\nProof. Since the regularizer is applied independently to individual rows, we can rewrite \u2207R(X), X \u2212 U = n i=1 \u2207R(X i ), X i \u2212 U i , and focus on i-th row.\nFor each row X i , \u2207R(X i ) is 0 when\nX i 2\u00b5 \u221a r/ \u221a d.\nIn that case \u2207R(X i ), X i \u2212 U i = 0. When X i is larger than 2\u00b5/ \u221a d, we know \u2207R(X i ) is always in the same direction as X i . In this case \u03bb\u2207R(X i ) = \u03b3X i for some \u03b3 > 0 and\nX i 2\u00b5 \u221a r/ \u221a d 2 Z i = 2 U i (\nwhere last equality is by Claim C.2). Therefore by triangle inequality\nX i , X i \u2212 U i X i 2 \u2212 X i U i X i 2 /2 > 0.\nThis then implies \u03bb\u2207R(X i ),\nX i \u2212 U i = \u03b3 X i , X i \u2212 U i > 0.\nNext we will prove the gradient of 1 2 P \u2126 (M \u2212 XX T ) 2 F has a large correlation with X \u2212 U . This is analogous to Proposition 4.2 in [SL15].\nClaim C.4. Suppose XX T \u2212 M F = \u03b5 \u03c3 min (Z) 2 /100, there exists a matrix U such that U U T = M and X \u2212 U F 5\u03b5 \u221a r/\u03c3 min (Z) 2 .\nProof. Without loss of generality we assume M is a diagonal matrix with first r diagonal terms being \u03c3 1 (Z) 2 , \u03c3 2 (Z) 2 , ..., \u03c3 r (Z) 2 (this can be done by a change of basis). That is, we assume M = diag(\u03c3 1 (Z) 2 , . . . , \u03c3 r (Z) 2 ), 0, . . . , 0). We use M to denote the first r \u00d7 r principle submatrix of M .\nWe write X = V W where V contains the first r rows of X, and W \u2208 R (d\u2212r)\u00d7r contains the remaining rows in X.\nWe write similarly U = P Q where P and Q denote the first r rows and the rest of rows respectively.\nIn order to construct U , we first notice that Q must be constructed as a zero matrix since M has non-zero diagonal only on the top-left corner. A natural guess of P then becomes a \"normalized\" version of V .\nConcretely, we construct P :\n= V S = V (V T (M ) \u22121 V ) \u22121/2 (where S := (V T (M ) \u22121 V ) \u22121/2\n). Thus, the difference between U and X is equal to U \u2212 X\nF P \u2212 V F + W F . Since XX T \u2212 M F \u03b5, we know M \u2212 V V T 2 F + 2 V W T 2 F \u03b5 2 .\nIn particular both terms are smaller than \u03b5 2 .\nFirst, we bound W F . Note that since M \u2212 V V T F \u03b5 \u03c3 min (Z) 2 /100, we know \u03c3 min (V ) 2 0.99\u03c3 min (Z) 2 . Therefore \u03c3 min (V ) 0.9\u03c3 min (Z). Now we know W F V W T F /\u03c3 min (V ) 2\u03b5/\u03c3 min (Z).", "publication_ref": ["b32", "b32", "b32", "b32", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Next we bound", "text": "P \u2212 V 2 F . Since M \u2212 V V T F \u03b5 \u03c3 min (Z) 2 /100, we know (1 \u2212 2\u03b5/\u03c3 min (Z) 2 )V V T M\n(1 + 2\u03b5 2 /\u03c3 min (Z) 2 )V V T . This implies V F 1.1 Z F , and (1 \u2212 2\u03b5/\u03c3 min (Z) 2 )I V T M \u22121 V (1 + 2\u03b5/\u03c3 min (Z) 2 )I. Therefore the matrix S is also very close to identity, in particular, S \u2212 I 2\u03b5/\u03c3 min (Z) 2 . Now we know P \u2212\nV F = V F S \u2212 I 3\u03b5 Z F /\u03c3 min (Z) 2 . Using the fact that Z F = 1 we know U \u2212 X F P \u2212 V F + W F 5\u03b5 \u221a r/\u03c3 min (Z) 2 .\nWe can now combine this Claim with a sampling lemma to show P \u2126 ((X \u2212 U )(X \u2212 U ) T ) 2 F is small: Lemma C.5. Under the same setting of Lemma C.1, with probability at least 1 \u2212 1/(2n) 4 over the choice of \u2126, if U satisfies conclusion of Claim C.4, then,\nP \u2126 ((X \u2212 U )(X \u2212 U ) T ) 2 F p 25 M \u2212 XX T 2 F .\nIntuitively, this Lemma is true because\n(X \u2212 U )(X \u2212 U ) T F 25 M \u2212 XX T 2 F r/\u03c3 min (Z) 4\n, which is much smaller than M \u2212XX T F when M \u2212XX T F is small. By concentration inequalities we expect P \u2126 ((X \u2212U )(X \u2212 U ) T ) 2\nF to be roughly equal to p (X \u2212 U )(X \u2212 U ) T F , therefore it must be much smaller than p M \u2212 XX T 2 F . The proof of this Lemma is exactly the same as Proposition 4.3 in [SL15] (in fact, it is directly implied by Proposition 4.3), so we omit the proof here. We also need a different concentration bound for the projection of the norm of the matrix a = U (X \u2212 U ) T + (X \u2212 U )U T . Unlike the previous lemma, here we want P \u2126 (a) F to be large.\nLemma C.6. Under the same setting of Lemma C.1, let a = U (X \u2212 U ) T + (X \u2212 U )U T where U is constructed as in Claim C.4. Then, with high probability, we have that for any X \u2208 B \u03b5 ,\nP \u2126 (a) 2 F 5p 6 a 2 F .\nIntuitively this should be true because a is in the tangent space {Z : Z = U W T + (W )U T } which has rank O(nr). The proof of this follows from Theorem 3.4 [Rec11], and is written in detail in Equations ( 37) -(41) in [SL15].\nFinally we are ready to prove the main lemma. The proof is the same as the outline given in Section 4.1 of [SL15]. We give it here for completeness.\nProof of Lemma C.1. Note that f (X) is equal to h(X) + \u03bbR(X) where where h(X) = 1 2 P \u2126 (M \u2212 XX T ) 2 F , and R(X) is the regularizer. By Claim C.3 we know \u2207R(X), X \u2212 U 0, so we only need to prove there exists a U such that U U T = Z and \u2207g(X),\nX \u2212 U p 4 M \u2212 XX T 2 F . Define a = U (X \u2212 U ) T + (X \u2212 U )U T , b = (U \u2212 X)(U \u2212 X) T , then XX T \u2212 M = a + b and (X \u2212 U )X T + X(X \u2212 U ) T = a + 2b. Now \u2207h(X), X \u2212 U = 2 P \u2126 (XX T \u2212 M )X, X \u2212 U = P \u2126 (XX T \u2212 M ), (X \u2212 U )X T + X(X \u2212 U ) T = P \u2126 (a + b), P \u2126 (a + 2b) = P \u2126 (a) 2 F + 2 P \u2126 (b) 2 F + 3 P \u2126 (a), P \u2126 (b) P \u2126 (a) 2 F + 2 P \u2126 (b) 2 F \u2212 3 P \u2126 (a) P \u2126 (b) . Let \u03b5 = M \u2212 XX T F .\nNote that from Claim C.4 and Lemma C.5, we know b F \u03b5/10, P \u2126 (b) F \u221a pd/5.\nTherefore as long as we can show P \u2126 (a) F is large we are done. This is true because a F M \u2212XX T F \u2212 b F 9\u03b5/10. Hence by Lemma C.6 we know\nP \u2126 (A) 2 F 5p 6 a 2 F 27 40 p\u03b5 2 .\nCombining the bounds for P \u2126 (a) F , P \u2126 (b) F , we know \u2207g(X), X \u2212 U p 4 M \u2212 XX T 2 F . Together with the fact that \u2207R(X), X \u2212 U 0, we know\n\u2207f (X), X \u2212 U p 4 M \u2212 XX T 2 F .", "publication_ref": ["b32", "b29", "b32", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "D Concentration inequality", "text": "In this section we prove the concentration inequalities used in the main part. We first show that the inner-product of two low rank matrices is preserved after restricting to the observed entries. This is mostly used in arguments about the second order necessary conditions.\nTheorem D.1. With high probability over the choice of \u2126, for any two rank-r matrices W, Z \u2208 R d\u00d7d , we have\n| P \u2126 (W ), P \u2126 (Z) \u2212 p W, Z | O(|W | \u221e |Z| \u221e dr log d + pdr|W | \u221e |Z| \u221e W F Z F log d)\nProof. Since both LHS and RHS are bilinaer in both W and Z, without loss of generality we assume the Frobenius norms of W and Z are all equal to 1. Note that in this case we should expect |W | \u221e 1/d. Let \u03b4 i,j be the indicator variable for \u2126, we know\nP \u2126 (W, Z = i,j \u03b4 i,j W i,j Z i,j ,\nand in expectation it is equal to p W, Z . Let Q = i,j (\u03b4 i,j \u2212p)W i,j Z i,j . We can then view Q as a sum of independent entries (note that \u03b4 i,j = \u03b4 j,i , but we can simply merge the two terms and the variance is at most a factor 2 larger). The expectation E[Q] = 0. Each entry in the sum is bounded by |W | \u221e |Z| \u221e , and the variance is bounded by\nV[Q] p i,j (W i,j Z i,j ) 2 p max i,j |W i,j | 2 i,j Z 2 i,j p|W | 2 \u221e .\nSimilarly, we also know V[Q] p|Z| 2 \u221e and hence V[Q] p|W | \u221e |Z| \u221e . Now we can apply Bernstein's inequality, with probability at most \u03b7,\n|Q \u2212 E[Q]| |W | \u221e |Z| \u221e log 1/\u03b7 + p|W | \u221e |Z| \u221e log(1/\u03b7).\nBy Proposition E.3, there is a set \u0393 of size d O(dr) such that for any rank r matrix X, there is a matrixX \u2208 \u0393 such that X \u2212X F 1/d 3 . When W and Z come from this set, we can set \u03b7 = d \u2212Cdr for a large enough constant C. By union bound, with high probability\n|Q \u2212 E[Q]| O(|W | \u221e |Z| \u221e dr log d + pdr|W | \u221e |Z| \u221e log d).\nWhen W and Z are not from this set \u0393, let\u0174 and\u1e90 be the closest matrix in \u0393, then we know | P \u2126 (W ), P \u2126 (Z) \u2212 p W, Z \u2212 ( P \u2126 (\u0174 ), P \u2126 (\u1e90) \u2212 p \u0174 ,\u1e90 Next Theorem shows P \u2126 (XX T )X is roughly equal to pXX T X, this is one of the major terms in the gradient.\n)| O(1/d 3 ) |W | \u221e |Z| \u221e dr log d.\nTheorem D.2. When p C\u03bd 6 r log 2 d d\u03b5 2\nfor a large enough constant C, With high probability over the randomness of \u2126, for any matrix X \u2208 R d\u00d7r such that X i \u03bd 1 d X F , we have P \u2126 (XX )X \u2212 pXX T X F p\u03b5 X 3\nF (D.1)\nProof. Without loss of generality we assume X F = 1. Let \u03b4 i,j be the indicator variable for \u2126, we first prove the result when \u03b4 i,j are independent, then we will use standard techniques to show the same argument works for \u03b4 i,j = \u03b4 j,i .\nNote that [P \u2126 (XX )X] i = j \u03b4 i,j X i , X j X j , whose expectation is equal to [pXX T X] i = p j X i , X j X j .\nWe know X i \u03bd 1 d , therefore each term is bounded by \u03bd 3 (1/d) 3/2 . Let Z i be a random variable that is equal to P \u2126 (XX )X] i \u2212 [pXX T X] i 2 , then it is easy to see E[Z i ] pd\u03bd 6 (r/d) 3 = p\u03bd 6 /d 2 . and the variance Finally we argue that random sampling of a matrix gives a nice spectral approximation. This is a standard Lemma that is used in arguing about the P \u2126 (M )X term in the gradient (P \u2126 (M \u2212 XX T )X).\nLemma D.4. Suppose W \u2208 R d\u00d7d satisfies that |W | \u221e \u03bd d W F , then with high probability (1 \u2212 d \u221210 ) over the choice of \u2126, P \u2126 (W ) \u2212 pW \u03b5p W F .\nwhere \u03b5 = O(\u03bd log d/(pd)).\nProof. Without loss of generality we assume W F = 1. The proof follows simply from application of Bernstein inequality. We view P \u2126 (W ) as\nP \u2126 (W ) = i,j\u2208[d] 2 s ij W ij \u03b4 ij\nwhere \u03b4 ij \u2208 R d\u00d7d is the indicator matrix for entry (i, j), and s ij \u2208 {0, 1} are independent Bernoulli variable with probability p of being 1. Then we have that E[P \u2126 (W )] = pW and s ij W ij \u03b4 ij \u03bd d W F . Moreover, we compute the variance by Concentration Lemmas for Noise Matrix N . Next we will state the concentration lemmas that are necessary when observed matrix is perturbed by Gaussian noise. The proof of these Lemmas are really exactly the same (in fact even simpler) than the corresponding Theorem that we have just proven. The first Lemma is used in the same settings as Theorem D.1.\ni,j\u2208[d] 2 E[s ij W 2 ij \u03b4 ij \u03b4 ij ] = i,j\u2208[d] 2 E[s ij W 2 ij \u03b4 jj ] = j\u2208[d] p i\u2208d W 2 ij \u03b4 jj (D.2) Therefore i,j\u2208[d] 2 E[s ij W 2 ij \u03b4 ij \u03b4 ij ]\nLemma D.5. Let N be a random matrix with independent Gaussian entries N (0, \u03c3 2 ). With high probability over the support \u2126 and the Gaussian N , for any low rank matrix W , we have\n| P \u2126 (N ), P \u2126 (W ) | O(\u03c3|Z| \u221e dr log d + pd 2 r\u03c3 2 |W | \u221e W F log d Proof.\nThe proof is exactly the same as Theorem D.1 as | P \u2126 (N ), P \u2126 (W ) | is a sum of independent entries that follows from the same Bernstein's inequality.\nNext we show that random sampling entries of a Gaussian matrix gives a matrix with low spectral norm.\nLemma D.6. Let N be a random Gaussian matrix with independent Gaussian entries N (0, \u03c3 2 ), with high probability over the choice of \u2126 and N , we have 1 + \u2022 \u2022 \u2022 + a 6 r C 6 r, which implis that max a i Cr 1/6 . Proposition E.3. For any \u03b6 \u2208 (0, 1), there is a set \u0393 of rank r d \u00d7 d matrices, such that for any rank r d \u00d7 d matrix X with Frobenius norm at most 1, there is a matrixX \u2208 \u0393 with X \u2212X F \u03b6. The size of \u0393 is bounded by (d/\u03b6) O(dr) .\nProof. Standard construction of \u03b5-net shows that there is a set P \u2282 R d of size (d/\u03b5) O(d) such that for any u 1, there is a\u00fb \u2208 P such that u \u2212\u00fb \u03b5. Such construction can also be applied to matrices and Frobenius norm as that is the same as vectors and 2 norm.\nHere we let \u03b5 = 0.1\u03b6, and construct three sets P 1 , P 2 , P 3 where P 1 is an \u03b5-net for d \u00d7 r matrices with Frobenius norm at most \u221a r, P 2 is an \u03b5-net for r \u00d7 r diagonal matrices whose Frobenius norm is bounded by 1, and P 3 is an \u03b5-net for r \u00d7 d matrices with Frobenius norm at most \u221a r. Now we define \u0393 = {\u00dbDV |\u00db \u2208 P 1 ,D \u2208 P 2 ,V \u2208 P 3 }. Clearly the size of \u0393 is as promised. For any rank r d \u00d7 d matrix X, suppose its Singular Value Decomposition is U DV , we can find\u00db \u2208 P 1 ,D \u2208 P 2 andV \u2208 P 3 that are \u03b5 close to U, D, V respectively. Therefore\u00dbDV \u2208 \u0393 and it is easy to check U DV \u2212\u00dbDV F 8\u03b5 \u03b6.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "(as long as p > 1/d). Our goal now is to prove d i=1 Z i p 2 \u03b5 2 for all X.\nLet Z i be a truncated version of Z i . That is, Z i = Z i when Z i [2pd\u03bd 3 (1/d) 3/2 ] 2 , and Z i = [2pd\u03bd 3 (1/d) 3/2 ] 2 otherwise. It's not hard to see Z i has smaller mean and variance compared to Z i . Also, by vector's Bernstein's inequality (Lemma E.1), we know\nNotice that this is only relevant when t O(p\u03bd 3 d \u22121/2 ) (because otherwise the probability is 0) and in that regime the variance term always dominates. Therefore Z i is the square of a subgaussian random variable.\nBy the Bernstein's inequality, we know the moments of\nNow we can use the concentration bound for quadratics of the subgaussian random variables [HKZ12]: we know that with probability exp(\u2212t),\nthis means with probability exp(\u2212Cdrlogd) with some large constant C, we know\nThe probability is low enough for us to union bound over all X in a standard \u03b5-net such that every other X is within distance (\u03b5/d) 6 . Therefore we know with high probability for all X in the \u03b5-net we have\nwhich is smaller than p 2 \u03b5 2 when p C\u03bd 6 r log 1.5 d d\u03b5 2\nfor a large enough constant C. For anyX that is not in the \u03b5-net, let X be the closest point of X in the net, then X \u2212X F 1/d 6 , therefore the bound ofX clearly follows from the bound of X. Now to convert sum of Z i to sum of Z i , notice that with high probability there are at most 2pd entries in \u2126 for every row. When that happens Z i is always bounded by\nfor all X, and let event 2 be that there are at most 2pd entries per row, we know with high probability both event happens, and in that case\nHandling \u03b4 i,j = \u03b4 j,i . First notice that the diagonal entries \u03b4 i,i 's cannot change the Frobenius norm by more than\np\u03b5 so we can ignore the diagonal terms. Now for independent terms \u03b4 i,j , let \u03b3 j,i = \u03b4 i,j , then by union bound both \u03b4 i,j and \u03b3 i,j satisfy the equation, and by triangle's inequality (\u03b4 i,j + \u03b3 i,j )/2 also satisfies the inequality. Let \u03c4 i,j be the true indicator of \u2126 (hence \u03c4 i,j = \u03c4 j,i ), and \u03c4 i,j be an independent copy, we know (\u03c4 i,j + \u03c4 i,j )/2 has the same distribution as (\u03b4 i,j + \u03b3 i,j )/2 (for off-diagonal entries), therefore with high probability the equation is true for (\u03c4 i,j + \u03c4 i,j )/2. The Theorem then follows from the standard Claim below for decoupling (note that sup X F =1 P \u2126 (XX T )X \u2212 pXX T X F is a norm for the indicator variables of \u2126):\nClaim D.3. Let X, Y be two iid random variables, then\nProof. Let X, Y, Z be iid random variables then,", "publication_ref": ["b12"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Uncovering shared structures in multiclass classification", "journal": "ACM", "year": "2007", "authors": "Yonatan Amit; Michael Fink; Nathan Srebro; Shimon Ullman"}, {"ref_id": "b1", "title": "On the low-rank approach for semidefinite programs arising in synchronization and community detection", "journal": "", "year": "2016", "authors": "S Afonso; Nicolas Bandeira; Vladislav Boumal;  Voroninski"}, {"ref_id": "b2", "title": "Neural networks and principal component analysis: Learning from examples without local minima", "journal": "Neural networks", "year": "1989", "authors": "Pierre Baldi; Kurt Hornik"}, {"ref_id": "b3", "title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "journal": "Mathematical Programming", "year": "2003", "authors": "Samuel Burer; D C Renato;  Monteiro"}, {"ref_id": "b4", "title": "Global Optimality of Local Search for Low Rank Matrix Recovery", "journal": "", "year": "2016-05", "authors": "S Bhojanapalli; B Neyshabur; N Srebro"}, {"ref_id": "b5", "title": "Robust principal component analysis", "journal": "Journal of the ACM (JACM)", "year": "2011", "authors": "J Emmanuel; Xiaodong Cand\u00e8s; Yi Li; John Ma;  Wright"}, {"ref_id": "b6", "title": "Exact matrix completion via convex optimization", "journal": "Foundations of Computational mathematics", "year": "2009", "authors": "J Emmanuel; Benjamin Cand\u00e8s;  Recht"}, {"ref_id": "b7", "title": "The power of convex relaxation: Near-optimal matrix completion. Information Theory", "journal": "IEEE Transactions on", "year": "2010", "authors": "J Emmanuel; Terence Cand\u00e8s;  Tao"}, {"ref_id": "b8", "title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees", "journal": "", "year": "2015", "authors": "Yudong Chen; J Martin;  Wainwright"}, {"ref_id": "b9", "title": "Ewout van den Berg, and Mary Wootters. 1-bit matrix completion", "journal": "Information and Inference", "year": "2014", "authors": "A Mark; Yaniv Davenport;  Plan"}, {"ref_id": "b10", "title": "Escaping from saddle points-online stochastic gradient for tensor decomposition", "journal": "", "year": "2015", "authors": "Rong Ge; Furong Huang; Chi Jin; Yang Yuan"}, {"ref_id": "b11", "title": "Understanding alternating minimization for matrix completion", "journal": "IEEE", "year": "2014", "authors": "Moritz Hardt"}, {"ref_id": "b12", "title": "A tail inequality for quadratic forms of subgaussian random vectors", "journal": "Electron. Commun. Probab", "year": "2012", "authors": "Daniel Hsu; M Sham; Tong Kakade;  Zhang"}, {"ref_id": "b13", "title": "Matrix completion and low-rank svd via fast alternating least squares", "journal": "Journal of Machine Learning Research", "year": "2014", "authors": "Trevor Hastie; Rahul Mazumder; Jason ; Reza Zadeh"}, {"ref_id": "b14", "title": "Fast matrix completion without the condition number", "journal": "", "year": "2014", "authors": "Moritz Hardt; Mary Wootters"}, {"ref_id": "b15", "title": "Sums of random Hermitian matrices and an inequality by Rudelson", "journal": "", "year": "2010-04", "authors": "R Imbuzeiro Oliveira"}, {"ref_id": "b16", "title": "Fast exact matrix completion with finite samples", "journal": "", "year": "2015", "authors": "Prateek Jain; Praneeth Netrapalli"}, {"ref_id": "b17", "title": "Low-rank matrix completion using alternating minimization", "journal": "ACM", "year": "2013", "authors": "Prateek Jain; Praneeth Netrapalli; Sujay Sanghavi"}, {"ref_id": "b18", "title": "Matrix completion from a few entries. Information Theory", "journal": "IEEE Transactions on", "year": "2010", "authors": "Andrea Raghunandan H Keshavan; Sewoong Montanari;  Oh"}, {"ref_id": "b19", "title": "Matrix completion from noisy entries", "journal": "The Journal of Machine Learning Research", "year": "2010", "authors": "Andrea Raghunandan H Keshavan; Sewoong Montanari;  Oh"}, {"ref_id": "b20", "title": "The bellkor solution to the netflix grand prize. Netflix prize documentation", "journal": "", "year": "2009", "authors": "Yehuda Koren"}, {"ref_id": "b21", "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization", "journal": "", "year": "2016", "authors": "Yuanzhi Li; Yingyu Liang; Andrej Risteski"}, {"ref_id": "b22", "title": "Gradient descent converges to minimizers", "journal": "", "year": "2016", "authors": "Max Jason D Lee;  Simchowitz; Benjamin Michael I Jordan;  Recht"}, {"ref_id": "b23", "title": "Support recovery without incoherence: A case for nonconvex regularization", "journal": "", "year": "2014", "authors": "Ling Po; Martin J Loh;  Wainwright"}, {"ref_id": "b24", "title": "Regularized m-estimators with nonconvexity: statistical and algorithmic theory for local optima", "journal": "Journal of Machine Learning Research", "year": "2015", "authors": "Ling Po; Martin J Loh;  Wainwright"}, {"ref_id": "b25", "title": "Spectral regularization algorithms for learning large incomplete matrices", "journal": "Journal of machine learning research", "year": "2010-08", "authors": "Rahul Mazumder; Trevor Hastie; Robert Tibshirani"}, {"ref_id": "b26", "title": "Cubic regularization of Newton method and its global performance", "journal": "Mathematical Programming", "year": "2006", "authors": "Yurii Nesterov; T Boris;  Polyak"}, {"ref_id": "b27", "title": "Restricted strong convexity and weighted matrix completion: Optimal bounds with noise", "journal": "Journal of Machine Learning Research", "year": "2012-05", "authors": "Sahand Negahban; J Martin;  Wainwright"}, {"ref_id": "b28", "title": "Nonconvergence to unstable points in urn models and stochastic approximations. The Annals of Probability", "journal": "", "year": "1990", "authors": "Robin Pemantle"}, {"ref_id": "b29", "title": "A simpler approach to matrix completion", "journal": "The Journal of Machine Learning Research", "year": "2011", "authors": "Benjamin Recht"}, {"ref_id": "b30", "title": "Fast maximum margin matrix factorization for collaborative prediction", "journal": "ACM", "year": "2005", "authors": "D M Jasson; Nathan Rennie;  Srebro"}, {"ref_id": "b31", "title": "Weighted low-rank approximations", "journal": "", "year": "2013", "authors": "Nathan Srebro; Tommi Jaakkola"}, {"ref_id": "b32", "title": "Guaranteed matrix completion via nonconvex factorization", "journal": "IEEE", "year": "2015", "authors": "Ruoyu Sun; Zhi-Quan Luo"}, {"ref_id": "b33", "title": "When are nonconvex problems not scary?", "journal": "", "year": "2015", "authors": "Ju Sun; Qing Qu; John Wright"}, {"ref_id": "b34", "title": "Maximum-margin matrix factorization", "journal": "", "year": "2004", "authors": "Nathan Srebro; Rennie ; Tommi S Jaakkola"}, {"ref_id": "b35", "title": "Global convergence of stochastic gradient descent for some non-convex matrix problems", "journal": "", "year": "2015-06-11", "authors": "Christopher Christopher De Sa; Kunle R\u00e9;  Olukotun"}, {"ref_id": "b36", "title": "Rank, trace-norm and max-norm", "journal": "Springer", "year": "2005", "authors": "Nathan Srebro; Adi Shraibman"}, {"ref_id": "b37", "title": "Low-rank solutions of linear matrix equations via procrustes flow", "journal": "", "year": "2015", "authors": "Stephen Tu; Ross Boczar; Mahdi Soltanolkotabi; Benjamin Recht"}, {"ref_id": "b38", "title": "Convergence analysis for rectangular matrix completion using burer-monteiro factorization and gradient descent", "journal": "", "year": "2016", "authors": "Qinqing Zheng; John Lafferty"}, {"ref_id": "b39", "title": "A nonconvex optimization framework for low rank matrix estimation", "journal": "", "year": "2015", "authors": "Tuo Zhao; Zhaoran Wang; Han Liu"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Lemma 3. 1 (1Partial observation case, informally stated). Under the setting of this section, in the domain B, all local mimina of the functiong(\u2022) are O( \u221a \u03b5)-close to \u00b1z.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Lemma 4.3. In the setting of Theorem 4.2, suppose x satisfies the first-order and second-order optimality condition (4.2) and (4.3). Then when p is defined as in Theorem 4.2, xx \u2212 zz 2 F O(\u03b5) .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: Partition of R d into regions where our Lemmas apply. For example, Lemma 3.8 rules out the possibility that a point x in the green region is local minimum. Here, The green region is the intersection of \u221e norm ball and 2 norm ball. Both the white region and yellow region have non-zero gradient but for different reasons.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Lemma 4. 8 (8Extension of Lemma 4.4). In the setting of Theorem 4.2, with high probability over the choice of \u2126, suppose x \u2208 B satisfies second-order optimality condition (4.3) or \u03c4 -relaxed condition for \u03c4 0.1p, we have x 2 1/8.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Therefore we still have| P \u2126 (W ), P \u2126 (Z) \u2212 p W, Z | O(|W | \u221e |Z| \u221e dr log d + pdr|W | \u221e |Z| \u221e W F Z F log d).", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "p\u03bd 2 d2Similarly we can controli,j\u2208[d] 2 E[s ij W 2 ij \u03b4 ij \u03b4 ij ]by p\u03bd 2 /d (again notice that although \u03b4 i,j = \u03b4 j,i the bounds here are correct up to constant factors). Then it follows from non-commutative Bernstein inequality[Imb10] thatP \u2126 [ P \u2126 (W ) \u2212 p(W )\u03b5p] d exp(\u22122\u03b5 2 pd/\u03bd 2 ) .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "2P \u2126 (N ) \u03b5p\u03c3d, where \u03b5 = O( log d/pd). Proof. Again the proof follows from the same argument as Lemma D.4. E Auxiliary Lemmas Lemma E.1. [Bernstein inequality, c.f. [Imb10]] Let v i 's be independent random vectors andv = n i=1 v i . Suppose \u03c3 2 = E[ n i=1 v i 2] and for all i v i R with probability 1, thenP[ v > t] d exp(\u2212t 2 /(3\u03c3 2 + 3tR)).Proposition E.2. Let a 1 , . . . , a r 0, C 0. Then C 4 (a2 1 +\u2022 \u2022 \u2022+a 2 r ) a 6 1 +\u2022 \u2022 \u2022+a 6 r implies that a 2 1 +\u2022 \u2022 \u2022+a 2 rC 2 r and that max a i Cr 1/6 . Proof. By Cauchy-Schwarz inequality, we have, Using the assumption and equation above we have that a 2 1 + \u2022 \u2022 \u2022 + a 2 r C 2 r. This implies with the condition that a 6", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Z i \u00b5/ \u221a d \u2022 Z F .", "formula_coordinates": [2.0, 265.03, 286.73, 86.92, 18.63]}, {"formula_id": "formula_1", "formula_text": "\u2126 \u2282 [d] \u00d7 [d]", "formula_coordinates": [3.0, 87.82, 617.73, 52.65, 8.74]}, {"formula_id": "formula_2", "formula_text": "B = x : x \u221e < 2\u00b5 \u221a d . (3.1)", "formula_coordinates": [4.0, 251.79, 470.02, 288.21, 23.67]}, {"formula_id": "formula_3", "formula_text": "g(x) = 1 2 \u2022 P \u2126 (M \u2212 xx ) 2 F . (3.2)", "formula_coordinates": [4.0, 241.61, 519.96, 298.39, 22.31]}, {"formula_id": "formula_4", "formula_text": "\u2126 = [d] \u00d7 [d]. The corresponding objective is g(x) = 1 2 \u2022 M \u2212 xx 2 F . (3.3)", "formula_coordinates": [4.0, 251.77, 623.05, 288.23, 40.4]}, {"formula_id": "formula_5", "formula_text": "\u2207g(x) = M x \u2212 x 2 x, and \u2207 2 g(x) = 2xx \u2212 M + x 2 \u2022 I .", "formula_coordinates": [5.0, 176.96, 152.21, 258.09, 11.03]}, {"formula_id": "formula_6", "formula_text": "Claim 1f. Suppose x \u2208 B satisfies \u2207g(x) = 0, then x, z 2 = x 4 . Proof. We have, \u2207g(x) = (zz \u2212 xx )x = 0 \u21d2 x, \u2207g(x) = x, (zz \u2212 xx )x = 0 (3.5) \u21d2 x, z 2 = x 4", "formula_coordinates": [5.0, 72.0, 535.12, 229.02, 93.29]}, {"formula_id": "formula_7", "formula_text": "Generalization to Lemma 3.1. Claim 1p. Suppose x \u2208 B satisfies \u2207g(x) = 0, then x, z 2 = x 4 \u2212 \u03b5.", "formula_coordinates": [5.0, 310.98, 508.89, 229.02, 44.47]}, {"formula_id": "formula_8", "formula_text": "\u2207g(x) = P \u2126 (zz \u2212 xx )x = 0 \u21d2 x, \u2207g(x) = x, P \u2126 (zz \u2212 xx )x = 0 (3.6) \u21d2 x, z 2 x 4 \u2212 \u03b5", "formula_coordinates": [5.0, 324.39, 594.02, 215.61, 53.4]}, {"formula_id": "formula_9", "formula_text": "z, \u2207 2 g(x)z = zx + xz 2 \u2212 2z (zz \u2212 xx )z 0aaaaaa (3.7) \u21d2 x 2 + 2 z, x 2 1 \u21d2 x 2 1/3 (since z, x 2 x 2 ) aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa Claim 2p. If x \u2208 B has positive Hessian \u2207 2g (x) 0, then x 2 1/3 \u2212 \u03b5.", "formula_coordinates": [6.0, 72.0, 73.58, 469.74, 170.93]}, {"formula_id": "formula_10", "formula_text": "z, \u2207 2g (x)z = P \u2126 (zx + xz ) 2 \u2212 2z P \u2126 (zz \u2212 xx )z 0 (3.8) \u21d2 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 (same step as the left) \u21d2 x 2 1/3 \u2212 \u03b5", "formula_coordinates": [6.0, 320.77, 150.91, 219.23, 71.41]}, {"formula_id": "formula_11", "formula_text": "\u2207 2g (x)z \u2248 p z, \u2207 2 g(x)z for any x \u2208 B.", "formula_coordinates": [6.0, 310.98, 233.98, 224.71, 22.49]}, {"formula_id": "formula_12", "formula_text": "(x) = 0 implies z, \u2207g(x) = z, (zz \u2212 xx )x = 0 (3.9) \u21d2 x, z (1 \u2212 x 2 ) = 0 \u21d2 x 2 = 1 (by x, z 2 1/9)", "formula_coordinates": [6.0, 82.74, 311.89, 218.28, 62.99]}, {"formula_id": "formula_13", "formula_text": "(x) = 0 implies z, \u2207g(x) = z, P \u2126 (zz \u2212 xx )x = 0 (3.10) \u21d2 \u2022 \u2022 \u2022 \u2022 \u2022 \u2022 (same step as the left) \u21d2 x 2 = 1 \u00b1 O(\u03b5) (same step as the left)", "formula_coordinates": [6.0, 322.95, 311.89, 217.06, 62.38]}, {"formula_id": "formula_14", "formula_text": "f (x) = 1 2 P \u2126 (M \u2212 xx ) 2 F + \u03bbR(x) . (4.1)", "formula_coordinates": [6.0, 225.25, 628.93, 314.75, 22.31]}, {"formula_id": "formula_15", "formula_text": "R(x) = d i=1 h(x i ), and h(t) = (|t| \u2212 \u03b1) 4 I t \u03b1 .", "formula_coordinates": [7.0, 209.22, 95.68, 193.56, 30.32]}, {"formula_id": "formula_16", "formula_text": "2P \u2126 (M \u2212 xx )x = \u03bb\u2207R(x) , (4.2)", "formula_coordinates": [7.0, 243.52, 202.57, 296.48, 9.65]}, {"formula_id": "formula_17", "formula_text": "\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v . (4.3)", "formula_coordinates": [7.0, 157.97, 242.71, 382.03, 12.69]}, {"formula_id": "formula_18", "formula_text": "\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v \u2212 \u03c4 v 2 . (4.4)", "formula_coordinates": [7.0, 139.33, 284.92, 400.67, 12.69]}, {"formula_id": "formula_19", "formula_text": "c\u00b5 6 log 1.5 d d", "formula_coordinates": [7.0, 177.91, 325.2, 39.03, 15.63]}, {"formula_id": "formula_20", "formula_text": "B = x : x \u221e 2\u00b5 \u221a d , x 1 . (4.5)", "formula_coordinates": [7.0, 232.62, 607.89, 307.38, 23.67]}, {"formula_id": "formula_21", "formula_text": "x 2 1/4.", "formula_coordinates": [8.0, 285.42, 106.96, 46.14, 10.81]}, {"formula_id": "formula_22", "formula_text": "P \u2126 (zx + xz ) 2 F 2z P \u2126 (M \u2212 xx )z .", "formula_coordinates": [8.0, 219.31, 170.53, 180.58, 16.59]}, {"formula_id": "formula_23", "formula_text": "LHS of (4.6) = p zx + xz 2 F \u00b1 O( pd x 2 \u221e z 2 \u221e x 2 z 2 ) = 2p x 2 z 2 + 2p x, z 2 \u00b1 O(p\u03b5) , (Since x, z \u2208 B)", "formula_coordinates": [8.0, 170.42, 303.74, 369.58, 29.99]}, {"formula_id": "formula_24", "formula_text": "(Since M = zz ) = 2p z 4 \u2212 2p x, z 2 \u00b1 O(p\u03b5)", "formula_coordinates": [8.0, 190.75, 377.56, 349.25, 24.92]}, {"formula_id": "formula_25", "formula_text": "2p x 2 z 2 + 2p x, z 2 \u00b1 O(p\u03b5) 2 z 4 \u2212 2 x, z 2 \u00b1 O(p\u03b5) , which implies that 6p x 2 z 2 2p x 2 z 2 + 4p x, z 2 2p z 4 \u2212 O(p\u03b5)", "formula_coordinates": [8.0, 71.64, 459.41, 364.8, 32.95]}, {"formula_id": "formula_26", "formula_text": "x z \u2212 x 2 x O(\u03b5) . (4.7)", "formula_coordinates": [8.0, 268.21, 580.96, 271.79, 11.03]}, {"formula_id": "formula_27", "formula_text": "P \u2126 (M \u2212 xx )x = P \u2126 (zz )x \u2212 P \u2126 (xx )x = 0 . (4.8)", "formula_coordinates": [8.0, 204.88, 644.51, 335.12, 9.65]}, {"formula_id": "formula_28", "formula_text": "P \u2126 (xx )x \u2212 pxx x F p\u03b5 x 3 p\u03b5 (4.9)", "formula_coordinates": [9.0, 227.12, 93.84, 312.88, 11.72]}, {"formula_id": "formula_29", "formula_text": "P \u2126 (zz ) \u2212 pzz \u03b5p .", "formula_coordinates": [9.0, 257.52, 137.41, 104.16, 9.65]}, {"formula_id": "formula_30", "formula_text": "xx \u2212 zz 2 F O(\u03b4) .", "formula_coordinates": [9.0, 260.87, 259.86, 97.46, 16.59]}, {"formula_id": "formula_31", "formula_text": "x z = u 2 x 2 x+u x 2 v, therefore \u03b4 z, x z \u2212 x 2 x = x 2 u 2 v 2 + (1 \u2212 u 2 ) 2 .", "formula_coordinates": [9.0, 72.0, 284.63, 469.24, 34.22]}, {"formula_id": "formula_32", "formula_text": "xx T \u2212 zz T : xx T \u2212 zz T = (1 \u2212 u 2 )xx T + uxv T + uvx T + vv T It is clear that all the terms have norm bounded by O(\u03b4), therefore xx \u2212 zz 2 F O(\u03b4).", "formula_coordinates": [9.0, 72.0, 337.73, 382.32, 44.94]}, {"formula_id": "formula_33", "formula_text": "|(P \u2126 (M )x) i | = P \u2126 (zz )x i = j\u2208S i z i z j x j |x i | j\u2208S i |z i z j | |x i | \u2022 \u00b5 2 /d \u2022 |S i | 2|x i |p\u00b5 2", "formula_coordinates": [9.0, 166.39, 644.12, 278.73, 50.95]}, {"formula_id": "formula_34", "formula_text": "(P \u2126 (xx )x) i = j\u2208S i x i x 2 j 0 ,", "formula_coordinates": [10.0, 235.19, 385.52, 141.62, 23.13]}, {"formula_id": "formula_35", "formula_text": "(\u03bb\u2207R(x)) i = 4\u03bb(|x i | \u2212 \u03b1) 3 sign(x i ) \u03bb 2 |x i | 3 (Since x i 2\u03b1)", "formula_coordinates": [10.0, 167.2, 433.75, 372.8, 22.31]}, {"formula_id": "formula_36", "formula_text": "4|x i |p\u00b5 2 2(P \u2126 (M \u2212 xx )x) i (\u03bb\u2207R(x)) i \u03bb 2 |x i | 3 , which implies that |x i | 4 p\u00b5 2 /\u03bb.", "formula_coordinates": [10.0, 71.64, 481.3, 360.63, 41.56]}, {"formula_id": "formula_37", "formula_text": "B = x \u2208 R d : x \u221e 4\u03b1 .", "formula_coordinates": [10.0, 242.29, 555.98, 127.42, 11.72]}, {"formula_id": "formula_38", "formula_text": "z, x z \u2212 x 2 x \u2212 \u03b3 \u2022 \u2207R(x) O(\u03b5) . (4.13)", "formula_coordinates": [11.0, 231.47, 106.6, 308.53, 11.03]}, {"formula_id": "formula_39", "formula_text": "x, z z \u2212 x 2 x O(\u03b5)", "formula_coordinates": [11.0, 260.92, 202.63, 101.22, 10.81]}, {"formula_id": "formula_40", "formula_text": "[d] \u00d7 [d] for \u2126 everywhere.", "formula_coordinates": [11.0, 72.0, 317.95, 468.0, 20.91]}, {"formula_id": "formula_41", "formula_text": "f (X) = 1 2 P \u2126 (M \u2212 XX ) 2 F + \u03bbR(X) , (5.1)", "formula_coordinates": [11.0, 217.18, 372.99, 322.82, 22.31]}, {"formula_id": "formula_42", "formula_text": "\u2200V \u2208 R d\u00d7r , P \u2126 (V X + XV ) 2 F + \u03bb V , \u2207 2 R(X)V 2 P \u2126 (M \u2212 XX ), V V . (5.3)", "formula_coordinates": [11.0, 122.43, 525.1, 417.57, 12.69]}, {"formula_id": "formula_43", "formula_text": "\u0393 ii = 4( Xi \u2212\u03b1) 4 Xi I Xi \u03b1 . As a direct consequence, (\u2207R(X)) i , X i 0 for every i \u2208 [d].", "formula_coordinates": [11.0, 71.39, 578.15, 470.35, 27.72]}, {"formula_id": "formula_44", "formula_text": "XX \u2212 M 2 F O(\u03b4/p).", "formula_coordinates": [11.0, 276.33, 681.73, 105.8, 12.48]}, {"formula_id": "formula_45", "formula_text": "X 2\u2192\u221e = max i X i", "formula_coordinates": [12.0, 213.4, 142.27, 83.91, 14.43]}, {"formula_id": "formula_46", "formula_text": "P \u2126 (ZZ ) i 1 = j\u2208S i | Z i , Z j | j\u2208S i Z i Z j j\u2208S i \u00b5 2 r/d|S 1 | (by incoherence of Z) 2\u00b5 2 rp . (by |S i | 2pd)", "formula_coordinates": [12.0, 156.22, 254.01, 383.78, 65.26]}, {"formula_id": "formula_47", "formula_text": "X i P \u2126 (ZZ ) i 1 X 1\u21922 2\u00b5 2 rp X 2\u2192\u221e (by X 2\u2192\u221e = X 1\u21922 ) = 2\u00b5 2 rp X i (5.6)", "formula_coordinates": [12.0, 223.74, 351.64, 316.26, 43.3]}, {"formula_id": "formula_48", "formula_text": "(P \u2126 (XX )X) i = j\u2208S i X i , X j X j = X i j\u2208X i X j X j ,", "formula_coordinates": [12.0, 185.75, 428.97, 240.51, 21.06]}, {"formula_id": "formula_49", "formula_text": "(P \u2126 (XX )X) i , X i = X i \uf8eb \uf8ed j\u2208X i X j X j \uf8f6 \uf8f8 X i 0 .", "formula_coordinates": [12.0, 190.71, 470.06, 234.45, 34.68]}, {"formula_id": "formula_50", "formula_text": "(P \u2126 (XX )X) i , (\u2207R(X)) i = \u0393 ii X i \uf8eb \uf8ed j\u2208X i X j X j \uf8f6 \uf8f8 X i 0 .", "formula_coordinates": [12.0, 168.42, 531.36, 279.04, 34.68]}, {"formula_id": "formula_51", "formula_text": "(P \u2126 (XX )X) i + (\u03bb\u2207R(X)) i (\u03bb\u2207R(X)) i (by equation (5.8)) = 4\u03bb( X i \u2212 \u03b1) 3 X i \u2022 X i (by Proposition 5.2) \u03bb 2 X i 3 (by the assumptino X i 2\u03b1)", "formula_coordinates": [12.0, 140.41, 598.28, 399.59, 64.96]}, {"formula_id": "formula_52", "formula_text": "2 F ZZ 2 X 2 F \u03c3 max (Z) 4 X 2 F = \u03c3 max (Z) 4 (\u03c3 2 1 + \u2022 \u2022 \u2022 + \u03c3 2 r ).", "formula_coordinates": [13.0, 72.0, 382.88, 468.0, 30.07]}, {"formula_id": "formula_53", "formula_text": "F = \u03c3 6 1 + \u2022 \u2022 \u2022 + \u03c3 6 r .", "formula_coordinates": [13.0, 304.0, 400.75, 79.37, 13.72]}, {"formula_id": "formula_54", "formula_text": "(1 + O(\u03b4))\u03c3 max (Z) 4 r i=1 \u03c3 2 i r i=1 \u03c3 6 i", "formula_coordinates": [13.0, 228.98, 421.32, 153.55, 30.32]}, {"formula_id": "formula_55", "formula_text": "\u03c3 min (X) 1 4 \u03c3 min (Z) (5.13)", "formula_coordinates": [13.0, 260.26, 565.36, 279.74, 22.31]}, {"formula_id": "formula_56", "formula_text": "1 2 \u03c3 min (Z). Let L = [d] \u2212 J.", "formula_coordinates": [13.0, 201.94, 619.39, 116.77, 13.47]}, {"formula_id": "formula_57", "formula_text": "\u03c3 min (Z J ) \u03c3 min (Z) \u2212 \u03c3 max (Z L ) \u03c3 min (Z) \u2212 Z L F \u03c3 min (Z) \u2212 |L|r\u00b5 2 /d \u03c3 min (Z) \u2212 2r 2 \u03c3 max (Z) 2 \u00b5 2 /(\u03b1 2 d) 1 2 \u03c3 min (Z) . (by \u03b1 r\u03ba\u00b5 \u221a d )", "formula_coordinates": [13.0, 150.25, 655.15, 311.5, 27.02]}, {"formula_id": "formula_58", "formula_text": "z J \u221e Z J 2\u2192\u221e \u03b2 O(\u00b5 r/d/\u03c3 min (Z)", "formula_coordinates": [14.0, 76.98, 141.8, 190.37, 9.65]}, {"formula_id": "formula_59", "formula_text": "P \u2126 (wz J + z J w ) 2 F 2 P \u2126 (ZZ \u2212 XX ), z J z J Note that we have that w \u221e X 2\u2192\u221e v \u00b5 r/d. Recalling that z J \u221e O(\u00b5\u03ba r/d), by Theorem D.1, we have that p wz J + z J w 2 F 2p ZZ \u2212 XX , z J z J \u2212 \u03b4p where \u03b4 = O(\u00b5 2 \u03bar 2 (pd) \u22121/2 ).", "formula_coordinates": [14.0, 71.64, 196.76, 469.6, 80.49]}, {"formula_id": "formula_60", "formula_text": "w, z J 2 + w 2 z J 2 + X z J 2 Z z J 2 \u2212 \u03b4/2 (5.14)", "formula_coordinates": [14.0, 197.6, 287.58, 342.4, 11.72]}, {"formula_id": "formula_61", "formula_text": "w, z J 2 + w 2 z J 2 + X z J 2 y, v 2 + w 2 + y 2 2 y 2 + \u03c3 2 min (X)", "formula_coordinates": [14.0, 106.17, 354.21, 247.29, 28.87]}, {"formula_id": "formula_62", "formula_text": "ZZ T X \u2212 XX T X 2 F ZZ T X \u2212 XX T X \u2212 \u03b3\u2207R(X) 2 F (5.16) Proof. Let L = {i : X i \u03b1}.", "formula_coordinates": [14.0, 72.0, 524.06, 468.0, 35.39]}, {"formula_id": "formula_63", "formula_text": "i \u2208 L, Z i Z X \u2212 X i X X 2 Z i Z X \u2212 X i X X \u2212 (\u03b3\u2207R(X)) i 2", "formula_coordinates": [14.0, 72.0, 561.75, 369.29, 23.04]}, {"formula_id": "formula_64", "formula_text": ")) i = \u0393 ii X i for \u0393 ii 0. Then (\u2207R(X)) i , X i X X = \u0393 ii X i , X i X X = \u0393 ii X i X XX i \u0393 ii X i 2 \u03c3 min (X T X) 1 16 \u0393 ii X i 2 \u03c3 min (Z) 2", "formula_coordinates": [14.0, 181.94, 622.29, 247.31, 74.74]}, {"formula_id": "formula_65", "formula_text": "(\u2207R(X)) i , Z i Z X = \u0393 ii X i , Z i Z X \u0393 ii X i Z i \u03c3 max (Z T X) \u0393 ii X i Z i \u03c3 max (Z)\u03c3 max (X) 2\u0393 ii X i Z i \u03c3 max (Z) 2 r 1/6", "formula_coordinates": [15.0, 103.61, 97.08, 345.3, 43.02]}, {"formula_id": "formula_66", "formula_text": "\u0393 ii X i 2 \u03c3 min (Z) 2 r \u22121/3 (because X i \u03b1 32\u03ba\u00b5r/ \u221a d 32 \u221a r Z i )", "formula_coordinates": [15.0, 214.74, 142.68, 325.26, 18.14]}, {"formula_id": "formula_67", "formula_text": "ZZ T X \u2212 XX T X 2 F \u03b4 2", "formula_coordinates": [15.0, 254.29, 235.6, 110.12, 16.52]}, {"formula_id": "formula_68", "formula_text": "XX \u2212 ZZ 2 F O(\u03b4\u03ba 2 /\u03c3 min (Z)).", "formula_coordinates": [15.0, 229.36, 282.87, 158.27, 12.69]}, {"formula_id": "formula_69", "formula_text": "ZZ T X = (U + V )(U + V ) T X = U U T X + V U T X.", "formula_coordinates": [15.0, 195.7, 376.35, 220.6, 10.81]}, {"formula_id": "formula_70", "formula_text": "ZZ T X \u2212 XX T X 2 F = U U T X \u2212 XX T X 2 F + V U T X 2 F \u03b4 2 .", "formula_coordinates": [15.0, 168.83, 432.14, 279.33, 12.69]}, {"formula_id": "formula_71", "formula_text": "U T X) \u03c3 min (XX T X) \u2212 \u03b4 \u03c3 min (Z) 3 /128 if \u03b4 \u03c3 min (Z) 3 /128. Therefore \u03c3 min (U T X) is at least \u03c3 min (Z) 3 / Z 128. Now V 2 F \u03b4 2 /\u03c3 min (U T X) 2 O(\u03b4 2 Z 2 /\u03c3 min (Z) 6 ). Finally, we can bound U V T F by U V F Z V F (last inequality is because U is a projection of Z), which at least \u2126( V 2 F ) when \u03b4 \u03c3 min (Z) 3 /128, therefore ZZ T \u2212 XX T F U U T \u2212 XX T F + 2 U V T F + V V T F O(\u03b4 Z 2 /\u03c3 min (Z) 3 ).", "formula_coordinates": [15.0, 71.64, 466.51, 469.6, 69.01]}, {"formula_id": "formula_72", "formula_text": "2P \u2126 (M \u2212 xx )x = \u03bb\u2207R(x) ,", "formula_coordinates": [19.0, 243.52, 139.27, 124.95, 9.65]}, {"formula_id": "formula_73", "formula_text": "\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v .", "formula_coordinates": [19.0, 157.97, 181.03, 296.07, 12.69]}, {"formula_id": "formula_74", "formula_text": "\u2200v \u2208 R d , P \u2126 (vx + xv ) 2 F + \u03bbv \u2207 2 R(x)v 2v P \u2126 (M \u2212 xx )v \u2212 \u03c4 v 2 .", "formula_coordinates": [19.0, 139.33, 224.87, 333.34, 12.69]}, {"formula_id": "formula_75", "formula_text": "f (x + \u03b4) = 1 2 P \u2126 (M \u2212 (x + \u03b4)(x + \u03b4) ) 2 F + \u03bbR(x + \u03b4) + o( \u03b4 2 ) = 1 2 P \u2126 (M \u2212 xx \u2212 (x\u03b4 + \u03b4x ) \u2212 \u03b4\u03b4 ) 2 F + \u03bb R(x) + \u2207R(x), \u03b4 + 1 2 \u03b4 T \u2207 2 R(x)\u03b4 + o( \u03b4 2 ) = 1 2 M \u2212 xx 2 \u2126 + \u03bbR(x) \u2212 P \u2126 (M \u2212 xx ), x\u03b4 + \u03b4x + \u2207R(x), \u03b4 + o( \u03b4 2 ) \u2212 P \u2126 (M \u2212 xx ), \u03b4\u03b4 + 1 2 P \u2126 (x\u03b4 + \u03b4x ) 2 F + 1 2 \u03bb\u03b4 \u2207 2 R(x)\u03b4 + o( \u03b4 2 ). By symmetry P \u2126 (M \u2212 xx ), x\u03b4 = P \u2126 (M \u2212 xx ), \u03b4x = P \u2126 (M \u2212 xx )x, \u03b4 , so the first order optimal- ity condition is \u2200\u03b4, \u22122P \u2126 (M \u2212 xx )x + \u03bb\u2207R(x), \u03b4 = 0, which is equivalent to that 2P \u2126 (M \u2212 xx )x = \u03bb\u2207R(x).", "formula_coordinates": [19.0, 72.0, 268.39, 469.74, 146.98]}, {"formula_id": "formula_76", "formula_text": "P \u2126 (z J x + xz J ) 2 F 2z J P \u2126 (M \u2212 xx )z J . (A.1)", "formula_coordinates": [19.0, 214.18, 544.92, 325.82, 16.6]}, {"formula_id": "formula_77", "formula_text": "z J \u00b5 \u221a d an, z J 1.", "formula_coordinates": [19.0, 76.98, 580.85, 99.56, 15.3]}, {"formula_id": "formula_78", "formula_text": "LHS of (A.1) = p z J x + xz J 2 F \u00b1 O(p\u03b5) = 2p x 2 z J 2 + 2p x, z J 2 \u00b1 O(p\u03b5) .", "formula_coordinates": [19.0, 133.29, 618.82, 345.42, 16.59]}, {"formula_id": "formula_79", "formula_text": "(Since M = zz ) = 2 z J 4 \u2212 2 x, z J 2 \u00b1 O(p\u03b5) .", "formula_coordinates": [19.0, 186.73, 657.14, 353.27, 25.83]}, {"formula_id": "formula_80", "formula_text": "p x 2 z J 2 + 2p x, z J 2 p z J 4 \u00b1 O(p\u03b5) ,", "formula_coordinates": [20.0, 211.36, 106.96, 189.29, 11.72]}, {"formula_id": "formula_81", "formula_text": "J 2 x 2 1 3 z J 4 \u2212 O(\u03b5). Finally, we claim that z J 2 1/2, which completes the proof since x 2 1 3 z J 2 \u2212 O(\u03b5) 1/8. Claim A.3. Suppose \u03b1 4\u00b5 \u221a d and x satisfies x \u221e 4\u03b1 and x 2. Let J = {i : |x i | \u03b1}. Then we have that z J 1/2.", "formula_coordinates": [20.0, 71.67, 129.07, 468.35, 53.74]}, {"formula_id": "formula_82", "formula_text": "2 = z 2 \u2212 z L 2 (1 \u2212 2\u00b5 2 /(d\u03b1 2 )) 1 2 because \u03b1 2\u00b5 \u221a d .", "formula_coordinates": [20.0, 240.81, 197.65, 248.33, 15.3]}, {"formula_id": "formula_83", "formula_text": "x, z z \u2212 x 2 x O(\u03b5)", "formula_coordinates": [20.0, 260.92, 244.38, 101.22, 10.81]}, {"formula_id": "formula_84", "formula_text": "(z i z x \u2212 x i x ) 2 (z i z x \u2212 x i x \u2212 (\u03b3\u2207R(x)) i ) 2", "formula_coordinates": [20.0, 195.69, 288.22, 220.13, 11.72]}, {"formula_id": "formula_85", "formula_text": "(\u2207R(x)) i (x i x 2 \u2212 z i z, x ) 0 (A.2)", "formula_coordinates": [20.0, 236.67, 318.11, 303.34, 11.72]}, {"formula_id": "formula_86", "formula_text": "(\u2207R(x)) i \u2022 x i x 2 = \u03b3 i x i , x i x 2 \u03b3 i x 2 i x 2 1 \u221a 8 \u03b3 i x 2 i x (since x 2 1/8)", "formula_coordinates": [20.0, 231.57, 357.96, 308.43, 55.66]}, {"formula_id": "formula_87", "formula_text": "(\u2207R(x)) i \u2022 z i z, x = \u03b3 i x i z i z, x 1 4 \u03b3 i x 2 i x z (by |x i | \u03b1 4|z i |)", "formula_coordinates": [20.0, 189.2, 445.88, 350.8, 36.3]}, {"formula_id": "formula_88", "formula_text": "Theorem B.1. Let\u03bc = max{\u00b5, 4\u03c3d \u221a log d r }. Suppose p C\u03bc 6 \u03ba 12 r 4 d \u22121 \u03b5 \u22122 log 1.5 d where C is a large enough constant. Let \u03b1 = 2\u03bc\u03bar/ \u221a d, \u03bb \u03bc 2 rp/\u03b1 2 .", "formula_coordinates": [20.0, 71.67, 597.65, 468.33, 31.74]}, {"formula_id": "formula_89", "formula_text": "X of f (\u2022) satisfies XX T \u2212 ZZ T F \u03b5.", "formula_coordinates": [20.0, 72.0, 632.6, 281.55, 21.61]}, {"formula_id": "formula_90", "formula_text": "B \u03b5 = X \u2208 R d\u00d7r : XX T \u2212 ZZ T F \u03b5, X 2\u2192\u221e 16\u00b5\u03bar \u221a d , (C.1)", "formula_coordinates": [21.0, 168.56, 323.15, 371.45, 23.67]}, {"formula_id": "formula_91", "formula_text": "X \u2212 U p 4 M \u2212 XX T 2 F .", "formula_coordinates": [21.0, 265.53, 373.88, 120.29, 22.31]}, {"formula_id": "formula_92", "formula_text": "p 4 P \u2126 (M \u2212 XX T ) 2", "formula_coordinates": [21.0, 225.6, 482.53, 86.84, 14.0]}, {"formula_id": "formula_93", "formula_text": "Claim C.2. Suppose U, Z \u2208 R d\u00d7r satisfy U U = ZZ . Then, for any i \u2208 [d] we have U i = Z i . Consequently, U F = Z F .", "formula_coordinates": [21.0, 71.67, 511.25, 469.57, 23.18]}, {"formula_id": "formula_94", "formula_text": "U i = Z i R.", "formula_coordinates": [21.0, 284.04, 568.61, 43.93, 9.65]}, {"formula_id": "formula_95", "formula_text": "X i 2\u00b5 \u221a r/ \u221a d.", "formula_coordinates": [22.0, 241.56, 91.9, 74.85, 18.14]}, {"formula_id": "formula_96", "formula_text": "X i 2\u00b5 \u221a r/ \u221a d 2 Z i = 2 U i (", "formula_coordinates": [22.0, 179.29, 115.81, 157.12, 18.14]}, {"formula_id": "formula_97", "formula_text": "X i , X i \u2212 U i X i 2 \u2212 X i U i X i 2 /2 > 0.", "formula_coordinates": [22.0, 199.39, 146.13, 217.1, 11.72]}, {"formula_id": "formula_98", "formula_text": "X i \u2212 U i = \u03b3 X i , X i \u2212 U i > 0.", "formula_coordinates": [22.0, 192.75, 166.14, 135.1, 9.65]}, {"formula_id": "formula_99", "formula_text": "= V S = V (V T (M ) \u22121 V ) \u22121/2 (where S := (V T (M ) \u22121 V ) \u22121/2", "formula_coordinates": [22.0, 212.19, 364.15, 275.2, 10.53]}, {"formula_id": "formula_100", "formula_text": "F P \u2212 V F + W F . Since XX T \u2212 M F \u03b5, we know M \u2212 V V T 2 F + 2 V W T 2 F \u03b5 2 .", "formula_coordinates": [22.0, 86.94, 377.68, 293.75, 22.86]}, {"formula_id": "formula_101", "formula_text": "P \u2212 V 2 F . Since M \u2212 V V T F \u03b5 \u03c3 min (Z) 2 /100, we know (1 \u2212 2\u03b5/\u03c3 min (Z) 2 )V V T M", "formula_coordinates": [22.0, 72.0, 435.88, 455.91, 22.27]}, {"formula_id": "formula_102", "formula_text": "V F = V F S \u2212 I 3\u03b5 Z F /\u03c3 min (Z) 2 . Using the fact that Z F = 1 we know U \u2212 X F P \u2212 V F + W F 5\u03b5 \u221a r/\u03c3 min (Z) 2 .", "formula_coordinates": [22.0, 76.98, 471.75, 463.37, 23.18]}, {"formula_id": "formula_103", "formula_text": "P \u2126 ((X \u2212 U )(X \u2212 U ) T ) 2 F p 25 M \u2212 XX T 2 F .", "formula_coordinates": [22.0, 206.4, 566.34, 204.17, 22.31]}, {"formula_id": "formula_104", "formula_text": "(X \u2212 U )(X \u2212 U ) T F 25 M \u2212 XX T 2 F r/\u03c3 min (Z) 4", "formula_coordinates": [22.0, 247.9, 597.68, 228.68, 12.48]}, {"formula_id": "formula_105", "formula_text": "P \u2126 (a) 2 F 5p 6 a 2 F .", "formula_coordinates": [23.0, 264.75, 106.55, 87.49, 22.31]}, {"formula_id": "formula_106", "formula_text": "X \u2212 U p 4 M \u2212 XX T 2 F . Define a = U (X \u2212 U ) T + (X \u2212 U )U T , b = (U \u2212 X)(U \u2212 X) T , then XX T \u2212 M = a + b and (X \u2212 U )X T + X(X \u2212 U ) T = a + 2b. Now \u2207h(X), X \u2212 U = 2 P \u2126 (XX T \u2212 M )X, X \u2212 U = P \u2126 (XX T \u2212 M ), (X \u2212 U )X T + X(X \u2212 U ) T = P \u2126 (a + b), P \u2126 (a + 2b) = P \u2126 (a) 2 F + 2 P \u2126 (b) 2 F + 3 P \u2126 (a), P \u2126 (b) P \u2126 (a) 2 F + 2 P \u2126 (b) 2 F \u2212 3 P \u2126 (a) P \u2126 (b) . Let \u03b5 = M \u2212 XX T F .", "formula_coordinates": [23.0, 72.0, 216.76, 469.93, 156.52]}, {"formula_id": "formula_107", "formula_text": "P \u2126 (A) 2 F 5p 6 a 2 F 27 40 p\u03b5 2 .", "formula_coordinates": [23.0, 243.76, 438.79, 129.46, 22.31]}, {"formula_id": "formula_108", "formula_text": "\u2207f (X), X \u2212 U p 4 M \u2212 XX T 2 F .", "formula_coordinates": [23.0, 230.06, 497.41, 155.75, 22.31]}, {"formula_id": "formula_109", "formula_text": "| P \u2126 (W ), P \u2126 (Z) \u2212 p W, Z | O(|W | \u221e |Z| \u221e dr log d + pdr|W | \u221e |Z| \u221e W F Z F log d)", "formula_coordinates": [23.0, 112.47, 653.95, 388.72, 11.14]}, {"formula_id": "formula_110", "formula_text": "P \u2126 (W, Z = i,j \u03b4 i,j W i,j Z i,j ,", "formula_coordinates": [24.0, 245.96, 122.05, 122.29, 19.91]}, {"formula_id": "formula_111", "formula_text": "V[Q] p i,j (W i,j Z i,j ) 2 p max i,j |W i,j | 2 i,j Z 2 i,j p|W | 2 \u221e .", "formula_coordinates": [24.0, 244.27, 197.77, 122.55, 67.94]}, {"formula_id": "formula_112", "formula_text": "|Q \u2212 E[Q]| |W | \u221e |Z| \u221e log 1/\u03b7 + p|W | \u221e |Z| \u221e log(1/\u03b7).", "formula_coordinates": [24.0, 180.82, 311.59, 250.36, 9.65]}, {"formula_id": "formula_113", "formula_text": "|Q \u2212 E[Q]| O(|W | \u221e |Z| \u221e dr log d + pdr|W | \u221e |Z| \u221e log d).", "formula_coordinates": [24.0, 176.18, 375.33, 259.64, 9.65]}, {"formula_id": "formula_114", "formula_text": ")| O(1/d 3 ) |W | \u221e |Z| \u221e dr log d.", "formula_coordinates": [24.0, 240.7, 407.37, 150.74, 11.23]}, {"formula_id": "formula_115", "formula_text": "F (D.1)", "formula_coordinates": [24.0, 384.32, 538.31, 155.68, 10.3]}, {"formula_id": "formula_116", "formula_text": "P \u2126 (W ) = i,j\u2208[d] 2 s ij W ij \u03b4 ij", "formula_coordinates": [26.0, 249.79, 205.92, 111.52, 20.53]}, {"formula_id": "formula_117", "formula_text": "i,j\u2208[d] 2 E[s ij W 2 ij \u03b4 ij \u03b4 ij ] = i,j\u2208[d] 2 E[s ij W 2 ij \u03b4 jj ] = j\u2208[d] p i\u2208d W 2 ij \u03b4 jj (D.2) Therefore i,j\u2208[d] 2 E[s ij W 2 ij \u03b4 ij \u03b4 ij ]", "formula_coordinates": [26.0, 71.69, 276.07, 468.31, 109.9]}, {"formula_id": "formula_118", "formula_text": "| P \u2126 (N ), P \u2126 (W ) | O(\u03c3|Z| \u221e dr log d + pd 2 r\u03c3 2 |W | \u221e W F log d Proof.", "formula_coordinates": [26.0, 72.0, 559.32, 377.89, 27.85]}], "doi": ""}