{"title": "Expected Eligibility Traces", "authors": "Hado Van Hasselt; Sephora Madjiheurem; Matteo Hessel; David Silver; Andr\u00e9 Barreto; Diana Borsa;  Deepmind", "pub_date": "2021-02-08", "abstract": "The question of how to determine which states and actions are responsible for a certain outcome is known as the credit assignment problem and remains a central research question in reinforcement learning and artificial intelligence. Eligibility traces enable efficient credit assignment to the recent sequence of states and actions experienced by the agent, but not to counterfactual sequences that could also have led to the current state. In this work, we introduce expected eligibility traces. Expected traces allow, with a single update, to update states and actions that could have preceded the current state, even if they did not do so on this occasion. We discuss when expected traces provide benefits over classic (instantaneous) traces in temporal-difference learning, and show that sometimes substantial improvements can be attained. We provide a way to smoothly interpolate between instantaneous and expected traces by a mechanism similar to bootstrapping, which ensures that the resulting algorithm is a strict generalisation of TD(\u03bb). Finally, we discuss possible extensions and connections to related ideas, such as successor features.", "sections": [{"heading": "", "text": "Appropriate credit assignment has long been a major research topic in artificial intelligence (Minsky 1963). To make effective decisions and understand the world, we need to accurately associate events, like rewards or penalties, to relevant earlier decisions or situations. This is important both for learning accurate predictions, and for making good decisions.\nTemporal credit assignment can be achieved with repeated temporal-difference (TD) updates (Sutton 1988). One-step TD updates propagate information slowly: when a surprising value is observed, the state immediately preceding it is updated, but no earlier states or decisions are updated. Multistep updates (Sutton 1988;Sutton and Barto 2018) propagate information faster over longer temporal spans, speeding up credit assignment and learning. Multi-step updates can be implemented online using eligibility traces (Sutton 1988), without incurring significant additional computational expense, even if the time spans are long; these algorithms have computation that is independent of the temporal span of the prediction (van Hasselt and Sutton 2015).\nTraces provide temporal credit assignment, but do not assign credit counterfactually to states or actions that could have led to the current state, but did not do so this time.\nCopyright \u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.", "publication_ref": ["b21", "b37", "b37", "b39", "b37", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "MDP", "text": "True value TD(0) TD( ) ET( ) Figure 1: A comparison of TD(0), TD(\u03bb), and the new expected-trace algorithm ET(\u03bb) (with \u03bb = 0.9). The MDP is illustrated on the left. Each episode, the agent moves randomly down and right from the top left to the bottom right, where any action terminates the episode. Reward on termination are +1 with probability 0.2, and zero otherwise-all other rewards are zero. We plot the value estimates after the first positive reward, which occurred in episode 7. We see a) TD(0) only updated the last state, b) TD(\u03bb) updated the trajectory in this episode, and c) ET(\u03bb) additionally updated trajectories from earlier (unrewarding) episodes.\nCredit will eventually trickle backwards over the course of multiple visits, but this can take many iterations. As an example, suppose we collect a key to open a door, which leads to an unexpected reward. Using standard one-step TD learning, we would update the state in which the door opened. Using eligibility traces, we would also update the preceding trajectory, including the acquisition of the key. But we would not update other sequences that could have led to the reward, such as collecting a spare key or finding a different entrance.\nThe problem of credit assignment to counterfactual states may be addressed by learning a model, and using the model to propagate credit (Sutton 1990;Moore and Atkeson 1993); however, it has often proven challenging to construct and use models effectively in complex environments (cf. van Hasselt, Hessel, and Aslanides 2019). Similarly, source traces (Pitis 2018) model full potential histories in tabular settings, but rely on estimated importance-sampling ratios of state distributions, which are hard to estimate in non-tabular settings.\nWe introduce a new approach to counterfactual credit assignment, based on the concept of expected eligibility traces. We present a family of algorithms, which we call ET(\u03bb), that use expected traces to update their predictions. We analyse the nature of these expected traces, and illustrate their benefits empirically in several settings-see Figure 1 for a first illustration. We introduce a bootstrapping mechanism that provides a spectrum of algorithms between standard eligibility traces and expected eligibility traces, and also discuss ways to apply these ideas with deep neural networks. Finally, we discuss possible extensions and connections to related ideas such as successor features.", "publication_ref": ["b38", "b24", "b48", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Sequential decision problems can be modelled as Markov decision processes 1 (MDP) (S, A, p) (Puterman 1994), with state space S, action space A, and a joint transition and reward distribution p(r, s |s, a). An agent selects actions according to its policy \u03c0, such that A t \u223c \u03c0(\u2022|S t ) where \u03c0(a|s) denotes the probability of selecting a in s, and observes random rewards and states generated according to the MDP, resulting in trajectories \u03c4 t:T = {S t , A t , R t+1 , S t+1 , . . . , S T }. A central goal is to predict returns of future discounted rewards (Sutton and Barto 2018)\nG t \u2261 G(\u03c4 t:T ) = R t+1 + \u03b3 t+1 R t+2 + \u03b3 t+1 \u03b3 t+2 R t+3 + . . . = T i=1 \u03b3 (i\u22121) t+i R t+i ,\nwhere T is for instance the time the current episode terminates or T = \u221e, and where \u03b3 t \u2208 [0, 1] is a (possibly constant) discount factor and \u03b3\n(i) t = i k=1 \u03b3 t+k . The value v \u03c0 (s) = E [ G t |S t = s, \u03c0 ] of\nstate s is the expected return for a policy \u03c0. Rather than writing the return as a random variable G t , it will be convenient to instead write it as an explicit function G(\u03c4 ) of the random trajectory \u03c4 . Note that G(\u03c4 t:T ) = R t+1 + \u03b3 t+1 G(\u03c4 t+1:T ).\nWe approximate the value with a function v w (s) \u2248 v \u03c0 (s). This can for instance be a table-with a single separate entry w[s] for each state-a linear function of some input features, or a non-linear function such as a neural network with parameters w. The goal is to iteratively update w with w t+1 = w t + \u2206w t such that v w approaches the true v \u03c0 . Perhaps the simplest algorithm to do so is the Monte Carlo (MC) algorithm\n\u2206w t \u2261 \u03b1(R t+1 + \u03b3 t+1 G(\u03c4 t+1:T ) \u2212 v w (S t ))\u2207 w v w (S t ) .\nMonte Carlo is effective, but has high variance, which can lead to slow learning. TD learning (Sutton 1988;Sutton and Barto 2018) instead replaces the return with the current estimate of its expectation v(S t+1 ) \u2248 G(\u03c4 t+1:T ), yielding\n\u2206w t \u2261 \u03b1\u03b4 t \u2207 w v w (S t ) , (1\n)\nwhere \u03b4 t \u2261 R t+1 + \u03b3 t+1 v w (S t+1 ) \u2212 v w (S t ) ,\nwhere \u03b4 t is called the temporal-difference (TD) error. We can interpolate between these extremes, for instance with \u03bb-returns which smoothly mix values and sampled returns:\nG \u03bb (\u03c4 t:T ) = R t+1 +\u03b3 t+1 (1\u2212\u03bb)v w (S t+1 )+\u03bbG \u03bb (\u03c4 t+1:T ) .\n'Forward view' algorithms, like the MC algorithm, use returns that depend on future trajectories and need to wait until the end of an episode to construct their updates, which can take a long time. Conversely, 'backward view' algorithms rely only on past experiences and can update their predictions online, during an episode. Such algorithms build an eligibility trace (Sutton 1988;Sutton and Barto 2018). An example is TD(\u03bb): \u2206w t \u2261 \u03b1\u03b4 t e t , with e t = \u03b3 t \u03bbe t\u22121 + \u2207 w v w (S t ) , where e t is an accumulating eligibility trace. This trace can be viewed as a function e t \u2261 e(\u03c4 0:t ) of the trajectory of past transitions. The TD update in (1) is known as TD(0), because it corresponds to using \u03bb = 0. TD(\u03bb = 1) corresponds to an online implementation of the MC algorithm. Other variants exist, using other kinds of traces, and equivalences have been shown between these algorithms and their forward view using \u03bb-returns: these backward-view algorithms converge to the same solution as the corresponding forward view, and can in some cases yield equivalent weight updates (Sutton 1988;van Seijen and Sutton 2014;van Hasselt and Sutton 2015).", "publication_ref": ["b31", "b37", "b39", "b37", "b39", "b37", "b51", "b50"], "figure_ref": [], "table_ref": []}, {"heading": "Expected traces", "text": "The main idea is to use the concept of an expected eligibility trace, defined as\nz(s) \u2261 E [ e t | S t = s ] ,\nwhere the expectation is over the agent's policy and the MDP dynamics. We introduce a concrete family of algorithms, which we call ET(\u03bb) and ET(\u03bb, \u03b7), that learn expected traces and use them in value updates. We analyse these algorithms theoretically, describe specific instances, and discuss computational and algorithmic properties.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ET(\u03bb)", "text": "We propose to learn approximations z \u03b8 (S t ) \u2248 z(S t ), with parameters \u03b8 \u2208 R d (e.g., the weights of a neural network). One way to learn z \u03b8 is by updating it toward the instantaneous trace e t , by minimizing an empirical loss L(e t , z \u03b8 (S t )). For instance, L could be a component-wise squared loss, optimized with stochastic gradient descent:\n\u03b8 t+1 = \u03b8 t + \u2206\u03b8 t , where \u2206\u03b8 t = \u2212\u03b2 \u2202 \u2202\u03b8 1 2 (e t \u2212 z \u03b8 (S t )) (e t \u2212 z \u03b8 (S t )) = \u03b2 \u2202z \u03b8 (S t ) \u2202\u03b8 (e t \u2212 z \u03b8 (S t )) ,\nwhere\n\u2202z \u03b8 (St)\n\u2202\u03b8 is a |\u03b8| \u00d7 |e| Jacobian 2 and \u03b2 is a step size. The idea is then to use z \u03b8 (s) \u2248 E [ e t | S t = s ] in place of e t in the value update, which becomes \u2206w t \u2261 \u03b4 t z \u03b8 (S t ) .\n(2)\nWe call this ET(\u03bb). Below, we prove that this update can be unbiased and can have lower variance than TD(\u03bb). Algorithm 1 shows pseudo-code for a concrete instance of ET(\u03bb).\nAlgorithm 1 ET(\u03bb) \u03b4 \u2190 R + \u03b3v w (S ) \u2212 v w (S)\n8:\ne \u2190 \u03b3\u03bbe + \u2207 w v w (S) S) \u2202\u03b8 (e \u2212 z \u03b8 (S))\n9: \u03b8 \u2190 \u03b8 + \u03b2 \u2202z \u03b8 (\n10:\nw \u2190 w + \u03b1\u03b4z \u03b8 (S)\n11:\nuntil S is terminal 12: end for 13: Return w", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Interpretation and ET(\u03bb, \u03b7)", "text": "We can interpret TD(0) as taking the MC update and replacing the return from the subsequent state, which is a function of the future trajectory, with a state-based estimate of its expectation:\nv(S t+1 ) \u2248 E [ G(\u03c4 t+1:T )|S t+1 ]\n. This becomes most clear when juxtaposing the updates\n\u2206w t \u2261 \u03b1(R t+1 + \u03b3 t+1 G(\u03c4 t+1:T ) \u2212 v w (S t ))\u2207 t , (MC) \u2206w t \u2261 \u03b1(R t+1 + \u03b3 t+1 v w (S t+1 ) \u2212 v w (S t ))\u2207 t , (TD)\nwhere we used a shorthand \u2207 t \u2261 \u2207 w v w (S t ).\nTD(\u03bb) also uses a function of a trajectory: the trace e t . We propose replacing this as well with a function state z \u03b8 (S t ) \u2248 E [ e(\u03c4 0:t )|S t ]: the expected trace. Again juxtaposing: \u2206w t \u2261 \u03b1\u03b4 t e(\u03c4 0:t ) , (TD(\u03bb)) \u2206w t \u2261 \u03b1\u03b4 t z \u03b8 (S t ) .\n(ET(\u03bb))\nWhen switching from MC to TD(0), the dependence on the trajectory was replaced with a state-based value estimate to bootstrap on. We can interpolate smoothly between MC and TD(0) via \u03bb. This is often useful to trade off variance of the return with potential bias of the value estimate. For instance, we might not have access to the true state s, and might instead have to rely on features x(s). Then we cannot always represent or learn the true values v(s)-for instance different states may be aliased (Whitehead and Ballard 1991).\nSimilarly, when moving from TD(\u03bb) to ET(\u03bb) we replaced a trajectory-based trace with a state-based estimate. This might induce bias and, again, we can smoothly interpolate by using a recursively defined mixture trace y t , as defined as 3\ny t = (1 \u2212 \u03b7)z \u03b8 (S t ) + \u03b7 \u03b3 t \u03bby t\u22121 + \u2207 w v w (S t ) . (3)\nThis recursive usage of the estimates z \u03b8 (s) at previous states is analogous to bootstrapping on future state values when using a \u03bb-return, with the important difference that the arrow of time is opposite. This means we do not first have to convert this into a backward view: the quantity can already be computed from past experience directly. We call the algorithm that uses this mixture trace ET(\u03bb, \u03b7): \u2206w t \u2261 \u03b1\u03b4 t y(S t ) .\n(ET(\u03bb, \u03b7))\nNote that if \u03b7 = 1 then y t = e t equals the instantaneous trace: ET(\u03bb, 1) is equivalent to TD(\u03bb). If \u03b7 = 0 then y t = z t equals the expected trace; the algorithm introduced earlier as ET(\u03bb) is equivalent to ET(\u03bb, 0). By setting \u03b7 \u2208 (0, 1), we can smoothly interpolate between these extremes.", "publication_ref": ["b55"], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical analysis", "text": "We now analyse the new ET algorithms theoretically. First we show that if we use z(s) directly and s is Markov then the update has the same expectation as TD(\u03bb) (though possibly with lower variance), and therefore also inherits the same fixed point and convergence properties.\nLemma 1. If s is Markov, then E [ \u03b4 t e t | S t = s ] = E [ \u03b4 t | S t = s ] E [ e t | S t = s ] .\nProof. In Appendix . \nE [ \u03b1 t \u03b4 t z(S t )|S t = s ] = E [ \u03b1 t \u03b4 t e t |S t = s ] and V[\u03b1 t \u03b4 t z(S t )|S t = s] \u2264 V[\u03b1 t \u03b4 t e t |S t = s] ,\nwhere the second inequality holds component-wise for the update vector, and is strict when V[e t |S t ] > 0.\nProof. We have\nE [ \u03b1 t \u03b4 t e t | S t = s ] = E [ \u03b1 t \u03b4 t | S t = s ] E [ e t | S t = s ] (Lemma 1) = E [ \u03b1 t \u03b4 t | S t = s ] z(s) = E [ \u03b1 t \u03b4 t z(S t ) | S t = s ] .(4)\nDenote the i-th component of z(S t ) by z t,i and the i-th component of e t by e t,i . Then, we also have\nE (\u03b1 t \u03b4 t z t,i ) 2 |S t = s = E \u03b1 2 t \u03b4 2 t | S t = s z 2 t,i = E \u03b1 2 t \u03b4 2 t | S t = s E [ e t,i |S t = s ] 2 = E \u03b1 2 t \u03b4 2 t | S t = s E e 2 t,i |S t = s \u2212 V[e t,i |S t = s] \u2264 E \u03b1 2 t \u03b4 2 t | S t = s E e 2 t,i | S t = s = E (\u03b1 t \u03b4 t e t,i ) 2 | S t = s ,\nwhere the last step used the fact that s is Markov, and the inequality is strict when V[e t |S t ] > 0. Since the expectations are equal, as shown in (4), the conclusion follows.\nInterpretation Proposition 1 is a strong result: it holds for any trace update, including accumulating traces (Sutton 1984(Sutton , 1988, replacing traces (Singh and Sutton 1996), dutch traces (van Seijen and Sutton 2014;van Hasselt, Mahmood, and Sutton 2014;van Hasselt and Sutton 2015), and future traces that may be discovered. It implies convergence of ET(\u03bb) under the same conditions as TD(\u03bb) (Dayan 1992;Peng 1993;Tsitsiklis 1994) with lower variance when V[e t |S t ] > 0, which is the common case.\nNext, we consider what happens if we violate the assumptions of Proposition 1. We start by analysing the case of a learned approximation z t (s) \u2248 z(s) that relies solely on observed experience.\nProposition 2. Let e t an instantaneous trace vector. Then let z t (s) be the empirical mean z t (s) = 1 nt(s) nt(s) i e t s i , where t s i -s denote past times when we have been in state s, that is S t s i = s, and n t (s) is the number of visits to s in the first t steps. Consider the expected trace algorithm w t+1 = w t + \u03b1 t \u03b4 t z t . If S t is Markov, the expectation of this update is equal to the expected update with instantaneous traces e t , while attaining a potentially lower variance:\nE [ \u03b1 t \u03b4 t z t (S t ) | S t ] = E [ \u03b1 t \u03b4 t e t | S t ]\nand\nV[\u03b1 t \u03b4 t z t (S t ) | S t ] \u2264 V[\u03b1 t \u03b4 t e t | S t ] ,\nwhere the second inequality holds component-wise. The inequality is strict when V[e t | S t ] > 0.\nProof. In Appendix.\nInterpretation Proposition 2 mirrors Proposition 1 but, importantly, covers the case where we estimate the expected traces from data, rather than relying on exact estimates. This means the benefits extend to this pure learning setting. Again, the result holds for any trace update. The inequality is typically strict when the path leading to state S t = s is stochastic (due to environment or policy).\nNext we consider what happens if we do not have Markov states and instead have to rely on, possibly non-Markovian, features x(s). We then have to pick a function class and for the purpose of this analysis we consider linear expected traces z \u0398 (s) = \u0398x(s) and values v w (s) = w x(s), as convergence for non-linear values can not always be assured even for standard TD(\u03bb) (Tsitsiklis and Van Roy 1997), without additional assumptions (e.g., Ollivier 2018;Brandfonbrener and Bruna 2020). The following property of the mixture trace is used in the proposition below.\nProposition 3. The mixture trace y t defined in (3) can be written as y t = \u00b5y t\u22121 + u t with decay parameter \u00b5 = \u03b7\u03b3\u03bb and signal u\nt = (1 \u2212 \u03b7)z \u03b8 (S t ) + \u03b7 \u2207 w v w (S t ), such that y t = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z \u03b8 (S t\u2212k ) + \u03b7 \u2207 w v w (S t\u2212k )] . (5)\nProof. In Appendix.\nRecall y t = e t when \u03b7 = 1, and y t = z \u03b8 (S t ) when \u03b7 = 0, as can be verified by inspecting (5) (and using the convention 0 0 = 1). We use this proposition to prove the following. Proposition 4. When using approximations z \u0398 (s) = \u0398x(s) and v w (s) = w x(s) then, if (1 \u2212 \u03b7)\u0398 + \u03b7I is non-singular, ET(\u03bb, \u03b7) has the same fixed point as TD(\u03bb\u03b7).\nProof. In Appendix.", "publication_ref": ["b36", "b37", "b35", "b51", "b49", "b50", "b8", "b26", "b44", "b45", "b25", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "TD(0)", "text": "Episode 7 first reward TD( = 0.9) ET( = 0.9)  1, we show later value estimates after more rewards have been observed. TD(0) learns slowly but steadily, TD(\u03bb) learns faster but with higher variance, and ET(\u03bb) learns both fast and stable.\nInterpretation This result implies that linear ET(\u03bb, \u03b7) converges under similar conditions as linear TD(\u03bb ) for \u03bb = \u03bb\u2022\u03b7.\nIn particular, when \u0398 is non-singular, using the approximation z \u0398 (s) = \u0398x(s) in ET(\u03bb, 0) = ET(\u03bb) implies convergence to the fixed point of TD(0). Though ET(\u03bb, \u03b7) and TD(\u03bb\u03b7) have the same fixed point, the algorithms are not equivalent. In general, their updates are not the same. Linear approximations are more general than tabular functions (which are linear functions of a indicator vector for the current state), and we have already seen in Figure 1 that ET(\u03bb) behaves quite differently from both TD(0) and TD(\u03bb), and we have seen its variance can be lower in Propositions 1 and 2. Interestingly, \u0398 resembles a preconditioner that speeds up the linear semi-gradient TD update, similar to how second-order optimisation algorithms (Amari 1998;Martens 2016) precondition the gradient updates.", "publication_ref": ["b0", "b20"], "figure_ref": [], "table_ref": []}, {"heading": "Empirical analysis", "text": "From the insights above, we expect that ET(\u03bb) yields lower prediction errors because it has lower variance and aggregates information across episodes better. In this section we empirically investigate expected traces in several experiments. Whenever we refer to ET(\u03bb), this is equivalent to ET(\u03bb, 0).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "An open world", "text": "First consider the grid world depicted in Figure 1. The agent randomly moves right or down (excluding moves that would hit a wall), starting from the top-left corner. Any action in the bottom-right corner terminates the episode with +1 reward with probability 0.2, and 0 otherwise. All other rewards are 0.\nFigure 1 shows the value estimates after the first positive reward, which occurred in the seventh episode. TD(0) updated a single state, TD(\u03bb) updated earlier states in that episode, and ET(\u03bb) additionally updated states from previous episodes. Figure 2 shows the values after the second reward, and after roughly 20, 200, and 2000 rewards (or 100, 1000, and 10, 000 episodes, respectively). ET(\u03bb) converged faster than TD(0), which propagated information slowly, and than Figure 3: Multi-chain environment. Each episode starts in the left-most (white) state, and randomly transitions to one of m parallel (blue) chains of identical length n. After n steps, the agent always transitions to the same (orange) state, regardless of the chain it was in. The next step the episode terminates. Each reward is +1, except on termination when it either is +1 with probability 0.9 or \u22121 with probability 0.1. TD(\u03bb), which had higher variance. All step sizes decayed as \u03b1 = \u03b2 = 1/k, where k is the current episode number.", "publication_ref": [], "figure_ref": ["fig_5"], "table_ref": []}, {"heading": "A multi-chain", "text": "We now consider the multi-chain shown in Figure 3. We first compare TD(\u03bb) and ET(\u03bb) with tabular values on various variants of the multi-chain, corresponding n = 4 and m \u2208 {1, 2, 4, 8, ..., 128}. The left-most plot in Figure 4 shows the average root mean squared error (RMSE) of the value predictions after 1024 episodes. We ran 10 seeds for each combination of step size 1/t d with d \u2208 {0.5, 0.8, 0.9, 1} and \u03bb \u2208 {0, 0.5, 0.8, 0.9, 0.95, 1}.\nThe left plot in Figure 4 shows value errors for different m, minimized over d and \u03bb. The prediction error of TD(\u03bb) (blue) grew quickly with the number of parallel chains. ET(\u03bb) (orange) scaled better, because it updates values in multiple chains (from past episodes) upon receiving a surprising reward (e.g., \u22121) on termination. The other three plots in Figure 4 show value error as a function of \u03bb for a subset of problems corresponding to m \u2208 {8, 32, 128}. The dependence on \u03bb differs across algorithms and problem instances; ET(\u03bb) always achieved lower error than TD(\u03bb). Further analysis, including on step-size sensitivity, is included in the appendix.\nNext, we encode each state with a feature vector x(s) containing a binary indicator vector of the branch, a binary indicator of the progress along the chain, a bias that always equals one, and two binary features indicating when we are in the start (white) or bottleneck (orange) state. We extend the lengths of the chains to n = 16. Both TD(\u03bb) and ET(\u03bb) use a linear value function v w (s) = w x(s), and ET(\u03bb) uses a linear expected trace z \u0398 (s) = \u0398x(s). All updates use the same constant step size \u03b1. The left plot in Figure 5 shows the average root mean squared value error after 1024 episodes (averaged over 10 seeds). For each point the best constant step size \u03b1 \u2208 {0.01, 0.03, 0.1} (shared across all updates) and \u03bb \u2208 {0, 0.5, 0.8, 0.9, 0.95, 1} is selected. ET(\u03bb) (orange) attained lower errors across all values of m (left plot), and for all \u03bb (center two plots, for two specific m). The right plot shows results for smooth interpolations via \u03b7, for \u03bb = 0.9 and m = 16. The full expected trace (\u03b7 = 0) performed well here, we expect in other settings the additional flexibility of \u03b7 could be beneficial.\nExpected traces in deep reinforcement learning (Deep) neural networks are a common choice of function class in reinforcement learning (e.g., Werbos 1990;Tesauro 1992Tesauro , 1994Bertsekas and Tsitsiklis 1996;Prokhorov and Wunsch 1997;Riedmiller 2005;van Hasselt 2012;Mnih et al. 2015;Wang et al. 2016;Silver et al. 2016;Duan et al. 2016;Hessel et al. 2018). Eligibility traces are not very commonly combined with deep networks (but see Tesauro 1992; Elfwing, Uchibe, and Doya 2018), perhaps in part because of the popularity of experience replay (Lin 1992;Mnih et al. 2015;Horgan et al. 2018).\nPerhaps the simplest way to extend expected traces to deep neural networks is to first separate the value function into a representation x(s) and a value v (w,\u03be) (s) = w x \u03be (s), where x \u03be is some (non-linear) function of the observations s. 4 We can then apply the same expected trace algorithm as used in the previous sections by learning a separate linear function z \u0398 (s) = \u0398x(s) using the representation which is learned by backpropagating the value updates:\n\u03be t+1 = \u03be t + \u03b1\u03b4e \u03be t and w t+1 = w t + \u03b1\u03b4z \u0398 (S t ) ,\nwhere e \u03be t = \u03b3 t \u03bbe \u03be t\u22121 + \u2207 \u03be v (w,\u03be) (S t ) , e w t = \u03b3 t \u03bbe w t\u22121 + \u2207 w v (w,\u03be) (S t ) , and then updating \u0398 by minimising the sum of componentwise squared differences between e w t and z \u0398t (S t ). Interesting challenges appear outside the fully linear case. First, the representation will itself be updated and will have its own trace e \u03be t . Second, in the control case we optimise behaviour: the policy will change. Both these properties of the non-linear control setting imply that the expected traces must track a non-stationary target. We found that being able to track this rather quickly improves performance: the expected trace parameters \u0398 in the following experiment were updated with a step size of \u03b2 = 0.1.\nWe tested this idea on two canonical Atari games: Pong and Ms. Pac-Man. The results in Figure 6 show that the expected traces helped speed up learning compared to the baseline which uses accumulating traces, for various step sizes. Unlike most prior work on this domain, which often relies on replay (Mnih et al. 2015;Schaul et al. 2016;Horgan et al. 2018) or parallel streams of experience (Mnih et al. 2016), these algorithms updated the values online from a single stream of experience. Further details are in the Appendix.\nThese experiments demonstrate that the idea of expected traces already extends to non-linear function approximation, such as deep neural networks. We consider this to be a rich area of further investigations. The results presented here are similar to earlier results (e.g., Mnih et al. 2015) and are not meant to compete with state-of-the-art performance results, which often depend on replay and much larger amounts of experience (e.g., Horgan et al. 2018).", "publication_ref": ["b54", "b42", "b43", "b4", "b30", "b32", "b45", "b23", "b53", "b34", "b10", "b13", "b19", "b23", "b14", "b23", "b33", "b14", "b22", "b23", "b14"], "figure_ref": ["fig_5", "fig_0", "fig_0", "fig_0", "fig_1", "fig_2"], "table_ref": []}, {"heading": "Discussion and extensions", "text": "We now discuss various interesting interpretations and relations, and discuss promising extensions.  ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Predecessor features", "text": "For linear value functions the expected trace z(s) can be expressed non recursively as follows:\nz(s) = E \u221e n=0 \u03bb (n) t \u03b3 (n) t x t\u2212n | S t = s ,(6)\nwhere \u03b3\n(n) k \u2261 k j=k\u2212n \u03b3 j . This is interestingly similar to the definition of the successor features (Barreto et al. 2017):\n\u03c8(s) = E \u221e n=1 \u03b3 (n\u22121) t x t+n | S t = s . (7\n)\nThe summation in ( 7) is over future features, while in ( 6) we have a sum over features already observed by the agent. We can thus think of linear expected traces as predecessor features. A similar connection was made in the tabular setting by Pitis (2018), relating source traces, which aim to estimate the source matrix (I \u2212 \u03b3P ) \u22121 , to successor representations (Dayan 1993). In a sense, the above generalises this insight. In addition to being interesting in its own right, this connection allows for an intriguing interpretation of z(s) as a multidimensional value function. Like with successor features, the features x t play the role of rewards, discounted with \u03b3 \u2022 \u03bb rather than \u03b3, and with time flowing backwards.\nAlthough the predecessor interpretation only holds in the linear case, it is also of interest as a means to obtain a practical implementation of expected traces with non-linear function approximation, for instance applied only to the linear 'head' of a deep neural network. We used this 'predecessor feature trick' in our Atari experiments described earlier.", "publication_ref": ["b1", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Relation to model-based reinforcement learning", "text": "Model-based reinforcement learning provides an alternative approach to efficient credit assignment. The general idea is to construct a model that estimates state-transition dynamics, and to update the value function based upon hypothetical transitions drawn from the model (Sutton 1990), for example by prioritised sweeping (Moore and Atkeson 1993;van Seijen and Sutton 2013). In practice, model-based approaches have proven challenging in environments (such as Atari games) with rich perceptual observations, compared to model-free approaches that more directly update the agent's policy and predictions (van Hasselt, Hessel, and Aslanides 2019).\nIn some sense, expected traces also construct a model of the environment-but one that differs in several key regards from standard state-to-state models used in model-based reinforcement learning. First, expected traces estimate past quantities rather than future quantities. Second, they estimate the accumulation of gradients over a multi-step trajectory, rather than full transition dynamics, thereby focusing on those aspects that matter for the update. Third, they allow credit assignment across these potential past trajectories with a single update, without the iterative computation that is typically required when using a more explicit model. These differences may be important to side-step some of the challenges faced in model-based learning.", "publication_ref": ["b38", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Batch learning and replay", "text": "We have mainly considered the online learning setting in this paper. It is often convenient to learn from batches of data, or replay transitions repeatedly, to enhance data efficiency. A natural extension is replay the experiences sequentially (e.g. Kapturowski et al. 2018), but perhaps alternatives exist. We now discuss one potential extension. We defined a mixed trace y t that mixes the instantaneous and expected traces. Optionally the expected trace z t can be updated towards the mixed trace y t as well, instead of towards the instantaneous trace e t . Analogously to TD(\u03bb) we propose to then use at least one real step of data:\n\u2206\u03b8 t \u2261 \u03b2 (\u2207 t + \u03b3 t \u03bb t y t\u22121 \u2212 z \u03b8 (S t )) \u2202z \u03b8 (S t ) \u2202\u03b8 ,(8)\nwith \u2207 t \u2261 \u2207 w v w (S t ). This is akin to a forward-view \u03bbreturn update, with \u2207 w v w (S t ) in the role of (vector) reward, and z \u03b8 of value, and discounted by \u03bb t \u03b3 t , but reversed in time.\nIn other words, this can be considered a sampled Bellman equation (Bellman 1957) but backward in time.\nWhen we then choose \u03b7 = 0, then y t\u22121 = z \u03b8 (S t\u22121 ), and then the target in (8) only depends on a single transition. Interestingly, that means we can then learn expected traces from individual transitions, sampled out of temporal order, for instance in batch settings or when using replay.", "publication_ref": ["b17", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Application to other traces", "text": "We can apply the idea of expected trace to more traces than considered here. We can for instance consider the characteristic eligibility trace used in REINFORCE (Williams 1992) and related policy-gradient algorithms (Sutton et al. 2000).\nAnother appealing application is to the follow-on trace or emphasis, used in emphatic temporal difference learning (Sutton, Mahmood, and White 2016) and related algorithms (e.g., Imani, Graves, and White 2018). Emphatic TD was proposed to correct an important issue with off-policy learning, which can be unstable and lead to diverging learning dynamics. Emphatic TD weights updates according to 1) the inherent interest in having accurate predictions in that state and, 2) the importance of predictions in that state for updating other predictions. Emphatic TD uses scalar 'follow-on' traces to determine the 'emphasis' for each update. However, this follow-on trace can have very high, even infinite, variance. Instead, we might estimate and use its expectation instead of the instantaneous emphasis. A related idea was explored by Zhang, Boehmer, and Whiteson (2019) to obtain off-policy actor critic algorithms.", "publication_ref": ["b56", "b41", "b40", "b15", "b57"], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "We have proposed a mechanism for efficient credit assignment, using the expectation of an eligibility trace. We have demonstrated this can sometimes speed up credit assignment greatly, and have analyzed concrete algorithms theoretically and empirically to increase understanding of the concept.\nExpected traces have several interpretations. First, we can interpret the algorithm as counterfactually updating multiple possible trajectories leading up to the current state. Second, they can be understood as trading off bias and variance, which can be done smoothly via a unifying \u03b7 parameter, between standard eligibility traces (low bias, high variance) and estimated traces (possibly higher bias, but lower variance). Furthermore, with tabular or linear function approximation we can interpret the resulting expected traces as predecessor states or features-object analogous to successor states or features, but time-reversed. Finally, we can interpret the linear algorithm as preconditioning the standard TD update, thereby potentially speeding up learning. These interpretations suggest that a variety of complementary ways to potentially extend these concepts and algorithms.\nUsing the above, we can prove Lemma 1:\nLemma 1. If s is Markov, then E [ \u03b4 t e t | S t = s ] = E [ \u03b4 t | S t = s ] E [ e t | S t = s ].\nProof. First note that the expectations above are with respect to the transition probabilities p \u03c0 as defined in ( 9). That noted, the result trivially follows from the fact that, when s is Markov, the two random variables \u03b4 t and e t are independent conditioned on S t . To see why this is so, note that \u03b4 t is defined as\n\u03b4 t = R t+1 + \u03b3 t+1 v w (S t+1 ) \u2212 v w (S t ).(10)\nSince we are conditioning on the event that S t = s, the only two random quantities in the definition of \u03b4 t are R t+1 and S t+1 . Thus, because s is Markov, we have that\np \u03c0 (\u03b4 t |S t , R t , S t\u22121 , R t\u22121 , ...) = p \u03c0 (\u03b4 t |S t ), (Property 1)\nthat is, S t fully defines the distribution of \u03b4 t . This means that p \u03c0 (\u03b4 t |S t , X t ) = p \u03c0 (\u03b4 t |S t ) for any t \u2264 t, where X t is a random variable that only depends on events that occurred up to time t . Replacing X t with e t , we have that p \u03c0 (\u03b4 t |S t , e t ) = p \u03c0 (\u03b4 t |S t ), which implies that \u03b4 t and e t are independent conditioned on S t .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Proof of Proposition 2", "text": "Proposition 2. Let e t an instantaneous trace vector. Then let z t (s) be the empirical mean z t (s) = 1 nt(s) nt(s) i e t s i , where t s i -s denote past times when we have been in state s, that is S t s i = s, and n t (s) is the number of visits to s in the first t steps. Consider the expected trace algorithm w t+1 = w t + \u03b1 t \u03b4 t z t . If S t is Markov, the expectation of this update is equal to the expected update with instantaneous traces e t , while attaining a potentially lower variance:\nE [ \u03b1 t \u03b4 t z t (S t ) | S t ] = E [ \u03b1 t \u03b4 t e t | S t ] and V[\u03b1 t \u03b4 t z t (S t ) | S t ] \u2264 V[\u03b1 t \u03b4 t e t | S t ]\n, where the second inequality holds component-wise. The inequality is strict when V[e t | S t ] > 0.\nProof. We have\nE [ \u03b1 t \u03b4 t e t | S t = s ] = E [ \u03b1 t \u03b4 t | S t = s ] E [ e t | S t = s ] (as s is Markov) = E [ \u03b1 t \u03b4 t | S t = s ] E [ z t | S t = s ] (as z t = 1 n n i e t s i ) = E [ \u03b1 t \u03b4 t z t | S t = s ] .\nNow let us look at the conditional variance for each of the dimension of the update vector \u03b1 t \u03b4 t z t : V[\u03b1 t \u03b4 t z t,i | S t = s], where z t,i denotes the i-th component of vector z t .\nV[\u03b1 t \u03b4 t z t,i | S t = s] = E (\u03b1 t \u03b4 t z t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t z t,i | S t = s ] 2 = E \u03b1 2 t \u03b4 2 t (z t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 E [ z t,i | S t = s ] 2 = E \u03b1 2 t \u03b4 2 t | S t = s E (z t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 E [ z t,i | S t = s ]\nBy a similar argument, we have\nV[\u03b1 t \u03b4 t e t,i | S t = s] = E \u03b1 2 t \u03b4 2 t | S t = s E (e t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 E [ e t,i | S t = s ] 2\nNow, we also know that E [ z t | S t = s ] = E [ e t | S t = s ] = \u00b5 t , as z t is the empirical mean of e t . Thus we also have, component-wise,\nE [ z t,i | S t = s ] = E [ e t,i | S t = s ] = \u00b5 t,i\nMoreover, from the same reason we have that V(z t,i |S t = s) = 1 ns V(e t,i |S t = s). Thus we obtain:\nV[\u03b1 t \u03b4 t z t,i | S t = s] = E \u03b1 2 t \u03b4 2 t | S t = s E z t,i (z t,i ) T | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 \u00b5 2 t,i\nThus:\nV[\u03b1 t \u03b4 t z t,i | S t = s] \u2212 V[\u03b1 t \u03b4 t e t,i | S t = s] = E \u03b1 2 t \u03b4 2 t | S t = s E z t,i (z t,i ) T | S t = s \u2212 E e t,i (e t,i ) T | S t = s \u22640, from definition of zt,i \u2264 0 ,\nwith equality holding, if and only if:\ni E (z t,i ) 2 | S t = s = E (e t,i ) 2 | S t = s \u21d2 V(z t,i |S t = s) = V(e t,i |S t = s), but V(z t,i |S t = s) = 1 ns V(e t,i |S t = s\n) by definition of z t,i as the running mean on samples e t,i . This can only happen for n s = 1, or in the absence of stochasticity, for every state s. Thus, in the most general case, this implies V(z\nt,i |S t = s) = V(e t,i |S t = s) = 0; or ii E \u03b1 2 t \u03b4 2 t | S t = s = 0 \u21d2 \u03b4 t = 0\nThus, we have equality only with we have exactly one sample for the average z t so far, or only one sample is needed (thus z t and e t are not actual random variables and there is only one deterministic path to s); or when the TD errors are zero for all transitions following s.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Properties of mixture traces", "text": "In this section we explore and proof some of the properties of the proposed mixture trace, defined in Equation (3) in the main text and repeated here: y t = (1 \u2212 \u03b7)z \u03b8 (S t ) + \u03b7 \u03b3 t \u03bb t y t\u22121 + \u2207 w v w (S t ) .\n(3) The proofs, in this section we will use the notation x t to denote the features used in a linear approximation for the value function(s) constructed. Just note that this term can be substituted, in general, by the gradient term \u2207 w v w (S t ) in the equation above. Proposition 3. The mixture trace y t defined in (3) can be written as y t = \u00b5y t\u22121 + u t with decay parameter \u00b5 = \u03b7\u03b3\u03bb and signal u t = (1 \u2212 \u03b7)z \u03b8 (S t ) + \u03b7 \u2207 w v w (S t ), such that\ny t = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z \u03b8 (S t\u2212k ) + \u03b7 \u2207 w v w (S t\u2212k )] .(5)\nProof. As mentioned before, under a linear parameterization \u2207 w v w (S t ) = x(S t ) := x t Let us start with the definition of the mixture trace y t :\ny t = (1 \u2212 \u03b7)z t + \u03b7(\u03b3 t \u03bb t y t\u22121 + x t ) = [(1 \u2212 \u03b7)z t + \u03b7x t ] + \u03b7\u03b3 t \u03bb t y t\u22121 = [(1 \u2212 \u03b7)z t + \u03b7x t ] + \u03b7\u03b3 t \u03bb t [(1 \u2212 \u03b7)z t\u22121 + \u03b7x t\u22121 ] + \u03b7 2 \u03b3 t \u03bb t \u03b3 t\u22121 \u03bb t\u22121 y t\u22122 = (1 \u2212 \u03b7) z t + \u03b7\u03b3 t \u03bb t z t\u22121 + \u03b7 2 \u03b3 t \u03bb t \u03b3 t\u22121 \u03bb t\u22121 z t\u22122 + \u2022 \u2022 \u2022 + + \u03b7 x t + \u03b7\u03b3 t \u03bb t x t\u22121 + \u03b7 2 \u03b3 t \u03bb t \u03b3 t\u22121 \u03bb t\u22121 x t\u22122 + \u2022 \u2022 \u2022 = (1 \u2212 \u03b7) t k=0 (\u03b7\u03b3\u03bb) k z t\u2212k + \u03b7 t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z t\u2212k + \u03b7x t\u2212k ]\nSubstituting x t in the above derivation by \u2207 w v w (S t ) leads to (3).\nProposition 4. When using approximations z \u0398 (s) = \u0398x(s) and v w (s) = w x(s) then, if (1 \u2212 \u03b7)\u0398 + \u03b7I is non-singular, ET(\u03bb, \u03b7) has the same fixed point as TD(\u03bb\u03b7).\nProof. By Proposition 3 we have that y t can be re-written as:\ny t = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z \u03b8 (S t\u2212k ) + \u03b7x(S t\u2212k )] = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)\u0398x(S t\u2212k ) + \u03b7x(S t\u2212k )] (11) = [(1 \u2212 \u03b7)\u0398 + \u03b7I] t k=0 (\u03b7\u03b3\u03bb) k x(S t\u2212k )\ninstantaneous trace e \u03bb\u03b7 t .\nWe examine the fixed point w * of the algorithm using this approximation of the expected trace:\nE [ \u03b4 t y t ] = E y t (R t+1 + \u03b3x(S t+1 ) w * \u2212 x(S t ) w * ) = 0 .\nThis implies the fixed point is\nw * = E y t (\u03b3x(S t+1 ) \u2212 x(S t )) \u22121 E [ y t R t+1 ] .\nNow, plugging in the relation in ( 12) above, we get:\nw * = E [(1 \u2212 \u03b7)\u0398 + \u03b7I] e \u03bb\u03b7 t (\u03b3x(S t+1 ) \u2212 x(S t )) \u22121 E [(1 \u2212 \u03b7)\u0398 + \u03b7I] e \u03bb\u03b7 t R t+1 = E e \u03bb\u03b7 t (\u03b3x(S t+1 ) \u2212 x(S t )) \u22121 [(1 \u2212 \u03b7)\u0398 + \u03b7I] \u22121 [(1 \u2212 \u03b7)\u0398 + \u03b7I] E e \u03bb\u03b7 t R t+1\n= E e \u03bb\u03b7 t (\u03b3x(S t+1 ) \u2212 x(S t ))\n\u22121 E e \u03bb\u03b7 t R t+1 .\nThis last term is the fixed point for TD(\u03bb\u03b7).\nMoreover, it is worth noting that the above equality recovers, for the extreme values of \u03b7:\n\u2022 \u03b7 = 1 \u21d2 y t = t k=0 (\u03b3\u03bb) k x t\u2212k (instantaneous trace for TD(\u03bb)) \u2022 \u03b7 = 0 \u21d2 y t = t k=0 (\u03b7\u03b3\u03bb) k z t\u2212k = z t (expected trace for TD(\u03bb))\nMoreover, as the extreme values already suggest, the expected update of the mixture traces follows the TD(\u03bb) learning, in expectation, for all the intermediate values \u03b7 \u2208 (0, 1) as well, trading off variance of estimates as \u03b7 approaches 0.\nProposition 5. Let e \u03bb t be a \u03bb trace vector. Let y t = (1 \u2212 \u03b7)z t + \u03b7(\u03b3\u03bby t\u22121 + x t ) (as defined in (3)). Consider the ET(\u03bb, \u03b7) algorithm w t+1 = w t + \u03b1 t \u03b4 t y t . For all Markov states s the expectation of this update is equal to the expected update with instantaneous traces e \u03bb t :\nE [ \u03b1 t \u03b4 t y(S t )|S t = s ] = E \u03b1 t \u03b4 t e \u03bb t |S t = s ,\nfor every \u03b7 \u2208 [0, 1] and any \u03bb \u2208 [0, 1].\nProof. Let us revisit Eq. 5 in Proposition 3:\nE [ yt ] = E t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z t\u2212k + \u03b7x t\u2212k ] = E t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)E [ (x t\u2212k + \u03b3\u03bbz t\u2212k\u22121 ) ] + \u03b7x t\u2212k ] = E \uf8ee \uf8ef \uf8ef \uf8f0 t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7)\u03b3\u03bb t\u22121 k=0 (\u03b7\u03b3\u03bb) k z t\u2212k\u22121 E[ (x t\u2212k\u22121 +\u03b3\u03bbz t\u2212k\u22122 ) ] \uf8f9 \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8f0 t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7)\u03b3\u03bb t\u22121 k=0 (\u03b7\u03b3\u03bb) k x t\u2212k\u22121 + (1 \u2212 \u03b7)(\u03b3\u03bb) 2 t\u22122 k=0 (\u03b7\u03b3\u03bb) k z t\u2212k\u22122 E[ x t\u2212k\u22122 +\u03b3\u03bbz t\u2212k\u22123 ] \uf8f9 \uf8fa \uf8fa \uf8fb = E t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7) t\u22121 i=1 (\u03b3\u03bb) i t\u2212i k=0 (\u03b7\u03b3\u03bb) k x t\u2212k\u2212i\nNow, re-writing the sum, gathering all the weighting for each feature x t\u2212k\u2212i we get:\nE [ yt ] = E t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7) t\u22121 i=1 (\u03b3\u03bb) i t\u2212i k=0 (\u03b7\u03b3\u03bb) k x t\u2212k\u2212i = E xt + t k=1 x t\u2212k (\u03b7\u03b3\u03bb) k + (1 \u2212 \u03b7) k i=1 (\u03b3\u03bb) i \u2022 (\u03b3\u03bb\u03b7) k\u2212i = E xt + t k=1 x t\u2212k (\u03b3\u03bb) k \u03b7 k + (1 \u2212 \u03b7) k i=1 \u03b7 k\u2212i = E xt + t k=1 x t\u2212k (\u03b3\u03bb) k \u03b7 k + (1 \u2212 \u03b7) 1 \u2212 \u03b7 k (1 \u2212 \u03b7) = E xt + t k=1 x t\u2212k (\u03b3\u03bb) k = E t k=0 (\u03b3\u03bb) k x t\u2212k Thus E [ yt ] = E t k=0 (\u03b3\u03bb) k x t\u2212k = E e \u03bb t\n, where e \u03bb t is the instantaneous \u03bb trace on feature space x. Thus E [ y(s) ] = z \u03bb * (s) = E e \u03bb t . Finally we can plug-in this result in the expected update:\nE [ \u03b1t\u03b4ty(St)|St = s ] = E [ \u03b1t\u03b4t|St = s ] E [ y(St)|St = s ] = E [ \u03b1t\u03b4t|St = s ] z \u03bb * (s) = E [ \u03b1t\u03b4t|St = s ] E e \u03bb t |St = s = E \u03b1t\u03b4te \u03bb t |St = s .\nFinally, please note that in this proposition and its proof we drop the time indices t for \u03bb and \u03b3 parameters in the definition of y t . This is purely to ease the notation and promote compactness in the derivation", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameter study", "text": "Figure 7 contains a parameter study in which we compare the performance of TD(\u03bb) and ET(\u03bb) across different step sizes. The data used for this figure is the same as used to generate the plots in Figure 4, but now we look explicitly at the effect of the step size parameter. ", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "RMSE", "text": "TD( ), = 1/n TD( ), = 1/n 0.9 TD( ), = 1/n 0.8 TD( ), = 1/ n ET( ), = 1/n ET( ), = 1/n 0.9 ET( ), = 1/n 0.8 ET( ), = 1/ n Figure 7: Comparison of prediction errors (lower is better) of TD(\u03bb) and ET(\u03bb) across different \u03bbs and different step sizes in the multi-chain world 3. The data underpinning these plots is the same as the data used for Figure 4, with 32 parallel chains. In all cases the step size was \u03b1 = 1/n t (S t ) d , where n t (s) = t i=0 I(S i = s) is the number of visits to state s in the first t time steps, and where d is a hyper-parameter. Note that the step size is lower when the exponent is higher. We see that TD(0) performed best with a high step size, and that for high \u03bb lower step sizes performed better-TD(0) with the highest step size (\u03b1 t = 1/ n t (S t )) and TD(1) with the lowest step size (\u03b1 t = 1/n t (S t )) both performed poorly. In contrast, ET(\u03bb) here performed well for any combination of step size and trace parameter \u03bb.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiment details for Atari experiments", "text": "For our deep reinforcement learning experiments on Atari games, we compare to an implementation of online Q(\u03bb). We first describe this algorithm, and then describe the expected-trace variant. All the Atari experiments were run with the ALE (Bellemare et al. 2013), exactly as described in Mnih et al. (2015), including using action repeats (4x), downsampling (to 84 \u00d7 84), and frame stacking. These experiments were conducted using Jax (Bradbury et al. 2018a).\nIn all cases, we used -greedy exploration (cf. Sutton and Barto 2018), with an that quickly decayed from 1 to 0.01 according to 0 = 1 and t = t\u22121 + 0.01(0.01 \u2212 t\u22121 ). Unlike Mnih et al. (2015), we did not clip rewards, and we also did not apply any target normalisation (cf.  or non-linear value transformations (Pohlen et al. 2018;. We conjecture that such extensions could be beneficial for performance, but they are orthogonal to the main research questions investigated here and are therefore left for future work. w \u2190 w + \u2206w 15: until done Deep Q(\u03bb) We assume the typical setting (e.g., Mnih et al. 2015) where we have a neural network q w that outputs |A| numbers, such that q(s, a) = q w (s) [a]. That is, we forward the observation s through network q w with weights w and |A| outputs, and then select the a th output to represent the value of taking action a.\nAlgorithm 2 then works as follows. For each transition, we first compute a telescoping TD error \u03b4 = r + \u03b3 v \u2212 v (line 5), where \u03b3 = 0 on termination (and then S is the first observation of the next episode) and, in our experiments, \u03b3 = 0.995 otherwise. We update the trace e as usual (line 11), using accumulating traces. Note that the weights and, hence, trace will also have elements corresponding to the weights of actions that were not selected. The gradient with respect to those elements is considered to be zero, as is conventional.\nThen, we compute a weight update \u2206w = \u03b4e + (v \u2212 q w (S, A))\u2207 w q w (S, A). The additional term corrects for the fact that our TD error is a telescoping error, and does not have the usual ' \u2212 q(s, a)' term. This is akin to the Q(\u03bb) algorithm proposed by Peng and Williams (1996).\nFinally, we transform the resulting update, using a transformation exactly like ADAM (Kingma and Adam 2015), but applied to the update \u2206w rather than a gradient. The hyper-parameters were \u03b2 1 = 0.9, \u03b2 2 = 0.999, and = 0.0001, and one of the step sizes as given in Figure 6. We then apply the resulting transformed update by adding it to the weights (line 14).\nFor the Atari experiments, we used the same preprocessing and network architecture as Mnih et al. (2015), except that we used 128 channels in each convolutional layer because we ran experiments on TPUs (version 3.0, using a single core per experiment) which are most efficient when using tensors where one dimension is a multiple of 128. The experiments were written using JAX (Bradbury et al. 2018b) and Haiku (Hennigan et al. 2020).\nDeep QET(\u03bb) We now describe the expected-trace algorithm, shown in Algorithm 3, which was used for the Atari experiments. It is very similar to the Q(\u03bb) algorithm described above, and in fact equivalent when we set \u03b7 = 1.\nThe first main change is that we will split the computation of q(s, a) into two separate parts, such that q (w,\u03be) (s, a) = w a x \u03be (s). This is equivalent to the previous algorithm: we have just labeled separate subsets of parameters as (w, \u03be) rather than merging all of them into a single vector w, and we have labeled the last hidden layer as x(s). We keep separate traces for these subset (lines 11 and 12), but this is equivalent to keeping one big trace for the combined set. This split in parameters helps avoid learning an expected trace for the full trace, which has millions of elements. Instead, we only learn expectations for traces corresponding to the last layer, denoted e w . Importantly, the function z \u03b8 (s, a) should condition on both state and action. This was implemented as a tensor \u03b8 \u2208 R |A|\u00d7|A|\u00d7|x| , such that its tensor multiplication with the features x(s) yields a |A| \u00d7 |A| matrix Z. Then, we interpret the vector z a = [Z] a as the approximation to the expected trace E [ e t | S t = s, A t = a ], and update it accordingly, using a squared loss (and, again, ADAM-ifying the update before applying it to the parameters). The step size for the expected trace update was always \u03b2 = 0.1 in our experiments, and the expected trace loss was not back-propagated into the feature representation. This can be done, but we leave any investigation of this for future work, as it would present a conflating factor for our experiments, because the expected trace update would then serve as an additional learning signal for the features that are also used for the value approximations.", "publication_ref": ["b2", "b23", "b5", "b39", "b23", "b29", "b23", "b27", "b18", "b23", "b6", "b12"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Appendix Proof of Lemma 1", "text": "We start with a formal definition of a Markov state: Definition 1. Markov property: we say a state s is Markov if p(R t+1 , S t+1 |A t , S t , R t\u22121 , A t\u22121 , S t\u22121 ...) = p(R t+1 , S t+1 |A t , S t ).\nNext, we show that a Markov state implies a similar property for the transition probabilities induced by a policy \u03c0: Property 1. Let p \u03c0 be the transition probabilities induced by policy \u03c0. Then, if s is Markov, we have that\nProof.\n= a \u03c0(a|S t )p(S t+1 , R t+1 |A t = a, S t )da.\n(Markov property)", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Natural gradient works efficiently in learning", "journal": "Neural computation", "year": "1998", "authors": "S I Amari"}, {"ref_id": "b1", "title": "Successor features for transfer in reinforcement learning", "journal": "", "year": "2017", "authors": "A Barreto; W Dabney; R Munos; J J Hunt; T Schaul; H P Van Hasselt; D Silver"}, {"ref_id": "b2", "title": "The Arcade Learning Environment: An Evaluation Platform for General Agents", "journal": "J. Artif. Intell. Res. (JAIR)", "year": "2013", "authors": "M G Bellemare; Y Naddaf; J Veness; M Bowling"}, {"ref_id": "b3", "title": "Dynamic Programming", "journal": "Princeton University Press", "year": "1957", "authors": "R Bellman"}, {"ref_id": "b4", "title": "Neuro-dynamic Programming", "journal": "Athena Scientific", "year": "1996", "authors": "D P Bertsekas; J N Tsitsiklis"}, {"ref_id": "b5", "title": "JAX: composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "J Bradbury; R Frostig; P Hawkins; M J Johnson; C Leary; D Maclaurin; S Wanderman-Milne"}, {"ref_id": "b6", "title": "JAX: composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "J Bradbury; R Frostig; P Hawkins; M J Johnson; C Leary; D Maclaurin; S Wanderman-Milne"}, {"ref_id": "b7", "title": "Geometric Insights into the Convergence of Non-linear TD Learning", "journal": "", "year": "2020", "authors": "D Brandfonbrener; J Bruna"}, {"ref_id": "b8", "title": "The convergence of TD(\u03bb) for general lambda", "journal": "Machine Learning", "year": "1992", "authors": "P Dayan"}, {"ref_id": "b9", "title": "Improving generalization for temporal difference learning: The successor representation", "journal": "Neural Computation", "year": "1993", "authors": "P Dayan"}, {"ref_id": "b10", "title": "Benchmarking deep reinforcement learning for continuous control", "journal": "", "year": "2016", "authors": "Y Duan; X Chen; R Houthooft; J Schulman; P Abbeel"}, {"ref_id": "b11", "title": "Sigmoidweighted linear units for neural network function approximation in reinforcement learning", "journal": "Neural Networks", "year": "2018", "authors": "S Elfwing; E Uchibe; K Doya"}, {"ref_id": "b12", "title": "Haiku: Sonnet for JAX", "journal": "", "year": "2020", "authors": "T Hennigan; T Cai; T Norman; I Babuschkin"}, {"ref_id": "b13", "title": "Rainbow: Combining Improvements in Deep Reinforcement Learning", "journal": "AAAI", "year": "2018", "authors": "M Hessel; J Modayil; H P Van Hasselt; T Schaul; G Ostrovski; W Dabney; D Horgan; B Piot; M Azar; D Silver"}, {"ref_id": "b14", "title": "Distributed Prioritized Experience Replay", "journal": "", "year": "2018", "authors": "D Horgan; J Quan; D Budden; G Barth-Maron; M Hessel; H P Van Hasselt; D Silver"}, {"ref_id": "b15", "title": "An Off-policy Policy Gradient Theorem Using Emphatic Weightings", "journal": "Curran Associates, Inc", "year": "2018", "authors": "E Imani; E Graves; M White; S Bengio; H Wallach; H Larochelle; K Grauman; N Cesa-Bianchi; R Garnett"}, {"ref_id": "b16", "title": "Planning and Acting in Partially Observable Stochastic Domains", "journal": "", "year": "1995", "authors": "L P Kaelbling; M L Littman; A R Cassandra"}, {"ref_id": "b17", "title": "Recurrent experience replay in distributed reinforcement learning", "journal": "", "year": "2018", "authors": "S Kapturowski; G Ostrovski; J Quan; R Munos; W Dabney"}, {"ref_id": "b18", "title": "A method for stochastic optimization", "journal": "", "year": "2015", "authors": "D P Kingma; J B Adam"}, {"ref_id": "b19", "title": "Self-improving reactive agents based on reinforcement learning, planning and teaching", "journal": "Machine learning", "year": "1992", "authors": "L Lin"}, {"ref_id": "b20", "title": "Second-order optimization for neural networks", "journal": "", "year": "2016", "authors": "J Martens"}, {"ref_id": "b21", "title": "Steps Toward Artificial Intelligence", "journal": "McGraw-Hill", "year": "1963", "authors": "M Minsky"}, {"ref_id": "b22", "title": "Asynchronous Methods for Deep Reinforcement Learning", "journal": "", "year": "2016", "authors": "V Mnih; A P Badia; M Mirza; A Graves; T Lillicrap; T Harley; D Silver; K Kavukcuoglu"}, {"ref_id": "b23", "title": "Human-level control through deep reinforcement learning", "journal": "Nature", "year": "2015", "authors": "V Mnih; K Kavukcuoglu; D Silver; A A Rusu; J Veness; M G Bellemare; A Graves; M Riedmiller; A K Fidjeland; G Ostrovski; S Petersen; C Beattie; A Sadik; I Antonoglou; H King; D Kumaran; D Wierstra; S Legg; D Hassabis"}, {"ref_id": "b24", "title": "Prioritized Sweeping: Reinforcement Learning with less Data and less Time", "journal": "Machine Learning", "year": "1993", "authors": "A W Moore; C G Atkeson"}, {"ref_id": "b25", "title": "Approximate Temporal Difference Learning is a Gradient Descent for Reversible Policies", "journal": "", "year": "2018", "authors": "Y Ollivier"}, {"ref_id": "b26", "title": "Efficient dynamic programming-based learning for control", "journal": "", "year": "1993", "authors": "J Peng"}, {"ref_id": "b27", "title": "Incremental Multi-step Q-learning", "journal": "Machine Learning", "year": "1996", "authors": "J Peng; R J Williams"}, {"ref_id": "b28", "title": "Source Traces for Temporal Difference Learning", "journal": "AAAI Press", "year": "2018", "authors": "S Pitis"}, {"ref_id": "b29", "title": "Observe and look further: Achieving consistent performance on Atari", "journal": "", "year": "2018", "authors": "T Pohlen; B Piot; T Hester; M G Azar; D Horgan; D Budden; G Barth-Maron; H P Van Hasselt; J Quan; M Ve\u010der\u00edk; M Hessel; R Munos; O Pietquin"}, {"ref_id": "b30", "title": "Adaptive critic designs", "journal": "IEEE Transactions on Neural Networks", "year": "1997", "authors": "D V Prokhorov; D C Wunsch"}, {"ref_id": "b31", "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming", "journal": "John Wiley & Sons, Inc", "year": "1994", "authors": "M L Puterman"}, {"ref_id": "b32", "title": "Neural Fitted Q Iteration -First Experiences with a Data Efficient Neural Reinforcement Learning Method", "journal": "Springer", "year": "2005", "authors": "M Riedmiller; J Gama; R Camacho; P Brazdil; A Jorge; L Torgo"}, {"ref_id": "b33", "title": "Prioritized Experience Replay", "journal": "", "year": "2016", "authors": "T Schaul; J Quan; I Antonoglou; D Silver"}, {"ref_id": "b34", "title": "Mastering the game of Go with deep neural networks and tree search", "journal": "Nature", "year": "2016", "authors": "D Silver; A Huang; C J Maddison; A Guez; L Sifre; G Van Den Driessche; J Schrittwieser; I Antonoglou; V Panneershelvam; M Lanctot"}, {"ref_id": "b35", "title": "Reinforcement Learning with replacing eligibility traces", "journal": "Machine Learning", "year": "1996", "authors": "S P Singh; R S Sutton"}, {"ref_id": "b36", "title": "Temporal Credit Assignment in Reinforcement Learning", "journal": "", "year": "1984", "authors": "R S Sutton"}, {"ref_id": "b37", "title": "Learning to predict by the methods of temporal differences", "journal": "Machine learning", "year": "1988", "authors": "R S Sutton"}, {"ref_id": "b38", "title": "Integrated architectures for learning, planning, and reacting based on approximating dynamic programming", "journal": "", "year": "1990", "authors": "R S Sutton"}, {"ref_id": "b39", "title": "Reinforcement Learning: An Introduction", "journal": "The MIT press", "year": "2018", "authors": "R S Sutton; A G Barto"}, {"ref_id": "b40", "title": "An Emphatic Approach to the Problem of Off-policy Temporal-Difference Learning", "journal": "Journal of Machine Learning Research", "year": "2016", "authors": "R S Sutton; A R Mahmood; M White"}, {"ref_id": "b41", "title": "Policy gradient methods for reinforcement learning with function approximation", "journal": "", "year": "2000", "authors": "R S Sutton; D Mcallester; S Singh; Y Mansour"}, {"ref_id": "b42", "title": "Practical Issues in Temporal Difference Learning", "journal": "Morgan Kaufmann", "year": "1992", "authors": "G Tesauro"}, {"ref_id": "b43", "title": "TD-Gammon, a self-teaching backgammon program, achieves master-level play", "journal": "Neural computation", "year": "1994", "authors": "G J Tesauro"}, {"ref_id": "b44", "title": "Asynchronous stochastic approximation and Q-learning", "journal": "Machine Learning", "year": "1994", "authors": "J N Tsitsiklis"}, {"ref_id": "b45", "title": "An analysis of temporal-difference learning with function approximation", "journal": "Springer", "year": "1997", "authors": "J N Tsitsiklis; B Van Roy; H P Van Hasselt"}, {"ref_id": "b46", "title": "Learning values across many orders of magnitude", "journal": "", "year": "2016-12-05", "authors": "H P Van Hasselt; A Guez; M Hessel; V Mnih; D Silver"}, {"ref_id": "b47", "title": "Deep reinforcement learning with double Q-Learning", "journal": "", "year": "2016", "authors": "H P Van Hasselt; A Guez; D Silver"}, {"ref_id": "b48", "title": "When to use parametric models in reinforcement learning?", "journal": "", "year": "2019", "authors": "H P Van Hasselt; M Hessel; J Aslanides"}, {"ref_id": "b49", "title": "Off-policy TD(\u03bb) with a true online equivalence", "journal": "", "year": "2014", "authors": "H P Van Hasselt; A R Mahmood; R S Sutton"}, {"ref_id": "b50", "title": "Planning by Prioritized Sweeping with Small Backups", "journal": "", "year": "2013", "authors": "H P Van Hasselt; J Quan; M Hessel; Z Xu; D Borsa; A Barreto; H P Van Hasselt; R S. ; H Sutton; R S Sutton"}, {"ref_id": "b51", "title": "True online TD(\u03bb)", "journal": "", "year": "2014", "authors": "H Van Seijen; R S Sutton"}, {"ref_id": "b52", "title": "International Conference on Machine Learning", "journal": "", "year": "", "authors": ""}, {"ref_id": "b53", "title": "Dueling Network Architectures for Deep Reinforcement Learning", "journal": "", "year": "2016", "authors": "Z Wang; N De Freitas; T Schaul; M Hessel; H P Van Hasselt; M Lanctot"}, {"ref_id": "b54", "title": "A menu of designs for reinforcement learning over time", "journal": "Neural networks for control", "year": "1990", "authors": "P J Werbos"}, {"ref_id": "b55", "title": "Learning to perceive and act by trial and error", "journal": "Machine Learning", "year": "1991", "authors": "S D Whitehead; D H Ballard"}, {"ref_id": "b56", "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "journal": "Machine Learning", "year": "1992", "authors": "R J Williams"}, {"ref_id": "b57", "title": "Generalized off-policy actor-critic", "journal": "", "year": "2001", "authors": "S Zhang; W Boehmer; S Whiteson"}], "figures": [{"figure_label": "4", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 4 :4Figure 4: Prediction errors in the multi-chain. ET(\u03bb) (orange) consistently outperformed TD(\u03bb) (blue). Shaded areas depict standard errors across 10 seeds.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 5 :5Figure 5: Comparing value error with linear function approximation a) as function of the number of branches (left), b) as function of \u03bb (center two plots) and c) as function of \u03b7 (right). The left three plots show comparisons of TD(\u03bb) (blue) and ET(\u03bb) (orange), showing ET(\u03bb) attained lower prediction errors.The right plot interpolates between these algorithms via ET(\u03bb, \u03b7), from ET(\u03bb) = ET(\u03bb, 0) to ET(\u03bb, 1) = TD(\u03bb), with \u03bb = 0.9 (corresponding to a vertical slice indicated in the second plot).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 6 :6Figure 6: Performance of Q(\u03bb) (\u03b7 = 1, blue) and QET(\u03bb) (\u03b7 = 0, orange) on Pong and Ms.Pac-Man for various learning rates. Shaded regions show standard error across 10 random seeds. All results are for \u03bb = 0.95. Further implementation details and hyper-parameters are in the appendix.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "e = 0 3: observe initial state S 4: pick action A \u223c \u03c0(q w (S)) 5: v \u2190 max a q w (S, a) 6: \u03b3 = 0 7: repeat 8: take action A, observe R, \u03b3 and S # \u03b3 = 0 on a terminating transition 9: v \u2190 max a q w (S , a) 10: \u03b4 \u2190 R + \u03b3v \u2212 v 11: e \u2190 \u03b3\u03bbe + \u2207 w q w (S, A) 12: \u2206w \u2190 \u03b4e + (v \u2212 q w (S, A))\u2207 w q w (S, A)", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Algorithm 33QET(\u03bb)    1: initialise w, \u03be, \u03b8 2: initialise e = 0, y = 0 3: observe initial state S 4: pick action A \u223c \u03c0(q(S))5: v \u2190 max a q (w,\u03be) (S , a) # q (w,\u03be) (s, a) = w a x \u03be (s), where w = (w 1 , . . . , w |A| ) 6: \u03b3 = 0 7: repeat 8: take action A, observe R, \u03b3 and S # \u03b3 = 0 on any terminating transition 9: v \u2190 max a q (w,\u03be) (S , a) 10: \u03b4 \u2190 R + \u03b3v \u2212 v 11: e w \u2190 \u03b3\u03bby + \u2207 w q (w,\u03be) (S, A) 12: e \u03be \u2190 \u03b3\u03bbe \u03be + \u2207 \u03be q (w,\u03be) (S, A) 13: \u2206w \u2190 \u03b4e w + (v \u2212 q (w,\u03be) (S, A))\u2207 w q (w,\u03be) (S, A) 14: \u2206\u03be \u2190 \u03b4e \u03be + (v \u2212 q (w,\u03be) (S, A))\u2207 \u03be q (w,\u03be) (S, A) 15: \u2206\u03b8 \u2190 \u2207 \u03b8 e \u03be \u2212 z \u03b8 (S, A) 1 \u2212 \u03b7)z \u03b8 (s, a) + \u03b7e w 23: until done", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "G t \u2261 G(\u03c4 t:T ) = R t+1 + \u03b3 t+1 R t+2 + \u03b3 t+1 \u03b3 t+2 R t+3 + . . . = T i=1 \u03b3 (i\u22121) t+i R t+i ,", "formula_coordinates": [2.0, 54.61, 253.02, 237.27, 45.4]}, {"formula_id": "formula_1", "formula_text": "(i) t = i k=1 \u03b3 t+k . The value v \u03c0 (s) = E [ G t |S t = s, \u03c0 ] of", "formula_coordinates": [2.0, 54.0, 326.74, 238.5, 23.74]}, {"formula_id": "formula_2", "formula_text": "\u2206w t \u2261 \u03b1(R t+1 + \u03b3 t+1 G(\u03c4 t+1:T ) \u2212 v w (S t ))\u2207 w v w (S t ) .", "formula_coordinates": [2.0, 57.34, 501.18, 231.83, 9.65]}, {"formula_id": "formula_3", "formula_text": "\u2206w t \u2261 \u03b1\u03b4 t \u2207 w v w (S t ) , (1", "formula_coordinates": [2.0, 97.7, 567.9, 190.93, 9.65]}, {"formula_id": "formula_4", "formula_text": ")", "formula_coordinates": [2.0, 288.63, 568.22, 3.87, 8.64]}, {"formula_id": "formula_5", "formula_text": "where \u03b4 t \u2261 R t+1 + \u03b3 t+1 v w (S t+1 ) \u2212 v w (S t ) ,", "formula_coordinates": [2.0, 78.03, 581.84, 190.43, 9.65]}, {"formula_id": "formula_6", "formula_text": "G \u03bb (\u03c4 t:T ) = R t+1 +\u03b3 t+1 (1\u2212\u03bb)v w (S t+1 )+\u03bbG \u03bb (\u03c4 t+1:T ) .", "formula_coordinates": [2.0, 54.0, 636.68, 238.5, 11.72]}, {"formula_id": "formula_7", "formula_text": "z(s) \u2261 E [ e t | S t = s ] ,", "formula_coordinates": [2.0, 388.85, 320.64, 99.79, 9.68]}, {"formula_id": "formula_8", "formula_text": "\u03b8 t+1 = \u03b8 t + \u2206\u03b8 t , where \u2206\u03b8 t = \u2212\u03b2 \u2202 \u2202\u03b8 1 2 (e t \u2212 z \u03b8 (S t )) (e t \u2212 z \u03b8 (S t )) = \u03b2 \u2202z \u03b8 (S t ) \u2202\u03b8 (e t \u2212 z \u03b8 (S t )) ,", "formula_coordinates": [2.0, 344.46, 508.06, 188.58, 61.56]}, {"formula_id": "formula_9", "formula_text": "\u2202z \u03b8 (St)", "formula_coordinates": [2.0, 347.17, 577.51, 27.32, 6.79]}, {"formula_id": "formula_10", "formula_text": "9: \u03b8 \u2190 \u03b8 + \u03b2 \u2202z \u03b8 (", "formula_coordinates": [3.0, 67.35, 148.97, 106.04, 11.53]}, {"formula_id": "formula_11", "formula_text": "v(S t+1 ) \u2248 E [ G(\u03c4 t+1:T )|S t+1 ]", "formula_coordinates": [3.0, 105.18, 275.97, 126.98, 9.65]}, {"formula_id": "formula_12", "formula_text": "\u2206w t \u2261 \u03b1(R t+1 + \u03b3 t+1 G(\u03c4 t+1:T ) \u2212 v w (S t ))\u2207 t , (MC) \u2206w t \u2261 \u03b1(R t+1 + \u03b3 t+1 v w (S t+1 ) \u2212 v w (S t ))\u2207 t , (TD)", "formula_coordinates": [3.0, 62.11, 304.0, 230.39, 23.6]}, {"formula_id": "formula_13", "formula_text": "y t = (1 \u2212 \u03b7)z \u03b8 (S t ) + \u03b7 \u03b3 t \u03bby t\u22121 + \u2207 w v w (S t ) . (3)", "formula_coordinates": [3.0, 65.98, 564.52, 226.52, 9.68]}, {"formula_id": "formula_14", "formula_text": "Lemma 1. If s is Markov, then E [ \u03b4 t e t | S t = s ] = E [ \u03b4 t | S t = s ] E [ e t | S t = s ] .", "formula_coordinates": [3.0, 319.5, 198.26, 225.44, 28.07]}, {"formula_id": "formula_15", "formula_text": "E [ \u03b1 t \u03b4 t z(S t )|S t = s ] = E [ \u03b1 t \u03b4 t e t |S t = s ] and V[\u03b1 t \u03b4 t z(S t )|S t = s] \u2264 V[\u03b1 t \u03b4 t e t |S t = s] ,", "formula_coordinates": [3.0, 335.27, 321.04, 206.96, 23.63]}, {"formula_id": "formula_16", "formula_text": "E [ \u03b1 t \u03b4 t e t | S t = s ] = E [ \u03b1 t \u03b4 t | S t = s ] E [ e t | S t = s ] (Lemma 1) = E [ \u03b1 t \u03b4 t | S t = s ] z(s) = E [ \u03b1 t \u03b4 t z(S t ) | S t = s ] .(4)", "formula_coordinates": [3.0, 342.56, 402.31, 215.44, 51.52]}, {"formula_id": "formula_17", "formula_text": "E (\u03b1 t \u03b4 t z t,i ) 2 |S t = s = E \u03b1 2 t \u03b4 2 t | S t = s z 2 t,i = E \u03b1 2 t \u03b4 2 t | S t = s E [ e t,i |S t = s ] 2 = E \u03b1 2 t \u03b4 2 t | S t = s E e 2 t,i |S t = s \u2212 V[e t,i |S t = s] \u2264 E \u03b1 2 t \u03b4 2 t | S t = s E e 2 t,i | S t = s = E (\u03b1 t \u03b4 t e t,i ) 2 | S t = s ,", "formula_coordinates": [3.0, 322.27, 494.24, 224.86, 77.94]}, {"formula_id": "formula_18", "formula_text": "E [ \u03b1 t \u03b4 t z t (S t ) | S t ] = E [ \u03b1 t \u03b4 t e t | S t ]", "formula_coordinates": [4.0, 77.02, 228.0, 154.5, 9.68]}, {"formula_id": "formula_19", "formula_text": "V[\u03b1 t \u03b4 t z t (S t ) | S t ] \u2264 V[\u03b1 t \u03b4 t e t | S t ] ,", "formula_coordinates": [4.0, 81.45, 241.95, 150.07, 9.68]}, {"formula_id": "formula_20", "formula_text": "t = (1 \u2212 \u03b7)z \u03b8 (S t ) + \u03b7 \u2207 w v w (S t ), such that y t = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z \u03b8 (S t\u2212k ) + \u03b7 \u2207 w v w (S t\u2212k )] . (5)", "formula_coordinates": [4.0, 54.0, 534.7, 242.39, 49.2]}, {"formula_id": "formula_21", "formula_text": "\u03be t+1 = \u03be t + \u03b1\u03b4e \u03be t and w t+1 = w t + \u03b1\u03b4z \u0398 (S t ) ,", "formula_coordinates": [5.0, 342.86, 272.53, 218.54, 13.37]}, {"formula_id": "formula_22", "formula_text": "z(s) = E \u221e n=0 \u03bb (n) t \u03b3 (n) t x t\u2212n | S t = s ,(6)", "formula_coordinates": [6.0, 88.72, 412.78, 203.78, 30.2]}, {"formula_id": "formula_23", "formula_text": "\u03c8(s) = E \u221e n=1 \u03b3 (n\u22121) t x t+n | S t = s . (7", "formula_coordinates": [6.0, 91.96, 475.2, 196.67, 30.2]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [6.0, 288.63, 485.93, 3.87, 8.64]}, {"formula_id": "formula_25", "formula_text": "\u2206\u03b8 t \u2261 \u03b2 (\u2207 t + \u03b3 t \u03bb t y t\u22121 \u2212 z \u03b8 (S t )) \u2202z \u03b8 (S t ) \u2202\u03b8 ,(8)", "formula_coordinates": [7.0, 68.27, 430.15, 224.23, 22.31]}, {"formula_id": "formula_26", "formula_text": "Lemma 1. If s is Markov, then E [ \u03b4 t e t | S t = s ] = E [ \u03b4 t | S t = s ] E [ e t | S t = s ].", "formula_coordinates": [10.0, 54.0, 279.06, 337.58, 9.72]}, {"formula_id": "formula_27", "formula_text": "\u03b4 t = R t+1 + \u03b3 t+1 v w (S t+1 ) \u2212 v w (S t ).(10)", "formula_coordinates": [10.0, 227.52, 334.72, 330.48, 9.65]}, {"formula_id": "formula_28", "formula_text": "p \u03c0 (\u03b4 t |S t , R t , S t\u22121 , R t\u22121 , ...) = p \u03c0 (\u03b4 t |S t ), (Property 1)", "formula_coordinates": [10.0, 220.4, 377.54, 337.6, 9.65]}, {"formula_id": "formula_29", "formula_text": "E [ \u03b1 t \u03b4 t z t (S t ) | S t ] = E [ \u03b1 t \u03b4 t e t | S t ] and V[\u03b1 t \u03b4 t z t (S t ) | S t ] \u2264 V[\u03b1 t \u03b4 t e t | S t ]", "formula_coordinates": [10.0, 165.52, 503.47, 280.96, 23.63]}, {"formula_id": "formula_30", "formula_text": "E [ \u03b1 t \u03b4 t e t | S t = s ] = E [ \u03b1 t \u03b4 t | S t = s ] E [ e t | S t = s ] (as s is Markov) = E [ \u03b1 t \u03b4 t | S t = s ] E [ z t | S t = s ] (as z t = 1 n n i e t s i ) = E [ \u03b1 t \u03b4 t z t | S t = s ] .", "formula_coordinates": [10.0, 113.77, 567.01, 444.23, 40.79]}, {"formula_id": "formula_31", "formula_text": "V[\u03b1 t \u03b4 t z t,i | S t = s] = E (\u03b1 t \u03b4 t z t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t z t,i | S t = s ] 2 = E \u03b1 2 t \u03b4 2 t (z t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 E [ z t,i | S t = s ] 2 = E \u03b1 2 t \u03b4 2 t | S t = s E (z t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 E [ z t,i | S t = s ]", "formula_coordinates": [10.0, 139.0, 640.95, 329.54, 61.38]}, {"formula_id": "formula_32", "formula_text": "V[\u03b1 t \u03b4 t e t,i | S t = s] = E \u03b1 2 t \u03b4 2 t | S t = s E (e t,i ) 2 | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 E [ e t,i | S t = s ] 2", "formula_coordinates": [11.0, 139.01, 74.15, 333.48, 27.43]}, {"formula_id": "formula_33", "formula_text": "E [ z t,i | S t = s ] = E [ e t,i | S t = s ] = \u00b5 t,i", "formula_coordinates": [11.0, 218.12, 130.87, 175.25, 9.68]}, {"formula_id": "formula_34", "formula_text": "V[\u03b1 t \u03b4 t z t,i | S t = s] = E \u03b1 2 t \u03b4 2 t | S t = s E z t,i (z t,i ) T | S t = s \u2212 E [ \u03b1 t \u03b4 t | S t = s ] 2 \u00b5 2 t,i", "formula_coordinates": [11.0, 111.98, 164.63, 387.54, 13.59]}, {"formula_id": "formula_35", "formula_text": "V[\u03b1 t \u03b4 t z t,i | S t = s] \u2212 V[\u03b1 t \u03b4 t e t,i | S t = s] = E \u03b1 2 t \u03b4 2 t | S t = s E z t,i (z t,i ) T | S t = s \u2212 E e t,i (e t,i ) T | S t = s \u22640, from definition of zt,i \u2264 0 ,", "formula_coordinates": [11.0, 134.49, 201.61, 343.03, 41.59]}, {"formula_id": "formula_36", "formula_text": "i E (z t,i ) 2 | S t = s = E (e t,i ) 2 | S t = s \u21d2 V(z t,i |S t = s) = V(e t,i |S t = s), but V(z t,i |S t = s) = 1 ns V(e t,i |S t = s", "formula_coordinates": [11.0, 61.75, 266.76, 493.14, 13.47]}, {"formula_id": "formula_37", "formula_text": "t,i |S t = s) = V(e t,i |S t = s) = 0; or ii E \u03b1 2 t \u03b4 2 t | S t = s = 0 \u21d2 \u03b4 t = 0", "formula_coordinates": [11.0, 58.98, 291.83, 417.68, 24.84]}, {"formula_id": "formula_38", "formula_text": "y t = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z \u03b8 (S t\u2212k ) + \u03b7 \u2207 w v w (S t\u2212k )] .(5)", "formula_coordinates": [11.0, 184.81, 480.07, 373.19, 30.55]}, {"formula_id": "formula_39", "formula_text": "y t = (1 \u2212 \u03b7)z t + \u03b7(\u03b3 t \u03bb t y t\u22121 + x t ) = [(1 \u2212 \u03b7)z t + \u03b7x t ] + \u03b7\u03b3 t \u03bb t y t\u22121 = [(1 \u2212 \u03b7)z t + \u03b7x t ] + \u03b7\u03b3 t \u03bb t [(1 \u2212 \u03b7)z t\u22121 + \u03b7x t\u22121 ] + \u03b7 2 \u03b3 t \u03bb t \u03b3 t\u22121 \u03bb t\u22121 y t\u22122 = (1 \u2212 \u03b7) z t + \u03b7\u03b3 t \u03bb t z t\u22121 + \u03b7 2 \u03b3 t \u03bb t \u03b3 t\u22121 \u03bb t\u22121 z t\u22122 + \u2022 \u2022 \u2022 + + \u03b7 x t + \u03b7\u03b3 t \u03bb t x t\u22121 + \u03b7 2 \u03b3 t \u03bb t \u03b3 t\u22121 \u03bb t\u22121 x t\u22122 + \u2022 \u2022 \u2022 = (1 \u2212 \u03b7) t k=0 (\u03b7\u03b3\u03bb) k z t\u2212k + \u03b7 t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z t\u2212k + \u03b7x t\u2212k ]", "formula_coordinates": [11.0, 145.13, 545.95, 321.23, 141.87]}, {"formula_id": "formula_40", "formula_text": "y t = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z \u03b8 (S t\u2212k ) + \u03b7x(S t\u2212k )] = t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)\u0398x(S t\u2212k ) + \u03b7x(S t\u2212k )] (11) = [(1 \u2212 \u03b7)\u0398 + \u03b7I] t k=0 (\u03b7\u03b3\u03bb) k x(S t\u2212k )", "formula_coordinates": [12.0, 206.74, 140.43, 351.26, 99.96]}, {"formula_id": "formula_42", "formula_text": "E [ \u03b4 t y t ] = E y t (R t+1 + \u03b3x(S t+1 ) w * \u2212 x(S t ) w * ) = 0 .", "formula_coordinates": [12.0, 189.94, 297.18, 226.31, 23.91]}, {"formula_id": "formula_43", "formula_text": "w * = E y t (\u03b3x(S t+1 ) \u2212 x(S t )) \u22121 E [ y t R t+1 ] .", "formula_coordinates": [12.0, 200.56, 354.29, 210.88, 14.1]}, {"formula_id": "formula_44", "formula_text": "w * = E [(1 \u2212 \u03b7)\u0398 + \u03b7I] e \u03bb\u03b7 t (\u03b3x(S t+1 ) \u2212 x(S t )) \u22121 E [(1 \u2212 \u03b7)\u0398 + \u03b7I] e \u03bb\u03b7 t R t+1 = E e \u03bb\u03b7 t (\u03b3x(S t+1 ) \u2212 x(S t )) \u22121 [(1 \u2212 \u03b7)\u0398 + \u03b7I] \u22121 [(1 \u2212 \u03b7)\u0398 + \u03b7I] E e \u03bb\u03b7 t R t+1", "formula_coordinates": [12.0, 122.39, 404.67, 360.36, 41.51]}, {"formula_id": "formula_45", "formula_text": "\u2022 \u03b7 = 1 \u21d2 y t = t k=0 (\u03b3\u03bb) k x t\u2212k (instantaneous trace for TD(\u03bb)) \u2022 \u03b7 = 0 \u21d2 y t = t k=0 (\u03b7\u03b3\u03bb) k z t\u2212k = z t (expected trace for TD(\u03bb))", "formula_coordinates": [12.0, 55.49, 545.23, 275.73, 37.11]}, {"formula_id": "formula_46", "formula_text": "E [ \u03b1 t \u03b4 t y(S t )|S t = s ] = E \u03b1 t \u03b4 t e \u03bb t |S t = s ,", "formula_coordinates": [12.0, 212.44, 668.47, 187.12, 12.69]}, {"formula_id": "formula_47", "formula_text": "E [ yt ] = E t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)z t\u2212k + \u03b7x t\u2212k ] = E t k=0 (\u03b7\u03b3\u03bb) k [(1 \u2212 \u03b7)E [ (x t\u2212k + \u03b3\u03bbz t\u2212k\u22121 ) ] + \u03b7x t\u2212k ] = E \uf8ee \uf8ef \uf8ef \uf8f0 t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7)\u03b3\u03bb t\u22121 k=0 (\u03b7\u03b3\u03bb) k z t\u2212k\u22121 E[ (x t\u2212k\u22121 +\u03b3\u03bbz t\u2212k\u22122 ) ] \uf8f9 \uf8fa \uf8fa \uf8fb = E \uf8ee \uf8ef \uf8ef \uf8f0 t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7)\u03b3\u03bb t\u22121 k=0 (\u03b7\u03b3\u03bb) k x t\u2212k\u22121 + (1 \u2212 \u03b7)(\u03b3\u03bb) 2 t\u22122 k=0 (\u03b7\u03b3\u03bb) k z t\u2212k\u22122 E[ x t\u2212k\u22122 +\u03b3\u03bbz t\u2212k\u22123 ] \uf8f9 \uf8fa \uf8fa \uf8fb = E t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7) t\u22121 i=1 (\u03b3\u03bb) i t\u2212i k=0 (\u03b7\u03b3\u03bb) k x t\u2212k\u2212i", "formula_coordinates": [13.0, 87.19, 76.52, 436.08, 186.64]}, {"formula_id": "formula_48", "formula_text": "E [ yt ] = E t k=0 (\u03b7\u03b3\u03bb) k x t\u2212k + (1 \u2212 \u03b7) t\u22121 i=1 (\u03b3\u03bb) i t\u2212i k=0 (\u03b7\u03b3\u03bb) k x t\u2212k\u2212i = E xt + t k=1 x t\u2212k (\u03b7\u03b3\u03bb) k + (1 \u2212 \u03b7) k i=1 (\u03b3\u03bb) i \u2022 (\u03b3\u03bb\u03b7) k\u2212i = E xt + t k=1 x t\u2212k (\u03b3\u03bb) k \u03b7 k + (1 \u2212 \u03b7) k i=1 \u03b7 k\u2212i = E xt + t k=1 x t\u2212k (\u03b3\u03bb) k \u03b7 k + (1 \u2212 \u03b7) 1 \u2212 \u03b7 k (1 \u2212 \u03b7) = E xt + t k=1 x t\u2212k (\u03b3\u03bb) k = E t k=0 (\u03b3\u03bb) k x t\u2212k Thus E [ yt ] = E t k=0 (\u03b3\u03bb) k x t\u2212k = E e \u03bb t", "formula_coordinates": [13.0, 63.96, 291.73, 371.16, 207.77]}, {"formula_id": "formula_49", "formula_text": "E [ \u03b1t\u03b4ty(St)|St = s ] = E [ \u03b1t\u03b4t|St = s ] E [ y(St)|St = s ] = E [ \u03b1t\u03b4t|St = s ] z \u03bb * (s) = E [ \u03b1t\u03b4t|St = s ] E e \u03bb t |St = s = E \u03b1t\u03b4te \u03bb t |St = s .", "formula_coordinates": [13.0, 194.73, 521.57, 222.54, 61.73]}], "doi": ""}