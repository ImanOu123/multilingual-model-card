{"title": "Multi-party Multimodal Conversations Between Patients, Their Companions, and a Social Robot in a Hospital Memory Clinic", "authors": "Angus Addlesee; Neeraj Cherakara; Nivan Nelson; Daniel Hern\u00e1ndez Garc\u00eda; Nancie Gunson; Weronika Siei\u0144ska; Christian Dondrup; Oliver Lemon", "pub_date": "", "abstract": "We have deployed an LLM-based spoken dialogue system in a real hospital. The ARI social robot embodies our system, which patients and their companions can have multi-party conversations with together. In order to enable this multi-party ability, multimodality is critical. Our system, therefore, receives speech and video as input, and generates both speech and gestures (arm, head, and eye movements). In this paper, we describe our complex setting and the architecture of our dialogue system. Each component is detailed, and a video of the full system is available with the appropriate components highlighted in real-time. Our system decides when it should take its turn, generates human-like clarification requests when the patient pauses mid-utterance, answers indomain questions (grounding to the in-prompt knowledge), and responds appropriately to outof-domain requests (like generating jokes or quizzes). This latter feature is particularly remarkable as real patients often utter unexpected sentences that could not be handled previously.", "sections": [{"heading": "Introduction", "text": "Both commercial and research spoken dialogue systems (SDSs), conversational agents, and social robots have been designed with a focus on dyadic interactions. That is, a two-party conversation between one individual user and a single system/robot. These are only guaranteed in specific settings, like people interacting with Siri on their own phone, or with Amazon Alexa in single-occupant homes. When Alexa is in a family home, their lack of multiparty capabilities are apparent (Porcheron et al., 2018), but this becomes a critical limitation when deploying social robots in public spaces. Families visit museums and libraries, groups of friends roam shopping malls and bars, and couples travel through airports and support each other at hospital appointments. Social robots are being deployed and tested in all of these settings (Al Moubayed et al., 2012; Figure 1: Hospital memory clinic visitors using our SDS on the ARI social robot (Cooper et al., 2020). Keizer et al., 2014;Furhat Robotics, 2015;Foster et al., 2019;Vlachos et al., 2020;Gunson et al., 2022), in which multi-party conversations (MPCs), involving people talking to both the robot and each other, do commonly occur (see Figure 1).\nTasks that are typically trivial in the dyadic setting become considerably more complex when conversing with multiple users (Traum, 2004;Gu et al., 2022b): (1) The speaker is no longer simply the other person, so the meaning of the dialogue depends on recognising who said each utterance (see (A) in Table 1); (2) addressee recognition is similarly more complicated (see Sec 3.2) as people address each other, the robot, and groups; and (3) response generation depends on who said what to whom, relying on the semantic content and surrounding multi-party context. To make things even more difficult, MPCs provide additional unique challenges that are underexplored. Dyadic SDSs must identify and answer the user's goals to be practically useful. In MPCs, users can provide another person's goal (see (B) (Schauer et al., 2023). Examples D & E: (Addlesee, 2024). each other's goals (see (C) in Table 1), and even share goals (see (D) in Table 1, (Eshghi and Healey, 2016)). We therefore established multi-party goaltracking in previous work (Addlesee et al., 2023d).\nBoth dyadic and multi-party human conversations are subtly guided and supported by visual cues (Goodwin, 1981;Bavelas and Gerwing, 2011;Addlesee et al., 2019). Screwing-up of the face, brow furrows, looking up, nodding, smiling, eyecontact, etc... though crucial, are lost completely by current commercial SDSs. Due to the added complexity of MPCs, visual cues are even more crucial (Moujahid et al., 2022). For example, It is ambiguous who U1 is addressing in Example (C) in Table 1 because gaze behaviour is essential (Auer, 2018), yet missing.\nIn this paper, we present our multi-party multimodal SDS embodied by the ARI social robot (Cooper et al., 2020) that is currently deployed in a hospital, and interacts with memory clinic patients and their companions. It can give directions, provide light entertainment (like quizzes and jokes), and inform people about bus times, the cafe menu, and more. Large language models (LLMs) have revolutionised our field, they are excellent at language understanding, and this includes MPCs (Hu et al., 2019;Gu et al., 2021Gu et al., , 2022aZhong et al., 2022) as their pre-training includes scripts and meeting transcripts containing multiple people. They also hold a wealth of general knowledge, enabling abilities like question answering (QA), joke telling, and playing quizzes. Our SDS is therefore LLM-based to provide a state-of-the-art experience for hospital patients. We first describe our setting, and then detail each module of our system's architecture in Figure 2. A demo video of this system is available on YouTube 1 .", "publication_ref": ["b44", "b11", "b21", "b37", "b24", "b49", "b32", "b48", "b29", "b45", "b1", "b23", "b9", "b27", "b15", "b6", "b40", "b14", "b21", "b35", "b30", "b28", "b52"], "figure_ref": ["fig_0"], "table_ref": ["tab_1", "tab_1", "tab_1", "tab_1"]}, {"heading": "The Hospital Setting", "text": "Dementia diagnosis is a stressful process. Patients typically spend entire days at the hospital with a friend or family member for support. The hours are filled with multiple appointments, but a large portion of the day is also spent waiting anxiously for test results or the next appointment. Our goal is to provide a system that is both practically useful, but also entertaining, to provide participants with some light distraction from their otherwise stressful day. The research staff at the hospital are our collaborators on the SPRING project, and they run the experiments with volunteer patients, their companions, and the ARI robot (see Figure 1). The EU's H2020 SPRING project aims to explore \"how to create robots able to move, see, hear and communicate with several actors, in complex and unstructured populated spaces\" 2 . We are one of eight project partners, and our focus is the SDS. Other partners work on collision prevention during navigation, route planning, ego-noise suppression, gaze tracking, running live experiments with patients in the hospital, and more.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Dialogue System", "text": "Our system presented in this paper has been iteratively improved through regular user tests and interviews with patients visiting the hospital memory clinic. The initial system (Gunson et al., 2022) was developed before the recent LLM advance, relying on a 'traditional' modular architecture based upon Alana V2 (Papaioannou et al., 2017;Curry et al., 2018). As patients were usually accompanied by a companion, the lack of multi-party capabilities proved problematic. It interrupted users as it responded to every turn, not allowing them to talk to each other at any point. We therefore designed and ran a multi-party data collection in a wizard-of-oz setup (Addlesee et al., 2023c,d), and have used this data to motivate and evaluate the system we present here. Not only is this new system multi-party and multimodal, it improves QA accuracy, improves accessibility to people with dementia (Addlesee, 2024), and enables added functionality. Where previously, we had to specifically design the system to tell jokes and run entertaining quizzes Schauer et al., 2023), LLMs can now handle this inherently due to their world knowledge. Most importantly, both users and the hospital staff have reported that the user experience has improved drastically. In this section, we detail each system component illustrated in Figure 2.", "publication_ref": ["b32", "b43", "b22", "b1", "b45"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Robot Platform", "text": "Our system is deployed on the ARI humanoid robot, designed for use as a socially assistive companion (Cooper et al., 2020). ARI is 1.65m tall, has a mobile base, a touch-screen on the torso, movable arms to gesture, and a head with LCD eyes that enable gaze behaviour. A photo of ARI can be seen in Figure 1 and component (A) in Figure 2. It is equipped with a ReSpeaker Mic v2.0 array 3 , an RGB camera (in the head), and a 180\u00ba fish-eye camera (in the chest) allowing us to capture and record the audio and video of the whole interaction from the robot's perspective. The robot verbalises given responses using Acapela Text-To-Speech 4 .", "publication_ref": ["b21"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Detecting the User's Addressee", "text": "Dyadic SDSs reply to every user turn. As discussed in Section 1, people talk to both the robot and each other in MPCs. If the robot replied to U1 in Example (C), Table 1, then it would have interrupted U2. The addressee of U1's turn is ambiguous given the text alone. Alternatively, if the user said \"Do you want to sit down?\", it would be clear that ARI is not being addressed from just the text. In order to measure how effective gaze information is to determine the addressee in our specific setting, we annotated real MPCs collected in the hospital. We have video recordings of the interactions with the robot's cameras and an external camera. Using both the video and audio, the gold addressee of each turn was annotated along with whether the user was looking at ARI or not.\nUsing the Vicuna-13b-v1.5 LLM (Chiang et al., 2023), we created two addressee detectors. In one case, we prompted it with the dialogue history and current user's turn. In the second case, we added whether the user is looking at ARI or not. Both prompts asked the LLM whether the user \"is currently addressing the other person or the robot\" 5 .\nAddressee detection accuracy increased from 53.35% to 85.40% when given the gaze information. Reducing interruption of the user is a huge improvement, but we do not want the robot to start ignoring people entirely. That is, we do not want the patient to address the robot and get no response. It is therefore critical to maximise recall, which increased from 31.33% to 91.00% when provided gaze information. A gaze detection model (Tonini et al., 2023) is used to get information on when a speaker is looking at ARI, and this is fed into component (B) in Figure 2.", "publication_ref": ["b18", "b47"], "figure_ref": ["fig_0"], "table_ref": ["tab_1"]}, {"heading": "Generating Clarification Requests", "text": "In a hospital's memory clinic, voice accessibility is critical (Addlesee, 2023), and people with dementia pause more frequently and for longer durations mid-sentence due to word-finding problems (Boschi et al., 2017;Slegers et al., 2018). These pauses are mistaken as end of turn by the ASR, resulting in the user being interrupted with nonsense or a generic response like \"I'm sorry, I didn't understand that\". The user is forced to repeat their entire turn again, a frustrating and unnatural interaction (Nakano et al., 2007;Jiang et al., 2013;Panfili et al., 2021).\nAccessibility settings, in Siri for example (Apple, 2022), allow users to modify how long the ASR waits until it decides that a sentence is complete. This is a wonderful temporary solution for people with more progressed cognitive impairment, but it is not naturally interactive, as the user would then have to wait for long durations between every turn. Producing incremental clarification requests (iCRs) is, therefore, important for building naturally interactive SDSs (Chiyah-Garcia et al., 2023).", "publication_ref": ["b0", "b16", "b46", "b41", "b36", "b42", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "CR Corpus and Taxonomy", "text": "Corpora of interrupted sentences paired with their meaning representations were recently released (Addlesee and Damonte, 2023a,b), finding that interrupted sentence recovery pipelines reliant on CRs were best at recovering the intended meaning of the question. They did not focus on generating natural, human-like iCRs in response to partial sentences. Using a subset of their SLUICE corpus (Addlesee and Damonte, 2023a), we elicited 12 CRs from annotators for 250 interrupted questions. This new corpus SLUICE-CR, therefore, contains a total of 3,000 human CRs (Addlesee, 2024).\nAll CRs within SLUICE-CR are intended to elicit how the interlocutor would have gone on to complete their turn. Example (E) in Table 1 illustrates this. Each CR in the corpus is classified into one of four distinct categories. First, there are  1.", "publication_ref": ["b1"], "figure_ref": [], "table_ref": ["tab_1", "tab_1"]}, {"heading": "CR Results", "text": "With that taxonomy in mind, we evaluated LLMs using SLUICE-CR (Addlesee, 2024). The results relevant to the hospital deployment can be found in Table 2. The 'basic' prompt simply passed the truncated turn to each LLM with no context. The 'annotation' prompt contained the task instructions given to the human annotators, which contains CR examples, and the 'reasoning' prompt added a reason for each example (Fu et al., 2022).\nOf the models that learned to generate iCRs, GPT-4 and Vicuna-13b-v1.5 both relied more on SCRs. Llama-70b-chat generated more RCRs, opting to commonly forego the sluice entirely. Generating human-like iCRs is practically useless if they are not semantically appropriate. 85.5% of the human CRs contained a sluice, so we devised a new metric called the sluice match accuracy (SMA): measuring the percentage of model generated CRs with a wh-word that is an exact match to at least one of the wh-words in the 12 human CRs for each partial question. SMA thereby preserves semantic type ambiguity captured by the human-annotators.\nFrom these metrics alone, it is clear that GPT-4 is outstanding if data privacy is not a concern. In sensitive settings without hardware limitations, Llama-2-70b-chat is best. Given our sensitive setting with hardware limitations, we use Vicuna-13b-v1.5 as our system's core LLM. In order to handle our user's incomplete sentences, we first ask the LLM whether the turn was a complete sentence. If it is not, we use the 'reasoning' prompt to generate an iCR to create a more accessible and naturally interactive conversational system. This can be seen in the architecture in Figure 2, denoted by (C).", "publication_ref": ["b1", "b25"], "figure_ref": ["fig_0"], "table_ref": ["tab_2"]}, {"heading": "Generating Responses", "text": "Unlike older dialogue systems, we interface with our core LLM using prompts. As mentioned in Section 3.3, we are using Vicuna-13b-v1.5. We provide the hospital information in a prompt with some additional guardrails, like \"you are not qualified to give any medical advice or make medical diagnoses\" and \"you do not have access to individual patient records or schedules\". Both patients and hospital staff reported that our new LLM-based system has improved greatly, compared to our previous system (Gunson et al., 2022). In order to measure the improvement in its QA capabilities, we created a set of 100 in-domain questions that were designed to provide broad coverage of the modular system capabilities. These were a mix of hand-crafted and real questions asked by patients in our collected data. In-domain error rates, where incorrect or no information was given in response to the question, improved from 29.2% to 11.5%.\nOne huge benefit of using LLMs is their inherent ability to perform general chit-chat, tell jokes, and access a wealth of general knowledge. In the original system, we could only respond suitably to utterances that the system was pre-designed to handle -and we would attempt to respond to unexpected utterances with tips, teaching the user what the system can do (e.g. \"I'm not sure, but I can help you with directions and menu information.\"). Many of these unexpected utterances can now be handled directly by the LLM.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Grounding Responses to the Provided", "text": "In-prompt Knowledge\nCertain LLMs, like ChatGPT and Bard, are regularly asked general knowledge questions and ex-pected to understand chit-chat. General LLM evaluation has therefore focused on their world knowledge learned at pre-training. For example, the popular Hugging Face Open LLM benchmark (the de facto standard leaderboard) ranks each model based on their performance across four tasks:\n(1) The AI2 Reasoning Challenge (Clark et al., 2018), a set of grade-school science questions;\n(2) MMLU (Hendrycks et al., 2020), a set of elementary level questions covering mathematics, US history, computer science, law, and more; (3) Hel-loSwag (Zellers et al., 2019), testing whether the model can select \"what will happen next?\" given a common sense scenario and some options; and (4) TruthfulQA (Lin et al., 2022), a set of 817 questions on various topics, like law and politics. These corpora highlight the field's effort to reduce model hallucination. It is vital to clarify that they focus on hallucination reduction of outputs generated from the LLM's static world knowledge. In fact, this world knowledge can generate harmful hallucinations due to conflicts with the information given in the prompt. The text in red in Example (D) in Table 1 highlights this issue. Our prompt does not state that patients must fast before their appointment, and this response would result in a hospital patient going hungry. Other examples include how long a patient must wait for their medication to wear off before driving (Addlesee, 2024).\nTo tackle this problem, we must coax the LLM to ground its response to the in-prompt knowledge given at runtime, and not rely on non-domainspecific and potentially out-of-date knowledge learned at pre-training. To measure the impact of in-prompt grounding strategies, we used 50 questions from our project paired with a text passage. We do not always know what an LLM is trained on, and this could potentially include the website of our real hospital, so this passage described a fictitious hospital that no LLM could possibly know. We provide four prompts: Basic: The passage followed by the question. Jodie: Our prompt provides the passage as a quote by Jodie W. Jenkins, a fictitious non-celebrity name (according to Google). We then ask the LLM to answer according to Jodie. The exact pattern is this: 'Jodie W. Jenkins said \"PASSAGE\". Answer according to Jodie W. Jenkins. QUESTION'. Expert: In order to ensure any prompt-grounding benefit is not simply a result of adding \"according to\", we again provide the passage as a quote by Jodie W. Jenkins, but add \"Answer according to indicates an improvement compared to the 'basic' prompt.\nindicates a performance drop compared to the 'basic' prompt. Bold marks the best scores per model (Addlesee, 2024 UnitedHealth\" instead of Jodie W. Jenkins. Wikipedia: The Expert prompt with one word replaced. The expert name is set to \"Wikipedia\".", "publication_ref": ["b20", "b51", "b39", "b1", "b1"], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Response Grounding Results", "text": "In related work, Weller et al. (2023) measured LLM grounding to world knowledge. In order to measure how well an LLM's output was grounded to Wikipedia, they devised a metric: QUIP-score. This score is the character n-gram precision of the generated output compared to the source corpus. It is a useful metric in our case too, as we can measure how precisely each LLM's output is grounded in the given in-prompt knowledge. This focus on precision also punishes a model's output when it hallucinates -our goal here too. Using our corpus (Addlesee, 2024), we used this QUIP-score and the answer's accuracy to measure in-prompt grounding performance, as grounding is impractical if it does not preserve QA performance. Table 3 illustrates the impressive performance of our 'Jodie' prompt. The Quip-score did decrease for two of the models, but the accuracy never deteriorated, and increased by up to 28% (mean: 10%). Even though the 'Expert' and 'Wikipedia' prompts differ from the 'Jodie' prompt by just one name, they generate more text that is not contained in the given prompt (as shown by the lower Quip-scores), and these additional hallucinations result in an accuracy drop.\nOur current SDS utilises this 'Jodie' prompt in component (D) in Figure 2 to improve in-prompt grounding, reducing potential user harm.", "publication_ref": ["b50", "b1"], "figure_ref": ["fig_0"], "table_ref": ["tab_3"]}, {"heading": "Gesture Generation", "text": "As discussed in Section 1, MPCs are far more complex than two-party interactions. The SDS must track who said what to whom (Gu et al., 2022b), track the goals of multiple users (Addlesee et al., 2023d), and generate responses to specific addressees. As our SDS is embodied by the ARI social robot (Cooper et al., 2020), we can produce helpful gestures with its controllable arms, head, and eyes. While some gestures are charming, like facing the robot's palms upward when welcoming a user to an interaction, other gestures are more functional. The robot can look at the user it is addressing, point when giving directions, and indicate that it is passing the turn to another user with its arm. These functional gestures are what we evaluate here. In component (E) in Figure 2, you can see that we generate gestures using the Vicuna-13b-v1.5 LLM (component (F)) in parallel with the grounded answer generation. In the prompt, we provide some examples of functional gestures, using the gesture tags that the robot expects (Cherakara et al., 2023). The answer text is passed to ARI's text-to-speech, and the generated gesture tags are passed to ARI's movement controls. We do not generate gestures when listening to the user, as the microphones become saturated by ego-noise (motor sounds), and the ASR fails to hear the user's utterance .\nWe annotated a set of 110 generated system responses with gold functional gesture tags. Using our gesture generation method, the generated gestures were accurate 86% of the time. Generating an incorrect gesture (e.g. pointing in the wrong direction) is more problematic than missing a gesture, and the gesture generation precision was 0.91.", "publication_ref": ["b29", "b9", "b21", "b17"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Conclusions and Future Work", "text": "We have iteratively developed and deployed a multimodal, multi-party spoken dialogue system in a hospital memory clinic. This SDS is embodied by the ARI social robot, allowing us to generate gestures in addition to speech. Using data collected with real memory clinic patients in this complex setting, our system is able to decide when to take its turn, generate natural clarification requests (im-proving accessibility for people with memory impairment), answer in-domain questions grounded to our domain specific knowledge, and respond appropriately to out-of-domain requests like generating jokes, quizzes, and general chit-chat.\nWe are currently running further data collection in the hospital with the LLM-based SDS. Using this data, we will further refine our system and curate corpora that will be released to allow other researchers to work on this complex, yet vital task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Ethical Considerations", "text": "Some LLMs, like ChatGPT, can only be used through an API. This is a huge privacy concern, especially in the healthcare setting. Even if participants were instructed carefully, it is impossible to ensure they would not reveal personally identifiable information -this problem is exacerbated in a memory clinic setting (Addlesee and Albert, 2020). For this reason, we must use more open and transparent LLMs (Liesenfeld et al., 2023). We selected Vicuna-13b-v1.5 as it was the best performing model that could run on our hardware.\nIn Section 3.4 we detailed our in-prompt hallucination reduction efforts, but these will never reach zero. Hospital staff run the experiments, so they can correct the robot if it ever produces a hospitalrelated hallucination. This is also why we do not provide the SDS with any personal information like patient appointment schedules -we do not want to cause confusion.\nIn a real deployment, prompt poisoning could be an issue. A bad actor can manipulate the system to output incorrect responses through dialogue. This is not possible in our data collection, as we reset the system between participants (the patients are also unlikely to be bad actors). If deployed, speaker diarization and dialogue history deletion can mitigate this risk, but it is critical to highlight that LLMs can be manipulated.", "publication_ref": ["b2", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This research was funded by the EU H2020 program under grant agreement no. 871245 (https: //spring-h2020.eu/). We would also like to thank our anonymous reviewers for their time and valuable feedback.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Voice assistant accessibility", "journal": "", "year": "2023", "authors": "Angus Addlesee"}, {"ref_id": "b1", "title": "Incremental Multi-party Conversational AI for People with Dementia", "journal": "", "year": "2024", "authors": "Angus Addlesee"}, {"ref_id": "b2", "title": "Ethically collecting multi-modal spontaneous conversations with people that have cognitive impairments", "journal": "", "year": "2020-05", "authors": "Angus Addlesee; Pierre Albert"}, {"ref_id": "b3", "title": "Understanding and answering incomplete questions", "journal": "", "year": "2023", "authors": "Angus Addlesee; Marco Damonte"}, {"ref_id": "b4", "title": "Understanding disrupted sentences using underspecified abstract meaning representation", "journal": "", "year": "2023", "authors": "Angus Addlesee; Marco Damonte"}, {"ref_id": "b5", "title": "Detecting agreement in multi-party dialogue: evaluating speaker diarisation versus a procedural baseline to enhance user engagement", "journal": "", "year": "2023", "authors": "Angus Addlesee; Daniel Denley; Andy Edmondson; Nancie Gunson; Daniel Hern\u00e1ndez Garcia; Alexandre Kha; Oliver Lemon; James Ndubuisi; O' Neil; Lia Reilly; Rapha\u00ebl Perochaud; Miebaka Valeri;  Worika"}, {"ref_id": "b6", "title": "Current challenges in spoken dialogue systems and why they are critical for those living with dementia", "journal": "", "year": "2019", "authors": "Angus Addlesee; Arash Eshghi; Ioannis Konstas"}, {"ref_id": "b7", "title": "Building for speech: Designing the next generation of social robots for audio interaction", "journal": "", "year": "2023", "authors": "Angus Addlesee; Ioannis Papaioannou; Oliver Lemon"}, {"ref_id": "b8", "title": "Data collection for multi-party task-based dialogue in social robotics", "journal": "", "year": "2023", "authors": "Angus Addlesee; Weronika Siei\u0144ska; Nancie Gunson; Daniel Hern\u00e1ndez Garcia; Christian Dondrup; Oliver Lemon"}, {"ref_id": "b9", "title": "Multi-party Goal Tracking with LLMs: Comparing Pre-training, Fine-tuning, and Prompt Engineering", "journal": "", "year": "2023", "authors": "Angus Addlesee; Weronika Siei\u0144ska; Nancie Gunson; Daniel Hern\u00e1ndez Garc\u00eda; Christian Dondrup; Oliver Lemon"}, {"ref_id": "b10", "title": "Annual Meeting of the Special Interest Group on Discourse and Dialogue", "journal": "", "year": "", "authors": ""}, {"ref_id": "b11", "title": "Furhat: a backprojected human-like robot head for multiparty", "journal": "", "year": "2012", "authors": "Jonas Samer Al Moubayed; Gabriel Beskow; Bj\u00f6rn Skantze;  Granstr\u00f6m"}, {"ref_id": "b12", "title": "Cognitive Behavioural Systems: COST 2102 International Training School", "journal": "Springer", "year": "2011", "authors": ""}, {"ref_id": "b13", "title": "Use accessibility features with siri on iphone", "journal": "", "year": "2022-04", "authors": " Apple"}, {"ref_id": "b14", "title": "Gaze, addressee selection and turntaking in three-party interaction. Eye-tracking in interaction: Studies on the role of eye gaze in dialogue", "journal": "", "year": "2018", "authors": "Peter Auer"}, {"ref_id": "b15", "title": "The listener as addressee in face-to-face dialogue", "journal": "International Journal of Listening", "year": "2011", "authors": "Janet Beavin Bavelas; Jennifer Gerwing"}, {"ref_id": "b16", "title": "Connected speech in neurodegenerative language disorders: a review", "journal": "Frontiers in psychology", "year": "2017", "authors": "Veronica Boschi; Eleonora Catricala; Monica Consonni; Cristiano Chesi; Andrea Moro; Stefano F Cappa"}, {"ref_id": "b17", "title": "FurChat: An embodied conversational agent using LLMs, combining open and closeddomain dialogue with facial expressions", "journal": "Association for Computational Linguistics", "year": "2023", "authors": "Neeraj Cherakara; Finny Varghese; Sheena Shabana; Nivan Nelson; Abhiram Karukayil; Rohith Kulothungan; Mohammed Afil Farhan; Birthe Nesset; Meriam Moujahid; Tanvi Dinkar; Verena Rieser; Oliver Lemon"}, {"ref_id": "b18", "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality", "journal": "", "year": "2023-04", "authors": "Wei-Lin Chiang; Zhuohan Li; Zi Lin; Ying Sheng; Zhanghao Wu; Hao Zhang; Lianmin Zheng; Siyuan Zhuang; Yonghao Zhuang; Joseph E Gonzalez"}, {"ref_id": "b19", "title": "2023. 'what are you referring to?' evaluating the ability of multi-modal dialogue models to process clarificational exchanges", "journal": "Association for Computational Linguistics", "year": "", "authors": "Javier Chiyah-Garcia; Alessandro Suglia; Arash Eshghi; Helen Hastie"}, {"ref_id": "b20", "title": "Think you have solved question answering? try arc, the ai2 reasoning challenge", "journal": "", "year": "2018", "authors": "Peter Clark; Isaac Cowhey; Oren Etzioni; Tushar Khot; Ashish Sabharwal; Carissa Schoenick; Oyvind Tafjord"}, {"ref_id": "b21", "title": "ARI: The Social Assistive Robot and Companion", "journal": "", "year": "2020", "authors": "Sara Cooper; Alessandro Di Fava; Carlos Vivas; Luca Marchionni; Francesco Ferro"}, {"ref_id": "b22", "title": "Alana v2: Entertaining and informative open-domain social dialogue using ontologies and entity linking", "journal": "", "year": "2018", "authors": "Amanda Cercas Curry; Ioannis Papaioannou; Alessandro Suglia; Shubham Agarwal; Igor Shalyminov; Xinnuo Xu; Ondrej Du\u0161ek; Arash Eshghi; Ioannis Konstas; Verena Rieser"}, {"ref_id": "b23", "title": "Collective contexts in conversation: Grounding by proxy", "journal": "Cognitive science", "year": "2016", "authors": "Arash Eshghi; G T Patrick;  Healey"}, {"ref_id": "b24", "title": "MuM-MER: Socially intelligent human-robot interaction in public spaces", "journal": "", "year": "2019", "authors": "Mary Ellen Foster; Bart Craenen; Amol Deshmukh; Oliver Lemon; Emanuele Bastianelli; Christian Dondrup; Ioannis Papaioannou; Andrea Vanzo; Jean-Marc Odobez; Olivier Can\u00e9vet"}, {"ref_id": "b25", "title": "Complexity-based prompting for multi-step reasoning", "journal": "", "year": "2022", "authors": "Yao Fu; Hao Peng; Ashish Sabharwal; Peter Clark; Tushar Khot"}, {"ref_id": "b26", "title": "Franny, frankfurt airport's new multilingual robot concierge can help you in over 35 languages", "journal": "Furhat Robotics Press Release", "year": "2015", "authors": ""}, {"ref_id": "b27", "title": "Conversational organization: interaction between speakers and hearers", "journal": "Academic Press", "year": "1981", "authors": " Charles;  Goodwin"}, {"ref_id": "b28", "title": "HeterMPC: A Heterogeneous Graph Neural Network for Response Generation in Multi-Party Conversations", "journal": "Long Papers", "year": "2022", "authors": "Jia-Chen Gu; Chao-Hong Tan; Chongyang Tao; Zhen-Hua Ling; Huang Hu; Xiubo Geng; Daxin Jiang"}, {"ref_id": "b29", "title": "WHO Says WHAT to WHOM: A Survey of Multi-Party Conversations", "journal": "", "year": "2022", "authors": "Jia-Chen Gu; Chongyang Tao; Zhen-Hua Ling"}, {"ref_id": "b30", "title": "MPC-BERT: A pre-trained language model for multi-party conversation understanding", "journal": "", "year": "2021", "authors": "Jia-Chen Gu; Chongyang Tao; Zhenhua Ling; Can Xu; Xiubo Geng; Daxin Jiang"}, {"ref_id": "b31", "title": "Annual Meeting of the Association for Computational Linguistics", "journal": "", "year": "", "authors": ""}, {"ref_id": "b32", "title": "Developing a social conversational robot for the hospital waiting room", "journal": "IEEE", "year": "2022", "authors": "Nancie Gunson; Daniel Hern\u00e1ndez Garc\u00eda; Weronika Siei\u0144ska; Christian Dondrup; Oliver Lemon"}, {"ref_id": "b33", "title": "Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding", "journal": "", "year": "", "authors": "Dan Hendrycks; Collin Burns; Steven Basart; Andy Zou"}, {"ref_id": "b34", "title": "Finishing each other's... responding to incomplete contributions in dialogue", "journal": "", "year": "2012", "authors": "Christine Howes; G T Ptarick; Matthew Healey; Arash Purver;  Eshghi"}, {"ref_id": "b35", "title": "GSN: A graphstructured network for multi-party dialogues", "journal": "", "year": "2019", "authors": "Wenpeng Hu; Zhangming Chan; Bing Liu; Dongyan Zhao; Jinwen Ma; Rui Yan"}, {"ref_id": "b36", "title": "How do users respond to voice input errors? lexical and phonetic query reformulation in voice search", "journal": "", "year": "2013", "authors": "Jiepu Jiang; Wei Jeng; Daqing He"}, {"ref_id": "b37", "title": "Machine learning for social multiparty human-robot interaction", "journal": "", "year": "2014", "authors": "Simon Keizer; Mary Ellen Foster; Zhuoran Wang; Oliver Lemon"}, {"ref_id": "b38", "title": "Opening up chatgpt: Tracking openness, transparency, and accountability in instructiontuned text generators", "journal": "", "year": "2023", "authors": "Andreas Liesenfeld; Alianda Lopez; Mark Dingemanse"}, {"ref_id": "b39", "title": "Truthfulqa: Measuring how models mimic human falsehoods", "journal": "Long Papers", "year": "2022", "authors": "Stephanie Lin; Jacob Hilton; Owain Evans"}, {"ref_id": "b40", "title": "Multi-party interaction with a robot receptionist", "journal": "IEEE", "year": "2022", "authors": "Meriam Moujahid; Helen Hastie; Oliver Lemon"}, {"ref_id": "b41", "title": "Analysis of user reactions to turntaking failures in spoken dialogue systems", "journal": "", "year": "2007", "authors": "Mikio Nakano; Yuka Nagano; Kotaro Funakoshi; Toshihiko Ito; Kenji Araki; Yuji Hasegawa; Hiroshi Tsujino"}, {"ref_id": "b42", "title": "Human-ai interactions through a gricean lens", "journal": "Proceedings of the Linguistic Society of America", "year": "2021", "authors": "Laura Panfili; Steve Duman; Andrew Nave; Katherine Phelps Ridgeway; Nathan Eversole; Ruhi Sarikaya"}, {"ref_id": "b43", "title": "Alana: Social dialogue using an ensemble model and a ranker trained on user feedback", "journal": "Alexa Prize Proceedings", "year": "2017", "authors": "Ioannis Papaioannou; Amanda Cercas Curry; Jose L Part; Igor Shalyminov; Xinnuo Xu; Yanchao Yu; Ondrej Du\u0161ek; Verena Rieser; Oliver Lemon"}, {"ref_id": "b44", "title": "Voice interfaces in everyday life", "journal": "", "year": "2018", "authors": "Martin Porcheron; Joel E Fischer; Stuart Reeves; Sarah Sharples"}, {"ref_id": "b45", "title": "Detecting agreement in multi-party conversational ai", "journal": "", "year": "2023", "authors": "Laura Schauer; Jason Sweeny; Charlie Lyttle; Zein Said; Aron Szeles; Cale Clark; Katie Mcaskill; Xander Wickham; Tom Byars; Daniel Hern\u00e1ndez Garcia; Nancie Gunson; Angus Addlesee; Oliver Lemon"}, {"ref_id": "b46", "title": "Connected speech features from picture description in alzheimer's disease: A systematic review", "journal": "Journal of Alzheimer's Disease", "year": "2018", "authors": "Antoine Slegers; Renee-Pier Filiou; Maxime Montembeault; Simona Maria Brambati"}, {"ref_id": "b47", "title": "Object-aware gaze target detection", "journal": "", "year": "2023", "authors": "Francesco Tonini; Nicola Dall'asen; Cigdem Beyan; Elisa Ricci"}, {"ref_id": "b48", "title": "Issues in multiparty dialogues", "journal": "Springer", "year": "2003", "authors": "David Traum"}, {"ref_id": "b49", "title": "A robot in the library", "journal": "Springer", "year": "2020", "authors": "Evgenios Vlachos; Anne Faber Hansen; Jakob Povl Holck"}, {"ref_id": "b50", "title": "prompting language models improves quoting from pre-training data", "journal": "", "year": "2023", "authors": "Orion Weller; Marc Marone; Nathaniel Weir; Dawn Lawrie; Daniel Khashabi; Benjamin Van Durme"}, {"ref_id": "b51", "title": "Hellaswag: Can a machine really finish your sentence?", "journal": "", "year": "2019", "authors": "Rowan Zellers; Ari Holtzman; Yonatan Bisk; Ali Farhadi; Yejin Choi"}, {"ref_id": "b52", "title": "DialogLM: Pre-trained model for long dialogue understanding and summarization", "journal": "", "year": "2022", "authors": "Ming Zhong; Yang Liu; Yichong Xu; Chenguang Zhu; Michael Zeng"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure2: The architecture of our multi-party multimodal dialogue system deployed on the ARI robot.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "in Table 1), answer", "figure_data": "Example User UtteranceNote of Interest(A)U1 U1I think it is London Yeah... LondonIf turn 2 was U2, it would be agreement, so speaker recognition changes meaning.(B)U1My husband needs the bathroomProviding other user's goal.(C)U1 U2What time is my appointment? It's at 10amU2 answers U1's question, but addressee was ambiguous without gaze info.(D)U1 ARIWe are hungry The caf\u00e9 is through the door on your left, but you should fast before your visit.Shared goal indicated by 'we', and robot can point to the 'left'. Fasting is in red as it is a world-knowledge hallucination.(E)U1 ARI U1 ARIName a song by... By who? Queen Bohemian RhapsodyThis is an OOD question that could not be answered without the LLM-based SDS. The partial utterance is handled naturally which improves accessibility."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Utterances and interactions that illustrate behaviours of interest to this paper (referred to where appropriate). Examples B & C from MPCs with hospital memory clinic patients, their companions, and our SDS on the ARI robot. Example A:", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Clarification request generation results. SMA: Sluice Match Accuracy. SentCR: Sentential CR. RCR: Reprise CR. SCR: Sluice CR. Prompt styles = Basic, Annotation, and Reasoning.", "figure_data": "Model Human GPT-4 Llama-2 13b-chat Llama-2 70b-chat Vicuna 13b-v1.5Prompt SMA SentCR RCR SCR --3.8 39.6 35.2 B 11.7 91.2 0.0 0.0 A 98.4 6.8 1.2 79.6 R 97.6 0.8 1.2 86.0 B 3.3 91.6 0.4 0.0 A 0.0 100 0.0 0.0 R 2.0 99.2 0.0 0.0 B 2.6 99.6 0.0 0.0 A 91.6 69.2 7.6 8.4 R 86.0 51.6 20.0 12.0 B 11.7 98.4 0.0 0.0 A 83.9 73.2 0.0 20.4 R 87.0 66.4 2.4 20.0"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Knowledge grounding results.", "figure_data": ""}], "formulas": [], "doi": "10.21437/Interspeech.2023-307"}