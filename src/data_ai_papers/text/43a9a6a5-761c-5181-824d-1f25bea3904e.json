{"title": "Operator-Potential Heuristics for Symbolic Search", "authors": "Daniel Fi\u0161er; \u00c1lvaro Torralba; J\u00f6rg Hoffmann", "pub_date": "", "abstract": "Symbolic search, using Binary Decision Diagrams (BDDs) to represent sets of states, is a competitive approach to optimal planning. Yet heuristic search in this context remains challenging. The many advances on admissible planning heuristics are not directly applicable, as they evaluate one state at a time. Indeed, progress using heuristic functions in symbolic search has been limited and even very informed heuristics have been shown to be detrimental. Here we show how this connection can be made stronger for LP-based potential heuristics. Our key observation is that, for this family of heuristic functions, the change of heuristic value induced by each operator can be precomputed. This facilitates their smooth integration into symbolic search. Our experiments show that this can pay off significantly: we establish a new state of the art in optimal symbolic planning.", "sections": [{"heading": "Introduction", "text": "A * search with admissible heuristics and symbolic search are currently the two main contenders for the state of the art in cost-optimal planning. In principle, these are two orthogonal enhancements of a vanilla search algorithmadmissible heuristics aim to reduce the number of explored states, and symbolic search uses Binary Decision Diagrams (BDDs) (Bryant 1986) to efficiently represent and manipulate sets of states, greatly speeding up exhaustive search. A natural idea is to combine the two, and indeed that idea has been presented decades ago in the BDDA * algorithm (Edelkamp and Reffel 1998;Edelkamp 2002).\nYet that combination has not been an unqualified success. For a heuristic to be effective in symbolic search, two properties are required: (1) it must be possible to efficiently evaluate sets of states represented as BDDs, without evaluating the heuristic on each represented state individually; and (2) it must induce a good partitioning, so that sets of states with the same gand h-value can be efficiently represented as BDDs. Property (1) is fulfilled by some of the strongest heuristics for explicit-state search (e.g., symbolic PDBs (Kissmann and Edelkamp 2011;Franco et al. 2017;Torralba, L\u00f3pez, and Borrajo 2018)) so they can be used in BDDA * . However, it has been shown that even very informative heuristics can be detrimental in symbolic Copyright \u00a9 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nsearch (Speck, Gei\u00dfer, and Mattm\u00fcller 2020), when they do not fulfill property (2). The main reason is that reducing the amount of expanded states may be detrimental if the underlying BDD representation is less efficient. Due to all this, symbolic bidirectional blind search (without heuristics) is considered the dominant symbolic search approach, and the use of heuristic search in this context has lost traction.\nHere we challenge this trend by showing that potential heuristics ) yield fresh synergy between heuristic and symbolic search. Such heuristics assign a numeric value (a potential) to each fact of the planning task, in a way so that the sum of the potentials of the facts true in a state is an admissible estimate of the state's goal distance. As we show, potential heuristics are particularly well suited for combination with symbolic search.\nOur key observation is that potentials can be computed for each operator rather than for each fact. Such operator potentials combine synergically with symbolic search as they have property (1): Under certain conditions, the operator potential of an operator o is equal to the difference in heuristic values h(s ) \u2212 h(s) for any state transition s \u2192 s induced by the operator o. This allows for an efficient encoding of potential heuristics in symbolic search without having to compute the heuristic value of each explored state (Jensen, Veloso, and Bryant 2008). The main difficulty in doing so is that these operator potentials are real (floating-point) numbers, which can lead to rounding and precision issues. Naively rounding these values may lead to a path-dependent and inconsistent heuristic. We show that this can be dealt with by rounding operator potentials within the mixed-integer linear program (MIP) that derives the potential heuristics.\nOur empirical analysis shows that potential heuristics also fulfill property (2). That is, they not only reduce the number of explored states, but also lead to improvements on the number of BDD nodes on average. This makes potential heuristics very helpful in symbolic search across a large number of benchmark domains. Overall, symbolic forward search with potential heuristics soundly outperforms symbolic bidirectional blind search, thus establishing a new state of the art in optimal symbolic planning.", "publication_ref": ["b4", "b7", "b5", "b16", "b11", "b28", "b24", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Preliminaries", "text": "We consider the finite domain representation (FDR) of planning tasks (B\u00e4ckstr\u00f6m and Nebel 1995). An FDR planning\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence  task \u03a0 is specified by a tuple \u03a0 = V, O, I, G . V is a finite set of variables, each variable V \u2208 V has a finite domain dom(V ). A fact V, v is a pair of a variable V \u2208 V and one of its values v \u2208 dom(V ). The set of all facts is denoted by\nF = { V, v | V \u2208 V, v \u2208 dom(V )}, and the set of facts of variable V is denoted by F V = { V, v | v \u2208 dom(V )}.\nA partial state p is a variable assignment over some variables vars(p) \u2286 V. We write p[V ] for the value assigned to the variable V \u2208 vars(p) in the partial state p. We also identify p with the set of facts contained in p, i.e., \np = { V, p[V ] | V \u2208 vars(p)}. A partial state s is a state if vars(s) = V. I is an initial state. G is a\no s such that o s [V ] = eff(o)[V ] for ev- ery V \u2208 vars(eff(o)), and o s [V ] = s[V ] for every V \u2208 V \\vars(eff(o)). We also assume that pre(o)[V ] = eff(o)[V ] for every V \u2208 vars(pre(o)) \u2229 vars(eff(o)).\nGiven a non-negative integer n \u2208 N 0 , [n] denotes the set {1, . . . , n} with [0] defined as an empty set. A sequence of operators\n\u03c0 = o 1 , . . . , o n is applicable in a state s 0 if there are states s 1 , . . . , s n such that o i is applicable in s i\u22121 and s i = o i s i\u22121 for i \u2208 [n].\nThe resulting state of this application is \u03c0 s 0 = s n and cost(\u03c0) = n i=1 cost(o i ) denotes the cost of this sequence of operators. A sequence of operators \u03c0 is called an s-plan iff \u03c0 is applicable in a state s and \u03c0 s is a goal state. An s-plan \u03c0 is called optimal if its cost is minimal among all s-plans.\nA state s is reachable if there exists an operator sequence \u03c0 applicable in I such that \u03c0 I = s. Otherwise, we say that s is unreachable. The set of all reachable states is denoted by R. An operator o is reachable iff it is applicable in some reachable state. A state s is a dead-end state iff G \u2286 s and there is no s-plan. A set of facts M \u2286 F is a mutex if M \u2286 s for every reachable state s \u2208 R.\nA heuristic h : R \u2192 R \u222a {\u221e} estimates the cost of optimal s-plans. The optimal heuristic h (s) maps each reachable state s to the cost of the optimal s-plan or to \u221e if s is a dead-end state. A heuristic h is called (a) admissible iff h(s) \u2264 h (s) for every reachable state s \u2208 R; (b) goalaware iff h(s) \u2264 0 for every reachable goal state s; and (c) consistent iff h(s) \u2264 h(o s ) + cost(o) for all reachable states s \u2208 R and operators o \u2208 O applicable in s. It is well-known that goal-aware and consistent heuristics are also admissible. In the context of heuristic search, h-value of a state node s refers to the heuristic value of s, g-value to the cost of the sequence of operators leading to s, and f -value is the sum of g-value and the maximum of h-value and zero (since we allow negative h-values).", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Symbolic Search Background", "text": "Explicit state-space search operates on individual states, whereas symbolic search (McMillan 1993) works on sets of states represented by their characteristic functions. A characteristic function f S of a set of states S is a Boolean function assigning 1 to states that belong to S and 0 to states that do not belong to S. Operations like the union (\u222a), intersection (\u2229), and complement of sets of states correspond to the disjunction (\u2228), conjunction (\u2227), and negation (\u00ac) of their characteristic functions, respectively. Binary Decision Diagrams (BDDs) (Bryant 1986) are a efficient data-structure to represent Boolean functions in the form of a directed acyclic graph. The size of a BDD is the number of nodes in this representation. The main advantage of using BDDs is that often a BDD is much smaller than the number of states it represents. In fact, BDDs can be exponentially smaller, as certain sets containing exponentially many states can be represented by BDDs of polynomial size (Edelkamp and Kissmann 2008). Most operations on BDDs take only polynomial time in the size of the BDD, which enables the efficient manipulation of large sets of states.\nThe most prominent implementation of symbolic heuristic search in the context of automated planning is BDDA * (Edelkamp and Reffel 1998) which is a variant of A (Hart, Nilsson, and Raphael 1968) using BDDs to represent sets of states. In BDDA * , operators of planning tasks are represented as transition relations (TRs), also using BDDs. A TR of an operator o is a characteristic function of pairs of states (s, o s ) for all states s such that o is applicable in s. Having a TR T o for every operator o \u2208 O, we can construct a TR of a set of operators with the same cost c as T c = o\u2208O,cost(o)=c T o . As the size of T c may be exponential in the number of operators with cost c, in practice, it is often a good idea to use disjunctive partitioning to keep the size at bay (Jensen, Veloso, and Bryant 2008;Torralba et al. 2017). Moreover, mutexes can be used for a more accurate approximation of reachable states (Torralba et al. 2017).\nLike A * , BDDA * expands states by ascending order of their f -value. To take advantage of the symbolic representation, BDDA * represents all states with the same g and h value in a single BDD S g,h (disjunctive partitioning of S g,h can also be used). Given a set of states S g,h and a TR T c , image(S g,h , T c ) computes the set of successor states reachable from any state in S g,h by applying any operator represented by T c . 1 The g-value of the resulting set of successor states is simply g + c. These successor states have to be split according to their h value. This can usually be performed efficiently (e.g., with symbolic PDBs (Kissmann and Edelkamp 2011)) by representing the heuristic as a BDD S h per heuristic value that represents the set of states with that value and performing a conjunction.\nGHSETA * and FSETA * (Jensen, Veloso, and Bryant 2008) encode the heuristic function as part of the transition relation, creating multiple TRs depending on the impact of the operators on heuristic value. This is a very efficient way of evaluating the heuristics within symbolic search. However, up to now, all heuristics known to be suitable for this representation were either non-informative, inadmissible, or domain dependent.", "publication_ref": ["b4", "b6", "b12", "b15", "b26", "b26", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Potential Heuristics Background", "text": "Potential heuristics  assign a numerical value to each fact, and the heuristic value for a state s is then simply a sum of the potentials of all facts in s. Definition 1. Let \u03a0 denote a planning task with facts F . A potential function is a function P : F \u2192 R. A potential heuristic for P maps each state s \u2208 R to the sum of potentials of facts in s, i.e., h P (s) = f \u2208s P(f ).\nWe will leverage prior work on so-called disambiguation (Alc\u00e1zar et al. 2013) to strengthen potential heuristics (Fi\u0161er, Hor\u010d\u00edk, and Komenda 2020). A disambiguation of a variable V for a given set of facts p is simply a set of facts F \u2286 F V from variable V such that every reachable state extending p contains one of the facts from F . Definition 2. Let \u03a0 denote a planning task with facts F and variables V, let V \u2208 V denote a variable, and let p denote a partial state. A set of facts\nF \u2286 F V is called a disambigua- tion of V for p if for every reachable state s \u2208 R such that p \u2286 s it holds that F \u2229 s = \u2205 (i.e., V, s[V ] \u2208 F ).\nClearly, every F V is a disambiguation of V for all possible partial states, and if V, v \u2208 p and there exists a reachable state extending p, then { V, v } is a disambiguation of V for p. Moreover, if the disambiguation of V for p is an empty set (for any V ), then all states extending p are unreachable. Therefore, we can use empty disambiguations to determine unsolvability of planning tasks (if G extends p), or to prune unreachable operators (if a precondition of the operator extends p). So, from now on we will consider only nonempty disambiguations. Fi\u0161er, Hor\u010d\u00edk, and Komenda (2020) showed how to use mutexes to find disambiguations, so here we will assume we already have disambiguations inferred. Furthermore, to simplify the notation, we introduce a disambiguation map.\nDefinition 3. A mapping D : (O \u00d7 V) \u222a V \u2192 2 F is called a disambiguation map if (i) for every operator o \u2208 O and every variable V \u2208 vars(eff(o)) it holds that D(o, V ) \u2286 F V is a disambiguation of V for pre(o) such that |D(o, V )| \u2265 1; and (ii) for every variable V \u2208 V it holds that D(V ) \u2286 F V is a disambiguation of V for G such that |D(V )| \u2265 1.\nNow we can state sufficient conditions for the potential heuristic to be admissible, which we will need later on. Theorem 4. (Fi\u0161er, Hor\u010d\u00edk, and Komenda 2020) Let \u03a0 = V, O, I, G denote a planning task with facts F , and let P denote a potential function, and let D denote a disambiguation map. If\nV \u2208V max f \u2208D(V ) P(f ) \u2264 0 (1) and for every operator o \u2208 O it holds that V \u2208vars(eff(o)) max f \u2208D(o,V ) P(f ) \u2212 f \u2208eff(o) P(f ) \u2264 cost(o), (2)\nthen the potential heuristic for P is admissible.\nIn practice, we can obtain potentials as a solution to a linear program (LP) with constraints corresponding to conditions from Theorem 4: For each f \u2208 F , we create a (realvalued) variable P(f ), add constraints Eq. (1) and Eq. (2), and then a solution of such LP for any objective function results in a goal-aware and consistent potential function. So far, potential heuristics have been used as described in Definition 1, i.e., each fact gets assigned a potential value and the heuristic value for a state s is the sum of potentials of all facts in s.", "publication_ref": ["b9", "b9", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Operator-Potential Heuristics", "text": "Our key observation is that potentials can also be designed in a different way, yielding a new synergy with symbolic search: We can assign a potential to each operator and compute an admissible heuristic value for a state s reached by a sequence of operators \u03c0 as a sum of operator potentials of all operators in \u03c0. We start by introducing an operator-potential function. Definition 5. Given a potential function P, and a disambiguation map D, a function Q : O \u2192 R is called an operator-potential function for P and D if\nQ(o) = f \u2208eff(o) P(f ) \u2212 V \u2208vars(eff(o)) max f \u2208D(o,V ) P(f ) (3) for every operator o \u2208 O.\nNote that the value of Q(o) is just the value of the left hand side of Eq. (2) with the opposite sign. Or in other words, the operator-potential function for an operator o gives us the lower bound on the change of the heuristic value of the corresponding potential heuristic for the given potential function P and disambiguation map D. This immediately leads to another observation that for every sequence of operators \u03c0 = o 1 , . . . , o n applicable in the initial state I such that s = \u03c0 I it holds that h P (I) + i\u2208[n] Q(o i ) \u2264 h P (s). So, we can compute an admissible heuristic estimate for any state s reachable by a sequence of operators \u03c0 as h P (I) + i\u2208[n] Q(o i ). However, this estimate is pathdependent and therefore not necessarily consistent. Consider a planning task with a binary variable V , and an operator o with empty precondition and eff(o) = { V, 1 }. It is easy to see that the value Q(o) is fixed for the operator as the smallest change in the heuristic value it can induce, but the actual change of the heuristic value may be different if the operator is applied on the state where V, 0 or V, 1 is set.\nTo avoid this difficulty, we now show that, if the disambiguation map D maps every operator o and every effect variable V \u2208 vars(eff(o)) to a singleton, then h P (I) +\ni\u2208[n] Q(o i ) = h P (s) for every sequence of operators \u03c0 = o 1 , . . . , o n such that \u03c0 I = s.\nThat is, as long as the preconditions on the variables affected by any operator o are known precisely, the potential h-value for any state s can be computed as the potential h-value for the initial state plus the sum of operator potentials of operators from any sequence of operators \u03c0 leading to s. Lemma 6 shows that equality holds for any two consecutive states, and Lemma 7 shows it holds over any sequence of operators applicable in the initial state. Lemma 6. Let P denote a potential function, D a disambiguation map, Q an operator-potential function for P and D, s a reachable state, and let o denote an operator applicable in s.\nIf |D(o, V )| = 1 for every V \u2208 vars(eff(o)), then f \u2208s P(f ) + Q(o) = f \u2208o s P(f ). Proof. Let A = V \u2208vars(eff(o)) D(o, V ). Since |D(o, V )| = 1 for every V \u2208 vars(eff(o)), Equation (3) can be rewritten as Q(o) = f \u2208eff(o) P(f ) \u2212 f \u2208A P(f ).\nAnd since s is reachable and o is applicable in s, it holds that A \u2286 s.\nLet B = s \\ A. Clearly, o s = B \u222a eff(o) and B \u2229 eff(o) = \u2205. Therefore, f \u2208s P(f ) + Q(o) = f \u2208o s P(f ) can be rewritten to f \u2208B P(f ) + f \u2208A P(f ) + Q(o) = f \u2208B P(f ) + f \u2208eff(o) P(f ), and further simplified to \nf \u2208A P(f ) + Q(o) = f \u2208eff(o) P(f ). Expanding Q(o) gives us f \u2208A P(f ) + f \u2208eff(o) P(f ) \u2212 f \u2208A P(f ) = f \u2208eff(o) P(f ),\nP(f )+ i\u2208[n\u22121] Q(o i ) = f \u2208s P(f )\n, and we need to prove that\nf \u2208I P(f ) + i\u2208[n] Q(o i ) = f \u2208s P(f ). From the as- sumption, it follows that f \u2208I P(f ) + i\u2208[n\u22121] Q(o i ) + Q(o n ) = f \u2208s P(f ) + Q(o n ), so it is enough to show that f \u2208s P(f ) + Q(o n ) = f \u2208s P(f ) (Lemma 6).\nNow, getting to the main result of this section, we formulate an operator-potential heuristic and we prove that this heuristic is well-defined and equal to the corresponding (fact) potential heuristic.\nDefinition 8. Let Q denote an operator-potential function for P and D such that\n|D(o, V )| = 1 for every o \u2208 O and every V \u2208 vars(eff(o)). An operator-potential heuristic h Q : R \u2192 R \u222a {\u221e} for Q is defined as h Q (s) = f \u2208I P(f ) + i\u2208[n] Q(o i )(4)\nfor any sequence of operators \u03c0 = o 1 , . . . , o n such that \u03c0 I = s.\nTheorem 9. Let D denote a disambiguation map such that |D(o, V )| = 1 for every o \u2208 O and every V \u2208 vars(eff(o)), let P denote a potential function, and let Q denote an operator-potential function for P and D. Then h Q is welldefined, and h Q (s) = h P (s) for every reachable state s, and h Q is admissible (goal-aware, consistent) if h P is admissible (goal-aware, consistent).\nProof. It follows directly from Lemma 7.\nNote that every planning task can be transformed into another task where |D(o, V )| = 1 holds for every operator o and variable V \u2208 vars(eff(o)). One possibility is the use of transition normal form ) \nI s 1 s 2 s 3 G o 1 o 2 o 3 o 4 o 5\no 1 ) = eff(o 2 ) = eff(o 3 ) = eff(o 4 ) = eff(o), but different precon- ditions pre(o 1 ) = pre(o) \u222a {f 1 , f 3 }, pre(o 2 ) = pre(o) \u222a {f 1 , f 4 }, pre(o 3 ) = pre(o) \u222a {f 2 , f 3 }, and pre(o 4 ) = pre(o) \u222a {f 2 , f 4 }.\nThis way, the transformed planning task can grow exponentially in the number of effects not appearing in the preconditions of an operator. However, we can prune operators whose preconditions form mutex, and in our experiments, we ran out of memory in only one task.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Handling Floating-Point Potentials", "text": "Although Theorem 9 identifies conditions under which operator-potential heuristics are consistent and equal to the corresponding potential heuristics, in practice there is an additional complication resulting from the fact that the Q(o) values are typically represented as floating-point numbers. So, we should not compare Q(o) values on equality. Moreover, having floating-point h-values is an even larger issue in symbolic search, as states are aggregated based on their hvalue. If floating-point numbers are used, one could get different BDDs for every state in the search, greatly reducing the efficiency of symbolic search. So, it is desirable to represent in a single BDD all states whose h-values are rounded to the same integer value. Rounding operator potentials down to the nearest integers would resolve this problem and it would keep the heuristic function admissible. Unfortunately, this kind of rounding could make the heuristic inconsistent.\nConsider a planning task depicted in Figure 1. Clearly, the operator-potential heuristic h Q is both admissible and consistent. Now, letQ denote an operator potential function obtained by rounding down Q to the nearest integers, i.e.,Q(o 1 ) = 1,Q(o 2 ) = 0,Q(o 3 ) = 0,Q(o 4 ) = 0, and Q(o 5 ) = \u22121. The sum f \u2208I P(f ) + i\u2208[n] Q(o i ) (cf. Definition 8) provides an admissible estimate, because rounding down can only make the sum smaller. However, rounding down can also make this estimate path-dependent, i.e., we can obtain different values for a state depending on the path by which we reached the state, and inconsistent.\nConsider the states s 1 and s , and operator sequences \u03c0 = o 1 and \u03c0 = o 3 , o 4 from Figure 1. Since the heuristic value for the initial state is zero, the inequality h Q (s 1 ) \u2212 h Q (s 2 ) \u2264 cost(o 2 ) holds, because h Q (s 1 ) = 1, h Q (s 2 ) = 1, and cost(o 2 ) = 0. But after rounding, we get Q(o 1 ) = 1 andQ(o 3 ) +Q(o 4 ) = 0 resulting in a higher estimate for s 1 than for s 2 usingQ. We resolve this issue by encoding the rounding directly into the mixed-integer linear program (MIP) expressing the potentials. Since the operator potential is just a left hand side of Eq. (2) with the opposite sign, we can extend the original LP for the inference of potentials with new integer variables, each corresponding to operator-potential Q(o), and add a new constraint Eq. (3) for each Q(o). That is, the new MIP has a real-valued variable P(f ) for each f \u2208 F , an integer variable Q(o) for each operator o \u2208 O, and a set of constraints Eq. (1), Eq. (2), and Eq. (3). A solution to such MIP for any objective function yields a goal-aware and consistent operator-potential function. Moreover, since the variables corresponding to Q(o) are integer-valued, the resulting operator-potentials are also integer-valued while the fact potentials can remain real-valued. Therefore, the MIP solver finds operator-potentials that are properly rounded, so that we can compare the operator-potentials on equality without running into problems with floating-point numbers.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Symbolic Search with Operator-Potentials", "text": "Using potential heuristics in BDDA * is not straightforward, as the standard way of evaluating the heuristics by constructing a BDD S h representing all states with a heuristic value equal to h may not always be feasible. The naive approach would be to enumerate all possible sub-sets of features whose potentials add up to h. However, this requires enumerating exponentially many sub-sets and it may result in an exponentially large intermediate BDD. We overcome this difficulty by using operator-potential heuristics instead.\nTo do so, we use GHSETA * (Jensen, Veloso, and Bryant 2008), a symbolic heuristic search algorithm which partitions the TRs not only by the cost of the corresponding operators, but also by the change of the heuristic value they induce. 2 That is, instead of creating a TR T c for all operators o having cost(o) = c, we create a TR T c,q representing all operators o such that cost(o) = c and Q(o) = q. For the initial state, the g-value is set to zero, and the hvalue is set to f \u2208I P(f ). For all subsequent states S g,h expanded by the TR T c,q , the g-value and h-value of the resulting state S g ,h = image(S g,h , T c,q ) is set to g = g + c and h = h + q, respectively.\nThe pseudocode for the algorithm using a consistent operator-potential heuristic is encapsulated in Algorithm 1. On lines 1 and 2, the TRs corresponding to all unique pairs of operator costs and Q(o) values are constructed. The heuristic value for the initial state is computed on line 3. The open list of sets of states (represented by BDDs) ordered by f = g + max(0, h) values is initialized with the initial state Algorithm 1: Symbolic forward A * with a consistent operator-potential heuristic.\nInput: A planning task \u03a0, a consistent operator potential function Q. Output: An optimal plan or \"unsolvable\". \n1 for each c, q \u2208 {cost(o), Q(o) | o \u2208 O} do 2 Construct Tc,q from {o \u2208 O | cost(o) = c, Q(o) = q}; 3 hI \u2190 f \u2208I P(f ); 4 S 0,h I \u2190 {I}; 5 open \u2190 { max(0, hI ), S\nf \u2190 g + c + max(0, h + q); 16\nInsertOrUpdate(open, f, S g+c,h+q ); 17 return \"unsolvable\"; on lines 4 and 5. On line 6, a BDD representing all closed states is constructed. The while-cycle on lines 7-16 is an A * algorithm adapted to the symbolic search. On line 8, we extract the set of states with the lowest f -value from the open list and remove all closed states from this set. If a goal state is reached (line 9 and 10), an optimal plan is extracted and returned (for details see (Torralba et al. 2017)). If the current set of states S g,h does not contain a goal state, then all these states are added to the set of closed states (line 11). On lines 12-16, all operators are applied and the resulting states that were not closed yet are assigned the correct g and h values and either inserted into the open list (if there is no S g+c,h+q in the open list), or the set of states in the open list is extended with the new set of states.\nNote that we need the operator-potential heuristic to be consistent in order to avoid re-opening states (cf. lines 8 and 13). This also means that if the computation of consistent operator-potentials require the transformation of the planning task described in Section 5, then the same transformed task must be used also for the symbolic search.\nGHSETA * can also be adapted to work with the pathdependent (and thus possibly inconsistent) variant of the operator-potential heuristic that does not require transforming the planning task. We need to allow re-opening states that were previously closed with a higher g-value. We can achieve that by maintaining not one BDD representing closed states, but one BDD for each g-value (cf. lines 6 and 11). And when removing closed states from the set of states, we remove only the closed states with the same or lower gvalue (cf. lines 8 and 13).", "publication_ref": ["b15", "b26"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "We implemented our search algorithm in C. 3 Operators and facts are pruned with the h 2 heuristic in forward and back-  ward direction (Alc\u00e1zar and Torralba 2015), and the translation from PDDL to FDR uses the inference of mutex groups proposed by Fi\u0161er (2020). We used all planning domains from the optimal track of International Planning Competitions (IPCs) from 1998 to 2018 excluding the ones containing conditional effects after translation. We merged, for each domain, all benchmark suites across different IPCs. This leaves 48 domains overall. We used a cluster of computing nodes with Intel Xeon Scalable Gold 6146 processors and CPLEX (I)LP solver v12.10. The time and memory limits were set to 30 minutes and 8 GB, respectively. We used a time limit of 30 seconds for applying mutexes on the goal BDD and 10 seconds for merging transition relation BDDs (Torralba et al. 2017).\nWe evaluated GHSETA * with the following variants of the consistent operator-potential heuristics obtained on the transformed planning tasks:\n\u2022 I: maximize the h Q -value of the initial state ). \u2022 A+I: maximize the h Q -value for the average (syntactic) state while enforcing the maximum h Q (I) (Seipp, Pommerening, and Helmert 2015;Fi\u0161er, Hor\u010d\u00edk, and Komenda 2020). \u2022 S 1k +I: maximize the h Q -value for 1 000 states sampled using random walks, while enforcing the maximum h Q (I) (Seipp, Pommerening, and Helmert 2015;Fi\u0161er, Hor\u010d\u00edk, and Komenda 2020). \u2022 M 2 +I: maximize the h Q -value for all reachable states approximated with mutex pairs while enforcing the maximum h Q (I) (Fi\u0161er, Hor\u010d\u00edk, and Komenda 2020).\nWe also evaluated the same consistent operator-potential heuristics with the tasks transformed to the transition normal form  (prefixed with tnf-); and the path-dependent operator-potential heuristics on the original planning task (prefixed with pd-).\nWe compare these to symbolic uniform-cost search using forward (bfw) and bidirectional search (bbi) 4 . Furthermore, we compare to other state-of-the-art planners. We ran 4 Our implementation does not lack in performance behind the IPC 2018 SymbA planner-its overall coverage is 942 and 852 for blind bidirectional and blind forward search, respectively.   and column y is the number of domains where x solved more tasks than y, it is bold if higher than the value in row y and column x. \"tot\" shows overall number of solved tasks.\nA * with the LM-Cut (lmc) heuristic (Helmert and Domshlak 2009), with the merge-and-shrink (ms) heuristic with SCC-DFP merge strategy and non-greedy bisimulation shrink strategy (Helmert et al. 2014;Sievers, Wehrle, and Helmert 2016), and with the potential heuristic (pot A+I ), i.e., a variant of A+I for A * . We further compare to two of the bestperforming non-portfolio planners from IPC 2018: Comple-mentary2 (comp2) (Franco et al. 2017;Franco, Lelis, and Barley 2018), and Scorpion (scrp) (Seipp 2018;Seipp and Helmert 2018). Table 1 shows the comparison across all planners in terms of total coverage, and in terms of the number of domains in which each planner is superior to others.", "publication_ref": ["b2", "b8", "b26", "b22", "b9", "b22", "b9", "b9", "b13", "b14", "b23", "b11", "b10", "b20", "b21"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Operator Potentials in Symbolic Search", "text": "The comparison of symbolic potential variants against the baseline forward search without heuristics (bfw) clearly demonstrates that potential heuristics are beneficial for the performance of symbolic search over a wide range of different domains. The best variant of GHSETA * with an operator-potential heuristic (A+I) solves 173 more tasks than the baseline (bfw). It increases coverage on 30 different domains, and it is detrimental in only 7 domains. Among the different variants of potential heuristics, we observe that the optimization criteria can have a significant impact on performance. The best variant is A+I, closely followed by S 1k +I and M 2 +I, and significantly better than I. These results are well in line with the results of the same potential heuristics in explicit search (Fi\u0161er, Hor\u010d\u00edk, and Komenda 2020).\nFor a heuristic to be beneficial in symbolic search, it is required that sets of states with the same g and h value are efficiently represented with BDDs. The positive coverage results from Table 1 suggest that this is indeed the case for the operator-potential heuristics. To confirm this, Figure 2 compares the performance of the baseline, symbolic search without any heuristics (bfw), against the best configuration of our symbolic search with potential heuristics (A+I).\nFirst, we observe that the average number of BDD nodes per expanded BDD was almost always lower (Figure 2a). This means that, indeed, the partitioning induced by operator-potential heuristics is often beneficial, resulting on sets of states during the search that have a concise BDD representation. This property, however, is not guaranteed by the method. In particular, the average number of BDD nodes per expanded BDD was higher for bfw in only ten tasks, though only by a small margin. On the other hand, the number of expanded BDDs is almost always increased (Figure 2b), which is not surprising, as sets of states during the search are not only partitioned by g-value, but also by h-value.\nMost remarkably, the number of BDD nodes from all expanded BDDs often decreased with potential heuristics (Figure 2c). This confirms that these heuristics are not only informative for explicit-state search, avoiding expansion of certain states, but also beneficial in symbolic search by inducing a good BDD partitioning. This contrasts with previous results on very informative heuristics ( 1 2 h * , 3 4 h * ), that despite their accuracy are often detrimental for the performance of symbolic search (Speck, Gei\u00dfer, and Mattm\u00fcller 2020).\nFurthermore, the runtime of the planner is often decreased (Figure 2d), confirming that partitioning the TRs using operator potentials is an effective way of evaluating the heuristic in symbolic search. So, not only potential heuristics can be informative for symbolic search, but they can also be efficiently evaluated. The increase in runtime for some of the tasks is often due to increased computational effort in the inference of integer operator potentials.\nCompared against symbolic bidirectional uniform cost search (bbi), which has state-of-the-art performance in symbolic search planning (Torralba et al. 2014;Torralba, Linares L\u00f3pez, and Borrajo 2016), A+I solves 105 more tasks. The two algorithms are still quite complementary though, with A+I being superior in 24 domains and bbi in 14. This suggests that there is a potential to further improve bidirectional heuristic search by integrating operator-potential heuristics.", "publication_ref": ["b9", "b24", "b25", "b27"], "figure_ref": ["fig_3", "fig_3", "fig_3", "fig_3", "fig_3"], "table_ref": ["tab_3"]}, {"heading": "Explicit-State Search with Potential Heuristics", "text": "Compared to the potential heuristics in explicit-state search, A+I solves 109 instances more than pot A+I . Moreover, there are only 9 domains where using symbolic search with the same heuristics is detrimental, compared to 26 domains where it is beneficial.\nNote that these two configurations are using the same optimization criteria to compute the potentials. However, as explained in Section 6, in order to obtain consistent operatorpotential heuristics, we must (1) split the operators so that all variables mentioned in the effect appear in the preconditions; and (2) use MIP to ensure that operator potentials have an integer value. In most domains this is not an issue, and there are only a few domains where the number of operators increases significantly (e.g., only in agricola and maintenance the task size at least doubled). Nevertheless, this sometimes has an overhead on the computation of the potential heuristics, as illustrated by Figure 2e, which makes the improvements in coverage even more remarkable.\nTo analyze if this overhead could be easily avoided, we also compared A+I against pd-A+I and tnf-A+I. The path-dependent variant pd-A+I partially avoids the overhead shown in Figure 2e, because it does not require the transformation of tasks as per ( 1). Yet, in terms of coverage, results are inferior in general, reducing coverage in 9 domains and improving only in agricola, and caldera. We also tried a variant of pd-A+I with floating-point potentials (i.e., only LP is solved) and rounding to nearest smaller integer (to deal with partitioning of states), but it solved 107 fewer tasks from 20 domains. This shows that it pays off to force the heuristic to be consistent, unless the task size increases significantly.\nFinally, using the transition normal form (tnf-A+I) turns out to be detrimental for symbolic search and never pays-off, because the symbolic search must also use the task in transition normal form with a lot of auxiliary zero-cost operators.", "publication_ref": [], "figure_ref": ["fig_3", "fig_3"], "table_ref": []}, {"heading": "Comparison Against State of the Art", "text": "Compare finally the performance of our best-performing variants (A+I, M 2 +I, and S 1k +I) against the unrelated approaches lmc, ms, comp2, and scrp. Our planners clearly beat lmc and ms in terms of overall coverage and frequency of per-domain superiority. The state-of-the-art planners comp2 and scrp are roughly on par in overall coverage. In terms of individual domains, the clear conclusion is that our new techniques are highly complementary to the previous state of the art: A+I, M 2 +I, and S 1k +I outperform comp2 and scrp in 12 to 16 domains.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Conclusion", "text": "While heuristic search and symbolic search are both contenders for the throne in optimal planning, and their combination is a natural and promising avenue, the results with that combination have thus far been disappointing. As we show, this picture changes dramatically when leveraging the fact that potential heuristics can be viewed as potentials over operators, which enables their smooth integration into symbolic search. We have shown that and how this can be done, in particular while retaining consistency. Our empirical results show that this boosts the performance of optimal symbolic planning, which is now on par with the best heuristic search based optimal planners.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)project number 389792660-TRR 248 (see https: //perspicuous-computing.science).\nThe experimental evaluation was supported by the OP VVV funded project CZ.02.1.01/0.0/0.0/16 019/0000765 \"Research Center for Informatics\".", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": "V Alc\u00e1zar; D Borrajo; S Fern\u00e1ndez; R Fuentetaja"}, {"ref_id": "b1", "title": "Revisiting Regression in Planning", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "A Reminder about the Importance of Computing and Exploiting Invariants in Planning", "journal": "", "year": "2015", "authors": "V Alc\u00e1zar; \u00c1 Torralba"}, {"ref_id": "b3", "title": "", "journal": "Complexity Results for SAS + Planning. Computational Intelligence", "year": "1995", "authors": "C B\u00e4ckstr\u00f6m; B Nebel"}, {"ref_id": "b4", "title": "Graph-Based Algorithms for Boolean Function Manipulation", "journal": "IEEE Transactions on Computers", "year": "1986", "authors": "R E Bryant"}, {"ref_id": "b5", "title": "Symbolic Pattern Databases in Heuristic Search Planning", "journal": "", "year": "2002", "authors": "S Edelkamp"}, {"ref_id": "b6", "title": "Limits and Possibilities of BDDs in State Space Search", "journal": "", "year": "2008", "authors": "S Edelkamp; P Kissmann"}, {"ref_id": "b7", "title": "OBDDs in Heuristic Search", "journal": "Springer", "year": "1998", "authors": "S Edelkamp; F Reffel"}, {"ref_id": "b8", "title": "Lifted Fact-Alternating Mutex Groups and Pruned Grounding of Classical Planning Problems", "journal": "", "year": "2020", "authors": "D Fi\u0161er"}, {"ref_id": "b9", "title": "Strengthening Potential Heuristics with Mutexes and Disambiguations", "journal": "", "year": "2020", "authors": "D Fi\u0161er; R Hor\u010d\u00edk; A Komenda"}, {"ref_id": "b10", "title": "The Com-plementary2 Planner in IPC 2018", "journal": "", "year": "2018", "authors": "S Franco; L H S Lelis; M Barley"}, {"ref_id": "b11", "title": "On Creating Complementary Pattern Databases", "journal": "", "year": "2017", "authors": "S Franco; A Torralba; L H Lelis; M Barley"}, {"ref_id": "b12", "title": "A Formal Basis for the Heuristic Determination of Minimum Cost Paths", "journal": "IEEE Transactions on Systems Science and Cybernetics", "year": "1968", "authors": "P E Hart; N J Nilsson; B Raphael"}, {"ref_id": "b13", "title": "Landmarks, Critical Paths and Abstractions: What's the Difference Anyway?", "journal": "", "year": "2009", "authors": "M Helmert; C Domshlak"}, {"ref_id": "b14", "title": "Merge & Shrink Abstraction: A Method for Generating Lower Bounds in Factored State Spaces", "journal": "Journal of the Association for Computing Machinery", "year": "2014", "authors": "M Helmert; P Haslum; J Hoffmann; R Nissim"}, {"ref_id": "b15", "title": "Stateset branching: Leveraging BDDs for heuristic search", "journal": "Artificial Intelligence", "year": "2008", "authors": "R M Jensen; M M Veloso; R E Bryant"}, {"ref_id": "b16", "title": "Improving Cost-Optimal Domain-Independent Symbolic Planning", "journal": "", "year": "2011", "authors": "P Kissmann; S Edelkamp"}, {"ref_id": "b17", "title": "Symbolic Model Checking", "journal": "Kluwer Academic Publishers", "year": "1993", "authors": "K L Mcmillan"}, {"ref_id": "b18", "title": "A Normal Form for Classical Planning Tasks", "journal": "", "year": "2015", "authors": "F Pommerening; M Helmert"}, {"ref_id": "b19", "title": "From Non-Negative to General Operator Cost Partitioning", "journal": "", "year": "2015", "authors": "F Pommerening; M Helmert; G R\u00f6ger; J Seipp"}, {"ref_id": "b20", "title": "Fast Downward Scorpion", "journal": "", "year": "2018", "authors": "J Seipp"}, {"ref_id": "b21", "title": "Counterexample-Guided Cartesian Abstraction Refinement for Classical Planning", "journal": "Journal of Artificial Intelligence Research", "year": "2018", "authors": "J Seipp; M Helmert"}, {"ref_id": "b22", "title": "New Optimization Functions for Potential Heuristics", "journal": "", "year": "2015", "authors": "J Seipp; F Pommerening; M Helmert"}, {"ref_id": "b23", "title": "An Analysis of Merge Strategies for Merge-and-Shrink Heuristics", "journal": "", "year": "2016", "authors": "S Sievers; M Wehrle; M Helmert"}, {"ref_id": "b24", "title": "When Perfect Is Not Good Enough: On the Search Behaviour of Symbolic Heuristic Search", "journal": "", "year": "2020", "authors": "D Speck; F Gei\u00dfer; R Mattm\u00fcller"}, {"ref_id": "b25", "title": "SymBA*: A Symbolic Bidirectional A* Planner", "journal": "", "year": "2014", "authors": "\u00c1 Torralba; V Alc\u00e1zar; D Borrajo; P Kissmann; S Edelkamp"}, {"ref_id": "b26", "title": "Efficient symbolic search for cost-optimal planning", "journal": "Artificial Intelligence", "year": "2017", "authors": "\u00c1 Torralba; V Alc\u00e1zar; P Kissmann; S Edelkamp"}, {"ref_id": "b27", "title": "Abstraction Heuristics for Symbolic Bidirectional Search", "journal": "", "year": "2016", "authors": "\u00c1 Torralba; C Linares L\u00f3pez; D Borrajo"}, {"ref_id": "b28", "title": "Symbolic perimeter abstraction heuristics for cost-optimal planning", "journal": "Artificial Intelligence", "year": "2018", "authors": "\u00c1 Torralba; C L L\u00f3pez; D Borrajo"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "which concludes the proof. Lemma 7. Let P, D, and Q be as before, and let \u03c0 = o 1 , . . . , o n denote a sequence of operators applicable in I, and let s = \u03c0 I . If |D(o, V )| = 1 for every o \u2208 O and every V \u2208 vars(eff(o)), then f \u2208I P(f ) + i\u2208[n] Q(o i ) = f \u2208s P(f ). Proof. (By induction) It clearly holds for an empty sequence \u03c0. Let s denote a state reachable from I by a sequence \u03c0 = o 1 , . . . , o n\u22121 , and let o n \u2208 O denote an operator applicable in s , and let s = o n s . Now, assume that f \u2208I", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Let cost(o i ) = 0 for all i \u2208 [4], and cost(o 5 ) = 1, and let Q(o 1 ) = 1, Q(o 2 ) = 0, Q(o 3 ) = 0.9, Q(o 4 ) = 0.1, and Q(o 5 ) = \u22121, and let h Q (I) = 0. A simple example showing inconsistency after rounding operator potentials down to the nearest integers.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: (a)-(d): Comparison of symbolic forward uniform-cost search (bfw) against GHSETA * with the best-performing variant of the consistent operator-potential heuristic (A+I) on commonly solved tasks. (e): Time (s) spent computing potentials for the original formulation (LP) and with added constraints on integer operator potentials (MIP) on transformed tasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "S1k+I 13 3 6 16 -9 22 22 22 29 29 35 34 1 081 pd-A+I 12 2 5 16 15 -22 22 23 27 30 32 34 1", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "partial state called goal, and a state s is a goal state iff G \u2286 s. Let p, t be partial states. We say that t extends p if p \u2286 t.O is a finite set of operators, each operator o \u2208 O has a precondition pre(o) and effect eff(o), which are partial states over V, and a cost cost(o) \u2208 R + 0 . An operator o is applicable in a state s iff pre(o) \u2286 s. The resulting state of applying an applicable operator o in a state s is another state", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Summary of domain coverage. A value in row x", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "F = { V, v | V \u2208 V, v \u2208 dom(V )}, and the set of facts of variable V is denoted by F V = { V, v | v \u2208 dom(V )}.", "formula_coordinates": [2.0, 54.0, 100.99, 238.51, 20.61]}, {"formula_id": "formula_1", "formula_text": "p = { V, p[V ] | V \u2208 vars(p)}. A partial state s is a state if vars(s) = V. I is an initial state. G is a", "formula_coordinates": [2.0, 54.0, 166.68, 238.5, 19.99]}, {"formula_id": "formula_2", "formula_text": "o s such that o s [V ] = eff(o)[V ] for ev- ery V \u2208 vars(eff(o)), and o s [V ] = s[V ] for every V \u2208 V \\vars(eff(o)). We also assume that pre(o)[V ] = eff(o)[V ] for every V \u2208 vars(pre(o)) \u2229 vars(eff(o)).", "formula_coordinates": [2.0, 54.0, 267.29, 238.5, 41.83]}, {"formula_id": "formula_3", "formula_text": "\u03c0 = o 1 , . . . , o n is applicable in a state s 0 if there are states s 1 , . . . , s n such that o i is applicable in s i\u22121 and s i = o i s i\u22121 for i \u2208 [n].", "formula_coordinates": [2.0, 54.0, 333.85, 238.5, 31.57]}, {"formula_id": "formula_4", "formula_text": "F \u2286 F V is called a disambigua- tion of V for p if for every reachable state s \u2208 R such that p \u2286 s it holds that F \u2229 s = \u2205 (i.e., V, s[V ] \u2208 F ).", "formula_coordinates": [3.0, 54.0, 244.18, 238.51, 30.94]}, {"formula_id": "formula_5", "formula_text": "Definition 3. A mapping D : (O \u00d7 V) \u222a V \u2192 2 F is called a disambiguation map if (i) for every operator o \u2208 O and every variable V \u2208 vars(eff(o)) it holds that D(o, V ) \u2286 F V is a disambiguation of V for pre(o) such that |D(o, V )| \u2265 1; and (ii) for every variable V \u2208 V it holds that D(V ) \u2286 F V is a disambiguation of V for G such that |D(V )| \u2265 1.", "formula_coordinates": [3.0, 54.0, 434.44, 238.51, 65.33]}, {"formula_id": "formula_6", "formula_text": "V \u2208V max f \u2208D(V ) P(f ) \u2264 0 (1) and for every operator o \u2208 O it holds that V \u2208vars(eff(o)) max f \u2208D(o,V ) P(f ) \u2212 f \u2208eff(o) P(f ) \u2264 cost(o), (2)", "formula_coordinates": [3.0, 54.0, 579.4, 238.5, 62.98]}, {"formula_id": "formula_7", "formula_text": "Q(o) = f \u2208eff(o) P(f ) \u2212 V \u2208vars(eff(o)) max f \u2208D(o,V ) P(f ) (3) for every operator o \u2208 O.", "formula_coordinates": [3.0, 319.5, 264.17, 238.5, 34.51]}, {"formula_id": "formula_8", "formula_text": "i\u2208[n] Q(o i ) = h P (s) for every sequence of operators \u03c0 = o 1 , . . . , o n such that \u03c0 I = s.", "formula_coordinates": [3.0, 323.37, 547.17, 234.62, 23.92]}, {"formula_id": "formula_9", "formula_text": "If |D(o, V )| = 1 for every V \u2208 vars(eff(o)), then f \u2208s P(f ) + Q(o) = f \u2208o s P(f ). Proof. Let A = V \u2208vars(eff(o)) D(o, V ). Since |D(o, V )| = 1 for every V \u2208 vars(eff(o)), Equation (3) can be rewritten as Q(o) = f \u2208eff(o) P(f ) \u2212 f \u2208A P(f ).", "formula_coordinates": [3.0, 330.02, 684.24, 227.98, 22.11]}, {"formula_id": "formula_10", "formula_text": "f \u2208A P(f ) + Q(o) = f \u2208eff(o) P(f ). Expanding Q(o) gives us f \u2208A P(f ) + f \u2208eff(o) P(f ) \u2212 f \u2208A P(f ) = f \u2208eff(o) P(f ),", "formula_coordinates": [4.0, 54.0, 155.38, 238.49, 37.55]}, {"formula_id": "formula_11", "formula_text": "P(f )+ i\u2208[n\u22121] Q(o i ) = f \u2208s P(f )", "formula_coordinates": [4.0, 64.52, 305.86, 227.97, 23.98]}, {"formula_id": "formula_12", "formula_text": "f \u2208I P(f ) + i\u2208[n] Q(o i ) = f \u2208s P(f ). From the as- sumption, it follows that f \u2208I P(f ) + i\u2208[n\u22121] Q(o i ) + Q(o n ) = f \u2208s P(f ) + Q(o n ), so it is enough to show that f \u2208s P(f ) + Q(o n ) = f \u2208s P(f ) (Lemma 6).", "formula_coordinates": [4.0, 54.0, 331.91, 238.5, 50.4]}, {"formula_id": "formula_13", "formula_text": "|D(o, V )| = 1 for every o \u2208 O and every V \u2208 vars(eff(o)). An operator-potential heuristic h Q : R \u2192 R \u222a {\u221e} for Q is defined as h Q (s) = f \u2208I P(f ) + i\u2208[n] Q(o i )(4)", "formula_coordinates": [4.0, 54.0, 452.78, 238.51, 64.15]}, {"formula_id": "formula_14", "formula_text": "I s 1 s 2 s 3 G o 1 o 2 o 3 o 4 o 5", "formula_coordinates": [4.0, 371.72, 54.81, 134.05, 38.0]}, {"formula_id": "formula_15", "formula_text": "o 1 ) = eff(o 2 ) = eff(o 3 ) = eff(o 4 ) = eff(o), but different precon- ditions pre(o 1 ) = pre(o) \u222a {f 1 , f 3 }, pre(o 2 ) = pre(o) \u222a {f 1 , f 4 }, pre(o 3 ) = pre(o) \u222a {f 2 , f 3 }, and pre(o 4 ) = pre(o) \u222a {f 2 , f 4 }.", "formula_coordinates": [4.0, 319.5, 281.73, 238.5, 53.49]}, {"formula_id": "formula_16", "formula_text": "1 for each c, q \u2208 {cost(o), Q(o) | o \u2208 O} do 2 Construct Tc,q from {o \u2208 O | cost(o) = c, Q(o) = q}; 3 hI \u2190 f \u2208I P(f ); 4 S 0,h I \u2190 {I}; 5 open \u2190 { max(0, hI ), S", "formula_coordinates": [5.0, 321.49, 112.88, 221.12, 49.69]}, {"formula_id": "formula_17", "formula_text": "f \u2190 g + c + max(0, h + q); 16", "formula_coordinates": [5.0, 319.7, 254.33, 163.33, 17.34]}], "doi": ""}