{"title": "EPro-PnP: Generalized End-to-End Probabilistic Perspective-n-Points for Monocular Object Pose Estimation", "authors": "Hansheng Chen; Pichao Wang; Fan Wang; Wei Tian; Lu Xiong; Hao Li", "pub_date": "2022-08-11", "abstract": "Locating 3D objects from a single RGB image via Perspective-n-Points (PnP) is a long-standing problem in computer vision. Driven by end-to-end deep learning, recent studies suggest interpreting PnP as a differentiable layer, so that 2D-3D point correspondences can be partly learned by backpropagating the gradient w.r.t. object pose. Yet, learning the entire set of unrestricted 2D-3D points from scratch fails to converge with existing approaches, since the deterministic pose is inherently non-differentiable. In this paper, we propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation, which outputs a distribution of pose on the SE(3) manifold, essentially bringing categorical Softmax to the continuous domain. The 2D-3D coordinates and corresponding weights are treated as intermediate variables learned by minimizing the KL divergence between the predicted and target pose distribution. The underlying principle unifies the existing approaches and resembles the attention mechanism. EPro-PnP significantly outperforms competitive baselines, closing the gap between PnP-based method and the taskspecific leaders on the LineMOD 6DoF pose estimation and nuScenes 3D object detection benchmarks. 3 * Part of work done during an internship at Alibaba Group.", "sections": [{"heading": "Introduction", "text": "Estimating the pose (i.e., position and orientation) of 3D objects from a single RGB image is an important task in computer vision. This field is often subdivided into specific tasks, e.g., 6DoF pose estimation for robot manipulation and 3D object detection for autonomous driving. Although they share the same fundamentals of pose estimation, the different nature of the data leads to biased choice of methods. Top performers [34,48,50] on the 3D object detection benchmarks [8,17] fall into the category of direct 4DoF pose prediction, leveraging the advances in end-toend deep learning. On the other hand, the 6DoF pose estimation benchmark [23] is largely dominated by geometrybased methods [24,52], which exploit the provided 3D object models and achieve a stable generalization performance. However, it is quite challenging to bring together the best of both worlds, i.e., training a geometric model to learn the object pose in an end-to-end manner. There has been recent proposals for an end-to-end framework based on the Perspective-n-Points (PnP) approach [4,6,9,12]. The PnP algorithm itself solves the pose from a set of 3D points in object space and their corresponding 2D projections in image space, leaving the problem of constructing these correspondences. Vanilla correspondence learning [11, 28, 29, 35, 35-37, 40, 46, 52] leverages the geometric prior to build surrogate loss functions, forcing the network to learn a set of pre-defined correspondences. Endto-end correspondence learning [4,6,9,12] interprets the PnP as a differentiable layer and employs pose-driven loss function, so that gradient of the pose error can be backpropagated to the 2D-3D correspondences.\nHowever, existing work on differentiable PnP learns only a portion of the correspondences (either 2D coordinates [12], 3D coordinates [4,6] or corresponding weights [9]), assuming other components are given a priori. This raises an important question: why not learn the entire set of points and weights altogether in an end-to-end manner? The simple answer is: the solution of the PnP problem is inherently non-differentiable at some points, causing training difficulties and convergence issues. For example, a PnP problem can have ambiguous solutions [32,38].\nTo overcome the above limitations, we propose a generalized end-to-end probabilistic PnP (EPro-PnP) approach that enables learning the weighted 2D-3D point correspondences entirely from scratch (Figure 1). The main idea is straightforward: deterministic pose is non-differentiable, but the probability density of pose is apparently differentiable, just like categorical classification scores. Therefore, we interpret the output of PnP as a probabilistic distribution parameterized by the learnable 2D-3D correspondences. During training, the Kullback-Leibler (KL) divergence between the predicted and target pose distributions is minimized as the loss function, which can be efficiently implemented by the Adaptive Multiple Importance Sampling [14] algorithm.\nAs a general approach, EPro-PnP inherently unifies existing correspondence learning techniques (Section 3.1). Moreover, just like the attention mechanism [44], the corresponding weights can be trained to automatically focus on important point pairs, allowing the networks to be designed with inspiration from attention-related work [10,49,54].\nTo summarize, our main contributions are as follows:\n\u2022 We propose the EPro-PnP, a probabilistic PnP layer for general end-to-end pose estimation via learnable 2D-3D correspondences. \u2022 We demonstrate that EPro-PnP can easily reach toptier performance for 6DoF pose estimation by simply inserting it into the CDPN [29] framework. \u2022 We demonstrate the flexibility of EPro-PnP by proposing deformable correspondence learning for accurate 3D object detection, where the entire 2D-3D correspondences are learned from scratch.", "publication_ref": ["b33", "b47", "b49", "b7", "b16", "b22", "b23", "b51", "b3", "b5", "b8", "b11", "b3", "b5", "b8", "b11", "b11", "b3", "b5", "b8", "b31", "b37", "b13", "b43", "b9", "b48", "b53", "b28"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Related Work", "text": "Geometry-Based Object Pose Estimation In general, geometry-based methods exploit the points, edges or other types of representation that are subject to the projection constraints under the perspective camera. Then, the pose can be solved by optimization. A large body of work utilizes point representation, which can be categorized into sparse keypoints and dense correspondences. BB8 [37] and RTM3D [28] locate the corners of the 3D bounding box as keypoints, while PVNet [36] defines the keypoints by farthest point sampling and Deep MANTA [11] by handcrafted templates. On the other hand, dense correspondence methods [13,29,35,46,52] predict pixel-wise 3D coordinates within a cropped 2D region. Most existing geometry-based methods follow a two-stage strategy, where the intermediate representations (i.e., 2D-3D correspondences) are learned with a surrogate loss function, which is sub-optimal compared to end-to-end learning.", "publication_ref": ["b36", "b27", "b35", "b10", "b12", "b28", "b34", "b45", "b51"], "figure_ref": [], "table_ref": []}, {"heading": "End-to-End Correspondence Learning", "text": "To mitigate the limitation of surrogate correspondence learning, end-to-end approaches have been proposed to backpropagate the gradient from pose to intermediate representation. By differentiating the PnP operation, Brachmann and Rother [6] propose a dense correspondence network where 3D points are learnable, BPnP [12] predicts 2D keypoint locations, and BlindPnP [9] learns the corresponding weight matrix given a set of unordered 2D/3D points. Beyond point correspondence, RePOSE [24] proposes a feature-metric correspondence network trained in a similar end-to-end fashion. The above methods are all coupled with surrogate regularization loss, otherwise convergence is not guaranteed due to the non-differentiable nature of deterministic pose. Under the probabilistic framework, these methods can be regarded as a Laplace approximation approach (Section 3.1) or a local regularization technique (Section 3.4).", "publication_ref": ["b5", "b11", "b8", "b23"], "figure_ref": [], "table_ref": []}, {"heading": "Probabilistic Deep Learning", "text": "Probabilistic methods account for uncertainty in the model and the data, known respectively as epistemic and aleatoric uncertainty [25]. The latter involves interpreting the prediction as learnable probabilistic distributions. Discrete categorical distribution via Softmax has been widely adopted as a smooth approximation of one-hot arg max for end-to-end classification. This inspired works such as DSAC [4], a smooth RANSAC with a finite hypothesis pool. Meanwhile, tractable parametric distributions (e.g., normal distribution) are often used in predicting continuous variables [13,18,22,25,26,51], and mixture distributions can be employed to further capture ambiguity [3,5,31], e.g., ambiguous 6DoF pose [7]. In this paper, we propose yet a unique contribution: backpropagating a complicated continuous distribution derived from a nested optimization layer (the PnP layer), essentially making it a continuous counterpart of Softmax.", "publication_ref": ["b24", "b3", "b12", "b17", "b21", "b24", "b25", "b50", "b2", "b4", "b30", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Generalized End-to-End Probabilistic PnP", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Overview", "text": "Given an object proposal, our goal is to predict a set\nX = x 3D i , x 2D i , w 2D i i = 1 \u2022 \u2022 \u2022 N of N corresponding points, with 3D object coordinates x 3D i \u2208 R 3 , 2D image coordinates x 2D i \u2208 R 2 , and 2D weights w 2D i \u2208 R 2 + , from\nwhich a weighted PnP problem can be formulated to estimate the object pose relative to the camera. The essence of a PnP layer is searching for an optimal pose y (expanded as rotation matrix R and translation vector t) that minimizes the cumulative squared weighted reprojection error:\narg min y 1 2 N i=1 w 2D i \u2022 \u03c0(Rx 3D i + t) \u2212 x 2D i fi(y)\u2208R 2 2 , (1\n)\nwhere \u03c0(\u2022) is the projection function with camera intrinsics involved, \u2022 stands for element-wise product, and f i (y) compactly denotes the weighted reprojection error. Eq. (1) formulates a non-linear least squares problem that may have non-unique solutions, i.e., pose ambiguity [32,38]. Previous work [6,9,12] only backpropagates through a local solution y * , which is inherently unstable and non-differentiable. To construct a differentiable alternative for end-to-end learning, we model the PnP output as a distribution of pose, which guarantees differentiable probability density. Consider the cumulative error to be the negative logarithm of the likelihood function p(X|y) defined as:\np(X|y) = exp \u2212 1 2 N i=1 f i (y) 2 .(2)\nWith an additional prior pose distribution p(y), we can derive the posterior pose p(y|X) via the Bayes theorem. Using an uninformative prior, the posterior density is simplified to the normalized likelihood:\np(y|X) = exp \u2212 1 2 N i=1 f i (y) 2 exp \u2212 1 2 N i=1 f i (y) 2 dy .(3)\nEq. (3) can be interpreted as a continuous counterpart of categorical Softmax. KL Loss Function During training, given a target pose distribution with probability density t(y), the KL divergence D KL (t(y) p(y|X)) is minimized as training loss. Intuitively, pose ambiguity can be captured by the multiple modes of p(y|X), and convergence is ensured such that wrong modes are suppressed by the loss function. Dropping the constant, the KL divergence loss can be written as:\nL KL = \u2212 t(y) log p(X|y) dy + log p(X|y) dy. (4)\nWe empirically found it effective to set a narrow (Diraclike) target distribution centered at the ground truth y gt , yielding the simplified loss (after substituting Eq. (2)):\nL KL = 1 2 N i=1 f i (y gt ) 2 Ltgt (reproj. at target pose) + log exp \u2212 1 2 N i=1 f i (y) 2 dy\nLpred (reproj. at predicted pose)\n.\n(5) The only remaining problem is the integration in the second term, which is elaborated in Section 3.2. Comparison to Reprojection-Based Method The two terms in Eq. ( 5) are concerned with the reprojection errors at target and predicted pose respectively. The former is often used as a surrogate loss in previous work [6,12,13]. However, the first term alone cannot handle learning all 2D-3D points without imposing strict regularization, as the minimization could simply drive all the points to a concentrated location without pose discrimination. The second term originates from the normalization factor in Eq. (3), and is crucial to a discriminative loss function, as shown in Figure 2.\nComparison to Implicit Differentiation Method Existing work on end-to-end PnP [9,12] derives a single solution of a particular solver y * = PnP (X) via implicit function theorem [19]. In the probabilistic framework, this is essentially the Laplace method that approximates the posterior by N (y * , \u03a3 y * ), where both y * and \u03a3 y * can be estimated by the PnP solver with analytical derivatives [13]. A special case is that, with \u03a3 y * simplified to be isotropic, the approximated KL divergence can be simplified to the L2 loss y * \u2212 y gt 2 used in [9]. However, the Laplace approximation is inaccurate for non-normal posteriors with ambiguity, therefore does not guarantee global convergence.", "publication_ref": ["b31", "b37", "b5", "b8", "b11", "b5", "b11", "b12", "b8", "b11", "b18", "b12", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Monte Carlo Pose Loss", "text": "In this section, we introduce a GPU-friendly efficient Monte Carlo approach to the integration in the proposed loss function, based on the Adaptive Multiple Importance Sampling (AMIS) algorithm [14].\nConsidering q(y) to be the probability density function of a proposal distribution that approximates the shape of the\nintegrand exp \u2212 1 2 N i=1 f i (y)\n2 , and y j to be one of the K samples drawn from q(y), the estimation of the second term L pred in Eq. (5) is thus:\nL pred \u2248 log 1 K K j=1 exp \u2212 1 2 N i=1 f i (y j ) 2 q(y j ) vj (importance weight) ,(6)\nwhere v j compactly denotes the importance weight at y j . Eq. (6) gives the vanilla importance sampling, where the choice of proposal q(y) strongly affects the numerical stability. The AMIS algorithm is a better alternative as it iteratively adapts the proposal to the integrand. In brief, AMIS utilizes the sampled importance weights from past iterations to estimate the new proposal. Then, all previous samples are re-weighted as being homogeneously sampled from a mixture of the overall sum of proposals. Initial proposal can be determined by the mode and covariance of the predicted pose distribution (see supplementary for details). A pseudo-code is given in Algorithm 1.", "publication_ref": ["b13", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Choice of Proposal Distribution", "text": "The proposal distributions for position and orientation have to be chosen separately in a decoupled manner, since the orientation space is non-Euclidean. For position, we adopt the 3DoF multivariate t-distribution. For 1D yaw-only orientation, we use a mixture of von Mises and uniform distribution. For 3D orientation represented by unit quaternion, the angular central Gaussian distribution [43] is adopted.", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "Backpropagation", "text": "In general, the partial derivatives of the loss function defined in Eq. ( 5) is:\n\u2202L KL \u2202(\u2022) = \u2202 \u2202(\u2022) 1 2 N i=1 f i (y gt ) 2 \u2212 E y\u223cp(y|X) \u2202 \u2202(\u2022) 1 2 N i=1 f i (y) 2 ,(7)\nwhere the first term is the gradient of reprojection errors at target pose, and the second term is the expected gradient of reprojection errors over predicted pose distribution, which is approximated by backpropagating each weighted sample in the Monte Carlo pose loss.\nBalancing Uncertainty and Discrimination Consider the negative gradient w.r.t. the corresponding weights w 2D i :\n\u2212 \u2202L KL \u2202w 2D i = w 2D i \u2022 \u2212r \u20222 i (y gt ) + E y\u223cp(y|X) r \u20222 i (y) ,(8)\nwhere r i (y) = \u03c0(Rx 3D i + t) \u2212 x 2D i (unweighted reprojection error), and (\u2022) \u20222 stands for element-wise square. The first bracketed term \u2212r \u20222 i (y gt ) with negative sign indicates that correspondences with large reprojection error (hence high uncertainty) shall be weighted less. The second term Ey\u223cp(y|X) r \u20222 i (y) is relevant to the variance of reprojection error over the predicted pose. The positive sign indicates that sensitive correspondences should be weighted more, because they provide stronger pose discrimination. The final gradient is thus a balance between the uncertainty and discrimination, as shown in Figure 3. Existing work [13,36] on learning uncertainty-aware correspondences only considers the former, hence lacking the discriminative ability.", "publication_ref": ["b12", "b35"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "Local Regularization of Derivatives", "text": "While the KL divergence is a good metric for the probabilistic distribution, for inference it is still required to es-Algorithm 1: AMIS-based Monte Carlo pose loss\nInput : X = {x 3D i , x 2D i , w 2D i } Output: L pred 1 y * , \u03a3 y * \u2190 PnP (X) // Laplace approximation 2 Fit q 1 (y) to y * , \u03a3 y * // initial proposal 3 for 1 \u2264 t \u2264 T do 4 Generate K samples y t j=1\u2022\u2022\u2022K from q t (y) 5 for 1 \u2264 j \u2264 K do 6 P t j \u2190 p(X|y t j ) // evaluate integrand 7 for 1 \u2264 \u03c4 \u2264 t and 1 \u2264 j \u2264 K do 8 Q \u03c4 j \u2190 1 t t m=1 q m (y \u03c4 j )\n// eval proposal mix 9 v \u03c4 j \u2190 P \u03c4 j /Q \u03c4 j // importance weight 10 if t < T then 11\nEstimate q t+1 (y) from all weighted samples timate the exact pose y * by solving the PnP problem in Eq. (1). The common choice of high precision is to utilize the iterative PnP solver based on the Levenberg-Marquardt (LM) algorithm -a robust variant of the Gauss-Newton (GN) algorithm, which solves the non-linear least squares by the first and approximated second order derivatives. To aid derivative-based optimization, we regularize the derivatives of the log density log p(y|X) w.r.t. the pose y, by encouraging the LM step \u2206y to find the true pose y gt .\n{y \u03c4 j , v \u03c4 j | 1 \u2264 \u03c4 \u2264 t, 1 \u2264 j \u2264 K } 12 L pred \u2190 log 1 T K T t=1 K j=1 v\nTo employ the regularization during training, a detached solution y * is obtained first. Then, at y * , another iteration step is evaluated via the GN algorithm (which ideally equals 0 if y * has converged to the local optimum):\n\u2206y = \u2212(J T J + \u03b5I) \u22121 J T F (y * ),(9)\nwhere\nF (y * ) = f T 1 (y * ), f T 2 (y * ), \u2022 \u2022 \u2022 , f T N (y * )\nT is the concatenated weighted reprojection errors of all points, J = \u2202F (y)/ \u2202y T y=y * is the Jacobian matrix, and \u03b5 is a small value for numerical stability. Note that \u2206y is analytically differentiable. We therefore design the regularization loss as follows:\nL reg = l(y * + \u2206y, y gt ),(10)\nwhere l(\u2022, \u2022) is a distance metric for pose. We adopt smooth L1 for position and cosine similarity for orientation (see supplementary materials for details). Note that the gradient is only backpropagated through \u2206y, encouraging the step to be non-zero if y * = y gt . This regularization loss can be also used as a standalone objective to train pose estimators [24]. However, this objective alone cannot handle pose ambiguity properly, and is thus regarded as a secondary regularization in this paper.", "publication_ref": ["b23"], "figure_ref": [], "table_ref": []}, {"heading": "Attention-Inspired Correspondence Networks", "text": "As discussed in Section 3.3, the balance between uncertainty and discrimination enables locating important correspondences in an attention-like manner. This inspires us to take elements from attention-related work, i.e., the Softmax layer and the deformable sampling [54].\nIn this section, we present two networks with EPro-PnP layer for 6DoF pose estimation and 3D object detection, respectively. For the former, EPro-PnP is incorporated into the existing dense correspondence architecture [29]. For the latter, we propose a radical deformable correspondence network to explore the flexibility of EPro-PnP.", "publication_ref": ["b53", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "Dense Correspondence Network", "text": "For a strict comparison against existing PnP-based pose estimators, this paper takes the network from CDPN [29] as a baseline, adding minor modifications to fit the EPro-PnP.\nThe original CDPN feeds cropped image regions within the detected 2D boxes into the pose estimation network, to which two decoupled heads are appended for rotation and translation respectively. The rotation head is PnP-based while the translation head uses direct regression. This paper discards the translation head to focus entirely on PnP.\nModifications are only made to the output layers. As shown in Figure 4, the original confidence map is expanded to two-channel XY weights with spatial Softmax and dynamic global weight scaling. Inspired by the attention mechanism [44], the Softmax layer is a vital element for stable training, as it translates the absolute corresponding weights into a relative measurement. On the other hand, the global weight scaling factors represent the global concentration of the predicted pose distribution, ensuring a better convergence of the KL divergence loss.\nThe dense correspondence network can be trained solely with the KL divergence loss L KL to achieve decent performance. For top-tier performance, it is still beneficial to utilize additional coordinate regression as intermediate supervision, not to stabilize convergence but to introduce the geometric knowledge from the 3D models. Therefore, we keep the masked coordinate regression loss from CDPN [29] but leave out its confidence loss. Furthermore, the performance can be elevated by imposing the regularization loss L reg in Eq. (10).", "publication_ref": ["b28", "b43", "b28"], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Deformable Correspondence Network", "text": "Inspired by Deformable DETR [54], we propose a novel deformable correspondence network for 3D object detection, in which the entire 2D-3D coordinates and weights are learned from scratch.\nAs shown in Figure 5, the deformable correspondence network is an extension of the FCOS3D [47] framework. The original FCOS3D is a one-stage detector that directly regresses the center offset, depth, and yaw orientation of multiple objects for 4DoF pose estimation. In our adaptation, the outputs of the multi-level FCOS head [41] are modified to generate object queries instead of directly predicting the pose. Also inspired by Deformable DETR [54], the appearance and position of a query is disentangled into the embedding vector and the reference point. A multi-head deformable attention layer [54] is adopted to sample the key-value pairs from the dense features, with the value projected into point-wise features, and meanwhile aggregated into the object-level features.\nThe point features are passed into a subnet that predicts the 3D points and corresponding weights (normalized by Softmax). Following MonoRUn [13], the 3D points are set in the normalized object coordinate (NOC) space to handle categorical objects of various sizes.\nThe object features are responsible for predicting the object-level properties: (a) the 3D score (i.e., 3D localization confidence), (b) the weight scaling factor (same as in Section 4.1), (c) the 3D box size for recovering the absolute scale of the 3D points, and (d) other optional properties (velocity, attribute) required by the nuScenes benchmark [8].\nThe deformable 2D-3D correspondences can be learned solely with the KL divergence loss L KL , preferably in conjunction with the regularization loss L reg . Other auxiliary losses can be imposed onto the dense features for enhanced accuracy. Details are given in supplementary materials. (inferring mode)\nFigure 5. The deformable correspondence network based on the FCOS3D [47] detector. Note that the sampled point-wise features are shared by the point-level subnet and the deformable attention layer that aggregates the features for object-level predictions.", "publication_ref": ["b53", "b46", "b40", "b53", "b53", "b12", "b7", "b46"], "figure_ref": [], "table_ref": []}, {"heading": "Experiments", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets and Metrics", "text": "LineMOD Dataset and Metrics The LineMOD dataset [23] consists of 13 sequences, each containing about 1.2K images annotated with 6DoF poses of a single object. Following [5], the images are split into the training and testing sets, with about 200 images per object for training. For data augmentation, we use the same synthetic data as in CDPN [29]. We use two common metrics for evaluation: ADD(-S) and n\u00b0, n cm. The ADD measures whether the average deviation of the transformed model points is less than a certain fraction of the object's diameter (e.g., ADD-0.1d). For symmetric objects, ADD-S computes the average distance to the closest model point. n\u00b0, n cm measures the accuracy of pose based on angular/positional error thresholds. All metrics are presented as percentages.\nnuScenes Dataset and Metrics The nuScenes 3D object detection benchmark [8]  Finally, there is a nuScenes detection score (NDS) computed as a weighted average of the above metrics.", "publication_ref": ["b22", "b4", "b28", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "EPro-PnP Configuration For the PnP formulation in Eq. (1), in practice the actual reprojection costs are robustified by the Huber kernel \u03c1(\u2022):\narg min y 1 2 N i=1 \u03c1 f i (y) 2 . (11\n)\nThe Huber kernel with threshold \u03b4 is defined as:\n\u03c1(s) = s, s \u2264 \u03b4 2 , \u03b4(2 \u221a s \u2212 \u03b4), s > \u03b4 2 . (12\n)\nWe use an adaptive threshold as described in the supplementary materials. For Monte Carlo pose loss, we set the AMIS iteration count T to 4 and the number of samples per iteration K to 128. The loss weights are tuned such that L KL produces roughly the same magnitude of gradient as typical coordinate regression, while the gradient from L reg are kept very low. The weight normalization technique in [13] is adopted to compute the dynamic loss weight for L KL .\nTraining the Dense Correspondence Network General settings are kept the same as in CDPN [29] (with ResNet-34 [21] as backbone) for strict comparison, except that we increase the batch size to 32 for less training wall time.\nThe network is trained for 160 epochs by RMSprop on the LineMOD dataset [23]. To reduce the Monte Carlo overhead, 512 points are randomly sampled from the 64\u00d764 dense points to compute L KL .", "publication_ref": ["b12", "b28", "b20", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Training the Deformable Correspondence Network", "text": "We adopt the same detector architecture as in FCOS3D [47], with ResNet-101-DCN [15] as backbone. The network is trained for 12 epochs by the AdamW [30] optimizer, with a batch size of 12 images across 4 GPUs on the nuScenes dataset [8].", "publication_ref": ["b46", "b14", "b29", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "Results on the LineMOD Benchmark", "text": "Comparison to the CDPN baseline with Ablations The contributions of every single modification to CDPN [29] are revealed in Table 1. From the results it can be observed that: The results clearly demonstrate that EPro-PnP can unleash the enormous potential of the classical PnP approach, without any fancy network design or decoupling tricks.\n\u2022\nComparison to the State of the Art As shown in Table 2, despite modified from the lower baseline, EPro-PnP easily reaches comparable performance to the top pose refiner RePOSE [24], which adds extra overhead to the PnP-based initial estimator PVNet [36]. Among all these entries, EPro-PnP is the most straightforward as it simply solves the PnP problem itself, without refinement network [24,52], disentangled translation [29,45], or multiple representations [40].  1, detaching the weights from the end-to-end loss has a stronger impact to the performance than detaching the coordinates (\u22128.69 vs. \u22123.08), stressing the importance of attention-like end-to-end weight learning.\nOn the Importance of the Softmax Layer Learning the corresponding weights without the normalization denominator of spatial Softmax (so it becomes exponential activation as in [13]) does not converge, as listed in Table 1.", "publication_ref": ["b28", "b23", "b35", "b23", "b51", "b28", "b44", "b39", "b12"], "figure_ref": [], "table_ref": ["tab_4", "tab_4", "tab_4"]}, {"heading": "Results on the nuScenes Benchmark", "text": "We evaluate 3 variants of EPro-PnP: (a) the basic approach that learns deformable correspondences without geometric prior (enhanced with regularization), (b) adding coordinate regression loss with sparse ground truth extracted from the available LiDAR points as in [13], (c) further adding test-time flip augmentation (TTA) for fair comparison against [47,48]. All results on the validation/test sets are Table 3. Comparison between loss functions by experiments conducted on the same dense correspondence network. For implicit differentiation, we minimize the distance metric of pose in Eq. (10) instead of the reprojection-metric pose loss in BPnP [12].\npresented in Table 4 with comparison to other approaches.\nFrom the validation results it can be observed that:\n\u2022 The basic EPro-PnP significantly outperforms the FCOS3D [47] baseline (NDS 0.425 vs. 0.372). Although it partially benefits from more parameters from the correspondence head, there is still good evidence that: with a proper end-to-end pipeline, PnP can outperform direct pose prediction on a large scale of data.   On the test data, with the advantage in pose accuracy (mATE and mAOE), EPro-PnP achieves the highest NDS score among other task-specific competitors.", "publication_ref": ["b12", "b46", "b47", "b11", "b46"], "figure_ref": [], "table_ref": ["tab_6"]}, {"heading": "Qualitative Analysis", "text": "As illustrated in Figure 7, the dense weight and coordinate maps learned with EPro-PnP generally capture less details compared to CDPN [29], as a result of higher uncertainty around sharp edges. Surprisingly, even though the learned-from-scratch coordinate maps seem to be a mess, the end-to-end pipeline gains comparable pose accuracy to the CDPN baseline (79.46 vs. 79.96). When initialized with pretrained CDPN, EPro-PnP inherits the detailed geometric profile, therefore confining the active weights within the foreground region and achieving the overall best performance. Also note that the weight maps of both derivative regularization and implicit differentiation [12] are more concentrated, biasing towards discrimination over uncertainty.\nFigure 6 shows that the flexibility of EPro-PnP allows predicting multimodal distributions with strong expressive power, successfully capturing the orientation ambiguity without discrete multi-bin classification [33,47] or complicated mixture model [7]. Owing to the ability to model orientation ambiguity, EPro-PnP outperforms other competitors by a wide margin in terms of the AOE metric in Table 4. ", "publication_ref": ["b28", "b11", "b32", "b46", "b6"], "figure_ref": ["fig_5", "fig_4"], "table_ref": ["tab_6"]}, {"heading": "Conclusion", "text": "This paper proposes the EPro-PnP, which translates the non-differentiable deterministic PnP operation into a differentiable probabilistic layer, empowering end-to-end 2D-3D correspondence learning of unprecedented flexibility. The connections to previous work [6,9,12,13] have been thoroughly discussed with theoretical and experimental proofs. For application, EPro-PnP can inspire novel solutions such as the deformable correspondence, or it can be simply integrated into existing PnP-based networks. Beyond the PnP problem, the underlying principles are theoretically generalizable to other learning models with nested optimization layer, known as declarative networks [19].", "publication_ref": ["b5", "b8", "b11", "b12", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "A. Levenberg-Marquardt PnP Solver", "text": "For scalability, we have implemented a PyTorch-based batch Levenberg-Marquardt (LM) PnP solver. The implementation generally follows the Ceres solver [1]. Here, we discuss some important details that are related to the proposed Monte Carlo pose sampling and derivative regularization.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "A.1. Adaptive Huber Kernel", "text": "To robustify the weighted reprojection errors of various scales, we adopt an adaptive Huber kernel with a dynamic threshold \u03b4 for each object, defined as a function of the weights w 2D i and 2D coordinates x 2D i :\n\u03b4 = \u03b4 rel w 2D 1 2 1 N \u2212 1 N i=1 x 2D i \u2212x 2D 2 1 2 ,(13)\nwith the relative threshold \u03b4 rel as hyperparameter, and the mean vectorsw\n2D = 1 N N i=1 w 2D i ,x 2D = 1 N N i=1 x 2D i .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2. LM Step with Huber Kernel", "text": "Adding the Huber kernel influences every related element from the likelihood function to the LM iteration step and derivative regularization loss. Thanks to PyTorch's automatic differentiation, the robustified Monte Carlo KL divergence loss does not require much special handling. For the LM solver, however, the residual F (y) (concatenated weighted reprojection errors) and the Jacobian matrix J have to be rescaled before computing the robustified LM step [42].\nThe rescaled residual blockf i (y) and Jacobian block J i (y) of the i-th point pair are defined as:\nf i (y) = \u03c1 i f i (y),(14)\nJ i (y) = \u03c1 i J i (y),(15)\nwhere\n\u03c1 i = \uf8f1 \uf8f2 \uf8f3 1, f i (y) \u2264 \u03b4, \u03b4 f i (y) , f i (y) > \u03b4,(16)\nJ i (y) = \u2202f i (y) \u2202y T . (17\n)\nFollowing the implementation of Ceres solver [1], the robustified LM iteration step is:\n\u2206y = \u2212 J TJ + \u03bbD 2 \u22121J TF ,(18)\nwhereJ\n= \uf8ee \uf8ef \uf8f0J 1 (y) . . . J N (y) \uf8f9 \uf8fa \uf8fb,F = \uf8ee \uf8ef \uf8f0f 1 (y) . . . f N (y) \uf8f9 \uf8fa \uf8fb,(19)\nD is the square root of the diagonal of the matrixJ TJ , and \u03bb is the reciprocal of the LM trust region radius [1]. Note that the rescaled residual and Jacobian affects the derivative regularization (Eq. ( 10)), as well as the covariance estimation in the next subsection.\nFast Inference Mode We empirically found that in a well-trained model, the LM trust region radius can be initialized with a very large value, effectively rendering the LM algorithm redundant. We therefore use the simple Gauss-Newton implementation for fast inference:\n\u2206y = \u2212 J TJ + \u03b5I \u22121J TF , (20\n)\nwhere \u03b5 is a small value for numerical stability.", "publication_ref": ["b41", "b0", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "A.3. Covariance Estimation", "text": "During training, the concentration of the AMIS proposal is determined by the local estimation of pose covariance matrix \u03a3 y * , defined as:\n\u03a3 y * = J TJ + \u03b5I \u22121 y=y * , (21\n)\nwhere y * is the LM solution that determines the location of the proposal distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.4. Initialization", "text": "Since the LM solver only finds a local solution, initialization plays a determinant role in dealing with ambiguity. Standard EPnP [27] initialization can handle the dense correspondence network trained on the LineMOD [23] dataset, where ambiguity is not noticeable. For the deformable correspondence network trained on the nuScenes [8] dataset and more general cases, we implement a random sampling algorithm analogous to RANSAC, to search for the global optimum efficiently.\nGiven the N -point correspondence set\nX = x 3D i , x 2D i , w 2D i i = 1 \u2022 \u2022 \u2022 N , we generate M subsets consisting of n corresponding points each (3 \u2264 n < N ),\nby repeatedly sub-sampling n indices without replacement from a multinomial distribution, whose probability mass function p(i) is defined by the corresponding weights:\np(i) = w 2D i 1 N i=1 w 2D i 1 . (22\n)\nFrom each subset, a pose hypothesis can be solved via the LM algorithm with very few iterations (we use 3 iterations). This is implemented as a batch operation on GPU, and is rather efficient for small subsets. We take the hypothesis of maximum log-likelihood log p(X|y) as the initial point, starting from which subsequent LM iterations are computed on the full set X.\nTraining Mode During training, the LM PnP solver is utilized for estimating the location and concentration of the initial proposal distribution in the AMIS algorithm. The location is very important to the stability of Monte Carlo training. If the LM solver fails to find the global optimum and the location of the local optimum is far from the true pose y gt , the balance between the two opposite signed terms in Eq. ( 5) may be broken, leading to exploding gradient in the worst case scenario. To avoid such problem, we adopt a simple initialization trick: we compare the log-likelihood log p(X|y) of the ground truth y gt and the selected hypothesis, and then keep the one with higher likelihood as the initial state of the LM solver.", "publication_ref": ["b26", "b22", "b7"], "figure_ref": [], "table_ref": []}, {"heading": "B. Details on Monte Carlo Pose Sampling B.1. Proposal Distribution for Position", "text": "For the proposal distribution of the translation vector t \u2208 R 3 , we adopt the multivariate t-distribution, with the following probability density function (PDF):\nq T (t) = \u0393 \u03bd+3 2 \u0393 \u03bd 2 \u03bd 3 \u03c0 3 |\u03a3| 1 + 1 \u03bd t \u2212 \u00b5 2 \u03a3 \u2212 \u03bd+3 2 ,(23)\nwhere\nt \u2212 \u00b5 2 \u03a3 = (t \u2212 \u00b5) T \u03a3 \u22121 (t \u2212 \u00b5)\n, with the location \u00b5, the 3\u00d73 positive definite scale matrix \u03a3, and the degrees of freedom \u03bd. Following [14], we set \u03bd to 3. Compared to the multivariate normal distribution, the t-distribution has a heavier tail, which is ideal for robust sampling.\nThe multivariate t-distribution has been implemented in the Pyro [2] package.", "publication_ref": ["b13", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Initial Parameters", "text": "The initial location and scale is determined by the PnP solution and covariance matrix, i.e., \u00b5 \u2190 t * , \u03a3 \u2190 \u03a3 t * , where \u03a3 t * is the 3\u00d73 submatrix of the full pose covariance \u03a3 p * . Note that the actual covariance of the t-distribution is thus \u03bd \u03bd\u22121 \u03a3 t * , which is intentionally scaled up for robust sampling in a wider range.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Parameter Estimation from Weighted Samples", "text": "To update the proposal, we let the location \u00b5 and scale \u03a3 be the first and second moment of the weighted samples (i.e., weighted mean and covariance), respectively.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2. Proposal Distribution for 1D Orientation", "text": "For the proposal distribution of the 1D yaw-only orientation \u03b8, we adopt a mixture of von Mises and uniform distribution. The von Mises is also known as the circular normal distribution, and its PDF is given by:\nq VM (\u03b8) = exp (\u03ba cos (\u03b8 \u2212 \u00b5)) 2\u03c0I 0 (\u03ba) ,(24)\nwhere \u00b5 is the location parameter, \u03ba is the concentration parameter, and I 0 (\u2022) is the modified Bessel function with order zero. The mixture PDF is thus:\nq mix (\u03b8) = (1 \u2212 \u03b1)q VM (\u03b8) + \u03b1q uniform (\u03b8),(25)\nwith the uniform mixture weight \u03b1. The uniform component is added in order to capture other potential modes under orientation ambiguity. We set \u03b1 to a fixed value of 1/4. PyTorch has already implemented the von Mises distribution, but its random sample generation is rather slow. As an alternative we use the NumPy implementation for random sampling.\nInitial Parameters With the yaw angle \u03b8 * and its variance \u03c3 2 \u03b8 * from the PnP solver, the parameters of the von Mises proposal is initialized by\n\u00b5 \u2190 \u03b8 * , \u03ba \u2190 1 3\u03c3 2 \u03b8 * .\nParameter Estimation from Weighted Samples For the location \u00b5, we simply adopt its maximum likelihood estimation, i.e., the circular mean of the weighted samples. For the concentration \u03ba, we first compute an approximated estimation [16] \nby:\u03ba =r (2 \u2212r 2 ) 1 \u2212r 2 ,(26)\nwherer = j v j [sin \u03b8 j , cos \u03b8 j ] T / j v j is the norm of the mean orientation vector, with the importance weight v j for the j-th sample \u03b8 j . Finally, the concentration is scaled down for robust sampling, such that \u03ba \u2190\u03ba/3.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "B.3. Proposal Distribution for 3D Orientation", "text": "Regarding the quaternion based parameterization of 3D orientation, which can be represented by a unit 4D vector l, we adopt the angular central Gaussian (ACG) distribution as the proposal. The support of the 4-dimensional ACG distribution is the unit hypersphere, and the PDF is given by:\nq ACG (l) = (l T \u039b \u22121 l) \u22122 S 4 |\u039b| 1 2 ,(27)\nwhere S 4 = 2\u03c0 2 is the 3D surface area of the 4D sphere, and \u039b is a 4\u00d74 positive definite matrix.\nThe ACG density can be derived by integrating the zeromean multivariate normal distribution N (0, \u039b) along the radial direction from 0 to inf. Therefore, drawing samples from the ACG distribution is equivalent to sampling from N (0, \u039b) and then normalizing the samples to unit radius. Initial Parameters Consider l * to be the PnP solution and \u03a3 \u22121 l * to be the estimated 4\u00d74 inverse covariance matrix. Note that \u03a3 \u22121 l * is only valid in the local tangent space with rank 3, satisfying l * T \u03a3 \u22121 l * l * = 0. The initial parameters are determined by:\n\u039b \u2190\u039b + \u03b1|\u039b| 1 4 I,(28)\nwhere\u039b = \u03a3 \u22121 l * + I \u22121 , and \u03b1 is a hyperparameter that controls the dispersion of the proposal for robust sampling. We set \u03b1 to 0.001 in the experiments.\nParameter Estimation from Weighted Samples Based on the samples l j and weights v j , the maximum likelihood estimation\u039b is the solution to the following equation:\n\u039b = 4 j v j j v j l j l T j l T j\u039b \u22121 l j .(29)\nThe solution to Eq. ( 29) can be computed by fixed-point iteration [43]. The final parameters of the updated proposal is determined the same way as in Eq. (28).", "publication_ref": ["b42"], "figure_ref": [], "table_ref": []}, {"heading": "C. Details on Derivative Regularization Loss", "text": "As stated in the main paper, the derivative regularization loss L reg consists of the position loss L pos and the orientation loss L orient .\nFor L pos , we adopt the smooth L1 loss based on the Euclidean distance d t = t * + \u2206t \u2212 t gt , given by:\nL pos = \uf8f1 \uf8f2 \uf8f3 d 2 t 2\u03b2 , d t \u2264 \u03b2, d t \u2212 0.5\u03b2, d t > \u03b2,(30)\nwith the hyperparameter \u03b2.\nFor L orient , we adopt the cosine similarity loss based on the angular distance d \u03b8 . For 1D orientation parameterized by the angle \u03b8, d \u03b8 = \u03b8 * + \u2206\u03b8 \u2212 \u03b8 gt . For 3D orientation parameterized by the quaternion vector l, d \u03b8 = 2 arccos (l * + \u2206l) T l gt . The loss function is therefore defined as:\nL orient = 1 \u2212 cos d \u03b8 .(31)\nFor 3D orientation, after the substitution, the loss function can be simplified to:\nL orient = 2 \u2212 2 (l * + \u2206l) T l gt 2 .(32)\nFor the specific settings of the hyperparameter \u03b2 and loss weights, please refer to the experiment configuration code.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D. Details on the Deformable Correspondence Network D.1. Network Architecture", "text": "The detailed network architecture of the deformable correspondence network is shown in Figure 8. Following deformable DETR [54], this paper adopts the multi-head deformable sampling. Let n head be the number of heads and n hpts be the number of points per head, a total number of N = n head n hpts points are sampled for each object. The sampling locations relative to the reference point are generated from the object embedding by a single layer of linear transformation. We set n head to 8, which yields 256/n head = 32 channels for the point features. The point-level branch on the left side of Figure 8 is responsible for predicting the 3D points x 3D i and corresponding weights w 2D\ni . The sampled point features are first enhanced by the object-level context, by adding the reshaped head-wise object embedding to the point features. Then, the features of the N points are processed by the self attention layer, for which the 2D points are transformed into positional encoding. The attention layer is followed by standard layers of normalization, skip connection, and feedforward network (FFN).\nRegarding the object-level branch on the right side of Figure 8, a multi-head attention layer is employed to aggregate the sampled point features. Unlike the original deformable attention layer [54] that predicts the attention weights by linear projection of the object embedding, we adopt the full Q-K dot-product attention with positional encoding. After being processed by the subsequent layers, the object-level features are finally transformed into to the object-level predictions, consisting of the 3D localization score, weight scale, 3D bounding box size, and other optional properties (velocity and attribute). Note that the attention layer is actually not a necessary component for object-level predictions, but rather a byproduct of the deformable point samples whose features can be leveraged with little computation overhead.", "publication_ref": ["b53", "b53"], "figure_ref": [], "table_ref": []}, {"heading": "D.2. Loss Functions for Object-Level Predictions", "text": "As in FCOS3D [47], we adopt smooth L1 regression loss for 3D box size and velocity, and cross-entropy classification loss for attribute. Additionally, a binary cross-entropy loss is imposed upon the 3D localization score, with the target c tgt defined as a function of the position error:\nc tgt = Score( t * XZ \u2212 t XZgt ) = max(0, min(1, \u2212a log t * XZ \u2212 t XZgt + b)), (33\n)\nwhere t * XZ is the XZ components of the PnP solution, t XZgt is the XZ components of the true pose, and a, b are the linear coefficients. The predicted 3D localization score c pred shall reflect the positional uncertainty of an object, as a faster alternative to evaluating the uncertainty via the Monte Carlo method during inference (Section F.2). The final detection score is defined as the product of the predicted 3D score and the classification score from the base detector.", "publication_ref": ["b46"], "figure_ref": [], "table_ref": []}, {"heading": "D.3. Auxiliary Loss Functions", "text": "To regularize the dense features, we append an auxiliary branch that predicts the multi-head dense 3D coordinates and corresponding weights, as shown in Figure 9. Leveraging the ground truth of object 2D boxes, the features within the box regions are densely sampled via RoI Align [20], and transformed into the 3D coordinates x 3D and weights w 2D via an independent linear layer. Besides, the attention weights \u03c6 are obtained via Q-K dot-product and normalized along the n head dimension and across the overlapping region of multiple RoIs via Softmax.\nDuring training, we impose the reprojection-based auxiliary loss for the multi-head dense predictions, formulated as the negative log-likelihood (NLL) of the Gaussian mixture model [3]. The loss function for each sampled point is defined as:\nL proj = \u2212 log RoI nhead k=1 \u03c6 k | diag w 2D k | exp \u2212 1 2 f k (y gt ) 2 ,(34)\nwhere k is the head index, f k (y gt ) is the weighted reprojection error of the k-th head at the truth pose y gt . In the above equation, the diagonal matrix diag w 2D k is interpreted as the inverse square root of the covariance matrix of the normal distribution, i.e., diag w 2D k = \u03a3 \u2212 1 2 , and the head attention weight \u03c6 k is interpreted as the mixture component weight.\nRoI is a special operation that takes the overlapping region of multiple RoIs into account, formulating a mixture of multiple heads and multiple RoIs (see code for details).\nAnother auxiliary loss is the coordinate regression loss that introduces the geometric knowledge. Following MonoRUn [13], we extract the sparse ground truth of 3D coordinates x 3D gt from the 3D LiDAR point cloud. The multihead coordinate regression loss for each sampled point with available ground truth is defined as:   where \u03c1(\u2022) is the Huber kernel. L regr is essentially a weighted smooth L1 loss (although we write the Huber kernel for convenience in notation).\nL regr = nhead k=1 \u03c6 k \u03c1 x 3D k \u2212 x 3D gt 2 , (35\n)\ndense", "publication_ref": ["b19", "b2", "b12"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "D.4. Training Strategy", "text": "During training, we randomly sample 48 positive object queries from the FCOS3D [47] detector for each image, which limits the batch size of the deformable correspondence network to control the computation overhead of the Monte Carlo pose loss.", "publication_ref": ["b46"], "figure_ref": [], "table_ref": []}, {"heading": "E. Additional Results of the Dense Correspondence Network E.1. Convergence Behavior", "text": "The convergence behaviors of EPro-PnP and CDPN [29] are compared in Figure 10. The original CDPN-Full is trained in 3 stages (rotation head -translation head -both together) with a total of 480 epochs. In contrast, EPro-PnP with derivative regularization clearly outperforms CDPN-Full within one stage, and goes further when initialized from the pretrained first-stage CDPN. ", "publication_ref": ["b28"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "E.2. Inference Time", "text": "Compared to the inference pipeline of CDPN-Full [29], EPro-PnP does not use the RANSAC algorithm or extra translation head, so the overall inference speed is more than twice as fast as CDPN-Full (at a batch size of 32), even though we introduces the iterative LM solver.\nRegarding the LM solver itself, inference takes 7.3 ms for a batch of one object, measured on RTX 2080 Ti GPU, excluding EPnP [27] initialization. As a reference, the stateof-the-art pose refiner RePOSE [24] (also based on the LM algorithm) adds 10.9 ms overhead to the base pose estimator PVNet [36] at the same batch size, measured on RTX 2080 Super GPU, which is slower than ours. Nevertheless, faster inference is possible if the number of points N = 64 \u00d7 64 is reduced to an optimal level.", "publication_ref": ["b28", "b26", "b23", "b35"], "figure_ref": [], "table_ref": []}, {"heading": "F. Additional Experiments on the Deformable", "text": "Correspondence Network", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "F.1. On the Auxiliary Reprojection Loss", "text": "As shown in Table 5, removing the auxiliary reprojection loss in Eq. 34 lowers the 3D object detection accuracy (NDS 0.408 vs. 0.425). Among the true positive metrics, the orientation metric mAOE is the most affected. The results indicate that, although the deformable correspondences can be learned solely with the end-to-end loss, it is still beneficial to add auxiliary task for further regularization, even if the task itself does not involve extra annotation.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "F.2. On the Uncertainty of Object Pose", "text": "The dispersion of the inferred pose distribution reflects the aleatoric uncertainty of the predicted pose. Previous work [13] reasons the pose uncertainty by propagating the reprojection uncertainty learned from a surrogate loss through the PnP operation, but that uncertainty requires calibration and is not reliable enough. In our work, the pose uncertainty is learned with the KL-divergence-based pose loss in an end-to-end manner, which is much more reliable in theory.\nTo quantitatively evaluate the reliability of the pose un-certainty in terms of measuring the localization confidence, a straightforward approach is to compute the 3D localization score c MC via Monte Carlo pose sampling, and compare the resulting mAP against the standard implementation with 3D score c pred predicted from the object-level branch. With the PnP solution t * , the sampled translation vector t j , and its importance weight v j , the Monte Carlo score is computed by:\nc MC = 1 j v j j v j Score t * XZ \u2212 t XZj ,(36)\nwhere the subscript (\u2022) XZ denotes taking the XZ components, and the function Score(\u2022) is the same as in Eq. 33. Furthermore, the final score can also be a mixture of the two sources, defined as:\nc mix = c \u03b1 MC c 1\u2212\u03b1 pred , (37\n)\nwhere \u03b1 is the mixture weight.\nThe evaluation results under different mixture weights are presented in Table 5. Regarding the mAP metric, the Monte Carlo score is on par with the standard implementation (0.350 vs. 0.350 vs. 0.349), indicating that the pose uncertainty is a reliable measure of the detection confidence. Nevertheless, due to the much longer runtime of inferring with Monte Carlo pose sampling, training a standard score branch is still a more practical choice.", "publication_ref": ["b12"], "figure_ref": [], "table_ref": ["tab_9"]}, {"heading": "F.3. On the Network Redundancy and Potential for Future Improvement", "text": "Since the main concern of this paper is to propose a novel differentiable PnP layer, we did not have enough time and resources to fine-tune the architecture and parameters of the deformable correspondence network at the time of submitting the manuscript. Therefore, the network described in Sections 4.2 and D.1 was crafted with some redundancy in mind, being not very efficient in terms of FLOP count, memory footprint and inference time, leaving large potential for improvement.\nTo demonstrate the potential for improvement, we train a more compact network with lower resolution (stride=8) for the dense feature map, and the number of points per head n hpts reduced from 32 to 16, and squeeze the batch of 12 images into 2 RTX 3090 GPUs. As shown in Table 5, the overall performance is actually slightly better than the original version (NDS 0.434 vs. 0.430). Still, a more efficient architecture is yet to be determined in future work.\nInference Time Regarding the compact network, the average inference time per frame (comprising a batch of 6 surrounding 1600\u00d7672 5 images, without TTA) is shown in Table 6, measured on RTX 3090 GPU and Core i9-10920X CPU. On average, the batch PnP solver processes 625.97 objects per frame before non-maximum suppression (NMS).  ", "publication_ref": ["b4"], "figure_ref": [], "table_ref": ["tab_9", "tab_11"]}, {"heading": "G. Limitation", "text": "EPro-PnP is a versatile pose estimator for general problems, yet it has to be acknowledged that training the network with the Monte Carlo pose loss is inevitably slower than the baseline. At the batch size of 32, training the CDPN (without translation head) takes 143 seconds per epoch with the original coordinate regression loss, and 241 seconds per epoch with the Monte Carlo pose loss, which is about 70% longer time, as measured on GTX 1080 Ti GPU. However, the training time can be controlled by adjusting the number of Monte Carlo samples or the number of 2D-3D corresponding points. In this paper, the choice of these hyperparameters generally leans towards redundancy. 5 The original size is 1600\u00d7900. We crop the images for efficiency.  ", "publication_ref": ["b4"], "figure_ref": [], "table_ref": []}, {"heading": "I. Notation", "text": "Notation Description\nx 3D i \u2208 R 3\nCoordinate vector of the i-th 3D object point\nx 2D i \u2208 R 2\nCoordinate vector of the i-th 2D image point Huber kernel function \u03c1 i\nw 2D i \u2208 R 2 + Weight\nThe derivative of the Huber kernel function of the i-th correspondence \u03b4\nThe Huber threshold p(X|y)\nLikelihood function of object pose p(y)\nPDF of the prior pose distribution p(y|X)\nPDF of the posterior pose distribution t(y)\nPDF of the target pose distribution q(y), q t (y)\nPDF of the proposal pose distribution (of the t-th AMIS iteration) y j , y t", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "j", "text": "The j-th random pose sample (of the t-th AMIS iteration) v j , v t j Importance weight of the j-th pose sample (of the t-th AMIS iteration) ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": "Sameer Agarwal; Keir Mierle; Others Ceres"}, {"ref_id": "b1", "title": "Pyro: Deep Universal Probabilistic Programming", "journal": "Journal of Machine Learning Research", "year": "2018", "authors": "Eli Bingham; Jonathan P Chen; Martin Jankowiak; Fritz Obermeyer; Neeraj Pradhan; Theofanis Karaletsos; Rohit Singh; Paul Szerlip; Paul Horsfall; Noah D Goodman"}, {"ref_id": "b2", "title": "Mixture density networks", "journal": "", "year": "1994", "authors": "Christopher M Bishop"}, {"ref_id": "b3", "title": "Dsac -differentiable ransac for camera localization", "journal": "", "year": "2017", "authors": "Eric Brachmann; Alexander Krull; Sebastian Nowozin; Jamie Shotton; Frank Michel; Stefan Gumhold; Carsten Rother"}, {"ref_id": "b4", "title": "Uncertainty-driven 6d pose estimation of objects and scenes from a single rgb image", "journal": "", "year": "2016", "authors": "Eric Brachmann; Frank Michel; Alexander Krull; Michael Ying Yang; Stefan Gumhold; Carsten Rother"}, {"ref_id": "b5", "title": "Learning less is more -6d camera localization via 3d surface regression", "journal": "", "year": "2008", "authors": "Eric Brachmann; Carsten Rother"}, {"ref_id": "b6", "title": "Slobodan Ilic, and Nassir Navab. 6d camera relocalization in ambiguous scenes via continuous multimodal inference", "journal": "", "year": "2008", "authors": "Mai Bui; Tolga Birdal; Haowen Deng; Shadi Albarqouni; Leonidas Guibas"}, {"ref_id": "b7", "title": "nuscenes: A multimodal dataset for autonomous driving", "journal": "", "year": "2011", "authors": "Holger Caesar; Varun Bankiti; Alex H Lang; Sourabh Vora; Venice Erin Liong; Qiang Xu; Anush Krishnan; Yu Pan; Giancarlo Baldan; Oscar Beijbom"}, {"ref_id": "b8", "title": "Solving the blind perspective-n-point problem end-to-end with robust differentiable geometric optimization", "journal": "", "year": "2008", "authors": "Dylan Campbell; Liu Liu; Stephen Gould"}, {"ref_id": "b9", "title": "End-toend object detection with transformers", "journal": "", "year": "", "authors": "Nicolas Carion; Francisco Massa; Gabriel Synnaeve; Nicolas Usunier; Alexander Kirillov; Sergey Zagoruyko"}, {"ref_id": "b10", "title": "Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image", "journal": "", "year": "2017", "authors": "Florian Chabot; Mohamed Chaouch; Jaonary Rabarisoa; C\u00e9line Teuli\u00e8re; Thierry Chateau"}, {"ref_id": "b11", "title": "End-to-end learnable geometric vision by backpropagating pnp optimization", "journal": "", "year": "2008", "authors": "Bo Chen; Alvaro Parra; Jiewei Cao; Nan Li; Tat-Jun Chin"}, {"ref_id": "b12", "title": "Monorun: Monocular 3d object detection by reconstruction and uncertainty propagation", "journal": "", "year": "2021", "authors": "Hansheng Chen; Yuyao Huang; Wei Tian; Zhong Gao; Lu Xiong"}, {"ref_id": "b13", "title": "Adaptive multiple importance sampling", "journal": "Scandinavian Journal of Statistics", "year": "2012", "authors": "Jean-Marie Cornuet; Jean-Michel Marin; Antonietta Mira; Christian P Robert"}, {"ref_id": "b14", "title": "Deformable convolutional networks", "journal": "", "year": "2017", "authors": "Jifeng Dai; Haozhi Qi; Yuwen Xiong; Yi Li; Guodong Zhang; Han Hu; Yichen Wei"}, {"ref_id": "b15", "title": "Modeling data using directional distributions", "journal": "", "year": "2003", "authors": "S Inderjit; Suvrit Dhillon;  Sra"}, {"ref_id": "b16", "title": "Are we ready for autonomous driving? the kitti vision benchmark suite", "journal": "", "year": "2012", "authors": "Andreas Geiger; Philip Lenz; Raquel Urtasun"}, {"ref_id": "b17", "title": "Deep orientation uncertainty learning based on a bingham loss", "journal": "", "year": "", "authors": "Igor Gilitschenski; Roshni Sahoo; Wilko Schwarting; Alexander Amini; Sertac Karaman; Daniela Rus"}, {"ref_id": "b18", "title": "Deep declarative networks", "journal": "IEEE TPAMI", "year": "2008", "authors": "Stephen Gould; Richard Hartley; Dylan John Campbell"}, {"ref_id": "b19", "title": "Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn", "journal": "", "year": "2017", "authors": "Kaiming He; Georgia Gkioxari"}, {"ref_id": "b20", "title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"ref_id": "b21", "title": "Bounding box regression with uncertainty for accurate object detection", "journal": "", "year": "2019", "authors": "Yihui He; Chenchen Zhu; Jianren Wang; Marios Savvides; Xiangyu Zhang"}, {"ref_id": "b22", "title": "Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes", "journal": "", "year": "2006", "authors": "Stefan Hinterstoisser; Stefan Holzer; Cedric Cagniart; Slobodan Ilic; Kurt Konolige; Nassir Navab; Vincent Lepetit"}, {"ref_id": "b23", "title": "Repose: Fast 6d object pose refinement via deep texture rendering", "journal": "", "year": "2007", "authors": "Xingyu Shun Iwase; Rawal Liu; Rio Khirodkar; Kris M Yokota;  Kitani"}, {"ref_id": "b24", "title": "What uncertainties do we need in bayesian deep learning for computer vision", "journal": "", "year": "2017", "authors": "Alex Kendall; Yarin Gal"}, {"ref_id": "b25", "title": "Auto-encoding variational bayes", "journal": "", "year": "2014", "authors": "P Diederik; Max Kingma;  Welling"}, {"ref_id": "b26", "title": "Epnp: An accurate o(n) solution to the pnp problem", "journal": "International Journal Of Computer Vision", "year": "2009", "authors": "Vincent Lepetit; Francesc Moreno-Noguer; Pascal Fua"}, {"ref_id": "b27", "title": "Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving", "journal": "", "year": "2020", "authors": "Peixuan Li; Huaici Zhao; Pengfei Liu; Feidao Cao"}, {"ref_id": "b28", "title": "Cdpn: Coordinates-based disentangled pose network for real-time rgb-based 6-dof object pose estimation", "journal": "", "year": "2019", "authors": "Zhigang Li; Gu Wang; Xiangyang Ji"}, {"ref_id": "b29", "title": "Decoupled weight decay regularization", "journal": "", "year": "2019", "authors": "Ilya Loshchilov; Frank Hutter"}, {"ref_id": "b30", "title": "Overcoming limitations of mixture density networks: A sampling and fitting framework for multimodal future prediction", "journal": "", "year": "2019", "authors": "Osama Makansi; Eddy Ilg; Ozgun Cicek; Thomas Brox"}, {"ref_id": "b31", "title": "Explaining the ambiguity of object detection and 6d pose from visual data", "journal": "", "year": "2019", "authors": "Fabian Manhardt; Diego Mart\u00edn Arroyo; Christian Rupprecht; Benjamin Busam; Nassir Navab; Federico Tombari"}, {"ref_id": "b32", "title": "3d bounding box estimation using deep learning and geometry", "journal": "", "year": "2017", "authors": "Arsalan Mousavian; Dragomir Anguelov; John Flynn; Jana Kosecka"}, {"ref_id": "b33", "title": "Is pseudo-lidar needed for monocular 3d object detection", "journal": "", "year": "", "authors": "Dennis Park; Rares Ambrus; Vitor Guizilini; Jie Li; Adrien Gaidon"}, {"ref_id": "b34", "title": "Pix2pose: Pixel-wise coordinate regression of objects for 6d pose estimation", "journal": "", "year": "2019", "authors": "Kiru Park; Timothy Patten; Markus Vincze"}, {"ref_id": "b35", "title": "Pvnet: Pixel-wise voting network for 6dof pose estimation", "journal": "", "year": "2007", "authors": "Sida Peng; Yuan Liu; Qixing Huang; Xiaowei Zhou; Hujun Bao"}, {"ref_id": "b36", "title": "BB8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth", "journal": "", "year": "2017", "authors": "Mahdi Rad; Vincent Lepetit"}, {"ref_id": "b37", "title": "Robust pose estimation from a planar target", "journal": "IEEE TPAMI", "year": "2006", "authors": "Gerald Schweighofer; Axel Pinz"}, {"ref_id": "b38", "title": "Disentangling monocular 3d object detection", "journal": "", "year": "2019", "authors": "Andrea Simonelli; Samuel Rota Bul\u00f2; Lorenzo Porzi; Manuel L\u00f3pez-Antequera; Peter Kontschieder"}, {"ref_id": "b39", "title": "Hybridpose: 6d object pose estimation under hybrid representations", "journal": "", "year": "2007", "authors": "Chen Song; Jiaru Song; Qixing Huang"}, {"ref_id": "b40", "title": "Fcos: Fully convolutional one-stage object detection", "journal": "", "year": "2019", "authors": "Zhi Tian; Chunhua Shen; Hao Chen; Tong He"}, {"ref_id": "b41", "title": "Bundle adjustment: A modern synthesis", "journal": "", "year": "2000", "authors": "Bill Triggs; Philip F Mclauchlan; Richard I Hartley; Andrew W Fitzgibbon"}, {"ref_id": "b42", "title": "Statistical analysis for the angular central gaussian distribution on the sphere", "journal": "Biometrika", "year": "1987", "authors": "David E Tyler"}, {"ref_id": "b43", "title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"ref_id": "b44", "title": "Gdr-net: Geometry-guided direct regression network for monocular 6d object pose estimation", "journal": "", "year": "2021", "authors": "Gu Wang; Fabian Manhardt; Federico Tombari; Xiangyang Ji"}, {"ref_id": "b45", "title": "Normalized object coordinate space for category-level 6d object pose and size estimation", "journal": "", "year": "2002", "authors": "He Wang; Srinath Sridhar; Jingwei Huang; Julien Valentin; Shuran Song; Leonidas J Guibas"}, {"ref_id": "b46", "title": "FCOS3D: Fully convolutional one-stage monocular 3d object detection", "journal": "", "year": "2021", "authors": "Tai Wang; Xinge Zhu; Jiangmiao Pang; Dahua Lin"}, {"ref_id": "b47", "title": "Probabilistic and geometric depth: Detecting objects in perspective", "journal": "", "year": "2008", "authors": "Tai Wang; Xinge Zhu; Jiangmiao Pang; Dahua Lin"}, {"ref_id": "b48", "title": "Abhinav Gupta, and Kaiming He. Non-local neural networks", "journal": "", "year": "2018", "authors": "Xiaolong Wang; Ross Girshick"}, {"ref_id": "b49", "title": "Detr3d: 3d object detection from multi-view images via 3d-to-2d queries", "journal": "", "year": "", "authors": "Yue Wang; Vitor Guizilini; Tianyuan Zhang; Yilun Wang; Hang Zhao; Justin Solomon"}, {"ref_id": "b50", "title": "Unsupervised learning of probably symmetric deformable 3d objects from images in the wild", "journal": "", "year": "", "authors": "Shangzhe Wu; Christian Rupprecht; Andrea Vedaldi"}, {"ref_id": "b51", "title": "Dpod: 6d pose object detector and refiner", "journal": "", "year": "2007", "authors": "Sergey Zakharov; Ivan Shugurov; Slobodan Ilic"}, {"ref_id": "b52", "title": "Objects as points", "journal": "", "year": "2019", "authors": "Xingyi Zhou; Dequan Wang; Philipp Kr\u00e4henb\u00fchl"}, {"ref_id": "b53", "title": "Deformable detr: Deformable transformers for end-to-end object detection", "journal": "", "year": "", "authors": "Xizhou Zhu; Weijie Su; Lewei Lu; Bin Li; Xiaogang Wang; Jifeng Dai"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 .1Figure 1. EPro-PnP is a general solution to end-to-end 2D-3D correspondence learning. In this paper, we present two distinct networks trained with EPro-PnP: (a) an off-the-shelf dense correspondence network whose potential is unleashed by end-to-end training, (b) a novel deformable correspondence network that explores new possibilities of fully learnable 2D-3D points.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 .3Figure 3. The learned corresponding weight can be factorized into inverse uncertainty and discrimination. Typically, inverse uncertainty roughly resembles the foreground mask, while discrimination emphasizes the 3D extremities of the object.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 .4Figure 4. The 6DoF pose estimation network modified from CDPN [29]. with spatial Softmax and global weight scaling.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "provides a large scale of data collected in 1000 scenes. Each scene contains 40 keyframes, annotated with a total of 1.4M 3D bounding boxes from 10 categories. Each keyframe includes 6 RGB images collected from surrounding cameras. The data is split into 700/150/150 scenes for training/validation/testing. The official benchmark evaluates the average precision with true positives judged by 2D center error on the ground plane. The mAP metric is computed by averaging over the thresholds of 0.5, 1, 2, 4 meters. Besides, there are 5 true positive metrics: Average Translation Error (ATE), Average Scale Error (ASE), Average Orientation Error (AOE), Average Velocity Error (AVE) and Average Attribute Error (AAE).", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 .6Figure 6. Visualization of the predicted pose distribution. The orientation density is clearly multimodal, capturing the pose ambiguity of symmetric objects (Barrier, Cone) and uncertain observations (Pedestrian).", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 7 .7Figure 7. Visualization of the inferred weight and coordinate maps on LineMOD test data.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 9 .9Figure 9. Architecture of the auxiliary branch. This branch shares the same weights of Q, K projection with the deformable attention layer in the lower right of Figure 8.", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Figure 10 .10Figure 10. Testing accuracy vs. training progress on LineMOD.", "figure_data": ""}, {"figure_label": "111213", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 11 .Figure 12 .Figure 13 .111213Figure 11. Inferred results on LineMOD test set by EPro-PnP with derivative regularization and pretrained CDPN weights, Part I.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 14 .14Figure 14. Inferred results on nuScenes validation set by the Basic EPro-PnP.", "figure_data": ""}, {"figure_label": "32", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "R 3 232vector of the i-th 2D-3D point pair X The set of weighted 2D-3D correspondences y Object pose y gt Ground truth of object pose y * Object pose estimated by the PnP solver R 3\u00d73 rotation matrix representation of object orientation \u03b8 1D yaw angle representation of object orientation l Unit quaternion representation of object orientation t \u2208 Translation vector representation of object position \u03a3 y * Pose covariance estimated by the PnP solver J Jacobian matrix J Rescaled Jacobian matrix F Concatenated vector of weighted reprojection errors of all points F Concatenated vector of rescaled weighted reprojection errors of all points \u03c0(\u2022) : R 3 \u2192 R 2 Camera projection function f i (y) \u2208 R 2 Weighted reprojection error of the i-th correspondence at pose y r i (y) \u2208 R Unweighted reprojection error of the i-th correspondence at pose y \u03c1(\u2022)", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "The original CDPN heavily relies on direct position regression, and the performance drops greatly (-17.46) when reduced to a pure PnP estimator, although the LM solver partially recovers the mean metric (+6.29). Strong improvement (+5.46) is seen when initialized from A1, because CDPN has been trained with the extra ground truth of object masks, providing a good initial state highlighting the foreground.\u2022 Finally, the performance benefits (+0.97) from more training epochs (160 ep. from A1 + 320 ep.) as equivalent to CDPN-Full[29] (3 stages \u00d7 160 ep.).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Full [29] 29.10 69.50 91.03 63.21 A1 CDPN w/o trans. head 15.93 46.79 74.54 45.75 (\u221217.46) A2 + Batch=32, LM solver 21.17 55.00 79.96 52.04 (+ 6.29) B0 Basic EPro-PnP 32.14 72.83 92.66 65.88 (+13.84) B1 + Regularize derivatives 35.44 74.41 93.43 67.76 (+ 1.88) B2 + Initialize from A1 42.92 80.98 95.76 73.22 (+ 5.46) B3 + Long sched. (320 ep.) 44.81 81.96 95.80 74.19 (+ 0.97) Comparison to the CDPN baseline with Ablation Studies. Results of CDPN are reproduced with the official code.4 In C0/C1 either component is detached individually from the KL loss, while adding a surrogate mask regression loss[29] in C1.", "figure_data": "ADD(-S)ID Method0.02d 0.05d 0.1dMeanA0 CDPN-C0 B0 \u2192 Detach coords.29.57 68.61 90.23 62.80 (\u2212 3.08)C1 B0 \u2192 Detach weights22.99 61.31 87.27 57.19 (\u2212 8.69)D0 B0 \u2192 No Softmax denom.divergenceADD(-S)Method2\u00b0, 2 cm 5\u00b0, 5 cm0.02d 0.05d 0.1dCDPN [29]-94.31--89.86HybridPose [40]----91.3GDRNet* [45]67.1-35.676.0 93.6DPOD [52]----95.15PVNet-RePOSE [24]----96.1EPro-PnP80.9998.5444.81 81.96 95.80Main LossCoord. Regr.2\u00b02 cm 2\u00b0, 2 cmADD(-S) 0.1dImplicit diff. [12]divergenceReprojection [13]0.32 42.300.1614.56Monte Carlo (ours)44.18 81.5540.9679.46Implicit diff. [12]56.13 91.1353.3388.74Reprojection [13]62.79 92.9160.6592.04Monte Carlo (ours)65.75 93.9063.8092.66"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "3D object detection results on the nuScenes benchmark. Methods with extra pretraining other than ImageNet backbone are not included for comparison.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Additional results of the deformable correspondence network tested on the nuScenes[8] benchmark.", "figure_data": "True positive metrics (lower is better)ID MethodData NDS mAPmATE mASE mAOE mAVE mAAEA0 Basic EPro-PnPVal 0.425 0.349 0.676 0.263 0.363 1.0350.196A1 A0 + coord. regr.Val 0.430 0.352 0.667 0.258 0.337 1.0310.193B0 A0 \u2192 No reprojection L projVal 0.408 0.337 0.721 0.267 0.452 1.1130.166C0 A0 \u2192 50% Monte Carlo scoreVal 0.424 0.350 0.673 0.264 0.373 1.0420.198C1 A0 \u2192 100% Monte Carlo score Val 0.424 0.350 0.675 0.264 0.367 1.0480.199D0 A1 \u2192 Compact networkVal 0.434 0.358 0.672 0.264 0.351 0.9830.181D1 D0 + TTAVal 0.446 0.367 0.664 0.260 0.320 0.9510.179"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Inference time (sec) of the deformable correspondence network on nuScenes object detection dataset[8]. The PnP solver (including the random sampling initialization in Section A.4) works faster (26 ms) with PyTorch v1.8.1, for which the code was originally developed, while the full model works faster (301 ms) with PyTorch v1.10.1.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "X = x 3D i , x 2D i , w 2D i i = 1 \u2022 \u2022 \u2022 N of N corresponding points, with 3D object coordinates x 3D i \u2208 R 3 , 2D image coordinates x 2D i \u2208 R 2 , and 2D weights w 2D i \u2208 R 2 + , from", "formula_coordinates": [2.0, 308.86, 678.94, 236.25, 36.01]}, {"formula_id": "formula_1", "formula_text": "arg min y 1 2 N i=1 w 2D i \u2022 \u03c0(Rx 3D i + t) \u2212 x 2D i fi(y)\u2208R 2 2 , (1", "formula_coordinates": [3.0, 73.52, 146.9, 208.97, 35.29]}, {"formula_id": "formula_2", "formula_text": ")", "formula_coordinates": [3.0, 282.49, 157.64, 3.87, 8.64]}, {"formula_id": "formula_3", "formula_text": "p(X|y) = exp \u2212 1 2 N i=1 f i (y) 2 .(2)", "formula_coordinates": [3.0, 103.47, 336.38, 182.89, 30.32]}, {"formula_id": "formula_4", "formula_text": "p(y|X) = exp \u2212 1 2 N i=1 f i (y) 2 exp \u2212 1 2 N i=1 f i (y) 2 dy .(3)", "formula_coordinates": [3.0, 87.65, 422.79, 198.71, 30.56]}, {"formula_id": "formula_5", "formula_text": "L KL = \u2212 t(y) log p(X|y) dy + log p(X|y) dy. (4)", "formula_coordinates": [3.0, 57.8, 579.61, 228.56, 9.81]}, {"formula_id": "formula_6", "formula_text": "L KL = 1 2 N i=1 f i (y gt ) 2 Ltgt (reproj. at target pose) + log exp \u2212 1 2 N i=1 f i (y) 2 dy", "formula_coordinates": [3.0, 52.96, 641.84, 227.44, 44.24]}, {"formula_id": "formula_7", "formula_text": "integrand exp \u2212 1 2 N i=1 f i (y)", "formula_coordinates": [3.0, 308.86, 595.82, 122.37, 14.56]}, {"formula_id": "formula_8", "formula_text": "L pred \u2248 log 1 K K j=1 exp \u2212 1 2 N i=1 f i (y j ) 2 q(y j ) vj (importance weight) ,(6)", "formula_coordinates": [3.0, 338.01, 641.57, 207.1, 41.17]}, {"formula_id": "formula_9", "formula_text": "\u2202L KL \u2202(\u2022) = \u2202 \u2202(\u2022) 1 2 N i=1 f i (y gt ) 2 \u2212 E y\u223cp(y|X) \u2202 \u2202(\u2022) 1 2 N i=1 f i (y) 2 ,(7)", "formula_coordinates": [4.0, 51.31, 344.97, 237.0, 35.49]}, {"formula_id": "formula_10", "formula_text": "\u2212 \u2202L KL \u2202w 2D i = w 2D i \u2022 \u2212r \u20222 i (y gt ) + E y\u223cp(y|X) r \u20222 i (y) ,(8)", "formula_coordinates": [4.0, 67.42, 477.77, 218.94, 24.76]}, {"formula_id": "formula_11", "formula_text": "Input : X = {x 3D i , x 2D i , w 2D i } Output: L pred 1 y * , \u03a3 y * \u2190 PnP (X) // Laplace approximation 2 Fit q 1 (y) to y * , \u03a3 y * // initial proposal 3 for 1 \u2264 t \u2264 T do 4 Generate K samples y t j=1\u2022\u2022\u2022K from q t (y) 5 for 1 \u2264 j \u2264 K do 6 P t j \u2190 p(X|y t j ) // evaluate integrand 7 for 1 \u2264 \u03c4 \u2264 t and 1 \u2264 j \u2264 K do 8 Q \u03c4 j \u2190 1 t t m=1 q m (y \u03c4 j )", "formula_coordinates": [4.0, 310.85, 91.8, 229.28, 127.17]}, {"formula_id": "formula_12", "formula_text": "{y \u03c4 j , v \u03c4 j | 1 \u2264 \u03c4 \u2264 t, 1 \u2264 j \u2264 K } 12 L pred \u2190 log 1 T K T t=1 K j=1 v", "formula_coordinates": [4.0, 305.87, 259.18, 185.55, 35.56]}, {"formula_id": "formula_13", "formula_text": "\u2206y = \u2212(J T J + \u03b5I) \u22121 J T F (y * ),(9)", "formula_coordinates": [4.0, 360.94, 607.31, 184.18, 11.03]}, {"formula_id": "formula_14", "formula_text": "F (y * ) = f T 1 (y * ), f T 2 (y * ), \u2022 \u2022 \u2022 , f T N (y * )", "formula_coordinates": [4.0, 335.42, 629.7, 157.59, 12.47]}, {"formula_id": "formula_15", "formula_text": "L reg = l(y * + \u2206y, y gt ),(10)", "formula_coordinates": [4.0, 380.07, 702.12, 165.04, 11.88]}, {"formula_id": "formula_16", "formula_text": "arg min y 1 2 N i=1 \u03c1 f i (y) 2 . (11", "formula_coordinates": [6.0, 111.84, 686.01, 170.37, 30.32]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [6.0, 282.21, 696.74, 4.15, 8.64]}, {"formula_id": "formula_18", "formula_text": "\u03c1(s) = s, s \u2264 \u03b4 2 , \u03b4(2 \u221a s \u2212 \u03b4), s > \u03b4 2 . (12", "formula_coordinates": [6.0, 361.84, 236.89, 179.12, 26.99]}, {"formula_id": "formula_19", "formula_text": ")", "formula_coordinates": [6.0, 540.96, 247.35, 4.15, 8.64]}, {"formula_id": "formula_20", "formula_text": "\u2022", "formula_coordinates": [6.0, 320.32, 608.87, 3.49, 8.64]}, {"formula_id": "formula_21", "formula_text": "\u03b4 = \u03b4 rel w 2D 1 2 1 N \u2212 1 N i=1 x 2D i \u2212x 2D 2 1 2 ,(13)", "formula_coordinates": [11.0, 64.55, 246.16, 221.81, 34.92]}, {"formula_id": "formula_22", "formula_text": "2D = 1 N N i=1 w 2D i ,x 2D = 1 N N i=1 x 2D i .", "formula_coordinates": [11.0, 112.7, 299.64, 163.4, 14.56]}, {"formula_id": "formula_23", "formula_text": "f i (y) = \u03c1 i f i (y),(14)", "formula_coordinates": [11.0, 129.81, 481.44, 273.15, 10.93]}, {"formula_id": "formula_24", "formula_text": "J i (y) = \u03c1 i J i (y),(15)", "formula_coordinates": [11.0, 129.16, 503.61, 157.2, 10.93]}, {"formula_id": "formula_25", "formula_text": "\u03c1 i = \uf8f1 \uf8f2 \uf8f3 1, f i (y) \u2264 \u03b4, \u03b4 f i (y) , f i (y) > \u03b4,(16)", "formula_coordinates": [11.0, 103.48, 529.93, 182.89, 38.15]}, {"formula_id": "formula_26", "formula_text": "J i (y) = \u2202f i (y) \u2202y T . (17", "formula_coordinates": [11.0, 134.59, 574.67, 147.63, 22.31]}, {"formula_id": "formula_27", "formula_text": ")", "formula_coordinates": [11.0, 282.21, 581.73, 4.15, 8.64]}, {"formula_id": "formula_28", "formula_text": "\u2206y = \u2212 J TJ + \u03bbD 2 \u22121J TF ,(18)", "formula_coordinates": [11.0, 104.4, 633.73, 181.96, 15.91]}, {"formula_id": "formula_29", "formula_text": "= \uf8ee \uf8ef \uf8f0J 1 (y) . . . J N (y) \uf8f9 \uf8fa \uf8fb,F = \uf8ee \uf8ef \uf8f0f 1 (y) . . . f N (y) \uf8f9 \uf8fa \uf8fb,(19)", "formula_coordinates": [11.0, 114.44, 672.92, 171.92, 42.32]}, {"formula_id": "formula_30", "formula_text": "\u2206y = \u2212 J TJ + \u03b5I \u22121J TF , (20", "formula_coordinates": [11.0, 367.65, 207.2, 173.32, 15.91]}, {"formula_id": "formula_31", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 214.47, 4.15, 8.64]}, {"formula_id": "formula_32", "formula_text": "\u03a3 y * = J TJ + \u03b5I \u22121 y=y * , (21", "formula_coordinates": [11.0, 368.39, 321.12, 172.57, 22.08]}, {"formula_id": "formula_33", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 328.39, 4.15, 8.64]}, {"formula_id": "formula_34", "formula_text": "X = x 3D i , x 2D i , w 2D i i = 1 \u2022 \u2022 \u2022 N , we generate M subsets consisting of n corresponding points each (3 \u2264 n < N ),", "formula_coordinates": [11.0, 308.86, 512.17, 236.25, 32.87]}, {"formula_id": "formula_35", "formula_text": "p(i) = w 2D i 1 N i=1 w 2D i 1 . (22", "formula_coordinates": [11.0, 380.9, 592.67, 160.06, 30.04]}, {"formula_id": "formula_36", "formula_text": ")", "formula_coordinates": [11.0, 540.96, 602.21, 4.15, 8.64]}, {"formula_id": "formula_37", "formula_text": "q T (t) = \u0393 \u03bd+3 2 \u0393 \u03bd 2 \u03bd 3 \u03c0 3 |\u03a3| 1 + 1 \u03bd t \u2212 \u00b5 2 \u03a3 \u2212 \u03bd+3 2 ,(23)", "formula_coordinates": [12.0, 59.02, 322.65, 227.34, 31.47]}, {"formula_id": "formula_38", "formula_text": "t \u2212 \u00b5 2 \u03a3 = (t \u2212 \u00b5) T \u03a3 \u22121 (t \u2212 \u00b5)", "formula_coordinates": [12.0, 82.29, 363.32, 129.3, 12.48]}, {"formula_id": "formula_39", "formula_text": "q VM (\u03b8) = exp (\u03ba cos (\u03b8 \u2212 \u00b5)) 2\u03c0I 0 (\u03ba) ,(24)", "formula_coordinates": [12.0, 105.62, 658.77, 180.74, 23.23]}, {"formula_id": "formula_40", "formula_text": "q mix (\u03b8) = (1 \u2212 \u03b1)q VM (\u03b8) + \u03b1q uniform (\u03b8),(25)", "formula_coordinates": [12.0, 345.26, 94.7, 199.85, 9.81]}, {"formula_id": "formula_41", "formula_text": "\u00b5 \u2190 \u03b8 * , \u03ba \u2190 1 3\u03c3 2 \u03b8 * .", "formula_coordinates": [12.0, 435.88, 223.81, 78.38, 16.11]}, {"formula_id": "formula_42", "formula_text": "by:\u03ba =r (2 \u2212r 2 ) 1 \u2212r 2 ,(26)", "formula_coordinates": [12.0, 358.12, 291.99, 186.99, 33.26]}, {"formula_id": "formula_43", "formula_text": "q ACG (l) = (l T \u039b \u22121 l) \u22122 S 4 |\u039b| 1 2 ,(27)", "formula_coordinates": [12.0, 379.04, 479.06, 166.08, 26.23]}, {"formula_id": "formula_44", "formula_text": "\u039b \u2190\u039b + \u03b1|\u039b| 1 4 I,(28)", "formula_coordinates": [12.0, 389.7, 658.08, 155.42, 12.33]}, {"formula_id": "formula_45", "formula_text": "\u039b = 4 j v j j v j l j l T j l T j\u039b \u22121 l j .(29)", "formula_coordinates": [13.0, 116.45, 120.64, 169.91, 29.1]}, {"formula_id": "formula_46", "formula_text": "L pos = \uf8f1 \uf8f2 \uf8f3 d 2 t 2\u03b2 , d t \u2264 \u03b2, d t \u2212 0.5\u03b2, d t > \u03b2,(30)", "formula_coordinates": [13.0, 106.39, 297.67, 179.97, 39.09]}, {"formula_id": "formula_47", "formula_text": "L orient = 1 \u2212 cos d \u03b8 .(31)", "formula_coordinates": [13.0, 127.51, 432.87, 158.85, 9.81]}, {"formula_id": "formula_48", "formula_text": "L orient = 2 \u2212 2 (l * + \u2206l) T l gt 2 .(32)", "formula_coordinates": [13.0, 104.17, 482.92, 182.19, 14.07]}, {"formula_id": "formula_49", "formula_text": "c tgt = Score( t * XZ \u2212 t XZgt ) = max(0, min(1, \u2212a log t * XZ \u2212 t XZgt + b)), (33", "formula_coordinates": [14.0, 60.84, 109.01, 221.37, 27.79]}, {"formula_id": "formula_50", "formula_text": ")", "formula_coordinates": [14.0, 282.21, 126.34, 4.15, 8.64]}, {"formula_id": "formula_51", "formula_text": "L proj = \u2212 log RoI nhead k=1 \u03c6 k | diag w 2D k | exp \u2212 1 2 f k (y gt ) 2 ,(34)", "formula_coordinates": [14.0, 57.24, 458.69, 229.12, 31.49]}, {"formula_id": "formula_52", "formula_text": "L regr = nhead k=1 \u03c6 k \u03c1 x 3D k \u2212 x 3D gt 2 , (35", "formula_coordinates": [14.0, 100.25, 685.62, 181.96, 30.72]}, {"formula_id": "formula_53", "formula_text": ")", "formula_coordinates": [14.0, 282.21, 696.52, 4.15, 8.64]}, {"formula_id": "formula_54", "formula_text": "dense", "formula_coordinates": [14.0, 354.91, 206.31, 24.18, 8.69]}, {"formula_id": "formula_55", "formula_text": "c MC = 1 j v j j v j Score t * XZ \u2212 t XZj ,(36)", "formula_coordinates": [15.0, 332.55, 324.34, 212.57, 26.65]}, {"formula_id": "formula_56", "formula_text": "c mix = c \u03b1 MC c 1\u2212\u03b1 pred , (37", "formula_coordinates": [15.0, 393.21, 416.75, 147.76, 13.45]}, {"formula_id": "formula_57", "formula_text": ")", "formula_coordinates": [15.0, 540.96, 419.29, 4.15, 8.64]}, {"formula_id": "formula_58", "formula_text": "x 3D i \u2208 R 3", "formula_coordinates": [19.0, 104.31, 124.23, 59.48, 12.42]}, {"formula_id": "formula_59", "formula_text": "x 2D i \u2208 R 2", "formula_coordinates": [19.0, 104.31, 136.18, 59.48, 12.32]}, {"formula_id": "formula_60", "formula_text": "w 2D i \u2208 R 2 + Weight", "formula_coordinates": [19.0, 103.46, 148.14, 122.61, 12.32]}], "doi": ""}