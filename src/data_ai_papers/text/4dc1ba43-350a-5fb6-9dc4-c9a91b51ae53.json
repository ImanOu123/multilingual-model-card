{"title": "Towards Streaming Perception", "authors": "Mengtian Li; Yu-Xiong Wang; Deva Ramanan;  Cmu; Argo Ai", "pub_date": "", "abstract": "Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. The responsiveness of the agent is largely governed by latency of its processing pipeline. While past work has studied the algorithmic trade-off between latency and accuracy, there has not been a clear metric to compare different methods along the Pareto optimal latency-accuracy curve. We point out a discrepancy between standard offline evaluation and real-time applications: by the time an algorithm finishes processing a particular frame, the surrounding world has changed. To these ends, we present an approach that coherently integrates latency and accuracy into a single metric for real-time online perception, which we refer to as \"streaming accuracy\". The key insight behind this metric is to jointly evaluate the output of the entire perception stack at every time instant, forcing the stack to consider the amount of streaming data that should be ignored while computation is occurring. More broadly, building upon this metric, we introduce a meta-benchmark that systematically converts any single-frame task into a streaming perception task. We focus on the illustrative tasks of object detection and instance segmentation in urban video streams, and contribute a novel dataset with high-quality and temporally-dense annotations. Our proposed solutions and their empirical analysis demonstrate a number of surprising conclusions: (1) there exists an optimal \"sweet spot\" that maximizes streaming accuracy along the Pareto optimal latency-accuracy curve, (2) asynchronous tracking and future forecasting naturally emerge as internal representations that enable streaming perception, and (3) dynamic scheduling can be used to overcome temporal aliasing, yielding the paradoxical result that latency is sometimes minimized by sitting idle and \"doing nothing\".", "sections": [{"heading": "Introduction", "text": "Embodied perception refers to the ability of an autonomous agent to perceive its environment so that it can (re)act. A crucial quantity governing the responsiveness of the agent is its reaction time. Practical applications, such as self-driving vehicles or augmented reality and virtual reality (AR/VR), may require reaction time that rivals that of humans, which is typically 200 milliseconds (ms) for visual stimuli [22]. In such settings, low-latency algorithms are imperative to ensure safe operation or enable a truly immersive experience.\nHistorically, the computer vision community has not particularly focused on algorithmic latency. This is one reason why a disparate set of techniques (and arXiv:2005.10420v2 [cs.CV] 25 Aug 2020\nA B = 1 = 2 Computation\nFig. 1. Latency is inevitable in a real-world perception system. The system takes a snapshot of the world at t1 (the car is at location A), and when the algorithm finishes processing this observation, the surrounding world has already changed at t2 (the car is now at location B, and thus there is a mismatch between prediction A and ground truth B). If we define streaming perception as a task of continuously reporting back the current state of the world, then how should one evaluate vision algorithms under such a setting? We invite the readers to watch a video on the project website that compares a standard frame-aligned visualization with our latency-aware visualization [Link].\nconference venues) have been developed for robotic vision. Interestingly, latency has been well studied recently (e.g., fast but not necessarily state-of-the-art accurate detectors such as [34,27,25]). But it has still been primarily explored in an offline setting. Vision-for-online-perception imposes quite different latency demands as shown in Fig. 1, because by the time an algorithm finishes processing a particular frame -say, after 200ms -the surrounding world has changed! This forces perception to be ultimately predictive of the future. In fact, such predictive forecasting is a fundamental property of human vision (e.g., as required whenever a baseball player strikes a fast ball [31]). So we argue that streaming perception should be of interest to general computer vision researchers.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Contribution (meta-benchmark)", "text": "To help explore embodied vision in a truly online streaming context, we introduce a general meta-benchmark that systematically converts any single-frame task into a streaming perception task. Our key insight is that streaming perception requires understanding the state of the world at all time instants -when a new frame arrives, streaming algorithms must report the state of the world even if they have not done processing the previous frame. Within this meta-benchmark, we introduce an approach to measure the real-time performance of perception systems. The approach is as simple as querying the state of the world at all time instants, and the quality of the response is measured by the original task metric. Such an approach naturally merges latency and accuracy into a single metric. Therefore, the trade-off between accuracy versus latency can now be measured quantitatively. Interestingly, our meta-benchmark naturally evaluates the perception stack as a whole.\nFor example, a stack may include detection, tracking, and forecasting modules.\nOur meta-benchmark can be used to directly compare such modular stacks to end-to-end black-box algorithms [28]. In addition, our approach addresses the issue that overall latency of concurrent systems is hard to evaluate (e.g., latency cannot be simply characterized by the runtime of a single module).\nContribution (analysis) Motivated by perception for autonomous vehicles, we instantiate our meta-benchmark on the illustrative tasks of object detection and instance segmentation in urban video streams. Accompanied with our streaming evaluation is a novel dataset with high-quality, high-frame-rate, and temporallydense annotations of urban videos. Our evaluation on these tasks demonstrates a number of surprising conclusions. (1) Streaming perception is significantly more challenging than offline perception. Standard metrics like object-detection average precision (AP) dramatically drop (from 38.0 to 6.2), indicating the need for the community to focus on such problems. (2) Decision-theoretic scheduling, asynchronous tracking, and future forecasting naturally emerge as internal representations that enable accurate streaming perception, recovering much of the performance drop (boosting performance to 17.8). With simulation, we can verify that infinite compute resources modestly improves performance to 20.3, implying that our conclusions are fundamental to streaming processing, no matter the hardware. (3) It is well known that perception algorithms can be tuned to trade off accuracy versus latency. Our analysis shows that there exists an optimal \"sweet spot\" that uniquely maximizes streaming accuracy. This provides a different perspective on such well-explored trade-offs. (4) Finally, we demonstrate the effectiveness of decision-theoretic reasoning that dynamically schedules which frame to process at what time. Our analysis reveals the paradox that latency is minimized by sometimes sitting idle and \"doing nothing\"! Intuitively, it is sometimes better to wait for a fresh frame rather than to begin processing one that will soon become \"stale\".", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Latency evaluation Latency is a well-studied subject in computer vision. One school of research focuses on reducing the FLOPS of backbone networks [20,40], while another school focuses on reducing the runtime of testing time algorithms [34,27,25]. We follow suit and create a latency-accuracy plot under our experiment setting (Fig. 2). While such a plot is suggestive of the trade-off for offline data processing (e.g., archived video footage), it fails to capture the fact that when the algorithm finishes processing, the surrounding world has already changed. Therefore, we believe that existing plots do not reveal the streaming performance of these algorithms. Aside from computational latency, prior work has also investigated algorithmic latency [30], evaluated by running algorithms on a video in the offline fashion and measuring how many frames are required to detect an object after it appears. In comparison, our evaluation is done in the more realistic online real-time setting, and applies to any single-frame task, instead of just object detection. Prior art routinely explores the trade-off between detection accuracy versus runtime. We generate the above plot by varying the input resolution of each detection network. We argue that such plots are exclusive to offline processing and fail to capture latencyaccuracy trade-offs in streaming perception. AP stands for average precision, and is a standard metric for object detection [26].\nReal-time evaluation There has not been much prior effort to evaluate vision algorithms in the real-time fashion in the research community. Notable exceptions include work on real-time tracking and real-time simultaneous localization and mapping (SLAM). First, the VOT2017 tracking benchmark specifically included a real-time challenge [23]. Its benchmark toolkit sends out frames at 20 FPS to participants' trackers and asks them to report back results before the next frame arrives. If the tracker fails to respond in time, the last reported result is used. This is equivalent to applying zero-order hold to trackers' outputs. In our benchmarks, we adopt a similar zero-order hold strategy, but extend it to a broader context of arbitrary single-frame tasks and allow for a more delicate interplay between detection, tracking, and forecasting. Second, the literature on real-time SLAM also considers benchmark evaluation under a \"hard-enforced\" real-time requirement [5,14]. Our analysis suggests that hardenforcement is too stringent of a formulation; algorithms should be allowed to run longer than the frame rate, but should still be scored on their ability to report the state of the world (e.g., localized map) at frame rate.\nProgressive and anytime algorithms There exists a body of work on progressive and anytime algorithms that can generate outputs with lower latency. Such work can be traced back to classic research on intelligent planning under resource constraints [4] and flexible computation [19], studied in the context of AI with bounded rationality [35]. Progressive processing [42] is a paradigm that splits up an algorithm into sequential modules that can be dynamically scheduled. Often, scheduling is formulated as a decision-theoretic problem under resource constraints, which can be solved in some cases with Markov decision processes (MDPs) [41,42]. Anytime algorithms are capable of returning a solution at any point in time [41]. Our work revisits these classic computation paradigms in the context of streaming perception, specifically demonstrating that classic visual tasks (like tracking and forecasting) naturally emerge in such bounded resource settings.", "publication_ref": ["b4", "b13", "b3", "b18"], "figure_ref": ["fig_10"], "table_ref": []}, {"heading": "Proposed Evaluation", "text": "In the previous section, we have shown that existing latency evaluation fails to capture the streaming performance. t Fig. 3. Our proposed streaming perception evaluation. A streaming algorithm f is provided with (timestamped) observations up until the current time t and refreshes an output buffer with its latest prediction of the current state of the world. At the same time, the benchmark constantly queries the output buffer for estimates of world states. Crucially, f must consider the amount of streaming observations that should be ignored while computation is occurring. method of evaluation. Intuitively, a streaming benchmark no longer evaluates a function, but a piece of executable code over a continuous time frame. The code has access to a sensor input buffer that stores the most recent image frame. The code is responsible for maintaining an output buffer that represents the up-todate estimate of the state of the world (e.g., a list of bounding boxes of objects in the scene). The benchmark examines this output buffer, comparing it with a ground truth stream of the actual world state (Fig. 3).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Formal definition", "text": "We model a data stream as a set of sensor observations, ground-truth world states, and timestamps, denoted respectively as {(x i , y i , t i )} T i=1 . Let f be a streaming algorithm to be evaluated. At any continuous time t, the algorithm f is provided with observations (and timestamps) that have appeared so far:\n{(x i , t i )|t i \u2264 t} [accessible input at time t] (1)\nWe allow the algorithm f to generate an output prediction at any time. Let s j be the timestamp that indicates when a particular prediction\u0177 j is produced. The subscript j indexes over the N outputs generated by f over the entire stream:\n{(\u0177 j , s j )} N j=1 [all outputs by f ] (2)\nNote that this output stream is not synchronized with the input stream, and N has no direct relationship with T . Generally speaking, we expect algorithms to run slower than the frame rate (N < T ).\nWe benchmark the algorithm f by comparing its most recent output at time t i to the ground-truth y i . We first compute the index of the most recent output:\n\u03d5(t) = arg max j s j < t [real-time constraint] (3)\nThis is equivalent to the benchmark applying a zero-order hold for the algorithm's outputs to produce continuous estimation of the world states. Given an arbitrary single-frame loss L, the benchmark formally evaluates:\nL streaming = L({(y i ,\u0177 \u03d5(ti) )} T i=1 ) [evaluation] (4)\nBy construction, the streaming loss above can be applied to any single-frame task that computes a loss over a set of ground truth and prediction pairs.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Emergent tracking and forecasting", "text": "At first glance, \"instant\" evaluation may seem unreasonable: the benchmark at time t queries the state at time t. Although x t is made available to the algorithm, any finite-time algorithm cannot make use of it to generate its prediction.\nFor example, if the algorithm takes time \u2206t to perform its computation, then to make a prediction at time t, it can only use data before time t \u2212 \u2206t. We argue that this is the realistic setting for streaming perception, both in biological and robotic systems. Humans and autonomous vehicles must react to the instantaneous state of the world when interacting with dynamic scenes. Such requirements strongly suggest that perception should be inherently predictive of the future. Our benchmark similarly \"forces\" algorithms to reason and forecast into the future, to compensate for the mismatch between the last processed observation and the present. One may also wish to take into account the inference time of downstream actuation modules (that say, need to optimize a motion plan that will be executed given the perceived state of the world). It is straightforward to extend our benchmark to require algorithms to generate a forecast of the world state when the downstream module finishes its processing. For example, at time t the benchmark queries the state of the world at time t + \u03b7, where \u03b7 > 0 represents the inference time of the downstream actuation module.\nIn order to forecast, the algorithms need to reason temporally through tracking (in the case of object detection). For example, constant velocity forecasting requires the tracks of each object over time in order to compute the velocity. Generally, there are two categories of trackers -post-hoc association [3] and template-based visual tracking [29]. In this paper, we refer them in short as \"association\" and \"tracking\", respectively. Association of previously computed detections can be made extremely lightweight with simple linking of bounding boxes (e.g., based on the overlap). However, association does not make use of the image itself as done in (visual) tracking. We posit that trackers may produce better streaming accuracy for scenes with highly unpredictable motion. As part of emergent solutions to our streaming perception problem, we include both association and tracking in our experiments in the next section.\nFinally, it is natural to seek out an end-to-end system that directly optimizes streaming perception accuracy. We include one such method in Appendix C.2 to show that tracking and forecasting-based representations may also emerge from gradient-based learning. Fig. 4. Two computation models considered in our evaluation. Each block represents an algorithm running on a device and its length indicates its runtime.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Computational constraints", "text": "Because our metric is runtime dependent, we need to specify the computational constraints to enable a fair comparison between algorithms. We first investigate a single GPU model (Fig. 4a), which is used for existing latency analysis in prior art. In the single GPU model, only a single GPU job (e.g., detection or visual tracking) can run at a time. Such a restriction avoids multi-job interference and memory capacity issues. Note that a reasonable number of CPU jobs are allowed to run concurrently with the GPU job. For example, we allow bounding box association and forecasting modules to run on the CPU in Fig. 7.\nNowadays, it is common to have multiple GPUs in a single system. We investigate an infinite GPU model (Fig. 4b), with no restriction on the number of GPU jobs that can run concurrently. We implement this infinite computation model with simulation, described in the next subsection.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Challenges for practical implementation", "text": "While our benchmark is conceptually simple, there are several practical hurdles. First, we require high-frame-rate ground truth annotations. However, due to high annotation cost, most existing video datasets are annotated at rather sparse frame rates. For example, YouTube-VIS is annotated at 6 FPS, while the video data rate is 30 FPS [39]. Second, our evaluation is hardware dependent -the same algorithm on different hardware may yield different streaming performance. Such hardware-in-the-loop testing is commonplace in control systems [1] and arguably vital for embodied perception (which should by definition, depend on the agent's body!). Third, stochasticity in actual runtimes yields stochasticity in the streaming performance. Note that the last two issues are also prevalent in existing offline runtime analyses. Here we present high-level ideas for the solutions and leave additional details to Appendix A.2 & A.3.", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Pseudo ground truth", "text": "We explore the use of pseudo ground truth labels as a surrogate to manual high-frame-rate annotations. The pseudo labels are obtained by running state-of-the-art, arbitrarily expensive offline algorithms on each frame of a benchmark video. While the absolute performance numbers (when benchmarked on ground truth and pseudo ground truth labels) differ, we find that the rankings of algorithms are remarkably stable. The Pearson correlation coefficient of the scores of the two ground truth sets is 0.9925, suggesting that the real score is literally a linear function of the pseudo score. Moreover, we find that offline pseudo ground truth could also be used to self-supervise the training of streaming algorithms.\nSimulation While streaming performance is hardware dependent, we now demonstrate that the benchmark can be evaluated on simulated hardware. In simulation, the benchmark assigns a runtime to each module of the algorithm, instead of measuring the wall-clock time. Then based on the assigned runtime, the simulator generates the corresponding output timestamps. The assigned runtime to each module provides a layer of abstraction on the hardware.\nThe benefit of simulation is to allow us to assess the algorithm performance on non-existent hardware, e.g., a future GPU that is 20% faster or infinite GPUs in a single system. Simulation also allows our benchmark to inform practitioners about the design of future hardware platforms, e.g., one can verify with simulation that 4 GPUs may be \"optimal\" (producing the same streaming accuracy as infinite GPUs).\nRuntime-induced variance Due to algorithmic choice and system scheduling, different runs of the same algorithm may end up with different runtimes. This variation across runs also affects the overall streaming performance. Fortunately, we empirically find that such variance causes a standard deviation of up to 0.5% under our experiment setting. Therefore, we omit variance report in our experiments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Solutions and Analysis", "text": "In this section, we instantiate our meta-benchmark on the illustrative task of object detection. While we show results on streaming detection, several key ideas also generalize to other tasks. An instantiation on instance segmentation can be found in Appendix A.6. We first explain the setup and present the solutions and analysis. For the solutions, we first consider single-frame detectors, and then add forecasting and tracking one by one into the discussion. We focus on the most effective combination of detectors, trackers, and forecasters which we have evaluated, but include additional methods in Appendix C.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Setup", "text": "We extend the publicly available video dataset Argoverse 1.1 [7] with our own annotations for streaming evaluation, which we name Argoverse-HD (High-framerate Detection). It contains diverse urban outdoor scenes from two US cities. We select Argoverse for its embodied setting (autonomous driving) and its highframe-rate sensor data (30 FPS). We focus on the task of 2D object detection for our streaming evaluation. Under this setting, the state of the world y t is a list of bounding boxes of the objects of interest. While Argoverse has multiple sensors, we only use the center RGB camera for simplicity. We collect our own  5 shows a comparison of our annotation with that of MS COCO. Additional comparison with other related datasets can be found in Appendix A.4. All output timing is measured on a single Geforce GTX 1080 Ti GPU (a Tesla V100 counterpart is provided in Appendix A.7).", "publication_ref": ["b6"], "figure_ref": [], "table_ref": []}, {"heading": "Detection-Only", "text": "Table 1 includes the main results of using just detectors for streaming perception. We first examine the case of running a state-of-the-art detector -Hybrid Task Cascade (HTC) [8], both in the offline and the streaming settings. The AP drops Table 1. Performance of existing detectors for streaming perception. The number after @ is the input scale (the full resolution is 1920 \u00d7 1200). * means using GPU for image pre-processing as opposed to using CPU in the off-the-shelf setting. The last column is the mean runtime of the detector for a single frame in milliseconds (mask branch disabled if applicable). The first baseline is to run an accurate detector (row 1), and we observe a significant drop of AP in the online real-time setting (row 2). Another commonly adopted baseline for embodied perception is to run a fast detector (row 3-4), whose runtime is smaller than the frame interval (33ms for 30 FPS streams). Neither of these baselines achieves good performance. Searching over a wide suite of detectors and input scales, we find that the optimal solution is Mask R-CNN (ResNet 50) operating at 0.5 input scale (row 5-6). In addition, our scheduling algorithm (Alg. 1) boosts the performance by 1.0/2.3 for AP/APL (row 7). In the hypothetical infinite GPU setting, a more expensive detector yields better trade-off (input scale switching from 0.5 to 0.75, almost doubling the runtime), and it further boosts the performance to 14.4 (row 8), which is the optimal solution achieved by just running the detector. Simulation suggests that 4 GPUs suffice to maximize streaming accuracy for this solution significantly in the streaming setting. Such a result is not entirely surprising due to its high runtime (700ms). A commonly adopted strategy for real-time applications is to run a detector that is within the frame rate. We point out that this strategy may be problematic, since such a hard-constrained time budget results in poor accuracy for challenging tasks (Table 1 row 3-4). In addition, we find that many existing network implementations are optimized for throughput rather than latency, reflecting the bias of the community for offline versus online processing! For example, image pre-processing (e.g., resizing and normalizing) is often done on CPU, where it can be pipelined with data pre-fetching. By moving it to GPU, we save 21ms in latency (for an input of size 960 \u00d7 600).\nOur benchmarks allow streaming algorithms to choose which frames to process/ignore. Figure 6 compares a straight-forward schedule with our dynamic schedule (Alg. 1), which attempts to address temporal aliasing of the former. While spatial aliasing and quantization has been studied in computer vision [18], temporal quantization in the streaming setting has not been well explored. Noteably, it is difficult to pre-compute the optimal schedule because of the stochasticity of actual runtimes. Our proposed scheduling policy (Alg. 1) tries to minimize the expected temporal mismatch of the output stream and the data stream, thus increasing the overall streaming performance. Empirically, we find that it raises the AP for the detector (Table 1 row 7). We provide a theoretical analysis of Fig. 6. Algorithm scheduling for streaming perception with a single GPU. (a) A fast detector finishes processing the current frame before the next frame arrives. An accurate (but slow) detector cannot process every frame due to high latency. In this example, frame 1 is skipped. Note that the goal of streaming perception is not to process every frame but to produce accurate state estimates in a timely manner. (b) One straightforward schedule is to simply process the latest available frame upon the completion of the previous processing (idle-free). However, if latest available frame will soon become stale, it might be better to idle and wait for a fresh frame (our dynamic schedule, Alg. 1). In this illustration, Alg. 1 determines that frame 2 will soon become stale and decides to wait (visualized in red) for frame 3 by comparing the tails \u03c42 and \u03c43.\nAlgorithm 1 Shrinking-tail policy 1: Given finishing time s and algorithm runtime r in the unit of frames (assuming r > 1), this policy returns whether the algorithm should wait for the next frame 2: Define tail function\n\u03c4 (t) = t \u2212 t 3: return [\u03c4 (s + r) < \u03c4 (s)] (Iverson bracket)\nthe algorithm and additional empirical results for a wide suite of detectors in Appendix B.1. Note that Alg. 1 is by construction task agnostic (not specific to object detection).", "publication_ref": ["b7", "b17"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Forecasting", "text": "Now we expand our solution space to include forecasting methods. We experimented with both constant velocity models and first-order Kalman filters. We find good performance with the latter, given a small modification to handle asynchronous sensor measurements (Fig. 7). The classic Kalman filter [21] operates on uniform time steps, coupling prediction and correction updates at each step. In our case, we perform correction updates only when a sensor measurement is available, but predict at every step. Second, due to frame-skipping, the Kalman filter should be time-varying (the transition and the process noise depend on the length of the time interval, details can be found in Appendix B.2). Association for bounding boxes across frames is required to update the Kalman filter, and we apply IoU-based greedy matching. For association and forecasting, the computation involves only bounding box coordinates and therefore is very lightweight (< 2ms on CPU). We find that such overhead has little influence on the overall AP. The results are summarized in Table 2.\nStreamer (meta-detector) Note that our dynamic scheduler (Alg. 1) and asynchronous Kalman forecaster can be applied to any off-the-shelf detector, Fig. 7. Scheduling for association and forecasting. Association takes place immediately after a new detection result becomes available, and it links the bounding boxes in two consecutive detection results. Forecasting takes place right before the next time step and it uses an asynchronous Kalman filter to produce an output as the estimation of the current world state. By default, the prediction step also updates internal states in the Kalman filter and is always called before the update step. In our case, we perform multiple update-free predictions (green blocks) until we receive a frame result. regardless of its underlying latency (or accuracy). This means that we can assemble these modules into a meta-detector -which we call Streamer -that converts any detector into a streaming detection system that reports real-time detections at an arbitrary framerate. Appendix B.4 evaluates the improvement in streaming AP across 80 different settings (8 detectors \u00d7 5 image scales \u00d7 2 compute models), which vary from 4% to 80% with an average improvement of 33%.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Visual tracking", "text": "Visual tracking is an alternative for low-latency inference, due to its faster speed than a detector. For our experiments, we adopt the state-of-the-art multi-object tracker [2] (which is second place in the MOT'19 challenge [11] and is open sourced), and modify it to only track previously identified objects to make it faster than the base detector (see Appendix B.3). This tracker is built upon a two-stage detector and for our experiment, we try out the configurations of Mask R-CNN with different backbones and with different input scales. Also, we need a scheduling scheme for this detection plus tracking setting. For simplicity, we only explored running detection at fixed strides of 2, 5, 15, and 30. For example, stride 30 means that we run the detector once and then run the tracker 29 times, with the tracker getting reset after each new detection. Table 3 row 1 contains the best configuration over backbone, input scale, and detection stride.", "publication_ref": ["b1", "b10"], "figure_ref": [], "table_ref": ["tab_3"]}, {"heading": "Discussion", "text": "Streaming perception remains a challenge Our analysis suggests that streaming perception involves careful integration of detection, tracking, forecasting, and dynamic scheduling. While we present several strong solutions for streaming perception, the gap between the streaming performance and the offline performance remains significant (20.3 versus 38.0 in AP). This suggests that there is considerable room for improvement by building a better detector, tracker, forecaster, or even an end-to-end model that blurs boundary of these modules.\nFormulations of real-time computation Common folk wisdom for real-time applications like online detection requires that detectors run within the sensor frame rate. Indeed, classic formulations of anytime processing require algorithms to satisfy a \"contract\" that they will finish under a compute budget [41]. Our analysis suggests that this view of computation might be too myopic as evidenced by contemporary robotic systems [33]. Instead, we argue that the sensor rate and compute budget should be seen as design choices that can be tuned to optimize a downstream task. Our streaming benchmark allows for such a global perspective.\nGeneralization to other tasks By construction, our meta-benchmark and dynamic scheduler (Alg. 1) are not restricted to object detection. We illustrate such generalization with an additional task of instance segmentation (Fig. 9). However, there are several practical concerns that need to be addressed. Densely annotating video frames for instance segmentation is almost prohibitively expensive. Therefore, we adopt offline pseudo ground truth (Section 3.4) to evaluate streaming performance. Another concern is that the forecasting module is taskspecific. In the case of instance segmentation, we implement it as forecasting  the bounding boxes and then warping the masks accordingly. Please refer to Appendix A.6 for the complete streaming instance segmentation benchmark.", "publication_ref": [], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "We introduce a meta-benchmark for systematically converting any single-frame task into a streaming perception task that naturally trades off computation between multiple modules (e.g., detection versus tracking). We instantiate this meta-benchmark on tasks of object detection and instance segmentation. In general, we find online perception to be dramatically more challenging than its offline counterpart, though significant performance can be recovered by incorporating forecasting. We use our analysis to develop a simple meta-detector that converts any detector (with any internal latency) into a streaming perception system that can operate at any frame rate dictated by a downstream task (such as a motion planner). We hope that our analysis will lead to future endeavor in this under-explored but crucial aspect of real-time embodied perception. For example, streaming benchmarks can be used to motivate attentional processing; by spending more compute only on spatially [16] or temporally [32] challenging regions, one may achieve even better efficiency-accuracy tradeoffs. We summary the contents of the appendix as follows. Appendix A describes additional details of our meta-benchmark, including discussion on the definition, pseudo ground-truth, simulation, dataset and instantiations for novel hardware and task. Appendix B provides additional details of our proposed solutions, including scheduling, tracking and forecasting. Finally, Appendix C includes additional baselines for a more thorough evaluation.", "publication_ref": ["b15"], "figure_ref": [], "table_ref": []}, {"heading": "A Benchmark Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.1 Additional Discussion on the Benchmark Definition", "text": "In Section 3.1, we defined our benchmark as evaluation over a discrete set of frames. One might point out that a continuous definition is more consistent with the notion of estimating the state of the world at all time instants for streaming perception. First, we note that it is possible to define a continuous-time counterpart, where the ground truth can be obtained via polynomial interpolation and the algorithm prediction can be represented as a function of time (e.g., simply derived from extrapolating the discrete output). Also in Eq 4, the aggregation function (implicit in L) could be integration. However, our choice of a discrete definition is mainly for two reasons: (1) we believe a high-frame-rate data stream is able to approximate the continuous evaluation; (2) most existing single-frame metrics (L, e.g., average-precision) is defined with a discrete set of input and we prefer that our streaming metric is compatible with these existing metrics.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Pseudo Ground Truth", "text": "We use manually obtained ground-truth for bounding-box-based object detection. As we point out in the main text, one could make use of pseudo ground truth by simply running an (expensive but accurate) off-line detector to generate detections that could be used to evaluate on-line streaming detectors.\nHere, we analyze the effectiveness of pseudo ground truth detection as a proxy for ground-truth. We adopt the state-of-the-art detector -Hybrid Task Cascade (HTC) [8] for computing the offline pseudo ground truth. As shown in Table 1, this offline detector dramatically outperforms all real-time streaming methods by a large margin. As shown in the main text, pseudo-streaming AP correlates extraordinarily well with ground-truth-streaming AP, with a normalized correlation coefficient of 0.9925. This suggests that pseudo ground truth can be used to rank streaming perception algorithms.\nWe emphasize that since we have constructed Argoverse-HD by deliberately annotating high frame rate bounding boxes, we use real ground truth for evaluating detection performance. However, obtaining such high-frame-rate annotations for instance segmentation is expensive. Hence we make use of pseudo groundtruth instance masks (provided by HTC) to benchmark streaming instance segmentation (Section A.6).", "publication_ref": ["b7"], "figure_ref": [], "table_ref": []}, {"heading": "A.3 Simulation", "text": "In true hardware-in-the-loop benchmarking, the output timestamp s j is simply the wall-clock time at which an algorithm produces an output. While we hold this as the gold-standard, one can dramatically simplify benchmarking by making use of simulation, where s j is computed using runtimes of different modules. For example, s j for a single-frame detector on a single GPU can be simulated by adding its runtime to the time when it starts processing a frame. Complicated perception stacks require considering runtimes of all modules (we model those that contribute > 1 ms) in order to accurately simulate timestamps.\nModeling runtime distribution Existing latency analysis [34,27,25] usually reports only the mean runtime of an algorithm. However, empirical runtimes are in fact stochastic (Fig. A), due to the underlying operating system scheduling and even due to the algorithm itself (e.g., proposal-based detectors often take longer when processing a scene with many objects). Because scene-complexity is often correlated across time, runtimes will also be correlated (a long runtime for a given frame may also hold for the next frame).\nWe performed a statistical analysis of runtimes, and found that a marginal empirical distribution to work well. We first run the algorithm over the entire dataset to get the empirical distribution of runtimes. At test time, we randomly sample a runtime when needed from the empirical distribution, without considering the correlation across time. Empirically, we found that the results (streaming AP) from a simulated run is within the variance of a real run.\n5XQWLPHPV\n'HQVLW\\ Simulation for non-existent hardware/algorithm Through simulation, our evaluation protocol does not directly depend on hardware, but on a collection of runtime distributions for different modules (known as a runtime profile).\nOne thus has the freedom to alter the distributions. For example, we can simulate a faster algorithm simply by scaling down the runtime profile. Table 3, uses simulation to evaluate the streaming performance of a non-existent tracker that runs twice as fast as the actual implementation on-hand. The reduced runtime could have arisen from better hardware; one can run the benchmark on a Geforce GTX 1080 Ti GPU and simulate the performance on a Tesla V100 GPU. We find that Tesla V100 makes our detectors run 16% faster, implying we can scale runtime profiles accordingly. For example, Mask R-CNN R50 @ s0.5 produces a simulated-streaming AP of 12.652 while the real-streaming AP (on a V100) is 12.645, suggesting that effectivness of simulated benchmarking.\nInfinite GPUs In simulation, we are not restricted by the number of physical GPUs present in a system. Therefore, we are able to perform analysis in the infinite GPU setting. In this setting, each detector or visual tracker runs on a different device without any interference with each other. Equivalently, we run a new GPU job on an existing device as long as it is idle. As a result, the simulation also provides information on how many GPUs are required for a particular infinite GPU experiment in practice (i.e., the maximum number of concurrent jobs). We summarize the number of GPUs required for the experiments in the main text in Table A. This implies that our streaming benchmark can be used to inform hardware design of future robotic platforms. ", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": ["tab_3", "tab_4"]}, {"heading": "A.5 Experiment Settings", "text": "Platforms The CPU used in our experiments is Xeon Gold 5120, and the GPU is Geforce GTX 1080 Ti. The software environment is PyTorch 1.1 with CUDA 10.0.\nTiming The setup which we time single-frame algorithms mimics the scenario in real-world applications. The offline pipeline involves several steps: loading data from the disk, image pre-processing, neural network forward pass, and result post-processing. Our timing excludes the first step of loading data from the disk. This step is mainly for dataset-based evaluation. In actual embodied applications, data come from sensors instead of disks. This is implemented by loading the entire video to the main memory before the evaluation starts. In summary, our timing (e.g., the last column of Table 1) starts at the time when the algorithm receives the image in the main memory, and ends at the time when the results are available in the main memory (instead of in the GPU memory).\nA.6 Alternate Task: Instance Segmentation In the main text, we propose a meta-benchmark and mention that it can be instantiated with different tasks. In this section, we include full benchmark evaluation for streaming instance segmentation.\nInstance segmentation is a more fine-grained task than object detection. This creates challenges for streaming evaluation as annotation becomes more expensive and forecasting is not straight-forward. We address these two issues by leveraging pseudo ground truth and warping masks according to the forecasted bounding boxes.\nAnother issue which we observed is that off-the-shelf pipelines are usually designed for benchmark evaluation or visualization. First, similar to object detection, we adopt GPU image pre-processing by default. Second, we found that more than 90% of the time within the mask head of Mask R-CNN is spent on transforming masks from the RoI space to the image space and compressing them in a format to be recognized by the COCO evaluation toolkit. Clearly, compression can be disabled for streaming perception. We point out that mask transformation can also be disabled. In practice, masks are used to tell if a specific point or region contains the object. Instead of transforming the mask (which involves object-specific image resizing operations), we can transform the query points or regions, which is simply a linear transformation over points or control points. Therefore, our timing does not include RoI-to-image transformation or mask compression. Furthermore, this also implies that we do not pay an additional cost for masks in forecasting, since only the box coordinates are updated but the masks remain in the RoI space.\nFor the instance segmentation benchmark, we use the same dataset and the same method HTC [8] for the pseudo ground truth as for detection, and we include 4 methods: Mask R-CNN [18] and Cascade Mask R-CNN [6] with ResNet 50 and ResNet 101 backbones. Since these are hybrid methods that produce both instance boxes and masks, we can measure the overhead of including masks as the difference between runtime with and without the mask head in Table C. We find that the average overhead is around 13%. We include the streaming evaluation in Tables D and E (with forecasting).\nTable D. Streaming evaluation for instance segmentation. We find that many of our observations for object detection still hold for instance segmentation: (1) AP drops significantly when moving from offline to real time, (2) the optimal \"sweet spot\" is not the fastest algorithm but the algorithm with runtime more than the unit frame interval, and (3) both our dynamic scheduling and infinite GPUs further boost the performance. Note that the absolute numbers might appear higher than the tables in the main text since we use pseudo ground truth here A.7 Alternate Hardware: Tesla V100\nIn the main text, we propose a meta-benchmark and mention that it can be instantiated with different hardware platforms. In this section, we include full benchmark evaluation for streaming detection with Tesla V100 (a faster GPU than GTX 1080 Ti used in the main text). While our benchmark is hardware dependent, the method of evaluation generalizes across hardware platforms, and our conclusions largely hold when the hardware environment changes. We follow the same setup as in the experiments in the main text, except that we use Tesla V100 from Amazon Web Services (EC2 instance of type p3.2xlarge). We provide the results for detection, forecasting, and tracking in Tables F, G, and H, respectively. We see that the improvement due to better hardware is largely orthogonal to the algorithmic improvement proposed in the main text.\nTable F. Performance of detectors for streaming perception on Tesla V100 (a faster GPU than the Geforce GTX 1080 Ti used in the main text). By comparing with Table 1 in the main text, we see that runtime is shortened and the AP is increased due to the boost of hardware performance. Different from Table 1, we only consider GPU image pre-processing here for simplicity. Interestingly, with additional computation power, Tesla V100 enables more expensive models like input scale 0.75 (row 4) and Cascade Mask R-CNN (row 5) to be the optimal configurations (detector and scale) under their corresponding settings. Note that the improvement from our dynamic scheduler is orthogonal to the boost from hardware performance Table G. Streaming perception with joint detection, association, and forecasting on Tesla V100 (corresponding to Table 2 in the main text). We observe similar boost as in the detection only setting (Table F). The \"re-optimize detection\" step finds that Mask R-CNN R50 @ s1.0 outperforms Cascade Mask R-CNN R50 @ s0.5 with forecasting (row2), and it also happens to be the optimal detector with infinite GPUs (row 3) \nID", "publication_ref": ["b7", "b17", "b5"], "figure_ref": [], "table_ref": ["tab_5", "tab_2"]}, {"heading": "B Solution Details", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Dynamic Scheduling", "text": "In the main text, we propose the dynamic scheduling algorithm (Alg. 1) to reduce temporal aliasing. Such an algorithm is counter-intuitive in that it minimizes latency by sometimes sitting idle. In this subsection, we provide additional theoretical analysis and empirical results for algorithm scheduling. We first introduce the framework to study algorithm scheduling for streaming perception.\nNext, we show theoretically that our dynamic scheduling outperforms naive idlefree scheduling for any constant runtime larger than the frame interval and any long-enough sequence length. Lastly, we verify empirically the superiority of our dynamic scheduling.\nTo study algorithm scheduling, we assume no concurrency (i.e., a single job at a time) and that jobs are not interruptible. For notational simplicity, we assume a fixed input frame rate where frame x i is the frame available at time i \u2208 {0, . . . , T \u2212 1} (i.e., zero-based indexing), and therefore i can be used to denote both frame index and time. We assume that time (time axis, runtime, and latency) is represented in the units of the number of frames. We also assume g to be a single-frame algorithm, and the streaming algorithm f is thus composed of g and a scheduling policy. No tracking or forecasting is used in the discussion below. Let k j be the input frame index that was processed to generate output o j = (\u0177 j , s j ): if\u0177 j = g(x i ), then k j = i. We denote the runtime of g as r. Definition (Temporal Mismatch) When the benchmark queries for the state of the world at frame i, the temporal mismatch is \u03b4 i := i \u2212 k j , where j = 12\n0 1 2 3 5 4 Input & Execution 6 ( 0 ) ( 1 )( 3 )", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Latest Prediction", "text": "Temporal Mismatch Cumulative Mismatch\n( 1 ) ( 0 ) ( 0 ) ( 1 ) \u2205 \u2205 Input Index For Each Query 0 0 3 1 1 \u2205 \u2205 Fig. B.\nTemporal mismatch for single-frame algorithms. Take t = 3 (query index i = 3) as an example (highlighted in orange): when the benchmark queries for y3, the latest prediction is g(x0), whose input index is 0, thus leading to a temporal mismatch of 3 (frames). arg max j s j < i. If there is no output available, \u03b4 i := 0. We denote the average temporal mismatch over the entire sequence as\u03b4.\nIntuitively, the temporal mismatch measures the latency of a streaming algorithm f in the unit of the number of frames (Fig. B). This latency is typically higher than the runtime of the single-frame algorithm g itself due to the blocking effect of consecutive execution blocks. For example, in Figure B, although runtime r < 2, the average mismatch\u03b4 = 15/7 > 2 for T = 7. Note that we define \u03b4 i := 0 if there is no output available. To avoid the degenerate case where an algorithm processes nothing and yields a zero cumulative temporal mismatch, we assume that all schedules start processing the first frame immediately at t = 0.\nMDP Naive idle-free scheduling processes the next available frame immediately after the previous execution is finished. However, a scheduler can choose when and which frames to process. Selection among such choices over the data sequence can be modeled as a decision policy under a Markov decision process (MDP).\nAn MDP formulation allows one to compute the expected future cumulative mismatch for a given policy under stochastic runtimes r. In theory, one may also be able to compute the optimal schedule (that minimizes expected cumulative mismatches) through policy search algorithms. However, Figure A shows that practical runtime profiles have low variance and are unimodal. If one assumes that runtimes are deterministic and fixed at a constant value, we will now show that our shrinking-tail policy outperforms idle-free over a range of runtimes r and sequence lengths T . We believe that constant runtime is a reasonable assumption for our setting, and empirically verify so after our theoretical analysis.\nPattern analysis Crucially, constant runtimes ensure that all transitions are deterministic, allowing for a global view of the sequence. Our key observation is that the global sequence will contain repeating mismatch patterns. Analysis of one such pattern sheds light on the cumulative mismatch of the entire sequence. For example, r = 1.5 under idle-free repeats itself every 2 processing blocks. However, different patterns emerge for different values of r and for different policies. We assume that r > 1 to avoid the trivial schedule where an algorithm consis- tently finishes before the next frame arrives. We write\u03b4 if and\u03b4 st for the average temporal mismatch\u03b4 for the idle-free and shrinking-tail policies, respectively. Our analysis is based on the concept of tail: \u03c4 (t) := t \u2212 t . We denote \u03c4 (r) as \u03c4 r for short. Note that the integral part of runtime does not contribute to the temporal quantization effect, and we thus focus on the discussion of 1 < r \u2264 2 for simplicity. We split our analysis into 3 different cases: r = 2, 1.5 \u2264 r < 2, and 1 < r < 1.5.", "publication_ref": [], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "Case 1", "text": "The first is a special case where \u03c4 r = 0. It can be easily verified that idle-free is equivalent to shrinking-tail, and thus\u03b4 st =\u03b4 if .\nCase 2 Now we inspect the case with 1.5 \u2264 r < 2. Since \u03c4 (2r) < 0.5 \u2264 \u03c4 (r), the shrinking-tail policy will output true (waiting) after processing the first frame. The waiting aligns the execution again with the integral time step, and thus for the subsequent processing blocks, it also outputs true (waiting). In summary, shrinking-tail always outputs true in this case, and its pattern in mismatch is agnostic to the specific runtime r (Fig. C). Let\u03b4 r denote\u03b4 with runtime r, then we can draw the conclusion that\u03b4 r1 st =\u03b4 r2 st for r 1 = r 2 , \u03c4 (r 1 ) \u2265 0.5, and \u03c4 (r 2 ) \u2265 0.5.\nWe then focus on a particular case of r = 1.5. As shown in Figure D, idle-free repeats itself in a period of 3 frames, and shrinking-tail repeats itself in a period of 2 frames. Together, they form a joint pattern that repeats itself in a period of 6 frames (their least common multiple). The diagram shows that within each common period, the difference of cumulative mismatch between idle-free and Note that each policy has its own repeating period and shrinking-tail always achieves 1 less cumulative mismatch within each common period.\nshrinking-tail is increased by 1. And it is the same for all common periods. Therefore, if T = 6n + 1 for some positive integer n (intuitively, the entire sequence is a multiple of several common periods),\u03b4 1.5 st <\u03b4 1.5 if . Additionally, Figure D enumerates all possible cases, where the sequence ends before a common period is over or in the middle of a common period. All these cases have\u03b4 1.5 st \u2264 \u03b4 1.5 if .\nNext, it is straightforward to see thatthe cumulative mismatch will not decrease if one increases the runtime r of g:\u03b4 r1 \u2264\u03b4 r2 if r 1 \u2264 r 2 . Therefore, for 1.5 \u2264 r < 2, we have\u03b4 r st =\u03b4 1.5 st \u2264\u03b4 1.5 if \u2264\u03b4 r if (\nCase 3 The last case with 1 < r < 1.5 (i.e., \u03c4 r < 0.5) is more complicated than previous cases because the underlying repeating pattern never exactly repeats itself. To address this issue, we must introduce several new concepts to characterize such near-repeating patterns. We first observe a special type of execution block: Definition (Shrinking-Tail Block) Denoting the start and the end time of an execution block as t 1 and t 2 , a shrinking-tail block is an execution block such that \u03c4 (t 1 ) > \u03c4 (t 2 ). As shown in Figure E, a shrinking-tail block increases temporal mismatch.\n0 + 1", "publication_ref": [], "figure_ref": ["fig_14"], "table_ref": []}, {"heading": "Waiting: False", "text": "0 0 + 2 0 + 3 0 + 4 1 Waiting: True ( 0 +1 ) (( 0 )\nInput Index Definition (Shrinking-Tail Cycle) A sequence of execution blocks can be divided into segments by a shrinking-tail block or an idle gap. A shrinkingtail cycle is a set of queries covered by the segment between these dividers. Specifically, the cycle starts from the 0-th query, the last query of a shrinkingtail block, or the query at the end of an idle gap. The cycle ends either when the sequence ends or the next cycle starts. The length of a cycle is the number of queries it covers.  Intuitively, blocks within each shrinking-tail cycle has tails increasing (\u03c41 < \u03c42 < \u03c43 and \u03c45 < \u03c46 < \u03c47). It ends when the tail decreases or there is an idle gap, and thus the tail \"shrinks\".\n0 0 + 1 0 Input Index 0 0 + 2 0 Difference in Mismatch 0 1 0 0 + 5\nAs shown in Figure F, shrinking-tail cycles are small segments of the entire sequence and they may have different lengths. Note that the definitions of both shrinking-tail block and cycle are agnostic to r, but we only refer to them during our discussion for 1 < r < 1.5. Instead of comparing\u03b4 for idle-free and shrinking-tail directly, we compare them for each cycle (denoted as\u03b4 (c) if and \u03b4 (c) st respectively). First, we observe that a shrinking-tail cycle starts with either a shrinking-tail block or an idle gap and ends with consecutive tail-increasing blocks. Second, we observe that most queries have a mismatch of 2 for both policies (e.g., Cycle 2's queries 20 to 21 and Cycle 4's queries 18 to 19 in Fig. F), and that the second query in a cycle is always 3 due to having a shrinking-tail block or an idle-gap before it. This is the rounding effect when adding multiple fractional numbers. The difference between the two policies is thus the mismatch of the first query. For 1 < r < 1.5, the first query of idle-free has a mismatch of 3, while shrinking-tail has a mismatch of 2. Intuitively, given that the majority of queries are with mismatch 2, the number of queries with mismatch 3 determines the relationship between\u03b4 (c) :\u03b4\n(c) st <\u03b4 (c)\nif . Therefore, when the sequence length is long enough, the policy with a smaller\u03b4 (c) leads to a smaller overall cumulative mismatch. Now, we present a more formal analysis on the above statement. To quantify the cycle patterns, we first quantify the number of consecutive tail-increasing blocks. Let the number of consecutive tail-increasing blocks be a and the tail of the first block covered by the cycle be b (in the case where the first block starts after an idle gap, we define b to be 0). We first observe that a = max{a |a \u03c4 r +b \u2264 1, a \u2208 N } = 1\u2212b \u03c4r . Also, b has its own range for each policy. Since a might vary from cycle to cycle, we introduce a reference quantity that is constant and can be used to measure the length of cycles. Let a 0 be the a when b = 0, i.e., a 0 = 1 \u03c4r , and c be the length of a cycle. For idle-free policy, c = a 0 + 2 or a 0 + 1. The variable length in cycles is due to variable b between cycles. When b \u2264 1 \u2212 a 0 \u03c4 r , we have the first type of cycle with length a 0 + 2 (denoted as c 1 ); when b > 1 \u2212 a 0 \u03c4 r , we have the second type of cycle of length a 0 + 1 (denoted as c 2 ). The starting cycle in a sequence is always the first type, while the ensuing cycles can be either the first or second type. Note that it is possible that all cycles are the first type. For example, when r = 1.25, each cycle resets itself and b = 0 for all cycles. For shrinking-tail policy, each cycle resets itself (whose length denoted as c 3 ). Note that c 1 , c 2 , c 3 denotes the length of the 3 types of cycles and Cycle 1, 2, 3, ... in the figures denote specific cycle instances. From the above analysis, we can see\nc 1 = a 0 + 2, c 2 = a 0 + 1, c 3 = a 0 + 1. (6\n)\n\u03b4 (c1) if = 2 a 0 + 2 + 2,\u03b4 (c2) if = 2 a 0 + 1 + 2,\u03b4 (c3) st = 1 a 0 + 1 + 2. (7\n)\nTherefore,\u03b4\n(c3) st <\u03b4 (c1) if <\u03b4 (c2) if(8)\nNext, we explain how to infer the relationship between\u03b4 from that between \u03b4 (c) . To analyze the mismatch of the whole sequence, we need to inspect the boundary cases at the start and the end of the sequence, where the cycle-based analysis might not hold. As shown in Figure G, the first cycles for both policies have different mismatch patterns due to empty detection at the first two queries. Compared to regular cycles in Figure F, the first cycle has 6 and 5 less total mismatch for idle-free and shrinking-tail policy respectively. Let m 1 , m 2 , and m 3 be the number of complete cycles of type c 1 , c 2 , and c 3 in a sequence, respectively, d be the number of residual queries at the end of the sequence that do not complete a cycle, and e be the total mismatch of these d queries, then we have\nT = m 1 c 1 + m 2 c 2 + d if (9) T = m 3 c 3 + d st (10\n)\n\u03b4 if = (m 1 c 1\u03b4 (c1) if + m 2 c 2\u03b4 (c2) if \u2212 6 + e if )/T (11\n)\n\u03b4 st = (m 3 c 3\u03b4 (c3) st \u2212 5 + e st )/T (12\n)\nNote that the above holds only when m 1 \u2265 1 and m 3 \u2265 1 (the sequence is at least one cycle long for both policies). If T is smaller or equal to one cycle, the two policies are equivalent and \u03b4 if = \u03b4 st . When T is large enough (e.g., T \u2192 \u221e), the\u03b4 (c) terms dominate Eq 11 and Eq 12, and due to Eq 7, we have\u03b4 st <\u03b4 if , which shows that the shrinking-tail policy is superior. Formally, when T > C(r), where C(r) is some constant depending on r,\u03b4 st <\u03b4 if .\nSummary of the theoretical analysis Considering all 3 cases, we can draw the conclusion that\u03b4 st \u2264\u03b4 if when T is large enough (greater than C(r) if \u03c4 r < 0.5, and no requirement otherwise). By achieving less average mismatch, shrinking-tail outperforms idle-free.\nPractical Performance of Dynamic Scheduling We apply our dynamic schedule (Alg. 1) to a wide suite of detectors under the same settings as our main experiments and summarize the results in Table I. In practice, runtime is stochastic due to complicated software and hardware scheduling or running an input adaptive model, but we find the theoretical results obtained under constant runtime assumption generalizes to most of the practical cases under our experiment setting.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Additional Details for Forecasting", "text": "We use an asynchronous Kalman filter for our forecasting module. The state representation which we choose is [x, y, w, h,\u1e8b,\u1e8f,\u1e87,\u1e23], where [x, y, w, h] are the top-left coordinates, and width and height of the bounding box, and the remaining four are their derivatives. The state transition is assumed to be linear. We also test with the representation used in SORT [3], which assumes that the area (the product of the width and the height) varies linearly instead of that each of the width and the height varies linearly. We find that such a representation produces lower numbers in AP.\nAs explained in the main text, Kalman filter needs to be asynchronous and time-varying for streaming perception. Let \u2206t k be the time-varying intervals Table I. Empirical performance comparison before and after using Alg. 1. We see that our shrinking-tail policy consistently boosts the streaming performance for different detectors and for different input scales. We also observe some failure cases (last two rows), where runtime is close to one frame duration. This is because our theoretical analysis assumes constant runtime, while it is dynamic in practice. Hence, the variance in runtime when it is a boundary value can make a noticeable difference on the performance Method AP (Before) AP (After) Runtime (ms) Runtime (frames) SSD @ s0.5 9.7 9.7 66.7 2.0 RetinaNet R50 @ s0. 5 10.9 11.6 54.5 1.6 RetinaNet R101 @ s0. 5 9.9 9.9 66.8 2.0 Mask R-CNN R101 @ s0. 5 11 between updates or prediction steps, we pick the transition matrix to be: 13) and the process noise to be\nF k = I 4\u00d74 \u2206t k I 4\u00d74 I 4\u00d74(\nQ k = \u2206t 2 k I 8\u00d78(14)\nIntuitively, the process noise is larger the longer between the updates.\nAll forecasting modules are implemented on the CPU and thus can be parallelized while the detector runs on the GPU. Our batched (over multiple objects) implementation of the asynchronous Kalman filter takes 0.98 \u00b1 0.39ms for the update step and 0.22 \u00b1 0.07ms for the prediction step, which are relatively very small overheads compared to detector runtimes. For scalable evaluation, we assume zero runtime for the association and forecasting module, and implement forecasting as post-processing of the detection outputs. One might wonder that a simulated post-processing run and an actual real-time parallel execution might have different final APs. We have also implemented the latter for verification purposes. For most settings the differences are within 1%. Although for some settings the difference can reach 3%, we find such fluctuation does not affect the relative rankings.", "publication_ref": ["b2", "b4", "b4", "b4", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Additional Details for Visual Tracking", "text": "For our tracking experiments (Section 4.4), we adapt and modify the state-ofthe-art multi-object tracker [2]. A component breakdown in Fig. H   The advantage of a visual tracker is that it runs faster than a detector and thus yields lower latency for streaming perception. The multi-object tracker used here is modified from [2]. It is mostly the same as a two-stage detector, except that its box head uses the last known object location as input in place of region proposals. Therefore, we get a computation saving by not running the RPN head. Runtime is measured for Mask R-CNN (ResNet 50) with input scale 0.5.", "publication_ref": ["b1", "b1"], "figure_ref": ["fig_18"], "table_ref": []}, {"heading": "B.4 Evaluation of Our Meta-Detector Streamer", "text": "Streamer is introduced in Section 4.3 in the main text. Given a detector and an input scale, Streamer automatically schedules the detector and employs forecasting to compensate for some of its latency. In the single GPU case, our dynamic schedule (Alg. 1) is used and in the infinite GPU case, idle-free scheduling (Fig. 4c) is used. Proper scheduling requires the knowledge of runtime of the algorithm, which is known in the case of benchmark evaluation. When applied in the wild, we can optionally track runtime of the algorithm on unseen data and adjust the scheduling accordingly. The forecasting module is implemented with asynchronous Kalman filter (Section B.2). Streamer has several key features. First, it enables synchronous processing for an asynchronous problem. Under the commonly studied settings (both offline and online), computation is synchronous in that the outputs and the inputs have a natural one-to-one correspondence. Therefore, many existing temporal reasoning models assume that the inputs are at a uniform rate and each input corresponds to an output [12,17,15]. In the real-time setting, however, such a relationship does not exist due to the latency of the algorithm, i.e., the number of outputs can be arbitrary. Streamer converts detectors with arbitrary runtimes into systems that output at a designated fixed rate. In short, it abstracts away the asynchronous Table J. Performance boost after applying Streamer. \"(B)\" standards for \"Before\", and \"(A)\" standards for \"After\". The evaluation setting is the same as Table 1  nature of the problem and therefore allows downstream synchronous processing. Second, by adopting forecasting, Streamer significantly boosts the performance of streaming perception. In Tables J and K, we evaluate the detection AP before and after applying our meta-detector. We observe relative improvement from 4% to 80% with an average of 33% in detection AP under 80 different settings (8 detectors \u00d7 5 image scales \u00d7 2 compute models). Note that the difference of this evaluation and benchmark evaluation in the main text is that we fix the detector and input scale here, while benchmark evaluation searches over the best configuration of detectors and input scales. For the infinite GPU settings, we discount the boost from additional compute itself.", "publication_ref": ["b11", "b16", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "B.5 Implementation Details", "text": "Detectors We experiment with a large suite of object detectors: SSD [27], RetinaNet [25], Mask R-CNN [18], Cascade Mask R-CNN [6], and HTC [8]. The \"optimized\" and \"re-optimized\" rows in all tables represent the optimal configuration over all detectors and all input scales of 0.2, 0.25, 0.5, 0.75, and 1.0. We adopt mmdetection codebase [9] (one of the fastest open-source implementation for Mask R-CNN) for object detectors. Note that for all detectors, the implementation has reproduced both the accuracy and runtime reported in the original papers.\nPotentially better implementation We acknowledge that there are additional bells and whistles to reduce runtime of object detectors, which might further improve the results on our benchmark. We focus on general techniques instead of device-or application-specific ones. For example, we have explored The scheduling is similar as with the Kalman filter case in that both are asynchronous. The difference is that linear forecasting does not explicitly maintain a state representation but only stores two latest detection results. Association takes place immediately after a new detection result becomes available, and it links the bounding boxes in two consecutive detection results and computes a velocity estimate. Forecasting takes place right before the next time step, and it uses linear extrapolation to produce an output as the estimation of the current world state. The equations represent the computation for reporting to benchmark query at t = 4. b is a simplified representation for object location. At this time, only detection results for frame 0 and 1 are available, but through association and forecasting, the algorithm can make a better prediction for the current world state.\nGPU image pre-processing, which is applicable to all GPUs. Another implementation technique is to use half-precision floating-point numbers (FP16), which we have not explored, since it will only pay off for certain GPUs that have been optimized for FP16 computation (it is reported that FP16 yields only marginal testing time speed boost on 1080 Ti [10]).", "publication_ref": ["b17", "b5", "b7", "b8", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "C Additional Baselines", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.1 Forecasting Baselines", "text": "We have also tested linear extrapolation (i.e., constant velocity) and quadratic extrapolation for forecasting detection results. We include an illustration of linear forecasting in Fig. I, and the quadratic counterpart is a straight-forward extension that involves three latest detection results. Though they produce inferior results than Kalman filter, we include the results in Table L for completeness. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.2 An End-to-End Baseline", "text": "In the main text, we break down the streaming detection task into detection, tracking, and forecasting for modular analysis. Alternatively, it is also possible to train a model that directly outputs detection results in the future. F2F [28] is one such model. Building upon Mask R-CNN, it does temporal reasoning and forecasting at the level of FPN feature maps. Note that no explicit tracking is performed. In this section, we compare against this end-to-end baseline in both offline and streaming settings.\nIn the offline setting, the algorithm is given s frames as input history, and outputs detection results for t frames ahead. This is the same as the evaluation in [28]. We set both s and t to be 3, as the optimal detector in our forecasting experiments (Table 2) has runtime of 2.78 frames. Since F2F forecasts at the FPN feature level, it is agnostic to second stage tasks. In our evaluation, we focus on the bounding box detection task instead of instance segmentation.  [7]). We implement our own version of F2F based on mmdetection (instead of Detectron as done in [28]). We train the model for 12 epochs end-to-end (a 50% longer schedule than combined stages in [28]). For a fair comparison, we also finetuned the detectors on Argoverse with the same pseudo ground truth. For Mask R-CNN ResNet 50 at scale 0.5, it boosts the offline box AP from 19.4 to 22.9. We use this finetuned detector in our method to compare against F2F.\nThe results are summarized in Table M. We see that an end-to-end solution does not immediately boost the performance. We believe that it is still an open problem on how to effectively replace tracking and forecasting with an end-to-end solution.\nIn the streaming setting, F2F can be viewed as a detector that compensates its own latency. The results are summarized in Table N. We see that F2F is too expensive compared with other streaming solutions, showing that forecasting can help only if it is fast under our evaluation. Note that the detectors (row 1-2) are not finetuned as in the offline case, which means that they can be further improved.\nTable N. Streaming evaluation for the end-to-end method F2F [28]. The setting is the same as the experiments in the main text. Rows 1 and 2 are the optimized detector and the Kalman filter forecasting solution from the main text. The underlying detectors used are Mask R-CNN ResNet 50 at scale 0.5 and scale 0.75 respectively. Row 3 suggests that F2F has a low streaming AP, due to its forecasting module being very expensive (last column, runtime in milliseconds). For diagnostics purpose, we assume F2F to run as fast as our optimized detector (row 4), and arm it with our scheduling algorithm (row 5). But even so, F2F still under-performs the simple Kalman filter solution ", "publication_ref": ["b6"], "figure_ref": [], "table_ref": ["tab_2", "tab_13"]}, {"heading": "", "text": "Acknowledgements: This work was supported by the CMU Argo AI Center for Autonomous Vehicle Research and was supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. HR001117C0051. Annotations for Argoverse-HD were provided by Scale AI.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "On hardware-in-the-loop simulation", "journal": "IEEE Conference on Decision and Control", "year": "2005", "authors": "M Bacic"}, {"ref_id": "b1", "title": "Tracking without bells and whistles", "journal": "ICCV", "year": "2019", "authors": "P Bergmann; T Meinhardt; L Leal-Taix\u00e9"}, {"ref_id": "b2", "title": "Simple online and realtime tracking", "journal": "ICIP", "year": "2016", "authors": "A Bewley; Z Ge; L Ott; F Ramos; B Upcroft"}, {"ref_id": "b3", "title": "Deliberation scheduling for problem solving in timeconstrained environments", "journal": "Artificial Intelligence", "year": "1994", "authors": "M Boddy; T L Dean"}, {"ref_id": "b4", "title": "Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age", "journal": "IEEE Transactions on Robotics", "year": "2016", "authors": "C Cadena; L Carlone; H Carrillo; Y Latif; D Scaramuzza; J Neira; I Reid; J J Leonard"}, {"ref_id": "b5", "title": "Cascade R-CNN: Delving into high quality object detection", "journal": "CVPR", "year": "2018", "authors": "Z Cai; N Vasconcelos"}, {"ref_id": "b6", "title": "Argoverse: 3D tracking and forecasting with rich maps", "journal": "CVPR", "year": "2019", "authors": "M F Chang; J W Lambert; P Sangkloy; J Singh; S Bak; A Hartnett; D Wang; P Carr; S Lucey; D Ramanan; J Hays"}, {"ref_id": "b7", "title": "Hybrid task cascade for instance segmentation", "journal": "CVPR", "year": "2019", "authors": "K Chen; J Pang; J Wang; Y Xiong; X Li; S Sun; W Feng; Z Liu; J Shi; W Ouyang; C C Loy; D Lin"}, {"ref_id": "b8", "title": "MMDetection: Open mmlab detection toolbox and benchmark", "journal": "", "year": "2019", "authors": "K Chen; J Wang; J Pang; Y Cao; Y Xiong; X Li; S Sun; W Feng; Z Liu; J Xu; Z Zhang; D Cheng; C Zhu; T Cheng; Q Zhao; B Li; X Lu; R Zhu; Y Wu; J Dai; J Wang; J Shi; W Ouyang; C C Loy; D Lin"}, {"ref_id": "b9", "title": "SimpleDet: A simple and versatile distributed framework for object detection and instance recognition", "journal": "JMLR", "year": "2019", "authors": "Y Chen; C Han; Y Li; Z Huang; Y Jiang; N Wang; Z Zhang"}, {"ref_id": "b10", "title": "CVPR19 tracking and detection challenge: How crowded can it get", "journal": "", "year": "2019", "authors": "P Dendorfer; H Rezatofighi; A Milan; J Shi; D Cremers; I Reid; S Roth; K Schindler; L Leal-Taix\u00e9"}, {"ref_id": "b11", "title": "Long-term recurrent convolutional networks for visual recognition and description", "journal": "CVPR", "year": "2015", "authors": "J Donahue; L A Hendricks; M Rohrbach; S Venugopalan; S Guadarrama; K Saenko; T Darrell"}, {"ref_id": "b12", "title": "The unmanned aerial vehicle benchmark: Object detection and tracking", "journal": "ECCV", "year": "2018", "authors": "D Du; Y Qi; H Yu; Y F Yang; K Duan; G Li; W Zhang; Q Huang; Q Tian"}, {"ref_id": "b13", "title": "Direct sparse odometry", "journal": "TPAMI", "year": "2017", "authors": "J Engel; V Koltun; D Cremers"}, {"ref_id": "b14", "title": "Slowfast networks for video recognition", "journal": "ICCV", "year": "2019", "authors": "C Feichtenhofer; H Fan; J Malik; K He"}, {"ref_id": "b15", "title": "Dynamic zoom-in network for fast object detection in large images", "journal": "CVPR", "year": "2018", "authors": "M Gao; R Yu; A Li; V I Morariu; L S Davis"}, {"ref_id": "b16", "title": "A better baseline for AVA", "journal": "", "year": "2018", "authors": "R Girdhar; J Carreira; C Doersch; A Zisserman"}, {"ref_id": "b17", "title": "", "journal": "Mask R-CNN", "year": "2017", "authors": "K He; G Gkioxari; P Doll\u00e1r; R B Girshick"}, {"ref_id": "b18", "title": "Computation and action under bounded resources", "journal": "", "year": "1990", "authors": "E J Horvitz"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig.2. Prior art routinely explores the trade-off between detection accuracy versus runtime. We generate the above plot by varying the input resolution of each detection network. We argue that such plots are exclusive to offline processing and fail to capture latencyaccuracy trade-offs in streaming perception. AP stands for average precision, and is a standard metric for object detection[26].", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 8 .8Fig. 8. Qualitative results. Video results can be found on the project website [Link].", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 9 .9Fig. 9. Generalization to instance segmentation. (a) The offline pseudo ground truth we adopt for evaluation is of high quality. (b) A similar latency pattern can be observed for instance segmentation as in object detection. (c) Forecasting for instance segmentation can be implemented as forecasting the bounding boxes and then warping the masks accordingly.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "20. Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: Efficient convolutional neural networks for mobile vision applications. ArXiv abs/1704.04861 (2017) 3 21. Kalman, R.E.: A new approach to linear filtering and prediction problems. Transactions of the ASME-Journal of Basic Engineering 82(Series D), 35-45 (1960) 11 22. Kosinski, R.J.: A literature review on reaction time. Clemson University 10 (2008) 1 23. Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pflugfelder, R.,\u010cehovin Zajc, L., Vojir, T., H\u00e4ger, G., Luke\u017ei\u010d, A., Eldesokey, A., Fernandez, G.: The visual object tracking VOT2017 challenge results (2017) 4 24. Li, M., Yumer, E., Ramanan, D.: Budgeted training: Rethinking deep neural network training under resource constraints. In: ICLR (2020) 20 25. Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017) 2, 3, 19, 37 26. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014) 4, 9, 20, 21 27. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C.Y., Berg, A.C.: SSD: Single shot multibox detector. In: ECCV (2016) 2, 3, 19, 37 28. Luc, P., Couprie, C., LeCun, Y., Verbeek, J.: Predicting future instance segmentations by forecasting convolutional features. In: ECCV (2018) 3, 38, 39 29. Lukezic, A., Voj\u00edr, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation filter with channel and spatial reliability. In: CVPR (2017) 6 30. Mao, H., Yang, X., Dally, W.J.: A delay metric for video object detection: What average precision fails to tell. In: ICCV (2019) 3 31. McLeod, P.: Visual reaction time and high-speed ball games. Perception (1987) 2 32. Mullapudi, R.T., Chen, S., Zhang, K., Ramanan, D., Fatahalian, K.: Online model distillation for efficient video inference. In: ICCV (2019) 14 33. Quigley, M., Conley, K., Gerkey, B., Faust, J., Foote, T., Leibs, J., Wheeler, R., Ng, A.Y.: ROS: an open-source robot operating system. In: ICRA workshop on open source software. Kobe, Japan (2009) 13 34. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: CVPR (2016) 2, 3, 19 35. Russell, S.J., Wefald, E.: Do the right thing: studies in limited rationality. MIT press (1991) 4 36. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo, J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H., Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J., Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo open dataset (2019) 21 37. Voigtlaender, P., Krause, M., Osep, A., Luiten, J., Sekar, B.B.G., Geiger, A., Leibe, B.: MOTS: Multi-object tracking and segmentation. In: CVPR (2019) 21 38. Wen, L., Du, D., Cai, Z., Lei, Z., Chang, M., Qi, H., Lim, J., Yang, M., Lyu, S.: DETRAC: A new benchmark and protocol for multi-object detection and tracking. arXiv abs/1511.04136 (2015) 21 39. Yang, L., Fan, Y., Xu, N.: Video instance segmentation. In: ICCV (2019) 7, 21 40. Zhang, X., Zhou, X., Lin, M., Sun, J.: ShuffleNet: An extremely efficient convolutional neural network for mobile devices. In: CVPR (2018) 3 41. Zilberstein, S.: Using anytime algorithms in intelligent systems. AI magazine (1996) 4, 13 42. Zilberstein, S., Mouaddib, A.I.: Optimal scheduling of progressive processing tasks. International Journal of Approximate Reasoning (2000) 4", "figure_data": ""}, {"figure_label": "A", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Fig. A .AFig. A.Runtime distribution for an object detector. Note that runtime is not constant, and this variance needs to be modeled in a simulation. This plot is obtained by running RetinaNet (ResNet 50) [25] on Argoverse 1.1[7] with input scale 0.5.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "2 62(Alg. 1) Cascade MRCNN R50 @ s0.5 14.0 28.8 9.9 1.0 26.8 12.2 60.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "FigFig. C. Mismatch is the same for the shrinking-tail policy with different runtime r1 and r2 as long as r1 = r2 , \u03c4 (r1) \u2265 0.5, and \u03c4 (r2) \u2265 0.5.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Fig. D.For r = 1.5, shrinking-tail achieves less cumulative mismatch than idle-free. Note that each policy has its own repeating period and shrinking-tail always achieves 1 less cumulative mismatch within each common period.", "figure_data": ""}, {"figure_label": "E", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Fig. E .EFig. E. A shrinking-tail execution block (orange) increases temporal mismatch.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "FigFig. F.Shrinking-tail cycle. Intuitively, blocks within each shrinking-tail cycle has tails increasing (\u03c41 < \u03c42 < \u03c43 and \u03c45 < \u03c46 < \u03c47). It ends when the tail decreases or there is an idle gap, and thus the tail \"shrinks\".", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_16", "figure_caption": "For idle-free, 0 \u2264 b < \u03c4 r , and for shrinking-tail, b = 0. Taking Figure F for example (\u03c4 r = 0.3), Cycle 1 has a = 3 and b = 0, Cycle 2 has a = 2 and b = \u03c4 4 , and Cycle 4 has a = 3 and b = 0.", "figure_data": ""}, {"figure_label": "6G", "figure_type": "figure", "figure_id": "fig_17", "figure_caption": "6 Fig. G .6GFig. G. The first cycles for both policies have different mismatch patterns.", "figure_data": ""}, {"figure_label": "H", "figure_type": "figure", "figure_id": "fig_18", "figure_caption": "Fig. H .HFig. H.Multi-object visual tracker. The advantage of a visual tracker is that it runs faster than a detector and thus yields lower latency for streaming perception. The multi-object tracker used here is modified from[2]. It is mostly the same as a two-stage detector, except that its box head uses the last known object location as input in place of region proposals. Therefore, we get a computation saving by not running the RPN head. Runtime is measured for Mask R-CNN (ResNet 50) with input scale 0.5.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_19", "figure_caption": "Fig. I.Scheduling for linear forecasting. The scheduling is similar as with the Kalman filter case in that both are asynchronous. The difference is that linear forecasting does not explicitly maintain a state representation but only stores two latest detection results. Association takes place immediately after a new detection result becomes available, and it links the bounding boxes in two consecutive detection results and computes a velocity estimate. Forecasting takes place right before the next time step, and it uses linear extrapolation to produce an output as the estimation of the current world state. The equations represent the computation for reporting to benchmark query at t = 4. b is a simplified representation for object location. At this time, only detection results for frame 0 and 1 are available, but through association and forecasting, the algorithm can make a better prediction for the current world state.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_20", "figure_caption": "24.3 7.9 1.0 25.1 10.1 56.7 2 + Scheduling (Alg. 1) + KF 17.8 33.3 16.3 3.2 35.2 16.5 92.7 3 F2F 6.2 11.1 3.4 0.8 13.1 5.2 321.6 4 F2F (Simulated Fast) 14.1 29.1 12.7 1.9 28.9 12.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Streaming perception with joint detection, visual tracking, and forecasting. We see that initially visual trackers do not outperform simple association (Table2) with the corresponding setting in the single GPU case. But that is reversed if the tracker can be optimized to run faster (2x) while maintaining the same accuracy (row 6). Such an assumption is not unreasonable given the fact that the tracker's job is as simple as updating locations of previously detected objects", "figure_data": "ID Method"}, {"figure_label": "A", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Summary of the experiments in the infinite GPU settings (in the main text) and the number of GPUs needed in practice to achieve this performance (i.e., the maximum number of concurrent jobs).Runtime-induced varianceAs mentioned in the previous section, runtime is stochastic and has a variance up to 11.1% (standard deviation normalized by mean). Fortunately, such a variance does not transfer to the variance of our streaming metric. Empirically, we found that the variance of streaming AP Dataset Annotation and Comparison Based on the publicly available video dataset Argoverse 1.1[7], we build our dataset with high-frame-rate annotations for streaming evaluation -Argoverse-HD (High-frame-rate Detection). One key feature is that the annotation follows MS COCO [26] standards, thus allowing direct evaluation of COCO pre-trained models on this self-driving vehicle dataset. The annotation is done at 30 FPS without any interpolation used. Unlike some self-driving vehicle datasets where only cars on the road are annotated [37], we also annotate background objects since they can potentially enter the drivable area. Of course, objects that are too small are omitted and our minimum size is 5 \u00d7 15 or 15 \u00d7 5 (based on the aspect ratio of the object). We outsourced the annotation job to Scale AI.", "figure_data": "In Table B,we compare our annotation with existing datasets: DETRAC [38], KITTI-MOTS[37], MOTS [37], UAVDT [13], Waymo [36], and Youtube-VIS [39].NameCamera SetupImage Res Image FPS Annot FPS Classes BoxesDETRACSurvelliance960 \u00d7 54030641.21MKITTI-MOTSEgo-Vehicle1242 \u00d7 3751010246KMOTS UAVDT identify the optimal hardware configuration Generic 1920 \u00d7 1080 This suggest that our simulation can also 30 30 2 30K UAV Survelliance 1080 \u00d7 540 30 30 1 842K Waymo Ego-Vehicle 1920 \u00d7 1280 10 10 4 11.8MYoutube-VISGeneric1280 \u00d7 72030640131KMethod Argoverse-HD (Ours)Ego-Vehicle1920 \u00d7 120030# of GPUs 308250KDet (Table 1, row 8)4Det + Associate + Forecast (Table 2, row 3)4Det + Visual Track (Table 3, row 4)9Det + Visual Track + Forecast (Table 3, row 5)9"}, {"figure_label": "C", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Instance segmentation overhead compared with object detection. This table lists runtimes of several methods with and without the mask head, and their differences are the extra cost which one has to pay for instance segmentation. All numbers are milliseconds except the scale column and the last column.", "figure_data": "The average overhead is17ms or 13%MethodScale w/o Mask w/ Mask Overhead Overhead0.234.341.47.121%0.2536.144.38.223%Mask R-CNN ResNet 500.556.765.68.816%0.7592.7101.08.39%1.0139.6147.78.16%0.238.446.47.921%0.2540.948.77.819%Mask R-CNN ResNet 1010.568.876.47.611%0.75119.7127.17.56%1.0183.8190.87.04%0.260.966.05.18%0.2559.269.19.917%Cascade MRCNN ResNet 500.580.095.415.319%0.75118.1133.815.713%1.0164.6181.917.310%0.266.471.04.67%0.2565.475.29.715%Cascade MRCNN ResNet 101 0.592.2106.614.416%0.75143.4159.215.811%1.0208.2225.116.98%"}, {"figure_label": "E", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Streaming evaluation for instance segmentation with forecasting. Despite that we only forecast boxes and warp masks accordingly, we still observe significant improvement from forecasting for mask AP. The optimized algorithm for row 1 is Mask R-CNN ResNet 50 @ s0.5, and for row 2 is Mask R-CNN ResNet 50 @ s0.75", "figure_data": "ID MethodAP APL APM APS AP50 AP751 Detection + Scheduling + Association + Forecasting 24.1 32.4 23.0 6.0 43.7 22.02 + Infinite GPUs29.2 30.7 30.2 11.4 53.0 26.7"}, {"figure_label": "H", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Streaming perception with joint detection, visual tracking, and forecasting on Tesla V100 (corresponding to Table3in the main text). We find the similar conclusions that visual tracking with forecasting does not outperform association with forecasting in the single GPU case and achieves comparable performance in the infinite GPU case", "figure_data": "MethodAP APL APM APS AP50 AP751 Detection + Scheduling + Association + Forecasting 18.2 42.7 16.1 1.1 30.9 17.72 + Re-optimize Detection19.6 33.0 19.2 5.3 38.5 17.93 + Infinite GPUs22.9 38.7 23.1 6.9 43.8 21.2ID MethodAP APL APM APS AP50 AP751 Detection + Visual Tracking12.6 21.5 9.0 2.2 27.1 10.52 + Forecasting18.0 34.7 16.8 3.2 36.0 16.43 + Infinite GPUs w/o Forecasting14.4 24.2 11.2 2.8 30.6 12.04 + Forecasting22.8 38.6 23.0 6.9 43.7 21.0"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "explains how this tracker works and why it has the potential to achieve better performance under the streaming setting.", "figure_data": "LatestObservationFPNRPN Head24% (19ms)Box Head30% (23ms)BoxFPN Feature Extraction46% (35ms)Last KnownUpdatedObject LocationsObject Locations(a) Multi-Object Tracker(b) Computation Saving"}, {"figure_label": "K", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "in the main text. This table assumes a single GPU, and an infinite GPU counterpart can be found in TableK. Under this setting, we observe significant improvement in AP, ranging from 5% to 78%, and averaging at 34% Performance boost after applying Streamer. \"(B)\" standards for \"Before\", and \"(A)\" standards for \"After\". The evaluation setting is the same as Table1in the main text. This table assumes infinite GPUs, and a single GPU counterpart can be found in TableJ. Under this setting, we observe significant improvement in AP, ranging from 4% to 80%, and averaging at 32%", "figure_data": "Method MethodScale AP(B) AP(A) Boost APL(B) APL(A) Boost Scale AP(B) AP(A) Boost APL(B) APL(A) Boost0.2 0.29.5 9.910.4 10.69% 7%23.5 25.528.6 29.421% 15%0.25 0.259.3 9.610.6 10.714% 12%23.9 24.931.5 31.732% 27%SSD SSD0.5 0.59.7 11.313.5 14.740% 30%20.0 24.132.4 35.462% 47%0.75 0.756.0 8.010.7 13.378% 66%11.5 14.619.8 25.672% 76%1.0 1.04.2 5.57.3 9.876% 80%7.3 10.012.5 16.572% 65%0.2 0.26.0 6.16.3 6.35% 4%18.1 18.621.3 21.317% 15%0.25 0.256.9 7.17.5 7.69% 8%19.8 21.426.2 27.133% 26%RetinaNet R50 RetinaNet R500.5 0.510.9 12.314.2 14.730% 20%24.1 28.138.3 40.159% 42%0.75 0.7510.8 13.116.1 18.050% 37%20.2 24.332.9 37.863% 56%1.0 1.09.9 11.714.1 17.342% 48%16.7 19.524.7 31.348% 60%0.2 0.25.4 5.55.9 6.09% 9%14.7 15.319.8 20.135% 32%0.25 0.256.5 6.77.4 7.514% 12%18.2 18.825.8 26.142% 38%RetinaNet R101 RetinaNet R1010.5 0.59.9 11.313.0 14.031% 24%21.5 25.333.6 38.156% 50%0.75 0.759.9 11.814.5 17.047% 44%18.1 21.327.7 34.353% 61%1.0 1.08.9 10.812.7 16.342% 51%14.7 18.222.0 28.250% 55%0.2 0.26.5 6.77.2 7.411% 10%18.0 20.025.1 26.240% 31%0.25 0.257.7 7.89.1 9.219% 17%20.1 20.829.9 30.149% 45%Mask R-CNN R50 Mask R-CNN R500.5 0.512.0 13.916.7 17.439% 26%24.3 29.039.9 42.664% 47%0.75 0.7511.5 14.417.8 20.354% 40%19.5 24.333.3 38.571% 59%1.0 1.010.6 12.415.0 18.742% 51%16.6 19.425.0 31.450% 62%0.2 0.26.3 6.57.2 7.314% 13%16.7 17.424.1 24.345% 40%0.25 0.257.6 7.99.0 9.117% 15%19.3 20.528.5 28.948% 41%Mask R-CNN R101 Mask R-CNN R1010.5 0.511.0 11.915.2 16.239% 36%21.6 23.735.4 38.464% 62%0.75 0.7510.0 12.415.3 18.552% 49%16.8 20.328.0 35.367% 74%1.0 1.08.8 10.612.4 16.242% 53%13.7 16.921.2 27.755% 64%0.2 0.26.2 7.07.8 7.925% 13%15.4 18.925.5 26.566% 40%0.25 0.257.5 8.59.6 9.928% 16%18.4 22.330.1 31.763% 42%Cascade MRCNN R50 Cascade MRCNN R500.5 0.511.3 12.916.4 17.645% 37%22.6 26.037.5 41.266% 58%0.75 0.7510.9 13.216.7 19.954% 51%18.6 22.129.8 36.560% 65%1.0 1.010.1 12.615.7 19.855% 57%15.4 19.025.3 31.864% 67%0.2 0.26.1 6.87.3 7.920% 17%15.2 17.823.1 26.652% 49%0.25 0.257.4 8.39.5 9.828% 18%17.6 21.029.6 31.769% 50%Cascade MRCNN R101 0.5 Cascade MRCNN R101 0.510.3 12.615.4 17.049% 35%20.5 25.034.1 38.566% 54%0.75 0.759.5 11.414.7 17.754% 56%16.1 19.026.1 32.762% 72%1.0 1.08.8 10.512.9 16.646% 59%13.7 16.721.8 27.659% 65%0.2 0.25.6 6.36.8 8.022% 27%12.0 14.017.0 21.842% 55%0.25 0.256.3 7.38.3 9.831% 34%13.0 15.719.8 25.553% 62%HTC HTC0.5 0.57.9 9.210.8 13.738% 50%13.3 16.319.9 26.949% 65%0.75 0.757.1 8.28.6 11.422% 39%11.4 13.214.8 20.530% 55%1.0 1.06.4 7.47.2 9.312% 25%9.6 11.111.4 15.818% 43%"}, {"figure_label": "L", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Comparison of different forecasting methods for streaming perception. We see that both linear and Kalman filter forecasting methods significantly improve the streaming performance. Kalman filter further outperforms the linear forecasting. The quadratic forecasting decreases the AP, suggesting that high-order extrapolation is not suitable for this task. The detection used here is Mask R-CNN ResNet 50 @ s0.5 with dynamic scheduling (Alg. 1)", "figure_data": "ID MethodAP APL APM APS AP50 AP751 No Forecasting13.0 26.6 9.21.1 26.8 11.12 Linear (constant velocity) 15.7 38.1 13.8 1.1 30.2 14.83 Quadratic9.7 23.8 6.60.4 21.47.94 Kalman filter16.7 39.8 14.9 1.2 31.2 16.0"}, {"figure_label": "M", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Standard offline forecasting evaluation for the end-to-end method F2F[28]. The goal is to forecast 3 frames into the future. Surprisingly, the more expensive F2F method performs worse than the simpler Kalman filter in terms of the overall APID Method AP APL APM APS AP50 AP75 1 None (copy last) 13.4 24.3 10.9 1.9 27.9 11.3 2 Linear 16.3 34.8 16.8 1.8 32.9 14.3 3 Kalman filter 19.1 40.3 19.8 2.6 35.8 17.7 4 F2F 18.3 41.0 20.0 2.5 33.9 17.1 Also, we conduct experiments on Argoverse-HD, consistent with the setting in our other experiments. Due to a lack of annotation, we adopt pseudo ground truth (Section A.2) for training (data from the original training set of Argoverse 1.1", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "A B = 1 = 2 Computation", "formula_coordinates": [2.0, 125.4, 66.08, 170.68, 99.29]}, {"formula_id": "formula_1", "formula_text": "{(x i , t i )|t i \u2264 t} [accessible input at time t] (1)", "formula_coordinates": [5.0, 108.0, 405.84, 271.84, 9.65]}, {"formula_id": "formula_2", "formula_text": "{(\u0177 j , s j )} N j=1 [all outputs by f ] (2)", "formula_coordinates": [5.0, 108.11, 470.76, 271.73, 12.69]}, {"formula_id": "formula_3", "formula_text": "\u03d5(t) = arg max j s j < t [real-time constraint] (3)", "formula_coordinates": [5.0, 106.11, 563.73, 273.73, 16.37]}, {"formula_id": "formula_4", "formula_text": "L streaming = L({(y i ,\u0177 \u03d5(ti) )} T i=1 ) [evaluation] (4)", "formula_coordinates": [6.0, 100.09, 81.06, 279.75, 12.69]}, {"formula_id": "formula_5", "formula_text": "\u03c4 (t) = t \u2212 t 3: return [\u03c4 (s + r) < \u03c4 (s)] (Iverson bracket)", "formula_coordinates": [11.0, 37.91, 262.23, 182.33, 18.82]}, {"formula_id": "formula_6", "formula_text": "ID", "formula_coordinates": [25.0, 38.17, 173.91, 9.63, 7.31]}, {"formula_id": "formula_7", "formula_text": "0 1 2 3 5 4 Input & Execution 6 ( 0 ) ( 1 )( 3 )", "formula_coordinates": [26.0, 95.33, 369.16, 216.39, 42.95]}, {"formula_id": "formula_8", "formula_text": "( 1 ) ( 0 ) ( 0 ) ( 1 ) \u2205 \u2205 Input Index For Each Query 0 0 3 1 1 \u2205 \u2205 Fig. B.", "formula_coordinates": [26.0, 34.02, 426.25, 274.79, 121.47]}, {"formula_id": "formula_10", "formula_text": "0 0 + 2 0 + 3 0 + 4 1 Waiting: True ( 0 +1 ) (( 0 )", "formula_coordinates": [29.0, 98.67, 299.1, 177.09, 95.07]}, {"formula_id": "formula_12", "formula_text": "0 0 + 1 0 Input Index 0 0 + 2 0 Difference in Mismatch 0 1 0 0 + 5", "formula_coordinates": [29.0, 99.02, 299.24, 205.78, 149.03]}, {"formula_id": "formula_13", "formula_text": "(c) st <\u03b4 (c)", "formula_coordinates": [30.0, 167.82, 459.18, 38.54, 13.74]}, {"formula_id": "formula_14", "formula_text": "c 1 = a 0 + 2, c 2 = a 0 + 1, c 3 = a 0 + 1. (6", "formula_coordinates": [31.0, 66.38, 225.71, 504.38, 9.65]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [31.0, 570.76, 225.71, 101.82, 8.74]}, {"formula_id": "formula_16", "formula_text": "\u03b4 (c1) if = 2 a 0 + 2 + 2,\u03b4 (c2) if = 2 a 0 + 1 + 2,\u03b4 (c3) st = 1 a 0 + 1 + 2. (7", "formula_coordinates": [31.0, 56.19, 239.71, 319.41, 23.22]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [31.0, 375.6, 246.45, 4.24, 8.74]}, {"formula_id": "formula_18", "formula_text": "(c3) st <\u03b4 (c1) if <\u03b4 (c2) if(8)", "formula_coordinates": [31.0, 169.61, 291.32, 210.24, 14.3]}, {"formula_id": "formula_19", "formula_text": "T = m 1 c 1 + m 2 c 2 + d if (9) T = m 3 c 3 + d st (10", "formula_coordinates": [32.0, 122.05, 119.19, 417.89, 24.6]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [32.0, 539.94, 134.13, 59.27, 8.74]}, {"formula_id": "formula_21", "formula_text": "\u03b4 if = (m 1 c 1\u03b4 (c1) if + m 2 c 2\u03b4 (c2) if \u2212 6 + e if )/T (11", "formula_coordinates": [32.0, 119.6, 148.98, 421.49, 14.3]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [32.0, 541.1, 152.11, 59.66, 8.74]}, {"formula_id": "formula_23", "formula_text": "\u03b4 st = (m 3 c 3\u03b4 (c3) st \u2212 5 + e st )/T (12", "formula_coordinates": [32.0, 118.06, 166.95, 257.35, 13.74]}, {"formula_id": "formula_24", "formula_text": ")", "formula_coordinates": [32.0, 375.41, 170.09, 4.43, 8.74]}, {"formula_id": "formula_25", "formula_text": "F k = I 4\u00d74 \u2206t k I 4\u00d74 I 4\u00d74(", "formula_coordinates": [33.0, 160.17, 339.27, 206.39, 21.64]}, {"formula_id": "formula_26", "formula_text": "Q k = \u2206t 2 k I 8\u00d78(14)", "formula_coordinates": [33.0, 175.63, 400.73, 204.22, 12.69]}], "doi": ""}