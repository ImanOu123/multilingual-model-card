{"title": "Competitive Distribution Estimation: Why is Good-Turing Good", "authors": "Alon Orlitsky; Ananda Theertha Suresh", "pub_date": "", "abstract": "Estimating distributions over large alphabets is a fundamental machine-learning tenet. Yet no method is known to estimate all distributions well. For example, add-constant estimators are nearly min-max optimal but often perform poorly in practice, and practical estimators such as absolute discounting, Jelinek-Mercer, and Good-Turing are not known to be near optimal for essentially any distribution. We describe the first universally near-optimal probability estimators. For every discrete distribution, they are provably nearly the best in the following two competitive ways. First they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the distribution up to a permutation. Second, they estimate every distribution nearly as well as the best estimator designed with prior knowledge of the exact distribution, but as all natural estimators, restricted to assign the same probability to all symbols appearing the same number of times. Specifically, for distributions over k symbols and n samples, we show that for both comparisons, a simple variant of Good-Turing estimator is always within KL divergence of (3 + o n (1))/n 1/3 from the best estimator, and that a more involved estimator is within\u00d5 n (min(k/n, 1/ \u221a n)). Conversely, we show that any estimator must have a KL divergence at least\u03a9 n (min(k/n, 1/n 2/3 )) over the best estimator for the first comparison, and at least\u03a9 n (min(k/n, 1/ \u221a n)) for the second.", "sections": [{"heading": "", "text": "1 Introduction", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "Many learning applications, ranging from language-processing staples such as speech recognition and machine translation to biological studies in virology and bioinformatics, call for estimating large discrete distributions from their samples. Probability estimation over large alphabets has therefore long been the subject of extensive research, both by practitioners deriving practical estimators [1,2], and by theorists searching for optimal estimators [3].\nYet even after all this work, provably-optimal estimators remain elusive. The add-constant estimators frequently analyzed by theoreticians are nearly min-max optimal, yet perform poorly for many practical distributions, while common practical estimators, such as absolute discounting [4], Jelinek-Mercer [5], and Good-Turing [6], are not well understood and lack provable performance guarantees.\nTo understand the terminology and approach a solution we need a few definitions. The performance of an estimator q for an underlying distribution p is typically evaluated in terms of the Kullback-Leibler (KL) divergence [7],\nD(p||q) def = x p x log p x q x ,\nreflecting the expected increase in the ambiguity about the outcome of p when it is approximated by q. KL divergence is also the increase in the number of bits over the entropy that q uses to compress the output of p, and is also the log-loss of estimating p by q. It is therefore of interest to construct estimators that approximate a large class of distributions to within small KL divergence. We now describe one of the problem's simplest formulations.", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "Min-max loss", "text": "A distribution estimator over a support set X associates with any observed sample sequence x * \u2208 X * a distribution q(x * ) over X . Given n samples X n def = X 1 , X 2 , . . . , X n , generated independently according to a distribution p over X , the expected KL loss of q is r n (q, p) = E X n \u223cp n [D(p||q(X n ))].\nLet P be a known collection of distributions over a discrete set X . The worst-case loss of an estimator q over all distributions in P is r n (q, P)\ndef = max p\u2208P r n (q, p),(1)\nand the lowest worst-case loss for P, achieved by the best estimator, is the min-max loss \nMin-max performance can be viewed as regret relative to an oracle that knows the underlying distribution. Hence from here on we refer to it as regret.\nThe most natural and important collection of distributions, and the one we study here, is the set of all discrete distributions over an alphabet of some size k, which without loss of generality we assume to be [k] = {1, 2, . . . k}. Hence the set of all distributions is the simplex in k dimensions, \u2206 k def = {(p 1 , . . . , p k ) : p i \u2265 0 and p i = 1}. Following [8], researchers have studied r n (\u2206 k ) and related quantities, for example see [9]. We outline some of the results derived.", "publication_ref": ["b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Add-constant estimators", "text": "The add-\u03b2 estimator assigns to a symbol that appeared t times a probability proportional to t+\u03b2. For example, if three coin tosses yield one heads and two tails, the add-1/2 estimator assigns probability 1.5/(1.5 + 2.5) = 3/8 to heads, and 2.5/(1.5 + 2.5) = 5/8 to tails. [10] showed that as for every k, as n \u2192 \u221e, an estimator related to add-3/4 is near optimal and achieves\nr n (\u2206 k ) = k \u2212 1 2n \u2022 (1 + o(1)).(3)\nThe more challenging, and practical, regime is where the sample size n is not overwhelmingly larger than the alphabet size k. For example in English text processing, we need to estimate the distribution of words following a context. But the number of times a context appears in a corpus may not be much larger than the vocabulary size. Several results are known for other regimes as well. When the sample size n is linear in the alphabet size k, r n (\u2206 k ) can be shown to be a constant, and [3] showed that as k/n \u2192 \u221e, add-constant estimators achieve the optimal r n (\u2206 k ) = log\nk n \u2022 (1 + o(1)),(4)\nWhile add-constant estimators are nearly min-max optimal, the distributions attaining the min-max regret are near uniform. In practice, large-alphabet distributions are rarely uniform, and instead, tend to follow a power-law. For these distributions, add-constant estimators under-perform the estimators described in the next subsection.", "publication_ref": ["b9", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Practical estimators", "text": "For real applications, practitioners tend to use more sophisticated estimators, with better empirical performance. These include the Jelinek-Mercer estimator that cross-validates the sample to find the best fit for the observed data. Or the absolute-discounting estimators that rather than add a positive constant to each count, do the opposite, and subtract a positive constant.\nPerhaps the most popular and enduring have been the Good-Turing estimator [6] and some of its variations. Let n x def = n x (x n ) be the number of times a symbol x appears in x n and let \u03d5 t def = \u03d5 t (x n ) be the number of symbols appearing t times in x n . The basic Good-Turing estimator posits that if n\nx = t, q x (x n ) = \u03d5 t+1 \u03d5 t \u2022 t + 1 n ,\nsurprisingly relating the probability of an element not just to the number of times it was observed, but also to the number other elements appearing as many, and one more, times. It is easy to see that this basic version of the estimator may not work well, as for example it assigns any element appearing \u2265 n/2 times 0 probability. Hence in practice the estimator is modified, for example, using empirical frequency to elements appearing many times.\nThe Good-Turing Estimator was published in 1953, and quickly adapted for language-modeling use, but for half a century no proofs of its performance were known. Following [11], several papers, e.g., [12,13], showed that Good-Turing variants estimate the combined probability of symbols appearing any given number of times with accuracy that does not depend on the alphabet size, and [14] showed that a different variation of Good-Turing similarly estimates the probabilities of each previously-observed symbol, and all unseen symbols combined.\nHowever, these results do not explain why Good-Turing estimators work well for the actual probability estimation problem, that of estimating the probability of each element, not of the combination of elements appearing a certain number of times. To define and derive uniformly-optimal estimators, we take a different, competitive, approach.", "publication_ref": ["b5", "b10", "b11", "b12", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "Competitive optimality 2.1 Overview", "text": "To evaluate an estimator, we compare its performance to the best possible performance of two estimators designed with some prior knowledge of the underlying distribution. The first estimator is designed with knowledge of the underlying distribution up to a permutation of the probabilities, namely knowledge of the probability multiset, e.g., {.5, .3, .2}, but not of the association between probabilities and symbols. The second estimator is designed with exact knowledge of the distribution, but like all natural estimators, forced to assign the same probabilities to symbols appearing the same number of times. For example, upon observing the sample a, b, c, a, b, d, e, the estimator must assign the same probability to a and b, and the same probability to c, d, and e.\nThese estimators cannot be implemented in practice as in reality we do not have prior knowledge of the estimated distribution. But the prior information is chosen to allow us to determine the best performance of any estimator designed with that information, which in turn is better than the performance of any data-driven estimator designed without prior information. We then show that certain variations of the Good-Turing estimators, designed without any prior knowledge, approach the performance of both prior-knowledge estimators for every underlying distribution.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Competing with near full information", "text": "We first define the performance of an oracle-aided estimator, designed with some knowledge of the underlying distribution. Suppose that the estimator is designed with the aid of an oracle that knows the value of f (p) for some given function f over the class \u2206 k of distributions.\nThe function f partitions \u2206 k into subsets, each corresponding to one possible value of f . We denote the subsets by P , and the partition by P, and as before, denote the individual distributions by p.\nThen the oracle knows the unique partition part P such that p \u2208 P \u2208 P. For example, if f (p) is the multiset of p, then each subset P corresponds to set of distributions with the same probability multiset, and the oracle knows the multiset of probabilities.\nFor every partition part P \u2208 P, an estimator q incurs the worst-case regret in (1), r n (q, P ) = max p\u2208P r n (q, p).\nThe oracle, knowing the unique partition part P , incurs the least worst-case regret (2), r n (P ) = min q r n (q, P ).\nThe competitive regret of q over the oracle, for all distributions in P is r n (q, P ) \u2212 r n (P ),\nthe competitive regret over all partition parts and all distributions in each is\nr P n (q, \u2206 k ) def = max P \u2208P (r n (q, P ) \u2212 r n (P )) ,\nand the best possible competitive regret is\nr P n (\u2206 k ) def = min q r P n (q, \u2206 k ).\nConsolidating the intermediate definitions,\nr P n (\u2206 k ) = min q max P \u2208P max p\u2208P r n (q, p) \u2212 r n (P ) .\nNamely, an oracle-aided estimator who knows the partition part incurs a worst-case regret r n (P ) over each part P , and the competitive regret r P n (\u2206 k ) of data-driven estimators is the least overall increase in the part-wise regret due to not knowing P . In Appendix A.1, we give few examples of such partitions.\nA partition P refines a partition P if every part in P is partitioned by some parts in P . For example {{a, b}, {c}, {d, e}} refines {{a, b, c}, {d, e}}. In Appendix A.2, we show that if P refines P then for every q r\nP n (q, \u2206 k ) \u2265 r P n (q, \u2206 k ).(5)\nConsidering the collection \u2206 k of all distributions over [k], it follows that as we start with single-part partition {\u2206 k } and keep refining it till the oracle knows p, the competitive regret of estimators will increase from 0 to r n (q, \u2206 k ). A natural question is therefore how much information can the oracle have and still keep the competitive regret low? We show that the oracle can know the distribution exactly up to permutation, and still the regret will be very small.\nTwo distributions p and p permutation equivalent if for some permutation \u03c3 of [k],\np \u03c3(i) = p i ,\nfor all 1 \u2264 i \u2264 k. For example, (0.5, 0.3, 0.2) and (0.3, 0.5, 0.2) are permutation equivalent. Permutation equivalence is clearly an equivalence relation, and hence partitions the collection of distributions over [k] into equivalence classes. Let P \u03c3 be the corresponding partition. We construct estimators q that uniformly bound r P\u03c3 n (q, \u2206 k ), thus the same estimator uniformly bounds r P n (q, \u2206 k ) for any coarser partition of \u2206 k , such as partitions into classes of distributions with the same support size, or entropy. Note that the partition P \u03c3 corresponds to knowing the underlying distribution up to permutation, hence r P\u03c3 n (\u2206 k ) is the additional KL loss compared to an estimator designed with knowledge of the underlying distribution up to permutation. This notion of competitiveness has appeared in several contexts. In data compression it is called twice-redundancy [15,16,17,18], while in statistics it is often called adaptive or local minmax [19,20,21,22,23], and recently in property testing it is referred as competitive [24,25,26] or instance-by-instance [27]. Subsequent to this work, [28] studied competitive estimation in 1 distance, however their regret is poly(1/ log n), compared to our\u00d5(1/ \u221a n).", "publication_ref": ["b14", "b15", "b16", "b17", "b18", "b19", "b20", "b21", "b22", "b23", "b24", "b25", "b26", "b27"], "figure_ref": [], "table_ref": []}, {"heading": "Competing with natural estimators", "text": "Our second comparison is with an estimator designed with exact knowledge of p, but forced to be natural, namely, to assign the same probability to all symbols appearing the same number of times in the sample. For example, for the observed sample a, b, c, a, b, d, e, the same probability must be assigned to a and b, and the same probability to c, d, and e. Since data-driven estimators derive all their knowledge of the distribution from the data, we expect them to be natural.\nWe compare the regret of data-driven estimators to that of natural oracle-aided estimators. Let Q nat be the set of all natural estimators. For a distribution p, the lowest regret of a natural estimator, designed with prior knowledge of p is r nat n (p) def = min q\u2208Q nat r n (q, p), and the regret of an estimator q relative to the least-regret natural-estimator is\nr nat n (q, p) = r n (q, p) \u2212 r nat n (p). Thus the regret of an estimator q over all distributions in \u2206 k is r nat n (q, \u2206 k ) = max p\u2208\u2206 k r nat n (q, p),\nand the best possible competitive regret is r nat n (\u2206 k ) = min q r nat n (q, \u2206 k ). In the next section we state the results, showing in particular that r nat n (\u2206 k ) is uniformly bounded. In Section 5, we outline the proofs, and in Section 4 we describe experiments comparing the performance of competitive estimators to that of min-max motivated estimators.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Good-Turing estimators are often used in conjunction with empirical frequency, where Good-Turing estimates low probabilities and empirical frequency estimates large probabilities. We first show that even this simple Good-Turing version, defined in Appendix C and denoted q , is uniformly optimal for all distributions. For simplicity we prove the result when the number of samples is n \u223c poi(n), a Poisson random variable with mean n. Let r P\u03c3 poi(n) (q , \u2206 k ) and r nat poi(n) (q , \u2206 k ) be the regrets in this sampling process. A similar result holds with exactly n samples, but the proof is more involved as the multiplicities are dependent. Theorem 1 (Appendix C). For any k and n,\nr P\u03c3 poi(n) (q , \u2206 k ) \u2264 r nat poi(n) (q , \u2206 k ) \u2264 3 + o n (1) n 1/3 .\nFurthermore, a lower bound in [13] shows that this bound is optimal up to logarithmic factors.\nA more complex variant of Good-Turing, denoted q , was proposed in [13]. We show that its regret diminishes uniformly in both the partial-information and natural-estimator formulations. Theorem 2 (Section 5). For any k and n, r\nP\u03c3 n (q , \u2206 k ) \u2264 r nat n (q , \u2206 k ) \u2264\u00d5 n min 1 \u221a n , k n .\nWhere\u00d5 n , and below also\u03a9 n , hide multiplicative logarithmic factors in n. Lemma 6 in Section 5 and a lower bound in [13] can be combined to prove a matching lower bound on the competitive regret of any estimator for the second formulation,\nr nat n (\u2206 k ) \u2265\u03a9 n min 1 \u221a n , k n .\nHence q has near-optimal competitive regret relative to natural estimators.\nFano's inequality usually yields lower bounds on KL loss, not regret. By carefully constructing distribution classes, we lower bound the competitive regret relative to the oracle-aided estimators. Theorem 3 (Appendix D). For any k and n, r\nP\u03c3 n (\u2206 k ) \u2265\u03a9 n min 1 n 2/3 , k n .", "publication_ref": ["b12", "b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Illustration and implications", "text": "Figure 1 demonstrates some of the results. The horizontal axis reflects the set \u2206 k of distributions illustrated on one dimension. The vertical axis indicates the KL loss, or absolute regret, for clarity, shown for k n. The blue line is the previously-known min-max upper bound on the regret, which by ( 4) is very high for this regime, log(k/n). The red line is the regret of the estimator designed with prior knowledge of the probability multiset. Observe that while for some probability multisets the regret approaches the log(k/n) min-max upper bound, for other probability multisets it is much lower, and for some, such as uniform over 1 or over k symbols, where the probability multiset determines the distribution it is even 0. For many practically relevant distributions, such as power-law distributions and sparse distributions, the regret is small compared to log(k/n). The green line is an upper bound on the absolute regret of the data-driven estimator q . By Theorem 2, it is always at most 1/ \u221a n larger than the red line. It follows that for many distributions, possibly for distributions with more structure, such as those occurring in nature, the regret of q is significantly smaller than the pessimistic min-max bound implies. We observe a few consequences of these results.\nr n (\u2206 k ) = log k n Uniform distribution KL loss Distributions \u2264\u00d5 min( 1 \u221a n , k n\n\u2022 Theorems 1 and 2 establish two uniformly-optimal estimators q and q . Their relative regrets diminish to zero at least as fast as 1/n 1/3 , and 1/ \u221a n respectively, independent of how large the alphabet size k is.\n\u2022 Although the results are for relative regret, as shown in Figure 1, they lead to estimator with smaller absolute regret, namely, the expected KL divergence. \u2022 The same regret upper bounds hold for all coarser partitions of \u2206 k i.e., where instead of knowing the multiset, the oracle knows some property of multiset such as entropy.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Experiments", "text": "Recall that for a sequence x n , n x denotes the number of times a symbol x appears and \u03d5 t denotes the number of symbols appearing t times. For small values of n and k, the estimator proposed in [13] simplifies to a combination of Good-Turing and empirical estimators. By [13, Lemmas 10 and 11], for symbols appearing t times, if \u03d5 t+1 \u2265\u03a9(t), then the Good-Turing estimate is close to the underlying total probability mass, otherwise the empirical estimate is closer. Hence, for a symbol appearing t times, if \u03d5 t+1 \u2265 t we use the Good-Turing estimator, otherwise we use the empirical estimator. If n x = t,\nq x = t N if t > \u03d5 t+1 , \u03d5t+1+1 \u03d5t \u2022 t+1 N else,\nwhere N is a normalization factor. Note that we have replaced \u03d5 t+1 in the Good-Turing estimator by \u03d5 t+1 + 1 to ensure that every symbol is assigned a non-zero probability.  We compare the performance of this estimator to four estimators: three popular add-\u03b2 estimators and the optimal natural estimator. An add-beta estimator\u015c has the form\nq\u015c x = n x + \u03b2\u015c nx N (\u015c) ,\nwhere N (\u015c) is a normalization factor to ensure that the probabilities add up to 1. The Laplace estimator, \u03b2 L t = 1 \u2200 t, minimizes the expected loss when the underlying distribution is generated by a uniform prior over \u2206 k . The Krichevsky-Trofimov estimator, \u03b2 KT t = 1/2 \u2200 t, is asymptotically min-max optimal for the cumulative regret, and minimizes the expected loss when the underlying distribution is generated according to a Dirichlet-1/2 prior. The Braess-Sauer estimator,\n\u03b2 BS 0 = 1/2, \u03b2 BS 1 = 1, \u03b2 BS t = 3/4 \u2200 t > 1\n, is asymptotically min-max optimal for r n (\u2206 k ). Finally, as shown in Lemma 10, the optimal estimator q x = Sn x \u03d5n x achieves the lowest loss of any natural estimator designed with knowledge of the underlying distribution.\nWe compare the performance of the proposed estimator to that of the four estimators above. We consider six distributions: uniform distribution, step distribution with half the symbols having probability 1/2k and the other half have probability 3/2k, Zipf distribution with parameter 1 (p i \u221d i \u22121 ), Zipf distribution with parameter 1.5 (p i \u221d i \u22121.5 ), a distribution generated by the uniform prior on \u2206 k , and a distribution generated from Dirichlet-1/2 prior. All distributions have support size k = 10000. n ranges from 1000 to 50000 and the results are averaged over 200 trials.\nFigure 2 shows the results. Observe that the proposed estimator performs similarly to the best natural estimator for all six distributions. It also significantly outperforms the other estimators for Zipf, uniform, and step distributions.\nThe performance of other estimators depends on the underlying distribution. For example, since Laplace is the optimal estimator when the underlying distribution is generated from the uniform prior, it performs well in Figure 2(e), however performs poorly on other distributions. Furthermore, even though for distributions generated by Dirichlet priors, all the estimators have similar looking regrets (Figures 2(e), 2(f)), the proposed estimator performs better than estimators which are not designed specifically for that prior.", "publication_ref": ["b12"], "figure_ref": ["fig_3", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Proof sketch of Theorem 2", "text": "The proof consists of two parts. We first show that for every estimator q, r P\u03c3 n (q, \u2206 k ) \u2264 r nat n (q, \u2206 k ) and then upper bound r nat n (q, \u2206 k ) using results on combined probability mass. Lemma 4 (Appendix B.1). For every estimator q, r\nP\u03c3 n (q, \u2206 k ) \u2264 r nat n (q, \u2206 k ).\nThe proof of the above lemma relies on showing that the optimal estimator for every class in P \u2208 P \u03c3 is natural.\n5.1 Relation between r nat n (q, \u2206 k ) and combined probability estimation\nWe now relate the regret in estimating distribution to that of estimating the combined or total probability mass, defined as follows. Recall that \u03d5 t denotes the number of symbols appearing t times.\nFor a sequence x n , let S t def = S t (x n ) denote the total probability of symbols appearing t times. For notational convenience, we use S t to denote both S t (x n ) and S t (X n ) and the usage becomes clear in the context. Similar to KL divergence between distributions, we define KL divergence between S and their estimates\u015c as\nD(S||\u015c) = n t=0 S t log S t S t .\nSince the natural estimator assigns same probability to symbols that appear the same number of times, estimating probabilities is same as estimating the total probability of symbols appearing a given number of times. We formalize it in the next lemma.\nLemma 5 (Appendix B.2). For a natural estimator q let\u015c t (x n ) = x:nx=t q x (x n ), then r nat n (q, p) = E[D(S||\u015c)].\nIn Lemma 11(Appendix B.3), we show that there is a natural estimator that achieves r nat n (\u2206 k ). Taking maximum over all distributions p and minimum over all estimators q results in Lemma 6. For a natural estimator q let\u015c t (x n ) = x:nx=t q x (x n ), then\nr nat n (q, \u2206 k ) = max p\u2208\u2206 k E[D(S||\u015c)]. Furthermore, r nat n (\u2206 k ) = min S max p\u2208\u2206 k E[D(S||\u015c)].\nThus finding the best competitive natural estimator is same as finding the best estimator for the combined probability mass S. [13] proposed an algorithm for estimating S such that for all k and for all p \u2208 \u2206 k , with\nprobability \u2265 1 \u2212 1/n , D(S||\u015c) =\u00d5 n 1 \u221a n .\nThe result is stated in Theorem 2 of [13]. One can convert this result to a result on expectation easily using the property that their estimator is bounded below by 1/2n and show that\nmax p\u2208\u2206 k E[D(S||\u015c)] =\u00d5 n 1 \u221a n .\nA slight modification of their proofs for Lemma 17 and Theorem 2 in their paper using\nn t=1 \u221a \u03d5 t \u2264 n t=1 \u03d5 t \u2264 k\nshows that their estimator\u015c for the combined probability mass S satisfies\nmax p\u2208\u2206 k E[D(S||\u015c)] =\u00d5 n min 1 \u221a n , k n .\nThe above equation together with Lemmas 4 and 6 results in Theorem 2.", "publication_ref": ["b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We thank Jayadev Acharya, Moein Falahatgar, Paul Ginsparg, Ashkan Jafarpour, Mesrob Ohannessian, Venkatadheeraj Pichapati, Yihong Wu, and the anonymous reviewers for helpful comments.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A Proofs for competitive formulation A.1 Examples of partitions", "text": "The following examples evaluate r P n (\u2206 k ) for the two simplest partitions. Example 7. The singleton partition consists of |\u2206 k | parts, each a single distribution in \u2206 k ,\nP |\u2206 k | def = {{p} : p \u2208 \u2206 k } .\nAn oracle-aided estimator that knows the part containing p knows p. The competitive regret of data-driven estimators is therefore the min-max regret, r\nP |\u2206 k | n (\u2206 k ) = min q max p\u2208\u2206 k (r n (q, {p}) \u2212 r n ({p})) = min q max p\u2208\u2206 k r n (q, p) = r n (\u2206 k ),\nwhere the middle equality follows as r n (q, {p}) = r n (q, p), and r n ({p}) = 0. Example 8. The whole-collection partition has only one part, the whole collection \u2206 k ,\nP 1 def = {\u2206 k } .\nAn estimator aided by an oracle that knows the part containing p has no additional information, hence no advantage over a data-driven estimator, and the competitive regret is 0,\nr P1 n (\u2206 k ) = min q max P \u2208{\u2206 k } max p\u2208P r n (q, p) \u2212 r n (P ) = min q max p\u2208\u2206 k r n (q, p) \u2212 r n (\u2206 k ) = min q max p\u2208\u2206 k (r n (q, p)) \u2212 r n (\u2206 k ) = r n (\u2206 k ) \u2212 r n (\u2206 k ) = 0.\nThe examples show that for the coarsest partition of \u2206 k , into a single part, the competitive regret is the lowest possible, 0, while for the finest partition, into singletons, the competitive regret is the highest possible, r n (\u2206 k ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A.2 Proof of Equation (5)", "text": "The definition implies that if P \u2286 P then r n (P ) \u2264 r n (P ), for every distribution class P and P . Hence for every q,\nr P n (q, \u2206 k ) = max P \u2208P (r n (q, P ) \u2212 r n (P )) = max P \u2208P max P \u2287P \u2208P (r n (q, P ) \u2212 r n (P )) \u2265 max P \u2208P max P \u2287P \u2208P (r n (q, P ) \u2212 r n (P )) = max P \u2208P max P \u2287P \u2208P r n (q, P ) \u2212 r n (P ) = max P \u2208P (r n (q, P ) \u2212 r n (P )) = r P n (q, \u2206 k ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B Upper bounds", "text": "For a distribution p and sequence x n , let p(x n ) be the probability of observing x n under p. Recall that for a symbol x, we abbreviate p(x) to be p x .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.1 Proof of Lemma 4", "text": "The proof uses the following result. Lemma 9. For every class P \u2208 P \u03c3 , r n (P ) \u2265 max p\u2208P r nat n (p).\nProof. We first show that there is an optimal estimator q that is natural. In particular, let\nq y (x n ) = p\u2208P p(x n y) p \u2208P p (x n )\n.\nWe show that q y (x n ) is an optimal estimator for P . Since q y (x n ) = q \u03c3(y) (\u03c3(x n )) for any permutation \u03c3, the estimator achieves the same loss for every p \u2208 P ,\nmax p\u2208P r n (q , p) = 1 k! p\u2208P r n (q , p ).(6)\nFor any estimator q,\nmax p\u2208P E[D(p||q)] (a) \u2265 1 k! p\u2208P E p [D(p||q)] (b) = 1 k! p\u2208P x n \u2208X n y\u2208X p(x n y) log 1 q y (x n ) \u2212 H(p) = 1 k! x n \u2208X n y\u2208X p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) (c) \u2265 1 k! x n \u2208X n y\u2208X p\u2208P p(x n y) log p \u2208P p (x n ) p \u2208P p (x n y) \u2212 H(p) = 1 k! p\u2208P x n \u2208X n y\u2208X p(x n y) log 1 q y (x n ) \u2212 H(p) (d) = 1 k! p\u2208P r n (q , p).\n(a) follows from the fact that maximum is larger than the average. (b) follows from the fact that every distribution in P has the same entropy. Non-negativity of KL divergence implies (c). All distributions in P has the same entropy and hence (d). Hence together with Equation (6)\nr n (P ) = min q max p\u2208P E[D(p||q)] \u2265 1 k! p\u2208P r n (q , p ) = max p\u2208P r n (q , p).\nHence q is an optimal estimator. Recall that n y denote the number of times symbol y appears in the sequence. q is natural as if n y = n y , then q y (x n ) = q y (x n ). Since there is a natural estimator that achieves minimum in r n (P ), where the last inequality follows from the fact that min-max is bigger than max-min.\nWe can now prove Lemma 4.\nProof of Lemma 4.\nr P\u03c3 n (q, \u2206 k ) = max P \u2208P\u03c3 max p\u2208P E[D(p||q)] \u2212 r n (P ) (a) \u2264 max P \u2208P\u03c3 max p\u2208P E[D(p||q)] \u2212 max p\u2208P r nat n (p) (b) \u2264 max P \u2208P\u03c3 max p\u2208P (E[D(p||q)] \u2212 r nat n (p)) = max p\u2208\u2206 k (E[D(p||q)] \u2212 r nat n (p)) = r nat n (q, \u2206 k ).\nLemma 9 implies (a). Difference of maximums is smaller than maximum of differences, hence (b).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.2 Proof of Lemma 5", "text": "The proof uses the following lemma which computes the best natural estimator. For a random sequence X n , let \u03a6 t def = \u03d5 t (X n ). Recall that S t (x n ) is the sum of probabilities of symbols that appears t times in x n . For notational convenience we use S t to denote both S t (x n ) and S t (X n ). Proof. For a natural estimator q, if n y = n y , then q y (x n ) = q y (x n ) . Hence, with a slight abuse of notation let q ny (x n ) = q y (x n ). For a sequence x n and estimator q,\ny\u2208X p y log 1 q y (x n ) \u2212 n t=0 S t log \u03d5 t S t = n t=0 y:ny=t p y log 1 q y (x n ) \u2212 n t=0 S t log \u03d5 t S t = n t=0 S t log 1 q t (x n ) \u2212 n t=0 S t log \u03d5 t S t = n t=0 S t log S t \u03d5 t q t (x n ) \u2265 0,\nwhere the last inequality follows from the fact that n t=0 S t = n t=0 \u03d5 t q t (x n ) = 1 and KL divergence is non-negative. Furthermore, equality is achieved only by the estimator that assigns\nq * x = Sn x \u03d5n x . Hence, r nat n (p) = min q\u2208Q nat E \uf8ee \uf8f0 y\u2208X p y log p y q y (X n ) \uf8f9 \uf8fb = \u2212H(p) + E n t=0 S t log \u03a6 t S t .\nProof of Lemma 5. As before, with a slight abuse of notation let q ny (x n ) = q y (x n ) for natural estimators q. For any natural estimator q and sequence x n ,\ny\u2208X p y log 1 q y (x n ) = n t=0 y:ny=t p y log 1 q y (x n ) = n t=0 S t log S t \u03d5 t q t (x n ) + n t=0 S t log \u03d5 t S t = n t=0 S t log S t S t + n t=0 S t log \u03d5 t S t .\nThus by Lemma 10,\nr nat n (q, p) = \u2212H(p) + E n t=0 S t log S t S t + n t=0 S t log \u03a6 t S t + H(p) \u2212 E n t=0 S t log \u03a6 t S t = E n t=0 S t log S t S t = E[D(S||\u015c)].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "B.3 Optimality of natural estimators", "text": "We now show that exist natural estimators that achieve r nat n (\u2206 k ) and r P\u03c3 n (\u2206 k ). Lemma 11. The exists a natural estimator q such that\nr nat n (q , \u2206 k ) = r nat n (\u2206 k ).\nSimilar there exists a natural estimator q such that\nr P\u03c3 n (q , \u2206 k ) = r P\u03c3 n (\u2206 k ).\nProof. We prove the result for r nat n (\u2206 k ).\nThe result for r P\u03c3 n (\u2206 k ) is similar and omitted. Let profile\u03c6 of a sequence x n be the vector of its prevalences i.e.,\u03c6(\nx n ) def = (\u03d5 0 (x n ), \u03d5 1 (x n ), \u03d5 2 (x n ), . . . \u03d5 n (x n )).\nFor any optimal estimator q and sequence x n y such that \u03d5(x n ) =\u03c6 n and n y (x n ) = t , let\nq y (x n ) = w n z:\u03c6(w n )=\u03c6n,nz=t q z (w n ) u n v:\u03c6(u n )=\u03c6n,nv=t 1 .\nq is a natural estimator as if for any sequence x n , n y (x n ) = n y (x n ), then q y (x n ) = q y (x n ).\nWe show that q is an optimal estimator. Observe that for any P \u2208 P \u03c3 r n (q, P )\n(a) \u2265 1 k! p\u2208P r n (q, p) (b) \u2265 1 k! p\u2208P r n (q , p) (c) = r n (q , P ). (7\n)\nMaximum is larger than average and hence (a). Every distribution in P has the same KL loss for q and hence (c). To prove (b), observe that\np\u2208P r n (q, p) = p\u2208P x n \u2208X n y\u2208X p(x n y) log 1 q y (x n ) \u2212 H(p) = x n \u2208X n y\u2208X p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) = \u03c6n,t x n :\u03c6(x n )=\u03c6n y:ny=t p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) (d) \u2265 \u03c6n,t x n :\u03c6(x n )=\u03c6n y:ny=t p\u2208P p(x n y) log u n ,v:\u03c6(u n )=\u03c6n,nv=t 1 w n ,z:\u03c6(w n )=\u03c6n,nz=t q z (w n ) \u2212 H(p) = \u03c6n,t x n :\u03c6(x n )=\u03c6n y:ny=t p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) = p\u2208P r n (q , p),\nFor all sequences x n y with the same\u03c6(x n ) and n y (x n ), p\u2208P p(x n y) is the same. Hence, applying log-sum inequality results in (d). By Lemma 10, every p \u2208 P has the same r nat n (p), hence subtracting r nat n (p) from both sides of Equation ( 7) results in max\np\u2208P (r n (q, p) \u2212 r nat n (p)) \u2265 max p\u2208P (r n (q , p) \u2212 r nat n (p)) .\nHence for the optimal estimator q,\nr nat n (\u2206 k ) = max p\u2208\u2206 k (r n (q, p) \u2212 r nat n (p)) = max P \u2208P\u03c3 max p\u2208P (r n (q, p) \u2212 r nat n (p)) \u2265 max P \u2208P\u03c3 max p\u2208P (r n (q , p) \u2212 r nat n (p)) = max p\u2208\u2206 k (r n (q , p) \u2212 r nat n (p)) = r n (q , \u2206 k ).\nThus q is an optimal estimator and furthermore it is natural, hence the lemma.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C Regret bounds on the Good-Turing estimator C.1 Preliminaries", "text": "In practice, often the Good-Turing estimator is used for small multiplicities and empirical estimators are used for large multiplicities. We analyze this estimator and bound its regret. For a symbol appearing t times, we assign probability q x =\u015c t /\u03d5 t , where\u015c t = C t /N . N is the normalization factor to ensure that\n\u221e t=0\u015c t = 1 and C t = \u03d5 t \u2022 t n if t \u2265 t 0 , (\u03d5 t+1 + 1) \u2022 t+1 n else.\nWe set t 0 \u221d n 1/3 later. Similar to our experiments, we have modified the Good-Turing estimator to (\u03d5 t+1 + 1) \u2022 t+1 n , thus ensuring that we never assign a non-zero probability. However, unlike our experiments, where we decided between empirical and Good-Turing estimators depending on if \u03d5 t+1 \u2265 t, for our proofs we just decide it based on t for convenience. We remark that in our experiments the estimator in Section 4 performed better than the one above.\nIdeally we would like to analyze this estimator when the number of samples is n. However, such analysis is complicated as the number of times symbols appear are dependent, for example, they add to n. A standard approach to overcome the dependence, e.g., [29], samples the distribution a random number of times \u223c poi(n), the Poisson distribution with parameter n. Some useful properties of Poisson sampling include: (i) A symbol with probability p appears poi(np) times, (ii) The numbers of times different symbols appear are independent of each other, (iii) For any fixed n 0 , conditioned on the length poi(n) \u2265 n 0 , the distribution of the first n 0 elements is identical to sampling p i.i.d. exactly n 0 times. Thus, to simplify the analysis of the estimator, we assume that the number of samples is a Poisson random variable with mean n. A similar result holds with n samples.\nWe first relate the KL regret to a chi-squared like distance between S and C. Lemma 12. For any distribution p \u2208 \u2206 k ,\nE[D(S||\u015c)] \u2264 t0\u22121 t=0 E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n + \u221e t=t0 E (S t \u2212 t\u03a6 t /n) 2 \u03a6 t t/n . Proof. Since log(1 + y) \u2264 y, \u221e t=0 S t = 1,and\n\u221e t=0 C t = N , D(S||\u015c) = \u221e t=0 S t log S t S t = \u221e t=0 S t log N S t C t = \u221e t=0 S t log S t C t + \u221e t=0 S t log N = \u221e t=0 S t log 1 + S t \u2212 C t C t + log N \u2264 \u221e t=0 S t S t \u2212 C t C t + log N = \u221e t=0 (S t \u2212 C t ) S t \u2212 C t C t + \u221e t=0 C t S t \u2212 C t C t + log N = \u221e t=0 (S t \u2212 C t ) S t \u2212 C t C t + \u221e t=0 (S t \u2212 C t ) + log N = \u221e t=0 (S t \u2212 C t ) 2 C t + 1 \u2212 N + log N \u2264 \u221e t=0 (S t \u2212 C t ) 2 C t = t0\u22121 t=0 (S t \u2212 C t ) 2 C t + \u221e t=t0 (S t \u2212 C t ) 2 C t .\nTaking expectations on both sides and substituting C t results in the lemma.", "publication_ref": ["b28"], "figure_ref": [], "table_ref": []}, {"heading": "C.2 Empirical estimators", "text": "All of our results including the next lemma hold for all distributions in \u2206 k and hence stated without any condition on the underlying distribution. Let N x def = n x (X n ) for a random sequence X n . Lemma 13. For any n and t 0 ,\n\u221e t=t0 E (S t \u2212 t\u03a6 t /n) 2 t\u03a6 t /n \u2264 1 t 0 . Proof. \u221e t=t0 (S t \u2212 t\u03a6 t /n) 2 t\u03a6 t /n \u2264 \u221e t=t0 (S t \u2212 t\u03a6 t /n) 2 \u03a6 t t 0 /n (a) \u2264 \u221e t=t0 x 1 Nx=t (p x \u2212 t/n) 2 t 0 /n = x \u221e t=t0 1 Nx=t (p x \u2212 t/n) 2 t 0 /n \u2264 x \u221e t=0 1 Nx=t (p x \u2212 t/n) 2 t 0 /n .\n(a) follows from the fact that\n( m x=1 ax) 2 m \u2264 m i=1 a 2\nx for a x = 1 Nx=t (p x \u2212 t/n) and m = \u03a6 t . Taking expectations on both sides,\n\u221e t=t0 E (S t \u2212 t\u03a6 t /n) 2 ] \u03a6 t t/n \u2264 x E[ \u221e t=0 1 Nx=t (p x \u2212 t/n) 2 ] t 0 /n \u2264 x p x /n t 0 /n = 1 t 0 ,\nwhere the second inequality follows from observing that E[\n\u221e t=0 1 Nx=t (p x \u2212 t/n) 2 ]\nis the variance of a Poisson random variable with mean np x .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "C.3 Good-Turing estimators", "text": "To bound the regret corresponding to the Good-Turing estimator, we need few auxiliary results. The next set of equations follow from results in [13], For any n and t,\nE[S t ] = t + 1 n \u2022 E[\u03a6 t+1 ].(8)\nVar(S t )\n\u2264 (t + 1)(t + 2) n 2 \u2022 E[\u03a6 t+2 ].(9)\nE S t \u2212 (t + 1)\u03a6 t+1 n 2 \u2264 (t + 1)(t + 2)E[\u03a6 t+2 ] n 2 + (t + 1) 2 E[\u03a6 t+1 ] n 2 . (10\n)\nThe next lemma relates\nE[\u03a6 t+1 ] to E[\u03a6 t ].\nLemma 14. For any n and t \u2265 1, (a) follows from the fact that second term is a decreasing as a function of np x in the range [r(t + 1), \u221e). (b) follows from the fact that e \u2212r(t+1) (r(t + 1)) t t! = e \u2212rt r t \u2022 e \u2212t (t + 1) t t! \u2264 e \u2212rt r t \u2264 e \u2212rt/2 .\nE[\u03a6 t+1 ] \u2264 E[\u03a6 t ] 2 t log n + t t + 1 + 1 t + 1 . Proof. Let r \u2265 t t+1 . E[\u03a6 t+1 ] = E x 1 Nx=t+1 = x e \u2212npx (np x ) t+1 (t + 1)! = x n t + 1 \u2022 e \u2212npx (\nChoosing r = 2 t log n + t t+1 , yields\nE[\u03a6 t+1 ] \u2264 E[\u03a6 t ] 2 t log n + t t + 1 + 1 t + 1\n.\nThe final auxiliary lemma bounds the inverse moment of Poisson binomial distributions. Lemma 15. Let X i for 1 \u2264 i \u2264 n be Bernoulli random variables, then\nE 1 n i=1 X i + 1 \u2264 1 n i=1 E[X i ] . Proof. Let s i = E[X i ].\nWe show that of all tuples s 1 , s 2 , . . . , s n such that n i=1 s i = ns, the one that maximizes the expectation is s i = s, \u2200i. Suppose for some i, j, s i > s j , we show that if we decrease s i and increase s j keeping the sum same, then the expectation increases. Let Y = 1 + k / \u2208{i,j} X k . For any instance of X n , taking expectation with respect to only X i and X j .\nE\n1 X i + X j + Y | Y = (1 \u2212 s i )(1 \u2212 s j ) Y + s i (1 \u2212 s j ) + (1 \u2212 s i )s j Y + 1 + s i s j Y + 2 = 1 Y + (s i + s j ) 1 Y + 1 \u2212 1 Y + s i s j 2 Y (Y + 1)(Y + 2)\n.\nThus if we decrease s i and increase s j (keeping s i + s j fixed), then s i s j increases and hence the expectation increases. Hence the maximum occurs when s i = s j for all i, j and\nE 1 n i=1 X i + 1 \u2264 E 1 Z + 1 ,\nwhere Z is a binomial random variable with parameters n and s = n i=1 E[X i ]/n. The expectation can be bounded as\nE 1 Z + 1 = n j=0 1 j + 1 n j s j (1 \u2212 s) n\u2212j = 1 (n + 1)s n j=0 n + 1 j + 1 s j+1 (1 \u2212 s) n+1\u2212(j+1) \u2264 1 (n + 1)s \u2264 1 ns = 1 n i=1 E[X i ] .\nUsing the above lemma, we first bound the expectation of S 2 t /(\u03a6 t+1 + 1). Lemma 16. For any n and t, if\nE[\u03a6 t+1 ] > 2, then E S 2 t \u03a6 t+1 + 1 \u2264 E[S 2 t ] E[\u03a6 t+1 ] \u2212 1 .\nProof. We first observe that for any x,\nE[1 Nx=t+1 ] = e \u2212npx (np x ) t+1 (t + 1)! \u2264 e \u2212t\u22121 (t + 1) t+1 (t + 1)! \u2264 1 e . (11\n)\nSince S t = x p x 1 Nx=t and \u03a6 t+1 = x 1 Nx=t+1 , S 2 t \u03a6 t+1 + 1 = x y p x p y 1 Nx=t 1 Ny=t z 1 Nz=t+1 + 1 = x y p x p y 1 Nx=t 1 Ny=t z:z =x,z =y 1 Nz=t+1 + 1 ,\nwhere the equality follows from the fact that symbol cannot appear both t and t + 1 times thus only one of 1 Nx=t and 1 Nx=t+1 can be 1. The numerator and the denominator of the terms on RHS are independent of each other, hence \nE p x p y 1 Nx=t 1 Ny=t z 1 Nz=t+1 + 1 = E p x p y 1 Nx=t 1 Ny=t z:z =x,z =y 1 Nz=t+1 + 1 = E p x p y 1 Nx=t 1 Ny=t E 1 z:z =x,z =y 1 Nz=t+1 + 1 (a) \u2264 E p x p y 1 Nx=t 1 Ny=t z:z =x,z =y E[1 Nz=t+1 ] (b) \u2264 E p x p y 1 Nx=t 1 Ny=t E[\u03a6 t+1 \u2212 1] ,(\nE[1 Nz=t+1 ] = z E[1 Nz=t+1 ] \u2212 E[1 Nx=t+1 ] \u2212 E[1 Ny=t+1 ] \u2265 E[\u03a6 t+1 ] \u2212 1.\nSumming over x and y results in the lemma.\nWe now have all the tools to bound the error of the Good-Turing estimator. We divide the set of values into two groups, depending on the value of E[\u03a6 t+1 ].\nLemma 17. For any n and\nt if E[\u03a6 t+1 ] \u2264 2, then E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t n + 4 log n n t + 2 t + 1 + 6 n . Proof. Let Z = S t \u2212 (t + 1)\u03a6 t+1 /n. E Z \u2212 t + 1 n 2 (a) = E[Z 2 ] + (t + 1) 2 n 2 (b) \u2264 (t + 1)(t + 2)E[\u03a6 t+2 ] n 2 + (t + 1) 2 E[\u03a6 t+1 ] n 2 + (t + 1) 2 n 2 (c) \u2264 2 (t + 1)(t + 2) n 2 \u2022 2 log n t + 1 + t + 1 t + 2 + (t + 1)(t + 2) n 2 (t + 2) + 3(t + 1) 2 n 2 .\nEquation ( 8) implies Z is a zero mean random variable and hence (a). Equation ( 10) implies (b) and (c) follows by Lemma 14 and the fact that\nE[\u03a6 t+1 ] \u2264 2. Hence, E (Z \u2212 (t + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 E[(Z \u2212 (t + 1)/n) 2 ] (t + 1)/n \u2264 2(t + 2) n \u2022 2 log n t + 1 + t + 1 t + 2 + 1 n + 3(t + 1) n = 5t n + 4 log n(t + 2) n(t + 1) + 6 n .\nLemma 18. For any n and\nt if E[\u03a6 t+1 ] > 2, then E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t n + 4 log n n t + 2 t + 1 + 6 n .\nProof.\n(S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n = S 2 t (\u03a6 t+1 + 1)(t + 1)/n + (t + 1)(\u03a6 t+1 + 1) n \u2212 2S t .\nThus by Equation ( 8),\nE (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n = E S 2 t (\u03a6 t+1 + 1)(t + 1)/n \u2212 (t + 1)E[\u03a6 t+1 ] n + t + 1 n .(12)\nBy Lemma 16 and Equations ( 8), ( 9),\nE S 2 t (\u03a6 t+1 + 1)(t + 1)/n \u2264 E[S 2 t ] E[\u03a6 t+1 \u2212 1](t + 1)/n \u2264 t + 1 n E[\u03a6 t+1 ] 2 E[\u03a6 t+1 \u2212 1] + t + 2 n E[\u03a6 t+2 ] E[\u03a6 t+1 \u2212 1] .\nSubstituting the above equation in Equation (12) and simplifying,\nE (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 (t + 1)E[\u03a6 t+1 ] + (t + 2)E[\u03a6 t+2 ] nE[\u03a6 t+1 \u2212 1] + t + 1 n (a) \u2264 2 (t + 1)E[\u03a6 t+1 ] + (t + 2)E[\u03a6 t+2 ] nE[\u03a6 t+1 ] + t + 1 n (b) \u2264 2 t + 1 n + t + 2 n 2 log n t + 1 + t + 1 t + 2 + 1 2(t + 2) + t + 1 n = 5t n + 4 log n n t + 2 t + 1 + 6 n . Since E[\u03a6 t+1 ] \u2265 2, E[\u03a6 t+1 ] \u2212 1 \u2265 E[\u03a6 t+1\n]/2 and hence (a). Lemma 14 implies (b).\nCombining the above two lemmas results in Lemma 19. For any t 0 \u2265 1,\nt0\u22121 t=0 E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t 2 0 2n + 4 log n n (t 0 + log t 0 + 1) + 7t 0 2n .\nProof. By Lemmas 17 and 18, regardless of the value of\nE[\u03a6 t+1 ], E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t n + 4 log n n t + 2 t + 1 + 6 n .\nSumming the above expression for 0 \u2264 t \u2264 t 0 \u2212 1 results in the lemma.\nSubstituting the results from Lemmas 13 and 19 in Lemma 12,\nE[D(S||\u015c)] \u2264 1 t 0 + 5t 2 0 2n + 4 log n n (t 0 + log t 0 + 1) + 7t 0 2n . Substituting t 0 = n 1/3 /5 1/3 results in Theorem 1. r nat poi(n) (q , \u2206 k ) \u2264 max p\u2208\u2206 k E[D(S||\u015c)] \u2264 2.6 n 1/3 + 2.4 log n(n 1/3 + log n + 1) n + 2.1 n 2/3 \u2264 3 + o n (1) n 1/3 .", "publication_ref": ["b12", "b11"], "figure_ref": [], "table_ref": []}, {"heading": "D Proof of Theorem 3", "text": "To lower bound r P\u03c3 n (\u2206 k ) it is sufficient to lower bound r P\u03c3 n (P) for any subset P \u2286 \u2206 k . We construct a subset P by considering a set of distributions {pv :v \u2208 {\u22121, 1} m\u22121 } and all their possible permutations. The lower bound argument uses Fano's inequality and Gilbert Varshamov bounds.\nWe choose P to be the set of distributions whose probability multiset are close to that of a distribution p 0 , where p 0 is defined as follows.\nLet c be a sufficiently large constant.\nLet m be the largest odd number less than min(k, (n/(c 2 log 2 n)) 1/3 ). Let p 0 be the following distribution. For 1 \u2264 i \u2264 m \u2212 1,\np 0 i = log n 6n c 2 n m n c 2 m log 2 n + i and p 0 m = 1 \u2212 m\u22121 i=1 p 0 i . Observe that for all 1 \u2264 i \u2264 m \u2212 1, 1/(6m) \u2264 p 0 i \u2264 1/(3m) and p 0 m \u2265 2/3.\nWe choose the close-by distributions as follows. Let = c * mn , where c * is some sufficiently small constant. For a binary vectorv \u2208 {\u22121, 1} m\u22121 , let pv be the distribution such that pv\ni = p 0 i +v i for 1 \u2264 i \u2264 m \u2212 1 and pv(m) = 1 \u2212 m\u22121 i=1 pv i .\nNote that by the properties of p 0 and , pv is a valid distribution for everyv. Let C be the largest subset of {\u22121, 1} m\u22121 such that for everyv \u2208 C, iv i = 0 and for every pairv,v \u2208 C, i |v i \u2212v i | \u2265 c (m\u22121) for some constant c . The following variation of Gilbert Varshamov lemma lower bounds size of C. Lemma 20. There exists a set of vectors C over {\u22121, 1} m\u22121 of size 2 c \u2022(m\u22121) such that the minimum hamming distance between any two vectors is \u2265 c (m\u22121) for some universal constants c > 0, c > 0 and iv i = 0 for allv \u2208 C.\nLet P = {pv :v \u2208 C} and Pv = {pv(\u03c3(\u2022)) : \u03c3 \u2208 \u03a3 m\u22121 } be the set of all permutations of a distribution pv, i.e., all distributions with the same multiset as pv. Let P = \u222av \u2208C Pv.\nWe first bound the regret of the induced permutation class Pv that contains all permutations of a distribution pv. Proof. We prove the bound by constructing an estimator q. Consider the estimator q which sorts the multiplicities and assigns the i th -frequently occurred symbol probability pv i . Since this is a natural estimator, it occurs the same loss for all distributions in Pv and hence, (a) follows from the fact that the estimator makes an error only if two multiplicities cross over and if it does make an error, the maximum KL divergence is at most log(p max /p min ) \u2264 log n. Since probabilities for any two symbols i and j differ by at least log n 6n \u2022 c 2 n m and the probabilities themselves lie between 1/(6m) and 1/(3m), by choosing a sufficiently large c, the cross over probability can be bounded by e \u22122 log n using the Chernoff bound and hence (b). Proof. We now state Fano's inequality for distribution estimation. Lemma 23. Let p 1 , p 2 , . . . p r+1 be distributions such that D(p i ||p j ) \u2264 \u03b2 and ||p i \u2212 p j || 1 \u2265 \u03b1, for all i, j. For any estimator q,", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D(pv||pv )", "text": "\u2264 m i=1 (pv i \u2212 pv i ) 2 pv i (b) \u2264 2 m i=1 (pv i \u2212 pv i ) 2 p 0 i \u2264 2 m\u22121 i=1 (v i \u2212v i ) 2 ( c * /nm) 2 1/(6m) \u2264 12 m\u22121 i=1 (v i \u2212v i ) 2 c * n \u2264 24 m\u22121 i=1 |v i \u2212v i |c * n = 24||v \u2212v || 1 c * n \u2264 48mc * n .(a)\nsup i E i [||p i \u2212 q|| 1 ] \u2265 \u03b1 2 1 \u2212 n\u03b2 + log 2 log r .\nWe now have all the tools for the lower bound. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Good-turing frequency estimation without tears", "journal": "Journal of Quantitative Linguistics", "year": "1995", "authors": "A William; Geoffrey Gale;  Sampson"}, {"ref_id": "b1", "title": "An empirical study of smoothing techniques for language modeling", "journal": "", "year": "1996", "authors": "S F Chen; J Goodman"}, {"ref_id": "b2", "title": "Variational minimax estimation of discrete distributions under KL loss", "journal": "", "year": "2004", "authors": "Liam Paninski"}, {"ref_id": "b3", "title": "On structuring probabilistic dependences in stochastic language modelling", "journal": "Computer Speech & Language", "year": "1994", "authors": "Hermann Ney; Ute Essen; Reinhard Kneser"}, {"ref_id": "b4", "title": "Probability distribution estimation from sparse data", "journal": "IBM Tech. Disclosure Bull", "year": "1984", "authors": "Fredrick Jelinek; Robert L Mercer"}, {"ref_id": "b5", "title": "The population frequencies of species and the estimation of population parameters", "journal": "Biometrika", "year": "1953", "authors": "J Irving;  Good"}, {"ref_id": "b6", "title": "Elements of information theory", "journal": "Wiley", "year": "2006", "authors": "M Thomas; Joy A Cover;  Thomas"}, {"ref_id": "b7", "title": "Universal Compression and Retrieval", "journal": "Kluwer", "year": "1994", "authors": "R Krichevsky"}, {"ref_id": "b8", "title": "On learning distributions from their samples", "journal": "", "year": "2015", "authors": "Sudeep Kamath; Alon Orlitsky; Dheeraj Pichapati; Ananda Theertha Suresh"}, {"ref_id": "b9", "title": "Bernstein polynomials and learning theory", "journal": "Journal of Approximation Theory", "year": "2004", "authors": "Dietrich Braess; Thomas Sauer"}, {"ref_id": "b10", "title": "On the convergence rate of Good-Turing estimators", "journal": "", "year": "2000", "authors": "A David; Robert E Mcallester;  Schapire"}, {"ref_id": "b11", "title": "Concentration bounds for unigrams language model", "journal": "", "year": "2004", "authors": "Evgeny Drukh; Yishay Mansour"}, {"ref_id": "b12", "title": "Alon Orlitsky, and Ananda Theertha Suresh. Optimal probability estimation with applications to prediction and classification", "journal": "", "year": "2013", "authors": "Jayadev Acharya; Ashkan Jafarpour"}, {"ref_id": "b13", "title": "Always Good Turing: Asymptotically optimal probability estimation", "journal": "", "year": "2003", "authors": "Alon Orlitsky; P Narayana; Junan Santhanam;  Zhang"}, {"ref_id": "b14", "title": "Twice-universal coding", "journal": "Problemy Peredachi Informatsii", "year": "1984", "authors": " Boris Yakovlevich Ryabko"}, {"ref_id": "b15", "title": "Fast adaptive coding algorithm", "journal": "Problemy Peredachi Informatsii", "year": "1990", "authors": " Boris Yakovlevich Ryabko"}, {"ref_id": "b16", "title": "About adaptive coding on countable alphabets", "journal": "IEEE Transactions on Information Theory", "year": "2014", "authors": "Dominique Bontemps; St\u00e9phane Boucheron; Elisabeth Gassiat"}, {"ref_id": "b17", "title": "About adaptive coding on countable alphabets: Max-stable envelope classes", "journal": "", "year": "2014", "authors": "St\u00e9phane Boucheron; Elisabeth Gassiat; Mesrob I Ohannessian"}, {"ref_id": "b18", "title": "Ideal spatial adaptation by wavelet shrinkage", "journal": "Biometrika", "year": "1994", "authors": "L David; Jain M Johnstone Donoho"}, {"ref_id": "b19", "title": "Adapting to unknown sparsity by controlling the false discovery rate", "journal": "The Annals of Statistics", "year": "2006", "authors": "Felix Abramovich; Yoav Benjamini; L David; Iain M Donoho;  Johnstone"}, {"ref_id": "b20", "title": "Efficient and adaptive estimation for semiparametric models", "journal": "Johns Hopkins University Press", "year": "1993", "authors": "J Peter; Chris A Bickel; Ya'acov Klaassen; Jon A Ritov;  Wellner"}, {"ref_id": "b21", "title": "Risk bounds for model selection via penalization. Probability theory and related fields", "journal": "", "year": "1999", "authors": "Andrew Barron; Lucien Birg\u00e9; Pascal Massart"}, {"ref_id": "b22", "title": "Introduction to Nonparametric Estimation", "journal": "Springer", "year": "2004", "authors": "Alexandre B Tsybakov"}, {"ref_id": "b23", "title": "Alon Orlitsky, and Shengjun Pan", "journal": "", "year": "2011", "authors": "Jayadev Acharya; Hirakendu Das; Ashkan Jafarpour"}, {"ref_id": "b24", "title": "Alon Orlitsky, Shengjun Pan, and Ananda Theertha Suresh. Competitive classification and closeness testing", "journal": "", "year": "2012", "authors": "Jayadev Acharya; Hirakendu Das; Ashkan Jafarpour"}, {"ref_id": "b25", "title": "Alon Orlitsky, and Ananda Theertha Suresh. A competitive test for uniformity of monotone distributions", "journal": "", "year": "2013", "authors": "Jayadev Acharya; Ashkan Jafarpour"}, {"ref_id": "b26", "title": "An automatic inequality prover and instance optimal identity testing", "journal": "", "year": "2014", "authors": "Gregory Valiant; Paul Valiant"}, {"ref_id": "b27", "title": "", "journal": "", "year": "2015", "authors": "Gregory Valiant; Paul Valiant"}, {"ref_id": "b28", "title": "Probability and computing: Randomized algorithms and probabilistic analysis", "journal": "Cambridge University Press", "year": "2005", "authors": "Michael Mitzenmacher; Eli Upfal"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Qualitative behavior of the KL loss as a function of distributions in different formulations", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Good-Turing + empirical (e) Uniform prior (Dirichlet 1) Good-Turing + empirical (f) Dirichlet 1/2 prior", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: Simulation results for support 10000, number of samples ranging from 1000 to 50000, averaged over 200 trials.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "q\u2208Q nat max p\u2208P E[D(p||q)] \u2265 max p\u2208P min q\u2208Q nat E[D(p||q)] = max p\u2208P r nat n (p),", "figure_data": ""}, {"figure_label": "10", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Lemma 10 .10Let q * x (x n ) = Sn x", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "a) follows from Lemma 15 and (b) follows from Equation (11) as z:z =x,z =y", "figure_data": ""}, {"figure_label": "21", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Lemma 21 .21For every induced permutation class Pv, r n (Pv) \u2264 1 n .", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "\u2264Pr(\u2203i, j : N i > N j , pv i < pv j ) log n", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "We now lower bound the KL divergence between pv and pv for every pair of vectorsv andv . Let the Hamming distance between two vectorsv andv be||v \u2212v || 1 = m\u22121 i=1 |v i \u2212v i |.Lemma 22. For two distributions pv and pv in P ,", "figure_data": ""}, {"figure_label": "a2", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "( a ) 2 ,a2follows from bounding the KL divergence by the Chi-squared distance and (b) follows from the fact that 1/m. For the lower bound, where (a) follows from Pinsker's inequality, (b) follows by construction, and m \u2212 1 \u2265 2 and hence (c).", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Proof of Theorem 3 .3For every permutation subclass Pv in P, by Lemma 21 r n (Pv) , hence (a). (b) follows from Pinsker's inequality and (c) follows from convexity. By construction, for every pair of distributions in P , \u03b2 = D(p||p ) \u2264 48c * m/n and \u03b1 = ||p \u2212 p || 1 \u2265 \u2126( m/n) (Lemma 22). Furthermore by Lemma 20, P has r +1 = 2 c (m\u22121) distributions. Setting c * to be a sufficiently small constant and applying Lemma 23 to P with the above values of \u03b1, \u03b2, and r results in (d). Substituting the value of m in the above equation results in the Theorem.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "D(p||q) def = x p x log p x q x ,", "formula_coordinates": [2.0, 254.24, 93.04, 103.52, 26.35]}, {"formula_id": "formula_1", "formula_text": "def = max p\u2208P r n (q, p),(1)", "formula_coordinates": [2.0, 291.43, 310.49, 212.57, 17.96]}, {"formula_id": "formula_3", "formula_text": "r n (\u2206 k ) = k \u2212 1 2n \u2022 (1 + o(1)).(3)", "formula_coordinates": [2.0, 245.18, 554.74, 258.82, 22.31]}, {"formula_id": "formula_4", "formula_text": "k n \u2022 (1 + o(1)),(4)", "formula_coordinates": [2.0, 306.14, 661.64, 197.86, 22.31]}, {"formula_id": "formula_5", "formula_text": "x = t, q x (x n ) = \u03d5 t+1 \u03d5 t \u2022 t + 1 n ,", "formula_coordinates": [3.0, 113.98, 191.94, 240.63, 32.51]}, {"formula_id": "formula_6", "formula_text": "r P n (q, \u2206 k ) def = max P \u2208P (r n (q, P ) \u2212 r n (P )) ,", "formula_coordinates": [4.0, 226.39, 253.66, 159.22, 18.0]}, {"formula_id": "formula_7", "formula_text": "r P n (\u2206 k ) def = min q r P n (q, \u2206 k ).", "formula_coordinates": [4.0, 252.78, 299.15, 106.45, 17.52]}, {"formula_id": "formula_8", "formula_text": "r P n (\u2206 k ) = min q max P \u2208P max p\u2208P r n (q, p) \u2212 r n (P ) .", "formula_coordinates": [4.0, 210.27, 350.64, 191.45, 15.38]}, {"formula_id": "formula_9", "formula_text": "P n (q, \u2206 k ) \u2265 r P n (q, \u2206 k ).(5)", "formula_coordinates": [4.0, 261.21, 461.83, 242.79, 11.37]}, {"formula_id": "formula_10", "formula_text": "p \u03c3(i) = p i ,", "formula_coordinates": [4.0, 284.1, 563.71, 43.8, 10.87]}, {"formula_id": "formula_11", "formula_text": "r nat n (q, p) = r n (q, p) \u2212 r nat n (p). Thus the regret of an estimator q over all distributions in \u2206 k is r nat n (q, \u2206 k ) = max p\u2208\u2206 k r nat n (q, p),", "formula_coordinates": [5.0, 108.0, 237.51, 256.8, 42.81]}, {"formula_id": "formula_12", "formula_text": "r P\u03c3 poi(n) (q , \u2206 k ) \u2264 r nat poi(n) (q , \u2206 k ) \u2264 3 + o n (1) n 1/3 .", "formula_coordinates": [5.0, 212.03, 470.49, 187.94, 22.49]}, {"formula_id": "formula_13", "formula_text": "P\u03c3 n (q , \u2206 k ) \u2264 r nat n (q , \u2206 k ) \u2264\u00d5 n min 1 \u221a n , k n .", "formula_coordinates": [5.0, 200.73, 554.93, 215.05, 22.35]}, {"formula_id": "formula_14", "formula_text": "r nat n (\u2206 k ) \u2265\u03a9 n min 1 \u221a n , k n .", "formula_coordinates": [5.0, 235.32, 625.15, 141.37, 22.35]}, {"formula_id": "formula_15", "formula_text": "P\u03c3 n (\u2206 k ) \u2265\u03a9 n min 1 n 2/3 , k n .", "formula_coordinates": [5.0, 236.24, 706.85, 144.01, 22.49]}, {"formula_id": "formula_16", "formula_text": "r n (\u2206 k ) = log k n Uniform distribution KL loss Distributions \u2264\u00d5 min( 1 \u221a n , k n", "formula_coordinates": [6.0, 161.52, 269.44, 280.97, 128.02]}, {"formula_id": "formula_17", "formula_text": "q x = t N if t > \u03d5 t+1 , \u03d5t+1+1 \u03d5t \u2022 t+1 N else,", "formula_coordinates": [6.0, 233.51, 677.43, 143.78, 26.83]}, {"formula_id": "formula_18", "formula_text": "q\u015c x = n x + \u03b2\u015c nx N (\u015c) ,", "formula_coordinates": [7.0, 272.16, 420.42, 67.69, 24.42]}, {"formula_id": "formula_19", "formula_text": "\u03b2 BS 0 = 1/2, \u03b2 BS 1 = 1, \u03b2 BS t = 3/4 \u2200 t > 1", "formula_coordinates": [7.0, 108.0, 495.68, 396.0, 23.16]}, {"formula_id": "formula_20", "formula_text": "P\u03c3 n (q, \u2206 k ) \u2264 r nat n (q, \u2206 k ).", "formula_coordinates": [8.0, 259.33, 145.72, 97.83, 11.37]}, {"formula_id": "formula_21", "formula_text": "D(S||\u015c) = n t=0 S t log S t S t .", "formula_coordinates": [8.0, 253.46, 288.7, 105.09, 30.2]}, {"formula_id": "formula_22", "formula_text": "Lemma 5 (Appendix B.2). For a natural estimator q let\u015c t (x n ) = x:nx=t q x (x n ), then r nat n (q, p) = E[D(S||\u015c)].", "formula_coordinates": [8.0, 108.0, 355.91, 358.21, 29.06]}, {"formula_id": "formula_23", "formula_text": "r nat n (q, \u2206 k ) = max p\u2208\u2206 k E[D(S||\u015c)]. Furthermore, r nat n (\u2206 k ) = min S max p\u2208\u2206 k E[D(S||\u015c)].", "formula_coordinates": [8.0, 108.0, 435.46, 266.15, 47.06]}, {"formula_id": "formula_24", "formula_text": "probability \u2265 1 \u2212 1/n , D(S||\u015c) =\u00d5 n 1 \u221a n .", "formula_coordinates": [8.0, 190.14, 512.31, 165.48, 35.99]}, {"formula_id": "formula_25", "formula_text": "max p\u2208\u2206 k E[D(S||\u015c)] =\u00d5 n 1 \u221a n .", "formula_coordinates": [8.0, 239.27, 576.99, 133.45, 22.35]}, {"formula_id": "formula_26", "formula_text": "n t=1 \u221a \u03d5 t \u2264 n t=1 \u03d5 t \u2264 k", "formula_coordinates": [8.0, 118.52, 598.44, 385.48, 31.14]}, {"formula_id": "formula_27", "formula_text": "max p\u2208\u2206 k E[D(S||\u015c)] =\u00d5 n min 1 \u221a n , k n .", "formula_coordinates": [8.0, 216.41, 632.58, 179.19, 22.35]}, {"formula_id": "formula_28", "formula_text": "P |\u2206 k | def = {{p} : p \u2208 \u2206 k } .", "formula_coordinates": [10.0, 253.83, 160.19, 104.34, 14.69]}, {"formula_id": "formula_29", "formula_text": "P |\u2206 k | n (\u2206 k ) = min q max p\u2208\u2206 k (r n (q, {p}) \u2212 r n ({p})) = min q max p\u2208\u2206 k r n (q, p) = r n (\u2206 k ),", "formula_coordinates": [10.0, 213.8, 208.4, 188.9, 53.67]}, {"formula_id": "formula_30", "formula_text": "P 1 def = {\u2206 k } .", "formula_coordinates": [10.0, 279.9, 299.7, 52.21, 13.71]}, {"formula_id": "formula_31", "formula_text": "r P1 n (\u2206 k ) = min q max P \u2208{\u2206 k } max p\u2208P r n (q, p) \u2212 r n (P ) = min q max p\u2208\u2206 k r n (q, p) \u2212 r n (\u2206 k ) = min q max p\u2208\u2206 k (r n (q, p)) \u2212 r n (\u2206 k ) = r n (\u2206 k ) \u2212 r n (\u2206 k ) = 0.", "formula_coordinates": [10.0, 204.72, 353.43, 195.23, 93.06]}, {"formula_id": "formula_32", "formula_text": "r P n (q, \u2206 k ) = max P \u2208P (r n (q, P ) \u2212 r n (P )) = max P \u2208P max P \u2287P \u2208P (r n (q, P ) \u2212 r n (P )) \u2265 max P \u2208P max P \u2287P \u2208P (r n (q, P ) \u2212 r n (P )) = max P \u2208P max P \u2287P \u2208P r n (q, P ) \u2212 r n (P ) = max P \u2208P (r n (q, P ) \u2212 r n (P )) = r P n (q, \u2206 k ).", "formula_coordinates": [10.0, 207.63, 556.12, 196.73, 114.57]}, {"formula_id": "formula_33", "formula_text": "q y (x n ) = p\u2208P p(x n y) p \u2208P p (x n )", "formula_coordinates": [11.0, 251.17, 162.12, 105.69, 27.78]}, {"formula_id": "formula_34", "formula_text": "max p\u2208P r n (q , p) = 1 k! p\u2208P r n (q , p ).(6)", "formula_coordinates": [11.0, 234.67, 227.74, 269.33, 26.8]}, {"formula_id": "formula_35", "formula_text": "max p\u2208P E[D(p||q)] (a) \u2265 1 k! p\u2208P E p [D(p||q)] (b) = 1 k! p\u2208P x n \u2208X n y\u2208X p(x n y) log 1 q y (x n ) \u2212 H(p) = 1 k! x n \u2208X n y\u2208X p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) (c) \u2265 1 k! x n \u2208X n y\u2208X p\u2208P p(x n y) log p \u2208P p (x n ) p \u2208P p (x n y) \u2212 H(p) = 1 k! p\u2208P x n \u2208X n y\u2208X p(x n y) log 1 q y (x n ) \u2212 H(p) (d) = 1 k! p\u2208P r n (q , p).", "formula_coordinates": [11.0, 153.92, 282.55, 304.16, 186.78]}, {"formula_id": "formula_36", "formula_text": "r n (P ) = min q max p\u2208P E[D(p||q)] \u2265 1 k! p\u2208P r n (q , p ) = max p\u2208P r n (q , p).", "formula_coordinates": [11.0, 245.5, 519.47, 121.0, 66.74]}, {"formula_id": "formula_37", "formula_text": "r P\u03c3 n (q, \u2206 k ) = max P \u2208P\u03c3 max p\u2208P E[D(p||q)] \u2212 r n (P ) (a) \u2264 max P \u2208P\u03c3 max p\u2208P E[D(p||q)] \u2212 max p\u2208P r nat n (p) (b) \u2264 max P \u2208P\u03c3 max p\u2208P (E[D(p||q)] \u2212 r nat n (p)) = max p\u2208\u2206 k (E[D(p||q)] \u2212 r nat n (p)) = r nat n (q, \u2206 k ).", "formula_coordinates": [12.0, 197.06, 141.25, 210.54, 109.97]}, {"formula_id": "formula_38", "formula_text": "y\u2208X p y log 1 q y (x n ) \u2212 n t=0 S t log \u03d5 t S t = n t=0 y:ny=t p y log 1 q y (x n ) \u2212 n t=0 S t log \u03d5 t S t = n t=0 S t log 1 q t (x n ) \u2212 n t=0 S t log \u03d5 t S t = n t=0 S t log S t \u03d5 t q t (x n ) \u2265 0,", "formula_coordinates": [12.0, 146.09, 507.18, 318.13, 111.36]}, {"formula_id": "formula_39", "formula_text": "q * x = Sn x \u03d5n x . Hence, r nat n (p) = min q\u2208Q nat E \uf8ee \uf8f0 y\u2208X p y log p y q y (X n ) \uf8f9 \uf8fb = \u2212H(p) + E n t=0 S t log \u03a6 t S t .", "formula_coordinates": [12.0, 108.0, 652.01, 345.36, 58.95]}, {"formula_id": "formula_40", "formula_text": "y\u2208X p y log 1 q y (x n ) = n t=0 y:ny=t p y log 1 q y (x n ) = n t=0 S t log S t \u03d5 t q t (x n ) + n t=0 S t log \u03d5 t S t = n t=0 S t log S t S t + n t=0 S t log \u03d5 t S t .", "formula_coordinates": [13.0, 188.32, 120.21, 233.67, 98.13]}, {"formula_id": "formula_41", "formula_text": "r nat n (q, p) = \u2212H(p) + E n t=0 S t log S t S t + n t=0 S t log \u03a6 t S t + H(p) \u2212 E n t=0 S t log \u03a6 t S t = E n t=0 S t log S t S t = E[D(S||\u015c)].", "formula_coordinates": [13.0, 131.35, 259.22, 341.8, 79.81]}, {"formula_id": "formula_42", "formula_text": "r nat n (q , \u2206 k ) = r nat n (\u2206 k ).", "formula_coordinates": [13.0, 258.22, 458.28, 95.55, 11.26]}, {"formula_id": "formula_43", "formula_text": "r P\u03c3 n (q , \u2206 k ) = r P\u03c3 n (\u2206 k ).", "formula_coordinates": [13.0, 256.74, 510.16, 98.52, 11.37]}, {"formula_id": "formula_44", "formula_text": "x n ) def = (\u03d5 0 (x n ), \u03d5 1 (x n ), \u03d5 2 (x n ), . . . \u03d5 n (x n )).", "formula_coordinates": [13.0, 108.0, 572.54, 396.0, 23.99]}, {"formula_id": "formula_45", "formula_text": "q y (x n ) = w n z:\u03c6(w n )=\u03c6n,nz=t q z (w n ) u n v:\u03c6(u n )=\u03c6n,nv=t 1 .", "formula_coordinates": [13.0, 220.73, 622.62, 170.54, 28.17]}, {"formula_id": "formula_46", "formula_text": "(a) \u2265 1 k! p\u2208P r n (q, p) (b) \u2265 1 k! p\u2208P r n (q , p) (c) = r n (q , P ). (7", "formula_coordinates": [13.0, 217.45, 706.64, 282.68, 27.21]}, {"formula_id": "formula_47", "formula_text": ")", "formula_coordinates": [13.0, 500.13, 714.1, 3.87, 8.64]}, {"formula_id": "formula_48", "formula_text": "p\u2208P r n (q, p) = p\u2208P x n \u2208X n y\u2208X p(x n y) log 1 q y (x n ) \u2212 H(p) = x n \u2208X n y\u2208X p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) = \u03c6n,t x n :\u03c6(x n )=\u03c6n y:ny=t p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) (d) \u2265 \u03c6n,t x n :\u03c6(x n )=\u03c6n y:ny=t p\u2208P p(x n y) log u n ,v:\u03c6(u n )=\u03c6n,nv=t 1 w n ,z:\u03c6(w n )=\u03c6n,nz=t q z (w n ) \u2212 H(p) = \u03c6n,t x n :\u03c6(x n )=\u03c6n y:ny=t p\u2208P p(x n y) log 1 q y (x n ) \u2212 H(p) = p\u2208P r n (q , p),", "formula_coordinates": [14.0, 116.41, 113.65, 379.19, 186.6]}, {"formula_id": "formula_49", "formula_text": "p\u2208P (r n (q, p) \u2212 r nat n (p)) \u2265 max p\u2208P (r n (q , p) \u2212 r nat n (p)) .", "formula_coordinates": [14.0, 199.9, 352.62, 213.66, 15.13]}, {"formula_id": "formula_50", "formula_text": "r nat n (\u2206 k ) = max p\u2208\u2206 k (r n (q, p) \u2212 r nat n (p)) = max P \u2208P\u03c3 max p\u2208P (r n (q, p) \u2212 r nat n (p)) \u2265 max P \u2208P\u03c3 max p\u2208P (r n (q , p) \u2212 r nat n (p)) = max p\u2208\u2206 k (r n (q , p) \u2212 r nat n (p)) = r n (q , \u2206 k ).", "formula_coordinates": [14.0, 214.16, 395.68, 176.35, 107.5]}, {"formula_id": "formula_51", "formula_text": "\u221e t=0\u015c t = 1 and C t = \u03d5 t \u2022 t n if t \u2265 t 0 , (\u03d5 t+1 + 1) \u2022 t+1 n else.", "formula_coordinates": [14.0, 200.31, 620.6, 179.01, 48.66]}, {"formula_id": "formula_52", "formula_text": "E[D(S||\u015c)] \u2264 t0\u22121 t=0 E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n + \u221e t=t0 E (S t \u2212 t\u03a6 t /n) 2 \u03a6 t t/n . Proof. Since log(1 + y) \u2264 y, \u221e t=0 S t = 1,and", "formula_coordinates": [15.0, 108.0, 218.05, 364.72, 56.79]}, {"formula_id": "formula_53", "formula_text": "\u221e t=0 C t = N , D(S||\u015c) = \u221e t=0 S t log S t S t = \u221e t=0 S t log N S t C t = \u221e t=0 S t log S t C t + \u221e t=0 S t log N = \u221e t=0 S t log 1 + S t \u2212 C t C t + log N \u2264 \u221e t=0 S t S t \u2212 C t C t + log N = \u221e t=0 (S t \u2212 C t ) S t \u2212 C t C t + \u221e t=0 C t S t \u2212 C t C t + log N = \u221e t=0 (S t \u2212 C t ) S t \u2212 C t C t + \u221e t=0 (S t \u2212 C t ) + log N = \u221e t=0 (S t \u2212 C t ) 2 C t + 1 \u2212 N + log N \u2264 \u221e t=0 (S t \u2212 C t ) 2 C t = t0\u22121 t=0 (S t \u2212 C t ) 2 C t + \u221e t=t0 (S t \u2212 C t ) 2 C t .", "formula_coordinates": [15.0, 165.19, 260.72, 280.53, 346.47]}, {"formula_id": "formula_54", "formula_text": "\u221e t=t0 E (S t \u2212 t\u03a6 t /n) 2 t\u03a6 t /n \u2264 1 t 0 . Proof. \u221e t=t0 (S t \u2212 t\u03a6 t /n) 2 t\u03a6 t /n \u2264 \u221e t=t0 (S t \u2212 t\u03a6 t /n) 2 \u03a6 t t 0 /n (a) \u2264 \u221e t=t0 x 1 Nx=t (p x \u2212 t/n) 2 t 0 /n = x \u221e t=t0 1 Nx=t (p x \u2212 t/n) 2 t 0 /n \u2264 x \u221e t=0 1 Nx=t (p x \u2212 t/n) 2 t 0 /n .", "formula_coordinates": [15.0, 243.94, 699.2, 124.12, 30.03]}, {"formula_id": "formula_55", "formula_text": "( m x=1 ax) 2 m \u2264 m i=1 a 2", "formula_coordinates": [16.0, 233.4, 256.76, 94.12, 16.53]}, {"formula_id": "formula_56", "formula_text": "\u221e t=t0 E (S t \u2212 t\u03a6 t /n) 2 ] \u03a6 t t/n \u2264 x E[ \u221e t=0 1 Nx=t (p x \u2212 t/n) 2 ] t 0 /n \u2264 x p x /n t 0 /n = 1 t 0 ,", "formula_coordinates": [16.0, 183.8, 297.01, 243.2, 88.14]}, {"formula_id": "formula_57", "formula_text": "\u221e t=0 1 Nx=t (p x \u2212 t/n) 2 ]", "formula_coordinates": [16.0, 353.7, 398.31, 91.05, 14.11]}, {"formula_id": "formula_58", "formula_text": "E[S t ] = t + 1 n \u2022 E[\u03a6 t+1 ].(8)", "formula_coordinates": [16.0, 255.7, 506.37, 248.3, 22.31]}, {"formula_id": "formula_59", "formula_text": "\u2264 (t + 1)(t + 2) n 2 \u2022 E[\u03a6 t+2 ].(9)", "formula_coordinates": [16.0, 266.84, 559.99, 237.17, 22.31]}, {"formula_id": "formula_60", "formula_text": "E S t \u2212 (t + 1)\u03a6 t+1 n 2 \u2264 (t + 1)(t + 2)E[\u03a6 t+2 ] n 2 + (t + 1) 2 E[\u03a6 t+1 ] n 2 . (10", "formula_coordinates": [16.0, 156.1, 614.75, 343.75, 25.51]}, {"formula_id": "formula_61", "formula_text": ")", "formula_coordinates": [16.0, 499.85, 625.01, 4.15, 8.64]}, {"formula_id": "formula_62", "formula_text": "E[\u03a6 t+1 ] to E[\u03a6 t ].", "formula_coordinates": [16.0, 203.58, 661.88, 69.96, 9.65]}, {"formula_id": "formula_63", "formula_text": "E[\u03a6 t+1 ] \u2264 E[\u03a6 t ] 2 t log n + t t + 1 + 1 t + 1 . Proof. Let r \u2265 t t+1 . E[\u03a6 t+1 ] = E x 1 Nx=t+1 = x e \u2212npx (np x ) t+1 (t + 1)! = x n t + 1 \u2022 e \u2212npx (", "formula_coordinates": [16.0, 212.35, 706.85, 187.31, 22.31]}, {"formula_id": "formula_64", "formula_text": "E[\u03a6 t+1 ] \u2264 E[\u03a6 t ] 2 t log n + t t + 1 + 1 t + 1", "formula_coordinates": [17.0, 212.35, 439.63, 183.34, 22.31]}, {"formula_id": "formula_65", "formula_text": "E 1 n i=1 X i + 1 \u2264 1 n i=1 E[X i ] . Proof. Let s i = E[X i ].", "formula_coordinates": [17.0, 108.0, 528.95, 269.64, 49.29]}, {"formula_id": "formula_66", "formula_text": "1 X i + X j + Y | Y = (1 \u2212 s i )(1 \u2212 s j ) Y + s i (1 \u2212 s j ) + (1 \u2212 s i )s j Y + 1 + s i s j Y + 2 = 1 Y + (s i + s j ) 1 Y + 1 \u2212 1 Y + s i s j 2 Y (Y + 1)(Y + 2)", "formula_coordinates": [17.0, 150.28, 622.19, 322.22, 50.43]}, {"formula_id": "formula_67", "formula_text": "E 1 n i=1 X i + 1 \u2264 E 1 Z + 1 ,", "formula_coordinates": [17.0, 236.45, 711.39, 139.11, 24.8]}, {"formula_id": "formula_68", "formula_text": "E 1 Z + 1 = n j=0 1 j + 1 n j s j (1 \u2212 s) n\u2212j = 1 (n + 1)s n j=0 n + 1 j + 1 s j+1 (1 \u2212 s) n+1\u2212(j+1) \u2264 1 (n + 1)s \u2264 1 ns = 1 n i=1 E[X i ] .", "formula_coordinates": [18.0, 184.55, 117.95, 242.41, 144.72]}, {"formula_id": "formula_69", "formula_text": "E[\u03a6 t+1 ] > 2, then E S 2 t \u03a6 t+1 + 1 \u2264 E[S 2 t ] E[\u03a6 t+1 ] \u2212 1 .", "formula_coordinates": [18.0, 236.35, 307.81, 132.96, 41.62]}, {"formula_id": "formula_70", "formula_text": "E[1 Nx=t+1 ] = e \u2212npx (np x ) t+1 (t + 1)! \u2264 e \u2212t\u22121 (t + 1) t+1 (t + 1)! \u2264 1 e . (11", "formula_coordinates": [18.0, 190.37, 379.66, 309.48, 23.89]}, {"formula_id": "formula_71", "formula_text": ")", "formula_coordinates": [18.0, 499.85, 388.29, 4.15, 8.64]}, {"formula_id": "formula_72", "formula_text": "Since S t = x p x 1 Nx=t and \u03a6 t+1 = x 1 Nx=t+1 , S 2 t \u03a6 t+1 + 1 = x y p x p y 1 Nx=t 1 Ny=t z 1 Nz=t+1 + 1 = x y p x p y 1 Nx=t 1 Ny=t z:z =x,z =y 1 Nz=t+1 + 1 ,", "formula_coordinates": [18.0, 108.0, 412.57, 354.73, 46.11]}, {"formula_id": "formula_73", "formula_text": "E p x p y 1 Nx=t 1 Ny=t z 1 Nz=t+1 + 1 = E p x p y 1 Nx=t 1 Ny=t z:z =x,z =y 1 Nz=t+1 + 1 = E p x p y 1 Nx=t 1 Ny=t E 1 z:z =x,z =y 1 Nz=t+1 + 1 (a) \u2264 E p x p y 1 Nx=t 1 Ny=t z:z =x,z =y E[1 Nz=t+1 ] (b) \u2264 E p x p y 1 Nx=t 1 Ny=t E[\u03a6 t+1 \u2212 1] ,(", "formula_coordinates": [18.0, 108.0, 509.43, 350.39, 137.84]}, {"formula_id": "formula_74", "formula_text": "E[1 Nz=t+1 ] = z E[1 Nz=t+1 ] \u2212 E[1 Nx=t+1 ] \u2212 E[1 Ny=t+1 ] \u2265 E[\u03a6 t+1 ] \u2212 1.", "formula_coordinates": [18.0, 171.3, 659.28, 309.01, 19.61]}, {"formula_id": "formula_75", "formula_text": "t if E[\u03a6 t+1 ] \u2264 2, then E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t n + 4 log n n t + 2 t + 1 + 6 n . Proof. Let Z = S t \u2212 (t + 1)\u03a6 t+1 /n. E Z \u2212 t + 1 n 2 (a) = E[Z 2 ] + (t + 1) 2 n 2 (b) \u2264 (t + 1)(t + 2)E[\u03a6 t+2 ] n 2 + (t + 1) 2 E[\u03a6 t+1 ] n 2 + (t + 1) 2 n 2 (c) \u2264 2 (t + 1)(t + 2) n 2 \u2022 2 log n t + 1 + t + 1 t + 2 + (t + 1)(t + 2) n 2 (t + 2) + 3(t + 1) 2 n 2 .", "formula_coordinates": [19.0, 108.0, 85.02, 385.6, 151.33]}, {"formula_id": "formula_76", "formula_text": "E[\u03a6 t+1 ] \u2264 2. Hence, E (Z \u2212 (t + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 E[(Z \u2212 (t + 1)/n) 2 ] (t + 1)/n \u2264 2(t + 2) n \u2022 2 log n t + 1 + t + 1 t + 2 + 1 n + 3(t + 1) n = 5t n + 4 log n(t + 2) n(t + 1) + 6 n .", "formula_coordinates": [19.0, 150.85, 253.58, 309.11, 107.83]}, {"formula_id": "formula_77", "formula_text": "t if E[\u03a6 t+1 ] > 2, then E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t n + 4 log n n t + 2 t + 1 + 6 n .", "formula_coordinates": [19.0, 171.71, 384.43, 268.59, 38.74]}, {"formula_id": "formula_78", "formula_text": "(S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n = S 2 t (\u03a6 t+1 + 1)(t + 1)/n + (t + 1)(\u03a6 t+1 + 1) n \u2212 2S t .", "formula_coordinates": [19.0, 137.67, 450.12, 337.86, 24.8]}, {"formula_id": "formula_79", "formula_text": "E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n = E S 2 t (\u03a6 t+1 + 1)(t + 1)/n \u2212 (t + 1)E[\u03a6 t+1 ] n + t + 1 n .(12)", "formula_coordinates": [19.0, 112.98, 493.07, 391.02, 24.8]}, {"formula_id": "formula_80", "formula_text": "E S 2 t (\u03a6 t+1 + 1)(t + 1)/n \u2264 E[S 2 t ] E[\u03a6 t+1 \u2212 1](t + 1)/n \u2264 t + 1 n E[\u03a6 t+1 ] 2 E[\u03a6 t+1 \u2212 1] + t + 2 n E[\u03a6 t+2 ] E[\u03a6 t+1 \u2212 1] .", "formula_coordinates": [19.0, 163.87, 536.16, 284.26, 53.1]}, {"formula_id": "formula_81", "formula_text": "E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 (t + 1)E[\u03a6 t+1 ] + (t + 2)E[\u03a6 t+2 ] nE[\u03a6 t+1 \u2212 1] + t + 1 n (a) \u2264 2 (t + 1)E[\u03a6 t+1 ] + (t + 2)E[\u03a6 t+2 ] nE[\u03a6 t+1 ] + t + 1 n (b) \u2264 2 t + 1 n + t + 2 n 2 log n t + 1 + t + 1 t + 2 + 1 2(t + 2) + t + 1 n = 5t n + 4 log n n t + 2 t + 1 + 6 n . Since E[\u03a6 t+1 ] \u2265 2, E[\u03a6 t+1 ] \u2212 1 \u2265 E[\u03a6 t+1", "formula_coordinates": [19.0, 108.0, 607.41, 398.24, 125.3]}, {"formula_id": "formula_82", "formula_text": "t0\u22121 t=0 E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t 2 0 2n + 4 log n n (t 0 + log t 0 + 1) + 7t 0 2n .", "formula_coordinates": [20.0, 141.59, 117.15, 328.82, 30.31]}, {"formula_id": "formula_83", "formula_text": "E[\u03a6 t+1 ], E (S t \u2212 (t + 1)(\u03a6 t+1 + 1)/n) 2 (\u03a6 t+1 + 1)(t + 1)/n \u2264 5t n + 4 log n n t + 2 t + 1 + 6 n .", "formula_coordinates": [20.0, 171.71, 161.69, 268.59, 41.53]}, {"formula_id": "formula_84", "formula_text": "E[D(S||\u015c)] \u2264 1 t 0 + 5t 2 0 2n + 4 log n n (t 0 + log t 0 + 1) + 7t 0 2n . Substituting t 0 = n 1/3 /5 1/3 results in Theorem 1. r nat poi(n) (q , \u2206 k ) \u2264 max p\u2208\u2206 k E[D(S||\u015c)] \u2264 2.6 n 1/3 + 2.4 log n(n 1/3 + log n + 1) n + 2.1 n 2/3 \u2264 3 + o n (1) n 1/3 .", "formula_coordinates": [20.0, 108.0, 250.98, 394.05, 74.72]}, {"formula_id": "formula_85", "formula_text": "p 0 i = log n 6n c 2 n m n c 2 m log 2 n + i and p 0 m = 1 \u2212 m\u22121 i=1 p 0 i . Observe that for all 1 \u2264 i \u2264 m \u2212 1, 1/(6m) \u2264 p 0 i \u2264 1/(3m) and p 0 m \u2265 2/3.", "formula_coordinates": [20.0, 108.0, 475.82, 396.0, 57.9]}, {"formula_id": "formula_86", "formula_text": "i = p 0 i +v i for 1 \u2264 i \u2264 m \u2212 1 and pv(m) = 1 \u2212 m\u22121 i=1 pv i .", "formula_coordinates": [20.0, 108.0, 558.43, 391.46, 25.81]}, {"formula_id": "formula_87", "formula_text": "\u2264 m i=1 (pv i \u2212 pv i ) 2 pv i (b) \u2264 2 m i=1 (pv i \u2212 pv i ) 2 p 0 i \u2264 2 m\u22121 i=1 (v i \u2212v i ) 2 ( c * /nm) 2 1/(6m) \u2264 12 m\u22121 i=1 (v i \u2212v i ) 2 c * n \u2264 24 m\u22121 i=1 |v i \u2212v i |c * n = 24||v \u2212v || 1 c * n \u2264 48mc * n .(a)", "formula_coordinates": [21.0, 264.46, 513.2, 128.25, 218.6]}, {"formula_id": "formula_88", "formula_text": "sup i E i [||p i \u2212 q|| 1 ] \u2265 \u03b1 2 1 \u2212 n\u03b2 + log 2 log r .", "formula_coordinates": [22.0, 216.39, 315.8, 179.22, 23.11]}], "doi": ""}