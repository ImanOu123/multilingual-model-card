{"title": "QuickScorer: a Fast Algorithm to Rank Documents with Additive Ensembles of Regression Trees", "authors": "Claudio Lucchese; Franco Maria Nardini; Salvatore Orlando; Raffaele Perego; Nicola Tonellotto; Rossano Venturini", "pub_date": "", "abstract": "Learning-to-Rank models based on additive ensembles of regression trees have proven to be very effective for ranking query results returned by Web search engines, a scenario where quality and efficiency requirements are very demanding. Unfortunately, the computational cost of these ranking models is high. Thus, several works already proposed solutions aiming at improving the efficiency of the scoring process by dealing with features and peculiarities of modern CPUs and memory hierarchies. In this paper, we present QuickScorer, a new algorithm that adopts a novel bitvector representation of the tree-based ranking model, and performs an interleaved traversal of the ensemble by means of simple logical bitwise operations. The performance of the proposed algorithm are unprecedented, due to its cacheaware approach, both in terms of data layout and access patterns, and to a control flow that entails very low branch mis-prediction rates. The experiments on real Learning-to-Rank datasets show that QuickScorer is able to achieve speedups over the best state-of-the-art baseline ranging from 2x to 6.5x.", "sections": [{"heading": "", "text": "problem can leverage machine learning techniques. A LtRbased function, which scores a set of candidate documents according to their relevance to a given user query, is learned from a ground-truth composed of many training examples. The examples are basically a collection of queries Q, where each query q \u2208 Q is associated with a set of assessed documents D = {d0, d1, . . .}. Each pair (q, di) is in turn labeled by a relevance judgment yi, usually a positive integer in a fixed range, stating the degree of relevance of the document for the query. These labels induce a partial ordering over the assessed documents, thus defining their ideal ranking [6]. The scoring function learned by a LtR algorithm aims to approximate the ideal ranking from the examples observed in the training set.\nThe ranking process is particularly challenging for Web search engines, which, besides the demanding requirements for result pages of high quality in response to user queries, have also to deal with efficiency constraints, which are not so common in other ranking-based applications. Indeed, two of the most effective LtR-based rankers are based on additive ensembles of regression trees, namely Gradient-Boosted Regression Trees (GBRT) [4], and Lambda-MART (\u03bb-MART) [18]. Due to the thousands of trees to be traversed at scoring time for each document, these rankers are also the most expensive in terms of computational time, thus impacting on response time and throughput of query processing. Therefore, devising techniques and strategies to speed-up document ranking without losing in quality is definitely an urgent research topic in Web search [14,3,10,5,19].\nUsually, LtR-based scorers are embedded in complex twostage ranking architectures [3,16], which avoid applying them to all the documents possibly matching a user query. The first stage retrieves from the inverted index a sufficiently large set of possibly relevant documents matching the user query. This phase is aimed at optimizing the recall and is usually carried out by using a simple and fast ranking function, e.g., BM25 combined with some document-level scores [9]. LtR-based scorers are used in the second stage to re-rank the candidate documents coming from the first stage, and are optimized for high precision. In this twostage architecture, the time budget available to re-rank the candidate documents is limited, due to the incoming rate of queries and the users' expectations in terms of quality-ofservice. Strongly motivated by time budget considerations, the IR community has started to investigate low-level optimizations to reduce the scoring time of the most effective LtR rankers based on ensembles of regression trees, by dealing with features and peculiarities of modern CPUs and memory hierarchies [1,12]. In this work we advance the state of the art in this field, and propose QuickScorer (QS), a new algorithm to score documents with an ensemble of regression trees. The main contributions of our proposal are:\n\u2022 a novel representation of an ensemble of binary regression trees based on bitvectors, allowing QS to perform a fast interleaved traversal of the trees by using efficient logical bitwise operations. The performance benefits of the resulting traversal are unprecedented, due to a cache-aware approach, both in terms of data layout and access patterns, and to a program control flow that entails very low branch mis-prediction rates;\n\u2022 an extensive experimental assessment conducted on publicly available LtR datasets with various \u03bb-MART models, differing for both the size of the ensemble and the number of tree leaves. The results of the experiments show that QS achieves impressive speedups over the best state-of-the-art competitor, ranging from 2x up to 6.5x. Moreover, to motivate the very good performance of QS over competitors, we evaluate in-depth some CPU counters that measure important performance events, such as number of instructions executed, cache-misses suffered, or branches mis-predicted;\n\u2022 a block-wise version of QS for scoring large tree ensembles and large sets of documents. BlockWise-QS (BWQS) splits the set of documents and the tree ensemble in disjoint groups that can be processed separately. Our experiments show that BWQS performs up to 1.55 times better than the original QS, thanks to cache reuse which reduces cache misses.\nThe rest of the paper is structured as follows: Section 2 provides background information and discusses the related work, while Section 3 details the QS algorithm and its features. Then, Section 4 reports on the results of our comprehensive evaluation. Finally, we conclude our investigation in Section 5 by reporting some conclusions and suggestions for future research.", "publication_ref": ["b7", "b5", "b19", "b15", "b4", "b11", "b6", "b20", "b4", "b17", "b10", "b2", "b13"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND AND RELATED WORK", "text": "Gradient-Boosted Regression Trees (GBRT) [4] and Lambda-MART (\u03bb-MART) [18] are two of the most effective Learning-to-Rank (LtR) algorithms. They both generate additive ensembles of regression trees aiming at predicting the relevance labels yi of a query document pair (q, di). The GBRT algorithm builds a model by approximating the root mean squared error on a given training set. This loss function makes GBRT a point-wise LtR algorithm, i.e., query-document pairs are exploited independently. The \u03bb-MART algorithm improves over GBRT by directly optimizing list-wise information retrieval measures such as NDCG [6]. Thus, \u03bb-MART aims at finding a scoring function that generates an ordering of documents as close as possible to the ideal ranking. In terms of scoring process there is thus no difference between \u03bb-MART and GBRT, since they both generate a set of weighted regression trees.\nIn this work, we discuss algorithms and optimizations for scoring efficiently documents by means of regression tree ensembles. Indeed, the findings of this work apply beyond LtR, and in any application where large ensembles of regression trees are used for classification or regression tasks.\nEach query-document pair (q, di) is represented by a realvalued vector x of features, namely x \u2208 R |F | where F = {f0, f1, . . .} is the set of features characterizing the candidate document di and the user query q, and x[i] stores feature fi. Let T be an ensemble of trees representing the ranking model. Each tree T = (N, L) in T is a decision tree composed of a set of internal nodes N = {n0, n1, . . .}, and a set of leaves L = {l0, l1, . . .}. Each n \u2208 N is associated with a Boolean test over a specific feature with id \u03c6, i.e., f \u03c6 \u2208 F , and a constant threshold \u03b3 \u2208 R. This test is in the form x[\u03c6] \u2264 \u03b3. Each leaf l \u2208 L stores the prediction l.val \u2208 R, representing the potential contribution of tree T to the final score of the document.\nAll the nodes whose Boolean conditions evaluate to False are called false nodes, and true nodes otherwise. The scoring of a document represented by a feature vector x requires the traversing of all the trees in the ensemble, starting at their root nodes. If a visited node in N is a false one, then the right branch is taken, and the left branch otherwise. The visit continues recursively until a leaf node is reached, where the value of the prediction is returned. Such leaf node is named exit leaf and denoted by e(x) \u2208 L. We omit x when it is clear from the context.\nHereinafter, we assume that nodes of T are numbered in breadth-first order and leaves from left to right, and let \u03c6i and \u03b3i be the feature id and threshold associated with i-th internal node, respectively. It is worth noting that the same feature can be involved in multiple nodes of the same tree. For example, in the tree shown in Figure 1, the features f0 and f2 are used twice. Assuming that x is such that x[2] > \u03b30, x[3] \u2264 \u03b32, and x[0] \u2264 \u03b33, the exit leaf e of the tree in the Figure 1 is the leaf l2.\nThe tree traversal process is repeated for all the trees of the ensemble T , denoted by T = {T0, T1, . . .}. The score s(x) of the whole ensemble is finally computed as a weighted sum over the contributions of each tree T h = (N h , L h ) in T as:\ns(x) = |T |\u22121 h=0 w h \u2022 e h (x).val\nwhere e h (x).val is the predicted value of tree T h , having weight w h \u2208 R.\nIn the following we review state-of-the-art optimization techniques for the implementation of additive ensemble of regression trees and their use in document scoring.\nTree traversal optimization. A na\u00efve implementation of a tree traversal may exploit a node data structure that stores the feature id, the threshold and the pointers to the left and right children nodes. The traversal starts from the root and moves down to the leaves accordingly to the results of the Boolean conditions on the traversed nodes. This method can be enhanced by using the optimized data layout in [1]. The resulting algorithm is named Struct+. This simple approach entails a number of issues. First, the next node to be processed is known only after the test is evaluated. As the next instruction to be executed is not known, this induces frequent control hazards, i.e., instruction dependencies introduced by conditional branches. As a consequence, the efficiency of a code strongly depends on the branch misprediction rate [8]. Finally, due to the unpredictability of the path visited by a given document, the traversal has low temporal and spatial locality, generating low cache hit ratio. This is apparent when processing a large number of documents with a large ensemble of trees, since neither the documents nor the trees may fit in cache.\nAnother basic, but well performing approach is If-Then-Else. Each decision tree is translated into a sequence of if-then-else blocks, e.g., in C++ language. The resulting code is compiled to generate an efficient document scorer. If-Then-Else aims at taking advantage of compiler optimization strategies, which can potentially re-arrange the tree ensemble traversal into a more efficient procedure. The size of the resulting code is proportional to the total number of nodes in the ensemble. This makes it impossible to exploit successfully the instruction cache. If-Then-Else was proven to be efficient with small feature sets [1], but it still suffers from control hazards.\nAsadi et al. [1] proposed to rearrange the computation to transform control hazards into data hazards, i.e., data dependencies introduced when one instruction requires the result of another. To this end, node ns of a tree stores, in addition to a feature id \u03c6s and a threshold \u03b3s, an array idx of two positions holding the addresses of the left and right children nodes data structures. Then, the output of the test x[\u03c6s] > \u03b3s is directly used as an index of such array in order to retrieve the next node to be processed. The visit of a tree of depth d is then statically \"un-rolled\" in d operations, starting from the root node n0, as follows:\nd steps \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 i \u2190 n 0 .idx [x[\u03c6 0 ] > \u03b3 0 ] i \u2190 n i .idx [x[\u03c6 i ] > \u03b3 i ] . . . . . . i \u2190 n i .idx [x[\u03c6 i ] > \u03b3 i ]\nLeaf nodes are encoded so that the indexes in idx generate self loops, with dummy \u03c6s and \u03b3s. At the end of the visit, the exit leaf is identified by variable i, and a look-up table is used to retrieve the prediction of the tree. This approach, named Pred, removes control hazards as the next instruction to be executed is always known. On the other hand, data dependencies are not solved as the output of one instruction is required to execute the subsequent. Memory access patterns are not improved either, as they depend on the path along the tree traversed by a document. Finally, Pred introduces a new source of overhead: for a tree of depth d, even if document reaches a leaf early, the above d steps are executed anyway.\nTo reduce data hazards the same authors proposed a vectorized version of the scoring algorithm, named VPred, by interleaving the evaluation of a small set of documents (16 was the best setting). VPred was shown to be 25% to 70% faster than Pred on synthetic data, and to outperform other approaches. The same approach of Pred was also adopted in some previous works exploiting GPUs [11], and a more recent survey evaluates the trade-off among multi-core CPUs, GPUs and FPGA [13].\nIn this work we compare against VPred which can be considered the best performing algorithm at the state of the art. In the experimental section, we show that the proposed QS algorithm has reduced control hazard, smaller branch mis-prediction rate and better memory access patterns.\nMemory latency issues of scoring algorithms are tackled in Tang et al. [12]. In most cases, the cache memory may be insufficient to store the candidate documents to be scored and/or the set of regression trees. The authors proposed a cache-conscious optimization by splitting documents and regression trees in blocks, such that one block of documents and one block of trees can both be stored in cache at the same time. Computing the score of all documents requires to evaluate all the tree blocks against all the document blocks. Authors applied this computational scheme on top of both If-Then-Else and Pred, with an average improvement of about 28% and 24% respectively. The blocking technique is indeed very general and can be used by all algorithms. The same computational schema is applied to QS in order to improve the cache hit ratio when large ensembles are used.\nOther approaches and optimizations. Unlike our method that aims to devise an efficient strategy for fully evaluating the ensemble of trees, other approaches tries to approximate the computation over the ensemble for reducing the scoring time. Cambazoglu et al. [3] proposed to early terminate the scoring of documents that are unlikely to be ranked within the top-k results. Their work applies to an ensemble of additive trees like ours, but the authors aims to save scoring time by reducing the number of tree traversals, and trades better efficiency for little loss in raking quality. Although our method is thought for globally optimizing the traversal of thousands of trees, the idea of early termination can be applied as well along with our method, by evaluating some proper exit strategy after the evaluation of some subsets of the regression trees.\nWang et al. [15,17,16] deeply investigated different efficiency aspects of the ranking pipeline. In particular, in [16] they propose a novel cascade ranking model, which unlike previous approaches, can simultaneously improve both topk ranked effectiveness and retrieval efficiency. Their work is mainly related to the tuning of a two-stage ranking pipeline.", "publication_ref": ["b5", "b19", "b7", "b2", "b9", "b2", "b2", "b12", "b14", "b13", "b4", "b16", "b18", "b17", "b17"], "figure_ref": [], "table_ref": []}, {"heading": "QUICKSCORER: AN EFFICIENT TREE ENSEMBLE TRAVERSAL ALGORITHM", "text": "In order to efficiently exploit memory hierarchies and to reduce the branch mis-prediction rate, we propose an algorithm based on a totally novel traversal of the trees ensemble, called QuickScorer (QS). The building block of our approach is an alternative method for tree traversal based on bitvector computations, which is presented in Subsection 3.1. Given a tree and a vector of document features, our traversal processes all its nodes and produces a bitvector which encodes the exit leaf for the given document. In isolation this traversal is not particularly advantageous over the others, since in principle it requires to evaluate all the nodes of a tree. However, it has the nice property of being insensitive to the order in which the nodes are processed. This makes it possible to interleave the evaluation of the trees in the ensemble in a cache-aware fashion. In addition, the proposed bitvector encoding allows to save the computation of many test conditions.\nThe interleaved evaluation of a trees ensemble is discussed in Subsection 3.2. Intuitively, rather than traversing the ensemble tree after tree, our algorithm performs a global visit of the ensemble by traversing portions of all the trees together, feature by feature. For each feature, we store all the associated thresholds occurring anywhere in the ensemble in a sorted array, to easily to compute the result of all the test conditions involved. A bitvector for each tree is updated after each test, in such a way to encode, at the end of the process, the exit leaves in each tree for a given document. These bitvector are eventually used to lookup the predicted value of each tree.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Tree traversal using bitvectors", "text": "We start by presenting a simpler version of our tree traversal and, then, we introduce two crucial refinements for the performance of this algorithm when used in the interleaved evaluation of all the trees as described in Subsection 3.2.\nGiven an input feature vector x and a tree T h = (N h , L h ), our tree traversal algorithm processes the internal nodes of T h with the goal of identifying a set of candidate exit leaves, denoted by C h with C h \u2286 L h , which includes the actual exit leaf e h . Initially C h contains all the leaves in L h , i.e., C h = L h . Then, the algorithm evaluates one after the other in an arbitrary order the test conditions of all the internal nodes of T h . Considering the result of the test for a certain internal node n \u2208 N h , the algorithm is able to infer that some leaves cannot be the exit leaf and, thus, it can safely remove them from C h . Indeed, if n is a false node (i.e., its test condition is false), the leaves in the left subtree of n cannot be the exit leaf and they can be safely removed from C h . Similarly, if n is a true node, the leaves in the right subtree of n can be removed from C h . It is easy to see that, once all the nodes have been processed, the only leaf left in C h is the exit leaf e h .\nThe first refinement turns the above algorithm into a lazy one. This lazy algorithm uses an oracle, called FindFalse, that, given T h and x, returns the false nodes in N h without the need of evaluating all the associated test conditions. Then, the algorithm removes from C h the leaves in the left subtrees of all the false nodes returned by the oracle. For the moment we concentrate on the set C h obtained at the end of the algorithm and we defer the materialization of the above oracle to Subsection 3.2 where the interleaved evaluation of all the trees makes its implementation possible. Observe that C h may now contain several leaves. As an extreme example, the set C h , in absence of false nodes, will contain all the leaves in L h . Interestingly, we can prove (see Theorem 1 below) that the exit leaf e h is always the one associated with the smallest identifier in C h , i.e., the leftmost leaf in the tree. A running example is reported in Figure 2 which shows the actual traversal (bold arrows) for a vector x, and also the true and false nodes. The figure shows also the set C h after the removal of the leaves of the left subtrees of false nodes: C h is {l2, l3, l5} and, indeed, the exit leaf is the leftmost leaf in C h , i.e., e h = l2. The second refinement implements the operations on C h with fast operations on compact bitvectors. The idea is to represent C h with a bitvector v h , where each bit corresponds to a distinct leaf in L h , i.e., v h is the characteristic vector of C h . Every internal node n is associated with a node bitvector (of the same length), acting as a bitmask that encodes (with 0's) the set of leaves to be removed from C h whenever n is a false node. This way, the bitwise logical AND between v h and the node bitvector of a false node n corresponds to the removal of the leaves in the left subtree of n from C h . We finally observe that the exit leaf corresponds to the leftmost bit set to 1 in v h . Figure 2 shows how the initial bitvector v h is updated by using bitwise logical AND operations.\nThe full approach is described in Algorithm 1. Given a binary tree T h = (N h , L h ) and an input feature vector x, let u.bitvector be the precomputed bitwise mask associated with a generic n \u2208 N h . First the result bitvector v h is initialized with all bits set to 1. Then, FindFalse(x, T h ) returns all the false nodes in N h . For each of such nodes, v h is masked with the corresponding node bitvector. Finally, the position of the leftmost bit of v h identifies the exit leaf e h , whose output value is returned. The correctness of this approach is stated by the following theorem. Theorem 1. Algorithm 1 is correct.\nProof. We prove that for each binary decision tree T h and input feature vector x, Algorithm 1 always computes Algorithm 1: Scoring a feature vector x using a binary decision tree T h Input :\n\u2022 x: input feature vector \u2022 T h = (N h , L h ): binary decision tree, with -N h = {n 0 , n 1 , . . .}: internal nodes of T h -L h = {l 0 , l 1 , . . .}: leaves of T h -n.\nbitvector: node bitvector associated with n \u2208 N h -l j .val: output value associated with l j \u2208 L h Output:\n\u2022 tree traversal output value Score(x,T h ):\n1 v h \u2190 11 . . . 11 2 U \u2190 FindFalse(x, T h ) 3 foreach node u \u2208 U do 4 v h \u2190 v h \u2227 u.bitvector 5 j \u2190 index of leftmost bit set to 1 of v h 6 return l j .val\na result bitvector v h , where the leftmost bit set to 1 corresponds to the exit leaf e h .\nFirst, we prove that the bit corresponding to the exit leaf e h in the result bitvector v h is always set to 1. Consider the internal nodes along the path from the root to e h , and observe that only the bitvectors applied for those nodes may change the e h 's bit to 0. Since e h is the exit leaf, it belongs to the left subtree of any true node and to the right subtree of any false node in this path. Thus, since the bitvectors are used to set to 0 leaves in the left subtrees of false nodes, the bit corresponding to e h remains unmodified, and, thus, will be 1 at the end of Algorithm 1.\nSecond, we prove that the leftmost bit equal to 1 in v h corresponds to the exit leaf e h . Let l\u2190 be the leaf corresponding to the leftmost bit set to 1 in v h . Assume by contradiction that e h is not the leftmost bit set to 1 in v h , namely, l\u2190 = e h . Let u be their lowest common ancestor node in the tree. Since l\u2190 is smaller than e h , the leaf l\u2190 belongs to u's left subtree while the leaf e h belongs to u's right subtree. This leads to a contradiction. Indeed, on one hand, the node u should be a true node otherwise its bitvector would have been applied setting l\u2190's bit to 0. On the other hand, the node u should be a false node since e h is in its right subtree. Thus, we conclude that l\u2190 = e h proving the correctness of Algorithm 1.\nAlgorithm 1 represents a general technique to compute the output value of a single binary decision tree stored as a set of precomputed bitvectors. Given an additive ensemble of binary decision trees, to score a document x we have to loop over all the trees T h \u2208 T by repeatedly applying Algorithm 1. Unfortunately, this na\u00efve algorithm is inefficient, since this method does not permit us to implement efficiently FindFalse(x, T h ).\nIn the following section we present QS, which overcomes this issue by performing a global visit of the whole tree ensemble T . The QS algorithm realizes the goal of identifying efficiently the false nodes of all the tree ensemble by exploiting an interleaved evaluation of all the trees in the ensemble.", "publication_ref": [], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "The QS Algorithm", "text": "Our QS algorithm scores a feature vector x with an interleaved execution of several tree traversals, one for each tree in the ensemble. The algorithm does not loop over all the trees in T one at the time, as one would expect, but does loop instead over all the features in F, hence incrementally discovering for each f k \u2208 F the false nodes involving f k in any tree of the ensemble. This is a very convenient order for two reasons: i) we are able to identify all the false nodes for all the trees without even considering their true nodes, thus effectively implementing the oracle introduced in the previous section; ii) we are able to operate in a cache-aware fashion with a small number of Boolean comparisons and branch mis-predictions.\nDuring its execution, QS has to maintain the bitvectors v h 's, encoding the set C h 's for all the tree T h in the ensemble. The bitvector v h of a certain tree is updated as soon as a false node for that tree is identified. Once the algorithm has processed all the features in F, each of these v h is guaranteed to encode the exit leaf in the corresponding tree. Now the algorithm can compute the overall score of x by summing up (and, possibly, weighting) the scores of all these exit leaves.\nLet us concentrate on the processing of a feature f k and describe the portion of the data structure of interest for this feature. The overall algorithm simply iterates this process over all features in F. Each node involving f k in any tree T h \u2208 T is represented by a triple containing: (i) the feature threshold involved in the Boolean test; (ii) the id of the tree that contains the node, where the id is used to identify the bitvector v h to update; (iii) the node bitvector used to possibly update v h . We sort these triples in ascending order of their feature thresholds.\nThis sorting is crucial for obtaining a fast implementation of our oracle. Recall that all the conditions occurring in the internal nodes of the trees are of the form x[k] \u2264 \u03b3 h s . Hence, given the sorted list of all the thresholds involving f k \u2208 F, the feature value x[k] splits the list in two, possibly empty, sublists. The first sublist contains all the thresholds \u03b3 h s for which the test condition x[k] \u2264 \u03b3 h s evaluates to False, while the second sublists contains all thresholds for which the test condition evaluates to True. Thus, if we sequentially scan the sorted list of the thresholds associated with f k , all the values in the first sublist will cause negative tests. Associated with these thresholds entailing false tests, we have false nodes belonging to the trees in T . Therefore, for all these false nodes we can take in sequence the corresponding bitvector, and perform a bitwise logical AND with the appropriate result bitvector v h .\nThis large sequence of tests that evaluates to False corresponds to the repeated execution of conditional branch instructions, whose behavior is indeed very predictable. This Algorithm 2: The QuickScorer Algorithm Input :\n\u2022 x: input feature vector \u2022 T : ensemble of binary decision trees, with -w 0 , . . . , w |T |\u22121 : weights, one per tree thresholds: sorted sublists of thresholds, one sublist per feature -tree_ids: tree's ids, one per threshold bitvectors: node bitvectors, one per threshold offsets: offsets of the blocks of triples v: result bitvectors, one per each tree leaves: output values, one per each tree leaf Output:\n\u2022 Final score of x QuickScorer(x,T ):\n1 foreach h \u2208 0, 1, . . . , |T | \u2212 1 do 2 v[h]\u2190 11 . . . 11 3 foreach k \u2208 0, 1, . . . , |F | \u2212 1 do // Step 4 i \u2190 offsets[k] 5 end \u2190 offsets[k + 1] 6 while x[k] > thresholds[i] do 7 h \u2190 tree_ids[i] 8 v[h] \u2190 v[h] \u2227 bitvectors[i] 9 i \u2190 i + 1 10 if i \u2265 end then 11 break 12 score \u2190 0 13 foreach h \u2208 0, 1, . . . , |T | \u2212 1 do // Step 14 j \u2190 index of leftmost bit set to 1 of v[h] 15 l \u2190 h \u2022 |L h | + j 16 score \u2190 score + w h \u2022 leaves[l]\n17 return score is confirmed by our experimental results, showing that our code incurs in very few branch mis-predictions. We now present the layout in memory of the required data structure since it is crucial for the efficiency of our algorithm. The triples of each feature are stored in three separate arrays, one for each component: thresholds, tree_ids, and bitvectors. The use of three distinct arrays solves some data alignment issues arising when tuples of heterogeneous data types are stored contiguously in memory. The arrays of the different features are then juxtaposed one after the other as illustrated in Figure 3. Since arrays of different features may have different lengths, we use an auxiliary array offsets which marks the starting position of each array in the global array. We also juxtapose the bitvectors v h into a global array v. Finally, we use an array leaves which stores the output values of the leaves of each tree (ordered from left to right) grouped by their tree id.\nAlgorithm 2 reports the steps of QS as informally described above. After the initialization of the result bitvectors of each tree (loop starting al line 1), we have the first step of QS that exactly corresponds to what we discussed above (loop starting at line 3). The algorithm iterates over all features, and inspects the sorted lists of thresholds to update the result bitvectors. Upon completion of the first step, we have the second step of the algorithm (loop starting at line 13), which simply inspects all the result bitvectors, and for each of them identifies the position of the leftmost bit set to 1, and uses this position to access the value associated with the corresponding leaf stored array leaves. The value of the leaf is finally used to update the final score. Implementation details. In the following we discuss some details about our data structures, their size and access modes.\nA few important remarks concern the bitvectors stored in v and bitvectors. The learning algorithm controls the accuracy of each single tree with a parameter \u039b, which determines the maximal number of leaves for each T h = (N h , L h ) in T , namely |L h | \u2264 \u039b. Usually, the value of \u039b is kept small (\u2264 64). Thus, the length of bitvectors, which have to encode tree leaves, is equal to (or less than) a typical machine word of modern CPUs (64 bits). As a consequence, the bitwise operations performed by Algorithm 2 on them can be realized very efficiently, because they involve machine words (or halfwords, etc).\nWe avoid any possible performance overhead due to shifting operations to align the operands of bitwise logical ANDs by forcing the bitvectors to have uniform length of B bytes. To this end, we pad each bitvector on its right side with a string of 0 bits, if necessary. We always select the minimum number of bytes B \u2208 {1, 2, 4, 8} fitting \u039b.\nLet us now consider Table 1, which shows an upper bound for the size of each linear array used by our algorithm. The array offsets has |F| entries, one entry for each distinct feature. The array v, instead, has an entry for each tree in T , thus, |T | entries overall. The sizes of the other data structures depends on the number of total internal nodes or leaves in the ensemble T , besides the datatype sizes. Any internal node of some tree of T contributes with an entry in each array thresholds, bitvectors and tree_ids. Therefore the total number of entries of each of these arrays, i.e.,\n|T |\u22121 0 |N h |, can be upper bounded by |T | \u2022 \u039b, because for every tree T h we have |N h | < |N h | + 1 = |L h | \u2264 \u039b.\nFinally, the array leaves has an entry for each leaf in a tree of T , hence, no more than |T | \u2022 \u039b in total. \nthresholds |T | \u2022 \u039b \u2022 sizeof(float) 1. Sequential (R) tree_ids |T | \u2022 \u039b \u2022 sizeof(uint) bitvectors |T | \u2022 \u039b \u2022 B offsets |F | \u2022 sizeof(uint) v |T | \u2022 B 1. Random (R/W) 2. Sequential (R) leaves |T | \u2022 \u039b \u2022 sizeof(double) 2. Seq. Sparse (R)\nThe last column of Table 1 reports the data access modes to the arrays, where the leading number, either 1 or 2, corresponds to the step of the algorithm during which the data structures are read/written. Recall that the first step of QS starts at line 3 of Algorithm 2, while the second at line 13. We first note that v is the only array used in both phases of function QuickScorer(x, T ). During the first step v is accessed randomly in reading/writing to update the v h 's. During the second step the same array is accessed sequentially in reading mode to identify the exit leafs l h of each tree T h , and then to access the array leaves to read the contribution of tree T h to the output of the regression function. Even if the trees and their leaves are accessed sequentially during the second step of QS, the reading access to array leaves is sequential, but very sparse: only one leaf of each block of |L h | elements is actually read.\nFinally, note that the arrays storing the triples, i.e., thresholds, tree_ids, and bitvectors, are all sequentially read during the first step, though not completely, since for each feature we stop its inspection at the first test condition that evaluates to True. The cache usage can greatly benefit from the layout and access modes of our data structures, thanks to the increased references locality.\nWe finally describe an optimization which aims at reducing the number of comparisons performed at line 6 of Algorithm 2. The (inner) while loop in line 6 iterates over the list of threshold values associated with a certain feature f k \u2208 F until we find the first index j where the test fails, namely, the value of the k th feature of vector x is greater than thresholds[j]. Thus, a test on the feature value and the current threshold is carried out at each iteration. Instead of testing each threshold in a prefix of thresholds[i : end], our optimized implementation tests only one every \u2206 thresholds, where \u2206 is a parameter. Since the subvector thresholds[i : end] is sorted in ascending order, if a test succeed the same necessarily holds for all the preceding \u2206 \u2212 1 thresholds. Therefore, we can go directly to update the result bitvector v h of the corresponding trees, saving \u2206 \u2212 1 comparisons. Instead, if the test fails, we scan the preceding \u2206 \u2212 1 thresholds to identify the target index j and we conclude. In our implementation we set \u2206 equal to 4, which is the value giving the best results in our experiments. We remark that in principle one could identify j by binary searching the subvector thresholds[i : end]. Experiments have shown that the use of binary search is not profitable because in general the subvector is not sufficiently long.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": ["tab_0", "tab_0"]}, {"heading": "EXPERIMENTS", "text": "In this section we provide an extensive experimental evaluation that compares our QS algorithm with other state-ofthe-art competitors and baselines over standard datasets.\nDatasets and experimental settings. Experiments are conducted by using publicly available LtR datasets: the MSN 1 and the Yahoo! LETOR 2 challenge datasets. The first one is split into five folds, consisting of vectors of 136 features extracted from query-document pairs, while the second one consists of two distinct datasets (Y!S1 and Y!S2), made up of vectors of 700 features. In this work, we focus on MSN-1, the first MSN fold, and Y!S1 datasets. The features vectors of the two selected datasets are labeled with relevance judgments ranging from 0 (irrelevant) to 4 (perfectly relevant). Each dataset is split in training, validation and test sets. The MSN-1 dataset consists of 6,000, 2,000, and 2,000 queries for training, validation and testing respectively. The Y!S1 dataset consists of 19,944 training queries, 2,994 validation queries and 6,983 test queries.\nWe exploit the following experimental methodology. We use training and validation sets from MSN-1 and Y!S1 to train \u03bb-MART [18] models with 8, 16, 32 and 64 leaves. We use QuickRank 3 an open-source parallel implementation of \u03bb-MART written in C++11 for performing the training phase. During this step we optimize NDCG@10. The results of the paper can be also applied to analogous treebased models generated by different state-of-the-art learning algorithms, e.g., GBRT [4].\nIn our experiments we compare the scoring efficiency of QS 4 with the following competitors:\n\u2022 If-Then-Else is a baseline that translates each tree of the forest as a nested block of if-then-else.\n\u2022 VPred and Struct+ [1] kindly made available by the authors 5 .\nGiven a trained model and a test set, all the above scoring methods achieve the same result in terms of effectiveness. Hence, we do not report quality measures.\nAll the algorithms are compiled with GCC 4.9.2 with the highest optimization settings. The tests are performed by using a single core on a machine equipped with an Intel Core i7-4770K clocked at 3.50Ghz, with 32GiB RAM, running Ubuntu Linux 3.13.0. The Intel Core i7-4770K CPU has three levels of cache. Level 1 cache has size 32 KB, one for each of the four cores, level 2 cache has size 256 KB for each core, and at level 3 there is a shared cache of 8 MB.\nTo measure the efficiency of each of the above methods, we run 10 times the scoring code on the test sets of the MSN-1 and Y!S1 datasets. We then compute the average per-document scoring cost. Moreover, to deeply profile the behavior of each method above we employ perf 6 , a performance analysis tool available under Ubuntu Linux distributions. We analyze each method by monitoring several CPU counters that measure the total number of instructions executed, number of branches, number of branch mispredictions, cache references, and cache misses.\nScoring time analysis. The average time (in \u00b5s) needed by the different algorithms to score each document of the two datasets MSN-1 and Y!S1 are reported in Table 2. In particular, the table reports the per-document scoring time by varying the number of trees and the leaves of the ensemble employed. For each test the table also reports between parentheses the gain factor of QS over its competitors. At a first glance, these gains are impressive, with speedups that in many cases are above one order of magnitude. Depending on the number of trees and of leaves, QS outperforms VPred, the most efficient solution so far, of factors ranging from 2.0x up to 6.5x. For example, the average time required by QS and VPred to score a document in the MSN-1 test set with a model composed of 1, 000 trees and 64 leaves, are 9.5 and 62.2 \u00b5s, respectively. The comparison between QS and If-Then-Else is even more one-sided, with improvements of up to 23.4x for the model with 10, 000 trees and 32 leaves trained on the MSN-1 dataset. In this case the QS average per-document scoring time is 59.6 \u00b5s with respect to the 1396.8 \u00b5s of If-Then-Else. The last baseline reported, i.e., Struct+, behaves worst in all the tests conducted. Its performance is very low when compared not only to QS (up to 38.2x times faster), but even with respect to the other two algorithms VPred and If-Then-Else. The reasons of the superior performance of QS over competitor algorithms are manyfold. We analyse the most relevant in the following.\nInstruction level analysis. We used the perf tool to measure the total number of instructions, number of branches, number of branch mis-predictions, L3 cache references, and L3 cache misses of the different algorithms by considering only their scoring phase. Table 3 reports the results we obtained by scoring the MSN-1 test set by varying the number of trees and by fixing the number of leaves to 64. Experiments on Y!S1 are not reported here, but they exhibited similar behavior. As a clarification, L3 cache references accounts for those references which are not found in any of the previous level of cache, while L3 cache misses are the ones among them which miss in L3 as well. Table 3 also reports the number of visited nodes. All measurements are per-document and per-tree normalized. ", "publication_ref": ["b19", "b5", "b2", "b6"], "figure_ref": [], "table_ref": ["tab_1", "tab_2", "tab_2"]}, {"heading": "QS", "text": "We first observe that VPred executes the largest number of instructions. This is because VPred always runs d steps if d is the depth of a tree, even if a document might reach an exit leaf earlier. If-Then-Else executes much less instructions as it follows the document traversal path. Struct+ introduces some data structures overhead w.r.t. If-Then-Else. QS executes the smallest number instructions. This is due to the different traversal strategy of the ensemble, as QS needs to process the false nodes only. Indeed, QS always visits less than 18 nodes on average, out of the 64 present in each tree of the ensemble. Note that If-Then-Else traverses between 31 and 40 nodes per tree, and the same trivially holds for Struct+. This means that the interleaved traversal strategy of QS needs to process less nodes than in a traditional root-to-leaf visit. This mostly explains the results achieved by QS.\nAs far as number of branches is concerned, we note that, not surprisingly, QS and VPred are much more efficient than If-Then-Else and Struct+ with this respect. QS has a larger total number of branches than VPred, which uses scoring functions that are branch-free. However, those branches are highly predictable, so that the mis-prediction rate is very low, thus, confirming our claims in Section 3.\nObserving again the timings in Table 2 we notice that, by fixing the number of leaves, we have a super-linear growth of QS's timings when increasing the number of trees. For example, since on MSN-1 with \u039b = 64 and 1, 000 trees QS scores a document in 9.5 \u00b5s, one would expect to score a document 20 times slower, i.e., 190 \u00b5s, when the ensemble size increases to 20, 000 trees. However, the reported timing of QS in this setting is 425.1 \u00b5s, i.e., roughly 44 times slower than with 1000 trees. This effect is observable only when the number of leaves \u039b = {32, 64} and the number of trees is larger than 5, 000. Table 3 relates this super-linear growth to the numbers of L3 cache misses.\nConsidering the sizes of the arrays as reported in Table 1 in Section 3, we can estimate the minimum number of trees that let the size of the QS's data structure to exceed the cache capacity, and, thus, the algorithm starts to have more cache misses. This number is estimated in 6, 000 trees when the number of leaves is 64. Thus, we expect that the number of L3 cache miss starts increasing around this number of trees. Possibly, this number is slightly larger, because portions of the data structure may be infrequently accessed at scoring time, due the the small fraction of false nodes and associated bitvectors accessed by QS.\nThese considerations are further confirmed by Figure 4, which shows the average per-tree per-document scoring time (\u00b5s) and percentage of cache misses QS when scoring the MSN-1 and the Y!S1 with \u039b = 64 by varying the number of trees. First, there exists a strong correlation between QS's timings and its number of L3 cache misses. Second, the number of L3 cache misses starts increasing when dealing with 9, 000 trees on MSN and 8, 000 trees on Y!S1.", "publication_ref": [], "figure_ref": ["fig_3"], "table_ref": ["tab_1", "tab_2", "tab_0"]}, {"heading": "BWQS: a block-wise variant of QS", "text": "The previous experiments suggest that improving the cache efficiency of QS may result in significant benefits. As in Tang et al. [12], we can split the tree ensemble in disjoint blocks of size \u03c4 that are processed separately in order to let the corresponding data structures fit into the faster levels of the memory hierarchy. This way, we are essentially scoring each document over each tree blocks that partition the original ensemble, thus inheriting the efficiency of QS on smaller ensembles. Indeed, the size of the arrays required to score the documents over a block of trees depends now on \u03c4 instead of |T | (see Table 1 in Section 3). We have, however, to keep an array that stores the partial scoring computed so far for each document.\nThe temporal locality of this approach can be improved by allowing the algorithm to score blocks of documents together over the same block of trees before moving to the next block of documents. To allow the algorithm to score a block of \u03b4 documents in a single run we have to replicate in \u03b4 copies the array v. Obviously, this increases the space occupancy and may result in a worse use of the cache. Therefore, we need to find the best balance between the number of documents \u03b4 and the number of trees \u03c4 to process in the body of a nested loop that first runs over the blocks of trees (outer loop) and then over the blocks of documents to score (inner loop).\nThis algorithm is called BlockWise-QS (BWQS) and its efficiency is discussed in the remaining part of this section. Table 4 reports average per-document scoring time in \u00b5s of algorithms QS, VPred, and BWQS. The experiments were conducted on both the MSN-1 and Y!S1 datasets by varying \u039b and by fixing the number of trees to 20, 000. It is worth noting that our QS algorithm can be thought as a limit case of BWQS, where the blocks are trivially composed of 1 document and the whole ensemble of trees. VPred instead vectorizes the process and scores 16 documents at the time over the entire ensemble. With BWQS the sizes of document and tree blocks can be instead flexibly optimized according to the cache parameters. Table 4 reports the best execution times, along with the values of \u03b4 and \u03c4 for which BWQS obtained such results.\nThe blocking strategy can improve the performance of QS when large tree ensembles are involved. Indeed, the largest improvements are measured in the tests conducted on models having 64 leaves. For example, to score a document of MSN-1, BWQS with blocks of 3, 000 trees and a single document takes 274.7 \u00b5s in average, against the 425.1 \u00b5s required by QS with an improvement of 1.55x. The reason of the improvements highlighted in the table are apparent from the two plots reported in Figure 4. These plots report for MSN-1 and Y!S1 the per-document and per-tree average scoring time of BWQS and its cache misses ratio. As already mentioned, the plot shows that the average per-document per-tree scoring time of QS is strongly correlated to the cache misses measured. The more the cache misses, the larger the per-tree per-document time needed to apply the model. On the other hand, the BWQS cache misses curve shows that the block-wise implementation incurs in a negligible number of cache misses. This cache-friendliness is directly reflected in the per-document per-tree scoring time, which is only slightly influenced by the number of trees of the ensemble.", "publication_ref": ["b13"], "figure_ref": ["fig_3"], "table_ref": ["tab_0", "tab_3", "tab_3"]}, {"heading": "CONCLUSIONS", "text": "We presented a novel algorithm to efficiently score documents by using a machine-learned ranking function modeled by an additive ensemble of regression trees. Our main contribution is a new representation of the tree ensemble based on bitvectors, where the tree traversal, aimed to detect the leaves that contribute to the final scoring of a document, is performed through efficient logical bitwise operations. In addition, the traversal is not performed one tree after another, as one would expect, but it is interleaved, feature by feature, over the whole tree ensemble. Our tests conducted on publicly available LtR datasets confirm unprecedented speedups (up to 6.5x) over the best state-of-the-art competitor. The motivations of the very good performance figures of our QS algorithm are diverse. First, linear arrays are used to store the tree ensemble, while the algorithm exploits cache-friendly access patterns (mainly sequential patterns) to these data structures. Second, the interleaved tree traversal counts on an effective oracle that, with a few branch mis-predictions, is able to detect and return only the internal node in the tree whose conditions evaluate to False. Third, the number of internal nodes visited by QS is in most cases consistently lower than in traditional methods, which recursively visits the small and unbalanced trees of the ensemble from the root to the exit leaf. All these remarks are confirmed by the deep performance assessment conducted by also analyzing low-level CPU hardware counters. This analysis shows that QS exhibits very low cache misses and branch mis-prediction rates, while the instruction count is consistently smaller than the counterparts. When the size of the data structures implementing the tree ensemble becomes larger the last level of the cache (L3 in our experimental setting), we observed a slight degradation of performance. To show that our method can be made scalable, we also present BWQS, a block-wise version of QS that splits the sets of feature vectors and trees in disjoint blocks that entirely fit in the cache and can be processed separately. Our experiments show that BWQS performs up to 1.55 times better than the original QS on large tree ensembles.\nAs future work, we plan to apply the same devised algorithm to other contexts, when a tree-based machine learned model must be applied to big data for classification/prediction purposes. Moreover, we aim at investigating whether we can introduce further optimizations in the algorithms, considering that the same tree-based model is applied to a multitude of feature vectors, and thus we could have the chance of partially reusing some work. Finally, we plan to investigate the parallelization of our method, which can involve various dimensions, i.e., the parallelization of the scoring task of each single feature vector, or the parallelization of the simultaneous scoring of many feature vectors.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgements", "text": "We acknowledge the support of Tiscali S.p.A. In particular, we wish to warmly thank Domenico Dato and Monica Mori (Istella) for fruitful discussions and valuable feedbacks helping us in concretizing this paper.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "18.2x) 1218.6 (37.6x)", "journal": "", "year": "", "authors": ""}, {"ref_id": "b1", "title": "34.2x) 834.6 (24.3x) 1806.7 (30.3x", "journal": "", "year": "", "authors": ""}, {"ref_id": "b2", "title": "Runtime optimizations for tree-based machine learning models", "journal": "IEEE Trans. Knowl. Data Eng", "year": "2014", "authors": "N Asadi; J Lin; A P De Vries"}, {"ref_id": "b3", "title": "From ranknet to lambdarank to lambdamart: An overview", "journal": "", "year": "2010", "authors": "C J Burges"}, {"ref_id": "b4", "title": "Early exit optimizations for additive machine learned ranking systems", "journal": "ACM", "year": "2010", "authors": "B B Cambazoglu; H Zaragoza; O Chapelle; J Chen; C Liao; Z Zheng; J Degenhardt"}, {"ref_id": "b5", "title": "Greedy function approximation: a gradient boosting machine", "journal": "Annals of Statistics", "year": "2001", "authors": "J H Friedman"}, {"ref_id": "b6", "title": "Bagging gradient-boosted trees for high precision, low variance ranking models", "journal": "ACM", "year": "2011", "authors": "Y Ganjisaffar; R Caruana; C V Lopes"}, {"ref_id": "b7", "title": "Cumulated gain-based evaluation of ir techniques", "journal": "ACM Trans. Inf. Syst", "year": "2002", "authors": "K J\u00e4rvelin; J Kek\u00e4l\u00e4inen"}, {"ref_id": "b8", "title": "Learning to rank for information retrieval", "journal": "Foundations and Trends in Information Retrieval", "year": "2009", "authors": "T.-Y Liu"}, {"ref_id": "b9", "title": "Computer Organization and Design", "journal": "Morgan Kaufmann", "year": "2009", "authors": "D Patterson; J Hennessy"}, {"ref_id": "b10", "title": "The probabilistic relevance framework: Bm25 and beyond", "journal": "Found. Trends Inf. Retr", "year": "2009", "authors": "S Robertson; H Zaragoza"}, {"ref_id": "b11", "title": "Machine learning in search quality at yandex. ACM SIGIR, Industry track", "journal": "", "year": "2010", "authors": "I Segalovich"}, {"ref_id": "b12", "title": "Implementing decision trees and forests on a gpu", "journal": "Springer", "year": "2008", "authors": "T Sharp"}, {"ref_id": "b13", "title": "Cache-conscious runtime optimization for ranking ensembles", "journal": "", "year": "2014", "authors": "X Tang; X Jin; T Yang"}, {"ref_id": "b14", "title": "Accelerating a random forest classifier: Multi-core, gp-gpu, or fpga?", "journal": "IEEE", "year": "2012", "authors": "B Van Essen; C Macaraeg; M Gokhale; R Prenger"}, {"ref_id": "b15", "title": "Robust real-time face detection", "journal": "Int. J. Comput. Vision", "year": "2004", "authors": "P Viola; M J Jones"}, {"ref_id": "b16", "title": "Learning to efficiently rank", "journal": "", "year": "2010", "authors": "L Wang; J J Lin; D Metzler"}, {"ref_id": "b17", "title": "A cascade ranking model for efficient ranked retrieval", "journal": "", "year": "2011", "authors": "L Wang; J J Lin; D Metzler"}, {"ref_id": "b18", "title": "Ranking under temporal constraints", "journal": "", "year": "2010", "authors": "L Wang; D Metzler; J J Lin"}, {"ref_id": "b19", "title": "Adapting boosting for information retrieval measures", "journal": "Information Retrieval", "year": "2010", "authors": "Q Wu; C Burges; K Svore; J Gao"}, {"ref_id": "b20", "title": "The greedy miser: Learning under test-time budgets", "journal": "ACM", "year": "2012", "authors": "Z Xu; K Weinberger; O Chapelle"}], "figures": [{"figure_label": "41", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "4 Figure 1 :41Figure 1: A decision tree.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Tree traversal example.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Arrays used by algorithm QS.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Per-tree per-document scoring time in \u00b5s and percentage of cache misses of QS and BWQS on MSN-1 (left) and Y!S1 (right) with 64-leaves \u03bb-MART models.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Data structures used by QS, the corresponding maximum sizes, and the access modes.", "figure_data": "Data structure Maximum Size (bytes)Data access modes"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Per-document scoring time in \u00b5s of QS, VPred, If-Then-Else and Struct+ on MSN-1 and Y!S1 datasets. Gain factors are reported in parentheses.", "figure_data": "Number of trees/datasetMethod\u039b1, 0005, 00010, 00020, 000MSN-1Y!S1MSN-1Y!S1MSN-1Y!S1MSN-1Y!S1QS2.2 (-)4.3 (-)10.5 (-)14.3 (-)20.0 (-)25.4 (-)40.5 (-)48.1 (-)VPred If-Then-Else87.9 (3.6x) 8.2 (3.7x) 10.3 (2.4x) 8.5 (2.0x)40.2 (3.8x) 81.0 (7.7x)41.6 (2.9x) 85.8 (6.0x)80.5 (4.0x) 185.1 (9.3x)82.7 (3.3) 185.8 (7.3x) 709.0 (17.5x) 772.2 (16.0x) 161.4 (4.0x) 164.8 (3.4x)Struct+21.2 (9.6x) 23.1 (5.4x) 107.7 (10.3x)112.6 (7.9x) 373.7 (18.7x) 390.8 (15.4x) 1150.4 (28.4x) 1141.6 (23.7x)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Per-tree per-document low-level statistics on MSN-1 with 64-leaves \u03bb-MART models.", "figure_data": "MethodNumber of Trees 1, 000 5, 000 10, 000 15, 000 20, 000Instruction CountQS5875869197VPred580599594588516If-Then-Else142139133130116Struct+341332315308272Num. branch mis-predictions (above)Num. branches (below)QS0.162 0.035 0.017 0.011 0.009 6.04 7.13 8.23 8.63 9.3VPred0.013 0.042 0.045 0.049 0.2 0.21 0.18 0.210.049 0.21If-Then-Else1.541 1.608 1.615 1.627 42.61 41.31 39.16 38.041.748 33.65Struct+4.498 5.082 5.864 6.339 89.9 88.91 85.55 83.835.535 74.69L3 cache misses (above)L3 cache references (below)QS0.004 0.001 0.121 0.323 1.78 1.47 1.52 2.140.51 2.33VPred0.005 0.166 0.326 0.363 0.356 12.55 12.6 13.74 15.04 12.77If-Then-Else0.001 17.772 30.331 29.615 29.577 27.66 38.14 40.25 40.76 36.47Struct+0.039 12.791 17.147 15.923 13.971 7.37 18.64 20.52 19.87 18.38Num. Visited Nodes (above)Visited Nodes/Total Nodes (below)QS9.71 13.40 15.79 16.65 18.00 15% 21% 25% 26% 29%VPred54.38 56.23 55.79 55.23 86% 89% 89% 88%48.45 77%Struct+40.61 39.29 37.16 36.1531.75If-Then-Else64%62%59%57%50%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Per-document scoring time in \u00b5s of BWQS, QS and VPred algorithms on MSN-1.", "figure_data": "MSN-1Y!S1\u039b Method\u03b4Block \u03c4Time\u03b4Block \u03c4TimeBWQS8 20,00033.5 (-) 8 20,00040.5 (-)8QS1 20,00040.5 (1.21x) 1 20,00048.1 (1.19x)VPred 16 20,000 161.4 (4.82x) 16 20,000 164.8 (4.07x)BWQS8 5,00059.6 (-) 8 10,00072.34 (-)16QS1 20,00067.8 (1.14x) 1 20,00081.0 (1.12x)VPred 16 20,000 336.4 (5.64x) 16 20,000 336.1 (4.65x)BWQS2 5,000135.5 (-) 8 5,000141.2 (-)32QS1 20,000 155.8 (1.15x) 1 20,000 160.1 (1.13x)VPred 16 20,000 711.9 (5.25x) 16 20,000 694.8 (4.92x)BWQS1 3,000274.7 (-) 1 4,000236.0 (-)64QS1 20,000 425.1 (1.55x) 1 20,000 343.7 (1.46x)VPred 16 20,000 1309.7 (4.77x) 16 20,000 1420.7 (6.02x)"}], "formulas": [{"formula_id": "formula_0", "formula_text": "s(x) = |T |\u22121 h=0 w h \u2022 e h (x).val", "formula_coordinates": [2.0, 382.71, 617.16, 107.32, 27.73]}, {"formula_id": "formula_1", "formula_text": "d steps \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 i \u2190 n 0 .idx [x[\u03c6 0 ] > \u03b3 0 ] i \u2190 n i .idx [x[\u03c6 i ] > \u03b3 i ] . . . . . . i \u2190 n i .idx [x[\u03c6 i ] > \u03b3 i ]", "formula_coordinates": [3.0, 107.96, 510.16, 121.11, 48.9]}, {"formula_id": "formula_2", "formula_text": "\u2022 x: input feature vector \u2022 T h = (N h , L h ): binary decision tree, with -N h = {n 0 , n 1 , . . .}: internal nodes of T h -L h = {l 0 , l 1 , . . .}: leaves of T h -n.", "formula_coordinates": [4.0, 325.53, 570.26, 164.57, 42.88]}, {"formula_id": "formula_3", "formula_text": "1 v h \u2190 11 . . . 11 2 U \u2190 FindFalse(x, T h ) 3 foreach node u \u2208 U do 4 v h \u2190 v h \u2227 u.bitvector 5 j \u2190 index of leftmost bit set to 1 of v h 6 return l j .val", "formula_coordinates": [4.0, 317.06, 651.58, 166.55, 56.16]}, {"formula_id": "formula_4", "formula_text": "1 foreach h \u2208 0, 1, . . . , |T | \u2212 1 do 2 v[h]\u2190 11 . . . 11 3 foreach k \u2208 0, 1, . . . , |F | \u2212 1 do // Step 4 i \u2190 offsets[k] 5 end \u2190 offsets[k + 1] 6 while x[k] > thresholds[i] do 7 h \u2190 tree_ids[i] 8 v[h] \u2190 v[h] \u2227 bitvectors[i] 9 i \u2190 i + 1 10 if i \u2265 end then 11 break 12 score \u2190 0 13 foreach h \u2208 0, 1, . . . , |T | \u2212 1 do // Step 14 j \u2190 index of leftmost bit set to 1 of v[h] 15 l \u2190 h \u2022 |L h | + j 16 score \u2190 score + w h \u2022 leaves[l]", "formula_coordinates": [5.0, 312.55, 539.01, 219.04, 156.46]}, {"formula_id": "formula_5", "formula_text": "|T |\u22121 0 |N h |, can be upper bounded by |T | \u2022 \u039b, because for every tree T h we have |N h | < |N h | + 1 = |L h | \u2264 \u039b.", "formula_coordinates": [6.0, 316.81, 223.81, 239.1, 21.03]}, {"formula_id": "formula_6", "formula_text": "thresholds |T | \u2022 \u039b \u2022 sizeof(float) 1. Sequential (R) tree_ids |T | \u2022 \u039b \u2022 sizeof(uint) bitvectors |T | \u2022 \u039b \u2022 B offsets |F | \u2022 sizeof(uint) v |T | \u2022 B 1. Random (R/W) 2. Sequential (R) leaves |T | \u2022 \u039b \u2022 sizeof(double) 2. Seq. Sparse (R)", "formula_coordinates": [6.0, 328.86, 323.69, 228.75, 62.01]}], "doi": ""}