{"title": "Performance Analysis of Online Anticipatory Algorithms for Large Multistage Stochastic Integer Programs", "authors": "Luc Mercier; Pascal Van Hentenryck", "pub_date": "", "abstract": "Despite significant algorithmic advances in recent years, finding optimal policies for large-scale, multistage stochastic combinatorial optimization problems remains far beyond the reach of existing methods. This paper studies a complementary approach, online anticipatory algorithms, that make decisions at each step by solving the anticipatory relaxation for a polynomial number of scenarios. Online anticipatory algorithms have exhibited surprisingly good results on a variety of applications and this paper aims at understanding their success. In particular, the paper derives sufficient conditions under which online anticipatory algorithms achieve good expected utility and studies the various types of errors arising in the algorithms including the anticipativity and sampling errors. The sampling error is shown to be negligible with a logarithmic number of scenarios. The anticipativity error is harder to bound and is shown to be low, both theoretically and experimentally, for the existing applications.", "sections": [{"heading": "Introduction", "text": "Online stochastic algorithms for solving large multistage stochastic integer programs have attracted increasing interest in recent years. They are motivated by applications in which different types of requests arrive dynamically, and it is the role of the algorithm to decide which requests to serve and how. Unlike traditional online algorithms, these applications assume that the uncertainty is stochastic and that distributions of the requests are given.\nConsider the packet scheduling problem from [Chang et al., 2000]. A router receives a set of packets at each time step and must choose which packet to serve. Packets can be served only for a limited time and they are characterized by a value. The goal is to maximize the values of the served packets. The packet distributions are specified by Markov models whose states specify arrival frequencies for the packet type.\nOnline reservation systems [Benoist et al., 2001] are another such application. Customers place requests in real time for some service at a fixed date. The resources are modeled by a multiknapsack constraint. (Think of tour operators requesting rooms in hotels for a group: the choice of a specific hotel is not pertinent for the group but all group members must be allocated to the same hotel). Customers must be immediately notified of acceptance or rejection of their requests, and accepted requests must be satisfied. Accepted requests must also be assigned to a specific resource at reservation time and this choice cannot be reconsidered. The goal is to maximize the profit of the served requests which come from different types with different characteristics and arrival frequencies.\nOnline multiple vehicle routing with time windows [Bent and Van Hentenryck, 2003] captures an important class of applications arising in transportation and distribution systems. In these problems, a fleet of vehicles serve clients which are located in many different locations and place request for service in real-time in specific time windows. Clients must be immediately notified of acceptance or rejection of their requests, and all accepted requests must be satisfied. Routing decisions however can be delayed if necessary. The goal is to maximize the number of satisfied requests.\nAll these problems share several characteristics. First, they can be modeled as multistage integer stochastic programs. Second, the number of stages is large. In packet scheduling, time is discrete by nature, and experiments were made with 200,000 stages. For the two other applications, time is continuous but a reasonable discretization of time would require 200 stages. Third, the set of feasible decisions at each stage is finite. Finally, these applications require fast decision-making. These characteristics prohibit the use of a priori methods for (Partially Observable) Markov Decision Processes and for Stochastic Programs. Indeed, [Chang et al., 2000] and [Benoist et al., 2001] have shown that (PO)MDPs do not scale on these applications. Moreover, successful algorithms for 2-stage stochastic optimization, such as the Sample Average Approximation method, are shown to require a number of samples exponential in the number of stages [Shapiro, 2006], precluding their use on these applications. Interestingly, high-quality solutions to these applications have been obtained by online algorithms that relax the nonanticipativity constraints in the stochastic programs. These online anticipatory algorithms make decisions online at a time t in three steps:\n1. sample the distribution to obtain scenarios of the future; 2. optimize each scenario for each possible decision; 3. select the best decision over all scenarios.\nIt is clear that this strategy is necessarily suboptimal, even with many scenarios. However, experimental results have been surprisingly good, especially with the Regret algorithm [Bent and Van Hentenryck, 2004;Hentenryck et al., 2006] which is an efficient way of implementing step 2. Our goal in this paper is to demystify these results by providing a theoretical analysis of these algorithms. Section 2 describes the model and the algorithm. Section 3 analyses the performance of the online anticipatory algorithm and isolates two fundamental sources of error: a sampling error and a quantity called the global anticipatory gap which is inherent to the problem. Section 4 shows how to bound the anticipatory gap theoretically and experimentally. Section 5 analyzes the effect of approximating the optimization problem. Section 6 compares the anticipatory gap to the expected value of perfect information. Section 7 presents directions for future research.", "publication_ref": ["b1", "b0", "b1", "b0", "b2", "b1", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Model and Algorithm", "text": "We consider finite stochastic integer programs of the form\nQ = max x1\u2208X (s1) E max x2\u2208X (s2) E . . . max xT \u2208X (sT ) f (x, \u03be) ,\nwhere \u03be is a stochastic process, with \u03be t being the observation at time t, (with \u03be 1 being deterministic), s\nt = (x 1..t\u22121 , \u03be 1..t )\nis the state at time t, X maps states to non-empty subsets of a finite set X of decisions (so the max's are well-defined), and f is the utility function bounded by F max . We denote respectively x and \u03be the vectors x 1..T and \u03be 1..T .\nA decision process is a stochastic process x such that \u2200t : x t \u2208 X (s t ). We can assume that the computation of each x t requires exactly one random variable \u03b3 t . These variables are independent and independent of \u03be.\nIn practice, decisions cannot be made based on future observations. A decision process x is non-anticipative if x t is a deterministic function of \u03b3 1..t and \u03be 1..t (that is, if x is adapted to the filtration F t = \u03c3(\u03b3 1..t , \u03be 1..t )). We can rewrite the stochastic program as Q = max {E [f (x, \u03be)] | x non-anticip. dec. proc.} .\nA scenario is a realization of the process \u03be. The offline problem is the problem a decision maker would face if, in a given state s t , the future observations are revealed; we define O(s t , x t , \u03be) = max {f (y, \u03be) | y dec. proc., y 1..\nt = x 1..t } , O(s t , \u03be) = max {f (y, \u03be) | y dec. proc., y 1..t\u22121 = x 1..t\u22121 } = max x\u2208X (st) O(s t , x, \u03be).\nNote that these two problems are deterministic. Finally, the expected value of the clairvoyant (EVC ) is defined as the expected utility of a clairvoyant decision maker, that is, EVC = E [O(s 1 , \u03be)]. The problems discussed in the introduction all fit in this model: in particular, the utility is bounded thanks to capacity constraints. The model can also be generalized to the case in which f (x, \u03be) has finite first and second moments for every x.\nThe anticipatory algorithm studied here is Algorithm MakeDecision, parametrized by the number of scenarios m, whose successive decisions form a non-anticipative process: Function MakeDecision(s t , \u03b3 t ) Use \u03b3 t to compute scenarios \u03be 1 . . . \u03be m where \u03be i 1..\nt = \u03be 1..t foreach x \u2208 X (s t ) do g(x) \u2190 1 m m i=1 O(s t , x, \u03be i ) x t \u2190 argmax x\u2208X (st) g(x)\n3 Analysis of the Anticipatory Algorithm\nWe compare the performance of the anticipatory algorithm with the offline, a posteriori solution in the expected sense, as is typically done in online algorithms [Borodin and El-Yaniv, 1998]. In other words, for the decision process x produced by the anticipatory algorithm, we bound EVC \u2212 E [f (x , \u03be)], which we call the expected global loss (EGL).", "publication_ref": ["b1"], "figure_ref": [], "table_ref": []}, {"heading": "Local and Global Losses", "text": "We first show that the EGL is the sum of the expected losses of the stages. Definition 1 Let s t be a state. The expected local loss of decision x \u2208 X (s t ) is defined as\n\u2206(s t , x) = E [O(s t , \u03be) \u2212 O(s t , x, \u03be) |s t ] .\nNote that conditioning on a state s t does not provide any information on \u03b3 t : when reading an expression of the form E [. . . |s t ], keep in mind that there is uncertainty on \u03be t+1 , . . . , \u03be T and on \u03b3 t , . . . , \u03b3 T .\nLemma 1 (Global Loss = Sum of Local Losses) For any decision process x,\nEVC \u2212 E [f (x , \u03be)] = T t=1 E [\u2206(s t , x t )]\nProof. Let C t be the random variable O(s t , x t , \u03be) and\nA t = E [C t \u2212 f (x, \u03be)]\n. Then A T = 0 and, for t < T ,\nA t = E [C t \u2212 C t+1 + C t+1 \u2212 f (x, \u03be)] = E [C t \u2212 C t+1 ] + A t+1 = E [\u2206(s t+1 , x t+1 )] + A t+1 .\nThe last equality comes from decomposing and re-assembling amongst all possible values of x t .\nFinally EVC \u2212 E [f (x , \u03be)] = E [C 0 \u2212 f (x , \u03be)] = A 0 . 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Decomposition of the Local Loss", "text": "We now show that the local loss at a state s t consists of a sampling error and the anticipatory gap.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Definition 2", "text": "The anticipatory gap of a state s t is defined as\n\u2206 g (s t ) = min x\u2208X (st) \u2206(s t , x).\nThe choice error of x wrt s t is defined as\n\u2206 c (s t , x) = \u2206(s t , x) \u2212 \u2206 g (s t ).\nThe anticipatory gap is inherent to the problem and independent of the decision process x. An equivalent definition is\nmax x\u2208X (st) E [O(s t , x, \u03be)|st] \u2212 E max x\u2208X (st) O(s t , x, \u03be) st .\nThis expression shows that this gap can be interpreted as the cost of commuting of E and max. We now bound \u2206 c (s t , x).\nLemma 2 (Sampling Error) Let x t be computed by the anticipatory algorithm using m samples per decision. Let s t be a state and\nx \u22c6 be argmax E [O(s t , x, \u03be) |s t ] (break ties arbi- trarily). Then E [\u2206 c (s t , x t ) |s t ] \u2264 x\u2208X (st) \u2206 c (s t , x) exp \u2212m\u2206 c (s t , x) 2 2\u03c3(s t , x) 2 , where \u03c3(s t , x) is the standard deviation of O(s t , x, \u03be) \u2212 O(s t , x \u22c6 , \u03be) given s t .\nProof. Here all probabilities and expectations are implicitly conditional on s t . The left-hand side can be decomposed as\nE [\u2206 c (s t , x t )] = x\u2208X (st) \u2206 c (s t , x)P(x t = x).\nDue to the argmax in MakeDecision, the event\nx t = x implies \u2200x \u2032 \u2208 X (s t ), g(x \u2032 ) \u2264 g(x). Therefore P (x t = x) \u2264 P (g(x) \u2265 g(x \u22c6 )).\nSince f is bounded, O(s t , x, \u03be) \u2212 O(s t , x \u22c6 , \u03be) has a finite expectation and variance. Now,\ng(x) \u2212 g(x \u22c6 ) = 1 m m i=1 O(s t , x, \u03be i ) \u2212 O(s t , x \u22c6 , \u03be i )\nand, by the central limit theorem, this difference is normally distributed for m large enough, with mean \u2212\u2206 c (s t , x) and variance\n1 m \u03c3(s t , x) 2 . Finally, if X \u223c N (\u00b5, \u03c3 2 ) with \u00b5 < 0, then P (X \u2265 0) \u2264 exp \u2212 \u00b5 2 2\u03c3 2 (Chernoff bound). 2", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Performance of the Algorithm", "text": "We now assemble the previous results. Definition 3 The Global Anticipatory Gap of the problem is\nGAG = E \uf8ee \uf8f0 max x1..T xi\u2208X (si) T t=1 \u2206 g (s t ) \uf8f9 \uf8fb .\nOnce again, this quantity is inherent to the problem.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 1", "text": "The expected global loss of the anticipatory algorithm satisfies\nEGL \u2264 GAG + O e \u2212Km\nwhere m is the number of samples per decision and\nK = min st,x\u2208X (st) \u2206c(st,x)>0 \u2206 c (s t , x) 2 2\u03c3(s t , x) 2 .\nProof. We have\nEGL = T t=1 E [\u2206(s t , x t )] \u2264 T t=1 E [\u2206 g (s t )] + E [\u2206 c (s t , x t )] .\nThe term GAG comes from\nT t=1 E [\u2206 g (s t )] = E T t=1 \u2206 g (s t ) \u2264 E max x1..T T t=1 \u2206 g (s t ) ,\nand the global sampling error satisfies\nT t=1 E [\u2206 c (s t , x t )] \u2264 T |X| F max e \u2212Km . 2\nAn important consequence of this theorem is that the sampling error can be made smaller than some constant a by choosing m \u2265 1 /K log ( 1 /aT |X| F max ). [Shapiro, 2006] argues that the SAA method does not scale to multistage problems, because the number of samples to achieve a given accuracy grows exponentially with T . The anticipatory algorithm only requires m to grow logarithmically with T |X|, which makes it highly scalable. Of course, it only produces highquality decisions when the anticipatory gap is small.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Bounding the Global Anticipatory Gap", "text": "This section provides theoretical and experimental results on the anticipatory gap, explaining why anticipatory algorithms are effective in the applications mentioned in the introduction.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Theoretical Proof on Packet Scheduling.", "text": "We first show how to compute an upper bound on GAG for a simplified version of the packet scheduling problem. Suppose that there are k types of packets whose values are v 1 < . . . < v k respectively. At each step from 1 to T \u2212 1, a packet of type i arrives with probability p i . All these random variables are independent. Each packet has a lifetime of 2, meaning a packet received at time t can be scheduled either at time t or at time t + 1. The utility is the sum of scheduled packets over the T stages. All packets take a single time step to serve. For convenience, we introduce a packet type 0 with value v 0 = 0 and probability p 0 = 1. It should be clear that this problem satisfies all assumptions above. In particular, the utility is bounded\n(0 \u2264 f \u2264 T v k ).\nWhy is the GAG small on this problem? We show that \u2206 g is rarely high, inducing a small GAG. For s t a state and x, y \u2208 X (s t ), we say that x dominates y if O(s t , x, \u03be) \u2265 O(s t , y, \u03be) almost surely given s t . Studying \u2206 g (s t ) only requires to focus on non-dominated decisions: there are at most two non-dominated decisions for a given state, which consists of scheduling\n\u2022 the most valuable packet, of type i, received at time t\u2212 1 and not already scheduled; or \u2022 the most valuable packet, of type j, received at time t.\nMoreover, if i \u2265 j, then choosing j is dominated, since i is more valuable and will be lost if not chosen now. Also, if i < j but the second most valuable packet received at t is of type k \u2265 i, then choosing i is dominated. If one of these two conditions holds, a decision dominates all the other ones, and thus \u2206 g (s t ) = 0.\nSuppose now that s t does not satisfy any of them. By the dominance property, scenarios can be partitioned into those where scheduling i (resp. j) is the unique offline, optimal decision and those on which there is a tie. Introduce the random variable y t , taking values i, j or \u22a5 in these three respective cases. We then have\n\u2206(s t , i) = E [O(s t , \u03be) \u2212 O(s t , i, \u03be) |s t ] = E [O(s t , \u03be) \u2212 O(s t , i, \u03be) |s t , y t = j ] P (y t = j) and symmetrically \u2206(s t , j) = E [O(s t , \u03be) \u2212 O(s t , j, \u03be) |s t ] = E [O(s t , \u03be) \u2212 O(s t , j, \u03be) |s t , y t = i ] P (y t = i) .\nNow, if i is scheduled and the optimal offline solution was to schedule j, then the loss cannot exceed v j \u2212 v i , since the rest of the optimal offline schedule is still feasible. Hence E [O(s t , \u03be) \u2212 O(s t , i, \u03be) |s t , y t = j ] \u2264 v j \u2212 v i . Moreover, for the optimal offline schedule to choose j at time t, it is necessary to have a packet of value greater than v i arriving at t + 1 and thus P (y t = j) \u2264 1 \u2212 k>i q k where q k = 1 \u2212 p k . Finally, we find\n\u2206(s t , i) \u2264 (v j \u2212 v i ) 1 \u2212 k>i q k .\nThe other case is harder to study, but a trivial upper bound is \u2206(s t , j) \u2264 v i . Now it remains to bound the expectation of \u2206 g (s t ) = min(\u2206(s t , i), \u2206(s t , j)) by enumerating the possible values of i and j and weighting each case with its probability of occurrence. This weight is, in fact, bounded by the product of the probabilities of the 3 following events:\n\u2022 a packet of type i arrived at time t \u2212 1;\n\u2022 the most valuable packet arrived at t is of type j;\n\u2022 no packet of type i \u2264 k < j arrived at time t.\nHere is a numerical example. Suppose there are 4 types of packets, with the following values and emission probabilities: The upper bound on \u2206 g depending on i and j, is (j) 1 2 .496 3 1.00 .560 4 1.00 1.68 .400 1 2 3 4 (i)\nWe find that E [\u2206 g (s t )] \u2264 .125. On the other hand, a simple lower bound on the EVC is the expectation of the value of the most valuable packet arriving at each stage multiplied by the number of stages. In this case, that leads to EVC \u2265 2 .25T . As a result, the ratio of GAG over EVC on this problem is less than 5.55%. Because this analysis is not tight -especially the lower bound of the EVC -, the anticipatory algorithm is likely to have an even better expected global loss. This analysis also hints at why online anticipatory algorithms are so effective on packet scheduling and why they outperform competitive algorithms on these instances.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Discussion on Practical Problems.", "text": "The previous section shows how to bound the GAG on a particular problem: study dominance properties between the decisions, bound the loss of making a non-optimal (in the offline sense) decision, and bound the probability of this event. We are currently applying this method on more complex problems but proofs quickly become very cumbersome. As an alternative, we discuss another, empirical way to argue that the GAG of a problem is small. We have emphasized in the theoretical discussion the importance of bounding the probability that the chosen decision is not a posteriori optimal.\nWe call P (O(s t , x t , \u03be) = O(s t , \u03be) |s t ) the consensus rate. This quantity can be estimated easily during the computation of an anticipatory algorithm. It suffices to count how many scenarios make the same decision, i.e., 1 m i \u2208 {1, . . . , m} O(s t , x, \u03be i ) = O(s t , x t , \u03be i ) .\n[ Hentenryck et al., 2006] kindly gave us some of these statistics on online reservation systems: they depict the consensus rate (min/max/avg) as a function of the number of scenarios. On this class of instances, there are 6 possible decisions in each state. Therefore, one could expect an average consensus rate of 20%, but it is actually much higher, at about 80%. Moreover the maximal offline loss of a bad decision can easily be bounded in this problem. By Markov inequality, the GAG is low. Similar observations were made in the packet scheduling problem, where the measured average consensus was about 90%, and in the vehicle routing problem, where the rate varies among the stages, exhibiting an increasing trend from 65 to 100%. This argument, however, would be useless when F max is high, e.g. problems with high penalty states. More generally, the following theorem gives a way to measure the anticipatory gap of a state.\nTheorem 2 Let s t be a state. Define \u2206 g (s t ) as 1 m m i=1 max x\u2208X (st) O(s t , x, \u03be i ) \u2212 max x\u2208X (st) m i=1 O(s t , x, \u03be i ) ,\nthen this is a strongly consistent estimator of \u2206 g (s t ), i.e.,\nP\nlim m\u2192+\u221e \u2206 g (s t ) = \u2206 g (s t ) s t = 1.\nProof. Apply the strong law of large numbers to O(s t , x, \u03be) for all x and conclude with the finiteness of X (s t ). 2 \u2206 g can be computed by the online anticipatory algorithm at no additional cost.", "publication_ref": ["b2"], "figure_ref": [], "table_ref": []}, {"heading": "Approximating the Offline Problem", "text": "Theorem 1 explains why the anticipatory algorithm provides good results when the GAG is small. However, practical implementations use a variant of Algorithm 1 in which O is replaced by a fast approximation O. This is the case for the three applications mentioned earlier which use an approximating technique called Regret [Bent and Van Hentenryck, 2004;Hentenryck et al., 2006]. The Regret algorithm can be seen as a discrete version of sensitivity analysis: instead of computing O(s t , x, \u03be) for each for each x \u2208 X (s t ), the idea is to compute x * = argmax x (O(s t , x, \u03be i )) first and then to compute a vector of approximations O(s t , x, \u03be i )\nx\u2208X (st) using x * . Each entry in this vector is derived by approximating the loss of selecting a specific x in X (s t ) instead of the optimal decision x * . As a result, the Regret algorithm ensures (i) O \u2264 O and (ii) max x ( O(s t , x, \u03be i )) = max x (O(s t , x, \u03be i )). See  for a discussion on the complexity of computing this approximated vector.\nIt is not easy to provide tight theoretical bounds on the expected global loss for the Regret approximation. We thus measured the empirical distribution of O \u2212 O on online stochastic reservation systems from [Hentenryck et al., 2006]. The difference O \u2212 O is zero in 80% of the cases and its mean is very small (around .2 while the typical values of O are in the range [400,500]), although it can occasionally be large. This intuitively justifies the quality of Regret, whose expected global loss is not significatively different from the anticipatory algorithm for the same sample size.\nFinally, recall that, on online reservation systems, the consensus rate is very high on average.\nLet x \u22c6 = argmax x m i=1 O(s t , x, \u03be i ) and let the consensus rate be \u03b1. By properties (i) and (ii) of Regret, the approximated \"score\" m i=1 O(s t , x \u22c6 , \u03be i ) of decision x \u22c6 may only exhibit errors in (1 \u2212 \u03b1)m scenarios and hence will be very close to \u03b1m i=1 O(s t , x \u22c6 , \u03be i ). Moreover, other decisions have an approximated score where (almost all) the terms of the sum has a negative error. Therefore the approximated decision is biased toward consensual decisions and a high consensus rate tends to hide the approximation errors O \u2212 O of Regret.\nIn summary, a high consensus rate not only makes the AG small but also allows Regret to produce decisions close in quality to the exact anticipatory algorithm. This does not mean that a brutal Regret approximation, e.g., assigning zero to each non-optimal decision, would be as effective [Bent and Van Hentenryck, 2004].", "publication_ref": ["b1", "b2", "b2", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "GAG Versus EVPI", "text": "This section studies the relationships between the anticipatory gap and the expected value of perfect information (EVPI ). Since these concepts are seemingly close, it is useful to explain how they differ and why we chose to introduce the notion of anticipatory gap.\nConsider the following two maps assigning values to states: the offline value and the online value of state s t , respectively denoted by \u03c6(s t ) and \u03c0(s t ) and defined by\n\u03c6(s t ) = max {E [f (x, \u03be)|s t ] | x dec. proc.} \u03c0(s t ) = max {E [f (x, \u03be)|s t ] | x non-anticip. dec. proc.} .\nNote that \u03c6(s t ) \u2265 \u03c0(s t ) for all state s t . The difference \u03b7(s t ) = \u03c6(s t ) \u2212 \u03c0(s t ).\nis the (local) expected value of perfect information (EVPI ). The expected value of perfect information of the problem is \u03b7(s 1 ), that is, the advantage, in the expected sense, of a clairvoyant over a non-clairvoyant (both with infinite computational resources). The next lemma relates the offline problem and \u03c6 and shows that the operators max and E commute for clairvoyant decision processes.  2 In two-stage stochastic programming, a low EVPI makes the problem much easier because an optimal decision is also a good one for each specific scenario [Birge and Louveaux, 1997, ch. 4]. However, this is no longer true in the multistage case. Consider the three-stage problem depicted in Figure 1. Black dots represent decisions variables x 1 and x 2 . Stochastic variables \u03be 1 and \u03be 2 have no influence and are not represented. The white dot represents \u03be 3 which take values 0 and 1 with equal probability. Leaves are tagged with their utilities and a is large positive number. The value of the EVPI and the anticipatory gap \u2206 g for each state are the following:\nstate root state x 1 = 0 \u2206 g 0 1 /2(\u03b5 + a) \u03b7 \u03b5 1 /2(\u03b5 + a)\nOn this problem, the EVPI is \u03b5: an optimal solution has a score of \u03b5, whatever the scenario. The expected value of the optimal policy is zero. However, the online anticipatory algorithm always chooses x 1 = 0 and thus has an expected utility of 1 /2(\u03b5 \u2212 a). Therefore anticipatory algorithms may behave poorly even with a low EVPI . Moreover, in this case, the inequality of Theorem 1 is tight when m converges to +\u221e, since the GAG equals 1 /2(\u03b5 + a). The phenomenon comes from the fact that the EVPI of the problem is low although the EVPI of the node (x 1 = 0) is \u03b5 \u2212 1 /2(\u03b5 \u2212 a) = 1 /2(\u03b5 + a) and thus much larger. This does not contradict the super-martingale property of [Dempster, 1998] because Dempster considers optimal decision processes, which is not the case of anticipatory algorithms. As a result, the expected global loss of the anticipatory algorithm cannot be bounded by the root EVPI . The example may suggest that the maximum of the EVPIs at each node of the tree gives an upper bound of the EGL, but this is not true either. Figure 2 presents a stochastic program, where 'Sub' are clones of the problem in Figure 1, with variables indices shifted. On this problem, the optimal solutions to the scenarios have an expected expected utility of \u03b5, and those of the anticipatory algorithm (with m = \u221e) have expected utility 1 /4(\u22123a+ \u03b5); the EGL thus equals 3 /4(a+ \u03b5). By Theorem 1, the GAG is not smaller than the EGL (m = \u221e: no sampling error). As a result, the GAG is greater than the maximum of the EVPI over all nodes, which is equal to 1 /2(\u03b5 + a). Finally, the following theorem gives one more reason why the concept of anticipatory gap is of interest.\nTheorem 3 For any state s t , we have \u03b7(s t ) \u2265 \u2206 g (s t ) and there exist cases in which the inequality is strict.\nProof. \u03b7(s t ) = \u03c6(s t ) \u2212 \u03c0(s t ). Recall that \u03c0(s t ) is the optimal expected utility of a non-anticipative decision process given s t . Because of non-anticipativity and because X (s t ) is finite, there exists an optimal decision x \u22c6 \u2208 X (s t ) such that \u03c0(s\nt ) = E [\u03c0(s t+1 ) |s t , x t = x \u22c6 ]. Now max x\u2208X (st) E [O(s t , x, \u03be) |s t ] \u2265 E [O(s t , x \u22c6 , \u03be) |s t ] \u2265 E [\u03c0(s t+1 ) |s t , x t = x \u22c6 ] .\nand thus, using lemma 3,\n\u03b7(s t ) \u2265 E [O(s t , \u03be) |s t ] \u2212 max x\u2208X (st) E [O(s t , x, \u03be) |s t ] \u2265 \u2206 g (s t ).\nThis proves the inequality. The second part of the theorem is proven by the example of Figure 1, on which the root node s 1 satisfies \u03b7(s 1 ) = \u03b5 > 0 = \u2206 g (s 1 ). 2", "publication_ref": ["b2"], "figure_ref": ["fig_3", "fig_4", "fig_3", "fig_3"], "table_ref": []}, {"heading": "Conclusion and Research Perspectives", "text": "Anticipatory algorithms have been shown experimentally to be successful in tackling a variety of large multistage stochastic integer programs which were outside the scope of a priori methods such as (PO)MDPs and multistage stochastic programming. This paper studied the performance of anticipatory algorithms in terms of their sampling error and the anticipatory gap of the problems. It showed that, whenever the anticipatory gap is small, anticipatory algorithms are scalable and provide high-quality solutions in the expected sense with a logarithmic number of samples in the problem size.\nThe paper also studied how to bound the anticipatory gap both theoretically and experimentally, showing that a simple packet scheduling problem admits a small anticipatory gap and providing experimental evidence on several large multistage stochastic programs. Finally, the paper indicated that the anticipatory gap is an important concept and studied its relationships with the expected value of perfect information.\nThere are many research directions opened by this research. First, It is desirable to to deepen the understanding of the problem features (both combinatorial and statistical) which lead to small (or large) anticipatory gaps. Second, it is also important to study novel anticipatory algorithms for applications with non-negligible anticipatory gaps. Here an interesting direction is to borrow ideas from Real-Time Dynamic Programming [Barto et al., 1995;Paquet et al., 2005]. Indeed, because the estimation of \u03c6(s t ) obtained by relaxing non-anticipativity constraints is an upper bound of its online value \u03c0(s t ) with high probability, a RTDP approach can produce increasingly tighter approximations of the optimal policy until decision time. Despite negative complexity results ([Kearns et al., 1999]), we believe that, if the GAG is not too large, high-quality decisions could be obtained in reasonable time. Indeed, since the far future is unlikely to be as important as the near future for the current decision, we may hope that small trees will be sufficient for many applications.", "publication_ref": ["b0", "b2"], "figure_ref": [], "table_ref": []}, {"heading": "Acknowledgments", "text": "This research is partly supported by NSF Award DMI-0600384 and ONR DEPSCOR Award N000140610607. Thanks to the reviewers for their interesting suggestions.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Towards stochastic constraint programming: A study of online multi-choice knapsack with deadlines", "journal": "", "year": "1995", "authors": "[ References;  Barto"}, {"ref_id": "b1", "title": "Regrets only! online stochastic optimization under time constraints", "journal": "Cambridge University Press", "year": "1997", "authors": "; R Van Hentenryck; P Bent; ; R Van Hentenryck; I Bent; P Katriel; ; J R Van Hentenryck; F Birge; ; Louveaux;  Borodin; ; A El-Yaniv; R Borodin; ; H El-Yaniv; R Chang; E Givan;  Chong"}, {"ref_id": "b2", "title": "A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes", "journal": "Springer", "year": "1998", "authors": "; M A H Dempster;  Dempster;  Hentenryck"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Lemma 33For any state s t , \u03c6(s t ) = E [O(s t , \u03be)|s t ] .", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 1 :1Figure 1: low EVPI but high Global Anticipatory Gap", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: GAG higher than max of the EVPIs", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "Q = max x1\u2208X (s1) E max x2\u2208X (s2) E . . . max xT \u2208X (sT ) f (x, \u03be) ,", "formula_coordinates": [2.0, 69.72, 280.41, 211.57, 16.05]}, {"formula_id": "formula_1", "formula_text": "t = (x 1..t\u22121 , \u03be 1..t )", "formula_coordinates": [2.0, 221.28, 313.41, 75.76, 10.65]}, {"formula_id": "formula_2", "formula_text": "t = x 1..t } , O(s t , \u03be) = max {f (y, \u03be) | y dec. proc., y 1..t\u22121 = x 1..t\u22121 } = max x\u2208X (st) O(s t , x, \u03be).", "formula_coordinates": [2.0, 67.68, 523.88, 233.7, 44.26]}, {"formula_id": "formula_3", "formula_text": "t = \u03be 1..t foreach x \u2208 X (s t ) do g(x) \u2190 1 m m i=1 O(s t , x, \u03be i ) x t \u2190 argmax x\u2208X (st) g(x)", "formula_coordinates": [2.0, 324.96, 74.13, 227.53, 45.21]}, {"formula_id": "formula_4", "formula_text": "\u2206(s t , x) = E [O(s t , \u03be) \u2212 O(s t , x, \u03be) |s t ] .", "formula_coordinates": [2.0, 352.08, 292.76, 168.85, 10.65]}, {"formula_id": "formula_5", "formula_text": "EVC \u2212 E [f (x , \u03be)] = T t=1 E [\u2206(s t , x t )]", "formula_coordinates": [2.0, 358.08, 381.4, 156.73, 30.31]}, {"formula_id": "formula_6", "formula_text": "A t = E [C t \u2212 f (x, \u03be)]", "formula_coordinates": [2.0, 315.0, 416.13, 242.95, 21.45]}, {"formula_id": "formula_7", "formula_text": "A t = E [C t \u2212 C t+1 + C t+1 \u2212 f (x, \u03be)] = E [C t \u2212 C t+1 ] + A t+1 = E [\u2206(s t+1 , x t+1 )] + A t+1 .", "formula_coordinates": [2.0, 358.8, 441.68, 155.29, 38.61]}, {"formula_id": "formula_8", "formula_text": "Finally EVC \u2212 E [f (x , \u03be)] = E [C 0 \u2212 f (x , \u03be)] = A 0 . 2", "formula_coordinates": [2.0, 315.0, 495.44, 243.0, 21.5]}, {"formula_id": "formula_9", "formula_text": "\u2206 g (s t ) = min x\u2208X (st) \u2206(s t , x).", "formula_coordinates": [2.0, 381.12, 578.85, 110.77, 16.05]}, {"formula_id": "formula_10", "formula_text": "\u2206 c (s t , x) = \u2206(s t , x) \u2212 \u2206 g (s t ).", "formula_coordinates": [2.0, 371.52, 614.72, 129.85, 10.65]}, {"formula_id": "formula_11", "formula_text": "max x\u2208X (st) E [O(s t , x, \u03be)|st] \u2212 E max x\u2208X (st) O(s t , x, \u03be) st .", "formula_coordinates": [2.0, 325.44, 662.0, 222.13, 16.42]}, {"formula_id": "formula_12", "formula_text": "x \u22c6 be argmax E [O(s t , x, \u03be) |s t ] (break ties arbi- trarily). Then E [\u2206 c (s t , x t ) |s t ] \u2264 x\u2208X (st) \u2206 c (s t , x) exp \u2212m\u2206 c (s t , x) 2 2\u03c3(s t , x) 2 , where \u03c3(s t , x) is the standard deviation of O(s t , x, \u03be) \u2212 O(s t , x \u22c6 , \u03be) given s t .", "formula_coordinates": [3.0, 54.0, 77.45, 243.08, 85.45]}, {"formula_id": "formula_13", "formula_text": "E [\u2206 c (s t , x t )] = x\u2208X (st) \u2206 c (s t , x)P(x t = x).", "formula_coordinates": [3.0, 88.56, 197.61, 173.77, 21.57]}, {"formula_id": "formula_14", "formula_text": "x t = x implies \u2200x \u2032 \u2208 X (s t ), g(x \u2032 ) \u2264 g(x). Therefore P (x t = x) \u2264 P (g(x) \u2265 g(x \u22c6 )).", "formula_coordinates": [3.0, 54.0, 225.81, 243.06, 31.92]}, {"formula_id": "formula_15", "formula_text": "g(x) \u2212 g(x \u22c6 ) = 1 m m i=1 O(s t , x, \u03be i ) \u2212 O(s t , x \u22c6 , \u03be i )", "formula_coordinates": [3.0, 67.2, 274.13, 210.64, 30.61]}, {"formula_id": "formula_16", "formula_text": "1 m \u03c3(s t , x) 2 . Finally, if X \u223c N (\u00b5, \u03c3 2 ) with \u00b5 < 0, then P (X \u2265 0) \u2264 exp \u2212 \u00b5 2 2\u03c3 2 (Chernoff bound). 2", "formula_coordinates": [3.0, 54.0, 330.17, 243.09, 30.25]}, {"formula_id": "formula_17", "formula_text": "GAG = E \uf8ee \uf8f0 max x1..T xi\u2208X (si) T t=1 \u2206 g (s t ) \uf8f9 \uf8fb .", "formula_coordinates": [3.0, 104.28, 407.17, 142.45, 40.85]}, {"formula_id": "formula_18", "formula_text": "EGL \u2264 GAG + O e \u2212Km", "formula_coordinates": [3.0, 118.2, 496.48, 108.91, 11.33]}, {"formula_id": "formula_19", "formula_text": "K = min st,x\u2208X (st) \u2206c(st,x)>0 \u2206 c (s t , x) 2 2\u03c3(s t , x) 2 .", "formula_coordinates": [3.0, 119.04, 529.37, 112.81, 32.17]}, {"formula_id": "formula_20", "formula_text": "EGL = T t=1 E [\u2206(s t , x t )] \u2264 T t=1 E [\u2206 g (s t )] + E [\u2206 c (s t , x t )] .", "formula_coordinates": [3.0, 54.0, 583.24, 247.09, 30.31]}, {"formula_id": "formula_21", "formula_text": "T t=1 E [\u2206 g (s t )] = E T t=1 \u2206 g (s t ) \u2264 E max x1..T T t=1 \u2206 g (s t ) ,", "formula_coordinates": [3.0, 56.28, 628.84, 239.05, 30.49]}, {"formula_id": "formula_22", "formula_text": "T t=1 E [\u2206 c (s t , x t )] \u2264 T |X| F max e \u2212Km . 2", "formula_coordinates": [3.0, 95.4, 676.61, 201.6, 30.49]}, {"formula_id": "formula_23", "formula_text": "(0 \u2264 f \u2264 T v k ).", "formula_coordinates": [3.0, 351.84, 365.84, 63.57, 10.65]}, {"formula_id": "formula_24", "formula_text": "\u2206(s t , i) = E [O(s t , \u03be) \u2212 O(s t , i, \u03be) |s t ] = E [O(s t , \u03be) \u2212 O(s t , i, \u03be) |s t , y t = j ] P (y t = j) and symmetrically \u2206(s t , j) = E [O(s t , \u03be) \u2212 O(s t , j, \u03be) |s t ] = E [O(s t , \u03be) \u2212 O(s t , j, \u03be) |s t , y t = i ] P (y t = i) .", "formula_coordinates": [3.0, 315.0, 635.24, 240.49, 66.45]}, {"formula_id": "formula_25", "formula_text": "\u2206(s t , i) \u2264 (v j \u2212 v i ) 1 \u2212 k>i q k .", "formula_coordinates": [4.0, 102.48, 160.28, 146.05, 21.09]}, {"formula_id": "formula_26", "formula_text": "Theorem 2 Let s t be a state. Define \u2206 g (s t ) as 1 m m i=1 max x\u2208X (st) O(s t , x, \u03be i ) \u2212 max x\u2208X (st) m i=1 O(s t , x, \u03be i ) ,", "formula_coordinates": [4.0, 315.0, 398.13, 238.57, 48.09]}, {"formula_id": "formula_27", "formula_text": "P", "formula_coordinates": [4.0, 349.08, 477.74, 6.09, 9.32]}, {"formula_id": "formula_28", "formula_text": "lim m\u2192+\u221e \u2206 g (s t ) = \u2206 g (s t ) s t = 1.", "formula_coordinates": [4.0, 364.08, 477.09, 159.97, 14.96]}, {"formula_id": "formula_29", "formula_text": "\u03c6(s t ) = max {E [f (x, \u03be)|s t ] | x dec. proc.} \u03c0(s t ) = max {E [f (x, \u03be)|s t ] | x non-anticip. dec. proc.} .", "formula_coordinates": [5.0, 57.84, 548.84, 235.33, 24.57]}, {"formula_id": "formula_30", "formula_text": "state root state x 1 = 0 \u2206 g 0 1 /2(\u03b5 + a) \u03b7 \u03b5 1 /2(\u03b5 + a)", "formula_coordinates": [5.0, 376.44, 371.37, 120.16, 32.28]}, {"formula_id": "formula_31", "formula_text": "t ) = E [\u03c0(s t+1 ) |s t , x t = x \u22c6 ]. Now max x\u2208X (st) E [O(s t , x, \u03be) |s t ] \u2265 E [O(s t , x \u22c6 , \u03be) |s t ] \u2265 E [\u03c0(s t+1 ) |s t , x t = x \u22c6 ] .", "formula_coordinates": [6.0, 66.84, 263.21, 217.33, 48.97]}, {"formula_id": "formula_32", "formula_text": "\u03b7(s t ) \u2265 E [O(s t , \u03be) |s t ] \u2212 max x\u2208X (st) E [O(s t , x, \u03be) |s t ] \u2265 \u2206 g (s t ).", "formula_coordinates": [6.0, 70.32, 334.04, 210.37, 31.89]}], "doi": ""}