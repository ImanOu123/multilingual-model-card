{"title": "Multi-Source Neural Machine Translation with Data Augmentation", "authors": "Yuta Nishimura; Katsuhito Sudoh; Graham Neubig; Satoshi Nakamura", "pub_date": "", "abstract": "Multi-source translation systems translate from multiple languages to a single target language. By using information from these multiple sources, these systems achieve large gains in accuracy. To train these systems, it is necessary to have corpora with parallel text in multiple sources and the target language. However, these corpora are rarely complete in practice due to the difficulty of providing human translations in all of the relevant languages. In this paper, we propose a data augmentation approach to fill such incomplete parts using multi-source neural machine translation (NMT). In our experiments, results varied over different language combinations but significant gains were observed when using a source language similar to the target language.", "sections": [{"heading": "Introduction", "text": "Machine Translation (MT) systems usually translate one source language to one target language. However, in many real situations, there are multiple languages in the corpus of interest. Examples of this situation include the multilingual official document collections of the European parliament [1] and the United Nations [2]. These documents are manually translated into all official languages of the respective organizations. Many methods have been proposed to use these multiple languages in translation systems to improve the translation accuracy [3,4,5,6]. In almost all cases, multilingual machine translation systems output better translations than one-to-one systems, as the system has access to multiple sources of information to reduce ambiguity in the target sentence structure or word choice.\nHowever, in contrast to the more official document collections mentioned above where it is mandated that all translations in all languages, there are also more informal multilingual captions such as those of talks [7] and movies [8]. Because these are based on voluntary translation efforts, large portions of them are not translated, especially into languages with a relatively small number of speakers.\nNishimura et al. [9] have recently proposed a method for multi-source NMT that is able to deal with the case of missing source data encountered in these corpora. The implementation is simple: missing source translations are replaced with a special symbol NULL as shown in Figure 1(a ", "publication_ref": ["b0", "b1", "b2", "b3", "b4", "b5", "b6", "b7", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Multi-source NMT", "text": "There are two major approaches to multi-source NMT; multiencoder NMT [10] and mixture of NMT Experts [11]. In this work, we focus on the multi-encoder NMT that showed better performance in most cases in Nishimura et al. [9]. Multi-encoder NMT [10] is similar to the standard attentional NMT framework [12] but uses multiple encoders corresponding to the source languages and a single decoder.\nSuppose we have two LSTM-based encoders and their hidden and cell states at the end of the inputs are h 1 , h 2 and c 1 , c 2 , respectively. Multi-encoder NMT initializes its decoder hidden state h and cell state c using these encoder states as follows:\nh = tanh(W c [h 1 ; h 2 ])(1)\nc = c 1 + c 2 (2)\nAttention is then defined over encoder states at each time step t and resulting context vectors d 1 t and d 2 t are concatenated together with the corresponding decoder hidden state h t to calculate the final context vectorh t .\nh t = tanh(W c [h t ; d 1 t ; d 2 t ])(3)\nOur multi-encoder NMT implementation is basically similar to the original one [10] but has a difference in its attention mechanism. We use global attention used in Nishimura et al. [9], while Zoph and Knight used local-p attention. The global attention allows the decoder to look at everywhere in the input, while the local-p attention forces to focus on a part of the input [13].", "publication_ref": ["b9", "b10", "b8", "b9", "b11", "b9", "b8", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Data Augmentation for NMT", "text": "Sennrich et al. proposed a method to use monolingual training data in the target language for training NMT systems, with no changes to the network architecture [14]. It first trains a seed target-to-source NMT model using a parallel corpus and then translates the monolingual target language sentences into the source language to create a synthetic parallel corpus. It finally trains a source-to-target NMT model using the seed and synthetic parallel corpora. This very simple method called back-translation makes effective use of available resources, and achieves substantial gains. Imamura et al. proposed a method that enhances the encoder and attention using target monolingual corpora by generating mutliple source sentences via sampling as an extension of the backtranslation [15].\nThere are also other approaches for data augmentation other than back-translation. Wang et al. proposed a method of randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies [16]. Kim and Rush proposed a sequence-level knowledge distillation in NMT that uses machine translation results by a large teacher model to train a small student model as well as ground-truth translations [17].\nOur work is an extension of the back-translation approach in multilingual situations by generating pseudotranslations using multi-source NMT.", "publication_ref": ["b13", "b14", "b15", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Proposed Method", "text": "We propose three types of data augmentation for multiencoder NMT; \"fill-in\", \"fill-in and replace\" and \"fill-in and add.\" Firstly, we explain about the data requirements and overall framework using Figure 1(b). We used three languages; English, Czech and Slovak. Our goal is to get the Slovak translation, and to do so we take three steps. There are not any missing data in English translations, but Slovak and Czech translations have some missing data. In the first step, we train a multi-encoder NMT model (Source: English and Slovak, Target: Czech) to get Czech pseudotranslations using the baseline method, which is to replace a missing input sentence with a special symbol NULL . In the second step, we create Czech pseudo-translations using multi-encoder NMT which was trained on the first step. We conducted three types of augmentation, which we introduce later. Finally in the third step, we switch the role of Czech and Slovak, in other words, we train a new multi-encoder NMT model (Source: English and Czech, Target: Slovak). At this time, we use Slovak pseudo-translations in the source language side. This method is similar to back-translation but taking advantage of the fact that we have an additional source of knowledge (Czech or Slovak) when trying to augment the other language (Slovak or Czech respectively).\nWe next introduce three types of augmentation. Figure 2 illustrates their examples in {English, Czech}-to-{Slovak} case where one Czech sentence is missing.\n(a) fill-in: where only missing parts in the corpus are filled up with pseudo-translations.\n(b) fill-in and replace: where we both augment the missing part and replace original translations with pseudotranslations in the source language except English whose translations has not any missing data. The motivation behind this method is not to use unreliable translation. Morishita et al. [18] demonstrated the effectiveness of applying back-translation for an unreliable part of a provided corpus. Translations of TED talks are from many independent volunteers, so there may be some differences between translations other than original English, or even they may include some free or over-simplified translations. We aim to fill such a gap using data augmentation.\n(c) fill-in and add: where we both augment the missing part and added pseudo-translations from original translations in the source language except English. This helps prevent introduction of too much noise due to the complete replacement of original translations with pseudo-translations in the second method. ", "publication_ref": ["b17"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Experiment", "text": "We conducted MT experiments to examine the performance of the proposed method using actual multilingual corpora of TED Talks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Data", "text": "We used a collection of transcriptions of TED Talks and their multilingual translations. The numbers of these voluntary translations differs significantly by language. We chose three different language sets for the experiments: {English (en), Croatian (hr), Serbian (sr)}, {English (en), Slovak (sk), Czech (cs)}, and {English (en), Vietnamese (vi), Indonesian (id)}. Since the great majority of TED talks are in English, the experiments were designed for the translation from English to another language with the help of the other language in the language set, with no missing portions in the English sentences. Table 1 shows the number of training sentences for each language set. At test time, we experiment with a complete corpus with both source sentences represented, as this is the sort of multi-source translation setting that we are aiming to create models for.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_1"]}, {"heading": "Baseline Methods", "text": "We compared the proposed methods with the following three baseline methods.\nOne-to-one NMT: a standard NMT model from one source language to another target language. The source language is fixed to English in the experiments. If the target language part is missing in the parallel corpus, such sentences pairs cannot be used in training so they are excluded from the training set.\nMulti-encoder NMT with back-translation: a multiencoder NMT system using English-to-X NMT to fill up the missing parts in the other source language X. 1\nMulti-encoder NMT with NULL : a multi-encoder NMT system using a special symbol NULL to fill up the missing parts in the other source language X [9].", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "NMT settings", "text": "NMT settings are the same for all the methods in the experiments. We use bidirectional LSTM encoders [12], and global attention and input feeding for the NMT model [13].\nThe number of dimensions is set to 512 for the hidden and embedding layers. Subword segmentation was applied using SentencePiece [19]. We trained one subword segmentation model for English and another shared between the other two languages in the language set because the amount of training data for the languages other than English was small. For parameter optimization, we used Adam [20] with gradient clipping of 5. We performed early stopping, saving parameter values that had the best log likelihoods on the validation data and used them when decoding test data.", "publication_ref": ["b11", "b12", "b18", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "Table 2 shows the results in BLEU [21]. We can see that our proposed methods demonstrate larger gains in BLEU than baseline methods in two language sets: {English, Croatian, Serbian}, {English, Slovak, Czech}. On these pairs, we can say that our proposed method is an effective way for using incomplete multilingual corpora, exceeding other reasonably strong baselines. However, in {English, Vietnamese, Indone-sian}, our proposed methods obtained lower scores than the baseline methods. We observed that the improvement by the use of multi-encoder NMT against one-to-one NMT in the baseline was significantly smaller than the other language sets, so multi-encoder NMT was not as effective compared to one-to-one NMT in the first place. Our proposed method is affected by which languages to use, and the proposed method is likely more effective for similar language pairs because the expected accuracy of the pseudo-translation gets better by the help of lexical and syntactic similarity including shared subword entries.", "publication_ref": ["b20"], "figure_ref": [], "table_ref": ["tab_2"]}, {"heading": "Discussion", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Different Types of Augmentation", "text": "We examined three types of augmentation: \"fill-in\", \"fill-in and replace\", \"fill-in and add\". In Table 2, we can see that there were no significant differences among them, despite the fact that their training data were very different from each other. We conducted additional experiments using incomplete corpora with lower quality augmentation by one-to-one NMT to investigate the differences of the three types of augmentation. We created three types of pseudo-multilingual corpora using back-translation from one-to-one NMT and trained multi-encoder NMT models using them. Our expectation here was that the aggressive use of low quality pseudotranslations caused to contaminate the training data and to decrease the translation accuracy. Table 3 shows the results. In {English, Croatian, Ser-bian} and {English, Slovak, Czech}, we obtained significant drop in BLEU scores with the aggressive strategies (\"fill-in and replace\" and \"fill-in and add\"), while there are few differences in {English, Vietnamese, Indonesian}. One possible reason is that the quality of pseudo-translations by one-toone NMT in Indonesian and Vietnamese was better than the other languages; in other words, the BLEU from one-to-one NMT in Table 2 was sufficiently good without multi-source NMT. Thus the translation performance for Croatian, Serbian, Slovak and Czech could not improve in the experiments here due to noisy pseudo-translations of those languages. Contrary, the BLEU from \"fill-in and add\" was the highest when the target language was Indonesian. We hypothesize that this is due to much smaller fraction of the missing parts in Indonesian corpus as shown in Table 1, so there should be little room for improvement if we fill in only the missing parts even if the accuracy of the pseudo-translations is relatively high.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_2", "tab_3", "tab_2", "tab_1"]}, {"heading": "Iterative Augmentation", "text": "It can be noted that if we have a better multi-source NMT system, it can be used to produce better pseudo-translations. This leads to a natural iterative training procedure where we alternatively update the multi-source NMT systems into the two target languages.\nTable 4 shows the results of {English, Croatian, Ser-bian}. We found that this produced negative results; BLEU decreased gradually in every step. We observed very similar results in the other language pairs, while we omit the actual numbers here. This indicates that the iterative training may be introducing more noise than it is yielding improvements, and thus may be less promising than initially hypothesized.", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_4"]}, {"heading": "Non-parallelism", "text": "A problem in the use of multilingual corpora is nonparallelism. In case of TED multilingual captions, they are translated from English transcripts independently by many volunteers, which may cause some differences in details of the translation in the various target languages. For example in {English, Croatian, Serbian}, Croatian and Serbian translations may not be completely parallel. Table 5 shows such an example where the Serbian translation does not have a phrase corresponding to \"let me.\" This kind of non-parallelism may be resolved by overriding such translations with pseudo-translations with \"fill-in and replace\" and \"fill-in and add\". Here, the Serbian pseudo-translation includes the corresponding phrase \"Dozvolite mi\" and can be used to compensate for the missing information. This would be one possible reason of the improvements by \"fill-up and replace\" or \"fill-up and add\".", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Conclusions", "text": "In this paper, we examined data augmentation of incomplete multilingual corpora in multi-source NMT. We proposed three types of augmentation; fill-in, fill-in and replace, fill-in and add. Our proposed methods proved better than baseline system using the corpus where missing part was filled up with \" NULL \", although results depended on the language pair. One limitation in the current experiments with a set of three languages was that missing parts in the test sets could not be filled in. This can be resolved if we use more languages, and we will investigate this in future work.    ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "journal": "AAMT", "year": "2005", "authors": "P Koehn"}, {"ref_id": "b1", "title": "The United Nations Parallel Corpus v1.0", "journal": "", "year": "2016-05", "authors": "M Ziemski; M Junczys-Dowmunt; B Pouliquen"}, {"ref_id": "b2", "title": "Multi-Task Learning for Multiple Language Translation", "journal": "Association for Computational Linguistics", "year": "2015-07", "authors": "D Dong; H Wu; W He; D Yu; H Wang"}, {"ref_id": "b3", "title": "Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "journal": "Association for Computational Linguistics", "year": "2016-06", "authors": "O Firat; K Cho; Y Bengio"}, {"ref_id": "b4", "title": "Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "M Johonson; M Schuster; Q V Le; M Krikun; Y Wu; Z Chen; N Thorat; F Vigas; M Wattenberg; G Corrado; M Hughes; J Dean"}, {"ref_id": "b5", "title": "Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder", "journal": "", "year": "2016-12", "authors": "T.-L Ha; J Niehues; A Waibel"}, {"ref_id": "b6", "title": "WIT 3 : Web Inventory of Transcribed and Translated Talks", "journal": "", "year": "2012-05", "authors": "M Cettolo; C Girardi; M Federico"}, {"ref_id": "b7", "title": "News from OPUS -A Collection of Multilingual Parallel Corpora with Tools and Interfaces", "journal": "John Benjamins", "year": "2009", "authors": "J Tiedemann"}, {"ref_id": "b8", "title": "Multi-Source Neural Machine Translaion with Missing Data", "journal": "Association for Computational Linguistics", "year": "2018-07", "authors": "Y Nishimura; K Sudoh; G Neubig; S Nakamura"}, {"ref_id": "b9", "title": "Multi-Source Neural Translation", "journal": "", "year": "", "authors": "B Zoph; K Knight"}, {"ref_id": "b10", "title": "Ensemble Learning for Multi-Source Neural Machine Translation", "journal": "", "year": "2016-12", "authors": "E Garmash; C Monz"}, {"ref_id": "b11", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "journal": "", "year": "2015-05", "authors": "D Bahdanau; K Cho; Y Bengio"}, {"ref_id": "b12", "title": "Effective Approaches to Attention-based Neural Machine Translation", "journal": "", "year": "2015-09", "authors": "T Luong; H Pham; C D Manning"}, {"ref_id": "b13", "title": "Improving neural machine translation models with monolingual data", "journal": "", "year": "2016", "authors": "R Sennrich; B Haddow; A Birch"}, {"ref_id": "b14", "title": "Enhancement of encoder and attention using target monolingual corpora in neural machine translation", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "K Imamura; A Fujita; E Sumita"}, {"ref_id": "b15", "title": "Switchout: an efficient data augmentation algorithm for neural machine translation", "journal": "", "year": "2018", "authors": "X Wang; H Pham; Z Dai; G Neubig"}, {"ref_id": "b16", "title": "Sequence-level knowledge distillation", "journal": "Association for Computational Linguistics", "year": "2016-11", "authors": "Y Kim; A M Rush"}, {"ref_id": "b17", "title": "NTT Neural Machine Translation Systems at WAT 2017", "journal": "", "year": "2017", "authors": "M Morishita; J Suzuki; M Nagata"}, {"ref_id": "b18", "title": "Subword regularization: Improving neural network translation models with multiple subword candidates", "journal": "", "year": "2018", "authors": "T Kudo"}, {"ref_id": "b19", "title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015-05", "authors": "D P K Kingma; J Ba"}, {"ref_id": "b20", "title": "BLEU: a Method for Automatic Evaluation of Machine Translation", "journal": "", "year": "2002-07", "authors": "K Papineni; S Roukos; T Ward; W.-J Zhu"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Example of three types of augmentation; Language Pair is {English, Czech}-to-{Slovak} and Czech translation corresponding to \"How are you?\" is missisng. In this example, the dotted background indicates the pseudo-translation produced from multi-source NMT and the white background means the original translation.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "\"train\" shows the number of available training sentences, and \"missing\" shows the number and the fraction of missing sentences in comparison with English ones.", "figure_data": "PairTrgtrainmissingen-hr/srhr sr118949 35564 (29.9%) 133558 50203 (37.6%)en-sk/cssk 100600 58602 (57.7%) cs 59918 17380 (29.0%)en-vi/idvi id160984 87816 (54.5%) 82592 9424 (11.4%)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Main results in BLEU for English-Croatian/Serbian (en-hr/sr), English-Slovak/Czech (en-sk/cs), and English-Vietnamese/Indonesian (en-vi/id).", "figure_data": "baseline methodproposed methodPairTrgone-to-one (En-to-Trg)multi-encoder NMT (fill up with symbol)multi-encoder NMT (back translation)fill-infill-in and replacefill-in and adden-hr/srhr sr20.21 16.4228.18 23.8527.57 22.7329.17 24.4129.37 24.9629.40 24.15en-sk/cssk cs13.79 14.7220.27 19.8819.83 19.5420.26 20.7820.43 20.9020.59 20.61en-vi/idvi id24.60 24.8925.70 26.8926.66 26.3426.73 26.4026.48 25.7326.32 26.21"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The difference of three types of augmen-", "figure_data": "tation in BLEU for English-Croatian/Serbian (en-hr/sr), English-Slovak/Czech (en-sk/cs), and English-Vietnamese/Indonesian (en-vi/id). We used one-to-one model to produce pseudo-translations.multi-encoder NMT (back-translation)PairTrg fill-infill-in and replacefill-in and adden-hr/srhr sr27.57 22.7324.05 17.7724.79 22.02en-sk/cssk 19.83 cs 19.5416.75 17.0418.16 18.40en-vi/idvi id26.66 26.3426.39 23.9026.65 26.677. AcknowledgementsPart of this work was supported by JSPS KAKENHI Grant Numbers and JP16H05873 and JP17H06101."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "BLEU (and BLEU gains compared to step 1) in each step of iterative augmentation. +0.00) 29.03 (-0.14) 29.10 (-0.07) 29.05 (-0.12) sr 24.41 (+0.00) 24.18 (-0.23) 24.17 (-0.24) 23.95 (-0.46)", "figure_data": "PairTrgstep 1step 2step 3step 4en-hr/srhr29.17 ("}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Example of the Serbian pseudo-translation. This pseudo-translation is the output of {English, Croatian}-to-Serbian translation. So let me conclude with just a remark to bring it back to the theme of choices. Dozvolite mi da zakljuim samo jednom opaskom, da se vratim na temu izbora.", "figure_data": "TypeSentenceOriginal (En) Original (Sr) Pseudo (Sr)Da zakljuim jednom konstatacijom kojom se vraam na temu izbora."}], "formulas": [{"formula_id": "formula_0", "formula_text": "h = tanh(W c [h 1 ; h 2 ])(1)", "formula_coordinates": [2.0, 125.93, 272.24, 165.76, 12.44]}, {"formula_id": "formula_1", "formula_text": "c = c 1 + c 2 (2)", "formula_coordinates": [2.0, 147.54, 294.85, 144.15, 12.44]}, {"formula_id": "formula_2", "formula_text": "h t = tanh(W c [h t ; d 1 t ; d 2 t ])(3)", "formula_coordinates": [2.0, 117.89, 378.64, 173.79, 13.55]}], "doi": ""}