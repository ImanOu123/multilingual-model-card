{"title": "Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner Product Search (MIPS)", "authors": "Anshumali Shrivastava; Ping Li", "pub_date": "2014-05-22", "abstract": "We 1 present the first provably sublinear time algorithm for approximate Maximum Inner Product Search (MIPS). Our proposal is also the first hashing algorithm for searching with (un-normalized) inner product as the underlying similarity measure. Finding hashing schemes for MIPS was considered hard. We formally show that the existing Locality Sensitive Hashing (LSH) framework is insufficient for solving MIPS, and then we extend the existing LSH framework to allow asymmetric hashing schemes. Our proposal is based on an interesting mathematical phenomenon in which inner products, after independent asymmetric transformations, can be converted into the problem of approximate near neighbor search. This key observation makes efficient sublinear hashing scheme for MIPS possible. In the extended asymmetric LSH (ALSH) framework, we provide an explicit construction of provably fast hashing scheme for MIPS. The proposed construction and the extended LSH framework could be of independent theoretical interest. Our proposed algorithm is simple and easy to implement. We evaluate the method, for retrieving inner products, in the collaborative filtering task of item recommendations on Netflix and Movielens datasets.", "sections": [{"heading": "Introduction and Motivation", "text": "The focus of this paper is on the problem of Maximum Inner Product Search (MIPS). In this problem, we are given a giant data vector collection S of size N , where S \u2282 R D , and a given query point q \u2208 R D . We are interested in searching for p \u2208 S which maximizes (or approximately maximizes) the inner product q T p. Formally, we are interested in efficiently computing p = arg max\nx\u2208S q T x (1)\nThe MIPS problem is related to the problem of near neighbor search (NNS), which instead requires computing p = arg min\nx\u2208S q \u2212 x 2 2 = arg min x\u2208S ( x 2 2 \u2212 2q T x)(2)\nThese two problems are equivalent if the norm of every element x \u2208 S is constant. Note that the value of the norm q 2 has no effect as it is a constant throughout and does not change the identity of arg max or arg min. There are many scenarios in which MIPS arises naturally at places where the norms of the elements in S have very significant variations [17] and can not be controlled. As a consequence, existing fast algorithms for the problem of approximate NNS can not be directly used for solving MIPS.\nHere we list a number of practical scenarios where the MIPS problem is solved as a subroutine: (i) recommender system, (ii) large-scale object detection with DPM, (iii) structural SVM, and (iv) multi-class label prediction.", "publication_ref": ["b16"], "figure_ref": [], "table_ref": []}, {"heading": "Recommender Systems", "text": "Recommender systems are often based on collaborative filtering which relies on past behavior of users, e.g., past purchases and ratings. Latent factor modelling based on matrix factorization [19] is a popular approach for solving collaborative filtering. In a typical matrix factorization model, a user i is associated with a latent user characteristic vector u i , and similarly, an item j is associated with a latent item characteristic vector v j . The rating r i,j of item j by user i is modeled as the inner product between the corresponding characteristic vectors. A popular generalization of this framework, which combines neighborhood information with latent factor approach [18], leads to the following model:\nr i,j = \u00b5 + b i + b j + u T i v j (3\n)\nwhere \u00b5 is the over all constant mean rating value, b i and b j are user and item biases, respectively. Note that Eq. (3) can also be written as r i,j = \u00b5 + [u i ; b i ; 1] T [v j ; 1; b j ], where the form [x; y] is the concatenation of vectors x and y.\nRecently, [6] showed that a simple computation of u i and v j based on naive SVD of the sparse rating matrix outperforms existing models, including the neighborhood model, in recommending top-ranked items. In this setting, given a user i and the corresponding learned latent vector u i finding the right item j, to recommend to this user, involves computing j = arg max\nj \u2032 r i,j \u2032 = arg max j \u2032 u T i v j \u2032 (4)\nwhich is an instance of the standard MIPS problem. It should be noted that we do not have control over the norm of the learned characteristic vector, i.e., v j 2 , which often has a wide range in practice [17].\nIf there are N items to recommend, solving (4) requires computing N inner products. Recommendation systems are typically deployed in on-line application over web where the number N is huge. A brute force linear scan over all items, for computing arg max, would be prohibitively expensive.", "publication_ref": ["b18", "b17", "b5", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Large-Scale Object Detection with DPM", "text": "Deformable Part Model (DPM) based representation of images is the state-of-the-art in object detection tasks [11]. In DPM model, first a set of part filters are learned from the train dataset. During detection, these learned filter activations over various patches of the test image are used to score the test image. The activation of a filter on an image patch is an inner product between them. Typically, the number of possible filters are large (e.g., millions) and so scoring the test image is costly. Very recently, it was shown that scoring based only on filters with high activations performs well in practice [10]. Identifying filters, from a large collection of possible filters, having high activations on a given image patch requires computing top inner products. Consequently, an efficient solution to the MIPS problem will benefit large scale object detections based on DPM.", "publication_ref": ["b10", "b9"], "figure_ref": [], "table_ref": []}, {"heading": "Structural SVM", "text": "Structural SVM, with cutting plane training [16], is one of the popular methods for learning over structured data. The most expensive step with cutting plane iteration is the call to the separation oracle which identifies the most violated constraint. In particular, given the current SVM estimate w, the separation oracle compute\u015d\ny i = arg max y\u2208Y \u2206(y i ,\u0177) + w T \u03a8(x i ,\u0177)(5)\nwhere \u03a8(x i ,\u0177) is the joint feature representation of data with the possible label\u0177 and \u2206(y i ,\u0177) is the loss function. Clearly, this is again an instance of the MIPS problem. This step is expensive in that the number of possible elements, i.e., the size of Y, is possibly exponential. Many heuristics were deployed to hopefully improve the computation of arg max in (5), for instance caching [16]. An efficient MIPS routine can make structural SVM faster and more scalable.", "publication_ref": ["b15", "b15"], "figure_ref": [], "table_ref": []}, {"heading": "Multi-Class Label Prediction", "text": "The models for multi-class SVM (or logistic regression) learn a weight vector w i for each of the class label i. After the weights are learned, given a new test data vector x test , predicting its class label is basically an MIPS problem:\ny test = arg max i\u2208L x T test w i (6\n)\nwhere L is the set of possible class labels. Note that the norms of the weight vectors w i 2 are not constant. The size, L , of the set of class labels differs in applications. Classifying with large number of possible class labels is common in fine grained object classification, for instance, prediction task with 100,000 classes [10] (i.e., L = 100, 000). Computing such high-dimensional vector multiplications for predicting the class label of a single instance can be expensive in, for example, user-facing applications.", "publication_ref": ["b9"], "figure_ref": [], "table_ref": []}, {"heading": "The Need for Hashing Inner Products", "text": "Recall the MIPS problem is to find x \u2208 S which maximizes the inner product between x and the given query q, i.e. max x\u2208S q T x. A brute force scan of all elements of S can be prohibitively costly in applications which deal with massive data and care about the latency (e.g., search).\nOwing to the significance of the problem, there was an attempt to efficiently solve MIPS by making use of tree data structure combined with branch and bound space partitioning technique [27,17] similar to k-d trees [12]. That method did not come with provable runtime guarantees. In fact, it is also well-known that techniques based on space partitioning (such as k-d trees) suffer from the curse of dimensionality. For example, it was shown in [30] (both empirically and theoretically) that all current techniques (based on space partitioning) degrade to linear search, even for dimensions as small as 10 or 20.\nLocality Sensitive Hashing (LSH) [15] based randomized techniques are common and successful in industrial practice for efficiently solving NNS (near neighbor search). Unlike space partitioning techniques, both the running time as well as the accuracy guarantee of LSH based NNS are in a way independent of the dimensionality of the data. This makes LSH suitable for large scale processing system dealing with ultrahigh dimensional datasets which are common these days. Furthermore, LSH based schemes are massively parallelizable, which makes them ideal for modern \"Big\" datasets. The prime focus of this paper will be on efficient hashing based algorithms for MIPS, which do not suffer from the curse of dimensionality.", "publication_ref": ["b26", "b16", "b11", "b29", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Our Contributions", "text": "We develop Asymmetric LSH (ALSH), an extended LSH scheme for efficiently solving the approximate MIPS problem. Finding hashing based algorithms for MIPS was considered hard [27,17]. In this paper, we formally show that this is indeed the case with the current framework of LSH, and there can not exist any LSH for solving MIPS. Despite this negative result, we show that it is possible to relax the current LSH framework to allow asymmetric hash functions which can efficiently solve MIPS. This generalization comes with no cost and the extended framework inherits all the theoretical guarantees of LSH.\nOur construction of asymmetric LSH is based on an interesting mathematical phenomenon that the original MIPS problem, after asymmetric transformations, reduces to the problem of approximate near neighbor search. Based on this key observation, we show an explicit construction of asymmetric hash function, leading to the first provably sublinear query time algorithm for approximate similarity search with (unnormalized) inner product as the similarity. The construction of asymmetric hash function and the new LSH framework could be of independent theoretical interest.\nExperimentally, we evaluate our algorithm for the task of recommending top-ranked items, under the collaborative filtering framework, with the proposed asymmetric hashing scheme on Netflix and Movielens datasets. Our evaluations support the theoretical results and clearly show that the proposed asymmetric hash function is superior for retrieving inner products, compared to the well known hash function based on pstable distribution for L2 norm [9] (which is also part of standard LSH package [2]). This is not surprising because L2 distances and inner products may have very different orderings.", "publication_ref": ["b26", "b16", "b8", "b1"], "figure_ref": [], "table_ref": []}, {"heading": "Background", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Locality Sensitive Hashing (LSH)", "text": "Approximate versions of the near neighbor search problem [15] were proposed to break the linear query time bottleneck. The following formulation is commonly adopted. Definition: (c-Approximate Near Neighbor or c-NN) Given a set of points in a D-dimensional space R D , and parameters S 0 > 0, \u03b4 > 0, construct a data structure which, given any query point q, does the following with probability 1 \u2212 \u03b4: if there exists an S 0 -near neighbor of q in P , it reports some cS 0 -near neighbor of q in P .\nThe usual notion of S 0 -near neighbor is in terms of distance. Since we deal with similarities, we can equivalently define S 0 -near neighbor of point q as a point p with Sim(q, p) \u2265 S 0 , where Sim is the similarity function of interest.\nThe popular technique for c-NN uses the underlying theory of Locality Sensitive Hashing (LSH) [15]. LSH is a family of functions, with the property that more similar input objects in the domain of these functions have a higher probability of colliding in the range space than less similar ones. In formal terms, consider S a family of hash functions mapping R D to some set I.", "publication_ref": ["b14", "b14"], "figure_ref": [], "table_ref": []}, {"heading": "Definition: (Locality Sensitive Hashing (LSH))", "text": "A family H is called (S 0 , cS 0 , p 1 , p 2 )-sensitive if, for any two point x, y \u2208 R D , h chosen uniformly from H satisfies the following:\n\u2022 if Sim(x, y) \u2265 S 0 then P r H (h(x) = h(y)) \u2265 p 1 \u2022 if Sim(x, y) \u2264 cS 0 then P r H (h(x) = h(y)) \u2264 p 2\nFor efficient approximate nearest neighbor search, p 1 > p 2 and c < 1 is needed.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fast Similarity Search with LSH", "text": "In a typical task of similarity search, we are given a query q, and our aim is to find x \u2208 S with high value of Sim(q, x). LSH provides a clean mechanism of creating hash tables [2]. The idea is to concatenate K independent hash functions to create a meta-hash function of the form\nB l (x) = [h 1 (x); h 2 (x); ...; h K (x)](7)\nwhere h i , i = {1, 2, ..., K} are K independent hash functions sampled from the LSH family. The LSH algorithm needs L independent meta hash functions B l (x), l = 1, 2, ..., L.\n\u2022 Pre-processing Step: During preprocessing, we assign x i \u2208 S to the bucket B l (x i ) in the hash table l, for l = 1, 2, ..., L.\n\u2022 Querying Step: Given a query q, we retrieve union of all elements from buckets B l (q), where the union is taken over all hash tables l, for l = 1, 2, ..., L.\nThe bucket B l (q) contains elements x i \u2208 S whose K different hash values collide with that of the query. By the LSH property of the hash function, these elements have higher probability of being similar to the query q compared to a random point. This probability value can be tuned by choosing appropriate value for parameters K and L. Optimal choices lead to fast query time algorithm: Fact 1: Given a family of (S 0 , cS 0 , p 1 , p 2 ) -sensitive hash functions, one can construct a data structure for c-NN with O(n \u03c1 log n) query time and space O(n 1+\u03c1 ), where \u03c1 = log p 1 log p 2 < 1.\nLSH trades off query time with extra (one time) preprocessing cost and space. Existence of an LSH family translates into provably sublinear query time algorithm for c-NN problems. It should be noted that the worst case query time for LSH is only dependent on \u03c1 and n. Thus, LSH based near neighbor search in a sense does not suffer from the curse of dimensionality. This makes LSH a widely popular technique in industrial practice [14,25,7]. [9] presented a novel LSH family for all L p (p \u2208 (0, 2]) distances. In particular, when p = 2, this scheme provides an LSH family for L 2 distances. Formally, given a fixed number r, we choose a random vector a with each component generated from i.i.d. normal, i.e., a i \u223c N (0, 1), and a scalar b generated uniformly at random from [0, r]. The hash function is defined as:", "publication_ref": ["b1", "b13", "b24", "b6", "b8"], "figure_ref": [], "table_ref": []}, {"heading": "LSH for L2 distance", "text": "h L2 a,b (x) = \u230a a T x + b r \u230b (8\n)\nwhere \u230a\u230b is the floor operation. The collision probability under this scheme can be shown to be\nP r(h L2 a,b (x) = h L2 a,b (y)) = F r (d),(9)\nF r (d) = 1 \u2212 2\u03a6(\u2212r d) \u2212 2 \u221a 2\u03c0(r d) 1 \u2212 e \u2212(r d) 2 2 (10)\nwhere\n\u03a6(x) = \u222b x \u2212\u221e 1 \u221a 2\u03c0 e \u2212 x 2\n2 dx is the cumulative density function (cdf) of standard normal distribution and d = x \u2212 y 2 is the Euclidean distance between the vectors x and y. This collision probability F r (d) is a monotonically decreasing function of the distance d and hence h L2 a,b is an LSH for L2 distances. This scheme is also the part of LSH package [2]. Here r is a parameter which can be tuned.\nAs argued previously, x \u2212 y 2 = ( x 2 2 + y 2 2 \u2212 2x T y) is not monotonic in the inner product x T y unless the given data has a constant norm. Hence, h L2 a,b is not suitable for MIPS. In Section 4, we will experimentally show that our proposed method compares favorably to h L2 a,b hash function for MIPS. Very recently [23] reported an improvement of this well-known hashing scheme when the data can be normalized (for example, when x and y both have unit L 2 norm). However, in our problem setting, since the data can not be normalized, we can not take advantage of the new results of [23], at the moment.", "publication_ref": ["b1", "b22", "b22"], "figure_ref": [], "table_ref": []}, {"heading": "Hashing for MIPS 3.1 A Negative Result", "text": "We first show that, under the current LSH framework, it is impossible to obtain a locality sensitive hashing for MIPS. In [27,17], the authors also argued that finding locality sensitive hashing for inner products could be hard, but to the best of our knowledge we have not seen a formal proof.", "publication_ref": ["b26", "b16"], "figure_ref": [], "table_ref": []}, {"heading": "Theorem 1 There can not exist any LSH family for MIPS.", "text": "Proof: Suppose, there exists such hash function h. For un-normalized inner products the self similarity of a point x with itself is Sim(x, x) = x T x = x 2 2 and there may exist another points y, such that Sim(x, y) = y T x > x 2 2 +M , for any constant M . Under any single randomized hash function h, the collision probability of the event {h(x) = h(x)} is always 1. So if h is an LSH for inner product then the event {h(x) = h(y)} should have higher probability compared to the event {h(x) = h(x)} (which already has probability 1). This is not possible because the probability can not be greater than 1 . Note that we can always choose y with Sim(x, y) = S 0 + \u03b4 > S 0 and cS 0 > Sim(x, x) \u2200S 0 and \u2200c < 1. This completes the proof. \u25fb Note that in [4] it was shown that we can not have a hash function where the collision probability is equal to the inner product, because \"1 -inner product\" does not satisfy the triangle inequality. This does not totally eliminates the existence of LSH. For instance, under L2Hash, the collision probability is a monotonic function of distance and not the distance itself.", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "Our Proposal: Asymmetric LSH (ALSH)", "text": "The basic idea of LSH is probabilistic bucketing and it is more general than the requirement of having a single hash function h. In the LSH algorithm (Section 2.2), we use the same hash function h for both the preprocessing step and the query step. We assign buckets in the hash table to all the candidates x \u2208 S using h. We use the same h on the query q to identify relevant buckets. The only requirement for the proof, of Fact 1, to work is that the collision probability of the event {h(q) = h(x)} increases with the similarity Sim(q, x). The theory [13] behind LSH still works if we use hash function h 1 for preprocessing x \u2208 S and a different hash function h 2 for querying, as long as the probability of the event {h 2 (q) = h 1 (x)} increases with Sim(q, x), and there exist p 1 and p 2 with the required property. The traditional LSH definition does not allow this asymmetry but it is not a required condition in the proof. For this reason, we can relax the definition of c-NN without loosing runtime guarantees.\nAs the first step, we define a modified locality sensitive hashing, in a slightly different form which will be useful later.\nDefinition: (Asymmetric Locality Sensitive Hashing (ALSH)) A family H, along with the two vector functions Q \u2236 R D \u21a6 R D \u2032 (Query Transformation) and P \u2236 R D \u21a6 R D \u2032 (Preprocessing Transformation), is called (S 0 , cS 0 , p 1 , p 2 )-sensitive if for a given c-NN instance with query q, and the hash function h chosen uniformly from H satisfies the following:\n\u2022 if Sim(q, x) \u2265 S 0 then P r H (h(Q(q))) = h(P (x))) \u2265 p 1 \u2022 if Sim(q, x) \u2264 cS 0 then P r H (h(Q(q)) = h(P (x))) \u2264 p 2\nHere x is any point in the collection S.\nWhen Q(x) = P (x) = x, we recover the vanilla LSH definition with h(.) as the required hash function. Coming back to the problem of MIPS, if Q and P are different, the event {h(Q(x)) = h(P (x))} will not have probability equal to 1 in general. Thus, Q \u2260 P can counter the fact that self similarity is not highest with inner products. We just need the probability of the new collision event {h(Q(q)) = h(P (y))} to satisfy the conditions of Definition of c-NN for Sim(q, y) = q T y. Note that the query transformation Q is only applied on the query and the pre-processing transformation P is applied to x \u2208 S while creating hash tables. It is this asymmetry which will allow us to solve MIPS efficiently. In Section 3.3, we explicitly show a construction (and hence the existence) of asymmetric locality sensitive hash function for solving MIPS. The source of randomization h for both q and x \u2208 S is the same. Formally, it is not difficult to show a result analogous to Fact 1.\nTheorem 2 Given a family of hash function H and the associated query and preprocessing transformations P and Q, which is (S 0 , cS 0 , p 1 , p 2 ) -sensitive, one can construct a data structure for c-NN with O(n \u03c1 log n) query time and space O(n 1+\u03c1 ), where \u03c1 = log p 1 log p 2 .\nProof: Use the standard LSH procedure (Section 2.2) with a slight modification. While preprocessing, we assign x i to bucket B l (P (x i )) in table l. While querying with query q, we retrieve elements from bucket B l (Q(q)) in the hash table l. By definition of asymmetric LSH, the probability of retrieving an element, under this modified scheme, follows the same expression as in the original LSH. The proof can be completed by exact same arguments used for proving Fact 1 (See [13] for details). \u25fb", "publication_ref": ["b12", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "From MIPS to Near Neighbor Search (NNS)", "text": "Without loss of generality, we can assume that for the problem of MIPS the query q is normalized, i.e., q 2 = 1, because in computing p = arg max x\u2208S q T x the argmax is independent of q 2 . In particular, we can choose to let U < 1 be a number such that\nx i 2 \u2264 U < 1, \u2200x i \u2208 S (11\n)\nIf this is not the case then during the one time preprocessing we can always divide all x i s by max\nx i \u2208S x i 2 U\n. Note that scaling all x i 's by the same constant does not change arg max x\u2208S q T x.\nWe are now ready to describe the key step in our algorithm. First, we define two vector transformations P \u2236 R D \u21a6 R D+m and Q \u2236 R D \u21a6 R D+m as follows:\nP (x) = [x; x 2 2 ; x 4 2 ; ....; x 2 m 2 ] (12) Q(x) = [x; 1 2; 1 2; ....; 1 2],(13)\nwhere [;] is the concatenation. P (x) appends m scalers of the form x 2 i 2 at the end of the vector x, while Q(x) simply appends m \"1/2\" to the end of the vector x.\nBy observing that\nP (x i ) 2 2 = x i 2 2 + x i 4 2 + ... + x i 2 m 2 + x i 2 m+1 2(14)\nQ(q) 2 2 = q 2 2 + m 4 = 1 + m 4 (15\n)\nQ(q) T P (x i ) = q T x i + 1 2 ( x i 2 2 + x i 4 2 + ... + x i 2 m 2 )(16)\nwe obtain the following key equality:\nQ(q) \u2212 P (x i ) 2 2 = (1 + m 4) \u2212 2q T x i + x i 2 m+1 2 (17\n)\nSince x i 2 \u2264 U < 1 x i 2 m+1\n\u2192 0, at the tower rate (exponential to exponential). The term (1 + m 4) is a fixed constant. As long as m is not too small (e.g., m \u2265 3 would suffice), we have\narg max x\u2208S q T x \u2243 arg min x\u2208S Q(q) \u2212 P (x) 2 (18\n)\nTo the best of our knowledge, this is the first connection between solving un-normalized MIPS and approximate near neighbor search. Transformations P and Q, when norms are less than 1, provide correction to the L2 distance Q(q) \u2212 P (x i ) 2 making it rank correlate with the (un-normalized) inner product. This works only after shrinking the norms, as norms greater than 1 will instead blow the term\nx i 2 m+1 2\n. This interesting mathematical phenomenon connects MIPS with approximate near neighbor search.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Fast Algorithms for MIPS", "text": "Eq. (18) shows that MIPS reduces to the standard approximate near neighbor search problem which can be efficiently solved. As the error term x i 2 m+1 2 < U 2 m+1 goes to zero at a tower rate, it quickly becomes negligible for any practical purposes. In fact, from theoretical perspective, since we are interested in guarantees for c-approximate solutions, this additional error can be absorbed in the approximation parameter c.\nFormally, we can state the following theorem.\nTheorem 3 Given a c-approximate instance of MIPS, i.e., Sim(q, x) = q T x, and a query q such that q 2 = 1 along with a collection S having x 2 \u2264 U < 1 \u2200x \u2208 S. Let P and Q be the vector transformations defined in Eq. (12) and Eq. (13), respectively. We have the following two conditions for hash function h L2 a,b (defined by Eq. ( 8))\n\u2022 if q T x \u2265 S 0 then P r[h L2 a,b (Q(q)) = h L2 a,b (P (x))] \u2265 F r 1 + m 4 \u2212 2S 0 + U 2 m+1 \u2022 if q T x \u2264 cS 0 then P r[h L2 a,b (Q(q)) = h L2 a,b (P (x))] \u2264 F r 1 + m 4 \u2212 2cS 0\nwhere the function F r is defined by Eq. (10).\nProof: From Eq. (9), we have\nP r[h L2 a,b (Q(q)) = h L2 a,b (P (x))] =F r ( Q(q) \u2212 P (x) 2 ) =F r 1 + m 4 \u2212 2q T x + x 2 m+1 2 \u2265F r 1 + m 4 \u2212 2S 0 + U 2 m+1\nThe last step follows from the monotonically decreasing nature of F combined with inequalities q T x \u2265 S 0 and x 2 \u2264 U . We have also used the monotonicity of the square root function. The second inequality similarly follows using q T x \u2264 cS 0 and x 2 \u2265 0.. This completes the proof. \u25fb\nThe conditions q 2 = 1 and x 2 \u2264 U < 1, \u2200x \u2208 S can be absorbed in the transformations Q and P respectively, but we show it explicitly for clarity.\nThus, we have obtained p 1 = F r (1 + m 4) \u2212 2S 0 + U 2 m+1 and p 2 = F r (1 + m 4) \u2212 2cS 0 . Applying Theorem 2, we can construct data structures with worst case O(n \u03c1 log n) query time guarantees for c-approximate MIPS, where\n\u03c1 = log F r 1 + m 4 \u2212 2S 0 + U 2 m+1 log F r 1 + m 4 \u2212 2cS 0(19)\nWe also need p 1 > p 2 in order for \u03c1 < 1. This requires us to have \u22122S 0 + U 2 m+1 < \u22122cS 0 , which boils down to the condition c < 1 \u2212 U 2 m+1 2S 0 . Note that U 2 m+1 2S 0 can be made arbitrarily close to zero with the appropriate value of m. For any given c < 1, there always exist U < 1 and m such that \u03c1 < 1. This way, we obtain a sublinear query time algorithm for MIPS. The guarantee holds for any values of U and m satisfying m \u2208 N + and U \u2208 (0, 1). We also have one more parameter r for the hash function h a,b . Recall the definition of F r in Eq. (10):\nF r (d) = 1 \u2212 2\u03a6(\u2212r d) \u2212 2 \u221a 2\u03c0(r d) 1 \u2212 e \u2212(r d) 2 2 .\nGiven a c-approximate MIPS instance, \u03c1 is a function of 3 parameters: U , m, r. The algorithm with the best query time chooses U , m and r, which minimizes the value of \u03c1. For convenience, we define\n\u03c1 * = min U,m,r log F r 1 + m 4 \u2212 2S 0 + U 2 m+1 log F r 1 + m 4 \u2212 2cS 0 (20) s.t. U 2 m+1 2S 0 < 1 \u2212 c, m \u2208 N + , r > 0 and 0 < U < 1.\nSee Figure 1 for the plots of \u03c1 * . With this best value of \u03c1, we can state our main result in Theorem 4.  Just like in the typical LSH framework, the value of \u03c1 * in Theorem 4 depends on the c-approximate instance we aim to solve, which requires knowing the similarity threshold S 0 and the approximation ratio c. Since, q 2 = 1 and x 2 \u2264 U < 1, \u2200x \u2208 S, we have q t x \u2264 U . A reasonable choice of the threshold S 0 is to choose a high fraction of U, for example, S 0 = 0.9U or S 0 = 0.8U .\nThe computation of \u03c1 * and the optimal values of corresponding parameters can be conducted via a grid search over the possible values of U , m and r, as we only have 3 parameters. We compute the values of \u03c1 * along with the corresponding optimal values of U , m and r for S 0 \u2208 {0.9U, 0.8U, 0.7U, 0.6U, 0.5U } for different approximation ratios c ranging from 0 to 1. The plot of the optimal \u03c1 * is shown in Figure 1, and the corresponding optimal values of U , m and r are shown in Figure 2.  ", "publication_ref": ["b17", "b11", "b9"], "figure_ref": ["fig_0", "fig_0", "fig_2"], "table_ref": []}, {"heading": "Practical Recommendation of Parameters", "text": "In practice, the actual choice of S 0 and c is dependent on the data and is often unknown. Figure 2 illustrates that m \u2208 {2, 3, 4}, U \u2208 [0.8, 0.85], and r \u2208 [1.5, 3] are reasonable choices. For convenience, we recommend to use m = 3, U = 0.83, and r = 2.5. With this choice of the parameters, Figure 3 shows that the \u03c1 values using these parameters are very close to the optimal \u03c1 * values. ", "publication_ref": [], "figure_ref": ["fig_2", "fig_3"], "table_ref": []}, {"heading": "More Insight: The Trade-off between U and m", "text": "Let \u01eb = U 2 m+1 be the error term in Eq. (17). As long as \u01eb is small the MIPS problem reduces to standard near neighbor search via the transformations P and Q. There are two ways to make \u01eb small, either we choose a small value of U or a large value of m. There is an interesting trade-off between parameters U and m. To see this, consider the following Figure 4 for F r (d), i.e., the collision probability defined in Eq. (10). \nSuppose \u01eb = U 2 m+1 is small, then p 1 = F r ( 1 + m 4 \u2212 2q T x) and p 2 = F r ( 1 + m 4 \u2212 2cq T x). Be- cause of the bounded norms, we have \u2212 q 2 x 2 = \u2212U \u2264 q T x \u2264 U = q 2 x 2 . Consider the high similarity range d \u2208 [ 1 + m 4 \u2212 2U , 1 + m 4 \u2212 2cU ],\nfor some appropriately chosen c < 1. This range is wider, if U is close to 1, if U is close to 0 then 2U \u2243 2cU making the arguments to F close to each other, which in turn decreases the gap between p 1 and p 2 . On the other hand, when m is larger, it adds bigger offset (term m 4 inside square root) to both the arguments. Adding offset, makes p 1 and p 2 shift towards right in the collision probability plot. Right shift, makes p 1 and p 2 closer because of the nature of the collision probability curve (see Figure 4). Thus, bigger m makes p 1 and p 2 closer. Smaller m pushes p 1 and p 2 towards left and thereby increasing the gap between them.\nSmall \u03c1 = log p 1 log p 2 roughly boils down to having a bigger gap between p 1 and p 2 [26]. Ideally, for small \u03c1, we would like to use big U and small m. The error \u01eb = U 2 m+1 exactly follows the opposite trend. For error to be small we want small U and not too small m.", "publication_ref": ["b16", "b25"], "figure_ref": ["fig_4", "fig_4"], "table_ref": []}, {"heading": "Parallelization", "text": "To conclude this section, we should mention that the hash table based scheme is massively parallelizable. Different nodes on cluster need to maintain their own hash tables and hash functions. The operation of retrieving from buckets and computing the maximum inner product over those retrieved candidates, given a query, is a local operation. Computing the final maximum can be conducted efficiently by simply communicating one single number per nodes. Scalability of hashing based methods is one of the reasons which account for their popularity in industrial practice.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluations", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Datasets", "text": "We evaluate our proposed hash function for the MIPS problem on two popular collaborative filtering datasets (on the task of item recommendations):\n\u2022 Movielens. We choose the largest available Movielens dataset, the Movielens 10M, which contains around 10 million movie ratings from 70,000 users over 10,000 movie titles. The ratings are between 1 to 5, with increments of 0.5 (i.e., 10 possible ratings in total).\n\u2022 Netflix. This dataset contains 100 million movie ratings from 480,000 users over 17,000 movie titles.\nThe ratings are on a scale from 1 to 5 (integer).\nEach dataset forms a sparse user-item matrix R, where the value of R(i, j) indicates the rating of user i for movie j. Given the user-item ratings matrix R, we follow the PureSVD procedure described in [6] to generate user and item latent vectors. That is, we compute the SVD of the ratings matrix\nR = W \u03a3V T\nwhere W is n users \u00d7 f matrix and V is n item \u00d7 f matrix for some appropriately chosen rank f (which is also called the latent dimension).\nAfter the SVD step, the rows of matrix U = W \u03a3 are treated as the user characteristic vectors while rows of matrix V correspond to the item characteristic vectors. More specifically u i , the i th row of matrix U , denotes the characteristic vector of user i, while the j th row of V , i.e., v j , corresponds to the characteristic vector for item j. The compatibility between item i and item j is given by the inner product between the corresponding user and item characteristic vectors. Therefore, in recommending top-ranked items to users i, the PureSVD method returns top-ranked items based on the inner products u T i v j , \u2200j. The PureSVD procedure, despite its simplicity, outperforms other popular recommendation algorithms for the task of top-ranking recommendations (see [6] for more details) on these two datasets. Following [6], we use the same choices for the latent dimension f , i.e., f = 150 for Movielens and f = 300 for Netflix.", "publication_ref": ["b5", "b5", "b5"], "figure_ref": [], "table_ref": []}, {"heading": "Baseline Hash Function", "text": "Our proposal is the first provable hashing scheme in the literature for retrieving inner products and hence there is no existing baseline. Since, our hash function uses Hashing for L2 distance after asymmetric transformation P (12) and Q (13), we would like to know if such transformations are even needed and furthermore get an estimate of the improvements obtained using these transformations. We therefore compare our proposal with L2LSH the hashing scheme h L2 a,b described by Eq. (8). It is implemented in the LSH package for near neighbor search with Euclidean distance.\nAlthough, L2LSH are not optimal for retrieving un-normalized inner products, it does provide some indexing capability. Our experiments will show that the proposed method outperform L2LSH, often significantly so, in retrieving inner products. This is not surprising as we know that the rankings of L2 distance can be different from rankings of inner products.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Evaluations", "text": "We are interested in knowing, how the two hash functions correlate with the top-T inner products. For this task, given a user i and its corresponding user vector u i , we compute the top-T gold standard items based on the actual inner products u T i v j , \u2200j. We then compute K different hash codes of the vector u i and all the item vectors v j s. For every item v j , we then compute the number of times its hash values matches (or collides) with the hash values of query which is user u i , i.e., we compute\nM atches j = K t=1 \u00bd(h t (u i ) = h t (v j )),(21)\nwhere \u00bd is the indicator function. Based on M atches j we rank all the items. This procedure generates a sorted list of all the items for a given user vector u i corresponding to every hash function under consideration. Here, we use h = h L2 a,b for L2LSH. For our proposed asymmetric hash function we have h(u i ) = h L2 a,b (Q(u i )), since u i is the query, and h(v j ) = h L2 a,b (P (v j )) for all the items. The subscript t is used to distinguish independent draws of h. Ideally, for a better hashing scheme, M atches j should be higher for items having higher inner products with the given user u i .\nWe compute the precision and recall of the top-T items for T \u2208 {1, 5, 10}, obtained from the sorted list based on M atches. To compute this precision and recall, we start at the top of the ranked item list and walk down in order. Suppose we are at the k th ranked item, we check if this item belongs to the gold standard top-T list. If it is one of the top-T gold standard item, then we increment the count of relevant seen by 1, else we move to k + 1. By k th step, we have already seen k items, so the total items seen is k. The precision and recall at that point is then computed as:\nP recision = relevant seen k , Recall = relevant seen T (22)\nWe vary a large number of k values to obtain continuously-looking precision-recall curves. Note that it is important to balance both precision and recall. Methodology which obtains higher precision at a given recall is superior. Higher precision indicates higher ranking of the relevant items. We average this value of precision and recall over 2000 randomly chosen users.\nChoice of r. We need to specify this important parameter for our proposed algorithm as well as L2LSH.\nWe have shown in Figure 3 that, for our proposed algorithm, it is overall good to choose m = 3, U = 0.83 and r = 2.5. This theoretical result largely frees us from the burden of choosing parameters. We still need to choose r for L2LSH. To ensure that L2LSH is not being suboptimal because of the choice of r, we report the results of L2LSH for all r \u2208 {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}, and we show that our proposed method (with r = 2.5) very significantly outperforms L2LSH at all choices of r, in Figure 5 and Figure 6, for K = 64, 128, 256, 512 hashes. The good performance of our algorithm shown in Figure 5 and Figure 6 is not surprising because we know from Theorem 3 that the collision under the new hash function is a direct indicator of high inner product. As the number of hash codes is increased, the performance of the proposed methodology shows bigger improvements over L2LSH. The suboptimal performance of L2LSH clearly indicates that the norms of the item characteristic vectors do play a significant role in item recommendation task. Also, the experiments clearly establishes the need of proposed asymmetric transformation P and Q.  We vary the number of hashes K from 64 to 512. The proposed algorithm (solid, red if color is available) significantly outperforms L2LSH. We fix the parameters m = 3, U = 0.83, and r = 2.5 for our proposed method and we present the results of L2LSH for all r values in {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}. Because the difference between our method and L2LSH is large, we do not label curves at different r values for L2LSH.  We vary the number of hashes K from 64 to 512. The proposed algorithm (solid, red if color is available) significantly outperforms L2LSH. We fix the parameters m = 3, U = 0.83, and r = 2.5 for our proposed method and we present the results of L2LSH for all r values in {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}. Because the difference between our method and L2LSH is large, we do not label curves at different r values for L2LSH.\nTo close this section, we present Figure 7 to visualize the impact of the parameter r on the performance of our proposed method. Our theory and Figure 3 have already shown that we can achieve close to optimal performance by choosing m = 3, U = 0.83, and r = 2.5. Nevertheless, it is still interesting to see how the theory is confirmed by experiments. Figure 7 presents the precision-recall curves for our proposed method with m = 3, U = 0.83, and r \u2208 {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}. For clarify, we only differentiate r = 1, 2.5, 5 from the rest of the curves. The results demonstrate that r = 2.5 is indeed a good choice and the performance is not too sensitive to r unless it is much away from 2.5. Figure 7: Impact of r on the performance of the proposed method. We present the precision-recall curves for m = 3, U = 0.83, and r \u2208 {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}, which demonstrate that, (i) r = 2.5 is indeed a good choice; and (ii) the performance is not too sensitive to r unless it is much away from 2.5.", "publication_ref": [], "figure_ref": ["fig_3", "fig_6", "fig_8", "fig_6", "fig_8", "fig_3"], "table_ref": []}, {"heading": "Conclusion and Future Work", "text": "MIPS (maximum inner product search) naturally arises in numerous practical scenarios, e.g., collaborative filtering. Given a query data vector, the task of MIPS is to find data vectors from the repository which are most similar to the query in terms of (un-normalized) inner product (instead of distance). This problem is challenging and, prior to our work, there existed no provably sublinear time algorithms for MIPS. The current framework of LSH (locality sensitive hashing) is not sufficient for solving MIPS.\nIn this study, we develop ALSH (asymmetric LSH), which generalizes the existing LSH framework by applying (appropriately chosen) asymmetric transformations to the input query vector and the data vectors in the repository. We present an implementation of ALSH by proposing a novel transformation which converts the original inner products into L2 distances in the transformed space. We demonstrate, both theoretically and empirically, that this implementation of ALSH provides a provably efficient solution to MIPS.\nWe believe our work will lead to several interesting (and practically useful) lines of future work:\n\u2022 Three-way (and higher-order) maximum inner product search: In this paper, we have focused on pairwise similarity search. Three-way (or even higher-order) search can also be important in practice and might attract more attentions in the near future due to the recent pilot studies on efficient three-way similarity computation [20,22,28]. Extending ALSH to three-way MIPS could be very useful.\n\u2022 Other efficient similarities: Finding other similarities for which there exist fast retrieval algorithms is an important problem [5]. Exploring other similarity functions, which can be efficient solved using asymmetric hashing is an interesting future area. One good example is to find special ALSH schemes for binary data by exploring prior powerful hashing methods for binary data such as (b-bit) minwise hashing and one permutation hashing [3,24].\n\u2022 Fast hashing for MIPS: Our proposed hash function uses random projection as the main hashing scheme. There is a rich set of literature [21,1,8,24,29] on making hashing faster. Evaluations of these new faster techniques will further improve the runtime guarantees in this paper.\n\u2022 Applications: We have evaluated the efficiency of our scheme in the collaborative filtering application. An interesting set of future work consist of applying this hashing scheme for other applications mentioned in Section 1. In particular, it will be exciting to apply efficient MIPS subroutines to improve the DPM based object detection [10] and structural SVMs [16].", "publication_ref": ["b19", "b21", "b27", "b4", "b2", "b23", "b20", "b0", "b7", "b23", "b28", "b9", "b15"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Approximate nearest neighbors and the fast Johnson-Lindenstrauss transform", "journal": "", "year": "2006", "authors": "N Ailon; B Chazelle"}, {"ref_id": "b1", "title": "E2lsh: Exact euclidean locality sensitive hashing", "journal": "", "year": "2004", "authors": "A Andoni; P Indyk"}, {"ref_id": "b2", "title": "On the resemblance and containment of documents", "journal": "", "year": "1997", "authors": "A Z Broder"}, {"ref_id": "b3", "title": "Similarity estimation techniques from rounding algorithms", "journal": "", "year": "2002", "authors": "M S Charikar"}, {"ref_id": "b4", "title": "Lsh-preserving functions and their applications", "journal": "", "year": "2012", "authors": "F Chierichetti; R Kumar"}, {"ref_id": "b5", "title": "Performance of recommender algorithms on top-n recommendation tasks", "journal": "ACM", "year": "2010", "authors": "P Cremonesi; Y Koren; R Turrin"}, {"ref_id": "b6", "title": "Google news personalization: scalable online collaborative filtering", "journal": "ACM", "year": "2007", "authors": "A S Das; M Datar; A Garg; S Rajaram"}, {"ref_id": "b7", "title": "Fast locality-sensitive hashing", "journal": "", "year": "2011", "authors": "A Dasgupta; R Kumar; T Sarl\u00f3s"}, {"ref_id": "b8", "title": "Locality-sensitive hashing scheme based on p-stable distributions", "journal": "", "year": "2004", "authors": "M Datar; N Immorlica; P Indyk; V S Mirrokn"}, {"ref_id": "b9", "title": "Fast, accurate detection of 100,000 object classes on a single machine", "journal": "IEEE", "year": "2013", "authors": "T Dean; M A Ruzon; M Segal; J Shlens; S Vijayanarasimhan; J Yagnik"}, {"ref_id": "b10", "title": "Object detection with discriminatively trained part-based models. Pattern Analysis and Machine Intelligence", "journal": "IEEE Transactions on", "year": "2010", "authors": "P F Felzenszwalb; R B Girshick; D Mcallester; D Ramanan"}, {"ref_id": "b11", "title": "A projection pursuit algorithm for exploratory data analysis", "journal": "IEEE Transactions on Computers", "year": "1974", "authors": "J H Friedman; J W Tukey"}, {"ref_id": "b12", "title": "Approximate nearest neighbor: Towards removing the curse of dimensionality", "journal": "Theory of Computing", "year": "2012", "authors": "S Har-Peled; P Indyk; R Motwani"}, {"ref_id": "b13", "title": "Finding near-duplicate web pages: a large-scale evaluation of algorithms", "journal": "", "year": "2006", "authors": "M R Henzinger"}, {"ref_id": "b14", "title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "journal": "", "year": "1998", "authors": "P Indyk; R Motwani"}, {"ref_id": "b15", "title": "Cutting-plane training of structural svms", "journal": "", "year": "2009", "authors": "T Joachims; T Finley; C.-N J Yu"}, {"ref_id": "b16", "title": "Efficient retrieval of recommendations in a matrix factorization framework", "journal": "", "year": "2012", "authors": "N Koenigstein; P Ram; Y Shavitt"}, {"ref_id": "b17", "title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "journal": "ACM", "year": "2008", "authors": "Y Koren"}, {"ref_id": "b18", "title": "Matrix factorization techniques for recommender systems", "journal": "", "year": "", "authors": "Y Koren; R Bell; C Volinsky"}, {"ref_id": "b19", "title": "A sketch algorithm for estimating two-way and multi-way associations", "journal": "", "year": "2007", "authors": "P Li; K W Church"}, {"ref_id": "b20", "title": "Very sparse random projections", "journal": "", "year": "2006", "authors": "P Li; T J Hastie; K W Church"}, {"ref_id": "b21", "title": "b-bit minwise hashing for estimating three-way similarities", "journal": "", "year": "2010", "authors": "P Li; A C K\u00f6nig; W Gui"}, {"ref_id": "b22", "title": "Coding for random projections and approximate near neighbor search", "journal": "", "year": "2014", "authors": "P Li; M Mitzenmacher; A Shrivastava"}, {"ref_id": "b23", "title": "One permutation hashing", "journal": "", "year": "2012", "authors": "P Li; A B Owen; C.-H Zhang"}, {"ref_id": "b24", "title": "Detecting Near-Duplicates for Web-Crawling", "journal": "", "year": "2007", "authors": "G S Manku; A Jain; A D Sarma"}, {"ref_id": "b25", "title": "Mining of Massive Datasets", "journal": "", "year": "", "authors": "A Rajaraman; J Ullman"}, {"ref_id": "b26", "title": "Maximum inner-product search using cone trees", "journal": "", "year": "2012", "authors": "P Ram; A G Gray"}, {"ref_id": "b27", "title": "Beyond pairwise: Provably fast algorithms for approximate k-way similarity search", "journal": "", "year": "2013", "authors": "A Shrivastava; P Li"}, {"ref_id": "b28", "title": "Densifying one permutation hashing via rotation for fast near neighbor search", "journal": "", "year": "2014", "authors": "A Shrivastava; P Li"}, {"ref_id": "b29", "title": "A quantitative analysis and performance study for similaritysearch methods in high-dimensional spaces", "journal": "", "year": "1998", "authors": "R Weber; H.-J Schek; S Blott"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Optimal values of \u03c1 * with respect to approximation ratio c for different S 0 . The optimization of Eq. (20) was done by a grid search over parameters r, U and m, given S 0 and c. See Figure 2 for the corresponding optimal values of parameters.", "figure_data": ""}, {"figure_label": "2", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Optimal values of parameters m, U and r with respect to the approximation ratio c for relatively high similarity threshold S 0 . The corresponding optimal values of \u03c1 * are shown in Figure 1.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 3 :3Figure 3: \u03c1 values (dashed curves) for m = 3, U = 0.83 and r = 2.5. The solid curves are the optimal \u03c1 * values as shown in Figure 1.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Plot of the monotonically decreasing function F r (d) (Eq. (10)). The curve is steeper when d is close to 1, and flatter when d goes away from 1. This leads to a tradeoff between the choices of U and m.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: Movielens. Precision-Recall curves (higher is better), of retrieving top-T items, for T = 1, 5, 10.We vary the number of hashes K from 64 to 512. The proposed algorithm (solid, red if color is available) significantly outperforms L2LSH. We fix the parameters m = 3, U = 0.83, and r = 2.5 for our proposed method and we present the results of L2LSH for all r values in {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}. Because the difference between our method and L2LSH is large, we do not label curves at different r values for L2LSH.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 6 :6Figure 6: Netflix. Precision-Recall curves (higher is better), of retrieving top-T items, for T = 1, 5, 10.We vary the number of hashes K from 64 to 512. The proposed algorithm (solid, red if color is available) significantly outperforms L2LSH. We fix the parameters m = 3, U = 0.83, and r = 2.5 for our proposed method and we present the results of L2LSH for all r values in {1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}. Because the difference between our method and L2LSH is large, we do not label curves at different r values for L2LSH.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "For the problem of c-approximate MIPS, one can construct a data structure having O(n \u03c1 * log n) query time and space O(n 1+\u03c1", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "x\u2208S q T x (1)", "formula_coordinates": [2.0, 302.4, 159.99, 237.51, 19.32]}, {"formula_id": "formula_1", "formula_text": "x\u2208S q \u2212 x 2 2 = arg min x\u2208S ( x 2 2 \u2212 2q T x)(2)", "formula_coordinates": [2.0, 240.96, 204.77, 298.95, 34.23]}, {"formula_id": "formula_2", "formula_text": "r i,j = \u00b5 + b i + b j + u T i v j (3", "formula_coordinates": [2.0, 253.68, 493.06, 282.02, 15.42]}, {"formula_id": "formula_3", "formula_text": ")", "formula_coordinates": [2.0, 535.7, 496.67, 4.21, 9.82]}, {"formula_id": "formula_4", "formula_text": "j \u2032 r i,j \u2032 = arg max j \u2032 u T i v j \u2032 (4)", "formula_coordinates": [2.0, 266.88, 629.86, 273.03, 19.98]}, {"formula_id": "formula_5", "formula_text": "y i = arg max y\u2208Y \u2206(y i ,\u0177) + w T \u03a8(x i ,\u0177)(5)", "formula_coordinates": [3.0, 222.96, 301.42, 316.95, 20.1]}, {"formula_id": "formula_6", "formula_text": "y test = arg max i\u2208L x T test w i (6", "formula_coordinates": [3.0, 250.92, 474.1, 284.78, 20.1]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 535.7, 477.71, 4.21, 9.82]}, {"formula_id": "formula_8", "formula_text": "\u2022 if Sim(x, y) \u2265 S 0 then P r H (h(x) = h(y)) \u2265 p 1 \u2022 if Sim(x, y) \u2264 cS 0 then P r H (h(x) = h(y)) \u2264 p 2", "formula_coordinates": [5.0, 90.0, 130.13, 223.95, 56.67]}, {"formula_id": "formula_9", "formula_text": "B l (x) = [h 1 (x); h 2 (x); ...; h K (x)](7)", "formula_coordinates": [5.0, 228.72, 277.61, 311.19, 34.23]}, {"formula_id": "formula_10", "formula_text": "h L2 a,b (x) = \u230a a T x + b r \u230b (8", "formula_coordinates": [5.0, 260.28, 694.42, 275.42, 35.75]}, {"formula_id": "formula_11", "formula_text": ")", "formula_coordinates": [5.0, 535.7, 705.35, 4.21, 9.82]}, {"formula_id": "formula_12", "formula_text": "P r(h L2 a,b (x) = h L2 a,b (y)) = F r (d),(9)", "formula_coordinates": [6.0, 190.92, 91.49, 348.99, 34.23]}, {"formula_id": "formula_13", "formula_text": "F r (d) = 1 \u2212 2\u03a6(\u2212r d) \u2212 2 \u221a 2\u03c0(r d) 1 \u2212 e \u2212(r d) 2 2 (10)", "formula_coordinates": [6.0, 190.92, 114.05, 348.99, 34.35]}, {"formula_id": "formula_14", "formula_text": "\u03a6(x) = \u222b x \u2212\u221e 1 \u221a 2\u03c0 e \u2212 x 2", "formula_coordinates": [6.0, 101.4, 154.55, 93.65, 25.23]}, {"formula_id": "formula_15", "formula_text": "\u2022 if Sim(q, x) \u2265 S 0 then P r H (h(Q(q))) = h(P (x))) \u2265 p 1 \u2022 if Sim(q, x) \u2264 cS 0 then P r H (h(Q(q)) = h(P (x))) \u2264 p 2", "formula_coordinates": [7.0, 90.0, 224.33, 260.19, 56.43]}, {"formula_id": "formula_16", "formula_text": "x i 2 \u2264 U < 1, \u2200x i \u2208 S (11", "formula_coordinates": [7.0, 258.24, 662.69, 277.17, 34.23]}, {"formula_id": "formula_17", "formula_text": ")", "formula_coordinates": [7.0, 535.4, 672.11, 4.51, 9.82]}, {"formula_id": "formula_18", "formula_text": "x i \u2208S x i 2 U", "formula_coordinates": [7.0, 497.52, 695.19, 38.09, 20.38]}, {"formula_id": "formula_19", "formula_text": "P (x) = [x; x 2 2 ; x 4 2 ; ....; x 2 m 2 ] (12) Q(x) = [x; 1 2; 1 2; ....; 1 2],(13)", "formula_coordinates": [8.0, 230.88, 103.01, 309.03, 50.79]}, {"formula_id": "formula_20", "formula_text": "P (x i ) 2 2 = x i 2 2 + x i 4 2 + ... + x i 2 m 2 + x i 2 m+1 2(14)", "formula_coordinates": [8.0, 190.8, 209.21, 349.11, 34.23]}, {"formula_id": "formula_21", "formula_text": "Q(q) 2 2 = q 2 2 + m 4 = 1 + m 4 (15", "formula_coordinates": [8.0, 190.8, 227.09, 344.6, 34.23]}, {"formula_id": "formula_22", "formula_text": ")", "formula_coordinates": [8.0, 535.4, 236.51, 4.51, 9.82]}, {"formula_id": "formula_23", "formula_text": "Q(q) T P (x i ) = q T x i + 1 2 ( x i 2 2 + x i 4 2 + ... + x i 2 m 2 )(16)", "formula_coordinates": [8.0, 186.0, 249.53, 353.91, 34.23]}, {"formula_id": "formula_24", "formula_text": "Q(q) \u2212 P (x i ) 2 2 = (1 + m 4) \u2212 2q T x i + x i 2 m+1 2 (17", "formula_coordinates": [8.0, 200.28, 298.61, 335.12, 34.23]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [8.0, 535.4, 308.03, 4.51, 9.82]}, {"formula_id": "formula_26", "formula_text": "Since x i 2 \u2264 U < 1 x i 2 m+1", "formula_coordinates": [8.0, 72.0, 326.86, 240.77, 29.69]}, {"formula_id": "formula_27", "formula_text": "arg max x\u2208S q T x \u2243 arg min x\u2208S Q(q) \u2212 P (x) 2 (18", "formula_coordinates": [8.0, 217.68, 391.37, 317.72, 34.23]}, {"formula_id": "formula_28", "formula_text": ")", "formula_coordinates": [8.0, 535.41, 400.79, 4.51, 9.82]}, {"formula_id": "formula_29", "formula_text": "x i 2 m+1 2", "formula_coordinates": [8.0, 479.88, 464.63, 32.81, 15.68]}, {"formula_id": "formula_30", "formula_text": "\u2022 if q T x \u2265 S 0 then P r[h L2 a,b (Q(q)) = h L2 a,b (P (x))] \u2265 F r 1 + m 4 \u2212 2S 0 + U 2 m+1 \u2022 if q T x \u2264 cS 0 then P r[h L2 a,b (Q(q)) = h L2 a,b (P (x))] \u2264 F r 1 + m 4 \u2212 2cS 0", "formula_coordinates": [8.0, 90.0, 678.82, 353.69, 54.83]}, {"formula_id": "formula_31", "formula_text": "P r[h L2 a,b (Q(q)) = h L2 a,b (P (x))] =F r ( Q(q) \u2212 P (x) 2 ) =F r 1 + m 4 \u2212 2q T x + x 2 m+1 2 \u2265F r 1 + m 4 \u2212 2S 0 + U 2 m+1", "formula_coordinates": [9.0, 228.36, 163.01, 148.97, 96.87]}, {"formula_id": "formula_32", "formula_text": "\u03c1 = log F r 1 + m 4 \u2212 2S 0 + U 2 m+1 log F r 1 + m 4 \u2212 2cS 0(19)", "formula_coordinates": [9.0, 221.28, 395.69, 318.63, 54.99]}, {"formula_id": "formula_33", "formula_text": "F r (d) = 1 \u2212 2\u03a6(\u2212r d) \u2212 2 \u221a 2\u03c0(r d) 1 \u2212 e \u2212(r d) 2 2 .", "formula_coordinates": [9.0, 114.84, 519.05, 222.45, 34.23]}, {"formula_id": "formula_34", "formula_text": "\u03c1 * = min U,m,r log F r 1 + m 4 \u2212 2S 0 + U 2 m+1 log F r 1 + m 4 \u2212 2cS 0 (20) s.t. U 2 m+1 2S 0 < 1 \u2212 c, m \u2208 N + , r > 0 and 0 < U < 1.", "formula_coordinates": [9.0, 194.4, 585.17, 345.51, 78.99]}, {"formula_id": "formula_35", "formula_text": "Suppose \u01eb = U 2 m+1 is small, then p 1 = F r ( 1 + m 4 \u2212 2q T x) and p 2 = F r ( 1 + m 4 \u2212 2cq T x). Be- cause of the bounded norms, we have \u2212 q 2 x 2 = \u2212U \u2264 q T x \u2264 U = q 2 x 2 . Consider the high similarity range d \u2208 [ 1 + m 4 \u2212 2U , 1 + m 4 \u2212 2cU ],", "formula_coordinates": [11.0, 72.0, 570.53, 468.03, 75.87]}, {"formula_id": "formula_36", "formula_text": "R = W \u03a3V T", "formula_coordinates": [12.0, 278.64, 463.18, 53.05, 13.94]}, {"formula_id": "formula_37", "formula_text": "M atches j = K t=1 \u00bd(h t (u i ) = h t (v j )),(21)", "formula_coordinates": [13.0, 225.48, 271.83, 314.43, 34.38]}, {"formula_id": "formula_38", "formula_text": "P recision = relevant seen k , Recall = relevant seen T (22)", "formula_coordinates": [13.0, 183.0, 485.39, 356.91, 24.81]}], "doi": ""}