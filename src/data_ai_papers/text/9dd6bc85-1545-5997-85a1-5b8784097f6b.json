{"title": "Reinforcement Learning-based Dialogue Guided Event Extraction to Exploit Argument Relations", "authors": "Qian Li; Hao Peng; Jianxin Li; Jia Wu; Yuanxing Ning; Lihong Wang; Philip S Yu; Zheng Wang", "pub_date": "", "abstract": "Event extraction is a fundamental task for natural language processing. Finding the roles of event arguments like event participants is essential for event extraction. However, doing so for real-life event descriptions is challenging because an argument's role often varies in different contexts. While the relationship and interactions between multiple arguments are useful for settling the argument roles, such information is largely ignored by existing approaches. This paper presents a better approach for event extraction by explicitly utilizing the relationships of event arguments. We achieve this through a carefully designed task-oriented dialogue system. To model the argument relation, we employ reinforcement learning and incremental learning to extract multiple arguments via a multiturned, iterative process. Our approach leverages knowledge of the already extracted arguments of the same sentence to determine the role of arguments that would be difficult to decide individually. It then uses the newly obtained information to improve the decisions of previously extracted arguments. This twoway feedback process allows us to exploit the argument relations to effectively settle argument roles, leading to better sentence understanding and event extraction. Experimental results show that our approach consistently outperforms seven state-of-theart event extraction methods for the classification of events and argument role and argument identification.", "sections": [{"heading": "Takedown", "text": "If you consider content in White Rose Research Online to be in breach of UK law, please notify us by emailing eprints@whiterose.ac.uk including the URL of the record and the reason for the withdrawal request.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "I. INTRODUCTION", "text": "E VENT extraction aims to detect, from the text, the occurrence of events of specific types and to extract arguments (e.g., typed event participants or other attributes) that are associated with an event [1]. It is a fundamental technique underpinning many Natural Language Processing Jia Wu is with the Department of Computing, Macquarie University, Sydney, Australia. E-mail: jia.wu@mq.edu.au.\nLihong Wang is with the National Computer Network Emergency Response Technical Team/Coordination Center of China, Beijing 100029, China. Email: wlh@isc.org.cn.\nPhilip S. Yu is with the Department of Computer Science, University of Illinois at Chicago, Chicago 60607, USA. E-mail: psyu@uic.edu.\nZheng Wang is with the School of Computing, University of Leeds, Leeds LS2 9JT, UK. E-mail: z.wang5@leeds.ac.uk. (NLP) tasks like knowledge reasoning [2], text summarization [3], and event prediction [4].\nEvent extraction requires extracting all arguments and their roles corresponding to each event. Doing so is challenging because an event is often associated with more than one argument, whose role can vary in different contexts. For example, the argument \"troops\" has different roles in multiple sentences, as shown in Fig. 1. This argument has the role of being the \"target\" in sentence S1, while in sentence S2, its role is the \"attacker\". In sentence S3, the argument \"troops\" could be either the \"target\" or the \"attacker\". To extract an event, we need to identify the argument role correctly. Failing to do so can lead to erroneous information propagation, affecting the recognition of other event arguments and sentence understanding. For example, incorrectly associating the \"troops\" argument in the sentence S2 as the target will lead to misunderstanding of the sentence. Unfortunately, argument role detection remains an open problem because an argument can be associated with multiple roles.\nOur work aims to find new ways to identify event argument roles for event extraction. Our key insight is that multiple arguments associated with an event are typically strongly correlated. Their correlations can provide useful information for determining the role of an event argument. Consider again our example given earlier in Fig. 1. To determine the role of argument \"troops\" in the sentence S1, we can consider its relevant arguments of \"weapons\" and \"use\". The roles and appearance order of \"weapons\" and \"use\" suggest \"troops\" is the target in the context. Although the argument \"use\" has many subtle roles, we can still attribute it with the \"Conflict: Attack\" event type in sentence S1 by using \"weapons\" as a hint. Moreover, the role of argument \"troops\" can be recognized to role by looking at the argument \"weapons\" and \"use\". If the \"troops\" is detected firstly, it may misidentify its argument role and other arguments. As can be seen from this representative example, the relation among arguments can help in inferring the argument roles that are essential for event extraction. However, prior work largely ignores such relations, leaving The killing instrument is explosives. Who is the killing victim of the Life:Die event using explosives?\nThe ... soldiers.", "publication_ref": ["b0", "b1", "b2", "b3"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "What agent killing the", "text": "What is the detonated target of Conflict: Attack event using explosives and car?\nWho is the detonated attacker of the Attack event using explosives and car?\nThe attacker ... man. Fig. 2: An example of our dialogue guided event extraction. The input sentence is: \"As the soldiers approached, the man detonated explosives in the car killing all four of the soldiers\". It needs 10 turns to complete event extraction on the sentence.\nmuch room for improvement. Most existing methods extract all arguments simultaneously [5,6] or individual arguments sequentially [7], all of which do not consider the effect of argument extraction order. Our work seeks to close this gap by explicitly modeling the argument relation for event extraction.\nOur approach is enabled by the recent advance in taskoriented dialogue systems [8,9] that are shown to be highly effective in entity-relation extraction [10]. A task-oriented dialogue system uses domain knowledge, e.g., knowledge structures of intentions extracted from sentences, to complete a specific task. The natural language understanding task is structured as many slots are to be filled, where each slot can take a set of possible values. For example, a travel query could be translated into a structure consisting of slots like original city, destination city, departure time and arrival time. The goal of the dialogue system in this context is thus to extract the right values from the user sentence to fill the slots. The recent progress in a task-oriented dialogue system allows one to effectively exploit the dialogue historical information to optimize slot filling with the right order [11][12][13].\nIn this work, we formulate the event extraction task within a task-oriented dialogue system, as shown in Fig. 2. We consider the problem of event extraction as filling slots of relevant arguments and their roles extracted from the input sentence. To this end, we develop a multi-turn dialogue system [9] with two agents to iteratively solve the slot filling problem. During each turn, one agent selects an argument role and generates a query through a dialogue generator. For example, the query for the role of \"instrument\" could be \"What is the killing instrument of event Life: Die?\" in Turn 3 of Fig. 2. The other agent answers the query by identifying the right argument or event type from the sentence. This iterative generation and answering paradigm enable us to introduce knowledge obtained from previous turns when extracting a current argument. For instance, it can exploit the argument relation like \"weapon\", \"use\" and \"troops\" in sentence S1 in Fig. 1 to improve the quality of event extraction. Our dialogue-based method extracts the argument of victim \"soldiers\" according to the argument of trigger killing and instrument explosives for event Life: Die in Fig. 2. This multi-turn process also enables us to leverage additional information about the newly extracted argument to update and correct the argument roles identified in the previous turn. As we repeat the process, we will obtain more information about event arguments and better understand the sentence over time. This richer information helps us extract argument roles more precisely towards the end of the process.\nWhile our multi-turn dialogue system provides a potentially powerful event extraction capability, its potential can only be fully unlocked if the arguments are processed in the right order. Since we extract arguments and determine their roles in sequential order by utilizing the knowledge obtained from previously extracted arguments, the order of argument extraction is crucial. Ideally, we would like to start from event arguments whose argument roles are likely to be accurately decided using already extracted information and leave the more challenging ones later once we have obtained sufficient information from others. For instance, we may wish to extract argument \"weapon\" before \"use\" of sentence S1 given in Fig. 1 because determining the former's role is more straightforward than doing that for the latter.\nWe address the challenge of argument extraction order by employing Reinforcement Learning (RL) to rank arguments to best utilize the argument relation. To allow RL to navigate the potentially large problem space, we need to find the right representation of each word in the target sentence and use the representation to predict the start and end position of each argument. To that end, we use both a lexiconbased graph attention network [14] and an event-based BERT model [15] for learning the word representation from semantics and context two perspectives. We then utilize the learned representation to determine which argument to extract and in what order. We go further by designing an incremental learning strategy to iteratively incorporate the argument relation into the multi-turned event extraction process by continually updating the event representation across turns. By doing so, the representation becomes increasingly more accurate as the argument extraction process proceeds, which, in turn, enhances the quality of the resulting argument and event extraction.\nWe evaluate our approach 1 by applying it to sentence-level event extraction performed on the ACE 2005 dataset [16]. We compare our approach to 7 recently proposed event extraction approaches [5, 6, 17-19, 7, 20]. Experimental results show that our approach can effectively utilize the argument relation to identify the argument roles, leading to better event extraction performance. We show that our incremental event learning strategy is particularly useful when the amount of labeled data is limited. This paper makes the following contributions. It is the first to:\n\u2022 develop a multi-turned, task-oriented dialogue guided event extraction framework aiming to fill arguments extracted from input text for specific arguments roles (Section III); \u2022 employ reinforcement learning to rank argument extraction order to utilize argument correlation for event extraction (Section III-B); \u2022 leverage a lexicon-based graph attention network and event-based BERT, under an incremental learning framework to learn word representation for even extraction (Sections III-A); II. RELATED WORK Event extraction [21][22][23] is a form of information representation to extract what users are interested in massive data and present it to users in a certain way. For event extraction task, it can be divided into four subtasks: event classification, trigger identification, argument identification, and argument role classification. Most recent event extraction works are based on a neural network architecture like Convolutional Neural Network (CNN) [24,25], Recurrent Neural Network (RNN) [26,5], Graph Neural Network (GNN) [6,27], Transformer [19,28], or other networks [29,30]. The method of event extraction based on deep learning first adopts pipeline. The pipeline-based method [24,19,31] is the earliest event extraction based on neural networks and extracts event arguments by utilizing triggers. It realizes event trigger identification, event classification, event argument identification, and argument role classification tasks successively [32] and takes the results of previous tasks as prior knowledge. The first two tasks are usually called event detection and the last two tasks are called argument extraction. Chen et al. [24] and Nguyen et al. [26,33] use the CNN model to capture sentence-level clues and overcome complex feature engineering compared with traditional feature-based approaches. DBRNN [5] have been proved to be influential in introducing graph information into event extraction tasks. JMEE [6] is proposed with an attention-based GCN, learning syntactic contextual node representations through first-order neighbors of the graph. As we all know, an argument usually plays different roles in different events, enhancing the difficulty of the event extraction task. Yang et al. [19] propose a pre-trained language model-based event extractor [34] to learn contextualized representations proven helpful for event extraction. It separates argument predictions according to the roles and overcomes the argument overlap problem. However, this requires a high accuracy of trigger identification. A wrong trigger will seriously affect the accuracy of argument identification and argument role classification. Therefore, the pipeline based method considers the event trigger as the core of an event [5,35,36] and requires high accuracy of event trigger identification, avoiding an adverse effect on event argument extraction. In order to overcome the propagation of error information caused by event detection, joint-based event extraction methods are proposed. The joint event extraction method avoids the influence of trigger identification error on event argument extraction. It reduces the propagation of error information by combining trigger identification and argument extraction tasks.\nThe existing event extraction corpus has a few labeled data and hard to expand, such as ACE 2005 with only 599 annotated documents [16]. Existing deep learning-based approaches usually require lots of manually annotated training data. Consequently, except for the difficulty of event extraction itself, inadequate training data also hinders. The zero-shot learning method is the right choice, which has been widely applied in NLP tasks [37]. Based on this, Huang et al. [38] design a transferable neural architecture and stipulate a graph structure to transfer knowledge from the existing types to the extraction of unseen types. It finds the event types graph structure, which learns representation almost matching representations from the parsed AMR structure [39]. Existing event extraction systems, which usually adopt a supervised learning paradigm, have to rely on labelled training data, but the scarcity of high-quality training data is a common problem [40,31].\nMachine Reading Comprehension (MRC) tasks extract a span from text [41], is a basic task of question answering. MRC based event extraction [28,7] designs questions for each argument, helps capture semantic relationship between question and input sentence. Question answering methods are emerging as a new way for extracting important keywords from a sentence [28,7]. By incorporating domain-knowledge to the question set, one can guide the extraction framework to focus on essential semantics to be extracted from a sentence. Existing approaches do not utilize the relations among multiple arguments, leaving much room for improvement. Our work aims to close this gap by using the argument relations to infer roles of arguments that are hard to settle in isolation, leading to better performance for argument and event classification.\nTask-oriented dialogue systems aim to assist the user to complete specific tasks according to existing corpora [9,42]. The typical task-oriented dialogue system has four components: natural language understanding, dialogue state tracker, dialogue policy learning, and natural language generation [43]. Ramadan et al. [44] introduce an approach utilizing semantic correlation in ontology terminologies and dialogue utterances. It easily utilizes history knowledge for current dialogue [8,9], aiming to build the connection among dialogue. Therefore, there have been some explorations on formulating NLP tasks as a dialogue task. It can overcome the inadequacy problem of historical information utilization in MRC. Therefore, there have been some explorations on formulating NLP tasks as a dialogue task. We treat event extraction as a dialogue for capturing relationships among arguments. Our work leverages the recent development in task-oriented dialogue systems [9,42]. Such a system decouples the problem to be solved through a multi-turn dialogue, allowing the system to connect and exploit information obtained during multiple conversations to solve a new task [8,9]. III. EVENT EXTRACTION FRAMEWORK Fig. 3 gives an overview of our framework that consists of three components for (1) double perspective event representation, (2) ranked argument extraction, and (3) event classification. The argument extraction module automatically generates the dialogue based on the event type and the selected predicted arguments. The selected arguments are produced by incremental event learning method to add the pseudo label as the training data and append the pseudo relation to the lexiconbased graph. The pseudo label is the arguments predicted by our ranked argument extraction model. The pseudo relation is the argument role of the predicted argument.\nWe add a pseudo edge by appending an edge among predicted arguments of an event type to update word representation using existing prediction results. It takes the sentence and event type as input. Event classification module detects whether the input sentence is an event and classifies the event type to which the text belongs. We design a multitask learning module to calculate the combined loss of the two tasks to overcome the event type imbalance results in a low recall. For different event type, different event schema are designed for extracting different arguments according to the schema. Our framework is first trained offline using a small amount of labeled data. To expand the training data, we design a dialogue generation module, generating multiple question-answering pairs for each trigger or argument for data enhance. The trained models can then be applied to extract event types and associated event arguments.\nDuring the training phase, the reinforcement learning-based, dialogue-guided argument extraction model learns how to extract event arguments by taking as input the target sentence and a label of the event type. Our framework will first learn several rounds of conversational argument extraction according to event types and sentences, and train the event classification model according to event arguments. In each turn, the predicted argument is provided as a pseudo relation in the lexicon-based graph and a pseudo label in role embeddings of event-based BERT used by the incremental event learning method. It updates the textual representation by adding the pseudo argument knowledge. The event classification model is then trained to predict the event type using the pseudo relation knowledge provided by the event extraction module.\nDuring deployment, we used the trained models in an iterative process to perform event extraction. We will first predict the event type with the event classification model, and then implement argument extraction according to the predicted event type. So the model ends up running through all the predicted event types. To do so, we first use the event classification model to predict the event type without a pseudo label and relation. Next, we use the argument extraction model to identify all arguments associated with the predicted event type. We then go back to ask the event classification module again to update the event type using the pseudo label and argument relations extracted by the argument extraction model. This 2-stage iterative process uses the predicted event type to extract arguments, and the extracted information helps the event classification model improve its prediction.", "publication_ref": ["b4", "b5", "b6", "b7", "b8", "b9", "b10", "b11", "b12", "b8", "b13", "b14", "b15", "b20", "b21", "b22", "b23", "b24", "b25", "b4", "b5", "b26", "b18", "b27", "b28", "b29", "b23", "b18", "b30", "b31", "b23", "b25", "b32", "b4", "b5", "b18", "b33", "b4", "b34", "b35", "b15", "b36", "b37", "b38", "b39", "b30", "b40", "b27", "b6", "b27", "b6", "b8", "b41", "b42", "b43", "b7", "b8", "b8", "b41", "b7", "b8"], "figure_ref": ["fig_1", "fig_1", "fig_2"], "table_ref": []}, {"heading": "A. Double Perspective Event Representation", "text": "The first step of our argument extraction pipeline is to learn the representation (or embeddings) to be used for argument selection. We do so by first constructing a lexicon-based graph, from which we learn the lexicon-based representation of individual words. We then learn the context representation at the sentence-level across multiple words. To that end, ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Lexicon-based Graph Construction Lexicon-based Representation Incremental Lexicon-based Graph", "text": "in lexicon between lexicon relay co-occurrence in lexicon between lexicon relay co-occurrence pseudo hotel Fig. 4: The flowchart of lexicon-based representation. The input sentence is \"In Baghdad, a cameraman died when an American tank fired on the Palestine hotel.\". Before constructing the graph, we remove punctuation marks and prepositions from the sentence, which can reduce the complexity of the graph calculation. The graph construction module has five edges: the black edge connects the relay node and all words; the orange edge is the phase's connection; the blue is the association between phrases; the green connects words with high co-occurrence probability; the red is the pseudo argument relation.\nwe use both a lexicon-based graph attention network [14] and an event-based BERT model [15] for learning the word representation from semantics and context two perspectives.\n1) Lexicon-based Representation: Lexicon-based graph neural network has been designed for the node classification task [14], proved to be an effective way to learn global semantics. Thus, we use lexical knowledge to concatenate characters capturing the local composition and a global relay node to capture long-range dependency.\nWe convert the sentence to a directed graph (as shown in Fig. 4) where each word is represented as a graph node, and a graph edge represents one of the five relations: words in a lexicon; lexicon to lexicon; a relay node connecting to all nodes; co-occurrence words; and pseudo relation among arguments of an event. The first connects words in the phrase sequentially until the last word. The second is to create a line between phrases that the last word of the current phrase is connected with the latter phrase, and each edge represents the potential characteristics of the word that may exist. We also use a relay node as a virtual hub, which is connected to all other nodes. It gathers all the edges and nodes' information, eliminating the boundary ambiguity between words, and learning longrange dependency. Therefore, the representation of the relay node can be regarded as the representation of the sentence. The fourth edge represents pseudo argument relation by connecting the predicted arguments in an event. The last one is calculating the co-occurrence probability of words within sliding windows in the corpus. The edge weights are measured by pointwise mutual information (PMI):\nPMI (w i , w j ) = log p(w i , w j ) p(w i )p(w j ) = log N wi,wj N s N wi N wj ,(1)\nwhere N wi , N wj , N wi,wj are the number of sliding windows containing word w i , w j and both w i , w j , and i, j\n\u2208 [1, N ].\nN s is the total number of sliding windows in the corpus.\nTo learn the word-level representation, we extend the lexicon-based graph attention network (LGAT) that is designed for learning global semantics for node classification [14]. We extend the LGAT by adding a pseudo edge, i.e., by appending an edge among predicted arguments of an event type. Our goal is to update word representation using existing prediction results. Given an N-word text T = {T 1 , T 2 , . . . , T N }, the model embeds the initial input text ET = {ET 1 , ET 2 , . . . , ET N } through the pre-trained embedding matrix. The embedding of predicted event role R = {R 1 , R 2 , . . . , R N } is represented as ER = {ER 1 , ER 2 , . . . , ER N }. The model takes as input EI = ET \u2295 ER where \u2295 means concatenation and produce a hidden representation\nH n = {h n 1 , h n 2 , . . . , h n N }, for text T .\nHere, h n i represents the text feature of the i-th word on the n-th hidden layer. Therefore, the final node representation is:\nLT i = f (H N i ), i = 0, 1, 2, . . . , N,(2)\nwhere LT 0 is the relay node used to predict the event type.\nIt is then used to obtain the optimal decision through the conditional random field (CRF). The probability of a label sequence\u0177 =\u0109 1 ,\u0109 2 , . . . ,\u0109 k can be defined as follows:\np(\u0177 | s) = exp N i=1 \u03c6 (\u0109 i\u22121 ,\u0109 i , LT) y \u2032 \u2208L(s) exp N i=1 \u03c6 c \u2032 i\u22121 , c \u2032 i , LT ,(3)\nwhere L(s) is the set of all arbitrary label sequences.\n\u03c6 (\u0109 i\u22121 ,\u0109 i , h) = W (ci\u22121,ci) c T i + b (ci\u22121,ci) ,(4)\nwhere W (ci\u22121,ci) \u2208 R k\u00d7N and b (ci\u22121,ci) \u2208 R k\u00d7N are the weight and bias parameters specific to the labels c i\u22121 and c i , k is the number of event types, and N is the input length of text.\n2) Event-based Context Representation: BERT is a multilayer bidirectional Transformer [15], achieving significant performance improvement on event extraction task [19]. We use a BERT model [19] to learn the context representation. Specifically, we feed the sentence text into an event-based BERT model to encode the input text T , the predicted event role R, and the embedding of Agent A (used for dialogue generation described in Section III-B1) EA = {EA 1 , EA 2 , . . . , EA M }. We extend BERT by adding a self-attention mechanism to learn new contextual representations of Agent A (denoted as SA,) and input text (denoted as ST ).\n3) Final Representation: We concatenate the lexicon and event-based representation to produce the final representation CT i , to be used for argument extraction.\nCT i = LT i \u2295 ST i .\n(5)", "publication_ref": ["b13", "b14", "b13", "b13", "b14", "b18", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "B. Ranked Argument Extraction", "text": "Given an event type, our argument extraction component aims to generate high-quality dialogue content by ranking the argument extraction order and utilizing the historical dialogue content. It consists of four main modules: dialogue generation, argument extraction, incremental event learning, and reinforcement learning based dialogue management. The dialogue guided argument extraction model automatically extracts the arguments by inputting the actual event type and text. The incremental event learning module then adds the pseudo label as the training data and appends the pseudo-relationship to the lexicon-based graph. After generating the target sentence's event representation, we follow a dialogue-guided strategy for argument extraction. Specifically, Agent A uses the question set from [7] to generate a query (or question) about an event or a chosen argument (e.g., \"What is the trigger of event X?\"). Then, Agent B answers the query by predicting an argument role or event type. Based on the answer, Agent A will then generate a new query in the next turn of argument extraction. This iterative process is driven by an RL-based dialogue management system described in Section III-B4, aiming to optimize the order of argument extraction. The answer given by Agent B will also be fed into an incremental event learning module described later to update the previous answers to be used for the next turn of argument extraction. Later in Table III, we give an example dialogue generated by our approach. The automatically generated dialogue produces extra information to be used during the next-turn argument extraction. Our approach is highly flexible, allowing one to tailor the event extraction framework to a specific domain by populating the question set with domain knowledge.\n1) Dialogue Generation: Our dialogue generation module uses two agents (A and B) to assist event extraction through a sequence of question-answer conversations. Here, Agent A generates dialogue content according to the currently processed role. For each current role, it generates a question set [7] to create more training data for argument extraction. For example, when the goal of Agent A is to generate dialogue for the argument role\"Instrument\", we select one of the predesigned question set templates to generate dialogue. All we need to do is filling the argument roles for the given template. Agent A produces a hybrid content consisting of both the current argument role and arguments already extracted. Agent B then generates the content, including the predicted argument given by the argument extraction (described in the next paragraph). The predicted argument is then fed into Agent A to generate a new dialogue for the next turn of argument extraction. Similar to Agent A, Agent B also provides a dialog content generation template and only needs to fill the template with predicted arguments. If the predicted argument meets the confidence conditions, it will be part of the content of Agent A. The content of Agent B will also be fed into the incremental event learning module described later to add high confidence results for the next turn of argument extraction.\n2) Argument Extraction: Agent B responds to a query by filling the answer slots of a simple answer template designed for each specific question template. It does so by using the learned representation, CT i , to locate the start (i s ) and end (i e ) position of an argument within the target sentence. Specifically, we obtain the word probability of a chosen argument as:\nP start (r, t, k) = exp (W rs CT k ) i=N i=1 exp (W r CT i ) ,(6)\nP end (r, t, k) = exp (W re CT k ) i=N i=1 exp (W r CT i ) ,(7)\nwhere W rs and W re are vectors to map CT k to a scalar. Each event type t has a type-specific W rs and W re . The probability, P span (r, t, a is,ie ), of an argument being the description (or answer) for argument role r and event type t is given as follows:\nP span (r, t, a is,ie ) = P start (r, t, i s ) \u00d7 P end (r, t, i e ).\n3) Incremental Event Learning: Our incremental argument learning module tries to incorporate the information obtained at the current argument extraction turn to extract a new argument in the next turn. We do so by adding the extracted argument roles (i.e., pseudo labels) whose reward (evaluated by RL) is greater than a configurable threshold to the input text to provide additional information for extracting new arguments. We also add a new edge (i.e., pseudo relation) to connect the extracted arguments in the lexicon-based graph for an event, so that we can update the lexicon representation to be used for the dialogue in the next turn.\n4) Reinforcement Learning-based Dialogue Management: We use RL to optimize the argument extraction order during our iterative, dialogue-guided argument extraction process.\nDialogue action. The dialogue-guided event extraction method defines the action as the set of event schema and argument. It indicates that the reinforcement learning algorithm requires determining the argument roles to go from the current argument to the following argument. Different from the previous reinforcement learning-based method, we design two agents with varying spaces of action. For Agent A, action a A is a role from the event schema, which is the action of Agent A. For Agent B, action a B is the argument, which is the action of Agent B. While it is essential to determine if the event type of current dialogue turn needs to be converted to the next event type. It means our method can determine well of event type changing.\nDialogue state. Defined at time step t, state S t \u01ebS, is characterized through S t = (R t , c A t , c B t , Q 0 , H C ). Where R t is the argument role selected through the reinforcement learning-based dialogue management, c A t , c B t are the current embedding of Agent A and B, Q 0 is the initial question of Agent A about event type, H C represents the history of the conversation. Different historical conversation H C contributes differently to the target argument extraction. States s A and s B are history-dependent, encoding the historical dialogue and T . s A and s B are the concatenation of the state s A t\u22121 , s B t\u22121 from the last dialogue content and the current content embedding c A t , c B t , represented as:\ns A t = s B t\u22121 \u2295 c A t ,(9)\ns B t = s A t \u2295 c B t .(10)\nDialogue policy network. The policy is choosing the right action for role selection. The policy network is a parameterized probability map in action space and confidence degree, which aims to maximize the expected accumulated reward.\n\u03c0 \u03b8 a A , a B | s A , s B ) = \u03c0 a A , a B | (s A , s B ); \u03b8 = P a A t = a A , a B t = a B | s A t = s A , s B t = s B , \u03b8 t = \u03b8 ,(11)\nwhere \u03b8 is the learnable parameter representing the weight on our dialogue policy network. The dialogue policy network decides that actions are chosen of T . It consists of two networks. The first network is the feed-forward network for encoding the dialogue histories is implemented using a softmax function. The second network is the BiLSTM for encoding the dialogue histories\nH C t = (H C t\u22121 , a t\u22121 , s A t , s B t )\nbeing a continuous vector h C t , and H C t is an observation, and a t\u22121 is an action sequence, updated through BiLSTM.\ne T = softmax W T F s A , s B + b T ,(12)\nh C t = BiLST M h C t\u22121 , a t\u22121 ; s A t , s B t ,(13)\nwhere W T and b T are the parameters, F (s t ) is the state feature vector, e T is the vector of the input sentence T , h C t\u22121 is the representation in t \u2212 1 conversation, a t\u22121 , s A t and s B t are the action representation and the current state representation of Agent A and B respectively.\nDialogue reward. The dialogue management module saves the historical dialogues. For the specific event type, the search space is limit. We design a reward function to evaluate all actions. The reward R(s B , a A , a B ) is defined as the relatedness of the argument extraction part.\nR(s B , a A , a B ) = i P span (r i , t, a is,ie ).(14)\nThe policy agent can be effectively optimized using the reward signal. Note that we simultaneously identify all remaining arguments whose reward is less than a threshold in the final turn to avoid error propagation. The threshold in our model is 0.75.", "publication_ref": ["b6", "b6"], "figure_ref": [], "table_ref": []}, {"heading": "C. Event Classification", "text": "Event classification detects whether the input sentence is an event and classifies the event type to which the sentence belongs. Each sentence is fed into a lexicon-based graph neural network model and an event-based BERT [34] model to learn the global knowledge and context knowledge of the sentence, respectively. The event classification model detects what kinds of events the sentence contains by adding pseudo argument relation knowledge. If the sentence does not contain an event, NULL is output, and the subsequent modules are not executed. enabling it to distinguish the prediction error.\nWe use a fully connected layer to compute the context-aware utterance representation y i as follows:\ny i = ReLU (W (LT i \u2295 ST i ) + b),(15)\nwhere W and b are trainable parameters, and \u2295 denotes vector concatenation operation. For event classification sub-task, an ReLU activation is used to enforce sparsity. To improve event classification performance, we design an extra taskpredicting the number of event types. Our multi-task event classification model calculates the combined loss of the two tasks to overcome the event type imbalance results in a low recall.", "publication_ref": ["b33"], "figure_ref": [], "table_ref": []}, {"heading": "1) Trigger Classification:", "text": "The existing event classification is based on the trigger identification to identify the event type, but our method is directly according to the input sentence to identify the event type. Therefore, when we evaluate the performance of trigger classification, we use the trigger predicted in the previous Section III-B of ranked argument extraction. We concatenate the predicted trigger and the input sentence to classify the event type.\n2) Multi-task Joint Loss for Event Classification: The multi-task joint loss function estimates the difference in the predicted result and ground-truth value. We design two tasks to learn the distinction between prediction error. For the event classification task, we employ the cross-entropy loss function defined as:\nL T = \u2212 i yt i log \u0177 t i + (1 \u2212 yt i ) log 1 \u2212\u0177t i , (16)\nwhere yt i and\u0177t i are the i-th real label and the predicted label on the event classification task. For the type number prediction task, we adopt the mean square loss with the L 2 -norm:\nL N = i \u0177 l i \u2212 yl i 2 + \u03b7 \u0398 2 , (17\n)\nwhere yl i and\u0177l i are the i-th label and prediction on the second task. \u0398 is model parameters, and \u03b7 is a regularization factor. The overall loss function for optimizing the whole event classification model is:\nL = \u03bb 1 L T + \u03bb 2 L N ,(18)\nwhere \u03bb 1 , \u03bb 2 are hyperparameters to balance the two loss. The \u03bb 1 is 0.65 and \u03bb 2 is 0.35 in our model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "IV. EXPERIMENTS AND RESULTS", "text": "", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A. Tasks", "text": "We test our approach on the ACE 2005 [16] dataset, which is the most widely-used dataset in event extraction. It contains 599 documents, annotated with 8 coarse-grained event types, 33 event subtypes, and 36 argument roles. The part that we use for evaluation is fully annotated with 5,272 event triggers and 9,612 arguments. To make our results directly comparable, we keep the same data split as previous work [6,19,7,20]  I: The P (precision), R (recall), and F1 score on event classification, argument identification and argument role classification results performed on the ACE 2005 test set. Best results are highlighted in bold and \"-\" means results are not available. We use the two perspective event representation module and multi-turn dialogue module in our dialogue guided model. The difference between our full model and dialogue-guided model is whether using reinforcement learning. dialogue content generation of Agent A, we generate content for the argument role selected by the reinforcement learningbased dialogue management module. For example, if the event type is \"Life:Die\" and the argument role is \"instrument\" for example in Fig. 2, the generated content is \"What is the killing instrument of event Life:Die?\". In dialogue, we add predicted arguments from Agent B. To keep the sentence grammatically correct, we add fixed compositions, such as adding \"using\" before the role of \"instrument\". For the dialogue content generation of Agent B, we generate content according to the predicted argument. Here, as with Agent A, the template is pre-designed. We evaluate the performance of our model and comparison models for trigger classification (TC), trigger identification (TI), argument identification (AI), and argument role classification (ARC) sub-tasks. The evaluation metrics include precision (P), recall (R), and F1.", "publication_ref": ["b15", "b5", "b18", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "B. Parameters", "text": "We implement our model based on BERT [34]. We use BERT [34] as sequence encoding for queries and the hyperparameters of the decoder are the same as for the encoder. It has 12 layers, 768-dimensional hidden embeddings, 12 attention heads, and 110 million parameters. The dialogue generation module generates multiple questions for the same trigger and argument. The final number of sentences is added to 2,4000 in training, validation, and test sets of 19,200, 2,400, and 2,400 sentences, at an 8:1:1 ratio. The maximum sequence length is 512-word pieces, the learning rate is 3 \u00d7 10 5 with an Adam optimizer, the maximum gradient norm for gradient clipping is 1.0. The model is trained for 10 epochs and the batch size is 8, where we set the max question length to 128 and the max answer length to 64. The optimal hyperparameters are tuned on the validation set by grid search, and we tried each hyperparameter five times. The dialogue policy network contains a 128 unit bidirectional LSTM and a softmax layer. The dimension of BERT-based word embeddings is 512 dimensions. The mini-batch size is 128 in training. We use Adam as optimization algorithm with the gradient clipping being 5.", "publication_ref": ["b33", "b33"], "figure_ref": [], "table_ref": []}, {"heading": "C. Comparisons.", "text": "We compare our extraction method with eight event extraction methods: DBRNN [5] leverages the dependency graph information to extract event triggers and argument roles. JMEE [6], a jointly event extraction framework, introduces attention-based GCN to model graph information. Joint3EE [17] is a multi-task model that performs entity recognition, trigger detection and argument role assignment by shared Bi-GRU hidden representations. GAIL-ELMO [18] is an ELMobased model that utilizes generative adversarial network to focus on harder-to-detect events. PLMEE [19] is a BERTbased pipeline event extraction method and employs event classification depending on trigger.\nChen et al. [45] use bleached statements giving models acquire to information included in annotation manuals. Du et al. [7] apply machine reading comprehension method employs event extraction and enhance data by constructing multiple question for each argument. MQAEE [20] is a multiturn question answering method expediently utilizing history answer to implement event extraction.", "publication_ref": ["b4", "b5", "b16", "b17", "b18", "b44", "b6", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "D. Main Results", "text": "Table I shows the overall results of each approach per evaluation task with six GAT layers. In the trigger classification task, in order to make our model and baseline model adopt the same evaluation index, we supplement the trigger classification experiment. We use the predicted triggers together with the text as the input of the event classification model to predict the event type corresponding to the current triggers. In our dialogue guided model, we use our event representation module and multi-turn dialogue model without reinforcement learning. In our full model, we introduce reinforcement learning-based dialogue management. The difference between full model and dialogue-guided model is whether using reinforcement learning to learn the order of argument extraction. Our model consistently outperforms all other approaches on F1 and precision. Compare to Du et al. [7] and MQAEE [20], two recent machine reading comprehension models, our dialogue-guided model respectively achieves 7.23% and 7.92% improvements on the F1-score on the TC sub-task. For TI and AI, our dialogue-guided model improves the F1-score by at least 10% through reinforcement learning to guide the argument extraction order. It achieves 6.11% and 6.02% improvements on the ARC sub-task. It shows that our method is significantly     superior to the MRC methods, which only utilize relationships among arguments.\nOur dialogue guided model also consistently outperforms PLMEE [19], the best-performing baseline model. The results show the importance of exploiting argument knowledge and relation for event classification and argument extraction. Our full model boosts the F1-score by 9.01%, 13.23%, 14.77%, and 8.02% on TC, TI, AI, and RC, respectively, when compared to the MQAEE, the best-performing alternative on F1-score. Our approach delivers higher precision than other approaches. On some tasks, our approach gives a lower Recall compared to the best-performing baseline model, but the resulting Recall is not far from the best model JMEE (0.47%) and our dialogue guided model exceeds it. By utilizing argument extraction order, our approach delivers the best overall results.\nWe compare the runtime of our model with BERT-based baseline models, including the QA-based model. In our model, event extraction is realized through multiple rounds of our dialogue system, and reinforcement learning is introduced to optimize the argument extraction order. It can accomplish event extraction more accurately but increases training complexity. The original intention of our model is to improve the precision, recall and F1 of event extraction by making full use of the dependence among arguments under limited data. We will consider improving the accuracy without increasing the model complexity and training difficulty in future work.", "publication_ref": ["b6", "b19", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "E. Impact of GAT Layer Number", "text": "A GAT model in a semi-supervised task with a lower label rate, which means the proportion of labeled data to the ACE dataset, requires more graph attention layers to maintain the best performance. Fig 5 presents some empirical evidence to demonstrate the layer effect on our lexicon-based graph for all sub-tasks. We test the performance on TC, TI, AI, and RC four tasks and observe that the F1-score enhances when the GAT layer increases until the sixth layer reach the maximum. With respect to Precision and Recall, GAT achieves the best performance on the sixth and seventh layer.\nAs shown in Fig 6, we also test our model's performance with different layers in special label rates for TC, AI and RC sub-tasks. It is apparent to note that the model gets the best performance under different label rates when the GAT layer is 6, and the number of layers under the best performance exhibits an increasing trend as the label rate increases. It demonstrates that the best GAT layer gets a stable performance under different tasks and changing the label rate. As can be seen from Fig. 5 and Fig. 6, using more GAT layers can improve the performance, judging by the Precision, Recall and F1-score. However, the improvement reaches a peak when using 6 GAT layers across all three evaluated tasks, and a further increase in the number of layers does not give improved performance. Therefore, we choose to use 6 GAT layers.       ", "publication_ref": [], "figure_ref": ["fig_6", "fig_7", "fig_6", "fig_7"], "table_ref": []}, {"heading": "F. Impact of Data Settings", "text": "To verify that our model can achieve significant performance improvement even with little labeled data, we test the model's performance by changing the training data amount. We evaluate the impact of training data on three subtasks in Fig . 7. Compared with the other two sub-tasks, trigger classification is the least affected by the labelled data ratio, which can basically reach 75% F1-score. It shows that our model can achieve relatively stable results under different data scales. Fig. 7c shows how the F1-score changes as the ratio of labeled data available to our scheme changes on argument role classification task. As expected, using more labelled data thus improves the performance of our models. Our reinforcement learning module can achieve good performance, even when the amount of labeled data is small. The combination of reinforcement and incremental event learning can improve the robustness of our approach, leading to better overall results than individual techniques. Still, when we remove the incremental event learning and reinforcement learning module, respectively, the former changes dramatically than the latter. It indicates that the incremental event learning module can make our model insensitive to the data scale change.  Fig. 8 shows the impact of argument compositions. In this experiment, we divide the test data into three parts where each word has (1) one role (One Role of Each Word), (2) more than one role (Multiple Role of Each Word) and (3) a mixture of both (All Circumstances). Our model achieves 92.74%, 76.74% and 63.00% F1-score on the part of one role of each word. As expected, our approach gives better results when the argument has just one role than other data compositions. Nonetheless, our framework still delivers good performance for other data settings and can use the increased labeled data to improve performance. For sentences having multiple roles of words, our model is less affected by the label rate in the event classification task, but more affected by the label rate in the argument extraction sub-task. In general, our model can have relatively stable performance in both one role and multiple roles for each word.", "publication_ref": [], "figure_ref": ["fig_9", "fig_9", "fig_12"], "table_ref": []}, {"heading": "G. The influence on RLD module", "text": "In order to verify the reinforcement learning helping for argument extraction, and the model performance is improved through iterative training. We demonstrate this from three aspects: average loss, average reward, and F1 score. Our model converges in the 110th iteration, with a loss value of 0.0282. The average reward of reinforcement learning converges in the 120th iteration and is 10.7256. The F1 score or argument identification and role classification converge in the 120th iteration. The F1 argument identification subtask is 0.6142, and the F1 of role classification is 0.6997. Therefore, our model converges on 120th iterations when we add the RLD module.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "H. Ablation Study", "text": "We evaluate four variants of our approach, given in Table II. We remove the Reinforcement Learning-based Dialogue (RLD) module, which is the most key module. The descending on TI, AI and RC sub-tasks is 1.40%, 2.85% and 1.62% F1score, leading to performance changing significantly. When it comes to the Incremental Event Learning (IEL) module, F1score decreases 0.58%, 0.88%, 2.34% and 0.64%, respectively. It may prove that our IEL module can provide useful pseudo labels and relations to model the arguments relation. We remove the Lexicon-based Graph Attention network (LGAT) module. F1-score decreases on all four sub-tasks, which shows that the LGAT module positively affects learning lexiconbased knowledge. It may prove that our lexicon-based graph attention network and event-based BERT model can learn better of the event representation. Moreover, the Multi-Task Learning (MTL) module is employed for TC. It improves 1.22% F1 score by distinguishing the error types and contributes F1 score of TI in terms of 0.92%, AI in terms of 2.25% and ARC in terms of 1.20%. It suggests that the MTL module can accurately identify the sentence with multiple same event types. The results suggest that all variants are useful, and the RLD is the most important, as removing it can result in the most drastic performance degradation. By utilizing historical dialogue knowledge, our approach achieves about 6.55%, 11.83%, 11.92% and 6.40% F1-score gains than the bestreported question answering based method MQAEE [20] on the four subtasks. It may demonstrate the effectiveness of reinforcement learning-based dialogue generation. For argument role classification, the precision enhances 7.52% compared with the best-reported model PLMEE [19]. It may prove that our lexicon-based graph attention network and event-based BERT model can learn better of the event representation.\nSentence: Some 70 people were arrested Saturday as demonstrators clashed with police at the end of a major peace rally here, as at least 200,000 anti war protesters took to the streets across the United States and Canada.\nTrigger Time-With ", "publication_ref": ["b19", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "I. Case Study", "text": "Table III gives a dialogue conversation generated by our approach (Section III-B). Our model can solve arguments with multiple roles, including words with different roles in different contexts. It can recognize the argument more completely by learning the relationship between arguments and the order of argument extraction. For example, in the sentence \"As the soldiers approached [...]\", the word \"soldiers\" can play different roles. When the event type is \"Life: Die\", its role is \"Victim\", while when the event type is \"Conflict: Attack\", its role becomes \"Target\". By contrast, the word \"explosives\" plays the same role (\"Instrument\") in different events. Both cases can be correctly recognized by our approach.\nIn some scenarios where the text contains multiple events, our approach may fail to identify some arguments of scattered distribution. For example, as shown in Fig 10, the sentence \"Some 70 people were arrested Saturday [...]\" contains four event types: \"Justice: Arrest-Jail\", \"Conflict: Attack\", \"Conflict: Demonstrate\" and \"Movement: Transport\". For the latest event type, our model does not recognize \"Saturday\" although it has the same role in multiple events. This is because this argument is further away from other arguments, and our model does not explicitly capture such relation when determining the event extraction order. Our future work will look into this.", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "V. CONCLUSION", "text": "We have presented a new approach for event extraction by utilizing event arguments' relationships. We tackle the problem within a task-oriented dialogue guided framework designed for event extraction. Our framework is driven by reinforcement learning. We use RL to decide the order for extracting arguments of a sentence, aiming to maximize the likelihood of successfully inferring the argument role. We then leverage the already extracted arguments to help resolve arguments whose roles would be difficult to settle by considering the argument in isolation. Our multi-turn event extraction process also uses the newly obtained argument information to update decisions of the previously extracted arguments. This dualway feedback process enables us to exploit the relation among event arguments to classify the argument's role in different text contexts. We evaluated our approach on the ACE 2005 dataset and compared to 7 prior event extraction methods. Experimental results show that our approach can enhance event extraction, outperforming competing methods in the majorities of the tasks. In the future, we plan to improve the multi-semantic representation of the dialogue guided event extraction by introducing commonsense knowledge.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "ACKNOWLEDGMENT", "text": "We thank the anonymous reviewers for their insightful comments and suggestions. Jianxin Li is the corresponding author. The authors of this paper were supported by the NSFC through grant (No.U20B2053), the ARC DECRA Project (No. DE200100964), and in part by NSF under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Qian Li is currently pursuing the Ph.D. degree with the Department of Computer Science and Engineering, Beihang University (BUAA), Beijing, China. Her research interests include text mining, representation learning, and event extraction. ", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Information extraction", "journal": "Commun. ACM", "year": "1996", "authors": "J R Cowie; W G Lehnert"}, {"ref_id": "b1", "title": "Knowledge Representation, Reasoning and Declarative Problem Solving", "journal": "Cambridge University Press", "year": "2010", "authors": "C Baral"}, {"ref_id": "b2", "title": "Advances in automatic text summarization", "journal": "Inf. Retr", "year": "2001", "authors": "E D Liddy"}, {"ref_id": "b3", "title": "Event prediction", "journal": "", "year": "2009", "authors": "S Wasserkrug"}, {"ref_id": "b4", "title": "Jointly extracting event triggers and arguments by dependency-bridge RNN and tensor-based argument interaction", "journal": "", "year": "2018", "authors": "L Sha; F Qian; B Chang; Z Sui"}, {"ref_id": "b5", "title": "Jointly multiple events extraction via attention-based graph information aggregation", "journal": "", "year": "2018-11-04", "authors": "X Liu; Z Luo; H Huang"}, {"ref_id": "b6", "title": "Event extraction by answering (almost) natural questions", "journal": "", "year": "2020", "authors": "X Du; C Cardie"}, {"ref_id": "b7", "title": "Bbqnetworks: Efficient exploration in deep reinforcement learning for taskoriented dialogue systems", "journal": "", "year": "2018", "authors": "Z C Lipton; X Li; J Gao; L Li; F Ahmed; L Deng"}, {"ref_id": "b8", "title": "Budgeted policy learning for task-oriented dialogue systems", "journal": "Long Papers", "year": "2019-08-02", "authors": "Z Zhang; X Li; J Gao; E Chen"}, {"ref_id": "b9", "title": "Entity-relation extraction as multi-turn question answering", "journal": "Long Papers", "year": "2019-08-02", "authors": "X Li; F Yin; Z Sun; X Li; A Yuan; D Chai; M Zhou; J Li"}, {"ref_id": "b10", "title": "Neural belief tracker: Data-driven dialogue state tracking", "journal": "Long Papers", "year": "2017-07-30", "authors": "N Mrksic; D \u00d3 S\u00e9aghdha; T Wen; B Thomson; S J Young"}, {"ref_id": "b11", "title": "Multi-turn dialogue generation in e-commerce platform with the context of historical dialogue", "journal": "", "year": "2020-11-20", "authors": "W Zhang; K Song; Y Kang; Z Wang; C Sun; X Liu; S Li; M Zhang; L Si"}, {"ref_id": "b12", "title": "History-adaption knowledge incorporation mechanism for multi-turn dialogue system", "journal": "", "year": "2020", "authors": "Y Sun; Y Hu; L Xing; J Yu; Y Xie"}, {"ref_id": "b13", "title": "A lexicon-based graph neural network for chinese NER", "journal": "", "year": "2019-11-03", "authors": "T Gui; Y Zou; Q Zhang; M Peng; J Fu; Z Wei; X Huang"}, {"ref_id": "b14", "title": "Attention is all you need", "journal": "", "year": "2017-12-04", "authors": "A Vaswani; N Shazeer; N Parmar; J Uszkoreit; L Jones; A N Gomez; L Kaiser; I Polosukhin"}, {"ref_id": "b15", "title": "The automatic content extraction (ACE) program -tasks, data, and evaluation", "journal": "", "year": "2004-05-26", "authors": "G R Doddington; A Mitchell; M A Przybocki; L A Ramshaw; S M Strassel; R M Weischedel"}, {"ref_id": "b16", "title": "One for all: Neural joint modeling of entities and events", "journal": "", "year": "2019-02-01", "authors": "T M Nguyen; T H Nguyen"}, {"ref_id": "b17", "title": "Joint entity and event extraction with generative adversarial imitation learning", "journal": "Data Intell", "year": "2019", "authors": "T Zhang; H Ji; A Sil"}, {"ref_id": "b18", "title": "Exploring pre-trained language models for event extraction and generation", "journal": "Long Papers", "year": "2019-08-02", "authors": "S Yang; D Feng; L Qiao; Z Kan; D Li"}, {"ref_id": "b19", "title": "Event extraction as multi-turn question answering", "journal": "", "year": "2020-11-20", "authors": "F Li; W Peng; Y Chen; Q Wang; L Pan; Y Lyu; Y Zhu"}, {"ref_id": "b20", "title": "Liberal event extraction and event schema induction", "journal": "Long Papers", "year": "2016", "authors": "L Huang; T Cassidy; X Feng; H Ji; C R Voss; J Han; A Sil"}, {"ref_id": "b21", "title": "Joint extraction of events and entities within a document context", "journal": "", "year": "2016", "authors": "B Yang; T M Mitchell"}, {"ref_id": "b22", "title": "Semisupervised event extraction with paraphrase clusters", "journal": "", "year": "2018", "authors": "J Ferguson; C Lockard; D S Weld; H Hajishirzi"}, {"ref_id": "b23", "title": "Event extraction via dynamic multi-pooling convolutional neural networks", "journal": "Long Papers", "year": "2015-07-26", "authors": "Y Chen; L Xu; K Liu; D Zeng; J Zhao"}, {"ref_id": "b24", "title": "Joint event extraction based on skip-window convolutional neural networks", "journal": "", "year": "2016", "authors": "Z Zhang; W Xu; Q Chen"}, {"ref_id": "b25", "title": "Modeling skip-grams for event detection with convolutional neural networks", "journal": "", "year": "2016-11-01", "authors": "T H Nguyen; R Grishman"}, {"ref_id": "b26", "title": "Biomedical event extraction on graph edge-conditioned attention networks with hierarchical knowledge graphs", "journal": "", "year": "2020-11-20", "authors": "K Huang; M Yang; N Peng"}, {"ref_id": "b27", "title": "Event extraction as machine reading comprehension", "journal": "", "year": "2020", "authors": "J Liu; Y Chen; K Liu; W Bi; X Liu"}, {"ref_id": "b28", "title": "Extracting entities and events as a single task using a transition-based neural model", "journal": "", "year": "2019", "authors": "J Zhang; Y Qin; Y Zhang; M Liu; D Ji"}, {"ref_id": "b29", "title": "Joint event extraction with hierarchical policy network", "journal": "", "year": "2020", "authors": "P Huang; X Zhao; R Takanobu; Z Tan; W Xiao"}, {"ref_id": "b30", "title": "Scale up event extraction learning via automatic training data generation", "journal": "", "year": "2018", "authors": "Y Zeng; Y Feng; R Ma; Z Wang; R Yan; C Shi; D Zhao"}, {"ref_id": "b31", "title": "A survey of event extraction from text", "journal": "IEEE Access", "year": "2019", "authors": "W Xiang; B Wang"}, {"ref_id": "b32", "title": "Event detection and domain adaptation with convolutional neural networks", "journal": "Short Papers", "year": "2015-07-26", "authors": "T H Nguyen; R Grishman"}, {"ref_id": "b33", "title": "BERT: pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "J Devlin; M Chang; K Lee; K Toutanova"}, {"ref_id": "b34", "title": "A survey of textual event extraction from social networks", "journal": "", "year": "2017", "authors": "M Mejri; J Akaichi"}, {"ref_id": "b35", "title": "Event extraction via bidirectional long short-term memory tensor neural networks", "journal": "", "year": "2016-10-15", "authors": "Y Chen; S Liu; S He; K Liu; J Zhao"}, {"ref_id": "b36", "title": "An embarrassingly simple approach to zero-shot learning", "journal": "", "year": "2015-06-11", "authors": "B Romera-Paredes; P H S Torr"}, {"ref_id": "b37", "title": "Zeroshot transfer learning for event extraction", "journal": "Long Papers", "year": "2018-07-15", "authors": "L Huang; H Ji; K Cho; I Dagan; S Riedel; C R Voss"}, {"ref_id": "b38", "title": "Abstract meaning representation for sembanking", "journal": "", "year": "2013", "authors": "L Banarescu; C Bonial; S Cai; M Georgescu; K Griffitt; U Hermjakob; K Knight; P Koehn; M Palmer; N Schneider"}, {"ref_id": "b39", "title": "Event nugget annotation: Processes and issues", "journal": "", "year": "2015-06-04", "authors": "T Mitamura; Y Yamakawa; S Holm; Z Song; A Bies; S Kulick; S M Strassel"}, {"ref_id": "b40", "title": "Asking effective and diverse questions: A machine reading comprehension based framework for joint entity-relation extraction", "journal": "", "year": "2020", "authors": "T Zhao; Z Yan; Y Cao; Z Li"}, {"ref_id": "b41", "title": "Meta-learning for lowresource natural language generation in task-oriented dialogue systems", "journal": "", "year": "2019", "authors": "F Mi; M Huang; J Zhang; B Faltings"}, {"ref_id": "b42", "title": "A survey on dialogue systems: Recent advances and new frontiers", "journal": "SIGKDD Explor", "year": "2017", "authors": "H Chen; X Liu; D Yin; J Tang"}, {"ref_id": "b43", "title": "Large-scale multi-domain belief tracking with knowledge sharing", "journal": "ACL", "year": "2018", "authors": "O Ramadan; P Budzianowski; M Gasic"}, {"ref_id": "b44", "title": "Reading the manual: Event extraction as definition comprehension", "journal": "", "year": "2020-11-20", "authors": "Y Chen; T Chen; S Ebner; A S White; B V Durme"}], "figures": [{"figure_label": "", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Manuscript received May 2021, revised September 2021, accepted December 2021. This work was supported by the NSFC through grants (No.U20B2053, and 62002007), State Key Laboratory of Software Development Environment (SKLSDE-2020ZX-12), NSF (III-1526499, III-1763325, III-1909323), and the UK EPSRC (EP/T01461X/1). This work was also sponsored by CAAI-Huawei MindSpore Open Fund. Thanks for computing infrastructure provided by Huawei MindSpore platform. (Corresponding author: Jianxin Li.) Qian Li, Hao Peng, Jianxin Li and Yuanxing Ning are with Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing 100191, China. E-mail: {liqian, penghao, lijx, ningyx}@act.buaa.edu.cn.", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 1 :1Fig. 1: Three example sentences belong to the \"Conflict: Attack\" event type from the ACE 2005 dataset.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 3 :3Fig. 3: Overview of our event extraction framework. The training flow is: \u2461 \u2192 \u2462; the testing flow during deployment is: \u2462 \u2192 \u2461 \u2192 \u2462. LT is the lexicon-based word representation. EA, ER, and ET are the dialogue content embedding of Agent A, argument role embedding, and event type embedding. SA and ST are the dialogue content representation of Agent A and representation of ET after BERT. CT is the final representation of input text.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": ". The number of documents for the training set, validation set, and test set is 529, 30, and 40, respectively. It contains a complete set of training data in English, Arabic, and Chinese for the ACE 2005 technology evaluation. For the", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Fig. 5 :5Fig. 5: The change of Precision, Recall and F1 for our model with different GAT layers under different tasks.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_7", "figure_caption": "Fig. 6 :6Fig. 6: The change of sub-tasks for our model with different GAT layers under different label rates.", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Fig. 7 :7Fig.7: The resulting F1 score of our approach as the ratio of labeled data changes for different tasks on different tasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Impact of argument roles.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Impact of argument roles.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_12", "figure_caption": "Fig. 8 :8Fig.8: The F1-score of our approach as the ratio of labeled data changes for multiple type test data on different tasks.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_13", "figure_caption": "Fig. 9 :9Fig. 9: The influence of reinforcement learning on argument extraction task.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "What is the first event type? B: The first event type is Life: Die. A: What is the trigger of event Life:Die? B: The trigger of event Life:Die is killing. A: What is the killing instrument of event Life:Die? B: The killing instrument of event Life:Die is explosives. A: Who is the killing victim of event Life:Die using explosives? B: The killing victim of event Life:Die is the soldiers. A: What agent killing the soldiers of event Life:Die using explosives? B: The agent of event Life:Die is the man. A: What is the second event type? B: The second event type is Conflict: Attack. A: What is the trigger of event Conflict: Attack? B: The trigger of event Conflict: Attack is detonated. A: What is the detonated instrument of event Conflict: Attack? B: The detonated instrument of event Conflict: Attack is explosives and car. A: What is the detonated target of event Conflict: Attack using explosives and car? B: The detonated target of event Conflict: Attack is soldiers. A: Who is the detonated attacker of event Conflict: Attack using explosives and car? B: The detonated attacker of event Conflict: Attack is man.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "What is the first event type in the text\uff1f", "figure_data": "Turn 1Turn 2Turn 3generatoranswergeneratoranswergeneratoranswergeneratoranswergeneratoranswer..............................generator...answergeneratoranswergeneratoranswergeneratoranswergenerator.............................."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The ... man.What is the triggerWhat is the killing instrument", "figure_data": "StartTurn 4Turn 5Input Texttype is Life:Die.Life:Die The trigger of event is killing....of event Life:Die?of event Life:Die?event using explosives? soldiers of the Life:DieTurn 6Event TypeEndTurn 10Turn 9Turn 8Turn 7The target ... soldiers.The instrument ... explosives and car. of Conflict: Attack event? What is the detonated instrumentWhat is the trigger of event Conflict: Attack? The trigger ... detonated.What is the second event type? The second... Conflict: Attack."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "73.85 75.24 77.34 79.67 80.53 80.71 79.24 78.34 80.9 82.23 84.82 86.19 87.02 87.73 86.19 85.54 64.78 65.9 66.67 67.02 68.09 69.97 68.98 68.12 77.24 78.14 79.35 80.0 80.46 79.84 80.75 82.45 84.35 86.99 87.1 87.22 87.61 87.35 56.97 58.86 60.74 63.76 64.53 65.3 66.24 66.02 51.09 52.02 53.29 53.65 54.53 54.43 54.92 54.24", "figure_data": "48.41 50.1753.98 55.87 Trigger Classification 59.17 61.42 60.9 60.03 Trigger Identification Argument Identification Role ClassificationPrecision of Different Task (%)55 60 65 70 75 80 85 901 2 3 4 5 6 7 8 Number of GAT layers 75.47 77.22 78.73 79.15 80.93 81.23 79.82 78.34 80.16 82.25 84.7 86.99 87.13 87.94 87.41 86.04 69.24 70.8 71.71 72.9 73.56 73.43 72.49 72.1 66.5 67.22 68.18 69.37 69.26 69.82 69.93 69.07 Trigger Classification Trigger Identification Argument Identification Role ClassificationRecall of Different Task (%)90 40 50 60 70 80Number of GAT layers 1 2 3 4 5 6 7 8 73.24 75.66 Trigger Classification Trigger Identification Argument Identification Role Classification(a) GAT for different tasks on F1.(b) GAT for different tasks on precision.(c) GAT for different tasks on recall."}, {"figure_label": "IIIII", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Ablation study on global constraints on F1-score (%). A dialogue generated by our RL-based dialogue system for sentence: \"As the soldiers approached, the man detonated explosives in the car, killing all four of the soldiers.\".", "figure_data": "TasksTrigger ClassificationTrigger IdentificationArgument IdentificationArgument Role ClassificationPF1PF1PF1PF1Ours81.2380.7187.9487.7373.4369.9769.8261.42-RLD79.3478.2586.2186.3371.8267.1267.9259.80-IEL79.6280.1387.0186.8571.9267.6367.9960.78-LGAT80.5280.4686.2186.1072.0369.4568.5161.01-MTL80.0279.4986.9386.8171.8967.7267.2060.22"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Fig. 10: An example on ACE 2005 with a standard answer table for event extraction. The words in bold are the event arguments.", "figure_data": "ArtifactDestinationEvent 1Event 2Event 3Event 4Type: Justice: Arrest-JailType: Conflict: AttackType: Conflict: DemonstrateType: Movement: TransportTrigger: arrestedTrigger: clashedTrigger: rallyTrigger: tookTime-Within: SaturdayTime-Within: SaturdayTime-Within: SaturdayTime-Within: SaturdayPlace: herePlace: herePlace: hereArtifact: protestersPerson: peopleAttacker: demonstrators, policeEntity: protestersDestination: streets"}], "formulas": [{"formula_id": "formula_0", "formula_text": "PMI (w i , w j ) = log p(w i , w j ) p(w i )p(w j ) = log N wi,wj N s N wi N wj ,(1)", "formula_coordinates": [6.0, 65.12, 620.22, 234.91, 24.23]}, {"formula_id": "formula_1", "formula_text": "\u2208 [1, N ].", "formula_coordinates": [6.0, 262.85, 666.01, 37.17, 9.96]}, {"formula_id": "formula_2", "formula_text": "H n = {h n 1 , h n 2 , . . . , h n N }, for text T .", "formula_coordinates": [6.0, 411.53, 347.66, 151.5, 12.8]}, {"formula_id": "formula_3", "formula_text": "LT i = f (H N i ), i = 0, 1, 2, . . . , N,(2)", "formula_coordinates": [6.0, 367.85, 393.84, 195.19, 12.97]}, {"formula_id": "formula_4", "formula_text": "p(\u0177 | s) = exp N i=1 \u03c6 (\u0109 i\u22121 ,\u0109 i , LT) y \u2032 \u2208L(s) exp N i=1 \u03c6 c \u2032 i\u22121 , c \u2032 i , LT ,(3)", "formula_coordinates": [6.0, 327.36, 476.0, 235.67, 35.11]}, {"formula_id": "formula_5", "formula_text": "\u03c6 (\u0109 i\u22121 ,\u0109 i , h) = W (ci\u22121,ci) c T i + b (ci\u22121,ci) ,(4)", "formula_coordinates": [6.0, 349.28, 545.15, 213.75, 12.97]}, {"formula_id": "formula_6", "formula_text": "CT i = LT i \u2295 ST i .", "formula_coordinates": [7.0, 136.02, 99.24, 76.93, 10.39]}, {"formula_id": "formula_7", "formula_text": "P start (r, t, k) = exp (W rs CT k ) i=N i=1 exp (W r CT i ) ,(6)", "formula_coordinates": [7.0, 357.44, 158.93, 205.59, 28.42]}, {"formula_id": "formula_8", "formula_text": "P end (r, t, k) = exp (W re CT k ) i=N i=1 exp (W r CT i ) ,(7)", "formula_coordinates": [7.0, 360.02, 191.19, 203.01, 28.42]}, {"formula_id": "formula_10", "formula_text": "s A t = s B t\u22121 \u2295 c A t ,(9)", "formula_coordinates": [8.0, 140.84, 87.73, 159.18, 13.29]}, {"formula_id": "formula_11", "formula_text": "s B t = s A t \u2295 c B t .(10)", "formula_coordinates": [8.0, 144.13, 110.11, 155.89, 13.28]}, {"formula_id": "formula_12", "formula_text": "\u03c0 \u03b8 a A , a B | s A , s B ) = \u03c0 a A , a B | (s A , s B ); \u03b8 = P a A t = a A , a B t = a B | s A t = s A , s B t = s B , \u03b8 t = \u03b8 ,(11)", "formula_coordinates": [8.0, 61.44, 186.38, 238.58, 33.2]}, {"formula_id": "formula_13", "formula_text": "H C t = (H C t\u22121 , a t\u22121 , s A t , s B t )", "formula_coordinates": [8.0, 86.82, 292.52, 130.62, 11.37]}, {"formula_id": "formula_14", "formula_text": "e T = softmax W T F s A , s B + b T ,(12)", "formula_coordinates": [8.0, 95.54, 336.69, 204.48, 12.0]}, {"formula_id": "formula_15", "formula_text": "h C t = BiLST M h C t\u22121 , a t\u22121 ; s A t , s B t ,(13)", "formula_coordinates": [8.0, 91.22, 359.26, 208.8, 12.97]}, {"formula_id": "formula_16", "formula_text": "R(s B , a A , a B ) = i P span (r i , t, a is,ie ).(14)", "formula_coordinates": [8.0, 91.87, 508.76, 208.15, 22.26]}, {"formula_id": "formula_17", "formula_text": "y i = ReLU (W (LT i \u2295 ST i ) + b),(15)", "formula_coordinates": [8.0, 366.35, 87.49, 196.69, 10.39]}, {"formula_id": "formula_18", "formula_text": "L T = \u2212 i yt i log \u0177 t i + (1 \u2212 yt i ) log 1 \u2212\u0177t i , (16)", "formula_coordinates": [8.0, 324.29, 373.74, 238.74, 22.49]}, {"formula_id": "formula_19", "formula_text": "L N = i \u0177 l i \u2212 yl i 2 + \u03b7 \u0398 2 , (17", "formula_coordinates": [8.0, 368.92, 443.58, 189.96, 26.93]}, {"formula_id": "formula_20", "formula_text": ")", "formula_coordinates": [8.0, 558.89, 450.6, 4.15, 8.91]}, {"formula_id": "formula_21", "formula_text": "L = \u03bb 1 L T + \u03bb 2 L N ,(18)", "formula_coordinates": [8.0, 396.03, 533.21, 167.01, 10.7]}], "doi": "10.1109/TASLP.2021.3138670"}