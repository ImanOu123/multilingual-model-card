{"title": "Total Capture: A 3D Deformation Model for Tracking Faces, Hands, and Bodies *", "authors": "Hanbyul Joo; Tomas Simon; Yaser Sheikh", "pub_date": "2018-01-05", "abstract": "Figure 1: Frankenstein (silver) and Adam (gold). This paper presents a 3D human model capable of concurrently tracking the large-scale posture of the body along with the smaller details of a persons facial expressions and hand gestures.", "sections": [{"heading": "Introduction", "text": "Social communication is a key function of human motion [7]. We communicate tremendous amounts of information with the subtlest movements. Between a group of * Website: http://www.cs.cmu.edu/\u02dchanbyulj/totalcapture interacting individuals, gestures such as a gentle shrug of the shoulders, a quick turn of the head, or an uneasy shifting of weight from foot to foot, all transmit critical information about the attention, emotion, and intention to observers. Notably, these social signals are usually transmitted by the organized motion of the whole body: with facial expressions, hand gestures, and body posture. These rich signals layer upon goal-directed activity in constructing the behavior of humans, and are therefore crucial for the machine perception of human activity.\nHowever, there are no existing systems that can track, without markers, the human body, face, and hands simultaneously. Current markerless motion capture systems focus at a particular scale or on a particular part. Each area has its own preferred capture configuration: (1) torso and limb motions are captured in a sufficiently large working volume where people can freely move [17,21,44,19]; (2) facial motion is captured at close range, mostly frontal, and assuming little global head motion [5,24,6,9,51]; (3) finger motion is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [36,49,42,50]. These configurations make it difficult to analyze these gestures in the context of social communication.\nIn this paper, we present a novel approach to capture the motion of the principal body parts for multiple interacting people (see Fig. 1). The fundamental difficulty of such capture is caused by the scale differences of each part. For example, the torso and limbs are relatively large and necessitate coverage over a sufficiently large working volume, while fingers and faces, due to their smaller feature size, require close distance capture with high resolution and frontal imaging. With off-the-shelf cameras, the resolution for face and hand parts will be limited in a room-scale, multi-person capture setup.\nTo overcome this sensing challenge, we use two general approaches: (1) we leverage keypoint detection (e.g., faces [18], bodies [54,14,35], and hands [41]) in multiple views to obtain 3D keypoints, which is robust to multiple people and object interactions; (2) to compensate for the limited sensor resolution, we present a novel generative body deformation model, which has the ability to express the motion of the each of the principal body parts. In particular, we describe a procedure to build an initial body model, named \"Frankenstein\", by seamlessly consolidating available part template models [33,13] into a single skeleton hierarchy. We optimize this initialization using a capture of 70 people, and learn a new deformation model, named \"Adam\", capable of additionally capturing variations of hair and clothing, with a simplified parameterization. We present a method to capture the total body motion of multiple people with the 3D deformable model. Finally, we demonstrate the performance of our method on various sequences of social behavior and person-object interactions, where the combination of face, limb, and finger motion emerges naturally.", "publication_ref": ["b6", "b16", "b20", "b44", "b18", "b4", "b23", "b5", "b8", "b51", "b35", "b49", "b42", "b50", "b17", "b54", "b13", "b34", "b41", "b1", "b32", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Motion capture systems performed by tracking retroreflective markers [55] are the most widely used motion capture technology due to their high accuracy. Markerless motion capture methods [23,17,21,44] have been explored over the past two decades to achieve the same goal without markers, but they tend to implicitly admit that their performance is inferior by treating the output of marker based methods as a ground truth or an upper bound. However, over the last few years, we have witnessed a great advance in key point detections from images (e.g., faces [18], bodies [54,14,35], and hands [41]), which can provide reliable anatomical landmark measurements for markerless motion capture methods [19,28,41], while the performance of marker based methods relatively remains the same with their major disadvantages including: (1) a necessity of sparsity in marker density for reliable tracking which limits the spatial resolution of motion measurements, and (2) a limitation in automatically handling occluded markers which requires an expensive manual clean-up. Especially, capturing high-fidelity hand motion is still challenging in marker-based motion capture systems due to the severe self-occlusions of hands [59], while occlusions are implicitly handled by guessing the occluded parts with uncertainty using the prior learnt from a large scale dataset [41]. Our method shows that the markerless motion capture approach potentially begins to outperform the marker-based counterpart by leveraging the learning based image measurements. As an evidence we demonstrate the motion capture from total body, which has not been demonstrated by other existing marker based methods. In this section, we review the most relevant markerless motion capture approaches to our method.\nMarkerless motion capture largely focuses on the motion of the torso and limbs. The standard pipeline is based on a multiview camera setup and tracking with a 3D template model [32,23,15,10,29,16,52,11,44,17,20,19]. In this approach, motion capture is performed by aligning the 3D template model to the measurements, which distinguish the various approaches and may include color, texture, silhouettes, point clouds, and landmarks. A parallel track of related work therefore focuses on capturing and improving body models for tracking, for which a highly controlled multiview capture system-specialized for single person capture-is used to build precise models. With the introduction of commodity depth sensors, single-view depth-based body motion capture became a popular direction [3,40]. A recent collection of approaches aims to reconstruct 3D skeletons directly from monocular images, either by fitting 2D keypoint detections with a prior on human pose [60,8] or getting even closer to direct regression methods [61,34,48].\nFacial scanning and performance capture has been greatly advanced over the last decade. There exist multiview based methods showing excellent performance on high-quality facial scanning [5,24] and facial motion capture [6,9,51]. Recently, light-weighed systems based on a single camera show a compelling performance by leveraging morphable 3D face model on 2D measurements [22,18,31,47,13,12,56]. Hand motion captures are mostly lead by single depth sensor based methods [36,46,49,30,57,45,53,43,39,42,50,58], with few exceptions based on multi-view systems [4,43,38]. In this work, we take the latter approach and use the method of [41] who introduced a hand keypoint detector for RGB images which can be directly applicable in multiview systems to reconstruct 3D hand joints.\nAs a way to reduce the parameter space and overcome the complexity of the problems, generative 3D template models have been proposed in each field, for example the methods of [2,33,37] in body motion capture, the method of [13] for facial motion capture, and very recently, the combined body+hands model of Romero et al. [38]. A generative model with expressive power for total body motion has not been introduced. The body model [33]; (b) the face model [13]; and (c) a hand rig, where red dots have corresponding 3D keypoints reconstructed from detectors in (a-c). (d) Aligned face and hand models (gray meshes) to the body model (the blue wireframe mesh); and (e) the seamless Frankenstein model.", "publication_ref": ["b55", "b22", "b16", "b20", "b44", "b17", "b54", "b13", "b34", "b41", "b18", "b27", "b41", "b59", "b41", "b31", "b22", "b14", "b9", "b28", "b15", "b52", "b10", "b44", "b16", "b19", "b18", "b2", "b40", "b60", "b7", "b61", "b33", "b48", "b4", "b23", "b5", "b8", "b51", "b21", "b17", "b30", "b47", "b12", "b11", "b56", "b35", "b46", "b49", "b29", "b57", "b45", "b53", "b43", "b38", "b42", "b50", "b58", "b3", "b43", "b37", "b41", "b1", "b32", "b36", "b12", "b37", "b32", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Frankenstein Model", "text": "The motivation for building the Frankenstein body model is to leverage existing part models-SMPL [33] for the body, FaceWarehouse [13] for the face, and an artistdefined hand rig-each of which capture shape and motion details at an appropriate scale for the corresponding part. This choice is not driven merely by the free availability of the component models: note that due to the trade-off between image resolution and field of view of today's 3D scanning systems, scans used to build detailed face models will generally be captured using a different system than that used for the rest of the body. For our model, we merge all transform bones into a single skeletal hierarchy but keep the native parameterization of each component part to express identity and motion variations, as explained below. As the final output, the Frankenstein model produces motion parameters capturing the total body motion of humans, and generates a seamless mesh by blending the vertices of the component meshes.", "publication_ref": ["b32", "b12"], "figure_ref": [], "table_ref": []}, {"heading": "Stitching Part Models", "text": "The Frankenstein model M U is parameterized by motion parameters \u03b8 U , shape (or identity) parameters \u03c6 U , and a global translation parameter t U ,\nV U = M U (\u03b8 U , \u03c6 U , t U ),(1)\nwhere V U is a seamless mesh expressing the motion and shape of the target subject. The motion and shape parameters of the model are a union of the part models' parameters:\n\u03b8 U = {\u03b8 B , \u03b8 F , \u03b8 LH , \u03b8 RH },(2)\n\u03c6 U = {\u03c6 B , \u03c6 F , \u03c6 LH , \u03c6 RH },(3)\nwhere the superscripts represent each part model: B for the body model, F for the face model, LH for for the left hand model, and RH for the right hand model.\nEach of the component part models maps from a subset of the above parameters to a set of vertices, respectively,\nV B \u2208 R N B \u00d73 , V F \u2208 R N F \u00d73 , V LH \u2208 R N H \u00d73 , and V RH \u2208 R N H \u00d73\n, where the number of vertices of each mesh part is N B =6890, N H =2068, and N F =11510. The final mesh of the Frankenstein model, V U \u2208R N U \u00d73 , is defined by linearly blending them with a matrix C \u2208 R N U \u00d7(N B +N F +2N H ) :\nV U = C V B T V F T V LH T V RH T T ,(4)\nwhere T denotes the transpose of a matrix. Note that V U has fewer vertices than the sum of part models because there are redundant parts in the body model (e.g., face and hands of the body model). In particular, our final mesh has N U =18540 vertices. Figure 2 shows the part models which are aligned by manually clicking corresponding points between parts, and also shows the final mesh topology of Frankenstein model at the mean shape in the rest pose. The blending matrix C is a very sparse matrix and most rows have a single column set to one with zeros elsewhere, simply copying the vertex locations from the corresponding part models with minimal interpolation at the seams.\nIn the Frankenstein model, all parts are rigidly linked by a single skeletal hierarchy. This unification is achieved by substituting the hands and face branches of the SMPL body skeleton with the corresponding skeletal hierarchies of the detailed part models. All parameters of the Frankenstein model are jointly optimized for motion tracking and identity fitting. The parameterization of each of the part models is detailed in the following sections.", "publication_ref": [], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Body Model", "text": "For the body, we use the SMPL model [33] with minor modifications. In this section, we summarize the salient aspects of the model in our notation. The body model, M B , is defined as follows,\nV B = M B (\u03b8 B , \u03c6 B , t B ) (5\n)\nwith V B = {v B i } N B i=1 .\nThe model uses a template mesh of N B =6890 vertices, where we denote the i-th vertex as v B\ni \u2208 R 3 . The vertices of this template mesh are first displaced by a set of blendshapes describing the identity or body shape. Given the vertices in the rest pose, the posed mesh vertices are obtained by linear blend skinning using transformation matrices T B j \u2208 SE(3) for each of J joints,\nv B i = I 3\u00d74 \u2022 J B j=1 w B i,j T B j v B0 i + K b k=1 b k i \u03c6 B k 1 ,(6)\nwhere b k i \u2208 R 3 is the i-th vertex of the k-th blendshape, \u03c6 B k is the k-th shape coefficient in \u03c6 B \u2208 R K b with K b =10 the number of identity body shape coefficients, and v B0 i is the i-th vertex of the mean shape. The transformation matrices T B j encode the transform for each joint j from the rest pose to the posed mesh in world coordinates, which is constructed by following skeleton hierarchy from the root joint with pose parameter \u03b8 B (see [33]). The j-th pose parameter \u03b8 B j is the angle-axis representation of the relative rotation of joint j with respect to its parent joints. w B i,j is the weight with which transform T B j affects vertex i, with J j=1 w B i,j =1 and I 3\u00d74 is the 3\u00d74 truncated identity matrix to transform from homogenous coordinates to a 3 dimensional vector. We use J B =21 with \u03b8 B \u2208 R 21\u00d73 , ignoring the last joint of each hand of the original body model. For simplicity, we do not use the pose-dependent blendshapes.", "publication_ref": ["b32", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Face Model", "text": "As a face model, we build a generative PCA model from the FaceWarehouse dataset [13]. Specifically, the face part model, M F , is defined as follows,\nV F = M F (\u03b8 F , \u03c6 F , T F ),(7)\nwith V F = {v F i } N F i=1\n, where the i-th vertex is v F i \u2208 R 3 , and N F =11510. The vertices are represented by the linear combination of the subspaces:\nv F i = v F 0 i + K f k=1 f k i \u03c6 F k + Ke s=1 e s i \u03b8 F s (8)\nwhere, as before, v F 0 i denotes i-th vertex of the mean shape, and \u03c6 F k and \u03b8 F s are k-th face shape identity (shape) and sth facial expression (pose) parameters respectively. Here, f k i \u2208 R 3 is the i-th vertex of the k-th identity blendshape (K f = 150), and e s i \u2208 R 3 is the i-th vertex of the s-th expression blendshape (K e = 200).\nFinally, a transformation T F brings the face vertices into world coordinates. To ensure that the face vertices transform in accordance to the rest of the body, we manually align the mean face v F 0 i with the body mean shape, as shown in Fig. 2. This way, we can apply the transformation of the body model's head joint T B j=F (\u03b8 B ) as a global transformation for the face model in Eq. 9. However, to keep the face in alignment with the body, an additional transform matrix \u0393 F \u2208 SE(3) is required to compensate for displacements in the root location of the face joint due to body shape changes in Eq. 6.\nFinally, each face vertex position is given by:\nv F i = I 3\u00d74 \u2022 T B j=F \u2022 \u0393 F v F i 1 ,(9)\nwhere the transform \u0393 F , directly determined by the body shape parameters \u03c6 B , aligns the face model with the body model.", "publication_ref": ["b12"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Hand Model", "text": "We use an artist rigged hand mesh. Our hand model has J H =16 joints and the mesh is deformed via linear blend skinning. The hand model has a fixed shape, but we introduce scaling parameters for each bone to allow for different finger sizes. The transform for each joint j is parameterized by the Euler angle rotation with respect to its parent, \u03b8 j \u2208 R 3 , and an additional anisotropic scaling factor along each axis, \u03c6 j \u2208 R 3 . Specifically, the linear transform for joint j in the bone's local reference frame becomes eul(\u03b8 j )\u2022diag(s j ), where eul(\u03b8 j ) converts from an Euler angle representation to a 3 \u00d7 3 rotation matrix and diag(\u03c6 j ) is the 3 \u00d7 3 diagonal matrix with the X,Y ,Z scaling factors \u03c6 j on the diagonal. The vertices of the hand in world coordinates are given by LBS with weights w H i,j :\nv i = I 3\u00d74 \u2022 T B j=H \u2022 \u0393 H \u2022 J j=1 w H i,j T H j v 0 i 1 . (10\n)\nwhere T H j is each bone's composed transform (with all parents in the hierarchy), T B j=H is the transformation of the corresponding hand joint in the body model, and \u0393 H is the transformation that aligns the hand model to the body model. As with the face, this transform depends on the shape parameters of the body model.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Motion Capture with Frankenstein Model", "text": "We fit the Frankenstein model to data to capture the total body motion, including the major limbs, the face, and fingers. Our motion capture method relies heavily on fitting mesh correspondences to 3D keypoints, which are obtained by triangulation of 2D keypoint detections across multiple camera views. To capture shape information we also use point clouds generated by multiview stereo reconstructions. Model fitting is performed by an optimization framework to minimize distances between corresponded model joints and surface points and 3D keypoint detections, and iterative closest point (ICP) to the 3D point cloud.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "3D Measurements", "text": "We incorporate two types of measurements in our framework as shown in Fig. 3: (1) corresponded 3D keypoints, which map to known joints or surface points on the mesh models (see Fig. 2), and (2) uncorresponded 3D points from multiview stereo reconstruction, which we match using ICP.\n3D Body, Face, and Hand Keypoints: We use the OpenPose detector [25] in each available view, which produces 2D keypoints on the body with the method of [14], Figure 3: 3D measurements and Frankenstein fitting result. and hand and face keypoints using the method of [41]. 3D body skeletons are obtained from the 2D detections using the method of [28], which uses known camera calibration parameters for reconstruction. The 3D hand keypoints are obtained by triangulating 2D hand pose detections, following the method of [41], and similarly for the facial keypoints. Note that subsets of 3D keypoints can be entirely missing if there aren't enough 2D detections for triangulation, which can happen in challenging scenes with interocclusions or motion blur.\n3D Feet Keypoints: An important cue missing from the OpenPose detector are landmarks on the feet. For motion capture, this is an essential feature to prevent footskate, as well as to accurately determine the orientation of the feet. We therefore train a keypoint detector for the tip of the big toe, the tip of the little toe, and the ball of the foot. We annotate these 3 keypoints per foot in each of around 5000 person instances of the COCO dataset, and use the neural network architecture presented by [54] with a bounding box around the feet determined by the 3D body detections 1 .\n3D Point Clouds: We use the commercial software Capturing Reality to obtain 3D point clouds from the multiview images, with associated point normals.", "publication_ref": ["b24", "b13", "b41", "b27", "b41", "b54", "b0"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Objective Function", "text": "We initially fit every frame in the sequence independently. For clarity, we drop the time index from the notation and describe the process for a single frame, which optimizes the following cost function: (11) Anatomical Keypoint Cost: the term E keypoints matches 3D keypoint detections which are in direct corresponce to our mesh models. This includes joints (or end effects) in the body and hands, and also contains points corresponding to the surface of the mesh (e.g., facial keypoints and the tips of fingers and toes). Both of these types of correspondence 1 More details provided in the supplementary material.\nE \u03b8 U , \u03c6 U , t U = E keypoints + E icp + E seam + E prior\nare expressed as combinations of vertices via a regression matrix J \u2208 R C\u00d7N U\n, where C denotes the number of correspondences and N U is the number of vertices in the model. Let D denote the set of available detections in a particular frame. The cost is then:\nE keypoints = \u03bb keypoints i\u2208D ||J i V \u2212 y T i || 2 ,(12)\nwhere J i indexes a row in the correspondence regression matrix and represents an interpolated position using a small number of vertices, and y i \u2208 R 3\u00d71 is the 3D detection. The \u03bb keypoints is a relative weight for this term. ICP Cost: The 3D point cloud measurements are not a priori in correspondence with the model meshes. We therefore establish their correspondence to the mesh using Iterative Closest Point (ICP) during each solver iteration. We find the closest 3D point in the point cloud to each of the mesh vertices,\ni * = arg min i ||x i \u2212 v j || 2 ,(13)\nwhere x i * is the closest 3D point to vertex j, where v j is a vertex 2 in V U of the Frankenstein model. To ensure that this is a correct correspondence, we use thresholds for the distance and normals during the correspondence search. Finally, for each vertex j we compute the point-to-plane residual, i.e., the distance along the normal direction,\nE icp = \u03bb icp vj \u2208V U t n(x i * ) T (x i * \u2212 v j ),(14)\nwhere n(\u2022) \u2208 R 3 represents the point's normal, and \u03bb icp is a relative weight for this term.\nSeam Constraints: The part models composing the Frankenstein model are rigidly linked by the skeletal hierarchy. However, the independent surface parameterizations of each of the part models may introduce discontinuities at the boundary between parts (e.g., a fat arm with a thin wrist). To avoid this artifact, we encourage the vertices around the seam parts to be close by penalizing differences between the last two rings of vertices around the seam of each part, and the corresponding closest point in the body model in the rest pose expressed as barycentric coordinates (see the supplementary materials for details).\nPrior Cost: Depending on the number of measurements available in a particular frame, the set of parameters of M u may not be determined uniquely (e.g., the width of the fingers). More importantly, the 3D point clouds are noisy and cannot be well explained by the model due to hair and clothing, which are not captured by the SMPL and FaceWarehouse meshes, which can result in erroneous correspondences during ICP. Additionally, the joint locations of the models are not necessarily consistent with the annotation criteria used to train the 2D detectors. We are therefore forced to set priors over model parameters to avoid the model from overfitting to these sources of noise,\nE prior = E F prior + E B prior + E H prior .\nThe prior for each part is defined by corresponding shape and pose priors, for which we use 0-mean standard normal priors for each parameter except for scaling factors, which are encouraged to be close to 1. Details and relative weights can be found in supplementary materials.", "publication_ref": ["b10", "b0"], "figure_ref": [], "table_ref": []}, {"heading": "Optimization Procedure", "text": "The complete model is highly nonlinear, and due to the limited degrees of freedom of the skeletal joints, the optimization can get stuck in bad local minima. Therefore, instead of optimizing the complete model initially, we fit the model in phases, starting with a subset of measurements and strong priors that are relaxed as optimization progresses.\nModel fitting is performed on each frame independently. To initialize the overall translation and rotation, we use four keypoints on the torso (left and right shoulders and hips) without using the ICP term, and with strong weight on the priors. Once the torso parts are approximately aligned, we use all available keypoints of all body parts, with small weight for the priors. The results at this stage already provide reasonable motion capture but do not accurately capture the shape (i.e., silhouette) of the subject. Finally, the entire optimization is performed including the ICP term to find correspondences with the 3D point cloud. We run the final optimization two times, finding new correspondences each time. For the optimization we use Levenberg-Marquardt with the Ceres Solver library [1].", "publication_ref": ["b0"], "figure_ref": [], "table_ref": []}, {"heading": "Creating Adam", "text": "We derive a new model, which we call Adam, enabling total body motion capture with a simpler parameterization than the part-based Frankenstein model. In particular, this new model has a single joint hierarchy and a common parameterization for all shape degrees of freedom, tying together the face, hand, and body shapes and avoiding the need for seam constraints. To build the model, it is necessary to reconstruct the shape and the motion of all body parts (face, body, and hands) from diverse subjects where model can learn the variations. To do this, we leverage our Frankenstein model and apply it on a dataset of 70 subjects where each of them performs a short range of motion in a multiview camera system. We selected 5 frames for each person in different poses and use the the reconstruction results to build Adam. From the data, both joint location information and linear shape blendshapes are learnt. Because we derive the model from clothed people, the blendshapes explain some variations of them.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Regressing Detection Targets", "text": "There exists a discrepancy between the joint locations of the body model (e.g., SMPL model in our case) and the location of the keypoint detections (i.e., a model joint vs. a detection joint), as shown in Fig. 4. This affects mainly the shoulder and hip joints, which are hard to precisely annotate. This difference has the effect of pulling the Frankenstein model towards a bad fit even while achieving a low keypoint cost, E keypoints . We alleviate this problem by computing the relative location of the 3D detections with respect to the fitted mesh vertices by leveraging the the reconstructed 70 people data. This allows us to define new targets for the keypoint detection cost that, on average, are a better match for the location of the 3D detections with respect to the mesh model, as shown in Fig. 4. In particular, given the fitting results of 70 identities, we approximate the target 3D keypoint locations as a function of the final fitted mesh vertices following the procedure of [33] to find a sparse, linear combination of vertices that approximates the position of the target 3D keypoint. Note that we do not change the joint location used in the skeleton hierarchy during LBS deformation, only the regression matrices J i in Eq. (12).", "publication_ref": ["b32"], "figure_ref": ["fig_1", "fig_1"], "table_ref": []}, {"heading": "Fitting Clothes and Hair", "text": "The SMPL model captures the shape variability of human bodies, but does not account for clothing or hair. Similarly, the FaceWarehouse template mesh was not design to model hair. However, for the natural interactions that we are most interested in capturing, people wear everyday clothing and sport typical hairstyles. To learn a new set of linear blendshapes that better capture the rough geometry of clothed people and jointly model face, it is required to reconstruct the accurate geometry of the source data. For this purpose, we reconstruct the out-of-shape spaces in the reconstructed 70 people results by Frankenstein model fitting.\nFor each vertex in the Frankenstein model, we writ\u1ebd\nv i = v i + n(v i )\u03b4 i ,(15)\nwhere \u03b4 i \u2208 R is a scalar displacement meant to compensate for the discrepancy between the Frankenstein model vertices and the 3D point cloud, along the normal direction at each vertex. We pose the problem as a linear system,\nN T (WLN) T \u2206 = (P \u2212 V U ) T 0 ,(16)\nwhere \u2206 \u2208 R N U contains the stacked per-vertex displacements, V U are the vertices in the Frankenstein model, P \u2208 R N U \u00d73 are corresponding point cloud points, N \u2208 R N U \u00d73 contains the mesh vertex normals, and L \u2208 R N U \u00d7N U is the Laplace-Beltrami operator to regularize the deformation. We also use a weight matrix W to avoid large deformations where the 3D point cloud has lower resolution than the original mesh, like details in the face and hands.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Building the Shape Deformation Space", "text": "After \u2206 fitting, we warp each frame's surface to the rest pose, applying the inverse of the LBS transform. With the fitted surfaces warped to this canonical pose, we do PCA analysis to build a joint linear shape space that captures shape variations across the entire body. As in Section 3.3, we separate the expression basis for the face and retain the expression basis from the FaceWarehouse model, as our MVS point clouds are of too low resolution to fit facial expressions.\nThis model now can have shape variation for all parts, including body, hand, and face. The model also includes deformation of hair and clothing. That is this model can substitute parameters of \u03c6 F , \u03c6 B , and \u03c6 H .\nM T (\u03b8 T , \u03c6 T , t T ) = V T (17\n)\nwith V T = {v T i } N T i=1 and N T =18540. As in SMPL, the vertices of this template mesh are first displaced by a set of blendshapes in the rest pose,v\nT i = v T 0 i + K T k=1 s k i \u03c6 B k , where s k i \u2208 R 3 is the i-th vertex of the k-th blendshape, \u03c6 T k\nis the k-th shape coefficients of \u03c6 T \u2208 R K b , and K T = 40 is the number of identity coefficients, v T 0 is the mean shape and v T 0 is its i-th vertex. However, these blendshapes now capture variation across the face, hands, and body. These are then posed using LBS as in Eq. (6). We define the joints and weights for LBS followoing the part models, which is further explained in the supplementary material.", "publication_ref": ["b5"], "figure_ref": [], "table_ref": []}, {"heading": "Tracking with Adam", "text": "The cost function to capture total body motion using Adam model is similar to Eqn. 11 without the seam term:  However, Adam is much easier to use than Frankenstein, because it only has a single type of shapes and pose parameters for all parts. Conceptually, it is based on the SMPL model parameterization, but with additional joints for the hands and facial expression blendshapes. Optical Flow Propagation: While fitting each frame independently has benefits--it does not suffer from error accumulation and frames can be fit in parallel-it typically produces jittery motion. To reduce this jitter, we use optical flow to propagate the initial, per-frame fit to neighboring frames to find a smoother solution. More concretely, given the fitting results at the frame t, we propagate this mesh to frames t\u22121 and t+1 using optical flow at each vertex, which is triangulated into 3D using the method of [27]. Therefore, each vertex has at most three candidate positions: the original mesh, and the forward and backward propagated vertices (subject to a forward-backward consistency check). Given these propagated meshes, we reoptimize the model parameters by using all propagated mesh vertices as additional keypoints to find a compromise mesh. We run this process multiple times (3, in our case), to further reduce jitter and fill in frames with missing detections.\nE \u03b8 T , \u03c6 T , t T = E keypoints + E icp + E prior . (18\n)", "publication_ref": ["b26"], "figure_ref": [], "table_ref": []}, {"heading": "Results", "text": "We perform total motion capture using our two models, Frankenstein and Adam, on various challenging sequences. Figure 6: Total body reconstruction results on various human body motions. For each example scene, the fitting results from three different models are shown by different colors (pink for SMPL [33], silver for Frankenstein, and gold for Adam).\nFor experiments, we use the dataset captured in the CMU Panoptic Studio [26]. We use 140 VGA cameras to reconstruct 3D body keypoints, 480 VGA cameras for feet, and 31 HD cameras for faces and hands keypoints, and 3D point clouds. We compare the fits produced by our models with the body-only SMPL model [33].", "publication_ref": ["b32", "b25", "b32"], "figure_ref": [], "table_ref": []}, {"heading": "Quantitative Evaluation", "text": "We evaluate how well each model can match a moving person by measuring overlap with the ground truth silhouette across 5 different viewpoints for a 10 second range of motion sequence. To obtain the ground truth silhouette, we run a background subtraction algorithm using a Gaussian model for the background of each pixel with a postprocessing to remove noise by morphological transforms. As an evaluation metric, we compute the percentage of overlapping region compared to the union between the GT silhouettes and the rendered forground masks after fitting each model. Here, we compare the fitting results of 3 different models: SMPL, our Frankenstein, and our Adam models. An example result is shown in Figure 5, and the results are shown in Fig 5 and Table 1. We first compare accuracy between SMPL and Frankenstein model by using only 3D keypoints as measurement cues. The major source of im-provement of Frankenstein over SMPL is in the articulated hand model (by construction, the body is almost identical), as seen in Fig. 5 (a). Including ICP term as cues provides better accuracy. Finally in the comparison between our two models, they show almost similar performance. Ideally we expect the Adam outperforms Frankenstein because it has more expressive power for hair and clothing, and it shows it shows better performance in a certain body shape (frame 50-75 in Fig 5). However, Adam sometimes produces artifacts showing lower accuracy; it tends to generate thinner legs, mainly due to poor 3D point cloud reconstructions on the source data on which Adam is trained. However, Adam is simpler for total body motion capture purpose and has potential to be improved once a large scale dataset is available with more optimized capture setup.", "publication_ref": [], "figure_ref": ["fig_2", "fig_2", "fig_2"], "table_ref": []}, {"heading": "Qualitative Results", "text": "We run our method on sequences where face and hand motions are naturally emerging with body motions. The sequences include short range of motions for 70 people used to build Adam, social communications of multiple people, a furniture building sequence with dexterous hand motions, musical performances such as cello and guitars, and commonly observable daily motions such as keyboard typing.\nMost of these sequences are rarely demonstrated in previous markerless motion capture methods since capturing subtle details are the key to achieve the goal. The example results are shown in Figure 6. Here, we also qualitatively compare our models (in silver color for Frankenstein, and gold for Adam) with the SMPL model (in pink) [33]. It should be noted that the total body motion capture results based on our models produce much better realism for the scene by capturing the subtle details from hands and faces. Our results are best shown in the accompanying videos.", "publication_ref": ["b32"], "figure_ref": [], "table_ref": []}, {"heading": "Discussion", "text": "We present the first markerless method to capture total body motion including facial expression, coarse body motion from torso and limbs, and hand gestures at a distance. To achieve this, we present two types of models which can express motion in each of the parts. Our reconstruction results show compelling and realistic results, even when using only sparse 3D keypoint detections to drive the models.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "", "authors": "S Agarwal; K Mierle; Others Ceres"}, {"ref_id": "b1", "title": "Scape: shape completion and animation of people", "journal": "", "year": "2005", "authors": "D Anguelov; P Srinivasan; D Koller; S Thrun; J Rodgers; J Davis"}, {"ref_id": "b2", "title": "A data-driven approach for real-time full body pose reconstruction from a depth camera", "journal": "Springer", "year": "2013", "authors": "A Baak; M M\u00fcller; G Bharaj; H.-P Seidel; C Theobalt"}, {"ref_id": "b3", "title": "Motion capture of hands in action using discriminative salient points", "journal": "", "year": "2012", "authors": "L Ballan; A Taneja; J Gall; L Van Gool; M Pollefeys"}, {"ref_id": "b4", "title": "High-quality single-shot capture of facial geometry", "journal": "", "year": "2010", "authors": "T Beeler; B Bickel; P Beardsley; B Sumner; M Gross"}, {"ref_id": "b5", "title": "High-quality passive facial performance capture using anchor frames", "journal": "", "year": "2011", "authors": "T Beeler; F Hahn; D Bradley; B Bickel; P Beardsley; C Gotsman; R Sumner; M Gross"}, {"ref_id": "b6", "title": "Kinesics and context: Essays on body motion communication", "journal": "University of Pennsylvania Press", "year": "1970", "authors": "R Birdwhistell"}, {"ref_id": "b7", "title": "Keep it SMPL: automatic estimation of 3d human pose and shape from a single image", "journal": "", "year": "2016", "authors": "F Bogo; A Kanazawa; C Lassner; P V Gehler; J Romero; M J Black"}, {"ref_id": "b8", "title": "High resolution passive facial performance capture", "journal": "", "year": "2010", "authors": "D Bradley; W Heidrich; T Popa; A Sheffer"}, {"ref_id": "b9", "title": "Twist based acquisition and tracking of animal and human kinematics", "journal": "", "year": "2004", "authors": "C Bregler; J Malik; K Pullen"}, {"ref_id": "b10", "title": "Combined region and motion-based 3D tracking of rigid and articulated objects", "journal": "", "year": "2010", "authors": "T Brox; B Rosenhahn; J Gall; D Cremers"}, {"ref_id": "b11", "title": "Real-time highfidelity facial performance capture", "journal": "", "year": "2015", "authors": "C Cao; D Bradley; K Zhou; T Beeler"}, {"ref_id": "b12", "title": "Facewarehouse: A 3d facial expression database for visual computing", "journal": "", "year": "2014", "authors": "C Cao; Y Weng; S Zhou; Y Tong; K Zhou"}, {"ref_id": "b13", "title": "Realtime multiperson 2d pose estimation using part affinity fields", "journal": "", "year": "2017", "authors": "Z Cao; T Simon; S.-E Wei; Y Sheikh"}, {"ref_id": "b14", "title": "Shape-fromsilhouette across time part i: Theory and algorithms", "journal": "", "year": "2005", "authors": "K M Cheung; S Baker; T Kanade"}, {"ref_id": "b15", "title": "Markerless Motion Capture through Visual Hull, Articulated ICP and Subject Specific Model Generation", "journal": "", "year": "2010", "authors": "S Corazza; L M\u00fcndermann; E Gambaretto; G Ferrigno; T P Andriacchi"}, {"ref_id": "b16", "title": "Performance capture from sparse multi-view video", "journal": "", "year": "2008", "authors": "E De Aguiar; C Stoll; C Theobalt; N Ahmed; H.-P Seidel; S Thrun"}, {"ref_id": "b17", "title": "", "journal": "Intraface. In FG", "year": "2015", "authors": "F De La Torre; W.-S Chu; X Xiong; F Vicente; X Ding; J F Cohn"}, {"ref_id": "b18", "title": "Efficient convnet-based marker-less motion capture in general scenes with a low number of cameras", "journal": "", "year": "2015", "authors": "A Elhayek; E Aguiar; A Jain; J Tompson; L Pishchulin; M Andriluka; C Bregler; B Schiele; C Theobalt"}, {"ref_id": "b19", "title": "Dense 3d motion capture from synchronized video streams", "journal": "", "year": "2008", "authors": "Y Furukawa; J Ponce"}, {"ref_id": "b20", "title": "Motion capture using joint skeleton tracking and surface estimation", "journal": "", "year": "2009", "authors": "J Gall; C Stoll; E De Aguiar; C Theobalt; B Rosenhahn; H.-P Seidel"}, {"ref_id": "b21", "title": "Reconstructing detailed dynamic face geometry from monocular video", "journal": "", "year": "2013", "authors": "P Garrido; L Valgaerts; C Wu; C Theobalt"}, {"ref_id": "b22", "title": "Tracking of humans in action: A 3-D model-based approach", "journal": "", "year": "1996", "authors": "D Gavrila; L Davis"}, {"ref_id": "b23", "title": "Multiview face capture using polarized spherical gradient illumination", "journal": "", "year": "2011", "authors": "A Ghosh; G Fyffe; B Tunwattanapong; J Busch; X Yu; P Debevec"}, {"ref_id": "b24", "title": "", "journal": "", "year": "", "authors": "G Hidalgo; Z Cao; T Simon; S.-E Wei; H Joo; Y Sheikh;  Openpose"}, {"ref_id": "b25", "title": "Panoptic studio: A massively multiview system for social motion capture", "journal": "", "year": "2015", "authors": "H Joo; H Liu; L Tan; L Gui; B Nabbe; I Matthews; T Kanade; S Nobuhara; Y Sheikh"}, {"ref_id": "b26", "title": "Map visibility estimation for large-scale dynamic 3d reconstruction", "journal": "", "year": "2014", "authors": "H Joo; H S Park; Y Sheikh"}, {"ref_id": "b27", "title": "Panoptic studio: A massively multiview system for social interaction capture", "journal": "", "year": "2017", "authors": "H Joo; T Simon; X Li; H Liu; L Tan; L Gui; S Banerjee; T Godisart; B Nabbe; I Matthews"}, {"ref_id": "b28", "title": "Markerless tracking of complex human motions from multiple views", "journal": "", "year": "2006", "authors": "R Kehl; L V Gool"}, {"ref_id": "b29", "title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "journal": "", "year": "2012", "authors": "C Keskin; F K\u0131ra\u00e7; Y E Kara; L Akarun"}, {"ref_id": "b30", "title": "Realtime facial animation with on-the-fly correctives", "journal": "", "year": "2013", "authors": "H Li; J Yu; Y Ye; C Bregler"}, {"ref_id": "b31", "title": "Markerless motion capture of multiple characters using multiview image segmentation", "journal": "", "year": "2013", "authors": "Y Liu; J Gall; C Stoll; Q Dai; H.-P Seidel; C Theobalt"}, {"ref_id": "b32", "title": "Smpl: A skinned multi-person linear model", "journal": "", "year": "2015", "authors": "M Loper; N Mahmood; J Romero; G Pons-Moll; M J Black"}, {"ref_id": "b33", "title": "Vnect: Real-time 3d human pose estimation with a single RGB camera", "journal": "", "year": "2017", "authors": "D Mehta; S Sridhar; O Sotnychenko; H Rhodin; M Shafiei; H Seidel; W Xu; D Casas; C Theobalt"}, {"ref_id": "b34", "title": "Stacked hourglass networks for human pose estimation", "journal": "", "year": "2016", "authors": "A Newell; K Yang; J Deng"}, {"ref_id": "b35", "title": "Tracking the articulated motion of two strongly interacting hands", "journal": "", "year": "2012", "authors": "I Oikonomidis; N Kyriazis; A A Argyros"}, {"ref_id": "b36", "title": "Dyna: A model of dynamic human shape in motion", "journal": "", "year": "2015", "authors": "G Pons-Moll; J Romero; N Mahmood; M J Black"}, {"ref_id": "b37", "title": "Embodied hands: Modeling and capturing hands and bodies together", "journal": "", "year": "2017", "authors": "J Romero; D Tzionas; M J Black"}, {"ref_id": "b38", "title": "", "journal": "", "year": "", "authors": "T Sharp; C Keskin; D Robertson; J Taylor; J Shotton; D Kim; C Rhemann; I Leichter; A Vinnikov; Y Wei"}, {"ref_id": "b39", "title": "Accurate, robust, and flexible real-time hand tracking", "journal": "", "year": "2015", "authors": ""}, {"ref_id": "b40", "title": "Real-time human pose recognition in parts from single depth images", "journal": "", "year": "2011", "authors": "J Shotton; A Fitzgibbon; M Cook; T Sharp"}, {"ref_id": "b41", "title": "Hand keypoint detection in single images using multiview bootstrapping", "journal": "", "year": "2017", "authors": "T Simon; H Joo; I Matthews; Y Sheikh"}, {"ref_id": "b42", "title": "Fast and robust hand tracking using detection-guided optimization", "journal": "", "year": "2015", "authors": "S Sridhar; F Mueller; A Oulasvirta; C Theobalt"}, {"ref_id": "b43", "title": "Interactive markerless articulated hand motion tracking using RGB and depth data", "journal": "", "year": "2013", "authors": "S Sridhar; A Oulasvirta; C Theobalt"}, {"ref_id": "b44", "title": "Fast articulated motion tracking using a sums of gaussians body model", "journal": "", "year": "2011", "authors": "C Stoll; N Hasler; J Gall; H.-P Seidel; C Theobalt"}, {"ref_id": "b45", "title": "Cascaded hand pose regression", "journal": "", "year": "2015", "authors": "X Sun; Y Wei; S Liang; X Tang; J Sun"}, {"ref_id": "b46", "title": "Latent regression forest: Structured estimation of 3D articulated hand posture", "journal": "", "year": "2014", "authors": "D Tang; H Chang; A Tejani; T.-K Kim"}, {"ref_id": "b47", "title": "Face2face: Real-time face capture and reenactment of rgb videos", "journal": "", "year": "2016", "authors": "J Thies; M Zollhofer; M Stamminger; C Theobalt; M Nie\u00dfner"}, {"ref_id": "b48", "title": "Lifting from the deep: Convolutional 3d pose estimation from a single image", "journal": "", "year": "2017", "authors": "D Tome; C Russell; L Agapito"}, {"ref_id": "b49", "title": "Joint training of a convolutional network and a graphical model for human pose estimation", "journal": "", "year": "2014", "authors": "J J Tompson; A Jain; Y Lecun; C Bregler"}, {"ref_id": "b50", "title": "Capturing hands in action using discriminative salient points and physics simulation", "journal": "", "year": "2016", "authors": "D Tzionas; L Ballan; A Srikantha; P Aponte; M Pollefeys; J Gall"}, {"ref_id": "b51", "title": "Lightweight binocular facial performance capture under uncontrolled lighting", "journal": "", "year": "2012", "authors": "L Valgaerts; C Wu; A Bruhn; H.-P Seidel; C Theobalt"}, {"ref_id": "b52", "title": "Articulated mesh animation from multi-view silhouettes", "journal": "", "year": "2008", "authors": "D Vlasic; I Baran; W Matusik; J Popovi\u0107"}, {"ref_id": "b53", "title": "Direction matters: hand pose estimation from local surface normals", "journal": "", "year": "2016", "authors": "C Wan; A Yao; L Van Gool"}, {"ref_id": "b54", "title": "Convolutional pose machines", "journal": "", "year": "2016", "authors": "S.-E Wei; V Ramakrishna; T Kanade; Y Sheikh"}, {"ref_id": "b55", "title": "New possibilities for human motion studies by real-time light spot position measurement", "journal": "", "year": "1973", "authors": "H Woltring"}, {"ref_id": "b56", "title": "An anatomically-constrained local deformation model for monocular face capture", "journal": "", "year": "2016", "authors": "C Wu; D Bradley; M Gross; T Beeler"}, {"ref_id": "b57", "title": "Efficient hand pose estimation from a single depth image", "journal": "", "year": "2013", "authors": "C Xu; L Cheng"}, {"ref_id": "b58", "title": "Spatial attention deep net with partial pso for hierarchical hybrid hand pose estimation", "journal": "", "year": "2016", "authors": "Q Ye; S Yuan; T.-K Kim"}, {"ref_id": "b59", "title": "Combining marker-based mocap and rgb-d camera for acquiring high-fidelity hand motion data", "journal": "", "year": "2012", "authors": "W Zhao; J Chai; Y.-Q Xu"}, {"ref_id": "b60", "title": "3d shape estimation from 2d landmarks: A convex relaxation approach", "journal": "", "year": "2015", "authors": "X Zhou; S Leonardos; X Hu; K Daniilidis"}, {"ref_id": "b61", "title": "Deep kinematic pose regression", "journal": "", "year": "2016", "authors": "X Zhou; X Sun; W Zhang; S Liang; Y Wei"}], "figures": [{"figure_label": "2", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Part models and a unified Frankenstein model. (a)The body model[33]; (b) the face model[13]; and (c) a hand rig, where red dots have corresponding 3D keypoints reconstructed from detectors in (a-c). (d) Aligned face and hand models (gray meshes) to the body model (the blue wireframe mesh); and (e) the seamless Frankenstein model.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure 4: Regressing detection target positions. (Left) The template model is aligned with target object. (Mid.) The torso joints of the template model (magenta) have discrepancy from the joint definitions of 3D keypoint detection (cyan). (Right) The newly regressed target locations (green) are more consistent with 3D keypoint detections.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 5 :5Figure 5: (Top) Visualization of silhouette from different methods with Ground-truth. The ground truth is drawn on red channel and the rendered silhouette masks from each model is drawn on green channel. Thus, the correctly overlapped region is shown as yellow color.; (Bottom) Silhouette accuracy compared to the ground truth silhouette.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Accuracy of Silhouettes from different models", "figure_data": "SMPL[33]FrankenFranken ICPAdam ICPMean84.79%85.91%87.68%87.74%Std.4.554.574.534.18"}], "formulas": [{"formula_id": "formula_0", "formula_text": "V U = M U (\u03b8 U , \u03c6 U , t U ),(1)", "formula_coordinates": [3.0, 116.71, 556.21, 169.66, 11.37]}, {"formula_id": "formula_1", "formula_text": "\u03b8 U = {\u03b8 B , \u03b8 F , \u03b8 LH , \u03b8 RH },(2)", "formula_coordinates": [3.0, 113.37, 638.52, 172.99, 11.37]}, {"formula_id": "formula_2", "formula_text": "\u03c6 U = {\u03c6 B , \u03c6 F , \u03c6 LH , \u03c6 RH },(3)", "formula_coordinates": [3.0, 107.46, 655.31, 178.9, 11.37]}, {"formula_id": "formula_3", "formula_text": "V B \u2208 R N B \u00d73 , V F \u2208 R N F \u00d73 , V LH \u2208 R N H \u00d73 , and V RH \u2208 R N H \u00d73", "formula_coordinates": [3.0, 308.86, 97.07, 236.25, 25.69]}, {"formula_id": "formula_4", "formula_text": "V U = C V B T V F T V LH T V RH T T ,(4)", "formula_coordinates": [3.0, 315.79, 182.94, 229.33, 16.79]}, {"formula_id": "formula_5", "formula_text": "V B = M B (\u03b8 B , \u03c6 B , t B ) (5", "formula_coordinates": [3.0, 376.54, 553.97, 164.7, 11.37]}, {"formula_id": "formula_6", "formula_text": ")", "formula_coordinates": [3.0, 541.24, 556.7, 3.87, 8.64]}, {"formula_id": "formula_7", "formula_text": "with V B = {v B i } N B i=1 .", "formula_coordinates": [3.0, 308.86, 579.05, 93.78, 13.97]}, {"formula_id": "formula_8", "formula_text": "v B i = I 3\u00d74 \u2022 J B j=1 w B i,j T B j v B0 i + K b k=1 b k i \u03c6 B k 1 ,(6)", "formula_coordinates": [3.0, 320.61, 678.03, 224.5, 31.97]}, {"formula_id": "formula_9", "formula_text": "V F = M F (\u03b8 F , \u03c6 F , T F ),(7)", "formula_coordinates": [4.0, 115.07, 343.88, 171.29, 11.37]}, {"formula_id": "formula_10", "formula_text": "with V F = {v F i } N F i=1", "formula_coordinates": [4.0, 50.11, 368.51, 88.35, 13.97]}, {"formula_id": "formula_11", "formula_text": "v F i = v F 0 i + K f k=1 f k i \u03c6 F k + Ke s=1 e s i \u03b8 F s (8)", "formula_coordinates": [4.0, 96.94, 417.43, 189.43, 31.69]}, {"formula_id": "formula_12", "formula_text": "v F i = I 3\u00d74 \u2022 T B j=F \u2022 \u0393 F v F i 1 ,(9)", "formula_coordinates": [4.0, 102.73, 687.08, 183.64, 22.27]}, {"formula_id": "formula_13", "formula_text": "v i = I 3\u00d74 \u2022 T B j=H \u2022 \u0393 H \u2022 J j=1 w H i,j T H j v 0 i 1 . (10", "formula_coordinates": [4.0, 327.41, 314.48, 213.55, 30.32]}, {"formula_id": "formula_14", "formula_text": ")", "formula_coordinates": [4.0, 540.96, 325.21, 4.15, 8.64]}, {"formula_id": "formula_15", "formula_text": "E \u03b8 U , \u03c6 U , t U = E keypoints + E icp + E seam + E prior", "formula_coordinates": [5.0, 57.01, 602.03, 205.36, 11.88]}, {"formula_id": "formula_16", "formula_text": "E keypoints = \u03bb keypoints i\u2208D ||J i V \u2212 y T i || 2 ,(12)", "formula_coordinates": [5.0, 346.84, 144.54, 198.27, 22.13]}, {"formula_id": "formula_17", "formula_text": "i * = arg min i ||x i \u2212 v j || 2 ,(13)", "formula_coordinates": [5.0, 372.67, 306.5, 172.44, 11.72]}, {"formula_id": "formula_18", "formula_text": "E icp = \u03bb icp vj \u2208V U t n(x i * ) T (x i * \u2212 v j ),(14)", "formula_coordinates": [5.0, 349.66, 413.0, 195.46, 24.81]}, {"formula_id": "formula_19", "formula_text": "E prior = E F prior + E B prior + E H prior .", "formula_coordinates": [6.0, 50.11, 317.47, 129.44, 12.62]}, {"formula_id": "formula_20", "formula_text": "v i = v i + n(v i )\u03b4 i ,(15)", "formula_coordinates": [7.0, 129.15, 94.96, 157.21, 9.68]}, {"formula_id": "formula_21", "formula_text": "N T (WLN) T \u2206 = (P \u2212 V U ) T 0 ,(16)", "formula_coordinates": [7.0, 101.44, 168.52, 184.92, 22.28]}, {"formula_id": "formula_22", "formula_text": "M T (\u03b8 T , \u03c6 T , t T ) = V T (17", "formula_coordinates": [7.0, 120.14, 487.69, 162.07, 11.03]}, {"formula_id": "formula_23", "formula_text": ")", "formula_coordinates": [7.0, 282.21, 490.09, 4.15, 8.64]}, {"formula_id": "formula_24", "formula_text": "T i = v T 0 i + K T k=1 s k i \u03c6 B k , where s k i \u2208 R 3 is the i-th vertex of the k-th blendshape, \u03c6 T k", "formula_coordinates": [7.0, 50.11, 533.19, 236.25, 25.9]}, {"formula_id": "formula_25", "formula_text": "E \u03b8 T , \u03c6 T , t T = E keypoints + E icp + E prior . (18", "formula_coordinates": [7.0, 73.4, 698.54, 208.81, 11.88]}, {"formula_id": "formula_26", "formula_text": ")", "formula_coordinates": [7.0, 282.21, 700.93, 4.15, 8.64]}], "doi": ""}