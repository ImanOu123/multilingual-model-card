{"title": "Network Density of States", "authors": "Kun Dong; Austin R Benson; David Bindel", "pub_date": "2019-05-23", "abstract": "Spectral analysis connects graph structure to the eigenvalues and eigenvectors of associated matrices. Much of spectral graph theory descends directly from spectral geometry, the study of differentiable manifolds through the spectra of associated differential operators. But the translation from spectral geometry to spectral graph theory has largely focused on results involving only a few extreme eigenvalues and their associated eigenvalues. Unlike in geometry, the study of graphs through the overall distribution of eigenvalues -the spectral density -is largely limited to simple random graph models. The interior of the spectrum of real-world graphs remains largely unexplored, difficult to compute and to interpret. In this paper, we delve into the heart of spectral densities of real-world graphs. We borrow tools developed in condensed matter physics, and add novel adaptations to handle the spectral signatures of common graph motifs. The resulting methods are highly efficient, as we illustrate by computing spectral densities for graphs with over a billion edges on a single compute node. Beyond providing visually compelling fingerprints of graphs, we show how the estimation of spectral densities facilitates the computation of many common centrality measures, and use spectral densities to estimate meaningful information about graph structure that cannot be inferred from the extremal eigenpairs alone.", "sections": [{"heading": "INTRODUCTION", "text": "Spectral theory is a powerful analysis tool in graph theory [9,10,13], geometry [6], and physics [27]. One follows the same steps in each setting:\n\u2022 Identify an object of interest, such as a graph or manifold;\n\u2022 Associate the object with a matrix or operator, often the generator of a linear dynamical system or the Hessian of a quadratic form over functions on the object; and \u2022 Connect spectral properties of the matrix or operator to structural properties of the original object.\nIn each case, the complete spectral decomposition is enough to recover the original object; the interesting results relate structure to partial spectral information.\nMany spectral methods use extreme eigenvalues and associated eigenvectors. These are easy to compute by standard methods, and are easy to interpret in terms of the asymptotic behavior of dynamical systems or the solutions to quadratic optimization problems with quadratic constraints. Several network centrality measures, such as PageRank [40], are expressed via the stationary vectors of transition matrices, and the rate of convergence to stationarity is bounded via the second-largest eigenvalue. In geometry and graph theory, Cheeger's inequality relates the second-smallest eigenvalue of a Laplacian or Laplace-Beltrami operator to the size of the smallest bisecting cut [7,37]; in the graph setting, the associated eigenvector (the Fiedler vector) is the basis for spectral algorithms for graph partitioning [42]. Spectral algorithms for graph coordinates and clustering use the first few eigenvectors of a transition matrix or (normalized) adjacency or Laplacian [5,39]. For a survey of such approaches in network science, we refer to [9]. Mark Kac popularized an alternate approach to spectral analysis in an expository article [28] in which he asked whether one can determine the shape of a physical object (Kac used a drum as an example) given the spectrum of the Laplace operator; that is, can one \"hear\" the shape of a drum? One can ask a similar question in graph theory: can one uniquely determine the structure of a network from the spectrum of the Laplacian or another related matrix? Though the answer is negative in both cases [13,22], the spectrum is enormously informative even without eigenvector information. Unlike the extreme eigenvalues and vectors, eigenvalues deep in the spectrum are difficult to compute and to interpret, but the overall distribution of eigenvalues -known as the spectral density or density of states -provides valuable structural information. For example, knowing the spectrum of a graph adjacency matrix is equivalent to knowing trace(A k ), the number of closed walks of any given length k. In some cases, one wants local spectral densities in which the eigenvalues also have positive weights associated with a location. Following Kac, this would give us not only the frequencies of a drum, but also amplitudes based on where the drum is struck. In a graph setting, the local spectral density of an adjacency matrix at node j is equivalent to knowing (A k ) j j , the number of closed walks of any given length k that begin and end at the node.\nUnfortunately, the analysis of spectral densities of networks has been limited by a lack of scalable algorithms. While the normalized Laplacian spectra of Erd\u0151s-R\u00e9nyi random graphs have an approximately semicircular distribution [51], and the spectral distributions for other popular scale-free and small-world random graph models are also known [19], there has been relatively little work on computing spectral densities of large \"real-world\" networks. Obtaining the full eigendecomposition is O(N 3 ) for a graph with N nodes, which is prohibitive for graphs of more than a few thousand nodes. In prior work, researchers have employed methods, such as thickrestart Lanczos, that still do not scale to very large graphs [19], or heuristic approximations with no convergence analysis [2]. It is only recently that clever computational methods were developed simply to test for hypothesized power laws in the spectra of large real-world matrices by computing only part of the spectrum [18].\nIn this paper, we show how methods used to study densities of states in condensed matter physics [49] can be used to study spectral densities in networks. We study these methods for both the global density of states and for local densities of states weighted by specific eigenvector components. We adapt these methods to take advantage of graph-specific structure not present in most physical systems, and analyze the stability of the spectral density to perturbations as well as the convergence of our computational methods. Our methods are remarkably efficient, as we illustrate by computing densities for graphs with billions of edges and tens of millions of nodes on a single cloud compute node. We use our methods for computing these densities to create compelling visual fingerprints that summarize a graph. We also show how the density of states reveals graph properties that are not evident from the extremal eigenvalues and eigenvectors alone, and use it as a tool for fast computation of standard measures of graph connectivity and node centrality. This opens the door for the use of complete spectral information as a tool in large-scale network analysis.", "publication_ref": ["b8", "b9", "b12", "b5", "b25", "b38", "b6", "b35", "b40", "b4", "b37", "b8", "b26", "b12", "b20", "b49", "b17", "b17", "b1", "b16", "b47"], "figure_ref": [], "table_ref": []}, {"heading": "BACKGROUND 2.1 Graph Operators and Eigenvalues", "text": "We consider weighted, undirected graphs G = (V , E) with vertices V = {v 1 , \u2022 \u2022 \u2022 , v N } and edges E \u2286 V \u00d7 V . The weighted adjacency matrix A \u2208 R N \u00d7N has entries a i j > 0 to give the weight of an edge (i, j) \u2208 E and a i j = 0 otherwise. The degree matrix D \u2208 R N \u00d7N is the diagonal matrix of weighted node degrees, i.e. D ii = j a i j . Several of the matrices in spectral graph theory are defined in terms of D and A. We describe a few of these below, along with their connections to other research areas. For each operator, we let \u03bb 1 \u2264 . . . \u2264 \u03bb N denotes the eigenvalues in ascending order. Adjacency Matrix: A A A. Many studies on the spectrum of A originate from random matrix theory where A represents a random graph model. In these cases, the limiting behavior of eigenvalues as N \u2192 \u221e is of particular interest. Besides the growth of extremal eigenvalues [10], Wigner's semicircular law is the most renowned result about the spectral distribution of the adjacency matrix [51]. When the edges are i.i.d. random variables with bounded moments, the density of eigenvalues within a range converges to a semicircular distribution. One famous graph model of this type is the Erd\u0151s-R\u00e9nyi graph, where a i j = a ji = 1 with probability p < 1, and 0 with probability 1 \u2212 p. Farkas et al. [19] has extended the semicircular law by investigating the spectrum of scale-free and small-world random graph models. They show the spectra of these random graph models relate to geometric characteristics such as the number of cycles and the degree distribution.\nLaplacian Matrix: L = D \u2212 A L = D \u2212 A L = D \u2212 A.\nThe Laplace operator arises naturally from the study of dynamics in both spectral geometry and spectral graph theory. The continuous Laplace operator and its generalizations are central to the description of physical systems including heat diffusion [34], wave propagation [32], and quantum mechanics [17]. It has infinitely many non-negative eigenvalues, and Weyl's law [50] relates their asymptotic distribution to the volume and dimension of the manifold. On the other hand, the discrete Laplace matrix appears in the formulation of graph partitioning problems. If f \u2208 {\u00b11} N is an indicator vector for a partition V = V + \u222a V \u2212 , then f T Lf /4 is the number of edges between V + and V \u2212 , also known as the cut size. L is a positive-semidefinite matrix with the vector of all ones as a null vector. The eigenvalue \u03bb 2 , called the algebraic connectivity, bounds from below the smallest bisecting cut size; \u03bb 2 = 0 if and only if the graph is disconnected. In addition, eigenvalues of L also appear in bounds for vertex connectivity (\u03bb 2 ) [12], minimal bisection (\u03bb 2 ) [15], and maximum cut (\u03bb N ) [47].\nNormalized Laplacian Matrix:\nL = I \u2212 D \u22121/2 AD \u22121/2 L = I \u2212 D \u22121/2 AD \u22121/2 L = I \u2212 D \u22121/2 AD \u22121/2 .\nWe will also mention the normalized adjacency matrix A = D \u22121/2 AD \u22121/2 and graph random walk matrix P = D \u22121 A here, because these matrices have the same eigenvalues asL up to a shift. The connection to some of the most influential results in spectral geometry is established in terms of eigenvalues and eigenvectors of normalized Laplacian. A prominent example is the extension of Cheeger's inequality to the discrete case, which relates the set of smallest conductance h(G) (the Cheeger constant) to the second smallest eigenvalue of the normalized Laplacian, \u03bb 2 (L) [38]:\n\u03bb 2 (L)/2 \u2264 h(G) = min S \u2282V |{(i, j) \u2208 E, i \u2208 S, j S }| min(vol(S), vol(V \\S)) \u2264 2\u03bb 2 (L),\nwhere vol(T ) = i \u2208T N j=1 a i j . Cheeger's inequality offers crucial insights and powerful techniques for understanding popular spectral graph algorithms for partitioning [35] and clustering [39]. It also plays a key role in analyzing the mixing time of Markov chains and random walks on a graph [36,44]. For all these problems, extremal eigenvalues again emerge from relevant optimization formulations.", "publication_ref": ["b9", "b49", "b17", "b32", "b30", "b15", "b48", "b11", "b14", "b45", "b36", "b33", "b37", "b34", "b42"], "figure_ref": [], "table_ref": []}, {"heading": "Spectral Density (Density of States -DOS)", "text": "Let H = R N \u00d7N be any symmetric graph matrix with an eigendecomposition H = Q\u039bQ T , where\n\u039b = diag(\u03bb 1 , \u2022 \u2022 \u2022 , \u03bb N ) and Q = [q 1 , \u2022 \u2022 \u2022 , q N ] is orthogonal. The spectral density induced by H is the generalized function \u00b5(\u03bb) = 1 N N i=1 \u03b4 (\u03bb \u2212 \u03bb i ), \u222b f (\u03bb)\u00b5(\u03bb) = trace(f (H ))(1)\nwhere \u03b4 is the Dirac delta function and f is any analytic test function. The spectral density \u00b5 is also referred to as the density of states (DOS) in the condensed matter physics literature [49], as it describes the number of states at different energy levels. For any vector u \u2208 R N , the local density of states (LDOS) is\n\u00b5(\u03bb; u) = N i=1 |u T q i | 2 \u03b4 (\u03bb \u2212 \u03bb i ), \u222b f (\u03bb)\u00b5(\u03bb; u) = u T f (H )u. (2)\nMost of the time, we are interested in the case u = e k where e k is the kth standard basis vector-this provides the spectral information about a particular node. We will write \u00b5 k (\u03bb) = \u00b5(\u03bb; e k ) for the pointwise density of states (PDOS) for node v k . It is noteworthy\n|e T k q i | = |q i (k)|\ngives the magnitude of the weight for v k in the i-th eigenvector, thereby the set of {\u00b5 k } encodes the entire spectral information of the graph up to sign differences. These concepts can  be easily extended to directed graphs with asymmetric matrices, for which the eigenvalues are replaced by singular values, and eigenvectors by left/right singular vectors.\nNaively, to obtain the DOS and LDOS requires computing all eigenvalues and eigenvectors for an N -by-N matrix, which is infeasible for large graphs. Therefore, we turn to algorithms that approximate these densities. Since the DOS is a generalized function, it is important we specify how the estimation is evaluated. One choice is to treat \u00b5 (or \u00b5 k ) as a distribution, and measure its approximation error with respect to a chosen function space L. For example, when L is the set of Lipschitz continuous functions taking the value 0 at 0, the error for estimated \u00b5 is in the Wasserstein distance (a.k.a. earth-mover distance) [29] \nW 1 (\u00b5, \u00b5) = sup \u222b (\u00b5(\u03bb) \u2212 \u00b5(\u03bb))f (\u03bb)d\u03bb : Lip(f ) \u2264 1 . (3\n)\nThis notion is particularly useful when \u00b5 is integrated against in applications such as computing centrality measures.\nOn the other hand, we can regularize \u00b5 with a mollifier K \u03c3 (i.e., a smooth approximation of the identity function):\n(K \u03c3 * \u00b5)(\u03bb) = \u222b R \u03c3 \u22121 K \u03bb \u2212 \u03bd \u03c3 \u00b5(\u03bd )d\u03bd(4)\nA simplified approach is numerically integrating \u00b5 over small intervals of equal size to generate a spectral histogram. The advantage is the error is now easily measured and visualized in the L \u221e norm. For example, Figure 1 shows the exact and approximated spectral histogram for the normalized adjacency matrix of an Internet topology.", "publication_ref": ["b47", "b27"], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "METHODS", "text": "The density of states plays a significant role in understanding electronic band structure in solid state physics, and so several methods have been proposed in that literature to estimate spectral densities.\nWe review two such methods: the kernel polynomial method (KPM) which involves a polynomial expansion of the DOS/LDOS, and the Gauss Quadrature via Lanczos iteration (GQL). These methods have not previously been applied in the network setting, though Cohen-Steiner et al. [11] have independently invented an approach similar to KPM for the global DOS alone, albeit using a less numerically stable polynomial basis (the power basis associated with random walks). We then introduce a new direct nested dissection method for LDOS, as well as new graph-specific modifications to improve the convergence of the KPM and GQL approaches. Throughout this section, H denotes any symmetric matrix.", "publication_ref": ["b10"], "figure_ref": [], "table_ref": []}, {"heading": "Kernel Polynomial Method (KPM)", "text": "The Kernel Polynomial Method (KPM) [49] approximates the spectral density through an expansion in the dual basis of an orthogonal polynomial basis. Traditionally, the Chebyshev basis {T m } is used because of its connection to the best polynomial interpolation. Chebyshev approximation requires the spectrum to be supported on the interval [\u22121, 1] for numerical stability. However, this condition can be satisfied by any graph matrix after shifting and rescaling:\nH = 2H \u2212 (\u03bb max (H ) + \u03bb min (H )) \u03bb max (H ) \u2212 \u03bb min (H )(5)\nWe can compute these extremal eigenvalues efficiently for our sparse matrix H , so the pre-computation is not an issue [41].\nThe Chebyshev polynomials T m (x) satisfy the recurrence\nT 0 (x) = 1, T 1 (x) = x, T m+1 (x) = 2xT m (x) \u2212 T m\u22121 (x).(6)\nThey are orthogonal with respect to w\n(x) = 2/[(1 + \u03b4 0n )\u03c0 \u221a 1 \u2212 x 2 ]: \u222b 1 \u22121 w(x)T m (x)T n (x)dx = \u03b4 mn .(7)\n(Here and elsewhere, \u03b4 i j is the Kronecker delta: 1 if i = j and 0 otherwise.) Therefore, T * m (x) = w(x)T m (x) also forms the dual Chebyshev basis. Using (7), we can expand our DOS \u00b5(\u03bb) as\n\u00b5(x) = \u221e m=1 d m T * m (\u03bb)(8)\nd m = \u222b 1 \u22121 T m (\u03bb)\u00b5(\u03bb)d\u03bb = 1 N N i=1 T m (\u03bb i ) = 1 N trace(T m (H )), (9)\nHere, T m (H ) is the mth Chebyshev polynomial of the matrix H . The last equality comes from the spectral mapping theorem, which says that taking a polynomial of H maps the eigenvalues by the same polynomial. Similarly, we express the PDOS \u00b5 k (\u03bb) as\nd mk = \u222b 1 \u22121 T m (\u03bb)\u00b5 k (\u03bb)d\u03bb = N i=1 |q i (k)| 2 T m (\u03bb i ) = T m (H ) kk . (10\n)\nWe want to efficiently extract the diagonal elements of the matrices {T m (H )} without forming them explicitly; the key idea is to apply the stochastic trace/diagonal estimation, proposed by Hutchinson [25] and Bekas et al. [4]. Given a random probe vector z such that z i 's are i.i.d. with mean 0 and variance 1,\nE[z T Hz] = i, j H i j E[z i z j ] = trace(H ) (11) E[z \u2299 Hz] = diag(H )(12)\nwhere \u2299 represents the Hadamard (elementwise) product. Choosing N z independent probe vectors Z j , we obtain the unbiased estimator\ntrace(H ) = E[z T Hz] \u2248 1 N z N z j=1\nZ T j HZ j and similarly for the diagonal. Avron and Toledo [1] review many possible choices of probes for eqs. ( 11) and ( 12); a common choice is vectors with independent standard normal entries. Using the Chebyshev recurrence (eq. ( 6)), we can compute the sequence T j (H )z for each probe at a cost of one matrix-vector product per term, for a total cost of O(|E|N z ) time per moment T m (H ).\nIn practice, we only use a finite number of moments rather than an infinite expansion. The number of moments required depends on the convergence rate of the Chebyshev approximation for the class of functions DOS/LDOS is integrated with. For example, the approximation error decays exponentially for test functions that are smooth over the spectrum [45], so only a few moments are needed. On the other hand, such truncation leads to Gibbs oscillations that cause error in the interpolation [46]. However, to a large extent, we can use smoothing techniques such as Jackson damping to resolve this issue [26] (we will formalize this in theorem 4.1).", "publication_ref": ["b47", "b39", "b23", "b3", "b0", "b43", "b44", "b24"], "figure_ref": [], "table_ref": []}, {"heading": "Gauss Quadrature and Lanczos (GQL)", "text": "Golub and Meurant developed the well-known Gauss Quadrature and Lanczos (GQL) algorithm to approximate bilinear forms for smooth functions of a matrix [21]. Using the same stochastic estimation from \u00a73.1, we can also apply GQL to compute DOS.\nFor a starting vector z and graph matrix H , Lanczos iterations after M steps produce a decomposition\nHZ M = Z T M \u0393 M + r M e T M\nwhere\nZ T M Z M = I M , Z T M r M = 0, and \u0393 M tridiagonal. GQL approxi- mates z T f (H )z with \u2225z \u2225 2 e T 1 f (T M )e 1 , implying z T f (H )z = N i=1 |z T q i | 2 f (\u03bb i ) \u2248 \u2225z \u2225 2 M i=1 |p i1 | 2 f (\u03c4 i )\nwhere\n(\u03c4 1 , p 1 ) \u2022 \u2022 \u2022 , (\u03c4 M , p M ) are the eigenpairs of \u0393 M . Consequently, \u2225z \u2225 2 M i=1 |p i1 | 2 \u03b4 (\u03bb \u2212 \u03c4 i )\napproximates the LDOS \u00b5(\u03bb; z). Building upon the stochastic estimation idea and the invariance of probe vectors under orthogonal transformation, we have\nE[\u00b5(\u03bb; z)] = N i=1 \u03b4 (\u03bb \u2212 \u03bb i ) = N \u00b5(\u03bb) Hence \u00b5(\u03bb) \u2248 M i=1 |p i1 | 2 \u03b4 (\u03bb \u2212 \u03c4 i ).\nThe approximate generalized function is exact when applied to polynomials of degree \u2264 2M + 1. Furthermore, if we let z = e k then GQL also provides an estimation for the PDOS \u00b5 k (\u03bb). Estimation from GQL can also be converted to Chebyshev moments if needed.", "publication_ref": ["b19"], "figure_ref": [], "table_ref": []}, {"heading": "Nested Dissection (ND)", "text": "The estimation error via Monte Carlo method intrinsically decays at the rate O(1/ \u221a N z ), where N z is the number of random probing vectors. Hence, we have to tolerate the higher variance when increasing the number of probe vectors becomes too expensive. This is particularly problematic when we try to compute the PDOS for all nodes using the stochastic diagonal estimator. Therefore, we introduce an alternative divide-and-conquer method, which computes more accurate PDOS for any set of nodes at a cost comparable to the stochastic approach in practice. Suppose the graph can be partitioned into two subgraphs by removal of a small vertex separator. Permuting the vertices so that the two partitions appear first, followed by the separator vertices. Up to vertex permutations, we can rewrite H in block form as\nH = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 H 11 0 H 13 0 H 22 H 23 H T 13 H T 23 H 33 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,\nwhere the indices indicate the groups identities. Leveraging this structure, we can update the recurrence relation for Chebyshev polynomials to become\nT m+1 (H ) 11 = 2H 11 T m (H ) 11 \u2212 T m\u22121 (H ) 11 + 2H 13 T m (H ) 31 (13)\nRecursing on the partitioning will lead to a nested dissection, after which we will use direct computation on sufficiently small sub-blocks. We denote the indexing of each partition with I (t )\np = I (t ) s I (t ) \u2113 I (t )\nr , which represents all nodes in the current partition, the separators, and two sub-partitions, respectively. For the separators, equation 13 leads to\nT m+1 (H )(I (t ) p , I (t ) s ) = 2H (I (t ) p , I (t ) p )T m (H )(I (t ) p , I (t ) s ) \u2212 T m\u22121 (H )(I (t ) p , I (t ) s ) + 2 t \u2032 \u2208S t H (I (t ) p , I (t \u2032 ) s )T m (H )(I (t \u2032 ) s , I (t ) s ) (14)\nwhere S t is the path from partition t to the root; and for the leaf blocks, I \nif I (t )\np is a leaf block then Compute the diagonal entries with equation (14). end end end\nThe multilevel nested dissection process itself has a well-established algorithm by Karypis and Kumar, and efficient implementation is available in METIS [30]. Note that this approach is only viable when the graph can be partitioned with a separator of small size. Empirically, we observe this assumption to hold for many real-world networks. The biggest advantage of this approach is we can very efficiently obtain PDOS estimation for a subset of nodes with much better accuracy than KPM.", "publication_ref": ["b13", "b28"], "figure_ref": [], "table_ref": []}, {"heading": "+1 +1 +1", "text": "\u22121 \u22121 \u22121 \u22121 +1 \u22121 (a) \u03bb = 0 +1 +1 +1 \u00b11 \u00b11 \u00b11 \u22121 \u22121 \u22121 \u22131 \u22131 \u22131 \u00b11 \u00b11 \u00b11 +1 +1 +1 \u22121 \u22121 \u22121 \u22131 \u22131 \u22131 (b) \u03bb = \u00b11/2 +1 +1 +1 \u22121 \u22121 \u22121 (c) \u03bb = \u22121/2 \u00b11 \u00b11 \u00b11 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 \u00b11 \u00b11 \u00b11 (d) \u03bb = \u00b11/ \u221a 2\nFigure 2: Common motifs (induced subgraphs) in graph data that result in localized spikes in the spectral density. Each motif generates a specific eigenvalue with locally-supported eigenvectors. Here we uses the normalized adjacency matrix to represent the graph, although we can perform the same analysis for the adjacency, Laplacian, or normalized Laplacian (only the eigenvalues would be different). The eigenvectors are supported only on the labeled nodes.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Motif Filtering", "text": "In many graphs, there are large spikes around particular eigenvalues; for example, see fig. 1. This phenomenon affects the accuracy of DOS estimation in two ways. First, the singularity-like behavior means we need many more moments to obtain a good approximation in polynomial basis. Secondly, due to the equi-oscillation property of Chebyshev approximation, error in irregularities (say, at a point of high concentration in the spectral density), spreads to other parts of the spectrum. This is a problem in our case, as the spectral density of real-world networks are far from uniform. High multiplicity eigenvalues are typically related to local symmetries in a graph. The most prevalent example is two dangling nodes attached to the same neighbor as shown in 2a, which accounts for most eigenvalues around 0 for (normalized) adjacency matrix with a localized eigenvector taking value +1 on one node and \u22121 on the other. In addition, we list a few more motifs in figure 2 that appear most frequently in real-world graphs. All of them can be associated with specific eigenvalues, and we include the corresponding ones in normalized adjacency matrix for our example.\nTo detect these motifs in large graphs, we deploy a randomized hashing technique. Given a random vector z, the hashing weight w = Hz encodes all the neighborhood information of each node.\nTo find node copies (left in Figure 2a), we seek pairs (i, j) such that w i = w j ; with high probability, this only happens when v i and v j share the same neighbors. Similarly, all motifs in Figure 2 can be characterized by union and intersection of neighborhood lists.\nAfter identifying motifs, we need only approximate the (relatively smooth) density of the remaining spectrum. The eigenvectors associated with these remaining non-motif eigenvalues must be constant across cycles in the canonical decomposition of the associated permutations. Let P \u2208 R N \u00d7r denote an orthonormal basis for the space of such vectors formed from columns of the identity and (normalized) indicators for nodes cyclically permuted by the motif. The matrix H r = P T HP then has identical eigenvalues to H , except with all the motif eigenvalues omitted. We may form H r explicitly, as it has the same sparsity structure as H but with a supernode replacing the nodes in each instance of a motif cycle; or we can achieve the same result by replacing each random probe Z with the projected probe Z r = PP T Z at an additional cost of O(N motif ) per probe, where N motif is the number of nodes involved in motifs.\nThe motif filtering method essentially allow us to isolate the spiky components from the spectrum. As a result, we are able to obtain a more accurate approximation using fewer Chebyshev moments. Figure 3 demonstrates the improvement on the approximation as we procedurally filter out motifs at 0, \u22121/3, \u22121/2, and \u22121/4. The eigenvalue \u22121/m can be generated by an edge attached to the graph through m \u2212 1 nodes, similar to motif (2c).", "publication_ref": [], "figure_ref": ["fig_1"], "table_ref": []}, {"heading": "ERROR ANALYSIS 4.1 KPM Approximation Error", "text": "This section provides an error bound for our regularized DOS approximation K \u03c3 * \u00b5. We will start with the following theorem. We can pick a smooth mollifier K with Lip(K) = 1. For any \u03bd \u2208 R and \u03bb \u2208 [\u22121, 1] there exists a degree M polynomial such that\n|K \u03c3 (\u03bd \u2212 \u03bb) \u2212 K M \u03c3 (\u03bd \u2212 \u03bb)| < 6L M\u03c3 Define \u00b5 M = M m=0 J m d m \u03d5 m to be the truncated DOS series, \u222b 1 \u22121 f M (\u03bb)\u00b5(\u03bb)d\u03bb = \u222b 1 \u22121 f (\u03bb) \u00b5 M (\u03bb)d\u03bb = M m=0 J m c m d m .\nTherefore,\n\u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u221e = max \u03bd \u222b 1 \u22121 K \u03c3 (\u03bd \u2212 \u03bb)(\u00b5(\u03bb) \u2212 \u00b5 M (\u03bb))d\u03bb \u2264 max \u03bd \u222b 1 \u22121 |K \u03c3 (\u03bd \u2212 \u03bb) \u2212 K M \u03c3 (\u03bd \u2212 \u03bb)|\u00b5(\u03bb)d\u03bb \u2264 6L M\u03c3 .\nConsider \u00b5 M to be the degree M approximation from KPM, shows the relative L 1 error of the spectral histogram when using no filter, filter at \u03bb = 0, and all filters.\n\u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u221e \u2264 \u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u221e + \u2225K \u03c3 \u2225 \u221e \u2225 \u00b5 M \u2212 \u00b5 M \u2225 1 .\nIf we use a probe z with independent standard normal entries for the trace estimation,\n\u00b5(\u03bb) = N i=1 w 2 i \u03b4 (\u03bb \u2212 \u03bb i )\nwhere w = Q T z is the weight for z in the eigenbasis. Hence\n\u2225 \u00b5 M \u2212 \u00b5 M \u2225 1 \u2264 N i=1 |1 \u2212 w 2 i |.\nFinally,\nE \u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u2264 1 \u03c3 6L M + \u2225K \u2225 \u221e E[|1 \u2212 w 2 1 |]\nIf we take N z independent probe vectors, then N z w 2 1 \u223c \u03c7 2 (N z ), which means the expectation decays asymptotically like 2/(\u03c0 N z ).", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Perturbation Analysis", "text": "In this section, we limit our attention to symmetric graph matrix H . Extracting graph information using DOS, whether as a distribution for functions on a graph or as a direct feature in the form of spectral moments, requires stability under small perturbations. In the case of removing/adding a few number of nodes/edges, the Cauchy Interlacing Theorem [33] gives a bound on each individual new eigenvalue by the old ones. For example, if we remove r \u226a N nodes to get a new graph matrix H , then\n\u03bb i (H ) \u2264 \u03bb i ( H ) \u2264 \u03bb i+r (H ) for i \u2264 N \u2212 r(15)\nHowever, this bound may not be helpful when the impact of the change is not reflected by its size. Hence, we provide a theorem that relates the Wasserstein distance (see equation 3) change and the Frobenius norm of the perturbation. Without loss of generality, we assume the eigenvalues of H lie in [\u22121, 1] already.\nTheorem 4.2. Suppose H = H + \u03b4H is the perturbed graph matrix with spectral density \u00b5, then\nW 1 (\u00b5, \u00b5) \u2264 \u2225\u03b4H \u2225 F Proof. Let L be the space of Lipschitz functions with f (0) = 0. W 1 (\u00b5, \u00b5) = sup f \u2208L,Lip(f )=1 \u222b f (\u03bb)(\u00b5(\u03bb) \u2212 \u00b5(\u03bb))d\u03bb = 1 N sup f \u2208L,Lip(f )=1 trace(f (H ) \u2212 f ( H )) \u2264 sup f \u2208L,Lip(f )=1, \u2225v \u2225=1 v T (f (H ) \u2212 f ( H ))v.\nBy Theorem 3.8 from Higham [23], the perturbation on f (H ) is bounded by the Fr\u00e9chet derivative,\n\u2225 f (H ) \u2212 f ( H )\u2225 2 \u2264 Lip(f )\u2225\u03b4H \u2225 F + o(\u2225\u03b4H \u2225 F ). \u25a1", "publication_ref": ["b31", "b21"], "figure_ref": [], "table_ref": []}, {"heading": "EXPERIMENTS 5.1 Gallery of DOS/PDOS", "text": "We first present our spectral histogram approximation from DOS/ PDOS on a wide variety of graphs, including collaboration networks, online social networks, road networks and autonomous systems (dataset details are in the appendix). For all examples, we apply our methods to the normalized adjacency matrices using 500 Chebyshev moments and 20 Hadamard probe vectors. Afterwards, the spectral density is integrated into 50 histogram bins. In figure 4, the DOS approximation is on the first row, and the PDOS approximation is on the second. When a spike exists in the spectrum, we apply motif filtering, and DOS is zoomed appropriately to show the remaining part. For PDOS, we stack the spectral histograms for all nodes vertically, sorted by their projected weights on the leading left singular vector. Red indicates that a node has high weight at certain parts of the spectrum, whereas blue indicates low weight. We observe many distinct shapes of spectrum in our examples. The eigenvalues of denser graphs, such as the Marvel characters  The true spectrum for the California Road Network (4j) is omitted, as it is too large to compute exactly (1,965,206 nodes).\nnetwork (4c: average degree 52.16) and Facebook union of ego networks (4d: average degree 43.69), exhibit decay similar to thepowerlaw around \u03bb = 0. There has been study on the power-law distribution in the eigenvalues of the adjacency and the Laplacian matrix, but it only focuses on the leading eigenvalues rather than the entire spectrum [18] for large real-world datasets. Relatively sparse graphs (4a: average degree 3.06,; 4b: average degree 4.13) often possess spikes, especially around \u03bb = 0, which reflect a larger set of loosely-connected boundary nodes. It is much more evident in the PDOS spectral histograms, which allow us to pick out the nodes with dominant weights at \u03bb = 0 and those that contribute most to local structures. Finally, though the road network is quite sparse (ave. deg 2.50), its regularity results in a lack of special features, and most nodes contribute evenly to the spectrum according to PDOS.", "publication_ref": ["b16"], "figure_ref": ["fig_6"], "table_ref": []}, {"heading": "Computation time", "text": "In this experiment, we show the scaling of our methods by applying them to graphs of varying size of nodes, edges, and sparsity patterns. Rather than computation power, the memory cost of loading a graph with 100M-1B edges is more often the constraint. Hence, we report runtimes for a Python version on a Google Cloud instance with 200GB memory and an Intel Xeon E5 v3 CPU at 2.30GHz. The datasets we use are obtained from the SNAP repository [31]. For each graph, we compute the first 10 Chebyshev moments using KPM with 20 probe vectors. Most importantly, the cost for each moment is independent of the total number of moments we compute. Table 1 reports number of nodes, number of edges, average degree of nodes, and the average runtime for computing each moment. We can observe that the runtime is in accordance with the theoretical complexity O(N z (|V | + |E|)). For the Friendster social network with about 1.8 billion edges, computing each moment takes about 1000 seconds to compute, which means we could obtain a rough approximation to its spectrum within a day. As the dominant cost is matrix-matrix multiplication and we use several probe vectors, our approach has ample opportunity for parallel computation.   ", "publication_ref": ["b29"], "figure_ref": [], "table_ref": ["tab_0"]}, {"heading": "Model Verification", "text": "In this experiment, we investigate the spectrum for some of the popular graph models, and whether they resemble the behavior of real-world data. Two of the most popular models used to describe real-world graphs are the scale-free model [3] and the small-world model [48]. Farkas et al. [19] has analyzed the spectrum of the adjacency matrix; we instead consider the normalized adjacency.\nThe scale-free model grows a random graph with the preferential attachment process, starting from an initial seed graph and adding one node and m edges at every step. Figure 5 shows spectral histograms for this model with 5000 nodes and different choices of m. When m = 1, the generated graph has abundant local motifs like many sparse real-world graphs. By searching in PDOS for the nodes that have high weight at the two spikes, we find nodedoubles (\u03bb = 0) and singly-attached chains (\u03bb = \u00b11/ \u221a\n2). When m = 5, the graph is denser, without any particular motifs, resulting in an approximately semicircular spectral distribution.\nThe small-world model generates a random graph by re-wiring edges of a ring lattice with a certain probability p. Here we construct these graphs on 5000 nodes with p = 0.5; the pattern in spectrum is insensitive for a wide range of p. In Figure 6, when the graph is sparse with 5000 edges, the spectrum has spikes at 0 and \u00b11, indicating local symmetries, bipartite structure, and disconnected components. With 50000 edges, localized structures disappear and the spectrum has narrower support.\nFinally, we investigate the Block Two-Level Erd\u00f6s-R\u00e9nyi (BTER) model [43], which directly fits an input graph. BTER constructs a similar graph by a two-step process: first create a collection of Erd\u00f6s-R\u00e9nyi subgraphs, then interconnect those using a Chung-Lu  model [8]. Seshadhri et al. showed their model accurately captures the observable properties of the given graph, including the eigenvalues of the adjacency matrix. Figure 7 compares the DOS/PDOS of the Erd\u00f6s collaboration network and its BTER counterpart. Unlike the original graph, most 0 eigenvalues in BTER graph come from isolated nodes. The BTER graph also has many more isolated edges (\u03bb = \u00b11), singly-attached chains (\u03bb = \u00b11/ \u221a\n2)), and singly-attached triangles (\u03bb = \u22121/2). We locate these motifs by inspecting nodes with high weights at respective part of the spectrum.", "publication_ref": ["b2", "b46", "b17", "b41", "b7"], "figure_ref": ["fig_8"], "table_ref": []}, {"heading": "DISCUSSION", "text": "In this paper, we make the computation of spectral densities a practical tool for the analysis of large real-world network. Our approach borrows from methods in solid state physics, but with adaptations that improve performance in the network analysis setting by special handling of graph motifs that leave distinctive spectral fingerprints. We show that the spectral densities are stable to small changes in the graph, as well as providing an analysis of the approximation error in our methods. We illustrate the efficiency of our approach by treating graphs with tens of millions of nodes and billions of edges using only a single compute node. The method provides a compelling visual fingerprint of a graph, and we show how this fingerprint can be used for tasks such as model verification.\nOur approach opens the door for the use of complete spectral information in large-scale network analysis. It provides a framework for scalable computation of quantities already used in network science, such as common centrality measures and graph connectivity indices (such as the Estrada index) that can be expressed in terms of the diagonals and traces of matrix functions. But we expect it to serve more generally to define new families of features that describe graphs and the roles nodes play within those graphs. We have shown that graphs from different backgrounds demonstrate distinct spectral characteristics, and thus can be clustered based on those. Looking at LDOS across nodes for role discovery, we can identify the ones with high similarity in their local structures. Moreover, extracting nodes with large weights at various points of the spectrum uncovers motifs and symmetries. In the future, we expect to use DOS/LDOS as graph features for applications in graph clustering, graph matching, role classification, and other tasks.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "Acknowledgments. We thank NSF DMS-1620038 for supporting this work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A DATA SOURCE", "text": "The datasets used in this paper mainly come from the SNAP [31] and RODGER repositories [20]. Table 2 is a list of the networks from these two sources. ", "publication_ref": ["b29", "b18"], "figure_ref": [], "table_ref": []}, {"heading": "B CODE RELEASE", "text": "Code for reproducible experiments and figures are available at https://github.com/kd383/NetworkDOS.", "publication_ref": [], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix", "journal": "Journal of the ACM (JACM)", "year": "2011", "authors": "Haim Avron; Sivan Toledo"}, {"ref_id": "b1", "title": "The spectrum of the graph Laplacian as a tool for analyzing structure and evolution of networks", "journal": "", "year": "2008", "authors": "Anirban Banerjee"}, {"ref_id": "b2", "title": "Emergence of scaling in random networks", "journal": "science", "year": "1999", "authors": "Albert-L\u00e1szl\u00f3 Barab\u00e1si; R\u00e9ka Albert"}, {"ref_id": "b3", "title": "An estimator for the diagonal of a matrix", "journal": "Applied Numerical Mathematics", "year": "2007", "authors": "Costas Bekas; Effrosyni Kokiopoulou; Yousef Saad"}, {"ref_id": "b4", "title": "Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering", "journal": "MIT Press", "year": "2001", "authors": "Mikhail Belkin; Partha Niyogi"}, {"ref_id": "b5", "title": "Eigenvalues in Riemannian geometry", "journal": "Academic press", "year": "1984", "authors": "Isaac Chavel"}, {"ref_id": "b6", "title": "A lower bound for the smallest eigenvalue of the Laplacian", "journal": "", "year": "1969", "authors": "Jeff Cheeger"}, {"ref_id": "b7", "title": "Connected components in random graphs with given expected degree sequences", "journal": "Annals of combinatorics", "year": "2002", "authors": "Fan Chung; Linyuan Lu"}, {"ref_id": "b8", "title": "Complex graphs and networks", "journal": "American Mathematical Soc", "year": "2006", "authors": "Fan Chung; Linyuan Lu"}, {"ref_id": "b9", "title": "Spectral graph theory. Number 92", "journal": "American Mathematical Soc", "year": "1997", "authors": "R K Fan; Fan Chung Chung;  Graham"}, {"ref_id": "b10", "title": "Approximating the Spectrum of a Graph", "journal": "ACM", "year": "2018", "authors": "David Cohen-Steiner; Weihao Kong; Christian Sohler; Gregory Valiant"}, {"ref_id": "b11", "title": "An introduction to the theory of graph spectra", "journal": "Cambridge University Press", "year": "2009", "authors": "Drago\u0161 Cvetkovic; Slobodan Simic; Peter Rowlinson"}, {"ref_id": "b12", "title": "Spectra of Graphs: Theory and Applications", "journal": "Wiley", "year": "1998", "authors": "D M Cvetkovi\u0107; M Doob; H Sachs"}, {"ref_id": "b13", "title": "", "journal": "", "year": "2014", "authors": "Tim Davis;  Hager;  Duff"}, {"ref_id": "b14", "title": "Lower bounds for the partitioning of graphs", "journal": "Papers Of Alan J Hoffman: With Commentary. World Scientific", "year": "2003", "authors": "E William; Alan J Donath;  Hoffman"}, {"ref_id": "b15", "title": "Moments developments and their application to the electronic charge distribution of d bands", "journal": "Journal of Physics and Chemistry of Solids", "year": "1970", "authors": "Francois Ducastelle; Fran\u00e7oise Cyrot-Lackmann"}, {"ref_id": "b16", "title": "Revisiting Power-law Distributions in Spectra of Real World Networks", "journal": "", "year": "2017", "authors": "Nicole Eikmeier; F David;  Gleich"}, {"ref_id": "b17", "title": "Spectra of \"real-world\" graphs: Beyond the semicircle law", "journal": "Physical Review E", "year": "2001", "authors": "J Ill\u00e9s; Imre Farkas; Albert-L\u00e1szl\u00f3 Der\u00e9nyi; Tamas Barab\u00e1si;  Vicsek"}, {"ref_id": "b18", "title": "Repository of Difficult Graph Experiments and Results (RODGER)", "journal": "", "year": "2016", "authors": "David Gleich"}, {"ref_id": "b19", "title": "Matrices, moments and quadrature II; how to compute the norm of the error in iterative methods", "journal": "BIT Numerical Mathematics", "year": "1997", "authors": "H Gene; G\u00e9rard Golub;  Meurant"}, {"ref_id": "b20", "title": "One Cannot Hear the Shape of a Drum", "journal": "Bull. Amer. Math. Soc", "year": "1992", "authors": "Carolyn Gordon; David L Webb; Scott Wolpert"}, {"ref_id": "b21", "title": "Functions of matrices: theory and computation", "journal": "", "year": "2008", "authors": "J Nicholas;  Higham"}, {"ref_id": "b22", "title": "Internet Topology Data Comparison", "journal": "", "year": "2012", "authors": "B Huffaker; M Fomenkov; K Claffy"}, {"ref_id": "b23", "title": "A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines", "journal": "Communications in Statistics-Simulation and Computation", "year": "1990", "authors": "F Michael;  Hutchinson"}, {"ref_id": "b24", "title": "\u00dcber die Genauigkeit der Ann\u00e4herung stetiger Funktionen durch ganze rationale Funktionen gegebenen Grades und trigonometrische", "journal": "", "year": "1911", "authors": "Dunham Jackson"}, {"ref_id": "b25", "title": "Mathematics for Quantum Mechanics: An Introductory Survey of Operators, Eigenvalues, and Linear Vector Spaces", "journal": "Dover Publications", "year": "2006", "authors": "John David; Jackson "}, {"ref_id": "b26", "title": "Can One Hear the Shape of a Drum?", "journal": "The American Mathematical Monthly", "year": "1966", "authors": "Mark Kac"}, {"ref_id": "b27", "title": "On a space of completely additive functions", "journal": "Vestnik Leningrad. Univ", "year": "1958", "authors": "Leonid Vasilevich Kantorovich;  Gennady S Rubinstein"}, {"ref_id": "b28", "title": "A fast and high quality multilevel scheme for partitioning irregular graphs", "journal": "SIAM J. on Scientific Computing", "year": "1998", "authors": "George Karypis; Vipin Kumar"}, {"ref_id": "b29", "title": "SNAP Datasets: Stanford Large Network Dataset Collection", "journal": "", "year": "2014", "authors": "Jure Leskovec; Andrej Krevl"}, {"ref_id": "b30", "title": "Laplace-Beltrami eigenfunctions towards an algorithm that \"understands\" geometry", "journal": "IEEE", "year": "2006", "authors": "Bruno L\u00e9vy"}, {"ref_id": "b31", "title": "Matrix differential calculus with applications in statistics and econometrics. Wiley series in probability and mathematical statistics", "journal": "", "year": "1988", "authors": "R Jan; Heinz Magnus;  Neudecker"}, {"ref_id": "b32", "title": "Selberg's trace formula as applied to a compact Riemann surface", "journal": "Communications on Pure and Applied Mathematics", "year": "1972", "authors": "H P Mckean"}, {"ref_id": "b33", "title": "Spectral partitioning of random graphs", "journal": "", "year": "2001", "authors": "Frank Mcsherry"}, {"ref_id": "b34", "title": "Conductance and convergence of Markov chains-a combinatorial treatment of expanders", "journal": "", "year": "1989", "authors": "M "}, {"ref_id": "b35", "title": "Isoperimetric numbers of graphs", "journal": "Journal of Combinatorial Theory, Series B", "year": "1989", "authors": "Bojan Mohar"}, {"ref_id": "b36", "title": "Mathematical aspects of mixing times in Markov chains", "journal": "Foundations and Trends\u00ae in Theoretical Computer Science", "year": "2006", "authors": "Ravi Montenegro; Prasad Tetali"}, {"ref_id": "b37", "title": "On spectral clustering: Analysis and an algorithm", "journal": "", "year": "2002", "authors": "Y Andrew; Michael I Ng; Yair Jordan;  Weiss"}, {"ref_id": "b38", "title": "The PageRank citation ranking: Bringing order to the web", "journal": "", "year": "1999", "authors": "Lawrence Page; Sergey Brin; Rajeev Motwani; Terry Winograd"}, {"ref_id": "b39", "title": "The Software Scene in the Extraction of Eigenvalues from Sparse Matrices", "journal": "SIAM J. Sci. Statist. Comput", "year": "1984-09", "authors": "B N Parlett"}, {"ref_id": "b40", "title": "Partitioning Sparse Matrices with Eigenvectors of Graphs", "journal": "SIAM J. Matrix Anal. Appl", "year": "1990", "authors": "Alex Pothen; Horst D Simon; Kan-Pu Liou"}, {"ref_id": "b41", "title": "Community structure and scale-free collections of Erd\u0151s-R\u00e9nyi graphs", "journal": "Physical Review E", "year": "2012", "authors": "Comandur Seshadhri; G Tamara; Ali Kolda;  Pinar"}, {"ref_id": "b42", "title": "Approximate counting, uniform generation and rapidly mixing Markov chains", "journal": "Information and Computation", "year": "1989-07", "authors": "Alistair Sinclair; Mark Jerrum"}, {"ref_id": "b43", "title": "Approximation theory and approximation practice", "journal": "", "year": "2013", "authors": " Lloyd N Trefethen"}, {"ref_id": "b44", "title": "Approximation theory and approximation practice", "journal": "", "year": "2013", "authors": " Lloyd N Trefethen"}, {"ref_id": "b45", "title": "Max cut and the smallest eigenvalue", "journal": "SIAM J. Comput", "year": "2012", "authors": "Luca Trevisan"}, {"ref_id": "b46", "title": "Collective dynamics of \u00e2\u0102\u0178small-world\u00e2\u0102\u0179networks", "journal": "nature", "year": "1998", "authors": "J Duncan;  Watts; H Steven;  Strogatz"}, {"ref_id": "b47", "title": "Andreas Alvermann, and Holger Fehske", "journal": "Reviews of modern physics", "year": "2006", "authors": "Alexander Wei\u00dfe; Gerhard Wellein"}, {"ref_id": "b48", "title": "\u00dcber die asymptotische Verteilung der Eigenwerte", "journal": "Nachrichten von der Gesellschaft der Wissenschaften zu G\u00f6ttingen, Mathematisch-Physikalische Klasse", "year": "1911", "authors": "Hermann Weyl"}, {"ref_id": "b49", "title": "On the distribution of the roots of certain symmetric matrices", "journal": "Annals of Mathematics", "year": "1958", "authors": "Eugene P Wigner"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Spectral histogram for the normalized adjacency matrix for the CAIDA autonomous systems graph [24], an Internet topology with 22965 nodes and 47193 edges. Blue bars are the real spectrum, and red points are the approximated heights. (1a) contains high multiplicity around eigenvalue 0, so (1b) zooms in to height between [0, 500].", "figure_data": ""}, {"figure_label": "1", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "pAlgorithm 1 :1in equation 14. The result is Algorithm 1. Nested Dissection for PDOS Approximation Input: Symmetric graph matrix H with eigenvalues in [\u22121, 1] Output: C \u2208 R N \u00d7M where c i j is the j-th Chebyshev moment for i-th node. begin Obtain partitions {I (t ) p } in a tree structure through multilevel nested dissection. for m = 1 to M do Traverse partition tree in pre-order: Compute the separator columns with eq. (14).", "figure_data": ""}, {"figure_label": "41", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Theorem 4 . 1 (41Jackson's Theorem[26]). If f : [\u22121, 1] \u2192 R is Lipschitz continuous with constant L, its best degree M polynomial approximation f M has an L \u221e error of at most 6L/M. The approximation can be constructed asf M = M m=0 J m c m T m (x)where J m are Jackson smoothing factors and c m are the Chebyshev coefficients.", "figure_data": ""}, {"figure_label": "31213", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "3 - 1 2 - 1 Figure 3 :31213Figure 3: The improvement in accuracy of the spectral histogram approximation on the normalized adjacency matrix for the High Energy Physics Theory (HepTh) Collaboration Network, as we sweep through spectrum and filter out motifs. The graph has 8638 nodes and 24816 edges. Blue bars are the real spectrum, and red points are the approximated heights. (3a-3e) use 100 moments and 20 probe vectors. (3f)shows the relative L 1 error of the spectral histogram when using no filter, filter at \u03bb = 0, and all filters.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 4 :4Figure 4: DOS(top)/PDOS(bottom) histograms for the normalized adjacency of 10 networks from five domains. For DOS, blue bars are the true spectrum, and red points are from KPM (500 moments and 20 Hadamard probes). For PDOS, the spectral histograms of all nodes are aligned vertically. Red indicates high weight around an eigenvalue, and blue indicates low weight.The true spectrum for the California Road Network (4j) is omitted, as it is too large to compute exactly(1,965,206 nodes).", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 5 :5Figure 5: Spectral histogram for scale-free model with 5000 nodes and different m. Blue bars are the real spectrum, red points are from KPM (500 moments and 20 probes).", "figure_data": ""}, {"figure_label": "67", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 6 :Figure 7 :67Figure 6: Spectral histograms for small-world model with 5000 nodes and re-wiring probability p = 0.5, starting with 5000 (6a) and 50000 (6b edges. Blue bars are the real spectrum, red points are from KPM (5000 moments and 20 probes).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Average time to compute each Chebyshev moment (with 20 probes) for graphs from the SNAP repository.", "figure_data": "Network # Nodes# EdgesAvg. Deg. Time (s)Facebook 4,03988,23443.690.007AstroPh 18,772198,11021.110.028Enron 36,692183,83110.020.046Gplus 107,61413,673,453254.121.133Amazon 334,863925,8725.530.628Neuron 1,018,52424,735,50348.579.138RoadNetCA 1,965,2062,766,6072.822.276Orkut 3,072,441117,185,08376.28153.7LiveJournal 3,997,96234,681,18917.3514.52Friendster 65,608,366 1,806,067,135 55.061,017"}], "formulas": [{"formula_id": "formula_0", "formula_text": "Laplacian Matrix: L = D \u2212 A L = D \u2212 A L = D \u2212 A.", "formula_coordinates": [2.0, 53.8, 667.5, 113.37, 9.22]}, {"formula_id": "formula_1", "formula_text": "L = I \u2212 D \u22121/2 AD \u22121/2 L = I \u2212 D \u22121/2 AD \u22121/2 L = I \u2212 D \u22121/2 AD \u22121/2 .", "formula_coordinates": [2.0, 444.88, 228.36, 79.92, 10.92]}, {"formula_id": "formula_2", "formula_text": "\u03bb 2 (L)/2 \u2264 h(G) = min S \u2282V |{(i, j) \u2208 E, i \u2208 S, j S }| min(vol(S), vol(V \\S)) \u2264 2\u03bb 2 (L),", "formula_coordinates": [2.0, 330.29, 344.34, 215.43, 20.78]}, {"formula_id": "formula_3", "formula_text": "\u039b = diag(\u03bb 1 , \u2022 \u2022 \u2022 , \u03bb N ) and Q = [q 1 , \u2022 \u2022 \u2022 , q N ] is orthogonal. The spectral density induced by H is the generalized function \u00b5(\u03bb) = 1 N N i=1 \u03b4 (\u03bb \u2212 \u03bb i ), \u222b f (\u03bb)\u00b5(\u03bb) = trace(f (H ))(1)", "formula_coordinates": [2.0, 317.6, 474.21, 240.61, 64.93]}, {"formula_id": "formula_4", "formula_text": "\u00b5(\u03bb; u) = N i=1 |u T q i | 2 \u03b4 (\u03bb \u2212 \u03bb i ), \u222b f (\u03bb)\u00b5(\u03bb; u) = u T f (H )u. (2)", "formula_coordinates": [2.0, 324.85, 601.51, 233.35, 28.67]}, {"formula_id": "formula_5", "formula_text": "|e T k q i | = |q i (k)|", "formula_coordinates": [2.0, 318.31, 676.24, 57.19, 14.32]}, {"formula_id": "formula_6", "formula_text": "W 1 (\u00b5, \u00b5) = sup \u222b (\u00b5(\u03bb) \u2212 \u00b5(\u03bb))f (\u03bb)d\u03bb : Lip(f ) \u2264 1 . (3", "formula_coordinates": [3.0, 75.67, 400.83, 215.2, 14.94]}, {"formula_id": "formula_7", "formula_text": ")", "formula_coordinates": [3.0, 290.87, 409.46, 3.17, 4.09]}, {"formula_id": "formula_8", "formula_text": "(K \u03c3 * \u00b5)(\u03bb) = \u222b R \u03c3 \u22121 K \u03bb \u2212 \u03bd \u03c3 \u00b5(\u03bd )d\u03bd(4)", "formula_coordinates": [3.0, 103.59, 474.26, 190.45, 22.57]}, {"formula_id": "formula_9", "formula_text": "H = 2H \u2212 (\u03bb max (H ) + \u03bb min (H )) \u03bb max (H ) \u2212 \u03bb min (H )(5)", "formula_coordinates": [3.0, 379.7, 224.31, 178.5, 21.05]}, {"formula_id": "formula_10", "formula_text": "T 0 (x) = 1, T 1 (x) = x, T m+1 (x) = 2xT m (x) \u2212 T m\u22121 (x).(6)", "formula_coordinates": [3.0, 340.45, 287.91, 217.75, 9.78]}, {"formula_id": "formula_11", "formula_text": "(x) = 2/[(1 + \u03b4 0n )\u03c0 \u221a 1 \u2212 x 2 ]: \u222b 1 \u22121 w(x)T m (x)T n (x)dx = \u03b4 mn .(7)", "formula_coordinates": [3.0, 382.66, 297.15, 176.6, 45.22]}, {"formula_id": "formula_12", "formula_text": "\u00b5(x) = \u221e m=1 d m T * m (\u03bb)(8)", "formula_coordinates": [3.0, 400.77, 382.4, 157.43, 28.39]}, {"formula_id": "formula_13", "formula_text": "d m = \u222b 1 \u22121 T m (\u03bb)\u00b5(\u03bb)d\u03bb = 1 N N i=1 T m (\u03bb i ) = 1 N trace(T m (H )), (9)", "formula_coordinates": [3.0, 325.96, 413.99, 232.24, 28.67]}, {"formula_id": "formula_14", "formula_text": "d mk = \u222b 1 \u22121 T m (\u03bb)\u00b5 k (\u03bb)d\u03bb = N i=1 |q i (k)| 2 T m (\u03bb i ) = T m (H ) kk . (10", "formula_coordinates": [3.0, 323.36, 495.13, 231.42, 28.67]}, {"formula_id": "formula_15", "formula_text": ")", "formula_coordinates": [3.0, 554.78, 507.48, 3.42, 4.09]}, {"formula_id": "formula_16", "formula_text": "E[z T Hz] = i, j H i j E[z i z j ] = trace(H ) (11) E[z \u2299 Hz] = diag(H )(12)", "formula_coordinates": [3.0, 370.08, 586.09, 188.12, 37.42]}, {"formula_id": "formula_17", "formula_text": "trace(H ) = E[z T Hz] \u2248 1 N z N z j=1", "formula_coordinates": [3.0, 368.47, 654.78, 110.2, 29.02]}, {"formula_id": "formula_18", "formula_text": "HZ M = Z T M \u0393 M + r M e T M", "formula_coordinates": [4.0, 130.76, 334.08, 84.89, 13.94]}, {"formula_id": "formula_19", "formula_text": "Z T M Z M = I M , Z T M r M = 0, and \u0393 M tridiagonal. GQL approxi- mates z T f (H )z with \u2225z \u2225 2 e T 1 f (T M )e 1 , implying z T f (H )z = N i=1 |z T q i | 2 f (\u03bb i ) \u2248 \u2225z \u2225 2 M i=1 |p i1 | 2 f (\u03c4 i )", "formula_coordinates": [4.0, 53.8, 351.84, 241.76, 59.6]}, {"formula_id": "formula_20", "formula_text": "(\u03c4 1 , p 1 ) \u2022 \u2022 \u2022 , (\u03c4 M , p M ) are the eigenpairs of \u0393 M . Consequently, \u2225z \u2225 2 M i=1 |p i1 | 2 \u03b4 (\u03bb \u2212 \u03c4 i )", "formula_coordinates": [4.0, 77.61, 416.63, 217.42, 43.89]}, {"formula_id": "formula_21", "formula_text": "E[\u00b5(\u03bb; z)] = N i=1 \u03b4 (\u03bb \u2212 \u03bb i ) = N \u00b5(\u03bb) Hence \u00b5(\u03bb) \u2248 M i=1 |p i1 | 2 \u03b4 (\u03bb \u2212 \u03c4 i ).", "formula_coordinates": [4.0, 53.8, 502.85, 182.35, 71.35]}, {"formula_id": "formula_22", "formula_text": "H = \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 H 11 0 H 13 0 H 22 H 23 H T 13 H T 23 H 33 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb ,", "formula_coordinates": [4.0, 393.14, 168.93, 89.69, 37.31]}, {"formula_id": "formula_23", "formula_text": "T m+1 (H ) 11 = 2H 11 T m (H ) 11 \u2212 T m\u22121 (H ) 11 + 2H 13 T m (H ) 31 (13)", "formula_coordinates": [4.0, 327.53, 245.5, 230.67, 9.78]}, {"formula_id": "formula_24", "formula_text": "p = I (t ) s I (t ) \u2113 I (t )", "formula_coordinates": [4.0, 317.82, 285.83, 240.38, 27.06]}, {"formula_id": "formula_25", "formula_text": "T m+1 (H )(I (t ) p , I (t ) s ) = 2H (I (t ) p , I (t ) p )T m (H )(I (t ) p , I (t ) s ) \u2212 T m\u22121 (H )(I (t ) p , I (t ) s ) + 2 t \u2032 \u2208S t H (I (t ) p , I (t \u2032 ) s )T m (H )(I (t \u2032 ) s , I (t ) s ) (14)", "formula_coordinates": [4.0, 324.46, 338.27, 233.74, 40.7]}, {"formula_id": "formula_26", "formula_text": "if I (t )", "formula_coordinates": [4.0, 364.58, 545.67, 20.71, 11.87]}, {"formula_id": "formula_27", "formula_text": "\u22121 \u22121 \u22121 \u22121 +1 \u22121 (a) \u03bb = 0 +1 +1 +1 \u00b11 \u00b11 \u00b11 \u22121 \u22121 \u22121 \u22131 \u22131 \u22131 \u00b11 \u00b11 \u00b11 +1 +1 +1 \u22121 \u22121 \u22121 \u22131 \u22131 \u22131 (b) \u03bb = \u00b11/2 +1 +1 +1 \u22121 \u22121 \u22121 (c) \u03bb = \u22121/2 \u00b11 \u00b11 \u00b11 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 +1/ \u221a 2 \u00b11 \u00b11 \u00b11 (d) \u03bb = \u00b11/ \u221a 2", "formula_coordinates": [5.0, 68.57, 88.59, 218.36, 221.75]}, {"formula_id": "formula_28", "formula_text": "|K \u03c3 (\u03bd \u2212 \u03bb) \u2212 K M \u03c3 (\u03bd \u2212 \u03bb)| < 6L M\u03c3 Define \u00b5 M = M m=0 J m d m \u03d5 m to be the truncated DOS series, \u222b 1 \u22121 f M (\u03bb)\u00b5(\u03bb)d\u03bb = \u222b 1 \u22121 f (\u03bb) \u00b5 M (\u03bb)d\u03bb = M m=0 J m c m d m .", "formula_coordinates": [5.0, 317.96, 520.9, 222.08, 68.21]}, {"formula_id": "formula_29", "formula_text": "\u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u221e = max \u03bd \u222b 1 \u22121 K \u03c3 (\u03bd \u2212 \u03bb)(\u00b5(\u03bb) \u2212 \u00b5 M (\u03bb))d\u03bb \u2264 max \u03bd \u222b 1 \u22121 |K \u03c3 (\u03bd \u2212 \u03bb) \u2212 K M \u03c3 (\u03bd \u2212 \u03bb)|\u00b5(\u03bb)d\u03bb \u2264 6L M\u03c3 .", "formula_coordinates": [5.0, 326.74, 608.53, 222.87, 70.65]}, {"formula_id": "formula_30", "formula_text": "\u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u221e \u2264 \u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u221e + \u2225K \u03c3 \u2225 \u221e \u2225 \u00b5 M \u2212 \u00b5 M \u2225 1 .", "formula_coordinates": [5.0, 322.39, 698.19, 231.67, 12.21]}, {"formula_id": "formula_31", "formula_text": "\u00b5(\u03bb) = N i=1 w 2 i \u03b4 (\u03bb \u2212 \u03bb i )", "formula_coordinates": [6.0, 132.83, 528.68, 82.32, 28.67]}, {"formula_id": "formula_32", "formula_text": "\u2225 \u00b5 M \u2212 \u00b5 M \u2225 1 \u2264 N i=1 |1 \u2212 w 2 i |.", "formula_coordinates": [6.0, 122.87, 591.6, 102.4, 28.67]}, {"formula_id": "formula_33", "formula_text": "E \u2225K \u03c3 * (\u00b5 \u2212 \u00b5 M )\u2225 \u2264 1 \u03c3 6L M + \u2225K \u2225 \u221e E[|1 \u2212 w 2 1 |]", "formula_coordinates": [6.0, 79.6, 655.49, 184.45, 18.4]}, {"formula_id": "formula_34", "formula_text": "\u03bb i (H ) \u2264 \u03bb i ( H ) \u2264 \u03bb i+r (H ) for i \u2264 N \u2212 r(15)", "formula_coordinates": [6.0, 358.71, 197.39, 199.49, 9.78]}, {"formula_id": "formula_35", "formula_text": "W 1 (\u00b5, \u00b5) \u2264 \u2225\u03b4H \u2225 F Proof. Let L be the space of Lipschitz functions with f (0) = 0. W 1 (\u00b5, \u00b5) = sup f \u2208L,Lip(f )=1 \u222b f (\u03bb)(\u00b5(\u03bb) \u2212 \u00b5(\u03bb))d\u03bb = 1 N sup f \u2208L,Lip(f )=1 trace(f (H ) \u2212 f ( H )) \u2264 sup f \u2208L,Lip(f )=1, \u2225v \u2225=1 v T (f (H ) \u2212 f ( H ))v.", "formula_coordinates": [6.0, 327.92, 309.63, 231.65, 114.63]}, {"formula_id": "formula_36", "formula_text": "\u2225 f (H ) \u2212 f ( H )\u2225 2 \u2264 Lip(f )\u2225\u03b4H \u2225 F + o(\u2225\u03b4H \u2225 F ). \u25a1", "formula_coordinates": [6.0, 355.36, 464.44, 202.84, 28.0]}], "doi": "10.1145/3292500.3330891"}