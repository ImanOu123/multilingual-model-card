{"title": "", "authors": "Sebastian Flennerhag", "pub_date": "2022-03-16", "abstract": "Meta-learning empowers artificial intelligence to increase its efficiency by learning how to learn. Unlocking this potential involves overcoming a challenging metaoptimisation problem. We propose an algorithm that tackles this problem by letting the meta-learner teach itself. The algorithm first bootstraps a target from the metalearner, then optimises the meta-learner by minimising the distance to that target under a chosen (pseudo-)metric. Focusing on meta-learning with gradients, we establish conditions that guarantee performance improvements and show that the metric can control meta-optimisation. Meanwhile, the bootstrapping mechanism can extend the effective meta-learning horizon without requiring backpropagation through all updates. We achieve a new state-of-the art for model-free agents on the Atari ALE benchmark and demonstrate that it yields both performance and efficiency gains in multi-task meta-learning. Finally, we explore how bootstrapping opens up new possibilities and find that it can meta-learn efficient exploration in an \u03b5-greedy Q-learning agent-without backpropagating through the update rule.", "sections": [{"heading": "INTRODUCTION", "text": "In a standard machine learning problem, a learner or agent learns a task by iteratively adjusting its parameters under a given update rule, such as Stochastic Gradient Descent (SGD). Typically, the learner's update rule must be tuned manually. In contrast, humans learn seamlessly by relying on previous experiences to inform their learning processes (Spelke & Kinzler, 2007).\nFor a (machine) learner to have the same capability, it must be able to learn its update rule (or such inductive biases). Meta-learning is one approach that learns (parts of) an update rule by applying it for some number of steps and then evaluating the resulting performance (Schmidhuber, 1987;Hinton & Plaut, 1987;Bengio et al., 1991). For instance, a well-studied and often successful approach is to tune parameters of a gradient-based update, either online during training on a single task (Bengio, 2000;Maclaurin et al., 2015;Xu et al., 2018;Zahavy et al., 2020), or meta-learned over a distribution of tasks (Finn et al., 2017;Rusu et al., 2019;Flennerhag et al., 2020;Jerfel et al., 2019;Denevi et al., 2019). More generally, the update rule can be an arbitrary parameterised function (Hochreiter et al., 2001;Andrychowicz et al., 2016;Kirsch et al., 2019;, or the function itself can be meta-learned jointly with its parameters (Alet et al., 2020;Real et al., 2020).\nMeta-learning is challenging because to evaluate an update rule, it must first be applied. This often leads to high computational costs. As a result most works optimise performance after K applications of the update rule and assume that this yields improved performance for the remainder of the learner's lifetime (Bengio et al., 1991;Maclaurin et al., 2015;Metz et al., 2019). When this assumption fails, meta-learning suffers from a short-horizon bias Metz et al., 2019). Similarly, optimizing the learner's performance after K updates can fail to account for the process of learning, causing another form of myopia (Flennerhag et al., 2019;Stadie et al., 2018;Chen et al., 2016;Cao et al., 2019). Challenges in meta-optimisation have been observed to cause degraded lifetime performance (Lv et al., 2017;Wichrowska et al., 2017), collapsed exploration (Stadie et al., 2018;Chen et al., 2016), biased learner updates (Stadie et al., 2018;Zheng et al., 2018), and poor generalisation performance Yin et al., 2020;Triantafillou et al., 2020).\n1. It bootstraps a target from the learner's new parameters. In this paper, we generate targets by continuing to update the learner's parameters-either under the meta-learned update rule or another update rule-for some number of steps. 2. The learner's new parameters-which are a function of the meta-learner's parameters-and the target are projected onto a matching space. A simple example is Euclidean parameter space. To control curvature, we may choose a different (pseudo-)metric space. For instance, a common choice under probabilistic models is the Kullback-Leibler (KL) divergence.\nThe meta-learner is optimised by minimising distance to the bootstrapped target. We focus on gradient-based optimisation, but other optimisation routines are equally applicable. By optimising meta-parameters in a well-behaved space, we can drastically reduce ill-conditioning and other phenomena that disrupt meta-optimisation. In particular, this form of Bootstrapped Meta-Gradient (BMG) enables us to infuse information about future learning dynamics without increasing the number of update steps to backpropagate through. In effect, the meta-learner becomes its own teacher. We show that BMG can guarantee performance improvements (Theorem 1) and that this guarantee can be stronger than under standard meta-gradients (Corollary 1). Empirically, we find that BMG provides substantial performance improvements over standard meta-gradients in various settings. We obtain a new state-of-the-art result for model-free agents on Atari (Section 5.2) and improve upon MAML (Finn et al., 2017) in the few-shot setting (Section 6). Finally, we demonstrate how BMG enables new forms of meta-learning, exemplified by meta-learning \u03b5-greedy exploration (Section 5.1).", "publication_ref": ["b32", "b26", "b2", "b13", "b45", "b47", "b25", "b4", "b3", "b9", "b23", "b13", "b14", "b14", "b33", "b11", "b42", "b33", "b33", "b48", "b46", "b38"], "figure_ref": [], "table_ref": []}, {"heading": "RELATED WORK", "text": "Bootstrapping as used here stems from temporal difference (TD) algorithms in reinforcement learning (RL) (Sutton, 1988). In these algorithms, an agent learns a value function by using its own future predictions as targets. Bootstrapping has recently been introduced in the self-supervised setting (Guo et al., 2020;Grill et al., 2020). In this paper, we introduce the idea of bootstrapping in the context of meta-learning, where a meta-learner learns about an update rule by generating future targets from it.\nOur approach to target matching is related to methods in multi-task meta-learning (Flennerhag et al., 2019;Nichol et al., 2018) that meta-learn an initialisation for SGD by minimising the Euclidean distance to task-optimal parameters. BMG generalise this concept by allowing for arbitrary metaparameters, matching functions, and target bootstraps. It is further related the more general concept of self-referential meta-learning (Schmidhuber, 1987;1993), where the meta-learned update rule is used to optimise its own meta-objective.\nTarget matching under KL divergences results in a form of distillation (Hinton et al., 2015), where an online network (student) is encouraged to match a target network (teacher). In a typical setup, the target is either a fixed (set of) expert(s) (Hinton et al., 2015;Rusu et al., 2015) or a moving aggregation of current experts (Teh et al., 2017;Grill et al., 2020), whereas BMG bootstraps a target by following an update rule. Finally, BMG is loosely inspired by trust-region methods that introduce a distance function to regularize gradient updates (Pascanu & Bengio, 2014;Schulman et al., 2015;Tomar et al., 2020;Hessel et al., 2021).\n3 BOOTSTRAPPED META-GRADIENTS We begin in the single-task setting and turn to multi-task meta-learning in Section 6. The learner's problem is to minimize a stochastic objective f (x) := E[ (x; \u03b6)] over a data distribution p(\u03b6), where \u03b6 denotes a source of data and x \u2208 X \u2282 R nx denotes the learner's parameters. In RL, f is typically the (negative) expected value of a policy \u03c0 x ; in supervised learning, f may be the expected negative loglikelihood under a probabilistic model \u03c0 x . We provide precise formulations in Sections 5 and 6.\nThe meta-learner's problem is to learn an update rule \u03d5 : X \u00d7 H \u00d7 W \u2192 X that updates the learner's parameters by x (1) = x + \u03d5(x, h, w) given x \u2208 X , a learning state h \u2208 H, and meta-parameters w \u2208 W \u2282 R nw of the update rule. We make no assumptions on the update rule other than differentiability in w. As such, \u03d5 can be a recurrent neural network (Hochreiter et al., 2001;Wang et al., 2016;Andrychowicz et al., 2016) or gradient descent (Bengio, 2000;Maclaurin et al., 2015;Finn et al., 2017).\nThe learning state h contains any other data required to compute the update; in a black-box setting h contains an observation and the recurrent state of the network; for gradient-based updates, h contains the (estimated) gradient of f at x along with any auxiliary information; for instance, SGD is given by\nx (1) = x \u2212\u03b1\u2207 x f (x) with h = \u2207 x f (x), w = \u03b1 \u2208 R + .\nThe standard meta-gradient (MG) optimises meta-parameters w by taking K steps under \u03d5 and evaluating the resulting learner parameter vector under f . With a slight abuse of notation, let x (K) (w) denote the learner's parameters after K applications of \u03d5 starting from some (x, h, w), where (x, h) evolve according to \u03d5 and the underlying data distribution. The MG update is defined by\nw = w \u2212\u03b2 \u2207 w f x (K) (w) , \u03b2 \u2208 R + .(1)\nExtensions involve averaging the performance over all iterates x (1) , . . . , x (K) (Andrychowicz et al., 2016;Chen et al., 2016;Antoniou et al., 2019) or using validation data in the meta-objective (Bengio et al., 1991;Maclaurin et al., 2015;Finn et al., 2017;Xu et al., 2018). We observe two bottlenecks in the meta-objective in Eq. 1. First, the meta-objective is subject to the same curvature as the learner. Thus if f is ill-conditioned, so will the meta-objective be. Second, the meta-objective is only able to evaluate the meta-learner on dynamics up to the Kth step, but ignores effects of future updates.\nTo tackle myopia, we introduce a Target Bootstrap (TB) \u03be : X \u2192 X that maps the meta-learner's output x (K) into a bootstrapped targetx = \u03be(x (K) ). We focus on TBs that unroll \u03d5 a further L \u2212 1 steps before taking final gradient step on f , with targets of the formx = x (K+L\u22121) \u2212\u03b1\u2207f (x (K+L\u22121) ). This TB encourages the meta-learner to reach future states on its trajectory faster while nudging the trajectory in a descent direction. Crucially, regardless of the bootstrapping strategy, we do not backpropagate through the target. Akin to temporal difference learning in RL (Sutton, 1988), the target is a fixed goal that the meta-learner should try to produce within the K-step budget.\nFinally, to improve the meta-optimisation landscape, we introduce a matching function \u00b5 : X \u00d7 X \u2192 R + that measures the (dis)similarity between the meta-learner's output, x (K) (w), and the target,x, in a matching space defined by \u00b5 (see Figure 1). Taken together, the BMG update is defined b\u1ef9\nw = w \u2212\u03b2 \u2207 w \u00b5 x, x (K) (w) , \u03b2 \u2208 R + ,(2)\nwhere the gradient is with respect to the second argument of \u00b5. Thus, BMG describes a family of algorithms based on the choice of matching function \u00b5 and TB \u03be. In particular, MG is a special case of BMG under matching function \u00b5(x, x (K) ) = x \u2212 x (K) 2 2 and TB \u03be(x (K) ) = x (K) \u2212 1 2 \u2207 x f (x (K) ), since the bootstrapped meta-gradient reduces to the standard meta-gradient:\n\u2207 w x \u2212 x (K) (w) 2 2 = \u22122D x \u2212 x (K) = D\u2207 x f x (K) = \u2207 w f x (K) (w) ,(3)\nwhere D denotes the (transposed) Jacobian of x (K) (w). For other matching functions and target strategies, BMG produces different meta-updates compared to MG. We discuss these choices below.\nMatching Function Of primary concern to us are models that output a probabilistic distribution, \u03c0 x . A common pseudo-metric over a space of probability distributions is the Kullback-Leibler (KL) divergence. For instance, Natural Gradients (Amari, 1998) point in the direction of steepest descent under the KL-divergence, often approximated through a KL-regularization term (Pascanu & Bengio, 2014). KL-divergences also arise naturally in RL algorithms (Kakade, 2001;Schulman et al., 2015;Abdolmaleki et al., 2018). Hence, a natural starting point is to consider KL-divergences between the target and the iterate, e.g. \u00b5(x, x (K) ) = KL (\u03c0x \u03c0 x (K) ). In actor-critic algorithms (Sutton et al., 1999), the policy defines only part of the agent-the value function defines the other. Thus, we also consider a composite matching function over both policy and value function.", "publication_ref": ["b34", "b16", "b26", "b27", "b1", "b1", "b24", "b36", "b19", "b30", "b37", "b0", "b3", "b41", "b13", "b13", "b45", "b34", "b19", "b6", "b30", "b35"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Target Bootstrap", "text": "We analyze conditions under which BMG guarantees performance improvements in Section 4 and find that the target should co-align with the gradient direction. Thus, in this paper we focus on gradient-based TBs and find that they perform well empirically. As with matching functions, this is a small subset of all possible choices; we leave the exploration of other choices for future work.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "PERFORMANCE GUARANTEES", "text": "In this analysis, we restrict attention to the noise-less setting (true expectations). In this setting, we ask three questions: (1) what local performance guarantees are provided by MG? (2) What performance guarantees can BMG provide? (3) How do these guarantees relate to each other? To answer these questions, we analyse how the performance around f (x (K) (w)) changes by updating w either under standard meta-gradients (Eq. 1) or bootstrapped meta-gradients (Eq. 2).\nFirst, consider improvements under the MG update. In online optimisation, the MG update can achieve strong convergence guarantees if the problem is well-behaved (van Erven & Koolen, 2016), with similar guarantees in the multi-task setting (Balcan et al., 2019;Khodak et al., 2019;Denevi et al., 2019). A central component of these results is that the MG update guarantees a local improvement in the objective. Lemma 1 below presents this result in our setting, with the following notation: let u A := u, A u for any square real matrix A.\nLet G T = D T D \u2208 R nx\u00d7nx , with D := \u2202 \u2202 w x (K) (w) T \u2208 R nw\u00d7nx . Note that \u2207 w f (x (K) (w)) = D\u2207 x f (x (K) ).\nLemma 1 (MG Descent). Let w be given by Eq. 1. For \u03b2 sufficiently small, f\nx (K) (w ) \u2212 f x (K) (w) = \u2212\u03b2 \u2207 x f (x (K) ) 2 G T + O(\u03b2 2 ) < 0.\nWe defer all proofs to Appendix A. Lemma 1 relates the gains obtained under standard meta-gradients to the local gradient norm of the objective. Because the meta-objective is given by f , the MG update is not scale-free (c.f. Schraudolph, 1999), nor invariant to re-parameterisation. If f is highly non-linear, the meta-gradient can vary widely, preventing efficient performance improvement. Next, we turn to BMG, where we assume \u00b5 is differentiable and convex, with 0 being its minimum. Theorem 1 (BMG Descent). Letw be given by Eq. 2 for some TB \u03be. The BMG update satisfies\nf x (K) (w) \u2212 f x (K) (w) = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)).\nFor (\u03b1, \u03b2) sufficiently small, there exists infinitely many \u03be for which f x (K) (w) \u2212f x (K) (w) < 0.\nIn particular, \u03be(x (K) ) = x (K) \u2212\u03b1G T g yields improvements\nf x (K) (w) \u2212 f x (K) (w) = \u2212 \u03b2 \u03b1 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)) < 0.\nThis is not an optimal rate; there exists infinitely many TBs that yield greater improvements.\nTheorem 1 portrays the inherent trade-off in BMG; targets should align with the local direction of steepest descent, but provide as much learning signal as possible. Importantly, this theorem also establishes that \u00b5 directly controls for curvature as improvements are expressed in terms of \u00b5. While the TB \u03be \u03b1 G (x (K) ) := x (K) \u2212\u03b1G T g yields performance improvements that are proportional to the meta-loss itself, larger improvements are possible by choosing a TB that carries greater learning signal (by increasing \u00b5(x, x (K) )). To demonstrate that BMG can guarantee larger improvements to the update rule than MG, we consider the TB \u03be \u03b1 G with \u00b5 the (squared) Euclidean norm. Let r := \u2207f x (K) 2 / G T \u2207f x (K) 2 denote the gradient norm ratio. K) ). Let w be given by Eq. 1 andw be given by Eq. 2. For \u03b2 sufficiently small, f x (K) (w) \u2264 f x (K) (w ) , strictly if GG T = G T and G T \u2207 x f (x (K) ) = 0.\nCorollary 1. Let \u00b5 = \u2022 2 2 andx = \u03be r G (x(\nDiscussion Our analysis focuses on an arbitrary (albeit noiseless) objective f and establishes that BMG can guarantee improved performance under a variety of TBs. We further show that BMG can yield larger local improvements than MG. To identify optimal TBs, further assumptions are required on f and \u00b5, but given these Theorem 1 can serve as a starting point for more specialised analysis. Empirically, we find that taking L steps on the meta-learned update with an final gradient step on the objective performs well. Theorem 1 exposes a trade-off for targets that are \"far\" away. Empirically, we observe clear benefits from bootstraps that unroll the meta-learner for several steps before taking a gradient step on f ; exploring other forms of bootstraps is an exciting area for future research.", "publication_ref": ["b39", "b8", "b8", "b29"], "figure_ref": [], "table_ref": []}, {"heading": "REINFORCEMENT LEARNING", "text": "We consider a typical reinforcement learning problem, modelled as an MDP M = (S, A, P, R, \u03b3). Given an initial state s 0 \u2208 S, at each time step t \u2208 N, the agent takes an action a t \u223c \u03c0 x (a | s t ) from a policy \u03c0 : S \u00d7 A \u2192 [0, 1] parameterised by x. The agent obtains a reward r t+1 \u223c R(s t , a t , s t+1 ) based on the transition s t+1 \u223c P(s t+1 | s t , a t ). The action-value of the agent's policy given a state s 0 and action a 0 is given by\nQ x (s 0 , a 0 ) := E[ \u221e t=0 \u03b3 t r t+1 | s 0 , a 0 , \u03c0 x ] under discount rate \u03b3 \u2208 [0, 1). The corresponding value of policy \u03c0 x is given by V x (s 0 ) := E a0\u223c\u03c0x(a | s0) [Q x (s 0 , a 0 )].\nThe agent's problem is to learn a policy that maximises the value given an expectation over s 0 , defined either by an initial state distribution in the episodic setting (e.g. Atari, Section 5.2) or the stationary state-visitation distribution under the policy in the non-episodic setting (Section 5.1). Central to RL is the notion of policy-improvement, which takes a current policy \u03c0 x and constructs a new policy \u03c0\nx such that E[V x ] \u2265 E[V x ]. A common policy-improvement step is arg max x E a\u223c\u03c0 x (a|s) [Q x (s, a)].\nMost works in meta-RL rely on actor-critic algorithms (Sutton et al., 1999). These treat the above policy-improvement step as an optimisation problem and estimate a policy-gradient (Williams & Peng, 1991;Sutton et al., 1999) to optimise x. To estimate V x , these introduce a critic v z that is jointly trained with the policy. The policy is optimised under the current estimate of its value function, while the critic is tracking the value function by minimizing a Temporal-Difference (TD) error. Given a rollout \u03c4 = (s 0 , a 0 , r 1 , s 1 , . . . , r T , s T ), the objective is given by f (x, z) = PG PG (x) + EN EN (x) + TD TD (z), PG , EN , TD \u2208 R + , where\nEN (x) = t\u2208\u03c4 a\u2208A \u03c0 x (a | s t ) log \u03c0 x (a | s t ), TD (z) = 1 2 t\u2208\u03c4 G (n) t \u2212 v z (s t ) 2 , PG (x) = \u2212 t\u2208\u03c4 \u03c1 t log \u03c0 x (a t | s t ) G (n) t \u2212 v z (s t ) ,(4)\nwhere \u03c1 t denotes an importance weight and G\n(n) t denotes an n-step bootstrap target. Its form depends on the algorithm; in Section 5.1, we generate rollouts from \u03c0 x (on-policy), in which case \u03c1 t = 1 and G (n) t = (n\u22121) i=0 \u03b3 i r t+i+1 + \u03b3 n vz(s t+n ) \u2200t, wherez denotes fixed (non-differentiable) parameters. In the off-policy setting (Section 5.2), \u03c1 corrects for sampling bias and G  We begin with a tabular grid-world with two items to collect. Once an item is collected, it is randomly re-spawned. One item yields a reward of +1 and the other a reward of \u22121. The reward is flipped every 100,000 steps. To succeed, a memory-less agent must efficiently re-explore the environment. We study an on-policy actor-critic agent with PG = TD = 1. As baseline, we tune a fixed entropy-rate weight = EN . We compare against agents that meta-learn online. For MG, we use the actor-critic loss as meta-objective ( fixed), as per Eq. 1. The setup is described in full in Appendix B.1\nBMG Our primary focus is on the effect of bootstrapping. Because this setup is fully online, we can generate targets using the most recent L\u22121 parameter updates and a final agent parameter update using = 0. Hence, the computational complexity of BMG is constant in L under this implementation (see Appendix B.2). We define the matching function as the KL-divergence between x (K) and the target, \u00b5(x, x (K) (w)) = KL (\u03c0x \u03c0 x (K) ).\nFigure 2 presents our main findings. Both MG and BMG learn adaptive entropy-rate schedules that outperform the baseline. However, MG fails if = 0 in the meta-objective, as it becomes overly greedy (Figure 9). MG shows no clear benefit of longer meta-learning horizons, indicating that myopia stems from the objective itself. In contrast, BMG exhibits greater adaptive capacity and is able to utilise greater meta-learning horizons. Too short horizons induce myopia, whereas too long prevent efficient adaptation. For a given horizon, increasing K is uniformly beneficial. Finally, we find that BMG outperforms MG for a given horizon without backpropagating through all updates.\nFor instance, for K = 8, BMG outperforms MG with K = 1 and L = 7. Our ablation studies (Appendix B.2) show that increasing the target bootstrap length counters myopia; however, using the meta-learned update rule for all L steps can derail meta-optimization.\nNext, we consider a new form of meta-learning: learning \u03b5-greedy exploration in a Q(\u03bb)-agent (precise formulation in Appendix B.3). While the \u03b5 parameter has a similar effect to entropyregularization, \u03b5 is a parameter applied in the behaviour-policy while acting. As it does not feature in the loss function, it is not readily optimized by existing meta-gradient approaches. In contrast, BMG can be implemented by matching the policy derived from a target action-value function, precisely as in the actor-critic case. An implication is that BMG can meta-learn without backpropagating through the update rule. Significantly, this opens up to meta-learning (parts of) the behaviour policy, which is hard to achieve in the MG setup as the behaviour policy is not used in the update rule. Figure 3 shows that meta-learning \u03b5-greedy exploration in this environment significantly outperforms the best fixed \u03b5 found by hyper-parameter tuning. As in the actor-critic case, we find that BMG responds positively to longer meta-learning horizons (larger L); see Appendix B.3, Figure 12 for detailed results. ", "publication_ref": ["b35", "b43", "b35"], "figure_ref": ["fig_6", "fig_1", "fig_0"], "table_ref": []}, {"heading": "ATARI", "text": "High-performing RL agents tend to rely on distributed learning systems to improve data efficiency (Kapturowski et al., 2018;Espeholt et al., 2018). This presents serious challenges for meta-learning as the policy gradient becomes noisy and volatile due to off-policy estimation (Xu et al., 2018;Zahavy et al., 2020). Theorem 1 suggests that BMG can be particularly effective in this setting under the appropriate distance function. To test these predictions, we adapt the Self-Tuning Actor-Critic (STACX; Zahavy et al., 2020) to meta-learn under BMG on the 57 environments in the Atari Arcade Learning Environment (ALE; Bellemare et al., 2013).\nProtocol We follow the original IMPALA setup (Espeholt et al., 2018), but we do not downsample or gray-scale inputs. Following the literature, we train for 200 million frames and evaluate agent performance by median Human Normalized Score (HNS) across 3 seeds (Espeholt et al., 2018;Xu et al., 2018;Zahavy et al., 2020).\nSTACX The IMPALA actor-critic agent runs multiple actors asynchronously to generate experience for a centralized learner. The learner uses truncated importance sampling to correct for off-policy data in the actor-critic update, which adjusts \u03c1 andV in Eq. 4. The STACX agent (Zahavy et al., 2020) is a state-of-the-art meta-RL agent. It builds on IMPALA in two ways: (1) it introduces auxiliary tasks in the form of additional objectives that differ only in their hyper-parameters; (2) it meta-learns the hyper-parameters of each loss function (main and auxiliary). Meta-parameters are given by w = (\u03b3 i , i\nPG , i EN , i TD , \u03bb i , \u03b1 i ) 1+n i=1\n, where \u03bb and \u03b1 are hyper-parameters of the importance weighting mechanism and n = 2 denotes the number of auxiliary tasks. STACX uses the IMPALA objective as the meta-objective with K = 1. See Appendix C for a complete description.\nBMG We conduct ceteris-paribus comparisons that only alter the meta-objective: agent parameter updates are identical to those in STACX. When L = 1, the target takes a gradient step on the original IMPALA loss, and hence the only difference is the form of the meta-objective; they both use the same data and gradient information. For L > 1, the first L \u2212 1 steps bootstrap from the meta-learned update rule itself. To avoid overfitting, each of the L \u2212 1 steps use separate replay data; this extra data is not used anywhere else. To understand matching functions, we test policy matching and value matching. Policy matching is defined by \u00b5(x, x (K) (w)) = KL (\u03c0x \u03c0 x (1) ); we also test a symmetric KL-divergence (KL-S). Value matching is defined by \u00b5(z,\nz (1) (w)) := E (vz \u2212 v z (1) ) 2 .\nFigure 4 presents our main comparison. BMG with L = 1 and policy-matching (KL) obtains a median HNS of~500%, compared to~350% for STACX. Recall that for L = 1, BMG uses the same data to compute agent parameter update, target update, and matching loss; hence this is an apples-to-apples comparison. Using both policy matching and value matching (with 0.25 weight on the latter) further improves the score to~520% and outperforms STACX across almost all 57 games, with a few minor exceptions (left panel, Figure 4). These results are obtained without tuning hyper-parameters for BMG. Finally, extending the meta-learning horizon by setting L = 4 and adjusting gradient clipping from .3 to .2 obtains a score of~610%. In Figure 5, we turn to ablations. In the left-panel, we deconstruct BMG into STACX (i.e., MG) and compare performances. We find that roughly 45% of the performance gains comes from curvature correction (given by using RMSProp in the target bootstrap). The matching function can further control curvature to obtain performance improvements, accounting for roughly 25%. Finally, increasing L, thereby reducing myopia, accounts for about 30% of the performance improvement. Comparing the cosine similarity between consequtive meta-gradients, we find that BMG improves upon STACX by two orders of magnitude. Detailed ablations in Appendix C.1.\nSGD L2 L=1 RMS L2 L=1 RMS KL L=1 RMS KL & V L=1 RMS KL & V\nThe center panel of Figure 5 provides a deep-dive in the effect of increasing the meta-learning horizon (L > 1) in Ms Pacman. Performance is uniformly increasing in L, providing further support that BMG can increase the effective meta-horizon without increasing the number of update steps to backpropagate through. A more in-depth analysis Appendix C.3 reveals that K is more sensitive to curvature and the quality of data. However, bootstrapping only from the meta-learner for all L steps can lead to degeneracy (Appendix C.2, Figure 14). In terms of replay (Appendix C.2), while standard MG degrades with more replay, BMG benefits from more replay in the target bootstrap.\nThe right panel of Figure 5 studies the effect of the matching function. Overall, joint policy and value matching exhibits best performance. In contrast to recent work (Tomar et al., 2020;Hessel et al., 2021), we do not find that reversing the KL-direction is beneficial. Using only value-matching results in worse performance, as it does not optimise for efficient policy improvements. Finally, we conduct detailed analysis of scalability in Appendix C.4. While BMG is 20% slower for K = 1, L = 1 due to the target bootstrap, it is 200% faster when MG uses K = 4 and BMG uses K = 1, L = 3.", "publication_ref": ["b7", "b45", "b47", "b47", "b45", "b47", "b47", "b37", "b0"], "figure_ref": ["fig_2", "fig_2", "fig_3", "fig_3", "fig_0", "fig_3"], "table_ref": []}, {"heading": "MULTI-TASK FEW-SHOT LEARNING", "text": "Multi-task meta-learning introduces an expectation over task objectives. BMG is applied by computing task-specific bootstrap targets, with the meta-gradient being the expectation over task-specific matching losses. For a general multi-task formulation, see Appendix D; here we focus on the few-shot classification paradigm. Let f D : X \u2192 R denote the negative log-likelihood loss on some data D. A task is defined as a pair of datasets (D \u03c4 , D \u03c4 ), where D \u03c4 is a training set and D \u03c4 is a validation set. In the M -shot-N -way setting, each task has N classes and D \u03c4 contains M observations per class.\nThe goal of this experiment is to study how the BMG objective behaves in the multi-task setting. For this purpose, we focus on the canonical MAML setup (Finn et al., 2017), which meta-learns an initialisation x (0) \u03c4 = w for SGD that is shared across a task distribution p(\u03c4 ). Adaptation is defined by x\n(k) \u03c4 = x (k\u22121) \u03c4 +\u03b1\u2207f D\u03c4 (x (k\u22121) \u03c4\n), with \u03b1 \u2208 R + fixed. The meta-objective is the validation loss in expectation over the task distribution:\nE[f D \u03c4 (x (K) \u03c4 (w))].\nSeveral works have extended this setup by altering the update rule (\u03d5) (Lee & Choi, 2018;Zintgraf et al., 2019;Park & Oliva, 2019;Flennerhag et al., 2020). As our focus is on the meta-objective, we focus on comparisons with MAML. BMG For each task, a targetx \u03c4 is bootstrapped by taking L SGD steps from x\n(K) \u03c4 using validation data. The BMG objective is the expected distance, E[\u00b5(x \u03c4 , x (K) \u03c4 )].\nThe KL-divergence as matching function has an interesting connection to MG. The targetx \u03c4 can be seen as an \"expert\" on task \u03c4 so that BMG is a form of distillation (Hinton et al., 2015). The log-likelihood loss used by MG is also a KL divergence, but w.r.t. a \"cold\" expert that places all mass on the true label. Raising the temperature in the target can allow BMG to transfer more information (Hinton & Plaut, 1987).\nSetup We use the MiniImagenet benchmark (Vinyals et al., 2016) and study two forms of efficiency: for data efficiency, we compare meta-test performance as function of the number of meta-training batches; for computational efficiency, we compare meta-test performance as a function of training time. To reflect what each method would achieve for a given computational budget, we report meta-test performance for the hyper-parameter configuration with best meta-validation performance. For MG, we tune the meta-learning rate \u03b2 \u2208 {10 \u22123 , 10 \u22124 }, K \u2208 {1, 5, 10}, and options to use first-order approximations ((FOMAML;Finn et al., 2017) or (ANIL; Raghu et al., 2020)). For BMG, we tune \u03b2 \u2208 {10 \u22123 , 10 \u22124 }, K \u2208 {1, 5}, as well as L \u2208 {1, 5, 10}, and the direction of the KL.\nThe left panel of Figure 6 presents results on data efficiency. For few meta-updates, MG and BMG are on par. For 50 000 meta-updates and beyond, BMG achieves strictly superior performance, with the performance delta increasing over meta-updates. The central panel presents results on computational efficiency; we plot the time required to reach a given meta-test performance. This describes the relationship between performance and computational complexity. We find BMG exhibits better scaling properties, reaching the best performance of MG in approximately half the time. Finally, in the right panel, we study the effect of varying K. BMG achieves higher performance for both K = 1 and K = 5. We allow MG to also use K = 10, but this did not yield any significant gains. We conduct an analysis of the impact BMG has on curvature and meta-gradient variance in Appendix D.3. To summarise, we find that BMG significantly improves upon the MG meta-objective, both in terms of data efficiency, computational efficiency, and final performance.", "publication_ref": ["b10", "b49", "b18", "b1", "b2", "b40", "b21"], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "CONCLUSION", "text": "In this paper, we have put forth the notion that efficient meta-learning does not require the metaobjective to be expressed directly in terms of the learner's objective. Instead, we present an alternative approach that relies on having the meta-learner match a desired target. Here, we bootstrap from the meta-learned update rule itself to produce future targets. While using the meta-learned update rule as the bootstrap allows for an open-ended meta-learning process, some grounding is necessary. As an instance of this approach, we study bootstrapped meta-gradients, which can guarantee performance improvements under appropriate choices of targets and matching functions that can be larger than those of standard meta-gradients. Empirically, we observe substantial improvements on Atari and achieve a new state-of-the-art, while obtaining significant efficiency gains in a multi-task metalearning setting. We explore new possibilities afforded by the target-matching nature of the algorithm and demonstrate that it can learn to explore in an -greedy Q-learning agent. ", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "A PROOFS", "text": "This section provides complete proofs for the results in Section 4. Throughout, we assume that (x (0) , h (0) , w) is given and write x := x (0) , h := h (0) . We assume that h evolves according to some process that maps a history H (k) := (x (0) , h (0) , . . . , k) , including any sampling of data (c.f. Section 3). Recall that we restrict attention to the noiseless setting, and hence updates are considered in expectation. We define the map x (K) (w) by\nx (k\u22121) , h (k\u22121) , x (k) ) into a new learner state h(\nx (1) = x (0) + \u03d5 x (0) , h (0) , w x (2) = x (1) + \u03d5 x (1) , h (1) , w . . . x (K) = x (K\u22121) + \u03d5 x (K\u22121) , h (K\u22121) , w .\nThe derivative \u2202 \u2202 w x (K) (w) differentiates through each step of this process (Hochreiter et al., 2001). As previously stated, we assume f is Lipschitz and that x (K) is Lipschitz w.r.t. w. We are now in a position to prove results from the main text. We re-state them for convenience.\nLemma 1 (MG Descent). Let w be given by Eq. 1. For \u03b2 sufficiently small, f\nx (K) (w ) \u2212 f x (K) (w) = \u2212\u03b2 \u2207 x f (x (K) ) 2 G T + o(\u03b2 2 ) < 0.\nProof. Define g := \u2207 x f (x (K) (w)). The meta-gradient at (x, h, w) is given by \u2207 w f (x (K) (w)) = D g. Under Eq. 1, we find w = w \u2212\u03b2D g. By first-order Taylor Series Expansion of f around (x, h, w ) with respect to w:\nf x (K) (w ) = f x (K) (w) + D g, w \u2212 w + o(\u03b2 2 g 2 G T ) = f x (K) (w) \u2212 \u03b2 D g, D g + o(\u03b2 2 g 2 G T ) = f x (K) (w) \u2212 \u03b2 g 2 G T + o(\u03b2 2 g 2 G T ),\nwith g 2 G T \u2265 0 by virtue of positive semi-definiteness of G. Hence, for \u03b2 2 small the residual vanishes and the conclusion follows.\nTheorem 1 (BMG Descent). Letw be given by Eq. 2 for some TB \u03be. The BMG update satisfies\nf x (K) (w) \u2212 f x (K) (w) = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)).\nFor (\u03b1, \u03b2) sufficiently small, there exists infinitely many \u03be for which f x (K) (w) \u2212f x (K) (w) < 0.\nIn particular, \u03be(x (K) ) = x (K) \u2212\u03b1G T g yields improvements\nf x (K) (w) \u2212 f x (K) (w) = \u2212 \u03b2 \u03b1 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)) < 0.\nThis is not an optimal rate; there exists infinitely many TBs that yield greater improvements.\nProof. The bootstrapped meta-gradient at (x, h, w) is given by \u2207 w \u00b5 x, x (K) (w) = D u, where u := \u2207 z \u00b5 x, z z=x (K) .\nUnder Eq. 2, we findw = w \u2212\u03b2D u. Define g := \u2207 x f (x (K) ). By first-order Taylor Series Expansion of f around (x, h,w) with respect to w:\nf x (K) (w) = f x (K) (w) + D g,w \u2212 w + o(\u03b2 2 D u 2 2 ) = f x (K) (w) \u2212 \u03b2 D g, D u + o(\u03b2 2 D u 2 2 ) = f x (K) (w) \u2212 \u03b2 u, G T g + o(\u03b2 2 u 2 G T ). (5\n)\nTo bound the inner product, expand \u00b5(x, \u2022) around a point x (K) + d, where d \u2208 R nx , w.r.t.\nx (K) : \u00b5(x, x (K) + d) = \u00b5(x, x (K) ) + u, d + o( d 2 2\n). Thus, choose d = \u2212\u03b1G T g, for some \u03b1 \u2208 R + and rearrange to get\n\u2212\u03b2 u, G T g = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b1\u03b2 g 2 G T ).\nSubstitute into Eq. 5 to obtain\nf x (K) (w) \u2212 f x (K) (w) = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) (6) + o(\u03b1\u03b2 g 2 G T + \u03b2 2 u 2 G T )\n. Thus, the BMG update comes out as the difference between to distances. The first distance is a distortion terms that measures how well the target aligns to the tangent vector \u2212G T g, which is the direction of steepest descent in the immediate vicinity of x (K) (c.f. Lemma 1). The second term measures learning; greater distance carry more signal for meta-learning. The two combined captures the inherent trade-off in BMG; moving the target further away increases distortions from curvature, but may also increase the learning signal. Finally, the residual captures distortions due to curvature.\nExistence. To show that there always exists a target that guarantees a descent direction, choos\u1ebd x = x (K) \u2212\u03b1G T g. This eliminates the first distance in Eq. 6 as the target is perfectly aligned the direction of steepest descent and we obtain\nf x (K) (w) \u2212 f x (K) (w) = \u2212 \u03b2 \u03b1 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)).\nThe residual vanishes exponentially fast as \u03b1 and \u03b2 go to 0. Hence, there is some\n(\u1fb1,\u03b2) \u2208 R 2 + such that for any (\u03b1, \u03b2) \u2208 (0,\u1fb1) \u00d7 (0,\u03b2), f x (K) (w) \u2212 f x (K) (w) < 0. For any such choice of (\u03b1, \u03b2), by virtue of differentiability in \u00b5 there exists some neighborhood N around x (K) \u2212\u03b1G T g for which anyx \u2208 N satisfy f x (K) (w) \u2212 f x (K) (w) < 0.\nEfficiency. We are to show that, given (\u03b1, \u03b2), the set of optimal targets does not includex = x (K) \u2212\u03b1G T g. To show this, it is sufficient to demonstrate that show that this is not a local minimum of the right hand-side in Eq. 6. Indeed,\n\u2207x \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b1\u03b2 g 2 G T + \u03b2 2 u 2 G T ) x=x (K) \u2212\u03b1G T g = \u2212 \u03b2 \u03b1 \u2207x \u00b5(x, x (K) ) x=x (K) \u2212\u03b1G T g + \u03b2 2 o = 0,\nwhere \u03b2 2 o is the gradient of the residual ( u 2 2 depends onx) w.r.t.x = x (K) \u2212\u03b1G T g. To complete the proof, let\u0169 denote the above gradient. Construct an alternative targetx =x \u2212 \u03b7\u0169 for some \u03b7 \u2208 R + . By standard gradient descent argument, there is some\u03b7 such that any \u03b7 \u2208 (0,\u03b7) yields an alternate targetx that improves overx.\nWe now prove that, controlling for scale, BMG can yield larger performance gains than MG. Recall that \u03be \u03b1 G (x (K) ) = x (K) \u2212\u03b1G T \u2207f x (K) . Consider \u03be r G , with r := \u2207f (x K) ). Let w be given by Eq. 1 andw be given by Eq. 2. For \u03b2 sufficiently small, f x (K) (w) \u2264 f x (K) (w ) , with strict inequality if\n(K) ) 2 / G T \u2207f (x (K) ) 2 . Corollary 1. Let \u00b5 = \u2022 2 2 andx = \u03be r G (x(\nGG T = G T . Proof. Let g := \u2207 x f x (K) . By Lemma 1, f x (K) (w ) \u2212f x (K) (w) = \u2212\u03b2 G T g, g +O(\u03b2 2 ). From Theorem 1, with \u00b5 = \u2022 2 2 , f x (K) (w) \u2212 f x (K) (w) = \u2212r G T g, G T g + O(\u03b2(\u03b1 + \u03b2))\n. For \u03b2 sufficiently small, the inner products dominate and we have f\nx (K) (w) \u2212 f x (K) (w ) \u2248 \u2212\u03b2 r G T g, G T g \u2212 G T g, g .\nTo determine the sign of the expression in parenthesis, consider the problem\nmax v\u2208R nx G T g, v s.t. v 2 \u2264 1. Form the Lagrangian L(v, \u03bb) := G T g, v \u2212 \u03bb( v 2 \u2212 1)\n. Solve for first-order conditions:\nG T g \u2212\u03bb v * v * 2 = 0 =\u21d2 v * = v * 2 \u03bb G T g .\nIf \u03bb = 0, then we must have v * 2 0, which clearly is not an optimal solution. Complementary slackness then implies v * 2 = 1, which gives \u03bb = v * 2 G T g 2 and hence v * = G T g / G T g 2 . By virtue of being the maximiser, v * attains a higher function value than any other v with v 2 \u2264 1, in particular v = g / g 2 . Evaluating the objective at these two points gives\nG T g, G T g G T g 2 \u2265 G T g, g g 2 =\u21d2 r G T g, G T g \u2265 G T g, g ,\nwhere we use that r = g 2 / G T g 2 by definition. Thus f x (K) (w) \u2264 f x (K) (w ) , with strict inequality if GG T = G T and G T g = 0. This experiment is designed to provide a controlled setting to delineate the differences between standard meta-gradients and bootstrapped meta-gradients. The environment is a 5\u00d75 grid world with two objects; a blue and a red square (Figure 7). Thus, we refer to this environment as the two-colors domain. At each step, the agent (green) can take an action to move either up, down, left, or right and observes the position of each square and itself. If the agent reaches a coloured square, it obtains a reward of either +1 or \u22121 while the colour is randomly moved to an unoccupied location. Every 100 000 steps, the reward for each object flips. For all other transitions, the agent obtains a reward of \u22120.04. Observations are constructed by concatenating one-hot encodings of the each xand y-coordinate of the two colours and the agent's position, with a total dimension of 2 \u00d7 3 \u00d7 5 = 30 (two coordinates for each of three objects, with each one-hot vector being 5-dimensional).\nThe two-colors domain is designed such that the central component determining how well a memory-less agent adapts is its exploration. Our agents can only regulate exploration through policy entropy. Thus, to converge on optimal task behaviour, the agent must reduce policy entropy. Once the task switches, the agent encounters what is effectively a novel task (due to it being memory-less). To rapidly adapt the agent must first increase entropy in the policy to cover the state-space. Once the agent observe rewarding behaviour, it must then reduce entropy to converge on task-optimal behaviour.\nAll experiments run on the CPU of a single machine. The agent interacts with the environment and update its parameters synchronously in a single stream of experience. A step is thus comprised of the following operations, in order: (1) given observation, agent takes action, (2) if applicable, agent update its parameters, (3) environment transitions based on action and return new observation. The parameter update step is implemented differently depending on the agent, described below. \nfor k = 1, 2, . . . , K do s, B \u2190 ActorLoop(x, s, N ) Algorithm 1. (x, z) \u2190 \u03d5((x, z), B, w)\nInner update step. end for return s, x, z, B Algorithm 3 Online RL with BMG Require: N, K, L Rollout length, meta-update length, bootstrap length.\nRequire: x \u2208 R nx , z \u2208 R nz , w \u2208 R nw\nPolicy, value function, and meta parameters. Require: s Environment state. u \u2190 (x, z) while True do s, u (K) , _ \u2190 InnerLoop(u, w, s, N, K) K-step inner loop, Algorithm 2. s, u (K+L\u22121) , B \u2190 InnerLoop(u (K) , w, s, N, L \u2212 1)\nL \u2212 1 bootstrap, Algorithm 2. u \u2190 u (K+L\u22121) \u2212\u03b1\u2207 u (u (K+L\u22121) , B)\nGradient step on objective . w \u2190 w \u2212\u03b2\u2207 w \u00b5(\u0169, u (K) (w)) BMG outer step. u \u2190 u (K+L\u22121)\nContinue from most resent parameters. end while", "publication_ref": ["b3"], "figure_ref": [], "table_ref": []}, {"heading": "B.2 ACTOR-CRITIC EXPERIMENTS", "text": "Agent The first agent we evaluate is a simple actor-critic which implements a softmax policy (\u03c0 x ) and a critic (v z ) using separate feed-forward MLPs. Agent parameter updates are done according to the actor-critic loss in Eq. 4 with the on-policy n-step return target. For a given parameterisation of the agent, we interact with the environment for N = 16 steps, collecting all observations, rewards, and actions into a rollout (Algorithm 1). When the rollout is full, the agent update its parameters under the actor-critic loss with SGD as the optimiser (Algorithm 2). To isolate the effect of meta-learning, all hyper-parameters except the entropy regularization weight ( = EN ) are fixed (Table 1); for each agent, we sweep for the learning rate that yields highest cumulative reward within a 10 million step budget. For the non-adaptive baseline, we additionally sweep for the best regularization weight.\nMeta-learning To meta-learn the entropy regularization weight, we introduce a small MLP with meta-parameters w that ingests a statistic t of the learning process-the average reward over each of the 10 most recent rollouts-and predicts the entropy rate w (t) \u2208 R + to use in the agent's parameter update of x. To compute meta-updates, for a given horizon T = K or T = K + (L \u2212 1), we fix w and make T agent parameter updates to obtain a sequence (\u03c4 1 , x (1) , z (1) , . . . , \u03c4 T , x (T ) , z (T ) ). MG is optimised by averaging each policy and entropy loss encountered in the sequence, i.e. the meta-objective is given by\n1 T T t=1 t PG (x (t) (w)) + meta t EN (x (t) (w))\n, where meta \u2208 {0, 0.1} is a fixed hyper-parameter and t implies that the objective is computed under \u03c4 t .\nBMG is optimised by computing the matching loss \u00b5 \u03c4 T (x, x (K) (w)), wherex is given byx =\nx (T ) \u2212\u03b2\u2207 x ( T PG (x (T ) ) + meta T EN (x (T )\n)). That is to say, the TB \"unrolls\" the meta-learner for L \u2212 1 steps, starting from (x (K) , z (K) ), and takes a final policy-gradient step ( meta = 0 unless otherwise noted). Thus, in this setting, our TB exploits that the first (L \u2212 1) steps have already been taken by the agent during the course of learning (Algorithm 3). Moreover, the final Lth step only differs in the entropy regularization weight, and can therefore be implemented without an extra gradient computation. As such, the meta-update under BMG exhibit no great computational overhead to the MG update. In practice, we observe no significant difference in wall-clock speed for a given K.\nMain experiment: detailed results The purpose of our main experiment Section 5.1 is to (a) test whether larger meta-learning horizons-particularly by increasing L-can mitigate the short-horizon bias, and (b) test whether the agent can learn an exploration schedule without explicit domain knowledge in the meta-objective (in the form of entropy regularization). As reported in Section 5.1, we find the answer to be affirmative in both cases. To shed further light on these findings, Figure 8 0.0 0.2 0.4 0.6 0.   Figure 10: Ablations for actor-critic agent with BMG. Each shaded area shows the range of entropy regularization weights generated by the meta-learner. The range is computed as the difference between at the beginning and end of each reward-cycle. Left: entropy regularization weight range when K = 1 and L = 7. Center: entropy regularization weight range when K = 1 and L = 1. Right: For K = 1 effect of increasing L with or without meta-entropy regularization. Result aggregated over 50 seeds.\nreports cumulative reward curves for our main experiment in Section 5.1. We note that MG tends to collapse for any K unless the meta-objective is explicitly regularized via meta . To characterise why MG fail for meta = 0, Figure 9 portrays the policy entropy range under either MG or BMG. MG is clearly overly myopic by continually shrinking the entropy range, ultimately resulting in a non-adaptive policy. Ablation: meta-regularization To fully control for the role of meta-regularization, we conduct further experiments by comparing BMG with and without entropy regularization (i.e. meta ) in the Lth target update step. Figure 10 demonstrates that BMG indeed suffers from myopia when L = 1, resulting in a collapse of the entropy regularization weight range. However, increasing the meta-learning horizon by setting L = 7 obtains a wide entropy regularization weight range. While adding metaregularization does expand the range somewhat, the difference in total return is not statistically significant (right panel, Figure 10).\nAblation: target bootstrap Our main TB takes L \u2212 1 steps under the meta-learned update rule, i.e. the metalearned entropy regularization weight schedule, and an Lth policy-gradient step without entropy regularization.\nIn this ablation, we very that taking a final step under a different update rule is indeed critical. Figure 10 shows that, for K = 1 and L \u2208 {1, 7}, using the meta-learned update rule for all target update steps leads to a positive feedback loop that results in maximal entropy regularization, leading to a catastrophic loss of performance (right panel, Figure 10).\nAblation: matching function Finally, we control for different choices of matching function.\nFigure 11 contrasts the mode-covering version, KL-1, with the mode-seeking version, KL-2, as well as the symmetric KL. We observe that, in this experiment, this choice is not as significant as in other experiments. However, as in Atari, we find a the mode-covering version to perform slightly better.", "publication_ref": [], "figure_ref": ["fig_5", "fig_0", "fig_6", "fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": ["tab_5"]}, {"heading": "B.3 Q-LEARNING EXPERIMENTS", "text": "Agent In this experiment, we test Peng's Q(\u03bb) (Peng & Williams, 1994) agent with \u03b5-greedy exploration. The agent implements a feed-forward MLP to represent a Q-function q x that is optimised online. Thus, agent parameter update steps do not use batching but is done online (i.e. on each step).\nTo avoid instability, we use a momentum term that maintains an Exponentially Moving Average (EMA) over the agent parameter gradient. In this experiment we fix all hyper-parameters of the update rule (Table 1) and instead focuses on meta-learned \u03b5-greedy exploration. BMG We implement BMG in a similar fashion to the actor-critic case. The meta-learner is represented by a smaller MLP \u03b5 w (\u2022) with meta-parameters w that ingests the last 50 rewards, denoted by t, and outputs the \u03b5 to use on the current time-step. That is to say, given meta-parameters w, the agent's policy is defined by\n\u03c0 x (a | s t , t t , w) = \uf8f1 \uf8f2 \uf8f3 1 \u2212 \u03b5 w (t t ) + \u03b5w(tt) |A| if a = arg max b q x (s t , b) \u03b5w(tt) |A| else.\nPolicy-matching This policy can be seen as a stochastic policy which takes the Q-maximizing action with probability 1 \u2212 \u03b5 and otherwise picks an action uniformly at random. The level of entropy in this policy is regulated by the meta-learner. We define a TB by defining a target policy under qx, wherex is given by taking L update steps. Since there are no meta-parameters in the update rule, all L steps use the same update rule. However, we define the target policy as the greedy policy\n\u03c0x(a | s t ) = \uf8f1 \uf8f2 \uf8f3 1 if a = arg max b qx(s t , b) 0 else.\nThe resulting BMG update is simple: minimize the KL-divergence \u00b5 \u03c0 (x, x) := KL (\u03c0x \u03c0 x ) by adjusting the entropy in \u03c0 x through \u03b5 w . Thus, policy-matching under this target encourages the meta-learner to match a greedy policy-improvement operation on a target qx that has been trained for a further L steps. More specifically, if arg max b qx(s, b) = arg max b q x (s, b), so that the greedy policy improvement matches the target, then the matching loss is minimised by setting \u03b5 = 0. If greedy policy improvement does not correspond, so that acting greedily w.r.t. q x does not match the target, then the matching loss is minimised by increasing entropy, i.e. increasing \u03b5. The meta-objective is defined in terms of x as it does not require differentiation through the update-rule.\n'Value'-matching A disadvantage of policy matching is that it provides a sparse learning signal: \u03b5 is increased when the target-policy differs from the current policy and decreased otherwise. The magnitude of the change depends solely on the current value of \u03b5. It is therefore desirable to evaluate alternative matching functions that provide a richer signal. Inspired by value-matching for actor-critic agents, we construct a form of 'value' matching by taking the expectation over q x under the induced stochastic policy, u x (s) := a\u2208A \u03c0 x (a | s)q x (s, a). The resulting matching objective is given by\n\u00b5 u (x, x) = E (ux(s) \u2212 u x (s; t, w)) 2 .\nWhile the objective is structurally similar to value-matching, u does not correspond to well-defined value-function since q x is not an estimate of the action-value of \u03c0 x .", "publication_ref": ["b20"], "figure_ref": [], "table_ref": ["tab_5"]}, {"heading": "Detailed results", "text": "Figure 12 shows the learned \u03b5-schedules for different meta-learning horizons: if L is large enough, the agent is able to increase exploration when the task switches and quickly recovers a near-optimal policy for the current cycle. Figure 12 further shows that a richer matching function, in this case in the form of 'value' matching, can yield improved performance.  (Xu et al., 2018;Zahavy et al., 2020), we treat each game level as a separate learning problem; the agent is randomly initialized at the start of each learning run and meta-learning is conducted online during learning on a single task, see Algorithm 6. We evaluate final performance between 190-200 million frames. All experiments are conducted with 3 independent runs under different seeds. Each of the 57 levels in the Atari suite is a unique environment with distinct visuals and game mechanics. Exploiting this independence, statistical tests of aggregate performance relies on a total sample size per agent of 3 \u00d7 57 = 171.\nAgent We use a standard feed-forward agent that received a stack of the 4 most recent frames (Mnih et al., 2013) and outputs a softmax action probability along with a value prediction. The agent is implemented as a deep neural network; we use the IMPALA network architecture without LSTMs, with larger convolution kernels to compensate for more a complex input space, and with a larger conv-to-linear projection. We add experience replay (as per (Schmitt et al., 2020)) to allow multiple steps on the target. All agents use the same number of online samples; unless otherwise stated, they also use the same number of replay samples. We ablate the role of replay data in Appendix C.2.\nSTACX The IMPALA agent introduces specific form of importance sampling in the actor critic update and while STACX largely rely on the same importance sampling mechanism, it differs slightly to facilitate the meta-gradient flow. The actor-critic update in STACX is defined by Eq. 4 with the following definitions of \u03c1 and G. Let\u03c1 \u2265c \u2208 R + be given and let \u03bd : S \u00d7 A \u2192 [0, 1] represent the behaviour policy that generated the rollout. Given \u03c0 x and vz, define the Leaky V-Trace target by\n\u03b7 t := \u03c0 x (a t | s t ) / \u03bd(a t | s t ) \u03c1 t := \u03b1 \u03c1 min{\u03b7 t ,\u03c1} + (1 \u2212 \u03b1 \u03c1 )\u03b7 t c i := \u03bb (\u03b1 c min{\u03b7 i ,c} + (1 \u2212 \u03b1 c )\u03b7 i ) \u03b4 t := \u03c1 t (\u03b3vz(s t+1 ) + r t+1 \u2212 vz(s t )) G (n) t = vz(s t ) + (n\u22121) i=0 \u03b3 i \uf8eb \uf8ed i\u22121 j=0 c t+j \uf8f6 \uf8f8 \u03b4 t+i ,\nwith \u03b1 \u03c1 \u2265 \u03b1 c . Note that-assumingc \u2265 1 and \u03bb = 1-in the on-policy setting this reduces to the n-step return since \u03b7 t = 1, so \u03c1 t = c t = 1. The original v-trace target sets \u03b1 \u03c1 = \u03b1 c = 1.\nSTACX defines the main \"task\" as a tuple (\u03c0 0 , v 0 , f (\u2022, w 0 )), consisting of a policy, critic, and an actor-critic objective (Eq. 4) under Leaky V-trace correction with meta-parameters w 0 . Auxiliary tasks are analogously defined tuples (\u03c0 i , v i , f (\u2022, w i )), i \u2265 1. All policies and critics share the same feature extractor but differ in a separate MLP for each \u03c0 i and v i . The objectives differ in their hyper-parameters, with all hyper-parameters being meta-learned. Auxiliary policies are not used for acting; only the main policy \u03c0 0 interacts with the environment. The objective used to update the agent's parameters is the sum of all tasks (each task is weighted through PG , EN , TD ). The objective used for the MG update is the original IMPALA objective under fixed hyper-parameters p (see Meta-Optimisation in Table 2). Updates to agent parameters and meta-parameters happen simultaneously on rollouts \u03c4 . Concretely, let m denote parameters of the feature extractor, with (x i , z i ) denoting parameters of task i's policy MLP and critic MLP. Let u i := (m, x i , z i ) denote parameters of (\u03c0 i , v i ), with u := (m, x 0 , z 0 , . . . x n , z n ). Let w = (w 0 , . . . , w n ) and denote by h auxiliary vectors of the optimiser. Given (a batch of) rollout(s) \u03c4 , the STACX update is given by\nu (1) , h (1) u = RMSProp (u, h u , g u ) g u = \u2207 u n i=1 f \u03c4 u i ; w i w (1) , h (1) w = Adam (w, h w , g w ) g w = \u2207 w f \u03c4 u\n(1) 0 (w); p .\nBMG We use the same setup, architecture, and hyper-parameters for BMG as for STACX unless otherwise noted; the central difference is the computation of g w . For L = 1, we compute the  We report human normalized score (median, quantiles, 1 2 IQR) between 190-200M frames over all 57 games, with 3 independent runs for each configuration.\nIn this section, we decompose the BMG agent to understand where observed gains come from. To do so, we begin by noting that-by virtue of Eq. 3-STACX is a special case of BMG under \u00b5(\u0169, u\n(1) 0 (w)) = \u0169 \u2212 u (1) 0 (w) 2 2 with\u0169 = u (1) 0 \u2212 1 2 \u2207 u f \u03c4 (u(1)\n0 ; p). That is to say, if the target is generated by a pure SGD step and the matching function is the squared L2 objective. We will refer to this configurations as SGD, L2. From this baseline-i.e. STACX-a minimal change is to retain the matching function but use RMSProp to generate the target. We refer t o this configuration as RMS, L2. From Corollary 1, we should suspect that correcting for curvature should improve performance. While RMSProp is not a representation of the metric G in the analysis, it is nevertheless providing some form of curvature correction. The matching function can then be used for further corrections.\nFigure 13 shows that changing the target update rule from SGD to RMSProp, thereby correcting for curvature, yields a substantial gain. This supports our main claim that BMG can control for curvature and thereby facilitate metaoptimisation. Using the squared Euclidean distance in parameter space (akin to (Nichol et al., 2018;Flennerhag et al., 2019)) is surprisingly effective. However, it exhibits substantial volatility and is prone to crashing (c.f. Figure 15); changing the matching function to policy KL-divergence stabilizes meta-optimisation. Pure policy-matching leaves the role of the critic-i.e. policy evaluation-implicit. Having an accurate value function approximation is important to obtain high-quality policy gradients. It is therefore unsurprising that adding value matching provides a statistically significant improvement. Finally, we find that BMG can also mitigate myopia by extending the meta-learning horizon, in our TB by unrolling the meta-learned update rule for L \u2212 1 steps. This is roughly as important as correcting for curvature, in terms of the relative performance gain.\nTo further support these findings, we estimate the effect BMG has on ill-conditioning and metagradient variance on three games where both STACX and BMG exhibit stable learning (to avoid confounding factors of non-stationary dynamics): Kangaroo, Star Gunner, and Ms Pacman. While the Hessian of the meta-gradient is intractable, an immediate effect of ill-conditioning is gradient interference, which we can estimate through cosine similarity between consecutive meta-gradients. We estimate meta-gradient variance on a per-batch basis. Table 3 presents mean statistics between 50M and 150M frames, with standard deviation over 3 seeds. BMG achieves a meta-gradient cosine similarity that is generally 2 orders of magnitude larger than that of STACX. It also explicitly demonstrates that using the KL divergence as matching function results in better curvature relative to using the L2 distance. The variance of the meta-gradient is larger for BMG than for STACX (under KL). This is due to intrinsically different gradient magnitudes. To make comparisons, we report the gradient norm to gradient variance ratio, which roughly indicates signal to noise. We note that in this metric, BMG tends to be on par with or lower than that of STACX. We find that extending the meta-learning horizon by taking more steps on the target leads to large performance improvements. To obtain these improvements, we find that it is critical to re-sample replay data for each step, as opposed to re-using the same data for each rollout. Figure 14 demonstrates this for L = 4 on MsPacman. This can be explained by noting that reusing data allows the target to overfit to the current batch. By re-sampling replay data we obtain a more faithful simulation of what the meta-learned update rule would produce in L \u2212 1 steps.", "publication_ref": ["b45", "b47", "b15", "b28", "b16"], "figure_ref": ["fig_0", "fig_0", "fig_0", "fig_0", "fig_0"], "table_ref": ["tab_6", "tab_7"]}, {"heading": "C.2 EFFECT", "text": "The amount of replay data is a confounding factor in the meta-objective. We stress that the agent parameter update is always the same in any experiment we run. That is to say, the additional use of replay data only affects the computation of the meta-objective. To control for this additional data in the meta-objective, we consider a subset of games where we see large improvements from L > 1.\nWe run STACX and BMG with L = 1, but increase the amount of replay data used to compute the meta-objective to match the total amount of replay data used in the metaobjective when L = 4. This changes the online-to-replay ratio from 6 : 12 to 6 : 48 in the meta objective.\nFigure 15 shows that the additional replay data is not responsible for the performance improvements we see for L = 4. In fact, we find that increasing the amount of replay data in the meta-objective exacerbates off-policy issues and leads to reduced performance. It is striking that BMG can make use of this extra off-policy data. Recall that we use only off-policy replay data to take the first L \u2212 1 steps on the target, and use the original online-to-replay ratio (6 : 12) in the Lth step. In Figure 14, we test the effect of using only replay for all L steps and find that having online data in the Lth update step is critical. These results indicate that BMG can make effective use of replay by simulating the effect of the meta-learned update rule on off-policy data and correct for potential bias using online data.", "publication_ref": [], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "C.3 L VS K", "text": "Given that increasing L yields substantial gains in performance, it is interesting to compare against increasing K, the number of agent parameter updates to backpropagate through. For fair comparison, we use an identical setup as for L > 1, in the sense that we use new replay data for each of the initial K \u2212 1 steps, while we use the default rollout \u03c4 for the Kth step. Hence, the data characteristics for K > 1 are identical to those of L > 1.\nHowever, an important difference arise because each update step takes K steps on the agent's parameters. This means that-withing the 200 million frames budget, K > 1 has a computational advantage as it is able to do more updates to the agent's parameters. With that said, these additional K \u2212 1 updates use replay data only. Figure 15: Atari experience replay ablation. We report episode returns, normalized to be in the range [0, max return] for each game for ease of comparison. Shading depicts standard deviation across 3 seeds. D denotes default BMG configuration for L = 1, with L = 4 analgously defined. R denotes L = 1, but with additional replay in the meta-objective to match the amount of replay used in L = 4.\nFigure 16 demonstrates that increasing K is fundamentally different from increasing L. We generally observe a loss of performance, again due to interference from replay. This suggests that target bootstrapping allows a fundamentally different way of extending the meta-learning horizon. In particular, these results suggests that meta-bootstrapping allows us to use relatively poor-quality (as evidence by K > 1) approximations to long-term consequences of the meta-learned update rule without impairing the agent's actual parameter update. Finally, there are substantial computational gains from increasing the meta-learning horizon via L rather than K (Figure 17).", "publication_ref": [], "figure_ref": ["fig_0", "fig_0", "fig_0"], "table_ref": []}, {"heading": "C.4 COMPUTATIONAL CHARACTERISTICS", "text": "IMPALA's distributed setup is implemented on a single machine with 56 CPU cores and 8 TPU (Jouppi et al., 2017) cores. 2 TPU cores are used to act in 48 environments asynchronously in parallel, sending rollouts to a replay buffer that a centralized learner use to update agent parameters and meta-parameters. Gradient computations are distributed along the batch dimension across the remaining 6 TPU cores. All Atari experiments use this setup; training for 200 millions frames takes 24 hours.\nFigure 17 describes the computational properties of STACX and BMG as a function of the number of agent parameters and the meta-learning horizon, H. For STACX, the meta-learning horizon is defined by the number of update steps to backpropagate through, K. For BMG, we test one version which holds L = 1 fixed and varies K, as in for STACX, and one version which holds K = 1 ficed and varies L. To control for network size, we vary the number of channels in the convolutions of the network. We use a base of channels per layer, x = (16, 32, 32, 16), that we multiply by a factor 1, 2, 4. Thus we consider networks with kernel channels 1x = (16, 32, 32, 16), 2x = (32, 64, 64, 32), and 4x = (64, 128, 128, 64). Our main agent uses a network size (Table 2) equal to 4x. We found that larger networks would not fit into memory when K > 1.\nFirst, consider the effect of increasing K (with L = 1 for BMG). For the small network (1x), BMG is roughly on par with STACX for all values of K considered. However, BMG exhibits poorer scaling Figure 16: Atari K vs L ablation. We report episode returns, normalized to be in the range [0, max return] for each game for ease of comparison. Shading depicts standard deviation across 3 seeds. D denotes default BMG configuration for L = 1, with L = 4 analogously defined. K = 2 denotes L = 1, but K = 2 steps on agent parameters. in network size, owing to the additional update step required to compute the target bootstrap. For 4x, our main network configuration, we find that BMG is 20% slower in terms of wall-clock time. Further, we find that neither STACX nor BMG can fit the 4x network size in memory when K = 8.\nSecond, consider the effect of increasing L with BMG (with K = 1). For 1x, we observe no difference in speed for any H. However, increasing L exhibits a dramatic improvement in scaling for H > 2-especially for larger networks. In fact, L = 4 exhibits a factor 2 speed-up compared to STACX for H = 4, 4x and is two orders of magnitude faster for H = 8, 2x.", "publication_ref": ["b5"], "figure_ref": ["fig_0", "fig_0"], "table_ref": ["tab_6"]}, {"heading": "C.5 ADDITIONAL RESULTS", "text": "Figure 19 presents per-game results learning curve for main configurations considered in this paper.  configurations. Finally, we consider two variations of BMG in the L = 1 regime (Figure 18); one version (NS) re-computes the agent update after updating meta-parameters in a form of trust-region method. The other version (DB) exploits that the target has a taken a further update step and uses the target as new agent parameters. While NS is largely on par, interestingly, DB fails completely.  (Mnih et al., 2013;Espeholt et al., 2018;Zahavy et al., 2020;Schmitt et al., 2020). Network, optimisation and meta-optimisation hyper-parameters are based on the original STACX implementation and tuned for optimal performance. Our median human normalized score matches published results. For BMG, we did not tune these hyper-parameters, except for L > 1. In this case, we observed that unique replay data in the initial L \u2212 1 steps was necessary to yield any benefits. We observed a tendency to crash, and thus reduced the gradient clipping ratio from .3 to .2. For BMG configurations that use both policy and value matching, we tuned the weight on value matching by a grid search over {0.25, 0.5, 0.75} on Ms Pacman, Zaxxon, Wizard of Wor, and Seaquest, with 0.25 performing best.\nD MULTI-TASK META-LEARNING D.1 PROBLEM FORMULATION Let p(\u03c4 ) denote a given task distribution, where \u03c4 \u2208 N indexes a task f \u03c4 . Each task is also associated with distinct learner states h \u03c4 and task parameters x \u03c4 , but all task learners use the same meta-learned update rule defined by meta-parameters w. Hence, the meta-learner's problem is again to learn an update rule, but now in expectation over all learning problems. The MG update (Eq. 1) thus takes the form w = w \u2212\u03b2\u2207 w E \u03c4 [f \u03c4 (x\n(K) \u03c4 (w))]\n, where the expectation is with respect to (f \u03c4 , h \u03c4 , x \u03c4 ) and x\n(K) \u03c4 (w) is the K-step update on task \u03c4 given (f \u03c4 , h \u03c4 , x \u03c4 ). Since p(\u03c4 ) is independent of w, this update becomes w = w \u2212\u03b2E \u03c4 [\u2207 w f \u03c4 (x (K)\n\u03c4 (w))], i.e. the single-task meta-gradient in Section 3 in expectation over the task distribution.\nWith that said, the expectation involves integrating over (h \u03c4 , x \u03c4 ). This distribution is defined differently depending on the problem setup. In few-shot learning, x \u03c4 and h \u03c4 are typically a shared initialisations (Finn et al., 2017;Nichol et al., 2018;Flennerhag et al., 2019) and f \u03c4 differ in terms of the data (Vinyals et al., 2016). However, it is possible to view the expectation as a prior distribution over task parameters (Grant et al., 2018;Flennerhag et al., 2020). In online multi-task learning, this expectation often reduces to an expectation over current task-learning states (Rusu et al., 2015;Denevi et al., 2019).\nThe BMG update is analogously defined. Given a TB \u03be, define the task-specific targetx \u03c4 given x , where \u00b5 \u03c4 is the matching loss defined on task data from \u03c4 . Hence, as with MG, the multi-task BMG update is an expectation over the single-task BMG update in Section 3. See Algorithm 7 for a detailed description.\nTable 6: Effect of BMG on ill-conditioning and meta-gradient variance on 5-way-5-shot MiniImagenet. Estimated meta-gradient cosine similarity (\u03b8) between consecutive gradients, meta-gradient variance (V), and meta-gradient norm to variance ratio (\u03c1). Standard deviation across 5 independent seeds.  In terms of data-efficiency, Table 7 reports best hyper-parameters for each data budget. For both BMG and MG, we note that small budgets rely on fewer steps to backpropagate through and a higher learning rate. BMG tends to prefer a higher target bootstrap in this regime. MG switches to backpropagation through K > 1 sooner than BMG, roughly around 70 000 meta-updates, while BMG switches around 120 000 meta-updates. This accounts for why BMG can achieve higher performance faster, as it can achieve similar performance without backpropagating through more than one update. It is worth noting that as BMG is given larger training budgets, to prevent meta-overfitting, shorter target bootstraps generalize better. We find that other hyper-parameters are not important for overall performance.", "publication_ref": ["b15", "b47", "b28", "b16", "b40", "b24"], "figure_ref": ["fig_0", "fig_0"], "table_ref": []}, {"heading": "MAML BMG", "text": "K L \u03b8 V \u03c1 \u03b8 V \u03c1 1 1\nIn terms of computational efficiency, Table 7 reports best hyper-parameters for each time budget. The pattern here follows a similar trend. MG does btter under a lower learning rate already after 4 hours, whereas BMG switches after about 8 hours. This data highlights the dominant role K plays in determining training time.\nWe compare wall-clock time per meta-training step for various values of K and L Table 5. In our main configuration, i.e. K = 5, L = 10, BMG achieves a throughput of 2.5 meta-training steps per second, compared to 4.4 for MAML, making BMG 50% slower. In this setting, BMG has an effective meta-learning horizon of 15, whereas MAML has a horizon of 5. For MAML to achieve an effective horizon of 15, it's throughput would be reduced to 1.4, instead making MAML 56% slower than BMG.\nFinally, we conduct a similar analysis as on Atari (Appendix C.1) to study the effect BMG has on ill-conditioning and meta-gradient variance. We estimate ill-conditioning through cosine similarity between consecutive meta-gradients, and meta-gradient variance on a per meta-batch basis. We report mean statistics for the 5-way-5-shot setup between 100 000 and 150 000 meta-gradient steps, with standard deviation over 5 independent seeds, in Table 6. Unsurprisingly, MAML and BMG are similar in terms of curvature, as both can have a KL-divergence type of meta-objective. BMG obtains greater cosine similarity as L increases, suggesting that BMG can transfer more information by having a higher temperature in its target. However, BMG exhibits substantially lower meta-gradient variance, and the ratio of meta-gradient norm to variance is an order of magnitude larger.\nTable 7: Data-efficiency: mean meta-test accuracy over 3 seeds for best hyper-parameters per data budget. \u00b5 = 1 corresponds to KL (x \u2022) and \u00b5 = 2 to KL (\u2022 x).\nStep  ", "publication_ref": [], "figure_ref": [], "table_ref": ["tab_10"]}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors would like to thank Guillaume Desjardins, Junhyuk Oh, Louisa Zintgraf, Razvan Pascanu, and Nando de Freitas for insightful feedback on earlier versions of this paper. The author are also grateful for useful feedback from anonymous reviewers, that helped improve the paper and its results. This work was funded by DeepMind.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "", "text": "bootstrapped meta-gradient under \u00b5 \u03c4 on data \u03c4 by g w = \u2207 w \u00b5 \u03c4 \u0169 0 , u (1) 0 (w) , where \u0169 0 , _ = RMSProp u\n(1) 0 , h (1) u , \u2207 u f \u03c4 u\n(1) 0 ; p .\nNote that the target uses the same gradient \u2207 u f (u (1) 0 ; p) as the outer objective in STACX; hence, BMG does not use additional gradient information or additional data for L = 1. The only extra computation is the element-wise update required to compute\u0169 0 and the computation of the matching loss. We discuss computational considerations in Appendix C.4. For L > 1, we take L \u2212 1 step under the meta-learned objective with different replay data in each update. To write this explicitly, let \u03c4 be the rollout data as above. Let\u03c4 (l) denote a separate sample of only replay data used in the lth target update step. For L > 1, the TB is described by the process\nTargets and corresponding momentum vectors are discarded upon computing the meta-gradient. This TB corresponds to following the meta-learned update rule for L \u2212 1 steps, with a final step under the IMPALA objective. We show in Appendix C.3 that this final step is crucial to stabilise meta-learning. For pseudo-code, see Algorithm 6.\nMatching functions are defined in terms of the rollout \u03c4 and with targets defined in terms of the main task u 0 . Concretely, we define the following objectives:\n0 (w) , \u03bb = 0.25, \nOptional: continue from K + L \u2212 1 update. Send parameters x from learner to actors. end while", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "D.2 FEW-SHOT MINIIMAGENET", "text": "Setup MiniImagenet (Vinyals et al., 2016;Ravi & Larochelle, 2017) is a sub-sample of the Imagenet dataset (Deng et al., 2009). Specifically, it is a subset of 100 classes sampled randomly from the 1000 classes in the ILSVRC-12 training set, with 600 images for each class. We follow the standard protocol (Ravi & Larochelle, 2017) and split classes into a non-overlapping meta-training, meta-validation, and meta-tests sets with 64, 16, and 20 classes in each, respectively. The datasset is licenced under the MIT licence and the ILSVRC licence. The dataset can be obtained from https://paperswithcode. com/dataset/miniimagenet-1. M -shot-N -way classification tasks are sampled following standard protocol (Vinyals et al., 2016). For each task, M = 5 classes are randomly sampled from the train, validation, or test set, respectively. For each class, K observations are randomly sampled without replacement. The task validation set is constructed similarly from a disjoint set of L = 5 images per class. We follow the original MAML protocol for meta-training (Finn et al., 2017), taking K task adaptation steps during meta-training and 10 adaptation steps during meta testing.\nWe study how the data-efficiency and computational efficiency of the BMG meta-objective compares against that of the MG meta-objective. To this end, for data efficiency, we report the meta-test set performance as we vary the number of meta-batches each algorithm is allow for meta-training. As more meta-batches mean more meta-tasks, this metric captures how well they leverage additional data. For computational efficiency, we instead report meta-test set performace as a function of total meta-training time. This metric captures computational trade-offs that arise in either method.\nFor any computational budget in either regime (i.e. N meta-batches or T hours of training), we report meta-test set performance across 3 seeds for the hyper-configuration with best validation performance (Table 4). This reflects the typical protocol for selecting hyper-parameters, and what each method would attain under a given budget. For both methods, we sweep over the meta-learning rate \u03b2; for shorter training runs, a higher meta-learning is critical to quickly converge. This however lead to sub-optimal performance for larger meta-training budgets, where a smaller meta-learning rate can produce better results. The main determinant for computational cost is the number of steps to backpropagate through, K. For BMG, we sweep over K \u2208 {1, 5}. For MG, we sweep over K \u2208 {1, 5, 10}. We allow K = 10 for MAML to ensure fair comparison, as BMG can extend its effective meta-learning horizon through the target bootstrap; we sweep over L \u2208 {1, 5, 10}. Note that the combination of K and L effectively lets BMG interpolate between different computational trade-offs. Standard MG does not have this property, but several first-order approximations have been proposed: we allow the MG approach to switch from a full meta-gradient to either the FOMAML approximation (Finn et al., 2017) or the ANIL approximation (Raghu et al., 2020).\nModel, compute, and shared hyper-parameters We use the standard convolutional model (Vinyals et al., 2016), which is a 4-layer convolutional model followed by a final linear layer. Each convolutional layer is defined by a 3 \u00d7 3 kernel with 32 channels, strides of 1, with batch normalisation, a ReLU activation and 2 \u00d7 2 max-pooling. We use the same hyper-parameters of optimisation and meta-optimisation as in the original MAML implementation except as specified in Table 4. Each model is trained on a single machine and runs on a V100 NVIDIA GPU. \nSample batch of task test data. ", "publication_ref": ["b40", "b22", "b22", "b40", "b21", "b40"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "Muesli: Combining Improvements in Policy Optimization", "journal": "", "year": "2021", "authors": "Abbas Abdolmaleki; Jost Tobias Springenberg; Yuval Tassa; Remi Munos; Nicolas Heess; Martin Matteo Hessel; Ivo Danihelka; Fabio Viola; Arthur Guez; Simon Schmitt; Laurent Sifre; Theophane Weber; David Silver; Hado Van Hasselt"}, {"ref_id": "b1", "title": "Distilling the Knowledge in a Neural Network", "journal": "", "year": "2015", "authors": "Geoffrey Hinton; Oriol Vinyals; Jeff Dean"}, {"ref_id": "b2", "title": "Using Fast Weights to Deblur Old Memories", "journal": "", "year": "1987", "authors": "Geoffrey E Hinton; David C Plaut"}, {"ref_id": "b3", "title": "Learning To Learn Using Gradient Descent", "journal": "", "year": "2001", "authors": "Sepp Hochreiter; A Steven Younger; Peter R Conwell"}, {"ref_id": "b4", "title": "Reconciling Meta-Learning and Continual Learning with Online Mixtures of Tasks", "journal": "", "year": "2019", "authors": "Ghassen Jerfel; Erin Grant; Tom Griffiths; Katherine A Heller"}, {"ref_id": "b5", "title": "In-Datacenter Performance Analysis of a Tensor Processing Unit", "journal": "", "year": "2017", "authors": "P Norman; Cliff Jouppi; Nishant Young; David Patil; Gaurav Patterson; Raminder Agrawal; Sarah Bajwa; Suresh Bates; Nan Bhatia; Al Boden;  Borchers"}, {"ref_id": "b6", "title": "A Natural Policy Gradient", "journal": "", "year": "2001", "authors": "M Sham;  Kakade"}, {"ref_id": "b7", "title": "Recurrent Experience Replay in Distributed Reinforcement Learning", "journal": "", "year": "2018", "authors": "Steven Kapturowski; Georg Ostrovski; John Quan; Remi Munos; Will Dabney"}, {"ref_id": "b8", "title": "Adaptive Gradient-Based Meta-Learning Methods", "journal": "", "year": "2019", "authors": "Mikhail Khodak; Maria-Florina F Balcan; Ameet S Talwalkar"}, {"ref_id": "b9", "title": "Improving Generalization in Meta Reinforcement Learning Using Learned Objectives", "journal": "", "year": "2019", "authors": "Louis Kirsch; J\u00fcrgen Sjoerd Van Steenkiste;  Schmidhuber"}, {"ref_id": "b10", "title": "Gradient-Based Meta-Learning with Learned Layerwise Metric and Subspace", "journal": "", "year": "2018", "authors": "Yoonho Lee; Seungjin Choi"}, {"ref_id": "b11", "title": "Learning Gradient Descent: Better Generalization and Longer Horizons", "journal": "", "year": "2017", "authors": "Kaifeng Lv; Shunhua Jiang; Jian Li"}, {"ref_id": "b12", "title": "Revisiting the Arcade Learning Environment: Evaluation Protocols and Open Problems for General Agents", "journal": "Journal of Artificial Intelligence Research", "year": "2018", "authors": "C Marlos; Marc G Machado; Erik Bellemare; Joel Talvitie; Matthew Veness; Michael Hausknecht;  Bowling"}, {"ref_id": "b13", "title": "Gradient-Based Hyperparameter Optimization Through Reversible Learning", "journal": "PMLR", "year": "2015", "authors": "Dougal Maclaurin; David Duvenaud; Ryan Adams"}, {"ref_id": "b14", "title": "Understanding and Correcting Pathologies in the Training of Learned Optimizers", "journal": "", "year": "2019", "authors": "Luke Metz; Niru Maheswaranathan; Jeremy Nixon; Daniel Freeman; Jascha Sohl-Dickstein"}, {"ref_id": "b15", "title": "", "journal": "", "year": "2013", "authors": "Volodymyr Mnih; Koray Kavukcuoglu; David Silver; Alex Graves; Ioannis Antonoglou; Daan Wierstra; Martin Riedmiller"}, {"ref_id": "b16", "title": "On First-Order Meta-Learning Algorithms", "journal": "", "year": "2018", "authors": "Alex Nichol; Joshua Achiam; John Schulman"}, {"ref_id": "b17", "title": "Discovering Reinforcement Learning Algorithms", "journal": "", "year": "2020", "authors": "Junhyuk Oh; Matteo Hessel; Wojciech M Czarnecki; Zhongwen Xu; Satinder Hado P Van Hasselt; David Singh;  Silver"}, {"ref_id": "b18", "title": "Advances in Neural Information Processing Systems", "journal": "", "year": "2019", "authors": "Eunbyung Park; B Junier;  Oliva"}, {"ref_id": "b19", "title": "Revisiting Natural Gradient for Deep Networks", "journal": "", "year": "2014", "authors": "Razvan Pascanu; Yoshua Bengio"}, {"ref_id": "b20", "title": "Incremental Multi-Step Q-Learning", "journal": "", "year": "1994", "authors": "Jing Peng; Ronald J Williams"}, {"ref_id": "b21", "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "journal": "", "year": "2020", "authors": "Aniruddh Raghu; Maithra Raghu; Samy Bengio; Oriol Vinyals"}, {"ref_id": "b22", "title": "Optimization as a Model for Few-Shot Learning", "journal": "", "year": "2017", "authors": "Sachin Ravi; Hugo Larochelle"}, {"ref_id": "b23", "title": "AutoML-Zero: Evolving Machine Learning Algorithms From Scratch", "journal": "", "year": "2020", "authors": "Esteban Real; Chen Liang; David R So; Quoc V Le"}, {"ref_id": "b24", "title": "", "journal": "", "year": "2015", "authors": "Sergio Gomez Andrei A Rusu; Caglar Colmenarejo; Guillaume Gulcehre; James Desjardins; Razvan Kirkpatrick; Volodymyr Pascanu; Koray Mnih; Raia Kavukcuoglu;  Hadsell"}, {"ref_id": "b25", "title": "Simon Osindero, and Raia Hadsell. Meta-Learning with Latent Embedding Optimization", "journal": "", "year": "2019", "authors": "Andrei A Rusu; Dushyant Rao; Jakub Sygnowski; Oriol Vinyals; Razvan Pascanu"}, {"ref_id": "b26", "title": "Evolutionary Principles in Self-Referential Learning", "journal": "", "year": "1987", "authors": "J\u00fcrgen Schmidhuber"}, {"ref_id": "b27", "title": "A 'self-referential' weight matrix", "journal": "Springer", "year": "1993", "authors": "J\u00fcrgen Schmidhuber"}, {"ref_id": "b28", "title": "Off-Policy Actor-Critic with Shared Experience Replay", "journal": "", "year": "2020", "authors": "Simon Schmitt; Matteo Hessel; Karen Simonyan"}, {"ref_id": "b29", "title": "Local Gain Adaptation in Stochastic Gradient Descent", "journal": "", "year": "1999", "authors": "Nicol N Schraudolph"}, {"ref_id": "b30", "title": "Trust Region Policy Optimization", "journal": "", "year": "2015", "authors": "John Schulman; Sergey Levine; Pieter Abbeel; Michael Jordan; Philipp Moritz"}, {"ref_id": "b31", "title": "", "journal": "", "year": "2017", "authors": "John Schulman; Filip Wolski; Prafulla Dhariwal; Alec Radford; Oleg Klimov"}, {"ref_id": "b32", "title": "", "journal": "Core Knowledge. Developmental science", "year": "2007", "authors": "Elizabeth S Spelke; Katherine D Kinzler"}, {"ref_id": "b33", "title": "Some Considerations on Learning to Explore via Meta-Reinforcement Learning", "journal": "", "year": "2018", "authors": "Bradly C Stadie; Ge Yang; Rein Houthooft; Xi Chen; Yan Duan; Yuhuai Wu; Pieter Abbeel; Ilya Sutskever"}, {"ref_id": "b34", "title": "Learning to Predict by the Methods of Temporal Differences", "journal": "Machine learning", "year": "1988", "authors": "Richard S Sutton"}, {"ref_id": "b35", "title": "Policy Gradient Methods for Reinforcement Learning with Function Approximation", "journal": "", "year": "1999", "authors": "S Richard; David A Sutton;  Mcallester; P Satinder; Yishay Singh;  Mansour"}, {"ref_id": "b36", "title": "Distral: Robust Multitask Reinforcement Learning", "journal": "", "year": "2017", "authors": "Yee Whye Teh; Victor Bapst; Wojciech M Czarnecki; John Quan; James Kirkpatrick; Raia Hadsell; Nicolas Heess; Razvan Pascanu"}, {"ref_id": "b37", "title": "", "journal": "", "year": "2020", "authors": "Manan Tomar; Lior Shani; Yonathan Efroni; Mohammad Ghavamzadeh"}, {"ref_id": "b38", "title": "Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples. International Conference on Learning Representations", "journal": "", "year": "2020", "authors": "Eleni Triantafillou; Tyler Zhu; Vincent Dumoulin; Pascal Lamblin; Utku Evci; Kelvin Xu; Ross Goroshin; Carles Gelada; Kevin Swersky; Pierre-Antoine Manzagol"}, {"ref_id": "b39", "title": "MetaGrad: Multiple Learning Rates in Online Learning", "journal": "", "year": "2016", "authors": "Tim Van Erven;  Wouter M Koolen"}, {"ref_id": "b40", "title": "Matching Networks for One Shot Learning", "journal": "", "year": "2016", "authors": "Oriol Vinyals; Charles Blundell; Timothy Lillicrap; Koray Kavukcuoglu; Daan Wierstra"}, {"ref_id": "b41", "title": "Learning to Reinforcement Learn", "journal": "", "year": "2016", "authors": "Jane X Wang; Zeb Kurth-Nelson; Dhruva Tirumala; Hubert Soyer; Joel Z Leibo; R\u00e9mi Munos; Charles Blundell; Dharshan Kumaran; Matthew Botvinick"}, {"ref_id": "b42", "title": "Nando de Freitas, and Jascha Sohl-Dickstein. Learned Optimizers that Scale and Generalize", "journal": "", "year": "2017", "authors": "Olga Wichrowska; Niru Maheswaranathan; Matthew W Hoffman; Sergio G\u00f3mez Colmenarejo; Misha Denil"}, {"ref_id": "b43", "title": "Function Optimization using Connectionist Reinforcement Learning Algorithms", "journal": "Connection Science", "year": "1991", "authors": "J Ronald; Jing Williams;  Peng"}, {"ref_id": "b44", "title": "Understanding Short-Horizon Bias in Stochastic Meta-Optimization", "journal": "", "year": "2018", "authors": "Yuhuai Wu; Mengye Ren; Renjie Liao; Roger B Grosse"}, {"ref_id": "b45", "title": "Meta-Gradient Reinforcement Learning", "journal": "", "year": "2018", "authors": "Zhongwen Xu; P Hado; David Van Hasselt;  Silver"}, {"ref_id": "b46", "title": "Meta-Learning without Memorization", "journal": "", "year": "2020", "authors": "Mingzhang Yin; George Tucker; Mingyuan Zhou; Sergey Levine; Chelsea Finn"}, {"ref_id": "b47", "title": "A Self-Tuning Actor-Critic Algorithm", "journal": "", "year": "2020", "authors": "Tom Zahavy; Zhongwen Xu; Vivek Veeriah; Matteo Hessel; Junhyuk Oh; David Hado P Van Hasselt; Satinder Silver;  Singh"}, {"ref_id": "b48", "title": "On Learning Intrinsic Rewards for Policy Gradient Methods", "journal": "", "year": "2018", "authors": "Zeyu Zheng; Junhyuk Oh; Satinder Singh"}, {"ref_id": "b49", "title": "Fast Context Adaptation via Meta-Learning", "journal": "", "year": "2019", "authors": "Luisa Zintgraf; Kyriacos Shiarli; Vitaly Kurin; Katja Hofmann; Shimon Whiteson"}], "figures": [{"figure_label": "1", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Bootstrapped Meta-Gradients.", "figure_data": ""}, {"figure_label": "3", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure 3: BMG \u03b5-greedy exploration under a Q(\u03bb)-agent.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Figure 4 :4Figure 4: Human-normalized score across the 57 games in Atari ALE. Left: per-game difference in score between BMG and our implementation of STACX * at 200M frames. Right: Median scores over learning compared to published baselines. Shading depict standard deviation across 3 seeds.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Figure 5 :5Figure 5: Ablations on Atari. Left: human normalized score decomposition of TB w.r.t. optimizer (SGD, RMS), matching function (L2, KL, KL & V), and bootstrap steps (L). BMG with (SGD, L2, L = 1) is equivalent to STACX. Center: episode return on Ms Pacman for different L. Right: distribution of episode returns over all 57 games, normalized per-game by mean and standard deviation. All results are reported between 190-200M frames over 3 independent seeds.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Figure 6 :6Figure 6: MiniImagenet 5-way-5-shot meta-test performance. Left: performance as a function of meta-training batches. Center: performance as a function of wall-clock time. Right: best reported performance under each K. Error bars depict standard deviation across 3 seeds.", "figure_data": ""}, {"figure_label": "8", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Figure 8 :8Figure 8: Total rewards on two-colors with actor-critics. Shading: standard deviation over 50 seeds.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_6", "figure_caption": "Figure 9 :9Figure9: Range of the entropy of a softmax-policy over time(2-colors). Each shaded area shows the difference between the entropy 3333 steps after the agent observes a new entropy and the entropy after training on the reward-function for 100000 steps. Meta-gradients without explicit entropyregularization (left) reduce entropy over time while Bootstrapped meta-gradients (right) maintain entropy with a large enough meta-learning horizon. Averaged across 50 seeds.", "figure_data": ""}, {"figure_label": "11", "figure_type": "figure", "figure_id": "fig_8", "figure_caption": "Figure 11 :11Figure11: Total reward on two-colors with an actor-critic agent and different matching functions for BMG. Shading: standard deviation over 50 seeds.", "figure_data": ""}, {"figure_label": "12", "figure_type": "figure", "figure_id": "fig_9", "figure_caption": "Figure 12 :12Figure 12: Results on two-colors under a Q(\u03bb) agent with meta-learned \u03b5-greedy exploration under BMG. Averaged over 50 seeds.", "figure_data": ""}, {"figure_label": "13", "figure_type": "figure", "figure_id": "fig_10", "figure_caption": "Figure 13 :13Figure13: Atari BMG decomposition. We report human normalized score (median, quantiles, 1 2 IQR) between 190-200M frames over all 57 games, with 3 independent runs for each configuration.", "figure_data": ""}, {"figure_label": "14", "figure_type": "figure", "figure_id": "fig_11", "figure_caption": "Figure 14 :14Figure 14: Atari, learning curves on MS Pacman for KL &V . L = 4, R computes the Lthe step on only replay data. L = 4, w uses the meta-learned objective for the Lth step (with Lth step computed on online and replay data, as per default). Shading depicts standard deviation across 3 seeds.", "figure_data": ""}, {"figure_label": "17", "figure_type": "figure", "figure_id": "fig_14", "figure_caption": "Figure 17 :17Figure17: Atari: Computational characteristics as a function of network size (see Appendix C.4) and meta-learning horizon H. When H = K, we vary the number of update steps to backpropagate through (with L = 1 for BMG). When H = L, we vary the number of target update steps (with K = 1). Measurements are taken over the first 20 million learning frames on the game Pong.", "figure_data": ""}, {"figure_label": "", "figure_type": "figure", "figure_id": "fig_15", "figure_caption": "\u03c4 ). The BMG meta-loss takes the form w = w \u2212\u03b2\u2207 w E \u03c4 [\u00b5 \u03c4 (x \u03c4 , x (K) \u03c4 (w))], where \u00b5 \u03c4 is defined on data from task \u03c4 . As with the MG update, as the task distribution is independent of w, this simplifies to w = w \u2212\u03b2E \u03c4 [\u2207 w \u00b5 \u03c4 (x \u03c4 , x (K) \u03c4 (w))]", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Sebastian Flennerhag, Andrei A.Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and  Raia Hadsell. Meta-Learning with Warped Gradient Descent. In International Conference on Learning Representations, 2020. Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas L. Griffiths. Recasting Gradient-Based Meta-Learning as Hierarchical Bayes. In International Conference on Learning Representations, 2018. Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal Piot, koray kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning. In Advances in Neural Information Processing Systems, 2020. Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-Bastien Grill, Florent Altch\u00e9, R\u00e9mi Munos, and Mohammad Gheshlaghi Azar. Bootstrap Latent-Predictive Representations for Multitask Reinforcement Learning. In International Conference on Machine Learning, 2020.", "figure_data": "Bootstrapped Meta-Learning: AppendixCONTENTSAppendix A: proofs accompanying Section 4.Appendix B: non-stationary Grid-World (Section 5.1).Appendix C: ALE Atari (Section 5.2).Appendix D: Multi-task meta-learning, Few-Shot Learning on MiniImagenet (Section 6)."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Algorithm 1 N -step RL actor loop \u2208 R nx , z \u2208 R nz , w \u2208 R nwPolicy, value function, and meta parameters.", "figure_data": "Require: N Require: x \u2208 R nxRollout length. Policy parameters.Require: sEnvironment state.B \u2190 (s)Initialise rollout.for t = 1, 2, . . . , N doa \u223c \u03c0 x (s)Sample action.s, r \u2190 env(s, a)Take a step in environment.B \u2190 B \u222a(a, r, s)Add to rollout.end forreturn s, BAlgorithm 2 K-step online learning loopRequire: N, KRollout length, meta-update length.Require: x"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Two-colors hyper-parametersSetup Hyper-parameters are reported in Table2. We follow the original IMPALA(Espeholt et al.,  2018)  setup, but do not down-sample or gray-scale frames from the environment. Following previous works", "figure_data": "Actor-criticInner LearnerOptimiserSGDLearning rate0.1Batch size16 (losses are averaged)\u03b30.99\u00b5KL(\u03c0x||\u03c0 x )MLP hidden layers (v, \u03c0) 2MLP feature size (v, \u03c0)256Activation FunctionReLUMeta-learnerOptimiserAdam(Adam)10 \u22124\u03b2 1 , \u03b2 20.9, 0.999Learning rate candidates{3 \u2022 10 \u22126 , 10 \u22125 , 3 \u2022 10 \u22125 , 10 \u22124 , 3 \u2022 10 \u22124 }MLP hidden layers ( )1MLP feature size ( )32Activation FunctionReLUOutput ActivationSigmoidQ(\u03bb)Inner LearnerOptimiserAdamLearning Rate3 \u2022 10 \u22125(Adam)10 \u22124\u03b2 1 , \u03b2 20.9, 0.999Gradient EMA0.9\u03bb0.7\u03b30.99MLP hidden layers (Q)2MLP feature size (Q)256Activation FunctionReLUMeta-learnerLearning Rate10 \u22124(Adam)10 \u22124\u03b2 1 , \u03b2 20.9, 0.999Gradient EMA0.9MLP hidden layers ( )1MLP feature size ( )32Activation FunctionReLUOutput ActivationSigmoid"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": Atari hyper-parametersALE (Bellemare et al., 2013)Frame dimensions (H, W, D)160, 210, 3Frame poolingNoneFrame grayscalingNoneNum. stacked frames4Num. action repeats4Sticky actions (Machado et al., 2018)FalseReward clipping[\u22121, 1]\u03b3 = 0 loss of lifeTrueMax episode length108 000 framesInitial noop actions30IMPALA Network (Espeholt et al., 2018)Convolutional layers4Channel depths64, 128, 128, 64Kernel size3Kernel stride1Pool size3Pool stride2Padding'SAME'Residual blocks per layer2Conv-to-linear feature size512STACX (Zahavy et al., 2020)Auxiliary tasks2MLP hidden layers2MLP feature size256Max entropy loss value0.9OptimisationUnroll length20Batch size18of which from replay12of which is online data6Replay buffer size10 000LASER (Schmitt et al., 2020) KL-threshold 2OptimiserRMSPropInitial learning rate10 \u22124Learning rate decay interval200 000 framesLearning rate decay rateLinear to 0Momentum decay0.99Epsilon10 \u22124Gradient clipping, max norm0.3Meta-Optimisation\u03b3, \u03bb,\u03c1,c, \u03b10.995, 1, 1, 1, 1PG , EN , TD1, 0.01, 0.25OptimiserAdamLearning rate10 \u22123\u03b2 1 , \u03b2 20.9, 0.999Epsilon10 \u22124Gradient clipping, max norm0.3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Meta-gradient cosine similarity and variance per-game at 50-150M frames over 3 seeds.", "figure_data": "KLKL & VL2STACXKangarooCosine similarity0.19 (0.02)0.11 (0.01)0.001 (1e-4)0.009 (0.01)Meta-gradient variance0.05 (0.01) 0.002 (1e-4) 2.3e-9 (4e-9) 6.4e-4 (7e-4)Meta-gradient norm variance49684744Ms PacmanCosine similarity0.11 (0.006) 0.03 (0.006) 0.002 (4e-4) -0.005 (0.01)Meta-gradient variance90 (12)0.8 (0.2)9.6e-7 (2e-8)0.9 (0.2)Meta-gradient norm variance2.17.94.22.1Star GunnerCosine similarity0.13 (0.008) 0.07 (0.001) 0.003 (5e-4)0.002 (0.02)Meta-gradient variance4.2 (1.1)1.5 (2.3)1.9e-7 (3e-7)0.06 (0.03)Meta-gradient norm variance6.16.611.76.5"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "presents mean episode returns per game between 190-200 millions frames for all main", "figure_data": "Seconds per update step0.2 0.3 0.4 0.51x2x H=14x0.35 0.40 0.45 0.50 0.55 0.30 0.251xH=2 2x0.8 1.0 1.2 0.6 Network size 4x1xH=4 BMG, H=L BMG, H=K STACX, H=K 2x4x100 200 300 400 01xH=8 2x4x"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Meta-training steps per second for MAML and BMG on 5-way-5-shot MiniImagenet. Standard deviation across 5 seeds in parenthesis.", "figure_data": "K LH = K + L MAMLBMG11214.3 (0.4) 12.4 (0.5)56-6.9 (0.3)10 11-4.4 (0.1)5164.4 (0.06) 4.2 (0.04)510-3.2 (0.03)10 15-2.5 (0.01)10 1112.3 (0.01) 2.2 (0.01)515-1.9 (0.01)10 20-1.7 (0.01)15 -151.4 (0.01) -20 -201.1 (0.01) -"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Computational-efficiency: mean meta-test accuracy over 3 seeds for best hyper-parameters per time budget. \u00b5 = 1 corresponds to KL (x \u2022) and \u00b5 = 2 to KL (\u2022 x).", "figure_data": "Time (h)\u03b2KL \u00b5 Acc. (%)\u03b2K FOMAML ANIL Acc. (%)110 \u2212311 263.5 10 \u221231False False63.0210 \u2212311 263.6 10 \u22123 10FalseTrue63.0310 \u2212351 163.7 10 \u221235False False63.0410 \u2212351 163.8 10 \u221245FalseTrue63.1410 \u2212351 163.8 10 \u221241FalseTrue63.4510 \u2212351 163.8 10 \u221245False False63.5610 \u221235 10 163.8 10 \u221245False False63.6710 \u2212451 163.8 10 \u221245False False63.6810 \u2212351 163.8 10 \u221245False False63.6910 \u2212451 163.9 10 \u221245False False63.61010 \u2212451 164.2 10 \u221245False False63.71110 \u2212455 164.3 10 \u221245False False63.81210 \u2212455 164.5 10 \u221245False False63.91310 \u2212455 164.6 10 \u221245False False63.91410 \u2212451 264.7 10 \u221245False False63.81510 \u2212451 164.8 10 \u221245False False63.41610 \u2212451 164.8 10 \u22123 10False False63.21710 \u2212451 164.8 10 \u22124 10False False63.31810 \u221245 10 164.8 10 \u22124 10False False63.51910 \u2212455 164.8 10 \u22124 10False False63.62010 \u2212455 164.7 10 \u22124 10False False63.82110 \u221245 10 164.7 10 \u22124 10False False63.92110 \u221245 10 164.7 10 \u22124 10False False63.82210 \u2212455 164.7 10 \u22124 10False False63.92310 \u221245 10 164.7 10 \u22124 10False False63.82410 \u221245 10 164.7-----"}], "formulas": [{"formula_id": "formula_0", "formula_text": "x (1) = x \u2212\u03b1\u2207 x f (x) with h = \u2207 x f (x), w = \u03b1 \u2208 R + .", "formula_coordinates": [3.0, 107.64, 421.44, 189.56, 22.18]}, {"formula_id": "formula_1", "formula_text": "w = w \u2212\u03b2 \u2207 w f x (K) (w) , \u03b2 \u2208 R + .(1)", "formula_coordinates": [3.0, 214.09, 501.17, 289.92, 11.72]}, {"formula_id": "formula_2", "formula_text": "w = w \u2212\u03b2 \u2207 w \u00b5 x, x (K) (w) , \u03b2 \u2208 R + ,(2)", "formula_coordinates": [3.0, 209.39, 719.49, 294.61, 11.72]}, {"formula_id": "formula_3", "formula_text": "\u2207 w x \u2212 x (K) (w) 2 2 = \u22122D x \u2212 x (K) = D\u2207 x f x (K) = \u2207 w f x (K) (w) ,(3)", "formula_coordinates": [4.0, 139.19, 132.1, 364.81, 22.08]}, {"formula_id": "formula_4", "formula_text": "Let G T = D T D \u2208 R nx\u00d7nx , with D := \u2202 \u2202 w x (K) (w) T \u2208 R nw\u00d7nx . Note that \u2207 w f (x (K) (w)) = D\u2207 x f (x (K) ).", "formula_coordinates": [4.0, 113.35, 505.01, 390.65, 29.14]}, {"formula_id": "formula_5", "formula_text": "x (K) (w ) \u2212 f x (K) (w) = \u2212\u03b2 \u2207 x f (x (K) ) 2 G T + O(\u03b2 2 ) < 0.", "formula_coordinates": [4.0, 108.0, 536.89, 396.0, 28.08]}, {"formula_id": "formula_6", "formula_text": "f x (K) (w) \u2212 f x (K) (w) = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)).", "formula_coordinates": [4.0, 127.11, 644.15, 357.77, 22.31]}, {"formula_id": "formula_7", "formula_text": "f x (K) (w) \u2212 f x (K) (w) = \u2212 \u03b2 \u03b1 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)) < 0.", "formula_coordinates": [4.0, 167.8, 698.26, 276.39, 22.31]}, {"formula_id": "formula_8", "formula_text": "Corollary 1. Let \u00b5 = \u2022 2 2 andx = \u03be r G (x(", "formula_coordinates": [5.0, 108.0, 181.18, 170.11, 12.48]}, {"formula_id": "formula_9", "formula_text": "Q x (s 0 , a 0 ) := E[ \u221e t=0 \u03b3 t r t+1 | s 0 , a 0 , \u03c0 x ] under discount rate \u03b3 \u2208 [0, 1). The corresponding value of policy \u03c0 x is given by V x (s 0 ) := E a0\u223c\u03c0x(a | s0) [Q x (s 0 , a 0 )].", "formula_coordinates": [5.0, 108.0, 389.13, 396.0, 23.88]}, {"formula_id": "formula_10", "formula_text": "x such that E[V x ] \u2265 E[V x ]. A common policy-improvement step is arg max x E a\u223c\u03c0 x (a|s) [Q x (s, a)].", "formula_coordinates": [5.0, 108.0, 456.39, 397.64, 18.85]}, {"formula_id": "formula_11", "formula_text": "EN (x) = t\u2208\u03c4 a\u2208A \u03c0 x (a | s t ) log \u03c0 x (a | s t ), TD (z) = 1 2 t\u2208\u03c4 G (n) t \u2212 v z (s t ) 2 , PG (x) = \u2212 t\u2208\u03c4 \u03c1 t log \u03c0 x (a t | s t ) G (n) t \u2212 v z (s t ) ,(4)", "formula_coordinates": [5.0, 140.21, 570.99, 363.79, 55.44]}, {"formula_id": "formula_12", "formula_text": "PG , i EN , i TD , \u03bb i , \u03b1 i ) 1+n i=1", "formula_coordinates": [7.0, 187.83, 509.21, 92.54, 13.24]}, {"formula_id": "formula_13", "formula_text": "z (1) (w)) := E (vz \u2212 v z (1) ) 2 .", "formula_coordinates": [7.0, 384.57, 627.83, 121.17, 12.03]}, {"formula_id": "formula_14", "formula_text": "SGD L2 L=1 RMS L2 L=1 RMS KL L=1 RMS KL & V L=1 RMS KL & V", "formula_coordinates": [8.0, 135.99, 188.01, 100.41, 19.72]}, {"formula_id": "formula_15", "formula_text": "(k) \u03c4 = x (k\u22121) \u03c4 +\u03b1\u2207f D\u03c4 (x (k\u22121) \u03c4", "formula_coordinates": [8.0, 126.11, 684.11, 122.92, 12.78]}, {"formula_id": "formula_16", "formula_text": "E[f D \u03c4 (x (K) \u03c4 (w))].", "formula_coordinates": [8.0, 259.61, 698.01, 74.79, 14.54]}, {"formula_id": "formula_17", "formula_text": "(K) \u03c4 using validation data. The BMG objective is the expected distance, E[\u00b5(x \u03c4 , x (K) \u03c4 )].", "formula_coordinates": [9.0, 108.0, 271.82, 396.0, 26.34]}, {"formula_id": "formula_18", "formula_text": "x (k\u22121) , h (k\u22121) , x (k) ) into a new learner state h(", "formula_coordinates": [14.0, 108.0, 277.89, 396.0, 24.32]}, {"formula_id": "formula_19", "formula_text": "x (1) = x (0) + \u03d5 x (0) , h (0) , w x (2) = x (1) + \u03d5 x (1) , h (1) , w . . . x (K) = x (K\u22121) + \u03d5 x (K\u22121) , h (K\u22121) , w .", "formula_coordinates": [14.0, 218.95, 323.26, 174.11, 67.69]}, {"formula_id": "formula_20", "formula_text": "x (K) (w ) \u2212 f x (K) (w) = \u2212\u03b2 \u2207 x f (x (K) ) 2 G T + o(\u03b2 2 ) < 0.", "formula_coordinates": [14.0, 108.0, 447.22, 396.0, 28.08]}, {"formula_id": "formula_21", "formula_text": "f x (K) (w ) = f x (K) (w) + D g, w \u2212 w + o(\u03b2 2 g 2 G T ) = f x (K) (w) \u2212 \u03b2 D g, D g + o(\u03b2 2 g 2 G T ) = f x (K) (w) \u2212 \u03b2 g 2 G T + o(\u03b2 2 g 2 G T ),", "formula_coordinates": [14.0, 176.12, 529.56, 259.76, 48.06]}, {"formula_id": "formula_22", "formula_text": "f x (K) (w) \u2212 f x (K) (w) = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)).", "formula_coordinates": [14.0, 127.11, 633.51, 357.77, 22.31]}, {"formula_id": "formula_23", "formula_text": "f x (K) (w) \u2212 f x (K) (w) = \u2212 \u03b2 \u03b1 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)) < 0.", "formula_coordinates": [14.0, 167.8, 694.71, 276.39, 22.31]}, {"formula_id": "formula_24", "formula_text": "f x (K) (w) = f x (K) (w) + D g,w \u2212 w + o(\u03b2 2 D u 2 2 ) = f x (K) (w) \u2212 \u03b2 D g, D u + o(\u03b2 2 D u 2 2 ) = f x (K) (w) \u2212 \u03b2 u, G T g + o(\u03b2 2 u 2 G T ). (5", "formula_coordinates": [15.0, 177.73, 180.77, 322.4, 46.74]}, {"formula_id": "formula_25", "formula_text": ")", "formula_coordinates": [15.0, 500.13, 216.79, 3.87, 8.64]}, {"formula_id": "formula_26", "formula_text": "x (K) : \u00b5(x, x (K) + d) = \u00b5(x, x (K) ) + u, d + o( d 2 2", "formula_coordinates": [15.0, 201.27, 241.28, 297.08, 32.1]}, {"formula_id": "formula_27", "formula_text": "\u2212\u03b2 u, G T g = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b1\u03b2 g 2 G T ).", "formula_coordinates": [15.0, 156.38, 298.11, 299.24, 22.31]}, {"formula_id": "formula_28", "formula_text": "f x (K) (w) \u2212 f x (K) (w) = \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) (6) + o(\u03b1\u03b2 g 2 G T + \u03b2 2 u 2 G T )", "formula_coordinates": [15.0, 160.2, 343.66, 343.8, 37.2]}, {"formula_id": "formula_29", "formula_text": "f x (K) (w) \u2212 f x (K) (w) = \u2212 \u03b2 \u03b1 \u00b5(x, x (K) ) + o(\u03b2(\u03b1 + \u03b2)).", "formula_coordinates": [15.0, 176.94, 500.73, 258.13, 22.31]}, {"formula_id": "formula_30", "formula_text": "(\u1fb1,\u03b2) \u2208 R 2 + such that for any (\u03b1, \u03b2) \u2208 (0,\u1fb1) \u00d7 (0,\u03b2), f x (K) (w) \u2212 f x (K) (w) < 0. For any such choice of (\u03b1, \u03b2), by virtue of differentiability in \u00b5 there exists some neighborhood N around x (K) \u2212\u03b1G T g for which anyx \u2208 N satisfy f x (K) (w) \u2212 f x (K) (w) < 0.", "formula_coordinates": [15.0, 106.83, 528.92, 397.17, 50.11]}, {"formula_id": "formula_31", "formula_text": "\u2207x \u03b2 \u03b1 \u00b5(x, x (K) \u2212\u03b1G T g) \u2212 \u00b5(x, x (K) ) + o(\u03b1\u03b2 g 2 G T + \u03b2 2 u 2 G T ) x=x (K) \u2212\u03b1G T g = \u2212 \u03b2 \u03b1 \u2207x \u00b5(x, x (K) ) x=x (K) \u2212\u03b1G T g + \u03b2 2 o = 0,", "formula_coordinates": [15.0, 117.79, 629.23, 375.81, 51.27]}, {"formula_id": "formula_32", "formula_text": "(K) ) 2 / G T \u2207f (x (K) ) 2 . Corollary 1. Let \u00b5 = \u2022 2 2 andx = \u03be r G (x(", "formula_coordinates": [16.0, 108.0, 95.44, 390.03, 27.18]}, {"formula_id": "formula_33", "formula_text": "GG T = G T . Proof. Let g := \u2207 x f x (K) . By Lemma 1, f x (K) (w ) \u2212f x (K) (w) = \u2212\u03b2 G T g, g +O(\u03b2 2 ). From Theorem 1, with \u00b5 = \u2022 2 2 , f x (K) (w) \u2212 f x (K) (w) = \u2212r G T g, G T g + O(\u03b2(\u03b1 + \u03b2))", "formula_coordinates": [16.0, 108.0, 122.73, 397.74, 51.35]}, {"formula_id": "formula_34", "formula_text": "x (K) (w) \u2212 f x (K) (w ) \u2248 \u2212\u03b2 r G T g, G T g \u2212 G T g, g .", "formula_coordinates": [16.0, 180.52, 187.13, 263.13, 10.81]}, {"formula_id": "formula_35", "formula_text": "max v\u2208R nx G T g, v s.t. v 2 \u2264 1. Form the Lagrangian L(v, \u03bb) := G T g, v \u2212 \u03bb( v 2 \u2212 1)", "formula_coordinates": [16.0, 108.0, 220.7, 276.67, 31.39]}, {"formula_id": "formula_36", "formula_text": "G T g \u2212\u03bb v * v * 2 = 0 =\u21d2 v * = v * 2 \u03bb G T g .", "formula_coordinates": [16.0, 208.86, 254.13, 194.28, 24.8]}, {"formula_id": "formula_37", "formula_text": "G T g, G T g G T g 2 \u2265 G T g, g g 2 =\u21d2 r G T g, G T g \u2265 G T g, g ,", "formula_coordinates": [16.0, 182.85, 340.15, 251.36, 24.8]}, {"formula_id": "formula_38", "formula_text": "for k = 1, 2, . . . , K do s, B \u2190 ActorLoop(x, s, N ) Algorithm 1. (x, z) \u2190 \u03d5((x, z), B, w)", "formula_coordinates": [17.0, 117.96, 282.14, 387.78, 30.72]}, {"formula_id": "formula_39", "formula_text": "Require: x \u2208 R nx , z \u2208 R nz , w \u2208 R nw", "formula_coordinates": [17.0, 108.0, 385.11, 159.43, 11.65]}, {"formula_id": "formula_40", "formula_text": "1 T T t=1 t PG (x (t) (w)) + meta t EN (x (t) (w))", "formula_coordinates": [18.0, 219.2, 336.55, 169.98, 14.56]}, {"formula_id": "formula_41", "formula_text": "x (T ) \u2212\u03b2\u2207 x ( T PG (x (T ) ) + meta T EN (x (T )", "formula_coordinates": [18.0, 107.7, 389.6, 156.63, 12.59]}, {"formula_id": "formula_42", "formula_text": "\u03c0 x (a | s t , t t , w) = \uf8f1 \uf8f2 \uf8f3 1 \u2212 \u03b5 w (t t ) + \u03b5w(tt) |A| if a = arg max b q x (s t , b) \u03b5w(tt) |A| else.", "formula_coordinates": [20.0, 156.53, 297.37, 297.75, 38.35]}, {"formula_id": "formula_43", "formula_text": "\u03c0x(a | s t ) = \uf8f1 \uf8f2 \uf8f3 1 if a = arg max b qx(s t , b) 0 else.", "formula_coordinates": [20.0, 207.66, 415.89, 195.48, 34.67]}, {"formula_id": "formula_44", "formula_text": "\u00b5 u (x, x) = E (ux(s) \u2212 u x (s; t, w)) 2 .", "formula_coordinates": [20.0, 225.08, 634.47, 161.84, 12.62]}, {"formula_id": "formula_45", "formula_text": "\u03b7 t := \u03c0 x (a t | s t ) / \u03bd(a t | s t ) \u03c1 t := \u03b1 \u03c1 min{\u03b7 t ,\u03c1} + (1 \u2212 \u03b1 \u03c1 )\u03b7 t c i := \u03bb (\u03b1 c min{\u03b7 i ,c} + (1 \u2212 \u03b1 c )\u03b7 i ) \u03b4 t := \u03c1 t (\u03b3vz(s t+1 ) + r t+1 \u2212 vz(s t )) G (n) t = vz(s t ) + (n\u22121) i=0 \u03b3 i \uf8eb \uf8ed i\u22121 j=0 c t+j \uf8f6 \uf8f8 \u03b4 t+i ,", "formula_coordinates": [22.0, 218.21, 372.55, 175.58, 89.8]}, {"formula_id": "formula_46", "formula_text": "u (1) , h (1) u = RMSProp (u, h u , g u ) g u = \u2207 u n i=1 f \u03c4 u i ; w i w (1) , h (1) w = Adam (w, h w , g w ) g w = \u2207 w f \u03c4 u", "formula_coordinates": [22.0, 140.16, 650.2, 300.78, 48.17]}, {"formula_id": "formula_47", "formula_text": "(1) 0 (w)) = \u0169 \u2212 u (1) 0 (w) 2 2 with\u0169 = u (1) 0 \u2212 1 2 \u2207 u f \u03c4 (u(1)", "formula_coordinates": [26.0, 107.64, 139.35, 227.5, 28.78]}, {"formula_id": "formula_48", "formula_text": "(K) \u03c4 (w))]", "formula_coordinates": [30.0, 229.94, 519.33, 35.05, 12.46]}, {"formula_id": "formula_49", "formula_text": "(K) \u03c4 (w) is the K-step update on task \u03c4 given (f \u03c4 , h \u03c4 , x \u03c4 ). Since p(\u03c4 ) is independent of w, this update becomes w = w \u2212\u03b2E \u03c4 [\u2207 w f \u03c4 (x (K)", "formula_coordinates": [30.0, 108.0, 533.23, 396.0, 26.68]}, {"formula_id": "formula_50", "formula_text": "K L \u03b8 V \u03c1 \u03b8 V \u03c1 1 1", "formula_coordinates": [32.0, 113.98, 140.51, 348.94, 23.5]}], "doi": ""}