{"title": "GANimation: Anatomically-aware Facial Animation from a Single Image", "authors": "Albert Pumarola; Antonio Agudo; Aleix M Martinez; Alberto Sanfeliu; Francesc Moreno-Noguer", "pub_date": "", "abstract": "Recent advances in Generative Adversarial Networks (GANs) have shown impressive results for task of facial expression synthesis. The most successful architecture is StarGAN [4], that conditions GANs' generation process with images of a specific domain, namely a set of images of persons sharing the same expression. While effective, this approach can only generate a discrete number of expressions, determined by the content of the dataset. To address this limitation, in this paper, we introduce a novel GAN conditioning scheme based on Action Units (AU) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression. Our approach allows controlling the magnitude of activation of each AU and combine several of them. Additionally, we propose a fully unsupervised strategy to train the model, that only requires images annotated with their activated AUs, and exploit attention mechanisms that make our network robust to changing backgrounds and lighting conditions. Extensive evaluation show that our approach goes beyond competing conditional generators both in the capability to synthesize a much wider range of expressions ruled by anatomically feasible muscle movements, as in the capacity of dealing with images in the wild.", "sections": [{"heading": "Introduction", "text": "Being able to automatically animate the facial expression from a single image would open the door to many new exciting applications in different areas, including the movie industry, photography technologies, fashion and e-commerce business, to name but a few. As Generative and Adversarial Networks have become more prevalent, this task has experienced significant advances, with architectures such as StarGAN [4], which is able not only to synthesize novel expressions, but also to change other attributes of the face, such as age, hair color or gender. Despite its generality, StarGAN can only change a particular aspect of a face among a discrete number of attributes defined by the annotation granularity of the dataset. For instance, for the facial expression synthesis task, arXiv:1807.09251v2 [cs.CV] 28 Aug 2018 Fig. 1. Facial animation from a single image. We propose an anatomically coherent approach that is not constrained to a discrete number of expressions and can animate a given image and render novel expressions in a continuum. In these examples, we are given solely the left-most input image Iy r (highlighted by a green square), and the parameter \u03b1 controls the degree of activation of the target action units involved in a smiling-like expression. Additionally, our system can handle images with unnatural illumination conditions, such as the example in the bottom row. [4] is trained on the RaFD [16] dataset which has only 8 binary labels for facial expressions, namely sad, neutral, angry, contemptuous, disgusted, surprised, fearful and happy.\nFacial expressions, however, are the result of the combined and coordinated action of facial muscles that cannot be categorized in a discrete and low number of classes. Ekman and Friesen [6] developed the Facial Action Coding System (FACS) for describing facial expressions in terms of the so-called Action Units (AUs), which are anatomically related to the contractions of specific facial muscles. Although the number of action units is relatively small (30 AUs were found to be anatomically related to the contraction of specific facial muscles), more than 7,000 different AU combinations have been observed [30]. For example, the facial expression for fear is generally produced with activations: Inner Brow Raiser (AU1), Outer Brow Raiser (AU2), Brow Lowerer (AU4), Upper Lid Raiser (AU5), Lid Tightener (AU7), Lip Stretcher (AU20) and Jaw Drop (AU26) [5]. Depending on the magnitude of each AU, the expression will transmit the emotion of fear to a greater or lesser extent.\nIn this paper we aim at building a model for synthetic facial animation with the level of expressiveness of FACS, and being able to generate anatomically-aware expressions in a continuous domain, without the need of obtaining any facial landmarks [36]. For this purpose we leverage on the recent EmotioNet dataset [3], which consists of one million images of facial expressions (we use 200,000 of them) of emotion in the wild annotated with discrete AUs activations 1 . We build a GAN architecture which, instead of being conditioned with images of a specific domain as in [4], it is conditioned on a one-dimensional vector indicating the presence/absence and the magnitude of each action unit. We train this architecture in an unsupervised manner that only requires images with their activated AUs. To circumvent the need for pairs of training images of the same person under different expressions, we split the problem in two main stages. First, we consider an AU-conditioned bidirectional adversarial architecture which, given a single training photo, initially renders a new image under the desired expression. This synthesized image is then rendered-back to the original pose, hence being directly comparable to the input image. We incorporate very recent losses to assess the photorealism of the generated image. Additionally, our system also goes beyond state-of-the-art in that it can handle images under changing backgrounds and illumination conditions. We achieve this by means of an attention layer that focuses the action of the network only in those regions of the image that are relevant to convey the novel expression.\nAs a result, we build an anatomically coherent facial expression synthesis method, able to render images in a continuous domain, and which can handle images in the wild with complex backgrounds and illumination conditions. As we will show in the results section, it compares favorably to other conditioned-GANs schemes, both in terms of the visual quality of the results, and the possibilities of generation. Figure 1 shows some example of the results we obtain, in which given one input image, we gradually change the magnitude of activation of the AUs used to produce a smile.", "publication_ref": ["b3", "b3", "b15", "b5", "b29", "b4", "b35", "b2", "b0", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Related Work", "text": "Generative Adversarial Networks. GANs are a powerful class of generative models based on game theory. A typical GAN optimization consists in simultaneously training a generator network to produce realistic fake samples and a discriminator network trained to distinguish between real and fake data. This idea is embedded by the so-called adversarial loss. Recent works [1,9] have shown improved stability relaying on the continuous Earth Mover Distance metric, which we shall use in this paper to train our model. GANs have been shown to produce very realistic images with a high level of detail and have been successfully used for image translation [38,10,13], face generation [12,28] , super-resolution imaging [34,18], indoor scene modeling [12,33] and human poses editing [27].\nConditional GANs. An active area of research is designing GAN models that incorporate conditions and constraints into the generation process. Prior studies have explored combining several conditions, such as text descriptions [29,39,37] and class information [24,23]. Particularly interesting for this work are those methods exploring image based conditioning as in image super-resolution [18], future frame prediction [22], image in-painting [25], image-to-image translation [10] and multi-target domain transfer [4]. Unpaired Image-to-Image Translation. As in our framework, several works have also tackled the problem of using unpaired training data. First attempts [21] relied on Markov random field priors for Bayesian based generation models using images from the marginal distributions in individual domains. Others explored enhancing GANS with Variational Auto-Encoder strategies [21,15]. Later, several works [25,19] have exploited the idea of driving the system to produce mappings transforming the style without altering the original input image content. Our approach is more related to those works exploiting cycle consistency to preserve key attributes between the input and the mapped image, such as Cy-cleGAN [38], DiscoGAN [13] and StarGAN [4]. Face Image Manipulation. Face generation and editing is a well-studied topic in computer vision and generative models. Most works have tackled the task on attribute editing [17,26,31] trying to modify attribute categories such as adding glasses, changing color hair, gender swapping and aging. The works that are most related to ours are those synthesizing facial expressions. Early approaches addressed the problem using mass-and-spring models to physically approximate skin and muscle movement [7]. The problem with this approach is that is difficult to generate natural looking facial expressions as there are many subtle skin movements that are difficult to render with simple spring models. Another line of research relied on 2D and 3D morphings [35], but produced strong artifacts around the region boundaries and was not able to model illumination changes.\nMore recent works [4,24,20] train highly complex convolutional networks able to work with images in the wild. However, these approaches have been conditioned on discrete emotion categories (e.g., happy, neutral, and sad). Instead, our model resumes the idea of modeling skin and muscles, but we integrate it in modern deep learning machinery. More specifically, we learn a GAN model conditioned on a continuous embedding of muscle movements, allowing to generate a large range of anatomically possible face expressions as well as smooth facial movement transitions in video sequences.", "publication_ref": ["b0", "b8", "b37", "b9", "b12", "b11", "b27", "b33", "b17", "b11", "b32", "b26", "b28", "b38", "b36", "b23", "b22", "b17", "b21", "b24", "b9", "b3", "b20", "b20", "b14", "b24", "b18", "b37", "b12", "b3", "b16", "b25", "b30", "b6", "b34", "b3", "b23", "b19"], "figure_ref": [], "table_ref": []}, {"heading": "Problem Formulation", "text": "Let us define an input RGB image as I yr \u2208 R H\u00d7W \u00d73 , captured under an arbitrary facial expression. Every gesture expression is encoded by means of a set of N action units y r = (y 1 , . . . , y N ) , where each y n denotes a normalized value between 0 and 1 to module the magnitude of the n-th action unit. It is worth pointing out that thanks to this continuous representation, a natural interpolation can be done between different expressions, allowing to render a wide range of realistic and smooth facial expressions.\nOur aim is to learn a mapping M to translate I yr into an output image I yg conditioned on an action-unit target y g , i.e., we seek to estimate the mapping\nG A G I G I D y G A W D D IF ig. 2.\nOverview of our approach to generate photo-realistic conditioned images. The proposed architecture consists of two main blocks: a generator G to regress attention and color masks; and a critic D to evaluate the generated image in its photorealism DI and expression conditioning fulfillment\u0177g. Note that our systems does not require supervision, i.e., no pairs of images of the same person with different expressions, nor the target image Iy g are assumed to be known.\nM : (I yr , y g ) \u2192 I yg . To this end, we propose to train M in an unsupervised manner, using\nM training triplets {I m yr , y m r , y m g } M m=1\n, where the target vectors y m g are randomly generated. Importantly, we neither require pairs of images of the same person under different expressions, nor the expected target image I yg .", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Our Approach", "text": "This section describes our novel approach to generate photo-realistic conditioned images, which, as shown in Fig. 2, consists of two main modules. On the one hand, a generator G(I yr |y g ) is trained to realistically transform the facial expression in image I yr to the desired y g . Note that G is applied twice, first to map the input image I yr \u2192 I yg , and then to render it back I yg \u2192\u00ce yr . On the other hand, we use a WGAN-GP [9] based critic D(I yg ) to evaluate the quality of the generated image as well as its expression.", "publication_ref": ["b8"], "figure_ref": [], "table_ref": []}, {"heading": "Network Architecture", "text": "Generator. Let G be the generator block. Since it will be applied bidirectionally (i.e., to map either input image to desired expression and vice-versa) in the following discussion we use subscripts o and f to indicate origin and final.\nGiven the image I yo \u2208 R H\u00d7W \u00d73 and the N -vector y f encoding the desired expression, we form the input of generator as a concatenation (I yo , y o ) \u2208 R H\u00d7W \u00d7(N +3) , where y o has been represented as N arrays of size H \u00d7 W .\nOne key ingredient of our system is to make G focus only on those regions of the image that are responsible of synthesizing the novel expression and keep the rest elements of the image such as hair, glasses, hats or jewelery untouched. For this purpose, we have embedded an attention mechanism to the generator. Concretely, instead of regressing a full image, our generator outputs two masks, a color mask C and attention mask A. The final image can be obtained as:\nI y f = (1 \u2212 A) \u2022 C + A \u2022 I yo ,(1)\nwhere A = G A (I yo |y f ) \u2208 {0, . . . , 1} H\u00d7W and C = G C (I yo |y f ) \u2208 R H\u00d7W \u00d73 . The mask A indicates to which extend each pixel of the C contributes to the output image I y f . In this way, the generator does not need to render static elements, and can focus exclusively on the pixels defining the facial movements, leading to sharper and more realistic synthetic images. This process is depicted in Fig. 3.\nConditional Critic. This is a network trained to evaluate the generated images in terms of their photo-realism and desired expression fulfillment. The structure of D(I) resembles that of the PatchGan [10] network mapping from the input image I to a matrix Y I \u2208 R H/2 6 \u00d7W/2 6 , where Y I [i, j] represents the probability of the overlapping patch ij to be real. Also, to evaluate its conditioning, on top of it we add an auxiliary regression head that estimates the AUs activation\u015d y = (\u0177 1 , . . . ,\u0177 N ) in the image.", "publication_ref": ["b9"], "figure_ref": ["fig_0"], "table_ref": []}, {"heading": "Learning the Model", "text": "The loss function we define contains four terms, namely an image adversarial loss [1] with the modification proposed by Gulrajani et al. [9] that pushes the distribution of the generated images to the distribution of the training images; the attention loss to drive the attention masks to be smooth and prevent them from saturating; the conditional expression loss that conditions the expression of the generated images to be similar to the desired one; and the identity loss that favors to preserve the person texture identity. Image Adversarial Loss. In order to learn the parameters of the generator G, we use the modification of the standard GAN algorithm [8] proposed by WGAN-GP [9]. Specifically, the original GAN formulation is based on the Jensen-Shannon (JS) divergence loss function and aims to maximize the probability of correctly classifying real and rendered images while the generator tries to foul the discriminator. This loss is potentially not continuous with respect to the generators parameters and can locally saturate leading to vanishing gradients in the discriminator. This is addressed in WGAN [1] by replacing JS with the continuous Earth Mover Distance. To maintain a Lipschitz constraint, WGAN-GP [9] proposes to add a gradient penalty for the critic network computed as the norm of the gradients with respect to the critic input. Formally, let I yo be the input image with the initial condition y o , y f the desired final condition, P o the data distribution of the input image, and P I the random interpolation distribution. Then, the critic loss L I (G, D I , I yo , y f ) we use is:\nE Iy o \u223cPo [D I (G(I yo |y f ))] \u2212 E Iy o \u223cPo [D I (I yo )] + \u03bb gp E I\u223cP I ( \u2207 I D I ( I) 2 \u2212 1) 2 ,\nwhere \u03bb gp is a penalty coefficient.\nAttention Loss. When training the model we do not have ground-truth annotation for the attention masks A. Similarly as for the color masks C, they are learned from the resulting gradients of the critic module and the rest of the losses. However, the attention masks can easily saturate to 1 which makes that I yo = G(I yo |y f ), that is, the generator has no effect. To prevent this situation, we regularize the mask with a l 2 -weight penalty. Also, to enforce smooth spatial color transformation when combining the pixel from the input image and the color transformation C, we perform a Total Variation Regularization over A.\nThe attention loss L A (G, I yo , y f ) can therefore be defined as:\n\u03bb TV E Iy o \u223cPo \uf8ee \uf8f0 H,W i,j (A i+1,j \u2212 A i,j ) 2 + (A i,j+1 \u2212 A i,j ) 2 \uf8f9 \uf8fb + E Iy o \u223cPo [ A 2 ] (2)\nwhere A = G A (I yo |y f ) and A i,j is the i, j entry of A. \u03bb TV is a penalty coefficient.\nConditional Expression Loss. While reducing the image adversarial loss, the generator must also reduce the error produced by the AUs regression head on top of D. In this way, G not only learns to render realistic samples but also learns to satisfy the target facial expression encoded by y f . This loss is defined with two components: an AUs regression loss with fake images used to optimize G, and an AUs regression loss of real images used to learn the regression head on top of D. This loss L y (G, D y , I yo , y o , y f ) is computed as:\nE Iy o \u223cPo D y (G(I yo |y f ))] \u2212 y f 2 2 + E Iy o \u223cPo D y (I yo ) \u2212 y o 2 2 .(3)\nIdentity Loss. With the previously defined losses the generator is enforced to generate photo-realistic face transformations. However, without ground-truth supervision, there is no constraint to guarantee that the face in both the input and output images correspond to the same person. Using a cycle consistency loss [38] we force the generator to maintain the identity of each individual by penalizing the difference between the original image I yo and its reconstruction:\nL idt (G, I yo , y o , y f ) = E Iy o \u223cPo [ G(G(I yo |y f )|y o ) \u2212 I yo 1 ] .(4)\nTo produce realistic images it is critical for the generator to model both low and high frequencies. Our PatchGan based critic D I already enforces high-frequency correctness by restricting our attention to the structure in local image patches.\nTo also capture low-frequencies it is sufficient to use l 1 -norm. In preliminary experiments, we also tried replacing l 1 -norm with a more sophisticated Perceptual [11] loss, although we did not observe improved performance. Full Loss. To generate the target image I yg , we build a loss function L by linearly combining all previous partial losses:\nL =L I (G, D I , I yr , y g ) + \u03bb y L y (G, D y , I yr , y r , y g ) (5) + \u03bb A L A (G, I yg , y r ) + L A (G, I yr , y g ) + \u03bb idt L idt (G, I yr , y r , y g ),\nwhere \u03bb A , \u03bb y and \u03bb idt are the hyper-parameters that control the relative importance of every loss term. Finally, we can define the following minimax problem:\nG = arg min G max D\u2208D L ,(6)\nwhere G draws samples from the data distribution. Additionally, we constrain our discriminator D to lie in D, that represents the set of 1-Lipschitz functions.", "publication_ref": ["b0", "b8", "b7", "b8", "b0", "b8", "b37", "b10"], "figure_ref": [], "table_ref": []}, {"heading": "Implementation Details", "text": "Our generator builds upon the variation of the network from Johnson et al. [11] proposed by [38] as it proved to achieve impressive results for image-to-image mapping. We have slightly modified it by substituting the last convolutional layer with two parallel convolutional layers, one to regress the color mask C and the other to define the attention mask A. We also observed that changing batch normalization in the generator by instance normalization improved training stability. For the critic we have adopted the PatchGan architecture of [10], but removing feature normalization. Otherwise, when computing the gradient penalty, the norm of the critic's gradient would be computed with respect to the entire batch and not with respect to each input independently. The model is trained on the EmotioNet dataset [3]. We use a subset of 200,000 samples (over 1 million) to reduce training time. We use Adam [14] with learning rate of 0.0001, beta1 0.5, beta2 0.999 and batch size 25. We train for 30 epochs and linearly decay the rate to zero over the last 10 epochs. Every 5 optimization steps of the critic network we perform a single optimization step of the generator. The weight coefficients for the loss terms in Eq. ( 5) are set to \u03bb gp = 10, \u03bb A = 0.1, \u03bb TV = 0.0001, \u03bb y = 4000, \u03bb idt = 10. To improve stability we tried updating the critic using a buffer with generated images in different updates of the generator as proposed in [32] but we did not observe performance improvement. The model takes two days to train with a single GeForce R GTX 1080 Ti GPU. ", "publication_ref": ["b10", "b37", "b9", "b2", "b13", "b31"], "figure_ref": [], "table_ref": []}, {"heading": "Experimental Evaluation", "text": "This section provides a thorough evaluation of our system. We first test the main component, namely the single and multiple AUs editing. We then compare our model against current competing techniques in the task of discrete emotions editing and demonstrate our model's ability to deal with images in the wild and its capability to generate a wide range of anatomically coherent face transformations. Finally, we discuss the model's limitations and failure cases.\nIt is worth noting that in some of the experiments the input faces are not cropped. In this cases we first use a detector 2 to localize and crop the face, apply the expression transformation to that area with Eq. (1), and finally place the generated face back to its original position in the image. The attention mechanism guaranties a smooth transition between the morphed cropped face and the original image. As we shall see later, this three steps process results on higher resolution images compared to previous models. Supplementary material can be found on http://www.albertpumarola.com/research/GANimation/.", "publication_ref": [], "figure_ref": [], "table_ref": []}, {"heading": "Single Action Units Edition", "text": "We first evaluate our model's ability to activate AUs at different intensities while preserving the person's identity. Figure 4 shows a subset of 9 AUs individually transformed with four levels of intensity (0, 0.33, 0.66, 1). For the case of 0 intensity it is desired not to change the corresponding AU. The model properly handles this situation and generates an identical copy of the input image for every case. The ability to apply an identity transformation is essential to ensure that non-desired facial movement will not be introduced. For the non-zero cases, it can be observed how each AU is progressively accentuated. Note the difference between generated images at intensity 0 and 1. The model convincingly renders complex facial movements which in most cases are difficult to distinguish from real images. It is also worth mentioning that the independence of facial muscle cluster is properly learned by the generator. AUs relative to the eyes and half-upper part of the face (AUs 1, 2, 4, 5, 45) do not affect the muscles of the mouth. Equivalently, mouth related transformations (AUs 10, 12, 15, 25) do not affect eyes nor eyebrow muscles. Fig. 5 displays, for the same experiment, the attention A and color C masks that produced the final result I yg . Note how the model has learned to focus its attention (darker area) onto the corresponding AU in an unsupervised manner. In this way, it relieves the color mask from having to accurately regress each pixel value. Only the pixels relevant to the expression change are carefully estimated, the rest are just noise. For example, the attention is clearly obviating background pixels allowing to directly copy them from the original image. This is a key ingredient to later being able to handle images in the wild (see Section 6.5).", "publication_ref": [], "figure_ref": ["fig_1", "fig_2"], "table_ref": []}, {"heading": "Simultaneous Edition of Multiple AUs", "text": "We next push the limits of our model and evaluate it in editing multiple AUs. Additionally, we also assess its ability to interpolate between two expressions. The results of this experiment are shown in Fig. 1, the first column is the original image with expression y r , and the right-most column is a synthetically generated image conditioned on a target expression y g . The rest of columns result from evaluating the generator conditioned with a linear interpolation of the original and target expressions: \u03b1y g + (1 \u2212 \u03b1)y r . The outcomes show a very remarkable smooth an consistent transformation across frames. We have intentionally selected challenging samples to show robustness to light conditions and even, as in the case of the avatar, to non-real world data distributions which were not previously seen by the model. These results are encouraging to further extend the model to video generation in future works.  [20], CycleGAN [28], IcGAN [26] and StarGAN [4]; and ours. In all cases, we represent the input image and seven different facial expressions. As it can be seen, our solution produces the best trade-off between visual accuracy and spatial resolution. Some of the results of StarGAN, the best current approach, show certain level of blur. Images of previous models were taken from [4].", "publication_ref": ["b19", "b27", "b25", "b3", "b3"], "figure_ref": [], "table_ref": []}, {"heading": "Discrete Emotions Editing", "text": "We next compare our approach, against the baselines DIAT [20], CycleGAN [28], IcGAN [26] and StarGAN [4]. For a fair comparison, we adopt the results of these methods trained by the most recent work, StarGAN, on the task of rendering discrete emotions categories (e.g., happy, sad and fearful) in the RaFD dataset [16]. Since DIAT [20] and CycleGAN [28] do not allow conditioning, they were independently trained for every possible pair of source/target emotions. We next briefly discuss the main aspects of each approach: DIAT [20]. Given an input image x \u2208 X and a reference image y \u2208 Y , DIAT learns a GAN model to render the attributes of domain Y in the image x while conserving the person's identity. It is trained with the classic adversarial loss and a cycle loss x \u2212 G Y \u2192X (G X\u2192Y (x)) 1 to preserve the person's identity.\nCycleGAN [28]. Similar to DIAT [20], CycleGAN also learns the mapping between two domains X \u2192 Y and Y \u2192 X. To train the domain transfer, it uses a regularization term denoted cycle consistency loss combining two cycles:\nx \u2212 G Y \u2192X (G X\u2192Y (x)) 1 and y \u2212 G X\u2192Y (G Y \u2192X (y)) 1 .\nIcGAN [26]. Given an input image, IcGAN uses a pretrained encoder-decoder to encode the image into a latent representation in concatenation with an expression vector y to then reconstruct the original image. It can modify the expression by replacing y with the desired expression before going through the decoder. StarGAN [4]. An extension of cycle loss for simultaneously training between multiple datasets with different data domains. It uses a mask vector to ignore unspecified labels and optimize only on known ground-truth labels. It yields more realistic results when training simultaneously with multiple datasets.\nOur model differs from these approaches in two main aspects. First, we do not condition the model on discrete emotions categories, but we learn a basis of anatomically feasible warps that allows generating a continuum of expressions. Secondly, the use of the attention mask allows applying the transformation only on the cropped face, and put it back onto the original image without producing any artifact. As shown in Fig. 6, besides estimating more visually compelling images than other approaches, this results on images of higher spatial resolution.", "publication_ref": ["b19", "b27", "b25", "b3", "b15", "b19", "b27", "b19", "b27", "b19", "b25", "b3"], "figure_ref": ["fig_3"], "table_ref": []}, {"heading": "High Expressions Variability", "text": "Given a single image, we next use our model to produce a wide range of anatomically feasible face expressions while conserving the person's identity. In Fig. 7 all faces are the result of conditioning the input image in the top-left corner with a desired face configuration defined by only 14 AUs. Note the large variability of anatomically feasible expressions that can be synthesized with only 14 AUs.", "publication_ref": [], "figure_ref": ["fig_4"], "table_ref": []}, {"heading": "Images in the Wild", "text": "As previously seen in Fig. 5, the attention mechanism not only learns to focus on specific areas of the face but also allows merging the original and generated image background. This allows our approach to be easily applied to images in the wild while still obtaining high resolution images. For these images we follow the Fig. 8. Qualitative evaluation on images in the wild. Top: We represent an image (left) from the film \"Pirates of the Caribbean\" and an its generated image obtained by our approach (right). Bottom: In a similar manner, we use an image frame (left) from the series \"Game of Thrones\" to synthesize five new images with different expressions.\ndetection and cropping scheme we described before. Fig. 8 shows two examples on these challenging images. Note how the attention mask allows for a smooth and unnoticeable merge between the entire frame and the generated faces.", "publication_ref": [], "figure_ref": ["fig_2"], "table_ref": []}, {"heading": "Pushing the Limits of the Model", "text": "We next push the limits of our network and discuss the model limitations. We have split success cases into six categories which we summarize in Fig. 9-top. The first two examples (top-row) correspond to human-like sculptures and nonrealistic drawings. In both cases, the generator is able to maintain the artistic effects of the original image. Also, note how the attention mask ignores artifacts such as the pixels occluded by the glasses. The third example shows robustness to non-homogeneous textures across the face. Observe that the model is not trying to homogenize the texture by adding/removing the beard's hair. The middleright category relates to anthropomorphic faces with non-real textures. As for the Avatar image, the network is able to warp the face without affecting its texture. The next category is related to non-standard illuminations/colors for which the model has already been shown robust in Fig. 1. The last and most surprising category is face-sketches (bottom-right). Although the generated face suffers from some artifacts, it is still impressive how the proposed method is still capable of finding sufficient features on the face to transform its expression from worried to excited. The second case shows failures with non-previously seen occlusions such as an eye patch causing artifacts in the missing face attributes.\nWe have also categorized the failure cases in Fig. 9-bottom, all of them presumably due to insufficient training data. The first case is related to errors in the attention mechanism when given extreme input expressions. The attention does not weight sufficiently the color transformation causing transparencies.\nThe model also fails when dealing with non-human anthropomorphic distributions as in the case of cyclopes. Lastly, we tested the model behavior when dealing with animals and observed artifacts like human face features. In all cases, we represent the source image Iy r , the target one Iy g , and the color and attention masks C and A, respectively. Top: Some success cases in extreme situations. Bottom: Several failure cases.", "publication_ref": [], "figure_ref": ["fig_5", "fig_5"], "table_ref": []}, {"heading": "Conclusions", "text": "We have presented a novel GAN model for face animation in the wild that can be trained in a fully unsupervised manner. It advances current works which, so far, had only addressed the problem for discrete emotions category editing and portrait images. Our model encodes anatomically consistent face deformations parameterized by means of AUs. Conditioning the GAN model on these AUs allows the generator to render a wide range of expressions by simple interpolation. Additionally, we embed an attention model within the network which allows focusing only on those regions of the image relevant for every specific expression. By doing this, we can easily process images in the wild, with distracting backgrounds and illumination artifacts. We have exhaustively evaluated the model capabilities and limits in the EmotioNet [3] and RaFD [16] datasets as well as in images from movies. The results are very promising, and show smooth transitions between different expressions. This opens the possibility of applying our approach to video sequences, which we plan to do in the future.", "publication_ref": ["b2", "b15"], "figure_ref": [], "table_ref": []}], "references": [{"ref_id": "b0", "title": "", "journal": "", "year": "2017", "authors": "M Arjovsky; S Chintala; L Bottou"}, {"ref_id": "b1", "title": "Cross-dataset learning and personspecific normalisation for automatic action unit detection", "journal": "FG", "year": "2015", "authors": "T Baltru\u0161aitis; M Mahmoud; P Robinson"}, {"ref_id": "b2", "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild", "journal": "CVPR", "year": "2016", "authors": "C F Benitez-Quiroz; R Srinivasan; A M Martinez"}, {"ref_id": "b3", "title": "Stargan: Unified generative adversarial networks for multi-domain image-to-image translation", "journal": "CVPR", "year": "2018", "authors": "Y Choi; M Choi; M Kim; J W Ha; S Kim; J Choo"}, {"ref_id": "b4", "title": "Compound facial expressions of emotion. Proceedings of the National Academy of Sciences", "journal": "", "year": "2014", "authors": "S Du; Y Tao; A M Martinez"}, {"ref_id": "b5", "title": "Facial action coding system: A technique for the measurement of facial movement", "journal": "Consulting Psychologists Press", "year": "1978", "authors": "P Ekman; W Friesen"}, {"ref_id": "b6", "title": "The representation and matching of pictorial structures", "journal": "IEEE Transactions on Computers", "year": "1973", "authors": "M A Fischler; R A Elschlager"}, {"ref_id": "b7", "title": "Generative adversarial nets", "journal": "", "year": "2014", "authors": "I Goodfellow; J Pouget-Abadie; M Mirza; B Xu; D Warde-Farley; S Ozair; A Courville; Y Bengio"}, {"ref_id": "b8", "title": "Improved training of wasserstein GANs", "journal": "NIPS", "year": "2017", "authors": "I Gulrajani; F Ahmed; M Arjovsky; V Dumoulin; A C Courville"}, {"ref_id": "b9", "title": "Image-to-image translation with conditional adversarial networks", "journal": "CVPR", "year": "2017", "authors": "P Isola; J Y Zhu; T Zhou; A A Efros"}, {"ref_id": "b10", "title": "Perceptual losses for real-time style transfer and super-resolution", "journal": "ECCV", "year": "2016", "authors": "J Johnson; A Alahi; L Fei-Fei"}, {"ref_id": "b11", "title": "Progressive growing of GANs for improved quality, stability, and variation", "journal": "ICLR", "year": "2018", "authors": "T Karras; T Aila; S Laine; J Lehtinen"}, {"ref_id": "b12", "title": "Learning to discover cross-domain relations with generative adversarial networks", "journal": "ICML", "year": "2017", "authors": "T Kim; M Cha; H Kim; J Lee; J Kim"}, {"ref_id": "b13", "title": "ADAM: A method for stochastic optimization", "journal": "ICLR", "year": "2015", "authors": "D Kingma; J Ba"}, {"ref_id": "b14", "title": "Auto-encoding variational bayes", "journal": "ICLR", "year": "2014", "authors": "D P Kingma; M Welling"}, {"ref_id": "b15", "title": "Presentation and validation of the radboud faces database", "journal": "Cognition and emotion", "year": "2010", "authors": "O Langner; R Dotsch; G Bijlstra; D H Wigboldus; S T Hawk; A Van Knippenberg"}, {"ref_id": "b16", "title": "Autoencoding beyond pixels using a learned similarity metric", "journal": "ICML", "year": "2016", "authors": "A B L Larsen; S K S\u00f8nderby; H Larochelle; O Winther"}, {"ref_id": "b17", "title": "Photo-realistic single image superresolution using a generative adversarial network", "journal": "CVPR", "year": "2017", "authors": "C Ledig; L Theis; F Husz\u00e1r; J Caballero; A Cunningham; A Acosta; A Aitken; A Tejani; J Totz; Z Wang"}, {"ref_id": "b18", "title": "Precomputed real-time texture synthesis with markovian generative adversarial networks", "journal": "ECCV", "year": "2016", "authors": "C Li; M Wand"}, {"ref_id": "b19", "title": "Deep identity-aware transfer of facial attributes", "journal": "", "year": "2016", "authors": "M Li; W Zuo; D Zhang"}, {"ref_id": "b20", "title": "Unsupervised image-to-image translation networks", "journal": "NIPS", "year": "2017", "authors": "M Y Liu; T Breuel; J Kautz"}, {"ref_id": "b21", "title": "Deep multi-scale video prediction beyond mean square error", "journal": "ICLR", "year": "2016", "authors": "M Mathieu; C Couprie; Y Lecun"}, {"ref_id": "b22", "title": "Conditional generative adversarial nets", "journal": "", "year": "2014", "authors": "M Mirza; S Osindero"}, {"ref_id": "b23", "title": "Conditional image synthesis with auxiliary classifier GANs", "journal": "ICML", "year": "2017", "authors": "A Odena; C Olah; J Shlens"}, {"ref_id": "b24", "title": "Context encoders: Feature learning by inpainting", "journal": "CVPR", "year": "2016", "authors": "D Pathak; P Krahenbuhl; J Donahue; T Darrell; A A Efros"}, {"ref_id": "b25", "title": "Invertible conditional GANs for image editing", "journal": "", "year": "2016", "authors": "G Perarnau; J Van De Weijer; B Raducanu; J M \u00c1lvarez"}, {"ref_id": "b26", "title": "Unsupervised person image synthesis in arbitrary poses", "journal": "CVPR", "year": "2018", "authors": "A Pumarola; A Agudo; A Sanfeliu; F Moreno-Noguer"}, {"ref_id": "b27", "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "journal": "ICLR", "year": "2016", "authors": "A Radford; L Metz; S Chintala"}, {"ref_id": "b28", "title": "Generative adversarial text to image synthesis", "journal": "ICML", "year": "2016", "authors": "S Reed; Z Akata; X Yan; L Logeswaran; B Schiele; H Lee"}, {"ref_id": "b29", "title": "Emotion as a process: Function, origin and regulation", "journal": "Social Science Information", "year": "1982", "authors": "K R Scherer"}, {"ref_id": "b30", "title": "Learning residual images for face attribute manipulation", "journal": "CVPR", "year": "2017", "authors": "W Shen; R Liu"}, {"ref_id": "b31", "title": "Learning from simulated and unsupervised images through adversarial training", "journal": "CVPR", "year": "2017", "authors": "A Shrivastava; T Pfister; O Tuzel; J Susskind; W Wang; R Webb"}, {"ref_id": "b32", "title": "Generative image modeling using style and structure adversarial networks", "journal": "ECCV", "year": "2016", "authors": "X Wang; A Gupta"}, {"ref_id": "b33", "title": "Deep networks for image superresolution with sparse prior", "journal": "ICCV", "year": "2015", "authors": "Z Wang; D Liu; J Yang; W Han; T Huang"}, {"ref_id": "b34", "title": "Perception-driven facial expression synthesis", "journal": "Computers & Graphics", "year": "2012", "authors": "H Yu; O G Garrod; P G Schyns"}, {"ref_id": "b35", "title": "The menpo facial landmark localisation challenge: A step towards the solution", "journal": "CVPRW", "year": "2017", "authors": "S Zafeiriou; G Trigeorgis; G Chrysos; J Deng; J Shen"}, {"ref_id": "b36", "title": "Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks", "journal": "ICCV", "year": "2017", "authors": "H Zhang; T Xu; H Li; S Zhang; X Huang; X Wang; D Metaxas"}, {"ref_id": "b37", "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "journal": "ICCV", "year": "2017", "authors": "J Y Zhu; T Park; P Isola; A A Efros"}, {"ref_id": "b38", "title": "Be your own prada: Fashion synthesis with structural coherence", "journal": "ICCV", "year": "2017", "authors": "S Zhu; S Fidler; R Urtasun; D Lin; C C Loy"}], "figures": [{"figure_label": "3", "figure_type": "figure", "figure_id": "fig_0", "figure_caption": "Fig. 3 .3Fig. 3. Attention-based generator. Given an input image and the target expression, the generator regresses and attention mask A and an RGB color transformation C over the entire image. The attention mask defines a per pixel intensity specifying to which extend each pixel of the original image will contribute in the final rendered image.", "figure_data": ""}, {"figure_label": "4", "figure_type": "figure", "figure_id": "fig_1", "figure_caption": "Fig. 4 .4Fig.4. Single AUs edition. Specific AUs are activated at increasing levels of intensity (from 0.33 to 1). The first row corresponds to a zero intensity application of the AU which correctly produces the original image in all cases.", "figure_data": ""}, {"figure_label": "5", "figure_type": "figure", "figure_id": "fig_2", "figure_caption": "Fig. 5 .5Fig. 5. Attention Model. Details of the intermediate attention mask A (first row) and the color mask C (second row). The bottom row images are the synthesized expressions. Darker regions of the attention mask A show those areas of the image more relevant for each specific AU. Brighter areas are retained from the original image.", "figure_data": ""}, {"figure_label": "6", "figure_type": "figure", "figure_id": "fig_3", "figure_caption": "Fig. 6 .6Fig.6. Qualitative comparison with state-of-the-art. Facial Expression Synthesis results for: DIAT[20], CycleGAN[28], IcGAN[26] and StarGAN[4]; and ours. In all cases, we represent the input image and seven different facial expressions. As it can be seen, our solution produces the best trade-off between visual accuracy and spatial resolution. Some of the results of StarGAN, the best current approach, show certain level of blur. Images of previous models were taken from[4].", "figure_data": ""}, {"figure_label": "7", "figure_type": "figure", "figure_id": "fig_4", "figure_caption": "Fig. 7 .7Fig.7. Sampling the face expression distribution space. As a result of applying our AU-parametrization through the vector yg, we can synthesize, from the same source image Iy r , a large variety of photo-realistic images.", "figure_data": ""}, {"figure_label": "9", "figure_type": "figure", "figure_id": "fig_5", "figure_caption": "Fig. 9 .9Fig.9. Success and Failure Cases. In all cases, we represent the source image Iy r , the target one Iy g , and the color and attention masks C and A, respectively. Top: Some success cases in extreme situations. Bottom: Several failure cases.", "figure_data": ""}], "formulas": [{"formula_id": "formula_0", "formula_text": "G A G I G I D y G A W D D IF ig. 2.", "formula_coordinates": [5.0, 40.68, 44.45, 293.8, 130.75]}, {"formula_id": "formula_1", "formula_text": "M training triplets {I m yr , y m r , y m g } M m=1", "formula_coordinates": [5.0, 100.03, 261.35, 165.03, 12.2]}, {"formula_id": "formula_2", "formula_text": "I y f = (1 \u2212 A) \u2022 C + A \u2022 I yo ,(1)", "formula_coordinates": [6.0, 144.86, 257.65, 234.99, 10.35]}, {"formula_id": "formula_3", "formula_text": "E Iy o \u223cPo [D I (G(I yo |y f ))] \u2212 E Iy o \u223cPo [D I (I yo )] + \u03bb gp E I\u223cP I ( \u2207 I D I ( I) 2 \u2212 1) 2 ,", "formula_coordinates": [7.0, 38.64, 190.73, 336.58, 15.67]}, {"formula_id": "formula_4", "formula_text": "\u03bb TV E Iy o \u223cPo \uf8ee \uf8f0 H,W i,j (A i+1,j \u2212 A i,j ) 2 + (A i,j+1 \u2212 A i,j ) 2 \uf8f9 \uf8fb + E Iy o \u223cPo [ A 2 ] (2)", "formula_coordinates": [7.0, 39.06, 351.87, 340.79, 33.53]}, {"formula_id": "formula_5", "formula_text": "E Iy o \u223cPo D y (G(I yo |y f ))] \u2212 y f 2 2 + E Iy o \u223cPo D y (I yo ) \u2212 y o 2 2 .(3)", "formula_coordinates": [7.0, 67.32, 509.4, 312.53, 13.33]}, {"formula_id": "formula_6", "formula_text": "L idt (G, I yo , y o , y f ) = E Iy o \u223cPo [ G(G(I yo |y f )|y o ) \u2212 I yo 1 ] .(4)", "formula_coordinates": [8.0, 81.0, 68.43, 298.84, 11.29]}, {"formula_id": "formula_7", "formula_text": "L =L I (G, D I , I yr , y g ) + \u03bb y L y (G, D y , I yr , y r , y g ) (5) + \u03bb A L A (G, I yg , y r ) + L A (G, I yr , y g ) + \u03bb idt L idt (G, I yr , y r , y g ),", "formula_coordinates": [8.0, 56.72, 193.61, 323.12, 25.72]}, {"formula_id": "formula_8", "formula_text": "G = arg min G max D\u2208D L ,(6)", "formula_coordinates": [8.0, 160.54, 260.3, 219.31, 14.58]}, {"formula_id": "formula_9", "formula_text": "x \u2212 G Y \u2192X (G X\u2192Y (x)) 1 and y \u2212 G X\u2192Y (G Y \u2192X (y)) 1 .", "formula_coordinates": [11.0, 39.0, 523.32, 245.56, 9.65]}], "doi": ""}